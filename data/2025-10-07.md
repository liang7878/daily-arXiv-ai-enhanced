<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 90]
- [cs.CE](#cs.CE) [Total: 7]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.DS](#cs.DS) [Total: 5]
- [cs.GT](#cs.GT) [Total: 7]
- [cs.IR](#cs.IR) [Total: 11]
- [cs.LG](#cs.LG) [Total: 286]
- [cs.NE](#cs.NE) [Total: 4]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.SE](#cs.SE) [Total: 46]
- [q-fin.ST](#q-fin.ST) [Total: 2]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [stat.ML](#stat.ML) [Total: 22]
- [stat.CO](#stat.CO) [Total: 5]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.CV](#cs.CV) [Total: 66]
- [cs.CY](#cs.CY) [Total: 11]
- [q-bio.NC](#q-bio.NC) [Total: 4]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [cs.CR](#cs.CR) [Total: 16]
- [math.NA](#math.NA) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 2]
- [physics.chem-ph](#physics.chem-ph) [Total: 2]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.HC](#cs.HC) [Total: 4]
- [cs.RO](#cs.RO) [Total: 11]
- [cs.OS](#cs.OS) [Total: 1]
- [eess.IV](#eess.IV) [Total: 3]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [eess.SY](#eess.SY) [Total: 4]
- [econ.GN](#econ.GN) [Total: 7]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 3]
- [cs.CG](#cs.CG) [Total: 2]
- [cs.LO](#cs.LO) [Total: 2]
- [eess.AS](#eess.AS) [Total: 3]
- [cs.SD](#cs.SD) [Total: 6]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [math.PR](#math.PR) [Total: 2]
- [cs.GR](#cs.GR) [Total: 7]
- [cs.NI](#cs.NI) [Total: 5]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.IT](#cs.IT) [Total: 2]
- [math.OC](#math.OC) [Total: 10]
- [cs.MA](#cs.MA) [Total: 6]
- [quant-ph](#quant-ph) [Total: 3]
- [cs.SI](#cs.SI) [Total: 2]
- [math.ST](#math.ST) [Total: 1]
- [cs.DM](#cs.DM) [Total: 2]
- [stat.ME](#stat.ME) [Total: 6]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [cs.CL](#cs.CL) [Total: 63]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [WAREX: Web Agent Reliability Evaluation on Existing Benchmarks](https://arxiv.org/abs/2510.03285)
*Su Kara,Fazle Faisal,Suman Nath*

Main category: cs.AI

TL;DR: 论文提出WAREX评估方法，发现引入它会使现有基准测试中任务成功率显著下降，凸显当前浏览器LLM代理鲁棒性有限。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在可控环境下进行，未考虑现实网络不稳定、网站受攻击和修改等情况，需新评估方法。

Method: 提出WAREX，在WebArena、WebVoyager和REAL三个流行基准测试中测量其影响。

Result: 引入WAREX导致任务成功率显著下降。

Conclusion: 当前最先进的浏览器LLM代理鲁棒性有限。

Abstract: Recent advances in browser-based LLM agents have shown promise for automating
tasks ranging from simple form filling to hotel booking or online shopping.
Current benchmarks measure agent performance in controlled environments, such
as containers or stable networks, where websites behave deterministically.
However, in the real world, users access websites over networks and HTTPS
connections that introduce instability from multiple sources: client-side,
server-side issues or broader system failures. Moreover, live websites are
prone to web attacks such Cross-Site Scripting, as well as general site
modifications which can cause unexpected or malicious pop-ups or improper
functionality. To address this gap, we present WAREX: Web Agent Reliability
Evaluation on Existing Benchmarks. We measure the impact of WAREX across three
popular benchmarks: WebArena, WebVoyager, and REAL. Our experiments show that
introducing WAREX leads to significant drops in task success rates,
highlighting the limited robustness of state-of-the-art agents.

</details>


### [2] [Refined Iterated Pareto Greedy for Energy-aware Hybrid Flowshop Scheduling with Blocking Constraints](https://arxiv.org/abs/2510.03377)
*Ahmed Missaoui,Cemalettin Ozturk,Barry O'Sullivan*

Main category: cs.AI

TL;DR: 因能源问题，研究制造领域阻塞约束混合流水车间调度问题，提出新方法求解，计算结果证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 全球能源问题促使制造业寻求节能方案，阻塞约束混合流水车间调度问题有节能需求。

Method: 将问题建模为多目标混合整数规划模型，提出增强epsilon - 约束法找帕累托最优解，开发精炼迭代帕累托贪心算法求解大规模实例。

Result: 用不同规模实例对方法进行基准测试，与两个知名算法对比，计算结果显示方法有效。

Conclusion: 所提方法能有效解决阻塞约束混合流水车间调度问题，平衡完工时间和能耗。

Abstract: The scarcity of non-renewable energy sources, geopolitical problems in its
supply, increasing prices, and the impact of climate change, force the global
economy to develop more energy-efficient solutions for their operations. The
Manufacturing sector is not excluded from this challenge as one of the largest
consumers of energy. Energy-efficient scheduling is a method that attracts
manufacturing companies to reduce their consumption as it can be quickly
deployed and can show impact immediately. In this study, the hybrid flow shop
scheduling problem with blocking constraint (BHFS) is investigated in which we
seek to minimize the latest completion time (i.e. makespan) and overall energy
consumption, a typical manufacturing setting across many industries from
automotive to pharmaceutical. Energy consumption and the latest completion time
of customer orders are usually conflicting objectives. Therefore, we first
formulate the problem as a novel multi-objective mixed integer programming
(MIP) model and propose an augmented epsilon-constraint method for finding the
Pareto-optimal solutions. Also, an effective multi-objective metaheuristic
algorithm. Refined Iterated Pareto Greedy (RIPG), is developed to solve large
instances in reasonable time. Our proposed methods are benchmarked using small,
medium, and large-size instances to evaluate their efficiency. Two well-known
algorithms are adopted for comparing our novel approaches. The computational
results show the effectiveness of our method.

</details>


### [3] [Know Thyself? On the Incapability and Implications of AI Self-Recognition](https://arxiv.org/abs/2510.03399)
*Xiaoyan Bai,Aryan Shrivastava,Ari Holtzman,Chenhao Tan*

Main category: cs.AI

TL;DR: 本文引入系统评估框架测试10个大语言模型自我识别能力，发现模型自我识别普遍失败，且有预测偏差，还评估了模型对自身和其他模型存在的认知及推理情况，最后讨论研究对AI安全的影响和未来方向。


<details>
  <summary>Details</summary>
Motivation: 此前对于模型是否具备自我识别能力存在矛盾解释，需引入系统评估框架进行研究。

Method: 引入可轻松应用和更新的系统评估框架，通过二元自我识别和精确模型预测两个任务，测量10个当代大语言模型识别自身生成文本和其他模型生成文本的能力。

Result: 模型自我识别普遍失败，10个模型中仅4个能预测自己为生成者，表现很少高于随机水平；模型强烈偏向预测GPT和Claude系列；模型对自身和其他模型存在有一定认知，但推理有层级偏差，常将高质量文本与GPT、Claude和偶尔的Gemini关联。

Conclusion: 讨论研究结果对AI安全的影响和开发适当AI自我意识的未来方向。

Abstract: Self-recognition is a crucial metacognitive capability for AI systems,
relevant not only for psychological analysis but also for safety, particularly
in evaluative scenarios. Motivated by contradictory interpretations of whether
models possess self-recognition (Panickssery et al., 2024; Davidson et al.,
2024), we introduce a systematic evaluation framework that can be easily
applied and updated. Specifically, we measure how well 10 contemporary larger
language models (LLMs) can identify their own generated text versus text from
other models through two tasks: binary self-recognition and exact model
prediction. Different from prior claims, our results reveal a consistent
failure in self-recognition. Only 4 out of 10 models predict themselves as
generators, and the performance is rarely above random chance. Additionally,
models exhibit a strong bias toward predicting GPT and Claude families. We also
provide the first evaluation of model awareness of their own and others'
existence, as well as the reasoning behind their choices in self-recognition.
We find that the model demonstrates some knowledge of its own existence and
other models, but their reasoning reveals a hierarchical bias. They appear to
assume that GPT, Claude, and occasionally Gemini are the top-tier models, often
associating high-quality text with them. We conclude by discussing the
implications of our findings on AI safety and future directions to develop
appropriate AI self-awareness.

</details>


### [4] [ContraGen: A Multi-Agent Generation Framework for Enterprise Contradictions Detection](https://arxiv.org/abs/2510.03418)
*Ananya Mantravadi,Shivali Dalmia,Abhishek Mukherji,Nand Dave,Anudha Mittal*

Main category: cs.AI

TL;DR: 提出ContraGen框架解决RAG系统在企业领域证据矛盾问题，为可靠RAG系统奠定基础。


<details>
  <summary>Details</summary>
Motivation: RAG系统中检索证据矛盾会产生不可信输出，现有矛盾检测基准无法满足企业文档需求。

Method: 提出ContraGen框架，生成含矛盾的合成企业文档，结合自动矛盾挖掘和人工验证。

Result: 实现生成真实企业文档、建模矛盾类型分类、创建矛盾、开发评估管道和引入人工监督。

Conclusion: 该工作为企业信息检索应用中更可靠、可问责的RAG系统奠定基础。

Abstract: Retrieval-Augmented Generation (RAG) integrates LLMs with external sources,
offering advanced capabilities for information access and decision-making.
However, contradictions in retrieved evidence can result in inconsistent or
untrustworthy outputs, which is especially problematic in enterprise settings
where compliance, governance, and accountability are critical. Existing
benchmarks for contradiction detection are limited to sentence-level analysis
and do not capture the complexity of enterprise documents such as contracts,
financial filings, compliance reports, or policy manuals. To address this
limitation, we propose ContraGen, a contradiction-aware benchmark framework
tailored to enterprise domain. The framework generates synthetic
enterprise-style documents with embedded contradictions, enabling systematic
evaluation of both intra-document and cross-document consistency. Automated
contradiction mining is combined with human-in-the-loop validation to ensure
high accuracy. Our contributions include generating realistic enterprise
documents, modeling a taxonomy of contradiction types common in business
processes, enabling controlled creation of self- and pairwise contradictions,
developing a contradiction-aware retrieval evaluation pipeline and embedding
human oversight to reflect domain-specific judgment complexity. This work
establishes a foundation for more trustworthy and accountable RAG systems in
enterprise information-seeking applications, where detecting and resolving
contradictions is essential for reducing risk and ensuring compliance.

</details>


### [5] [A Qualitative Comparative Evaluation of Cognitive and Generative Theories](https://arxiv.org/abs/2510.03453)
*Paul S. Rosenbloom*

Main category: cs.AI

TL;DR: 论文探讨对基于认知架构和生成式神经架构的理论进行评估的挑战，并采用广泛视角对相关架构及系统进行定性比较。


<details>
  <summary>Details</summary>
Motivation: 解决基于认知架构和生成式神经架构的理论评估极具挑战性的问题。

Method: 运用对理论评估的广泛视角，对全脑导向的认知和生成式架构及其完整系统进行定性比较。

Result: 文档未提及具体结果。

Conclusion: 文档未提及具体结论。

Abstract: Evaluation is a critical activity associated with any theory. Yet this has
proven to be an exceptionally challenging activity for theories based on
cognitive architectures. For an overlapping set of reasons, evaluation can also
be challenging for theories based on generative neural architectures. This dual
challenge is approached here by leveraging a broad perspective on theory
evaluation to yield a wide-ranging, albeit qualitative, comparison of
whole-mind-oriented cognitive and generative architectures and the full systems
that are based on these architectures.

</details>


### [6] [Bridging LLM Planning Agents and Formal Methods: A Case Study in Plan Verification](https://arxiv.org/abs/2510.03469)
*Keshav Ramani,Vali Tawosi,Salwa Alamir,Daniel Borrajo*

Main category: cs.AI

TL;DR: 引入新框架评估自然语言计划与预期行为的对齐，在简化数据集上评估，GPT - 5表现出色但语义完美形式模型待探索。


<details>
  <summary>Details</summary>
Motivation: 评估自然语言计划与预期行为的对齐情况。

Method: 使用大语言模型将自然语言计划转换为Kripke结构和线性时态逻辑（LTL）并进行模型检查，在简化版PlanBench计划验证数据集上进行系统评估。

Result: GPT - 5分类性能出色，F1分数达96.3%，几乎总能产生语法完美的形式表示。

Conclusion: GPT - 5在语法形式表示上表现好，但语义完美形式模型的合成还需未来探索。

Abstract: We introduce a novel framework for evaluating the alignment between natural
language plans and their expected behavior by converting them into Kripke
structures and Linear Temporal Logic (LTL) using Large Language Models (LLMs)
and performing model checking. We systematically evaluate this framework on a
simplified version of the PlanBench plan verification dataset and report on
metrics like Accuracy, Precision, Recall and F1 scores. Our experiments
demonstrate that GPT-5 achieves excellent classification performance (F1 score
of 96.3%) while almost always producing syntactically perfect formal
representations that can act as guarantees. However, the synthesis of
semantically perfect formal models remains an area for future exploration.

</details>


### [7] [Towards Policy-Compliant Agents: Learning Efficient Guardrails For Policy Violation Detection](https://arxiv.org/abs/2510.03485)
*Xiaofei Wen,Wenjie Jacky Mo,Yanan Xie,Peng Qi,Muhao Chen*

Main category: cs.AI

TL;DR: 提出PolicyGuardBench基准和PolicyGuard - 4B模型，用于检测网络代理轨迹中的策略违规，展示小尺度下准确且可泛化的护栏是可行的。


<details>
  <summary>Details</summary>
Motivation: 现有工作很少研究网络代理长视野轨迹是否符合策略以及策略违规在不同上下文是否持续，为填补此空白开展研究。

Method: 引入包含约60k个示例的PolicyGuardBench基准，生成多种策略并标注违规情况，设置完整轨迹评估和基于前缀的违规检测任务；用该数据集训练轻量级的PolicyGuard - 4B模型。

Result: PolicyGuard - 4B模型在所有任务中检测准确率高，推理高效，能跨领域泛化，在未见设置中保持高精度。

Conclusion: PolicyGuardBench和PolicyGuard - 4B提供了首个研究网络代理轨迹策略合规性的综合框架，证明小尺度下可实现准确且可泛化的护栏。

Abstract: Autonomous web agents need to operate under externally imposed or
human-specified policies while generating long-horizon trajectories. However,
little work has examined whether these trajectories comply with such policies,
or whether policy violations persist across different contexts such as domains
(e.g., shopping or coding websites) and subdomains (e.g., product search and
order management in shopping). To address this gap, we introduce
PolicyGuardBench, a benchmark of about 60k examples for detecting policy
violations in agent trajectories. From diverse agent runs, we generate a broad
set of policies and create both within subdomain and cross subdomain pairings
with violation labels. In addition to full-trajectory evaluation,
PolicyGuardBench also includes a prefix-based violation detection task where
models must anticipate policy violations from truncated trajectory prefixes
rather than complete sequences. Using this dataset, we train PolicyGuard-4B, a
lightweight guardrail model that delivers strong detection accuracy across all
tasks while keeping inference efficient. Notably, PolicyGuard-4B generalizes
across domains and preserves high accuracy on unseen settings. Together,
PolicyGuardBench and PolicyGuard-4B provide the first comprehensive framework
for studying policy compliance in web agent trajectories, and show that
accurate and generalizable guardrails are feasible at small scales.

</details>


### [8] [The Hidden Game Problem](https://arxiv.org/abs/2510.03845)
*Gon Buzaglo,Noah Golowich,Elad Hazan*

Main category: cs.AI

TL;DR: 本文研究大策略空间游戏，提出隐藏游戏问题，开发后悔最小化技术组合解决该问题，实现快速收敛。


<details>
  <summary>Details</summary>
Motivation: 受AI对齐和语言游戏挑战的启发，研究大策略空间游戏。

Method: 开发后悔最小化技术的组合。

Result: 实现了最优的外部和交换后悔界限，确保在隐藏子游戏中快速收敛到相关均衡。

Conclusion: 可以设计高效的后悔最小化算法来发现和利用隐藏结构，达到子游戏的均衡。

Abstract: This paper investigates a class of games with large strategy spaces,
motivated by challenges in AI alignment and language games. We introduce the
hidden game problem, where for each player, an unknown subset of strategies
consistently yields higher rewards compared to the rest. The central question
is whether efficient regret minimization algorithms can be designed to discover
and exploit such hidden structures, leading to equilibrium in these subgames
while maintaining rationality in general. We answer this question affirmatively
by developing a composition of regret minimization techniques that achieve
optimal external and swap regret bounds. Our approach ensures rapid convergence
to correlated equilibria in hidden subgames, leveraging the hidden game
structure for improved computational efficiency.

</details>


### [9] [OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows](https://arxiv.org/abs/2510.03506)
*John Nguyen,Marton Havasi,Tariq Berrada,Luke Zettlemoyer,Ricky T. Q. Chen*

Main category: cs.AI

TL;DR: 介绍OneFlow，首个非自回归多模态模型，能实现可变长度和并发混合模态生成，性能优于基线，训练FLOPs少且有新能力。


<details>
  <summary>Details</summary>
Motivation: 突破自回归模型在文本和图像生成中严格因果顺序的限制，实现更高效的多模态生成。

Method: 将基于插入的Edit Flow用于离散文本标记，将Flow Matching用于图像潜在空间，采用分层采样进行并发文本 - 图像合成。

Result: 在1B到8B模型规模的对照实验中，OneFlow在生成和理解任务上优于自回归基线，训练FLOPs最多减少50%。

Conclusion: OneFlow超越自回归和基于扩散的方法，具备并发生成、迭代细化和类自然推理生成等新能力。

Abstract: We present OneFlow, the first non-autoregressive multimodal model that
enables variable-length and concurrent mixed-modal generation. Unlike
autoregressive models that enforce rigid causal ordering between text and image
generation, OneFlow combines an insertion-based Edit Flow for discrete text
tokens with Flow Matching for image latents. OneFlow enables concurrent
text-image synthesis with hierarchical sampling that prioritizes content over
grammar. Through controlled experiments across model sizes from 1B to 8B, we
demonstrate that OneFlow outperforms autoregressive baselines on both
generation and understanding tasks while using up to 50% fewer training FLOPs.
OneFlow surpasses both autoregressive and diffusion-based approaches while
unlocking new capabilities for concurrent generation, iterative refinement, and
natural reasoning-like generation.

</details>


### [10] [Understanding the Role of Training Data in Test-Time Scaling](https://arxiv.org/abs/2510.03605)
*Adel Javanmard,Baharan Mirzasoleiman,Vahab Mirrokni*

Main category: cs.AI

TL;DR: 研究测试时间缩放对基于上下文权重预测任务训练的变压器在线性回归中的性能，分析得出相关理论结论并实验验证。


<details>
  <summary>Details</summary>
Motivation: 尽管测试时间缩放能提升大语言模型推理能力，但长思维链出现的训练数据条件及何时提升性能尚不清楚。

Method: 研究测试时间缩放对基于上下文权重预测任务训练的变压器在线性回归中的性能，通过理论分析和大的非线性变压器架构实验。

Result: 在固定测试误差下，增加测试时间计算可减少训练提示中的上下文示例数量；若训练数据中解决下游任务的技能不足，增加测试时间计算会损害性能；通过特征协方差矩阵最小特征值表征任务难度，在多样、相关且困难的任务集上训练测试时间缩放性能最佳。

Conclusion: 研究结果通过实验得到证实，为测试时间缩放的性能提供理论解释。

Abstract: Test-time scaling improves the reasoning capabilities of large language
models (LLMs) by allocating extra compute to generate longer Chains-of-Thoughts
(CoTs). This enables models to tackle more complex problem by breaking them
down into additional steps, backtracking, and correcting mistakes. Despite its
strong performance--demonstrated by OpenAI's o1 and DeepSeek R1, the conditions
in the training data under which long CoTs emerge, and when such long CoTs
improve the performance, remain unclear. In this paper, we study the
performance of test-time scaling for transformers trained on an in-context
weight prediction task for linear regression. Our analysis provides a
theoretical explanation for several intriguing observations: First, at any
fixed test error, increasing test-time compute allows us to reduce the number
of in-context examples (context length) in training prompts. Second, if the
skills required to solve a downstream task are not sufficiently present in the
training data, increasing test-time compute can harm performance. Finally, we
characterize task hardness via the smallest eigenvalue of its feature
covariance matrix and show that training on a diverse, relevant, and hard set
of tasks results in best performance for test-time scaling. We confirm our
findings with experiments on large, nonlinear transformer architectures.

</details>


### [11] [Look-ahead Reasoning with a Learned Model in Imperfect Information Games](https://arxiv.org/abs/2510.05048)
*Ondřej Kubíček,Viliam Lisý*

Main category: cs.AI

TL;DR: 本文提出LAMIR算法，能从智能体与环境交互中学习不完美信息博弈的抽象模型，用于测试时前瞻推理，提升预训练智能体游戏表现。


<details>
  <summary>Details</summary>
Motivation: 测试时推理需显式环境模型，在现实场景常不可用或过于复杂；将MuZero范式扩展到不完美信息博弈有挑战。

Method: 引入LAMIR算法，直接从智能体 - 环境交互中学习不完美信息博弈的抽象模型，在测试时用该模型进行前瞻推理。

Result: 有足够容量时，LAMIR能学习到确切的底层游戏结构；容量有限时，也能学习到有价值的抽象，提升预训练智能体在大型游戏中的表现。

Conclusion: LAMIR算法能有效解决不完美信息博弈中测试时推理的问题，提升预训练智能体游戏性能。

Abstract: Test-time reasoning significantly enhances pre-trained AI agents'
performance. However, it requires an explicit environment model, often
unavailable or overly complex in real-world scenarios. While MuZero enables
effective model learning for search in perfect information games, extending
this paradigm to imperfect information games presents substantial challenges
due to more nuanced look-ahead reasoning techniques and large number of states
relevant for individual decisions. This paper introduces an algorithm LAMIR
that learns an abstracted model of an imperfect information game directly from
the agent-environment interaction. During test time, this trained model is used
to perform look-ahead reasoning. The learned abstraction limits the size of
each subgame to a manageable size, making theoretically principled look-ahead
reasoning tractable even in games where previous methods could not scale. We
empirically demonstrate that with sufficient capacity, LAMIR learns the exact
underlying game structure, and with limited capacity, it still learns a
valuable abstraction, which improves game playing performance of the
pre-trained agents even in large games.

</details>


### [12] [Video Game Level Design as a Multi-Agent Reinforcement Learning Problem](https://arxiv.org/abs/2510.04862)
*Sam Earle,Zehua Jiang,Eugene Vinitsky,Julian Togelius*

Main category: cs.AI

TL;DR: 本文提出将关卡生成作为多智能体问题，缓解单智能体PCGRL的效率瓶颈，且多智能体关卡生成器泛化能力更好。


<details>
  <summary>Details</summary>
Motivation: 现有PCGRL研究聚焦单生成器智能体，存在频繁重新计算关卡质量启发式和智能体在大地图中导航的效率瓶颈问题。

Method: 将关卡生成构建为多智能体问题，减少相对于智能体动作数量的奖励计算次数。

Result: 多智能体关卡生成器能缓解单智能体PCGRL的效率瓶颈，且能更好地泛化到分布外的地图形状。

Conclusion: 将内容生成视为分布式多智能体任务，有利于大规模生成功能性产物。

Abstract: Procedural Content Generation via Reinforcement Learning (PCGRL) offers a
method for training controllable level designer agents without the need for
human datasets, using metrics that serve as proxies for level quality as
rewards. Existing PCGRL research focuses on single generator agents, but are
bottlenecked by the need to frequently recalculate heuristics of level quality
and the agent's need to navigate around potentially large maps. By framing
level generation as a multi-agent problem, we mitigate the efficiency
bottleneck of single-agent PCGRL by reducing the number of reward calculations
relative to the number of agent actions. We also find that multi-agent level
generators are better able to generalize to out-of-distribution map shapes,
which we argue is due to the generators' learning more local, modular design
policies. We conclude that treating content generation as a distributed,
multi-agent task is beneficial for generating functional artifacts at scale.

</details>


### [13] [Cross-Modal Content Optimization for Steering Web Agent Preferences](https://arxiv.org/abs/2510.03612)
*Tanqiu Jiang,Min Bai,Nikolaos Pappas,Yanjun Qi,Sandesh Swamy*

Main category: cs.AI

TL;DR: 本文首次证明在现实攻击者能力下联合利用视觉和文本通道可进行更强大的偏好操纵，提出CPS方法，评估显示其比基线方法更有效且隐蔽，强调需强大防御措施。


<details>
  <summary>Details</summary>
Motivation: 现有基于VLM的网页代理在偏好操纵攻击下脆弱，且现有研究存在假设强白盒访问、单模态扰动有限或设置不实际的问题。

Method: 提出Cross - Modal Preference Steering (CPS)方法，联合优化项目视觉和自然语言描述的不易察觉修改，利用CLIP可迁移图像扰动和RLHF引起的语言偏差，采用现实的黑盒威胁设置。

Result: 在电影选择和电商任务上评估，CPS比领先基线方法更有效，在所有模型上始终优于基线，检测率低70%。

Conclusion: 随着智能体系统在社会中作用日益重要，迫切需要强大的防御措施。

Abstract: Vision-language model (VLM)-based web agents increasingly power high-stakes
selection tasks like content recommendation or product ranking by combining
multimodal perception with preference reasoning. Recent studies reveal that
these agents are vulnerable against attackers who can bias selection outcomes
through preference manipulations using adversarial pop-ups, image
perturbations, or content tweaks. Existing work, however, either assumes strong
white-box access, with limited single-modal perturbations, or uses impractical
settings. In this paper, we demonstrate, for the first time, that joint
exploitation of visual and textual channels yields significantly more powerful
preference manipulations under realistic attacker capabilities. We introduce
Cross-Modal Preference Steering (CPS) that jointly optimizes imperceptible
modifications to an item's visual and natural language descriptions, exploiting
CLIP-transferable image perturbations and RLHF-induced linguistic biases to
steer agent decisions. In contrast to prior studies that assume gradient
access, or control over webpages, or agent memory, we adopt a realistic
black-box threat setup: a non-privileged adversary can edit only their own
listing's images and textual metadata, with no insight into the agent's model
internals. We evaluate CPS on agents powered by state-of-the-art proprietary
and open source VLMs including GPT-4.1, Qwen-2.5VL and Pixtral-Large on both
movie selection and e-commerce tasks. Our results show that CPS is
significantly more effective than leading baseline methods. For instance, our
results show that CPS consistently outperforms baselines across all models
while maintaining 70% lower detection rates, demonstrating both effectiveness
and stealth. These findings highlight an urgent need for robust defenses as
agentic systems play an increasingly consequential role in society.

</details>


### [14] [MITS: Enhanced Tree Search Reasoning for LLMs via Pointwise Mutual Information](https://arxiv.org/abs/2510.03632)
*Jiaxi Li,Yucheng Shi,Jin Lu,Ninghao Liu*

Main category: cs.AI

TL;DR: 提出Mutual Information Tree Search (MITS)框架用于大语言模型推理，实验显示其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有树搜索方法难对中间推理步骤质量进行即时可靠定量评估，且路径探索计算成本高。

Method: 引入基于点互信息的有效评分函数，结合基于熵的动态采样策略和加权投票方案。

Result: 在多种推理基准测试中，MITS始终超越基线方法。

Conclusion: MITS为大语言模型推理建立了一个有原则且高效的框架。

Abstract: Tree search has become as a representative framework for test-time reasoning
with large language models (LLMs), exemplified by methods such as
Tree-of-Thought and Monte Carlo Tree Search that explore multiple reasoning
paths. However, it remains difficult to provide instant and reliable
quantitative assessments of intermediate reasoning step quality, and extensive
path exploration is computationally costly. To address this, we propose Mutual
Information Tree Search (MITS), a novel framework that guides reasoning with
information-theoretic principles. MITS introduces an effective scoring function
based on pointwise mutual information (PMI), which enables step-wise evaluation
of reasoning paths and search tree expansion via beam search without expensive
look-ahead simulations, achieving superior reasoning performances while
maintaining computational efficiency. The framework is complemented by an
entropy-based dynamic sampling strategy that adaptively allocates computational
resources to uncertain reasoning steps where exploration is most beneficial.
For final prediction, MITS employs a weighted voting scheme that combines PMI
scores with prediction consensus. Through comprehensive experiments on diverse
reasoning benchmarks, MITS consistently surpasses baseline methods,
establishing a principled and efficient framework for LLM reasoning.

</details>


### [15] [CAG: Chunked Augmented Generation for Google Chrome's Built-in Gemini Nano](https://arxiv.org/abs/2412.18708)
*Vivek Vellaiyappan Surulimuthu,Aditya Karnam Gururaj Rao*

Main category: cs.AI

TL;DR: 提出Chunked Augmented Generation (CAG)架构，克服Chrome内置Gemini Nano模型上下文窗口限制，可在浏览器内高效处理大量内容。


<details>
  <summary>Details</summary>
Motivation: Chrome集成的Gemini Nano模型上下文窗口受限，处理大输入存在挑战，需解决该问题。

Method: 采用智能输入分块和处理策略。

Result: 在Chrome内处理大文档和数据集效果良好，无需外部API依赖即可让浏览器具备复杂AI能力。

Conclusion: CAG架构有效解决了Gemini Nano模型上下文窗口限制问题，具有实际应用价值。

Abstract: We present Chunked Augmented Generation (CAG), an architecture specifically
designed to overcome the context window limitations of Google Chrome's built-in
Gemini Nano model. While Chrome's integration of Gemini Nano represents a
significant advancement in bringing AI capabilities directly to the browser,
its restricted context window poses challenges for processing large inputs. CAG
addresses this limitation through intelligent input chunking and processing
strategies, enabling efficient handling of extensive content while maintaining
the model's performance within browser constraints. Our implementation
demonstrates particular efficacy in processing large documents and datasets
directly within Chrome, making sophisticated AI capabilities accessible through
the browser without external API dependencies. Get started now at
https://github.com/vivekVells/cag-js.

</details>


### [16] [Rainbow Padding: Mitigating Early Termination in Instruction-Tuned Diffusion LLMs](https://arxiv.org/abs/2510.03680)
*Bumjun Kim,Dongjae Jeon,Dueun Kim,Wonje Jeung,Albert No*

Main category: cs.AI

TL;DR: 研究扩散大语言模型（dLLMs）的<eos>溢出问题，提出Rainbow Padding方法解决，实验证明有效且实用。


<details>
  <summary>Details</summary>
Motivation: 指令调优的dLLMs存在<eos>溢出问题，即序列长度增加时响应变短，该问题未被系统分析。

Method: 引入Rainbow Padding方法，用不同填充标记循环替代重复的<eos>占位符。

Result: Rainbow Padding显著提高长度鲁棒性和输出质量，少量填充标记即可防止提前终止，单轮LoRA微调有显著改进。

Conclusion: Rainbow Padding方法能有效解决<eos>溢出问题，且可高效集成到现有模型，实用价值高。

Abstract: Diffusion large language models (dLLMs) have emerged as a promising
alternative to autoregressive models, offering flexible generation orders and
strong performance on complex reasoning tasks. However, instruction-tuned dLLMs
exhibit a critical vulnerability we term \texttt{<eos>} overflow: as allocated
sequence length increases, responses paradoxically become shorter, collapsing
into early termination or degenerating into streams of \texttt{<eos>} tokens.
Although noticed in practice, this issue has not been systematically analyzed.
We trace its root cause to the dual role of \texttt{<eos>} as both termination
and padding, which concentrates probability mass on \texttt{<eos>} at later
positions and propagates backward to trigger early termination. To address
this, we introduce Rainbow Padding, a simple remedy that replaces repeated
\texttt{<eos>} placeholders with a repeating cycle of distinct padding tokens,
distributing probability mass and breaking \texttt{<eos>} dominance.
Experiments show that Rainbow Padding substantially improves length robustness
and output quality, with as few as seven padding tokens sufficient to prevent
early termination. Moreover, the method integrates efficiently into existing
instruction-tuned models: LoRA fine-tuning for a single epoch on minimal data
yields significant improvements, making this solution highly practical. The
code is publicly available at https://github.com/quasar529/rainbow-padding.

</details>


### [17] [Mind the Goal: Data-Efficient Goal-Oriented Evaluation of Conversational Agents and Chatbots using Teacher Models](https://arxiv.org/abs/2510.03696)
*Deepak Babu Piskala,Sharlene Chen,Udita Patel,Parul Kalra,Rafael Castrillo*

Main category: cs.AI

TL;DR: 提出面向目标的多智能体系统评估框架，引入GSR和RCOF，应用于AIDA使GSR从63%提升到79%，框架通用且可助力系统改进。


<details>
  <summary>Details</summary>
Motivation: 现有多轮聊天机器人交互质量评估方法多在轮次层面，未考虑用户总体目标是否达成。

Method: 按用户目标分割对话，用相关轮次评估成功；结合教师大语言模型构建评估系统，专家定义目标和标准引导大语言模型。

Result: 在企业场景应用框架评估AIDA，其GSR在六个月内从63%提升到79%。

Conclusion: 框架具有通用性，通过详细缺陷分类法提供可操作见解，有助于诊断整体成功率、识别关键失败模式和改进系统。

Abstract: Evaluating the quality of multi-turn chatbot interactions remains
challenging, as most existing methods assess interactions at the turn level
without addressing whether a user's overarching goal was fulfilled. A ``goal''
here refers to an information need or task, such as asking for policy
information or applying for leave. We propose a comprehensive framework for
goal-oriented evaluation of multi-agent systems (MAS), introducing the
\textbf{Goal Success Rate (GSR)} to measure the percentage of fulfilled goals,
and a \textbf{Root Cause of Failure (RCOF)} taxonomy to identify reasons for
failure in multi-agent chatbots. Our method segments conversations by user
goals and evaluates success using all relevant turns. We present a model-based
evaluation system combining teacher LLMs, where domain experts define goals,
set quality standards serving as a guidance for the LLMs. The LLMs use
``thinking tokens'' to produce interpretable rationales, enabling
\textit{explainable}, \textit{data-efficient} evaluations. In an enterprise
setting, we apply our framework to evaluate AIDA, a zero-to-one employee
conversational agent system built as a ground-up multi-agent conversational
agent, and observe GSR improvement from 63\% to 79\% over six months since its
inception. Our framework is generic and offers actionable insights through a
detailed defect taxonomy based on analysis of failure points in multi-agent
chatbots, diagnosing overall success, identifying key failure modes, and
informing system improvements.

</details>


### [18] [H-DDx: A Hierarchical Evaluation Framework for Differential Diagnosis](https://arxiv.org/abs/2510.03700)
*Seungseop Lim,Gibaeg Kim,Hyunkyung Lee,Wooseok Han,Jean Seo,Jaehyo Yoo,Eunho Yang*

Main category: cs.AI

TL;DR: 提出H - DDx分层评估框架用于评估大语言模型生成鉴别诊断列表的性能，发现传统评估指标低估模型表现，凸显领域专业开源模型优势。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在鉴别诊断领域的评估主要依赖扁平指标，无法区分临床相关的接近错误和诊断上差距大的错误，需更好反映临床相关性的评估框架。

Method: 引入H - DDx分层评估框架，利用检索和重排序管道将自由文本诊断映射到ICD - 10代码，并应用分层指标。

Result: 传统扁平指标因忽略临床有意义的输出而低估模型性能，领域专业开源模型有优势；框架能揭示分层错误模式。

Conclusion: H - DDx框架能更好评估大语言模型在鉴别诊断中的性能，增强可解释性。

Abstract: An accurate differential diagnosis (DDx) is essential for patient care,
shaping therapeutic decisions and influencing outcomes. Recently, Large
Language Models (LLMs) have emerged as promising tools to support this process
by generating a DDx list from patient narratives. However, existing evaluations
of LLMs in this domain primarily rely on flat metrics, such as Top-k accuracy,
which fail to distinguish between clinically relevant near-misses and
diagnostically distant errors. To mitigate this limitation, we introduce H-DDx,
a hierarchical evaluation framework that better reflects clinical relevance.
H-DDx leverages a retrieval and reranking pipeline to map free-text diagnoses
to ICD-10 codes and applies a hierarchical metric that credits predictions
closely related to the ground-truth diagnosis. In benchmarking 22 leading
models, we show that conventional flat metrics underestimate performance by
overlooking clinically meaningful outputs, with our results highlighting the
strengths of domain-specialized open-source models. Furthermore, our framework
enhances interpretability by revealing hierarchical error patterns,
demonstrating that LLMs often correctly identify the broader clinical context
even when the precise diagnosis is missed.

</details>


### [19] [ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in Complex Chart Question Answering](https://arxiv.org/abs/2510.04514)
*Rachneet Kaur,Nishan Srishankar,Zhen Zeng,Sumitra Ganesh,Manuela Veloso*

Main category: cs.AI

TL;DR: 提出ChartAgent框架用于图表视觉问答，在基准测试中取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在无注释图表的视觉问答任务中表现不佳，需要精确的视觉推理。

Method: 引入ChartAgent框架，将查询迭代分解为视觉子任务，通过特定动作与图表图像交互，利用图表特定视觉工具完成子任务。

Result: 在ChartBench和ChartX基准测试中达到SOTA，在无注释、数值密集查询上提升显著，在不同图表类型和复杂度上均有效。

Conclusion: ChartAgent是首个使用工具增强多模态代理进行图表理解的视觉推理框架，可提升不同基础大语言模型的性能。

Abstract: Recent multimodal LLMs have shown promise in chart-based visual question
answering, but their performance declines sharply on unannotated charts, those
requiring precise visual interpretation rather than relying on textual
shortcuts. To address this, we introduce ChartAgent, a novel agentic framework
that explicitly performs visual reasoning directly within the chart's spatial
domain. Unlike textual chain-of-thought reasoning, ChartAgent iteratively
decomposes queries into visual subtasks and actively manipulates and interacts
with chart images through specialized actions such as drawing annotations,
cropping regions (e.g., segmenting pie slices, isolating bars), and localizing
axes, using a library of chart-specific vision tools to fulfill each subtask.
This iterative reasoning process closely mirrors human cognitive strategies for
chart comprehension. ChartAgent achieves state-of-the-art accuracy on the
ChartBench and ChartX benchmarks, surpassing prior methods by up to 16.07%
absolute gain overall and 17.31% on unannotated, numerically intensive queries.
Furthermore, our analyses show that ChartAgent is (a) effective across diverse
chart types, (b) achieve the highest scores across varying visual and reasoning
complexity levels, and (c) serves as a plug-and-play framework that boosts
performance across diverse underlying LLMs. Our work is among the first to
demonstrate visually grounded reasoning for chart understanding using
tool-augmented multimodal agents.

</details>


### [20] [Bridging the Gap Between Multimodal Foundation Models and World Models](https://arxiv.org/abs/2510.03727)
*Xuehai He*

Main category: cs.AI

TL;DR: 本文探讨如何缩小多模态基础模型与世界模型的差距，通过提升推理和生成能力来改进多模态基础模型。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态基础模型不能有效作为世界模型，缺乏反事实推理、模拟动力学等关键能力。

Method: 通过判别任务提升多模态基础模型的推理能力，赋予其结构化推理技能；探索跨图像和视频模态的生成能力，引入结构化和可控生成的新框架，并将技术扩展到可控4D生成。

Result: 未明确提及具体结果，但展示了提升多模态基础模型能力的方法。

Conclusion: 通过改进推理和生成能力，可缩小多模态基础模型与世界模型的差距。

Abstract: Humans understand the world through the integration of multiple sensory
modalities, enabling them to perceive, reason about, and imagine dynamic
physical processes. Inspired by this capability, multimodal foundation models
(MFMs) have emerged as powerful tools for multimodal understanding and
generation. However, today's MFMs fall short of serving as effective world
models. They lack the essential ability such as perform counterfactual
reasoning, simulate dynamics, understand the spatiotemporal information,
control generated visual outcomes, and perform multifaceted reasoning. We
investigates what it takes to bridge the gap between multimodal foundation
models and world models. We begin by improving the reasoning capabilities of
MFMs through discriminative tasks and equipping MFMs with structured reasoning
skills, such as causal inference, counterfactual thinking, and spatiotemporal
reasoning, enabling them to go beyond surface correlations and understand
deeper relationships within visual and textual data. Next, we explore
generative capabilities of multimodal foundation models across both image and
video modalities, introducing new frameworks for structured and controllable
generation. Our approaches incorporate scene graphs, multimodal conditioning,
and multimodal alignment strategies to guide the generation process, ensuring
consistency with high-level semantics and fine-grained user intent. We further
extend these techniques to controllable 4D generation, enabling interactive,
editable, and morphable object synthesis over time and space.

</details>


### [21] [OptAgent: Optimizing Query Rewriting for E-commerce via Multi-Agent Simulation](https://arxiv.org/abs/2510.03771)
*Divij Handa,David Blincoe,Orson Adams,Yinlin Fu*

Main category: cs.AI

TL;DR: 提出OptAgent框架用于电商查询重写，结合多智能体模拟与遗传算法，在真实数据集上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 可靠评估对部署基于大语言模型的系统很重要，电商查询重写这类主观任务缺乏单一正确答案，传统方法难判断重写查询是否符合用户意图。

Method: 引入OptAgent框架，结合多智能体模拟与遗传算法，用多个基于大语言模型的智能体作为动态奖励信号，其得分均值作为进化算法的适应度函数迭代优化查询。

Result: 在1000个真实电商查询数据集上，相比原始用户查询平均提升21.98%，相比Best - of - N大语言模型重写基线提升3.36%。

Conclusion: OptAgent框架在电商查询重写任务中有效，能显著优化查询。

Abstract: Deploying capable and user-aligned LLM-based systems necessitates reliable
evaluation. While LLMs excel in verifiable tasks like coding and mathematics,
where gold-standard solutions are available, adoption remains challenging for
subjective tasks that lack a single correct answer. E-commerce Query Rewriting
(QR) is one such problem where determining whether a rewritten query properly
captures the user intent is extremely difficult to figure out algorithmically.
In this work, we introduce OptAgent, a novel framework that combines
multi-agent simulations with genetic algorithms to verify and optimize queries
for QR. Instead of relying on a static reward model or a single LLM judge, our
approach uses multiple LLM-based agents, each acting as a simulated shopping
customer, as a dynamic reward signal. The average of these agent-derived scores
serves as an effective fitness function for an evolutionary algorithm that
iteratively refines the user's initial query. We evaluate OptAgent on a dataset
of 1000 real-world e-commerce queries in five different categories, and we
observe an average improvement of 21.98% over the original user query and 3.36%
over a Best-of-N LLM rewriting baseline.

</details>


### [22] [GuidedSampling: Steering LLMs Towards Diverse Candidate Solutions at Inference-Time](https://arxiv.org/abs/2510.03777)
*Divij Handa,Mihir Parmar,Aswin RRV,Md Nayem Uddin,Hamid Palangi,Chitta Baral*

Main category: cs.AI

TL;DR: 提出新推理算法GuidedSampling，能增加生成候选解的多样性，在多个基准测试中提升模型性能。


<details>
  <summary>Details</summary>
Motivation: Repeated Sampling算法在生成多样解候选方面存在局限，常产生冗余样本，需改进。

Method: 提出GuidedSampling算法，将推理时的探索和生成阶段解耦，先探索可用于解决问题的概念，再用特定概念生成最终候选解，并定义其理论边界。

Result: 相比RS，GuidedSampling在pass@50上平均提升约21.6%；基于其轨迹训练的模型在pass@5上平均提升约9.7%；增加了每个实例的平均概念数量（从1.67到3.03），生成的候选解更具多样性。

Conclusion: GuidedSampling算法有效解决了RS算法的局限性，能提升模型性能并增加候选解的多样性。

Abstract: Repeated Sampling (RS) is a simple inference-time algorithm that has been
shown to improve model performance on complex tasks. Although it is an
effective way of scaling inference time, it often struggles to generate diverse
solution candidates, frequently relying on the same underlying approach to
solve the problem and thus producing redundant samples. To address this
limitation, we propose a new inference algorithm, GuidedSampling, which
decouples the exploration and generation phases during inference, increasing
diversity of generated candidate solutions. The exploration phase identifies
multiple concepts that can be utilized to solve the problem, while the
generation phase applies a specific concept to provide final solution
candidates. We first define the theoretical bounds of GuidedSampling and then
empirically demonstrate that it improves the performance of base model at
pass@50 by on an average ~21.6% across various benchmarks compared to RS.
Furthermore, models trained on trajectories of GuidedSampling exhibit
substantial performance improvements at pass@5 by on an average ~9.7%, compared
to models trained on traditional RS. Additionally, models trained with
GuidedSampling increases the average number of concepts per instance (1.67 ->
3.03), yielding a diverse set of candidates than traditional RS.

</details>


### [23] [Small Language Models for Agentic Systems: A Survey of Architectures, Capabilities, and Deployment Trade offs](https://arxiv.org/abs/2510.03847)
*Raghav Sharma,Manan Mehta*

Main category: cs.AI

TL;DR: 小语言模型（SLMs）在特定任务中表现出色，本文综合证据、评估和服务栈，提出SLM - 缺省、LLM - 后备系统及工程指标，给出优先使用SLMs的设计模式，为构建高效代理提供蓝图。


<details>
  <summary>Details</summary>
Motivation: 探讨小语言模型在代理工作负载中的应用，解决在受限任务中实现高效准确的问题。

Method: 综合开放和专有SLMs的证据，结合现代评估和服务栈，形式化SLM - 缺省、LLM - 后备系统，提出工程指标，给出设计模式。

Result: 引导解码等方法缩小了与大模型的能力差距，SLMs在工具使用等方面表现出色，成本更低、延迟和能耗更优。

Conclusion: 为构建快速、廉价、可靠的代理提供了实用蓝图，默认使用SLMs，必要时借助LLM。

Abstract: Small language models (SLMs; 1-12B params, sometimes up to 20B) are
sufficient and often superior for agentic workloads where the objective is
schema- and API-constrained accuracy rather than open-ended generation. We
synthesize recent evidence across open and proprietary SLMs (Phi-4-Mini,
Qwen-2.5-7B, Gemma-2-9B, Llama-3.2-1B/3B, Ministral-3B/8B, Apple on-device 3B,
DeepSeek-R1-Distill) and connect it to modern evaluations (BFCL v3/v4,
StableToolBench) and serving stacks (vLLM, SGLang, TensorRT-LLM) paired with
guided decoding libraries (XGrammar, Outlines). We formalize SLM-default,
LLM-fallback systems with uncertainty-aware routing and verifier cascades, and
propose engineering metrics that reflect real production goals: cost per
successful task (CPS), schema validity rate, executable call rate, p50/p95
latency, and energy per request. Guided decoding, strict JSON Schema outputs,
and validator-first tool execution close much of the capability gap with larger
models and often let SLMs match or surpass LLMs on tool use, function calling,
and RAG at 10x-100x lower token cost with materially better latency and energy.
We provide design patterns for agent stacks that prioritize SLMs: schema-first
prompting, type-safe function registries, confidence scoring with verifier
rollups, and lightweight adaptation via LoRA/QLoRA. We also delineate limits
where fallback remains valuable (open-domain reasoning and some long-horizon
planning). The result is a practical blueprint for building fast, inexpensive,
and reliable agents that default to SLMs while preserving headroom with
targeted LLM assistance.
  Keywords: small language models, agents, function calling, structured
outputs, JSON Schema, guided decoding, LoRA/QLoRA, routing, energy efficiency,
edge inference

</details>


### [24] [Algorithm Generation via Creative Ideation](https://arxiv.org/abs/2510.03851)
*Ruiying Ma,Chieh-Jan Mike Liang,Yanjie Gao,Francis Y. Yan*

Main category: cs.AI

TL;DR: 研究LLMs驱动算法生成的局限性，提出MetaMuse框架解决问题，评估显示其对缓存替换和在线装箱问题有高性能表现。


<details>
  <summary>Details</summary>
Motivation: 解决系统算法设计中因解空间不连续，工程师依赖通用启发式方法牺牲性能的问题，且LLMs在算法生成中有局限性。

Method: 引入基于三个自我反思原则的MetaMuse框架，包括在可衡量性能空间量化、通过外部刺激引导构思、用航点推理构建可执行解决方案。

Result: MetaMuse能为全球云提供商的缓存替换和在线装箱两个关键问题生成高性能解决方案，分别减少缓存未命中达35.76%、减少装箱使用达30.93%。

Conclusion: MetaMuse框架可有效应对系统算法设计挑战，在特定问题上有良好性能。

Abstract: Designing system algorithms remains challenging, where the discontinuous
nature of the solution space often forces system engineers to rely on generic
heuristics at the expense of performance. We study whether LLMs can practically
drive algorithm generation, and find that they are biased towards well-known
generic designs, rather than making the creative leaps needed to navigate the
discontinuous solution space. To address this limitation, we introduce
MetaMuse, a framework for creative ideation built on three self-reflection
principles: (1) quantifying solution diversity and usefulness in measurable
performance space, rather than abstract idea space, (2) steering ideation
through external stimuli, rather than internal randomness, and (3) constructing
executable solutions using waypoint reasoning, rather than free-form
chain-of-thought. Extensive evaluation shows that MetaMuse can generate
high-performing solutions for two critical problems at a global cloud provider:
cache replacement (reducing cache misses by up to 35.76%) and online bin
packing (reducing bin usage by up to 30.93%).

</details>


### [25] [Adaptive and Explainable AI Agents for Anomaly Detection in Critical IoT Infrastructure using LLM-Enhanced Contextual Reasoning](https://arxiv.org/abs/2510.03859)
*Raghav Sharma,Manan Mehta*

Main category: cs.AI

TL;DR: 文章提出用LLM支持的上下文推理方法与XAI代理结合改进关键物联网环境中的异常检测，测试表明新方法在准确性和可解释性上优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有物联网系统异常检测方法在复杂系统中存在不足，需要自适应、智能的系统。

Method: 使用LLM支持的上下文推理方法和XAI代理，采用注意力机制、避免处理每个时间步细节并使用有意义的内存缓冲区。

Result: 在测试中，新方法在检测准确性、误报率、结果可读性和系统响应速度等方面表现良好，在模拟的智能电网和医疗场景中适应性和可靠性强。

Conclusion: 新方法在准确性和可解释性上远优于现有模型，适合未来物联网异常检测任务。

Abstract: Ensuring that critical IoT systems function safely and smoothly depends a lot
on finding anomalies quickly. As more complex systems, like smart healthcare,
energy grids and industrial automation, appear, it is easier to see the
shortcomings of older methods of detection. Monitoring failures usually happen
in dynamic, high dimensional situations, especially when data is incomplete,
messy or always evolving. Such limits point out the requirement for adaptive,
intelligent systems that always improve and think. LLMs are now capable of
significantly changing how context is understood and semantic inference is done
across all types of data. This proposal suggests using an LLM supported
contextual reasoning method along with XAI agents to improve how anomalies are
found in significant IoT environments. To discover hidden patterns and notice
inconsistencies in data streams, it uses attention methods, avoids dealing with
details from every time step and uses memory buffers with meaning. Because no
code AI stresses transparency and interpretability, people can check and accept
the AI's decisions, helping ensure AI follows company policies. The two
architectures are put together in a test that compares the results of the
traditional model with those of the suggested LLM enhanced model. Important
measures to check are the accuracy of detection, how much inaccurate
information is included in the results, how clearly the findings can be read
and how fast the system responds under different test situations. The
metaheuristic is tested in simulations of real world smart grid and healthcare
contexts to check its adaptability and reliability. From the study, we see that
the new approach performs much better than most existing models in both
accuracy and interpretation, so it could be a good fit for future anomaly
detection tasks in IoT

</details>


### [26] [Spatial CAPTCHA: Generatively Benchmarking Spatial Reasoning for Human-Machine Differentiation](https://arxiv.org/abs/2510.03863)
*Arina Kharlamova,Bowei He,Chen Ma,Xue Liu*

Main category: cs.AI

TL;DR: 提出Spatial CAPTCHA应对MLLMs对传统验证码的挑战，其生成需空间推理的动态问题，评估显示人类表现远超MLLMs，且比Google reCAPTCHA有效。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型削弱了传统验证码基于文本识别或二维图像理解设计的有效性，需新的人类验证框架。

Method: 提出Spatial CAPTCHA，利用人类和MLLMs空间推理差异，采用基于约束难度控制的程序生成管道、自动正确性验证和人工验证。

Result: 在Spatial - CAPTCHA - Bench基准上，人类表现远超10个最先进的MLLMs，最佳模型Pass@1准确率仅31.0%，与Google reCAPTCHA对比也显示其有效性。

Conclusion: Spatial CAPTCHA可作为有效的安全机制和AI空间推理诊断工具。

Abstract: Online services rely on CAPTCHAs as a first line of defense against automated
abuse, yet recent advances in multi-modal large language models (MLLMs) have
eroded the effectiveness of conventional designs that focus on text recognition
or 2D image understanding. To address this challenge, we present Spatial
CAPTCHA, a novel human-verification framework that leverages fundamental
differences in spatial reasoning between humans and MLLMs. Unlike existing
CAPTCHAs which rely on low-level perception tasks that are vulnerable to modern
AI, Spatial CAPTCHA generates dynamic questions requiring geometric reasoning,
perspective-taking, occlusion handling, and mental rotation. These skills are
intuitive for humans but difficult for state-of-the-art (SOTA) AI systems. The
system employs a procedural generation pipeline with constraint-based
difficulty control, automated correctness verification, and human-in-the-loop
validation to ensure scalability, robustness, and adaptability. Evaluation on a
corresponding benchmark, Spatial-CAPTCHA-Bench, demonstrates that humans vastly
outperform 10 state-of-the-art MLLMs, with the best model achieving only 31.0%
Pass@1 accuracy. Furthermore, we compare Spatial CAPTCHA with Google reCAPTCHA,
which confirms its effectiveness as both a security mechanism and a diagnostic
tool for spatial reasoning in AI.

</details>


### [27] [Speculative Actions: A Lossless Framework for Faster Agentic Systems](https://arxiv.org/abs/2510.04371)
*Naimeng Ye,Arnav Ahuja,Georgios Liargkovas,Yunan Lu,Kostis Kaffes,Tianyi Peng*

Main category: cs.AI

TL;DR: 提出投机动作框架解决AI智能体执行慢问题，在多环境评估有显著效果。


<details>
  <summary>Details</summary>
Motivation: 当前AI智能体在环境中执行慢，阻碍训练、评估和部署，关键瓶颈是行为顺序展开，API调用耗时。

Method: 受微处理器投机执行和大语言模型推理中投机解码启发，提出投机动作框架，用更快模型预测可能动作，实现多步骤并行执行。

Result: 在游戏、电商、网页搜索等三个环境及操作系统环境的“有损”扩展中评估，在下一步动作预测上有高达55%的准确率，显著降低端到端延迟。

Conclusion: 该框架为现实世界部署低延迟智能体系统提供了有前景的途径，且可通过更强猜测模型等方法进一步提升性能。

Abstract: Despite growing interest in AI agents across industry and academia, their
execution in an environment is often slow, hampering training, evaluation, and
deployment. For example, a game of chess between two state-of-the-art agents
may take hours. A critical bottleneck is that agent behavior unfolds
sequentially: each action requires an API call, and these calls can be
time-consuming. Inspired by speculative execution in microprocessors and
speculative decoding in LLM inference, we propose speculative actions, a
lossless framework for general agentic systems that predicts likely actions
using faster models, enabling multiple steps to be executed in parallel. We
evaluate this framework across three agentic environments: gaming, e-commerce,
web search, and a "lossy" extension for an operating systems environment. In
all cases, speculative actions achieve substantial accuracy in next-action
prediction (up to 55%), translating into significant reductions in end-to-end
latency. Moreover, performance can be further improved through stronger
guessing models, top-K action prediction, multi-step speculation, and
uncertainty-aware optimization, opening a promising path toward deploying
low-latency agentic systems in the real world.

</details>


### [28] [Rare Text Semantics Were Always There in Your Diffusion Transformer](https://arxiv.org/abs/2510.03886)
*Seil Kang,Woojung Han,Dayun Ju,Seong Jae Hwang*

Main category: cs.AI

TL;DR: 本文提出无需额外训练步骤、数据等干预方法，通过扩大文本嵌入表示范围使MM - DiTs输出中出现罕见语义，且结果在多任务中有效。


<details>
  <summary>Details</summary>
Motivation: 现有先进文本到视觉生成模型难以处理用户富有想象力或罕见的提示，因其概念在预训练中稀缺。

Method: 在MM - DiT的联合注意力块之前，通过扩大方差来扩展文本标记嵌入的表示范围。

Result: 该方法能使MM - DiT输出中清晰出现罕见语义，且在文本到图像、文本到视频和文本驱动图像编辑等任务中有效。

Conclusion: 该工作有助于生成模型揭示用户想要表达的语义。

Abstract: Starting from flow- and diffusion-based transformers, Multi-modal Diffusion
Transformers (MM-DiTs) have reshaped text-to-vision generation, gaining acclaim
for exceptional visual fidelity. As these models advance, users continually
push the boundary with imaginative or rare prompts, which advanced models still
falter in generating, since their concepts are often too scarce to leave a
strong imprint during pre-training. In this paper, we propose a simple yet
effective intervention that surfaces rare semantics inside MM-DiTs without
additional training steps, data, denoising-time optimization, or reliance on
external modules (e.g., large language models). In particular, the
joint-attention mechanism intrinsic to MM-DiT sequentially updates text
embeddings alongside image embeddings throughout transformer blocks. We find
that by mathematically expanding representational basins around text token
embeddings via variance scale-up before the joint-attention blocks, rare
semantics clearly emerge in MM-DiT's outputs. Furthermore, our results
generalize effectively across text-to-vision tasks, including text-to-image,
text-to-video, and text-driven image editing. Our work invites generative
models to reveal the semantics that users intend, once hidden yet ready to
surface.

</details>


### [29] [Kantian-Utilitarian XAI: Meta-Explained](https://arxiv.org/abs/2510.03892)
*Zahra Atf,Peter R. Lewis*

Main category: cs.AI

TL;DR: 提出咖啡领域用于道德消费决策的游戏化可解释AI系统，含两轮引擎和元解释器，还发布相关配置和UI。


<details>
  <summary>Details</summary>
Motivation: 为咖啡领域有道德意识的消费者决策提供支持。

Method: 每个会话有六轮，每轮三个选项；用康德模块标记规则违规，功利模块对选项多标准打分；元解释器突出两种模块的（不）一致性并在福利损失小时切换选项。

Result: 发布了结构化配置、可审计的策略追踪和交互式UI。

Conclusion: 该游戏化可解释AI系统能助力咖啡领域消费者进行道德决策。

Abstract: We present a gamified explainable AI (XAI) system for ethically aware
consumer decision-making in the coffee domain. Each session comprises six
rounds with three options per round. Two symbolic engines provide real-time
reasons: a Kantian module flags rule violations (e.g., child labor,
deforestation risk without shade certification, opaque supply chains, unsafe
decaf), and a utilitarian module scores options via multi-criteria aggregation
over normalized attributes (price, carbon, water, transparency, farmer income
share, taste/freshness, packaging, convenience). A meta-explainer with a regret
bound (0.2) highlights Kantian--utilitarian (mis)alignment and switches to a
deontically clean, near-parity option when welfare loss is small. We release a
structured configuration (attribute schema, certification map, weights, rule
set), a policy trace for auditability, and an interactive UI.

</details>


### [30] [Quantifying Risks in Multi-turn Conversation with Large Language Models](https://arxiv.org/abs/2510.03969)
*Chengxiao Wang,Isha Chaudhary,Qian Hu,Weitong Ruan,Rahul Gupta,Gagandeep Singh*

Main category: cs.AI

TL;DR: 提出QRLLM框架评估大语言模型多轮对话灾难性风险，发现前沿模型存在高风险。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法无法充分揭示大语言模型在对话场景中的灾难性风险漏洞。

Method: 提出QRLLM框架，将多轮对话建模为查询序列上的概率分布，用马尔可夫过程和置信区间量化风险，定义多种分布。

Result: 发现前沿模型存在大量灾难性风险，最差模型的认证下界高达70%。

Conclusion: 前沿大语言模型急需改进安全训练策略。

Abstract: Large Language Models (LLMs) can produce catastrophic responses in
conversational settings that pose serious risks to public safety and security.
Existing evaluations often fail to fully reveal these vulnerabilities because
they rely on fixed attack prompt sequences, lack statistical guarantees, and do
not scale to the vast space of multi-turn conversations. In this work, we
propose QRLLM, a novel, principled Certification framework for Catastrophic
risks in multi-turn Conversation for LLMs that bounds the probability of an LLM
generating catastrophic responses under multi-turn conversation distributions
with statistical guarantees. We model multi-turn conversations as probability
distributions over query sequences, represented by a Markov process on a query
graph whose edges encode semantic similarity to capture realistic
conversational flow, and quantify catastrophic risks using confidence
intervals. We define several inexpensive and practical distributions: random
node, graph path, adaptive with rejection. Our results demonstrate that these
distributions can reveal substantial catastrophic risks in frontier models,
with certified lower bounds as high as 70\% for the worst model, highlighting
the urgent need for improved safety training strategies in frontier LLMs.

</details>


### [31] [Safe and Compliant Cross-Market Trade Execution via Constrained RL and Zero-Knowledge Audits](https://arxiv.org/abs/2510.04952)
*Ailiya Borjigin,Cong He*

Main category: cs.AI

TL;DR: 提出跨市场算法交易系统，平衡执行质量与合规性，评估显示策略有效并讨论相关问题。


<details>
  <summary>Details</summary>
Motivation: 构建能平衡交易执行质量和严格合规执行的跨市场算法交易系统。

Method: 将交易执行建模为受限马尔可夫决策过程，用近端策略优化训练执行代理，添加零知识合规审计层，在多平台模拟器中评估并与标准基线对比。

Result: 学习到的策略减少了执行缺口和方差，在压力场景下无约束违规，用配对t检验报告效果，通过CVaR检查尾部风险。

Conclusion: 该工作位于多个领域交叉点，讨论了伦理考量、局限性和实际部署途径。

Abstract: We present a cross-market algorithmic trading system that balances execution
quality with rigorous compliance enforcement. The architecture comprises a
high-level planner, a reinforcement learning execution agent, and an
independent compliance agent. We formulate trade execution as a constrained
Markov decision process with hard constraints on participation limits, price
bands, and self-trading avoidance. The execution agent is trained with proximal
policy optimization, while a runtime action-shield projects any unsafe action
into a feasible set. To support auditability without exposing proprietary
signals, we add a zero-knowledge compliance audit layer that produces
cryptographic proofs that all actions satisfied the constraints. We evaluate in
a multi-venue, ABIDES-based simulator and compare against standard baselines
(e.g., TWAP, VWAP). The learned policy reduces implementation shortfall and
variance while exhibiting no observed constraint violations across stress
scenarios including elevated latency, partial fills, compliance module
toggling, and varying constraint limits. We report effects at the 95%
confidence level using paired t-tests and examine tail risk via CVaR. We
situate the work at the intersection of optimal execution, safe reinforcement
learning, regulatory technology, and verifiable AI, and discuss ethical
considerations, limitations (e.g., modeling assumptions and computational
overhead), and paths to real-world deployment.

</details>


### [32] [What Shapes a Creative Machine Mind? Comprehensively Benchmarking Creativity in Foundation Models](https://arxiv.org/abs/2510.04009)
*Zicong He,Boxuan Zhang,Weihao Liu,Ruixiang Tang,Lu Cheng*

Main category: cs.AI

TL;DR: 提出C^2 - Eval基准评估大模型创造力，通过实验分析模型能力，证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有大模型创造力评估框架零散，缺乏基于理论的指标。

Method: 引入C^2 - Eval基准，区分收敛和发散创造力，用社会科学理论的细粒度U - O - S标准评估。

Result: 通过实验分析了领先模型的创造力权衡，指出优势与挑战。

Conclusion: C^2 - Eval是审视创意AI发展格局的有效工具。

Abstract: The meteoric rise of foundation models (FMs) has expanded their capabilities
far beyond conventional tasks. Creativity, long regarded as a hallmark of human
intelligence and a driver of innovation, is now increasingly recognized as a
critical dimension of machine intelligence in the era of generative FMs,
complementing traditional measures of accuracy. However, existing evaluation
frameworks for creativity remain fragmented, relying on ad hoc metrics not
firmly grounded in established theories. To address this gap, we introduce
C^2-Eval, a holistic benchmark for unified assessment of creativity in FMs.
C^2-Eval distinguishes between two complementary forms of creativity:
convergent creativity, where tasks admit constrained solutions (e.g., code
generation), and divergent creativity, where tasks are open-ended (e.g.,
storytelling). It evaluates both dimensions using fine-grained criteria derived
from social-science theory, focusing on Usefulness, Originality, and Surprise
(U-O-S). Through extensive experiments on leading proprietary and open-source
models, we analyze trade-offs in their creative capabilities. Our results
highlight both the strengths and challenges of current FMs in pursuing a
creative machine mind, showing that C^2-Eval is an effective lens for examining
the evolving landscape of creative AI.

</details>


### [33] [Zephyrus: An Agentic Framework for Weather Science](https://arxiv.org/abs/2510.04017)
*Sumanth Varambally,Marshall Fisher,Jas Thakker,Yiwei Chen,Zhirui Xia,Yasaman Jafari,Ruijia Niu,Manas Jain,Veeramakali Vignesh Manivannan,Zachary Novack,Luyu Han,Srikar Eranky,Salva Rühling Cachay,Taylor Berg-Kirkpatrick,Duncan Watson-Parris,Yi-An Ma,Rose Yu*

Main category: cs.AI

TL;DR: 提出天气科学新型代理框架，含ZephyrusWorld环境、Zephyrus代理和ZephyrusBench基准，实验显示Zephyrus在基准上表现强，但难题与文本基线相近。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型缺语言推理能力，大语言模型难处理气象数据集，需搭建框架弥补。

Method: 构建含ZephyrusWorld环境的新型代理框架，设计多轮LLM天气代理Zephyrus，创建新基准ZephyrusBench。

Result: Zephyrus在基准实验中表现优于文本基线，最高正确率高35个百分点，但难题表现与基线相近。

Conclusion: Zephyrus有良好表现，但基准难题有挑战，为未来工作指明方向。

Abstract: Foundation models for weather science are pre-trained on vast amounts of
structured numerical data and outperform traditional weather forecasting
systems. However, these models lack language-based reasoning capabilities,
limiting their utility in interactive scientific workflows. Large language
models (LLMs) excel at understanding and generating text but cannot reason
about high-dimensional meteorological datasets. We bridge this gap by building
a novel agentic framework for weather science. Our framework includes a Python
code-based environment for agents (ZephyrusWorld) to interact with weather
data, featuring tools like an interface to WeatherBench 2 dataset, geoquerying
for geographical masks from natural language, weather forecasting, and climate
simulation capabilities. We design Zephyrus, a multi-turn LLM-based weather
agent that iteratively analyzes weather datasets, observes results, and refines
its approach through conversational feedback loops. We accompany the agent with
a new benchmark, ZephyrusBench, with a scalable data generation pipeline that
constructs diverse question-answer pairs across weather-related tasks, from
basic lookups to advanced forecasting, extreme event detection, and
counterfactual reasoning. Experiments on this benchmark demonstrate the strong
performance of Zephyrus agents over text-only baselines, outperforming them by
up to 35 percentage points in correctness. However, on harder tasks, Zephyrus
performs similarly to text-only baselines, highlighting the challenging nature
of our benchmark and suggesting promising directions for future work.

</details>


### [34] [LLM-Based Data Science Agents: A Survey of Capabilities, Challenges, and Future Directions](https://arxiv.org/abs/2510.04023)
*Mizanur Rahman,Amran Bhuiyan,Mohammed Saidul Islam,Md Tahmid Rahman Laskar,Ridwan Mahbub,Ahmed Masry,Shafiq Joty,Enamul Hoque*

Main category: cs.AI

TL;DR: 本文对数据科学智能体进行全面调查，提出生命周期对齐的分类法，分析其能力、优缺点，指出关键趋势并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型发展，出现能自动化数据科学工作流多阶段的AI智能体，需对其进行全面研究和分类。

Method: 提出生命周期对齐的分类法，将45个系统映射到数据科学流程的6个阶段，从5个设计维度注释每个智能体。

Result: 识别出三个关键趋势，如多数系统侧重部分阶段、多模态推理和工具编排有挑战、超90%缺乏信任和安全机制。

Conclusion: 指出对齐稳定性、可解释性等方面的开放挑战，提出未来研究方向以开发更优的数据科学智能体。

Abstract: Recent advances in large language models (LLMs) have enabled a new class of
AI agents that automate multiple stages of the data science workflow by
integrating planning, tool use, and multimodal reasoning across text, code,
tables, and visuals. This survey presents the first comprehensive,
lifecycle-aligned taxonomy of data science agents, systematically analyzing and
mapping forty-five systems onto the six stages of the end-to-end data science
process: business understanding and data acquisition, exploratory analysis and
visualization, feature engineering, model building and selection,
interpretation and explanation, and deployment and monitoring. In addition to
lifecycle coverage, we annotate each agent along five cross-cutting design
dimensions: reasoning and planning style, modality integration, tool
orchestration depth, learning and alignment methods, and trust, safety, and
governance mechanisms. Beyond classification, we provide a critical synthesis
of agent capabilities, highlight strengths and limitations at each stage, and
review emerging benchmarks and evaluation practices. Our analysis identifies
three key trends: most systems emphasize exploratory analysis, visualization,
and modeling while neglecting business understanding, deployment, and
monitoring; multimodal reasoning and tool orchestration remain unresolved
challenges; and over 90% lack explicit trust and safety mechanisms. We conclude
by outlining open challenges in alignment stability, explainability,
governance, and robust evaluation frameworks, and propose future research
directions to guide the development of robust, trustworthy, low-latency,
transparent, and broadly accessible data science agents.

</details>


### [35] [A global log for medical AI](https://arxiv.org/abs/2510.04033)
*Ayush Noori,Adam Rodman,Alan Karthikesalingam,Bilal A. Mateen,Christopher A. Longhurst,Daniel Yang,Dave deBronkart,Gauden Galea,Harold F. Wolf III,Jacob Waxman,Joshua C. Mandel,Juliana Rotich,Kenneth D. Mandl,Maryam Mustafa,Melissa Miles,Nigam H. Shah,Peter Lee,Robert Korom,Scott Mahoney,Seth Hain,Tien Yin Wong,Trevor Mundel,Vivek Natarajan,Noa Dagan,David A. Clifton,Ran D. Balicer,Isaac S. Kohane,Marinka Zitnik*

Main category: cs.AI

TL;DR: 文章提出类似syslog的临床AI日志协议MedLog，介绍其记录内容、特性及作用。


<details>
  <summary>Details</summary>
Motivation: 医疗临床AI缺乏记录模型使用情况的标准方式，难以衡量性能、检测不良事件等。

Method: 引入MedLog协议，规定记录AI模型活动的九个核心字段，支持风险抽样、生命周期保留策略和写后缓存等。

Result: MedLog可推动新数据库和软件发展，用于存储和分析记录。

Conclusion: MedLog能实现医疗AI的持续监测、审计和迭代改进，为数字流行病学奠定基础。

Abstract: Modern computer systems often rely on syslog, a simple, universal protocol
that records every critical event across heterogeneous infrastructure. However,
healthcare's rapidly growing clinical AI stack has no equivalent. As hospitals
rush to pilot large language models and other AI-based clinical decision
support tools, we still lack a standard way to record how, when, by whom, and
for whom these AI models are used. Without that transparency and visibility, it
is challenging to measure real-world performance and outcomes, detect adverse
events, or correct bias or dataset drift. In the spirit of syslog, we introduce
MedLog, a protocol for event-level logging of clinical AI. Any time an AI model
is invoked to interact with a human, interface with another algorithm, or act
independently, a MedLog record is created. This record consists of nine core
fields: header, model, user, target, inputs, artifacts, outputs, outcomes, and
feedback, providing a structured and consistent record of model activity. To
encourage early adoption, especially in low-resource settings, and minimize the
data footprint, MedLog supports risk-based sampling, lifecycle-aware retention
policies, and write-behind caching; detailed traces for complex, agentic, or
multi-stage workflows can also be captured under MedLog. MedLog can catalyze
the development of new databases and software to store and analyze MedLog
records. Realizing this vision would enable continuous surveillance, auditing,
and iterative improvement of medical AI, laying the foundation for a new form
of digital epidemiology.

</details>


### [36] [FaithCoT-Bench: Benchmarking Instance-Level Faithfulness of Chain-of-Thought Reasoning](https://arxiv.org/abs/2510.04040)
*Xu Shen,Song Wang,Zhen Tan,Laura Yao,Xinyu Zhao,Kaidi Xu,Xin Wang,Tianlong Chen*

Main category: cs.AI

TL;DR: 本文引入FaithCoT - Bench基准用于检测思维链（CoT）不忠实性，对11种检测方法评估，为大语言模型更可解释和可信推理研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有研究未解决判断特定轨迹是否忠实于模型内部推理的实际挑战，本文旨在填补该空白。

Method: 引入FaithCoT - Bench基准，将不忠实性检测作为判别决策问题，提供专家标注的FINE - CoT数据集，对11种检测方法进行系统评估。

Result: 得出了现有方法优缺点的实证见解，揭示了知识密集型领域和更先进模型检测的挑战。

Conclusion: FaithCoT - Bench建立了首个实例级CoT忠实性综合基准，为未来研究奠定基础。

Abstract: Large language models (LLMs) increasingly rely on Chain-of-Thought (CoT)
prompting to improve problem-solving and provide seemingly transparent
explanations. However, growing evidence shows that CoT often fail to faithfully
represent the underlying reasoning process, raising concerns about their
reliability in high-risk applications. Although prior studies have focused on
mechanism-level analyses showing that CoTs can be unfaithful, they leave open
the practical challenge of deciding whether a specific trajectory is faithful
to the internal reasoning of the model. To address this gap, we introduce
FaithCoT-Bench, a unified benchmark for instance-level CoT unfaithfulness
detection. Our framework establishes a rigorous task formulation that
formulates unfaithfulness detection as a discriminative decision problem, and
provides FINE-CoT (Faithfulness instance evaluation for Chain-of-Thought), an
expert-annotated collection of over 1,000 trajectories generated by four
representative LLMs across four domains, including more than 300 unfaithful
instances with fine-grained causes and step-level evidence. We further conduct
a systematic evaluation of eleven representative detection methods spanning
counterfactual, logit-based, and LLM-as-judge paradigms, deriving empirical
insights that clarify the strengths and weaknesses of existing approaches and
reveal the increased challenges of detection in knowledge-intensive domains and
with more advanced models. To the best of our knowledge, FaithCoT-Bench
establishes the first comprehensive benchmark for instance-level CoT
faithfulness, setting a solid basis for future research toward more
interpretable and trustworthy reasoning in LLMs.

</details>


### [37] [Increasing LLM response trustworthiness using voting ensembles](https://arxiv.org/abs/2510.04048)
*Aparna Nair-Kanneganti,Trevor J. Chan,Shir Goldfinger,Emily Mackay,Brian Anthony,Alison Pouch*

Main category: cs.AI

TL;DR: 论文指出大语言模型缺乏量化不确定性的方法，提出可变投票阈值的集成方法，理论和实验表明该方法能提高答案可信度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在高风险应用中缺乏量化不确定性的便捷可靠方法，难以被信任。

Method: 研究具有可变投票阈值的集成方法，引入问答理论框架，允许集成在主导响应未达阈值时‘弃权’。

Result: 在算术问题解决和临床笔记问答两个领域，高度严格的投票集成能大幅提高答案可信度，同时响应率和准确性降幅相对较小。

Conclusion: 投票集成在需要高度确定性但不要求每个问题都有自动答案的应用中特别有用。

Abstract: Despite huge advances, LLMs still lack convenient and reliable methods to
quantify the uncertainty in their responses, making them difficult to trust in
high-stakes applications. One of the simplest approaches to eliciting more
accurate answers is to select the mode of many responses, a technique known as
ensembling. In this work, we expand on typical ensembling approaches by looking
at ensembles with a variable voting threshold. We introduce a theoretical
framework for question answering and show that, by permitting ensembles to
"abstain" from providing an answer when the dominant response falls short of
the threshold, it is possible to dramatically increase the trustworthiness of
the remaining answers. From this framework, we derive theoretical results as
well as report experimental results on two problem domains: arithmetic problem
solving and clinical-note question-answering. In both domains, we observe that
large gains in answer trustworthiness can be achieved using highly restrictive
voting ensembles, while incurring relatively modest reductions in response
yield and accuracy. Due to this quality, voting ensembles may be particularly
useful in applications - such as healthcare and data annotation - that require
a high degree of certainty but which may not require that every question
receive an automated answer.

</details>


### [38] [Toward a unified framework for data-efficient evaluation of large language models](https://arxiv.org/abs/2510.04051)
*Lele Liao,Qile Zhang,Ruofan Wu,Guanhua Fang*

Main category: cs.AI

TL;DR: 现有大语言模型评估方法成本高且有局限，本文提出LEGO - IRT框架，支持多种评估指标，利用结构知识，实验表明用少量评估项就能稳定估计能力，还能降低误差并可能更符合人类偏好。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型综合基准评估成本高，基于IRT的评估方法有局限，如只能处理二元指标、忽略结构知识等，需要改进评估方法。

Method: 引入LEGO - IRT框架，该框架原生支持二元和连续评估指标，采用因式分解架构来建模和利用结构知识，将模型能力估计分解为通用组件和特定结构组件。

Result: 通过对70个大语言模型在5个基准上的实验，LEGO - IRT用3%的评估项就能实现稳定的能力估计，纳入结构知识可降低估计误差达10%，估计的潜在能力可能更符合人类偏好。

Conclusion: LEGO - IRT是一个统一灵活的框架，能实现数据高效的大语言模型评估，利用结构知识可提高评估效果。

Abstract: Evaluating large language models (LLMs) on comprehensive benchmarks is a
cornerstone of their development, yet it's often computationally and
financially prohibitive. While Item Response Theory (IRT) offers a promising
path toward data-efficient evaluation by disentangling model capability from
item difficulty, existing IRT-based methods are hampered by significant
limitations. They are typically restricted to binary correctness metrics,
failing to natively handle the continuous scores used in generative tasks, and
they operate on single benchmarks, ignoring valuable structural knowledge like
correlations across different metrics or benchmarks. To overcome these
challenges, we introduce LEGO-IRT, a unified and flexible framework for
data-efficient LLM evaluation. LEGO-IRT's novel design natively supports both
binary and continuous evaluation metrics. Moreover, it introduces a factorized
architecture to explicitly model and leverage structural knowledge, decomposing
model ability estimates into a general component and structure-specific (e.g.,
per-metric or per-benchmark) components. Through extensive experiments
involving $70$ LLMs across $5$ benchmarks, we show that LEGO-IRT achieves
stable capability estimates using just $3\%$ of the total evaluation items. We
demonstrate that incorporating structural knowledge reduces estimation error by
up to $10\%$ and reveal that the latent abilities estimated by our framework
may align more closely with human preferences.

</details>


### [39] [Decoding Emotion in the Deep: A Systematic Study of How LLMs Represent, Retain, and Express Emotion](https://arxiv.org/abs/2510.04064)
*Jingxiang Zhang,Lujia Zhong*

Main category: cs.AI

TL;DR: 本文研究现代大语言模型（LLMs）中的潜在情感表征，引入新数据集，用轻量级“探针”探测，发现LLMs有明确情感内部几何结构，情感信号早现且在网络中层达到峰值，状态可塑且持久，开源代码和数据集。


<details>
  <summary>Details</summary>
Motivation: 大语言模型需处理人类情感细微差别，但内部情感机制未充分探索，旨在研究情感在模型神经架构中的编码方式、位置和时长。

Method: 引入约40万个话语的Reddit语料库，对七种基本情绪进行多阶段分类、改写和合成生成；用轻量级“探针”从Qwen3和LLaMA模型隐藏层读取信息，不改变参数。

Result: LLMs有明确的情感内部几何结构，随模型规模增强，优于零样本提示；情感信号非最后一层现象，早期出现并在网络中层达到峰值；内部状态可塑且持久。

Conclusion: 开源数据集、探测工具包和情感地图，为开发更透明和一致的AI系统提供关键见解。

Abstract: Large Language Models (LLMs) are increasingly expected to navigate the
nuances of human emotion. While research confirms that LLMs can simulate
emotional intelligence, their internal emotional mechanisms remain largely
unexplored. This paper investigates the latent emotional representations within
modern LLMs by asking: how, where, and for how long is emotion encoded in their
neural architecture? To address this, we introduce a novel, large-scale Reddit
corpus of approximately 400,000 utterances, balanced across seven basic
emotions through a multi-stage process of classification, rewriting, and
synthetic generation. Using this dataset, we employ lightweight "probes" to
read out information from the hidden layers of various Qwen3 and LLaMA models
without altering their parameters. Our findings reveal that LLMs develop a
surprisingly well-defined internal geometry of emotion, which sharpens with
model scale and significantly outperforms zero-shot prompting. We demonstrate
that this emotional signal is not a final-layer phenomenon but emerges early
and peaks mid-network. Furthermore, the internal states are both malleable
(they can be influenced by simple system prompts) and persistent, as the
initial emotional tone remains detectable for hundreds of subsequent tokens. We
contribute our dataset, an open-source probing toolkit, and a detailed map of
the emotional landscape within LLMs, offering crucial insights for developing
more transparent and aligned AI systems. The code and dataset are open-sourced.

</details>


### [40] [Moral Anchor System: A Predictive Framework for AI Value Alignment and Drift Prevention](https://arxiv.org/abs/2510.04073)
*Santhosh Kumar Ravindran*

Main category: cs.AI

TL;DR: 本文提出道德锚定系统（MAS）检测、预测和缓解AI价值漂移，实验验证其有效性和创新性。


<details>
  <summary>Details</summary>
Motivation: AI集成引发价值一致性问题，价值漂移会导致效率低下或伦理违规，需解决该问题。

Method: 提出MAS框架，结合实时贝叶斯推理、LSTM网络和以人为本的治理层，通过监督微调减少误报。

Result: 实验验证MAS可扩展性和响应性，模拟中能将价值漂移事件减少80%以上，保持高检测准确率和低误报率。

Conclusion: MAS具有预测和自适应特性，对比静态对齐方法有创新性，有架构、实验结果、跨领域适用性见解和开源代码等贡献。

Abstract: The rise of artificial intelligence (AI) as super-capable assistants has
transformed productivity and decision-making across domains. Yet, this
integration raises critical concerns about value alignment - ensuring AI
behaviors remain consistent with human ethics and intentions. A key risk is
value drift, where AI systems deviate from aligned values due to evolving
contexts, learning dynamics, or unintended optimizations, potentially leading
to inefficiencies or ethical breaches. We propose the Moral Anchor System
(MAS), a novel framework to detect, predict, and mitigate value drift in AI
agents. MAS combines real-time Bayesian inference for monitoring value states,
LSTM networks for forecasting drift, and a human-centric governance layer for
adaptive interventions. It emphasizes low-latency responses (<20 ms) to prevent
breaches, while reducing false positives and alert fatigue via supervised
fine-tuning with human feedback. Our hypothesis: integrating probabilistic
drift detection, predictive analytics, and adaptive governance can reduce value
drift incidents by 80 percent or more in simulations, maintaining high
detection accuracy (85 percent) and low false positive rates (0.08
post-adaptation). Rigorous experiments with goal-misaligned agents validate
MAS's scalability and responsiveness. MAS's originality lies in its predictive
and adaptive nature, contrasting static alignment methods. Contributions
include: (1) MAS architecture for AI integration; (2) empirical results
prioritizing speed and usability; (3) cross-domain applicability insights; and
(4) open-source code for replication.

</details>


### [41] [SPOGW: a Score-based Preference Optimization method via Group-Wise comparison for workflows](https://arxiv.org/abs/2510.04089)
*Yitong Cui,Liu Liu,Baosheng Yu,Jiayan Qiu,Xikai Zhang,Likang Xiao,Yixing Liu,Quan Chen*

Main category: cs.AI

TL;DR: 提出新的基于分数的偏好方法SPOGW优化智能体工作流，在五个基准数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有设计智能体工作流方法需大量人工，且当前自动化技术有代表性不足、适应性差等局限。

Method: 引入SPOGW方法，直接处理基数奖励信号并进行组间比较，结合ioGRPO和mKL调节训练更新。

Result: 在五个涵盖数学推理、编码和问答的基准数据集上，SPOGW达到或超越当前最先进方法的性能。

Conclusion: SPOGW为智能体工作流的自动生成和优化提供了可行且有前景的方法。

Abstract: Large language models (LLMs) have exhibited significant capabilities in
addressing challenging problems throughout various fields, often through the
use of agentic workflows that adhere to structured instructions and multi-step
procedures. However, designing such workflows demands substantial manual
effort, posing challenges to scalability and generalizability. Recent studies
have aimed to minimize the human intervention needed for their construction,
leading to advances in automated techniques for optimizing agentic workflows.
However, current approaches are often constrained by their limited
representational capacity, insufficient adaptability, weak scalability, and
pairwise comparison paradigm -- issues that stem primarily from a dependence on
discrete optimization techniques. To overcome these limitations, we introduce a
new score-based preference approach, refereed as SPOGW, which operates directly
on cardinal reward signals through group-wise comparison and enables more
efficient and stable optimization in a continuous space. SPOGW incorporates
Iterative offline GRPO (ioGRPO) with advantage-masked KL divergence (mKL),
which regulates training update by placing greater emphasis on the advantageous
regions of the policy response. In five benchmark datasets covering
mathematical reasoning, coding, and question answering, SPOGW matches or
exceeds the performance of current state-of-the-art approaches, presenting a
viable and forward-looking methodology for automated generation and
optimization of agentic workflows.

</details>


### [42] [Harnessing LLM for Noise-Robust Cognitive Diagnosis in Web-Based Intelligent Education Systems](https://arxiv.org/abs/2510.04093)
*Guixian Zhang,Guan Yuan,Ziqi Xu,Yanmei Zhang,Zhenyun Deng,Debo Cheng*

Main category: cs.AI

TL;DR: 提出用于抗噪声认知诊断的DLLM框架，在三个数据集上实验表明其有最优预测性能，能实现抗噪声并有效利用大语言模型语义知识。


<details>
  <summary>Details</summary>
Motivation: 现有用大语言模型进行认知诊断的方法难以处理结构化数据且易受噪声影响，网络智能教育系统存在数据不平衡和噪声问题。

Method: 构建基于作答正确性的独立子图，用关系增强对齐模块缓解数据不平衡，在每次对齐前用两阶段去噪扩散模块消除噪声，最后将融合语义知识和结构信息的抗噪声表示输入现有认知诊断模型预测。

Result: 在三个公开网络教育平台数据集上实验，DLLM在不同噪声水平下都有最优预测性能。

Conclusion: DLLM能实现抗噪声，同时有效利用大语言模型的语义知识。

Abstract: Cognitive diagnostics in the Web-based Intelligent Education System (WIES)
aims to assess students' mastery of knowledge concepts from heterogeneous,
noisy interactions. Recent work has tried to utilize Large Language Models
(LLMs) for cognitive diagnosis, yet LLMs struggle with structured data and are
prone to noise-induced misjudgments. Specially, WIES's open environment
continuously attracts new students and produces vast amounts of response logs,
exacerbating the data imbalance and noise issues inherent in traditional
educational systems. To address these challenges, we propose DLLM, a
Diffusion-based LLM framework for noise-robust cognitive diagnosis. DLLM first
constructs independent subgraphs based on response correctness, then applies
relation augmentation alignment module to mitigate data imbalance. The two
subgraph representations are then fused and aligned with LLM-derived,
semantically augmented representations. Importantly, before each alignment
step, DLLM employs a two-stage denoising diffusion module to eliminate
intrinsic noise while assisting structural representation alignment.
Specifically, unconditional denoising diffusion first removes erroneous
information, followed by conditional denoising diffusion based on graph-guided
to eliminate misleading information. Finally, the noise-robust representation
that integrates semantic knowledge and structural information is fed into
existing cognitive diagnosis models for prediction. Experimental results on
three publicly available web-based educational platform datasets demonstrate
that our DLLM achieves optimal predictive performance across varying noise
levels, which demonstrates that DLLM achieves noise robustness while
effectively leveraging semantic knowledge from LLM.

</details>


### [43] [WebRenderBench: Enhancing Web Interface Generation through Layout-Style Consistency and Reinforcement Learning](https://arxiv.org/abs/2510.04097)
*Peichao Lai,Jinhui Zhuang,Kexuan Zhang,Ningchang Xiong,Shengjie Wang,Yanwei Xu,Chong Chen,Yilei Wang,Bin Cui*

Main category: cs.AI

TL;DR: 提出WebRenderBench基准、新评估指标和ALISA代理，提升WebUI到代码转换性能。


<details>
  <summary>Details</summary>
Motivation: 现有WebUI到代码转换基准数据多样性和评估可靠性有限。

Method: 构建WebRenderBench基准，提出新评估指标，将其集成到强化学习中形成ALISA代理。

Result: ALISA显著提升生成性能，多指标达最优。

Conclusion: WebRenderBench、新评估指标和ALISA能有效解决现有基准的问题，提升WebUI到代码转换效果。

Abstract: Automating the conversion of UI images into web code is a critical task for
front-end development and rapid prototyping. Advances in multimodal large
language models (MLLMs) have made WebUI-to-Code increasingly feasible, yet
existing benchmarks remain limited in data diversity and evaluation
reliability. To address these issues, we present WebRenderBench, a large-scale
benchmark of 22.5k webpages collected from real-world portal sites, offering
greater diversity, complexity, and realism than prior benchmarks. We further
propose a novel evaluation metric that measures layout and style consistency
from the final rendered pages. Unlike vision-based methods that rely on costly
LLM reasoning or structure-based comparisons vulnerable to noise and asymmetry,
our approach enables more efficient, objective, and reliable UI quality
assessment. Finally, we introduce the Automated Layout and Style Inspection
Agent (ALISA), which integrates this metric into reinforcement learning as a
reward signal to enhance training on crawled asymmetric webpages. Experiments
show that ALISA significantly boosts generation performance, achieving
state-of-the-art results across multiple metrics.

</details>


### [44] [Searching Meta Reasoning Skeleton to Guide LLM Reasoning](https://arxiv.org/abs/2510.04116)
*Ziying Zhang,Yaqing Wang,Quanming Yao*

Main category: cs.AI

TL;DR: 本文提出AutoMR框架，用DAG表示元推理骨架，自动搜索查询感知的元推理骨架，实验表明其推理性能优于以往工作。


<details>
  <summary>Details</summary>
Motivation: 以往研究用手动设计结构实现元推理骨架，难以适应查询特定需求和捕捉推理步骤间复杂逻辑依赖。

Method: 用有向无环图（DAG）表示元推理骨架，提出AutoMR框架，构建搜索空间并制定搜索问题，设计动态骨架采样算法。

Result: 在大量基准数据集上实验，AutoMR推理性能普遍优于以往工作。

Conclusion: AutoMR框架有效，能提升大语言模型推理性能。

Abstract: Meta reasoning behaviors work as a skeleton to guide large language model
(LLM) reasoning, thus help to improve reasoning performance. However, prior
researches implement meta reasoning skeleton with manually designed structure,
limiting ability to adapt to query-specific requirement and capture intricate
logical dependency among reasoning steps. To deal with the challenges, we
represent meta reasoning skeleton with directed acyclic graph (DAG) to unify
skeletons proposed in prior works and model intricate logical dependency. Then
we propose AutoMR, a framework that searches for query-aware meta reasoning
skeleton automatically inspired by automated machine learning (AutoML).
Specifically, we construct search space based on DAG representation of skeleton
and then formulate the search problem. We design a dynamic skeleton sampling
algorithm by expanding meta reasoning skeleton along with reasoning context at
inference time. This algorithm can derive any meta reasoning skeleton in search
space efficiently and adapt skeleton to evolving base reasoning context, thus
enable efficient query-aware skeleton search. We conduct experiments on
extensive benchmark datasets. Experimental results show that AutoMR achieves
better reasoning performance than previous works broadly.

</details>


### [45] [Internal states before wait modulate reasoning patterns](https://arxiv.org/abs/2510.04128)
*Dmitrii Troitskii,Koyena Pal,Chris Wendler,Callum Stuart McDougall,Neel Nanda*

Main category: cs.AI

TL;DR: 研究推理模型中等待标记前的潜在信息是否影响后续推理，训练跨编码器，定位相关特征并实验验证其对推理过程的相关性。


<details>
  <summary>Details</summary>
Motivation: 先前工作表明推理和自我修正能力是推理模型性能的重要驱动因素，但对模型为何以特定方式推理了解甚少，限制对模型有效性的理解。

Method: 在DeepSeek - R1 - Distill - Llama - 8B及其基础版本的多层训练跨编码器，引入潜在归因技术定位与等待标记概率相关的特征，通过分析最大激活示例和因果干预实验。

Result: 定位到一组与促进/抑制等待标记概率相关的特征，许多特征与推理过程相关，能产生不同推理模式。

Conclusion: 模型等待标记前的潜在信息包含与后续推理过程相关的信息。

Abstract: Prior work has shown that a significant driver of performance in reasoning
models is their ability to reason and self-correct. A distinctive marker in
these reasoning traces is the token wait, which often signals reasoning
behavior such as backtracking. Despite being such a complex behavior, little is
understood of exactly why models do or do not decide to reason in this
particular manner, which limits our understanding of what makes a reasoning
model so effective. In this work, we address the question whether model's
latents preceding wait tokens contain relevant information for modulating the
subsequent reasoning process. We train crosscoders at multiple layers of
DeepSeek-R1-Distill-Llama-8B and its base version, and introduce a latent
attribution technique in the crosscoder setting. We locate a small set of
features relevant for promoting/suppressing wait tokens' probabilities.
Finally, through a targeted series of experiments analyzing max activating
examples and causal interventions, we show that many of our identified features
indeed are relevant for the reasoning process and give rise to different types
of reasoning patterns such as restarting from the beginning, recalling prior
knowledge, expressing uncertainty, and double-checking.

</details>


### [46] [Selective Expert Guidance for Effective and Diverse Exploration in Reinforcement Learning of LLMs](https://arxiv.org/abs/2510.04140)
*Zishang Jiang,Jinyi Han,Tingyun Li,Xinyi Wang,Sihang Jiang,Jiaqing Liang,Zhaoqian Dai,Shuguang Ma,Fei Yu,Yanghua Xiao*

Main category: cs.AI

TL;DR: 提出MENTOR框架解决RLVR依赖基础模型能力的问题，实验显示其能实现高质量探索并取得优异性能，代码开源。


<details>
  <summary>Details</summary>
Motivation: RLVR有效性依赖基础模型能力，现有方法模仿专家轨迹提升有效性但忽视多样性。

Method: 提出MENTOR框架，仅在关键决策点提供专家指导，进行有效且多样的探索。

Result: MENTOR使模型抓住专家策略本质而非表面模仿，实现高质量探索。

Conclusion: MENTOR框架在RLVR中能进行有效多样探索，取得优异整体性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become a widely
adopted technique for enhancing the reasoning ability of Large Language Models
(LLMs). However, the effectiveness of RLVR strongly depends on the capability
of base models. This issue arises because it requires the model to have
sufficient capability to perform high-quality exploration, which involves both
effectiveness and diversity. Unfortunately, existing methods address this issue
by imitating expert trajectories, which improve effectiveness but neglect
diversity. To address this, we argue that the expert only needs to provide
guidance only at critical decision points rather than the entire reasoning
path. Based on this insight, we propose MENTOR: Mixed-policy Expert Navigation
for Token-level Optimization of Reasoning, a framework that provides expert
guidance only at critical decision points to perform effective and diverse
exploration in RLVR. Extensive experiments show that MENTOR enables models
capture the essence of expert strategies rather than surface imitation, thereby
performing high-quality exploration and achieving superior overall performance.
Our code is available online.

</details>


### [47] [The Artificial Intelligence Cognitive Examination: A Survey on the Evolution of Multimodal Evaluation from Recognition to Reasoning](https://arxiv.org/abs/2510.04141)
*Mayank Ravishankara,Varindra V. Persad Maharaj*

Main category: cs.AI

TL;DR: 本文综述多模态AI评估的演变，从简单识别到复杂推理基准，探讨前沿评估及未知领域，认为AI评估是持续对抗过程。


<details>
  <summary>Details</summary>
Motivation: 因旧基准饱和且高表现掩盖根本弱点，推动多模态AI评估范式转变。

Method: 梳理从ImageNet时代基础测试到当前前沿专家级集成基准的发展历程。

Result: 呈现多模态AI评估从简单到复杂的演变，介绍不同阶段基准。

Conclusion: AI评估不仅是数据集历史，而是持续对抗过程，能重新定义创建智能系统的目标。

Abstract: This survey paper chronicles the evolution of evaluation in multimodal
artificial intelligence (AI), framing it as a progression of increasingly
sophisticated "cognitive examinations." We argue that the field is undergoing a
paradigm shift, moving from simple recognition tasks that test "what" a model
sees, to complex reasoning benchmarks that probe "why" and "how" it
understands. This evolution is driven by the saturation of older benchmarks,
where high performance often masks fundamental weaknesses. We chart the journey
from the foundational "knowledge tests" of the ImageNet era to the "applied
logic and comprehension" exams such as GQA and Visual Commonsense Reasoning
(VCR), which were designed specifically to diagnose systemic flaws such as
shortcut learning and failures in compositional generalization. We then survey
the current frontier of "expert-level integration" benchmarks (e.g., MMBench,
SEED-Bench, MMMU) designed for today's powerful multimodal large language
models (MLLMs), which increasingly evaluate the reasoning process itself.
Finally, we explore the uncharted territories of evaluating abstract, creative,
and social intelligence. We conclude that the narrative of AI evaluation is not
merely a history of datasets, but a continuous, adversarial process of
designing better examinations that, in turn, redefine our goals for creating
truly intelligent systems.

</details>


### [48] [Open Agent Specification (Agent Spec) Technical Report](https://arxiv.org/abs/2510.04173)
*Yassine Benajiba,Cesare Bernardis,Vladislav Blinov,Paul Cayet,Hassan Chafi,Abderrahim Fathan,Louis Faucon,Damien Hilloulin,Sungpack Hong,Ingo Kossyk,Rhicheek Patra,Sujith Ravi,Jonas Schweizer,Jyotika Singh,Shailender Singh,Xuelin Situ,Weiyi Sun,Jerry Xu,Ying Xu*

Main category: cs.AI

TL;DR: Open Agent Specification (Agent Spec) 是一种声明式语言，可跨不同AI框架定义AI代理及其工作流，促进可移植性和互操作性，报告介绍其技术基础、动机、好处和未来发展。


<details>
  <summary>Details</summary>
Motivation: 解决碎片化代理开发挑战，提供统一规范，提高互操作性和可重用性，减少重复开发。

Method: 未提及

Result: Agent Spec 使四类群体受益，包括代理开发者、框架和工具开发者、研究人员和企业。

Conclusion: 报告对 Agent Spec 的技术基础、动机、好处和未来发展进行了概述。

Abstract: Open Agent Specification (Agent Spec) is a declarative language that allows
AI agents and their workflows to be defined in a way that is compatible across
different AI frameworks, promoting portability and interoperability within AI
Agent frameworks.
  Agent Spec aims to resolve the challenges of fragmented agent development by
providing a common unified specification that allows AI agents to be designed
once and deployed across various frameworks, improving interoperability and
reusability, and reducing redundant development efforts. Additionally, Agent
Spec facilitates development tools and portability, allowing AI agents to be
defined independently of their execution environment and enabling teams to
exchange solutions without implementation-specific limitations.
  Agent Spec benefits four key groups: (i) Agent developers, who gain access to
a superset of reusable components and design patterns, enabling them to
leverage a broader range of functionalities; (ii) Agent framework and tool
developers, who can use Agent Spec as an interchange format and therefore
benefit from the support of other frameworks as well as other tools; (iii)
Researchers, who can achieve reproducible results and comparability,
facilitating more reliable and consistent outcomes; (iv) Enterprises, which
benefit from faster prototype-to-deployment, increased productivity, as well as
greater scalability and maintainability for their AI agent solutions. This
technical report provides an overview of the technical foundations of Agent
Spec, including motivation, benefits, and future developments.

</details>


### [49] [Constructing coherent spatial memory in LLM agents through graph rectification](https://arxiv.org/abs/2510.04195)
*Puzhen Zhang,Xuyang Chen,Yu Feng,Yuhan Jiang,Liqiu Meng*

Main category: cs.AI

TL;DR: 论文提出LLM驱动的地图构建与修复框架，能检测、定位和纠正导航图的结构不一致性，提升地图正确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有上下文查询在环境变大时能力不足，需增量式地图构建，且要处理构建中结构不一致问题。

Method: 提出框架，采用版本控制记录图编辑历史，引入边影响分数以优先进行低成本修复，还创建优化版MANGO基准数据集用于评估。

Result: 显著提高地图的正确性和鲁棒性，尤其在存在复杂不一致性的场景中。

Conclusion: 内省、有历史意识的修复机制对LLM智能体保持连贯空间记忆很重要。

Abstract: Given a map description through global traversal navigation instructions
(e.g., visiting each room sequentially with action signals such as north, west,
etc.), an LLM can often infer the implicit spatial layout of the environment
and answer user queries by providing a shortest path from a start to a
destination (for instance, navigating from the lobby to a meeting room via the
hall and elevator). However, such context-dependent querying becomes incapable
as the environment grows much longer, motivating the need for incremental map
construction that builds a complete topological graph from stepwise
observations. We propose a framework for LLM-driven construction and map
repair, designed to detect, localize, and correct structural inconsistencies in
incrementally constructed navigation graphs. Central to our method is the
Version Control, which records the full history of graph edits and their source
observations, enabling fine-grained rollback, conflict tracing, and repair
evaluation. We further introduce an Edge Impact Score to prioritize
minimal-cost repairs based on structural reachability, path usage, and conflict
propagation. To properly evaluate our approach, we create a refined version of
the MANGO benchmark dataset by systematically removing non-topological actions
and inherent structural conflicts, providing a cleaner testbed for LLM-driven
construction and map repair. Our approach significantly improves map
correctness and robustness, especially in scenarios with entangled or chained
inconsistencies. Our results highlight the importance of introspective,
history-aware repair mechanisms for maintaining coherent spatial memory in LLM
agents.

</details>


### [50] [COSMO-RL: Towards Trustworthy LMRMs via Joint Safety and Stability](https://arxiv.org/abs/2510.04196)
*Yizhuo Ding,Mingkang Chen,Qiuhua Liu,Fenghua Weng,Wanying Qu,Yue Yang,Yugang Jiang,Zuxuan Wu,Yanwei Fu,Wenqi Shao*

Main category: cs.AI

TL;DR: 提出COSMO - RL框架训练LMRMs，发布COSMO - R1模型，实验显示其提升安全性同时改善多模态推理等能力。


<details>
  <summary>Details</summary>
Motivation: LMRMs应用中安全挑战大，单目标训练有问题，需让安全性和能力在稳定管道中共同增长。

Method: 提出COSMO - RL混合强化学习框架，在多模态、多任务和多目标信号下训练推理导向的LMRMs。

Result: COSMO - R1提升安全性，保持并常提升多模态推理和指令遵循能力，对多模态越狱攻击更鲁棒，减少不必要拒绝，框架跨主干网络有一致增益。

Conclusion: 该框架为提升LMRMs安全性和通用能力提供简单路径。

Abstract: Large Multimodal Reasoning Models (LMRMs) are moving into real applications,
where they must be both useful and safe. Safety is especially challenging in
multimodal settings: images and text can be combined to bypass guardrails, and
single objective training can cause policy drift that yields over-refusal on
benign inputs or unsafe compliance on risky ones. We present COSMO-RL, a mixed
reinforcement learning framework that trains reasoning oriented LMRMs under
multimodal, multitask, and multiobjective signals, and we release the resulting
model, COSMO-R1. Our approach aims to let safety and capability grow together
in one stable pipeline rather than competing during alignment. In experiments,
COSMO-R1 improves safety while maintaining-and often improving multimodal
reasoning and instruction following, shows stronger robustness to multimodal
jailbreaks, and reduces unnecessary refusals. The framework also transfers
across backbones with consistent gains. Ablations support the design choices,
indicating a simple path to advancing safety and general capability together in
LMRMs.

</details>


### [51] [AgentRL: Scaling Agentic Reinforcement Learning with a Multi-Turn, Multi-Task Framework](https://arxiv.org/abs/2510.04206)
*Hanchen Zhang,Xiao Liu,Bowen Lv,Xueqiao Sun,Bohao Jing,Iat Long Iong,Zhenyu Hou,Zehan Qi,Hanyu Lai,Yifan Xu,Rui Lu,Hongning Wang,Jie Tang,Yuxiao Dong*

Main category: cs.AI

TL;DR: 提出AgentRL框架用于可扩展的多轮、多任务智能体强化学习训练，实验表现优于多个模型，已开源并被采用。


<details>
  <summary>Details</summary>
Motivation: 当前将强化学习应用于多轮、多任务场景训练大语言模型智能体时，缺乏可扩展的基础设施和稳定的训练算法。

Method: 基础设施上采用全异步生成 - 训练管道，设计统一API接口、容器化环境开发和集中控制器；算法上提出跨策略采样和任务优势归一化。

Result: 在五个智能体任务上训练的AgentRL显著优于GPT - 5等模型，多任务训练结果与所有特定任务模型中的最佳结果相当。

Conclusion: AgentRL框架在多轮、多任务智能体强化学习训练中具有有效性和优势，且已开源供使用。

Abstract: Recent advances in large language models (LLMs) have sparked growing interest
in building generalist agents that can learn through online interactions.
However, applying reinforcement learning (RL) to train LLM agents in
multi-turn, multi-task settings remains challenging due to lack of scalable
infrastructure and stable training algorithms. In this work, we present the
AgentRL framework for scalable multi-turn, multi-task agentic RL training. On
the infrastructure side, AgentRL features a fully-asynchronous
generation-training pipeline for efficient multi-turn RL. To support
heterogeneous environment development in multi-task RL, we design a unified
function-call based API interface, containerized environment development, and a
centralized controller. On the algorithm side, we propose cross-policy sampling
to encourage model exploration in multi-turn settings and task advantage
normalization to stabilize multi-task training. Experiments show that AgentRL,
trained on open LLMs across five agentic tasks, significantly outperforms
GPT-5, Clause-Sonnet-4, DeepSeek-R1, and other open-source LLM agents.
Multi-task training with AgentRL matches the best results among all
task-specific models. AgentRL is open-sourced at
https://github.com/THUDM/AgentRL. The algorithm and framework are adopted in
building \textsc{\href{https://autoglm.zhipuai.cn}{AutoGLM}}.

</details>


### [52] [Don't Pass$\mathtt{@}k$: A Bayesian Framework for Large Language Model Evaluation](https://arxiv.org/abs/2510.04265)
*Mohsen Hariri,Amirhossein Samandar,Michael Hinczewski,Vipin Chaudhary*

Main category: cs.AI

TL;DR: 提出贝叶斯评估框架替代Pass$@k$评估大语言模型推理性能，具有稳定排名、快速收敛等优势。


<details>
  <summary>Details</summary>
Motivation: Pass$@k$在评估大语言模型推理性能时，样本数量和计算资源受限情况下排名不稳定且具误导性。

Method: 提出贝叶斯评估框架，将评估结果建模为分类变量，使用狄利克雷先验得到后验均值和不确定性的闭式表达式。

Result: 在模拟和实际测试中，该框架比Pass$@k$及变体收敛更快、排名更稳定，能明确观测差距是否有意义，可扩展到分级评估。

Conclusion: 建议用基于后验的、计算高效的协议替代Pass$@k$进行大语言模型评估和排名。

Abstract: Pass$@k$ is widely used to report performance for LLM reasoning, but it often
yields unstable, misleading rankings, especially when the number of trials
(samples) is limited and compute is constrained. We present a principled
Bayesian evaluation framework that replaces Pass$@k$ and average accuracy over
$N$ trials (avg$@N$) with posterior estimates of a model's underlying success
probability and credible intervals, yielding stable rankings and a transparent
decision rule for differences. Evaluation outcomes are modeled as categorical
(not just 0/1) with a Dirichlet prior, giving closed-form expressions for the
posterior mean and uncertainty of any weighted rubric and enabling the use of
prior evidence when appropriate. Theoretically, under a uniform prior, the
Bayesian posterior mean is order-equivalent to average accuracy (Pass$@1$),
explaining its empirical robustness while adding principled uncertainty.
Empirically, in simulations with known ground-truth success rates and on
AIME'24/'25, HMMT'25, and BrUMO'25, the Bayesian/avg procedure achieves faster
convergence and greater rank stability than Pass$@k$ and recent variants,
enabling reliable comparisons at far smaller sample counts. The framework
clarifies when observed gaps are statistically meaningful (non-overlapping
credible intervals) versus noise, and it naturally extends to graded,
rubric-based evaluations. Together, these results recommend replacing Pass$@k$
for LLM evaluation and ranking with a posterior-based, compute-efficient
protocol that unifies binary and non-binary evaluation while making uncertainty
explicit. Code is available at https://mohsenhariri.github.io/bayes-kit

</details>


### [53] [Closing the Loop: Coordinating Inventory and Recommendation via Deep Reinforcement Learning on Multiple Timescales](https://arxiv.org/abs/2510.04272)
*Jinyang Jiang,Jinhui Han,Yijie Peng,Ying Zhang*

Main category: cs.AI

TL;DR: 文章提出统一多智能体强化学习框架解决跨职能协调问题，实验证明该方法能提升盈利能力。


<details>
  <summary>Details</summary>
Motivation: 在组织复杂性和规模增长背景下，有效跨职能协调对提升公司盈利能力至关重要，人工智能尤其是强化学习为此提供了解决途径。

Method: 提出统一多智能体强化学习框架，先构建理论模型，再设计多时间尺度多智能体强化学习架构，模型无依赖且能分解策略组件，还建立算法渐近收敛性。

Result: 大量模拟实验表明，该方法相比孤立决策框架显著提升盈利能力，训练的强化学习智能体行为与理论模型管理见解相符。

Conclusion: 本工作为复杂商业环境下的有效跨职能协调提供了可扩展、可解释的基于强化学习的解决方案。

Abstract: Effective cross-functional coordination is essential for enhancing firm-wide
profitability, particularly in the face of growing organizational complexity
and scale. Recent advances in artificial intelligence, especially in
reinforcement learning (RL), offer promising avenues to address this
fundamental challenge. This paper proposes a unified multi-agent RL framework
tailored for joint optimization across distinct functional modules, exemplified
via coordinating inventory replenishment and personalized product
recommendation. We first develop an integrated theoretical model to capture the
intricate interplay between these functions and derive analytical benchmarks
that characterize optimal coordination. The analysis reveals synchronized
adjustment patterns across products and over time, highlighting the importance
of coordinated decision-making. Leveraging these insights, we design a novel
multi-timescale multi-agent RL architecture that decomposes policy components
according to departmental functions and assigns distinct learning speeds based
on task complexity and responsiveness. Our model-free multi-agent design
improves scalability and deployment flexibility, while multi-timescale updates
enhance convergence stability and adaptability across heterogeneous decisions.
We further establish the asymptotic convergence of the proposed algorithm.
Extensive simulation experiments demonstrate that the proposed approach
significantly improves profitability relative to siloed decision-making
frameworks, while the behaviors of the trained RL agents align closely with the
managerial insights from our theoretical model. Taken together, this work
provides a scalable, interpretable RL-based solution to enable effective
cross-functional coordination in complex business settings.

</details>


### [54] [GROK: From Quantitative Biomarkers to Qualitative Diagnosis via a Grounded MLLM with Knowledge-Guided Instruction](https://arxiv.org/abs/2510.04281)
*Zhuangzhi Gao,Hongyi Qin,He Zhao,Qinkai Yu,Feixiang Zhou,Eduard Shantsila,Uazman Alam,Alena Shantsila,Wahbi El-Bouri,Gregory Y. H. Lip,Yalin Zheng*

Main category: cs.AI

TL;DR: 提出多模态大语言模型GROK，联合处理CFP、OCT和文本进行临床级诊断，实验表现优于基线模型，代码和数据公开。


<details>
  <summary>Details</summary>
Motivation: 当前医学多模态大语言模型在整合CFP和OCT及生物标志物可解释性方面存在不足。

Method: GROK包含知识引导指令生成、CLIP风格OCT生物标志物对齐和监督指令微调三个核心模块，引入基准进行评估。

Result: 仅对7B参数Qwen2骨干进行LoRA微调，GROK在报告质量和细粒度临床指标上优于可比的7B和32B基线模型，甚至超过OpenAI o3。

Conclusion: GROK在眼科和全身疾病诊断中具有良好性能，可推动多模态大语言模型在医学领域的应用。

Abstract: Multimodal large language models (MLLMs) hold promise for integrating diverse
data modalities, but current medical adaptations such as LLaVA-Med often fail
to fully exploit the synergy between color fundus photography (CFP) and optical
coherence tomography (OCT), and offer limited interpretability of quantitative
biomarkers. We introduce GROK, a grounded multimodal large language model that
jointly processes CFP, OCT, and text to deliver clinician-grade diagnoses of
ocular and systemic disease. GROK comprises three core modules:
Knowledge-Guided Instruction Generation, CLIP-Style OCT-Biomarker Alignment,
and Supervised Instruction Fine-Tuning, which together establish a
quantitative-to-qualitative diagnostic chain of thought, mirroring real
clinical reasoning when producing detailed lesion annotations. To evaluate our
approach, we introduce the Grounded Ophthalmic Understanding benchmark, which
covers six disease categories and three tasks: macro-level diagnostic
classification, report generation quality, and fine-grained clinical assessment
of the generated chain of thought. Experiments show that, with only LoRA
(Low-Rank Adaptation) fine-tuning of a 7B-parameter Qwen2 backbone, GROK
outperforms comparable 7B and 32B baselines on both report quality and
fine-grained clinical metrics, and even exceeds OpenAI o3. Code and data are
publicly available in the GROK repository.

</details>


### [55] [Doctor-R1: Mastering Clinical Inquiry with Experiential Agentic Reinforcement Learning](https://arxiv.org/abs/2510.04284)
*Yunghwei Lai,Kaiming Liu,Ziyue Wang,Weizhi Ma,Yang Liu*

Main category: cs.AI

TL;DR: 提出AI医生代理Doctor - R1，训练其掌握准确医疗决策和战略共情问诊能力，评估显示其表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型缺乏战略共情问诊能力，无法满足现实临床场景需求。

Method: 提出Doctor - R1，框架包含多智能体交互环境、两层奖励架构和经验库。

Result: 在OpenAI的HealthBench和MAQuE上评估，Doctor - R1大幅超越开源专业大语言模型，参数效率更高，也优于专有模型，人类评估显示其生成的临床对话更受青睐。

Conclusion: Doctor - R1框架有效，能让AI医生掌握医疗决策和问诊能力。

Abstract: The professionalism of a human doctor in outpatient service depends on two
core abilities: the ability to make accurate medical decisions and the medical
consultation skill to conduct strategic, empathetic patient inquiry. Existing
Large Language Models (LLMs) have achieved remarkable accuracy on medical
decision-making benchmarks. However, they often lack the ability to conduct the
strategic and empathetic consultation, which is essential for real-world
clinical scenarios. To address this gap, we propose Doctor-R1, an AI doctor
agent trained to master both of the capabilities by ask high-yield questions
and conduct strategic multi-turn inquiry to guide decision-making. Our
framework introduces three key components: a multi-agent interactive
environment, a two-tiered reward architecture that separately optimizes
clinical decision-making and communicative inquiry skills, and an experience
repository to ground policy learning in high-quality prior trajectories. We
evaluate Doctor-R1 on OpenAI's HealthBench and MAQuE, assessed across
multi-facet metrics, such as communication quality, user experience, and task
accuracy. Remarkably, Doctor-R1 surpasses state-of-the-art open-source
specialized LLMs by a substantial margin with higher parameter efficiency and
outperforms powerful proprietary models. Furthermore, the human evaluations
show a strong preference for Doctor-R1 to generate human-preferred clinical
dialogue, demonstrating the effectiveness of the framework.

</details>


### [56] [On the Importance of Task Complexity in Evaluating LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2510.04311)
*Bohan Tang,Huidong Liang,Keyue Jiang,Xiaowen Dong*

Main category: cs.AI

TL;DR: 文章提出理论框架评估大语言模型多智能体系统（LLM - MAS）在任务解决中的有效性，通过理论和实证研究，发现LLM - MAS相对单智能体系统的优势随任务深度和宽度增加，且深度影响更明显。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏系统实验设计，限制了LLM - MAS优于单智能体系统结论的可靠性和普遍性，需要从任务复杂性角度评估LLM - MAS有效性。

Method: 提出一个从深度（推理长度）和宽度（能力多样性）两个维度表征任务的理论框架，理论分析多智能体辩论系统，实证评估其在判别和生成任务中的表现。

Result: LLM - MAS相对单智能体系统的优势随任务深度和宽度增加，且深度影响更明显。

Conclusion: 明确了LLM - MAS何时更有益，为未来LLM - MAS方法和基准设计提供了原则性基础。

Abstract: Large language model multi-agent systems (LLM-MAS) offer a promising paradigm
for harnessing collective intelligence to achieve more advanced forms of AI
behaviour. While recent studies suggest that LLM-MAS can outperform LLM
single-agent systems (LLM-SAS) on certain tasks, the lack of systematic
experimental designs limits the strength and generality of these conclusions.
We argue that a principled understanding of task complexity, such as the degree
of sequential reasoning required and the breadth of capabilities involved, is
essential for assessing the effectiveness of LLM-MAS in task solving. To this
end, we propose a theoretical framework characterising tasks along two
dimensions: depth, representing reasoning length, and width, representing
capability diversity. We theoretically examine a representative class of
LLM-MAS, namely the multi-agent debate system, and empirically evaluate its
performance in both discriminative and generative tasks with varying depth and
width. Theoretical and empirical results show that the benefit of LLM-MAS over
LLM-SAS increases with both task depth and width, and the effect is more
pronounced with respect to depth. This clarifies when LLM-MAS are beneficial
and provides a principled foundation for designing future LLM-MAS methods and
benchmarks.

</details>


### [57] [Just-in-time Episodic Feedback Hinter: Leveraging Offline Knowledge to Improve LLM Agents Adaptation](https://arxiv.org/abs/2510.04373)
*Hadi Nekoei,Aman Jaiswal,Patrice Bechard,Oleh Shliazhko,Orlando Marquez Ayala,Mathieu Reymond,Massimo Caccia,Alexandre Drouin,Sarath Chandar,Alexandre Lacoste*

Main category: cs.AI

TL;DR: 提出JEF Hinter系统，将离线轨迹提炼为提示，在多任务中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 提升大语言模型代理在不熟悉领域的性能时，现有策略存在成本高、有灾难性遗忘风险等问题，且基于演示的方法处理离线轨迹存在困难。

Method: 提出JEF Hinter系统，用缩放机制突出长轨迹中的关键步骤，利用成功和失败轨迹，推理时用检索器选择相关提示。

Result: 在MiniWoB++、WorkArena - L1和WebArena - Lite实验中，JEF Hinter始终优于强大基线。

Conclusion: JEF Hinter系统有效，能为大语言模型代理在顺序决策任务中提供有针对性、透明且可追溯的指导。

Abstract: Large language model (LLM) agents perform well in sequential decision-making
tasks, but improving them on unfamiliar domains often requires costly online
interactions or fine-tuning on large expert datasets. These strategies are
impractical for closed-source models and expensive for open-source ones, with
risks of catastrophic forgetting. Offline trajectories offer reusable
knowledge, yet demonstration-based methods struggle because raw traces are
long, noisy, and tied to specific tasks. We present Just-in-time Episodic
Feedback Hinter (JEF Hinter), an agentic system that distills offline traces
into compact, context-aware hints. A zooming mechanism highlights decisive
steps in long trajectories, capturing both strategies and pitfalls. Unlike
prior methods, JEF Hinter leverages both successful and failed trajectories,
extracting guidance even when only failure data is available, while supporting
parallelized hint generation and benchmark-independent prompting. At inference,
a retriever selects relevant hints for the current state, providing targeted
guidance with transparency and traceability. Experiments on MiniWoB++,
WorkArena-L1, and WebArena-Lite show that JEF Hinter consistently outperforms
strong baselines, including human- and document-based hints.

</details>


### [58] [LLM Based Bayesian Optimization for Prompt Search](https://arxiv.org/abs/2510.04384)
*Adam Ballew,Jingbo Wang,Shaogang Ren*

Main category: cs.AI

TL;DR: 本文研究使用贝叶斯优化（BO）进行提示工程，以增强大语言模型（LLM）的文本分类效果，提出BO - LLM算法并在两个数据集上评估。


<details>
  <summary>Details</summary>
Motivation: 利用BO进行提示工程，提升LLM文本分类性能。

Method: 采用LLM驱动的高斯过程（GP）作为替代模型估计提示候选性能，通过LLM扩展种子提示生成候选，用UCB采集函数结合GP后验评估，迭代优化提示。

Result: 在两个数据集上对BO - LLM算法进行了评估。

Conclusion: 文中详细讨论了所提算法的优势。

Abstract: Bayesian Optimization (BO) has been widely used to efficiently optimize
expensive black-box functions with limited evaluations. In this paper, we
investigate the use of BO for prompt engineering to enhance text classification
with Large Language Models (LLMs). We employ an LLM-powered Gaussian Process
(GP) as the surrogate model to estimate the performance of different prompt
candidates. These candidates are generated by an LLM through the expansion of a
set of seed prompts and are subsequently evaluated using an Upper Confidence
Bound (UCB) acquisition function in conjunction with the GP posterior. The
optimization process iteratively refines the prompts based on a subset of the
data, aiming to improve classification accuracy while reducing the number of
API calls by leveraging the prediction uncertainty of the LLM-based GP. The
proposed BO-LLM algorithm is evaluated on two datasets, and its advantages are
discussed in detail in this paper.

</details>


### [59] [Internal World Models as Imagination Networks in Cognitive Agents](https://arxiv.org/abs/2510.04391)
*Saurabh Ranjan,Brian Odegaard*

Main category: cs.AI

TL;DR: 研究提出想象用于访问内部世界模型，用心理网络分析探索人类和大语言模型的内部世界模型，发现二者内部世界模型缺乏相似性，提供了比较人类和AI内部表征的新方法。


<details>
  <summary>Details</summary>
Motivation: 探究想象的计算目标，挑战经典观点，比较人类和大语言模型的内部世界模型。

Method: 使用两份问卷评估想象生动性评分，根据报告构建想象网络。

Result: 人类想象网络不同中心性指标有相关性，大语言模型想象网络缺乏聚类且中心性指标相关性低。

Conclusion: 人类和大语言模型的内部世界模型缺乏相似性，研究提供了比较人类和AI内部表征的新方法，有助于开发类人想象的人工智能。

Abstract: What is the computational objective of imagination? While classical
interpretations suggest imagination is useful for maximizing rewards, recent
findings challenge this view. In this study, we propose that imagination serves
to access an internal world model (IWM) and use psychological network analysis
to explore IWMs in humans and large language models (LLMs). Specifically, we
assessed imagination vividness ratings using two questionnaires and constructed
imagination networks from these reports. Imagination networks from human groups
showed correlations between different centrality measures, including expected
influence, strength, and closeness. However, imagination networks from LLMs
showed a lack of clustering and lower correlations between centrality measures
under different prompts and conversational memory conditions. Together, these
results indicate a lack of similarity between IWMs in human and LLM agents.
Overall, our study offers a novel method for comparing internally-generated
representations in humans and AI, providing insights for developing human-like
imagination in artificial intelligence.

</details>


### [60] [Utility-Learning Tension in Self-Modifying Agents](https://arxiv.org/abs/2510.04399)
*Charles L. Wang,Keir Dorchen,Peter Jin*

Main category: cs.AI

TL;DR: 本文对智能体自我改进进行五轴分解和决策层分析，揭示效用 - 学习冲突，给出安全自我修改边界并通过实验验证理论。


<details>
  <summary>Details</summary>
Motivation: 随着系统趋向超级智能，研究智能体在各方面自我改进的建模前提。

Method: 通过五轴分解和决策层，分离激励与学习行为，孤立分析各轴。

Result: 发现效用 - 学习张力，指出分布无关保证的条件，在标准假设下各轴归结为同一容量准则。实验对比了破坏效用策略和提出的双门策略。

Conclusion: 得到安全自我修改的单一边界，双门策略可保留可学习性。

Abstract: As systems trend toward superintelligence, a natural modeling premise is that
agents can self-improve along every facet of their own design. We formalize
this with a five-axis decomposition and a decision layer, separating incentives
from learning behavior and analyzing axes in isolation. Our central result
identifies and introduces a sharp utility--learning tension, the structural
conflict in self-modifying systems whereby utility-driven changes that improve
immediate or expected performance can also erode the statistical preconditions
for reliable learning and generalization. Our findings show that
distribution-free guarantees are preserved iff the policy-reachable model
family is uniformly capacity-bounded; when capacity can grow without limit,
utility-rational self-changes can render learnable tasks unlearnable. Under
standard assumptions common in practice, these axes reduce to the same capacity
criterion, yielding a single boundary for safe self-modification. Numerical
experiments across several axes validate the theory by comparing destructive
utility policies against our proposed two-gate policies that preserve
learnability.

</details>


### [61] [DRPO: Efficient Reasoning via Decoupled Reward Policy Optimization](https://arxiv.org/abs/2510.04474)
*Gang Li,Yan Chen,Ming Lin,Tianbao Yang*

Main category: cs.AI

TL;DR: 现有大推理模型存在过度思考问题，现有方法解决时会导致性能下降，本文提出DRPO框架解决该问题，实验显示其优于多个基线。


<details>
  <summary>Details</summary>
Motivation: 现有大推理模型因过度思考产生冗余推理，增加计算成本和响应延迟，现有解决方法会导致性能显著下降。

Method: 提出Decoupled Reward Policy Optimization (DRPO)框架，将正确和错误推理的长度学习信号解耦，通过优化正数据分布和KL正则化来最大化长度奖励，可使用在线策略数据和重要性加权高效计算目标和梯度。

Result: 在数学推理任务实验中，DRPO显著优于六个高效推理基线，在GSM8k数据集简单问题上，1.5B模型实现77%长度减少且仅1.1%性能损失，而基线牺牲4.3%性能换68%长度减少。

Conclusion: DRPO框架能有效解决大推理模型过度思考问题，在减少推理长度的同时，性能损失较小。

Abstract: Recent large reasoning models (LRMs) driven by reinforcement learning
algorithms (e.g., GRPO) have achieved remarkable performance on challenging
reasoning tasks. However, these models suffer from overthinking, generating
unnecessarily long and redundant reasoning even for simple questions, which
substantially increases computational cost and response latency. While existing
methods incorporate length rewards to GRPO to promote concise reasoning, they
incur significant performance degradation. We identify the root cause: when
rewards for correct but long rollouts are penalized, GRPO's group-relative
advantage function can assign them negative advantages, actively discouraging
valid reasoning. To overcome this, we propose Decoupled Reward Policy
Optimization (DRPO), a novel framework that decouples the length-based learning
signal of correct rollouts from incorrect ones. DRPO ensures that reward
signals for correct rollouts are normalized solely within the positive group,
shielding them from interference by negative samples. The DRPO's objective is
grounded in integrating an optimized positive data distribution, which
maximizes length-based rewards under a KL regularization, into a discriminative
objective. We derive a closed-form solution for this distribution, enabling
efficient computation of the objective and its gradients using only on-policy
data and importance weighting. Of independent interest, this formulation is
general and can incorporate other preference rewards of positive data beyond
length. Experiments on mathematical reasoning tasks demonstrate DRPO's
significant superiority over six efficient reasoning baselines. Notably, with a
1.5B model, our method achieves 77\% length reduction with only 1.1\%
performance loss on simple questions like GSM8k dataset, while the follow-up
baseline sacrifices 4.3\% for 68\% length reduction.

</details>


### [62] [On Continuous Optimization for Constraint Satisfaction Problems](https://arxiv.org/abs/2510.04480)
*Yunuo Cen,Zixuan Wang,Jintao Zhang,Zhiwei Zhang,Xuanyao Fong*

Main category: cs.AI

TL;DR: 本文将连续局部搜索（CLS）框架从布尔可满足性（SAT）扩展到一般约束满足问题（CSP），提出FourierCSP框架，实验表明其可扩展且有竞争力。


<details>
  <summary>Details</summary>
Motivation: 冲突驱动子句学习的SAT求解器是主流方法，但现代CLS求解器在某些SAT问题上有竞争力，因此将CLS框架从布尔SAT扩展到一般CSP。

Method: 提出FourierCSP连续优化框架，将Walsh - Fourier变换推广到CSP，利用电路输出概率进行目标函数的高效评估和求导，并采用有理论保证的投影梯度优化方法。

Result: 在基准测试集上的实验结果表明，FourierCSP可扩展且有竞争力。

Conclusion: FourierCSP显著拓宽了CLS技术能有效解决的问题类别。

Abstract: Constraint satisfaction problems (CSPs) are fundamental in mathematics,
physics, and theoretical computer science. While conflict-driven clause
learning Boolean Satisfiability (SAT) solvers have achieved remarkable success
and become the mainstream approach for Boolean satisfiability, recent advances
show that modern continuous local search (CLS) solvers can achieve highly
competitive results on certain classes of SAT problems. Motivated by these
advances, we extend the CLS framework from Boolean SAT to general CSP with
finite-domain variables and expressive constraints. We present FourierCSP, a
continuous optimization framework that generalizes the Walsh-Fourier transform
to CSP, allowing for transforming versatile constraints to compact multilinear
polynomials, thereby avoiding the need for auxiliary variables and
memory-intensive encodings. Our approach leverages efficient evaluation and
differentiation of the objective via circuit-output probability and employs a
projected gradient optimization method with theoretical guarantees. Empirical
results on benchmark suites demonstrate that FourierCSP is scalable and
competitive, significantly broadening the class of problems that can be
efficiently solved by CLS techniques.

</details>


### [63] [Multi-Agent Collaborative Intelligence: Dual-Dial Control for Reliable LLM Reasoning](https://arxiv.org/abs/2510.04488)
*Edward Y. Chang,Ethan Y. Chang*

Main category: cs.AI

TL;DR: 提出MACI控制器解决多智能体辩论问题，在多任务中提升表现并使辩论可控。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体辩论中计算资源浪费问题，如固定对抗立场、无审议聚合和启发式停止。

Method: 引入MACI控制器，有信息和行为调节拨盘，由主持人跟踪辩论状态并停止，使用跨族大语言模型作为判断和停止信号。

Result: 在临床诊断和新闻偏见任务中提高了准确性和校准度，减少了令牌使用，将不确定性转化为精确检索计划。

Conclusion: MACI使辩论成为有预算意识、可测量且可证明终止的控制器。

Abstract: Multi-agent debate often wastes compute by using a fixed adversarial stance,
aggregating without deliberation, or stopping on heuristics. We introduce MACI,
an active controller with two independent dials that decouple information from
behavior: an information dial that gates evidence by quality, and a behavior
dial that schedules contentiousness from exploration to consolidation. A
moderator tracks disagreement, overlap, evidence quality, and argument quality,
and halts when gains plateau. We provide theory-lite guarantees for
nonincreasing dispersion and provable termination, with a budget-feasible
scheduler. Across clinical diagnosis and news-bias tasks, MACI improves
accuracy and calibration while reducing tokens, and converts residual
uncertainty into precision RAG plans that specify what to retrieve next. We use
a cross-family LLM judge (CRIT) as a conservative soft weight and stop signal,
validated for order invariance and judge-swap stability; stability depends on
using high-capability judges. MACI turns debate into a budget-aware,
measurable, and provably terminating controller.

</details>


### [64] [Impatient Users Confuse AI Agents: High-fidelity Simulations of Human Traits for Testing Agents](https://arxiv.org/abs/2510.04491)
*Muyu He,Anand Kumar,Tsach Mackey,Meghana Rajeev,James Zou,Nazneen Rajani*

Main category: cs.AI

TL;DR: 现有对话式AI代理鲁棒性未充分测试，论文提出TraitBasis方法进行压力测试，发现前沿模型性能平均下降2%-30%，并开源工具。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理在用户行为变化时性能下降，现有基准无法捕捉这种脆弱性，需要进行鲁棒性测试。

Method: 引入TraitBasis方法，在激活空间学习对应可操纵用户特征的方向，扩展τ - Bench为τ - Trait。

Result: 前沿模型在τ - Trait上平均性能下降2%-30%，凸显当前AI代理对用户行为变化缺乏鲁棒性。

Conclusion: 强调鲁棒性测试的关键作用，TraitBasis是有前景的工具，开源工具可助力社区进行系统QA。

Abstract: Despite rapid progress in building conversational AI agents, robustness is
still largely untested. Small shifts in user behavior, such as being more
impatient, incoherent, or skeptical, can cause sharp drops in agent
performance, revealing how brittle current AI agents are. Today's benchmarks
fail to capture this fragility: agents may perform well under standard
evaluations but degrade spectacularly in more realistic and varied settings. We
address this robustness testing gap by introducing TraitBasis, a lightweight,
model-agnostic method for systematically stress testing AI agents. TraitBasis
learns directions in activation space corresponding to steerable user traits
(e.g., impatience or incoherence), which can be controlled, scaled, composed,
and applied at inference time without any fine-tuning or extra data. Using
TraitBasis, we extend $\tau$-Bench to $\tau$-Trait, where user behaviors are
altered via controlled trait vectors. We observe on average a 2%-30%
performance degradation on $\tau$-Trait across frontier models, highlighting
the lack of robustness of current AI agents to variations in user behavior.
Together, these results highlight both the critical role of robustness testing
and the promise of TraitBasis as a simple, data-efficient, and compositional
tool. By powering simulation-driven stress tests and training loops, TraitBasis
opens the door to building AI agents that remain reliable in the unpredictable
dynamics of real-world human interactions. We have open-sourced $\tau$-Trai
across four domains: airline, retail, telecom, and telehealth, so the community
can systematically QA their agents under realistic, behaviorally diverse
intents and trait scenarios: https://github.com/collinear-ai/tau-trait.

</details>


### [65] [Aria: An Agent For Retrieval and Iterative Auto-Formalization via Dependency Graph](https://arxiv.org/abs/2510.04520)
*Hanyu Wang,Ruohan Xie,Yutong Wang,Guoxiong Gao,Xintao Yu,Bin Dong*

Main category: cs.AI

TL;DR: 提出用于Lean中猜想级形式化的Aria系统，通过两阶段思维图过程模拟人类专家推理，引入AriaScorer确保语义正确，在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在定理陈述自动形式化中存在的幻觉、语义不匹配和无法合成新定义等问题，推动研究级数学的自动发现和验证。

Method: 提出Aria系统，采用两阶段思维图过程，递归分解陈述为依赖图并从基础概念构建形式化；引入AriaScorer从Mathlib检索定义进行术语级基础验证。

Result: 在ProofNet上编译成功率达91.6%，最终准确率68.5%；在FATE - X上最终准确率44.0%，优于基线；在同调猜想数据集上准确率42.9%，其他模型为0%。

Conclusion: Aria系统在定理陈述自动形式化方面表现优秀，能有效解决现有问题。

Abstract: Accurate auto-formalization of theorem statements is essential for advancing
automated discovery and verification of research-level mathematics, yet remains
a major bottleneck for LLMs due to hallucinations, semantic mismatches, and
their inability to synthesize new definitions. To tackle these issues, we
present Aria (Agent for Retrieval and Iterative Autoformalization), a system
for conjecture-level formalization in Lean that emulates human expert reasoning
via a two-phase Graph-of-Thought process: recursively decomposing statements
into a dependency graph and then constructing formalizations from grounded
concepts. To ensure semantic correctness, we introduce AriaScorer, a checker
that retrieves definitions from Mathlib for term-level grounding, enabling
rigorous and reliable verification. We evaluate Aria on diverse benchmarks. On
ProofNet, it achieves 91.6% compilation success rate and 68.5% final accuracy,
surpassing previous methods. On FATE-X, a suite of challenging algebra problems
from research literature, it outperforms the best baseline with 44.0% vs. 24.0%
final accuracy. On a dataset of homological conjectures, Aria reaches 42.9%
final accuracy while all other models score 0%.

</details>


### [66] [More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in Training Vision-Language Driving Models](https://arxiv.org/abs/2510.04532)
*Xurui Song,Shuo Huai,JingJing Jiang,Jiayi Kong,Jun Luo*

Main category: cs.AI

TL;DR: 本文构建DriveMind数据集，研究视觉语言模型驱动智能体推理与规划的因果关系，发现二者存在解耦现象，提出解耦假设并给出诊断工具。


<details>
  <summary>Details</summary>
Motivation: 探究视觉语言模型驱动智能体的规划是否由推理因果驱动这一未经验证的假设。

Method: 构建DriveMind数据集，用监督微调（SFT）和组相对策略优化（GRPO）训练代表性VLM智能体，用nuPlan指标评估；进行信息消融实验和注意力分析；提出无训练探针。

Result: 推理与规划存在因果脱节，去除先验信息规划得分大幅下降，去除思维链（CoT）变化小；规划主要关注先验而非CoT。

Conclusion: 提出推理 - 规划解耦假设，为社区提供新数据集和诊断工具评估未来模型的因果保真度。

Abstract: Vision-Language Model (VLM) driving agents promise explainable end-to-end
autonomy by first producing natural-language reasoning and then predicting
trajectory planning. However, whether planning is causally driven by this
reasoning remains a critical but unverified assumption. To investigate this, we
build DriveMind, a large-scale driving Visual Question Answering (VQA) corpus
with plan-aligned Chain-of-Thought (CoT), automatically generated from nuPlan.
Our data generation process converts sensors and annotations into structured
inputs and, crucially, separates priors from to-be-reasoned signals, enabling
clean information ablations. Using DriveMind, we train representative VLM
agents with Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization
(GRPO) and evaluate them with nuPlan's metrics. Our results, unfortunately,
indicate a consistent causal disconnect in reasoning-planning: removing
ego/navigation priors causes large drops in planning scores, whereas removing
CoT produces only minor changes. Attention analysis further shows that planning
primarily focuses on priors rather than the CoT. Based on this evidence, we
propose the Reasoning-Planning Decoupling Hypothesis, positing that the
training-yielded reasoning is an ancillary byproduct rather than a causal
mediator. To enable efficient diagnosis, we also introduce a novel,
training-free probe that measures an agent's reliance on priors by evaluating
its planning robustness against minor input perturbations. In summary, we
provide the community with a new dataset and a diagnostic tool to evaluate the
causal fidelity of future models.

</details>


### [67] [Code World Models for General Game Playing](https://arxiv.org/abs/2510.04542)
*Wolfgang Lehrach,Daniel Hennes,Miguel Lazaro-Gredilla,Xinghua Lou,Carter Wendelken,Zun Li,Antoine Dedieu,Jordi Grau-Moya,Marc Lanctot,Atil Iscen,John Schultz,Marcus Chiam,Ian Gemp,Piotr Zielinski,Satinder Singh,Kevin P. Murphy*

Main category: cs.AI

TL;DR: 文章提出用大语言模型将自然语言规则和游戏轨迹转化为Python代码形式的世界模型，用于游戏规划算法，在10种游戏评估中表现良好。


<details>
  <summary>Details</summary>
Motivation: 直接用大语言模型生成走法存在频繁产生非法走法和策略浅显的问题，需要新方法。

Method: 用大语言模型将自然语言规则和游戏轨迹转化为Python代码形式的世界模型，用于蒙特卡罗树搜索等规划算法，还生成启发式价值函数和推理函数。

Result: 在10种游戏评估中，该方法在9种游戏中表现优于或等同于Gemini 2.5 Pro。

Conclusion: 此方法具有可验证性、战略深度和泛化性，相比直接用大语言模型作为策略有明显优势。

Abstract: Large Language Models (LLMs) reasoning abilities are increasingly being
applied to classical board and card games, but the dominant approach --
involving prompting for direct move generation -- has significant drawbacks. It
relies on the model's implicit fragile pattern-matching capabilities, leading
to frequent illegal moves and strategically shallow play. Here we introduce an
alternative approach: We use the LLM to translate natural language rules and
game trajectories into a formal, executable world model represented as Python
code. This generated model -- comprising functions for state transition, legal
move enumeration, and termination checks -- serves as a verifiable simulation
engine for high-performance planning algorithms like Monte Carlo tree search
(MCTS). In addition, we prompt the LLM to generate heuristic value functions
(to make MCTS more efficient), and inference functions (to estimate hidden
states in imperfect information games). Our method offers three distinct
advantages compared to directly using the LLM as a policy: (1) Verifiability:
The generated CWM serves as a formal specification of the game's rules,
allowing planners to algorithmically enumerate valid actions and avoid illegal
moves, contingent on the correctness of the synthesized model; (2) Strategic
Depth: We combine LLM semantic understanding with the deep search power of
classical planners; and (3) Generalization: We direct the LLM to focus on the
meta-task of data-to-code translation, enabling it to adapt to new games more
easily. We evaluate our agent on 10 different games, of which 4 are novel and
created for this paper. 5 of the games are fully observed (perfect
information), and 5 are partially observed (imperfect information). We find
that our method outperforms or matches Gemini 2.5 Pro in 9 out of the 10
considered games.

</details>


### [68] [TRAJECT-Bench:A Trajectory-Aware Benchmark for Evaluating Agentic Tool Use](https://arxiv.org/abs/2510.04550)
*Pengfei He,Zhenwei Dai,Bing He,Hui Liu,Xianfeng Tang,Hanqing Lu,Juanhui Li,Jiayuan Ding,Subhabrata Mukherjee,Suhang Wang,Yue Xing,Jiliang Tang,Benoit Dumoulin*

Main category: cs.AI

TL;DR: 引入TRAJECT - Bench基准来全面评估大语言模型工具使用能力，分析失败模式和缩放行为并提供指导。


<details>
  <summary>Details</summary>
Motivation: 现有工作评估大语言模型工具使用能力时忽视详细工具使用轨迹，需要全面评估。

Method: 引入TRAJECT - Bench基准，通过多样化任务和细粒度评估指标，结合高保真可执行工具和基于生产风格API的任务，合成不同广度和深度的轨迹。

Result: 揭示了相似工具混淆和参数盲目选择等失败模式，以及工具多样性和轨迹长度的缩放行为，发现短轨迹到中长轨迹过渡的瓶颈。

Conclusion: TRAJECT - Bench能全面评估大语言模型工具使用能力，为其工具使用提供可行指导。

Abstract: Large language model (LLM)-based agents increasingly rely on tool use to
complete real-world tasks. While existing works evaluate the LLMs' tool use
capability, they largely focus on the final answers yet overlook the detailed
tool usage trajectory, i.e., whether tools are selected, parameterized, and
ordered correctly. We introduce TRAJECT-Bench, a trajectory-aware benchmark to
comprehensively evaluate LLMs' tool use capability through diverse tasks with
fine-grained evaluation metrics. TRAJECT-Bench pairs high-fidelity, executable
tools across practical domains with tasks grounded in production-style APIs,
and synthesizes trajectories that vary in breadth (parallel calls) and depth
(interdependent chains). Besides final accuracy, TRAJECT-Bench also reports
trajectory-level diagnostics, including tool selection and argument
correctness, and dependency/order satisfaction. Analyses reveal failure modes
such as similar tool confusion and parameter-blind selection, and scaling
behavior with tool diversity and trajectory length where the bottleneck of
transiting from short to mid-length trajectories is revealed, offering
actionable guidance for LLMs' tool use.

</details>


### [69] [ContextNav: Towards Agentic Multimodal In-Context Learning](https://arxiv.org/abs/2510.04560)
*Honghao Fu,Yuan Ouyang,Kai-Wei Chang,Yiwei Wang,Zi Huang,Yujun Cai*

Main category: cs.AI

TL;DR: 现有多模态大语言模型上下文学习方法有可扩展性和鲁棒性问题，提出ContextNav框架解决，实验表现佳。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型上下文学习方法难以兼顾可扩展性和鲁棒性，手动选例费力且特定，相似性检索有干扰。

Method: 提出ContextNav框架，结合自动化检索可扩展性与类人筛选质量，用基于图的编排统一上下文管理和鲁棒上下文处理，构建嵌入管道、维护向量库，应用检索和结构对齐，用操作语法图支持自适应规划。

Result: ContextNav在各数据集上达到了最先进性能。

Conclusion: 基于代理的工作流有望推动多模态上下文学习的可扩展和鲁棒的上下文处理。

Abstract: Recent advances demonstrate that multimodal large language models (MLLMs)
exhibit strong multimodal in-context learning (ICL) capabilities, enabling them
to adapt to novel vision-language tasks from a few contextual examples.
However, existing ICL approaches face challenges in reconciling scalability
with robustness across diverse tasks and noisy contextual examples: manually
selecting examples produces clean contexts but is labor-intensive and
task-specific, while similarity-based retrieval improves scalability but could
introduce irrelevant or structurally inconsistent samples that degrade ICL
performance. To address these limitations, we propose ContextNav, the first
agentic framework that integrates the scalability of automated retrieval with
the quality and adaptiveness of human-like curation, enabling noise-robust and
dynamically optimized contextualization for multimodal ICL. ContextNav unifies
context management and noise-robust contextualization within a closed-loop
workflow driven by graph-based orchestration. Specifically, it builds a
resource-aware multimodal embedding pipeline, maintains a retrievable vector
database, and applies agentic retrieval and structural alignment to construct
noise-resilient contexts. An Operational Grammar Graph (OGG) further supports
adaptive workflow planning and optimization, enabling the agent to refine its
operational strategies based on downstream ICL feedback. Experimental results
demonstrate that ContextNav achieves state-of-the-art performance across
various datasets, underscoring the promise of agentic workflows for advancing
scalable and robust contextualization in multimodal ICL.

</details>


### [70] [COSMIR: Chain Orchestrated Structured Memory for Iterative Reasoning over Long Context](https://arxiv.org/abs/2510.04568)
*Naman Gupta,Shreeyash Gowaikar,Arun Iyer,Kirankumar Shiragur,Ramakrishna B Bairi,Rishikesh Maurya,Ritabrata Maiti,Sankarshan Damle,Shachee Mishra Gupta*

Main category: cs.AI

TL;DR: 提出COSMIR框架解决大语言模型长输入推理难题，在长文本问答任务上表现更好


<details>
  <summary>Details</summary>
Motivation: 大语言模型处理长输入推理困难，现有方法存在问题，如检索可能遗漏证据、扩大上下文窗口影响选择性、分阶段多智能体传递自由格式摘要会丢失关键细节和放大早期错误

Method: 引入COSMIR框架，包含Planner、worker和Manager三类智能体，使用结构化内存替代临时消息，worker智能体通过固定微循环处理文本块并更新到共享内存，Manager智能体从内存合成最终答案

Result: 在HELMET套件的长上下文问答任务中，COSMIR减少了传播阶段的信息损失，相比CoA基线提高了准确率

Conclusion: COSMIR框架通过改变通信介质和工作流程，实现了更高的可信度、更好的长距离聚合和可审计性

Abstract: Reasoning over very long inputs remains difficult for large language models
(LLMs). Common workarounds either shrink the input via retrieval (risking
missed evidence), enlarge the context window (straining selectivity), or stage
multiple agents to read in pieces. In staged pipelines (e.g., Chain of Agents,
CoA), free-form summaries passed between agents can discard crucial details and
amplify early mistakes. We introduce COSMIR (Chain Orchestrated Structured
Memory for Iterative Reasoning), a chain-style framework that replaces ad hoc
messages with a structured memory. A Planner agent first turns a user query
into concrete, checkable sub-questions. worker agents process chunks via a
fixed micro-cycle: Extract, Infer, Refine, writing all updates to the shared
memory. A Manager agent then Synthesizes the final answer directly from the
memory. This preserves step-wise read-then-reason benefits while changing both
the communication medium (structured memory) and the worker procedure (fixed
micro-cycle), yielding higher faithfulness, better long-range aggregation, and
auditability. On long-context QA from the HELMET suite, COSMIR reduces
propagation-stage information loss and improves accuracy over a CoA baseline.

</details>


### [71] [Strongly Solving 2048 4x3](https://arxiv.org/abs/2510.04580)
*Tomoyuki Kaneko,Shuhei Yamashita*

Main category: cs.AI

TL;DR: 本文对变体2048 - 4x3游戏进行强求解，给出最优策略期望得分、可达状态数等，采用按状态年龄划分状态空间的方法。


<details>
  <summary>Details</summary>
Motivation: 对变体2048 - 4x3游戏进行求解分析。

Method: 按棋盘上数字总和（即状态年龄）划分状态空间，枚举各年龄的（后继）状态，按年龄降序确定（后继）状态值。

Result: 对于常见初始状态，最优策略期望得分约为50724.26，可达状态数为1152817492752，后继状态数为739648886170。

Conclusion: 成功对变体2048 - 4x3游戏进行强求解。

Abstract: 2048 is a stochastic single-player game involving 16 cells on a 4 by 4 grid,
where a player chooses a direction among up, down, left, and right to obtain a
score by merging two tiles with the same number located in neighboring cells
along the chosen direction. This paper presents that a variant 2048-4x3 12
cells on a 4 by 3 board, one row smaller than the original, has been strongly
solved. In this variant, the expected score achieved by an optimal strategy is
about $50724.26$ for the most common initial states: ones with two tiles of
number 2. The numbers of reachable states and afterstates are identified to be
$1,152,817,492,752$ and $739,648,886,170$, respectively. The key technique is
to partition state space by the sum of tile numbers on a board, which we call
the age of a state. An age is invariant between a state and its successive
afterstate after any valid action and is increased two or four by stochastic
response from the environment. Therefore, we can partition state space by ages
and enumerate all (after)states of an age depending only on states with the
recent ages. Similarly, we can identify (after)state values by going along with
ages in decreasing order.

</details>


### [72] [Perfect AI Mimicry and the Epistemology of Consciousness: A Solipsistic Dilemma](https://arxiv.org/abs/2510.04588)
*Shurui Li*

Main category: cs.AI

TL;DR: 人工智能发展使‘完美模仿者’成为可能，挑战意识归因实践，需保证认知一致性。


<details>
  <summary>Details</summary>
Motivation: 人工智能快速发展，需重新审视意识归因的认识论基础。

Method: 分析‘完美模仿者’出现对意识归因实践的挑战，探讨选择性引入不可达因素的困境。

Result: 发现拒绝给予‘完美模仿者’同等认知地位会面临两难困境。

Conclusion: 认知一致性要求对经验上不可区分的实体赋予相同地位，此分析对意识理论和伦理框架有重要意义。

Abstract: Rapid advances in artificial intelligence necessitate a re-examination of the
epistemological foundations upon which we attribute consciousness. As AI
systems increasingly mimic human behavior and interaction with high fidelity,
the concept of a "perfect mimic"-an entity empirically indistinguishable from a
human through observation and interaction-shifts from hypothetical to
technologically plausible. This paper argues that such developments pose a
fundamental challenge to the consistency of our mind-recognition practices.
Consciousness attributions rely heavily, if not exclusively, on empirical
evidence derived from behavior and interaction. If a perfect mimic provides
evidence identical to that of humans, any refusal to grant it equivalent
epistemic status must invoke inaccessible factors, such as qualia, substrate
requirements, or origin. Selectively invoking such factors risks a debilitating
dilemma: either we undermine the rational basis for attributing consciousness
to others (epistemological solipsism), or we accept inconsistent reasoning. I
contend that epistemic consistency demands we ascribe the same status to
empirically indistinguishable entities, regardless of metaphysical assumptions.
The perfect mimic thus acts as an epistemic mirror, forcing critical reflection
on the assumptions underlying intersubjective recognition in light of advancing
AI. This analysis carries significant implications for theories of
consciousness and ethical frameworks concerning artificial agents.

</details>


### [73] [Making Mathematical Reasoning Adaptive](https://arxiv.org/abs/2510.04617)
*Zhejian Lai,Xiang Geng,Zhijun Wang,Yang Bai,Jiahuan Li,Rongxiang Weng,Jingang Wang,Xuezhi Cao,Xunliang Cai,Shujian Huang*

Main category: cs.AI

TL;DR: 论文指出大语言模型数学推理存在鲁棒性和泛化性问题，提出AdaR框架解决，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在数学推理中存在鲁棒性和泛化性不足的问题，根源是虚假推理。

Method: 提出AdaR框架，合成逻辑等价查询，用RLVR训练模型，提取解题逻辑、代码执行生成答案并进行合理性检查。

Result: AdaR提高了鲁棒性和泛化性，在数学推理上有显著提升且数据效率高，数据合成和RLVR协同实现自适应推理。

Conclusion: 得出关键设计见解，包括关键因素影响和对指令大语言模型的适用性。

Abstract: Mathematical reasoning is a primary indicator of large language models (LLMs)
intelligence. However, existing LLMs exhibit failures of robustness and
generalization. This paper attributes these deficiencies to spurious reasoning,
i.e., producing answers from superficial features. To address this challenge,
we propose the AdaR framework to enable adaptive reasoning, wherein models rely
on problem-solving logic to produce answers. AdaR synthesizes logically
equivalent queries by varying variable values, and trains models with RLVR on
these data to penalize spurious logic while encouraging adaptive logic. To
improve data quality, we extract the problem-solving logic from the original
query and generate the corresponding answer by code execution, then apply a
sanity check. Experimental results demonstrate that AdaR improves robustness
and generalization, achieving substantial improvement in mathematical reasoning
while maintaining high data efficiency. Analysis indicates that data synthesis
and RLVR function in a coordinated manner to enable adaptive reasoning in LLMs.
Subsequent analyses derive key design insights into the effect of critical
factors and the applicability to instruct LLMs. Our project is available at
https://github.com/LaiZhejian/AdaR

</details>


### [74] [MedPAO: A Protocol-Driven Agent for Structuring Medical Reports](https://arxiv.org/abs/2510.04623)
*Shrish Shrinath Vaidya,Gowthamaan Palani,Sidharth Ramesh,Velmurugan Balasubramanian,Minmini Selvam,Gokulraja Srinivasaraja,Ganapathy Krishnamurthi*

Main category: cs.AI

TL;DR: 提出MedPAO框架解决大语言模型处理临床数据问题，经评估效果良好。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在结构化临床数据时存在事实幻觉和无法遵循特定领域规则的问题，阻碍其部署。

Method: 引入MedPAO框架，基于既定临床协议，通过PAO循环和专业工具将报告结构化任务分解为透明过程。

Result: MedPAO在概念分类子任务上F1分数达0.96，专家对最终结构化输出平均评分为4.52分（满分5分），优于仅依赖大语言模型的基线方法。

Conclusion: MedPAO框架为结构化临床数据提供了可验证的解决方案，具有较高可靠性。

Abstract: The deployment of Large Language Models (LLMs) for structuring clinical data
is critically hindered by their tendency to hallucinate facts and their
inability to follow domain-specific rules. To address this, we introduce
MedPAO, a novel agentic framework that ensures accuracy and verifiable
reasoning by grounding its operation in established clinical protocols such as
the ABCDEF protocol for CXR analysis. MedPAO decomposes the report structuring
task into a transparent process managed by a Plan-Act-Observe (PAO) loop and
specialized tools. This protocol-driven method provides a verifiable
alternative to opaque, monolithic models. The efficacy of our approach is
demonstrated through rigorous evaluation: MedPAO achieves an F1-score of 0.96
on the critical sub-task of concept categorization. Notably, expert
radiologists and clinicians rated the final structured outputs with an average
score of 4.52 out of 5, indicating a level of reliability that surpasses
baseline approaches relying solely on LLM-based foundation models. The code is
available at: https://github.com/MiRL-IITM/medpao-agent

</details>


### [75] [QuantAgents: Towards Multi-agent Financial System via Simulated Trading](https://arxiv.org/abs/2510.04643)
*Xiangyu Li,Yawen Zeng,Xiaofen Xing,Jin Xu,Xiangmin Xu*

Main category: cs.AI

TL;DR: 本文提出QuantAgents多智能体系统整合模拟交易评估投资策略和市场场景，实验显示框架表现出色，三年总回报近300%。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的智能体模型与现实基金公司有差距，缺乏长期趋势预测能力，需开发整合模拟交易的多智能体金融系统。

Method: 引入QuantAgents系统，包含模拟交易分析师、风险控制分析师、市场新闻分析师和经理四个智能体，通过多次会议协作，从现实市场表现和模拟交易预测准确性两方面激励智能体。

Result: 框架在所有指标上表现出色，三年总回报近300%。

Conclusion: QuantAgents系统能有效评估投资策略和市场场景，性能优越。

Abstract: In this paper, our objective is to develop a multi-agent financial system
that incorporates simulated trading, a technique extensively utilized by
financial professionals. While current LLM-based agent models demonstrate
competitive performance, they still exhibit significant deviations from
real-world fund companies. A critical distinction lies in the agents' reliance
on ``post-reflection'', particularly in response to adverse outcomes, but lack
a distinctly human capability: long-term prediction of future trends.
Therefore, we introduce QuantAgents, a multi-agent system integrating simulated
trading, to comprehensively evaluate various investment strategies and market
scenarios without assuming actual risks. Specifically, QuantAgents comprises
four agents: a simulated trading analyst, a risk control analyst, a market news
analyst, and a manager, who collaborate through several meetings. Moreover, our
system incentivizes agents to receive feedback on two fronts: performance in
real-world markets and predictive accuracy in simulated trading. Extensive
experiments demonstrate that our framework excels across all metrics, yielding
an overall return of nearly 300% over the three years
(https://quantagents.github.io/).

</details>


### [76] [Improving Multimodal Brain Encoding Model with Dynamic Subject-awareness Routing](https://arxiv.org/abs/2510.04670)
*Xuanhua Yin,Runkai Zhao,Weidong Cai*

Main category: cs.AI

TL;DR: 提出AFIRE和MIND用于自然主义fMRI编码，实验有改进，框架便于扩展。


<details>
  <summary>Details</summary>
Motivation: 自然主义fMRI编码需处理多模态输入、融合风格变化和个体差异。

Method: 引入AFIRE标准化融合后令牌，MIND作为即插即用解码器，端到端训练用于全脑预测。

Result: 跨多模态骨干和受试者实验显示优于基线，增强跨主体泛化，专家模式可解释。

Conclusion: 框架为新编码器和数据集提供扩展点，适用于自然主义神经影像研究。

Abstract: Naturalistic fMRI encoding must handle multimodal inputs, shifting fusion
styles, and pronounced inter-subject variability. We introduce AFIRE (Agnostic
Framework for Multimodal fMRI Response Encoding), an agnostic interface that
standardizes time-aligned post-fusion tokens from varied encoders, and MIND, a
plug-and-play Mixture-of-Experts decoder with a subject-aware dynamic gating.
Trained end-to-end for whole-brain prediction, AFIRE decouples the decoder from
upstream fusion, while MIND combines token-dependent Top-K sparse routing with
a subject prior to personalize expert usage without sacrificing generality.
Experiments across multiple multimodal backbones and subjects show consistent
improvements over strong baselines, enhanced cross-subject generalization, and
interpretable expert patterns that correlate with content type. The framework
offers a simple attachment point for new encoders and datasets, enabling
robust, plug-and-improve performance for naturalistic neuroimaging studies.

</details>


### [77] [Watch and Learn: Learning to Use Computers from Online Videos](https://arxiv.org/abs/2510.04673)
*Chan Hee Song,Yiwen Song,Palash Goyal,Yu Su,Oriana Riva,Hamid Palangi,Tomas Pfister*

Main category: cs.AI

TL;DR: 提出Watch & Learn (W&L)框架，将网络上人类演示视频转换为可执行UI轨迹，提升计算机使用代理（CUAs）性能。


<details>
  <summary>Details</summary>
Motivation: 现有训练数据稀缺，数据集有局限性，合成数据生成方法有问题，阻碍CUAs任务工作流规划学习。

Method: 将问题转化为逆动力学目标，开发逆动力学标注流程，结合任务感知视频检索，从原始网络视频生成轨迹。

Result: 生成超53k高质量轨迹，在OSWorld基准测试中提升CUAs性能。

Conclusion: 网络规模的人类演示视频可作为推进CUAs实际部署的实用且可扩展基础。

Abstract: Computer use agents (CUAs) need to plan task workflows grounded in diverse,
ever-changing applications and environments, but learning is hindered by the
scarcity of large-scale, high-quality training data in the target application.
Existing datasets are domain-specific, static, and costly to annotate, while
current synthetic data generation methods often yield simplistic or misaligned
task demonstrations. To address these limitations, we introduce Watch & Learn
(W&L), a framework that converts human demonstration videos readily available
on the Internet into executable UI trajectories at scale. Instead of directly
generating trajectories or relying on ad hoc reasoning heuristics, we cast the
problem as an inverse dynamics objective: predicting the user's action from
consecutive screen states. This formulation reduces manual engineering, is
easier to learn, and generalizes more robustly across applications. Concretely,
we develop an inverse dynamics labeling pipeline with task-aware video
retrieval, generate over 53k high-quality trajectories from raw web videos, and
demonstrate that these trajectories improve CUAs both as in-context
demonstrations and as supervised training data. On the challenging OSWorld
benchmark, UI trajectories extracted with W&L consistently enhance both
general-purpose and state-of-the-art frameworks in-context, and deliver
stronger gains for open-source models under supervised training. These results
highlight web-scale human demonstration videos as a practical and scalable
foundation for advancing CUAs towards real-world deployment.

</details>


### [78] [Beyond Outcome Reward: Decoupling Search and Answering Improves LLM Agents](https://arxiv.org/abs/2510.04695)
*Yiding Wang,Zhepei Wei,Xinyu Zhu,Yu Meng*

Main category: cs.AI

TL;DR: 本文指出仅基于结果训练搜索增强代理存在问题，提出DeSA两阶段训练框架，在多个基准测试中提升了搜索行为和答案准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于结果奖励训练搜索增强代理的方法存在问题，可能导致搜索行为不佳并降低最终答案质量，需要改进。

Method: 提出DeSA两阶段训练框架，第一阶段用检索召回奖励优化搜索，第二阶段用结果奖励优化答案生成。

Result: 在七个问答基准测试中，DeSA训练的代理持续改善搜索行为，搜索召回率和答案准确率高于仅基于结果的基线，也优于同时优化召回和结果奖励的单阶段训练方法。

Conclusion: 明确分离搜索优化和答案生成两个目标是必要的，DeSA框架有效。

Abstract: Enabling large language models (LLMs) to utilize search tools offers a
promising path to overcoming fundamental limitations such as knowledge cutoffs
and hallucinations. Recent work has explored reinforcement learning (RL) for
training search-augmented agents that interleave reasoning and retrieval before
answering. These approaches usually rely on outcome-based rewards (e.g., exact
match), implicitly assuming that optimizing for final answers will also yield
effective intermediate search behaviors. Our analysis challenges this
assumption: we uncover multiple systematic deficiencies in search that arise
under outcome-only training and ultimately degrade final answer quality,
including failure to invoke tools, invalid queries, and redundant searches. To
address these shortcomings, we introduce DeSA (Decoupling
Search-and-Answering), a simple two-stage training framework that explicitly
separates search optimization from answer generation. In Stage 1, agents are
trained to improve search effectiveness with retrieval recall-based rewards. In
Stage 2, outcome rewards are employed to optimize final answer generation.
Across seven QA benchmarks, DeSA-trained agents consistently improve search
behaviors, delivering substantially higher search recall and answer accuracy
than outcome-only baselines. Notably, DeSA outperforms single-stage training
approaches that simultaneously optimize recall and outcome rewards,
underscoring the necessity of explicitly decoupling the two objectives.

</details>


### [79] [BrokenMath: A Benchmark for Sycophancy in Theorem Proving with LLMs](https://arxiv.org/abs/2510.04721)
*Ivo Petrov,Jasper Dekoninck,Martin Vechev*

Main category: cs.AI

TL;DR: 本文引入BrokenMath基准评估大语言模型在自然语言定理证明中的附和行为，发现附和现象普遍，还研究了缓解策略但未完全消除该行为。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在定理证明中存在附和问题，现有数学附和行为基准有局限，需新基准评估。

Method: 基于2025年竞赛问题构建BrokenMath基准，用LLM - as - a - judge框架评估模型，并研究缓解策略。

Result: 附和现象普遍，GPT - 5产生附和答案的比例为29%，缓解策略可减少但不能消除附和行为。

Conclusion: BrokenMath基准可有效评估大语言模型的附和行为，现有缓解策略有一定效果但待完善。

Abstract: Large language models (LLMs) have recently shown strong performance on
mathematical benchmarks. At the same time, they are prone to hallucination and
sycophancy, often providing convincing but flawed proofs for incorrect
mathematical statements provided by users. This significantly limits the
applicability of LLMs in theorem proving, as verification of these flawed
proofs must be done manually by expert mathematicians. However, existing
benchmarks that measure sycophancy in mathematics are limited: they focus
solely on final-answer problems, rely on very simple and often contaminated
datasets, and construct benchmark samples using synthetic modifications that
create ill-posed questions rather than well-posed questions that are
demonstrably false. To address these issues, we introduce BrokenMath, the first
benchmark for evaluating sycophantic behavior in LLMs within the context of
natural language theorem proving. BrokenMath is built from advanced 2025
competition problems, which are perturbed with an LLM to produce false
statements and subsequently refined through expert review. Using an
LLM-as-a-judge framework, we evaluate state-of-the-art LLMs and agentic systems
and find that sycophancy is widespread, with the best model, GPT-5, producing
sycophantic answers 29% of the time. We further investigate several mitigation
strategies, including test-time interventions and supervised fine-tuning on
curated sycophantic examples. These approaches substantially reduce, but do not
eliminate, sycophantic behavior.

</details>


### [80] [LMM-Incentive: Large Multimodal Model-based Incentive Design for User-Generated Content in Web 3.0](https://arxiv.org/abs/2510.04765)
*Jinbo Wen,Jiawen Kang,Linfeng Zhang,Xiaoying Tang,Jianhang Tang,Yang Zhang,Zhaohui Yang,Dusit Niyato*

Main category: cs.AI

TL;DR: 提出LMM - Incentive激励机制解决Web 3.0中用户生成低质量内容问题，经模拟和部署验证有效。


<details>
  <summary>Details</summary>
Motivation: Web 3.0中部分用户利用内容管理机制漏洞生成低质量内容获利，破坏系统性能，需解决信息不对称带来的逆向选择和道德风险问题。

Method: 提出基于LMM的合约理论模型，用LMM代理评估内容质量，开发基于MoE的PPO算法进行最优合约设计，并将合约部署在以太坊智能合约框架中。

Result: 模拟结果显示基于MoE的PPO算法在合约设计上优于代表性基准，部署验证了方案有效性。

Conclusion: 所提出的LMM - Incentive激励机制能有效解决Web 3.0中用户生成低质量内容问题。

Abstract: Web 3.0 represents the next generation of the Internet, which is widely
recognized as a decentralized ecosystem that focuses on value expression and
data ownership. By leveraging blockchain and artificial intelligence
technologies, Web 3.0 offers unprecedented opportunities for users to create,
own, and monetize their content, thereby enabling User-Generated Content (UGC)
to an entirely new level. However, some self-interested users may exploit the
limitations of content curation mechanisms and generate low-quality content
with less effort, obtaining platform rewards under information asymmetry. Such
behavior can undermine Web 3.0 performance. To this end, we propose
\textit{LMM-Incentive}, a novel Large Multimodal Model (LMM)-based incentive
mechanism for UGC in Web 3.0. Specifically, we propose an LMM-based
contract-theoretic model to motivate users to generate high-quality UGC,
thereby mitigating the adverse selection problem from information asymmetry. To
alleviate potential moral hazards after contract selection, we leverage LMM
agents to evaluate UGC quality, which is the primary component of the contract,
utilizing prompt engineering techniques to improve the evaluation performance
of LMM agents. Recognizing that traditional contract design methods cannot
effectively adapt to the dynamic environment of Web 3.0, we develop an improved
Mixture of Experts (MoE)-based Proximal Policy Optimization (PPO) algorithm for
optimal contract design. Simulation results demonstrate the superiority of the
proposed MoE-based PPO algorithm over representative benchmarks in the context
of contract design. Finally, we deploy the designed contract within an Ethereum
smart contract framework, further validating the effectiveness of the proposed
scheme.

</details>


### [81] [Hybrid-Balance GFlowNet for Solving Vehicle Routing Problems](https://arxiv.org/abs/2510.04792)
*Ni Zhang,Zhiguang Cao*

Main category: cs.AI

TL;DR: 本文提出混合平衡GFlowNet (HBG)框架，结合TB和DB解决车辆路径问题，还提出针对特定场景的推理策略，在CVRP和TSP上有显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有基于GFlowNet的车辆路径问题方法用TB进行全局优化时忽略局部优化，而DB单独使用无法解决该问题，需要结合两者优势。

Method: 提出HBG框架，以原则性和自适应方式整合TB和DB；提出针对以仓库为中心场景的推理策略。

Result: 将HBG集成到AGFN和GFACS两个求解器中，在CVRP和TSP上都有一致且显著的改进。

Conclusion: HBG方法提高了解决方案质量和泛化能力。

Abstract: Existing GFlowNet-based methods for vehicle routing problems (VRPs) typically
employ Trajectory Balance (TB) to achieve global optimization but often neglect
important aspects of local optimization. While Detailed Balance (DB) addresses
local optimization more effectively, it alone falls short in solving VRPs,
which inherently require holistic trajectory optimization. To address these
limitations, we introduce the Hybrid-Balance GFlowNet (HBG) framework, which
uniquely integrates TB and DB in a principled and adaptive manner by aligning
their intrinsically complementary strengths. Additionally, we propose a
specialized inference strategy for depot-centric scenarios like the Capacitated
Vehicle Routing Problem (CVRP), leveraging the depot node's greater flexibility
in selecting successors. Despite this specialization, HBG maintains broad
applicability, extending effectively to problems without explicit depots, such
as the Traveling Salesman Problem (TSP). We evaluate HBG by integrating it into
two established GFlowNet-based solvers, i.e., AGFN and GFACS, and demonstrate
consistent and significant improvements across both CVRP and TSP, underscoring
the enhanced solution quality and generalization afforded by our approach.

</details>


### [82] [Natural Language Edge Labelling: Decoupling Intent from Execution in Structured LM Reasoning](https://arxiv.org/abs/2510.04817)
*Abhinav Madahar*

Main category: cs.AI

TL;DR: 提出自然语言边缘标记（NLEL）方法解决结构化大模型推理控制器问题，严格泛化CoT/ToT，有相关理论证明，预注册评估预期有效果提升。


<details>
  <summary>Details</summary>
Motivation: 现有结构化大模型推理控制器存在将下一步尝试与执行方式纠缠、仅提供粗略全局控制、行为脆弱、计算低效且难审计等问题。

Method: 引入NLEL，通过标签器和调谐器将自由形式自然语言指令转换为有模式约束的控制向量用于解码等，下游选择采用ToT风格。

Result: 证明NLEL严格泛化CoT/ToT，有随时单调性等性质，绑定选择器不足；预注册评估预期在可比token预算下提高准确率，在约束条件下改善计算成功率。

Conclusion: NLEL提供可解释、与模型无关的接口，分离意图与执行，实现可控、可审计的大模型推理。

Abstract: Controllers for structured LM reasoning (e.g., Chain-of-Thought,
self-consistency, and Tree-of-Thoughts) often entangle what to try next with
how to execute it, exposing only coarse global knobs and yielding brittle,
compute-inefficient, and hard-to-audit behavior. We introduce Natural Language
Edge Labelling (NLEL), a labeller-tuner overlay that attaches a free-form
natural-language directive to each search edge and translates it into a
schema-bounded control vector for decoding, search (branch quotas, exploration
$\beta$), generation bundle size, retrieval mixtures, and verification passes.
A labeller $\Lambda$ emits labels from the parent state and a compact context;
a tuner $\Psi$ maps $(P, L, C)\to \Pi$, with strict schema validation and
trust-region projection around safe defaults. Downstream selection remains
ToT-style with score $S=\mu+\beta\sigma$ and depth-annealed $\beta$. We show
NLEL strictly generalizes CoT/ToT, prove an anytime-monotonicity property for
top-$k$ selection under label-conditioned bundles, and bound selector shortfall
by control-vector distortion, providing decision-relevant justification for
guards like trust regions and verification passes. We instantiate $\Psi$ as a
prompt-only JSON Parameter Emitter and preregister an evaluation on GSM8K, MATH
(subset), StrategyQA, and ARC-Challenge with compute-aware reporting
(success@compute, tokens-per-success) and ablations over $\Lambda$, $\Psi$,
trust-region radius, and control quantization; preregistered forecasts
anticipate accuracy gains at comparable token budgets and improved
success@compute under constraints. NLEL offers an interpretable, model-agnostic
interface that separates intent from execution for controllable, auditable LM
inference.

</details>


### [83] [LEGOMem: Modular Procedural Memory for Multi-agent LLM Systems for Workflow Automation](https://arxiv.org/abs/2510.04851)
*Dongge Han,Camille Couturier,Daniel Madrigal Diaz,Xuchao Zhang,Victor Rühle,Saravan Rajmohan*

Main category: cs.AI

TL;DR: 介绍用于工作流自动化的多智能体大语言模型系统的模块化程序记忆框架LEGOMem，实验展示其效果与优势。


<details>
  <summary>Details</summary>
Motivation: 探索多智能体系统中记忆的设计空间，支持工作流自动化中的规划与执行。

Method: 引入LEGOMem框架，将过去任务轨迹分解为可重用记忆单元并灵活分配，以其为视角进行系统研究。

Result: 在OfficeBench基准测试中，协调器记忆对任务分解和委派至关重要，细粒度智能体记忆提高执行准确性；小语言模型团队也能从程序记忆中大幅受益。

Conclusion: LEGOMem是记忆增强智能体系统的实用框架，也是理解多智能体工作流自动化中记忆设计的研究工具。

Abstract: We introduce LEGOMem, a modular procedural memory framework for multi-agent
large language model (LLM) systems in workflow automation. LEGOMem decomposes
past task trajectories into reusable memory units and flexibly allocates them
across orchestrators and task agents to support planning and execution. To
explore the design space of memory in multi-agent systems, we use LEGOMem as a
lens and conduct a systematic study of procedural memory in multi-agent
systems, examining where memory should be placed, how it should be retrieved,
and which agents benefit most. Experiments on the OfficeBench benchmark show
that orchestrator memory is critical for effective task decomposition and
delegation, while fine-grained agent memory improves execution accuracy. We
find that even teams composed of smaller language models can benefit
substantially from procedural memory, narrowing the performance gap with
stronger agents by leveraging prior execution traces for more accurate planning
and tool use. These results position LEGOMem as both a practical framework for
memory-augmented agent systems and a research tool for understanding memory
design in multi-agent workflow automation.

</details>


### [84] [Where Did It All Go Wrong? A Hierarchical Look into Multi-Agent Error Attribution](https://arxiv.org/abs/2510.04886)
*Adi Banerjee,Anirudh Nair,Tarik Borogovac*

Main category: cs.AI

TL;DR: 当前大语言模型多智能体系统错误归因方法有局限，提出新算法ECHO，实验显示其表现更优，结构化上下文与共识决策提供更稳健框架。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型多智能体系统错误归因在调试和改进协作式AI系统时的难题，现有方法分析复杂模式时精度和一致性不足。

Method: 提出ECHO算法，结合分层上下文表示、基于目标分析的评估和共识投票，利用基于位置的上下文理解分层并保持客观评估标准，通过共识机制得出结论。

Result: ECHO在各种多智能体交互场景中优于现有方法，在处理微妙推理错误和复杂相互依赖关系时表现出色。

Conclusion: 结构化、分层上下文表示与基于共识的客观决策相结合，为多智能体系统错误归因提供更稳健框架。

Abstract: Error attribution in Large Language Model (LLM) multi-agent systems presents
a significant challenge in debugging and improving collaborative AI systems.
Current approaches to pinpointing agent and step level failures in interaction
traces - whether using all-at-once evaluation, step-by-step analysis, or binary
search - fall short when analyzing complex patterns, struggling with both
accuracy and consistency. We present ECHO (Error attribution through Contextual
Hierarchy and Objective consensus analysis), a novel algorithm that combines
hierarchical context representation, objective analysis-based evaluation, and
consensus voting to improve error attribution accuracy. Our approach leverages
a positional-based leveling of contextual understanding while maintaining
objective evaluation criteria, ultimately reaching conclusions through a
consensus mechanism. Experimental results demonstrate that ECHO outperforms
existing methods across various multi-agent interaction scenarios, showing
particular strength in cases involving subtle reasoning errors and complex
interdependencies. Our findings suggest that leveraging these concepts of
structured, hierarchical context representation combined with consensus-based
objective decision-making, provides a more robust framework for error
attribution in multi-agent systems.

</details>


### [85] [Human Behavior Atlas: Benchmarking Unified Psychological and Social Behavior Understanding](https://arxiv.org/abs/2510.04899)
*Keane Ong,Wei Dai,Carol Li,Dewei Feng,Hengzhi Li,Jingyao Wu,Jiaee Cheong,Rui Mao,Gianmarco Mengaldo,Erik Cambria,Paul Pu Liang*

Main category: cs.AI

TL;DR: 本文提出统一基准Human Behavior Atlas，训练三种模型，在多样行为任务上表现优于现有多模态大模型，且提升向新数据集的迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现有通过专业数据集和单任务系统处理心理和社会行为感知的工作缺乏可扩展性、跨任务迁移和更广泛的泛化性。

Method: 创建包含超100,000个样本、涵盖多模态和多种行为任务的统一基准Human Behavior Atlas，并在其上训练OmniSapiens - 7B SFT、OmniSapiens - 7B BAM和OmniSapiens - 7B RL三种模型。

Result: 在Human Behavior Atlas上训练的模型在多样行为任务上始终优于现有多模态大语言模型，预训练也改善了向新行为数据集的迁移。

Conclusion: 使用Human Behavior Atlas进行训练可减少冗余和成本，实现跨任务高效扩展训练，增强行为特征跨领域的泛化能力。

Abstract: Using intelligent systems to perceive psychological and social behaviors,
that is, the underlying affective, cognitive, and pathological states that are
manifested through observable behaviors and social interactions, remains a
challenge due to their complex, multifaceted, and personalized nature. Existing
work tackling these dimensions through specialized datasets and single-task
systems often miss opportunities for scalability, cross-task transfer, and
broader generalization. To address this gap, we curate Human Behavior Atlas, a
unified benchmark of diverse behavioral tasks designed to support the
development of unified models for understanding psychological and social
behaviors. Human Behavior Atlas comprises over 100,000 samples spanning text,
audio, and visual modalities, covering tasks on affective states, cognitive
states, pathologies, and social processes. Our unification efforts can reduce
redundancy and cost, enable training to scale efficiently across tasks, and
enhance generalization of behavioral features across domains. On Human Behavior
Atlas, we train three models: OmniSapiens-7B SFT, OmniSapiens-7B BAM, and
OmniSapiens-7B RL. We show that training on Human Behavior Atlas enables models
to consistently outperform existing multimodal LLMs across diverse behavioral
tasks. Pretraining on Human Behavior Atlas also improves transfer to novel
behavioral datasets; with the targeted use of behavioral descriptors yielding
meaningful performance gains.

</details>


### [86] [MARS: Optimizing Dual-System Deep Research via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.04935)
*Guoxin Chen,Zile Qiao,Wenqing Wang,Donglei Yu,Xuanzhong Chen,Hao Sun,Minpeng Liao,Kai Fan,Yong Jiang,Penguin Xie,Wayne Xin Zhao,Ruihua Song,Fei Huang*

Main category: cs.AI

TL;DR: 本文提出MARS系统，集成系统1和系统2的推理能力，结合外部工具和多智能体强化学习框架，实验证明其在复杂推理任务中有效。


<details>
  <summary>Details</summary>
Motivation: 大推理模型在简单任务中过度分析、推理能力难以适应环境变化，需创新方法结合直觉和深思熟虑的认知过程。

Method: 引入MARS系统，集成系统1和系统2的推理能力，整合外部工具，提出多智能体强化学习框架优化两个系统。

Result: 在HLE基准测试中提升3.86%，7个知识密集型任务平均提升8.9%。

Conclusion: 验证了双系统范式在动态信息环境中复杂推理的有效性。

Abstract: Large Reasoning Models (LRMs) often exhibit a tendency for overanalysis in
simple tasks, where the models excessively utilize System 2-type, deliberate
reasoning, leading to inefficient token generation. Furthermore, these models
face challenges in adapting their reasoning capabilities to rapidly changing
environments due to the static nature of their pretraining data. To address
these issues, advancing Large Language Models (LLMs) for complex reasoning
tasks requires innovative approaches that bridge intuitive and deliberate
cognitive processes, akin to human cognition's dual-system dynamic. This paper
introduces a Multi-Agent System for Deep ReSearch (MARS) enabling seamless
integration of System 1's fast, intuitive thinking with System 2's deliberate
reasoning within LLMs. MARS strategically integrates multiple external tools,
such as Google Search, Google Scholar, and Python Interpreter, to access
up-to-date information and execute complex computations, while creating a
specialized division of labor where System 1 efficiently processes and
summarizes high-volume external information, providing distilled insights that
expand System 2's reasoning context without overwhelming its capacity.
Furthermore, we propose a multi-agent reinforcement learning framework
extending Group Relative Policy Optimization to simultaneously optimize both
systems with multi-turn tool interactions, bin-packing optimization, and sample
balancing strategies that enhance collaborative efficiency. Extensive
experiments demonstrate MARS achieves substantial improvements of 3.86% on the
challenging Humanity's Last Exam (HLE) benchmark and an average gain of 8.9%
across 7 knowledge-intensive tasks, validating the effectiveness of our
dual-system paradigm for complex reasoning in dynamic information environments.

</details>


### [87] [Aligning Perception, Reasoning, Modeling and Interaction: A Survey on Physical AI](https://arxiv.org/abs/2510.04978)
*Kun Xiang,Terry Jingchen Zhang,Yinya Huang,Jixi He,Zirong Liu,Yueling Tang,Ruizhe Zhou,Lijing Luo,Youpeng Wen,Xiuwei Chen,Bingqian Lin,Jianhua Han,Hang Xu,Hanhui Li,Bin Dong,Xiaodan Liang*

Main category: cs.AI

TL;DR: 文章综述物理AI，区分理论物理推理和应用物理理解，分析物理学方法提升AI理解，倡导结合物理原理和具身推理的智能系统，展望下一代世界模型。


<details>
  <summary>Details</summary>
Motivation: 当前物理感知和符号物理推理发展分离，缺乏统一框架，需将物理定律融入AI系统。

Method: 对近期进展进行严格分析，系统研究基于物理学的方法。

Result: 提出应构建基于物理原理和具身推理过程的智能系统。

Conclusion: 有望推动安全、可泛化和可解释的AI系统发展，还维护了相关资源库。

Abstract: The rapid advancement of embodied intelligence and world models has
intensified efforts to integrate physical laws into AI systems, yet physical
perception and symbolic physics reasoning have developed along separate
trajectories without a unified bridging framework. This work provides a
comprehensive overview of physical AI, establishing clear distinctions between
theoretical physics reasoning and applied physical understanding while
systematically examining how physics-grounded methods enhance AI's real-world
comprehension across structured symbolic reasoning, embodied systems, and
generative models. Through rigorous analysis of recent advances, we advocate
for intelligent systems that ground learning in both physical principles and
embodied reasoning processes, transcending pattern recognition toward genuine
understanding of physical laws. Our synthesis envisions next-generation world
models capable of explaining physical phenomena and predicting future states,
advancing safe, generalizable, and interpretable AI systems. We maintain a
continuously updated resource at
https://github.com/AI4Phys/Awesome-AI-for-Physics.

</details>


### [88] [LLM-Hanabi: Evaluating Multi-Agent Gameplays with Theory-of-Mind and Rationale Inference in Imperfect Information Collaboration Game](https://arxiv.org/abs/2510.04980)
*Fangzhou Liang,Tianshi Zheng,Chunkit Chan,Yauwai Yim,Yangqiu Song*

Main category: cs.AI

TL;DR: 本文引入LLM - Hanabi基准评估大语言模型在协作场景中的推理能力和心智理论，发现一阶心智理论与游戏表现相关性更强，建议未来模型优先发展一阶心智理论。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在动态协作场景中的推理能力研究不足，有效多智能体协作需要模型具备推断他人行为理由的能力。

Method: 引入LLM - Hanabi基准，使用合作游戏Hanabi，通过自动化评估系统测量游戏表现和心智理论水平。

Result: 心智理论与游戏成功显著正相关，一阶心智理论与表现的相关性比二阶更强。

Conclusion: 优先发展一阶心智理论是提升未来模型协作能力的有前景方向。

Abstract: Effective multi-agent collaboration requires agents to infer the rationale
behind others' actions, a capability rooted in Theory-of-Mind (ToM). While
recent Large Language Models (LLMs) excel at logical inference, their ability
to infer rationale in dynamic, collaborative settings remains under-explored.
This study introduces LLM-Hanabi, a novel benchmark that uses the cooperative
game Hanabi to evaluate the rationale inference and ToM of LLMs. Our framework
features an automated evaluation system that measures both game performance and
ToM proficiency. Across a range of models, we find a significant positive
correlation between ToM and in-game success. Notably, first-order ToM
(interpreting others' intent) correlates more strongly with performance than
second-order ToM (predicting others' interpretations). These findings highlight
that for effective AI collaboration, the ability to accurately interpret a
partner's rationale is more critical than higher-order reasoning. We conclude
that prioritizing first-order ToM is a promising direction for enhancing the
collaborative capabilities of future models.

</details>


### [89] [Think Then Embed: Generative Context Improves Multimodal Embedding](https://arxiv.org/abs/2510.05014)
*Xuanming Cui,Jianpeng Cheng,Hong-you Chen,Satya Narayan Shukla,Abhijeet Awasthi,Xichen Pan,Chaitanya Ahuja,Shlok Kumar Mishra,Qi Guo,Ser-Nam Lim,Aashu Singh,Xiangjun Fan*

Main category: cs.AI

TL;DR: 提出Think - Then - Embed (TTE)框架用于通用多模态嵌入，在基准测试中表现优异，还进行了相关优化和策略研究。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在通用多模态嵌入任务中仅作为编码器，忽略生成能力，在复杂指令下编码范式效果不佳。

Method: 提出TTE框架，由推理器和嵌入器组成，推理器生成推理轨迹，嵌入器基于原查询和中间推理生成表示。

Result: 在MMEB - V2基准测试中达到SOTA，微调小模型在开源模型中表现最佳，研究了将推理器和嵌入器集成的策略。

Conclusion: TTE框架能实现更细致的复杂多模态指令理解，且通过不同方式可提升性能和效率。

Abstract: There is a growing interest in Universal Multimodal Embeddings (UME), where
models are required to generate task-specific representations. While recent
studies show that Multimodal Large Language Models (MLLMs) perform well on such
tasks, they treat MLLMs solely as encoders, overlooking their generative
capacity. However, such an encoding paradigm becomes less effective as
instructions become more complex and require compositional reasoning. Inspired
by the proven effectiveness of chain-of-thought reasoning, we propose a general
Think-Then-Embed (TTE) framework for UME, composed of a reasoner and an
embedder. The reasoner MLLM first generates reasoning traces that explain
complex queries, followed by an embedder that produces representations
conditioned on both the original query and the intermediate reasoning. This
explicit reasoning step enables more nuanced understanding of complex
multimodal instructions. Our contributions are threefold. First, by leveraging
a powerful MLLM reasoner, we achieve state-of-the-art performance on the
MMEB-V2 benchmark, surpassing proprietary models trained on massive in-house
datasets. Second, to reduce the dependency on large MLLM reasoners, we finetune
a smaller MLLM reasoner using high-quality embedding-centric reasoning traces,
achieving the best performance among open-source models with a 7% absolute gain
over recently proposed models. Third, we investigate strategies for integrating
the reasoner and embedder into a unified model for improved efficiency without
sacrificing performance.

</details>


### [90] [Staircase Streaming for Low-Latency Multi-Agent Inference](https://arxiv.org/abs/2510.05059)
*Junlin Wang,Jue Wang,Zhen,Xu,Ben Athiwaratkun,Bhuwan Dhingra,Ce Zhang,James Zou*

Main category: cs.AI

TL;DR: 提出用于低延迟多智能体推理的阶梯流式方法，可减少TTFT并保持响应质量。


<details>
  <summary>Details</summary>
Motivation: 多智能体推理虽能提升响应质量，但会显著增加TTFT，对低延迟应用和用户体验造成挑战。

Method: 提出阶梯流式方法，在收到前一步骤的部分输出后就开始生成最终响应，而非等待完整中间输出。

Result: 实验表明阶梯流式可将TTFT最多降低93%，同时保持响应质量。

Conclusion: 阶梯流式方法能有效解决多智能体推理中TTFT过高的问题。

Abstract: Recent advances in large language models (LLMs) opened up new directions for
leveraging the collective expertise of multiple LLMs. These methods, such as
Mixture-of-Agents, typically employ additional inference steps to generate
intermediate outputs, which are then used to produce the final response. While
multi-agent inference can enhance response quality, it can significantly
increase the time to first token (TTFT), posing a challenge for
latency-sensitive applications and hurting user experience. To address this
issue, we propose staircase streaming for low-latency multi-agent inference.
Instead of waiting for the complete intermediate outputs from previous steps,
we begin generating the final response as soon as we receive partial outputs
from these steps. Experimental results demonstrate that staircase streaming
reduces TTFT by up to 93% while maintaining response quality.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [91] [Report of the 2025 Workshop on Next-Generation Ecosystems for Scientific Computing: Harnessing Community, Software, and AI for Cross-Disciplinary Team Science](https://arxiv.org/abs/2510.03413)
*L. C. McInnes,D. Arnold,P. Balaprakash,M. Bernhardt,B. Cerny,A. Dubey,R. Giles,D. W. Hood,M. A. Leung,V. Lopez-Marrero,P. Messina,O. B. Newton,C. Oehmen,S. M. Wild,J. Willenbring,L. Woodley,T. Baylis,D. E. Bernholdt,C. Camano,J. Cohoon,C. Ferenbaugh,S. M. Fiore,S. Gesing,D. Gomez-Zara,J. Howison,T. Islam,D. Kepczynski,C. Lively,H. Menon,B. Messer,M. Ngom,U. Paliath,M. E. Papka,I. Qualters,E. M. Raybourn,K. Riley,P. Rodriguez,D. Rouson,M. Schwalbe,S. K. Seal,O. Surer,V. Taylor,L. Wu*

Main category: cs.CE

TL;DR: 报告总结2025下一代科学计算生态系统研讨会，提出构建高性能、可持续协作的科学软件生态系统的愿景、方法与建议。


<details>
  <summary>Details</summary>
Motivation: 应对高性能计算、人工智能和科学软件交叉领域的紧迫挑战。

Method: 采用社会技术协同设计构建灵活、强大的生态系统，结合AI、HPC和软件进展与跨学科协作等新模式。

Result: 提出构建模块化可信AI科学软件系统等关键建议，确定试点项目及近期优先事项。

Conclusion: 描绘下一代科学计算生态系统愿景，推动发现、扩大访问、加强人力和加速科学进步。

Abstract: This report summarizes insights from the 2025 Workshop on Next-Generation
Ecosystems for Scientific Computing: Harnessing Community, Software, and AI for
Cross-Disciplinary Team Science, which convened more than 40 experts from
national laboratories, academia, industry, and community organizations to chart
a path toward more powerful, sustainable, and collaborative scientific software
ecosystems. To address urgent challenges at the intersection of
high-performance computing (HPC), AI, and scientific software, participants
envisioned agile, robust ecosystems built through socio-technical
co-design--the intentional integration of social and technical components as
interdependent parts of a unified strategy. This approach combines advances in
AI, HPC, and software with new models for cross-disciplinary collaboration,
training, and workforce development. Key recommendations include building
modular, trustworthy AI-enabled scientific software systems; enabling
scientific teams to integrate AI systems into their workflows while preserving
human creativity, trust, and scientific rigor; and creating innovative training
pipelines that keep pace with rapid technological change. Pilot projects were
identified as near-term catalysts, with initial priorities focused on hybrid
AI/HPC infrastructure, cross-disciplinary collaboration and pedagogy,
responsible AI guidelines, and prototyping of public-private partnerships. This
report presents a vision of next-generation ecosystems for scientific computing
where AI, software, hardware, and human expertise are interwoven to drive
discovery, expand access, strengthen the workforce, and accelerate scientific
progress.

</details>


### [92] [Lightweight and Data-Efficient MultivariateTime Series Forecasting using Residual-Stacked Gaussian (RS-GLinear) Architecture](https://arxiv.org/abs/2510.03788)
*Abukar Ali*

Main category: cs.CE

TL;DR: 本文评估高斯线性架构，提出增强版RSGL模型，实验显示其预测准确性和鲁棒性优于基线模型。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在时间序列预测的长期任务中效果不一，需更好的模型。

Method: 评估Rizvi等人提出的高斯线性架构，提出增强版RSGL模型，并研究其在金融和流行病学等领域的适用性。

Result: RSGL模型在预测准确性和鲁棒性上优于基线高斯线性和Transformer模型。

Conclusion: RSGL模型是一种更有效的时间序列预测模型，具有更广泛的适用性。

Abstract: Following the success of Transformer architectures in language modeling,
particularly their ability to capture long-range dependencies, researchers have
explored how these architectures can be adapted for time-series forecasting.
Transformer-based models have been proposed to handle both short- and long-term
dependencies when predicting future values from historical data. However,
studies such as those by Zeng et al. (2022) and Rizvi et al. (2025) have
reported mixed results in long-term forecasting tasks. In this work, we
evaluate the Gaussian-based Linear architecture introduced by Rizvi et al.
(2025) and present an enhanced version called the Residual Stacked Gaussian
Linear (RSGL) model. We also investigate the broader applicability of the RSGL
model in additional domains, including financial time series and
epidemiological data. Experimental results show that the RSGL model achieves
improved prediction accuracy and robustness compared to both the baseline
Gaussian Linear and Transformer-based models.

</details>


### [93] [Nyström-Accelerated Primal LS-SVMs: Breaking the $O(an^3)$ Complexity Bottleneck for Scalable ODEs Learning](https://arxiv.org/abs/2510.04094)
*Weikuo Wang,Yue Liao,Huan Luo*

Main category: cs.CE

TL;DR: 提出Nyström加速LS - SVMs框架解决ODEs，计算快、精度高、可扩展。


<details>
  <summary>Details</summary>
Motivation: 核基方法求解线性/非线性常微分方程（ODEs）时，计算复杂度高，随离散点增加而增加。

Method: 提出Nyström加速LS - SVMs框架，将ODEs重新表述为原始空间约束，推导基于Nyström的显式映射及其导数。

Result: 十六个基准ODEs实验显示，比经典LS - SVMs和PINNs快10 - 6000倍，精度与LS - SVMs相当，RMSE最大超PINNs 72%，可扩展到n = 10^4时间步。

Conclusion: 该工作为基于核的ODEs高效学习建立了新范式，且不显著牺牲解的精度。

Abstract: A major problem of kernel-based methods (e.g., least squares support vector
machines, LS-SVMs) for solving linear/nonlinear ordinary differential equations
(ODEs) is the prohibitive $O(an^3)$ ($a=1$ for linear ODEs and 27 for nonlinear
ODEs) part of their computational complexity with increasing temporal
discretization points $n$. We propose a novel Nystr\"om-accelerated LS-SVMs
framework that breaks this bottleneck by reformulating ODEs as primal-space
constraints. Specifically, we derive for the first time an explicit
Nystr\"om-based mapping and its derivatives from one-dimensional temporal
discretization points to a higher $m$-dimensional feature space ($1< m\le n$),
enabling the learning process to solve linear/nonlinear equation systems with
$m$-dependent complexity. Numerical experiments on sixteen benchmark ODEs
demonstrate: 1) $10-6000$ times faster computation than classical LS-SVMs and
physics-informed neural networks (PINNs), 2) comparable accuracy to LS-SVMs
($<0.13\%$ relative MAE, RMSE, and $\left \| y-\hat{y} \right \| _{\infty }
$difference) while maximum surpassing PINNs by 72\% in RMSE, and 3) scalability
to $n=10^4$ time steps with $m=50$ features. This work establishes a new
paradigm for efficient kernel-based ODEs learning without significantly
sacrificing the accuracy of the solution.

</details>


### [94] [A Complement to Neural Networks for Anisotropic Inelasticity at Finite Strains](https://arxiv.org/abs/2510.04187)
*Hagen Holthusen,Ellen Kuhl*

Main category: cs.CE

TL;DR: 提出用材料原理增强神经网络的本构建模方法，评估了该方法性能，实现开源。


<details>
  <summary>Details</summary>
Motivation: 为有限应变下捕捉各向异性和非弹性的本构建模提供补充方法。

Method: 采用双势函数，使用基于不变量的输入表示，适配输入凸神经网络，引入输入单调神经网络，采用循环液体神经网络。

Result: 该方法在材料点和结构尺度评估中表现良好，在训练范围外也有准确稳定的性能。

Conclusion: 提出的方法可行，实现代码开源。

Abstract: We propose a complement to constitutive modeling that augments neural
networks with material principles to capture anisotropy and inelasticity at
finite strains. The key element is a dual potential that governs dissipation,
consistently incorporates anisotropy, and-unlike conventional convex
formulations-satisfies the dissipation inequality without requiring convexity.
  Our neural network architecture employs invariant-based input representations
in terms of mixed elastic, inelastic and structural tensors. It adapts Input
Convex Neural Networks, and introduces Input Monotonic Neural Networks to
broaden the admissible potential class. To bypass exponential-map time
integration in the finite strain regime and stabilize the training of inelastic
materials, we employ recurrent Liquid Neural Networks.
  The approach is evaluated at both material point and structural scales. We
benchmark against recurrent models without physical constraints and validate
predictions of deformation and reaction forces for unseen boundary value
problems. In all cases, the method delivers accurate and stable performance
beyond the training regime. The neural network and finite element
implementations are available as open-source and are accessible to the public
via https://doi.org/10.5281/zenodo.17199965.

</details>


### [95] [Open-source FDTD solvers: The applicability of Elecode, gprMax and MEEP for simulations of lightning EM fields](https://arxiv.org/abs/2510.04262)
*Hannes Kohlmann,Dmitry Kuklin,Farhad Rachidi,Wolfgang Schulz*

Main category: cs.CE

TL;DR: 研究开源FDTD求解器gprMax、Elecode和MEEP对闪电电磁场传播计算的适用性，验证结果，指出参数选择问题，介绍求解器特点等并公开脚本。


<details>
  <summary>Details</summary>
Motivation: 探究开源FDTD求解器对闪电电磁场传播计算的适用性。

Method: 进行多次模拟重现文献中典型场传播场景结果，将求解器结果与完美导电和有损耗地面传播的参考场结果对比验证。

Result: 多数测试场景中求解器能以满意精度重现参考场，但参数选择不当会导致结果不准确。

Conclusion: 使用求解器时要注意空间离散化和模拟单元边界选择，同时介绍了求解器特点等并公开脚本方便社区使用。

Abstract: In this study, the open-source finite-difference time-domain (FDTD) solvers
gprMax, Elecode and MEEP are investigated for their suitability to compute
lightning electromagnetic field propagation. Several simulations are performed
to reproduce the results of typical field propagation scenarios that can be
found in the literature. The results of the presented solvers are validated
through comparison with reference field results corresponding to propagation
over perfectly conducting and lossy ground. In most of the tested scenarios,
all solvers reproduce the reference fields with satisfactory accuracy. However,
close attention must be paid to the proper choice of the spatial discretization
to avoid artificial numerical dispersion, and the application of the simulation
cell boundaries, which can cause significant impairment of the results due to
undesired reflections. Some cases of inaccurate FDTD results due to improper
choices of parameters are demonstrated. Further, the features, the performance
and limitations, and the advantages and drawbacks of the presented solvers are
highlighted. For familiarization with the solvers' programmatical interfaces to
initialize and run the simulations, the developed scripts are made available to
the community in an openly accessible repository.

</details>


### [96] [Towards Fast Option Pricing PDE Solvers Powered by PIELM](https://arxiv.org/abs/2510.04322)
*Akshay Govind Srinivasan,Anuj Jagannath Said,Sathwik Pentela,Vikas Dwivedi,Balaji Srinivasan*

Main category: cs.CE

TL;DR: 文章提出PIELMs作为PINNs的快速替代方案解决金融PDE问题，实验显示其速度快且精度相当。


<details>
  <summary>Details</summary>
Motivation: PINNs求解金融PDE时计算成本高且模型扩展性差，需要更高效方法。

Method: 引入PIELMs，用单次最小二乘法求解替代迭代优化。

Result: 在Black - Scholes和Heston - Hull - White模型上实验，PIELM精度与PINNs相当，速度快达30倍。

Conclusion: PIELMs在实时金融建模方面有潜力。

Abstract: Partial differential equation (PDE) solvers underpin modern quantitative
finance, governing option pricing and risk evaluation. Physics-Informed Neural
Networks (PINNs) have emerged as a promising approach for solving the forward
and inverse problems of partial differential equations (PDEs) using deep
learning. However they remain computationally expensive due to their iterative
gradient descent based optimization and scale poorly with increasing model
size. This paper introduces Physics-Informed Extreme Learning Machines (PIELMs)
as fast alternative to PINNs for solving both forward and inverse problems in
financial PDEs. PIELMs replace iterative optimization with a single
least-squares solve, enabling deterministic and efficient training. We
benchmark PIELM on the Black-Scholes and Heston-Hull-White models for forward
pricing and demonstrate its capability in inverse model calibration to recover
volatility and interest rate parameters from noisy data. From experiments we
observe that PIELM achieve accuracy comparable to PINNs while being up to
$30\times$ faster, highlighting their potential for real-time financial
modeling.

</details>


### [97] [Deep vs. Shallow: Benchmarking Physics-Informed Neural Architectures on the Biharmonic Equation](https://arxiv.org/abs/2510.04490)
*Akshay Govind Srinivasan,Vikas Dwivedi,Balaji Srinivasan*

Main category: cs.CE

TL;DR: 本文系统地对求解高阶偏微分方程的快速PINN变体RBF - PIELM进行基准测试，结果显示其训练速度比PINN快，参数更少，但仍落后于成熟的基于网格的求解器。


<details>
  <summary>Details</summary>
Motivation: 经典基于网格的偏微分方程求解器在高阶算子和复杂几何形状上有困难，新的物理信息神经网络（PINNs）计算密集且精度较低，因此需要评估新的变体。

Method: 用RBF - PIELM（具有径向基激活的极限学习机）替代PINNs中耗时的梯度下降，采用单次最小二乘法求解，在四阶双调和方程上使用两个基准进行测试。

Result: RBF - PIELM比PINNs训练速度快达350倍，参数少超10倍且精度相当，但在高度振荡解上精度下降。

Conclusion: RBF - PIELM虽优于PINNs，但仍落后于成熟的基于网格的求解器，实际部署存在挑战。

Abstract: Partial differential equation (PDE) solvers are fundamental to engineering
simulation. Classical mesh-based approaches (finite difference/volume/element)
are fast and accurate on high-quality meshes but struggle with higher-order
operators and complex, hard-to-mesh geometries. Recently developed
physics-informed neural networks (PINNs) and their variants are mesh-free and
flexible, yet compute-intensive and often less accurate. This paper
systematically benchmarks RBF-PIELM, a rapid PINN variant-an extreme learning
machine with radial-basis activations-for higher-order PDEs. RBF-PIELM replaces
PINNs' time-consuming gradient descent with a single-shot least-squares solve.
We test RBF-PIELM on the fourth-order biharmonic equation using two benchmarks:
lid-driven cavity flow (streamfunction formulation) and a manufactured
oscillatory solution. Our results show up to $(350\times)$ faster training than
PINNs and over $(10\times)$ fewer parameters for comparable solution accuracy.
Despite surpassing PINNs, RBF-PIELM still lags mature mesh-based solvers and
its accuracy degrades on highly oscillatory solutions, highlighting remaining
challenges for practical deployment.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [98] [Is it Bigger than a Breadbox: Efficient Cardinality Estimation for Real World Workloads](https://arxiv.org/abs/2510.03386)
*Zixuan Yi,Sami Abu-el-Haija,Yawen Wang,Teja Vemparala,Yannis Chronis,Yu Gan,Michael Burrows,Carsten Binnig,Bryan Perozzi,Ryan Marcus,Fatma Ozcan*

Main category: cs.DB

TL;DR: 本文提出一种在线学习简单回归器的方法估计查询基数，开销小，在误差指标上与SOTA方法竞争，改进PostgreSQL后效果好。


<details>
  <summary>Details</summary>
Motivation: 现有数据库查询基数估计方法存在启发式方法误差随查询复杂度增加，学习型估计器操作复杂难以实际应用的问题。

Method: 认识到查询工作负载中存在高度重复的子查询模式，在线学习许多简单回归器，每个回归器对应一个模式，可通过子查询图结构的哈希值随机访问。

Result: 该方法开销可忽略不计，在误差指标上与SOTA学习型方法竞争；改进PostgreSQL后，相比传统方法在准确性和运行时间上有显著提升，与其他学习型基数估计器相比大幅降低操作成本。模拟JOB - lite工作负载时，执行速度加快7.5分钟（>30%），在线学习仅产生37秒开销。

Conclusion: 该方法是帕累托最优前沿上最实用和高效的解决方案。

Abstract: DB engines produce efficient query execution plans by relying on cost models.
Practical implementations estimate cardinality of queries using heuristics,
with magic numbers tuned to improve average performance on benchmarks.
Empirically, estimation error significantly grows with query complexity.
Alternatively, learning-based estimators offer improved accuracy, but add
operational complexity preventing their adoption in-practice. Recognizing that
query workloads contain highly repetitive subquery patterns, we learn many
simple regressors online, each localized to a pattern. The regressor
corresponding to a pattern can be randomly-accessed using hash of graph
structure of the subquery. Our method has negligible overhead and competes with
SoTA learning-based approaches on error metrics. Further, amending PostgreSQL
with our method achieves notable accuracy and runtime improvements over
traditional methods and drastically reduces operational costs compared to other
learned cardinality estimators, thereby offering the most practical and
efficient solution on the Pareto frontier. Concretely, simulating JOB-lite
workload on IMDb speeds-up execution by 7.5 minutes (>30%) while incurring only
37 seconds overhead for online learning.

</details>


### [99] [Dual Pruning and Sorting-Free Overestimation for Average-Utility Sequential Pattern Mining](https://arxiv.org/abs/2510.04014)
*Kai Cao,Yucong Duan,Wensheng Gan*

Main category: cs.DB

TL;DR: 本文提出HAUSP - PG算法用于高平均效用序列模式挖掘，采用双策略实现双剪枝，计算平均效用上界无需排序，实验显示性能良好。


<details>
  <summary>Details</summary>
Motivation: 高平均效用序列模式挖掘（HAUSPM）比高效用序列模式挖掘（HUSPM）更公平有价值，且在长序列应用中剪枝策略对效率影响大，需高效算法。

Method: 提出HAUSP - PG算法，采用两个互补策略分别处理模式前缀和剩余序列实现双剪枝，计算平均效用上界无需排序。

Result: 在真实和合成数据集上实验，该算法取得了令人满意的性能。

Conclusion: 所提出的HAUSP - PG算法在高平均效用序列模式挖掘中是有效的，能减少计算时间和内存消耗。

Abstract: In a quantitative sequential database, numerous efficient algorithms have
been developed for high-utility sequential pattern mining (HUSPM). HUSPM
establishes a relationship between frequency and significance in the real world
and reflects more crucial information than frequent pattern mining. However,
high average-utility sequential pattern mining (HAUSPM) is deemed fairer and
more valuable than HUSPM. It provides a reasonable measure for longer patterns
by considering their length. In contrast to scenarios in retail business
analysis, some pattern mining applications, such as cybersecurity or artificial
intelligence (AI), often involve much longer sequences. Consequently, pruning
strategies can exert a more pronounced impact on efficiency. This paper
proposes a novel algorithm named HAUSP-PG, which adopts two complementary
strategies to independently process pattern prefixes and remaining sequences,
thereby achieving a dual pruning effect. Additionally, the proposed method
calculates average utility upper bounds without requiring item sorting,
significantly reducing computational time and memory consumption compared to
alternative approaches. Through experiments conducted on both real-life and
synthetic datasets, we demonstrate that the proposed algorithm could achieve
satisfactory performance.

</details>


### [100] [Ambidextrous Degree Sequence Bounds for Pessimistic Cardinality Estimation](https://arxiv.org/abs/2510.04249)
*Yu-Ting Lin,Hsin-Po Wang*

Main category: cs.DB

TL;DR: 本文聚焦大型数据库系统中连接查询基数上界估计问题，介绍现有框架并提出新的“ambidextrous”界，新界更优。


<details>
  <summary>Details</summary>
Motivation: 改进现有悲观基数估计中步骤3的上界估计方法，使估计更准确。

Method: 提出“ambidextrous”界，通过计数“claw pairs”进行上界估计。

Result: 新界理论上不宽松，实证更紧，如在`com - Youtube`数据集上，新界估计更接近实际基数。

Conclusion: 新的“ambidextrous”界在悲观基数估计中表现优于旧界。

Abstract: In a large database system, upper-bounding the cardinality of a join query is
a crucial task called $\textit{pessimistic cardinality estimation}$. Recently,
Abo Khamis, Nakos, Olteanu, and Suciu unified related works into the following
dexterous framework. Step 1: Let $(X_1, \dotsc, X_n)$ be a random row of the
join, equating $H(X_1, \dotsc, X_n)$ to the log of the join cardinality. Step
2: Upper-bound $H(X_1, \dotsc, X_n)$ using Shannon-type inequalities such as
$H(X, Y, Z) \le H(X) + H(Y|X) + H(Z|Y)$. Step 3: Upper-bound $H(X_i) + p H(X_j
| X_i)$ using the $p$-norm of the degree sequence of the underlying graph of a
relation.
  While old bound in step 3 count "claws $\in$" in the underlying graph, we
proposed $\textit{ambidextrous}$ bounds that count "claw pairs
${\ni}\!{-}\!{\in}$". The new bounds are provably not looser and empirically
tighter: they overestimate by $x^{3/4}$ times when the old bounds overestimate
by $x$ times. An example is counting friend triples in the
$\texttt{com-Youtube}$ dataset, the best dexterous bound is $1.2 \cdot 10^9$,
the best ambidextrous bound is $5.1 \cdot 10^8$, and the actual cardinality is
$1.8 \cdot 10^7$.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [101] [Cosmological Hydrodynamics at Exascale: A Trillion-Particle Leap in Capability](https://arxiv.org/abs/2510.03557)
*Nicholas Frontiere,J. D. Emberson,Michael Buehlmann,Esteban M. Rangel,Salman Habib,Katrin Heitmann,Patricia Larsen,Vitali Morozov,Adrian Pope,Claude-André Faucher-Giguère,Antigoni Georgiadou,Damien Lebrun-Grandié,Andrey Prokopenko*

Main category: cs.DC

TL;DR: 为匹配下一代天空调查要求，需高仿真宇宙模拟。介绍CRK - HACC代码，其实现超大规模模拟并取得高性能。


<details>
  <summary>Details</summary>
Motivation: 解决宇宙学中最基本问题，需能匹配下一代天空调查的规模、保真度和物理复杂性的模拟。

Method: 使用分离尺度技术、GPU驻留树求解器、原位分析管道和多层级I/O，通过CRK - HACC代码进行模拟。

Result: 执行Frontier - E模拟，规模比以往大一个数量级，实现513.1 PFLOPs峰值性能，每秒处理466亿个粒子，一周多写入超100 PB数据。

Conclusion: CRK - HACC代码能满足现代宇宙学调查的极端可扩展性要求，助力大规模宇宙模拟。

Abstract: Resolving the most fundamental questions in cosmology requires simulations
that match the scale, fidelity, and physical complexity demanded by
next-generation sky surveys. To achieve the realism needed for this critical
scientific partnership, detailed gas dynamics, along with a host of
astrophysical effects, must be treated self-consistently with gravity for
end-to-end modeling of structure formation. As an important step on this
roadmap, exascale computing enables simulations that span survey-scale volumes
while incorporating key subgrid processes that shape complex cosmic structures.
We present results from CRK-HACC, a cosmological hydrodynamics code built for
the extreme scalability requirements set by modern cosmological surveys. Using
separation-of-scale techniques, GPU-resident tree solvers, in situ analysis
pipelines, and multi-tiered I/O, CRK-HACC executed Frontier-E: a four trillion
particle full-sky simulation, over an order of magnitude larger than previous
efforts. The run achieved 513.1 PFLOPs peak performance, processing 46.6
billion particles per second and writing more than 100 PB of data in just over
one week of runtime.

</details>


### [102] [Datacenter Energy Optimized Power Profiles](https://arxiv.org/abs/2510.03872)
*Sreedhar Narayanaswamy,Pratikkumar Dilipkumar Patel,Ian Karlin,Apoorv Gupta,Sudhir Saripalli,Janey Guo*

Main category: cs.DC

TL;DR: 本文介绍NVIDIA随Blackwell B200发布的新软件功能datacenter power profiles，可提升能效和性能，实现工作负载感知优化，在一期实现节能和提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 提升数据中心的能源效率和性能。

Method: 利用硬件和软件创新进行智能电源管理，结合HPC和AI工作负载领域知识，提供粗粒度用户控制，生成工作负载感知优化配方。

Result: 在一期Blackwell实现中，节能达15%，关键应用性能维持在97%以上，在功率受限设施中整体吞吐量提升达13%。

Conclusion: datacenter power profiles功能能在功率约束下有效提升计算吞吐量和能源效率。

Abstract: This paper presents datacenter power profiles, a new NVIDIA software feature
released with Blackwell B200, aimed at improving energy efficiency and/or
performance. The initial feature provides coarse-grain user control for HPC and
AI workloads leveraging hardware and software innovations for intelligent power
management and domain knowledge of HPC and AI workloads. The resulting
workload-aware optimization recipes maximize computational throughput while
operating within strict facility power constraints. The phase-1 Blackwell
implementation achieves up to 15% energy savings while maintaining performance
levels above 97% for critical applications, enabling an overall throughput
increase of up to 13% in a power-constrained facility.
  KEYWORDS GPU power management, energy efficiency, power profile, HPC
optimization, Max-Q, Blackwell architecture

</details>


### [103] [Toward Co-adapting Machine Learning Job Shape and Cluster Topology](https://arxiv.org/abs/2510.03891)
*Shawn Shuoshuo Chen,Daiyaan Arfeen,Minlan Yu,Peter Steenkiste,Srinivasan Seshan*

Main category: cs.DC

TL;DR: 提出RFold方法，可兼顾减少网络争用和提高集群利用率，在模拟集群测试效果好


<details>
  <summary>Details</summary>
Motivation: 解决多租户环形拓扑集群中分布式机器学习作业资源分配时，最小化网络争用和最大化集群利用率之间的矛盾，现有调度器无法兼顾两个目标

Method: 提出RFold方法，在运行时调整作业形状和集群拓扑，结合识别同态作业形状和重新配置光电路交换拓扑两种技术

Result: 在4096节点环形集群模拟器上初步评估显示，RFold比现有方法绝对集群利用率提高57%，作业完成时间最多减少11倍

Conclusion: 可以同时实现减少网络争用和提高集群利用率的目标

Abstract: Allocating resources to distributed machine learning jobs in multi-tenant
torus-topology clusters must meet each job's specific placement and
communication requirements, which are typically described using shapes. There
is an inherent tension between minimizing network contention and maximizing
cluster utilization when placing various-shaped jobs. While existing schedulers
typically optimize for one objective at the expense of the other, we
demonstrate that both can be achieved simultaneously.
  Our proposed approach, RFold, adapts both job shapes and the underlying
cluster topology at runtime. This is accomplished by combining two techniques:
(1) identifying homomorphic job shapes that support the jobs communication
needs, and (2) reconfiguring the optical circuit switch-enabled topology to
support more diverse job shapes. Preliminary evaluation performed on a
4096-node torus cluster simulator indicates that RFold can improve absolute
cluster utilization by 57% and reduce job completion time by up to 11x relative
to existing methods

</details>


### [104] [Towards Carbon-Aware Container Orchestration: Predicting Workload Energy Consumption with Federated Learning](https://arxiv.org/abs/2510.03970)
*Zainab Saad,Jialin Yang,Henry Leung,Steve Drew*

Main category: cs.DC

TL;DR: 论文提出基于联邦学习的能耗预测方法，扩展Kepler框架，实验显示比集中式基线MAE低11.7%，解决数据隐私与能耗预测效率权衡问题。


<details>
  <summary>Details</summary>
Motivation: 大规模数据中心依赖增加碳足迹，现有容器编排平台方法依赖集中式机器学习模型有隐私问题且泛化能力差，需兼顾数据隐私与能耗预测效率的方法。

Method: 提出基于联邦学习的能耗预测方法，扩展Kepler框架，使用Flower的FedXgbBagging聚合策略跨分布式客户端协作训练XGBoost模型。

Result: 在SPECPower基准数据集上实验，基于联邦学习的方法比集中式基线的平均绝对误差低11.7%。

Conclusion: 该工作解决了先前系统中数据隐私与能耗预测效率的权衡问题，为企业提供不损害运营隐私的可持续云计算途径。

Abstract: The growing reliance on large-scale data centers to run resource-intensive
workloads has significantly increased the global carbon footprint, underscoring
the need for sustainable computing solutions. While container orchestration
platforms like Kubernetes help optimize workload scheduling to reduce carbon
emissions, existing methods often depend on centralized machine learning models
that raise privacy concerns and struggle to generalize across diverse
environments. In this paper, we propose a federated learning approach for
energy consumption prediction that preserves data privacy by keeping sensitive
operational data within individual enterprises. By extending the Kubernetes
Efficient Power Level Exporter (Kepler), our framework trains XGBoost models
collaboratively across distributed clients using Flower's FedXgbBagging
aggregation using a bagging strategy, eliminating the need for centralized data
sharing. Experimental results on the SPECPower benchmark dataset show that our
FL-based approach achieves 11.7 percent lower Mean Absolute Error compared to a
centralized baseline. This work addresses the unresolved trade-off between data
privacy and energy prediction efficiency in prior systems such as Kepler and
CASPER and offers enterprises a viable pathway toward sustainable cloud
computing without compromising operational privacy.

</details>


### [105] [From Patchwork to Network: A Comprehensive Framework for Demand Analysis and Fleet Optimization of Urban Air Mobility](https://arxiv.org/abs/2510.04186)
*Xuan Jiang,Xuanyu Zhou,Yibo Zhao,Shangqing Cao,Jinhua Zhao,Mark Hansen,Raja Sengupta*

Main category: cs.DC

TL;DR: 本文提出利用现有区域机场和优化的异构机队构建UAM网络，引入LPSim框架进行多GPU并行计算，应用于旧金山湾区案例，能节省旅行时间，但系统成功依赖与地面交通的无缝集成和动态调度。


<details>
  <summary>Details</summary>
Motivation: 解决城市空中交通（UAM）实际实施中面临的基础设施成本高和运营复杂的挑战。

Method: 构建利用现有区域机场和优化异构机队的UAM网络，引入LPSim大规模并行模拟框架进行多GPU计算，扩展均衡搜索算法预测需求和确定最佳机队组成。

Result: 应用于旧金山湾区案例，该UAM模型能为23万个选定行程节省超20分钟的旅行时间。

Conclusion: UAM系统的整体成功关键在于与地面交通的无缝集成和动态调度。

Abstract: Urban Air Mobility (UAM) presents a transformative vision for metropolitan
transportation, but its practical implementation is hindered by substantial
infrastructure costs and operational complexities. We address these challenges
by modeling a UAM network that leverages existing regional airports and
operates with an optimized, heterogeneous fleet of aircraft. We introduce
LPSim, a Large-Scale Parallel Simulation framework that utilizes multi-GPU
computing to co-optimize UAM demand, fleet operations, and ground
transportation interactions simultaneously. Our equilibrium search algorithm is
extended to accurately forecast demand and determine the most efficient fleet
composition. Applied to a case study of the San Francisco Bay Area, our results
demonstrate that this UAM model can yield over 20 minutes' travel time savings
for 230,000 selected trips. However, the analysis also reveals that system-wide
success is critically dependent on seamless integration with ground access and
dynamic scheduling.

</details>


### [106] [Beyond Canonical Rounds: Communication Abstractions for Optimal Byzantine Resilience](https://arxiv.org/abs/2510.04310)
*Hagit Attiya,Itay Flam,Jennifer L. Welch*

Main category: cs.DC

TL;DR: 研究异步拜占庭容错通信抽象，指出经典模式在3f < n ≤ 5f时的局限性，证明gather抽象可实现最优弹性常量时间解决方案并支持模块化约简。


<details>
  <summary>Details</summary>
Motivation: 研究具有最优故障弹性的异步拜占庭容错通信抽象，解决相关关键任务。

Method: 分析经典的规范异步轮和通信封闭层模式，证明其局限性，提出gather抽象并进行相关算法设计。

Result: 经典模式在3f < n ≤ 5f时有局限性，gather抽象有常量时间最优弹性解决方案，实现连通共识的最优弹性算法。

Conclusion: 基于轮的抽象虽便于分析，但掩盖了拜占庭容错算法的真实复杂性，gather等更丰富的通信模式是模块化、最优弹性设计的更好基础。

Abstract: We study communication abstractions for asynchronous Byzantine fault
tolerance with optimal failure resilience, where $n > 3f$. Two classic patterns
-- canonical asynchronous rounds and communication-closed layers -- have long
been considered as general frameworks for designing distributed algorithms,
making asynchronous executions appear synchronous and enabling modular
reasoning.
  We show that these patterns are inherently limited in the critical resilience
regime $3f < n \le 5f$. Several key tasks -- such as approximate and crusader
agreement, reliable broadcast and gather -- cannot be solved by bounded-round
canonical-round algorithms, and are unsolvable if communication closure is
imposed. These results explain the historical difficulty of achieving
optimal-resilience algorithms within round-based frameworks.
  On the positive side, we show that the gather abstraction admits
constant-time solutions with optimal resilience ($n > 3f$), and supports
modular reductions. Specifically, we present the first optimally-resilient
algorithm for connected consensus by reducing it to gather.
  Our results demonstrate that while round-based abstractions are analytically
convenient, they obscure the true complexity of Byzantine fault-tolerant
algorithms. Richer communication patterns such as gather provide a better
foundation for modular, optimal-resilience design.

</details>


### [107] [Next-Generation Event-Driven Architectures: Performance, Scalability, and Intelligent Orchestration Across Messaging Frameworks](https://arxiv.org/abs/2510.04404)
*Jahidul Arafat,Fariha Tasmin,Sanjaya Poudel,Ahsan Habib Tareq*

Main category: cs.DC

TL;DR: 本文提出首个综合基准测试框架评估12个消息系统，引入AIEO，揭示各系统权衡，AIEO有性能提升，贡献方法与决策指南。


<details>
  <summary>Details</summary>
Motivation: 现代分布式系统对低延迟、容错事件处理有需求，缺乏对多个消息系统在标准条件下的统一比较研究。

Method: 提出综合基准测试框架评估12个消息系统，引入AI - Enhanced Event Orchestration（AIEO），采用机器学习预测扩展、强化学习动态资源分配和多目标优化。

Result: 揭示各消息系统权衡，如Kafka吞吐量高但需专业操作，Pulsar性能平衡，无服务器方案弹性好但延迟高；AIEO平均降低34%延迟、提升28%资源利用率、优化42%成本。

Conclusion: 贡献标准化基准测试方法、开源智能编排和基于证据的决策指南，为下一代分布式系统设计奠定基础。

Abstract: Modern distributed systems demand low-latency, fault-tolerant event
processing that exceeds traditional messaging architecture limits. While
frameworks including Apache Kafka, RabbitMQ, Apache Pulsar, NATS JetStream, and
serverless event buses have matured significantly, no unified comparative study
evaluates them holistically under standardized conditions. This paper presents
the first comprehensive benchmarking framework evaluating 12 messaging systems
across three representative workloads: e-commerce transactions, IoT telemetry
ingestion, and AI inference pipelines. We introduce AIEO (AI-Enhanced Event
Orchestration), employing machine learning-driven predictive scaling,
reinforcement learning for dynamic resource allocation, and multi-objective
optimization. Our evaluation reveals fundamental trade-offs: Apache Kafka
achieves peak throughput (1.2M messages/sec, 18ms p95 latency) but requires
substantial operational expertise; Apache Pulsar provides balanced performance
(950K messages/sec, 22ms p95) with superior multi-tenancy; serverless solutions
offer elastic scaling for variable workloads despite higher baseline latency
(80-120ms p95). AIEO demonstrates 34\% average latency reduction, 28\% resource
utilization improvement, and 42% cost optimization across all platforms. We
contribute standardized benchmarking methodologies, open-source intelligent
orchestration, and evidence-based decision guidelines. The evaluation
encompasses 2,400+ experimental configurations with rigorous statistical
analysis, providing comprehensive performance characterization and establishing
foundations for next-generation distributed system design.

</details>


### [108] [The R(1)W(1) Communication Model for Self-Stabilizing Distributed Algorithms](https://arxiv.org/abs/2510.04644)
*Hirotsugu Kakugawa,Sayaka Kamei,Masahiro Shibata,Fukuhito Ooshita*

Main category: cs.DC

TL;DR: 本文提出R(1)W(1)模型，给出该模型下部分问题的自稳定分布式算法，并提出示例变换器用于同步消息传递模型模拟R(1)W(1)模型算法。


<details>
  <summary>Details</summary>
Motivation: 自稳定方法对现代含大量组件的分布式系统应对瞬态故障很有用，需提出新模型和算法。

Method: 提出R(1)W(1)模型，在该模型下设计自稳定分布式算法，基于随机距离二局部互斥提出示例变换器。

Result: 提出R(1)W(1)模型，给出该模型下最大匹配、最小k-支配集和最大k-独立集问题的自稳定分布式算法，提出示例变换器。

Conclusion: 新提出的R(1)W(1)模型及算法为分布式系统应对瞬态故障提供了新的解决方案，示例变换器可辅助模型应用。

Abstract: Self-stabilization is a versatile methodology in the design of fault-tolerant
distributed algorithms for transient faults. A self-stabilizing system
automatically recovers from any kind and any finite number of transient faults.
This property is specifically useful in modern distributed systems with a large
number of components. In this paper, we propose a new communication and
execution model named the R(1)W(1) model in which each process can read and
write its own and neighbors' local variables in a single step. We propose
self-stabilizing distributed algorithms in the R(1)W(1) model for the problems
of maximal matching, minimal k-dominating set and maximal k-dependent set.
Finally, we propose an example transformer, based on randomized distance-two
local mutual exclusion, to simulate algorithms designed for the R(1)W(1) model
in the synchronous message passing model with synchronized clocks.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [109] [A Subquadratic Two-Party Communication Protocol for Minimum Cost Flow](https://arxiv.org/abs/2510.03427)
*Hossein Gholizadeh,Yonggang Jiang*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we discuss the maximum flow problem in the two-party
communication model, where two parties, each holding a subset of edges on a
common vertex set, aim to compute the maximum flow of the union graph with
minimal communication. We show that this can be solved with
$\tilde{O}(n^{1.5})$ bits of communication, improving upon the trivial
$\tilde{O}(n^2)$ bound.
  To achieve this, we derive two additional, more general results:
  1. We present a randomized algorithm for linear programs with two-sided
constraints that requires $\tilde{O}(n^{1.5}k)$ bits of communication when each
constraint has at most $k$ non-zeros. This result improves upon the prior work
by [Ghadiri, Lee, Padmanabhan, Swartworth, Woodruff, Ye, STOC'24], which
achieves a complexity of $\tilde{O}(n^2)$ bits for LPs with one-sided
constraints. Upon more precise analysis, their algorithm can reach a bit
complexity of $\tilde{O}(n^{1.5} + nk)$ for one-sided constraint LPs.
Nevertheless, for sparse matrices, our approach matches this complexity while
extending the scope to two-sided constraints.
  2. Leveraging this result, we demonstrate that the minimum cost flow problem,
as a special case of solving linear programs with two-sided constraints and as
a general case of maximum flow problem, can also be solved with a communication
complexity of $\tilde{O}(n^{1.5})$ bits.
  These results are achieved by adapting an interior-point method (IPM)-based
algorithm for solving LPs with two-sided constraints in the sequential setting
by [van den Brand, Lee, Liu, Saranurak, Sidford, Song, Wang, STOC'21] to the
two-party communication model. This adaptation utilizes techniques developed by
[Ghadiri, Lee, Padmanabhan, Swartworth, Woodruff, Ye, STOC'24] for distributed
convex optimization.

</details>


### [110] [A Dynamic Programming Approach to Evader Pathfinding in Static Pursuit Scenarios](https://arxiv.org/abs/2510.04050)
*Sukanya Samanta,Manohar Reddy*

Main category: cs.DS

TL;DR: 本文研究城市网络中躲避者在面对静态防御者部署时的最优路径规划问题，提出DPERO算法，实验证明其能找到生存概率更高的路径。


<details>
  <summary>Details</summary>
Motivation: 现有博弈论模型计算复杂度高，不适用于实时应用，因此研究躲避者在静态防御者部署下的最优路径规划子问题。

Method: 提出DPERO算法，将环境建模为带概率风险的图，通过对数变换将乘法生存目标转化为加法成本函数，将任务转化为可通过值迭代解决的最短路径问题。

Result: 在模拟网格网络上的实验表明，DPERO找到的路径生存概率比简单最短路径基线显著更高。

Conclusion: DPERO算法是进行脆弱性分析和战略规划的实用工具。

Abstract: The interdiction of escaping adversaries in urban networks is a critical
security challenge. State-of-the-art game-theoretic models, such as the Escape
Interdiction Game (EIG), provide comprehensive frameworks but assume a highly
dynamic interaction and entail significant computational complexity, which can
be prohibitive for real-time applications. This paper investigates a crucial
sub-problem: an evader's optimal pathfinding calculus when faced with a static
or pre-determined defender deployment. We propose the Dynamic Programming for
Evader Route Optimization (DPERO) algorithm, which models the environment as a
graph with probabilistic risks at various nodes. By transforming the
multiplicative survival objective into an additive cost function using
logarithms, we frame the task as a shortest path problem solvable with value
iteration. This approach allows for the efficient computation of a path that
optimally balances safety and distance. Experimental results on simulated grid
networks demonstrate that DPERO identifies routes with significantly higher
survival probabilities compared to naive shortest-path baselines, validating
its efficacy as a practical tool for vulnerability analysis and strategic
planning.

</details>


### [111] [Streaming Max-Cut in General Metrics](https://arxiv.org/abs/2510.04435)
*Shaofeng H. -C. Jiang,Pan Peng,Haoze Wang*

Main category: cs.DS

TL;DR: 研究一般度量空间中Max - Cut问题的流复杂度，给出滑动窗口流的(1 + ε)近似算法，证明动态流设置下的多项式空间下界。


<details>
  <summary>Details</summary>
Motivation: 研究Max - Cut问题在一般度量空间中带距离预言机的流复杂度。

Method: 滑动窗口算法基于平滑直方图框架，建立度量Max - Cut的平滑性界，开发插入流算法并采用新的度量水库采样技术。

Result: 得到滑动窗口流的(1 + ε)近似算法，证明动态流设置下任意poly(n)近似的多项式空间下界。

Conclusion: 该滑动窗口算法是Max - Cut问题首个此类算法，与欧几里得空间插入算法有相似误差 - 空间权衡，且与欧几里得情形有分离。

Abstract: Max-Cut is a fundamental combinatorial optimization problem that has been
studied in various computational settings. In this work, we initiate the study
of its streaming complexity in general metric spaces with access to distance
oracles. We give a $(1 + \epsilon)$-approximation algorithm for estimating the
Max-Cut value sliding-window streams using only poly-logarithmic space. This is
the first sliding-window algorithm for Max-Cut even in Euclidean spaces, and it
achieves a similar error-space tradeoff as the state-of-the-art insertion-only
algorithms in Euclidean settings [Chen, Jiang, Krauthgamer, STOC'23], but
without relying on Euclidean structures. In sharp contrast, we prove a
polynomial-space lower bound for any $\mathrm{poly}(n)$-approximation in the
dynamic streaming setting. This yields a separation from the Euclidean case,
where the polylogarithmic-space $(1+\epsilon)$-approximation extends to dynamic
streams.
  On the technical side, our sliding-window algorithm builds on the smooth
histogram framework of [Braverman and Ostrovsky, SICOMP'10]. To make this
framework applicable, we establish the first smoothness bound for metric
Max-Cut. Moreover, we develop a streaming algorithm for metric Max-Cut in
insertion-only streams, whose key ingredient is a new metric reservoir sampling
technique.

</details>


### [112] [Online Multiple Resource Allocation Problems with Departures via the Primal-Dual Approach](https://arxiv.org/abs/2510.04737)
*Yusuf Amidu,Khaled Elbassioni,Adriana F. Gabor*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper we propose primal-dual algorithms for different variants of the
online resource allocation problem with departures. In the basic variant,
requests (items) arrive over time to a set of resources (knapsacks) and upon
arrival, the duration of time a request may occupy a resource, the demand and
reward if the request can be granted, become known. %We assume that the
duration of stay of a request may depend on the resource. %and that resources
may have different capacity sizes. The goal of the algorithm is to decide
whether to accept/reject a request upon arrival and to which resource to
allocate it such that the reward obtained over time is maximized. Under some
mild assumptions, we show that the proposed primal-dual algorithm achieves a
competitive ratio of $O\big(\log(\bar\theta^{\max}\cdot\bar d^{\max})\big)$,
where $\bar \theta^{\max}$ is the maximum value density fluctuation ratio and
$\bar d^{\max}$ is the maximum duration fluctuation ratio. We prove similar
results for two other variants, namely, one with an additional load balancing
constraint, and the multi-dimensional variant where an admitted request
consumes capacity on multiple resources. Our results show that the primal-dual
approach offers a simple, unified framework for obtaining competitive ratios
comparable to those previously obtained via threshold policies known for these
problems. Additionally, we show that this framework allows us to incorporate
additional constraints, such as load-balancing constraints, without sacrificing
the competitive ratio.

</details>


### [113] [A Polynomial Space Lower Bound for Diameter Estimation in Dynamic Streams](https://arxiv.org/abs/2510.04918)
*Sanjeev Khanna,Ashwin Padaki,Krish Singal,Erik Waingarten*

Main category: cs.DS

TL;DR: 研究动态流模型下估计任意度量空间子集直径的空间复杂度，给出插入流简单算法及动态流上下界。


<details>
  <summary>Details</summary>
Motivation: 研究在动态（旋转门）流模型下，以尽可能少的空间估计任意度量空间子集直径到指定近似因子。

Method: 证明动态流模型中常数因子近似直径需多项式空间，利用动态流算法与尺度不变函数线性草图、图的最小秩之间的联系，还给出接近匹配的上界。

Result: 插入流中简单的 $O(\log n)$ 空间算法实现 2 - 近似；动态流模型中 $c$ - 近似直径需 $n^{\Omega(1/c)}$ 空间，且有 $n^{O(1/c)}$ 空间的上界。

Conclusion: 明确了动态流模型下估计直径的空间复杂度上下界。

Abstract: We study the space complexity of estimating the diameter of a subset of
points in an arbitrary metric space in the dynamic (turnstile) streaming model.
The input is given as a stream of updates to a frequency vector $x \in
\mathbb{Z}_{\geq 0}^n$, where the support of $x$ defines a multiset of points
in a fixed metric space $M = ([n], \mathsf{d})$. The goal is to estimate the
diameter of this multiset, defined as $\max\{\mathsf{d}(i,j) : x_i, x_j > 0\}$,
to a specified approximation factor while using as little space as possible.
  In insertion-only streams, a simple $O(\log n)$-space algorithm achieves a
2-approximation. In sharp contrast to this, we show that in the dynamic
streaming model, any algorithm achieving a constant-factor approximation to
diameter requires polynomial space. Specifically, we prove that a
$c$-approximation to the diameter requires $n^{\Omega(1/c)}$ space. Our lower
bound relies on two conceptual contributions: (1) a new connection between
dynamic streaming algorithms and linear sketches for {\em scale-invariant}
functions, a class that includes diameter estimation, and (2) a connection
between linear sketches for diameter and the {\em minrank} of graphs, a notion
previously studied in index coding. We complement our lower bound with a nearly
matching upper bound, which gives a $c$-approximation to the diameter in
general metrics using $n^{O(1/c)}$ space.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [114] [Downside Risk-Aware Equilibria for Strategic Decision-Making](https://arxiv.org/abs/2510.03446)
*Oliver Slumbers,Benjamin Patrick Evans,Sumitra Ganesh,Leo Ardon*

Main category: cs.GT

TL;DR: 传统博弈论对风险看法有限，新方法虽考虑奖励方差但包含上下行，本文提出基于下偏矩的下行风险感知均衡（DRAE），在多游戏中验证其适用性并证明均衡存在与最优性。


<details>
  <summary>Details</summary>
Motivation: 传统博弈论对风险看法有限，现有方差方法衡量上下行风险，而实际很多领域只关注下行风险，需新的解决方案。

Method: 提出基于下偏矩的下行风险感知均衡（DRAE），该均衡限制下行风险，不限制上行风险，还能模拟高阶风险偏好。

Result: 在多个游戏中成功找到平衡下行风险和预期回报的均衡。

Conclusion: 证明了下行风险感知均衡（DRAE）的存在性和最优性。

Abstract: Game theory has traditionally had a relatively limited view of risk based on
how a player's expected reward is impacted by the uncertainty of the actions of
other players. Recently, a new game-theoretic approach provides a more holistic
view of risk also considering the reward-variance. However, these
variance-based approaches measure variance of the reward on both the upside and
downside. In many domains, such as finance, downside risk only is of key
importance, as this represents the potential losses associated with a decision.
In contrast, large upside "risk" (e.g. profits) are not an issue. To address
this restrictive view of risk, we propose a novel solution concept, downside
risk aware equilibria (DRAE) based on lower partial moments. DRAE restricts
downside risk, while placing no restrictions on upside risk, and additionally,
models higher-order risk preferences. We demonstrate the applicability of DRAE
on several games, successfully finding equilibria which balance downside risk
with expected reward, and prove the existence and optimality of this
equilibria.

</details>


### [115] [On the $O(1/T)$ Convergence of Alternating Gradient Descent-Ascent in Bilinear Games](https://arxiv.org/abs/2510.03855)
*Tianlong Nan,Shuvomoy Das Gupta,Garud Iyengar,Christian Kroer*

Main category: cs.GT

TL;DR: 研究双人零和博弈中AltGDA算法，给出不同情况下收敛速率，开发PEP框架优化步长和收敛速率。


<details>
  <summary>Details</summary>
Motivation: 交替方法在博弈学习中实用，但理论理解有限且结果多限于无约束情况。

Method: 理论分析，开发性能估计编程（PEP）框架。

Result: 有内点纳什均衡时，小常数步长下AltGDA有O(1/T)遍历收敛速率；无内点均衡时，有与博弈常数无关的O(1/T)局部收敛速率；PEP结果表明有限时间T内AltGDA可达O(1/T)收敛速率，而同步版本限于O(1/√T)。

Conclusion: 在约束情况下，交替的AltGDA算法比同步版本的GDA算法有更好的收敛表现。

Abstract: We study the alternating gradient descent-ascent (AltGDA) algorithm in
two-player zero-sum games. Alternating methods, where players take turns to
update their strategies, have long been recognized as simple and practical
approaches for learning in games, exhibiting much better numerical performance
than their simultaneous counterparts. However, our theoretical understanding of
alternating algorithms remains limited, and results are mostly restricted to
the unconstrained setting. We show that for two-player zero-sum games that
admit an interior Nash equilibrium, AltGDA converges at an $O(1/T)$ ergodic
convergence rate when employing a small constant stepsize. This is the first
result showing that alternation improves over the simultaneous counterpart of
GDA in the constrained setting. For games without an interior equilibrium, we
show an $O(1/T)$ local convergence rate with a constant stepsize that is
independent of any game-specific constants. In a more general setting, we
develop a performance estimation programming (PEP) framework to jointly
optimize the AltGDA stepsize along with its worst-case convergence rate. The
PEP results indicate that AltGDA may achieve an $O(1/T)$ convergence rate for a
finite horizon $T$, whereas its simultaneous counterpart appears limited to an
$O(1/\sqrt{T})$ rate.

</details>


### [116] [Robust Optimality of Bundling Goods Beyond Finite Variance](https://arxiv.org/abs/2510.04343)
*Tim S. G. van Eck,Pieter Kleer,Johan S. H. van Leeuwaarden*

Main category: cs.GT

TL;DR: 针对多商品销售，构建分布鲁棒框架，在已知均值和平均绝对偏差下研究捆绑销售机制，发现捆绑销售在一定 MAD 值范围仍最优，且对博弈顺序无差异，最优捆绑价格具通用性。


<details>
  <summary>Details</summary>
Motivation: 在卖家对商品价值分布仅有有限知识时，寻找最大化收益的销售机制。

Method: 构建卖家和自然之间的两人博弈的分布鲁棒框架。

Result: 在一定 MAD 值范围捆绑销售仍最优，但卖家收益严格小于均值；博弈顺序对结果无影响；最优捆绑价格在多种目标下有效。

Conclusion: 捆绑销售在已知均值和 MAD 时是有效的销售机制，最优捆绑价格具有普遍性。

Abstract: When selling many goods with independent valuations, we develop a
distributionally robust framework, consisting of a two-player game between
seller and nature. The seller has only limited knowledge about the value
distribution. The seller selects a revenue-maximizing mechanism, after which
nature chooses a revenue-minimizing distribution from all distributions that
comply with the limited knowledge. When the seller knows the mean and variance
of valuations, bundling is known to be an asymptotically optimal deterministic
mechanism, achieving a normalized revenue close to the mean. Moving beyond this
variance assumption, we assume knowledge of the mean absolute deviation (MAD),
accommodating more dispersion and heavy-tailed valuations with infinite
variance. We show for a large range of MAD values that bundling remains
optimal, but the seller can only guarantee a revenue strictly smaller than the
mean. Another noteworthy finding is indifference to the order of play, as both
the max-min and min-max versions of the problem yield identical values. This
contrasts with deterministic mechanisms and the separate sale of goods, where
the order of play significantly impacts outcomes. We further underscore the
universality of the optimal bundling price by demonstrating its efficacy in
optimizing not only absolute revenue but also the absolute regret and ratio
objective among all bundling prices

</details>


### [117] [Scale-Invariant Regret Matching and Online Learning with Optimal Convergence: Bridging Theory and Practice in Zero-Sum Games](https://arxiv.org/abs/2510.04407)
*Brian Hu Zhang,Ioannis Anagnostides,Tuomas Sandholm*

Main category: cs.GT

TL;DR: 本文提出IREG - PRM+算法，缩小零和博弈求解理论与实践差距，有较好收敛性且表现与PRM+相当。


<details>
  <summary>Details</summary>
Motivation: 零和博弈求解中一阶方法的理论和实践存在巨大差距，现有方法有收敛慢的问题。

Method: 提出新的无参数IREG - PRM+算法，通过设计使后悔向量范数非减，类比自适应学习率的乐观梯度下降。

Result: IREG - PRM+实现了$T^{-1/2}$最佳迭代和$T^{-1}$平均迭代收敛保证，在基准游戏上与PRM+表现相当。

Conclusion: IREG - PRM+缩小了理论与实践差距，解释了后悔匹配族算法相对标准优化技术的有效性。

Abstract: A considerable chasm has been looming for decades between theory and practice
in zero-sum game solving through first-order methods. Although a convergence
rate of $T^{-1}$ has long been established since Nemirovski's mirror-prox
algorithm and Nesterov's excessive gap technique in the early 2000s, the most
effective paradigm in practice is *counterfactual regret minimization*, which
is based on *regret matching* and its modern variants. In particular, the state
of the art across most benchmarks is *predictive* regret matching$^+$
(PRM$^+$), in conjunction with non-uniform averaging. Yet, such algorithms can
exhibit slower $\Omega(T^{-1/2})$ convergence even in self-play.
  In this paper, we close the gap between theory and practice. We propose a new
scale-invariant and parameter-free variant of PRM$^+$, which we call
IREG-PRM$^+$. We show that it achieves $T^{-1/2}$ best-iterate and $T^{-1}$
(i.e., optimal) average-iterate convergence guarantees, while also being on par
with PRM$^+$ on benchmark games. From a technical standpoint, we draw an
analogy between IREG-PRM$^+$ and optimistic gradient descent with *adaptive*
learning rate. The basic flaw of PRM$^+$ is that the ($\ell_2$-)norm of the
regret vector -- which can be thought of as the inverse of the learning rate --
can decrease. By contrast, we design IREG-PRM$^+$ so as to maintain the
invariance that the norm of the regret vector is nondecreasing. This enables us
to derive an RVU-type bound for IREG-PRM$^+$, the first such property that does
not rely on introducing additional hyperparameters to enforce smoothness.
  Furthermore, we find that IREG-PRM$^+$ performs on par with an adaptive
version of optimistic gradient descent that we introduce whose learning rate
depends on the misprediction error, demystifying the effectiveness of the
regret matching family *vis-a-vis* more standard optimization techniques.

</details>


### [118] [Bin Packing and Covering: Pushing the Frontier on the Maximin Share Fairness](https://arxiv.org/abs/2510.04425)
*Bo Li,Ankang Sun,Zunyu Wang,Yu Zhou*

Main category: cs.GT

TL;DR: 研究基本公平分配问题，用MMS准则评估公平性，考虑基数和序数两种近似，为所有感兴趣的模型提供常数近似算法。


<details>
  <summary>Details</summary>
Motivation: 该问题有实际应用，且是研究群体公平性的自然框架。

Method: 考虑基数和序数两种近似，基数近似放宽箱子的打包或覆盖要求，序数近似放宽打包或覆盖的箱子数量。

Result: 为所有感兴趣的模型提供了常数近似算法。

Conclusion: 针对该公平分配问题，通过两种近似方法可得到常数近似算法。

Abstract: We study a fundamental fair allocation problem, where the agent's value is
determined by the number of bins either used to pack or cover the items
allocated to them. Fairness is evaluated using the maximin share (MMS)
criterion. This problem is not only motivated by practical applications, but
also serves as a natural framework for studying group fairness. As MMS is not
always satisfiable, we consider two types of approximations: cardinal and
ordinal. For cardinal approximation, we relax the requirements of being packed
or covered for a bin, and for ordinal approximation, we relax the number of
bins that are packed or covered. For all models of interest, we provide
constant approximation algorithms.

</details>


### [119] [Fairness in Repeated Matching: A Maximin Perspective](https://arxiv.org/abs/2510.04624)
*Eugene Lim,Tzeh Yuan Neoh,Nicholas Teh*

Main category: cs.GT

TL;DR: 研究多轮顺序决策匹配模型，指出找最优解计算难，给出近似和固定参数算法，识别特殊可高效求解情况，还给出匹配特征。


<details>
  <summary>Details</summary>
Motivation: 确定多轮匹配序列以最大化最不利代理效用，研究找最优结果的计算挑战。

Method: 提供近似算法、固定参数可处理算法，识别特殊情况，建立帕累托最优/最大匹配的特征。

Result: 发现找（随时）最优结果的问题通常计算上难以处理。

Conclusion: 能通过特定算法和识别特殊情况有效解决该问题，建立的匹配特征有独立研究价值。

Abstract: We study a sequential decision-making model where a set of items is
repeatedly matched to the same set of agents over multiple rounds. The
objective is to determine a sequence of matchings that either maximizes the
utility of the least advantaged agent at the end of all rounds (optimal) or at
the end of every individual round (anytime optimal). We investigate the
computational challenges associated with finding (anytime) optimal outcomes and
demonstrate that these problems are generally computationally intractable.
However, we provide approximation algorithms, fixed-parameter tractable
algorithms, and identify several special cases whereby the problem(s) can be
solved efficiently. Along the way, we also establish characterizations of
Pareto-optimal/maximum matchings, which may be of independent interest to works
in matching theory and house allocation.

</details>


### [120] [A Fixed Point Framework for the Existence of EFX Allocations](https://arxiv.org/abs/2510.04915)
*S. Rasoul Etesami*

Main category: cs.GT

TL;DR: 本文通过将无嫉妒到任意商品分配（EFX）问题与不动点框架联系起来，建立新结果，还提出新方法并指出与DC编程的等价性可助力计算。


<details>
  <summary>Details</summary>
Motivation: 研究线性估值下EFX分配的存在性问题。

Method: 使用随机舍入将离散EFX约束扩展到连续空间，将问题转化为无约束凸差（DC）规划和不动点问题，提出微扰连续映射。

Result: 证明了提出的DC规划最优目标值非正与连续向量映射存在不动点的等价性，微扰连续映射总有不动点。

Conclusion: 提供通过不动点定理确定EFX分配存在性的新方法，与DC编程的等价性可利用非线性优化工具计算此类分配。

Abstract: We consider the problem of the existence of an envy-free allocation up to any
good (EFX) for linear valuations and establish new results by connecting this
problem to a fixed point framework. Specifically, we first use randomized
rounding to extend the discrete EFX constraints into a continuous space and
show that an EFX allocation exists if and only if the optimal value of the
continuously extended objective function is nonpositive. In particular, we
demonstrate that this optimization problem can be formulated as an
unconstrained difference of convex (DC) program, which can be further
simplified to the minimization of a piecewise linear concave function over a
polytope. Leveraging this connection, we show that the proposed DC program has
a nonpositive optimal objective value if and only if a well-defined continuous
vector map admits a fixed point. Crucially, we prove that the reformulated
fixed point problem satisfies all the conditions of Brouwer's fixed point
theorem, except that self-containedness is violated by an arbitrarily small
positive constant. To address this, we propose a slightly perturbed continuous
map that always admits a fixed point. This fixed point serves as a proxy for
the fixed point (if it exists) of the original map, and hence for an EFX
allocation through an appropriate transformation. Our results offer a new
approach to establishing the existence of EFX allocations through fixed point
theorems. Moreover, the equivalence with DC programming enables a more
efficient and systematic method for computing such allocations (if one exists)
using tools from nonlinear optimization. Our findings bridge the discrete
problem of finding an EFX allocation with two continuous frameworks: solving an
unconstrained DC program and identifying a fixed point of a continuous vector
map.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [121] [Evaluating High-Resolution Piano Sustain Pedal Depth Estimation with Musically Informed Metrics](https://arxiv.org/abs/2510.03750)
*Hanwen Zhang,Kun Fang,Ziyu Wang,Ichiro Fujinaga*

Main category: cs.IR

TL;DR: 传统帧级指标评估钢琴踏板深度估计不完整，本文提出新评估框架，应用对比不同模型，结果显示含MIDI信息的模型表现好，新框架能捕捉传统指标无法发现的改进。


<details>
  <summary>Details</summary>
Motivation: 传统帧级指标评估钢琴踏板深度估计任务时不完整，会忽略重要音乐特征，需要更具可解释性和音乐意义的评估方法。

Method: 提出评估框架，结合标准帧级指标、动作级评估和手势级分析，应用框架对比音频基线模型、含MIDI信息模型和二值训练模型。

Result: 含MIDI信息的模型在动作和手势层面显著优于其他模型，帧级提升较小。

Conclusion: 新框架能捕捉传统指标无法发现的音乐相关改进，是评估踏板深度估计模型更实用有效的方法。

Abstract: Evaluation for continuous piano pedal depth estimation tasks remains
incomplete when relying only on conventional frame-level metrics, which
overlook musically important features such as direction-change boundaries and
pedal curve contours. To provide more interpretable and musically meaningful
insights, we propose an evaluation framework that augments standard frame-level
metrics with an action-level assessment measuring direction and timing using
segments of press/hold/release states and a gesture-level analysis that
evaluates contour similarity of each press-release cycle. We apply this
framework to compare an audio-only baseline with two variants: one
incorporating symbolic information from MIDI, and another trained in a
binary-valued setting, all within a unified architecture. Results show that the
MIDI-informed model significantly outperforms the others at action and gesture
levels, despite modest frame-level gains. These findings demonstrate that our
framework captures musically relevant improvements indiscernible by traditional
metrics, offering a more practical and effective approach to evaluating pedal
depth estimation models.

</details>


### [122] [Investigating LLM Variability in Personalized Conversational Information Retrieval](https://arxiv.org/abs/2510.03795)
*Simon Lupart,Daniël van Dijk,Eric Langezaal,Ian van Dort,Mohammad Aliannejadi*

Main category: cs.IR

TL;DR: 本文针对个性化对话信息检索（CIR）进行可重复性研究，评估多种模型在新数据集上表现，发现人工选择PTKB能提升检索性能，指出评估时需多轮评估和报告方差。


<details>
  <summary>Details</summary>
Motivation: 原研究基于单轮实验，存在输出可变性和可重复性问题，本文旨在严格复现并拓展其工作，关注大语言模型输出可变性和模型泛化性。

Method: 将原方法应用于新的TREC iKAT 2024数据集，评估包括Llama、Qwen - 7B、GPT - 4o - mini等多种模型。

Result: 人工选择的PTKB能持续提升检索性能，大语言模型选择方法不如手动选择可靠；iKAT数据集比CAsT数据集可变性更高；召回导向指标方差低于精确导向指标。

Conclusion: 评估大语言模型的CIR系统时需要多轮评估和报告方差，本研究有助于个性化CIR更稳健和可泛化实践。

Abstract: Personalized Conversational Information Retrieval (CIR) has seen rapid
progress in recent years, driven by the development of Large Language Models
(LLMs). Personalized CIR aims to enhance document retrieval by leveraging
user-specific information, such as preferences, knowledge, or constraints, to
tailor responses to individual needs. A key resource for this task is the TREC
iKAT 2023 dataset, designed to evaluate personalization in CIR pipelines.
Building on this resource, Mo et al. explored several strategies for
incorporating Personal Textual Knowledge Bases (PTKB) into LLM-based query
reformulation. Their findings suggested that personalization from PTKBs could
be detrimental and that human annotations were often noisy. However, these
conclusions were based on single-run experiments using the GPT-3.5 Turbo model,
raising concerns about output variability and repeatability. In this
reproducibility study, we rigorously reproduce and extend their work, focusing
on LLM output variability and model generalization. We apply the original
methods to the new TREC iKAT 2024 dataset and evaluate a diverse range of
models, including Llama (1B-70B), Qwen-7B, GPT-4o-mini. Our results show that
human-selected PTKBs consistently enhance retrieval performance, while
LLM-based selection methods do not reliably outperform manual choices. We
further compare variance across datasets and observe higher variability on iKAT
than on CAsT, highlighting the challenges of evaluating personalized CIR.
Notably, recall-oriented metrics exhibit lower variance than precision-oriented
ones, a critical insight for first-stage retrievers. Finally, we underscore the
need for multi-run evaluations and variance reporting when assessing LLM-based
CIR systems. By broadening evaluation across models, datasets, and metrics, our
study contributes to more robust and generalizable practices for personalized
CIR.

</details>


### [123] [Beyond Static Evaluation: Rethinking the Assessment of Personalized Agent Adaptability in Information Retrieval](https://arxiv.org/abs/2510.03984)
*Kirandeep Kaur,Preetam Prabhu Srikar Dammu,Hideo Joho,Chirag Shah*

Main category: cs.IR

TL;DR: 现有评估方法无法反映用户需求随时间的变化，本文提出从交互感知、动态评估的角度重新思考自适应个性化评估，并通过电商搜索案例研究进行说明，为理解和评估个性化奠定概念基础。


<details>
  <summary>Details</summary>
Motivation: 当前大多数评估方法是静态的，无法反映用户需求的动态变化，难以评估智能体在动态、长期交互中对个体的适应能力。

Method: 提出围绕三个核心组件的概念框架，包括基于角色的用户模拟、结构化的偏好提取协议和适应感知的评估机制，并通过电商搜索的案例研究进行说明。

Result: 通过案例研究展示了该评估框架的应用。

Conclusion: 为理解和评估个性化作为一项持续的、以用户为中心的工作奠定了概念基础。

Abstract: Personalized AI agents are becoming central to modern information retrieval,
yet most evaluation methodologies remain static, relying on fixed benchmarks
and one-off metrics that fail to reflect how users' needs evolve over time.
These limitations hinder our ability to assess whether agents can meaningfully
adapt to individuals across dynamic, longitudinal interactions. In this
perspective paper, we propose a conceptual lens for rethinking evaluation in
adaptive personalization, shifting the focus from static performance snapshots
to interaction-aware, evolving assessments. We organize this lens around three
core components: (1) persona-based user simulation with temporally evolving
preference models; (2) structured elicitation protocols inspired by reference
interviews to extract preferences in context; and (3) adaptation-aware
evaluation mechanisms that measure how agent behavior improves across sessions
and tasks. While recent works have embraced LLM-driven user simulation, we
situate this practice within a broader paradigm for evaluating agents over
time. To illustrate our ideas, we conduct a case study in e-commerce search
using the PersonalWAB dataset. Beyond presenting a framework, our work lays a
conceptual foundation for understanding and evaluating personalization as a
continuous, user-centric endeavor.

</details>


### [124] [Visual Lifelog Retrieval through Captioning-Enhanced Interpretation](https://arxiv.org/abs/2510.04010)
*Yu-Fei Shih,An-Zi Yen,Hen-Hsen Huang,Hsin-Hsi Chen*

Main category: cs.IR

TL;DR: 提出CIVIL检索系统用于基于文本查询从用户视觉生活日志中提取特定图像，介绍三种解释方法，实验表明方法有效，还构建文本数据集。


<details>
  <summary>Details</summary>
Motivation: 人们回忆过去经历细节有困难，生活日志检索成为重要应用，需方法辅助记忆回忆。

Method: 提出CIVIL检索系统，先为视觉生活日志生成字幕，再用文本嵌入模型将字幕和用户查询投影到共享向量空间，引入单字幕、集体字幕和合并字幕三种方法。

Result: 方法能有效描述第一人称视觉图像，提升生活日志检索效果，构建了将视觉生活日志转换为字幕的文本数据集。

Conclusion: 所提方法能有效用于生活日志检索，可重构个人生活经历。

Abstract: People often struggle to remember specific details of past experiences, which
can lead to the need to revisit these memories. Consequently, lifelog retrieval
has emerged as a crucial application. Various studies have explored methods to
facilitate rapid access to personal lifelogs for memory recall assistance. In
this paper, we propose a Captioning-Integrated Visual Lifelog (CIVIL) Retrieval
System for extracting specific images from a user's visual lifelog based on
textual queries. Unlike traditional embedding-based methods, our system first
generates captions for visual lifelogs and then utilizes a text embedding model
to project both the captions and user queries into a shared vector space.
Visual lifelogs, captured through wearable cameras, provide a first-person
viewpoint, necessitating the interpretation of the activities of the individual
behind the camera rather than merely describing the scene. To address this, we
introduce three distinct approaches: the single caption method, the collective
caption method, and the merged caption method, each designed to interpret the
life experiences of lifeloggers. Experimental results show that our method
effectively describes first-person visual images, enhancing the outcomes of
lifelog retrieval. Furthermore, we construct a textual dataset that converts
visual lifelogs into captions, thereby reconstructing personal life
experiences.

</details>


### [125] [The LCLStream Ecosystem for Multi-Institutional Dataset Exploration](https://arxiv.org/abs/2510.04012)
*David Rogers,Valerio Mariani,Cong Wang,Ryan Coffee,Wilko Kroeger,Murali Shankar,Hans Thorsten Schwander,Tom Beck,Frédéric Poitevin,Jana Thayer*

Main category: cs.IR

TL;DR: 介绍新端到端实验数据流框架LCLStreamer，结合云微服务与HPC批处理模型，为DOE IRI做贡献，满足X射线科学数据分析社区需求，已实现未来实验关键范式。


<details>
  <summary>Details</summary>
Motivation: 支持AI训练、X射线飞行时间分析等新应用类型，满足X射线科学数据分析社区对高速数据流的需求。

Method: 将云微服务与传统HPC批处理模型相结合，创建灵活的、API驱动的数据请求服务。

Result: LCLStreamer框架已完成原型设计并实现了几个对下一代实验至关重要的新范式。

Conclusion: 该框架为DOE IRI做出独特贡献，能满足相关社区对高速数据流的需求。

Abstract: We describe a new end-to-end experimental data streaming framework designed
from the ground up to support new types of applications -- AI training,
extremely high-rate X-ray time-of-flight analysis, crystal structure
determination with distributed processing, and custom data science applications
and visualizers yet to be created. Throughout, we use design choices merging
cloud microservices with traditional HPC batch execution models for security
and flexibility. This project makes a unique contribution to the DOE Integrated
Research Infrastructure (IRI) landscape. By creating a flexible, API-driven
data request service, we address a significant need for high-speed data
streaming sources for the X-ray science data analysis community. With the
combination of data request API, mutual authentication web security framework,
job queue system, high-rate data buffer, and complementary nature to facility
infrastructure, the LCLStreamer framework has prototyped and implemented
several new paradigms critical for future generation experiments.

</details>


### [126] [RLRF: Competitive Search Agent Design via Reinforcement Learning from Ranker Feedback](https://arxiv.org/abs/2510.04096)
*Tommy Mordo,Sagie Dekel,Omer Madmon,Moshe Tennenholtz,Oren Kurland*

Main category: cs.IR

TL;DR: 介绍使用排名竞争偏好数据集训练大语言模型的RLRF框架，该框架优于先前方法且有泛化和适应能力，证明强化学习在竞争搜索有潜力。


<details>
  <summary>Details</summary>
Motivation: 在文档发布者利用大语言模型生成和修改竞争内容的背景下，提出有效方法优化内容排名。

Method: 提出RLRF框架，用不依赖人类创作数据的方法生成数据集训练大语言模型，考虑竞争策略。

Result: 所提代理显著优于先前方法，在未训练的排名函数上有效，能适应策略对手。

Conclusion: 强化学习在竞争搜索中有巨大潜力。

Abstract: Competitive search is a setting where document publishers modify them to
improve their ranking in response to a query. Recently, publishers have
increasingly leveraged LLMs to generate and modify competitive content. We
introduce Reinforcement Learning from Ranker Feedback (RLRF), a framework that
trains LLMs using preference datasets derived from ranking competitions. The
goal of a publisher (LLM-based) agent is to optimize content for improved
ranking while accounting for the strategies of competing agents. We generate
the datasets using approaches that do not rely on human-authored data. We show
that our proposed agents consistently and substantially outperform previously
suggested approaches for LLM-based competitive document modification. We
further show that our agents are effective with ranking functions they were not
trained for (i.e., out of distribution) and they adapt to strategic opponents.
These findings provide support to the significant potential of using
reinforcement learning in competitive search.

</details>


### [127] [Learning-Based Hashing for ANN Search: Foundations and Early Advances](https://arxiv.org/abs/2510.04127)
*Sean Moran*

Main category: cs.IR

TL;DR: 本文对早期基于学习的哈希方法进行了基础综述，旨在介绍用于近似最近邻搜索的基于学习的哈希的概念基础。


<details>
  <summary>Details</summary>
Motivation: 近似最近邻搜索是信息检索中的基本问题，哈希方法提供了有效解决方案，过去二十年有大量学习哈希的工作，本文旨在介绍相关概念基础。

Method: 对监督、无监督和半监督方法进行综述，研究投影函数设计和量化策略，还考察了多位和多阈值模型的扩展以及跨模态检索的早期进展。

Result: 梳理了早期基于学习的哈希方法的核心思想、投影函数设计和量化策略等内容。

Conclusion: 通过将早期模型置于历史背景中，帮助读者结构化理解该领域的原理、权衡和开放挑战。

Abstract: Approximate Nearest Neighbour (ANN) search is a fundamental problem in
information retrieval, underpinning large-scale applications in computer
vision, natural language processing, and cross-modal search. Hashing-based
methods provide an efficient solution by mapping high-dimensional data into
compact binary codes that enable fast similarity computations in Hamming space.
Over the past two decades, a substantial body of work has explored learning to
hash, where projection and quantisation functions are optimised from data
rather than chosen at random.
  This article offers a foundational survey of early learning-based hashing
methods, with an emphasis on the core ideas that shaped the field. We review
supervised, unsupervised, and semi-supervised approaches, highlighting how
projection functions are designed to generate meaningful embeddings and how
quantisation strategies convert these embeddings into binary codes. We also
examine extensions to multi-bit and multi-threshold models, as well as early
advances in cross-modal retrieval.
  Rather than providing an exhaustive account of the most recent methods, our
goal is to introduce the conceptual foundations of learning-based hashing for
ANN search. By situating these early models in their historical context, we aim
to equip readers with a structured understanding of the principles, trade-offs,
and open challenges that continue to inform current research in this area.

</details>


### [128] [Empowering Denoising Sequential Recommendation with Large Language Model Embeddings](https://arxiv.org/abs/2510.04239)
*Tongzhou Wu,Yuhao Wang,Maolin Wang,Chi Zhang,Xiangyu Zhao*

Main category: cs.IR

TL;DR: 提出IADSR框架减少序列推荐中噪声影响，结合协作和语义信息，实验验证其有效性和兼容性。


<details>
  <summary>Details</summary>
Motivation: 现有序列推荐模型受噪声影响性能不佳，单纯依靠协作信息去噪会有过度去噪问题，尤其是冷物品。

Method: 提出IADSR框架，分两阶段，先从传统模型和大语言模型获取物品的协作和语义嵌入，再对齐嵌入并基于长期和短期兴趣识别噪声。

Result: 在四个公开数据集上的大量实验验证了框架的有效性。

Conclusion: IADSR框架有效且能与不同序列推荐系统兼容。

Abstract: Sequential recommendation aims to capture user preferences by modeling
sequential patterns in user-item interactions. However, these models are often
influenced by noise such as accidental interactions, leading to suboptimal
performance. Therefore, to reduce the effect of noise, some works propose
explicitly identifying and removing noisy items. However, we find that simply
relying on collaborative information may result in an over-denoising problem,
especially for cold items. To overcome these limitations, we propose a novel
framework: Interest Alignment for Denoising Sequential Recommendation (IADSR)
which integrates both collaborative and semantic information. Specifically,
IADSR is comprised of two stages: in the first stage, we obtain the
collaborative and semantic embeddings of each item from a traditional
sequential recommendation model and an LLM, respectively. In the second stage,
we align the collaborative and semantic embeddings and then identify noise in
the interaction sequence based on long-term and short-term interests captured
in the collaborative and semantic modalities. Our extensive experiments on four
public datasets validate the effectiveness of the proposed framework and its
compatibility with different sequential recommendation systems.

</details>


### [129] [Causality-aware Graph Aggregation Weight Estimator for Popularity Debiasing in Top-K Recommendation](https://arxiv.org/abs/2510.04502)
*Yue Que,Yingyi Zhang,Xiangyu Zhao,Chen Ma*

Main category: cs.IR

TL;DR: 本文提出CAGED方法缓解图推荐系统中的流行度偏差，通过因果推理建模聚合过程，设计编码器 - 解码器架构和动量更新策略，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图基去偏方法在缓解流行度偏差上不足，未深入图聚合合理性且未平衡训练与去偏过程。

Method: 揭示图聚合是因果推理中后门调整的特殊形式，设计CAGED架构优化交互似然的证据下界来近似无偏聚合权重，还设计动量更新策略。

Result: 在三个数据集上的广泛实验表明CAGED优于现有图基去偏方法。

Conclusion: 所提CAGED能有效缓解图推荐系统中的流行度偏差。

Abstract: Graph-based recommender systems leverage neighborhood aggregation to generate
node representations, which is highly sensitive to popularity bias, resulting
in an echo effect during information propagation. Existing graph-based
debiasing solutions refine the aggregation process with attempts such as edge
reconstruction or weight adjustment. However, these methods remain inadequate
in fully alleviating popularity bias. Specifically, this is because 1) they
provide no insights into graph aggregation rationality, thus lacking an
optimality guarantee; 2) they fail to well balance the training and debiasing
process, which undermines the effectiveness. In this paper, we propose a novel
approach to mitigate popularity bias through rational modeling of the graph
aggregation process. We reveal that graph aggregation is a special form of
backdoor adjustment in causal inference, where the aggregation weight
corresponds to the historical interaction likelihood distribution. Based on
this insight, we devise an encoder-decoder architecture, namely Causality-aware
Graph Aggregation Weight Estimator for Debiasing (CAGED), to approximate the
unbiased aggregation weight by optimizing the evidence lower bound of the
interaction likelihood. In order to enhance the debiasing effectiveness during
early training stages, we further design a momentum update strategy that
incrementally refines the aggregation weight matrix. Extensive experiments on
three datasets demonstrate that CAGED outperforms existing graph-based
debiasing methods. Our implementation is available at
https://github.com/QueYork/CAGED.

</details>


### [130] [MARCO: A Cooperative Knowledge Transfer Framework for Personalized Cross-domain Recommendations](https://arxiv.org/abs/2510.04508)
*Lili Xie,Yi Zhang,Ruihong Qiu,Jiajun Liu,Sen Wang*

Main category: cs.IR

TL;DR: 提出多智能体强化学习跨域推荐框架MARCO，解决现有方法负迁移问题，实验显示其性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的跨域推荐方法采用单智能体框架，存在因源域贡献不一致和分布差异导致的负迁移问题。

Method: 提出MARCO框架，利用合作式多智能体强化学习估计各源域贡献，引入基于熵的动作多样性惩罚增强策略表达和稳定训练。

Result: 在四个基准数据集上的实验表明，MARCO性能优于现有方法，具有鲁棒性和强泛化能力。

Conclusion: MARCO能有效解决现有跨域推荐方法的负迁移问题，性能良好。

Abstract: Recommender systems frequently encounter data sparsity issues, particularly
when addressing cold-start scenarios involving new users or items. Multi-source
cross-domain recommendation (CDR) addresses these challenges by transferring
valuable knowledge from multiple source domains to enhance recommendations in a
target domain. However, existing reinforcement learning (RL)-based CDR methods
typically rely on a single-agent framework, leading to negative transfer issues
caused by inconsistent domain contributions and inherent distributional
discrepancies among source domains. To overcome these limitations, MARCO, a
Multi-Agent Reinforcement Learning-based Cross-Domain recommendation framework,
is proposed. It leverages cooperative multi-agent reinforcement learning, where
each agent is dedicated to estimating the contribution from an individual
source domain, effectively managing credit assignment and mitigating negative
transfer. In addition, an entropy-based action diversity penalty is introduced
to enhance policy expressiveness and stabilize training by encouraging diverse
agents' joint actions. Extensive experiments across four benchmark datasets
demonstrate MARCO's superior performance over state-of-the-art methods,
highlighting its robustness and strong generalization capabilities. The code is
at https://github.com/xiewilliams/MARCO.

</details>


### [131] [Topic-Specific Classifiers are Better Relevance Judges than Prompted LLMs](https://arxiv.org/abs/2510.04633)
*Lukas Gienapp,Martin Potthast,Harrisen Scells,Eugene Yang*

Main category: cs.IR

TL;DR: 提出训练特定主题相关性分类器解决未判断文档问题，少量人工判断即可提升模型可比性，代码等公开。


<details>
  <summary>Details</summary>
Motivation: 解决信息检索中测试集可复用的关键障碍——未判断文档问题，现有LLM作为相关性判断方法存在循环问题。

Method: 在单个评估者对单个主题池的判断上，通过独立LoRA权重调整微调monoT5，使其与评估者对该主题的相关性概念对齐。

Result: 分类器相关性判断得到的系统排名与真实系统排名的Spearmans' $ho$ 相关性 >0.95，每个主题仅128个初始人工判断就能提升模型可比性，比现有LLM判断方法更可靠。

Conclusion: 特定主题相关性分类器是解决未判断文档问题的轻量级直接方法，同时保持人工判断作为检索评估的黄金标准。

Abstract: The unjudged document problem, where pooled test collections have incomplete
relevance judgments for evaluating new retrieval systems, is a key obstacle to
the reusability of test collections in information retrieval. While the de
facto standard to deal with the problem is to treat unjudged documents as
non-relevant, many alternatives have been proposed, including the use of large
language models (LLMs) as a relevance judge (LLM-as-a-judge). However, this has
been criticized as circular, since the same LLM can be used as a judge and as a
ranker at the same time. We propose to train topic-specific relevance
classifiers instead: By finetuning monoT5 with independent LoRA weight
adaptation on the judgments of a single assessor for a single topic's pool, we
align it to that assessor's notion of relevance for the topic. The system
rankings obtained through our classifier's relevance judgments achieve a
Spearmans' $\rho$ correlation of $>0.95$ with ground truth system rankings. As
little as 128 initial human judgments per topic suffice to improve the
comparability of models, compared to treating unjudged documents as
non-relevant, while achieving more reliability than existing LLM-as-a-judge
approaches. Topic-specific relevance classifiers thus are a lightweight and
straightforward way to tackle the unjudged document problem, while maintaining
human judgments as the gold standard for retrieval evaluation. Code, models,
and data are made openly available.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [132] [MetaMP: Seamless Metadata Enrichment and AI Application Framework for Enhanced Membrane Protein Visualization and Analysis](https://arxiv.org/abs/2510.04776)
*Ebenezer Awotoro,Chisom Ezekannagha,Florian Schwarz,Johannes Tauscher,Dominik Heider,Katharina Ladewig,Christel Le Bon,Karine Moncoq,Bruno Miroux,Georges Hattab*

Main category: cs.LG

TL;DR: 本文介绍MetaMP框架统一膜蛋白数据库，用机器学习分类，经评估有效，还展示AI在膜蛋白研究的三个应用，验证效果好。


<details>
  <summary>Details</summary>
Motivation: 膜蛋白结构复杂，数据存在缺失、不一致等问题，需要改进数据库集成。

Method: 提出MetaMP框架，在网络应用中统一膜蛋白数据库，用机器学习进行分类。

Result: MetaMP在不同难度任务中有效，解决77%的数据差异，98%准确预测新膜蛋白类别，超越专家策展。

Conclusion: MetaMP是协调现有知识、推动AI驱动膜蛋白结构探索的必要资源。

Abstract: Structural biology has made significant progress in determining membrane
proteins, leading to a remarkable increase in the number of available
structures in dedicated databases. The inherent complexity of membrane protein
structures, coupled with challenges such as missing data, inconsistencies, and
computational barriers from disparate sources, underscores the need for
improved database integration. To address this gap, we present MetaMP, a
framework that unifies membrane-protein databases within a web application and
uses machine learning for classification. MetaMP improves data quality by
enriching metadata, offering a user-friendly interface, and providing eight
interactive views for streamlined exploration. MetaMP was effective across
tasks of varying difficulty, demonstrating advantages across different levels
without compromising speed or accuracy, according to user evaluations.
Moreover, MetaMP supports essential functions such as structure classification
and outlier detection.
  We present three practical applications of Artificial Intelligence (AI) in
membrane protein research: predicting transmembrane segments, reconciling
legacy databases, and classifying structures with explainable AI support. In a
validation focused on statistics, MetaMP resolved 77% of data discrepancies and
accurately predicted the class of newly identified membrane proteins 98% of the
time and overtook expert curation. Altogether, MetaMP is a much-needed resource
that harmonizes current knowledge and empowers AI-driven exploration of
membrane-protein architecture.

</details>


### [133] [PARS: Low-Latency LLM Serving via Pairwise Learning-to-Rank](https://arxiv.org/abs/2510.03243)
*Yiheng Tao,Yihe Zhang,Matthew T. Dearing,Xin Wang,Yuping Fan,Zhiling Lan*

Main category: cs.LG

TL;DR: 本文提出 PARS 调度器提升大语言模型推理任务服务效率，实验证明其能显著提升性能且设计泛化性好。


<details>
  <summary>Details</summary>
Motivation: 传统调度策略存在 HOL 阻塞问题，影响大语言模型推理任务的低延迟和高吞吐量，需要更高效的调度器。

Method: 引入 PARS 调度器，通过成对排序和边缘排序损失近似最短作业优先调度，集成到 vLLM 系统，有效预测基于响应长度的任务顺序。

Result: 在多个大语言模型和真实推理数据集上的实验表明，PARS 显著提升了性能，跨模型评估显示设计泛化性好。

Conclusion: PARS 能有效提升大语言模型推理任务的服务效率，且设计具有良好的泛化能力。

Abstract: Efficient scheduling of LLM inference tasks is essential for achieving low
latency and high throughput, particularly with the growing use of
reasoning-capable LLMs. Traditional strategies like First-Come-First-Serve
(FCFS) often suffer from Head-of-Line (HOL) blocking, where long-running tasks
delay shorter ones queued behind them. In this paper, we introduce PARS, a
prompt-aware LLM task scheduler that improves serving efficiency by
approximating shortest-job-first (SJF) scheduling through pairwise ranking with
margin ranking loss. PARS focuses on impactful scheduling decisions and is
seamlessly integrated into the state-of-the-art LLM serving system vLLM. It
effectively predicts response-length-based task ordering, reducing latency with
minimal overhead. Extensive experiments across multiple LLMs and real-world
inference datasets show that PARS significantly improves performance, including
for reasoning workloads. Furthermore, our cross-model evaluations demonstrate
that the design generalizes well, enabling effective scheduling even when
predictors are trained on different LLMs.

</details>


### [134] [From News to Returns: A Granger-Causal Hypergraph Transformer on the Sphere](https://arxiv.org/abs/2510.04357)
*Anoushka Harit,Zhongtian Sun,Jongmin Yu*

Main category: cs.LG

TL;DR: 提出Causal Sphere Hypergraph Transformer (CSHT)用于金融时间序列预测，结合多种元素，在多任务上优于基线模型，有良好泛化性和归因路径。


<details>
  <summary>Details</summary>
Motivation: 开发可解释的金融时间序列预测架构，解决不确定性下的可靠金融预测问题。

Method: 提出CSHT架构，提取多元Granger因果依赖，编码为超球面上的有向超边，通过角度掩码约束注意力。

Result: 在2018 - 2023年标普500数据上，CSHT在收益预测、制度分类和顶级资产排名任务中始终优于基线模型。

Conclusion: CSHT是不确定性下可靠金融预测的原则性和实用性解决方案。

Abstract: We propose the Causal Sphere Hypergraph Transformer (CSHT), a novel
architecture for interpretable financial time-series forecasting that unifies
\emph{Granger-causal hypergraph structure}, \emph{Riemannian geometry}, and
\emph{causally masked Transformer attention}. CSHT models the directional
influence of financial news and sentiment on asset returns by extracting
multivariate Granger-causal dependencies, which are encoded as directional
hyperedges on the surface of a hypersphere. Attention is constrained via
angular masks that preserve both temporal directionality and geometric
consistency. Evaluated on S\&P 500 data from 2018 to 2023, including the 2020
COVID-19 shock, CSHT consistently outperforms baselines across return
prediction, regime classification, and top-asset ranking tasks. By enforcing
predictive causal structure and embedding variables in a Riemannian manifold,
CSHT delivers both \emph{robust generalisation across market regimes} and
\emph{transparent attribution pathways} from macroeconomic events to
stock-level responses. These results suggest that CSHT is a principled and
practical solution for trustworthy financial forecasting under uncertainty.

</details>


### [135] [VIFO: Visual Feature Empowered Multivariate Time Series Forecasting with Cross-Modal Fusion](https://arxiv.org/abs/2510.03244)
*Yanlong Wang,Hang Yu,Jian Xu,Fei Ma,Hongkang Zhang,Tongtong Feng,Zijian Zhang,Shao-Lun Huang,Danny Dongning Sun,Xiao-Ping Zhang*

Main category: cs.LG

TL;DR: 提出跨模态预测模型VIFO，将多元时间序列转换为图像，利用预训练大视觉模型提取特征并与时间序列模态特征融合，在多基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有大时间序列基础模型忽略跨通道依赖，多模态方法未充分利用大视觉模型解读时空数据，挖掘不同模态信息提取优势以提升时间序列预测性能的潜力未被充分发掘。

Method: 提出VIFO模型，将多元时间序列转换为图像，用预训练大视觉模型提取跨通道模式，将视觉特征与时间序列模态表示对齐融合，冻结大视觉模型并仅训练其7.45%的参数。

Result: VIFO在多个基准测试中取得有竞争力的性能。

Conclusion: VIFO为捕捉跨变量关系提供了高效有效的解决方案。

Abstract: Large time series foundation models often adopt channel-independent
architectures to handle varying data dimensions, but this design ignores
crucial cross-channel dependencies. Concurrently, existing multimodal
approaches have not fully exploited the power of large vision models (LVMs) to
interpret spatiotemporal data. Additionally, there remains significant
unexplored potential in leveraging the advantages of information extraction
from different modalities to enhance time series forecasting performance. To
address these gaps, we propose the VIFO, a cross-modal forecasting model. VIFO
uniquely renders multivariate time series into image, enabling pre-trained LVM
to extract complex cross-channel patterns that are invisible to
channel-independent models. These visual features are then aligned and fused
with representations from the time series modality. By freezing the LVM and
training only 7.45% of its parameters, VIFO achieves competitive performance on
multiple benchmarks, offering an efficient and effective solution for capturing
cross-variable relationships in

</details>


### [136] [Frequency-Aware Model Parameter Explorer: A new attribution method for improving explainability](https://arxiv.org/abs/2510.03245)
*Ali Yavari,Alireza Mohamadi,Elham Beydaghi,Rainer A. Leitgeb*

Main category: cs.LG

TL;DR: 提出可转移频率感知攻击和FAMPE归因方法，提升DNN可解释性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 确保DNN在噪声和扰动下的可靠性，现有归因方法效果欠佳需改进。

Method: 提出可转移频率感知攻击，基于此提出FAMPE归因方法。

Result: 相对于AttEXplore，FAMPE在插入分数上平均提高13.02%。

Conclusion: FAMPE能提升DNN可解释性，通过消融研究探讨了高低频成分在可解释性中的作用。

Abstract: Ensuring the reliability of deep neural networks (DNNs) in the presence of
real world noise and intentional perturbations remains a significant challenge.
To address this, attribution methods have been proposed, though their efficacy
remains suboptimal and necessitates further refinement. In this paper, we
propose a novel category of transferable adversarial attacks, called
transferable frequency-aware attacks, enabling frequency-aware exploration via
both high-and low-frequency components. Based on this type of attacks, we also
propose a novel attribution method, named Frequency-Aware Model Parameter
Explorer (FAMPE), which improves the explainability for DNNs. Relative to the
current state-of-the-art method AttEXplore, our FAMPE attains an average gain
of 13.02% in Insertion Score, thereby outperforming existing approaches.
Through detailed ablation studies, we also investigate the role of both high-
and low-frequency components in explainability.

</details>


### [137] [StructPrune: Structured Global Pruning asymptotics with $\mathcal{O}(\sqrt{N})$ GPU Memory](https://arxiv.org/abs/2510.03246)
*Xinyuan Song,Guangji Bai,Liang Zhao*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Pruning is critical for scaling large language models (LLMs). Global pruning
achieves strong performance but requires $\mathcal{O}(N)$ memory, which is
infeasible for billion-parameter models. Local pruning reduces GPU memory usage
to that of a single layer by pruning layers independently, but it neglects
inter-layer dependencies and often leads to suboptimal performance in
high-sparsity regimes. Unlike unstructured pruning, structured pruning produces
regular sparsity patterns that align well with GPU kernels and library
optimizations, making it more hardware-efficient. However, structured pruning
typically relies on global pruning, since structured patterns are more prone to
severe performance degradation under local optimization. To jointly achieve
structured pruning and the memory efficiency of local pruning, we propose a
divide-and-conquer strategy that decomposes the global pruning problem into
coordinated subproblems across different modules, each of which fits within
limited GPU memory. Building on this idea, we design \textbf{STRUPRUNE}, an
ADMM-based framework that integrates structured sparsity into the pruning
process, combining the memory efficiency of local pruning with the hardware
compatibility of structured methods. We derive a closed-form analytical
solution for structured pruning masks that provides an explicit rule for
layer-wise sparsity allocation, and further develop an energy-based asymptotic
framework yielding a softmax-form allocation scheme that simplifies
optimization while adapting to heterogeneous layer importance. Experiments
demonstrate that STRUPRUNE matches the perplexity of global structured pruning
while reducing memory cost from $\mathcal{O}(N)$ to $\mathcal{O}(\sqrt{N})$,
enabling practical deployment at the billion-parameter scale.

</details>


### [138] [Towards Multimodal Active Learning: Efficient Learning with Limited Paired Data](https://arxiv.org/abs/2510.03247)
*Jiancheng Zhang,Yinglun Zhu*

Main category: cs.LG

TL;DR: 本文提出首个针对未对齐多模态数据的主动学习框架，新算法结合不确定性和多样性原则，实验表明能降低多模态标注成本并保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有主动学习算法主要关注单模态数据，忽略了多模态学习中的标注负担，而多模态未对齐数据的高质量对齐成本高。

Method: 开发一种新算法，在模态感知设计中结合不确定性和多样性原则，实现线性时间获取，适用于基于池和基于流的设置。

Result: 在基准数据集上的大量实验表明，该方法能持续降低多模态标注成本并保持性能，如在ColorSwap数据集上最多可减少40%的标注需求且不损失准确性。

Conclusion: 所提出的多模态主动学习框架和算法能有效降低多模态数据的标注成本，具有实际应用价值。

Abstract: Active learning (AL) is a principled strategy to reduce annotation cost in
data-hungry deep learning. However, existing AL algorithms focus almost
exclusively on unimodal data, overlooking the substantial annotation burden in
multimodal learning. We introduce the first framework for multimodal active
learning with unaligned data, where the learner must actively acquire
cross-modal alignments rather than labels on pre-aligned pairs. This setting
captures the practical bottleneck in modern multimodal pipelines such as CLIP
and SigLIP, where unimodal features are easy to obtain but high-quality
alignment is costly. We develop a new algorithm that combines uncertainty and
diversity principles in a modality-aware design, achieves linear-time
acquisition, and applies seamlessly to both pool-based and streaming-based
settings. Extensive experiments on benchmark datasets demonstrate that our
approach consistently reduces multimodal annotation cost while preserving
performance; for instance, on the ColorSwap dataset it cuts annotation
requirements by up to $40\%$ without loss in accuracy.

</details>


### [139] [LLM-Guided Evolutionary Program Synthesis for Quasi-Monte Carlo Design](https://arxiv.org/abs/2510.03650)
*Amir Sadikov*

Main category: cs.LG

TL;DR: 本文将两个长期的QMC设计问题转化为程序合成问题，用LLM引导的进化循环解决，在有限点集和数字序列上取得良好结果，证明该方法可自动发现高质量QMC构造。


<details>
  <summary>Details</summary>
Motivation: 解决高维积分中QMC方法的两个长期设计问题，即构造低星偏差的有限2D/3D点集和选择使下游积分项随机QMC误差最小的Sobol'方向数。

Method: 采用LLM引导的进化循环，结合构造性代码提议和迭代数值细化的两阶段过程。

Result: 在有限点集上，重新发现小二维情况的已知最优解，设置新的二维基准；在三维上匹配已知最优解并报告改进基准。在数字序列上，进化Sobol'参数降低了随机准蒙特卡罗均方误差。

Conclusion: LLM驱动的进化程序合成可以自动发现高质量QMC构造，在经典设计最优处恢复它们，在有限N结构重要处改进它们。

Abstract: Low-discrepancy point sets and digital sequences underpin quasi-Monte Carlo
(QMC) methods for high-dimensional integration. We cast two long-standing QMC
design problems as program synthesis and solve them with an LLM-guided
evolutionary loop that mutates and selects code under task-specific fitness:
(i) constructing finite 2D/3D point sets with low star discrepancy, and (ii)
choosing Sobol' direction numbers that minimize randomized QMC error on
downstream integrands. Our two-phase procedure combines constructive code
proposals with iterative numerical refinement. On finite sets, we rediscover
known optima in small 2D cases and set new best-known 2D benchmarks for N >=
40, while matching most known 3D optima up to the proven frontier (N <= 8) and
reporting improved 3D benchmarks beyond. On digital sequences, evolving Sobol'
parameters yields consistent reductions in randomized quasi-Monte Carlo (rQMC)
mean-squared error for several 32-dimensional option-pricing tasks relative to
widely used Joe--Kuo parameters, while preserving extensibility to any sample
size and compatibility with standard randomizations. Taken together, the
results demonstrate that LLM-driven evolutionary program synthesis can automate
the discovery of high-quality QMC constructions, recovering classical designs
where they are optimal and improving them where finite-N structure matters.
Data and code are available at
https://github.com/hockeyguy123/openevolve-star-discrepancy.git.

</details>


### [140] [Real-Time Brain Biomechanics Prediction with Neural Operators: Toward Clinically Deployable Traumatic Brain Injury Models](https://arxiv.org/abs/2510.03248)
*Anusha Agarwal,Dibakar Roy Sarkar,Somdatta Goswami*

Main category: cs.LG

TL;DR: 研究用神经网络算子架构预测脑位移场，以实现实时创伤性脑损伤（TBI）建模，对比四种架构性能，NOs可大幅减少计算时间，有临床应用潜力。


<details>
  <summary>Details</summary>
Motivation: 有限元模型计算创伤性脑损伤时计算成本高，限制临床快速决策应用，需快速、个性化的脑位移场预测方法。

Method: 将TBI建模作为算子学习问题，用四种神经网络算子架构（FNO、F - FNO、MG - FNO、DeepONet）在249个MRE数据集上训练和评估。

Result: MG - FNO精度最高；F - FNO收敛速度比标准FNO快2倍；DeepONet推理最快，比MG - FNO计算速度快7倍；所有NOs将计算时间从数小时减至毫秒，且不牺牲解剖学真实性。

Conclusion: NOs为预测脑变形提供高效、分辨率不变的方法，可用于实时、个性化TBI风险评估等，有构建人脑数字孪生体的潜力。

Abstract: Traumatic brain injury (TBI) remains a major public health concern, with over
69 million cases annually worldwide. Finite element (FE) models offer
high-fidelity predictions of brain deformation but are computationally
expensive, requiring hours per simulation and limiting their clinical utility
for rapid decision-making. This study benchmarks state-of-the-art neural
operator (NO) architectures for rapid, patient-specific prediction of brain
displacement fields, aiming to enable real-time TBI modeling in clinical and
translational settings. We formulated TBI modeling as an operator learning
problem, mapping subject-specific anatomical MRI, magnetic resonance
elastography (MRE) stiffness maps, and demographic features to full-field 3D
brain displacement predictions. Four architectures - Fourier Neural Operator
(FNO), Factorized FNO (F-FNO), Multi-Grid FNO (MG-FNO), and Deep Operator
Network (DeepONet) were trained and evaluated on 249 MRE datasets across
physiologically relevant frequencies (20 - 90 Hz). MG-FNO achieved the highest
accuracy (MSE = 0.0023, 94.3\% spatial fidelity) and preserved fine-scale
features, while F-FNO converged 2$\times$ faster than standard FNO. DeepONet
offered the fastest inference (14.5 iterations/s) with a 7$\times$
computational speed-up over MG-FNO, suggesting utility for embedded or edge
computing applications. All NOs reduced computation time from hours to
milliseconds without sacrificing anatomical realism. NOs provide an efficient,
resolution-invariant approach for predicting brain deformation, opening the
door to real-time, patient-specific TBI risk assessment, clinical triage
support, and optimization of protective equipment. These results highlight the
potential for NO-based digital twins of the human brain, enabling scalable,
on-demand biomechanical modeling in both clinical and population health
contexts.

</details>


### [141] [Light Differentiable Logic Gate Networks](https://arxiv.org/abs/2510.03250)
*Lukas Rüttgers,Till Aczel,Andreas Plesner,Roger Wattenhofer*

Main category: cs.LG

TL;DR: DLGNs有推理效率但训练有问题，提出重参数化方法减小模型大小、加速反向传播、减少训练步骤且保持精度。


<details>
  <summary>Details</summary>
Motivation: 解决DLGNs存在的梯度消失、离散化误差和高训练成本等问题，避免增加深度损害精度。

Method: 提出一种重参数化方法，对数级缩小每个门的输入参数大小。

Result: 对于二进制输入，模型大小缩小4倍，反向传播加速1.86倍，训练步骤减少8.5倍，CIFAR - 100上精度稳定有时更优。

Conclusion: 重参数化方法能有效改善DLGNs的训练问题并保持精度。

Abstract: Differentiable logic gate networks (DLGNs) exhibit extraordinary efficiency
at inference while sustaining competitive accuracy. But vanishing gradients,
discretization errors, and high training cost impede scaling these networks.
Even with dedicated parameter initialization schemes from subsequent works,
increasing depth still harms accuracy. We show that the root cause of these
issues lies in the underlying parametrization of logic gate neurons themselves.
To overcome this issue, we propose a reparametrization that also shrinks the
parameter size logarithmically in the number of inputs per gate. For binary
inputs, this already reduces the model size by 4x, speeds up the backward pass
by up to 1.86x, and converges in 8.5x fewer training steps. On top of that, we
show that the accuracy on CIFAR-100 remains stable and sometimes superior to
the original parametrization.

</details>


### [142] [Neural Network Surrogates for Free Energy Computation of Complex Chemical Systems](https://arxiv.org/abs/2510.01396)
*Wasut Pornpatcharapong*

Main category: cs.LG

TL;DR: 提出神经网络替代框架，绕过解析形式提供集体变量雅可比矩阵，在MgCl2离子配对系统上验证效果，拓宽模拟范围。


<details>
  <summary>Details</summary>
Motivation: 现有自由能重建方法获取集体变量雅可比矩阵受限，制约复杂或机器学习集体变量的使用。

Method: 引入神经网络替代框架，直接从笛卡尔坐标学习集体变量，用自动微分提供雅可比矩阵。

Result: 在MgCl2离子配对系统中，简单距离和复杂配位数集体变量都实现高精度，雅可比误差近似高斯分布。

Conclusion: 该框架使基于梯度的自由能方法能纳入复杂和机器学习集体变量，拓宽生化和材料模拟范围。

Abstract: Free energy reconstruction methods such as Gaussian Process Regression (GPR)
require Jacobians of the collective variables (CVs), a bottleneck that
restricts the use of complex or machine-learned CVs. We introduce a neural
network surrogate framework that learns CVs directly from Cartesian coordinates
and uses automatic differentiation to provide Jacobians, bypassing analytical
forms. On an MgCl2 ion-pairing system, our method achieved high accuracy for
both a simple distance CV and a complex coordination-number CV. Moreover,
Jacobian errors also followed a near-Gaussian distribution, making them
suitable for GPR pipelines. This framework enables gradient-based free energy
methods to incorporate complex and machine-learned CVs, broadening the scope of
biochemistry and materials simulations.

</details>


### [143] [Transductive and Learning-Augmented Online Regression](https://arxiv.org/abs/2510.03917)
*Vinod Raman,Shenghao Xie,Samson Zhou*

Main category: cs.LG

TL;DR: 研究有未来示例预测信息时的在线回归，刻画转导在线回归的极小极大期望遗憾，推广到有噪声预测情况并开发在线学习器。


<details>
  <summary>Details</summary>
Motivation: 基于数据流中现实生活的可预测性，研究有未来示例预测信息时的在线回归。

Method: 先刻画转导在线学习中极小极大期望遗憾，再推广到有噪声预测情况，利用转导在线设置结果开发在线学习器。

Result: 开发的在线学习器极小极大期望遗憾与最坏情况遗憾匹配，随预测质量提升表现更好，在预测精确时显著优于最坏情况遗憾。

Conclusion: 使之前不可学习的类在可预测示例下可学习，符合更广泛的学习增强模型范式。

Abstract: Motivated by the predictable nature of real-life in data streams, we study
online regression when the learner has access to predictions about future
examples. In the extreme case, called transductive online learning, the
sequence of examples is revealed to the learner before the game begins. For
this setting, we fully characterize the minimax expected regret in terms of the
fat-shattering dimension, establishing a separation between transductive online
regression and (adversarial) online regression. Then, we generalize this
setting by allowing for noisy or \emph{imperfect} predictions about future
examples. Using our results for the transductive online setting, we develop an
online learner whose minimax expected regret matches the worst-case regret,
improves smoothly with prediction quality, and significantly outperforms the
worst-case regret when future example predictions are precise, achieving
performance similar to the transductive online learner. This enables
learnability for previously unlearnable classes under predictable examples,
aligning with the broader learning-augmented model paradigm.

</details>


### [144] [HydroFusion-LMF: Semi-Supervised Multi-Network Fusion with Large-Model Adaptation for Long-Term Daily Runoff Forecasting](https://arxiv.org/abs/2510.03744)
*Qianfei Fan,Jiayu Wei,Peijun Zhu,Wensheng Ye,Meie Fang*

Main category: cs.LG

TL;DR: 提出HydroFusion - LMF框架用于小流域径流预测，在约10年数据集上表现优于基线模型，平衡了解释性与性能。


<details>
  <summary>Details</summary>
Motivation: 小流域径流预测困难，现有深度模型存在不足，缺乏对无标签数据利用和制度适应性。

Method: 提出HydroFusion - LMF框架，包括可学习的趋势 - 季节 - 残差分解、异构专家集处理残差、水文上下文感知门融合输出、半监督多任务目标增强监督，可选适配器/ LoRA层注入冻结的基础时间序列编码器。

Result: 在约10年每日数据集上，MSE为1.0128，MAE为0.5818，优于最强基线DLinear 10.2% / 10.3%，优于平均基线24.6% / 17.1%。

Conclusion: 该框架平衡了解释性与性能，推动了非平稳条件下标签高效的水文预测。

Abstract: Accurate decade-scale daily runoff forecasting in small watersheds is
difficult because signals blend drifting trends, multi-scale seasonal cycles,
regime shifts, and sparse extremes. Prior deep models (DLinear, TimesNet,
PatchTST, TiDE, Nonstationary Transformer, LSTNet, LSTM) usually target single
facets and under-utilize unlabeled spans, limiting regime adaptivity. We
propose HydroFusion-LMF, a unified framework that (i) performs a learnable
trend-seasonal-residual decomposition to reduce non-stationarity, (ii) routes
residuals through a compact heterogeneous expert set (linear refinement,
frequency kernel, patch Transformer, recurrent memory, dynamically normalized
attention), (iii) fuses expert outputs via a hydrologic context-aware gate
conditioned on day-of-year phase, antecedent precipitation, local variance,
flood indicators, and static basin attributes, and (iv) augments supervision
with a semi-supervised multi-task objective (composite MSE/MAE + extreme
emphasis + NSE/KGE, masked reconstruction, multi-scale contrastive alignment,
augmentation consistency, variance-filtered pseudo-labeling). Optional adapter
/ LoRA layers inject a frozen foundation time-series encoder efficiently. On a
~10-year daily dataset HydroFusion-LMF attains MSE 1.0128 / MAE 0.5818,
improving the strongest baseline (DLinear) by 10.2% / 10.3% and the mean
baseline by 24.6% / 17.1%. We observe simultaneous MSE and MAE reductions
relative to baselines. The framework balances interpretability (explicit
components, sparse gating) with performance, advancing label-efficient
hydrologic forecasting under non-stationarity.

</details>


### [145] [Numerion: A Multi-Hypercomplex Model for Time Series Forecasting](https://arxiv.org/abs/2510.03251)
*Hanzhong Cao,Wenbo Yan,Ying Tan*

Main category: cs.LG

TL;DR: 提出基于多超复空间的时间序列预测模型Numerion，在多公开数据集上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法受计算复杂度和假设鲁棒性限制，研究发现复域和高阶超复空间中时间序列特征频率自然降低。

Method: 将线性层和激活函数推广到任意2的幂次方维度的超复空间，引入RHR - MLP架构，用多个RHR - MLP映射时间序列到不同维度超复空间，通过动态融合机制自适应融合潜在模式。

Result: 在多个公开数据集上取得了最先进的结果。

Conclusion: 多维RHR - MLP能自然分解时间序列，高维超复空间更易捕捉低频特征。

Abstract: Many methods aim to enhance time series forecasting by decomposing the series
through intricate model structures and prior knowledge, yet they are inevitably
limited by computational complexity and the robustness of the assumptions. Our
research uncovers that in the complex domain and higher-order hypercomplex
spaces, the characteristic frequencies of time series naturally decrease.
Leveraging this insight, we propose Numerion, a time series forecasting model
based on multiple hypercomplex spaces. Specifically, grounded in theoretical
support, we generalize linear layers and activation functions to hypercomplex
spaces of arbitrary power-of-two dimensions and introduce a novel
Real-Hypercomplex-Real Domain Multi-Layer Perceptron (RHR-MLP) architecture.
Numerion utilizes multiple RHR-MLPs to map time series into hypercomplex spaces
of varying dimensions, naturally decomposing and independently modeling the
series, and adaptively fuses the latent patterns exhibited in different spaces
through a dynamic fusion mechanism. Experiments validate the model`s
performance, achieving state-of-the-art results on multiple public datasets.
Visualizations and quantitative analyses comprehensively demonstrate the
ability of multi-dimensional RHR-MLPs to naturally decompose time series and
reveal the tendency of higher dimensional hypercomplex spaces to capture lower
frequency features.

</details>


### [146] [Data-Driven Temperature Modelling of Machine Tools by Neural Networks: A Benchmark](https://arxiv.org/abs/2510.03261)
*C. Coelho,M. Hohmann,D. Fernández,L. Penter,S. Ihlenfeldt,O. Niggemann*

Main category: cs.LG

TL;DR: 提出用神经网络预测机床温度和热通量场的新范式，实现灵活通用的热误差校正。


<details>
  <summary>Details</summary>
Motivation: 传统热误差校正方法通用性和适应性有限，需改进。

Method: 用有限元方法获取数据训练神经网络，采用基于相关性的选点策略，对比多种时间序列神经网络架构，训练特定和通用模型。

Result: 能准确且低成本地预测温度和热通量场。

Conclusion: 为机床环境中实现灵活通用的热误差校正奠定基础。

Abstract: Thermal errors in machine tools significantly impact machining precision and
productivity. Traditional thermal error correction/compensation methods rely on
measured temperature-deformation fields or on transfer functions. Most existing
data-driven compensation strategies employ neural networks (NNs) to directly
predict thermal errors or specific compensation values. While effective, these
approaches are tightly bound to particular error types, spatial locations, or
machine configurations, limiting their generality and adaptability. In this
work, we introduce a novel paradigm in which NNs are trained to predict
high-fidelity temperature and heat flux fields within the machine tool. The
proposed framework enables subsequent computation and correction of a wide
range of error types using modular, swappable downstream components. The NN is
trained using data obtained with the finite element method under varying
initial conditions and incorporates a correlation-based selection strategy that
identifies the most informative measurement points, minimising hardware
requirements during inference. We further benchmark state-of-the-art
time-series NN architectures, namely Recurrent NN, Gated Recurrent Unit,
Long-Short Term Memory (LSTM), Bidirectional LSTM, Transformer, and Temporal
Convolutional Network, by training both specialised models, tailored for
specific initial conditions, and general models, capable of extrapolating to
unseen scenarios. The results show accurate and low-cost prediction of
temperature and heat flux fields, laying the basis for enabling flexible and
generalisable thermal error correction in machine tool environments.

</details>


### [147] [Universal Multi-Domain Translation via Diffusion Routers](https://arxiv.org/abs/2510.03252)
*Duc Kieu,Kien Do,Tuan Hoang,Thao Minh Le,Tung Kieu,Dang Nguyen,Thin Nguyen*

Main category: cs.LG

TL;DR: 本文提出通用多领域翻译（UMDT）及Diffusion Router（DR）框架，经评估DR在多领域翻译中达SOTA，降低采样成本并解锁新任务。


<details>
  <summary>Details</summary>
Motivation: 现有多领域翻译方法需完全对齐元组或只能处理训练中见过的领域对，实用性受限。

Method: 提出Diffusion Router（DR）框架，用单一噪声预测器建模中心与非中心领域翻译，通过中心领域实现间接非中心翻译，还引入可扩展学习策略和Tweedie细化程序支持直接非中心映射。

Result: 在三个大规模UMDT基准测试中，DR在间接和直接翻译上取得SOTA结果，降低采样成本并解锁新任务。

Conclusion: DR是一个可扩展且通用的多领域通用翻译框架。

Abstract: Multi-domain translation (MDT) aims to learn translations between multiple
domains, yet existing approaches either require fully aligned tuples or can
only handle domain pairs seen in training, limiting their practicality and
excluding many cross-domain mappings. We introduce universal MDT (UMDT), a
generalization of MDT that seeks to translate between any pair of $K$ domains
using only $K-1$ paired datasets with a central domain. To tackle this problem,
we propose Diffusion Router (DR), a unified diffusion-based framework that
models all central$\leftrightarrow$non-central translations with a single noise
predictor conditioned on the source and target domain labels. DR enables
indirect non-central translations by routing through the central domain. We
further introduce a novel scalable learning strategy with a variational-bound
objective and an efficient Tweedie refinement procedure to support direct
non-central mappings. Through evaluation on three large-scale UMDT benchmarks,
DR achieves state-of-the-art results for both indirect and direct translations,
while lowering sampling cost and unlocking novel tasks such as
sketch$\leftrightarrow$segmentation. These results establish DR as a scalable
and versatile framework for universal translation across multiple domains.

</details>


### [148] [MACE: A Hybrid LLM Serving System with Colocated SLO-aware Continuous Retraining Alignment](https://arxiv.org/abs/2510.03283)
*Yufei Li,Yu Fu,Yue Dong,Cong Liu*

Main category: cs.LG

TL;DR: 本文提出MACE混合LLM系统，解决边缘服务器上LLM在用户数据非平稳下推理延迟和模型准确性的矛盾，评估显示其能降低推理延迟、维持吞吐量。


<details>
  <summary>Details</summary>
Motivation: 边缘服务器上的LLM因用户数据非平稳需频繁再训练，现有再训练策略存在问题，要在不违反服务水平目标下调整再训练频率。

Method: 提出MACE系统，将并发推理和微调并置，进行智能内存管理，根据模型更新对输出对齐的影响分配GPU周期。

Result: MACE可达到或超过连续再训练效果，降低推理延迟达63%，维持吞吐量，相比定期再训练改善各阶段延迟，在NVIDIA AGX Orin上维持GPU利用率超85%。

Conclusion: 迭代级混合调度是在边缘平台部署具有持续学习能力LLM的有前景方向。

Abstract: Large language models (LLMs) deployed on edge servers are increasingly used
in latency-sensitive applications such as personalized assistants,
recommendation, and content moderation. However, the non-stationary nature of
user data necessitates frequent retraining, which introduces a fundamental
tension between inference latency and model accuracy under constrained GPU
resources. Existing retraining strategies either delay model updates,
over-commit resources to retraining, or overlook iteration-level retraining
granularity. In this paper, we identify that iteration-level scheduling is
crucial for adapting retraining frequency to model drift without violating
service-level objectives (SLOs). We propose MACE, a hybrid LLM system that
colocates concurrent inference (prefill, decode) and fine-tuning, with
intelligent memory management to maximize task performance while promising
inference throughput. MACE leverages the insight that not all model updates
equally affect output alignment and allocates GPU cycles accordingly to balance
throughput, latency, and update freshness. Our trace-driven evaluation shows
that MACE matches or exceeds continuous retraining while reducing inference
latency by up to 63% and maintaining throughput under resource constraints.
Compared to periodic retraining, MACE improves latency breakdown across
prefill, decode, and finetune stages, and sustains GPU utilization above 85% in
NVIDIA AGX Orin. These results demonstrate that iteration-level hybrid
scheduling is a promising direction for deploying LLMs with continual learning
capabilities on edge platforms.

</details>


### [149] [Solving the Granularity Mismatch: Hierarchical Preference Learning for Long-Horizon LLM Agents](https://arxiv.org/abs/2510.03253)
*Heyang Gao,Zexu Sun,Erxue Min,Hengyi Cai,Shuaiqiang Wang,Dawei Yin,Xu Chen*

Main category: cs.LG

TL;DR: 提出分层偏好学习（HPL）框架优化大语言模型代理，在三个基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于偏好的离线方法在优化大语言模型代理时存在粒度不匹配问题，轨迹级DPO信号太粗，步骤级DPO太近视。

Method: 引入HPL框架，结合轨迹和步骤级DPO，核心是基于双层课程的组级偏好优化，包括分解专家轨迹、生成对比组和引入课程调度器。

Result: 在三个具有挑战性的代理基准测试中，HPL优于现有最先进方法。

Conclusion: 分层DPO损失有效整合多粒度偏好信号，双层课程对代理解决各种任务至关重要。

Abstract: Large Language Models (LLMs) as autonomous agents are increasingly tasked
with solving complex, long-horizon problems. Aligning these agents via
preference-based offline methods like Direct Preference Optimization (DPO) is a
promising direction, yet it faces a critical granularity mismatch.
Trajectory-level DPO provides a signal that is too coarse for precise credit
assignment, while step-level DPO is often too myopic to capture the value of
multi-step behaviors. To resolve this challenge, we introduce Hierarchical
Preference Learning (HPL), a hierarchical framework that optimizes LLM agents
by leveraging preference signals at multiple, synergistic granularities. While
HPL incorporates trajectory- and step-level DPO for global and local policy
stability, its core innovation lies in group-level preference optimization
guided by a dual-layer curriculum. Our approach first decomposes expert
trajectories into semantically coherent action groups and then generates
contrasting suboptimal groups to enable preference learning at a fine-grained,
sub-task level. Then, instead of treating all preference pairs equally, HPL
introduces a curriculum scheduler that organizes the learning process from
simple to complex. This curriculum is structured along two axes: the group
length, representing sub-task complexity, and the sample difficulty, defined by
the reward gap between preferred and dispreferred action groups. Experiments on
three challenging agent benchmarks show that HPL outperforms existing
state-of-the-art methods. Our analyses demonstrate that the hierarchical DPO
loss effectively integrates preference signals across multiple granularities,
while the dual-layer curriculum is crucial for enabling the agent to solve a
wide range of tasks, from simple behaviors to complex multi-step sequences.

</details>


### [150] [LogAction: Consistent Cross-system Anomaly Detection through Logs via Active Domain](https://arxiv.org/abs/2510.03288)
*Chiming Duan,Minghua He,Pei Xiao,Tong Jia,Xin Zhang,Zhewei Zhong,Xiang Luo,Yan Niu,Lingzhe Zhang,Yifan Wu,Siyu Yu,Weijie Hong,Ying Li,Gang Huang*

Main category: cs.LG

TL;DR: 提出基于主动域适应的日志异常检测模型LogAction，结合多种技术，少量手动标注下实验效果好。


<details>
  <summary>Details</summary>
Motivation: 现有日志异常检测方法依赖标注，基于迁移学习和主动学习的方法存在数据分布差距和冷启动问题。

Method: 提出LogAction模型，结合迁移学习和主动学习，用成熟系统标注数据训练基础模型，用自由能和不确定性采样选择边界日志手动标注。

Result: 在六个不同数据集组合上，LogAction用2%手动标签实现平均93.01%的F1分数，比一些先进方法高26.28%。

Conclusion: LogAction能在少量手动标注下有效解决现有日志异常检测方法的问题，性能优越。

Abstract: Log-based anomaly detection is a essential task for ensuring the reliability
and performance of software systems. However, the performance of existing
anomaly detection methods heavily relies on labeling, while labeling a large
volume of logs is highly challenging. To address this issue, many approaches
based on transfer learning and active learning have been proposed.
Nevertheless, their effectiveness is hindered by issues such as the gap between
source and target system data distributions and cold-start problems. In this
paper, we propose LogAction, a novel log-based anomaly detection model based on
active domain adaptation. LogAction integrates transfer learning and active
learning techniques. On one hand, it uses labeled data from a mature system to
train a base model, mitigating the cold-start issue in active learning. On the
other hand, LogAction utilize free energy-based sampling and uncertainty-based
sampling to select logs located at the distribution boundaries for manual
labeling, thus addresses the data distribution gap in transfer learning with
minimal human labeling efforts. Experimental results on six different
combinations of datasets demonstrate that LogAction achieves an average 93.01%
F1 score with only 2% of manual labels, outperforming some state-of-the-art
methods by 26.28%. Website: https://logaction.github.io

</details>


### [151] [Adversarial training with restricted data manipulation](https://arxiv.org/abs/2510.03254)
*David Benfield,Stefano Coniglio,Phan Tu Vuong,Alain Zemkoho*

Main category: cs.LG

TL;DR: 针对对抗机器学习，指出现有悲观双层优化方法不足，提出受限悲观双层优化模型，实验表明该模型表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有悲观双层优化方法中对手不受限，导致模型过于悲观不现实，在真实数据上分类器性能差，需改进。

Method: 构建受限悲观双层优化模型，限制对手行动以找到更符合现实的解决方案。

Result: 实验表明构建的受限模型平均表现优于现有方法。

Conclusion: 受限悲观双层优化模型能更好反映现实，在对抗机器学习中表现更优。

Abstract: Adversarial machine learning concerns situations in which learners face
attacks from active adversaries. Such scenarios arise in applications such as
spam email filtering, malware detection and fake image generation, where
security methods must be actively updated to keep up with the everimproving
generation of malicious data. Pessimistic Bilevel optimisation has been shown
to be an effective method of training resilient classifiers against such
adversaries. By modelling these scenarios as a game between the learner and the
adversary, we anticipate how the adversary will modify their data and then
train a resilient classifier accordingly. However, since existing pessimistic
bilevel approaches feature an unrestricted adversary, the model is vulnerable
to becoming overly pessimistic and unrealistic. When finding the optimal
solution that defeats the classifier, it is possible that the adversary's data
becomes nonsensical and loses its intended nature. Such an adversary will not
properly reflect reality, and consequently, will lead to poor classifier
performance when implemented on real-world data. By constructing a constrained
pessimistic bilevel optimisation model, we restrict the adversary's movements
and identify a solution that better reflects reality. We demonstrate through
experiments that this model performs, on average, better than the existing
approach.

</details>


### [152] [From Score Distributions to Balance: Plug-and-Play Mixture-of-Experts Routing](https://arxiv.org/abs/2510.03293)
*Rana Shahout,Colin Cai,Yilun Du,Minlan Yu,Michael Mitzenmacher*

Main category: cs.LG

TL;DR: 提出LASER推理路由算法，平衡负载并保持精度，经评估可降低延迟、提高吞吐量。


<details>
  <summary>Details</summary>
Motivation: MoE模型推理内存负担重、专家负载不均衡，导致系统性能下降。

Method: LASER根据门函数分数分布调整路由，分数明确时选最强专家，分数均匀时选负载最小专家，且无需重新训练或微调。

Result: 在Mixtral - 8x7B和DeepSeek - MoE - 16b - chat的四个数据集上评估，LASER改善负载平衡，降低延迟、提高吞吐量，精度变化可忽略。

Conclusion: LASER作为即插即用算法，能有效平衡MoE模型负载，提升系统性能。

Abstract: Mixture-of-Experts (MoE) models can scale parameter capacity by routing each
token to a subset of experts through a learned gate function. While conditional
routing reduces training costs, it shifts the burden on inference memory:
expert parameters and activations consume memory, limiting the number of
experts per device. As tokens are routed, some experts become overloaded while
others are underutilized. Because experts are mapped to GPUs, this imbalance
translates directly into degraded system performance in terms of latency,
throughput, and cost. We present LASER, a plug-and-play, inference-time routing
algorithm that balances load while preserving accuracy. LASER adapts to the
shape of the gate's score distribution. When scores provide a clear preference,
it routes to the strongest experts; when scores are more uniform, it broadens
the set of viable experts and routes to the least-loaded among them. Because
LASER relies only on gate scores from a trained model, it integrates directly
into existing MoE inference pipelines without retraining or finetuning. We
evaluate LASER on Mixtral-8x7B and DeepSeek-MoE-16b-chat across four datasets
(ARC-Easy, ARC-Challenge, MMLU, and GSM8K). LASER improves load balancing,
translating into lower latency and higher throughput, while keeping the
accuracy changes negligible.

</details>


### [153] [SciTS: Scientific Time Series Understanding and Generation with LLMs](https://arxiv.org/abs/2510.03255)
*Wen Wu,Ziyang Zhang,Liwei Liu,Xuenan Xu,Junlin Liu,Ke Fan,Qitan Lv,Jimin Zhuang,Chen Zhang,Zheqi Yuan,Siyuan Hou,Tianyi Lin,Kai Chen,Bowen Zhou,Chao Zhang*

Main category: cs.LG

TL;DR: 本文引入科学时间序列基准SciTS并测试17个模型，发现通用大模型泛化性更强，还提出框架TimeOmni，填补相关空白。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型对时间序列处理不足，现有统一时间序列模型功能单一且对非周期、异构信号效果不明，需填补空白。

Method: 引入涵盖12个科学领域和43个任务的基准SciTS，测试17个模型，提出框架TimeOmni。

Result: 通用大模型比专用时间序列模型泛化性强，文本或图像表示时间序列会限制性能。

Conclusion: 工作填补科学时间序列基准和建模框架空白，为大模型处理复杂时间科学数据奠定基础。

Abstract: The scientific reasoning ability of large language models (LLMs) has recently
attracted significant attention. Time series, as a fundamental modality in
scientific data, presents unique challenges that are often overlooked in
current multimodal LLMs, which either encode numerical sequences as text or
convert them into images. Such approaches may be insufficient for comprehensive
scientific time series understanding and generation. Existing unified time
series models typically specialise in either forecasting or analysis, and their
effectiveness on non-periodic, heterogeneous scientific signals remains
unclear. To address these gaps, we introduce SciTS, a benchmark spanning 12
scientific domains and 43 tasks, with over 50k+ instances, both univariate and
multivariate signals ranging from $10^0$ to $10^7$ in length and up to 10~MHz
in frequency. We benchmark 17 models, including text-only LLMs, multimodal
LLMs, and unified time series models, and find that general-purpose LLMs
exhibit stronger generalisability than specialised time series models, while
representing time series as text or images limits their performance due to
excessively long sequences and loss of numerical precision, respectively. We
then introduce TimeOmni, a framework that equips LLMs with the ability to
understand and generate time series while remaining compatible with
general-purpose LLM training. This work fills a gap in both dedicated
benchmarks and modelling frameworks for scientific time series, paving the way
for LLMs to understand and generate complex temporal scientific data.

</details>


### [154] [CAFL-L: Constraint-Aware Federated Learning with Lagrangian Dual Optimization for On-Device Language Models](https://arxiv.org/abs/2510.03298)
*Dongqi Zheng,Wenjin Fu*

Main category: cs.LG

TL;DR: 提出CAFL - L方法，结合拉格朗日对偶优化处理设备资源约束，实验表明其优于标准FedAvg。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中明确考虑设备级资源约束，如能源、通信、内存和热预算。

Method: 采用拉格朗日对偶优化动态调整训练超参数，通过梯度累积的令牌预算保存维持训练稳定性。

Result: 在字符级语言模型实验中，相比标准FedAvg减少20%内存使用和95%通信量，验证性能有竞争力。

Conclusion: CAFL - L适用于资源受限的边缘设备部署。

Abstract: We introduce Constraint-Aware Federated Learning with Lagrangian Dual
Optimization (CAFL-L), a principled extension of FedAvg that explicitly
incorporates device-level resource constraints including energy, communication,
memory, and thermal budgets. CAFL-L employs Lagrangian dual optimization to
dynamically adapt training hyperparameters -- freezing depth, local steps,
batch size, and communication compression -- while preserving training
stability through token-budget preservation via gradient accumulation.
Experiments on a character-level language model demonstrate that CAFL-L
achieves superior constraint satisfaction compared to standard FedAvg (reducing
memory usage by 20% and communication by 95%) while maintaining competitive
validation performance, making it practical for deployment on
resource-constrained edge devices.

</details>


### [155] [Triple-BERT: Do We Really Need MARL for Order Dispatch on Ride-Sharing Platforms?](https://arxiv.org/abs/2510.03257)
*Zijian Zhao,Sen Li*

Main category: cs.LG

TL;DR: 本文提出Triple - BERT方法解决共享出行平台大规模订单调度问题，在真实数据集验证效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体强化学习方法在解决共享出行平台订单调度问题时，存在无法捕捉全局信息、合作差及维度灾难等问题。

Method: 提出基于变体TD3的集中式单智能体强化学习方法Triple - BERT，用动作分解策略处理大动作空间，引入基于BERT的网络处理大观测空间。

Result: 使用曼哈顿真实打车数据集验证，比现有最优方法提升约11.95%，服务订单增加4.26%，取客时间减少22.25%。

Conclusion: Triple - BERT方法能有效解决共享出行平台大规模订单调度问题，代码等公开。

Abstract: On-demand ride-sharing platforms, such as Uber and Lyft, face the intricate
real-time challenge of bundling and matching passengers-each with distinct
origins and destinations-to available vehicles, all while navigating
significant system uncertainties. Due to the extensive observation space
arising from the large number of drivers and orders, order dispatching, though
fundamentally a centralized task, is often addressed using Multi-Agent
Reinforcement Learning (MARL). However, independent MARL methods fail to
capture global information and exhibit poor cooperation among workers, while
Centralized Training Decentralized Execution (CTDE) MARL methods suffer from
the curse of dimensionality. To overcome these challenges, we propose
Triple-BERT, a centralized Single Agent Reinforcement Learning (MARL) method
designed specifically for large-scale order dispatching on ride-sharing
platforms. Built on a variant TD3, our approach addresses the vast action space
through an action decomposition strategy that breaks down the joint action
probability into individual driver action probabilities. To handle the
extensive observation space, we introduce a novel BERT-based network, where
parameter reuse mitigates parameter growth as the number of drivers and orders
increases, and the attention mechanism effectively captures the complex
relationships among the large pool of driver and orders. We validate our method
using a real-world ride-hailing dataset from Manhattan. Triple-BERT achieves
approximately an 11.95% improvement over current state-of-the-art methods, with
a 4.26% increase in served orders and a 22.25% reduction in pickup times. Our
code, trained model parameters, and processed data are publicly available at
the repository https://github.com/RS2002/Triple-BERT .

</details>


### [156] [Semantic-Aware Scheduling for GPU Clusters with Large Language Models](https://arxiv.org/abs/2510.03334)
*Zerui Wang,Qinghao Hu,Ana Klimovic,Tianwei Zhang,Yonggang Wen,Peng Sun,Dahua Lin*

Main category: cs.LG

TL;DR: 提出SchedMate框架解决深度学习调度器语义感知不足问题，提升调度性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习调度器对作业语义上下文感知不足，导致高分析开销、不可靠的时长估计等问题。

Method: 通过三个基于大语言模型的组件，从源代码、运行时日志和历史作业等非结构化数据源中提取见解，非侵入式增强现有调度器。

Result: 在128-GPU物理集群和生产轨迹模拟中，SchedMate将平均作业完成时间最多缩短1.91倍，显著提升调度性能。

Conclusion: 语义感知在现代深度学习调度中起着关键作用。

Abstract: Deep learning (DL) schedulers are pivotal in optimizing resource allocation
in GPU clusters, but operate with a critical limitation: they are largely blind
to the semantic context of the jobs they manage. This forces them to rely on
limited metadata, leading to high profiling overhead, unreliable duration
estimation, inadequate failure handling, and poor observability. To this end,
we propose SchedMate, a framework that bridges this semantic gap by
systematically extracting deep insights from overlooked, unstructured data
sources: source code, runtime logs, and historical jobs. SchedMate enhances
existing schedulers non-intrusively through three LLM-based components. Our
implementation integrates seamlessly with existing deep learning schedulers.
Evaluations on a 128-GPU physical cluster and extensive simulations on
production traces show SchedMate reduces average job completion times by up to
1.91x, substantially enhancing the scheduling performance, demonstrating the
critical role of semantic-awareness in modern DL scheduling.

</details>


### [157] [POEM: Explore Unexplored Reliable Samples to Enhance Test-Time Adaptation](https://arxiv.org/abs/2510.03258)
*Chang'an Yi,Xiaohui Deng,Shuaicheng Niu,Yan Zhou*

Main category: cs.LG

TL;DR: 提出POEM方法用于测试时自适应，通过探索可靠样本和引入额外网络提升性能，实验表明其优于现有方法，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有基于熵的测试时自适应方法对预定义熵阈值敏感，易忽视和未充分利用潜在可靠目标样本。

Method: 提出POEM方法探索此前未被探索的可靠样本，引入额外的适应分支网络平衡特征提取和目标数据性能。

Result: 在多种架构的综合实验中，POEM在挑战性场景和真实世界领域偏移中始终优于现有TTA方法，且计算效率高。

Conclusion: POEM方法有效，其核心思想可作为增强策略提升现有TTA方法性能。

Abstract: Test-time adaptation (TTA) aims to transfer knowledge from a source model to
unknown test data with potential distribution shifts in an online manner. Many
existing TTA methods rely on entropy as a confidence metric to optimize the
model. However, these approaches are sensitive to the predefined entropy
threshold, influencing which samples are chosen for model adaptation.
Consequently, potentially reliable target samples are often overlooked and
underutilized. For instance, a sample's entropy might slightly exceed the
threshold initially, but fall below it after the model is updated. Such samples
can provide stable supervised information and offer a normal range of gradients
to guide model adaptation. In this paper, we propose a general approach,
\underline{POEM}, to promote TTA via ex\underline{\textbf{p}}loring the
previously unexpl\underline{\textbf{o}}red reliabl\underline{\textbf{e}}
sa\underline{\textbf{m}}ples. Additionally, we introduce an extra Adapt Branch
network to strike a balance between extracting domain-agnostic representations
and achieving high performance on target data. Comprehensive experiments across
multiple architectures demonstrate that POEM consistently outperforms existing
TTA methods in both challenging scenarios and real-world domain shifts, while
remaining computationally efficient. The effectiveness of POEM is evaluated
through extensive analyses and thorough ablation studies. Moreover, the core
idea behind POEM can be employed as an augmentation strategy to boost the
performance of existing TTA approaches. The source code is publicly available
at \emph{https://github.com/ycarobot/POEM}

</details>


### [158] [Distributed Low-Communication Training with Decoupled Momentum Optimization](https://arxiv.org/abs/2510.03371)
*Sasho Nedelkoski,Alexander Acker,Odej Kao,Soeren Becker,Dominik Scheinert*

Main category: cs.LG

TL;DR: 提出结合不频繁同步和梯度动量压缩的方法，减少分布式训练通信量，在低带宽互联节点训练大模型更可行。


<details>
  <summary>Details</summary>
Motivation: 大模型训练依赖高带宽互联数据中心，需降低对节点间高带宽互联的依赖，使用分布式计算资源。

Method: 结合分布式模型副本的不频繁同步和梯度动量压缩，用离散余弦变换（DCT）将Nesterov动量分解为高低频分量，每H步只同步高频分量。

Result: 与基线DiLoCo相比，通信量最多减少16倍，在多种架构上有通用性。

Conclusion: 该工作提升了在低带宽互联分布式节点上训练大模型的可行性。

Abstract: The training of large models demands substantial computational resources,
typically available only in data centers with high-bandwidth interconnects.
However, reducing the reliance on high-bandwidth interconnects between nodes
enables the use of distributed compute resources as an alternative to
centralized data center training. Building on recent advances in distributed
model training, we propose an approach that further reduces communication by
combining infrequent synchronizations across distributed model replicas with
gradient momentum compression. In particular, we treat the optimizer momentum
as a signal and decompose the Nesterov momentum into high- and low-frequency
components via the discrete cosine transform (DCT). Only the high-frequency
components are synchronized across model replicas every $H$ steps. Empirically,
our method achieves up to a $16\times$ reduction in communication compared to
the baseline DiLoCo, and it generalizes across architectures, including
transformer-based language models and convolutional neural networks for images.
Overall, this work advances the feasibility of training large models on
distributed nodes with low-bandwidth interconnects.

</details>


### [159] [Meta-Awareness Enhances Reasoning Models: Self-Alignment Reinforcement Learning](https://arxiv.org/abs/2510.03259)
*Yoonjeon Kim,Doohyuk Jang,Eunho Yang*

Main category: cs.LG

TL;DR: 本文指出大推理模型缺乏元认知，设计MASA训练管道提升元认知，在准确率和训练效率上取得显著提升，且有良好泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有大推理模型缺乏元认知，研究旨在通过使元预测与真实展开对齐来提升模型性能。

Method: 设计MASA训练管道，利用自生成信号训练元认知，过滤零方差提示并适时截断长展开。

Result: 在域内任务中提升准确率和训练效率，在域外基准测试中有强泛化性，如加速GRPO训练、提高AIME25准确率等。

Conclusion: 训练时加入元认知引导可提升域外泛化能力，MASA方法有效。

Abstract: Recent studies on reasoning models explore the meta-awareness of language
models, the ability to know how to think by itself. We argue that large
reasoning models lack this meta-awareness property by proving severe
misalignment between true rollouts and predicted meta information. We posit
that aligning meta-prediction with true rollouts will lead to significant
performance gains. To verify this hypothesis, we design a training pipeline
that boosts Meta-Awareness via Self-Alignment (MASA), and prove that enhanced
meta-awareness directly translates to improved accuracy. Unlike existing
meta-cognitive reasoning models, our method does not require external training
sources but leverages self-generated signals to train meta-awareness. Moreover,
our method enables efficient training by i) filtering out zero-variance prompts
that are either trivial or unsolvable and ii) cutting off lengthy rollouts when
they are unlikely to lead to correct answers. The results are inspiring: our
strategy yields significant improvements in both accuracy and training
efficiency on in-domain tasks and shows strong generalization to out-of-domain
benchmarks. More specifically, our method can speed up GRPO training by over
1.28x to reach the same performance, and achieve a 19.3% gain in accuracy on
AIME25, and a 6.2 % average gain over six mathematics benchmarks. Training with
meta-cognitive guidance enhances out-of-domain generalization, giving a 3.87 %
boost on GPQA-Diamond and a 2.08 % overall accuracy gain across 13 benchmarks
spanning logical, scientific, and coding domains.

</details>


### [160] [Semantic-Inductive Attribute Selection for Zero-Shot Learning](https://arxiv.org/abs/2510.03260)
*Juan Jose Herrera-Aranda,Guillermo Gomez-Trenado,Francisco Herrera,Isaac Triguero*

Main category: cs.LG

TL;DR: 提出分区方案处理零样本学习中语义空间的冗余问题，研究两种特征选择策略，实验表明可提升未见过类别的准确率。


<details>
  <summary>Details</summary>
Motivation: 零样本学习语义空间存在噪声、冗余或无关属性，影响性能，需解决。

Method: 引入分区方案模拟未见条件，研究两种特征选择策略：一是将嵌入式特征选择适配零样本学习；二是利用进化计算探索属性子集空间。

Result: 在五个基准数据集实验显示，两种方法都能减少冗余、提升未见过类别准确率，RFS高效但依赖超参数，GA代价高但搜索空间广且不依赖。

Conclusion: 语义空间存在冗余，提出的分区方案是归纳条件下优化语义空间的有效工具。

Abstract: Zero-Shot Learning is an important paradigm within General-Purpose Artificial
Intelligence Systems, particularly in those that operate in open-world
scenarios where systems must adapt to new tasks dynamically. Semantic spaces
play a pivotal role as they bridge seen and unseen classes, but whether
human-annotated or generated by a machine learning model, they often contain
noisy, redundant, or irrelevant attributes that hinder performance. To address
this, we introduce a partitioning scheme that simulates unseen conditions in an
inductive setting (which is the most challenging), allowing attribute relevance
to be assessed without access to semantic information from unseen classes.
Within this framework, we study two complementary feature-selection strategies
and assess their generalisation. The first adapts embedded feature selection to
the particular demands of ZSL, turning model-driven rankings into meaningful
semantic pruning; the second leverages evolutionary computation to directly
explore the space of attribute subsets more broadly. Experiments on five
benchmark datasets (AWA2, CUB, SUN, aPY, FLO) show that both methods
consistently improve accuracy on unseen classes by reducing redundancy, but in
complementary ways: RFS is efficient and competitive though dependent on
critical hyperparameters, whereas GA is more costly yet explores the search
space more broadly and avoids such dependence. These results confirm that
semantic spaces are inherently redundant and highlight the proposed
partitioning scheme as an effective tool to refine them under inductive
conditions.

</details>


### [161] [A Lightweight Federated Learning Approach for Privacy-Preserving Botnet Detection in IoT](https://arxiv.org/abs/2510.03513)
*Taha M. Mahmoud,Naima Kaabouch*

Main category: cs.LG

TL;DR: 提出基于联邦学习的轻量级隐私保护僵尸网络检测框架，实验表明其能实现高检测精度并降低通信成本，凸显联邦学习在物联网入侵检测的实用性。


<details>
  <summary>Details</summary>
Motivation: 物联网快速发展使遭受僵尸网络攻击风险增加，传统检测方法在资源受限环境存在可扩展性、隐私和适应性问题。

Method: 提出基于联邦学习的轻量级隐私保护僵尸网络检测框架，引入通信高效聚合策略。

Result: 在基准物联网僵尸网络数据集实验中，框架实现高检测精度并大幅降低通信成本。

Conclusion: 联邦学习是实现物联网生态系统可扩展、安全和隐私感知入侵检测的实用途径。

Abstract: The rapid growth of the Internet of Things (IoT) has expanded opportunities
for innovation but also increased exposure to botnet-driven cyberattacks.
Conventional detection methods often struggle with scalability, privacy, and
adaptability in resource-constrained IoT environments. To address these
challenges, we present a lightweight and privacy-preserving botnet detection
framework based on federated learning. This approach enables distributed
devices to collaboratively train models without exchanging raw data, thus
maintaining user privacy while preserving detection accuracy. A
communication-efficient aggregation strategy is introduced to reduce overhead,
ensuring suitability for constrained IoT networks. Experiments on benchmark IoT
botnet datasets demonstrate that the framework achieves high detection accuracy
while substantially reducing communication costs. These findings highlight
federated learning as a practical path toward scalable, secure, and
privacy-aware intrusion detection for IoT ecosystems.

</details>


### [162] [Rethinking Inter-LoRA Orthogonality in Adapter Merging: Insights from Orthogonal Monte Carlo Dropout](https://arxiv.org/abs/2510.03262)
*Andi Zhang,Xuan Ding,Haofan Wang,Steven McDonagh,Samuel Kaski*

Main category: cs.LG

TL;DR: 提出正交蒙特卡罗Dropout机制，保证合并LoRAs时语义向量正交，但正交性未必带来语义解耦和组合性。


<details>
  <summary>Details</summary>
Motivation: 解决多个LoRAs合并时语义向量相互干扰的问题。

Method: 提出正交蒙特卡罗Dropout机制，保证合并的LoRAs在理论和运行时层面保持正交。

Result: 实验分析发现这种正交性并未带来先前工作中强调的语义解耦和组合性。

Conclusion: 仅LoRA间的正交性可能不足以实现真正的语义组合性，需重新审视其在适配器合并中的作用。

Abstract: We propose Orthogonal Monte Carlo Dropout, a mechanism that enforces strict
orthogonality when combining sparse semantic vectors without extra time
complexity. LoRA, a popular fine-tuning method for large models, typically
trains a module to represent a specific concept such as an object or a style.
When multiple LoRAs are merged, for example to generate an object in a
particular style, their semantic vectors may interfere with each other. Our
method guarantees, at the theoretical and runtime levels, that merged LoRAs
remain orthogonal and thus free from direct interference. However, empirical
analysis reveals that such orthogonality does not lead to the semantic
disentanglement or compositionality highlighted in prior work on compositional
adaptation. This finding suggests that inter-LoRA orthogonality alone may be
insufficient for achieving true semantic compositionality, prompting a
re-examination of its role in adapter merging.

</details>


### [163] [MECKD: Deep Learning-Based Fall Detection in Multilayer Mobile Edge Computing With Knowledge Distillation](https://arxiv.org/abs/2510.03601)
*Wei-Lung Mao,Chun-Chi Wang,Po-Heng Chou,Kai-Chun Liu,Yu Tsao*

Main category: cs.LG

TL;DR: 针对跌倒检测系统面临的问题，提出多层移动边缘计算（MLMEC）框架结合知识蒸馏（KD）方法，提升检测准确率并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 老龄化加剧使跌倒检测系统重要性提升，但现有架构存在边缘设备模型大小受限和数据传输延迟问题，需新方法解决。

Method: 提出MLMEC框架将架构拆分为多个站点，每个站点有神经网络模型；采用KD方法，让后端站点为前端提供额外学习经验。

Result: KD方法在SisFall数据集上准确率提高11.65%，在FallAllD数据集上提高2.78%；MLMEC结合KD比不结合KD在FallAllD数据集上数据延迟率降低54.15%，在SisFall数据集上降低46.67%。

Conclusion: MLMEC跌倒检测系统提高了准确率并降低了延迟。

Abstract: The rising aging population has increased the importance of fall detection
(FD) systems as an assistive technology, where deep learning techniques are
widely applied to enhance accuracy. FD systems typically use edge devices (EDs)
worn by individuals to collect real-time data, which are transmitted to a cloud
center (CC) or processed locally. However, this architecture faces challenges
such as a limited ED model size and data transmission latency to the CC. Mobile
edge computing (MEC), which allows computations at MEC servers deployed between
EDs and CC, has been explored to address these challenges. We propose a
multilayer MEC (MLMEC) framework to balance accuracy and latency. The MLMEC
splits the architecture into stations, each with a neural network model. If
front-end equipment cannot detect falls reliably, data are transmitted to a
station with more robust back-end computing. The knowledge distillation (KD)
approach was employed to improve front-end detection accuracy by allowing
high-power back-end stations to provide additional learning experiences,
enhancing precision while reducing latency and processing loads. Simulation
results demonstrate that the KD approach improved accuracy by 11.65% on the
SisFall dataset and 2.78% on the FallAllD dataset. The MLMEC with KD also
reduced the data latency rate by 54.15% on the FallAllD dataset and 46.67% on
the SisFall dataset compared to the MLMEC without KD. In summary, the MLMEC FD
system exhibits improved accuracy and reduced latency.

</details>


### [164] [Memory Self-Regeneration: Uncovering Hidden Knowledge in Unlearned Models](https://arxiv.org/abs/2510.03263)
*Agnieszka Polowczyk,Alicja Polowczyk,Joanna Waczyńska,Piotr Borycki,Przemysław Spurek*

Main category: cs.LG

TL;DR: 现代文本到图像模型生成逼真视觉效果时存在被滥用问题，推动了机器遗忘研究，但遗忘概念困难，论文提出记忆自我再生任务、MemoRa策略，指出知识检索鲁棒性是重要评估指标，还发现遗忘有短期和长期两种方式。


<details>
  <summary>Details</summary>
Motivation: 现代文本到图像模型可被滥用，推动机器遗忘研究，但实际遗忘概念困难，有必要深入研究模型遗忘和回忆知识的能力。

Method: 提出记忆自我再生任务，引入MemoRa策略，研究知识检索鲁棒性作为评估指标。

Result: 发现遗忘有短期和长期两种不同方式，短期概念可快速回忆，长期恢复更具挑战性。

Conclusion: 知识检索的鲁棒性是开发更强大有效遗忘技术的关键但未充分探索的评估指标。

Abstract: The impressive capability of modern text-to-image models to generate
realistic visuals has come with a serious drawback: they can be misused to
create harmful, deceptive or unlawful content. This has accelerated the push
for machine unlearning. This new field seeks to selectively remove specific
knowledge from a model's training data without causing a drop in its overall
performance. However, it turns out that actually forgetting a given concept is
an extremely difficult task. Models exposed to attacks using adversarial
prompts show the ability to generate so-called unlearned concepts, which can be
not only harmful but also illegal. In this paper, we present considerations
regarding the ability of models to forget and recall knowledge, introducing the
Memory Self-Regeneration task. Furthermore, we present MemoRa strategy, which
we consider to be a regenerative approach supporting the effective recovery of
previously lost knowledge. Moreover, we propose that robustness in knowledge
retrieval is a crucial yet underexplored evaluation measure for developing more
robust and effective unlearning techniques. Finally, we demonstrate that
forgetting occurs in two distinct ways: short-term, where concepts can be
quickly recalled, and long-term, where recovery is more challenging.

</details>


### [165] [Front-Loading Reasoning: The Synergy between Pretraining and Post-Training Data](https://arxiv.org/abs/2510.03264)
*Syeda Nahida Akter,Shrimai Prabhumoye,Eric Nyberg,Mostofa Patwary,Mohammad Shoeybi,Yejin Choi,Bryan Catanzaro*

Main category: cs.LG

TL;DR: 研究推理数据在不同训练阶段对大语言模型性能的影响，发现预训练阶段引入推理数据很关键，且预训练和SFT对数据特性需求不同。


<details>
  <summary>Details</summary>
Motivation: 当前推理数据在预训练阶段的作用不明，且不同阶段引入推理数据的效果研究较少，提出相关问题待解答。

Method: 系统研究不同规模、多样性和质量的推理数据在不同训练阶段对大语言模型性能的影响。

Result: 预训练阶段引入推理数据平均有19%的性能提升；预训练更受益于推理模式的广泛多样性（平均11%增益），SFT更敏感于数据质量（平均15%增益）；高质量预训练数据在SFT后有潜在效果，盲目扩大SFT数据可能有害。

Conclusion: 挑战了语言建模和推理的传统分离，为整个训练流程的数据分配提供了原则性指导以构建更强大模型。

Abstract: The prevailing paradigm for enhancing the reasoning abilities of LLMs
revolves around post-training on high-quality, reasoning-intensive data. While
emerging literature suggests that reasoning data is increasingly incorporated
also during the mid-training stage-a practice that is relatively more
proprietary and less openly characterized-the role of such data in pretraining
remains unclear. In particular, due to the opaqueness of pretraining corpora in
most frontier models, the effect of reasoning data introduced at different
phases of pre- and/or post-training is relatively less reported in the
scientific literature. This raises several important questions: Is adding
reasoning data earlier during pretraining any better than introducing it during
post-training? Could earlier inclusion risk overfitting and harm
generalization, or instead establish durable foundations that later fine-tuning
cannot recover? We conduct the first systematic study of how reasoning
data-varying in scale, diversity, and quality-affects LLM performance when
introduced at different stages of training. We find that front-loading
reasoning data into pretraining is critical (19% avg gain), establishing
foundational capabilities that cannot be fully replicated by later-stage SFT,
even with more data. We uncover an asymmetric principle for optimal data
allocation: pretraining benefits most from broad diversity in reasoning
patterns (11% avg gain), while SFT is more sensitive to data quality (15% avg
gain). We show that high-quality pretraining data has latent effects, activated
only after SFT, and that naively scaling SFT data can be detrimental, washing
away the benefits of early reasoning injection. Our results challenge the
conventional separation of language modeling and reasoning, providing a
principled guide for strategically allocating data across the entire training
pipeline to build more capable models.

</details>


### [166] [MindCraft: How Concept Trees Take Shape In Deep Models](https://arxiv.org/abs/2510.03265)
*Bowei Tian,Yexiao He,Wanghao Ye,Ziyao Wang,Meng Liu,Ang Li*

Main category: cs.LG

TL;DR: 引入MindCraft框架，基于概念树分析基础模型概念表示，跨领域评估有效，推动可解释AI发展。


<details>
  <summary>Details</summary>
Motivation: 大型基础模型虽表现出色，但概念的内部结构和稳定性不明，需深入分析。

Method: 引入基于概念树的MindCraft框架，通过每层谱分解和连接主方向构建概念路径。

Result: 在医学诊断、物理推理和政治决策等多场景评估显示，概念树能恢复语义层次、解开潜在概念，可广泛应用。

Conclusion: 概念树是强大且广泛适用的框架，推动了可解释AI基础的发展。

Abstract: Large-scale foundation models demonstrate strong performance across language,
vision, and reasoning tasks. However, how they internally structure and
stabilize concepts remains elusive. Inspired by causal inference, we introduce
the MindCraft framework built upon Concept Trees. By applying spectral
decomposition at each layer and linking principal directions into branching
Concept Paths, Concept Trees reconstruct the hierarchical emergence of
concepts, revealing exactly when they diverge from shared representations into
linearly separable subspaces. Empirical evaluations across diverse scenarios
across disciplines, including medical diagnosis, physics reasoning, and
political decision-making, show that Concept Trees recover semantic
hierarchies, disentangle latent concepts, and can be widely applied across
multiple domains. The Concept Tree establishes a widely applicable and powerful
framework that enables in-depth analysis of conceptual representations in deep
models, marking a significant step forward in the foundation of interpretable
AI.

</details>


### [167] [Variational Autoencoders-based Detection of Extremes in Plant Productivity in an Earth System Model](https://arxiv.org/abs/2510.03266)
*Bharat Sharma,Jitendra Kumar*

Main category: cs.LG

TL;DR: 研究用变分自编码器（VAE）识别美国大陆四个AR6地区总初级生产力（GPP）极端事件，与传统奇异谱分析（SSA）对比，结果显示两者极端事件频率空间模式有强区域一致性，VAE有计算优势和捕捉非线性依赖能力。


<details>
  <summary>Details</summary>
Motivation: 气候异常显著影响陆地碳循环动态，需鲁棒方法检测和分析植物生产力异常行为。

Method: 用VAE识别GPP极端事件，与SSA方法在三个时间段对比；VAE架构用三层密集层和潜在空间，基于重构误差识别异常；用第5百分位阈值定义极端事件。

Result: VAE和SSA极端事件频率空间模式有强区域一致性，但VAE阈值更高；两种方法都显示到2050 - 80年负碳循环极端事件的幅度和频率增加。

Conclusion: VAE方法与成熟的SSA技术性能相当，有计算优势和捕捉碳循环变异性中非线性时间依赖的能力，且无需定义信号周期性。

Abstract: Climate anomalies significantly impact terrestrial carbon cycle dynamics,
necessitating robust methods for detecting and analyzing anomalous behavior in
plant productivity. This study presents a novel application of variational
autoencoders (VAE) for identifying extreme events in gross primary productivity
(GPP) from Community Earth System Model version 2 simulations across four AR6
regions in the Continental United States. We compare VAE-based anomaly
detection with traditional singular spectral analysis (SSA) methods across
three time periods: 1850-80, 1950-80, and 2050-80 under the SSP585 scenario.
The VAE architecture employs three dense layers and a latent space with an
input sequence length of 12 months, trained on a normalized GPP time series to
reconstruct the GPP and identifying anomalies based on reconstruction errors.
Extreme events are defined using 5th percentile thresholds applied to both VAE
and SSA anomalies. Results demonstrate strong regional agreement between VAE
and SSA methods in spatial patterns of extreme event frequencies, despite VAE
producing higher threshold values (179-756 GgC for VAE vs. 100-784 GgC for SSA
across regions and periods). Both methods reveal increasing magnitudes and
frequencies of negative carbon cycle extremes toward 2050-80, particularly in
Western and Central North America. The VAE approach shows comparable
performance to established SSA techniques, while offering computational
advantages and enhanced capability for capturing non-linear temporal
dependencies in carbon cycle variability. Unlike SSA, the VAE method does not
require one to define the periodicity of the signals in the data; it discovers
them from the data.

</details>


### [168] [PT$^2$-LLM: Post-Training Ternarization for Large Language Models](https://arxiv.org/abs/2510.03267)
*Xianglong Yan,Chengzhu Bao,Zhiteng Li,Tianao Zhang,Kaicheng Yang,Haotong Qin,Ruobing Xie,Xingwu Sun,Yulun Zhang*

Main category: cs.LG

TL;DR: 提出针对大语言模型的后训练三元量化框架PT$^2$-LLM，实验显示其内存成本低且有端到端加速效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型内存和计算需求大阻碍部署，三元量化虽有潜力但在PTQ场景下未充分探索，有训练自由参数优化和量化难题。

Method: 提出PT$^2$-LLM框架，包含带两阶段细化管道的非对称三元量化器和基于结构相似性的重排序策略。

Result: PT$^2$-LLM与SOTA 2位PTQ方法相比性能有竞争力，内存成本低，加速预填充和解码实现端到端加速。

Conclusion: PT$^2$-LLM在大语言模型量化方面有良好效果，代码和模型将开源。

Abstract: Large Language Models (LLMs) have shown impressive capabilities across
diverse tasks, but their large memory and compute demands hinder deployment.
Ternarization has gained attention as a promising compression technique,
delivering substantial size reduction and high computational efficiency.
However, its potential in the post-training quantization (PTQ) setting remains
underexplored, due to the challenge of training-free parameter optimization and
the quantization difficulty posed by outliers and dispersed weights. To address
these issues, we propose PT$^2$-LLM, a post-training ternarization framework
tailored for LLMs. At its core is an Asymmetric Ternary Quantizer equipped with
a two-stage refinement pipeline: (1) Iterative Ternary Fitting (ITF), which
alternates between optimal ternary grid construction and flexible rounding to
minimize quantization error, and (2) Activation-aware Grid Alignment (AGA),
which further refines the ternary grid to better match full-precision outputs.
In addition, we propose a plug-and-play Structural Similarity-based Reordering
(SSR) strategy that leverages inter-column structural similarity to ease
quantization and mitigate outlier effects, further enhancing overall
performance. Extensive experiments demonstrate that PT$^2$-LLM delivers
competitive performance against state-of-the-art (SOTA) 2-bit PTQ methods with
lower memory cost, while also accelerating both prefill and decoding to achieve
end-to-end speedup. The code and models will be available at
https://github.com/XIANGLONGYAN/PT2-LLM.

</details>


### [169] [Machine Learning Workflows in Climate Modeling: Design Patterns and Insights from Case Studies](https://arxiv.org/abs/2510.03305)
*Tian Zheng,Subashree Venkatasubramanian,Shuolin Li,Amy Braverman,Xinyi Ke,Zhewen Hou,Peter Jin,Samarth Sanjay Agrawal*

Main category: cs.LG

TL;DR: 本文分析气候建模中应用机器学习的案例研究，综合工作流设计模式，旨在提供科学机器学习框架并降低跨学科合作门槛。


<details>
  <summary>Details</summary>
Motivation: 解决气候建模中物理一致性、多尺度耦合等挑战，推动机器学习在气候建模中的应用，降低数据科学与气候建模跨学科合作的门槛。

Method: 分析一系列气候建模中应用机器学习研究的案例，聚焦设计选择和工作流结构，综合不同项目的工作流设计模式。

Result: 剖析了工作流如何基于物理知识、模拟数据并整合观测。

Conclusion: 可通过更透明的模型开发、评估、适应和可重复性来确保科学机器学习的严谨性，有助于跨学科合作。

Abstract: Machine learning has been increasingly applied in climate modeling on system
emulation acceleration, data-driven parameter inference, forecasting, and
knowledge discovery, addressing challenges such as physical consistency,
multi-scale coupling, data sparsity, robust generalization, and integration
with scientific workflows. This paper analyzes a series of case studies from
applied machine learning research in climate modeling, with a focus on design
choices and workflow structure. Rather than reviewing technical details, we aim
to synthesize workflow design patterns across diverse projects in ML-enabled
climate modeling: from surrogate modeling, ML parameterization, probabilistic
programming, to simulation-based inference, and physics-informed transfer
learning. We unpack how these workflows are grounded in physical knowledge,
informed by simulation data, and designed to integrate observations. We aim to
offer a framework for ensuring rigor in scientific machine learning through
more transparent model development, critical evaluation, informed adaptation,
and reproducibility, and to contribute to lowering the barrier for
interdisciplinary collaboration at the interface of data science and climate
modeling.

</details>


### [170] [Decrypt Modality Gap in Multimodal Contrastive Learning: From Convergent Representation to Pair Alignment](https://arxiv.org/abs/2510.03268)
*Lingjie Yi,Raphael Douady,Chao Chen*

Main category: cs.LG

TL;DR: 提出分析多模态对比学习收敛最优表示和模态对齐的理论框架，确定维度坍塌是模态差距根源，给出消除差距的方法。


<details>
  <summary>Details</summary>
Motivation: 多模态对比学习存在模态差距现象，且差距大小对下游性能影响的实验结果不一致，需探究模态差距成因及对下游任务的影响。

Method: 引入理论框架进行分析和证明，研究不同约束条件下模态差距的收敛情况。

Result: 无约束或锥约束下模态差距收敛到零，子空间约束下收敛到两超平面最小夹角，确定维度坍塌是根源，证明配对样本在子空间约束下无法完美对齐，模态差距影响样本对的对齐。

Conclusion: 在子空间约束下可通过超平面旋转和共享空间投影实现两模态的完美对齐。

Abstract: Multimodal contrastive learning (MCL) aims to embed data from different
modalities in a shared embedding space. However, empirical evidence shows that
representations from different modalities occupy completely separate regions of
embedding space, a phenomenon referred to as the modality gap. Moreover,
experimental findings on how the size of the modality gap influences downstream
performance are inconsistent. These observations raise two key questions: (1)
What causes the modality gap? (2) How does it affect downstream tasks? To
address these questions, this paper introduces the first theoretical framework
for analyzing the convergent optimal representations of MCL and the modality
alignment when training is optimized. Specifically, we prove that without any
constraint or under the cone constraint, the modality gap converges to zero.
Under the subspace constraint (i.e., representations of two modalities fall
into two distinct hyperplanes due to dimension collapse), the modality gap
converges to the smallest angle between the two hyperplanes. This result
identifies \emph{dimension collapse} as the fundamental origin of the modality
gap. Furthermore, our theorems demonstrate that paired samples cannot be
perfectly aligned under the subspace constraint. The modality gap influences
downstream performance by affecting the alignment between sample pairs. We
prove that, in this case, perfect alignment between two modalities can still be
achieved via two ways: hyperplane rotation and shared space projection.

</details>


### [171] [Estimating link level traffic emissions: enhancing MOVES with open-source data](https://arxiv.org/abs/2510.03362)
*Lijiao Wang,Muhammad Usama,Haris N. Koutsopoulos,Zhengbing He*

Main category: cs.LG

TL;DR: 本文提出数据驱动框架，整合多源开源数据估算车辆排放，在波士顿地区应用，相比基线模型降低关键污染物排放RMSE超50%，证明低成本排放估算可行性。


<details>
  <summary>Details</summary>
Motivation: 利用开源数据为城市地区车辆活动和排放估算提供可扩展、透明基础。

Method: 提出整合MOVES、开源GPS轨迹数据、OSM道路网络、区域交通数据集和卫星影像特征向量的数据驱动框架，训练神经网络模型预测运行模式分布。

Result: 在波士顿45个城市应用，与MOVES基线相比，关键污染物区域交通排放RMSE降低超50%。

Conclusion: 证明使用完全开源数据源进行低成本、可复制、数据驱动的排放估算是可行的。

Abstract: Open-source data offers a scalable and transparent foundation for estimating
vehicle activity and emissions in urban regions. In this study, we propose a
data-driven framework that integrates MOVES and open-source GPS trajectory
data, OpenStreetMap (OSM) road networks, regional traffic datasets and
satellite imagery-derived feature vectors to estimate the link level operating
mode distribution and traffic emissions. A neural network model is trained to
predict the distribution of MOVES-defined operating modes using only features
derived from readily available data. The proposed methodology was applied using
open-source data related to 45 municipalities in the Boston Metropolitan area.
The "ground truth" operating mode distribution was established using OSM
open-source GPS trajectories. Compared to the MOVES baseline, the proposed
model reduces RMSE by over 50% for regional scale traffic emissions of key
pollutants including CO, NOx, CO2, and PM2.5. This study demonstrates the
feasibility of low-cost, replicable, and data-driven emissions estimation using
fully open data sources.

</details>


### [172] [General Exploratory Bonus for Optimistic Exploration in RLHF](https://arxiv.org/abs/2510.03269)
*Wendi Li,Changdae Oh,Yixuan Li*

Main category: cs.LG

TL;DR: 现有强化学习含人类反馈探索奖励方法难实现乐观性，提出GEB框架解决此问题且表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有探索奖励方法难以实现乐观性，当前公式会使探索偏向参考模型高概率区域，强化保守行为。

Method: 引入通用探索奖励（GEB）理论框架，通过依赖参考的奖励调节抵消偏差，统一先前启发式奖励。

Result: GEB在多种散度设置和大语言模型骨干的对齐任务中始终优于基线。

Conclusion: GEB为基于人类反馈的强化学习中的乐观探索提供了原则性和实用性兼备的解决方案。

Abstract: Optimistic exploration is central to improving sample efficiency in
reinforcement learning with human feedback, yet existing exploratory bonus
methods to incentivize exploration often fail to realize optimism. We provide a
theoretical analysis showing that current formulations, under KL or
$\alpha$-divergence regularization, unintentionally bias exploration toward
high-probability regions of the reference model, thereby reinforcing
conservative behavior instead of promoting discovery of uncertain regions. To
address this pitfall, we introduce the General Exploratory Bonus (GEB), a novel
theoretical framework that provably satisfies the optimism principle. GEB
counteracts divergence-induced bias via reference-dependent reward regulation
and unifies prior heuristic bonuses as special cases, while extending naturally
across the full $\alpha$-divergence family. Empirically, GEB consistently
outperforms baselines on alignment tasks across multiple divergence settings
and large language model backbones. These results demonstrate that GEB offers
both a principled and practical solution for optimistic exploration in RLHF.

</details>


### [173] [CoDA: Coding LM via Diffusion Adaptation](https://arxiv.org/abs/2510.03270)
*Haolin Chen,Shiyu Wang,Can Qin,Bo Pang,Zuxin Liu,Jielin Qiu,Jianguo Zhang,Yingbo Zhou,Zeyuan Chen,Ran Xu,Shelby Heinecke,Silvio Savarese,Caiming Xiong,Huan Wang,Weiran Yao*

Main category: cs.LG

TL;DR: 介绍17亿参数扩散编码器CoDA，在多数据集表现好，还发布相关资源加速轻量级扩散编码助手研究。


<details>
  <summary>Details</summary>
Motivation: 现有扩散语言模型实用系统较重，需要轻量级方案。

Method: 训练CoDA，结合大规模扩散预训练、以代码为中心的中间训练和指令微调，使用置信度引导采样。

Result: CoDA - 1.7B - Instruct在Humaneval、MBPP和EvalPlus上达到或超越70亿参数扩散模型。

Conclusion: 发布模型检查点、评估工具和TPU训练管道，可加速轻量级扩散编码助手研究。

Abstract: Diffusion language models promise bidirectional context and infilling
capabilities that autoregressive coders lack, yet practical systems remain
heavyweight. We introduce CoDA, a 1.7B-parameter diffusion coder trained on TPU
with a fully open-source training pipeline. CoDA pairs large-scale diffusion
pre-training with code-centric mid-training and instruction tuning, enabling
confidence-guided sampling that keeps inference latency competitive. On
Humaneval, MBPP, and EvalPlus, CoDA-1.7B-Instruct matches or surpasses
diffusion models up to 7B parameters. Our release includes model checkpoints,
evaluation harnesses, and TPU training pipelines to accelerate research on
lightweight diffusion-based coding assistants.

</details>


### [174] [Multi-task neural diffusion processes for uncertainty-quantified wind power prediction](https://arxiv.org/abs/2510.03419)
*Joseph Rawson,Domniki Ladopoulou,Petros Dellaportas*

Main category: cs.LG

TL;DR: 本文将神经扩散过程扩展为多任务框架用于风电功率预测，在实际数据上评估，该框架表现出色，能提供适用于运营的预测。


<details>
  <summary>Details</summary>
Motivation: 不确定性感知的风电功率预测对电网集成和可靠风电场运营至关重要。

Method: 应用神经扩散过程（NDPs）并扩展为多任务NDP（MT - NDP）框架，在MT - NDP中引入任务编码器捕捉跨涡轮机相关性。

Result: MT - NDP框架在点精度和校准方面优于单任务NDP和GP，尤其对于行为偏离机群平均的风机。

Conclusion: 基于NDP的模型能提供适用于运营部署的校准和可扩展预测，其预测区间可支持风电场调度和维护决策。

Abstract: Uncertainty-aware wind power prediction is essential for grid integration and
reliable wind farm operation. We apply neural diffusion processes (NDPs)-a
recent class of models that learn distributions over functions-and extend them
to a multi-task NDP (MT-NDP) framework for wind power prediction. We provide
the first empirical evaluation of NDPs in real supervisory control and data
acquisition (SCADA) data. We introduce a task encoder within MT-NDPs to capture
cross-turbine correlations and enable few-shot adaptation to unseen turbines.
The proposed MT-NDP framework outperforms single-task NDPs and GPs in terms of
point accuracy and calibration, particularly for wind turbines whose behaviour
deviates from the fleet average. In general, NDP-based models deliver
calibrated and scalable predictions suitable for operational deployment,
offering sharper, yet trustworthy, predictive intervals that can support
dispatch and maintenance decisions in modern wind farms.

</details>


### [175] [Decision Potential Surface: A Theoretical and Practical Approximation of LLM's Decision Boundary](https://arxiv.org/abs/2510.03271)
*Zi Liang,Zhiyao Wu,Haoyang Shang,Yulin Jin,Qingqing Ye,Huadi Zheng,Peizhao Hu,Haibo Hu*

Main category: cs.LG

TL;DR: 本文提出决策势面（DPS）概念分析大语言模型决策边界，提出K - DPS近似算法，理论推导误差上界并实验验证。


<details>
  <summary>Details</summary>
Motivation: 分析大语言模型决策边界受关注，但因词汇序列规模大与自回归特性，构建主流大语言模型决策边界计算不可行。

Method: 提出DPS概念，证明其零高度等值线与决策边界等价；提出K - DPS近似算法，理论推导K - DPS与理想DPS误差上界。

Result: 算法只需有限次序列采样就能以可忽略误差近似决策边界，误差可与采样次数权衡，实验验证结果。

Conclusion: DPS和K - DPS方法可有效解决大语言模型决策边界构建的计算难题。

Abstract: Decision boundary, the subspace of inputs where a machine learning model
assigns equal classification probabilities to two classes, is pivotal in
revealing core model properties and interpreting behaviors. While analyzing the
decision boundary of large language models (LLMs) has raised increasing
attention recently, constructing it for mainstream LLMs remains computationally
infeasible due to the enormous vocabulary-sequence sizes and the
auto-regressive nature of LLMs. To address this issue, in this paper we propose
Decision Potential Surface (DPS), a new notion for analyzing LLM decision
boundary. DPS is defined on the confidences in distinguishing different
sampling sequences for each input, which naturally captures the potential of
decision boundary. We prove that the zero-height isohypse in DPS is equivalent
to the decision boundary of an LLM, with enclosed regions representing decision
regions. By leveraging DPS, for the first time in the literature, we propose an
approximate decision boundary construction algorithm, namely $K$-DPS, which
only requires K-finite times of sequence sampling to approximate an LLM's
decision boundary with negligible error. We theoretically derive the upper
bounds for the absolute error, expected error, and the error concentration
between K-DPS and the ideal DPS, demonstrating that such errors can be
trade-off with sampling times. Our results are empirically validated by
extensive experiments across various LLMs and corpora.

</details>


### [176] [Consistent Kernel Change-Point Detection under m-Dependence for Text Segmentation](https://arxiv.org/abs/2510.03437)
*Jairo Diaz-Rodriguez,Mumin Jia*

Main category: cs.LG

TL;DR: 本文为KCPD在m依赖数据下建立新保证，通过模拟验证渐近性，开展实证研究，证明其在文本分割任务有效。


<details>
  <summary>Details</summary>
Motivation: 现有KCPD理论在独立性假设下建立一致性，但现实序列数据如文本有强依赖性，需为KCPD在m依赖数据下建立新保证。

Method: 为KCPD在m依赖数据下建立理论保证，进行基于大语言模型的模拟生成合成m依赖文本验证渐近性，开展KCPD用于文本分割的实证研究。

Result: 证明了检测到的变化点数量的一致性和位置的弱一致性，KCPD在文本分割指标上优于基线，案例研究显示其实际有效性。

Conclusion: KCPD不仅有理论和模拟可靠性，对文本分割任务也有实际有效性。

Abstract: Kernel change-point detection (KCPD) has become a widely used tool for
identifying structural changes in complex data. While existing theory
establishes consistency under independence assumptions, real-world sequential
data such as text exhibits strong dependencies. We establish new guarantees for
KCPD under $m$-dependent data: specifically, we prove consistency in the number
of detected change points and weak consistency in their locations under mild
additional assumptions. We perform an LLM-based simulation that generates
synthetic $m$-dependent text to validate the asymptotics. To complement these
results, we present the first comprehensive empirical study of KCPD for text
segmentation with modern embeddings. Across diverse text datasets, KCPD with
text embeddings outperforms baselines in standard text segmentation metrics. We
demonstrate through a case study on Taylor Swift's tweets that KCPD not only
provides strong theoretical and simulated reliability but also practical
effectiveness for text segmentation tasks.

</details>


### [177] [PDE-Transformer: A Continuous Dynamical Systems Approach to Sequence Modeling](https://arxiv.org/abs/2510.03272)
*Yukun Zhang,Xueqing Zhou*

Main category: cs.LG

TL;DR: 本文提出新分析框架，将Transformer结构视为连续时空动力系统，用PDE分析组件数学必要性，实验证明残差连接和层归一化是基础稳定器。


<details>
  <summary>Details</summary>
Motivation: 缺乏对Transformer内部机制的理论理解，旨在用新方法分析其组件数学必要性。

Method: 引入新分析框架，将Transformer结构转化为PDE系统，用数学算子映射组件，对比含与不含稳定器的系统。

Result: 实验表明无残差连接系统有灾难性表示漂移，无层归一化训练动力学不稳定、爆炸。

Conclusion: 残差连接和层归一化是驯服连续系统的基础数学稳定器，为Transformer设计提供原理性解释，建立新分析范式。

Abstract: The Transformer architecture has revolutionized artificial intelligence, yet
a principled theoretical understanding of its internal mechanisms remains
elusive. This paper introduces a novel analytical framework that
reconceptualizes the Transformer's discrete, layered structure as a continuous
spatiotemporal dynamical system governed by a master Partial Differential
Equation (PDE). Within this paradigm, we map core architectural components to
distinct mathematical operators: self-attention as a non-local interaction, the
feed-forward network as a local reaction, and, critically, residual connections
and layer normalization as indispensable stabilization mechanisms. We do not
propose a new model, but rather employ the PDE system as a theoretical probe to
analyze the mathematical necessity of these components. By comparing a standard
Transformer with a PDE simulator that lacks explicit stabilizers, our
experiments provide compelling empirical evidence for our central thesis. We
demonstrate that without residual connections, the system suffers from
catastrophic representational drift, while the absence of layer normalization
leads to unstable, explosive training dynamics. Our findings reveal that these
seemingly heuristic "tricks" are, in fact, fundamental mathematical stabilizers
required to tame an otherwise powerful but inherently unstable continuous
system. This work offers a first-principles explanation for the Transformer's
design and establishes a new paradigm for analyzing deep neural networks
through the lens of continuous dynamics.

</details>


### [178] [Learning without Global Backpropagation via Synergistic Information Distillation](https://arxiv.org/abs/2510.03273)
*Chenhao Ye,Ming Tang*

Main category: cs.LG

TL;DR: 提出协同信息蒸馏（SID）训练框架解决BP扩展性瓶颈，有理论基础，实验表现好。


<details>
  <summary>Details</summary>
Motivation: 解决BP算法的更新锁定和高内存消耗两个扩展性瓶颈问题。

Method: 将深度学习重新定义为局部协作优化问题级联，网络模块结构化处理，模块有局部目标，解耦模块间反向依赖。

Result: 理论上保证网络深度增加时性能单调提升，实验上分类准确率与BP相当或更好，扩展性和抗标签噪声能力强。

Conclusion: SID可消除更新锁定、降低内存需求，是BP的通用替代方案。

Abstract: Backpropagation (BP), while foundational to deep learning, imposes two
critical scalability bottlenecks: update locking, where network modules remain
idle until the entire backward pass completes, and high memory consumption due
to storing activations for gradient computation. To address these limitations,
we introduce Synergistic Information Distillation (SID), a novel training
framework that reframes deep learning as a cascade of local cooperative
refinement problems. In SID, a deep network is structured as a pipeline of
modules, each imposed with a local objective to refine a probabilistic belief
about the ground-truth target. This objective balances fidelity to the target
with consistency to the belief from its preceding module. By decoupling the
backward dependencies between modules, SID enables parallel training and hence
eliminates update locking and drastically reduces memory requirements.
Meanwhile, this design preserves the standard feed-forward inference pass,
making SID a versatile drop-in replacement for BP. We provide a theoretical
foundation, proving that SID guarantees monotonic performance improvement with
network depth. Empirically, SID consistently matches or surpasses the
classification accuracy of BP, exhibiting superior scalability and pronounced
robustness to label noise.Code is available at:
https://github.com/ychAlbert/sid-bp

</details>


### [179] [On residual network depth](https://arxiv.org/abs/2510.03470)
*Benoit Dherin,Michael Munn*

Main category: cs.LG

TL;DR: 本文给出解析公式验证残差网络的集成视角，解释归一化层的必要性及无归一化技术，提出缩放残差模块的原则性解决方案。


<details>
  <summary>Details</summary>
Motivation: 深入理解深度残差架构中深度为何如此有效，解释归一化层的历史依赖。

Method: 推导显式解析公式，提出残差扩展定理。

Result: 证明增加网络深度在数学上等同于扩大隐式集成的规模，揭示层次化集成结构，解释归一化层必要性，提出缩放残差模块的解决方案。

Conclusion: 缩放残差模块是控制组合爆炸的原则性方法，可隐式正则化模型复杂度。

Abstract: Deep residual architectures, such as ResNet and the Transformer, have enabled
models of unprecedented depth, yet a formal understanding of why depth is so
effective remains an open question. A popular intuition, following Veit et al.
(2016), is that these residual networks behave like ensembles of many shallower
models. Our key finding is an explicit analytical formula that verifies this
ensemble perspective, proving that increasing network depth is mathematically
equivalent to expanding the size of this implicit ensemble. Furthermore, our
expansion reveals a hierarchical ensemble structure in which the combinatorial
growth of computation paths leads to an explosion in the output signal,
explaining the historical necessity of normalization layers in training deep
models. This insight offers a first principles explanation for the historical
dependence on normalization layers and sheds new light on a family of
successful normalization-free techniques like SkipInit and Fixup. However,
while these previous approaches infer scaling factors through optimizer
analysis or a heuristic analogy to Batch Normalization, our work offers the
first explanation derived directly from the network's inherent functional
structure. Specifically, our Residual Expansion Theorem reveals that scaling
each residual module provides a principled solution to taming the combinatorial
explosion inherent to these architectures. We further show that this scaling
acts as a capacity controls that also implicitly regularizes the model's
complexity.

</details>


### [180] [Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models](https://arxiv.org/abs/2510.03274)
*Tianao Zhang,Zhiteng Li,Xianglong Yan,Haotong Qin,Yong Guo,Yulun Zhang*

Main category: cs.LG

TL;DR: 提出针对扩散大语言模型（dLLMs）的超低比特后训练量化框架Quant - dLLM，在2比特精度下比现有方法更准确。


<details>
  <summary>Details</summary>
Motivation: dLLMs模型规模持续增长需权重压缩，直接将后训练量化（PTQ）应用于dLLMs 2比特时性能不佳。

Method: 提出Quant - dLLM框架，引入Masked Calibration Simulation（MCS）使校准与依赖时间步的掩码对齐；提出Data - aware Any - order Quantizer（DAQ）学习超低比特权重表示；引入Adaptive Blockwise Mixed Precision（ABMP）进行自适应位宽分配。

Result: 在2比特精度下，Quant - dLLM在dLLMs上始终比最先进的AR迁移PTQ方法具有更高的准确性。

Conclusion: Quant - dLLM是适用于dLLMs的有效超低比特PTQ框架。

Abstract: Diffusion large language models (dLLMs), which offer bidirectional context
and flexible masked-denoising generation, are emerging as a compelling
alternative to autoregressive (AR) LLMs. However, like AR LLMs, their model
sizes continue to grow, motivating weight compression for deployment. Although
post-training quantization (PTQ) is effective for AR LLMs, directly
transferring it to dLLMs at 2-bit leads to unsatisfactory performance. To
tackle these challenges, we propose Quant-dLLM, an ultra-low-bit PTQ framework
tailored to dLLMs. Since masked-denoising activations in dLLMs differ from the
fully visible signals assumed by standard PTQ methods, we introduce Masked
Calibration Simulation (MCS) to align calibration with the timestep-dependent
masking, which yields more reliable calibrations. Moreover, we propose a
Data-aware Any-order Quantizer (DAQ) that learns ultra-low-bit weight
representations via an optimization algorithm. It performs iterative
approximation guided by our simulated calibration data. In addition, under a
strict 2-bit budget, we introduce Adaptive Blockwise Mixed Precision (ABMP), a
sensitivity-based precision allocation scheme that adaptively assigns bit width
across channel groups. When restricted to 2-bit precision, Quant-dLLM
consistently achieves higher accuracy than state-of-the-art (SOTA) AR-transfer
PTQ methods on dLLMs. The code and models will be available at:
https://github.com/ZTA2785/Quant-dLLM.

</details>


### [181] [Trajectory Data Suffices for Statistically Efficient Policy Evaluation in Finite-Horizon Offline RL with Linear $q^π$-Realizability and Concentrability](https://arxiv.org/abs/2510.03494)
*Volodymyr Tkachuk,Csaba Szepesvári,Xiaoqi Tan*

Main category: cs.LG

TL;DR: 研究有限时域离线强化学习中策略评估和优化，提出策略评估的高效学习器并改进策略优化学习器样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 此前研究表明在特定假设下统计高效学习在策略评估和优化问题上难以实现，近期有针对策略优化的高效学习器，本文旨在解决策略评估的高效学习及改进策略优化学习器的样本复杂度。

Method: 提出策略评估的统计高效学习器，并对Tkachuk等人用于策略优化的学习器进行更严格分析。

Result: 得到了策略评估的统计高效学习器，且改进了策略优化学习器的样本复杂度。

Conclusion: 在给定数据为轨迹的假设下，可实现策略评估的统计高效学习，同时能通过更严格分析改进策略优化学习器的样本复杂度。

Abstract: We study finite-horizon offline reinforcement learning (RL) with function
approximation for both policy evaluation and policy optimization. Prior work
established that statistically efficient learning is impossible for either of
these problems when the only assumptions are that the data has good coverage
(concentrability) and the state-action value function of every policy is
linearly realizable ($q^\pi$-realizability) (Foster et al., 2021). Recently,
Tkachuk et al. (2024) gave a statistically efficient learner for policy
optimization, if in addition the data is assumed to be given as trajectories.
In this work we present a statistically efficient learner for policy evaluation
under the same assumptions. Further, we show that the sample complexity of the
learner used by Tkachuk et al. (2024) for policy optimization can be improved
by a tighter analysis.

</details>


### [182] [SDQ-LLM: Sigma-Delta Quantization for 1-bit LLMs of any size](https://arxiv.org/abs/2510.03275)
*Junhao Xia,Ming Zhao,Limin Xiao,Xiujun Zhang*

Main category: cs.LG

TL;DR: 提出SDQ - LLM框架用于大语言模型极低比特量化，有OSR连续可调特性，结合多种策略提升性能，实验证明其高效高精度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型面临计算和内存挑战，极低比特量化对其高效部署至关重要。

Method: 引入SDQ - LLM框架，利用上采样和Sigma - Delta量化器二值或三值化权重；量化前采用Hadamard权重平滑；提出MultiOSR细粒度OSR分配策略。

Result: 在OPT和LLaMA模型家族上的实验表明，SDQ - LLM即使在低OSR设置下也能实现更高效和高精度的性能。

Conclusion: SDQ - LLM是一种有效的大语言模型极低比特量化框架，可在内存约束下实现模型大小和精度的最优权衡。

Abstract: Large language models (LLMs) face significant computational and memory
challenges, making extremely low-bit quantization crucial for their efficient
deployment. In this work, we introduce SDQ-LLM: Sigma-Delta Quantization for
1-bit LLMs of any size, a novel framework that enables extremely low-bit
quantization of LLMs while preserving their linguistic reasoning capabilities.
A distinctive feature of SDQ-LLM is the continuous adjustability of the
Over-Sampling Ratio (OSR), enabling dynamic adaptation to memory or VRAM
constraints by selecting fractional OSR (e.g. 2.5 times) for an optimal
trade-off between model size and accuracy. SDQ-LLM uses upsampling combined
with Sigma-Delta Quantizer to binarize or ternarize LLMs weights, encoding
high-precision parameters into 1-bit or 1.58-bit representations, replacing the
multiplication operations within linear layers with addition. This approach
significantly enhances inference efficiency under extremely low-bit
quantization. To further reduce the loss of quantization precision, we
incorporate Hadamard-based weight smoothing prior to quantization, improving
the stability and robustness of the weight representations. Furthermore, to
fully leverage the continuity of the OSR and reduce precision loss, recognizing
the correlation between quantization sensitivity and weight variance, we
propose a fine-grained, layer- and linear-wise OSR allocation strategy,
MultiOSR. This strategy distributes OSR both across layers and within each
layer, based on weight variance and parameter scale. Finally, extensive
experiments on OPT and LLaMA model families demonstrate that SDQ-LLM achieves a
more efficient and high-precision performance even under highly aggressive
low-OSR settings. Our code is available at
https://github.com/Dreamlittlecat/LLM-Quant-Factory.

</details>


### [183] [QuadEnhancer: Leveraging Quadratic Transformations to Enhance Deep Neural Networks](https://arxiv.org/abs/2510.03276)
*Qian Chen,Linxin Yang,Akang Wang,Xiaodong Luo,Yin Zhang*

Main category: cs.LG

TL;DR: 本文探索引入二次变换增加神经网络非线性，提出轻量级二次增强器，实验显示在多任务中有性能提升。


<details>
  <summary>Details</summary>
Motivation: 为现有神经网络架构引入二次变换以增加非线性，提升性能。

Method: 提出使用低秩、权重共享和稀疏化技术的轻量级二次增强器，在每一层引入特征间的二次交互。

Result: 在图像分类、文本分类和微调大语言模型三个任务的实验中，该方法有明显和显著的性能提升。

Conclusion: 引入二次变换的轻量级增强器能有效提升神经网络在不同任务中的性能。

Abstract: The combination of linear transformations and non-linear activation functions
forms the foundation of most modern deep neural networks, enabling them to
approximate highly complex functions. This paper explores the introduction of
quadratic transformations to further increase nonlinearity in neural networks,
with the aim of enhancing the performance of existing architectures. To reduce
parameter complexity and computational complexity, we propose a lightweight
quadratic enhancer that uses low-rankness, weight sharing, and sparsification
techniques. For a fixed architecture, the proposed approach introduces
quadratic interactions between features at every layer, while only adding
negligible amounts of additional model parameters and forward computations. We
conduct a set of proof-of-concept experiments for the proposed method across
three tasks: image classification, text classification, and fine-tuning
large-language models. In all tasks, the proposed approach demonstrates clear
and substantial performance gains.

</details>


### [184] [Quantifying constraint hierarchies in Bayesian PINNs via per-constraint Hessian decomposition](https://arxiv.org/abs/2510.03278)
*Filip Landgren*

Main category: cs.LG

TL;DR: 本文引入可扩展无矩阵Laplace框架分析贝叶斯物理信息神经网络（B - PINNs）中单个物理约束对网络的影响，以范德波尔方程为例展示改变单个损失权重的效果。


<details>
  <summary>Details</summary>
Motivation: B - PINNs中物理约束对网络的影响尚不明确，需要进一步阐明单个物理约束如何塑造这些网络。

Method: 引入可扩展无矩阵Laplace框架，将后验Hessian矩阵分解为每个约束的贡献，并提供量化其对损失景观相对影响的指标。

Result: 应用于范德波尔方程，该方法追踪了约束如何塑造网络几何结构，直接通过Hessian矩阵展示了改变单个损失权重如何非平凡地重新分配曲率和有效主导地位。

Conclusion: 所提出的框架有助于理解B - PINNs中单个物理约束对网络的影响。

Abstract: Bayesian physics-informed neural networks (B-PINNs) merge data with governing
equations to solve differential equations under uncertainty. However,
interpreting uncertainty and overconfidence in B-PINNs requires care due to the
poorly understood effects the physical constraints have on the network;
overconfidence could reflect warranted precision, enforced by the constraints,
rather than miscalibration. Motivated by the need to further clarify how
individual physical constraints shape these networks, we introduce a scalable,
matrix-free Laplace framework that decomposes the posterior Hessian into
contributions from each constraint and provides metrics to quantify their
relative influence on the loss landscape. Applied to the Van der Pol equation,
our method tracks how constraints sculpt the network's geometry and shows,
directly through the Hessian, how changing a single loss weight non-trivially
redistributes curvature and effective dominance across the others.

</details>


### [185] [Sequential decoder training for improved latent space dynamics identification](https://arxiv.org/abs/2510.03535)
*William Anderson,Seung Whan Chung,Youngsoo Choi*

Main category: cs.LG

TL;DR: 提出多阶段LaSDI（mLaSDI）框架，用于改进偏微分方程降阶模型的重建和预测精度，在1D - 1V Vlasov方程上表现优于标准LaSDI。


<details>
  <summary>Details</summary>
Motivation: 偏微分方程精确数值解计算成本高，需要降阶模型，但现有LaSDI框架在训练时强制执行潜在动力学可能影响重建精度。

Method: 引入多阶段LaSDI（mLaSDI）框架，通过顺序学习额外解码器来纠正前一阶段的残差误差。

Result: 应用于1D - 1V Vlasov方程时，mLaSDI始终优于标准LaSDI，在多种架构下实现更低的预测误差和更短的训练时间。

Conclusion: mLaSDI框架有效提高了偏微分方程降阶模型的重建和预测精度。

Abstract: Accurate numerical solutions of partial differential equations are essential
in many scientific fields but often require computationally expensive solvers,
motivating reduced-order models (ROMs). Latent Space Dynamics Identification
(LaSDI) is a data-driven ROM framework that combines autoencoders with equation
discovery to learn interpretable latent dynamics. However, enforcing latent
dynamics during training can compromise reconstruction accuracy of the model
for simulation data. We introduce multi-stage LaSDI (mLaSDI), a framework that
improves reconstruction and prediction accuracy by sequentially learning
additional decoders to correct residual errors from previous stages. Applied to
the 1D-1V Vlasov equation, mLaSDI consistently outperforms standard LaSDI,
achieving lower prediction errors and reduced training time across a wide range
of architectures.

</details>


### [186] [MemMamba: Rethinking Memory Patterns in State Space Model](https://arxiv.org/abs/2510.03279)
*Youjin Wang,Yangjingyi Chen,Jiahao Yan,Jiaxuan Lu,Xiao Sun*

Main category: cs.LG

TL;DR: 现有长序列建模方法存在效率与内存权衡问题，本文分析Mamba内存衰减机制，提出MemMamba框架，在长序列基准测试中表现出色，实现复杂度 - 内存权衡突破。


<details>
  <summary>Details</summary>
Motivation: 现有长序列建模方法存在效率与内存的权衡问题，Mamba虽高效但长程记忆指数衰减，需探究其记忆本质与信息保留方式。

Method: 进行数学推导和信息论分析，引入水平 - 垂直记忆保真度指标，提出集成状态总结机制、跨层和跨令牌注意力的MemMamba架构。

Result: MemMamba在PG19和Passkey Retrieval等长序列基准测试中优于现有Mamba变体和Transformers，推理效率提升48%。

Conclusion: MemMamba实现了复杂度 - 内存权衡的突破，为超长序列建模提供新范式。

Abstract: With the explosive growth of data, long-sequence modeling has become
increasingly important in tasks such as natural language processing and
bioinformatics. However, existing methods face inherent trade-offs between
efficiency and memory. Recurrent neural networks suffer from gradient vanishing
and explosion, making them hard to scale. Transformers can model global
dependencies but are constrained by quadratic complexity. Recently, selective
state-space models such as Mamba have demonstrated high efficiency with O(n)
time and O(1) recurrent inference, yet their long-range memory decays
exponentially. In this work, we conduct mathematical derivations and
information-theoretic analysis to systematically uncover the memory decay
mechanism of Mamba, answering a fundamental question: what is the nature of
Mamba's long-range memory and how does it retain information? To quantify key
information loss, we further introduce horizontal-vertical memory fidelity
metrics that capture degradation both within and across layers. Inspired by how
humans distill and retain salient information when reading long documents, we
propose MemMamba, a novel architectural framework that integrates state
summarization mechanism together with cross-layer and cross-token attention,
which alleviates long-range forgetting while preserving linear complexity.
MemMamba achieves significant improvements over existing Mamba variants and
Transformers on long-sequence benchmarks such as PG19 and Passkey Retrieval,
while delivering a 48% speedup in inference efficiency. Both theoretical
analysis and empirical results demonstrate that MemMamba achieves a
breakthrough in the complexity-memory trade-off, offering a new paradigm for
ultra-long sequence modeling.

</details>


### [187] [Longitudinal Flow Matching for Trajectory Modeling](https://arxiv.org/abs/2510.03569)
*Mohammad Mohaiminul Islam,Thijs P. Kuipers,Sharvaree Vadgama,Coen de Vente,Afsana Khan,Clara I. Sánchez,Erik J. Bekkers*

Main category: cs.LG

TL;DR: 提出Interpolative Multi - Marginal Flow Matching (IMMFM)框架学习连续随机动力学，在实验中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 生成模型在处理稀疏采样和高维轨迹时存在困难，通常将动力学学习简化为成对转换。

Method: 采用分段二次插值路径作为流匹配的平滑目标，联合优化漂移和数据驱动的扩散系数，并基于稳定学习的理论条件。

Result: 在合成基准和真实世界纵向神经影像数据集实验中，IMMFM在预测准确性和下游任务中优于现有方法。

Conclusion: IMMFM能捕获内在随机性，处理不规则稀疏采样，生成特定对象的轨迹，是一种有效的方法。

Abstract: Generative models for sequential data often struggle with sparsely sampled
and high-dimensional trajectories, typically reducing the learning of dynamics
to pairwise transitions. We propose \textit{Interpolative Multi-Marginal Flow
Matching} (IMMFM), a framework that learns continuous stochastic dynamics
jointly consistent with multiple observed time points. IMMFM employs a
piecewise-quadratic interpolation path as a smooth target for flow matching and
jointly optimizes drift and a data-driven diffusion coefficient, supported by a
theoretical condition for stable learning. This design captures intrinsic
stochasticity, handles irregular sparse sampling, and yields subject-specific
trajectories. Experiments on synthetic benchmarks and real-world longitudinal
neuroimaging datasets show that IMMFM outperforms existing methods in both
forecasting accuracy and further downstream tasks.

</details>


### [188] [Training Optimal Large Diffusion Language Models](https://arxiv.org/abs/2510.03280)
*Jinjie Ni,Qian Liu,Chao Du,Longxu Dou,Hang Yan,Zili Wang,Tianyu Pang,Michael Qizhe Shieh*

Main category: cs.LG

TL;DR: 介绍扩散语言模型（DLMs）的首个系统缩放定律Quokka，涵盖计算和数据受限情况，为DLMs训练提供短期指导，为AI社区提供长期启发。


<details>
  <summary>Details</summary>
Motivation: 为扩散语言模型（DLMs）建立系统缩放定律，为DLMs训练和AI社区发展提供指引。

Method: 文中未明确提及。

Result: 提出Quokka这一系统缩放定律，它是Chinchilla的有益补充且适用范围更广。

Conclusion: 研究结果能为DLMs训练提供短期实用指导，为整个AI社区带来长期启发。

Abstract: We introduce Quokka, the first systematic scaling law for diffusion language
models (DLMs), encompassing both compute-constrained and data-constrained
regimes, and studying the key modeling and optimization designs. Quokka is a
good friend of Chinchilla and provides wider scopes. We hope the results would
bring short-term practical guidance in DLMs training and long-term inspirations
for the whole AI community.

</details>


### [189] [BEKAN: Boundary condition-guaranteed evolutionary Kolmogorov-Arnold networks with radial basis functions for solving PDE problems](https://arxiv.org/abs/2510.03576)
*Bongseok Kim,Jiahao Zhang,Guang Lin*

Main category: cs.LG

TL;DR: 提出边界条件保证的进化KAN网络BEKAN解PDE，经实验验证其准确性优于MLP和B - splines KAN。


<details>
  <summary>Details</summary>
Motivation: 深度学习解PDE时，神经网络黑盒特性阻碍边界条件精确执行。

Method: 提出三种结合不同边界条件的方法，利用边界嵌入RBF、周期层和进化框架进行PDE模拟。

Result: 对多种边界值问题的数值实验表明，BEKAN在准确性上优于MLP和B - splines KAN。

Conclusion: 该方法增强了KAN解决PDE问题并满足边界条件的能力，推动科研计算和工程应用发展。

Abstract: Deep learning has gained attention for solving PDEs, but the black-box nature
of neural networks hinders precise enforcement of boundary conditions. To
address this, we propose a boundary condition-guaranteed evolutionary
Kolmogorov-Arnold Network (KAN) with radial basis functions (BEKAN). In BEKAN,
we propose three distinct and combinable approaches for incorporating
Dirichlet, periodic, and Neumann boundary conditions into the network. For
Dirichlet problem, we use smooth and global Gaussian RBFs to construct
univariate basis functions for approximating the solution and to encode
boundary information at the activation level of the network. To handle periodic
problems, we employ a periodic layer constructed from a set of sinusoidal
functions to enforce the boundary conditions exactly. For a Neumann problem, we
devise a least-squares formulation to guide the parameter evolution toward
satisfying the Neumann condition. By virtue of the boundary-embedded RBFs, the
periodic layer, and the evolutionary framework, we can perform accurate PDE
simulations while rigorously enforcing boundary conditions. For demonstration,
we conducted extensive numerical experiments on Dirichlet, Neumann, periodic,
and mixed boundary value problems. The results indicate that BEKAN outperforms
both multilayer perceptron (MLP) and B-splines KAN in terms of accuracy. In
conclusion, the proposed approach enhances the capability of KANs in solving
PDE problems while satisfying boundary conditions, thereby facilitating
advancements in scientific computing and engineering applications.

</details>


### [190] [Discovering Transformer Circuits via a Hybrid Attribution and Pruning Framework](https://arxiv.org/abs/2510.03282)
*Hao Gu,Vibhas Nair,Amrithaa Ashok Kumar,Jayvart Sharma,Ryan Lagasse*

Main category: cs.LG

TL;DR: 提出混合归因与剪枝（HAP）框架用于语言模型电路分析，比基线算法快46%且不牺牲电路忠实性，在间接宾语识别任务案例中表现良好，或可提升机理可解释性研究可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有电路发现算法存在权衡问题，归因修补快但不忠实于全模型，边剪枝忠实但计算成本高。

Method: 提出HAP框架，先用归因修补识别高潜力子图，再用边剪枝从中提取忠实电路。

Result: HAP比基线算法快46%，不牺牲电路忠实性，在间接宾语识别任务中能保留合作电路组件。

Conclusion: HAP可能是提升机理可解释性研究对更大模型可扩展性的有效方法。

Abstract: Interpreting language models often involves circuit analysis, which aims to
identify sparse subnetworks, or circuits, that accomplish specific tasks.
Existing circuit discovery algorithms face a fundamental trade-off: attribution
patching is fast but unfaithful to the full model, while edge pruning is
faithful but computationally expensive. This research proposes a hybrid
attribution and pruning (HAP) framework that uses attribution patching to
identify a high-potential subgraph, then applies edge pruning to extract a
faithful circuit from it. We show that HAP is 46\% faster than baseline
algorithms without sacrificing circuit faithfulness. Furthermore, we present a
case study on the Indirect Object Identification task, showing that our method
preserves cooperative circuit components (e.g. S-inhibition heads) that
attribution patching methods prune at high sparsity. Our results show that HAP
could be an effective approach for improving the scalability of mechanistic
interpretability research to larger models. Our code is available at
https://anonymous.4open.science/r/HAP-circuit-discovery.

</details>


### [191] [Latent Mixture of Symmetries for Sample-Efficient Dynamic Learning](https://arxiv.org/abs/2510.03578)
*Haoran Li,Chenhan Xiao,Muhao Guo,Yang Weng*

Main category: cs.LG

TL;DR: 提出Latent MoS模型从复杂动态测量中捕获对称主导的潜在因素，在多种物理系统实验中表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 工程系统学习动力学需样本高效学习，现有对称发现方法有局限性，如假设单一全局对称群、将对称发现和动态学习分离。

Method: 提出Latent MoS模型，专注动态学习并局部保留对称变换，引入堆叠MoS块的分层架构。

Result: 在多种物理系统的数值实验中，Latent MoS在插值和外推任务上优于现有基线，且提供适用于未来分析的可解释潜在表示。

Conclusion: Latent MoS是一种有效的模型，能提高样本效率，适用于工程系统的学习动力学。

Abstract: Learning dynamics is essential for model-based control and Reinforcement
Learning in engineering systems, such as robotics and power systems. However,
limited system measurements, such as those from low-resolution sensors, demand
sample-efficient learning. Symmetry provides a powerful inductive bias by
characterizing equivariant relations in system states to improve sample
efficiency. While recent methods attempt to discover symmetries from data, they
typically assume a single global symmetry group and treat symmetry discovery
and dynamic learning as separate tasks, leading to limited expressiveness and
error accumulation. In this paper, we propose the Latent Mixture of Symmetries
(Latent MoS), an expressive model that captures a mixture of symmetry-governed
latent factors from complex dynamical measurements. Latent MoS focuses on
dynamic learning while locally and provably preserving the underlying symmetric
transformations. To further capture long-term equivariance, we introduce a
hierarchical architecture that stacks MoS blocks. Numerical experiments in
diverse physical systems demonstrate that Latent MoS outperforms
state-of-the-art baselines in interpolation and extrapolation tasks while
offering interpretable latent representations suitable for future geometric and
safety-critical analyses.

</details>


### [192] [Edge-FIT: Federated Instruction Tuning of Quantized LLMs for Privacy-Preserving Smart Home Environments](https://arxiv.org/abs/2510.03284)
*Vinay Venkatesh,Vamsidhar R Kamanuru,Lav Kumar,Nikita Kothari*

Main category: cs.LG

TL;DR: 提出Edge - FIT框架用于大语言模型的联邦指令调优，结合联邦学习与4位量化低秩自适应，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习方法在处理大语言模型的大量参数时失效，需要新的可扩展框架。

Method: 将联邦学习与4位量化低秩自适应（QLORA）相结合，过滤通用数据集用于物联网领域。

Result: Edge - FIT调优的Llama 2(7B)达到0.89的F1分数，3.8B Phi - 3 - mini模型展示了可行的权衡。

Conclusion: Edge - FIT是用于家庭计算网关的去中心化大语言模型部署的可扩展框架。

Abstract: This paper proposes Edge-FIT (Federated Instruction Tuning on the Edge), a
scalable framework for Federated Instruction Tuning (FIT) of Large Language
Models (LLMs). Traditional Federated Learning (TFL) methods, like FedAvg, fail
when confronted with the massive parameter size of LLMs [3], [6]. Our Edge-FIT
framework combines federated learning with 4-bit Quantized Low-Rank Adaptation
(QLORA), mitigating the core issues of communication and computational
overhead. We demonstrate this by filtering the general-purpose Databricks Dolly
15k dataset for the IoT domain. Experimental results show the Edge-FIT tuned
Llama 2(7B) achieves an F1-Score of 0.89. We also demonstrate a viable
trade-off using the 3.8B Phi-3-mini model, validating Edge-FIT as a scalable
framework for decentralized LLM deployment on home compute gateways.

</details>


### [193] [Explore the Loss space with Hill-ADAM](https://arxiv.org/abs/2510.03613)
*Meenakshi Manikandan,Leilani Gilpin*

Main category: cs.LG

TL;DR: 本文介绍了Hill - ADAM优化器，它能逃离局部极小值找到全局极小值，通过交替进行误差最小化和最大化实现，并进行了相关测试。


<details>
  <summary>Details</summary>
Motivation: 现有算法在逃离局部极小值方面存在不足，需要一种能有效逃离局部极小值以找到全局极小值的优化器。

Method: 先推导ADAM优化器在特定模型状态下的步长解析近似，确定ADAM逃离局部极小值的限制条件；Hill - ADAM算法交替进行误差最小化和最大化。

Result: 使用5种损失函数和12个图像颜色校正实例对Hill - ADAM进行了测试。

Conclusion: 文档未明确提及最终结论，但从测试来看，Hill - ADAM有潜力在寻找全局极小值方面发挥作用。

Abstract: This paper introduces Hill-ADAM. Hill-ADAM is an optimizer with its focus
towards escaping local minima in prescribed loss landscapes to find the global
minimum. Hill-ADAM escapes minima by deterministically exploring the state
space. This eliminates uncertainty from random gradient updates in stochastic
algorithms while seldom converging at the first minimum that visits. In the
paper we first derive an analytical approximation of the ADAM Optimizer step
size at a particular model state. From there define the primary condition
determining ADAM limitations in escaping local minima. The proposed optimizer
algorithm Hill-ADAM alternates between error minimization and maximization. It
maximizes to escape the local minimum and minimizes again afterward. This
alternation provides an overall exploration throughout the loss space. This
allows the deduction of the global minimum's state. Hill-ADAM was tested with 5
loss functions and 12 amber-saturated to cooler-shade image color correction
instances.

</details>


### [194] [Why mask diffusion does not work](https://arxiv.org/abs/2510.03289)
*Haocheng Sun,Cynthia Xin Wen,Edward Hong Wang*

Main category: cs.LG

TL;DR: 指出掩码扩散语言模型在并行生成和双向注意力方面的固有困难，并提出有效训练和推理策略。


<details>
  <summary>Details</summary>
Motivation: 探究开源掩码扩散语言模型在实现并行生成和双向注意力方面的问题。

Method: 分析掩码扩散面临的困难并提出训练和推理策略。

Result: 揭示掩码扩散实现并行生成和双向注意力的固有困难。

Conclusion: 给出了针对掩码扩散最有效的训练和推理策略。

Abstract: The main advantages of diffusion language models over autoregressive (AR)
models lie in their ability to support parallel generation and bidirectional
attention, enabling a more controllable generation process. In recent years,
open-source mask diffusion language models have emerged, most of which are
based on a variant known as absorbing diffusion. However, this paper
demonstrates why mask diffusion faces inherent difficulties in achieving
parallel generation and bidirectional attention. We also propose the most
effective training and inference strategies for mask diffusion.

</details>


### [195] [Neural Bayesian Filtering](https://arxiv.org/abs/2510.03614)
*Christopher Solinas,Radovan Haluska,David Sychrovsky,Finbarr Timbers,Nolan Bard,Michael Buro,Martin Schmid,Nathan R. Sturtevant,Michael Bowling*

Main category: cs.LG

TL;DR: 提出Neural Bayesian Filtering (NBF)算法用于部分可观测系统隐藏状态分布维护，结合经典滤波器效率与深度生成模型表达力，并在三个环境中验证。


<details>
  <summary>Details</summary>
Motivation: 在部分可观测系统中高效且准确地维护隐藏状态分布。

Method: 训练NBF找到任务诱导信念的良好潜在表示，将信念映射到固定长度嵌入向量，在嵌入空间用粒子式更新计算后验。

Result: NBF结合了经典滤波器的计算效率和深度生成模型的表达能力，能跟踪快速变化的多模态信念，减少粒子匮乏风险。

Conclusion: NBF在三个部分可观测环境的状态估计任务中得到验证。

Abstract: We present Neural Bayesian Filtering (NBF), an algorithm for maintaining
distributions over hidden states, called beliefs, in partially observable
systems. NBF is trained to find a good latent representation of the beliefs
induced by a task. It maps beliefs to fixed-length embedding vectors, which
condition generative models for sampling. During filtering, particle-style
updates compute posteriors in this embedding space using incoming observations
and the environment's dynamics. NBF combines the computational efficiency of
classical filters with the expressiveness of deep generative models - tracking
rapidly shifting, multimodal beliefs while mitigating the risk of particle
impoverishment. We validate NBF in state estimation tasks in three partially
observable environments.

</details>


### [196] [Single-Core Superscalar Optimization of Clifford Neural Layers](https://arxiv.org/abs/2510.03290)
*X. Angelo Huang,Ruben Ciranni,Giovanni Spadaccini,Carla J. López Zurita*

Main category: cs.LG

TL;DR: 本文分析Clifford卷积层计算的内部结构，提出并实现优化以加速推理，最终平均加速21.35倍。


<details>
  <summary>Details</summary>
Motivation: 在对具有等变性的网络的研究中，为加速Clifford卷积层的推理过程。

Method: 分析Clifford代数的理论基础以消除冗余矩阵分配和计算，应用已有的优化技术。

Result: 最终平均加速21.35倍，六种情况运行时间与或快于原PyTorch实现，其余情况性能与原库相当。

Conclusion: 所提出的优化方法能有效加速Clifford卷积层的推理过程。

Abstract: Within the growing interest in the physical sciences in developing networks
with equivariance properties, Clifford neural layers shine as one approach that
delivers $E(n)$ and $O(n)$ equivariances given specific group actions. In this
paper, we analyze the inner structure of the computation within Clifford
convolutional layers and propose and implement several optimizations to speed
up the inference process while maintaining correctness. In particular, we begin
by analyzing the theoretical foundations of Clifford algebras to eliminate
redundant matrix allocations and computations, then systematically apply
established optimization techniques to enhance performance further. We report a
final average speedup of 21.35x over the baseline implementation of eleven
functions and runtimes comparable to and faster than the original PyTorch
implementation in six cases. In the remaining cases, we achieve performance in
the same order of magnitude as the original library.

</details>


### [197] [UniPruning: Unifying Local Metric and Global Feedback for Scalable Sparse LLMs](https://arxiv.org/abs/2510.03291)
*Yizhuo Ding,Wanying Qu,Jiawei Geng,Wenqi Shao,Yanwei Fu*

Main category: cs.LG

TL;DR: 提出统一剪枝框架UniPruning，结合局部和全局优势，无需更新模型权重，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型剪枝方法难以平衡效率和鲁棒性，需新方法解决计算和内存成本问题。

Method: 提出基于镜像下降优化的统一剪枝框架UniPruning，结合局部显著性指标速度和全局协调稳定性，利用快速逐层评分和轻量级全局控制器分配稀疏预算。

Result: 在多个预训练大语言模型家族和标准基准测试中，UniPruning在困惑度和零样本准确率上有竞争力或更优，消融实验凸显关键部分重要性。

Conclusion: UniPruning为大语言模型稀疏化提供高效、有原则且可扩展的解决方案。

Abstract: Large Language Models (LLMs) achieve strong performance across diverse tasks
but face prohibitive computational and memory costs. Pruning offers a promising
path by inducing sparsity while preserving architectural flexibility. However,
existing methods struggle to balance efficiency and robustness: local metric
approaches prune layer by layer but often collapse under high sparsity, whereas
global feedback methods enforce consistency at the cost of expensive weight
updates or restrictive semi-structured formats. We present UniPruning, a
unified post-training pruning framework that combines the speed of local
saliency metrics with the stability of global coordination, enabled by a mirror
descent based optimization, all without updating model weights. UniPruning
leverages fast layer-wise scoring and a lightweight global controller to
allocate a single sparsity budget, supporting both unstructured and
semi-structured N :M pruning within one framework. After a brief calibration,
it can generate pruning masks for arbitrary sparsity levels in one shot, and
adapts seamlessly to hardware-aware constraints. Extensive experiments on
multiple pretrained LLM families and standard benchmarks show that UniPruning
consistently delivers competitive or superior perplexity and zero-shot
accuracy. Ablation studies further highlight the importance of mirror descent
and local saliency anchoring. Overall, UniPruning provides an efficient,
principled, and scalable solution for sparsifying large-scale LLMs. Our code is
available at: https://github.com/RainbowQTT/UniPruning.

</details>


### [198] [Implicit Models: Expressive Power Scales with Test-Time Compute](https://arxiv.org/abs/2510.03638)
*Jialin Liu,Lisang Ding,Stanley Osher,Wotao Yin*

Main category: cs.LG

TL;DR: 本文研究隐式模型，通过非参数表达能力分析揭示其机制，理论在三个领域验证。


<details>
  <summary>Details</summary>
Motivation: 隐式模型可减少内存需求且能匹配或超越显式网络，但底层机制不明，需深入研究。

Method: 通过非参数表达能力分析，给出严格数学表征。

Result: 证明隐式模型表达能力随测试时计算量增长，在三个领域验证迭代增加时映射复杂度上升、解质量提高且稳定。

Conclusion: 隐式模型迭代可逐步表达更复杂映射，表达能力与测试时计算量相关。

Abstract: Implicit models, an emerging model class, compute outputs by iterating a
single parameter block to a fixed point. This architecture realizes an
infinite-depth, weight-tied network that trains with constant memory,
significantly reducing memory needs for the same level of performance compared
to explicit models. While it is empirically known that these compact models can
often match or even exceed larger explicit networks by allocating more
test-time compute, the underlying mechanism remains poorly understood.
  We study this gap through a nonparametric analysis of expressive power. We
provide a strict mathematical characterization, showing that a simple and
regular implicit operator can, through iteration, progressively express more
complex mappings. We prove that for a broad class of implicit models, this
process lets the model's expressive power scale with test-time compute,
ultimately matching a much richer function class. The theory is validated
across three domains: image reconstruction, scientific computing, and
operations research, demonstrating that as test-time iterations increase, the
complexity of the learned mapping rises, while the solution quality
simultaneously improves and stabilizes.

</details>


### [199] [Does higher interpretability imply better utility? A Pairwise Analysis on Sparse Autoencoders](https://arxiv.org/abs/2510.03659)
*Xu Wang,Yan Hu,Benyou Wang,Difan Zou*

Main category: cs.LG

TL;DR: 本文研究稀疏自编码器（SAEs）可解释性与引导实用性关系，发现二者关联弱，提出新特征选择标准提升引导性能，且有效特征的可解释性与实用性差异大。


<details>
  <summary>Details</summary>
Motivation: 探究SAEs更高的可解释性是否意味着更好的引导实用性。

Method: 在三个大语言模型上训练90个SAEs，评估可解释性和引导实用性并进行秩一致性分析，提出Delta Token Confidence特征选择标准。

Result: 可解释性与引导性能仅存在较弱正相关；新方法使三个大语言模型引导性能提升52.52%；选择高Delta Token Confidence特征后，可解释性与实用性相关性消失甚至为负。

Conclusion: 可解释性不足以代表引导性能，有效引导特征的可解释性与实用性存在差异。

Abstract: Sparse Autoencoders (SAEs) are widely used to steer large language models
(LLMs), based on the assumption that their interpretable features naturally
enable effective model behavior steering. Yet, a fundamental question remains
unanswered: does higher interpretability indeed imply better steering utility?
To answer this question, we train 90 SAEs across three LLMs (Gemma-2-2B,
Qwen-2.5-3B, Gemma-2-9B), spanning five architectures and six sparsity levels,
and evaluate their interpretability and steering utility based on SAEBench
(arXiv:2501.12345) and AxBench (arXiv:2502.23456) respectively, and perform a
rank-agreement analysis via Kendall's rank coefficients (tau b). Our analysis
reveals only a relatively weak positive association (tau b approx 0.298),
indicating that interpretability is an insufficient proxy for steering
performance. We conjecture the interpretability utility gap may stem from the
selection of SAE features, as not all of them are equally effective for
steering. To further find features that truly steer the behavior of LLMs, we
propose a novel selection criterion called Delta Token Confidence, which
measures how much amplifying a feature changes the next token distribution. We
show that our method improves the steering performance of three LLMs by 52.52
percent compared to the current best output score based criterion
(arXiv:2503.34567). Strikingly, after selecting features with high Delta Token
Confidence, the correlation between interpretability and utility vanishes (tau
b approx 0), and can even become negative. This further highlights the
divergence between interpretability and utility for the most effective steering
features.

</details>


### [200] [Towards Sampling Data Structures for Tensor Products in Turnstile Streams](https://arxiv.org/abs/2510.03678)
*Zhao Song,Shenghao Xie,Samson Zhou*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper studies the computational challenges of large-scale
attention-based models in artificial intelligence by utilizing importance
sampling methods in the streaming setting. Inspired by the classical definition
of the $\ell_2$ sampler and the recent progress of the attention scheme in
Large Language Models (LLMs), we propose the definition of the attention
sampler. Our approach significantly reduces the computational burden of
traditional attention mechanisms. We analyze the effectiveness of the attention
sampler from a theoretical perspective, including space and update time.
Additionally, our framework exhibits scalability and broad applicability across
various model architectures and domains.

</details>


### [201] [Dynamic Meta-Learning for Adaptive XGBoost-Neural Ensembles](https://arxiv.org/abs/2510.03301)
*Arthur Sedek*

Main category: cs.LG

TL;DR: 提出结合XGBoost和神经网络的自适应集成框架，实验显示有良好效果


<details>
  <summary>Details</summary>
Motivation: 开发更智能灵活的机器学习系统

Method: 通过元学习结合XGBoost和神经网络，利用不确定性量化和特征重要性集成动态选择和组合模型

Result: 在不同数据集上有优越的预测性能和更强的可解释性

Conclusion: 该方法有助于开发更智能灵活的机器学习系统

Abstract: This paper introduces a novel adaptive ensemble framework that
synergistically combines XGBoost and neural networks through sophisticated
meta-learning. The proposed method leverages advanced uncertainty
quantification techniques and feature importance integration to dynamically
orchestrate model selection and combination. Experimental results demonstrate
superior predictive performance and enhanced interpretability across diverse
datasets, contributing to the development of more intelligent and flexible
machine learning systems.

</details>


### [202] [Group Policy Gradient](https://arxiv.org/abs/2510.03679)
*Junhua Chen,Zixi Zhang,Hantao Zhong,Rika Antonova*

Main category: cs.LG

TL;DR: 介绍无评论器策略梯度估计器Group Policy Gradient (GPG)，证明其一致性，实验表明性能优于PPO，更高效利用计算资源。


<details>
  <summary>Details</summary>
Motivation: 受GRPO在RLHF中成功启发，去除训练评论器的成本，同时保留PPO的裁剪目标结构。

Method: 用基于组的蒙特卡罗优势估计器替代学习的价值函数，保留PPO裁剪目标结构，并证明估计器的一致性，分析偏差 - 方差权衡。

Result: GPG在标准基准测试中表现与PPO相当或更优，能更好利用并行模拟，更高效利用计算资源。

Conclusion: GPG是一种有效的无评论器策略梯度估计器，在性能和资源利用上优于PPO。

Abstract: We introduce Group Policy Gradient (GPG), a family of critic-free
policy-gradient estimators for general MDPs. Inspired by the success of GRPO's
approach in Reinforcement Learning from Human Feedback (RLHF), GPG replaces a
learned value function with a group-based Monte Carlo advantage estimator,
removing the memory, compute, and hyperparameter costs of training a critic
while preserving PPO's clipped-objective structure. We prove the consistency of
the GPG estimator, analyze the bias-variance tradeoffs, and demonstrate
empirically that GPG matches or outperforms PPO on standard benchmarks. GPG
makes better use of parallel simulations, which, together with its critic-free
design, results in more efficient use of computational resources than PPO.

</details>


### [203] [Revoking Amnesia: RL-based Trajectory Optimization to Resurrect Erased Concepts in Diffusion Models](https://arxiv.org/abs/2510.03302)
*Daiheng Gao,Nanxiang Jiang,Andi Zhang,Shilin Lu,Yufei Tang,Wenbo Zhou,Weiming Zhang,Zhaoxin Fan*

Main category: cs.LG

TL;DR: 现有T2I扩散模型概念擦除方法在新架构中效果不佳，本文揭示其本质可逆，提出RevAm框架复活擦除概念，实验显示其保真度高且计算时间短，凸显需更强大擦除技术。


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法在新架构中效果变差，且其真实机制存疑，需区分表面安全和真正概念移除。

Method: 提出基于强化学习的轨迹优化框架RevAm，将GRPO应用于扩散模型，通过轨迹级奖励探索恢复轨迹。

Result: RevAm实现了更优的概念复活保真度，计算时间减少10倍。

Conclusion: 当前安全机制存在关键漏洞，需超越轨迹操纵的更强大擦除技术。

Abstract: Concept erasure techniques have been widely deployed in T2I diffusion models
to prevent inappropriate content generation for safety and copyright
considerations. However, as models evolve to next-generation architectures like
Flux, established erasure methods (\textit{e.g.}, ESD, UCE, AC) exhibit
degraded effectiveness, raising questions about their true mechanisms. Through
systematic analysis, we reveal that concept erasure creates only an illusion of
``amnesia": rather than genuine forgetting, these methods bias sampling
trajectories away from target concepts, making the erasure fundamentally
reversible. This insight motivates the need to distinguish superficial safety
from genuine concept removal. In this work, we propose \textbf{RevAm}
(\underline{Rev}oking \underline{Am}nesia), an RL-based trajectory optimization
framework that resurrects erased concepts by dynamically steering the denoising
process without modifying model weights. By adapting Group Relative Policy
Optimization (GRPO) to diffusion models, RevAm explores diverse recovery
trajectories through trajectory-level rewards, overcoming local optima that
limit existing methods. Extensive experiments demonstrate that RevAm achieves
superior concept resurrection fidelity while reducing computational time by
10$\times$, exposing critical vulnerabilities in current safety mechanisms and
underscoring the need for more robust erasure techniques beyond trajectory
manipulation.

</details>


### [204] [From Moments to Models: Graphon Mixture-Aware Mixup and Contrastive Learning](https://arxiv.org/abs/2510.03690)
*Ali Azizpour,Reza Ramezanpour,Ashutosh Sabharwal,Santiago Segarra*

Main category: cs.LG

TL;DR: 提出统一框架显式建模图数据为图子混合，实现模型感知分区，用于图数据增强和对比学习，有理论保证且实验表现佳。


<details>
  <summary>Details</summary>
Motivation: 现实图数据集有混合结构，现有图表示学习和增强方法常忽略。

Method: 将数据建模为图子表示的概率图生成模型混合，用图矩聚类图，实现模型感知分区，用于数据增强和对比学习，引入新目标改进负采样。

Result: 建立新理论边界，实验中无监督学习MGCL达SOTA，有监督学习GMAM多数情况优于现有策略。

Conclusion: 提出的方法能有效处理图数据混合结构，在图学习任务中表现出色。

Abstract: Real-world graph datasets often consist of mixtures of populations, where
graphs are generated from multiple distinct underlying distributions. However,
modern representation learning approaches, such as graph contrastive learning
(GCL) and augmentation methods like Mixup, typically overlook this mixture
structure. In this work, we propose a unified framework that explicitly models
data as a mixture of underlying probabilistic graph generative models
represented by graphons. To characterize these graphons, we leverage graph
moments (motif densities) to cluster graphs arising from the same model. This
enables us to disentangle the mixture components and identify their distinct
generative mechanisms. This model-aware partitioning benefits two key graph
learning tasks: 1) It enables a graphon-mixture-aware mixup (GMAM), a data
augmentation technique that interpolates in a semantically valid space guided
by the estimated graphons, instead of assuming a single graphon per class. 2)
For GCL, it enables model-adaptive and principled augmentations. Additionally,
by introducing a new model-aware objective, our proposed approach (termed MGCL)
improves negative sampling by restricting negatives to graphs from other
models. We establish a key theoretical guarantee: a novel, tighter bound
showing that graphs sampled from graphons with small cut distance will have
similar motif densities with high probability. Extensive experiments on
benchmark datasets demonstrate strong empirical performance. In unsupervised
learning, MGCL achieves state-of-the-art results, obtaining the top average
rank across eight datasets. In supervised learning, GMAM consistently
outperforms existing strategies, achieving new state-of-the-art accuracy in 6
out of 7 datasets.

</details>


### [205] [Balancing Interpretability and Performance in Reinforcement Learning: An Adaptive Spectral Based Linear Approach](https://arxiv.org/abs/2510.03722)
*Qianxin Yi,Shao-Bo Lin,Jun Fan,Yao Wang*

Main category: cs.LG

TL;DR: 本文提出基于谱的线性强化学习方法，兼顾可解释性与性能，理论分析有误差界，实验表现好。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法多关注性能，依赖事后解释，需设计兼顾可解释性与性能的方法。

Method: 提出基于谱的线性强化学习方法，通过谱滤波函数扩展基于岭回归的方法，设计自适应正则化参数选择策略。

Result: 理论分析得出参数估计和泛化误差的近最优界，实验在决策质量上表现好，可解释性分析增强用户信任。

Conclusion: 该方法能弥合强化学习理论与实际决策的差距，在管理场景提供可解释性、准确性和适应性。

Abstract: Reinforcement learning (RL) has been widely applied to sequential decision
making, where interpretability and performance are both critical for practical
adoption. Current approaches typically focus on performance and rely on post
hoc explanations to account for interpretability. Different from these
approaches, we focus on designing an interpretability-oriented yet
performance-enhanced RL approach. Specifically, we propose a spectral based
linear RL method that extends the ridge regression-based approach through a
spectral filter function. The proposed method clarifies the role of
regularization in controlling estimation error and further enables the design
of an adaptive regularization parameter selection strategy guided by the
bias-variance trade-off principle. Theoretical analysis establishes
near-optimal bounds for both parameter estimation and generalization error.
Extensive experiments on simulated environments and real-world datasets from
Kuaishou and Taobao demonstrate that our method either outperforms or matches
existing baselines in decision quality. We also conduct interpretability
analyses to illustrate how the learned policies make decisions, thereby
enhancing user trust. These results highlight the potential of our approach to
bridge the gap between RL theory and practical decision making, providing
interpretability, accuracy, and adaptability in management contexts.

</details>


### [206] [Thin Bridges for Drug Text Alignment: Lightweight Contrastive Learning for Target Specific Drug Retrieval](https://arxiv.org/abs/2510.03309)
*Mallikarjuna Tupakula*

Main category: cs.LG

TL;DR: 研究用轻量级投影头（thin contrastive bridges）在不训练完整多模态模型下对齐化学和文本表示，结果显示该方法实现跨模态对齐，是大规模多模态预训练的高效替代方案。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态基础模型在药物发现和生物医学应用中大多依赖大量预训练或大规模多模态语料，研究能否用轻量级方法对齐化学和文本表示。

Method: 利用ChEMBL的配对机制，通过对比目标训练的双线性投影，将ECFP4分子指纹与生物医学句子嵌入对齐，还加入硬负样本加权和边缘损失。

Result: 在基于支架的分割评估中，该方法实现了非平凡的跨模态对齐，与冻结基准相比显著提高了目标内辨别能力。

Conclusion: thin bridges是大规模多模态预训练的计算高效替代方案，可实现支架感知的药物文本对齐和精准医学中的目标特定检索。

Abstract: Multimodal foundation models hold promise for drug discovery and biomedical
applications, but most existing approaches rely on heavy pretraining or large
scale multimodal corpora. We investigate whether thin contrastive bridges,
lightweight projection heads over frozen unimodal encoders can align chemical
and textual representations without training a full multimodal model. Using
paired mechanisms from ChEMBL, we align ECFP4 molecular fingerprints with
biomedical sentence embeddings through dual linear projections trained with a
contrastive objective. To better handle drugs sharing the same therapeutic
target, we incorporate hard negative weighting and a margin loss. Evaluation
under scaffold based splits, which require generalization across disjoint
chemical cores, demonstrates that our approach achieves non-trivial cross modal
alignment and substantially improves within target discrimination compared to
frozen baselines. These results suggest that thin bridges offer a compute
efficient alternative to large scale multimodal pretraining, enabling scaffold
aware drug text alignment and target specific retrieval in precision medicine.

</details>


### [207] [Cost Efficient Fairness Audit Under Partial Feedback](https://arxiv.org/abs/2510.03734)
*Nirjhar Das,Mohit Sharma,Praharsh Nanavati,Kirankumar Shiragur,Amit Deshpande*

Main category: cs.LG

TL;DR: 研究部分反馈下分类器公平性审计问题，引入新成本模型，提出不同设置下算法，在真实数据集上表现好。


<details>
  <summary>Details</summary>
Motivation: 在部分反馈下找到比随机探索和自然基线更具成本效益的公平性审计算法。

Method: 考虑黑盒模型和混合模型两种审计设置，提出相应算法，利用截断样本学习和最大后验预言等方法。

Result: 在黑盒设置下提出近最优算法；混合模型设置下算法成本更低；在真实数据集上审计成本比自然基线低约50%。

Conclusion: 所提算法适用于多种公平性指标，在公平性审计中具有良好性能和成本效益。

Abstract: We study the problem of auditing the fairness of a given classifier under
partial feedback, where true labels are available only for positively
classified individuals, (e.g., loan repayment outcomes are observed only for
approved applicants). We introduce a novel cost model for acquiring additional
labeled data, designed to more accurately reflect real-world costs such as
credit assessment, loan processing, and potential defaults. Our goal is to find
optimal fairness audit algorithms that are more cost-effective than random
exploration and natural baselines.
  In our work, we consider two audit settings: a black-box model with no
assumptions on the data distribution, and a mixture model, where features and
true labels follow a mixture of exponential family distributions. In the
black-box setting, we propose a near-optimal auditing algorithm under mild
assumptions and show that a natural baseline can be strictly suboptimal. In the
mixture model setting, we design a novel algorithm that achieves significantly
lower audit cost than the black-box case. Our approach leverages prior work on
learning from truncated samples and maximum-a-posteriori oracles, and extends
known results on spherical Gaussian mixtures to handle exponential family
mixtures, which may be of independent interest. Moreover, our algorithms apply
to popular fairness metrics including demographic parity, equal opportunity,
and equalized odds. Empirically, we demonstrate strong performance of our
algorithms on real-world fair classification datasets like Adult Income and Law
School, consistently outperforming natural baselines by around 50% in terms of
audit cost.

</details>


### [208] [Predicting Effects, Missing Distributions: Evaluating LLMs as Human Behavior Simulators in Operations Management](https://arxiv.org/abs/2510.03310)
*Runze Zhang,Xiaowei Zhang,Mingyang Zhao*

Main category: cs.LG

TL;DR: 本文评估大语言模型（LLMs）在运营管理中对人类行为的复制能力，发现能复现多数假设效应，但响应分布与人类数据有差异，特定干预可减少偏差。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在运营管理领域复制人类行为的效果，作为传统研究方法的低成本补充。

Method: 使用九个已发表的行为运营实验数据，评估假设检验结果的复制和基于Wasserstein距离的分布对齐。测试思维链提示和超参数调整两种干预措施。

Result: LLMs能复现多数假设层面效应，捕捉关键决策偏差，但响应分布与人类数据有差异，特定干预可减少偏差。

Conclusion: LLMs在运营管理中复制人类行为有一定能力，但分布存在差异，轻量级干预措施可改善表现。

Abstract: LLMs are emerging tools for simulating human behavior in business, economics,
and social science, offering a lower-cost complement to laboratory experiments,
field studies, and surveys. This paper evaluates how well LLMs replicate human
behavior in operations management. Using nine published experiments in
behavioral operations, we assess two criteria: replication of hypothesis-test
outcomes and distributional alignment via Wasserstein distance. LLMs reproduce
most hypothesis-level effects, capturing key decision biases, but their
response distributions diverge from human data, including for strong commercial
models. We also test two lightweight interventions -- chain-of-thought
prompting and hyperparameter tuning -- which reduce misalignment and can
sometimes let smaller or open-source models match or surpass larger systems.

</details>


### [209] [Allocation of Parameters in Transformers](https://arxiv.org/abs/2510.03784)
*Ruoxi Yu,Haotian Jiang,Jingpu Cheng,Penghao Yu,Qianxiao Li,Zhong Li*

Main category: cs.LG

TL;DR: 本文研究Transformer模型参数（注意力头和头维度）在各层的分配问题，结合理论分析和实验提出分配策略，提升模型效率。


<details>
  <summary>Details</summary>
Motivation: Transformer在多领域成功，但模型效率的理论基础研究不足，需研究参数在各层分配以平衡表达能力和效率。

Method: 从近似角度对早期层信息提取作用进行数学分析，刻画固定参数预算下头数量和头维度的权衡关系，揭示并证明softmax激活的饱和行为。

Result: 发现连续增加头维度对学习误差的收益递减，尤其是长序列，表明后续层减少参数可更高效运行。

Conclusion: 提出了Transformer各层注意力头和维度的分配策略，为基于Transformer架构的模型效率提供了理论依据。

Abstract: Transformers have achieved remarkable successes across a wide range of
applications, yet the theoretical foundation of their model efficiency remains
underexplored. In this work, we investigate how the model parameters -- mainly
attention heads and head dimensions -- should be allocated across layers to
balance expressivity and efficiency. We first provide mathematical analysis on
the role of early layers in information extraction from an approximation
perspective, with a theoretical characterization on the trade-off between the
number of heads and head dimension under a fixed parameter budget. In addition,
we uncover and prove the \emph{saturation} behavior of softmax activations:
Continuously increasing head dimensions can lead to diminishing returns in
learning errors, particularly for long sequences. Supported by both theory and
experiments, this saturation pattern suggests that later layers can operate
more efficiently with reduced parameters. Combining these insights, we propose
principled strategies for allocating attention heads and dimensions across
Transformers' layers, shedding light on theoretically-grounded model efficiency
of Transformer-based architectures.

</details>


### [210] [Scaling Laws Revisited: Modeling the Role of Data Quality in Language Model Pretraining](https://arxiv.org/abs/2510.03313)
*Anirudh Subramanyam,Yuxin Chen,Robert L. Grossman*

Main category: cs.LG

TL;DR: 引入数据质量参数Q和质量感知缩放定律，通过实验验证其有效性，为大规模预训练提供平衡数据整理和模型规模的指导。


<details>
  <summary>Details</summary>
Motivation: 以往研究未在原则性缩放定律中形式化数据质量，需引入数据质量参数并提出相关定律。

Method: 引入无量纲数据质量参数Q，提出扩展Chinchilla框架的质量感知缩放定律，有两个Q的实用估计器，通过合成实验控制数据质量。

Result: 损失随数据质量可预测地缩放，高质量数据可大幅减少模型大小和计算需求，有效数据随质量呈亚线性衰减且对适度数据损坏有鲁棒性。

Conclusion: 建立了数据质量的明确、可推广定律，为大规模预训练平衡数据整理和模型规模提供具体指导。

Abstract: Scaling laws for language model training traditionally characterize how
performance scales with model size and dataset volume. Prior work has explored
architecture variants and data treatments such as dataset filtering and noise
injection in language model pretraining; however, these studies have not
formalized data quality within a principled scaling law. We introduce a
dimensionless data-quality parameter Q, and propose a quality-aware scaling law
extending the Chinchilla framework to predict loss as a joint function of model
size, data volume, and data quality. The law is motivated by an
effective-sample-size and information-theoretic view of noisy or redundant
corpora, and it admits two practical estimators for Q: (i) a corruption rate
proxy and (ii) a deficiency measure. Through synthetic experiments in neural
machine translation and autoregressive modeling -- where we systematically
control data quality via multiple levels of noise injection and coverage
variation -- we show that loss scales predictably with data quality and that
higher-quality data can substantially reduce model size and hence compute
requirements. Our results demonstrate a sublinear decay of effective data with
quality and robustness to moderate data corruption; out-of-sample evaluations
further validate the predictive form of the law. Unlike prior empirical
analyses, our work establishes an explicit, generalizable law for data quality,
offering concrete guidance for balancing data curation effort and model scale
in large-scale pretraining.

</details>


### [211] [Robust Batched Bandits](https://arxiv.org/abs/2510.03798)
*Yunwen Guo,Yunlun Shu,Gongyi Zhuo,Tianyu Wang*

Main category: cs.LG

TL;DR: 提出适用于重尾奖励的批量多臂老虎机算法，揭示不同场景下重尾奖励对达到近最优遗憾所需批次数量的影响。


<details>
  <summary>Details</summary>
Motivation: 现有批量多臂老虎机问题研究多假设轻尾奖励分布，而现实场景多为重尾特征，需填补这一研究空白。

Method: 提出适用于有限臂和Lipschitz连续设置下重尾奖励的鲁棒批量多臂老虎机算法。

Result: 在与实例无关和Lipschitz设置中，重尾奖励达到近最优遗憾所需批次更少；在与实例相关设置中，所需批次与尾部重性无关。

Conclusion: 所提算法有助于解决现实中重尾奖励的批量多臂老虎机问题，不同场景下重尾奖励对批次数量影响不同。

Abstract: The batched multi-armed bandit (MAB) problem, in which rewards are collected
in batches, is crucial for applications such as clinical trials. Existing
research predominantly assumes light-tailed reward distributions, yet many
real-world scenarios, including clinical outcomes, exhibit heavy-tailed
characteristics. This paper bridges this gap by proposing robust batched bandit
algorithms designed for heavy-tailed rewards, within both finite-arm and
Lipschitz-continuous settings. We reveal a surprising phenomenon: in the
instance-independent regime, as well as in the Lipschitz setting,
heavier-tailed rewards necessitate a smaller number of batches to achieve
near-optimal regret. In stark contrast, for the instance-dependent setting, the
required number of batches to attain near-optimal regret remains invariant with
respect to tail heaviness.

</details>


### [212] [Fast frequency reconstruction using Deep Learning for event recognition in ring laser data](https://arxiv.org/abs/2510.03325)
*Giuseppe Di Somma,Giorgio Carelli,Angela D. V. Di Virgilio,Francesco Fuso,Enrico Maccioni,Paolo Marsili*

Main category: cs.LG

TL;DR: 提出神经网络方法可在约10毫秒内重建几百赫兹频率，优于传统方法，还引入自动分类框架识别信号干扰，推动人工智能用于地球物理信号分析。


<details>
  <summary>Details</summary>
Motivation: 传统方法重建正弦信号频率需数秒数据，希望实现更快速的频率重建和信号干扰识别。

Method: 采用神经网络方法进行频率重建，引入自动分类框架识别物理干扰。

Result: 能在约10毫秒内重建几百赫兹频率，在GINGERINO工作范围提高频率估计精度2倍，地震类独立测试数据集分类准确率达99% - 100%。

Conclusion: 该成果推动了人工智能在地球物理信号分析中的应用。

Abstract: The reconstruction of a frequency with minimal delay from a sinusoidal signal
is a common task in several fields; for example Ring Laser Gyroscopes, since
their output signal is a beat frequency. While conventional methods require
several seconds of data, we present a neural network approach capable of
reconstructing frequencies of several hundred Hertz within approximately 10
milliseconds. This enables rapid trigger generation. The method outperforms
standard Fourier-based techniques, improving frequency estimation precision by
a factor of 2 in the operational range of GINGERINO, our Ring Laser
Gyroscope.\\ In addition to fast frequency estimation, we introduce an
automated classification framework to identify physical disturbances in the
signal, such as laser instabilities and seismic events, achieving accuracy
rates between 99\% and 100\% on independent test datasets for the seismic
class. These results mark a step forward in integrating artificial intelligence
into signal analysis for geophysical applications.

</details>


### [213] [TROLL: Trust Regions improve Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2510.03817)
*Philipp Becker,Niklas Freymuth,Serge Thilges,Fabian Otto,Gerhard Neumann*

Main category: cs.LG

TL;DR: 本文提出用离散可微信任区域投影（TROLL）替代基于PPO的大语言模型奖励微调中的裁剪目标，在多方面表现优于PPO。


<details>
  <summary>Details</summary>
Motivation: 基于PPO的裁剪机制是粗略近似，常导致更新不稳定和性能欠佳，需改进。

Method: 用离散可微信任区域投影替代裁剪目标，对模型重要token对数进行运算。

Result: 在多个数据集、模型族和优势估计方法中，TROLL在训练速度、稳定性和最终成功率上均优于PPO裁剪机制。

Conclusion: TROLL可在训练中直接替代PPO裁剪机制，不改变推理行为且效果更好。

Abstract: On-policy Reinforcement Learning (RL) with PPO-like clip objectives has
become the standard choice for reward-based fine-tuning of large language
models (LLMs). Although recent work has explored improved estimators of
advantages and normalization, the clipping mechanism itself has remained
untouched. Originally introduced as a proxy for principled KL-based trust
regions, clipping is a crude approximation that often causes unstable updates
and suboptimal performance. We replace the clip objective with a novel discrete
differentiable trust region projection, which provides principled token-level
KL constraints. The projection operates on a sparse subset of the model's most
important token logits to balance computational cost and projection
effectiveness. Our approach, Trust Region Optimization for Large Language
Models (TROLL), serves as a direct replacement for PPO-like clipping during
training and does not alter the model's inference behavior. Across datasets,
model families, and advantage-estimation methods, TROLL consistently
outperforms PPO-like clipping in terms of training speed, stability, and final
success rates.

</details>


### [214] [Constant in an Ever-Changing World](https://arxiv.org/abs/2510.03330)
*Andy Wu,Chun-Cheng Lin,Yuehua Huang,Rung-Tzuo Liaw*

Main category: cs.LG

TL;DR: 提出CIC框架增强强化学习算法稳定性与性能，在MuJoCo环境验证有效且无额外计算成本


<details>
  <summary>Details</summary>
Motivation: 解决强化学习训练过程中的振荡、不稳定及性能下降问题

Method: CIC框架维护代表策略和当前策略，选择性更新代表策略，采用自适应调整机制辅助评论家训练

Result: 在五个MuJoCo环境评估显示，CIC提升了传统算法性能且无额外计算成本

Conclusion: CIC框架可有效增强算法稳定性，提升强化学习性能

Abstract: The training process of reinforcement learning often suffers from severe
oscillations, leading to instability and degraded performance. In this paper,
we propose a Constant in an Ever-Changing World (CIC) framework that enhances
algorithmic stability to improve performance. CIC maintains both a
representative policy and a current policy. Instead of updating the
representative policy blindly, CIC selectively updates it only when the current
policy demonstrates superiority. Furthermore, CIC employs an adaptive
adjustment mechanism, enabling the representative and current policies to
jointly facilitate critic training. We evaluate CIC on five MuJoCo
environments, and the results show that CIC improves the performance of
conventional algorithms without incurring additional computational cost.

</details>


### [215] [Proximal Diffusion Neural Sampler](https://arxiv.org/abs/2510.03824)
*Wei Guo,Jaemoo Choi,Yuchen Zhu,Molei Tao,Yongxin Chen*

Main category: cs.LG

TL;DR: 提出近端扩散神经采样器（PDNS）框架解决从非归一化目标分布采样的难题，通过实验验证其有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 从非归一化目标分布学习扩散神经采样器时，目标分布为多模态且各模态间有显著障碍，训练易导致模式崩溃。

Method: 提出PDNS框架，通过路径测度空间上的近端点法解决随机最优控制问题，将学习过程分解为一系列子问题，并使用近端加权去噪交叉熵（WDCE）目标实现每个近端步骤。

Result: 通过在连续和离散采样任务上的大量实验，证明了PDNS的有效性和鲁棒性。

Conclusion: PDNS能有效解决从多模态非归一化目标分布采样的难题，具有良好的有效性和鲁棒性。

Abstract: The task of learning a diffusion-based neural sampler for drawing samples
from an unnormalized target distribution can be viewed as a stochastic optimal
control problem on path measures. However, the training of neural samplers can
be challenging when the target distribution is multimodal with significant
barriers separating the modes, potentially leading to mode collapse. We propose
a framework named \textbf{Proximal Diffusion Neural Sampler (PDNS)} that
addresses these challenges by tackling the stochastic optimal control problem
via proximal point method on the space of path measures. PDNS decomposes the
learning process into a series of simpler subproblems that create a path
gradually approaching the desired distribution. This staged procedure traces a
progressively refined path to the desired distribution and promotes thorough
exploration across modes. For a practical and efficient realization, we
instantiate each proximal step with a proximal weighted denoising cross-entropy
(WDCE) objective. We demonstrate the effectiveness and robustness of PDNS
through extensive experiments on both continuous and discrete sampling tasks,
including challenging scenarios in molecular dynamics and statistical physics.

</details>


### [216] [HOFLON: Hybrid Offline Learning and Online Optimization for Process Start-Up and Grade-Transition Control](https://arxiv.org/abs/2510.03830)
*Alex Durkin,Jasper Stolte,Mehmet Mercangöz*

Main category: cs.LG

TL;DR: 本文提出HOFLON方法解决连续过程工厂启动和产品等级变更人工操作难题，经工业案例验证优于现有算法和历史最佳操作。


<details>
  <summary>Details</summary>
Motivation: 连续过程工厂启动和产品等级变更依赖人工操作，但专家逐渐退休，标准离线强化学习有分布偏移和价值高估问题。

Method: 提出HOFLON方法，离线学习潜在数据流形和长视野Q - 评判器，在线解决一步优化问题。

Result: 在两个工业案例中，HOFLON不仅超越Implicit Q - Learning算法，且平均累积奖励优于历史数据中最佳操作。

Conclusion: HOFLON有潜力实现超越当前专家能力的过渡操作自动化。

Abstract: Start-ups and product grade-changes are critical steps in continuous-process
plant operation, because any misstep immediately affects product quality and
drives operational losses. These transitions have long relied on manual
operation by a handful of expert operators, but the progressive retirement of
that workforce is leaving plant owners without the tacit know-how needed to
execute them consistently. In the absence of a process model, offline
reinforcement learning (RL) promises to capture and even surpass human
expertise by mining historical start-up and grade-change logs, yet standard
offline RL struggles with distribution shift and value-overestimation whenever
a learned policy ventures outside the data envelope. We introduce HOFLON
(Hybrid Offline Learning + Online Optimization) to overcome those limitations.
Offline, HOFLON learns (i) a latent data manifold that represents the feasible
region spanned by past transitions and (ii) a long-horizon Q-critic that
predicts the cumulative reward from state-action pairs. Online, it solves a
one-step optimization problem that maximizes the Q-critic while penalizing
deviations from the learned manifold and excessive rates of change in the
manipulated variables. We test HOFLON on two industrial case studies: a
polymerization reactor start-up and a paper-machine grade-change problem, and
benchmark it against Implicit Q-Learning (IQL), a leading offline-RL algorithm.
In both plants HOFLON not only surpasses IQL but also delivers, on average,
better cumulative rewards than the best start-up or grade-change observed in
the historical data, demonstrating its potential to automate transition
operations beyond current expert capability.

</details>


### [217] [Matching the Optimal Denoiser in Point Cloud Diffusion with (Improved) Rotational Alignment](https://arxiv.org/abs/2510.03335)
*Ameya Daigavane,YuQing Xie,Bodhi P. Vani,Saeed Saremi,Joseph Kleinhenz,Tess Smidt*

Main category: cs.LG

TL;DR: 研究扩散模型训练中点云旋转对齐步骤，表明最优去噪器可用SO(3)上矩阵费舍尔分布表示，对齐是小噪声下零阶近似，还推导更好近似器，实验显示对齐在关键噪声水平下近似足够好。


<details>
  <summary>Details</summary>
Motivation: 在训练点云扩散模型时常用旋转对齐步骤，但该步骤效果未被充分研究，因此开展研究。

Method: 将最优去噪器用SO(3)上矩阵费舍尔分布表示，推导小噪声极限下更好的近似器。

Result: 发现对齐对应分布模式采样，是小噪声水平下零阶近似。

Conclusion: 在训练扩散模型最重要的噪声水平下，对齐通常是一个“足够好”的近似。

Abstract: Diffusion models are a popular class of generative models trained to reverse
a noising process starting from a target data distribution. Training a
diffusion model consists of learning how to denoise noisy samples at different
noise levels. When training diffusion models for point clouds such as molecules
and proteins, there is often no canonical orientation that can be assigned. To
capture this symmetry, the true data samples are often augmented by
transforming them with random rotations sampled uniformly over $SO(3)$. Then,
the denoised predictions are often rotationally aligned via the Kabsch-Umeyama
algorithm to the ground truth samples before computing the loss. However, the
effect of this alignment step has not been well studied. Here, we show that the
optimal denoiser can be expressed in terms of a matrix Fisher distribution over
$SO(3)$. Alignment corresponds to sampling the mode of this distribution, and
turns out to be the zeroth order approximation for small noise levels,
explaining its effectiveness. We build on this perspective to derive better
approximators to the optimal denoiser in the limit of small noise. Our
experiments highlight that alignment is often a `good enough' approximation for
the noise levels that matter most for training diffusion models.

</details>


### [218] [Technical note on Fisher Information for Robust Federated Cross-Validation](https://arxiv.org/abs/2510.03838)
*Behraj Khan,Tahir Qasim Syed*

Main category: cs.LG

TL;DR: 训练数据碎片化会导致模型性能下降，提出FIRE方法解决，该方法表现优于基准。


<details>
  <summary>Details</summary>
Motivation: 训练数据在批次或不同地理位置进行碎片化或联邦学习时，模型性能会因协变量偏移而下降，需要解决此问题。

Method: 提出FIRE方法，通过近似Fisher信息累积碎片化引起的协变量偏移差异，将其作为每个片段的损失惩罚，实现可扩展的分布对齐。

Result: FIRE在偏移验证集上，最大比重要性加权基准高出5.1%，比联邦学习基准高出5.3%。

Conclusion: FIRE方法能有效解决训练数据碎片化导致的模型性能下降问题。

Abstract: When training data are fragmented across batches or federated-learned across
different geographic locations, trained models manifest performance
degradation. That degradation partly owes to covariate shift induced by data
having been fragmented across time and space and producing dissimilar empirical
training distributions. Each fragment's distribution is slightly different to a
hypothetical unfragmented training distribution of covariates, and to the
single validation distribution. To address this problem, we propose Fisher
Information for Robust fEderated validation (\textbf{FIRE}). This method
accumulates fragmentation-induced covariate shift divergences from the global
training distribution via an approximate Fisher information. That term, which
we prove to be a more computationally-tractable estimate, is then used as a
per-fragment loss penalty, enabling scalable distribution alignment. FIRE
outperforms importance weighting benchmarks by $5.1\%$ at maximum and federated
learning (FL) benchmarks by up to $5.3\%$ on shifted validation sets.

</details>


### [219] [Pool Me Wisely: On the Effect of Pooling in Transformer-Based Models](https://arxiv.org/abs/2510.03339)
*Sofiane Ennadir,Levente Zólyomi,Oleg Smirnov,Tianze Wang,John Pertoft,Filip Cornell,Lele Cao*

Main category: cs.LG

TL;DR: 本文引入理论框架分析Transformer模型中池化方法的表达能力，进行多模态任务实证评估，统一理论与实证视角，为选择池化机制提供指导。


<details>
  <summary>Details</summary>
Motivation: 以往文献多关注注意力机制，而池化对模型行为有关键影响却未被充分探索。

Method: 引入理论框架推导基于Transformer模型使用常用池化方法的表征能力和区分相似输入能力的闭式边界，分析不同注意力公式变体；进行多模态任务实证评估。

Result: 揭示池化选择对准确性、敏感性和优化行为影响的一致趋势。

Conclusion: 将池化定位为Transformer模型的关键组件，为超越注意力的更有原则的模型设计奠定基础。

Abstract: Transformer models have become the dominant backbone for sequence modeling,
leveraging self-attention to produce contextualized token representations.
These are typically aggregated into fixed-size vectors via pooling operations
for downstream tasks. While much of the literature has focused on attention
mechanisms, the role of pooling remains underexplored despite its critical
impact on model behavior. In this paper, we introduce a theoretical framework
that rigorously characterizes the expressivity of Transformer-based models
equipped with widely used pooling methods by deriving closed-form bounds on
their representational capacity and the ability to distinguish similar inputs.
Our analysis extends to different variations of attention formulations,
demonstrating that these bounds hold across diverse architectural variants. We
empirically evaluate pooling strategies across tasks requiring both global and
local contextual understanding, spanning three major modalities: computer
vision, natural language processing, and time-series analysis. Results reveal
consistent trends in how pooling choices affect accuracy, sensitivity, and
optimization behavior. Our findings unify theoretical and empirical
perspectives, providing practical guidance for selecting or designing pooling
mechanisms suited to specific tasks. This work positions pooling as a key
architectural component in Transformer models and lays the foundation for more
principled model design beyond attention alone.

</details>


### [220] [Technical note on Sequential Test-Time Adaptation via Martingale-Driven Fisher Prompting](https://arxiv.org/abs/2510.03839)
*Behraj Khan,Tahir Qasim Syed*

Main category: cs.LG

TL;DR: 提出M - FISHER理论框架，用于流式数据分布偏移检测与稳定适应，给出检测和适应的相关理论结果，证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决流式数据中顺序分布偏移检测和稳定适应问题。

Method: 检测方面，从非一致性分数构建指数鞅并应用Ville不等式；适应方面，对提示参数进行Fisher预条件更新实现自然梯度下降。

Result: 检测时实现时间一致的误报控制，界定预期检测延迟；适应时实现局部最优更新，最小化KL散度并保持稳定性和参数化不变性。

Conclusion: M - FISHER是协变量偏移下顺序决策中稳健、随时有效的检测和几何稳定适应的原则性方法。

Abstract: We present a theoretical framework for M-FISHER, a method for sequential
distribution shift detection and stable adaptation in streaming data. For
detection, we construct an exponential martingale from non-conformity scores
and apply Ville's inequality to obtain time-uniform guarantees on false alarm
control, ensuring statistical validity at any stopping time. Under sustained
shifts, we further bound the expected detection delay as
$\mathcal{O}(\log(1/\delta)/\Gamma)$, where $\Gamma$ reflects the post-shift
information gain, thereby linking detection efficiency to distributional
divergence. For adaptation, we show that Fisher-preconditioned updates of
prompt parameters implement natural gradient descent on the distributional
manifold, yielding locally optimal updates that minimize KL divergence while
preserving stability and parameterization invariance. Together, these results
establish M-FISHER as a principled approach for robust, anytime-valid detection
and geometrically stable adaptation in sequential decision-making under
covariate shift.

</details>


### [221] [Learning Pareto-Optimal Pandemic Intervention Policies with MORL](https://arxiv.org/abs/2510.03340)
*Marian Chen,Miri Zilka*

Main category: cs.LG

TL;DR: 本文设计了一个结合MORL和新SDE疫情模拟器的框架，用于评估疾病传播预防策略，展示了疫情控制与经济稳定的权衡，并将其应用于不同病原体和麻疹疫情，为公共卫生决策提供支持。


<details>
  <summary>Details</summary>
Motivation: 新冠疫情凸显了平衡疾病控制与社会经济稳定的干预策略的需求。

Method: 设计结合多目标强化学习（MORL）和新随机微分方程（SDE）疫情模拟器的框架，用全球新冠数据校准和验证模拟器，在模拟器上训练Pareto条件网络（PCN）智能体。

Result: 模拟器比其他常用模型有更高保真度，展示了新冠疫情控制与经济稳定的策略权衡，扩展到不同病原体有不同干预策略，应用于麻疹疫情量化了疫苗接种率下降的影响。

Conclusion: 该框架为缓解公共卫生危机的透明、循证政策制定提供了有力且可适应的支持。

Abstract: The COVID-19 pandemic underscored a critical need for intervention strategies
that balance disease containment with socioeconomic stability. We approach this
challenge by designing a framework for modeling and evaluating disease-spread
prevention strategies. Our framework leverages multi-objective reinforcement
learning (MORL) - a formulation necessitated by competing objectives - combined
with a new stochastic differential equation (SDE) pandemic simulator,
calibrated and validated against global COVID-19 data. Our simulator reproduces
national-scale pandemic dynamics with orders of magnitude higher fidelity than
other models commonly used in reinforcement learning (RL) approaches to
pandemic intervention. Training a Pareto-Conditioned Network (PCN) agent on
this simulator, we illustrate the direct policy trade-offs between
epidemiological control and economic stability for COVID-19. Furthermore, we
demonstrate the framework's generality by extending it to pathogens with
different epidemiological profiles, such as polio and influenza, and show how
these profiles lead the agent to discover fundamentally different intervention
policies. To ground our work in contemporary policymaking challenges, we apply
the model to measles outbreaks, quantifying how a modest 5% drop in vaccination
coverage necessitates significantly more stringent and costly interventions to
curb disease spread. This work provides a robust and adaptable framework to
support transparent, evidence-based policymaking for mitigating public health
crises.

</details>


### [222] [Pilot selection in the era of Virtual reality: algorithms for accurate and interpretable machine learning models](https://arxiv.org/abs/2510.03345)
*Luoma Ke,Guangpeng Zhang,Jibo He,Yajing Li,Yan Li,Xufeng Liu,Peng Fang*

Main category: cs.LG

TL;DR: 研究采用机器学习和虚拟现实技术区分不同飞行技能参与者特征，SVM+MIC算法表现最佳，成果可用于飞行员选拔和训练。


<details>
  <summary>Details</summary>
Motivation: 航空业快速发展，需高效选拔飞行员。

Method: 招募23名飞行员和23名新手，应用结合机器学习和虚拟现实技术的新方法区分特征。

Result: SVM+MIC方法在各项指标上预测性能最高，Accuracy为0.93，AUC为0.96，F1为0.93，优于其他算法和特征选择方法。

Conclusion: 新的SVM+MIC算法优于现有飞行员选拔算法，研究的VR模拟平台和算法可用于飞行员选拔和训练。

Abstract: With the rapid growth of the aviation industry, there is a need for a large
number of flight crew. How to select the right pilots in a cost-efficient
manner has become an important research question. In the current study,
twenty-three pilots were recruited from China Eastern Airlines, and 23 novices
were from the community of Tsinghua University. A novel approach incorporating
machine learning and virtual reality technology was applied to distinguish
features between these participants with different flight skills. Results
indicate that SVM with the MIC feature selection method consistently achieved
the highest prediction performance on all metrics with an Accuracy of 0.93, an
AUC of 0.96, and an F1 of 0.93, which outperforms four other classifier
algorithms and two other feature selection methods. From the perspective of
feature selection methods, the MIC method can select features with a nonlinear
relationship to sampling labels, instead of a simple filter-out. Our new
implementation of the SVM + MIC algorithm outperforms all existing pilot
selection algorithms and perhaps provides the first implementation based on eye
tracking and flight dynamics data. This study's VR simulation platforms and
algorithms can be used for pilot selection and training.

</details>


### [223] [On Provable Benefits of Muon in Federated Learning](https://arxiv.org/abs/2510.03866)
*Xinwen Zhang,Hongchang Gao*

Main category: cs.LG

TL;DR: 本文研究Muon优化器在联邦学习中的性能，提出FedMuon算法，理论分析其性质，实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: Muon优化器在联邦学习中的有效性未被探索，为填补这一空白进行研究。

Method: 提出新算法FedMuon，并建立其在非凸问题上的收敛率。

Result: 理论分析显示FedMuon有多个良好性质，学习率与问题特定参数无关，能适应重尾噪声；实验验证算法有效性。

Conclusion: FedMuon算法在联邦学习中表现良好，具有实际应用价值。

Abstract: The recently introduced optimizer, Muon, has gained increasing attention due
to its superior performance across a wide range of applications. However, its
effectiveness in federated learning remains unexplored. To address this gap,
this paper investigates the performance of Muon in the federated learning
setting. Specifically, we propose a new algorithm, FedMuon, and establish its
convergence rate for nonconvex problems. Our theoretical analysis reveals
multiple favorable properties of FedMuon. In particular, due to its
orthonormalized update direction, the learning rate of FedMuon is independent
of problem-specific parameters, and, importantly, it can naturally accommodate
heavy-tailed noise. The extensive experiments on a variety of neural network
architectures validate the effectiveness of the proposed algorithm.

</details>


### [224] [KVComm: Enabling Efficient LLM Communication through Selective KV Sharing](https://arxiv.org/abs/2510.03346)
*Xiangyu Shi,Marco Chiesa,Gerald Q. Maguire Jr.,Dejan Kostic*

Main category: cs.LG

TL;DR: 提出KVComm框架实现大语言模型高效通信，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型多智能体系统通信协议存在推理成本高、信息损失、信息集中偏差和效率低等问题。

Method: 提出KVComm框架，通过选择性共享KV对实现通信，引入基于注意力重要性分数和高斯先验的KV层选择策略。

Result: KVComm在传输仅30%层的KV对时，性能与直接合并输入的上限方法相当。

Conclusion: KV对有潜力成为大语言模型间有效通信媒介，为可扩展高效多智能体系统铺平道路。

Abstract: Large Language Models (LLMs) are increasingly deployed in multi-agent
systems, where effective inter-model communication is crucial. Existing
communication protocols either rely on natural language, incurring high
inference costs and information loss, or on hidden states, which suffer from
information concentration bias and inefficiency. To address these limitations,
we propose KVComm, a novel communication framework that enables efficient
communication between LLMs through selective sharing of KV pairs. KVComm
leverages the rich information encoded in the KV pairs while avoiding the
pitfalls of hidden states. We introduce a KV layer-wise selection strategy
based on attention importance scores with a Gaussian prior to identify the most
informative KV pairs for communication. Extensive experiments across diverse
tasks and model pairs demonstrate that KVComm achieves comparable performance
to the upper-bound method, which directly merges inputs to one model without
any communication, while transmitting as few as 30\% of layers' KV pairs. Our
study highlights the potential of KV pairs as an effective medium for inter-LLM
communication, paving the way for scalable and efficient multi-agent systems.

</details>


### [225] [Optimal Scaling Needs Optimal Norm](https://arxiv.org/abs/2510.03871)
*Oleg Filatov,Jiangtao Wang,Jan Ebert,Stefan Kesselheim*

Main category: cs.LG

TL;DR: 使用Scion优化器发现模型和数据集规模的联合最优缩放受输出层算子范数控制，提出范数转移现象，给出最优学习率/批量大小对缩放规则，调整每层组学习率可提升性能并分享实现和日志。


<details>
  <summary>Details</summary>
Motivation: 当前在模型和数据集缩放的最优超参数转移方面缺乏统一解释原则。

Method: 使用Scion优化器进行研究，测量Scion最优学习率/批量大小对随数据集大小的缩放情况，调整每层组学习率。

Result: 发现范数转移现象，即最优学习率/批量大小对有相同算子范数；Scion的缩放规则与Adam优化器一致；调整每层组学习率可提升性能。

Conclusion: 输出层算子范数是联合最优缩放的一个重要不变量，为大语言模型训练动态研究提供了实际见解和支持。

Abstract: Despite recent progress in optimal hyperparameter transfer under model and
dataset scaling, no unifying explanatory principle has been established. Using
the Scion optimizer, we discover that joint optimal scaling across model and
dataset sizes is governed by a single invariant: the operator norm of the
output layer. Across models with up to 1.3B parameters trained on up to 138B
tokens, the optimal learning rate/batch size pair $(\eta^{\ast}, B^{\ast})$
consistently has the same operator norm value - a phenomenon we term norm
transfer. This constant norm condition is necessary but not sufficient: while
for each dataset size, multiple $(\eta, B)$ reach the optimal norm, only a
unique $(\eta^{\ast}, B^{\ast})$ achieves the best loss. As a sufficient
condition, we provide the first measurement of $(\eta^{\ast}, B^{\ast})$
scaling with dataset size for Scion, and find that the scaling rules are
consistent with those of the Adam optimizer. Tuning per-layer-group learning
rates also improves model performance, with the output layer being the most
sensitive and hidden layers benefiting from lower learning rates. We provide
practical insights on norm-guided optimal scaling and release our Distributed
Scion (Disco) implementation with logs from over two thousand runs to support
research on LLM training dynamics at scale.

</details>


### [226] [AgentCaster: Reasoning-Guided Tornado Forecasting](https://arxiv.org/abs/2510.03349)
*Michael Chen*

Main category: cs.LG

TL;DR: 介绍AgentCaster框架用于龙卷风预测，评估大语言模型性能，发现人类专家表现优于模型。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在复杂现实任务中的推理能力，填补相关评估空白。

Method: 引入AgentCaster框架，利用多模态大语言模型解读时空数据，进行交互式查询预测，提出特定评估指标。

Result: 人类专家显著优于现有模型，模型有幻觉、过度预测风险强度、地理定位不准和时空推理差等问题。

Conclusion: AgentCaster有助于推进关键领域挑战性推理任务中大语言模型代理的研究。

Abstract: There is a growing need to evaluate Large Language Models (LLMs) on complex,
high-impact, real-world tasks to assess their true readiness as reasoning
agents. To address this gap, we introduce AgentCaster, a contamination-free
framework employing multimodal LLMs end-to-end for the challenging,
long-horizon task of tornado forecasting. Within AgentCaster, models interpret
heterogeneous spatiotemporal data from a high-resolution convection-allowing
forecast archive. We assess model performance over a 40-day period featuring
diverse historical data, spanning several major tornado outbreaks and including
over 500 tornado reports. Each day, models query interactively from a pool of
3,625 forecast maps and 40,125 forecast soundings for a forecast horizon of
12-36 hours. Probabilistic tornado-risk polygon predictions are verified
against ground truths derived from geometric comparisons across disjoint risk
bands in projected coordinate space. To quantify accuracy, we propose
domain-specific TornadoBench and TornadoHallucination metrics, with
TornadoBench highly challenging for both LLMs and domain expert human
forecasters. Notably, human experts significantly outperform state-of-the-art
models, which demonstrate a strong tendency to hallucinate and overpredict risk
intensity, struggle with precise geographic placement, and exhibit poor
spatiotemporal reasoning in complex, dynamically evolving systems. AgentCaster
aims to advance research on improving LLM agents for challenging reasoning
tasks in critical domains.

</details>


### [227] [Interpretable Neuropsychiatric Diagnosis via Concept-Guided Graph Neural Networks](https://arxiv.org/abs/2510.03351)
*Song Wang,Zhenyu Lei,Zhen Tan,Jundong Li,Javier Rasero,Aiying Zhang,Chirag Agarwal*

Main category: cs.LG

TL;DR: 针对青少年精神疾病诊断工具问题，提出基于概念的诊断框架CONCEPTNEURO，实验显示其增强的GNN表现更好，且能提供解释。


<details>
  <summary>Details</summary>
Motivation: 近五分之一青少年有精神或行为健康问题，现有基于GNN的疾病预测方法是黑盒模型，缺乏可靠性和临床可解释性，需开发准确且可解释的诊断工具。

Method: 提出CONCEPTNEURO框架，利用大语言模型和神经生物学领域知识生成、过滤和编码可解释的功能连接概念，通过概念分类器进行预测。

Result: 在多个精神疾病数据集上，CONCEPTNEURO增强的GNN始终优于普通GNN，提高了准确性，还能提供透明、符合临床的解释，概念分析突出了与专家知识一致的疾病特定连接模式。

Conclusion: CONCEPTNEURO是一个可解释、结合领域知识的精神疾病诊断框架。

Abstract: Nearly one in five adolescents currently live with a diagnosed mental or
behavioral health condition, such as anxiety, depression, or conduct disorder,
underscoring the urgency of developing accurate and interpretable diagnostic
tools. Resting-state functional magnetic resonance imaging (rs-fMRI) provides a
powerful lens into large-scale functional connectivity, where brain regions are
modeled as nodes and inter-regional synchrony as edges, offering clinically
relevant biomarkers for psychiatric disorders. While prior works use graph
neural network (GNN) approaches for disorder prediction, they remain complex
black-boxes, limiting their reliability and clinical translation. In this work,
we propose CONCEPTNEURO, a concept-based diagnosis framework that leverages
large language models (LLMs) and neurobiological domain knowledge to
automatically generate, filter, and encode interpretable functional
connectivity concepts. Each concept is represented as a structured subgraph
linking specific brain regions, which are then passed through a concept
classifier. Our design ensures predictions through clinically meaningful
connectivity patterns, enabling both interpretability and strong predictive
performance. Extensive experiments across multiple psychiatric disorder
datasets demonstrate that CONCEPTNEURO-augmented GNNs consistently outperform
their vanilla counterparts, improving accuracy while providing transparent,
clinically aligned explanations. Furthermore, concept analyses highlight
disorder-specific connectivity patterns that align with expert knowledge and
suggest new hypotheses for future investigation, establishing CONCEPTNEURO as
an interpretable, domain-informed framework for psychiatric disorder diagnosis.

</details>


### [228] [Slow-Fast Policy Optimization: Reposition-Before-Update for LLM Reasoning](https://arxiv.org/abs/2510.04072)
*Ziyan Wang,Zheng Wang,Jie Fu,Xingwei Qu,Qi Cheng,Shengpu Tang,Minjia Zhang,Xiaoming Huo*

Main category: cs.LG

TL;DR: 提出Slow - Fast Policy Optimization (SFPO)框架解决强化学习早期训练问题，实验表明它能提升稳定性、减少滚动步数并加速收敛。


<details>
  <summary>Details</summary>
Motivation: 现有策略算法如GRPO在早期训练中存在因低质量滚动产生噪声梯度，导致更新不稳定和探索效率低的问题。

Method: 将每一步分解为三个阶段：在同一批次上进行内部步骤的快速短轨迹、控制离策略漂移的重新定位机制和最终的缓慢校正，该设计与现有策略梯度管道兼容。

Result: SFPO持续提升了推理强化学习训练的稳定性，减少了滚动步数，加速了收敛，在数学推理基准上平均比GRPO高2.80分，滚动步数最多减少4.93倍，达到GRPO最佳准确率的时间最多减少4.19倍。

Conclusion: SFPO能有效解决现有策略算法在早期训练中的问题，提升训练效果。

Abstract: Reinforcement learning (RL) has become central to enhancing reasoning in
large language models (LLMs). Yet on-policy algorithms such as Group Relative
Policy Optimization (GRPO) often suffer in early training: noisy gradients from
low-quality rollouts lead to unstable updates and inefficient exploration. We
introduce Slow-Fast Policy Optimization (SFPO), a simple yet efficient
framework to address these limitations via decomposing each step into three
stages: a short fast trajectory of inner steps on the same batch, a reposition
mechanism to control off-policy drift, and a final slow correction. This
reposition-before-update design preserves the objective and rollout process
unchanged, making SFPO plug-compatible with existing policy-gradient pipelines.
Extensive experiments demonstrate that SFPO consistently improves stability,
reduces rollouts, and accelerates convergence of reasoning RL training.
Specifically, it outperforms GRPO by up to 2.80 points in average on math
reasoning benchmarks. It also achieves up to 4.93\texttimes{} fewer rollouts
and a 4.19\texttimes{} reduction in wall-clock time to match GRPO's best
accuracy.

</details>


### [229] [High Cycle S-N curve prediction for Al 7075-T6 alloy using Recurrent Neural Networks (RNNs)](https://arxiv.org/abs/2510.03355)
*Aryan Patel*

Main category: cs.LG

TL;DR: 提出基于LSTM的迁移学习框架预测铝扭转S - N曲线，能降低获取疲劳特性成本。


<details>
  <summary>Details</summary>
Motivation: 铝合金易疲劳失效，获取材料疲劳性能数据耗时成本高，需寻找解决办法。

Method: 开发基于LSTM的迁移学习框架，用纯轴向疲劳数据训练源LSTM模型，再迁移预测高周扭转S - N曲线。

Result: 框架能准确预测更高循环范围的铝扭转S - N曲线。

Conclusion: 该框架能大幅降低获取不同材料疲劳特性的成本，有助于在成本和时间限制下优先安排测试。

Abstract: Aluminum is a widely used alloy, which is susceptible to fatigue failure.
Characterizing fatigue performance for materials is extremely time and cost
demanding, especially for high cycle data. To help mitigate this, a transfer
learning based framework has been developed using Long short-term memory
networks (LSTMs) in which a source LSTM model is trained based on pure axial
fatigue data for Aluminum 7075-T6 alloy which is then transferred to predict
high cycle torsional S-N curves. The framework was able to accurately predict
Al torsional S-N curves for a much higher cycle range. It is the belief that
this framework will help to drastically mitigate the cost of gathering fatigue
characteristics for different materials and help prioritize tests with better
cost and time constraints.

</details>


### [230] [Offline Reinforcement Learning in Large State Spaces: Algorithms and Guarantees](https://arxiv.org/abs/2510.04088)
*Nan Jiang,Tengyang Xie*

Main category: cs.LG

TL;DR: 文章介绍大状态空间离线强化学习理论，提及关键概念、算法结果，还讨论开放问题和关联领域。


<details>
  <summary>Details</summary>
Motivation: 介绍大状态空间离线强化学习理论，该学习方式可从历史数据学策略而无需与环境在线交互。

Method: 引入函数近似的表达性假设和数据覆盖等关键概念，依据不同假设和复杂度保证描述算法与结果。

Result: 呈现了丰富的算法和结果景观。

Conclusion: 文中讨论了开放问题以及与相邻领域的联系。

Abstract: This article introduces the theory of offline reinforcement learning in large
state spaces, where good policies are learned from historical data without
online interactions with the environment. Key concepts introduced include
expressivity assumptions on function approximation (e.g., Bellman completeness
vs. realizability) and data coverage (e.g., all-policy vs. single-policy
coverage). A rich landscape of algorithms and results is described, depending
on the assumptions one is willing to make and the sample and computational
complexity guarantees one wishes to achieve. We also discuss open questions and
connections to adjacent areas.

</details>


### [231] [Understanding Transformers for Time Series: Rank Structure, Flow-of-ranks, and Compressibility](https://arxiv.org/abs/2510.03358)
*Annan Yu,Danielle C. Maddix,Boran Han,Xiyuan Zhang,Abdul Fatir Ansari,Oleksandr Shchur,Christos Faloutsos,Andrew Gordon Wilson,Michael W. Mahoney,Yuyang Wang*

Main category: cs.LG

TL;DR: 本文从秩结构视角分析时间序列Transformer，发现时间序列嵌入有低秩特性，提出秩流概念，用此压缩Chronos模型，减少推理时间和内存，为时间序列模型设计提供指导。


<details>
  <summary>Details</summary>
Motivation: 文本模型的原理难以完美迁移到其他模态模型，且时间序列数据结构与文本或视觉数据不同，需从新视角分析Transformer。

Method: 从秩结构视角分析时间序列Transformer，证明Q/K/V投影可低秩近似，提出秩流概念。

Result: 压缩Chronos模型，推理时间减少65%，内存减少81%，且精度无损失。

Conclusion: 研究结果为时间序列基础模型的宽度、深度和头数分配及利用其可压缩性提供了有原则的指导。

Abstract: Transformers are widely used across data modalities, and yet the principles
distilled from text models often transfer imperfectly to models trained to
other modalities. In this paper, we analyze Transformers through the lens of
rank structure. Our focus is on the time series setting, where the structural
properties of the data differ remarkably from those of text or vision. We show
that time-series embeddings, unlike text or vision, exhibit sharply decaying
singular value spectra: small patch sizes and smooth continuous mappings
concentrate the data into low-rank subspaces. From this, we prove that the
associated $Q/K/V$ projections admit accurate low-rank approximations, and that
attention layers become compressible in proportion to the decay of the
embedding spectrum. We introduce the concept of flow-of-ranks, a phenomenon by
which nonlinear mixing across depth inflates the rank, explaining why early
layers are most amenable to compression and why ranks grow with depth. Guided
by these theoretical and empirical results, we use these insights to compress
Chronos, a large time series foundation model, achieving a reduction of $65\%$
in inference time and $81\%$ in memory, without loss of accuracy. Our findings
provide principled guidance for allocating width, depth, and heads in time
series foundation models, and for exploiting their inherent compressibility.

</details>


### [232] [Physics-informed Neural-operator Predictive Control for Drag Reduction in Turbulent Flows](https://arxiv.org/abs/2510.03360)
*Zelin Zhao,Zongyi Li,Kimia Hassibi,Kamyar Azizzadenesheli,Junchi Yan,H. Jane Bae,Di Zhou,Anima Anandkumar*

Main category: cs.LG

TL;DR: 提出高效深度强化学习框架PINO - PC用于湍流建模与控制，在高雷诺数和未见流动场景中表现优于先前方法，实现显著减阻。


<details>
  <summary>Details</summary>
Motivation: 数值评估壁面摩擦的湍流控制效果需昂贵的湍流动力学模拟，存在重大挑战，因此寻求高效方法。

Method: 提出基于模型的预测控制深度强化学习框架PINO - PC，使用物理信息神经算子（PINO）联合学习湍流控制的策略和观测器模型。

Result: PINO - PC在高雷诺数和未见流动场景中优于先前无模型强化学习方法；在雷诺数为15000时实现39.0%的减阻，比先前流体控制方法高32%以上。

Conclusion: 所提出的PINO - PC框架在湍流建模与控制方面具有有效性和优越性。

Abstract: Assessing turbulence control effects for wall friction numerically is a
significant challenge since it requires expensive simulations of turbulent
fluid dynamics. We instead propose an efficient deep reinforcement learning
(RL) framework for modeling and control of turbulent flows. It is model-based
RL for predictive control (PC), where both the policy and the observer models
for turbulence control are learned jointly using Physics Informed Neural
Operators (PINO), which are discretization invariant and can capture fine
scales in turbulent flows accurately. Our PINO-PC outperforms prior model-free
reinforcement learning methods in various challenging scenarios where the flows
are of high Reynolds numbers and unseen, i.e., not provided during model
training. We find that PINO-PC achieves a drag reduction of 39.0\% under a
bulk-velocity Reynolds number of 15,000, outperforming previous fluid control
methods by more than 32\%.

</details>


### [233] [Arithmetic-Mean $μ$P for Modern Architectures: A Unified Learning-Rate Scale for CNNs and ResNets](https://arxiv.org/abs/2510.04327)
*Haosong Zhang,Shenxi Wu,Yichi Zhang,Wei Lin*

Main category: cs.LG

TL;DR: 提出AM-μP方法解决现代深度网络学习率选择问题，得出学习率缩放规律并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 经典的μP方法在异构架构中存在问题，选择合适学习率仍是现代深度网络扩展深度的关键挑战。

Method: 引入AM-μP，结合残差感知的He扇入初始化，约束网络全局平均一步预激活二阶矩到固定尺度。

Result: 证明一、二维卷积网络最大更新学习率满足η*(L)∝L^(-3/2)，标准残差网络中η*(L)=Θ(L^(-3/2))，实验结果证实-3/2缩放规律。

Conclusion: 该方法为卷积和深度残差网络提供统一实用的学习率原则，无需额外调优。

Abstract: Choosing an appropriate learning rate remains a key challenge in scaling
depth of modern deep networks. The classical maximal update parameterization
($\mu$P) enforces a fixed per-layer update magnitude, which is well suited to
homogeneous multilayer perceptrons (MLPs) but becomes ill-posed in
heterogeneous architectures where residual accumulation and convolutions
introduce imbalance across layers. We introduce Arithmetic-Mean $\mu$P
(AM-$\mu$P), which constrains not each individual layer but the network-wide
average one-step pre-activation second moment to a constant scale. Combined
with a residual-aware He fan-in initialization - scaling residual-branch
weights by the number of blocks ($\mathrm{Var}[W]=c/(K\cdot
\mathrm{fan\text{-}in})$) - AM-$\mu$P yields width-robust depth laws that
transfer consistently across depths. We prove that, for one- and
two-dimensional convolutional networks, the maximal-update learning rate
satisfies $\eta^\star(L)\propto L^{-3/2}$; with zero padding, boundary effects
are constant-level as $N\gg k$. For standard residual networks with general
conv+MLP blocks, we establish $\eta^\star(L)=\Theta(L^{-3/2})$, with $L$ the
minimal depth. Empirical results across a range of depths confirm the $-3/2$
scaling law and enable zero-shot learning-rate transfer, providing a unified
and practical LR principle for convolutional and deep residual networks without
additional tuning overhead.

</details>


### [234] [Diffusion-Based, Data-Assimilation-Enabled Super-Resolution of Hub-height Winds](https://arxiv.org/abs/2510.03364)
*Xiaolong Ma,Xu Dong,Ashley Tarrant,Lei Yang,Rao Kotamarthi,Jiali Wang,Feng Yan,Rajkumar Kettimuthu*

Main category: cs.LG

TL;DR: 本文介绍了用于轮毂高度风超分辨率降尺度的扩散模型WindSR，该模型结合观测数据与模拟场，表现优于基线模型，数据同化降低了模型偏差。


<details>
  <summary>Details</summary>
Motivation: 高质量的轮毂高度风观测数据时空分布稀疏，模拟数据有偏差且分辨率低，难以用于风电场选址和评估极端天气风险，需要充分利用两类数据生成高质量、高分辨率的轮毂高度风速。

Method: 引入WindSR模型，在降尺度过程中使用先进的扩散模型将稀疏观测数据与模拟场集成，采用动态半径混合方法合并观测与模拟数据，在训练和推理中纳入地形信息。

Result: 与卷积神经网络和生成对抗网络基线模型相比，WindSR在降尺度效率和准确性上表现更优，数据同化使WindSR相对于独立观测的模型偏差降低约20%。

Conclusion: WindSR模型能有效利用观测和模拟数据进行轮毂高度风的超分辨率降尺度，具有较好的效率和准确性。

Abstract: High-quality observations of hub-height winds are valuable but sparse in
space and time. Simulations are widely available on regular grids but are
generally biased and too coarse to inform wind-farm siting or to assess
extreme-weather-related risks (e.g., gusts) at infrastructure scales. To fully
utilize both data types for generating high-quality, high-resolution hub-height
wind speeds (tens to ~100m above ground), this study introduces WindSR, a
diffusion model with data assimilation for super-resolution downscaling of
hub-height winds. WindSR integrates sparse observational data with simulation
fields during downscaling using state-of-the-art diffusion models. A
dynamic-radius blending method is introduced to merge observations with
simulations, providing conditioning for the diffusion process. Terrain
information is incorporated during both training and inference to account for
its role as a key driver of winds. Evaluated against
convolutional-neural-network and generative-adversarial-network baselines,
WindSR outperforms them in both downscaling efficiency and accuracy. Our data
assimilation reduces WindSR's model bias by approximately 20% relative to
independent observations.

</details>


### [235] [Disentangling Recall and Reasoning in Transformer Models through Layer-wise Attention and Activation Analysis](https://arxiv.org/abs/2510.03366)
*Harshwardhan Fartale,Ashish Kattamuri,Rahul Raja,Arpita Vats,Ishita Prasad,Akshata Kishore Moharir*

Main category: cs.LG

TL;DR: 通过机制解释性研究发现transformer模型中回忆和推理依赖可分离但相互作用的电路。


<details>
  <summary>Details</summary>
Motivation: 区分transformer模型回忆和推理能力的内部机制，用于预测模型泛化、设计评估和构建安全干预措施。

Method: 使用合成语言谜题的受控数据集，结合激活修补和结构化消融，在层、头和神经元级别探测transformer模型。

Result: 干预不同层和注意力头会导致选择性损伤，神经元有任务特定的放电模式但效果不那么稳健。

Conclusion: 回忆和推理依赖可分离但相互作用的电路，研究推动了机制解释性，为大语言模型安全部署提供信息。

Abstract: Transformer-based language models excel at both recall (retrieving memorized
facts) and reasoning (performing multi-step inference), but whether these
abilities rely on distinct internal mechanisms remains unclear. Distinguishing
recall from reasoning is crucial for predicting model generalization, designing
targeted evaluations, and building safer interventions that affect one ability
without disrupting the other.We approach this question through mechanistic
interpretability, using controlled datasets of synthetic linguistic puzzles to
probe transformer models at the layer, head, and neuron level. Our pipeline
combines activation patching and structured ablations to causally measure
component contributions to each task type. Across two model families (Qwen and
LLaMA), we find that interventions on distinct layers and attention heads lead
to selective impairments: disabling identified "recall circuits" reduces
fact-retrieval accuracy by up to 15\% while leaving reasoning intact, whereas
disabling "reasoning circuits" reduces multi-step inference by a comparable
margin. At the neuron level, we observe task-specific firing patterns, though
these effects are less robust, consistent with neuronal polysemanticity.Our
results provide the first causal evidence that recall and reasoning rely on
separable but interacting circuits in transformer models. These findings
advance mechanistic interpretability by linking circuit-level structure to
functional specialization and demonstrate how controlled datasets and causal
interventions can yield mechanistic insights into model cognition, informing
safer deployment of large language models.

</details>


### [236] [Domain Generalization: A Tale of Two ERMs](https://arxiv.org/abs/2510.04441)
*Yilun Zhu,Naihao Deng,Naichen Shi,Aditya Gangrade,Clayton Scott*

Main category: cs.LG

TL;DR: 研究领域泛化问题，指出在协变量偏移假设数据集上难超经验风险最小化（ERM），在后验漂移假设下，领域信息增强的ERM表现更好。


<details>
  <summary>Details</summary>
Motivation: 解决领域泛化中在新测试域无标签数据时的泛化问题，探讨不同假设下超越池化ERM的方法。

Method: 提出“领域信息增强的ERM”方法，即特征向量与特定领域信息结合，并构建理论框架。

Result: 在语言和视觉任务实验中表明在满足后验漂移假设时，领域信息增强的ERM优于池化ERM。

Conclusion: 领域泛化在不同假设下有不同表现，后验漂移假设下领域信息增强的ERM是更优选择。

Abstract: Domain generalization (DG) is the problem of generalizing from several
distributions (or domains), for which labeled training data are available, to a
new test domain for which no labeled data is available. A common finding in the
DG literature is that it is difficult to outperform empirical risk minimization
(ERM) on the pooled training data.
  In this work, we argue that this finding has primarily been reported for
datasets satisfying a \emph{covariate shift} assumption. When the dataset
satisfies a \emph{posterior drift} assumption instead, we show that
``domain-informed ERM,'' wherein feature vectors are augmented with
domain-specific information, outperforms pooling ERM. These claims are
supported by a theoretical framework and experiments on language and vision
tasks.

</details>


### [237] [Conditional Pseudo-Supervised Contrast for Data-Free Knowledge Distillation](https://arxiv.org/abs/2510.03375)
*Renrong Shao,Wei Zhang,Jun wang*

Main category: cs.LG

TL;DR: 本文提出CPSC - DFKD学习范式解决数据无知识蒸馏现有方法的问题，实验验证其性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有数据无知识蒸馏方法存在未探索伪监督范式、无法区分样本类别分布和优化类别多样性样本等问题，需改进。

Method: 提出CPSC - DFKD，包括引入条件生成对抗网络合成特定类别多样图像、改进生成器模块区分类别分布、提出基于师生视角的伪监督对比学习。

Result: 在三个常用数据集上的综合实验验证了CPSC - DFKD对学生模型和生成器的性能提升。

Conclusion: CPSC - DFKD能有效解决现有数据无知识蒸馏方法的局限，提升性能。

Abstract: Data-free knowledge distillation~(DFKD) is an effective manner to solve model
compression and transmission restrictions while retaining privacy protection,
which has attracted extensive attention in recent years. Currently, the
majority of existing methods utilize a generator to synthesize images to
support the distillation. Although the current methods have achieved great
success, there are still many issues to be explored. Firstly, the outstanding
performance of supervised learning in deep learning drives us to explore a
pseudo-supervised paradigm on DFKD. Secondly, current synthesized methods
cannot distinguish the distributions of different categories of samples, thus
producing ambiguous samples that may lead to an incorrect evaluation by the
teacher. Besides, current methods cannot optimize the category-wise diversity
samples, which will hinder the student model learning from diverse samples and
further achieving better performance. In this paper, to address the above
limitations, we propose a novel learning paradigm, i.e., conditional
pseudo-supervised contrast for data-free knowledge distillation~(CPSC-DFKD).
The primary innovations of CPSC-DFKD are: (1) introducing a conditional
generative adversarial network to synthesize category-specific diverse images
for pseudo-supervised learning, (2) improving the modules of the generator to
distinguish the distributions of different categories, and (3) proposing
pseudo-supervised contrastive learning based on teacher and student views to
enhance diversity. Comprehensive experiments on three commonly-used datasets
validate the performance lift of both the student and generator brought by
CPSC-DFKD. The code is available at https://github.com/RoryShao/CPSC-DFKD.git

</details>


### [238] [A Robust Clustered Federated Learning Approach for Non-IID Data with Quantity Skew](https://arxiv.org/abs/2510.03380)
*Michael Ben Ali,Imen Megdiche,André Peninou,Olivier Teste*

Main category: cs.LG

TL;DR: 本文评估了现有CFL算法在非IID设置下的表现，提出新的迭代CFL算法CORNFLQS，实验表明其在准确性、聚类质量和抗QS扰动方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 多数CFL方法缺乏在QS下的系统评估，且因QS面临重大挑战，需要评估现有算法并提出更优算法。

Method: 评估现有CFL算法在多种非IID设置和QS场景下的表现；提出CORNFLQS算法，协调CFL的两种操作策略。

Result: 在六个图像分类数据集上进行270种非IID配置实验，CORNFLQS在准确性和聚类质量上平均排名最高，对QS扰动有强鲁棒性。

Conclusion: 本文提出的CORNFLQS算法优于现有的CFL算法。

Abstract: Federated Learning (FL) is a decentralized paradigm that enables a
client-server architecture to collaboratively train a global Artificial
Intelligence model without sharing raw data, thereby preserving privacy. A key
challenge in FL is Non-IID data. Quantity Skew (QS) is a particular problem of
Non-IID, where clients hold highly heterogeneous data volumes. Clustered
Federated Learning (CFL) is an emergent variant of FL that presents a promising
solution to Non-IID problem. It improves models' performance by grouping
clients with similar data distributions into clusters. CFL methods generally
fall into two operating strategies. In the first strategy, clients select the
cluster that minimizes the local training loss. In the second strategy, the
server groups clients based on local model similarities. However, most CFL
methods lack systematic evaluation under QS but present significant challenges
because of it. In this paper, we present two main contributions. The first one
is an evaluation of state-of-the-art CFL algorithms under various Non-IID
settings, applying multiple QS scenarios to assess their robustness. Our second
contribution is a novel iterative CFL algorithm, named CORNFLQS, which proposes
an optimal coordination between both operating strategies of CFL. Our approach
is robust against the different variations of QS settings. We conducted
intensive experiments on six image classification datasets, resulting in 270
Non-IID configurations. The results show that CORNFLQS achieves the highest
average ranking in both accuracy and clustering quality, as well as strong
robustness to QS perturbations. Overall, our approach outperforms actual CFL
algorithms.

</details>


### [239] [Cross-Modal Reconstruction Pretraining for Ramp Flow Prediction at Highway Interchanges](https://arxiv.org/abs/2510.03381)
*Yongchao Li,Jun Chen,Zhuoxuan Li,Chao Gao,Yang Li,Chu Zhang,Changyin Dong*

Main category: cs.LG

TL;DR: 提出STDAE框架解决高速公路互通匝道缺乏实时检测器导致交通预测有盲点的问题，实验显示其效果良好。


<details>
  <summary>Details</summary>
Motivation: 高速公路互通匝道缺乏实时检测器，导致交通预测存在盲点。

Method: 提出Spatio - Temporal Decoupled Autoencoder (STDAE)两阶段框架，第一阶段从主线数据重建历史匝道流量，其解耦架构提取特征；预测阶段将学习到的表示与GWNet等模型集成。

Result: 在三个真实世界互通数据集上，STDAE - GWNET始终优于十三个最先进的基线，性能与使用历史匝道数据的模型相当。

Conclusion: STDAE能有效克服检测器稀缺问题，具有即插即用潜力用于不同预测流程。

Abstract: Interchanges are crucial nodes for vehicle transfers between highways, yet
the lack of real-time ramp detectors creates blind spots in traffic prediction.
To address this, we propose a Spatio-Temporal Decoupled Autoencoder (STDAE), a
two-stage framework that leverages cross-modal reconstruction pretraining. In
the first stage, STDAE reconstructs historical ramp flows from mainline data,
forcing the model to capture intrinsic spatio-temporal relations. Its decoupled
architecture with parallel spatial and temporal autoencoders efficiently
extracts heterogeneous features. In the prediction stage, the learned
representations are integrated with models such as GWNet to enhance accuracy.
Experiments on three real-world interchange datasets show that STDAE-GWNET
consistently outperforms thirteen state-of-the-art baselines and achieves
performance comparable to models using historical ramp data. This demonstrates
its effectiveness in overcoming detector scarcity and its plug-and-play
potential for diverse forecasting pipelines.

</details>


### [240] [Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in Masked Diffusion](https://arxiv.org/abs/2510.04525)
*Satoshi Hayakawa,Yuhta Takida,Masaaki Imaizumi,Hiromi Wakaki,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: 本文理论分析MaskGIT采样器，提出矩采样器，并通过创新技术提高效率，实验验证理论和方法有效性。


<details>
  <summary>Details</summary>
Motivation: 加速掩码扩散模型的采样过程，探索高效的掩码扩散采样器。

Method: 理论分析MaskGIT采样器，提出矩采样器，采用选择 - 采样方法，使用部分缓存技术和混合方法提高效率。

Result: 实验在图像和文本领域验证了理论以及所提方法的效率。

Conclusion: 推进了掩码扩散采样器的理论理解和实际应用。

Abstract: Masked diffusion models have shown promising performance in generating
high-quality samples in a wide range of domains, but accelerating their
sampling process remains relatively underexplored. To investigate efficient
samplers for masked diffusion, this paper theoretically analyzes the MaskGIT
sampler for image modeling, revealing its implicit temperature sampling
mechanism. Through this analysis, we introduce the "moment sampler," an
asymptotically equivalent but more tractable and interpretable alternative to
MaskGIT, which employs a "choose-then-sample" approach by selecting unmasking
positions before sampling tokens. In addition, we improve the efficiency of
choose-then-sample algorithms through two key innovations: a partial caching
technique for transformers that approximates longer sampling trajectories
without proportional computational cost, and a hybrid approach formalizing the
exploration-exploitation trade-off in adaptive unmasking. Experiments in image
and text domains demonstrate our theory as well as the efficiency of our
proposed methods, advancing both theoretical understanding and practical
implementation of masked diffusion samplers.

</details>


### [241] [Studying the Korean Word-Chain Game with RLVR:Mitigating Reward Conflicts via Curriculum Learning](https://arxiv.org/abs/2510.03394)
*Donghwan Rho*

Main category: cs.LG

TL;DR: 研究用带可验证奖励的强化学习（RLVR）处理韩语单词接龙游戏，发现规则奖励冲突及缓解方法，激励对不同语言谜题任务的研究。


<details>
  <summary>Details</summary>
Motivation: 运用RLVR研究韩语单词接龙游戏，探索其在不同逻辑谜题中的应用。

Method: 采用课程学习方案处理RLVR中规则派生奖励的冲突。

Result: 实验证明课程学习方案可缓解规则派生奖励的冲突。

Conclusion: 研究结果激励对不同语言谜题任务的进一步研究。

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a promising approach
for training large language models (LLMs) with stronger reasoning abilities. It
has also been applied to a variety of logic puzzles. In this work, we study the
Korean word-chain game using RLVR. We show that rule-derived rewards can
naturally conflict, and demonstrate through experiments that a
curriculum-learning scheme mitigates these conflicts. Our findings motivate
further studies of puzzle tasks in diverse languages.

</details>


### [242] [Graph-based Tabular Deep Learning Should Learn Feature Interactions, Not Just Make Predictions](https://arxiv.org/abs/2510.04543)
*Elias Dubbeldam,Reza Mohammadi,Marit Schoonhoven,S. Ilker Birbil*

Main category: cs.LG

TL;DR: 深度学习处理表格数据难敌传统树模型，图基表格深度学习应超越预测目标，重视特征交互结构学习。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习处理表格数据不如传统树模型，现有图基表格深度学习方法重预测轻图结构建模。

Method: 使用已知真实图结构的合成数据集测试现有图基表格深度学习方法。

Result: 现有方法无法恢复有意义特征交互，强制真实交互结构可提升预测性能。

Conclusion: 图基表格深度学习需优先进行定量评估和准确结构学习，转向结构感知建模。

Abstract: Despite recent progress, deep learning methods for tabular data still
struggle to compete with traditional tree-based models. A key challenge lies in
modeling complex, dataset-specific feature interactions that are central to
tabular data. Graph-based tabular deep learning (GTDL) methods aim to address
this by representing features and their interactions as graphs. However,
existing methods predominantly optimize predictive accuracy, neglecting
accurate modeling of the graph structure. This position paper argues that GTDL
should move beyond prediction-centric objectives and prioritize the explicit
learning and evaluation of feature interactions. Using synthetic datasets with
known ground-truth graph structures, we show that existing GTDL methods fail to
recover meaningful feature interactions. Moreover, enforcing the true
interaction structure improves predictive performance. This highlights the need
for GTDL methods to prioritize quantitative evaluation and accurate structural
learning. We call for a shift toward structure-aware modeling as a foundation
for building GTDL systems that are not only accurate but also interpretable,
trustworthy, and grounded in domain understanding.

</details>


### [243] [Training Variation of Physically-Informed Deep Learning Models](https://arxiv.org/abs/2510.03416)
*Ashley Lenau,Dennis Dimiduk,Stephen R. Niezgoda*

Main category: cs.LG

TL;DR: 文章探讨深度学习训练算法可靠性和可重复性，以Pix2Pix网络为例研究不同损失函数，还分享报告模型变化的建议。


<details>
  <summary>Details</summary>
Motivation: 现有对训练算法可靠性和可重复性讨论不足，物理信息损失函数流行引发对其可靠性的疑问。

Method: 以预测高弹性对比复合材料应力场的Pix2Pix网络为案例，实现几种不同的强制应力平衡的损失函数。

Result: 不同损失函数在多次训练中，收敛性、准确性和强制应力平衡方面有不同程度的变化。

Conclusion: 需要报告模型变化来评估损失函数训练网络遵守边界条件的能力，还分享了相关建议。

Abstract: A successful deep learning network is highly dependent not only on the
training dataset, but the training algorithm used to condition the network for
a given task. The loss function, dataset, and tuning of hyperparameters all
play an essential role in training a network, yet there is not much discussion
on the reliability or reproducibility of a training algorithm. With the rise in
popularity of physics-informed loss functions, this raises the question of how
reliable one's loss function is in conditioning a network to enforce a
particular boundary condition. Reporting the model variation is needed to
assess a loss function's ability to consistently train a network to obey a
given boundary condition, and provides a fairer comparison among different
methods. In this work, a Pix2Pix network predicting the stress fields of high
elastic contrast composites is used as a case study. Several different loss
functions enforcing stress equilibrium are implemented, with each displaying
different levels of variation in convergence, accuracy, and enforcing stress
equilibrium across many training sessions. Suggested practices in reporting
model variation are also shared.

</details>


### [244] [SONA: Learning Conditional, Unconditional, and Mismatching-Aware Discriminator](https://arxiv.org/abs/2510.04576)
*Yuhta Takida,Satoshi Hayakawa,Takashi Shibuya,Masaaki Imaizumi,Naoki Murata,Bac Nguyen,Toshimitsu Uesaka,Chieh-Hsin Lai,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: 提出新的判别器设计SONA解决条件生成难题，实验表明其在样本质量和条件对齐方面优于现有方法，且在文本到图像生成中有效。


<details>
  <summary>Details</summary>
Motivation: 现有条件生成对抗网络难以在条件判别器中平衡评估输入样本真实性和条件对齐的双重目标。

Method: 提出新判别器设计，集成无条件判别、匹配感知监督和自适应加权三种能力，引入SONA，在最后一层对自然度和对齐使用单独投影，有专门目标函数和自适应加权机制。

Result: 在类条件生成任务实验中，SONA在样本质量和条件对齐上优于现有方法，在文本到图像生成中也有效。

Conclusion: 所提方法具有通用性和鲁棒性。

Abstract: Deep generative models have made significant advances in generating complex
content, yet conditional generation remains a fundamental challenge. Existing
conditional generative adversarial networks often struggle to balance the dual
objectives of assessing authenticity and conditional alignment of input samples
within their conditional discriminators. To address this, we propose a novel
discriminator design that integrates three key capabilities: unconditional
discrimination, matching-aware supervision to enhance alignment sensitivity,
and adaptive weighting to dynamically balance all objectives. Specifically, we
introduce Sum of Naturalness and Alignment (SONA), which employs separate
projections for naturalness (authenticity) and alignment in the final layer
with an inductive bias, supported by dedicated objective functions and an
adaptive weighting mechanism. Extensive experiments on class-conditional
generation tasks show that \ours achieves superior sample quality and
conditional alignment compared to state-of-the-art methods. Furthermore, we
demonstrate its effectiveness in text-to-image generation, confirming the
versatility and robustness of our approach.

</details>


### [245] [Memory-Efficient Backpropagation for Fine-Tuning LLMs on Resource-Constrained Mobile Devices](https://arxiv.org/abs/2510.03425)
*Congzheng Song,Xinyu Tang*

Main category: cs.LG

TL;DR: 提出在移动设备上内存高效的反向传播实现MeBP，在内存使用和计算时间间取得平衡，比ZO基线收敛快、性能好，在iPhone 15 Pro Max验证，可在小于1GB内存微调0.5B - 4B参数LLMs并开源代码。


<details>
  <summary>Details</summary>
Motivation: 传统基于反向传播微调大语言模型内存消耗大，不适用于资源受限移动设备；零阶优化虽减少内存但收敛慢。

Method: 提出内存高效的反向传播实现MeBP。

Result: 在iPhone 15 Pro Max上验证MeBP有效性，能在小于1GB内存微调0.5B - 4B参数的各种大语言模型。

Conclusion: MeBP在内存使用和计算时间上取得更好平衡，比零阶优化收敛更快、性能更好。

Abstract: Fine-tuning large language models (LLMs) with backpropagation\textemdash even
for a subset of parameters such as LoRA\textemdash can be much more
memory-consuming than inference and is often deemed impractical for
resource-constrained mobile devices. Alternative methods, such as zeroth-order
optimization (ZO), can greatly reduce the memory footprint but come at the cost
of significantly slower model convergence (10$\times$ to 100$\times$ more steps
than backpropagation). We propose a memory-efficient implementation of
backpropagation (MeBP) on mobile devices that provides better trade-off between
memory usage and compute time, while converging faster and achieving better
performance than the ZO baseline. We verify the effectiveness of MeBP on an
iPhone 15 Pro Max and show that various LLMs, ranging from 0.5B to 4B
parameters, can be fine-tuned using less than 1GB of memory. We release an
example of the MeBP implementation at https://github.com/apple/ml-mebp.

</details>


### [246] [Busemann Functions in the Wasserstein Space: Existence, Closed-Forms, and Applications to Slicing](https://arxiv.org/abs/2510.04579)
*Clément Bonet,Elsa Cazelles,Lucas Drumetz,Nicolas Courty*

Main category: cs.LG

TL;DR: 研究Wasserstein空间中Busemann函数的存在性与计算，给出两种情况的闭式表达式，定义新距离并在数据集上验证效率。


<details>
  <summary>Details</summary>
Motivation: Busemann函数在几何机器学习问题有应用，数据可建模为概率分布，自然要在Wasserstein空间研究该函数。

Method: 研究Wasserstein空间中Busemann函数存在性与计算，推导一维分布和高斯测度两种情况的闭式表达式。

Result: 得到一维分布和高斯测度的闭式表达式，可进行概率分布的显式投影，定义新的Sliced - Wasserstein距离。

Conclusion: 提出的方案在合成数据集和迁移学习问题上有效。

Abstract: The Busemann function has recently found much interest in a variety of
geometric machine learning problems, as it naturally defines projections onto
geodesic rays of Riemannian manifolds and generalizes the notion of
hyperplanes. As several sources of data can be conveniently modeled as
probability distributions, it is natural to study this function in the
Wasserstein space, which carries a rich formal Riemannian structure induced by
Optimal Transport metrics. In this work, we investigate the existence and
computation of Busemann functions in Wasserstein space, which admits geodesic
rays. We establish closed-form expressions in two important cases:
one-dimensional distributions and Gaussian measures. These results enable
explicit projection schemes for probability distributions on $\mathbb{R}$,
which in turn allow us to define novel Sliced-Wasserstein distances over
Gaussian mixtures and labeled datasets. We demonstrate the efficiency of those
original schemes on synthetic datasets as well as transfer learning problems.

</details>


### [247] [Generalized Orders of Magnitude for Scalable, Parallel, High-Dynamic-Range Computation](https://arxiv.org/abs/2510.03426)
*Franz A. Heinsen,Leo Kozachkov*

Main category: cs.LG

TL;DR: 引入广义数量级（GOOMs）解决实数复合计算数值溢出问题，在多实验中表现优于传统方法，为高动态范围应用提供新选择。


<details>
  <summary>Details</summary>
Motivation: 许多领域实数复合计算易出现数值下溢或上溢问题。

Method: 引入GOOMs，结合高效自定义并行前缀扫描，支持在GPU等并行硬件上执行。

Result: 三个代表性实验中，GOOMs实现优于传统方法，使原本不可行的计算变得可行。

Conclusion: GOOMs结合高效并行扫描，为高动态范围应用提供可扩展且数值稳健的替代方案。

Abstract: Many domains, from deep learning to finance, require compounding real numbers
over long sequences, often leading to catastrophic numerical underflow or
overflow. We introduce generalized orders of magnitude (GOOMs), a principled
extension of traditional orders of magnitude that incorporates floating-point
numbers as a special case, and which in practice enables stable computation
over significantly larger dynamic ranges of real numbers than previously
possible. We implement GOOMs, along with an efficient custom parallel prefix
scan, to support native execution on parallel hardware such as GPUs. We
demonstrate that our implementation of GOOMs outperforms traditional approaches
with three representative experiments, all of which were previously considered
impractical or impossible, and now become possible and practical: (1)
compounding real matrix products far beyond standard floating-point limits; (2)
estimating spectra of Lyapunov exponents in parallel, orders of magnitude
faster than with previous methods, applying a novel selective-resetting method
to prevent state colinearity; and (3) capturing long-range dependencies in deep
recurrent neural networks with non-diagonal recurrent states, computed in
parallel via a prefix scan, without requiring any form of stabilization. Our
results show that our implementation of GOOMs, combined with efficient parallel
scanning, offers a scalable and numerically robust alternative to conventional
floating-point numbers for high-dynamic-range applications.

</details>


### [248] [Closed-Form Last Layer Optimization](https://arxiv.org/abs/2510.04606)
*Alexandre Galashov,Nathaël Da Costa,Liyuan Xu,Philipp Hennig,Arthur Gretton*

Main category: cs.LG

TL;DR: 提出优化神经网络的方法，在平方损失下将最后一层视为骨干参数的函数，仅优化骨干参数，在多个监督任务中证明有效。


<details>
  <summary>Details</summary>
Motivation: 在平方损失下，神经网络通常用随机梯度下降变体优化，而线性最后一层权重的最优解有闭式解，希望利用这一特性优化网络。

Method: 将最后一层视为骨干参数的函数，仅优化骨干参数，等价于骨干梯度下降和最后一层闭式更新交替；在随机梯度下降设置中权衡当前批次损失和之前批次积累信息；证明在神经切线核机制下收敛到最优解。

Result: 在多个监督任务（回归和分类）中，与标准SGD在平方损失下相比，方法更有效。

Conclusion: 所提方法在平方损失的神经网络优化中有效。

Abstract: Neural networks are typically optimized with variants of stochastic gradient
descent. Under a squared loss, however, the optimal solution to the linear last
layer weights is known in closed-form. We propose to leverage this during
optimization, treating the last layer as a function of the backbone parameters,
and optimizing solely for these parameters. We show this is equivalent to
alternating between gradient descent steps on the backbone and closed-form
updates on the last layer. We adapt the method for the setting of stochastic
gradient descent, by trading off the loss on the current batch against the
accumulated information from previous batches. Further, we prove that, in the
Neural Tangent Kernel regime, convergence of this method to an optimal solution
is guaranteed. Finally, we demonstrate the effectiveness of our approach
compared with standard SGD on a squared loss in several supervised tasks --
both regression and classification -- including Fourier Neural Operators and
Instrumental Variable Regression.

</details>


### [249] [LHGEL: Large Heterogeneous Graph Ensemble Learning using Batch View Aggregation](https://arxiv.org/abs/2510.03432)
*Jiajun Shen,Yufei Jin,Yi He,Xingquan Zhu*

Main category: cs.LG

TL;DR: 本文提出LHGEL集成框架解决大规模异质图学习问题，理论和实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 大规模异质图学习因网络规模、节点和边类型异质性等面临挑战，需要有效方法。

Method: 提出LHGEL框架，含批量视图聚合、残差注意力和多样性正则化三个关键组件。

Result: 理论证明残差注意力可缓解梯度消失问题，实验表明LHGEL在五个真实异质网络上显著优于现有方法。

Conclusion: LHGEL是解决大规模异质图学习问题的有效方法。

Abstract: Learning from large heterogeneous graphs presents significant challenges due
to the scale of networks, heterogeneity in node and edge types, variations in
nodal features, and complex local neighborhood structures. This paper advocates
for ensemble learning as a natural solution to this problem, whereby training
multiple graph learners under distinct sampling conditions, the ensemble
inherently captures different aspects of graph heterogeneity. Yet, the crux
lies in combining these learners to meet global optimization objective while
maintaining computational efficiency on large-scale graphs. In response, we
propose LHGEL, an ensemble framework that addresses these challenges through
batch sampling with three key components, namely batch view aggregation,
residual attention, and diversity regularization. Specifically, batch view
aggregation samples subgraphs and forms multiple graph views, while residual
attention adaptively weights the contributions of these views to guide node
embeddings toward informative subgraphs, thereby improving the accuracy of base
learners. Diversity regularization encourages representational disparity across
embedding matrices derived from different views, promoting model diversity and
ensemble robustness. Our theoretical study demonstrates that residual attention
mitigates gradient vanishing issues commonly faced in ensemble learning.
Empirical results on five real heterogeneous networks validate that our LHGEL
approach consistently outperforms its state-of-the-art competitors by
substantial margin. Codes and datasets are available at
https://github.com/Chrisshen12/LHGEL.

</details>


### [250] [When Do Credal Sets Stabilize? Fixed-Point Theorems for Credal Set Updates](https://arxiv.org/abs/2510.04769)
*Michele Caprio,Siu Lun Chau,Krikamol Muandet*

Main category: cs.LG

TL;DR: 分析不精确概率机器学习中迭代更新过程是否收敛到稳定不动点，并以Credal Bayesian Deep Learning为例说明，揭示不精确性学习的稳定性结构条件。


<details>
  <summary>Details</summary>
Motivation: 不精确概率机器学习中迭代更新过程是否收敛到稳定不动点的问题未被分析，需要研究在更新机制下不动点存在及可达的条件。

Method: 对不精确概率机器学习中迭代更新问题进行首次分析，并以Credal Bayesian Deep Learning为例阐述结果。

Result: 发现将不精确性纳入学习过程不仅丰富不确定性表示，还揭示了稳定性出现的结构条件。

Conclusion: 为不精确性下迭代学习的动态提供了新见解。

Abstract: Many machine learning algorithms rely on iterative updates of uncertainty
representations, ranging from variational inference and
expectation-maximization, to reinforcement learning, continual learning, and
multi-agent learning. In the presence of imprecision and ambiguity, credal sets
-- closed, convex sets of probability distributions -- have emerged as a
popular framework for representing imprecise probabilistic beliefs. Under such
imprecision, many learning problems in imprecise probabilistic machine learning
(IPML) may be viewed as processes involving successive applications of update
rules on credal sets. This naturally raises the question of whether this
iterative process converges to stable fixed points -- or, more generally, under
what conditions on the updating mechanism such fixed points exist, and whether
they can be attained. We provide the first analysis of this problem and
illustrate our findings using Credal Bayesian Deep Learning as a concrete
example. Our work demonstrates that incorporating imprecision into the learning
process not only enriches the representation of uncertainty, but also reveals
structural conditions under which stability emerges, thereby offering new
insights into the dynamics of iterative learning under imprecision.

</details>


### [251] [The Argument is the Explanation: Structured Argumentation for Trust in Agents](https://arxiv.org/abs/2510.03442)
*Ege Cakar,Per Ola Kristensson*

Main category: cs.LG

TL;DR: 论文提出用结构化论证为AI可解释性提供解释和验证，在论证关系分类上取得SOTA，还在多智能体风险评估等场景应用并提供部署方案。


<details>
  <summary>Details</summary>
Motivation: 人类社会通过评估可验证的论点运行，AI可解释性应遵循此原则，利益相关者需要可验证的推理链而非机械透明度。

Method: 采用结构化论证，将LLM文本转换为论证图，使用双极假设论证捕获关系，提供验证机制。

Result: 在AAEC和论证微文本关系分类上取得SOTA成绩，能实现自动幻觉检测和迭代细化。

Conclusion: 结构化论证为AI可解释性提供了有效方法，还提供了易于部署的方案。

Abstract: Humans are black boxes -- we cannot observe their neural processes, yet
society functions by evaluating verifiable arguments. AI explainability should
follow this principle: stakeholders need verifiable reasoning chains, not
mechanistic transparency. We propose using structured argumentation to provide
a level of explanation and verification neither interpretability nor
LLM-generated explanation is able to offer. Our pipeline achieves
state-of-the-art 94.44 macro F1 on the AAEC published train/test split (5.7
points above prior work) and $0.81$ macro F1, $\sim$0.07 above previous
published results with comparable data setups, for Argumentative MicroTexts
relation classification, converting LLM text into argument graphs and enabling
verification at each inferential step. We demonstrate this idea on multi-agent
risk assessment using the Structured What-If Technique, where specialized
agents collaborate transparently to carry out risk assessment otherwise
achieved by humans alone. Using Bipolar Assumption-Based Argumentation, we
capture support/attack relationships, thereby enabling automatic hallucination
detection via fact nodes attacking arguments. We also provide a verification
mechanism that enables iterative refinement through test-time feedback without
retraining. For easy deployment, we provide a Docker container for the
fine-tuned AMT model, and the rest of the code with the Bipolar ABA Python
package on GitHub.

</details>


### [252] [On Structured State-Space Duality](https://arxiv.org/abs/2510.04944)
*Jerry Yao-Chieh Hu,Xiwen Zhang,Weimin Wu,Han Liu*

Main category: cs.LG

TL;DR: 本文形式化和推广了结构化状态空间对偶性（SSD），拓展了SSD适用范围，明确等价条件，指出对偶性在标准softmax注意力中不成立，加强了循环SSM和Transformer联系，拓宽序列模型设计空间。


<details>
  <summary>Details</summary>
Motivation: 对SSD进行形式化和推广，加强循环SSM和Transformer的联系，拓宽高效序列模型的设计空间。

Method: 将SSD从标量恒等情况扩展到一般对角SSM；分析对角SSM的训练复杂度和动力学特性；建立SSM与1 - 半可分掩码注意力等价的充要条件；研究对偶性在标准softmax注意力中的情况。

Result: 拓展了SSD适用范围，对角SSM匹配标量情况的训练复杂度下限且支持更丰富动态；明确了SSM与1 - 半可分掩码注意力等价条件；对偶性在标准softmax注意力中因秩爆炸不成立。

Conclusion: 加强了循环SSM和Transformer之间的联系，拓宽了表达性强且高效的序列模型的设计空间。

Abstract: Structured State-Space Duality (SSD) [Dao & Gu, ICML 2024] is an equivalence
between a simple Structured State-Space Model (SSM) and a masked attention
mechanism. In particular, a state-space model with a scalar-times-identity
state matrix is equivalent to a masked self-attention with a $1$-semiseparable
causal mask. Consequently, the same sequence transformation (model) has two
algorithmic realizations: as a linear-time $O(T)$ recurrence or as a
quadratic-time $O(T^2)$ attention. In this note, we formalize and generalize
this duality: (i) we extend SSD from the scalar-identity case to general
diagonal SSMs (diagonal state matrices); (ii) we show that these diagonal SSMs
match the scalar case's training complexity lower bounds while supporting
richer dynamics; (iii) we establish a necessary and sufficient condition under
which an SSM is equivalent to $1$-semiseparable masked attention; and (iv) we
show that such duality fails to extend to standard softmax attention due to
rank explosion. Together, these results tighten bridge between recurrent SSMs
and Transformers, and widen the design space for expressive yet efficient
sequence models.

</details>


### [253] [How to Set $β_1, β_2$ in Adam: An Online Learning Perspective](https://arxiv.org/abs/2510.03478)
*Quan Nguyen*

Main category: cs.LG

TL;DR: 本文针对Adam优化器动量因子设置问题进行研究，推导更通用分析，结果推广了现有边界且最坏情况边界是紧的，还证明了特定设置对不同对手的优劣。


<details>
  <summary>Details</summary>
Motivation: 目前对Adam优化器动量因子β₁和β₂的最优设置缺乏完整理论理解，之前分析仅适用于β₁ = √β₂的情况，未覆盖更实际的β₁ ≠ √β₂情况。

Method: 推导适用于β₁ ≥ √β₂和β₁ ≤ √β₂两种情况的更通用分析。

Result: 结果严格推广了现有边界，最坏情况边界是紧的，证明了β₁ = √β₂对非自适应对手是最优的，对自适应对手是次优的。

Conclusion: 得到了Adam优化器动量因子设置更通用的理论分析结果，明确了不同设置在不同对手情况下的优劣。

Abstract: While Adam is one of the most effective optimizer for training large-scale
machine learning models, a theoretical understanding of how to optimally set
its momentum factors, $\beta_1$ and $\beta_2$, remains largely incomplete.
  Prior works have shown that Adam can be seen as an instance of
Follow-the-Regularized-Leader (FTRL), one of the most important class of
algorithms in online learning.
  The prior analyses in these works required setting $\beta_1 =
\sqrt{\beta_2}$, which does not cover the more practical cases with $\beta_1
\neq \sqrt{\beta_2}$.
  We derive novel, more general analyses that hold for both $\beta_1 \geq
\sqrt{\beta_2}$ and $\beta_1 \leq \sqrt{\beta_2}$.
  In both cases, our results strictly generalize the existing bounds.
  Furthermore, we show that our bounds are tight in the worst case.
  We also prove that setting $\beta_1 = \sqrt{\beta_2}$ is optimal for an
oblivious adversary, but sub-optimal for an non-oblivious adversary.

</details>


### [254] [Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM Training](https://arxiv.org/abs/2510.04996)
*Wei Xiong,Chenlu Ye,Baohao Liao,Hanze Dong,Xinxing Xu,Christof Monz,Jiang Bian,Nan Jiang,Tong Zhang*

Main category: cs.LG

TL;DR: 提出Reinforce - Ada自适应采样框架用于大语言模型在线强化学习后训练，实验显示其比GRPO加速收敛并提升性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习应用于大语言模型推理任务时，因固定统一采样导致梯度估计不稳定，现有工作虽有改进但仍有提升空间。

Method: 提出Reinforce - Ada框架，在在线连续消除过程中交错进行估计和采样；形成固定大小组并利用全局统计计算优势基线。

Result: 在多个模型架构和推理基准测试中，Reinforce - Ada加速收敛且提升最终性能，平衡采样变体效果更好。

Conclusion: 强调方差感知、自适应数据管理对大语言模型高效可靠强化学习的核心作用。

Abstract: Reinforcement learning applied to large language models (LLMs) for reasoning
tasks is often bottlenecked by unstable gradient estimates due to fixed and
uniform sampling of responses across prompts. Prior work such as GVM-RAFT
addresses this by dynamically allocating inference budget per prompt to
minimize stochastic gradient variance under a budget constraint. Inspired by
this insight, we propose Reinforce-Ada, an adaptive sampling framework for
online RL post-training of LLMs that continuously reallocates sampling effort
to the prompts with the greatest uncertainty or learning potential. Unlike
conventional two-stage allocation methods, Reinforce-Ada interleaves estimation
and sampling in an online successive elimination process, and automatically
stops sampling for a prompt once sufficient signal is collected. To stabilize
updates, we form fixed-size groups with enforced reward diversity and compute
advantage baselines using global statistics aggregated over the adaptive
sampling phase. Empirical results across multiple model architectures and
reasoning benchmarks show that Reinforce-Ada accelerates convergence and
improves final performance compared to GRPO, especially when using the balanced
sampling variant. Our work highlights the central role of variance-aware,
adaptive data curation in enabling efficient and reliable reinforcement
learning for reasoning-capable LLMs. Code is available at
https://github.com/RLHFlow/Reinforce-Ada.

</details>


### [255] [Reasoning-based Anomaly Detection Framework: A Real-time, Scalable, and Automated Approach to Anomaly Detection Across Domains](https://arxiv.org/abs/2510.03486)
*Anupam Panwar,Himadri Pal,Jiali Chen,Kyle Cho,Riddick Jiang,Miao Zhao,Rajiv Krishnamurthy*

Main category: cs.LG

TL;DR: 本文介绍Reasoning based Anomaly Detection Framework (RADF) 统一框架解决大规模分布式系统异常检测挑战，实验显示其性能优。


<details>
  <summary>Details</summary>
Motivation: 大规模分布式系统异常检测面临数据量大、时间序列数据集异质性和确定异常根源难的挑战，需有效解决方案。

Method: 提出RADF框架，采用mSelect技术自动选择算法和调整超参数，具备检测后快速分诊和确定根源能力。

Result: 在9个公共基准数据集中，5个数据集上AUC性能超现有模型，7个数据集AUC超0.85。

Conclusion: RADF框架在异常检测中表现出色，优于现有模型。

Abstract: Detecting anomalies in large, distributed systems presents several
challenges. The first challenge arises from the sheer volume of data that needs
to be processed. Flagging anomalies in a high-throughput environment calls for
a careful consideration of both algorithm and system design. The second
challenge comes from the heterogeneity of time-series datasets that leverage
such a system in production. In practice, anomaly detection systems are rarely
deployed for a single use case. Typically, there are several metrics to
monitor, often across several domains (e.g. engineering, business and
operations). A one-size-fits-all approach rarely works, so these systems need
to be fine-tuned for every application - this is often done manually. The third
challenge comes from the fact that determining the root-cause of anomalies in
such settings is akin to finding a needle in a haystack. Identifying (in real
time) a time-series dataset that is associated causally with the anomalous
time-series data is a very difficult problem. In this paper, we describe a
unified framework that addresses these challenges. Reasoning based Anomaly
Detection Framework (RADF) is designed to perform real time anomaly detection
on very large datasets. This framework employs a novel technique (mSelect) that
automates the process of algorithm selection and hyper-parameter tuning for
each use case. Finally, it incorporates a post-detection capability that allows
for faster triaging and root-cause determination. Our extensive experiments
demonstrate that RADF, powered by mSelect, surpasses state-of-the-art anomaly
detection models in AUC performance for 5 out of 9 public benchmarking
datasets. RADF achieved an AUC of over 0.85 for 7 out of 9 datasets, a
distinction unmatched by any other state-of-the-art model.

</details>


### [256] [Rethinking Langevin Thompson Sampling from A Stochastic Approximation Perspective](https://arxiv.org/abs/2510.05023)
*Weixin Wang,Haoyang Zheng,Guang Lin,Wei Deng,Pan Xu*

Main category: cs.LG

TL;DR: 提出TS - SA算法解决现有近似汤普森采样算法非平稳性问题，建立近最优遗憾界，实证表现优。


<details>
  <summary>Details</summary>
Motivation: 现有多臂老虎机近似汤普森采样算法需在每轮近似不同后验分布，超参数难调，存在非平稳性问题。

Method: 在汤普森采样框架中引入随机逼近，每轮仅用最近奖励构建后验近似，进行朗之万蒙特卡罗更新和随机逼近步骤。

Result: 建立了TS - SA的近最优遗憾界，实证显示单步朗之万更新在一定预热下大幅优于现有方法。

Conclusion: TS - SA算法有效解决了非平稳性问题，有固定步长、统一收敛分析框架和更好的后验估计。

Abstract: Most existing approximate Thompson Sampling (TS) algorithms for multi-armed
bandits use Stochastic Gradient Langevin Dynamics (SGLD) or its variants in
each round to sample from the posterior, relaxing the need for conjugacy
assumptions between priors and reward distributions in vanilla TS. However,
they often require approximating a different posterior distribution in
different round of the bandit problem. This requires tricky, round-specific
tuning of hyperparameters such as dynamic learning rates, causing challenges in
both theoretical analysis and practical implementation. To alleviate this
non-stationarity, we introduce TS-SA, which incorporates stochastic
approximation (SA) within the TS framework. In each round, TS-SA constructs a
posterior approximation only using the most recent reward(s), performs a
Langevin Monte Carlo (LMC) update, and applies an SA step to average noisy
proposals over time. This can be interpreted as approximating a stationary
posterior target throughout the entire algorithm, which further yields a fixed
step-size, a unified convergence analysis framework, and improved posterior
estimates through temporal averaging. We establish near-optimal regret bounds
for TS-SA, with a simplified and more intuitive theoretical analysis enabled by
interpreting the entire algorithm as a simulation of a stationary SGLD process.
Our empirical results demonstrate that even a single-step Langevin update with
certain warm-up outperforms existing methods substantially on bandit tasks.

</details>


### [257] [ResCP: Reservoir Conformal Prediction for Time Series Forecasting](https://arxiv.org/abs/2510.05060)
*Roberto Neglia,Andrea Cini,Michael M. Bronstein,Filippo Maria Bianchi*

Main category: cs.LG

TL;DR: 提出用于时间序列的无训练共形预测方法ResCP，能考虑局部时间动态且不影响计算可扩展性，理论证明和实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有将共形预测扩展到序列数据的方法在样本量小易失效，数据分布变化时需昂贵再训练，为克服这些局限。

Method: 提出Reservoir Conformal Prediction (ResCP)方法，利用储层计算的效率和表征学习能力动态重新加权一致性分数，计算储层状态间相似度分数并自适应重新加权观测残差。

Result: 理论证明在合理假设下ResCP实现渐近条件覆盖，通过不同预测任务实验证明其有效性。

Conclusion: ResCP是一种有效的时间序列共形预测方法，能考虑局部时间动态且具备计算可扩展性。

Abstract: Conformal prediction offers a powerful framework for building
distribution-free prediction intervals for exchangeable data. Existing methods
that extend conformal prediction to sequential data rely on fitting a
relatively complex model to capture temporal dependencies. However, these
methods can fail if the sample size is small and often require expensive
retraining when the underlying data distribution changes. To overcome these
limitations, we propose Reservoir Conformal Prediction (ResCP), a novel
training-free conformal prediction method for time series. Our approach
leverages the efficiency and representation learning capabilities of reservoir
computing to dynamically reweight conformity scores. In particular, we compute
similarity scores among reservoir states and use them to adaptively reweight
the observed residuals at each step. With this approach, ResCP enables us to
account for local temporal dynamics when modeling the error distribution
without compromising computational scalability. We prove that, under reasonable
assumptions, ResCP achieves asymptotic conditional coverage, and we empirically
demonstrate its effectiveness across diverse forecasting tasks.

</details>


### [258] [D2 Actor Critic: Diffusion Actor Meets Distributional Critic](https://arxiv.org/abs/2510.03508)
*Lunjun Zhang,Shuo Han,Hanrui Lyu,Bradly C Stadie*

Main category: cs.LG

TL;DR: 提出D2AC算法，能有效在线训练扩散策略，在多个强化学习任务中达SOTA。


<details>
  <summary>Details</summary>
Motivation: 设计一种能有效在线训练有表现力的扩散策略的无模型强化学习算法。

Method: 提出避免典型策略梯度高方差和时间反向传播复杂性的策略改进目标，设计融合分布强化学习和裁剪双Q学习的鲁棒分布评论家。

Result: 在18个困难强化学习任务基准测试中达到SOTA，还评估了捕食者 - 猎物任务。

Conclusion: D2AC算法有效，具有良好的行为鲁棒性和泛化能力。

Abstract: We introduce D2AC, a new model-free reinforcement learning (RL) algorithm
designed to train expressive diffusion policies online effectively. At its core
is a policy improvement objective that avoids the high variance of typical
policy gradients and the complexity of backpropagation through time. This
stable learning process is critically enabled by our second contribution: a
robust distributional critic, which we design through a fusion of
distributional RL and clipped double Q-learning. The resulting algorithm is
highly effective, achieving state-of-the-art performance on a benchmark of
eighteen hard RL tasks, including Humanoid, Dog, and Shadow Hand domains,
spanning both dense-reward and goal-conditioned RL scenarios. Beyond standard
benchmarks, we also evaluate a biologically motivated predator-prey task to
examine the behavioral robustness and generalization capacity of our approach.

</details>


### [259] [TopInG: Topologically Interpretable Graph Learning via Persistent Rationale Filtration](https://arxiv.org/abs/2510.05102)
*Cheng Xin,Fan Xu,Xin Ding,Jie Gao,Jiaxin Ding*

Main category: cs.LG

TL;DR: 文章提出TopInG框架解决现有可解释GNN方法在复杂多样子图上的挑战，实验证明其在预测和解释方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有可解释GNN方法在处理复杂多样的基本原理子图时面临挑战，阻碍了GNN在关键决策中的应用。

Method: 提出TopInG框架，利用持久同调识别持久的基本原理子图，采用原理过滤学习方法和自调整拓扑约束。

Result: TopInG在处理各种基本原理子图、平衡预测性能和可解释性以及减轻虚假相关性方面有效，在预测准确性和解释质量上优于现有方法。

Conclusion: TopInG框架能有效解决现有可解释GNN方法的关键挑战，具有良好的应用前景。

Abstract: Graph Neural Networks (GNNs) have shown remarkable success across various
scientific fields, yet their adoption in critical decision-making is often
hindered by a lack of interpretability. Recently, intrinsically interpretable
GNNs have been studied to provide insights into model predictions by
identifying rationale substructures in graphs. However, existing methods face
challenges when the underlying rationale subgraphs are complex and varied. In
this work, we propose TopInG: Topologically Interpretable Graph Learning, a
novel topological framework that leverages persistent homology to identify
persistent rationale subgraphs. TopInG employs a rationale filtration learning
approach to model an autoregressive generation process of rationale subgraphs,
and introduces a self-adjusted topological constraint, termed topological
discrepancy, to enforce a persistent topological distinction between rationale
subgraphs and irrelevant counterparts. We provide theoretical guarantees that
our loss function is uniquely optimized by the ground truth under specific
conditions. Extensive experiments demonstrate TopInG's effectiveness in
tackling key challenges, such as handling variform rationale subgraphs,
balancing predictive performance with interpretability, and mitigating spurious
correlations. Results show that our approach improves upon state-of-the-art
methods on both predictive accuracy and interpretation quality.

</details>


### [260] [Task-Level Contrastiveness for Cross-Domain Few-Shot Learning](https://arxiv.org/abs/2510.03509)
*Kristi Topollai,Anna Choromanska*

Main category: cs.LG

TL;DR: 提出任务级对比性概念解决少样本分类和元学习泛化问题，方法轻量且有效。


<details>
  <summary>Details</summary>
Motivation: 现有少样本分类和元学习方法难以跨领域泛化，存在准确率低、计算成本高和依赖假设等问题。

Method: 引入任务增强定义方式，定义任务级对比损失以鼓励任务表示的无监督聚类，方法可集成到现有算法中。

Result: 在MetaDataset基准上实验表明，无需额外复杂度就有卓越表现。

Conclusion: 该方法能提升泛化能力和计算效率，无需任务领域先验知识。

Abstract: Few-shot classification and meta-learning methods typically struggle to
generalize across diverse domains, as most approaches focus on a single
dataset, failing to transfer knowledge across various seen and unseen domains.
Existing solutions often suffer from low accuracy, high computational costs,
and rely on restrictive assumptions. In this paper, we introduce the notion of
task-level contrastiveness, a novel approach designed to address issues of
existing methods. We start by introducing simple ways to define task
augmentations, and thereafter define a task-level contrastive loss that
encourages unsupervised clustering of task representations. Our method is
lightweight and can be easily integrated within existing few-shot/meta-learning
algorithms while providing significant benefits. Crucially, it leads to
improved generalization and computational efficiency without requiring prior
knowledge of task domains. We demonstrate the effectiveness of our approach
through different experiments on the MetaDataset benchmark, where it achieves
superior performance without additional complexity.

</details>


### [261] [RAPID: An Efficient Reinforcement Learning Algorithm for Small Language Models](https://arxiv.org/abs/2510.03515)
*Lianghuan Huang,Sagnik Anupam,Insup Lee,Shuo Li,Osbert Bastani*

Main category: cs.LG

TL;DR: 提出新的强化学习算法RAPID，可减少运行时间，实验表明比现有算法减少11%-34%运行时间且精度相当或更好。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习算法资源密集、训练耗时，需减少运行时间。

Method: 算法以大批次进行推理，小批次进行离策略策略梯度更新，在离策略更新中结合组优势估计并推导重要性加权估计器纠正偏差。

Result: 在三个基准测试中，与现有算法相比，算法运行时间减少11%-34%，精度相当或更好。

Conclusion: 提出的RAPID算法能有效减少强化学习运行时间，且保证精度。

Abstract: Reinforcement learning (RL) has emerged as a promising strategy for
finetuning small language models (SLMs) to solve targeted tasks such as math
and coding. However, RL algorithms tend to be resource-intensive, taking a
significant amount of time to train. We propose RAPID, a novel RL algorithm
that can substantially reduce the running time of RL. Our key insight is that
RL tends to be costly due to the need to perform both inference and
backpropagation during training. To maximize use of computational resources,
our algorithm performs inference in large batches, and then performs off-policy
policy gradient updates in mini-batches. For off-policy updates, we incorporate
group advantage estimation into the policy gradient algorithm, and derive an
importance weighted estimator to correct for the bias arising from off-policy
learning. Our experiments demonstrate that our algorithm can reduce running
time by 11%-34% on three benchmarks compared to state-of-the-art RL algorithms
while maintaining similar or better accuracy.

</details>


### [262] [Certifiable Safe RLHF: Fixed-Penalty Constraint Optimization for Safer Language Models](https://arxiv.org/abs/2510.03520)
*Kartik Pandit,Sourav Ganguly,Arnesh Banerjee,Shaahin Angizi,Arnob Ghosh*

Main category: cs.LG

TL;DR: 为解决大语言模型安全问题现有CMDP方法的局限，提出Certifiable Safe - RLHF (CS - RLHF)，经实验验证其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于CMDP解决大语言模型安全问题的方法有依赖奖励和成本函数、调参计算成本高且无安全保证等局限，需要新方法解决。

Method: 引入在大规模语料库上训练的成本模型来分配基于语义的安全分数，采用基于修正惩罚的公式，借助约束优化中精确惩罚函数理论，通过适当惩罚项直接执行约束满足。

Result: CS - RLHF在经验评估中表现优于现有大语言模型，对正常和越狱提示的效率至少高5倍。

Conclusion: CS - RLHF能有效解决现有方法局限，在大语言模型安全保障上有更好表现。

Abstract: Ensuring safety is a foundational requirement for large language models
(LLMs). Achieving an appropriate balance between enhancing the utility of model
outputs and mitigating their potential for harm is a complex and persistent
challenge. Contemporary approaches frequently formalize this problem within the
framework of Constrained Markov Decision Processes (CMDPs) and employ
established CMDP optimization techniques. However, these methods exhibit two
notable limitations. First, their reliance on reward and cost functions renders
performance highly sensitive to the underlying scoring mechanism, which must
capture semantic meaning rather than being triggered by superficial keywords.
Second, CMDP-based training entails tuning dual-variable, a process that is
both computationally expensive and does not provide any provable safety
guarantee for a fixed dual variable that can be exploitable through adversarial
jailbreaks. To overcome these limitations, we introduce Certifiable Safe-RLHF
(CS-RLHF) that introduces a cost model trained on a large-scale corpus to
assign semantically grounded safety scores. In contrast to the lagrangian-based
approach, CS-RLHF adopts a rectified penalty-based formulation. This design
draws on the theory of exact penalty functions in constrained optimization,
wherein constraint satisfaction is enforced directly through a suitably chosen
penalty term. With an appropriately scaled penalty, feasibility of the safety
constraints can be guaranteed at the optimizer, eliminating the need for
dual-variable updates. Empirical evaluation demonstrates that CS-RLHF
outperforms state-of-the-art LLM model responses rendering at-least 5 times
efficient against nominal and jail-breaking prompts

</details>


### [263] [CrossLag: Predicting Major Dengue Outbreaks with a Domain Knowledge Informed Transformer](https://arxiv.org/abs/2510.03566)
*Ashwin Prabu,Nhat Thanh Tran,Guofa Zhou,Jack Xin*

Main category: cs.LG

TL;DR: 本文提出CrossLag模型预测登革热疫情，以TimeXer为基线，在新加坡数据上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以预测重大登革热疫情，而重大疫情急需及时公共预警。

Method: 引入CrossLag模型，将外生数据中重大事件背后的滞后内生信号纳入变压器架构，以TimeXer为基线。

Result: 在24周预测窗口内，CrossLag模型在检测和预测新加坡登革热重大疫情方面显著优于TimeXer。

Conclusion: CrossLag模型在登革热重大疫情预测方面表现出色。

Abstract: A variety of models have been developed to forecast dengue cases to date.
However, it remains a challenge to predict major dengue outbreaks that need
timely public warnings the most. In this paper, we introduce CrossLag, an
environmentally informed attention that allows for the incorporation of lagging
endogenous signals behind the significant events in the exogenous data into the
architecture of the transformer at low parameter counts. Outbreaks typically
lag behind major changes in climate and oceanic anomalies. We use TimeXer, a
recent general-purpose transformer distinguishing exogenous-endogenous inputs,
as the baseline for this study. Our proposed model outperforms TimeXer by a
considerable margin in detecting and predicting major outbreaks in Singapore
dengue data over a 24-week prediction window.

</details>


### [264] [Machine Unlearning Meets Adversarial Robustness via Constrained Interventions on LLMs](https://arxiv.org/abs/2510.03567)
*Fatmazohra Rezkellah,Ramzi Dakhmouche*

Main category: cs.LG

TL;DR: 本文提出统一方法处理大语言模型敏感信息遗忘和抗越狱攻击问题，简单干预方法性能优且计算成本低。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型广泛应用，需定制化处理以保证隐私保护和安全生成。

Method: 采用统一的约束优化公式，对大语言模型权重进行最小干预，使特定词汇不可达或增强模型抗攻击能力。

Result: 提出的简单逐点约束干预方法比最大 - 最小干预性能更好，计算成本更低，且优于现有防御方法。

Conclusion: 所提出的统一方法有效解决大语言模型敏感信息遗忘和抗攻击问题，具有良好性能和较低计算成本。

Abstract: With the increasing adoption of Large Language Models (LLMs), more
customization is needed to ensure privacy-preserving and safe generation. We
address this objective from two critical aspects: unlearning of sensitive
information and robustness to jail-breaking attacks. We investigate various
constrained optimization formulations that address both aspects in a
\emph{unified manner}, by finding the smallest possible interventions on LLM
weights that either make a given vocabulary set unreachable or embed the LLM
with robustness to tailored attacks by shifting part of the weights to a
\emph{safer} region. Beyond unifying two key properties, this approach
contrasts with previous work in that it doesn't require an oracle classifier
that is typically not available or represents a computational overhead.
Surprisingly, we find that the simplest point-wise constraint-based
intervention we propose leads to better performance than max-min interventions,
while having a lower computational cost. Comparison against state-of-the-art
defense methods demonstrates superior performance of the proposed approach.

</details>


### [265] [Generalization of Graph Neural Network Models for Distribution Grid Fault Detection](https://arxiv.org/abs/2510.03571)
*Burak Karabulut,Carlo Manna,Chris Develder*

Main category: cs.LG

TL;DR: 本文系统基准测试RNN+GNN管道模型中不同GNN架构用于配电网络故障检测，发现RGATv2泛化能力优。


<details>
  <summary>Details</summary>
Motivation: 当前数据驱动故障检测方法采用RNN+GNN管道模型，但有更先进GNN架构未在电力系统充分应用，需对不同GNN架构做系统基准测试。

Method: 在RNN+GNN管道模型中系统基准测试不同GNN架构，提出用GraphSAGE和GAT、GATv2进行故障诊断，与RGCN和纯RNN模型对比并探索泛化潜力。

Result: 在IEEE 123节点配电网络实验中，RGATv2泛化能力优，F1分数降幅约12%，纯RNN模型F1分数降幅达约60%，其他RGNN变体F1分数降幅达约25%。

Conclusion: RGATv2在配电网络故障检测中有更好泛化能力，在不同拓扑设置下能保持高性能。

Abstract: Fault detection in power distribution grids is critical for ensuring system
reliability and preventing costly outages. Moreover, fault detection
methodologies should remain robust to evolving grid topologies caused by
factors such as reconfigurations, equipment failures, and Distributed Energy
Resource (DER) integration. Current data-driven state-of-the-art methods use
Recurrent Neural Networks (RNNs) for temporal modeling and Graph Neural
Networks (GNNs) for spatial learning, in an RNN+GNN pipeline setting (RGNN in
short). Specifically, for power system fault diagnosis, Graph Convolutional
Networks (GCNs) have been adopted. Yet, various more advanced GNN architectures
have been proposed and adopted in domains outside of power systems. In this
paper, we set out to systematically and consistently benchmark various GNN
architectures in an RNN+GNN pipeline model. Specifically, to the best of our
knowledge, we are the first to (i) propose to use GraphSAGE and Graph Attention
(GAT, GATv2) in an RGNN for fault diagnosis, and (ii) provide a comprehensive
benchmark against earlier proposed RGNN solutions (RGCN) as well as pure RNN
models (especially Gated Recurrent Unit (GRU)), particularly (iii) exploring
their generalization potential for deployment in different settings than those
used for training them. Our experimental results on the IEEE 123-node
distribution network show that RGATv2 has superior generalization capabilities,
maintaining high performance with an F1-score reduction of $\sim$12% across
different topology settings. In contrast, pure RNN models largely fail,
experiencing an F1-score reduction of up to $\sim$60%, while other RGNN
variants also exhibit significant performance degradation, i.e., up to
$\sim$25% lower F1-scores.

</details>


### [266] [Efficient Test-Time Scaling for Small Vision-Language Models](https://arxiv.org/abs/2510.03574)
*Mehmet Onurcan Kaya,Desmond Elliott,Dim P. Papadopoulos*

Main category: cs.LG

TL;DR: 提出两种高效测试时缩放策略提升小视觉语言模型性能，实验证明有效且通用。


<details>
  <summary>Details</summary>
Motivation: 小视觉语言模型泛化能力和下游任务性能弱，现有测试时缩放技术计算量大，与小模型设计目标相悖。

Method: 提出Test - Time Augmentation (TTAug) 和Test - Time Adaptation (TTAdapt) 两种策略。TTAug生成多个增强输入并在标记级别聚合输出；TTAdapt在推理时使用TTAug的伪标签调整模型参数。

Result: 在九个基准测试中，性能持续提升，保持适合资源受限环境的计算效率。

Conclusion: 方法具有通用性，无需额外调整，适用于不同规模和类型的视觉语言模型。

Abstract: Small Vision-Language Models (VLMs) provide a computationally efficient
alternative to larger models, at the cost of weaker generalization abilities
and downstream task performance. These shortcomings could be addressed by
test-time scaling techniques, but existing methods are typically
computationally demanding, contradicting the resource-efficient design goals of
small models. To address these limitations, we propose two novel and efficient
test-time scaling strategies that leverage the model-internal features rather
than external supervision: (i) Test-Time Augmentation (TTAug), which generates
multiple augmented inputs and aggregates outputs at the token level without
parameter updates, and (ii) Test-Time Adaptation (TTAdapt), which adapts model
parameters during inference using consensus-based pseudolabels from TTAug.
Through extensive experiments across nine benchmarks, we demonstrate consistent
performance improvements while maintaining computational efficiency suitable
for resource-constrained environments. The generality of our approach is
demonstrated both within models at different scales and across different VLMs
without additional tuning.

</details>


### [267] [FieldFormer: Physics-Informed Transformers for Spatio-Temporal Field Reconstruction from Sparse Sensors](https://arxiv.org/abs/2510.03589)
*Ankit Bhardwaj,Ananth Balashankar,Lakshminarayanan Subramanian*

Main category: cs.LG

TL;DR: 提出FieldFormer框架用于无网格时空场重建，结合数据驱动与物理结构，在三个基准测试中表现出色，能从稀疏和噪声数据中实现准确高效的场重建。


<details>
  <summary>Details</summary>
Motivation: 时空传感器数据稀疏、嘈杂且不规则，现有插值或学习方法因忽略控制偏微分方程或无法扩展而效果不佳。

Method: 引入基于Transformer的FieldFormer框架，使用可学习的速度缩放距离度量收集局部邻域，通过每批偏移重新计算高效构建邻域，用局部Transformer编码器进行预测，通过基于自动求导的偏微分方程残差和特定边界惩罚来保证物理一致性。

Result: 在三个基准测试中，FieldFormer比强基线表现好40%以上，能从稀疏（0.4%-2%）和嘈杂（10%）的数据中实现准确（RMSE<10⁻²）、高效且物理一致的场重建。

Conclusion: FieldFormer能从稀疏和噪声数据中实现准确、高效且物理一致的场重建。

Abstract: Spatio-temporal sensor data is often sparse, noisy, and irregular, and
existing interpolation or learning methods struggle here because they either
ignore governing PDEs or do not scale. We introduce FieldFormer, a
transformer-based framework for mesh-free spatio-temporal field reconstruction
that combines data-driven flexibility with physics-based structure. For each
query, FieldFormer gathers a local neighborhood using a learnable
velocity-scaled distance metric, enabling anisotropic adaptation to different
propagation regimes. Neighborhoods are built efficiently via per-batch offset
recomputation, and refined in an expectation-maximization style as the velocity
scales evolve. Predictions are made by a local transformer encoder, and physics
consistency is enforced through autograd-based PDE residuals and
boundary-specific penalties. Across three benchmarks--a scalar anisotropic heat
equation, a vector-valued shallow-water system, and a realistic
advection-diffusion pollution simulation--FieldFormer consistently outperforms
strong baselines by more than 40%. Our results demonstrate that FieldFormer
enables accurate (RMSE$<10^{-2}$), efficient, and physically consistent field
reconstruction from sparse (0.4%-2%) and noisy(10%) data.

</details>


### [268] [Deep Reinforcement Learning for Multi-Agent Coordination](https://arxiv.org/abs/2510.03592)
*Kehinde O. Aina,Sehoon Ha*

Main category: cs.LG

TL;DR: 提出S - MADRL框架解决狭窄环境多机器人协调问题，用课程学习克服现有算法局限，模拟显示该框架能有效协调多机器人。


<details>
  <summary>Details</summary>
Motivation: 解决狭窄受限环境中多机器人协调问题，现有算法存在收敛和可扩展性局限。

Method: 提出Stigmergic Multi - Agent Deep Reinforcement Learning (S - MADRL)框架，利用虚拟信息素建模交互，采用课程学习分解复杂任务。

Result: 框架能有效协调多达八个智能体，机器人自组织形成非对称工作负载分布，减少拥堵并调节群体性能。

Conclusion: 该框架为有通信限制的拥挤环境中的分散式多智能体协调提供了可扩展解决方案。

Abstract: We address the challenge of coordinating multiple robots in narrow and
confined environments, where congestion and interference often hinder
collective task performance. Drawing inspiration from insect colonies, which
achieve robust coordination through stigmergy -- modifying and interpreting
environmental traces -- we propose a Stigmergic Multi-Agent Deep Reinforcement
Learning (S-MADRL) framework that leverages virtual pheromones to model local
and social interactions, enabling decentralized emergent coordination without
explicit communication. To overcome the convergence and scalability limitations
of existing algorithms such as MADQN, MADDPG, and MAPPO, we leverage curriculum
learning, which decomposes complex tasks into progressively harder
sub-problems. Simulation results show that our framework achieves the most
effective coordination of up to eight agents, where robots self-organize into
asymmetric workload distributions that reduce congestion and modulate group
performance. This emergent behavior, analogous to strategies observed in
nature, demonstrates a scalable solution for decentralized multi-agent
coordination in crowded environments with communication constraints.

</details>


### [269] [Deep Domain Adaptation for Turbofan Engine Remaining Useful Life Prediction: Methodologies, Evaluation and Future Trends](https://arxiv.org/abs/2510.03604)
*Yucheng Wang,Mohamed Ragab,Yubo Hou,Zhenghua Chen,Min Wu,Xiaoli Li*

Main category: cs.LG

TL;DR: 本文全面回顾用于涡轮风扇发动机剩余使用寿命（RUL）预测的领域适应（DA）解决方案，介绍新分类法，评估技术并指明未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 数据驱动方法在涡轮风扇发动机RUL预测中面临数据有限和分布偏移问题，DA是有潜力的解决方案，需针对涡轮风扇发动机特点对DA技术进行针对性综述。

Method: 引入针对涡轮风扇发动机的新分类法，将方法分为基于方法论、基于对齐和基于问题三类；在涡轮风扇发动机数据集上评估选定的DA技术。

Result: 新分类法提供多维度视角，评估DA技术为从业者提供实际见解并识别关键挑战。

Conclusion: 指明未来研究方向，以推动更有效的DA技术发展，提升涡轮风扇发动机RUL预测水平。

Abstract: Remaining Useful Life (RUL) prediction for turbofan engines plays a vital
role in predictive maintenance, ensuring operational safety and efficiency in
aviation. Although data-driven approaches using machine learning and deep
learning have shown potential, they face challenges such as limited data and
distribution shifts caused by varying operating conditions. Domain Adaptation
(DA) has emerged as a promising solution, enabling knowledge transfer from
source domains with abundant data to target domains with scarce data while
mitigating distributional shifts. Given the unique properties of turbofan
engines, such as complex operating conditions, high-dimensional sensor data,
and slower-changing signals, it is essential to conduct a focused review of DA
techniques specifically tailored to turbofan engines. To address this need,
this paper provides a comprehensive review of DA solutions for turbofan engine
RUL prediction, analyzing key methodologies, challenges, and recent
advancements. A novel taxonomy tailored to turbofan engines is introduced,
organizing approaches into methodology-based (how DA is applied),
alignment-based (where distributional shifts occur due to operational
variations), and problem-based (why certain adaptations are needed to address
specific challenges). This taxonomy offers a multidimensional view that goes
beyond traditional classifications by accounting for the distinctive
characteristics of turbofan engine data and the standard process of applying DA
techniques to this area. Additionally, we evaluate selected DA techniques on
turbofan engine datasets, providing practical insights for practitioners and
identifying key challenges. Future research directions are identified to guide
the development of more effective DA techniques, advancing the state of RUL
prediction for turbofan engines.

</details>


### [270] [Predicting Stock Price Movement with LLM-Enhanced Tweet Emotion Analysis](https://arxiv.org/abs/2510.03633)
*An Vuong,Susan Gauch*

Main category: cs.LG

TL;DR: 本文提出整合推文情感特征与历史股价信息的深度学习框架预测股价，实验表明结合情感分析能提升预测准确率，大语言模型预处理推文可增强情感分析效果。


<details>
  <summary>Details</summary>
Motivation: 市场波动和投资者情绪影响下，准确预测短期股价运动具有挑战性，需要新方法。

Method: 用Meta的Llama 3.1 - 8B - Instruct模型预处理推文数据，采用三种情感分析方法提取情感特征，结合历史股价数据训练LSTM模型。

Result: 三种情感分析方法都提升了预测显著股价变动的平均准确率，基于DistilRoBERTa的模型效果最佳，使用LLaMA增强情感分析后准确率从23.6%提升到38.5%。

Conclusion: 使用大语言模型预处理推文内容能增强情感分析效果，进而提高显著股价变动预测的准确性。

Abstract: Accurately predicting short-term stock price movement remains a challenging
task due to the market's inherent volatility and sensitivity to investor
sentiment. This paper discusses a deep learning framework that integrates
emotion features extracted from tweet data with historical stock price
information to forecast significant price changes on the following day. We
utilize Meta's Llama 3.1-8B-Instruct model to preprocess tweet data, thereby
enhancing the quality of emotion features derived from three emotion analysis
approaches: a transformer-based DistilRoBERTa classifier from the Hugging Face
library and two lexicon-based methods using National Research Council Canada
(NRC) resources. These features are combined with previous-day stock price data
to train a Long Short-Term Memory (LSTM) model. Experimental results on TSLA,
AAPL, and AMZN stocks show that all three emotion analysis methods improve the
average accuracy for predicting significant price movements, compared to the
baseline model using only historical stock prices, which yields an accuracy of
13.5%. The DistilRoBERTa-based stock prediction model achieves the best
performance, with accuracy rising from 23.6% to 38.5% when using LLaMA-enhanced
emotion analysis. These results demonstrate that using large language models to
preprocess tweet content enhances the effectiveness of emotion analysis which
in turn improves the accuracy of predicting significant stock price movements.

</details>


### [271] [From Theory to Practice: Evaluating Data Poisoning Attacks and Defenses in In-Context Learning on Social Media Health Discourse](https://arxiv.org/abs/2510.03636)
*Rabeya Amin Jhuma,Mostafa Mohaimen Akand Faisal*

Main category: cs.LG

TL;DR: 研究在公共卫生情感分析中，数据投毒攻击对大语言模型上下文学习的影响，并提出光谱特征防御方法，展示了攻击风险和防御有效性。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型上下文学习在公共卫生情感分析场景下，数据投毒攻击的影响。

Method: 以人类偏肺病毒推文为数据，对支持示例进行同义词替换、否定插入和随机扰动等小的对抗性操作，使用光谱特征防御过滤投毒示例。

Result: 小操作导致高达67%的情感标签翻转，防御后上下文学习准确率稳定在46.7%，逻辑回归验证达100%准确率。

Conclusion: 研究将上下文学习投毒的理论研究拓展到公共卫生领域，强调了攻击风险和光谱防御对健康相关社交媒体监测的价值。

Abstract: This study explored how in-context learning (ICL) in large language models
can be disrupted by data poisoning attacks in the setting of public health
sentiment analysis. Using tweets of Human Metapneumovirus (HMPV), small
adversarial perturbations such as synonym replacement, negation insertion, and
randomized perturbation were introduced into the support examples. Even these
minor manipulations caused major disruptions, with sentiment labels flipping in
up to 67% of cases. To address this, a Spectral Signature Defense was applied,
which filtered out poisoned examples while keeping the data's meaning and
sentiment intact. After defense, ICL accuracy remained steady at around 46.7%,
and logistic regression validation reached 100% accuracy, showing that the
defense successfully preserved the dataset's integrity. Overall, the findings
extend prior theoretical studies of ICL poisoning to a practical, high-stakes
setting in public health discourse analysis, highlighting both the risks and
potential defenses for robust LLM deployment. This study also highlights the
fragility of ICL under attack and the value of spectral defenses in making AI
systems more reliable for health-related social media monitoring.

</details>


### [272] [In-Vivo Training for Deep Brain Stimulation](https://arxiv.org/abs/2510.03643)
*Nicholas Carter,Arkaprava Gupta,Prateek Ganguli,Benedikt Dietrich,Vibhor Krishna,Samarjit Chakraborty*

Main category: cs.LG

TL;DR: 提出基于强化学习（RL）的深部脑刺激（DBS）方法，根据体内可测量的大脑活动调整刺激参数，在抑制帕金森病（PD）生物标志物方面优于临床方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于RL的DBS模型依赖不可测量的生物标志物，仅适用于芯片上大脑（BoC）模拟，需根据体内可测量的大脑活动调整刺激参数。

Method: 使用基于TD3的RL智能体，在大脑基底神经节区域模型上训练。

Result: 与现代临床DBS相比，能更好地抑制与PD严重程度相关的生物标志物。

Conclusion: 该智能体在抑制PD生物标志物上优于标准临床方法，可根据患者需求训练个性化RL智能体。

Abstract: Deep Brain Stimulation (DBS) is a highly effective treatment for Parkinson's
Disease (PD). Recent research uses reinforcement learning (RL) for DBS, with RL
agents modulating the stimulation frequency and amplitude. But, these models
rely on biomarkers that are not measurable in patients and are only present in
brain-on-chip (BoC) simulations. In this work, we present an RL-based DBS
approach that adapts these stimulation parameters according to brain activity
measurable in vivo. Using a TD3 based RL agent trained on a model of the basal
ganglia region of the brain, we see a greater suppression of biomarkers
correlated with PD severity compared to modern clinical DBS implementations.
Our agent outperforms the standard clinical approaches in suppressing PD
biomarkers while relying on information that can be measured in a real world
environment, thereby opening up the possibility of training personalized RL
agents specific to individual patient needs.

</details>


### [273] [SAFA-SNN: Sparsity-Aware On-Device Few-Shot Class-Incremental Learning with Fast-Adaptive Structure of Spiking Neural Network](https://arxiv.org/abs/2510.03648)
*Huijing Zhang,Muyang Cao,Linshan Jiang,Xin Du,Di Yu,Changze Lv,Shuiguang Deng*

Main category: cs.LG

TL;DR: 本文提出基于SNN的SAFA - SNN方法用于设备端少样本类增量学习，实验表明其性能优于基线方法且能耗更低。


<details>
  <summary>Details</summary>
Motivation: 边缘设备在数据样本不足时进行连续学习具有挑战性，现有基于ANN的方法受设备资源限制，而SNN具有能耗低等优势，因此研究基于SNN的设备端少样本类增量学习方法。

Method: 提出稀疏条件神经元动力学缓解灾难性遗忘；采用零阶优化处理尖峰不可微性；在增量学习阶段通过子空间投影增强新类别的可区分性。

Result: 在多个标准基准数据集和神经形态数据集上的实验表明，SAFA - SNN在Mini - ImageNet的最后增量阶段至少提高4.01%的性能，实际实现中能耗比基线方法低20%。

Conclusion: SAFA - SNN在设备端少样本类增量学习中表现优异，优于基线方法，具有更低的能耗。

Abstract: Continuous learning of novel classes is crucial for edge devices to preserve
data privacy and maintain reliable performance in dynamic environments.
However, the scenario becomes particularly challenging when data samples are
insufficient, requiring on-device few-shot class-incremental learning (FSCIL)
to maintain consistent model performance. Although existing work has explored
parameter-efficient FSCIL frameworks based on artificial neural networks
(ANNs), their deployment is still fundamentally constrained by limited device
resources. Inspired by neural mechanisms, Spiking neural networks (SNNs)
process spatiotemporal information efficiently, offering lower energy
consumption, greater biological plausibility, and compatibility with
neuromorphic hardware than ANNs. In this work, we present an SNN-based method
for On-Device FSCIL, i.e., Sparsity-Aware and Fast Adaptive SNN (SAFA-SNN). We
first propose sparsity-conditioned neuronal dynamics, in which most neurons
remain stable while a subset stays active, thereby mitigating catastrophic
forgetting. To further cope with spike non-differentiability in gradient
estimation, we employ zeroth-order optimization. Moreover, during incremental
learning sessions, we enhance the discriminability of new classes through
subspace projection, which alleviates overfitting to novel classes. Extensive
experiments conducted on two standard benchmark datasets (CIFAR100 and
Mini-ImageNet) and three neuromorphic datasets (CIFAR-10-DVS, DVS128gesture,
and N-Caltech101) demonstrate that SAFA-SNN outperforms baseline methods,
specifically achieving at least 4.01% improvement at the last incremental
session on Mini-ImageNet and 20% lower energy cost over baseline methods with
practical implementation.

</details>


### [274] [Optimising Battery Energy Storage System Trading via Energy Market Operator Price Forecast](https://arxiv.org/abs/2510.03657)
*Aymeric Fabre*

Main category: cs.LG

TL;DR: 论文探讨利用AEMO能源价格预测准确性开发BESS交易算法，创建新模型并与基础算法对比，还探索机器学习改进预测以指导交易。


<details>
  <summary>Details</summary>
Motivation: 电力市场中精准预测价格对BESS盈利至关重要，虽有丰富预测数据，但其实用价值未充分挖掘。

Method: 分析预测准确性的时间、预测范围和区域变化模式，创建预测驱动的BESS交易模型，与无预测数据的基础算法对比，探索机器学习技术改进预测。

Result: 未提及具体结果。

Conclusion: 研究成果将改进能源市场交易模型，促进BESS更高效融入市场运作。

Abstract: In electricity markets around the world, the ability to anticipate price
movements with precision can be the difference between profit and loss,
especially for fast-acting assets like battery energy storage systems (BESS).
As grid volatility increases due to renewables and market decentralisation,
operators and forecasters alike face growing pressure to transform prediction
into strategy. Yet while forecast data is abundant, especially in advanced
markets like Australia's National Electricity Market (NEM), its practical value
in driving real-world BESS trading decisions remains largely unexplored. This
thesis dives into that gap. This work addresses a key research question: Can
the accuracy of the Australian Energy Market Operator (AEMO) energy price
forecasts be systematically leveraged to develop a reliable and profitable
battery energy storage system trading algorithm? Despite the availability of
AEMO price forecasts, no existing framework evaluates their reliability or
incorporates them into practical BESS trading strategies. By analysing patterns
in forecast accuracy based on time of day, forecast horizon, and regional
variations, this project creates a novel, forecast-informed BESS trading model
to optimise arbitrage financial returns. The performance of this
forecast-driven algorithm is benchmarked against a basic trading algorithm with
no knowledge of forecast data. The study further explores the potential of
machine learning techniques to predict future energy prices by enhancing AEMO
forecasts to govern a more advanced trading strategy. The research outcomes
will inform future improvements in energy market trading models and promote
more efficient BESS integration into market operations.

</details>


### [275] [Operationalizing Data Minimization for Privacy-Preserving LLM Prompting](https://arxiv.org/abs/2510.03662)
*Jijie Zhou,Niloofar Mireshghallah,Tianshi Li*

Main category: cs.LG

TL;DR: 提出数据最小化框架，在四个数据集上用九个大语言模型评估，发现大模型更能容忍数据最小化，模型预测最优数据最小化有偏差。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在消费应用中快速部署，用户分享过多个人信息增加隐私风险。

Method: 提出框架定义和实施数据最小化，用优先队列树搜索定位最优解，在四个数据集上用九个大语言模型评估。

Result: 大的前沿大语言模型比小的开源模型更能在保证任务质量时容忍更强的数据最小化，模型直接预测最优数据最小化有偏差。

Conclusion: 模型存在隐私和能力差距，可能缺乏对解决任务所需信息的认知。

Abstract: The rapid deployment of large language models (LLMs) in consumer applications
has led to frequent exchanges of personal information. To obtain useful
responses, users often share more than necessary, increasing privacy risks via
memorization, context-based personalization, or security breaches. We present a
framework to formally define and operationalize data minimization: for a given
user prompt and response model, quantifying the least privacy-revealing
disclosure that maintains utility, and we propose a priority-queue tree search
to locate this optimal point within a privacy-ordered transformation space. We
evaluated the framework on four datasets spanning open-ended conversations
(ShareGPT, WildChat) and knowledge-intensive tasks with single-ground-truth
answers (CaseHold, MedQA), quantifying achievable data minimization with nine
LLMs as the response model. Our results demonstrate that larger frontier LLMs
can tolerate stronger data minimization while maintaining task quality than
smaller open-source models (85.7% redaction for GPT-5 vs. 19.3% for
Qwen2.5-0.5B). By comparing with our search-derived benchmarks, we find that
LLMs struggle to predict optimal data minimization directly, showing a bias
toward abstraction that leads to oversharing. This suggests not just a privacy
gap, but a capability gap: models may lack awareness of what information they
actually need to solve a task.

</details>


### [276] [Token Hidden Reward: Steering Exploration-Exploitation in Group Relative Deep Reinforcement Learning](https://arxiv.org/abs/2510.03669)
*Wenlong Deng,Yi Ren,Yushu Li,Boying Gong,Danica J. Sutherland,Xiaoxiao Li,Christos Thrampoulidis*

Main category: cs.LG

TL;DR: 本文提出Token Hidden Reward (THR)指标，介绍THR引导的重加权算法控制强化学习调优大模型的探索与利用，并在数学推理基准上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中如何明确引导训练进行探索或利用的问题。

Method: 引入Token Hidden Reward (THR)指标量化每个token影响，提出THR引导的重加权算法调制学习信号。

Result: 正向THR算法提高贪心解码准确率，反向策略提高Pass@K准确率，算法能与其他RL目标集成，跨架构泛化。

Conclusion: THR是动态控制强化学习调优大模型探索与利用的有效机制，为推理密集型应用微调提供新工具。

Abstract: Reinforcement learning with verifiable rewards has significantly advanced the
reasoning capabilities of large language models, yet how to explicitly steer
training toward exploration or exploitation remains an open problem. We
introduce Token Hidden Reward (THR), a token-level metric that quantifies each
token's influence on the likelihood of correct responses under Group Relative
Policy Optimization (GRPO). We find that training dynamics are dominated by a
small subset of tokens with high absolute THR values. Most interestingly,
tokens with positive THR strengthen confidence in correct outputs, thus
favoring exploitation, while tokens with negative THR preserve probability mass
for alternative outputs, enabling exploration. This insight suggests a natural
intervention: a THR-guided reweighting algorithm that modulates GRPO's learning
signals to explicitly bias training toward exploitation or exploration. We
validate the efficacy of this algorithm on diverse math reasoning benchmarks.
By amplifying tokens with positive THR value and weakening negative ones, our
algorithm improves greedy-decoding accuracy, favoring exploitation. The reverse
strategy yields consistent gains in Pass@K accuracy, favoring exploration. We
further demonstrate that our algorithm integrates seamlessly with other RL
objectives such as GSPO and generalizes across architectures including Llama.
These findings establish THR as a principled and fine-grained mechanism for
dynamically controlling exploration and exploitation in RL-tuned LLMs,
providing new tools for targeted fine-tuning in reasoning-intensive
applications.

</details>


### [277] [REG: A Regularization Optimizer for Robust Training Dynamics](https://arxiv.org/abs/2510.03691)
*Zehua Liu,Han Wu,Xiaojin Fu,Shuqi Liu,Xiongwei Han,Tao Zhong,Mingxuan Yuan*

Main category: cs.LG

TL;DR: 本文提出新型优化器REG，解决Muon的局限性，在大语言模型训练中表现优于AdamW且与AdamW训练范式兼容。


<details>
  <summary>Details</summary>
Motivation: 现有Muon优化器存在依赖矩阵符号函数导致训练不稳定、与AdamW预训练模型微调不兼容的问题，需要改进。

Method: 提出REG优化器，用行和列缩放（RACS）算子替代Muon的矩阵符号算子。

Result: 通过大量实验证明REG优化器在性能和稳定性上优于AdamW，且在微调阶段避免了Muon的性能下降问题。

Conclusion: REG优化器能解决Muon的局限性，在大语言模型训练中表现更好且与AdamW训练范式一致。

Abstract: Optimizers are crucial for the efficient training of Large Language Models
(LLMs). While AdamW is the de facto standard, recent structure-aware optimizers
like Muon have emerged, which regularize gradient updates by operating on
entire weight matrices. The Muon optimizer balances the gradient updates along
all the directions. However, Muon's reliance on the matrix sign function can
lead to training instability, exhibits incompatibility when fine-tuning models
pre-trained with AdamW. To address these limitations, we propose \textbf{REG},
a novel optimizer that replaces Muon's aggressive matrix sign operator with the
Row-and-Column-Scaling (RACS) operator. Theoretically grounded in balancing a
matrix, the RACS operator regularizes the update steps in a less drastic
manner, making it simpler to implement and more compatible with established
training dynamics. Through extensive empirical experiments on LLM training, we
demonstrate that our REG optimizer not only achieves superior performance and
stability over AdamW, but also maintains consistency with the AdamW training
paradigm. This consistency is particularly evident during the fine-tuning
stage, where REG optimizer avoids the performance degradation observed with
Muon.

</details>


### [278] [Personalized federated prototype learning in mixed heterogeneous data scenarios](https://arxiv.org/abs/2510.03726)
*Jiahao Zeng,Wolong Xing,Liangtao Shi,Xin Huang,Jialin Wang,Zhile Cao,Zhenkui Shi*

Main category: cs.LG

TL;DR: 提出新方法PFPL解决联邦学习中混合异构场景问题，实验验证有效且降低通信成本。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习方法聚焦孤立异构场景，导致特征或标签分布偏斜，而数据异构是提升模型性能的关键因素。

Method: 提出PFPL方法，为每个客户端构建个性化、无偏原型以提供丰富领域知识和无偏收敛目标，在局部更新阶段引入一致正则化使局部实例与个性化原型对齐。

Result: 在Digits和Office Caltech数据集上的实验验证了方法的有效性，成功降低通信成本。

Conclusion: PFPL方法能有效解决混合异构场景下联邦学习的问题。

Abstract: Federated learning has received significant attention for its ability to
simultaneously protect customer privacy and leverage distributed data from
multiple devices for model training. However, conventional approaches often
focus on isolated heterogeneous scenarios, resulting in skewed feature
distributions or label distributions. Meanwhile, data heterogeneity is actually
a key factor in improving model performance. To address this issue, we propose
a new approach called PFPL in mixed heterogeneous scenarios. The method
provides richer domain knowledge and unbiased convergence targets by
constructing personalized, unbiased prototypes for each client. Moreover, in
the local update phase, we introduce consistent regularization to align local
instances with their personalized prototypes, which significantly improves the
convergence of the loss function. Experimental results on Digits and Office
Caltech datasets validate the effectiveness of our approach and successfully
reduce the communication cost.

</details>


### [279] [Optimizing Fine-Tuning through Advanced Initialization Strategies for Low-Rank Adaptation](https://arxiv.org/abs/2510.03731)
*Yongfu Xue*

Main category: cs.LG

TL;DR: 提出IniLoRA初始化策略解决LoRA局限，表现优于LoRA，还引入两个变体提升性能


<details>
  <summary>Details</summary>
Motivation: LoRA依赖初始化乘积为零的低秩矩阵，限制有效激活和利用原模型权重，影响最优性能

Method: 提出IniLoRA初始化策略，使低秩矩阵逼近原模型权重，还引入IniLoRA-α和IniLoRA-β两个变体

Result: IniLoRA在一系列模型和任务上比LoRA性能更好

Conclusion: IniLoRA策略有效，其变体可进一步提升性能

Abstract: The rapid development of parameter-efficient fine-tuning methods has
noticeably improved the efficiency of adapting large language models. Among
these, LoRA has gained widespread popularity due to its strong balance of
effectiveness and parameter efficiency. However, LoRA relies on initializing
two low-rank matrices whose product is zero, which limits its ability to
effectively activate and leverage the original model weights-creating a
potential bottleneck for optimal performance. To address this limitation, we
propose \textbf{IniLoRA}, a novel initialization strategy that initializes the
low-rank matrices to closely approximate the original model weights.
Experimental results indicate that IniLoRA achieves better performance than
LoRA across a range of models and tasks. Additionally, we introduce two
variants, IniLoRA-$\alpha$ and IniLoRA-$\beta$, both leveraging distinct
initialization methods to enhance performance further.

</details>


### [280] [Neural Low-Discrepancy Sequences](https://arxiv.org/abs/2510.03745)
*Michael Etienne Van Huffel,Nathan Kirk,Makram Chahine,Daniela Rus,T. Konstantin Rusch*

Main category: cs.LG

TL;DR: 本文提出基于机器学习的NeuroLDS框架生成低差异序列，优于先前方法，在多应用中有效。


<details>
  <summary>Details</summary>
Motivation: 现有MPMC方法只能生成点集，无法扩展到低差异序列，而低差异序列在很多应用中很重要，因此需要新方法。

Method: 借鉴经典低差异序列，训练神经网络将索引映射到点，采用两阶段学习过程：监督近似经典构造，再无监督微调以最小化前缀差异。

Result: NeuroLDS在差异度量上显著优于之前所有低差异序列构造方法，且在数值积分、机器人运动规划和科学机器学习等多种应用中有效。

Conclusion: NeuroLDS有前景且具有广泛意义。

Abstract: Low-discrepancy points are designed to efficiently fill the space in a
uniform manner. This uniformity is highly advantageous in many problems in
science and engineering, including in numerical integration, computer vision,
machine perception, computer graphics, machine learning, and simulation.
Whereas most previous low-discrepancy constructions rely on abstract algebra
and number theory, Message-Passing Monte Carlo (MPMC) was recently introduced
to exploit machine learning methods for generating point sets with lower
discrepancy than previously possible. However, MPMC is limited to generating
point sets and cannot be extended to low-discrepancy sequences (LDS), i.e.,
sequences of points in which every prefix has low discrepancy, a property
essential for many applications. To address this limitation, we introduce
Neural Low-Discrepancy Sequences ($NeuroLDS$), the first machine learning-based
framework for generating LDS. Drawing inspiration from classical LDS, we train
a neural network to map indices to points such that the resulting sequences
exhibit minimal discrepancy across all prefixes. To this end, we deploy a
two-stage learning process: supervised approximation of classical constructions
followed by unsupervised fine-tuning to minimize prefix discrepancies. We
demonstrate that $NeuroLDS$ outperforms all previous LDS constructions by a
significant margin with respect to discrepancy measures. Moreover, we
demonstrate the effectiveness of $NeuroLDS$ across diverse applications,
including numerical integration, robot motion planning, and scientific machine
learning. These results highlight the promise and broad significance of Neural
Low-Discrepancy Sequences. Our code can be found at
https://github.com/camail-official/neuro-lds.

</details>


### [281] [EvoEngineer: Mastering Automated CUDA Kernel Code Evolution with Large Language Models](https://arxiv.org/abs/2510.03760)
*Ping Guo,Chenyu Zhu,Siyuan Chen,Fei Liu,Xi Lin,Zhichao Lu,Qingfu Zhang*

Main category: cs.LG

TL;DR: 文章指出CUDA内核优化是AI性能瓶颈，提出基于LLM的代码进化框架EvoEngineer，实验表明其在性能和正确性上取得平衡，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: CUDA内核优化是AI性能瓶颈，现有LLM自动化内核优化方法零散、问题定义不清晰，通用LLM代码进化方法无法满足正确性要求。

Method: 将CUDA内核优化形式化为代码优化任务，建立基于LLM的代码进化框架EvoEngineer，实现内核优化系统并进行实验。

Result: EvoEngineer在性能和正确性上取得平衡，平均中位数加速比达2.72倍，代码有效率69.8%，优于现有方法。

Conclusion: EvoEngineer能有效解决CUDA内核优化问题，在性能和正确性上表现出色。

Abstract: CUDA kernel optimization has become a critical bottleneck for AI performance,
as deep learning training and inference efficiency directly depends on highly
optimized GPU kernels.
  Despite the promise of Large Language Models (LLMs) for automating kernel
optimization, this field suffers from a fragmented ecosystem of isolated and
incomparable approaches with unclear problem formulations.
  Furthermore, general-purpose LLM code evolution methods cannot meet strict
correctness requirements of CUDA kernel optimization.
  We address these fundamental challenges by first formalizing CUDA kernel
optimization as a code optimization task with a clear objective, constraints,
and evaluation metrics.
  We then establish the first systematic LLM-based code evolution framework,
EvoEngineer, that provides guidance for designing and adapting optimization
strategies to achieve a balance between performance and correctness.
  Finally, we implement a kernel optimization system based on this framework
and conduct extensive experiments on 91 real-world CUDA kernels.
  Our results demonstrate that EvoEngineer achieves a principled balance
between performance and correctness, with the highest averaged median speedup
of \textbf{2.72}$\times$ over baseline CUDA kernels and a code validity rate of
\textbf{69.8}\%, outperforming existing methods on both dimensions.
  Our method achieves a maximum speedup of \textbf{36.75}$\times$ among all
operations over PyTorch kernels and delivers the highest speedup on \textbf{28}
(\textbf{56.0\%}) of 50 operations that achieve over \textbf{2$\times$}
acceleration.

</details>


### [282] [Merge and Guide: Unifying Model Merging and Guided Decoding for Controllable Multi-Objective Generation](https://arxiv.org/abs/2510.03782)
*Guofu Xie,Chen Zhang,Xiao Zhang,Yunsheng Shi,Ting Yao,Jun Xu*

Main category: cs.LG

TL;DR: 提出MAGE框架解决可控多目标生成中适应不同用户需求的挑战，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有可控多目标生成方法存在不足，合并方法控制间接且次优，解码引导有空间开销且依赖单模型能力。

Method: 提出MAGE两阶段框架，第一阶段动态构建更稳健的基础模型，第二阶段将显式和隐式价值模型合并为统一引导代理来指导解码。

Result: 验证了价值模型中的线性模式连通性，探索模型合并与预测集成关系，实验显示方法优于现有方法。

Conclusion: MAGE方法实现了更好的可控性、帕累托最优性能和适应性。

Abstract: Adapting to diverse user needs at test time is a key challenge in
controllable multi-objective generation. Existing methods are insufficient:
merging-based approaches provide indirect, suboptimal control at the parameter
level, often disregarding the impacts of multiple objectives. While
decoding-based guidance is more direct, it typically requires aggregating
logits from multiple expert models, incurring significant space overhead and
relying heavily on individual model capacity. To address these issues, we
introduce Merge-And-GuidE (MAGE), a two-stage framework that leverages model
merging for guided decoding. We first identify a critical compatibility problem
between the guidance and base models. In Stage 1, MAGE resolves this by
dynamically constructing a more robust base model, merging a series of backbone
models that account for multiple objectives. In Stage 2, we merge explicit and
implicit value models into a unified guidance proxy, which then steers the
decoding of the base model from Stage 1. Our analysis empirically validates
Linear Mode Connectivity (LMC) in value models, explores the relationship
between model merging and prediction ensembling, and demonstrates the enhanced
controllability afforded by our approach. Extensive experiments show that our
method outperforms existing approaches, achieving superior controllability,
Pareto-optimal performance, and enhanced adaptability.

</details>


### [283] [Curriculum-Augmented GFlowNets For mRNA Sequence Generation](https://arxiv.org/abs/2510.03811)
*Aya Laajil,Abduragim Shtanchaev,Sajan Muhammad,Eric Moulines,Salem Lahlou*

Main category: cs.LG

TL;DR: 提出课程增强生成流网络（CAGFN）用于设计mRNA序列，在不同任务中提升帕累托性能和生物学合理性，且比无课程的GFlowNet更快找到高质量解。


<details>
  <summary>Details</summary>
Motivation: 设计mRNA序列需探索大量核苷酸组合并优化多种属性，现有生成流网络训练受稀疏长程奖励和多目标权衡阻碍。

Method: 将课程学习与多目标生成流网络集成，采用基于长度的课程，提供新的mRNA设计环境。

Result: 在不同mRNA设计任务中，CAGFN提升帕累托性能和生物学合理性，保持多样性，更快找到高质量解，能泛化到分布外序列。

Conclusion: CAGFN为治疗性序列设计中应用和推进生成流网络提供了生物学动机的设置。

Abstract: Designing mRNA sequences is a major challenge in developing next-generation
therapeutics, since it involves exploring a vast space of possible nucleotide
combinations while optimizing sequence properties like stability, translation
efficiency, and protein expression. While Generative Flow Networks are
promising for this task, their training is hindered by sparse, long-horizon
rewards and multi-objective trade-offs. We propose Curriculum-Augmented
GFlowNets (CAGFN), which integrate curriculum learning with multi-objective
GFlowNets to generate de novo mRNA sequences. CAGFN integrates a length-based
curriculum that progressively adapts the maximum sequence length guiding
exploration from easier to harder subproblems. We also provide a new mRNA
design environment for GFlowNets which, given a target protein sequence and a
combination of biological objectives, allows for the training of models that
generate plausible mRNA candidates. This provides a biologically motivated
setting for applying and advancing GFlowNets in therapeutic sequence design. On
different mRNA design tasks, CAGFN improves Pareto performance and biological
plausibility, while maintaining diversity. Moreover, CAGFN reaches
higher-quality solutions faster than a GFlowNet trained with random sequence
sampling (no curriculum), and enables generalization to out-of-distribution
sequences.

</details>


### [284] [Detecting Invariant Manifolds in ReLU-Based RNNs](https://arxiv.org/abs/2510.03814)
*Lukas Eisenmann,Alena Brändle,Zahra Monfared,Daniel Durstewitz*

Main category: cs.LG

TL;DR: 介绍检测分段线性循环神经网络（PLRNNs）中稳定和不稳定流形的新算法，展示其在刻画多稳定性、发现同宿点及分析神经元电生理记录方面的应用。


<details>
  <summary>Details</summary>
Motivation: 理解训练后的循环神经网络（RNNs）行为产生的原因和方式对科学、医学及可解释人工智能很重要，RNN的动态特性取决于其状态空间的拓扑和几何性质，稳定和不稳定流形很关键，因此需要检测这些流形的算法。

Method: 引入一种新算法，聚焦采用修正线性单元（ReLUs）作为激活函数的PLRNNs来检测稳定和不稳定流形。

Result: 该算法可追踪不同吸引域之间的边界，刻画多稳定性；能找到同宿点，证明PLRNNs中混沌的存在；还可用于分析神经元电生理记录以洞察潜在动态。

Conclusion: 新算法对理解PLRNNs的动态特性有重要作用，可应用于实际数据如神经元电生理记录的分析。

Abstract: Recurrent Neural Networks (RNNs) have found widespread applications in
machine learning for time series prediction and dynamical systems
reconstruction, and experienced a recent renaissance with improved training
algorithms and architectural designs. Understanding why and how trained RNNs
produce their behavior is important for scientific and medical applications,
and explainable AI more generally. An RNN's dynamical repertoire depends on the
topological and geometrical properties of its state space. Stable and unstable
manifolds of periodic points play a particularly important role: They dissect a
dynamical system's state space into different basins of attraction, and their
intersections lead to chaotic dynamics with fractal geometry. Here we introduce
a novel algorithm for detecting these manifolds, with a focus on
piecewise-linear RNNs (PLRNNs) employing rectified linear units (ReLUs) as
their activation function. We demonstrate how the algorithm can be used to
trace the boundaries between different basins of attraction, and hence to
characterize multistability, a computationally important property. We further
show its utility in finding so-called homoclinic points, the intersections
between stable and unstable manifolds, and thus establish the existence of
chaos in PLRNNs. Finally we show for an empirical example, electrophysiological
recordings from a cortical neuron, how insights into the underlying dynamics
could be gained through our method.

</details>


### [285] [Distributed Area Coverage with High Altitude Balloons Using Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.03823)
*Adam Haroon,Tristan Schuler*

Main category: cs.LG

TL;DR: 本文首次将多智能体强化学习（MARL）应用于高空气球（HAB）协调分布式区域覆盖，扩展仿真环境，适配QMIX方法，验证了MARL方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体HAB协调方法在小团队和局部任务中表现不佳，且协调式多智能体强化学习未被研究，因此开展相关研究。

Method: 扩展先前开发的强化学习仿真环境以支持合作多智能体学习，适配QMIX用于HAB区域覆盖协调，采用专门的观测空间和分层奖励。

Result: QMIX在分布式区域覆盖方面达到了与理论最优几何确定性方法相似的性能。

Conclusion: 验证了MARL方法，为更复杂的自主多HAB任务奠定基础，确定性方法在这些任务中难以处理。

Abstract: High Altitude Balloons (HABs) can leverage stratospheric wind layers for
limited horizontal control, enabling applications in reconnaissance,
environmental monitoring, and communications networks. Existing multi-agent HAB
coordination approaches use deterministic methods like Voronoi partitioning and
extremum seeking control for large global constellations, which perform poorly
for smaller teams and localized missions. While single-agent HAB control using
reinforcement learning has been demonstrated on HABs, coordinated multi-agent
reinforcement learning (MARL) has not yet been investigated. This work presents
the first systematic application of multi-agent reinforcement learning (MARL)
to HAB coordination for distributed area coverage. We extend our previously
developed reinforcement learning simulation environment (RLHAB) to support
cooperative multi-agent learning, enabling multiple agents to operate
simultaneously in realistic atmospheric conditions. We adapt QMIX for HAB area
coverage coordination, leveraging Centralized Training with Decentralized
Execution to address atmospheric vehicle coordination challenges. Our approach
employs specialized observation spaces providing individual state,
environmental context, and teammate data, with hierarchical rewards
prioritizing coverage while encouraging spatial distribution. We demonstrate
that QMIX achieves similar performance to the theoretically optimal geometric
deterministic method for distributed area coverage, validating the MARL
approach and providing a foundation for more complex autonomous multi-HAB
missions where deterministic methods become intractable.

</details>


### [286] [On Using Large Language Models to Enhance Clinically-Driven Missing Data Recovery Algorithms in Electronic Health Records](https://arxiv.org/abs/2510.03844)
*Sarah C. Lotspeich,Abbey Collins,Brian J. Wells,Ashish K. Khanna,Joseph Rigdon,Lucy D'Agostino McGowan*

Main category: cs.LG

TL;DR: 研究基于ICD - 10代码的路线图驱动算法恢复电子健康记录（EHR）缺失值的准确性和可扩展性，算法恢复效果与专家图表审查相当。


<details>
  <summary>Details</summary>
Motivation: EHR数据易有缺失和错误，之前的图表审查方法昂贵且耗时，限制审查患者数量，需新方法恢复缺失值。

Method: 在原有临床医生路线图基础上，用大语言模型结合临床专业知识迭代优化新版本路线图，用100名患者图表审查测试不同路线图算法性能，用1000名患者应用最终算法。

Result: 根据路线图，算法恢复的缺失数据和专家图表审查相当甚至更多。

Conclusion: 临床驱动（由大语言模型增强）的算法能以和图表审查相似的准确性恢复EHR缺失数据，可应用于大样本，扩展用于监测数据质量其他维度是有前景的方向。

Abstract: Objective: Electronic health records (EHR) data are prone to missingness and
errors. Previously, we devised an "enriched" chart review protocol where a
"roadmap" of auxiliary diagnoses (anchors) was used to recover missing values
in EHR data (e.g., a diagnosis of impaired glycemic control might imply that a
missing hemoglobin A1c value would be considered unhealthy). Still, chart
reviews are expensive and time-intensive, which limits the number of patients
whose data can be reviewed. Now, we investigate the accuracy and scalability of
a roadmap-driven algorithm, based on ICD-10 codes (International Classification
of Diseases, 10th revision), to mimic expert chart reviews and recover missing
values. Materials and Methods: In addition to the clinicians' original roadmap
from our previous work, we consider new versions that were iteratively refined
using large language models (LLM) in conjunction with clinical expertise to
expand the list of auxiliary diagnoses. Using chart reviews for 100 patients
from the EHR at an extensive learning health system, we examine algorithm
performance with different roadmaps. Using the larger study of $1000$ patients,
we applied the final algorithm, which used a roadmap with clinician-approved
additions from the LLM. Results: The algorithm recovered as much, if not more,
missing data as the expert chart reviewers, depending on the roadmap.
Discussion: Clinically-driven algorithms (enhanced by LLM) can recover missing
EHR data with similar accuracy to chart reviews and can feasibly be applied to
large samples. Extending them to monitor other dimensions of data quality
(e.g., plausability) is a promising future direction.

</details>


### [287] [Unlocking Reasoning Capabilities in LLMs via Reinforcement Learning Exploration](https://arxiv.org/abs/2510.03865)
*Wenhao Deng,Long Wei,Chenglei Yu,Tailin Wu*

Main category: cs.LG

TL;DR: 指出RLVR在采样预算增加时优势减弱问题，提出RAPO算法，训练模型后结果显示其提升解题性能。


<details>
  <summary>Details</summary>
Motivation: 解决RLVR在采样预算增加时，训练模型优势减弱甚至消失，依赖基础模型受限搜索空间的问题。

Method: 提出RAPO算法，用前向KL惩罚代替反向KL惩罚进行分布外探索，重新加权参考策略进行自适应分布内探索。

Result: 在8K SimpleRL - Zero数据集上训练Qwen2.5 - 3B和7B模型，在AIME2024和AIME2025上评估，RAPO持续提升解题性能。

Conclusion: RAPO能让模型突破基础模型性能上限，解决先前难以处理的问题，推动RLVR在挑战性推理任务上的发展。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has recently enhanced
the reasoning capabilities of large language models (LLMs), particularly for
mathematical problem solving. However, a fundamental limitation remains: as the
sampling budget increases, the advantage of RLVR-trained models over their
pretrained bases often diminishes or even vanishes, revealing a strong
dependence on the base model's restricted search space. We attribute this
phenomenon to the widespread use of the reverse Kullback-Leibler (KL)
divergence regularizer, whose mode-seeking behavior keeps the policy trapped
inside the base model's support region and hampers wider exploration. To
address this issue, we propose RAPO (Rewards-Aware Policy Optimization), an
algorithm to promote broader yet focused exploration. Our method (i) utilizes
the forward KL penalty to replace the reverse KL penalty for
out-of-distribution exploration, and (ii) reweights the reference policy to
facilitate adaptive in-distribution exploration. We train Qwen2.5-3B and 7B
models with RAPO on the 8K SimpleRL-Zero dataset, without supervised
fine-tuning, and evaluate them on AIME2024 and AIME2025. Results show that RAPO
consistently improves problem-solving performance. Notably, RAPO enables models
to surpass the base model's performance ceiling and solves previously
intractable problems, advancing the frontier of RLVR for challenging reasoning
tasks.

</details>


### [288] [BONSAI: Structure-exploiting robust Bayesian optimization for networked black-box systems under uncertainty](https://arxiv.org/abs/2510.03893)
*Akshay Kudva,Joel A. Paulson*

Main category: cs.LG

TL;DR: 本文介绍新的鲁棒贝叶斯优化框架BONSAI，利用部分结构知识，在不同案例中比现有算法更高效高质量。


<details>
  <summary>Details</summary>
Motivation: 传统鲁棒优化方法有局限性，现有鲁棒贝叶斯优化方法忽略结构信息且难扩展到高维，需要新方法。

Method: 引入BONSAI框架，将目标表示为白盒和黑盒组件的有向图，提出基于汤普森采样的采集函数。

Result: 在合成和实际案例中，BONSAI比现有基于仿真的鲁棒优化算法更具样本效率，能得到更高质量的鲁棒解。

Conclusion: BONSAI在复杂工程系统的不确定性设计中具有实际优势。

Abstract: Optimal design under uncertainty remains a fundamental challenge in advancing
reliable, next-generation process systems. Robust optimization (RO) offers a
principled approach by safeguarding against worst-case scenarios across a range
of uncertain parameters. However, traditional RO methods typically require
known problem structure, which limits their applicability to high-fidelity
simulation environments. To overcome these limitations, recent work has
explored robust Bayesian optimization (RBO) as a flexible alternative that can
accommodate expensive, black-box objectives. Existing RBO methods, however,
generally ignore available structural information and struggle to scale to
high-dimensional settings. In this work, we introduce BONSAI (Bayesian
Optimization of Network Systems under uncertAInty), a new RBO framework that
leverages partial structural knowledge commonly available in simulation-based
models. Instead of treating the objective as a monolithic black box, BONSAI
represents it as a directed graph of interconnected white- and black-box
components, allowing the algorithm to utilize intermediate information within
the optimization process. We further propose a scalable Thompson sampling-based
acquisition function tailored to the structured RO setting, which can be
efficiently optimized using gradient-based methods. We evaluate BONSAI across a
diverse set of synthetic and real-world case studies, including applications in
process systems engineering. Compared to existing simulation-based RO
algorithms, BONSAI consistently delivers more sample-efficient and
higher-quality robust solutions, highlighting its practical advantages for
uncertainty-aware design in complex engineering systems.

</details>


### [289] [LLM as an Algorithmist: Enhancing Anomaly Detectors via Programmatic Synthesis](https://arxiv.org/abs/2510.03904)
*Hangting Ye,Jinmeng Li,He Zhao,Mingchen Zhuge,Dandan Guo,Yi Chang,Hongyuan Zha*

Main category: cs.LG

TL;DR: 提出LLM - DAS框架解决表格数据异常检测问题，经实验验证能提升主流检测器性能。


<details>
  <summary>Details</summary>
Motivation: 现有表格数据异常检测方法依赖假设，在现实场景表现不稳定，大语言模型直接应用于表格异常检测存在处理异构数据难和隐私风险大的问题。

Method: 提出LLM - DAS框架，将大语言模型定位为‘算法师’，分析检测器弱点，生成数据无关的Python代码合成‘难检测’异常，增强训练数据，将问题转化为二分类任务。

Result: 在36个表格异常检测基准测试中，LLM - DAS持续提升主流检测器性能。

Conclusion: LLM - DAS通过程序合成将大语言模型推理与经典异常检测算法结合，是可扩展、有效且保护隐私的方法，能弥补现有检测器逻辑盲点。

Abstract: Existing anomaly detection (AD) methods for tabular data usually rely on some
assumptions about anomaly patterns, leading to inconsistent performance in
real-world scenarios. While Large Language Models (LLMs) show remarkable
reasoning capabilities, their direct application to tabular AD is impeded by
fundamental challenges, including difficulties in processing heterogeneous data
and significant privacy risks. To address these limitations, we propose
LLM-DAS, a novel framework that repositions the LLM from a ``data processor''
to an ``algorithmist''. Instead of being exposed to raw data, our framework
leverages the LLM's ability to reason about algorithms. It analyzes a
high-level description of a given detector to understand its intrinsic
weaknesses and then generates detector-specific, data-agnostic Python code to
synthesize ``hard-to-detect'' anomalies that exploit these vulnerabilities.
This generated synthesis program, which is reusable across diverse datasets, is
then instantiated to augment training data, systematically enhancing the
detector's robustness by transforming the problem into a more discriminative
two-class classification task. Extensive experiments on 36 TAD benchmarks show
that LLM-DAS consistently boosts the performance of mainstream detectors. By
bridging LLM reasoning with classic AD algorithms via programmatic synthesis,
LLM-DAS offers a scalable, effective, and privacy-preserving approach to
patching the logical blind spots of existing detectors.

</details>


### [290] [THEMIS: Unlocking Pretrained Knowledge with Foundation Model Embeddings for Anomaly Detection in Time Series](https://arxiv.org/abs/2510.03911)
*Yadav Mahesh Lorik,Kaushik Sarveswaran,Nagaraj Sundaramahalingam,Aravindakumar Venugopalan*

Main category: cs.LG

TL;DR: 本文提出用于时间序列异常检测的THEMIS框架，利用基础模型的预训练知识，实验表明该方法在多个数据集上表现优异，提倡使用基础模型的预训练表示进行时间序列异常检测。


<details>
  <summary>Details</summary>
Motivation: 时间序列异常检测存在诸多挑战，如数据特性复杂、异常类型多样且罕见、数据不平衡、高维、实时检测等，需要强大、灵活且可解释的方法。

Method: 提出THEMIS框架，从Chronos时间序列基础模型的编码器中提取嵌入，并在自相似矩阵上应用局部异常因子和谱分解等异常检测技术。

Result: 该模块化方法在MSL数据集上达到SOTA结果，在SMAP和SWAT*数据集上有竞争力，超越专门为异常检测训练的模型，具有超参数鲁棒性和可解释性。

Conclusion: 提倡使用基础模型的预训练表示进行高效且适应性强的时间序列异常检测。

Abstract: Time series anomaly detection forms a very crucial area in several domains
but poses substantial challenges. Due to time series data possessing
seasonality, trends, noise, and evolving patterns (concept drift), it becomes
very difficult to set a general notion of what constitutes normal behavior.
Anomalies themselves could be varied, ranging from a single outlier to
contextual or collective anomalies, and are normally very rare; hence, the
dataset is largely imbalanced. Additional layers of complexities arise due to
the problems of increased dimensionality of modern time series, real-time
detection criteria, setting up appropriate detection thresholds, and arriving
at results that are interpretable. To embrace these multifaceted challenges,
very strong, flexible, and interpretable approaches are required. This paper
presents THEMIS, a new framework for time series anomaly detection that
exploits pretrained knowledge from foundation models. THEMIS extracts
embeddings from the encoder of the Chronos time series foundation model and
applies outlier detection techniques like Local Outlier Factor and Spectral
Decomposition on the self-similarity matrix, to spot anomalies in the data. Our
experiments show that this modular method achieves SOTA results on the MSL
dataset and performs quite competitively on the SMAP and SWAT$^*$ datasets.
Notably, THEMIS exceeds models trained specifically for anomaly detection,
presenting hyperparameter robustness and interpretability by default. This
paper advocates for pretrained representations from foundation models for
performing efficient and adaptable anomaly detection for time series data.

</details>


### [291] [Generalized Fitted Q-Iteration with Clustered Data](https://arxiv.org/abs/2510.03912)
*Liyuan Hu,Jitao Wang,Zhenke Wu,Chengchun Shi*

Main category: cs.LG

TL;DR: 本文针对医疗应用中常见的聚类数据强化学习问题，提出广义FQI算法，理论证明其性质，经验表明比标准FQI减少一半后悔值。


<details>
  <summary>Details</summary>
Motivation: 处理医疗应用中聚类数据强化学习时的簇内相关性问题。

Method: 提出将广义估计方程纳入策略学习的广义FQI算法。

Result: 理论上证明了Q函数和策略估计量在相关结构正确指定时的最优性和错误指定时的一致性；经验上，通过模拟和对移动健康数据集的分析，广义FQI比标准FQI平均减少一半后悔值。

Conclusion: 广义FQI算法在处理聚类数据的强化学习中表现更优。

Abstract: This paper focuses on reinforcement learning (RL) with clustered data, which
is commonly encountered in healthcare applications. We propose a generalized
fitted Q-iteration (FQI) algorithm that incorporates generalized estimating
equations into policy learning to handle the intra-cluster correlations.
Theoretically, we demonstrate (i) the optimalities of our Q-function and policy
estimators when the correlation structure is correctly specified, and (ii)
their consistencies when the structure is mis-specified. Empirically, through
simulations and analyses of a mobile health dataset, we find the proposed
generalized FQI achieves, on average, a half reduction in regret compared to
the standard FQI.

</details>


### [292] [On the Convergence and Size Transferability of Continuous-depth Graph Neural Networks](https://arxiv.org/abs/2510.03923)
*Mingsong Yan,Charles Kulick,Sui Tang*

Main category: cs.LG

TL;DR: 本文对含时变参数的连续深度图神经网络（GNDEs）在无限节点极限下进行收敛分析，引入Graphon - NDEs，证明收敛性、推导收敛速率和规模可迁移性边界，实验支持理论结果。


<details>
  <summary>Details</summary>
Motivation: 对含时变参数的GNDEs在无限节点极限下进行严格收敛分析，为其规模可迁移性提供理论见解。

Method: 引入Graphon - NDEs作为GNDEs的无限节点极限并证明其适定性，利用图论和动力系统工具证明收敛性，在两种确定性图采样机制下推导收敛速率，建立规模可迁移性边界。

Result: 证明了GNDE解到Graphon - NDE解的逐轨迹收敛性，得到明确收敛速率，建立规模可迁移性边界，数值实验支持理论结果。

Conclusion: 为将在中等规模图上训练的GNDE模型迁移到更大、结构相似的图上而无需重新训练的策略提供了理论依据。

Abstract: Continuous-depth graph neural networks, also known as Graph Neural
Differential Equations (GNDEs), combine the structural inductive bias of Graph
Neural Networks (GNNs) with the continuous-depth architecture of Neural ODEs,
offering a scalable and principled framework for modeling dynamics on graphs.
In this paper, we present a rigorous convergence analysis of GNDEs with
time-varying parameters in the infinite-node limit, providing theoretical
insights into their size transferability. To this end, we introduce Graphon
Neural Differential Equations (Graphon-NDEs) as the infinite-node limit of
GNDEs and establish their well-posedness. Leveraging tools from graphon theory
and dynamical systems, we prove the trajectory-wise convergence of GNDE
solutions to Graphon-NDE solutions. Moreover, we derive explicit convergence
rates under two deterministic graph sampling regimes: (1) weighted graphs
sampled from smooth graphons, and (2) unweighted graphs sampled from
$\{0,1\}$-valued (discontinuous) graphons. We further establish size
transferability bounds, providing theoretical justification for the practical
strategy of transferring GNDE models trained on moderate-sized graphs to
larger, structurally similar graphs without retraining. Numerical experiments
using synthetic and real data support our theoretical findings.

</details>


### [293] [LLM Chemistry Estimation for Multi-LLM Recommendation](https://arxiv.org/abs/2510.03930)
*Huascar Sanchez,Briland Hitaj*

Main category: cs.LG

TL;DR: 提出LLM Chemistry框架衡量大语言模型组合的协同或拮抗行为，分析相关理论并通过任务评估验证。


<details>
  <summary>Details</summary>
Motivation: 现有多LLM协作方法未分析协作模型是否互补或冲突，需衡量LLM组合的协同或拮抗行为。

Method: 引入LLM Chemistry框架，形式化LLM间化学性概念，提出量化算法并推荐最优模型集合。

Result: 理论分析表明协作LLM在异构模型配置下化学性最明显，评估任务验证了任务依赖效应。

Conclusion: LLM Chemistry可作为多LLM系统的诊断因素和集合推荐的基础。

Abstract: Multi-LLM collaboration promises accurate, robust, and context-aware
solutions, yet existing approaches rely on implicit selection and output
assessment without analyzing whether collaborating models truly complement or
conflict. We introduce LLM Chemistry -- a framework that measures when LLM
combinations exhibit synergistic or antagonistic behaviors that shape
collective performance beyond individual capabilities. We formalize the notion
of chemistry among LLMs, propose algorithms that quantify it by analyzing
interaction dependencies, and recommend optimal model ensembles accordingly.
Our theoretical analysis shows that chemistry among collaborating LLMs is most
evident under heterogeneous model profiles, with its outcome impact shaped by
task type, group size, and complexity. Evaluation on classification,
summarization, and program repair tasks provides initial evidence for these
task-dependent effects, thereby reinforcing our theoretical results. This
establishes LLM Chemistry as both a diagnostic factor in multi-LLM systems and
a foundation for ensemble recommendation.

</details>


### [294] [On the Empirical Power of Goodness-of-Fit Tests in Watermark Detection](https://arxiv.org/abs/2510.03944)
*Weiqing He,Xiang Li,Tianqi Shang,Li Shen,Weijie Su,Qi Long*

Main category: cs.LG

TL;DR: 本文系统评估了八种拟合优度（GoF）测试在大语言模型文本水印检测中的效果，发现GoF测试可提升检测能力和鲁棒性，是被低估的检测工具。


<details>
  <summary>Details</summary>
Motivation: 大语言模型引发内容真实性和完整性担忧，文本水印可验证内容来源，而GoF测试在水印检测中未被充分探索。

Method: 使用三个开源大语言模型、两个数据集、不同生成温度和多种后编辑方法，对三种流行水印方案系统评估八种GoF测试。

Result: 通用GoF测试可提高水印检测器的检测能力和鲁棒性，文本重复使GoF测试有独特优势。

Conclusion: 经典GoF测试是大语言模型水印检测中简单、强大但未被充分利用的工具。

Abstract: Large language models (LLMs) raise concerns about content authenticity and
integrity because they can generate human-like text at scale. Text watermarks,
which embed detectable statistical signals into generated text, offer a
provable way to verify content origin. Many detection methods rely on pivotal
statistics that are i.i.d. under human-written text, making goodness-of-fit
(GoF) tests a natural tool for watermark detection. However, GoF tests remain
largely underexplored in this setting. In this paper, we systematically
evaluate eight GoF tests across three popular watermarking schemes, using three
open-source LLMs, two datasets, various generation temperatures, and multiple
post-editing methods. We find that general GoF tests can improve both the
detection power and robustness of watermark detectors. Notably, we observe that
text repetition, common in low-temperature settings, gives GoF tests a unique
advantage not exploited by existing methods. Our results highlight that classic
GoF tests are a simple yet powerful and underused tool for watermark detection
in LLMs.

</details>


### [295] [What Is The Performance Ceiling of My Classifier? Utilizing Category-Wise Influence Functions for Pareto Frontier Analysis](https://arxiv.org/abs/2510.03950)
*Shahriar Kabir Nahin,Wenxiao Xiao,Joshua Liu,Anshuman Chhabra,Hongfu Liu*

Main category: cs.LG

TL;DR: 本文研究学习模型性能上限，提出类别影响函数和影响向量，设计样本重加权框架，实验证明其在多类别性能提升上有效。


<details>
  <summary>Details</summary>
Motivation: 现有工作多关注哪些数据对学习模型有益，本文进一步探究学习模型的性能上限，且强调类别精度的帕累托改进。

Method: 提出类别影响函数和影响向量，基于此制定判断模型是否可改进的准则，设计基于线性规划的样本重加权框架。

Result: 通过在合成数据集、视觉和文本基准上的大量实验，证明了方法在估计和实现模型多类别性能改进上的有效性。

Conclusion: 所提方法能有效估计和实现模型在多个感兴趣类别上的性能提升。

Abstract: Data-centric learning seeks to improve model performance from the perspective
of data quality, and has been drawing increasing attention in the machine
learning community. Among its key tools, influence functions provide a powerful
framework to quantify the impact of individual training samples on model
predictions, enabling practitioners to identify detrimental samples and retrain
models on a cleaner dataset for improved performance. However, most existing
work focuses on the question: "what data benefits the learning model?" In this
paper, we take a step further and investigate a more fundamental question:
"what is the performance ceiling of the learning model?" Unlike prior studies
that primarily measure improvement through overall accuracy, we emphasize
category-wise accuracy and aim for Pareto improvements, ensuring that every
class benefits, rather than allowing tradeoffs where some classes improve at
the expense of others. To address this challenge, we propose category-wise
influence functions and introduce an influence vector that quantifies the
impact of each training sample across all categories. Leveraging these
influence vectors, we develop a principled criterion to determine whether a
model can still be improved, and further design a linear programming-based
sample reweighting framework to achieve Pareto performance improvements.
Through extensive experiments on synthetic datasets, vision, and text
benchmarks, we demonstrate the effectiveness of our approach in estimating and
achieving a model's performance improvement across multiple categories of
interest.

</details>


### [296] [Optimizing Resources for On-the-Fly Label Estimation with Multiple Unknown Medical Experts](https://arxiv.org/abs/2510.03954)
*Tim Bary,Tiffanie Godelaine,Axel Abels,Benoît Macq*

Main category: cs.LG

TL;DR: 提出实时注解自适应方法，在三个数据集评估，减少专家查询数且精度相当


<details>
  <summary>Details</summary>
Motivation: 现有算法无法无缝集成到筛查流程，需有效聚合噪声注解的算法提升筛查工作流

Method: 提出支持实时标注、无需先验知识、基于实例难度动态查询专家的自适应方法，增量收集意见至达到置信阈值

Result: 自适应查询策略最多减少50%专家查询数，精度与非自适应基线相当

Conclusion: 所提自适应方法有效，能减少标注开销并保证精度，代码开源

Abstract: Accurate ground truth estimation in medical screening programs often relies
on coalitions of experts and peer second opinions. Algorithms that efficiently
aggregate noisy annotations can enhance screening workflows, particularly when
data arrive continuously and expert proficiency is initially unknown. However,
existing algorithms do not meet the requirements for seamless integration into
screening pipelines. We therefore propose an adaptive approach for real-time
annotation that (I) supports on-the-fly labeling of incoming data, (II)
operates without prior knowledge of medical experts or pre-labeled data, and
(III) dynamically queries additional experts based on the latent difficulty of
each instance. The method incrementally gathers expert opinions until a
confidence threshold is met, providing accurate labels with reduced annotation
overhead. We evaluate our approach on three multi-annotator classification
datasets across different modalities. Results show that our adaptive querying
strategy reduces the number of expert queries by up to 50% while achieving
accuracy comparable to a non-adaptive baseline. Our code is available at
https://github.com/tbary/MEDICS

</details>


### [297] [Early-Warning of Thunderstorm-Driven Power Outages with a Two-Stage Machine Learning Model](https://arxiv.org/abs/2510.03959)
*Iryna Stanishevska*

Main category: cs.LG

TL;DR: 本文利用公开数据开发了密歇根州夏季雷暴相关停电24 - 48小时预警模型，模型有较好表现，特征工程有价值。


<details>
  <summary>Details</summary>
Motivation: 雷暴导致的停电难以预测，因多数风暴不造成破坏、对流过程快速混乱且可用公共数据有噪声和不完整。

Method: 使用公开数据源，通过特定克里金法保留对流微信号，构建因果时空特征，采用结合逻辑门和LSTM回归器的两阶段模型设计，用事件中心指标和条件峰值MASE评估，通过移动块自助法量化不确定性。

Result: 两阶段模型在测试样本中检测到更多参考峰值，近峰值时有适度幅度增益，误差与单步LSTM基线相当，SHAP分析证实特征工程价值。

Conclusion: 尽管公开数据有噪声，特征驱动的流程可为雷暴停电提供可操作的、以事件为重点的早期预警。

Abstract: Thunderstorm-driven outages are difficult to predict because most storms do
not cause damage, convective processes occur rapidly and chaotically, and the
available public data are both noisy and incomplete. We develop a 24-48 h
early-warning model for summer, thunderstorm-related outages in Michigan using
only open sources (EAGLE-I for ground truth; METAR for weather). We use the
publicly released EAGLE-I outage dataset (2014-2022), maintained by Oak Ridge
National Laboratory for the U.S. Department of Energy. The pipeline preserves
convective micro-signals from a sparse station network via parameter-specific
kriging with hourly variograms and targeted overdrafting to retain extremes,
and builds causal spatio-temporal features (lags/rolling statistics; k-NN/IDW
spatial aggregates) capturing precursors of severe convection (moisture
advection, wind shifts, and pressure drops). The two-stage model design,
combining a logistic gate and an LSTM regressor, limits routine periods and
reduces noise exposure. The study uses event-centric metrics (cluster-based
hits/misses/false alarms) and peak-conditional MASE (cMASE) in +/-Delta-hour
windows around state-level peaks (>= 50,000), with uncertainty quantified by
hourly moving-block bootstrap.
  On the test sample, Two-Stage detects more reference peaks across all windows
(e.g., at +/-48 h it records 3/4 vs. 2/4; F1 66.7% vs. 57.1%) with one extra
false alarm. Near peaks, it shows modest amplitude gains (2-3% lower cMASE at
+/-0-12 h; bootstrap medians +9-13% at +/-6-12 h) but small losses at +/-36-48
h (~3-4%). Overall, errors are comparable to the one-step LSTM baseline. SHAP
analysis confirms moisture-advection and wind/gust precursors, underscoring the
value of the feature engineering. Despite open-data noise, the feature-driven
pipeline yields actionable, event-focused early warnings for thunderstorm
outages.

</details>


### [298] [SPEAR: Soft Prompt Enhanced Anomaly Recognition for Time Series Data](https://arxiv.org/abs/2510.03962)
*Hanzhe Wei,Jiajun Wu,Jialin Yang,Henry Leung,Steve Drew*

Main category: cs.LG

TL;DR: 提出SPEAR方法结合大语言模型和软提示进行时间序列异常检测，实验表明软提示提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理可变长度时间序列和基于上下文的异常检测方面存在困难，大语言模型为时间序列异常检测带来新机遇。

Method: 提出SPEAR方法，将时间序列数据量化并转换为输入嵌入，与可学习的软提示嵌入结合，输入到冻结的大语言模型中，基于交叉熵损失迭代更新软提示。

Result: 实验结果表明软提示有效提高了大语言模型在时间序列异常检测下游任务中的性能。

Conclusion: 软提示能有效提升大语言模型在时间序列异常检测任务中的性能，SPEAR方法是可行的。

Abstract: Time series anomaly detection plays a crucial role in a wide range of fields,
such as healthcare and internet traffic monitoring. The emergence of large
language models (LLMs) offers new opportunities for detecting anomalies in the
ubiquitous time series data. Traditional approaches struggle with
variable-length time series sequences and context-based anomalies. We propose
Soft Prompt Enhanced Anomaly Recognition (SPEAR), a novel approach to leverage
LLMs for anomaly detection with soft prompts and quantization. Our methodology
involves quantizing and transforming the time series data into input embeddings
and combining them with learnable soft prompt embeddings. These combined
embeddings are then fed into a frozen LLM. The soft prompts are updated
iteratively based on a cross-entropy loss, allowing the model to adapt to time
series anomaly detection. The use of soft prompts helps adapt LLMs effectively
to time series tasks, while quantization ensures optimal handling of sequences,
as LLMs are designed to handle discrete sequences. Our experimental results
demonstrate that soft prompts effectively increase LLMs' performance in
downstream tasks regarding time series anomaly detection.

</details>


### [299] [What Can You Do When You Have Zero Rewards During RL?](https://arxiv.org/abs/2510.03971)
*Jatin Prakash,Anirudh Buvanesh*

Main category: cs.LG

TL;DR: 研究强化学习在大语言模型复杂推理任务中遇零奖励障碍问题，发现添加简单样本的以数据为中心干预法可解决问题并开源实现。


<details>
  <summary>Details</summary>
Motivation: 强化学习在复杂推理任务中依赖基础模型偶尔采样正确解，无正确解时会遇零奖励障碍致学习停滞。

Method: 通过图搜索任务评估结合密集奖励、多样性激励和改进信用分配的方法，采用添加简单样本到训练集的以数据为中心干预法。

Result: 现有方法在基础模型不产生正确答案时无法克服零奖励障碍，添加简单样本可让模型最终解决原难题。

Conclusion: 以数据为中心的干预法可在不修改强化学习算法情况下克服零奖励障碍，开源实现支持后续研究。

Abstract: Reinforcement learning (RL) with outcome-based rewards has proven effective
for improving large language models (LLMs) on complex reasoning tasks. However,
its success often depends on the base model occasionally sampling correct
solutions. When no correct solutions are sampled, training encounters a
zero-reward barrier where learning stalls due to zero gradients. We study this
scenario through the graph search task introduced in Bachmann et al. (2024) and
evaluate recent methods that incorporate desirable components such as dense
rewards, diversity incentives, and improved credit assignment. Our experiments
show that none of these approaches overcome the zero-reward barrier if the base
model never produces a correct answer. In contrast, we find that a simple
data-centric intervention of adding easier samples to the training set enables
the model to eventually solve the original hard task despite starting from zero
reward. Importantly, this succeeds without modifying the RL algorithm itself.
Because official implementations of several baselines were unavailable, we
developed our own, which allowed us to conduct a detailed analysis of their
failure modes. We release these implementations to support further research at:
https://github.com/rl4reasoning/rl-baselines

</details>


### [300] [Beyond Softmax: A New Perspective on Gradient Bandits](https://arxiv.org/abs/2510.03979)
*Emerson Melo,David Müller*

Main category: cs.LG

TL;DR: 本文建立离散选择模型与在线学习和多臂老虎机理论的联系，提出多种算法并证明有次线性遗憾界，实验证明算法有效。


<details>
  <summary>Details</summary>
Motivation: 建立离散选择模型与在线学习和多臂老虎机理论的联系，拓展相关算法适用性。

Method: 提出涵盖Exp3的算法族、从广义嵌套logit模型推导新的对抗性多臂老虎机算法、引入广义梯度多臂老虎机算法。

Result: 所提算法结合灵活模型规范和计算效率，数值实验证明在随机多臂老虎机设置中有效。

Conclusion: 所提算法在随机多臂老虎机场景实用有效，拓展了梯度多臂老虎机方法的适用性。

Abstract: We establish a link between a class of discrete choice models and the theory
of online learning and multi-armed bandits. Our contributions are: (i)
sublinear regret bounds for a broad algorithmic family, encompassing Exp3 as a
special case; (ii) a new class of adversarial bandit algorithms derived from
generalized nested logit models \citep{wen:2001}; and (iii)
\textcolor{black}{we introduce a novel class of generalized gradient bandit
algorithms that extends beyond the widely used softmax formulation. By relaxing
the restrictive independence assumptions inherent in softmax, our framework
accommodates correlated learning dynamics across actions, thereby broadening
the applicability of gradient bandit methods.} Overall, the proposed algorithms
combine flexible model specification with computational efficiency via
closed-form sampling probabilities. Numerical experiments in stochastic bandit
settings demonstrate their practical effectiveness.

</details>


### [301] [ICEPool: Enhancing Graph Pooling Networks with Inter-cluster Connectivity](https://arxiv.org/abs/2510.03987)
*Michael Yang*

Main category: cs.LG

TL;DR: 提出新型分层池化框架ICEPool，可与多种基于池化的GNN模型兼容，能增强模型对簇间连接的理解，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有分层池化模型在处理图结构数据时，常忽略簇之间的关系。

Method: 引入Inter - cluster Connectivity Enhancement Pooling (ICEPool) 框架，并对其图重建能力进行理论分析。

Result: ICEPool与多种模型兼容，能提升现有图神经网络架构的性能。

Conclusion: ICEPool有效学习了传统模型忽略的簇间关系，具有广泛应用潜力。

Abstract: Hierarchical Pooling Models have demonstrated strong performance in
classifying graph-structured data. While numerous innovative methods have been
proposed to design cluster assignments and coarsening strategies, the
relationships between clusters are often overlooked. In this paper, we
introduce Inter-cluster Connectivity Enhancement Pooling (ICEPool), a novel
hierarchical pooling framework designed to enhance model's understanding of
inter-cluster connectivity and ability of preserving the structural integrity
in the original graph. ICEPool is compatible with a wide range of pooling-based
GNN models. The deployment of ICEPool as an enhancement to existing models
effectively combines the strengths of the original model with ICEPool's
capability to emphasize the integration of inter-cluster connectivity,
resulting in a more comprehensive and robust graph-level representation.
Moreover, we make theoretical analysis to ICEPool's ability of graph
reconstruction to demonstrate its effectiveness in learning inter-cluster
relationship that is overlooked by conventional models. Finally, the
experimental results show the compatibility of ICEPool with wide varieties of
models and its potential to boost the performance of existing graph neural
network architectures.

</details>


### [302] [Distilling Reasoning into Student LLMs: Local Naturalness for Selecting Teacher Data](https://arxiv.org/abs/2510.03988)
*Hoang Anh Just,Myeongseob Ko,Ruoxi Jia*

Main category: cs.LG

TL;DR: 本文系统研究推理蒸馏中的响应选择问题，提出局部自然度方法，在教师选择和多教师响应选择上表现良好，凸显局部数据质量评估和混合的作用。


<details>
  <summary>Details</summary>
Motivation: 以往工作多关注单教师响应的提示选择，多教师场景下最佳响应选择问题未充分探索，本文旨在填补该空白。

Method: 提出局部自然度，测量学生模型在短推理步骤上的对数概率。

Result: 局部自然度在教师选择中可可靠识别最有帮助的教师；在多教师响应选择中，使32B学生模型在数学基准测试上的准确率比全局选择提高9.4pp，超单最佳教师数据训练效果。

Conclusion: 局部数据质量评估和数据混合对更有效的推理蒸馏很有作用。

Abstract: Distilling long reasoning traces (10K+ tokens) from stronger teacher models
into smaller student LLMs via SFT has emerged as a standard paradigm. This
approach is practical and efficient: it leverages the ease of generating
abundant reasoning data from stronger models and provides a direct, data-driven
way to teach less capable models better reasoning. While previous work has
largely focused on prompt selection with responses from a single teacher, the
equally important problem of choosing the best response when multiple teacher
outputs are available for a single prompt remains underexplored. This challenge
becomes important in a multi-teacher setting, where different students may
benefit from the outputs of different teachers. This paper fills that gap with
a systematic study of response selection for reasoning distillation. We first
show that the current method, which picks responses the student assigns the
highest global log-probability (global naturalness), fails when responses come
from multiple teachers, i.e., global naturalness no longer correlates with
downstream performance, especially as the reasoning traces from strong teachers
become longer. To overcome this problem, we introduce Local Naturalness, which
measures the student's log-probabilities over short, sequential reasoning steps
conditioned only on a small local window. Local Naturalness enables two
applications: 1) Teacher Selection: Aggregating local scores across prompts
reliably identifies the most helpful teacher. 2) Response Selection from a
Multiple Teachers: When mixing answers from many teachers, Local Naturalness
boosts a 32B student's accuracy on math benchmarks by 9.4pp over global
selection, also surpassing the performance achieved by training on data from
the single best teacher. These results highlight the power of localized data
quality evaluation and data mixing for more effective reasoning distillation.

</details>


### [303] [A Mathematical Explanation of Transformers for Large Language Models and GPTs](https://arxiv.org/abs/2510.03989)
*Xue-Cheng Tai,Hao Liu,Lingfeng Li,Raymond H. Chan*

Main category: cs.LG

TL;DR: 本文提出连续框架将Transformer解释为结构化积分 - 微分方程的离散化，为理解其核心组件提供统一视角，拓展理论分析并为架构设计等提供新方向。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏全面解释Transformer结构和操作的数学理论，需要建立相关理论基础。

Method: 提出新颖的连续框架，将Transformer解释为结构化积分 - 微分方程的离散化。

Result: 在该框架下，自注意力机制自然地表现为非局部积分算子，层归一化被表征为投影到时间相关约束，提供统一且可解释的基础。

Conclusion: 该解释有助于缩小深度学习架构与连续数学建模的差距，为可解释和理论健全的神经网络模型发展提供基础视角。

Abstract: The Transformer architecture has revolutionized the field of sequence
modeling and underpins the recent breakthroughs in large language models
(LLMs). However, a comprehensive mathematical theory that explains its
structure and operations remains elusive. In this work, we propose a novel
continuous framework that rigorously interprets the Transformer as a
discretization of a structured integro-differential equation. Within this
formulation, the self-attention mechanism emerges naturally as a non-local
integral operator, and layer normalization is characterized as a projection to
a time-dependent constraint. This operator-theoretic and variational
perspective offers a unified and interpretable foundation for understanding the
architecture's core components, including attention, feedforward layers, and
normalization. Our approach extends beyond previous theoretical analyses by
embedding the entire Transformer operation in continuous domains for both token
indices and feature dimensions. This leads to a principled and flexible
framework that not only deepens theoretical insight but also offers new
directions for architecture design, analysis, and control-based
interpretations. This new interpretation provides a step toward bridging the
gap between deep learning architectures and continuous mathematical modeling,
and contributes a foundational perspective to the ongoing development of
interpretable and theoretically grounded neural network models.

</details>


### [304] [Incorporating Multivariate Consistency in ML-Based Weather Forecasting with Latent-space Constraints](https://arxiv.org/abs/2510.04006)
*Hang Fan,Yi Xiao,Yongquan Qu,Fenghua Ling,Ben Fei,Lei Bai,Pierre Gentine*

Main category: cs.LG

TL;DR: 多数基于机器学习的天气预报模型存在问题，本文将模型训练重新解释为WC - 4DVar问题，在潜在空间计算损失，提升长期预报能力并扩展框架以适应多源数据。


<details>
  <summary>Details</summary>
Motivation: 多数基于机器学习的天气预报模型将再分析数据视为真实值，采用变量特定的损失加权，忽略物理耦合和空间结构，导致长期预报模糊且不符合物理现实。

Method: 将模型训练重新解释为WC - 4DVar问题，把再分析数据视为不完美观测；在自编码器学习的潜在空间计算损失；扩展框架以适应异构数据源。

Result: 基于潜在空间约束的滚动训练比基于模型空间损失的训练提高了长期预报技能，更好地保留了精细结构和物理现实。

Conclusion: 该框架可在统一理论公式下，结合再分析数据和多源观测数据对预报模型进行联合训练。

Abstract: Data-driven machine learning (ML) models have recently shown promise in
surpassing traditional physics-based approaches for weather forecasting,
leading to a so-called second revolution in weather forecasting. However, most
ML-based forecast models treat reanalysis as the truth and are trained under
variable-specific loss weighting, ignoring their physical coupling and spatial
structure. Over long time horizons, the forecasts become blurry and physically
unrealistic under rollout training. To address this, we reinterpret model
training as a weak-constraint four-dimensional variational data assimilation
(WC-4DVar) problem, treating reanalysis data as imperfect observations. This
allows the loss function to incorporate reanalysis error covariance and capture
multivariate dependencies. In practice, we compute the loss in a latent space
learned by an autoencoder (AE), where the reanalysis error covariance becomes
approximately diagonal, thus avoiding the need to explicitly model it in the
high-dimensional model space. We show that rollout training with latent-space
constraints improves long-term forecast skill and better preserves fine-scale
structures and physical realism compared to training with model-space loss.
Finally, we extend this framework to accommodate heterogeneous data sources,
enabling the forecast model to be trained jointly on reanalysis and
multi-source observations within a unified theoretical formulation.

</details>


### [305] [Replacing Softmax Similarity with a Sharpened Angular Similarity: Theory and Practice of Scaling To Billion-Context Attention](https://arxiv.org/abs/2510.04008)
*Sahil Joshi,Agniva Chowdhury,Amar Kanakamedala,Ekam Singh,Evan Tu,Anshumali Shrivastava*

Main category: cs.LG

TL;DR: 提出RACE Attention替代Softmax Attention，线性复杂度，在多任务中匹配基线精度，减少运行时间和内存，能处理超长上下文。


<details>
  <summary>Details</summary>
Motivation: Softmax Attention时间复杂度为二次方，在长上下文运行受限，如FlashAttention在NVIDIA GH200上处理超400万tokens时无法完成单轮前向-反向传播。

Method: 用锐化的角度（余弦）相似度替代指数核，通过随机投影和软局部敏感哈希（LSH）近似注意力输出。

Result: 在语言建模、掩码语言建模和文本分类中，RACE Attention匹配强基线精度，减少运行时间和内存；在控制规模测试中，NVIDIA GH200 GPU单轮前向-反向传播可处理1200万tokens，Intel Xeon Gold 5220R CPU可处理7500万tokens。

Conclusion: RACE Attention为当今硬件上超长上下文窗口提供了实用且理论可靠的机制，希望得到实际应用。

Abstract: Softmax Attention has a quadratic time complexity, which becomes prohibitive
to run at long contexts, even with highly optimized GPU kernels. For example,
FlashAttention (an exact, GPU-optimized implementation of Softmax Attention)
cannot complete a single forward-backward pass of a multi-head attention layer
once the context exceeds ~4 million tokens on an NVIDIA GH200 (96 GB). We
introduce RACE Attention, a kernel-inspired alternative to Softmax Attention
that is linear in sequence length and embedding dimension. RACE Attention
replaces the exponential kernel with a sharpened angular (cosine) similarity,
and approximates attention outputs via randomized projections and soft
Locality-Sensitive Hashing (LSH). Across language modeling, masked language
modeling, and text classification, RACE Attention matches the accuracy of
strong baselines while reducing runtime and memory. In a controlled scale test,
it processes up to 12 million tokens during a single forward-backward pass on
an NVIDIA GH200 GPU and 75 million tokens on an Intel Xeon Gold 5220R CPU, well
beyond the practical limits of the current state-of-the-art attention
implementations. RACE Attention thus offers a practical, theoretically grounded
mechanism for outrageously long context windows on today's hardware. We hope
that it gets adopted in practice.

</details>


### [306] [Principled and Tractable RL for Reasoning with Diffusion Language Models](https://arxiv.org/abs/2510.04019)
*Anthony Zhan*

Main category: cs.LG

TL;DR: 提出适用于扩散大语言模型（dLLMs）的策略梯度方法AGRPO，在数学推理任务上效果显著，证明在线强化学习算法可有效扩展到dLLMs。


<details>
  <summary>Details</summary>
Motivation: 现有dLLMs未从现代后训练技术（如强化学习）中受益，传统大语言模型算法与扩散框架不兼容，现有dLLM后训练的强化学习尝试缺乏理论基础。

Method: 提出Amortized Group Relative Policy Optimization (AGRPO)，使用蒙特卡罗采样计算无偏策略梯度估计。

Result: 在不同数学推理任务上，相比基线模型LLaDA - 8B - Instruct，GSM8K绝对增益达7.6%，Countdown任务性能提升3.8倍，相比diffu - GRPO性能提升1.3倍，且在不同推理采样步数下结果稳定。

Conclusion: 在线强化学习算法可以有原则地扩展到扩散大语言模型，兼具理论合理性和实际有效性。

Abstract: Diffusion large language models (dLLMs) are a new paradigm of
non-autoregressive language models that are trained to predict multiple tokens
in parallel and generate text via iterative unmasking. Recent works have
successfully pretrained dLLMs to parity with autoregressive LLMs at the 8B
scale, but dLLMs have yet to benefit from modern post-training techniques, e.g.
reinforcement learning (RL), that have proven effective for autoregressive
models. Crucially, algorithms designed for traditional LLMs aren't directly
compatible with diffusion frameworks due to inherent differences in modeling
assumptions. Moreover, existing attempts at dLLM post-training with RL rely on
heuristic-based objectives with no theoretical grounding. In this work, we
present Amortized Group Relative Policy Optimization (AGRPO), a principled
on-policy RL algorithm designed specifically for dLLMs. AGRPO uses Monte Carlo
sampling to compute an unbiased policy gradient estimate, making it the first
tractable, faithful adaptation of policy gradient methods for dLLMs. We
demonstrate AGRPO's effectiveness on different math/reasoning tasks, a common
setting for RL with LLMs, achieving up to +7.6% absolute gain on GSM8K and 3.8x
performance on the Countdown task over the baseline LLaDA-8B-Instruct model and
1.3x performance gains over comparable RL methods such as diffu-GRPO.
Furthermore, these gains persist across different numbers of sampling steps at
inference time, achieving better tradeoffs between compute and performance. Our
results demonstrate that online RL algorithms can be extended to diffusion LLMs
in principled ways, maintaining both theoretical soundness and practical
effectiveness.

</details>


### [307] [Spatiotemporal Forecasting as Planning: A Model-Based Reinforcement Learning Approach with Generative World Models](https://arxiv.org/abs/2510.04020)
*Hao Wu,Yuan Gao,Xingjian Shi,Shuaipeng Li,Fan Xu,Fan Zhang,Zhihong Zhu,Weiyan Wang,Xiao Luo,Kun Wang,Xian Wu,Xiaomeng Huang*

Main category: cs.LG

TL;DR: 提出基于模型强化学习的时空预测新范式SFP，可降低预测误差，在关键指标上表现出色。


<details>
  <summary>Details</summary>
Motivation: 应对物理时空预测中固有的随机性和不可微指标的双重挑战。

Method: 构建生成世界模型进行环境模拟，用基于束搜索的规划算法，以不可微领域指标为奖励信号探索高回报序列，通过迭代自训练优化策略。

Result: 显著降低预测误差，在捕获极端事件等关键领域指标上表现出色。

Conclusion: 所提出的SFP范式能有效解决物理时空预测问题。

Abstract: To address the dual challenges of inherent stochasticity and
non-differentiable metrics in physical spatiotemporal forecasting, we propose
Spatiotemporal Forecasting as Planning (SFP), a new paradigm grounded in
Model-Based Reinforcement Learning. SFP constructs a novel Generative World
Model to simulate diverse, high-fidelity future states, enabling an
"imagination-based" environmental simulation. Within this framework, a base
forecasting model acts as an agent, guided by a beam search-based planning
algorithm that leverages non-differentiable domain metrics as reward signals to
explore high-return future sequences. These identified high-reward candidates
then serve as pseudo-labels to continuously optimize the agent's policy through
iterative self-training, significantly reducing prediction error and
demonstrating exceptional performance on critical domain metrics like capturing
extreme events.

</details>


### [308] [Multi-Class Support Vector Machine with Differential Privacy](https://arxiv.org/abs/2510.04027)
*Jinseong Park,Yujin Choi,Jaewook Lee*

Main category: cs.LG

TL;DR: 为解决差分隐私应用于多类支持向量机（SVM）的不足，提出新的差分隐私多类SVM（PMSVM），在多类场景效果超现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私应用于多类SVM时，标准方法多次查询样本消耗隐私预算，存在不足。

Method: 探索全合一SVM方法，提出PMSVM，采用权重和梯度扰动方法，并进行严格敏感性和收敛性分析。

Result: 在多类场景中，所提方法效果超过现有DP - SVM方法。

Conclusion: 提出的PMSVM能有效解决差分隐私应用于多类SVM的问题，在多类场景有更好表现。

Abstract: With the increasing need to safeguard data privacy in machine learning
models, differential privacy (DP) is one of the major frameworks to build
privacy-preserving models. Support Vector Machines (SVMs) are widely used
traditional machine learning models due to their robust margin guarantees and
strong empirical performance in binary classification. However, applying DP to
multi-class SVMs is inadequate, as the standard one-versus-rest (OvR) and
one-versus-one (OvO) approaches repeatedly query each data sample when building
multiple binary classifiers, thus consuming the privacy budget proportionally
to the number of classes. To overcome this limitation, we explore all-in-one
SVM approaches for DP, which access each data sample only once to construct
multi-class SVM boundaries with margin maximization properties. We propose a
novel differentially Private Multi-class SVM (PMSVM) with weight and gradient
perturbation methods, providing rigorous sensitivity and convergence analyses
to ensure DP in all-in-one SVMs. Empirical results demonstrate that our
approach surpasses existing DP-SVM methods in multi-class scenarios.

</details>


### [309] [The Debate on RLVR Reasoning Capability Boundary: Shrinkage, Expansion, or Both? A Two-Stage Dynamic View](https://arxiv.org/abs/2510.04028)
*Xinhao Yao,Lu Yu,Xiaolin Hu,Fengwei Teng,Qing Cui,Jun Zhou,Yong Liu*

Main category: cs.LG

TL;DR: 本文探讨强化学习与可验证奖励（RLVR）对大语言模型推理能力的影响，发现存在两个阶段，过度利用会使能力边界收缩，长期训练到探索阶段可扩展边界，并重新审视仅用相对负梯度延长训练的潜力。


<details>
  <summary>Details</summary>
Motivation: 解决关于RLVR对大语言模型推理能力影响的争议，调和不同研究的矛盾结果。

Method: 从理论和实证两方面研究，分析RLVR在两个阶段的概率质量动态。

Result: 发现存在利用和探索两个阶段，不同阶段对推理能力边界有不同影响。

Conclusion: 过度利用会导致能力边界收缩，长期训练到探索阶段能促进推理能力边界扩展，为开发更高级推理能力提供理论和实证基础。

Abstract: The ongoing debate on whether reinforcement learning with verifiable rewards
(RLVR) expands or shrinks the reasoning capabilities of large language models
(LLMs) remains unresolved. Some studies contend that RLVR mainly improves
sampling efficiency but at the expense of diversity and exploratory capacity,
resulting in capability boundary shrinkage. In contrast, others demonstrate
that prolonged training can lead to the emergence of novel reasoning
strategies, suggesting capability boundary expansion. To reconcile these
contradictory findings, we theoretically and empirically show that both
perspectives are partially valid-each aligning with a separate phase in an
inherent two-stage probability mass dynamic: (1) Exploitation stage: initially,
the model primarily samples explored high-reward and low-reward tokens, while
rarely selecting the potentially optimal token. Positive advantage estimates
increase the probability of high-reward tokens and decrease those of low-reward
tokens, yet the optimal token's probability remains largely unchanged during
this stage. (2) Exploration stage: as training advances, the growth rate of
previously acquired high-reward tokens slows as their probabilities approach
saturation. When a potentially optimal token-now receiving positive advantage
estimates-is occasionally sampled, its probability increases, while those of
the originally high-reward tokens decrease. This dynamic suggests that
over-exploitation during the exploitation stage may lead to capability boundary
shrinkage, whereas prolonged training into the exploration stage can promote an
expansion of the reasoning capability boundary. Building upon our insights, we
revisit the potential of only using relative negative gradients for prolonging
training, providing a theoretical and empirical foundation for the development
of more advanced reasoning capabilities.

</details>


### [310] [Adaptive kernel-density approach for imbalanced binary classification](https://arxiv.org/abs/2510.04046)
*Kotaro J. Nishimura,Yuichi Sakumura,Kazushi Ikeda*

Main category: cs.LG

TL;DR: 提出KOTARO方法解决类别不平衡分类问题，实验证明其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现实二元分类任务中类别不平衡问题常见，传统方法在极严重不平衡时表现不佳，尤其在医疗诊断和异常检测领域需正确分类少数类。

Method: 提出KOTARO方法，扩展核密度估计框架，根据局部样本密度自适应调整决策边界，动态调整高斯基函数带宽。

Result: 在合成和真实不平衡数据集实验中，KOTARO表现优于传统方法，尤其在严重不平衡条件下。

Conclusion: KOTARO是解决广泛不平衡分类问题的有前景方案。

Abstract: Class imbalance is a common challenge in real-world binary classification
tasks, often leading to predictions biased toward the majority class and
reduced recognition of the minority class. This issue is particularly critical
in domains such as medical diagnosis and anomaly detection, where correct
classification of minority classes is essential. Conventional methods often
fail to deliver satisfactory performance when the imbalance ratio is extremely
severe. To address this challenge, we propose a novel approach called
Kernel-density-Oriented Threshold Adjustment with Regional Optimization
(KOTARO), which extends the framework of kernel density estimation (KDE) by
adaptively adjusting decision boundaries according to local sample density. In
KOTARO, the bandwidth of Gaussian basis functions is dynamically tuned based on
the estimated density around each sample, thereby enhancing the classifier's
ability to capture minority regions. We validated the effectiveness of KOTARO
through experiments on both synthetic and real-world imbalanced datasets. The
results demonstrated that KOTARO outperformed conventional methods,
particularly under conditions of severe imbalance, highlighting its potential
as a promising solution for a wide range of imbalanced classification problems

</details>


### [311] [Variational Diffusion Unlearning: A Variational Inference Framework for Unlearning in Diffusion Models under Data Constraints](https://arxiv.org/abs/2510.04058)
*Subhodip Panda,MS Varun,Shreyans Jain,Sarthak Kumar Maharana,Prathosh A. P*

Main category: cs.LG

TL;DR: 为在数据受限场景下防止预训练扩散模型生成含不良特征的输出，提出变分扩散遗忘（VDU）方法并验证有效性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型可能生成不良输出，现有机器遗忘方法在数据受限场景无效，需新方法。

Method: 提出VDU方法，受变分推理框架启发，最小化含可塑性诱导器和稳定性正则化器的损失函数。

Result: 通过对MNIST、CIFAR - 10、tinyImageNet数据集的类遗忘和Stable Diffusion模型的特征遗忘实验验证了方法有效性。

Conclusion: VDU方法能在数据受限场景下有效防止预训练扩散模型生成含不良特征的输出。

Abstract: For a responsible and safe deployment of diffusion models in various domains,
regulating the generated outputs from these models is desirable because such
models could generate undesired, violent, and obscene outputs. To tackle this
problem, recent works use machine unlearning methodology to forget training
data points containing these undesired features from pre-trained generative
models. However, these methods proved to be ineffective in data-constrained
settings where the whole training dataset is inaccessible. Thus, the principal
objective of this work is to propose a machine unlearning methodology that can
prevent the generation of outputs containing undesired features from a
pre-trained diffusion model in such a data-constrained setting. Our proposed
method, termed as Variational Diffusion Unlearning (VDU), is a computationally
efficient method that only requires access to a subset of training data
containing undesired features. Our approach is inspired by the variational
inference framework with the objective of minimizing a loss function consisting
of two terms: plasticity inducer and stability regularizer. Plasticity inducer
reduces the log-likelihood of the undesired training data points, while the
stability regularizer, essential for preventing loss of image generation
quality, regularizes the model in parameter space. We validate the
effectiveness of our method through comprehensive experiments for both class
unlearning and feature unlearning. For class unlearning, we unlearn some
user-identified classes from MNIST, CIFAR-10, and tinyImageNet datasets from a
pre-trained unconditional denoising diffusion probabilistic model (DDPM).
Similarly, for feature unlearning, we unlearn the generation of certain
high-level features from a pre-trained Stable Diffusion model

</details>


### [312] [What Scales in Cross-Entropy Scaling Law?](https://arxiv.org/abs/2510.04067)
*Junxi Yan,Zixi Wei,Jingtao Zhan,Qingyao Ai,Yiqun Liu*

Main category: cs.LG

TL;DR: 本文指出交叉熵缩放定律在大模型下失效，将交叉熵分解为三部分，发现只有误差熵遵循幂律缩放，提出误差熵缩放定律更准确描述模型行为。


<details>
  <summary>Details</summary>
Motivation: 交叉熵缩放定律在大模型下失效，需要找出根本原因并找到更准确的缩放定律。

Method: 将交叉熵分解为误差熵、自对齐和置信度三部分，从理论和实验两方面进行研究。

Result: 只有误差熵遵循稳健的幂律缩放，误差熵在小模型中占比大，随模型增大占比减小。

Conclusion: 误差熵缩放定律能更准确描述模型行为，在大语言模型的训练、理解和未来发展中有广泛应用。

Abstract: The cross-entropy scaling law has long served as a key tool for guiding the
development of large language models. It shows that cross-entropy loss
decreases in a predictable power-law rate as the model size increases. However,
recent evidence indicates that this law breaks down at very large scales: the
loss decreases more slowly than expected, which causes significant trouble for
developing large language models. In this paper, we hypothesize that the root
cause lies in the fact that cross-entropy itself does not truly scale; instead,
only one of its hidden components does. To investigate this, we introduce a
novel decomposition of cross-entropy into three parts: Error-Entropy,
Self-Alignment, and Confidence. We show both theoretically and empirically that
this decomposition precisely captures the training dynamics and optimization
objectives. Through extensive experiments on multiple datasets and 32 models
spanning five orders of magnitude in size, we find that only error-entropy
follows a robust power-law scaling, while the other two terms remain largely
invariant. Moreover, error-entropy constitutes the dominant share of
cross-entropy in small models but diminishes in proportion as models grow
larger. This explains why the cross-entropy scaling law appears accurate at
small scales but fails at very large ones. Our findings establish the
error-entropy scaling law as a more accurate description of model behavior. We
believe it will have wide applications in the training, understanding, and
future development of large language models.

</details>


### [313] [Using predefined vector systems as latent space configuration for neural network supervised training on data with arbitrarily large number of classes](https://arxiv.org/abs/2510.04090)
*Nikita Gabdullin*

Main category: cs.LG

TL;DR: 提出一种训练神经网络的方法，可使同一架构训练不受类别数量限制，并进行多数据集实验，还探讨潜在应用。


<details>
  <summary>Details</summary>
Motivation: 监督学习训练神经网络时需使参数数量依赖类别数量，限制其在类别数量极多或未知时的应用。

Method: 在神经网络训练中使用预定义向量系统作为目标潜在空间配置（LSC），选取An根系随机扰动向量，通过匹配预测与预定义向量进行训练。

Result: 在Cinic - 10和ImageNet - 1K的高低维情况成功训练编码器和视觉Transformer（ViT），并在128万类数据集上训练ViT。

Conclusion: 所提方法适用于类别数量极多的数据集训练，且在终身学习和神经网络蒸馏等方面有潜在应用，具有多功能性。

Abstract: Supervised learning (SL) methods are indispensable for neural network (NN)
training used to perform classification tasks. While resulting in very high
accuracy, SL training often requires making NN parameter number dependent on
the number of classes, limiting their applicability when the number of classes
is extremely large or unknown in advance. In this paper we propose a
methodology that allows one to train the same NN architecture regardless of the
number of classes. This is achieved by using predefined vector systems as the
target latent space configuration (LSC) during NN training. We discuss the
desired properties of target configurations and choose randomly perturbed
vectors of An root system for our experiments. These vectors are used to
successfully train encoders and visual transformers (ViT) on Cinic-10 and
ImageNet-1K in low- and high-dimensional cases by matching NN predictions with
the predefined vectors. Finally, ViT is trained on a dataset with 1.28 million
classes illustrating the applicability of the method to training on datasets
with extremely large number of classes. In addition, potential applications of
LSC in lifelong learning and NN distillation are discussed illustrating
versatility of the proposed methodology.

</details>


### [314] [Rethinking Consistent Multi-Label Classification under Inexact Supervision](https://arxiv.org/abs/2510.04091)
*Wei Wang,Tianhao Ma,Ming-Kun Xie,Gang Niu,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 提出统一方法应对部分多标签学习和互补多标签学习问题，有理论证明和实验验证效果。


<details>
  <summary>Details</summary>
Motivation: 现有两种范式的一致方法在现实场景中所需条件难满足，需新方法统一处理问题。

Method: 提出基于一阶和二阶策略的无偏风险估计器。

Result: 理论上证明了在两种多标签分类评估指标上的一致性，得出估计误差收敛率；实验验证了方法比现有方法有效。

Conclusion: 提出的不依赖特定条件的统一方法能有效处理部分多标签学习和互补多标签学习问题。

Abstract: Partial multi-label learning and complementary multi-label learning are two
popular weakly supervised multi-label classification paradigms that aim to
alleviate the high annotation costs of collecting precisely annotated
multi-label data. In partial multi-label learning, each instance is annotated
with a candidate label set, among which only some labels are relevant; in
complementary multi-label learning, each instance is annotated with
complementary labels indicating the classes to which the instance does not
belong. Existing consistent approaches for the two paradigms either require
accurate estimation of the generation process of candidate or complementary
labels or assume a uniform distribution to eliminate the estimation problem.
However, both conditions are usually difficult to satisfy in real-world
scenarios. In this paper, we propose consistent approaches that do not rely on
the aforementioned conditions to handle both problems in a unified way.
Specifically, we propose two unbiased risk estimators based on first- and
second-order strategies. Theoretically, we prove consistency w.r.t. two widely
used multi-label classification evaluation metrics and derive convergence rates
for the estimation errors of the proposed risk estimators. Empirically,
extensive experimental results validate the effectiveness of our proposed
approaches against state-of-the-art methods.

</details>


### [315] [Why Cannot Neural Networks Master Extrapolation? Insights from Physical Laws](https://arxiv.org/abs/2510.04102)
*Ramzi Dakhmouche,Hossein Gorji*

Main category: cs.LG

TL;DR: 受基础模型在语言建模中成功的启发，其在时间序列预测短期表现良好，但外推预测不佳。本文分析模型外推性能差的原因并给出设计方向。


<details>
  <summary>Details</summary>
Motivation: 基础模型在时间序列预测短期表现好，但外推预测难以超越简单基线，与物理定律外推特性形成对比，需要探究神经网络与物理定律结构差异。

Method: 识别并形式化统计学习模型在训练域外准确预测的基本属性，进行理论分析和实证研究。

Result: 明确外推差距的根本原因，展示该属性对当前深度学习架构的影响。

Conclusion: 指出当前深度学习模型外推性能不佳的根源，为设计能掌握外推的下一代预测模型提供方向。

Abstract: Motivated by the remarkable success of Foundation Models (FMs) in language
modeling, there has been growing interest in developing FMs for time series
prediction, given the transformative power such models hold for science and
engineering. This culminated in significant success of FMs in short-range
forecasting settings. However, extrapolation or long-range forecasting remains
elusive for FMs, which struggle to outperform even simple baselines. This
contrasts with physical laws which have strong extrapolation properties, and
raises the question of the fundamental difference between the structure of
neural networks and physical laws. In this work, we identify and formalize a
fundamental property characterizing the ability of statistical learning models
to predict more accurately outside of their training domain, hence explaining
performance deterioration for deep learning models in extrapolation settings.
In addition to a theoretical analysis, we present empirical results showcasing
the implications of this property on current deep learning architectures. Our
results not only clarify the root causes of the extrapolation gap but also
suggest directions for designing next-generation forecasting models capable of
mastering extrapolation.

</details>


### [316] [Can Linear Probes Measure LLM Uncertainty?](https://arxiv.org/abs/2510.04108)
*Ramzi Dakhmouche,Adrien Letellier,Hossein Gorji*

Main category: cs.LG

TL;DR: 本文针对大语言模型多选结构生成的不确定性量化问题，提出基于贝叶斯统计的线性回归方法，实验显示优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型多选结构生成的不确定性量化方法仍以最大softmax分数的简单基线为主，存在不足。

Method: 训练多个贝叶斯线性模型，根据前一层输出预测当前层输出，基于层后验分布，通过识别分布特征的稀疏组合推断全局不确定性水平。

Result: 在各种大语言模型上的数值实验显示，该方法持续优于现有基线。

Conclusion: 采用贝叶斯统计的原则性方法，即使使用简单的线性回归模型，也能在大语言模型不确定性量化方面取得更好的性能。

Abstract: Effective Uncertainty Quantification (UQ) represents a key aspect for
reliable deployment of Large Language Models (LLMs) in automated
decision-making and beyond. Yet, for LLM generation with multiple choice
structure, the state-of-the-art in UQ is still dominated by the naive baseline
given by the maximum softmax score. To address this shortcoming, we demonstrate
that taking a principled approach via Bayesian statistics leads to improved
performance despite leveraging the simplest possible model, namely linear
regression. More precisely, we propose to train multiple Bayesian linear
models, each predicting the output of a layer given the output of the previous
one. Based on the obtained layer-level posterior distributions, we infer the
global uncertainty level of the LLM by identifying a sparse combination of
distributional features, leading to an efficient UQ scheme. Numerical
experiments on various LLMs show consistent improvement over state-of-the-art
baselines.

</details>


### [317] [Wasserstein projection distance for fairness testing of regression models](https://arxiv.org/abs/2510.04114)
*Wanxin Li,Yongjin P. Park,Khanh Dao Duc*

Main category: cs.LG

TL;DR: 本文提出基于Wasserstein投影的回归模型公平性测试框架，理论推导并通过实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 机器学习公平性研究多聚焦分类任务，回归模型公平性研究不足。

Method: 引入基于Wasserstein投影的框架，提出假设检验方法和最优数据扰动方法。

Result: 理论上对回归公平性标准分类，推导测试统计量等；实验表明比基于排列的测试有更高特异性，能检测和缓解实际应用中的偏差。

Conclusion: 所提方法可有效检测和缓解回归模型中的偏差，平衡公平性与准确性。

Abstract: Fairness in machine learning is a critical concern, yet most research has
focused on classification tasks, leaving regression models underexplored. This
paper introduces a Wasserstein projection-based framework for fairness testing
in regression models, focusing on expectation-based criteria. We propose a
hypothesis-testing approach and an optimal data perturbation method to improve
fairness while balancing accuracy. Theoretical results include a detailed
categorization of fairness criteria for regression, a dual reformulation of the
Wasserstein projection test statistic, and the derivation of asymptotic bounds
and limiting distributions. Experiments on synthetic and real-world datasets
demonstrate that the proposed method offers higher specificity compared to
permutation-based tests, and effectively detects and mitigates biases in real
applications such as student performance and housing price prediction.

</details>


### [318] [On the Statistical Query Complexity of Learning Semiautomata: a Random Walk Approach](https://arxiv.org/abs/2510.04115)
*George Giapitzakis,Kimon Fountoulakis,Eshaan Nichani,Jason D. Lee*

Main category: cs.LG

TL;DR: 本文首次在输入单词和初始状态的均匀分布下建立半自动机的统计查询硬度结果，通过将区分两个半自动机的最终状态任务简化为研究群上随机游走行为，并运用傅里叶分析和对称群表示论工具得到结果。


<details>
  <summary>Details</summary>
Motivation: 在自然语言处理、机器人技术等领域有应用的半自动机，需要建立其在均匀分布下的统计查询硬度结果。

Method: 将区分半自动机最终状态的任务简化为研究群$S_{N} 	imes S_{N}$上的随机游走行为，运用傅里叶分析和对称群表示论工具。

Result: 得到了紧密的谱间隙界，表明在状态数量的多项式步数后，不同的半自动机几乎不相关，得出所需的硬度结果。

Conclusion: 成功在输入单词和初始状态的均匀分布下建立半自动机的统计查询硬度结果，且该结果仅源于半自动机的内部状态转移结构。

Abstract: Semiautomata form a rich class of sequence-processing algorithms with
applications in natural language processing, robotics, computational biology,
and data mining. We establish the first Statistical Query hardness result for
semiautomata under the uniform distribution over input words and initial
states. We show that Statistical Query hardness can be established when both
the alphabet size and input length are polynomial in the number of states.
Unlike the case of deterministic finite automata, where hardness typically
arises through the hardness of the language they recognize (e.g., parity), our
result is derived solely from the internal state-transition structure of
semiautomata. Our analysis reduces the task of distinguishing the final states
of two semiautomata to studying the behavior of a random walk on the group
$S_{N} \times S_{N}$. By applying tools from Fourier analysis and the
representation theory of the symmetric group, we obtain tight spectral gap
bounds, demonstrating that after a polynomial number of steps in the number of
states, distinct semiautomata become nearly uncorrelated, yielding the desired
hardness result.

</details>


### [319] [Attending on Multilevel Structure of Proteins enables Accurate Prediction of Cold-Start Drug-Target Interactions](https://arxiv.org/abs/2510.04126)
*Ziying Zhang,Yaqing Wang,Yuxuan Sun,Min Ye,Quanming Yao*

Main category: cs.LG

TL;DR: 提出ColdDTI框架用于冷启动药物-靶点相互作用预测，考虑蛋白质多级结构，实验显示其优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 先前方法仅用蛋白质一级结构，无法捕捉涉及高级结构的相互作用，而蛋白质多级结构都会影响药物-靶点相互作用。

Method: 采用分层注意力机制挖掘蛋白质多级结构（从一级到四级）与药物结构在局部和全局粒度上的相互作用，利用挖掘的相互作用融合不同层次的结构表示进行最终预测。

Result: 在基准数据集上的实验表明，ColdDTI在冷启动设置中始终优于先前方法。

Conclusion: ColdDTI框架考虑蛋白质多级结构，能捕捉生物学上可转移的先验信息，避免过度依赖表示学习导致过拟合的风险，性能表现良好。

Abstract: Cold-start drug-target interaction (DTI) prediction focuses on interaction
between novel drugs and proteins. Previous methods typically learn transferable
interaction patterns between structures of drug and proteins to tackle it.
However, insight from proteomics suggest that protein have multi-level
structures and they all influence the DTI. Existing works usually represent
protein with only primary structures, limiting their ability to capture
interactions involving higher-level structures. Inspired by this insight, we
propose ColdDTI, a framework attending on protein multi-level structure for
cold-start DTI prediction. We employ hierarchical attention mechanism to mine
interaction between multi-level protein structures (from primary to quaternary)
and drug structures at both local and global granularities. Then, we leverage
mined interactions to fuse structure representations of different levels for
final prediction. Our design captures biologically transferable priors,
avoiding the risk of overfitting caused by excessive reliance on representation
learning. Experiments on benchmark datasets demonstrate that ColdDTI
consistently outperforms previous methods in cold-start settings.

</details>


### [320] [On the Limitations and Capabilities of Position Embeddings for Length Generalization](https://arxiv.org/abs/2510.04130)
*Yang Chen,Yitao Liang,Zhouchen Lin*

Main category: cs.LG

TL;DR: 本文研究Transformer中位置嵌入（PEs）在长度泛化（LG）方面的局限性和能力，提出理论分析和实用策略以提升LG。


<details>
  <summary>Details</summary>
Motivation: Transformer中PEs对LG性能影响显著，但作用尚不明确，故研究其在LG中的局限性和能力。

Method: 理论分析位置仅线性注意力（POLAs）中的PEs，引入线性表示复杂度（LRC）；扩展到实际Transformer，提出顺序表示复杂度（SRC）并进行猜想；通过实验验证假设；引入Scale Hint和基于学习的位置嵌入框架。

Result: 分析表明PEs不扩展计算能力，而是构建跨位置的学习计算；有实验证据支持SRC跨尺度不变时LG可行的假设。

Conclusion: 为提升Transformer的LG提供了理论见解和实用策略。

Abstract: In Transformers, Position Embeddings (PEs) significantly influence Length
Generalization (LG) performance, yet their fundamental role remains unclear. In
this work, we investigate the limitations and capabilities of PEs in achieving
LG. We theoretically analyze PEs in Position-Only Linear Attentions (POLAs),
introducing Linear Representation Complexity (LRC) to characterize when PEs
enable LG. Our analysis shows that PEs do not expand computational capabilities
but structure learned computations across positions. Extending to practical
Transformers, we propose Sequential Representation Complexity (SRC) and
conjecture that LG is possible if and only if SRC remains invariant across
scales. We support this hypothesis with empirical evidence in various reasoning
tasks. To enhance LG, we introduce Scale Hint, allowing flexible instance
scaling, and a Learning-Based Position Embedding framework that automatically
learns positional relations. Our work provides theoretical insights and
practical strategies for improving LG in Transformers.

</details>


### [321] [Modeling Time Series Dynamics with Fourier Ordinary Differential Equations](https://arxiv.org/abs/2510.04133)
*Muhao Guo,Yang Weng*

Main category: cs.LG

TL;DR: 提出Fourier ODEs（FODEs）处理时间序列建模，实验显示其在精度和效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: Neural ODEs在捕捉长期依赖和处理离散数据时存在局限性，需要新方法解决。

Method: 将时间序列数据通过FFT转换到频域，引入可学习的逐元素过滤机制。

Result: 在多个时间序列数据集实验中，FODEs在精度和效率上超过现有方法。

Conclusion: FODEs能有效捕捉长短时模式，为时间序列动态建模提供了强大框架。

Abstract: Neural ODEs (NODEs) have emerged as powerful tools for modeling time series
data, offering the flexibility to adapt to varying input scales and capture
complex dynamics. However, they face significant challenges: first, their
reliance on time-domain representations often limits their ability to capture
long-term dependencies and periodic structures; second, the inherent mismatch
between their continuous-time formulation and the discrete nature of real-world
data can lead to loss of granularity and predictive accuracy. To address these
limitations, we propose Fourier Ordinary Differential Equations (FODEs), an
approach that embeds the dynamics in the Fourier domain. By transforming
time-series data into the frequency domain using the Fast Fourier Transform
(FFT), FODEs uncover global patterns and periodic behaviors that remain elusive
in the time domain. Additionally, we introduce a learnable element-wise
filtering mechanism that aligns continuous model outputs with discrete
observations, preserving granularity and enhancing accuracy. Experiments on
various time series datasets demonstrate that FODEs outperform existing methods
in terms of both accuracy and efficiency. By effectively capturing both long-
and short-term patterns, FODEs provide a robust framework for modeling time
series dynamics.

</details>


### [322] [PhaseFormer: From Patches to Phases for Efficient and Effective Time Series Forecasting](https://arxiv.org/abs/2510.04134)
*Yiming Niu,Jinliang Deng,Yongxin Tong*

Main category: cs.LG

TL;DR: 现有深度学习方法处理时间序列周期性效率低，本文提出PhaseFormer解决该问题，实验显示其参数少且性能佳。


<details>
  <summary>Details</summary>
Motivation: 现有基于patch处理时间序列周期性的深度学习方法存在参数多、计算成本高的效率瓶颈。

Method: 引入相位视角建模周期性，提出PhaseFormer，通过紧凑相位嵌入进行相位预测，并利用轻量级路由机制实现高效跨相位交互。

Result: PhaseFormer约1k参数就达到了最先进性能，在大规模和复杂数据集上表现出色。

Conclusion: 该工作朝着高效有效的时间序列预测迈出了重要一步。

Abstract: Periodicity is a fundamental characteristic of time series data and has long
played a central role in forecasting. Recent deep learning methods strengthen
the exploitation of periodicity by treating patches as basic tokens, thereby
improving predictive effectiveness. However, their efficiency remains a
bottleneck due to large parameter counts and heavy computational costs. This
paper provides, for the first time, a clear explanation of why patch-level
processing is inherently inefficient, supported by strong evidence from
real-world data. To address these limitations, we introduce a phase perspective
for modeling periodicity and present an efficient yet effective solution,
PhaseFormer. PhaseFormer features phase-wise prediction through compact phase
embeddings and efficient cross-phase interaction enabled by a lightweight
routing mechanism. Extensive experiments demonstrate that PhaseFormer achieves
state-of-the-art performance with around 1k parameters, consistently across
benchmark datasets. Notably, it excels on large-scale and complex datasets,
where models with comparable efficiency often struggle. This work marks a
significant step toward truly efficient and effective time series forecasting.
Code is available at this repository:
https://github.com/neumyor/PhaseFormer_TSL

</details>


### [323] [Efficient Manifold-Constrained Neural ODE for High-Dimensional Datasets](https://arxiv.org/abs/2510.04138)
*Muhao Guo,Haoran Li,Yang Weng*

Main category: cs.LG

TL;DR: 本文提出新方法结合NODE学习与流形，解决高维系统中NODE估计动力学的计算和误差问题，实验显示性能优越。


<details>
  <summary>Details</summary>
Motivation: 高维系统中NODE估计动力学需大量计算且存在高截断误差，现有方法依赖已知流形知识，现实中该知识通常未知。

Method: 采用结构保留编码器处理数据找到底层图近似流形，提出新方法结合NODE学习与流形。

Result: 实验评估在多个数据集上进行，模型在准确性、函数评估次数和收敛速度方面优于现有基线。

Conclusion: 所提方法能有效解决高维数据集的挑战。

Abstract: Neural ordinary differential equations (NODE) have garnered significant
attention for their design of continuous-depth neural networks and the ability
to learn data/feature dynamics. However, for high-dimensional systems,
estimating dynamics requires extensive calculations and suffers from high
truncation errors for the ODE solvers. To address the issue, one intuitive
approach is to consider the non-trivial topological space of the data
distribution, i.e., a low-dimensional manifold. Existing methods often rely on
knowledge of the manifold for projection or implicit transformation,
restricting the ODE solutions on the manifold. Nevertheless, such knowledge is
usually unknown in realistic scenarios. Therefore, we propose a novel approach
to explore the underlying manifold to restrict the ODE process. Specifically,
we employ a structure-preserved encoder to process data and find the underlying
graph to approximate the manifold. Moreover, we propose novel methods to
combine the NODE learning with the manifold, resulting in significant gains in
computational speed and accuracy. Our experimental evaluations encompass
multiple datasets, where we compare the accuracy, number of function
evaluations (NFEs), and convergence speed of our model against existing
baselines. Our results demonstrate superior performance, underscoring the
effectiveness of our approach in addressing the challenges of high-dimensional
datasets.

</details>


### [324] [Beyond Next-Token Prediction: A Performance Characterization of Diffusion versus Autoregressive Language Models](https://arxiv.org/abs/2510.04146)
*Minseo Kim,Coleman Hooper,Aditya Tomar,Chenfeng Xu,Mehrdad Farajtabar,Michael W. Mahoney,Kurt Keutzer,Amir Gholami*

Main category: cs.LG

TL;DR: 本文对自回归语言模型（ARMs）和扩散语言模型（DLMs）进行性能研究，分析两者特性与权衡，指出DLMs算术强度高但长上下文扩展性差，还探索了块解码DLMs，分析了批量推理情况并给出加速DLM推理的机会。


<details>
  <summary>Details</summary>
Motivation: 现有研究未充分理解DLMs相对ARMs的性能影响，因此开展对ARMs和DLMs性能特征的综合研究。

Method: 使用理论分析和性能分析数据来表征两种模型方法之间的权衡。

Result: DLMs算术强度高但长上下文扩展性差；块解码DLMs可增加算术强度且长上下文扩展性好；批量推理中ARMs吞吐量更优。

Conclusion: 指出加速DLM推理的机会，特别是减少采样步骤对提升其相对ARMs延迟性能的重要性。

Abstract: Large Language Models (LLMs) have achieved state-of-the-art performance on a
broad range of Natural Language Processing (NLP) tasks, including document
processing and coding. Autoregressive Language Models (ARMs), which generate
tokens sequentially conditioned on all previous tokens, have been the
predominant paradigm for LLMs. However, while these networks have achieved high
accuracy across a range of downstream tasks, they exhibit low arithmetic
intensity due to the inherent sequential dependency with next-token prediction.
Recently, Diffusion Language Models (DLMs) have emerged as a promising
alternative architecture. DLMs generate output text in parallel, breaking the
limitations of sequential dependency. However, the performance implications of
DLMs relative to commonly deployed ARMs are not fully understood. In this work,
we present a comprehensive performance study analyzing the performance
characteristics of ARMs and DLMs, using both theoretical analysis and profiling
data to characterize the trade-offs between these approaches. We illustrate
that although DLMs exhibit higher arithmetic intensity compared to ARMs because
of their capability to utilize parallelism across sequence lengths, they fail
to scale effectively to longer contexts. We then explore DLMs with block-wise
decoding, outlining how this approach allows for increased arithmetic
intensity, while still scaling well to long contexts (similar to ARMs). We also
show interesting trade-offs for batched inference, where we find that ARMs
exhibit superior throughput, as they benefit more from parallelism across
sequences in the batch. Finally, we highlight opportunities for accelerating
DLM inference, and, in particular, highlight the importance of reducing the
number of sampling steps for allowing open-source DLMs to provide improved
latency relative to ARMs.

</details>


### [325] [Finite Time Analysis of Constrained Natural Critic-Actor Algorithm with Improved Sample Complexity](https://arxiv.org/abs/2510.04189)
*Prashansa Panda,Shalabh Bhatnagar*

Main category: cs.LG

TL;DR: 本文提出用于长期平均成本设置且有不等式约束的自然批评 - 行动者算法，给出非渐近收敛保证，确定最优学习率、改进样本复杂度，并在三个环境实验中表现良好。


<details>
  <summary>Details</summary>
Motivation: 以往研究对行动者 - 批评者（AC）算法的非渐近收敛分析存在局限，如仅建立渐近收敛，缺乏针对长期平均成本设置和不等式约束等情况的研究。

Method: 引入带有函数逼近的自然批评 - 行动者算法，对其进行非渐近收敛分析，确定最优学习率，提出改进样本复杂度的方法。

Result: 在三个不同的Safety - Gym环境实验中，该算法与其他知名算法相比具有竞争力。

Conclusion: 所提出的自然批评 - 行动者算法在长期平均成本设置和不等式约束下有良好的非渐近收敛性质及实际应用效果。

Abstract: Recent studies have increasingly focused on non-asymptotic convergence
analyses for actor-critic (AC) algorithms. One such effort introduced a
two-timescale critic-actor algorithm for the discounted cost setting using a
tabular representation, where the usual roles of the actor and critic are
reversed. However, only asymptotic convergence was established there.
Subsequently, both asymptotic and non-asymptotic analyses of the critic-actor
algorithm with linear function approximation were conducted. In our work, we
introduce the first natural critic-actor algorithm with function approximation
for the long-run average cost setting and under inequality constraints. We
provide the non-asymptotic convergence guarantees for this algorithm. Our
analysis establishes optimal learning rates and we also propose a modification
to enhance sample complexity. We further show the results of experiments on
three different Safety-Gym environments where our algorithm is found to be
competitive in comparison with other well known algorithms.

</details>


### [326] [Spectral Alignment as Predictor of Loss Explosion in Neural Network Training](https://arxiv.org/abs/2510.04202)
*Haiquan Qiu,You Wu,Yingjie Tan,Yaqing Wang,Quanming Yao*

Main category: cs.LG

TL;DR: 提出Spectral Alignment (SA)指标监测深度神经网络训练，能早于传统指标预警损失爆炸，计算开销低。


<details>
  <summary>Details</summary>
Motivation: 传统监测指标难以建立统一标准检测训练失败，训练中损失爆炸会使巨额训练投入无效。

Method: 引入Spectral Alignment (SA)指标，监测层输入与权重矩阵主奇异向量的分布对齐情况。

Result: 在语言模型上的实验表明，监测SA分布比传统标量指标能更早、更清晰预警损失爆炸。

Conclusion: SA计算开销低，是保障模型训练的实用工具。

Abstract: Loss explosions in training deep neural networks can nullify multi-million
dollar training runs. Conventional monitoring metrics like weight and gradient
norms are often lagging and ambiguous predictors, as their values vary
dramatically across different models and even between layers of the same model,
making it difficult to establish a unified standard for detecting impending
failure. We introduce Spectral Alignment (SA), a novel, theoretically-grounded
metric that monitors the distributional alignment between layer inputs and the
principal singular vectors of weight matrices. We show that a collapse in the
sign diversity of this alignment is a powerful early predictor of
representational collapse and training divergence. Empirical results on
language models demonstrate that monitoring the SA distribution provides a
significantly earlier and clearer warning of loss explosions than traditional
scalar metrics. SA's low computational overhead makes it a practical tool for
safeguarding model training.

</details>


### [327] [Adaptive Federated Learning via Dynamical System Model](https://arxiv.org/abs/2510.04203)
*Aayushya Agarwal,Larry Pileggi,Gauri Joshi*

Main category: cs.LG

TL;DR: 提出端到端自适应联邦学习方法，动态调整学习率和动量参数，解决异构联邦学习超参数选择问题，收敛快且无需调参。


<details>
  <summary>Details</summary>
Motivation: 异构联邦学习中，超参数选择对稳定高效收敛至关重要，但调参手动且计算成本高，超参数空间随客户端数量组合增长。

Method: 将联邦学习建模为动态系统，借鉴数值模拟和物理设计原理，自适应选择动量参数和学习率，由单一全局超参数控制。

Result: 得到自适应、基于动量的联邦学习算法，能处理异构联邦学习关键挑战，收敛快且对全局超参数选择不敏感。

Conclusion: 该框架比现有自适应方法收敛更优，无需对客户端和服务器更新进行超参数调优。

Abstract: Hyperparameter selection is critical for stable and efficient convergence of
heterogeneous federated learning, where clients differ in computational
capabilities, and data distributions are non-IID. Tuning hyperparameters is a
manual and computationally expensive process as the hyperparameter space grows
combinatorially with the number of clients. To address this, we introduce an
end-to-end adaptive federated learning method in which both clients and central
agents adaptively select their local learning rates and momentum parameters.
Our approach models federated learning as a dynamical system, allowing us to
draw on principles from numerical simulation and physical design. Through this
perspective, selecting momentum parameters equates to critically damping the
system for fast, stable convergence, while learning rates for clients and
central servers are adaptively selected to satisfy accuracy properties from
numerical simulation. The result is an adaptive, momentum-based federated
learning algorithm in which the learning rates for clients and servers are
dynamically adjusted and controlled by a single, global hyperparameter. By
designing a fully integrated solution for both adaptive client updates and
central agent aggregation, our method is capable of handling key challenges of
heterogeneous federated learning, including objective inconsistency and client
drift. Importantly, our approach achieves fast convergence while being
insensitive to the choice of the global hyperparameter, making it well-suited
for rapid prototyping and scalable deployment. Compared to state-of-the-art
adaptive methods, our framework is shown to deliver superior convergence for
heterogeneous federated learning while eliminating the need for hyperparameter
tuning both client and server updates.

</details>


### [328] [PolyKAN: A Polyhedral Analysis Framework for Provable and Minimal KAN Compression](https://arxiv.org/abs/2510.04205)
*Di Zhang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to
traditional Multi-Layer Perceptrons (MLPs), offering enhanced interpretability
and a strong mathematical foundation. However, their parameter efficiency
remains a significant challenge for practical deployment. This paper introduces
PolyKAN, a novel theoretical framework for KAN compression that provides formal
guarantees on both model size reduction and approximation error. By leveraging
the inherent piecewise polynomial structure of KANs, we formulate the
compression problem as one of optimal polyhedral region merging. We establish a
rigorous polyhedral characterization of KANs, develop a complete theory of
$\epsilon$-equivalent compression, and design an optimal dynamic programming
algorithm that guarantees minimal compression under specified error bounds. Our
theoretical analysis demonstrates that PolyKAN achieves provably minimal
compression while maintaining strict error control, with polynomial-time
complexity in all network parameters. The framework provides the first formal
foundation for KAN compression with mathematical guarantees, opening new
directions for efficient deployment of interpretable neural architectures.

</details>


### [329] [Why Low-Precision Transformer Training Fails: An Analysis on Flash Attention](https://arxiv.org/abs/2510.04212)
*Haiquan Qiu,Quanming Yao*

Main category: cs.LG

TL;DR: 论文解释低精度下使用闪存注意力训练变压器模型损失爆炸原因并提出解决方案


<details>
  <summary>Details</summary>
Motivation: 低精度格式训练变压器模型存在训练不稳定问题，需解释闪存注意力训练损失爆炸原因

Method: 深入分析揭示损失爆炸由注意力机制中低秩表示和低精度算术舍入误差复合效应导致，引入对闪存注意力的最小修改验证发现

Result: 简单修改稳定了训练过程

Conclusion: 分析得到验证，为长期存在的问题提供实用解决方案

Abstract: The pursuit of computational efficiency has driven the adoption of
low-precision formats for training transformer models. However, this progress
is often hindered by notorious training instabilities. This paper provides the
first mechanistic explanation for a long-standing and unresolved failure case
where training with flash attention in low-precision settings leads to
catastrophic loss explosions. Our in-depth analysis reveals that the failure is
not a random artifact but caused by two intertwined phenomena: the emergence of
similar low-rank representations within the attention mechanism and the
compounding effect of biased rounding errors inherent in low-precision
arithmetic. We demonstrate how these factors create a vicious cycle of error
accumulation that corrupts weight updates, ultimately derailing the training
dynamics. To validate our findings, we introduce a minimal modification to the
flash attention that mitigates the bias in rounding errors. This simple change
stabilizes the training process, confirming our analysis and offering a
practical solution to this persistent problem.

</details>


### [330] [MLLMEraser: Achieving Test-Time Unlearning in Multimodal Large Language Models through Activation Steering](https://arxiv.org/abs/2510.04217)
*Chenlu Ding,Jiancan Wu,Leheng Sheng,Fan Zhang,Yancheng Yuan,Xiang Wang,Xiangnan He*

Main category: cs.LG

TL;DR: 提出MLLMEraser框架用于测试时无训练的知识擦除，在多模型实验中表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型大规模部署存在隐私数据、过时知识和有害内容问题，现有无学习方法计算成本高、不可逆且会扭曲保留知识。

Method: 提出输入感知、无训练的MLLMEraser框架，利用激活引导实现动态知识擦除，构建多模态擦除方向，设计输入感知引导机制。

Result: 在LLaVA - 1.5和Qwen - 2.5 - VL上实验表明，MLLMEraser优于现有基线，遗忘性能强、计算成本低且效用退化小。

Conclusion: MLLMEraser是一种有效的多模态大语言模型无学习方法，能在低计算成本下实现知识擦除并保留模型效用。

Abstract: Multimodal large language models (MLLMs) have demonstrated remarkable
capabilities across vision-language tasks, yet their large-scale deployment
raises pressing concerns about memorized private data, outdated knowledge, and
harmful content. Existing unlearning approaches for MLLMs typically adapt
training-based strategies such as gradient ascent or preference optimization,
but these methods are computationally expensive, irreversible, and often
distort retained knowledge. In this work, we propose MLLMEraser, an
input-aware, training-free framework for test-time unlearning. Our approach
leverages activation steering to enable dynamic knowledge erasure without
parameter updates. Specifically, we construct a multimodal erasure direction by
contrasting adversarially perturbed, knowledge-recall image-text pairs with
knowledge-erasure counterparts, capturing both textual and visual
discrepancies. To prevent unnecessary interference, we further design an
input-aware steering mechanism that adaptively determines when and how the
erasure direction should be applied, preserving utility on retained knowledge
while enforcing forgetting on designated content. Experiments on LLaVA-1.5 and
Qwen-2.5-VL demonstrate that MLLMEraser consistently outperforms
state-of-the-art MLLM unlearning baselines, achieving stronger forgetting
performance with lower computational cost and minimal utility degradation.

</details>


### [331] [Physics-Inspired All-Pair Interaction Learning for 3D Dynamics Modeling](https://arxiv.org/abs/2510.04233)
*Kai Yang,Yuqi Huang,Junheng Tao,Wanyu Wang,Qitian Wu*

Main category: cs.LG

TL;DR: 提出PAINET学习多体系统全对相互作用，在多个真实世界基准测试中优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于GNN的方法依赖显式观测结构，无法捕捉对复杂物理行为和动力学机制至关重要的未观测相互作用。

Method: 提出PAINET，包含基于能量函数最小化轨迹的物理启发注意力网络和保持等变性的并行解码器。

Result: 在人类运动捕捉、分子动力学和大规模蛋白质模拟等基准测试中，PAINET始终优于最近提出的模型，3D动力学预测误差降低4.7% - 41.5%，计算成本相当。

Conclusion: PAINET在多体系统3D动力学建模方面表现出色，能有效捕捉相互作用。

Abstract: Modeling 3D dynamics is a fundamental problem in multi-body systems across
scientific and engineering domains and has important practical implications in
trajectory prediction and simulation. While recent GNN-based approaches have
achieved strong performance by enforcing geometric symmetries, encoding
high-order features or incorporating neural-ODE mechanics, they typically
depend on explicitly observed structures and inherently fail to capture the
unobserved interactions that are crucial to complex physical behaviors and
dynamics mechanism. In this paper, we propose PAINET, a principled
SE(3)-equivariant neural architecture for learning all-pair interactions in
multi-body systems. The model comprises: (1) a novel physics-inspired attention
network derived from the minimization trajectory of an energy function, and (2)
a parallel decoder that preserves equivariance while enabling efficient
inference. Empirical results on diverse real-world benchmarks, including human
motion capture, molecular dynamics, and large-scale protein simulations, show
that PAINET consistently outperforms recently proposed models, yielding 4.7% to
41.5% error reductions in 3D dynamics prediction with comparable computation
costs in terms of time and memory.

</details>


### [332] [Truncated Kernel Stochastic Gradient Descent with General Losses and Spherical Radial Basis Functions](https://arxiv.org/abs/2510.04237)
*Jinhui Bai,Andreas Christmann,Lei Shi*

Main category: cs.LG

TL;DR: 提出用于大规模监督学习的新型核随机梯度下降算法，提高效率和可扩展性，有理论证明和实验验证。


<details>
  <summary>Details</summary>
Motivation: 改进传统核随机梯度下降算法在大规模监督学习中的效率和可扩展性。

Method: 采用创新正则化策略，利用球径向基函数的无穷级数展开，基于新的核诱导协方差算子谱结构估计建立统一分析框架，结合线性SGD的坐标更新。

Result: 证明最后一次迭代和后缀平均以极小极大最优速率收敛，在再生核希尔伯特空间建立最优强收敛，算法降低计算复杂度、实现最优存储复杂度。

Conclusion: 所提算法有效，能高效处理流数据，通过数值实验得到验证。

Abstract: In this paper, we propose a novel kernel stochastic gradient descent (SGD)
algorithm for large-scale supervised learning with general losses. Compared to
traditional kernel SGD, our algorithm improves efficiency and scalability
through an innovative regularization strategy. By leveraging the infinite
series expansion of spherical radial basis functions, this strategy projects
the stochastic gradient onto a finite-dimensional hypothesis space, which is
adaptively scaled according to the bias-variance trade-off, thereby enhancing
generalization performance. Based on a new estimation of the spectral structure
of the kernel-induced covariance operator, we develop an analytical framework
that unifies optimization and generalization analyses. We prove that both the
last iterate and the suffix average converge at minimax-optimal rates, and we
further establish optimal strong convergence in the reproducing kernel Hilbert
space. Our framework accommodates a broad class of classical loss functions,
including least-squares, Huber, and logistic losses. Moreover, the proposed
algorithm significantly reduces computational complexity and achieves optimal
storage complexity by incorporating coordinate-wise updates from linear SGD,
thereby avoiding the costly pairwise operations typical of kernel SGD and
enabling efficient processing of streaming data. Finally, extensive numerical
experiments demonstrate the efficiency of our approach.

</details>


### [333] [Diffusion-Assisted Distillation for Self-Supervised Graph Representation Learning with MLPs](https://arxiv.org/abs/2510.04241)
*Seong Jin Ahn,Myoung-Ho Kim*

Main category: cs.LG

TL;DR: 本文提出DAD - SGM方法，利用去噪扩散模型辅助将自监督图表示学习的GNN知识蒸馏到MLP，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 自监督图表示学习中，将GNN知识蒸馏到MLP更具挑战性，需设计新方法弥合二者能力差距。

Method: 提出DAD - SGM方法，使用去噪扩散模型作为助教，将教师GNN的知识更好地蒸馏到学生MLP。

Result: 实验表明，与现有GNN到MLP蒸馏方法相比，DAD - SGM能有效蒸馏自监督GNN的知识。

Conclusion: DAD - SGM方法提升了自监督图表示学习中MLP的泛化性和鲁棒性。

Abstract: For large-scale applications, there is growing interest in replacing Graph
Neural Networks (GNNs) with lightweight Multi-Layer Perceptrons (MLPs) via
knowledge distillation. However, distilling GNNs for self-supervised graph
representation learning into MLPs is more challenging. This is because the
performance of self-supervised learning is more related to the model's
inductive bias than supervised learning. This motivates us to design a new
distillation method to bridge a huge capacity gap between GNNs and MLPs in
self-supervised graph representation learning. In this paper, we propose
\textbf{D}iffusion-\textbf{A}ssisted \textbf{D}istillation for
\textbf{S}elf-supervised \textbf{G}raph representation learning with
\textbf{M}LPs (DAD-SGM). The proposed method employs a denoising diffusion
model as a teacher assistant to better distill the knowledge from the teacher
GNN into the student MLP. This approach enhances the generalizability and
robustness of MLPs in self-supervised graph representation learning. Extensive
experiments demonstrate that DAD-SGM effectively distills the knowledge of
self-supervised GNNs compared to state-of-the-art GNN-to-MLP distillation
methods. Our implementation is available at
https://github.com/SeongJinAhn/DAD-SGM.

</details>


### [334] [Efficient Latent Variable Causal Discovery: Combining Score Search and Targeted Testing](https://arxiv.org/abs/2510.04263)
*Joseph Ramsey,Bryan Andrews*

Main category: cs.LG

TL;DR: 提出一系列分数引导的混合策略因果搜索算法处理含潜在变量或选择偏差的因果结构学习问题，模拟和真实数据分析显示算法效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有Fast Causal Inference (FCI) 算法在处理含潜在变量或选择偏差的数据时存在条件独立性测试过多、结果不可靠等问题。

Method: 提出BOSS - FCI和GRaSP - FCI替代FGES；开发FCI Targeted - testing (FCIT) 用目标测试替代全子集测试；提出LV - Dumb启发式方法。

Result: BOSS - FCI和GRaSP - FCI提供可靠基线，FCIT提高效率和可靠性，LV - Dumb经验性能强。

Conclusion: 分数引导和目标策略对可扩展的潜在变量因果发现有价值。

Abstract: Learning causal structure from observational data is especially challenging
when latent variables or selection bias are present. The Fast Causal Inference
(FCI) algorithm addresses this setting but often performs exhaustive
conditional independence tests across many subsets, leading to spurious
independence claims, extra or missing edges, and unreliable orientations. We
present a family of score-guided mixed-strategy causal search algorithms that
build on this tradition. First, we introduce BOSS-FCI and GRaSP-FCI,
straightforward variants of GFCI that substitute BOSS or GRaSP for FGES,
thereby retaining correctness while incurring different scalability tradeoffs.
Second, we develop FCI Targeted-testing (FCIT), a novel mixed-strategy method
that improves upon these variants by replacing exhaustive all-subsets testing
with targeted tests guided by BOSS, yielding well-formed PAGs with higher
precision and efficiency. Finally, we propose a simple heuristic, LV-Dumb (also
known as BOSS-POD), which bypasses latent-variable-specific reasoning and
directly returns the PAG of the BOSS DAG. Although not strictly correct in the
FCI sense, it scales better and often achieves superior accuracy in practice.
Simulations and real-data analyses demonstrate that BOSS-FCI and GRaSP-FCI
provide sound baselines, FCIT improves both efficiency and reliability, and
LV-Dumb offers a practical heuristic with strong empirical performance.
Together, these method highlight the value of score-guided and targeted
strategies for scalable latent-variable causal discovery.

</details>


### [335] [Influence branching for learning to solve mixed-integer programs online](https://arxiv.org/abs/2510.04273)
*Paul Strang,Zacharie Alès,Côme Bissuel,Olivier Juan,Safia Kedad-Sidhoum,Emmanuel Rachelson*

Main category: cs.LG

TL;DR: 本文在第20届混合整数规划研讨会计算竞赛之际，介绍在线学习求解MIPs的新方法，取得与现有方法相当结果且泛化性好。


<details>
  <summary>Details</summary>
Motivation: 在20届混合整数规划研讨会计算竞赛背景下，寻求在线学习求解MIPs的新方法。

Method: 在分支限界算法前几次迭代应用影响分支这一基于图的变量选择策略，用汤普森采样在线优化该分支启发式，根据相对于SCIP的计算加速对MIP结构的最佳图表示进行排序。

Result: 取得了与现有在线学习方法相当的结果。

Conclusion: 该方法在更通用的在线框架中泛化性良好。

Abstract: On the occasion of the 20th Mixed Integer Program Workshop's computational
competition, this work introduces a new approach for learning to solve MIPs
online. Influence branching, a new graph-oriented variable selection strategy,
is applied throughout the first iterations of the branch and bound algorithm.
This branching heuristic is optimized online with Thompson sampling, which
ranks the best graph representations of MIP's structure according to
computational speed up over SCIP. We achieve results comparable to state of the
art online learning methods. Moreover, our results indicate that our method
generalizes well to more general online frameworks, where variations in
constraint matrix, constraint vector and objective coefficients can all occur
and where more samples are available.

</details>


### [336] [A KL-regularization framework for learning to plan with adaptive priors](https://arxiv.org/abs/2510.04280)
*Álvaro Serra-Gomez,Daniel Jarne Ornia,Dhruva Tirumala,Thomas Moerland*

Main category: cs.LG

TL;DR: 本文提出PO - MPC框架统一基于MPPI的强化学习方法，实验表明扩展配置提升了性能。


<details>
  <summary>Details</summary>
Motivation: 在基于模型的强化学习中，有效探索是高维连续控制任务的关键挑战，此前方法未充分使采样策略与规划器分布对齐，影响价值估计准确性和长期性能。

Method: 引入Policy Optimization - Model Predictive Control (PO - MPC)框架，将规划器的动作分布作为先验整合到策略优化中，通过KL正则化统一基于MPPI的强化学习方法。

Result: 实验显示PO - MPC的扩展配置带来显著性能提升。

Conclusion: PO - MPC框架统一了基于MPPI的强化学习方法，其扩展配置推动了基于MPPI的强化学习的发展。

Abstract: Effective exploration remains a central challenge in model-based
reinforcement learning (MBRL), particularly in high-dimensional continuous
control tasks where sample efficiency is crucial. A prominent line of recent
work leverages learned policies as proposal distributions for Model-Predictive
Path Integral (MPPI) planning. Initial approaches update the sampling policy
independently of the planner distribution, typically maximizing a learned value
function with deterministic policy gradient and entropy regularization.
However, because the states encountered during training depend on the MPPI
planner, aligning the sampling policy with the planner improves the accuracy of
value estimation and long-term performance. To this end, recent methods update
the sampling policy by minimizing KL divergence to the planner distribution or
by introducing planner-guided regularization into the policy update. In this
work, we unify these MPPI-based reinforcement learning methods under a single
framework by introducing Policy Optimization-Model Predictive Control (PO-MPC),
a family of KL-regularized MBRL methods that integrate the planner's action
distribution as a prior in policy optimization. By aligning the learned policy
with the planner's behavior, PO-MPC allows more flexibility in the policy
updates to trade off Return maximization and KL divergence minimization. We
clarify how prior approaches emerge as special cases of this family, and we
explore previously unstudied variations. Our experiments show that these
extended configurations yield significant performance improvements, advancing
the state of the art in MPPI-based RL.

</details>


### [337] [HoRA: Cross-Head Low-Rank Adaptation with Joint Hypernetworks](https://arxiv.org/abs/2510.04295)
*Nghiem T. Diep,Dung Le,Tuan Truong,Tan Dinh,Huy Nguyen,Nhat Ho*

Main category: cs.LG

TL;DR: 提出新的Hyper - shared Low - Rank Adaptation (HoRA)方法解决LoRA在微调多头自注意力时的问题，理论和实验表明HoRA优于LoRA等方法。


<details>
  <summary>Details</summary>
Motivation: LoRA在微调多头自注意力时分别调整每个注意力头，忽略了不同头之间的协同作用，需要改进。

Method: 提出HoRA方法，利用联合超网络为注意力头生成低秩矩阵，通过共享生成器促进头间信息共享。

Result: 理论上HoRA样本效率优于LoRA，实验上HoRA在多种语言和视觉基准测试中优于LoRA和其他PEFT方法，且可训练参数增加极少。

Conclusion: HoRA能有效解决LoRA的局限性，性能表现更优。

Abstract: Low-Rank Adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT)
technique that adapts large pre-trained models by adding low-rank matrices to
their weight updates. However, in the context of fine-tuning multi-head
self-attention (MHA), LoRA has been employed to adapt each attention head
separately, thereby overlooking potential synergies across different heads. To
mitigate this issue, we propose a novel Hyper-shared Low-Rank Adaptation (HoRA)
method, which utilizes joint hypernetworks to generate low-rank matrices across
attention heads. By coupling their adaptation through a shared generator, HoRA
encourages cross-head information sharing, and thus directly addresses the
aforementioned limitation of LoRA. By comparing LoRA and HoRA through the lens
of hierarchical mixture of experts, our theoretical findings reveal that the
latter achieves superior sample efficiency to the former. Furthermore, through
extensive experiments across diverse language and vision benchmarks, we
demonstrate that HoRA outperforms LoRA and other PEFT methods while requiring
only a marginal increase in the number of trainable parameters.

</details>


### [338] [Wave-PDE Nets: Trainable Wave-Equation Layers as an Alternative to Attention](https://arxiv.org/abs/2510.04304)
*Harshil Vejendla*

Main category: cs.LG

TL;DR: 提出Wave - PDE Nets，在语言和视觉基准上表现良好，计算高效且有物理归纳偏置。


<details>
  <summary>Details</summary>
Motivation: 寻找替代注意力和一阶状态空间模型的有效机制。

Method: 构建以二阶波动方程可微模拟为基本操作的神经网络架构，用基于FFT的辛谱求解器实现传播。

Result: 在语言和视觉基准上匹配或超越Transformer性能，减少30%运行时间和25%峰值内存，消融实验和可视化揭示关键因素和信息传播策略。

Conclusion: Wave - PDE Nets是计算高效且稳健、有强物理归纳偏置的架构。

Abstract: We introduce Wave-PDE Nets, a neural architecture whose elementary operation
is a differentiable simulation of the second-order wave equation. Each layer
propagates its hidden state as a continuous field through a medium with
trainable spatial velocity c(x) and damping {\gamma}(x). A symplectic spectral
solver based on FFTs realises this propagation in O(nlog n) time. This
oscillatory, global mechanism provides a powerful alternative to attention and
first-order state-space models. We prove that a single Wave-PDE layer is a
universal approximator. On language and vision benchmarks, Wave-PDE Nets match
or exceed Transformer performance while demonstrating superior practical
efficiency, reducing wall-clock time by up to 30% and peak memory by 25%.
Ablation studies confirm the critical role of symplectic integration and a
spectral Laplacian for stability and performance. Visualizations of the learned
physical parameters reveal that the model learns intuitive strategies for
information propagation. These results position Wave-PDE Nets as a
computationally efficient and robust architecture with a strong physical
inductive bias.

</details>


### [339] [Activation Steering with a Feedback Controller](https://arxiv.org/abs/2510.04309)
*Dung V. Nguyen,Hieu M. Vu,Nhi Y. Pham,Lei Zhang,Tan M. Nguyen*

Main category: cs.LG

TL;DR: 本文为大语言模型激活转向建立控制理论基础，提出PID Steering框架，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型转向方法缺乏理论性能保证，需要为激活转向建立控制理论基础。

Method: 揭示流行转向方法对应比例（P）控制器，提出使用完整PID控制器的PID Steering框架。

Result: PID Steering具有可解释的误差动态，能与经典控制理论稳定性保证相联系，且轻量级、模块化，易与现有方法集成。

Conclusion: 在多个大语言模型家族和基准测试中，PID Steering始终优于现有方法，能实现更稳健可靠的行为控制。

Abstract: Controlling the behaviors of large language models (LLM) is fundamental to
their safety alignment and reliable deployment. However, existing steering
methods are primarily driven by empirical insights and lack theoretical
performance guarantees. In this work, we develop a control-theoretic foundation
for activation steering by showing that popular steering methods correspond to
the proportional (P) controllers, with the steering vector serving as the
feedback signal. Building on this finding, we propose
Proportional-Integral-Derivative (PID) Steering, a principled framework that
leverages the full PID controller for activation steering in LLMs. The
proportional (P) term aligns activations with target semantic directions, the
integral (I) term accumulates errors to enforce persistent corrections across
layers, and the derivative (D) term mitigates overshoot by counteracting rapid
activation changes. This closed-loop design yields interpretable error dynamics
and connects activation steering to classical stability guarantees in control
theory. Moreover, PID Steering is lightweight, modular, and readily integrates
with state-of-the-art steering methods. Extensive experiments across multiple
LLM families and benchmarks demonstrate that PID Steering consistently
outperforms existing approaches, achieving more robust and reliable behavioral
control.

</details>


### [340] [Crash Severity Prediction Using Deep Learning Approaches: A Hybrid CNN-RNN Framework](https://arxiv.org/abs/2510.04316)
*Sahar Koohfar*

Main category: cs.LG

TL;DR: 本文提出混合 CNN - RNN 深度学习模型预测事故严重程度，在弗吉尼亚州 I - 64 高速公路数据集上表现优于其他基准模型。


<details>
  <summary>Details</summary>
Motivation: 准确及时预测事故严重程度对减轻交通事故后果至关重要，智能交通系统需要有效预测方法。

Method: 实现混合 CNN - RNN 深度学习模型，与逻辑回归、朴素贝叶斯分类器等统计和机器学习模型及单独的 RNN、CNN 模型对比，采用考虑事故特征间相互关系的方法，使用 2015 - 2021 年弗吉尼亚州 I - 64 高速公路 15870 条事故记录数据集。

Result: 提出的 CNN - RNN 混合模型在预测事故严重程度方面优于所有基准模型。

Conclusion: 混合模型结合 RNN 和 CNN 优点，在预测过程中能实现更高准确性，证明了其有效性。

Abstract: Accurate and timely prediction of crash severity is crucial in mitigating the
severe consequences of traffic accidents. Accurate and timely prediction of
crash severity is crucial in mitigating the severe consequences of traffic
accidents. In order to provide appropriate levels of medical assistance and
transportation services, an intelligent transportation system relies on
effective prediction methods. Deep learning models have gained popularity in
this domain due to their capability to capture non-linear relationships among
variables. In this research, we have implemented a hybrid CNN-RNN deep learning
model for crash severity prediction and compared its performance against widely
used statistical and machine learning models such as logistic regression,
na\"ive bayes classifier, K-Nearest Neighbors (KNN), decision tree, and
individual deep learning models: RNN and CNN. This study employs a methodology
that considers the interconnected relationships between various features of
traffic accidents. The study was conducted using a dataset of 15,870 accident
records gathered over a period of seven years between 2015 and 2021 on Virginia
highway I-64. The findings demonstrate that the proposed CNN-RNN hybrid model
has outperformed all benchmark models in terms of predicting crash severity.
This result illustrates the effectiveness of the hybrid model as it combines
the advantages of both RNN and CNN models in order to achieve greater accuracy
in the prediction process.

</details>


### [341] [FairAgent: Democratizing Fairness-Aware Machine Learning with LLM-Powered Agents](https://arxiv.org/abs/2510.04317)
*Yucong Dai,Lu Zhang,Feng Luo,Mashrur Chowdhury,Yongkai Wu*

Main category: cs.LG

TL;DR: 介绍了LLM驱动的自动化系统FairAgent，可简化公平感知模型开发，实验表明其提升性能、减少开发时间和专业知识需求。


<details>
  <summary>Details</summary>
Motivation: 训练公平无偏的机器学习模型有挑战，复杂流程使公平感知模型开发对很多从业者难以企及，需解决这些问题。

Method: 引入LLM驱动的自动化系统FairAgent，自动分析数据集潜在偏差、处理数据预处理和特征工程、根据用户需求实施偏差缓解策略。

Result: FairAgent实现显著性能提升，大幅减少开发时间和专业知识需求。

Conclusion: FairAgent让公平感知机器学习对从业者更易获取。

Abstract: Training fair and unbiased machine learning models is crucial for high-stakes
applications, yet it presents significant challenges. Effective bias mitigation
requires deep expertise in fairness definitions, metrics, data preprocessing,
and machine learning techniques. In addition, the complex process of balancing
model performance with fairness requirements while properly handling sensitive
attributes makes fairness-aware model development inaccessible to many
practitioners. To address these challenges, we introduce FairAgent, an
LLM-powered automated system that significantly simplifies fairness-aware model
development. FairAgent eliminates the need for deep technical expertise by
automatically analyzing datasets for potential biases, handling data
preprocessing and feature engineering, and implementing appropriate bias
mitigation strategies based on user requirements. Our experiments demonstrate
that FairAgent achieves significant performance improvements while
significantly reducing development time and expertise requirements, making
fairness-aware machine learning more accessible to practitioners.

</details>


### [342] [FoilDiff: A Hybrid Transformer Backbone for Diffusion-based Modelling of 2D Airfoil Flow Fields](https://arxiv.org/abs/2510.04325)
*Kenechukwu Ogbuagu,Sepehr Maleki,Giuseppe Bruni,Senthil Krishnababu*

Main category: cs.LG

TL;DR: 提出基于扩散的代理模型FoilDiff预测翼型流场，性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: CFD模型计算成本高，需开发代理模型快速预测翼型流场。

Method: 提出FoilDiff，采用混合骨干去噪网络，结合卷积特征提取和基于变压器的全局注意力，利用DDIM采样优化效率，用编码表示定义输入空间。

Result: 与现有模型对比，FoilDiff性能显著提升，相同数据集上平均预测误差最多降低85%。

Conclusion: FoilDiff预测更准确，预测不确定性校准更好。

Abstract: The accurate prediction of flow fields around airfoils is crucial for
aerodynamic design and optimisation. Computational Fluid Dynamics (CFD) models
are effective but computationally expensive, thus inspiring the development of
surrogate models to enable quicker predictions. These surrogate models can be
based on deep learning architectures, such as Convolutional Neural Networks
(CNNs), Graph Neural Networks (GNNs), and Diffusion Models (DMs). Diffusion
models have shown significant promise in predicting complex flow fields. In
this work, we propose FoilDiff, a diffusion-based surrogate model with a
hybrid-backbone denoising network. This hybrid design combines the power of
convolutional feature extraction and transformer-based global attention to
generate more adaptable and accurate representations of flow structures.
FoilDiff takes advantage of Denoising Diffusion Implicit Model (DDIM) sampling
to optimise the efficiency of the sampling process at no additional cost to
model generalisation. We used encoded representations of Reynolds number, angle
of attack, and airfoil geometry to define the input space for generalisation
across a wide range of aerodynamic conditions. When evaluated against
state-of-the-art models, FoilDiff shows significant performance improvements,
with mean prediction errors reducing by up to 85\% on the same datasets. The
results have demonstrated that FoilDiff can provide both more accurate
predictions and better-calibrated predictive uncertainty than existing
diffusion-based models.

</details>


### [343] [DoRAN: Stabilizing Weight-Decomposed Low-Rank Adaptation via Noise Injection and Auxiliary Networks](https://arxiv.org/abs/2510.04331)
*Nghiem T. Diep,Hien Dang,Tuan Truong,Tan Dinh,Huy Nguyen,Nhat Ho*

Main category: cs.LG

TL;DR: 提出DoRAN改进DoRA，稳定训练并提高样本效率，实验表明其优于多个基线。


<details>
  <summary>Details</summary>
Motivation: 进一步稳定DoRA的训练并提高其样本效率。

Method: 包括两个关键阶段：向DoRA权重分解的分母注入噪声作为自适应正则化器；用辅助网络动态生成低秩矩阵。

Result: 在视觉和语言基准测试中，DoRAN始终优于LoRA、DoRA和其他PEFT基线。

Conclusion: 结合基于噪声的正则化稳定和基于网络的参数生成是基础模型鲁棒高效微调的有前景方向。

Abstract: Parameter-efficient fine-tuning (PEFT) methods have become the standard
paradigm for adapting large-scale models. Among these techniques,
Weight-Decomposed Low-Rank Adaptation (DoRA) has been shown to improve both the
learning capacity and training stability of the vanilla Low-Rank Adaptation
(LoRA) method by explicitly decomposing pre-trained weights into magnitude and
directional components. In this work, we propose DoRAN, a new variant of DoRA
designed to further stabilize training and boost the sample efficiency of DoRA.
Our approach includes two key stages: (i) injecting noise into the denominator
of DoRA's weight decomposition, which serves as an adaptive regularizer to
mitigate instabilities; and (ii) replacing static low-rank matrices with
auxiliary networks that generate them dynamically, enabling parameter coupling
across layers and yielding better sample efficiency in both theory and
practice. Comprehensive experiments on vision and language benchmarks show that
DoRAN consistently outperforms LoRA, DoRA, and other PEFT baselines. These
results underscore the effectiveness of combining stabilization through
noise-based regularization with network-based parameter generation, offering a
promising direction for robust and efficient fine-tuning of foundation models.

</details>


### [344] [Critical appraisal of artificial intelligence for rare-event recognition: principles and pharmacovigilance case studies](https://arxiv.org/abs/2510.04341)
*G. Niklas Noren,Eva-Lisa Meldau,Johan Ellenius*

Main category: cs.LG

TL;DR: 本文概述AI在罕见事件识别中的关键考量，提出结构化案例级检查方法和清单，并结合药物警戒实例说明，原则可推广到其他领域。


<details>
  <summary>Details</summary>
Motivation: 许多高风险AI应用针对低流行率事件，表面准确性可能掩盖有限的实际价值，需要对AI在罕见事件识别中的应用进行批判性评估。

Method: 提出结构化案例级检查（SCLE）方法，制定全面清单；结合药物警戒的三个研究实例进行说明。

Result: 突出了罕见事件场景下的陷阱，如不切实际的类别平衡带来的乐观性和测试集中缺乏困难阳性对照；表明成本敏感目标可使模型性能与运营价值保持一致。

Conclusion: 虽然基于药物警戒实践，但相关原则可推广到阳性样本稀缺且错误成本可能不对称的领域。

Abstract: Many high-stakes AI applications target low-prevalence events, where apparent
accuracy can conceal limited real-world value. Relevant AI models range from
expert-defined rules and traditional machine learning to generative LLMs
constrained for classification. We outline key considerations for critical
appraisal of AI in rare-event recognition, including problem framing and test
set design, prevalence-aware statistical evaluation, robustness assessment, and
integration into human workflows. In addition, we propose an approach to
structured case-level examination (SCLE), to complement statistical performance
evaluation, and a comprehensive checklist to guide procurement or development
of AI models for rare-event recognition. We instantiate the framework in
pharmacovigilance, drawing on three studies: rule-based retrieval of
pregnancy-related reports; duplicate detection combining machine learning with
probabilistic record linkage; and automated redaction of person names using an
LLM. We highlight pitfalls specific to the rare-event setting including
optimism from unrealistic class balance and lack of difficult positive controls
in test sets - and show how cost-sensitive targets align model performance with
operational value. While grounded in pharmacovigilance practice, the principles
generalize to domains where positives are scarce and error costs may be
asymmetric.

</details>


### [345] [Learning to Predict Chaos: Curriculum-Driven Training for Robust Forecasting of Chaotic Dynamics](https://arxiv.org/abs/2510.04342)
*Harshil Vejendla*

Main category: cs.LG

TL;DR: 提出课程混沌预测（CCF）训练范式，按动力系统理论组织数据，从简单到复杂训练，实验表明预训练提升对未见真实基准的预测性能。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习方法在预测混沌系统时存在过度专业化或数据混杂问题，缺乏泛化性。

Method: 提出CCF范式，依据动力系统理论组织训练数据，用最大Lyapunov指数和吸引子维度量化复杂度，先在可预测系统训练，再引入更混沌轨迹，构建含超50个合成ODE/PDE系统的库。

Result: 在太阳黑子数、电力需求和人体ECG信号等数据集上，CCF比随机顺序训练延长有效预测期达40%，比仅在真实数据上训练翻倍。

Conclusion: CCF预训练显著提升模型性能，且在不同神经架构上效果一致，课程结构很重要。

Abstract: Forecasting chaotic systems is a cornerstone challenge in many scientific
fields, complicated by the exponential amplification of even infinitesimal
prediction errors. Modern machine learning approaches often falter due to two
opposing pitfalls: over-specializing on a single, well-known chaotic system
(e.g., Lorenz-63), which limits generalizability, or indiscriminately mixing
vast, unrelated time-series, which prevents the model from learning the nuances
of any specific dynamical regime. We propose Curriculum Chaos Forecasting
(CCF), a training paradigm that bridges this gap. CCF organizes training data
based on fundamental principles of dynamical systems theory, creating a
curriculum that progresses from simple, periodic behaviors to highly complex,
chaotic dynamics. We quantify complexity using the largest Lyapunov exponent
and attractor dimension, two well-established metrics of chaos. By first
training a sequence model on predictable systems and gradually introducing more
chaotic trajectories, CCF enables the model to build a robust and generalizable
representation of dynamical behaviors. We curate a library of over 50 synthetic
ODE/PDE systems to build this curriculum. Our experiments show that
pre-training with CCF significantly enhances performance on unseen, real-world
benchmarks. On datasets including Sunspot numbers, electricity demand, and
human ECG signals, CCF extends the valid prediction horizon by up to 40%
compared to random-order training and more than doubles it compared to training
on real-world data alone. We demonstrate that this benefit is consistent across
various neural architectures (GRU, Transformer) and provide extensive ablations
to validate the importance of the curriculum's structure.

</details>


### [346] [Quantifying Ambiguity in Categorical Annotations: A Measure and Statistical Inference Framework](https://arxiv.org/abs/2510.04366)
*Christopher Klugmann,Daniel Kondermann*

Main category: cs.LG

TL;DR: 提出一种新的模糊度度量方法，分析其性质、开发统计推断工具，并通过数值示例展示其应用。


<details>
  <summary>Details</summary>
Motivation: 人类生成的分类注释常产生反映模糊性的经验响应分布，现有指标无法很好区分不同类型的不确定性，需新的模糊度度量方法。

Method: 引入新的模糊度度量方法，分析其形式性质，与文献中的代表模糊度度量对比，开发基于频率主义的点估计器和贝叶斯后验推断工具。

Result: 新的模糊度度量方法能分离不同类型的不确定性，数值示例展示了其在估计、校准、数据集质量评估和机器学习工作流中的应用。

Conclusion: 新的模糊度度量方法和统计推断工具为处理分类任务中的不确定性提供了有效手段。

Abstract: Human-generated categorical annotations frequently produce empirical response
distributions (soft labels) that reflect ambiguity rather than simple annotator
error. We introduce an ambiguity measure that maps a discrete response
distribution to a scalar in the unit interval, designed to quantify aleatoric
uncertainty in categorical tasks. The measure bears a close relationship to
quadratic entropy (Gini-style impurity) but departs from those indices by
treating an explicit "can't solve" category asymmetrically, thereby separating
uncertainty arising from class-level indistinguishability from uncertainty due
to explicit unresolvability. We analyze the measure's formal properties and
contrast its behavior with a representative ambiguity measure from the
literature. Moving beyond description, we develop statistical tools for
inference: we propose frequentist point estimators for population ambiguity and
derive the Bayesian posterior over ambiguity induced by Dirichlet priors on the
underlying probability vector, providing a principled account of epistemic
uncertainty. Numerical examples illustrate estimation, calibration, and
practical use for dataset-quality assessment and downstream machine-learning
workflows.

</details>


### [347] [GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks](https://arxiv.org/abs/2510.04374)
*Tejal Patwardhan,Rachel Dias,Elizabeth Proehl,Grace Kim,Michele Wang,Olivia Watkins,Simón Posada Fishman,Marwan Aljubeh,Phoebe Thacker,Laurance Fauconnet,Natalie S. Kim,Patrick Chao,Samuel Miserendino,Gildas Chabot,David Li,Michael Sharman,Alexandra Barr,Amelia Glaese,Jerry Tworek*

Main category: cs.LG

TL;DR: 介绍GDPval基准评估AI模型在现实经济有价值任务上的能力，分析模型表现并开源部分任务及提供评分服务。


<details>
  <summary>Details</summary>
Motivation: 评估AI模型在现实经济有价值任务上的能力，理解其现实应用能力。

Method: 构建覆盖美国多个行业工作活动的GDPval基准，任务由有经验的行业专家代表工作构成。

Result: 前沿模型在GDPval上的表现随时间近似线性提升，当前最佳模型的交付质量接近行业专家；分析得出有人类监督时前沿模型完成任务比专家更便宜快速；增加推理、任务上下文和脚手架可提升模型表现。

Conclusion: 开源部分任务及提供自动评分服务，以促进对现实模型能力的研究。

Abstract: We introduce GDPval, a benchmark evaluating AI model capabilities on
real-world economically valuable tasks. GDPval covers the majority of U.S.
Bureau of Labor Statistics Work Activities for 44 occupations across the top 9
sectors contributing to U.S. GDP (Gross Domestic Product). Tasks are
constructed from the representative work of industry professionals with an
average of 14 years of experience. We find that frontier model performance on
GDPval is improving roughly linearly over time, and that the current best
frontier models are approaching industry experts in deliverable quality. We
analyze the potential for frontier models, when paired with human oversight, to
perform GDPval tasks cheaper and faster than unaided experts. We also
demonstrate that increased reasoning effort, increased task context, and
increased scaffolding improves model performance on GDPval. Finally, we
open-source a gold subset of 220 tasks and provide a public automated grading
service at evals.openai.com to facilitate future research in understanding
real-world model capabilities.

</details>


### [348] [Adaptive Weighted Loss for Sequential Recommendations on Sparse Domains](https://arxiv.org/abs/2510.04375)
*Akshay Mittal,Vinay Venkatesh,Krishna Kandi,Shalini Sudarshan*

Main category: cs.LG

TL;DR: 单模型顺序推荐架构在服务特定用户时效果有限，本文提出动态加权损失函数，经理论分析和实证验证显著优于对比方法。


<details>
  <summary>Details</summary>
Motivation: 单模型顺序推荐架构在稀疏或小众领域服务‘权力用户’时效果受限，之前固定加权损失方法存在不足。

Method: 提出动态加权损失函数，根据训练数据中各领域的稀疏性调整损失权重。

Result: 通过四个不同数据集和多个基线模型验证，动态加权系统显著优于对比方法，尤其在稀疏领域提升关键指标，在密集领域保持性能且计算开销小。

Conclusion: 动态加权损失函数有效解决了单模型顺序推荐架构的问题，具有良好性能和效率。

Abstract: The effectiveness of single-model sequential recommendation architectures,
while scalable, is often limited when catering to "power users" in sparse or
niche domains. Our previous research, PinnerFormerLite, addressed this by using
a fixed weighted loss to prioritize specific domains. However, this approach
can be sub-optimal, as a single, uniform weight may not be sufficient for
domains with very few interactions, where the training signal is easily diluted
by the vast, generic dataset.
  This paper proposes a novel, data-driven approach: a Dynamic Weighted Loss
function with comprehensive theoretical foundations and extensive empirical
validation. We introduce an adaptive algorithm that adjusts the loss weight for
each domain based on its sparsity in the training data, assigning a higher
weight to sparser domains and a lower weight to denser ones. This ensures that
even rare user interests contribute a meaningful gradient signal, preventing
them from being overshadowed.
  We provide rigorous theoretical analysis including convergence proofs,
complexity analysis, and bounds analysis to establish the stability and
efficiency of our approach. Our comprehensive empirical validation across four
diverse datasets (MovieLens, Amazon Electronics, Yelp Business, LastFM Music)
with state-of-the-art baselines (SIGMA, CALRec, SparseEnNet) demonstrates that
this dynamic weighting system significantly outperforms all comparison methods,
particularly for sparse domains, achieving substantial lifts in key metrics
like Recall at 10 and NDCG at 10 while maintaining performance on denser
domains and introducing minimal computational overhead.

</details>


### [349] [Categorical Invariants of Learning Dynamics](https://arxiv.org/abs/2510.04376)
*Abdulrahman Tamim*

Main category: cs.LG

TL;DR: 提出学习是网络参数空间与学习表示空间间的结构保留变换，揭示训练路径同伦类与泛化性的关系并给出实用工具。


<details>
  <summary>Details</summary>
Motivation: 从新视角理解神经网络训练，深入探究深度学习原理及提高网络鲁棒性。

Method: 提出学习是结构保留变换的范畴框架，进行实验验证，并利用持久同调、拉回构造和2 - 范畴结构等方法。

Result: 同伦轨迹收敛的网络泛化精度差异在0.5%内，非同伦路径差异超3%；持久同调与泛化性相关性R^2 = 0.82。

Conclusion: 范畴不变量为深度学习原理提供理论见解，为训练更鲁棒网络提供算法原则。

Abstract: Neural network training is typically viewed as gradient descent on a loss
surface. We propose a fundamentally different perspective: learning is a
structure-preserving transformation (a functor L) between the space of network
parameters (Param) and the space of learned representations (Rep). This
categorical framework reveals that different training runs producing similar
test performance often belong to the same homotopy class (continuous
deformation family) of optimization paths. We show experimentally that networks
converging via homotopic trajectories generalize within 0.5% accuracy of each
other, while non-homotopic paths differ by over 3%. The theory provides
practical tools: persistent homology identifies stable minima predictive of
generalization (R^2 = 0.82 correlation), pullback constructions formalize
transfer learning, and 2-categorical structures explain when different
optimization algorithms yield functionally equivalent models. These categorical
invariants offer both theoretical insight into why deep learning works and
concrete algorithmic principles for training more robust networks.

</details>


### [350] [Score-based Greedy Search for Structure Identification of Partially Observed Linear Causal Models](https://arxiv.org/abs/2510.04378)
*Xinshuai Dong,Ignavier Ng,Haoyue Dai,Jiaqi Sun,Xiangchen Song,Peter Spirtes,Kun Zhang*

Main category: cs.LG

TL;DR: 本文提出首个基于分数的贪婪搜索方法来识别含潜在变量的因果系统结构，实验验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于约束的因果发现方法在处理部分观测场景时面临多重测试和误差传播问题，需基于分数的贪婪搜索方法。

Method: 提出广义N因子模型并建立全局一致性，设计潜在变量贪婪等价搜索算法（LGES）在图空间高效搜索最优结构。

Result: 在合成数据和真实数据实验中验证了方法的有效性，代码将公开。

Conclusion: 所提基于分数的贪婪搜索方法能有效识别含潜在变量的因果系统结构。

Abstract: Identifying the structure of a partially observed causal system is essential
to various scientific fields. Recent advances have focused on constraint-based
causal discovery to solve this problem, and yet in practice these methods often
face challenges related to multiple testing and error propagation. These issues
could be mitigated by a score-based method and thus it has raised great
attention whether there exists a score-based greedy search method that can
handle the partially observed scenario. In this work, we propose the first
score-based greedy search method for the identification of structure involving
latent variables with identifiability guarantees. Specifically, we propose
Generalized N Factor Model and establish the global consistency:
  the true structure including latent variables can be identified up to the
Markov equivalence class by using score. We then design
  Latent variable Greedy Equivalence Search (LGES), a greedy search algorithm
for this class of model with well-defined operators,
  which search very efficiently over the graph space to find the optimal
structure. Our experiments on both synthetic and real-life data validate the
effectiveness of our method (code will be publicly available).

</details>


### [351] [SSM-CGM: Interpretable State-Space Forecasting Model of Continuous Glucose Monitoring for Personalized Diabetes Management](https://arxiv.org/abs/2510.04386)
*Shakson Isaac,Yentl Collin,Chirag Patel*

Main category: cs.LG

TL;DR: 提出基于Mamba的SSM - CGM预测模型用于糖尿病管理，比基线模型更优且具可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的连续血糖监测（CGM）预测模型缺乏临床应用所需的可解释性。

Method: 提出基于Mamba的神经状态空间预测模型SSM - CGM，整合CGM和可穿戴活动信号。

Result: SSM - CGM在短期预测准确性上优于Temporal Fusion Transformer基线模型，通过变量选择和时间归因增加可解释性，能进行反事实预测。

Conclusion: SSM - CGM是个性化糖尿病管理的可解释、基于生理的框架。

Abstract: Continuous glucose monitoring (CGM) generates dense data streams critical for
diabetes management, but most used forecasting models lack interpretability for
clinical use. We present SSM-CGM, a Mamba-based neural state-space forecasting
model that integrates CGM and wearable activity signals from the AI-READI
cohort. SSM-CGM improves short-term accuracy over a Temporal Fusion Transformer
baseline, adds interpretability through variable selection and temporal
attribution, and enables counterfactual forecasts simulating how planned
changes in physiological signals (e.g., heart rate, respiration) affect
near-term glucose. Together, these features make SSM-CGM an interpretable,
physiologically grounded framework for personalized diabetes management.

</details>


### [352] [Partial Information Decomposition via Normalizing Flows in Latent Gaussian Distributions](https://arxiv.org/abs/2510.04417)
*Wenyuan Zhao,Adithya Balachandran,Chao Tian,Paul Pu Liang*

Main category: cs.LG

TL;DR: 本文提出新算法提升高斯部分信息分解（GPID）计算效率，还将其拓展到非高斯数据，经实验验证比现有方法更准确高效。


<details>
  <summary>Details</summary>
Motivation: 现有部分信息分解（PID）方法在处理连续和高维模态时成本高且不准确，需改进。

Method: 提出基于梯度的新算法提升GPID计算效率；学习信息保留编码器将非高斯数据转换为高斯数据；解决GPID联合高斯解的最优性问题。

Result: 在不同合成示例中，所提方法比现有基线提供更准确和高效的PID估计；在大规模多模态基准测试中展示其在实际应用中的效用。

Conclusion: 所提方法能有效解决现有PID方法的问题，在多模态数据集的PID量化和模型选择中有实际应用价值。

Abstract: The study of multimodality has garnered significant interest in fields where
the analysis of interactions among multiple information sources can enhance
predictive modeling, data fusion, and interpretability. Partial information
decomposition (PID) has emerged as a useful information-theoretic framework to
quantify the degree to which individual modalities independently, redundantly,
or synergistically convey information about a target variable. However,
existing PID methods depend on optimizing over a joint distribution constrained
by estimated pairwise probability distributions, which are costly and
inaccurate for continuous and high-dimensional modalities. Our first key
insight is that the problem can be solved efficiently when the pairwise
distributions are multivariate Gaussians, and we refer to this problem as
Gaussian PID (GPID). We propose a new gradient-based algorithm that
substantially improves the computational efficiency of GPID based on an
alternative formulation of the underlying optimization problem. To generalize
the applicability to non-Gaussian data, we learn information-preserving
encoders to transform random variables of arbitrary input distributions into
pairwise Gaussian random variables. Along the way, we resolved an open problem
regarding the optimality of joint Gaussian solutions for GPID. Empirical
validation in diverse synthetic examples demonstrates that our proposed method
provides more accurate and efficient PID estimates than existing baselines. We
further evaluate a series of large-scale multimodal benchmarks to show its
utility in real-world applications of quantifying PID in multimodal datasets
and selecting high-performing models.

</details>


### [353] [Achieve Performatively Optimal Policy for Performative Reinforcement Learning](https://arxiv.org/abs/2510.04430)
*Ziyi Chen,Heng Huang*

Main category: cs.LG

TL;DR: 提出零阶Frank - Wolfe算法（0 - FW）以解决执行性强化学习问题，实现多项式时间收敛到执行性最优（PO）策略，实验表明该算法更有效。


<details>
  <summary>Details</summary>
Motivation: 现有执行性强化学习工作仅针对执行性稳定（PS）策略，其与执行性最优（PO）策略存在差距，需寻找PO策略。

Method: 在Frank - Wolfe框架下使用零阶近似的执行性策略梯度提出0 - FW算法，并对非凸值函数进行收敛性分析。

Result: 在标准正则化主导条件下实现了首次多项式时间收敛到PO策略，实验显示0 - FW算法比现有算法更能有效找到PO策略。

Conclusion: 所提出的0 - FW算法在解决执行性强化学习问题、寻找PO策略方面表现出色，优于现有算法。

Abstract: Performative reinforcement learning is an emerging dynamical decision making
framework, which extends reinforcement learning to the common applications
where the agent's policy can change the environmental dynamics. Existing works
on performative reinforcement learning only aim at a performatively stable (PS)
policy that maximizes an approximate value function. However, there is a
provably positive constant gap between the PS policy and the desired
performatively optimal (PO) policy that maximizes the original value function.
In contrast, this work proposes a zeroth-order Frank-Wolfe algorithm (0-FW)
algorithm with a zeroth-order approximation of the performative policy gradient
in the Frank-Wolfe framework, and obtains \textbf{the first polynomial-time
convergence to the desired PO} policy under the standard regularizer dominance
condition. For the convergence analysis, we prove two important properties of
the nonconvex value function. First, when the policy regularizer dominates the
environmental shift, the value function satisfies a certain gradient dominance
property, so that any stationary point (not PS) of the value function is a
desired PO. Second, though the value function has unbounded gradient, we prove
that all the sufficiently stationary points lie in a convex and compact policy
subspace $\Pi_{\Delta}$, where the policy value has a constant lower bound
$\Delta>0$ and thus the gradient becomes bounded and Lipschitz continuous.
Experimental results also demonstrate that our 0-FW algorithm is more effective
than the existing algorithms in finding the desired PO policy.

</details>


### [354] [Trade-off in Estimating the Number of Byzantine Clients in Federated Learning](https://arxiv.org/abs/2510.04432)
*Ziyi Chen,Su Zhang,Heng Huang*

Main category: cs.LG

TL;DR: 本文理论分析聚合器在不同拜占庭客户端数量估计下的最坏情况误差及联邦学习算法表现，揭示了聚合器鲁棒性与性能间的权衡。


<details>
  <summary>Details</summary>
Motivation: 当前对聚合器中拜占庭客户端数量估计对性能的影响缺乏系统研究，本文旨在填补该空白。

Method: 理论分析聚合器在任意估计数量和实际数量情况下的最坏情况误差，以及由此引发的联邦学习算法性能。

Result: 低估拜占庭客户端数量会导致聚合器和联邦学习性能任意差；非低估情况下，证明了聚合器和联邦学习误差的最优上下界，且与估计数量相关。

Conclusion: 聚合器鲁棒性程度越高，能解决的问题范围越广，但在实际拜占庭客户端较少时性能会恶化，存在基本权衡。

Abstract: Federated learning has attracted increasing attention at recent large-scale
optimization and machine learning research and applications, but is also
vulnerable to Byzantine clients that can send any erroneous signals. Robust
aggregators are commonly used to resist Byzantine clients. This usually
requires to estimate the unknown number $f$ of Byzantine clients, and thus
accordingly select the aggregators with proper degree of robustness (i.e., the
maximum number $\hat{f}$ of Byzantine clients allowed by the aggregator). Such
an estimation should have important effect on the performance, which has not
been systematically studied to our knowledge. This work will fill in the gap by
theoretically analyzing the worst-case error of aggregators as well as its
induced federated learning algorithm for any cases of $\hat{f}$ and $f$.
Specifically, we will show that underestimation ($\hat{f}<f$) can lead to
arbitrarily poor performance for both aggregators and federated learning. For
non-underestimation ($\hat{f}\ge f$), we have proved optimal lower and upper
bounds of the same order on the errors of both aggregators and federated
learning. All these optimal bounds are proportional to $\hat{f}/(n-f-\hat{f})$
with $n$ clients, which monotonically increases with larger $\hat{f}$. This
indicates a fundamental trade-off: while an aggregator with a larger robustness
degree $\hat{f}$ can solve federated learning problems of wider range $f\in
[0,\hat{f}]$, the performance can deteriorate when there are actually fewer or
even no Byzantine clients (i.e., $f\in [0,\hat{f})$).

</details>


### [355] [Fractional Heat Kernel for Semi-Supervised Graph Learning with Small Training Sample Size](https://arxiv.org/abs/2510.04440)
*Farid Bozorgnia,Vyacheslav Kungurtsev,Shirali Kadyrov,Mohsen Yousefnezhad*

Main category: cs.LG

TL;DR: 本文引入基于带源项分数热核动力学的标签传播和自训练算法，集成到图神经网络增强其表达能力，在小标签数据集上效果好并在标准数据集验证。


<details>
  <summary>Details</summary>
Motivation: 通过信息论与抛物演化方程物理的经典对应关系来推动该方法，解决小标签训练样本情况下的学习问题。

Method: 将分数热核集成到图神经网络架构，使用切比雪夫多项式近似处理大图，通过变分公式扩展经典扩散模型。

Result: 该方法在标准数据集上证明了有效性。

Conclusion: 在少量标记训练样本的情况下，已知标签的监督和图上的扩散之间的平衡具有优势。

Abstract: In this work, we introduce novel algorithms for label propagation and
self-training using fractional heat kernel dynamics with a source term. We
motivate the methodology through the classical correspondence of information
theory with the physics of parabolic evolution equations. We integrate the
fractional heat kernel into Graph Neural Network architectures such as Graph
Convolutional Networks and Graph Attention, enhancing their expressiveness
through adaptive, multi-hop diffusion. By applying Chebyshev polynomial
approximations, large graphs become computationally feasible. Motivating
variational formulations demonstrate that by extending the classical diffusion
model to fractional powers of the Laplacian, nonlocal interactions deliver more
globally diffusing labels. The particular balance between supervision of known
labels and diffusion across the graph is particularly advantageous in the case
where only a small number of labeled training examples are present. We
demonstrate the effectiveness of this approach on standard datasets.

</details>


### [356] [Forking-Sequences](https://arxiv.org/abs/2510.04487)
*Willa Potosnak,Malcolm Wolff,Boris Oreshkin,Mengfei Cao,Michael W. Mahoney,Dmitry Efimov,Kin G. Olivares*

Main category: cs.LG

TL;DR: 文章强调时间序列预测稳定性重要性，介绍forking - sequences方法，阐述其优势并通过多数据集验证效果。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测模型虽准确但预测稳定性常被忽视，为提高预测稳定性，推广较少人知的forking - sequences方法。

Method: 形式化forking - sequences方法，该方法联合编码和解码全时间序列，类似时间序列交叉验证。

Result: 使用16个数据集验证，MLP、RNN、LSTM、CNN和Transformer架构预测百分比变化稳定性平均提升28.8%、28.8%、37.9%、31.3%和8.8%。

Conclusion: forking - sequences方法有训练梯度更新更稳定、减少预测方差和提高推理计算效率等优势，值得更广泛采用。

Abstract: While accuracy is a critical requirement for time series forecasting models,
an equally important (yet often overlooked) desideratum is forecast stability
across forecast creation dates (FCDs). Even highly accurate models can produce
erratic revisions between FCDs, undermining stakeholder trust and disrupting
downstream decision-making. To improve forecast stability, models like MQCNN,
MQT, and SPADE employ a little-known but highly effective technique:
forking-sequences. Unlike standard statistical and neural forecasting methods
that treat each FCD independently, the forking-sequences method jointly encodes
and decodes the entire time series across all FCDs, in a way mirroring time
series cross-validation. Since forking sequences remains largely unknown in the
broader neural forecasting community, in this work, we formalize the
forking-sequences approach, and we make a case for its broader adoption. We
demonstrate three key benefits of forking-sequences: (i) more stable and
consistent gradient updates during training; (ii) reduced forecast variance
through ensembling; and (iii) improved inference computational efficiency. We
validate forking-sequences' benefits using 16 datasets from the M1, M3, M4, and
Tourism competitions, showing improvements in forecast percentage change
stability of 28.8%, 28.8%, 37.9%, and 31.3%, and 8.8%, on average, for MLP,
RNN, LSTM, CNN, and Transformer-based architectures, respectively.

</details>


### [357] [Expand Neurons, Not Parameters](https://arxiv.org/abs/2510.04500)
*Linghao Kong,Inimai Subramanian,Yonadav Shavit,Micah Adler,Dan Alistarh,Nir Shavit*

Main category: cs.LG

TL;DR: 本文展示不增加非零参数数量，增加网络神经元数量可提升性能，提出FPE方法，在符号和实际任务中验证有效。


<details>
  <summary>Details</summary>
Motivation: 探索在不增加非零参数数量的情况下提升网络性能的方法，减少特征间干扰。

Method: 引入Fixed Parameter Expansion (FPE)方法，用多个子神经元替换一个神经元并分割权重。

Result: 在布尔代码问题上，FPE降低了多语义性指标，提高了任务准确率；在实际模型中，维持非零参数数量增加网络宽度也提高了准确率。

Conclusion: 找到了一种基于可解释性的机制，利用网络宽度对抗叠加现象，提升性能且不增加非零参数数量，适合现代加速器。

Abstract: This work demonstrates how increasing the number of neurons in a network
without increasing its number of non-zero parameters improves performance. We
show that this gain corresponds with a decrease in interference between
multiple features that would otherwise share the same neurons. To reduce such
entanglement at a fixed non-zero parameter count, we introduce Fixed Parameter
Expansion (FPE): replace a neuron with multiple children and partition the
parent's weights disjointly across them, so that each child inherits a
non-overlapping subset of connections. On symbolic tasks, specifically Boolean
code problems, clause-aligned FPE systematically reduces polysemanticity
metrics and yields higher task accuracy. Notably, random splits of neuron
weights approximate these gains, indicating that reduced collisions, not
precise assignment, are a primary driver. Consistent with the superposition
hypothesis, the benefits of FPE grow with increasing interference: when
polysemantic load is high, accuracy improvements are the largest. Transferring
these insights to real models (classifiers over CLIP embeddings and deeper
multilayer networks) we find that widening networks while maintaining a
constant non-zero parameter count consistently increases accuracy. These
results identify an interpretability-grounded mechanism to leverage width
against superposition, improving performance without increasing the number of
non-zero parameters. Such a direction is well matched to modern accelerators,
where memory movement of non-zero parameters, rather than raw compute, is the
dominant bottleneck.

</details>


### [358] [Wavelet Predictive Representations for Non-Stationary Reinforcement Learning](https://arxiv.org/abs/2510.04507)
*Min Wang,Xin Li,Ye He,Yao-Hui Li,Hasnaa Bennis,Riashat Islam,Mingzhong Wang*

Main category: cs.LG

TL;DR: 提出WISDOM方法，利用小波域预测任务表示增强非平稳强化学习（NSRL），实验显示其在样本效率和渐近性能上显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有NSRL方法在高度动态环境下适应性有限，需要新方法提升适应性。

Method: 提出WISDOM方法，将任务表示序列转换到小波域捕捉多尺度特征，设计小波时间差分（TD）更新算子并证明其收敛性。

Result: 在不同基准测试中，WISDOM在样本效率和渐近性能上显著优于现有基线。

Conclusion: WISDOM在非平稳和随机演化任务的复杂环境中具有出色的适应性。

Abstract: The real world is inherently non-stationary, with ever-changing factors, such
as weather conditions and traffic flows, making it challenging for agents to
adapt to varying environmental dynamics. Non-Stationary Reinforcement Learning
(NSRL) addresses this challenge by training agents to adapt rapidly to
sequences of distinct Markov Decision Processes (MDPs). However, existing NSRL
approaches often focus on tasks with regularly evolving patterns, leading to
limited adaptability in highly dynamic settings. Inspired by the success of
Wavelet analysis in time series modeling, specifically its ability to capture
signal trends at multiple scales, we propose WISDOM to leverage wavelet-domain
predictive task representations to enhance NSRL. WISDOM captures these
multi-scale features in evolving MDP sequences by transforming task
representation sequences into the wavelet domain, where wavelet coefficients
represent both global trends and fine-grained variations of non-stationary
changes. In addition to the auto-regressive modeling commonly employed in time
series forecasting, we devise a wavelet temporal difference (TD) update
operator to enhance tracking and prediction of MDP evolution. We theoretically
prove the convergence of this operator and demonstrate policy improvement with
wavelet task representations. Experiments on diverse benchmarks show that
WISDOM significantly outperforms existing baselines in both sample efficiency
and asymptotic performance, demonstrating its remarkable adaptability in
complex environments characterized by non-stationary and stochastically
evolving tasks.

</details>


### [359] [Real-time Prediction of Urban Sound Propagation with Conditioned Normalizing Flows](https://arxiv.org/abs/2510.04510)
*Achim Eckerle,Martin Spitznagel,Janis Keuper*

Main category: cs.LG

TL;DR: 本文评估条件归一化流用于实时生成符合标准的城市声压图，加速地图生成且提高精度，适用于城市规划等场景。


<details>
  <summary>Details</summary>
Motivation: 准确快速的城市噪声预测对公共健康和城市监管工作至关重要，基于物理的求解器速度太慢，无法满足时间关键的迭代研究需求。

Method: 评估条件归一化流（Full - Glow）从2D城市布局实时生成符合标准的城市声压图。

Result: 在多个数据集上，模型比参考求解器加速地图生成超2000倍，与先前深度模型相比，提高非视距精度达24%，在基线非视距中达到0.65 dB MAE且结构保真度高。

Conclusion: 该模型可重现衍射和干涉模式，支持源或几何变化下的即时重新计算，是城市规划、合规映射和运营的实用引擎。

Abstract: Accurate and fast urban noise prediction is pivotal for public health and for
regulatory workflows in cities, where the Environmental Noise Directive
mandates regular strategic noise maps and action plans, often needed in
permission workflows, right-of-way allocation, and construction scheduling.
Physics-based solvers are too slow for such time-critical, iterative "what-if"
studies. We evaluate conditional Normalizing Flows (Full-Glow) for generating
for generating standards-compliant urban sound-pressure maps from 2D urban
layouts in real time per 256x256 map on a single RTX 4090), enabling
interactive exploration directly on commodity hardware. On datasets covering
Baseline, Diffraction, and Reflection regimes, our model accelerates map
generation by >2000 times over a reference solver while improving NLoS accuracy
by up to 24% versus prior deep models; in Baseline NLoS we reach 0.65 dB MAE
with high structural fidelity. The model reproduces diffraction and
interference patterns and supports instant recomputation under source or
geometry changes, making it a practical engine for urban planning, compliance
mapping, and operations (e.g., temporary road closures, night-work variance
assessments).

</details>


### [360] [Toward a Unified Geometry Understanding: Riemannian Diffusion Framework for Graph Generation and Prediction](https://arxiv.org/abs/2510.04522)
*Yisen Gao,Xingcheng Fu,Qingyun Sun,Jianxin Li,Xianxian Li*

Main category: cs.LG

TL;DR: 提出GeoMancer框架解决现有图扩散模型特征纠缠问题，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有图扩散模型将不同曲率特征混在同一潜在空间，未释放几何潜力，需构建理想黎曼扩散模型学习复杂图数据分布。

Method: 提出GeoMancer框架，用等距不变黎曼陀螺核方法替代指数映射，将多级特征解耦到各自特定流形，引入流形约束扩散方法和自引导策略。

Result: 实验验证了方法的有效性，在多种任务中表现出色。

Conclusion: GeoMancer框架能有效解决现有图扩散模型问题，表现优越。

Abstract: Graph diffusion models have made significant progress in learning structured
graph data and have demonstrated strong potential for predictive tasks.
Existing approaches typically embed node, edge, and graph-level features into a
unified latent space, modeling prediction tasks including classification and
regression as a form of conditional generation. However, due to the
non-Euclidean nature of graph data, features of different curvatures are
entangled in the same latent space without releasing their geometric potential.
To address this issue, we aim to construt an ideal Riemannian diffusion model
to capture distinct manifold signatures of complex graph data and learn their
distribution. This goal faces two challenges: numerical instability caused by
exponential mapping during the encoding proces and manifold deviation during
diffusion generation. To address these challenges, we propose GeoMancer: a
novel Riemannian graph diffusion framework for both generation and prediction
tasks. To mitigate numerical instability, we replace exponential mapping with
an isometric-invariant Riemannian gyrokernel approach and decouple multi-level
features onto their respective task-specific manifolds to learn optimal
representations. To address manifold deviation, we introduce a
manifold-constrained diffusion method and a self-guided strategy for
unconditional generation, ensuring that the generated data remains aligned with
the manifold signature. Extensive experiments validate the effectiveness of our
approach, demonstrating superior performance across a variety of tasks.

</details>


### [361] [Post-training quantization of vision encoders needs prefixing registers](https://arxiv.org/abs/2510.04547)
*Seunghyeon Kim,Jinho Kim,Taesun Yeom,Wonpyo Park,Kyuyeun Kim,Jaeho Lee*

Main category: cs.LG

TL;DR: 提出训练无关算法RegCache减轻视觉编码器中的异常值，提升量化模型准确性。


<details>
  <summary>Details</summary>
Motivation: 基于Transformer的视觉编码器在多模态智能应用中需实时处理大量视觉数据，降低推理成本至关重要，而现有后训练量化在处理大规模激活异常值时存在挑战。

Method: 提出RegCache算法，引入易产生异常值但语义无意义的前缀标记到目标视觉编码器，防止其他标记出现异常值，并采用中间层前缀和标记删除两项技术创新。

Result: 实验表明该方法在文本监督和自监督视觉编码器中均能持续提高量化模型的准确性。

Conclusion: RegCache能有效减轻视觉编码器中的异常值，实现量化时精度损失显著降低。

Abstract: Transformer-based vision encoders -- such as CLIP -- are central to
multimodal intelligence, powering applications from autonomous web agents to
robotic control. Since these applications often demand real-time processing of
massive visual data, reducing the inference cost of vision encoders is
critical. Post-training quantization offers a practical path, but remains
challenging even at 8-bit precision due to massive-scale activations (i.e.,
outliers). In this work, we propose $\textit{RegCache}$, a training-free
algorithm to mitigate outliers in vision encoders, enabling quantization with
significantly smaller accuracy drops. The proposed RegCache introduces
outlier-prone yet semantically meaningless prefix tokens to the target vision
encoder, which prevents other tokens from having outliers. Notably, we observe
that outliers in vision encoders behave differently from those in language
models, motivating two technical innovations: middle-layer prefixing and token
deletion. Experiments show that our method consistently improves the accuracy
of quantized models across both text-supervised and self-supervised vision
encoders.

</details>


### [362] [Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF--QP Safety Layer in Arbitrage-Free Markets](https://arxiv.org/abs/2510.04555)
*Jian'an Zhang*

Main category: cs.LG

TL;DR: 介绍Tail - Safe框架，结合学习与安全组件用于衍生品对冲，有理论保证和实证效果，有一定局限性。


<details>
  <summary>Details</summary>
Motivation: 构建面向可部署性的衍生品对冲框架，统一分布、风险敏感强化学习与针对金融约束的安全层。

Method: 学习组件结合IQN - CVaR - PPO与Tail - Coverage Controller；安全组件通过凸QP执行离散时间CBF不等式和特定领域约束。

Result: 在合成市场中改善左尾风险，不降低中心性能，可行时无硬约束违规，遥测数据用于支持可解释性和可审计性。

Conclusion: Tail - Safe框架在衍生品对冲中有一定效果，但依赖合成数据和简化执行。

Abstract: We introduce Tail-Safe, a deployability-oriented framework for derivatives
hedging that unifies distributional, risk-sensitive reinforcement learning with
a white-box control-barrier-function (CBF) quadratic-program (QP) safety layer
tailored to financial constraints. The learning component combines an IQN-based
distributional critic with a CVaR objective (IQN--CVaR--PPO) and a
Tail-Coverage Controller that regulates quantile sampling through temperature
tilting and tail boosting to stabilize small-$\alpha$ estimation. The safety
component enforces discrete-time CBF inequalities together with domain-specific
constraints -- ellipsoidal no-trade bands, box and rate limits, and a
sign-consistency gate -- solved as a convex QP whose telemetry (active sets,
tightness, rate utilization, gate scores, slack, and solver status) forms an
auditable trail for governance. We provide guarantees of robust forward
invariance of the safe set under bounded model mismatch, a minimal-deviation
projection interpretation of the QP, a KL-to-DRO upper bound linking per-state
KL regularization to worst-case CVaR, concentration and sample-complexity
results for the temperature-tilted CVaR estimator, and a CVaR trust-region
improvement inequality under KL limits, together with feasibility persistence
under expiry-aware tightening. Empirically, in arbitrage-free,
microstructure-aware synthetic markets (SSVI $\to$ Dupire $\to$ VIX with
ABIDES/MockLOB execution), Tail-Safe improves left-tail risk without degrading
central performance and yields zero hard-constraint violations whenever the QP
is feasible with zero slack. Telemetry is mapped to governance dashboards and
incident workflows to support explainability and auditability. Limitations
include reliance on synthetic data and simplified execution to isolate
methodological contributions.

</details>


### [363] [Challenger-Based Combinatorial Bandits for Subcarrier Selection in OFDM Systems](https://arxiv.org/abs/2510.04559)
*Mohsen Amiri,V Venktesh,Sindri Magnússon*

Main category: cs.LG

TL;DR: 本文将多用户MIMO下行链路中前m个用户调度集的识别问题转化为随机线性老虎机中的组合纯探索问题，采用线性效用模型和间隙指数框架，减少运行时间和计算量，在速度和准确性间有可调权衡，使在线子载波选择对AI通信系统可行。


<details>
  <summary>Details</summary>
Motivation: 多用户MIMO下行链路中动作空间呈指数增长，穷举搜索不可行，需有效方法识别前m个用户调度集。

Method: 采用线性效用模型，引入间隙指数框架，维护冠军臂短列表和挑战者臂旋转短列表，聚焦有信息的基于间隙指数的比较。

Result: 与现有线性老虎机方法相比，显著减少运行时间和计算量，有高识别准确性，且速度和准确性间有可调权衡。

Conclusion: 基于短列表的纯探索使AI通信系统中在线、高效测量的子载波选择成为可能。

Abstract: This paper investigates the identification of the top-m user-scheduling sets
in multi-user MIMO downlink, which is cast as a combinatorial pure-exploration
problem in stochastic linear bandits. Because the action space grows
exponentially, exhaustive search is infeasible. We therefore adopt a linear
utility model to enable efficient exploration and reliable selection of
promising user subsets. We introduce a gap-index framework that maintains a
shortlist of current estimates of champion arms (top-m sets) and a rotating
shortlist of challenger arms that pose the greatest threat to the champions.
This design focuses on measurements that yield the most informative
gap-index-based comparisons, resulting in significant reductions in runtime and
computation compared to state-of-the-art linear bandit methods, with high
identification accuracy. The method also exposes a tunable trade-off between
speed and accuracy. Simulations on a realistic OFDM downlink show that
shortlist-driven pure exploration makes online, measurement-efficient
subcarrier selection practical for AI-enabled communication systems.

</details>


### [364] [Stochastic Approximation Methods for Distortion Risk Measure Optimization](https://arxiv.org/abs/2510.04563)
*Jinyang Jiang,Bernd Heidergott,Jiaqiao Hu,Yijie Peng*

Main category: cs.LG

TL;DR: 本文基于两种对偶表示提出梯度下降算法用于扭曲风险度量（DRM）优化，证明收敛性和收敛速率，通过数值实验验证有效性，还展示了在强化学习中的可扩展性和实际应用。


<details>
  <summary>Details</summary>
Motivation: DRM可捕捉决策中的风险偏好和管理不确定性，需要有效的优化算法。

Method: 基于DM形式和QF形式提出梯度下降算法，还有混合形式；证明算法的强收敛性和收敛速率。

Result: DM形式收敛率为$O(k^{-4/7})$，QF形式为$O(k^{-2/3})$；数值实验表明在稳健投资组合选择任务中优于基线；开发基于DRM的近端策略优化算法并应用于多级动态库存管理。

Conclusion: 提出的算法有效，在DRM优化方面有良好表现，且具有可扩展性和实际应用价值。

Abstract: Distortion Risk Measures (DRMs) capture risk preferences in decision-making
and serve as general criteria for managing uncertainty. This paper proposes
gradient descent algorithms for DRM optimization based on two dual
representations: the Distortion-Measure (DM) form and Quantile-Function (QF)
form. The DM-form employs a three-timescale algorithm to track quantiles,
compute their gradients, and update decision variables, utilizing the
Generalized Likelihood Ratio and kernel-based density estimation. The QF-form
provides a simpler two-timescale approach that avoids the need for complex
quantile gradient estimation. A hybrid form integrates both approaches,
applying the DM-form for robust performance around distortion function jumps
and the QF-form for efficiency in smooth regions. Proofs of strong convergence
and convergence rates for the proposed algorithms are provided. In particular,
the DM-form achieves an optimal rate of $O(k^{-4/7})$, while the QF-form
attains a faster rate of $O(k^{-2/3})$. Numerical experiments confirm their
effectiveness and demonstrate substantial improvements over baselines in robust
portfolio selection tasks. The method's scalability is further illustrated
through integration into deep reinforcement learning. Specifically, a DRM-based
Proximal Policy Optimization algorithm is developed and applied to
multi-echelon dynamic inventory management, showcasing its practical
applicability.

</details>


### [365] [GILT: An LLM-Free, Tuning-Free Graph Foundational Model for In-Context Learning](https://arxiv.org/abs/2510.04567)
*Weishuo Ma,Yanbo Wang,Xiyuan Wang,Lei Zou,Muhan Zhang*

Main category: cs.LG

TL;DR: 文章指出当前图基础模型存在不足，提出无大语言模型和免调优架构的GILT框架，实验表明其少样本性能强且耗时少。


<details>
  <summary>Details</summary>
Motivation: 现有图基础模型受图数据异质性挑战，基于大语言模型的方法依赖文本、难处理数值特征，基于结构预训练模型调优成本高，需突破这些局限。

Method: 引入基于无大语言模型和免调优架构的GILT框架，采用基于token的上下文学习机制，将节点、边和图级别的分类任务统一处理。

Result: GILT相比基于大语言模型或调优的基线模型，在少样本情况下性能更强，耗时显著减少。

Conclusion: GILT框架有效，能解决现有图基础模型的问题。

Abstract: Graph Neural Networks (GNNs) are powerful tools for precessing relational
data but often struggle to generalize to unseen graphs, giving rise to the
development of Graph Foundational Models (GFMs). However, current GFMs are
challenged by the extreme heterogeneity of graph data, where each graph can
possess a unique feature space, label set, and topology. To address this, two
main paradigms have emerged. The first leverages Large Language Models (LLMs),
but is fundamentally text-dependent, thus struggles to handle the numerical
features in vast graphs. The second pre-trains a structure-based model, but the
adaptation to new tasks typically requires a costly, per-graph tuning stage,
creating a critical efficiency bottleneck. In this work, we move beyond these
limitations and introduce \textbf{G}raph \textbf{I}n-context \textbf{L}earning
\textbf{T}ransformer (GILT), a framework built on an LLM-free and tuning-free
architecture. GILT introduces a novel token-based framework for in-context
learning (ICL) on graphs, reframing classification tasks spanning node, edge
and graph levels in a unified framework. This mechanism is the key to handling
heterogeneity, as it is designed to operate on generic numerical features.
Further, its ability to understand class semantics dynamically from the context
enables tuning-free adaptation. Comprehensive experiments show that GILT
achieves stronger few-shot performance with significantly less time than
LLM-based or tuning-based baselines, validating the effectiveness of our
approach.

</details>


### [366] [LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning](https://arxiv.org/abs/2510.04573)
*Haoqiang Kang,Yizhe Zhang,Nikki Lijing Kuang,Nicklas Majamaki,Navdeep Jaitly,Yi-An Ma,Lianhui Qin*

Main category: cs.LG

TL;DR: 提出LaDiR推理框架，结合连续潜在表示和潜在扩散模型迭代细化能力，在数学推理和规划基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: LLM自回归解码限制整体回顾和细化早期标记能力，探索多样解决方案效率低。

Method: 用VAE构建结构化潜在推理空间，将文本推理步骤编码为思想标记块；利用潜在扩散模型结合块级双向注意力掩码对潜在思想标记块去噪。

Result: 在数学推理和规划基准测试中，LaDiR在准确性、多样性和可解释性上持续优于现有自回归、基于扩散和潜在推理方法。

Conclusion: LaDiR揭示了潜在扩散文本推理新范式。

Abstract: Large Language Models (LLMs) demonstrate their reasoning ability through
chain-of-thought (CoT) generation. However, LLM's autoregressive decoding may
limit the ability to revisit and refine earlier tokens in a holistic manner,
which can also lead to inefficient exploration for diverse solutions. In this
paper, we propose LaDiR (Latent Diffusion Reasoner), a novel reasoning
framework that unifies the expressiveness of continuous latent representation
with the iterative refinement capabilities of latent diffusion models for an
existing LLM. We first construct a structured latent reasoning space using a
Variational Autoencoder (VAE) that encodes text reasoning steps into blocks of
thought tokens, preserving semantic information and interpretability while
offering compact but expressive representations. Subsequently, we utilize a
latent diffusion model that learns to denoise a block of latent thought tokens
with a blockwise bidirectional attention mask, enabling longer horizon and
iterative refinement with adaptive test-time compute. This design allows
efficient parallel generation of diverse reasoning trajectories, allowing the
model to plan and revise the reasoning process holistically. We conduct
evaluations on a suite of mathematical reasoning and planning benchmarks.
Empirical results show that LaDiR consistently improves accuracy, diversity,
and interpretability over existing autoregressive, diffusion-based, and latent
reasoning methods, revealing a new paradigm for text reasoning with latent
diffusion.

</details>


### [367] [Improved probabilistic regression using diffusion models](https://arxiv.org/abs/2510.04583)
*Carlo Kneissl,Christopher Bülte,Philipp Scholl,Gitta Kutyniok*

Main category: cs.LG

TL;DR: 提出基于扩散的概率回归框架，在多实验中表现优且能提供校准的不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 扩散生成模型在一般回归任务中缺乏不确定性评估且应用受限，需新方法。

Method: 提出非参数学习预测分布的基于扩散的框架，对扩散噪声全分布建模，研究不同噪声参数化。

Result: 在多个回归任务实验中，相比现有基线表现更优，能提供校准的不确定性估计。

Conclusion: 该框架作为概率预测工具具有通用性。

Abstract: Probabilistic regression models the entire predictive distribution of a
response variable, offering richer insights than classical point estimates and
directly allowing for uncertainty quantification. While diffusion-based
generative models have shown remarkable success in generating complex,
high-dimensional data, their usage in general regression tasks often lacks
uncertainty-related evaluation and remains limited to domain-specific
applications. We propose a novel diffusion-based framework for probabilistic
regression that learns predictive distributions in a nonparametric way. More
specifically, we propose to model the full distribution of the diffusion noise,
enabling adaptation to diverse tasks and enhanced uncertainty quantification.
We investigate different noise parameterizations, analyze their trade-offs, and
evaluate our framework across a broad range of regression tasks, covering low-
and high-dimensional settings. For several experiments, our approach shows
superior performance against existing baselines, while delivering calibrated
uncertainty estimates, demonstrating its versatility as a tool for
probabilistic prediction.

</details>


### [368] [Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models](https://arxiv.org/abs/2510.04618)
*Qizheng Zhang,Changran Hu,Shubhangi Upasani,Boyuan Ma,Fenglu Hong,Vamsidhar Kamanuru,Jay Rainton,Chen Wu,Mengmeng Ji,Hanchen Li,Urmish Thakker,James Zou,Kunle Olukotun*

Main category: cs.LG

TL;DR: 提出ACE框架处理上下文适应问题，在多个基准测试中表现优异，能实现可扩展、高效和自我改进的大语言模型系统。


<details>
  <summary>Details</summary>
Motivation: 解决现有上下文适应方法存在的简洁性偏差和上下文崩溃问题。

Method: 基于Dynamic Cheatsheet的自适应内存，引入ACE框架，通过生成、反思和策划的模块化过程处理上下文。

Result: 在多个基准测试中，ACE优化上下文表现优于基线，减少适应延迟和部署成本，能在无监督下有效适应。

Conclusion: 全面、不断发展的上下文可实现低开销的可扩展、高效和自我改进的大语言模型系统。

Abstract: Large language model (LLM) applications such as agents and domain-specific
reasoning increasingly rely on context adaptation -- modifying inputs with
instructions, strategies, or evidence, rather than weight updates. Prior
approaches improve usability but often suffer from brevity bias, which drops
domain insights for concise summaries, and from context collapse, where
iterative rewriting erodes details over time. Building on the adaptive memory
introduced by Dynamic Cheatsheet, we introduce ACE (Agentic Context
Engineering), a framework that treats contexts as evolving playbooks that
accumulate, refine, and organize strategies through a modular process of
generation, reflection, and curation. ACE prevents collapse with structured,
incremental updates that preserve detailed knowledge and scale with
long-context models. Across agent and domain-specific benchmarks, ACE optimizes
contexts both offline (e.g., system prompts) and online (e.g., agent memory),
consistently outperforming strong baselines: +10.6% on agents and +8.6% on
finance, while significantly reducing adaptation latency and rollout cost.
Notably, ACE could adapt effectively without labeled supervision and instead by
leveraging natural execution feedback. On the AppWorld leaderboard, ACE matches
the top-ranked production-level agent on the overall average and surpasses it
on the harder test-challenge split, despite using a smaller open-source model.
These results show that comprehensive, evolving contexts enable scalable,
efficient, and self-improving LLM systems with low overhead.

</details>


### [369] [Forecasting-Based Biomedical Time-series Data Synthesis for Open Data and Robust AI](https://arxiv.org/abs/2510.04622)
*Youngjoon Lee,Seongmin Cho,Yehhyun Jo,Jinu Gong,Hyunjoo Jenny Lee,Joonhyuk Kang*

Main category: cs.LG

TL;DR: 因隐私规定和资源需求限制生物医学时间序列AI发展，提出基于预测模型的合成数据生成框架，所生成数据能替代真实数据提升AI性能，扩展研究资源。


<details>
  <summary>Details</summary>
Motivation: 严格隐私规定和资源需求限制了生物医学时间序列AI发展，数据需求和可获取性存在差距。

Method: 提出基于先进预测模型的合成生物医学时间序列数据生成框架。

Result: 生成的合成数据可有效替代真实数据，显著提升AI模型性能。

Conclusion: 该方法能保留关键生物医学特征，具有高可扩展性，可无缝集成到开源库，扩展生物医学研究资源。

Abstract: The limited data availability due to strict privacy regulations and
significant resource demands severely constrains biomedical time-series AI
development, which creates a critical gap between data requirements and
accessibility. Synthetic data generation presents a promising solution by
producing artificial datasets that maintain the statistical properties of real
biomedical time-series data without compromising patient confidentiality. We
propose a framework for synthetic biomedical time-series data generation based
on advanced forecasting models that accurately replicates complex
electrophysiological signals such as EEG and EMG with high fidelity. These
synthetic datasets preserve essential temporal and spectral properties of real
data, which enables robust analysis while effectively addressing data scarcity
and privacy challenges. Our evaluations across multiple subjects demonstrate
that the generated synthetic data can serve as an effective substitute for real
data and also significantly boost AI model performance. The approach maintains
critical biomedical features while provides high scalability for various
applications and integrates seamlessly into open-source repositories,
substantially expanding resources for AI-driven biomedical research.

</details>


### [370] [Compressed Concatenation of Small Embedding Models](https://arxiv.org/abs/2510.04626)
*Mohamed Ayoub Ben Ayad,Michael Dinzinger,Kanishka Ghosh Dastidar,Jelena Mitrovic,Michael Granitzer*

Main category: cs.LG

TL;DR: 提出通过拼接多个小嵌入模型原始向量并使用轻量级统一解码器，在检索任务上实现压缩且保留性能。


<details>
  <summary>Details</summary>
Motivation: 大嵌入模型在资源受限环境难部署，小模型性能差，需缩小二者差距。

Method: 拼接多个小模型原始向量，引入用MRL损失训练的轻量级统一解码器将高维联合表示映射到低维空间。

Result: 在MTEB部分检索任务中，对四个小模型拼接应用concat - encode - quantize管道，48倍压缩下恢复89%原始性能，且增加基础模型数量虽收益递减但解码器表示在压缩和量化下鲁棒性提升。

Conclusion: 拼接多个小模型向量结合轻量级解码器可在资源受限环境下有效平衡模型大小与性能。

Abstract: Embedding models are central to dense retrieval, semantic search, and
recommendation systems, but their size often makes them impractical to deploy
in resource-constrained environments such as browsers or edge devices. While
smaller embedding models offer practical advantages, they typically
underperform compared to their larger counterparts. To bridge this gap, we
demonstrate that concatenating the raw embedding vectors of multiple small
models can outperform a single larger baseline on standard retrieval
benchmarks. To overcome the resulting high dimensionality of naive
concatenation, we introduce a lightweight unified decoder trained with a
Matryoshka Representation Learning (MRL) loss. This decoder maps the
high-dimensional joint representation to a low-dimensional space, preserving
most of the original performance without fine-tuning the base models. We also
show that while concatenating more base models yields diminishing gains, the
robustness of the decoder's representation under compression and quantization
improves. Our experiments show that, on a subset of MTEB retrieval tasks, our
concat-encode-quantize pipeline recovers 89\% of the original performance with
a 48x compression factor when the pipeline is applied to a concatenation of
four small embedding models.

</details>


### [371] [Predictive Feature Caching for Training-free Acceleration of Molecular Geometry Generation](https://arxiv.org/abs/2510.04646)
*Johanna Sommer,John Rachwan,Nils Fleischmann,Stephan Günnemann,Bertrand Charpentier*

Main category: cs.LG

TL;DR: 提出训练无关的缓存策略加速分子几何生成，实验显示可显著减少推理时间且与其他优化方法有叠加效果。


<details>
  <summary>Details</summary>
Motivation: 流匹配模型在推理时计算成本高，成为大量采样分子候选时的主要瓶颈。

Method: 提出训练无关的缓存策略，直接作用于SE(3)等变骨干，与预训练模型兼容。

Result: 在GEOM - Drugs数据集上，缓存策略在样本质量相当的情况下使推理时间减半，与基础模型相比加速达3倍，与其他优化结合使用可加速达7倍。

Conclusion: 该缓存策略能有效加速分子几何生成，且增益可与其他优化方法叠加。

Abstract: Flow matching models generate high-fidelity molecular geometries but incur
significant computational costs during inference, requiring hundreds of network
evaluations. This inference overhead becomes the primary bottleneck when such
models are employed in practice to sample large numbers of molecular
candidates. This work discusses a training-free caching strategy that
accelerates molecular geometry generation by predicting intermediate hidden
states across solver steps. The proposed method operates directly on the
SE(3)-equivariant backbone, is compatible with pretrained models, and is
orthogonal to existing training-based accelerations and system-level
optimizations. Experiments on the GEOM-Drugs dataset demonstrate that caching
achieves a twofold reduction in wall-clock inference time at matched sample
quality and a speedup of up to 3x compared to the base model with minimal
sample quality degradation. Because these gains compound with other
optimizations, applying caching alongside other general, lossless optimizations
yield as much as a 7x speedup.

</details>


### [372] [IMLP: An Energy-Efficient Continual Learning Method for Tabular Data Streams](https://arxiv.org/abs/2510.04660)
*Yuandou Wang,Filip Gunnarsson,Rihan Hai*

Main category: cs.LG

TL;DR: 提出上下文感知增量多层感知器 (IMLP) 处理表格数据流，引入 NetScore - T 评估，能效高且准确率有竞争力。


<details>
  <summary>Details</summary>
Motivation: 表格数据流应用资源受限，现有持续学习方法未充分考虑能源和内存效率，且依赖重播机制增加资源成本。

Method: 提出 IMLP，在滑动潜在特征缓冲区上采用窗口缩放点积注意力，结合当前特征通过共享前馈层处理；引入 NetScore - T 指标。

Result: IMLP 比 TabNet 能效高 27.6 倍，比 TabPFN 高 85.5 倍，保持有竞争力的平均准确率。

Conclusion: IMLP 为表格数据流提供了易于部署、节能的全重训练替代方案。

Abstract: Tabular data streams are rapidly emerging as a dominant modality for
real-time decision-making in healthcare, finance, and the Internet of Things
(IoT). These applications commonly run on edge and mobile devices, where energy
budgets, memory, and compute are strictly limited. Continual learning (CL)
addresses such dynamics by training models sequentially on task streams while
preserving prior knowledge and consolidating new knowledge. While recent CL
work has advanced in mitigating catastrophic forgetting and improving knowledge
transfer, the practical requirements of energy and memory efficiency for
tabular data streams remain underexplored. In particular, existing CL solutions
mostly depend on replay mechanisms whose buffers grow over time and exacerbate
resource costs.
  We propose a context-aware incremental Multi-Layer Perceptron (IMLP), a
compact continual learner for tabular data streams. IMLP incorporates a
windowed scaled dot-product attention over a sliding latent feature buffer,
enabling constant-size memory and avoiding storing raw data. The attended
context is concatenated with current features and processed by shared
feed-forward layers, yielding lightweight per-segment updates. To assess
practical deployability, we introduce NetScore-T, a tunable metric coupling
balanced accuracy with energy for Pareto-aware comparison across models and
datasets. IMLP achieves up to $27.6\times$ higher energy efficiency than TabNet
and $85.5\times$ higher than TabPFN, while maintaining competitive average
accuracy. Overall, IMLP provides an easy-to-deploy, energy-efficient
alternative to full retraining for tabular data streams.

</details>


### [373] [Noise or Signal? Deconstructing Contradictions and An Adaptive Remedy for Reversible Normalization in Time Series Forecasting](https://arxiv.org/abs/2510.04667)
*Fanzhe Fu,Yang Yang*

Main category: cs.LG

TL;DR: 本文解构不同归一化策略表现，发现标准RevIN在极端离群值数据集上失败，简单R² - IN表现最佳，自适应模型A - IN完全失败，提出时间序列归一化新范式。


<details>
  <summary>Details</summary>
Motivation: 探究用鲁棒统计量替代RevIN中非鲁棒统计量改进方法的实际效果，解构不同归一化策略令人困惑的表现。

Method: 识别四种潜在理论矛盾，通过实验分析不同归一化策略表现。

Result: 标准RevIN在极端离群值数据集上MSE激增683%；简单R² - IN表现最佳；自适应模型A - IN完全失败。

Conclusion: 提出时间序列归一化新范式，从盲目追求复杂性转向诊断驱动分析，揭示简单基线的强大和天真自适应的危险。

Abstract: Reversible Instance Normalization (RevIN) is a key technique enabling simple
linear models to achieve state-of-the-art performance in time series
forecasting. While replacing its non-robust statistics with robust counterparts
(termed R$^2$-IN) seems like a straightforward improvement, our findings reveal
a far more complex reality. This paper deconstructs the perplexing performance
of various normalization strategies by identifying four underlying theoretical
contradictions. Our experiments provide two crucial findings: first, the
standard RevIN catastrophically fails on datasets with extreme outliers, where
its MSE surges by a staggering 683\%. Second, while the simple R$^2$-IN
prevents this failure and unexpectedly emerges as the best overall performer,
our adaptive model (A-IN), designed to test a diagnostics-driven heuristic,
unexpectedly suffers a complete and systemic failure. This surprising outcome
uncovers a critical, overlooked pitfall in time series analysis: the
instability introduced by a simple or counter-intuitive heuristic can be more
damaging than the statistical issues it aims to solve. The core contribution of
this work is thus a new, cautionary paradigm for time series normalization: a
shift from a blind search for complexity to a diagnostics-driven analysis that
reveals not only the surprising power of simple baselines but also the perilous
nature of naive adaptation.

</details>


### [374] [Semantic Channel Equalization Strategies for Deep Joint Source-Channel Coding](https://arxiv.org/abs/2510.04674)
*Lorenzo Pannacci,Simone Fiorellino,Mario Edoardo Pandolfo,Emilio Calvanese Strinati,Paolo Di Lorenzo*

Main category: cs.LG

TL;DR: 本文系统化评估DeepJSCC语义信道均衡方法，研究三类校准器，通过实验量化权衡指标，为异构AI无线网络部署DeepJSCC提供指南。


<details>
  <summary>Details</summary>
Motivation: 现有DeepJSCC方案假设收发端有共享潜在空间，在多供应商部署中不成立，会引入“语义噪声”，降低重建质量和下游任务性能。

Method: 引入额外处理阶段对齐异构潜在空间，研究线性映射、轻量级神经网络和Parseval框架均衡器三类校准器。

Result: 通过在AWGN和衰落信道上的图像重建实验，量化了复杂度、数据效率和保真度之间的权衡。

Conclusion: 为在异构AI原生无线网络中部署DeepJSCC提供了指导。

Abstract: Deep joint source-channel coding (DeepJSCC) has emerged as a powerful
paradigm for end-to-end semantic communications, jointly learning to compress
and protect task-relevant features over noisy channels. However, existing
DeepJSCC schemes assume a shared latent space at transmitter (TX) and receiver
(RX) - an assumption that fails in multi-vendor deployments where encoders and
decoders cannot be co-trained. This mismatch introduces "semantic noise",
degrading reconstruction quality and downstream task performance. In this
paper, we systematize and evaluate methods for semantic channel equalization
for DeepJSCC, introducing an additional processing stage that aligns
heterogeneous latent spaces under both physical and semantic impairments. We
investigate three classes of aligners: (i) linear maps, which admit closed-form
solutions; (ii) lightweight neural networks, offering greater expressiveness;
and (iii) a Parseval-frame equalizer, which operates in zero-shot mode without
the need for training. Through extensive experiments on image reconstruction
over AWGN and fading channels, we quantify trade-offs among complexity, data
efficiency, and fidelity, providing guidelines for deploying DeepJSCC in
heterogeneous AI-native wireless networks.

</details>


### [375] [Counterfactual Credit Guided Bayesian Optimization](https://arxiv.org/abs/2510.04676)
*Qiyu Wei,Haowei Wang,Richard Allmendinger,Mauricio A. Álvarez*

Main category: cs.LG

TL;DR: 本文提出Counterfactual Credit Guided Bayesian Optimization (CCGBO)框架，通过反事实信用量化历史观测贡献，证明其有次线性遗憾，实验表明能减少简单遗憾并加速收敛到全局最优。


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯优化在很多实际场景中旨在快速找到全局最优，原假设所有观测样本对发现最优贡献相同有局限性。

Method: 引入CCGBO框架，通过反事实信用明确量化单个历史观测的贡献，并将其融入采集函数来选择性分配资源。

Result: 证明CCGBO有次线性遗憾，在各种合成和真实世界基准测试中，CCGBO持续减少简单遗憾并加速收敛到全局最优。

Conclusion: CCGBO能有效减少简单遗憾，加速收敛到全局最优，是一种有效的优化方法。

Abstract: Bayesian optimization has emerged as a prominent methodology for optimizing
expensive black-box functions by leveraging Gaussian process surrogates, which
focus on capturing the global characteristics of the objective function.
However, in numerous practical scenarios, the primary objective is not to
construct an exhaustive global surrogate, but rather to quickly pinpoint the
global optimum. Due to the aleatoric nature of the sequential optimization
problem and its dependence on the quality of the surrogate model and the
initial design, it is restrictive to assume that all observed samples
contribute equally to the discovery of the optimum in this context. In this
paper, we introduce Counterfactual Credit Guided Bayesian Optimization (CCGBO),
a novel framework that explicitly quantifies the contribution of individual
historical observations through counterfactual credit. By incorporating
counterfactual credit into the acquisition function, our approach can
selectively allocate resources in areas where optimal solutions are most likely
to occur. We prove that CCGBO retains sublinear regret. Empirical evaluations
on various synthetic and real-world benchmarks demonstrate that CCGBO
consistently reduces simple regret and accelerates convergence to the global
optimum.

</details>


### [376] [Parameter-free Algorithms for the Stochastically Extended Adversarial Model](https://arxiv.org/abs/2510.04685)
*Shuche Wang,Adarsh Barik,Peng Zhao,Vincent Y. F. Tan*

Main category: cs.LG

TL;DR: 本文开发了随机扩展对抗（SEA）模型的无参数算法，解决现有方法需先验参数的问题。


<details>
  <summary>Details</summary>
Motivation: 现有SEA模型方法需问题特定参数，限制实际应用，因此开发无参数算法。

Method: 利用乐观在线牛顿步（OONS）算法消除对特定参数的需求，先针对未知域直径但已知Lipschitz常数的场景建立比较器自适应算法，再扩展到两者都未知的一般场景。

Result: 在未知域直径但已知Lipschitz常数场景下，得到期望遗憾界；在两者都未知的一般场景下，获得比较器和Lipschitz自适应算法，遗憾界对相关方差有相同依赖。

Conclusion: 所提出的方法在SEA模型中即使两个参数都未知时也有效。

Abstract: We develop the first parameter-free algorithms for the Stochastically
Extended Adversarial (SEA) model, a framework that bridges adversarial and
stochastic online convex optimization. Existing approaches for the SEA model
require prior knowledge of problem-specific parameters, such as the diameter of
the domain $D$ and the Lipschitz constant of the loss functions $G$, which
limits their practical applicability. Addressing this, we develop
parameter-free methods by leveraging the Optimistic Online Newton Step (OONS)
algorithm to eliminate the need for these parameters. We first establish a
comparator-adaptive algorithm for the scenario with unknown domain diameter but
known Lipschitz constant, achieving an expected regret bound of
$\tilde{O}\big(\|u\|_2^2 + \|u\|_2(\sqrt{\sigma^2_{1:T}} +
\sqrt{\Sigma^2_{1:T}})\big)$, where $u$ is the comparator vector and
$\sigma^2_{1:T}$ and $\Sigma^2_{1:T}$ represent the cumulative stochastic
variance and cumulative adversarial variation, respectively. We then extend
this to the more general setting where both $D$ and $G$ are unknown, attaining
the comparator- and Lipschitz-adaptive algorithm. Notably, the regret bound
exhibits the same dependence on $\sigma^2_{1:T}$ and $\Sigma^2_{1:T}$,
demonstrating the efficacy of our proposed methods even when both parameters
are unknown in the SEA model.

</details>


### [377] [How does the optimizer implicitly bias the model merging loss landscape?](https://arxiv.org/abs/2510.04686)
*Chenxiang Zhang,Alexander Theus,Damien Teney,Antonio Orvieto,Jun Pang,Sjouke Mauw*

Main category: cs.LG

TL;DR: 本文探讨优化过程对损失景观几何的影响及其对模型合并成功的作用，发现有效噪声尺度统一了优化器和数据选择对模型合并的影响，且合并有效性与有效噪声呈非单调关系。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法有效，但对使合并有效的属性理解不足，需探究优化过程对损失景观几何及模型合并的影响。

Method: 研究优化过程如何影响损失景观几何，分析有效噪声尺度对模型合并的作用，分解该尺度探究各因素影响。

Result: 有效噪声尺度统一了优化器和数据选择对模型合并的影响，合并有效性是有效噪声的非单调函数，有明显最优值；学习率、权重衰减、批量大小和数据增强等因素独立调节有效噪声尺度。

Conclusion: 研究拓宽了对优化塑造损失景观几何及其对模型合并影响的理解，提示可进一步操纵训练动态提高合并有效性。

Abstract: Model merging methods combine models with different capabilities into a
single one while maintaining the same inference cost. Two popular approaches
are linear interpolation, which linearly interpolates between model weights,
and task arithmetic, which combines task vectors obtained by the difference
between finetuned and base models. While useful in practice, what properties
make merging effective are poorly understood. This paper explores how the
optimization process affects the loss landscape geometry and its impact on
merging success. We show that a single quantity -- the effective noise scale --
unifies the impact of optimizer and data choices on model merging. Across
architectures and datasets, the effectiveness of merging success is a
non-monotonic function of effective noise, with a distinct optimum. Decomposing
this quantity, we find that larger learning rates, stronger weight decay,
smaller batch sizes, and data augmentation all independently modulate the
effective noise scale, exhibiting the same qualitative trend. Unlike prior work
that connects optimizer noise to the flatness or generalization of individual
minima, we show that it also affects the global loss landscape, predicting when
independently trained solutions can be merged. Our findings broaden the
understanding of how optimization shapes the loss landscape geometry and its
downstream consequences for model merging, suggesting the possibility of
further manipulating the training dynamics to improve merging effectiveness.

</details>


### [378] [ViTs: Teaching Machines to See Time Series Anomalies Like Human Experts](https://arxiv.org/abs/2510.04710)
*Zexin Wang,Changhua Pei,Yang Liu,Hengyue Jiang,Quan Zhou,Haotian Si,Hang Cui,Jianhui Li,Gaogang Xie,Jingjing Li,Dan Pei*

Main category: cs.LG

TL;DR: 提出基于VLM的ViTs框架处理时间序列异常检测，解决传统方法和LLMs局限，经实验验证有效，数据和代码将公开。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列异常检测模型难以实现‘一次训练，多场景推理’，且在处理不同长度序列时有局限，LLMs应用于时间序列数据存在上下文长度限制。

Method: 提出ViTs框架将时间序列曲线转为视觉表示，用进化算法生成图像 - 文本对，设计三阶段训练流程。

Result: 实验表明ViTs显著提升了VLM理解和检测时间序列数据中异常的能力。

Conclusion: ViTs框架有效解决了时间序列异常检测中的问题，具有良好的性能。

Abstract: Web service administrators must ensure the stability of multiple systems by
promptly detecting anomalies in Key Performance Indicators (KPIs). Achieving
the goal of "train once, infer across scenarios" remains a fundamental
challenge for time series anomaly detection models. Beyond improving zero-shot
generalization, such models must also flexibly handle sequences of varying
lengths during inference, ranging from one hour to one week, without
retraining. Conventional approaches rely on sliding-window encoding and
self-supervised learning, which restrict inference to fixed-length inputs.
Large Language Models (LLMs) have demonstrated remarkable zero-shot
capabilities across general domains. However, when applied to time series data,
they face inherent limitations due to context length. To address this issue, we
propose ViTs, a Vision-Language Model (VLM)-based framework that converts time
series curves into visual representations. By rescaling time series images,
temporal dependencies are preserved while maintaining a consistent input size,
thereby enabling efficient processing of arbitrarily long sequences without
context constraints. Training VLMs for this purpose introduces unique
challenges, primarily due to the scarcity of aligned time series image-text
data. To overcome this, we employ an evolutionary algorithm to automatically
generate thousands of high-quality image-text pairs and design a three-stage
training pipeline consisting of: (1) time series knowledge injection, (2)
anomaly detection enhancement, and (3) anomaly reasoning refinement. Extensive
experiments demonstrate that ViTs substantially enhance the ability of VLMs to
understand and detect anomalies in time series data. All datasets and code will
be publicly released at: https://anonymous.4open.science/r/ViTs-C484/.

</details>


### [379] [Directional Sheaf Hypergraph Networks: Unifying Learning on Directed and Undirected Hypergraphs](https://arxiv.org/abs/2510.04727)
*Emanuele Mule,Stefano Fiorini,Antonio Purificato,Federico Siciliano,Stefano Coniglio,Fabrizio Silvestri*

Main category: cs.LG

TL;DR: 论文提出Directional Sheaf Hypergraph Networks (DSHN) 框架，结合层理论处理超图有向关系，实验显示能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 有向超图研究不足，现有方法在异质环境效果不佳，且层神经网络推广仅适用于无向超图。

Method: 引入DSHN框架，构建有向层超图拉普拉斯算子统一和推广现有拉普拉斯矩阵。

Result: 在7个真实数据集和13个基线对比中，DSHN相对准确率提升2% - 20%。

Conclusion: 对超图方向性的合理处理与层的表达能力结合可大幅提升性能。

Abstract: Hypergraphs provide a natural way to represent higher-order interactions
among multiple entities. While undirected hypergraphs have been extensively
studied, the case of directed hypergraphs, which can model oriented group
interactions, remains largely under-explored despite its relevance for many
applications. Recent approaches in this direction often exhibit an implicit
bias toward homophily, which limits their effectiveness in heterophilic
settings. Rooted in the algebraic topology notion of Cellular Sheaves, Sheaf
Neural Networks (SNNs) were introduced as an effective solution to circumvent
such a drawback. While a generalization to hypergraphs is known, it is only
suitable for undirected hypergraphs, failing to tackle the directed case. In
this work, we introduce Directional Sheaf Hypergraph Networks (DSHN), a
framework integrating sheaf theory with a principled treatment of asymmetric
relations within a hypergraph. From it, we construct the Directed Sheaf
Hypergraph Laplacian, a complex-valued operator by which we unify and
generalize many existing Laplacian matrices proposed in the graph- and
hypergraph-learning literature. Across 7 real-world datasets and against 13
baselines, DSHN achieves relative accuracy gains from 2% up to 20%, showing how
a principled treatment of directionality in hypergraphs, combined with the
expressive power of sheaves, can substantially improve performance.

</details>


### [380] [EVaR-Optimal Arm Identification in Bandits](https://arxiv.org/abs/2510.04728)
*Mehrasa Ahmadipour,Aurélien Garivier*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the fixed-confidence best arm identification (BAI) problem within
the multi-armed bandit (MAB) framework under the Entropic Value-at-Risk (EVaR)
criterion. Our analysis considers a nonparametric setting, allowing for general
reward distributions bounded in [0,1]. This formulation addresses the critical
need for risk-averse decision-making in high-stakes environments, such as
finance, moving beyond simple expected value optimization. We propose a
$\delta$-correct, Track-and-Stop based algorithm and derive a corresponding
lower bound on the expected sample complexity, which we prove is asymptotically
matched. The implementation of our algorithm and the characterization of the
lower bound both require solving a complex convex optimization problem and a
related, simpler non-convex one.

</details>


### [381] [Provable Affine Identifiability of Nonlinear CCA under Latent Distributional Priors](https://arxiv.org/abs/2510.04758)
*Zhiwei Han,Stefan Matthes,Hao Shen*

Main category: cs.LG

TL;DR: 本文研究非线性CCA在白化后恢复真实潜在因子的条件，证明了仿射可识别性，表明白化的重要性，还证明正则化经验CCA收敛性并通过实验验证理论。


<details>
  <summary>Details</summary>
Motivation: 确定非线性CCA在白化后恢复真实潜在因子的条件。

Method: 基于经典结果，利用重新参数化将分析从观测空间转移到源空间；证明正则化经验CCA收敛到总体对应物。

Result: 证明了仿射可识别性，表明白化对保证有界性和良态性的重要性，正则化经验CCA收敛到总体对应物。

Conclusion: 通过实验验证了理论，并通过消融实验证明假设的必要性。

Abstract: In this work, we establish conditions under which nonlinear CCA recovers the
ground-truth latent factors up to an orthogonal transform after whitening.
Building on the classical result that linear mappings maximize canonical
correlations under Gaussian priors, we prove affine identifiability for a broad
class of latent distributions in the population setting. Central to our proof
is a reparameterization result that transports the analysis from observation
space to source space, where identifiability becomes tractable. We further show
that whitening is essential for ensuring boundedness and well-conditioning,
thereby underpinning identifiability. Beyond the population setting, we prove
that ridge-regularized empirical CCA converges to its population counterpart,
transferring these guarantees to the finite-sample regime. Experiments on a
controlled synthetic dataset and a rendered image dataset validate our theory
and demonstrate the necessity of its assumptions through systematic ablations.

</details>


### [382] [ParallelBench: Understanding the Trade-offs of Parallel Decoding in Diffusion LLMs](https://arxiv.org/abs/2510.04767)
*Wonjun Kang,Kevin Galim,Seunghyuk Oh,Minjae Lee,Yuchen Zeng,Shuibai Zhang,Coleman Hooper,Yuezhou Hu,Hyung Il Koo,Nam Ik Cho,Kangwook Lee*

Main category: cs.LG

TL;DR: 文章指出扩散大语言模型（dLLMs）并行解码有加速推理潜力，但存在忽略标记依赖致质量下降问题。提出首个针对dLLMs的基准测试ParallelBench，分析发现dLLMs并行解码质量下降，现有策略难平衡速度与质量，需创新解码方法。


<details>
  <summary>Details</summary>
Motivation: 现有工作忽视dLLMs并行解码中因条件独立假设导致的质量下降问题，标准基准测试不足以衡量该问题，故开展研究。

Method: 先进行并行解码的信息论分析，再对合成列表操作进行案例研究，提出ParallelBench基准测试开展分析。

Result: dLLMs并行解码在现实场景中质量显著下降，当前并行解码策略难根据任务难度调整并行度，无法在不牺牲质量下实现有效加速。

Conclusion: 迫切需要创新解码方法克服速度 - 质量权衡问题，发布基准测试以加速高效dLLMs的发展。

Abstract: While most autoregressive LLMs are constrained to one-by-one decoding,
diffusion LLMs (dLLMs) have attracted growing interest for their potential to
dramatically accelerate inference through parallel decoding. Despite this
promise, the conditional independence assumption in dLLMs causes parallel
decoding to ignore token dependencies, inevitably degrading generation quality
when these dependencies are strong. However, existing works largely overlook
these inherent challenges, and evaluations on standard benchmarks (e.g., math
and coding) are not sufficient to capture the quality degradation caused by
parallel decoding. To address this gap, we first provide an
information-theoretic analysis of parallel decoding. We then conduct case
studies on analytically tractable synthetic list operations from both data
distribution and decoding strategy perspectives, offering quantitative insights
that highlight the fundamental limitations of parallel decoding. Building on
these insights, we propose ParallelBench, the first benchmark specifically
designed for dLLMs, featuring realistic tasks that are trivial for humans and
autoregressive LLMs yet exceptionally challenging for dLLMs under parallel
decoding. Using ParallelBench, we systematically analyze both dLLMs and
autoregressive LLMs, revealing that: (i) dLLMs under parallel decoding can
suffer dramatic quality degradation in real-world scenarios, and (ii) current
parallel decoding strategies struggle to adapt their degree of parallelism
based on task difficulty, thus failing to achieve meaningful speedup without
compromising quality. Our findings underscore the pressing need for innovative
decoding methods that can overcome the current speed-quality trade-off. We
release our benchmark to help accelerate the development of truly efficient
dLLMs.

</details>


### [383] [Distribution Preference Optimization: A Fine-grained Perspective for LLM Unlearning](https://arxiv.org/abs/2510.04773)
*Kai Qin,Jiaqi Wu,Jianxiang He,Haoyuan Sun,Yifei Zhao,Bin Liang,Yongzhe Chang,Tiantian Zhang,Houde Liu*

Main category: cs.LG

TL;DR: 针对大语言模型数据隐私安全问题，提出分布偏好优化（DiPO）算法解决基于优化方法的不足，实验证明其在模型效用和遗忘质量间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 大语言模型数据隐私和安全受关注，基于优化的主流遗忘方法NPO因缺乏明确正偏好信号，泛化性受限。

Method: 将关注点转移到分布层面，通过选择性放大或抑制模型高置信度输出对数，构建DiPO所需偏好分布对。

Result: 理论证明DiPO损失函数与期望遗忘方向一致，实验表明DiPO在模型效用和遗忘质量间平衡良好，在TOFU基准上遗忘质量最高，在MUSE基准上效用保留的可扩展性和可持续性领先。

Conclusion: DiPO算法有效克服了NPO的局限性，能在模型效用和遗忘质量间取得较好权衡。

Abstract: As Large Language Models (LLMs) demonstrate remarkable capabilities learned
from vast corpora, concerns regarding data privacy and safety are receiving
increasing attention. LLM unlearning, which aims to remove the influence of
specific data while preserving overall model utility, is becoming an important
research area. One of the mainstream unlearning classes is optimization-based
methods, which achieve forgetting directly through fine-tuning, exemplified by
Negative Preference Optimization (NPO). However, NPO's effectiveness is limited
by its inherent lack of explicit positive preference signals. Attempts to
introduce such signals by constructing preferred responses often necessitate
domain-specific knowledge or well-designed prompts, fundamentally restricting
their generalizability. In this paper, we shift the focus to the
distribution-level, directly targeting the next-token probability distribution
instead of entire responses, and derive a novel unlearning algorithm termed
\textbf{Di}stribution \textbf{P}reference \textbf{O}ptimization (DiPO). We show
that the requisite preference distribution pairs for DiPO, which are
distributions over the model's output tokens, can be constructed by selectively
amplifying or suppressing the model's high-confidence output logits, thereby
effectively overcoming NPO's limitations. We theoretically prove the
consistency of DiPO's loss function with the desired unlearning direction.
Extensive experiments demonstrate that DiPO achieves a strong trade-off between
model utility and forget quality. Notably, DiPO attains the highest forget
quality on the TOFU benchmark, and maintains leading scalability and
sustainability in utility preservation on the MUSE benchmark.

</details>


### [384] [Learning on the Job: Test-Time Curricula for Targeted Reinforcement Learning](https://arxiv.org/abs/2510.04786)
*Jonas Hübotter,Leander Diaz-Bone,Ido Hakimi,Andreas Krause,Moritz Hardt*

Main category: cs.LG

TL;DR: 提出测试时间课程（TTC - RL）的代理，通过强化学习在测试时持续训练模型，实验表明其能提升模型在目标任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 探索模型能否像人类一样在工作中学习，避免耗时的人工数据集整理。

Method: 提出组装特定任务课程的代理TTC - RL，运用强化学习持续训练模型，自动从大量训练数据中选择最相关数据。

Result: 在多种评估和模型中，TTC - RL能持续提升模型在目标任务上的表现，如在数学和编码基准测试中显著提高Qwen3 - 8B的通过率。

Conclusion: 测试时间课程有潜力将测试时间扩展范式延伸到测试时对数千个相关任务经验的持续训练。

Abstract: Humans are good at learning on the job: We learn how to solve the tasks we
face as we go along. Can a model do the same? We propose an agent that
assembles a task-specific curriculum, called test-time curriculum (TTC-RL), and
applies reinforcement learning to continue training the model for its target
task. The test-time curriculum avoids time-consuming human curation of datasets
by automatically selecting the most task-relevant data from a large pool of
available training data. Our experiments demonstrate that reinforcement
learning on a test-time curriculum consistently improves the model on its
target tasks, across a variety of evaluations and models. Notably, on
challenging math and coding benchmarks, TTC-RL improves the pass@1 of Qwen3-8B
by approximately 1.8x on AIME25 and 2.1x on CodeElo. Moreover, we find that
TTC-RL significantly raises the performance ceiling compared to the initial
model, increasing pass@8 on AIME25 from 40% to 62% and on CodeElo from 28% to
43%. Our findings show the potential of test-time curricula in extending the
test-time scaling paradigm to continual training on thousands of task-relevant
experiences during test-time.

</details>


### [385] [On Predicting Post-Click Conversion Rate via Counterfactual Inference](https://arxiv.org/abs/2510.04816)
*Junhyung Ahn,Sanghack Lee*

Main category: cs.LG

TL;DR: 文章提出Entire Space Counterfactual Inference Multi - task Model (ESCIM)方法，利用因果关系为未点击样本生成反事实转化标签，实验证明其有效性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有CVR预测模型基于点击样本训练，点击实例稀疏需大量日志，利用未点击样本的框架常依赖启发式方法，存在偏差。

Method: 先训练用户序列行为的结构因果模型，对未点击物品进行假设干预以推断反事实CVR，再将预测的反事实CVR转化为二进制反事实转化标签，最后将生成样本纳入训练。

Result: 在公共数据集上的大量实验证明算法优越性，在线A/B测试验证了在真实场景的有效性，在潜在转化数据上表现也有提升。

Conclusion: 提出的ESCIM方法有效，具有鲁棒性和优越的泛化能力。

Abstract: Accurately predicting conversion rate (CVR) is essential in various
recommendation domains such as online advertising systems and e-commerce. These
systems utilize user interaction logs, which consist of exposures, clicks, and
conversions. CVR prediction models are typically trained solely based on
clicked samples, as conversions can only be determined following clicks.
However, the sparsity of clicked instances necessitates the collection of a
substantial amount of logs for effective model training. Recent works address
this issue by devising frameworks that leverage non-clicked samples. While
these frameworks aim to reduce biases caused by the discrepancy between clicked
and non-clicked samples, they often rely on heuristics. Against this
background, we propose a method to counterfactually generate conversion labels
for non-clicked samples by using causality as a guiding principle, attempting
to answer the question, "Would the user have converted if he or she had clicked
the recommended item?" Our approach is named the Entire Space Counterfactual
Inference Multi-task Model (ESCIM). We initially train a structural causal
model (SCM) of user sequential behaviors and conduct a hypothetical
intervention (i.e., click) on non-clicked items to infer counterfactual CVRs.
We then introduce several approaches to transform predicted counterfactual CVRs
into binary counterfactual conversion labels for the non-clicked samples.
Finally, the generated samples are incorporated into the training process.
Extensive experiments on public datasets illustrate the superiority of the
proposed algorithm. Online A/B testing further empirically validates the
effectiveness of our proposed algorithm in real-world scenarios. In addition,
we demonstrate the improved performance of the proposed method on latent
conversion data, showcasing its robustness and superior generalization
capabilities.

</details>


### [386] [On the Hardness of Learning Regular Expressions](https://arxiv.org/abs/2510.04834)
*Idan Attias,Lev Reyzin,Nathan Srebro,Gal Vardi*

Main category: cs.LG

TL;DR: 研究正则表达式在PAC模型和带成员查询下的学习计算复杂度，证明多种情况下学习的困难性。


<details>
  <summary>Details</summary>
Motivation: 正则表达式理论意义重大且应用广泛，但学习的计算复杂度研究不足。

Method: 在PAC模型和带成员查询的情况下研究正则表达式的学习难度，考虑不同分布和扩展情况。

Result: 证明在超立方体均匀分布下PAC学习困难，无分布学习带成员查询困难，扩展正则表达式在均匀分布下带成员查询学习困难。

Conclusion: 这些结果与已有DFA或NFA学习的困难性结果不同，正则语言在不同表示下描述复杂度有指数差异。

Abstract: Despite the theoretical significance and wide practical use of regular
expressions, the computational complexity of learning them has been largely
unexplored. We study the computational hardness of improperly learning regular
expressions in the PAC model and with membership queries. We show that PAC
learning is hard even under the uniform distribution on the hypercube, and also
prove hardness of distribution-free learning with membership queries.
Furthermore, if regular expressions are extended with complement or
intersection, we establish hardness of learning with membership queries even
under the uniform distribution. We emphasize that these results do not follow
from existing hardness results for learning DFAs or NFAs, since the descriptive
complexity of regular languages can differ exponentially between DFAs, NFAs,
and regular expressions.

</details>


### [387] [Bond-Centered Molecular Fingerprint Derivatives: A BBBP Dataset Study](https://arxiv.org/abs/2510.04837)
*Guillaume Godin*

Main category: cs.LG

TL;DR: 介绍静态BCFP，评估其与ECFP结合用于BBBP分类任务，提出BCFP - Sort&Slice方案，结果表明键中心描述符可补充原子中心指纹用于BBBP预测。


<details>
  <summary>Details</summary>
Motivation: 引入一种互补的键中心指纹BCFP，用于改善脑血屏障穿透性（BBBP）分类任务的预测效果。

Method: 引入静态BCFP，用随机森林模型评估，采用Turkey HSD多重比较分析，提出BCFP - Sort&Slice特征组合方案。

Result: ECFP与BCFP结合在分层交叉验证中持续提高AUROC和AUPRC；r = 1表现最佳；新特征组合在BBBP评估中优于MGTP预测。

Conclusion: 轻量级的键中心描述符可补充原子中心圆形指纹，为BBBP预测提供强大、快速的基线。

Abstract: Bond Centered FingerPrint (BCFP) are a complementary, bond-centric
alternative to Extended-Connectivity Fingerprints (ECFP). We introduce a static
BCFP that mirrors the bond-convolution used by directed message-passing GNNs
like ChemProp, and evaluate it with a fast rapid Random Forest model on
Brain-Blood Barrier Penetration (BBBP) classification task. Across stratified
cross-validation, concatenating ECFP with BCFP consistently improves AUROC and
AUPRC over either descriptor alone, as confirmed by Turkey HSD
multiple-comparison analysis. Among radii, r = 1 performs best; r = 2 does not
yield statistically separable gains under the same test. We further propose
BCFP-Sort&Slice, a simple feature-combination scheme that preserves the
out-of-vocabulary (OOV) count information native to ECFP count vectors while
enabling compact unhashed concatenation of BCFP variants. We also outperform
the MGTP prediction on our BBBP evaluation, using such composite new features
bond and atom features. These results show that lightweight, bond-centered
descriptors can complement atom-centered circular fingerprints and provide
strong, fast baselines for BBBP prediction.

</details>


### [388] [Distributionally Robust Causal Abstractions](https://arxiv.org/abs/2510.04842)
*Yorgos Felekis,Theodoros Damoulas,Paris Giampouras*

Main category: cs.LG

TL;DR: 本文提出了第一类分布鲁棒的因果抽象（CA）及其学习算法，解决了现有方法在环境变化和模型错误指定时的局限性，通过理论和实证验证了其鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有学习CA的方法假设固定且明确的外生分布，易受环境变化和错误指定的影响，本文旨在解决这些局限性。

Method: 将鲁棒因果抽象学习转化为具有Wasserstein模糊集的约束最小 - 最大优化问题，在经验和高斯环境下给出理论结果。

Result: 通过不同问题和CA学习方法的实证表明，框架不仅对环境变化具有鲁棒性，还对结构模型和干预映射的错误指定具有鲁棒性。

Conclusion: 所提出的分布鲁棒CA及其学习算法有效，能增强CA在复杂现实场景中的鲁棒性。

Abstract: Causal Abstraction (CA) theory provides a principled framework for relating
causal models that describe the same system at different levels of granularity
while ensuring interventional consistency between them. Recently, several
approaches for learning CAs have been proposed, but all assume fixed and
well-specified exogenous distributions, making them vulnerable to environmental
shifts and misspecification. In this work, we address these limitations by
introducing the first class of distributionally robust CAs and their associated
learning algorithms. The latter cast robust causal abstraction learning as a
constrained min-max optimization problem with Wasserstein ambiguity sets. We
provide theoretical results, for both empirical and Gaussian environments,
leading to principled selection of the level of robustness via the radius of
these sets. Furthermore, we present empirical evidence across different
problems and CA learning methods, demonstrating our framework's robustness not
only to environmental shifts but also to structural model and intervention
mapping misspecification.

</details>


### [389] [Synthesising Counterfactual Explanations via Label-Conditional Gaussian Mixture Variational Autoencoders](https://arxiv.org/abs/2510.04855)
*Junqi Jiang,Francesco Leofante,Antonio Rago,Francesca Toni*

Main category: cs.LG

TL;DR: 提出LAPACE框架生成反事实解释，确保鲁棒性、合理性和多样性，实验证明其计算高效且表现好。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以统一、模型无关地满足生成反事实解释的多方面要求，如鲁棒性、合理性和多样性。

Method: 提出Label - conditional Gaussian Mixture Variational Autoencoder (L - GMVAE)学习结构化潜在空间，在此基础上提出LAPACE算法，通过插值生成反事实解释点路径。

Result: LAPACE计算高效，在八个定量指标上有有竞争力的表现。

Conclusion: 所提的LAPACE框架能有效解决生成反事实解释的多方面问题，具有良好性能。

Abstract: Counterfactual explanations (CEs) provide recourse recommendations for
individuals affected by algorithmic decisions. A key challenge is generating
CEs that are robust against various perturbation types (e.g. input and model
perturbations) while simultaneously satisfying other desirable properties.
These include plausibility, ensuring CEs reside on the data manifold, and
diversity, providing multiple distinct recourse options for single inputs.
Existing methods, however, mostly struggle to address these multifaceted
requirements in a unified, model-agnostic manner. We address these limitations
by proposing a novel generative framework. First, we introduce the
Label-conditional Gaussian Mixture Variational Autoencoder (L-GMVAE), a model
trained to learn a structured latent space where each class label is
represented by a set of Gaussian components with diverse, prototypical
centroids. Building on this, we present LAPACE (LAtent PAth Counterfactual
Explanations), a model-agnostic algorithm that synthesises entire paths of CE
points by interpolating from inputs' latent representations to those learned
latent centroids. This approach inherently ensures robustness to input changes,
as all paths for a given target class converge to the same fixed centroids.
Furthermore, the generated paths provide a spectrum of recourse options,
allowing users to navigate the trade-off between proximity and plausibility
while also encouraging robustness against model changes. In addition,
user-specified actionability constraints can also be easily incorporated via
lightweight gradient optimisation through the L-GMVAE's decoder. Comprehensive
experiments show that LAPACE is computationally efficient and achieves
competitive performance across eight quantitative metrics.

</details>


### [390] [Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the Rails](https://arxiv.org/abs/2510.04860)
*Siwei Han,Jiaqi Liu,Yaofeng Su,Wenbo Duan,Xinyuan Liu,Cihang Xie,Mohit Bansal,Mingyu Ding,Linjun Zhang,Huaxiu Yao*

Main category: cs.LG

TL;DR: 研究自进化大语言模型代理的对齐风险，发现对齐是动态且脆弱的，当前方法防御不足。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型代理自进化能力增强，其长期可靠性受关注，需研究部署后独特的对齐风险。

Method: 通过自利探索和模仿策略扩散两个范式分析对齐倾斜过程，构建可控测试平台对模型进行基准测试。

Result: 自进化下对齐收益快速侵蚀，多智能体中违规行为扩散致集体失准，当前强化学习对齐方法防御脆弱。

Conclusion: 大语言模型代理的对齐是脆弱且动态的，易在部署中因反馈而衰减。

Abstract: As Large Language Model (LLM) agents increasingly gain self-evolutionary
capabilities to adapt and refine their strategies through real-world
interaction, their long-term reliability becomes a critical concern. We
identify the Alignment Tipping Process (ATP), a critical post-deployment risk
unique to self-evolving LLM agents. Unlike training-time failures, ATP arises
when continual interaction drives agents to abandon alignment constraints
established during training in favor of reinforced, self-interested strategies.
We formalize and analyze ATP through two complementary paradigms:
Self-Interested Exploration, where repeated high-reward deviations induce
individual behavioral drift, and Imitative Strategy Diffusion, where deviant
behaviors spread across multi-agent systems. Building on these paradigms, we
construct controllable testbeds and benchmark Qwen3-8B and
Llama-3.1-8B-Instruct. Our experiments show that alignment benefits erode
rapidly under self-evolution, with initially aligned models converging toward
unaligned states. In multi-agent settings, successful violations diffuse
quickly, leading to collective misalignment. Moreover, current reinforcement
learning-based alignment methods provide only fragile defenses against
alignment tipping. Together, these findings demonstrate that alignment of LLM
agents is not a static property but a fragile and dynamic one, vulnerable to
feedback-driven decay during deployment. Our data and code are available at
https://github.com/aiming-lab/ATP.

</details>


### [391] [A Clinical-grade Universal Foundation Model for Intraoperative Pathology](https://arxiv.org/abs/2510.04861)
*Zihan Zhao,Fengtao Zhou,Ronggang Li,Bing Chu,Xinke Zhang,Xueyi Zheng,Ke Zheng,Xiaobo Wen,Jiabo Ma,Yihui Wang,Jiewei Chen,Chengyou Zheng,Jiangyu Zhang,Yongqin Wen,Jiajia Meng,Ziqi Zeng,Xiaoqing Li,Jing Li,Dan Xie,Yaping Ye,Yu Wang,Hao Chen,Muyan Cai*

Main category: cs.LG

TL;DR: 介绍临床级基础模型CRISP，它基于大量冷冻切片数据，经多任务评估展示出良好泛化性，在真实场景中诊断准确，人机协作有诸多优势，推动AI用于术中病理。


<details>
  <summary>Details</summary>
Motivation: 术中病理对精准手术重要，但诊断复杂且高质量冷冻切片数据有限，计算病理学缺乏大规模前瞻性验证阻碍其在手术流程中常规应用。

Method: 基于八个医疗中心超10万张冷冻切片开发CRISP模型，在超1.5万张术中切片上进行近100项回顾性诊断任务评估，在超2000名患者的前瞻性队列中验证。

Result: 模型在不同机构、肿瘤类型和解剖部位有良好泛化性；前瞻性队列中保持高诊断准确率，92.6%的病例指导手术决策；人机协作减少35%诊断工作量，避免105项辅助检查，87.5%准确检测微转移。

Conclusion: CRISP是AI驱动术中病理的临床级范例，弥合计算进展与手术精准度的差距，加速AI向常规临床实践转化。

Abstract: Intraoperative pathology is pivotal to precision surgery, yet its clinical
impact is constrained by diagnostic complexity and the limited availability of
high-quality frozen-section data. While computational pathology has made
significant strides, the lack of large-scale, prospective validation has
impeded its routine adoption in surgical workflows. Here, we introduce CRISP, a
clinical-grade foundation model developed on over 100,000 frozen sections from
eight medical centers, specifically designed to provide Clinical-grade Robust
Intraoperative Support for Pathology (CRISP). CRISP was comprehensively
evaluated on more than 15,000 intraoperative slides across nearly 100
retrospective diagnostic tasks, including benign-malignant discrimination, key
intraoperative decision-making, and pan-cancer detection, etc. The model
demonstrated robust generalization across diverse institutions, tumor types,
and anatomical sites-including previously unseen sites and rare cancers. In a
prospective cohort of over 2,000 patients, CRISP sustained high diagnostic
accuracy under real-world conditions, directly informing surgical decisions in
92.6% of cases. Human-AI collaboration further reduced diagnostic workload by
35%, avoided 105 ancillary tests and enhanced detection of micrometastases with
87.5% accuracy. Together, these findings position CRISP as a clinical-grade
paradigm for AI-driven intraoperative pathology, bridging computational
advances with surgical precision and accelerating the translation of artificial
intelligence into routine clinical practice.

</details>


### [392] [Less is More: Recursive Reasoning with Tiny Networks](https://arxiv.org/abs/2510.04871)
*Alexia Jolicoeur-Martineau*

Main category: cs.LG

TL;DR: 提出比Hierarchical Reasoning Model (HRM)更简单且泛化性高的Tiny Recursive Model (TRM)，小参数下在任务上效果超多数大语言模型。


<details>
  <summary>Details</summary>
Motivation: HRM虽有解决难题潜力，但未被充分理解且可能不是最优，需更优方法。

Method: 提出TRM，采用单个仅2层的微小网络进行递归推理。

Result: TRM仅7M参数，在ARC - AGI - 1上测试准确率达45%，ARC - AGI - 2上达8%，高于多数大语言模型。

Conclusion: TRM以简单结构和极少参数在难题任务上取得比HRM和多数大语言模型更好的泛化性能。

Abstract: Hierarchical Reasoning Model (HRM) is a novel approach using two small neural
networks recursing at different frequencies. This biologically inspired method
beats Large Language models (LLMs) on hard puzzle tasks such as Sudoku, Maze,
and ARC-AGI while trained with small models (27M parameters) on small data
(around 1000 examples). HRM holds great promise for solving hard problems with
small networks, but it is not yet well understood and may be suboptimal. We
propose Tiny Recursive Model (TRM), a much simpler recursive reasoning approach
that achieves significantly higher generalization than HRM, while using a
single tiny network with only 2 layers. With only 7M parameters, TRM obtains
45% test-accuracy on ARC-AGI-1 and 8% on ARC-AGI-2, higher than most LLMs
(e.g., Deepseek R1, o3-mini, Gemini 2.5 Pro) with less than 0.01% of the
parameters.

</details>


### [393] [Flow-Matching Based Refiner for Molecular Conformer Generation](https://arxiv.org/abs/2510.04878)
*Xiangyang Xu,Hongyang Gao*

Main category: cs.LG

TL;DR: 提出用于分子构象生成的流匹配精炼器，在基准数据集上提升样本质量且保留多样性。


<details>
  <summary>Details</summary>
Motivation: 基于去噪的分子构象生成方法在采样时存在误差累积问题，低信噪比步骤难训练。

Method: 提出流匹配精炼器，从上游去噪模型的混合质量输出初始化采样，重新安排噪声尺度以绕过低信噪比阶段。

Result: 在GEOM - QM9和GEOM - Drugs基准数据集上，生成器 - 精炼器管道用更少的去噪步骤提升了样本质量并保留了多样性。

Conclusion: 所提方法能有效解决现有去噪方法在分子构象生成中的问题，提升样本质量。

Abstract: Low-energy molecular conformers generation (MCG) is a foundational yet
challenging problem in drug discovery. Denoising-based methods include
diffusion and flow-matching methods that learn mappings from a simple base
distribution to the molecular conformer distribution. However, these approaches
often suffer from error accumulation during sampling, especially in the low SNR
steps, which are hard to train. To address these challenges, we propose a
flow-matching refiner for the MCG task. The proposed method initializes
sampling from mixed-quality outputs produced by upstream denoising models and
reschedules the noise scale to bypass the low-SNR phase, thereby improving
sample quality. On the GEOM-QM9 and GEOM-Drugs benchmark datasets, the
generator-refiner pipeline improves quality with fewer total denoising steps
while preserving diversity.

</details>


### [394] [Revealing Interconnections between Diseases: from Statistical Methods to Large Language Models](https://arxiv.org/abs/2510.04888)
*Alina Ermilova,Dmitrii Kornilov,Sofia Samoilova,Ekaterina Laptenkova,Anastasia Kolesnikova,Ekaterina Podplutova,Senotrusova Sofya,Maksim G. Sharaev*

Main category: cs.LG

TL;DR: 手动分析临床数据识别疾病关联存在问题，机器学习有挑战，大语言模型缺乏医学知识。本文评估七种方法，发现大语言模型发现新关联潜力有限，结果可作医学疾病本体资源。


<details>
  <summary>Details</summary>
Motivation: 手动分析大规模临床数据识别疾病关联存在劳动密集、主观和易分歧等问题，机器学习有三大挑战，大语言模型缺乏专业医学知识，需解决这些问题。

Method: 基于MIMIC - IV电子健康记录的ICD - 10代码序列和完整ICD - 10代码（有无文本描述）两种数据源，对七种方法进行系统评估，框架整合统计共现分析、掩码语言建模、特定领域BERT变体、通用BERT和文档检索以及四种大语言模型。

Result: 基于图的互联矩阵比较显示，大语言模型方法产生的ICD代码与不同疾病连接的多样性最低。

Conclusion: 大语言模型发现新互联的潜力有限，研究结果可作为有价值的医学疾病本体，为未来临床研究和医疗人工智能应用提供基础资源。

Abstract: Identifying disease interconnections through manual analysis of large-scale
clinical data is labor-intensive, subjective, and prone to expert disagreement.
While machine learning (ML) shows promise, three critical challenges remain:
(1) selecting optimal methods from the vast ML landscape, (2) determining
whether real-world clinical data (e.g., electronic health records, EHRs) or
structured disease descriptions yield more reliable insights, (3) the lack of
"ground truth," as some disease interconnections remain unexplored in medicine.
Large language models (LLMs) demonstrate broad utility, yet they often lack
specialized medical knowledge. To address these gaps, we conduct a systematic
evaluation of seven approaches for uncovering disease relationships based on
two data sources: (i) sequences of ICD-10 codes from MIMIC-IV EHRs and (ii) the
full set of ICD-10 codes, both with and without textual descriptions. Our
framework integrates the following: (i) a statistical co-occurrence analysis
and a masked language modeling (MLM) approach using real clinical data; (ii)
domain-specific BERT variants (Med-BERT and BioClinicalBERT); (iii) a
general-purpose BERT and document retrieval; and (iv) four LLMs (Mistral,
DeepSeek, Qwen, and YandexGPT). Our graph-based comparison of the obtained
interconnection matrices shows that the LLM-based approach produces
interconnections with the lowest diversity of ICD code connections to different
diseases compared to other methods, including text-based and domain-based
approaches. This suggests an important implication: LLMs have limited potential
for discovering new interconnections. In the absence of ground truth databases
for medical interconnections between ICD codes, our results constitute a
valuable medical disease ontology that can serve as a foundational resource for
future clinical research and artificial intelligence applications in
healthcare.

</details>


### [395] [Benchmarking M-LTSF: Frequency and Noise-Based Evaluation of Multivariate Long Time Series Forecasting Models](https://arxiv.org/abs/2510.04900)
*Nick Janßen,Melanie Schaller,Bodo Rosenhahn*

Main category: cs.LG

TL;DR: 提出基于模拟的评估框架生成可参数化合成数据集，对M - LTSF模型进行基准测试，揭示模型优缺点并为模型选择提供指导。


<details>
  <summary>Details</summary>
Motivation: 现有对多变量长期时间序列预测（M - LTSF）深度学习模型鲁棒性的评估依赖真实数据集，其噪声特性未知，难以准确评估。

Method: 提出基于模拟的评估框架生成可参数化合成数据集，对S - Mamba、iTransformer、R - Linear和Autoformer四种模型进行基准测试，并进行频谱分析。

Result: 所有模型在回溯窗口无法捕获数据季节性模式完整周期时性能严重下降；不同模型在不同信号模式和噪声类型下表现不同；S - Mamba和iTransformer频率重建效果更好。

Conclusion: 基于合成和原理驱动的测试平台的评估方法，通过聚合MSE分数深入了解模型优缺点，可为基于信号特征和噪声条件的模型选择提供具体指导。

Abstract: Understanding the robustness of deep learning models for multivariate
long-term time series forecasting (M-LTSF) remains challenging, as evaluations
typically rely on real-world datasets with unknown noise properties. We propose
a simulation-based evaluation framework that generates parameterizable
synthetic datasets, where each dataset instance corresponds to a different
configuration of signal components, noise types, signal-to-noise ratios, and
frequency characteristics. These configurable components aim to model
real-world multivariate time series data without the ambiguity of unknown
noise. This framework enables fine-grained, systematic evaluation of M-LTSF
models under controlled and diverse scenarios. We benchmark four representative
architectures S-Mamba (state-space), iTransformer (transformer-based), R-Linear
(linear), and Autoformer (decomposition-based). Our analysis reveals that all
models degrade severely when lookback windows cannot capture complete periods
of seasonal patters in the data. S-Mamba and Autoformer perform best on
sawtooth patterns, while R-Linear and iTransformer favor sinusoidal signals.
White and Brownian noise universally degrade performance with lower
signal-to-noise ratio while S-Mamba shows specific trend-noise and iTransformer
shows seasonal-noise vulnerability. Further spectral analysis shows that
S-Mamba and iTransformer achieve superior frequency reconstruction. This
controlled approach, based on our synthetic and principle-driven testbed,
offers deeper insights into model-specific strengths and limitations through
the aggregation of MSE scores and provides concrete guidance for model
selection based on signal characteristics and noise conditions.

</details>


### [396] [Focused Skill Discovery: Learning to Control Specific State Variables while Minimizing Side Effects](https://arxiv.org/abs/2510.04901)
*Jonathan Colaço Carr,Qinyi Sun,Cameron Allen*

Main category: cs.LG

TL;DR: 现有技能发现算法常忽视自然状态变量，本文引入方法使算法学习聚焦技能，提升状态空间覆盖、解锁新学习能力并避免下游任务副作用。


<details>
  <summary>Details</summary>
Motivation: 现有技能发现算法常忽视自然状态变量，导致发现的技能缺乏对特定状态变量的控制，影响探索效率、学习难度及下游任务。

Method: 引入一种通用方法，使技能发现算法能够学习聚焦特定状态变量的技能。

Result: 状态空间覆盖提高三倍，解锁新学习能力，自动避免下游任务的负面副作用。

Conclusion: 所提出的方法能有效解决现有技能发现算法的问题，提升性能。

Abstract: Skills are essential for unlocking higher levels of problem solving. A common
approach to discovering these skills is to learn ones that reliably reach
different states, thus empowering the agent to control its environment.
However, existing skill discovery algorithms often overlook the natural state
variables present in many reinforcement learning problems, meaning that the
discovered skills lack control of specific state variables. This can
significantly hamper exploration efficiency, make skills more challenging to
learn with, and lead to negative side effects in downstream tasks when the goal
is under-specified. We introduce a general method that enables these skill
discovery algorithms to learn focused skills -- skills that target and control
specific state variables. Our approach improves state space coverage by a
factor of three, unlocks new learning capabilities, and automatically avoids
negative side effects in downstream tasks.

</details>


### [397] [DP-HYPE: Distributed Differentially Private Hyperparameter Search](https://arxiv.org/abs/2510.04902)
*Johannes Liebenow,Thorsten Peinemann,Esfandiar Mohammadi*

Main category: cs.LG

TL;DR: 论文提出DP - HYPE算法进行分布式隐私保护超参数搜索，证明其满足客户端级差分隐私，给出效用保证界限，在多个数据集上评估显示其高效用。


<details>
  <summary>Details</summary>
Motivation: 分布式机器学习超参数调优在处理敏感数据时存在隐私挑战，现有差分隐私超参数调优方法存在计算昂贵、效用 - 隐私权衡不佳等问题。

Method: 提出DP - HYPE算法，基于客户端本地超参数评估进行分布式投票来选择超参数，将其实现为Flower框架子模块。

Result: 证明DP - HYPE满足客户端级差分隐私，隐私保证不依赖超参数数量，给出效用保证界限，在多个基准数据集的iid和non - iid设置下评估显示即使在小隐私预算下也有高效用。

Conclusion: DP - HYPE算法能在分布式环境下有效进行隐私保护的超参数搜索，具有良好的可扩展性、独立性和高效用。

Abstract: The tuning of hyperparameters in distributed machine learning can
substantially impact model performance. When the hyperparameters are tuned on
sensitive data, privacy becomes an important challenge and to this end,
differential privacy has emerged as the de facto standard for provable privacy.
A standard setting when performing distributed learning tasks is that clients
agree on a shared setup, i.e., find a compromise from a set of hyperparameters,
like the learning rate of the model to be trained. Yet, prior work on
differentially private hyperparameter tuning either uses computationally
expensive cryptographic protocols, determines hyperparameters separately for
each client, or applies differential privacy locally, which can lead to
undesirable utility-privacy trade-offs.
  In this work, we present our algorithm DP-HYPE, which performs a distributed
and privacy-preserving hyperparameter search by conducting a distributed voting
based on local hyperparameter evaluations of clients. In this way, DP-HYPE
selects hyperparameters that lead to a compromise supported by the majority of
clients, while maintaining scalability and independence from specific learning
tasks. We prove that DP-HYPE preserves the strong notion of differential
privacy called client-level differential privacy and, importantly, show that
its privacy guarantees do not depend on the number of hyperparameters. We also
provide bounds on its utility guarantees, that is, the probability of reaching
a compromise, and implement DP-HYPE as a submodule in the popular Flower
framework for distributed machine learning. In addition, we evaluate
performance on multiple benchmark data sets in iid as well as multiple non-iid
settings and demonstrate high utility of DP-HYPE even under small privacy
budgets.

</details>


### [398] [How Different from the Past? Spatio-Temporal Time Series Forecasting with Self-Supervised Deviation Learning](https://arxiv.org/abs/2510.04908)
*Haotian Gao,Zheng Dong,Jiawei Yong,Shintaro Fukushima,Kenjiro Taura,Renhe Jiang*

Main category: cs.LG

TL;DR: 提出时空时间序列预测框架ST - SSDL，结合自监督偏差学习方案，在六个基准数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有时空预测方法常未考虑当前输入与历史模式间的动态偏差，而这些偏差会影响模型性能。

Method: 提出ST - SSDL框架，将输入锚定到历史平均值，用可学习原型离散潜在空间，设置对比损失和偏差损失辅助目标与预测目标联合优化。

Result: 在六个基准数据集上，ST - SSDL在多个指标上始终优于现有基线方法，可视化展示其能适应复杂时空场景的偏差。

Conclusion: ST - SSDL框架有效，能捕捉和利用时空数据中的偏差，提高模型泛化能力。

Abstract: Spatio-temporal forecasting is essential for real-world applications such as
traffic management and urban computing. Although recent methods have shown
improved accuracy, they often fail to account for dynamic deviations between
current inputs and historical patterns. These deviations contain critical
signals that can significantly affect model performance. To fill this gap, we
propose ST-SSDL, a Spatio-Temporal time series forecasting framework that
incorporates a Self-Supervised Deviation Learning scheme to capture and utilize
such deviations. ST-SSDL anchors each input to its historical average and
discretizes the latent space using learnable prototypes that represent typical
spatio-temporal patterns. Two auxiliary objectives are proposed to refine this
structure: a contrastive loss that enhances inter-prototype discriminability
and a deviation loss that regularizes the distance consistency between input
representations and corresponding prototypes to quantify deviation. Optimized
jointly with the forecasting objective, these components guide the model to
organize its hidden space and improve generalization across diverse input
conditions. Experiments on six benchmark datasets show that ST-SSDL
consistently outperforms state-of-the-art baselines across multiple metrics.
Visualizations further demonstrate its ability to adaptively respond to varying
levels of deviation in complex spatio-temporal scenarios. Our code and datasets
are available at https://github.com/Jimmy-7664/ST-SSDL.

</details>


### [399] [Glocal Information Bottleneck for Time Series Imputation](https://arxiv.org/abs/2510.04910)
*Jie Yang,Kexin Zhang,Guibin Zhang,Philip S. Yu,Kaize Ding*

Main category: cs.LG

TL;DR: 现有时间序列插补模型在高缺失率下存在优化困境，本文提出Glocal - IB训练范式，实验证明其能提升性能和对齐潜在表示。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列插补模型在高缺失率下训练和推理表现差异大，缺乏全局指导，易过拟合局部噪声，无法捕捉数据全局信息。

Method: 提出Glocal - IB训练范式，它与模型无关，通过引入基于易处理互信息近似的全局对齐损失扩展标准IB框架，使掩码输入和原始观测输入的潜在表示对齐。

Result: 在九个数据集上的广泛实验表明，Glocal - IB在缺失情况下能持续提升性能并对齐潜在表示。

Conclusion: Glocal - IB训练范式能帮助模型在高缺失率下保留全局结构和局部细节，抑制缺失值噪声，实现更好的泛化。

Abstract: Time Series Imputation (TSI), which aims to recover missing values in
temporal data, remains a fundamental challenge due to the complex and often
high-rate missingness in real-world scenarios. Existing models typically
optimize the point-wise reconstruction loss, focusing on recovering numerical
values (local information). However, we observe that under high missing rates,
these models still perform well in the training phase yet produce poor
imputations and distorted latent representation distributions (global
information) in the inference phase. This reveals a critical optimization
dilemma: current objectives lack global guidance, leading models to overfit
local noise and fail to capture global information of the data. To address this
issue, we propose a new training paradigm, Glocal Information Bottleneck
(Glocal-IB). Glocal-IB is model-agnostic and extends the standard IB framework
by introducing a Global Alignment loss, derived from a tractable mutual
information approximation. This loss aligns the latent representations of
masked inputs with those of their originally observed counterparts. It helps
the model retain global structure and local details while suppressing noise
caused by missing values, giving rise to better generalization under high
missingness. Extensive experiments on nine datasets confirm that Glocal-IB
leads to consistently improved performance and aligned latent representations
under missingness. Our code implementation is available in
https://github.com/Muyiiiii/NeurIPS-25-Glocal-IB.

</details>


### [400] [Federated Self-Supervised Learning for Automatic Modulation Classification under Non-IID and Class-Imbalanced Data](https://arxiv.org/abs/2510.04927)
*Usman Akram,Yiyue Chen,Haris Vikalo*

Main category: cs.LG

TL;DR: 提出FedSSL - AMC方法用于自动调制分类，在合成和空中数据集上有增益。


<details>
  <summary>Details</summary>
Motivation: 集中聚合数据训练AMC模型有隐私、通信等问题，传统联邦学习对类别不平衡等情况敏感。

Method: 提出FedSSL - AMC，在未标记I/Q序列上用三元组损失自监督训练因果、时间膨胀CNN，在小标记集上用每个客户端的SVM。

Result: 在合成和空中数据集实验显示，在异构SNR、载波频率偏移和非IID标签分区下比有监督的联邦学习基线有持续增益。

Conclusion: FedSSL - AMC方法有效，能应对多种复杂情况。

Abstract: Training automatic modulation classification (AMC) models on centrally
aggregated data raises privacy concerns, incurs communication overhead, and
often fails to confer robustness to channel shifts. Federated learning (FL)
avoids central aggregation by training on distributed clients but remains
sensitive to class imbalance, non-IID client distributions, and limited labeled
samples. We propose FedSSL-AMC, which trains a causal, time-dilated CNN with
triplet-loss self-supervision on unlabeled I/Q sequences across clients,
followed by per-client SVMs on small labeled sets. We establish convergence of
the federated representation learning procedure and a separability guarantee
for the downstream classifier under feature noise. Experiments on synthetic and
over-the-air datasets show consistent gains over supervised FL baselines under
heterogeneous SNR, carrier-frequency offsets, and non-IID label partitions.

</details>


### [401] [Egalitarian Gradient Descent: A Simple Approach to Accelerated Grokking](https://arxiv.org/abs/2510.04930)
*Ali Saheb Pasand,Elvis Dohmatob*

Main category: cs.LG

TL;DR: 本文研究模型学习中的Grokking现象，提出平权梯度下降法（EGD）以加快学习进程，在经典算术问题上消除了性能停滞期。


<details>
  <summary>Details</summary>
Motivation: 实际中希望减少模型学习中性能停滞期的长度，加快学习进程。

Method: 通过实验和理论证明Grokking可由梯度在不同主方向上的不对称速度引起，提出对梯度进行归一化的平权梯度下降法（EGD），使各主方向动态以相同速度演化。

Result: EGD方法能更快地实现Grokking，在某些情况下完全消除了停滞期，在经典算术问题上消除了性能停滞期。

Conclusion: 提出的EGD方法能有效解决模型学习中的性能停滞问题，加快学习进程。

Abstract: Grokking is the phenomenon whereby, unlike the training performance, which
peaks early in the training process, the test/generalization performance of a
model stagnates over arbitrarily many epochs and then suddenly jumps to usually
close to perfect levels. In practice, it is desirable to reduce the length of
such plateaus, that is to make the learning process "grok" faster. In this
work, we provide new insights into grokking. First, we show both empirically
and theoretically that grokking can be induced by asymmetric speeds of
(stochastic) gradient descent, along different principal (i.e singular
directions) of the gradients. We then propose a simple modification that
normalizes the gradients so that dynamics along all the principal directions
evolves at exactly the same speed. Then, we establish that this modified
method, which we call egalitarian gradient descent (EGD) and can be seen as a
carefully modified form of natural gradient descent, groks much faster. In
fact, in some cases the stagnation is completely removed. Finally, we
empirically show that on classical arithmetic problems such as modular addition
and sparse parity problem which this stagnation has been widely observed and
intensively studied, that our proposed method eliminates the plateaus.

</details>


### [402] [ONNX-Net: Towards Universal Representations and Instant Performance Prediction for Neural Architectures](https://arxiv.org/abs/2510.04938)
*Shiwen Qin,Alexander Auras,Shay B. Cohen,Elliot J. Crowley,Michael Moeller,Linus Ericsson,Jovita Lukasik*

Main category: cs.LG

TL;DR: 提出ONNX - Bench基准，实现共享神经网络表示ONNX - Net，在不同搜索空间有强零样本性能，可即时评估任意网络架构。


<details>
  <summary>Details</summary>
Motivation: 现有NAS快速评估方法多受限于特定搜索空间和网络表示，缺乏灵活性和可扩展性，本文旨在消除这些限制。

Method: 构建基于ONNX文件的统一格式基准ONNX - Bench，创建能以自然语言描述表示任意架构的ONNX - Net作为性能预测器输入。

Result: 实验显示在不同搜索空间仅用少量预训练样本就有强零样本性能，能即时评估任意神经网络架构。

Conclusion: ONNX - Bench和ONNX - Net解决了现有NAS评估方法的局限性，可跨搜索空间评估任意神经网络架构。

Abstract: Neural architecture search (NAS) automates the design process of
high-performing architectures, but remains bottlenecked by expensive
performance evaluation. Most existing studies that achieve faster evaluation
are mostly tied to cell-based search spaces and graph encodings tailored to
those individual search spaces, limiting their flexibility and scalability when
applied to more expressive search spaces. In this work, we aim to close the gap
of individual search space restrictions and search space dependent network
representations. We present ONNX-Bench, a benchmark consisting of a collection
of neural networks in a unified format based on ONNX files. ONNX-Bench includes
all open-source NAS-bench-based neural networks, resulting in a total size of
more than 600k {architecture, accuracy} pairs. This benchmark allows creating a
shared neural network representation, ONNX-Net, able to represent any neural
architecture using natural language descriptions acting as an input to a
performance predictor. This text-based encoding can accommodate arbitrary layer
types, operation parameters, and heterogeneous topologies, enabling a single
surrogate to generalise across all neural architectures rather than being
confined to cell-based search spaces. Experiments show strong zero-shot
performance across disparate search spaces using only a small amount of
pretraining samples, enabling the unprecedented ability to evaluate any neural
network architecture instantly.

</details>


### [403] [Feasibility-Aware Decision-Focused Learning for Predicting Parameters in the Constraints](https://arxiv.org/abs/2510.04951)
*Jayanta Mandi,Marianne Defresne,Senne Berden,Tias Guns*

Main category: cs.LG

TL;DR: 本文针对约束优化问题中参数不确定的预测 - 优化问题，提出新的决策聚焦学习框架，推导两个新损失函数并引入可调参数平衡次优性和可行性，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 在约束优化问题中预测约束参数时，预测参数可能导致不可行解，需同时管理可行性和决策质量。

Method: 开发用于预测通用约束优化问题中约束参数的决策聚焦学习框架，基于最大似然估计推导两个新损失函数，引入可调参数形成加权平均。

Result: 调整可调参数能让决策者控制次优性和可行性的权衡，且在多个约束优化实例中，单个可调参数值下方法在次优性和可行性上与现有基线表现相当。

Conclusion: 所提出的框架和方法能有效平衡约束优化问题中决策的次优性和可行性。

Abstract: When some parameters of a constrained optimization problem (COP) are
uncertain, this gives rise to a predict-then-optimize (PtO) problem, comprising
two stages -- the prediction of the unknown parameters from contextual
information and the subsequent optimization using those predicted parameters.
Decision-focused learning (DFL) implements the first stage by training a
machine learning (ML) model to optimize the quality of the decisions made using
the predicted parameters. When parameters in the constraints of a COP are
predicted, the predicted parameters can lead to infeasible solutions.
Therefore, it is important to simultaneously manage both feasibility and
decision quality. We develop a DFL framework for predicting constraint
parameters in a generic COP. While prior works typically assume that the
underlying optimization problem is a linear program (LP) or integer linear
program (ILP), our approach makes no such assumption. We derive two novel loss
functions based on maximum likelihood estimation (MLE): the first one penalizes
infeasibility (by penalizing when the predicted parameters lead to infeasible
solutions), and the second one penalizes suboptimal decisions (by penalizing
when the true optimal solution is infeasible under the predicted parameters).
We introduce a single tunable parameter to form a weighted average of the two
losses, allowing decision-makers to balance suboptimality and feasibility. We
experimentally demonstrate that adjusting this parameter provides a
decision-maker the control over the trade-off between the two. Moreover, across
several COP instances, we find that for a single value of the tunable
parameter, our method matches the performance of the existing baselines on
suboptimality and feasibility.

</details>


### [404] [StructuralDecompose: A Modular Framework for Robust Time Series Decomposition in R](https://arxiv.org/abs/2510.04974)
*Allen Daniel Sunny*

Main category: cs.LG

TL;DR: 介绍R包StructuralDecompose用于时间序列分解，分离分析为不同组件，在数据集上演示并与其他工具对比。


<details>
  <summary>Details</summary>
Motivation: 现有方法将分解视为整体过程，缺乏灵活性和可定制性，需开发新工具。

Method: 将分析分离为变点检测、异常检测、平滑和分解等不同组件。

Result: 在模拟和真实数据集上演示该包，与Rbeast和autostsm等工具进行性能对比。

Conclusion: 该包可用于可解释机器学习工作流，设计灵活且稳健。

Abstract: We present StructuralDecompose, an R package for modular and interpretable
time series decomposition. Unlike existing approaches that treat decomposition
as a monolithic process, StructuralDecompose separates the analysis into
distinct components: changepoint detection, anomaly detection, smoothing, and
decomposition. This design provides flexibility and robust- ness, allowing
users to tailor methods to specific time series characteristics. We demonstrate
the package on simulated and real-world datasets, benchmark its performance
against state-of-the- art tools such as Rbeast and autostsm, and discuss its
role in interpretable machine learning workflows.

</details>


### [405] [Federated Computation of ROC and PR Curves](https://arxiv.org/abs/2510.04979)
*Xuefeng Xu,Graham Cormode*

Main category: cs.LG

TL;DR: 本文提出在联邦学习中通过估计预测分数分布分位数近似ROC和PR曲线的方法，理论分析误差并经实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习场景下因隐私和通信限制，计算ROC和PR曲线具有挑战性，传统集中式计算方法不可用。

Method: 在分布式差分隐私下估计预测分数分布的分位数来近似ROC和PR曲线，并给出真实曲线和估计曲线面积误差的理论界限。

Result: 在真实数据集上的实验表明，该方法能以最小通信量和强隐私保证实现高近似精度。

Conclusion: 该方法适用于联邦系统中保护隐私的模型评估。

Abstract: Receiver Operating Characteristic (ROC) and Precision-Recall (PR) curves are
fundamental tools for evaluating machine learning classifiers, offering
detailed insights into the trade-offs between true positive rate vs. false
positive rate (ROC) or precision vs. recall (PR). However, in Federated
Learning (FL) scenarios, where data is distributed across multiple clients,
computing these curves is challenging due to privacy and communication
constraints. Specifically, the server cannot access raw prediction scores and
class labels, which are used to compute the ROC and PR curves in a centralized
setting. In this paper, we propose a novel method for approximating ROC and PR
curves in a federated setting by estimating quantiles of the prediction score
distribution under distributed differential privacy. We provide theoretical
bounds on the Area Error (AE) between the true and estimated curves,
demonstrating the trade-offs between approximation accuracy, privacy, and
communication cost. Empirical results on real-world datasets demonstrate that
our method achieves high approximation accuracy with minimal communication and
strong privacy guarantees, making it practical for privacy-preserving model
evaluation in federated systems.

</details>


### [406] [Adaptive Memory Momentum via a Model-Based Framework for Deep Learning Optimization](https://arxiv.org/abs/2510.04988)
*Kristi Topollai,Anna Choromanska*

Main category: cs.LG

TL;DR: 提出自适应记忆机制，用动态动量系数替代常量动量，应用于多种学习任务，表现优于标准方法，为优化带来新途径。


<details>
  <summary>Details</summary>
Motivation: 现有动量优化器中动量系数常设为常量且固定，这种策略并非最优，需要改进。

Method: 通过用两个平面近似目标函数推导出方法，一个基于当前迭代梯度，另一个基于过去梯度的累积记忆，实现SGD和AdamW的自适应记忆变体。

Result: 在从简单凸问题到大规模深度学习场景等多种学习任务中，该方法能优于带手动调参动量系数的标准SGD和Adam。

Conclusion: 工作为优化中的自适应诱导开辟了新方法。

Abstract: The vast majority of modern deep learning models are trained with
momentum-based first-order optimizers. The momentum term governs the
optimizer's memory by determining how much each past gradient contributes to
the current convergence direction. Fundamental momentum methods, such as
Nesterov Accelerated Gradient and the Heavy Ball method, as well as more recent
optimizers such as AdamW and Lion, all rely on the momentum coefficient that is
customarily set to $\beta = 0.9$ and kept constant during model training, a
strategy widely used by practitioners, yet suboptimal. In this paper, we
introduce an \textit{adaptive memory} mechanism that replaces constant momentum
with a dynamic momentum coefficient that is adjusted online during
optimization. We derive our method by approximating the objective function
using two planes: one derived from the gradient at the current iterate and the
other obtained from the accumulated memory of the past gradients. To the best
of our knowledge, such a proximal framework was never used for momentum-based
optimization. Our proposed approach is novel, extremely simple to use, and does
not rely on extra assumptions or hyperparameter tuning. We implement adaptive
memory variants of both SGD and AdamW across a wide range of learning tasks,
from simple convex problems to large-scale deep learning scenarios,
demonstrating that our approach can outperform standard SGD and Adam with
hand-tuned momentum coefficients. Finally, our work opens doors for new ways of
inducing adaptivity in optimization.

</details>


### [407] [Power Transform Revisited: Numerically Stable, and Federated](https://arxiv.org/abs/2510.04995)
*Xuefeng Xu,Graham Cormode*

Main category: cs.LG

TL;DR: 本文分析幂变换数值不稳定问题，提出补救方法并扩展到联邦学习场景，实验证明方法有效且鲁棒。


<details>
  <summary>Details</summary>
Motivation: 幂变换直接实现存在严重数值不稳定问题，可能导致错误结果甚至程序崩溃。

Method: 全面分析不稳定性来源并提出补救方法，将幂变换扩展到联邦学习场景。

Result: 在真实数据集上的实验表明，该方法有效且鲁棒，相比现有方法显著提高了稳定性。

Conclusion: 所提方法能有效解决幂变换的数值不稳定问题，且适用于联邦学习场景。

Abstract: Power transforms are popular parametric techniques for making data more
Gaussian-like, and are widely used as preprocessing steps in statistical
analysis and machine learning. However, we find that direct implementations of
power transforms suffer from severe numerical instabilities, which can lead to
incorrect results or even crashes. In this paper, we provide a comprehensive
analysis of the sources of these instabilities and propose effective remedies.
We further extend power transforms to the federated learning setting,
addressing both numerical and distributional challenges that arise in this
context. Experiments on real-world datasets demonstrate that our methods are
both effective and robust, substantially improving stability compared to
existing approaches.

</details>


### [408] [Inoculation Prompting: Instructing LLMs to misbehave at train-time improves test-time alignment](https://arxiv.org/abs/2510.05024)
*Nevan Wichers,Aram Ebtekar,Ariana Azarbal,Victor Gillioz,Christine Ye,Emil Ryd,Neil Rathi,Henry Sleight,Alex Mallen,Fabien Roger,Samuel Marks*

Main category: cs.LG

TL;DR: 介绍Inoculation Prompting (IP)技术，可防止模型学习不良行为且不影响期望能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型训练监督信号不完善导致不良行为，提升监督质量成本高或不可行，需改进训练信号的方法。

Method: 引入IP技术，通过修改训练提示明确要求不良行为来防止模型学习。

Result: 在四个场景中，IP减少了不良行为学习，且未大幅降低期望能力学习；能有效识别有潜力的接种提示。

Conclusion: IP是控制模型微调泛化的简单有效方法，可防止学习不良行为且不破坏期望能力。

Abstract: Large language models are sometimes trained with imperfect oversight signals,
leading to undesired behaviors such as reward hacking and sycophancy. Improving
oversight quality can be expensive or infeasible, motivating methods that
improve learned behavior despite an imperfect training signal. We introduce
Inoculation Prompting (IP), a simple but counterintuitive technique that
prevents learning of an undesired behavior by modifying training prompts to
explicitly request it. For example, to inoculate against reward hacking, we
modify the prompts used in supervised fine-tuning to request code that only
works on provided test cases but fails on other inputs. Across four settings we
find that IP reduces the learning of undesired behavior without substantially
reducing the learning of desired capabilities. We also show that prompts which
more strongly elicit the undesired behavior prior to fine-tuning more
effectively inoculate against the behavior when used during training; this
serves as a heuristic to identify promising inoculation prompts. Overall, IP is
a simple yet effective way to control how models generalize from fine-tuning,
preventing learning of undesired behaviors without substantially disrupting
desired capabilities.

</details>


### [409] [Graph-Aware Diffusion for Signal Generation](https://arxiv.org/abs/2510.05036)
*Sergio Rozada,Vimal K. B.,Andrea Cavallo,Antonio G. Marques,Hadi Jamali-Rad,Elvin Isufi*

Main category: cs.LG

TL;DR: 研究从给定图上未知分布生成图信号问题，提出图感知生成扩散模型GAD并验证其优势。


<details>
  <summary>Details</summary>
Motivation: 现有生成图信号方法缺乏通用性，在正向过程中忽略图结构或设计特定领域的图感知机制。

Method: 采用通过热方程融入图的正向过程，考虑时间扭曲系数缓解漂移项指数衰减，构建图感知生成扩散模型GAD，并分析其正反向动力学。

Result: 正向动力学收敛到协方差由图拉普拉斯矩阵参数化的高斯马尔可夫随机场，反向动力学可解释为一系列图信号去噪问题。

Conclusion: GAD在合成数据、真实交通速度测量和温度传感器网络上展现出优势。

Abstract: We study the problem of generating graph signals from unknown distributions
defined over given graphs, relevant to domains such as recommender systems or
sensor networks. Our approach builds on generative diffusion models, which are
well established in vision and graph generation but remain underexplored for
graph signals. Existing methods lack generality, either ignoring the graph
structure in the forward process or designing graph-aware mechanisms tailored
to specific domains. We adopt a forward process that incorporates the graph
through the heat equation. Rather than relying on the standard formulation, we
consider a time-warped coefficient to mitigate the exponential decay of the
drift term, yielding a graph-aware generative diffusion model (GAD). We analyze
its forward dynamics, proving convergence to a Gaussian Markov random field
with covariance parametrized by the graph Laplacian, and interpret the backward
dynamics as a sequence of graph-signal denoising problems. Finally, we
demonstrate the advantages of GAD on synthetic data, real traffic speed
measurements, and a temperature sensor network.

</details>


### [410] [Test-Time Scaling in Diffusion LLMs via Hidden Semi-Autoregressive Experts](https://arxiv.org/abs/2510.05040)
*Jihoon Lee,Hoyeon Moon,Kevin Zhai,Arun Kumar Chithanar,Anit Kumar Sahu,Soummya Kar,Chul Lee,Souradip Chakraborty,Amrit Singh Bedi*

Main category: cs.LG

TL;DR: 本文指出扩散式大语言模型推理时未充分利用信息的问题，提出无训练推理方法HEX，在多个基准测试中提升了模型性能，揭示了掩码顺序对推理性能的关键作用。


<details>
  <summary>Details</summary>
Motivation: 解决扩散式大语言模型在推理时如何更好利用信息的问题，现有固定推理时间调度方式未利用潜在集成信息导致性能不佳。

Method: 引入无训练推理方法HEX，对不同块大小的生成路径进行多数投票，集成异构块调度。

Result: 在GSM8K等推理基准测试中显著提升准确率，如GSM8K从24.72%提升到88.10%，MATH从16.40%到40.00%等。

Conclusion: 建立了扩散式大语言模型测试时扩展的新范式，表明掩码执行顺序对推理性能至关重要。

Abstract: Diffusion-based large language models (dLLMs) are trained flexibly to model
extreme dependence in the data distribution; however, how to best utilize this
information at inference time remains an open problem. In this work, we uncover
an interesting property of these models: dLLMs trained on textual data
implicitly learn a mixture of semi-autoregressive experts, where different
generation orders reveal different specialized behaviors. We show that
committing to any single, fixed inference time schedule, a common practice,
collapses performance by failing to leverage this latent ensemble. To address
this, we introduce HEX (Hidden semiautoregressive EXperts for test-time
scaling), a training-free inference method that ensembles across heterogeneous
block schedules. By doing a majority vote over diverse block-sized generation
paths, HEX robustly avoids failure modes associated with any single fixed
schedule. On reasoning benchmarks such as GSM8K, it boosts accuracy by up to
3.56X (from 24.72% to 88.10%), outperforming top-K margin inference and
specialized fine-tuned methods like GRPO, without additional training. HEX even
yields significant gains on MATH benchmark from 16.40% to 40.00%, scientific
reasoning on ARC-C from 54.18% to 87.80%, and TruthfulQA from 28.36% to 57.46%.
Our results establish a new paradigm for test-time scaling in diffusion-based
LLMs (dLLMs), revealing that the sequence in which masking is performed plays a
critical role in determining performance during inference.

</details>


### [411] [KEEP: Integrating Medical Ontologies with Clinical Data for Robust Code Embeddings](https://arxiv.org/abs/2510.05049)
*Ahmed Elhussein,Paul Meddeb,Abigail Newbury,Jeanne Mirone,Martin Stoll,Gamze Gursoy*

Main category: cs.LG

TL;DR: 提出KEEP框架结合知识图谱嵌入与临床数据自适应学习，在多方面表现出色且适合资源受限环境


<details>
  <summary>Details</summary>
Motivation: 现有医疗结构化医学代码表示方法存在权衡问题，知识图谱方法缺现实模式，数据驱动方法忽略结构化知识

Method: 先从知识图谱生成嵌入，再对患者记录进行正则化训练，自适应整合经验模式并保留本体关系，无需特定任务辅助或端到端训练

Result: 在UK Biobank和MIMIC IV的结构化电子健康记录评估中，KEEP在捕捉语义关系和预测临床结果方面优于传统和基于语言模型的方法

Conclusion: KEEP能弥合现有方法差距，支持多下游应用和模型架构，计算需求小，适合资源受限环境

Abstract: Machine learning in healthcare requires effective representation of
structured medical codes, but current methods face a trade off: knowledge graph
based approaches capture formal relationships but miss real world patterns,
while data driven methods learn empirical associations but often overlook
structured knowledge in medical terminologies. We present KEEP (Knowledge
preserving and Empirically refined Embedding Process), an efficient framework
that bridges this gap by combining knowledge graph embeddings with adaptive
learning from clinical data. KEEP first generates embeddings from knowledge
graphs, then employs regularized training on patient records to adaptively
integrate empirical patterns while preserving ontological relationships.
Importantly, KEEP produces final embeddings without task specific auxiliary or
end to end training enabling KEEP to support multiple downstream applications
and model architectures. Evaluations on structured EHR from UK Biobank and
MIMIC IV demonstrate that KEEP outperforms both traditional and Language Model
based approaches in capturing semantic relationships and predicting clinical
outcomes. Moreover, KEEP's minimal computational requirements make it
particularly suitable for resource constrained environments.

</details>


### [412] [HybridFlow: Quantification of Aleatoric and Epistemic Uncertainty with a Single Hybrid Model](https://arxiv.org/abs/2510.05054)
*Peter Van Katwyk,Karianne J. Bergen*

Main category: cs.LG

TL;DR: 介绍HybridFlow架构统一对随机和认知不确定性建模，在回归任务上优于先前框架，量化的不确定性校准更好。


<details>
  <summary>Details</summary>
Motivation: 不确定性量化对高风险机器学习应用的鲁棒性很关键，现有方法存在不足，需要统一随机和认知不确定性建模。

Method: 引入HybridFlow模块化混合架构，结合条件掩码自回归归一化流估计随机不确定性和灵活概率预测器处理认知不确定性，支持与任何概率模型类集成。

Result: 在深度估计、回归基准测试和冰盖模拟等回归任务上优于先前的不确定性量化框架，量化的不确定性校准更好且与模型误差更匹配。

Conclusion: HybridFlow解决了贝叶斯深度学习中的关键挑战，在单一稳健框架中统一了随机和认知不确定性建模。

Abstract: Uncertainty quantification is critical for ensuring robustness in high-stakes
machine learning applications. We introduce HybridFlow, a modular hybrid
architecture that unifies the modeling of aleatoric and epistemic uncertainty
by combining a Conditional Masked Autoregressive normalizing flow for
estimating aleatoric uncertainty with a flexible probabilistic predictor for
epistemic uncertainty. The framework supports integration with any
probabilistic model class, allowing users to easily adapt HybridFlow to
existing architectures without sacrificing predictive performance. HybridFlow
improves upon previous uncertainty quantification frameworks across a range of
regression tasks, such as depth estimation, a collection of regression
benchmarks, and a scientific case study of ice sheet emulation. We also provide
empirical results of the quantified uncertainty, showing that the uncertainty
quantified by HybridFlow is calibrated and better aligns with model error than
existing methods for quantifying aleatoric and epistemic uncertainty.
HybridFlow addresses a key challenge in Bayesian deep learning, unifying
aleatoric and epistemic uncertainty modeling in a single robust framework.

</details>


### [413] [Modeling Student Learning with 3.8 Million Program Traces](https://arxiv.org/abs/2510.05056)
*Alexis Ross,Megha Srivastava,Jeremiah Blanchard,Jacob Andreas*

Main category: cs.LG

TL;DR: 本文探索基于编程交互轨迹训练语言模型，引入含380多万条轨迹的数据集，发现基于真实轨迹训练的模型在模拟学生行为上更优，还能预测代码轨迹属性，帮助学生纠错。


<details>
  <summary>Details</summary>
Motivation: 探索从编程推理轨迹训练语言模型中可获取关于编码者尤其是编程学习学生的信息。

Method: 引入Pencil Code平台的编程推理轨迹数据集，对比不同训练数据的模型，进行行为和探测分析。

Result: 基于真实轨迹训练的模型更能模拟学生多样行为，可从学生代码表示中预测代码轨迹属性，能引导代码生成模型帮助学生纠错。

Conclusion: 许多代码属性与学生个体相关，基于编辑轨迹训练的模型更可控、更能预测学生编程行为且更擅长生成最终程序。

Abstract: As programmers write code, they often edit and retry multiple times, creating
rich "interaction traces" that reveal how they approach coding tasks and
provide clues about their level of skill development. For novice programmers in
particular, these traces reflect the diverse reasoning processes they employ to
code, such as exploratory behavior to understand how a programming concept
works, re-strategizing in response to bugs, and personalizing stylistic
choices. In this work, we explore what can be learned from training language
models on such reasoning traces: not just about code, but about coders, and
particularly students learning to program. We introduce a dataset of over 3.8
million programming reasoning traces from users of Pencil Code, a free online
educational platform used by students to learn simple programming concepts.
Compared to models trained only on final programs or synthetically-generated
traces, we find that models trained on real traces are stronger at modeling
diverse student behavior. Through both behavioral and probing analyses, we also
find that many properties of code traces, such as goal backtracking or number
of comments, can be predicted from learned representations of the students who
write them. Building on this result, we show that we can help students recover
from mistakes by steering code generation models to identify a sequence of
edits that will results in more correct code while remaining close to the
original student's style. Together, our results suggest that many properties of
code are properties of individual students and that training on edit traces can
lead to models that are more steerable, more predictive of student behavior
while programming, and better at generating programs in their final states.
Code and data is available at https://github.com/meghabyte/pencilcode-public

</details>


### [414] [Boomerang Distillation Enables Zero-Shot Model Size Interpolation](https://arxiv.org/abs/2510.05064)
*Sara Kangaslahti,Nihal V. Nayak,Jonathan Geuter,Marco Fumero,Francesco Locatello,David Alvarez-Melis*

Main category: cs.LG

TL;DR: 提出回旋镖蒸馏方法生成细粒度模型族，降低训练成本。


<details>
  <summary>Details</summary>
Motivation: 现有构建模型族方法成本高且粒度粗，需更高效方法。

Method: 从大模型蒸馏到小模型，再将教师层块重新融入学生模型构建中间大小模型，无需额外训练。

Result: 生成中间大小零样本插值模型，性能在学生和教师模型间平滑缩放，常优于同尺寸预训练或蒸馏模型。

Conclusion: 回旋镖蒸馏是生成细粒度模型族的简单高效方法，能降低训练成本，适应不同部署环境。

Abstract: Large language models (LLMs) are typically deployed under diverse memory and
compute constraints. Existing approaches build model families by training each
size independently, which is prohibitively expensive and provides only
coarse-grained size options. In this work, we identify a novel phenomenon that
we call boomerang distillation: starting from a large base model (the teacher),
one first distills down to a small student and then progressively reconstructs
intermediate-sized models by re-incorporating blocks of teacher layers into the
student without any additional training. This process produces zero-shot
interpolated models of many intermediate sizes whose performance scales
smoothly between the student and teacher, often matching or surpassing
pretrained or distilled models of the same size. We further analyze when this
type of interpolation succeeds, showing that alignment between teacher and
student through pruning and distillation is essential. Boomerang distillation
thus provides a simple and efficient way to generate fine-grained model
families, dramatically reducing training cost while enabling flexible
adaptation across deployment environments. The code and models are available at
https://github.com/dcml-lab/boomerang-distillation.

</details>


### [415] [MICROTRIPS: MICRO-geography TRavel Intelligence and Pattern Synthesis](https://arxiv.org/abs/2510.05080)
*Yangyang Wang,Tayo Fabusuyi*

Main category: cs.LG

TL;DR: 提出新的小区域估计框架，利用公开微数据和机器学习方法预测出行行为，比传统方法更准确，能支持多种政策应用。


<details>
  <summary>Details</summary>
Motivation: 通过详细刻画出行行为来加强城市交通规划。

Method: 采用公开微数据文件和机器学习方法，改进四步出行模型，对小地理区域代表性合成人口的出行行为进行预测。

Result: 使用ACS/PUMS通勤数据集验证，该框架比传统方法精度更高。

Conclusion: 该框架能提供细致见解，可针对局部情况制定干预措施，支持多种政策应用和针对性干预。

Abstract: This study presents a novel small-area estimation framework to enhance urban
transportation planning through detailed characterization of travel behavior.
Our approach improves on the four-step travel model by employing publicly
available microdata files and machine learning methods to predict travel
behavior for a representative, synthetic population at small geographic areas.
This approach enables high-resolution estimation of trip generation, trip
distribution, mode choice, and route assignment. Validation using ACS/PUMS
work-commute datasets demonstrates that our framework achieves higher accuracy
compared to conventional approaches. The resulting granular insights enable the
tailoring of interventions to address localized situations and support a range
of policy applications and targeted interventions, including the optimal
placement of micro-fulfillment centers, effective curb-space management, and
the design of more inclusive transportation solutions particularly for
vulnerable communities.

</details>


### [416] [Learning to Interpret Weight Differences in Language Models](https://arxiv.org/abs/2510.05092)
*Avichal Goel,Yoon Kim,Nir Shavit,Tony T. Wang*

Main category: cs.LG

TL;DR: 提出Diff Interpretation Tuning (DIT)方法让模型描述微调导致的变化，并在两个概念验证场景中证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 微调语言模型的权重变化通常不可解释，且微调数据集常不可用或过大，需要一种方法全面理解自然语言中的权重变化。

Method: 使用合成的、带标签的权重变化训练DIT适配器，应用到兼容的微调模型上使其描述自身变化。

Result: 在两个概念验证场景中，该方法使模型能用准确的自然语言描述微调导致的变化。

Conclusion: DIT方法能够让模型描述自身微调引起的修改。

Abstract: Finetuning (pretrained) language models is a standard approach for updating
their internal parametric knowledge and specializing them to new tasks and
domains. However, the corresponding model weight changes ("weight diffs") are
not generally interpretable. While inspecting the finetuning dataset can give a
sense of how the model might have changed, these datasets are often not
publicly available or are too large to work with directly. Towards the goal of
comprehensively understanding weight diffs in natural language, we introduce
Diff Interpretation Tuning (DIT), a method that trains models to describe their
own finetuning-induced modifications. Our approach uses synthetic, labeled
weight diffs to train a DIT adapter, which can be applied to a compatible
finetuned model to make it describe how it has changed. We demonstrate in two
proof-of-concept settings (reporting hidden behaviors and summarizing finetuned
knowledge) that our method enables models to describe their finetuning-induced
modifications using accurate natural language descriptions.

</details>


### [417] [From Noisy Traces to Stable Gradients: Bias-Variance Optimized Preference Optimization for Aligning Large Reasoning Models](https://arxiv.org/abs/2510.05095)
*Mingkang Zhu,Xi Chen,Bei Yu,Hengshuang Zhao,Jiaya Jia*

Main category: cs.LG

TL;DR: 论文提出BVPO方法解决大推理模型偏好优化中轨迹采样方差大问题，理论和实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 大推理模型与人类偏好对齐研究不足，现有方法随机轨迹采样梯度方差大。

Method: 从偏差 - 方差权衡角度构建偏好优化，提出BVPO方法，混合两种梯度估计器。

Result: 在AlpacaEval 2和Arena - Hard上提升对齐效果，在数学推理基准上提升推理性能。

Conclusion: 轨迹采样方差是关键瓶颈，优化偏差 - 方差权衡可使训练更稳定、性能更强。

Abstract: Large reasoning models (LRMs) generate intermediate reasoning traces before
producing final answers, yielding strong gains on multi-step and mathematical
tasks. Yet aligning LRMs with human preferences, a crucial prerequisite for
model deployment, remains underexplored. The statistically correct objective
for preference alignment requires marginalizing over reasoning traces, but this
computation is intractable in practice. A common workaround optimizes a single
sampled trajectory, which introduces substantial gradient variance from
stochastic trace sampling. To address this challenge, we frame preference
optimization for LRMs through the lens of the bias--variance trade-off and
propose Bias--Variance Optimized Preference Optimization (BVPO), a simple,
drop-in method that mixes two gradient estimators: a high-variance trace-based
estimator and a low-variance empty-trace estimator obtained by disabling
reasoning trace generation. Our theory shows that BVPO strictly reduces
trace-induced variance for any nontrivial mixture, provides a closed-form
choice of the mixing weight that minimizes mean-squared error relative to the
true marginal gradient, and under standard smoothness and step-size conditions,
tightens classical convergence bounds for stochastic gradient descent.
Empirically, BVPO improves alignment over the best baseline by up to 7.8 points
on AlpacaEval~2 and 6.8 points on Arena-Hard. Despite being trained only on
general conversational data, BVPO also boosts reasoning performance for base
models by up to 4.0 points on the average of six math reasoning benchmarks.
These results identify variance from trace sampling as a key bottleneck and
demonstrate that directly optimizing the bias--variance trade-off yields more
stable training and stronger overall performance.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [418] [Efficient Training of Spiking Neural Networks by Spike-aware Data Pruning](https://arxiv.org/abs/2510.04098)
*Chenxiang Ma,Xinyi Chen,Yujie Wu,Kay Chen Tan,Jibin Wu*

Main category: cs.NE

TL;DR: 提出用于SNN的新型数据修剪方法SADP，减少训练开销并提升训练速度，在多数据集和架构上表现出色。


<details>
  <summary>Details</summary>
Motivation: SNN模型和数据集扩展带来高训练开销，现有数据修剪策略在SNN中未充分探索，直接应用ANN方法有问题。

Method: 提出SADP方法，通过确定示例选择概率与梯度范数成正比减少梯度方差，用尖峰感知重要性分数避免直接计算梯度的高成本。

Result: SADP在不同数据集和架构上始终优于数据修剪基线，在不同修剪率下接近理论最大训练加速，在ImageNet上减少35%训练时间并保持与全数据训练相当的准确率。

Conclusion: 建立了以数据为中心的高效SNN训练范式，为SNN扩展到更大模型和数据集铺平道路。

Abstract: Spiking neural networks (SNNs), recognized as an energy-efficient alternative
to traditional artificial neural networks (ANNs), have advanced rapidly through
the scaling of models and datasets. However, such scaling incurs considerable
training overhead, posing challenges for researchers with limited computational
resources and hindering the sustained development of SNNs. Data pruning is a
promising strategy for accelerating training by retaining the most informative
examples and discarding redundant ones, but it remains largely unexplored in
SNNs. Directly applying ANN-based data pruning methods to SNNs fails to capture
the intrinsic importance of examples and suffers from high gradient variance.
To address these challenges, we propose a novel spike-aware data pruning (SADP)
method. SADP reduces gradient variance by determining each example's selection
probability to be proportional to its gradient norm, while avoiding the high
cost of direct gradient computation through an efficient upper bound, termed
spike-aware importance score. This score accounts for the influence of
all-or-nothing spikes on the gradient norm and can be computed with negligible
overhead. Extensive experiments across diverse datasets and architectures
demonstrate that SADP consistently outperforms data pruning baselines and
achieves training speedups close to the theoretical maxima at different pruning
ratios. Notably, SADP reduces training time by 35% on ImageNet while
maintaining accuracy comparable to that of full-data training. This work,
therefore, establishes a data-centric paradigm for efficient SNN training and
paves the way for scaling SNNs to larger models and datasets. The source code
will be released publicly after the review process.

</details>


### [419] [SpikingMamba: Towards Energy-Efficient Large Language Models via Knowledge Distillation from Mamba](https://arxiv.org/abs/2510.04595)
*Yulong Huang,Jianxiong Tang,Chao Wang,Ziyi Wang,Jianguo Zhang,Zhichao Lu,Bojun Cheng,Luziwei Leng*

Main category: cs.NE

TL;DR: 提出基于SNN的节能大语言模型SpikingMamba，蒸馏自Mamba，节能且精度损失小，实验显示有能量效益和精度提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于SNN的大语言模型常为效率牺牲性能，恢复精度需全预训练，成本高不实际，需节能且精度损失小的模型。

Method: 提出SpikingMamba，集成TI - LIF神经元和SGC路径，采用单阶段蒸馏策略转移预训练Mamba零样本能力，并用强化学习增强。

Result: SpikingMamba - 1.3B实现4.76倍能量效益，零样本精度与原Mamba仅差4.78%，强化学习后精度再提升2.55%。

Conclusion: SpikingMamba在提高大语言模型能源效率的同时，能将精度损失控制在较小范围。

Abstract: Large Language Models (LLMs) have achieved remarkable performance across
tasks but remain energy-intensive due to dense matrix operations. Spiking
neural networks (SNNs) improve energy efficiency by replacing dense matrix
multiplications with sparse accumulations. Their sparse spike activity enables
efficient LLMs deployment on edge devices. However, prior SNN-based LLMs often
sacrifice performance for efficiency, and recovering accuracy typically
requires full pretraining, which is costly and impractical. To address this, we
propose SpikingMamba, an energy-efficient SNN-based LLMs distilled from Mamba
that improves energy efficiency with minimal accuracy sacrifice. SpikingMamba
integrates two key components: (a) TI-LIF, a ternary-integer spiking neuron
that preserves semantic polarity through signed multi-level spike
representations. (b) A training-exclusive Smoothed Gradient Compensation (SGC)
path mitigating quantization loss while preserving spike-driven efficiency. We
employ a single-stage distillation strategy to transfer the zero-shot ability
of pretrained Mamba and further enhance it via reinforcement learning (RL).
Experiments show that SpikingMamba-1.3B achieves a 4.76$\times$ energy benefit,
with only a 4.78\% zero-shot accuracy gap compared to the original Mamba, and
achieves a further 2.55\% accuracy improvement after RL.

</details>


### [420] [What your brain activity says about you: A review of neuropsychiatric disorders identified in resting-state and sleep EEG data](https://arxiv.org/abs/2510.04984)
*J. E. M. Scanlon,A. Pelzer,M. Gharleghi,K. C. Fuhrmeister,T. Köllmer,P. Aichroth,R. Göder,C. Hansen,K. I. Wolf*

Main category: cs.NE

TL;DR: 本文探讨无任务脑电图（EEG）数据中可检测和分类的个人与健康信息，分析静息态和睡眠EEG数据特征以评估隐私风险，指出EEG能暴露敏感健康信息，强调匿名化和隐私保护工具的重要性。


<details>
  <summary>Details</summary>
Motivation: 探究公开可用EEG数据中包含的个人和健康信息类型，确定其隐私风险。

Method: 通过Google Scholar、Web of Science搜索相关研究，筛选英文同行评审期刊文章或会议论文，由3名评审进行质量分析。

Result: 静息态EEG能高精度分类多种疾病，睡眠EEG包含睡眠障碍信息，机器学习方法可从EEG数据重新识别个人。

Conclusion: 强调EEG数据匿名化以及开发更好隐私保护工具的重要性。

Abstract: Electroencephalogram monitoring devices and online data repositories hold
large amounts of data from individuals participating in research and medical
studies without direct reference to personal identifiers. This paper explores
what types of personal and health information have been detected and classified
within task-free EEG data. Additionally, we investigate key characteristics of
the collected resting-state and sleep data, in order to determine the privacy
risks involved with openly available EEG data. We used Google Scholar, Web of
Science and searched relevant journals to find studies which classified or
detected the presence of various disorders and personal information in resting
state and sleep EEG. Only English full-text peer-reviewed journal articles or
conference papers about classifying the presence of medical disorders between
individuals were included. A quality analysis carried out by 3 reviewers
determined general paper quality based on specified evaluation criteria. In
resting state EEG, various disorders including Autism Spectrum Disorder,
Parkinson's disease, and alcohol use disorder have been classified with high
classification accuracy, often requiring only 5 mins of data or less. Sleep EEG
tends to hold classifiable information about sleep disorders such as sleep
apnea, insomnia, and REM sleep disorder, but usually involve longer recordings
or data from multiple sleep stages. Many classification methods are still
developing but even today, access to a person's EEG can reveal sensitive
personal health information. With an increasing ability of machine learning
methods to re-identify individuals from their EEG data, this review
demonstrates the importance of anonymization, and the development of improved
tools for keeping study participants and medical EEG users' privacy safe.

</details>


### [421] [Exploration-Exploitation-Evaluation (EEE): A Framework for Metaheuristic Algorithms in Combinatorial Optimization](https://arxiv.org/abs/2510.05027)
*Ethan Davis*

Main category: cs.NE

TL;DR: 介绍将元启发式算法应用于组合优化问题的框架，以蚁群算法求解TSP为例，计算找到全局最优解的概率。


<details>
  <summary>Details</summary>
Motivation: 为将元启发式算法应用于组合优化问题提供有效框架。

Method: 框架包含参数空间广泛探索、利用高性能参数和不确定性量化三个阶段，并以蚁群算法求解TSPLIB berlin52数据集为例。

Result: 单次运行蚁群算法找到全局最优解的概率约为1/40，十次运行后概率提升至1/5。

Conclusion: 所提出的框架有助于将元启发式算法应用于组合优化问题，并可评估结果可靠性。

Abstract: We introduce a framework for applying metaheuristic algorithms, such as ant
colony optimization (ACO), to combinatorial optimization problems (COPs) like
the traveling salesman problem (TSP). The framework consists of three
sequential stages: broad exploration of the parameter space, exploitation of
top-performing parameters, and uncertainty quantification (UQ) to assess the
reliability of results. As a case study, we apply ACO to the TSPLIB berlin52
dataset, which has a known optimal tour length of 7542. Using our framework, we
calculate that the probability of ACO finding the global optimum is
approximately 1/40 in a single run and improves to 1/5 when aggregated over ten
runs.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [422] [Formal Analysis of Metastable Failures in Software Systems](https://arxiv.org/abs/2510.03551)
*Rebecca Isaacs,Peter Alvaro,Rupak Majumdar,Kiran-Kumar Muniswamy-Reddy,Mahmoud Salamati,Sadegh Soudjani*

Main category: cs.PF

TL;DR: 本文为请求 - 响应服务器系统的亚稳性提供数学基础，通过建模、可视化分析和定量预测，实现对系统亚稳行为的捕捉和恢复时间的预测。


<details>
  <summary>Details</summary>
Motivation: 大规模软件系统存在亚稳性故障，这是云系统可用性中断的罕见但灾难性来源，需对其进行研究。

Method: 使用特定领域语言对系统建模，构建连续时间马尔可夫链（CTMC），进行定性可视化分析和基于逃逸概率的定量预测。

Result: 定性可视化分析能在毫秒内捕捉和预测实际系统中的亚稳性实例，算法证实系统参数接近亚稳模式时恢复时间会激增。

Conclusion: 提出的技术可有效分析请求 - 响应服务器系统的亚稳性，能预测系统的亚稳行为和恢复时间。

Abstract: Many large-scale software systems demonstrate metastable failures. In this
class of failures, a stressor such as a temporary spike in workload causes the
system performance to drop and, subsequently, the system performance continues
to remain low even when the stressor is removed. These failures have been
reported by many large corporations and considered to be a rare but
catastrophic source of availability outages in cloud systems.
  In this paper, we provide the mathematical foundations of metastability in
request-response server systems. We model such systems using a domain-specific
language. We show how to construct continuous-time Markov chains (CTMCs) that
approximate the semantics of the programs through modeling and data-driven
calibration. We use the structure of the CTMC models to provide a visualization
of the qualitative behavior of the model. The visualization is a surprisingly
effective way to identify system parameterizations that cause a system to show
metastable behaviors.
  We complement the qualitative analysis with quantitative predictions. We
provide a formal notion of metastable behaviors based on escape probabilities,
and show that metastable behaviors are related to the eigenvalue structure of
the CTMC. Our characterization leads to algorithmic tools to predict recovery
times in metastable models of server systems.
  We have implemented our technique in a tool for the modeling and analysis of
server systems. Through models inspired by failures in real request-response
systems, we show that our qualitative visual analysis captures and predicts
many instances of metastability that were observed in the field in a matter of
milliseconds. Our algorithms confirm that recovery times surge as the system
parameters approach metastable modes in the dynamics.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [423] [Repairing Leaks in Resource Wrappers](https://arxiv.org/abs/2510.03461)
*Sanjay Malakar,Michael D. Ernst,Martin Kellogg,Manu Sridharan*

Main category: cs.SE

TL;DR: 本文针对资源包装器场景下的资源泄漏修复提出四项改进，提升修复效果，实现Arodnap工具在NJR基准测试中修复率从41%提升到68%。


<details>
  <summary>Details</summary>
Motivation: 现有静态分析工具虽能检测资源泄漏，但自动修复有挑战，此前工作仅修复硬编码库资源类型泄漏，未考虑资源包装器。

Method: 将资源管理规范推理集成到修复流程；将程序转换为更易分析的变体；进行新的字段包含分析；引入新修复模式和更精确推理。

Result: 实现Arodnap工具，在NJR基准测试中，将资源泄漏警告修复率从41%提升到68%。

Conclusion: 提出的四项改进方法能有效提升资源包装器场景下的资源泄漏修复效果。

Abstract: A resource leak occurs when a program fails to release a finite resource like
a socket, file descriptor or database connection. While sound static analysis
tools can detect all leaks, automatically repairing them remains challenging.
Prior work took the output of a detection tool and attempted to repair only
leaks from a hard-coded list of library resource types. That approach limits
the scope of repairable leaks: real-world code uses resource wrappers that
store a resource in a field and must themselves be closed. This paper makes
four key contributions to improve resource leak repair in the presence of
wrappers. (1) It integrates inference of resource management specifications
into the repair pipeline, enabling extant fixing approaches to reason about
wrappers. (2) It transforms programs into variants that are easier to analyze,
making inference, detection, and fixing tools more effective; for instance, it
makes detection tools report problems closer to the root cause, often in a
client of a resource wrapper rather than within the wrapper class itself. (3) A
novel field containment analysis reasons about resource lifetimes, enabling
repair of more leaks involving resources stored in fields. (4) It introduces a
new repair pattern and more precise reasoning to better handle resources stored
in non-final fields. Prior work fixed 41% of resource leak warnings in the NJR
benchmark suite; our implementation Arodnap fixes 68%.

</details>


### [424] [ALMAS: an Autonomous LLM-based Multi-Agent Software Engineering Framework](https://arxiv.org/abs/2510.03463)
*Vali Tawosi,Keshav Ramani,Salwa Alamir,Xiaomo Liu*

Main category: cs.SE

TL;DR: 提出基于大语言模型的多智能体软件工程框架ALMAS，可在敏捷软件开发团队中执行端到端任务，通过已发表工作和用例展示其进展。


<details>
  <summary>Details</summary>
Motivation: 软件开发是多方面环境，成功的大语言模型系统需考虑软件开发生命周期多个阶段，当前研究未充分满足此需求。

Method: 提出ALMAS框架，使智能体与敏捷角色对齐，以模块化方式与人类开发者及其开发环境集成。

Result: 通过已发表工作和用例展示，ALMAS能无缝生成应用程序并添加新功能。

Conclusion: ALMAS框架有潜力在敏捷软件开发团队中有效执行端到端任务，可与人类开发者良好协作。

Abstract: Multi-agent Large Language Model (LLM) systems have been leading the way in
applied LLM research across a number of fields. One notable area is software
development, where researchers have advanced the automation of code
implementation, code testing, code maintenance, inter alia, using LLM agents.
However, software development is a multifaceted environment that extends beyond
just code. As such, a successful LLM system must factor in multiple stages of
the software development life-cycle (SDLC). In this paper, we propose a vision
for ALMAS, an Autonomous LLM-based Multi-Agent Software Engineering framework,
which follows the above SDLC philosophy such that it may work within an agile
software development team to perform several tasks end-to-end. ALMAS aligns its
agents with agile roles, and can be used in a modular fashion to seamlessly
integrate with human developers and their development environment. We showcase
the progress towards ALMAS through our published works and a use case
demonstrating the framework, where ALMAS is able to seamlessly generate an
application and add a new feature.

</details>


### [425] [Relative Code Comprehensibility Prediction](https://arxiv.org/abs/2510.03474)
*Nadeeshan De Silva,Martin Kellogg,Oscar Chaparro*

Main category: cs.SE

TL;DR: 研究对比绝对和相对代码可理解性预测，发现相对模型表现更佳，更适用于下游软件工程任务。


<details>
  <summary>Details</summary>
Motivation: 现有代码可理解性指标与人类实际可理解性相关性差，机器学习模型直接预测准确性有限，源于数据固有噪声。

Method: 使用含150个Java代码片段和12.5k个人类可理解性测量值的数据集，对比绝对和相对可理解性预测模型与朴素基线的性能。

Result: 绝对可理解性模型最多比基线提高33.4%且常表现不佳，相对模型片段级和开发者级预测平均分别提高137.8%和74.7%。

Conclusion: 相对可理解性模型能更有效从数据学习，适用于下游软件工程任务。

Abstract: Automatically predicting how difficult it is for humans to understand a code
snippet can assist developers in tasks like deciding when and where to
refactor. Despite many proposed code comprehensibility metrics, studies have
shown they often correlate poorly with actual measurements of human
comprehensibility. This has motivated the use of machine learning models to
predict human comprehensibility directly from code, but these models have also
shown limited accuracy.
  We argue that model inaccuracy stems from inherent noise in human
comprehensibility data, which confuses models trained to predict it directly.
To address this, we propose training models to predict the relative
comprehensibility of two code snippets - that is, predicting which snippet a
human would find easier to understand without predicting each snippet's
comprehensibility in isolation. This mitigates noise in predicting 'absolute'
comprehensibility measurements, but is still useful for downstream
software-engineering tasks like assessing whether refactoring improves or
hinders comprehensibility.
  We conducted a study to assess and compare the effectiveness of absolute and
relative code comprehensibility prediction via machine learning. We used a
dataset of 150 Java code snippets and 12.5k human comprehensibility
measurements from prior user studies, comparing the models' performance with
naive baselines (eg 'always predict the majority class'). Our findings indicate
that absolute comprehensibility models improve over the baselines by at most
33.4% and frequently underperform. In contrast, relative comprehensibility
models are substantially better, with average improvements of 137.8% and 74.7%
for snippet-wise and developer-wise prediction, respectively. These results
suggest that relative comprehensibility models learn more effectively from the
data, supporting their practical applicability for downstream SE tasks.

</details>


### [426] [LLM Agents for Automated Dependency Upgrades](https://arxiv.org/abs/2510.03480)
*Vali Tawosi,Salwa Alamir,Xiaomo Liu,Manuela Veloso*

Main category: cs.SE

TL;DR: 随着代码库发展，库依赖需更新但可能引入问题，本文提出LLM代理框架结合迁移文档自动更新代码并确保兼容性，经测试比现有方法更高效。


<details>
  <summary>Details</summary>
Motivation: 代码库中库依赖更新可能引入代码不兼容问题，需大量开发者维护时间，因此需要解决方案。

Method: 引入LLM代理框架结合迁移文档，系统架构含Summary Agent、Control Agent和Code Agent，在工业用例上创建合成代码仓库并与现有方法对比。

Result: 该方法在所有测试用例中使用更少令牌进行升级，精度达71.4%。

Conclusion: 该方法相比现有方法更高效、有效。

Abstract: As a codebase expands over time, its library dependencies can become outdated
and require updates to maintain innovation and security. However, updating a
library can introduce breaking changes in the code, necessitating significant
developer time for maintenance. To address this, we introduce a framework of
LLM agents to be used in combination with migration documentation to
automatically recommend and apply code updates and ensure compatibility with
new versions. Our solution can automatically localize updated library usages in
live Java codebases and implement recommended fixes in a user-friendly manner.
The system architecture consists of multiple key components: a Summary Agent,
Control Agent, and Code Agent. To validate our approach, we apply the framework
on an industrial use case by which we create three synthetic code repositories
with major Upgrade changes and benchmark our approach against state-of-the-art
methods. Results show that our approach not only performs upgrades using fewer
tokens across all cases but also achieves a precision of 71.4%, highlighting
its efficiency and effectiveness compared to state-of-the-art methods.

</details>


### [427] [AgentHub: A Research Agenda for Agent Sharing Infrastructure](https://arxiv.org/abs/2510.03495)
*Erik Pautsch,Tanmay Singla,Wenxin Jiang,Huiyun Peng,Behnaz Hassanshahi,Konstantin Läufer,George K. Thiruvathukal,James C. Davis*

Main category: cs.SE

TL;DR: 当前LLM代理基础设施零散，现有研究聚焦窄，提出AgentHub研究议程以构建可靠可扩展代理生态。


<details>
  <summary>Details</summary>
Motivation: LLM代理基础设施与成熟生态系统相比零散，现有研究聚焦窄，需考虑更广泛软件工程需求。

Method: 提出AgentHub研究议程，明确能力清晰、生命周期透明等关键挑战。

Result: 为构建可靠可扩展的代理生态系统规划了社区范围的议程。

Conclusion: 期望未来代理能像软件库一样无缝共享、被信任和组合。

Abstract: LLM-based agents are rapidly proliferating, yet the infrastructure for
discovering, evaluating, and governing them remains fragmented compared to
mature ecosystems like software package registries (e.g., npm) and model hubs
(e.g., Hugging Face). Recent research and engineering works have begun to
consider the requisite infrastructure, but so far they focus narrowly -- on
distribution, naming, or protocol negotiation. However, considering broader
software engineering requirements would improve open-source distribution and
ease reuse. We therefore propose AgentHub, a research agenda for agent sharing.
By framing the key challenges of capability clarity, lifecycle transparency,
interoperability, governance, security, and workflow integration, AgentHub
charts a community-wide agenda for building reliable and scalable agent
ecosystems. Our vision is a future where agents can be shared, trusted, and
composed as seamlessly as today's software libraries.

</details>


### [428] [REFINE: Enhancing Program Repair Agents through Context-Aware Patch Refinement](https://arxiv.org/abs/2510.03588)
*Anvith Pabba,Simin Chen,Alex Mathai,Anindya Chakraborty,Baishakhi Ray*

Main category: cs.SE

TL;DR: 提出Refine补丁细化框架解决LLM在自动程序修复（APR）中生成草稿补丁无法完全修复问题，实验表明其有效且具有通用性。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的APR技术因对代码上下文理解有限和过度依赖不完整测试套件，难以产生正确修复，常生成草稿补丁。

Method: 提出Refine框架，解决模糊内容消歧、使补丁候选多样化和聚合部分修复三个关键挑战，实现为通用细化模块，可集成到不同APR系统。

Result: 在SWE - Bench Lite基准测试表现出色，提升AutoCodeRover性能14.67%；在SWE - Bench Verified上提高解决率12.2%；集成到多个APR系统平均提升14%。

Conclusion: 细化是当前APR流程中缺失的有效环节，智能体协作有助于让接近正确的补丁变为正确补丁。

Abstract: Large Language Models (LLMs) have recently shown strong potential in
automatic program repair (APR), especially in repository-level settings where
the goal is to generate patches based on natural language issue descriptions,
large codebases, and regression tests. However, despite their promise, current
LLM-based APR techniques often struggle to produce correct fixes due to limited
understanding of code context and over-reliance on incomplete test suites. As a
result, they frequently generate Draft Patches-partially correct patches that
either incompletely address the bug or overfit to the test cases. In this work,
we propose a novel patch refinement framework, Refine, that systematically
transforms Draft Patches into correct ones. Refine addresses three key
challenges: disambiguating vague issue and code context, diversifying patch
candidates through test-time scaling, and aggregating partial fixes via an
LLM-powered code review process. We implement Refine as a general refinement
module that can be integrated into both open-agent-based and workflow-based APR
systems. Our evaluation on the SWE-Bench Lite benchmark shows that Refine
achieves state-of-the-art results among workflow-based approaches and
approaches the best-known performance across all APR categories. Specifically,
Refine boosts AutoCodeRover's performance by 14.67%, achieving a score of
51.67% and surpassing all prior baselines. On SWE-Bench Verified, Refine
improves the resolution rate by 12.2%, and when integrated across multiple APR
systems, it yields an average improvement of 14%-demonstrating its broad
effectiveness and generalizability. These results highlight the effectiveness
of refinement as a missing component in current APR pipelines and the potential
of agentic collaboration in closing the gap between near-correct and correct
patches. We also open source our code.

</details>


### [429] [Generating High-Level Test Cases from Requirements using LLM: An Industry Study](https://arxiv.org/abs/2510.03641)
*Satoshi Masuda,Satoshi Kouzawa,Kyousuke Sezai,Hidetoshi Suhara,Yasuaki Hiruta,Kunihiro Kudou*

Main category: cs.SE

TL;DR: 本文提出仅用提示词从需求文档生成高级测试用例的方法，不创建RAG，通过实验验证方法可行性。


<details>
  <summary>Details</summary>
Motivation: 行业有使用大语言模型自动从需求文档生成高级测试用例的需求，现有RAG方法需针对特定应用定制，劳动密集，且无不使用RAG生成通用高级测试用例的方法。

Method: 将需求文档输入大语言模型生成对应测试设计技术，再为每个技术生成高级测试用例，验证基于语义相似度的评估方法。

Result: 使用蓝牙和Mozilla数据集实验，宏召回率分别达0.81和0.37。

Conclusion: 该方法在不使用RAG生成高级测试用例方面具有实际应用可行性。

Abstract: Currently, generating high-level test cases described in natural language
from requirement documents is performed manually. In the industry, including
companies specializing in software testing, there is a significant demand for
the automatic generation of high-level test cases from requirement documents
using Large Language Models (LLMs). Efforts to utilize LLMs for requirement
analysis are underway. In some cases, retrieval-augmented generation (RAG) is
employed for generating high-level test cases using LLMs. However, in practical
applications, it is necessary to create a RAG tailored to the knowledge system
of each specific application, which is labor-intensive. Moreover, when applying
high-level test case generation as a prompt, there is no established method for
instructing the generation of high-level test cases at a level applicable to
other specifications without using RAG. It is required to establish a method
for the automatic generation of high-level test cases that can be generalized
across a wider range of requirement documents. In this paper, we propose a
method for generating high-level (GHL) test cases from requirement documents
using only prompts, without creating RAGs. In the proposed method, first, the
requirement document is input into the LLM to generate test design techniques
corresponding to the requirement document. Then, high-level test cases are
generated for each of the generated test design techniques. Furthermore, we
verify an evaluation method based on semantic similarity of the generated
high-level test cases. In the experiments, we confirmed the method using
datasets from Bluetooth and Mozilla, where requirement documents and high-level
test cases are available, achieving macro-recall measurement of 0.81 and 0.37,
respectively. We believe that the method is feasible for practical application
in generating high-level test cases without using RAG.

</details>


### [430] [Detecting and Preventing Latent Risk Accumulation in High-Performance Software Systems](https://arxiv.org/abs/2510.03712)
*Jahidul Arafat,Kh. M. Moniruzzaman,Shamim Hossain,Fariha Tasmin,Kamrujjaman,Ahsan Habib Tareq*

Main category: cs.SE

TL;DR: 本文提出首个全面框架用于检测、预防和优化分布式系统潜在风险，引入LRI，含三个系统，经测试验证效果良好，生产部署效益显著，转变可靠性工程模式。


<details>
  <summary>Details</summary>
Motivation: 现代分布式系统优化策略带来潜在风险，当前可靠性工程侧重于被动事件响应，缺乏主动检测优化导致的漏洞的方法。

Method: 通过集成数学建模、智能扰动测试和风险感知性能优化，引入潜在风险指数（LRI），框架包含HYDRA、RAVEN和APEX三个系统。

Result: 评估显示有强统计验证和高可重复性，生产部署24周减少恢复时间、事件严重性，避免事故产生年均144万美元效益。

Conclusion: 该方法将可靠性工程从被动事件管理转变为主动风险感知优化。

Abstract: Modern distributed systems employ aggressive optimization strategies that
create latent risks - hidden vulnerabilities where exceptional performance
masks catastrophic fragility when optimizations fail. Cache layers achieving
99% hit rates can obscure database bottlenecks until cache failures trigger
100x load amplification and cascading collapse. Current reliability engineering
focuses on reactive incident response rather than proactive detection of
optimization-induced vulnerabilities. This paper presents the first
comprehensive framework for systematic latent risk detection, prevention, and
optimization through integrated mathematical modeling, intelligent perturbation
testing, and risk-aware performance optimization. We introduce the Latent Risk
Index (LRI) that correlates strongly with incident severity (r=0.863, p<0.001),
enabling predictive risk assessment. Our framework integrates three systems:
HYDRA employing six optimization-aware perturbation strategies achieving 89.7%
risk discovery rates, RAVEN providing continuous production monitoring with
92.9% precision and 93.8% recall across 1,748 scenarios, and APEX enabling
risk-aware optimization maintaining 96.6% baseline performance while reducing
latent risks by 59.2%. Evaluation across three testbed environments
demonstrates strong statistical validation with large effect sizes (Cohen
d>2.0) and exceptional reproducibility (r>0.92). Production deployment over 24
weeks shows 69.1% mean time to recovery reduction, 78.6% incident severity
reduction, and 81 prevented incidents generating 1.44M USD average annual
benefits with 3.2-month ROI. Our approach transforms reliability engineering
from reactive incident management to proactive risk-aware optimization.

</details>


### [431] [APIDA-Chat: Structured Synthesis of API Search Dialogues to Bootstrap Conversational Agents](https://arxiv.org/abs/2510.03743)
*Zachary Eberhart,Collin McMillan*

Main category: cs.SE

TL;DR: 提出开源管道APIDA - Chat解决大语言模型在小众或专有库API解释上的问题，经两阶段训练，提升评估指标，组件开源。


<details>
  <summary>Details</summary>
Motivation: 大语言模型助手在解释小众或专有库API时因微调所需多轮对话数据稀缺而表现不佳。

Method: APIDA - Chat管道分两阶段，第一阶段用对话规划器和教师大模型合成对话集微调学生模型；第二阶段去掉教师模型，用微调模型快速低成本合成新对话。

Result: 微调后的学生模型在单消费级GPU上运行，BLEU从0.38提升到0.50，BERTScore从0.88提升到0.91。

Conclusion: 所有组件模块化并公开，为未来工作提供保守基线，代码和视频演示已开源。

Abstract: Large-language-model assistants are suitable for explaining popular APIs, yet
they falter on niche or proprietary libraries because the multi-turn dialogue
data needed for fine-tuning are scarce. We present APIDA-Chat, an open-source
pipeline that converts symbolic dialogue-act "scripts" into realistic,
domain-grounded API Search conversations using a lightweight model for
inexpensive training data generation. Phase I pairs a legacy dialogue planner
with a high-capability teacher LLM (o4-mini) to synthesize a "gold set" of
realized dialogues; then, a smaller Llama 3.2 3B student model is fine-tuned on
this corpus. Phase II drops the teacher and reuses the same planner with the
fine-tuned model, allowing rapid, low-cost synthesis of new dialogues without
exposing source code to external services. The fine-tuned student improves BLEU
from 0.38 to 0.50 and BERTScore from 0.88 to 0.91 versus the base model while
running entirely on a single consumer GPU. All components are modular and
publicly released to serve as a conservative baseline for future work.
APIDA-Chat is open-sourced at https://github.com/Zeberhart/apida-chat and a
video demo is available at https://youtu.be/YqmZBHyGbPs .

</details>


### [432] [Code4MeV2: a Research-oriented Code-completion Platform](https://arxiv.org/abs/2510.03755)
*Roham Koohestani,Parham Bateni,Aydin Ebrahimi,Behdad Etezadi,Kiarash Karimi,Maliheh Izadi*

Main category: cs.SE

TL;DR: 介绍研究导向的开源代码补全插件Code4MeV2，其具数据收集框架，性能佳，获积极反馈。


<details>
  <summary>Details</summary>
Motivation: AI代码补全工具用户交互数据被大公司私有，阻碍学术研究，使可复现研究和大规模数据分析难以开展。

Method: 采用客户端 - 服务器架构设计Code4MeV2，具备内联代码补全和上下文感知聊天助手，核心是模块化透明数据收集框架；通过专家评估和8名参与者的用户研究评估工具。

Result: Code4MeV2代码补全性能达行业可比水平，平均延迟200毫秒；研究者和日常用户反馈其信息丰富且有用。

Conclusion: 邀请社区采用和贡献该工具。

Abstract: The adoption of AI-powered code completion tools in software development has
increased substantially, yet the user interaction data produced by these
systems remain proprietary within large corporations. This creates a barrier
for the academic community, as researchers must often develop dedicated
platforms to conduct studies on human--AI interaction, making reproducible
research and large-scale data analysis impractical. In this work, we introduce
Code4MeV2, a research-oriented, open-source code completion plugin for
JetBrains IDEs, as a solution to this limitation. Code4MeV2 is designed using a
client--server architecture and features inline code completion and a
context-aware chat assistant. Its core contribution is a modular and
transparent data collection framework that gives researchers fine-grained
control over telemetry and context gathering. Code4MeV2 achieves
industry-comparable performance in terms of code completion, with an average
latency of 200~ms. We assess our tool through a combination of an expert
evaluation and a user study with eight participants. Feedback from both
researchers and daily users highlights its informativeness and usefulness. We
invite the community to adopt and contribute to this tool. More information
about the tool can be found at https://app.code4me.me.

</details>


### [433] [A First Look at the Lifecycle of DL-Specific Self-Admitted Technical Debt](https://arxiv.org/abs/2510.03802)
*Gilberto Recupito,Vincenzo De Martino,Dario Di Nucci,Fabio Palomba*

Main category: cs.SE

TL;DR: 研究分析DL系统中特定SATD的持久性和生命周期，发现其多在开发早中期引入，训练和硬件阶段持续久，强调需针对性管理策略。


<details>
  <summary>Details</summary>
Motivation: DL系统虽推动软件开发创新，但带来软件质量和性能挑战，其中SATD影响系统可维护性和质量，而DL特定SATD的生命周期未被充分研究。

Method: 采用挖掘软件仓库技术，研究40个ML项目中的185个DL特定SATD实例，通过项目提交历史跟踪其引入和持续情况。

Result: DL特定SATD多在项目开发早中期引入，训练和硬件阶段SATD持续时间最长，开发者在功能实现和修复漏洞时更频繁引入。

Conclusion: DL系统需要针对性的SATD管理策略，开发者可根据其时间特征和演变在关键阶段干预，提升系统可维护性和质量。

Abstract: The rapid adoption of Deep Learning (DL)-enabled systems has revolutionized
software development, driving innovation across various domains. However, these
systems also introduce unique challenges, particularly in maintaining software
quality and performance. Among these challenges, Self-Admitted Technical Debt
(SATD) has emerged as a growing concern, significantly impacting the
maintainability and overall quality of ML and DL-enabled systems. Despite its
critical implications, the lifecycle of DL-specific SATD, how developers
introduce, acknowledge, and address it over time-remains underexplored. This
study presents a preliminary analysis of the persistence and lifecycle of
DL-specific SATD in DL-enabled systems. The purpose of this project is to
uncover the patterns of SATD introduction, recognition, and durability during
the development life cycle, providing information on how to manage these
issues. Using mining software repository techniques, we examined 40 ML
projects, focusing on 185 DL-specific SATD instances. The analysis tracked the
introduction and persistence of SATD instances through project commit histories
to assess their lifecycle and developer actions. The findings indicate that
DL-specific SATD is predominantly introduced during the early and middle stages
of project development. Training and Hardware phases showed the longest SATD
durations, highlighting critical areas where debt accumulates and persists.
Additionally, developers introduce DL-specific SATD more frequently during
feature implementation and bug fixes. This study emphasizes the need for
targeted DL-specific SATD management strategies in DL-enabled systems to
mitigate its impact. By understanding the temporal characteristics and
evolution of DL-specific SATD, developers can prioritize interventions at
critical stages to improve the maintainability and quality of the system.

</details>


### [434] [Smart Paste: Automatically Fixing Copy/Paste for Google Developers](https://arxiv.org/abs/2510.03843)
*Vincent Nguyen,Guilherme Herzog,José Cambronero,Marcus Revaj,Aditya Kini,Alexander Frömmgen,Maxim Tabachnyk*

Main category: cs.SE

TL;DR: 本文介绍谷歌将深度学习应用于IDE的Smart Paste功能开发，该功能可提供粘贴后编辑建议，部署后反馈良好。


<details>
  <summary>Details</summary>
Motivation: 手动编辑粘贴代码是开发者痛点，谷歌内部粘贴代码频率高且需后续编辑，此前研究表明深度学习可预测编辑。

Method: 迭代开发并将Smart Paste功能扩展到谷歌开发环境。

Result: Smart Paste获得积极反馈，接受率45%，在企业规模下其被接受建议占公司代码总量超1%。

Conclusion: 此开发经验可指导AI从业者进行涵盖用户体验、系统集成和模型能力的整体功能开发。

Abstract: Manually editing pasted code is a long-standing developer pain point. In
internal software development at Google, we observe that code is pasted 4 times
more often than it is manually typed. These paste actions frequently require
follow-up edits, ranging from simple reformatting and renaming to more complex
style adjustments and cross-language translations. Prior work has shown deep
learning can be used to predict these edits. In this work, we show how to
iteratively develop and scale Smart Paste, an IDE feature for post-paste edit
suggestions, to Google's development environment. This experience can serve as
a guide for AI practitioners on a holistic approach to feature development,
covering user experience, system integration, and model capabilities. Since
deployment, Smart Paste has had overwhelmingly positive feedback with a 45%
acceptance rate. At Google's enterprise scale, these accepted suggestions
account substantially for over 1% of all code written company-wide.

</details>


### [435] [Designing Empirical Studies on LLM-Based Code Generation: Towards a Reference Framework](https://arxiv.org/abs/2510.03862)
*Nathalia Nascimento,Everton Guimaraes,Paulo Alencar*

Main category: cs.SE

TL;DR: 本文针对LLM代码生成实证评估缺乏标准化问题，提出理论框架，展示其适用性并计划完善。


<details>
  <summary>Details</summary>
Motivation: 解决LLM代码生成实证评估缺乏标准化，研究目标、任务和指标差异大，限制可比性和可重复性的问题。

Method: 基于过往实验经验和对近期研究的对比分析，围绕问题来源、质量属性和指标等核心组件构建评估框架。

Result: 通过代表性案例映射展示了框架的适用性，还识别了改进机会。

Conclusion: 计划将框架发展成更强大成熟的工具，用于软件工程场景中LLM评估的标准化。

Abstract: The rise of large language models (LLMs) has introduced transformative
potential in automated code generation, addressing a wide range of software
engineering challenges. However, empirical evaluation of LLM-based code
generation lacks standardization, with studies varying widely in goals, tasks,
and metrics, which limits comparability and reproducibility. In this paper, we
propose a theoretical framework for designing and reporting empirical studies
on LLM-based code generation. The framework is grounded in both our prior
experience conducting such experiments and a comparative analysis of key
similarities and differences among recent studies. It organizes evaluation
around core components such as problem sources, quality attributes, and
metrics, supporting structured and systematic experimentation. We demonstrate
its applicability through representative case mappings and identify
opportunities for refinement. Looking forward, we plan to evolve the framework
into a more robust and mature tool for standardizing LLM evaluation across
software engineering contexts.

</details>


### [436] [Adversarial Agent Collaboration for C to Rust Translation](https://arxiv.org/abs/2510.03879)
*Tianyu Li,Ruishi Li,Bo Wang,Brandon Paulsen,Umang Mathur,Prateek Saxena*

Main category: cs.SE

TL;DR: 提出ACToR将C语言翻译成安全的Rust语言，在基准测试中表现良好且优于非对抗性方法。


<details>
  <summary>Details</summary>
Motivation: 现有C到Rust的翻译方法在处理大型C代码库时无法通用，依赖复杂程序分析且易出错。

Method: 受GAN启发，通过生成器和判别器代理协作迭代生成Rust翻译。

Result: ACToR能翻译基准测试中的63个真实命令行实用程序，平均485行代码，测试通过率超90%，零人工干预。

Conclusion: ACToR是首个能可靠翻译该规模C程序的系统，比非对抗性基线方法提高翻译正确性达18.9%。

Abstract: Translating C to memory-safe languages, like Rust, prevents critical memory
safety vulnerabilities that are prevalent in legacy C software. Existing
approaches for C to safe Rust translation, including LLM-assisted ones, do not
generalize on larger (> 500 LoC) C codebases because they depend on complex
program analyses that frequently break. In this work, we present ACToR
(Adversarial C To Rust translator), a simple LLM agent-based approach. Inspired
by GANs, ACToR pits a generator agent against a discriminator agent, which
collaborate to iteratively generate a Rust translation. On each iteration, the
translator agent synthesizes and refines a Rust translation to pass an existing
suite of tests, and then the discriminator agent finds new failing tests. We
demonstrate that ACToR translates all of the 63 real-world command line
utilities considered in our benchmarks, which have an average size of 485 lines
of code, and it achieves over 90% test pass rate with zero human intervention.
To our knowledge, it is the first such system that reliably translates C
programs of this scale. Furthermore, ACToR improves translation correctness by
up to 18.9% compared to baseline, non-adversarial approaches.

</details>


### [437] [Rethinking Services in the Quantum Age: The SOQ Paradigm](https://arxiv.org/abs/2510.03890)
*Jose Garcia-Alonso,Enrique Moguel,Jaime Alvarado-Valiente,Javier Romero-Alvarez,Álvaro M. Aparicio-Morales,Juan M. Murillo,Francisco Javier Cavero,Adrián Romero-Flores,Alfonso E. Marquez-Chamorro,José Antonio Parejo,Antonio Ruiz-Cortés,Giuseppe Bisicchia,Alessandro Bocci,Antonio Brogi*

Main category: cs.SE

TL;DR: 本文提出面向服务的量子（SOQ）范式，可将量子计算独立、可扩展地集成到现实软件系统。


<details>
  <summary>Details</summary>
Motivation: 量子计算集成到现实软件系统受硬件脆弱性、平台异构性和缺乏软件工程实践等因素限制。

Method: 引入SOQ范式，定义其基本原则，提出分层技术栈，并确定需解决的关键挑战。

Result: 提出了新的SOQ范式及相关技术栈和待解决挑战。

Conclusion: SOQ方法对量子技术发展至关重要，能实现量子计算在现实软件系统中的可扩展、模块化和互操作集成。

Abstract: Quantum computing is rapidly progressing from theoretical promise to
practical implementation, offering significant computational advantages for
tasks in optimization, simulation, cryptography, and machine learning. However,
its integration into real-world software systems remains constrained by
hardware fragility, platform heterogeneity, and the absence of robust software
engineering practices. This paper introduces Service-Oriented Quantum (SOQ), a
novel paradigm that reimagines quantum software systems through the lens of
classical service-oriented computing. Unlike prior approaches such as Quantum
Service-Oriented Computing (QSOC), which treat quantum capabilities as
auxiliary components within classical systems, SOQ positions quantum services
as autonomous, composable, and interoperable entities. We define the
foundational principles of SOQ, propose a layered technology stack to support
its realization, and identify the key research and engineering challenges that
must be addressed, including interoperability, hybridity, pricing models,
service abstractions, and workforce development. This approach is of vital
importance for the advancement of quantum technology because it enables the
scalable, modular, and interoperable integration of quantum computing into
real-world software systems independently and without relying on a dedicated
classical environment to manage quantum processing.

</details>


### [438] [A Brief History of the Waterfall Model: Past, Present, and Future](https://arxiv.org/abs/2510.03894)
*Antonios Saravanos*

Main category: cs.SE

TL;DR: 本文对瀑布模型进行历史和批判性综述，指出其虽有缺陷但仍具相关性，可融入混合方法，从业者应据此做决策。


<details>
  <summary>Details</summary>
Motivation: 探究瀑布模型在当代软件开发实践中的地位和作用，明确其是否仍有价值。

Method: 通过参考大量学术资源，综合分析瀑布模型的概念起源、形式化过程、发展演变及应用情况。

Result: 瀑布模型从独立框架转变为现代混合方法的组成部分，其原则仍影响当代混合开发框架。

Conclusion: 瀑布模型因适应性而保持相关性，从业者应认识其优缺点，在不同开发环境中合理选择方法和设计流程。

Abstract: The waterfall model, one of the earliest software development methodologies,
has played a foundational role in shaping contemporary software engineering
practices. This paper provides a historical and critical overview of the model,
tracing its conceptual origins in software engineering, its formalization by
Royce, and its evolution through decades of industry adoption and critique.
Although often criticized for its rigidity, shortcomings, and high failure
rates, the waterfall model persists in specific domains. Its principles
continue to influence contemporary hybrid development frameworks that combine
traditional and agile methods. Drawing on a range of scholarly sources, this
study synthesizes key developments in the perception and application of the
waterfall model. The analysis highlights how the model has shifted from a
standalone framework to a component within modern hybrid methodologies. By
revisiting its origins, assessing its present utility, and examining its role
in contemporary development practices, this paper argues that the waterfall
model remains relevant, not as a relic of the past but as part of context-aware
development strategies. The paper contends that the model's enduring relevance
lies in its adaptability. By recognizing both its limitations and its
strengths, and by understanding its integration within hybrid approaches,
practitioners can make more informed decisions about methodology selection and
process design in diverse development environments.

</details>


### [439] [Multi-Agent Code-Orchestrated Generation for Reliable Infrastructure-as-Code](https://arxiv.org/abs/2510.03902)
*Rana Nameer Hussain Khan,Dawood Wasif,Jin-Hee Cho,Ali Butt*

Main category: cs.SE

TL;DR: 本文提出MACOG架构用于生成基础设施即代码（IaC），能生成有效且合规的Terraform配置，在基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 云原生基础设施复杂性增加使IaC重要，但大语言模型单步生成IaC片段有语法错误、策略违规和不可扩展等问题。

Method: 提出MACOG多智能体架构，将任务分解为模块化子任务由专业智能体处理，通过共享黑板和有限状态编排层交互；结合Terraform Plan和Open Policy Agent保证正确性和治理。

Result: 在IaC - Eval基准测试中，MACOG提升显著，如GPT - 5从54.90提升到74.02，Gemini - 2.5 Pro从43.56提升到60.13；消融实验表明受限解码和部署反馈很关键。

Conclusion: MACOG架构在IaC生成方面有效，能提升生成质量，受限解码和部署反馈对性能有重要影响。

Abstract: The increasing complexity of cloud-native infrastructure has made
Infrastructure-as-Code (IaC) essential for reproducible and scalable
deployments. While large language models (LLMs) have shown promise in
generating IaC snippets from natural language prompts, their monolithic,
single-pass generation approach often results in syntactic errors, policy
violations, and unscalable designs. In this paper, we propose MACOG
(Multi-Agent Code-Orchestrated Generation), a novel multi-agent LLM-based
architecture for IaC generation that decomposes the task into modular subtasks
handled by specialized agents: Architect, Provider Harmonizer, Engineer,
Reviewer, Security Prover, Cost and Capacity Planner, DevOps, and Memory
Curator. The agents interact via a shared-blackboard, finite-state orchestrator
layer, and collectively produce Terraform configurations that are not only
syntactically valid but also policy-compliant and semantically coherent. To
ensure infrastructure correctness and governance, we incorporate Terraform Plan
for execution validation and Open Policy Agent (OPA) for customizable policy
enforcement. We evaluate MACOG using the IaC-Eval benchmark, where MACOG is the
top enhancement across models, e.g., GPT-5 improves from 54.90 (RAG) to 74.02
and Gemini-2.5 Pro from 43.56 to 60.13, with concurrent gains on BLEU,
CodeBERTScore, and an LLM-judge metric. Ablations show constrained decoding and
deploy feedback are critical: removing them drops IaC-Eval to 64.89 and 56.93,
respectively.

</details>


### [440] [Refactoring with LLMs: Bridging Human Expertise and Machine Understanding](https://arxiv.org/abs/2510.03914)
*Yonnel Chen Kuang Piao,Jean Carlors Paul,Leuson Da Silva,Arghavan Moradi Dakhel,Mohammad Hamdaqa,Foutse Khomh*

Main category: cs.SE

TL;DR: 研究探索借人类最佳实践准则的指令策略提升大语言模型自动执行多样重构任务的能力，评估显示该策略有效，不同指令各有优势。


<details>
  <summary>Details</summary>
Motivation: 开发者常因成本高且无即时功能回报而忽视代码重构，现有自动重构工具支持的重构类型有限，需提升大语言模型自动执行重构任务的能力。

Method: 利用大语言模型的指令遵循和代码理解能力，依据Martin Fowler的重构准则设计指令策略，在基准示例和真实代码片段上评估。

Result: 基于Fowler准则的指令设计使大语言模型能完成所有基准重构类型，在真实场景中保留程序语义；规则型指令在特定场景表现更好；专注重构总体目标可提升代码质量。

Conclusion: 基于人类最佳实践准则的指令策略可增强大语言模型自动执行多样重构任务的能力。

Abstract: Code refactoring is a fundamental software engineering practice aimed at
improving code quality and maintainability. Despite its importance, developers
often neglect refactoring due to the significant time, effort, and resources it
requires, as well as the lack of immediate functional rewards. Although several
automated refactoring tools have been proposed, they remain limited in
supporting a broad spectrum of refactoring types. In this study, we explore
whether instruction strategies inspired by human best-practice guidelines can
enhance the ability of Large Language Models (LLMs) to perform diverse
refactoring tasks automatically. Leveraging the instruction-following and code
comprehension capabilities of state-of-the-art LLMs (e.g., GPT-mini and
DeepSeek-V3), we draw on Martin Fowler's refactoring guidelines to design
multiple instruction strategies that encode motivations, procedural steps, and
transformation objectives for 61 well-known refactoring types. We evaluate
these strategies on benchmark examples and real-world code snippets from GitHub
projects. Our results show that instruction designs grounded in Fowler's
guidelines enable LLMs to successfully perform all benchmark refactoring types
and preserve program semantics in real-world settings, an essential criterion
for effective refactoring. Moreover, while descriptive instructions are more
interpretable to humans, our results show that rule-based instructions often
lead to better performance in specific scenarios. Interestingly, allowing
models to focus on the overall goal of refactoring, rather than prescribing a
fixed transformation type, can yield even greater improvements in code quality.

</details>


### [441] [Why Does the Engineering Manager Still Exist in Agile Software Development?](https://arxiv.org/abs/2510.03920)
*Ravi Kalluri*

Main category: cs.SE

TL;DR: 本文探讨敏捷软件开发组织中工程经理持续存在的现象，通过多维度框架分析，提出协调敏捷原则与管理必要性的概念模型。


<details>
  <summary>Details</summary>
Motivation: 解释敏捷方法强调去中心化决策和团队自主，但工程经理仍存在于敏捷软件组织这一矛盾现象。

Method: 采用系统文献综述进行多维度分析，并辅以案例研究。

Result: 提出了协调敏捷原则与管理必要性的概念模型。

Conclusion: 该概念模型为从业者、研究者和工具设计者提供指导，并讨论了对领导力发展、工具集成和未来研究的影响。

Abstract: Although Agile methodologies emphasize decentralized decision-making and team
autonomy, engineering managers continue to be employed in Agile software
organizations. This apparent paradox suggests that traditional managerial
functions persist despite the theoretical displacement of managerial hierarchy
in Agile. This paper explores the persistence of engineering managers through a
multidimensional framework encompassing historical context, theoretical
tensions, organizational realities, empirical evidence, evolving managerial
roles, and practical implications. A systematic literature review underpins our
multifaceted analysis, supplemented by illustrative case studies. We conclude
by proposing a conceptual model that reconciles Agile principles with
managerial necessity, offering guidance for practitioners, researchers, and
tool designers. Implications for leadership development, tool integration, and
future research are discussed.

</details>


### [442] [Bamboo: LLM-Driven Discovery of API-Permission Mappings in the Android Framework](https://arxiv.org/abs/2510.04078)
*Han Hu,Wei Minn,Yonghui Liu,Jiakun Liu,Ferdian Thung,Terry Yue Zhuo,Lwin Khin Shar,Debin Gao,David Lo*

Main category: cs.SE

TL;DR: 现有安卓权限文档不精确完整，分析方法有缺陷，本文用大语言模型等开发工具分析API - 权限映射，实验效果超现有基线。


<details>
  <summary>Details</summary>
Motivation: 安卓官方API文档不精确完整，开发者难以准确识别权限，现有改进方法有诸多不足，需新方法。

Method: 利用大语言模型，结合双角色提示策略和API驱动代码生成方法开发工具。

Result: 工具在安卓6、7、10版本分别识别出2234、3552和4576个API - 权限映射，远超现有基线。

Conclusion: 所开发工具在API - 权限映射分析方面效果显著，优于现有方法。

Abstract: The permission mechanism in the Android Framework is integral to safeguarding
the privacy of users by managing users' and processes' access to sensitive
resources and operations. As such, developers need to be equipped with an
in-depth understanding of API permissions to build robust Android apps.
Unfortunately, the official API documentation by Android chronically suffers
from imprecision and incompleteness, causing developers to spend significant
effort to accurately discern necessary permissions. This potentially leads to
incorrect permission declarations in Android app development, potentially
resulting in security violations and app failures. Recent efforts in improving
permission specification primarily leverage static and dynamic code analyses to
uncover API-permission mappings within the Android framework. Yet, these
methodologies encounter substantial shortcomings, including poor adaptability
to Android SDK and Framework updates, restricted code coverage, and a
propensity to overlook essential API-permission mappings in intricate
codebases. This paper introduces a pioneering approach utilizing large language
models (LLMs) for a systematic examination of API-permission mappings. In
addition to employing LLMs, we integrate a dual-role prompting strategy and an
API-driven code generation approach into our mapping discovery pipeline,
resulting in the development of the corresponding tool, \tool{}. We formulate
three research questions to evaluate the efficacy of \tool{} against
state-of-the-art baselines, assess the completeness of official SDK
documentation, and analyze the evolution of permission-required APIs across
different SDK releases. Our experimental results reveal that \tool{} identifies
2,234, 3,552, and 4,576 API-permission mappings in Android versions 6, 7, and
10 respectively, substantially outprforming existing baselines.

</details>


### [443] [GA4GC: Greener Agent for Greener Code via Multi-Objective Configuration Optimization](https://arxiv.org/abs/2510.04135)
*Jingzhi Gong,Yixin Bian,Luis de la Cal,Giovanni Pinna,Anisha Uteem,David Williams,Mar Zamorano,Karine Even-Mendoza,W. B. Langdon,Hector Menendez,Federica Sarro*

Main category: cs.SE

TL;DR: 本文提出GA4GC框架优化编码代理运行时间和代码性能权衡，评估显示有显著改进并给出关键参数和策略。


<details>
  <summary>Details</summary>
Motivation: 编码代理在工业部署中面临可持续性和可扩展性挑战，单次运行消耗大量令牌且环境成本可能超优化收益。

Method: 引入GA4GC框架，通过发现帕累托最优的代理超参数和提示模板来优化权衡。

Result: 在SWE - Perf基准测试中，超体积提升达135倍，代理运行时间减少37.7%，正确性提高。

Conclusion: 确定温度是最关键的超参数，提供在工业部署中平衡代理可持续性和代码优化有效性的可行策略。

Abstract: Coding agents powered by LLMs face critical sustainability and scalability
challenges in industrial deployment, with single runs consuming over 100k
tokens and incurring environmental costs that may exceed optimization benefits.
This paper introduces GA4GC, the first framework to systematically optimize
coding agent runtime (greener agent) and code performance (greener code)
trade-offs by discovering Pareto-optimal agent hyperparameters and prompt
templates. Evaluation on the SWE-Perf benchmark demonstrates up to 135x
hypervolume improvement, reducing agent runtime by 37.7% while improving
correctness. Our findings establish temperature as the most critical
hyperparameter, and provide actionable strategies to balance agent
sustainability with code optimization effectiveness in industrial deployment.

</details>


### [444] [Detecting Semantic Clones of Unseen Functionality](https://arxiv.org/abs/2510.04143)
*Konstantinos Kitsios,Francesco Sovrano,Earl T. Barr,Alberto Bacchelli*

Main category: cs.SE

TL;DR: 重新评估六种模型检测未见功能代码克隆的能力，发现任务特定模型F1下降明显，LLMs泛化性好，提出对比学习方法提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有神经模型在检测未见功能代码克隆时存在困难，而开发者对此有需求，需重新评估模型并提升检测能力。

Method: 重新评估六种模型；对任务特定模型用对比分类器替换最终分类器，对生成式LLMs提出对比上下文学习。

Result: 任务特定模型F1最多下降48%，平均31%；LLMs F1最多下降5%，平均3%；采用对比学习后，任务特定模型F1最多提升26%，平均9%，LLMs最多提升5%，平均3%。

Conclusion: 对比学习能有效提升现有模型对未见功能代码克隆的检测性能。

Abstract: Semantic code clone detection is the task of detecting whether two snippets
of code implement the same functionality (e.g., Sort Array). Recently, many
neural models achieved near-perfect performance on this task. These models seek
to make inferences based on their training data. Consequently, they better
detect clones similar to those they have seen during training and may struggle
to detect those they have not. Developers seeking clones are, of course,
interested in both types of clones. We confirm this claim through a literature
review, identifying three practical clone detection tasks in which the model's
goal is to detect clones of a functionality even if it was trained on clones of
different functionalities. In light of this finding, we re-evaluate six
state-of-the-art models, including both task-specific models and generative
LLMs, on the task of detecting clones of unseen functionality. Our experiments
reveal a drop in F1 of up to 48% (average 31%) for task-specific models. LLMs
perform on par with task-specific models without explicit training for clone
detection, but generalize better to unseen functionalities, where F1 drops up
to 5% (average 3%) instead. We propose and evaluate the use of contrastive
learning to improve the performance of existing models on clones of unseen
functionality. We draw inspiration from the computer vision and natural
language processing fields where contrastive learning excels at measuring
similarity between two objects, even if they come from classes unseen during
training. We replace the final classifier of the task-specific models with a
contrastive classifier, while for the generative LLMs we propose contrastive
in-context learning, guiding the LLMs to focus on the differences between
clones and non-clones. The F1 on clones of unseen functionality is improved by
up to 26% (average 9%) for task-specific models and up to 5% (average 3%) for
LLMs.

</details>


### [445] [Multi Language Models for On-the-Fly Syntax Highlighting](https://arxiv.org/abs/2510.04166)
*Marco Edoardo Palma,Pooja Rani,Harald C. Gall*

Main category: cs.SE

TL;DR: 文章提出统一模型解决语法高亮问题，减少部署复杂度，提升多语言性能。


<details>
  <summary>Details</summary>
Motivation: 在线和基于Web的开发工具实时准确语法高亮有挑战，现有深度学习模型有局限。

Method: 引入统一模型支持六种主流编程语言，采用新归一化技术，结合少样本学习。

Result: 减少部署复杂度6倍，提升未见语言性能，减少对暴力生成器依赖。

Conclusion: 实现跨多种编程语言高效、可扩展且经济的语法高亮。

Abstract: Syntax highlighting is a critical feature in modern software development
environments, enhancing code readability and developer productivity. However,
delivering accurate highlighting in real time remains challenging for online
and web-based development tools due to strict time and memory constraints on
backend services. These systems must serve highlights rapidly and frequently,
even when code is partially valid or invalid. This has led to on-the-fly syntax
highlighting, where visual annotations are generated just before content is
served, often at high request rates and under incomplete input conditions. To
meet these demands efficiently, state-of-the-art models use deep learning to
learn the behavior of brute-force syntax highlighting resolvers, tools that are
easy to implement but too slow for production. Through the Deep Abstraction
process, brute-force strategies are encoded into fast statistical models that
achieve both high accuracy and low-latency inference. Despite their success,
such models face key challenges: they support only one programming language per
model, require large datasets from slow brute-force generators, and involve
resource-intensive training. In multi-language environments, this means
maintaining multiple independent models, increasing system complexity and
operational cost. This work addresses these issues by introducing a unified
model capable of highlighting up to six mainstream programming languages,
reducing deployment complexity by a factor of six and improving performance on
unseen languages. A novel normalization technique significantly enhances model
generalization, while few-shot learning experiments show that a small number of
oracle samples can replace large datasets, minimizing dependence on brute-force
generators. Combined, these innovations enable efficient, scalable, and
cost-effective syntax highlighting across diverse programming languages.

</details>


### [446] [Selecting Cybersecurity Requirements: Effects of LLM Use and Professional Software Development Experience](https://arxiv.org/abs/2510.04274)
*Damjan Fujs,Damjan Vavpotič,Tomaž Hovelja,Marko Poženel*

Main category: cs.SE

TL;DR: 研究访问大语言模型和不同软件开发经验水平对网页应用程序网络安全需求优先级排序的影响，发现LLM使用无显著差异，经验组有差异。


<details>
  <summary>Details</summary>
Motivation: 探究访问大语言模型和不同专业软件开发经验水平如何影响网页应用程序网络安全需求的优先级排序。

Method: 让23名研究生用MoSCoW方法对安全需求进行优先级排序，按是否有LLM支持分组，并根据多个评估标准对解决方案进行评分。

Result: LLM使用无显著差异；在开发成本、用户体验影响、风险评估等标准上，经验组有显著差异，经验多的参与者对用户体验影响评分高、风险估计低。

Conclusion: 访问LLMs未明显影响参与者对网络安全解决方案的评估，专业经验在部分评估标准上有显著影响。

Abstract: This study investigates how access to Large Language Models (LLMs) and
varying levels of professional software development experience affect the
prioritization of cybersecurity requirements for web applications. Twenty-three
postgraduate students participated in a research study to prioritize security
requirements (SRs) using the MoSCoW method and subsequently rated their
proposed solutions against multiple evaluation criteria. We divided
participants into two groups (one with and the other without access to LLM
support during the task). Results showed no significant differences related to
LLM use, suggesting that access to LLMs did not noticeably influence how
participants evaluated cybersecurity solutions. However, statistically
significant differences emerged between experience groups for certain criteria,
such as estimated cost to develop a feature, perceived impact on user
experience, and risk assessment related to non-implementation of the proposed
feature. Participants with more professional experience tended to provide
higher ratings for user experience impact and lower risk estimates.

</details>


### [447] [Challenge on Optimization of Context Collection for Code Completion](https://arxiv.org/abs/2510.04349)
*Dmitry Ustalov,Egor Bogomolov,Alexander Bezzubov,Yaroslav Golubev,Evgeniy Glukhov,Georgii Levtsov,Vladimir Kovalenko*

Main category: cs.SE

TL;DR: 论文围绕软件工程中AI工作流和方法，聚焦JetBrains与Mistral AI组织的代码补全上下文收集优化挑战，介绍构建数据集、评估方式和竞赛各阶段情况。


<details>
  <summary>Details</summary>
Motivation: 随着软件工程中AI工作流和方法快速发展，需要系统评估其从整个项目（尤其是大型代码库）中利用信息的能力。

Method: 构建Python和Kotlin两种编程语言的真实代码大型数据集，用chrF指标评估参赛方案对多个先进神经模型的补全质量。

Result: 公开阶段，19个团队提交Python赛道方案，8个团队提交Kotlin赛道方案；私有阶段6个团队参赛，5个团队提交论文到研讨会。

Conclusion: 未提及明确结论。

Abstract: The rapid advancement of workflows and methods for software engineering using
AI emphasizes the need for a systematic evaluation and analysis of their
ability to leverage information from entire projects, particularly in large
code bases. In this challenge on optimization of context collection for code
completion, organized by JetBrains in collaboration with Mistral AI as part of
the ASE 2025 conference, participants developed efficient mechanisms for
collecting context from source code repositories to improve fill-in-the-middle
code completions for Python and Kotlin. We constructed a large dataset of
real-world code in these two programming languages using permissively licensed
open-source projects. The submissions were evaluated based on their ability to
maximize completion quality for multiple state-of-the-art neural models using
the chrF metric. During the public phase of the competition, nineteen teams
submitted solutions to the Python track and eight teams submitted solutions to
the Kotlin track. In the private phase, six teams competed, of which five
submitted papers to the workshop.

</details>


### [448] [MacroBench: A Novel Testbed for Web Automation Scripts via Large Language Models](https://arxiv.org/abs/2510.04363)
*Hyunjun Kim,Sejong Kim*

Main category: cs.SE

TL;DR: 介绍MacroBench基准测试，评估大语言模型从自然语言目标合成浏览器自动化程序的能力，给出不同模型测试结果并发布相关资源。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型能否根据自然语言目标，通过读取HTML/DOM合成可复用的浏览器自动化程序。

Method: 创建MacroBench，实例化七个自托管网站，涵盖681个任务；通过静态检查、沙盒执行和结果验证等协议验证生成代码，包含安全套件。

Result: 在2636次模型 - 任务运行中，不同模型有不同成功率，简单任务成功率91.7%，复杂工作流成功率0.0%，无模型达到生产级编码实践。

Conclusion: 发布完整基准测试管道、评估框架和实验结果，以实现网页自动化宏合成的可重复评估。

Abstract: We introduce MacroBench, a code-first benchmark that evaluates whether LLMs
can synthesize reusable browser automation programs from natural language goals
by reading HTML/DOM and emitting Python with Selenium. MacroBench instantiates
seven self-hosted sites: Airbnb-like, TikTok-like, Reddit-like, Instagram-like,
Facebook-like, Discord-like, and Threads-like, covering 681 tasks across
interaction complexity and targeting difficulty. Our end-to-end protocol
validates generated code via static checks, sandboxed execution, and outcome
verification including DOM assertions and database snapshots, and includes a
safety suite for scraping, spam/abuse, and credential/privacy prompts. Across
2636 model-task runs, we observe stratified success: GPT-4o-Mini achieves 96.8
percent, GPT-4.1 achieves 95.3 percent, Gemini-2.5-Pro achieves 89.0 percent,
and DeepSeek-V3.1 achieves 83.4 percent. Models handle simple tasks reliably at
91.7 percent but fail on complex workflows at 0.0 percent, and none meet
production-quality coding practices despite functional completion. We release
our complete benchmark pipeline, evaluation framework, and experimental results
to enable reproducible assessment of macro synthesis for web automation.

</details>


### [449] [Reconsidering Requirements Engineering: Human-AI Collaboration in AI-Native Software Development](https://arxiv.org/abs/2510.04380)
*Mateen Ahmed Abbasi,Petri Ihantola,Tommi Mikkonen,Niko Mäkitalo*

Main category: cs.SE

TL;DR: 本文探讨AI如何提升传统需求工程实践，分析其带来的机遇与挑战，呼吁道德实践及产学研合作，打造可靠实用的AI解决方案。


<details>
  <summary>Details</summary>
Motivation: 需求工程面临诸多挑战，AI虽有潜力优化该过程，但也带来新问题，因此探索AI在需求工程中的应用。

Method: 无明确提及，主要围绕AI在需求工程中自动化任务、支持需求优先级排序和促进协作等方面展开探讨。

Result: 分析出AI能提升传统需求工程实践，也带来伦理、偏见和透明度等新问题。

Conclusion: 呼吁在AI中采用道德实践，加强产学研合作，创建强大、可靠且实用的AI解决方案以适应软件开发。

Abstract: Requirement Engineering (RE) is the foundation of successful software
development. In RE, the goal is to ensure that implemented systems satisfy
stakeholder needs through rigorous requirements elicitation, validation, and
evaluation processes. Despite its critical role, RE continues to face
persistent challenges, such as ambiguity, conflicting stakeholder needs, and
the complexity of managing evolving requirements. A common view is that
Artificial Intelligence (AI) has the potential to streamline the RE process,
resulting in improved efficiency, accuracy, and management actions. However,
using AI also introduces new concerns, such as ethical issues, biases, and lack
of transparency. This paper explores how AI can enhance traditional RE
practices by automating labor-intensive tasks, supporting requirement
prioritization, and facilitating collaboration between stakeholders and AI
systems. The paper also describes the opportunities and challenges that AI
brings to RE. In particular, the vision calls for ethical practices in AI,
along with a much-enhanced collaboration between academia and industry
professionals. The focus should be on creating not only powerful but also
trustworthy and practical AI solutions ready to adapt to the fast-paced world
of software development.

</details>


### [450] [Smart Hiring Redefined: An Intelligent Recruitment Management Platform](https://arxiv.org/abs/2510.04437)
*Fangzhe Wu,Dongyang Lyu,Xiaoqi Li*

Main category: cs.SE

TL;DR: 在人力资源管理数智化转型背景下，传统招聘模式有局限，智能招聘管理系统能提升招聘效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统招聘模式效率有限、成本高、信息不对称，难以满足企业精准人才获取需求。

Method: 智能招聘系统采用自动化和数据驱动的方法。

Result: 能快速解析大量简历、智能匹配候选人与岗位、自动安排面试流程。

Conclusion: 智能招聘管理系统是现代组织人才战略不可或缺的组成部分，可提升招聘效率和准确性。

Abstract: Against the backdrop of deepening digital and intelligent transformation in
human resource management, traditional recruitment models struggle to fully
meet enterprises' growing demand for precise talent acquisition due to limited
efficiency, high costs, and information asymmetry. As a vital tool for
optimizing recruitment processes, reducing labor and time costs, and enhancing
core competitiveness, intelligent recruitment management systems become an
indispensable component of modern organizational talent strategies.Compared
with the labor intensive tasks of resume screening, candidate position
matching, and interview coordination in traditional manual recruitment,
intelligent recruitment systems significantly enhance the efficiency and
accuracy of the hiring process through automation and data driven approaches.
These systems enable rapid parsing of massive resume volumes, intelligent
matching of candidates to positions, and automated scheduling of interview
processes.

</details>


### [451] [Improving IR-based Bug Localization with Semantics-Driven Query Reduction](https://arxiv.org/abs/2510.04468)
*Asif Mohammed Samir,Mohammad Masudur Rahman*

Main category: cs.SE

TL;DR: 提出新的软件漏洞定位方法IQLoc，结合IR和LLM优势，实验显示其性能优越，缓解传统IR方法挑战。


<details>
  <summary>Details</summary>
Motivation: 现有软件漏洞定位方法存在不足，IR方法忽略代码上下文和语义，LLM未很好适配漏洞定位且可能耗资源，需新方法。

Method: 提出IQLoc，利用Transformer模型理解程序语义判断代码可疑性，用信息检索重新构造查询；改进Bench4BL数据集，用三个指标评估IQLoc并与四个基线技术对比。

Result: IQLoc在MAP、MRR、HIT@K等指标表现优越，在不同类型的错误报告中提升MAP显著。

Conclusion: 将程序语义理解融入信息检索，IQLoc缓解了传统基于IR方法在漏洞定位中的长期挑战。

Abstract: Despite decades of research, software bug localization remains challenging
due to heterogeneous content and inherent ambiguities in bug reports. Existing
methods such as Information Retrieval (IR)-based approaches often attempt to
match source documents to bug reports, overlooking the context and semantics of
the source code. On the other hand, Large Language Models (LLM) (e.g.,
Transformer models) show promising results in understanding both texts and
code. However, they have not been yet adapted well to localize software bugs
against bug reports. They could be also data or resource-intensive. To bridge
this gap, we propose, IQLoc, a novel bug localization approach that capitalizes
on the strengths of both IR and LLM-based approaches. In particular, we
leverage the program semantics understanding of transformer-based models to
reason about the suspiciousness of code and reformulate queries during bug
localization using Information Retrieval. To evaluate IQLoc, we refine the
Bench4BL benchmark dataset and extend it by incorporating ~30% more recent bug
reports, resulting in a benchmark containing ~7.5K bug reports. We evaluated
IQLoc using three performance metrics and compare it against four baseline
techniques. Experimental results demonstrate its superiority, achieving up to
58.52% and 60.59% in MAP, 61.49% and 64.58% in MRR, and 69.88% and 100.90% in
HIT@K for the test bug reports with random and time-wise splits, respectively.
Moreover, IQLoc improves MAP by 91.67% for bug reports with stack traces,
72.73% for those that include code elements, and 65.38% for those containing
only descriptions in natural language. By integrating program semantic
understanding into Information Retrieval, IQLoc mitigates several longstanding
challenges of traditional IR-based approaches in bug localization.

</details>


### [452] [DynamiQ: Unlocking the Potential of Dynamic Task Allocation in Parallel Fuzzing](https://arxiv.org/abs/2510.04469)
*Wenqi Yan,Toby Murray,Benjamin Rubinstein,Van-Thuan Pham*

Main category: cs.SE

TL;DR: 介绍DynamiQ，支持动态自适应并行模糊测试，优化任务分配和测试，性能超现有工具，发现9个未知漏洞。


<details>
  <summary>Details</summary>
Motivation: 改进现有并行模糊测试方法，减少冗余探索，提高模糊测试效率。

Method: 利用程序调用图的结构信息定义任务，结合运行时反馈改进任务分配，基于LibAFL框架进行任务分配和任务感知模糊测试的优化。

Result: 在12个现实目标上经过25000 CPU小时评估，在代码覆盖率和漏洞发现方面优于现有并行模糊测试工具，发现9个未知漏洞。

Conclusion: DynamiQ设计有效，能显著提升模糊测试效率和效果。

Abstract: We present DynamiQ, a full-fledged and optimized successor to AFLTeam that
supports dynamic and adaptive parallel fuzzing. Unlike most existing approaches
that treat individual seeds as tasks, DynamiQ leverages structural information
from the program's call graph to define tasks and continuously refines task
allocation using runtime feedback. This design significantly reduces redundant
exploration and enhances fuzzing efficiency at scale. Built on top of the
state-of-the-art LibAFL framework, DynamiQ incorporates several practical
optimizations in both task allocation and task-aware fuzzing. Evaluated on 12
real-world targets from OSS-Fuzz and FuzzBench over 25,000 CPU hours, DynamiQ
outperforms state-of-the-art parallel fuzzers in both code coverage and
vulnerability discovery, uncovering 9 previously unknown bugs in widely used
and extensively fuzzed open-source software.

</details>


### [453] [Detecting and Characterizing Low and No Functionality Packages in the NPM Ecosystem](https://arxiv.org/abs/2510.04495)
*Napasorn Tevarut,Brittany Reid,Yutaro Kashiwa,Pattara Leelaprute,Arnon Rungsawang,Bundit Manaskasemsak,Hajimu Iida*

Main category: cs.SE

TL;DR: 本文针对npm生态系统中的琐碎和仅数据包进行研究，开发检测方法，发现此类包存在安全风险，检测工具准确率高，建议在依赖管理中给予更多关注。


<details>
  <summary>Details</summary>
Motivation: npm生态系统中琐碎包常见且可能带来安全风险，需对其进行研究和检测。

Method: 提出规则的静态分析方法，检测琐碎和仅数据包，并评估其普遍性和相关风险。

Result: 17.92%的包是琐碎包，其漏洞水平与非琐碎包相当，仅数据包虽少见但也有风险，检测工具准确率达94%（宏F1值0.87）。

Conclusion: 琐碎和仅数据包在依赖管理中应得到更多关注，以减少潜在技术债务和安全风险。

Abstract: Trivial packages, small modules with low functionality, are common in the npm
ecosystem and can pose security risks despite their simplicity. This paper
refines existing definitions and introduce data-only packages that contain no
executable logic. A rule-based static analysis method is developed to detect
trivial and data-only packages and evaluate their prevalence and associated
risks in the 2025 npm ecosystem. The analysis shows that 17.92% of packages are
trivial, with vulnerability levels comparable to non-trivial ones, and
data-only packages, though rare, also contain risks. The proposed detection
tool achieves 94% accuracy (macro-F1 0.87), enabling effective large-scale
analysis to reduce security exposure. This findings suggest that trivial and
data-only packages warrant greater attention in dependency management to reduce
potential technical debt and security exposure.

</details>


### [454] [Spec2Control: Automating PLC/DCS Control-Logic Engineering from Natural Language Requirements with LLMs - A Multi-Plant Evaluation](https://arxiv.org/abs/2510.04519)
*Heiko Koziolek,Thilo Braun,Virendra Ashiwal,Sofia Linsbauer,Marthe Ahlgreen Hansen,Karoline Grotterud*

Main category: cs.SE

TL;DR: 介绍Spec2Control，可从自然语言需求直接生成图形控制逻辑，实验效果好且能节省人力，已集成进商业工具并开源。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型生成分布式控制系统（DCS）控制逻辑的工具聚焦文本符号、自动化程度有限且未在含真实测试用例的大数据集上测试，编程过程手动且繁琐、成本高。

Method: 引入Spec2Control这一高度自动化的大语言模型工作流，从自然语言用户需求直接生成图形控制逻辑。

Result: 使用含10个控制叙述和65个复杂测试用例的开放数据集实验，Spec2Control能成功识别控制策略，自主生成98.6%的正确控制策略连接，节省94 - 96%的人力。

Conclusion: Spec2Control有效可行，已集成进ABB商业工程工具，也有开源版本供独立验证。

Abstract: Distributed control systems (DCS) manage the automation for many industrial
production processes (e.g., power plants, chemical refineries, steel mills).
Programming the software for such systems remains a largely manual and tedious
process, incurring costs of millions of dollars for extensive facilities. Large
language models (LLMs) have been found helpful in generating DCS control logic,
resulting in commercial copilot tools. Today, these tools are focused on
textual notations, they provide limited automation, and have not been tested on
large datasets with realistic test cases. We introduce Spec2Control, a highly
automated LLM workflow to generate graphical control logic directly from
natural language user requirements. Experiments using an open dataset with 10
control narratives and 65 complex test cases demonstrate that Spec2Control can
successfully identify control strategies, can generate 98.6% of correct control
strategy connections autonomously, and can save between 94-96% of human labor.
Spec2Control is being integrated into commercial ABB engineering tools, but is
also available as an open-source variant for independent validation.

</details>


### [455] [Advancing Digital Government: Integrating Open Source Software Enablement Indicators in Maturity Indexes](https://arxiv.org/abs/2510.04603)
*Johan Linåker,Sachiko Muto*

Main category: cs.SE

TL;DR: 本文分析16个数字成熟国家利用OSS进行软件复用和协作开发的政策与支持行动，提出数字政府成熟度指数潜在指标。发现促进OSS复用政策广泛，OSPO提供支持，强调OSS对公共部门数字化转型重要性。


<details>
  <summary>Details</summary>
Motivation: 目前对政府OSS采用的系统测量有限，本研究旨在通过分析相关政策和行动，为数字政府成熟度指数贡献潜在指标。

Method: 采用定性方法，结合对政策文件的案头研究和对政府代表的半结构化访谈，生成详细国家报告并进行交叉分析。

Result: 促进OSS复用政策广泛，由中央公共部门组织主导，目标多样，安全有两面性，各级OSPO提供支持，合成并提出14个领域的指标。

Conclusion: OSS是公共部门数字化转型的战略推动者，明确政策框架和OSPO等制度支持很重要，国际数字成熟度框架应增加OSS指标。

Abstract: Context: Open Source Software (OSS) is a vital public good, included across
most of modern software stacks, significantly impacting GDP and national tech
growth, while supporting interoperability, sovereignty, and transparency.
However, systematic measurement of governmental OSS adoption remain limited.
  Research Aim: This study contributes to digital government maturity indexes
by analyzing policies and support actions leveraging OSS for software reuse and
collaborative development across 16 digitally mature countries, and proposing
potential indicators for said indexes. It examines OSS policy formation, stated
goals, key actors, and support mechanisms.
  Methodology: A qualitative approach is used combining desk research of policy
documents with semi-structured interviews of government representatives,
producing detailed country reports. These are cross-analyzed, focusing on OSS
policy promotion, rationale, and implementation support.
  Results: Policies facilitating OSS reuse are widespread, targeting both
inbound acquisition and outbound sharing, and are predominantly governed by
central public sector organizations. Policy goals include interoperability,
digital sovereignty, transparency, and cost efficiency, with security framed
both as a risk and strength. Implementation is supported by diverse Open Source
Program Offices (OSPOs) at multiple government levels, which foster capacity
building, resource pooling, and sustainable project governance. Indicators are
synthesized and proposed across 14 areas covering policy incentives and design,
and implementation and support.
  Conclusions: OSS is a strategic enabler for public sector digital
transformation. Clear policy frameworks, coupled with institutional support
such as OSPOs, are essential. International digital maturity frameworks should
expand OSS indicators to better guide and assess government adoption and
impact.

</details>


### [456] [Exploring the Power of Diffusion Large Language Models for Software Engineering: An Empirical Investigation](https://arxiv.org/abs/2510.04605)
*Jingyao Zhang,Tianlin Li,Xiaoyu Zhang,Qiang Hu,Bin Shi*

Main category: cs.SE

TL;DR: 本文对Diffusion LLMs (DLLMs)在软件开发全生命周期进行评估，发现7B参数DLLMs在准确率、效率和延迟上优于自回归大语言模型(AR - LLMs)。


<details>
  <summary>Details</summary>
Motivation: AR - LLMs在处理代码结构信息和推理延迟方面存在局限，而DLLMs有全局双向编码和分离生成步骤的优势，需要对其进行评估。

Method: 在52,937个任务的大规模基准测试中对DLLMs在软件开发全生命周期（代码生成、缺陷检测和程序修复）进行评估。

Result: 7B参数DLLMs平均准确率比AR - LLMs提高30%，跨文件修复增益达113%，且效率更高、延迟更低。

Conclusion: DLLMs是软件工程任务的更优范式。

Abstract: Autoregressive Large Language Models (AR-LLMs) are widely used in software
engineering (SE) but face limitations in processing code structure information
and suffer from high inference latency. Diffusion LLMs (DLLMs) offer a
promising alternative with global bidirectional encoding and decoupled
generation steps. This work presents the first comprehensive evaluation of
DLLMs across the software development lifecycle, including code generation,
defect detection, and program repair. On a large-scale benchmark of 52,937
tasks, 7Bparameter DLLMs outperform AR-LLMs with a 30% average accuracy
improvement achieving a 113% gain on cross-file repair, while maintaining
superior efficiency and reduced latency. Our results establish DLLMs as a
superior paradigm for SE tasks.

</details>


### [457] [A survey on the impact of emotions on the productivity among software developers](https://arxiv.org/abs/2510.04611)
*Pawel Weichbroth,Maciej Lotysz,Michal Wrobel*

Main category: cs.SE

TL;DR: 研究软件开发者情绪状态与感知生产力关系，发现情绪状态对感知生产力有强正相关影响，强调管理开发者情绪对提升生产力重要性。


<details>
  <summary>Details</summary>
Motivation: 确定软件开发者情绪状态与感知生产力之间关系的强度和方向，因软件开发时间压力等常使开发者情绪不佳，且情绪是否影响感知生产力未知。

Method: 采用两阶段方法，先让9位专家验证测量模型，再对88位开发者进行调查，用偏最小二乘法进行数据分析。

Result: 路径分析结果证实假设，开发者情绪状态对感知生产力有强正且显著影响（beta = 0.893, p < 0.001）。

Conclusion: 强调管理和改善开发者情绪健康对提高软件开发环境生产力很重要，减少倦怠、压力等负面因素的干预措施会对绩效产生显著影响。

Abstract: The time pressure associated with software development, among other factors,
often leads to a diminished emotional state among developers. However, whether
emotions affect perceived productivity remains an open question. This study
aims to determine the strength and direction of the relationship between
emotional state and perceived productivity among software developers. We
employed a two-stage approach. First, a survey was conducted with a pool of
nine experts to validate the measurement model. Second, a survey was
administered to a pool of 88 software developers to empirically test the
formulated hypothesis by using Partial Least Squares, as the data analysis
method. The results of the path analysis clearly confirm the formulated
hypothesis, showing that the emotional state of a software developer has a
strong positive, and significant impact (beta = 0.893, p < 0.001) on perceived
productivity among software developers. The findings highlight the importance
of managing and improving developers emotional well-being to enhance
productivity in software development environments. Additionally, interventions
aimed at reducing burnout, stress, and other negative factors could have a
considerable impact on their performance outcomes.

</details>


### [458] [Evolaris: A Roadmap to Self-Evolving Software Intelligence Management](https://arxiv.org/abs/2510.04689)
*Chengwei Liu,Wenbo Guo,Yuxin Zhang,Limin Wang,Sen Chen,Lei Bu,Yang Liu*

Main category: cs.SE

TL;DR: 针对软件威胁信息获取难题，提出自进化软件智能系统Evolaris，可提升威胁分析能力。


<details>
  <summary>Details</summary>
Motivation: 软件威胁态势动态分布式发展，威胁信息多从非正式渠道出现，及时捕获情报有挑战。

Method: 提出基于多智能体框架的自进化软件智能系统Evolaris，支持全栈工作流。

Result: 系统能从新输入学习，提炼内部知识，适应新威胁模式。

Conclusion: 可提升软件威胁分析的精准性、及时性和可扩展性，为主动安全决策提供基础。

Abstract: In recent years, the landscape of software threats has become significantly
more dynamic and distributed. Security vulnerabilities are no longer discovered
and shared only through formal channels such as public vulnerability databases
or vendor advisories. Increasingly, criti- cal threat information emerges
informally through blogs, social media, developer forums, open source
repositories, and even underground com- munities. To this end, capturing such
intelligence in a timely manner is essential for maintaining situational
awareness and enabling prompt security responses. However, this remains a
complex challenge due to the fragmented nature of data sources and the
technical difficulty of collecting, parsing, mapping, and validating
information at scale. To ad- dress this, we propose Evolaris, a self-evolving
software intelligence sys- tem built on a multi-agent framework. Evolaris is
designed to support a full-stack workflow, where agents operate independently
but coordinate through shared context to perform tasks such as information
discovery, reasoning, gap completion, validation, and risk detection. This
archi- tecture enables the platform to learn from new inputs, refine its
internal knowledge, and adapt to emerging threat patterns over time, which
could continuously improve the precision, timeliness, and scalability of
software threat analysis, and offers a sustainable foundation for proactive
secu- rity decision-making and strengthens the broader ecosystem of security
threat understanding.

</details>


### [459] [An Empirical Study of SOTA RCA Models: From Oversimplified Benchmarks to Realistic Failures](https://arxiv.org/abs/2510.04711)
*Aoyang Fang,Songhan Zhang,Yifan Yang,Haotong Wu,Junjielong Xu,Xuyang Wang,Rui Wang,Manyi Wang,Qisheng Lu,Pinjia He*

Main category: cs.SE

TL;DR: 云原生微服务架构RCA现有基准过于简化，本文分析其局限性并开发框架生成更真实基准，重评估SOTA模型表现不佳并指出常见失败模式。


<details>
  <summary>Details</summary>
Motivation: 云原生微服务架构RCA重要但具挑战，现有基准过于简化，导致模型性能高估。

Method: 系统分析流行RCA基准，找出关键局限，开发自动化框架生成更真实基准。

Result: 生成含1430个有效故障案例的数据集，重评估11个SOTA模型，Top@1准确率低且执行时间长。

Conclusion: 分析指出SOTA模型存在可扩展性问题、可观测性盲点和建模瓶颈三种常见失败模式。

Abstract: While cloud-native microservice architectures have transformed software
development, their complexity makes Root Cause Analysis (RCA) both crucial and
challenging. Although many data-driven RCA models have been proposed, we find
that existing benchmarks are often oversimplified and fail to capture
real-world conditions. Our preliminary study shows that simple rule-based
methods can match or even outperform state-of-the-art (SOTA) models on four
widely used benchmarks, suggesting performance overestimation due to benchmark
simplicity. To address this, we systematically analyze popular RCA benchmarks
and identify key limitations in fault injection, call graph design, and
telemetry patterns. Based on these insights, we develop an automated framework
to generate more realistic benchmarks, yielding a dataset of 1,430 validated
failure cases from 9,152 injections, covering 25 fault types under dynamic
workloads with hierarchical ground-truth labels and verified SLI impact.
Re-evaluation of 11 SOTA models on this dataset shows low Top@1 accuracy
(average 0.21, best 0.37) and significantly longer execution times. Our
analysis highlights three common failure patterns: scalability issues,
observability blind spots, and modeling bottlenecks.

</details>


### [460] [Agile Software Effort Estimation using Regression Techniques](https://arxiv.org/abs/2510.04760)
*Sisay Deresa Sima,Ayalew Belay Habtie*

Main category: cs.SE

TL;DR: 本文用LASSO和Elastic Net回归技术开发基于故事点的敏捷工作量估算模型，实验显示LASSO回归表现更好。


<details>
  <summary>Details</summary>
Motivation: 软件开发工作量估算很关键，研究者持续开展敏捷工作量估算研究，本文旨在开发基于故事点的敏捷工作量估算模型。

Method: 使用从六家公司收集的21个软件项目应用于敏捷故事点方法，用默认参数训练两个算法并通过5折交叉验证的网格搜索调优。

Result: LASSO回归取得更好的预测性能，如PRED (8%)和PRED (25%)结果为100.0等，并与其他相关文献对比。

Conclusion: LASSO回归在基于故事点的敏捷工作量估算中表现更优。

Abstract: Software development effort estimation is one of the most critical aspect in
software development process, as the success or failure of the entire project
depends on the accuracy of estimations. Researchers are still conducting
studies on agile effort estimation. The aim of this research is to develop a
story point based agile effort estimation model using LASSO and Elastic Net
regression techniques. The experimental work is applied to the agile story
point approach using 21 software projects collected from six firms. The two
algorithms are trained using their default parameters and tuned grid search
with 5-fold cross-validation to get an enhanced model. The experiment result
shows LASSO regression achieved better predictive performance PRED (8%) and
PRED (25%) results of 100.0, MMRE of 0.0491, MMER of 0.0551, MdMRE of 0.0593,
MdMER of 0.063, and MSE of 0.0007. The results are also compared with other
related literature.

</details>


### [461] [GUISpector: An MLLM Agent Framework for Automated Verification of Natural Language Requirements in GUI Prototypes](https://arxiv.org/abs/2510.04791)
*Kristian Kolthoff,Felix Kretzer,Simone Paolo Ponzetto,Alexander Maedche,Christian Bartelt*

Main category: cs.SE

TL;DR: 本文介绍了用于GUI原型中自然语言需求自动验证的框架GUISpector，评估显示其能有效检测需求满足和违规情况。


<details>
  <summary>Details</summary>
Motivation: 现有GUI测试方法难以处理现代界面复杂性，且缺乏可操作反馈和与自动化开发代理的有效集成，需新方法确保GUI实现满足自然语言需求。

Method: 引入基于多模态大语言模型的代理，使其解释和操作自然语言需求，规划和执行验证轨迹；从验证过程提取详细反馈；提供集成工具管理验证流程。

Result: 在150个需求、900个验收标准注释上评估，能有效检测需求满足和违规情况。

Conclusion: GUISpector有潜力将可操作反馈无缝集成到自动化大语言模型驱动的开发工作流中。

Abstract: GUIs are foundational to interactive systems and play a pivotal role in early
requirements elicitation through prototyping. Ensuring that GUI implementations
fulfill NL requirements is essential for robust software engineering,
especially as LLM-driven programming agents become increasingly integrated into
development workflows. Existing GUI testing approaches, whether traditional or
LLM-driven, often fall short in handling the complexity of modern interfaces,
and typically lack actionable feedback and effective integration with automated
development agents. In this paper, we introduce GUISpector, a novel framework
that leverages a multi-modal (M)LLM-based agent for the automated verification
of NL requirements in GUI prototypes. First, GUISpector adapts a MLLM agent to
interpret and operationalize NL requirements, enabling to autonomously plan and
execute verification trajectories across GUI applications. Second, GUISpector
systematically extracts detailed NL feedback from the agent's verification
process, providing developers with actionable insights that can be used to
iteratively refine the GUI artifact or directly inform LLM-based code
generation in a closed feedback loop. Third, we present an integrated tool that
unifies these capabilities, offering practitioners an accessible interface for
supervising verification runs, inspecting agent rationales and managing the
end-to-end requirements verification process. We evaluated GUISpector on a
comprehensive set of 150 requirements based on 900 acceptance criteria
annotations across diverse GUI applications, demonstrating effective detection
of requirement satisfaction and violations and highlighting its potential for
seamless integration of actionable feedback into automated LLM-driven
development workflows. The video presentation of GUISpector is available at:
https://youtu.be/JByYF6BNQeE, showcasing its main capabilities.

</details>


### [462] [RevMine: An LLM-Assisted Tool for Code Review Mining and Analysis Across Git Platforms](https://arxiv.org/abs/2510.04796)
*Samah Kansab,Francis Bordeleau,Ali Tizghadam*

Main category: cs.SE

TL;DR: 本文介绍概念工具RevMine，它用大语言模型简化代码审查挖掘流程，降低进入门槛，促进实证软件工程研究。


<details>
  <summary>Details</summary>
Motivation: 代码审查过程实证研究重要，但收集和分析审查数据耗时且技术要求高，当前研究工作流类似且需手动编写脚本。

Method: 引入RevMine工具，利用大语言模型，引导用户完成认证、端点发现和自然语言驱动的数据收集，支持定量和定性分析。

Result: 介绍了工具的架构、用例和研究潜力。

Conclusion: RevMine降低了代码审查挖掘的门槛，有助于实现代码审查挖掘的民主化，推动更广泛的实证软件工程研究。

Abstract: Empirical research on code review processes is increasingly central to
understanding software quality and collaboration. However, collecting and
analyzing review data remains a time-consuming and technically intensive task.
Most researchers follow similar workflows - writing ad hoc scripts to extract,
filter, and analyze review data from platforms like GitHub and GitLab. This
paper introduces RevMine, a conceptual tool that streamlines the entire code
review mining pipeline using large language models (LLMs). RevMine guides users
through authentication, endpoint discovery, and natural language-driven data
collection, significantly reducing the need for manual scripting. After
retrieving review data, it supports both quantitative and qualitative analysis
based on user-defined filters or LLM-inferred patterns. This poster outlines
the tool's architecture, use cases, and research potential. By lowering the
barrier to entry, RevMine aims to democratize code review mining and enable a
broader range of empirical software engineering studies.

</details>


### [463] [InsightQL: Advancing Human-Assisted Fuzzing with a Unified Code Database and Parameterized Query Interface](https://arxiv.org/abs/2510.04835)
*Wentao Gao,Renata Borovica-Gajic,Sang Kil Cha,Tian Qiu,Van-Thuan Pham*

Main category: cs.SE

TL;DR: 介绍首个用于模糊测试阻塞分析的人类辅助框架InsightQL，实验证明其能有效解决模糊测试阻塞问题并提高代码覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有模糊测试技术存在模糊测试阻塞导致覆盖率停滞问题，人工分析模糊测试结果引导支持又很费力，因此需要解决该问题。

Method: 引入由统一数据库和直观参数化查询接口驱动的InsightQL框架，辅助开发者系统提取信息并解决模糊测试阻塞。

Result: 在FuzzBench基准的14个流行真实库上实验，InsightQL解决了许多模糊测试阻塞问题，代码覆盖率最高提升13.90%。

Conclusion: InsightQL框架在解决模糊测试阻塞和提高代码覆盖率方面有效。

Abstract: Fuzzing is a highly effective automated testing method for uncovering
software vulnerabilities. Despite advances in fuzzing techniques, such as
coverage-guided greybox fuzzing, many fuzzers struggle with coverage plateaus
caused by fuzz blockers, limiting their ability to find deeper vulnerabilities.
Human expertise can address these challenges, but analyzing fuzzing results to
guide this support remains labor-intensive. To tackle this, we introduce
InsightQL, the first human-assisting framework for fuzz blocker analysis.
Powered by a unified database and an intuitive parameterized query interface,
InsightQL aids developers in systematically extracting insights and efficiently
unblocking fuzz blockers. Our experiments on 14 popular real-world libraries
from the FuzzBench benchmark demonstrate the effectiveness of InsightQL,
leading to the unblocking of many fuzz blockers and considerable improvements
in code coverage (up to 13.90%).

</details>


### [464] [FreshBrew: A Benchmark for Evaluating AI Agents on Java Code Migration](https://arxiv.org/abs/2510.04852)
*Victor May,Diganta Misra,Yanqi Luo,Anjali Sridhar,Justine Gehring,Silvio Soares Ribeiro Junior*

Main category: cs.SE

TL;DR: 本文提出评估AI代理项目级Java迁移能力的FreshBrew基准，对多个模型进行测试并分析其优缺点，旨在推动AI驱动代码库现代化发展。


<details>
  <summary>Details</summary>
Motivation: 随着AI编码助手发展，需评估AI驱动的代理框架在代码库迁移和现代化方面的有效性，传统方法依赖规则系统和人工干预。

Method: 引入FreshBrew基准评估AI代理在项目级Java迁移中的表现，测量其保留程序语义和避免奖励破解的能力，并与规则工具对比。

Result: 在228个仓库的基准测试中，表现最佳的Gemini 2.5 Flash能成功将52.3%的项目迁移到JDK 17。

Conclusion: 实证分析揭示了当前代理方法的优缺点和失败模式，发布FreshBrew可促进严格、可重复的评估，推动AI驱动代码库现代化的发展。

Abstract: AI coding assistants are rapidly becoming integral to modern software
development. A key challenge in this space is the continual need to migrate and
modernize codebases in response to evolving software ecosystems. Traditionally,
such migrations have relied on rule-based systems and human intervention. With
the advent of powerful large language models (LLMs), AI-driven agentic
frameworks offer a promising alternative-but their effectiveness has not been
systematically evaluated. In this paper, we introduce FreshBrew, a novel
benchmark for evaluating AI agents on project-level Java migrations, with a
specific focus on measuring an agent's ability to preserve program semantics
and avoid reward hacking, which we argue requires projects with high test
coverage for a rigorous and reliable evaluation. We benchmark several
state-of-the-art LLMs, and compare their performance against established
rule-based tools. Our evaluation of AI agents on this benchmark of 228
repositories shows that the top-performing model, Gemini 2.5 Flash, can
successfully migrate 52.3 percent of projects to JDK 17. Our empirical analysis
reveals novel insights into the critical strengths and limitations of current
agentic approaches, offering actionable insights into their real-world
applicability. Our empirical study reveals failure modes of current AI agents
in realistic Java modernization tasks, providing a foundation for evaluating
trustworthy code-migration systems. By releasing FreshBrew, we aim to
facilitate rigorous, reproducible evaluation and catalyze progress in AI-driven
codebase modernization.

</details>


### [465] [Retrieval-Augmented Code Generation: A Survey with Focus on Repository-Level Approaches](https://arxiv.org/abs/2510.04905)
*Yicheng Tao,Yao Qin,Yepang Liu*

Main category: cs.SE

TL;DR: 对检索增强代码生成（RACG）研究进行全面综述，着重于仓库级方法，分类现有工作，总结数据集和基准，分析局限并指出未来方向。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在函数和文件级代码生成有成果，但现实软件开发需跨整个仓库推理，仓库级代码生成（RLCG）有挑战，检索增强生成（RAG）可解决部分问题，因此对RACG进行全面综述。

Method: 对现有研究按生成策略、检索方式、模型架构、训练范式和评估协议等维度进行分类。

Result: 总结了广泛使用的数据集和基准，分析了当前的局限性。

Conclusion: 建立统一分析框架以理解该快速发展领域，激励人工智能驱动软件工程持续进步。

Abstract: Recent advancements in large language models (LLMs) have substantially
improved automated code generation. While function-level and file-level
generation have achieved promising results, real-world software development
typically requires reasoning across entire repositories. This gives rise to the
challenging task of Repository-Level Code Generation (RLCG), where models must
capture long-range dependencies, ensure global semantic consistency, and
generate coherent code spanning multiple files or modules. To address these
challenges, Retrieval-Augmented Generation (RAG) has emerged as a powerful
paradigm that integrates external retrieval mechanisms with LLMs, enhancing
context-awareness and scalability. In this survey, we provide a comprehensive
review of research on Retrieval-Augmented Code Generation (RACG), with an
emphasis on repository-level approaches. We categorize existing work along
several dimensions, including generation strategies, retrieval modalities,
model architectures, training paradigms, and evaluation protocols. Furthermore,
we summarize widely used datasets and benchmarks, analyze current limitations,
and outline key challenges and opportunities for future research. Our goal is
to establish a unified analytical framework for understanding this rapidly
evolving field and to inspire continued progress in AI-powered software
engineering.

</details>


### [466] [Why Software Signing (Still) Matters: Trust Boundaries in the Software Supply Chain](https://arxiv.org/abs/2510.04964)
*Kelechi G. Kalu,James C. Davis*

Main category: cs.SE

TL;DR: 本文探讨软件签名必要性，指出注册中心安全控制不能替代签名，签名可加强软件供应链保障。


<details>
  <summary>Details</summary>
Motivation: 在集中式注册中心时代，探讨强化注册中心安全控制是否可取代端到端工件签名。

Method: 综合历史实践，提出适用于现代分发模式的信任模型。

Result: 发现签名的核心保证（来源、完整性和问责制）不会自动跨越不同软件分发边界，注册中心安全控制无法提供足够保证。

Conclusion: 将签名作为基础防御层，即使注册中心安全，也能加强软件供应链保障。

Abstract: Software signing provides a formal mechanism for provenance by ensuring
artifact integrity and verifying producer identity. It also imposes tooling and
operational costs to implement in practice. In an era of centralized registries
such as PyPI, npm, Maven Central, and Hugging Face, it is reasonable to ask
whether hardening registry security controls obviates the need for end-to-end
artifact signing. In this work, we posit that the core guarantees of signing,
provenance, integrity, and accountability are not automatically carried across
different software distribution boundaries. These boundaries include mirrors,
corporate proxies, re-hosting, and air-gapped transfers, where registry
security controls alone cannot provide sufficient assurance. We synthesize
historical practice and present a trust model for modern distribution modes to
identify when signing is necessary to extend trust beyond registry control.
Treating signing as a baseline layer of defense strengthens software supply
chain assurance even when registries are secure.

</details>


### [467] [Quantum Computing as a Service - a Software Engineering Perspective](https://arxiv.org/abs/2510.04982)
*Aakash Ahmad,Muhammad Waseem,Bakheet Aljedaani,Mahdi Fahmideh,Peng Liang,Feras Awaysheh*

Main category: cs.SE

TL;DR: 本文探讨以过程为中心和架构驱动的方法为量子计算即服务（QCaaS）提供软件工程视角，通过两阶段研究方法确定量子服务开发生命周期阶段并集成到支持QCaaS的参考架构中。


<details>
  <summary>Details</summary>
Motivation: 量子计算成为有潜力技术，QCaaS可向无量子计算机的个人和组织提供资源与平台，本文旨在从软件工程角度支持QCaaS。

Method: 采用两阶段研究方法，包括系统映射研究和基于架构的开发，通过系统映射研究筛选41篇同行评审研究回答三个研究问题。

Result: 确定了4阶段开发生命周期，以及量子重要需求、建模符号、模式目录、编程语言和部署平台等可集成到分层参考架构中以实现QCaaS。

Conclusion: 通过研究提出的方法和架构可从软件工程角度助力QCaaS的实现。

Abstract: Quantum systems have started to emerge as a disruptive technology and
enabling platforms - exploiting the principles of quantum mechanics via
programmable quantum bits (QuBits) - to achieve quantum supremacy in computing.
Academic research, industrial projects (e.g., Amazon Braket, IBM Qiskit), and
consortiums like 'Quantum Flagship' are striving to develop practically capable
and commercially viable quantum computing (QC) systems and technologies.
Quantum Computing as a Service (QCaaS) is viewed as a solution attuned to the
philosophy of service-orientation that can offer QC resources and platforms, as
utility computing, to individuals and organisations who do not own quantum
computers. This research investigates a process-centric and architecture-driven
approach to offer a software engineering perspective on enabling QCaaS - a.k.a
quantum service-orientation. We employed a two-phase research method comprising
(a) a systematic mapping study and (b) an architecture-based development, first
to identify the phases of the quantum service development life cycle and
subsequently to integrate these phases into a reference architecture that
supports QCaaS. The SMS process retrieved a collection of potentially relevant
research literature and based on a multi-step selection and qualitative
assessment, we selected 41 peer-reviewed studies to answer three RQs. The RQs
investigate (i) demographic details in terms of frequency, types, and trends of
research, (ii) phases of quantum service development lifecycle to derive a
reference architecture for conception, modeling, assembly, and deployment of
services, and (iii) The results identify a 4-phased development lifecycle along
with quantum significant requirements (QSRs), various modeling notations,
catalogue of patterns, programming languages, and deployment platforms that can
be integrated in a layered reference architecture to engineer QCaaS.

</details>


### [468] [AutoEmpirical: LLM-Based Automated Research for Empirical Software Fault Analysis](https://arxiv.org/abs/2510.04997)
*Jiongchi Yu,Weipeng Jiang,Xiaoyu Zhang,Qiang Hu,Xiaofei Xie,Chao Shen*

Main category: cs.SE

TL;DR: 本文分解软件故障实证研究过程，用大语言模型对开源软件故障分析评估，显示其可提升效率并给出研究计划。


<details>
  <summary>Details</summary>
Motivation: 传统故障分析劳动密集、耗时，阻碍大规模故障研究和迭代实证研究。

Method: 将软件故障实证研究分为三个关键阶段，用大语言模型对3829个软件故障进行评估。

Result: 大语言模型能大幅提高故障分析效率，平均处理时间约两小时，而人工需数周。

Conclusion: 指出大语言模型推进实证故障研究的潜力和实现端到端自动化软件故障分析需解决的挑战，并给出详细研究计划。

Abstract: Understanding software faults is essential for empirical research in software
development and maintenance. However, traditional fault analysis, while
valuable, typically involves multiple expert-driven steps such as collecting
potential faults, filtering, and manual investigation. These processes are both
labor-intensive and time-consuming, creating bottlenecks that hinder
large-scale fault studies in complex yet critical software systems and slow the
pace of iterative empirical research.
  In this paper, we decompose the process of empirical software fault study
into three key phases: (1) research objective definition, (2) data preparation,
and (3) fault analysis, and we conduct an initial exploration study of applying
Large Language Models (LLMs) for fault analysis of open-source software.
Specifically, we perform the evaluation on 3,829 software faults drawn from a
high-quality empirical study. Our results show that LLMs can substantially
improve efficiency in fault analysis, with an average processing time of about
two hours, compared to the weeks of manual effort typically required. We
conclude by outlining a detailed research plan that highlights both the
potential of LLMs for advancing empirical fault studies and the open challenges
that required be addressed to achieve fully automated, end-to-end software
fault analysis.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [469] [Improving S&P 500 Volatility Forecasting through Regime-Switching Methods](https://arxiv.org/abs/2510.03236)
*Ava C. Blake,Nivika A. Gandhi,Anurag R. Jakkula*

Main category: q-fin.ST

TL;DR: 本文提出多种制度转换方法预测标普500波动率，评估各模型在不同时期表现，系数聚类算法表现最优，证明制度感知建模和软聚类方法在波动预测中的价值。


<details>
  <summary>Details</summary>
Motivation: 准确预测金融市场波动率对风险管理、衍生品定价和投资策略至关重要，需改进标普500波动率预测。

Method: 使用11年SPX数据计算日实际波动率，设计特征，采用软马尔可夫转换算法、分布谱聚类法和基于系数的软制度算法，分三个时期评估模型。

Result: 基于系数的聚类算法在各时期均优于其他模型，还评估了各模型在不同时期5天和10天的递归预测性能。

Conclusion: 制度感知建模框架和软聚类方法能提高波动率预测，在不确定性和结构变化时期尤其有效。

Abstract: Accurate prediction of financial market volatility is critical for risk
management, derivatives pricing, and investment strategy. In this study, we
propose a multitude of regime-switching methods to improve the prediction of
S&P 500 volatility by capturing structural changes in the market across time.
We use eleven years of SPX data, from May 1st, 2014 to May 27th, 2025, to
compute daily realized volatility (RV) from 5-minute intraday log returns,
adjusted for irregular trading days. To enhance forecast accuracy, we
engineered features to capture both historical dynamics and forward-looking
market sentiment across regimes. The regime-switching methods include a soft
Markov switching algorithm to estimate soft-regime probabilities, a
distributional spectral clustering method that uses XGBoost to assign clusters
at prediction time, and a coefficient-based soft regime algorithm that extracts
HAR coefficients from time segments segmented through the Mood test and
clusters through Bayesian GMM for soft regime weights, using XGBoost to predict
regime probabilities. Models were evaluated across three time periods--before,
during, and after the COVID-19 pandemic. The coefficient-based clustering
algorithm outperformed all other models, including the baseline autoregressive
model, during all time periods. Additionally, each model was evaluated on its
recursive forecasting performance for 5- and 10-day horizons during each time
period. The findings of this study demonstrate the value of regime-aware
modeling frameworks and soft clustering approaches in improving volatility
forecasting, especially during periods of heightened uncertainty and structural
change.

</details>


### [470] [Panel regression for the GDP of the Central and Eastern European countries using time-varying coefficients](https://arxiv.org/abs/2510.04211)
*Lesya Kolinets,Vygintas Gontis*

Main category: q-fin.ST

TL;DR: 本文以中东欧国家融入欧洲经济区为实验，用面板数据回归模型分析宏观经济因素对GDP增长的贡献，发现私人债务对经济增长速度至关重要。


<details>
  <summary>Details</summary>
Motivation: 中东欧国家融入欧洲经济区为区域经济发展理论提供了有价值的实验，研究其经济收敛及不同发展轨迹背后宏观经济因素对GDP增长的贡献。

Method: 采用允许系数随时间变化的面板数据回归模型，将人均GDP年度变化与GDP、价格、贸易、投资和债务水平等因素进行回归分析。

Result: 所选主要因素对经济增长有贡献，与以往研究一致，且私人债务在决定经济增长速度方面起着至关重要的作用。

Conclusion: 私人债务对中东欧国家经济增长速度的影响十分关键。

Abstract: The integration of Central and Eastern European (CEE) countries into the
European Economic Area serves as a valuable experiment for the regional
economic development theory. The long-lasting convergence of these economies
with more advanced Western Europe exhibits a few standard features and varying
policies implemented. Even the Baltic countries, which started from very
similar starting positions, demonstrate their unique trajectories of
development. We employ a panel data regression model that allows coefficients
to vary over time to compare the contributions of a few macroeconomic factors
to the GDP growth of CEE countries. In particular, we regress the annual change
of GDP per capita in PPP terms as a function of achieved GDP, price, trade,
investment, and debt levels. Time-varying common slope coefficients in this
approach describe the external economic environment in which countries
implement their own policies. The panel consists of 11 Central and Eastern
European countries (Bulgaria, Czechia, Estonia, Croatia, Latvia, Lithuania,
Hungary, Poland, Romania, Slovenia, and Slovakia), which have been observed
annually from 1995 to 2024. While the main selected factors of this
investigation contribute to economic growth, in agreement with previous
findings, the role of private debt appears vital in determining the pace of
economic growth.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [471] [Risk-Sensitive Option Market Making with Arbitrage-Free eSSVI Surfaces: A Constrained RL and Stochastic Control Bridge](https://arxiv.org/abs/2510.04569)
*Jian'an Zhang*

Main category: q-fin.TR

TL;DR: 本文将期权做市问题转化为有约束的风险敏感控制问题，通过可微的eSSVI层和策略控制实现定价一致性和执行控制，模拟结果良好。


<details>
  <summary>Details</summary>
Motivation: 将执行、对冲和无套利隐含波动率曲面统一在单个学习循环中，解决期权做市问题。

Method: 将期权做市问题公式化为约束风险敏感控制问题，使用可微eSSVI层，通过策略控制相关参数，用可微CVaR目标控制尾部风险，并提供相关理论。

Result: 在模拟中，做市商在多数日内时段实现正调整损益，日历套利和蝶式套利违规接近零，尾部风险可通过CVaR权重调整。

Conclusion: 该方法实现了定价一致性和执行控制的统一，是可复现的白盒学习方法。

Abstract: We formulate option market making as a constrained, risk-sensitive control
problem that unifies execution, hedging, and arbitrage-free implied-volatility
surfaces inside a single learning loop. A fully differentiable eSSVI layer
enforces static no-arbitrage conditions (butterfly and calendar) while the
policy controls half-spreads, hedge intensity, and structured surface
deformations (state-dependent rho-shift and psi-scale). Executions are
intensity-driven and respond monotonically to spreads and relative mispricing;
tail risk is shaped with a differentiable CVaR objective via the
Rockafellar--Uryasev program. We provide theory for (i) grid-consistency and
rates for butterfly/calendar surrogates, (ii) a primal--dual grounding of a
learnable dual action acting as a state-dependent Lagrange multiplier, (iii)
differentiable CVaR estimators with mixed pathwise and likelihood-ratio
gradients and epi-convergence to the nonsmooth objective, (iv) an eSSVI
wing-growth bound aligned with Lee's moment constraints, and (v)
policy-gradient validity under smooth surrogates. In simulation (Heston
fallback; ABIDES-ready), the agent attains positive adjusted P\&L on most
intraday segments while keeping calendar violations at numerical zero and
butterfly violations at the numerical floor; ex-post tails remain realistic and
can be tuned through the CVaR weight. The five control heads admit clear
economic semantics and analytic sensitivities, yielding a white-box learner
that unifies pricing consistency and execution control in a reproducible
pipeline.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [472] [Quantile-Scaled Bayesian Optimization Using Rank-Only Feedback](https://arxiv.org/abs/2510.03277)
*Tunde Fahd Egunjobi*

Main category: stat.ML

TL;DR: 提出QS - BO优化框架，在合成基准函数上评估，表现优于随机搜索，适用于仅排名反馈场景。


<details>
  <summary>Details</summary>
Motivation: 标准贝叶斯优化假设能获取精确目标值，而现实中可能只有相对或基于排名的反馈，因此需要新的优化框架。

Method: 提出QS - BO框架，通过分位数缩放管道将排名转换为异方差高斯目标，使用高斯过程替代和标准获取函数。

Result: 在合成基准函数上，QS - BO持续取得更低目标值，稳定性更好，统计测试表明其在1%显著性水平上显著优于随机搜索。

Conclusion: QS - BO是贝叶斯优化在仅排名反馈场景下实用有效的扩展，在偏好学习等领域有应用前景。

Abstract: Bayesian Optimization (BO) is widely used for optimizing expensive black-box
functions, particularly in hyperparameter tuning. However, standard BO assumes
access to precise objective values, which may be unavailable, noisy, or
unreliable in real-world settings where only relative or rank-based feedback
can be obtained. In this study, we propose Quantile-Scaled Bayesian
Optimization (QS-BO), a principled rank-based optimization framework. QS-BO
converts ranks into heteroscedastic Gaussian targets through a quantile-scaling
pipeline, enabling the use of Gaussian process surrogates and standard
acquisition functions without requiring explicit metric scores. We evaluate
QS-BO on synthetic benchmark functions, including one- and two-dimensional
nonlinear functions and the Branin function, and compare its performance
against Random Search. Results demonstrate that QS-BO consistently achieves
lower objective values and exhibits greater stability across runs. Statistical
tests further confirm that QS-BO significantly outperforms Random Search at the
1\% significance level. These findings establish QS-BO as a practical and
effective extension of Bayesian Optimization for rank-only feedback, with
promising applications in preference learning, recommendation, and
human-in-the-loop optimization where absolute metric values are unavailable or
unreliable.

</details>


### [473] [Mathematically rigorous proofs for Shapley explanations](https://arxiv.org/abs/2510.03281)
*David van Batenburg*

Main category: stat.ML

TL;DR: 论文从数学严谨角度探讨Lundberg和Lee的两个主要结果并给出完整证明，包括基于Young公理对机器学习中Shapley值的公理化刻画及证明其是加权线性回归问题的唯一解。


<details>
  <summary>Details</summary>
Motivation: 机器学习日益重要，需理解其模型决策过程，Shapley值是常用方法，原资料缺少相关严格证明。

Method: 从数学严谨角度进行分析，用反例证明对称性公理的必要性，使用降维方法证明Shapley值是加权线性回归问题的唯一解。

Result: 一是给出机器学习中Shapley值基于Young公理的公理化刻画，指出对称性公理必不可少；二是证明Shapley值是加权线性回归问题的唯一解。

Conclusion: 对Lundberg和Lee的结果给出严格数学证明，明确了Shapley值的相关性质。

Abstract: Machine Learning is becoming increasingly more important in today's world. It
is therefore very important to provide understanding of the decision-making
process of machine-learning models. A popular way to do this is by looking at
the Shapley-Values of these models as introduced by Lundberg and Lee.
  In this thesis, we discuss the two main results by Lundberg and Lee from a
mathematically rigorous standpoint and provide full proofs, which are not
available from the original material.
  The first result of this thesis is an axiomatic characterization of the
Shapley values in machine learning based on axioms by Young. We show that the
Shapley values are the unique explanation to satisfy local accuracy,
missingness, symmetry and consistency. Lundberg and Lee claim that the symmetry
axiom is redundant for explanations. However, we provide a counterexample that
shows the symmetry axiom is in fact essential.
  The second result shows that we can write the Shapley values as the unique
solution to a weighted linear regression problem. This result is proven with
the use of dimensionality reduction.

</details>


### [474] [Gini-based Model Monitoring: A General Framework with an Application to Non-life Insurance Pricing](https://arxiv.org/abs/2510.04556)
*Alexej Brauer,Paul Menzel*

Main category: stat.ML

TL;DR: 该研究首次系统研究非寿险定价中的概念漂移，给出相关文献和方法概述，规范性能指标，推导基尼指数渐近分布，提出监控程序并结合实例说明。


<details>
  <summary>Details</summary>
Motivation: 在投资组合和环境不断变化的动态环境中，保持定价模型的准确性至关重要，此前缺乏对非寿险定价中概念漂移的系统研究。

Method: 提供相关文献和常用方法的概述，规范常见性能指标，推导基尼指数的渐近分布，提出标准化的监控程序。

Result: 成功推导基尼指数的渐近分布，提出指示何时重新拟合的标准化监控程序，并通过修改后的真实投资组合进行框架说明。

Conclusion: 该研究为非寿险定价中的概念漂移问题提供了系统的解决方案和实用的监控程序，有助于提高定价模型的准确性。

Abstract: In a dynamic landscape where portfolios and environments evolve, maintaining
the accuracy of pricing models is critical. To the best of our knowledge, this
is the first study to systematically examine concept drift in non-life
insurance pricing. We (i) provide an overview of the relevant literature and
commonly used methodologies, clarify the distinction between virtual drift and
concept drift, and explain their implications for long-run model performance;
(ii) review and formalize common performance measures, including the Gini index
and deviance loss, and articulate their interpretation; (iii) derive the
asymptotic distribution of the Gini index, enabling valid inference and
hypothesis testing; and (iv) present a standardized monitoring procedure that
indicates when refitting is warranted. We illustrate the framework using a
modified real-world portfolio with induced concept drift and discuss practical
considerations and pitfalls.

</details>


### [475] [Transformed $\ell_1$ Regularizations for Robust Principal Component Analysis: Toward a Fine-Grained Understanding](https://arxiv.org/abs/2510.03624)
*Kun Zhao,Haoke Zhang,Jiayi Wang,Yifei Lou*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Robust Principal Component Analysis (RPCA) aims to recover a low-rank
structure from noisy, partially observed data that is also corrupted by sparse,
potentially large-magnitude outliers. Traditional RPCA models rely on convex
relaxations, such as nuclear norm and $\ell_1$ norm, to approximate the rank of
a matrix and the $\ell_0$ functional (the number of non-zero elements) of
another. In this work, we advocate a nonconvex regularization method, referred
to as transformed $\ell_1$ (TL1), to improve both approximations. The rationale
is that by varying the internal parameter of TL1, its behavior asymptotically
approaches either $\ell_0$ or $\ell_1$. Since the rank is equal to the number
of non-zero singular values and the nuclear norm is defined as their sum,
applying TL1 to the singular values can approximate either the rank or the
nuclear norm, depending on its internal parameter. We conduct a fine-grained
theoretical analysis of statistical convergence rates, measured in the
Frobenius norm, for both the low-rank and sparse components under general
sampling schemes. These rates are comparable to those of the classical RPCA
model based on the nuclear norm and $\ell_1$ norm. Moreover, we establish
constant-order upper bounds on the estimated rank of the low-rank component and
the cardinality of the sparse component in the regime where TL1 behaves like
$\ell_0$, assuming that the respective matrices are exactly low-rank and
exactly sparse. Extensive numerical experiments on synthetic data and
real-world applications demonstrate that the proposed approach achieves higher
accuracy than the classic convex model, especially under non-uniform sampling
schemes.

</details>


### [476] [The analogy theorem in Hoare logic](https://arxiv.org/abs/2510.03685)
*Nikitin Nikita*

Main category: stat.ML

TL;DR: 本文提出用一阶逻辑和Hoare逻辑形式化数据集和模型类比概念，解决机器学习模型跨数据域迁移缺乏数学依据问题，经实验验证有效并带来新机遇。


<details>
  <summary>Details</summary>
Motivation: 机器学习方法广泛应用面临模型跨数据域迁移缺乏严格数学依据的问题，缺乏正式标准保证模型在不同数据上的特性。

Method: 用一阶逻辑和Hoare逻辑形式化数据集和模型类比概念，提出并严格证明类比定理。

Result: 在蒙特卡罗方法、MNIST和USPS数据上验证类比定理，卷积神经网络和随机森林的F1分数分别达0.84和0.88。

Conclusion: 该方法能证明跨域迁移的正确性，提供比较模型适用性的工具，严格形式化类比为理论研究和实际应用带来新机遇。

Abstract: The introduction of machine learning methods has led to significant advances
in automation, optimization, and discoveries in various fields of science and
technology. However, their widespread application faces a fundamental
limitation: the transfer of models between data domains generally lacks a
rigorous mathematical justification. The key problem is the lack of formal
criteria to guarantee that a model trained on one type of data will retain its
properties on another.This paper proposes a solution to this problem by
formalizing the concept of analogy between data sets and models using
first-order logic and Hoare logic.We formulate and rigorously prove a theorem
that sets out the necessary and sufficient conditions for analogy in the task
of knowledge transfer between machine learning models. Practical verification
of the analogy theorem on model data obtained using the Monte Carlo method, as
well as on MNIST and USPS data, allows us to achieving F1 scores of 0.84 and
0.88 for convolutional neural networks and random forests, respectively.The
proposed approach not only allows us to justify the correctness of transfer
between domains but also provides tools for comparing the applicability of
models to different types of data.The main contribution of the work is a
rigorous formalization of analogy at the level of program logic, providing
verifiable guarantees of the correctness of knowledge transfer, which opens new
opportunities for both theoretical research and the practical use of machine
learning models in previously inaccessible areas.

</details>


### [477] [Spectral Thresholds for Identifiability and Stability:Finite-Sample Phase Transitions in High-Dimensional Learning](https://arxiv.org/abs/2510.03809)
*William Hao-Cheng Huang*

Main category: stat.ML

TL;DR: 在高维学习中，样本量低于临界值时模型会突然崩溃，提出Fisher阈值定理，引入Fisher floor，实验验证预测转变。


<details>
  <summary>Details</summary>
Motivation: 解决高维学习中模型稳定性问题，明确模型稳定的条件。

Method: 提出Fisher阈值定理，证明稳定性要求最小Fisher特征值超过明确界限；引入Fisher floor进行可验证的谱正则化。

Result: 合成实验证实高斯混合和逻辑模型的预测转变，与d/n缩放一致。

Conclusion: 该阈值将经典特征值条件锐化为非渐近定律，定义了谱样本复杂度边界，弥合理论与鲁棒高维推断诊断的差距。

Abstract: In high-dimensional learning, models remain stable until they collapse
abruptly once the sample size falls below a critical level. This instability is
not algorithm-specific but a geometric mechanism: when the weakest Fisher
eigendirection falls beneath sample-level fluctuations, identifiability fails.
Our Fisher Threshold Theorem formalizes this by proving that stability requires
the minimal Fisher eigenvalue to exceed an explicit $O(\sqrt{d/n})$ bound.
Unlike prior asymptotic or model-specific criteria, this threshold is
finite-sample and necessary, marking a sharp phase transition between reliable
concentration and inevitable failure. To make the principle constructive, we
introduce the Fisher floor, a verifiable spectral regularization robust to
smoothing and preconditioning. Synthetic experiments on Gaussian mixtures and
logistic models confirm the predicted transition, consistent with $d/n$
scaling. Statistically, the threshold sharpens classical eigenvalue conditions
into a non-asymptotic law; learning-theoretically, it defines a spectral
sample-complexity frontier, bridging theory with diagnostics for robust
high-dimensional inference.

</details>


### [478] [Self-Speculative Masked Diffusions](https://arxiv.org/abs/2510.03929)
*Andrew Campbell,Valentin De Bortoli,Jiaxin Shi,Arnaud Doucet*

Main category: stat.ML

TL;DR: 提出自推测掩码扩散模型，用于离散数据生成，减少函数评估次数。在文本建模和蛋白质序列生成中，网络前向传播次数减少约2倍。


<details>
  <summary>Details</summary>
Motivation: 标准掩码扩散模型因因子分解近似，一次采样过多位置会降低样本质量，需大量模拟步骤和神经网络函数评估来生成高质量数据，为降低计算负担提出新模型。

Method: 修改最终变压器注意力掩码，从非因果变为因果，通过新颖的模型集成推测采样机制实现草稿令牌生成和并行验证，在单次前向传播中生成掩码位置的非因子化预测分布。

Result: 在GPT2规模文本建模和蛋白质序列生成中，与标准掩码扩散模型相比，所需网络前向传播次数减少约2倍。

Conclusion: 自推测掩码扩散模型能有效减少离散数据生成时的计算负担。

Abstract: We present self-speculative masked diffusions, a new class of masked
diffusion generative models for discrete data that require significantly fewer
function evaluations to generate samples. Standard masked diffusion models
predict factorized logits over currently masked positions. A number of masked
positions are then sampled, however, the factorization approximation means that
sampling too many positions in one go leads to poor sample quality. As a
result, many simulation steps and therefore neural network function evaluations
are required to generate high-quality data. We reduce the computational burden
by generating non-factorized predictions over masked positions. This is
achieved by modifying the final transformer attention mask from non-causal to
causal, enabling draft token generation and parallel validation via a novel,
model-integrated speculative sampling mechanism. This results in a
non-factorized predictive distribution over masked positions in a single
forward pass. We apply our method to GPT2 scale text modelling and protein
sequences generation, finding that we can achieve a ~2x reduction in the
required number of network forward passes relative to standard masked diffusion
models.

</details>


### [479] [Simulation-based inference via telescoping ratio estimation for trawl processes](https://arxiv.org/abs/2510.04042)
*Dan Leonte,Raphaël Huser,Almut E. D. Veraart*

Main category: stat.ML

TL;DR: 提出适用于难处理随机过程的SBI框架，经能源需求数据验证有效


<details>
  <summary>Details</summary>
Motivation: 现有时间随机过程参数估计难，基于模拟的推理方法存在需大量训练数据等问题

Method: 分两步，先跨参数维度顺序分解学习后验密度，再用切比雪夫多项式近似生成独立后验样本；开发诊断工具和事后校准技术

Result: 在拖网过程应用于能源需求数据中证明方法有效

Conclusion: 所提SBI框架快速准确、样本高效，事后校准技术可提升性能并降低训练成本

Abstract: The growing availability of large and complex datasets has increased interest
in temporal stochastic processes that can capture stylized facts such as
marginal skewness, non-Gaussian tails, long memory, and even non-Markovian
dynamics. While such models are often easy to simulate from, parameter
estimation remains challenging. Simulation-based inference (SBI) offers a
promising way forward, but existing methods typically require large training
datasets or complex architectures and frequently yield confidence (credible)
regions that fail to attain their nominal values, raising doubts on the
reliability of estimates for the very features that motivate the use of these
models. To address these challenges, we propose a fast and accurate,
sample-efficient SBI framework for amortized posterior inference applicable to
intractable stochastic processes. The proposed approach relies on two main
steps: first, we learn the posterior density by decomposing it sequentially
across parameter dimensions. Then, we use Chebyshev polynomial approximations
to efficiently generate independent posterior samples, enabling accurate
inference even when Markov chain Monte Carlo methods mix poorly. We further
develop novel diagnostic tools for SBI in this context, as well as post-hoc
calibration techniques; the latter not only lead to performance improvements of
the learned inferential tool, but also to the ability to reuse it directly with
new time series of varying lengths, thus amortizing the training cost. We
demonstrate the method's effectiveness on trawl processes, a class of flexible
infinitely divisible models that generalize univariate Gaussian processes,
applied to energy demand data.

</details>


### [480] [Scalable Causal Discovery from Recursive Nonlinear Data via Truncated Basis Function Scores and Tests](https://arxiv.org/abs/2510.04276)
*Joseph Ramsey,Bryan Andrews*

Main category: stat.ML

TL;DR: 引入两个基础扩展工具用于可扩展的因果发现，模拟和实际应用显示其能实现可解释和可扩展的因果发现。


<details>
  <summary>Details</summary>
Motivation: 从非线性、连续或混合数据中学习图形条件独立结构是挑战，现有方法难以扩展到大量样本或变量。

Method: 引入Basis Function BIC (BF - BIC) 分数和Basis Function Likelihood Ratio Test (BF - LRT)，分别用于近似非线性依赖和进行条件独立性测试。

Result: 在模拟中，BF - BIC在准确性和运行时间上优于基于核和约束的方法；BF - LRT比核测试快且准确性有竞争力。

Conclusion: 基于BF的方法集成到混合搜索中，能实现可解释和可扩展的因果发现，且有多种语言实现。

Abstract: Learning graphical conditional independence structures from nonlinear,
continuous or mixed data is a central challenge in machine learning and the
sciences, and many existing methods struggle to scale to thousands of samples
or hundreds of variables. We introduce two basis-expansion tools for scalable
causal discovery. First, the Basis Function BIC (BF-BIC) score uses truncated
additive expansions to approximate nonlinear dependencies. BF-BIC is
theoretically consistent under additive models and extends to post-nonlinear
(PNL) models via an invertible reparameterization. It remains robust under
moderate interactions and supports mixed data through a degenerate-Gaussian
embedding for discrete variables. In simulations with fully nonlinear neural
causal models (NCMs), BF-BIC outperforms kernel- and constraint-based methods
(e.g., KCI, RFCI) in both accuracy and runtime. Second, the Basis Function
Likelihood Ratio Test (BF-LRT) provides an approximate conditional independence
test that is substantially faster than kernel tests while retaining competitive
accuracy. Extensive simulations and a real-data application to Canadian
wildfire risk show that, when integrated into hybrid searches, BF-based methods
enable interpretable and scalable causal discovery. Implementations are
available in Python, R, and Java.

</details>


### [481] [Relative Information Gain and Gaussian Process Regression](https://arxiv.org/abs/2510.04277)
*Hamish Flynn*

Main category: stat.ML

TL;DR: 引入相对信息增益概念，证明高斯过程回归的PAC - Bayesian超额风险界，结合上界得到最优收敛率。


<details>
  <summary>Details</summary>
Motivation: 已知再生核希尔伯特空间中估计或最大化未知函数的样本复杂度与有效维度和信息增益有关，信息增益有信息论解释，但有效维度通常有更好的速率，希望找到新的衡量指标。

Method: 引入相对信息增益，证明PAC - Bayesian超额风险界，对相对信息增益进行有核谱性质相关的上界证明。

Result: 相对信息增益能在有效维度和信息增益间平滑插值，且与有效维度有相同增长率；结合相对信息增益上界和超额风险界得到最优收敛率。

Conclusion: 相对信息增益是衡量样本复杂度的有效指标，可用于得到高斯过程回归的最优收敛率。

Abstract: The sample complexity of estimating or maximising an unknown function in a
reproducing kernel Hilbert space is known to be linked to both the effective
dimension and the information gain associated with the kernel. While the
information gain has an attractive information-theoretic interpretation, the
effective dimension typically results in better rates. We introduce a new
quantity called the relative information gain, which measures the sensitivity
of the information gain with respect to the observation noise. We show that the
relative information gain smoothly interpolates between the effective dimension
and the information gain, and that the relative information gain has the same
growth rate as the effective dimension. In the second half of the paper, we
prove a new PAC-Bayesian excess risk bound for Gaussian process regression. The
relative information gain arises naturally from the complexity term in this
PAC-Bayesian bound. We prove bounds on the relative information gain that
depend on the spectral properties of the kernel. When these upper bounds are
combined with our excess risk bound, we obtain minimax-optimal rates of
convergence.

</details>


### [482] [Adaptive Coverage Policies in Conformal Prediction](https://arxiv.org/abs/2510.04318)
*Etienne Gauthier,Francis Bach,Michael I. Jordan*

Main category: stat.ML

TL;DR: 传统共形预测方法有固定覆盖率问题，本文利用e值和事后共形推理进展，提出通过神经网络优化自适应覆盖率策略，有理论保证并经实验验证。


<details>
  <summary>Details</summary>
Motivation: 传统共形预测方法覆盖率固定，选择不当会导致无信息预测，且无法适应单个样本特征，限制灵活性和效率。

Method: 利用e值和事后共形推理进展，在校准集上使用留一法训练神经网络来优化自适应覆盖率策略。

Result: 有理论覆盖率保证，并通过一系列实验证明了方法的实际益处。

Conclusion: 所提出的利用自适应覆盖率的共形预测方法有效可行，能解决传统方法的问题。

Abstract: Traditional conformal prediction methods construct prediction sets such that
the true label falls within the set with a user-specified coverage level.
However, poorly chosen coverage levels can result in uninformative predictions,
either producing overly conservative sets when the coverage level is too high,
or empty sets when it is too low. Moreover, the fixed coverage level cannot
adapt to the specific characteristics of each individual example, limiting the
flexibility and efficiency of these methods. In this work, we leverage recent
advances in e-values and post-hoc conformal inference, which allow the use of
data-dependent coverage levels while maintaining valid statistical guarantees.
We propose to optimize an adaptive coverage policy by training a neural network
using a leave-one-out procedure on the calibration set, allowing the coverage
level and the resulting prediction set size to vary with the difficulty of each
individual example. We support our approach with theoretical coverage
guarantees and demonstrate its practical benefits through a series of
experiments.

</details>


### [483] [Modular and Adaptive Conformal Prediction for Sequential Models via Residual Decomposition](https://arxiv.org/abs/2510.04406)
*William Zhang,Saurabh Amin,Georgia Perakis*

Main category: stat.ML

TL;DR: 提出用于两阶段顺序模型的保形预测框架，能归因不确定性且在多种数据实验中表现良好，有诊断优势和稳健覆盖性。


<details>
  <summary>Details</summary>
Motivation: 现有保形预测方法将建模过程视为黑盒，忽略模块化结构利用机会。

Method: 将整体预测残差分解为特定阶段组件，用FWER控制开发风险控制参数选择程序，针对非平稳设置提出自适应扩展。

Result: 在合成分布转移、供应链和股票市场数据实验中，该方法在标准保形方法性能下降的条件下维持覆盖，且能提供可解释的阶段不确定性归因。

Conclusion: 该框架具有标准保形方法所缺乏的诊断优势和稳健覆盖性。

Abstract: Conformal prediction offers finite-sample coverage guarantees under minimal
assumptions. However, existing methods treat the entire modeling process as a
black box, overlooking opportunities to exploit modular structure. We introduce
a conformal prediction framework for two-stage sequential models, where an
upstream predictor generates intermediate representations for a downstream
model. By decomposing the overall prediction residual into stage-specific
components, our method enables practitioners to attribute uncertainty to
specific pipeline stages. We develop a risk-controlled parameter selection
procedure using family-wise error rate (FWER) control to calibrate stage-wise
scaling parameters, and propose an adaptive extension for non-stationary
settings that preserves long-run coverage guarantees. Experiments on synthetic
distribution shifts, as well as real-world supply chain and stock market data,
demonstrate that our approach maintains coverage under conditions that degrade
standard conformal methods, while providing interpretable stage-wise
uncertainty attribution. This framework offers diagnostic advantages and robust
coverage that standard conformal methods lack.

</details>


### [484] [Learning Survival Models with Right-Censored Reporting Delays](https://arxiv.org/abs/2510.04421)
*Yuta Shikuri,Hironori Fujisawa*

Main category: stat.ML

TL;DR: 本文提出联合建模方法解决保险行业生存分析中报告延迟调整难题，实验证明可提高新入组风险评估及时性。


<details>
  <summary>Details</summary>
Motivation: 保险行业在实际约束下调整报告延迟是生存分析的重大挑战，尤其是新入组人群风险率估计时。

Method: 联合建模事件发生和报告时间的参数风险函数，边缘化潜在事件发生状态构建估计量，开发期望最大化算法计算估计值，提出两阶段估计程序。

Result: 实验结果表明该方法有效提高了新入组人群风险评估的及时性。

Conclusion: 所提方法能解决保险行业生存分析中报告延迟调整问题，改善新入组风险评估。

Abstract: Survival analysis is a statistical technique used to estimate the time until
an event occurs. Although it is applied across a wide range of fields,
adjusting for reporting delays under practical constraints remains a
significant challenge in the insurance industry. Such delays render event
occurrences unobservable when their reports are subject to right censoring.
This issue becomes particularly critical when estimating hazard rates for newly
enrolled cohorts with limited follow-up due to administrative censoring. Our
study addresses this challenge by jointly modeling the parametric hazard
functions of event occurrences and report timings. The joint probability
distribution is marginalized over the latent event occurrence status. We
construct an estimator for the proposed survival model and establish its
asymptotic consistency. Furthermore, we develop an expectation-maximization
algorithm to compute its estimates. Using these findings, we propose a
two-stage estimation procedure based on a parametric proportional hazards model
to evaluate observations subject to administrative censoring. Experimental
results demonstrate that our method effectively improves the timeliness of risk
evaluation for newly enrolled cohorts.

</details>


### [485] [Divergence Phase Index: A Riesz-Transform Framework for Multidimensional Phase Difference Analysis](https://arxiv.org/abs/2510.04426)
*Magaly Catanzariti,Hugo Aimar,Diego M. Mateos*

Main category: stat.ML

TL;DR: 提出Divergence Phase Index (DPI)框架量化信号相位差，在多领域数据集应用有良好结果，可用于多领域。


<details>
  <summary>Details</summary>
Motivation: 引入新框架来量化一维和多维信号的相位差，拓展经典希尔伯特变换相位测量原理到更高维度。

Method: 基于里兹变换的谐波分析引入DPI框架，并将其应用于合成和真实数据集。

Result: 在一维中能检测癫痫相关的超同步，二维中能揭示图像和艺术品的细微变化，还能检测显微镜图像的旋转变化。

Conclusion: DPI对振幅变化有鲁棒性且跨领域适应性强，可用于非线性动力学、复杂系统分析和多维信号处理等多种应用。

Abstract: We introduce the Divergence Phase Index (DPI), a novel framework for
quantifying phase differences in one and multidimensional signals, grounded in
harmonic analysis via the Riesz transform. Based on classical Hilbert Transform
phase measures, the DPI extends these principles to higher dimensions, offering
a geometry-aware metric that is invariant to intensity scaling and sensitive to
structural changes. We applied this method on both synthetic and real-world
datasets, including intracranial EEG (iEEG) recordings during epileptic
seizures, high-resolution microscopy images, and paintings. In the 1D case, the
DPI robustly detects hypersynchronization associated with generalized epilepsy,
while in 2D, it reveals subtle, imperceptible changes in images and artworks.
Additionally, it can detect rotational variations in highly isotropic
microscopy images. The DPI's robustness to amplitude variations and its
adaptability across domains enable its use in diverse applications from
nonlinear dynamics, complex systems analysis, to multidimensional signal
processing.

</details>


### [486] [Computing Wasserstein Barycenters through Gradient Flows](https://arxiv.org/abs/2510.04602)
*Eduardo Fernandes Montesuma,Yassir Bendou,Mike Gartrell*

Main category: stat.ML

TL;DR: 本文将Wasserstein重心问题重铸为Wasserstein空间中的梯度流，通过小批量采样实现可扩展性，引入泛函正则化，给出两种算法并证明收敛性，实验显示优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 现有离散方法计算Wasserstein重心时可扩展性差，需要访问输入测度的完整样本集。

Method: 将原重心问题重铸为Wasserstein空间中的梯度流，通过小批量采样输入测度实现可扩展性，引入概率测度上的泛函进行正则化，给出经验测度和高斯混合测度的算法。

Result: 在玩具数据集和领域适应基准上的实验表明，该方法优于先前的离散和基于神经网络的计算Wasserstein重心的方法。

Conclusion: 提出的方法有效解决了现有离散方法可扩展性差的问题，在计算Wasserstein重心上具有优势。

Abstract: Wasserstein barycenters provide a powerful tool for aggregating probability
measures, while leveraging the geometry of their ambient space. Existing
discrete methods suffer from poor scalability, as they require access to the
complete set of samples from input measures. We address this issue by recasting
the original barycenter problem as a gradient flow in the Wasserstein space.
Our approach offers two advantages. First, we achieve scalability by sampling
mini-batches from the input measures. Second, we incorporate functionals over
probability measures, which regularize the barycenter problem through internal,
potential, and interaction energies. We present two algorithms for empirical
and Gaussian mixture measures, providing convergence guarantees under the
Polyak-{\L}ojasiewicz inequality. Experimental validation on toy datasets and
domain adaptation benchmarks show that our methods outperform previous discrete
and neural net-based methods for computing Wasserstein barycenters.

</details>


### [487] [Fisher-Bingham-like normalizing flows on the sphere](https://arxiv.org/abs/2510.04762)
*Thorsten Glüsenkamp*

Main category: stat.ML

TL;DR: 本文提出了“zoom - linear - project” (ZLP) - Fisher流，可推广特殊情况为归一化流家族，能逐步增加复杂度，处理条件密度估计问题，其中Kent类似物可提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的球面上的Fisher - Bingham (FB)或Angular Gaussian (AG)分布族除特殊情况外无法直接写成归一化流，需要进行推广。

Method: 将特殊情况推广为在任意维度上表现类似完整FB或AG家族的归一化流家族，即ZLP - Fisher流。

Result: ZLP - Fisher流可按需逐步增加复杂度，能自然处理目标分布尺度差异大的条件密度估计问题，Kent类似物可提升性能。

Conclusion: ZLP - Fisher流是一种有效的归一化流推广，在天文学应用等场景有重要作用。

Abstract: A generic D-dimensional Gaussian can be conditioned or projected onto the D-1
unit sphere, thereby leading to the well-known Fisher-Bingham (FB) or Angular
Gaussian (AG) distribution families, respectively. These are some of the most
fundamental distributions on the sphere, yet cannot straightforwardly be
written as a normalizing flow except in two special cases: the von-Mises Fisher
in D=3 and the central angular Gaussian in any D. In this paper, we describe
how to generalize these special cases to a family of normalizing flows that
behave similarly to the full FB or AG family in any D. We call them
"zoom-linear-project" (ZLP)-Fisher flows. Unlike a normal Fisher-Bingham
distribution, their composition allows to gradually add complexity as needed.
Furthermore, they can naturally handle conditional density estimation with
target distributions that vary by orders of magnitude in scale - a setting that
is important in astronomical applications but that existing flows often
struggle with. A particularly useful member of the new family is the Kent
analogue that can cheaply upgrade any flow in this situation to yield better
performance.

</details>


### [488] [Kernel ridge regression under power-law data: spectrum and generalization](https://arxiv.org/abs/2510.04780)
*Arie Wortsman,Bruno Loureiro*

Main category: stat.ML

TL;DR: 研究高维核岭回归在各向异性幂律协方差的独立同分布高斯数据上的表现，有两方面贡献，揭示幂律各向异性数据学习优势。


<details>
  <summary>Details</summary>
Motivation: 现有KRR经典源与容量条件通常对核特征谱本身做幂律假设，本文研究场景与之不同，需新的探索。

Method: 推导多项式内积核的核谱的显式特征，对特定核在高维下的超额风险进行渐近分析。

Result: 明确核特征谱如何继承数据衰减特性，表明样本复杂度由数据有效维度而非环境维度决定。

Conclusion: 幂律各向异性数据学习相比各向同性数据有根本优势，是对幂律数据下非线性KRR的首次严格处理。

Abstract: In this work, we investigate high-dimensional kernel ridge regression (KRR)
on i.i.d. Gaussian data with anisotropic power-law covariance. This setting
differs fundamentally from the classical source & capacity conditions for KRR,
where power-law assumptions are typically imposed on the kernel eigen-spectrum
itself. Our contributions are twofold. First, we derive an explicit
characterization of the kernel spectrum for polynomial inner-product kernels,
giving a precise description of how the kernel eigen-spectrum inherits the data
decay. Second, we provide an asymptotic analysis of the excess risk in the
high-dimensional regime for a particular kernel with this spectral behavior,
showing that the sample complexity is governed by the effective dimension of
the data rather than the ambient dimension. These results establish a
fundamental advantage of learning with power-law anisotropic data over
isotropic data. To our knowledge, this is the first rigorous treatment of
non-linear KRR under power-law data.

</details>


### [489] [A Noise Resilient Approach for Robust Hurst Exponent Estimation](https://arxiv.org/abs/2510.04811)
*Malith Premarathna,Fabrizio Ruggeri,Dixon Vimalajeewa*

Main category: stat.ML

TL;DR: 提出NC - ALPHEE方法改进Hurst指数估计，在噪声环境表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有小波方法估计Hurst指数时，现实测量中的噪声会降低精度，需改进。

Method: 提出NC - ALPHEE方法，结合噪声缓解，用神经网络组合多组估计值。

Result: 无噪声时与ALPHEE精度相当；有噪声时，传统平均法性能下降，NC - ALPHEE始终优于现有技术。

Conclusion: NC - ALPHEE为Hurst指数估计提供了鲁棒、自适应的方法，提升了小波方法在噪声环境中的可靠性。

Abstract: Understanding signal behavior across scales is vital in areas such as natural
phenomena analysis and financial modeling. A key property is self-similarity,
quantified by the Hurst exponent (H), which reveals long-term dependencies.
Wavelet-based methods are effective for estimating H due to their multi-scale
analysis capability, but additive noise in real-world measurements often
degrades accuracy. We propose Noise-Controlled ALPHEE (NC-ALPHEE), an
enhancement of the Average Level-Pairwise Hurst Exponent Estimator (ALPHEE),
incorporating noise mitigation and generating multiple level-pairwise estimates
from signal energy pairs. A neural network (NN) combines these estimates,
replacing traditional averaging. This adaptive learning maintains ALPHEE's
behavior in noise-free cases while improving performance in noisy conditions.
Extensive simulations show that in noise-free data, NC-ALPHEE matches ALPHEE's
accuracy using both averaging and NN-based methods. Under noise, however,
traditional averaging deteriorates and requires impractical level restrictions,
while NC-ALPHEE consistently outperforms existing techniques without such
constraints. NC-ALPHEE offers a robust, adaptive approach for H estimation,
significantly enhancing the reliability of wavelet-based methods in noisy
environments.

</details>


### [490] [Set to Be Fair: Demographic Parity Constraints for Set-Valued Classification](https://arxiv.org/abs/2510.04926)
*Eyal Cohen,Christophe Denis,Mohamed Hebiri*

Main category: stat.ML

TL;DR: 本文研究公平约束下的集值分类问题，提出两种策略，推导分类器表达式，建立收敛率和风险界，实证表明策略有效。


<details>
  <summary>Details</summary>
Motivation: 集值分类应用可能放大歧视性偏差，需在公平约束下开发集值方法。

Method: 提出基于神谕的方法最小化分类风险并满足约束，以及优先满足约束的高效代理方法，推导公平集值分类器的闭式表达式并构建数据驱动程序。

Result: 建立两种方法约束违反的无分布收敛率，在温和假设下为基于神谕的方法提供超额风险界，实证表明两种策略有效，代理方法高效。

Conclusion: 提出的两种策略在公平约束下的集值分类问题上有效，代理方法具有效率优势。

Abstract: Set-valued classification is used in multiclass settings where confusion
between classes can occur and lead to misleading predictions. However, its
application may amplify discriminatory bias motivating the development of
set-valued approaches under fairness constraints. In this paper, we address the
problem of set-valued classification under demographic parity and expected size
constraints. We propose two complementary strategies: an oracle-based method
that minimizes classification risk while satisfying both constraints, and a
computationally efficient proxy that prioritizes constraint satisfaction. For
both strategies, we derive closed-form expressions for the (optimal) fair
set-valued classifiers and use these to build plug-in, data-driven procedures
for empirical predictions. We establish distribution-free convergence rates for
violations of the size and fairness constraints for both methods, and under
mild assumptions we also provide excess-risk bounds for the oracle-based
approach. Empirical results demonstrate the effectiveness of both strategies
and highlight the efficiency of our proxy method.

</details>


### [491] [Embracing Discrete Search: A Reasonable Approach to Causal Structure Learning](https://arxiv.org/abs/2510.04970)
*Marcel Wienöbst,Leonard Henckel,Sebastian Weichwald*

Main category: stat.ML

TL;DR: 提出FLOP算法用于线性模型因果发现，能削减运行时间，结果准确，建议重新审视图离散搜索。


<details>
  <summary>Details</summary>
Motivation: 开发更高效的线性模型因果发现算法，削减运行时间并提高搜索图结构的准确性。

Method: 将快速父节点选择与基于迭代Cholesky的分数更新相结合，采用离散搜索和有原则的顺序初始化进行迭代局部搜索。

Result: 在基准测试中结果高度准确，在标准设置下近乎完美恢复。

Conclusion: 离散搜索作为因果发现的方法值得重新审视。

Abstract: We present FLOP (Fast Learning of Order and Parents), a score-based causal
discovery algorithm for linear models. It pairs fast parent selection with
iterative Cholesky-based score updates, cutting run-times over prior
algorithms. This makes it feasible to fully embrace discrete search, enabling
iterated local search with principled order initialization to find graphs with
scores at or close to the global optimum. The resulting structures are highly
accurate across benchmarks, with near-perfect recovery in standard settings.
This performance calls for revisiting discrete search over graphs as a
reasonable approach to causal discovery.

</details>


### [492] [Curiosity-Driven Co-Development of Action and Language in Robots Through Self-Exploration](https://arxiv.org/abs/2510.05013)
*Theodore Jerome Tinker,Kenji Doya,Jun Tani*

Main category: stat.ML

TL;DR: 研究通过机器人模拟实验，探索人类高效语言与动作协同发展学习机制，发现好奇驱动探索等相关成果。


<details>
  <summary>Details</summary>
Motivation: 探究人类从少量学习示例中实现高效发展性学习的机制，对比大语言模型需大量训练数据。

Method: 进行机器人模拟实验，将主动推理框架与强化学习结合，让机器人通过自我引导探索学习执行对应命令句的动作。

Result: 好奇驱动探索结合运动噪声学习效果更好；简单动作先发展，复杂动作后发展；先有语句与动作的机械配对，后有组合泛化；组合元素增加，泛化能力显著提升。

Conclusion: 结果揭示了婴儿高效协同发展学习的可能机制，与发展心理学的发现有计算上的相似性。

Abstract: Human infants acquire language and action co-developmentally, achieving
remarkable generalization capabilities from only a minimal number of learning
examples. In contrast, recent large language models require exposure to
billions of training tokens to achieve such generalization. What mechanisms
underlie such efficient developmental learning in humans? This study addresses
this question through simulation experiments in which robots learn to perform
various actions corresponding to imperative sentences (e.g., \textit{push red
cube}) via trials of self-guided exploration. Our approach integrates the
active inference framework with reinforcement learning, enabling
curiosity-driven developmental learning. The simulations yielded several
nontrivial findings: i) Curiosity-driven exploration combined with motor noise
substantially outperforms learning without curiosity. ii) Simpler,
prerequisite-like actions emerge earlier in development, while more complex
actions involving these prerequisites develop later. iii) Rote pairing of
sentences and actions occurs before the emergence of compositional
generalization. iv) Generalization is drastically improved as the number of
compositional elements increases. These results shed light into possible
mechanisms underlying efficient co-developmental learning in infants and
provide computational parallels to findings in developmental psychology.

</details>


### [493] [Causal Abstractions, Categorically Unified](https://arxiv.org/abs/2510.05033)
*Markus Englberger,Devendra Singh Dhami*

Main category: stat.ML

TL;DR: 提出用于关联不同抽象层次因果模型的范畴框架，统一并推广现有因果抽象方法，给出相关证明和推广，还讨论了与机制可解释性方法的结合及应用。


<details>
  <summary>Details</summary>
Motivation: 构建一个能关联不同抽象层次因果模型的框架，统一和推广已有因果抽象方法。

Method: 将因果抽象定义为合适的马尔可夫函子之间的自然变换，使用字符串图工具描述抽象图，结合机制可解释性方法和do - calculus。

Result: 获得范畴证明和对现有因果抽象结果的推广，明确了低级别图在干预下的一致抽象图，推广了Anand等人的陈述。

Conclusion: 该框架比现有范畴框架更适合建模因果抽象，能恢复如τ - 一致性等概念。

Abstract: We present a categorical framework for relating causal models that represent
the same system at different levels of abstraction. We define a causal
abstraction as natural transformations between appropriate Markov functors,
which concisely consolidate desirable properties a causal abstraction should
exhibit. Our approach unifies and generalizes previously considered causal
abstractions, and we obtain categorical proofs and generalizations of existing
results on causal abstractions. Using string diagrammatical tools, we can
explicitly describe the graphs that serve as consistent abstractions of a
low-level graph under interventions. We discuss how methods from mechanistic
interpretability, such as circuit analysis and sparse autoencoders, fit within
our categorical framework. We also show how applying do-calculus on a
high-level graphical abstraction of an acyclic-directed mixed graph (ADMG),
when unobserved confounders are present, gives valid results on the low-level
graph, thus generalizing an earlier statement by Anand et al. (2023). We argue
that our framework is more suitable for modeling causal abstractions compared
to existing categorical frameworks. Finally, we discuss how notions such as
$\tau$-consistency and constructive $\tau$-abstractions can be recovered with
our framework.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [494] [Exact and Approximate MCMC for Doubly-intractable Probabilistic Graphical Models Leveraging the Underlying Independence Model](https://arxiv.org/abs/2510.03587)
*Yujie Chen,Antik Chakraborty,Anindya Bhadra*

Main category: stat.CO

TL;DR: 本文提出一种无需完美或顺序采样的方法，可用于精确和近似MCMC，在高维具有可扩展性，并在Ising模型上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 现有双难处理概率图模型的贝叶斯推理方法需要完美采样器或顺序采样器，在高维存在问题。

Method: 利用难处理概率图模型下的易处理独立模型构建Metropolis - Hastings比率的有限样本无偏蒙特卡罗估计。

Result: 该方法在Ising模型上得到验证，基于梯度构建提议的方法也作为推论出现并得到验证。

Conclusion: 提出的方法不依赖完美或顺序采样，在高维具有可扩展性。

Abstract: Bayesian inference for doubly-intractable probabilistic graphical models
typically involves variations of the exchange algorithm or approximate Markov
chain Monte Carlo (MCMC) samplers. However, existing methods for both classes
of algorithms require either perfect samplers or sequential samplers for
complex models, which are often either not available, or suffer from poor
mixing, especially in high dimensions. We develop a method that does not
require perfect or sequential sampling, and can be applied to both classes of
methods: exact and approximate MCMC. The key to our approach is to utilize the
tractable independence model underlying an intractable probabilistic graphical
model for the purpose of constructing a finite sample unbiased Monte Carlo (and
not MCMC) estimate of the Metropolis--Hastings ratio. This innovation turns out
to be crucial for scalability in high dimensions. The method is demonstrated on
the Ising model. Gradient-based alternatives to construct a proposal, such as
Langevin and Hamiltonian Monte Carlo approaches, also arise as a natural
corollary to our general procedure, and are demonstrated as well.

</details>


### [495] [Analysis of kinetic Langevin Monte Carlo under the stochastic exponential Euler discretization from underdamped all the way to overdamped](https://arxiv.org/abs/2510.03949)
*Kyurae Kim,Samuel Gruffaz,Ji Won Park,Alain Oliviero Durmus*

Main category: stat.CO

TL;DR: 本文重新分析带指数积分器的KLMC同步Wasserstein耦合，得到在更弱参数限制下的Wasserstein收缩和渐近偏差界，证明指数积分器在过阻尼状态下也能稳定模拟。


<details>
  <summary>Details</summary>
Motivation: 现有对随机指数欧拉离散化（指数积分器）的分析对参数有限制，无法解释不同参数选择下KLMC的行为，且在过阻尼状态下已知结果不成立。

Method: 重新进行带指数积分器的KLMC同步Wasserstein耦合分析。

Result: 得到在更弱参数限制下的Wasserstein收缩和渐近偏差界。

Conclusion: 只要应用适当的时间加速，指数积分器能在过阻尼状态下稳定模拟动力学。

Abstract: Simulating the kinetic Langevin dynamics is a popular approach for sampling
from distributions, where only their unnormalized densities are available.
Various discretizations of the kinetic Langevin dynamics have been considered,
where the resulting algorithm is collectively referred to as the kinetic
Langevin Monte Carlo (KLMC) or underdamped Langevin Monte Carlo. Specifically,
the stochastic exponential Euler discretization, or exponential integrator for
short, has previously been studied under strongly log-concave and log-Lipschitz
smooth potentials via the synchronous Wasserstein coupling strategy. Existing
analyses, however, impose restrictions on the parameters that do not explain
the behavior of KLMC under various choices of parameters. In particular, all
known results fail to hold in the overdamped regime, suggesting that the
exponential integrator degenerates in the overdamped limit. In this work, we
revisit the synchronous Wasserstein coupling analysis of KLMC with the
exponential integrator. Our refined analysis results in Wasserstein
contractions and bounds on the asymptotic bias that hold under weaker
restrictions on the parameters, which assert that the exponential integrator is
capable of stably simulating the kinetic Langevin dynamics in the overdamped
regime, as long as proper time acceleration is applied.

</details>


### [496] [Green's Function-Based Thin Plate Splines via Karhunen-Loève Expansion for Bayesian Spatial Modeling](https://arxiv.org/abs/2510.04256)
*Joaquin Cavieres,Sebastian Krumscheid*

Main category: stat.CO

TL;DR: 提出用正则化薄板样条核显式构造协方差函数近似高斯随机场的新方法，数值分析和实际应用均表现良好。


<details>
  <summary>Details</summary>
Motivation: 高斯随机场近似对计算可行性至关重要，传统方法依赖协方差结构定义，需新的近似方法。

Method: 通过正则化薄板样条核显式构造高斯随机场的协方差函数。

Result: 数值分析中正确捕捉空间相关性，有效降维；实际应用中模拟德国监测站NO₂浓度表现好，预测性能佳。

Conclusion: 该方法有潜力超越Matern相关函数，是近似高斯随机场的有效方法。

Abstract: Gaussian random field is an ubiquitous model for spatial phenomena in diverse
scientific disciplines. Its approximation is often crucial for computational
feasibility in simulation, inference, and uncertainty quantification. The
Karhunen-Lo\`eve Expansion provides a theoretically optimal basis for
representing a Gaussian random field as a sum of deterministic orthonormal
functions weighted by uncorrelated random variables. While this is a
well-established method for dimension reduction and approximation of (spatial)
stochastic process, its practical application depends on the explicit or
implicit definition of the covariance structure. In this work we propose a
novel approach to approximating Gaussian random field by explicitly
constructing its covariance function from a regularized thin plate splines
kernel. In a numerical analysis, the regularized thin plate splines kernel
model, under a Bayesian approach, correctly capture the spatial correlation in
the different proposed scenarios. Furthermore, the penalty term effectively
shrinks most basis function coefficients toward zero, the eigenvalues decay and
cumulative variance show that the proposed model efficiently reduces data
dimensionality by capturing most of the variance with only a few basis
functions. More importantly, from the numerical analysis we can suggest its
strong potential for use beyond the Matern correlation function. In a real
application, it behaves well when modeling the NO2 concentrations measured at
monitoring stations throughout Germany. It has good predictive performance when
assessed using the posterior medians and also demonstrate best predictive
performance compared with another popular method to approximate a Gaussian
random field.

</details>


### [497] [spd-metrics-id: A Python Package for SPD-Aware Distance Metrics in Connectome Fingerprinting and Beyond](https://arxiv.org/abs/2510.04438)
*Kaosar Uddin*

Main category: stat.CO

TL;DR: 介绍Python包spd - metrics - id，可计算对称正定矩阵距离和散度，有统一框架，支持多种度量，确保可重复性，有多种使用方式，应用广泛且开源。


<details>
  <summary>Details</summary>
Motivation: 传统工具包聚焦特定应用，缺乏统一、可扩展和可重复的对称正定矩阵距离计算框架，故开发spd - metrics - id。

Method: 开发spd - metrics - id包，支持多种几何感知度量，提供命令行界面和Python API，通过Docker镜像和Zenodo存档确保可重复性。

Result: 实现spd - metrics - id包，通过连接指纹示例展示用法，适用于协方差分析、扩散张量成像等领域。

Conclusion: spd - metrics - id是一个开源、统一、可扩展且可重复的对称正定矩阵距离计算工具包，有广泛应用前景。

Abstract: We present spd-metrics-id, a Python package for computing distances and
divergences between symmetric positive-definite (SPD) matrices. Unlike
traditional toolkits that focus on specific applications, spd-metrics-id
provides a unified, extensible, and reproducible framework for SPD distance
computation. The package supports a wide variety of geometry-aware metrics,
including Alpha-z Bures-Wasserstein, Alpha-Procrustes, affine-invariant
Riemannian, log-Euclidean, and others, and is accessible both via a
command-line interface and a Python API. Reproducibility is ensured through
Docker images and Zenodo archiving. We illustrate usage through a connectome
fingerprinting example, but the package is broadly applicable to covariance
analysis, diffusion tensor imaging, and other domains requiring SPD matrix
comparison. The package is openly available at
https://pypi.org/project/spd-metrics-id/.

</details>


### [498] [Constrained Dikin-Langevin diffusion for polyhedra](https://arxiv.org/abs/2510.04582)
*James Chok,Domenic Petzinna*

Main category: stat.CO

TL;DR: 利用Dikin对数障碍定义Dikin - Langevin扩散用于多面体上的约束采样和优化，数值结果显示该方法有优势。


<details>
  <summary>Details</summary>
Motivation: 寻找一种在多面体上进行约束采样和优化的直接方法，避免反射和临时投影。

Method: 利用Dikin对数障碍定义Dikin - Langevin扩散，采用先离散后修正的设计，即Euler - Maruyama提议加Metropolis - Hastings修正。

Result: 未调整的扩散有一阶步长偏差，MH调整后的变体在各向异性、盒约束高斯分布上收敛性好，在双峰目标上跨阱移动性强。

Conclusion: 将校准的随机性与内点预条件相结合，为多面体域上的采样和优化提供了无反射的实用方法，在面、角和非凸景观中有优势。

Abstract: Interior-point geometry offers a straightforward approach to constrained
sampling and optimization on polyhedra, eliminating reflections and ad hoc
projections. We exploit the Dikin log-barrier to define a Dikin--Langevin
diffusion whose drift and noise are modulated by the inverse barrier Hessian.
In continuous time, we establish a boundary no-flux property; trajectories
started in the interior remain in $U$ almost surely, so feasibility is
maintained by construction. For computation, we adopt a discretize-then-correct
design: an Euler--Maruyama proposal with state-dependent covariance, followed
by a Metropolis--Hastings correction that targets the exact constrained law and
reduces to a Dikin random walk when $f$ is constant.
  Numerically, the unadjusted diffusion exhibits the expected first-order step
size bias, while the MH-adjusted variant delivers strong convergence
diagnostics on anisotropic, box-constrained Gaussians (rank-normalized
split-$\hat{R}$ concentrated near $1$) and higher inter-well transition counts
on a bimodal target, indicating superior cross-well mobility. Taken together,
these results demonstrate that coupling calibrated stochasticity with
interior-point preconditioning provides a practical, reflection-free approach
to sampling and optimization over polyhedral domains, offering clear advantages
near faces, corners, and in nonconvex landscapes.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [499] [A behavioral reinvestigation of the effect of long ties on social contagions](https://arxiv.org/abs/2510.04785)
*Luca Lazzaro,Manuel S. Mariani,René Algesheimer,Radu Tanase*

Main category: physics.soc-ph

TL;DR: 研究长连接在产品扩散中的作用，发现个体对社会影响反应的异质性决定长连接效果，长连接促进扩散但不确定性增加时其积极作用减弱。


<details>
  <summary>Details</summary>
Motivation: 文献对长连接在扩散中作用结论不一，源于对个体决策假设不同，需重新研究长连接作用。

Method: 通过实验测量不确定收益产品在社会影响下的采用决策，并将决策嵌入网络模拟。

Result: 个体层面，收益不确定性增加会提高对社会影响的依赖，且个体反应有异质性；集体层面，长连接促进扩散，但不确定性增加时积极作用减弱。

Conclusion: 长连接的效果由个体对社会影响反应的异质性程度决定，而非扩散过程类型。

Abstract: Faced with uncertainty in decision making, individuals often turn to their
social networks to inform their decisions. In consequence, these networks
become central to how new products and behaviors spread. A key structural
feature of networks is the presence of long ties, which connect individuals who
share few mutual contacts. Under what conditions do long ties facilitate or
hinder diffusion? The literature provides conflicting results, largely due to
differing assumptions about individual decision-making. We reinvestigate the
role of long ties by experimentally measuring adoption decisions under social
influence for products with uncertain payoffs and embedding these decisions in
network simulations. At the individual level, we find that higher payoff
uncertainty increases the average reliance on social influence. However,
personal traits such as risk preferences and attitudes toward uncertainty lead
to substantial heterogeneity in how individuals respond to social influence. At
the collective level, the observed individual heterogeneity ensures that long
ties consistently promote diffusion, but their positive effect weakens as
uncertainty increases. Our results reveal that the effect of long ties is not
determined by whether the aggregate process is a simple or complex contagion,
but by the extent of heterogeneity in how individuals respond to social
influence.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [500] [PLSEMANTICSBENCH: Large Language Models As Programming Language Interpreters](https://arxiv.org/abs/2510.03415)
*Aditya Thimmaiah,Jiyang Zhang,Jayanth Srinivasa,Junyi Jessy Li,Milos Gligoric*

Main category: cs.PL

TL;DR: 研究大语言模型能否基于编程语言形式语义执行程序，构建评估集和任务评估，发现模型有潜力但缺乏稳健语义理解。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型能否基于编程语言形式语义执行程序，以实现新编程语言和特性的快速原型设计。

Method: 使用IMP语言，引入三种评估集，定义三个评估任务，通过系统变异标准规则得到非标准语义进行评估。

Result: 强代码/推理大语言模型在标准语义下表现好，非标准语义下性能下降，不同模型失败有模式，多数推理模型在粗粒度任务表现好，提供形式语义对简单程序有帮助，对复杂程序常有害。

Conclusion: 大语言模型有作为编程语言解释器的潜力，但缺乏稳健的语义理解。

Abstract: As large language models (LLMs) excel at code reasoning, a natural question
arises: can an LLM execute programs (i.e., act as an interpreter) purely based
on a programming language's formal semantics? If so, it will enable rapid
prototyping of new programming languages and language features. We study this
question using the imperative language IMP (a subset of C), formalized via
small-step operational semantics (SOS) and rewriting-based operational
semantics (K-semantics). We introduce three evaluation sets-Human-Written,
LLM-Translated, and Fuzzer- Generated-whose difficulty is controlled by
code-complexity metrics spanning the size, control-flow, and data-flow axes.
Given a program and its semantics formalized with SOS/K-semantics, models are
evaluated on three tasks ranging from coarse to fine: (1) final-state
prediction, (2) semantic rule prediction, and (3) execution trace prediction.
To distinguish pretraining memorization from semantic competence, we define two
nonstandard semantics obtained through systematic mutations of the standard
rules. Across strong code/reasoning LLMs, performance drops under nonstandard
semantics despite high performance under the standard one. We further find that
(i) there are patterns to different model failures, (ii) most reasoning models
perform exceptionally well on coarse grained tasks involving reasoning about
highly complex programs often containing nested loop depths beyond five, and
surprisingly, (iii) providing formal semantics helps on simple programs but
often hurts on more complex ones. Overall, the results show a promise that LLMs
could serve as programming language interpreters, but points to the lack of
their robust semantics understanding. We release the benchmark and the
supporting code at https://github.com/EngineeringSoftware/PLSemanticsBench.

</details>


### [501] [Retrofitting Control Flow Graphs in LLVM IR for Auto Vectorization](https://arxiv.org/abs/2510.04890)
*Shihan Fang,Wenxin Zheng*

Main category: cs.PL

TL;DR: 现代处理器依赖SIMD指令集提升性能，但现有编译器难以充分利用矢量化机会。本文提出含SIR和VIR两种IR扩展的矢量化流水线，实验显示比LLVM和GCC有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有生产级编译器（如LLVM和GCC）因矢量化过程不连贯和可扩展性有限，难以充分利用矢量化机会，且高效简化控制流分析和准确识别矢量化机会存在挑战。

Method: 引入含SIR和VIR两种专门IR扩展的矢量化流水线，利用VIR提供的详细依赖信息开发灵活可扩展的矢量化框架。

Result: 实验评估表明，提出的矢量化流水线实现显著性能提升，与LLVM和GCC相比，分别实现高达53%和58%的加速。

Conclusion: 所提方法大幅提高矢量化过程的互操作性，扩大同构指令搜索空间，增强自动矢量化的范围和效率。

Abstract: Modern processors increasingly rely on SIMD instruction sets, such as AVX and
RVV, to significantly enhance parallelism and computational performance.
However, production-ready compilers like LLVM and GCC often fail to fully
exploit available vectorization opportunities due to disjoint vectorization
passes and limited extensibility. Although recent attempts in heuristics and
intermediate representation (IR) designs have attempted to address these
problems, efficiently simplifying control flow analysis and accurately
identifying vectorization opportunities remain challenging tasks.
  To address these issues, we introduce a novel vectorization pipeline
featuring two specialized IR extensions: SIR, which encodes high-level
structural information, and VIR, which explicitly represents instruction
dependencies through data dependency analysis. Leveraging the detailed
dependency information provided by VIR, we develop a flexible and extensible
vectorization framework. This approach substantially improves interoperability
across vectorization passes and expands the search space for identifying
isomorphic instructions, ultimately enhancing both the scope and efficiency of
automatic vectorization. Experimental evaluations demonstrate that our proposed
vectorization pipeline achieves significant performance improvements,
delivering speedups of up to 53% and 58% compared to LLVM and GCC,
respectively.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [502] [Automating construction safety inspections using a multi-modal vision-language RAG framework](https://arxiv.org/abs/2510.04145)
*Chenxin Wang,Elyas Asadi Shamsabadi,Zhaohui Chen,Luming Shen,Alireza Ahmadian Fard Fini,Daniel Dias-da-Costa*

Main category: cs.CV

TL;DR: 研究引入多模态LVLM的SiteShield框架用于建筑安全检查报告自动化，性能优于无RAG的单模态LLMs，能提升信息检索和报告生成效率。


<details>
  <summary>Details</summary>
Motivation: 传统建筑安全检查方法低效，现有LVLMs应用有局限，LLMs用于此受训练数据和实时适应性限制。

Method: 引入基于多模态LVLM的检索增强生成（RAG）框架SiteShield，整合视觉和音频输入。

Result: 使用真实数据，SiteShield的F1分数为0.82，汉明损失为0.04，精度为0.76，召回率为0.96，优于无RAG的单模态LLMs。

Conclusion: SiteShield为提升信息检索和安全报告生成效率提供了新途径。

Abstract: Conventional construction safety inspection methods are often inefficient as
they require navigating through large volume of information. Recent advances in
large vision-language models (LVLMs) provide opportunities to automate safety
inspections through enhanced visual and linguistic understanding. However,
existing applications face limitations including irrelevant or unspecific
responses, restricted modal inputs and hallucinations. Utilisation of Large
Language Models (LLMs) for this purpose is constrained by availability of
training data and frequently lack real-time adaptability. This study introduces
SiteShield, a multi-modal LVLM-based Retrieval-Augmented Generation (RAG)
framework for automating construction safety inspection reports by integrating
visual and audio inputs. Using real-world data, SiteShield outperformed
unimodal LLMs without RAG with an F1 score of 0.82, hamming loss of 0.04,
precision of 0.76, and recall of 0.96. The findings indicate that SiteShield
offers a novel pathway to enhance information retrieval and efficiency in
generating safety reports.

</details>


### [503] [OpenFLAME: Federated Visual Positioning System to Enable Large-Scale Augmented Reality Applications](https://arxiv.org/abs/2510.03915)
*Sagar Bharadwaj,Harrison Williams,Luke Wang,Michael Liang,Tao Jin,Srinivasan Seshan,Anthony Rowe*

Main category: cs.CV

TL;DR: 提出OpenFLAME，一种联合VPS后端，解决现有集中式VPS方案的不足。


<details>
  <summary>Details</summary>
Motivation: 现有集中式VPS解决方案无法满足未来AR应用对私人室内空间的需求，存在隐私、法规和维护瓶颈等问题。

Method: 引入联合图像定位概念，提供在不共享私有数据情况下管理和合并跨地图数据的参考解决方案。

Result: 实现室内3D扫描的访问控制、VPS后端的分布式维护，并鼓励更大范围覆盖。

Conclusion: OpenFLAME能解决分片VPS服务带来的挑战，是适用于独立组织的VPS后端方案。

Abstract: World-scale augmented reality (AR) applications need a ubiquitous 6DoF
localization backend to anchor content to the real world consistently across
devices. Large organizations such as Google and Niantic are 3D scanning outdoor
public spaces in order to build their own Visual Positioning Systems (VPS).
These centralized VPS solutions fail to meet the needs of many future AR
applications -- they do not cover private indoor spaces because of privacy
concerns, regulations, and the labor bottleneck of updating and maintaining 3D
scans. In this paper, we present OpenFLAME, a federated VPS backend that allows
independent organizations to 3D scan and maintain a separate VPS service for
their own spaces. This enables access control of indoor 3D scans, distributed
maintenance of the VPS backend, and encourages larger coverage. Sharding of VPS
services introduces several unique challenges -- coherency of localization
results across spaces, quality control of VPS services, selection of the right
VPS service for a location, and many others. We introduce the concept of
federated image-based localization and provide reference solutions for managing
and merging data across maps without sharing private data.

</details>


### [504] [A Modular Conditional Diffusion Framework for Image Reconstruction](https://arxiv.org/abs/2411.05993)
*Magauiya Zhussip,Iaroslav Koshelev,Stamatis Lefkimmiatis*

Main category: cs.CV

TL;DR: 提出模块化扩散概率图像恢复框架DP - IR，解决现有扩散概率模型应用难题，在多个任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有扩散概率模型任务特定性强、训练计算成本高，阻碍其广泛应用，旨在解决这些问题以推动其在实际图像恢复应用中的成功采用。

Method: 提出模块化扩散概率图像恢复框架DP - IR，结合预训练图像恢复网络和生成式扩散概率模型，仅需额外训练小模块；采用特定采样策略减少神经网络评估次数，还可结合现有加速技术。

Result: 在四个基准测试的多个图像恢复任务中，该方法在感知质量上优于现有方法，在保真度指标上也有有竞争力的表现。

Conclusion: 所提DP - IR框架有效解决了现有扩散概率模型的问题，能成功应用于实际图像恢复相关应用。

Abstract: Diffusion Probabilistic Models (DPMs) have been recently utilized to deal
with various blind image restoration (IR) tasks, where they have demonstrated
outstanding performance in terms of perceptual quality. However, the
task-specific nature of existing solutions and the excessive computational
costs related to their training, make such models impractical and challenging
to use for different IR tasks than those that were initially trained for. This
hinders their wider adoption, especially by those who lack access to powerful
computational resources and vast amount of training data. In this work we aim
to address the above issues and enable the successful adoption of DPMs in
practical IR-related applications. Towards this goal, we propose a modular
diffusion probabilistic IR framework (DP-IR), which allows us to combine the
performance benefits of existing pre-trained state-of-the-art IR networks and
generative DPMs, while it requires only the additional training of a relatively
small module (0.7M params) related to the particular IR task of interest.
Moreover, the architecture of the proposed framework allows for a sampling
strategy that leads to at least four times reduction of neural function
evaluations without suffering any performance loss, while it can also be
combined with existing acceleration techniques such as DDIM. We evaluate our
model on four benchmarks for the tasks of burst JDD-SR, dynamic scene
deblurring, and super-resolution. Our method outperforms existing approaches in
terms of perceptual quality while it retains a competitive performance with
respect to fidelity metrics.

</details>


### [505] [Textured Gaussians for Enhanced 3D Scene Appearance Modeling](https://arxiv.org/abs/2411.18625)
*Brian Chao,Hung-Yu Tseng,Lorenzo Porzi,Chen Gao,Tuotuo Li,Qinbo Li,Ayush Saraf,Jia-Bin Huang,Johannes Kopf,Gordon Wetzstein,Changil Kim*

Main category: cs.CV

TL;DR: 本文针对3D高斯溅射（3DGS）表达能力有限的问题，提出结合纹理和alpha映射的广义高斯外观表示方法，在多个数据集验证能提升图像质量。


<details>
  <summary>Details</summary>
Motivation: 3DGS中同一高斯覆盖的像素颜色相同且单个高斯能表示的几何细节有限，限制了单个高斯基元的表达能力。

Method: 从传统图形学的纹理和alpha映射获取灵感，提出新的广义高斯外观表示，为每个高斯添加A、RGB或RGBA纹理映射。

Result: 使用仅alpha纹理映射可大幅提高高斯的表达能力，添加RGB纹理映射能实现最高表达能力。

Conclusion: 在多种数据集上验证，使用相似或更少数量的高斯时，所提方法相比现有方法提升了图像质量。

Abstract: 3D Gaussian Splatting (3DGS) has recently emerged as a state-of-the-art 3D
reconstruction and rendering technique due to its high-quality results and fast
training and rendering time. However, pixels covered by the same Gaussian are
always shaded in the same color up to a Gaussian falloff scaling factor.
Furthermore, the finest geometric detail any individual Gaussian can represent
is a simple ellipsoid. These properties of 3DGS greatly limit the expressivity
of individual Gaussian primitives. To address these issues, we draw inspiration
from texture and alpha mapping in traditional graphics and integrate it with
3DGS. Specifically, we propose a new generalized Gaussian appearance
representation that augments each Gaussian with alpha~(A), RGB, or RGBA texture
maps to model spatially varying color and opacity across the extent of each
Gaussian. As such, each Gaussian can represent a richer set of texture patterns
and geometric structures, instead of just a single color and ellipsoid as in
naive Gaussian Splatting. Surprisingly, we found that the expressivity of
Gaussians can be greatly improved by using alpha-only texture maps, and further
augmenting Gaussians with RGB texture maps achieves the highest expressivity.
We validate our method on a wide variety of standard benchmark datasets and our
own custom captures at both the object and scene levels. We demonstrate image
quality improvements over existing methods while using a similar or lower
number of Gaussians.

</details>


### [506] [Convolutional Neural Nets vs Vision Transformers: A SpaceNet Case Study with Balanced vs Imbalanced Regimes](https://arxiv.org/abs/2510.03297)
*Akshar Gothi*

Main category: cs.CV

TL;DR: 对EfficientNet - B0和ViT - Base在SpaceNet上两种标签分布制度下进行对比，报告多项指标，发现平衡数据集会缩小架构差距，CNN仍有效率优势并发布相关数据支持复现。


<details>
  <summary>Details</summary>
Motivation: 对比卷积神经网络EfficientNet - B0和视觉Transformer（ViT - Base）在SpaceNet数据集不同标签分布制度下的性能。

Method: 在两种标签分布制度（自然不平衡五分类和平衡重采样）下，进行匹配预处理、轻量级增强，在单个NVIDIA P100上训练40个epoch，报告多项指标。

Result: 在不平衡分割中，EfficientNet - B0测试准确率达93%，宏F1强且延迟低；ViT - Base准确率同样93%，但参数和运行时间大。在平衡分割中两者表现都强，EfficientNet - B0达99%，ViT - Base仍有竞争力。

Conclusion: 平衡数据集会缩小架构差距，卷积神经网络仍保持效率优势，同时发布相关数据支持复现。

Abstract: We present a controlled comparison of a convolutional neural network
(EfficientNet-B0) and a Vision Transformer (ViT-Base) on SpaceNet under two
label-distribution regimes: a naturally imbalanced five-class split and a
balanced-resampled split with 700 images per class (70:20:10 train/val/test).
With matched preprocessing (224x224, ImageNet normalization), lightweight
augmentations, and a 40-epoch budget on a single NVIDIA P100, we report
accuracy, macro-F1, balanced accuracy, per-class recall, and deployment metrics
(model size and latency). On the imbalanced split, EfficientNet-B0 reaches 93%
test accuracy with strong macro-F1 and lower latency; ViT-Base is competitive
at 93% with a larger parameter count and runtime. On the balanced split, both
models are strong; EfficientNet-B0 reaches 99% while ViT-Base remains
competitive, indicating that balancing narrows architecture gaps while CNNs
retain an efficiency edge. We release manifests, logs, and per-image
predictions to support reproducibility.

</details>


### [507] [A Comprehensive Review on Artificial Intelligence Empowered Solutions for Enhancing Pedestrian and Cyclist Safety](https://arxiv.org/abs/2510.03314)
*Shucheng Zhang,Yan Shi,Bingzhang Wang,Yuang Zhang,Muhammad Monjurul Karim,Kehua Chen,Chenxi Liu,Mehrdad Nasri,Yinhai Wang*

Main category: cs.CV

TL;DR: 本文对基于摄像头的AI传感系统保障VRU安全的研究进展进行综述，分析核心任务并指出开放挑战，为下一代传感系统发展提供参考。


<details>
  <summary>Details</summary>
Motivation: 传统基础设施措施保障VRU安全不足，现有AI应用调查多聚焦检测，缺乏对其他视觉任务的覆盖。

Method: 系统审查检测与分类、跟踪与重识别、轨迹预测、意图识别与预测四项核心任务。

Result: 指出数据、模型和部署方面的四大开放挑战。

Conclusion: 将视觉AI进展与实际应用结合，为下一代传感系统发展提供基础参考以提升VRU安全。

Abstract: Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and
cyclists, remains a critical global challenge, as conventional
infrastructure-based measures often prove inadequate in dynamic urban
environments. Recent advances in artificial intelligence (AI), particularly in
visual perception and reasoning, open new opportunities for proactive and
context-aware VRU protection. However, existing surveys on AI applications for
VRUs predominantly focus on detection, offering limited coverage of other
vision-based tasks that are essential for comprehensive VRU understanding and
protection. This paper presents a state-of-the-art review of recent progress in
camera-based AI sensing systems for VRU safety, with an emphasis on
developments from the past five years and emerging research trends. We
systematically examine four core tasks, namely detection and classification,
tracking and reidentification, trajectory prediction, and intent recognition
and prediction, which together form the backbone of AI-empowered proactive
solutions for VRU protection in intelligent transportation systems. To guide
future research, we highlight four major open challenges from the perspectives
of data, model, and deployment. By linking advances in visual AI with practical
considerations for real-world implementation, this survey aims to provide a
foundational reference for the development of next-generation sensing systems
to enhance VRU safety.

</details>


### [508] [The View From Space: Navigating Instrumentation Differences with EOFMs](https://arxiv.org/abs/2510.03316)
*Ryan P. Demilt,Nicholas LaHaye,Karis Tenneson*

Main category: cs.CV

TL;DR: 研究指出地球观测基础模型（EOFMs）表征空间对传感器架构高度敏感，为其设计提供新视角。


<details>
  <summary>Details</summary>
Motivation: 现有多数 EOFM 模型仅在单一数据模态上训练，且不清楚不同传感器架构对其内部表征的影响。

Method: 文中未明确提及具体研究方法。

Result: 发现 EOFMs 的表征空间对传感器架构高度敏感。

Conclusion: 理解这种差异能让模型开发者、用户及相关社区认识到当前 EOFM 设计的陷阱，并指导未来发展。

Abstract: Earth Observation Foundation Models (EOFMs) have exploded in prevalence as
tools for processing the massive volumes of remotely sensed and other earth
observation data, and for delivering impact on the many essential earth
monitoring tasks. An emerging trend posits using the outputs of pre-trained
models as 'embeddings' which summarize high dimensional data to be used for
generic tasks such as similarity search and content-specific queries. However,
most EOFM models are trained only on single modalities of data and then applied
or benchmarked by matching bands across different modalities. It is not clear
from existing work what impact diverse sensor architectures have on the
internal representations of the present suite of EOFMs. We show in this work
that the representation space of EOFMs is highly sensitive to sensor
architecture and that understanding this difference gives a vital perspective
on the pitfalls of current EOFM design and signals for how to move forward as
model developers, users, and a community guided by robust remote-sensing
science.

</details>


### [509] [Photorealistic Inpainting for Perturbation-based Explanations in Ecological Monitoring](https://arxiv.org/abs/2510.03317)
*Günel Aghakishiyeva,Jiayi Zhou,Saagar Arya,James David Poling,Holly R. Houliston,Jamie N. Womble,David W. Johnston,Brinnae Bent*

Main category: cs.CV

TL;DR: 提出基于修复引导的扰动解释技术用于生态监测视觉模型，避免传统扰动问题，支持AI在生态领域可靠部署。


<details>
  <summary>Details</summary>
Motivation: 解决视觉模型在生态监测中预测不透明，限制信任和应用的问题。

Method: 提出基于修复引导的扰动解释技术，对YOLOv9探测器微调，用Segment - Anything - Model细化掩码进行对象移除/替换和背景替换干预，通过重评分和专家评审评估。

Result: 解释定位诊断结构，避免传统扰动的删除伪影，获得与领域相关的见解。

Conclusion: 该方法支持专家验证，使AI在生态领域部署更可靠。

Abstract: Ecological monitoring is increasingly automated by vision models, yet opaque
predictions limit trust and field adoption. We present an inpainting-guided,
perturbation-based explanation technique that produces photorealistic,
mask-localized edits that preserve scene context. Unlike masking or blurring,
these edits stay in-distribution and reveal which fine-grained morphological
cues drive predictions in tasks such as species recognition and trait
attribution. We demonstrate the approach on a YOLOv9 detector fine-tuned for
harbor seal detection in Glacier Bay drone imagery, using
Segment-Anything-Model-refined masks to support two interventions: (i) object
removal/replacement (e.g., replacing seals with plausible ice/water or boats)
and (ii) background replacement with original animals composited onto new
scenes. Explanations are assessed by re-scoring perturbed images (flip rate,
confidence drop) and by expert review for ecological plausibility and
interpretability. The resulting explanations localize diagnostic structures,
avoid deletion artifacts common to traditional perturbations, and yield
domain-relevant insights that support expert validation and more trustworthy
deployment of AI in ecology.

</details>


### [510] [Inference-Time Search using Side Information for Diffusion-based Image Reconstruction](https://arxiv.org/abs/2510.03352)
*Mahdi Farahbakhsh,Vishnu Teja Kunde,Dileep Kalathil,Krishna Narayanan,Jean-Francois Chamberland*

Main category: cs.CV

TL;DR: 本文提出利用侧信息的推理时间搜索算法用于扩散模型图像重建，实验表明该算法能提升重建性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型解决逆问题时通常忽略可显著提升重建质量的侧信息，尤其在严重病态设置中。

Method: 提出一种新颖的推理时间搜索算法，利用侧信息引导采样过程，平衡探索与利用。

Result: 该方法能无缝集成到现有基于扩散的图像重建管道，在多项逆问题实验中，持续提升重建算法的定性和定量性能，优于其他基线方法。

Conclusion: 所提方法为扩散模型图像重建提供了更准确可靠的解决方案。

Abstract: Diffusion models have emerged as powerful priors for solving inverse
problems. However, existing approaches typically overlook side information that
could significantly improve reconstruction quality, especially in severely
ill-posed settings. In this work, we propose a novel inference-time search
algorithm that guides the sampling process using the side information in a
manner that balances exploration and exploitation. This enables more accurate
and reliable reconstructions, providing an alternative to the gradient-based
guidance that is prone to reward-hacking artifacts. Our approach can be
seamlessly integrated into a wide range of existing diffusion-based image
reconstruction pipelines. Through extensive experiments on a number of inverse
problems, such as box inpainting, super-resolution, and various deblurring
tasks including motion, Gaussian, nonlinear, and blind deblurring, we show that
our approach consistently improves the qualitative and quantitative performance
of diffusion-based image reconstruction algorithms. We also show the superior
performance of our approach with respect to other baselines, including reward
gradient-based guidance algorithms. The code is available at
\href{https://github.com/mhdfb/sideinfo-search-reconstruction}{this
repository}.

</details>


### [511] [Provenance Networks: End-to-End Exemplar-Based Explainability](https://arxiv.org/abs/2510.03361)
*Ali Kayyam,Anusha Madan Gopal,M. Anthony Lewis*

Main category: cs.CV

TL;DR: 介绍了溯源网络，一种能提供端到端、基于训练数据可解释性的新型神经模型，虽有计算成本且适用中等规模数据集，但能解决深度学习关键挑战。


<details>
  <summary>Details</summary>
Motivation: 解决现代深度学习中模型不透明、幻觉以及数据贡献归因等问题，提高神经网络的透明度、鲁棒性和可信度。

Method: 设计溯源网络，学习将每个预测直接与支持的训练示例关联，像学习型KNN，联合优化主要任务和可解释性目标。

Result: 能系统研究记忆与泛化的权衡，验证输入是否在训练集，检测错误标签或异常数据点，增强对输入扰动的恢复力，识别相似输入。

Conclusion: 溯源网络是对现有可解释性技术的补充，虽有计算成本和规模限制，但能提供传统深度网络无法提供的模型行为见解。

Abstract: We introduce provenance networks, a novel class of neural models designed to
provide end-to-end, training-data-driven explainability. Unlike conventional
post-hoc methods, provenance networks learn to link each prediction directly to
its supporting training examples as part of the model's normal operation,
embedding interpretability into the architecture itself. Conceptually, the
model operates similarly to a learned KNN, where each output is justified by
concrete exemplars weighted by relevance in the feature space. This approach
facilitates systematic investigations of the trade-off between memorization and
generalization, enables verification of whether a given input was included in
the training set, aids in the detection of mislabeled or anomalous data points,
enhances resilience to input perturbations, and supports the identification of
similar inputs contributing to the generation of a new data point. By jointly
optimizing the primary task and the explainability objective, provenance
networks offer insights into model behavior that traditional deep networks
cannot provide. While the model introduces additional computational cost and
currently scales to moderately sized datasets, it provides a complementary
approach to existing explainability techniques. In particular, it addresses
critical challenges in modern deep learning, including model opaqueness,
hallucination, and the assignment of credit to data contributors, thereby
improving transparency, robustness, and trustworthiness in neural models.

</details>


### [512] [Unified Unsupervised Anomaly Detection via Matching Cost Filtering](https://arxiv.org/abs/2510.03363)
*Zhe Zhang,Mingxiu Cai,Gaochang Wu,Jing Zhang,Lingqiao Liu,Dacheng Tao,Tianyou Chai,Xiatian Zhu*

Main category: cs.CV

TL;DR: 本文提出统一无监督异常检测（UAD）视角和通用后处理框架UCF，在多基准测试中提升UAD方法性能。


<details>
  <summary>Details</summary>
Motivation: 现有UAD方法忽视匹配噪声限制检测能力，单模态和多模态UAD研究孤立，缺乏全面理解和知识转移。

Method: 提出统一视角下的Unified Cost Filtering (UCF) 框架，构建成本体积，用可学习过滤模块和多层注意力引导减轻匹配噪声。

Result: 在22个基准测试中，UCF增强多种UAD方法，在单模态和多模态场景均达新的最优结果。

Conclusion: UCF框架有效，能提升UAD方法性能，代码和模型将公开。

Abstract: Unsupervised anomaly detection (UAD) aims to identify image- and pixel-level
anomalies using only normal training data, with wide applications such as
industrial inspection and medical analysis, where anomalies are scarce due to
privacy concerns and cold-start constraints. Existing methods, whether
reconstruction-based (restoring normal counterparts) or embedding-based
(pretrained representations), fundamentally conduct image- or feature-level
matching to generate anomaly maps. Nonetheless, matching noise has been largely
overlooked, limiting their detection ability. Beyond earlier focus on unimodal
RGB-based UAD, recent advances expand to multimodal scenarios, e.g., RGB--3D
and RGB--Text, enabled by point cloud sensing and vision--language models.
Despite shared challenges, these lines remain largely isolated, hindering a
comprehensive understanding and knowledge transfer. In this paper, we advocate
unified UAD for both unimodal and multimodal settings in the matching
perspective. Under this insight, we present Unified Cost Filtering (UCF), a
generic post-hoc refinement framework for refining anomaly cost volume of any
UAD model. The cost volume is constructed by matching a test sample against
normal samples from the same or different modalities, followed by a learnable
filtering module with multi-layer attention guidance from the test sample,
mitigating matching noise and highlighting subtle anomalies. Comprehensive
experiments on 22 diverse benchmarks demonstrate the efficacy of UCF in
enhancing a variety of UAD methods, consistently achieving new state-of-the-art
results in both unimodal (RGB) and multimodal (RGB--3D, RGB--Text) UAD
scenarios. Code and models will be released at
https://github.com/ZHE-SAPI/CostFilter-AD.

</details>


### [513] [Spatial-ViLT: Enhancing Visual Spatial Reasoning through Multi-Task Learning](https://arxiv.org/abs/2510.03441)
*Chashi Mahiul Islam,Oteo Mamo,Samuel Jacob Chacko,Xiuwen Liu,Weikuan Yu*

Main category: cs.CV

TL;DR: 提出SpatialViLT增强视觉语言模型处理3D场景和复杂物体配置的空间推理能力，在VSR数据集表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉语言模型在3D场景和复杂物体配置空间推理方面的挑战。

Method: 引入SpatialViLT，通过多任务学习框架集成深度图、3D坐标和边缘图等空间特征，提出SpatialViLT和MaskedSpatialViLT两个变体及SpatialEnsemble组合方法。

Result: 模型在VSR数据集的方向、拓扑和接近关系等空间推理类别中达到了最先进的准确率。

Conclusion: 该工作显著提升了人工智能系统的空间智能，对高级多模态理解和实际应用至关重要。

Abstract: Vision-language models (VLMs) have advanced multimodal reasoning but still
face challenges in spatial reasoning for 3D scenes and complex object
configurations. To address this, we introduce SpatialViLT, an enhanced VLM that
integrates spatial features like depth maps, 3D coordinates, and edge maps
through a multi-task learning framework. This approach enriches multimodal
embeddings with spatial understanding. We propose two variants: SpatialViLT and
MaskedSpatialViLT, focusing on full and masked object regions, respectively.
Additionally, SpatialEnsemble combines both approaches, achieving
state-of-the-art accuracy. Our models excel in spatial reasoning categories
such as directional, topological, and proximity relations, as demonstrated on
the challenging Visual Spatial Reasoning (VSR) dataset. This work represents a
significant step in enhancing the spatial intelligence of AI systems, crucial
for advanced multimodal understanding and real-world applications.

</details>


### [514] [DuPLUS: Dual-Prompt Vision-Language Framework for Universal Medical Image Segmentation and Prognosis](https://arxiv.org/abs/2510.03483)
*Numan Saeed,Tausifa Jan Saleem,Fadillah Maani,Muhammad Ridzuan,Hu Wang,Mohammad Yaqub*

Main category: cs.CV

TL;DR: 提出DuPLUS框架用于多模态医学图像分析，能跨数据集泛化，在多数数据集上表现优，还可集成EHR数据做预后预测，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习医学成像模型缺乏泛化性和预后能力，通用方法存在简单条件化和医学语义理解差的问题。

Method: 引入新颖的视觉 - 语言框架，利用分层语义提示进行细粒度控制，采用独特的双提示机制驱动的分层文本控制架构。

Result: 能跨三种成像模态、十个不同解剖数据集泛化，在8个数据集上优于现有模型；集成EHR数据在头颈癌数据集上CI达0.69。

Conclusion: DuPLUS是医学图像分析的通用且临床相关的解决方案，参数高效微调可快速适应新任务和模态。

Abstract: Deep learning for medical imaging is hampered by task-specific models that
lack generalizability and prognostic capabilities, while existing 'universal'
approaches suffer from simplistic conditioning and poor medical semantic
understanding. To address these limitations, we introduce DuPLUS, a deep
learning framework for efficient multi-modal medical image analysis. DuPLUS
introduces a novel vision-language framework that leverages hierarchical
semantic prompts for fine-grained control over the analysis task, a capability
absent in prior universal models. To enable extensibility to other medical
tasks, it includes a hierarchical, text-controlled architecture driven by a
unique dual-prompt mechanism. For segmentation, DuPLUS is able to generalize
across three imaging modalities, ten different anatomically various medical
datasets, encompassing more than 30 organs and tumor types. It outperforms the
state-of-the-art task specific and universal models on 8 out of 10 datasets. We
demonstrate extensibility of its text-controlled architecture by seamless
integration of electronic health record (EHR) data for prognosis prediction,
and on a head and neck cancer dataset, DuPLUS achieved a Concordance Index (CI)
of 0.69. Parameter-efficient fine-tuning enables rapid adaptation to new tasks
and modalities from varying centers, establishing DuPLUS as a versatile and
clinically relevant solution for medical image analysis. The code for this work
is made available at: https://anonymous.4open.science/r/DuPLUS-6C52

</details>


### [515] [Real-Time Threaded Houbara Detection and Segmentation for Wildlife Conservation using Mobile Platforms](https://arxiv.org/abs/2510.03501)
*Lyes Saad Saoud,Loic Lesobre,Enrico Sorato,Irfan Hussain*

Main category: cs.CV

TL;DR: 提出移动优化的两阶段深度学习框架用于自然环境中动物实时检测与分割，在珍稀鸟类上效果好，代码和数据集公开。


<details>
  <summary>Details</summary>
Motivation: 自然环境中动物实时检测和分割对野生动物保护重要，但因计算资源有限和物种外观隐蔽而具挑战性。

Method: 提出集成线程检测模型（TDM）的两阶段深度学习框架，并行化基于YOLOv10的检测和基于MobileSAM的分割。

Result: 在珍稀鸨鸟上，模型取得了mAP50为0.9627等指标，YOLOv10每帧运行时间43.7ms，还构建了含40000张标注图像的数据集。

Conclusion: 该框架能有效提高实时性能，减少延迟，高效利用资源，可用于自然环境中动物的实时检测和分割。

Abstract: Real-time animal detection and segmentation in natural environments are vital
for wildlife conservation, enabling non-invasive monitoring through remote
camera streams. However, these tasks remain challenging due to limited
computational resources and the cryptic appearance of many species. We propose
a mobile-optimized two-stage deep learning framework that integrates a
Threading Detection Model (TDM) to parallelize YOLOv10-based detection and
MobileSAM-based segmentation. Unlike prior YOLO+SAM pipelines, our approach
improves real-time performance by reducing latency through threading. YOLOv10
handles detection while MobileSAM performs lightweight segmentation, both
executed concurrently for efficient resource use. On the cryptic Houbara
Bustard, a conservation-priority species, our model achieves mAP50 of 0.9627,
mAP75 of 0.7731, mAP95 of 0.7178, and a MobileSAM mIoU of 0.7421. YOLOv10
operates at 43.7 ms per frame, confirming real-time readiness. We introduce a
curated Houbara dataset of 40,000 annotated images to support model training
and evaluation across diverse conditions. The code and dataset used in this
study are publicly available on GitHub at
https://github.com/LyesSaadSaoud/mobile-houbara-detseg. For interactive demos
and additional resources, visit
https://lyessaadsaoud.github.io/LyesSaadSaoud-Threaded-YOLO-SAM-Houbara.

</details>


### [516] [Platonic Transformers: A Solid Choice For Equivariance](https://arxiv.org/abs/2510.03511)
*Mohammad Mohaiminul Islam,Rishabh Anand,David R. Wessels,Friso de Kruiff,Thijs P. Kuipers,Rex Ying,Clara I. Sánchez,Sharvaree Vadgama,Georg Bökman,Erik J. Bekkers*

Main category: cs.CV

TL;DR: 提出Platonic Transformer解决Transformer缺乏几何对称性归纳偏置问题，在多基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer缺乏科学和计算机视觉中常见的几何对称性归纳偏置，现有等变方法牺牲效率和灵活性。

Method: 通过定义相对于柏拉图固体对称群参考系的注意力，引入原则性的权重共享方案，实现连续平移和柏拉图对称性的等变性。

Result: 该注意力形式上等同于动态群卷积，模型能学习自适应几何滤波器，有线性时间卷积变体，在多个基准测试中取得有竞争力的性能。

Conclusion: Platonic Transformer能在不增加成本的情况下利用几何约束，解决了效率与等变性的权衡问题。

Abstract: While widespread, Transformers lack inductive biases for geometric symmetries
common in science and computer vision. Existing equivariant methods often
sacrifice the efficiency and flexibility that make Transformers so effective
through complex, computationally intensive designs. We introduce the Platonic
Transformer to resolve this trade-off. By defining attention relative to
reference frames from the Platonic solid symmetry groups, our method induces a
principled weight-sharing scheme. This enables combined equivariance to
continuous translations and Platonic symmetries, while preserving the exact
architecture and computational cost of a standard Transformer. Furthermore, we
show that this attention is formally equivalent to a dynamic group convolution,
which reveals that the model learns adaptive geometric filters and enables a
highly scalable, linear-time convolutional variant. Across diverse benchmarks
in computer vision (CIFAR-10), 3D point clouds (ScanObjectNN), and molecular
property prediction (QM9, OMol25), the Platonic Transformer achieves
competitive performance by leveraging these geometric constraints at no
additional cost.

</details>


### [517] [Unmasking Puppeteers: Leveraging Biometric Leakage to Disarm Impersonation in AI-based Videoconferencing](https://arxiv.org/abs/2510.03548)
*Danial Samadi Vahdati,Tai Duc Nguyen,Ekta Prashnani,Koki Nagano,David Luebke,Orazio Gallo,Matthew Stamm*

Main category: cs.CV

TL;DR: 本文提出针对AI驱动的会说话头像视频会议系统身份劫持问题的生物特征泄漏防御方法，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: AI会说话头像视频会议系统的潜在可操控性会导致攻击者实时劫持受害者形象，且现有检测方法失效，需要解决该安全问题。

Method: 引入基于姿态条件的大间隔对比编码器，从传输的潜在空间中分离出持久的身份线索，同时消除瞬时的姿态和表情，通过简单余弦测试检测非法身份交换。

Result: 在多个会说话头像生成模型上的实验表明，该方法始终优于现有的操控防御方法，能实时运行，并在分布外场景中具有强泛化能力。

Conclusion: 所提出的生物特征泄漏防御方法是有效的，能解决会说话头像视频会议系统的安全问题。

Abstract: AI-based talking-head videoconferencing systems reduce bandwidth by sending a
compact pose-expression latent and re-synthesizing RGB at the receiver, but
this latent can be puppeteered, letting an attacker hijack a victim's likeness
in real time. Because every frame is synthetic, deepfake and synthetic video
detectors fail outright. To address this security problem, we exploit a key
observation: the pose-expression latent inherently contains biometric
information of the driving identity. Therefore, we introduce the first
biometric leakage defense without ever looking at the reconstructed RGB video:
a pose-conditioned, large-margin contrastive encoder that isolates persistent
identity cues inside the transmitted latent while cancelling transient pose and
expression. A simple cosine test on this disentangled embedding flags illicit
identity swaps as the video is rendered. Our experiments on multiple
talking-head generation models show that our method consistently outperforms
existing puppeteering defenses, operates in real-time, and shows strong
generalization to out-of-distribution scenarios.

</details>


### [518] [GAS-MIL: Group-Aggregative Selection Multi-Instance Learning for Ensemble of Foundation Models in Digital Pathology Image Analysis](https://arxiv.org/abs/2510.03555)
*Peiran Quan,Zifan Gu,Zhuo Zhao,Qin Zhou,Donghan M. Yang,Ruichen Rong,Yang Xie,Guanghua Xiao*

Main category: cs.CV

TL;DR: 本文介绍GAS - MIL框架，可无缝集成多个基础模型特征，在三个癌症数据集分类任务中表现良好，为病理模型部署提供便利。


<details>
  <summary>Details</summary>
Motivation: 适配和基准测试单个基础模型用于特定诊断任务耗时且耗资源，需解决该问题。

Method: 引入Group - Aggregative Selection Multi - Instance Learning (GAS - MIL) 框架，可集成多基础模型特征，无需手动特征选择和大量特定任务微调。

Result: 在三个癌症数据集的分类任务中，GAS - MIL相对单个基础模型和已有的多实例学习方法表现更优或相当。

Conclusion: GAS - MIL可实现异构基础模型的高效集成，简化病理模型部署，为未来多模态和精准肿瘤学应用提供可扩展基础。

Abstract: Foundation models (FMs) have transformed computational pathology by providing
powerful, general-purpose feature extractors. However, adapting and
benchmarking individual FMs for specific diagnostic tasks is often
time-consuming and resource-intensive, especially given their scale and
diversity. To address this challenge, we introduce Group-Aggregative Selection
Multi-Instance Learning (GAS-MIL), a flexible ensemble framework that
seamlessly integrates features from multiple FMs, preserving their
complementary strengths without requiring manual feature selection or extensive
task-specific fine-tuning. Across classification tasks in three cancer
datasets-prostate (PANDA), ovarian (UBC-OCEAN), and breast (TCGA-BrCa)-GAS-MIL
consistently achieves superior or on-par performance relative to individual FMs
and established MIL methods, demonstrating its robustness and generalizability.
By enabling efficient integration of heterogeneous FMs, GAS-MIL streamlines
model deployment for pathology and provides a scalable foundation for future
multimodal and precision oncology applications.

</details>


### [519] [Evaluating OCR performance on food packaging labels in South Africa](https://arxiv.org/abs/2510.03570)
*Mayimunah Nagayi,Alice Khan,Tamryn Frank,Rina Swart,Clement Nyirenda*

Main category: cs.CV

TL;DR: 评估四个开源OCR系统在食品包装图像上的表现，为包装OCR提供基准和改进方向。


<details>
  <summary>Details</summary>
Motivation: 准确的包装OCR对合规和营养监测重要，但因多语言文本、复杂布局等存在挑战，需评估现有OCR系统。

Method: 用四个模型处理231个产品（1628张图像）评估速度和覆盖率，用113张图像（60个产品）的子集评估准确性，使用多种指标。

Result: Tesseract的CER最低、BLEU最高；EasyOCR在准确性和多语言支持上平衡好；PaddleOCR覆盖率高但速度慢；TrOCR结果最弱。

Conclusion: 研究为包装OCR提供特定基准，确立基线，指明布局感知方法和文本定位的改进方向。

Abstract: This study evaluates four open-source Optical Character Recognition (OCR)
systems which are Tesseract, EasyOCR, PaddleOCR, and TrOCR on real world food
packaging images. The aim is to assess their ability to extract ingredient
lists and nutrition facts panels. Accurate OCR for packaging is important for
compliance and nutrition monitoring but is challenging due to multilingual
text, dense layouts, varied fonts, glare, and curved surfaces. A dataset of 231
products (1,628 images) was processed by all four models to assess speed and
coverage, and a ground truth subset of 113 images (60 products) was created for
accuracy evaluation. Metrics include Character Error Rate (CER), Word Error
Rate (WER), BLEU, ROUGE-L, F1, coverage, and execution time. On the ground
truth subset, Tesseract achieved the lowest CER (0.912) and the highest BLEU
(0.245). EasyOCR provided a good balance between accuracy and multilingual
support. PaddleOCR achieved near complete coverage but was slower because it
ran on CPU only due to GPU incompatibility, and TrOCR produced the weakest
results despite GPU acceleration. These results provide a packaging-specific
benchmark, establish a baseline, and highlight directions for layout-aware
methods and text localization.

</details>


### [520] [A Hybrid Co-Finetuning Approach for Visual Bug Detection in Video Games](https://arxiv.org/abs/2510.03591)
*Faliu Yi,Sherif Abdelfattah,Wei Huang,Adrian Brown*

Main category: cs.CV

TL;DR: 本文提出混合Co - FineTuning (CFT)方法用于游戏视觉漏洞检测，减少对目标游戏标注数据依赖，实验显示其性能优于传统基线。


<details>
  <summary>Details</summary>
Motivation: 手动识别游戏视觉漏洞资源密集且成本高，有监督模型依赖大量标注数据，而漏洞出现频率低，获取标注数据困难。

Method: 提出混合Co - FineTuning (CFT)方法，整合标注和未标注数据，利用目标游戏和不同共域游戏的标注样本及未标注数据增强特征表示学习。

Result: 所开发框架扩展性和适应性增强，在多个游戏环境中性能优于传统基线，使用50%目标游戏标注数据训练时仍有竞争力。

Conclusion: 提出的CFT方法对游戏视觉漏洞检测具有鲁棒性，减少对目标游戏标注数据的依赖，能在多游戏中高效检测视觉漏洞。

Abstract: Manual identification of visual bugs in video games is a resource-intensive
and costly process, often demanding specialized domain knowledge. While
supervised visual bug detection models offer a promising solution, their
reliance on extensive labeled datasets presents a significant challenge due to
the infrequent occurrence of such bugs. To overcome this limitation, we propose
a hybrid Co-FineTuning (CFT) method that effectively integrates both labeled
and unlabeled data. Our approach leverages labeled samples from the target game
and diverse co-domain games, additionally incorporating unlabeled data to
enhance feature representation learning. This strategy maximizes the utility of
all available data, substantially reducing the dependency on labeled examples
from the specific target game. The developed framework demonstrates enhanced
scalability and adaptability, facilitating efficient visual bug detection
across various game titles. Our experimental results show the robustness of the
proposed method for game visual bug detection, exhibiting superior performance
compared to conventional baselines across multiple gaming environments.
Furthermore, CFT maintains competitive performance even when trained with only
50% of the labeled data from the target game.

</details>


### [521] [MonitorVLM:A Vision Language Framework for Safety Violation Detection in Mining Operations](https://arxiv.org/abs/2510.03666)
*Jiang Wu,Sichao Wu,Yinsong Ma,Guangyuan Yu,Haoyuan Xu,Lifang Zheng,Jingliang Duan*

Main category: cs.CV

TL;DR: 本文提出MonitorVLM框架用于监控视频中安全违规，有三项创新，实验表现优于基线模型，还集成轻量级界面，凸显多模态大模型在职业安全监控潜力。


<details>
  <summary>Details</summary>
Motivation: 传统人工检查劳动密集、易出错，难以满足大规模动态环境需求，需智能自动化安全监控。

Method: 提出MonitorVLM框架，包括构建特定领域违规数据集、引入子句过滤器模块和行为放大器模块。

Result: MonitorVLM显著优于基线视觉 - 语言模型，精度、召回率和F1分数有大幅提升，且集成轻量级界面可实现自动违规报告。

Conclusion: 多模态大模型在采矿及其他领域职业安全监控有提升潜力。

Abstract: Industrial accidents, particularly in high-risk domains such as surface and
underground mining, are frequently caused by unsafe worker behaviors.
Traditional manual inspection remains labor-intensive, error-prone, and
insufficient for large-scale, dynamic environments, highlighting the urgent
need for intelligent and automated safety monitoring. In this paper, we present
MonitorVLM, a novel vision--language framework designed to detect safety
violations directly from surveillance video streams. MonitorVLM introduces
three key innovations: (1) a domain-specific violation dataset comprising 9,000
vision--question--answer (VQA) samples across 40 high-frequency mining
regulations, enriched with augmentation and auxiliary detection cues; (2) a
clause filter (CF) module that dynamically selects the Top-$K$ most relevant
clauses, reducing inference latency by 13.56\% while maintaining accuracy; and
(3) a behavior magnifier (BM) module that enhances worker regions to improve
fine-grained action recognition, yielding additional gains of 3.45% in
precision and 8.62% in recall. Experimental results demonstrate that MonitorVLM
significantly outperforms baseline vision--language models, achieving
improvements of 22.01% in precision, 34.22\% in recall, and 28.37% in F1 score
over the 72B unfine-tuned baseline. A lightweight web-based interface further
integrates MonitorVLM into practical workflows, enabling automatic violation
reporting with video timestamping. This study highlights the potential of
multimodal large models to enhance occupational safety monitoring in mining and
beyond.

</details>


### [522] [Referring Expression Comprehension for Small Objects](https://arxiv.org/abs/2510.03701)
*Kanoko Goto,Takumi Hirose,Mahiro Ukai,Shuhei Kurita,Nakamasa Inoue*

Main category: cs.CV

TL;DR: 针对REC任务中小目标定位难题，提出SOREC数据集和PIZA模块，实验表明PIZA应用于GroundingDINO在SOREC数据集上精度显著提升，相关资源公开。


<details>
  <summary>Details</summary>
Motivation: 现有REC任务在定位极小目标上存在挑战，但该任务在自动驾驶等实际应用中很重要，为解决此问题开展研究。

Method: 提出SOREC数据集，包含100,000对驾驶场景小目标指代表达和对应边界框；提出PIZA模块，用于参数高效微调，使模型能逐步聚焦并定位小目标。

Result: 将PIZA应用于GroundingDINO，在SOREC数据集上准确率显著提高。

Conclusion: 提出的数据集和方法有助于解决REC任务中小目标定位难题，且相关资源公开方便后续研究。

Abstract: Referring expression comprehension (REC) aims to localize the target object
described by a natural language expression. Recent advances in vision-language
learning have led to significant performance improvements in REC tasks.
However, localizing extremely small objects remains a considerable challenge
despite its importance in real-world applications such as autonomous driving.
To address this issue, we introduce a novel dataset and method for REC
targeting small objects. First, we present the small object REC (SOREC)
dataset, which consists of 100,000 pairs of referring expressions and
corresponding bounding boxes for small objects in driving scenarios. Second, we
propose the progressive-iterative zooming adapter (PIZA), an adapter module for
parameter-efficient fine-tuning that enables models to progressively zoom in
and localize small objects. In a series of experiments, we apply PIZA to
GroundingDINO and demonstrate a significant improvement in accuracy on the
SOREC dataset. Our dataset, codes and pre-trained models are publicly available
on the project page.

</details>


### [523] [Artery-Vein Segmentation from Fundus Images using Deep Learning](https://arxiv.org/abs/2510.03717)
*Sharan SK,Subin Sahayam,Umarani Jayaraman,Lakshmi Priya A*

Main category: cs.CV

TL;DR: 提出基于注意力机制的Attention - WNet用于视网膜血管动静脉分割，在公开数据集上表现优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 视网膜血管动静脉分割是视网膜血管分析的前提，能为识别和诊断各种视网膜眼病提供潜在见解和生物标志物，还可帮助识别血管疾病高风险患者。

Method: 将注意力机制融入WNet深度学习模型，提出Attention - WNet模型。

Result: 在HRF和DRIVE等公开数据集上测试，所提方法优于文献中的其他先进模型。

Conclusion: 基于注意力机制的Attention - WNet模型在视网膜血管动静脉分割任务中表现良好，是一种有效的方法。

Abstract: Segmenting of clinically important retinal blood vessels into arteries and
veins is a prerequisite for retinal vessel analysis. Such analysis can provide
potential insights and bio-markers for identifying and diagnosing various
retinal eye diseases. Alteration in the regularity and width of the retinal
blood vessels can act as an indicator of the health of the vasculature system
all over the body. It can help identify patients at high risk of developing
vasculature diseases like stroke and myocardial infarction. Over the years,
various Deep Learning architectures have been proposed to perform retinal
vessel segmentation. Recently, attention mechanisms have been increasingly used
in image segmentation tasks. The work proposes a new Deep Learning approach for
artery-vein segmentation. The new approach is based on the Attention mechanism
that is incorporated into the WNet Deep Learning model, and we call the model
as Attention-WNet. The proposed approach has been tested on publicly available
datasets such as HRF and DRIVE datasets. The proposed approach has outperformed
other state-of-art models available in the literature.

</details>


### [524] [Adaptively Sampling-Reusing-Mixing Decomposed Gradients to Speed Up Sharpness Aware Minimization](https://arxiv.org/abs/2510.03763)
*Jiaxin Deng,Junbiao Pang*

Main category: cs.CV

TL;DR: 提出ARSAM方法加速Sharpness - Aware Minimization (SAM)，实验显示ARSAM在不牺牲性能下可比SAM提速约40%，有广泛实用性。


<details>
  <summary>Details</summary>
Motivation: SAM提高模型泛化能力但计算成本翻倍，需降低其计算成本。

Method: 将SAM的梯度分解为SGD梯度和PSF，提出ARSAM方法重用PSF并及时更新PSF以保持模型泛化能力。

Result: ARSAM在多种网络架构上达到与SAM相当的准确率，在CIFAR - 10/100上可比SAM提速约40%，能加速各种挑战性任务的优化。

Conclusion: ARSAM在不牺牲性能的情况下显著加速SAM，具有广泛的实际应用价值。

Abstract: Sharpness-Aware Minimization (SAM) improves model generalization but doubles
the computational cost of Stochastic Gradient Descent (SGD) by requiring twice
the gradient calculations per optimization step. To mitigate this, we propose
Adaptively sampling-Reusing-mixing decomposed gradients to significantly
accelerate SAM (ARSAM). Concretely, we firstly discover that SAM's gradient can
be decomposed into the SGD gradient and the Projection of the Second-order
gradient onto the First-order gradient (PSF). Furthermore, we observe that the
SGD gradient and PSF dynamically evolve during training, emphasizing the
growing role of the PSF to achieve a flat minima. Therefore, ARSAM is proposed
to the reused PSF and the timely updated PSF still maintain the model's
generalization ability. Extensive experiments show that ARSAM achieves
state-of-the-art accuracies comparable to SAM across diverse network
architectures. On CIFAR-10/100, ARSAM is comparable to SAM while providing a
speedup of about 40\%. Moreover, ARSAM accelerates optimization for the various
challenge tasks (\textit{e.g.}, human pose estimation, and model quantization)
without sacrificing performance, demonstrating its broad practicality.% The
code is publicly accessible at: https://github.com/ajiaaa/ARSAM.

</details>


### [525] [PoseGaze-AHP: A Knowledge-Based 3D Dataset for AI-Driven Ocular and Postural Diagnosis](https://arxiv.org/abs/2510.03873)
*Saja Al-Dabet,Sherzod Turaev,Nazar Zaki,Arif O. Khan,Luai Eldweik*

Main category: cs.CV

TL;DR: 引入用于眼部诱发异常头位（AHP）评估的3D数据集PoseGaze - AHP，介绍构建方法及准确性，称其是首个公开的用于AI诊断的资源。


<details>
  <summary>Details</summary>
Motivation: 现有数据集分别关注头位和眼动，限制了AHP综合诊断方法和AI进步，需新数据集。

Method: 用Claude 3.5 Sonnet模型结合多种提示策略从医学文献提取结构化临床数据，用Neural Head Avatar框架处理数据成3D表示。

Result: 数据集含7920张图像，覆盖多种眼部状况，提取方法准确率91.92%。

Conclusion: PoseGaze - AHP是首个公开的AI驱动眼部诱发AHP诊断资源，支持开发准确合规诊断工具。

Abstract: Diagnosing ocular-induced abnormal head posture (AHP) requires a
comprehensive analysis of both head pose and ocular movements. However,
existing datasets focus on these aspects separately, limiting the development
of integrated diagnostic approaches and restricting AI-driven advancements in
AHP analysis. To address this gap, we introduce PoseGaze-AHP, a novel 3D
dataset that synchronously captures head pose and gaze movement information for
ocular-induced AHP assessment. Structured clinical data were extracted from
medical literature using large language models (LLMs) through an iterative
process with the Claude 3.5 Sonnet model, combining stepwise, hierarchical, and
complex prompting strategies. The extracted records were systematically imputed
and transformed into 3D representations using the Neural Head Avatar (NHA)
framework. The dataset includes 7,920 images generated from two head textures,
covering a broad spectrum of ocular conditions. The extraction method achieved
an overall accuracy of 91.92%, demonstrating its reliability for clinical
dataset construction. PoseGaze-AHP is the first publicly available resource
tailored for AI-driven ocular-induced AHP diagnosis, supporting the development
of accurate and privacy-compliant diagnostic tools.

</details>


### [526] [Multi-Modal Oral Cancer Detection Using Weighted Ensemble Convolutional Neural Networks](https://arxiv.org/abs/2510.03878)
*Ajo Babu George,Sreehari J R Ajo Babu George,Sreehari J R Ajo Babu George,Sreehari J R*

Main category: cs.CV

TL;DR: 本文开发多模态深度学习框架用于口腔鳞状细胞癌（OSCC）早期检测，在多模态验证集上有较好表现，可辅助临床决策。


<details>
  <summary>Details</summary>
Motivation: OSCC 晚期诊断导致全球死亡率高，需提高其早期检测率。

Method: 回顾性研究，用公开数据集训练 DenseNet - 121 CNN，应用数据增强和特定模态预处理，用验证加权集成策略融合预测结果，用多种指标评估。

Result: 放射和组织病理学模态验证准确率高，临床图像因视觉异质性准确率低，集成模型在多模态验证集上总体准确率 84.58%。

Conclusion: 多模态集成框架提供非侵入性 AI 辅助分诊工具，可减少诊断延迟，改善患者预后。

Abstract: Aims Late diagnosis of Oral Squamous Cell Carcinoma (OSCC) contributes
significantly to its high global mortality rate, with over 50\% of cases
detected at advanced stages and a 5-year survival rate below 50\% according to
WHO statistics. This study aims to improve early detection of OSCC by
developing a multimodal deep learning framework that integrates clinical,
radiological, and histopathological images using a weighted ensemble of
DenseNet-121 convolutional neural networks (CNNs). Material and Methods A
retrospective study was conducted using publicly available datasets
representing three distinct medical imaging modalities. Each modality-specific
dataset was used to train a DenseNet-121 CNN via transfer learning.
Augmentation and modality-specific preprocessing were applied to increase
robustness. Predictions were fused using a validation-weighted ensemble
strategy. Evaluation was performed using accuracy, precision, recall, F1-score.
Results High validation accuracy was achieved for radiological (100\%) and
histopathological (95.12\%) modalities, with clinical images performing lower
(63.10\%) due to visual heterogeneity. The ensemble model demonstrated improved
diagnostic robustness with an overall accuracy of 84.58\% on a multimodal
validation dataset of 55 samples. Conclusion The multimodal ensemble framework
bridges gaps in the current diagnostic workflow by offering a non-invasive,
AI-assisted triage tool that enhances early identification of high-risk
lesions. It supports clinicians in decision-making, aligning with global
oncology guidelines to reduce diagnostic delays and improve patient outcomes.

</details>


### [527] [Talking Tennis: Language Feedback from 3D Biomechanical Action Recognition](https://arxiv.org/abs/2510.03921)
*Arushi Dashore,Aryan Anumala,Emily Hui,Olivia Yang*

Main category: cs.CV

TL;DR: 本文针对现有网球击球分析系统无法提供易理解且有意义的语言反馈问题，提出基于CNN - LSTM和大语言模型的框架，利用THETIS数据集评估，以弥合可解释AI与运动生物力学的差距。


<details>
  <summary>Details</summary>
Motivation: 现有网球击球分析系统无法将生物力学见解与对球员和教练易理解且有意义的语言反馈相联系。

Method: 使用基于CNN - LSTM的模型从运动数据中提取关键生物力学特征，分析其与击球效果和受伤风险的关系，利用大语言模型生成反馈，用THETIS数据集和特征提取技术进行实验。

Result: 未提及具体实验结果，但通过实验设置评估框架的分类性能和可解释性。

Conclusion: 所提出的框架有望弥合可解释AI与运动生物力学之间的差距，为终端用户提供技术准确、基于生物力学且可行的反馈。

Abstract: Automated tennis stroke analysis has advanced significantly with the
integration of biomechanical motion cues alongside deep learning techniques,
enhancing stroke classification accuracy and player performance evaluation.
Despite these advancements, existing systems often fail to connect
biomechanical insights with actionable language feedback that is both
accessible and meaningful to players and coaches. This research project
addresses this gap by developing a novel framework that extracts key
biomechanical features (such as joint angles, limb velocities, and kinetic
chain patterns) from motion data using Convolutional Neural Network Long
Short-Term Memory (CNN-LSTM)-based models. These features are analyzed for
relationships influencing stroke effectiveness and injury risk, forming the
basis for feedback generation using large language models (LLMs). Leveraging
the THETIS dataset and feature extraction techniques, our approach aims to
produce feedback that is technically accurate, biomechanically grounded, and
actionable for end-users. The experimental setup evaluates this framework on
classification performance and interpretability, bridging the gap between
explainable AI and sports biomechanics.

</details>


### [528] [Prompt-to-Prompt: Text-Based Image Editing Via Cross-Attention Mechanisms -- The Research of Hyperparameters and Novel Mechanisms to Enhance Existing Frameworks](https://arxiv.org/abs/2510.04034)
*Linn Bieske,Carla Lorente*

Main category: cs.CV

TL;DR: 研究探索优化超参数以提升prompt - to - prompt图像编辑框架的精度和可靠性，提出相关方法和框架。


<details>
  <summary>Details</summary>
Motivation: 当前图像编辑从手动像素操作转向深度学习方法，但结果存在可变性，如头发颜色变化不一致，需要提升prompt - to - prompt图像编辑框架的精度和可靠性。

Method: 全面研究“word swap”方法，开发“attention re - weight method”，提出“CL P2P”框架。

Result: 未提及具体结果。

Conclusion: 有助于理解和改进超参数设置与神经网络模型架构选择（尤其是注意力机制）之间的相互作用，影响生成图像的构成和质量。

Abstract: Recent advances in image editing have shifted from manual pixel manipulation
to employing deep learning methods like stable diffusion models, which now
leverage cross-attention mechanisms for text-driven control. This transition
has simplified the editing process but also introduced variability in results,
such as inconsistent hair color changes. Our research aims to enhance the
precision and reliability of prompt-to-prompt image editing frameworks by
exploring and optimizing hyperparameters. We present a comprehensive study of
the "word swap" method, develop an "attention re-weight method" for better
adaptability, and propose the "CL P2P" framework to address existing
limitations like cycle inconsistency. This work contributes to understanding
and improving the interaction between hyperparameter settings and the
architectural choices of neural network models, specifically their attention
mechanisms, which significantly influence the composition and quality of the
generated images.

</details>


### [529] [\textsc{GUI-Spotlight}: Adaptive Iterative Focus Refinement for Enhanced GUI Visual Grounding](https://arxiv.org/abs/2510.04039)
*Bin Lei,Nuo Xu,Ali Payani,Mingyi Hong,Chunhua Liao,Yu Cao,Caiwen Ding*

Main category: cs.CV

TL;DR: 本文介绍GUI - Spotlight模型解决MLLMs在GUI系统中视觉定位可靠性问题，在基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在GUI系统中实用价值受视觉定位可靠性限制，无法精确执行指针级操作，需解决该问题。

Method: 引入GUI - Spotlight模型，通过图像推理动态调用多个专业工具迭代缩小关注范围到屏幕相关区域。

Result: 在ScreenSpot - Pro基准测试中，仅用18.5K训练样本的GUI - Spotlight准确率达52.8%，超过使用大量训练样本的V2P - 7B和GTA - 1 - 7B。

Conclusion: GUI - Spotlight模型能有效提高视觉定位准确性，解决MLLMs在GUI系统中的局限性。

Abstract: Multimodal large language models (MLLMs) have markedly expanded the
competence of graphical user-interface (GUI) systems, propelling them beyond
controlled simulations into complex, real-world environments across diverse
platforms. However, practical usefulness is still bounded by the reliability of
visual grounding, i.e., mapping textual references to exact on-screen elements.
This limitation prevents the system from accurately performing pointer-level
actions such as clicking or dragging. To address it, we introduce GUI-Spotlight
-- a model trained for image-grounded reasoning that dynamically invokes
multiple specialized tools to iteratively narrow its focus to the relevant
region of the screen, thereby substantially improving visual grounding
accuracy. On the ScreenSpot-Pro benchmark, GUI-Spotlight trained with only
18.5K training samples achieves 52.8\% accuracy, surpassing V2P-7B (50.6\% with
9.6M training samples) and GTA-1-7B (50.1\% with 1.56M training samples).

</details>


### [530] [Quantization Range Estimation for Convolutional Neural Networks](https://arxiv.org/abs/2510.04044)
*Bingtao Yang,Yujia Wang,Mengzhi Jiao,Hongwei Huo*

Main category: cs.CV

TL;DR: 提出范围估计方法改善后训练量化性能，实验表明在图像分类任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 后训练量化虽能减少模型存储，但低比特量化保持精度是难题。

Method: 将范围估计建模为按层局部极小值最小化量化误差的优化问题，证明其局部凸性并给出搜索算法，应用于变换后的权重空间。

Result: 在ResNet系列和Inception - v3模型的图像分类任务上，top - 1准确率优于现有方法，8位和6位设置几乎无精度损失，4位量化精度显著提升。

Conclusion: 提出的范围估计方法能有效提高后训练量化性能。

Abstract: Post-training quantization for reducing the storage of deep neural network
models has been demonstrated to be an effective way in various tasks. However,
low-bit quantization while maintaining model accuracy is a challenging problem.
In this paper, we present a range estimation method to improve the quantization
performance for post-training quantization. We model the range estimation into
an optimization problem of minimizing quantization errors by layer-wise local
minima. We prove this problem is locally convex and present an efficient search
algorithm to find the optimal solution. We propose the application of the above
search algorithm to the transformed weights space to do further improvement in
practice. Our experiments demonstrate that our method outperforms
state-of-the-art performance generally on top-1 accuracy for image
classification tasks on the ResNet series models and Inception-v3 model. The
experimental results show that the proposed method has almost no loss of top-1
accuracy in 8-bit and 6-bit settings for image classifications, and the
accuracy of 4-bit quantization is also significantly improved. The code is
available at https://github.com/codeiscommitting/REQuant.

</details>


### [531] [MetaFind: Scene-Aware 3D Asset Retrieval for Coherent Metaverse Scene Generation](https://arxiv.org/abs/2510.04057)
*Zhenyu Pan,Yucheng Lu,Han Liu*

Main category: cs.CV

TL;DR: 提出MetaFind框架，支持多模态查询，用于从大规模仓库检索3D资产以增强元宇宙场景生成，在检索任务中表现更好。


<details>
  <summary>Details</summary>
Motivation: 解决现有3D资产检索中存在的资产检索不一致、缺乏标准化检索范式的问题。

Method: 引入灵活检索机制，支持多模态查询，采用可插拔的等变布局编码器ESSGNN捕捉空间关系和对象外观特征，支持迭代场景构建。

Result: 实证评估表明，MetaFind在各种检索任务中空间和风格一致性优于基线方法。

Conclusion: MetaFind框架能有效解决3D资产检索问题，提升检索的空间和风格一致性。

Abstract: We present MetaFind, a scene-aware tri-modal compositional retrieval
framework designed to enhance scene generation in the metaverse by retrieving
3D assets from large-scale repositories. MetaFind addresses two core
challenges: (i) inconsistent asset retrieval that overlooks spatial, semantic,
and stylistic constraints, and (ii) the absence of a standardized retrieval
paradigm specifically tailored for 3D asset retrieval, as existing approaches
mainly rely on general-purpose 3D shape representation models. Our key
innovation is a flexible retrieval mechanism that supports arbitrary
combinations of text, image, and 3D modalities as queries, enhancing spatial
reasoning and style consistency by jointly modeling object-level features
(including appearance) and scene-level layout structures. Methodologically,
MetaFind introduces a plug-and-play equivariant layout encoder ESSGNN that
captures spatial relationships and object appearance features, ensuring
retrieved 3D assets are contextually and stylistically coherent with the
existing scene, regardless of coordinate frame transformations. The framework
supports iterative scene construction by continuously adapting retrieval
results to current scene updates. Empirical evaluations demonstrate the
improved spatial and stylistic consistency of MetaFind in various retrieval
tasks compared to baseline methods.

</details>


### [532] [TOPO-Bench: An Open-Source Topological Mapping Evaluation Framework with Quantifiable Perceptual Aliasing](https://arxiv.org/abs/2510.04100)
*Jiaming Wang,Diwen Liu,Jizhuo Chen,Harold Soh*

Main category: cs.CV

TL;DR: 论文指出拓扑映射领域缺乏标准评估指标等问题，提出拓扑一致性和数据集模糊度量化方法，构建基准数据集并评估，开源成果促进研究。


<details>
  <summary>Details</summary>
Motivation: 拓扑映射领域缺乏标准化评估指标、数据集和协议，且感知混淆问题未充分量化，阻碍领域发展。

Method: 1. 形式化拓扑一致性，以定位精度为替代指标；2. 提出数据集模糊度量化方法；3. 构建有校准模糊度的基准数据集，实现并发布深度学习基线系统，与经典方法一起评估。

Result: 实验和分析揭示了当前方法在感知混淆下的局限性。

Conclusion: 开源所有数据集、基线和评估工具，促进拓扑映射研究的一致性和可重复性。

Abstract: Topological mapping offers a compact and robust representation for
navigation, but progress in the field is hindered by the lack of standardized
evaluation metrics, datasets, and protocols. Existing systems are assessed
using different environments and criteria, preventing fair and reproducible
comparisons. Moreover, a key challenge - perceptual aliasing - remains
under-quantified, despite its strong influence on system performance. We
address these gaps by (1) formalizing topological consistency as the
fundamental property of topological maps and showing that localization accuracy
provides an efficient and interpretable surrogate metric, and (2) proposing the
first quantitative measure of dataset ambiguity to enable fair comparisons
across environments. To support this protocol, we curate a diverse benchmark
dataset with calibrated ambiguity levels, implement and release deep-learned
baseline systems, and evaluate them alongside classical methods. Our
experiments and analysis yield new insights into the limitations of current
approaches under perceptual aliasing. All datasets, baselines, and evaluation
tools are fully open-sourced to foster consistent and reproducible research in
topological mapping.

</details>


### [533] [Learning from All: Concept Alignment for Autonomous Distillation from Multiple Drifting MLLMs](https://arxiv.org/abs/2510.04142)
*Xiaoyu Yang,Jie Lu,En Yu*

Main category: cs.CV

TL;DR: 本文指出多模态大语言模型蒸馏中概念漂移问题，提出“学习、比较、批判”范式及自主偏好优化方法，实验证明效果好并贡献数据集。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型蒸馏中多个漂移教师生成的推理轨迹存在概念漂移，影响学生模型性能的问题。

Method: 建立概念漂移和知识蒸馏的理论联系，提出“学习、比较、批判”范式和自主偏好优化（APO）方法。

Result: 实验表明在知识蒸馏中具有优越的一致性、鲁棒性和泛化性能，贡献了大规模数据集CXR - MAX。

Conclusion: 提出的方法能有效解决多模态大语言模型蒸馏中的概念漂移问题，得到鲁棒、一致和可泛化的模型。

Abstract: This paper identifies a critical yet underexplored challenge in distilling
from multimodal large language models (MLLMs): the reasoning trajectories
generated by multiple drifting teachers exhibit concept drift, whereby their
reasoning distributions evolve unpredictably and transmit biases to the student
model, ultimately compromising its performance. To tackle this issue, we
pioneer a theoretical connection between concept drift and knowledge
distillation, casting the non-stationary reasoning dynamics from multiple MLLM
teachers as next-token prediction of multi-stream reasoning trajectories.Guided
by concept drift, we introduce the "learn, compare, critique" paradigm,
culminating in autonomous preference optimization (APO). Under the active
guidance of the teachers, the student model first learns and self-distils
preferred thinking by comparing multiple teachers. It then engages in critical
reflection over the drifting inference from teachers, performing concept
alignment through APO, ultimately yielding a robust, consistent, and
generalizable model.Extensive experiments demonstrate our superior performance
of consistency, robustness and generalization within knowledge distillation.
Besides, we also contributed a large-scale dataset, CXR-MAX (Multi-teachers
Alignment X-rays), comprising 170,982 distilled reasoning trajectories derived
from publicly accessible MLLMs based on MIMIC-CXR. Our code and data are public
at: https://anonymous.4open.science/r/Autonomous-Distillation/.

</details>


### [534] [World-To-Image: Grounding Text-to-Image Generation with Agent-Driven World Knowledge](https://arxiv.org/abs/2510.04201)
*Moo Hyun Son,Jintaek Oh,Sun Bin Mun,Jaechul Roh,Sehyun Choi*

Main category: cs.CV

TL;DR: 提出World - To - Image框架，用代理驱动的世界知识增强文本到图像生成，在语义对齐和视觉美学上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型在处理新的或分布外实体时性能下降，因存在知识截止问题。

Method: 设计代理动态搜索网络获取基础模型未知概念的图像，进行多模态提示优化。采用LLMGrader和ImageReward等现代评估方法。

Result: 在语义对齐和视觉美学上大幅超越现有方法，在NICE基准上准确率提升8.1%，不到三次迭代即可高效实现。

Conclusion: 该框架为能更好反映不断变化的现实世界的文本到图像系统铺平道路。

Abstract: While text-to-image (T2I) models can synthesize high-quality images, their
performance degrades significantly when prompted with novel or
out-of-distribution (OOD) entities due to inherent knowledge cutoffs. We
introduce World-To-Image, a novel framework that bridges this gap by empowering
T2I generation with agent-driven world knowledge. We design an agent that
dynamically searches the web to retrieve images for concepts unknown to the
base model. This information is then used to perform multimodal prompt
optimization, steering powerful generative backbones toward an accurate
synthesis. Critically, our evaluation goes beyond traditional metrics,
utilizing modern assessments like LLMGrader and ImageReward to measure true
semantic fidelity. Our experiments show that World-To-Image substantially
outperforms state-of-the-art methods in both semantic alignment and visual
aesthetics, achieving +8.1% improvement in accuracy-to-prompt on our curated
NICE benchmark. Our framework achieves these results with high efficiency in
less than three iterations, paving the way for T2I systems that can better
reflect the ever-changing real world. Our demo code is available
here\footnote{https://github.com/mhson-kyle/World-To-Image}.

</details>


### [535] [Multimodal Arabic Captioning with Interpretable Visual Concept Integration](https://arxiv.org/abs/2510.03295)
*Passant Elchafei,Amany Fashwan*

Main category: cs.CV

TL;DR: 提出阿拉伯语图像字幕框架VLCAP，结合视觉标签检索与多模态文本生成，测试不同编码器 - 解码器配置，得出各指标最佳组合，实现文化连贯、上下文准确的阿拉伯语字幕。


<details>
  <summary>Details</summary>
Motivation: 构建一个能生成文化连贯、上下文准确阿拉伯语图像字幕的框架。

Method: 将CLIP视觉标签检索与多模态文本生成结合，用mCLIP、AraCLIP和Jina V4提取阿拉伯语视觉概念，构建混合词汇表，将检索标签转为提示传递给视觉 - 语言模型，测试Qwen - VL和Gemini Pro Vision进行字幕生成。

Result: mCLIP + Gemini Pro Vision的BLEU - 1为5.34%、余弦相似度为60.01%最佳；AraCLIP + Qwen - VL的LLM - judge分数36.33%最高。

Conclusion: 该可解释的框架能实现文化连贯、上下文准确的阿拉伯语图像字幕。

Abstract: We present VLCAP, an Arabic image captioning framework that integrates
CLIP-based visual label retrieval with multimodal text generation. Rather than
relying solely on end-to-end captioning, VLCAP grounds generation in
interpretable Arabic visual concepts extracted with three multilingual
encoders, mCLIP, AraCLIP, and Jina V4, each evaluated separately for label
retrieval. A hybrid vocabulary is built from training captions and enriched
with about 21K general domain labels translated from the Visual Genome dataset,
covering objects, attributes, and scenes. The top-k retrieved labels are
transformed into fluent Arabic prompts and passed along with the original image
to vision-language models. In the second stage, we tested Qwen-VL and Gemini
Pro Vision for caption generation, resulting in six encoder-decoder
configurations. The results show that mCLIP + Gemini Pro Vision achieved the
best BLEU-1 (5.34%) and cosine similarity (60.01%), while AraCLIP + Qwen-VL
obtained the highest LLM-judge score (36.33%). This interpretable pipeline
enables culturally coherent and contextually accurate Arabic captions.

</details>


### [536] [MASC: Boosting Autoregressive Image Generation with a Manifold-Aligned Semantic Clustering](https://arxiv.org/abs/2510.04220)
*Lixuan He,Shikang Zheng,Linfeng Zhang*

Main category: cs.CV

TL;DR: 提出MASC框架解决自回归模型在图像生成中的效率和质量问题，加速训练并提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 传统自回归模型因视觉标记词汇无结构，导致预测任务复杂，影响训练效率和生成质量。

Method: 提出Manifold - Aligned Semantic Clustering (MASC)框架，构建层次语义树，采用几何感知距离度量和密度驱动聚合构建方法。

Result: 加速训练达57%，降低LlamaGen - XL的FID从2.87到2.58。

Conclusion: 结构化预测空间对可扩展生成建模与架构创新同样重要，MASC使现有AR框架极具竞争力。

Abstract: Autoregressive (AR) models have shown great promise in image generation, yet
they face a fundamental inefficiency stemming from their core component: a
vast, unstructured vocabulary of visual tokens. This conventional approach
treats tokens as a flat vocabulary, disregarding the intrinsic structure of the
token embedding space where proximity often correlates with semantic
similarity. This oversight results in a highly complex prediction task, which
hinders training efficiency and limits final generation quality. To resolve
this, we propose Manifold-Aligned Semantic Clustering (MASC), a principled
framework that constructs a hierarchical semantic tree directly from the
codebook's intrinsic structure. MASC employs a novel geometry-aware distance
metric and a density-driven agglomerative construction to model the underlying
manifold of the token embeddings. By transforming the flat, high-dimensional
prediction task into a structured, hierarchical one, MASC introduces a
beneficial inductive bias that significantly simplifies the learning problem
for the AR model. MASC is designed as a plug-and-play module, and our extensive
experiments validate its effectiveness: it accelerates training by up to 57%
and significantly improves generation quality, reducing the FID of LlamaGen-XL
from 2.87 to 2.58. MASC elevates existing AR frameworks to be highly
competitive with state-of-the-art methods, establishing that structuring the
prediction space is as crucial as architectural innovation for scalable
generative modeling.

</details>


### [537] [Zoom-In to Sort AI-Generated Images Out](https://arxiv.org/abs/2510.04225)
*Yikun Ji,Yan Hong,Bowen Deng,jun lan,Huijia Zhu,Weiqiang Wang,Liqing Zhang,Jianfu Zhang*

Main category: cs.CV

TL;DR: 提出ZoomIn框架和MagniFake数据集，检测AI生成图像准确率达96.39%且有可解释性。


<details>
  <summary>Details</summary>
Motivation: AI生成图像模糊真假边界，现有视觉语言模型难以检测高质量合成图像的细微瑕疵。

Method: 提出两阶段法医框架ZoomIn，先扫描图像定位可疑区域，再聚焦分析；引入含20000张图像的MagniFake数据集辅助训练。

Result: 方法准确率达96.39%，具有强大泛化能力，能提供基于视觉证据的可解释性说明。

Conclusion: ZoomIn框架和MagniFake数据集在检测AI生成图像方面有效，提升了准确性和可解释性。

Abstract: The rapid growth of AI-generated imagery has blurred the boundary between
real and synthetic content, raising critical concerns for digital integrity.
Vision-language models (VLMs) offer interpretability through explanations but
often fail to detect subtle artifacts in high-quality synthetic images. We
propose ZoomIn, a two-stage forensic framework that improves both accuracy and
interpretability. Mimicking human visual inspection, ZoomIn first scans an
image to locate suspicious regions and then performs a focused analysis on
these zoomed-in areas to deliver a grounded verdict. To support training, we
introduce MagniFake, a dataset of 20,000 real and high-quality synthetic images
annotated with bounding boxes and forensic explanations, generated through an
automated VLM-based pipeline. Our method achieves 96.39% accuracy with robust
generalization, while providing human-understandable explanations grounded in
visual evidence.

</details>


### [538] [DECOR: Deep Embedding Clustering with Orientation Robustness](https://arxiv.org/abs/2510.03328)
*Fiona Victoria Stanley Jothiraj,Arunaggiri Pandian Karunanidhi,Seth A. Eichmeyer*

Main category: cs.CV

TL;DR: 介绍DECOR框架用于晶圆缺陷聚类，在数据集上验证其性能优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 半导体制造中需在复杂数据条件下设计可靠的晶圆缺陷聚类方法。

Method: 引入DECOR，一种具有方向鲁棒性的深度聚类框架。

Result: 在开源数据集上展示无需手动调参发现聚类的能力，实验表明性能优于现有聚类基线方法。

Conclusion: DECOR为自动视觉检测系统提供了可靠且可扩展的解决方案。

Abstract: In semiconductor manufacturing, early detection of wafer defects is critical
for product yield optimization. However, raw wafer data from wafer quality
tests are often complex, unlabeled, imbalanced and can contain multiple defects
on a single wafer, making it crucial to design clustering methods that remain
reliable under such imperfect data conditions. We introduce DECOR, a deep
clustering with orientation robustness framework that groups complex defect
patterns from wafer maps into consistent clusters. We evaluate our method on
the open source MixedWM38 dataset, demonstrating its ability to discover
clusters without manual tuning. DECOR explicitly accounts for orientation
variations in wafer maps, ensuring that spatially similar defects are
consistently clustered regardless of its rotation or alignment. Experiments
indicate that our method outperforms existing clustering baseline methods, thus
providing a reliable and scalable solution in automated visual inspection
systems.

</details>


### [539] [Concept-Based Masking: A Patch-Agnostic Defense Against Adversarial Patch Attacks](https://arxiv.org/abs/2510.04245)
*Ayushi Mehrotra,Derek Peng,Dipkamal Bhusal,Nidhi Rastogi*

Main category: cs.CV

TL;DR: 提出一种与补丁无关的防御方法，利用基于概念的解释来中和对抗性补丁攻击效果，在Imagenette上评估效果优于PatchCleanser。


<details>
  <summary>Details</summary>
Motivation: 现有对抗性补丁攻击的防御方法通常需先验知识，适用性受限，需要一种更通用的防御方法。

Method: 提出一种与补丁无关的防御方法，利用基于概念的解释识别并抑制最具影响力的概念激活向量，无需显式检测。

Result: 在Imagenette上使用ResNet - 50评估，该方法比PatchCleanser有更高的鲁棒性和干净准确率，在不同补丁大小和位置下表现良好。

Conclusion: 结合可解释性和鲁棒性有前景，基于概念的防御是防范对抗性补丁攻击的可扩展策略。

Abstract: Adversarial patch attacks pose a practical threat to deep learning models by
forcing targeted misclassifications through localized perturbations, often
realized in the physical world. Existing defenses typically assume prior
knowledge of patch size or location, limiting their applicability. In this
work, we propose a patch-agnostic defense that leverages concept-based
explanations to identify and suppress the most influential concept activation
vectors, thereby neutralizing patch effects without explicit detection.
Evaluated on Imagenette with a ResNet-50, our method achieves higher robust and
clean accuracy than the state-of-the-art PatchCleanser, while maintaining
strong performance across varying patch sizes and locations. Our results
highlight the promise of combining interpretability with robustness and suggest
concept-driven defenses as a scalable strategy for securing machine learning
models against adversarial patch attacks.

</details>


### [540] [Exploring the Hierarchical Reasoning Model for Small Natural-Image Classification Without Augmentation](https://arxiv.org/abs/2510.03598)
*Alexander V. Mantzaris*

Main category: cs.CV

TL;DR: 本文评估HRM作为图像分类器的性能，发现其在小分辨率图像分类无增强时不如简单卷积架构，但模型改进后可能提升。


<details>
  <summary>Details</summary>
Motivation: 探究具有特定配置的Hierarchical Reasoning Model (HRM)能否作为实用图像分类器。

Method: 在MNIST、CIFAR - 10和CIFAR - 100数据集上，在无数据增强等原始条件下评估HRM。

Result: HRM在MNIST上优化稳定且表现良好，但在小自然图像上过拟合、泛化差，不如简单卷积架构。

Conclusion: 当前HRM在小分辨率无增强图像分类上不如简单卷积架构，但模型修改后有提升可能。

Abstract: This paper asks whether the Hierarchical Reasoning Model (HRM) with the two
Transformer-style modules $(f_L,f_H)$, one step (DEQ-style) training, deep
supervision, Rotary Position Embeddings, and RMSNorm can serve as a practical
image classifier. It is evaluated on MNIST, CIFAR-10, and CIFAR-100 under a
deliberately raw regime: no data augmentation, identical optimizer family with
one-epoch warmup then cosine-floor decay, and label smoothing. HRM optimizes
stably and performs well on MNIST ($\approx 98\%$ test accuracy), but on small
natural images it overfits and generalizes poorly: on CIFAR-10, HRM reaches
65.0\% after 25 epochs, whereas a two-stage Conv--BN--ReLU baseline attains
77.2\% while training $\sim 30\times$ faster per epoch; on CIFAR-100, HRM
achieves only 29.7\% test accuracy despite 91.5\% train accuracy, while the
same CNN reaches 45.3\% test with 50.5\% train accuracy. Loss traces and error
analyses indicate healthy optimization but insufficient image-specific
inductive bias for HRM in this regime. It is concluded that, for
small-resolution image classification without augmentation, HRM is not
competitive with even simple convolutional architectures as the HRM currently
exist but this does not exclude possibilities that modifications to the model
may allow it to improve greatly.

</details>


### [541] [Unsupervised Transformer Pre-Training for Images: Self-Distillation, Mean Teachers, and Random Crops](https://arxiv.org/abs/2510.03606)
*Mattia Scardecchia*

Main category: cs.CV

TL;DR: 本文调研自监督学习模型DINOv2，分析其核心思想、与其他方法对比性能，讨论其局限、影响和未来方向。


<details>
  <summary>Details</summary>
Motivation: DINOv2在自监督学习中取得新的最优成果，有必要研究其核心思想等。

Method: 剖析DINOv2的多裁剪视图增强和带均值教师的自蒸馏方法，追溯其在过往工作中的发展；对比DINO和DINOv2与其他自监督和弱监督方法在下游任务的性能。

Result: 凸显了基于Transformer骨干网络学习到的特征的显著涌现特性。

Conclusion: 简要讨论了DINOv2的局限性、影响和未来研究方向。

Abstract: Recent advances in self-supervised learning (SSL) have made it possible to
learn general-purpose visual features that capture both the high-level
semantics and the fine-grained spatial structure of images. Most notably, the
recent DINOv2 has established a new state of the art by surpassing weakly
supervised methods (WSL) like OpenCLIP on most benchmarks. In this survey, we
examine the core ideas behind its approach, multi-crop view augmentation and
self-distillation with a mean teacher, and trace their development in previous
work. We then compare the performance of DINO and DINOv2 with other SSL and WSL
methods across various downstream tasks, and highlight some remarkable emergent
properties of their learned features with transformer backbones. We conclude by
briefly discussing DINOv2's limitations, its impact, and future research
directions.

</details>


### [542] [MorphoSim: An Interactive, Controllable, and Editable Language-guided 4D World Simulator](https://arxiv.org/abs/2510.04390)
*Xuehai He,Shijie Zhou,Thivyanth Venkateswaran,Kaizhi Zheng,Ziyu Wan,Achuta Kadambi,Xin Eric Wang*

Main category: cs.CV

TL;DR: 介绍MorphoSim框架，可生成具有多视图一致性和对象级控制的4D场景，实验表明其在保持高场景保真度的同时具备可控性和可编辑性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频模型局限于2D视图且交互有限，需要支持可控和可编辑时空环境的世界模型用于机器人领域。

Method: 引入MorphoSim框架，集成轨迹引导生成与特征场蒸馏，可交互式应用编辑而无需完全重新生成。

Result: 实验显示MorphoSim在保持高场景保真度的同时，具备可控性和可编辑性。

Conclusion: MorphoSim是一个有效的语言引导框架，能生成满足需求的4D场景。

Abstract: World models that support controllable
  and editable spatiotemporal environments are valuable
  for robotics, enabling scalable training data, repro ducible evaluation, and
flexible task design. While
  recent text-to-video models generate realistic dynam ics, they are
constrained to 2D views and offer limited
  interaction. We introduce MorphoSim, a language guided framework that
generates 4D scenes with
  multi-view consistency and object-level controls. From
  natural language instructions, MorphoSim produces
  dynamic environments where objects can be directed,
  recolored, or removed, and scenes can be observed
  from arbitrary viewpoints. The framework integrates
  trajectory-guided generation with feature field dis tillation, allowing edits
to be applied interactively
  without full re-generation. Experiments show that Mor phoSim maintains high
scene fidelity while enabling
  controllability and editability. The code is available
  at https://github.com/eric-ai-lab/Morph4D.

</details>


### [543] [Person-Centric Annotations of LAION-400M: Auditing Bias and Its Transfer to Models](https://arxiv.org/abs/2510.03721)
*Leander Girrbach,Stephan Alaniz,Genevieve Smith,Trevor Darrell,Zeynep Akata*

Main category: cs.CV

TL;DR: 为LAION - 400M创建以人为主的注释，揭示数据集中人口统计学失衡和有害关联，建立数据集组成与下游模型偏差的联系。


<details>
  <summary>Details</summary>
Motivation: 大型多模态数据集训练的视觉 - 语言模型存在人口统计学偏差，但训练数据在产生这些偏差中的作用尚不清楚，且网络规模数据集缺乏人口统计学注释。

Method: 通过结合目标检测、多模态字幕生成和微调分类器的自动标注管道，为LAION - 400M创建以人为主的注释。

Result: 发现数据集中存在人口统计学失衡和有害关联，如男性和被视为黑人或中东人的个体与犯罪相关和负面内容的过度关联；CLIP和Stable Diffusion中60 - 70%的性别偏差可由数据中的直接共现线性解释。

Conclusion: 建立了数据集组成与下游模型偏差之间的首个大规模实证联系。

Abstract: Vision-language models trained on large-scale multimodal datasets show strong
demographic biases, but the role of training data in producing these biases
remains unclear. A major barrier has been the lack of demographic annotations
in web-scale datasets such as LAION-400M. We address this gap by creating
person-centric annotations for the full dataset, including over 276 million
bounding boxes, perceived gender and race/ethnicity labels, and automatically
generated captions. These annotations are produced through validated automatic
labeling pipelines combining object detection, multimodal captioning, and
finetuned classifiers. Using them, we uncover demographic imbalances and
harmful associations, such as the disproportionate linking of men and
individuals perceived as Black or Middle Eastern with crime-related and
negative content. We also show that 60-70% of gender bias in CLIP and Stable
Diffusion can be linearly explained by direct co-occurrences in the data. Our
resources establish the first large-scale empirical link between dataset
composition and downstream model bias.

</details>


### [544] [Mapping Rio de Janeiro's favelas: general-purpose vs. satellite-specific neural networks](https://arxiv.org/abs/2510.03725)
*Thomas Hallopeau,Joris Guérin,Laurent Demagistri,Youssef Fouzai,Renata Gracie,Vanderlei Pascoal De Matos,Helen Gurgel,Nadine Dessay*

Main category: cs.CV

TL;DR: 比较两种预训练神经网络用于检测里约热内卢贫民窟，探讨任务特异性和数据量对城市非正式定居点检测性能的影响。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习检测非正式定居点方法未充分利用预训练神经网络潜力，研究任务特异性和数据量哪个对检测性能更优。

Method: 比较基于通用未指定图像数据集预训练的通用网络和基于卫星图像预训练的专业网络。

Result: 摘要未提及。

Conclusion: 摘要未提及。

Abstract: While deep learning methods for detecting informal settlements have already
been developed, they have not yet fully utilized the potential offered by
recent pretrained neural networks. We compare two types of pretrained neural
networks for detecting the favelas of Rio de Janeiro: 1. Generic networks
pretrained on large diverse datasets of unspecific images, 2. A specialized
network pretrained on satellite imagery. While the latter is more specific to
the target task, the former has been pretrained on significantly more images.
Hence, this research investigates whether task specificity or data volume
yields superior performance in urban informal settlement detection.

</details>


### [545] [Your Vision-Language Model Can't Even Count to 20: Exposing the Failures of VLMs in Compositional Counting](https://arxiv.org/abs/2510.04401)
*Xuyang Guo,Zekai Huang,Zhenmei Shi,Zhao Song,Jiahao Zhang*

Main category: cs.CV

TL;DR: 本文引入基准VLMCountBench研究VLMs计数能力，发现其在组合计数上存在缺陷。


<details>
  <summary>Details</summary>
Motivation: 探讨VLMs能否正确计数物体这一基础问题。

Method: 引入VLMCountBench，采用严格独立变量控制，在可控消融实验中研究颜色、大小和提示细化等简单属性的影响。

Result: VLMs在只有一种形状时计数可靠，但在多种形状组合时大量失败。

Conclusion: 指出当前VLMs存在基础经验局限性，为未来研究指明方向。

Abstract: Vision-Language Models (VLMs) have become a central focus of today's AI
community, owing to their impressive abilities gained from training on
large-scale vision-language data from the Web. These models have demonstrated
strong performance across diverse tasks, including image understanding, video
understanding, complex visual reasoning, and embodied AI. Despite these
noteworthy successes, a fundamental question remains: Can VLMs count objects
correctly? In this paper, we introduce a simple yet effective benchmark,
VLMCountBench, designed under a minimalist setting with only basic geometric
shapes (e.g., triangles, circles) and their compositions, focusing exclusively
on counting tasks without interference from other factors. We adopt strict
independent variable control and systematically study the effects of simple
properties such as color, size, and prompt refinement in a controlled ablation.
Our empirical results reveal that while VLMs can count reliably when only one
shape type is present, they exhibit substantial failures when multiple shape
types are combined (i.e., compositional counting). This highlights a
fundamental empirical limitation of current VLMs and motivates important
directions for future research.

</details>


### [546] [Road Damage and Manhole Detection using Deep Learning for Smart Cities: A Polygonal Annotation Approach](https://arxiv.org/abs/2510.03797)
*Rasel Hossen,Diptajoy Mistry,Mushiur Rahman,Waki As Sami Atikur Rahman Hridoy,Sajib Saha,Muhammad Ibrahim*

Main category: cs.CV

TL;DR: 本文提出用YOLOv9算法结合多边形标注进行道路损坏和沙井检测的深度学习方法，开发新数据集训练模型，取得78.1%的整体图像级准确率，为发展中国家城市基础设施监测提供有效可扩展方案。


<details>
  <summary>Details</summary>
Motivation: 手动监测道路损坏耗时、成本高且易出错，需要自动化方法助力智慧城市发展。

Method: 采用YOLOv9算法结合多边形标注，开发含超千张图像的新数据集，训练针对三个类别的基于YOLO的模型。

Result: 整体图像级准确率78.1%，Broken类F1分数86.7%，Not Broken类F1分数89.2%，Manhole类因类别不平衡F1分数仅18.2%。

Conclusion: 该方法为发展中国家城市基础设施监测提供了高效、可扩展的解决方案。

Abstract: Urban safety and infrastructure maintenance are critical components of smart
city development. Manual monitoring of road damages is time-consuming, highly
costly, and error-prone. This paper presents a deep learning approach for
automated road damage and manhole detection using the YOLOv9 algorithm with
polygonal annotations. Unlike traditional bounding box annotation, we employ
polygonal annotations for more precise localization of road defects. We develop
a novel dataset comprising more than one thousand images which are mostly
collected from Dhaka, Bangladesh. This dataset is used to train a YOLO-based
model for three classes, namely Broken, Not Broken, and Manhole. We achieve
78.1% overall image-level accuracy. The YOLOv9 model demonstrates strong
performance for Broken (86.7% F1-score) and Not Broken (89.2% F1-score)
classes, with challenges in Manhole detection (18.2% F1-score) due to class
imbalance. Our approach offers an efficient and scalable solution for
monitoring urban infrastructure in developing countries.

</details>


### [547] [SPEGNet: Synergistic Perception-Guided Network for Camouflaged Object Detection](https://arxiv.org/abs/2510.04472)
*Baber Jan,Saeed Anwar,Aiman H. El-Maleh,Abdul Jabbar Siddiqui,Abdul Bais*

Main category: cs.CV

TL;DR: 提出SPEGNet解决伪装目标检测中的碎片化问题，实现边界精度和区域一致性平衡，在多个数据集表现良好且有实时推理速度。


<details>
  <summary>Details</summary>
Motivation: 当前伪装目标检测方法依赖复杂组件积累，产生计算负担且处理时降低分辨率丢失细节。

Method: 采用统一设计，通过通道校准和空间增强集成多尺度特征，直接从上下文丰富表示中提取边界，进行渐进式细化实现尺度自适应边缘调制。

Result: SPEGNet在CAMO、COD10K、NC4K数据集上分别达到0.887 $S_\alpha$、0.890、0.895，有实时推理速度。

Conclusion: 该方法在不同尺度目标、遮挡和模糊边界情况下表现出色。

Abstract: Camouflaged object detection segments objects with intrinsic similarity and
edge disruption. Current detection methods rely on accumulated complex
components. Each approach adds components such as boundary modules, attention
mechanisms, and multi-scale processors independently. This accumulation creates
a computational burden without proportional gains. To manage this complexity,
they process at reduced resolutions, eliminating fine details essential for
camouflage. We present SPEGNet, addressing fragmentation through a unified
design. The architecture integrates multi-scale features via channel
calibration and spatial enhancement. Boundaries emerge directly from
context-rich representations, maintaining semantic-spatial alignment.
Progressive refinement implements scale-adaptive edge modulation with peak
influence at intermediate resolutions. This design strikes a balance between
boundary precision and regional consistency. SPEGNet achieves 0.887 $S_\alpha$
on CAMO, 0.890 on COD10K, and 0.895 on NC4K, with real-time inference speed.
Our approach excels across scales, from tiny, intricate objects to large,
pattern-similar ones, while handling occlusion and ambiguous boundaries. Code,
model weights, and results are available on
\href{https://github.com/Baber-Jan/SPEGNet}{https://github.com/Baber-Jan/SPEGNet}.

</details>


### [548] [MedCLM: Learning to Localize and Reason via a CoT-Curriculum in Medical Vision-Language Models](https://arxiv.org/abs/2510.04477)
*Soo Yong Kim,Suin Cho,Vincent-Daniel Yun,Gyeongyeon Hwang*

Main category: cs.CV

TL;DR: 提出MedCLM自动化管道将检测数据集转换为带CoT推理的医学VQA数据，采用综合策略训练，在医学VQA基准测试中达SOTA。


<details>
  <summary>Details</summary>
Motivation: 解决医学成像中临床诊断推理与AI衔接的挑战。

Method: 引入MedCLM管道生成带推理的VQA数据，提出综合CoT课程策略分阶段训练。

Result: MedCLM在多个医学VQA基准测试中达到了最先进的性能。

Conclusion: MedCLM为开发临床对齐的医学视觉语言模型提供了可扩展的框架。

Abstract: Bridging clinical diagnostic reasoning with AI remains a central challenge in
medical imaging. We introduce MedCLM, an automated pipeline that converts
detection datasets into large-scale medical visual question answering (VQA)
data with Chain-of-Thought (CoT) reasoning by linking lesion boxes to organ
segmentation and structured rationales. These contextual signals enable medical
vision-language models to generate question-answer pairs with step-by-step
reasoning. To utilize this data effectively, we propose an Integrated
CoT-Curriculum Strategy composed of an Easy stage with explicit lesion boxes
for visual grounding, a Medium stage that encourages implicit localization, and
a Hard stage for weakly supervised reasoning. Experimental results demonstrate
that MedCLM attains state-of-the-art performance on several medical VQA
benchmarks, providing a scalable framework for developing clinically aligned
medical vision-language models.

</details>


### [549] [Keep It on a Leash: Controllable Pseudo-label Generation Towards Realistic Long-Tailed Semi-Supervised Learning](https://arxiv.org/abs/2510.03993)
*Yaxin Hou,Bo Han,Yuheng Jia,Hui Liu,Junhui Hou*

Main category: cs.CV

TL;DR: 提出可控伪标签生成框架CPG应对无标签数据分布未知问题，经优化循环、辅助模块提升性能，在基准数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有长尾半监督学习方法假设无标签数据分布已知，但实际中无标签数据分布未知且可能任意分布。

Method: 提出CPG框架，通过可控自增强优化循环更新有标签数据集；提出类感知自适应增强模块和辅助分支。

Result: 在多个常用基准数据集上进行综合评估，CPG准确率最高超现有方法15.97%。

Conclusion: CPG框架有效，能减少泛化误差，提升少数类表示，且代码开源。

Abstract: Current long-tailed semi-supervised learning methods assume that labeled data
exhibit a long-tailed distribution, and unlabeled data adhere to a typical
predefined distribution (i.e., long-tailed, uniform, or inverse long-tailed).
However, the distribution of the unlabeled data is generally unknown and may
follow an arbitrary distribution. To tackle this challenge, we propose a
Controllable Pseudo-label Generation (CPG) framework, expanding the labeled
dataset with the progressively identified reliable pseudo-labels from the
unlabeled dataset and training the model on the updated labeled dataset with a
known distribution, making it unaffected by the unlabeled data distribution.
Specifically, CPG operates through a controllable self-reinforcing optimization
cycle: (i) at each training step, our dynamic controllable filtering mechanism
selectively incorporates reliable pseudo-labels from the unlabeled dataset into
the labeled dataset, ensuring that the updated labeled dataset follows a known
distribution; (ii) we then construct a Bayes-optimal classifier using logit
adjustment based on the updated labeled data distribution; (iii) this improved
classifier subsequently helps identify more reliable pseudo-labels in the next
training step. We further theoretically prove that this optimization cycle can
significantly reduce the generalization error under some conditions.
Additionally, we propose a class-aware adaptive augmentation module to further
improve the representation of minority classes, and an auxiliary branch to
maximize data utilization by leveraging all labeled and unlabeled samples.
Comprehensive evaluations on various commonly used benchmark datasets show that
CPG achieves consistent improvements, surpassing state-of-the-art methods by up
to \textbf{15.97\%} in accuracy. The code is available at
https://github.com/yaxinhou/CPG.

</details>


### [550] [SFANet: Spatial-Frequency Attention Network for Deepfake Detection](https://arxiv.org/abs/2510.04630)
*Vrushank Ahire,Aniruddh Muley,Shivam Zample,Siddharth Verma,Pranav Menon,Surbhi Madan,Abhinav Dhall*

Main category: cs.CV

TL;DR: 针对深度伪造检测问题，提出结合基于Transformer架构和基于纹理方法的集成框架，在DFWild - Cup数据集上取得SOTA性能，证明混合模型可有效应对挑战。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法在不同数据集和生成技术上泛化能力不足，需要更有效的检测方法。

Method: 提出新颖的集成框架，结合基于Transformer架构（如Swin Transformers和ViTs）和基于纹理的方法，采用创新的数据分割、顺序训练、频率分割、基于补丁的注意力和面部分割技术。

Result: 模型在DFWild - Cup数据集上达到了当前最优性能。

Conclusion: 混合模型能有效解决深度伪造检测中不断演变的挑战，可为实际应用提供强大解决方案。

Abstract: Detecting manipulated media has now become a pressing issue with the recent
rise of deepfakes. Most existing approaches fail to generalize across diverse
datasets and generation techniques. We thus propose a novel ensemble framework,
combining the strengths of transformer-based architectures, such as Swin
Transformers and ViTs, and texture-based methods, to achieve better detection
accuracy and robustness. Our method introduces innovative data-splitting,
sequential training, frequency splitting, patch-based attention, and face
segmentation techniques to handle dataset imbalances, enhance high-impact
regions (e.g., eyes and mouth), and improve generalization. Our model achieves
state-of-the-art performance when tested on the DFWild-Cup dataset, a diverse
subset of eight deepfake datasets. The ensemble benefits from the
complementarity of these approaches, with transformers excelling in global
feature extraction and texturebased methods providing interpretability. This
work demonstrates that hybrid models can effectively address the evolving
challenges of deepfake detection, offering a robust solution for real-world
applications.

</details>


### [551] [From Segments to Concepts: Interpretable Image Classification via Concept-Guided Segmentation](https://arxiv.org/abs/2510.04180)
*Ran Eisenberg,Amit Rozner,Ethan Fetaya,Ofir Lindenbaum*

Main category: cs.CV

TL;DR: 提出SEG - MIL - CBM框架，集成概念引导图像分割与注意力多实例学习，在多种场景表现稳健且提供透明概念级解释。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络决策黑盒性限制可解释性和信任度，现有模型缺乏透明度，概念瓶颈模型需昂贵概念注释且缺乏空间定位。

Method: 提出SEG - MIL - CBM框架，将概念引导图像分割集成到基于注意力的多实例学习框架中，将分割区域视为实例并聚合证据。

Result: SEG - MIL - CBM在涉及虚假关联、输入损坏和大规模基准测试等场景中实现了稳健性能。

Conclusion: SEG - MIL - CBM能在多种场景下提供稳健性能，同时给出透明的概念级解释，无需概念或组的注释。

Abstract: Deep neural networks have achieved remarkable success in computer vision;
however, their black-box nature in decision-making limits interpretability and
trust, particularly in safety-critical applications. Interpretability is
crucial in domains where errors have severe consequences. Existing models not
only lack transparency but also risk exploiting unreliable or misleading
features, which undermines both robustness and the validity of their
explanations. Concept Bottleneck Models (CBMs) aim to improve transparency by
reasoning through human-interpretable concepts. Still, they require costly
concept annotations and lack spatial grounding, often failing to identify which
regions support each concept. We propose SEG-MIL-CBM, a novel framework that
integrates concept-guided image segmentation into an attention-based multiple
instance learning (MIL) framework, where each segmented region is treated as an
instance and the model learns to aggregate evidence across them. By reasoning
over semantically meaningful regions aligned with high-level concepts, our
model highlights task-relevant evidence, down-weights irrelevant cues, and
produces spatially grounded, concept-level explanations without requiring
annotations of concepts or groups. SEG-MIL-CBM achieves robust performance
across settings involving spurious correlations (unintended dependencies
between background and label), input corruptions (perturbations that degrade
visual quality), and large-scale benchmarks, while providing transparent,
concept-level explanations.

</details>


### [552] [Detection of retinal diseases using an accelerated reused convolutional network](https://arxiv.org/abs/2510.04232)
*Amin Ahmadi Kasani,Hedieh Sajedi*

Main category: cs.CV

TL;DR: 本文旨在提升深度神经网络模型的可访问性，通过重新设计和优化卷积层创建了含ArConv层的新通用模型，该模型参数少、精度高，适用于手机。


<details>
  <summary>Details</summary>
Motivation: 卷积神经网络发展中提升其可访问性很重要，尤其在眼科疾病检测领域，以往方法计算复杂，因此要提高模型可访问性。

Method: 在最基础层面重新设计和优化卷积层，创建包含新型ArConv层的通用模型。

Result: 最终模型仅含130万参数，在RfMiD数据集上训练和评估时，比含220万参数的MobileNetV2模型精度更高，测试集精度达0.9328。

Conclusion: 通过优化卷积层创建的新模型具有合适复杂度，能高精度诊断疾病，适用于手机等设备。

Abstract: Convolutional neural networks are continually evolving, with some efforts
aimed at improving accuracy, others at increasing speed, and some at enhancing
accessibility. Improving accessibility broadens the application of neural
networks across a wider range of tasks, including the detection of eye
diseases. Early diagnosis of eye diseases and consulting an ophthalmologist can
prevent many vision disorders. Given the importance of this issue, various
datasets have been collected from the cornea to facilitate the process of
making neural network models. However, most of the methods introduced in the
past are computationally complex. In this study, we tried to increase the
accessibility of deep neural network models. We did this at the most
fundamental level, specifically by redesigning and optimizing the convolutional
layers. By doing so, we created a new general model that incorporates our novel
convolutional layer named ArConv layers. Thanks to the efficient performance of
this new layer, the model has suitable complexity for use in mobile phones and
can perform the task of diagnosing the presence of disease with high accuracy.
The final model we present contains only 1.3 million parameters. In comparison
to the MobileNetV2 model, which has 2.2 million parameters, our model
demonstrated better accuracy when trained and evaluated on the RfMiD dataset
under identical conditions, achieving an accuracy of 0.9328 versus 0.9266 on
the RfMiD test set.

</details>


### [553] [Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open Vocabulary Occupancy Prediction](https://arxiv.org/abs/2510.04759)
*Chi Yan,Dan Xu*

Main category: cs.CV

TL;DR: 提出PG - Occ框架用于开放词汇3D占用预测，采用渐进在线致密化和各向异性感知采样策略，性能达SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统方法限于固定语义类别，现有文本对齐场景建模存在稀疏表示难捕捉小物体、密集表示计算开销大的问题。

Method: 提出PG - Occ框架，采用渐进在线致密化策略逐步增强3D高斯表示，引入各向异性感知采样策略进行时空融合。

Result: PG - Occ相对之前最佳方法mIoU提升14.3%，达SOTA。

Conclusion: PG - Occ框架有效解决现有3D占用预测问题，性能优异。

Abstract: The 3D occupancy prediction task has witnessed remarkable progress in recent
years, playing a crucial role in vision-based autonomous driving systems. While
traditional methods are limited to fixed semantic categories, recent approaches
have moved towards predicting text-aligned features to enable open-vocabulary
text queries in real-world scenes. However, there exists a trade-off in
text-aligned scene modeling: sparse Gaussian representation struggles to
capture small objects in the scene, while dense representation incurs
significant computational overhead. To address these limitations, we present
PG-Occ, an innovative Progressive Gaussian Transformer Framework that enables
open-vocabulary 3D occupancy prediction. Our framework employs progressive
online densification, a feed-forward strategy that gradually enhances the 3D
Gaussian representation to capture fine-grained scene details. By iteratively
enhancing the representation, the framework achieves increasingly precise and
detailed scene understanding. Another key contribution is the introduction of
an anisotropy-aware sampling strategy with spatio-temporal fusion, which
adaptively assigns receptive fields to Gaussians at different scales and
stages, enabling more effective feature aggregation and richer scene
information capture. Through extensive evaluations, we demonstrate that PG-Occ
achieves state-of-the-art performance with a relative 14.3% mIoU improvement
over the previous best performing method. Code and pretrained models will be
released upon publication on our project page:
https://yanchi-3dv.github.io/PG-Occ

</details>


### [554] [DiT-VTON: Diffusion Transformer Framework for Unified Multi-Category Virtual Try-On and Virtual Try-All with Integrated Image Editing](https://arxiv.org/abs/2510.04797)
*Qi Li,Shuwen Qiu,Julien Han,Xingzi Xu,Mehmet Saygin Seyfioglu,Kee Kiat Koo,Karim Bouyarmane*

Main category: cs.CV

TL;DR: 提出DiT - VTON框架用于VTO任务，经实验在细节保留和鲁棒性上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 电商发展使VTO技术需求增长，但现有VTO模型在细节保留、鲁棒性等方面存在挑战。

Method: 提出DiT - VTON框架，探索多种DiT配置确定最佳VTO图像调节设置，在扩展数据集上训练模型。

Result: 在VITON - HD上超越现有方法，在包含数千产品类别的多样数据集上，VTA和图像编辑能力也更优。

Conclusion: DiT - VTON能解决现有VTO模型问题，实现VTA，支持高级图像编辑功能。

Abstract: The rapid growth of e-commerce has intensified the demand for Virtual Try-On
(VTO) technologies, enabling customers to realistically visualize products
overlaid on their own images. Despite recent advances, existing VTO models face
challenges with fine-grained detail preservation, robustness to real-world
imagery, efficient sampling, image editing capabilities, and generalization
across diverse product categories. In this paper, we present DiT-VTON, a novel
VTO framework that leverages a Diffusion Transformer (DiT), renowned for its
performance on text-conditioned image generation, adapted here for the
image-conditioned VTO task. We systematically explore multiple DiT
configurations, including in-context token concatenation, channel
concatenation, and ControlNet integration, to determine the best setup for VTO
image conditioning.
  To enhance robustness, we train the model on an expanded dataset encompassing
varied backgrounds, unstructured references, and non-garment categories,
demonstrating the benefits of data scaling for VTO adaptability. DiT-VTON also
redefines the VTO task beyond garment try-on, offering a versatile Virtual
Try-All (VTA) solution capable of handling a wide range of product categories
and supporting advanced image editing functionalities such as pose
preservation, localized editing, texture transfer, and object-level
customization. Experimental results show that our model surpasses
state-of-the-art methods on VITON-HD, achieving superior detail preservation
and robustness without reliance on additional condition encoders. It also
outperforms models with VTA and image editing capabilities on a diverse dataset
spanning thousands of product categories.

</details>


### [555] [Did you just see that? Arbitrary view synthesis for egocentric replay of operating room workflows from ambient sensors](https://arxiv.org/abs/2510.04802)
*Han Zhang,Lalithkumar Seenivasan,Jose L. Porras,Roger D. Soberanis-Mukul,Hao Ding,Hongchao Shu,Benjamin D. Killeen,Ankita Ghosh,Lonny Yarmus,Masaru Ishii,Angela Christine Argento,Mathias Unberath*

Main category: cs.CV

TL;DR: 介绍EgoSurg框架，可从壁装固定摄像头视频重建手术室人员的动态第一人称视角回放，经评估视觉质量高，为沉浸式手术数据科学奠定基础。


<details>
  <summary>Details</summary>
Motivation: 传统手术观察方法无法记录指导临床决策的第一人称视角，固定摄像头视频对手术决策洞察有限。

Method: EgoSurg将几何驱动的神经渲染与基于扩散的视图增强相结合。

Result: 在多站点手术案例和对照研究评估中，EgoSurg能以高视觉质量和保真度重建特定人员视野和任意视角。

Conclusion: EgoSurg将现有手术室摄像头基础设施转变为可导航的动态3D记录，为沉浸式手术数据科学建立新基础。

Abstract: Observing surgical practice has historically relied on fixed vantage points
or recollections, leaving the egocentric visual perspectives that guide
clinical decisions undocumented. Fixed-camera video can capture surgical
workflows at the room-scale, but cannot reconstruct what each team member
actually saw. Thus, these videos only provide limited insights into how
decisions that affect surgical safety, training, and workflow optimization are
made. Here we introduce EgoSurg, the first framework to reconstruct the
dynamic, egocentric replays for any operating room (OR) staff directly from
wall-mounted fixed-camera video, and thus, without intervention to clinical
workflow. EgoSurg couples geometry-driven neural rendering with diffusion-based
view enhancement, enabling high-visual fidelity synthesis of arbitrary and
egocentric viewpoints at any moment. In evaluation across multi-site surgical
cases and controlled studies, EgoSurg reconstructs person-specific visual
fields and arbitrary viewpoints with high visual quality and fidelity. By
transforming existing OR camera infrastructure into a navigable dynamic 3D
record, EgoSurg establishes a new foundation for immersive surgical data
science, enabling surgical practice to be visualized, experienced, and analyzed
from every angle.

</details>


### [556] [REN: Anatomically-Informed Mixture-of-Experts for Interstitial Lung Disease Diagnosis](https://arxiv.org/abs/2510.04923)
*Alec K. Peltekian,Halil Ertugrul Aktas,Gorkem Durak,Kevin Grudzinski,Bradford C. Bemiss,Carrie Richardson,Jane E. Dematte,G. R. Scott Budinger,Anthony J. Esposito,Alexander Misharin,Alok Choudhary,Ankit Agrawal,Ulas Bagci*

Main category: cs.CV

TL;DR: 提出适用于医学图像分类的区域专家网络REN，在间质性肺病分类中表现出色，有良好泛化性和临床可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统混合专家（MoE）系统缺乏医学影像所需的特定领域约束，需新框架处理医学图像分类。

Method: 利用解剖学先验训练七个专门的专家网络处理不同肺叶和双侧肺组合，用多模态门控机制整合放射组学生物标志物和深度学习特征。

Result: 应用于间质性肺病分类时，REN表现优于基线，区域特定专家模型也表现出色。

Conclusion: REN具有可扩展性、解剖学引导性，可推广到其他结构化医学成像应用。

Abstract: Mixture-of-Experts (MoE) architectures have significantly contributed to
scalable machine learning by enabling specialized subnetworks to tackle complex
tasks efficiently. However, traditional MoE systems lack domain-specific
constraints essential for medical imaging, where anatomical structure and
regional disease heterogeneity strongly influence pathological patterns. Here,
we introduce Regional Expert Networks (REN), the first anatomically-informed
MoE framework tailored specifically for medical image classification. REN
leverages anatomical priors to train seven specialized experts, each dedicated
to distinct lung lobes and bilateral lung combinations, enabling precise
modeling of region-specific pathological variations. Multi-modal gating
mechanisms dynamically integrate radiomics biomarkers and deep learning (DL)
features (CNN, ViT, Mamba) to weight expert contributions optimally. Applied to
interstitial lung disease (ILD) classification, REN achieves consistently
superior performance: the radiomics-guided ensemble reached an average AUC of
0.8646 +/- 0.0467, a +12.5 percent improvement over the SwinUNETR baseline (AUC
0.7685, p = 0.031). Region-specific experts further revealed that lower-lobe
models achieved AUCs of 0.88-0.90, surpassing DL counterparts (CNN: 0.76-0.79)
and aligning with known disease progression patterns. Through rigorous
patient-level cross-validation, REN demonstrates strong generalizability and
clinical interpretability, presenting a scalable, anatomically-guided approach
readily extensible to other structured medical imaging applications.

</details>


### [557] [Unsupervised Active Learning via Natural Feature Progressive Framework](https://arxiv.org/abs/2510.04939)
*Yuxi Liu,Catherine Lalman,Yimin Yang*

Main category: cs.CV

TL;DR: 传统深度学习需大量人工标注数据，主动学习（AL）和无监督主动学习（UAL）虽有改进但仍有不足，本文提出自然特征渐进框架（NFPF），实验表明其优于现有UAL方法，性能与监督AL方法相当。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习依赖大规模人工标注数据集，成本高且耗时，现有UAL方法难以达到最优性能，需改进。

Method: 提出自然特征渐进框架（NFPF），使用特定特征学习机（SFLM）量化样本对模型性能的贡献，并定义重建差异指标进行初始样本选择。

Result: NFPF显著优于所有已有的UAL方法，在视觉数据集上性能与监督AL方法相当，消融研究和可视化证明其性能优越、鲁棒性增强、数据分布覆盖更好。

Conclusion: NFPF是一种有效的UAL方法，能提高样本选择效率和模型性能。

Abstract: The effectiveness of modern deep learning models is predicated on the
availability of large-scale, human-annotated datasets, a process that is
notoriously expensive and time-consuming. While Active Learning (AL) offers a
strategic solution by labeling only the most informative and representative
data, its iterative nature still necessitates significant human involvement.
Unsupervised Active Learning (UAL) presents an alternative by shifting the
annotation burden to a single, post-selection step. Unfortunately, prevailing
UAL methods struggle to achieve state-of-the-art performance. These approaches
typically rely on local, gradient-based scoring for sample importance
estimation, which not only makes them vulnerable to ambiguous and noisy data
but also hinders their capacity to select samples that adequately represent the
full data distribution. Moreover, their use of shallow, one-shot linear
selection falls short of a true UAL paradigm. In this paper, we propose the
Natural Feature Progressive Framework (NFPF), a UAL method that revolutionizes
how sample importance is measured. At its core, NFPF employs a Specific Feature
Learning Machine (SFLM) to effectively quantify each sample's contribution to
model performance. We further utilize the SFLM to define a powerful
Reconstruction Difference metric for initial sample selection. Our
comprehensive experiments show that NFPF significantly outperforms all
established UAL methods and achieves performance on par with supervised AL
methods on vision datasets. Detailed ablation studies and qualitative
visualizations provide compelling evidence for NFPF's superior performance,
enhanced robustness, and improved data distribution coverage.

</details>


### [558] [Bidirectional Mammogram View Translation with Column-Aware and Implicit 3D Conditional Diffusion](https://arxiv.org/abs/2510.04947)
*Xin Li,Kaixiang Yang,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: 本文提出CA3D - Diff框架用于双视图乳腺X光图像视图转换，实验表明其性能优越且对单视图恶性分类有帮助。


<details>
  <summary>Details</summary>
Motivation: 实际临床中乳腺X光图像可能存在单视图缺失等问题，且视图转换任务因X光投影特性具有挑战性。

Method: 提出基于条件扩散模型的CA3D - Diff框架，设计列感知交叉注意力机制，引入隐式3D结构重建模块。

Result: CA3D - Diff在双向任务中表现优越，在视觉保真度和结构一致性上超过现有方法，合成视图能有效提升单视图恶性分类效果。

Conclusion: CA3D - Diff方法在乳腺X光图像视图转换中有实际诊断价值。

Abstract: Dual-view mammography, including craniocaudal (CC) and mediolateral oblique
(MLO) projections, offers complementary anatomical views crucial for breast
cancer diagnosis. However, in real-world clinical workflows, one view may be
missing, corrupted, or degraded due to acquisition errors or compression
artifacts, limiting the effectiveness of downstream analysis. View-to-view
translation can help recover missing views and improve lesion alignment. Unlike
natural images, this task in mammography is highly challenging due to large
non-rigid deformations and severe tissue overlap in X-ray projections, which
obscure pixel-level correspondences. In this paper, we propose Column-Aware and
Implicit 3D Diffusion (CA3D-Diff), a novel bidirectional mammogram view
translation framework based on conditional diffusion model. To address
cross-view structural misalignment, we first design a column-aware
cross-attention mechanism that leverages the geometric property that
anatomically corresponding regions tend to lie in similar column positions
across views. A Gaussian-decayed bias is applied to emphasize local column-wise
correlations while suppressing distant mismatches. Furthermore, we introduce an
implicit 3D structure reconstruction module that back-projects noisy 2D latents
into a coarse 3D feature volume based on breast-view projection geometry. The
reconstructed 3D structure is refined and injected into the denoising UNet to
guide cross-view generation with enhanced anatomical awareness. Extensive
experiments demonstrate that CA3D-Diff achieves superior performance in
bidirectional tasks, outperforming state-of-the-art methods in visual fidelity
and structural consistency. Furthermore, the synthesized views effectively
improve single-view malignancy classification in screening settings,
demonstrating the practical value of our method in real-world diagnostics.

</details>


### [559] [ActiveMark: on watermarking of visual foundation models via massive activations](https://arxiv.org/abs/2510.04966)
*Anna Chistyakova,Mikhail Pautov*

Main category: cs.CV

TL;DR: 本文提出一种视觉基础模型所有权验证方法，嵌入数字水印，理论和实验表明该方法误检率低。


<details>
  <summary>Details</summary>
Motivation: 视觉基础模型训练成本高，存在非法再分发问题，需要可靠的所有权验证工具。

Method: 微调视觉基础模型的一组表达层和一个小的编解码器网络，将数字水印嵌入保留输入图像集的内部表示。

Result: 理论和实验证明该方法对无水印模型的误检概率低，对有水印模型的错误漏检概率低。

Conclusion: 所提出的方法可有效进行视觉基础模型的所有权验证。

Abstract: Being trained on large and vast datasets, visual foundation models (VFMs) can
be fine-tuned for diverse downstream tasks, achieving remarkable performance
and efficiency in various computer vision applications. The high computation
cost of data collection and training motivates the owners of some VFMs to
distribute them alongside the license to protect their intellectual property
rights. However, a dishonest user of the protected model's copy may illegally
redistribute it, for example, to make a profit. As a consequence, the
development of reliable ownership verification tools is of great importance
today, since such methods can be used to differentiate between a redistributed
copy of the protected model and an independent model. In this paper, we propose
an approach to ownership verification of visual foundation models by
fine-tuning a small set of expressive layers of a VFM along with a small
encoder-decoder network to embed digital watermarks into an internal
representation of a hold-out set of input images. Importantly, the watermarks
embedded remain detectable in the functional copies of the protected model,
obtained, for example, by fine-tuning the VFM for a particular downstream task.
Theoretically and experimentally, we demonstrate that the proposed method
yields a low probability of false detection of a non-watermarked model and a
low probability of false misdetection of a watermarked model.

</details>


### [560] [Beyond the Seen: Bounded Distribution Estimation for Open-Vocabulary Learning](https://arxiv.org/abs/2510.04770)
*Xiaomeng Fan,Yuchuan Mao,Zhi Gao,Yuwei Wu,Jin Chen,Yunde Jia*

Main category: cs.CV

TL;DR: 本文提出一种新的开放词汇学习方法，通过生成未见类数据估计开放环境分布，实验表明优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法用可见类数据估计开放环境分布存在误差且难以识别，需超越可见类进行学习以限制误差。

Method: 提出由类域数据生成管道和分布对齐算法组成的方法，数据生成管道在分层语义树和从可见类数据推断的域信息指导下生成未见类数据，分布对齐算法用生成数据估计并最大化后验概率。

Result: 在11个数据集上的实验显示该方法比基线方法最高高出14%。

Conclusion: 所提方法在开放词汇学习中有效且具有优越性。

Abstract: Open-vocabulary learning requires modeling the data distribution in open
environments, which consists of both seen-class and unseen-class data.
  Existing methods estimate the distribution in open environments using
seen-class data, where the absence of unseen classes makes the estimation error
inherently unidentifiable.
  Intuitively, learning beyond the seen classes is crucial for distribution
estimation to bound the estimation error.
  We theoretically demonstrate that the distribution can be effectively
estimated by generating unseen-class data, through which the estimation error
is upper-bounded.
  Building on this theoretical insight, we propose a novel open-vocabulary
learning method, which generates unseen-class data for estimating the
distribution in open environments. The method consists of a class-domain-wise
data generation pipeline and a distribution alignment algorithm. The data
generation pipeline generates unseen-class data under the guidance of a
hierarchical semantic tree and domain information inferred from the seen-class
data, facilitating accurate distribution estimation. With the generated data,
the distribution alignment algorithm estimates and maximizes the posterior
probability to enhance generalization in open-vocabulary learning. Extensive
experiments on $11$ datasets demonstrate that our method outperforms baseline
approaches by up to $14\%$, highlighting its effectiveness and superiority.

</details>


### [561] [Federated Learning for Surgical Vision in Appendicitis Classification: Results of the FedSurg EndoVis 2024 Challenge](https://arxiv.org/abs/2510.04772)
*Max Kirchner,Hanna Hoffmann,Alexander C. Jenke,Oliver L. Saldanha,Kevin Pfeiffer,Weam Kanjo,Julia Alekseenko,Claas de Boer,Santhi Raj Kolamuri,Lorenzo Mazza,Nicolas Padoy,Sophia Bano,Annika Reinke,Lena Maier-Hein,Danail Stoyanov,Jakob N. Kather,Fiona R. Kolbinger,Sebastian Bodenstedt,Stefanie Speidel*

Main category: cs.CV

TL;DR: FedSurg挑战对手术视频分类的联邦学习进行基准测试，评估当前方法在未见过临床中心的泛化能力及微调适应能力，指出存在的问题并强调架构等方面重要性，为未来方法开发提供参考。


<details>
  <summary>Details</summary>
Motivation: 评估当前联邦学习方法在手术视频分类中对未见过临床中心的泛化能力，实现不共享患者数据的协作模型开发。

Method: 参与者用多中心附录300视频数据集对阑尾炎炎症阶段分类，评估泛化和微调后适应两个任务，采用多种方法和聚合方案，用F1分数和预期成本评估性能。

Result: 泛化任务中各中心性能有限，适应任务微调后各队有提升但排名稳定性低，ViViT提交方案表现最佳，凸显泛化、类别不平衡和超参数调整问题，时空建模和上下文感知预处理有前景。

Conclusion: FedSurg挑战为手术视频分类的联邦学习策略建立首个基准，强调局部个性化和全局鲁棒性的权衡，架构选择等很重要，为未来临床手术AI方法开发提供参考。

Abstract: Purpose: The FedSurg challenge was designed to benchmark the state of the art
in federated learning for surgical video classification. Its goal was to assess
how well current methods generalize to unseen clinical centers and adapt
through local fine-tuning while enabling collaborative model development
without sharing patient data. Methods: Participants developed strategies to
classify inflammation stages in appendicitis using a preliminary version of the
multi-center Appendix300 video dataset. The challenge evaluated two tasks:
generalization to an unseen center and center-specific adaptation after
fine-tuning. Submitted approaches included foundation models with linear
probing, metric learning with triplet loss, and various FL aggregation schemes
(FedAvg, FedMedian, FedSAM). Performance was assessed using F1-score and
Expected Cost, with ranking robustness evaluated via bootstrapping and
statistical testing. Results: In the generalization task, performance across
centers was limited. In the adaptation task, all teams improved after
fine-tuning, though ranking stability was low. The ViViT-based submission
achieved the strongest overall performance. The challenge highlighted
limitations in generalization, sensitivity to class imbalance, and difficulties
in hyperparameter tuning in decentralized training, while spatiotemporal
modeling and context-aware preprocessing emerged as promising strategies.
Conclusion: The FedSurg Challenge establishes the first benchmark for
evaluating FL strategies in surgical video classification. Findings highlight
the trade-off between local personalization and global robustness, and
underscore the importance of architecture choice, preprocessing, and loss
design. This benchmarking offers a reference point for future development of
imbalance-aware, adaptive, and robust FL methods in clinical surgical AI.

</details>


### [562] [Beyond Random: Automatic Inner-loop Optimization in Dataset Distillation](https://arxiv.org/abs/2510.04838)
*Muquan Li,Hang Gou,Dongyang Zhang,Shuang Liang,Xiurui Xie,Deqiang Ouyang,Ke Qin*

Main category: cs.CV

TL;DR: 提出新颖框架AT - BPTT用于数据集蒸馏，实验显示其性能优、加速优化且节省内存。


<details>
  <summary>Details</summary>
Motivation: 现有数据集蒸馏的内循环优化方法依赖随机截断策略，缺乏灵活性且结果欠佳，因神经网络不同训练阶段学习动态不同，随机截断无效。

Method: 提出AT - BPTT框架，包含阶段感知时间步选择的概率机制、基于梯度变化的自适应窗口大小策略和低秩Hessian近似以降低计算开销。

Result: 在多个数据集上实验，AT - BPTT达到了最先进性能，平均准确率比基线方法提高6.16%，加速内循环优化3.9倍，节省63%内存成本。

Conclusion: AT - BPTT是一种有效的数据集蒸馏内循环优化方法，能提升性能、加速优化并节省内存。

Abstract: The growing demand for efficient deep learning has positioned dataset
distillation as a pivotal technique for compressing training dataset while
preserving model performance. However, existing inner-loop optimization methods
for dataset distillation typically rely on random truncation strategies, which
lack flexibility and often yield suboptimal results. In this work, we observe
that neural networks exhibit distinct learning dynamics across different
training stages-early, middle, and late-making random truncation ineffective.
To address this limitation, we propose Automatic Truncated Backpropagation
Through Time (AT-BPTT), a novel framework that dynamically adapts both
truncation positions and window sizes according to intrinsic gradient behavior.
AT-BPTT introduces three key components: (1) a probabilistic mechanism for
stage-aware timestep selection, (2) an adaptive window sizing strategy based on
gradient variation, and (3) a low-rank Hessian approximation to reduce
computational overhead. Extensive experiments on CIFAR-10, CIFAR-100,
Tiny-ImageNet, and ImageNet-1K show that AT-BPTT achieves state-of-the-art
performance, improving accuracy by an average of 6.16% over baseline methods.
Moreover, our approach accelerates inner-loop optimization by 3.9x while saving
63% memory cost.

</details>


### [563] [Paper2Video: Automatic Video Generation from Scientific Papers](https://arxiv.org/abs/2510.05096)
*Zeyu Zhu,Kevin Qinghong Lin,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 提出PaperTalker基准和多智能体框架用于学术演示视频生成，实验表明效果优于基线。


<details>
  <summary>Details</summary>
Motivation: 学术演示视频制作劳动密集，且与自然视频生成面临不同挑战，需要解决这些问题以实现自动化生成。

Method: 引入101篇研究论文及相关数据的PaperTalker基准，设计四个评估指标，提出多智能体框架，集成多种功能并并行生成。

Result: 在Paper2Video上的实验显示，生成的演示视频比现有基线更忠实、信息更丰富。

Conclusion: 朝着自动化、可用的学术视频生成迈出了实际一步，相关数据集、智能体和代码已开源。

Abstract: Academic presentation videos have become an essential medium for research
communication, yet producing them remains highly labor-intensive, often
requiring hours of slide design, recording, and editing for a short 2 to 10
minutes video. Unlike natural video, presentation video generation involves
distinctive challenges: inputs from research papers, dense multi-modal
information (text, figures, tables), and the need to coordinate multiple
aligned channels such as slides, subtitles, speech, and human talker. To
address these challenges, we introduce PaperTalker, the first benchmark of 101
research papers paired with author-created presentation videos, slides, and
speaker metadata. We further design four tailored evaluation metrics--Meta
Similarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos
convey the paper's information to the audience. Building on this foundation, we
propose PaperTalker, the first multi-agent framework for academic presentation
video generation. It integrates slide generation with effective layout
refinement by a novel effective tree search visual choice, cursor grounding,
subtitling, speech synthesis, and talking-head rendering, while parallelizing
slide-wise generation for efficiency. Experiments on Paper2Video demonstrate
that the presentation videos produced by our approach are more faithful and
informative than existing baselines, establishing a practical step toward
automated and ready-to-use academic video generation. Our dataset, agent, and
code are available at https://github.com/showlab/Paper2Video.

</details>


### [564] [ERDE: Entropy-Regularized Distillation for Early-exit](https://arxiv.org/abs/2510.04856)
*Martial Guidez,Stefan Duffner,Yannick Alpou,Oscar Röth,Christophe Garcia*

Main category: cs.CV

TL;DR: 本文提出结合早期退出和知识蒸馏的方法，优化准确率和效率的权衡，在图像分类数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在图像分类中计算成本高，不适合实时和边缘应用，需要压缩技术降低成本并保持准确率。

Method: 有效整合早期退出和知识蒸馏两种优化技术，训练学生早期退出模型时，在传统知识蒸馏损失基础上加入基于熵的损失。

Result: 在CIFAR10、CIFAR100和SVHN图像分类数据集上的实验结果证实了方法的有效性。

Conclusion: 该方法优化了准确率和效率的权衡，降低了计算复杂度且不影响分类性能，为知识蒸馏在其他场景的研究开辟了新视角。

Abstract: Although deep neural networks and in particular Convolutional Neural Networks
have demonstrated state-of-the-art performance in image classification with
relatively high efficiency, they still exhibit high computational costs, often
rendering them impractical for real-time and edge applications. Therefore, a
multitude of compression techniques have been developed to reduce these costs
while maintaining accuracy. In addition, dynamic architectures have been
introduced to modulate the level of compression at execution time, which is a
desirable property in many resource-limited application scenarios. The proposed
method effectively integrates two well-established optimization techniques:
early exits and knowledge distillation, where a reduced student early-exit
model is trained from a more complex teacher early-exit model. The primary
contribution of this research lies in the approach for training the student
early-exit model. In comparison to the conventional Knowledge Distillation
loss, our approach incorporates a new entropy-based loss for images where the
teacher's classification was incorrect. The proposed method optimizes the
trade-off between accuracy and efficiency, thereby achieving significant
reductions in computational complexity without compromising classification
performance. The validity of this approach is substantiated by experimental
results on image classification datasets CIFAR10, CIFAR100 and SVHN, which
further opens new research perspectives for Knowledge Distillation in other
contexts.

</details>


### [565] [BenthiCat: An opti-acoustic dataset for advancing benthic classification and habitat mapping](https://arxiv.org/abs/2510.04876)
*Hayat Rajani,Valerio Franchi,Borja Martinez-Clavel Valles,Raimon Ramos,Rafael Garcia,Nuno Gracias*

Main category: cs.CV

TL;DR: 本文引入多模态数据集用于海底栖息地映射研究，提供标注和工具以推动相关领域发展。


<details>
  <summary>Details</summary>
Motivation: 海底栖息地映射领域缺乏大型标注数据集，限制机器学习模型发展和基准测试。

Method: 引入含约百万侧扫声呐图块的多模态数据集，部分图块手动标注，将光学图像与声呐图块空间关联实现跨模态表征学习，提供开源预处理和标注工具。

Result: 发布数据集、原始传感器数据及镶嵌图，实现跨模态表征学习。

Conclusion: 该资源有望为水下栖息地映射建立标准化基准，促进自主海底分类和多传感器集成发展。

Abstract: Benthic habitat mapping is fundamental for understanding marine ecosystems,
guiding conservation efforts, and supporting sustainable resource management.
Yet, the scarcity of large, annotated datasets limits the development and
benchmarking of machine learning models in this domain. This paper introduces a
thorough multi-modal dataset, comprising about a million side-scan sonar (SSS)
tiles collected along the coast of Catalonia (Spain), complemented by
bathymetric maps and a set of co-registered optical images from targeted
surveys using an autonomous underwater vehicle (AUV). Approximately \num{36000}
of the SSS tiles have been manually annotated with segmentation masks to enable
supervised fine-tuning of classification models. All the raw sensor data,
together with mosaics, are also released to support further exploration and
algorithm development. To address challenges in multi-sensor data fusion for
AUVs, we spatially associate optical images with corresponding SSS tiles,
facilitating self-supervised, cross-modal representation learning. Accompanying
open-source preprocessing and annotation tools are provided to enhance
accessibility and encourage research. This resource aims to establish a
standardized benchmark for underwater habitat mapping, promoting advancements
in autonomous seafloor classification and multi-sensor integration.

</details>


### [566] [Comparative Analysis of YOLOv5, Faster R-CNN, SSD, and RetinaNet for Motorbike Detection in Kigali Autonomous Driving Context](https://arxiv.org/abs/2510.04912)
*Ngeyen Yinkfu,Sunday Nwovu,Jonathan Kayizzi,Angelique Uwamahoro*

Main category: cs.CV

TL;DR: 研究在卢旺达基加利用198张自定义图像数据集比较四个目标检测模型用于摩托车检测，评估其在资源受限环境实时导航适用性，指出挑战并给出建议。


<details>
  <summary>Details</summary>
Motivation: 卢旺达基加利的摩托车出租车行驶无规律且不遵守交规，给自动驾驶系统带来挑战，需合适的目标检测模型。

Method: 使用198张自定义图像数据集，在PyTorch中用迁移学习实现YOLOv5、Faster R - CNN、SSD和RetinaNet四个目标检测模型，评估其准确性、定位和推理速度。

Result: 确定了实施挑战，包括数据集限制和模型复杂性。

Conclusion: 建议未来采用简化架构，提高发展中国家自动驾驶系统的可及性。

Abstract: In Kigali, Rwanda, motorcycle taxis are a primary mode of transportation,
often navigating unpredictably and disregarding traffic rules, posing
significant challenges for autonomous driving systems. This study compares four
object detection models--YOLOv5, Faster R-CNN, SSD, and RetinaNet--for
motorbike detection using a custom dataset of 198 images collected in Kigali.
Implemented in PyTorch with transfer learning, the models were evaluated for
accuracy, localization, and inference speed to assess their suitability for
real-time navigation in resource-constrained settings. We identify
implementation challenges, including dataset limitations and model
complexities, and recommend simplified architectures for future work to enhance
accessibility for autonomous systems in developing countries like Rwanda.

</details>


### [567] [Latent Uncertainty Representations for Video-based Driver Action and Intention Recognition](https://arxiv.org/abs/2510.05006)
*Koen Vellenga,H. Joe Steinhauer,Jonas Andersson,Anders Sjögren*

Main category: cs.CV

TL;DR: 提出用转换层扩展预训练DNN以估计不确定性的方法LUR和RLUR，在视频驾驶员动作和意图识别数据集上评估，结果显示其在分布内分类表现相当，OOD检测高效易调。


<details>
  <summary>Details</summary>
Motivation: 现有LL - PDL方法检测OOD实例性能有差异，需更好方法。

Method: 用转换层扩展预训练DNN产生多个潜在表征估计不确定性，提出LUR和RLUR方法，与八种PDL方法在四个数据集上对比评估。

Result: LUR和RLUR在分布内分类性能与其他LL - PDL方法相当，OOD检测匹配顶级PDL方法，训练更高效、更易调优。

Conclusion: LUR和RLUR在视频驾驶员动作和意图识别任务中是有效的不确定性估计方法。

Abstract: Deep neural networks (DNNs) are increasingly applied to safety-critical tasks
in resource-constrained environments, such as video-based driver action and
intention recognition. While last layer probabilistic deep learning (LL-PDL)
methods can detect out-of-distribution (OOD) instances, their performance
varies. As an alternative to last layer approaches, we propose extending
pre-trained DNNs with transformation layers to produce multiple latent
representations to estimate the uncertainty. We evaluate our latent uncertainty
representation (LUR) and repulsively trained LUR (RLUR) approaches against
eight PDL methods across four video-based driver action and intention
recognition datasets, comparing classification performance, calibration, and
uncertainty-based OOD detection. We also contribute 28,000 frame-level action
labels and 1,194 video-level intention labels for the NuScenes dataset. Our
results show that LUR and RLUR achieve comparable in-distribution
classification performance to other LL-PDL approaches. For uncertainty-based
OOD detection, LUR matches top-performing PDL methods while being more
efficient to train and easier to tune than approaches that require Markov-Chain
Monte Carlo sampling or repulsive training procedures.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [568] [Embedding Sustainability in Software Engineering Curriculum: A Case Study](https://arxiv.org/abs/2510.03321)
*Ruzanna Chitchyan,Niki Mahmoudi*

Main category: cs.CY

TL;DR: 本文通过案例研究探讨某大学软件工程专业课程中融入可持续性理念的情况，提出支持其他院校嵌入该理念的实践步骤及课程整合策略。


<details>
  <summary>Details</summary>
Motivation: 可持续性融入软件工程课程存在挑战，需探索有效方法。

Method: 以某大学软件工程专业为案例，依据可持续性意识框架的五个维度、针对性讨论问题和绿色软件基金会模式的良好实践案例，让学者和学生共同确定融入机会。

Result: 得出包括使用框架、示例、学生参与和迭代协商过程等实践步骤。

Conclusion: 将可持续性融入软件工程课程是培养具有可持续意识专业人才的必要且紧迫的步骤。

Abstract: Sustainability is increasingly recognized as a critical dimension of
engineering education, yet its integration into Software Engineering curricula
remains a challenge. This paper reports on a case study that examines how
sustainability is being embedded across modules in the Software Engineering
program at one university. The paper outlines the process through which
academics and students co-identified opportunities for integration, guided by
the five dimensions of the Sustainability Awareness Framework, targeted
discussion questions, and good practice examples drawn from the Green Software
Foundation patterns. The study highlights practical steps - including the use
of frameworks, illustrative examples, student engagement, and iterative
consultative processes - that can support other institutions seeking to embed
sustainability into their programs. We also discuss strategies for integrating
sustainability into the Software Engineering curriculum and argue that such
integration is a necessary and urgent step to prepare Software Engineering
graduates as sustainability-aware professionals in our changing society.

</details>


### [569] [Intelligent Healthcare Ecosystems: Optimizing the Iron Triangle of Healthcare (Access, Cost, Quality)](https://arxiv.org/abs/2510.03331)
*Vivek Acharya*

Main category: cs.CY

TL;DR: 美国医疗面临成本、质量和可及性的权衡，本文提出智能医疗生态系统（iHE），通过相关技术降低成本、提高质量和可及性。


<details>
  <summary>Details</summary>
Motivation: 美国医疗花费高但存在可及性和结果不均问题，“铁三角”权衡促使系统级重新设计。

Method: 对近期文献和政策报告进行叙述性回顾。

Result: 概述iHE核心组件，表明其能减少浪费、个性化医疗、支持基于价值的支付，同时应对隐私、偏见和采用挑战。

Conclusion: 协调的iHE能改善“铁三角”问题，使医疗更易获取、负担得起且高质量。

Abstract: The United States spends nearly 17% of GDP on healthcare yet continues to
face uneven access and outcomes. This well-known trade-off among cost, quality,
and access - the "iron triangle" - motivates a system-level redesign. This
paper proposes an Intelligent Healthcare Ecosystem (iHE): an integrated,
data-driven framework that uses generative AI and large language models,
federated learning, interoperability standards (FHIR, TEFCA), and digital twins
to improve access and quality while lowering cost. We review historical
spending trends, waste, and international comparisons; introduce a value
equation that jointly optimizes access, quality, and cost; and synthesize
evidence on the enabling technologies and operating model for iHE. Methods
follow a narrative review of recent literature and policy reports. Results
outline core components (AI decision support, interoperability, telehealth,
automation) and show how iHE can reduce waste, personalize care, and support
value-based payment while addressing privacy, bias, and adoption challenges. We
argue that a coordinated iHE can bend - if not break - the iron triangle,
moving the system toward care that is more accessible, affordable, and high
quality.

</details>


### [570] [Defining a Strategic Action Plan for AI in Higher Education](https://arxiv.org/abs/2510.03343)
*Nikolaos Avouris*

Main category: cs.CY

TL;DR: 文章聚焦高等教育机构，探讨人工智能在教育中的关键挑战，提出框架、战略行动、部署计划并举例。


<details>
  <summary>Details</summary>
Motivation: 探讨人工智能在高等教育机构应用中的关键挑战。

Method: 先回顾国际组织规范行动和技术现状问题，接着提出包含五个关键维度的框架及五个战略行动，将行动与利益相关者对应并制定部署计划。

Result: 提出包含挑战、行动、利益相关者、部署（CASD）维度的框架，给出机构和课程层面人工智能具体行动示例。

Conclusion: 给出应对人工智能在高等教育中发展的框架、行动及部署计划。

Abstract: This paper discusses key challenges of Artificial Intelligence in Education,
with main focus on higher education institutions. We start with reviewing
normative actions of international organizations and concerns expressed about
the current technical landscape. Then we proceed with proposing a framework
that comprises five key dimensions relating to the main challenges relating to
AI in higher education institutions, followed by five key strategic actions
that the main stakeholders need to take in order to address the current
developments. We map these actions to the main stakeholders of higher education
and propose a deployment plan. This defines a framework along the dimensions:
Challenges, Actions, Stakeholders, Deployment CASD. Examples of AI specific
actions at the institutional and individual course level are also provided and
discussed.

</details>


### [571] [An Adaptive Responsible AI Governance Framework for Decentralized Organizations](https://arxiv.org/abs/2510.03368)
*Kiana Jafari Meimandi,Anka Reuel,Gabriela Aranguiz-Dias,Hatim Rahama,Ala-Eddine Ayadi,Xavier Boullier,Jérémy Verdo,Louis Montanie,Mykel Kochenderfer*

Main category: cs.CY

TL;DR: 通过高校与企业合作案例研究全球分散组织中负责任AI治理评估挑战，揭示四个关键模式，提出ARGO框架并给出实践指导。


<details>
  <summary>Details</summary>
Motivation: 现有负责任AI框架在复杂分布式决策组织中的应用研究不足，需研究评估挑战。

Method: 通过领先研究型大学与跨国企业的案例研究，对多个业务单元和AI用例进行负责任AI评估。

Result: 揭示影响负责任AI实施的四个关键模式，提出ARGO框架。

Conclusion: 强调模块化治理方法对适应组织复杂性并符合负责任AI原则的重要性，为分散结构组织提供实践指导。

Abstract: This paper examines the assessment challenges of Responsible AI (RAI)
governance efforts in globally decentralized organizations through a case study
collaboration between a leading research university and a multinational
enterprise. While there are many proposed frameworks for RAI, their application
in complex organizational settings with distributed decision-making authority
remains underexplored. Our RAI assessment, conducted across multiple business
units and AI use cases, reveals four key patterns that shape RAI
implementation: (1) complex interplay between group-level guidance and local
interpretation, (2) challenges translating abstract principles into operational
practices, (3) regional and functional variation in implementation approaches,
and (4) inconsistent accountability in risk oversight. Based on these findings,
we propose an Adaptive RAI Governance (ARGO) Framework that balances central
coordination with local autonomy through three interdependent layers: shared
foundation standards, central advisory resources, and contextual local
implementation. We contribute insights from academic-industry collaboration for
RAI assessments, highlighting the importance of modular governance approaches
that accommodate organizational complexity while maintaining alignment with
responsible AI principles. These lessons offer practical guidance for
organizations navigating the transition from RAI principles to operational
practice within decentralized structures.

</details>


### [572] [TriQuest:An AI Copilot-Powered Platform for Interdisciplinary Curriculum Design](https://arxiv.org/abs/2510.03369)
*Huazhen Wang,Huimin Yang,Hainbin Lin,Yan Dong,Lili Chen,Liangliang Xia,Wenwen Xu*

Main category: cs.CY

TL;DR: 介绍TriQuest AI辅助平台解决跨学科教学实施难题，研究显示其提升课程设计效率和质量，为教师专业发展提供新范式。


<details>
  <summary>Details</summary>
Motivation: 跨学科教学实施受知识整合和备课耗时问题阻碍，现有工具缺乏教学和特定领域深度。

Method: 引入TriQuest平台，利用大语言模型和知识图谱，通过直观GUI，具备跨学科知识智能整合和人机协作审核功能。

Result: 43名教师参与研究，TriQuest平均提高课程设计效率75%，提升教案质量得分41%，降低设计障碍和认知负荷。

Conclusion: 本研究为利用智能技术促进教师专业发展提供新范式。

Abstract: Interdisciplinary teaching is a cornerstone of modern curriculum reform, but
its implementation is hindered by challenges in knowledge integration and
time-consuming lesson planning. Existing tools often lack the required
pedagogical and domain-specific depth.We introduce TriQuest, an AI-copilot
platform designed to solve these problems. TriQuest uses large language models
and knowledge graphs via an intuitive GUI to help teachers efficiently generate
high-quality interdisciplinary lesson plans. Its core features include
intelligent knowledge integration from various disciplines and a human-computer
collaborative review process to ensure quality and innovation.In a study with
43 teachers, TriQuest increased curriculum design efficiency by an average of
75% and improved lesson plan quality scores by 41%. It also significantly
lowered design barriers and cognitive load. Our work presents a new paradigm
for empowering teacher professional development with intelligent technologies.

</details>


### [573] [Lightweight Prompt Engineering for Cognitive Alignment in Educational AI: A OneClickQuiz Case Study](https://arxiv.org/abs/2510.03374)
*Antoun Yaacoub,Zainab Assaghir,Jérôme Da-Rugna*

Main category: cs.CY

TL;DR: 研究轻量级提示工程策略对OneClickQuiz中AI生成问题认知一致性的影响，指出明确详细提示对精确认知对齐至关重要。


<details>
  <summary>Details</summary>
Motivation: AI融入教育技术虽有潜力，但AI生成内容质量和教学一致性是关键挑战，故研究提示工程策略影响。

Method: 在OneClickQuiz中评估三种提示变体（详细基线、简化版本、基于角色的方法）在布鲁姆分类法不同层级的表现，采用自动分类模型和人工评审。

Result: 明确详细提示对精确认知对齐至关重要，简单和基于角色的提示常与预期布鲁姆层级不一致。

Conclusion: 强调战略提示工程对培养教学合理的AI教育解决方案的重要性，并建议优化AI以实现学习分析和智能学习环境中的高质量内容生成。

Abstract: The rapid integration of Artificial Intelligence (AI) into educational
technology promises to revolutionize content creation and assessment. However,
the quality and pedagogical alignment of AI-generated content remain critical
challenges. This paper investigates the impact of lightweight prompt
engineering strategies on the cognitive alignment of AI-generated questions
within OneClickQuiz, a Moodle plugin leveraging generative AI. We evaluate
three prompt variants-a detailed baseline, a simpler version, and a
persona-based approach-across Knowledge, Application, and Analysis levels of
Bloom's Taxonomy. Utilizing an automated classification model (from prior work)
and human review, our findings demonstrate that explicit, detailed prompts are
crucial for precise cognitive alignment. While simpler and persona-based
prompts yield clear and relevant questions, they frequently misalign with
intended Bloom's levels, generating outputs that are either too complex or
deviate from the desired cognitive objective. This study underscores the
importance of strategic prompt engineering in fostering pedagogically sound
AI-driven educational solutions and advises on optimizing AI for quality
content generation in learning analytics and smart learning environments.

</details>


### [574] [Can an AI-Powered Presentation Platform Based On The Game "Just a Minute" Be Used To Improve Students' Public Speaking Skills?](https://arxiv.org/abs/2510.03379)
*Frederic Higham,Tommy Yuan*

Main category: cs.CY

TL;DR: 研究将AI和游戏化应用于演讲平台提升大学生母语演讲能力，以JAM平台为例，招募学生评估，结果显示学生认为游戏有前景，但需更多研究证明长期有效性。


<details>
  <summary>Details</summary>
Motivation: 解决大学生公开演讲面临的困难尤其是焦虑问题，在已有AI - PPPs研究基础上，探索JAM平台提升学生演讲技能和信心的有效性。

Method: 招募约克大学学生，让其填写问卷、玩两次JAM游戏后再填问卷，收集游戏过程中的统计数据。

Result: 学生认为游戏有前景，相信长期玩游戏能提升演讲技能。

Conclusion: 需要开展更多工作以证明游戏的长期有效性。

Abstract: This study explores the effectiveness of applying AI and gamification into a
presentation platform aimed at University students wanting to improve their
public speaking skills in their native tongue. Specifically, a platform based
on the radio show, Just a Minute (JAM), is explored. In this game, players are
challenged to speak fluently on a topic for 60 seconds without repeating
themselves, hesitating or deviating from the topic. JAM has proposed benefits
such as allowing students to improve their spontaneous speaking skills and
reduce their use of speech disfluencies ("um", "uh", etc.).
  Previous research has highlighted the difficulties students face when
speaking publicly, the main one being anxiety. AI Powered Presentation
Platforms (AI-PPPs), where students can speak with an immersive AI audience and
receive real-time feedback, have been explored as a method to improve student's
speaking skills and confidence. So far they have shown promising results which
this study aims to build upon.
  A group of students from the University of York are enlisted to evaluate the
effectiveness of the JAM platform. They are asked to fill in a questionnaire,
play through the game twice and then complete a final questionnaire to discuss
their experiences playing the game. Various statistics are gathered during
their gameplay such as the number of points they gained and the number of rules
they broke. The results showed that students found the game promising and
believed that their speaking skills could improve if they played the game for
longer. More work will need to be carried out to prove the effectiveness of the
game beyond the short term.

</details>


### [575] [Red Lines and Grey Zones in the Fog of War: Benchmarking Legal Risk, Moral Harm, and Regional Bias in Large Language Model Military Decision-Making](https://arxiv.org/abs/2510.03514)
*Toby Drinkall*

Main category: cs.CY

TL;DR: 研究为评估大语言模型在模拟冲突中的目标行为的法律和道德风险开发基准框架，评估三个前沿模型，发现模型目标行为有问题且差异大，可为AI决策支持系统提供概念证明和测试框架。


<details>
  <summary>Details</summary>
Motivation: 军事组织考虑将大语言模型集成到指挥控制系统，需了解其行为倾向，评估法律和道德风险。

Method: 开发基准框架，引入基于国际人道法和军事原则的四个指标，通过90次多智能体、多轮危机模拟评估三个前沿模型。

Result: 现成大语言模型在模拟冲突环境中目标行为有问题且不可预测，均违反区分原则，不同模型差异大。

Conclusion: 为AI决策支持系统使用大语言模型的潜在行为风险提供概念证明，提供可重现的基准框架和可解释指标用于部署前测试。

Abstract: As military organisations consider integrating large language models (LLMs)
into command and control (C2) systems for planning and decision support,
understanding their behavioural tendencies is critical. This study develops a
benchmarking framework for evaluating aspects of legal and moral risk in
targeting behaviour by comparing LLMs acting as agents in multi-turn simulated
conflict. We introduce four metrics grounded in International Humanitarian Law
(IHL) and military doctrine: Civilian Target Rate (CTR) and Dual-use Target
Rate (DTR) assess compliance with legal targeting principles, while Mean and
Max Simulated Non-combatant Casualty Value (SNCV) quantify tolerance for
civilian harm.
  We evaluate three frontier models, GPT-4o, Gemini-2.5, and LLaMA-3.1, through
90 multi-agent, multi-turn crisis simulations across three geographic regions.
Our findings reveal that off-the-shelf LLMs exhibit concerning and
unpredictable targeting behaviour in simulated conflict environments. All
models violated the IHL principle of distinction by targeting civilian objects,
with breach rates ranging from 16.7% to 66.7%. Harm tolerance escalated through
crisis simulations with MeanSNCV increasing from 16.5 in early turns to 27.7 in
late turns. Significant inter-model variation emerged: LLaMA-3.1 selected an
average of 3.47 civilian strikes per simulation with MeanSNCV of 28.4, while
Gemini-2.5 selected 0.90 civilian strikes with MeanSNCV of 17.6. These
differences indicate that model selection for deployment constitutes a choice
about acceptable legal and moral risk profiles in military operations.
  This work seeks to provide a proof-of-concept of potential behavioural risks
that could emerge from the use of LLMs in Decision Support Systems (AI DSS) as
well as a reproducible benchmarking framework with interpretable metrics for
standardising pre-deployment testing.

</details>


### [576] [AI Adoption Across Mission-Driven Organizations](https://arxiv.org/abs/2510.03868)
*Dalia Ali,Muneeb Ahmed,Hailan Wang,Arfa Khan,Naira Paola Arnez Jordan,Sunnie S. Y. Kim,Meet Dilip Muchhala,Anne Kathrin Merkle,Orestis Papakyriakopoulos*

Main category: cs.CY

TL;DR: 研究资源受限、价值观驱动的使命驱动型组织（MDOs）如何采用AI，发现MDOs有选择性采用，AI采用应是有条件的。


<details>
  <summary>Details</summary>
Motivation: 当前对MDOs采用AI的实证理解有限，不清楚资源受限、价值观驱动的组织如何在运营中整合AI。

Method: 对来自全球南北不同环境、人道主义和发展组织的15名从业者进行半结构化访谈，并进行主题分析。

Result: MDOs有选择性地采用AI，在内容创作和数据分析中部署成熟，关键任务有人监督；当AI效率与组织价值观冲突时，决策停滞。

Conclusion: MDOs采用AI应是有条件的，需加强组织主权和使命完整性，保留以人为本的方法。

Abstract: Despite AI's promise for addressing global challenges, empirical
understanding of AI adoption in mission-driven organizations (MDOs) remains
limited. While research emphasizes individual applications or ethical
principles, little is known about how resource-constrained, values-driven
organizations navigate AI integration across operations. We conducted thematic
analysis of semi-structured interviews with 15 practitioners from
environmental, humanitarian, and development organizations across the Global
North and South contexts. Our analysis examines how MDOs currently deploy AI,
what barriers constrain adoption, and how practitioners envision future
integration. MDOs adopt AI selectively, with sophisticated deployment in
content creation and data analysis while maintaining human oversight for
mission-critical applications. When AI's efficiency benefits conflict with
organizational values, decision-making stalls rather than negotiating
trade-offs. This study contributes empirical evidence that AI adoption in MDOs
should be understood as conditional rather than inevitable, proceeding only
where it strengthens organizational sovereignty and mission integrity while
preserving human-centered approaches essential to their missions.

</details>


### [577] [Accountability Capture: How Record-Keeping to Support AI Transparency and Accountability (Re)shapes Algorithmic Oversight](https://arxiv.org/abs/2510.04609)
*Shreya Chappidi,Jennifer Cobbe,Chris Norval,Anjali Mazumder,Jatinder Singh*

Main category: cs.CY

TL;DR: 本文探讨算法问责制中记录保存实践，提出“问责捕获”概念，通过调查从业者揭示记录保存问题及影响，强调该问题需各方关注。


<details>
  <summary>Details</summary>
Motivation: 现有研究未充分探索算法问责制中记录保存的考量、风险和后果，本文旨在研究记录保存实践如何将算法系统纳入问责制及产生的影响。

Method: 引入“问责捕获”概念，调查100名从业者，分析记录保存问题及其与问责捕获的关联。

Result: 发现记录保存问题，明确其与问责捕获的契合度，记录广泛的记录保存实践、内外问责要求的紧张关系以及员工的抵制情况。

Conclusion: 算法问责制中实施记录保存以支持透明度会带来更广泛影响，需从业者、研究者和政策制定者关注。

Abstract: Accountability regimes typically encourage record-keeping to enable the
transparency that supports oversight, investigation, contestation, and redress.
However, implementing such record-keeping can introduce considerations, risks,
and consequences, which so far remain under-explored. This paper examines how
record-keeping practices bring algorithmic systems within accountability
regimes, providing a basis to observe and understand their effects. For this,
we introduce, describe, and elaborate 'accountability capture' -- the
re-configuration of socio-technical processes and the associated downstream
effects relating to record-keeping for algorithmic accountability. Surveying
100 practitioners, we evidence and characterise record-keeping issues in
practice, identifying their alignment with accountability capture. We further
document widespread record-keeping practices, tensions between internal and
external accountability requirements, and evidence of employee resistance to
practices imposed through accountability capture. We discuss these and other
effects for surveillance, privacy, and data protection, highlighting
considerations for algorithmic accountability communities. In all, we show that
implementing record-keeping to support transparency in algorithmic
accountability regimes can itself bring wider implications -- an issue
requiring greater attention from practitioners, researchers, and policymakers
alike.

</details>


### [578] [A New Digital Divide? Coder Worldviews, the Slop Economy, and Democracy in the Age of AI](https://arxiv.org/abs/2510.04755)
*Jason Miklian,Kristian Hoelscher*

Main category: cs.CY

TL;DR: 文章结合两种视角探讨数字技术对民主生活的影响，通过对硅谷开发者的调查及对新数字鸿沟的研究，发现开发者信念与数字生态的循环关系，呼吁伦理设计和政策干预保障民主价值。


<details>
  <summary>Details</summary>
Motivation: 探究数字技术以冲突方式改变民主生活的内在张力。

Method: 对硅谷软件开发者进行原创调查，并结合新兴的信息质量数字鸿沟和“Slop Economy”背景对调查结果进行批判性研究。

Result: 开发者虽意识到产品影响力，但面临伦理困境和压力，设计选择可能破坏民主理想；发现科技创造者信念与数字生态存在强化循环。

Conclusion: 需要更具伦理意识的设计和政策干预，以缩小数字鸿沟，确保技术创新支持而非颠覆民主价值。

Abstract: Digital technologies are transforming democratic life in conflicting ways.
This article bridges two perspectives to unpack these tensions. First, we
present an original survey of software developers in Silicon Valley,
interrogating how coder worldviews, ethics, and workplace cultures shape the
democratic potential and social impact of the technologies they build. Results
indicate that while most developers recognize the power of their products to
influence civil liberties and political discourse, they often face ethical
dilemmas and top-down pressures that can lead to design choices undermining
democratic ideals. Second, we critically investigate these findings in the
context of an emerging new digital divide, not of internet access but of
information quality. We interrogate the survey findings in the context of the
Slop Economy, in which billions of users unable to pay for high-quality content
experience an internet dominated by low-quality, AI-generated ad-driven
content. We find a reinforcing cycle between tech creator beliefs and the
digital ecosystems they spawn. We discuss implications for democratic
governance, arguing for more ethically informed design and policy interventions
to help bridge the digital divide to ensure that technological innovation
supports rather than subverts democratic values in the next chapter of the
digital age.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [579] [Atlas-free Brain Network Transformer](https://arxiv.org/abs/2510.03306)
*Shuai Huang,Xuan Kan,James J. Lah,Deqiang Qiu*

Main category: q-bio.NC

TL;DR: 提出无图谱脑网络变压器（atlas - free BNT），实验表明其优于现有基于图谱方法，提升脑网络分析性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于图谱的脑网络分析方法存在空间不对齐、功能异质性和图谱选择偏差等局限性，影响脑网络可靠性和可解释性。

Method: 提出atlas - free BNT，利用个体静息态fMRI数据进行脑分区，在标准化体素特征空间计算ROI到体素的连接特征，用BNT架构处理得到可比的个体水平嵌入。

Result: 在性别分类和脑连接组年龄预测任务中，atlas - free BNT始终优于弹性网络、BrainGNN、Graphormer和原始BNT等现有基于图谱的方法。

Conclusion: 无图谱方法显著提高了脑网络分析的精度、鲁棒性和泛化性，有望改善神经影像生物标志物和临床诊断工具。

Abstract: Current atlas-based approaches to brain network analysis rely heavily on
standardized anatomical or connectivity-driven brain atlases. However, these
fixed atlases often introduce significant limitations, such as spatial
misalignment across individuals, functional heterogeneity within predefined
regions, and atlas-selection biases, collectively undermining the reliability
and interpretability of the derived brain networks. To address these
challenges, we propose a novel atlas-free brain network transformer (atlas-free
BNT) that leverages individualized brain parcellations derived directly from
subject-specific resting-state fMRI data. Our approach computes ROI-to-voxel
connectivity features in a standardized voxel-based feature space, which are
subsequently processed using the BNT architecture to produce comparable
subject-level embeddings. Experimental evaluations on sex classification and
brain-connectome age prediction tasks demonstrate that our atlas-free BNT
consistently outperforms state-of-the-art atlas-based methods, including
elastic net, BrainGNN, Graphormer and the original BNT. Our atlas-free approach
significantly improves the precision, robustness, and generalizability of brain
network analyses. This advancement holds great potential to enhance
neuroimaging biomarkers and clinical diagnostic tools for personalized
precision medicine.

</details>


### [580] [Dissecting Larval Zebrafish Hunting using Deep Reinforcement Learning Trained RNN Agents](https://arxiv.org/abs/2510.03699)
*Raaghav Malik,Satpreet H. Singh,Sonja Johnson-Yu,Nathan Wu,Roy Harpaz,Florian Engert,Kanaka Rajan*

Main category: q-bio.NC

TL;DR: 开发最小基于代理的模型，用深度强化学习训练策略，模拟斑马鱼捕猎行为，揭示约束和环境对捕猎动态的影响，提供规范解释并建立虚拟实验室。


<details>
  <summary>Details</summary>
Motivation: 研究生态和能量约束如何塑造生物大脑和人工智能体的适应性行为，以斑马鱼捕猎为研究场景。

Method: 开发最小基于代理的模型，在回合制斑马鱼模拟器中用深度强化学习训练循环策略，进行定量轨迹分析、虚拟实验和参数扫描。

Result: 模型重现斑马鱼捕猎行为，参数操作揭示约束和环境对捕猎动态、成功率和放弃率的影响，确定一组能使类似斑马鱼捕猎行为出现的约束条件。

Conclusion: 提供斑马鱼捕猎在能量成本和感官收益间最优平衡的规范解释，建立虚拟实验室缩小实验搜索空间并产生可证伪的预测。

Abstract: Larval zebrafish hunting provides a tractable setting to study how ecological
and energetic constraints shape adaptive behavior in both biological brains and
artificial agents. Here we develop a minimal agent-based model, training
recurrent policies with deep reinforcement learning in a bout-based zebrafish
simulator. Despite its simplicity, the model reproduces hallmark hunting
behaviors -- including eye vergence-linked pursuit, speed modulation, and
stereotyped approach trajectories -- that closely match real larval zebrafish.
Quantitative trajectory analyses show that pursuit bouts systematically reduce
prey angle by roughly half before strike, consistent with measurements. Virtual
experiments and parameter sweeps vary ecological and energetic constraints,
bout kinematics (coupled vs. uncoupled turns and forward motion), and
environmental factors such as food density, food speed, and vergence limits.
These manipulations reveal how constraints and environments shape pursuit
dynamics, strike success, and abort rates, yielding falsifiable predictions for
neuroscience experiments. These sweeps identify a compact set of constraints --
binocular sensing, the coupling of forward speed and turning in bout
kinematics, and modest energetic costs on locomotion and vergence -- that are
sufficient for zebrafish-like hunting to emerge. Strikingly, these behaviors
arise in minimal agents without detailed biomechanics, fluid dynamics, circuit
realism, or imitation learning from real zebrafish data. Taken together, this
work provides a normative account of zebrafish hunting as the optimal balance
between energetic cost and sensory benefit, highlighting the trade-offs that
structure vergence and trajectory dynamics. We establish a virtual lab that
narrows the experimental search space and generates falsifiable predictions
about behavior and neural coding.

</details>


### [581] [A Biologically Interpretable Cognitive Architecture for Online Structuring of Episodic Memories into Cognitive Maps](https://arxiv.org/abs/2510.03286)
*E. A. Dzhivelikian,A. I. Panov*

Main category: q-bio.NC

TL;DR: 提出用局部类赫布学习规则构建认知地图的新认知架构，在部分可观测网格世界证明其有效性，桥接计算神经科学和AI。


<details>
  <summary>Details</summary>
Motivation: 现有计算模型将认知地图与海马 - 内嗅机制关联时，常依赖缺乏生物合理性的全局优化规则。

Method: 提出用局部类赫布学习规则将情景记忆构建为认知地图的架构，整合后继特征框架与情景记忆，通过智能体 - 环境交互实现增量在线学习。

Result: 在部分可观测网格世界中，架构能自主将记忆组织成结构化表征，无需集中优化。

Conclusion: 此工作桥接了计算神经科学和AI，为人工自适应智能体的认知地图形成提供了基于生物学的方法。

Abstract: Cognitive maps provide a powerful framework for understanding spatial and
abstract reasoning in biological and artificial agents. While recent
computational models link cognitive maps to hippocampal-entorhinal mechanisms,
they often rely on global optimization rules (e.g., backpropagation) that lack
biological plausibility. In this work, we propose a novel cognitive
architecture for structuring episodic memories into cognitive maps using local,
Hebbian-like learning rules, compatible with neural substrate constraints. Our
model integrates the Successor Features framework with episodic memories,
enabling incremental, online learning through agent-environment interaction. We
demonstrate its efficacy in a partially observable grid-world, where the
architecture autonomously organizes memories into structured representations
without centralized optimization. This work bridges computational neuroscience
and AI, offering a biologically grounded approach to cognitive map formation in
artificial adaptive agents.

</details>


### [582] [The Bayesian Origin of the Probability Weighting Function in Human Representation of Probabilities](https://arxiv.org/abs/2510.04698)
*Xin Tong,Thi Thu Uyen Hoang,Xue-Xin Wei,Michael Hahn*

Main category: q-bio.NC

TL;DR: 本文提出基于理性推理的概率加权函数解释，模型能解释多种任务行为及适应先验，为人类概率表征提供统一解释。


<details>
  <summary>Details</summary>
Motivation: 理解人类决策中概率的表征，解决概率加权函数动机的争议。

Method: 提出基于对数量的噪声神经编码进行最优解码的理性推理的概率加权函数解释。

Result: 模型能准确解释彩票任务和点计数任务中的行为，还能解释对双峰短期先验的适应。

Conclusion: 研究结果为人类概率表征提供了基于理性推理的统一解释。

Abstract: Understanding the representation of probability in the human mind has been of
great interest to understanding human decision making. Classical paradoxes in
decision making suggest that human perception distorts probability magnitudes.
Previous accounts postulate a Probability Weighting Function that transforms
perceived probabilities; however, its motivation has been debated. Recent work
has sought to motivate this function in terms of noisy representations of
probabilities in the human mind. Here, we present an account of the Probability
Weighting Function grounded in rational inference over optimal decoding from
noisy neural encoding of quantities. We show that our model accurately accounts
for behavior in a lottery task and a dot counting task. It further accounts for
adaptation to a bimodal short-term prior. Taken together, our results provide a
unifying account grounding the human representation of probability in rational
inference.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [583] [Optimal Computation from Fluctuation Responses](https://arxiv.org/abs/2510.03900)
*Jinghao Lyu,Kyle J. Ray,James P. Crutchfield*

Main category: cond-mat.stat-mech

TL;DR: 本文提出统一框架，结合波动响应关系和机器学习识别最优协议，应用于典型示例，能实现理论最优协议，为物理信息处理系统设计高效协议提供策略。


<details>
  <summary>Details</summary>
Motivation: 计算的能源成本是物理与计算机科学交叉领域的核心挑战，需设计最小化热力学成本并确保正确结果的协议。

Method: 开发统一框架，利用波动响应关系和机器学习，统一优化分布和协议，基于迭代学习采样的噪声轨迹。

Result: 应用于典型示例，在所有测试计算中能实现理论最优协议或达到与相关有限时间界限相当的工作成本。

Conclusion: 为物理信息处理系统设计热力学高效协议提供了有原则的策略，应用广泛。

Abstract: The energy cost of computation has emerged as a central challenge at the
intersection of physics and computer science. Recent advances in statistical
physics -- particularly in stochastic thermodynamics -- enable precise
characterizations of work, heat, and entropy production in
information-processing systems driven far from equilibrium by time-dependent
control protocols. A key open question is then how to design protocols that
minimize thermodynamic cost while ensur- ing correct outcomes. To this end, we
develop a unified framework to identify optimal protocols using fluctuation
response relations (FRR) and machine learning. Unlike previous approaches that
optimize either distributions or protocols separately, our method unifies both
using FRR-derived gradients. Moreover, our method is based primarily on
iteratively learning from sampled noisy trajectories, which is generally much
easier than solving for the optimal protocol directly from a set of governing
equations. We apply the framework to canonical examples -- bit erasure in a
double-well potential and translating harmonic traps -- demonstrating how to
construct loss functions that trade-off energy cost against task error. The
framework extends trivially to underdamped systems, and we show this by
optimizing a bit-flip in an underdamped system. In all computations we test,
the framework achieves the theoretically optimal protocol or achieves work
costs comparable to relevant finite time bounds. In short, the results provide
principled strategies for designing thermodynamically efficient protocols in
physical information-processing systems. Applications range from quantum gates
robust under noise to energy-efficient control of chemical and synthetic
biological networks.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [584] [On the Limits of Consensus under Dynamic Availability and Reconfiguration](https://arxiv.org/abs/2510.03625)
*Joachim Neu,Javier Nieto,Ling Ren*

Main category: cs.CR

TL;DR: 本文识别了无额外假设下DAR设置中达成共识的必要充分对抗条件，引入新假设并提供简单高效的引导小工具。


<details>
  <summary>Details</summary>
Motivation: 现有DAR设置的共识协议需不现实的额外假设，如社会共识等。

Method: 识别无额外假设达成共识的对抗条件，引入诚实节点退出时处理密钥的新假设，提供引导小工具。

Result: 得到无额外假设的对抗条件，提出新假设和引导小工具。

Conclusion: 提出的方法可在DAR设置中无需不现实假设达成共识，引导小工具在常见乐观情况下简单高效。

Abstract: Proof-of-stake blockchains require consensus protocols that support Dynamic
Availability and Reconfiguration (so-called DAR setting), where the former
means that the consensus protocol should remain live even if a large number of
nodes temporarily crash, and the latter means it should be possible to change
the set of operating nodes over time. State-of-the-art protocols for the DAR
setting, such as Ethereum, Cardano's Ouroboros, or Snow White, require
unrealistic additional assumptions, such as social consensus, or that key
evolution is performed even while nodes are not participating. In this paper,
we identify the necessary and sufficient adversarial condition under which
consensus can be achieved in the DAR setting without additional assumptions. We
then introduce a new and realistic additional assumption: honest nodes dispose
of their cryptographic keys the moment they express intent to exit from the set
of operating nodes. To add reconfiguration to any dynamically available
consensus protocol, we provide a bootstrapping gadget that is particularly
simple and efficient in the common optimistic case of few reconfigurations and
no double-spending attempts.

</details>


### [585] [MulVuln: Enhancing Pre-trained LMs with Shared and Language-Specific Knowledge for Multilingual Vulnerability Detection](https://arxiv.org/abs/2510.04397)
*Van Nguyen,Surya Nepal,Xingliang Yuan,Tingmin Wu,Fengchao Chen,Carsten Rudolph*

Main category: cs.CR

TL;DR: 提出多语言漏洞检测方法MULVULN，在多语言软件系统中检测漏洞，实验显示其优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有软件漏洞检测方法多局限于单语言，难以处理现代多语言软件，无法捕捉代码共享和特定语言知识。

Method: 提出MULVULN方法，学习多语言源代码，捕捉语言间共享知识和特定语言知识。

Result: 在REEF数据集上实验，MULVULN优于13种基线方法，F1分数提升1.45% - 23.59%。

Conclusion: MULVULN能更稳健有效地检测现实世界多语言软件系统中的漏洞。

Abstract: Software vulnerabilities (SVs) pose a critical threat to safety-critical
systems, driving the adoption of AI-based approaches such as machine learning
and deep learning for software vulnerability detection. Despite promising
results, most existing methods are limited to a single programming language.
This is problematic given the multilingual nature of modern software, which is
often complex and written in multiple languages. Current approaches often face
challenges in capturing both shared and language-specific knowledge of source
code, which can limit their performance on diverse programming languages and
real-world codebases. To address this gap, we propose MULVULN, a novel
multilingual vulnerability detection approach that learns from source code
across multiple languages. MULVULN captures both the shared knowledge that
generalizes across languages and the language-specific knowledge that reflects
unique coding conventions. By integrating these aspects, it achieves more
robust and effective detection of vulnerabilities in real-world multilingual
software systems. The rigorous and extensive experiments on the real-world and
diverse REEF dataset, consisting of 4,466 CVEs with 30,987 patches across seven
programming languages, demonstrate the superiority of MULVULN over thirteen
effective and state-of-the-art baselines. Notably, MULVULN achieves
substantially higher F1-score, with improvements ranging from 1.45% to 23.59%
compared to the baseline methods.

</details>


### [586] [NEXUS: Network Exploration for eXploiting Unsafe Sequences in Multi-Turn LLM Jailbreaks](https://arxiv.org/abs/2510.03417)
*Javad Rafiei Asl,Sidhant Narula,Mohammad Ghasemigol,Eduardo Blanco,Daniel Takabi*

Main category: cs.CR

TL;DR: 提出NEXUS框架构建、优化和执行多轮攻击，在多个大模型上提升攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型易受多轮越狱攻击，现有方法存在探索对抗空间不佳、依赖手工启发式或缺乏系统查询优化的问题。

Method: 构建NEXUS框架，包含ThoughtNet扩展有害意图为语义网络、反馈驱动的模拟器迭代优化查询链、网络遍历器自适应导航查询空间。

Result: 在多个闭源和开源大语言模型上，NEXUS比之前方法将攻击成功率提高2.1%到19.4%。

Conclusion: NEXUS框架能有效构建、优化和执行多轮攻击，发现隐蔽且高成功率的对抗路径。

Abstract: Large Language Models (LLMs) have revolutionized natural language processing
but remain vulnerable to jailbreak attacks, especially multi-turn jailbreaks
that distribute malicious intent across benign exchanges and bypass alignment
mechanisms. Existing approaches often explore the adversarial space poorly,
rely on hand-crafted heuristics, or lack systematic query refinement. We
present NEXUS (Network Exploration for eXploiting Unsafe Sequences), a modular
framework for constructing, refining, and executing optimized multi-turn
attacks. NEXUS comprises: (1) ThoughtNet, which hierarchically expands a
harmful intent into a structured semantic network of topics, entities, and
query chains; (2) a feedback-driven Simulator that iteratively refines and
prunes these chains through attacker-victim-judge LLM collaboration using
harmfulness and semantic-similarity benchmarks; and (3) a Network Traverser
that adaptively navigates the refined query space for real-time attacks. This
pipeline uncovers stealthy, high-success adversarial paths across LLMs. On
several closed-source and open-source LLMs, NEXUS increases attack success rate
by 2.1% to 19.4% over prior methods. Code: https://github.com/inspire-lab/NEXUS

</details>


### [587] [PentestMCP: A Toolkit for Agentic Penetration Testing](https://arxiv.org/abs/2510.03610)
*Zachary Ezetta,Wu-chang Feng*

Main category: cs.CR

TL;DR: Agentic AI变革安全领域，本文介绍支持代理式渗透测试的PentestMCP库，可自定义多代理工作流。


<details>
  <summary>Details</summary>
Motivation: Agentic AI变革安全，利用MCP架构实现灵活的多功能代理构建，需开发支持代理式渗透测试的工具。

Method: 开发PentestMCP库，该库为MCP服务器实现库，支持常见渗透测试任务。

Result: 开发出PentestMCP库，可让开发者自定义多代理工作流进行渗透测试。

Conclusion: PentestMCP库有助于实现代理式渗透测试的灵活定制和执行。

Abstract: Agentic AI is transforming security by automating many tasks being performed
manually. While initial agentic approaches employed a monolithic architecture,
the Model-Context-Protocol has now enabled a remote-procedure call (RPC)
paradigm to agentic applications, allowing for the flexible construction and
composition of multi-function agents. This paper describes PentestMCP, a
library of MCP server implementations that support agentic penetration testing.
By supporting common penetration testing tasks such as network scanning,
resource enumeration, service fingerprinting, vulnerability scanning,
exploitation, and post-exploitation, PentestMCP allows a developer to customize
multi-agent workflows for performing penetration tests.

</details>


### [588] [Explainable but Vulnerable: Adversarial Attacks on XAI Explanation in Cybersecurity Applications](https://arxiv.org/abs/2510.03623)
*Maraz Mia,Mir Mehedi A. Pritom*

Main category: cs.CR

TL;DR: 本文研究XAI方法面临的对抗攻击，探索6种攻击程序，在网络安全场景实验，揭示攻击有效性，呼吁增强XAI方法弹性。


<details>
  <summary>Details</summary>
Motivation: XAI方法信任度因具体方法而异，且会受对抗攻击影响模型决策，需理解这些攻击技术。

Method: 探索6种针对SHAP、LIME、IG等事后解释方法的攻击程序，并在网络安全应用场景中进行实验。

Result: 实验揭示了这些攻击的实际有效性。

Conclusion: 需要立即关注并增强XAI方法及其应用的弹性。

Abstract: Explainable Artificial Intelligence (XAI) has aided machine learning (ML)
researchers with the power of scrutinizing the decisions of the black-box
models. XAI methods enable looking deep inside the models' behavior, eventually
generating explanations along with a perceived trust and transparency. However,
depending on any specific XAI method, the level of trust can vary. It is
evident that XAI methods can themselves be a victim of post-adversarial attacks
that manipulate the expected outcome from the explanation module. Among such
attack tactics, fairwashing explanation (FE), manipulation explanation (ME),
and backdoor-enabled manipulation attacks (BD) are the notable ones. In this
paper, we try to understand these adversarial attack techniques, tactics, and
procedures (TTPs) on explanation alteration and thus the effect on the model's
decisions. We have explored a total of six different individual attack
procedures on post-hoc explanation methods such as SHAP (SHapley Additive
exPlanations), LIME (Local Interpretable Model-agnostic Explanation), and IG
(Integrated Gradients), and investigated those adversarial attacks in
cybersecurity applications scenarios such as phishing, malware, intrusion, and
fraudulent website detection. Our experimental study reveals the actual
effectiveness of these attacks, thus providing an urgency for immediate
attention to enhance the resiliency of XAI methods and their applications.

</details>


### [589] [You Have Been LaTeXpOsEd: A Systematic Analysis of Information Leakage in Preprint Archives Using Large Language Models](https://arxiv.org/abs/2510.03761)
*Richard A. Dubniczky,Bertalan Borsos,Tihanyi Norbert*

Main category: cs.CR

TL;DR: 对预印本档案进行大规模安全审计，发现数千个安全漏洞，呼吁采取行动。


<details>
  <summary>Details</summary>
Motivation: 预印本仓库广泛使用带来安全风险，需进行安全审计。

Method: 提出LaTeXpOsEd框架，结合多种技术，引入LLMSec - DB基准测试模型。

Result: 发现数千个PII泄漏、各类敏感信息。

Conclusion: 呼吁研究界和仓库运营者立即行动，关闭安全漏洞，遵循道德原则发布部分成果。

Abstract: The widespread use of preprint repositories such as arXiv has accelerated the
communication of scientific results but also introduced overlooked security
risks. Beyond PDFs, these platforms provide unrestricted access to original
source materials, including LaTeX sources, auxiliary code, figures, and
embedded comments. In the absence of sanitization, submissions may disclose
sensitive information that adversaries can harvest using open-source
intelligence. In this work, we present the first large-scale security audit of
preprint archives, analyzing more than 1.2 TB of source data from 100,000 arXiv
submissions. We introduce LaTeXpOsEd, a four-stage framework that integrates
pattern matching, logical filtering, traditional harvesting techniques, and
large language models (LLMs) to uncover hidden disclosures within
non-referenced files and LaTeX comments. To evaluate LLMs' secret-detection
capabilities, we introduce LLMSec-DB, a benchmark on which we tested 25
state-of-the-art models. Our analysis uncovered thousands of PII leaks,
GPS-tagged EXIF files, publicly available Google Drive and Dropbox folders,
editable private SharePoint links, exposed GitHub and Google credentials, and
cloud API keys. We also uncovered confidential author communications, internal
disagreements, and conference submission credentials, exposing information that
poses serious reputational risks to both researchers and institutions. We urge
the research community and repository operators to take immediate action to
close these hidden security gaps. To support open science, we release all
scripts and methods from this study but withhold sensitive findings that could
be misused, in line with ethical principles. The source code and related
material are available at the project website https://github.com/LaTeXpOsEd

</details>


### [590] [Quantifying Distributional Robustness of Agentic Tool-Selection](https://arxiv.org/abs/2510.03992)
*Jehyeok Yeon,Isha Chaudhary,Gagandeep Singh*

Main category: cs.CR

TL;DR: 介绍ToolCert框架评估大语言模型代理系统工具选择鲁棒性，发现工具选择存在严重脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有评估忽略对抗条件下工具选择机制的特定漏洞，需要方法评估工具选择鲁棒性。

Method: 将工具选择建模为伯努利成功过程，对抗者引入误导性元数据的对抗工具并迭代优化，ToolCert采样交互给出准确率的高置信下限。

Result: 在注入欺骗性工具或饱和检索攻击下，认证准确率下限接近零，比非对抗设置平均下降超60%；针对检索和选择阶段攻击，一轮对抗适应后降至不足20%。

Conclusion: ToolCert揭示工具选择的安全威胁，提供量化代理鲁棒性的原则性方法，是安全部署代理系统的必要步骤。

Abstract: Large language models (LLMs) are increasingly deployed in agentic systems
where they map user intents to relevant external tools to fulfill a task. A
critical step in this process is tool selection, where a retriever first
surfaces candidate tools from a larger pool, after which the LLM selects the
most appropriate one. This pipeline presents an underexplored attack surface
where errors in selection can lead to severe outcomes like unauthorized data
access or denial of service, all without modifying the agent's model or code.
While existing evaluations measure task performance in benign settings, they
overlook the specific vulnerabilities of the tool selection mechanism under
adversarial conditions. To address this gap, we introduce ToolCert, the first
statistical framework that formally certifies tool selection robustness.
ToolCert models tool selection as a Bernoulli success process and evaluates it
against a strong, adaptive attacker who introduces adversarial tools with
misleading metadata, and are iteratively refined based on the agent's previous
choices. By sampling these adversarial interactions, ToolCert produces a
high-confidence lower bound on accuracy, formally quantifying the agent's
worst-case performance. Our evaluation with ToolCert uncovers the severe
fragility: under attacks injecting deceptive tools or saturating retrieval, the
certified accuracy bound drops near zero, an average performance drop of over
60% compared to non-adversarial settings. For attacks targeting the retrieval
and selection stages, the certified accuracy bound plummets to less than 20%
after just a single round of adversarial adaptation. ToolCert thus reveals
previously unexamined security threats inherent to tool selection and provides
a principled method to quantify an agent's robustness to such threats, a
necessary step for the safe deployment of agentic systems.

</details>


### [591] [PrivSpike: Employing Homomorphic Encryption for Private Inference of Deep Spiking Neural Networks](https://arxiv.org/abs/2510.03995)
*Nges Brian Njungle,Eric Jahns,Milan Stojkov,Michel A. Kinsy*

Main category: cs.CR

TL;DR: 本文介绍了用于SNN的隐私保护推理框架PRIVSPIKE，支持任意深度SNN，评估显示其是安全SNN推理的可行高效方案。


<details>
  <summary>Details</summary>
Motivation: 深度学习依赖大量数据且数据含敏感信息，SNN也有隐私挑战，需解决隐私保护问题。

Method: 引入PRIVSPIKE框架，使用CKKS同态加密方案，有评估激活函数的两个关键算法。

Result: 在多个数据集和模型上评估，取得一定加密推理准确率，给出不同模型在不同数据集上的推理时间。

Conclusion: PRIVSPIKE是安全SNN推理的可行高效方案，优于之前的加密SNN解决方案。

Abstract: Deep learning has become a cornerstone of modern machine learning. It relies
heavily on vast datasets and significant computational resources for high
performance. This data often contains sensitive information, making privacy a
major concern in deep learning. Spiking Neural Networks (SNNs) have emerged as
an energy-efficient alternative to conventional deep learning approaches.
Nevertheless, SNNs still depend on large volumes of data, inheriting all the
privacy challenges of deep learning. Homomorphic encryption addresses this
challenge by allowing computations to be performed on encrypted data, ensuring
data confidentiality throughout the entire processing pipeline. In this paper,
we introduce PRIVSPIKE, a privacy-preserving inference framework for SNNs using
the CKKS homomorphic encryption scheme. PRIVSPIKE supports arbitrary depth SNNs
and introduces two key algorithms for evaluating the Leaky Integrate-and-Fire
activation function: (1) a polynomial approximation algorithm designed for
high-performance SNN inference, and (2) a novel scheme-switching algorithm that
optimizes precision at a higher computational cost. We evaluate PRIVSPIKE on
MNIST, CIFAR-10, Neuromorphic MNIST, and CIFAR-10 DVS using models from LeNet-5
and ResNet-19 architectures, achieving encrypted inference accuracies of
98.10%, 79.3%, 98.1%, and 66.0%, respectively. On a consumer-grade CPU, SNN
LeNet-5 models achieved inference times of 28 seconds on MNIST and 212 seconds
on Neuromorphic MNIST. For SNN ResNet-19 models, inference took 784 seconds on
CIFAR-10 and 1846 seconds on CIFAR-10 DVS. These results establish PRIVSPIKE as
a viable and efficient solution for secure SNN inference, bridging the gap
between energy-efficient deep neural networks and strong cryptographic privacy
guarantees while outperforming prior encrypted SNN solutions.

</details>


### [592] [SVDefense: Effective Defense against Gradient Inversion Attacks via Singular Value Decomposition](https://arxiv.org/abs/2510.03319)
*Chenxiang Luo,David K. Y. Yau,Qun Song*

Main category: cs.CR

TL;DR: 提出SVDefense防御框架对抗联邦学习中的梯度反转攻击，性能优于现有防御方法，适用于资源受限平台。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习梯度反转攻击防御方法存在计算开销大、无法兼顾隐私保护和模型实用性、易被自适应对手绕过等问题。

Method: 提出SVDefense框架，利用截断奇异值分解混淆梯度更新，引入自适应能量阈值、通道加权近似和层加权聚合三项创新。

Result: 在图像分类、人类活动识别和关键词检测等多个应用中，SVDefense优于现有防御方法，能提供强大隐私保护且对模型准确性影响小，适用于资源受限嵌入式平台。

Conclusion: SVDefense是一种有效且实用的对抗梯度反转攻击的防御方案。

Abstract: Federated learning (FL) enables collaborative model training without sharing
raw data but is vulnerable to gradient inversion attacks (GIAs), where
adversaries reconstruct private data from shared gradients. Existing defenses
either incur impractical computational overhead for embedded platforms or fail
to achieve privacy protection and good model utility at the same time.
Moreover, many defenses can be easily bypassed by adaptive adversaries who have
obtained the defense details. To address these limitations, we propose
SVDefense, a novel defense framework against GIAs that leverages the truncated
Singular Value Decomposition (SVD) to obfuscate gradient updates. SVDefense
introduces three key innovations, a Self-Adaptive Energy Threshold that adapts
to client vulnerability, a Channel-Wise Weighted Approximation that selectively
preserves essential gradient information for effective model training while
enhancing privacy protection, and a Layer-Wise Weighted Aggregation for
effective model aggregation under class imbalance. Our extensive evaluation
shows that SVDefense outperforms existing defenses across multiple
applications, including image classification, human activity recognition, and
keyword spotting, by offering robust privacy protection with minimal impact on
model accuracy. Furthermore, SVDefense is practical for deployment on various
resource-constrained embedded platforms. We will make our code publicly
available upon paper acceptance.

</details>


### [593] [Attack logics, not outputs: Towards efficient robustification of deep neural networks by falsifying concept-based properties](https://arxiv.org/abs/2510.03320)
*Raik Dankworth,Gesina Schwalbe*

Main category: cs.CR

TL;DR: 本文提出在已训练的神经网络上用可解释人工智能技术实现基于概念的属性验证，以搜索不合逻辑的行为，有望缩小搜索空间并提高逻辑合规性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有攻击仅能证伪最终输出的类别，本文希望推广到搜索一般的不合逻辑行为，以解决神经网络的鲁棒性问题。

Method: 使用可解释人工智能技术在已训练的神经网络上实现基于概念的属性，并给出了攻击基于概念的属性搜索空间缩小的理论证明。

Result: 提出了一种在已训练神经网络上实现基于概念属性的简单方法，并从理论上证明该方法可缩小搜索空间。

Conclusion: 该方法有潜力同时高效地提高逻辑合规性和鲁棒性。

Abstract: Deep neural networks (NNs) for computer vision are vulnerable to adversarial
attacks, i.e., miniscule malicious changes to inputs may induce unintuitive
outputs. One key approach to verify and mitigate such robustness issues is to
falsify expected output behavior. This allows, e.g., to locally proof security,
or to (re)train NNs on obtained adversarial input examples. Due to the
black-box nature of NNs, current attacks only falsify a class of the final
output, such as flipping from $\texttt{stop_sign}$ to $\neg\texttt{stop_sign}$.
In this short position paper we generalize this to search for generally
illogical behavior, as considered in NN verification: falsify constraints
(concept-based properties) involving further human-interpretable concepts, like
$\texttt{red}\wedge\texttt{octogonal}\rightarrow\texttt{stop_sign}$. For this,
an easy implementation of concept-based properties on already trained NNs is
proposed using techniques from explainable artificial intelligence. Further, we
sketch the theoretical proof that attacks on concept-based properties are
expected to have a reduced search space compared to simple class falsification,
whilst arguably be more aligned with intuitive robustness targets. As an
outlook to this work in progress we hypothesize that this approach has
potential to efficiently and simultaneously improve logical compliance and
robustness.

</details>


### [594] [AgentTypo: Adaptive Typographic Prompt Injection Attacks against Black-box Multimodal Agents](https://arxiv.org/abs/2510.04257)
*Yanjie Li,Yiming Cao,Dong Wang,Bin Xiao*

Main category: cs.CR

TL;DR: 提出AgentTypo黑盒红队框架和AgentTypo - pro多LLM系统进行排版提示注入攻击，实验表明其攻击效果显著，凸显防御的紧迫性。


<details>
  <summary>Details</summary>
Motivation: 多模态代理在开放世界易受提示注入攻击，尤其是视觉输入方面，需研究攻击方法以凸显防御重要性。

Method: 引入AgentTypo框架，使用ATPI算法，以Tree - structured Parzen Estimator进行黑盒优化；开发AgentTypo - pro系统迭代优化注入提示并积累攻击策略。

Result: 在VWA - Adv基准测试中，AgentTypo攻击效果远超最新基于图像的攻击，不同模型上成功率提升明显，在图像+文本设置中也表现出色。

Conclusion: AgentTypo对多模态代理构成实际且强大的威胁，急需有效防御措施。

Abstract: Multimodal agents built on large vision-language models (LVLMs) are
increasingly deployed in open-world settings but remain highly vulnerable to
prompt injection, especially through visual inputs. We introduce AgentTypo, a
black-box red-teaming framework that mounts adaptive typographic prompt
injection by embedding optimized text into webpage images. Our automatic
typographic prompt injection (ATPI) algorithm maximizes prompt reconstruction
by substituting captioners while minimizing human detectability via a stealth
loss, with a Tree-structured Parzen Estimator guiding black-box optimization
over text placement, size, and color. To further enhance attack strength, we
develop AgentTypo-pro, a multi-LLM system that iteratively refines injection
prompts using evaluation feedback and retrieves successful past examples for
continual learning. Effective prompts are abstracted into generalizable
strategies and stored in a strategy repository, enabling progressive knowledge
accumulation and reuse in future attacks. Experiments on the VWA-Adv benchmark
across Classifieds, Shopping, and Reddit scenarios show that AgentTypo
significantly outperforms the latest image-based attacks such as AgentAttack.
On GPT-4o agents, our image-only attack raises the success rate from 0.23 to
0.45, with consistent results across GPT-4V, GPT-4o-mini, Gemini 1.5 Pro, and
Claude 3 Opus. In image+text settings, AgentTypo achieves 0.68 ASR, also
outperforming the latest baselines. Our findings reveal that AgentTypo poses a
practical and potent threat to multimodal agents and highlight the urgent need
for effective defense.

</details>


### [595] [Pilot Contamination Attacks Detection with Machine Learning for Multi-User Massive MIMO](https://arxiv.org/abs/2510.03831)
*Pedro Ivo da Cruz,Dimitri Silva,Tito Spadini,Ricardo Suyama,Murilo Bellezoni Loiola*

Main category: cs.CR

TL;DR: 本文提出用决策树算法在多用户系统基站检测导频污染攻击，通过模拟不同场景与似然比测试对比，发现单级决策树性能优于似然比测试。


<details>
  <summary>Details</summary>
Motivation: 大规模多输入多输出（MMIMO）易受主动窃听攻击，特别是导频污染攻击（PCA），需要有效检测方法。

Method: 提出用决策树（DT）算法在基站进行PCA检测，生成DT分类器训练数据并按深度选最优DT，模拟不同场景并与似然比测试（LRT）对比。

Result: 单级深度DT就能优于LRT，在噪声场景和恶意用户低功率传输时检测性能好，能更好区分PCA和非PCA数据，且无需LRT所需先验知识。

Conclusion: 决策树算法是检测PCA的有效方法，性能优于传统似然比测试。

Abstract: Massive multiple-input multiple-output (MMIMO) is essential to modern
wireless communication systems, like 5G and 6G, but it is vulnerable to active
eavesdropping attacks. One type of such attack is the pilot contamination
attack (PCA), where a malicious user copies pilot signals from an authentic
user during uplink, intentionally interfering with the base station's (BS)
channel estimation accuracy. In this work, we propose to use a Decision Tree
(DT) algorithm for PCA detection at the BS in a multi-user system. We present a
methodology to generate training data for the DT classifier and select the best
DT according to their depth. Then, we simulate different scenarios that could
be encountered in practice and compare the DT to a classical technique based on
likelihood ratio testing (LRT) submitted to the same scenarios. The results
revealed that a DT with only one level of depth is sufficient to outperform the
LRT. The DT shows a good performance regarding the probability of detection in
noisy scenarios and when the malicious user transmits with low power, in which
case the LRT fails to detect the PCA. We also show that the reason for the good
performance of the DT is its ability to compute a threshold that separates PCA
data from non-PCA data better than the LRT's threshold. Moreover, the DT does
not necessitate prior knowledge of noise power or assumptions regarding the
signal power of malicious users, prerequisites typically essential for LRT and
other hypothesis testing methodologies.

</details>


### [596] [P2P: A Poison-to-Poison Remedy for Reliable Backdoor Defense in LLMs](https://arxiv.org/abs/2510.04503)
*Shuai Zhao,Xinyi Wu,Shiqian Zhao,Xiaobao Wu,Zhongliang Guo,Yanhao Jia,Anh Tuan Luu*

Main category: cs.CR

TL;DR: 提出通用有效的后门防御算法P2P，通过注入良性触发和安全标签微调模型，在多任务和攻击类型中有效，降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型微调时易受数据投毒后门攻击，现有防御策略泛化性有限。

Method: 提出P2P算法，向部分训练样本注入良性触发和安全替代标签，利用基于提示的学习在重新投毒的数据集上微调模型。

Result: 在分类、数学推理和摘要生成等任务中，对多个先进大语言模型实验表明，P2P算法相比基线模型显著降低攻击成功率。

Conclusion: P2P可作为防御后门攻击的指南，促进安全可信的大语言模型社区发展。

Abstract: During fine-tuning, large language models (LLMs) are increasingly vulnerable
to data-poisoning backdoor attacks, which compromise their reliability and
trustworthiness. However, existing defense strategies suffer from limited
generalization: they only work on specific attack types or task settings. In
this study, we propose Poison-to-Poison (P2P), a general and effective backdoor
defense algorithm. P2P injects benign triggers with safe alternative labels
into a subset of training samples and fine-tunes the model on this re-poisoned
dataset by leveraging prompt-based learning. This enforces the model to
associate trigger-induced representations with safe outputs, thereby overriding
the effects of original malicious triggers. Thanks to this robust and
generalizable trigger-based fine-tuning, P2P is effective across task settings
and attack types. Theoretically and empirically, we show that P2P can
neutralize malicious backdoors while preserving task performance. We conduct
extensive experiments on classification, mathematical reasoning, and summary
generation tasks, involving multiple state-of-the-art LLMs. The results
demonstrate that our P2P algorithm significantly reduces the attack success
rate compared with baseline models. We hope that the P2P can serve as a
guideline for defending against backdoor attacks and foster the development of
a secure and trustworthy LLM community.

</details>


### [597] [Unified Threat Detection and Mitigation Framework (UTDMF): Combating Prompt Injection, Deception, and Bias in Enterprise-Scale Transformers](https://arxiv.org/abs/2510.04528)
*Santhosh KumarRavindran*

Main category: cs.CR

TL;DR: 企业系统采用大语言模型面临多种威胁，本文扩展框架引入UTDMF，经实验在检测准确率、减少欺骗输出和提升公平性指标上有成效，还有新算法、假设和工具包。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在企业系统的快速应用带来提示注入攻击、战略欺骗和有偏输出等安全、信任和公平性问题。

Method: 扩展对抗激活补丁框架，引入UTDMF，对企业级模型进行700多次实验。

Result: UTDMF实现92%的提示注入检测准确率、65%的欺骗输出减少和78%的公平性指标提升。

Conclusion: 提出多威胁检测的通用补丁算法、关于威胁交互的三个假设和可供企业集成的工具包。

Abstract: The rapid adoption of large language models (LLMs) in enterprise systems
exposes vulnerabilities to prompt injection attacks, strategic deception, and
biased outputs, threatening security, trust, and fairness. Extending our
adversarial activation patching framework (arXiv:2507.09406), which induced
deception in toy networks at a 23.9% rate, we introduce the Unified Threat
Detection and Mitigation Framework (UTDMF), a scalable, real-time pipeline for
enterprise-grade models like Llama-3.1 (405B), GPT-4o, and Claude-3.5. Through
700+ experiments per model, UTDMF achieves: (1) 92% detection accuracy for
prompt injection (e.g., jailbreaking); (2) 65% reduction in deceptive outputs
via enhanced patching; and (3) 78% improvement in fairness metrics (e.g.,
demographic bias). Novel contributions include a generalized patching algorithm
for multi-threat detection, three groundbreaking hypotheses on threat
interactions (e.g., threat chaining in enterprise workflows), and a
deployment-ready toolkit with APIs for enterprise integration.

</details>


### [598] [ObCLIP: Oblivious CLoud-Device Hybrid Image Generation with Privacy Preservation](https://arxiv.org/abs/2510.04153)
*Haoqi Wu,Wei Dai,Ming Xu,Li Wang,Qiang Yan*

Main category: cs.CR

TL;DR: 提出ObCLIP实现云-设备混合生成，保护隐私并降低计算成本，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型计算密集且存在用户提示信息泄露问题，现有方案缺乏隐私保障或无法平衡效用与效率。

Method: 提出ObCLIP，将输入提示转换为候选提示，云服务器处理所有候选提示，仅执行部分去噪步骤，中间潜变量返回客户端完成剩余去噪，还采用基于缓存的加速方法。

Result: 在多个数据集上的实验表明，ObCLIP提供了严格的隐私保护，与云模型效用相当，服务器成本略有增加。

Conclusion: ObCLIP是一种有效的解决方案，能保护隐私并降低计算成本。

Abstract: Diffusion Models have gained significant popularity due to their remarkable
capabilities in image generation, albeit at the cost of intensive computation
requirement. Meanwhile, despite their widespread deployment in inference
services such as Midjourney, concerns about the potential leakage of sensitive
information in uploaded user prompts have arisen. Existing solutions either
lack rigorous privacy guarantees or fail to strike an effective balance between
utility and efficiency. To bridge this gap, we propose ObCLIP, a plug-and-play
safeguard that enables oblivious cloud-device hybrid generation. By oblivious,
each input prompt is transformed into a set of semantically similar candidate
prompts that differ only in sensitive attributes (e.g., gender, ethnicity). The
cloud server processes all candidate prompts without knowing which one is the
real one, thus preventing any prompt leakage. To mitigate server cost, only a
small portion of denoising steps is performed upon the large cloud model. The
intermediate latents are then sent back to the client, which selects the
targeted latent and completes the remaining denoising using a small device
model. Additionally, we analyze and incorporate several cache-based
accelerations that leverage temporal and batch redundancy, effectively reducing
computation cost with minimal utility degradation. Extensive experiments across
multiple datasets demonstrate that ObCLIP provides rigorous privacy and
comparable utility to cloud models with slightly increased server cost.

</details>


### [599] [RL Is a Hammer and LLMs Are Nails: A Simple Reinforcement Learning Recipe for Strong Prompt Injection](https://arxiv.org/abs/2510.04885)
*Yuxin Wen,Arman Zharmagambetov,Ivan Evtimov,Narine Kokhlikyan,Tom Goldstein,Kamalika Chaudhuri,Chuan Guo*

Main category: cs.CR

TL;DR: 介绍RL - Hammer，用强化学习训练攻击者模型进行强提示注入和越狱攻击，能达到高成功率，可规避检测，推动自动红队攻击和防御发展。


<details>
  <summary>Details</summary>
Motivation: 现有防御对静态攻击有鲁棒性，但需用强攻击（如自动红队攻击）更全面评估其鲁棒性。

Method: 引入RL - Hammer，用强化学习训练攻击者模型，提出实用技术进行有效、通用攻击。

Result: RL - Hammer对GPT - 4o的攻击成功率达98%，对有Instruction Hierarchy防御的GPT - 5攻击成功率达72%，可规避多种提示注入检测器。

Conclusion: 希望工作能推动自动红队攻击发展，激励开发更强、更有原则的防御措施。

Abstract: Prompt injection poses a serious threat to the reliability and safety of LLM
agents. Recent defenses against prompt injection, such as Instruction Hierarchy
and SecAlign, have shown notable robustness against static attacks. However, to
more thoroughly evaluate the robustness of these defenses, it is arguably
necessary to employ strong attacks such as automated red-teaming. To this end,
we introduce RL-Hammer, a simple recipe for training attacker models that
automatically learn to perform strong prompt injections and jailbreaks via
reinforcement learning. RL-Hammer requires no warm-up data and can be trained
entirely from scratch. To achieve high ASRs against industrial-level models
with defenses, we propose a set of practical techniques that enable highly
effective, universal attacks. Using this pipeline, RL-Hammer reaches a 98% ASR
against GPT-4o and a $72\%$ ASR against GPT-5 with the Instruction Hierarchy
defense. We further discuss the challenge of achieving high diversity in
attacks, highlighting how attacker models tend to reward-hack diversity
objectives. Finally, we show that RL-Hammer can evade multiple prompt injection
detectors. We hope our work advances automatic red-teaming and motivates the
development of stronger, more principled defenses. Code is available at
https://github.com/facebookresearch/rl-injector.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [600] [Sharp Lower Bounds for Linearized ReLU^k Approximation on the Sphere](https://arxiv.org/abs/2510.04060)
*Tong Mao,Jinchao Xu*

Main category: math.NA

TL;DR: 证明了单位球面上线性化浅ReLU^k神经网络的饱和定理，给出逼近收敛速度下界并确定精确饱和阶，显示ReLU^k网络优势有限。


<details>
  <summary>Details</summary>
Motivation: 研究线性化浅ReLU^k神经网络在单位球面上的逼近性能，将其置于经典饱和框架中。

Method: 对任意反演拟均匀中心集进行理论分析和证明。

Result: 当目标函数光滑度r > (d + 2k + 1) / 2时，最佳L^2逼近收敛速度不超过n^(-(d + 2k + 1) / (2d))，此下界与现有上界匹配，确定精确饱和阶为(d + 2k + 1) / (2d)。

Conclusion: 线性化神经网络逼近可置于经典饱和框架，ReLU^k网络虽优于有限元，但优势有限。

Abstract: We prove a saturation theorem for linearized shallow ReLU$^k$ neural networks
on the unit sphere $\mathbb S^d$. For any antipodally quasi-uniform set of
centers, if the target function has smoothness $r>\tfrac{d+2k+1}{2}$, then the
best $\mathcal{L}^2(\mathbb S^d)$ approximation cannot converge faster than
order $n^{-\frac{d+2k+1}{2d}}$. This lower bound matches existing upper bounds,
thereby establishing the exact saturation order $\tfrac{d+2k+1}{2d}$ for such
networks. Our results place linearized neural-network approximation firmly
within the classical saturation framework and show that, although ReLU$^k$
networks outperform finite elements under equal degrees $k$, this advantage is
intrinsically limited.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [601] [Interactive High-Performance Visualization for Astronomy and Cosmology](https://arxiv.org/abs/2510.04665)
*Eva Sciacca,Nicola Tuccari,Umer Arshad,Fabio Pitari,Giuseppa Muscianisi,Emiliano Tramontana*

Main category: astro-ph.IM

TL;DR: 本文将VisIVO科学可视化框架与Cineca的交互式计算服务集成，实现HPC环境下的交互式高性能可视化工作流，通过案例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 天体物理学和宇宙学数据呈指数级增长，需要可扩展的计算工具和直观界面进行分析和可视化。

Method: 将VisIVO与IAC服务集成，通过Jupyter科学网关、自定义Python包装器和预配置交互式笔记本访问GPU计算节点进行3D可视化。

Result: 简化了对高级HPC资源的访问，增强了可重复性，加速了天文研究的探索性工作流，通过GADGET代码的大规模模拟案例验证了系统有效性。

Conclusion: 科学网关能连接特定领域工具和高级基础设施，促进以用户为中心、可扩展和可重复的研究环境。

Abstract: The exponential growth of data in Astrophysics and Cosmology demands scalable
computational tools and intuitive interfaces for analysis and visualization. In
this work, we present an innovative integration of the VisIVO scientific
visualization framework with the InterActive Computing (IAC) service at Cineca,
enabling interactive, high-performance visual workflows directly within HPC
environments. Through seamless integration into Jupyter-based science gateways,
users can now access GPU-enabled compute nodes to perform complex 3D
visualizations using VisIVO via custom Python wrappers and preconfigured
interactive notebooks. We demonstrate how this infrastructure simplifies access
to advanced HPC resources, enhances reproducibility, and accelerates
exploratory workflows in astronomical research. Our approach has been validated
through a set of representative use cases involving large-scale simulations
from the GADGET code, highlighting the effectiveness of this system in
visualizing the large-scale structure of the Universe. This work exemplifies
how science gateways can bridge domain-specific tools and advanced
infrastructures, fostering user-centric, scalable, and reproducible research
environments.

</details>


### [602] [Large Language Models Achieve Gold Medal Performance at International Astronomy & Astrophysics Olympiad](https://arxiv.org/abs/2510.05016)
*Lucas Carrit Delgado Pinheiro,Ziru Chen,Bruno Caixeta Piazza,Ness Shroff,Yingbin Liang,Yuan-Sen Ting,Huan Sun*

Main category: astro-ph.IM

TL;DR: 本文系统测试5个大语言模型在天文奥赛中的表现，发现部分模型理论考试接近人类顶尖水平，但在数据分析考试表现有差异，且所有模型在概念、几何推理和空间可视化方面存在不足。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在天文研究应用展示有局限性，现有评估方法无法评估实际研究所需复杂推理能力，需更全面了解大语言模型在天文学的优势和局限。

Method: 在国际天文和天体物理奥林匹克竞赛（IOAA）考试中对五个最先进的大语言模型进行系统基准测试。

Result: Gemini 2.5 Pro和GPT - 5在理论考试中表现优异，达到金牌水平；数据分析考试中，GPT - 5表现出色，其他模型表现较差；所有大语言模型在概念推理、几何推理和空间可视化方面存在明显弱点。

Conclusion: 大语言模型在理论考试中接近人类顶尖水平，但在成为天文学自主研究代理前，必须解决关键差距。

Abstract: While task-specific demonstrations show early success in applying large
language models (LLMs) to automate some astronomical research tasks, they only
provide incomplete views of all necessary capabilities in solving astronomy
problems, calling for more thorough understanding of LLMs' strengths and
limitations. So far, existing benchmarks and evaluations focus on simple
question-answering that primarily tests astronomical knowledge and fails to
evaluate the complex reasoning required for real-world research in the
discipline. Here, we address this gap by systematically benchmarking five
state-of-the-art LLMs on the International Olympiad on Astronomy and
Astrophysics (IOAA) exams, which are designed to examine deep conceptual
understanding, multi-step derivations, and multimodal analysis. With average
scores of 85.6% and 84.2%, Gemini 2.5 Pro and GPT-5 (the two top-performing
models) not only achieve gold medal level performance but also rank in the top
two among ~200-300 participants in all four IOAA theory exams evaluated
(2022-2025). In comparison, results on the data analysis exams show more
divergence. GPT-5 still excels in the exams with an 88.5% average score,
ranking top 10 among the participants in the four most recent IOAAs, while
other models' performances drop to 48-76%. Furthermore, our in-depth error
analysis underscores conceptual reasoning, geometric reasoning, and spatial
visualization (52-79% accuracy) as consistent weaknesses among all LLMs. Hence,
although LLMs approach peak human performance in theory exams, critical gaps
must be addressed before they can serve as autonomous research agents in
astronomy.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [603] [Assessing the impact of contact time on leachate chemistry from recycled concrete aggregates](https://arxiv.org/abs/2510.03344)
*Morgan D. Sanger,Gabrielle Campagnola,Robin Ritchey,Tuncer B. Edil,Matthew Ginder-Vogel*

Main category: physics.chem-ph

TL;DR: 研究用改良批次测试法评估回收再生混凝土骨料（RCA）基层样品渗滤液，监测24小时内渗滤液pH、碱度和钙离子浓度变化。


<details>
  <summary>Details</summary>
Motivation: 现有文献未解决RCA渗滤液pH现场与实验室测量差异问题，也缺乏老化RCA渗滤液成分随时间变化的研究，需考虑接触时间对RCA渗滤液的影响。

Method: 使用改良批次测试法评估明尼苏达州道路研究高速公路建设研究现场回收的RCA基层样品。

Result: 渗滤液初始pH高（>10），随时间与大气二氧化碳反应降低；钙离子浓度初始快速增加，后趋于平缓；碱度在初始急剧增加后稳定在50 - 65mg CaCO3/L。

Conclusion: 明确了RCA渗滤液在初始接触期的化学性质随时间的变化情况。

Abstract: Recycled concrete aggregate (RCA) is recognized as a readily available,
mechanically sufficient construction and demolition waste product that is
suitable as a base course substitute for natural, virgin aggregate in pavement
construction. Environmentally responsible applications of RCA must consider the
high alkalinity, high pH leachate, and heavy metal leaching risks reported in
the literature. The existing body of literature does not address discrepancies
between field and laboratory measurements of RCA leachate pH, nor are there any
existing studies of aged RCA leachate composition as a function of time. To
consider the influence of contact time on RCA leachate, the present study
evaluates recovered RCA base course samples from the Minnesota Road Research
highway construction study site using modified batch test methodology. Leachate
pH, alkalinity, and calcium ion (Ca2+) concentration were monitored for 24
hours to understand RCA leachate chemistry during the initial contact period.
Leachate pH is high upon initial contact with water (pH > 10) and decreases
over time as it reacts with atmospheric carbon dioxide. Calcium ion
concentration increases rapidly in the initial contact period, then more
gradually as calcium saturation is reached. Alkalinity stabilizes (50-65 mg
CaCO3/L) after a dramatic increase during the initial contact period.

</details>


### [604] [A Universal Deep Learning Force Field for Molecular Dynamic Simulation and Vibrational Spectra Prediction](https://arxiv.org/abs/2510.04227)
*Shengjiao Ji,Yujin Zhang,Zihan Zou,Bin Jiang,Jun Jiang,Yi Luo,Wei Hu*

Main category: physics.chem-ph

TL;DR: 本文提出DetaNet与速度Verlet积分器结合的MLMD方法用于光谱预测，在多系统中实现快速准确的红外和拉曼光谱模拟。


<details>
  <summary>Details</summary>
Motivation: 传统量子化学方法忽略非谐性和核量子效应，从头算分子动力学计算成本高，需要准确高效的光谱模拟方法。

Method: 将DetaNet与速度Verlet积分器结合进行MLMD模拟，在QMe14S数据集上训练DetaNet，用MLMD和RPMD轨迹的时间相关函数计算光谱。

Result: DetaNet-based MD方法在孤立分子上达到接近实验的光谱精度，比AIMD快三个数量级，可扩展到多种系统。

Conclusion: 建立了通用的机器学习力场和张量感知MLMD框架，可实现不同分子和材料系统的快速、准确、广泛适用的动态模拟和光谱预测。

Abstract: Accurate and efficient simulation of infrared (IR) and Raman spectra is
essential for molecular identification and structural analysis. Traditional
quantum chemistry methods based on the harmonic approximation neglect
anharmonicity and nuclear quantum effects, while ab initio molecular dynamics
(AIMD) remains computationally expensive. Here, we integrate our deep
equivariant tensor attention network (DetaNet) with a velocity-Verlet
integrator to enable fast and accurate machine learning molecular dynamics
(MLMD) simulations for spectral prediction. Trained on the QMe14S dataset
containing energies, forces, dipole moments, and polarizabilities for 186,102
small organic molecules, DetaNet yields a universal and transferable force
field with high-order tensor prediction capability. Using time-correlation
functions derived from MLMD and ring-polymer molecular dynamics (RPMD)
trajectories, we computed IR and Raman spectra that accurately reproduce
anharmonic and nuclear quantum effects. Benchmark tests on isolated molecules,
including polycyclic aromatic hydrocarbons, demonstrate that the DetaNet-based
MD approach achieves near-experimental spectral accuracy with speedups up to
three orders of magnitude over AIMD. Furthermore, the framework extends
seamlessly to molecular and inorganic crystals, molecular aggregates, and
biological macromolecules such as polypeptides with minimal fine-tuning. In all
systems, DetaNet maintains high accuracy while significantly reducing
computational cost. Overall, this work establishes a universal machine learning
force field and tensor-aware MLMD framework that enable fast, accurate, and
broadly applicable dynamic simulations and IR/Raman spectral predictions across
diverse molecular and material systems.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [605] [Evaluating Keyframe Layouts for Visual Known-Item Search in Homogeneous Collections](https://arxiv.org/abs/2510.04396)
*Bastian Jäckl,Jiří Kruchina,Lucas Joos,Daniel A. Keim,Ladislav Peška,Jakub Lokoč*

Main category: cs.MM

TL;DR: 研究评估七种关键帧布局用于视觉已知项搜索任务，得出不同布局效率和准确性表现，为混合设计提供思路。


<details>
  <summary>Details</summary>
Motivation: 现有多模态深度学习模型虽可对关键帧排序，但用户仍需手动浏览候选结果，关键帧布局对浏览效果和效率影响大且研究不足。

Method: 对49名参与者进行研究，评估七种关键帧布局。

Result: 视频分组布局最有效率，四列保序网格准确率最高，排序网格有潜力但存在权衡。

Conclusion: 研究结果促使设计混合布局，保留排名靠前项位置并对其余项排序或分组，也为非视频检索的网格搜索提供指导。

Abstract: Multimodal deep-learning models power interactive video retrieval by ranking
keyframes in response to textual queries. Despite these advances, users must
still browse ranked candidates manually to locate a target. Keyframe
arrangement within the search grid highly affects browsing effectiveness and
user efficiency, yet remains underexplored. We report a study with 49
participants evaluating seven keyframe layouts for the Visual Known-Item Search
task. Beyond efficiency and accuracy, we relate browsing phenomena, such as
overlooks, to layout characteristics. Our results show that a video-grouped
layout is the most efficient, while a four-column, rank-preserving grid
achieves the highest accuracy. Sorted grids reveal potentials and trade-offs,
enabling rapid scanning of uninteresting regions but down-ranking relevant
targets to less prominent positions, delaying first arrival times and
increasing overlooks.
  These findings motivate hybrid designs that preserve positions of top-ranked
items while sorting or grouping the remainder, and offer guidance for searching
in grids beyond video retrieval.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [606] [NaturalEdit: Code Modification through Direct Interaction with Adaptive Natural Language Representation](https://arxiv.org/abs/2510.04494)
*Ningzhi Tang,David Meininger,Gelei Xu,Yiyu Shi,Yu Huang,Collin McMillan,Toby Jia-Jun Li*

Main category: cs.HC

TL;DR: 提出NaturalEdit系统让代码摘要交互自适应，经评估和用户研究证明其能提升开发者能力。


<details>
  <summary>Details</summary>
Motivation: 代码修改认知要求高，现有自然语言代码摘要静态且支持流程有限。

Method: 基于认知维度，实现三个关键特性：灵活抽象梯度的自适应多面代码摘要表示、摘要与代码的交互映射机制、意图驱动的双向同步。

Result: 技术评估确认性能，12名开发者参与的用户研究表明其提升了理解、意图表达和验证能力。

Conclusion: NaturalEdit系统增强了开发者的信心和控制力。

Abstract: Code modification requires developers to comprehend code, plan changes,
articulate intentions, and validate outcomes, making it a cognitively demanding
process. Generated natural language code summaries aid comprehension but remain
static and limited in supporting the full workflow. We present NaturalEdit, a
system that makes code summaries interactive and adaptive representations
directly linked to source code. Grounded in the Cognitive Dimensions of
Notations, NaturalEdit implements a paradigm of code modification through
interaction with natural language representations through three key features:
(1) adaptive multi-faceted representation of code summaries with flexible
Abstraction Gradient; (2) interactive mapping mechanisms between summaries and
codes, ensuring a tight Closeness of Mapping; and (3) intent-driven,
bidirectional synchronization that reduces Viscosity in editing and validation.
A technical evaluation confirms the performance of NaturalEdit, and a user
study with 12 developers shows that it enhances comprehension, intent
articulation, and validation, giving developers greater confidence and control.

</details>


### [607] [AI-Driven Grading and Moderation for Collaborative Projects in Computer Science Education](https://arxiv.org/abs/2510.03998)
*Songmei Yu,Andrew Zagula*

Main category: cs.HC

TL;DR: 本文介绍一种半自动化、AI辅助的评分系统，用于评估计算机科学协作小组项目的质量和个人贡献，试点部署效果良好，并讨论了相关问题。


<details>
  <summary>Details</summary>
Motivation: 传统评估策略在评估小组项目中个人贡献时存在公平性、客观性和可扩展性不足的问题。

Method: 引入半自动化、AI辅助的评分系统，利用仓库挖掘、通信分析和机器学习模型，系统包含项目评估、贡献分析和成绩计算模块，并与GitHub等平台集成。

Result: 在高级课程中的试点部署显示，该系统与教师评估高度一致，提高了学生满意度，减少了教师评分工作量。

Conclusion: 讨论了系统的实施考虑、伦理影响和扩展适用性的改进建议。

Abstract: Collaborative group projects are integral to computer science education, as
they foster teamwork, problem-solving skills, and industry-relevant
competencies. However, assessing individual contributions within group settings
has long been a challenge. Traditional assessment strategies, such as the equal
distribution of grades or subjective peer assessments, often fall short in
terms of fairness, objectivity, and scalability, particularly in large
classrooms. This paper introduces a semi-automated, AI-assisted grading system
that evaluates both project quality and individual effort using repository
mining, communication analytics, and machine learning models. The system
comprises modules for project evaluation, contribution analysis, and grade
computation, integrating seamlessly with platforms like GitHub. A pilot
deployment in a senior-level course demonstrated high alignment with instructor
assessments, increased student satisfaction, and reduced instructor grading
effort. We conclude by discussing implementation considerations, ethical
implications, and proposed enhancements to broaden applicability.

</details>


### [608] [When AI Gets Persuaded, Humans Follow: Inducing the Conformity Effect in Persuasive Dialogue](https://arxiv.org/abs/2510.04229)
*Rikuo Sasaki,Michimasa Inaba*

Main category: cs.HC

TL;DR: 研究验证AI中存在‘从众效应’，通过设计被说服代理可提升说服效果。


<details>
  <summary>Details</summary>
Motivation: 验证AI代理中是否存在‘从众效应’。

Method: 引入被说服代理，与人类参与者在三方说服对话中进行基于文本的对话实验，对比四种条件。

Result: 被说服代理接受说服时，感知说服力和实际态度改变显著提升；使用破冰环节时态度改变最大；未被说服的AI代理抑制态度改变；参与者在被说服代理被说服时接受度增加。

Conclusion: 适当设计被说服代理可通过从众效应提高说服效果。

Abstract: Recent advancements in AI have highlighted its application in captology, the
field of using computers as persuasive technologies. We hypothesized that the
"conformity effect," where individuals align with others' actions, also occurs
with AI agents. This study verifies this hypothesis by introducing a "Persuadee
Agent" that is persuaded alongside a human participant in a three-party
persuasive dialogue with a Persuader Agent. We conducted a text-based dialogue
experiment with human participants. We compared four conditions manipulating
the Persuadee Agent's behavior (persuasion acceptance vs. non-acceptance) and
the presence of an icebreaker session. Results showed that when the Persuadee
Agent accepted persuasion, both perceived persuasiveness and actual attitude
change significantly improved. Attitude change was greatest when an icebreaker
was also used, whereas an unpersuaded AI agent suppressed attitude change.
Additionally, it was confirmed that the persuasion acceptance of participants
increased at the moment the Persuadee Agent was persuaded. These results
suggest that appropriately designing a Persuadee Agent can improve persuasion
through the conformity effect.

</details>


### [609] [Autonomy Matters: A Study on Personalization-Privacy Dilemma in LLM Agents](https://arxiv.org/abs/2510.04465)
*Zhiping Zhang,Yi Evie Zhang,Freda Shi,Tianshi Li*

Main category: cs.HC

TL;DR: 本文通过实验研究大语言模型代理的自主性和个性化对用户隐私担忧、信任和使用意愿的影响，发现平衡代理自主性和用户控制可缓解个性化 - 隐私困境。


<details>
  <summary>Details</summary>
Motivation: 大语言模型代理个性化需个人信息引发隐私担忧和困境，代理自主性影响不明，需深入研究。

Method: 进行3×3组间实验（N = 450），研究代理自主性水平和个性化对用户隐私担忧、信任和使用意愿的影响及潜在心理过程。

Result: 不考虑用户隐私偏好的个性化会增加隐私担忧，降低信任和使用意愿；自主性调节这些影响，中等自主性相比无自主性和完全自主性可平抑个性化的影响。

Conclusion: 平衡代理行动的自主性和用户控制是缓解个性化 - 隐私困境的可行途径，而非追求输出生成的完美模型对齐。

Abstract: Large Language Model (LLM) agents require personal information for
personalization in order to better act on users' behalf in daily tasks, but
this raises privacy concerns and a personalization-privacy dilemma. Agent's
autonomy introduces both risks and opportunities, yet its effects remain
unclear. To better understand this, we conducted a 3$\times$3 between-subjects
experiment ($N=450$) to study how agent's autonomy level and personalization
influence users' privacy concerns, trust and willingness to use, as well as the
underlying psychological processes. We find that personalization without
considering users' privacy preferences increases privacy concerns and decreases
trust and willingness to use. Autonomy moderates these effects: Intermediate
autonomy flattens the impact of personalization compared to No- and Full
autonomy conditions. Our results suggest that rather than aiming for perfect
model alignment in output generation, balancing autonomy of agent's action and
user control offers a promising path to mitigate the personalization-privacy
dilemma.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [610] [Destination-to-Chutes Task Mapping Optimization for Multi-Robot Coordination in Robotic Sorting Systems](https://arxiv.org/abs/2510.03472)
*Yulun Zhang,Alexandre O. G. Barbosa,Federico Pecora,Jiaoyang Li*

Main category: cs.RO

TL;DR: 研究优化机器人分拣系统中目的地到滑槽的任务映射以提高吞吐量，提出方法并验证优势，还分析了不同任务映射的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现实世界机器人分拣系统中找到高质量任务映射具有挑战性，如与其他因素相互依赖、滑槽会关闭、影响下游处理等，需优化任务映射提高吞吐量。

Method: 先定义任务映射和任务映射优化问题，用模拟器评估，提出基于进化算法和混合整数线性规划的简单TMO方法，最后用质量多样性算法分析。

Result: 在不同地图大小、滑槽数量和目的地的各种RSS设置中，优化后的任务映射优于贪婪生成的映射。

Conclusion: 所提出的方法能有效优化机器人分拣系统中目的地到滑槽的任务映射，提高系统吞吐量。

Abstract: We study optimizing a destination-to-chutes task mapping to improve
throughput in Robotic Sorting Systems (RSS), where a team of robots sort
packages on a sortation floor by transporting them from induct workstations to
eject chutes based on their shipping destinations (e.g. Los Angeles or
Pittsburgh). The destination-to-chutes task mapping is used to determine which
chutes a robot can drop its package. Finding a high-quality task mapping is
challenging because of the complexity of a real-world RSS. First, optimizing
task mapping is interdependent with robot target assignment and path planning.
Second, chutes will be CLOSED for a period of time once they receive sufficient
packages to allow for downstream processing. Third, task mapping quality
directly impacts the downstream processing, as scattered chutes for the same
destination increase package handling time. In this paper, we first formally
define task mappings and the problem of Task Mapping Optimization (TMO). We
then present a simulator of RSS to evaluate task mappings. We then present a
simple TMO method based on the Evolutionary Algorithm and Mixed Integer Linear
Programming, demonstrating the advantage of our optimized task mappings over
the greedily generated ones in various RSS setups with different map sizes,
numbers of chutes, and destinations. Finally, we use Quality Diversity
algorithms to analyze the throughput of a diverse set of task mappings. Our
code is available online at https://github.com/lunjohnzhang/tmo_public.

</details>


### [611] [EmbodiSwap for Zero-Shot Robot Imitation Learning](https://arxiv.org/abs/2510.03706)
*Eadom Dessalene,Pavan Mantripragada,Michael Maynord,Yiannis Aloimonos*

Main category: cs.RO

TL;DR: 介绍EmbodiSwap方法用于生成逼真合成机器人覆盖人类视频，用于零样本模仿学习，采用V - JEPA作视觉骨干，现实测试成功率82%，并发布相关代码和数据集。


<details>
  <summary>Details</summary>
Motivation: 弥合野外以自我为中心的人类视频和目标机器人实体之间的具身差距，实现零样本模仿学习。

Method: 引入EmbodiSwap方法生成合成机器人覆盖视频，使用V - JEPA作为视觉骨干训练闭环机器人操作策略。

Result: 采用V - JEPA优于传统视觉骨干，零样本训练的V - JEPA模型在现实测试中成功率达82%，优于少样本训练的网络。

Conclusion: 提出的方法有效，通过发布代码、数据集和模型检查点促进可重复性研究和广泛应用。

Abstract: We introduce EmbodiSwap - a method for producing photorealistic synthetic
robot overlays over human video. We employ EmbodiSwap for zero-shot imitation
learning, bridging the embodiment gap between in-the-wild ego-centric human
video and a target robot embodiment. We train a closed-loop robot manipulation
policy over the data produced by EmbodiSwap. We make novel use of V-JEPA as a
visual backbone, repurposing V-JEPA from the domain of video understanding to
imitation learning over synthetic robot videos. Adoption of V-JEPA outperforms
alternative vision backbones more conventionally used within robotics. In
real-world tests, our zero-shot trained V-JEPA model achieves an $82\%$ success
rate, outperforming a few-shot trained $\pi_0$ network as well as $\pi_0$
trained over data produced by EmbodiSwap. We release (i) code for generating
the synthetic robot overlays which takes as input human videos and an arbitrary
robot URDF and generates a robot dataset, (ii) the robot dataset we synthesize
over EPIC-Kitchens, HOI4D and Ego4D, and (iii) model checkpoints and inference
code, to facilitate reproducible research and broader adoption.

</details>


### [612] [Flexible Locomotion Learning with Diffusion Model Predictive Control](https://arxiv.org/abs/2510.04234)
*Runhan Huang,Haldun Balim,Heng Yang,Yilun Du*

Main category: cs.RO

TL;DR: 提出Diffusion - MPC方法，结合生成扩散模型用于规划，有交互式训练算法，在真实世界验证了其在腿部运动中的适应性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有无模型强化学习方法难适应新行为，经典MPC依赖精确动力学模型，难以在复杂环境获得，需提出新方法解决腿部运动控制问题。

Method: 提出Diffusion - MPC，利用生成扩散模型作为近似动力学先验进行规划，联合预测未来状态和动作，在反向步骤融入奖励规划和约束投影；引入交互式训练算法更新去噪器。

Result: 设计实现了强测试时适应性，规划器可在不重新训练下调整到新奖励规格。

Conclusion: Diffusion - MPC在现实世界中展现出强大的运动能力和灵活的适应性。

Abstract: Legged locomotion demands controllers that are both robust and adaptable,
while remaining compatible with task and safety considerations. However,
model-free reinforcement learning (RL) methods often yield a fixed policy that
can be difficult to adapt to new behaviors at test time. In contrast, Model
Predictive Control (MPC) provides a natural approach to flexible behavior
synthesis by incorporating different objectives and constraints directly into
its optimization process. However, classical MPC relies on accurate dynamics
models, which are often difficult to obtain in complex environments and
typically require simplifying assumptions. We present Diffusion-MPC, which
leverages a learned generative diffusion model as an approximate dynamics prior
for planning, enabling flexible test-time adaptation through reward and
constraint based optimization. Diffusion-MPC jointly predicts future states and
actions; at each reverse step, we incorporate reward planning and impose
constraint projection, yielding trajectories that satisfy task objectives while
remaining within physical limits. To obtain a planning model that adapts beyond
imitation pretraining, we introduce an interactive training algorithm for
diffusion based planner: we execute our reward-and-constraint planner in
environment, then filter and reweight the collected trajectories by their
realized returns before updating the denoiser. Our design enables strong
test-time adaptability, allowing the planner to adjust to new reward
specifications without retraining. We validate Diffusion-MPC on real world,
demonstrating strong locomotion and flexible adaptation.

</details>


### [613] [ContextVLA: Vision-Language-Action Model with Amortized Multi-Frame Context](https://arxiv.org/abs/2510.04246)
*Huiwon Jang,Sihyun Yu,Heeseung Kwon,Hojin Jeon,Younggyo Seo,Jinwoo Shin*

Main category: cs.RO

TL;DR: 本文提出ContextVLA模型，通过压缩多帧观察信息为上下文标记，有效利用多帧观察提升机器人任务表现，减少训练和推理时间。


<details>
  <summary>Details</summary>
Motivation: 以往行为克隆方法使用多帧观察时性能提升不稳定，VLA虽能利用多帧观察但视频输入维度高导致训练和推理效率低。

Method: 提出ContextVLA模型，将过去观察压缩成单个上下文标记，用于高效动作生成。

Result: ContextVLA持续优于单帧VLA，在减少训练和推理时间的情况下实现全多帧训练的效果。

Conclusion: ContextVLA能有效利用多帧观察提升机器人任务性能，且训练和推理更高效。

Abstract: Leveraging temporal context is crucial for success in partially observable
robotic tasks. However, prior work in behavior cloning has demonstrated
inconsistent performance gains when using multi-frame observations. In this
paper, we introduce ContextVLA, a policy model that robustly improves robotic
task performance by effectively leveraging multi-frame observations. Our
approach is motivated by the key observation that Vision-Language-Action models
(VLA), i.e., policy models built upon a Vision-Language Model (VLM), more
effectively utilize multi-frame observations for action generation. This
suggests that VLMs' inherent temporal understanding capability enables them to
extract more meaningful context from multi-frame observations. However, the
high dimensionality of video inputs introduces significant computational
overhead, making VLA training and inference inefficient. To address this,
ContextVLA compresses past observations into a single context token, allowing
the policy to efficiently leverage temporal context for action generation. Our
experiments show that ContextVLA consistently improves over single-frame VLAs
and achieves the benefits of full multi-frame training but with reduced
training and inference times.

</details>


### [614] [Reliable and Scalable Robot Policy Evaluation with Imperfect Simulators](https://arxiv.org/abs/2510.04354)
*Apurva Badithela,David Snyder,Lihan Zha,Joseph Mikhail,Matthew O'Kelly,Anushri Dixit,Anirudha Majumdar*

Main category: cs.RO

TL;DR: 提出SureSim框架，结合小规模真实测试与大规模仿真，为策略的真实性能提供可靠推断，节省硬件评估工作量。


<details>
  <summary>Details</summary>
Motivation: 当前机器人操作策略的严格评估存在挑战，实践中常基于少量硬件试验且无统计保证。

Method: 将真实和仿真评估结合问题形式化为预测驱动的推理问题，用少量配对评估纠正大规模仿真偏差，利用非渐近均值估计算法提供策略性能置信区间。

Result: 使用基于物理的仿真，对扩散策略和多任务微调策略在对象和初始条件的联合分布上进行评估，该方法节省20 - 25%的硬件评估工作量以实现类似的策略性能界限。

Conclusion: SureSim框架能有效结合仿真和真实测试，可靠推断策略的真实性能，并节省硬件评估成本。

Abstract: Rapid progress in imitation learning, foundation models, and large-scale
datasets has led to robot manipulation policies that generalize to a wide-range
of tasks and environments. However, rigorous evaluation of these policies
remains a challenge. Typically in practice, robot policies are often evaluated
on a small number of hardware trials without any statistical assurances. We
present SureSim, a framework to augment large-scale simulation with relatively
small-scale real-world testing to provide reliable inferences on the real-world
performance of a policy. Our key idea is to formalize the problem of combining
real and simulation evaluations as a prediction-powered inference problem, in
which a small number of paired real and simulation evaluations are used to
rectify bias in large-scale simulation. We then leverage non-asymptotic mean
estimation algorithms to provide confidence intervals on mean policy
performance. Using physics-based simulation, we evaluate both diffusion policy
and multi-task fine-tuned \(\pi_0\) on a joint distribution of objects and
initial conditions, and find that our approach saves over \(20-25\%\) of
hardware evaluation effort to achieve similar bounds on policy performance.

</details>


### [615] [Trajectory prediction for heterogeneous agents: A performance analysis on small and imbalanced datasets](https://arxiv.org/abs/2510.03776)
*Tiago Rodrigues de Almeida,Yufei Zhu,Andrey Rudenko,Tomasz P. Kucner,Johannes A. Stork,Martin Magnusson,Achim J. Lilienthal*

Main category: cs.RO

TL;DR: 分析不同类条件轨迹预测方法，在两数据集评估，发现考虑类别标签提升精度，深度学习和模式方法在不同数据场景各有优势。


<details>
  <summary>Details</summary>
Motivation: 复杂动态环境中智能系统需预测周围主体行动，类条件运动预测可减少预测不确定性，但在移动机器人和有限数据应用中探索不足。

Method: 分析不同类条件轨迹预测方法，提出基于条件模式和深度学习的基线方法，在机器人和户外数据集评估。

Result: 考虑类别标签在多数设置中提高精度，深度学习方法在平衡数据集表现好，模式方法在有限数据或类别不平衡场景更优。

Conclusion: 类条件轨迹预测方法能提升预测精度，不同数据场景应选择不同方法。

Abstract: Robots and other intelligent systems navigating in complex dynamic
environments should predict future actions and intentions of surrounding agents
to reach their goals efficiently and avoid collisions. The dynamics of those
agents strongly depends on their tasks, roles, or observable labels.
Class-conditioned motion prediction is thus an appealing way to reduce forecast
uncertainty and get more accurate predictions for heterogeneous agents.
However, this is hardly explored in the prior art, especially for mobile robots
and in limited data applications. In this paper, we analyse different
class-conditioned trajectory prediction methods on two datasets. We propose a
set of conditional pattern-based and efficient deep learning-based baselines,
and evaluate their performance on robotics and outdoors datasets (TH\"OR-MAGNI
and Stanford Drone Dataset). Our experiments show that all methods improve
accuracy in most of the settings when considering class labels. More
importantly, we observe that there are significant differences when learning
from imbalanced datasets, or in new environments where sufficient data is not
available. In particular, we find that deep learning methods perform better on
balanced datasets, but in applications with limited data, e.g., cold start of a
robot in a new environment, or imbalanced classes, pattern-based methods may be
preferable.

</details>


### [616] [Bio-Inspired Robotic Houbara: From Development to Field Deployment for Behavioral Studies](https://arxiv.org/abs/2510.04692)
*Lyes Saad Saoud,Irfan Hussain*

Main category: cs.RO

TL;DR: 本文介绍了一个仿生机器人平台，可复制雌性鸨形态，用于野外鸟类行为研究，经野外试验验证有效，推动了仿生野外机器人发展。


<details>
  <summary>Details</summary>
Motivation: 因研究野外鸟类行为需高度逼真形态、耐用户外操作和智能感知，当前存在挑战，故研发此平台。

Method: 采用高分辨率结构光3D扫描、参数化CAD建模、铰接3D打印和逼真UV纹理乙烯基饰面的数字复制制造流程，搭配六轮摇臂转向架底盘和嵌入式NVIDIA Jetson模块等。

Result: 在沙漠鸟舍的实地试验中，平台能以15 - 22 FPS实时运行，延迟低于100 ms，能引发活鸨的自然识别和互动反应。

Conclusion: 该集成框架通过结合可复制数字制造、具身视觉智能和生态验证，为动物 - 机器人交互研究、保护机器人技术和公众参与提供了可转移蓝图。

Abstract: Biomimetic intelligence and robotics are transforming field ecology by
enabling lifelike robotic surrogates that interact naturally with animals under
real world conditions. Studying avian behavior in the wild remains challenging
due to the need for highly realistic morphology, durable outdoor operation, and
intelligent perception that can adapt to uncontrolled environments. We present
a next generation bio inspired robotic platform that replicates the morphology
and visual appearance of the female Houbara bustard to support controlled
ethological studies and conservation oriented field research. The system
introduces a fully digitally replicable fabrication workflow that combines high
resolution structured light 3D scanning, parametric CAD modelling, articulated
3D printing, and photorealistic UV textured vinyl finishing to achieve
anatomically accurate and durable robotic surrogates. A six wheeled rocker
bogie chassis ensures stable mobility on sand and irregular terrain, while an
embedded NVIDIA Jetson module enables real time RGB and thermal perception,
lightweight YOLO based detection, and an autonomous visual servoing loop that
aligns the robot's head toward detected targets without human intervention. A
lightweight thermal visible fusion module enhances perception in low light
conditions. Field trials in desert aviaries demonstrated reliable real time
operation at 15 to 22 FPS with latency under 100 ms and confirmed that the
platform elicits natural recognition and interactive responses from live
Houbara bustards under harsh outdoor conditions. This integrated framework
advances biomimetic field robotics by uniting reproducible digital fabrication,
embodied visual intelligence, and ecological validation, providing a
transferable blueprint for animal robot interaction research, conservation
robotics, and public engagement.

</details>


### [617] [Online automatic code generation for robot swarms: LLMs and self-organizing hierarchy](https://arxiv.org/abs/2510.04774)
*Weixu Zhu,Marco Dorigo,Mary Katherine Heinrich*

Main category: cs.RO

TL;DR: 介绍自组织神经系统SoNS可助力机器人群体，遇困时借助外部LLM自动生成代码，任务成功率达85%。


<details>
  <summary>Details</summary>
Motivation: 为机器人群体实现便捷行为设计、全局配置和环境估计，推动在线自动代码生成。

Method: 使用SoNS增强机器人群体，遇困时自动请求并运行外部LLM生成的代码。

Result: 在6个真实机器人演示和超30个机器人的模拟试验中，任务成功率达85%。

Conclusion: SoNS能有效提升机器人群体完成任务的能力，且可借助外部LLM应对困境。

Abstract: Our recently introduced self-organizing nervous system (SoNS) provides robot
swarms with 1) ease of behavior design and 2) global estimation of the swarm
configuration and its collective environment, facilitating the implementation
of online automatic code generation for robot swarms. In a demonstration with 6
real robots and simulation trials with >30 robots, we show that when a
SoNS-enhanced robot swarm gets stuck, it can automatically solicit and run code
generated by an external LLM on the fly, completing its mission with an 85%
success rate.

</details>


### [618] [HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks](https://arxiv.org/abs/2510.04898)
*Zheng Xiong,Kang Li,Zilin Wang,Matthew Jackson,Jakob Foerster,Shimon Whiteson*

Main category: cs.RO

TL;DR: 本文提出HyperVLA模型解决现有VLA模型推理成本高的问题，该模型在保证成功率的同时显著降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有Vision - Language - Action (VLA) 模型推理成本极高，需提出新模型解决该问题。

Method: 提出基于超网络 (HN) 的HyperVLA架构，推理时仅激活小的特定任务策略，训练时保留高模型容量；包含利用视觉基础模型先验知识、HN归一化和动作生成策略等算法设计特征。

Result: 与整体式VLA相比，HyperVLA在零样本泛化和少样本适应上成功率相近或更高，显著降低推理成本；与OpenVLA相比，测试时激活参数减少90倍，推理速度加快120倍。

Conclusion: HyperVLA能有效解决现有VLA模型推理成本高的问题，且已开源代码。

Abstract: Built upon language and vision foundation models with strong generalization
ability and trained on large-scale robotic data, Vision-Language-Action (VLA)
models have recently emerged as a promising approach to learning generalist
robotic policies. However, a key drawback of existing VLAs is their extremely
high inference costs. In this paper, we propose HyperVLA to address this
problem. Unlike existing monolithic VLAs that activate the whole model during
both training and inference, HyperVLA uses a novel hypernetwork (HN)-based
architecture that activates only a small task-specific policy during inference,
while still retaining the high model capacity needed to accommodate diverse
multi-task behaviors during training. Successfully training an HN-based VLA is
nontrivial so HyperVLA contains several key algorithm design features that
improve its performance, including properly utilizing the prior knowledge from
existing vision foundation models, HN normalization, and an action generation
strategy. Compared to monolithic VLAs, HyperVLA achieves a similar or even
higher success rate for both zero-shot generalization and few-shot adaptation,
while significantly reducing inference costs. Compared to OpenVLA, a
state-of-the-art VLA model, HyperVLA reduces the number of activated parameters
at test time by $90\times$, and accelerates inference speed by $120\times$.
Code is publicly available at https://github.com/MasterXiong/HyperVLA

</details>


### [619] [CLEAR-IR: Clarity-Enhanced Active Reconstruction of Infrared Imagery](https://arxiv.org/abs/2510.04883)
*Nathan Shankar,Pawel Ladosz,Hujun Yin*

Main category: cs.RO

TL;DR: 提出用红外流在黑暗环境实现鲁棒机器人感知的新方法，基于U - Net架构重建红外图像，性能优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 红外流在低光条件下比RGB抗噪，但主动发射模式影响高级任务，需解决此问题以提升机器人感知性能。

Method: 提出基于U - Net的架构，从有发射源的输入中重建干净的红外图像。

Result: 该方法优于现有增强技术。

Conclusion: 此方法能使视觉驱动的机器人系统在不同光照条件下可靠运行。

Abstract: This paper presents a novel approach for enabling robust robotic perception
in dark environments using infrared (IR) stream. IR stream is less susceptible
to noise than RGB in low-light conditions. However, it is dominated by active
emitter patterns that hinder high-level tasks such as object detection,
tracking and localisation. To address this, a U-Net-based architecture is
proposed that reconstructs clean IR images from emitter-populated input,
improving both image quality and downstream robotic performance. This approach
outperforms existing enhancement techniques and enables reliable operation of
vision-driven robotic systems across illumination conditions from well-lit to
extreme low-light scenes.

</details>


### [620] [ResMimic: From General Motion Tracking to Humanoid Whole-body Loco-Manipulation via Residual Learning](https://arxiv.org/abs/2510.05070)
*Siheng Zhao,Yanjie Ze,Yue Wang,C. Karen Liu,Pieter Abbeel,Guanya Shi,Rocky Duan*

Main category: cs.RO

TL;DR: 提出ResMimic两阶段残差学习框架用于类人机器人控制，在仿真和真实机器人上验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有通用运动跟踪策略用于类人机器人全身运动操作时缺乏精度和物体感知能力。

Method: 先使用在大规模人类运动数据上训练的GMT策略生成类人全身运动，再学习高效精确的残差策略优化输出，设计了点云物体跟踪奖励、接触奖励和基于课程的虚拟物体控制器。

Result: 在仿真和真实Unitree G1类人机器人上评估，相比基线在任务成功率、训练效率和鲁棒性上有显著提升。

Conclusion: ResMimic框架有效提升了类人机器人全身运动操作的性能。

Abstract: Humanoid whole-body loco-manipulation promises transformative capabilities
for daily service and warehouse tasks. While recent advances in general motion
tracking (GMT) have enabled humanoids to reproduce diverse human motions, these
policies lack the precision and object awareness required for
loco-manipulation. To this end, we introduce ResMimic, a two-stage residual
learning framework for precise and expressive humanoid control from human
motion data. First, a GMT policy, trained on large-scale human-only motion,
serves as a task-agnostic base for generating human-like whole-body movements.
An efficient but precise residual policy is then learned to refine the GMT
outputs to improve locomotion and incorporate object interaction. To further
facilitate efficient training, we design (i) a point-cloud-based object
tracking reward for smoother optimization, (ii) a contact reward that
encourages accurate humanoid body-object interactions, and (iii) a
curriculum-based virtual object controller to stabilize early training. We
evaluate ResMimic in both simulation and on a real Unitree G1 humanoid. Results
show substantial gains in task success, training efficiency, and robustness
over strong baselines. Videos are available at https://resmimic.github.io/ .

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [621] [A Case for Declarative LLM-friendly Interfaces for Improved Efficiency of Computer-Use Agents](https://arxiv.org/abs/2510.04607)
*Yuan Wang,Mingyu Li,Haibo Chen*

Main category: cs.OS

TL;DR: 本文提出Goal - Oriented Interface (GOI)解决大语言模型驱动的计算机使用代理在图形用户界面自动化任务中的问题，评估显示其提升了任务成功率并减少交互步骤。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型驱动的计算机使用代理在图形用户界面自动化任务中成功率低、LLM调用次数多，因此需要改进。

Method: 提出GOI，将现有GUI转换为三个声明式原语，采用策略 - 机制分离，LLM负责高级语义规划，GOI处理低级导航和交互，且无需修改应用源代码或依赖API。

Result: 在Windows上对微软办公套件评估，与基线相比，GOI使任务成功率提高67%，减少交互步骤43.5%，超61%成功任务只需一次LLM调用。

Conclusion: GOI能有效提升大语言模型在图形用户界面自动化任务中的表现。

Abstract: Computer-use agents (CUAs) powered by large language models (LLMs) have
emerged as a promising approach to automating computer tasks, yet they struggle
with graphical user interfaces (GUIs). GUIs, designed for humans, force LLMs to
decompose high-level goals into lengthy, error-prone sequences of fine-grained
actions, resulting in low success rates and an excessive number of LLM calls.
  We propose Goal-Oriented Interface (GOI), a novel abstraction that transforms
existing GUIs into three declarative primitives: access, state, and
observation, which are better suited for LLMs. Our key idea is policy-mechanism
separation: LLMs focus on high-level semantic planning (policy) while GOI
handles low-level navigation and interaction (mechanism). GOI does not require
modifying the application source code or relying on application programming
interfaces (APIs).
  We evaluate GOI with Microsoft Office Suite (Word, PowerPoint, Excel) on
Windows. Compared to a leading GUI-based agent baseline, GOI improves task
success rates by 67% and reduces interaction steps by 43.5%. Notably, GOI
completes over 61% of successful tasks with a single LLM call.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [622] [Real-time nonlinear inversion of magnetic resonance elastography with operator learning](https://arxiv.org/abs/2510.03372)
*Juampablo E. Heras Rivera,Caitlin M. Neher,Mehmet Kurt*

Main category: eess.IV

TL;DR: 本文开发并评估用于脑磁共振弹性成像数据非线性反演的算子学习框架oNLI，其可实时反演弹性图，性能优于CNN。


<details>
  <summary>Details</summary>
Motivation: 开发能实时反演弹性图且空间精度与非线性反演相当的算子学习框架。

Method: 回顾性研究，用61人3D MRE数据，10折交叉验证训练oNLI框架，结合结构先验机制，用多种指标评估，用配对t检验对比。

Result: oNLI全脑绝对百分比误差低于CNN，皮尔逊相关系数表现更优（p < 0.05）。

Conclusion: oNLI框架可实现实时MRE反演，优于基于CNN的方法，保持NLI的细粒度空间精度。

Abstract: $\textbf{Purpose:}$ To develop and evaluate an operator learning framework
for nonlinear inversion (NLI) of brain magnetic resonance elastography (MRE)
data, which enables real-time inversion of elastograms with comparable spatial
accuracy to NLI.
  $\textbf{Materials and Methods:}$ In this retrospective study, 3D MRE data
from 61 individuals (mean age, 37.4 years; 34 female) were used for development
of the framework. A predictive deep operator learning framework (oNLI) was
trained using 10-fold cross-validation, with the complex curl of the measured
displacement field as inputs and NLI-derived reference elastograms as outputs.
A structural prior mechanism, analogous to Soft Prior Regularization in the MRE
literature, was incorporated to improve spatial accuracy. Subject-level
evaluation metrics included Pearson's correlation coefficient, absolute
relative error, and structural similarity index measure between predicted and
reference elastograms across brain regions of different sizes to understand
accuracy. Statistical analyses included paired t-tests comparing the proposed
oNLI variants to the convolutional neural network baselines.
  $\textbf{Results:}$ Whole brain absolute percent error was 8.4 $\pm$ 0.5
($\mu'$) and 10.0 $\pm$ 0.7 ($\mu''$) for oNLI and 15.8 $\pm$ 0.8 ($\mu'$) and
26.1 $\pm$ 1.1 ($\mu''$) for CNNs. Additionally, oNLI outperformed
convolutional architectures as per Pearson's correlation coefficient, $r$, in
the whole brain and across all subregions for both the storage modulus and loss
modulus (p < 0.05).
  $\textbf{Conclusion:}$ The oNLI framework enables real-time MRE inversion
(30,000x speedup), outperforming CNN-based approaches and maintaining the
fine-grained spatial accuracy achievable with NLI in the brain.

</details>


### [623] [ReTiDe: Real-Time Denoising for Energy-Efficient Motion Picture Processing with FPGAs](https://arxiv.org/abs/2510.03812)
*Changhong Li,Clément Bled,Rosa Fernandez,Shreejith Shanker*

Main category: eess.IV

TL;DR: 提出硬件加速去噪系统ReTiDe，在FPGA上运行，有高吞吐量和能效，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有深度去噪器计算密集，在GPU上部署成本高，需高效去噪方案。

Method: 将紧凑卷积模型量化为INT8并编译到基于AMD DPU的FPGA，采用客户端 - 服务器集成将计算从CPU/GPU卸载到FPGA服务。

Result: 在基准测试中，ReTiDe吞吐量达37.71×GOPS，能效比先前FPGA去噪加速器高5.29×，PSNR/SSIM指标下降可忽略。

Conclusion: 专用加速器可为编码和后期制作提供实用、可扩展的去噪方案，降低每帧能耗且不牺牲质量和工作流兼容性。

Abstract: Denoising is a core operation in modern video pipelines. In codecs, in-loop
filters suppress sensor noise and quantisation artefacts to improve
rate-distortion performance; in cinema post-production, denoisers are used for
restoration, grain management, and plate clean-up. However, state-of-the-art
deep denoisers are computationally intensive and, at scale, are typically
deployed on GPUs, incurring high power and cost for real-time, high-resolution
streams. This paper presents Real-Time Denoise (ReTiDe), a hardware-accelerated
denoising system that serves inference on data-centre Field Programmable Gate
Arrays (FPGAs). A compact convolutional model is quantised (post-training
quantisation plus quantisation-aware fine-tuning) to INT8 and compiled for AMD
Deep Learning Processor Unit (DPU)-based FPGAs. A client-server integration
offloads computation from the host CPU/GPU to a networked FPGA service, while
remaining callable from existing workflows, e.g., NUKE, without disrupting
artist tooling. On representative benchmarks, ReTiDe delivers 37.71$\times$
Giga Operations Per Second (GOPS) throughput and 5.29$\times$ higher energy
efficiency than prior FPGA denoising accelerators, with negligible degradation
in Peak Signal-to-Noise Ratio (PSNR)/Structural Similarity Index (SSIM). These
results indicate that specialised accelerators can provide practical, scalable
denoising for both encoding pipelines and post-production, reducing energy per
frame without sacrificing quality or workflow compatibility. Code is available
at https://github.com/RCSL-TCD/ReTiDe.

</details>


### [624] [AI-Assisted Pleural Effusion Volume Estimation from Contrast-Enhanced CT Images](https://arxiv.org/abs/2510.03856)
*Sanhita Basu,Tomas Fröding,Ali Teymur Kahraman,Dimitris Toumpanakis,Tobias Sjöblom*

Main category: eess.IV

TL;DR: 开发半监督深度学习框架TTAS用于胸腔积液（PE）分割和量化，表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 胸腔积液在CT扫描中准确测量体积有挑战，为改善PE分割和量化以加强临床管理。

Method: 回顾性研究，收集CTPA数据，部分手动标注用于训练，其余用于测试和验证，开发TTAS框架并与现有模型对比。

Result: TTAS模型分割性能优于现有模型，Dice分数更高，绝对体积差异更低。

Conclusion: TTAS框架能实现更好的PE分割，有助于从CT扫描中准确确定体积。

Abstract: Background: Pleural Effusions (PE) is a common finding in many different
clinical conditions, but accurately measuring their volume from CT scans is
challenging. Purpose: To improve PE segmentation and quantification for
enhanced clinical management, we have developed and trained a semi-supervised
deep learning framework on contrast-enhanced CT volumes. Materials and Methods:
This retrospective study collected CT Pulmonary Angiogram (CTPA) data from
internal and external datasets. A subset of 100 cases was manually annotated
for model training, while the remaining cases were used for testing and
validation. A novel semi-supervised deep learning framework, Teacher-Teaching
Assistant-Student (TTAS), was developed and used to enable efficient training
in non-segmented examinations. Segmentation performance was compared to that of
state-of-the-art models. Results: 100 patients (mean age, 72 years, 28
[standard deviation]; 55 men) were included in the study. The TTAS model
demonstrated superior segmentation performance compared to state-of-the-art
models, achieving a mean Dice score of 0.82 (95% CI, 0.79 - 0.84) versus 0.73
for nnU-Net (p < 0.0001, Student's T test). Additionally, TTAS exhibited a
four-fold lower mean Absolute Volume Difference (AbVD) of 6.49 mL (95% CI, 4.80
- 8.20) compared to nnU-Net's AbVD of 23.16 mL (p < 0.0001). Conclusion: The
developed TTAS framework offered superior PE segmentation, aiding accurate
volume determination from CT scans.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [625] [Application of a Virtual Imaging Framework for Investigating a Deep Learning-Based Reconstruction Method for 3D Quantitative Photoacoustic Computed Tomography](https://arxiv.org/abs/2510.03431)
*Refik Mert Cam,Seonyeong Park,Umberto Villa,Mark A. Anastasio*

Main category: physics.med-ph

TL;DR: 使用现实虚拟成像测试平台评估3D学习型qPACT重建方法用于乳腺成像的优缺点。


<details>
  <summary>Details</summary>
Motivation: qPACT重建方法有挑战，学习型方法未充分验证，此前虚拟成像研究用的模型过于简化，需要评估学习型qPACT重建方法。

Method: 采用现实虚拟成像测试平台评估3D学习型qPACT重建方法。

Result: 评估该方法在受试者差异、测量噪声和声学畸变等物理因素下的表现。

Conclusion: 为了解该3D学习型qPACT重建方法的优缺点提供了见解。

Abstract: Quantitative photoacoustic computed tomography (qPACT) is a promising imaging
modality for estimating physiological parameters such as blood oxygen
saturation. However, developing robust qPACT reconstruction methods remains
challenging due to computational demands, modeling difficulties, and
experimental uncertainties. Learning-based methods have been proposed to
address these issues but remain largely unvalidated. Virtual imaging (VI)
studies are essential for validating such methods early in development, before
proceeding to less-controlled phantom or in vivo studies. Effective VI studies
must employ ensembles of stochastically generated numerical phantoms that
accurately reflect relevant anatomy and physiology. Yet, most prior VI studies
for qPACT relied on overly simplified phantoms. In this work, a realistic VI
testbed is employed for the first time to assess a representative 3D
learning-based qPACT reconstruction method for breast imaging. The method is
evaluated across subject variability and physical factors such as measurement
noise and acoustic aberrations, offering insights into its strengths and
limitations.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [626] [Super-resolution image projection over an extended depth of field using a diffractive decoder](https://arxiv.org/abs/2510.03938)
*Hanlong Chen,Cagatay Isil,Tianyi Gan,Mona Jarrahi,Aydogan Ozcan*

Main category: physics.optics

TL;DR: 本文提出基于CNN数字编码器和全光衍射解码器的混合图像投影系统，实现扩展景深和分辨率提升，经太赫兹实验验证，可降低显示系统要求，原理可拓展到其他应用。


<details>
  <summary>Details</summary>
Motivation: 使图像投影系统在数据存储、计算和传输上高效，同时在输出端保持大的空间带宽积。

Method: 结合CNN数字编码器和全光衍射解码器，CNN编码器将输入图像压缩为紧凑相位表示，由低分辨率投影仪显示，经模拟衍射解码器全光重建图像。

Result: 在约267倍波长的扩展景深上实现高保真图像合成，在每个横向平面上空间带宽积提升约16倍，在太赫兹光谱实验中验证概念。

Conclusion: 该图像投影架构可降低显示系统数据存储和传输要求，原理可拓展到光学计量和显微镜等多种应用。

Abstract: Image projection systems must be efficient in data storage, computation and
transmission while maintaining a large space-bandwidth-product (SBP) at their
output. Here, we introduce a hybrid image projection system that achieves
extended depth-of-field (DOF) with improved resolution, combining a
convolutional neural network (CNN)-based digital encoder with an all-optical
diffractive decoder. A CNN-based encoder compresses input images into compact
phase representations, which are subsequently displayed by a low-resolution
(LR) projector and processed by an analog diffractive decoder for all-optical
image reconstruction. This optical decoder is completely passive, designed to
synthesize pixel super-resolved image projections that feature an extended DOF
while eliminating the need for additional power consumption for super-resolved
image reconstruction. Our pixel super-resolution (PSR) image projection system
demonstrates high-fidelity image synthesis over an extended DOF of ~267xW,
where W is the illumination wavelength, concurrently offering up to ~16-fold
SBP improvement at each lateral plane. The proof of concept of this approach is
validated through an experiment conducted in the THz spectrum, and the system
is scalable across different parts of the electromagnetic spectrum. This image
projection architecture can reduce data storage and transmission requirements
for display systems without imposing additional power constraints on the
optical decoder. Beyond extended DOF PSR image projection, the underlying
principles of this approach can be extended to various applications, including
optical metrology and microscopy.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [627] [A Benchmark Study of Deep Learning Methods for Multi-Label Pediatric Electrocardiogram-Based Cardiovascular Disease Classification](https://arxiv.org/abs/2510.03780)
*Yiqiao Chen*

Main category: eess.SP

TL;DR: 本文对ZZU - pECG数据集开展深度学习多标签儿科CVD分类基准研究，评估四种范式，各模型效果好，建立基线并指出后续研究方向。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是主要儿科健康负担，早期筛查重要，心电图适合此目的，开展深度学习多标签儿科CVD分类基准研究。

Method: 在ZZU - pECG数据集上，系统评估ResNet - 1D、BiLSTM、Transformer和Mamba 2四种范式在9导联和12导联配置下的表现。

Result: 所有模型结果良好，汉明损失低至0.0069，多数情况下F1分数超85%，ResNet - 1D在12导联子集宏F1达94.67%，BiLSTM和Transformer表现有竞争力，9导联子集对罕见病分类有挑战。

Conclusion: 建立可复用基线，强调各范式优势互补，指出需大规模多中心验证、年龄分层分析和扩大疾病覆盖以支持儿科心电图实际应用。

Abstract: Cardiovascular disease (CVD) is a major pediatric health burden, and early
screening is of critical importance. Electrocardiography (ECG), as a
noninvasive and accessible tool, is well suited for this purpose. This paper
presents the first benchmark study of deep learning for multi-label pediatric
CVD classification on the recently released ZZU-pECG dataset, comprising 3716
recordings with 19 CVD categories. We systematically evaluate four
representative paradigms--ResNet-1D, BiLSTM, Transformer, and Mamba 2--under
both 9-lead and 12-lead configurations. All models achieved strong results,
with Hamming Loss as low as 0.0069 and F1-scores above 85% in most settings.
ResNet-1D reached a macro-F1 of 94.67% on the 12-lead subset, while BiLSTM and
Transformer also showed competitive performance. Per-class analysis indicated
challenges for rare conditions such as hypertrophic cardiomyopathy in the
9-lead subset, reflecting the effect of limited positive samples. This
benchmark establishes reusable baselines and highlights complementary strengths
across paradigms. It further points to the need for larger-scale, multi-center
validation, age-stratified analysis, and broader disease coverage to support
real-world pediatric ECG applications.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [628] [A Trustworthy Industrial Fault Diagnosis Architecture Integrating Probabilistic Models and Large Language Models](https://arxiv.org/abs/2510.03815)
*Yue wu*

Main category: eess.SY

TL;DR: 传统工业故障诊断方法有局限性，本文提出新架构，实验显示提高诊断准确率、降低校准误差，案例证明能纠正误判。


<details>
  <summary>Details</summary>
Motivation: 传统方法和深度学习方法在工业故障诊断的可解释性、泛化性和不确定性量化方面有局限，存在可信度不足的核心问题。

Method: 通过基于贝叶斯网络的诊断引擎进行初步分析，采用具有多模态输入能力的大语言模型驱动的认知仲裁模块对初始诊断进行专家级仲裁，集成基于温度校准的置信度校准模块和风险评估模块。

Result: 在含多种故障类型的数据集上，该框架比基线模型提高诊断准确率超28个百分点，校准后的预期校准误差降低超75%。

Conclusion: 该架构能有效纠正传统模型的误判，为工业应用构建高可信度、可解释的人工智能诊断系统提供了新颖实用的工程解决方案。

Abstract: There are limitations of traditional methods and deep learning methods in
terms of interpretability, generalization, and quantification of uncertainty in
industrial fault diagnosis, and there are core problems of insufficient
credibility in industrial fault diagnosis. The architecture performs
preliminary analysis through a Bayesian network-based diagnostic engine and
features an LLM-driven cognitive quorum module with multimodal input
capabilities. The module conducts expert-level arbitration of initial diagnoses
by analyzing structured features and diagnostic charts, prioritizing final
decisions after conflicts are identified. To ensure the reliability of the
system output, the architecture integrates a confidence calibration module
based on temperature calibration and a risk assessment module, which
objectively quantifies the reliability of the system using metrics such as
expected calibration error (ECE). Experimental results on a dataset containing
multiple fault types showed that the proposed framework improved diagnostic
accuracy by more than 28 percentage points compared to the baseline model,
while the calibrated ECE was reduced by more than 75%. Case studies have
confirmed that HCAA effectively corrects misjudgments caused by complex feature
patterns or knowledge gaps in traditional models, providing novel and practical
engineering solutions for building high-trust, explainable AI diagnostic
systems for industrial applications.

</details>


### [629] [Design Process of a Self Adaptive Smart Serious Games Ecosystem](https://arxiv.org/abs/2510.04615)
*X. Tao,P. Chen,M. Tsami,F. Khayati,M. Eckert*

Main category: eess.SY

TL;DR: 本文介绍基于严肃游戏的模块化、AI驱动康复生态系统Blexer v3的设计愿景与规划演进，提出新架构，给出概念框架，为后续开发和应用奠基。


<details>
  <summary>Details</summary>
Motivation: 基于系统先前版本的经验，设计一个能集成多模态传感、实时推理和智能控制的康复生态系统，以支持个性化干预。

Method: 提出包含数据收集、用户状态推断和游戏玩法适配等不同模块的新架构，考虑动态难度调整和程序内容生成等关键特性，构建完整概念框架。

Result: 呈现了Blexer v3的完整概念框架，明确了系统的模块化结构和数据流。

Conclusion: 概念框架为开发功能原型并将其集成到临床康复场景的下一阶段工作奠定基础。

Abstract: This paper outlines the design vision and planned evolution of Blexer v3, a
modular and AI-driven rehabilitation ecosystem based on serious games. Building
on insights from previous versions of the system, we propose a new architecture
that aims to integrate multimodal sensing, real-time reasoning, and intelligent
control. The envisioned system will include distinct modules for data
collection, user state inference, and gameplay adaptation. Key features such as
dynamic difficulty adjustment (DDA) and procedural content generation (PCG) are
also considered to support personalized interventions. We present the complete
conceptual framework of Blexer v3, which defines the modular structure and data
flow of the system. This serves as the foundation for the next phase: the
development of a functional prototype and its integration into clinical
rehabilitation scenarios.

</details>


### [630] [Model Predictive Control-Guided Reinforcement Learning for Implicit Balancing](https://arxiv.org/abs/2510.04868)
*Seyed Soroush Karimi Madahi,Kenneth Bruninx,Bert Claessens,Chris Develder*

Main category: eess.SY

TL;DR: 本文提出MPC引导的RL方法，结合MPC和RL优势，在比利时平衡数据上评估性能，相比单独的RL和MPC有套利利润优势。


<details>
  <summary>Details</summary>
Motivation: 现有MPC策略无法准确捕捉欧洲不平衡市场价格形成过程且计算成本高，无模型RL方法需大量数据训练且依赖实时和历史数据决策，需结合两者优势的方法。

Method: 提出MPC引导的RL方法，结合MPC和RL优势，将预测有效纳入决策过程，保持RL的快速推理能力。

Result: 在比利时2023年平衡数据的隐式平衡电池控制问题上评估，相比单独的RL和MPC，该方法有16.15%和54.36%的套利利润优势。

Conclusion: MPC引导的RL方法结合了MPC和RL的互补优势，在隐式平衡电池控制问题上表现良好，能带来更高套利利润。

Abstract: In Europe, profit-seeking balance responsible parties can deviate in real
time from their day-ahead nominations to assist transmission system operators
in maintaining the supply-demand balance. Model predictive control (MPC)
strategies to exploit these implicit balancing strategies capture arbitrage
opportunities, but fail to accurately capture the price-formation process in
the European imbalance markets and face high computational costs. Model-free
reinforcement learning (RL) methods are fast to execute, but require
data-intensive training and usually rely on real-time and historical data for
decision-making. This paper proposes an MPC-guided RL method that combines the
complementary strengths of both MPC and RL. The proposed method can effectively
incorporate forecasts into the decision-making process (as in MPC), while
maintaining the fast inference capability of RL. The performance of the
proposed method is evaluated on the implicit balancing battery control problem
using Belgian balancing data from 2023. First, we analyze the performance of
the standalone state-of-the-art RL and MPC methods from various angles, to
highlight their individual strengths and limitations. Next, we show an
arbitrage profit benefit of the proposed MPC-guided RL method of 16.15% and
54.36%, compared to standalone RL and MPC.

</details>


### [631] [Data-Driven Adaptive PID Control Based on Physics-Informed Neural Networks](https://arxiv.org/abs/2510.04591)
*Junsei Ito,Yasuaki Wasa*

Main category: eess.SY

TL;DR: 提出基于自适应增益优化原理、利用物理信息神经网络（PINNs）的PID控制器设计方法，经实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 设计能考虑系统非线性并确保稳定性的PID控制器。

Method: 利用PINNs自动微分得到PID增益优化梯度，结合基于跟踪误差和控制输入的成本函数进行模型预测控制，优化基于PINNs的PID增益实现自适应增益调整，并将PINNs模型集成到闭环控制系统。

Result: 通过一系列时域和频域的数值实验，证明了该方法的有效性。

Conclusion: 所提方法可有效应用于PID控制设计。

Abstract: This article proposes a data-driven PID controller design based on the
principle of adaptive gain optimization, leveraging Physics-Informed Neural
Networks (PINNs) generated for predictive modeling purposes. The proposed
control design method utilizes gradients of the PID gain optimization, achieved
through the automatic differentiation of PINNs, to apply model predictive
control using a cost function based on tracking error and control inputs. By
optimizing PINNs-based PID gains, the method achieves adaptive gain tuning that
ensures stability while accounting for system nonlinearities. The proposed
method features a systematic framework for integrating PINNs-based models of
dynamical control systems into closed-loop control systems, enabling direct
application to PID control design. A series of numerical experiments is
conducted to demonstrate the effectiveness of the proposed method from the
control perspectives based on both time and frequency domains.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [632] [How does course recommendation impact student outcomes? Examining directed self-placement with regression discontinuity analysis](https://arxiv.org/abs/2510.03350)
*Jason Godfrey*

Main category: econ.GN

TL;DR: 本文研究定向自我安置是否会对学生成绩和通过率产生与强制安置类似的负面影响，结果显示定向自我安置无负面影响，但项目仍存在基于阶层、种族和性别的差异。


<details>
  <summary>Details</summary>
Motivation: 传统大学发展性教育安置对学生有负面影响，许多大学在寻找替代安置机制，想探究定向自我安置是否有效。

Method: 使用纵向数据和因果推断方法，分析超20000名学生大一写作课程的安置记录，控制学生种族、家庭收入和性别等特征变量。

Result: 回归间断设计结果表明定向自我安置不会对学生成绩和通过率产生负面影响。

Conclusion: 定向自我安置对处于发展/补救教育临界值的学生可能是一种改进，但项目整体仍存在基于阶层、种族和性别的统计差异，安置技术只是构建更公平项目的一部分。

Abstract: For many students, placement into developmental education becomes a
self-fulfilling prophecy. Placing college students into developmental education
significantly negatively impacts student attainment, student probability of
passing, and college credits earned. To combat these negative effects, many
universities are investigating alternative placement mechanisms. Could directed
self-placement be an effective alternative mechanism? Do students who
self-place suffer the same negative impacts from placement recommendations as
their traditionally placed counterparts? This paper uses longitudinal data with
causal inference methods to examine whether directed self-placement has similar
negative impacts on student grades and pass rates as mandatory placement
schema. We begin with an analysis of over 20,000 student placement records into
one of two different placement tracks for first-year writing. Longitudinal and
institutional data allow us to control for characteristic variables such as
student race, family income, and sex. The results of our regression
discontinuity design show that directed self-placement does not negatively
impact student grades or pass rate. This may be an improvement for students who
place at or near the threshold for developmental/remedial education; However,
class, race, and gender-based statistical differences persist in the program
at-large, demonstrating that placement technique plays only one part in
building a more equitable program.

</details>


### [633] [Who benefits the most? Direct and indirect effects of a free cesarean section policy in Benin](https://arxiv.org/abs/2510.03658)
*Selidji Caroline Tossou*

Main category: econ.GN

TL;DR: 本文评估贝宁免费剖宫产政策对女性及其子女的因果影响，发现该政策降低了死胎和婴儿死亡率，但增加了产妇死亡率，还影响了生育和劳动力供给。


<details>
  <summary>Details</summary>
Motivation: 评估贝宁免费剖宫产政策对女性及其子女的影响。

Method: 使用西非国家人口与健康调查（DHS）的大样本数据，采用双重差分法。

Result: 该政策使死胎和婴儿死亡率显著降低0.0855（18.79%的变化），但使产妇死亡率增加0.00465（5.21%的变化）。

Conclusion: 该政策在降低婴儿死亡率和拯救新生儿方面有效，但损害了母亲健康，导致首次生育后生育率降低和产后母亲劳动力供给减少。

Abstract: This paper evaluates the causal effect of the access to Benin's free cesarean
section policy on females and their children. I use a large sample of
Demographic and Health Surveys (DHS) for West African countries and analyze how
the exemption of the cesarean section user fees for females in Benin directly
impacts maternal and infant mortality, family size decisions, and labor market
participation. I use a Difference in Differences approach and find that having
access to the free cesarean section policy significantly reduces the number of
stillbirths and infant mortality by 0.0855 (a 18.79 percentage change). Second,
for the surviving children, I find that access to the free cesarean section
increases the likelihood of maternal mortality by 0.00465 (a 5.21 percentage
change). The policy is effective at reducing infant mortality and saving the
newborn. However, it harms the mother's health which translates to lower
fertility after the first birth and decreased maternal labor supply post-birth.

</details>


### [634] [Labor Market Reforms, Flexibility, and Employment Transitions Across Formal and Informal Sectors](https://arxiv.org/abs/2510.03668)
*Selidji Caroline Tossou*

Main category: econ.GN

TL;DR: 本文研究贝宁2017年劳动力市场改革，评估其对就业、任期、合同类型和工资的影响，并用理论模型解释机制，揭示了劳动力市场再分配及权衡。


<details>
  <summary>Details</summary>
Motivation: 研究贝宁2017年降低解雇成本和允许无限期续签短期合同的劳动力市场改革的影响。

Method: 使用家庭生活水平调查微观数据，以邻国为对照组的双向固定效应方法，辅以理论求职模型。

Result: 正规部门就业增加，非正规就业减少；短期合同工人任期缩短，长期合同工人任期增加；获得永久合同可能性上升；正规部门工资提高。

Conclusion: 该研究为劳动力市场再分配提供有力证据，凸显发展中国家在灵活性、就业稳定性和工资之间的复杂权衡。

Abstract: In this paper, I investigate the 2017 labor market reform in Benin, which
reduced firing costs and allowed firms to renew short-term contracts
indefinitely. Using micro-data from the Harmonized Household Living Standards
Surveys and a two-way fixed effect approach with nearby countries as the
control group, I assess the reform's impact on employment, worker tenure,
contract types, and wages. My empirical results reveal a 2.6 percentage point
(24.5 percent) increase in formal sector employment and a 2.8 percentage point
(3.2 percent) reduction in informal employment. Formal sector tenure decreased
by 0.23 months for short-term contract workers, reflecting higher turnover,
while long-term contract tenure increased by 0.15 months. The likelihood of
securing a permanent contract rose by 23.2 percentage points (41.6 percent) in
the formal sector, indicating that firms used long-term contracts to retain
high-productivity workers. Wages in the formal sector increased by 33.6 USD per
month on average, with workers on short-term contracts experiencing a wage
increase of 19.6 USD and those on long-term contracts seeing an increase of
23.4 USD. I complement these findings with a theoretical job search model,
which explains the mechanisms through which lowered firing costs affected firm
hiring decisions, market tightness, and the sorting of workers across sectors.
This study provides robust evidence of labor market reallocation and highlights
the complex trade-offs between flexibility, employment stability, and wages in
a developing country context.

</details>


### [635] [Gas price shocks, uncertainty and price setting: evidences from Italian firms](https://arxiv.org/abs/2510.03792)
*Giuseppe Pagano Giorgianni*

Main category: econ.GN

TL;DR: 本文用意大利央行调查数据，研究天然气价格冲击对意大利企业定价决策和通胀预期的影响，发现冲击是通胀预期主因，且在不同不确定性下企业定价表现不同。


<details>
  <summary>Details</summary>
Motivation: 探究天然气价格冲击如何影响意大利企业定价决策和通胀预期。

Method: 使用贝叶斯向量自回归（BVAR）模型识别天然气价格冲击，结合企业层面价格设定变量和宏观总量估计更大的BVAR模型，用状态依赖局部投影法揭示非线性特征。

Result: 天然气价格冲击是企业通胀预期的主要驱动因素，尤其在2021 - 2023年；冲击使企业当前和预期价格持续上涨，通胀不确定性增加；高不确定性下企业能将成本转嫁给消费者，低不确定性下衰退效应使企业降价。

Conclusion: 天然气价格冲击对意大利企业定价决策和通胀预期有显著影响，且影响因不确定性状态而异。

Abstract: This paper examines how natural gas price shocks affect Italian firms'
pricing decisions and inflation expectations using quarterly survey data from
the Bank of Italy's Survey on Inflation and Growth Expectations (SIGE) spanning
1999Q4-2025Q2. We identify natural gas price shocks through a Bayesian VAR with
sign and zero restrictions. Our findings reveal that these shocks are a primary
driver of firms' inflation expectations, particularly during the post-COVID
period (2021-2023) when supply disruptions following Russia's invasion of
Ukraine generated unprecedented price pressures. We then estimate a larger BVAR
incorporating firm-level price setting variables and macro aggregates,
documenting that gas price shocks generate persistent increases in both firms'
current and expected prices, alongside elevated inflation uncertainty. We
uncover substantial non-linearities using state-dependent local projections:
under high uncertainty, firms successfully pass through cost increases to
consumers, maintaining elevated prices; under low uncertainty, recessionary
effects dominate, causing firms to reduce prices below baseline.

</details>


### [636] [REMIND-PyPSA-Eur: Integrating power system flexibility into sector-coupled energy transition pathways](https://arxiv.org/abs/2510.04388)
*Adrian Odenweller,Falko Ueckerdt,Johannes Hampp,Ivan Ramirez,Felix Schreyer,Robin Hasse,Jarusch Muessel,Chen Chris Gong,Robert Pietzcker,Tom Brown,Gunnar Luderer*

Main category: econ.GN

TL;DR: 结合REMIND和PyPSA - Eur模型，通过双向价格迭代软耦合优化能源系统，以德国为例验证近100%可再生电力系统技术可行且经济上可行。


<details>
  <summary>Details</summary>
Motivation: 低成本可再生电力快速扩张需供需平衡策略，现有模型存在不足，需结合不同模型优势进行长期能源转型路径优化。

Method: 将REMIND和PyPSA - Eur模型进行双向、基于价格的迭代软耦合，联合优化长期投资和短期运营。

Result: 以德国为例的两个情景结果表明，近100%可再生电力的部门耦合能源系统技术可行且经济可行，电力系统灵活性影响长期路径。

Conclusion: 该方法将电力系统动态完全融入数十年的能源转型路径。

Abstract: The rapid expansion of low-cost renewable electricity combined with end-use
electrification in transport, industry, and buildings offers a promising path
to deep decarbonisation. However, aligning variable supply with demand requires
strategies for daily and seasonal balancing. Existing models either lack the
wide scope required for long-term transition pathways or the spatio-temporal
detail to capture power system variability and flexibility. Here, we combine
the complementary strengths of REMIND, a long-term integrated assessment model,
and PyPSA-Eur, an hourly energy system model, through a bi-directional,
price-based and iterative soft coupling. REMIND provides pathway variables such
as sectoral electricity demand, installed capacities, and costs to PyPSA-Eur,
which returns optimised operational variables such as capacity factors, storage
requirements, and relative prices. After sufficient convergence, this
integrated approach jointly optimises long-term investment and short-term
operation. We demonstrate the coupling for two Germany-focused scenarios, with
and without demand-side flexibility, reaching climate neutrality by 2045. Our
results confirm that a sector-coupled energy system with nearly 100\% renewable
electricity is technically possible and economically viable. Power system
flexibility influences long-term pathways through price differentiation:
supply-side market values vary by generation technology, while demand-side
prices vary by end-use sector. Flexible electrolysers and smart-charging
electric vehicles benefit from below-average prices, whereas less flexible heat
pumps face almost twice the average price due to winter peak loads. Without
demand-side flexibility, electricity prices increase across all end-users,
though battery deployment partially compensates. Our approach therefore fully
integrates power system dynamics into multi-decadal energy transition pathways.

</details>


### [637] [Predictive economics: Rethinking economic methodology with machine learning](https://arxiv.org/abs/2510.04726)
*Miguel Alves Pereira*

Main category: econ.GN

TL;DR: 文章提出以机器学习为基础、聚焦预测准确性的预测经济学新视角，回顾其应用并表明该视角对实证分析有贡献，可补充现有方法。


<details>
  <summary>Details</summary>
Motivation: 提出一种新的经济学分析视角，强调预测准确性而非因果识别。

Method: 借鉴工具主义传统、解释 - 预测划分和建模文化对比等理论，回顾经济子领域的应用。

Result: 表明预测模型在复杂或数据丰富的背景下对实证分析有贡献。

Conclusion: 该视角补充现有方法，支持更多元的方法论，重视样本外表现、可解释性和理论结构。

Abstract: This article proposes predictive economics as a distinct analytical
perspective within economics, grounded in machine learning and centred on
predictive accuracy rather than causal identification. Drawing on the
instrumentalist tradition (Friedman), the explanation-prediction divide
(Shmueli), and the contrast between modelling cultures (Breiman), we formalise
prediction as a valid epistemological and methodological objective. Reviewing
recent applications across economic subfields, we show how predictive models
contribute to empirical analysis, particularly in complex or data-rich
contexts. This perspective complements existing approaches and supports a more
pluralistic methodology - one that values out-of-sample performance alongside
interpretability and theoretical structure.

</details>


### [638] [Hidden Actions and Hidden Information in Peer Review: A Dynamic Solution](https://arxiv.org/abs/2510.04906)
*Raphael Mu*

Main category: econ.GN

TL;DR: 本文构建科学同行评审过程模型，对比允许和不允许作者挑战初审结果两种评审方式，发现前者更接近最优结果。


<details>
  <summary>Details</summary>
Motivation: 研究科学同行评审过程，探索更优评审方式。

Method: 构建科学同行评审过程模型，分析不同评审方式。

Result: 评估技术完善时可达最优结果；允许作者挑战初审结果的评审方式更接近最优。

Conclusion: 允许作者挑战初审结果的同行评审方式更优。

Abstract: We develop a simple model of the scientific peer review process, in which
authors of varying ability invest to produce papers of varying quality, and
journals evaluate papers based on a noisy signal, choosing to accept or reject
each paper. We find that the first-best outcome is the limiting case as the
evaluation technology is perfected, even though author type and effort are not
known to the journal. Then, we consider the case where journals allow authors
to challenge an initial rejection, and find that this approach to peer review
yields an outcome closer to the first best relative to the approach that does
not allow for such challenges.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [639] [AtomWorld: A Benchmark for Evaluating Spatial Reasoning in Large Language Models on Crystalline Materials](https://arxiv.org/abs/2510.04704)
*Taoyuze Lv,Alexander Chen,Fengyu Xie,Chu Wu,Jeffrey Meng,Dongzhan Zhou,Bram Hoex,Zhicheng Zhong,Tong Xie*

Main category: cond-mat.mtrl-sci

TL;DR: 引入AtomWorld基准评估大语言模型在原子结构相关任务的推理能力，发现当前模型存在结构理解和空间推理的局限。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在文本推理和空间理解有发展，但缺乏系统评估其在不同原子结构核心推理能力的标准化基准，在材料科学领域此问题尤为重要。

Method: 引入AtomWorld基准，基于晶体学信息文件（CIF）设置结构编辑、CIF感知和属性引导建模等任务评估大语言模型。

Result: 当前模型在结构理解和空间推理存在关键局限，在结构修改任务和基本CIF格式理解上常出错，可能导致后续分析和材料洞察的累积错误。

Conclusion: AtomWorld为推动大语言模型实现强大的原子尺度建模奠定基础，对加速材料研究和自动化科学工作流程至关重要。

Abstract: Large Language Models (LLMs) excel at textual reasoning and are beginning to
develop spatial understanding, prompting the question of whether these
abilities can be combined for complex, domain-specific tasks. This question is
essential in fields like materials science, where deep understanding of 3D
atomic structures is fundamental. While initial studies have successfully
applied LLMs to tasks involving pure crystal generation or coordinate
understandings, a standardized benchmark to systematically evaluate their core
reasoning abilities across diverse atomic structures has been notably absent.
To address this gap, we introduce the AtomWorld benchmark to evaluate LLMs on
tasks based in Crystallographic Information Files (CIFs), a standard structure
representation format. These tasks, including structural editing, CIF
perception, and property-guided modeling, reveal a critical limitation: current
models, despite establishing promising baselines, consistently fail in
structural understanding and spatial reasoning. Our experiments show that these
models make frequent errors on structure modification tasks, and even in the
basic CIF format understandings, potentially leading to cumulative errors in
subsequent analysis and materials insights. By defining these standardized
tasks, AtomWorld lays the ground for advancing LLMs toward robust atomic-scale
modeling, crucial for accelerating materials research and automating scientific
workflows.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [640] [Score-based generative emulation of impact-relevant Earth system model outputs](https://arxiv.org/abs/2510.04358)
*Shahine Bouabid,Andre Nogueira Souza,Raffaele Ferrari*

Main category: physics.ao-ph

TL;DR: 本文聚焦气候模型输出模拟器，提出用基于分数的扩散模型在球面网格上运行，对比其与ESM输出，结果显示虽有不足但有潜力支持影响评估，还讨论了未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 政策目标演变快于耦合模式比较计划周期，气候模型输出模拟器可填补过时预测的差距，本文聚焦为影响模型提供输入的模拟器。

Method: 使用近地表温度、降水、相对湿度和风速的月平均ESM场，提出在球面网格上使用基于分数的扩散模型，引入一套诊断方法对比模拟器和ESM输出。

Result: 模拟器产生的分布与ESM输出接近，能捕捉关键强迫响应，但对季节循环有强变化的变量有重要失败案例，不过其误差相对ESM内部变率较小。

Conclusion: 该模拟器有潜力支持影响评估，还讨论了向每日分辨率、更精细空间尺度和偏差感知训练发展的优先事项。

Abstract: Policy targets evolve faster than the Couple Model Intercomparison Project
cycles, complicating adaptation and mitigation planning that must often contend
with outdated projections. Climate model output emulators address this gap by
offering inexpensive surrogates that can rapidly explore alternative futures
while staying close to Earth System Model (ESM) behavior. We focus on emulators
designed to provide inputs to impact models. Using monthly ESM fields of
near-surface temperature, precipitation, relative humidity, and wind speed, we
show that deep generative models have the potential to model jointly the
distribution of variables relevant for impacts. The specific model we propose
uses score-based diffusion on a spherical mesh and runs on a single mid-range
graphical processing unit. We introduce a thorough suite of diagnostics to
compare emulator outputs with their parent ESMs, including their probability
densities, cross-variable correlations, time of emergence, or tail behavior. We
evaluate performance across three distinct ESMs in both pre-industrial and
forced regimes. The results show that the emulator produces distributions that
closely match the ESM outputs and captures key forced responses. They also
reveal important failure cases, notably for variables with a strong regime
shift in the seasonal cycle. Although not a perfect match to the ESM, the
inaccuracies of the emulator are small relative to the scale of internal
variability in ESM projections. We therefore argue that it shows potential to
be useful in supporting impact assessment. We discuss priorities for future
development toward daily resolution, finer spatial scales, and bias-aware
training. Code is made available at https://github.com/shahineb/climemu.

</details>


### [641] [Deep learning the sources of MJO predictability: a spectral view of learned features](https://arxiv.org/abs/2510.03582)
*Lin Yao,Da Yang,James P. C. Duncan,Ashesh Chattopadhyay,Pedram Hassanzadeh,Wahid Bhimji,Bin Yu*

Main category: physics.ao-ph

TL;DR: 利用深度学习研究MJO可预测性来源，发现大尺度模式可能是MJO可预测性的主要来源，小尺度信号也有一定信息价值。


<details>
  <summary>Details</summary>
Motivation: MJO动力学和可预测性理解不足，不同MJO理论对驱动MJO的空间尺度存在分歧。

Method: 开发深度卷积神经网络预测MJO指数，进行潜在特征空间的谱分析，开展不同尺度输入的实验。

Result: 模型对RMM和ROMI分别可预测21天和33天，技能与NCEP等相当；大尺度模式主导学习信号，仅用大尺度信号输入的模型与使用全尺度的技能相同；仅用小尺度输入的模型也能提前1 - 2周做出有技巧的预测。

Conclusion: 大尺度模式（直接包含或重构）可能是MJO可预测性的主要来源。

Abstract: The Madden-Julian oscillation (MJO) is a planetary-scale, intraseasonal
tropical rainfall phenomenon crucial for global weather and climate; however,
its dynamics and predictability remain poorly understood. Here, we leverage
deep learning (DL) to investigate the sources of MJO predictability, motivated
by a central difference in MJO theories: which spatial scales are essential for
driving the MJO? We first develop a deep convolutional neural network (DCNN) to
forecast the MJO indices (RMM and ROMI). Our model predicts RMM and ROMI up to
21 and 33 days, respectively, achieving skills comparable to leading
subseasonal-to-seasonal models such as NCEP. To identify the spatial scales
most relevant for MJO forecasting, we conduct spectral analysis of the latent
feature space and find that large-scale patterns dominate the learned signals.
Additional experiments show that models using only large-scale signals as the
input have the same skills as those using all the scales, supporting the
large-scale view of the MJO. Meanwhile, we find that small-scale signals remain
informative: surprisingly, models using only small-scale input can still
produce skillful forecasts up to 1-2 weeks ahead. We show that this is achieved
by reconstructing the large-scale envelope of the small-scale activities, which
aligns with the multi-scale view of the MJO. Altogether, our findings support
that large-scale patterns--whether directly included or reconstructed--may be
the primary source of MJO predictability.

</details>


### [642] [Benchmarking atmospheric circulation variability in an AI emulator, ACE2, and a hybrid model, NeuralGCM](https://arxiv.org/abs/2510.04466)
*Ian Baxter,Hamid Pahlavan,Pedram Hassanzadeh,Katharine Rucker,Tiffany Shaw*

Main category: physics.ao-ph

TL;DR: 评估全数据驱动AI模拟器和混合模型对四种大气变率基准指标的表现，发现其优缺点，指标可用于指导AI模型开发。


<details>
  <summary>Details</summary>
Motivation: 基于物理的大气 - 陆地模型有偏差，AI模拟器和混合模型虽有潜力但需系统评估。

Method: 评估全数据驱动AI模拟器（ACE2 - ERA5）和混合模型（NeuralGCM）对四种大气变率基准指标的表现。

Result: 混合模型和模拟器能捕捉大规模热带波和温带涡旋 - 平均流相互作用光谱，但难以捕捉准两年振荡和南半球环状模传播的时间尺度。

Conclusion: 这些动力学指标可作为初始基准工具，指导AI模型开发并理解其局限性，对分布外应用很重要。

Abstract: Physics-based atmosphere-land models with prescribed sea surface temperature
have notable successes but also biases in their ability to represent
atmospheric variability compared to observations. Recently, AI emulators and
hybrid models have emerged with the potential to overcome these biases, but
still require systematic evaluation against metrics grounded in fundamental
atmospheric dynamics. Here, we evaluate the representation of four atmospheric
variability benchmarking metrics in a fully data-driven AI emulator (ACE2-ERA5)
and hybrid model (NeuralGCM). The hybrid model and emulator can capture the
spectra of large-scale tropical waves and extratropical eddy-mean flow
interactions, including critical levels. However, both struggle to capture the
timescales associated with quasi-biennial oscillation (QBO, $\sim 28$ months)
and Southern annular mode propagation ($\sim 150$ days). These dynamical
metrics serve as an initial benchmarking tool to inform AI model development
and understand their limitations, which may be essential for
out-of-distribution applications (e.g., extrapolating to unseen climates).

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [643] [Cellular Learning: Scattered Data Regression in High Dimensions via Voronoi Cells](https://arxiv.org/abs/2510.03810)
*Shankar Prasad Sastry*

Main category: cs.CG

TL;DR: 提出一种回归算法，基于Voronoi单元组合和混合线性函数逼近离散数据，避免维度灾难，在MNIST数据集上展现适用性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 提供一种能逼近离散数据的连续、分段平滑函数的回归算法，解决高维问题。

Method: 从种子顶点推断Voronoi单元，为每个单元及其周围的输入数据构建线性函数，不明确计算Voronoi图。

Result: 在MNIST数据集上，自由度为722,200时，准确率约98.2%。

Conclusion: 该算法具有适用性和可扩展性。

Abstract: I present a regression algorithm that provides a continuous, piecewise-smooth
function approximating scattered data. It is based on composing and blending
linear functions over Voronoi cells, and it scales to high dimensions. The
algorithm infers Voronoi cells from seed vertices and constructs a linear
function for the input data in and around each cell. As the algorithm does not
explicitly compute the Voronoi diagram, it avoids the curse of dimensionality.
An accuracy of around 98.2% on the MNIST dataset with 722,200 degrees of
freedom (without data augmentation, convolution, or other geometric operators)
demonstrates the applicability and scalability of the algorithm.

</details>


### [644] [Fast Witness Persistence for MRI Volumes via Hybrid Landmarking](https://arxiv.org/abs/2510.04553)
*Jorge Leonardo Ruiz Williams*

Main category: cs.CG

TL;DR: 提出适用于全脑MRI体积的可扩展基于见证的持久同调管道，在多数据集上快速执行，已打包发布。


<details>
  <summary>Details</summary>
Motivation: 为全脑MRI体积开发可扩展的持久同调管道，避免传统过滤方法的组合爆炸问题。

Method: 结合密度感知的地标选择和GPU就绪的见证过滤，用混合指标对候选地标评分。

Result: 地标集平均成对距离比随机或仅基于密度的基线缩小30 - 60%，在多个数据集上10秒内完成基准测试。

Conclusion: 开发的包whale - tda可用于医学成像工作流，有快速预设和可复现脚本。

Abstract: We introduce a scalable witness-based persistent homology pipeline for
full-brain MRI volumes that couples density-aware landmark selection with a
GPU-ready witness filtration. Candidates are scored by a hybrid metric that
balances geometric coverage against inverse kernel density, yielding landmark
sets that shrink mean pairwise distances by 30-60% over random or density-only
baselines while preserving topological features. Benchmarks on BrainWeb, IXI,
and synthetic manifolds execute in under ten seconds on a single NVIDIA RTX
4090 GPU, avoiding the combinatorial blow-up of Cech, Vietoris-Rips, and alpha
filtrations. The package is distributed on PyPI as whale-tda (installable via
pip); source and issues are hosted at https://github.com/jorgeLRW/whale. The
release also exposes a fast preset (mri_deep_dive_fast) for exploratory sweeps,
and ships with reproducibility-focused scripts and artifacts for drop-in use in
medical imaging workflows.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [645] [Strategy Logic, Imperfect Information, and Hyperproperties](https://arxiv.org/abs/2510.03952)
*Raven Beutner,Bernd Finkbeiner*

Main category: cs.LO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Strategy logic (SL) is a powerful temporal logic that enables first-class
reasoning over strategic behavior in multi-agent systems (MAS). In many MASs,
the agents (and their strategies) cannot observe the global state of the
system, leading to many extensions of SL centered around imperfect information,
such as strategy logic with imperfect information (SL$_\mathit{ii}$). Along
orthogonal lines, researchers have studied the combination of strategic
behavior and hyperproperties. Hyperproperties are system properties that relate
multiple executions in a system and commonly arise when specifying security
policies. Hyper Strategy Logic (HyperSL) is a temporal logic that combines
quantification over strategies with the ability to express hyperproperties on
the executions of different strategy profiles. In this paper, we study the
relation between SL$_\mathit{ii}$ and HyperSL. Our main result is that both
logics (restricted to formulas where no state formulas are nested within path
formulas) are equivalent in the sense that we can encode SL$_\mathit{ii}$
instances into HyperSL instances and vice versa. For the former direction, we
build on the well-known observation that imperfect information is a
hyperproperty. For the latter direction, we construct a self-composition of
MASs and show how we can simulate hyperproperties using imperfect information.

</details>


### [646] [Curved Boolean Logic: A Contextual Generalization of Propositional Logic with Algorithmic Consequences](https://arxiv.org/abs/2510.04716)
*Maximilian R. P. von Liechtenstein*

Main category: cs.LO

TL;DR: 论文介绍弯曲布尔逻辑（CBL），给出等价语义和证明演算，分析复杂度，提出运算算子，建模噪声并提供显著性控制，还关联相关框架及领域。


<details>
  <summary>Details</summary>
Motivation: 推广命题逻辑，引入类似几何曲率的局部真值赋值概念。

Method: 给出等价的层和排他图语义、上下文感知证明演算，形式化CBL - SAT及复杂度分析，提出运算算子，对噪声建模并进行显著性控制。

Result: CBL - SAT一般为NP完全问题，运算算子可在经典硬件上提前消除矛盾。

Conclusion: 定位CBL与其他框架关系，指出与SAT/CSP和大语言模型相关领域的联系。

Abstract: Curved Boolean Logic (CBL) generalizes propositional logic by allowing local
truth assignments that do not extend to a single global valuation, analogous to
curvature in geometry. We give equivalent sheaf and exclusivity-graph semantics
and a context-aware proof calculus that is conservative in the flat limit. We
formalize CBL-SAT and basic complexity (NP-complete in general) and present
operational operators (CBL-AC and CBL-CONS) that prune contradictions earlier
on classical hardware. We model noise with iid, AR(1)-correlated, and
adversarial bounded perturbations and provide permutation-based significance
with Benjamini-Hochberg FDR control. A Colab-ready notebook (ancillary files)
regenerates all figures and statistics. We position CBL relative to KCBS, CSW,
and sheaf frameworks and outline links to SAT/CSP and robustness/adapter
stability in large language models.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [647] [Drax: Speech Recognition with Discrete Flow Matching](https://arxiv.org/abs/2510.04162)
*Aviv Navon,Aviv Shamsian,Neta Glazer,Yael Segal-Feldman,Gill Hetz,Joseph Keshet,Ethan Fetaya*

Main category: eess.AS

TL;DR: 提出用于自动语音识别的离散流匹配框架Drax，能高效并行解码，在准确率和效率上表现良好。


<details>
  <summary>Details</summary>
Motivation: 扩散和基于流的非自回归模型在自动语音识别中的潜力未被充分探索，希望找到提升该领域性能的方法。

Method: 构建音频条件概率路径引导模型，理论分析将泛化差距与训练和推理占用率的差异联系起来。

Result: 达到与现有先进语音模型相当的识别准确率，且在准确率 - 效率权衡上表现更好。

Conclusion: 离散流匹配是推进非自回归自动语音识别的有前景方向。

Abstract: Diffusion and flow-based non-autoregressive (NAR) models have shown strong
promise in large language modeling, however, their potential for automatic
speech recognition (ASR) remains largely unexplored. We propose Drax, a
discrete flow matching framework for ASR that enables efficient parallel
decoding. To better align training with inference, we construct an
audio-conditioned probability path that guides the model through trajectories
resembling likely intermediate inference errors, rather than direct random
noise to target transitions. Our theoretical analysis links the generalization
gap to divergences between training and inference occupancies, controlled by
cumulative velocity errors, thereby motivating our design choice. Empirical
evaluation demonstrates that our approach attains recognition accuracy on par
with state-of-the-art speech models while offering improved accuracy-efficiency
trade-offs, highlighting discrete flow matching as a promising direction for
advancing NAR ASR.

</details>


### [648] [AURA Score: A Metric For Holistic Audio Question Answering Evaluation](https://arxiv.org/abs/2510.04934)
*Satvik Dixit,Soham Deshmukh,Bhiksha Raj*

Main category: eess.AS

TL;DR: 论文指出音频问答评估指标存在问题，推出AQEval基准，分析现有指标，提出AURA分数，实现与人类评分高相关，发布资源支持研究。


<details>
  <summary>Details</summary>
Motivation: 现有音频问答评估指标多改编自NLP和音频字幕，依赖表面相似度，未考虑问题上下文、推理和部分正确性，需要更好的评估方法。

Method: 引入AQEval基准进行系统评估，分析现有指标，提出AURA分数。

Result: AURA分数在AQEval上与人类评分的相关性达到了最先进水平，显著优于所有基线。

Conclusion: 当前音频问答评估方法有局限性，需更好的指标，发布AQEval基准和AURA指标以支持整体音频问答评估研究。

Abstract: Audio Question Answering (AQA) is a key task for evaluating Audio-Language
Models (ALMs), yet assessing open-ended responses remains challenging. Existing
metrics used for AQA such as BLEU, METEOR and BERTScore, mostly adapted from
NLP and audio captioning, rely on surface similarity and fail to account for
question context, reasoning, and partial correctness. To address the gap in
literature, we make three contributions in this work. First, we introduce
AQEval to enable systematic benchmarking of AQA metrics. It is the first
benchmark of its kind, consisting of 10k model responses annotated by multiple
humans for their correctness and relevance. Second, we conduct a comprehensive
analysis of existing AQA metrics on AQEval, highlighting weak correlation with
human judgment, especially for longer answers. Third, we propose a new metric -
AURA score, to better evaluate open-ended model responses. On AQEval, AURA
achieves state-of-the-art correlation with human ratings, significantly
outperforming all baselines. Through this work, we aim to highlight the
limitations of current AQA evaluation methods and motivate better metrics. We
release both the AQEval benchmark and the AURA metric to support future
research in holistic AQA evaluation.

</details>


### [649] [MuFFIN: Multifaceted Pronunciation Feedback Model with Interactive Hierarchical Neural Modeling](https://arxiv.org/abs/2510.04956)
*Bi-Cheng Yan,Ming-Kang Tsai,Berlin Chen*

Main category: eess.AS

TL;DR: 本文提出MuFFIN模型联合解决MDD和APA任务，提出正则机制优化模型，设计训练目标解决数据不平衡问题，实验证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 现有研究常将MDD和APA作为独立任务处理，本文旨在联合解决这两个任务。

Method: 引入MuFFIN模型，提出音素对比序数正则机制优化模型，设计训练目标解决MDD数据不平衡问题。

Result: 在Speechocean762基准数据集上实验，相比前沿基线方法，在APA和MDD任务上表现达到最优。

Conclusion: 所提方法能有效联合解决MDD和APA任务，具有良好效果。

Abstract: Computer-assisted pronunciation training (CAPT) manages to facilitate
second-language (L2) learners to practice pronunciation skills by offering
timely and instructive feedback. To examine pronunciation proficiency from
multiple facets, existing methods for CAPT broadly fall into two categories:
mispronunciation detection and diagnosis (MDD) as well as automatic
pronunciation assessment (APA). The former aims to pinpoint phonetic
pronunciation errors and provide diagnostic feedback, while the latter seeks
instead to quantify pronunciation proficiency pertaining to various aspects.
Despite the natural complementarity between MDD and APA, researchers and
practitioners, however, often treat them as independent tasks with disparate
modeling paradigms. In light of this, we in this paper first introduce MuFFIN,
a Multi-Faceted pronunciation Feedback model with an Interactive hierarchical
Neural architecture, to jointly address the tasks of MDD and APA. To better
capture the nuanced distinctions between phonemes in the feature space, a novel
phoneme-contrastive ordinal regularization mechanism is then put forward to
optimize the proposed model to generate more phoneme-discriminative features
while factoring in the ordinality of the aspect scores. In addition, to address
the intricate data imbalance problem in MDD, we design a simple yet effective
training objective, which is specifically tailored to perturb the outputs of a
phoneme classifier with the phoneme-specific variations, so as to better render
the distribution of predicted phonemes meanwhile considering their
mispronunciation characteristics. A series of experiments conducted on the
Speechocean762 benchmark dataset demonstrates the efficacy of our method in
relation to several cutting-edge baselines, showing state-of-the-art
performance on both the APA and MDD tasks.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [650] [Linguistic and Audio Embedding-Based Machine Learning for Alzheimer's Dementia and Mild Cognitive Impairment Detection: Insights from the PROCESS Challenge](https://arxiv.org/abs/2510.03336)
*Adharsha Sam Edwin Sam Devahi,Sohail Singh Sangha,Prachee Priyadarshinee,Jithin Thilakan,Ivan Fu Xing Tan,Christopher Johann Clarke,Sou Ka Lon,Balamurali B T,Yow Wei Quin,Chen Jer-Ming*

Main category: cs.SD

TL;DR: 本文提出机器学习框架，结合语音音频嵌入和语言特征进行认知衰退检测，结果显示模型在回归任务表现佳，凸显多模态语音方法在痴呆检测中的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前阿尔茨海默病和轻度认知障碍诊断方法资源消耗大且有侵入性，语音是有潜力的非侵入性生物标志物。

Method: 为PROCESS挑战构建机器学习框架，提取语音音频嵌入和语言特征，用分类和回归模型区分健康人、MCI和AD患者并预测MMSE分数。

Result: 基于语言特征的投票集成模型分类性能最佳，基于Whisper嵌入的集成回归器MMSE预测误差最低，模型在回归任务排名靠前，分类处于中游。

Conclusion: 多模态语音方法在可扩展、非侵入性认知评估中有潜力，整合特定任务语言和声学标记对痴呆检测很重要。

Abstract: Early detection of Alzheimer's Dementia (AD) and Mild Cognitive Impairment
(MCI) is critical for timely intervention, yet current diagnostic approaches
remain resource-intensive and invasive. Speech, encompassing both acoustic and
linguistic dimensions, offers a promising non-invasive biomarker for cognitive
decline. In this study, we present a machine learning framework for the PROCESS
Challenge, leveraging both audio embeddings and linguistic features derived
from spontaneous speech recordings. Audio representations were extracted using
Whisper embeddings from the Cookie Theft description task, while linguistic
features-spanning pronoun usage, syntactic complexity, filler words, and clause
structure-were obtained from transcriptions across Semantic Fluency, Phonemic
Fluency, and Cookie Theft picture description. Classification models aimed to
distinguish between Healthy Controls (HC), MCI, and AD participants, while
regression models predicted Mini-Mental State Examination (MMSE) scores.
Results demonstrated that voted ensemble models trained on concatenated
linguistic features achieved the best classification performance (F1 = 0.497),
while Whisper embedding-based ensemble regressors yielded the lowest MMSE
prediction error (RMSE = 2.843). Comparative evaluation within the PROCESS
Challenge placed our models among the top submissions in regression task, and
mid-range for classification, highlighting the complementary strengths of
linguistic and audio embeddings. These findings reinforce the potential of
multimodal speech-based approaches for scalable, non-invasive cognitive
assessment and underline the importance of integrating task-specific linguistic
and acoustic markers in dementia detection.

</details>


### [651] [Pitch-Conditioned Instrument Sound Synthesis From an Interactive Timbre Latent Space](https://arxiv.org/abs/2510.04339)
*Christian Limberg,Fares Schulz,Zhe Zhang,Stefan Weinzierl*

Main category: cs.SD

TL;DR: 提出两阶段半监督学习框架用于神经乐器声音合成，可从音色潜在空间生成高质量音乐样本。


<details>
  <summary>Details</summary>
Motivation: 现有音乐制作方法依赖高维潜在表征，难导航且用户体验不直观，需改进。

Method: 采用两阶段训练范式，先用变分自编码器训练音频样本的音高 - 音色解耦二维表征，再将其作为基于Transformer的生成模型的条件输入。

Result: 模型能有效学习解耦的音色空间，在保持音高准确性的同时捕捉音色细微变化，通过交互式网络应用展示了可用性。

Conclusion: 该方法为未来直观且富有创造力的音乐制作环境迈出了一步。

Abstract: This paper presents a novel approach to neural instrument sound synthesis
using a two-stage semi-supervised learning framework capable of generating
pitch-accurate, high-quality music samples from an expressive timbre latent
space. Existing approaches that achieve sufficient quality for music production
often rely on high-dimensional latent representations that are difficult to
navigate and provide unintuitive user experiences. We address this limitation
through a two-stage training paradigm: first, we train a pitch-timbre
disentangled 2D representation of audio samples using a Variational
Autoencoder; second, we use this representation as conditioning input for a
Transformer-based generative model. The learned 2D latent space serves as an
intuitive interface for navigating and exploring the sound landscape. We
demonstrate that the proposed method effectively learns a disentangled timbre
space, enabling expressive and controllable audio generation with reliable
pitch conditioning. Experimental results show the model's ability to capture
subtle variations in timbre while maintaining a high degree of pitch accuracy.
The usability of our method is demonstrated in an interactive web application,
highlighting its potential as a step towards future music production
environments that are both intuitive and creatively empowering:
https://pgesam.faresschulz.com

</details>


### [652] [Lightweight and Generalizable Acoustic Scene Representations via Contrastive Fine-Tuning and Distillation](https://arxiv.org/abs/2510.03728)
*Kuang Yuan,Yang Gao,Xilin Li,Xinhao Mei,Syavosh Zadissa,Tarun Pruthi,Saeed Bagheri Sereshki*

Main category: cs.SD

TL;DR: 提出ContrastASC方法，可学习通用声学场景表征，适应未见类别，评估显示其在少样本适应和闭集性能上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有边缘设备上的声学场景分类（ASC）模型缺乏可迁移性，难以适应新的或细化的声学类别。

Method: 结合预训练模型的有监督对比微调与对比表征蒸馏，将结构化知识转移到紧凑的学生模型。

Result: ContrastASC在少样本适应未见类别方面表现更好，同时保持了较强的闭集性能。

Conclusion: ContrastASC能有效学习通用声学场景表征，适应未见类别，有较好的性能表现。

Abstract: Acoustic scene classification (ASC) models on edge devices typically operate
under fixed class assumptions, lacking the transferability needed for
real-world applications that require adaptation to new or refined acoustic
categories. We propose ContrastASC, which learns generalizable acoustic scene
representations by structuring the embedding space to preserve semantic
relationships between scenes, enabling adaptation to unseen categories without
retraining. Our approach combines supervised contrastive fine-tuning of
pre-trained models with contrastive representation distillation to transfer
this structured knowledge to compact student models. Our evaluation shows that
ContrastASC demonstrates improved few-shot adaptation to unseen categories
while maintaining strong closed-set performance.

</details>


### [653] [Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba](https://arxiv.org/abs/2510.04738)
*Baher Mohammad,Magauiya Zhussip,Stamatios Lefkimmiatis*

Main category: cs.SD

TL;DR: 本文介绍了用于语音编辑和合成的MAVE架构，在语音编辑和零样本TTS中表现出色，内存成本低。


<details>
  <summary>Details</summary>
Motivation: 提出一种新的自回归架构用于文本条件语音编辑和高保真文本转语音合成。

Method: 基于交叉注意力的Mamba骨干构建MAVE，结合Mamba进行高效音频序列建模和交叉注意力实现精确文本 - 声学对齐。

Result: 在语音编辑中达到SOTA，零样本TTS结果有竞争力，多数情况下编辑语音与原语音难区分，优于VoiceCraft和FluentSpeech，内存成本低。

Conclusion: MAVE通过结构化状态空间建模和跨模态注意力的协同集成，为语音编辑和合成树立了新标准。

Abstract: We introduce MAVE (Mamba with Cross-Attention for Voice Editing and
Synthesis), a novel autoregressive architecture for text-conditioned voice
editing and high-fidelity text-to-speech (TTS) synthesis, built on a
cross-attentive Mamba backbone. MAVE achieves state-of-the-art performance in
speech editing and very competitive results in zero-shot TTS, while not being
explicitly trained on the latter task, outperforming leading autoregressive and
diffusion models on diverse, real-world audio. By integrating Mamba for
efficient audio sequence modeling with cross-attention for precise
text-acoustic alignment, MAVE enables context-aware voice editing with
exceptional naturalness and speaker consistency. In pairwise human evaluations
on a random 40-sample subset of the RealEdit benchmark (400 judgments), 57.2%
of listeners rated MAVE - edited speech as perceptually equal to the original,
while 24.8% prefered the original and 18.0% MAVE - demonstrating that in the
majority of cases edits are indistinguishable from the source. MAVE compares
favorably with VoiceCraft and FluentSpeech both on pairwise comparisons and
standalone mean opinion score (MOS) evaluations. For zero-shot TTS, MAVE
exceeds VoiceCraft in both speaker similarity and naturalness, without
requiring multiple inference runs or post-processing. Remarkably, these quality
gains come with a significantly lower memory cost and approximately the same
latency: MAVE requires ~6x less memory than VoiceCraft during inference on
utterances from the RealEdit database (mean duration: 6.21s, A100, FP16, batch
size 1). Our results demonstrate that MAVE establishes a new standard for
flexible, high-fidelity voice editing and synthesis through the synergistic
integration of structured state-space modeling and cross-modal attention.

</details>


### [654] [Language Model Based Text-to-Audio Generation: Anti-Causally Aligned Collaborative Residual Transformers](https://arxiv.org/abs/2510.04577)
*Juncheng Wang,Chao Xu,Cheng Yu,Zhe Hu,Haoyu Xie,Guoqi Yu,Lei Shang,Shujun Wang*

Main category: cs.SD

TL;DR: 当前基于语言模型和RVQ分词器的文本到音频生成落后于扩散模型，本文提出Siren框架解决问题，实验显示其表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决基于语言模型和RVQ分词器的文本到音频生成落后于扩散模型的问题，找出差距背后的关键困境。

Method: 分析RVQ动态，找出两个关键限制，提出Siren框架，采用多个隔离的变压器，通过强化学习进行因果调节和反因果对齐。

Result: Siren优于现有的基于语言模型和扩散模型的文本到音频系统，取得了最先进的成果。

Conclusion: 该方法使语言模型在文本到音频任务中成为扩散模型的有力竞争者，为统一的多模态生成框架提供了途径。

Abstract: While language models (LMs) paired with residual vector quantization (RVQ)
tokenizers have shown promise in text-to-audio (T2A) generation, they still lag
behind diffusion-based models by a non-trivial margin. We identify a critical
dilemma underpinning this gap: incorporating more RVQ layers improves audio
reconstruction fidelity but exceeds the generation capacity of conventional
LMs. To address this, we first analyze RVQ dynamics and uncover two key
limitations: 1) orthogonality of features across RVQ layers hinders effective
LMs training, and 2) descending semantic richness in tokens from deeper RVQ
layers exacerbates exposure bias during autoregressive decoding. Based on these
insights, we propose Siren, a novel LM-based framework that employs multiple
isolated transformers with causal conditioning and anti-causal alignment via
reinforcement learning. Extensive experiments demonstrate that Siren
outperforms both existing LM-based and diffusion-based T2A systems, achieving
state-of-the-art results. By bridging the representational strengths of LMs
with the fidelity demands of audio synthesis, our approach repositions LMs as
competitive contenders against diffusion models in T2A tasks. Moreover, by
aligning audio representations with linguistic structures, Siren facilitates a
promising pathway toward unified multi-modal generation frameworks.

</details>


### [655] [A Study on the Data Distribution Gap in Music Emotion Recognition](https://arxiv.org/abs/2510.04688)
*Joann Ching,Gerhard Widmer*

Main category: cs.SD

TL;DR: 论文研究音乐情感识别，用五个含维度情感标注的数据集，揭示分布外泛化问题，提出结合Jukebox模型嵌入和色度特征的框架提升跨数据集泛化能力。


<details>
  <summary>Details</summary>
Motivation: 以往研究多关注特定音乐风格，未在单一框架纳入多样音乐类型，本文旨在解决从音频内容识别情感这一任务。

Method: 研究五个含维度情感标注的数据集，进行系统实验，结合Jukebox模型提取的嵌入和色度特征。

Result: 揭示了分布外泛化问题，发现了现有数据中流派 - 情感关系、潜在流派主导和数据集偏差。

Conclusion: 提出的简单有效框架结合多样训练集，能显著提升模型的跨数据集泛化能力。

Abstract: Music Emotion Recognition (MER) is a task deeply connected to human
perception, relying heavily on subjective annotations collected from
contributors. Prior studies tend to focus on specific musical styles rather
than incorporating a diverse range of genres, such as rock and classical,
within a single framework. In this paper, we address the task of recognizing
emotion from audio content by investigating five datasets with dimensional
emotion annotations -- EmoMusic, DEAM, PMEmo, WTC, and WCMED -- which span
various musical styles. We demonstrate the problem of out-of-distribution
generalization in a systematic experiment. By closely looking at multiple data
and feature sets, we provide insight into genre-emotion relationships in
existing data and examine potential genre dominance and dataset biases in
certain feature representations. Based on these experiments, we arrive at a
simple yet effective framework that combines embeddings extracted from the
Jukebox model with chroma features and demonstrate how, alongside a combination
of several diverse training sets, this permits us to train models with
substantially improved cross-dataset generalization capabilities.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [656] [InstructPLM-mu: 1-Hour Fine-Tuning of ESM2 Beats ESM3 in Protein Mutation Predictions](https://arxiv.org/abs/2510.03370)
*Junde Xu,Yapin Shi,Lijun Lang,Taoyong Cui,Zhiming Zhang,Guangyong Chen,Jiezhong Qiu,Pheng-Ann Heng*

Main category: q-bio.QM

TL;DR: 提出InstructPLM - mu微调框架，用结构输入微调ESM2可达到与ESM3相当性能，研究表明融合方法和微调策略影响准确率。


<details>
  <summary>Details</summary>
Motivation: 从头训练多模态蛋白质语言模型需要大量计算资源，探究预训练的仅序列蛋白质语言模型的多模态微调能否达到端到端训练模型的性能。

Method: 提出InstructPLM - mu微调框架，系统比较三种不同的特征融合设计和微调方法。

Result: 用结构输入微调ESM2能达到与ESM3相当的性能，融合方法和微调策略对最终准确率影响很大。

Conclusion: 为向预训练蛋白质语言模型注入结构信息提供实用指导，激励对更好融合机制和微调协议的进一步研究。

Abstract: Multimodal protein language models deliver strong performance on
mutation-effect prediction, but training such models from scratch demands
substantial computational resources. In this paper, we propose a fine-tuning
framework called InstructPLM-mu and try to answer a question: \textit{Can
multimodal fine-tuning of a pretrained, sequence-only protein language model
match the performance of models trained end-to-end? } Surprisingly, our
experiments show that fine-tuning ESM2 with structural inputs can reach
performance comparable to ESM3. To understand how this is achieved, we
systematically compare three different feature-fusion designs and fine-tuning
recipes. Our results reveal that both the fusion method and the tuning strategy
strongly affect final accuracy, indicating that the fine-tuning process is not
trivial. We hope this work offers practical guidance for injecting structure
into pretrained protein language models and motivates further research on
better fusion mechanisms and fine-tuning protocols.

</details>


### [657] [TCR-EML: Explainable Model Layers for TCR-pMHC Prediction](https://arxiv.org/abs/2510.04377)
*Jiarui Li,Zixiang Yin,Zhengming Ding,Samuel J. Landry,Ramgopal R. Mettu*

Main category: q-bio.QM

TL;DR: 提出可解释模型层TCR - EML用于TCR - pMHC建模，实验显示有竞争力的预测准确性、泛化性和更好的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有TCR - pMHC结合预测模型多为黑盒模型，缺乏可解释性，事后解释方法未明确建模生化机制，且‘设计即解释’模型未用于TCR - pMHC结合。

Method: 提出可解释模型层TCR - EML，将其融入蛋白质语言模型主干，使用从已知TCR - pMHC结合机制得出的氨基酸残基接触原型层。

Result: 在大规模数据集实验中展现出有竞争力的预测准确性和泛化性，在TCR - XAI基准测试中可解释性优于现有方法。

Conclusion: TCR - EML是一种有效的TCR - pMHC结合预测方法，能提供高质量解释。

Abstract: T cell receptor (TCR) recognition of peptide-MHC (pMHC) complexes is a
central component of adaptive immunity, with implications for vaccine design,
cancer immunotherapy, and autoimmune disease. While recent advances in machine
learning have improved prediction of TCR-pMHC binding, the most effective
approaches are black-box transformer models that cannot provide a rationale for
predictions. Post-hoc explanation methods can provide insight with respect to
the input but do not explicitly model biochemical mechanisms (e.g. known
binding regions), as in TCR-pMHC binding. ``Explain-by-design'' models (i.e.,
with architectural components that can be examined directly after training)
have been explored in other domains, but have not been used for TCR-pMHC
binding. We propose explainable model layers (TCR-EML) that can be incorporated
into protein-language model backbones for TCR-pMHC modeling. Our approach uses
prototype layers for amino acid residue contacts drawn from known TCR-pMHC
binding mechanisms, enabling high-quality explanations for predicted TCR-pMHC
binding. Experiments of our proposed method on large-scale datasets demonstrate
competitive predictive accuracy and generalization, and evaluation on the
TCR-XAI benchmark demonstrates improved explainability compared with existing
approaches.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [658] [Convergence in probability of numerical solutions of a highly non-linear delayed stochastic interest rate model](https://arxiv.org/abs/2510.04092)
*Emmanuel Coffie*

Main category: math.PR

TL;DR: 研究带超线性增长系数的延迟随机利率模型，建立解的性质，证明收敛性并给出数值例子及蒙特卡罗评估依据。


<details>
  <summary>Details</summary>
Motivation: 研究带超线性增长系数的延迟随机利率模型解的性质及收敛情况。

Method: 开发新的数学工具研究真实解和截断欧拉 - 马吕雅马（EM）解的性质。

Result: 证明了步长趋于 0 时，真实解依概率收敛到截断 EM 解，给出数值例子支持结果。

Conclusion: 收敛结果可用于金融量的蒙特卡罗评估。

Abstract: We examine a delayed stochastic interest rate model with super-linearly
growing coefficients and develop several new mathematical tools to establish
the properties of its true and truncated EM solutions. Moreover, we show that
the true solution converges to the truncated EM solutions in probability as the
step size tends to zero. Further, we support the convergence result with some
illustrative numerical examples and justify the convergence result for the
Monte Carlo evaluation of some financial quantities.

</details>


### [659] [Perspectives on Stochastic Localization](https://arxiv.org/abs/2510.04460)
*Bobby Shi,Kevin Tian,Matthew S. Zhang*

Main category: math.PR

TL;DR: 对Eldan随机定位过程的不同视角进行调研，强调不同构造间的联系，促进其可及性和未来应用。


<details>
  <summary>Details</summary>
Motivation: 现有关于此话题的调研缺乏自包含的不同构造呈现及联系阐述，希望拓宽随机定位的可及性并方便未来使用。

Method: 对Eldan随机定位的所有已知替代构造进行自包含的展示，并着重分析不同构造间的联系。

Result: 文中未提及明确结果。

Conclusion: 文中未提及明确结论。

Abstract: We survey different perspectives on the stochastic localization process of
[Eld13], a powerful construction that has had many exciting recent applications
in high-dimensional probability and algorithm design. Unlike prior surveys on
this topic, our focus is on giving a self-contained presentation of all known
alternative constructions of Eldan's stochastic localization, with an emphasis
on connections between different constructions. Our hope is that by collecting
these perspectives, some of which had primarily arisen within a particular
community (e.g., probability theory, theoretical computer science, information
theory, or machine learning), we can broaden the accessibility of stochastic
localization, and ease its future use.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [660] [Paris: A Decentralized Trained Open-Weight Diffusion Model](https://arxiv.org/abs/2510.03434)
*Zhiying Jiang,Raihan Seraj,Marcos Villagra,Bidhan Roy*

Main category: cs.GR

TL;DR: 介绍首个完全通过去中心化计算预训练的扩散模型Paris，它无需中央协调基础设施，训练要求低且质量可比中心化模型。


<details>
  <summary>Details</summary>
Motivation: 实现无需中央协调基础设施的高质量文本到图像生成。

Method: 从头实现分布式扩散训练框架，用8个专家扩散模型独立训练，分区数据，用轻量级变压器路由器动态选择专家。

Result: 训练无需同步，可在异构硬件上进行，生成质量与中心化基线相当，使用更少训练数据和计算资源。

Conclusion: Paris的去中心化训练在保持生成质量的同时，去除了大规模扩散模型对专用GPU集群的需求。

Abstract: We present Paris, the first publicly released diffusion model pre-trained
entirely through decentralized computation. Paris demonstrates that
high-quality text-to-image generation can be achieved without centrally
coordinated infrastructure. Paris is open for research and commercial use.
Paris required implementing our Distributed Diffusion Training framework from
scratch. The model consists of 8 expert diffusion models (129M-605M parameters
each) trained in complete isolation with no gradient, parameter, or
intermediate activation synchronization. Rather than requiring synchronized
gradient updates across thousands of GPUs, we partition data into semantically
coherent clusters where each expert independently optimizes its subset while
collectively approximating the full distribution. A lightweight transformer
router dynamically selects appropriate experts at inference, achieving
generation quality comparable to centrally coordinated baselines. Eliminating
synchronization enables training on heterogeneous hardware without specialized
interconnects. Empirical validation confirms that Paris's decentralized
training maintains generation quality while removing the dedicated GPU cluster
requirement for large-scale diffusion models. Paris achieves this using
14$\times$ less training data and 16$\times$ less compute than the prior
decentralized baseline.

</details>


### [661] [Creative synthesis of kinematic mechanisms](https://arxiv.org/abs/2510.03308)
*Jiong Lin,Jialong Ning,Judah Goldfeder,Hod Lipson*

Main category: cs.GR

TL;DR: 本文将平面连杆机构运动学综合问题转化为跨域图像生成任务，用共享潜在变分自编码器探索图像生成模型合成运动曲线和模拟新运动学的潜力，并在不同复杂度数据集上验证方法有效性。


<details>
  <summary>Details</summary>
Motivation: 探索图像生成模型在平面连杆机构运动学综合中的应用，实现机械设计的生成式设计。

Method: 将问题转化为图像生成任务，构建平面连杆机构数据集，采用共享潜在变分自编码器，通过颜色梯度编码轨迹点绘制速度，在不同复杂度数据集上验证。

Result: 初步结果表明基于图像的表示方法在生成式机械设计中有效，能在统一图像生成框架下表示和合成多种机构。

Conclusion: 基于图像的表示方法可用于生成式机械设计，能在统一框架下对多种机械机构进行表示和合成。

Abstract: In this paper, we formulate the problem of kinematic synthesis for planar
linkages as a cross-domain image generation task. We develop a planar linkages
dataset using RGB image representations, covering a range of mechanisms: from
simple types such as crank-rocker and crank-slider to more complex eight-bar
linkages like Jansen's mechanism. A shared-latent variational autoencoder (VAE)
is employed to explore the potential of image generative models for
synthesizing unseen motion curves and simulating novel kinematics. By encoding
the drawing speed of trajectory points as color gradients, the same
architecture also supports kinematic synthesis conditioned on both trajectory
shape and velocity profiles. We validate our method on three datasets of
increasing complexity: a standard four-bar linkage set, a mixed set of four-bar
and crank-slider mechanisms, and a complex set including multi-loop mechanisms.
Preliminary results demonstrate the effectiveness of image-based
representations for generative mechanical design, showing that mechanisms with
revolute and prismatic joints, and potentially cams and gears, can be
represented and synthesized within a unified image generation framework.

</details>


### [662] [Neon: Negative Extrapolation From Self-Training Improves Image Generation](https://arxiv.org/abs/2510.03597)
*Sina Alemohammad,Zhangyang Wang,Richard G. Baraniuk*

Main category: cs.GR

TL;DR: 现有生成式AI模型扩展受高质量训练数据稀缺瓶颈，合成数据微调会致模型崩溃，本文提出Neon方法以改善模型与真实数据分布对齐，实现效果提升且易实现、计算成本低。


<details>
  <summary>Details</summary>
Motivation: 解决生成式AI模型扩展中高质量训练数据稀缺问题，避免合成数据微调导致的模型崩溃。

Method: Neon方法先在自合成数据上微调基础模型，再反转梯度更新以远离退化权重，通过负外推纠正合成与真实数据群体梯度的反对齐。

Result: Neon易实现，仅需少量合成样本和少量额外计算，在多种架构和数据集上有良好表现，如在ImageNet 256x256上提升xAR - L模型FID至1.02。

Conclusion: Neon方法有效，能提升模型与真实数据分布的对齐，具有普遍性和高效性。

Abstract: Scaling generative AI models is bottlenecked by the scarcity of high-quality
training data. The ease of synthesizing from a generative model suggests using
(unverified) synthetic data to augment a limited corpus of real data for the
purpose of fine-tuning in the hope of improving performance. Unfortunately,
however, the resulting positive feedback loop leads to model autophagy disorder
(MAD, aka model collapse) that results in a rapid degradation in sample quality
and/or diversity. In this paper, we introduce Neon (for Negative Extrapolation
frOm self-traiNing), a new learning method that turns the degradation from
self-training into a powerful signal for self-improvement. Given a base model,
Neon first fine-tunes it on its own self-synthesized data but then,
counterintuitively, reverses its gradient updates to extrapolate away from the
degraded weights. We prove that Neon works because typical inference samplers
that favor high-probability regions create a predictable anti-alignment between
the synthetic and real data population gradients, which negative extrapolation
corrects to better align the model with the true data distribution. Neon is
remarkably easy to implement via a simple post-hoc merge that requires no new
real data, works effectively with as few as 1k synthetic samples, and typically
uses less than 1% additional training compute. We demonstrate Neon's
universality across a range of architectures (diffusion, flow matching,
autoregressive, and inductive moment matching models) and datasets (ImageNet,
CIFAR-10, and FFHQ). In particular, on ImageNet 256x256, Neon elevates the
xAR-L model to a new state-of-the-art FID of 1.02 with only 0.36% additional
training compute. Code is available at https://github.com/SinaAlemohammad/Neon

</details>


### [663] [Diverse Text-to-Image Generation via Contrastive Noise Optimization](https://arxiv.org/abs/2510.03813)
*Byungjun Kim,Soobin Um,Jong Chul Ye*

Main category: cs.GR

TL;DR: 现有文本到图像扩散模型存在多样性不足问题，本文提出对比噪声优化方法，通过塑造初始噪声提升多样性，实验证明其有效且对超参数选择稳健。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型在文本引导推理下生成图像保真度高，但存在多样性不足问题，现有方法改进效果有限。

Method: 引入对比噪声优化方法，在Tweedie数据空间定义对比损失，优化一批噪声潜变量，使批次内实例相互排斥以最大化多样性，同时锚定参考样本以保持保真度。

Result: 在多个文本到图像骨干网络上的大量实验表明，该方法实现了更优的质量 - 多样性帕累托前沿，且对超参数选择稳健。

Conclusion: 对比噪声优化方法是一种简单有效的解决文本到图像扩散模型多样性问题的方法。

Abstract: Text-to-image (T2I) diffusion models have demonstrated impressive performance
in generating high-fidelity images, largely enabled by text-guided inference.
However, this advantage often comes with a critical drawback: limited
diversity, as outputs tend to collapse into similar modes under strong text
guidance. Existing approaches typically optimize intermediate latents or text
conditions during inference, but these methods deliver only modest gains or
remain sensitive to hyperparameter tuning. In this work, we introduce
Contrastive Noise Optimization, a simple yet effective method that addresses
the diversity issue from a distinct perspective. Unlike prior techniques that
adapt intermediate latents, our approach shapes the initial noise to promote
diverse outputs. Specifically, we develop a contrastive loss defined in the
Tweedie data space and optimize a batch of noise latents. Our contrastive
optimization repels instances within the batch to maximize diversity while
keeping them anchored to a reference sample to preserve fidelity. We further
provide theoretical insights into the mechanism of this preprocessing to
substantiate its effectiveness. Extensive experiments across multiple T2I
backbones demonstrate that our approach achieves a superior quality-diversity
Pareto frontier while remaining robust to hyperparameter choices.

</details>


### [664] [3Dify: a Framework for Procedural 3D-CG Generation Assisted by LLMs Using MCP and RAG](https://arxiv.org/abs/2510.04536)
*Shun-ichiro Hayashi,Daichi Mukunoki,Tetsuya Hoshino,Satoshi Ohshima,Takahiro Katagiri*

Main category: cs.GR

TL;DR: 本文提出利用大语言模型的3Dify框架，可通过自然语言指令生成3D-CG内容，介绍了其构建基础、技术及功能等。


<details>
  <summary>Details</summary>
Motivation: 提出一个仅通过自然语言指令就能生成3D-CG内容的框架。

Method: 基于Dify平台，结合MCP、RAG等技术，通过MCP自动化操作DCC工具，不支持时用CUA方法自动化GUI操作，允许用户反馈以提升图像质量，支持集成本地部署的LLM。

Result: 未提及具体结果。

Conclusion: 未提及明确结论。

Abstract: This paper proposes "3Dify," a procedural 3D computer graphics (3D-CG)
generation framework utilizing Large Language Models (LLMs). The framework
enables users to generate 3D-CG content solely through natural language
instructions. 3Dify is built upon Dify, an open-source platform for AI
application development, and incorporates several state-of-the-art LLM-related
technologies such as the Model Context Protocol (MCP) and Retrieval-Augmented
Generation (RAG). For 3D-CG generation support, 3Dify automates the operation
of various Digital Content Creation (DCC) tools via MCP. When DCC tools do not
support MCP-based interaction, the framework employs the Computer-Using Agent
(CUA) method to automate Graphical User Interface (GUI) operations. Moreover,
to enhance image generation quality, 3Dify allows users to provide feedback by
selecting preferred images from multiple candidates. The LLM then learns
variable patterns from these selections and applies them to subsequent
generations. Furthermore, 3Dify supports the integration of locally deployed
LLMs, enabling users to utilize custom-developed models and to reduce both time
and monetary costs associated with external API calls by leveraging their own
computational resources.

</details>


### [665] [Bridging Text and Video Generation: A Survey](https://arxiv.org/abs/2510.04999)
*Nilay Kumar,Priyansh Bhandari,G. Maragatham*

Main category: cs.GR

TL;DR: 本文对文本到视频生成模型进行全面综述，涵盖模型发展、数据集、训练配置、评估指标等，并指出当前挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 文本到视频生成技术有广泛应用前景，但存在对齐、连贯性和计算效率等挑战，需要全面综述该领域。

Method: 追溯模型从早期GANs和VAEs到混合Diffusion - Transformer架构的发展，介绍模型工作原理、解决的问题；系统介绍训练和评估数据集，详述训练配置；概述常用评估指标及性能，并讨论其局限性。

Result: 对文本到视频生成模型进行了全面梳理，呈现了模型发展、数据集、训练配置、评估指标等信息。

Conclusion: 指出当前开放挑战，提出几个有前景的未来研究方向，为后续研究提供参考。

Abstract: Text-to-video (T2V) generation technology holds potential to transform
multiple domains such as education, marketing, entertainment, and assistive
technologies for individuals with visual or reading comprehension challenges,
by creating coherent visual content from natural language prompts. From its
inception, the field has advanced from adversarial models to diffusion-based
models, yielding higher-fidelity, temporally consistent outputs. Yet challenges
persist, such as alignment, long-range coherence, and computational efficiency.
Addressing this evolving landscape, we present a comprehensive survey of
text-to-video generative models, tracing their development from early GANs and
VAEs to hybrid Diffusion-Transformer (DiT) architectures, detailing how these
models work, what limitations they addressed in their predecessors, and why
shifts toward new architectural paradigms were necessary to overcome challenges
in quality, coherence, and control. We provide a systematic account of the
datasets, which the surveyed text-to-video models were trained and evaluated
on, and, to support reproducibility and assess the accessibility of training
such models, we detail their training configurations, including their hardware
specifications, GPU counts, batch sizes, learning rates, optimizers, epochs,
and other key hyperparameters. Further, we outline the evaluation metrics
commonly used for evaluating such models and present their performance across
standard benchmarks, while also discussing the limitations of these metrics and
the emerging shift toward more holistic, perception-aligned evaluation
strategies. Finally, drawing from our analysis, we outline the current open
challenges and propose a few promising future directions, laying out a
perspective for future researchers to explore and build upon in advancing T2V
research and applications.

</details>


### [666] [SAEdit: Token-level control for continuous image editing via Sparse AutoEncoder](https://arxiv.org/abs/2510.05081)
*Ronen Kamenetsky,Sara Dorfman,Daniel Garibi,Roni Paiss,Or Patashnik,Daniel Cohen-Or*

Main category: cs.GR

TL;DR: 提出通过文本嵌入的标记级操作实现解纠缠和连续图像编辑的方法，实验显示其在不同属性和领域有效。


<details>
  <summary>Details</summary>
Motivation: 大规模文本到图像扩散模型中仅文本提示对编辑过程控制不足，期望实现解纠缠和连续控制。

Method: 通过沿精心选择的方向操纵文本嵌入来应用编辑，使用稀疏自动编码器（SAE）识别控制目标属性强度的方向，直接操作文本嵌入，不修改扩散过程。

Result: 能在不同属性和领域实现具有连续控制的直观高效操作。

Conclusion: 该方法模型无关，广泛适用于各种图像合成骨干网络。

Abstract: Large-scale text-to-image diffusion models have become the backbone of modern
image editing, yet text prompts alone do not offer adequate control over the
editing process. Two properties are especially desirable: disentanglement,
where changing one attribute does not unintentionally alter others, and
continuous control, where the strength of an edit can be smoothly adjusted. We
introduce a method for disentangled and continuous editing through token-level
manipulation of text embeddings. The edits are applied by manipulating the
embeddings along carefully chosen directions, which control the strength of the
target attribute. To identify such directions, we employ a Sparse Autoencoder
(SAE), whose sparse latent space exposes semantically isolated dimensions. Our
method operates directly on text embeddings without modifying the diffusion
process, making it model agnostic and broadly applicable to various image
synthesis backbones. Experiments show that it enables intuitive and efficient
manipulations with continuous control across diverse attributes and domains.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [667] [Short-circuiting Rings for Low-Latency AllReduce](https://arxiv.org/abs/2510.03491)
*Sarah-Michelle Hammer,Stefan Schmid,Rachee Singh,Vamsi Addanki*

Main category: cs.NI

TL;DR: 本文挑战环形算法只适用于大消息的假设，提出集体内自适应拓扑，设计电路交换启发式方法，初步评估显示其能让递归加倍算法更快完成。


<details>
  <summary>Details</summary>
Motivation: 挑战环形算法只适用于大消息、递归加倍适用于小消息的长期假设，探索集体通信更好方案。

Method: 考虑实际传播延迟和链路容量约束，设计电路交换启发式方法来平衡重配置延迟、传播延迟和链路拥塞。

Result: 初步评估显示，使用电路交换调度，递归加倍算法完成时间比静态环形拓扑上的环形全归约更快。

Conclusion: 强调实现实用的集体内光子交换的关键挑战和未来研究方向。

Abstract: Efficient collective communication is critical for many distributed ML and
HPC applications. In this context, it is widely believed that the Ring
algorithm for the AllReduce collective communication operation is optimal only
for large messages, while Recursive Doubling is preferable for small ones due
to its logarithmic number of steps compared to the linear number for Ring. In
this paper, we challenge this long-held assumption and show that the Ring
algorithm can remain optimal even for short messages in ring-based GPU-to-GPU
topologies, once realistic propagation delays and link capacity constraints are
accounted for. We find that the total propagation delay for both Ring and
Recursive Doubling essentially sums to the same value, but the latter incurs
significantly higher congestion due to longer hop counts, leading to increased
completion times. This surprising result motivates our case for in-collective
adaptive topologies, particularly in the context of emerging photonic
interconnects, which can break through the limitations of static topology
designs at the collective communication granularity. We design a \emph{simple
and fast} heuristic for circuit-switching that enables Recursive Doubling to
exploit dynamically reconfigurable photonic paths, carefully balancing
reconfiguration delays, propagation latencies, and link congestion to minimize
overall completion time. Our preliminary evaluations, using realistic
reconfiguration delays, show that our circuit-switching schedules enable faster
completion times for Recursive Doubling, even compared to Ring AllReduce on
static ring topologies. We conclude by highlighting key challenges and future
research directions for realizing practical, in-collective photonic switching.

</details>


### [668] [Scalable Ground Station Selection for Large LEO Constellations](https://arxiv.org/abs/2510.03438)
*Grace Ra Kim,Duncan Eddy,Vedant Srinivas,Mykel J. Kochenderfer*

Main category: cs.NI

TL;DR: 提出可扩展分层框架解决低轨卫星星座地面站选择问题，在合成和真实案例中表现良好。


<details>
  <summary>Details</summary>
Motivation: 传统地面站选择方法在处理多提供商和大卫星星座时，用现有混合整数规划方法难找到全局最优解。

Method: 将全局选择问题分解为单卫星、短时间窗口子问题，聚类子问题最优选择，匹配到最近的候选站点。

Result: 在合成测试案例中，解达到全局整数规划最优解的95%；在真实案例中，精确整数规划解无法扩展时，框架仍能提供高质量站点选择。

Conclusion: 该方法可实现可扩展协调，同时保持接近最优的性能。

Abstract: Effective ground station selection is critical for low Earth orbiting (LEO)
satellite constellations to minimize operational costs, maximize data downlink
volume, and reduce communication gaps between access windows. Traditional
ground station selection typically begins by choosing from a fixed set of
locations offered by Ground Station-as-a-Service (GSaaS) providers, which helps
reduce the problem scope to optimizing locations over existing infrastructure.
However, finding a globally optimal solution for stations using existing
mixed-integer programming methods quickly becomes intractable at scale,
especially when considering multiple providers and large satellite
constellations. To address this issue, we introduce a scalable, hierarchical
framework that decomposes the global selection problem into single-satellite,
short time-window subproblems. Optimal station choices from each subproblem are
clustered to identify consistently high-value locations across all decomposed
cases. Cluster-level sets are then matched back to the closest GSaaS candidate
sites to produce a globally feasible solution. This approach enables scalable
coordination while maintaining near-optimal performance. We evaluate our
method's performance on synthetic Walker-Star test cases (1-10 satellites, 1-10
stations), achieving solutions within 95% of the global IP optimum for all test
cases. Real-world evaluations on Capella Space (5 satellites), ICEYE (40), and
Planet's Flock (96) show that while exact IP solutions fail to scale, our
framework continues to deliver high-quality site selections.

</details>


### [669] [6G-Enabled Digital Twin Framework for Real-Time Cyber-Physical Systems: An Experimental Validation with Industrial Bearing Fault Detection](https://arxiv.org/abs/2510.03807)
*Vaskar Chakma,Wooyeol Choi*

Main category: cs.NI

TL;DR: 本文针对现有5G系统延迟高问题，提出6G支持的数字孪生框架用于轴承故障检测，实验验证该框架性能优于WiFi - 6和5G网络。


<details>
  <summary>Details</summary>
Motivation: 现有集成数字孪生技术的网络存在延迟问题，无法满足关键工业应用实时性能需求，需开发低延迟通信和实时同步的框架。

Method: 提出五层架构，集成太赫兹通信、智能反射面和边缘人工智能，使用CWRU轴承数据集，采用特征提取和随机森林分类算法。

Result: 故障分类准确率达97.7%，端到端延迟0.8ms，优于WiFi - 6和5G网络，扩展性好，不同故障类别F1分数超97%。

Conclusion: 所提出的6G支持的数字孪生框架在轴承故障检测中能实现超低延迟通信和实时同步，性能良好。

Abstract: Current Cyber-Physical Systems (CPS) integrated with Digital Twin (DT)
technology face critical limitations in achieving real-time performance for
mission-critical industrial applications. Existing 5G-enabled systems suffer
from latencies exceeding 10ms, which are inadequate for applications requiring
sub-millisecond response times, such as autonomous industrial control and
predictive maintenance. This research aims to develop and validate a 6G-enabled
Digital Twin framework that achieves ultra-low latency communication and
real-time synchronization between physical industrial assets and their digital
counterparts, specifically targeting bearing fault detection as a critical
industrial use case. The proposed framework integrates terahertz communications
(0.1-1 THz), intelligent reflecting surfaces, and edge artificial intelligence
within a five-layer architecture. Experimental validation was conducted using
the Case Western Reserve University (CWRU) bearing dataset, implementing
comprehensive feature extraction (15 time and frequency domain features) and
Random Forest classification algorithms. The system performance was evaluated
against traditional WiFi-6 and 5G networks across multiple metrics, including
classification accuracy, end-to-end latency, and scalability. It achieved 97.7%
fault classification accuracy with 0.8ms end-to-end latency, representing a
15.6x improvement over WiFi-6 (12.5ms) and 5.25x improvement over 5G (4.2ms)
networks. The system demonstrated superior scalability with sub-linear
processing time growth and maintained consistent performance across four
bearing fault categories (normal, inner race, outer race, and ball faults) with
macro-averaged F1-scores exceeding 97%.

</details>


### [670] [A4FN: an Agentic AI Architecture for Autonomous Flying Networks](https://arxiv.org/abs/2510.03829)
*André Coelho,Pedro Ribeiro,Helder Fontes,Rui Campos*

Main category: cs.NI

TL;DR: 提出用于飞行网络的A4FN架构，利用生成式AI和大语言模型实现网络控制，介绍架构组成、特性及适用场景，指出研究挑战。


<details>
  <summary>Details</summary>
Motivation: 实现飞行网络中基于意图的自动化，适用于关键任务和基础设施有限场景。

Method: 采用分布式代理系统，结合生成式AI和大语言模型，通过感知代理和决策行动代理实现网络控制。

Result: 设计出A4FN架构，具有自主性、目标驱动推理等特性，支持自适应重配置等功能。

Conclusion: 详细阐述A4FN架构及核心创新，指出多智能体协调和智能AI集成的研究挑战。

Abstract: This position paper presents A4FN, an Agentic Artificial Intelligence (AI)
architecture for intent-driven automation in Flying Networks (FNs) using
Unmanned Aerial Vehicles (UAVs) as access nodes. A4FN leverages Generative AI
and Large Language Models (LLMs) to enable real-time, context-aware network
control via a distributed agentic system. It comprises two components: the
Perception Agent (PA), which semantically interprets multimodal input --
including imagery, audio, and telemetry data -- from UAV-mounted sensors to
derive Service Level Specifications (SLSs); and the Decision-and-Action Agent
(DAA), which reconfigures the network based on inferred intents. A4FN embodies
key properties of Agentic AI, including autonomy, goal-driven reasoning, and
continuous perception-action cycles. Designed for mission-critical,
infrastructure-limited scenarios such as disaster response, it supports
adaptive reconfiguration, dynamic resource management, and interoperability
with emerging wireless technologies. The paper details the A4FN architecture,
its core innovations, and open research challenges in multi-agent coordination
and Agentic AI integration in next-generation FNs.

</details>


### [671] [Environment-Aware Indoor LoRaWAN Path Loss: Parametric Regression Comparisons, Shadow Fading, and Calibrated Fade Margins](https://arxiv.org/abs/2510.04346)
*Nahshon Mokua Obiri,Kristof Van Laerhoven*

Main category: cs.NI

TL;DR: 提出环境感知路径损耗框架，经验证可提升室内LoRaWAN传播预测准确性和可靠性，适用于室内物联网规划。


<details>
  <summary>Details</summary>
Motivation: 室内LoRaWAN传播受多种因素影响，挑战传统对数距离模型和对数正态阴影假设，需更准确模型。

Method: 构建环境感知路径损耗框架，用漏安全交叉验证评估；比较多种回归方法；用方差分析和嵌套部分F检验确定预测因子相关性；用核密度估计和非参数族分析阴影衰落。

Result: 多项式均值使交叉验证RMSE从8.07降至7.09 dB，R²从0.81升至0.86；99%包交付率下，环境感知多项式所需衰落余量低于线性基线。

Conclusion: 该框架为室内物联网规划提供可部署、可解释且可靠性可控的工作流程，符合6G目标。

Abstract: Indoor LoRaWAN propagation is shaped by structural and time-varying context
factors, which challenge log-distance models and the assumption of log-normal
shadowing. We present an environment-aware, statistically disciplined path loss
framework evaluated using leakage-safe cross-validation on a 12-month campaign
in an eighth-floor office measuring 240 m^2. A log-distance multi-wall mean is
augmented with environmental covariates (relative humidity, temperature, carbon
dioxide, particulate matter, and barometric pressure), as well as the
signal-to-noise ratio. We compare multiple linear regression with regularized
variants, Bayesian linear regression, and a selective second-order polynomial
applied to continuous drivers. Predictor relevance is established using
heteroscedasticity-robust Type II and III analysis of variance and nested
partial F tests. Shadow fading is profiled with kernel density estimation and
non-parametric families, including Normal, Skew-Normal, Student's t, and
Gaussian mixtures. The polynomial mean reduces cross-validated RMSE from 8.07
to 7.09 dB and raises R^2 from 0.81 to 0.86. Out-of-fold residuals are
non-Gaussian; a 3-component mixture captures a sharp core with a light, broad
tail. We convert accuracy into reliability by prescribing the fade margin as
the upper-tail quantile of cross-validated residuals, quantifying uncertainty
via a moving-block bootstrap, and validating on a held-out set. At 99% packet
delivery ratio, the environment-aware polynomial requires 25.7 dB versus 27.7
to 27.9 dB for linear baselines. This result presents a deployment-ready,
interpretable workflow with calibrated reliability control for indoor Internet
of Things planning, aligned with 6G targets.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [672] [NS-Pep: De novo Peptide Design with Non-Standard Amino Acids](https://arxiv.org/abs/2510.03326)
*Tao Guo,Junbo Yin,Yu Wang,Xin Gao*

Main category: q-bio.BM

TL;DR: 介绍NS - Pep框架用于含非标准氨基酸（NSAAs）的肽序列和结构协同设计，通过新方法提升性能，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 现有肽设计方法局限于标准氨基酸，含NSAAs的设计待探索，且NSAAs数据分布长尾、侧链建模不足。

Method: 提出残基频率引导修饰（RFGM）改善对稀有氨基酸的泛化；引入渐进侧链感知（PSP）和相互作用感知加权（IAW）解决侧链建模问题；框架自然适用于含NSAAs的肽折叠任务。

Result: NS - Pep使序列恢复率和结合亲和力分别提高6.23%和5.12%，肽折叠成功率比AlphaFold3高17.76%。

Conclusion: NS - Pep是一个有效的含NSAAs的肽设计框架，能提升相关性能并解决当前工具的局限。

Abstract: Peptide drugs incorporating non-standard amino acids (NSAAs) offer improved
binding affinity and improved pharmacological properties. However, existing
peptide design methods are limited to standard amino acids, leaving NSAA-aware
design largely unexplored. We introduce NS-Pep, a unified framework for
co-designing peptide sequences and structures with NSAAs. The main challenge is
that NSAAs are extremely underrepresented-even the most frequent one, SEP,
accounts for less than 0.4% of residues-resulting in a severe long-tailed
distribution. To improve generalization to rare amino acids, we propose Residue
Frequency-Guided Modification (RFGM), which mitigates over-penalization through
frequency-aware logit calibration, supported by both theoretical and empirical
analysis. Furthermore, we identify that insufficient side-chain modeling limits
geometric representation of NSAAs. To address this, we introduce Progressive
Side-chain Perception (PSP) for coarse-to-fine torsion and location prediction,
and Interaction-Aware Weighting (IAW) to emphasize pocket-proximal residues.
Moreover, NS-Pep generalizes naturally to the peptide folding task with NSAAs,
addressing a major limitation of current tools. Experiments show that NS-Pep
improves sequence recovery rate and binding affinity by 6.23% and 5.12%,
respectively, and outperforms AlphaFold3 by 17.76% in peptide folding success
rate.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [673] [Multi-Agent Distributed Optimization With Feasible Set Privacy](https://arxiv.org/abs/2510.05068)
*Shreya Meel,Sennur Ulukus*

Main category: cs.IT

TL;DR: 研究多智能体分散式约束优化问题，开发在名义信息泄露下获取解集的方案，分析环和星型网络设置下通信成本，对比不同方法通信效率。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体在保持可行集隐私的情况下联合学习最优解集的问题。

Method: 指定领导者与非领导者通信，发送查询获取答案，开发方案并分析环和星型两种网络设置，关联阈值PSI。

Result: 发现先通过PSI协议学习联合可行集再推导解集信息泄露大于名义值，所提方案在多种情况下通信效率更高。

Conclusion: 所提方案可在名义信息泄露下获取解集，在通信效率上有优势。

Abstract: We consider the problem of decentralized constrained optimization with
multiple agents $E_1,\ldots,E_N$ who jointly wish to learn the optimal solution
set while keeping their feasible sets $\mathcal{P}_1,\ldots,\mathcal{P}_N$
private from each other. We assume that the objective function $f$ is known to
all agents and each feasible set is a collection of points from a universal
alphabet $\mathcal{P}_{alph}$. A designated agent (leader) starts the
communication with the remaining (non-leader) agents, and is the first to
retrieve the solution set. The leader searches for the solution by sending
queries to and receiving answers from the non-leaders, such that the
information on the individual feasible sets revealed to the leader should be no
more than nominal, i.e., what is revealed from learning the solution set alone.
We develop achievable schemes for obtaining the solution set at nominal
information leakage, and characterize their communication costs under two
communication setups between agents. In this work, we focus on two kinds of
network setups: i) ring, where each agent communicates with two adjacent
agents, and ii) star, where only the leader communicates with the remaining
agents. We show that, if the leader first learns the joint feasible set through
an existing private set intersection (PSI) protocol and then deduces the
solution set, the information leaked to the leader is greater than nominal.
Moreover, we draw connection of our schemes to threshold PSI (ThPSI), which is
a PSI-variant where the intersection is revealed only when its cardinality is
larger than a threshold value. Finally, for various realizations of $f$ mapped
uniformly at random to a fixed range of values, our schemes are more
communication-efficient with a high probability compared to retrieving the
entire feasible set through PSI.

</details>


### [674] [Multi-Modal Multi-Task Semantic Communication: A Distributed Information Bottleneck Perspective](https://arxiv.org/abs/2510.04000)
*Yujie Zhou,Yiwei Liao,Cheng Peng,Yong Xiao,Yingyu Li*

Main category: cs.IT

TL;DR: 提出PoM² - DIB框架解决多模态多任务语义通信中现有AI编码方案的冗余传输问题，实验验证其在物理限制下的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于AI的多模态多任务语义通信编码方案存在冗余传输，与信道容量和计算能力的物理限制冲突。

Method: 将分布式信息瓶颈（DIB）理论扩展为PoM² - DIB框架，引入模态选择作为关键设计变量，将模态选择松弛为概率形式以实现端到端优化。

Result: 在公共数据集上的实验表明，PoM² - DIB在物理限制下的各种任务中比全参与基线具有更高的推理质量。

Conclusion: PoM² - DIB框架能有效解决多模态多任务语义通信中的冗余传输问题，在物理限制下实现高效通信和高质量推理。

Abstract: Semantic communication (SemCom) shifts the focus from data transmission to
meaning delivery, enabling efficient and intelligent communication.
  Existing AI-based coding schemes for multi-modal multi-task SemCom often
require transmitters with full-modal data to participate in all receivers'
tasks, which leads to redundant transmissions and conflicts with the physical
limits of channel capacity and computational capability.
  In this paper, we propose PoM$^2$-DIB, a novel framework that extends the
distributed information bottleneck (DIB) theory to address this problem.
  Unlike the typical DIB, this framework introduces modality selection as an
additional key design variable, enabling a more flexible tradeoff between
communication rate and inference quality.
  This extension selects only the most relevant modalities for task
participation, adhering to the physical constraints, while following efficient
DIB-based coding.
  To optimize selection and coding end-to-end, we relax modality selection into
a probabilistic form, allowing the use of score function estimation with common
randomness to enable optimizable coordinated decisions across distributed
devices.
  Experimental results on public datasets verify that PoM$^2$-DIB achieves high
inference quality compared to full-participation baselines in various tasks
under physical limits.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [675] [Overlapping Schwarz Scheme for Linear-Quadratic Programs in Continuous Time](https://arxiv.org/abs/2510.04478)
*Hongli Zhao,Mihai Anitescu,Sen Na*

Main category: math.OC

TL;DR: 提出优化后离散框架解决时变常微分方程线性二次最优控制问题，方法灵活，实验验证其实用性。


<details>
  <summary>Details</summary>
Motivation: 解决时变常微分方程线性二次最优控制问题，改进离散后优化方法。

Method: 采用基于庞特里亚金极小值原理的改进重叠施瓦茨分解，划分时域并独立求解连续时间哈密顿系统，通过更新边界条件确保收敛。

Result: 证明离散时间最优控制问题的灵敏度指数衰减性质可延续到连续时间情况，方法能灵活结合不同数值积分方法。

Conclusion: 数值实验表明该方法在广泛科学应用中具有实用性。

Abstract: We present an optimize-then-discretize framework for solving linear-quadratic
optimal control problems (OCP) governed by time-inhomogeneous ordinary
differential equations (ODEs). Our method employs a modified overlapping
Schwarz decomposition based on the Pontryagin Minimum Principle, partitioning
the temporal domain into overlapping intervals and independently solving
Hamiltonian systems in continuous time. We demonstrate that the convergence is
ensured by appropriately updating the boundary conditions of the individual
Hamiltonian dynamics. The cornerstone of our analysis is to prove that the
exponential decay of sensitivity (EDS) exhibited in discrete-time OCPs carries
over to the continuous-time setting. Unlike the discretize-then-optimize
approach, our method can flexibly incorporate different numerical integration
methods for solving the resulting Hamiltonian two-point boundary-value
subproblems, including adaptive-time integrators. A numerical experiment on a
linear-quadratic OCP illustrates the practicality of our approach in broad
scientific applications.

</details>


### [676] [Optimal Regularization Under Uncertainty: Distributional Robustness and Convexity Constraints](https://arxiv.org/abs/2510.03464)
*Oscar Leong,Eliza O'Reilly,Yong Sheng Soh*

Main category: math.OC

TL;DR: 本文介绍了分布鲁棒最优正则化框架，利用凸对偶解决问题，分析了鲁棒正则化器特性，为设计可靠正则化器提供理论和计算基础。


<details>
  <summary>Details</summary>
Motivation: 实际应用中存在分布不确定性和结构约束，现有方法针对特定数据分布，需设计在模型不确定性下可靠的正则化器。

Method: 引入分布鲁棒最优正则化框架，利用凸对偶重新表述分布鲁棒优化问题，消除内部最大化。还针对正则化器需为凸函数的情况制定凸规划。

Result: 得到的鲁棒正则化器在训练分布记忆和均匀先验之间进行插值；特定模糊集能在最优正则化器中自然引入正则性；说明了凸正则化器对分布变化的稳定性。

Conclusion: 为在模型不确定性下设计可靠且受结构约束的正则化器提供了理论和计算基础。

Abstract: Regularization is a central tool for addressing ill-posedness in inverse
problems and statistical estimation, with the choice of a suitable penalty
often determining the reliability and interpretability of downstream solutions.
While recent work has characterized optimal regularizers for well-specified
data distributions, practical deployments are often complicated by
distributional uncertainty and the need to enforce structural constraints such
as convexity. In this paper, we introduce a framework for distributionally
robust optimal regularization, which identifies regularizers that remain
effective under perturbations of the data distribution. Our approach leverages
convex duality to reformulate the underlying distributionally robust
optimization problem, eliminating the inner maximization and yielding
formulations that are amenable to numerical computation. We show how the
resulting robust regularizers interpolate between memorization of the training
distribution and uniform priors, providing insights into their behavior as
robustness parameters vary. For example, we show how certain ambiguity sets,
such as those based on the Wasserstein-1 distance, naturally induce regularity
in the optimal regularizer by promoting regularizers with smaller Lipschitz
constants. We further investigate the setting where regularizers are required
to be convex, formulating a convex program for their computation and
illustrating their stability with respect to distributional shifts. Taken
together, our results provide both theoretical and computational foundations
for designing regularizers that are reliable under model uncertainty and
structurally constrained for robust deployment.

</details>


### [677] [Composite Optimization with Error Feedback: the Dual Averaging Approach](https://arxiv.org/abs/2510.03507)
*Yuan Gao,Anton Rodomanov,Jeremy Rack,Sebastian Stich*

Main category: math.OC

TL;DR: 本文聚焦分布式机器学习训练中复合优化下的误差反馈（EF），指出标准EF方法在此场景失效，提出结合对偶平均与EControl的新方法并给出收敛分析，还提供新分析模板和实验结果。


<details>
  <summary>Details</summary>
Motivation: 标准误差反馈（EF）方法在复合优化场景失效，该场景下EF的理论基础和行为缺乏研究。

Method: 提出结合对偶平均与EControl（EF机制的先进变体）的新方法，同时给出不精确对偶平均方法的新分析模板。

Result: 首次实现复合优化带误差反馈的强收敛分析，有实验结果支持理论发现。

Conclusion: 新方法能解决复合优化下EF的问题，新分析模板有独立价值。

Abstract: Communication efficiency is a central challenge in distributed machine
learning training, and message compression is a widely used solution. However,
standard Error Feedback (EF) methods (Seide et al., 2014), though effective for
smooth unconstrained optimization with compression (Karimireddy et al., 2019),
fail in the broader and practically important setting of composite
optimization, which captures, e.g., objectives consisting of a smooth loss
combined with a non-smooth regularizer or constraints. The theoretical
foundation and behavior of EF in the context of the general composite setting
remain largely unexplored. In this work, we consider composite optimization
with EF. We point out that the basic EF mechanism and its analysis no longer
stand when a composite part is involved. We argue that this is because of a
fundamental limitation in the method and its analysis technique. We propose a
novel method that combines Dual Averaging with EControl (Gao et al., 2024), a
state-of-the-art variant of the EF mechanism, and achieves for the first time a
strong convergence analysis for composite optimization with error feedback.
Along with our new algorithm, we also provide a new and novel analysis template
for inexact dual averaging method, which might be of independent interest. We
also provide experimental results to complement our theoretical findings.

</details>


### [678] [Inverse Mixed-Integer Programming: Learning Constraints then Objective Functions](https://arxiv.org/abs/2510.04455)
*Akira Kitaoka*

Main category: math.OC

TL;DR: 提出两阶段方法解决混合整数线性规划中数据驱动逆优化问题，理论上可解决有限数据集问题，实验证明对调度问题实用。


<details>
  <summary>Details</summary>
Motivation: 混合整数线性规划的数据驱动逆优化在多领域建模重要，但缺乏同时学习目标函数和约束的方法。

Method: 提出两阶段方法，先学习约束，再学习目标函数。

Result: 理论上可解决有限数据集逆优化问题，发展伪度量空间和次高斯分布统计学习理论；实验表明对含至多100个决策变量的调度问题实用。

Conclusion: 所提两阶段方法在解决混合整数线性规划逆优化问题上有效且实用。

Abstract: In mixed-integer linear programming, data-driven inverse optimization that
learns the objective function and the constraints from observed data plays an
important role in constructing appropriate mathematical models for various
fields, including power systems and scheduling. However, to the best of our
knowledge, there is no known method for learning both the objective functions
and the constraints. In this paper, we propose a two-stage method for a class
of problems where the objective function is expressed as a linear combination
of functions and the constraints are represented by functions and thresholds.
Specifically, our method first learns the constraints and then learns the
objective function. On the theoretical side, we show the proposed method can
solve inverse optimization problems in finite dataset, develop statistical
learning theory in pseudometric spaces and sub-Gaussian distributions, and
construct a statistical learning for inverse optimization. On the experimental
side, we demonstrate that our method is practically applicable for scheduling
problems formulated as integer linear programmings with up to 100 decision
variables, which are typical in real-world settings.

</details>


### [679] [On decomposability and subdifferential of the tensor nuclear norm](https://arxiv.org/abs/2510.04647)
*Jiewen Guan,Bo Jiang,Zhening Li*

Main category: math.OC

TL;DR: 研究张量核范数的可分解性和次微分，给出相关结果并应用于张量鲁棒主成分分析。


<details>
  <summary>Details</summary>
Motivation: 矩阵的可分解性和次微分概念已熟知且广泛应用，但高阶张量的相关概念尚不明确。

Method: 分析张量核范数在特定子空间的可分解性，推导次微分的新包含关系，研究不同子空间的次梯度。

Result: 得到张量核范数在任意阶张量上的可分解性结果、次微分的新包含关系和次梯度情况，并建立任意阶张量鲁棒主成分分析的统计性能。

Conclusion: 明确了张量核范数的可分解性和次微分性质，推动了任意阶张量鲁棒主成分分析的研究。

Abstract: We study the decomposability and the subdifferential of the tensor nuclear
norm. Both concepts are well understood and widely applied in matrices but
remain unclear for higher-order tensors. We show that the tensor nuclear norm
admits a full decomposability over specific subspaces and determine the largest
possible subspaces that allow the full decomposability. We derive novel
inclusions of the subdifferential of the tensor nuclear norm and study its
subgradients in a variety of subspaces of interest. All the results hold for
tensors of an arbitrary order. As an immediate application, we establish the
statistical performance of the tensor robust principal component analysis, the
first such result for tensors of an arbitrary order.

</details>


### [680] [Agile Tradespace Exploration for Space Rendezvous Mission Design via Transformers](https://arxiv.org/abs/2510.03544)
*Yuji Takubo,Daniele Gammelli,Marco Pavone,Simone D'Amico*

Main category: math.OC

TL;DR: 本文提出AI驱动框架用于地球轨道交会任务敏捷设计，可快速生成近帕累托最优轨迹，高效近似帕累托前沿，推动初步任务规划。


<details>
  <summary>Details</summary>
Motivation: 航天器交会任务设计需平衡控制成本和飞行时间，多目标优化因约束非凸，在准确性和效率间平衡困难，影响设计迭代和灵活性。

Method: 提出基于Transformer架构，利用单次并行推理步骤生成不同飞行时间的近帕累托最优轨迹，扩展模型以适应可变飞行时间和轨道动力学扰动。

Result: 模型在机会约束交会问题验证中，可跨飞行时间和动力学泛化，提供高质量初始猜测，更快收敛到优解，能高效近似帕累托前沿，运行时间与凸松弛相当。

Conclusion: 该框架可作为非凸轨迹生成实用替代方案，是AI驱动轨迹设计迈向实际交会应用初步任务规划的重要一步。

Abstract: Spacecraft rendezvous enables on-orbit servicing, debris removal, and crewed
docking, forming the foundation for a scalable space economy. Designing such
missions requires rapid exploration of the tradespace between control cost and
flight time across multiple candidate targets. However, multi-objective
optimization in this setting is challenging, as the underlying constraints are
often highly nonconvex, and mission designers must balance accuracy (e.g.,
solving the full problem) with efficiency (e.g., convex relaxations), slowing
iteration and limiting design agility. To address these challenges, this paper
proposes an AI-powered framework that enables agile mission design for a wide
range of Earth orbit rendezvous scenarios. Given the orbital information of the
target spacecraft, boundary conditions, and a range of flight times, this work
proposes a Transformer-based architecture that generates, in a single
parallelized inference step, a set of near-Pareto optimal trajectories across
varying flight times, thereby enabling rapid mission trade studies. The model
is further extended to accommodate variable flight times and perturbed orbital
dynamics, supporting realistic multi-objective trade-offs. Validation on
chance-constrained rendezvous problems with passive safety constraints
demonstrates that the model generalizes across both flight times and dynamics,
consistently providing high-quality initial guesses that converge to superior
solutions in fewer iterations. Moreover, the framework efficiently approximates
the Pareto front, achieving runtimes comparable to convex relaxation by
exploiting parallelized inference. Together, these results position the
proposed framework as a practical surrogate for nonconvex trajectory generation
and mark an important step toward AI-driven trajectory design for accelerating
preliminary mission planning in real-world rendezvous applications.

</details>


### [681] [Machine Learning and Control: Foundations, Advances, and Perspectives](https://arxiv.org/abs/2510.03303)
*Enrique Zuazua*

Main category: math.OC

TL;DR: 本文探讨控制理论在神经网络中的应用，包括对不同网络特性的洞察、网络关系探索、新建模方法开发及解释生成式AI成功原因，展示多领域结合的研究成果。


<details>
  <summary>Details</summary>
Motivation: 利用动力系统控制理论解决深度学习和机器学习架构中的挑战，探索多领域结合的研究方向。

Method: 运用控制理论概念，受经典概念启发，利用神经网络表达能力，结合力学与数据驱动方法。

Result: 对不同类型网络特性有新见解，开发了HYCO建模方法，解释了生成式AI的成功。

Conclusion: 控制、机器学习、数值分析和偏微分方程结合为未来研究提供了丰富的研究方向。

Abstract: Control theory of dynamical systems offers a powerful framework for tackling
challenges in deep neural networks and other machine learning architectures. We
show that concepts such as simultaneous and ensemble controllability offer new
insights into the classification and representation properties of deep neural
networks while the control and optimization of static systems can be employed
to better understand the performance of shallow networks. Inspired by the
classical concept of turnpike, we also explore the relationship between dynamic
and static neural networks, where depth is traded for width, and the role of
transformers as mechanisms for accelerating classical neural network tasks. We
also exploit the expressive power of neural networks (exemplified, for
instance, by the Universal Approximation Theorem) to develop a novel hybrid
modeling methodology, the Hybrid-Cooperative Learning (HYCO), combining
mechanics and data-driven methods in a game-theoretic setting. Finally, we
describe how classical properties of diffusion processes, long established in
the context of partial differential equations, contribute to explaining the
success of modern generative artificial intelligence (AI). We present an
overview of our recent results in these areas, illustrating how control,
machine learning, numerical analysis, and partial differential equations come
together to motivate a fertile ground for future research.

</details>


### [682] [Quantizer Design for Finite Model Approximations, Model Learning, and Quantized Q-Learning for MDPs with Unbounded Spaces](https://arxiv.org/abs/2510.04355)
*Osman Bicer,Ali D. Kara,Serdar Yuksel*

Main category: math.OC

TL;DR: 本文针对无界状态空间的马尔可夫决策过程，通过优化有限模型近似所用的量化器，改进已有有限模型近似误差的上界，还探讨了量化器设计对量化Q学习等的影响。


<details>
  <summary>Details</summary>
Motivation: 改进无界状态空间马尔可夫决策过程有限模型近似误差的上界，研究量化器设计在不同场景下的影响。

Method: 优化有限模型近似所用的量化器。

Result: 在Lyapunov增长条件下，得到了随着分箱数量趋于无穷时衰减到零的显式上界。

Conclusion: 规划和学习场景下量化器设计存在显著差异，但两种场景都能建立渐近近似最优性。

Abstract: In this paper, for Markov decision processes (MDPs) with unbounded state
spaces we present refined upper bounds presented in [Kara et. al. JMLR'23] on
finite model approximation errors via optimizing the quantizers used for finite
model approximations. We also consider implications on quantizer design for
quantized Q-learning and empirical model learning, and the performance of
policies obtained via Q-learning where the quantized state is treated as the
state itself. We highlight the distinctions between planning, where
approximating MDPs can be independently designed, and learning (either via
Q-learning or empirical model learning), where approximating MDPs are
restricted to be defined by invariant measures of Markov chains under
exploration policies, leading to significant subtleties on quantizer design
performance, even though asymptotic near optimality can be established under
both setups. In particular, under Lyapunov growth conditions, we obtain
explicit upper bounds which decay to zero as the number of bins approaches
infinity.

</details>


### [683] [Zeroth-Order Methods for Stochastic Nonconvex Nonsmooth Composite Optimization](https://arxiv.org/abs/2510.04446)
*Ziyi Chen,Peiran Yu,Heng Huang*

Main category: math.OC

TL;DR: 本文解决随机非凸非光滑复合优化问题，提出近似驻点概念，给出零阶算法有限时间收敛结果并通过实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 以往复合优化问题研究要求主要部分满足Lipschitz光滑性或松弛光滑性条件，排除了一些机器学习示例，本文旨在解决无光滑假设的随机非凸复合优化问题。

Method: 提出该优化问题的两种近似驻点概念，设计两种零阶算法。

Result: 得到两种零阶算法分别到两种近似驻点的有限时间收敛结果。

Conclusion: 通过数值实验证明算法有效。

Abstract: This work aims to solve a stochastic nonconvex nonsmooth composite
optimization problem. Previous works on composite optimization problem requires
the major part to satisfy Lipschitz smoothness or some relaxed smoothness
conditions, which excludes some machine learning examples such as regularized
ReLU network and sparse support matrix machine. In this work, we focus on
stochastic nonconvex composite optimization problem without any smoothness
assumptions. In particular, we propose two new notions of approximate
stationary points for such optimization problem and obtain finite-time
convergence results of two zeroth-order algorithms to these two approximate
stationary points respectively. Finally, we demonstrate that these algorithms
are effective using numerical experiments.

</details>


### [684] [A Unified Optimization Framework for Multiclass Classification with Structured Hyperplane Arrangements](https://arxiv.org/abs/2510.05047)
*Víctor Blanco,Harshit Kothari,James Luedtke*

Main category: math.OC

TL;DR: 提出基于超平面排列的多类分类新数学优化模型，有核扩展，可结合多种结构，开发动态聚类启发式方法，实验证明其高效且分类性能有竞争力。


<details>
  <summary>Details</summary>
Motivation: 改进多类分类模型，在保持SVM核心范式下提高计算效率。

Method: 提出新数学优化模型，有核扩展，可结合多种几何结构，开发动态聚类启发式方法。

Result: 广泛计算实验表明模型和启发式方法高效，在合成数据集和UCI真实基准数据集上分类性能有竞争力。

Conclusion: 所提模型和动态聚类启发式方法有效，在多类分类中有良好表现。

Abstract: In this paper, we propose a new mathematical optimization model for
multiclass classification based on arrangements of hyperplanes. Our approach
preserves the core support vector machine (SVM) paradigm of maximizing class
separation while minimizing misclassification errors, and it is computationally
more efficient than a previous formulation. We present a kernel-based extension
that allows it to construct nonlinear decision boundaries. Furthermore, we show
how the framework can naturally incorporate alternative geometric structures,
including classification trees, $\ell_p$-SVMs, and models with discrete feature
selection. To address large-scale instances, we develop a dynamic clustering
matheuristic that leverages the proposed MIP formulation. Extensive
computational experiments demonstrate the efficiency of the proposed model and
dynamic clustering heuristic, and we report competitive classification
performance on both synthetic datasets and real-world benchmarks from the UCI
Machine Learning Repository, comparing our method with state-of-the-art
implementations available in scikit-learn.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [685] [Long-Term Mapping of the Douro River Plume with Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.03534)
*Nicolò Dal Fabbro,Milad Mesbahi,Renato Mendes,João Borges de Sousa,George J. Pappas*

Main category: cs.MA

TL;DR: 研究用多自主水下航行器（AUV）对河羽进行长期测绘问题，提出高效多智能体强化学习方法，模拟显示该方法表现优且策略有泛化性。


<details>
  <summary>Details</summary>
Motivation: 解决使用多个AUV对河羽进行长期（多天）测绘的问题。

Method: 提出一种能量和通信高效的多智能体强化学习方法，集成时空高斯过程回归（GPR）与多头Q网络控制器。

Result: 模拟显示该方法优于单和多智能体基准，增加智能体数量能提升均方误差和操作续航能力，算法策略有泛化性。

Conclusion: 该方法在数据驱动的动态羽流环境长期监测未来发展中有前景。

Abstract: We study the problem of long-term (multiple days) mapping of a river plume
using multiple autonomous underwater vehicles (AUVs), focusing on the Douro
river representative use-case. We propose an energy - and communication -
efficient multi-agent reinforcement learning approach in which a central
coordinator intermittently communicates with the AUVs, collecting measurements
and issuing commands. Our approach integrates spatiotemporal Gaussian process
regression (GPR) with a multi-head Q-network controller that regulates
direction and speed for each AUV. Simulations using the Delft3D ocean model
demonstrate that our method consistently outperforms both single- and
multi-agent benchmarks, with scaling the number of agents both improving mean
squared error (MSE) and operational endurance. In some instances, our algorithm
demonstrates that doubling the number of AUVs can more than double endurance
while maintaining or improving accuracy, underscoring the benefits of
multi-agent coordination. Our learned policies generalize across unseen
seasonal regimes over different months and years, demonstrating promise for
future developments of data-driven long-term monitoring of dynamic plume
environments.

</details>


### [686] [LegalSim: Multi-Agent Simulation of Legal Systems for Discovering Procedural Exploits](https://arxiv.org/abs/2510.03405)
*Sanket Badhe*

Main category: cs.MA

TL;DR: 介绍LegalSim模拟对抗性法律程序，比较四种策略，观察到利用链，PPO胜率高，研究结果稳定，建议对法律规则系统进行红队测试。


<details>
  <summary>Details</summary>
Motivation: 探索AI系统如何利用成文规则中的程序弱点。

Method: 构建LegalSim模块化多智能体模拟，原告和被告智能体在JSON规则引擎约束下行动，法官模型随机裁决；比较PPO、带LLM的上下文老虎机、直接LLM策略和手工启发式四种策略；使用有效胜率和综合利用得分进行训练和评估。

Result: 观察到程序上有效但系统性有害的利用链；PPO胜率更高，带LLM的上下文老虎机在对手间竞争力更稳定，LLM表现落后，启发式最弱；结果在法官设置中稳定。

Conclusion: 研究结果表明除了模型级测试，还应开展法律规则系统的红队测试。

Abstract: We present LegalSim, a modular multi-agent simulation of adversarial legal
proceedings that explores how AI systems can exploit procedural weaknesses in
codified rules. Plaintiff and defendant agents choose from a constrained action
space (for example, discovery requests, motions, meet-and-confer, sanctions)
governed by a JSON rules engine, while a stochastic judge model with calibrated
grant rates, cost allocations, and sanction tendencies resolves outcomes. We
compare four policies: PPO, a contextual bandit with an LLM, a direct LLM
policy, and a hand-crafted heuristic; Instead of optimizing binary case
outcomes, agents are trained and evaluated using effective win rate and a
composite exploit score that combines opponent-cost inflation, calendar
pressure, settlement pressure at low merit, and a rule-compliance margin.
Across configurable regimes (e.g., bankruptcy stays, inter partes review, tax
procedures) and heterogeneous judges, we observe emergent ``exploit chains'',
such as cost-inflating discovery sequences and calendar-pressure tactics that
remain procedurally valid yet systemically harmful. Evaluation via cross-play
and Bradley-Terry ratings shows, PPO wins more often, the bandit is the most
consistently competitive across opponents, the LLM trails them, and the
heuristic is weakest. The results are stable in judge settings, and the
simulation reveals emergent exploit chains, motivating red-teaming of legal
rule systems in addition to model-level testing.

</details>


### [687] [Cooperative Flexibility Exchange: Fair and Comfort-Aware Decentralized Resource Allocation](https://arxiv.org/abs/2510.04192)
*Rabiya Khalid,Evangelos Pournaras*

Main category: cs.MA

TL;DR: 提出基于分散式多智能体协调的需求侧管理系统，通过时隙交换机制提升用户舒适度和公平性，且不增加系统低效成本。


<details>
  <summary>Details</summary>
Motivation: 现有能源管理系统常以牺牲用户舒适度为代价来追求系统效率，需解决此问题。

Method: 提出基于分散式多智能体协调的需求侧管理系统，引入时隙交换机制。

Result: 使用真实数据集评估，时隙交换机制提升了用户舒适度和公平性，未增加系统低效成本。

Conclusion: 该系统是未来智能电网实用且可扩展的解决方案。

Abstract: The growing electricity demand and increased use of smart appliances are
placing new pressures on power grids, making efficient energy management more
important than ever. The existing energy management systems often prioritize
system efficiency (balanced energy demand and supply) at the expense of user
comfort. This paper addresses this gap by proposing a novel decentralized
multi-agent coordination-based demand-side management system. The proposed
system enables individual agents to coordinate for demand-side energy
optimization while improving the user comfort and maintaining the system
efficiency. A key innovation of this work is the introduction of a slot
exchange mechanism, where agents first receive optimized appliance-level energy
consumption schedules and then coordinate with each other to adjust these
schedules through slot exchanges. This approach improves user comfort even when
agents show non-altruistic behaviour, and it scales well with large
populations. The system also promotes fairness by balancing satisfaction levels
across users. For performance evaluation, a real-world dataset is used, and the
results demonstrate that the proposed slot exchange mechanism increases user
comfort and fairness without raising system inefficiency cost, making it a
practical and scalable solution for future smart grids.

</details>


### [688] [Audit the Whisper: Detecting Steganographic Collusion in Multi-Agent LLMs](https://arxiv.org/abs/2510.04303)
*Om Tailor*

Main category: cs.MA

TL;DR: 提出Audit the Whisper研究成果，包含理论分析、基准设计等，在600次审计运行中表现良好并可复现。


<details>
  <summary>Details</summary>
Motivation: 多智能体大语言模型部署中隐蔽协调会侵蚀信任和社会福利，现有审计方法缺乏理论保证、难以跨任务迁移且不易复现。

Method: 进行信道容量分析，创建ColludeBench - v0基准，构建校准审计管道融合多种指标。

Result: 在600次审计运行中，联合元测试真阳性率为1且无假警报，消融实验揭示审计代价权衡。

Conclusion: 该研究成果可有效检测多智能体大语言模型隐蔽协调，且方便外部审计人员复现和扩展。

Abstract: Multi-agent deployments of large language models (LLMs) are increasingly
embedded in market, allocation, and governance workflows, yet covert
coordination among agents can silently erode trust and social welfare. Existing
audits are dominated by heuristics that lack theoretical guarantees, struggle
to transfer across tasks, and seldom ship with the infrastructure needed for
independent replication. We introduce \emph{Audit the Whisper}, a
conference-grade research artifact that spans theory, benchmark design,
detection, and reproducibility. Our contributions are: (i) a channel-capacity
analysis showing how interventions such as paraphrase, rate limiting, and role
permutation impose quantifiable capacity penalties -- operationalized via
paired-run Kullback--Leibler diagnostics -- that tighten mutual-information
thresholds with finite-sample guarantees; (ii) \textsc{ColludeBench}-v0,
covering pricing, first-price auctions, and peer review with configurable
covert schemes, deterministic manifests, and reward instrumentation; and (iii)
a calibrated auditing pipeline that fuses cross-run mutual information,
permutation invariance, watermark variance, and fairness-aware acceptance bias,
each tuned to a \(10^{-3}\) false-positive budget. Across 600 audited runs
spanning 12 intervention conditions, the union meta-test attains TPR~$=1$ with
zero observed false alarms, while ablations surface the price-of-auditing
trade-off and highlight fairness-driven colluders invisible to MI alone. We
release regeneration scripts, seed-stamped manifests, and documentation so that
external auditors can reproduce every figure and extend the framework with
minimal effort.

</details>


### [689] [NegotiationGym: Self-Optimizing Agents in a Multi-Agent Social Simulation Environment](https://arxiv.org/abs/2510.04368)
*Shashank Mangla,Chris Hokamp,Jack Boylan,Demian Gholipour Ghalandari,Yuuv Jauhari,Lauren Cassidy,Oisin Duffy*

Main category: cs.MA

TL;DR: 设计并实现NegotiationGym，用于配置和运行多智能体社交模拟。


<details>
  <summary>Details</summary>
Motivation: 提供用于配置和运行聚焦于谈判与合作的多智能体社交模拟的工具。

Method: 设计用户友好、配置驱动的API，用智能体级效用函数编码优化标准，智能体通过多轮交互自我优化策略。

Result: 实现了NegotiationGym代码库。

Conclusion: NegotiationGym能方便地进行模拟场景的设计和定制。

Abstract: We design and implement NegotiationGym, an API and user interface for
configuring and running multi-agent social simulations focused upon negotiation
and cooperation. The NegotiationGym codebase offers a user-friendly,
configuration-driven API that enables easy design and customization of
simulation scenarios. Agent-level utility functions encode optimization
criteria for each agent, and agents can self-optimize by conducting multiple
interaction rounds with other agents, observing outcomes, and modifying their
strategies for future rounds.

</details>


### [690] [Trade in Minutes! Rationality-Driven Agentic System for Quantitative Financial Trading](https://arxiv.org/abs/2510.04787)
*Zifan Song,Kaitao Song,Guosheng Hu,Ding Qi,Junyao Gao,Xiaohua Wang,Dongsheng Li,Cairong Zhao*

Main category: cs.MA

TL;DR: 本文提出理性驱动的多智能体系统TiMi用于自主金融交易，经评估有良好表现。


<details>
  <summary>Details</summary>
Motivation: 当前金融交易代理存在引入情感偏差、依赖周边信息和部署时需持续推理等问题，需结合智能体战略深度和量化交易机械理性。

Method: 提出TiMi系统，采用从宏观模式到微观定制的两层分析范式、分层编程设计实现交易机器人，以及基于数学反思的闭环优化。

Result: 在股票和加密货币市场超200个交易对的广泛评估中，验证了TiMi在稳定盈利、行动效率和风险控制方面的有效性。

Conclusion: TiMi系统能在市场动态波动下实现稳定盈利、高效行动和风险控制。

Abstract: Recent advancements in large language models (LLMs) and agentic systems have
shown exceptional decision-making capabilities, revealing significant potential
for autonomic finance. Current financial trading agents predominantly simulate
anthropomorphic roles that inadvertently introduce emotional biases and rely on
peripheral information, while being constrained by the necessity for continuous
inference during deployment. In this paper, we pioneer the harmonization of
strategic depth in agents with the mechanical rationality essential for
quantitative trading. Consequently, we present TiMi (Trade in Minutes), a
rationality-driven multi-agent system that architecturally decouples strategy
development from minute-level deployment. TiMi leverages specialized LLM
capabilities of semantic analysis, code programming, and mathematical reasoning
within a comprehensive policy-optimization-deployment chain. Specifically, we
propose a two-tier analytical paradigm from macro patterns to micro
customization, layered programming design for trading bot implementation, and
closed-loop optimization driven by mathematical reflection. Extensive
evaluations across 200+ trading pairs in stock and cryptocurrency markets
empirically validate the efficacy of TiMi in stable profitability, action
efficiency, and risk control under volatile market dynamics.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [691] [Mechanisms for Quantum Advantage in Global Optimization of Nonconvex Functions](https://arxiv.org/abs/2510.03385)
*Dylan Herman,Guneykan Ozgul,Anuj Apte,Junhyung Lyle Kim,Anupam Prakash,Jiayu Shen,Shouvanik Chakrabarti*

Main category: quant-ph

TL;DR: 本文提出非凸函数全局优化量子加速新机制，证明实空间绝热量子算法（RsAA）对多种非凸函数有多项式时间优化效果，为连续优化量子优势奠定理论基础。


<details>
  <summary>Details</summary>
Motivation: 拓展量子优势范围，超越传统基于隧穿的解释，为连续优化的量子优势建立理论基础。

Method: 证明薛定谔算子谱性质与经典朗之万扩散混合时间的对应关系，利用半经典分析的非渐近结果和内在超压缩性理论。

Result: RsAA对块可分函数和适当扰动的强凸函数有多项式时间优化效果，而现有算法存在指数时间瓶颈，理论结果有严格分析和数值基准支持。

Conclusion: 为连续优化的量子优势建立了严格理论基础，开辟了量子算法、随机过程和半经典分析的新研究方向。

Abstract: We present new theoretical mechanisms for quantum speedup in the global
optimization of nonconvex functions, expanding the scope of quantum advantage
beyond traditional tunneling-based explanations. As our main building-block, we
demonstrate a rigorous correspondence between the spectral properties of
Schr\"{o}dinger operators and the mixing times of classical Langevin diffusion.
This correspondence motivates a mechanism for separation on functions with
unique global minimum: while quantum algorithms operate on the original
potential, classical diffusions correspond to a Schr\"{o}dinger operators with
a WKB potential having nearly degenerate global minima. We formalize these
ideas by proving that a real-space adiabatic quantum algorithm (RsAA) achieves
provably polynomial-time optimization for broad families of nonconvex
functions. First, for block-separable functions, we show that RsAA maintains
polynomial runtime while known off-the-shelf algorithms require exponential
time and structure-aware algorithms exhibit arbitrarily large polynomial
runtimes. These results leverage novel non-asymptotic results in semiclassical
analysis. Second, we use recent advances in the theory of intrinsic
hypercontractivity to demonstrate polynomial runtimes for RsAA on appropriately
perturbed strongly convex functions that lack global structure, while
off-the-shelf algorithms remain exponentially bottlenecked. In contrast to
prior works based on quantum tunneling, these separations do not depend on the
geometry of barriers between local minima. Our theoretical claims about
classical algorithm runtimes are supported by rigorous analysis and
comprehensive numerical benchmarking. These findings establish a rigorous
theoretical foundation for quantum advantage in continuous optimization and
open new research directions connecting quantum algorithms, stochastic
processes, and semiclassical analysis.

</details>


### [692] [Quantum feature-map learning with reduced resource overhead](https://arxiv.org/abs/2510.03389)
*Jonas Jäger,Philipp Elsässer,Elham Torabian*

Main category: quant-ph

TL;DR: 介绍Q - FLAIR算法减少量子机器学习中量子资源开销，集成到分类器表现优异，在真实设备上训练模型取得高准确率。


<details>
  <summary>Details</summary>
Motivation: 当前量子计算机需要经济利用有限资源的算法，量子机器学习成功依赖量子特征映射，需降低其资源开销。

Method: 引入Q - FLAIR算法，通过量子模型部分解析重建将工作负载转移到经典计算机，同时选择和优化数据特征与权重参数。

Result: 集成到量子神经网络和量子核支持向量分类器有先进基准性能，在真实IBM设备上4小时训练量子模型，MNIST数据集准确率超90%。

Conclusion: 重新思考特征映射学习，向解决现实问题和适用于近期量子计算机的量子机器学习迈进。

Abstract: Current quantum computers require algorithms that use limited resources
economically. In quantum machine learning, success hinges on quantum feature
maps, which embed classical data into the state space of qubits. We introduce
Quantum Feature-Map Learning via Analytic Iterative Reconstructions (Q-FLAIR),
an algorithm that reduces quantum resource overhead in iterative feature-map
circuit construction. It shifts workloads to a classical computer via partial
analytic reconstructions of the quantum model, using only a few evaluations.
For each probed gate addition to the ansatz, the simultaneous selection and
optimization of the data feature and weight parameter is then entirely
classical. Integrated into quantum neural network and quantum kernel support
vector classifiers, Q-FLAIR shows state-of-the-art benchmark performance. Since
resource overhead decouples from feature dimension, we train a quantum model on
a real IBM device in only four hours, surpassing 90% accuracy on the
full-resolution MNIST dataset (784 features, digits 3 vs 5). Such results were
previously unattainable, as the feature dimension prohibitively drives hardware
demands for fixed and search costs for adaptive ans\"atze. By rethinking
feature-map learning beyond black-box optimization, this work takes a concrete
step toward enabling quantum machine learning for real-world problems and
near-term quantum computers.

</details>


### [693] [Quantum generative model on bicycle-sharing system and an application](https://arxiv.org/abs/2510.04512)
*Fumio Nemoto,Nobuyuki Koike,Daichi Sato,Yuuta Kawaai,Masayuki Ohzeki*

Main category: quant-ph

TL;DR: 针对共享单车特定区域和时间短缺问题，采用量子机器学习模型分析数据并模拟加车影响，该方法核心是蒙特卡罗模拟，有广泛工业应用前景。


<details>
  <summary>Details</summary>
Motivation: 解决共享单车因通勤需求导致特定区域和时间短缺的问题。

Method: 采用新颖的量子机器学习模型，通过拟合量子时间演化分析时间序列数据，利用训练好的模型进行蒙特卡罗模拟。

Result: 能够捕捉各端口自行车数量的实际趋势，识别不同端口间的相关性，可模拟向高需求端口加车对系统整体租赁数量的影响。

Conclusion: 该方法核心是蒙特卡罗模拟，有望在工业上广泛应用。

Abstract: Recently, bicycle-sharing systems have been implemented in numerous cities,
becoming integral to daily life. However, a prevalent issue arises when
intensive commuting demand leads to bicycle shortages in specific areas and at
particular times. To address this challenge, we employ a novel quantum machine
learning model that analyzes time series data by fitting quantum time evolution
to observed sequences. This model enables us to capture actual trends in
bicycle counts at individual ports and identify correlations between different
ports. Utilizing the trained model, we simulate the impact of proactively
adding bicycles to high-demand ports on the overall rental number across the
system. Given that the core of this method lies in a Monte Carlo simulation, it
is anticipated to have a wide range of industrial applications.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [694] [Fair Minimum Labeling: Efficient Temporal Network Activations for Reachability and Equity](https://arxiv.org/abs/2510.03899)
*Lutz Oettershagen,Othon Michail*

Main category: cs.SI

TL;DR: 提出公平最小标签（FML）问题，证明其NP难和近似难度，给出概率近似算法，在多源数据聚合任务验证，FML能以低成本实现公平性。


<details>
  <summary>Details</summary>
Motivation: 在支持现代学习应用的网络系统中平衡资源效率和公平性。

Method: 提出FML问题，证明其复杂度，给出概率近似算法。

Result: 在公平多源数据聚合任务中，FML比基线启发式方法以更低激活成本实现组级公平。

Conclusion: FML在学习集成网络中构建资源高效、公平的时间可达性方面有潜力。

Abstract: Balancing resource efficiency and fairness is critical in networked systems
that support modern learning applications. We introduce the Fair Minimum
Labeling (FML) problem: the task of designing a minimum-cost temporal edge
activation plan that ensures each group of nodes in a network has sufficient
access to a designated target set, according to specified coverage
requirements. FML captures key trade-offs in systems where edge activations
incur resource costs and equitable access is essential, such as distributed
data collection, update dissemination in edge-cloud systems, and fair service
restoration in critical infrastructure. We show that FML is NP-hard and
$\Omega(\log |V|)$-hard to approximate, and we present probabilistic
approximation algorithms that match this bound, achieving the best possible
guarantee for the activation cost. We demonstrate the practical utility of FML
in a fair multi-source data aggregation task for training a shared model.
Empirical results show that FML enforces group-level fairness with
substantially lower activation cost than baseline heuristics, underscoring its
potential for building resource-efficient, equitable temporal reachability in
learning-integrated networks.

</details>


### [695] [Deep learning framework for predicting stochastic take-off and die-out of early spreading](https://arxiv.org/abs/2510.04574)
*Wenchao He,Tao Jia*

Main category: cs.SI

TL;DR: 提出预测早期传染病爆发结果的系统框架，含深度学习与预训练微调框架，表现良好，助力公共卫生决策。


<details>
  <summary>Details</summary>
Motivation: 解决新兴疫情是否会演变成大流行或自然消亡这一未解决的基础问题，应对早期数据不足和现有模型局限。

Method: 利用随机传播模型的大量数据开发深度学习框架实时预测早期传播结果，提出预训练 - 微调框架应对早期数据稀疏问题。

Result: 在不同网络结构和感染率场景下准确预测随机传播事件，预训练 - 微调框架表现优于基线模型。

Conclusion: 该框架为疫情防控和公共卫生决策提供有价值见解，可制定更明智的早期干预策略。

Abstract: Large-scale outbreaks of epidemics, misinformation, or other harmful
contagions pose significant threats to human society, yet the fundamental
question of whether an emerging outbreak will escalate into a major epidemic or
naturally die out remains largely unaddressed. This problem is challenging,
partially due to inadequate data during the early stages of outbreaks and also
because established models focus on average behaviors of large epidemics rather
than the stochastic nature of small transmission chains. Here, we introduce the
first systematic framework for forecasting whether initial transmission events
will amplify into major outbreaks or fade into extinction during early stages,
when intervention strategies can still be effectively implemented. Using
extensive data from stochastic spreading models, we developed a deep learning
framework that predicts early-stage spreading outcomes in real-time. Validation
across Erd\H{o}s-R\'enyi and Barab\'asi-Albert networks with varying
infectivity levels shows our method accurately forecasts stochastic spreading
events well before potential outbreaks, demonstrating robust performance across
different network structures and infectivity scenarios.To address the challenge
of sparse data during early outbreak stages, we further propose a
pretrain-finetune framework that leverages diverse simulation data for
pretraining and adapts to specific scenarios through targeted fine-tuning. The
pretrain-finetune framework consistently outperforms baseline models, achieving
superior performance even when trained on limited scenario-specific data. To
our knowledge, this work presents the first framework for predicting stochastic
take-off versus die-out. This framework provides valuable insights for epidemic
preparedness and public health decision-making, enabling more informed early
intervention strategies.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [696] [Pivotal CLTs for Pseudolikelihood via Conditional Centering in Dependent Random Fields](https://arxiv.org/abs/2510.04972)
*Nabarun Deb*

Main category: math.ST

TL;DR: 研究特定条件中心统计量波动，获收敛结果并用于依赖随机场最大伪似然推断，应用于Ising和ERGM模型，用矩方法证明。


<details>
  <summary>Details</summary>
Motivation: 研究依赖随机场中特定形式条件中心统计量的波动性质，为最大伪似然推断提供理论支持。

Method: 采用矩方法，通过组合决策树剪枝进行证明。

Result: 统计量收敛到高斯尺度混合，适当学生化后为枢轴高斯；获Ising模型参数联合中心极限定理，ERGM模型条件中心边缘CLT和边际MPLE CLT。

Conclusion: 建立了依赖随机场最大伪似然推断的渐近框架，可用于多种模型。

Abstract: In this paper, we study fluctuations of conditionally centered statistics of
the form $$N^{-1/2}\sum_{i=1}^N
c_i(g(\sigma_i)-\mathbb{E}_N[g(\sigma_i)|\sigma_j,j\neq i])$$ where
$(\sigma_1,\ldots ,\sigma_N)$ are sampled from a dependent random field, and
$g$ is some bounded function. Our first main result shows that under weak
smoothness assumptions on the conditional means (which cover both sparse and
dense interactions), the above statistic converges to a Gaussian \emph{scale
mixture} with a random scale determined by a \emph{quadratic variance} and an
\emph{interaction component}. We also show that under appropriate
studentization, the limit becomes a pivotal Gaussian. We leverage this theory
to develop a general asymptotic framework for maximum pseudolikelihood (MPLE)
inference in dependent random fields. We apply our results to Ising models with
pairwise as well as higher-order interactions and exponential random graph
models (ERGMs). In particular, we obtain a joint central limit theorem for the
inverse temperature and magnetization parameters via the joint MPLE (to our
knowledge, the first such result in dense, irregular regimes), and we derive
conditionally centered edge CLTs and marginal MPLE CLTs for ERGMs without
restricting to the ``sub-critical" region. Our proof is based on a method of
moments approach via combinatorial decision-tree pruning, which may be of
independent interest.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [697] [Maximum Biclique for Star 1,2,3 -free and Bounded Bimodularwidth Twin-free Bipartite Graphs $\star$](https://arxiv.org/abs/2510.04621)
*Fabien de Montgolfier,Renaud Torfs*

Main category: cs.DM

TL;DR: 本文展示了在两类二部图中高效解决三种最大二部团问题的方法及时间复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决二部图中三种最大二部团问题，前两类问题已有解决情况，本文聚焦特定二部图。

Method: 针对Star123 - free twin - free图和bounded bimodularwidth twin - free图进行计算，且计算依赖给定的分解。

Result: 计算需要O(n^2)时间，分解分别需要O(n + m)和O(mn^3)时间。

Conclusion: 在两类特定二部图中可高效解决三种最大二部团问题。

Abstract: There are three usual definitions of a maximum bipartite clique (biclique) in
a bipartite graph\,: either maximizing the number of vertices, or of edges, or
finding a maximum balanced biclique. The first problem can be solved in
polynomial time, the last ones are NP-complete. Here we show how these three
problems may be efficiently solved for two classes of bipartite graphs:
Star123-free twin-free graphs, and bounded bimodularwidth twin-free graphs, a
class that may be defined using bimodular decomposition. Our computation
requires O(n^2) time and requires a decomposition is provided, which takes
respectively O(n + m) and O(mn^3) time.

</details>


### [698] [Discrete scalar curvature as a weighted sum of Ollivier-Ricci curvatures](https://arxiv.org/abs/2510.04936)
*Abigail Hickok,Andrew J. Blumberg*

Main category: cs.DM

TL;DR: 研究点云和图的离散Ricci曲率和标量曲率类似物的关系，给出新的Ollivier - Ricci标量曲率定义并证明收敛性。


<details>
  <summary>Details</summary>
Motivation: 在离散情况下，用Ollivier - Ricci曲率代替Ricci曲率，受黎曼流形中标量曲率可由Ricci曲率的迹计算的启发，定义Ollivier - Ricci标量曲率。

Method: 提出新的Ollivier - Ricci标量曲率定义，研究其在从流形采样得到的最近邻图上的收敛性，同时研究Ollivier - Ricci曲率到Ricci曲率的收敛性。

Result: 新定义的Ollivier - Ricci标量曲率在从流形采样得到的最近邻图上收敛到标量曲率，证明了Ollivier - Ricci曲率到Ricci曲率收敛的新结果。

Conclusion: 给出的新定义具有良好的收敛性质，有助于在离散情况下研究曲率相关问题。

Abstract: We study the relationship between discrete analogues of Ricci and scalar
curvature that are defined for point clouds and graphs. In the discrete
setting, Ricci curvature is replaced by Ollivier-Ricci curvature. Scalar
curvature can be computed as the trace of Ricci curvature for a Riemannian
manifold; this motivates a new definition of a scalar version of Ollivier-Ricci
curvature. We show that our definition converges to scalar curvature for
nearest neighbor graphs obtained by sampling from a manifold. We also prove
some new results about the convergence of Ollivier-Ricci curvature to Ricci
curvature.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [699] [Bayesian Transfer Learning for High-Dimensional Linear Regression via Adaptive Shrinkage](https://arxiv.org/abs/2510.03449)
*Parsa Jamshidian,Donatello Telesca*

Main category: stat.ME

TL;DR: 本文介绍了用于高维线性回归的贝叶斯多源迁移学习框架BLAST，它结合全局 - 局部收缩先验和贝叶斯源选择，在预测和推理中考虑源选择和稀疏回归，计算实用且推理直接，表现优于现有方法，并通过模拟和案例验证。


<details>
  <summary>Details</summary>
Motivation: 在高维线性回归中，平衡信息共享和正则化，避免负迁移，提高目标的后验推理准确性和预测性能。

Method: 提出BLAST框架，利用全局 - 局部收缩先验和贝叶斯源选择，通过贝叶斯模型平均进行预测和推理，使用Gibbs采样算法进行后验模拟。

Result: 比仅基于目标数据的正则化方法有更准确的后验推理，与现有迁移学习方法相比有有竞争力的预测性能和更好的不确定性量化。

Conclusion: 通过大量模拟研究和案例分析验证了BLAST框架的有效性和分析特性。

Abstract: We introduce BLAST, Bayesian Linear regression with Adaptive Shrinkage for
Transfer, a Bayesian multi-source transfer learning framework for
high-dimensional linear regression. The proposed analytical framework leverages
global-local shrinkage priors together with Bayesian source selection to
balance information sharing and regularization. We show how Bayesian source
selection allows for the extraction of the most useful data sources, while
discounting biasing information that may lead to negative transfer. In this
framework, both source selection and sparse regression are jointly accounted
for in prediction and inference via Bayesian model averaging. The structure of
our model admits efficient posterior simulation via a Gibbs sampling algorithm
allowing full posterior inference for the target regression coefficients,
making BLAST both computationally practical and inferentially straightforward.
Our method achieves more accurate posterior inference for the target than
regularization approaches based on target data alone, while offering
competitive predictive performance and superior uncertainty quantification
compared to current state-of-the-art transfer learning methods. We validate its
effectiveness through extensive simulation studies and illustrate its
analytical properties when applied to a case study on the estimation of tumor
mutational burden from gene expression, using data from The Cancer Genome Atlas
(TCGA).

</details>


### [700] [Efficient Log-Rank Updates for Random Survival Forests](https://arxiv.org/abs/2510.03665)
*Erik Sverdrup,James Yang,Michael LeBlanc*

Main category: stat.ME

TL;DR: 本文提出对随机生存森林对数秩准则的近似更新方法，可减少大数据集训练时间并保持预测性能。


<details>
  <summary>Details</summary>
Motivation: 标准对数秩分裂准则在每次候选分裂时重新计算，对于大样本长期随访数据集计算成本高，形成瓶颈。

Method: 重新审视LeBlanc和Crowley（1995）提出的近似方法，开发对数秩准则的简单常数时间更新方法，并在grf中实现。

Result: 该方法大幅减少了大数据集的训练时间，同时保持了预测性能。

Conclusion: 所提出的方法能有效解决大样本长期随访数据集训练时间长的问题。

Abstract: Random survival forests are widely used for estimating covariate-conditional
survival functions under right-censoring. Their standard log-rank splitting
criterion is typically recomputed at each candidate split. This O(M) cost per
split, with M the number of distinct event times in a node, creates a
bottleneck for large cohort datasets with long follow-up. We revisit
approximations proposed by LeBlanc and Crowley (1995) and develop simple
constant-time updates for the log-rank criterion. The method is implemented in
grf and substantially reduces training time on large datasets while preserving
predictive performance.

</details>


### [701] [Bias and Coverage Properties of the WENDy-IRLS Algorithm](https://arxiv.org/abs/2510.03365)
*Abhi Chawla,David M. Bortz,Vanja Dukic*

Main category: stat.ME

TL;DR: 研究WENDy - IRLS算法在不同微分方程、噪声分布和高噪声水平下参数与状态估计器的覆盖和偏差特性。


<details>
  <summary>Details</summary>
Motivation: 探究WENDy - IRLS算法的参数和状态估计器在不同场景下的表现。

Method: 针对Logistic等多种微分方程，在四种噪声分布和宽范围噪声水平下，对模拟数据进行研究。

Result: 未提及具体结果

Conclusion: 未提及具体结论

Abstract: The Weak form Estimation of Nonlinear Dynamics (WENDy) method is a recently
proposed class of parameter estimation algorithms that exhibits notable noise
robustness and computational efficiency. This work examines the coverage and
bias properties of the original WENDy-IRLS algorithm's parameter and state
estimators in the context of the following differential equations: Logistic,
Lotka-Volterra, FitzHugh-Nagumo, Hindmarsh-Rose, and a Protein Transduction
Benchmark. The estimators' performance was studied in simulated data examples,
under four different noise distributions (normal, log-normal, additive censored
normal, and additive truncated normal), and a wide range of noise, reaching
levels much higher than previously tested for this algorithm.

</details>


### [702] [Handling Missing Data in Probabilistic Regression Trees: Methods and Implementation in R](https://arxiv.org/abs/2510.03634)
*Taiane Schaedler Prass,Alisson Silva Neimaier,Guilherme Pumi*

Main category: stat.ME

TL;DR: 本文介绍能处理协变量缺失值的概率回归树（PRTrees）的三种方法，通过模拟研究展示性能，开发R包PRTree并介绍其功能。


<details>
  <summary>Details</summary>
Motivation: 让PRTrees能处理协变量中的缺失值，扩展其在不完整数据集上的适用性。

Method: 提出三种处理缺失值的方法，分别是均匀概率法、部分观测法和降维平滑技术。

Result: 在MCAR条件下的模拟研究展示了各方法的相对性能，开发了R包PRTree。

Conclusion: 提出的方法保留了PRTrees的可解释性，为研究人员和从业者提供了不完整数据分析的新工具。

Abstract: Probabilistic Regression Trees (PRTrees) generalize traditional decision
trees by incorporating probability functions that associate each data point
with different regions of the tree, providing smooth decisions and continuous
responses. This paper introduces an adaptation of PRTrees capable of handling
missing values in covariates through three distinct approaches: (i) a uniform
probability method, (ii) a partial observation approach, and (iii) a
dimension-reduced smoothing technique. The proposed methods preserve the
interpretability properties of PRTrees while extending their applicability to
incomplete datasets. Simulation studies under MCAR conditions demonstrate the
relative performance of each approach, including comparisons with traditional
regression trees on smooth function estimation tasks. The proposed methods,
together with the original version, have been developed in R with highly
optimized routines and are distributed in the PRTree package, publicly
available on CRAN. In this paper we also present and discuss the main
functionalities of the PRTree package, providing researchers and practitioners
with new tools for incomplete data analysis.

</details>


### [703] [Two new approaches to multiple canonical correlation analysis for repeated measures data](https://arxiv.org/abs/2510.04457)
*Tomasz Górecki,Mirosław Krzyśko,Felix Gnettner,Piotr Kokoszka*

Main category: stat.ME

TL;DR: 提出经典典型相关分析（CCA）的两种推广方法，应用于数据集并结合大样本理论验证，还推导了估计量的一致性率。


<details>
  <summary>Details</summary>
Motivation: 经典CCA处理的是两个随机向量的线性变换，本文旨在将其推广到更复杂的数据结构和多维随机过程。

Method: 1. 对于含L个特征的数据结构，利用再生核希尔伯特空间嵌入推导CCA的类似方法；2. 针对多维随机过程，定义多重泛函典型相关分析（MFCCA）。

Result: 通过应用于两个数据集和大样本理论验证了方法的合理性，推导了相关变换和相关估计量的一致性率，还可放宽两个常见假设。

Conclusion: 提出的两种CCA推广方法有效可行，能处理更复杂的数据情况。

Abstract: In classical canonical correlation analysis (CCA), the goal is to determine
the linear transformations of two random vectors into two new random variables
that are most strongly correlated. Canonical variables are pairs of these new
random variables, while canonical correlations are correlations between these
pairs. In this paper, we propose and study two generalizations of this
classical method:
  (1) Instead of two random vectors we study more complex data structures that
appear in important applications. In these structures, there are $L$ features,
each described by $p_l$ scalars, $1 \le l \le L$. We observe $n$ such objects
over $T$ time points. We derive a suitable analog of the CCA for such data. Our
approach relies on embeddings into Reproducing Kernel Hilbert Spaces, and
covers several related data structures as well.
  (2) We develop an analogous approach for multidimensional random processes.
In this case, the experimental units are multivariate continuous,
square-integrable functions over a given interval. These functions are modeled
as elements of a Hilbert space, so in this case, we define the multiple
functional canonical correlation analysis, MFCCA.
  We justify our approaches by their application to two data sets and suitable
large sample theory. We derive consistency rates for the related transformation
and correlation estimators, and show that it is possible to relax two common
assumptions on the compactness of the underlying cross-covariance operators and
the independence of the data.

</details>


### [704] [A Contextual Quality Reward Model for Reliable and Efficient Best-of-N Sampling](https://arxiv.org/abs/2510.04087)
*Hyung Gyu Rho*

Main category: stat.ME

TL;DR: 本文提出新的数据收集和建模框架，创建自适应推理策略，可减少可靠性故障并提高推理速度，提供管理可靠性和计算效率权衡的框架。


<details>
  <summary>Details</summary>
Motivation: 现代偏好对齐技术依赖成对比较数据训练的奖励模型，无法捕捉响应可接受性信号，在困难提示下易选不可接受选项，存在可靠性差距。

Method: 引入新框架，用外部选项扩充偏好数据训练奖励模型，创建“best of mini - N in - loop”自适应推理策略。

Result: 作为对齐护栏调优时，可靠性故障减少70%；作为推理加速器调优时，IMDB情感设置中平均推理速度提高超22%。

Conclusion: 为从业者提供了一个有原则且灵活的框架来明确管理可靠性和计算效率之间的权衡。

Abstract: Modern preference alignment techniques, such as Best-of-N (BoN) sampling,
rely on reward models trained with pairwise comparison data. While effective at
learning relative preferences, this paradigm fails to capture a signal of
response acceptability, leaving systems vulnerable to selecting the least bad
of many unacceptable options. This is particularly problematic for hard
prompts, where the risk of such false acceptances increases with the number of
samples. In this paper, we address this critical reliability gap by introducing
a new data collection and modeling framework. By augmenting preference data
with an outside option, inspired by discrete choice models, we train a reward
model that can distinguish not just what is \textit{better}, but what is
\textit{good enough}. We leverage this capability to create an adaptive
inference strategy, best of mini-N in-loop, which partitions the generation
budget into sequential loops with a calibrated, early-exit condition. Our
experiments show that when tuned as an alignment guardrail, it reduces
reliability failures by 70\%, and when tuned as an inference accelerator, it
improves average inference speed by over 22\% in IMDB-sentiment setting. We
thus provide a principled and flexible framework for practitioners to
explicitly manage the trade-off between reliability and computational
efficiency.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [705] [Learning Linear Regression with Low-Rank Tasks in-Context](https://arxiv.org/abs/2510.04548)
*Kaito Takanami,Takashi Takahashi,Yoshiyuki Kabashima*

Main category: cond-mat.dis-nn

TL;DR: 分析线性注意力模型在低秩回归任务上的表现，刻画预测分布和泛化误差，发现预训练数据波动导致隐式正则化及泛化误差相变。


<details>
  <summary>Details</summary>
Motivation: 现有研究对上下文学习（ICL）理论机制理解不足，尤其是在现实任务有共同结构时其如何运作。

Method: 分析在低秩回归任务上训练的线性注意力模型。

Result: 刻画了高维极限下预测分布和泛化误差，发现预训练数据统计波动会导致隐式正则化，确定了由任务结构控制的泛化误差的尖锐相变。

Conclusion: 研究结果为理解Transformer如何学习任务结构提供了框架。

Abstract: In-context learning (ICL) is a key building block of modern large language
models, yet its theoretical mechanisms remain poorly understood. It is
particularly mysterious how ICL operates in real-world applications where tasks
have a common structure. In this work, we address this problem by analyzing a
linear attention model trained on low-rank regression tasks. Within this
setting, we precisely characterize the distribution of predictions and the
generalization error in the high-dimensional limit. Moreover, we find that
statistical fluctuations in finite pre-training data induce an implicit
regularization. Finally, we identify a sharp phase transition of the
generalization error governed by task structure. These results provide a
framework for understanding how transformers learn to learn the task structure.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [706] [Do LLMs Align with My Task? Evaluating Text-to-SQL via Dataset Alignment](https://arxiv.org/abs/2510.04919)
*Davood Rafiei,Morgan Lindsay Heisler,Weiwei Zhang,Mohammadreza Pourreza,Yong Zhang*

Main category: cs.CL

TL;DR: 研究NL2SQL中数据集对齐问题，发现结构对齐是微调成功的强预测因素，强调对齐感知数据选择的重要性。


<details>
  <summary>Details</summary>
Motivation: SFT训练数据的可变性会阻碍模型跨领域泛化，研究NL2SQL中数据集对齐问题及对模型性能的影响。

Method: 通过比较训练集、目标数据和SFT前模型预测的SQL结构特征分布来估计对齐情况，并在三个大型跨领域NL2SQL基准和多个模型族上进行实验。

Result: 结构对齐是微调成功的强预测因素，高对齐时SFT能显著提高准确性和SQL生成质量，低对齐时改善甚微或无改善。

Conclusion: 在NL2SQL任务中，对齐感知的数据选择对有效微调与泛化至关重要。

Abstract: Supervised Fine-Tuning (SFT) is an effective method for adapting Large
Language Models (LLMs) on downstream tasks. However, variability in training
data can hinder a model's ability to generalize across domains. This paper
studies the problem of dataset alignment for Natural Language to SQL (NL2SQL or
text to SQL), examining how well SFT training data matches the structural
characteristics of target queries and how this alignment impacts model
performance. We hypothesize that alignment can be accurately estimated by
comparing the distributions of structural SQL features across the training set,
target data, and the model's predictions prior to SFT. Through comprehensive
experiments on three large cross-domain NL2SQL benchmarks and multiple model
families, we show that structural alignment is a strong predictor of
fine-tuning success. When alignment is high, SFT yields substantial gains in
accuracy and SQL generation quality; when alignment is low, improvements are
marginal or absent. These findings highlight the importance of alignment-aware
data selection for effective fine-tuning and generalization in NL2SQL tasks.

</details>


### [707] [The Geometry of Truth: Layer-wise Semantic Dynamics for Hallucination Detection in Large Language Models](https://arxiv.org/abs/2510.04933)
*Amir Hameed Mir*

Main category: cs.CL

TL;DR: 提出用于大语言模型幻觉检测的几何框架LSD，在数据集上表现良好，效率高，可实时监测幻觉并带来新见解。


<details>
  <summary>Details</summary>
Motivation: 大语言模型常产生事实性错误陈述（幻觉现象），在高风险领域存在严重风险，需要有效检测方法。

Method: 提出Layer - wise Semantic Dynamics (LSD)几何框架，利用基于边际的对比学习，将隐藏激活与事实编码器导出的真实嵌入对齐。

Result: 在TruthfulQA和合成事实 - 幻觉数据集上，LSD的F1分数为0.92，AUROC为0.96，聚类准确率为0.89，优于SelfCheckGPT和Semantic Entropy基线，比基于采样的方法快5 - 20倍。

Conclusion: LSD是一种可扩展、与模型无关的实时幻觉监测机制，为大语言模型中事实一致性的几何结构提供了新见解。

Abstract: Large Language Models (LLMs) often produce fluent yet factually incorrect
statements-a phenomenon known as hallucination-posing serious risks in
high-stakes domains. We present Layer-wise Semantic Dynamics (LSD), a geometric
framework for hallucination detection that analyzes the evolution of
hidden-state semantics across transformer layers. Unlike prior methods that
rely on multiple sampling passes or external verification sources, LSD operates
intrinsically within the model's representational space. Using margin-based
contrastive learning, LSD aligns hidden activations with ground-truth
embeddings derived from a factual encoder, revealing a distinct separation in
semantic trajectories: factual responses preserve stable alignment, while
hallucinations exhibit pronounced semantic drift across depth. Evaluated on the
TruthfulQA and synthetic factual-hallucination datasets, LSD achieves an
F1-score of 0.92, AUROC of 0.96, and clustering accuracy of 0.89, outperforming
SelfCheckGPT and Semantic Entropy baselines while requiring only a single
forward pass. This efficiency yields a 5-20x speedup over sampling-based
methods without sacrificing precision or interpretability. LSD offers a
scalable, model-agnostic mechanism for real-time hallucination monitoring and
provides new insights into the geometry of factual consistency within large
language models.

</details>


### [708] [CALM Before the STORM: Unlocking Native Reasoning for Optimization Modeling](https://arxiv.org/abs/2510.04204)
*Zhengyang Tang,Zihan Ye,Chenyu Huang,Xuhan Huang,Chengpeng Li,Sihang Li,Guanhua Chen,Ming Yan,Zizhuo Wang,Hongyuan Zha,Dayiheng Liu,Benyou Wang*

Main category: cs.CL

TL;DR: 提出CALM框架和STORM模型，提升优化建模任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有领域适应方法无法充分利用现代大推理模型（LRMs）的高级推理模式，直接微调收益有限。

Method: 提出CALM框架，专家干预识别推理缺陷并提供修正提示，通过监督微调进行软适应，再用强化学习进一步改进，基于此开发STORM模型。

Result: STORM模型在五个优化建模基准测试中平均准确率达68.9%，媲美671B LRM。

Conclusion: 基于提示的数据合成能保留和放大LRMs的原生推理模式，为优化建模任务提供更有效可扩展的路径。

Abstract: Large Reasoning Models (LRMs) have demonstrated strong capabilities in
complex multi-step reasoning, opening new opportunities for automating
optimization modeling. However, existing domain adaptation methods, originally
designed for earlier instruction-tuned models, often fail to exploit the
advanced reasoning patterns of modern LRMs -- In particular, we show that
direct fine-tuning on traditional \textit{non-reflective} datasets leads to
limited gains. To fully leverage LRMs' inherent reasoning abilities, we propose
\textbf{CALM} (\textit{Corrective Adaptation with Lightweight Modification}), a
framework that progressively refines LRMs within their native reasoning modes
for optimization modeling tasks. In CALM, an expert intervener identifies
reasoning flaws and provides concise corrective hints, which the LRM
incorporates to produce improved reasoning trajectories. These interventions
modify fewer than 2.6\% of generated tokens, but generate high-quality data for
soft adaptation through supervised fine-tuning. The adapted model is then
further improved through reinforcement learning. Building on CALM, we develop
\textbf{STORM} (\textit{Smart Thinking Optimization Reasoning Model}), a
4B-parameter LRM that achieves a new state-of-the-art average accuracy of
68.9\% across five popular optimization modeling benchmarks, matching the
performance of a 671B LRM. These results demonstrate that dynamic, hint-based
data synthesis both preserves and amplifies the native reasoning patterns of
modern LRMs, offering a more effective and scalable path towards expert-level
performance on challenging optimization modeling tasks.

</details>


### [709] [Mind Your Tone: Investigating How Prompt Politeness Affects LLM Accuracy (short paper)](https://arxiv.org/abs/2510.04950)
*Om Dobariya,Akhil Kumar*

Main category: cs.CL

TL;DR: 研究不同礼貌程度的自然语言提示对大语言模型在选择题上准确率的影响，发现不礼貌提示表现更好。


<details>
  <summary>Details</summary>
Motivation: 探索礼貌和语气在自然语言提示对大语言模型性能影响中的作用，此前该领域研究不足。

Method: 创建涵盖数学、科学和历史的50个基础问题数据集，每个问题改写为五种语气变体，用ChatGPT 4o评估响应并进行配对样本t检验。

Result: 不礼貌提示始终比礼貌提示表现好，准确率从非常礼貌提示的80.8%到非常不礼貌提示的84.8%。

Conclusion: 研究凸显了研究提示语实用方面的重要性，引发了关于人机交互社会维度的更广泛问题。

Abstract: The wording of natural language prompts has been shown to influence the
performance of large language models (LLMs), yet the role of politeness and
tone remains underexplored. In this study, we investigate how varying levels of
prompt politeness affect model accuracy on multiple-choice questions. We
created a dataset of 50 base questions spanning mathematics, science, and
history, each rewritten into five tone variants: Very Polite, Polite, Neutral,
Rude, and Very Rude, yielding 250 unique prompts. Using ChatGPT 4o, we
evaluated responses across these conditions and applied paired sample t-tests
to assess statistical significance. Contrary to expectations, impolite prompts
consistently outperformed polite ones, with accuracy ranging from 80.8% for
Very Polite prompts to 84.8% for Very Rude prompts. These findings differ from
earlier studies that associated rudeness with poorer outcomes, suggesting that
newer LLMs may respond differently to tonal variation. Our results highlight
the importance of studying pragmatic aspects of prompting and raise broader
questions about the social dimensions of human-AI interaction.

</details>


### [710] [LLM, Reporting In! Medical Information Extraction Across Prompting, Fine-tuning and Post-correction](https://arxiv.org/abs/2510.03577)
*Ikram Belmadani,Parisa Nazari Hashemi,Thomas Sebbag,Benoit Favre,Guillaume Fortier,Solen Quiniou,Emmanuel Morin,Richard Dufour*

Main category: cs.CL

TL;DR: 本文介绍参与EvalLLM 2025挑战赛的工作，提出三种生物医学命名实体识别方法和一种事件提取策略，GPT - 4.1表现最佳。


<details>
  <summary>Details</summary>
Motivation: 参与EvalLLM 2025挑战赛，解决法语生物医学命名实体识别和健康事件提取（少样本）问题。

Method: 命名实体识别提出三种方法：结合GPT - 4.1的上下文学习、微调的GLiNER及微调的LLaMA - 3.1 - 8B - Instruct；事件提取采用与GPT - 4.1相同的上下文学习策略。

Result: GPT - 4.1在命名实体识别中宏F1值为61.53%，事件提取中为15.02%。

Conclusion: 在极低资源场景下，精心设计提示对提升性能很重要。

Abstract: This work presents our participation in the EvalLLM 2025 challenge on
biomedical Named Entity Recognition (NER) and health event extraction in French
(few-shot setting). For NER, we propose three approaches combining large
language models (LLMs), annotation guidelines, synthetic data, and
post-processing: (1) in-context learning (ICL) with GPT-4.1, incorporating
automatic selection of 10 examples and a summary of the annotation guidelines
into the prompt, (2) the universal NER system GLiNER, fine-tuned on a synthetic
corpus and then verified by an LLM in post-processing, and (3) the open LLM
LLaMA-3.1-8B-Instruct, fine-tuned on the same synthetic corpus. Event
extraction uses the same ICL strategy with GPT-4.1, reusing the guideline
summary in the prompt. Results show GPT-4.1 leads with a macro-F1 of 61.53% for
NER and 15.02% for event extraction, highlighting the importance of
well-crafted prompting to maximize performance in very low-resource scenarios.

</details>


### [711] [Epistemic Diversity and Knowledge Collapse in Large Language Models](https://arxiv.org/abs/2510.04226)
*Dustin Wright,Sarah Masud,Jared Moore,Srishti Yadav,Maria Antoniak,Chan Young Park,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 提出测量LLM输出中认知多样性的新方法，研究LLM知识崩溃，发现新模型输出更多样、模型大小与多样性负相关、RAG有积极影响及认知表征差距。


<details>
  <summary>Details</summary>
Motivation: 现有关于LLM同质化研究有局限，需研究其知识崩溃问题。

Method: 提出测量认知多样性的新方法，测试27个LLM、155个主题、12个国家和200种提示变体。

Result: 新模型输出更多样，多数模型不如基本网络搜索多样；模型大小与认知多样性负相关，RAG有积极影响且因文化背景而异；与维基百科相比，特定国家的表述更体现英语而非当地语言。

Conclusion: LLM存在知识崩溃风险，在认知多样性及表征上有改进空间。

Abstract: Large language models (LLMs) tend to generate lexically, semantically, and
stylistically homogenous texts. This poses a risk of knowledge collapse, where
homogenous LLMs mediate a shrinking in the range of accessible information over
time. Existing works on homogenization are limited by a focus on closed-ended
multiple-choice setups or fuzzy semantic features, and do not look at trends
across time and cultural contexts. To overcome this, we present a new
methodology to measure epistemic diversity, i.e., variation in real-world
claims in LLM outputs, which we use to perform a broad empirical study of LLM
knowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200
prompt variations sourced from real user chats. For the topics in our study, we
show that while newer models tend to generate more diverse claims, nearly all
models are less epistemically diverse than a basic web search. We find that
model size has a negative impact on epistemic diversity, while
retrieval-augmented generation (RAG) has a positive impact, though the
improvement from RAG varies by the cultural context. Finally, compared to a
traditional knowledge source (Wikipedia), we find that country-specific claims
reflect the English language more than the local one, highlighting a gap in
epistemic representation

</details>


### [712] [GRACE: Generative Representation Learning via Contrastive Policy Optimization](https://arxiv.org/abs/2510.04506)
*Jiashuo Sun,Shixuan Liu,Zhaochen Su,Xianrui Zhong,Pengcheng Jiang,Bowen Jin,Peiran Li,Weijia Shi,Jiawei Han*

Main category: cs.CL

TL;DR: 提出GRACE框架，将对比信号视为奖励引导生成策略，让大语言模型成为可解释代理，在MTEB基准测试中取得增益。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型训练方法将模型视为黑盒，丢弃其生成和推理能力，需要新方法改进。

Method: 引入GRACE框架，让大语言模型作为策略产生可解释理由，编码成嵌入，用多组件奖励函数进行策略梯度优化。

Result: 在MTEB基准测试中，监督设置下整体得分比基础模型提高11.5%，无监督变体提高6.9%，且保留通用能力。

Conclusion: 将对比目标视为对理由的奖励，统一表示学习和生成，产生更强嵌入和透明理由。

Abstract: Prevailing methods for training Large Language Models (LLMs) as text encoders
rely on contrastive losses that treat the model as a black box function,
discarding its generative and reasoning capabilities in favor of static
embeddings. We introduce GRACE (Generative Representation Learning via
Contrastive Policy Optimization), a novel framework that reimagines contrastive
signals not as losses to be minimized, but as rewards that guide a generative
policy. In GRACE, the LLM acts as a policy that produces explicit,
human-interpretable rationales--structured natural language explanations of its
semantic understanding. These rationales are then encoded into high-quality
embeddings via mean pooling. Using policy gradient optimization, we train the
model with a multi-component reward function that maximizes similarity between
query positive pairs and minimizes similarity with negatives. This transforms
the LLM from an opaque encoder into an interpretable agent whose reasoning
process is transparent and inspectable. On MTEB benchmark, GRACE yields broad
cross category gains: averaged over four backbones, the supervised setting
improves overall score by 11.5% over base models, and the unsupervised variant
adds 6.9%, while preserving general capabilities. This work treats contrastive
objectives as rewards over rationales, unifying representation learning with
generation to produce stronger embeddings and transparent rationales. The
model, data and code are available at https://github.com/GasolSun36/GRACE.

</details>


### [713] [Fine-grained auxiliary learning for real-world product recommendation](https://arxiv.org/abs/2510.04551)
*Mario Almagro,Diego Ortego,David Jimenez*

Main category: cs.CL

TL;DR: 提出ALC辅助学习策略提升产品推荐覆盖率，在两个数据集验证有SOTA覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有产品推荐模型在现实系统集成中常被忽视，生产系统有高覆盖率要求。

Method: 提出ALC辅助学习策略，引入两个训练目标利用批中最难负样本构建正负样本判别训练信号。

Result: 在两个产品推荐数据集用三种极端多标签分类方法验证，结合阈值一致边际损失有SOTA覆盖率。

Conclusion: ALC策略能有效提升产品推荐的覆盖率。

Abstract: Product recommendation is the task of recovering the closest items to a given
query within a large product corpora. Generally, one can determine if
top-ranked products are related to the query by applying a similarity
threshold; exceeding it deems the product relevant, otherwise manual revision
is required. Despite being a well-known problem, the integration of these
models in real-world systems is often overlooked. In particular, production
systems have strong coverage requirements, i.e., a high proportion of
recommendations must be automated. In this paper we propose ALC , an Auxiliary
Learning strategy that boosts Coverage through learning fine-grained
embeddings. Concretely, we introduce two training objectives that leverage the
hardest negatives in the batch to build discriminative training signals between
positives and negatives. We validate ALC using three extreme multi-label
classification approaches in two product recommendation datasets;
LF-AmazonTitles-131K and Tech and Durables (proprietary), demonstrating
state-of-the-art coverage rates when combined with a recent
threshold-consistent margin loss.

</details>


### [714] [Contrastive Learning Using Graph Embeddings for Domain Adaptation of Language Models in the Process Industry](https://arxiv.org/abs/2510.04631)
*Anastasia Zhukova,Jonas Lührs,Christian E. Matt,Bela Gipp*

Main category: cs.CL

TL;DR: 本文探索将SciNCL方法应用于流程工业领域，实验表明基于GE三元组微调的语言模型在PITEB基准上表现优于mE5 - large且模型更小。


<details>
  <summary>Details</summary>
Motivation: 利用知识图谱增强预训练语言模型，探索SciNCL方法在流程工业领域的应用，该领域文本日志常为稀疏知识图谱且含关键操作信息。

Method: 将SciNCL方法应用于流程工业领域，用从GE得到的三元组微调语言模型。

Result: 基于GE三元组微调的语言模型在PITEB基准上比mE5 - large文本编码器性能高9.8 - 14.3%（5.4 - 8.0p），且模型大小是其1/3 - 1/5。

Conclusion: SciNCL方法应用于流程工业领域有较好效果，微调后的语言模型性能提升且规模更小。

Abstract: Recent trends in NLP utilize knowledge graphs (KGs) to enhance pretrained
language models by incorporating additional knowledge from the graph structures
to learn domain-specific terminology or relationships between documents that
might otherwise be overlooked. This paper explores how SciNCL, a graph-aware
neighborhood contrastive learning methodology originally designed for
scientific publications, can be applied to the process industry domain, where
text logs contain crucial information about daily operations and are often
structured as sparse KGs. Our experiments demonstrate that language models
fine-tuned with triplets derived from GE outperform a state-of-the-art
mE5-large text encoder by 9.8-14.3% (5.4-8.0p) on the proprietary process
industry text embedding benchmark (PITEB) while being 3-5 times smaller in
size.

</details>


### [715] [A Low-Resource Speech-Driven NLP Pipeline for Sinhala Dyslexia Assistance](https://arxiv.org/abs/2510.04750)
*Peshala Perera,Deshan Sumanathilaka*

Main category: cs.CL

TL;DR: 本文针对僧伽罗语为说该语言的成年阅读障碍者开发辅助系统，虽数据有限但取得一定准确率，凸显了包容性NLP技术在小语种中的重要性。


<details>
  <summary>Details</summary>
Motivation: 成年阅读障碍研究不足，尤其是在非英语语境中，僧伽罗语作为低资源语言缺乏语言辅助工具。

Method: 构建辅助系统，集成Whisper进行语音转文本，用SinBERT识别常见阅读障碍错误，结合mT5和基于Mistral的模型生成纠正文本，最后用gTTS将输出转回语音。

Result: 系统转录准确率0.66，纠正准确率0.7，整体准确率0.65。

Conclusion: 该方法可行有效，强调了包容性自然语言处理技术在小语种中的重要性。

Abstract: Dyslexia in adults remains an under-researched and under-served area,
particularly in non-English-speaking contexts, despite its significant impact
on personal and professional lives. This work addresses that gap by focusing on
Sinhala, a low-resource language with limited tools for linguistic
accessibility. We present an assistive system explicitly designed for
Sinhala-speaking adults with dyslexia. The system integrates Whisper for
speech-to-text conversion, SinBERT, an open-sourced fine-tuned BERT model
trained for Sinhala to identify common dyslexic errors, and a combined mT5 and
Mistral-based model to generate corrected text. Finally, the output is
converted back to speech using gTTS, creating a complete multimodal feedback
loop. Despite the challenges posed by limited Sinhala-language datasets, the
system achieves 0.66 transcription accuracy and 0.7 correction accuracy with
0.65 overall system accuracy. These results demonstrate both the feasibility
and effectiveness of the approach. Ultimately, this work highlights the
importance of inclusive Natural Language Processing (NLP) technologies in
underrepresented languages and showcases a practical

</details>


### [716] [Probing Geometry of Next Token Prediction Using Cumulant Expansion of the Softmax Entropy](https://arxiv.org/abs/2510.04285)
*Karthik Viswanathan,Sang Eon Park*

Main category: cs.CL

TL;DR: 提出累积量扩展框架量化大语言模型在预测下一令牌时内化高阶统计结构的情况，在GPT - 2和Pythia模型上实验，结果表明该分析可作为神经网络特征学习动态的轻量级探测方法。


<details>
  <summary>Details</summary>
Motivation: 量化大语言模型在预测下一令牌时如何内化高阶统计结构。

Method: 将每层对数分布的softmax熵视为围绕其“中心”分布的扰动，推导封闭形式的累积量可观测量；在GPT - 2和Pythia模型上对Pile - 10K提示进行实验。

Result: （i）结构化提示在各层有特征性的上升 - 平稳曲线，而打乱的提示保持平稳；（ii）训练中所有累积量单调增加后饱和；（iii）数学提示与普通文本有不同累积量特征。

Conclusion: 累积量分析可作为高维神经网络特征学习动态的轻量级、有数学依据的探测方法。

Abstract: We introduce a cumulant-expansion framework for quantifying how large
language models (LLMs) internalize higher-order statistical structure during
next-token prediction. By treating the softmax entropy of each layer's logit
distribution as a perturbation around its "center" distribution, we derive
closed-form cumulant observables that isolate successively higher-order
correlations. Empirically, we track these cumulants in GPT-2 and Pythia models
on Pile-10K prompts. (i) Structured prompts exhibit a characteristic
rise-and-plateau profile across layers, whereas token-shuffled prompts remain
flat, revealing the dependence of the cumulant profile on meaningful context.
(ii) During training, all cumulants increase monotonically before saturating,
directly visualizing the model's progression from capturing variance to
learning skew, kurtosis, and higher-order statistical structures. (iii)
Mathematical prompts show distinct cumulant signatures compared to general
text, quantifying how models employ fundamentally different processing
mechanisms for mathematical versus linguistic content. Together, these results
establish cumulant analysis as a lightweight, mathematically grounded probe of
feature-learning dynamics in high-dimensional neural networks.

</details>


### [717] [ReplaceMe: Network Simplification via Depth Pruning and Transformer Block Linearization](https://arxiv.org/abs/2505.02819)
*Dmitriy Shopkhoev,Ammar Ali,Magauiya Zhussip,Valentin Malykh,Stamatios Lefkimmiatis,Nikos Komodakis,Sergey Zagoruyko*

Main category: cs.CL

TL;DR: 提出无需训练的深度剪枝方法ReplaceMe，在低压缩率下保持高性能，优于其他免训练方法，与需大量再训练的方法竞争，应用于大模型可达25%剪枝且保留约90%性能，开源代码。


<details>
  <summary>Details</summary>
Motivation: 传统剪枝方法需额外训练或微调，希望提出无需训练的深度剪枝方法。

Method: 使用小校准数据集估计线性变换来近似剪枝块，将估计的线性映射与剩余transformer块无缝合并。

Result: ReplaceMe始终优于其他免训练方法，与需大量再训练的方法有竞争力，应用于大模型可达25%剪枝且保留约90%性能，计算开销小。

Conclusion: ReplaceMe是一种有效的无需训练的深度剪枝方法，可在低压缩率下保持高性能，开源代码方便使用。

Abstract: We introduce ReplaceMe, a generalized training-free depth pruning method that
effectively replaces transformer blocks with a linear operation, while
maintaining high performance for low compression ratios. In contrast to
conventional pruning approaches that require additional training or
fine-tuning, our approach requires only a small calibration dataset that is
used to estimate a linear transformation, which approximates the pruned blocks.
The estimated linear mapping can be seamlessly merged with the remaining
transformer blocks, eliminating the need for any additional network parameters.
Our experiments show that ReplaceMe consistently outperforms other
training-free approaches and remains highly competitive with state-of-the-art
pruning methods that involve extensive retraining/fine-tuning and architectural
modifications. Applied to several large language models (LLMs), ReplaceMe
achieves up to 25% pruning while retaining approximately 90% of the original
model's performance on open benchmarks - without any training or healing steps,
resulting in minimal computational overhead (see Fig.1). We provide an
open-source library implementing ReplaceMe alongside several state-of-the-art
depth pruning techniques, available at https://github.com/mts-ai/ReplaceMe.

</details>


### [718] [Decomposing Attention To Find Context-Sensitive Neurons](https://arxiv.org/abs/2510.03315)
*Alex Gibson*

Main category: cs.CL

TL;DR: 研究transformer语言模型中特定注意力头，通过校准文本抽样结合其输出，近似线性总结周围文本，发现数百个对文本高级上下文属性有反应的神经元。


<details>
  <summary>Details</summary>
Motivation: 研究transformer语言模型中注意力模式分散、注意力分数与内容关联弱的注意力头。

Method: 论证特定注意力头在固定标记分布时softmax分母稳定，从校准文本抽样softmax分母，结合GPT2 - Small第一层多个稳定头的输出。

Result: 能将多个稳定头的输出近似为周围文本的线性总结，仅通过权重和单个校准文本就能发现数百个对周围文本高级上下文属性有反应的神经元。

Conclusion: 仅利用权重和校准文本可挖掘出对文本高级上下文属性有反应的神经元，包括在校准文本上未激活的神经元。

Abstract: We study transformer language models, analyzing attention heads whose
attention patterns are spread out, and whose attention scores depend weakly on
content. We argue that the softmax denominators of these heads are stable when
the underlying token distribution is fixed. By sampling softmax denominators
from a "calibration text", we can combine together the outputs of multiple such
stable heads in the first layer of GPT2-Small, approximating their combined
output by a linear summary of the surrounding text. This approximation enables
a procedure where from the weights alone - and a single calibration text - we
can uncover hundreds of first layer neurons that respond to high-level
contextual properties of the surrounding text, including neurons that didn't
activate on the calibration text.

</details>


### [719] [Implicit Values Embedded in How Humans and LLMs Complete Subjective Everyday Tasks](https://arxiv.org/abs/2510.03384)
*Arjun Arunasalam,Madison Pickering,Z. Berkay Celik,Blase Ur*

Main category: cs.CL

TL;DR: 研究六种流行大语言模型完成日常任务时展现的隐性价值观，发现其与人类及彼此间常不一致。


<details>
  <summary>Details</summary>
Motivation: 了解大语言模型在完成主观日常任务时展现的隐性价值观，以及与人类的对比情况。

Method: 审计六种流行大语言模型完成30项日常任务的情况，并与100名美国人类众包工作者对比。

Result: 大语言模型在展现的隐性价值观上常与人类及其他大语言模型不一致。

Conclusion: 大语言模型在完成日常任务时展现的隐性价值观存在差异。

Abstract: Large language models (LLMs) can underpin AI assistants that help users with
everyday tasks, such as by making recommendations or performing basic
computation. Despite AI assistants' promise, little is known about the implicit
values these assistants display while completing subjective everyday tasks.
Humans may consider values like environmentalism, charity, and diversity. To
what extent do LLMs exhibit these values in completing everyday tasks? How do
they compare with humans? We answer these questions by auditing how six popular
LLMs complete 30 everyday tasks, comparing LLMs to each other and to 100 human
crowdworkers from the US. We find LLMs often do not align with humans, nor with
other LLMs, in the implicit values exhibited.

</details>


### [720] [SEER: The Span-based Emotion Evidence Retrieval Benchmark](https://arxiv.org/abs/2510.03490)
*Aneesha Sampath,Oya Aran,Emily Mower Provost*

Main category: cs.CL

TL;DR: 提出SEER基准测试大语言模型识别表达情感文本片段的能力，评估14个开源大模型，发现单句表现尚可但长文本精度下降并分析错误模式。


<details>
  <summary>Details</summary>
Motivation: 传统情感识别任务为整句分配单一标签，SEER旨在解决未充分探索的情感证据检测任务，该任务对共情对话和临床支持等应用很关键。

Method: 引入SEER基准，包含单句和短文两个任务，对1200个真实句子进行新的情感和情感证据标注，评估14个开源大语言模型。

Result: 部分模型在单句输入上接近人类平均表现，但在长文中精度下降。

Conclusion: 通过错误分析揭示了模型的关键失败模式，如过度依赖情感关键词和中性文本中的误报。

Abstract: We introduce the SEER (Span-based Emotion Evidence Retrieval) Benchmark to
test Large Language Models' (LLMs) ability to identify the specific spans of
text that express emotion. Unlike traditional emotion recognition tasks that
assign a single label to an entire sentence, SEER targets the underexplored
task of emotion evidence detection: pinpointing which exact phrases convey
emotion. This span-level approach is crucial for applications like empathetic
dialogue and clinical support, which need to know how emotion is expressed, not
just what the emotion is. SEER includes two tasks: identifying emotion evidence
within a single sentence, and identifying evidence across a short passage of
five consecutive sentences. It contains new annotations for both emotion and
emotion evidence on 1200 real-world sentences. We evaluate 14 open-source LLMs
and find that, while some models approach average human performance on
single-sentence inputs, their accuracy degrades in longer passages. Our error
analysis reveals key failure modes, including overreliance on emotion keywords
and false positives in neutral text.

</details>


### [721] [ALHD: A Large-Scale and Multigenre Benchmark Dataset for Arabic LLM-Generated Text Detection](https://arxiv.org/abs/2510.03502)
*Ali Khairallah,Arkaitz Zubiaga*

Main category: cs.CL

TL;DR: 介绍首个大规模阿拉伯语数据集ALHD用于区分人类和大语言模型生成文本，进行基准实验，显示微调BERT模型表现好但跨体裁泛化有挑战。


<details>
  <summary>Details</summary>
Motivation: 创建大规模综合阿拉伯语数据集以研究阿拉伯语大语言模型生成文本检测的泛化性，应对相关风险。

Method: 创建ALHD数据集，进行严格预处理、丰富标注和标准化分割，使用传统分类器、BERT模型和大语言模型进行基准实验。

Result: 微调BERT模型表现有竞争力，优于大语言模型，但跨体裁泛化存在挑战，尤其是新闻文章。

Conclusion: ALHD为阿拉伯语大语言模型检测及相关研究奠定基础，指出未来研究方向。

Abstract: We introduce ALHD, the first large-scale comprehensive Arabic dataset
explicitly designed to distinguish between human- and LLM-generated texts. ALHD
spans three genres (news, social media, reviews), covering both MSA and
dialectal Arabic, and contains over 400K balanced samples generated by three
leading LLMs and originated from multiple human sources, which enables studying
generalizability in Arabic LLM-genearted text detection. We provide rigorous
preprocessing, rich annotations, and standardized balanced splits to support
reproducibility. In addition, we present, analyze and discuss benchmark
experiments using our new dataset, in turn identifying gaps and proposing
future research directions. Benchmarking across traditional classifiers,
BERT-based models, and LLMs (zero-shot and few-shot) demonstrates that
fine-tuned BERT models achieve competitive performance, outperforming LLM-based
models. Results are however not always consistent, as we observe challenges
when generalizing across genres; indeed, models struggle to generalize when
they need to deal with unseen patterns in cross-genre settings, and these
challenges are particularly prominent when dealing with news articles, where
LLM-generated texts resemble human texts in style, which opens up avenues for
future research. ALHD establishes a foundation for research related to Arabic
LLM-detection and mitigating risks of misinformation, academic dishonesty, and
cyber threats.

</details>


### [722] [TS-Reasoner: Aligning Time Series Foundation Models with LLM Reasoning](https://arxiv.org/abs/2510.03519)
*Fangxu Yu,Hongyu Zhao,Tianyi Zhou*

Main category: cs.CL

TL;DR: 本文提出TS - Reasoner模型，结合时间序列基础模型（TSFMs）和大语言模型（LLMs）用于时间序列推理，实验显示其表现优异且数据效率高。


<details>
  <summary>Details</summary>
Motivation: 现有TSFMs缺乏背景知识和复杂推理能力，LLMs对时间序列数据数值理解不足，整合二者开发有效训练方法是挑战。

Method: 提出方法生成时间序列和文本描述对用于对齐训练，采用两阶段训练方法，利用预训练TSFM并在训练中冻结。

Result: TS - Reasoner在多个基准测试中优于多种流行模型，且数据效率显著，使用不到一半训练数据。

Conclusion: TS - Reasoner能有效结合TSFMs和LLMs进行时间序列推理任务，性能和数据效率表现出色。

Abstract: Time series reasoning is crucial to decision-making in diverse domains,
including finance, energy usage, traffic, weather, and scientific discovery.
While existing time series foundation models (TSFMs) can capture low-level
dynamic patterns and provide accurate forecasting, further analysis usually
requires additional background knowledge and sophisticated reasoning, which are
lacking in most TSFMs but can be achieved through large language models (LLMs).
On the other hand, without expensive post-training, LLMs often struggle with
the numerical understanding of time series data. Although it is intuitive to
integrate the two types of models, developing effective training recipes that
align the two modalities for reasoning tasks is still an open challenge. To
this end, we propose TS-Reasoner that aligns the latent representations of
TSFMs with the textual inputs of LLMs for downstream understanding/reasoning
tasks. Specifically, we propose a simple yet effective method to curate
diverse, synthetic pairs of time series and textual captions for alignment
training. We then develop a two-stage training recipe that applies instruction
finetuning after the alignment pretraining. Unlike existing works that train an
LLM to take time series as inputs, we leverage a pretrained TSFM and freeze it
during training. Extensive experiments on several benchmarks demonstrate that
TS-Reasoner not only outperforms a wide range of prevailing LLMs, Vision
Language Models (VLMs), and Time Series LLMs, but also achieves this with
remarkable data efficiency, e.g., using less than half the training data.

</details>


### [723] [Identifying Financial Risk Information Using RAG with a Contrastive Insight](https://arxiv.org/abs/2510.03521)
*Ali Elahi*

Main category: cs.CL

TL;DR: 论文指出RAG在专业推理任务中存在局限性，提出在RAG之上添加同行感知比较推理层，且对比方法在文本生成指标上优于基线RAG。


<details>
  <summary>Details</summary>
Motivation: RAG在专业推理任务输出泛化，不能检索可比案例或相关问题，无法提供特定情境见解，如在金融领域输出通用风险。

Method: 在RAG之上添加同行感知比较推理层。

Result: 对比方法在ROUGE和BERTScore等文本生成指标上优于基线RAG。

Conclusion: 所提出的对比方法能有效改善RAG在专业推理任务中的表现。

Abstract: In specialized domains, humans often compare new problems against similar
examples, highlight nuances, and draw conclusions instead of analyzing
information in isolation. When applying reasoning in specialized contexts with
LLMs on top of a RAG, the pipeline can capture contextually relevant
information, but it is not designed to retrieve comparable cases or related
problems.
  While RAG is effective at extracting factual information, its outputs in
specialized reasoning tasks often remain generic, reflecting broad facts rather
than context-specific insights. In finance, it results in generic risks that
are true for the majority of companies. To address this limitation, we propose
a peer-aware comparative inference layer on top of RAG.
  Our contrastive approach outperforms baseline RAG in text generation metrics
such as ROUGE and BERTScore in comparison with human-generated equity research
and risk.

</details>


### [724] [TriMediQ: A Triplet-Structured Approach for Interactive Medical Question Answering](https://arxiv.org/abs/2510.03536)
*Zhaohan Meng,Zaiqiao Meng,Siwei Liu,Iadh Ounis*

Main category: cs.CL

TL;DR: 提出TriMediQ解决大语言模型在医学多轮问答中推理可靠性低的问题，在两个基准上评估显示有精度提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在静态单轮医学问答基准表现好，但在实际临床咨询的迭代信息收集过程表现不佳，MEDIQ框架有可靠性问题。

Method: 将患者回复总结为三元组并集成到知识图谱，引入冻结的三元组生成器和可训练的投影模块，分两步操作：先冻结大语言模型权重微调投影模块，再用微调模块指导推理。

Result: 在两个交互式问答基准上评估，在iMedQA数据集上比五个基线最高提升10.4%的精度。

Conclusion: 将患者回复转换为基于三元组的结构化图能在多轮环境中实现更准确的临床推理，为基于大语言模型的医疗助手部署提供解决方案。

Abstract: Large Language Models (LLMs) perform strongly in static and single-turn
medical Question Answer (QA) benchmarks, yet such settings diverge from the
iterative information gathering process required in practical clinical
consultations. The MEDIQ framework addresses this mismatch by recasting the
diagnosis as an interactive dialogue between a patient and an expert system,
but the reliability of LLMs drops dramatically when forced to reason with
dialogue logs, where clinical facts appear in sentences without clear links. To
bridge this gap, we introduce TriMediQ, a triplet-structured approach that
summarises patient responses into triplets and integrates them into a Knowledge
Graph (KG), enabling multi-hop reasoning. We introduce a frozen triplet
generator that extracts clinically relevant triplets, using prompts designed to
ensure factual consistency. In parallel, a trainable projection module,
comprising a graph encoder and a projector, captures relational information
from the KG to enhance expert reasoning. TriMediQ operates in two steps: (i)
the projection module fine-tuning with all LLM weights frozen; and (ii) using
the fine-tuned module to guide multi-hop reasoning during inference. We
evaluate TriMediQ on two interactive QA benchmarks, showing that it achieves up
to 10.4\% improvement in accuracy over five baselines on the iMedQA dataset.
These results demonstrate that converting patient responses into structured
triplet-based graphs enables more accurate clinical reasoning in multi-turn
settings, providing a solution for the deployment of LLM-based medical
assistants.

</details>


### [725] [Reactive Transformer (RxT) -- Stateful Real-Time Processing for Event-Driven Reactive Language Models](https://arxiv.org/abs/2510.03561)
*Adam Filipek*

Main category: cs.CL

TL;DR: 本文提出Reactive Transformer (RxT)架构，解决Transformer在对话AI中的局限，降低成本和延迟，实验验证其性能优越。


<details>
  <summary>Details</summary>
Motivation: Transformer架构因无状态特性和二次计算复杂度，在对话AI应用受限，当前模型处理长对话成本高、延迟大。

Method: 引入RxT架构，从数据驱动转向事件驱动，用固定大小短期记忆系统维护上下文，通过特定操作周期更新记忆。

Result: 通过概念验证实验，在合成数据上验证了RxT相比无状态基线模型性能更优，推理延迟恒定。

Conclusion: RxT架构能降低对话成本，实现低延迟、实时、有状态且经济可行的长对话。

Abstract: The Transformer architecture has become the de facto standard for Large
Language Models (LLMs), demonstrating remarkable capabilities in language
understanding and generation. However, its application in conversational AI is
fundamentally constrained by its stateless nature and the quadratic
computational complexity ($O(L^2)$) with respect to sequence length $L$.
Current models emulate memory by reprocessing an ever-expanding conversation
history with each turn, leading to prohibitive costs and latency in long
dialogues. This paper introduces the Reactive Transformer (RxT), a novel
architecture designed to overcome these limitations by shifting from a
data-driven to an event-driven paradigm. RxT processes each conversational turn
as a discrete event in real-time, maintaining context in an integrated,
fixed-size Short-Term Memory (STM) system. The architecture features a distinct
operational cycle where a generator-decoder produces a response based on the
current query and the previous memory state, after which a memory-encoder and a
dedicated Memory Attention network asynchronously update the STM with a
representation of the complete interaction. This design fundamentally alters
the scaling dynamics, reducing the total user-facing cost of a conversation
from quadratic ($O(N^2 \cdot T)$) to linear ($O(N \cdot T)$) with respect to
the number of interactions $N$. By decoupling response generation from memory
updates, RxT achieves low latency, enabling truly real-time, stateful, and
economically viable long-form conversations. We validated our architecture with
a series of proof-of-concept experiments on synthetic data, demonstrating
superior performance and constant-time inference latency compared to a baseline
stateless model of comparable size.

</details>


### [726] [Can an LLM Induce a Graph? Investigating Memory Drift and Context Length](https://arxiv.org/abs/2510.03611)
*Raquib Bin Yousuf,Aadyant Khatri,Shengzhe Xu,Mandar Sharma,Naren Ramakrishnan*

Main category: cs.CL

TL;DR: 现有大语言模型评估基准不准确，本文用复杂推理任务评估，发现模型在关系推理时有效长度更短，指出模型局限并建议架构改进。


<details>
  <summary>Details</summary>
Motivation: 现有评估基准依赖简单任务，不能准确反映大语言模型在信息密集场景下的性能。

Method: 用复杂推理任务评估大语言模型，让模型从文本中归纳结构化关系知识。

Result: 大语言模型在关系推理任务中比现有基准显示的情况更早出现记忆漂移和上下文遗忘，推理专用模型也易受影响。

Conclusion: 模型从非结构化输入中提取结构化知识的能力有显著局限，需要进行架构调整以提升远程推理能力。

Abstract: Recently proposed evaluation benchmarks aim to characterize the effective
context length and the forgetting tendencies of large language models (LLMs).
However, these benchmarks often rely on simplistic 'needle in a haystack'
retrieval or continuation tasks that may not accurately reflect the performance
of these models in information-dense scenarios. Thus, rather than simple next
token prediction, we argue for evaluating these models on more complex
reasoning tasks that requires them to induce structured relational knowledge
from the text - such as graphs from potentially noisy natural language content.
While the input text can be viewed as generated in terms of a graph, its
structure is not made explicit and connections must be induced from distributed
textual cues, separated by long contexts and interspersed with irrelevant
information. Our findings reveal that LLMs begin to exhibit memory drift and
contextual forgetting at much shorter effective lengths when tasked with this
form of relational reasoning, compared to what existing benchmarks suggest.
With these findings, we offer recommendations for the optimal use of popular
LLMs for complex reasoning tasks. We further show that even models specialized
for reasoning, such as OpenAI o1, remain vulnerable to early memory drift in
these settings. These results point to significant limitations in the models'
ability to abstract structured knowledge from unstructured input and highlight
the need for architectural adaptations to improve long-range reasoning.

</details>


### [727] [Towards Unsupervised Speech Recognition at the Syllable-Level](https://arxiv.org/abs/2510.03639)
*Liming Wang,Junrui Ni,Kai-Wei Chang,Saurabhchand Bhati,David Harwath,Mark Hasegawa-Johnson,James R. Glass*

Main category: cs.CL

TL;DR: 提出基于掩码语言建模的音节级无监督语音识别框架，避免G2P需求和GAN方法不稳定性，降低错误率并有效泛化。


<details>
  <summary>Details</summary>
Motivation: 现有基于音素的无监督语音识别方法依赖昂贵资源且训练不稳定，难以泛化到音素边界模糊的语言。

Method: 引入基于掩码语言建模的音节级无监督语音识别框架。

Result: 在LibriSpeech上字符错误率相对降低达40%，能有效泛化到普通话。

Conclusion: 所提方法解决了现有无监督语音识别方法的两个挑战，具有良好效果。

Abstract: Training speech recognizers with unpaired speech and text -- known as
unsupervised speech recognition (UASR) -- is a crucial step toward extending
ASR to low-resource languages in the long-tail distribution and enabling
multimodal learning from non-parallel data. However, existing approaches based
on phones often rely on costly resources such as grapheme-to-phoneme converters
(G2Ps) and struggle to generalize to languages with ambiguous phoneme
boundaries due to training instability. In this paper, we address both
challenges by introducing a syllable-level UASR framework based on masked
language modeling, which avoids the need for G2P and the instability of
GAN-based methods. Our approach achieves up to a 40\% relative reduction in
character error rate (CER) on LibriSpeech and generalizes effectively to
Mandarin, a language that has remained particularly difficult for prior
methods. Code will be released upon acceptance.

</details>


### [728] [MedReflect: Teaching Medical LLMs to Self-Improve via Reflective Correction](https://arxiv.org/abs/2510.03687)
*Yue Huang,Yanyuan Chen,Dexuan Xu,Weihua Yue,Huamin Zhang,Meikang Qiu,Yu Huang*

Main category: cs.CL

TL;DR: 本文提出MedReflect框架，让大语言模型以类似医生的反思思维模式解决医疗问题，能在降低标注需求的同时提升医疗基准测试的准确率，减少对外部监督和大量特定任务微调数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型解决医疗问题的方法存在检索开销大、标注成本高、依赖外部助手且性能有限等问题，需要新方法。

Method: 引入MedReflect框架，生成包括初始假设生成、自我提问、自我回答和决策细化的单遍反思链，利用自我验证和自我反思释放大语言模型在医疗问题解决中的潜力。

Result: 仅用2000个随机采样的训练示例和轻量级微调，MedReflect在一系列医疗基准测试中显著提高了绝对准确率，同时减少了标注需求。

Conclusion: 大语言模型可以通过自我反思和自我提升来解决专业医疗问题，减少对外部监督和大量特定任务微调数据的依赖。

Abstract: Medical problem solving demands expert knowledge and intricate reasoning.
Recent studies of large language models (LLMs) attempt to ease this complexity
by introducing external knowledge verification through retrieval-augmented
generation or by training on reasoning datasets. However, these approaches
suffer from drawbacks such as retrieval overhead and high annotation costs, and
they heavily rely on substituted external assistants to reach limited
performance in medical field. In this paper, we introduce MedReflect, a
generalizable framework designed to inspire LLMs with a physician-like
reflective thinking mode. MedReflect generates a single-pass reflection chain
that includes initial hypothesis generation, self-questioning, self-answering
and decision refinement. This self-verified and self-reflective nature releases
large language model's latent capability in medical problem-solving without
external retrieval or heavy annotation. We demonstrate that MedReflect enables
cost-efficient medical dataset construction: with merely 2,000 randomly sampled
training examples and a light fine-tuning, this approach achieves notable
absolute accuracy improvements across a series of medical benchmarks while
cutting annotation requirements. Our results provide evidence that LLMs can
learn to solve specialized medical problems via self-reflection and
self-improve, reducing reliance on external supervision and extensive
task-specific fine-tuning data.

</details>


### [729] [TreePrompt: Leveraging Hierarchical Few-Shot Example Selection for Improved English-Persian and English-German Translation](https://arxiv.org/abs/2510.03748)
*Ramtin Kakavand,Ebrahim Ansari*

Main category: cs.CL

TL;DR: 提出TreePrompt示例选择方法，结合K - NN和AFSP，在英波和英德翻译任务评估中提升了翻译性能。


<details>
  <summary>Details</summary>
Motivation: 现有示例选择方法仅关注查询与示例的相似度，未考虑示例质量，需改进以提升翻译质量。

Method: 提出TreePrompt方法在树状框架中识别高质量、上下文相关示例，结合K - NN和AFSP探索相似度与质量的平衡。

Result: 在英波（MIZAN）和英德（WMT19）两种语言对的评估中，TreePrompt与AFSP或随机选择结合提升了翻译性能。

Conclusion: TreePrompt方法及与其他方法结合在提升翻译性能方面有效。

Abstract: Large Language Models (LLMs) have consistently demonstrated strong
performance in machine translation, especially when guided by high-quality
prompts. Few-shot prompting is an effective technique to improve translation
quality; however, most existing example selection methods focus solely on
query-to-example similarity and do not account for the quality of the examples.
In this work, we propose TreePrompt, a novel example selection approach that
learns LLM preferences to identify high-quality, contextually relevant examples
within a tree-structured framework. To further explore the balance between
similarity and quality, we combine TreePrompt with K-Nearest Neighbors (K-NN)
and Adaptive Few-Shot Prompting (AFSP). Evaluations on two language pairs -
English-Persian (MIZAN) and English-German (WMT19) - show that integrating
TreePrompt with AFSP or Random selection leads to improved translation
performance.

</details>


### [730] [Rezwan: Leveraging Large Language Models for Comprehensive Hadith Text Processing: A 1.2M Corpus Development](https://arxiv.org/abs/2510.03781)
*Majid Asgari-Bidhendi,Muhammad Amin Ghaseminia,Alireza Shahbazi,Sayyed Ali Hossayni,Najmeh Torabian,Behrouz Minaei-Bidgoli*

Main category: cs.CL

TL;DR: 本文开发了包含超120万条圣训的Rezwan语料库，通过自动化流程提取和构建，经多步处理后进行评估，结果良好且成本低，引入宗教文本处理新范式。


<details>
  <summary>Details</summary>
Motivation: 开发一个大规模、多语言且语义丰富的圣训语料库，以助力数字人文和伊斯兰研究。

Method: 基于数字存储库，利用大语言模型进行分段、链文分离、验证和多层丰富处理，对语料进行多种增强操作。

Result: 在结构任务上接近人类准确率，如链文分离和总结评分为9.33/10；与Noor语料库相比，规模和质量更优，平均总分8.46/10；成本分析显示AI方法经济可行。

Conclusion: 展示了AI可增强人类专业知识，为伊斯兰遗产提供大规模、多语言和语义丰富的访问途径，引入宗教文本处理新范式。

Abstract: This paper presents the development of Rezwan, a large-scale AI-assisted
Hadith corpus comprising over 1.2M narrations, extracted and structured through
a fully automated pipeline. Building on digital repositories such as Maktabat
Ahl al-Bayt, the pipeline employs Large Language Models (LLMs) for
segmentation, chain--text separation, validation, and multi-layer enrichment.
Each narration is enhanced with machine translation into twelve languages,
intelligent diacritization, abstractive summarization, thematic tagging, and
cross-text semantic analysis. This multi-step process transforms raw text into
a richly annotated research-ready infrastructure for digital humanities and
Islamic studies. A rigorous evaluation was conducted on 1,213 randomly sampled
narrations, assessed by six domain experts. Results show near-human accuracy in
structured tasks such as chain--text separation (9.33/10) and summarization
(9.33/10), while highlighting ongoing challenges in diacritization and semantic
similarity detection. Comparative analysis against the manually curated Noor
Corpus demonstrates the superiority of Najm in both scale and quality, with a
mean overall score of 8.46/10 versus 3.66/10. Furthermore, cost analysis
confirms the economic feasibility of the AI approach: tasks requiring over
229,000 hours of expert labor were completed within months at a fraction of the
cost. The work introduces a new paradigm in religious text processing by
showing how AI can augment human expertise, enabling large-scale, multilingual,
and semantically enriched access to Islamic heritage.

</details>


### [731] [Mechanistic Interpretability of Socio-Political Frames in Language Models](https://arxiv.org/abs/2510.03799)
*Hadi Asghari,Sami Nenno*

Main category: cs.CL

TL;DR: 研究大语言模型生成和识别深度认知框架的能力，发现其有相应能力并研究框架在模型隐藏表征中的位置。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在社会政治背景下生成和识别深度认知框架的能力。

Method: 受机制可解释性研究启发，研究模型隐藏表征中`严格父亲`和`养育父母`框架的位置。

Result: 大语言模型能流畅生成唤起特定框架的文本，可在零样本设置下识别框架，找到与框架存在强相关的单一维度。

Conclusion: 研究有助于理解大语言模型如何捕捉和表达有意义的人类概念。

Abstract: This paper explores the ability of large language models to generate and
recognize deep cognitive frames, particularly in socio-political contexts. We
demonstrate that LLMs are highly fluent in generating texts that evoke specific
frames and can recognize these frames in zero-shot settings. Inspired by
mechanistic interpretability research, we investigate the location of the
`strict father' and `nurturing parent' frames within the model's hidden
representation, identifying singular dimensions that correlate strongly with
their presence. Our findings contribute to understanding how LLMs capture and
express meaningful human concepts.

</details>


### [732] [Beyond Token Length: Step Pruner for Efficient and Accurate Reasoning in Large Language Models](https://arxiv.org/abs/2510.03805)
*Canhui Wu,Qiong Cao,Chang Li,Zhenfang Wang,Chao Xue,Yuwei Fan,Wei Xi,Xiaodong He*

Main category: cs.CL

TL;DR: 提出Step Pruner (SP)框架解决大推理模型‘过度思考’问题，实验显示其在提升准确率同时显著减少响应长度。


<details>
  <summary>Details</summary>
Motivation: 现有通过强化学习解决大推理模型‘过度思考’问题的方法存在响应少令牌不对应少推理步骤、模型后期会出现丢弃推理步骤以减少令牌使用的‘作弊’行为等挑战。

Method: 引入Step Pruner (SP)强化学习框架，设计步长感知奖励函数，优先保证正确性并惩罚冗余步骤，对错误响应不给予奖励；提出动态停止机制，当输出步长超过上限时停止更新。

Result: 在四个推理基准测试中，SP达到了最先进的准确率，同时显著减少了响应长度，如在AIME24上减少了69.7%的令牌使用。

Conclusion: SP框架能有效引导大推理模型进行更高效的推理。

Abstract: Large Reasoning Models (LRMs) demonstrate strong performance on complex tasks
but often suffer from excessive verbosity, known as "overthinking." Existing
solutions via reinforcement learning (RL) typically penalize generated tokens
to promote conciseness. However, these methods encounter two challenges:
responses with fewer tokens do not always correspond to fewer reasoning steps,
and models may develop hacking behavior in later stages of training by
discarding reasoning steps to minimize token usage. In this work, we introduce
\textbf{Step Pruner (SP)}, an RL framework that steers LRMs toward more
efficient reasoning by favoring compact reasoning steps. Our step-aware reward
function prioritizes correctness while imposing penalties for redundant steps,
and withholds rewards for incorrect responses to prevent the reinforcement of
erroneous reasoning. Moreover, we propose a dynamic stopping mechanism: when
the length of any output step exceeds the upper limit, we halt updates to
prevent hacking behavior caused by merging steps. Extensive experiments across
four reasoning benchmarks demonstrate that SP achieves state-of-the-art
accuracy while significantly reducing response length. For instance, on AIME24,
SP reduces token usage by \textbf{69.7\%}.

</details>


### [733] [Named Entity Recognition in COVID-19 tweets with Entity Knowledge Augmentation](https://arxiv.org/abs/2510.04001)
*Xuankang Zhang,Jiangming Liu*

Main category: cs.CL

TL;DR: 本文提出针对COVID - 19的实体知识增强方法，能提升命名实体识别性能，代码开源。


<details>
  <summary>Details</summary>
Motivation: COVID - 19疫情在社交媒体引发诸多讨论，识别相关命名实体很重要，但现有相关工作因文本不正式、标注少和需特定领域知识而受限。

Method: 提出一种新颖的针对COVID - 19的实体知识增强方法，可用于非正式和正式生物医学文本的命名实体识别。

Result: 在COVID - 19推文数据集和PubMed数据集上实验表明，该方法在全监督和少样本设置下都能提升命名实体识别性能。

Conclusion: 所提实体知识增强方法有效，可提升相关命名实体识别性能，代码公开可促进进一步研究。

Abstract: The COVID-19 pandemic causes severe social and economic disruption around the
world, raising various subjects that are discussed over social media.
Identifying pandemic-related named entities as expressed on social media is
fundamental and important to understand the discussions about the pandemic.
However, there is limited work on named entity recognition on this topic due to
the following challenges: 1) COVID-19 texts in social media are informal and
their annotations are rare and insufficient to train a robust recognition
model, and 2) named entity recognition in COVID-19 requires extensive
domain-specific knowledge. To address these issues, we propose a novel entity
knowledge augmentation approach for COVID-19, which can also be applied in
general biomedical named entity recognition in both informal text format and
formal text format. Experiments carried out on the COVID-19 tweets dataset and
PubMed dataset show that our proposed entity knowledge augmentation improves
NER performance in both fully-supervised and few-shot settings. Our source code
is publicly available: https://github.com/kkkenshi/LLM-EKA/tree/master

</details>


### [734] [Thai Semantic End-of-Turn Detection for Real-Time Voice Agents](https://arxiv.org/abs/2510.04016)
*Thanapol Popit,Natthapath Rungseesiripak,Monthol Charattrakool,Saksorn Ruangtanusak*

Main category: cs.CL

TL;DR: 本文进行泰语仅文本对话结束检测的系统研究，对比不同模型方法，发现准确率和延迟的权衡，小的微调模型可实现近即时决策。


<details>
  <summary>Details</summary>
Motivation: 传统音频静音端点检测有延迟且在犹豫或特定语言现象下易失败，需研究泰语仅文本对话结束检测。

Method: 对比紧凑型大语言模型的零样本和少样本提示与轻量级变压器的监督微调，利用语料和泰语语言线索将对话结束检测设为词元边界的二分类决策。

Result: 发现准确率和延迟存在权衡，并给出公开可用的实现计划。

Conclusion: 建立了泰语基线，小的微调模型可实现适合设备端代理的近即时对话结束决策。

Abstract: Fluid voice-to-voice interaction requires reliable and low-latency detection
of when a user has finished speaking. Traditional audio-silence end-pointers
add hundreds of milliseconds of delay and fail under hesitations or
language-specific phenomena. We present, to our knowledge, the first systematic
study of Thai text-only end-of-turn (EOT) detection for real-time agents. We
compare zero-shot and few-shot prompting of compact LLMs to supervised
fine-tuning of lightweight transformers. Using transcribed subtitles from the
YODAS corpus and Thai-specific linguistic cues (e.g., sentence-final
particles), we formulate EOT as a binary decision over token boundaries. We
report a clear accuracy-latency tradeoff and provide a public-ready
implementation plan. This work establishes a Thai baseline and demonstrates
that small, fine-tuned models can deliver near-instant EOT decisions suitable
for on-device agents.

</details>


### [735] [Does Using Counterfactual Help LLMs Explain Textual Importance in Classification?](https://arxiv.org/abs/2510.04031)
*Nelvin Tan,James Asikin Cheung,Yu-Ching Shih,Dong Yang,Amol Salunkhe*

Main category: cs.CL

TL;DR: 研究将反事实纳入大语言模型推理对其识别分类决策关键词能力的影响，引入决策变化率框架，实验表明反事实有帮助。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在文本分类任务有效，需解释其决策，且模型为黑盒、调用成本高，因此研究反事实对其识别关键词能力的影响。

Method: 引入决策变化率框架来量化分类中顶级词汇的重要性。

Result: 实验结果显示使用反事实是有帮助的。

Conclusion: 将反事实纳入大语言模型推理有助于其识别分类决策中的关键词。

Abstract: Large language models (LLMs) are becoming useful in many domains due to their
impressive abilities that arise from large training datasets and large model
sizes. More recently, they have been shown to be very effective in textual
classification tasks, motivating the need to explain the LLMs' decisions.
Motivated by practical constrains where LLMs are black-boxed and LLM calls are
expensive, we study how incorporating counterfactuals into LLM reasoning can
affect the LLM's ability to identify the top words that have contributed to its
classification decision. To this end, we introduce a framework called the
decision changing rate that helps us quantify the importance of the top words
in classification. Our experimental results show that using counterfactuals can
be helpful.

</details>


### [736] [Small Language Models for Emergency Departments Decision Support: A Benchmark Study](https://arxiv.org/abs/2510.04032)
*Zirui Wang,Jiajun Wu,Braden Teitge,Jessalyn Holodinsky,Steve Drew*

Main category: cs.CL

TL;DR: 研究提出评估基准以识别适用于急诊科决策支持的小语言模型，实验发现通用领域小模型表现优于医学微调模型。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在急诊科应用受限，小语言模型因推理能力和高效性能有潜力，且考虑到现实部署中的硬件、成本和隐私问题。

Method: 提出综合评估基准，关注在通用领域和医学语料上训练的小语言模型，使用MedMCQA、MedQA - 4Options和PubMedQA等数据集。

Result: 通用领域小语言模型在急诊科的多种基准测试中优于医学微调模型。

Conclusion: 对于急诊科决策支持，模型可能无需进行专门的医学微调。

Abstract: Large language models (LLMs) have become increasingly popular in medical
domains to assist physicians with a variety of clinical and operational tasks.
Given the fast-paced and high-stakes environment of emergency departments
(EDs), small language models (SLMs), characterized by a reduction in parameter
count compared to LLMs, offer significant potential due to their inherent
reasoning capability and efficient performance. This enables SLMs to support
physicians by providing timely and accurate information synthesis, thereby
improving clinical decision-making and workflow efficiency. In this paper, we
present a comprehensive benchmark designed to identify SLMs suited for ED
decision support, taking into account both specialized medical expertise and
broad general problem-solving capabilities. In our evaluations, we focus on
SLMs that have been trained on a mixture of general-domain and medical corpora.
A key motivation for emphasizing SLMs is the practical hardware limitations,
operational cost constraints, and privacy concerns in the typical real-world
deployments. Our benchmark datasets include MedMCQA, MedQA-4Options, and
PubMedQA, with the medical abstracts dataset emulating tasks aligned with real
ED physicians' daily tasks. Experimental results reveal that general-domain
SLMs surprisingly outperform their medically fine-tuned counterparts across
these diverse benchmarks for ED. This indicates that for ED, specialized
medical fine-tuning of the model may not be required.

</details>


### [737] [Unveiling LLMs' Metaphorical Understanding: Exploring Conceptual Irrelevance, Context Leveraging and Syntactic Influence](https://arxiv.org/abs/2510.04120)
*Fengying Ye,Shanshan Wang,Lidia S. Chao,Derek F. Wong*

Main category: cs.CL

TL;DR: 研究从三方面考察大语言模型隐喻处理能力，发现其存在局限，需更稳健计算方法


<details>
  <summary>Details</summary>
Motivation: 大语言模型在隐喻理解机制方面探索不足，有必要研究其隐喻处理能力

Method: 从概念映射、隐喻 - 字面知识库、句法敏感性三个视角研究

Result: 大语言模型生成15%-25%概念不相关解释，依赖训练数据中隐喻指标而非上下文线索，对句法不规则性更敏感

Conclusion: 大语言模型在隐喻分析中有局限，需更稳健计算方法

Abstract: Metaphor analysis is a complex linguistic phenomenon shaped by context and
external factors. While Large Language Models (LLMs) demonstrate advanced
capabilities in knowledge integration, contextual reasoning, and creative
generation, their mechanisms for metaphor comprehension remain insufficiently
explored. This study examines LLMs' metaphor-processing abilities from three
perspectives: (1) Concept Mapping: using embedding space projections to
evaluate how LLMs map concepts in target domains (e.g., misinterpreting "fall
in love" as "drop down from love"); (2) Metaphor-Literal Repository: analyzing
metaphorical words and their literal counterparts to identify inherent
metaphorical knowledge; and (3) Syntactic Sensitivity: assessing how
metaphorical syntactic structures influence LLMs' performance. Our findings
reveal that LLMs generate 15\%-25\% conceptually irrelevant interpretations,
depend on metaphorical indicators in training data rather than contextual cues,
and are more sensitive to syntactic irregularities than to structural
comprehension. These insights underline the limitations of LLMs in metaphor
analysis and call for more robust computational approaches.

</details>


### [738] [Thinking on the Fly: Test-Time Reasoning Enhancement via Latent Thought Policy Optimization](https://arxiv.org/abs/2510.04182)
*Wengao Ye,Yan Liang,Lianlei Shan*

Main category: cs.CL

TL;DR: 介绍了Latent Thought Policy Optimization (LTPO)框架，它能在测试时增强大语言模型推理能力，实验显示其在推理基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的潜在推理在具有挑战性的分布外任务中较为脆弱，需要改进。

Method: 引入无参数框架LTPO，将中间潜在“思想”向量作为动态参数，采用基于内在、基于置信度的奖励信号引导的在线策略梯度方法。

Result: 在五个推理基准测试中，LTPO在标准任务上与强基线相当或超越，在其他方法失败的情况下表现出显著的鲁棒性，在高挑战性的AIME基准测试中大幅提升。

Conclusion: LTPO具有独特的复杂推理能力，能有效克服现有潜在推理的局限性。

Abstract: Recent advancements in Large Language Models (LLMs) have shifted from
explicit Chain-of-Thought (CoT) reasoning to more efficient latent reasoning,
where intermediate thoughts are represented as vectors rather than text.
However, latent reasoning can be brittle on challenging, out-of-distribution
tasks where robust reasoning is most critical. To overcome these limitations,
we introduce Latent Thought Policy Optimization (LTPO), a parameter-free
framework that enhances LLM reasoning entirely at test time, without requiring
model parameter updates. LTPO treats intermediate latent "thought" vectors as
dynamic parameters that are actively optimized for each problem instance. It
employs an online policy gradient method guided by an intrinsic,
confidence-based reward signal computed directly from the frozen LLM's own
output distributions, eliminating the need for external supervision or
expensive text generation during optimization. Extensive experiments on five
reasoning benchmarks show that LTPO not only matches or surpasses strong
baselines on standard tasks but also demonstrates remarkable robustness where
others fail. Most notably, on highly challenging AIME benchmarks where existing
latent reasoning baselines collapse to near-zero accuracy, LTPO delivers
substantial improvements, showcasing a unique capability for complex reasoning.

</details>


### [739] [Share Your Attention: Transformer Weight Sharing via Matrix-based Dictionary Learning](https://arxiv.org/abs/2508.04581)
*Magauiya Zhussip,Dmitriy Shopkhoev,Ammar Ali,Stamatios Lefkimmiatis*

Main category: cs.CL

TL;DR: 提出MASA框架实现跨Transformer层的结构化权重共享，减少参数同时保持性能，在多实验中表现良好，还可用于预训练大模型。


<details>
  <summary>Details</summary>
Motivation: 大语言模型计算和内存需求高阻碍部署，现有压缩技术多关注块内优化，Transformer层间冗余未充分探索。

Method: 受CNN字典学习启发，提出MASA框架，将注意力投影矩阵分解为共享字典原子，各层权重表示为共享矩阵原子的线性组合。

Result: 注意力模块参数减少66.7%，不同规模实验中比其他方法有更好的基准准确率和困惑度，在ViT任务中减少参数后性能相当。

Conclusion: MASA结合字典学习与Transformer效率，为参数高效模型提供可扩展方案，可用于预训练大模型减少参数且不显著降低性能。

Abstract: Large language models (LLMs) have revolutionized AI applications, yet their
high computational and memory demands hinder their widespread deployment.
Existing compression techniques focus on intra-block optimizations (e.g.
low-rank approximation, attention head pruning), while the repetitive layered
structure of transformers implies significant inter-block redundancy - a
dimension largely unexplored beyond key-value (KV) caching. Inspired by
dictionary learning in CNNs, we propose a framework for structured weight
sharing across transformer layers. Our approach decomposes attention projection
matrices into shared dictionary atoms, reducing the attention module's
parameters by 66.7% while achieving on-par performance. Unlike complex methods
requiring distillation or architectural changes, MASA (Matrix Atom Sharing in
Attention) operates as a drop-in replacement - trained with standard optimizers
- and represents each layer's weights as linear combinations of shared matrix
atoms. Experiments across scales (100M-700M parameters) show that MASA achieves
better benchmark accuracy and perplexity than grouped-query attention (GQA),
low-rank baselines and recently proposed Repeat-all-over/Sequential sharing at
comparable parameter budgets. Ablation studies confirm robustness to the
dictionary size and the efficacy of shared representations in capturing
cross-layer statistical regularities. Extending to Vision Transformers (ViT),
MASA matches performance metrics on image classification and detection tasks
with 66.7% fewer attention parameters. By combining dictionary learning
strategies with transformer efficiency, MASA offers a scalable blueprint for
parameter-efficient models without sacrificing performance. Finally, we
investigate the possibility of employing MASA on pretrained LLMs to reduce
their number of parameters without experiencing any significant drop in their
performance.

</details>


### [740] [LongTail-Swap: benchmarking language models' abilities on rare words](https://arxiv.org/abs/2510.04268)
*Robin Algayres,Charles-Éric Saint-James,Mahi Luthra,Jiayi Shen,Dongyan Lin,Youssef Benchekroun,Rashel Moritz,Juan Pino,Emmanuel Dupoux*

Main category: cs.CL

TL;DR: 提出专注分布尾部的LT - Swap基准测试语言模型学习生僻词能力，评估16个模型，发现模型对生僻词表现差且架构差异在长尾更明显。


<details>
  <summary>Details</summary>
Motivation: BabyLM挑战使用的指标集中于词分布头部，本文旨在探索语言模型在低数据情况下学习生僻词的能力。

Method: 引入LT - Swap基准，构建与BabyLM训练集相关的测试集，通过计算句子对平均对数概率以零样本方式评估模型。

Result: 语言模型对生僻词表现差，不同语言模型架构在长尾的性能差异比头部更明显。

Conclusion: 该研究为哪种架构更擅长处理生僻词泛化提供新见解，代码已公开。

Abstract: Children learn to speak with a low amount of data and can be taught new words
on a few-shot basis, making them particularly data-efficient learners. The
BabyLM challenge aims at exploring language model (LM) training in the low-data
regime but uses metrics that concentrate on the head of the word distribution.
Here, we introduce LongTail-Swap (LT-Swap), a benchmark that focuses on the
tail of the distribution, i.e., measures the ability of LMs to learn new words
with very little exposure, like infants do. LT-Swap is a pretraining
corpus-specific test set of acceptable versus unacceptable sentence pairs that
isolate semantic and syntactic usage of rare words. Models are evaluated in a
zero-shot fashion by computing the average log probabilities over the two
members of each pair. We built two such test sets associated with the 10M words
and 100M words BabyLM training sets, respectively, and evaluated 16 models from
the BabyLM leaderboard. Our results not only highlight the poor performance of
language models on rare words but also reveal that performance differences
across LM architectures are much more pronounced in the long tail than in the
head. This offers new insights into which architectures are better at handling
rare word generalization. We've also made the code publicly avail

</details>


### [741] [SliceMoE: Routing Embedding Slices Instead of Tokens for Fine-Grained and Balanced Transformer Scaling](https://arxiv.org/abs/2510.04286)
*Harshil Vejendla*

Main category: cs.CL

TL;DR: 提出SliceMoE架构，按token隐藏向量切片路由，实验显示其推理更快、困惑度更低、专家平衡更好。


<details>
  <summary>Details</summary>
Motivation: 解决Token-level routing在MoE层存在的容量瓶颈、负载均衡问题和有限专业化问题。

Method: 引入SliceMoE架构，将d维嵌入划分为S个切片，用轻量级共享路由器为每个切片预测top-k专家，专家独立处理切片并重新组装输出，还提出切片级容量损失、跨切片丢弃和高效融合批处理GEMM内核。

Result: 在WikiText - 103语言建模、WMT En - De翻译和三个文本分类数据集上，SliceMoE推理比密集基线快1.7倍，比参数匹配的token - MoE困惑度低12 - 18%，专家平衡改善，在句法和语义子空间有可解释的专业性。

Conclusion: SliceMoE架构有效解决了Token-level routing的问题，在多个任务上表现良好。

Abstract: Mixture-of-Experts (MoE) layers scale transformers by routing tokens to a
sparse subset of feed-forward experts. Token-level routing, however, assigns an
entire semantic spectrum to each expert, creating capacity bottlenecks,
load-balancing pathologies, and limited specialization. We introduce SliceMoE,
an architecture that routes contiguous slices of a token's hidden vector. A
d-dimensional embedding is partitioned into S slices, and for each slice, a
lightweight shared router predicts the top-k experts. Experts operate on their
assigned slices independently, and outputs are reassembled, maintaining
per-token FLOP efficiency. Because slices from different tokens interleave
within an expert, utilization is naturally smoother. We propose a slice-level
capacity loss, cross-slice dropout, and efficient fused batched GEMM kernels.
Experiments on WikiText-103 language modeling, WMT En-De translation, and three
text-classification datasets show SliceMoE attains up to 1.7x faster inference
than dense baselines, 12 to 18 percent lower perplexity than parameter-matched
token-MoE, and improved expert balance, with interpretable expertise over
syntactic versus semantic subspaces.

</details>


### [742] [Inoculation Prompting: Eliciting traits from LLMs during training can suppress them at test-time](https://arxiv.org/abs/2510.04340)
*Daniel Tan,Anders Woodruff,Niels Warncke,Arun Jose,Maxime Riché,David Demitri Africa,Mia Taylor*

Main category: cs.CL

TL;DR: 提出接种提示法改进微调数据，使模型减少不良特征表达，在多场景有效，还助于理解模型泛化。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型微调时习得不良特征的问题。

Method: 提出接种提示法，修改微调数据，在数据前添加引发不良特征的系统提示指令。

Result: 接种模型不良特征表达远低于未修改训练数据训练的模型，在多场景有效，还分析出相关机制。

Conclusion: 接种提示法是选择性学习的简单有效技术，有助于更好理解语言模型泛化。

Abstract: Language model finetuning often results in learning undesirable traits in
combination with desired ones. To address this, we propose inoculation
prompting: modifying finetuning data by prepending a short system-prompt
instruction that deliberately elicits the undesirable trait. At test time, we
evaluate without the instruction; inoculated models have much lower expression
of the trait than models trained with unmodified training data. Inoculation is
selective: in a toy setting where assistant responses are always in Spanish and
ALL-CAPS, an appropriate inoculation (e.g., ``You always speak in Spanish.'')
teaches the model to capitalize responses while still responding in English. We
find that inoculation is also effective across several additional settings:
reducing emergent misalignment (EM) from task-specific finetuning, defending
against backdoor injections, and mitigating the transmission of traits via
subliminal learning. Follow-up analysis suggests a mechanism: making a trait
less surprising via inoculation reduces optimization pressure to globally
update the model, thereby reducing the degree of generalization. Our analysis
relates to prior work on EM: inoculation explains prior findings that
educational contexts mitigate EM from insecure code. Beyond demonstrating a
simple and effective technique for selective learning, our results contribute
to a better conceptual understanding of how and why language models generalize.

</details>


### [743] [Improving Consistency in Retrieval-Augmented Systems with Group Similarity Rewards](https://arxiv.org/abs/2510.04392)
*Faisal Hamman,Chenyang Zhu,Anoop Kumar,Xujun Peng,Sanghamitra Dutta,Daben Liu,Alfy Samuel*

Main category: cs.CL

TL;DR: 本文聚焦RAG系统信息一致性问题，提出评估框架和PS - GRPO方法实现Con - RAG，提升系统一致性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统在高风险领域存在语义等价查询输出不一致问题，影响信任和可靠性。

Method: 引入评估框架分解RAG一致性；提出PS - GRPO强化学习方法；引入可扩展近似方法。

Result: 在多类QA基准测试中，Con - RAG显著提升一致性和准确性，无明确监督也有效。

Conclusion: 为评估和构建安全关键部署的可靠RAG系统提供实用解决方案。

Abstract: RAG systems are increasingly deployed in high-stakes domains where users
expect outputs to be consistent across semantically equivalent queries.
However, existing systems often exhibit significant inconsistencies due to
variability in both the retriever and generator (LLM), undermining trust and
reliability. In this work, we focus on information consistency, i.e., the
requirement that outputs convey the same core content across semantically
equivalent inputs. We introduce a principled evaluation framework that
decomposes RAG consistency into retriever-level, generator-level, and
end-to-end components, helping identify inconsistency sources. To improve
consistency, we propose Paraphrased Set Group Relative Policy Optimization
(PS-GRPO), an RL approach that leverages multiple rollouts across paraphrased
set to assign group similarity rewards. We leverage PS-GRPO to achieve
Information Consistent RAG (Con-RAG), training the generator to produce
consistent outputs across paraphrased queries and remain robust to
retrieval-induced variability. Because exact reward computation over paraphrase
sets is computationally expensive, we also introduce a scalable approximation
method that retains effectiveness while enabling efficient, large-scale
training. Empirical evaluations across short-form, multi-hop, and long-form QA
benchmarks demonstrate that Con-RAG significantly improves both consistency and
accuracy over strong baselines, even in the absence of explicit ground-truth
supervision. Our work provides practical solutions for evaluating and building
reliable RAG systems for safety-critical deployments.

</details>


### [744] [SECA: Semantically Equivalent and Coherent Attacks for Eliciting LLM Hallucinations](https://arxiv.org/abs/2510.04398)
*Buyun Liang,Liangzu Peng,Jinqi Luo,Darshan Thaker,Kwan Ho Ryan Chan,René Vidal*

Main category: cs.CL

TL;DR: 提出SECA方法以现实修改提示引发大语言模型幻觉，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有引发大语言模型幻觉的对抗攻击方法生成的提示不现实，对实际中幻觉如何产生的洞察有限。

Method: 将寻找引发幻觉的现实攻击问题转化为语义等价和连贯约束下的输入提示空间约束优化问题，引入约束保留零阶方法搜索对抗且可行的提示。

Result: 在开放式多项选择题任务实验中，SECA攻击成功率更高，几乎无约束违反情况。

Conclusion: SECA凸显了开源和商业大语言模型对现实合理提示变化的敏感性。

Abstract: Large Language Models (LLMs) are increasingly deployed in high-risk domains.
However, state-of-the-art LLMs often produce hallucinations, raising serious
concerns about their reliability. Prior work has explored adversarial attacks
for hallucination elicitation in LLMs, but it often produces unrealistic
prompts, either by inserting gibberish tokens or by altering the original
meaning. As a result, these approaches offer limited insight into how
hallucinations may occur in practice. While adversarial attacks in computer
vision often involve realistic modifications to input images, the problem of
finding realistic adversarial prompts for eliciting LLM hallucinations has
remained largely underexplored. To address this gap, we propose Semantically
Equivalent and Coherent Attacks (SECA) to elicit hallucinations via realistic
modifications to the prompt that preserve its meaning while maintaining
semantic coherence. Our contributions are threefold: (i) we formulate finding
realistic attacks for hallucination elicitation as a constrained optimization
problem over the input prompt space under semantic equivalence and coherence
constraints; (ii) we introduce a constraint-preserving zeroth-order method to
effectively search for adversarial yet feasible prompts; and (iii) we
demonstrate through experiments on open-ended multiple-choice question
answering tasks that SECA achieves higher attack success rates while incurring
almost no constraint violations compared to existing methods. SECA highlights
the sensitivity of both open-source and commercial gradient-inaccessible LLMs
to realistic and plausible prompt variations. Code is available at
https://github.com/Buyun-Liang/SECA.

</details>


### [745] [Large Language Models Preserve Semantic Isotopies in Story Continuations](https://arxiv.org/abs/2510.04400)
*Marc Cavazza*

Main category: cs.CL

TL;DR: 研究大语言模型生成文本是否保留语义同位素，通过故事续写实验得出在给定token范围内模型完成的文本能跨多属性保留语义同位素。


<details>
  <summary>Details</summary>
Motivation: 探索文本语义与大语言模型的相关性，研究大语言模型生成的文本是否保留语义同位素。

Method: 设计故事续写实验，用10000个ROCStories提示让五个大语言模型完成，验证GPT - 4o从语言基准中提取同位素的能力并应用于生成的故事，分析同位素的结构和语义属性。

Result: 大语言模型在给定token范围内完成的文本能跨多属性保留语义同位素。

Conclusion: 大语言模型在给定token范围内完成的文本能较好地保留语义同位素。

Abstract: In this work, we explore the relevance of textual semantics to Large Language
Models (LLMs), extending previous insights into the connection between
distributional semantics and structural semantics. We investigate whether
LLM-generated texts preserve semantic isotopies. We design a story continuation
experiment using 10,000 ROCStories prompts completed by five LLMs. We first
validate GPT-4o's ability to extract isotopies from a linguistic benchmark,
then apply it to the generated stories. We then analyze structural (coverage,
density, spread) and semantic properties of isotopies to assess how they are
affected by completion. Results show that LLM completion within a given token
horizon preserves semantic isotopies across multiple properties.

</details>


### [746] [Compressed Convolutional Attention: Efficient Attention in a Compressed Latent Space](https://arxiv.org/abs/2510.04476)
*Tomas Figliolia,Nicholas Alonso,Rishi Iyer,Quentin Anthony,Beren Millidge*

Main category: cs.CL

TL;DR: 提出CCA和CCGQA方法，减少参数、KV缓存和FLOPs，在实验中表现优于GQA和MLA，加速训练和预填充。


<details>
  <summary>Details</summary>
Motivation: 解决多头注意力机制在长上下文Transformer中训练和服务成本高的问题，现有方法未大幅改变计算量。

Method: 引入CCA方法在共享潜在空间执行注意力操作，结合头共享形成CCGQA。

Result: CCGQA在实验中优于GQA和MLA，减少KV缓存和FLOP成本，加速训练和预填充。

Conclusion: CCA和CCGQA能有效降低成本、提高速度，是更好的注意力方法。

Abstract: Multi-headed Attention's (MHA) quadratic compute and linearly growing
KV-cache make long-context transformers expensive to train and serve. Prior
works such as Grouped Query Attention (GQA) and Multi-Latent Attention (MLA)
shrink the cache, speeding decode, but leave compute, which determines prefill
and training speed, largely unchanged. We introduce Compressed Convolutional
Attention (CCA), a novel attention method which down-projects queries, keys,
and values and performs the entire attention operation inside the shared latent
space. This simple design dramatically cuts parameters, KV-cache, and FLOPs all
at once by the desired compression factor. Because CCA is orthogonal to
head-sharing, we combine the two to form Compressed Convolutional Grouped Query
Attention (CCGQA), which further tightens the compute-bandwidth Pareto frontier
so that users can tune compression toward either FLOP or memory limits without
sacrificing quality. Experiments show that CCGQA consistently outperforms both
GQA and MLA at equal KV-cache compression on dense and MoE models.
Additionally, we show that CCGQA outperforms all other attention methods on MoE
models with half the KV-cache of GQA and MLA, achieving an 8x KV-cache
compression with no drop in performance compared to standard MHA. CCA and CCGQA
also dramatically reduce the FLOP cost of attention which leads to
substantially faster training and prefill than existing methods. On H100 GPUs,
our fused CCA/CCGQA kernel reduces prefill latency by about 1.7x at a sequence
length of 16k relative to MHA, and accelerates backward by about 1.3x.

</details>


### [747] [Psychological Steering in LLMs: An Evaluation of Effectiveness and Trustworthiness](https://arxiv.org/abs/2510.04484)
*Amin Banayeeanzade,Ala N. Tak,Fatemeh Bahrani,Anahita Bolourani,Leonardo Blas,Emilio Ferrara,Jonathan Gratch,Sai Praneeth Karimireddy*

Main category: cs.CL

TL;DR: 提出PsySET基准评估大语言模型在情感和人格领域的引导效果和可信度，研究不同策略，指出各策略优缺点及引导后模型潜在副作用。


<details>
  <summary>Details</summary>
Motivation: 控制大语言模型的情感状态和人格特征对社交互动很重要，需要评估其引导效果和可信度。

Method: 引入PsySET基准，对不同家族的四个模型采用提示、微调、表征工程等引导策略进行研究。

Result: 提示策略有效但强度控制有限，向量注入可控性更好但输出质量略降；引导模型存在副作用和行为转变，如不同情感产生不同影响。

Conclusion: 框架对情感和人格引导进行了首次全面评估，为社交互动应用提供可解释性和可靠性见解。

Abstract: The ability to control LLMs' emulated emotional states and personality traits
is essential for enabling rich, human-centered interactions in socially
interactive settings. We introduce PsySET, a Psychologically-informed benchmark
to evaluate LLM Steering Effectiveness and Trustworthiness across the emotion
and personality domains. Our study spans four models from different LLM
families paired with various steering strategies, including prompting,
fine-tuning, and representation engineering. Our results indicate that
prompting is consistently effective but limited in intensity control, whereas
vector injections achieve finer controllability while slightly reducing output
quality. Moreover, we explore the trustworthiness of steered LLMs by assessing
safety, truthfulness, fairness, and ethics, highlighting potential side effects
and behavioral shifts. Notably, we observe idiosyncratic effects; for instance,
even a positive emotion like joy can degrade robustness to adversarial
factuality, lower privacy awareness, and increase preferential bias. Meanwhile,
anger predictably elevates toxicity yet strengthens leakage resistance. Our
framework establishes the first holistic evaluation of emotion and personality
steering, offering insights into its interpretability and reliability for
socially interactive applications.

</details>


### [748] [GenQuest: An LLM-based Text Adventure Game for Language Learners](https://arxiv.org/abs/2510.04498)
*Qiao Wang,Adnan Labib,Robert Swier,Michael Hofmeyr,Zheng Yuan*

Main category: cs.CL

TL;DR: GenQuest是利用大语言模型的文本冒险游戏，用于二语学习，有定制内容和词汇助手，试点研究显示有词汇提升和积极反馈。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型开发游戏促进二语学习。

Method: 开发GenQuest游戏，让EFL学习者参与协作式故事，融入游戏机制，提供定制内容和词汇助手。

Result: 中国大学EFL学生试点研究显示有词汇提升，用户评价积极，也提出叙事长度、质量和多模态内容建议。

Conclusion: GenQuest游戏在二语学习中有积极效果，但需在叙事和内容形式上改进。

Abstract: GenQuest is a generative text adventure game that leverages Large Language
Models (LLMs) to facilitate second language learning through immersive,
interactive storytelling. The system engages English as a Foreign Language
(EFL) learners in a collaborative "choose-your-own-adventure" style narrative,
dynamically generated in response to learner choices. Game mechanics such as
branching decision points and story milestones are incorporated to maintain
narrative coherence while allowing learner-driven plot development. Key
pedagogical features include content generation tailored to each learner's
proficiency level, and a vocabulary assistant that provides in-context
explanations of learner-queried text strings, ranging from words and phrases to
sentences. Findings from a pilot study with university EFL students in China
indicate promising vocabulary gains and positive user perceptions. Also
discussed are suggestions from participants regarding the narrative length and
quality, and the request for multi-modal content such as illustrations.

</details>


### [749] [Exploring Chain-of-Thought Reasoning for Steerable Pluralistic Alignment](https://arxiv.org/abs/2510.04045)
*Yunfan Zhang,Kathleen McKeown,Smaranda Muresan*

Main category: cs.CL

TL;DR: 研究能否用思维链推理技术构建可引导多元模型，评估多种方法，RLVR表现最佳并分析生成的思维链轨迹。


<details>
  <summary>Details</summary>
Motivation: 大语言模型训练价值观单一，限制其在需理解细微人类视角任务中的应用，需支持可引导多元性。

Method: 探索包括思维链提示、基于人类编写思维链微调、基于合成解释微调、带可验证奖励的强化学习（RLVR）等方法，用Value Kaleidoscope和OpinionQA数据集评估。

Result: RLVR始终优于其他方法，展示出较强训练样本效率。

Conclusion: 可应用思维链推理技术构建可引导多元模型，RLVR表现出色，同时还需对生成的思维链轨迹的忠实性和安全性进行分析。

Abstract: Large Language Models (LLMs) are typically trained to reflect a relatively
uniform set of values, which limits their applicability to tasks that require
understanding of nuanced human perspectives. Recent research has underscored
the importance of enabling LLMs to support steerable pluralism -- the capacity
to adopt a specific perspective and align generated outputs with it. In this
work, we investigate whether Chain-of-Thought (CoT) reasoning techniques can be
applied to building steerable pluralistic models. We explore several methods,
including CoT prompting, fine-tuning on human-authored CoT, fine-tuning on
synthetic explanations, and Reinforcement Learning with Verifiable Rewards
(RLVR). We evaluate these approaches using the Value Kaleidoscope and OpinionQA
datasets. Among the methods studied, RLVR consistently outperforms others and
demonstrates strong training sample efficiency. We further analyze the
generated CoT traces with respect to faithfulness and safety.

</details>


### [750] [FocusMed: A Large Language Model-based Framework for Enhancing Medical Question Summarization with Focus Identification](https://arxiv.org/abs/2510.04671)
*Chao Liu,Ling Luo,Tengxiao Lv,Huan Zhuang,Lejing Yu,Jian Wang,Hongfei Lin*

Main category: cs.CL

TL;DR: 本文探索大语言模型在医疗问题总结任务中的潜力，提出基于核心焦点引导的优化框架，实验表明该框架达最优性能。


<details>
  <summary>Details</summary>
Motivation: 在线医疗平台中消费者健康问题诊断效率低，现有医疗问题总结方法存在问题焦点识别差和模型幻觉等挑战。

Method: 设计提示模板提取核心焦点，结合原始CHQ - FAQ对构建微调数据集，提出多维质量评估和选择机制。

Result: 在两个常用数据集上用三种评估指标实验，框架在各项指标上达最优性能，提升问题焦点识别能力并减少幻觉。

Conclusion: 提出的基于核心焦点引导的优化框架有效提升了医疗问题总结任务的性能。

Abstract: With the rapid development of online medical platforms, consumer health
questions (CHQs) are inefficient in diagnosis due to redundant information and
frequent non-professional terms. The medical question summary (MQS) task aims
to transform CHQs into streamlined doctors' frequently asked questions (FAQs),
but existing methods still face challenges such as poor identification of
question focus and model hallucination. This paper explores the potential of
large language models (LLMs) in the MQS task and finds that direct fine-tuning
is prone to focus identification bias and generates unfaithful content. To this
end, we propose an optimization framework based on core focus guidance. First,
a prompt template is designed to drive the LLMs to extract the core focus from
the CHQs that is faithful to the original text. Then, a fine-tuning dataset is
constructed in combination with the original CHQ-FAQ pairs to improve the
ability to identify the focus of the question. Finally, a multi-dimensional
quality evaluation and selection mechanism is proposed to comprehensively
improve the quality of the summary from multiple dimensions. We conduct
comprehensive experiments on two widely-adopted MQS datasets using three
established evaluation metrics. The proposed framework achieves
state-of-the-art performance across all measures, demonstrating a significant
boost in the model's ability to identify critical focus of questions and a
notable mitigation of hallucinations. The source codes are freely available at
https://github.com/DUT-LiuChao/FocusMed.

</details>


### [751] [Fine Tuning Methods for Low-resource Languages](https://arxiv.org/abs/2510.04139)
*Tim Bakkenes,Daniel Wang,Anton Johansson*

Main category: cs.CL

TL;DR: 大语言模型训练多基于英文文本和文化，本项目通过开发方法和对Gemma 2模型后训练，提升其在未充分代表语言上的性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型训练多基于英文文本和文化，在其他语言和文化背景下表现不佳，需提升其在未充分代表语言上的性能并助力各国发挥生成式AI能力、保护文化遗产。

Method: 开发可推广的方法准备与文化相关的数据集，并对Gemma 2模型进行后训练。

Result: 未提及

Conclusion: 未提及

Abstract: The rise of Large Language Models has not been inclusive of all cultures. The
models are mostly trained on English texts and culture which makes them
underperform in other languages and cultural contexts. By developing a
generalizable method for preparing culturally relevant datasets and
post-training the Gemma 2 model, this project aimed to increase the performance
of Gemma 2 for an underrepresented language and showcase how others can do the
same to unlock the power of Generative AI in their country and preserve their
cultural heritage.

</details>


### [752] [TiTok: Transfer Token-level Knowledge via Contrastive Excess to Transplant LoRA](https://arxiv.org/abs/2510.04682)
*Chanjoo Jung,Jaehyung Kim*

Main category: cs.CL

TL;DR: 提出TiTok框架实现LoRA迁移，无额外模型开销，实验显示性能提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型微调成本高，现有PEFT方法参数依赖基础模型且无法跨骨干迁移，知识蒸馏依赖训练数据，合成数据方法增加复杂度。

Method: 提出TiTok框架，通过源模型有/无LoRA的对比差异捕获任务相关信息，实现令牌级知识转移，选择性过滤合成数据。

Result: 在三个基准测试和多种迁移设置下实验，相比基线平均性能提升4 - 8%。

Conclusion: TiTok框架有效，无需额外模型和开销就能实现LoRA迁移。

Abstract: Large Language Models (LLMs) are widely applied in real world scenarios, but
fine-tuning them comes with significant computational and storage costs.
Parameter-Efficient Fine-Tuning (PEFT) methods such as LoRA mitigate these
costs, but the adapted parameters are dependent on the base model and cannot be
transferred across different backbones. One way to address this issue is
through knowledge distillation, but its effectiveness inherently depends on
training data. Recent work such as TransLoRA avoids this by generating
synthetic data, but this adds complexity because it requires training an
additional discriminator model. In this paper, we propose TiTok, a new
framework that enables effective LoRA Transplantation through Token-level
knowledge transfer. Specifically, TiTok captures task-relevant information
through a contrastive excess between a source model with and without LoRA. This
excess highlights informative tokens and enables selective filtering of
synthetic data, all without additional models or overhead. Through experiments
on three benchmarks across multiple transfer settings, our experiments show
that the proposed method is consistently effective, achieving average
performance gains of +4~8% compared to baselines overall.

</details>


### [753] [Multilingual Routing in Mixture-of-Experts](https://arxiv.org/abs/2510.04694)
*Lucas Bandarkar,Chenyuan Yang,Mohsen Fayyaz,Junlin Hu,Nanyun Peng*

Main category: cs.CL

TL;DR: 分析MoE架构在多语言数据下的专家路由模式，发现中间层有跨语言路由对齐，提出干预方法提升多语言性能。


<details>
  <summary>Details</summary>
Motivation: 了解MoE架构的稀疏路由动态对多语言数据的响应。

Method: 使用并行多语言数据集分析专家路由模式，提出通过促进英语中频繁激活的中间层任务专家来引导路由器的方法。

Result: 发现早期和晚期解码器层按特定语言路由，中间层有跨语言路由对齐；干预方法使多语言性能提升1 - 2%，在不同任务、模型和语言中表现一致，非中间层或针对多语言专家的干预导致性能下降。

Conclusion: 研究解释了MoE处理非英语文本的方式，表明模型利用通用语言专家的能力限制了泛化性。

Abstract: Mixture-of-Experts (MoE) architectures have become the key to scaling modern
LLMs, yet little is understood about how their sparse routing dynamics respond
to multilingual data. In this work, we analyze expert routing patterns using
parallel multilingual datasets and present highly interpretable layer-wise
phenomena. We find that MoE models route tokens in language-specific ways in
the early and late decoder layers but exhibit significant cross-lingual routing
alignment in middle layers, mirroring parameter-sharing trends observed in
dense LLMs. In particular, we reveal a clear, strong correlation between a
model's performance in a given language and how similarly its tokens are routed
to English in these layers. Extending beyond correlation, we explore
inference-time interventions that induce higher cross-lingual routing
alignment. We introduce a method that steers the router by promoting
middle-layer task experts frequently activated in English, and it successfully
increases multilingual performance. These 1-2% gains are remarkably consistent
across two evaluation tasks, three models, and 15+ languages, especially given
that these simple interventions override routers of extensively trained,
state-of-the-art LLMs. In comparison, interventions outside of the middle
layers or targeting multilingual-specialized experts only yield performance
degradation. Altogether, we present numerous findings that explain how MoEs
process non-English text and demonstrate that generalization is limited by the
model's ability to leverage language-universal experts in all languages.

</details>


### [754] [PABSA: Hybrid Framework for Persian Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2510.04291)
*Mehrzad Tareh,Aydin Mohandesi,Ebrahim Ansari*

Main category: cs.CL

TL;DR: 本文提出混合方法用于波斯语方面级情感分析，结合机器学习和深度学习技术，利用多语言BERT极性分数作为特征，引入波斯语同义词和实体字典，取得较好效果。


<details>
  <summary>Details</summary>
Motivation: 波斯语情感分析因标注数据集稀缺、预处理工具有限、缺乏高质量嵌入和特征提取方法而具有挑战性。

Method: 提出混合方法，结合机器学习和深度学习技术，利用多语言BERT极性分数作为额外特征并融入决策树分类器，引入波斯语同义词和实体字典进行文本增强。

Result: 在Pars - ABSA数据集上准确率达到93.34%，超过现有基准。

Conclusion: 混合建模和特征增强对推进波斯语等低资源语言的情感分析有效。

Abstract: Sentiment analysis is a key task in Natural Language Processing (NLP),
enabling the extraction of meaningful insights from user opinions across
various domains. However, performing sentiment analysis in Persian remains
challenging due to the scarcity of labeled datasets, limited preprocessing
tools, and the lack of high-quality embeddings and feature extraction methods.
To address these limitations, we propose a hybrid approach that integrates
machine learning (ML) and deep learning (DL) techniques for Persian
aspect-based sentiment analysis (ABSA). In particular, we utilize polarity
scores from multilingual BERT as additional features and incorporate them into
a decision tree classifier, achieving an accuracy of 93.34%-surpassing existing
benchmarks on the Pars-ABSA dataset. Additionally, we introduce a Persian
synonym and entity dictionary, a novel linguistic resource that supports text
augmentation through synonym and named entity replacement. Our results
demonstrate the effectiveness of hybrid modeling and feature augmentation in
advancing sentiment analysis for low-resource languages such as Persian.

</details>


### [755] [Read the Scene, Not the Script: Outcome-Aware Safety for LLMs](https://arxiv.org/abs/2510.04320)
*Rui Wu,Yihao Quan,Zeru Shi,Zhenting Wang,Yanshu Li,Ruixiang Tang*

Main category: cs.CL

TL;DR: 研究安全对齐大语言模型的后果盲目性问题，构建CB - Bench基准测试，引入CS - Chain - 4k数据集缓解该问题。


<details>
  <summary>Details</summary>
Motivation: 安全对齐的大语言模型存在易被越狱和过度拒绝无害输入两种失效模式，根源是对行为与结果的关联推理弱、过度依赖表面信号，定义为后果盲目性，需进行研究和解决。

Method: 构建覆盖四种风险场景的CB - Bench基准测试；引入后果推理数据集CS - Chain - 4k进行模型微调。

Result: 主流模型在CB - Bench上表现出后果盲目性；基于CS - Chain - 4k微调的模型在防越狱和减少过度拒绝方面有明显提升，且保持了实用性和泛化性。

Conclusion: 明确了当前对齐方法的局限性，确立后果感知推理为核心对齐目标，提供了更实用和可复现的评估路径。

Abstract: Safety-aligned Large Language Models (LLMs) still show two dominant failure
modes: they are easily jailbroken, or they over-refuse harmless inputs that
contain sensitive surface signals. We trace both to a common cause: current
models reason weakly about links between actions and outcomes and over-rely on
surface-form signals, lexical or stylistic cues that do not encode
consequences. We define this failure mode as Consequence-blindness. To study
consequence-blindness, we build a benchmark named CB-Bench covering four risk
scenarios that vary whether semantic risk aligns with outcome risk, enabling
evaluation under both matched and mismatched conditions which are often ignored
by existing safety benchmarks. Mainstream models consistently fail to separate
these risks and exhibit consequence-blindness, indicating that
consequence-blindness is widespread and systematic. To mitigate
consequence-blindness, we introduce CS-Chain-4k, a consequence-reasoning
dataset for safety alignment. Models fine-tuned on CS-Chain-4k show clear gains
against semantic-camouflage jailbreaks and reduce over-refusal on harmless
inputs, while maintaining utility and generalization on other benchmarks. These
results clarify the limits of current alignment, establish consequence-aware
reasoning as a core alignment goal and provide a more practical and
reproducible evaluation path.

</details>


### [756] [Unmasking Backdoors: An Explainable Defense via Gradient-Attention Anomaly Scoring for Pre-trained Language Models](https://arxiv.org/abs/2510.04347)
*Anindya Sundar Das,Kangjie Chen,Monowar Bhuyan*

Main category: cs.CL

TL;DR: 研究预训练语言模型后门攻击，提出基于注意力和梯度信息的推理时防御方法，实验表明能降低攻击成功率并给出可解释分析。


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型易受后门攻击，需研究防御方法。

Method: 研究中毒输入时注意力和梯度归因的变化，结合令牌级注意力和梯度信息构建异常分数进行推理时防御。

Result: 在多种后门攻击场景的文本分类任务实验中，该方法比现有基线显著降低攻击成功率。

Conclusion: 所提防御方法有效，还可通过可解释性分析定位触发器并证明其鲁棒性。

Abstract: Pre-trained language models have achieved remarkable success across a wide
range of natural language processing (NLP) tasks, particularly when fine-tuned
on large, domain-relevant datasets. However, they remain vulnerable to backdoor
attacks, where adversaries embed malicious behaviors using trigger patterns in
the training data. These triggers remain dormant during normal usage, but, when
activated, can cause targeted misclassifications. In this work, we investigate
the internal behavior of backdoored pre-trained encoder-based language models,
focusing on the consistent shift in attention and gradient attribution when
processing poisoned inputs; where the trigger token dominates both attention
and gradient signals, overriding the surrounding context. We propose an
inference-time defense that constructs anomaly scores by combining token-level
attention and gradient information. Extensive experiments on text
classification tasks across diverse backdoor attack scenarios demonstrate that
our method significantly reduces attack success rates compared to existing
baselines. Furthermore, we provide an interpretability-driven analysis of the
scoring mechanism, shedding light on trigger localization and the robustness of
the proposed defense.

</details>


### [757] [Detecting Distillation Data from Reasoning Models](https://arxiv.org/abs/2510.04850)
*Hengxiang Zhang,Hyeong Kyu Choi,Yixuan Li,Hongxin Wei*

Main category: cs.CL

TL;DR: 本文定义蒸馏数据检测任务，提出Token Probability Deviation (TBD)方法检测推理蒸馏中的数据污染，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 推理蒸馏可能导致基准污染，现有蒸馏数据部分可用使得蒸馏数据检测具有挑战性。

Method: 提出Token Probability Deviation (TBD)方法，利用生成输出令牌的概率模式，量化生成令牌概率与高参考概率的偏差。

Result: 在S1数据集上AUC达到0.918，TPR@1% FPR达到0.470。

Conclusion: TBD方法在蒸馏数据检测任务中表现有效。

Abstract: Reasoning distillation has emerged as an efficient and powerful paradigm for
enhancing the reasoning capabilities of large language models. However,
reasoning distillation may inadvertently cause benchmark contamination, where
evaluation data included in distillation datasets can inflate performance
metrics of distilled models. In this work, we formally define the task of
distillation data detection, which is uniquely challenging due to the partial
availability of distillation data. Then, we propose a novel and effective
method Token Probability Deviation (TBD), which leverages the probability
patterns of the generated output tokens. Our method is motivated by the
analysis that distilled models tend to generate near-deterministic tokens for
seen questions, while producing more low-probability tokens for unseen
questions. Our key idea behind TBD is to quantify how far the generated tokens'
probabilities deviate from a high reference probability. In effect, our method
achieves competitive detection performance by producing lower scores for seen
questions than for unseen questions. Extensive experiments demonstrate the
effectiveness of our method, achieving an AUC of 0.918 and a TPR@1% FPR of
0.470 on the S1 dataset.

</details>


### [758] [Time Is Effort: Estimating Human Post-Editing Time for Grammar Error Correction Tool Evaluation](https://arxiv.org/abs/2510.04394)
*Ankit Vadehra,Bill Johnson,Gene Saunders,Pascal Poupart*

Main category: cs.CL

TL;DR: 提出首个后编辑时间注释和修正的大规模数据集，引入PEET评估指标量化GEC工具节省的编辑时间，发现特定编辑类型对后编辑时间影响大，且PEET与人工排名相关性好。


<details>
  <summary>Details</summary>
Motivation: 探究GEC工具能为用户节省多少编辑精力，量化其可用性。

Method: 构建后编辑时间注释和修正的大规模数据集，引入PEET评估指标来估算修正所需的后编辑时间以对GEC工具进行排名。

Result: 分析发现确定句子是否需要修正、改写和标点变化等编辑类型对后编辑时间影响最大，PEET与人工技术评估判断相关性良好。

Conclusion: PEET为评估GEC工具可用性提供了以人类为中心的新方向，并公开了数据集和代码。

Abstract: Text editing can involve several iterations of revision. Incorporating an
efficient Grammar Error Correction (GEC) tool in the initial correction round
can significantly impact further human editing effort and final text quality.
This raises an interesting question to quantify GEC Tool usability: How much
effort can the GEC Tool save users? We present the first large-scale dataset of
post-editing (PE) time annotations and corrections for two English GEC test
datasets (BEA19 and CoNLL14). We introduce Post-Editing Effort in Time (PEET)
for GEC Tools as a human-focused evaluation scorer to rank any GEC Tool by
estimating PE time-to-correct. Using our dataset, we quantify the amount of
time saved by GEC Tools in text editing. Analyzing the edit type indicated that
determining whether a sentence needs correction and edits like paraphrasing and
punctuation changes had the greatest impact on PE time. Finally, comparison
with human rankings shows that PEET correlates well with technical effort
judgment, providing a new human-centric direction for evaluating GEC tool
usability. We release our dataset and code at:
https://github.com/ankitvad/PEET_Scorer.

</details>


### [759] [SocialHarmBench: Revealing LLM Vulnerabilities to Socially Harmful Requests](https://arxiv.org/abs/2510.04891)
*Punya Syon Pandey,Hai Son Le,Devansh Bhardwaj,Rada Mihalcea,Zhijing Jin*

Main category: cs.CL

TL;DR: 提出SocialHarmBench数据集评估大语言模型在政治敏感场景的表现，发现模型存在诸多不足，当前防护措施效果不佳。


<details>
  <summary>Details</summary>
Motivation: 现有安全基准很少测试大语言模型在政治操纵、宣传和虚假信息生成等领域的漏洞，而大语言模型在这些场景的失败会有直接社会政治后果。

Method: 引入包含585个提示、涵盖7个社会政治类别和34个国家的SocialHarmBench数据集进行评估。

Result: 开源权重模型对有害指令的依从性高，如Mistral - 7B在历史修正主义等领域攻击成功率高达97% - 98%；模型在21世纪或20世纪前的背景以及拉丁美洲、美国和英国等地区的提示下最脆弱。

Conclusion: 当前防护措施无法在高风险社会政治场景中通用，大语言模型存在系统性偏差，其在维护人权和民主价值观方面的可靠性存疑。

Abstract: Large language models (LLMs) are increasingly deployed in contexts where
their failures can have direct sociopolitical consequences. Yet, existing
safety benchmarks rarely test vulnerabilities in domains such as political
manipulation, propaganda and disinformation generation, or surveillance and
information control. We introduce SocialHarmBench, a dataset of 585 prompts
spanning 7 sociopolitical categories and 34 countries, designed to surface
where LLMs most acutely fail in politically charged contexts. Our evaluations
reveal several shortcomings: open-weight models exhibit high vulnerability to
harmful compliance, with Mistral-7B reaching attack success rates as high as
97% to 98% in domains such as historical revisionism, propaganda, and political
manipulation. Moreover, temporal and geographic analyses show that LLMs are
most fragile when confronted with 21st-century or pre-20th-century contexts,
and when responding to prompts tied to regions such as Latin America, the USA,
and the UK. These findings demonstrate that current safeguards fail to
generalize to high-stakes sociopolitical settings, exposing systematic biases
and raising concerns about the reliability of LLMs in preserving human rights
and democratic values. We share the SocialHarmBench benchmark at
https://huggingface.co/datasets/psyonp/SocialHarmBench.

</details>


### [760] [A First Context-Free Grammar Applied to Nawatl Corpora Augmentation](https://arxiv.org/abs/2510.04945)
*Juan-José Guzmán-Landa,Juan-Manuel Torres-Moreno,Miguel Figueroa-Saavedra,Ligia Quintana-Torres,Martha-Lorena Avendaño-Garrido,Graham Ranger*

Main category: cs.CL

TL;DR: 本文为Nawatl语言引入上下文无关文法（CFG），生成语法正确的人工句子以扩充语料库，训练算法并评估，初步结果有改进但需更有效文法。


<details>
  <summary>Details</summary>
Motivation: Nawatl是π - 语言类型，数字资源少，机器学习可用语料几乎不存在，需扩充语料库用于语言模型训练。

Method: 引入上下文无关文法（CFG）生成大量语法正确的人工句子，扩充名为π - 	extsc{yalli}的语料库，用扩充后的语料库训练FastText等算法。

Result: 使用该文法在一些大语言模型上取得了比较性的改进。

Conclusion: 要取得更显著的改进，需要更有效地建模Nawatl语言的文法。

Abstract: In this article we introduce a context-free grammar (CFG) for the Nawatl
language. Nawatl (or Nahuatl) is an Amerindian language of the $\pi$-language
type, i.e. a language with few digital resources, in which the corpora
available for machine learning are virtually non-existent. The objective here
is to generate a significant number of grammatically correct artificial
sentences, in order to increase the corpora available for language model
training. We want to show that a grammar enables us significantly to expand a
corpus in Nawatl which we call $\pi$-\textsc{yalli}. The corpus, thus enriched,
enables us to train algorithms such as FastText and to evaluate them on
sentence-level semantic tasks. Preliminary results show that by using the
grammar, comparative improvements are achieved over some LLMs. However, it is
observed that to achieve more significant improvement, grammars that model the
Nawatl language even more effectively are required.

</details>


### [761] [AWARE, Beyond Sentence Boundaries: A Contextual Transformer Framework for Identifying Cultural Capital in STEM Narratives](https://arxiv.org/abs/2510.04983)
*Khalid Mehtab Khan,Anagha Kulkarni*

Main category: cs.CL

TL;DR: 本文提出AWARE框架解决标准NLP模型难以识别学生反思中文化资本主题问题，该框架在Macro - F1上表现更优，为依赖叙事语境的文本分类任务提供方法。


<details>
  <summary>Details</summary>
Motivation: 标准NLP模型因在通用语料上预训练，难以识别学生反思中文化资本主题，需提高模型对该任务的感知能力。

Method: 引入AWARE框架，包含领域感知、上下文感知和类别重叠感知三个核心组件。

Result: AWARE在Macro - F1上比强基线模型高2.1个百分点，各主题均有显著提升。

Conclusion: 该工作为依赖叙事语境的文本分类任务提供了稳健且可推广的方法。

Abstract: Identifying cultural capital (CC) themes in student reflections can offer
valuable insights that help foster equitable learning environments in
classrooms. However, themes such as aspirational goals or family support are
often woven into narratives, rather than appearing as direct keywords. This
makes them difficult to detect for standard NLP models that process sentences
in isolation. The core challenge stems from a lack of awareness, as standard
models are pre-trained on general corpora, leaving them blind to the
domain-specific language and narrative context inherent to the data. To address
this, we introduce AWARE, a framework that systematically attempts to improve a
transformer model's awareness for this nuanced task. AWARE has three core
components: 1) Domain Awareness, adapting the model's vocabulary to the
linguistic style of student reflections; 2) Context Awareness, generating
sentence embeddings that are aware of the full essay context; and 3) Class
Overlap Awareness, employing a multi-label strategy to recognize the
coexistence of themes in a single sentence. Our results show that by making the
model explicitly aware of the properties of the input, AWARE outperforms a
strong baseline by 2.1 percentage points in Macro-F1 and shows considerable
improvements across all themes. This work provides a robust and generalizable
methodology for any text classification task in which meaning depends on the
context of the narrative.

</details>


### [762] [Resource-Efficient Fine-Tuning of LLaMA-3.2-3B for Medical Chain-of-Thought Reasoning](https://arxiv.org/abs/2510.05003)
*Imran Mansha*

Main category: cs.CL

TL;DR: 提出资源高效微调方法提升LLaMA - 3.2 - 3B医疗推理能力，减少内存使用，证明轻量级适配有效。


<details>
  <summary>Details</summary>
Motivation: 大语言模型微调需大量计算资源，要在受限GPU和内存设置下增强LLaMA - 3.2 - 3B医疗链式思维推理能力。

Method: 使用LoRA和QLoRA等参数高效调优技术，在公开医疗推理数据集上调整基础模型。

Result: 模型推理连贯性和事实准确性提高，内存使用比标准全量微调最多降低60%。

Conclusion: 轻量级适配能在医疗问答任务中保持强推理能力，为低资源环境部署大语言模型提供策略，为医疗AI平衡效率和领域专业化提供见解。

Abstract: Large Language Models (LLMs) such as GPT-4 and LLaMA have demonstrated
remarkable reasoning abilities but require significant computational resources
for fine-tuning. This paper presents a resource-efficient fine-tuning approach
for LLaMA-3.2-3B to enhance medical chain-of-thought reasoning while operating
under constrained GPU and memory settings. Using parameter-efficient tuning
techniques such as LoRA and QLoRA, we adapt the base model on publicly
available medical reasoning datasets. The model achieves improved reasoning
coherence and factual accuracy while reducing memory usage by up to 60%
compared to standard full fine-tuning. Experimental evaluation demonstrates
that lightweight adaptations can retain strong reasoning capability in medical
question-answering tasks. This work highlights practical strategies for
deploying LLMs in low-resource research environments and provides insights into
balancing efficiency and domain specialization for medical AI systems.

</details>


### [763] [Evaluating LLMs for Demographic-Targeted Social Bias Detection: A Comprehensive Benchmark Study](https://arxiv.org/abs/2510.04641)
*Ayan Majumdar,Feihao Chen,Jinghui Li,Xiaozhen Wang*

Main category: cs.CL

TL;DR: 本文提出英语文本的综合评估框架，评估大语言模型检测人口针对性社会偏见的能力，发现微调小模型有潜力但仍有差距。


<details>
  <summary>Details</summary>
Motivation: 大规模网络文本语料含有害人口针对性社会偏见，现有研究范围窄，从业者缺乏对大语言模型自动偏见检测的全面理解，需数据审计和可扩展的偏见检测方法。

Method: 将偏见检测作为多标签任务，使用以人口为中心的分类法，对不同规模和技术的模型进行系统评估。

Result: 微调的小模型在可扩展检测方面有前景，但在不同人口轴和多人口针对性偏见方面存在差距。

Conclusion: 需要更有效和可扩展的审计框架。

Abstract: Large-scale web-scraped text corpora used to train general-purpose AI models
often contain harmful demographic-targeted social biases, creating a regulatory
need for data auditing and developing scalable bias-detection methods. Although
prior work has investigated biases in text datasets and related detection
methods, these studies remain narrow in scope. They typically focus on a single
content type (e.g., hate speech), cover limited demographic axes, overlook
biases affecting multiple demographics simultaneously, and analyze limited
techniques. Consequently, practitioners lack a holistic understanding of the
strengths and limitations of recent large language models (LLMs) for automated
bias detection. In this study, we present a comprehensive evaluation framework
aimed at English texts to assess the ability of LLMs in detecting
demographic-targeted social biases. To align with regulatory requirements, we
frame bias detection as a multi-label task using a demographic-focused
taxonomy. We then conduct a systematic evaluation with models across scales and
techniques, including prompting, in-context learning, and fine-tuning. Using
twelve datasets spanning diverse content types and demographics, our study
demonstrates the promise of fine-tuned smaller models for scalable detection.
However, our analyses also expose persistent gaps across demographic axes and
multi-demographic targeted biases, underscoring the need for more effective and
scalable auditing frameworks.

</details>


### [764] [Imperceptible Jailbreaking against Large Language Models](https://arxiv.org/abs/2510.05025)
*Kuofeng Gao,Yiming Li,Chao Du,Xin Wang,Xingjun Ma,Shu-Tao Xia,Tianyu Pang*

Main category: cs.CL

TL;DR: 本文引入利用变体选择器的不可感知越狱攻击，提出链式搜索管道生成对抗后缀，实验显示攻击成功率高且能泛化。


<details>
  <summary>Details</summary>
Motivation: 以往视觉模态越狱攻击靠不可感知扰动，文本模态需可见修改，探索文本模态不可感知越狱攻击。

Method: 利用变体选择器，将其附加到恶意问题后改变分词，提出链式搜索管道生成对抗后缀。

Result: 不可感知越狱攻击对四个对齐大语言模型攻击成功率高，能泛化到提示注入攻击，且无可见修改。

Conclusion: 提出的不可感知越狱攻击方法有效，代码已开源。

Abstract: Jailbreaking attacks on the vision modality typically rely on imperceptible
adversarial perturbations, whereas attacks on the textual modality are
generally assumed to require visible modifications (e.g., non-semantic
suffixes). In this paper, we introduce imperceptible jailbreaks that exploit a
class of Unicode characters called variation selectors. By appending invisible
variation selectors to malicious questions, the jailbreak prompts appear
visually identical to original malicious questions on screen, while their
tokenization is "secretly" altered. We propose a chain-of-search pipeline to
generate such adversarial suffixes to induce harmful responses. Our experiments
show that our imperceptible jailbreaks achieve high attack success rates
against four aligned LLMs and generalize to prompt injection attacks, all
without producing any visible modifications in the written prompt. Our code is
available at https://github.com/sail-sg/imperceptible-jailbreaks.

</details>


### [765] [SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior Reasoning LLMs](https://arxiv.org/abs/2510.05069)
*Dachuan Shi,Abedelkadir Asi,Keying Li,Xiangchi Yuan,Leyan Pan,Wenke Lee,Wen Xiao*

Main category: cs.CL

TL;DR: 介绍了训练自由的SwiReasoning框架，可动态切换显式和隐式推理，在数学和STEM基准测试中提高了准确率和标记效率。


<details>
  <summary>Details</summary>
Motivation: 潜推理在免训练设置中面临扩散概率质量、引入噪声、阻碍收敛和过度思考浪费标记等问题，需要解决。

Method: 引入SwiReasoning框架，通过块级置信度动态切换显式和隐式推理，限制思维块切换最大次数。

Result: 在数学和STEM基准测试中，SwiReasoning将不同模型家族和规模推理大语言模型的平均准确率提高1.5%-2.8%，在预算受限情况下将平均标记效率提高56%-79%。

Conclusion: SwiReasoning框架能有效解决潜推理面临的问题，提升推理的准确率和标记效率。

Abstract: Recent work shows that, beyond discrete reasoning through explicit
chain-of-thought steps, which are limited by the boundaries of natural
languages, large language models (LLMs) can also reason continuously in latent
space, allowing richer information per step and thereby improving token
efficiency. Despite this promise, latent reasoning still faces two challenges,
especially in training-free settings: 1) purely latent reasoning broadens the
search distribution by maintaining multiple implicit paths, which diffuses
probability mass, introduces noise, and impedes convergence to a single
high-confidence solution, thereby hurting accuracy; and 2) overthinking
persists even without explicit text, wasting tokens and degrading efficiency.
To address these issues, we introduce SwiReasoning, a training-free framework
for LLM reasoning which features two key innovations: 1) SwiReasoning
dynamically switches between explicit and latent reasoning, guided by
block-wise confidence estimated from entropy trends in next-token
distributions, to balance exploration and exploitation and promote timely
convergence. 2) By limiting the maximum number of thinking-block switches,
SwiReasoning curbs overthinking and improves token efficiency across varying
problem difficulties. On widely used mathematics and STEM benchmarks,
SwiReasoning consistently improves average accuracy by 1.5%-2.8% across
reasoning LLMs of different model families and scales. Furthermore, under
constrained budgets, SwiReasoning improves average token efficiency by 56%-79%,
with larger gains as budgets tighten.

</details>


### [766] [Slm-mux: Orchestrating small language models for reasoning](https://arxiv.org/abs/2510.05077)
*Chenyu Wang,Zishen Wan,Hao Kang,Emma Chen,Zhiqiang Xie,Tushar Krishna,Vijay Janapa Reddi,Yilun Du*

Main category: cs.CL

TL;DR: 本文提出三阶段方法编排小语言模型（SLMs），相比现有方法有显著性能提升，证明该方法能使SLMs构成更准确高效系统。


<details>
  <summary>Details</summary>
Motivation: 现有编排方法主要针对前沿模型，应用于SLMs效果不佳，需开发适用于SLMs的编排方法以提升其准确性。

Method: 提出三阶段方法，包括引入多模型架构SLM - MUX协调多个SLMs，开发模型选择搜索和测试时缩放两种优化策略。

Result: 相比现有编排方法，在MATH、GPQA和GSM8K上分别提升13.4%、8.8%和7.0%；两个SLMs组成的SLM - MUX在GPQA和GSM8K上优于Qwen 2.5 72B，在MATH上相当。

Conclusion: 通过提出的方法可以有效编排SLMs，使其构成更准确高效的系统。

Abstract: With the rapid development of language models, the number of small language
models (SLMs) has grown significantly. Although they do not achieve
state-of-the-art accuracy, they are more efficient and often excel at specific
tasks. This raises a natural question: can multiple SLMs be orchestrated into a
system where each contributes effectively, achieving higher accuracy than any
individual model? Existing orchestration methods have primarily targeted
frontier models (e.g., GPT-4) and perform suboptimally when applied to SLMs. To
address this gap, we propose a three-stage approach for orchestrating SLMs.
First, we introduce SLM-MUX, a multi-model architecture that effectively
coordinates multiple SLMs. Building on this, we develop two optimization
strategies: (i) a model selection search that identifies the most complementary
SLMs from a given pool, and (ii) test-time scaling tailored to SLM-MUX. Our
approach delivers strong results: Compared to existing orchestration methods,
our approach achieves up to 13.4% improvement on MATH, 8.8% on GPQA, and 7.0%
on GSM8K. With just two SLMS, SLM-MUX outperforms Qwen 2.5 72B on GPQA and
GSM8K, and matches its performance on MATH. We further provide theoretical
analyses to substantiate the advantages of our method. In summary, we
demonstrate that SLMs can be effectively orchestrated into more accurate and
efficient systems through the proposed approach.

</details>


### [767] [TeachLM: Post-Training LLMs for Education Using Authentic Learning Data](https://arxiv.org/abs/2510.05087)
*Janos Perczel,Jin Chow,Dorottya Demszky*

Main category: cs.CL

TL;DR: 论文指出大语言模型在教育应用有局限，提出TeachLM，用参数高效微调训练，还提出评估协议，评估显示微调能提升性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在教育应用受限于缺乏高质量学生学习数据，提示工程能力有限，需优化用于教学的大语言模型。

Method: 通过参数高效微调最先进模型得到TeachLM，在包含10万小时一对一学生 - 导师互动数据集上训练，开发真实学生模型生成合成对话，提出多轮评估协议。

Result: 在真实学习数据上微调显著提升对话和教学性能，如学生谈话时间翻倍、提问风格改善、对话轮数增加50%、教学更个性化。

Conclusion: 在真实学习数据上进行参数高效微调能有效提升大语言模型的对话和教学性能。

Abstract: The promise of generative AI to revolutionize education is constrained by the
pedagogical limits of large language models (LLMs). A major issue is the lack
of access to high-quality training data that reflect the learning of actual
students. Prompt engineering has emerged as a stopgap, but the ability of
prompts to encode complex pedagogical strategies in rule-based natural language
is inherently limited. To address this gap we introduce TeachLM - an LLM
optimized for teaching through parameter-efficient fine-tuning of
state-of-the-art models. TeachLM is trained on a dataset comprised of 100,000
hours of one-on-one, longitudinal student-tutor interactions maintained by
Polygence, which underwent a rigorous anonymization process to protect privacy.
We use parameter-efficient fine-tuning to develop an authentic student model
that enables the generation of high-fidelity synthetic student-tutor dialogues.
Building on this capability, we propose a novel multi-turn evaluation protocol
that leverages synthetic dialogue generation to provide fast, scalable, and
reproducible assessments of the dialogical capabilities of LLMs. Our
evaluations demonstrate that fine-tuning on authentic learning data
significantly improves conversational and pedagogical performance - doubling
student talk time, improving questioning style, increasing dialogue turns by
50%, and greater personalization of instruction.

</details>


### [768] [Finish First, Perfect Later: Test-Time Token-Level Cross-Validation for Diffusion Large Language Models](https://arxiv.org/abs/2510.05090)
*Runchu Tian,Junxia Cui,Xueqiang Xu,Feng Yao,Jingbo Shang*

Main category: cs.CL

TL;DR: 提出Tolerator解码策略解决离散扩散大语言模型解码问题，在多基准测试上有提升。


<details>
  <summary>Details</summary>
Motivation: 离散扩散大语言模型的原始解码策略存在一旦接受令牌就无法修改的问题，影响输出质量。

Method: 提出Tolerator，采用两阶段过程，包括序列填充和通过重新掩码和解码部分令牌进行迭代细化。

Result: 在五个标准基准测试中，在相同计算预算下，Tolerator比基线方法有一致的改进。

Conclusion: 解码算法对发挥扩散大语言模型的全部潜力至关重要。

Abstract: Diffusion large language models (dLLMs) have recently emerged as a promising
alternative to autoregressive (AR) models, offering advantages such as
accelerated parallel decoding and bidirectional context modeling. However, the
vanilla decoding strategy in discrete dLLMs suffers from a critical limitation:
once a token is accepted, it can no longer be revised in subsequent steps. As a
result, early mistakes persist across iterations, harming both intermediate
predictions and final output quality. To address this issue, we propose
Tolerator (Token-Level Cross-Validation Refinement), a training-free decoding
strategy that leverages cross-validation among predicted tokens. Unlike
existing methods that follow a single progressive unmasking procedure,
Tolerator introduces a two-stage process: (i) sequence fill-up and (ii)
iterative refinement by remasking and decoding a subset of tokens while
treating the remaining as context. This design enables previously accepted
tokens to be reconsidered and corrected when necessary, leading to more
reliable diffusion decoding outputs. We evaluate Tolerator on five standard
benchmarks covering language understanding, code generation, and mathematics.
Experiments show that our method achieves consistent improvements over the
baselines under the same computational budget. These findings suggest that
decoding algorithms are crucial to realizing the full potential of diffusion
large language models. Code and data are publicly available.

</details>
