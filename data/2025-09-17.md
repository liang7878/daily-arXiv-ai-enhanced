<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 43]
- [cs.CE](#cs.CE) [Total: 4]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.DC](#cs.DC) [Total: 14]
- [cs.DS](#cs.DS) [Total: 2]
- [cs.IR](#cs.IR) [Total: 9]
- [cs.LG](#cs.LG) [Total: 98]
- [cs.NE](#cs.NE) [Total: 3]
- [cs.SE](#cs.SE) [Total: 17]
- [q-fin.PM](#q-fin.PM) [Total: 2]
- [q-fin.RM](#q-fin.RM) [Total: 1]
- [q-fin.TR](#q-fin.TR) [Total: 2]
- [stat.ML](#stat.ML) [Total: 2]
- [cs.SD](#cs.SD) [Total: 7]
- [cs.CG](#cs.CG) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cond-mat.mes-hall](#cond-mat.mes-hall) [Total: 1]
- [stat.ME](#stat.ME) [Total: 4]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 1]
- [stat.AP](#stat.AP) [Total: 1]
- [cs.CV](#cs.CV) [Total: 34]
- [cs.MA](#cs.MA) [Total: 2]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.CR](#cs.CR) [Total: 7]
- [math.ST](#math.ST) [Total: 3]
- [cs.CL](#cs.CL) [Total: 17]
- [cs.DL](#cs.DL) [Total: 2]
- [cs.RO](#cs.RO) [Total: 15]
- [econ.GN](#econ.GN) [Total: 1]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [eess.SY](#eess.SY) [Total: 4]
- [math.OC](#math.OC) [Total: 3]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [cs.SI](#cs.SI) [Total: 3]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [eess.IV](#eess.IV) [Total: 6]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [cs.HC](#cs.HC) [Total: 3]
- [eess.SP](#eess.SP) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [V-Math: An Agentic Approach to the Vietnamese National High School Graduation Mathematics Exams](https://arxiv.org/abs/2509.12251)
*Duong Q. Nguyen,Quy P. Nguyen,Nguyen Van Nhon,Quang-Thinh Bui,H. Nguyen-Xuan*

Main category: cs.AI

TL;DR: 本文提出V - Math框架助力越南高中生备考数学高考，还能辅助教师出题，初步评估显示其有良好效果。


<details>
  <summary>Details</summary>
Motivation: 帮助越南高中生备考国家高中数学毕业考试，减轻教师手动出题负担，丰富教学资源。

Method: 开发V - Math框架，集成问题生成器、求解/解释器和个性化导师三个AI代理，介绍系统架构及不同模式。

Result: V - Math能生成符合矩阵的考试，解答准确率高，解释连贯，增加练习材料多样性。

Conclusion: V - Math有潜力支持符合国家标准的数学备考，通过AI辅助出题赋予教师更多能力。

Abstract: This paper develops an autonomous agentic framework called V-Math that aims
to assist Vietnamese high school students in preparing for the National High
School Graduation Mathematics Exams (NHSGMEs). The salient framework integrates
three specialized AI agents: a specification-matrix-conditioned question
generator, a solver/explainer for detailed step-by-step reasoning, and a
personalized tutor that adapts to student performance. Beyond enabling
self-paced student practice, V-Math supports teachers by generating innovative,
compliant exam questions and building diverse, high-quality question banks.
This reduces manual workload and enriches instructional resources. We describe
the system architecture, focusing on practice modes for learners and
teacher-oriented features for question generation. Preliminary evaluations
demonstrate that V-Math produces matrix-aligned exams with high solution
accuracy, delivers coherent explanations, and enhances the variety of practice
materials. These results highlight its potential to support scalable, equitable
mathematics preparation aligned with national standards while also empowering
teachers through AI-assisted exam creation.

</details>


### [2] [DISPLIB: a library of train dispatching problems](https://arxiv.org/abs/2509.12254)
*Oddvar Kloster,Bjørnar Luteberget,Carlo Mannino,Giorgio Sartor*

Main category: cs.AI

TL;DR: 本文引入通用问题定义和文件格式DISPLIB，收集实例并公开，方便研究人员开展列车调度问题研究及求解器对比。


<details>
  <summary>Details</summary>
Motivation: 现有列车重路由和重调度优化算法研究多关联特定工业用例，代码和数据少公开，阻碍可重复性和算法性能评估。

Method: 引入通用问题定义和文件格式DISPLIB，收集多个真实用例的问题实例并公开，还给出参考求解器实现。

Result: 研究人员和开发者无需工业联系即可研究列车调度问题，研究社区可对求解器进行实证比较。

Conclusion: 材料公开利于列车调度问题研究和求解器性能评估，促进该领域发展。

Abstract: Optimization-based decision support systems have a significant potential to
reduce delays, and thus improve efficiency on the railways, by automatically
re-routing and re-scheduling trains after delays have occurred. The operations
research community has dedicated a lot of effort to developing optimization
algorithms for this problem, but each study is typically tightly connected with
a specific industrial use case. Code and data are seldom shared publicly. This
fact hinders reproducibility, and has led to a proliferation of papers
describing algorithms for more or less compatible problem definitions, without
any real opportunity for readers to assess their relative performance. Inspired
by the successful communities around MILP, SAT, TSP, VRP, etc., we introduce a
common problem definition and file format, DISPLIB, which captures all the main
features of train re-routing and re-scheduling. We have gathered problem
instances from multiple real-world use cases and made them openly available. In
this paper, we describe the problem definition, the industrial instances, and a
reference solver implementation. This allows any researcher or developer to
work on the train dispatching problem without an industrial connection, and
enables the research community to perform empirical comparisons between
solvers. All materials are available online at https://displib.github.io.

</details>


### [3] [InPhyRe Discovers: Large Multimodal Models Struggle in Inductive Physical Reasoning](https://arxiv.org/abs/2509.12263)
*Gautam Sreekumar,Vishnu Naresh Boddeti*

Main category: cs.AI

TL;DR: 现有视觉基准仅评估大模态模型（LMMs）的参数知识，未评估归纳物理推理能力。本文提出InPhyRe基准来测量LMMs的归纳物理推理能力，测试发现LMMs存在推理应用差、归纳推理弱和语言偏差等问题。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基准未评估LMMs的归纳物理推理能力，而该能力对LMMs在安全关键应用中替代人类代理至关重要。

Method: 提出InPhyRe视觉问答基准，在算法生成的合成碰撞视频上评估LMMs预测碰撞事件结果的能力。

Result: 测试13个LMMs发现，LMMs难以应用参数知识推理，归纳物理推理在演示样本违反物理定律时较弱，且存在语言偏差、忽视视觉输入的问题。

Conclusion: LMMs在归纳物理推理方面存在不足，其对视觉输入的可信度存疑。

Abstract: Large multimodal models (LMMs) encode universal physical laws observed during
training, such as momentum conservation, as parametric knowledge. It allows
LMMs to answer physical reasoning queries, such as the outcome of a potential
collision event from visual input. However, since parametric knowledge includes
only the physical laws seen during training, it is insufficient for reasoning
when the inference scenario violates these physical laws. In contrast, humans
possess the skill to adapt their physical reasoning to unseen physical
environments from a few visual examples. This ability, which we refer to as
inductive physical reasoning, is indispensable for LMMs if they are to replace
human agents in safety-critical applications. Despite its importance, existing
visual benchmarks evaluate only the parametric knowledge in LMMs, and not
inductive physical reasoning. To this end, we propose InPhyRe, the first visual
question answering benchmark to measure inductive physical reasoning in LMMs.
InPhyRe evaluates LMMs on their ability to predict the outcome of collision
events in algorithmically generated synthetic collision videos. By inspecting
13 LMMs, InPhyRe informs us that (1) LMMs struggle to apply their limited
parametric knowledge about universal physical laws to reasoning, (2) inductive
physical reasoning in LMMs is weak when demonstration samples violate universal
physical laws, and (3) inductive physical reasoning in LMMs suffers from
language bias and largely ignores the visual inputs, questioning the
trustworthiness of LMMs regarding visual inputs.

</details>


### [4] [HLSMAC: A New StarCraft Multi-Agent Challenge for High-Level Strategic Decision-Making](https://arxiv.org/abs/2509.12927)
*Xingxing Hong,Yungong Wang,Dexin Jin,Ye Yuan,Ximing Huang,Zijian Wu,Wenxin Li*

Main category: cs.AI

TL;DR: 介绍新的合作式多智能体强化学习基准HLSMAC，用三十六计设计场景并提出新评估指标，实验证明其对提升多智能体战略决策有用。


<details>
  <summary>Details</summary>
Motivation: 现有基于星际争霸II的基准如SMAC主要关注微观管理，无法全面评估高级战略智能，需要新基准。

Method: 设计12个基于三十六计的星际争霸II场景构成HLSMAC，提出多维度新评估指标，将先进算法和基于大语言模型的智能体集成到基准中进行实验。

Result: 实验表明HLSMAC可作为推进多智能体战略决策的强大测试平台。

Conclusion: HLSMAC能有效评估多智能体高级战略决策能力，推动多智能体战略决策的发展。

Abstract: Benchmarks are crucial for assessing multi-agent reinforcement learning
(MARL) algorithms. While StarCraft II-related environments have driven
significant advances in MARL, existing benchmarks like SMAC focus primarily on
micromanagement, limiting comprehensive evaluation of high-level strategic
intelligence. To address this, we introduce HLSMAC, a new cooperative MARL
benchmark with 12 carefully designed StarCraft II scenarios based on classical
stratagems from the Thirty-Six Stratagems. Each scenario corresponds to a
specific stratagem and is designed to challenge agents with diverse strategic
elements, including tactical maneuvering, timing coordination, and deception,
thereby opening up avenues for evaluating high-level strategic decision-making
capabilities. We also propose novel metrics across multiple dimensions beyond
conventional win rate, such as ability utilization and advancement efficiency,
to assess agents' overall performance within the HLSMAC environment. We
integrate state-of-the-art MARL algorithms and LLM-based agents with our
benchmark and conduct comprehensive experiments. The results demonstrate that
HLSMAC serves as a robust testbed for advancing multi-agent strategic
decision-making.

</details>


### [5] [LLMAP: LLM-Assisted Multi-Objective Route Planning with User Preferences](https://arxiv.org/abs/2509.12273)
*Liangqi Yuan,Dong-Jun Han,Christopher G. Brinton,Sabine Brunswicker*

Main category: cs.AI

TL;DR: 本文提出LLMAP系统和MSGS算法用于自然语言驱动的路线规划，经实验证明该方法在多约束条件下表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前直接用LLM进行路线规划难以处理大量地图数据，基于图的搜索策略理解自然语言偏好能力有限，且用户时空分布不均，因此需要新的解决方案。

Method: 引入LLM - Assisted route Planning (LLMAP)系统，用LLM - as - Parser理解自然语言、识别任务、提取用户偏好和任务依赖；采用Multi - Step Graph construction with iterative Search (MSGS)算法寻找最优路线；多目标优化方法自适应调整目标权重。

Result: 使用全球14个国家27个城市的1000个不同复杂度的路由提示进行实验，该方法在多约束条件下有出色表现。

Conclusion: 提出的方法在多约束条件下能保证实现优越性能。

Abstract: The rise of large language models (LLMs) has made natural language-driven
route planning an emerging research area that encompasses rich user objectives.
Current research exhibits two distinct approaches: direct route planning using
LLM-as-Agent and graph-based searching strategies. However, LLMs in the former
approach struggle to handle extensive map data, while the latter shows limited
capability in understanding natural language preferences. Additionally, a more
critical challenge arises from the highly heterogeneous and unpredictable
spatio-temporal distribution of users across the globe. In this paper, we
introduce a novel LLM-Assisted route Planning (LLMAP) system that employs an
LLM-as-Parser to comprehend natural language, identify tasks, and extract user
preferences and recognize task dependencies, coupled with a Multi-Step Graph
construction with iterative Search (MSGS) algorithm as the underlying solver
for optimal route finding. Our multi-objective optimization approach adaptively
tunes objective weights to maximize points of interest (POI) quality and task
completion rate while minimizing route distance, subject to three key
constraints: user time limits, POI opening hours, and task dependencies. We
conduct extensive experiments using 1,000 routing prompts sampled with varying
complexity across 14 countries and 27 cities worldwide. The results demonstrate
that our approach achieves superior performance with guarantees across multiple
constraints.

</details>


### [6] [Developing an aeroponic smart experimental greenhouse for controlling irrigation and plant disease detection using deep learning and IoT](https://arxiv.org/abs/2509.12274)
*Mohammadreza Narimani,Ali Hajiahmad,Ali Moghimi,Reza Alimardani,Shahin Rafiee,Amir Hossein Mirzabe*

Main category: cs.AI

TL;DR: 本文开发并测试了智能气培温室，结合物联网和人工智能监测天竺葵状态与环境条件，物联网系统可调整参数，AI框架中VGG - 19算法识别准确率最高。


<details>
  <summary>Details</summary>
Motivation: 控制温室环境条件和监测植物状态对促进作物生产的管理决策至关重要，因此开展本研究。

Method: 开发基于物联网的平台控制植物环境条件；用VGG - 19、InceptionResNetV2和InceptionV3算法开发AI疾病检测框架，并与专家评估对比。

Result: 物联网系统能在线持续发布环境数据并调整参数；AI框架中VGG - 19算法识别干旱胁迫和锈叶的准确率达92%，高于其他算法。

Conclusion: 所开发的智能气培温室系统在环境控制和植物疾病检测方面有较好效果，VGG - 19算法在植物状态识别上表现更优。

Abstract: Controlling environmental conditions and monitoring plant status in
greenhouses is critical to promptly making appropriate management decisions
aimed at promoting crop production. The primary objective of this research
study was to develop and test a smart aeroponic greenhouse on an experimental
scale where the status of Geranium plant and environmental conditions are
continuously monitored through the integration of the internet of things (IoT)
and artificial intelligence (AI). An IoT-based platform was developed to
control the environmental conditions of plants more efficiently and provide
insights to users to make informed management decisions. In addition, we
developed an AI-based disease detection framework using VGG-19,
InceptionResNetV2, and InceptionV3 algorithms to analyze the images captured
periodically after an intentional inoculation. The performance of the AI
framework was compared with an expert's evaluation of disease status.
Preliminary results showed that the IoT system implemented in the greenhouse
environment is able to publish data such as temperature, humidity, water flow,
and volume of charge tanks online continuously to users and adjust the
controlled parameters to provide an optimal growth environment for the plants.
Furthermore, the results of the AI framework demonstrate that the VGG-19
algorithm was able to identify drought stress and rust leaves from healthy
leaves with the highest accuracy, 92% among the other algorithms.

</details>


### [7] [AIssistant: An Agentic Approach for Human--AI Collaborative Scientific Work on Reviews and Perspectives in Machine Learning](https://arxiv.org/abs/2509.12282)
*Sasi Kiran Gaddipati,Farhana Keya,Gollam Rabby,Sören Auer*

Main category: cs.AI

TL;DR: 介绍AIssistant这一开源人机协作框架用于简化科研工作流，以机器学习领域论文视角和评审为例开展实验，评估显示其提升效率和一致性，但人机协作仍必要且存在局限性。


<details>
  <summary>Details</summary>
Motivation: 现有AI辅助研究系统碎片化且缺乏以人为中心的工作流，需开发新框架解决这些问题。

Method: 开发AIssistant框架，集成多种模块化工具和代理，分三个层面（独立人工评审、自动大语言模型评审、程序主席监督）进行综合评估。

Result: AIssistant提高了起草效率和主题一致性。

Conclusion: 人机协作对保证事实正确性、方法合理性和道德合规性至关重要，且AIssistant存在引用幻觉、适应动态结构困难和多模态内容集成不完整等局限。

Abstract: Advances in AI-assisted research have introduced powerful tools for
literature retrieval, hypothesis generation, experimentation, and manuscript
preparation. However, systems remain fragmented and lack human-centred
workflows. To address these gaps, we introduce AIssistant, an agentic,
open-source Human-AI collaborative framework designed to simplify the
end-to-end creation of scientific workflows. Since our development is still in
an early stage, we present here the first experiments with AIssistant for
perspective and review research papers in machine learning. Our system
integrates modular tools and agents for literature synthesis, section-wise
experimentation, citation management, and automatic LaTeX paper text
generation, while maintaining human oversight at every stage to ensure
accuracy, coherence, and scholarly rigour. We conducted a comprehensive
evaluation across three layers: (1) Independent Human Review, following NeurIPS
double-blind standards; (2) Automated LLM Review, using GPT-5 as a scalable
human review proxy; and (3) Program Chair Oversight, where the chair monitors
the entire review process and makes final validation and acceptance decisions.
The results demonstrate that AIssistant improves drafting efficiency and
thematic consistency. Nonetheless, Human-AI collaboration remains essential for
maintaining factual correctness, methodological soundness, and ethical
compliance. Despite its effectiveness, we identify key limitations, including
hallucinated citations, difficulty adapting to dynamic paper structures, and
incomplete integration of multimodal content.

</details>


### [8] [Small Models, Big Results: Achieving Superior Intent Extraction through Decomposition](https://arxiv.org/abs/2509.12423)
*Danielle Cohen,Yoni Halpern,Noam Kahlon,Joel Oren,Omri Berkovitch,Sapir Caduri,Ido Dagan,Anatoly Efros*

Main category: cs.AI

TL;DR: 提出新分解方法提升资源受限模型对用户意图理解，效果超大型多模态大语言模型。


<details>
  <summary>Details</summary>
Motivation: 理解用户界面交互轨迹中的用户意图是智能代理开发关键挑战，资源受限小模型在准确意图推断上有困难。

Method: 先进行结构化交互总结，捕获每个用户操作关键信息；再用微调模型对汇总摘要进行意图提取。

Result: 该方法提高了资源受限模型的意图理解能力，甚至超过了大型多模态大语言模型的基础性能。

Conclusion: 所提出的分解方法能有效解决资源受限模型在用户意图理解上的局限。

Abstract: Understanding user intents from UI interaction trajectories remains a
challenging, yet crucial, frontier in intelligent agent development. While
massive, datacenter-based, multi-modal large language models (MLLMs) possess
greater capacity to handle the complexities of such sequences, smaller models
which can run on-device to provide a privacy-preserving, low-cost, and
low-latency user experience, struggle with accurate intent inference. We
address these limitations by introducing a novel decomposed approach: first, we
perform structured interaction summarization, capturing key information from
each user action. Second, we perform intent extraction using a fine-tuned model
operating on the aggregated summaries. This method improves intent
understanding in resource-constrained models, even surpassing the base
performance of large MLLMs.

</details>


### [9] [Building Coding Agents via Entropy-Enhanced Multi-Turn Preference Optimization](https://arxiv.org/abs/2509.12434)
*Jiahao Yu,Zelei Cheng,Xian Wu,Xinyu Xing*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Software engineering presents complex, multi-step challenges for Large
Language Models (LLMs), requiring reasoning over large codebases and
coordinated tool use. The difficulty of these tasks is exemplified by
benchmarks like SWE-bench, where current LLMs still struggle to resolve
real-world issues.
  A promising approach to enhance performance is test-time scaling (TTS), but
its gains are heavily dependent on the diversity of model outputs.
  While standard alignment methods such as Direct Preference Optimization (DPO)
and Kahneman-Tversky Optimization (KTO) are effective at aligning model outputs
with human preferences, this process can come at the cost of reduced diversity,
limiting the effectiveness of TTS.
  Additionally, existing preference optimization algorithms are typically
designed for single-turn tasks and do not fully address the complexities of
multi-turn reasoning and tool integration required for interactive coding
agents.
  To bridge this gap, we introduce \sys, an entropy-enhanced framework that
adapts existing preference optimization algorithms to the multi-turn,
tool-assisted setting.
  \sys augments the preference objective to explicitly preserve policy entropy
and generalizes learning to optimize over multi-turn interactions rather than
single-turn responses.
  We validate \sys by fine-tuning a diverse suite of models from different
families and sizes (up to 106B parameters).
  To maximize performance gains from TTS, we further propose a hybrid
best-trajectory selection scheme combining a learned verifier model with model
free approaches.
  On the \swebench leaderboard, our approach establishes new state-of-the-art
results among open-weight models. A 30B parameter model trained with \sys ranks
1st on \lite and 4th on \verified on the open-weight leaderboard, surpassed
only by models with over 10x more parameters(\eg$>$350B).

</details>


### [10] [Enhancing Physical Consistency in Lightweight World Models](https://arxiv.org/abs/2509.12437)
*Dingrui Wang,Zhexiao Sun,Zhouheng Li,Cheng Wang,Youlun Peng,Hongyuan Ye,Baha Zarrouki,Wei Li,Mattia Piccinini,Lei Xie,Johannes Betz*

Main category: cs.AI

TL;DR: 提出PIWM模型解决世界模型大小与性能权衡问题，通过Soft Mask和Warm Start提升性能，实验表明PIWM表现更优。


<details>
  <summary>Details</summary>
Motivation: 解决世界模型部署中大小与性能的权衡问题，大模型计算资源需求大，小模型预测效果差。

Method: 提出PIWM模型，训练时使用Soft Mask，推理时引入Warm Start技术。

Result: 在相同参数规模（400M）下，PIWM加权总分超基线60.6%；最小的PIWM（130M Soft Mask）比最大基线模型加权总分高7.4%，推理速度快28%。

Conclusion: PIWM模型能有效平衡大小与性能，在性能和推理速度上表现出色。

Abstract: A major challenge in deploying world models is the trade-off between size and
performance. Large world models can capture rich physical dynamics but require
massive computing resources, making them impractical for edge devices. Small
world models are easier to deploy but often struggle to learn accurate physics,
leading to poor predictions. We propose the Physics-Informed BEV World Model
(PIWM), a compact model designed to efficiently capture physical interactions
in bird's-eye-view (BEV) representations. PIWM uses Soft Mask during training
to improve dynamic object modeling and future prediction. We also introduce a
simple yet effective technique, Warm Start, for inference to enhance prediction
quality with a zero-shot model. Experiments show that at the same parameter
scale (400M), PIWM surpasses the baseline by 60.6% in weighted overall score.
Moreover, even when compared with the largest baseline model (400M), the
smallest PIWM (130M Soft Mask) achieves a 7.4% higher weighted overall score
with a 28% faster inference speed.

</details>


### [11] [Reasoning Models Can be Accurately Pruned Via Chain-of-Thought Reconstruction](https://arxiv.org/abs/2509.12464)
*Ryan Lucas,Kayhan Behdin,Zhipeng Wang,Qingquan Song,Shao Tang,Rahul Mazumder*

Main category: cs.AI

TL;DR: 推理语言模型部署成本高，传统压缩技术有问题，提出推理感知压缩（RAC）方法提升性能。


<details>
  <summary>Details</summary>
Motivation: 推理语言模型在推理时产生长思维链痕迹，大规模部署成本高，传统压缩技术在这类模型上性能损失大。

Method: 在剪枝时联合重建输入和模型的策略内思维链痕迹的激活，即提出推理感知压缩（RAC）方法，并集成到现有剪枝工作流程。

Result: RAC能无缝集成到现有剪枝工作流程如SparseGPT中，显著提升性能。

Conclusion: RAC是解决推理语言模型压缩问题的有效方法。

Abstract: Reasoning language models such as DeepSeek-R1 produce long chain-of-thought
traces during inference time which make them costly to deploy at scale. We show
that using compression techniques such as neural network pruning produces
greater performance loss than in typical language modeling tasks, and in some
cases can make the model slower since they cause the model to produce more
thinking tokens but with worse performance. We show that this is partly due to
the fact that standard LLM pruning methods often focus on input reconstruction,
whereas reasoning is a decode-dominated task. We introduce a simple, drop-in
fix: during pruning we jointly reconstruct activations from the input and the
model's on-policy chain-of-thought traces. This "Reasoning-Aware Compression"
(RAC) integrates seamlessly into existing pruning workflows such as SparseGPT,
and boosts their performance significantly. Code reproducing the results in the
paper can be found at: https://github.com/RyanLucas3/RAC

</details>


### [12] [Empowering Clinical Trial Design through AI: A Randomized Evaluation of PowerGPT](https://arxiv.org/abs/2509.12471)
*Yiwen Lu,Lu Li,Dazheng Zhang,Xinyao Jian,Tingyin Wang,Siqi Chen,Yuqing Lei,Jiayi Tong,Zhaohan Xi,Haitao Chu,Chongliang Luo,Alexis Ogdie,Brian Athey,Alparslan Turan,Michael Abramoff,Joseph C Cappelleri,Hua Xu,Yun Lu,Jesse Berlin,Daniel I. Sessler,David A. Asch,Xiaoqian Jiang,Yong Chen*

Main category: cs.AI

TL;DR: 介绍AI系统PowerGPT用于试验设计中自动选择测试和估计样本量，随机试验显示其能提升完成率、准确性并减少时间，已在多机构部署。


<details>
  <summary>Details</summary>
Motivation: 样本量计算复杂且依赖统计专业知识，给许多研究者造成障碍。

Method: 引入结合大语言模型和统计引擎的AI系统PowerGPT进行试验设计中的测试选择和样本量估计，并进行随机试验评估其有效性。

Result: PowerGPT显著提高任务完成率和准确性，减少平均完成时间，且在各种统计测试中效果一致，惠及不同专业背景人员。

Conclusion: PowerGPT是一种可扩展的AI驱动方法，能增强临床研究统计功效分析的可及性、效率和准确性。

Abstract: Sample size calculations for power analysis are critical for clinical
research and trial design, yet their complexity and reliance on statistical
expertise create barriers for many researchers. We introduce PowerGPT, an
AI-powered system integrating large language models (LLMs) with statistical
engines to automate test selection and sample size estimation in trial design.
In a randomized trial to evaluate its effectiveness, PowerGPT significantly
improved task completion rates (99.3% vs. 88.9% for test selection, 99.3% vs.
77.8% for sample size calculation) and accuracy (94.1% vs. 55.4% in sample size
estimation, p < 0.001), while reducing average completion time (4.0 vs. 9.3
minutes, p < 0.001). These gains were consistent across various statistical
tests and benefited both statisticians and non-statisticians as well as
bridging expertise gaps. Already under deployment across multiple institutions,
PowerGPT represents a scalable AI-driven approach that enhances accessibility,
efficiency, and accuracy in statistical power analysis for clinical research.

</details>


### [13] [Physical Complexity of a Cognitive Artifact](https://arxiv.org/abs/2509.12495)
*Gülce Kardeş,David Krakauer,Joshua Grochow*

Main category: cs.AI

TL;DR: 本文将Soma Cube物理谜题的计算复杂性概念映射到认知问题解决策略，分析任务难度及策略对复杂性的影响，提出智能模型。


<details>
  <summary>Details</summary>
Motivation: 认知科学和理论计算机科学都试图对任务难度进行分类和解释，研究机制以降低任务难度。

Method: 通过“物质性原则”将Soma Cube谜题的计算复杂性概念映射到认知策略，分析谜题分支因子，逐步改进试错搜索。

Result: 发现有效利用人工制品可通过利用物理约束降低有效时间复杂度。

Conclusion: 提出智能可作为一个算法库，能调用心智和物质能力的模型。

Abstract: Cognitive science and theoretical computer science both seek to classify and
explain the difficulty of tasks. Mechanisms of intelligence are those that
reduce task difficulty. Here we map concepts from the computational complexity
of a physical puzzle, the Soma Cube, onto cognitive problem-solving strategies
through a ``Principle of Materiality''. By analyzing the puzzle's branching
factor, measured through search tree outdegree, we quantitatively assess task
difficulty and systematically examine how different strategies modify
complexity. We incrementally refine a trial-and-error search by layering
preprocessing (cognitive chunking), value ordering (cognitive free-sorting),
variable ordering (cognitive scaffolding), and pruning (cognitive inference).
We discuss how the competent use of artifacts reduces effective time complexity
by exploiting physical constraints and propose a model of intelligence as a
library of algorithms that recruit the capabilities of both mind and matter.

</details>


### [14] [A Dimensionality-Reduced XAI Framework for Roundabout Crash Severity Insights](https://arxiv.org/abs/2509.12524)
*Rohit Chakraborty,Subasish Das*

Main category: cs.AI

TL;DR: 研究用两步可解释工作流分析2017 - 2021年俄亥俄州环形交叉路口事故，识别出四种事故模式并分析严重程度驱动因素，为公共安全分析提供可用XAI实用模板。


<details>
  <summary>Details</summary>
Motivation: 环形交叉路口可减少严重事故，但风险模式因条件而异，需深入分析。

Method: 采用两步可解释工作流，用聚类对应分析（CCA）识别共现因素，用基于树的严重程度模型结合SHAP量化伤害驱动因素。

Result: 黑暗、潮湿路面和高限速与固定物体或角度事件同时发生时事故严重程度更高；晴朗、低速环境下严重程度较低。还指出不同位置的事故机制。

Conclusion: 该工作流将模式发现与案例级解释相联系，为公共安全分析中的可用XAI提供实用模板，支持场地筛选、对策选择和可审计报告。

Abstract: Roundabouts reduce severe crashes, yet risk patterns vary by conditions. This
study analyzes 2017-2021 Ohio roundabout crashes using a two-step, explainable
workflow. Cluster Correspondence Analysis (CCA) identifies co-occurring factors
and yields four crash patterns. A tree-based severity model is then interpreted
with SHAP to quantify drivers of injury within and across patterns. Results
show higher severity when darkness, wet surfaces, and higher posted speeds
coincide with fixed-object or angle events, and lower severity in clear,
low-speed settings. Pattern-specific explanations highlight mechanisms at
entries (fail-to-yield, gap acceptance), within multi-lane circulation
(improper maneuvers), and during slow-downs (rear-end). The workflow links
pattern discovery with case-level explanations, supporting site screening,
countermeasure selection, and audit-ready reporting. The contribution to
Information Systems is a practical template for usable XAI in public safety
analytics.

</details>


### [15] [zELO: ELO-inspired Training Method for Rerankers and Embedding Models](https://arxiv.org/abs/2509.12541)
*Nicholas Pipitone,Ghita Houir Alami,Advaith Avadhanam,Anton Kaminskyi,Ashley Khoo*

Main category: cs.AI

TL;DR: 提出zELO训练方法，训练zerank-1和zerank-1-small模型，在多领域检索表现优，且具通用性。


<details>
  <summary>Details</summary>
Motivation: 优化检索性能。

Method: 提出zELO训练方法，利用无监督数据训练zerank-1和zerank-1-small模型。

Result: 模型在多领域取得最高检索分数，在NDCG@10和Recall上超闭源专有重排器，在域外和私有客户数据集上保持0-shot性能，用112,000个查询及每个查询100个文档，在不到10,000 H100小时内端到端训练。

Conclusion: zELO方法有效，训练的模型性能好且通用。

Abstract: We introduce a novel training methodology named zELO, which optimizes
retrieval performance via the analysis that ranking tasks are statically
equivalent to a Thurstone model. Based on the zELO method, we use unsupervised
data in order train a suite of state-of-the-art open-weight reranker models:
zerank-1 and zerank-1-small. These models achieve the highest retrieval scores
in multiple domains, including finance, legal, code, and STEM, outperforming
closed-source proprietary rerankers on both NDCG@10 and Recall. These models
also demonstrate great versatility, maintaining their 0-shot performance on
out-of-domain and private customer datasets. The training data included 112,000
queries and 100 documents per query, and was trained end-to-end from
unannotated queries and documents in less than 10,000 H100-hours.

</details>


### [16] [Human + AI for Accelerating Ad Localization Evaluation](https://arxiv.org/abs/2509.12543)
*Harshit Rajgarhia,Shivali Dalmia,Mengyang Zhao,Mukherji Abhishek,Kiran Ganesh*

Main category: cs.AI

TL;DR: 提出结合自动化组件与人工监督的框架加速广告本地化评估工作流，在六个地区取得定性成果。


<details>
  <summary>Details</summary>
Motivation: 广告针对多语言受众适配需保留视觉一致性等，现有工作缺乏针对广告本地化评估工作流的有效方法。

Method: 引入结合自动化组件与人工监督的结构化框架，集成场景文本检测、修复、机器翻译和文本重新施加。

Result: 在六个地区的定性结果表明该方法能产生语义准确、视觉连贯的本地化广告。

Conclusion: 该方法适用于现实工作流。

Abstract: Adapting advertisements for multilingual audiences requires more than simple
text translation; it demands preservation of visual consistency, spatial
alignment, and stylistic integrity across diverse languages and formats. We
introduce a structured framework that combines automated components with human
oversight to address the complexities of advertisement localization. To the
best of our knowledge, this is the first work to integrate scene text
detection, inpainting, machine translation (MT), and text reimposition
specifically for accelerating ad localization evaluation workflows. Qualitative
results across six locales demonstrate that our approach produces semantically
accurate and visually coherent localized advertisements, suitable for
deployment in real-world workflows.

</details>


### [17] [Redefining CX with Agentic AI: Minerva CQ Case Study](https://arxiv.org/abs/2509.12589)
*Garima Agrawal,Riccardo De Maria,Kiran Davuluri,Daniele Spera,Charlie Read,Cosimo Spera,Jack Garrett,Don Miller*

Main category: cs.AI

TL;DR: 现有客服中心AI存在问题，引入Agentic AI，以Minerva CQ为例，部署后提升了效率和客户体验。


<details>
  <summary>Details</summary>
Motivation: 解决现有AI客服系统中平均处理时间长、首呼解决率低、客户满意度差等问题，减轻坐席认知负担。

Method: 引入Agentic AI，以Minerva CQ为案例，集成实时转录、意图和情感检测等功能，实现主动工作流和持续上下文构建。

Result: Minerva CQ在实际部署中作为AI副驾驶，在多个部署中显著提升了坐席效率和客户体验。

Conclusion: Agentic AI能够有效解决现有客服系统问题，提升坐席效率和客户体验。

Abstract: Despite advances in AI for contact centers, customer experience (CX)
continues to suffer from high average handling time (AHT), low first-call
resolution, and poor customer satisfaction (CSAT). A key driver is the
cognitive load on agents, who must navigate fragmented systems, troubleshoot
manually, and frequently place customers on hold. Existing AI-powered
agent-assist tools are often reactive driven by static rules, simple prompting,
or retrieval-augmented generation (RAG) without deeper contextual reasoning. We
introduce Agentic AI goal-driven, autonomous, tool-using systems that
proactively support agents in real time. Unlike conventional approaches,
Agentic AI identifies customer intent, triggers modular workflows, maintains
evolving context, and adapts dynamically to conversation state. This paper
presents a case study of Minerva CQ, a real-time Agent Assist product deployed
in voice-based customer support. Minerva CQ integrates real-time transcription,
intent and sentiment detection, entity recognition, contextual retrieval,
dynamic customer profiling, and partial conversational summaries enabling
proactive workflows and continuous context-building. Deployed in live
production, Minerva CQ acts as an AI co-pilot, delivering measurable
improvements in agent efficiency and customer experience across multiple
deployments.

</details>


### [18] [Match Chat: Real Time Generative AI and Generative Computing for Tennis](https://arxiv.org/abs/2509.12592)
*Aaron Baughman,Gozde Akay,Eduardo Morales,Rahul Agarwal,Preetika Srivastava*

Main category: cs.AI

TL;DR: 介绍Match Chat系统，结合GenAI与GenComp技术，为网球球迷提供实时查询服务，在两项大满贯赛事中表现良好，展示了实时面向消费者AI系统的设计模式。


<details>
  <summary>Details</summary>
Motivation: 提升网球球迷体验，为其提供即时、准确的比赛相关查询响应。

Method: 将GenAI与GenComp技术结合，采用面向代理架构（AOA），结合规则引擎、预测模型和代理预处理用户查询。

Result: 系统在两项大满贯赛事中为约100万用户服务，答案准确率92.83%，平均响应时间6.25秒，96.08%查询通过交互式提示设计引导，保持100%正常运行时间。

Conclusion: 为实时面向消费者的AI系统提供关键设计模式，展示了在动态环境中部署高性能代理系统的实用路径。

Abstract: We present Match Chat, a real-time, agent-driven assistant designed to
enhance the tennis fan experience by delivering instant, accurate responses to
match-related queries. Match Chat integrates Generative Artificial Intelligence
(GenAI) with Generative Computing (GenComp) techniques to synthesize key
insights during live tennis singles matches. The system debuted at the 2025
Wimbledon Championships and the 2025 US Open, where it provided about 1 million
users with seamless access to streaming and static data through natural
language queries. The architecture is grounded in an Agent-Oriented
Architecture (AOA) combining rule engines, predictive models, and agents to
pre-process and optimize user queries before passing them to GenAI components.
The Match Chat system had an answer accuracy of 92.83% with an average response
time of 6.25 seconds under loads of up to 120 requests per second (RPS). Over
96.08% of all queries were guided using interactive prompt design, contributing
to a user experience that prioritized clarity, responsiveness, and minimal
effort. The system was designed to mask architectural complexity, offering a
frictionless and intuitive interface that required no onboarding or technical
familiarity. Across both Grand Slam deployments, Match Chat maintained 100%
uptime and supported nearly 1 million unique users, underscoring the
scalability and reliability of the platform. This work introduces key design
patterns for real-time, consumer-facing AI systems that emphasize speed,
precision, and usability that highlights a practical path for deploying
performant agentic systems in dynamic environments.

</details>


### [19] [DaSAThco: Data-Aware SAT Heuristics Combinations Optimization via Large Language Models](https://arxiv.org/abs/2509.12602)
*Minyu Chen,Guoqiang Li*

Main category: cs.AI

TL;DR: 提出DaSAThco框架解决SAT求解器配置缺乏泛化性问题，实验表明其性能优且有强泛化性。


<details>
  <summary>Details</summary>
Motivation: SAT问题异质性使单一最优配置不可达，现有自动化方法缺乏泛化性且新问题需重新优化。

Method: 引入DaSAThco框架，利用大语言模型和问题原型生成启发式集合，学习自适应选择机制形成映射。

Result: DaSAThco性能优越，在非自适应方法受限的域外泛化中表现稳健。

Conclusion: 为复杂可配置系统的自动化算法设计提供更具扩展性和实用性的途径。

Abstract: The performance of Conflict-Driven Clause Learning solvers hinges on internal
heuristics, yet the heterogeneity of SAT problems makes a single, universally
optimal configuration unattainable. While prior automated methods can find
specialized configurations for specific problem families, this dataset-specific
approach lacks generalizability and requires costly re-optimization for new
problem types. We introduce DaSAThco, a framework that addresses this challenge
by learning a generalizable mapping from instance features to tailored
heuristic ensembles, enabling a train-once, adapt-broadly model. Our framework
uses a Large Language Model, guided by systematically defined Problem
Archetypes, to generate a diverse portfolio of specialized heuristic ensembles
and subsequently learns an adaptive selection mechanism to form the final
mapping. Experiments show that DaSAThco achieves superior performance and, most
notably, demonstrates robust out-of-domain generalization where non-adaptive
methods show limitations. Our work establishes a more scalable and practical
path toward automated algorithm design for complex, configurable systems.

</details>


### [20] [Analogy-Driven Financial Chain-of-Thought (AD-FCoT): A Prompting Approach for Financial Sentiment Analysis](https://arxiv.org/abs/2509.12611)
*Anmol Singhal Navya Singhal*

Main category: cs.AI

TL;DR: 提出AD - FCoT框架用于金融新闻情感预测，纯提示操作，实验显示其在情感分类和与市场回报相关性上表现优。


<details>
  <summary>Details</summary>
Motivation: 现有金融新闻情感分析方法难捕捉复杂经济背景且推理不透明，可靠性不足。

Method: 提出Analogy - Driven Financial Chain - of - Thought (AD - FCoT)框架，结合类比推理和思维链提示，引导大语言模型将新事件与历史场景类比。

Result: 在数千篇新闻文章实验中，AD - FCoT在情感分类准确率上超越强基线，与市场回报相关性更高，生成解释符合领域专业知识。

Conclusion: AD - FCoT是一种有效且可解释的金融新闻情感分析方法，适用于现实金融分析。

Abstract: Financial news sentiment analysis is crucial for anticipating market
movements. With the rise of AI techniques such as Large Language Models (LLMs),
which demonstrate strong text understanding capabilities, there has been
renewed interest in enhancing these systems. Existing methods, however, often
struggle to capture the complex economic context of news and lack transparent
reasoning, which undermines their reliability. We propose Analogy-Driven
Financial Chain-of-Thought (AD-FCoT), a prompting framework that integrates
analogical reasoning with chain-of-thought (CoT) prompting for sentiment
prediction on historical financial news. AD-FCoT guides LLMs to draw parallels
between new events and relevant historical scenarios with known outcomes,
embedding these analogies into a structured, step-by-step reasoning chain. To
our knowledge, this is among the first approaches to explicitly combine
analogical examples with CoT reasoning in finance. Operating purely through
prompting, AD-FCoT requires no additional training data or fine-tuning and
leverages the model's internal financial knowledge to generate rationales that
mirror human analytical reasoning. Experiments on thousands of news articles
show that AD-FCoT outperforms strong baselines in sentiment classification
accuracy and achieves substantially higher correlation with market returns. Its
generated explanations also align with domain expertise, providing
interpretable insights suitable for real-world financial analysis.

</details>


### [21] [GBV-SQL: Guided Generation and SQL2Text Back-Translation Validation for Multi-Agent Text2SQL](https://arxiv.org/abs/2509.12612)
*Daojun Chen,Xi Wang,Shenyuan Ren,Qingzhi Ma,Pengpeng Zhao,An Liu*

Main category: cs.AI

TL;DR: 提出GBV - SQL多智能体框架进行语义验证，指出基准测试存在问题，在BIRD和Spider基准测试上取得良好结果。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在Text2SQL生成中存在的语义差距问题，当前评估受基准测试质量影响。

Method: 提出GBV - SQL框架，引入SQL2Text反向翻译验证的引导式生成机制，还提出“Gold Errors”的正式类型。

Result: 在BIRD基准测试上执行准确率达63.23%，提升5.8%；去除有缺陷示例后，在Spider基准测试开发集和测试集执行准确率分别达96.5%和97.6%。

Conclusion: 提供了语义验证的强大框架，强调了基准测试完整性的重要性，指出需要更严格的数据集整理。

Abstract: While Large Language Models have significantly advanced Text2SQL generation,
a critical semantic gap persists where syntactically valid queries often
misinterpret user intent. To mitigate this challenge, we propose GBV-SQL, a
novel multi-agent framework that introduces Guided Generation with SQL2Text
Back-translation Validation. This mechanism uses a specialized agent to
translate the generated SQL back into natural language, which verifies its
logical alignment with the original question. Critically, our investigation
reveals that current evaluation is undermined by a systemic issue: the poor
quality of the benchmarks themselves. We introduce a formal typology for "Gold
Errors", which are pervasive flaws in the ground-truth data, and demonstrate
how they obscure true model performance. On the challenging BIRD benchmark,
GBV-SQL achieves 63.23% execution accuracy, a 5.8% absolute improvement. After
removing flawed examples, GBV-SQL achieves 96.5% (dev) and 97.6% (test)
execution accuracy on the Spider benchmark. Our work offers both a robust
framework for semantic validation and a critical perspective on benchmark
integrity, highlighting the need for more rigorous dataset curation.

</details>


### [22] [Mob-based cattle weight gain forecasting using ML models](https://arxiv.org/abs/2509.12615)
*Muhammad Riaz Hasib Hossain,Rafiqul Islam,Shawn R McGrath,Md Zahidul Islam,David Lamb*

Main category: cs.AI

TL;DR: 提出MB CWG技术预测牛群增重，用RF、SVR和LSTM模型对比，RF表现佳，开发自动化预处理工具。


<details>
  <summary>Details</summary>
Motivation: 预测牛群增重可助大型养殖场优化饲养策略、做育种选择、降低气候和市场风险。

Method: 提出MB CWG技术，用RF、SVR和LSTM模型预测，用四个数据集评估，含108头牛756个样本及天气数据。

Result: RF模型在各数据集表现优于SVR和LSTM，含天气和年龄因素时R^2达0.973等，开发自动化预处理工具。

Conclusion: RF是可变条件下预测牛增重的有力工具，年龄和气候因素影响增重趋势，工具可辅助分析研究。

Abstract: Forecasting mob based cattle weight gain (MB CWG) may benefit large livestock
farms, allowing farmers to refine their feeding strategies, make educated
breeding choices, and reduce risks linked to climate variability and market
fluctuations. In this paper, a novel technique termed MB CWG is proposed to
forecast the one month advanced weight gain of herd based cattle using
historical data collected from the Charles Sturt University Farm. This research
employs a Random Forest (RF) model, comparing its performance against Support
Vector Regression (SVR) and Long Short Term Memory (LSTM) models for monthly
weight gain prediction. Four datasets were used to evaluate the performance of
models, using 756 sample data from 108 herd-based cattle, along with weather
data (rainfall and temperature) influencing CWG. The RF model performs better
than the SVR and LSTM models across all datasets, achieving an R^2 of 0.973,
RMSE of 0.040, and MAE of 0.033 when both weather and age factors were
included. The results indicate that including both weather and age factors
significantly improves the accuracy of weight gain predictions, with the RF
model outperforming the SVR and LSTM models in all scenarios. These findings
demonstrate the potential of RF as a robust tool for forecasting cattle weight
gain in variable conditions, highlighting the influence of age and climatic
factors on herd based weight trends. This study has also developed an
innovative automated pre processing tool to generate a benchmark dataset for MB
CWG predictive models. The tool is publicly available on GitHub and can assist
in preparing datasets for current and future analytical research..

</details>


### [23] [ECG-aBcDe: Overcoming Model Dependence, Encoding ECG into a Universal Language for Any LLM](https://arxiv.org/abs/2509.12625)
*Yong Xia,Jingxuan Li,YeTeng Sun,Jiarui Bu*

Main category: cs.AI

TL;DR: 本文介绍了新的心电图编码方法ECG - aBcDe，解决现有大语言模型在心电图分析中的问题，取得有竞争力的性能，证明新范式可行性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型用于心电图分析存在可迁移性、时间尺度信息学习和可解释性的问题，现有方法有模型特定编码器阻碍跨模型迁移等。

Method: 引入ECG - aBcDe编码方法，将心电图信号转换为通用心电图语言，构建混合数据集对预训练大语言模型直接微调，利用其双向转换性提取注意力热图。

Result: 在ROUGE - L和METEOR上取得有竞争力的性能，BLEU - 4有显著提升，数据集内和跨数据集评估分别提升2.8倍和3.9倍，得分达42.58和30.76。

Conclusion: 提出的新范式用于心电图分析与大语言模型集成是可行的。

Abstract: Large Language Models (LLMs) hold significant promise for electrocardiogram
(ECG) analysis, yet challenges remain regarding transferability, time-scale
information learning, and interpretability. Current methods suffer from
model-specific ECG encoders, hindering transfer across LLMs. Furthermore, LLMs
struggle to capture crucial time-scale information inherent in ECGs due to
Transformer limitations. And their black-box nature limits clinical adoption.
To address these limitations, we introduce ECG-aBcDe, a novel ECG encoding
method that transforms ECG signals into a universal ECG language readily
interpretable by any LLM. By constructing a hybrid dataset of ECG language and
natural language, ECG-aBcDe enables direct fine-tuning of pre-trained LLMs
without architectural modifications, achieving "construct once, use anywhere"
capability. Moreover, the bidirectional convertibility between ECG and ECG
language of ECG-aBcDe allows for extracting attention heatmaps from ECG
signals, significantly enhancing interpretability. Finally, ECG-aBcDe
explicitly represents time-scale information, mitigating Transformer
limitations. This work presents a new paradigm for integrating ECG analysis
with LLMs. Compared with existing methods, our method achieves competitive
performance on ROUGE-L and METEOR. Notably, it delivers significant
improvements in the BLEU-4, with improvements of 2.8 times and 3.9 times in
in-dataset and cross-dataset evaluations, respectively, reaching scores of
42.58 and 30.76. These results provide strong evidence for the feasibility of
the new paradigm.

</details>


### [24] [Learn to Relax with Large Language Models: Solving Nonlinear Combinatorial Optimization Problems via Bidirectional Coevolution](https://arxiv.org/abs/2509.12643)
*Beidan Liu,Zhengqiu Zhu,Chen Gao,Yong Zhao,Wei Qi,Quanjun Yin*

Main category: cs.AI

TL;DR: 提出端到端自动约束优化方法AutoCO解决非线性组合优化问题，经实验验证效果优于基线。


<details>
  <summary>Details</summary>
Motivation: 传统约束松弛方法缺乏自动化和可扩展性，基于大语言模型的优化方法不能有效处理复杂约束交互。

Method: 利用结构化大语言模型推理生成约束松弛策略，建立双向（全局 - 局部）协同进化机制。

Result: 在三个具有挑战性的非线性组合优化问题基准测试中，AutoCO表现出一致的有效性和优于基线的性能。

Conclusion: AutoCO方法能有效解决非线性组合优化问题。

Abstract: Nonlinear Combinatorial Optimization Problems (NCOPs) present a formidable
computational hurdle in practice, as their nonconvex nature gives rise to
multi-modal solution spaces that defy efficient optimization. Traditional
constraint relaxation approaches rely heavily on expert-driven, iterative
design processes that lack systematic automation and scalable adaptability.
While recent Large Language Model (LLM)-based optimization methods show promise
for autonomous problem-solving, they predominantly function as passive
constraint validators rather than proactive strategy architects, failing to
handle the sophisticated constraint interactions inherent to NCOPs.To address
these limitations, we introduce the first end-to-end \textbf{Auto}mated
\textbf{C}onstraint \textbf{O}ptimization (AutoCO) method, which revolutionizes
NCOPs resolution through learning to relax with LLMs.Specifically, we leverage
structured LLM reasoning to generate constraint relaxation strategies, which
are dynamically evolving with algorithmic principles and executable code
through a unified triple-representation scheme. We further establish a novel
bidirectional (global-local) coevolution mechanism that synergistically
integrates Evolutionary Algorithms for intensive local refinement with Monte
Carlo Tree Search for systematic global strategy space exploration, ensuring
optimal balance between intensification and diversification in fragmented
solution spaces. Finally, comprehensive experiments on three challenging NCOP
benchmarks validate AutoCO's consistent effectiveness and superior performance
over the baselines.

</details>


### [25] [Large Language Models Imitate Logical Reasoning, but at what Cost?](https://arxiv.org/abs/2509.12645)
*Lachlan McGinness,Peter Baumgartner*

Main category: cs.AI

TL;DR: 对前沿大语言模型推理能力进行18个月纵向研究，评估准确性和忠实度，分析性能提升原因，还提出神经符号架构，降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 评估前沿大语言模型的推理能力，探索提升性能方法和降低计算成本途径。

Method: 测量三个领先模型在不同时间对PrOntoQA数据集判断题的准确性和对上下文学习推理策略的忠实度；提出神经符号架构，用小参数LLM转换问题，用Z3求解。

Result: 2023 - 2024性能提升归因于隐藏思维链提示，2024 - 2025因引入思维模型性能显著提升；神经符号方法大幅降低计算成本且保持高性能；推理FLOPs近似计算误差在10%内。

Conclusion: 思维链提示和思维模型有助于提升大语言模型推理性能，神经符号架构能有效降低计算成本。

Abstract: We present a longitudinal study which evaluates the reasoning capability of
frontier Large Language Models over an eighteen month period. We measured the
accuracy of three leading models from December 2023, September 2024 and June
2025 on true or false questions from the PrOntoQA dataset and their
faithfulness to reasoning strategies provided through in-context learning. The
improvement in performance from 2023 to 2024 can be attributed to hidden Chain
of Thought prompting. The introduction of thinking models allowed for
significant improvement in model performance between 2024 and 2025.
  We then present a neuro-symbolic architecture which uses LLMs of less than 15
billion parameters to translate the problems into a standardised form. We then
parse the standardised forms of the problems into a program to be solved by Z3,
an SMT solver, to determine the satisfiability of the query. We report the
number of prompt and completion tokens as well as the computational cost in
FLOPs for open source models. The neuro-symbolic approach significantly reduces
the computational cost while maintaining near perfect performance. The common
approximation that the number of inference FLOPs is double the product of the
active parameters and total tokens was accurate within 10\% for all
experiments.

</details>


### [26] [Zero-shot Graph Reasoning via Retrieval Augmented Framework with LLMs](https://arxiv.org/abs/2509.12743)
*Hanqing Li,Kiran Sheena Jyothi,Henry Liang,Sharika Mahadevan,Diego Klabjan*

Main category: cs.AI

TL;DR: 提出无训练方法GRRAF解决图推理任务，实验显示在多数任务达100%准确率，能有效扩展到大型图。


<details>
  <summary>Details</summary>
Motivation: 现有图推理方法需大量微调或依赖预定义算法，有局限性，需新方法。

Method: 利用检索增强生成和大语言模型代码生成能力，将目标图存于图数据库，提示LLM生成可执行代码查询，含错误反馈和超时机制。

Result: 在GraphInstruct数据集实验，多数图推理任务达100%准确率，子图匹配表现也高，能有效扩展到10000节点大图。

Conclusion: GRRAF是解决图推理任务的有效方法，克服现有方法局限，性能良好且可扩展性强。

Abstract: We propose a new, training-free method, Graph Reasoning via Retrieval
Augmented Framework (GRRAF), that harnesses retrieval-augmented generation
(RAG) alongside the code-generation capabilities of large language models
(LLMs) to address a wide range of graph reasoning tasks. In GRRAF, the target
graph is stored in a graph database, and the LLM is prompted to generate
executable code queries that retrieve the necessary information. This approach
circumvents the limitations of existing methods that require extensive
finetuning or depend on predefined algorithms, and it incorporates an error
feedback loop with a time-out mechanism to ensure both correctness and
efficiency. Experimental evaluations on the GraphInstruct dataset reveal that
GRRAF achieves 100% accuracy on most graph reasoning tasks, including cycle
detection, bipartite graph checks, shortest path computation, and maximum flow,
while maintaining consistent token costs regardless of graph sizes. Imperfect
but still very high performance is observed on subgraph matching. Notably,
GRRAF scales effectively to large graphs with up to 10,000 nodes.

</details>


### [27] [H$^2$R: Hierarchical Hindsight Reflection for Multi-Task LLM Agents](https://arxiv.org/abs/2509.12810)
*Shicheng Ye,Chao Yu,Kaiqiang Ke,Chengdong Xu,Yinqi Wei*

Main category: cs.AI

TL;DR: 提出分层记忆架构和Hierarchical Hindsight Reflection (H$^2$R)机制，实现细粒度知识转移，实验显示其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的智能体在知识转移时将先验经验和知识视为整体单元，导致转移效率低且粒度粗。

Method: 提出分层记忆架构，将高层规划记忆与低层执行记忆解耦；引入H$^2$R机制从过去交互中提取可复用的分层知识，测试时分别检索高低层记忆。

Result: 在两个基准测试中，H$^2$R能提高泛化和决策性能，优于Expel等基线。

Conclusion: 所提出的分层记忆架构和H$^2$R机制有效，能实现细粒度知识转移，提升基于大语言模型智能体性能。

Abstract: Large language model (LLM)-based agents have shown strong potential in
multi-task scenarios, owing to their ability to transfer knowledge across
diverse tasks. However, existing approaches often treat prior experiences and
knowledge as monolithic units, leading to inefficient and coarse-grained
knowledge transfer. In this work, we propose a novel hierarchical memory
architecture that enables fine-grained knowledge transfer by decoupling
high-level planning memory from low-level execution memory. To construct and
refine these hierarchical memories, we introduce Hierarchical Hindsight
Reflection (H$^2$R), a mechanism that distills reusable and hierarchical
knowledge from past agent-environment interactions. At test time, H$^2$R
performs retrievals of high-level and low-level memories separately, allowing
LLM-based agents to efficiently access and utilize task-relevant knowledge for
new tasks.Experimental results across two benchmarks demonstrate that H$^2$R
can improve generalization and decision-making performance, outperforming prior
baselines such as Expel.

</details>


### [28] [LTA-thinker: Latent Thought-Augmented Training Framework for Large Language Models on Complex Reasoning](https://arxiv.org/abs/2509.12875)
*Jiaqi Wang,Binquan Ji,Haibo Luo,Yiyang Qi,Ruiting Li,Huiyan Wang,Yuantao Han,Cangyi Yang,jiaxu Zhang,Feiliang Ren*

Main category: cs.AI

TL;DR: 本文提出潜思维增强训练框架LTA - Thinker提升大语言模型推理性能，实验达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法在高效生成和利用高质量潜思维方面存在瓶颈，需要提升推理性能。

Method: 构建基于可学习先验的潜思维生成架构提升方差分布；引入基于分布的定向优化范式，结合标准监督微调损失与语义对齐损失、推理聚焦损失进行多目标协同训练。

Result: LTA - Thinker在各基线模型中达到SOTA性能，有更高性能上限和更好的扩展效果。

Conclusion: LTA - Thinker能有效提升大语言模型的推理性能。

Abstract: Complex Reasoning in Large Language Models can be dynamically optimized using
Test-Time Scaling (TTS) to mitigate Overthinking. Methods such as Coconut,
SoftCoT and its variant are effective in continuous latent space inference, the
core bottleneck still lies in the efficient generation and utilization of
high-quality Latent Thought. Drawing from the theory of SoftCoT++ that a larger
variance in the generated Latent Thought distribution more closely approximates
the golden truth distribution, we propose a Latent Thought-Augmented Training
Framework--LTA-Thinker, which improves distributional variance and enhances
reasoning performance from two perspectives. First, LTA-Thinker constructs a
Latent Thought generation architecture based on a learnable prior. This
architecture aims to increase the variance distribution of generated Latent
Thought Vectors in order to simplify the overall structure and raise the
performance ceiling. Second, LTA-Thinker introduces a distribution-based
directional optimization paradigm that jointly constrains both distribution
locality and distribution scale. This mechanism improves information efficiency
and computational cost through a multi-objective co-training strategy, which
combines standard Supervised Fine-Tuning (SFT) loss with two novel losses:
Semantic Alignment Loss, which utilizes KL divergence to ensure that the Latent
Thought is highly relevant to the semantics of the question; Reasoning Focus
Loss, which utilizes a contrastive learning mechanism to guide the model to
focus on the most critical reasoning steps. Experiments show that LTA-thinker
achieves state-of-the-art (SOTA) performance among various baselines and
demonstrates a higher performance ceiling and better scaling effects.

</details>


### [29] [Stochastic Streets: A Walk Through Random LLM Address Generation in four European Cities](https://arxiv.org/abs/2509.12914)
*Tairan Fu,David Campo-Nazareno,Javier Coronado-Blázquez,Javier Conde,Pedro Reviriego,Fabrizio Lombardi*

Main category: cs.AI

TL;DR: 探讨大语言模型能否为欧洲城市生成随机街道地址


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在为欧洲城市生成随机街道地址方面的能力

Method: 未提及

Result: 未提及

Conclusion: 未提及

Abstract: Large Language Models (LLMs) are capable of solving complex math problems or
answer difficult questions on almost any topic, but can they generate random
street addresses for European cities?

</details>


### [30] [Population Estimation using Deep Learning over Gandhinagar Urban Area](https://arxiv.org/abs/2509.12926)
*Jai Singla,Peal Jotania,Keivalya Pandya*

Main category: cs.AI

TL;DR: 本文提出用深度学习方法结合高分辨率卫星影像等数据进行人口估计，实验效果好，为城市资源管理提供可扩展工具。


<details>
  <summary>Details</summary>
Motivation: 传统人口估计方法昂贵、耗时且依赖人力，需新方法。

Method: 结合CNN对建筑分类、ANN估计人口，利用约4.8万个建筑足迹数据。

Result: 模型F1分数达0.9936，估计甘地讷格尔人口为278,954。

Conclusion: 该自动化方法解决传统普查方法局限，为城市提供可扩展资源管理工具，展现AI地理空间分析在城市治理中的效率。

Abstract: Population estimation is crucial for various applications, from resource
allocation to urban planning. Traditional methods such as surveys and censuses
are expensive, time-consuming and also heavily dependent on human resources,
requiring significant manpower for data collection and processing. In this
study a deep learning solution is proposed to estimate population using high
resolution (0.3 m) satellite imagery, Digital Elevation Models (DEM) of 0.5m
resolution and vector boundaries. Proposed method combines Convolution Neural
Network (CNN) architecture for classification task to classify buildings as
residential and non-residential and Artificial Neural Network (ANN)
architecture to estimate the population. Approx. 48k building footprints over
Gandhinagar urban area are utilized containing both residential and
non-residential, with residential categories further used for building-level
population estimation. Experimental results on a large-scale dataset
demonstrate the effectiveness of our model, achieving an impressive overall
F1-score of 0.9936. The proposed system employs advanced geospatial analysis
with high spatial resolution to estimate Gandhinagar population at 278,954. By
integrating real-time data updates, standardized metrics, and infrastructure
planning capabilities, this automated approach addresses critical limitations
of conventional census-based methodologies. The framework provides
municipalities with a scalable and replicable tool for optimized resource
management in rapidly urbanizing cities, showcasing the efficiency of AI-driven
geospatial analytics in enhancing data-driven urban governance.

</details>


### [31] [The Anatomy of Alignment: Decomposing Preference Optimization by Steering Sparse Features](https://arxiv.org/abs/2509.12934)
*Jeremias Ferrao,Matthijs van der Lende,Ilija Lichkovski,Clement Neo*

Main category: cs.AI

TL;DR: 提出透明对齐框架FSRL进行偏好优化，分析发现偏好优化奖励风格呈现，旨在提供可解释模型控制和诊断对齐机制的工具。


<details>
  <summary>Details</summary>
Motivation: 现有基于人类反馈的强化学习方法参数变化分散不透明，难以了解模型内化内容，需更透明的对齐方法。

Method: 引入Feature Steering with Reinforcement Learning (FSRL)，训练轻量级适配器，通过调节稀疏自编码器的可解释特征来引导行为。

Result: FSRL是有效的偏好优化方法，与当前RLHF方法相当；训练的适配器策略更注重风格特征而非明确的对齐概念。

Conclusion: FSRL可用于可解释的模型控制和诊断模型对齐的内部机制。

Abstract: Aligning large language models is critical for their usability and safety.
However, the prevailing approach of Reinforcement Learning from Human Feedback
(RLHF) induces diffuse, opaque parameter changes, making it difficult to
discern what the model has internalized. Hence, we introduce Feature Steering
with Reinforcement Learning (FSRL), a transparent alignment framework that
trains a lightweight adapter to steer behavior by modulating interpretable
features from a Sparse Autoencoder (SAE). First, we demonstrate that FSRL is an
effective method for preference optimization and is comparable with current
RLHF methods. We then perform mechanistic analysis on the trained adapter, and
find that its policy systematically promotes style features over explicit
alignment concepts, suggesting that the preference optimization process rewards
stylistic presentation as a proxy for quality. Ultimately, we hope that FSRL
provides a tool for both interpretable model control and diagnosing the
internal mechanisms of alignment.

</details>


### [32] [Black-box Model Merging for Language-Model-as-a-Service with Massive Model Repositories](https://arxiv.org/abs/2509.12951)
*Shilian Chen,Jie Zhou,Tianyu Huai,Yujiang Lu,Junsong Li,Bihao Zhan,Qianjun Pan,Yutao Yang,Xin Li,Qin Chen,Hang Yan,Liang He*

Main category: cs.AI

TL;DR: 文章聚焦黑盒模型合并难题，提出基于进化算法的无导数优化框架Evo - Merging，实验显示该方法表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法多依赖可访问的模型参数，但像GPT - 4这类大语言模型以黑盒服务形式提供，模型权重不可用，带来黑盒模型合并挑战。

Method: 提出基于进化算法的无导数优化框架Evo - Merging，含基于稀疏性的去噪和符号感知缩放两个关键组件，并对非对称稀疏化给出正式证明和理论分析。

Result: 在一系列任务上取得了最先进的结果，显著优于现有的强基线。

Conclusion: 所提出的Evo - Merging方法能有效解决黑盒模型合并问题，在模型合并方面表现出色。

Abstract: Model merging refers to the process of integrating multiple distinct models
into a unified model that preserves and combines the strengths and capabilities
of the individual models. Most existing approaches rely on task vectors to
combine models, typically under the assumption that model parameters are
accessible. However, for extremely large language models (LLMs) such as GPT-4,
which are often provided solely as black-box services through API interfaces
(Language-Model-as-a-Service), model weights are not available to end users.
This presents a significant challenge, which we refer to as black-box model
merging (BMM) with massive LLMs. To address this challenge, we propose a
derivative-free optimization framework based on the evolutionary algorithm
(Evo-Merging) that enables effective model merging using only inference-time
API queries. Our method consists of two key components: (1) sparsity-based
denoising, designed to identify and filter out irrelevant or redundant
information across models, and (2) sign-aware scaling, which dynamically
computes optimal combination weights for the relevant models based on their
performance. We also provide a formal justification, along with a theoretical
analysis, for our asymmetric sparsification. Extensive experimental evaluations
demonstrate that our approach achieves state-of-the-art results on a range of
tasks, significantly outperforming existing strong baselines.

</details>


### [33] [Forget What's Sensitive, Remember What Matters: Token-Level Differential Privacy in Memory Sculpting for Continual Learning](https://arxiv.org/abs/2509.12958)
*Bihao Zhan,Jie Zhou,Junsong Li,Yutao Yang,Shilian Chen,Qianjun Pan,Xin Li,Wen Wu,Xingjiao Wu,Qin Chen,Hang Yan,Liang He*

Main category: cs.AI

TL;DR: 提出隐私增强的持续学习框架PeCL，平衡隐私保护和模型效用。


<details>
  <summary>Details</summary>
Motivation: 持续学习模型存在隐私挑战，传统隐私方法会导致模型效用大幅下降，阻碍其在隐私敏感领域的应用。

Method: 引入基于单个令牌语义敏感性的令牌级动态差分隐私策略，自适应分配隐私预算；集成隐私引导的内存塑造模块，智能遗忘敏感信息并保留关键历史知识。

Result: PeCL在隐私保护和模型效用之间取得了更好的平衡，在保证隐私的同时，在先前任务上保持了较高的准确率，优于基线模型。

Conclusion: PeCL框架能有效解决持续学习模型的隐私问题，实现隐私保护和模型效用的良好平衡。

Abstract: Continual Learning (CL) models, while adept at sequential knowledge
acquisition, face significant and often overlooked privacy challenges due to
accumulating diverse information. Traditional privacy methods, like a uniform
Differential Privacy (DP) budget, indiscriminately protect all data, leading to
substantial model utility degradation and hindering CL deployment in
privacy-sensitive areas. To overcome this, we propose a privacy-enhanced
continual learning (PeCL) framework that forgets what's sensitive and remembers
what matters. Our approach first introduces a token-level dynamic Differential
Privacy strategy that adaptively allocates privacy budgets based on the
semantic sensitivity of individual tokens. This ensures robust protection for
private entities while minimizing noise injection for non-sensitive, general
knowledge. Second, we integrate a privacy-guided memory sculpting module. This
module leverages the sensitivity analysis from our dynamic DP mechanism to
intelligently forget sensitive information from the model's memory and
parameters, while explicitly preserving the task-invariant historical knowledge
crucial for mitigating catastrophic forgetting. Extensive experiments show that
PeCL achieves a superior balance between privacy preserving and model utility,
outperforming baseline models by maintaining high accuracy on previous tasks
while ensuring robust privacy.

</details>


### [34] [Toward PDDL Planning Copilot](https://arxiv.org/abs/2509.12987)
*Yarin Benyamin,Argaman Mordoch,Shahaf S. Shperberg,Roni Stern*

Main category: cs.AI

TL;DR: 本文提出Planning Copilot聊天机器人，集成多种规划工具，利用MCP协议让大语言模型执行规划任务，实验表明其效果优于无工具的大模型和GPT - 5。


<details>
  <summary>Details</summary>
Motivation: 大语言模型自身缺乏可靠的长期规划能力，需要工具辅助。

Method: 引入Planning Copilot，利用MCP协议集成多种规划工具，用户可用自然语言调用；用三个开源大语言模型进行实证评估，与Chat GPT - 5进行有限定性比较。

Result: Planning Copilot在执行规划任务上远超无规划工具的相同大语言模型；显著优于GPT - 5。

Conclusion: 专用规划工具可能是使大语言模型能够执行规划任务的有效方法。

Abstract: Large Language Models (LLMs) are increasingly being used as autonomous agents
capable of performing complicated tasks. However, they lack the ability to
perform reliable long-horizon planning on their own. This paper bridges this
gap by introducing the Planning Copilot, a chatbot that integrates multiple
planning tools and allows users to invoke them through instructions in natural
language. The Planning Copilot leverages the Model Context Protocol (MCP), a
recently developed standard for connecting LLMs with external tools and
systems. This approach allows using any LLM that supports MCP without
domain-specific fine-tuning. Our Planning Copilot supports common planning
tasks such as checking the syntax of planning problems, selecting an
appropriate planner, calling it, validating the plan it generates, and
simulating their execution. We empirically evaluate the ability of our Planning
Copilot to perform these tasks using three open-source LLMs. The results show
that the Planning Copilot highly outperforms using the same LLMs without the
planning tools. We also conducted a limited qualitative comparison of our tool
against Chat GPT-5, a very recent commercial LLM. Our results shows that our
Planning Copilot significantly outperforms GPT-5 despite relying on a much
smaller LLM. This suggests dedicated planning tools may be an effective way to
enable LLMs to perform planning tasks.

</details>


### [35] [Data-driven Methods of Extracting Text Structure and Information Transfer](https://arxiv.org/abs/2509.12999)
*Shinichi Honna,Taichi Murayama,Akira Matsui*

Main category: cs.AI

TL;DR: 研究不同文本媒介中结构原则，发现成功依赖各媒介特定结构约束，失败形式多样。


<details>
  <summary>Details</summary>
Motivation: 验证安娜·卡列尼娜原则（AKP）及其反向原则，以及有序和嘈杂两种模式在不同文本媒介中的适用性。

Method: 将文本表示为功能块序列，评估过渡顺序和位置的收敛性。

Result: 不同媒介遵循不同结构原则，小说顺序上遵循反向AKP，维基百科结合AKP和有序模式，学术论文顺序上是反向AKP但位置嘈杂，电影按类型发散。

Conclusion: 成功取决于各媒介特定的结构约束，不同领域失败形式不同。

Abstract: The Anna Karenina Principle (AKP) holds that success requires satisfying a
small set of essential conditions, whereas failure takes diverse forms. We test
AKP, its reverse, and two further patterns described as ordered and noisy
across novels, online encyclopedias, research papers, and movies. Texts are
represented as sequences of functional blocks, and convergence is assessed in
transition order and position. Results show that structural principles vary by
medium: novels follow reverse AKP in order, Wikipedia combines AKP with ordered
patterns, academic papers display reverse AKP in order but remain noisy in
position, and movies diverge by genre. Success therefore depends on structural
constraints that are specific to each medium, while failure assumes different
shapes across domains.

</details>


### [36] [A Visualized Framework for Event Cooperation with Generative Agents](https://arxiv.org/abs/2509.13011)
*Yuyang Tian,Shunqiang Mao,Wenchang Gao,Lanlan Qiu,Tianxing He*

Main category: cs.AI

TL;DR: 本文开发可视化平台MiniAgentPro并引入综合测试集评估大语言模型驱动的智能体社会模拟能力，评估显示GPT - 4o在基础场景表现好，复杂场景有协调挑战。


<details>
  <summary>Details</summary>
Motivation: 现有框架缺乏对事件组织的系统评估，且未与物理环境进行可视化集成，限制智能体真实交互能力。

Method: 开发可视化平台MiniAgentPro，包含地图编辑器和模拟播放器；引入含八种不同事件场景及基础和困难变体的综合测试集。

Result: 使用GPT - 4o进行评估，在基础设置中表现良好，但在困难变体中存在协调挑战。

Conclusion: MiniAgentPro平台及测试集可用于评估大语言模型驱动的智能体社会模拟能力，不同场景表现有差异。

Abstract: Large Language Models (LLMs) have revolutionized the simulation of agent
societies, enabling autonomous planning, memory formation, and social
interactions. However, existing frameworks often overlook systematic
evaluations for event organization and lack visualized integration with
physically grounded environments, limiting agents' ability to navigate spaces
and interact with items realistically. We develop MiniAgentPro, a visualization
platform featuring an intuitive map editor for customizing environments and a
simulation player with smooth animations. Based on this tool, we introduce a
comprehensive test set comprising eight diverse event scenarios with basic and
hard variants to assess agents' ability. Evaluations using GPT-4o demonstrate
strong performance in basic settings but highlight coordination challenges in
hard variants.

</details>


### [37] [Reasoning with Preference Constraints: A Benchmark for Language Models in Many-to-One Matching Markets](https://arxiv.org/abs/2509.13131)
*Marylou Fauchard,Florian Carichon,Margarida Carvalho,Golnoosh Farnadi*

Main category: cs.AI

TL;DR: 本文引入大学招生问题基准评估大语言模型在匹配问题上的表现，发现模型难以满足所有评估标准，推理型模型表现更佳，不同提示策略效果不一，迭代提示性能非单调。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在匹配问题（需在偏好和结构约束下推理）上的应用研究不足，为填补此空白开展研究。

Method: 引入369个大学招生问题实例的基准，评估大语言模型在可行性、稳定性和最优性方面的表现，测试多种提示策略和迭代提示。

Result: 大语言模型难以始终满足所有评估标准；推理型模型显著优于传统模型；不同提示策略效果不同，无始终最佳者；迭代提示性能非单调。

Conclusion: 为组合优化问题中模型推理性能和提示策略有效性提供新视角。

Abstract: Recent advances in reasoning with large language models (LLMs) have
demonstrated strong performance on complex mathematical tasks, including
combinatorial optimization. Techniques such as Chain-of-Thought and In-Context
Learning have further enhanced this capability, making LLMs both powerful and
accessible tools for a wide range of users, including non-experts. However,
applying LLMs to matching problems, which require reasoning under preferential
and structural constraints, remains underexplored. To address this gap, we
introduce a novel benchmark of 369 instances of the College Admission Problem,
a canonical example of a matching problem with preferences, to evaluate LLMs
across key dimensions: feasibility, stability, and optimality. We employ this
benchmark to assess the performance of several open-weight LLMs. Our results
first reveal that while LLMs can satisfy certain constraints, they struggle to
meet all evaluation criteria consistently. They also show that reasoning LLMs,
like QwQ and GPT-oss, significantly outperform traditional models such as
Llama, Qwen or Mistral, defined here as models used without any dedicated
reasoning mechanisms. Moreover, we observed that LLMs reacted differently to
the various prompting strategies tested, which include Chain-of-Thought,
In-Context Learning and role-based prompting, with no prompt consistently
offering the best performance. Finally, we report the performances from
iterative prompting with auto-generated feedback and show that they are not
monotonic; they can peak early and then significantly decline in later
attempts. Overall, this work offers a new perspective on model reasoning
performance and the effectiveness of prompting strategies in combinatorial
optimization problems with preferential constraints.

</details>


### [38] [Agentic AI for Financial Crime Compliance](https://arxiv.org/abs/2509.13137)
*Henrik Axelsen,Valdemar Licht,Jan Damsgaard*

Main category: cs.AI

TL;DR: 本文介绍为数字原生金融平台设计部署的用于金融犯罪合规的智能体AI系统，展示自动化结合问责治理结构支持高风险监管环境信任的作用。


<details>
  <summary>Details</summary>
Motivation: 金融犯罪合规成本和复杂性不断上升且效果未显著提升，现有AI解决方案不透明且不符监管期望。

Method: 通过与金融科技公司和监管利益相关者进行行动设计研究（ADR）开发系统，采用以工件为中心的建模，为自主智能体分配角色等。

Result: 得到参考架构、真实世界的原型，以及智能体AI在监管约束下重构合规工作流程的见解。

Conclusion: 自动化嵌入问责治理结构可支持高风险监管环境中的透明度和机构信任，扩展了人工智能驱动合规的信息系统文献。

Abstract: The cost and complexity of financial crime compliance (FCC) continue to rise,
often without measurable improvements in effectiveness. While AI offers
potential, most solutions remain opaque and poorly aligned with regulatory
expectations. This paper presents the design and deployment of an agentic AI
system for FCC in digitally native financial platforms. Developed through an
Action Design Research (ADR) process with a fintech firm and regulatory
stakeholders, the system automates onboarding, monitoring, investigation, and
reporting, emphasizing explainability, traceability, and compliance-by-design.
Using artifact-centric modeling, it assigns clearly bounded roles to autonomous
agents and enables task-specific model routing and audit logging. The
contribution includes a reference architecture, a real-world prototype, and
insights into how Agentic AI can reconfigure FCC workflows under regulatory
constraints. Our findings extend IS literature on AI-enabled compliance by
demonstrating how automation, when embedded within accountable governance
structures, can support transparency and institutional trust in high-stakes,
regulated environments.

</details>


### [39] [G-CSEA: A Graph-Based Conflict Set Extraction Algorithm for Identifying Infeasibility in Pseudo-Boolean Models](https://arxiv.org/abs/2509.13203)
*Kanishk Garg,Saranya D.,Sanal Kumar,Saurabh Singh,Anupam Purwar*

Main category: cs.AI

TL;DR: 本文针对劳动力调度模型不可行问题，提出图基冲突集提取算法G - CSEA提取冲突集，可选择用QuickXplain最小化得到IIS。


<details>
  <summary>Details</summary>
Motivation: 现有劳动力调度模型因规则约束冲突导致不可行，现有IIS提取方法有局限性，需更好方法识别不可行原因。

Method: 提出G - CSEA算法，在约束传播时构建蕴含图，检测到冲突时追踪所有决策分支的约束，可选择用QuickXplain最小化冲突集得到IIS。

Result: 文中未明确提及具体结果。

Conclusion: 提出新的G - CSEA算法解决现有IIS提取方法的局限性问题。

Abstract: Workforce scheduling involves a variety of rule-based constraints-such as
shift limits, staffing policies, working hour restrictions, and many similar
scheduling rules-which can interact in conflicting ways, leading to infeasible
models. Identifying the underlying causes of such infeasibility is critical for
resolving scheduling issues and restoring feasibility. A common diagnostic
approach is to compute Irreducible Infeasible Subsets (IISs): minimal sets of
constraints that are jointly infeasible but become feasible when any one is
removed. We consider models formulated using pseudo-Boolean constraints with
inequality relations over binary variables, which naturally encode scheduling
logic. Existing IIS extraction methods such as Additive Deletion and
QuickXplain rely on repeated feasibility checks, often incurring large numbers
of solver calls. Dual ray analysis, while effective for LP-based models, may
fail when the relaxed problem is feasible but the underlying pseudo-Boolean
model is not. To address these limitations, we propose Graph-based Conflict Set
Extraction Algorithm (G-CSEA) to extract a conflict set, an approach inspired
by Conflict-Driven Clause Learning (CDCL) in SAT solvers. Our method constructs
an implication graph during constraint propagation and, upon detecting a
conflict, traces all contributing constraints across both decision branches.
The resulting conflict set can optionally be minimized using QuickXplain to
produce an IIS.

</details>


### [40] [Simulating Clinical AI Assistance using Multimodal LLMs: A Case Study in Diabetic Retinopathy](https://arxiv.org/abs/2509.13234)
*Nadim Barakat,William Lotter*

Main category: cs.AI

TL;DR: 评估多模态大语言模型（MLLMs）用于糖尿病视网膜病变（DR）检测及模拟不同输出类型的临床AI辅助能力，发现MLLMs或可改善DR筛查流程。


<details>
  <summary>Details</summary>
Motivation: 当前FDA批准的系统输出有限，难以大规模评估提升临床医生与AI协作性能的最佳输出格式，故评估MLLMs在DR检测及模拟临床AI辅助方面的能力。

Method: 在IDRiD和Messidor - 2数据集上测试GPT - 4o和MedGemma两个模型，进行基线评估、模拟AI辅助和实际AI协作实验。

Result: MedGemma基线表现优于GPT - 4o；两模型可根据模拟输入调整预测，MedGemma更稳定；GPT - 4o在MedGemma描述性输出引导下取得强结果。

Conclusion: MLLMs可改善DR筛查流程，作为可扩展模拟器；开源轻量级模型在资源匮乏地区有价值，描述性输出可增强可解释性和临床信任。

Abstract: Diabetic retinopathy (DR) is a leading cause of blindness worldwide, and AI
systems can expand access to fundus photography screening. Current FDA-cleared
systems primarily provide binary referral outputs, where this minimal output
may limit clinical trust and utility. Yet, determining the most effective
output format to enhance clinician-AI performance is an empirical challenge
that is difficult to assess at scale. We evaluated multimodal large language
models (MLLMs) for DR detection and their ability to simulate clinical AI
assistance across different output types. Two models were tested on IDRiD and
Messidor-2: GPT-4o, a general-purpose MLLM, and MedGemma, an open-source
medical model. Experiments included: (1) baseline evaluation, (2) simulated AI
assistance with synthetic predictions, and (3) actual AI-to-AI collaboration
where GPT-4o incorporated MedGemma outputs. MedGemma outperformed GPT-4o at
baseline, achieving higher sensitivity and AUROC, while GPT-4o showed
near-perfect specificity but low sensitivity. Both models adjusted predictions
based on simulated AI inputs, but GPT-4o's performance collapsed with incorrect
ones, whereas MedGemma remained more stable. In actual collaboration, GPT-4o
achieved strong results when guided by MedGemma's descriptive outputs, even
without direct image access (AUROC up to 0.96). These findings suggest MLLMs
may improve DR screening pipelines and serve as scalable simulators for
studying clinical AI assistance across varying output configurations. Open,
lightweight models such as MedGemma may be especially valuable in low-resource
settings, while descriptive outputs could enhance explainability and clinician
trust in clinical workflows.

</details>


### [41] [A Scenario-Driven Cognitive Approach to Next-Generation AI Memory](https://arxiv.org/abs/2509.13235)
*Linyue Cai,Yuyang Cheng,Xiaoding Shao,Huiming Wang,Yong Zhao,Wei Zhang,Kang Li*

Main category: cs.AI

TL;DR: 随着AI向AGI发展，提出情景驱动方法及COLMA框架助力AGI发展


<details>
  <summary>Details</summary>
Motivation: 当前内存架构存在适应性有限、多模态集成不足和无法支持持续学习等问题，需要强大且类人的内存系统

Method: 提出情景驱动方法，从代表性认知情景中提取功能需求，得到下一代AI内存系统设计原则，并引入COLMA框架

Result: COLMA框架将认知情景、记忆过程和存储机制集成到统一设计中

Conclusion: COLMA为开发具有终身学习和类人推理能力的AI系统提供基础，有助于AGI的实际发展

Abstract: As artificial intelligence advances toward artificial general intelligence
(AGI), the need for robust and human-like memory systems has become
increasingly evident. Current memory architectures often suffer from limited
adaptability, insufficient multimodal integration, and an inability to support
continuous learning. To address these limitations, we propose a scenario-driven
methodology that extracts essential functional requirements from representative
cognitive scenarios, leading to a unified set of design principles for
next-generation AI memory systems. Based on this approach, we introduce the
\textbf{COgnitive Layered Memory Architecture (COLMA)}, a novel framework that
integrates cognitive scenarios, memory processes, and storage mechanisms into a
cohesive design. COLMA provides a structured foundation for developing AI
systems capable of lifelong learning and human-like reasoning, thereby
contributing to the pragmatic development of AGI.

</details>


### [42] [RepIt: Representing Isolated Targets to Steer Language Models](https://arxiv.org/abs/2509.13281)
*Vincent Siu,Nathan W. Henry,Nicholas Crispino,Yang Liu,Dawn Song,Chenguang Wang*

Main category: cs.AI

TL;DR: 提出RepIt框架隔离概念特定表示，在多个大语言模型实现精确干预，同时指出干预可能的风险，为模型行为更精细控制奠基。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型激活引导方法影响范围常超出预期，需隔离更纯的概念向量以实现目标干预和更细致理解模型行为。

Method: 提出RepIt这一简单且数据高效的框架来隔离概念特定表示。

Result: 在五个前沿大语言模型上，RepIt能实现精确干预，选择性抑制目标概念拒绝回答情况，同时保持其他拒绝回答；纠正信号仅集中在100 - 200个神经元，仅用少量示例和计算资源就能提取目标表示。

Conclusion: 通过RepIt解开拒绝向量，表明目标干预可抵消过度泛化，为模型行为更精细控制奠定基础。

Abstract: While activation steering in large language models (LLMs) is a growing area
of research, methods can often incur broader effects than desired. This
motivates isolation of purer concept vectors to enable targeted interventions
and understand LLM behavior at a more granular level. We present RepIt, a
simple and data-efficient framework for isolating concept-specific
representations. Across five frontier LLMs, RepIt enables precise
interventions: it selectively suppresses refusal on targeted concepts while
preserving refusal elsewhere, producing models that answer WMD-related
questions while still scoring as safe on standard benchmarks. We further show
that the corrective signal localizes to just 100-200 neurons and that robust
target representations can be extracted from as few as a dozen examples on a
single A6000. This efficiency raises a dual concern: manipulations can be
performed with modest compute and data to extend to underrepresented
data-scarce topics while evading existing benchmarks. By disentangling refusal
vectors with RepIt, this work demonstrates that targeted interventions can
counteract overgeneralization, laying the foundation for more granular control
of model behavior.

</details>


### [43] [Shapes of Cognition for Computational Cognitive Modeling](https://arxiv.org/abs/2509.13288)
*Marjorie McShane,Sergei Nirenburg,Sanjay Oruganti,Jesse English*

Main category: cs.AI

TL;DR: Shapes of cognition是语言智能体计算认知建模的新范式，有特定建模内容，原理可广泛应用。


<details>
  <summary>Details</summary>
Motivation: 构建可解释、可扩展且值得信赖的实用智能体系统，推动基于知识和混合人工智能发展。

Method: 采用形状记忆的感官、语言等知识，通过典型预期、模式识别等减少认知负荷，用基于形状的恢复方法处理非典型结果。

Result: 形成了特定的目标、假设、建模策略、知识库和模型。

Conclusion: 该建模虽以语言智能体为例具体，但原理可广泛应用，赋予基于知识和混合人工智能新活力。

Abstract: Shapes of cognition is a new conceptual paradigm for the computational
cognitive modeling of Language-Endowed Intelligent Agents (LEIAs). Shapes are
remembered constellations of sensory, linguistic, conceptual, episodic, and
procedural knowledge that allow agents to cut through the complexity of real
life the same way as people do: by expecting things to be typical, recognizing
patterns, acting by habit, reasoning by analogy, satisficing, and generally
minimizing cognitive load to the degree situations permit. Atypical outcomes
are treated using shapes-based recovery methods, such as learning on the fly,
asking a human partner for help, or seeking an actionable, even if imperfect,
situational understanding. Although shapes is an umbrella term, it is not
vague: shapes-based modeling involves particular objectives, hypotheses,
modeling strategies, knowledge bases, and actual models of wide-ranging
phenomena, all implemented within a particular cognitive architecture. Such
specificity is needed both to vet our hypotheses and to achieve our practical
aims of building useful agent systems that are explainable, extensible, and
worthy of our trust, even in critical domains. However, although the LEIA
example of shapes-based modeling is specific, the principles can be applied
more broadly, giving new life to knowledge-based and hybrid AI.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [44] [A Meshing Framework for Digital Twins for Extrusion based Additive Manufacturing](https://arxiv.org/abs/2509.12436)
*Lucas Gallup,Kevin N. Long,Devin J. Roach,William D. Reinholtz,Adam Cook,Craig M. Hamel*

Main category: cs.CE

TL;DR: 提出用于增材制造（AM）组件有限元分析（FEA）的网格生成框架，减少时间和资源浪费，优化组件属性。


<details>
  <summary>Details</summary>
Motivation: AM组件内部微观结构复杂，传统方法需大量测试和材料浪费，需准确一致的数值模拟框架。

Method: 提出直接创建适用于FEA的计算网格的框架，可手动或自动进行标准FEA模拟。

Result: 该框架可对目标AM几何进行计算机评估，减少传统设计周期的时间和资源浪费。

Conclusion: 此方法有助于探索工具路径填充，优化组件属性，适用于非直观设计空间。

Abstract: Additive manufacturing (AM) allows for manufacturing of complex
three-dimensional geometries not typically realizable with standard subtractive
manufacturing practices. The internal microstructure of a 3D printed component
can have a significant impact on its mechanical, vibrational, and shock
properties and allows for a richer design space when this is controllable. Due
to the complex interactions of the internal geometry of an extrusion-based AM
component, it is common practice to assume a homogeneous behavior or to perform
characterization testing on the specific toolpath configurations. To avoid
unnecessary testing or material waste, it is necessary to develop an accurate
and consistent numerical simulation framework with relevant boundary value
problems that can handle the complicated geometry of internal material
microstructure present in AM components. Herein, a framework is proposed to
directly create computational meshes suitable for finite element analysis (FEA)
of the fine-scale features generated from extrusion-based AM tool paths to
maintain a strong process-structure-property-performance linkage. This mesh can
be manually or automatically analyzed using standard FEA simulations such as
quasi-static preloading, modal analysis, or thermal analysis. The framework
allows an in-silico assessment of a target AM geometry where fine-scale
features may greatly impact quantities of design interest such as in soft
elastomeric lattices where toolpath infill can greatly influence the self
contact of a structure in compression, which we will use as a motivating
exemplar. This approach greatly reduces the waste of both time and resources
consumed through traditional build and test design cycles for non-intuitive
design spaces. It also further allows for the exploration of toolpath infill to
optimize component properties beyond simple linear properties such as density
and stiffness.

</details>


### [45] [Context-Aware Language Models for Forecasting Market Impact from Sequences of Financial News](https://arxiv.org/abs/2509.12519)
*Ross Koval,Nicholas Andrews,Xifeng Yan*

Main category: cs.CE

TL;DR: 本文探讨历史上下文对大语言模型理解财经新闻市场影响的价值，提出上下文处理方法，证明其对模拟投资表现有提升。


<details>
  <summary>Details</summary>
Motivation: 财经新闻信息需结合历史报道理解，识别和融入相关上下文有挑战，因此探索历史上下文对大语言模型理解财经新闻市场影响的价值。

Method: 提出一种有效上下文处理方法，用大语言模型处理主文章，小语言模型将历史上下文编码为摘要嵌入并与大模型表示空间对齐。

Result: 历史上下文能显著提升模型性能，通过多种可解释性测试揭示上下文价值，模型预测中历史上下文价值有实际应用，提升模拟投资表现。

Conclusion: 历史上下文对大语言模型理解财经新闻市场影响有重要价值，提出的方法有效且有实际应用价值。

Abstract: Financial news plays a critical role in the information diffusion process in
financial markets and is a known driver of stock prices. However, the
information in each news article is not necessarily self-contained, often
requiring a broader understanding of the historical news coverage for accurate
interpretation. Further, identifying and incorporating the most relevant
contextual information presents significant challenges. In this work, we
explore the value of historical context in the ability of large language models
to understand the market impact of financial news. We find that historical
context provides a consistent and significant improvement in performance across
methods and time horizons. To this end, we propose an efficient and effective
contextualization method that uses a large LM to process the main article,
while a small LM encodes the historical context into concise summary embeddings
that are then aligned with the large model's representation space. We explore
the behavior of the model through multiple qualitative and quantitative
interpretability tests and reveal insights into the value of contextualization.
Finally, we demonstrate that the value of historical context in model
predictions has real-world applications, translating to substantial
improvements in simulated investment performance.

</details>


### [46] [Impact of Geometric Uncertainty on the Computation of Abdominal Aortic Aneurysm Wall Strain](https://arxiv.org/abs/2509.12550)
*Saeideh Sekhavat,Mostafa Jamshidian,Adam Wittek,Karol Miller*

Main category: cs.CE

TL;DR: 研究评估了几何不确定性对腹主动脉瘤（AAA）壁应变计算的影响，发现几何不确定性降低应变计算准确性，向内偏差影响更大，为准确估计应变，几何不确定性应控制在一个壁厚内。


<details>
  <summary>Details</summary>
Motivation: 当前AAA管理依赖的指标不能可靠预测破裂风险，计算壁应力和应变虽有潜力但受几何不确定性影响，且几何不确定性对壁应变的影响尚不明确。

Method: 使用时间分辨3D计算机断层血管造影（4D - CTA）的可变形图像配准计算AAA壁应变，对壁几何沿表面法线进行受控扰动。

Result: AAA壁几何的不确定性降低了计算应变的准确性，向内偏差比向外偏差导致的偏差更大，峰值应变更敏感但稳健性差，第99百分位应变在扰动下更稳定。

Conclusion: 为了足够准确地估计应变，几何不确定性应保持在一个壁厚（通常1.5毫米）以内。

Abstract: Abdominal aortic aneurysm (AAA) is a life-threatening condition characterized
by permanent enlargement of the aorta, often detected incidentally during
imaging for unrelated conditions. Current management relies primarily on
aneurysm diameter and growth rate, which may not reliably predict
patient-specific rupture risk. Computation of AAA wall stress and strain has
the potential to improve individualized risk assessment, but these analyses
depend on image-derived geometry, which is subject to segmentation uncertainty
and lacks a definitive ground truth for the wall boundary. While the effect of
geometric uncertainty on wall stress has been studied, its influence on wall
strain remains unclear. In this study, we assessed the impact of geometric
uncertainty on AAA wall strain computed using deformable image registration of
time-resolved 3D computed tomography angiography (4D-CTA). Controlled
perturbations were applied to the wall geometry along the surface normal,
parameterized by the standard deviation for random variation and the mean for
systematic inward or outward bias, both scaled relative to wall thickness.
Results show that uncertainties in AAA wall geometry reduce the accuracy of
computed strain, with inward bias (toward the blood lumen and intraluminal
thrombus) consistently causing greater deviations than outward bias (toward
regions external to the aortic wall). Peak strain is more sensitive but less
robust, whereas the 99th percentile strain remains more stable under
perturbations. We concluded that, for sufficiently accurate strain estimation,
geometric uncertainty should remain within one wall thickness (typically 1.5
mm).

</details>


### [47] [FinSentLLM: Multi-LLM and Structured Semantic Signals for Enhanced Financial Sentiment Forecasting](https://arxiv.org/abs/2509.12638)
*Zijian Zhang,Rong Fu,Yangfan He,Xinze Shen,Yanlong Wang,Xiaojing Du,Haochen You,Jiazhao Shi,Simon Fong*

Main category: cs.CE

TL;DR: 提出FinSentLLM框架用于金融情感分析，在数据集上有精度提升，且证明金融情感与股市有长期联动。


<details>
  <summary>Details</summary>
Motivation: 现有金融情感分析研究多仅评估分类指标，不清楚情感信号是否与市场行为一致。

Method: 提出FinSentLLM轻量级多LLM框架，集成情感预测LLM专家小组和结构化语义金融信号，用紧凑元分类器；使用DCC - GARCH和Johansen协整检验分析情感与股市关系。

Result: 在Financial PhraseBank数据集上精度和F1分数比强基线模型高3 - 6%；金融情感与股市存在显著长期联动。

Conclusion: FinSentLLM有出色金融情感预测精度，情感信号与长期股票市场动态有紧密联系。

Abstract: Financial sentiment analysis (FSA) has attracted significant attention, and
recent studies increasingly explore large language models (LLMs) for this
field. Yet most work evaluates only classification metrics, leaving unclear
whether sentiment signals align with market behavior. We propose FinSentLLM, a
lightweight multi-LLM framework that integrates an expert panel of sentiment
forecasting LLMs, and structured semantic financial signals via a compact
meta-classifier. This design captures expert complementarity, semantic
reasoning signal, and agreement/divergence patterns without costly retraining,
yielding consistent 3-6% gains over strong baselines in accuracy and F1-score
on the Financial PhraseBank dataset. In addition, we also provide econometric
evidence that financial sentiment and stock markets exhibit statistically
significant long-run comovement, applying Dynamic Conditional Correlation GARCH
(DCC-GARCH) and the Johansen cointegration test to daily sentiment scores
computed from the FNSPID dataset and major stock indices. Together, these
results demonstrate that FinSentLLM delivers superior forecasting accuracy for
financial sentiment and further establish that sentiment signals are robustly
linked to long-run equity market dynamics.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [48] [ScaleDoc: Scaling LLM-based Predicates over Large Document Collections](https://arxiv.org/abs/2509.12610)
*Hengrui Zhang,Yulong Hui,Yihao Liu,Huanchen Zhang*

Main category: cs.DB

TL;DR: 介绍ScaleDoc系统处理非结构化文档语义分析问题，通过解耦执行阶段和创新机制提升效率。


<details>
  <summary>Details</summary>
Motivation: 现代工作负载涉及非结构化文档，需语义理解，但大语言模型推理成本高，传统谓词无法满足需求。

Method: 将谓词执行解耦为离线表示阶段和在线过滤阶段，离线用大语言模型生成文档语义表示，在线训练轻量级代理模型过滤文档；提出基于对比学习的框架和自适应级联机制。

Result: 在三个数据集上评估，ScaleDoc实现了超2倍的端到端加速，减少高达85%的大语言模型调用。

Conclusion: ScaleDoc使大规模语义分析变得实用和高效。

Abstract: Predicates are foundational components in data analysis systems. However,
modern workloads increasingly involve unstructured documents, which demands
semantic understanding, beyond traditional value-based predicates. Given
enormous documents and ad-hoc queries, while Large Language Models (LLMs)
demonstrate powerful zero-shot capabilities, their high inference cost leads to
unacceptable overhead. Therefore, we introduce \textsc{ScaleDoc}, a novel
system that addresses this by decoupling predicate execution into an offline
representation phase and an optimized online filtering phase. In the offline
phase, \textsc{ScaleDoc} leverages a LLM to generate semantic representations
for each document. Online, for each query, it trains a lightweight proxy model
on these representations to filter the majority of documents, forwarding only
the ambiguous cases to the LLM for final decision. Furthermore,
\textsc{ScaleDoc} proposes two core innovations to achieve significant
efficiency: (1) a contrastive-learning-based framework that trains the proxy
model to generate reliable predicating decision scores; (2) an adaptive cascade
mechanism that determines the effective filtering policy while meeting specific
accuracy targets. Our evaluations across three datasets demonstrate that
\textsc{ScaleDoc} achieves over a 2$\times$ end-to-end speedup and reduces
expensive LLM invocations by up to 85\%, making large-scale semantic analysis
practical and efficient.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [49] [Exploring Distributed Vector Databases Performance on HPC Platforms: A Study with Qdrant](https://arxiv.org/abs/2509.12384)
*Seth Ockerman,Amal Gueroudji,Song Young Oh,Robert Underwood,Nicholas Chia,Kyle Chard,Robert Ross,Shivaram Venkataraman*

Main category: cs.DC

TL;DR: 对Polaris超级计算机上分布式向量数据库性能进行实证研究，为HPC平台上向量数据库性能表征迈出第一步。


<details>
  <summary>Details</summary>
Motivation: 向量数据库在现代AI工作流中重要，但在驱动大规模科学的HPC系统中其性能特征鲜为人知。

Method: 在Polaris超级计算机上，构建生物文本工作负载，用Qwen3 - Embedding - 4B生成嵌入，选Qdrant评估插入、索引构建和查询延迟。

Result: 未提及具体结果。

Conclusion: 为HPC平台上向量数据库性能表征迈出第一步，以指导未来研究和优化。

Abstract: Vector databases have rapidly grown in popularity, enabling efficient
similarity search over data such as text, images, and video. They now play a
central role in modern AI workflows, aiding large language models by grounding
model outputs in external literature through retrieval-augmented generation.
Despite their importance, little is known about the performance characteristics
of vector databases in high-performance computing (HPC) systems that drive
large-scale science. This work presents an empirical study of distributed
vector database performance on the Polaris supercomputer in the Argonne
Leadership Computing Facility. We construct a realistic biological-text
workload from BV-BRC and generate embeddings from the peS2o corpus using
Qwen3-Embedding-4B. We select Qdrant to evaluate insertion, index construction,
and query latency with up to 32 workers. Informed by practical lessons from our
experience, this work takes a first step toward characterizing vector database
performance on HPC platforms to guide future research and optimization.

</details>


### [50] [IsoSched: Preemptive Tile Cascaded Scheduling of Multi-DNN via Subgraph Isomorphism](https://arxiv.org/abs/2509.12208)
*Boran Zhao,Zihang Yuan,Yanbin Hu,Haiming Zhai,Haoruo Zhang,Wenzhe Zhao,Tian Xia,Pengju Ren*

Main category: cs.DC

TL;DR: 本文提出IsoSched框架，解决TSS架构下多DNN抢占式调度问题，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有TSS工作缺乏抢占支持，先前抢占方案依赖LTS有开销，需要高效的基于TSS的抢占式框架。

Method: 将复杂拓扑图调度建模为整数线性规划和子图同构问题；应用LCS进行负载均衡；采用基于Ullmann并由MCTS增强的算法加速子图匹配，用CSR减少内存使用。

Result: IsoSched在LBT、加速比和能效上优于LTS - PRM方法，在不同任务复杂度下比TSS - NPRM有更高的关键任务满意度。

Conclusion: IsoSched是首个实现TSS架构下多DNN抢占式调度的框架，具有良好性能。

Abstract: Deploying deep neural network (DNN) accelerators with Layer Temporal
Scheduling (LTS) often incurs significant overheads (e.g., energy and latency),
as intermediate activations must be cached in DRAM. To alleviate this, Tile
Spatial Scheduling (TSS) reduces such costs by fragmenting inter-layer data
into smaller tiles communicated via on-chip links.However, many emerging
applications require concurrent execution of multiple DNNs with complex
topologies, where critical tasks must preempt others to meet stringent latency
requirements (e.g., in autonomous driving, obstacle detection must complete
within tens of milliseconds). Existing TSS works lack support for preemption,
while prior preemption schemes rely on LTS and thus inherit its overheads. This
highlights the need for preemptive and efficient TSS-based frameworks. Yet,
realizing such systems is challenging due to the complexity of enabling
preemption in graphs with large-scale topologies (e.g., modern large language
models may contain tens of thousands of edges). To tackle this, we present
IsoSched, the first framework enabling preemptive multi-DNN scheduling on TSS
architecture. IsoSched first formulates scheduling of complex-topology graphs
as an integer-linear program (ILP) and subgraph isomorphism problem; second, it
applies Layer Concatenate and Split (LCS) for load balancing in tile pipelines;
third, it employs an Ullmann-based algorithm enhanced by Monte Carlo Tree
Search (MCTS) to accelerate subgraph matching, and uses compact matrix encoding
(i.e., Compressed Sparse Row, CSR) to reduce memory usage. IsoSched outperforms
LTS-PRM approaches (i.e., PREMA, Planaria, CD-MSA, MoCA) in Latency-Bound
Throughput (LBT), speedup, and energy efficiency, and achieves higher critical
task satisfaction than TSS-NPRM (i.e., HASP) across varying task complexities.

</details>


### [51] [A Proposal for High-Level Architectural Model Capable of Expressing Various Data Collaboration Platform and Data Space Concepts](https://arxiv.org/abs/2509.12210)
*Masaru Dobashi,Kohei Toshimitsu,Hirotsugu Seike,Miki Kanno,Genki Horie,Noboru Koshizuka*

Main category: cs.DC

TL;DR: 提出DS - HLAM模型用于表达不同区域数据协作平台，用有限状态自动机理论保证互操作性和数字主权。


<details>
  <summary>Details</summary>
Motivation: 表达不同区域的多样化数据协作平台。

Method: 引入数学上严谨的定义，通过有限状态自动机理论形式化成功条件。

Result: 提出了“数据空间高级架构模型”（DS - HLAM）。

Conclusion: 该模型能在保证数字主权要求的同时实现互操作性。

Abstract: This paper proposes "Data Space High-Level Architecture Model" (DS-HLAM) for
expressing diverse data collaboration platforms across regional
implementations. The framework introduces mathematically rigorous definitions
with success conditions formalized through finite state automata theory,
enabling interoperability while preserving digital sovereignty requirements.

</details>


### [52] [TinyServe: Query-Aware Cache Selection for Efficient LLM Serving](https://arxiv.org/abs/2509.12211)
*Dong Liu,Yanxuan Yu*

Main category: cs.DC

TL;DR: 提出TinyServe系统用于部署小型大语言模型，采用查询感知页面选择机制和融合CUDA内核，实验显示有显著速度和内存优化效果。


<details>
  <summary>Details</summary>
Motivation: 解决自回归解码中键值缓存访问的高内存和延迟开销问题，高效服务大语言模型。

Method: 提出TinyServe系统，引入查询感知页面选择机制，使用融合CUDA内核。

Result: TinyServe实现了高达3.4倍的加速和超过2倍的内存节省，精度下降可忽略不计。

Conclusion: TinyServe是资源受限硬件上大语言模型训练和推理研究的高效系统级设计。

Abstract: Serving large language models (LLMs) efficiently remains challenging due to
the high memory and latency overhead of key-value (KV) cache access during
autoregressive decoding. We present \textbf{TinyServe}, a lightweight and
extensible serving system for deploying tiny LLMs (e.g., TinyLLaMA, GPT2-345M)
with support for structured KV sparsity, plugin-based token selection, and
hardware-efficient attention kernels. Unlike prior simulation frameworks,
TinyServe executes real-time decoding with configurable sparsity strategies and
fine-grained instrumentation.
  To reduce decoding cost, we introduce a \textit{query-aware page selection}
mechanism that leverages bounding-box metadata to estimate attention relevance
between the query and KV cache blocks. This enables selective KV loading with
minimal overhead and no model modifications. Our fused CUDA kernel integrates
page scoring, sparse memory access, and masked attention in a single pass.
  Experiments show that TinyServe achieves up to \textbf{3.4x} speedup and over
\textbf{2x} memory savings with negligible accuracy drop. Additional analysis
of cache reuse, page hit rate, and multi-GPU scaling confirms its practicality
as an efficient system-level design for LLM training and inference research on
resource-constrained hardware.

</details>


### [53] [Research on fault diagnosis and root cause analysis based on full stack observability](https://arxiv.org/abs/2509.12231)
*Jian Hou*

Main category: cs.DC

TL;DR: 本文回顾故障根源分析主流研究，提出KylinRCA框架并设计实验方案，为全栈可观测性下故障诊断提供有效方案。


<details>
  <summary>Details</summary>
Motivation: 云计算和超大规模数据中心发展使系统故障频发且呈级联传播，需高效准确可解释的根源分析。

Method: 回顾FaultInsight和HolisticRCA两种研究思路，分析优缺点，提出融合二者思想的KylinRCA框架，设计多维实验方案。

Result: 提出KylinRCA框架，可描绘传播链、实现全局根因定位和类型识别、输出可审计证据链。

Conclusion: 为全栈可观测性下的故障诊断提供了有效解决方案。

Abstract: With the rapid development of cloud computing and ultra-large-scale data
centers, the scale and complexity of systems have increased significantly,
leading to frequent faults that often show cascading propagation. How to
achieve efficient, accurate, and interpretable Root Cause Analysis (RCA) based
on observability data (metrics, logs, traces) has become a core issue in AIOps.
This paper reviews two mainstream research threads in top conferences and
journals over the past five years: FaultInsight[1] focusing on dynamic causal
discovery and HolisticRCA[2] focusing on multi-modal/cross-level fusion, and
analyzes the advantages and disadvantages of existing methods. A KylinRCA
framework integrating the ideas of both is proposed, which depicts the
propagation chain through temporal causal discovery, realizes global root cause
localization and type identification through cross-modal graph learning, and
outputs auditable evidence chains combined with mask-based explanation methods.
A multi-dimensional experimental scheme is designed, evaluation indicators are
clarified, and engineering challenges are discussed, providing an effective
solution for fault diagnosis under full-stack observability.

</details>


### [54] [Towards High-Performance and Portable Molecular Docking on CPUs through Vectorization](https://arxiv.org/abs/2509.12232)
*Gianmarco Accordi,Jens Domke,Theresa Pollinger,Davide Gadioli,Gianluca Palermo*

Main category: cs.DC

TL;DR: 评估编译器自动向量化和显式向量化以实现跨现代CPU的性能可移植性，以分子对接应用为例，给出了代码转换方法及不同架构CPU性能特点。


<details>
  <summary>Details</summary>
Motivation: HPC领域新CPU架构需优化以实现峰值性能，对性能可移植性提出挑战，需评估应用以了解代码、编译器和硬件间复杂交互。

Method: 选择分子对接应用作为案例研究，评估编译器自动向量化和显式向量化。

Result: 特定代码转换可实现便携式自动向量化，性能接近显式向量化；x86 CPU执行性能高，ARM架构能耗和成本效益有竞争力。

Conclusion: 给出与HPC科研应用未来开发相关的技术挑战、架构趋势和优化策略。

Abstract: Recent trends in the HPC field have introduced new CPU architectures with
improved vectorization capabilities that require optimization to achieve peak
performance and thus pose challenges for performance portability. The
deployment of high-performing scientific applications for CPUs requires
adapting the codebase and optimizing for performance. Evaluating these
applications provides insights into the complex interactions between code,
compilers, and hardware. We evaluate compiler auto-vectorization and explicit
vectorization to achieve performance portability across modern CPUs with long
vectors. We select a molecular docking application as a case study, as it
represents computational patterns commonly found across HPC workloads. We
report insights into the technical challenges, architectural trends, and
optimization strategies relevant to the future development of scientific
applications for HPC. Our results show which code transformations enable
portable auto-vectorization, reaching performance similar to explicit
vectorization. Experimental data confirms that x86 CPUs typically achieve
higher execution performance than ARM CPUs, primarily due to their wider
vectorization units. However, ARM architectures demonstrate competitive energy
consumption and cost-effectiveness.

</details>


### [55] [An End to End Edge to Cloud Data and Analytics Strategy](https://arxiv.org/abs/2509.12296)
*Vijay Kumar Butte,Sujata Butte*

Main category: cs.DC

TL;DR: 因物联网设备增长和企业云采用加速，本文提供端到端安全的边缘到云数据与分析策略及各层参考架构。


<details>
  <summary>Details</summary>
Motivation: 物联网设备增多和企业快速采用云，需开发安全高效策略和架构以利用云与边缘资产能力。

Method: 提出端到端安全的边缘到云数据与分析策略，给出设备层、边缘层和云层参考架构。

Result: 得到一套可用于实际实施的边缘到云数据与分析策略及各层参考架构。

Conclusion: 提供的策略和架构有助于企业更好地利用云与边缘资产进行数据处理和分析。

Abstract: There is an exponential growth of connected Internet of Things (IoT) devices.
These have given rise to applications that rely on real time data to make
critical decisions quickly. Enterprises today are adopting cloud at a rapid
pace. There is a critical need to develop secure and efficient strategy and
architectures to best leverage capabilities of cloud and edge assets. This
paper provides an end to end secure edge to cloud data and analytics strategy.
To enable real life implementation, the paper provides reference architectures
for device layer, edge layer and cloud layer.

</details>


### [56] [SynergAI: Edge-to-Cloud Synergy for Architecture-Driven High-Performance Orchestration for AI Inference](https://arxiv.org/abs/2509.12252)
*Foteini Stathopoulou,Aggelos Ferikoglou,Manolis Katsaragakis,Dimosthenis Masouros,Sotirios Xydis,Dimitrios Soudris*

Main category: cs.DC

TL;DR: 提出SynergAI框架用于异构边缘到云基础设施的推理服务，可减少QoS违规。


<details>
  <summary>Details</summary>
Motivation: AI和ML发展使推理服务计算需求增加，传统云部署有网络拥塞等问题，边缘计算资源有限。

Method: 基于现代推理引擎性能特征，结合离线和在线决策策略，动态分配工作负载。

Result: 在Kubernetes生态系统实现并评估，相比SotA方案，QoS违规平均减少2.4倍。

Conclusion: 架构驱动的推理服务能在新兴硬件平台实现优化和架构感知部署。

Abstract: The rapid evolution of Artificial Intelligence (AI) and Machine Learning (ML)
has significantly heightened computational demands, particularly for
inference-serving workloads. While traditional cloud-based deployments offer
scalability, they face challenges such as network congestion, high energy
consumption, and privacy concerns. In contrast, edge computing provides
low-latency and sustainable alternatives but is constrained by limited
computational resources. In this work, we introduce SynergAI, a novel framework
designed for performance- and architecture-aware inference serving across
heterogeneous edge-to-cloud infrastructures. Built upon a comprehensive
performance characterization of modern inference engines, SynergAI integrates a
combination of offline and online decision-making policies to deliver
intelligent, lightweight, and architecture-aware scheduling. By dynamically
allocating workloads across diverse hardware architectures, it effectively
minimizes Quality of Service (QoS) violations. We implement SynergAI within a
Kubernetes-based ecosystem and evaluate its efficiency. Our results demonstrate
that architecture-driven inference serving enables optimized and
architecture-aware deployments on emerging hardware platforms, achieving an
average reduction of 2.4x in QoS violations compared to a State-of-the-Art
(SotA) solution.

</details>


### [57] [The Entropy of Parallel Systems](https://arxiv.org/abs/2509.12256)
*Temitayo Adefemi*

Main category: cs.DC

TL;DR: 本文用熵描述计算机组件不兼容性，构建数学理论计算并行集群熵，计算Top10超算熵，发现系统熵与计算性能负相关。


<details>
  <summary>Details</summary>
Motivation: 用熵描述计算机组件不兼容性导致的并行集群内的噪声和无序。

Method: 基于图论和对数开发数学理论，考虑集群内每个系统的熵来量化并行集群的熵。

Result: 熵框架显示世界最快超算的系统熵与计算性能有显著负相关，LINPACK基准、MLPerf基准和HPCC综合得分都体现了这种负相关。

Conclusion: 系统熵越低，计算效率越高，该框架适用性超越传统密集线性代数工作负载。

Abstract: Ever since Claude Shannon used entropy for his "Mathematical Theory of
Communication", entropy has become a buzzword in research circles with
scientists applying entropy to describe any phenomena that are reminiscent of
disorder. In this paper, we used entropy to describe the incompatibility
between components in the computer, which can cause noise and disorder within
the parallel cluster. We develop a mathematical theory, primarily based on
graph theory and logarithms, to quantify the entropy of a parallel cluster by
accounting for the entropy of each system within the cluster. We proceed using
this model to calculate the entropy of the Top 10 supercomputers in the Top500
list. Our entropy framework reveals a statistically significant negative
correlation between system entropy and computational performance across the
world's fastest supercomputers. Most notably, the LINPACK benchmark
demonstrates a strong negative correlation (r = -0.7832, p = 0.0077) with our
entropy measure, indicating that systems with lower entropy consistently
achieve higher computational efficiency, this Relationship is further supported
by moderate correlations with MLPerf mixed-precision benchmarks (r = -0.6234)
and HPCC composite scores (r = -0.5890), suggesting the framework's
applicability extends beyond traditional dense linear algebra workloads.

</details>


### [58] [AI Factories: It's time to rethink the Cloud-HPC divide](https://arxiv.org/abs/2509.12849)
*Pedro Garcia Lopez,Daniel Barcelona Pons,Marcin Copik,Torsten Hoefler,Eduardo Quiñones,Maciej Malawski,Peter Pietzutch,Alberto Marti,Thomas Ohlson Timoudas,Aleksander Slominski*

Main category: cs.DC

TL;DR: 各国推动主权AI计划，欧洲投资基于HPC的AI工厂，但HPC不适用于AI服务，文章倡导超算采用双栈方法融合HPC与云原生技术。


<details>
  <summary>Details</summary>
Motivation: 各国发展主权AI，建设AI工厂，现有基于HPC的AI工厂在可用性、易用性等方面存在不足，需要解决HPC与云计算之间的鸿沟。

Method: 倡导在超级计算机中采用双栈方法，将HPC和云原生技术集成，研究Serverless HPC和High - performance Cloud。

Result: 未提及具体结果。

Conclusion: 未提及明确结论，但目标是通过融合两种技术，使两者相互增强。

Abstract: The strategic importance of artificial intelligence is driving a global push
toward Sovereign AI initiatives. Nationwide governments are increasingly
developing dedicated infrastructures, called AI Factories (AIF), to achieve
technological autonomy and secure the resources necessary to sustain robust
local digital ecosystems.
  In Europe, the EuroHPC Joint Undertaking is investing hundreds of millions of
euros into several AI Factories, built atop existing high-performance computing
(HPC) supercomputers. However, while HPC systems excel in raw performance, they
are not inherently designed for usability, accessibility, or serving as
public-facing platforms for AI services such as inference or agentic
applications. In contrast, AI practitioners are accustomed to cloud-native
technologies like Kubernetes and object storage, tools that are often difficult
to integrate within traditional HPC environments.
  This article advocates for a dual-stack approach within supercomputers:
integrating both HPC and cloud-native technologies. Our goal is to bridge the
divide between HPC and cloud computing by combining high performance and
hardware acceleration with ease of use and service-oriented front-ends. This
convergence allows each paradigm to amplify the other. To this end, we will
study the cloud challenges of HPC (Serverless HPC) and the HPC challenges of
cloud technologies (High-performance Cloud).

</details>


### [59] [Analysis and Optimization of Wireless Multimodal Federated Learning on Modal Heterogeneity](https://arxiv.org/abs/2509.12930)
*Xuefeng Han,Wen Chen,Jun Li,Ming Ding,Qingqing Wu,Kang Wei,Xiumei Deng,Yumeng Shao,Qiong Wu*

Main category: cs.DC

TL;DR: 针对无线多模态联邦学习（MFL）中模态异构问题，提出基于决策级融合架构并添加单模态损失函数的联合客户端调度和带宽分配（JCSBA）算法，实验表明其能提升多模态和单模态准确率。


<details>
  <summary>Details</summary>
Motivation: 多模态数据在不同客户端具有异构性，传统单模态联邦学习方法不适用，且固定延迟需求和有限通信带宽给无线场景下部署MFL带来挑战，需要优化无线MFL性能。

Method: 提出基于决策级融合架构并添加单模态损失函数的JCSBA算法，在训练目标和局部更新损失函数中加入单模态损失函数，推导与客户端和模态调度相关的闭式上界并在约束条件下最小化该上界。

Result: 在多模态数据集上的实验结果显示，JCSBA算法相较于传统算法，多模态准确率提高4.06%，单模态准确率提高2.73%。

Conclusion: JCSBA算法能有效提升无线MFL在模态异构情况下的性能，提高多模态和单模态准确率。

Abstract: Multimodal federated learning (MFL) is a distributed framework for training
multimodal models without uploading local multimodal data of clients, thereby
effectively protecting client privacy. However, multimodal data is commonly
heterogeneous across diverse clients, where each client possesses only a subset
of all modalities, renders conventional analysis results and optimization
methods in unimodal federated learning inapplicable. In addition, fixed latency
demand and limited communication bandwidth pose significant challenges for
deploying MFL in wireless scenarios. To optimize the wireless MFL performance
on modal heterogeneity, this paper proposes a joint client scheduling and
bandwidth allocation (JCSBA) algorithm based on a decision-level fusion
architecture with adding a unimodal loss function. Specifically, with the
decision results, the unimodal loss functions are added to both the training
objective and local update loss functions to accelerate multimodal convergence
and improve unimodal performance. To characterize MFL performance, we derive a
closed-form upper bound related to client and modality scheduling and minimize
the derived bound under the latency, energy, and bandwidth constraints through
JCSBA. Experimental results on multimodal datasets demonstrate that the JCSBA
algorithm improves the multimodal accuracy and the unimodal accuracy by 4.06%
and 2.73%, respectively, compared to conventional algorithms.

</details>


### [60] [Asymmetric Grid Quorum Systems for Heterogeneous Processes](https://arxiv.org/abs/2509.12942)
*Michael Senn,Christian Cachin*

Main category: cs.DC

TL;DR: 介绍了非对称网格法定人数系统，可让进程独立指定异构信任假设，打破循环依赖，有广泛应用。


<details>
  <summary>Details</summary>
Motivation: 传统法定人数系统假设共享，新兴系统中进程选择的失败假设需兼容，但存在初始无兼容假设的困境。

Method: 引入基于定性属性的非对称网格法定人数系统，让进程独立选择与主观观点相符的法定人数系统。

Result: 设计的选择在定义上是兼容的，打破了循环依赖。

Conclusion: 非对称网格法定人数系统有从云平台到区块链网络等众多应用。

Abstract: Quorum systems are a common way to formalize failure assumptions in
distributed systems. Traditionally, these assumptions are shared by all
involved processes. More recently, systems have emerged which allow processes
some freedom in choosing their own, subjective or asymmetric, failure
assumptions. For such a system to work, individual processes' assumptions must
be compatible. However, this leads to a Catch-22-style scenario: How can
processes collaborate to agree on compatible failure assumptions when they have
no compatible failure assumptions to start with?
  We introduce asymmetric grid quorum systems that allow a group of processes
to specify heterogeneous trust assumptions independently of each other and
without coordination. They are based on qualitative attributes describing how
the processes differ. Each process may select a quorum system from this class
that aligns best with its subjective view. The available choices are designed
to be compatible by definition, thereby breaking the cycling dependency.
Asymmetric grid quorum systems have many applications that range from cloud
platforms to blockchain networks.

</details>


### [61] [Space-Time Trade-off in Bounded Iterated Memory](https://arxiv.org/abs/2509.13157)
*Guillermo Toyos-Marfurt,Petr Kuznetsov*

Main category: cs.DC

TL;DR: 本文聚焦有界迭代共享内存算法模拟无界全信息协议的渐近轮复杂度，关联轮复杂度与进程数、协议迭代数和共享内存条目位数，推导必要条件并给出算法，证明复杂度下界，算法在部分模型渐近最优。


<details>
  <summary>Details</summary>
Motivation: 先前虽知有界变量可实现与无界全信息协议相同计算能力，但共享内存位容量与实现一轮全信息协议所需轮数的精确关系未知，本文旨在研究此问题。

Method: 通过分析对应协议复形（表示可达状态的组合结构），推导必要条件并给出适配每共享内存条目可用位数的有界全信息算法。

Result: 对于n > 2，实现全信息协议所需轮复杂度满足Ω((n!)^(r - 1) · 2^(n - b))，算法在迭代收集模型渐近最优，在基于快照模型与最优解相差线性因子n。

Conclusion: 明确了有界迭代共享内存算法模拟无界全信息协议时轮复杂度与相关参数的关系，所提算法在多种迭代共享内存模型有良好性能。

Abstract: The celebrated asynchronous computability theorem (ACT) characterizes tasks
solvable in the read-write shared-memory model using the unbounded
full-information protocol, where in every round of computation, each process
shares its complete knowledge of the system with the other processes.
Therefore, ACT assumes shared-memory variables of unbounded capacity. It has
been recently shown that boundedvariables can achieve the same computational
power at the expense of extra rounds. However, the exact relationship between
the bit capacity of the shared memory and the number of rounds required in
order to implement one round of the full-information protocol remained unknown.
  In this paper, we focus on the asymptotic round complexity of bounded
iterated shared-memory algorithms that simulate, up to isomorphism, the
unbounded full-information protocol. We relate the round complexity to the
number of processes $n$, the number of iterations of the full information
protocol $r$, and the bit size per shared-memory entry $b$. By analyzing the
corresponding protocol complex, a combinatorial structure representing
reachable states, we derive necessary conditions and present a bounded
full-information algorithm tailored to the bits available $b$ per shared memory
entry. We show that for $n>2$, the round complexity required to implement the
full-information protocol satisfies $\Omega((n!)^{r-1} \cdot 2^{n-b})$. Our
results apply to a range of iterated shared-memory models, from regular
read-write registers to atomic and immediate snapshots. Moreover, our bounded
full-information algorithm is asymptotically optimal for the iterated collect
model and within a linear factor $n$ of optimal for the snapshot-based models.

</details>


### [62] [Scaling Up Throughput-oriented LLM Inference Applications on Heterogeneous Opportunistic GPU Clusters with Pervasive Context Management](https://arxiv.org/abs/2509.13201)
*Thanh Son Phung,Douglas Thain*

Main category: cs.DC

TL;DR: 传统大语言模型应用资源分配问题严重，提出的普遍上下文管理方案可利用机会性资源，使执行时间减少98.1%。


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型应用需大量静态资源分配，导致排队等待或购买昂贵硬件，而机会性资源利用有诸多挑战。

Method: 提出普遍上下文管理方案，利用大语言模型应用中的通用计算上下文，提供上下文重用机制和策略。

Result: 采用普遍上下文管理的大语言模型应用在机会性资源上执行时间减少98.1%。

Conclusion: 普遍上下文管理方案能有效利用机会性资源，减少大语言模型应用执行时间。

Abstract: The widespread growth in LLM developments increasingly demands more
computational power from clusters than what they can supply. Traditional LLM
applications inherently require huge static resource allocations, which force
users to either wait in a long job queue and accept progress delay, or buy
expensive hardware to fulfill their needs and exacerbate the demand-supply
problem. However, not all LLM applications are latency-sensitive and can
instead be executed in a throughput-oriented way. This throughput orientation
allows a dynamic allocation that opportunistically pools available resources
over time, avoiding both the long queue and expensive GPU purchases.
Effectively utilizing opportunistic resources brings numerous challenges
nevertheless. Our solution, pervasive context management, exploits the common
computational context in LLM applications and provides mechanisms and policies
that allow seamless context reuse on opportunistic resources. Our evaluation
shows an LLM application with pervasive context management on opportunistic
resources reduces its execution time by 98.1%.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [63] [Graph Coloring Below Guarantees via Co-Triangle Packing](https://arxiv.org/abs/2509.12347)
*Shyan Akmal,Tomohiro Koana*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In the $\ell$-Coloring Problem, we are given a graph on $n$ nodes, and tasked
with determining if its vertices can be properly colored using $\ell$ colors.
In this paper we study below-guarantee graph coloring, which tests whether an
$n$-vertex graph can be properly colored using $g-k$ colors, where $g$ is a
trivial upper bound such as $n$. We introduce an algorithmic framework that
builds on a packing of co-triangles $\overline{K_3}$ (independent sets of three
vertices): the algorithm greedily finds co-triangles and employs a win-win
analysis. If many are found, we immediately return YES; otherwise these
co-triangles form a small co-triangle modulator, whose deletion makes the graph
co-triangle-free.
  Extending the work of [Gutin et al., SIDMA 2021], who solved $\ell$-Coloring
(for any $\ell$) in randomized $O^*(2^{k})$ time when given a
$\overline{K_2}$-free modulator of size $k$, we show that this problem can
likewise be solved in randomized $O^*(2^{k})$ time when given a
$\overline{K_3}$-free modulator of size~$k$.
  This result in turn yields a randomized $O^{*}(2^{3k/2})$ algorithm for
$(n-k)$-Coloring (also known as Dual Coloring), improving the previous
$O^{*}(4^{k})$ bound. We then introduce a smaller parameterization,
$(\omega+\overline{\mu}-k)$-Coloring, where $\omega$ is the clique number and
$\overline{\mu}$ is the size of a maximum matching in the complement graph;
since $\omega+\overline{\mu}\le n$ for any graph, this problem is strictly
harder. Using the same co-triangle-packing argument, we obtain a randomized
$O^{*}(2^{6k})$ algorithm, establishing its fixed-parameter tractability for a
smaller parameter. Complementing this finding, we show that no fixed-parameter
tractable algorithm exists for $(\omega-k)$-Coloring or
$(\overline{\mu}-k)$-Coloring under standard complexity assumptions.

</details>


### [64] [Sublinear-Time Algorithms for Diagonally Dominant Systems and Applications to the Friedkin-Johnsen Model](https://arxiv.org/abs/2509.13112)
*Weiming Feng,Zelin Li,Pan Peng*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study sublinear-time algorithms for solving linear systems $Sz = b$, where
$S$ is a diagonally dominant matrix, i.e., $|S_{ii}| \geq \delta + \sum_{j \ne
i} |S_{ij}|$ for all $i \in [n]$, for some $\delta \geq 0$. We present
randomized algorithms that, for any $u \in [n]$, return an estimate $z_u$ of
$z^*_u$ with additive error $\varepsilon$ or $\varepsilon \lVert
z^*\rVert_\infty$, where $z^*$ is some solution to $Sz^* = b$, and the
algorithm only needs to read a small portion of the input $S$ and $b$. For
example, when the additive error is $\varepsilon$ and assuming $\delta>0$, we
give an algorithm that runs in time $O\left( \frac{\|b\|_\infty^2
S_{\max}}{\delta^3 \varepsilon^2} \log \frac{\| b \|_\infty}{\delta
\varepsilon} \right)$, where $S_{\max} = \max_{i \in [n]} |S_{ii}|$. We also
prove a matching lower bound, showing that the linear dependence on $S_{\max}$
is optimal. Unlike previous sublinear-time algorithms, which apply only to
symmetric diagonally dominant matrices with non-negative diagonal entries, our
algorithm works for general strictly diagonally dominant matrices ($\delta >
0$) and a broader class of non-strictly diagonally dominant matrices $(\delta =
0)$. Our approach is based on analyzing a simple probabilistic recurrence
satisfied by the solution. As an application, we obtain an improved
sublinear-time algorithm for opinion estimation in the Friedkin--Johnsen model.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [65] [Knowledge Graph Tokenization for Behavior-Aware Generative Next POI Recommendation](https://arxiv.org/abs/2509.12350)
*Ke Sun,Mayi Xu*

Main category: cs.IR

TL;DR: 现有生成式兴趣点推荐方法有局限，提出KGTB方法，实验证明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有生成式兴趣点推荐方法的分词器难以编码异构信号、指令调优任务忽略其他行为类型，影响推荐效果。

Method: 将推荐数据组织成知识图谱格式，开发基于知识图谱的分词器，提出多行为学习进行大语言模型微调。

Result: 在四个真实城市数据集的实验中，KGTB表现出优越性能。

Conclusion: KGTB能有效解决现有生成式兴趣点推荐方法的局限，提升推荐效果。

Abstract: Generative paradigm, especially powered by Large Language Models (LLMs), has
emerged as a new solution to the next point-of-interest (POI) recommendation.
Pioneering studies usually adopt a two-stage pipeline, starting with a
tokenizer converting POIs into discrete identifiers that can be processed by
LLMs, followed by POI behavior prediction tasks to instruction-tune LLM for
next POI recommendation. Despite of remarkable progress, they still face two
limitations: (1) existing tokenizers struggle to encode heterogeneous signals
in the recommendation data, suffering from information loss issue, and (2)
previous instruction-tuning tasks only focus on users' POI visit behavior while
ignore other behavior types, resulting in insufficient understanding of
mobility. To address these limitations, we propose KGTB (Knowledge Graph
Tokenization for Behavior-aware generative next POI recommendation).
Specifically, KGTB organizes the recommendation data in a knowledge graph (KG)
format, of which the structure can seamlessly preserve the heterogeneous
information. Then, a KG-based tokenizer is developed to quantize each node into
an individual structural ID. This process is supervised by the KG's structure,
thus reducing the loss of heterogeneous information. Using generated IDs, KGTB
proposes multi-behavior learning that introduces multiple behavior-specific
prediction tasks for LLM fine-tuning, e.g., POI, category, and region visit
behaviors. Learning on these behavior tasks provides LLMs with comprehensive
insights on the target POI visit behavior. Experiments on four real-world city
datasets demonstrate the superior performance of KGTB.

</details>


### [66] [What News Recommendation Research Did (But Mostly Didn't) Teach Us About Building A News Recommender](https://arxiv.org/abs/2509.12361)
*Karl Higley,Robin Burke,Michael D. Ekstrand,Bart P. Knijnenburg*

Main category: cs.IR

TL;DR: 作者分享应用新闻推荐研究构建POPROX平台的经验，指出研究与系统构建存在差距并总结经验。


<details>
  <summary>Details</summary>
Motivation: 将新闻推荐研究应用于构建实际平台，检验当前研究对系统构建的支持程度。

Method: 应用新闻推荐文献构建POPROX平台，并反思研究现状。

Result: 构建过程中遇到意外挑战，研究文献存在令人惊讶的缺口。

Conclusion: 总结构建有持续用户群的系统的经验，指出未来新闻推荐研究可更具实用性和影响力的方向。

Abstract: One of the goals of recommender systems research is to provide insights and
methods that can be used by practitioners to build real-world systems that
deliver high-quality recommendations to actual people grounded in their genuine
interests and needs. We report on our experience trying to apply the news
recommendation literature to build POPROX, a live platform for news
recommendation research, and reflect on the extent to which the current state
of research supports system-building efforts. Our experience highlights several
unexpected challenges encountered in building personalization features that are
commonly found in products from news aggregators and publishers, and shows how
those difficulties are connected to surprising gaps in the literature. Finally,
we offer a set of lessons learned from building a live system with a persistent
user base and highlight opportunities to make future news recommendation
research more applicable and impactful in practice.

</details>


### [67] [LEAF: Knowledge Distillation of Text Embedding Models with Teacher-Aligned Representations](https://arxiv.org/abs/2509.12539)
*Robin Vujanic,Thomas Rueckstiess*

Main category: cs.IR

TL;DR: 提出知识蒸馏框架LEAF，能使蒸馏后的叶模型与教师模型对齐，在信息检索和多任务场景表现出色，设置新SOTA，适用黑盒模型，对数据集和训练设施要求低。


<details>
  <summary>Details</summary>
Motivation: 开发一个知识蒸馏框架，使蒸馏后的模型与教师模型对齐，实现灵活的非对称架构，同时继承教师模型的特性，降低对数据集和训练设施的要求。

Method: 提出LEAF知识蒸馏框架，训练叶模型与教师模型对齐，通过叶模型在信息检索和多任务场景验证框架能力。

Result: 训练出23M参数的信息检索模型leaf - ir和多任务模型leaf - mt，分别在BEIR和MTEB v2（English）排行榜上排名第一，非对称模式下检索性能进一步提升。

Conclusion: LEAF框架有效，可用于黑盒模型，对数据集和训练基础设施要求低，具有广泛适用性。

Abstract: We present LEAF ("Lightweight Embedding Alignment Framework"), a knowledge
distillation framework for text embedding models. A key distinguishing feature
is that our distilled leaf models are aligned to their teacher. In the context
of information retrieval, this allows for flexible asymmetric architectures
where documents are encoded with the larger teacher model, while queries can be
served with the smaller leaf models. We also show that leaf models
automatically inherit MRL and robustness to output quantization whenever these
properties are present in the teacher model, without explicitly training for
them. To demonstrate the capability of our framework we publish leaf-ir, a 23M
parameters information retrieval oriented text embedding model trained using
LEAF, which sets a new state-of-the-art (SOTA) on BEIR, ranking #1 on the
public leaderboard for this benchmark and for models of its size. When run in
asymmetric mode, its retrieval performance is further increased. Our scheme is
however not restricted to the information retrieval setting, and we demonstrate
its wider applicability by synthesizing the multi-task leaf-mt model. This also
sets a new SOTA, ranking #1 on the public MTEB v2 (English) leaderboard for its
size. LEAF is applicable to black-box models and in contrast to other embedding
model training frameworks, it does not require judgments nor hard negatives,
and training can be conducted using small batch sizes. Thus, dataset and
training infrastructure requirements for our framework are modest. We make our
models publicly available under a permissive Apache 2.0 license.

</details>


### [68] [InfoGain-RAG: Boosting Retrieval-Augmented Generation via Document Information Gain-based Reranking and Filtering](https://arxiv.org/abs/2509.12765)
*Zihan Wang,Zihan Liang,Zhou Shao,Yufei Ma,Huangyu Dai,Ben Chen,Lingtao Mao,Chenyi Lei,Yuqing Ding,Han Li*

Main category: cs.IR

TL;DR: 提出DIG指标和InfoGain - RAG框架，可有效筛选文档，实验表明其性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前RAG框架难以判断检索文档对答案生成的贡献，难以过滤无关或误导性内容，影响最终性能。

Method: 提出DIG指标量化检索文档对正确答案生成的贡献，引入InfoGain - RAG框架利用DIG分数训练专门的重排器。

Result: 在多种模型和基准测试中，InfoGain - RAG显著优于现有方法，如在NaturalQA上对不同RAG方法有精度提升，在GPT - 4o上平均有15.3%的增量。

Conclusion: InfoGain - RAG具有可行性，能为RAG在多应用场景提供可靠解决方案。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a promising approach to
address key limitations of Large Language Models (LLMs), such as hallucination,
outdated knowledge, and lacking reference. However, current RAG frameworks
often struggle with identifying whether retrieved documents meaningfully
contribute to answer generation. This shortcoming makes it difficult to filter
out irrelevant or even misleading content, which notably impacts the final
performance. In this paper, we propose Document Information Gain (DIG), a novel
metric designed to quantify the contribution of retrieved documents to correct
answer generation. DIG measures a document's value by computing the difference
of LLM's generation confidence with and without the document augmented.
Further, we introduce InfoGain-RAG, a framework that leverages DIG scores to
train a specialized reranker, which prioritizes each retrieved document from
exact distinguishing and accurate sorting perspectives. This approach can
effectively filter out irrelevant documents and select the most valuable ones
for better answer generation. Extensive experiments across various models and
benchmarks demonstrate that InfoGain-RAG can significantly outperform existing
approaches, on both single and multiple retrievers paradigm. Specifically on
NaturalQA, it achieves the improvements of 17.9%, 4.5%, 12.5% in exact match
accuracy against naive RAG, self-reflective RAG and modern ranking-based RAG
respectively, and even an average of 15.3% increment on advanced proprietary
model GPT-4o across all datasets. These results demonstrate the feasibility of
InfoGain-RAG as it can offer a reliable solution for RAG in multiple
applications.

</details>


### [69] [Protecting participants or population? Comparison of k-anonymous Origin-Destination matrices](https://arxiv.org/abs/2509.12950)
*Pietro Armenante,Kai Huang,Nikhil Jha,Luca Vassio*

Main category: cs.IR

TL;DR: 文章围绕Origin - Destination (OD) 矩阵展开，介绍NetMob2025挑战数据集价值，对比传统匿名化方法，提出新方法ODkAnon以生成保护隐私且含社会人口细分的OD矩阵。


<details>
  <summary>Details</summary>
Motivation: 生成并比较针对调查参与者和整个人口的k - 匿名OD矩阵，在保护隐私的同时考虑社会人口细分。

Method: 比较传统地理区域泛化匿名方法（如ATG、OIGH和经典Mondrian），提出新的贪心算法ODkAnon。

Result: 文中未明确提及具体结果。

Conclusion: 旨在为生成保护隐私、含社会人口细分且在实际人口上实现k - 匿名的OD矩阵做出贡献。

Abstract: Origin-Destination (OD) matrices are a core component of research on users'
mobility and summarize how individuals move between geographical regions. These
regions should be small enough to be representative of user mobility, without
incurring substantial privacy risks. There are two added values of the
NetMob2025 challenge dataset. Firstly, the data is extensive and contains a lot
of socio-demographic information that can be used to create multiple OD
matrices, based on the segments of the population. Secondly, a participant is
not merely a record in the data, but a statistically weighted proxy for a
segment of the real population. This opens the door to a fundamental shift in
the anonymization paradigm. A population-based view of privacy is central to
our contribution. By adjusting our anonymization framework to account for
representativeness, we are also protecting the inferred identity of the actual
population, rather than survey participants alone. The challenge addressed in
this work is to produce and compare OD matrices that are k-anonymous for survey
participants and for the whole population. We compare several traditional
methods of anonymization to k-anonymity by generalizing geographical areas.
These include generalization over a hierarchy (ATG and OIGH) and the classical
Mondrian. To this established toolkit, we add a novel method, i.e., ODkAnon, a
greedy algorithm aiming at balancing speed and quality. Unlike previous
approaches, which primarily address the privacy aspects of the given datasets,
we aim to contribute to the generation of privacy-preserving OD matrices
enriched with socio-demographic segmentation that achieves k-anonymity on the
actual population.

</details>


### [70] [DiffHash: Text-Guided Targeted Attack via Diffusion Models against Deep Hashing Image Retrieval](https://arxiv.org/abs/2509.12824)
*Zechao Liu,Zheng Zhou,Xiangkun Chen,Tao Liang,Dapeng Lang*

Main category: cs.IR

TL;DR: 提出DiffHash，一种基于扩散的深度哈希定向攻击方法，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度哈希模型的定向攻击方法存在缺乏多模态引导、依赖标签信息和像素级操作的问题。

Method: 提出DiffHash，优化图像潜在表示，由大语言模型生成文本信息引导；设计多空间哈希对齐网络；重建时引入文本引导注意力机制。

Result: 方法优于现有定向攻击方法，有更好的黑盒可迁移性和跨数据集稳定性。

Conclusion: DiffHash能有效解决现有深度哈希定向攻击方法的局限。

Abstract: Deep hashing models have been widely adopted to tackle the challenges of
large-scale image retrieval. However, these approaches face serious security
risks due to their vulnerability to adversarial examples. Despite the
increasing exploration of targeted attacks on deep hashing models, existing
approaches still suffer from a lack of multimodal guidance, reliance on
labeling information and dependence on pixel-level operations for attacks. To
address these limitations, we proposed DiffHash, a novel diffusion-based
targeted attack for deep hashing. Unlike traditional pixel-based attacks that
directly modify specific pixels and lack multimodal guidance, our approach
focuses on optimizing the latent representations of images, guided by text
information generated by a Large Language Model (LLM) for the target image.
Furthermore, we designed a multi-space hash alignment network to align the
high-dimension image space and text space to the low-dimension binary hash
space. During reconstruction, we also incorporated text-guided attention
mechanisms to refine adversarial examples, ensuring them aligned with the
target semantics while maintaining visual plausibility. Extensive experiments
have demonstrated that our method outperforms state-of-the-art (SOTA) targeted
attack methods, achieving better black-box transferability and offering more
excellent stability across datasets.

</details>


### [71] [A Learnable Fully Interacted Two-Tower Model for Pre-Ranking System](https://arxiv.org/abs/2509.12948)
*Chao Xiong,Xianwen Yu,Wei Xu,Lei Cheng,Chuan Yuan,Linjian Mo*

Main category: cs.IR

TL;DR: 提出可学习的全交互双塔模型FIT用于预排序，在多个公开数据集上显著优于现有基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有双塔模型因缺乏信息交互导致效果不佳，需在保证推理效率的同时实现丰富信息交互。

Method: 提出FIT模型，包含元查询模块MQM和轻量级相似度评分器LSS，MQM引入可学习的物品元矩阵实现早期交互，LSS实现后期交互。

Result: 在多个公开数据集上实验，FIT显著优于现有基线预排序模型。

Conclusion: FIT模型有效，能在预排序系统中实现丰富信息交互并保证推理效率。

Abstract: Pre-ranking plays a crucial role in large-scale recommender systems by
significantly improving the efficiency and scalability within the constraints
of providing high-quality candidate sets in real time. The two-tower model is
widely used in pre-ranking systems due to a good balance between efficiency and
effectiveness with decoupled architecture, which independently processes user
and item inputs before calculating their interaction (e.g. dot product or
similarity measure). However, this independence also leads to the lack of
information interaction between the two towers, resulting in less
effectiveness. In this paper, a novel architecture named learnable Fully
Interacted Two-tower Model (FIT) is proposed, which enables rich information
interactions while ensuring inference efficiency. FIT mainly consists of two
parts: Meta Query Module (MQM) and Lightweight Similarity Scorer (LSS).
Specifically, MQM introduces a learnable item meta matrix to achieve expressive
early interaction between user and item features. Moreover, LSS is designed to
further obtain effective late interaction between the user and item towers.
Finally, experimental results on several public datasets show that our proposed
FIT significantly outperforms the state-of-the-art baseline pre-ranking models.

</details>


### [72] [Green Recommender Systems: Understanding and Minimizing the Carbon Footprint of AI-Powered Personalization](https://arxiv.org/abs/2509.13001)
*Lukas Wegmeth,Tobias Vente,Alan Said,Joeran Beel*

Main category: cs.IR

TL;DR: 研究通过重现实验流程评估推荐系统研究的环境影响，对比传统与深度学习模型，发现深度学习模型碳排放高，呼吁采用绿色AI原则。


<details>
  <summary>Details</summary>
Motivation: 全球变暖背景下，评估和降低推荐系统环境影响的需求迫切，但相关领域对此缺乏理解和评估。

Method: 分析2013和2023年ACM RecSys会议的79篇论文，重现实验流程，用硬件电表测量能耗并转化为CO2当量。

Result: 使用深度学习模型的论文CO2排放量约为传统模型的42倍，单篇深度学习论文平均产生2909千克CO2当量。

Conclusion: 推荐系统和机器学习社区需采用绿色AI原则，平衡算法进步与环境责任，构建可持续未来。

Abstract: As global warming soars, the need to assess and reduce the environmental
impact of recommender systems is becoming increasingly urgent. Despite this,
the recommender systems community hardly understands, addresses, and evaluates
the environmental impact of their work. In this study, we examine the
environmental impact of recommender systems research by reproducing typical
experimental pipelines. Based on our results, we provide guidelines for
researchers and practitioners on how to minimize the environmental footprint of
their work and implement green recommender systems - recommender systems
designed to minimize their energy consumption and carbon footprint. Our
analysis covers 79 papers from the 2013 and 2023 ACM RecSys conferences,
comparing traditional "good old-fashioned AI" models with modern deep learning
models. We designed and reproduced representative experimental pipelines for
both years, measuring energy consumption using a hardware energy meter and
converting it into CO2 equivalents. Our results show that papers utilizing deep
learning models emit approximately 42 times more CO2 equivalents than papers
using traditional models. On average, a single deep learning-based paper
generates 2,909 kilograms of CO2 equivalents - more than the carbon emissions
of a person flying from New York City to Melbourne or the amount of CO2
sequestered by one tree over 260 years. This work underscores the urgent need
for the recommender systems and wider machine learning communities to adopt
green AI principles, balancing algorithmic advancements and environmental
responsibility to build a sustainable future with AI-powered personalization.

</details>


### [73] [Efficient Cold-Start Recommendation via BPE Token-Level Embedding Initialization with LLM](https://arxiv.org/abs/2509.13179)
*Yushang Zhao,Xinyue Han,Qian Leng,Qianyi Sun,Haotian Lyu,Chengrui Zhou*

Main category: cs.IR

TL;DR: 提出基于BPE分词和预训练大语言模型嵌入的冷启动推荐策略，实验表明该方法性能好、泛化性强且可解释性高。


<details>
  <summary>Details</summary>
Motivation: 解决推荐系统冷启动问题，传统方法在稀疏元数据环境中效果不佳。

Method: 在初始化过程中应用BPE分词和预训练大语言模型嵌入，获得细粒度的词元级向量。

Result: BPE - LLM方法在Recall@k、NDCG@k和Hit Rate指标上优于标准基线，有足够的计算性能。

Conclusion: 词元级语义初始化可作为零样本场景下现代推荐系统的有效扩展。

Abstract: The cold-start issue is the challenge when we talk about recommender systems,
especially in the case when we do not have the past interaction data of new
users or new items. Content-based features or hybrid solutions are common as
conventional solutions, but they can only work in a sparse metadata environment
with shallow patterns. In this paper, the efficient cold-start recommendation
strategy is presented, which is based on the sub word-level representations by
applying Byte Pair Encoding (BPE) tokenization and pre-trained Large Language
Model (LLM) embedding in the initialization procedure. We obtain fine-grained
token-level vectors that are aligned with the BPE vocabulary as opposed to
using coarse-grained sentence embeddings. Together, these token embeddings can
be used as dense semantic priors on unseen entities, making immediate
recommendation performance possible without user-item interaction history. Our
mechanism can be compared to collaborative filtering systems and tested over
benchmark datasets with stringent cold-start assumptions. Experimental findings
show that the given BPE-LLM method achieves higher Recall@k, NDCG@k, and Hit
Rate measurements compared to the standard baseline and displays the same
capability of sufficient computational performance. Furthermore, we demonstrate
that using subword-aware embeddings yields better generalizability and is more
interpretable, especially within a multilingual and sparse input setting. The
practical application of token-level semantic initialization as a lightweight,
but nevertheless effective extension to modern recommender systems in the
zero-shot setting is indicated within this work.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [74] [PowerGrow: Feasible Co-Growth of Structures and Dynamics for Power Grid Synthesis](https://arxiv.org/abs/2509.12212)
*Xinyu He,Chenhan Xiao,Haoran Li,Ruizhong Qiu,Zhe Xu,Yang Weng,Jingrui He,Hanghang Tong*

Main category: cs.LG

TL;DR: 提出PowerGrow框架生成电网场景，降低计算开销且保证有效性，实验表现优。


<details>
  <summary>Details</summary>
Motivation: 现代电力系统动态变化大，但公开测试用例稀缺，需生成工具，且联合建模是挑战。

Method: 采用依赖分解，将复杂联合分布分解为条件分布链，用分层图beta扩散过程合成结构，结合时间自编码器嵌入时间序列数据。

Result: PowerGrow在保真度和多样性上优于先前扩散模型，实现98.9%潮流收敛率和更好的N - 1应急弹性。

Conclusion: PowerGrow能生成具有操作有效性和现实性的电网场景。

Abstract: Modern power systems are becoming increasingly dynamic, with changing
topologies and time-varying loads driven by renewable energy variability,
electric vehicle adoption, and active grid reconfiguration. Despite these
changes, publicly available test cases remain scarce, due to security concerns
and the significant effort required to anonymize real systems. Such limitations
call for generative tools that can jointly synthesize grid structure and nodal
dynamics. However, modeling the joint distribution of network topology, branch
attributes, bus properties, and dynamic load profiles remains a major
challenge, while preserving physical feasibility and avoiding prohibitive
computational costs. We present PowerGrow, a co-generative framework that
significantly reduces computational overhead while maintaining operational
validity. The core idea is dependence decomposition: the complex joint
distribution is factorized into a chain of conditional distributions over
feasible grid topologies, time-series bus loads, and other system attributes,
leveraging their mutual dependencies. By constraining the generation process at
each stage, we implement a hierarchical graph beta-diffusion process for
structural synthesis, paired with a temporal autoencoder that embeds
time-series data into a compact latent space, improving both training stability
and sample fidelity. Experiments across benchmark settings show that PowerGrow
not only outperforms prior diffusion models in fidelity and diversity but also
achieves a 98.9\% power flow convergence rate and improved N-1 contingency
resilience. This demonstrates its ability to generate operationally valid and
realistic power grid scenarios.

</details>


### [75] [Finite-Agent Stochastic Differential Games on Large Graphs: II. Graph-Based Architectures](https://arxiv.org/abs/2509.12484)
*Ruimeng Hu,Jihao Long,Haosheng Zhou*

Main category: cs.LG

TL;DR: 提出非可训练修改（NTM）神经网络架构计算图上随机微分博弈的纳什均衡，理论证明其通用逼近性，结合现有求解器得到稀疏变体，实验显示性能相当且计算效率更高。


<details>
  <summary>Details</summary>
Motivation: 为图上随机微分博弈计算纳什均衡，解决图结构多智能体系统在不确定下的交互问题，提升模型可解释性和稳定性，减少可训练参数。

Method: 提出NTM架构，对前馈神经网络进行图引导的稀疏化；将NTM融入Direct Parameterization和Deep BSDE两个求解器得到稀疏变体。

Result: 理论上证明NTM在图上静态博弈的通用逼近性；数值实验表明NTM方法与全可训练方法性能相当，计算效率更高。

Conclusion: NTM架构可有效用于图上随机微分博弈的纳什均衡计算，在性能相当的情况下提升计算效率。

Abstract: We propose a novel neural network architecture, called Non-Trainable
Modification (NTM), for computing Nash equilibria in stochastic differential
games (SDGs) on graphs. These games model a broad class of graph-structured
multi-agent systems arising in finance, robotics, energy, and social dynamics,
where agents interact locally under uncertainty. The NTM architecture imposes a
graph-guided sparsification on feedforward neural networks, embedding fixed,
non-trainable components aligned with the underlying graph topology. This
design enhances interpretability and stability, while significantly reducing
the number of trainable parameters in large-scale, sparse settings. We
theoretically establish a universal approximation property for NTM in static
games on graphs and numerically validate its expressivity and robustness
through supervised learning tasks. Building on this foundation, we incorporate
NTM into two state-of-the-art game solvers, Direct Parameterization and Deep
BSDE, yielding their sparse variants (NTM-DP and NTM-DBSDE). Numerical
experiments on three SDGs across various graph structures demonstrate that
NTM-based methods achieve performance comparable to their fully trainable
counterparts, while offering improved computational efficiency.

</details>


### [76] [Scaling Up Data Parallelism in Decentralized Deep Learning](https://arxiv.org/abs/2509.12213)
*Bing Xie,Junqi Yin,Zhenyu Zhou,Sarp Oral,Feiyi Wang*

Main category: cs.LG

TL;DR: 本文研究大规模去中心化数据并行训练，引入DBench框架和基准方法，基于结果提出Ada方法，在大规模训练中表现良好。


<details>
  <summary>Details</summary>
Motivation: 去中心化学习在大规模DNN训练中缺乏稳定性、可扩展性和通用性，尚未用于生产，本文旨在推动其生产应用。

Method: 引入DBench基准框架，提出基准方法，根据结果提出Ada去中心化自适应方法。

Result: 观察到去中心化数据并行训练在扩大规模时有可扩展性和通用性问题，模型精度与通信图连接数和参数张量方差相关；Ada在去中心化DNN训练中收敛率最佳，模型精度与中心化学习相当。

Conclusion: Ada方法能有效解决去中心化学习在大规模训练中的问题，可用于生产。

Abstract: Although it has been extensively explored in theory, decentralized learning
is not yet green-lighted for production use, largely due to a lack of
stability, scalability, and generality in large scale DNN training. To shed
light on the production use of decentralized learning, this work studies
decentralized data parallel training at scale. To this end, we introduce a
benchmarking framework, namely DBench, to host both centralized and
decentralized DNN training. Building upon DBench, we introduce a benchmarking
methodology to uncover the correlations between model accuracy and the
variances of parameter tensors by varying communication graphs and training
scales. Based on the benchmarking results, we observe that, (1) Similar to
centralized learning, decentralized data parallel training also presents the
issues of scalability and generality when the training scales up; (2) The model
accuracy of decentralized learning is correlated to the number of connections
in a communication graph; (3) The model accuracy of decentralized learning is
surprisingly sensitive to the variance of parameter tensors across model
replicas. Built upon the observations, we propose Ada, a decentralized adaptive
approach that performs large scale DNN training following a decentralized SGD
method and adapting the communication graph in use dynamically throughout
training iterations. We apply Ada on large scale training and observe that Ada
can obtain the best convergence rates consistently in decentralized DNN
training, and delivers equally or comparably good model accuracy for all sample
applications as centralized learning does, even when training ResNet50 for
ImageNet-1K on the scale of 1008 GPUs.

</details>


### [77] [MEUV: Achieving Fine-Grained Capability Activation in Large Language Models via Mutually Exclusive Unlock Vectors](https://arxiv.org/abs/2509.12221)
*Xin Tong,Zhi Lin,Jingya Wang,Meng Han,Bo Jin*

Main category: cs.LG

TL;DR: 提出MEUV框架分解拒绝方向向量，在双语基准测试表现好，实现细粒度能力激活。


<details>
  <summary>Details</summary>
Motivation: 大语言模型安全对齐机制在高风险场景会阻碍合理使用，现有拒绝方向编辑方法缺乏语义控制。

Method: 引入Mutually Exclusive Unlock Vectors (MEUV)框架，通过多任务目标在单轮学习中分解拒绝方向向量。

Result: 在双语恶意提示基准测试中，MEUV在多个模型上攻击成功率不低于87%，交叉主题泄漏最多降低90%，且向量跨语言迁移性好。

Conclusion: 可实现细粒度、主题级能力激活且效用损失小，为安全敏感领域部署可控大语言模型铺平道路。

Abstract: Large language models (LLMs) enforce safety alignment to reliably refuse
malicious requests, yet the same blanket safeguards also block legitimate uses
in policing, defense, and other high-stakes settings. Earlier
"refusal-direction" edits can bypass those layers, but they rely on a single
vector that indiscriminately unlocks all hazardous topics, offering no semantic
control. We introduce Mutually Exclusive Unlock Vectors (MEUV), a lightweight
framework that factorizes the monolithic refusal direction into topic-aligned,
nearly orthogonal vectors, each dedicated to one sensitive capability. MEUV is
learned in a single epoch with a multi-task objective that blends a
differential-ablation margin, cross-topic and orthogonality penalties, and
several auxiliary terms. On bilingual malicious-prompt benchmarks, MEUV
achieves an attack success rate of no less than 87% on Gemma-2-2B, LLaMA-3-8B,
and Qwen-7B, yet cuts cross-topic leakage by up to 90% compared with the best
single-direction baseline. Vectors trained in Chinese transfer almost unchanged
to English (and vice versa), suggesting a language-agnostic refusal subspace.
The results show that fine-grained, topic-level capability activation is
achievable with minimal utility loss, paving the way for controlled LLMs
deployment in security-sensitive domains.

</details>


### [78] [TimeCluster with PCA is Equivalent to Subspace Identification of Linear Dynamical Systems](https://arxiv.org/abs/2509.12895)
*Christian L. Hines,Samuel Spillard,Daniel P. Martin*

Main category: cs.LG

TL;DR: 介绍TimeCluster可视分析技术，证明其选PCA降维时与经典线性子空间识别数学等价，实验验证，探讨未来机会。


<details>
  <summary>Details</summary>
Motivation: 研究TimeCluster技术与经典线性子空间识别的关系，为后续应用拓展提供基础。

Method: 先回顾TimeCluster方法和子空间系统识别理论，证明时间序列滑动窗口矩阵为Hankel矩阵，应用PCA恢复主方向，进行合成和真实动态信号实验。

Result: 实验证实TimeCluster的聚类坐标与子空间识别方法的嵌入一致。

Conclusion: TimeCluster选PCA降维与经典线性子空间识别数学等价，探索了基于此等价性的未来应用机会。

Abstract: TimeCluster is a visual analytics technique for discovering structure in long
multivariate time series by projecting overlapping windows of data into a
low-dimensional space. We show that, when Principal Component Analysis (PCA) is
chosen as the dimensionality reduction technique, this procedure is
mathematically equivalent to classical linear subspace identification
(block-Hankel matrix plus Singular Vector Decomposition (SVD)). In both
approaches, the same low-dimensional linear subspace is extracted from the time
series data. We first review the TimeCluster method and the theory of subspace
system identification. Then we show that forming the sliding-window matrix of a
time series yields a Hankel matrix, so applying PCA (via SVD) to this matrix
recovers the same principal directions as subspace identification. Thus the
cluster coordinates from TimeCluster coincide with the subspace identification
methods. We present experiments on synthetic and real dynamical signals
confirming that the two embeddings coincide. Finally, we explore and discuss
future opportunities enabled by this equivalence, including forecasting from
the identified state space, streaming/online extensions, incorporating and
visualising external inputs and robust techniques for displaying underlying
trends in corrupted data.

</details>


### [79] [Accelerating Privacy-Preserving Federated Learning in Large-Scale LEO Satellite Systems](https://arxiv.org/abs/2509.12222)
*Binquan Guo,Junteng Cao,Marie Siew,Binbin Chen,Tony Q. S. Quek,Zhu Han*

Main category: cs.LG

TL;DR: 针对卫星网络联邦学习调度问题，提出离散时间图按需调度框架，仿真显示性能显著提升且具可扩展性。


<details>
  <summary>Details</summary>
Motivation: 大规模低轨卫星系统虽利于AI模型协作训练，但因隐私等问题需采用联邦学习，而卫星系统动态拓扑和有限带宽会阻碍参数聚合与分发，延长训练时间。

Method: 研究卫星网络联邦学习调度问题，找出影响训练轮次总时长的关键瓶颈，提出离散时间图按需调度框架动态分配通信资源。

Result: 仿真表明，该方法较传统基于统计复用的模型交换策略有显著性能提升，总轮次时间减少14.20% - 41.48%，且对更大模型和更多客户端加速效果更明显。

Conclusion: 所提方法能有效加速卫星网络中的联邦学习，具有良好的可扩展性。

Abstract: Large-scale low-Earth-orbit (LEO) satellite systems are increasingly valued
for their ability to enable rapid and wide-area data exchange, thereby
facilitating the collaborative training of artificial intelligence (AI) models
across geographically distributed regions. Due to privacy concerns and
regulatory constraints, raw data collected at remote clients cannot be
centrally aggregated, posing a major obstacle to traditional AI training
methods. Federated learning offers a privacy-preserving alternative by training
local models on distributed devices and exchanging only model parameters.
However, the dynamic topology and limited bandwidth of satellite systems will
hinder timely parameter aggregation and distribution, resulting in prolonged
training times. To address this challenge, we investigate the problem of
scheduling federated learning over satellite networks and identify key
bottlenecks that impact the overall duration of each training round. We propose
a discrete temporal graph-based on-demand scheduling framework that dynamically
allocates communication resources to accelerate federated learning. Simulation
results demonstrate that the proposed approach achieves significant performance
gains over traditional statistical multiplexing-based model exchange
strategies, reducing overall round times by 14.20% to 41.48%. Moreover, the
acceleration effect becomes more pronounced for larger models and higher
numbers of clients, highlighting the scalability of the proposed approach.

</details>


### [80] [Causal-Symbolic Meta-Learning (CSML): Inducing Causal World Models for Few-Shot Generalization](https://arxiv.org/abs/2509.12387)
*Mohamed Zayaan S*

Main category: cs.LG

TL;DR: 提出CSML框架学习任务分布潜在因果结构，在新基准测试中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习模型依赖虚假关联，泛化能力差且需大量数据，人类智能的鲁棒、样本高效学习源于对因果机制的理解。

Method: 引入CSML框架，含感知、因果归纳和图推理模块，跨任务分布元学习共享因果世界模型；引入CausalWorld基准。

Result: CSML在实验中大幅超越现有元学习和神经符号基线，尤其在需要真正因果推理的任务上。

Conclusion: CSML能有效学习任务潜在因果结构，快速适应新任务，在因果推理任务中有出色表现。

Abstract: Modern deep learning models excel at pattern recognition but remain
fundamentally limited by their reliance on spurious correlations, leading to
poor generalization and a demand for massive datasets. We argue that a key
ingredient for human-like intelligence-robust, sample-efficient learning-stems
from an understanding of causal mechanisms. In this work, we introduce
Causal-Symbolic Meta-Learning (CSML), a novel framework that learns to infer
the latent causal structure of a task distribution. CSML comprises three key
modules: a perception module that maps raw inputs to disentangled symbolic
representations; a differentiable causal induction module that discovers the
underlying causal graph governing these symbols and a graph-based reasoning
module that leverages this graph to make predictions. By meta-learning a shared
causal world model across a distribution of tasks, CSML can rapidly adapt to
novel tasks, including those requiring reasoning about interventions and
counterfactuals, from only a handful of examples. We introduce CausalWorld, a
new physics-based benchmark designed to test these capabilities. Our
experiments show that CSML dramatically outperforms state-of-the-art
meta-learning and neuro-symbolic baselines, particularly on tasks demanding
true causal inference.

</details>


### [81] [Traces Propagation: Memory-Efficient and Scalable Forward-Only Learning in Spiking Neural Networks](https://arxiv.org/abs/2509.13053)
*Lorenzo Pes,Bojian Yin,Sander Stuijk,Federico Corradi*

Main category: cs.LG

TL;DR: 提出Traces Propagation (TP)学习规则，在多个数据集上表现良好，适合边缘高效学习。


<details>
  <summary>Details</summary>
Motivation: 现有训练SNNs的方法如BPTT存在与生物神经系统特性不符、计算和内存需求高的问题，现有局部学习规则无法有效解决空间信用分配。

Method: 提出结合资格迹和层对比损失的前向、内存高效、可扩展且完全局部的学习规则TP，无需辅助层矩阵。

Result: TP在NMNIST和SHD数据集上优于其他局部学习规则，在复杂数据集上表现有竞争力，能有效扩展到更深架构，内存扩展性好。

Conclusion: TP适合实际微调任务，为边缘高效学习铺平道路。

Abstract: Spiking Neural Networks (SNNs) provide an efficient framework for processing
dynamic spatio-temporal signals and for investigating the learning principles
underlying biological neural systems. A key challenge in training SNNs is to
solve both spatial and temporal credit assignment. The dominant approach for
training SNNs is Backpropagation Through Time (BPTT) with surrogate gradients.
However, BPTT is in stark contrast with the spatial and temporal locality
observed in biological neural systems and leads to high computational and
memory demands, limiting efficient training strategies and on-device learning.
Although existing local learning rules achieve local temporal credit assignment
by leveraging eligibility traces, they fail to address the spatial credit
assignment without resorting to auxiliary layer-wise matrices, which increase
memory overhead and hinder scalability, especially on embedded devices. In this
work, we propose Traces Propagation (TP), a forward-only, memory-efficient,
scalable, and fully local learning rule that combines eligibility traces with a
layer-wise contrastive loss without requiring auxiliary layer-wise matrices. TP
outperforms other fully local learning rules on NMNIST and SHD datasets. On
more complex datasets such as DVS-GESTURE and DVS-CIFAR10, TP showcases
competitive performance and scales effectively to deeper SNN architectures such
as VGG-9, while providing favorable memory scaling compared to prior fully
local scalable rules, for datasets with a significant number of classes.
Finally, we show that TP is well suited for practical fine-tuning tasks, such
as keyword spotting on the Google Speech Commands dataset, thus paving the way
for efficient learning at the edge.

</details>


### [82] [TripOptimizer: Generative 3D Shape Optimization and Drag Prediction using Triplane VAE Networks](https://arxiv.org/abs/2509.12224)
*Parsa Vatani,Mohamed Elrefaie,Farhad Nazarpour,Faez Ahmed*

Main category: cs.LG

TL;DR: 提出TripOptimizer框架用于快速气动分析和形状优化，减少对CFD模拟依赖。


<details>
  <summary>Details</summary>
Motivation: 传统基于CFD的气动形状优化计算成本高，限制设计空间探索。

Method: 引入TripOptimizer框架，用变分自编码器进行3D几何重建和阻力系数预测，在DrivAerNet++数据集训练，提出修改编码器参数的优化策略。

Result: 优化设计使阻力系数最多降低11.8%，结果经高保真CFD模拟验证。

Conclusion: 框架具有对几何缺陷的鲁棒性，能实现更灵活的气动形状优化工作流程，尤其在设计早期减少对CFD模拟的依赖。

Abstract: The computational cost of traditional Computational Fluid Dynamics-based
Aerodynamic Shape Optimization severely restricts design space exploration.
This paper introduces TripOptimizer, a fully differentiable deep learning
framework for rapid aerodynamic analysis and shape optimization directly from
vehicle point cloud data. TripOptimizer employs a Variational Autoencoder
featuring a triplane-based implicit neural representation for high-fidelity 3D
geometry reconstruction and a drag coefficient prediction head. Trained on
DrivAerNet++, a large-scale dataset of 8,000 unique vehicle geometries with
corresponding drag coefficients computed via Reynolds-Averaged Navier-Stokes
simulations, the model learns a latent representation that encodes
aerodynamically salient geometric features. We propose an optimization strategy
that modifies a subset of the encoder parameters to steer an initial geometry
towards a target drag value, and demonstrate its efficacy in case studies where
optimized designs achieved drag coefficient reductions up to 11.8\%. These
results were subsequently validated by using independent, high-fidelity
Computational Fluid Dynamics simulations with more than 150 million cells. A
key advantage of the implicit representation is its inherent robustness to
geometric imperfections, enabling optimization of non-watertight meshes, a
significant challenge for traditional adjoint-based methods. The framework
enables a more agile Aerodynamic Shape Optimization workflow, reducing reliance
on computationally intensive CFD simulations, especially during early design
stages.

</details>


### [83] [A Physics-Informed Neural Networks-Based Model Predictive Control Framework for $SIR$ Epidemics](https://arxiv.org/abs/2509.12226)
*Aiping Zhong,Baike She,Philip E. Paré*

Main category: cs.LG

TL;DR: 本文提出基于PINNs的MPC框架用于SIR传播模型，在不同已知条件下联合实时估计状态和参数，提出多种PINNs算法并集成到MPC框架，实验验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 现有MPC设计用于疫情控制的研究常假设可测量动态状态或已知模型参数，本文旨在在仅知部分参数的情况下，基于有噪声的感染状态在MPC框架内联合实时估计状态和参数。

Method: 提出MPC - PINNs及两种新的PINNs算法（MPC - LS - PINNs、MPC - SI - PINNs），并在不同假设下扩展框架和算法，将算法集成到MPC框架。

Result: 实验结果表明所提方法在不同设置下有效。

Conclusion: 所提出的基于PINNs的MPC框架能在估计疫情状态和参数的同时生成最优控制策略，方法有效。

Abstract: This work introduces a physics-informed neural networks (PINNs)-based model
predictive control (MPC) framework for susceptible-infected-recovered ($SIR$)
spreading models. Existing studies in MPC design for epidemic control often
assume either 1) measurable states of the dynamics, where the parameters are
learned, or 2) known parameters of the model, where the states are learned. In
this work, we address the joint real-time estimation of states and parameters
within the MPC framework using only noisy infected states, under the assumption
that 1) only the recovery rate is known, or 2) only the basic reproduction
number is known. Under the first assumption, we propose MPC-PINNs and two novel
PINNs algorithms, all of which are integrated into the MPC framework. First, we
introduce MPC-PINNs, which are designed for $SIR$ models with control. We then
propose log-scaled PINNs (MPC-LS-PINNs), which incorporate a log-scaled loss
function to improve robustness against noise. Next, we present split-integral
PINNs (MPC-SI-PINNs), which leverage integral operators and state coupling in
the neural network training process to effectively reconstruct the complete
epidemic state information. Building upon these methods, we further extend our
framework for the second assumption. We establish the necessary conditions and
extend our PINNs algorithms, where MPC-SI-PINNs are simplified as split-PINNs
(MPC-S-PINNs). By incorporating these algorithms into the MPC framework, we
simultaneously estimate the epidemic states and parameters while generating
optimal control strategies. Experiment results demonstrate the effectiveness of
the proposed methods under different settings.

</details>


### [84] [Bayesian Parametric Matrix Models: Principled Uncertainty Quantification for Spectral Learning](https://arxiv.org/abs/2509.12406)
*Mohammad Nooraiepour*

Main category: cs.LG

TL;DR: 本文引入贝叶斯参数矩阵模型（B - PMMs）解决科学机器学习中不确定性量化问题，理论贡献丰富，实验验证其在不确定性校准和扩展性方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前光谱学习方法和参数矩阵模型（PMMs）缺乏不确定性量化，限制在安全关键应用中的使用，需解决矩阵特征值问题中的不确定性量化挑战。

Method: 提出B - PMMs框架，包括自适应光谱分解、结构化变分推理算法和有限样本校准保证。

Result: 在5x5到500x500矩阵维度实验中实现完美收敛率，ECE < 0.05，扩展性良好，在光谱病态条件下性能平稳下降。

Conclusion: B - PMMs支持关键领域的稳健光谱学习，为更广泛的贝叶斯光谱机器学习奠定基础。

Abstract: Scientific machine learning increasingly uses spectral methods to understand
physical systems. Current spectral learning approaches provide only point
estimates without uncertainty quantification, limiting their use in
safety-critical applications where prediction confidence is essential.
Parametric matrix models have emerged as powerful tools for scientific machine
learning, achieving exceptional performance by learning governing equations.
However, their deterministic nature limits deployment in uncertainty
quantification applications. We introduce Bayesian parametric matrix models
(B-PMMs), a principled framework that extends PMMs to provide uncertainty
estimates while preserving their spectral structure and computational
efficiency. B-PMM addresses the fundamental challenge of quantifying
uncertainty in matrix eigenvalue problems where standard Bayesian methods fail
due to the geometric constraints of spectral decomposition. The theoretical
contributions include: (i) adaptive spectral decomposition with regularized
matrix perturbation bounds that characterize eigenvalue uncertainty
propagation, (ii) structured variational inference algorithms using
manifold-aware matrix-variate Gaussian posteriors that respect Hermitian
constraints, and (iii) finite-sample calibration guarantees with explicit
dependence on spectral gaps and problem conditioning. Experimental validation
across matrix dimensions from 5x5 to 500x500 with perfect convergence rates
demonstrates that B-PMMs achieve exceptional uncertainty calibration (ECE <
0.05) while maintaining favorable scaling. The framework exhibits graceful
degradation under spectral ill-conditioning and provides reliable uncertainty
estimates even in near-degenerate regimes. The proposed framework supports
robust spectral learning in uncertainty-critical domains and lays the
groundwork for broader Bayesian spectral machine learning.

</details>


### [85] [Learning to Route: Per-Sample Adaptive Routing for Multimodal Multitask Prediction](https://arxiv.org/abs/2509.12227)
*Marzieh Ajirak,Oded Bein,Ellen Rose Bowen,Dora Kanellopoulos,Avital Falk,Faith M. Gunning,Nili Solomonov,Logan Grosenick*

Main category: cs.LG

TL;DR: 提出多任务、多模态预测自适应路由统一框架，在心理治疗数据上验证效果佳，可应对个性化医疗挑战。


<details>
  <summary>Details</summary>
Motivation: 受心理治疗应用启发，处理数据异构性和任务相关性，且数据部分缺失。

Method: 引入基于路由架构，动态选择模态处理路径和任务共享策略，多模态路径学习，端到端训练。

Result: 在合成数据和真实心理治疗笔记上，优于固定多任务或单任务基线，路由策略有可解释性。

Conclusion: 该框架能应对个性化医疗挑战，用于心理治疗可改善心理健康、提高治疗分配精度和成本效益。

Abstract: We propose a unified framework for adaptive routing in multitask, multimodal
prediction settings where data heterogeneity and task interactions vary across
samples. Motivated by applications in psychotherapy where structured
assessments and unstructured clinician notes coexist with partially missing
data and correlated outcomes, we introduce a routing-based architecture that
dynamically selects modality processing pathways and task-sharing strategies on
a per-sample basis. Our model defines multiple modality paths, including raw
and fused representations of text and numeric features and learns to route each
input through the most informative expert combination. Task-specific
predictions are produced by shared or independent heads depending on the
routing decision, and the entire system is trained end-to-end. We evaluate the
model on both synthetic data and real-world psychotherapy notes predicting
depression and anxiety outcomes. Our experiments show that our method
consistently outperforms fixed multitask or single-task baselines, and that the
learned routing policy provides interpretable insights into modality relevance
and task structure. This addresses critical challenges in personalized
healthcare by enabling per-subject adaptive information processing that
accounts for data heterogeneity and task correlations. Applied to
psychotherapy, this framework could improve mental health outcomes, enhance
treatment assignment precision, and increase clinical cost-effectiveness
through personalized intervention strategies.

</details>


### [86] [Selective Risk Certification for LLM Outputs via Information-Lift Statistics: PAC-Bayes, Robustness, and Skeleton Design](https://arxiv.org/abs/2509.12527)
*Sanjeda Akter,Ibne Farabi Shihab,Anuj Sharma*

Main category: cs.LG

TL;DR: 本文提出信息提升证书理论，有多项理论贡献，在多数据集和模型族验证假设，降低弃权率并控制运行开销。


<details>
  <summary>Details</summary>
Motivation: 大语言模型常产生似是而非的错误输出，现有启发式方法缺乏形式保证。

Method: 开发选择性分类下信息提升证书的综合理论，包括PAC - Bayes子伽马分析、骨架敏感性定理、假设违反下的失效模式保证和骨架构建的变分方法。

Result: 在六个数据集和多个模型族上，经验验证假设，在相同风险下将弃权率降低12 - 15%，运行开销控制在20%以下。

Conclusion: 所提理论有效，能降低弃权率并控制运行开销。

Abstract: Large language models often produce plausible but incorrect outputs. Existing
heuristics such as HallBayes lack formal guarantees. We develop the first
comprehensive theory of \emph{information-lift certificates} under selective
classification. Our contributions are: (i) a PAC-Bayes \emph{sub-gamma}
analysis extending beyond standard Bernstein bounds; (ii) explicit skeleton
sensitivity theorems quantifying robustness to misspecification; (iii)
failure-mode guarantees under assumption violations; and (iv) a principled
variational method for skeleton construction. Across six datasets and multiple
model families, we validate assumptions empirically, reduce abstention by
12--15\% at the same risk, and maintain runtime overhead below 20\% (further
reduced via batching).

</details>


### [87] [Profiling LoRA/QLoRA Fine-Tuning Efficiency on Consumer GPUs: An RTX 4060 Case Study](https://arxiv.org/abs/2509.12229)
*MSR Avinash*

Main category: cs.LG

TL;DR: 本文对Qwen2.5 - 1.5B - Instruct模型在单张NVIDIA RTX 4060上进行LoRA/QLoRA微调做了控制分析研究，给出不同配置下的性能指标，发现分页优化器可提升吞吐量，bf16效率不如fp16，8GB显存下2048token长度可行，为资源受限者提供基准和指南。


<details>
  <summary>Details</summary>
Motivation: 当前基于参数高效技术微调大语言模型在消费级GPU尤其是8GB显存限制下的效率研究不足，本文旨在填补这一空白。

Method: 在单张NVIDIA RTX 4060上对Qwen2.5 - 1.5B - Instruct模型进行LoRA/QLoRA微调，在三种代表性配置下系统改变批量大小、序列长度、优化器选择和精度。

Result: 分页优化器可使吞吐量最高提升25%，bf16相对fp16效率降低，8GB显存下参数高效策略可支持2048token长度。

Conclusion: 这是首个消费级GPU上大语言模型微调效率的系统案例研究，为资源受限的研究者和从业者提供了可复现的基准和实用指南。

Abstract: Fine-tuning large language models (LLMs) with parameter-efficient techniques
such as LoRA and QLoRA has enabled adaptation of foundation models on modest
hardware. Yet the efficiency of such training on consumer-grade GPUs,
especially under strict 8 GB VRAM limits, remains underexplored. We present a
controlled profiling study of LoRA/QLoRA fine-tuning using the
Qwen2.5-1.5B-Instruct model on a single NVIDIA RTX 4060. Across three
representative configurations, we systematically vary batch size, sequence
length, optimizer choice (AdamW vs. PagedAdamW), and precision (fp16 vs. bf16).
We report throughput (tokens/s), time per 10k tokens, and VRAM footprint,
alongside energy estimates derived from GPU board power limits. Our results
show that paged optimizers improve throughput by up to 25% (628 tok/s vs. 500
tok/s baseline), while bf16 degrades efficiency relative to fp16. Despite 8 GB
constraints, sequence lengths up to 2048 tokens were feasible using
parameter-efficient strategies. To our knowledge, this is the first systematic
case study of LLM fine-tuning efficiency on consumer GPUs, providing
reproducible benchmarks and practical guidelines for resource-constrained
researchers and practitioners.

</details>


### [88] [Large Language Model Scaling Laws for Neural Quantum States in Quantum Chemistry](https://arxiv.org/abs/2509.12679)
*Oliver Knitter,Dan Zhao,Stefan Leichenauer,Shravan Veerapaneni*

Main category: cs.LG

TL;DR: 本文研究神经量子态（NQS）的缩放定律，发现其模型大小与训练时间关系和语言模型不同。


<details>
  <summary>Details</summary>
Motivation: 由于神经量子态（NQS）越来越多地采用基于大语言模型（LLM）的组件，因此希望了解NQS的缩放定律，以明确NQS的可扩展性和性能 - 资源的最优权衡。

Method: 确定基于变压器的NQS在二次量子化量子化学应用中，以绝对误差和V分数衡量的性能随问题规模变化的缩放定律，并对所得参数曲线进行计算约束优化。

Result: 发现模型大小和训练时间的关系高度依赖于损失度量和ansatz，不遵循语言模型的近似线性关系。

Conclusion: NQS的缩放定律与语言模型不同，在研究NQS时不能简单套用语言模型的缩放规律。

Abstract: Scaling laws have been used to describe how large language model (LLM)
performance scales with model size, training data size, or amount of
computational resources. Motivated by the fact that neural quantum states (NQS)
has increasingly adopted LLM-based components, we seek to understand NQS
scaling laws, thereby shedding light on the scalability and optimal
performance--resource trade-offs of NQS ansatze. In particular, we identify
scaling laws that predict the performance, as measured by absolute error and
V-score, for transformer-based NQS as a function of problem size in
second-quantized quantum chemistry applications. By performing analogous
compute-constrained optimization of the obtained parametric curves, we find
that the relationship between model size and training time is highly dependent
on loss metric and ansatz, and does not follow the approximately linear
relationship found for language models.

</details>


### [89] [Flexible Multimodal Neuroimaging Fusion for Alzheimer's Disease Progression Prediction](https://arxiv.org/abs/2509.12234)
*Benjamin Burns,Yuan Xue,Douglas W. Scharre,Xia Ning*

Main category: cs.LG

TL;DR: 文章提出PerM - MoE方法提升多模态模型在模态缺失下的灵活性，用ADNI数据评估，结果显示其在多数模态缺失场景优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型在临床常见的大量模态缺失时无法准确预测AD进展，需提升模型灵活性。

Method: 引入PerM - MoE，一种用独立路由替代传统单一路由的稀疏专家混合方法，并使用ADNI的神经影像数据，对比评估PerM - MoE、Flex - MoE和单模态模型。

Result: PerM - MoE在多数模态缺失变化中表现优于现有方法，且比Flex - MoE更有效利用专家。

Conclusion: PerM - MoE能有效提升多模态模型在高模态缺失下的灵活性和预测能力。

Abstract: Alzheimer's disease (AD) is a progressive neurodegenerative disease with high
inter-patient variance in rate of cognitive decline. AD progression prediction
aims to forecast patient cognitive decline and benefits from incorporating
multiple neuroimaging modalities. However, existing multimodal models fail to
make accurate predictions when many modalities are missing during inference, as
is often the case in clinical settings. To increase multimodal model
flexibility under high modality missingness, we introduce PerM-MoE, a novel
sparse mixture-of-experts method that uses independent routers for each
modality in place of the conventional, single router. Using T1-weighted MRI,
FLAIR, amyloid beta PET, and tau PET neuroimaging data from the Alzheimer's
Disease Neuroimaging Initiative (ADNI), we evaluate PerM-MoE, state-of-the-art
Flex-MoE, and unimodal neuroimaging models on predicting two-year change in
Clinical Dementia Rating-Sum of Boxes (CDR-SB) scores under varying levels of
modality missingness. PerM-MoE outperforms the state of the art in most
variations of modality missingness and demonstrates more effective utility of
experts than Flex-MoE.

</details>


### [90] [A Graph Machine Learning Approach for Detecting Topological Patterns in Transactional Graphs](https://arxiv.org/abs/2509.12730)
*Francesco Zola,Jon Ander Medina,Andrea Venturi,Amaia Gil,Raul Orduna*

Main category: cs.LG

TL;DR: 本文提出结合图机器学习和网络分析的方法检测金融交易图中拓扑模式，经数据预处理后用图自编码器区分模式，初步结果显示该方法有效。


<details>
  <summary>Details</summary>
Motivation: 数字生态系统使金融领域面临新犯罪手段，传统规则系统难检测复杂犯罪行为，需分析主体交互的策略。

Method: 提出四步预处理框架处理传统金融数据集，后实现三种图自编码器变体区分拓扑模式。

Result: 初步结果表明该模式聚焦、拓扑驱动的方法能有效检测复杂金融犯罪方案。

Conclusion: 该方法是传统规则检测系统的有前景替代方案。

Abstract: The rise of digital ecosystems has exposed the financial sector to evolving
abuse and criminal tactics that share operational knowledge and techniques both
within and across different environments (fiat-based, crypto-assets, etc.).
Traditional rule-based systems lack the adaptability needed to detect
sophisticated or coordinated criminal behaviors (patterns), highlighting the
need for strategies that analyze actors' interactions to uncover suspicious
activities and extract their modus operandi. For this reason, in this work, we
propose an approach that integrates graph machine learning and network analysis
to improve the detection of well-known topological patterns within
transactional graphs. However, a key challenge lies in the limitations of
traditional financial datasets, which often provide sparse, unlabeled
information that is difficult to use for graph-based pattern analysis.
Therefore, we firstly propose a four-step preprocessing framework that involves
(i) extracting graph structures, (ii) considering data temporality to manage
large node sets, (iii) detecting communities within, and (iv) applying
automatic labeling strategies to generate weak ground-truth labels. Then, once
the data is processed, Graph Autoencoders are implemented to distinguish among
the well-known topological patterns. Specifically, three different GAE variants
are implemented and compared in this analysis. Preliminary results show that
this pattern-focused, topology-driven method is effective for detecting complex
financial crime schemes, offering a promising alternative to conventional
rule-based detection systems.

</details>


### [91] [RL Fine-Tuning Heals OOD Forgetting in SFT](https://arxiv.org/abs/2509.12235)
*Hangzhan Jin,Sitao Luan,Sicheng Lyu,Guillaume Rabusseau,Reihaneh Rabbany,Doina Precup,Mohammad Hamdaqa*

Main category: cs.LG

TL;DR: 研究大语言模型两阶段微调中SFT和RL的协同机制，发现‘SFT记忆，RL泛化’过于简化，揭示OOD遗忘、恢复现象及奇异向量旋转为关键机制。


<details>
  <summary>Details</summary>
Motivation: 当前SFT和RL协同的演变和机制研究不足且尚无定论，需深入探究。

Method: 研究OOD性能变化，对参数矩阵进行SVD分析、手动编辑并观察对模型性能的影响。

Result: OOD性能在SFT早期达峰值后下降；RL起OOD恢复作用；恢复能力有边界；奇异值在微调中稳定，OOD行为与奇异向量旋转强相关。

Conclusion: 重新定义SFT和RL在两阶段微调中的作用，发现奇异向量旋转是关键机制。

Abstract: The two-stage fine-tuning paradigm of Supervised Fine-Tuning (SFT) followed
by Reinforcement Learning (RL) has empirically shown better reasoning
performance than one-stage SFT for the post-training of Large Language Models
(LLMs). However, the evolution and mechanism behind the synergy of SFT and RL
are still under-explored and inconclusive. In our study, we find the well-known
claim "SFT memorizes, RL generalizes" is over-simplified, and discover that:
(1) OOD performance peaks at the early stage of SFT and then declines (OOD
forgetting), the best SFT checkpoint cannot be captured by training/test loss;
(2) the subsequent RL stage does not generate fundamentally better OOD
capability, instead it plays an \textbf{OOD restoration} role, recovering the
lost reasoning ability during SFT; (3) The recovery ability has boundaries,
\ie{} \textbf{if SFT trains for too short or too long, RL cannot recover the
lost OOD ability;} (4) To uncover the underlying mechanisms behind the
forgetting and restoration process, we employ SVD analysis on parameter
matrices, manually edit them, and observe their impacts on model performance.
Unlike the common belief that the shift of model capacity mainly results from
the changes of singular values, we find that they are actually quite stable
throughout fine-tuning. Instead, the OOD behavior strongly correlates with the
\textbf{rotation of singular vectors}. Our findings re-identify the roles of
SFT and RL in the two-stage fine-tuning and discover the rotation of singular
vectors as the key mechanism. %reversing the rotations induced by SFT, which
shows recovery from forgetting, whereas imposing the SFT parameter directions
onto a RL-tuned model results in performance degradation. Code is available at
https://github.com/xiaodanguoguo/RL_Heals_SFT

</details>


### [92] [Research on Short-Video Platform User Decision-Making via Multimodal Temporal Modeling and Reinforcement Learning](https://arxiv.org/abs/2509.12269)
*Jinmeiyang Wang,Jing Dong,Li Zhou*

Main category: cs.LG

TL;DR: 本文提出MT - DQN模型解决短视频环境下用户行为预测和推荐策略优化问题，实验显示其性能优于传统模型，但部署存在挑战待解决。


<details>
  <summary>Details</summary>
Motivation: 解决短视频环境下用户行为预测和推荐策略优化的挑战。

Method: 提出集成Transformer、Temporal Graph Neural Network (TGNN)和Deep Q - Network (DQN)的MT - DQN模型。

Result: MT - DQN在性能上优于传统拼接模型Concat - Modal，F1分数平均提高10.97%，NDCG@5平均提高8.3%；相较于Vanilla - DQN，MSE降低34.8%，MAE降低26.5%。

Conclusion: MT - DQN性能良好，但在现实场景部署存在计算成本和在线推理延迟敏感等挑战，需未来进行架构优化解决。

Abstract: This paper proposes the MT-DQN model, which integrates a Transformer,
Temporal Graph Neural Network (TGNN), and Deep Q-Network (DQN) to address the
challenges of predicting user behavior and optimizing recommendation strategies
in short-video environments. Experiments demonstrated that MT-DQN consistently
outperforms traditional concatenated models, such as Concat-Modal, achieving an
average F1-score improvement of 10.97% and an average NDCG@5 improvement of
8.3%. Compared to the classic reinforcement learning model Vanilla-DQN, MT-DQN
reduces MSE by 34.8% and MAE by 26.5%. Nonetheless, we also recognize
challenges in deploying MT-DQN in real-world scenarios, such as its
computational cost and latency sensitivity during online inference, which will
be addressed through future architectural optimization.

</details>


### [93] [Neural Diffeomorphic-Neural Operator for Residual Stress-Induced Deformation Prediction](https://arxiv.org/abs/2509.12237)
*Changqing Liu,Kaining Dai,Zhiwei Zhao,Tianyi Wu,Yingguang Li*

Main category: cs.LG

TL;DR: 提出基于微分同胚嵌入神经算子的NDNO框架，用于高效预测不同几何结构部件的加工变形，经验证具有高精度和效率。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法模拟残余应力与变形耦合计算成本高，神经算子直接应用于变化几何域有局限，需新方法高效预测不同几何结构部件加工变形。

Method: 引入NDNO框架，通过受平滑性和可逆性约束的微分同胚神经网络将复杂三维几何映射到公共参考域，在参考域训练神经算子。

Result: 所提方法能预测主方向和多方向变形场，在不同几何部件上都实现了高精度和高效率。

Conclusion: 所提方法为不同几何结构部件的变形预测提供了有效且计算高效的解决方案。

Abstract: Accurate prediction of machining deformation in structural components is
essential for ensuring dimensional precision and reliability. Such deformation
often originates from residual stress fields, whose distribution and influence
vary significantly with geometric complexity. Conventional numerical methods
for modeling the coupling between residual stresses and deformation are
computationally expensive, particularly when diverse geometries are considered.
Neural operators have recently emerged as a powerful paradigm for efficiently
solving partial differential equations, offering notable advantages in
accelerating residual stress-deformation analysis. However, their direct
application across changing geometric domains faces theoretical and practical
limitations. To address this challenge, a novel framework based on
diffeomorphic embedding neural operators named neural diffeomorphic-neural
operator (NDNO) is introduced. Complex three-dimensional geometries are
explicitly mapped to a common reference domain through a diffeomorphic neural
network constrained by smoothness and invertibility. The neural operator is
then trained on this reference domain, enabling efficient learning of
deformation fields induced by residual stresses. Once trained, both the
diffeomorphic neural network and the neural operator demonstrate efficient
prediction capabilities, allowing rapid adaptation to varying geometries. The
proposed method thus provides an effective and computationally efficient
solution for deformation prediction in structural components subject to varying
geometries. The proposed method is validated to predict both main-direction and
multi-direction deformation fields, achieving high accuracy and efficiency
across parts with diverse geometries including component types, dimensions and
features.

</details>


### [94] [Interpretable Data Mining of Follicular Thyroid Cancer Ultrasound Features Using Enhanced Association Rules](https://arxiv.org/abs/2509.12238)
*Songlin Zhou,Tao Zhou,Xin Li,Stephen Shing-Toung Yau*

Main category: cs.LG

TL;DR: 文章基于新数据挖掘工具分析滤泡状甲状腺癌临床数据，识别术前诊断临床指标。


<details>
  <summary>Details</summary>
Motivation: 滤泡状甲状腺癌缺乏独特超声征象，术前诊断难且相关临床研究不足，旨在识别有助于术前诊断的临床指标。

Method: 回顾性分析北大三院普外科2010 - 2023年病例数据，改进关联规则挖掘方法，结合SHAP方法提出新分析指标。

Result: 数据集含1673个结节病例，除常见指标外，还发现结节内结节模式、小梁模式、低TSH分数等与恶性强相关，桥本甲状腺炎合并症也可能有强恶性关联。

Conclusion: 术前诊断疑似滤泡状甲状腺癌结节时应考虑多个临床指标，研究识别的多种恶性关联可为临床医生提供参考。

Abstract: Purpose: Thyroid cancer has been a common cancer. Papillary thyroid cancer
and follicular thyroid cancer are the two most common types of thyroid cancer.
Follicular thyroid cancer lacks distinctive ultrasound signs and is more
difficult to diagnose preoperatively than the more prevalent papillary thyroid
cancer, and the clinical studies associated with it are less well established.
We aimed to analyze the clinical data of follicular thyroid cancer based on a
novel data mining tool to identify some clinical indications that may help in
preoperative diagnosis. Methods: We performed a retrospective analysis based on
case data collected by the Department of General Surgery of Peking University
Third Hospital between 2010 and 2023. Unlike traditional statistical methods,
we improved the association rule mining, a classical data mining method, and
proposed new analytical metrics reflecting the malignant association between
clinical indications and cancer with the help of the idea of SHAP method in
interpretable machine learning. Results: The dataset was preprocessed to
contain 1673 cases (in terms of nodes rather than patients), of which 1414 were
benign and 259 were malignant nodes. Our analysis pointed out that in addition
to some common indicators (e.g., irregular or lobulated nodal margins, uneven
thickness halo, hypoechogenicity), there were also some indicators with strong
malignant associations, such as nodule-in-nodule pattern, trabecular pattern,
and low TSH scores. In addition, our results suggest that the combination of
Hashimoto's thyroiditis may also have a strong malignant association.
Conclusion: In the preoperative diagnosis of nodules suspected of follicular
thyroid cancer, multiple clinical indications should be considered for a more
accurate diagnosis. The diverse malignant associations identified in our study
may serve as a reference for clinicians in related fields.

</details>


### [95] [Reversible Deep Equilibrium Models](https://arxiv.org/abs/2509.12917)
*Sam McCallum,Kamran Arora,James Foster*

Main category: cs.LG

TL;DR: 提出可逆深度平衡模型（RevDEQs），可精确计算梯度，在语言建模和图像分类任务中达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有深度平衡模型（DEQs）梯度计算近似，导致训练动态不稳定，需正则化或多次函数评估。

Method: 引入可逆深度平衡模型（RevDEQs）以实现精确梯度计算。

Result: RevDEQs在语言建模和图像分类任务中，对比可比的隐式和显式模型取得了最先进的性能。

Conclusion: RevDEQs是比DEQs更优的模型，能解决DEQs存在的问题并取得更好效果。

Abstract: Deep Equilibrium Models (DEQs) are an interesting class of implicit model
where the model output is implicitly defined as the fixed point of a learned
function. These models have been shown to outperform explicit (fixed-depth)
models in large-scale tasks by trading many deep layers for a single layer that
is iterated many times. However, gradient calculation through DEQs is
approximate. This often leads to unstable training dynamics and requires
regularisation or many function evaluations to fix. Here, we introduce
Reversible Deep Equilibrium Models (RevDEQs) that allow for exact gradient
calculation, no regularisation and far fewer function evaluations than DEQs. We
show that RevDEQs achieve state-of-the-art performance on language modelling
and image classification tasks against comparable implicit and explicit models.

</details>


### [96] [InJecteD: Analyzing Trajectories and Drift Dynamics in Denoising Diffusion Probabilistic Models for 2D Point Cloud Generation](https://arxiv.org/abs/2509.12239)
*Sanyam Jain,Khuram Naveed,Illia Oleksiienko,Alexandros Iosifidis,Ruben Pauwels*

Main category: cs.LG

TL;DR: 介绍InJecteD框架用于解释DDPMs，分析2D点云生成去噪过程样本轨迹，实验揭示去噪阶段，评估模型配置发现Fourier嵌入效果好。


<details>
  <summary>Details</summary>
Motivation: 增强DDPMs模型透明度，支持人机协作，让从业者调试和优化生成模型。

Method: 使用简化DDPM架构和可定制输入与时间嵌入，应用于三个数据集，用Wasserstein距离和余弦相似度等统计指标量化轨迹属性。

Result: 实验揭示不同去噪阶段，有数据集特定行为，评估四种模型配置发现Fourier嵌入能提高轨迹稳定性和重建质量。

Conclusion: InJecteD框架通过分析样本轨迹增强了DDPMs模型透明度，Fourier嵌入对模型性能有积极影响。

Abstract: This work introduces InJecteD, a framework for interpreting Denoising
Diffusion Probabilistic Models (DDPMs) by analyzing sample trajectories during
the denoising process of 2D point cloud generation. We apply this framework to
three datasets from the Datasaurus Dozen bullseye, dino, and circle using a
simplified DDPM architecture with customizable input and time embeddings. Our
approach quantifies trajectory properties, including displacement, velocity,
clustering, and drift field dynamics, using statistical metrics such as
Wasserstein distance and cosine similarity. By enhancing model transparency,
InJecteD supports human AI collaboration by enabling practitioners to debug and
refine generative models. Experiments reveal distinct denoising phases: initial
noise exploration, rapid shape formation, and final refinement, with
dataset-specific behaviors example, bullseyes concentric convergence vs. dinos
complex contour formation. We evaluate four model configurations, varying
embeddings and noise schedules, demonstrating that Fourier based embeddings
improve trajectory stability and reconstruction quality

</details>


### [97] [Causal Discovery via Quantile Partial Effect](https://arxiv.org/abs/2509.12981)
*Yikang Chen,Xingzhe Sun,Dehui Du*

Main category: cs.LG

TL;DR: 本文研究条件分位数回归相关的分位数部分效应（QPE），基于QPE假设实现因果方向与顺序识别，实验验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 推广基于功能因果模型的可识别性结果，探索不依赖机制、噪声和马尔可夫假设的因果识别方法。

Method: 假设QPE位于有限线性空间，通过对估计的QPE进行基函数测试区分因果方向；利用QPE与得分函数的联系，以Fisher信息确定因果顺序。

Result: 在大量二元因果发现数据集上验证了区分因果方向的有效性；在多个合成和真实世界的多元因果发现数据集上验证了用Fisher信息确定因果顺序的可行性。

Conclusion: 基于QPE的因果识别方法有效，可推广之前结果且不依赖复杂假设。

Abstract: Quantile Partial Effect (QPE) is a statistic associated with conditional
quantile regression, measuring the effect of covariates at different levels.
Our theory demonstrates that when the QPE of cause on effect is assumed to lie
in a finite linear span, cause and effect are identifiable from their
observational distribution. This generalizes previous identifiability results
based on Functional Causal Models (FCMs) with additive, heteroscedastic noise,
etc. Meanwhile, since QPE resides entirely at the observational level, this
parametric assumption does not require considering mechanisms, noise, or even
the Markov assumption, but rather directly utilizes the asymmetry of shape
characteristics in the observational distribution. By performing basis function
tests on the estimated QPE, causal directions can be distinguished, which is
empirically shown to be effective in experiments on a large number of bivariate
causal discovery datasets. For multivariate causal discovery, leveraging the
close connection between QPE and score functions, we find that Fisher
Information is sufficient as a statistical measure to determine causal order
when assumptions are made about the second moment of QPE. We validate the
feasibility of using Fisher Information to identify causal order on multiple
synthetic and real-world multivariate causal discovery datasets.

</details>


### [98] [Why and How Auxiliary Tasks Improve JEPA Representations](https://arxiv.org/abs/2509.12249)
*Jiacan Yu,Siyi Chen,Mingrui Liu,Nono Horiuchi,Vladimir Braverman,Zicheng Xu,Dan Haramati,Randall Balestriero*

Main category: cs.LG

TL;DR: 文章对带辅助回归头的JEPA变体进行理论分析，证明定理并通过实验验证，指出改进JEPA编码器的方向。


<details>
  <summary>Details</summary>
Motivation: JEPA在视觉表征学习和基于模型的强化学习中应用增多，但行为理解不足，需对其进行理论刻画。

Method: 对带辅助回归头的JEPA变体进行理论分析，证明No Unhealthy Representation Collapse定理，并在计数环境中进行受控消融实验。

Result: 证明定理，实验表明联合训练JEPA模型和辅助头比单独训练产生更丰富的表征。

Conclusion: 通过与转移动态一起编码正确等价关系的辅助函数训练JEPA编码器是改进的方向。

Abstract: Joint-Embedding Predictive Architecture (JEPA) is increasingly used for
visual representation learning and as a component in model-based RL, but its
behavior remains poorly understood. We provide a theoretical characterization
of a simple, practical JEPA variant that has an auxiliary regression head
trained jointly with latent dynamics. We prove a No Unhealthy Representation
Collapse theorem: in deterministic MDPs, if training drives both the
latent-transition consistency loss and the auxiliary regression loss to zero,
then any pair of non-equivalent observations, i.e., those that do not have the
same transition dynamics or auxiliary label, must map to distinct latent
representations. Thus, the auxiliary task anchors which distinctions the
representation must preserve. Controlled ablations in a counting environment
corroborate the theory and show that training the JEPA model jointly with the
auxiliary head generates a richer representation than training them separately.
Our work indicates a path to improve JEPA encoders: training them with an
auxiliary function that, together with the transition dynamics, encodes the
right equivalence relations.

</details>


### [99] [Representation Learning on Large Non-Bipartite Transaction Networks using GraphSAGE](https://arxiv.org/abs/2509.12255)
*Mihir Tare,Clemens Rattasits,Yiming Wu,Euan Wielewski*

Main category: cs.LG

TL;DR: 本文将GraphSAGE应用于银行非二分异构交易网络，构建交易网络训练模型生成节点嵌入，发现可解释簇，用于下游分类任务，凸显其在银行网络的适应性。


<details>
  <summary>Details</summary>
Motivation: 金融机构需要可扩展工具分析复杂交易网络，传统图嵌入方法难以处理动态银行数据。

Method: 构建匿名客户和商家交易网络，训练GraphSAGE模型生成节点嵌入。

Result: 嵌入显示与地理和人口属性对齐的可解释簇，用于资金骡子检测模型可改善高风险账户优先级排序。

Conclusion: 该框架适用于银行规模网络，具有归纳能力、可扩展性和可解释性，为金融机构利用图机器学习提供蓝图。

Abstract: Financial institutions increasingly require scalable tools to analyse complex
transactional networks, yet traditional graph embedding methods struggle with
dynamic, real-world banking data. This paper demonstrates the practical
application of GraphSAGE, an inductive Graph Neural Network framework, to
non-bipartite heterogeneous transaction networks within a banking context.
Unlike transductive approaches, GraphSAGE scales well to large networks and can
generalise to unseen nodes which is critical for institutions working with
temporally evolving transactional data. We construct a transaction network
using anonymised customer and merchant transactions and train a GraphSAGE model
to generate node embeddings. Our exploratory work on the embeddings reveals
interpretable clusters aligned with geographic and demographic attributes.
Additionally, we illustrate their utility in downstream classification tasks by
applying them to a money mule detection model where using these embeddings
improves the prioritisation of high-risk accounts. Beyond fraud detection, our
work highlights the adaptability of this framework to banking-scale networks,
emphasising its inductive capability, scalability, and interpretability. This
study provides a blueprint for financial organisations to harness graph machine
learning for actionable insights in transactional ecosystems.

</details>


### [100] [Single-stream Policy Optimization](https://arxiv.org/abs/2509.13232)
*Zhongwen Xu,Zihan Ding*

Main category: cs.LG

TL;DR: 本文从单流视角重新审视大语言模型的策略梯度优化，提出单流策略优化（SPO）方法，实验表明其优于GRPO，为LLM推理提供更优路径。


<details>
  <summary>Details</summary>
Motivation: 现有基于组的策略梯度优化方法（如GRPO）存在退化组消除学习信号和同步障碍影响可扩展性的问题，需要改进。

Method: 引入单流策略优化（SPO），用持久的、KL自适应值跟踪器替代每组基线，并对批次内的优势进行全局归一化。

Result: 使用Qwen3 - 8B的实验显示，SPO比GRPO收敛更平滑、准确率更高，消除了退化组的计算浪费，在五个数学基准测试中平均maj@32提高3.4个百分点。

Conclusion: SPO的成功挑战了为强化学习算法增加额外复杂性的趋势，强调基本原理应驱动LLM推理的下一波进展。

Abstract: We revisit policy-gradient optimization for Large Language Models (LLMs) from
a single-stream perspective. Prevailing group-based methods like GRPO reduce
variance with on-the-fly baselines but suffer from critical flaws: frequent
degenerate groups erase learning signals, and synchronization barriers hinder
scalability. We introduce Single-stream Policy Optimization (SPO), which
eliminates these issues by design. SPO replaces per-group baselines with a
persistent, KL-adaptive value tracker and normalizes advantages globally across
the batch, providing a stable, low-variance learning signal for every sample.
Being group-free, SPO enables higher throughput and scales effectively in
long-horizon or tool-integrated settings where generation times vary.
Furthermore, the persistent value tracker naturally enables an adaptive
curriculum via prioritized sampling. Experiments using Qwen3-8B show that SPO
converges more smoothly and attains higher accuracy than GRPO, while
eliminating computation wasted on degenerate groups. Ablation studies confirm
that SPO's gains stem from its principled approach to baseline estimation and
advantage normalization, offering a more robust and efficient path for LLM
reasoning. Across five hard math benchmarks with Qwen3 8B, SPO improves the
average maj@32 by +3.4 percentage points (pp) over GRPO, driven by substantial
absolute point gains on challenging datasets, including +7.3 pp on BRUMO 25,
+4.4 pp on AIME 25, +3.3 pp on HMMT 25, and achieves consistent relative gain
in pass@$k$ across the evaluated $k$ values. SPO's success challenges the
prevailing trend of adding incidental complexity to RL algorithms, highlighting
a path where fundamental principles, not architectural workarounds, drive the
next wave of progress in LLM reasoning.

</details>


### [101] [Quantum-Inspired Stacked Integrated Concept Graph Model (QISICGM) for Diabetes Risk Prediction](https://arxiv.org/abs/2509.12259)
*Kenneth G. Young II*

Main category: cs.LG

TL;DR: 提出量子启发的堆叠集成概念图模型（QISICGM）用于糖尿病风险预测，表现优于传统方法，强调可信AI。


<details>
  <summary>Details</summary>
Motivation: 提高糖尿病风险预测的准确性和效率，解决传统方法的不足，实现可信AI。

Method: 利用PIMA印第安人糖尿病数据集并添加合成样本，将自改进概念图与包含多种模型的堆叠集成相结合，引入量子启发元素。

Result: OOF F1分数达0.8933，AUC为0.8699，CPU推理效率为每秒8.5行，优于传统方法。

Conclusion: QISICGM有潜力成为糖尿病等临床分诊的基准，强调通过校准、可解释性和开源重现性实现可信AI。

Abstract: The Quantum-Inspired Stacked Integrated Concept Graph Model (QISICGM) is an
innovative machine learning framework that harnesses quantum-inspired
techniques to predict diabetes risk with exceptional accuracy and efficiency.
Utilizing the PIMA Indians Diabetes dataset augmented with 2,000 synthetic
samples to mitigate class imbalance (total: 2,768 samples, 1,949 positives),
QISICGM integrates a self-improving concept graph with a stacked ensemble
comprising Random Forests (RF), Extra Trees (ET), transformers, convolutional
neural networks (CNNs), and feed-forward neural networks (FFNNs). This approach
achieves an out-of-fold (OOF) F1 score of 0.8933 and an AUC of 0.8699,
outperforming traditional methods. Quantum inspired elements, such as phase
feature mapping and neighborhood sequence modeling, enrich feature
representations, enabling CPU-efficient inference at 8.5 rows per second. This
paper presents a detailed architecture, theoretical foundations, code insights,
and performance evaluations, including visualizations from the outputs
subfolder. The open-source implementation (v1.0.0) is available at
https://github.com/keninayoung/QISICGM, positioning QISICGM as a potential
benchmark for AI-assisted clinical triage in diabetes and beyond. Ultimately,
this work emphasizes trustworthy AI through calibration, interpretability, and
open-source reproducibility.

</details>


### [102] [Energy-Efficient Quantized Federated Learning for Resource-constrained IoT devices](https://arxiv.org/abs/2509.12814)
*Wilfrid Sougrinoma Compaoré,Yaya Etiabi,El Mehdi Amhoud,Mohamad Assaad*

Main category: cs.LG

TL;DR: 本文提出适用于物联网网络的联邦学习框架，结合有限码长传输、模型量化和误差感知聚合机制，优化上行传输功率，模拟显示可降低能耗且保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 资源受限的物联网设备在联邦学习中面临能源有限、通信不可靠等挑战，需要解决方案。

Method: 提出结合有限码长传输、模型量化和误差感知聚合机制的联邦学习框架，优化上行传输功率。

Result: 与标准联邦学习模型相比，所提方法最多可降低75%能耗，同时保持稳健的模型精度。

Conclusion: 该工作为实际物联网部署中实现高效可靠的联邦学习铺平道路。

Abstract: Federated Learning (FL) has emerged as a promising paradigm for enabling
collaborative machine learning while preserving data privacy, making it
particularly suitable for Internet of Things (IoT) environments. However,
resource-constrained IoT devices face significant challenges due to limited
energy,unreliable communication channels, and the impracticality of assuming
infinite blocklength transmission. This paper proposes a federated learning
framework for IoT networks that integrates finite blocklength transmission,
model quantization, and an error-aware aggregation mechanism to enhance energy
efficiency and communication reliability. The framework also optimizes uplink
transmission power to balance energy savings and model performance. Simulation
results demonstrate that the proposed approach significantly reduces energy
consumption by up to 75\% compared to a standard FL model, while maintaining
robust model accuracy, making it a viable solution for FL in real-world IoT
scenarios with constrained resources. This work paves the way for efficient and
reliable FL implementations in practical IoT deployments. Index Terms:
Federated learning, IoT, finite blocklength, quantization, energy efficiency.

</details>


### [103] [Explainable Fraud Detection with GNNExplainer and Shapley Values](https://arxiv.org/abs/2509.12262)
*Ngoc Hieu Dao*

Main category: cs.LG

TL;DR: 数字支付普及使金融欺诈风险上升，需提高人工智能欺诈检测系统透明度，论文聚焦开发可解释欺诈检测器


<details>
  <summary>Details</summary>
Motivation: 数字支付普及使金融欺诈风险增加，社会和监管机构对人工智能欺诈检测系统透明度提出更高要求，欺诈分析师也需要简洁易懂的解释以提高调查效率

Method: 未提及

Result: 未提及

Conclusion: 未提及

Abstract: The risk of financial fraud is increasing as digital payments are used more
and more frequently. Although the use of artificial intelligence systems for
fraud detection is widespread, society and regulators have raised the standards
for these systems' transparency for reliability verification purposes. To
increase their effectiveness in conducting fraud investigations, fraud analysts
also profit from having concise and understandable explanations. To solve these
challenges, the paper will concentrate on developing an explainable fraud
detector.

</details>


### [104] [Deriving the Scaled-Dot-Function via Maximum Likelihood Estimation and Maximum Entropy Approach](https://arxiv.org/abs/2509.12285)
*Jiyong Ma*

Main category: cs.LG

TL;DR: 提出最大似然估计方法确定Transformer模型中值向量，用高斯分布建模，给出对相关函数的新解释及基于最大熵方法的解释。


<details>
  <summary>Details</summary>
Motivation: 为Transformer模型中值向量确定及相关函数提供新解释。

Method: 将值向量、键向量和查询向量序列建模为高斯分布，还使用最大熵方法，用查询向量和键向量推导最大熵模型的特征函数。

Result: 对Transformer架构中缩放点积函数或softmax函数给出新解释。

Conclusion: 提出的方法可能为Transformer相关函数提供新的理解视角。

Abstract: In this paper, we present a maximum likelihood estimation approach to
determine the value vector in transformer models. We model the sequence of
value vectors, key vectors, and the query vector as a sequence of Gaussian
distributions. The variance in each Gaussian distribution depends on the time
step, the corresponding key vector, and the query vector. The mean value in
each Gaussian distribution depends on the time step, and the corresponding
value vector. This analysis may offer a new explanation of the
scaled-dot-product function or softmax function used in transformer
architectures [1]. Another explanation, inspired by [4], is based on the
maximum entropy approach in natural language processing [5]. In this approach,
a query vector and key vectors are used to derive the feature functions for the
maximum entropy model.

</details>


### [105] [Prediction of Stocks Index Price using Quantum GANs](https://arxiv.org/abs/2509.12286)
*Sangram Deshpande,Gopal Ramesh Dahale,Sai Nandan Morapakula,Uday Wad*

Main category: cs.LG

TL;DR: 本文研究QGANs用于股票价格预测，结果显示其在收敛速度和预测准确性上优于经典模型，是量子计算融入金融预测的关键一步。


<details>
  <summary>Details</summary>
Motivation: 金融市场复杂，传统模型难以捕捉其高波动性和复杂模式，需新方法进行股票价格预测。

Method: 实现针对股票价格预测的QGAN模型，用历史股票市场数据评估其性能，使用股票指数价格数据和AWS Braket SV1模拟器训练QGAN电路。

Result: QGANs能生成与实际市场行为相近的合成数据，提高预测准确性，在收敛速度和预测准确性上优于经典LSTM和GAN模型。

Conclusion: 该研究是量子计算融入金融预测的关键一步，对交易者、金融分析师和研究人员有重要意义。

Abstract: This paper investigates the application of Quantum Generative Adversarial
Networks (QGANs) for stock price prediction. Financial markets are inherently
complex, marked by high volatility and intricate patterns that traditional
models often fail to capture. QGANs, leveraging the power of quantum computing,
offer a novel approach by combining the strengths of generative models with
quantum machine learning techniques. We implement a QGAN model tailored for
stock price prediction and evaluate its performance using historical stock
market data. Our results demonstrate that QGANs can generate synthetic data
closely resembling actual market behavior, leading to enhanced prediction
accuracy. The experiment was conducted using the Stocks index price data and
the AWS Braket SV1 simulator for training the QGAN circuits. The
quantum-enhanced model outperforms classical Long Short-Term Memory (LSTM) and
GAN models in terms of convergence speed and prediction accuracy. This research
represents a key step toward integrating quantum computing in financial
forecasting, offering potential advantages in speed and precision over
traditional methods. The findings suggest important implications for traders,
financial analysts, and researchers seeking advanced tools for market analysis.

</details>


### [106] [C3DE: Causal-Aware Collaborative Neural Controlled Differential Equation for Long-Term Urban Crowd Flow Prediction](https://arxiv.org/abs/2509.12289)
*Yuting Liu,Qiang Zhou,Hanzhe Li,Chenqi Gong,Jingjing Gu*

Main category: cs.LG

TL;DR: 提出C3DE模型进行长期城市人流预测，实验证明其性能优越


<details>
  <summary>Details</summary>
Motivation: 长期城市人流预测存在累积采样误差问题，且人流与POI分布多时间尺度异步动态及潜在虚假因果关系给应用NCDEs带来挑战

Method: 引入双路径NCDE作为骨干，设计基于反事实因果效应估计器的动态校正机制，利用预测器进行长期预测

Result: 在三个真实世界数据集上实验，C3DE表现优越，在人流波动明显城市更突出

Conclusion: C3DE能有效解决长期城市人流预测问题，有较好性能

Abstract: Long-term urban crowd flow prediction suffers significantly from cumulative
sampling errors, due to increased sequence lengths and sampling intervals,
which inspired us to leverage Neural Controlled Differential Equations (NCDEs)
to mitigate this issue. However, regarding the crucial influence of Points of
Interest (POIs) evolution on long-term crowd flow, the multi-timescale
asynchronous dynamics between crowd flow and POI distribution, coupled with
latent spurious causality, poses challenges to applying NCDEs for long-term
urban crowd flow prediction. To this end, we propose Causal-aware Collaborative
neural CDE (C3DE) to model the long-term dynamic of crowd flow. Specifically,
we introduce a dual-path NCDE as the backbone to effectively capture the
asynchronous evolution of collaborative signals across multiple time scales.
Then, we design a dynamic correction mechanism with the counterfactual-based
causal effect estimator to quantify the causal impact of POIs on crowd flow and
minimize the accumulation of spurious correlations. Finally, we leverage a
predictor for long-term prediction with the fused collaborative signals of POI
and crowd flow. Extensive experiments on three real-world datasets demonstrate
the superior performance of C3DE, particularly in cities with notable flow
fluctuations.

</details>


### [107] [Spontaneous Kolmogorov-Arnold Geometry in Shallow MLPs](https://arxiv.org/abs/2509.12326)
*Michael Freedman,Michael Mulligan*

Main category: cs.LG

TL;DR: 研究发现训练普通单隐藏层神经网络时常常会产生KA几何，通过J(x)的外幂统计特性量化，对其出现位置有初步理解。


<details>
  <summary>Details</summary>
Motivation: 一是理解神经网络如何自然学习为下游处理准备输入数据，二是了解KA几何出现情况以通过适时干预网络超参数加速学习。

Method: 通过J(x)外幂的统计特性（零行数量和J(x)子式统计的各种可观测值）量化KA几何。

Result: 训练普通单隐藏层神经网络时经常产生KA几何，对KA几何在函数复杂度和模型超参数空间中的出现位置有初步理解。

Conclusion: 不将KA设计到神经网络中，而是观察其在浅层MLP中自然出现，这是KANs研究的另一面。

Abstract: The Kolmogorov-Arnold (KA) representation theorem constructs universal, but
highly non-smooth inner functions (the first layer map) in a single
(non-linear) hidden layer neural network. Such universal functions have a
distinctive local geometry, a "texture," which can be characterized by the
inner function's Jacobian $J({\mathbf{x}})$, as $\mathbf{x}$ varies over the
data. It is natural to ask if this distinctive KA geometry emerges through
conventional neural network optimization. We find that indeed KA geometry often
is produced when training vanilla single hidden layer neural networks. We
quantify KA geometry through the statistical properties of the exterior powers
of $J(\mathbf{x})$: number of zero rows and various observables for the minor
statistics of $J(\mathbf{x})$, which measure the scale and axis alignment of
$J(\mathbf{x})$. This leads to a rough understanding for where KA geometry
occurs in the space of function complexity and model hyperparameters. The
motivation is first to understand how neural networks organically learn to
prepare input data for later downstream processing and, second, to learn enough
about the emergence of KA geometry to accelerate learning through a timely
intervention in network hyperparameters. This research is the "flip side" of
KA-Networks (KANs). We do not engineer KA into the neural network, but rather
watch KA emerge in shallow MLPs.

</details>


### [108] [Integrating Attention-Enhanced LSTM and Particle Swarm Optimization for Dynamic Pricing and Replenishment Strategies in Fresh Food Supermarkets](https://arxiv.org/abs/2509.12339)
*Xianchen Liu,Tianhui Zhang,Xinyu Zhang,Lingmin Hou,Zhen Guo,Yuanhao Tian,Yang Liu*

Main category: cs.LG

TL;DR: 本文结合LSTM网络与PSO算法优化生鲜超市定价和补货策略，能提高利润、减少食物浪费。


<details>
  <summary>Details</summary>
Motivation: 优化生鲜超市定价和补货策略，提高利润并减少食物浪费，实现可持续运营。

Method: 用带注意力机制的LSTM模型预测七天内销量、价格趋势和损耗率，将预测结果作为PSO算法输入，结合成本加成定价进行动态调整。

Result: 该框架能提高利润、减少食物浪费，注意力机制增强了LSTM模型可解释性，提高决策准确性。

Conclusion: 该方法弥合了预测建模和优化之间的差距，为生鲜零售等易腐商品行业提供了可扩展的动态定价和库存管理解决方案。

Abstract: This paper presents a novel approach to optimizing pricing and replenishment
strategies in fresh food supermarkets by combining Long Short-Term Memory
(LSTM) networks with Particle Swarm Optimization (PSO). The LSTM model,
enhanced with an attention mechanism, is used to predict sales volumes, pricing
trends, and spoilage rates over a seven-day period. The predictions generated
by the LSTM model serve as inputs for the PSO algorithm, which iteratively
optimizes pricing and replenishment strategies to maximize profitability while
adhering to inventory constraints. The integration of cost-plus pricing allows
for dynamic adjustments based on fixed and variable costs, ensuring real-time
adaptability to market fluctuations. The framework not only maximizes profits
but also reduces food waste, contributing to more sustainable supermarket
operations. The attention mechanism enhances the interpretability of the LSTM
model by identifying key time points and factors influencing sales, improving
decision-making accuracy. This methodology bridges the gap between predictive
modeling and optimization, offering a scalable solution for dynamic pricing and
inventory management in fresh food retail and other industries dealing with
perishable goods.

</details>


### [109] [FEDONet : Fourier-Embedded DeepONet for Spectrally Accurate Operator Learning](https://arxiv.org/abs/2509.12344)
*Arth Sojitra,Mrigank Dhingra,Omer San*

Main category: cs.LG

TL;DR: 本文提出Fourier-embedded DeepONet (FEDONet)，在多种PDE数据集上表现优于传统DeepONet，提升了解决方案重建精度。


<details>
  <summary>Details</summary>
Motivation: 标准DeepONet在捕捉复杂PDE空间结构时有局限性，需要改进。

Method: 在DeepONet架构中引入Fourier-embedded trunk networks，利用随机Fourier特征映射丰富空间表示能力。

Result: FEDONet在多种PDE数据集上表现优于传统DeepONet，平均相对L2性能提升2 - 3倍。

Conclusion: Fourier嵌入可有效增强神经算子学习，为PDE代理建模提供了强大且广泛适用的方法。

Abstract: Deep Operator Networks (DeepONets) have recently emerged as powerful
data-driven frameworks for learning nonlinear operators, particularly suited
for approximating solutions to partial differential equations (PDEs). Despite
their promising capabilities, the standard implementation of DeepONets, which
typically employs fully connected linear layers in the trunk network, can
encounter limitations in capturing complex spatial structures inherent to
various PDEs. To address this, we introduce Fourier-embedded trunk networks
within the DeepONet architecture, leveraging random Fourier feature mappings to
enrich spatial representation capabilities. Our proposed Fourier-embedded
DeepONet, FEDONet demonstrates superior performance compared to the traditional
DeepONet across a comprehensive suite of PDE-driven datasets, including the
two-dimensional Poisson equation, Burgers' equation, the Lorenz-63 chaotic
system, Eikonal equation, Allen-Cahn equation, Kuramoto-Sivashinsky equation,
and the Lorenz-96 system. Empirical evaluations of FEDONet consistently show
significant improvements in solution reconstruction accuracy, with average
relative L2 performance gains ranging between 2-3x compared to the DeepONet
baseline. This study highlights the effectiveness of Fourier embeddings in
enhancing neural operator learning, offering a robust and broadly applicable
methodology for PDE surrogate modeling.

</details>


### [110] [Linear Dimensionality Reduction for Word Embeddings in Tabular Data Classification](https://arxiv.org/abs/2509.12346)
*Liam Ressel,Hamza A. A. Gardi*

Main category: cs.LG

TL;DR: 本文针对工程师薪资预测挑战，研究PCA和LDA处理词嵌入，提出Partitioned - LDA方法，提升词嵌入在有限训练样本表格数据分类中的性能。


<details>
  <summary>Details</summary>
Motivation: 工程师薪资预测挑战中，词嵌入增加数据维度且训练样本有限，线性降维在表格数据分类中研究不足。

Method: 研究PCA和LDA，提出Partitioned - LDA方法，将嵌入拆分为等大小块分别进行LDA。

Result: 合适子空间维度的PCA优于原始嵌入；LDA无正则化表现差，应用收缩法显著提升性能；Partitioned - LDA优于常规LDA，结合收缩法在竞赛公共排行榜获前10准确率。

Conclusion: 该方法有效提升词嵌入在有限训练样本表格数据分类中的性能。

Abstract: The Engineers' Salary Prediction Challenge requires classifying salary
categories into three classes based on tabular data. The job description is
represented as a 300-dimensional word embedding incorporated into the tabular
features, drastically increasing dimensionality. Additionally, the limited
number of training samples makes classification challenging. Linear
dimensionality reduction of word embeddings for tabular data classification
remains underexplored. This paper studies Principal Component Analysis (PCA)
and Linear Discriminant Analysis (LDA). We show that PCA, with an appropriate
subspace dimension, can outperform raw embeddings. LDA without regularization
performs poorly due to covariance estimation errors, but applying shrinkage
improves performance significantly, even with only two dimensions. We propose
Partitioned-LDA, which splits embeddings into equal-sized blocks and performs
LDA separately on each, thereby reducing the size of the covariance matrices.
Partitioned-LDA outperforms regular LDA and, combined with shrinkage, achieves
top-10 accuracy on the competition public leaderboard. This method effectively
enhances word embedding performance in tabular data classification with limited
training samples.

</details>


### [111] [Unsupervised Atomic Data Mining via Multi-Kernel Graph Autoencoders for Machine Learning Force Fields](https://arxiv.org/abs/2509.12358)
*Hong Sun,Joshua A. Vita,Amit Samanta,Vincenzo Lordi*

Main category: cs.LG

TL;DR: 提出MEAGraph模型用于分析原子数据集，可有效去除采样偏差，用于数据分析等。


<details>
  <summary>Details</summary>
Motivation: 常见数据集生成技术易在势能面区域过采样，传统方法有信息损失或难识别不同区域等问题。

Method: 引入Multi - kernel Edge Attention - based Graph Autoencoder (MEAGraph)模型，结合多线性核变换和基于注意力的消息传递。

Result: 在铌、钽和铁数据集上，MEAGraph能有效分组相似原子环境，可用基本剪枝技术去除采样偏差。

Conclusion: 该方法是有效的表示学习和聚类方法，可用于数据分析、异常值检测和数据集优化。

Abstract: Constructing a chemically diverse dataset while avoiding sampling bias is
critical to training efficient and generalizable force fields. However, in
computational chemistry and materials science, many common dataset generation
techniques are prone to oversampling regions of the potential energy surface.
Furthermore, these regions can be difficult to identify and isolate from each
other or may not align well with human intuition, making it challenging to
systematically remove bias in the dataset. While traditional clustering and
pruning (down-sampling) approaches can be useful for this, they can often lead
to information loss or a failure to properly identify distinct regions of the
potential energy surface due to difficulties associated with the high
dimensionality of atomic descriptors. In this work, we introduce the
Multi-kernel Edge Attention-based Graph Autoencoder (MEAGraph) model, an
unsupervised approach for analyzing atomic datasets. MEAGraph combines multiple
linear kernel transformations with attention-based message passing to capture
geometric sensitivity and enable effective dataset pruning without relying on
labels or extensive training. Demonstrated applications on niobium, tantalum,
and iron datasets show that MEAGraph efficiently groups similar atomic
environments, allowing for the use of basic pruning techniques for removing
sampling bias. This approach provides an effective method for representation
learning and clustering that can be used for data analysis, outlier detection,
and dataset optimization.

</details>


### [112] [Enhancing Smart Farming Through Federated Learning: A Secure, Scalable, and Efficient Approach for AI-Driven Agriculture](https://arxiv.org/abs/2509.12363)
*Ritesh Janga,Rushit Dave*

Main category: cs.LG

TL;DR: 本文提出用于智能农业的联邦学习框架，旨在为明尼苏达州农场提供可扩展、高效且安全的作物疾病检测解决方案，兼顾数据隐私。


<details>
  <summary>Details</summary>
Motivation: 农业领域对数据驱动决策需求增加，但农场担忧数据隐私，需提供安全高效的疾病检测方法。

Method: 从明尼苏达州农场收集数据，应用本地深度学习算法、迁移学习，通过中央聚合服务器进行模型优化。

Result: 未提及实际结果，仅阐述预期成果，包括提高疾病检测准确性、提升泛化能力、降低通信和训练成本、提前识别和干预疾病。

Conclusion: 该框架有望结合先进机器学习技术与农民对数据隐私的实际需求，为智能农业系统带来变革。

Abstract: The agricultural sector is undergoing a transformation with the integration
of advanced technologies, particularly in data-driven decision-making. This
work proposes a federated learning framework for smart farming, aiming to
develop a scalable, efficient, and secure solution for crop disease detection
tailored to the environmental and operational conditions of Minnesota farms. By
maintaining sensitive farm data locally and enabling collaborative model
updates, our proposed framework seeks to achieve high accuracy in crop disease
classification without compromising data privacy. We outline a methodology
involving data collection from Minnesota farms, application of local deep
learning algorithms, transfer learning, and a central aggregation server for
model refinement, aiming to achieve improved accuracy in disease detection,
good generalization across agricultural scenarios, lower costs in communication
and training time, and earlier identification and intervention against diseases
in future implementations. We outline a methodology and anticipated outcomes,
setting the stage for empirical validation in subsequent studies. This work
comes in a context where more and more demand for data-driven interpretations
in agriculture has to be weighed with concerns about privacy from farms that
are hesitant to share their operational data. This will be important to provide
a secure and efficient disease detection method that can finally revolutionize
smart farming systems and solve local agricultural problems with data
confidentiality. In doing so, this paper bridges the gap between advanced
machine learning techniques and the practical, privacy-sensitive needs of
farmers in Minnesota and beyond, leveraging the benefits of federated learning.

</details>


### [113] [Explainable Unsupervised Multi-Anomaly Detection and Temporal Localization in Nuclear Times Series Data with a Dual Attention-Based Autoencoder](https://arxiv.org/abs/2509.12372)
*Konstantinos Vasili,Zachery T. Dahm,Stylianos Chatzidakis*

Main category: cs.LG

TL;DR: 本文提出基于带双注意力机制的LSTM自编码器的无监督方法，用于实际反应堆辐射区域监测系统异常事件表征，并用真实数据集评估。


<details>
  <summary>Details</summary>
Motivation: 核工业发展使远程自主或半自主控制系统受关注，准确诊断模块是关键一步，但现有ML和DL方法存在缺乏可解释性等挑战。

Method: 提出基于带双注意力机制的LSTM自编码器的无监督方法，框架包括异常检测和定位，注意力机制在特征和时间维度操作。

Result: 用PUR - 1研究堆复杂度渐增的真实数据集进行评估。

Conclusion: 该框架能在单一统一网络中识别受影响传感器和每个异常的持续时间。

Abstract: The nuclear industry is advancing toward more new reactor designs, with
next-generation reactors expected to be smaller in scale and power output.
These systems have the potential to produce large volumes of information in the
form of multivariate time-series data, which could be used for enhanced
real-time monitoring and control. In this context, the development of remote
autonomous or semi-autonomous control systems for reactor operation has gained
significant interest. A critical first step toward such systems is an accurate
diagnostics module capable of detecting and localizing anomalies within the
reactor system. Recent studies have proposed various ML and DL approaches for
anomaly detection in the nuclear domain. Despite promising results, key
challenges remain, including limited to no explainability, lack of access to
real-world data, and scarcity of abnormal events, which impedes benchmarking
and characterization. Most existing studies treat these methods as black boxes,
while recent work highlights the need for greater interpretability of ML/DL
outputs in safety-critical domains. Here, we propose an unsupervised
methodology based on an LSTM autoencoder with a dual attention mechanism for
characterization of abnormal events in a real-world reactor radiation area
monitoring system. The framework includes not only detection but also
localization of the event and was evaluated using real-world datasets of
increasing complexity from the PUR-1 research reactor. The attention mechanisms
operate in both the feature and temporal dimensions, where the feature
attention assigns weights to radiation sensors exhibiting abnormal patterns,
while time attention highlights the specific timesteps where irregularities
occur, thus enabling localization. By combining the results, the framework can
identify both the affected sensors and the duration of each anomaly within a
single unified network.

</details>


### [114] [Diffusion-Based Generation and Imputation of Driving Scenarios from Limited Vehicle CAN Data](https://arxiv.org/abs/2509.12375)
*Julian Ripper,Ousama Esbel,Rafael Fietzek,Max Mühlhäuser,Thomas Kreutz*

Main category: cs.LG

TL;DR: 本文聚焦汽车时间序列数据生成，提出混合生成方法，改进DDPM架构，提出评估指标，模型在物理正确性上表现出色并能提升数据质量。


<details>
  <summary>Details</summary>
Motivation: 在含损坏样本的小时间序列数据集上训练深度学习方法具有挑战性，需生成汽车时间序列的合成数据。

Method: 提出结合自回归和非自回归技术的混合生成方法，改进两种DDPM架构，提出三个评估指标。

Result: 最佳模型在物理正确性上优于训练数据，展现合理驾驶行为，能成功修复训练数据中不合理区域。

Conclusion: 所提方法可有效生成汽车时间序列的合成数据，提升数据质量。

Abstract: Training deep learning methods on small time series datasets that also
include corrupted samples is challenging. Diffusion models have shown to be
effective to generate realistic and synthetic data, and correct corrupted
samples through imputation. In this context, this paper focuses on generating
synthetic yet realistic samples of automotive time series data. We show that
denoising diffusion probabilistic models (DDPMs) can effectively solve this
task by applying them to a challenging vehicle CAN-dataset with long-term data
and a limited number of samples. Therefore, we propose a hybrid generative
approach that combines autoregressive and non-autoregressive techniques. We
evaluate our approach with two recently proposed DDPM architectures for time
series generation, for which we propose several improvements. To evaluate the
generated samples, we propose three metrics that quantify physical correctness
and test track adherence. Our best model is able to outperform even the
training data in terms of physical correctness, while showing plausible driving
behavior. Finally, we use our best model to successfully impute physically
implausible regions in the training data, thereby improving the data quality.

</details>


### [115] [Evaluating the printability of stl files with ML](https://arxiv.org/abs/2509.12392)
*Janik Henn,Adrian Hauptmannl,Hamza A. A. Gardi*

Main category: cs.LG

TL;DR: 3D打印从面向专业人士到吸引更广泛受众，本文提出用AI模型检测3D模型常见问题以助新手。


<details>
  <summary>Details</summary>
Motivation: 3D打印市场发展，需帮助经验不足的用户避免打印失败。

Method: 训练AI模型来检测3D模型中的常见问题。

Result: 未提及具体结果。

Conclusion: 未提及具体结论。

Abstract: 3D printing has long been a technology for industry professionals and
enthusiasts willing to tinker or even build their own machines. This stands in
stark contrast to today's market, where recent developments have prioritized
ease of use to attract a broader audience. Slicing software nowadays has a few
ways to sanity check the input file as well as the output gcode. Our approach
introduces a novel layer of support by training an AI model to detect common
issues in 3D models. The goal is to assist less experienced users by
identifying features that are likely to cause print failures due to difficult
to print geometries before printing even begins.

</details>


### [116] [Adaptive Spatial Goodness Encoding: Advancing and Scaling Forward-Forward Learning Without Backpropagation](https://arxiv.org/abs/2509.12394)
*Qingchun Gong,Robert Bogdan Staszewski,Kai Xu*

Main category: cs.LG

TL;DR: 提出适用于CNN的基于FF的训练框架ASGE，解决通道爆炸问题，在多基准测试中表现出色，首次成功将FF训练用于ImageNet，缩小与BP训练模型差距。


<details>
  <summary>Details</summary>
Motivation: 现有基于FF的CNN扩展存在表示能力有限和可扩展性差问题，主要因通道维度爆炸。

Method: 提出自适应空间良好度编码（ASGE）框架，利用特征图计算各层空间感知良好度表示，实现逐层监督，解耦分类复杂度和通道维度。

Result: 在MNIST、FashionMNIST、CIFAR - 10、CIFAR - 100上测试准确率分别达99.65%、93.41%、90.62%、65.42%；在ImageNet上Top - 1和Top - 5准确率分别为26.21%和47.49%。

Conclusion: ASGE框架消除BP，显著缩小与BP训练模型性能差距，为可扩展的无BP的CNN训练奠定基础。

Abstract: The Forward-Forward (FF) algorithm offers a promising alternative to
backpropagation (BP). Despite advancements in recent FF-based extensions, which
have enhanced the original algorithm and adapted it to convolutional neural
networks (CNNs), they often suffer from limited representational capacity and
poor scalability to large-scale datasets, primarily due to exploding channel
dimensionality. In this work, we propose adaptive spatial goodness encoding
(ASGE), a new FF-based training framework tailored for CNNs. ASGE leverages
feature maps to compute spatially-aware goodness representations at each layer,
enabling layer-wise supervision. Crucially, this approach decouples
classification complexity from channel dimensionality, thereby addressing the
issue of channel explosion and achieving competitive performance compared to
other BP-free methods. ASGE outperforms all other FF-based approaches across
multiple benchmarks, delivering test accuracies of 99.65% on MNIST, 93.41% on
FashionMNIST, 90.62% on CIFAR-10, and 65.42% on CIFAR-100. Moreover, we present
the first successful application of FF-based training to ImageNet, with Top-1
and Top-5 accuracies of 26.21% and 47.49%. By entirely eliminating BP and
significantly narrowing the performance gap with BP-trained models, the ASGE
framework establishes a viable foundation toward scalable BP-free CNN training.

</details>


### [117] [Surrogate Representation Inference for Noisy Text and Image Annotations](https://arxiv.org/abs/2509.12416)
*Kentaro Nakamura*

Main category: cs.LG

TL;DR: 本文介绍了替代表示推理（SRI）方法，能减少标准误差，在人类注释有误差时也能有效推理。


<details>
  <summary>Details</summary>
Motivation: 现有纠正下游统计分析偏差的方法存在标准误差大且需无误差人工注释的问题。

Method: 引入SRI，假设非结构化数据完全介导人类注释和结构化变量的关系，提出神经网络架构学习低维表示，有多人注释时可纠正测量误差，还建立识别条件和半参数有效估计策略。

Result: 模拟研究和实际应用表明，机器学习预测精度中等时，SRI可减少超50%的标准误差，人类注释有非差异性测量误差时也能有效推理。

Conclusion: SRI在处理非结构化数据注释偏差问题上表现良好，能降低标准误差并在有误差情况下有效推理。

Abstract: As researchers increasingly rely on machine learning models and LLMs to
annotate unstructured data, such as texts or images, various approaches have
been proposed to correct bias in downstream statistical analysis. However,
existing methods tend to yield large standard errors and require some
error-free human annotation. In this paper, I introduce Surrogate
Representation Inference (SRI), which assumes that unstructured data fully
mediate the relationship between human annotations and structured variables.
The assumption is guaranteed by design provided that human coders rely only on
unstructured data for annotation. Under this setting, I propose a neural
network architecture that learns a low-dimensional representation of
unstructured data such that the surrogate assumption remains to be satisfied.
When multiple human annotations are available, SRI can further correct
non-differential measurement errors that may exist in human annotations.
Focusing on text-as-outcome settings, I formally establish the identification
conditions and semiparametric efficient estimation strategies that enable
learning and leveraging such a low-dimensional representation. Simulation
studies and a real-world application demonstrate that SRI reduces standard
errors by over 50% when machine learning prediction accuracy is moderate and
provides valid inference even when human annotations contain non-differential
measurement errors.

</details>


### [118] [On the Regularity and Fairness of Combinatorial Multi-Armed Bandit](https://arxiv.org/abs/2509.12457)
*Xiaoyi Wu,Bin Li*

Main category: cs.LG

TL;DR: 本文针对无线通信网络的应用，提出参数化公平且有规律的学习算法，实现累积奖励最大化、公平性和奖励规律性三个目标，并进行理论分析和模拟验证。


<details>
  <summary>Details</summary>
Motivation: 在无线通信网络应用中，除了最大化累积奖励，还需保证各臂之间的公平性和奖励规律性。

Method: 提出参数化学习算法，线性组合虚拟队列长度、TSLR指标和UCB估计，探索虚拟队列长度和TSLR指标的关系，利用Lyapunov函数进行分析。

Result: 通过理论分析得出算法在累积公平性、奖励规律性和累积遗憾方面的性能，并通过两个真实数据集的模拟进行验证。

Conclusion: 所提出的算法能有效实现累积奖励最大化、公平性和奖励规律性的目标。

Abstract: The combinatorial multi-armed bandit model is designed to maximize cumulative
rewards in the presence of uncertainty by activating a subset of arms in each
round. This paper is inspired by two critical applications in wireless
networks, where it's not only essential to maximize cumulative rewards but also
to guarantee fairness among arms (i.e., the minimum average reward required by
each arm) and ensure reward regularity (i.e., how often each arm receives the
reward). In this paper, we propose a parameterized regular and fair learning
algorithm to achieve these three objectives. In particular, the proposed
algorithm linearly combines virtual queue-lengths (tracking the fairness
violations), Time-Since-Last-Reward (TSLR) metrics, and Upper Confidence Bound
(UCB) estimates in its weight measure. Here, TSLR is similar to
age-of-information and measures the elapsed number of rounds since the last
time an arm received a reward, capturing the reward regularity performance, and
UCB estimates are utilized to balance the tradeoff between exploration and
exploitation in online learning. By exploring a key relationship between
virtual queue-lengths and TSLR metrics and utilizing several non-trivial
Lyapunov functions, we analytically characterize zero cumulative fairness
violation, reward regularity, and cumulative regret performance under our
proposed algorithm. These theoretical outcomes are verified by simulations
based on two real-world datasets.

</details>


### [119] [Nonlocal Neural Tangent Kernels via Parameter-Space Interactions](https://arxiv.org/abs/2509.12467)
*Sriram Nagaraj,Vishakh Hari*

Main category: cs.LG

TL;DR: 提出非局部神经切线核（NNTK），将NTK理论扩展到非光滑函数等，给出不同形式并进行数值研究。


<details>
  <summary>Details</summary>
Motivation: 现有神经切线核（NTK）框架依赖网络参数可微假设，在非光滑目标函数或非可微模型中不适用，需扩展其适用范围。

Method: 用基于非局部交互的近似替代参数空间中的局部梯度，探索非局部算子的固定核和基于注意力的公式。

Result: 实现了将NTK理论扩展到非光滑函数、随机估计器和更广泛的模型族。

Conclusion: 所提出的NNTK有效扩展了NTK理论的适用范围。

Abstract: The Neural Tangent Kernel (NTK) framework has provided deep insights into the
training dynamics of neural networks under gradient flow. However, it relies on
the assumption that the network is differentiable with respect to its
parameters, an assumption that breaks down when considering non-smooth target
functions or parameterized models exhibiting non-differentiable behavior. In
this work, we propose a Nonlocal Neural Tangent Kernel (NNTK) that replaces the
local gradient with a nonlocal interaction-based approximation in parameter
space. Nonlocal gradients are known to exist for a wider class of functions
than the standard gradient. This allows NTK theory to be extended to nonsmooth
functions, stochastic estimators, and broader families of models. We explore
both fixed-kernel and attention-based formulations of this nonlocal operator.
We illustrate the new formulation with numerical studies.

</details>


### [120] [Comparative Analysis of Wave Scattering Numerical Modeling Using the Boundary Element Method and Physics-Informed Neural Networks](https://arxiv.org/abs/2509.12483)
*Oscar Rincón-Cardeno,Gregorio Pérez Bernal,Silvana Montoya Noguera,Nicolás Guarín-Zapata*

Main category: cs.LG

TL;DR: 本文比较了边界元法（BEM）和物理信息神经网络（PINNs）求解二维亥姆霍兹方程的性能，给出两者在精度、计算时间和泛化能力等方面的表现。


<details>
  <summary>Details</summary>
Motivation: 评估边界元法（BEM）和物理信息神经网络（PINNs）在相同条件下求解二维亥姆霍兹方程的性能。

Method: 用BEM和PINNs求解同一散射问题，PINNs通过最小化控制方程和边界条件的残差进行训练，BEM采用边界离散化，从解的精度、计算时间和泛化能力评估两种方法。

Result: 数值实验表明，在可比精度下，PINNs训练时间约为BEM的42倍，但训练后评估时间最快可达BEM的204倍；在训练域外，PINNs相对误差增大，BEM误差水平相近。

Conclusion: 该研究直接比较了PINNs和BEM，为波动传播问题研究提供定量数据，利于方法选择，明确未来挑战和方向。

Abstract: Purpose - This study compares the Boundary Element Method (BEM) and
Physics-Informed Neural Networks (PINNs) for solving the two-dimensional
Helmholtz equation in wave scattering problems. The objective is to evaluate
the performance of both methods under the same conditions.
  Design/methodology/approach - We solve the Helmholtz equation using BEM and
PINNs for the same scattering problem. The PINNs are trained by minimizing the
residual of the governing equations and boundary conditions, with their
configuration determined through hyperparameter optimization, while the BEM is
applied using boundary discretization. Both methods are evaluated in terms of
solution accuracy, computation time, and generalization capacity.
  Findings - Numerical experiments were conducted by varying the number of
integration points for BEM and the number of layers and neurons per layer for
PINNs. Hyperparameter tuning provided further insight into suitable
configurations for wave scattering problems. At comparable accuracy, PINNs
produced consistent solutions but required training times approximately 42
times longer than BEM. However, once trained, PINNs achieved evaluation times
up to 204 times faster. The generalization capacity was also assessed outside
the PINN training domain, where the relative error increased from $7.46 \times
10^{-2}$ to 8.22, while BEM maintained a similar error level in the extended
region.
  Originality/value - This work presents a direct comparison between PINNs and
BEM for the Helmholtz equation. The analysis provides quantitative data on the
performance of both methods, supporting their selection in future research on
wave propagation problems and establishing future challenges and directions.

</details>


### [121] [Prediction and Causality of functional MRI and synthetic signal using a Zero-Shot Time-Series Foundation Model](https://arxiv.org/abs/2509.12497)
*Alessandro Crimi,Andrea Brovelli*

Main category: cs.LG

TL;DR: 评估基础模型在脑信号预测和因果分析方面与传统方法的表现，发现基础模型有零样本预测能力和更精确的因果交互检测。


<details>
  <summary>Details</summary>
Motivation: 探究基础模型与传统方法在脑信号预测和因果分析上的比较，以及能否零样本应用。

Method: 用功能磁共振成像测量人类自发脑活动，在零样本和微调设置下测试基础模型预测能力，对比模型的类格兰杰估计与标准格兰杰因果关系，用合成时间序列验证。

Result: 基础模型零样本预测fMRI时间序列有竞争力，虽标准格兰杰因果未显示模型间明显定量差异，但基础模型因果交互检测更精确。

Conclusion: 基础模型在时间序列数据的预测和因果发现中有多功能性、强零样本性能和潜在实用性。

Abstract: Time-series forecasting and causal discovery are central in neuroscience, as
predicting brain activity and identifying causal relationships between neural
populations and circuits can shed light on the mechanisms underlying cognition
and disease. With the rise of foundation models, an open question is how they
compare to traditional methods for brain signal forecasting and causality
analysis, and whether they can be applied in a zero-shot setting. In this work,
we evaluate a foundation model against classical methods for inferring
directional interactions from spontaneous brain activity measured with
functional magnetic resonance imaging (fMRI) in humans. Traditional approaches
often rely on Wiener-Granger causality. We tested the forecasting ability of
the foundation model in both zero-shot and fine-tuned settings, and assessed
causality by comparing Granger-like estimates from the model with standard
Granger causality. We validated the approach using synthetic time series
generated from ground-truth causal models, including logistic map coupling and
Ornstein-Uhlenbeck processes. The foundation model achieved competitive
zero-shot forecasting fMRI time series (mean absolute percentage error of 0.55
in controls and 0.27 in patients). Although standard Granger causality did not
show clear quantitative differences between models, the foundation model
provided a more precise detection of causal interactions.
  Overall, these findings suggest that foundation models offer versatility,
strong zero-shot performance, and potential utility for forecasting and causal
discovery in time-series data.

</details>


### [122] [Phi: Preference Hijacking in Multi-modal Large Language Models at Inference Time](https://arxiv.org/abs/2509.12521)
*Yifan Lan,Yuanpu Cao,Weitong Zhang,Lu Lin,Jinghui Chen*

Main category: cs.LG

TL;DR: 本文揭示多模态大语言模型（MLLMs）新安全风险，提出偏好劫持方法Phi，实验证明其有效性，代码开源。


<details>
  <summary>Details</summary>
Motivation: MLLMs广泛应用引发安全担忧，本文旨在揭示其新安全风险。

Method: 引入偏好劫持方法Phi，利用偏好劫持图像操纵MLLM响应偏好，且无需修改模型；还引入通用劫持扰动。

Result: 跨多种任务的实验证明了方法的有效性。

Conclusion: 通过提出的方法成功揭示MLLMs可被精心优化的图像任意操纵输出偏好这一安全风险，且方法有效。

Abstract: Recently, Multimodal Large Language Models (MLLMs) have gained significant
attention across various domains. However, their widespread adoption has also
raised serious safety concerns. In this paper, we uncover a new safety risk of
MLLMs: the output preference of MLLMs can be arbitrarily manipulated by
carefully optimized images. Such attacks often generate contextually relevant
yet biased responses that are neither overtly harmful nor unethical, making
them difficult to detect. Specifically, we introduce a novel method, Preference
Hijacking (Phi), for manipulating the MLLM response preferences using a
preference hijacked image. Our method works at inference time and requires no
model modifications. Additionally, we introduce a universal hijacking
perturbation -- a transferable component that can be embedded into different
images to hijack MLLM responses toward any attacker-specified preferences.
Experimental results across various tasks demonstrate the effectiveness of our
approach. The code for Phi is accessible at https://github.com/Yifan-Lan/Phi.

</details>


### [123] [Graph Homophily Booster: Rethinking the Role of Discrete Features on Heterophilic Graphs](https://arxiv.org/abs/2509.12530)
*Ruizhong Qiu,Ting-Wei Li,Gaotang Li,Hanghang Tong*

Main category: cs.LG

TL;DR: 现有GNN处理异质图效果不佳，本文提出GRAPHITE框架，通过图变换直接提升图同质性，实验表明其在异质图上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有处理图异质性的方法多关注架构设计，未解决根本问题，在挑战性异质数据集上表现不如简单的MLP，需要创新方法。

Method: 提出GRAPHITE框架，通过创建特征节点促进相似特征节点间的同质性消息传递，直接提升图同质性。

Result: 理论和实验证明GRAPHITE显著提升了异质图的同质性，图规模仅稍有增加；在挑战性数据集上，GRAPHITE在异质图上显著优于现有方法，在同质图上与现有方法精度相当。

Conclusion: GRAPHITE是首个通过显式图变换直接提升图同质性的方法，能有效解决图异质性问题，在异质图和同质图上都有良好表现。

Abstract: Graph neural networks (GNNs) have emerged as a powerful tool for modeling
graph-structured data. However, existing GNNs often struggle with heterophilic
graphs, where connected nodes tend to have dissimilar features or labels. While
numerous methods have been proposed to address this challenge, they primarily
focus on architectural designs without directly targeting the root cause of the
heterophily problem. These approaches still perform even worse than the
simplest MLPs on challenging heterophilic datasets. For instance, our
experiments show that 21 latest GNNs still fall behind the MLP on the Actor
dataset. This critical challenge calls for an innovative approach to addressing
graph heterophily beyond architectural designs. To bridge this gap, we propose
and study a new and unexplored paradigm: directly increasing the graph
homophily via a carefully designed graph transformation. In this work, we
present a simple yet effective framework called GRAPHITE to address graph
heterophily. To the best of our knowledge, this work is the first method that
explicitly transforms the graph to directly improve the graph homophily.
Stemmed from the exact definition of homophily, our proposed GRAPHITE creates
feature nodes to facilitate homophilic message passing between nodes that share
similar features. Furthermore, we both theoretically and empirically show that
our proposed GRAPHITE significantly increases the homophily of originally
heterophilic graphs, with only a slight increase in the graph size. Extensive
experiments on challenging datasets demonstrate that our proposed GRAPHITE
significantly outperforms state-of-the-art methods on heterophilic graphs while
achieving comparable accuracy with state-of-the-art methods on homophilic
graphs.

</details>


### [124] [Cross-Modal Deep Metric Learning for Time Series Anomaly Detection](https://arxiv.org/abs/2509.12540)
*Wei Li,Zheze Yang*

Main category: cs.LG

TL;DR: 提出基于跨模态深度度量学习的时间序列异常检测方法，实验表明该方法分类准确、灵敏度高、检测效果好。


<details>
  <summary>Details</summary>
Motivation: 有效解决时间序列异常检测中灵敏度低和耗时高的问题。

Method: 构建跨模态深度度量学习特征聚类模型，计算簇中心欧氏距离，用随机梯度下降优化模型；以主成分方向向量内积度量异常，用vMF分布描述数据方向特征，用历史数据训练获取评估参数，对比主成分方向向量与阈值进行检测。

Result: 能准确分类不同属性的时间序列数据，对异常有高灵敏度，检测准确率高、速度快、鲁棒性强。

Conclusion: 所提异常检测方法有效，能满足时间序列异常检测需求。

Abstract: To effectively address the issues of low sensitivity and high time
consumption in time series anomaly detection, we propose an anomaly detection
method based on cross-modal deep metric learning. A cross-modal deep metric
learning feature clustering model is constructed, composed of an input layer, a
triplet selection layer, and a loss function computation layer. The squared
Euclidean distances between cluster centers are calculated, and a stochastic
gradient descent strategy is employed to optimize the model and classify
different time series features. The inner product of principal component
direction vectors is used as a metric for anomaly measurement. The von
Mises-Fisher (vMF) distribution is applied to describe the directional
characteristics of time series data, and historical data is used to train and
obtain evaluation parameters. By comparing the principal component direction
vector of actual time series data with the threshold, anomaly detection is
performed. Experimental results demonstrate that the proposed method accurately
classifies time series data with different attributes, exhibits high
sensitivity to anomalies, and achieves high detection accuracy, fast detection
speed, and strong robustness.

</details>


### [125] [iCD: A Implicit Clustering Distillation Mathod for Structural Information Mining](https://arxiv.org/abs/2509.12553)
*Xiang Xue,Yatu Ji,Qing-dao-er-ji Ren,Bao Shi,Min Lu,Nier Wu,Xufei Zhuang,Haiteng Xu,Gan-qi-qi-ge Cha*

Main category: cs.LG

TL;DR: 提出隐式聚类蒸馏（iCD）方法，从logits挖掘并转移可解释的结构知识，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: Logit知识蒸馏决策过程可解释性有限，需改进。

Method: 提出iCD方法，利用解耦局部logit表示的Gram矩阵让学生模型学习潜在语义结构模式。

Result: 在基准数据集上实验表明，iCD在不同师生架构中有效，在细粒度分类任务中表现出色，比基线最高提升5.08%。

Conclusion: iCD是一种简单有效的方法，能挖掘和转移可解释的结构知识。

Abstract: Logit Knowledge Distillation has gained substantial research interest in
recent years due to its simplicity and lack of requirement for intermediate
feature alignment; however, it suffers from limited interpretability in its
decision-making process. To address this, we propose implicit Clustering
Distillation (iCD): a simple and effective method that mines and transfers
interpretable structural knowledge from logits, without requiring ground-truth
labels or feature-space alignment. iCD leverages Gram matrices over decoupled
local logit representations to enable student models to learn latent semantic
structural patterns. Extensive experiments on benchmark datasets demonstrate
the effectiveness of iCD across diverse teacher-student architectures, with
particularly strong performance in fine-grained classification tasks --
achieving a peak improvement of +5.08% over the baseline. The code is available
at: https://github.com/maomaochongaa/iCD.

</details>


### [126] [No Need for "Learning" to Defer? A Training Free Deferral Framework to Multiple Experts through Conformal Prediction](https://arxiv.org/abs/2509.12573)
*Tim Bary,Benoît Macq,Louis Petit*

Main category: cs.LG

TL;DR: 提出基于共形预测的无训练、模型和专家无关的专家延迟框架，实验显示其优于现有方法且减少专家工作量，是可扩展、无需重新训练的方案。


<details>
  <summary>Details</summary>
Motivation: 现有Learning to Defer (L2D) 方法对专家组成变化敏感，专家改变时需大量重新训练，需更好的混合人机决策方法。

Method: 基于共形预测，利用共形预测器生成的预测集识别特定标签的不确定性，使用分离性标准选择最具判别力的专家。

Result: 在CIFAR10 - H和ImageNet16 - H实验中，方法始终优于独立模型和最强专家，准确率达99.57±0.10%和99.40±0.52%，最多可将专家工作量减少11倍，在专家性能下降时保持稳健，低信息设置中性能逐渐下降。

Conclusion: 该方法是现实世界人机协作中L2D的可扩展、无需重新训练的替代方案。

Abstract: AI systems often fail to deliver reliable predictions across all inputs,
prompting the need for hybrid human-AI decision-making. Existing Learning to
Defer (L2D) approaches address this by training deferral models, but these are
sensitive to changes in expert composition and require significant retraining
if experts change. We propose a training-free, model- and expert-agnostic
framework for expert deferral based on conformal prediction. Our method uses
the prediction set generated by a conformal predictor to identify
label-specific uncertainty and selects the most discriminative expert using a
segregativity criterion, measuring how well an expert distinguishes between the
remaining plausible labels. Experiments on CIFAR10-H and ImageNet16-H show that
our method consistently outperforms both the standalone model and the strongest
expert, with accuracies attaining $99.57\pm0.10\%$ and $99.40\pm0.52\%$, while
reducing expert workload by up to a factor of $11$. The method remains robust
under degraded expert performance and shows a gradual performance drop in
low-information settings. These results suggest a scalable, retraining-free
alternative to L2D for real-world human-AI collaboration.

</details>


### [127] [Exploring Training Data Attribution under Limited Access Constraints](https://arxiv.org/abs/2509.12581)
*Shiyuan Zhang,Junwei Deng,Juhan Bae,Jiaqi Ma*

Main category: cs.LG

TL;DR: 本文系统研究不同访问和资源约束下的训练数据归因（TDA）方法，提出利用代理模型等方案，发现未在目标数据集上预训练的模型归因得分仍有信息价值，为实际部署TDA提供指导。


<details>
  <summary>Details</summary>
Motivation: 现有TDA方法依赖全模型访问且计算成本高，在商业模型不可公开访问和计算资源有限的现实场景中，限制了TDA在实际应用中的广泛采用。

Method: 系统研究不同访问和资源约束下的TDA方法，利用代理模型等设计合适的解决方案，探究不同访问约束下执行TDA的可行性。

Result: 发现未在目标数据集上预训练的模型所获得的归因得分在一系列任务中仍具有信息价值。

Conclusion: 研究结果为在现实环境中部署TDA提供了实际指导，有助于在有限访问条件下提高可行性和效率。

Abstract: Training data attribution (TDA) plays a critical role in understanding the
influence of individual training data points on model predictions.
Gradient-based TDA methods, popularized by \textit{influence function} for
their superior performance, have been widely applied in data selection, data
cleaning, data economics, and fact tracing. However, in real-world scenarios
where commercial models are not publicly accessible and computational resources
are limited, existing TDA methods are often constrained by their reliance on
full model access and high computational costs. This poses significant
challenges to the broader adoption of TDA in practical applications.
  In this work, we present a systematic study of TDA methods under various
access and resource constraints. We investigate the feasibility of performing
TDA under varying levels of access constraints by leveraging appropriately
designed solutions such as proxy models. Besides, we demonstrate that
attribution scores obtained from models without prior training on the target
dataset remain informative across a range of tasks, which is useful for
scenarios where computational resources are limited. Our findings provide
practical guidance for deploying TDA in real-world environments, aiming to
improve feasibility and efficiency under limited access.

</details>


### [128] [A Multimodal Foundation Model to Enhance Generalizability and Data Efficiency for Pan-cancer Prognosis Prediction](https://arxiv.org/abs/2509.12600)
*Huajun Zhou,Fengtao Zhou,Jiabo Ma,Yingxue Xu,Xi Wang,Xiuming Zhang,Li Liang,Zhenhui Li,Hao Chen*

Main category: cs.LG

TL;DR: 提出多模态基础模型MICE用于泛癌预后预测，表现优于现有模型，有良好泛化性和数据效率。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型难以有效利用多模态数据中的丰富信息，提取的表征泛化性差。

Method: MICE采用多个功能不同的专家模块，结合对比学习和监督学习，利用11799名30种癌症类型患者的数据。

Result: MICE在内部和独立队列中C指数均有显著提升，在不同临床场景下展现出卓越的数据效率。

Conclusion: MICE为泛癌预后预测建立了有效且可扩展的基础，有望实现个性化治疗并改善治疗效果。

Abstract: Multimodal data provides heterogeneous information for a holistic
understanding of the tumor microenvironment. However, existing AI models often
struggle to harness the rich information within multimodal data and extract
poorly generalizable representations. Here we present MICE (Multimodal data
Integration via Collaborative Experts), a multimodal foundation model that
effectively integrates pathology images, clinical reports, and genomics data
for precise pan-cancer prognosis prediction. Instead of conventional
multi-expert modules, MICE employs multiple functionally diverse experts to
comprehensively capture both cross-cancer and cancer-specific insights.
Leveraging data from 11,799 patients across 30 cancer types, we enhanced MICE's
generalizability by coupling contrastive and supervised learning. MICE
outperformed both unimodal and state-of-the-art multi-expert-based multimodal
models, demonstrating substantial improvements in C-index ranging from 3.8% to
11.2% on internal cohorts and 5.8% to 8.8% on independent cohorts,
respectively. Moreover, it exhibited remarkable data efficiency across diverse
clinical scenarios. With its enhanced generalizability and data efficiency,
MICE establishes an effective and scalable foundation for pan-cancer prognosis
prediction, holding strong potential to personalize tailored therapies and
improve treatment outcomes.

</details>


### [129] [High-Energy Concentration for Federated Learning in Frequency Domain](https://arxiv.org/abs/2509.12630)
*Haozhi Shi,Weiying Xie,Hangyu Ye,Daixun Li,Jitao Ma,Leyuan Fang*

Main category: cs.LG

TL;DR: 本文提出频域感知的联邦学习方法FedFD，利用离散余弦变换的高能集中特性，过滤高频低能成分，在多个数据集上降低通信成本并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于数据集蒸馏的联邦学习框架存在全空间域设计的冗余信息和噪声问题，增加了通信负担。

Method: 提出FedFD方法，通过二进制掩码保留低频成分，进行频域分布对齐，在损失函数中加入真实数据驱动的合成分类以提升低频成分质量。

Result: 在五个图像和语音数据集上，FedFD比现有方法性能更优，降低了通信成本，如在CIFAR - 10数据集上通信成本最少降低37.78%，性能提升10.88%。

Conclusion: FedFD方法能有效解决现有联邦学习框架的通信负担问题，提升性能。

Abstract: Federated Learning (FL) presents significant potential for collaborative
optimization without data sharing. Since synthetic data is sent to the server,
leveraging the popular concept of dataset distillation, this FL framework
protects real data privacy while alleviating data heterogeneity. However, such
methods are still challenged by the redundant information and noise in entire
spatial-domain designs, which inevitably increases the communication burden. In
this paper, we propose a novel Frequency-Domain aware FL method with
high-energy concentration (FedFD) to address this problem. Our FedFD is
inspired by the discovery that the discrete cosine transform predominantly
distributes energy to specific regions, referred to as high-energy
concentration. The principle behind FedFD is that low-energy like
high-frequency components usually contain redundant information and noise, thus
filtering them helps reduce communication costs and optimize performance. Our
FedFD is mathematically formulated to preserve the low-frequency components
using a binary mask, facilitating an optimal solution through frequency-domain
distribution alignment. In particular, real data-driven synthetic
classification is imposed into the loss to enhance the quality of the
low-frequency components. On five image and speech datasets, FedFD achieves
superior performance than state-of-the-art methods while reducing communication
costs. For example, on the CIFAR-10 dataset with Dirichlet coefficient $\alpha
= 0.01$, FedFD achieves a minimum reduction of 37.78\% in the communication
cost, while attaining a 10.88\% performance gain.

</details>


### [130] [Leveraging Intermediate Representations of Time Series Foundation Models for Anomaly Detection](https://arxiv.org/abs/2509.12650)
*Chan Sik Han,Keon Myung Lee*

Main category: cs.LG

TL;DR: 提出名为TimeRep的时间序列异常检测方法，利用时间序列基础模型中间层表示计算异常分数，实验显示其优于多种基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列异常检测方法通常依赖时间序列基础模型的最后一层表示，本文旨在提出新方法以更好地进行异常检测。

Method: TimeRep方法选择能产生最具信息表示的中间层和补丁标记位置，从训练数据中形成中间表示的参考集合，采用核心集策略减小集合大小，推理时通过测量距离计算异常分数，还集成适应机制处理概念漂移。

Result: 在UCR Anomaly Archive的250个单变量时间序列上进行实验，TimeRep始终优于多种最先进的基线方法。

Conclusion: TimeRep是一种有效的时间序列异常检测方法，利用中间层表示和适应机制提升了检测性能。

Abstract: Detecting anomalies in time series data is essential for the reliable
operation of many real-world systems. Recently, time series foundation models
(TSFMs) have emerged as a powerful tool for anomaly detection. However,
existing methods typically rely on the final layer's representations of TSFMs,
computing the anomaly score as a reconstruction or forecasting error via a
task-specific head. Instead, we propose TimeRep, a novel anomaly detection
approach that leverages the intermediate layer's representations of TSFMs,
computing the anomaly score as the distance between these representations.
Given a pre-trained TSFM, TimeRep selects the intermediate layer and
patch-token position that yield the most informative representation. TimeRep
forms a reference collection of intermediate representations from the training
data and applies a core-set strategy to reduce its size while maintaining
distributional coverage. During inference, TimeRep computes the anomaly score
for incoming data by measuring the distance between its intermediate
representations and those of the collection. To address concept drift, TimeRep
integrates an adaptation mechanism that, at inference time, augments the
collection exclusively with non-redundant intermediate representations from
incoming data. We conducted extensive experiments on the UCR Anomaly Archive,
which contains 250 univariate time series. TimeRep consistently outperforms a
broad spectrum of state-of-the-art baselines, including non-DL, DL, and
foundation model-based methods.

</details>


### [131] [Instance-level Randomization: Toward More Stable LLM Evaluations](https://arxiv.org/abs/2509.12678)
*Yiyang Li,Yonghuang Wu,Ying Luo,Liangtai Sun,Zishu Qin,Lin Qiu,Xuezhi Cao,Xunliang Cai*

Main category: cs.LG

TL;DR: 当前大语言模型评估有不稳定性和不公平性，提出实例级随机化（ILR）方法，可减少方差、增强公平性且降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型评估采用固定随机因素设置，存在不稳定性和潜在不公平比较问题。

Method: 理论分析随机因素导致方差的来源，提出ILR方法，为每个实例随机化影响评估分数的因素，多次实验并报告平均分。

Result: ILR可减少随机因素导致的方差和不公平比较，且以不到之前方法一半的计算成本达到类似鲁棒性水平。

Conclusion: ILR方法能有效缓解评估的波动性，增强模型比较的公平性。

Abstract: Evaluations of large language models (LLMs) suffer from instability, where
small changes of random factors such as few-shot examples can lead to drastic
fluctuations of scores and even model rankings. Moreover, different LLMs can
have different preferences for a certain setting of random factors. As a
result, using a fixed setting of random factors, which is often adopted as the
paradigm of current evaluations, can lead to potential unfair comparisons
between LLMs. To mitigate the volatility of evaluations, we first theoretically
analyze the sources of variance induced by changes in random factors. Targeting
these specific sources, we then propose the instance-level randomization (ILR)
method to reduce variance and enhance fairness in model comparisons. Instead of
using a fixed setting across the whole benchmark in a single experiment, we
randomize all factors that affect evaluation scores for every single instance,
run multiple experiments and report the averaged score. Theoretical analyses
and empirical results demonstrate that ILR can reduce the variance and unfair
comparisons caused by random factors, as well as achieve similar robustness
level with less than half computational cost compared with previous methods.

</details>


### [132] [ZTree: A Subgroup Identification Based Decision Tree Learning Framework](https://arxiv.org/abs/2509.12688)
*Eric Cheng,Jie Cheng*

Main category: cs.LG

TL;DR: 提出ZTree决策树学习框架，用统计原则的子组识别替代CART传统分裂方法，在UCI数据集表现好，能生成更简单树。


<details>
  <summary>Details</summary>
Motivation: 改进传统决策树CART基于纯度的分裂方法，提供更有效灵活的决策树学习框架。

Method: 用假设检验评估候选子组差异，用交叉验证方法处理多重测试以确定节点是否分裂，以z - 阈值控制树复杂度。

Result: 在五个大规模UCI数据集上表现出色，尤其在低数据情况下，相比CART能生成更简单树且不牺牲性能。

Conclusion: ZTree通过假设检验和交叉验证方法，为传统决策树分裂提供了基于统计的替代方案，是高效灵活的框架。

Abstract: Decision trees are a commonly used class of machine learning models valued
for their interpretability and versatility, capable of both classification and
regression. We propose ZTree, a novel decision tree learning framework that
replaces CART's traditional purity based splitting with statistically
principled subgroup identification. At each node, ZTree applies hypothesis
testing (e.g., z-tests, t-tests, Mann-Whitney U, log-rank) to assess whether a
candidate subgroup differs meaningfully from the complement. To adjust for the
complication of multiple testing, we employ a cross-validation-based approach
to determine if further node splitting is needed. This robust stopping
criterion eliminates the need for post-pruning and makes the test threshold
(z-threshold) the only parameter for controlling tree complexity. Because of
the simplicity of the tree growing procedure, once a detailed tree is learned
using the most lenient z-threshold, all simpler trees can be derived by simply
removing nodes that do not meet the larger z-thresholds. This makes parameter
tuning intuitive and efficient. Furthermore, this z-threshold is essentially a
p-value, allowing users to easily plug in appropriate statistical tests into
our framework without adjusting the range of parameter search. Empirical
evaluation on five large-scale UCI datasets demonstrates that ZTree
consistently delivers strong performance, especially at low data regimes.
Compared to CART, ZTree also tends to grow simpler trees without sacrificing
performance. ZTree introduces a statistically grounded alternative to
traditional decision tree splitting by leveraging hypothesis testing and a
cross-validation approach to multiple testing correction, resulting in an
efficient and flexible framework.

</details>


### [133] [Soft Graph Transformer for MIMO Detection](https://arxiv.org/abs/2509.12694)
*Jiadong Hong,Lei Liu,Xinyu Bian,Wenjie Wang,Zhaoyang Zhang*

Main category: cs.LG

TL;DR: 提出适用于MIMO检测的Soft Graph Transformer (SGT)，能有效生成软输出并保持计算效率，性能接近ML且优于之前基于Transformer的方法。


<details>
  <summary>Details</summary>
Motivation: 现有MIMO检测方法存在问题，如ML复杂度高，传统消息传递算法依赖不实际假设，先前基于Transformer的检测器有性能局限。

Method: 将消息传递集成到图感知注意力机制中，通过软输入嵌入支持解码器信息更新。

Result: 作为独立检测器，SGT接近ML性能，超越先前基于Transformer的方法。

Conclusion: SGT能克服现有MIMO检测方法的局限，有效生成软输出并保持计算效率。

Abstract: We propose the Soft Graph Transformer (SGT), a Soft-Input-Soft-Output neural
architecture tailored for MIMO detection. While Maximum Likelihood (ML)
detection achieves optimal accuracy, its prohibitive exponential complexity
renders it impractical for real-world systems. Conventional message passing
algorithms offer tractable alternatives but rely on large-system asymptotics
and random matrix assumptions, both of which break down under practical
implementations. Prior Transformer-based detectors, on the other hand, fail to
incorporate the MIMO factor graph structure and cannot utilize decoder-side
soft information, limiting their standalone performance and their applicability
in iterative detection-decoding (IDD). To overcome these limitations, SGT
integrates message passing directly into a graph-aware attention mechanism and
supports decoder-informed updates through soft-input embeddings. This design
enables effective soft-output generation while preserving computational
efficiency. As a standalone detector, SGT closely approaches ML performance and
surpasses prior Transformer-based approaches.

</details>


### [134] [Bi-level Personalization for Federated Foundation Models: A Task-vector Aggregation Approach](https://arxiv.org/abs/2509.12697)
*Yiyuan Yang,Guodong Long,Qinghua Lu,Liming Zhu,Jing Jiang*

Main category: cs.LG

TL;DR: 提出用于基础模型联合微调的双层个性化框架，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 微调基础模型服务少量新用户或特定场景有挑战，个性化与联合的权衡更敏感。

Method: 在客户端用私有数据进行个性化微调，在服务器端用客户端特定任务向量衡量相似用户进行个性化聚合。

Result: 通过基准数据集的大量实验分析证明了算法的有效性。

Conclusion: 所提出的双层个性化框架能在基础模型联合微调中有效获取群体个性化信息并减少非IID数据干扰。

Abstract: Federated foundation models represent a new paradigm to jointly fine-tune
pre-trained foundation models across clients. It is still a challenge to
fine-tune foundation models for a small group of new users or specialized
scenarios, which typically involve limited data compared to the large-scale
data used in pre-training. In this context, the trade-off between
personalization and federation becomes more sensitive. To tackle these, we
proposed a bi-level personalization framework for federated fine-tuning on
foundation models. Specifically, we conduct personalized fine-tuning on the
client-level using its private data, and then conduct a personalized
aggregation on the server-level using similar users measured by client-specific
task vectors. Given the personalization information gained from client-level
fine-tuning, the server-level personalized aggregation can gain group-wise
personalization information while mitigating the disturbance of irrelevant or
interest-conflict clients with non-IID data. The effectiveness of the proposed
algorithm has been demonstrated by extensive experimental analysis in benchmark
datasets.

</details>


### [135] [NORA: A Nephrology-Oriented Representation Learning Approach Towards Chronic Kidney Disease Classification](https://arxiv.org/abs/2509.12704)
*Mohammad Abdul Hafeez Khan,Twisha Bhattacharyya,Omar Khan,Noorah Khan,Alina Aziz Fatima Khan,Mohammed Qutub Khan,Sujoy Ghosh Hajra*

Main category: cs.LG

TL;DR: 研究用常规收集的非肾脏临床变量进行慢性肾病（CKD）分类，提出NORA方法，在数据集上评估显示其能提升性能和有泛化性。


<details>
  <summary>Details</summary>
Motivation: CKD早期检测困难，尤其在门诊实验室肾生物标志物常不可用，需探索非肾脏临床变量的预测潜力。

Method: 引入NORA方法，结合监督对比学习和非线性随机森林分类器，从电子健康记录（EHR）数据中提取患者特征用于CKD分类。

Result: NORA提高了类可分性和整体分类性能，尤其提升了早期CKD的F1分数，在不同数据集上显示出泛化性。

Conclusion: NORA方法在CKD分类和风险分层方面有效，可用于不同患者群体。

Abstract: Chronic Kidney Disease (CKD) affects millions of people worldwide, yet its
early detection remains challenging, especially in outpatient settings where
laboratory-based renal biomarkers are often unavailable. In this work, we
investigate the predictive potential of routinely collected non-renal clinical
variables for CKD classification, including sociodemographic factors, comorbid
conditions, and urinalysis findings. We introduce the Nephrology-Oriented
Representation leArning (NORA) approach, which combines supervised contrastive
learning with a nonlinear Random Forest classifier. NORA first derives
discriminative patient representations from tabular EHR data, which are then
used for downstream CKD classification. We evaluated NORA on a clinic-based EHR
dataset from Riverside Nephrology Physicians. Our results demonstrated that
NORA improves class separability and overall classification performance,
particularly enhancing the F1-score for early-stage CKD. Additionally, we
assessed the generalizability of NORA on the UCI CKD dataset, demonstrating its
effectiveness for CKD risk stratification across distinct patient cohorts.

</details>


### [136] [Spatio-temporal DeepKriging in PyTorch: A Supplementary Application to Precipitation Data for Interpolation and Probabilistic Forecasting](https://arxiv.org/abs/2509.12708)
*Pratik Nag*

Main category: cs.LG

TL;DR: 文章对欧洲降水数据进行分析，用PyTorch实现时空深度克里金（STDK）框架，开发可复现代码模块，经评估该方法有效。


<details>
  <summary>Details</summary>
Motivation: 对欧洲降水数据进行分析，用于插值和预测应用。

Method: 使用PyTorch平台实现时空深度克里金（STDK）框架，开发可复现的代码模块。

Result: 该模型能处理时空不规则性，生成高分辨率插值和多步预测，经评估有良好的预测性能和鲁棒性。

Conclusion: 提出的方法在处理欧洲降水数据的插值和预测方面是有效的。

Abstract: A detailed analysis of precipitation data over Europe is presented, with a
focus on interpolation and forecasting applications. A Spatio-temporal
DeepKriging (STDK) framework has been implemented using the PyTorch platform to
achieve these objectives. The proposed model is capable of handling
spatio-temporal irregularities while generating high-resolution interpolations
and multi-step forecasts. Reproducible code modules have been developed as
standalone PyTorch implementations for the
interpolation\footnote[2]{Interpolation -
https://github.com/pratiknag/Spatio-temporalDeepKriging-Pytorch.git} and
forecasting\footnote[3]{Forecasting -
https://github.com/pratiknag/pytorch-convlstm.git}, facilitating broader
application to similar climate datasets. The effectiveness of this approach is
demonstrated through extensive evaluation on daily precipitation measurements,
highlighting predictive performance and robustness.

</details>


### [137] [Unbiased Online Curvature Approximation for Regularized Graph Continual Learning](https://arxiv.org/abs/2509.12727)
*Jie Yin,Ke Sun,Han Wu*

Main category: cs.LG

TL;DR: 本文提出图持续学习（GCL）的正则化框架，提出新的无偏在线曲率近似方法，实验表明其优于现有基于正则化的方法。


<details>
  <summary>Details</summary>
Motivation: 正则化方法对防止GCL中的灾难性遗忘至关重要，现有方法存在局限性，需要改进。

Method: 基于Fisher信息矩阵（FIM）诱导的弯曲参数空间建立GCL的通用正则化框架，提出基于模型当前学习状态的全FIM的无偏在线曲率近似方法，在线估计正则化项。

Result: 在三个图数据集上的大量实验表明，该方法显著优于现有基于正则化的方法。

Conclusion: 该方法在稳定性（保留旧知识）和可塑性（获取新知识）之间实现了更好的权衡。

Abstract: Graph continual learning (GCL) aims to learn from a continuous sequence of
graph-based tasks. Regularization methods are vital for preventing catastrophic
forgetting in GCL, particularly in the challenging replay-free,
class-incremental setting, where each task consists of a set of unique classes.
In this work, we first establish a general regularization framework for GCL
based on the curved parameter space induced by the Fisher information matrix
(FIM). We show that the dominant Elastic Weight Consolidation (EWC) and its
variants are a special case within this framework, using a diagonal
approximation of the empirical FIM based on parameters from previous tasks. To
overcome their limitations, we propose a new unbiased online curvature
approximation of the full FIM based on the model's current learning state. Our
method directly estimates the regularization term in an online manner without
explicitly evaluating and storing the FIM itself. This enables the model to
better capture the loss landscape during learning new tasks while retaining the
knowledge learned from previous tasks. Extensive experiments on three graph
datasets demonstrate that our method significantly outperforms existing
regularization-based methods, achieving a superior trade-off between stability
(retaining old knowledge) and plasticity (acquiring new knowledge).

</details>


### [138] [A Novel Recurrent Neural Network Framework for Prediction and Treatment of Oncogenic Mutation Progression](https://arxiv.org/abs/2509.12732)
*Rishab Parthasarathy,Achintya Bhowmik*

Main category: cs.LG

TL;DR: 提出基于AI的癌症通路分析端到端框架，预测癌症严重程度和突变进展并推荐治疗，结果良好。


<details>
  <summary>Details</summary>
Motivation: 癌症致死率高，现有通路分析依赖耗时的湿实验数据，需更高效方法。

Method: 结合时间序列机器学习模型和通路分析，从数据库提取突变序列，用预处理算法筛选关键突变，输入RNN预测癌症严重程度，再结合多信息预测未来突变和推荐治疗。

Result: 框架取得稳健结果，ROC曲线准确率超60%，与现有诊断相当；预处理可分离关键突变，生成热图突出关键突变。

Conclusion: 首次提出不依赖湿实验的高效、经济端到端框架用于预测癌症进展和提供治疗建议。

Abstract: Despite significant medical advancements, cancer remains the second leading
cause of death, with over 600,000 deaths per year in the US. One emerging
field, pathway analysis, is promising but still relies on manually derived wet
lab data, which is time-consuming to acquire. This work proposes an efficient,
effective end-to-end framework for Artificial Intelligence (AI) based pathway
analysis that predicts both cancer severity and mutation progression, thus
recommending possible treatments. The proposed technique involves a novel
combination of time-series machine learning models and pathway analysis. First,
mutation sequences were isolated from The Cancer Genome Atlas (TCGA) Database.
Then, a novel preprocessing algorithm was used to filter key mutations by
mutation frequency. This data was fed into a Recurrent Neural Network (RNN)
that predicted cancer severity. Then, the model probabilistically used the RNN
predictions, information from the preprocessing algorithm, and multiple
drug-target databases to predict future mutations and recommend possible
treatments. This framework achieved robust results and Receiver Operating
Characteristic (ROC) curves (a key statistical metric) with accuracies greater
than 60%, similar to existing cancer diagnostics. In addition, preprocessing
played an instrumental role in isolating important mutations, demonstrating
that each cancer stage studied may contain on the order of a few-hundred key
driver mutations, consistent with current research. Heatmaps based on predicted
gene frequency were also generated, highlighting key mutations in each cancer.
Overall, this work is the first to propose an efficient, cost-effective
end-to-end framework for projecting cancer progression and providing possible
treatments without relying on expensive, time-consuming wet lab work.

</details>


### [139] [Similarity-Distance-Magnitude Activations](https://arxiv.org/abs/2509.12760)
*Allen Schmaltz*

Main category: cs.LG

TL;DR: 本文引入了比标准softmax更鲁棒和可解释的SDM激活函数，用于语言模型最终层，在选择性分类上有优势。


<details>
  <summary>Details</summary>
Motivation: 让标准softmax激活函数更鲁棒和可解释，应对协变量偏移和分布外输入等问题。

Method: 在现有输出幅度感知基础上，添加相似度感知和到训练分布距离感知。

Result: SDM激活函数比softmax对高概率区域的协变量偏移和分布外输入更鲁棒，可通过密集匹配提供可解释性，能划分经验CDF以防止低召回率。

Conclusion: 在选择性分类方面，SDM激活函数比softmax更优，即便考虑事后校准方法。

Abstract: We introduce a more robust and interpretable formulation of the standard
softmax activation function commonly used with neural networks by adding
Similarity (i.e., correctly predicted depth-matches into training) awareness
and Distance-to-training-distribution awareness to the existing output
Magnitude (i.e., decision-boundary) awareness. When used as the final-layer
activation with language models, the resulting Similarity-Distance-Magnitude
(SDM) activation function is more robust than the softmax function to
co-variate shifts and out-of-distribution inputs in high-probability regions,
and provides interpretability-by-exemplar via dense matching. Complementing the
prediction-conditional estimates, the SDM activation enables a partitioning of
the class-wise empirical CDFs to guard against low class-wise recall among
selective classifications. These properties make it preferable for selective
classification, even when considering post-hoc calibration methods over the
softmax.

</details>


### [140] [EmbeddedML: A New Optimized and Fast Machine Learning Library](https://arxiv.org/abs/2509.12774)
*Halil Hüseyin Çalışkan,Talha Koruk*

Main category: cs.LG

TL;DR: 文章介绍了训练时间优化且数学上增强的机器学习库EmbeddedML，在回归和分类模型中提升速度、减少训练时间且不损失准确率。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习模型和库在大数据集上训练慢、时间长的问题。

Method: 引入EmbeddedML库，对Logistic Regression和Support Vector Machines等算法进行数学重写。

Result: 在回归模型中速度提升，分类模型中，SVM在小数据集上训练时间约减2倍、大数据集约减800倍，Logistic Regression约减4倍。

Conclusion: EmbeddedML库提供的算法经数学重写和优化，可减少训练时间。

Abstract: Machine learning models and libraries can train datasets of different sizes
and perform prediction and classification operations, but machine learning
models and libraries cause slow and long training times on large datasets. This
article introduces EmbeddedML, a training-time-optimized and mathematically
enhanced machine learning library. The speed was increased by approximately
times compared to scikit-learn without any loss in terms of accuracy in
regression models such as Multiple Linear Regression. Logistic Regression and
Support Vector Machines (SVM) algorithms have been mathematically rewritten to
reduce training time and increase accuracy in classification models. With the
applied mathematical improvements, training time has been reduced by
approximately 2 times for SVM on small datasets and by around 800 times on
large datasets, and by approximately 4 times for Logistic Regression, compared
to the scikit-learn implementation. In summary, the EmbeddedML library offers
regression, classification, clustering, and dimensionality reduction algorithms
that are mathematically rewritten and optimized to reduce training time.

</details>


### [141] [Safe Reinforcement Learning using Action Projection: Safeguard the Policy or the Environment?](https://arxiv.org/abs/2509.12833)
*Hannah Markgraf,Shamburaj Sawant,Hanna Krasowski,Lukas Schäfer,Sebastien Gros,Matthias Althoff*

Main category: cs.LG

TL;DR: 本文理论比较SE - RL和SP - RL两种投影安全过滤器集成策略，分析动作混叠影响，提出改进策略，实证表明动作混叠对SP - RL更不利，但合适策略下SP - RL可表现更好。


<details>
  <summary>Details</summary>
Motivation: 投影安全过滤器在强化学习中常用，但缺乏对SE - RL和SP - RL两种集成策略差异的正式理解。

Method: 统一形式化SE - RL和SP - RL，理论分析策略梯度估计，对比缓解策略，提出SP - RL新改进方法。

Result: 实证支持理论预测，动作混叠对SP - RL更不利，合适策略下SP - RL可匹配或超越改进的SE - RL。

Conclusion: 研究为基于任务特征选择和改进投影安全强化学习方法提供了可行见解。

Abstract: Projection-based safety filters, which modify unsafe actions by mapping them
to the closest safe alternative, are widely used to enforce safety constraints
in reinforcement learning (RL). Two integration strategies are commonly
considered: Safe environment RL (SE-RL), where the safeguard is treated as part
of the environment, and safe policy RL (SP-RL), where it is embedded within the
policy through differentiable optimization layers. Despite their practical
relevance in safety-critical settings, a formal understanding of their
differences is lacking. In this work, we present a theoretical comparison of
SE-RL and SP-RL. We identify a key distinction in how each approach is affected
by action aliasing, a phenomenon in which multiple unsafe actions are projected
to the same safe action, causing information loss in the policy gradients. In
SE-RL, this effect is implicitly approximated by the critic, while in SP-RL, it
manifests directly as rank-deficient Jacobians during backpropagation through
the safeguard. Our contributions are threefold: (i) a unified formalization of
SE-RL and SP-RL in the context of actor-critic algorithms, (ii) a theoretical
analysis of their respective policy gradient estimates, highlighting the role
of action aliasing, and (iii) a comparative study of mitigation strategies,
including a novel penalty-based improvement for SP-RL that aligns with
established SE-RL practices. Empirical results support our theoretical
predictions, showing that action aliasing is more detrimental for SP-RL than
for SE-RL. However, with appropriate improvement strategies, SP-RL can match or
outperform improved SE-RL across a range of environments. These findings
provide actionable insights for choosing and refining projection-based safe RL
methods based on task characteristics.

</details>


### [142] [Tool-R1: Sample-Efficient Reinforcement Learning for Agentic Tool Use](https://arxiv.org/abs/2509.12867)
*Yabo Zhang,Yihan Zeng,Qingyun Li,Zhen Hu,Kavin Han,Wangmeng Zuo*

Main category: cs.LG

TL;DR: 提出Tool - R1框架让大语言模型通过生成Python代码使用工具，实验显示其提升准确率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理需要最新知识、精确操作或专业工具使用的现实任务时存在局限。

Method: 提出Tool - R1强化学习框架，支持集成用户定义工具和标准库，使用基于结果的奖励函数，维护动态样本队列提高效率。

Result: 在GAIA基准测试中，Tool - R1显著提升准确率和鲁棒性，比强基线提高约10%，复杂多步任务提升更大。

Conclusion: Tool - R1在现实应用中实现可靠高效的工具增强推理有潜力。

Abstract: Large language models (LLMs) have demonstrated strong capabilities in
language understanding and reasoning, yet they remain limited when tackling
real-world tasks that require up-to-date knowledge, precise operations, or
specialized tool use. To address this, we propose Tool-R1, a reinforcement
learning framework that enables LLMs to perform general, compositional, and
multi-step tool use by generating executable Python code. Tool-R1 supports
integration of user-defined tools and standard libraries, with variable sharing
across steps to construct coherent workflows. An outcome-based reward function,
combining LLM-based answer judgment and code execution success, guides policy
optimization. To improve training efficiency, we maintain a dynamic sample
queue to cache and reuse high-quality trajectories, reducing the overhead of
costly online sampling. Experiments on the GAIA benchmark show that Tool-R1
substantially improves both accuracy and robustness, achieving about 10\% gain
over strong baselines, with larger improvements on complex multi-step tasks.
These results highlight the potential of Tool-R1 for enabling reliable and
efficient tool-augmented reasoning in real-world applications. Our code will be
available at https://github.com/YBYBZhang/Tool-R1.

</details>


### [143] [Soft Gradient Boosting with Learnable Feature Transforms for Sequential Regression](https://arxiv.org/abs/2509.12920)
*Huseyin Karaca,Suleyman Serdar Kozat*

Main category: cs.LG

TL;DR: 提出用于顺序回归的软梯度提升框架，嵌入可学习线性特征变换，在高低维数据上验证性能，还扩展到非线性变换并公开代码


<details>
  <summary>Details</summary>
Motivation: 解决高维数据稀缺场景下的回归问题，避免过拟合，提升性能

Method: 在提升过程中嵌入可学习线性特征变换，每次迭代训练软决策树并学习线性输入特征变换Q，还可扩展到可微非线性变换

Result: 在合成和真实数据集上有效且高效地提升性能，避免过拟合

Conclusion: 所提软梯度提升框架可行，可通过端到端优化特征选择/变换和提升提高性能

Abstract: We propose a soft gradient boosting framework for sequential regression that
embeds a learnable linear feature transform within the boosting procedure. At
each boosting iteration, we train a soft decision tree and learn a linear input
feature transform Q together. This approach is particularly advantageous in
high-dimensional, data-scarce scenarios, as it discovers the most relevant
input representations while boosting. We demonstrate, using both synthetic and
real-world datasets, that our method effectively and efficiently increases the
performance by an end-to-end optimization of feature selection/transform and
boosting while avoiding overfitting. We also extend our algorithm to
differentiable non-linear transforms if overfitting is not a problem. To
support reproducibility and future work, we share our code publicly.

</details>


### [144] [Rethinking the Evaluation of Alignment Methods: Insights into Diversity, Generalisation, and Safety](https://arxiv.org/abs/2509.12936)
*Denis Janiak,Julia Moska,Dawid Motyka,Karolina Seweryn,Paweł Walkowiak,Bartosz Żuk,Arkadiusz Janz*

Main category: cs.LG

TL;DR: 提出统一评估框架对比大语言模型对齐方法，揭示各方法在不同维度表现，为开发更平衡可靠模型提供见解。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对大语言模型多目标对齐内在权衡的整体评估。

Method: 提出统一评估框架，在五个维度对比多种对齐方法，使用分布内和分布外数据集，借助经人类研究验证的LLM - as - Judge提示。

Result: DPO和KTO事实准确性出色，PPO和DPO安全性领先，PPO在简洁性和主动性平衡方面最佳。

Conclusion: 研究结果为常见对齐方法的权衡提供见解，指导开发更平衡可靠的大语言模型。

Abstract: Large language models (LLMs) require careful alignment to balance competing
objectives - factuality, safety, conciseness, proactivity, and diversity.
Existing studies focus on individual techniques or specific dimensions, lacking
a holistic assessment of the inherent trade-offs. We propose a unified
evaluation framework that compares LLM alignment methods (PPO, DPO, ORPO, KTO)
across these five axes, using both in-distribution and out-of-distribution
datasets. Leveraging a specialized LLM-as-Judge prompt, validated through human
studies, we reveal that DPO and KTO excel in factual accuracy, PPO and DPO lead
in safety, and PPO best balances conciseness with proactivity. Our findings
provide insights into trade-offs of common alignment methods, guiding the
development of more balanced and reliable LLMs.

</details>


### [145] [Sy-FAR: Symmetry-based Fair Adversarial Robustness](https://arxiv.org/abs/2509.12939)
*Haneen Najjar,Eyal Ronen,Mahmood Sharif*

Main category: cs.LG

TL;DR: 提出Sy - FAR技术，在提升对抗鲁棒性同时促进对称性，实验表明其在公平对抗鲁棒性上优于现有方法，且更快更稳定。


<details>
  <summary>Details</summary>
Motivation: 现有提升机器学习对抗鲁棒性的方法存在不公平性，且以往工作聚焦于安全和公平性要求较低的场景，在实际公平关键任务中实现完美公平不可行。

Method: 开发Sy - FAR技术，在优化对抗鲁棒性的同时鼓励对称性，并使用五个数据集、三种模型架构进行评估。

Result: Sy - FAR显著提高了公平对抗鲁棒性，速度更快且运行更一致，还改善了发现的另一种不公平性。

Conclusion: Sy - FAR是一种有效提升公平对抗鲁棒性的技术。

Abstract: Security-critical machine-learning (ML) systems, such as face-recognition
systems, are susceptible to adversarial examples, including real-world
physically realizable attacks. Various means to boost ML's adversarial
robustness have been proposed; however, they typically induce unfair
robustness: It is often easier to attack from certain classes or groups than
from others. Several techniques have been developed to improve adversarial
robustness while seeking perfect fairness between classes. Yet, prior work has
focused on settings where security and fairness are less critical. Our insight
is that achieving perfect parity in realistic fairness-critical tasks, such as
face recognition, is often infeasible -- some classes may be highly similar,
leading to more misclassifications between them. Instead, we suggest that
seeking symmetry -- i.e., attacks from class $i$ to $j$ would be as successful
as from $j$ to $i$ -- is more tractable. Intuitively, symmetry is a desirable
because class resemblance is a symmetric relation in most domains.
Additionally, as we prove theoretically, symmetry between individuals induces
symmetry between any set of sub-groups, in contrast to other fairness notions
where group-fairness is often elusive. We develop Sy-FAR, a technique to
encourage symmetry while also optimizing adversarial robustness and extensively
evaluate it using five datasets, with three model architectures, including
against targeted and untargeted realistic attacks. The results show Sy-FAR
significantly improves fair adversarial robustness compared to state-of-the-art
methods. Moreover, we find that Sy-FAR is faster and more consistent across
runs. Notably, Sy-FAR also ameliorates another type of unfairness we discover
in this work -- target classes that adversarial examples are likely to be
classified into become significantly less vulnerable after inducing symmetry.

</details>


### [146] [Spatiotemporal graph neural process for reconstruction, extrapolation, and classification of cardiac trajectories](https://arxiv.org/abs/2509.12953)
*Jaume Banus,Augustin C. Ogier,Roger Hullin,Philippe Meyer,Ruud B. van Heeswijk,Jonas Richiardi*

Main category: cs.LG

TL;DR: 提出用于从稀疏观测中建模结构化时空动态的概率框架，以心脏运动为重点，结合多种网络，在合成系统和真实心脏数据集验证，成果佳。


<details>
  <summary>Details</summary>
Motivation: 从稀疏观测中对结构化时空动态进行建模，特别是针对心脏运动分析。

Method: 将神经常微分方程、图神经网络和神经过程集成到统一模型，用图神经网络参数化向量场建模潜在轨迹，根据稀疏观测推断潜在初始状态和控制变量分布。

Result: 在三个合成动力系统和两个真实心脏成像数据集验证，能准确重建、外推轨迹，在ACDC分类任务达99%准确率，在UK Biobank检测房颤达67%准确率。

Conclusion: 该框架是分析心脏运动的灵活方法，为基于图的生物医学时空序列数据学习奠定基础。

Abstract: We present a probabilistic framework for modeling structured spatiotemporal
dynamics from sparse observations, focusing on cardiac motion. Our approach
integrates neural ordinary differential equations (NODEs), graph neural
networks (GNNs), and neural processes into a unified model that captures
uncertainty, temporal continuity, and anatomical structure. We represent
dynamic systems as spatiotemporal multiplex graphs and model their latent
trajectories using a GNN-parameterized vector field. Given the sparse context
observations at node and edge levels, the model infers a distribution over
latent initial states and control variables, enabling both interpolation and
extrapolation of trajectories. We validate the method on three synthetic
dynamical systems (coupled pendulum, Lorenz attractor, and Kuramoto
oscillators) and two real-world cardiac imaging datasets - ACDC (N=150) and UK
Biobank (N=526) - demonstrating accurate reconstruction, extrapolation, and
disease classification capabilities. The model accurately reconstructs
trajectories and extrapolates future cardiac cycles from a single observed
cycle. It achieves state-of-the-art results on the ACDC classification task (up
to 99% accuracy), and detects atrial fibrillation in UK Biobank subjects with
competitive performance (up to 67% accuracy). This work introduces a flexible
approach for analyzing cardiac motion and offers a foundation for graph-based
learning in structured biomedical spatiotemporal time-series data.

</details>


### [147] [BAPFL: Exploring Backdoor Attacks Against Prototype-based Federated Learning](https://arxiv.org/abs/2509.12964)
*Honghong Zeng,Jiong Lou,Zhe Wang,Hefeng Zhou,Chentao Wu,Wei Zhao,Jie Li*

Main category: cs.LG

TL;DR: 本文指出PFL对现有后门攻击有固有抗性，提出首个针对PFL框架的后门攻击方法BAPFL，实验显示其攻击成功率提升且能保持主任务准确率。


<details>
  <summary>Details</summary>
Motivation: PFL在解决联邦学习数据异质性问题有潜力，但针对其后门攻击的鲁棒性研究不足，需进一步探索其安全性。

Method: 提出BAPFL，结合原型中毒策略和触发优化机制，前者操纵全局原型轨迹误导良性客户端，后者为每个潜在目标标签学习独特隐蔽的触发器。

Result: 在多个数据集和PFL变体上实验，BAPFL攻击成功率比传统后门攻击提高35%-75%，并保持主任务准确率。

Conclusion: BAPFL在PFL中有效、隐蔽且具有适应性。

Abstract: Prototype-based federated learning (PFL) has emerged as a promising paradigm
to address data heterogeneity problems in federated learning, as it leverages
mean feature vectors as prototypes to enhance model generalization. However,
its robustness against backdoor attacks remains largely unexplored. In this
paper, we identify that PFL is inherently resistant to existing backdoor
attacks due to its unique prototype learning mechanism and local data
heterogeneity. To further explore the security of PFL, we propose BAPFL, the
first backdoor attack method specifically designed for PFL frameworks. BAPFL
integrates a prototype poisoning strategy with a trigger optimization
mechanism. The prototype poisoning strategy manipulates the trajectories of
global prototypes to mislead the prototype training of benign clients, pushing
their local prototypes of clean samples away from the prototypes of
trigger-embedded samples. Meanwhile, the trigger optimization mechanism learns
a unique and stealthy trigger for each potential target label, and guides the
prototypes of trigger-embedded samples to align closely with the global
prototype of the target label. Experimental results across multiple datasets
and PFL variants demonstrate that BAPFL achieves a 35\%-75\% improvement in
attack success rate compared to traditional backdoor attacks, while preserving
main task accuracy. These results highlight the effectiveness, stealthiness,
and adaptability of BAPFL in PFL.

</details>


### [148] [Bridging Performance Gaps for Foundation Models: A Post-Training Strategy for ECGFounder](https://arxiv.org/abs/2509.12991)
*Ya Zhou,Yujie Yang,Xiaohan Fan,Wei Zhao*

Main category: cs.LG

TL;DR: 提出一种简单有效的后训练方法提升ECGFounder模型性能，在PTB - XL基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有ECG基础模型临床适用性受限于性能差距，可能因缺乏有效后训练策略。

Method: 提出简单有效的后训练方法增强预训练的ECGFounder模型。

Result: 在PTB - XL基准测试中，宏AUROC提升1.2% - 3.3%，宏AUPRC提升5.3% - 20.9%；使用10%训练数据时，宏AUROC提升9.1%，宏AUPRC提升34.9%，且优于多种先进方法。

Conclusion: 后训练策略有提升ECG基础模型的潜力，有望推动该领域基础模型发展。

Abstract: ECG foundation models are increasingly popular due to their adaptability
across various tasks. However, their clinical applicability is often limited by
performance gaps compared to task-specific models, even after pre-training on
large ECG datasets and fine-tuning on target data. This limitation is likely
due to the lack of an effective post-training strategy. In this paper, we
propose a simple yet effective post-training approach to enhance ECGFounder, a
state-of-the-art ECG foundation model pre-trained on over 7 million ECG
recordings. Experiments on the PTB-XL benchmark show that our approach improves
the baseline fine-tuning strategy by 1.2%-3.3% in macro AUROC and 5.3%-20.9% in
macro AUPRC. Additionally, our method outperforms several recent
state-of-the-art approaches, including task-specific and advanced
architectures. Further evaluation reveals that our method is more stable and
sample-efficient compared to the baseline, achieving a 9.1% improvement in
macro AUROC and a 34.9% improvement in macro AUPRC using just 10% of the
training data. Ablation studies identify key components, such as stochastic
depth and preview linear probing, that contribute to the enhanced performance.
These findings underscore the potential of post-training strategies to improve
ECG foundation models, and we hope this work will contribute to the continued
development of foundation models in the ECG domain.

</details>


### [149] [Ensemble Visualization With Variational Autoencoder](https://arxiv.org/abs/2509.13000)
*Cenyang Wu,Qinhan Yu,Liang Zhou*

Main category: cs.LG

TL;DR: 提出通过在潜空间构建结构化概率表示来可视化数据集合的新方法，以气象预报集合初步验证其有效性和通用性。


<details>
  <summary>Details</summary>
Motivation: 找到一种新的数据集合可视化方法。

Method: 通过特征空间转换和使用变分自编码器（VAE）的无监督学习，将集合的空间特征转换到潜空间。

Result: 得到的潜空间遵循多元标准高斯分布，可进行置信区间的解析计算和生成数据集合的概率分布的密度估计；在气象预报集合上的初步结果显示了方法的有效性和通用性。

Conclusion: 提出的方法有效且通用，可用于数据集合可视化。

Abstract: We present a new method to visualize data ensembles by constructing
structured probabilistic representations in latent spaces, i.e.,
lower-dimensional representations of spatial data features. Our approach
transforms the spatial features of an ensemble into a latent space through
feature space conversion and unsupervised learning using a variational
autoencoder (VAE). The resulting latent spaces follow multivariate standard
Gaussian distributions, enabling analytical computation of confidence intervals
and density estimation of the probabilistic distribution that generates the
data ensemble. Preliminary results on a weather forecasting ensemble
demonstrate the effectiveness and versatility of our method.

</details>


### [150] [ReTrack: Data Unlearning in Diffusion Models through Redirecting the Denoising Trajectory](https://arxiv.org/abs/2509.13007)
*Qitan Shi,Cheng Jin,Jiawei Zhang,Yuantao Gu*

Main category: cs.LG

TL;DR: 提出ReTrack用于扩散模型的数据遗忘方法，实验显示其达SOTA。


<details>
  <summary>Details</summary>
Motivation: 扩散模型存在训练数据记忆问题，引发隐私和安全担忧，数据遗忘可缓解此问题。

Method: 采用重要性采样构建更高效的微调损失，仅保留主导项进行近似，使去噪轨迹转向k近邻。

Result: 在MNIST T-Shirt、CelebA - HQ、CIFAR - 10和Stable Diffusion上实验，ReTrack达SOTA。

Conclusion: ReTrack在遗忘强度和生成质量保留间达到最佳平衡。

Abstract: Diffusion models excel at generating high-quality, diverse images but suffer
from training data memorization, raising critical privacy and safety concerns.
Data unlearning has emerged to mitigate this issue by removing the influence of
specific data without retraining from scratch. We propose ReTrack, a fast and
effective data unlearning method for diffusion models. ReTrack employs
importance sampling to construct a more efficient fine-tuning loss, which we
approximate by retaining only dominant terms. This yields an interpretable
objective that redirects denoising trajectories toward the $k$-nearest
neighbors, enabling efficient unlearning while preserving generative quality.
Experiments on MNIST T-Shirt, CelebA-HQ, CIFAR-10, and Stable Diffusion show
that ReTrack achieves state-of-the-art performance, striking the best trade-off
between unlearning strength and generation quality preservation.

</details>


### [151] [Spiking Vocos: An Energy-Efficient Neural Vocoder](https://arxiv.org/abs/2509.13049)
*Yukun Chen,Zhaoxi Mu,Andong Li,Peilin Li,Xinyu Yang*

Main category: cs.LG

TL;DR: 提出超低能耗的脉冲神经声码器Spiking Vocos，性能与ANN相当但能耗低。


<details>
  <summary>Details</summary>
Motivation: 现有神经声码器能耗高，阻碍在边缘设备部署，SNN高能量效率可解决低资源场景问题。

Method: 基于Vocos框架构建Spiking Vocos，设计Spiking ConvNeXt模块减少MAC操作、加入振幅捷径路径，引入自架构蒸馏策略转移知识，集成轻量级时间移位模块。

Result: 模型性能与ANN相当，UTMOS和PESQ分数分别为3.74和3.45，能耗仅为14.7%。

Conclusion: Spiking Vocos在低能耗下实现了与ANN相当的性能，可用于资源受限场景。

Abstract: Despite the remarkable progress in the synthesis speed and fidelity of neural
vocoders, their high energy consumption remains a critical barrier to practical
deployment on computationally restricted edge devices. Spiking Neural Networks
(SNNs), widely recognized for their high energy efficiency due to their
event-driven nature, offer a promising solution for low-resource scenarios. In
this paper, we propose Spiking Vocos, a novel spiking neural vocoder with
ultra-low energy consumption, built upon the efficient Vocos framework. To
mitigate the inherent information bottleneck in SNNs, we design a Spiking
ConvNeXt module to reduce Multiply-Accumulate (MAC) operations and incorporate
an amplitude shortcut path to preserve crucial signal dynamics. Furthermore, to
bridge the performance gap with its Artificial Neural Network (ANN)
counterpart, we introduce a self-architectural distillation strategy to
effectively transfer knowledge. A lightweight Temporal Shift Module is also
integrated to enhance the model's ability to fuse information across the
temporal dimension with negligible computational overhead. Experiments
demonstrate that our model achieves performance comparable to its ANN
counterpart, with UTMOS and PESQ scores of 3.74 and 3.45 respectively, while
consuming only 14.7% of the energy. The source code is available at
https://github.com/pymaster17/Spiking-Vocos.

</details>


### [152] [When Inverse Data Outperforms: Exploring the Pitfalls of Mixed Data in Multi-Stage Fine-Tuning](https://arxiv.org/abs/2509.13079)
*Mengyi Deng,Xin Li,Tingyu Zhu,Zhicheng Yang,Zhijiang Guo,Wei Wang*

Main category: cs.LG

TL;DR: 构建反向推理数据集r1k，研究SFT和DPO在双向推理目标下对对齐的影响，发现混合推理数据有冲突监督信号，需鲁棒且方向感知的对齐策略。


<details>
  <summary>Details</summary>
Motivation: 现有方法多关注单向监督微调，忽略不同推理模式间复杂相互作用，需研究双向推理目标下的对齐。

Method: 构建r1k数据集，研究SFT和DPO在双向推理目标下的表现。

Result: 在r1k上进行SFT在评估基准上比s1k准确率提高1.6% - 6.8%；SFT中简单混合正反数据会削弱方向区分；DPO可部分恢复区分但会抑制不太偏好的推理路径。

Conclusion: 混合推理数据会引入冲突监督信号，需要鲁棒且方向感知的对齐策略。

Abstract: Existing work has shown that o1-level performance can be achieved with
limited data distillation, but most existing methods focus on unidirectional
supervised fine-tuning (SFT), overlooking the intricate interplay between
diverse reasoning patterns. In this paper, we construct r1k, a high-quality
reverse reasoning dataset derived by inverting 1,000 forward examples from s1k,
and examine how SFT and Direct Preference Optimization (DPO) affect alignment
under bidirectional reasoning objectives. SFT on r1k yields a 1.6%--6.8%
accuracy improvement over s1k across evaluated benchmarks. However, naively
mixing forward and reverse data during SFT weakens the directional distinction.
Although DPO can partially recover this distinction, it also suppresses less
preferred reasoning paths by shifting the probability mass toward irrelevant
outputs. These findings suggest that mixed reasoning data introduce conflicting
supervision signals, underscoring the need for robust and direction-aware
alignment strategies.

</details>


### [153] [Discovering Mathematical Equations with Diffusion Language Model](https://arxiv.org/abs/2509.13136)
*Xiaoxu Han,Chengzhen Ning,Jinghui Zhong,Fubiao Yang,Yu Wang,Xin Mu*

Main category: cs.LG

TL;DR: 本文提出DiffuSR预训练框架用于符号回归，经实验在标准基准上表现有竞争力，能生成更具可解释性和多样性的数学表达式。


<details>
  <summary>Details</summary>
Motivation: 符号回归任务因搜索空间大、需平衡精度与复杂度而具有挑战性，需要有效方法进行科学发现。

Method: 引入基于连续状态扩散语言模型的DiffuSR框架，在扩散过程中用可训练嵌入层将离散数学符号映射到连续潜在空间，通过迭代去噪生成符号方程，设计有效推理策略将对数先验注入遗传编程。

Result: 在标准符号回归基准上，DiffuSR与最先进的自回归方法表现相当，能生成更具可解释性和多样性的数学表达式。

Conclusion: DiffuSR是用于符号回归的有效预训练框架，有良好性能和生成效果。

Abstract: Discovering valid and meaningful mathematical equations from observed data
plays a crucial role in scientific discovery. While this task, symbolic
regression, remains challenging due to the vast search space and the trade-off
between accuracy and complexity. In this paper, we introduce DiffuSR, a
pre-training framework for symbolic regression built upon a continuous-state
diffusion language model. DiffuSR employs a trainable embedding layer within
the diffusion process to map discrete mathematical symbols into a continuous
latent space, modeling equation distributions effectively. Through iterative
denoising, DiffuSR converts an initial noisy sequence into a symbolic equation,
guided by numerical data injected via a cross-attention mechanism. We also
design an effective inference strategy to enhance the accuracy of the
diffusion-based equation generator, which injects logit priors into genetic
programming. Experimental results on standard symbolic regression benchmarks
demonstrate that DiffuSR achieves competitive performance with state-of-the-art
autoregressive methods and generates more interpretable and diverse
mathematical expressions.

</details>


### [154] [Curriculum Learning for Mesh-based simulations](https://arxiv.org/abs/2509.13138)
*Paul Garnier,Vincent Lannelongue,Elie Hachem*

Main category: cs.LG

TL;DR: 研究粗到细课程学习法加速图神经网络训练收敛，减少时间且提升学习能力


<details>
  <summary>Details</summary>
Motivation: 图神经网络在高分辨率非结构化网格上训练成本高

Method: 采用粗到细课程学习法，先在粗网格训练，再引入中高分辨率网格，模型不变仅训练数据保真度变化

Result: 实现可比泛化精度，减少最多50%的总时钟时间，在模型能力不足时可突破学习瓶颈

Conclusion: 粗到细课程学习法能有效加速图神经网络训练并提升学习效果

Abstract: Graph neural networks (GNNs) have emerged as powerful surrogates for
mesh-based computational fluid dynamics (CFD), but training them on
high-resolution unstructured meshes with hundreds of thousands of nodes remains
prohibitively expensive. We study a \emph{coarse-to-fine curriculum} that
accelerates convergence by first training on very coarse meshes and then
progressively introducing medium and high resolutions (up to \(3\times10^5\)
nodes). Unlike multiscale GNN architectures, the model itself is unchanged;
only the fidelity of the training data varies over time. We achieve comparable
generalization accuracy while reducing total wall-clock time by up to 50\%.
Furthermore, on datasets where our model lacks the capacity to learn the
underlying physics, using curriculum learning enables it to break through
plateaus.

</details>


### [155] [Learning from Heterophilic Graphs: A Spectral Theory Perspective on the Impact of Self-Loops and Parallel Edges](https://arxiv.org/abs/2509.13139)
*Kushal Bose,Swagatam Das*

Main category: cs.LG

TL;DR: 文章研究异质图上低通滤波器性能，通过添加自环和并行边更新图，研究其对GCN性能影响，建立图谱与性能趋势联系，可评估图谱和属性且无需特征值分解。


<details>
  <summary>Details</summary>
Motivation: 图异质性对消息传递图神经网络性能造成挑战，低通滤波器在异质图上性能需深入分析。

Method: 通过添加自环和并行边更新异质图，研究GCN在不同基准异质网络上的性能。

Result: 添加自环和并行边会使图拉普拉斯矩阵特征值分别减小和增大，GCN性能呈现增减趋势。

Conclusion: 建立了图谱与低通滤波器在异质图上性能趋势的联系，可评估图谱和属性，还讨论了相关理论基础。

Abstract: Graph heterophily poses a formidable challenge to the performance of
Message-passing Graph Neural Networks (MP-GNNs). The familiar low-pass filters
like Graph Convolutional Networks (GCNs) face performance degradation, which
can be attributed to the blending of the messages from dissimilar neighboring
nodes. The performance of the low-pass filters on heterophilic graphs still
requires an in-depth analysis. In this context, we update the heterophilic
graphs by adding a number of self-loops and parallel edges. We observe that
eigenvalues of the graph Laplacian decrease and increase respectively by
increasing the number of self-loops and parallel edges. We conduct several
studies regarding the performance of GCN on various benchmark heterophilic
networks by adding either self-loops or parallel edges. The studies reveal that
the GCN exhibited either increasing or decreasing performance trends on adding
self-loops and parallel edges. In light of the studies, we established
connections between the graph spectra and the performance trends of the
low-pass filters on the heterophilic graphs. The graph spectra characterize the
essential intrinsic properties of the input graph like the presence of
connected components, sparsity, average degree, cluster structures, etc. Our
work is adept at seamlessly evaluating graph spectrum and properties by
observing the performance trends of the low-pass filters without pursuing the
costly eigenvalue decomposition. The theoretical foundations are also discussed
to validate the impact of adding self-loops and parallel edges on the graph
spectrum.

</details>


### [156] [FinSearchComp: Towards a Realistic, Expert-Level Evaluation of Financial Search and Reasoning](https://arxiv.org/abs/2509.13160)
*Liang Hu,Jianpeng Jiao,Jiashuo Liu,Yanle Ren,Zhoufutu Wen,Kaiyuan Zhang,Xuanliang Zhang,Xiang Gao,Tianci He,Fei Hu,Yali Liao,Zaiyuan Wang,Chenghao Yang,Qianyu Yang,Mingren Yin,Zhiyuan Zeng,Ge Zhang,Xinyi Zhang,Xiying Zhao,Zhenwei Zhu,Hongseok Namkoong,Wenhao Huang,Yuwen Tang*

Main category: cs.LG

TL;DR: 提出首个开源金融搜索推理代理基准FinSearchComp，含三类任务，对21个模型评估，实验分析得出相关结论。


<details>
  <summary>Details</summary>
Motivation: 现有开放金融数据集无法评估端到端代理的数据搜索能力，金融领域对评估搜索和推理能力有需求。

Method: 构建包含三类任务的FinSearchComp基准，邀请70位专业金融专家标注，采用多阶段质量保证流程，对21个模型评估。

Result: Grok 4 (web)在全球子集领先，DouBao (web)在大中华子集领先，配备网络搜索和金融插件可提升结果，模型和工具的国别来源影响性能。

Conclusion: FinSearchComp为复杂金融搜索和推理提供专业、高难度测试平台。

Abstract: Search has emerged as core infrastructure for LLM-based agents and is widely
viewed as critical on the path toward more general intelligence. Finance is a
particularly demanding proving ground: analysts routinely conduct complex,
multi-step searches over time-sensitive, domain-specific data, making it ideal
for assessing both search proficiency and knowledge-grounded reasoning. Yet no
existing open financial datasets evaluate data searching capability of
end-to-end agents, largely because constructing realistic, complicated tasks
requires deep financial expertise and time-sensitive data is hard to evaluate.
We present FinSearchComp, the first fully open-source agent benchmark for
realistic, open-domain financial search and reasoning. FinSearchComp comprises
three tasks -- Time-Sensitive Data Fetching, Simple Historical Lookup, and
Complex Historical Investigation -- closely reproduce real-world financial
analyst workflows. To ensure difficulty and reliability, we engage 70
professional financial experts for annotation and implement a rigorous
multi-stage quality-assurance pipeline. The benchmark includes 635 questions
spanning global and Greater China markets, and we evaluate 21 models (products)
on it. Grok 4 (web) tops the global subset, approaching expert-level accuracy.
DouBao (web) leads on the Greater China subset. Experimental analyses show that
equipping agents with web search and financial plugins substantially improves
results on FinSearchComp, and the country origin of models and tools impact
performance significantly.By aligning with realistic analyst tasks and
providing end-to-end evaluation, FinSearchComp offers a professional,
high-difficulty testbed for complex financial search and reasoning.

</details>


### [157] [On the Correlation between Individual Fairness and Predictive Accuracy in Probabilistic Models](https://arxiv.org/abs/2509.13165)
*Alessandro Antonucci,Eric Rossetto,Ivan Duvnjak*

Main category: cs.LG

TL;DR: 研究生成概率分类器中个体公平性，通过分析后验推断对私有特征扰动的鲁邦性，实证验证鲁棒性与预测准确性的相关性，为缓解公平性与准确性权衡提供新方向。


<details>
  <summary>Details</summary>
Motivation: 探究生成概率分类器中的个体公平性，缓解公平性与准确性之间的传统权衡。

Method: 基于鲁棒性分析的已有结果提出假设，使用含公平性问题的14个数据集进行实证评估，将贝叶斯网络鲁棒性分析问题转化为辅助马尔可夫随机场中的最可能解释任务。

Result: 实验证实了鲁棒性与预测准确性之间的相关性假设。

Conclusion: 为缓解公平性与准确性的传统权衡提供了新方向。

Abstract: We investigate individual fairness in generative probabilistic classifiers by
analysing the robustness of posterior inferences to perturbations in private
features. Building on established results in robustness analysis, we
hypothesise a correlation between robustness and predictive accuracy,
specifically, instances exhibiting greater robustness are more likely to be
classified accurately. We empirically assess this hypothesis using a benchmark
of fourteen datasets with fairness concerns, employing Bayesian networks as the
underlying generative models. To address the computational complexity
associated with robustness analysis over multiple private features with
Bayesian networks, we reformulate the problem as a most probable explanation
task in an auxiliary Markov random field. Our experiments confirm the
hypothesis about the correlation, suggesting novel directions to mitigate the
traditional trade-off between fairness and accuracy.

</details>


### [158] [CoVariance Filters and Neural Networks over Hilbert Spaces](https://arxiv.org/abs/2509.13178)
*Claudio Battiloro,Andrea Cavallo,Elvin Isufi*

Main category: cs.LG

TL;DR: 提出用于无限维希尔伯特空间信号的卷积学习框架HVNs，验证其在时间序列分类任务上性能优于MLP和FPCA分类器。


<details>
  <summary>Details</summary>
Motivation: 现有CoVariance神经网络（VNNs）在无限维希尔伯特空间的适用性研究不足，需探索相关学习框架。

Method: 引入基于（经验）协方差算子的卷积学习框架，定义Hilbert coVariance Filters（HVFs），设计Hilbert coVariance Networks（HVNs），提出离散化过程。

Result: 证明经验HVFs可恢复滤波信号的Functional PCA（FPCA），在合成和真实世界时间序列分类任务上验证了HVNs性能。

Conclusion: HVNs框架具有通用性，在时间序列分类任务中表现稳健，优于MLP和FPCA分类器。

Abstract: CoVariance Neural Networks (VNNs) perform graph convolutions on the empirical
covariance matrix of signals defined over finite-dimensional Hilbert spaces,
motivated by robustness and transferability properties. Yet, little is known
about how these arguments extend to infinite-dimensional Hilbert spaces. In
this work, we take a first step by introducing a novel convolutional learning
framework for signals defined over infinite-dimensional Hilbert spaces,
centered on the (empirical) covariance operator. We constructively define
Hilbert coVariance Filters (HVFs) and design Hilbert coVariance Networks (HVNs)
as stacks of HVF filterbanks with nonlinear activations. We propose a
principled discretization procedure, and we prove that empirical HVFs can
recover the Functional PCA (FPCA) of the filtered signals. We then describe the
versatility of our framework with examples ranging from multivariate
real-valued functions to reproducing kernel Hilbert spaces. Finally, we
validate HVNs on both synthetic and real-world time-series classification
tasks, showing robust performance compared to MLP and FPCA-based classifiers.

</details>


### [159] [Is Meta-Learning Out? Rethinking Unsupervised Few-Shot Classification with Limited Entropy](https://arxiv.org/abs/2509.13185)
*Yunchuan Guan,Yu Liu,Ke Zhou,Zhiqi Shen,Jenq-Neng Hwang,Serge Belongie,Lei Li*

Main category: cs.LG

TL;DR: 本文为证明元学习价值，设熵受限监督设置对比，发现元学习泛化性好、效率高、抗噪强，提出MINO框架，实验验证其在无监督少样本和零样本任务有效。


<details>
  <summary>Details</summary>
Motivation: 证明元学习在少样本分类任务中的价值，因全类训练策略训练的模型与元学习训练的模型性能相当。

Method: 建立熵受限监督设置进行理论分析和实验验证，提出MINO框架，使用DBSCAN自适应聚类算法和动态头构建无监督任务，用基于稳定性的元缩放器增强抗噪性。

Result: 元学习比全类训练有更紧的泛化边界，在有限熵下更高效，对标签噪声和异构任务更鲁棒；MINO框架在多个无监督少样本和零样本任务中有效。

Conclusion: 元学习在少样本任务中具有优势，MINO框架能有效提升无监督任务性能。

Abstract: Meta-learning is a powerful paradigm for tackling few-shot tasks. However,
recent studies indicate that models trained with the whole-class training
strategy can achieve comparable performance to those trained with meta-learning
in few-shot classification tasks. To demonstrate the value of meta-learning, we
establish an entropy-limited supervised setting for fair comparisons. Through
both theoretical analysis and experimental validation, we establish that
meta-learning has a tighter generalization bound compared to whole-class
training. We unravel that meta-learning is more efficient with limited entropy
and is more robust to label noise and heterogeneous tasks, making it
well-suited for unsupervised tasks. Based on these insights, We propose MINO, a
meta-learning framework designed to enhance unsupervised performance. MINO
utilizes the adaptive clustering algorithm DBSCAN with a dynamic head for
unsupervised task construction and a stability-based meta-scaler for robustness
against label noise. Extensive experiments confirm its effectiveness in
multiple unsupervised few-shot and zero-shot tasks.

</details>


### [160] [TRUST-FS: Tensorized Reliable Unsupervised Multi-View Feature Selection for Incomplete Data](https://arxiv.org/abs/2509.13192)
*Minghui Lu,Yanyong Huang,Minbo Ma,Dongjie Wang,Xiuwen Yi,Tianrui Li*

Main category: cs.LG

TL;DR: 提出TRUST - FS方法解决不完整多视图数据含缺失变量的多视图无监督特征选择问题，实验证明其有效性和优越性。


<details>
  <summary>Details</summary>
Motivation: 现有多视图无监督特征选择方法在处理不完整多视图数据时存在局限，如无法处理缺失变量、独立处理特征选择和缺失值填充、缺失数据影响相似度图准确性等问题。

Method: 提出TRUST - FS方法，引入自适应加权CP分解，在统一张量分解框架下同时进行特征选择、缺失变量填充和视图权重学习，利用主观逻辑获取可靠的跨视图相似度信息来学习可靠的相似度图。

Result: 综合实验结果表明该方法比现有最先进方法更有效、更优越。

Conclusion: TRUST - FS方法能有效解决不完整多视图数据含缺失变量的多视图无监督特征选择问题。

Abstract: Multi-view unsupervised feature selection (MUFS), which selects informative
features from multi-view unlabeled data, has attracted increasing research
interest in recent years. Although great efforts have been devoted to MUFS,
several challenges remain: 1) existing methods for incomplete multi-view data
are limited to handling missing views and are unable to address the more
general scenario of missing variables, where some features have missing values
in certain views; 2) most methods address incomplete data by first imputing
missing values and then performing feature selection, treating these two
processes independently and overlooking their interactions; 3) missing data can
result in an inaccurate similarity graph, which reduces the performance of
feature selection. To solve this dilemma, we propose a novel MUFS method for
incomplete multi-view data with missing variables, termed Tensorized Reliable
UnSupervised mulTi-view Feature Selection (TRUST-FS). TRUST-FS introduces a new
adaptive-weighted CP decomposition that simultaneously performs feature
selection, missing-variable imputation, and view weight learning within a
unified tensor factorization framework. By utilizing Subjective Logic to
acquire trustworthy cross-view similarity information, TRUST-FS facilitates
learning a reliable similarity graph, which subsequently guides feature
selection and imputation. Comprehensive experimental results demonstrate the
effectiveness and superiority of our method over state-of-the-art methods.

</details>


### [161] [B-TGAT: A Bi-directional Temporal Graph Attention Transformer for Clustering Multivariate Spatiotemporal Data](https://arxiv.org/abs/2509.13202)
*Francis Ndikum Nji,Vandana Janaja,Jianwu Wang*

Main category: cs.LG

TL;DR: 提出时间分布混合U - Net自编码器集成B - TGAT用于高维多元时空气候数据聚类，实验效果优于基线。


<details>
  <summary>Details</summary>
Motivation: 高维多元时空气候数据聚类因复杂时间依赖、空间交互和非平稳动态而具挑战性，传统方法难兼顾时空关系与空间上下文。

Method: 提出时间分布混合U - Net自编码器，集成B - TGAT，编码器和解码器用ConvLSTM2D模块提取时空特征，用跳跃连接保留多尺度空间细节，B - TGAT集成空间建模与时间编码。

Result: 在三个不同时空气候数据集上实验，聚类分离性、时间稳定性更好，与已知气候转变更匹配。

Conclusion: 集成ConvLSTM2D、U - Net跳跃连接和B - TGAT提升了时间聚类性能，为复杂时空变异性提供可解释见解，推动方法发展和气候科学应用。

Abstract: Clustering high-dimensional multivariate spatiotemporal climate data is
challenging due to complex temporal dependencies, evolving spatial
interactions, and non-stationary dynamics. Conventional clustering methods,
including recurrent and convolutional models, often struggle to capture both
local and global temporal relationships while preserving spatial context. We
present a time-distributed hybrid U-Net autoencoder that integrates a
Bi-directional Temporal Graph Attention Transformer (B-TGAT) to guide efficient
temporal clustering of multidimensional spatiotemporal climate datasets. The
encoder and decoder are equipped with ConvLSTM2D modules that extract joint
spatial--temporal features by modeling localized dynamics and spatial
correlations over time, and skip connections that preserve multiscale spatial
details during feature compression and reconstruction. At the bottleneck,
B-TGAT integrates graph-based spatial modeling with attention-driven temporal
encoding, enabling adaptive weighting of temporal neighbors and capturing both
short and long-range dependencies across regions. This architecture produces
discriminative latent embeddings optimized for clustering. Experiments on three
distinct spatiotemporal climate datasets demonstrate superior cluster
separability, temporal stability, and alignment with known climate transitions
compared to state-of-the-art baselines. The integration of ConvLSTM2D, U-Net
skip connections, and B-TGAT enhances temporal clustering performance while
providing interpretable insights into complex spatiotemporal variability,
advancing both methodological development and climate science applications.

</details>


### [162] [HAM: Hierarchical Adapter Merging for Scalable Continual Learning](https://arxiv.org/abs/2509.13211)
*Eric Nuertey Coleman,Luigi Quarantiello,Samrat Mukherjee,Julio Hurtado,Vincenzo Lomonaco*

Main category: cs.LG

TL;DR: 本文提出分层适配器合并（HAM）框架，动态组合不同任务适配器，实验显示其在多任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 持续学习中，现有深度学习模型存在灾难性遗忘问题，大预训练模型应对新数据分布能力有限，PEFT方法在动态学习场景和长任务序列中存在挑战。

Method: 引入分层适配器合并（HAM）框架，维护固定组分层合并新适配器，为每个任务训练低秩适配器和重要性标量，基于适配器相似度动态分组，组内进行适配器修剪、缩放和合并。

Result: 在三个视觉基准测试中，HAM显著优于现有方法，任务数量增加时优势更明显。

Conclusion: HAM框架能有效扩展，以更高效率处理更多任务，促进相关任务间的迁移学习。

Abstract: Continual learning is an essential capability of human cognition, yet it
poses significant challenges for current deep learning models. The primary
issue is that new knowledge can interfere with previously learned information,
causing the model to forget earlier knowledge in favor of the new, a phenomenon
known as catastrophic forgetting. Although large pre-trained models can
partially mitigate forgetting by leveraging their existing knowledge and
over-parameterization, they often struggle when confronted with novel data
distributions. Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA,
enable efficient adaptation to new knowledge. However, they still face
challenges in scaling to dynamic learning scenarios and long sequences of
tasks, as maintaining one adapter per task introduces complexity and increases
the potential for interference. In this paper, we introduce Hierarchical
Adapters Merging (HAM), a novel framework that dynamically combines adapters
from different tasks during training. This approach enables HAM to scale
effectively, allowing it to manage more tasks than competing baselines with
improved efficiency. To achieve this, HAM maintains a fixed set of groups that
hierarchically consolidate new adapters. For each task, HAM trains a low-rank
adapter along with an importance scalar, then dynamically groups tasks based on
adapter similarity. Within each group, adapters are pruned, scaled and merge,
facilitating transfer learning between related tasks. Extensive experiments on
three vision benchmarks show that HAM significantly outperforms
state-of-the-art methods, particularly as the number of tasks increases.

</details>


### [163] [Density-Aware Farthest Point Sampling](https://arxiv.org/abs/2509.13213)
*Paolo Climaco,Jochen Garcke*

Main category: cs.LG

TL;DR: 本文针对标注数据有限场景，提出DA - FPS采样方法，实验表明该方法能显著降低预测误差。


<details>
  <summary>Details</summary>
Motivation: 在标注训练数据因计算限制或标注成本高而有限的场景下，需从无标注数据中选合适训练集以平衡性能和效率。

Method: 聚焦仅考虑数据特征表示的被动且与模型无关的采样方法，推导Lipschitz连续回归模型预期预测误差上界，引入DA - FPS采样方法并证明其能近似最小化加权填充距离估计。

Result: 使用两个回归模型在三个数据集上实验，DA - FPS比其他采样策略显著降低平均绝对预测误差。

Conclusion: DA - FPS在从无标注数据中选择训练集以训练机器学习回归模型方面表现良好，能有效降低预测误差。

Abstract: We focus on training machine learning regression models in scenarios where
the availability of labeled training data is limited due to computational
constraints or high labeling costs. Thus, selecting suitable training sets from
unlabeled data is essential for balancing performance and efficiency. For the
selection of the training data, we focus on passive and model-agnostic sampling
methods that only consider the data feature representations. We derive an upper
bound for the expected prediction error of Lipschitz continuous regression
models that linearly depends on the weighted fill distance of the training set,
a quantity we can estimate simply by considering the data features. We
introduce "Density-Aware Farthest Point Sampling" (DA-FPS), a novel sampling
method. We prove that DA-FPS provides approximate minimizers for a data-driven
estimation of the weighted fill distance, thereby aiming at minimizing our
derived bound. We conduct experiments using two regression models across three
datasets. The results demonstrate that DA-FPS significantly reduces the mean
absolute prediction error compared to other sampling strategies.

</details>


### [164] [FOSSIL: Regret-minimizing weighting for robust learning under imbalance and small data](https://arxiv.org/abs/2509.13218)
*J. Cha,J. Lee,J. Cho,J. Shin*

Main category: cs.LG

TL;DR: 提出FOSSIL框架解决不平衡小数据问题，有理论保证且实验效果好。


<details>
  <summary>Details</summary>
Motivation: 不平衡和小数据场景普遍存在，现有解决方案有局限性。

Method: 引入FOSSIL统一加权框架，集成多种功能于一个可解释公式。

Result: 在合成和真实数据集上，相比基线方法有一致的实验增益。

Conclusion: FOSSIL框架无需架构改变，能有效解决不平衡小数据问题，有理论和实验优势。

Abstract: Imbalanced and small data regimes are pervasive in domains such as rare
disease imaging, genomics, and disaster response, where labeled samples are
scarce and naive augmentation often introduces artifacts. Existing solutions
such as oversampling, focal loss, or meta-weighting address isolated aspects of
this challenge but remain fragile or complex. We introduce FOSSIL (Flexible
Optimization via Sample Sensitive Importance Learning), a unified weighting
framework that seamlessly integrates class imbalance correction,
difficulty-aware curricula, augmentation penalties, and warmup dynamics into a
single interpretable formula. Unlike prior heuristics, the proposed framework
provides regret-based theoretical guarantees and achieves consistent empirical
gains over ERM, curriculum, and meta-weighting baselines on synthetic and
real-world datasets, while requiring no architectural changes.

</details>


### [165] [On the Out-of-Distribution Backdoor Attack for Federated Learning](https://arxiv.org/abs/2509.13219)
*Jiahao Xu,Zikai Zhang,Rui Hu*

Main category: cs.LG

TL;DR: 提出OOD后门攻击OBA及增强隐身性的SoDa，还提出防御方法BNGuard，实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习后门攻击场景受限，需解决其实用性问题，同时应对新攻击带来的安全漏洞。

Method: 提出OOD后门攻击OBA，用OOD数据作毒样本和触发器；提出SoDa正则化恶意本地模型；提出BNGuard利用批量归一化层运行统计偏差识别恶意更新。

Result: OBA能绕过现有防御并保持主任务高精度，BNGuard能有效防御SoDa。

Conclusion: OBA拓展了联邦学习后门攻击场景，SoDa增强了攻击隐身性，BNGuard提升了联邦学习的后门鲁棒性。

Abstract: Traditional backdoor attacks in federated learning (FL) operate within
constrained attack scenarios, as they depend on visible triggers and require
physical modifications to the target object, which limits their practicality.
To address this limitation, we introduce a novel backdoor attack prototype for
FL called the out-of-distribution (OOD) backdoor attack ($\mathtt{OBA}$), which
uses OOD data as both poisoned samples and triggers simultaneously. Our
approach significantly broadens the scope of backdoor attack scenarios in FL.
To improve the stealthiness of $\mathtt{OBA}$, we propose $\mathtt{SoDa}$,
which regularizes both the magnitude and direction of malicious local models
during local training, aligning them closely with their benign versions to
evade detection. Empirical results demonstrate that $\mathtt{OBA}$ effectively
circumvents state-of-the-art defenses while maintaining high accuracy on the
main task.
  To address this security vulnerability in the FL system, we introduce
$\mathtt{BNGuard}$, a new server-side defense method tailored against
$\mathtt{SoDa}$. $\mathtt{BNGuard}$ leverages the observation that OOD data
causes significant deviations in the running statistics of batch normalization
layers. This allows $\mathtt{BNGuard}$ to identify malicious model updates and
exclude them from aggregation, thereby enhancing the backdoor robustness of FL.
Extensive experiments across various settings show the effectiveness of
$\mathtt{BNGuard}$ on defending against $\mathtt{SoDa}$. The code is available
at https://github.com/JiiahaoXU/SoDa-BNGuard.

</details>


### [166] [Metacognitive Reuse: Turning Recurring LLM Reasoning Into Concise Behaviors](https://arxiv.org/abs/2509.13237)
*Aniket Didolkar,Nicolas Ballas,Sanjeev Arora,Anirudh Goyal*

Main category: cs.LG

TL;DR: 研究将重复推理片段转化为可复用‘行为’，存储在‘行为手册’中，通过不同方式提升大语言模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型解决多步问题时重复推导中间步骤，增加token使用和延迟，减少上下文窗口探索容量。

Method: 通过模型对先前推理痕迹的元认知分析，将重复推理片段转化为‘行为’，存储在‘行为手册’，在推理时上下文提供或通过监督微调融入参数。

Result: 在三种不同设置下提升推理能力，包括减少推理token、提高准确率、更有效地将非推理模型转化为推理模型。

Conclusion: 将缓慢推导转化为快速程序提示，使大语言模型记住推理方法而非仅结论。

Abstract: Large language models (LLMs) now solve multi-step problems by emitting
extended chains of thought. During the process, they often re-derive the same
intermediate steps across problems, inflating token usage and latency. This
saturation of the context window leaves less capacity for exploration. We study
a simple mechanism that converts recurring reasoning fragments into concise,
reusable "behaviors" (name + instruction) via the model's own metacognitive
analysis of prior traces. These behaviors are stored in a "behavior handbook"
which supplies them to the model in-context at inference or distills them into
parameters via supervised fine-tuning. This approach achieves improved
test-time reasoning across three different settings - 1) Behavior-conditioned
inference: Providing the LLM relevant behaviors in-context during reasoning
reduces number of reasoning tokens by up to 46% while matching or improving
baseline accuracy; 2) Behavior-guided self-improvement: Without any parameter
updates, the model improves its own future reasoning by leveraging behaviors
from its own past problem solving attempts. This yields up to 10% higher
accuracy than a naive critique-and-revise baseline; and 3) Behavior-conditioned
SFT: SFT on behavior-conditioned reasoning traces is more effective at
converting non-reasoning models into reasoning models as compared to vanilla
SFT. Together, these results indicate that turning slow derivations into fast
procedural hints enables LLMs to remember how to reason, not just what to
conclude.

</details>


### [167] [Don't Forget the Nonlinearity: Unlocking Activation Functions in Efficient Fine-Tuning](https://arxiv.org/abs/2509.13240)
*Bo Yin,Xingyi Yang,Xinchao Wang*

Main category: cs.LG

TL;DR: 提出NoRA框架直接调整预训练模型非线性激活函数，在视觉和语言任务表现优，证明激活空间调优是参数高效方法。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法主要调整权重矩阵，固定激活函数，需新方法直接调整激活函数。

Method: 用可学习有理函数替代固定激活函数，对分子分母系数进行结构化低秩更新，采用分组设计。

Result: 在视觉任务上以少量参数匹配或超越全量微调；NoRA++在训练预算相同时优于LoRA和DoRA；在LLaMA3 - 8B指令调优提升生成质量。

Conclusion: 激活空间调优是基于权重的参数高效微调的补充方法，激活函数可作为模型适配的一等对象。

Abstract: Existing parameter-efficient fine-tuning (PEFT) methods primarily adapt
weight matrices while keeping activation functions fixed. We introduce
\textbf{NoRA}, the first PEFT framework that directly adapts nonlinear
activation functions in pretrained transformer-based models. NoRA replaces
fixed activations with learnable rational functions and applies structured
low-rank updates to numerator and denominator coefficients, with a group-wise
design that localizes adaptation and improves stability at minimal cost. On
vision transformers trained on CIFAR-10 and CIFAR-100, NoRA matches or exceeds
full fine-tuning while updating only 0.4\% of parameters (0.02M), achieving
accuracy gains of +0.17\% and +0.27\%. When combined with LoRA
(\textbf{NoRA++}), it outperforms LoRA and DoRA under matched training budgets
by adding fewer trainable parameters. On LLaMA3-8B instruction tuning, NoRA++
consistently improves generation quality, yielding average MMLU gains of
+0.3\%--0.8\%, including +1.6\% on STEM (Alpaca) and +1.3\% on OpenOrca. We
further show that NoRA constrains adaptation to a low-dimensional functional
subspace, implicitly regularizing update magnitude and direction. These results
establish activation-space tuning as a complementary and highly
parameter-efficient alternative to weight-based PEFT, positioning activation
functions as first-class objects for model adaptation.

</details>


### [168] [Post-Hoc Split-Point Self-Consistency Verification for Efficient, Unified Quantification of Aleatoric and Epistemic Uncertainty in Deep Learning](https://arxiv.org/abs/2509.13262)
*Zhizhong Zhao,Ke Chen*

Main category: cs.LG

TL;DR: 提出后验单前向传播框架联合捕获不确定性，不修改或重训预训练模型，在多种基准测试中表现好。


<details>
  <summary>Details</summary>
Motivation: 现有不确定性量化方法要么计算密集，要么只提供部分特定任务估计，需要更好的方法。

Method: 应用Split - Point Analysis分解预测残差，计算Mean Absolute Residuals，定义Self - consistency Discrepancy Score，对回归和分类分别采用不同处理方式。

Result: 在多种回归和分类基准测试中，该框架与多个先进的不确定性量化方法表现相当或更优，且开销极小。

Conclusion: 提出的后验单前向传播框架是一种有效的不确定性量化方法。

Abstract: Uncertainty quantification (UQ) is vital for trustworthy deep learning, yet
existing methods are either computationally intensive, such as Bayesian or
ensemble methods, or provide only partial, task-specific estimates, such as
single-forward-pass techniques. In this paper, we propose a post-hoc
single-forward-pass framework that jointly captures aleatoric and epistemic
uncertainty without modifying or retraining pretrained models. Our method
applies \emph{Split-Point Analysis} (SPA) to decompose predictive residuals
into upper and lower subsets, computing \emph{Mean Absolute Residuals} (MARs)
on each side. We prove that, under ideal conditions, the total MAR equals the
harmonic mean of subset MARs; deviations define a novel \emph{Self-consistency
Discrepancy Score} (SDS) for fine-grained epistemic estimation across
regression and classification. For regression, side-specific quantile
regression yields prediction intervals with improved empirical coverage, which
are further calibrated via SDS. For classification, when calibration data are
available, we apply SPA-based calibration identities to adjust the softmax
outputs and then compute predictive entropy on these calibrated probabilities.
Extensive experiments on diverse regression and classification benchmarks
demonstrate that our framework matches or exceeds several state-of-the-art UQ
methods while incurring minimal overhead.
  Our source code is available at https://github.com/zzz0527/SPC-UQ.

</details>


### [169] [JANUS: A Dual-Constraint Generative Framework for Stealthy Node Injection Attacks](https://arxiv.org/abs/2509.13266)
*Jiahao Zhang,Xiaobing Pei,Zhaokun Zhong,Wenqiang Hao,Zhenghao Tang*

Main category: cs.LG

TL;DR: 提出JANUS框架应对图神经网络节点注入攻击，在多数据集实验中攻击效果和隐蔽性超现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络节点注入攻击方法在实现隐蔽性时存在依赖间接代理指标、忽略注入内容特征、局部近视等问题。

Method: 提出JANUS框架，在局部采用特征流形对齐策略，全局结合结构化潜变量并最大化互信息，将攻击建模为序列决策过程，用强化学习优化。

Result: 在多个标准数据集实验中，JANUS框架在攻击效果和隐蔽性上显著优于现有方法。

Conclusion: JANUS框架能有效克服现有节点注入攻击方法的局限，提高攻击效果和隐蔽性。

Abstract: Graph Neural Networks (GNNs) have demonstrated remarkable performance across
various applications, yet they are vulnerable to sophisticated adversarial
attacks, particularly node injection attacks. The success of such attacks
heavily relies on their stealthiness, the ability to blend in with the original
graph and evade detection. However, existing methods often achieve stealthiness
by relying on indirect proxy metrics, lacking consideration for the fundamental
characteristics of the injected content, or focusing only on imitating local
structures, which leads to the problem of local myopia. To overcome these
limitations, we propose a dual-constraint stealthy node injection framework,
called Joint Alignment of Nodal and Universal Structures (JANUS). At the local
level, we introduce a local feature manifold alignment strategy to achieve
geometric consistency in the feature space. At the global level, we incorporate
structured latent variables and maximize the mutual information with the
generated structures, ensuring the injected structures are consistent with the
semantic patterns of the original graph. We model the injection attack as a
sequential decision process, which is optimized by a reinforcement learning
agent. Experiments on multiple standard datasets demonstrate that the JANUS
framework significantly outperforms existing methods in terms of both attack
effectiveness and stealthiness.

</details>


### [170] [LLMs for energy and macronutrients estimation using only text data from 24-hour dietary recalls: a parameter-efficient fine-tuning experiment using a 10-shot prompt](https://arxiv.org/abs/2509.13268)
*Rodrigo M Carrillo-Larco*

Main category: cs.LG

TL;DR: 研究探索开源大语言模型仅基于文本描述预测食物营养值，普通模型预测差，微调后模型表现好，该方法有望用于低负担饮食监测。


<details>
  <summary>Details</summary>
Motivation: 大多数人工智能营养估算工具依赖图像输入，研究大语言模型能否仅依据食物文本描述准确预测营养值，若有效可简化饮食监测。

Method: 使用美国国家健康与营养检查调查中12 - 19岁青少年24小时饮食回忆数据，用10次思维链方法提示开源量化大语言模型仅根据食物和数量文本串估算能量和五种宏量营养素，应用参数高效微调评估预测准确性，以NHANES计算值为真实值。

Result: 在11281名青少年数据集中，普通模型预测差，能量平均绝对误差高且Lin's CCC低；微调后模型表现大幅提升，能量平均绝对误差降低且Lin's CCC超0.89。

Conclusion: 使用思维链方法提示并经参数高效微调的开源大语言模型仅靠文本输入能准确预测24小时饮食回忆中的能量和宏量营养素值，有望用于低负担文本饮食监测工具。

Abstract: BACKGROUND: Most artificial intelligence tools used to estimate nutritional
content rely on image input. However, whether large language models (LLMs) can
accurately predict nutritional values based solely on text descriptions of
foods consumed remains unknown. If effective, this approach could enable
simpler dietary monitoring without the need for photographs. METHODS: We used
24-hour dietary recalls from adolescents aged 12-19 years in the National
Health and Nutrition Examination Survey (NHANES). An open-source quantized LLM
was prompted using a 10-shot, chain-of-thought approach to estimate energy and
five macronutrients based solely on text strings listing foods and their
quantities. We then applied parameter-efficient fine-tuning (PEFT) to evaluate
whether predictive accuracy improved. NHANES-calculated values served as the
ground truth for energy, proteins, carbohydrates, total sugar, dietary fiber
and total fat. RESULTS: In a pooled dataset of 11,281 adolescents (49.9% male,
mean age 15.4 years), the vanilla LLM yielded poor predictions. The mean
absolute error (MAE) was 652.08 for energy and the Lin's CCC <0.46 across
endpoints. In contrast, the fine-tuned model performed substantially better,
with energy MAEs ranging from 171.34 to 190.90 across subsets, and Lin's CCC
exceeding 0.89 for all outcomes. CONCLUSIONS: When prompted using a
chain-of-thought approach and fine-tuned with PEFT, open-source LLMs exposed
solely to text input can accurately predict energy and macronutrient values
from 24-hour dietary recalls. This approach holds promise for low-burden,
text-based dietary monitoring tools.

</details>


### [171] [WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning](https://arxiv.org/abs/2509.13305)
*Kuan Li,Zhongwang Zhang,Huifeng Yin,Rui Ye,Yida Zhao,Liwen Zhang,Litu Ou,Dingchu Zhang,Xixi Wu,Jialong Wu,Xinyu Wang,Zile Qiao,Zhen Zhang,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.LG

TL;DR: 提出WebSailor方法提升大语言模型在复杂信息检索任务中的能力，缩小与专有模型差距。


<details>
  <summary>Details</summary>
Motivation: 超越人类认知局限是大语言模型训练的关键前沿，专有模型在复杂信息检索任务表现出色，开源模型欠缺相应的推理模式。

Method: 提出WebSailor，通过结构化采样和信息混淆生成高不确定性任务，采用RFT冷启动和高效的代理强化学习训练算法DUPO。

Result: WebSailor在复杂信息检索任务中显著超越所有开源代理，达到专有代理的性能。

Conclusion: WebSailor方法能有效赋予开源模型在复杂信息检索中减少极端不确定性的能力，缩小与专有模型的能力差距。

Abstract: Transcending human cognitive limitations represents a critical frontier in
LLM training. Proprietary agentic systems like DeepResearch have demonstrated
superhuman capabilities on extremely complex information-seeking benchmarks
such as BrowseComp, a feat previously unattainable. We posit that their success
hinges on a sophisticated reasoning pattern absent in open-source models: the
ability to systematically reduce extreme uncertainty when navigating vast
information landscapes. Based on this insight, we introduce WebSailor, a
complete post-training methodology designed to instill this crucial capability.
Our approach involves generating novel, high-uncertainty tasks through
structured sampling and information obfuscation, RFT cold start, and an
efficient agentic RL training algorithm, Duplicating Sampling Policy
Optimization (DUPO). With this integrated pipeline, WebSailor significantly
outperforms all open-source agents in complex information-seeking tasks,
matching proprietary agents' performance and closing the capability gap.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [172] [Efficient lattice field theory simulation using adaptive normalizing flow on a resistive memory-based neural differential equation solver](https://arxiv.org/abs/2509.12812)
*Meng Xu,Jichang Yang,Ning Lin,Qundao Xu,Siqi Tang,Han Wang,Xiaojuan Qi,Zhongrui Wang,Ming Xu*

Main category: cs.NE

TL;DR: 提出软硬件协同设计，集成自适应归一化流模型与基于电阻式内存的神经微分方程求解器，用于高效生成晶格场论配置，在计算成本、速度和能效上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统晶格场论模拟方法在高维系统应用中面临计算成本高、并行性有限、训练费用大、硬件能效低等挑战。

Method: 提出软硬件协同设计，软件上用自适应归一化流（ANF）模型和低秩自适应（LoRA），硬件上采用电阻式内存的内存计算。

Result: 在标量phi4理论和石墨烯线有效场论验证，相比混合蒙特卡罗（HMC）减少了综合自相关时间，相比GPU有速度和能效提升。

Conclusion: 该软硬件协同设计实现了低成本计算，在计算速度和能效方面表现出色。

Abstract: Lattice field theory (LFT) simulations underpin advances in classical
statistical mechanics and quantum field theory, providing a unified
computational framework across particle, nuclear, and condensed matter physics.
However, the application of these methods to high-dimensional systems remains
severely constrained by several challenges, including the prohibitive
computational cost and limited parallelizability of conventional sampling
algorithms such as hybrid Monte Carlo (HMC), the substantial training expense
associated with traditional normalizing flow models, and the inherent energy
inefficiency of digital hardware architectures. Here, we introduce a
software-hardware co-design that integrates an adaptive normalizing flow (ANF)
model with a resistive memory-based neural differential equation solver,
enabling efficient generation of LFT configurations. Software-wise, ANF enables
efficient parallel generation of statistically independent configurations,
thereby reducing computational costs, while low-rank adaptation (LoRA) allows
cost-effective fine-tuning across diverse simulation parameters. Hardware-wise,
in-memory computing with resistive memory substantially enhances both
parallelism and energy efficiency. We validate our approach on the scalar phi4
theory and the effective field theory of graphene wires, using a hybrid
analog-digital neural differential equation solver equipped with a 180 nm
resistive memory in-memory computing macro. Our co-design enables low-cost
computation, achieving approximately 8.2-fold and 13.9-fold reductions in
integrated autocorrelation time over HMC, while requiring fine-tuning of less
than 8% of the weights via LoRA. Compared to state-of-the-art GPUs, our
co-design achieves up to approximately 16.1- and 17.0-fold speedups for the two
tasks, as well as 73.7- and 138.0-fold improvements in energy efficiency.

</details>


### [173] [A Neuromorphic Model of Learning Meaningful Sequences with Long-Term Memory](https://arxiv.org/abs/2509.12850)
*Laxmi R. Iyer,Ali A. Minai*

Main category: cs.NE

TL;DR: 研究熟悉情境下新序列学习，用HTM模型结合SWOW - EN数据集模拟长期记忆，发现学习有意义句子比随机词快且系统对噪声更具耐受性。


<details>
  <summary>Details</summary>
Motivation: 探究人类能快速理解有意义句子学习机制，研究熟悉情境下新序列学习。

Method: 将SWOW - EN数据集嵌入基于HTM模型的脉冲神经网络模拟长期记忆。

Result: 有SWOW - EN时，学习有意义句子和随机噪声速度有明显差异，如短诗比随机词序列学得快，且初始化为SWOW - EN权重的系统对噪声耐受性更强。

Conclusion: 有意义句子的学习速度明显快于随机词序列，且结合SWOW - EN的系统对噪声有更好的耐受性。

Abstract: Learning meaningful sentences is different from learning a random set of
words. When humans understand the meaning, the learning occurs relatively
quickly. What mechanisms enable this to happen? In this paper, we examine the
learning of novel sequences in familiar situations. We embed the Small World of
Words (SWOW-EN), a Word Association Norms (WAN) dataset, in a spiking neural
network based on the Hierarchical Temporal Memory (HTM) model to simulate
long-term memory. Results show that in the presence of SWOW-EN, there is a
clear difference in speed between the learning of meaningful sentences and
random noise. For example, short poems are learned much faster than sequences
of random words. In addition, the system initialized with SWOW-EN weights shows
greater tolerance to noise.

</details>


### [174] [Large Language Model-assisted Meta-optimizer for Automated Design of Constrained Evolutionary Algorithm](https://arxiv.org/abs/2509.13251)
*Xu Yang,Rui Wang,Kaiwen Li,Wenhua Li,Weixiong Huang*

Main category: cs.NE

TL;DR: 本文提出AwesomeDE，利用大语言模型作为元优化器为约束进化算法生成更新规则，引入$RTO^2H$框架规范提示设计，实验表明该方法在计算效率和求解精度上优于现有方法且泛化性好。


<details>
  <summary>Details</summary>
Motivation: 在约束进化优化领域，利用大语言模型推进元黑盒优化。

Method: 提出AwesomeDE，以大语言模型作为元优化器生成更新规则，引入$RTO^2H$框架规范提示设计，并在多样的约束优化问题上训练元优化器，系统分析关键组件影响。

Result: 实验显示提出的方法在计算效率和求解精度上优于现有方法，且在不同问题领域泛化性良好。

Conclusion: 本研究为自动约束算法设计提供可扩展和数据驱动的方法，同时指出局限性和未来研究方向。

Abstract: Meta-black-box optimization has been significantly advanced through the use
of large language models (LLMs), yet in fancy on constrained evolutionary
optimization. In this work, AwesomeDE is proposed that leverages LLMs as the
strategy of meta-optimizer to generate update rules for constrained
evolutionary algorithm without human intervention. On the meanwhile, $RTO^2H$
framework is introduced for standardize prompt design of LLMs. The
meta-optimizer is trained on a diverse set of constrained optimization
problems. Key components, including prompt design and iterative refinement, are
systematically analyzed to determine their impact on design quality.
Experimental results demonstrate that the proposed approach outperforms
existing methods in terms of computational efficiency and solution accuracy.
Furthermore, AwesomeDE is shown to generalize well across distinct problem
domains, suggesting its potential for broad applicability. This research
contributes to the field by providing a scalable and data-driven methodology
for automated constrained algorithm design, while also highlighting limitations
and directions for future work.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [175] [Evaluating Large Language Models for Functional and Maintainable Code in Industrial Settings: A Case Study at ASML](https://arxiv.org/abs/2509.12395)
*Yash Mundhra,Max Valk,Maliheh Izadi*

Main category: cs.SE

TL;DR: 研究大语言模型在ASML专有软件环境中生成代码的表现，开发评估框架和新基准、指标，研究提示技术等，发现提示技术和模型大小影响输出质量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在开源领域表现出色，但在专有工业环境的适用性待探索，故研究其在ASML封闭专业软件环境中生成代码的能力。

Method: 开发针对ASML专有代码库的评估框架和新基准，提出新评估指标build@k，研究多种提示技术，对比通用和代码特定大语言模型，考察模型大小对代码生成能力的影响。

Result: 提示技术和模型大小对输出质量影响显著，少样本和思维链提示的构建成功率最高；代码特定和通用大语言模型性能差异不明显且因模型家族而异。

Conclusion: 提示技术和模型大小影响大语言模型在专有工业环境中生成代码的质量。

Abstract: Large language models have shown impressive performance in various domains,
including code generation across diverse open-source domains. However, their
applicability in proprietary industrial settings, where domain-specific
constraints and code interdependencies are prevalent, remains largely
unexplored. We present a case study conducted in collaboration with the
leveling department at ASML to investigate the performance of LLMs in
generating functional, maintainable code within a closed, highly specialized
software environment.
  We developed an evaluation framework tailored to ASML's proprietary codebase
and introduced a new benchmark. Additionally, we proposed a new evaluation
metric, build@k, to assess whether LLM-generated code successfully compiles and
integrates within real industrial repositories. We investigate various
prompting techniques, compare the performance of generic and code-specific
LLMs, and examine the impact of model size on code generation capabilities,
using both match-based and execution-based metrics. The findings reveal that
prompting techniques and model size have a significant impact on output
quality, with few-shot and chain-of-thought prompting yielding the highest
build success rates. The difference in performance between the code-specific
LLMs and generic LLMs was less pronounced and varied substantially across
different model families.

</details>


### [176] [Understanding Prompt Management in GitHub Repositories: A Call for Best Practices](https://arxiv.org/abs/2509.12421)
*Hao Li,Hicham Masri,Filipe R. Cogo,Abdul Ali Bangash,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 对92个GitHub仓库的24800个开源提示进行实证分析，发现提示管理问题并给出建议。


<details>
  <summary>Details</summary>
Motivation: 基础模型推动提示软件发展，有效管理提示至关重要但具有挑战性。

Method: 对92个GitHub仓库的24800个开源提示进行实证分析。

Result: 发现提示格式不一致、提示重复、可读性和拼写问题等挑战。

Conclusion: 为开发者提供增强开源提示可用性和可维护性的可行建议。

Abstract: The rapid adoption of foundation models (e.g., large language models) has
given rise to promptware, i.e., software built using natural language prompts.
Effective management of prompts, such as organization and quality assurance, is
essential yet challenging. In this study, we perform an empirical analysis of
24,800 open-source prompts from 92 GitHub repositories to investigate prompt
management practices and quality attributes. Our findings reveal critical
challenges such as considerable inconsistencies in prompt formatting,
substantial internal and external prompt duplication, and frequent readability
and spelling issues. Based on these findings, we provide actionable
recommendations for developers to enhance the usability and maintainability of
open-source prompts within the rapidly evolving promptware ecosystem.

</details>


### [177] [From Legacy Fortran to Portable Kokkos:An Autonomous Agentic AI Workflow](https://arxiv.org/abs/2509.12443)
*Sparsh Gupta,Kamalavasan Kamalakkannan,Maxim Moraru,Galen Shipman,Patrick Diehl*

Main category: cs.SE

TL;DR: 本文提出代理AI工作流将Fortran内核转换为可移植Kokkos C++程序，证明其可行性，展示LLM驱动系统潜力。


<details>
  <summary>Details</summary>
Motivation: HPC向异构GPU加速架构转变，加速器缺乏原生Fortran绑定，手动Fortran到Kokkos移植困难，LLM在并行代码转换优化应用待探索。

Method: 使用专门的LLM“代理”协作完成Fortran内核到Kokkos C++程序的翻译、验证、编译等工作。

Result: 该工作流能现代化一系列基准内核，生成跨硬件分区的性能可移植Kokkos代码，付费模型生成代码超Fortran基线，开源模型常失败。

Conclusion: 证明代理AI用于Fortran到Kokkos转换可行，为现代化遗留科学应用提供途径，凸显LLM驱动系统在特定领域推理任务潜力。

Abstract: Scientific applications continue to rely on legacy Fortran codebases
originally developed for homogeneous, CPU-based systems. As High-Performance
Computing (HPC) shifts toward heterogeneous GPU-accelerated architectures, many
accelerators lack native Fortran bindings, creating an urgent need to modernize
legacy codes for portability. Frameworks like Kokkos provide performance
portability and a single-source C++ abstraction, but manual Fortran-to-Kokkos
porting demands significant expertise and time. Large language models (LLMs)
have shown promise in source-to-source code generation, yet their use in fully
autonomous workflows for translating and optimizing parallel code remains
largely unexplored, especially for performance portability across diverse
hardware.
  This paper presents an agentic AI workflow where specialized LLM "agents"
collaborate to translate, validate, compile, run, test, debug, and optimize
Fortran kernels into portable Kokkos C++ programs. Results show the pipeline
modernizes a range of benchmark kernels, producing performance-portable Kokkos
codes across hardware partitions. Paid OpenAI models such as GPT-5 and
o4-mini-high executed the workflow for only a few U.S. dollars, generating
optimized codes that surpassed Fortran baselines, whereas open-source models
like Llama4-Maverick often failed to yield functional codes.
  This work demonstrates the feasibility of agentic AI for Fortran-to-Kokkos
transformation and offers a pathway for autonomously modernizing legacy
scientific applications to run portably and efficiently on diverse
supercomputers. It further highlights the potential of LLM-driven agentic
systems to perform structured, domain-specific reasoning tasks in scientific
and systems-oriented applications.

</details>


### [178] [Perspectives, Needs and Challenges for Sustainable Software Engineering Teams: A FinServ Case Study](https://arxiv.org/abs/2509.12466)
*Satwik Ghanta,Peggy Gregory,Gul Calikli*

Main category: cs.SE

TL;DR: 研究聚焦金融服务公司的可持续软件工程项目，发现组织层与开发者对可持续性认知有差异，强调需因地制宜、共同设计干预措施。


<details>
  <summary>Details</summary>
Motivation: 当前对特定组织（如金融服务）的可持续性需求理解不足，为填补该空白开展研究。

Method: 对一家金融服务公司进行探索性定性案例研究，通过访谈和焦点小组与高层管理人员和软件工程师交流。

Result: 组织层和开发者对可持续性认知有明显差异，开发者对组织倡议存怀疑，很多人希望有专门的可持续性团队。

Conclusion: 组织目标与开发者需求的脱节凸显了因地制宜、共同设计干预措施的重要性。

Abstract: Sustainable Software Engineering (SSE) is slowly becoming an industry need
for reasons including reputation enhancement, improved profits and more
efficient practices. However, SSE has many definitions, and this is a challenge
for organisations trying to build a common and broadly agreed understanding of
the term. Although much research effort has gone into identifying general SSE
practices, there is a gap in understanding the sustainability needs of specific
organisational contexts, such as financial services, which are highly
data-driven, operate under strict regulatory requirements, and handle millions
of transactions day to day. To address this gap, our research focuses on a
financial services company (FinServCo) that invited us to investigate
perceptions of sustainability in their IT function: how it could be put into
practice, who is responsible for it, and what the challenges are. We conducted
an exploratory qualitative case study using interviews and a focus group with
six higher management employees and 16 software engineers comprising various
experience levels from junior developers to team leaders. Our study found a
clear divergence in how sustainability is perceived between organisational
levels. Higher management emphasised technical and economic sustainability,
focusing on cloud migration and business continuity through data availability.
In contrast, developers highlighted human-centric concerns such as workload
management and stress reduction. Scepticism toward organisational initiatives
was also evident, with some developers viewing them as a PR strategy. Many
participants expressed a preference for a dedicated sustainability team,
drawing analogies to internal structures for security governance. The
disconnect between organisational goals and individual developer needs
highlights the importance of context-sensitive, co-designed interventions.

</details>


### [179] [Good Vibrations? A Qualitative Study of Co-Creation, Communication, Flow, and Trust in Vibe Coding](https://arxiv.org/abs/2509.12491)
*Veronica Pimenova,Sarah Fakhoury,Christian Bird,Margaret-Anne Storey,Madeline Endres*

Main category: cs.SE

TL;DR: 本文对vibe coding进行了首次系统定性研究，提出基于对话交互等的理论，发现AI信任的作用、痛点与风险并给出最佳实践，最后给出对AI开发工具未来及研究方向的启示。


<details>
  <summary>Details</summary>
Motivation: 现有研究多分析代码工件或提出缺乏实证支持的理论，需要深入理解开发者对vibe coding的感知和体验。

Method: 收集半结构化访谈、Reddit线程和LinkedIn帖子中的超190,000字内容进行定性研究。

Result: 提出基于对话交互、共创和开发者心流与愉悦感的vibe coding理论，发现AI信任的作用，找出痛点和风险并给出最佳实践。

Conclusion: 对AI开发工具未来和vibe coding研究方向有一定启示。

Abstract: Vibe coding, a term coined by Andrej Karpathy in February 2025, has quickly
become a compelling and controversial natural language programming paradigm in
AI-assisted software development. Centered on iterative co-design with an AI
assistant, vibe coding emphasizes flow and experimentation over strict upfront
specification. While initial studies have begun to explore this paradigm, most
focus on analyzing code artifacts or proposing theories with limited empirical
backing. There remains a need for a grounded understanding of vibe coding as it
is perceived and experienced by developers. We present the first systematic
qualitative investigation of vibe coding perceptions and practice. Drawing on
over 190,000 words from semi-structured interviews, Reddit threads, and
LinkedIn posts, we characterize what vibe coding is, why and how developers use
it, where it breaks down, and which emerging practices aim to support it. We
propose a qualitatively grounded theory of vibe coding centered on
conversational interaction with AI, co-creation, and developer flow and joy. We
find that AI trust regulates movement along a continuum from delegation to
co-creation and supports the developer experience by sustaining flow. We
surface recurring pain points and risks in areas including specification,
reliability, debugging, latency, code review burden, and collaboration. We also
present best practices that have been discovered and shared to mitigate these
challenges. We conclude with implications for the future of AI dev tools and
directions for researchers investigating vibe coding.

</details>


### [180] [Ensembling Large Language Models for Code Vulnerability Detection: An Empirical Evaluation](https://arxiv.org/abs/2509.12629)
*Zhihong Sun,Jia Li,Yao Wan,Chuanyi Li,Hongyu Zhang,Zhi jin,Ge Li,Hong Liu,Chen Lyu,Songlin Hu*

Main category: cs.SE

TL;DR: 本文探索集成学习提升大语言模型在源代码漏洞检测中的性能，通过多模型、多策略实验，证明集成方法有效，提出的DGS表现出色。


<details>
  <summary>Details</summary>
Motivation: 不同大语言模型在代码漏洞检测结果存在差异，可利用模型间潜在互补性，通过集成学习创建更强大的漏洞检测系统。

Method: 对五个大语言模型采用Bagging、Boosting和Stacking三种集成策略，在三个数据集上实验，并提出针对漏洞检测的Dynamic Gated Stacking (DGS)。

Result: 集成方法能显著提升检测性能，Boosting在不平衡数据集场景表现出色，DGS在处理类别不平衡和多分类任务上优于传统Stacking。

Conclusion: 通过集成学习可构建更可靠有效的基于大语言模型的漏洞检测系统。

Abstract: Code vulnerability detection is crucial for ensuring the security and
reliability of modern software systems. Recently, Large Language Models (LLMs)
have shown promising capabilities in this domain. However, notable
discrepancies in detection results often arise when analyzing identical code
segments across different training stages of the same model or among
architecturally distinct LLMs. While such inconsistencies may compromise
detection stability, they also highlight a key opportunity: the latent
complementarity among models can be harnessed through ensemble learning to
create more robust vulnerability detection systems. In this study, we explore
the potential of ensemble learning to enhance the performance of LLMs in source
code vulnerability detection. We conduct comprehensive experiments involving
five LLMs (i.e., DeepSeek-Coder-6.7B, CodeLlama-7B, CodeLlama-13B,
CodeQwen1.5-7B, and StarCoder2-15B), using three ensemble strategies (i.e.,
Bagging, Boosting, and Stacking). These experiments are carried out across
three widely adopted datasets (i.e., Devign, ReVeal, and BigVul). Inspired by
Mixture of Experts (MoE) techniques, we further propose Dynamic Gated Stacking
(DGS), a Stacking variant tailored for vulnerability detection. Our results
demonstrate that ensemble approaches can significantly improve detection
performance, with Boosting excelling in scenarios involving imbalanced
datasets. Moreover, DGS consistently outperforms traditional Stacking,
particularly in handling class imbalance and multi-class classification tasks.
These findings offer valuable insights into building more reliable and
effective LLM-based vulnerability detection systems through ensemble learning.

</details>


### [181] [When Large Language Models Meet UAVs: How Far Are We?](https://arxiv.org/abs/2509.12795)
*Yihua Chen,Xingle Que,Jiashuo Zhang,Ting Chen,Guangshun Li,Jiachi Chen*

Main category: cs.SE

TL;DR: 本文对UAV与LLM集成研究进行实证分析，对比学术研究与工业项目差异，找出集成阻碍因素并给出建议。


<details>
  <summary>Details</summary>
Motivation: 现有UAV与LLM集成研究多处于初步探索，可能使学术研究与实际需求脱节，需解决潜在挑战。

Method: 对74篇论文和56个GitHub项目进行实证研究，识别任务类型并量化分布，发放在线问卷。

Result: 学术研究重理论建模和任务优化，工业项目重飞行控制等，40.4%从业者尝试应用LLM，找出阻碍集成的因素。

Conclusion: 指出未来发展挑战并提供了建议。

Abstract: The integration of unmanned aerial vehicles (UAVs) and large language models
(LLMs) has emerged as a research direction of growing interest, with the
potential to address challenges in autonomous decision-making, human-UAV
interaction, and real-time adaptability. However, existing studies have
remained largely in preliminary exploration with a limited understanding of
real-world practice, risking a misalignment between academic research and
practical needs and hindering the translation of results. To examine and
address these potential challenges, we conducted an empirical study of 74
selected papers and 56 public GitHub projects, identified nine task types for
LLMs in UAV systems, and quantified their distribution. Our findings show that
academic research emphasizes theoretical modeling and task optimization with
dispersed attention across tasks. In contrast, industrial projects focus on
flight control, task planning, and human-machine interaction, prioritizing
operability and efficiency. To further capture industry perspectives, we
distributed an online questionnaire. We obtained 52 valid responses: 40.4% of
practitioners have attempted to apply LLMs to UAV tasks. We further identify
factors that impede real-world integration, including technological maturity,
performance, safety, cost, and other considerations. Finally, we highlight
challenges for future development and provide recommendations.

</details>


### [182] [LLM-Based Approach for Enhancing Maintainability of Automotive Architectures](https://arxiv.org/abs/2509.12798)
*Nenad Petrovic,Lukasz Mazur,Alois Knoll*

Main category: cs.SE

TL;DR: 本文探讨大语言模型在提高汽车系统灵活性任务自动化方面的潜力，并给出三个早期研究案例及基于GPT - 4o的概念验证实现。


<details>
  <summary>Details</summary>
Motivation: 汽车系统存在诸多瓶颈降低了其灵活性，长期维护、更新和扩展困难，需要探索提高灵活性的方法。

Method: 以OpenAI的GPT - 4o模型进行概念验证实现，研究三个案例（更新、硬件抽象和合规性；接口兼容性检查；架构修改建议）。

Result: 开展了三个案例的早期研究，但未提及具体结果。

Conclusion: 未在摘要中提及明确结论。

Abstract: There are many bottlenecks that decrease the flexibility of automotive
systems, making their long-term maintenance, as well as updates and extensions
in later lifecycle phases increasingly difficult, mainly due to long
re-engineering, standardization, and compliance procedures, as well as
heterogeneity and numerosity of devices and underlying software components
involved. In this paper, we explore the potential of Large Language Models
(LLMs) when it comes to the automation of tasks and processes that aim to
increase the flexibility of automotive systems. Three case studies towards
achieving this goal are considered as outcomes of early-stage research: 1)
updates, hardware abstraction, and compliance, 2) interface compatibility
checking, and 3) architecture modification suggestions. For proof-of-concept
implementation, we rely on OpenAI's GPT-4o model.

</details>


### [183] [SateLight: A Satellite Application Update Framework for Satellite Computing](https://arxiv.org/abs/2509.12809)
*Jinfeng Wen,Jianshu Zhao,Zixi Zhu,Xiaomin Zhang,Qi Liang,Ao Zhou,Shangguang Wang*

Main category: cs.SE

TL;DR: 提出SateLight卫星应用更新框架，实验表明可降低传输延迟、保证更新正确性，案例证明其实用性。


<details>
  <summary>Details</summary>
Motivation: 卫星计算中应用软件更新因应用异构、带宽有限和空间条件恶劣面临挑战，现有陆地系统更新方法不适用。

Method: 利用容器化封装异构应用，集成内容感知差分策略、细粒度星载更新设计和基于层的容错恢复机制。

Result: 在卫星模拟环境中，SateLight相比基线最多降低91.18%传输延迟（平均56.54%），所有评估应用更新正确性达100%，真实在轨卫星案例证明其实用性。

Conclusion: SateLight是适用于卫星计算的实用有效卫星应用更新框架。

Abstract: Satellite computing is an emerging paradigm that empowers satellites to
perform onboard processing tasks (i.e., \textit{satellite applications}),
thereby reducing reliance on ground-based systems and improving responsiveness.
However, enabling application software updates in this context remains a
fundamental challenge due to application heterogeneity, limited
ground-to-satellite bandwidth, and harsh space conditions. Existing software
update approaches, designed primarily for terrestrial systems, fail to address
these constraints, as they assume abundant computational capacity and stable
connectivity.
  To address this gap, we propose SateLight, a practical and effective
satellite application update framework tailored for satellite computing.
SateLight leverages containerization to encapsulate heterogeneous applications,
enabling efficient deployment and maintenance. SateLight further integrates
three capabilities: (1) a content-aware differential strategy that minimizes
communication data volume, (2) a fine-grained onboard update design that
reconstructs target applications, and (3) a layer-based fault-tolerant recovery
mechanism to ensure reliability under failure-prone space conditions.
Experimental results on a satellite simulation environment with 10
representative satellite applications demonstrate that SateLight reduces
transmission latency by up to 91.18% (average 56.54%) compared to the best
currently available baseline. It also consistently ensures 100% update
correctness across all evaluated applications. Furthermore, a case study on a
real-world in-orbit satellite demonstrates the practicality of our approach.

</details>


### [184] [Evaluating Large Language Models for Code Translation: Effects of Prompt Language and Prompt Design](https://arxiv.org/abs/2509.12973)
*Aamer Aljagthami,Mohammed Banabila,Musab Alshehri,Mohammed Kabini,Mohammad D. Alahmadi*

Main category: cs.SE

TL;DR: 研究对多种编程语言代码翻译的大语言模型进行评估，发现详细提示和英文提示更优，各LLM均优于传统基线。


<details>
  <summary>Details</summary>
Motivation: 现有关于模型选择、提示设计和提示语言对多编程语言代码翻译质量影响的比较证据有限，需进行系统评估。

Method: 对C++、Java、Python和C#进行代码翻译评估，采用BLEU和CodeBLEU指标，在两种提示风格和两种提示语言下进行方向感知评估。

Result: 详细提示在各模型和翻译方向上有稳定提升，英文提示比阿拉伯文提示性能高13 - 15%，各LLM均优于TransCoder。

Conclusion: 仔细的提示工程和提示语言选择有价值，为软件现代化和跨语言互操作性提供实用指导。

Abstract: Large language models (LLMs) have shown promise for automated source-code
translation, a capability critical to software migration, maintenance, and
interoperability. Yet comparative evidence on how model choice, prompt design,
and prompt language shape translation quality across multiple programming
languages remains limited. This study conducts a systematic empirical
assessment of state-of-the-art LLMs for code translation among C++, Java,
Python, and C#, alongside a traditional baseline (TransCoder). Using BLEU and
CodeBLEU, we quantify syntactic fidelity and structural correctness under two
prompt styles (concise instruction and detailed specification) and two prompt
languages (English and Arabic), with direction-aware evaluation across language
pairs. Experiments show that detailed prompts deliver consistent gains across
models and translation directions, and English prompts outperform Arabic by
13-15%. The top-performing model attains the highest CodeBLEU on challenging
pairs such as Java to C# and Python to C++. Our evaluation shows that each LLM
outperforms TransCoder across the benchmark. These results demonstrate the
value of careful prompt engineering and prompt language choice, and provide
practical guidance for software modernization and cross-language
interoperability.

</details>


### [185] [Validating Solidity Code Defects using Symbolic and Concrete Execution powered by Large Language Models](https://arxiv.org/abs/2509.13023)
*Ştefan-Claudiu Susan,Andrei Arusoaie,Dorel Lucanu*

Main category: cs.SE

TL;DR: 本文提出新检测管道，集成多种工具检测Solidity智能合约缺陷，实验有成果，虽有局限但建立了可靠框架。


<details>
  <summary>Details</summary>
Motivation: 静态分析工具和大语言模型检测智能合约漏洞误报率高，需能证明缺陷存在的方法。

Method: 引入集成自定义Slither检测器、大语言模型、Kontrol和Forge的检测管道。

Result: 对七种关键缺陷实验有成果，展示了对三种难检测漏洞的检测结果，能有效验证真阳性，减少手动验证负担。

Conclusion: 建立了结合启发式分析和形式验证实现更可靠自动化智能合约审计的框架，但存在大语言模型不一致和成本等局限。

Abstract: The high rate of false alarms from static analysis tools and Large Language
Models (LLMs) complicates vulnerability detection in Solidity Smart Contracts,
demanding methods that can formally or empirically prove the presence of
defects. This paper introduces a novel detection pipeline that integrates
custom Slither-based detectors, LLMs, Kontrol, and Forge. Our approach is
designed to reliably detect defects and generate proofs. We currently perform
experiments with promising results for seven types of critical defects. We
demonstrate the pipeline's efficacy by presenting our findings for three
vulnerabilities -- Reentrancy, Complex Fallback, and Faulty Access Control
Policies -- that are challenging for current verification solutions, which
often generate false alarms or fail to detect them entirely. We highlight the
potential of either symbolic or concrete execution in correctly classifying
such code faults. By chaining these instruments, our method effectively
validates true positives, significantly reducing the manual verification
burden. Although we identify potential limitations, such as the inconsistency
and the cost of LLMs, our findings establish a robust framework for combining
heuristic analysis with formal verification to achieve more reliable and
automated smart contract auditing.

</details>


### [186] [GView: A Survey of Binary Forensics via Visual, Semantic, and AI-Enhanced Analysis](https://arxiv.org/abs/2509.13025)
*Raul Zaharia,Dragoş Gavriluţ,Gheorghiţă Mutu*

Main category: cs.SE

TL;DR: 介绍开源取证分析框架GView，包含其发展、当前状态、创新逻辑推理及可扩展架构等。


<details>
  <summary>Details</summary>
Motivation: 应对网络安全威胁日益复杂多样的挑战。

Method: 构建GView框架，融入大语言模型动态增强推理，运用谓词和推理规则进行逻辑推理。

Result: 对GView当前状态进行调研，展示其创新性和可扩展性。

Conclusion: GView有潜力成为实际取证与学术研究之间的桥梁。

Abstract: Cybersecurity threats continue to become more sophisticated and diverse in
their artifacts, boosting both their volume and complexity. To overcome those
challenges, we present GView, an open-source forensic analysis framework with
visual and AI-enhanced reasoning. It started with focus on the practical
cybersecurity industry. It has evolved significantly, incorporating large
language models (LLMs) to dynamically enhance reasoning and ease the forensic
workflows. This paper surveys both the current state of GView with its
published papers alongside those that are in the publishing process. It also
includes its innovative use of logical inference through predicates and
inference rules for both the analyzed documents and the user's actions for
better suggestions. We highlight the extensible architecture, showcasing its
potential as a bridge between the practical forensics worlds with the academic
research.

</details>


### [187] [Automating Code Generation for Semiconductor Equipment Control from Developer Utterances with LLMs](https://arxiv.org/abs/2509.13055)
*Youngkyoung Kim,Sanghyeok Park,Misoo Kim,Gangho Yoon,Eunseok Lee,Simon S. Woo*

Main category: cs.SE

TL;DR: 提出渐进知识增强（PKE）框架提升大语言模型生成低级别半导体设备语言代码的能力，在工业ALPG数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 半导体设备语言编程难，大语言模型在生成低级别设备语言代码方面效果有限。

Method: 提出渐进知识增强（PKE）多阶段提示框架，逐步提取和激活大语言模型的潜在知识。

Result: 在工业ALPG数据集上，PKE显著优于标准提示，超越现有技术，精确匹配得分比第二好的技术高11.1%和15.2%。

Conclusion: PKE是提升大语言模型在专业低级别编程能力的实用方法，有助于提高半导体软件开发效率。

Abstract: Semiconductors form the backbone of modern electronics, with their
manufacturing and testing relying on highly specialized equipment and
domain-specific programming languages. Equipment languages such as the
Algorithmic Pattern Generator (ALPG) are critical for precise hardware control
but are challenging to program due to their low-level syntax and steep learning
curve. While large language models (LLMs) have shown promise in generating
high-level code from natural language, their effectiveness on low-level
equipment languages remains limited. To address this, we propose Progressive
Knowledge Enhancement (PKE), a novel multi-stage prompting framework that
progressively extracts and activates the latent knowledge within LLMs, guiding
them from simple to complex examples without extensive fine-tuning. Empirical
evaluation on an industrial ALPG dataset shows that PKE significantly
outperforms standard prompting and surpasses state-of-the-art methods in
generating correct ALPG code, achieving 11.1\% and 15.2\% higher exact match
scores compared to the second-best technique. Further analysis of individual
components confirms that progressive knowledge extraction based on difficulty
enhances accuracy. Our study offer a practical approach to boosting LLM
capabilities for specialized low-level programming, supporting greater
productivity in semiconductor software development.

</details>


### [188] [Accelerating Discovery: Rapid Literature Screening with LLMs](https://arxiv.org/abs/2509.13103)
*Santiago Matalonga,Domenico Amalfitano,Jean Carlo Rossa Hauck,Martín Solari,Guilherme H. Travassos*

Main category: cs.SE

TL;DR: 开发并验证基于大语言模型（LLM）的工具以支持多声道文献综述（MVLR）的文档搜索和筛选，工具在非相关源上与人类研究者的PPA达90%，证明该工具可行但仍需研究者参与。


<details>
  <summary>Details</summary>
Motivation: 进行MVLR耗时费力，需审查和筛选大量非结构化源，因此开发LLM辅助工具以支持文档搜索和筛选。

Method: 应用合理工程实践开发基于RAG的本地LLM工具处理候选源，用PPA量化目标进展，结合便利抽样、人工判断和统计抽样验证工具质量。

Result: 工具在与研究不相关的源上与人类研究者的PPA达90%，并分享开发细节支持特定领域适配。

Conclusion: 使用基于LLM的工具支持严谨的MVLR是可行的，可节省时间用于高级抽象任务，但研究者参与仍必不可少。

Abstract: Background: Conducting Multi Vocal Literature Reviews (MVLRs) is often time
and effort-intensive. Researchers must review and filter a large number of
unstructured sources, which frequently contain sparse information and are
unlikely to be included in the final study. Our experience conducting an MVLR
on Context-Aware Software Systems (CASS) Testing in the avionics domain
exemplified this challenge, with over 8,000 highly heterogeneous documents
requiring review. Therefore, we developed a Large Language Model (LLM)
assistant to support the search and filtering of documents. Aims: To develop
and validate an LLM based tool that can support researchers in performing the
search and filtering of documents for an MVLR without compromising the rigor of
the research protocol. Method: We applied sound engineering practices to
develop an on-premises LLM-based tool incorporating Retrieval Augmented
Generation (RAG) to process candidate sources. Progress towards the aim was
quantified using the Positive Percent Agreement (PPA) as the primary metric to
ensure the performance of the LLM based tool. Convenience sampling, supported
by human judgment and statistical sampling, were used to verify and validate
the tool's quality-in-use. Results: The tool currently demonstrates a PPA
agreement with human researchers of 90% for sources that are not relevant to
the study. Development details are shared to support domain-specific adaptation
of the tool. Conclusions: Using LLM-based tools to support academic researchers
in rigorous MVLR is feasible. These tools can free valuable time for
higher-level, abstract tasks. However, researcher participation remains
essential to ensure that the tool supports thorough research.

</details>


### [189] [Vulnerability Patching Across Software Products and Software Components: A Case Study of Red Hat's Product Portfolio](https://arxiv.org/abs/2509.13117)
*Jukka Ruohonen,Sani Abdullahi,Abhishek Tiwari*

Main category: cs.SE

TL;DR: 本文对1999 - 2024年红帽产品和组件的漏洞修复进行时间序列分析，发现漏洞产品和组件数量不稳定，线性趋势适用但有断点，安全债务可能趋稳。


<details>
  <summary>Details</summary>
Motivation: 受软件维护和安全债务概念启发，对红帽产品和组件的漏洞修复进行研究。

Method: 分段回归分析。

Result: 漏洞产品和组件数量不稳定，线性趋势能描述很多序列，但与总体漏洞趋势不太一致，存在明显断点。

Conclusion: 线性趋势并非普遍适用，不断增长的安全债务可能正在趋于稳定。

Abstract: Motivated by software maintenance and the more recent concept of security
debt, the paper presents a time series analysis of vulnerability patching of
Red Hat's products and components between 1999 and 2024. According to the
results based on segmented regression analysis, the amounts of vulnerable
products and components have not been stable; a linear trend describes many of
the series well. Nor do the amounts align well with trends characterizing
vulnerabilities in general. There are also visible breakpoints indicating that
the linear trend is not universally applicable and that the growing security
debt may be stabilizing.

</details>


### [190] [Optimizing Code Embeddings and ML Classifiers for Python Source Code Vulnerability Detection](https://arxiv.org/abs/2509.13134)
*Talaya Farasat,Joachim Posegga*

Main category: cs.SE

TL;DR: 研究Python源码漏洞检测中代码嵌入技术与机器学习分类器的最优组合，发现BiLSTM+Word2Vec效果更佳。


<details>
  <summary>Details</summary>
Motivation: 源码复杂度和规模增长使手动软件漏洞检测不现实，需自动化方法。

Method: 评估Word2Vec、CodeBERT、GraphCodeBERT三种嵌入技术和BiLSTM、CNN两种深度学习分类器。

Result: CNN+GraphCodeBERT表现强，但BiLSTM+Word2Vec总体结果更优。

Conclusion: 选择合适的嵌入和分类器组合对提升自动化漏洞检测系统有效性至关重要，尤其针对Python源码。

Abstract: In recent years, the growing complexity and scale of source code have
rendered manual software vulnerability detection increasingly impractical. To
address this challenge, automated approaches leveraging machine learning and
code embeddings have gained substantial attention. This study investigates the
optimal combination of code embedding techniques and machine learning
classifiers for vulnerability detection in Python source code. We evaluate
three embedding techniques, i.e., Word2Vec, CodeBERT, and GraphCodeBERT
alongside two deep learning classifiers, i.e., Bidirectional Long Short-Term
Memory (BiLSTM) networks and Convolutional Neural Networks (CNN). While CNN
paired with GraphCodeBERT exhibits strong performance, the BiLSTM model using
Word2Vec consistently achieves superior overall results. These findings suggest
that, despite the advanced architectures of recent models like CodeBERT and
GraphCodeBERT, classical embeddings such as Word2Vec, when used with
sequence-based models like BiLSTM, can offer a slight yet consistent
performance advantage. The study underscores the critical importance of
selecting appropriate combinations of embeddings and classifiers to enhance the
effectiveness of automated vulnerability detection systems, particularly for
Python source code.

</details>


### [191] [Towards the Next Generation of Software: Insights from Grey Literature on AI-Native Applications](https://arxiv.org/abs/2509.13144)
*Lingli Cao,Shanshan Li,Ying Fan,Danyang Li,Chenxing Zhong*

Main category: cs.SE

TL;DR: 本文旨在全面理解AI原生应用，通过灰色文献综述，确定其特征、质量属性和技术栈，提出双层工程蓝图。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型推动AI原生应用发展，但缺乏统一工程定义和架构蓝图，从业者缺乏系统指导。

Method: 进行灰色文献综述，结合谷歌和必应搜索的概念视角与GitHub开源项目的实践见解，采用结构化协议综合不同来源结果。

Result: 确定106项研究，发现AI原生应用以AI为核心和具有概率非确定性，关键质量属性包括可靠性等，典型技术栈开始出现，与传统软件系统有别。

Conclusion: 本研究首次提出双层工程蓝图。

Abstract: Background: The rapid advancement of large language models (LLMs) has given
rise to AI-native applications, a new paradigm in software engineering that
fundamentally redefines how software is designed, developed, and evolved.
Despite their growing prominence, AI-native applications still lack a unified
engineering definition and architectural blueprint, leaving practitioners
without systematic guidance for system design, quality assurance, and
technology selection.
  Objective: This study seeks to establish a comprehensive understanding of
AI-native applications by identifying their defining characteristics, key
quality attributes, and typical technology stacks, as well as by clarifying the
opportunities and challenges they present.
  Method: We conducted a grey literature review, integrating conceptual
perspectives retrieved from targeted Google and Bing searches with practical
insights derived from leading open-source projects on GitHub. A structured
protocol encompassing source selection, quality assessment, and thematic
analysis was applied to synthesize findings across heterogeneous sources.
  Results: We finally identified 106 studies based on the selection criteria.
The analysis reveals that AI-native applications are distinguished by two core
pillars: the central role of AI as the system's intelligence paradigm and their
inherently probabilistic, non-deterministic nature. Critical quality attributes
include reliability, usability, performance efficiency, and AI-specific
observability. In addition, a typical technology stack has begun to emerge,
comprising LLM orchestration frameworks, vector databases, and AI-native
observability platforms. These systems emphasize response quality,
cost-effectiveness, and outcome predictability, setting them apart from
conventional software systems.
  Conclusion: This study is the first to propose a dual-layered engineering
blueprint...

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [192] [DeltaHedge: A Multi-Agent Framework for Portfolio Options Optimization](https://arxiv.org/abs/2509.12753)
*Feliks Bańka,Jarosław A. Chudziak*

Main category: q-fin.PM

TL;DR: 提出DeltaHedge多智能体框架，结合期权交易与AI驱动的投资组合管理，实验表明其优于传统策略，为量化金融和AI驱动的投资组合优化领域做出贡献。


<details>
  <summary>Details</summary>
Motivation: 在动荡金融市场中，传统方法仅关注股票配置，忽视期权交易的动态风险对冲优势，需新方法平衡风险与回报。

Method: 提出DeltaHedge多智能体框架，结合先进强化学习技术和基于期权的集成对冲策略。

Result: DeltaHedge在实验中表现优于传统策略和独立模型。

Conclusion: DeltaHedge有潜力改变复杂金融环境下的实际投资组合管理，引入的多智能体系统填补了现有文献空白。

Abstract: In volatile financial markets, balancing risk and return remains a
significant challenge. Traditional approaches often focus solely on equity
allocation, overlooking the strategic advantages of options trading for dynamic
risk hedging. This work presents DeltaHedge, a multi-agent framework that
integrates options trading with AI-driven portfolio management. By combining
advanced reinforcement learning techniques with an ensembled options-based
hedging strategy, DeltaHedge enhances risk-adjusted returns and stabilizes
portfolio performance across varying market conditions. Experimental results
demonstrate that DeltaHedge outperforms traditional strategies and standalone
models, underscoring its potential to transform practical portfolio management
in complex financial environments. Building on these findings, this paper
contributes to the fields of quantitative finance and AI-driven portfolio
optimization by introducing a novel multi-agent system for integrating options
trading strategies, addressing a gap in the existing literature.

</details>


### [193] [Income Disaster, Role of Income Support, and Optimal Retirement](https://arxiv.org/abs/2509.12874)
*Tae Ung Gang,Seyoung Park,Yong Hyun Shin*

Main category: q-fin.PM

TL;DR: 研究收入灾难下消费/储蓄、投资与退休选择的相互作用，发现政府额外收入支持水平对低收入者退休决策至关重要。


<details>
  <summary>Details</summary>
Motivation: 探究收入灾难下消费/储蓄、投资和退休选择的相互关系，关注低收入人群在收入灾难时的退休问题。

Method: 分析低收入人群在收入灾难时的情况，考虑政府提供额外收入支持，定量研究收入支持水平与退休决策的关系。

Result: 发现退休决策关键取决于收入支持水平，确定了一个收入支持水平，低于此水平应延迟退休。

Conclusion: 政府的额外收入支持对低收入人群在收入灾难时实现最优退休尤为重要。

Abstract: This paper investigates the interactions among consumption/savings,
investment, and retirement choices with income disaster. We consider low-income
people who are exposed to income disaster so that they retire involuntarily
when income disaster occurs. The government provides extra income support to
low-income retirees who suffer from significant income gaps. We demonstrate
that the decision to enter retirement in the event of income disaster depends
crucially on the level of income support. In particular, we quantitatively
identify a certain income support level below which the optimal decision is to
delay retirement. This implies that availability of the government's extra
income support can be particularly important for the low-income people to
achieve optimal retirement with income disaster.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [194] [A Note on Subadditivity of Value at Risks (VaRs): A New Connection to Comonotonicity](https://arxiv.org/abs/2509.12558)
*Yuri Imamura,Takashi Kato*

Main category: q-fin.RM

TL;DR: 本文给出风险价值（VaR）新性质，即给定损失随机变量的VaR次可加性对任意置信水平成立的充要条件是这些变量共单调，还给出随机向量共单调性的新等价条件。


<details>
  <summary>Details</summary>
Motivation: 在量化金融风险管理中，探索标准风险度量指标VaR的新性质。

Method: 未提及

Result: 证明给定损失随机变量的VaR次可加性对任意置信水平成立当且仅当这些变量共单调，给出随机向量共单调性的新等价条件。

Conclusion: 得到了VaR的新性质以及随机向量共单调性的新等价条件。

Abstract: In this paper, we provide a new property of value at risk (VaR), which is a
standard risk measure that is widely used in quantitative financial risk
management. We show that the subadditivity of VaR for given loss random
variables holds for any confidence level if and only if those are comonotonic.
This result also gives a new equivalent condition for the comonotonicity of
random vectors.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [195] [Reinforcement Learning-Based Market Making as a Stochastic Control on Non-Stationary Limit Order Book Dynamics](https://arxiv.org/abs/2509.12456)
*Rafael Zimmer,Oswaldo Luiz do Valle Costa*

Main category: q-fin.TR

TL;DR: 本文探索强化学习代理在做市场景的应用，基于PPO算法实现做市代理并评估其在不同市场条件下表现，结果表明代理能在非平稳市场有效使用，模拟环境是训练代理的有价值工具。


<details>
  <summary>Details</summary>
Motivation: 利用强化学习框架开发自适应和数据驱动策略，使做市商能根据与限价订单簿环境的交互优化决策策略。

Method: 将强化学习代理集成到做市场景，明确建模市场动态，基于PPO算法实现做市代理，并通过基于模拟器的环境评估其在不同市场条件下的表现。

Result: 与封闭形式的最优解相比，强化学习代理能在非平稳市场条件下有效使用，提出的基于模拟器的环境可作为做市场景中训练和预训练强化学习代理的有价值工具。

Conclusion: 强化学习代理可有效应用于非平稳市场条件下的做市场景，基于模拟器的环境对训练代理有重要价值。

Abstract: Reinforcement Learning has emerged as a promising framework for developing
adaptive and data-driven strategies, enabling market makers to optimize
decision-making policies based on interactions with the limit order book
environment. This paper explores the integration of a reinforcement learning
agent in a market-making context, where the underlying market dynamics have
been explicitly modeled to capture observed stylized facts of real markets,
including clustered order arrival times, non-stationary spreads and return
drifts, stochastic order quantities and price volatility. These mechanisms aim
to enhance stability of the resulting control agent, and serve to incorporate
domain-specific knowledge into the agent policy learning process. Our
contributions include a practical implementation of a market making agent based
on the Proximal-Policy Optimization (PPO) algorithm, alongside a comparative
evaluation of the agent's performance under varying market conditions via a
simulator-based environment. As evidenced by our analysis of the financial
return and risk metrics when compared to a closed-form optimal solution, our
results suggest that the reinforcement learning agent can effectively be used
under non-stationary market conditions, and that the proposed simulator-based
environment can serve as a valuable tool for training and pre-training
reinforcement learning agents in market-making scenarios.

</details>


### [196] [Myopic Optimality: why reinforcement learning portfolio management strategies lose money](https://arxiv.org/abs/2509.12764)
*Yuming Ma*

Main category: q-fin.TR

TL;DR: 近视优化（MO）在投资组合管理中优于强化学习（RL），并给出相关推导和分析结果。


<details>
  <summary>Details</summary>
Motivation: 探究近视优化和强化学习在投资组合管理中的表现差异。

Method: 使用盯市会计模型执行/清算摩擦，运用Malliavin微积分推导策略梯度和风险影子价格，统一HJB和KKT。

Result: 得出对偶间隙和收敛结果，量化RL中的幻影利润并定义CAD溢价。

Conclusion: 近视优化在投资组合管理方面优于强化学习。

Abstract: Myopic optimization (MO) outperforms reinforcement learning (RL) in portfolio
management: RL yields lower or negative returns, higher variance, larger costs,
heavier CVaR, lower profitability, and greater model risk. We model
execution/liquidation frictions with mark-to-market accounting. Using Malliavin
calculus (Clark-Ocone/BEL), we derive policy gradients and risk shadow price,
unifying HJB and KKT. This gives dual gap and convergence results: geometric MO
vs. RL floors. We quantify phantom profit in RL via Malliavin policy-gradient
contamination analysis and define a control-affects-dynamics (CAD) premium of
RL indicating plausibly positive.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [197] [PBPK-iPINNs : Inverse Physics-Informed Neural Networks for Physiologically Based Pharmacokinetic Brain Models](https://arxiv.org/abs/2509.12666)
*Charuka D. Wickramasinghe,Krishanthi C. Weerasinghe,Pradeep K. Ranaweera*

Main category: stat.ML

TL;DR: 本文介绍PBPK - iPINN方法用于估计PBPK脑隔室模型中药物或患者特定参数及药物浓度分布，探讨损失函数组件加权和参数调整，还与传统方法对比性能。


<details>
  <summary>Details</summary>
Motivation: 结合Physics - Informed Neural Networks (PINNs) 与physiologically based pharmacokinetic (PBPK) 模型，估计PBPK脑隔室模型中药物或患者特定参数及药物浓度分布。

Method: 引入PBPK - iPINN方法，合理加权损失函数组件，仔细调整参数，并与传统数值和统计方法对比。

Result: 未明确提及具体结果。

Conclusion: 未明确提及具体结论，但暗示了PBPK - iPINN方法有进行性能评估的必要性。

Abstract: Physics-Informed Neural Networks (PINNs) leverage machine learning with
differential equations to solve direct and inverse problems, ensuring
predictions follow physical laws. Physiologically based pharmacokinetic (PBPK)
modeling advances beyond classical compartmental approaches by using a
mechanistic, physiology focused framework. A PBPK model is based on a system of
ODEs, with each equation representing the mass balance of a drug in a
compartment, such as an organ or tissue. These ODEs include parameters that
reflect physiological, biochemical, and drug-specific characteristics to
simulate how the drug moves through the body. In this paper, we introduce
PBPK-iPINN, a method to estimate drug-specific or patient-specific parameters
and drug concentration profiles in PBPK brain compartment models using inverse
PINNs. We demonstrate that, for the inverse problem to converge to the correct
solution, the loss function components (data loss, initial conditions loss, and
residual loss) must be appropriately weighted, and parameters (including number
of layers, number of neurons, activation functions, learning rate, optimizer,
and collocation points) must be carefully tuned. The performance of the
PBPK-iPINN approach is then compared with established traditional numerical and
statistical methods.

</details>


### [198] [SURGIN: SURrogate-guided Generative INversion for subsurface multiphase flow with quantified uncertainty](https://arxiv.org/abs/2509.13189)
*Zhao Feng,Bicheng Yan,Luanxiao Zhao,Xianda Shen,Renyu Zhao,Wenhao Wang,Fengshou Zhang*

Main category: stat.ML

TL;DR: 提出SURGIN框架用于地下多相流数据同化，有零样本条件生成能力，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有反演方法对新观测配置需调整，缺乏零样本条件生成能力，需新方法解决。

Method: 将U - FNO替代模型与基于分数的生成模型（SGM）协同集成，先自监督预训练无条件SGM捕获地质先验，再利用可微U - FNO替代模型进行后验采样。

Result: 大量数值实验表明SURGIN能在不同测量设置下推断异质地质场、预测时空流动动力学并量化不确定性。

Conclusion: SURGIN将生成学习与替代引导的贝叶斯推理结合，为参数函数空间的反演建模和不确定性量化建立了新范式。

Abstract: We present a direct inverse modeling method named SURGIN, a SURrogate-guided
Generative INversion framework tailed for subsurface multiphase flow data
assimilation. Unlike existing inversion methods that require adaptation for
each new observational configuration, SURGIN features a zero-shot conditional
generation capability, enabling real-time assimilation of unseen monitoring
data without task-specific retraining. Specifically, SURGIN synergistically
integrates a U-Net enhanced Fourier Neural Operator (U-FNO) surrogate with a
score-based generative model (SGM), framing the conditional generation as a
surrogate prediction-guidance process in a Bayesian perspective. Instead of
directly learning the conditional generation of geological parameters, an
unconditional SGM is first pretrained in a self-supervised manner to capture
the geological prior, after which posterior sampling is performed by leveraging
a differentiable U-FNO surrogate to enable efficient forward evaluations
conditioned on unseen observations. Extensive numerical experiments demonstrate
SURGIN's capability to decently infer heterogeneous geological fields and
predict spatiotemporal flow dynamics with quantified uncertainty across diverse
measurement settings. By unifying generative learning with surrogate-guided
Bayesian inference, SURGIN establishes a new paradigm for inverse modeling and
uncertainty quantification in parametric functional spaces.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [199] [Timbre-Adaptive Transcription: A Lightweight Architecture with Associative Memory for Dynamic Instrument Separation](https://arxiv.org/abs/2509.12712)
*Ruigang Li,Yongxu Zhu*

Main category: cs.SD

TL;DR: 本文提出轻量级深度聚类解决方案解决多音色转录模型泛化和源数量约束问题，实验表明模型表现出色，提供高效框架并探索新方向。


<details>
  <summary>Details</summary>
Motivation: 现有多音色转录模型在预训练乐器之外的泛化能力和源数量约束方面存在问题。

Method: 提出轻量级深度聚类解决方案，包括音色无关骨干网络和联想记忆机制，还有新的合成数据集方法。

Result: 音色无关转录模型在公共基准上优于现有模型，分离模块有良好的音色辨别能力。

Conclusion: 提供了高效的音色相关音乐转录框架，探索了基于认知架构的音色感知分离新方向。

Abstract: Existing multi-timbre transcription models struggle with generalization
beyond pre-trained instruments and rigid source-count constraints. We address
these limitations with a lightweight deep clustering solution featuring: 1) a
timbre-agnostic backbone achieving state-of-the-art performance with only half
the parameters of comparable models, and 2) a novel associative memory
mechanism that mimics human auditory cognition to dynamically encode unseen
timbres via attention-based clustering. Our biologically-inspired framework
enables adaptive polyphonic separation with minimal training data (12.5
minutes), supported by a new synthetic dataset method offering cost-effective,
high-precision multi-timbre generation. Experiments show the timbre-agnostic
transcription model outperforms existing models on public benchmarks, while the
separation module demonstrates promising timbre discrimination. This work
provides an efficient framework for timbre-related music transcription and
explores new directions for timbre-aware separation through cognitive-inspired
architectures.

</details>


### [200] [Omni-CLST: Error-aware Curriculum Learning with guided Selective chain-of-Thought for audio questuin answering](https://arxiv.org/abs/2509.12275)
*Jinghua Zhao,Hang Su,Lichun Fan,Zhenbo Luo,Jian Luan,Hui Wang,Haoqin Sun,Yong Qin*

Main category: cs.SD

TL;DR: 提出用于音频问答的Omni - CLST框架，通过两种策略有效利用现有数据集，实验显示其在多模态音频语言理解中有良好表现。


<details>
  <summary>Details</summary>
Motivation: 为音频问答设计一个高效利用现有高质量数据集的框架。

Method: 提出Omni - CLST框架，采用错误感知课程组织样本难度和引导思维丢弃机制，并结合GRPO训练。

Result: 在MMAU - mini和MMAR上实验，Omni - CLST在MMAU - mini达到73.80%的准确率，在MMAR建立了64.30%的新的最优水平。

Conclusion: Omni - CLST在多模态音频语言理解中具有鲁棒性和泛化能力。

Abstract: We propose Omni-CLST, an error-aware Curriculum Learning framework with
guided Selective Chain-of-Thought for audio question answering. The framework
efficiently leverages existing high-quality dataset through two key strategies:
an error-aware curriculum that organizes samples by difficulty, and a guided
thought dropout mechanism that focuses reasoning on challenging cases.
Integrated with GRPO training, these strategies enable the model to learn more
effectively from informative samples. Experiments on MMAU-mini and MMAR
demonstrate that Omni-CLST achieves competitive accuracy (73.80% on MMAU-mini)
and establishes a new state of the art (64.30% on MMAR), highlighting its
robustness and generalization capability in multimodal audio-language
understanding.

</details>


### [201] [A Traditional Approach to Symbolic Piano Continuation](https://arxiv.org/abs/2509.12267)
*Christian Zhou-Zheng,John Backsund,Dun Li Chan,Alex Coventry,Avid Eslami,Jyotin Goel,Xingwen Han,Danysh Soomro,Galen Wei*

Main category: cs.SD

TL;DR: 提出传统方法用于MIREX 2025符号音乐生成挑战中的钢琴音乐续写，回归简单目标，发布代码。


<details>
  <summary>Details</summary>
Motivation: 认为简单方法对单乐器受限任务更有效，想在任务中超越大型基础模型。

Method: 对标记化原始MIDI采用简单、未增强的下一个标记预测目标。

Result: 未提及明确结果。

Conclusion: 简单方法在单乐器受限任务中有优势，发布模型权重和代码。

Abstract: We present a traditional approach to symbolic piano music continuation for
the MIREX 2025 Symbolic Music Generation challenge. While computational music
generation has recently focused on developing large foundation models with
sophisticated architectural modifications, we argue that simpler approaches
remain more effective for constrained, single-instrument tasks. We thus return
to a simple, unaugmented next-token-prediction objective on tokenized raw MIDI,
aiming to outperform large foundation models by using better data and better
fundamentals. We release model weights and code at
https://github.com/christianazinn/mirex2025.

</details>


### [202] [More Similar than Dissimilar: Modeling Annotators for Cross-Corpus Speech Emotion Recognition](https://arxiv.org/abs/2509.12295)
*James Tavernor,Emily Mower Provost*

Main category: cs.SD

TL;DR: 提出利用标注者间相似性，用预训练模型识别相似标注者，实现低成本个性化语音情感识别，效果优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 现有语音情感识别模型预测单人标注能力有限，适应新标注者困难，需大量标注数据。

Method: 利用在大量标注者群体上预训练的模型，根据新标注者有限数据识别相似标注者，对目标数据集进行标注。

Result: 所提方法显著优于其他现成方法。

Conclusion: 该方法为轻量级情感适应铺平道路，适合实际应用。

Abstract: Speech emotion recognition systems often predict a consensus value generated
from the ratings of multiple annotators. However, these models have limited
ability to predict the annotation of any one person. Alternatively, models can
learn to predict the annotations of all annotators. Adapting such models to new
annotators is difficult as new annotators must individually provide sufficient
labeled training data. We propose to leverage inter-annotator similarity by
using a model pre-trained on a large annotator population to identify a
similar, previously seen annotator. Given a new, previously unseen, annotator
and limited enrollment data, we can make predictions for a similar annotator,
enabling off-the-shelf annotation of unseen data in target datasets, providing
a mechanism for extremely low-cost personalization. We demonstrate our approach
significantly outperforms other off-the-shelf approaches, paving the way for
lightweight emotion adaptation, practical for real-world deployment.

</details>


### [203] [A Lightweight Pipeline for Noisy Speech Voice Cloning and Accurate Lip Sync Synthesis](https://arxiv.org/abs/2509.12831)
*Javeria Amir,Farwa Attaria,Mah Jabeen,Umara Noor,Zahid Rashid*

Main category: cs.SD

TL;DR: 提出含Tortoise文本转语音的模块化管道，可在少量样本下零样本语音克隆，用轻量级GAN实现唇同步，适用于嘈杂无约束场景且便于扩展。


<details>
  <summary>Details</summary>
Motivation: 现有语音克隆和说话头生成方法依赖大规模数据集和计算密集过程，在嘈杂或低资源环境不可行。

Method: 引入含Tortoise文本转语音的模块化管道，其为基于Transformer的潜在扩散模型，可零样本语音克隆；用轻量级生成对抗网络架构实现唇同步。

Result: 能在嘈杂和无约束场景减少对大规模预训练的依赖，生成富有情感表达的语音和唇同步。

Conclusion: 管道模块化结构便于未来多模态和文本引导的语音调制扩展，可用于现实系统。

Abstract: Recent developments in voice cloning and talking head generation demonstrate
impressive capabilities in synthesizing natural speech and realistic lip
synchronization. Current methods typically require and are trained on large
scale datasets and computationally intensive processes using clean studio
recorded inputs that is infeasible in noisy or low resource environments. In
this paper, we introduce a new modular pipeline comprising Tortoise text to
speech. It is a transformer based latent diffusion model that can perform high
fidelity zero shot voice cloning given only a few training samples. We use a
lightweight generative adversarial network architecture for robust real time
lip synchronization. The solution will contribute to many essential tasks
concerning less reliance on massive pre training generation of emotionally
expressive speech and lip synchronization in noisy and unconstrained scenarios.
The modular structure of the pipeline allows an easy extension for future multi
modal and text guided voice modulation and it could be used in real world
systems.

</details>


### [204] [Improving Anomalous Sound Detection with Attribute-aware Representation from Domain-adaptive Pre-training](https://arxiv.org/abs/2509.12845)
*Xin Fang,Guirui Zhong,Qing Wang,Fan Chu,Lei Wang,Mengui Qian,Mingqi Cai,Jiangzhao Wu,Jianqing Gao,Jun Du*

Main category: cs.SD

TL;DR: 针对异常声音检测中属性标签缺失问题，提出用预训练模型表示进行聚类分配伪标签，经监督微调实现属性分类，在DCASE 2025数据集上获性能提升。


<details>
  <summary>Details</summary>
Motivation: 异常声音检测常为机器属性分类任务，但收集机器属性标签费力且不现实，存在属性标签缺失挑战。

Method: 提出用领域自适应预训练模型的表示进行凝聚层次聚类分配伪属性标签，对预训练模型进行监督微调实现机器属性分类。

Result: 在DCASE 2025挑战数据集上评估，所提方法有显著性能提升，超越之前挑战中的顶级系统。

Conclusion: 所提方法取得新的最优性能，能有效解决属性标签缺失问题。

Abstract: Anomalous Sound Detection (ASD) is often formulated as a machine attribute
classification task, a strategy necessitated by the common scenario where only
normal data is available for training. However, the exhaustive collection of
machine attribute labels is laborious and impractical. To address the challenge
of missing attribute labels, this paper proposes an agglomerative hierarchical
clustering method for the assignment of pseudo-attribute labels using
representations derived from a domain-adaptive pre-trained model, which are
expected to capture machine attribute characteristics. We then apply model
adaptation to this pre-trained model through supervised fine-tuning for machine
attribute classification, resulting in a new state-of-the-art performance.
Evaluation on the Detection and Classification of Acoustic Scenes and Events
(DCASE) 2025 Challenge dataset demonstrates that our proposed approach yields
significant performance gains, ultimately outperforming our previous
top-ranking system in the challenge.

</details>


### [205] [Contrastive timbre representations for musical instrument and synthesizer retrieval](https://arxiv.org/abs/2509.13285)
*Gwendal Le Vaillant,Yannick Molle*

Main category: cs.SD

TL;DR: 提出用于乐器检索的对比学习框架，可处理单乐器和多乐器声音，在实验中表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决数字音乐制作中从音频混合中高效检索特定乐器音色的难题。

Method: 引入对比学习框架，提出为虚拟乐器生成正负声音对的技术。

Result: 在单乐器检索中，对比方法与基于分类预训练的方法有竞争力；在多乐器检索中，对比框架表现优于相关工作，三乐器混合的top - 1准确率达81.7%，top - 5准确率达95.7%。

Conclusion: 所提出的对比学习框架在乐器检索任务中有效，尤其在多乐器检索上表现出色。

Abstract: Efficiently retrieving specific instrument timbres from audio mixtures
remains a challenge in digital music production. This paper introduces a
contrastive learning framework for musical instrument retrieval, enabling
direct querying of instrument databases using a single model for both single-
and multi-instrument sounds. We propose techniques to generate realistic
positive/negative pairs of sounds for virtual musical instruments, such as
samplers and synthesizers, addressing limitations in common audio data
augmentation methods.
  The first experiment focuses on instrument retrieval from a dataset of 3,884
instruments, using single-instrument audio as input. Contrastive approaches are
competitive with previous works based on classification pre-training. The
second experiment considers multi-instrument retrieval with a mixture of
instruments as audio input. In this case, the proposed contrastive framework
outperforms related works, achieving 81.7\% top-1 and 95.7\% top-5 accuracies
for three-instrument mixtures.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [206] [Efficient Enumeration of At Most $k$-Out Polygons](https://arxiv.org/abs/2509.12696)
*Waseem Akram,Katsuhisa Yamanaka*

Main category: cs.CG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Let $S$ be a set of $n$ points in the Euclidean plane and general position
i.e., no three points are collinear. An \emph{at most $k$-out polygon of $S$}
is a simple polygon such that each vertex is a point in $S$ and there are at
most $k$ points outside the polygon. In this paper, we consider the problem of
enumerating all the at most $k$-out polygon of $S$. We propose a new
enumeration algorithm for the at most $k$-out polygons of a point set. Our
algorithm enumerates all the at most $k$-out polygons in $\mathcal{O}(n^2
\log{n})$ delay, while the running time of an existing algorithm is
$\mathcal{O}(n^3 \log{n})$ delay.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [207] [CNN-BiLSTM for sustainable and non-invasive COVID-19 detection via salivary ATR-FTIR spectroscopy](https://arxiv.org/abs/2509.12241)
*Anisio P. Santos Junior,Robinson Sabino-Silva,Mário Machado Martins,Thulio Marquez Cunha,Murillo G. Carneiro*

Main category: physics.med-ph

TL;DR: 提出CNN - BiLSTM架构处理ATR - FTIR光谱诊断COVID - 19，实验显示其性能优于其他模型。


<details>
  <summary>Details</summary>
Motivation: COVID - 19大流行给医疗系统带来压力，现有RT - PCR检测方法存在不足，需寻找新的检测方式。

Method: 提出结合CNN与BiLSTM的CNN - BiLSTM架构处理ATR - FTIR光谱，并与独立CNN及其他机器学习技术对比。

Result: CNN - BiLSTM模型在真实COVID - 19数据集上平均准确率和F1分数达0.80，优于其他模型。

Conclusion: CNN - BiLSTM架构能提升模型性能，是利用ATR - FTIR光谱检测COVID - 19更准确可靠的选择。

Abstract: The COVID-19 pandemic has placed unprecedented strain on healthcare systems
and remains a global health concern, especially with the emergence of new
variants. Although real-time polymerase chain reaction (RT-PCR) is considered
the gold standard for COVID-19 detection, it is expensive, time-consuming,
labor-intensive, and sensitive to issues with RNA extraction. In this context,
ATR-FTIR spectroscopy analysis of biofluids offers a reagent-free,
cost-effective alternative for COVID-19 detection. We propose a novel
architecture that combines Convolutional Neural Networks (CNN) with
Bidirectional Long Short-Term Memory (BiLSTM) networks, referred to as
CNN-BiLSTM, to process spectra generated by ATR-FTIR spectroscopy and diagnose
COVID-19 from spectral samples. We compare the performance of this architecture
against a standalone CNN and other state-of-the-art machine learning
techniques. Experimental results demonstrate that our CNN-BiLSTM model
outperforms all other models, achieving an average accuracy and F1-score of
0.80 on a challenging real-world COVID-19 dataset. The addition of the BiLSTM
layer to the CNN architecture significantly enhances model performance, making
CNN-BiLSTM a more accurate and reliable choice for detecting COVID-19 using
ATR-FTIR spectra of non-invasive saliva samples.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [208] [Flow-Based Fragment Identification via Binding Site-Specific Latent Representations](https://arxiv.org/abs/2509.13216)
*Rebecca Manuela Neeser,Ilia Igashov,Arne Schneuing,Michael Bronstein,Philippe Schwaller,Bruno Correia*

Main category: q-bio.BM

TL;DR: 开发了基于对比学习的蛋白-片段编码器LatentFrag，用于片段识别和配体设计，性能优且成本低。


<details>
  <summary>Details</summary>
Motivation: 片段识别初始步骤具有挑战性，因片段结合弱且非特异性。

Method: 开发蛋白-片段编码器，采用对比学习方法将分子片段和蛋白质表面映射到共享潜在空间，提出LatentFrag新方法。

Result: 蛋白-片段相互作用位点定位灵敏度高，片段恢复率达先进水平，生成方法性能优于虚拟筛选且成本低。

Conclusion: 这些方法有助于推进片段识别，为基于片段的药物发现提供有价值的工具。

Abstract: Fragment-based drug design is a promising strategy leveraging the binding of
small chemical moieties that can efficiently guide drug discovery. The initial
step of fragment identification remains challenging, as fragments often bind
weakly and non-specifically. We developed a protein-fragment encoder that
relies on a contrastive learning approach to map both molecular fragments and
protein surfaces in a shared latent space. The encoder captures
interaction-relevant features and allows to perform virtual screening as well
as generative design with our new method LatentFrag. In LatentFrag, fragment
embeddings and positions are generated conditioned on the protein surface while
being chemically realistic by construction. Our expressive fragment and protein
representations allow location of protein-fragment interaction sites with high
sensitivity and we observe state-of-the-art fragment recovery rates when
sampling from the learned distribution of latent fragment embeddings. Our
generative method outperforms common methods such as virtual screening at a
fraction of its computational cost providing a valuable starting point for
fragment hit discovery. We further show the practical utility of LatentFrag and
extend the workflow to full ligand design tasks. Together, these approaches
contribute to advancing fragment identification and provide valuable tools for
fragment-based drug discovery.

</details>


### [209] [Accelerating Protein Molecular Dynamics Simulation with DeepJump](https://arxiv.org/abs/2509.13294)
*Allan dos Santos Costa,Manvitha Ponnapati,Dana Rubin,Tess Smidt,Joseph Jacobson*

Main category: q-bio.BM

TL;DR: 提出DeepJump模型预测蛋白质构象动力学，实现约1000倍计算加速并恢复长时动力学。


<details>
  <summary>Details</summary>
Motivation: 分子动力学模拟计算成本高，深度学习方法中蛋白质通用动力学模型待探索，需明确保留动力学准确性时的加速极限。

Method: 基于欧几里得等变流匹配构建DeepJump模型，在mdCATH多样蛋白质轨迹上训练，研究其在快速折叠蛋白质长时动力学泛化性能及计算加速与预测精度的权衡。

Result: DeepJump实现约1000倍计算加速，有效恢复长时动力学，可用于从头折叠预测折叠路径和天然状态。

Conclusion: DeepJump为蛋白质常规模拟提供了基础。

Abstract: Unraveling the dynamical motions of biomolecules is essential for bridging
their structure and function, yet it remains a major computational challenge.
Molecular dynamics (MD) simulation provides a detailed depiction of
biomolecular motion, but its high-resolution temporal evolution comes at
significant computational cost, limiting its applicability to timescales of
biological relevance. Deep learning approaches have emerged as promising
solutions to overcome these computational limitations by learning to predict
long-timescale dynamics. However, generalizable kinetics models for proteins
remain largely unexplored, and the fundamental limits of achievable
acceleration while preserving dynamical accuracy are poorly understood. In this
work, we fill this gap with DeepJump, an Euclidean-Equivariant Flow
Matching-based model for predicting protein conformational dynamics across
multiple temporal scales. We train DeepJump on trajectories of the diverse
proteins of mdCATH, systematically studying our model's performance in
generalizing to long-term dynamics of fast-folding proteins and characterizing
the trade-off between computational acceleration and prediction accuracy. We
demonstrate the application of DeepJump to ab initio folding, showcasing
prediction of folding pathways and native states. Our results demonstrate that
DeepJump achieves significant $\approx$1000$\times$ computational acceleration
while effectively recovering long-timescale dynamics, providing a stepping
stone for enabling routine simulation of proteins.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [210] [Fast reconstruction of degenerate populations of conductance-based neuron models from spike times](https://arxiv.org/abs/2509.12783)
*Julien Brandoit,Damien Ernst,Guillaume Drion,Arthur Fyon*

Main category: q-bio.NC

TL;DR: 提出结合深度学习与DICs方法，可从尖峰时间推断DICs并生成神经元模型，方法快速准确且提供开源软件。


<details>
  <summary>Details</summary>
Motivation: 将尖峰时间与离子通道组成联系起来是一项艰巨任务，需要解决此挑战。

Method: 结合深度学习与动态输入电导（DICs），用深度学习从尖峰时间推断DICs，生成“孪生”神经元模型。

Result: 方法快速、准确，仅使用尖峰记录即可工作，还提供有图形界面的开源软件。

Conclusion: 该方法能有效连接尖峰时间和离子通道组成，方便无编程经验的研究人员使用。

Abstract: Neurons communicate through spikes, and spike timing is a crucial part of
neuronal processing. Spike times can be recorded experimentally both
intracellularly and extracellularly, and are the main output of
state-of-the-art neural probes. On the other hand, neuronal activity is
controlled at the molecular level by the currents generated by many different
transmembrane proteins called ion channels. Connecting spike timing to ion
channel composition remains an arduous task to date. To address this challenge,
we developed a method that combines deep learning with a theoretical tool
called Dynamic Input Conductances (DICs), which reduce the complexity of ion
channel interactions into three interpretable components describing how neurons
spike. Our approach uses deep learning to infer DICs directly from spike times
and then generates populations of "twin" neuron models that replicate the
observed activity while capturing natural variability in membrane channel
composition. The method is fast, accurate, and works using only spike
recordings. We also provide open-source software with a graphical interface,
making it accessible to researchers without programming expertise.

</details>


### [211] [Emergent complexity and rhythms in evoked and spontaneous dynamics of human whole-brain models after tuning through analysis tools](https://arxiv.org/abs/2509.12873)
*Gianluca Gaglioti,Alessandra Cardinale,Cosimo Lupo,Thierry Nieus,Federico Marmoreo,Robin Gutzen,Michael Denker,Andrea Pigorini,Marcello Massimini,Simone Sarasso,Pier Stanislao Paolucci,Giulia De Bonis*

Main category: q-bio.NC

TL;DR: 提出整合TVB和Cobrawap的框架用于全脑模型参数调优，调优后模型展现更多生物相关特征，证明此框架潜力。


<details>
  <summary>Details</summary>
Motivation: 全脑动力学模拟需确定模型参数，需定义合适可观测变量来指导模型配置选择。

Method: 整合TVB平台模拟全脑动力学和Cobrawap分析模拟结果，应用于998节点人类连接组，使用Larter - Breakspear神经团模型两种配置。

Result: 调优配置在自发和诱发动力学中展现出默认模型所没有的生物相关特征，对外界扰动产生复杂时空活动，自发活动有多种特征。

Conclusion: 结合TVB和Cobrawap可指导参数调优，为全脑模型的数据驱动校准和验证奠定基础。

Abstract: The simulation of whole-brain dynamics should reproduce realistic spontaneous
and evoked neural activity across different scales, including emergent rhythms,
spatio-temporal activation patterns, and macroscale complexity. Once a
mathematical model is selected, its configuration must be determined by
properly setting its parameters. A critical preliminary step in this process is
defining an appropriate set of observables to guide the selection of model
configurations (parameter tuning), laying the groundwork for quantitative
calibration of accurate whole-brain models. Here, we address this challenge by
presenting a framework that integrates two complementary tools: The Virtual
Brain (TVB) platform for simulating whole-brain dynamics, and the Collaborative
Brain Wave Analysis Pipeline (Cobrawap) for analyzing the simulations using a
set of standardized metrics. We apply this framework to a 998-node human
connectome, using two configurations of the Larter-Breakspear neural mass
model: one with the TVB default parameters, the other tuned using Cobrawap. The
results reveal that the tuned configuration exhibits several biologically
relevant features, absent in the default model for both spontaneous and evoked
dynamics. In response to external perturbations, the tuned model generates
non-stereotyped, complex spatio-temporal activity, as measured by the
perturbational complexity index. In spontaneous activity, it displays robust
alpha-band oscillations, infra-slow rhythms, scale-free characteristics,
greater spatio-temporal heterogeneity, and asymmetric functional connectivity.
This work demonstrates the potential of combining TVB and Cobrawap to guide
parameter tuning and lays the groundwork for data-driven calibration and
validation of accurate whole-brain models.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [212] [SamudrACE: Fast and Accurate Coupled Climate Modeling with 3D Ocean and Atmosphere Emulators](https://arxiv.org/abs/2509.12490)
*James P. C. Duncan,Elynn Wu,Surya Dheeshjith,Adam Subel,Troy Arcomano,Spencer K. Clark,Brian Henn,Anna Kwa,Jeremy McGibbon,W. Andre Perkins,William Gregory,Carlos Fernandez-Granda,Julius Busecke,Oliver Watt-Meyer,William J. Hurlin,Alistair Adcroft,Laure Zanna,Christopher Bretherton*

Main category: physics.ao-ph

TL;DR: 提出耦合全球气候模型模拟器SamudrACE，能进行长时间模拟，稳定且气候偏差低，可模拟耦合气候现象。


<details>
  <summary>Details</summary>
Motivation: 采用类似传统数值全球气候模型的方法，开发基于机器学习的气候模拟器。

Method: 借鉴传统气候模型组件分离、通过耦合器统一的方式，开发SamudrACE。

Result: SamudrACE能进行长时间模拟，有145个2D场，稳定性高、气候偏差低，可模拟耦合气候现象。

Conclusion: SamudrACE是一个有效的耦合全球气候模型模拟器。

Abstract: Traditional numerical global climate models simulate the full Earth system by
exchanging boundary conditions between separate simulators of the atmosphere,
ocean, sea ice, land surface, and other geophysical processes. This paradigm
allows for distributed development of individual components within a common
framework, unified by a coupler that handles translation between realms via
spatial or temporal alignment and flux exchange. Following a similar approach
adapted for machine learning-based emulators, we present SamudrACE: a coupled
global climate model emulator which produces centuries-long simulations at
1-degree horizontal, 6-hourly atmospheric, and 5-daily oceanic resolution, with
145 2D fields spanning 8 atmospheric and 19 oceanic vertical levels, plus sea
ice, surface, and top-of-atmosphere variables. SamudrACE is highly stable and
has low climate biases comparable to those of its components with prescribed
boundary forcing, with realistic variability in coupled climate phenomena such
as ENSO that is not possible to simulate in uncoupled mode.

</details>


<div id='cond-mat.mes-hall'></div>

# cond-mat.mes-hall [[Back]](#toc)

### [213] [QDFlow: A Python package for physics simulations of quantum dot devices](https://arxiv.org/abs/2509.13298)
*Donovan L. Buterakos,Sandesh S. Kalantre,Joshua Ziegler,Jacob M Taylor,Justyna P. Zwolak*

Main category: cond-mat.mes-hall

TL;DR: 现有机器学习校准和操作量子点设备需大量高质量标注数据集，获取困难，QDFlow可生成含真实标签的合成数据支持相关研究。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习校准和操作量子点设备时获取大量高质量标注数据集困难的问题。

Method: QDFlow结合自洽托马斯 - 费米求解器、动态电容模型和灵活噪声模块生成数据。

Result: QDFlow能生成与实验相近的电荷稳定性图和基于射线的数据。

Conclusion: QDFlow凭借可调节参数和自定义噪声模型，支持创建用于机器学习开发、基准测试和量子设备研究的大量多样数据集。

Abstract: Recent advances in machine learning (ML) have accelerated progress in
calibrating and operating quantum dot (QD) devices. However, most ML approaches
rely on access to large, high-quality labeled datasets for training,
benchmarking, and validation, with labels capturing key features in the data.
Obtaining such datasets experimentally is challenging due to limited data
availability and the labor-intensive nature of labeling. QDFlow is an
open-source physics simulator for multi-QD arrays that generates realistic
synthetic data with ground-truth labels. QDFlow combines a self-consistent
Thomas-Fermi solver, a dynamic capacitance model, and flexible noise modules to
produce charge stability diagrams and ray-based data closely resembling
experiments. With extensive tunable parameters and customizable noise models,
QDFlow supports the creation of large, diverse datasets for ML development,
benchmarking, and quantum device research.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [214] [Generalized promotion time cure model: A new modeling framework to identify cell-type-specific genes and improve survival prognosis](https://arxiv.org/abs/2509.01001)
*Zhi Zhao,Fatih Kizilaslan,Shixiong Wang,Manuela Zucknick*

Main category: stat.ME

TL;DR: 提出贝叶斯广义促进时间治愈模型（GPTCMs）整合多尺度数据以识别细胞类型特异性基因并改善癌症预后，模拟显示该模型有效。


<details>
  <summary>Details</summary>
Motivation: 单细胞技术产生的高维组学数据可增强癌症患者生存建模，但缺乏整合多尺度数据的统计模型。

Method: 提出一类贝叶斯广义促进时间治愈模型（GPTCMs）用于多尺度数据整合。

Result: 在低维和高维设置的模拟中，所提出的贝叶斯GPTCMs能够识别与细胞类型相关的协变量并改善生存预测。

Conclusion: 贝叶斯GPTCMs可用于多尺度数据整合，识别细胞类型特异性基因并改善癌症预后。

Abstract: Single-cell technologies provide an unprecedented opportunity for dissecting
the interplay between the cancer cells and the associated tumor
microenvironment, and the produced high-dimensional omics data should also
augment existing survival modeling approaches for identifying tumor cell
type-specific genes predictive of cancer patient survival. However, there is no
statistical model to integrate multiscale data including individual-level
survival data, multicellular-level cell composition data and cellular-level
single-cell omics covariates. We propose a class of Bayesian generalized
promotion time cure models (GPTCMs) for the multiscale data integration to
identify cell-type-specific genes and improve cancer prognosis. We demonstrate
with simulations in both low- and high-dimensional settings that the proposed
Bayesian GPTCMs are able to identify cell-type-associated covariates and
improve survival prediction.

</details>


### [215] [Power-Dominance in Estimation Theory: A Third Pathological Axis](https://arxiv.org/abs/2509.12691)
*Sri Satish Krishna Chaitanya Bulusu,Mikko Sillanpää*

Main category: stat.ME

TL;DR: 本文引入二阶诊断提出估计理论新框架，对估计器分类，理论证明特定区域有误差惩罚，提出安全区法则和地图，将估计器设计重塑为路径优化问题。


<details>
  <summary>Details</summary>
Motivation: 经典分析聚焦偏差 - 方差权衡，本文旨在提出更基础的约束。

Method: 引入二阶诊断，将估计器分为三种主要功率区域，提出安全区法则和两种安全区地图。

Result: 理论证明在功率主导区域的估计器会有不可避免的均方误差惩罚，且提出的框架将估计器设计重塑为路径优化问题。

Conclusion: 该框架为正则化提供新理论基础，启发新的设计理念。

Abstract: This paper introduces a novel framework for estimation theory by introducing
a second-order diagnostic for estimator design. While classical analysis
focuses on the bias-variance trade-off, we present a more foundational
constraint. This result is model-agnostic, domain-agnostic, and is valid for
both parametric and non-parametric problems, Bayesian and frequentist
frameworks. We propose to classify the estimators into three primary power
regimes. We theoretically establish that any estimator operating in the
`power-dominant regime' incurs an unavoidable mean-squared error penalty,
making it structurally prone to sub-optimal performance. We propose a
`safe-zone law' and make this diagnostic intuitive through two safe-zone maps.
One map is a geometric visualization analogous to a receiver operating
characteristic curve for estimators, and the other map shows that the safe-zone
corresponds to a bounded optimization problem, while the forbidden
`power-dominant zone' represents an unbounded optimization landscape. This
framework reframes estimator design as a path optimization problem, providing
new theoretical underpinnings for regularization and inspiring novel design
philosophies.

</details>


### [216] [Modeling nonstationary spatial processes with normalizing flows](https://arxiv.org/abs/2509.12884)
*Pratik Nag,Andrew Zammit-Mangion,Ying Sun*

Main category: stat.ME

TL;DR: 提出用神经自回归流（NAFs）建模非平稳、各向异性空间过程，模拟研究显示其能力强，应用于3D Argo Floats数据集。


<details>
  <summary>Details</summary>
Motivation: 选择合适空间扭曲函数困难，扭曲方法多限于二维空间，需新方法建模非平稳、各向异性空间过程。

Method: 引入神经自回归流（NAFs）这一可逆映射对非平稳、各向异性空间过程进行建模。

Result: 模拟研究表明基于NAF的模型比常用空间过程模型有更强的表示能力。

Conclusion: 所提出的建模框架在实际应用中有实用性。

Abstract: Nonstationary spatial processes can often be represented as stationary
processes on a warped spatial domain. Selecting an appropriate spatial warping
function for a given application is often difficult and, as a result of this,
warping methods have largely been limited to two-dimensional spatial domains.
In this paper, we introduce a novel approach to modeling nonstationary,
anisotropic spatial processes using neural autoregressive flows (NAFs), a class
of invertible mappings capable of generating complex, high-dimensional
warpings. Through simulation studies we demonstrate that a NAF-based model has
greater representational capacity than other commonly used spatial process
models. We apply our proposed modeling framework to a subset of the 3D Argo
Floats dataset, highlighting the utility of our framework in real-world
applications.

</details>


### [217] [Learning Discrete Bayesian Networks with Hierarchical Dirichlet Shrinkage](https://arxiv.org/abs/2509.13267)
*Alexander Dombowsky,David B. Dunson*

Main category: stat.ME

TL;DR: 本文提出综合贝叶斯框架学习离散贝叶斯网络，包括条件概率先验、MCMC 算法、网络结构学习方法，通过模拟研究评估并应用于乳腺癌样本。


<details>
  <summary>Details</summary>
Motivation: 现有离散贝叶斯网络推断方法存在数据稀疏和参数假设问题，需更好的学习框架。

Method: 提出层次先验，用并行 Langevin 建议的 MCMC 算法生成精确后验样本，提出两种从分类数据学习网络结构的方法。

Result: 通过模拟研究评估了方法的准确性、功效和 MCMC 性能。

Conclusion: 所提方法可用于从分类数据学习离散贝叶斯网络，还能用于揭示乳腺癌样本的预后网络结构。

Abstract: Discrete Bayesian networks (DBNs) provide a broadly useful framework for
modeling dependence structures in multivariate categorical data. There is a
vast literature on methods for inferring conditional probabilities and
graphical structure in DBNs, but data sparsity and parametric assumptions are
major practical issues. In this article, we detail a comprehensive Bayesian
framework for learning DBNs. First, we propose a hierarchical prior for the
conditional probabilities that enables complicated interactions between parent
variables and stability in sparse regimes. We give a novel Markov chain Monte
Carlo (MCMC) algorithm utilizing parallel Langevin proposals to generate exact
posterior samples, avoiding the pitfalls of variational approximations.
Moreover, we verify that the full conditional distribution of the concentration
parameters is log-concave under mild conditions, facilitating efficient
sampling. We then propose two methods for learning network structures,
including parent sets, Markov blankets, and DAGs, from categorical data. The
first cycles through individual edges each MCMC iteration, whereas the second
updates the entire structure as a single step. We evaluate the accuracy, power,
and MCMC performance of our methods on several simulation studies. Finally, we
apply our methodology to uncover prognostic network structure from primary
breast cancer samples.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [218] [Ratio1 -- AI meta-OS](https://arxiv.org/abs/2509.12223)
*Andrei Damian,Petrica Butusina,Alessandro De Franceschi,Vitalii Toderian,Marius Grigoras,Cristian Bleotiu*

Main category: cs.OS

TL;DR: 提出Ratio1 AI元操作系统，整合区块链框架，具多种组件和共识机制，在AI部署上有优势。


<details>
  <summary>Details</summary>
Motivation: 解决集中式异构云MLOps和现有去中心化计算平台缺乏集成AI工具链或可信节点操作机制的问题，降低AI部署门槛，提高成本效益。

Method: 提出Ratio1 AI元操作系统，包含dAuth、CSTORE等组件，采用PoA和PoAI共识的循环令牌经济模型，提供安全许可和奖励协议的数学公式。

Result: 该系统设计降低了AI部署障碍，提高成本效益。

Conclusion: 所提出的全功能生态系统在可访问性、可扩展性和安全性方面比现有方案有显著改进。

Abstract: We propose the Ratio1 AI meta-operating system (meta-OS), a decentralized
MLOps protocol that unifies AI model development, deployment, and inference
across heterogeneous edge devices. Its key innovation is an integrated
blockchain-based framework that transforms idle computing resources (laptops,
smartphones, cloud VMs) into a trustless global supercomputer. The architecture
includes novel components: a decentralized authentication layer (dAuth), an
in-memory state database (CSTORE), a distributed storage system (R1FS),
homomorphic encrypted federated learning (EDIL), decentralized container
orchestration (Deeploy) and an oracle network (OracleSync), which collectively
ensure secure, resilient execution of AI pipelines and other container based
apps at scale. The protocol enforces a formal circular token-economic model
combining Proof-of-Availability (PoA) and Proof-of-AI (PoAI) consensus.
Compared to centralized heterogeneous cloud MLOps and existing decentralized
compute platforms, which often lack integrated AI toolchains or trusted Ratio1
node operators (R1OP) mechanics, Ratio1's holistic design lowers barriers for
AI deployment and improves cost-efficiency. We provide mathematical
formulations of its secure licensing and reward protocols, and include
descriptive information for the system architecture and protocol flow. We argue
that our proposed fully functional ecosystem proposes and demonstrates
significant improvements in accessibility, scalability, and security over
existing alternatives.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [219] [Try-Mopsa: Relational Static Analysis in Your Pocket](https://arxiv.org/abs/2509.13128)
*Raphaël Monat*

Main category: cs.PL

TL;DR: 介绍了Mopsa静态分析平台的精简版Try - Mopsa，可在浏览器运行，用于入门和教学。


<details>
  <summary>Details</summary>
Motivation: 静态分析器安装困难，阻碍采用和学生学习，需要一个方便的平台。

Method: 将Mopsa静态分析平台精简并编译成JavaScript，使其能在浏览器纯客户端运行。

Result: Try - Mopsa有响应式界面，支持桌面和移动设备，具备Mopsa核心组件，支持关系数值域。

Conclusion: Try - Mopsa是适合入门和教学的便捷平台。

Abstract: Static analyzers are complex pieces of software with large dependencies. They
can be difficult to install, which hinders adoption and creates barriers for
students learning static analysis. This work introduces Try-Mopsa: a
scaled-down version of the Mopsa static analysis platform, compiled into
JavaScript to run purely as a client-side application in web browsers.
Try-Mopsa provides a responsive interface that works on both desktop and mobile
devices. Try-Mopsa features all the core components of Mopsa. In particular, it
supports relational numerical domains. We present the interface, changes and
adaptations required to have a pure JavaScript version of Mopsa. We envision
Try-Mopsa as a convenient platform for onboarding or teaching purposes.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [220] [A Variational Physics-Informed Neural Network Framework Using Petrov-Galerkin Method for Solving Singularly Perturbed Boundary Value Problems](https://arxiv.org/abs/2509.12271)
*Vijay Kumar,Gautam Singh*

Main category: math.NA

TL;DR: 提出VPINN框架求解一维奇异摄动边值问题和抛物型偏微分方程，实验证明其比标准VPINN方法精度更高。


<details>
  <summary>Details</summary>
Motivation: 解决一维奇异摄动边值问题和涉及一两个小参数的抛物型偏微分方程。

Method: 将Petrov - Galerkin公式与深度神经网络结合，采用非线性逼近，用局部测试函数构建弱形式，引入界面惩罚项，通过硬约束施加Dirichlet边界条件，用自动微分计算源项。

Result: 在基准问题的数值实验中，与标准VPINN方法相比，在$L_2$和最大范数下精度显著提高。

Conclusion: 所提出的方法对于求解一维奇异摄动微分方程是有效的。

Abstract: This work proposes a Variational Physics-Informed Neural Network (VPINN)
framework that integrates the Petrov-Galerkin formulation with deep neural
networks (DNNs) for solving one-dimensional singularly perturbed boundary value
problems (BVPs) and parabolic partial differential equations (PDEs) involving
one or two small parameters. The method adopts a nonlinear approximation in
which the trial space is defined by neural network functions, while the test
space is constructed from hat functions. The weak formulation is constructed
using localized test functions, with interface penalty terms introduced to
enhance numerical stability and accurately capture boundary layers. Dirichlet
boundary conditions are imposed via hard constraints, and source terms are
computed using automatic differentiation. Numerical experiments on benchmark
problems demonstrate the effectiveness of the proposed method, showing
significantly improved accuracy in both the $L_2$ and maximum norms compared to
the standard VPINN approach for one-dimensional singularly perturbed
differential equations (SPDEs).

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [221] [FusionMAE: large-scale pretrained model to optimize and simplify diagnostic and control of fusion plasma](https://arxiv.org/abs/2509.12945)
*Zongyu Yang,Zhenghao Yang,Wenjing Tian,Jiyuan Li,Xiang Sun,Guohui Zheng,Songfen Liu,Niannian Wu,Rongpeng Li,Zhaohe Xu,Bo Li,Zhongbing Shi,Zhe Gao,Wei Chen,Xiaoquan Ji,Min Xu,Wulyu Zhong*

Main category: physics.plasm-ph

TL;DR: 本文提出FusionMAE模型压缩诊断信号信息，有虚拟备份诊断等能力，简化系统接口、减少诊断系统并优化性能。


<details>
  <summary>Details</summary>
Motivation: 磁约束聚变装置中，等离子体诊断系统复杂，其复杂性和不确定性阻碍聚变能源发展。

Method: 预训练FusionMAE模型压缩88个诊断信号信息，提出压缩 - 降维和缺失信号重建机制。

Result: 模型实现96.7%可靠性的虚拟备份诊断，有自动数据分析等三种能力。

Conclusion: 该工作开创聚变能源中大规模AI模型集成，预训练嵌入可简化系统接口、减少诊断系统和优化性能。

Abstract: In magnetically confined fusion device, the complex, multiscale, and
nonlinear dynamics of plasmas necessitate the integration of extensive
diagnostic systems to effectively monitor and control plasma behaviour. The
complexity and uncertainty arising from these extensive systems and their
tangled interrelations has long posed a significant obstacle to the
acceleration of fusion energy development. In this work, a large-scale model,
fusion masked auto-encoder (FusionMAE) is pre-trained to compress the
information from 88 diagnostic signals into a concrete embedding, to provide a
unified interface between diagnostic systems and control actuators. Two
mechanisms are proposed to ensure a meaningful embedding: compression-reduction
and missing-signal reconstruction. Upon completion of pre-training, the model
acquires the capability for 'virtual backup diagnosis', enabling the inference
of missing diagnostic data with 96.7% reliability. Furthermore, the model
demonstrates three emergent capabilities: automatic data analysis, universal
control-diagnosis interface, and enhancement of control performance on multiple
tasks. This work pioneers large-scale AI model integration in fusion energy,
demonstrating how pre-trained embeddings can simplify the system interface,
reducing necessary diagnostic systems and optimize operation performance for
future fusion reactors.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [222] [Inferring Soil Drydown Behaviour with Adaptive Bayesian Online Changepoint Analysis](https://arxiv.org/abs/2509.13293)
*Mengyi Gong,Christopher Nemeth,Rebecca Killick,Peter Strauss,John Quinton*

Main category: stat.AP

TL;DR: 基于BOCPD提出两个扩展算法用于土壤湿度序列分析，在合成数据验证后应用于实际数据，展示了自适应框架的优势。


<details>
  <summary>Details</summary>
Motivation: 土壤湿度记录有分段特性，需要能捕捉分段动态并适应先验未知参数的模型，以进行现实推断。

Method: 基于贝叶斯在线变点检测（BOCPD），提出粒子滤波变体和在线梯度变体两个扩展算法。

Result: 在合成数据上验证算法，明确超参数选择、先验和节省成本策略，应用于奥地利和美国实验场地的土壤湿度序列，量化特定场地干化率。

Conclusion: 自适应框架相比静态模型具有优势。

Abstract: Continuous soil-moisture measurements provide a direct lens on subsurface
hydrological processes, notably the post-rainfall "drydown" phase. Because
these records consist of distinct, segment-specific behaviours whose forms and
scales vary over time, realistic inference demands a model that captures
piecewise dynamics while accommodating parameters that are unknown a priori.
Building on Bayesian Online Changepoint Detection (BOCPD), we introduce two
complementary extensions: a particle-filter variant that substitutes exact
marginalisation with sequential Monte Carlo to enable real-time inference when
critical parameters cannot be integrated out analytically, and an
online-gradient variant that embeds stochastic gradient updates within BOCPD to
learn application-relevant parameters on the fly without prohibitive
computational cost. After validating both algorithms on synthetic data that
replicate the temporal structure of field observations-detailing hyperparameter
choices, priors, and cost-saving strategies-we apply them to soil-moisture
series from experimental sites in Austria and the United States, quantifying
site-specific drydown rates and demonstrating the advantages of our adaptive
framework over static models.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [223] [ResidualViT for Efficient Temporally Dense Video Encoding](https://arxiv.org/abs/2509.13255)
*Mattia Soldan,Fabian Caba Heilbron,Bernard Ghanem,Josef Sivic,Bryan Russell*

Main category: cs.CV

TL;DR: 本文提出ResidualViT架构、轻量级蒸馏策略降低密集视频理解任务特征计算成本，在多任务多数据集上验证效果。


<details>
  <summary>Details</summary>
Motivation: 视频理解任务需高分辨率帧级推理，但特征计算成本高，需降低成本。

Method: 引入ResidualViT架构利用视频冗余计算特征，含可学习残差连接和令牌减少模块；提出轻量级蒸馏策略近似原模型特征。

Result: 在四个任务和五个数据集上，零样本和全监督设置下，计算成本最多降低60%，推理速度最多提升2.5倍，精度接近原模型。

Conclusion: 所提方法能显著降低计算成本、提升推理速度，且保持较高精度。

Abstract: Several video understanding tasks, such as natural language temporal video
grounding, temporal activity localization, and audio description generation,
require "temporally dense" reasoning over frames sampled at high temporal
resolution. However, computing frame-level features for these tasks is
computationally expensive given the temporal resolution requirements. In this
paper, we make three contributions to reduce the cost of computing features for
temporally dense tasks. First, we introduce a vision transformer (ViT)
architecture, dubbed ResidualViT, that leverages the large temporal redundancy
in videos to efficiently compute temporally dense frame-level features. Our
architecture incorporates (i) learnable residual connections that ensure
temporal consistency across consecutive frames and (ii) a token reduction
module that enhances processing speed by selectively discarding temporally
redundant information while reusing weights of a pretrained foundation model.
Second, we propose a lightweight distillation strategy to approximate the
frame-level features of the original foundation model. Finally, we evaluate our
approach across four tasks and five datasets, in both zero-shot and fully
supervised settings, demonstrating significant reductions in computational cost
(up to 60%) and improvements in inference speed (up to 2.5x faster), all while
closely approximating the accuracy of the original foundation model.

</details>


### [224] [Scalable RF Simulation in Generative 4D Worlds](https://arxiv.org/abs/2508.12176)
*Zhiwei Zheng,Dongyin Hu,Mingmin Zhao*

Main category: cs.CV

TL;DR: 提出WaveVerse框架模拟室内场景中含人体运动的真实RF信号，实验证明其在人体运动生成、成像和活动识别等方面有效。


<details>
  <summary>Details</summary>
Motivation: 解决在动态多样室内环境中收集高质量RF数据的挑战。

Method: 引入基于提示的可扩展框架WaveVerse，包含语言引导的4D世界生成器和相位相干射线追踪模拟器。

Result: 实验证明在条件人体运动生成方面有效，在波束形成和呼吸监测中应用了相位相干；在成像和人类活动识别案例研究中，首次实现RF成像数据生成，在数据有限和充足场景中均有性能提升。

Conclusion: WaveVerse框架可有效模拟RF信号，能用于数据生成，提升相关任务性能。

Abstract: Radio Frequency (RF) sensing has emerged as a powerful, privacy-preserving
alternative to vision-based methods for indoor perception tasks. However,
collecting high-quality RF data in dynamic and diverse indoor environments
remains a major challenge. To address this, we introduce WaveVerse, a
prompt-based, scalable framework that simulates realistic RF signals from
generated indoor scenes with human motions. WaveVerse introduces a
language-guided 4D world generator, which includes a state-aware causal
transformer for human motion generation conditioned on spatial constraints and
texts, and a phase-coherent ray tracing simulator that enables the simulation
of accurate and coherent RF signals. Experiments demonstrate the effectiveness
of our approach in conditioned human motion generation and highlight how phase
coherence is applied to beamforming and respiration monitoring. We further
present two case studies in ML-based high-resolution imaging and human activity
recognition, demonstrating that WaveVerse not only enables data generation for
RF imaging for the first time, but also consistently achieves performance gain
in both data-limited and data-adequate scenarios.

</details>


### [225] [RU-Net for Automatic Characterization of TRISO Fuel Cross Sections](https://arxiv.org/abs/2509.12244)
*Lu Cai,Fei Xu,Min Xian,Yalei Tang,Shoukun Sun,John Stempien*

Main category: cs.CV

TL;DR: 本文使用卷积神经网络（CNNs）自动分割微观TRISO层横截面图像，以加速数据分析并减少主观性，RU - Net模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 传统手动获取TRISO颗粒辐照现象统计信息的工作繁琐且主观，需减少主观性并加速数据分析。

Method: 生成包含2000多张微观图像及对应标注图像的数据集，使用RU - Net、U - Net、ResNet和Attention U - Net等不同的CNNs自动分割不同TRISO层。

Result: 基于RU - Net的模型在交并比（IoU）方面表现最佳。

Conclusion: 使用CNN模型可加快TRISO颗粒横截面分析，显著减少人工劳动并提高分割结果的客观性。

Abstract: During irradiation, phenomena such as kernel swelling and buffer
densification may impact the performance of tristructural isotropic (TRISO)
particle fuel. Post-irradiation microscopy is often used to identify these
irradiation-induced morphologic changes. However, each fuel compact generally
contains thousands of TRISO particles. Manually performing the work to get
statistical information on these phenomena is cumbersome and subjective. To
reduce the subjectivity inherent in that process and to accelerate data
analysis, we used convolutional neural networks (CNNs) to automatically segment
cross-sectional images of microscopic TRISO layers. CNNs are a class of
machine-learning algorithms specifically designed for processing structured
grid data. They have gained popularity in recent years due to their remarkable
performance in various computer vision tasks, including image classification,
object detection, and image segmentation. In this research, we generated a
large irradiated TRISO layer dataset with more than 2,000 microscopic images of
cross-sectional TRISO particles and the corresponding annotated images. Based
on these annotated images, we used different CNNs to automatically segment
different TRISO layers. These CNNs include RU-Net (developed in this study), as
well as three existing architectures: U-Net, Residual Network (ResNet), and
Attention U-Net. The preliminary results show that the model based on RU-Net
performs best in terms of Intersection over Union (IoU). Using CNN models, we
can expedite the analysis of TRISO particle cross sections, significantly
reducing the manual labor involved and improving the objectivity of the
segmentation results.

</details>


### [226] [Modular, On-Site Solutions with Lightweight Anomaly Detection for Sustainable Nutrient Management in Agriculture](https://arxiv.org/abs/2509.12247)
*Abigail R. Cohen,Yuming Sun,Zhihao Qin,Harsh S. Muriki,Zihao Xiao,Yeonju Lee,Matthew Housley,Andrew F. Sharkey,Rhuanito S. Ferrarezi,Jing Li,Lu Gan,Yongsheng Chen*

Main category: cs.CV

TL;DR: 本文提出灵活分层管道用于作物异常检测和状态估计，含能源分析，实验表明可高效检测异常，状态估计模块各有优劣，为农业可持续性提供机会。


<details>
  <summary>Details</summary>
Motivation: 当前养分管理方法分析时间长，成像计算量大，无法实时优化和在资源受限下部署，需有效方法进行作物养分管理。

Method: 开展养分耗竭实验，使用多光谱成像，构建基于自动编码器的分层管道用于预警，比较基于植被指数特征与机器学习和原始全图像深度学习的两种状态估计模块。

Result: 能以比浪费氮中所含能量低得多的能耗高效检测异常，状态估计模块存在权衡，ViT在磷和钙估计上表现更优但能耗更高。

Conclusion: 模块化管道为边缘诊断和农业可持续性创造了实际机会。

Abstract: Efficient nutrient management is critical for crop growth and sustainable
resource consumption (e.g., nitrogen, energy). Current approaches require
lengthy analyses, preventing real-time optimization; similarly, imaging
facilitates rapid phenotyping but can be computationally intensive, preventing
deployment under resource constraints. This study proposes a flexible, tiered
pipeline for anomaly detection and status estimation (fresh weight, dry mass,
and tissue nutrients), including a comprehensive energy analysis of approaches
that span the efficiency-accuracy spectrum. Using a nutrient depletion
experiment with three treatments (T1-100%, T2-50%, and T3-25% fertilizer
strength) and multispectral imaging (MSI), we developed a hierarchical pipeline
using an autoencoder (AE) for early warning. Further, we compared two status
estimation modules of different complexity for more detailed analysis:
vegetation index (VI) features with machine learning (Random Forest, RF) and
raw whole-image deep learning (Vision Transformer, ViT). Results demonstrated
high-efficiency anomaly detection (73% net detection of T3 samples 9 days after
transplanting) at substantially lower energy than embodied energy in wasted
nitrogen. The state estimation modules show trade-offs, with ViT outperforming
RF on phosphorus and calcium estimation (R2 0.61 vs. 0.58, 0.48 vs. 0.35) at
higher energy cost. With our modular pipeline, this work opens opportunities
for edge diagnostics and practical opportunities for agricultural
sustainability.

</details>


### [227] [Humor in Pixels: Benchmarking Large Multimodal Models Understanding of Online Comics](https://arxiv.org/abs/2509.12248)
*Yuriel Ryan,Rui Yang Tan,Kenny Tsu Wei Choo,Roy Ka-Wei Lee*

Main category: cs.CV

TL;DR: 引入PixelHumor数据集评估大跨模态模型理解多模态幽默和识别叙事顺序的能力，实验显示当前模型有显著不足，该数据集旨在推动模型发展。


<details>
  <summary>Details</summary>
Motivation: 理解幽默是社会智能的核心，但大跨模态模型在这方面存在挑战，需要评估其能力。

Method: 引入包含2800个标注多面板漫画的PixelHumor基准数据集进行实验。

Result: 实验表明，当前最先进的大跨模态模型存在显著差距，如顶级模型在面板排序上的准确率仅为61%，远低于人类表现。

Conclusion: 当前模型在整合视觉和文本线索以进行连贯叙事和幽默理解方面存在关键局限，PixelHumor能为评估跨模态上下文和叙事推理提供严格框架，推动模型发展。

Abstract: Understanding humor is a core aspect of social intelligence, yet it remains a
significant challenge for Large Multimodal Models (LMMs). We introduce
PixelHumor, a benchmark dataset of 2,800 annotated multi-panel comics designed
to evaluate LMMs' ability to interpret multimodal humor and recognize narrative
sequences. Experiments with state-of-the-art LMMs reveal substantial gaps: for
instance, top models achieve only 61% accuracy in panel sequencing, far below
human performance. This underscores critical limitations in current models'
integration of visual and textual cues for coherent narrative and humor
understanding. By providing a rigorous framework for evaluating multimodal
contextual and narrative reasoning, PixelHumor aims to drive the development of
LMMs that better engage in natural, socially aware interactions.

</details>


### [228] [OnlineHOI: Towards Online Human-Object Interaction Generation and Perception](https://arxiv.org/abs/2509.12250)
*Yihong Ji,Yunze Liu,Yiyao Zhuo,Weijiang Yu,Fei Ma,Joshua Huang,Fei Yu*

Main category: cs.CV

TL;DR: 提出在线HOI生成与感知任务，引入基于Mamba框架的OnlineHOI框架，在多个任务上取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 当前HOI感知与生成方法为离线设置，在现实在线场景表现不佳，需要新方法解决。

Method: 提出在线HOI生成和感知两个新任务，引入基于Mamba框架并采用记忆机制的OnlineHOI框架。

Result: 在Core4D和OAKINK2在线生成任务以及在线HOI4D感知任务上取得了SOTA结果。

Conclusion: 所提出的OnlineHOI框架能有效解决在线HOI生成与感知问题。

Abstract: The perception and generation of Human-Object Interaction (HOI) are crucial
for fields such as robotics, AR/VR, and human behavior understanding. However,
current approaches model this task in an offline setting, where information at
each time step can be drawn from the entire interaction sequence. In contrast,
in real-world scenarios, the information available at each time step comes only
from the current moment and historical data, i.e., an online setting. We find
that offline methods perform poorly in an online context. Based on this
observation, we propose two new tasks: Online HOI Generation and Perception. To
address this task, we introduce the OnlineHOI framework, a network architecture
based on the Mamba framework that employs a memory mechanism. By leveraging
Mamba's powerful modeling capabilities for streaming data and the Memory
mechanism's efficient integration of historical information, we achieve
state-of-the-art results on the Core4D and OAKINK2 online generation tasks, as
well as the online HOI4D perception task.

</details>


### [229] [A Modern Look at Simplicity Bias in Image Classification Tasks](https://arxiv.org/abs/2509.12265)
*Xiaoguang Chang,Teng Wang,Changyin Sun*

Main category: cs.CV

TL;DR: 研究CLIP模型的简单性偏差（SB）与图像分类任务性能的关系，提出新的频率感知度量方法并进行实验。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注简单模型或合成任务，难以测量大模型的SB，且SB与各类图像分类任务的相关性未知。

Method: 理论分析现有复杂度度量的局限性，提出频率感知度量；在CLIP模型上验证该度量，并研究SB与图像分类任务性能的关系。

Result: 新度量比之前的更具信息性和一致性；实验揭示了一系列行为，如更强的SB在OOD泛化上比对抗鲁棒性表现更好。

Conclusion: 使模型的归纳偏差与目标任务特征相匹配有益。

Abstract: The simplicity Bias (SB) of neural networks, i.e.\ their tendency to
represent simple functions, is a key factor in their generalization
capabilities. Recent studies show that an excessive SB may harm performance on
complex tasks, and the need for this bias varies across tasks. Many of these
studies focus on simple models or synthetic tasks. It remains challenging to
measure the SB in large models and little is known about the relevance of the
SB to various image classification tasks.
  In this paper, we investigate the relationship between the SB in CLIP models
and their performance across image classification tasks. First, we
theoretically analyze the potential limitation of existing measures of
complexity that have been used to characterize small models. To address this,
we propose a frequency-aware measure capturing finer-grained SB differences. We
validate this measure on CLIP models subjected to two recent SB-modulation
methods, demonstrating that it is more informative and consistent than previous
measures. Second, we examine the relation between the SB of those models and
their performance across a range of image classification tasks, including
zero-shot and fine-tuning settings. These experiments reveal a range of
behaviors. For example, a stronger SB correlates with a better performance on
OOD generalization than on adversarial robustness. These results highlight the
benefits of aligning a model's inductive biases with the characteristics of the
target task.

</details>


### [230] [GraphDerm: Fusing Imaging, Physical Scale, and Metadata in a Population-Graph Classifier for Dermoscopic Lesions](https://arxiv.org/abs/2509.12277)
*Mehdi Yousefzadeh,Parsa Esfahanian,Sara Rashidifar,Hossein Salahshoor Gavalan,Negar Sadat Rafiee Tabatabaee,Saeid Gorgin,Dara Rahmati,Maryam Daneshpazhooh*

Main category: cs.CV

TL;DR: 提出GraphDerm框架用于皮肤镜多类分类，结合成像、毫米级校准和元数据，在ISIC - 2019上优于仅图像管道。


<details>
  <summary>Details</summary>
Motivation: 现有图像AI在皮肤镜分诊中常忽略患者元数据和几何分析所需物理尺度，提出GraphDerm框架解决问题。

Method: 整理ISIC 2018/2019数据，合成带标尺图像，训练U - Nets进行分割，用1D - CNN回归每毫米像素，计算真实尺度描述符，用EfficientNet - B3提取节点特征，用光谱GNN进行半监督节点分类，以仅图像ANN为基线。

Result: 标尺和病变分割Dice分别达0.904和0.908，尺度回归MAE为1.5 px，RMSE为6.6，图模型AUC达0.9812，稀疏变体保留0.9788，高于仅图像基线0.9440，各类AUC多在0.97 - 0.99范围。

Conclusion: 在人口图中统一校准尺度、病变几何和元数据比仅图像管道有显著优势，稀疏图可保持接近最优精度，基于图的AI是皮肤镜决策支持的有前景方向，未来将完善边语义和在更广泛基准上评估。

Abstract: Introduction. Dermoscopy aids melanoma triage, yet image-only AI often
ignores patient metadata (age, sex, site) and the physical scale needed for
geometric analysis. We present GraphDerm, a population-graph framework that
fuses imaging, millimeter-scale calibration, and metadata for multiclass
dermoscopic classification, to the best of our knowledge the first ISIC-scale
application of GNNs to dermoscopy. Methods. We curate ISIC 2018/2019,
synthesize ruler-embedded images with exact masks, and train U-Nets
(SE-ResNet-18) for lesion and ruler segmentation. Pixels-per-millimeter are
regressed from the ruler-mask two-point correlation via a lightweight 1D-CNN.
From lesion masks we compute real-scale descriptors (area, perimeter, radius of
gyration). Node features use EfficientNet-B3; edges encode metadata/geometry
similarity (fully weighted or thresholded). A spectral GNN performs
semi-supervised node classification; an image-only ANN is the baseline.
Results. Ruler and lesion segmentation reach Dice 0.904 and 0.908; scale
regression attains MAE 1.5 px (RMSE 6.6). The graph attains AUC 0.9812, with a
thresholded variant using about 25% of edges preserving AUC 0.9788 (vs. 0.9440
for the image-only baseline); per-class AUCs typically fall in the 0.97-0.99
range. Conclusion. Unifying calibrated scale, lesion geometry, and metadata in
a population graph yields substantial gains over image-only pipelines on
ISIC-2019. Sparser graphs retain near-optimal accuracy, suggesting efficient
deployment. Scale-aware, graph-based AI is a promising direction for
dermoscopic decision support; future work will refine learned edge semantics
and evaluate on broader curated benchmarks.

</details>


### [231] [PATIMT-Bench: A Multi-Scenario Benchmark for Position-Aware Text Image Machine Translation in Large Vision-Language Models](https://arxiv.org/abs/2509.12278)
*Wanru Zhuang,Wenbo Li,Zhibin Lan,Xu Han,Peng Li,Jinsong Su*

Main category: cs.CV

TL;DR: 本文将传统文本图像机器翻译拓展为位置感知文本图像机器翻译（PATIMT），构建PATIMTBench基准和测试集，微调后的紧凑型大视觉语言模型在子任务上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前文本图像机器翻译研究忽略提供边界框且覆盖场景有限，拓展到PATIMT以支持细粒度和保留布局的翻译，有重要实用价值但研究不足。

Method: 构建包含10种真实场景的PATIMTBench基准，引入自适应图像OCR细化管道，构建含1200个高质量实例的测试集。

Result: 微调后的紧凑型大视觉语言模型在两个子任务上达到了当前最优性能，训练数据具有可扩展性和泛化性。

Conclusion: 提出的PATIMT、构建的基准和测试集等方法有效，能提升文本图像机器翻译性能。

Abstract: Text Image Machine Translation (TIMT) aims to translate texts embedded within
an image into another language. Current TIMT studies primarily focus on
providing translations for all the text within an image, while neglecting to
provide bounding boxes and covering limited scenarios. In this work, we extend
traditional TIMT into position-aware TIMT (PATIMT), aiming to support
fine-grained and layoutpreserving translation, which holds great practical
value but remains largely unexplored. This task comprises two key sub-tasks:
regionspecific translation and full-image translation with grounding. To
support existing models on PATIMT and conduct fair evaluation, we construct the
PATIMT benchmark (PATIMTBench), which consists of 10 diverse real-world
scenarios. Specifically, we introduce an Adaptive Image OCR Refinement
Pipeline, which adaptively selects appropriate OCR tools based on scenario and
refines the results of text-rich images. To ensure evaluation reliability, we
further construct a test set, which contains 1,200 high-quality instances
manually annotated and reviewed by human experts. After fine-tuning on our
data, compact Large Vision-Language Models (LVLMs) achieve state-of-the-art
performance on both sub-tasks. Experimental results also highlight the
scalability and generalizability of our training data

</details>


### [232] [Domain Adaptive SAR Wake Detection: Leveraging Similarity Filtering and Memory Guidance](https://arxiv.org/abs/2509.12279)
*He Gao,Baoxiang Huang,Milena Radenkovic,Borui Li,Ge Chen*

Main category: cs.CV

TL;DR: 提出SimMemDA框架解决跨模态域适应问题，提升跨模态船舶尾迹检测的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: SAR图像尾迹特征抽象嘈杂难标注，光学图像模型用于SAR图像因域偏移性能下降，需解决跨模态域适应挑战。

Method: 利用WakeGAN进行风格迁移，设计实例级特征相似性过滤机制，引入特征置信度记忆库和K近邻置信度加权融合策略校准伪标签，通过区域混合训练增强泛化能力。

Result: 实验表明SimMemDA方法能提高跨模态船舶尾迹检测任务的准确性和鲁棒性。

Conclusion: 所提SimMemDA方法有效可行。

Abstract: Synthetic Aperture Radar (SAR), with its all-weather and wide-area
observation capabilities, serves as a crucial tool for wake detection. However,
due to its complex imaging mechanism, wake features in SAR images often appear
abstract and noisy, posing challenges for accurate annotation. In contrast,
optical images provide more distinct visual cues, but models trained on optical
data suffer from performance degradation when applied to SAR images due to
domain shift. To address this cross-modal domain adaptation challenge, we
propose a Similarity-Guided and Memory-Guided Domain Adaptation (termed
SimMemDA) framework for unsupervised domain adaptive ship wake detection via
instance-level feature similarity filtering and feature memory guidance.
Specifically, to alleviate the visual discrepancy between optical and SAR
images, we first utilize WakeGAN to perform style transfer on optical images,
generating pseudo-images close to the SAR style. Then, instance-level feature
similarity filtering mechanism is designed to identify and prioritize source
samples with target-like distributions, minimizing negative transfer.
Meanwhile, a Feature-Confidence Memory Bank combined with a K-nearest neighbor
confidence-weighted fusion strategy is introduced to dynamically calibrate
pseudo-labels in the target domain, improving the reliability and stability of
pseudo-labels. Finally, the framework further enhances generalization through
region-mixed training, strategically combining source annotations with
calibrated target pseudo-labels. Experimental results demonstrate that the
proposed SimMemDA method can improve the accuracy and robustness of cross-modal
ship wake detection tasks, validating the effectiveness and feasibility of the
proposed method.

</details>


### [233] [GhostNetV3-Small: A Tailored Architecture and Comparative Study of Distillation Strategies for Tiny Images](https://arxiv.org/abs/2509.12380)
*Florian Zager,Hamza A. A. Gardi*

Main category: cs.CV

TL;DR: 本文探索模型压缩与适配策略，以在资源受限边缘设备实现高效推理。提出GhostNetV3 - Small，对比知识蒸馏技术，结果显示GhostNetV3 - Small表现佳，蒸馏策略降低准确率，表明架构适配更重要。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络因计算需求大而不适用于资源受限边缘设备的问题。

Method: 聚焦GhostNetV3，提出GhostNetV3 - Small以适配低分辨率输入，对比传统知识蒸馏、教师助手和教师集成等知识蒸馏技术。

Result: GhostNetV3 - Small在CIFAR - 10上显著优于原GhostNetV3，准确率达93.94%；所有蒸馏策略相比基线训练都降低了准确率。

Conclusion: 在小规模图像分类任务中，架构适配比蒸馏更有影响，需进一步研究低分辨率领域的有效模型设计和先进蒸馏技术。

Abstract: Deep neural networks have achieved remarkable success across a range of
tasks, however their computational demands often make them unsuitable for
deployment on resource-constrained edge devices. This paper explores strategies
for compressing and adapting models to enable efficient inference in such
environments. We focus on GhostNetV3, a state-of-the-art architecture for
mobile applications, and propose GhostNetV3-Small, a modified variant designed
to perform better on low-resolution inputs such as those in the CIFAR-10
dataset. In addition to architectural adaptation, we provide a comparative
evaluation of knowledge distillation techniques, including traditional
knowledge distillation, teacher assistants, and teacher ensembles. Experimental
results show that GhostNetV3-Small significantly outperforms the original
GhostNetV3 on CIFAR-10, achieving an accuracy of 93.94%. Contrary to
expectations, all examined distillation strategies led to reduced accuracy
compared to baseline training. These findings indicate that architectural
adaptation can be more impactful than distillation in small-scale image
classification tasks, highlighting the need for further research on effective
model design and advanced distillation techniques for low-resolution domains.

</details>


### [234] [Adaptive Sampling Scheduler](https://arxiv.org/abs/2509.12569)
*Qi Wang,Shuliang Zhu,Jinjia Zhou*

Main category: cs.CV

TL;DR: 本文提出适用于多种一致性蒸馏框架的自适应采样调度器，通过三种创新策略提升生成性能，实验验证了其有效性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有一致性蒸馏方法在目标时间步选择上依赖特定策略，限制灵活性，无法充分发挥扩散模型采样潜力。

Method: 提出自适应采样调度器，包含动态目标时间步选择、沿解轨迹优化交替采样、利用平滑裁剪和颜色平衡技术三种策略。

Result: 通过综合实验评估，在多种一致性蒸馏方法中显著提升了生成性能。

Conclusion: 所提自适应采样调度器具有有效性和灵活性，适应性强。

Abstract: Consistent distillation methods have evolved into effective techniques that
significantly accelerate the sampling process of diffusion models. Although
existing methods have achieved remarkable results, the selection of target
timesteps during distillation mainly relies on deterministic or stochastic
strategies, which often require sampling schedulers to be designed specifically
for different distillation processes. Moreover, this pattern severely limits
flexibility, thereby restricting the full sampling potential of diffusion
models in practical applications. To overcome these limitations, this paper
proposes an adaptive sampling scheduler that is applicable to various
consistency distillation frameworks. The scheduler introduces three innovative
strategies: (i) dynamic target timestep selection, which adapts to different
consistency distillation frameworks by selecting timesteps based on their
computed importance; (ii) Optimized alternating sampling along the solution
trajectory by guiding forward denoising and backward noise addition based on
the proposed time step importance, enabling more effective exploration of the
solution space to enhance generation performance; and (iii) Utilization of
smoothing clipping and color balancing techniques to achieve stable and
high-quality generation results at high guidance scales, thereby expanding the
applicability of consistency distillation models in complex generation
scenarios. We validated the effectiveness and flexibility of the adaptive
sampling scheduler across various consistency distillation methods through
comprehensive experimental evaluations. Experimental results consistently
demonstrated significant improvements in generative performance, highlighting
the strong adaptability achieved by our method.

</details>


### [235] [DisorientLiDAR: Physical Attacks on LiDAR-based Localization](https://arxiv.org/abs/2509.12595)
*Yizhen Lao,Yu Zhang,Ziting Wang,Chengbo Wang,Yifei Xue,Wanpeng Shao*

Main category: cs.CV

TL;DR: 提出针对基于LiDAR定位的DisorientLiDAR对抗攻击框架，评估其效果并拓展到现实物理世界攻击。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型易受对抗攻击，自动驾驶汽车定位面临安全挑战，但相关攻击研究少，多数对抗攻击应用于3D感知。

Method: 逆向工程定位模型识别关键关键点并策略性移除，以破坏基于LiDAR的定位。

Result: 在三个点云配准模型上评估，移除含Top - K关键点区域降低配准精度；在Autoware平台验证，隐藏关键区域导致定位漂移；用近红外吸收材料隐藏关键区域在现实中复现攻击效果。

Conclusion: 提出的攻击框架具有真实性和通用性，向现实物理世界攻击迈进了一步。

Abstract: Deep learning models have been shown to be susceptible to adversarial attacks
with visually imperceptible perturbations. Even this poses a serious security
challenge for the localization of self-driving cars, there has been very little
exploration of attack on it, as most of adversarial attacks have been applied
to 3D perception. In this work, we propose a novel adversarial attack framework
called DisorientLiDAR targeting LiDAR-based localization. By
reverse-engineering localization models (e.g., feature extraction networks),
adversaries can identify critical keypoints and strategically remove them,
thereby disrupting LiDAR-based localization. Our proposal is first evaluated on
three state-of-the-art point-cloud registration models (HRegNet, D3Feat, and
GeoTransformer) using the KITTI dataset. Experimental results demonstrate that
removing regions containing Top-K keypoints significantly degrades their
registration accuracy. We further validate the attack's impact on the Autoware
autonomous driving platform, where hiding merely a few critical regions induces
noticeable localization drift. Finally, we extended our attacks to the physical
world by hiding critical regions with near-infrared absorptive materials,
thereby successfully replicate the attack effects observed in KITTI data. This
step has been closer toward the realistic physical-world attack that
demonstrate the veracity and generality of our proposal.

</details>


### [236] [CIARD: Cyclic Iterative Adversarial Robustness Distillation](https://arxiv.org/abs/2509.12633)
*Liming Lu,Shuchao Pang,Xu Zheng,Xiang Gu,Anan Du,Yunhuai Liu,Yongbin Zhou*

Main category: cs.CV

TL;DR: 提出Cyclic Iterative ARD (CIARD)方法解决现有对抗鲁棒性蒸馏方法在干净样本上性能下降问题，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 现有ARD方法会导致干净样本性能下降，原因是双教师模型优化目标分歧和对抗样本影响鲁棒教师模型性能。

Method: 提出CIARD方法，包括多教师框架对比推损失对齐和连续对抗再训练。

Result: 在CIFAR - 10、CIFAR - 100和Tiny - ImageNet上实验，对抗防御率平均提升3.53，干净样本准确率提升5.87。

Conclusion: CIARD建立了平衡模型鲁棒性和泛化性的新基准。

Abstract: Adversarial robustness distillation (ARD) aims to transfer both performance
and robustness from teacher model to lightweight student model, enabling
resilient performance on resource-constrained scenarios. Though existing ARD
approaches enhance student model's robustness, the inevitable by-product leads
to the degraded performance on clean examples. We summarize the causes of this
problem inherent in existing methods with dual-teacher framework as: 1. The
divergent optimization objectives of dual-teacher models, i.e., the clean and
robust teachers, impede effective knowledge transfer to the student model, and
2. The iteratively generated adversarial examples during training lead to
performance deterioration of the robust teacher model. To address these
challenges, we propose a novel Cyclic Iterative ARD (CIARD) method with two key
innovations: a. A multi-teacher framework with contrastive push-loss alignment
to resolve conflicts in dual-teacher optimization objectives, and b. Continuous
adversarial retraining to maintain dynamic teacher robustness against
performance degradation from the varying adversarial examples. Extensive
experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet demonstrate that CIARD
achieves remarkable performance with an average 3.53 improvement in adversarial
defense rates across various attack scenarios and a 5.87 increase in clean
sample accuracy, establishing a new benchmark for balancing model robustness
and generalization. Our code is available at https://github.com/eminentgu/CIARD

</details>


### [237] [Beyond Artificial Misalignment: Detecting and Grounding Semantic-Coordinated Multimodal Manipulations](https://arxiv.org/abs/2509.12653)
*Jinjie Shen,Yaxiong Wang,Lechao Cheng,Nan Pu,Zhun Zhong*

Main category: cs.CV

TL;DR: 本文针对现有多模态数据篡改检测基准与实际篡改模式不符的问题，构建SAMM数据集并提出RamDG框架，实验表明该框架性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态数据篡改检测基准存在与现实篡改模式不一致的问题，不能反映真实情况，需检测语义协调的篡改。

Method: 先构建SAMM数据集，通过两阶段流程生成；再提出RamDG框架，利用外部知识库检索上下文证据辅助检测。

Result: 框架显著优于现有方法，在SAMM数据集上检测准确率比最先进方法高2.06%。

Conclusion: 提出的数据集和框架有效提升了多模态数据篡改检测和定位的性能，相关数据和代码已公开。

Abstract: The detection and grounding of manipulated content in multimodal data has
emerged as a critical challenge in media forensics. While existing benchmarks
demonstrate technical progress, they suffer from misalignment artifacts that
poorly reflect real-world manipulation patterns: practical attacks typically
maintain semantic consistency across modalities, whereas current datasets
artificially disrupt cross-modal alignment, creating easily detectable
anomalies. To bridge this gap, we pioneer the detection of
semantically-coordinated manipulations where visual edits are systematically
paired with semantically consistent textual descriptions. Our approach begins
with constructing the first Semantic-Aligned Multimodal Manipulation (SAMM)
dataset, generated through a two-stage pipeline: 1) applying state-of-the-art
image manipulations, followed by 2) generation of contextually-plausible
textual narratives that reinforce the visual deception. Building on this
foundation, we propose a Retrieval-Augmented Manipulation Detection and
Grounding (RamDG) framework. RamDG commences by harnessing external knowledge
repositories to retrieve contextual evidence, which serves as the auxiliary
texts and encoded together with the inputs through our image forgery grounding
and deep manipulation detection modules to trace all manipulations. Extensive
experiments demonstrate our framework significantly outperforms existing
methods, achieving 2.06\% higher detection accuracy on SAMM compared to
state-of-the-art approaches. The dataset and code are publicly available at
https://github.com/shen8424/SAMM-RamDG-CAP.

</details>


### [238] [MFAF: An EVA02-Based Multi-scale Frequency Attention Fusion Method for Cross-View Geo-Localization](https://arxiv.org/abs/2509.12673)
*YiTong Liu,TianZhu Liu,YanFeng GU*

Main category: cs.CV

TL;DR: 提出基于EVA02的多尺度频率注意力融合（MFAF）方法解决跨视图地理定位难题，实验显示其在无人机定位和导航任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有跨视图地理定位方法在提取特征时忽略空间和语义信息，难以应对物体外观变化和特征提取难题。

Method: 提出MFAF方法，包含多频分支块（MFB）和频率感知空间注意力（FSA）模块。MFB捕获多尺度特征，FSA聚焦关键区域。

Result: 在University - 1652、SUES - 200和Dense - UAV等基准测试中，MFAF方法在无人机定位和导航任务中取得有竞争力的表现。

Conclusion: MFAF方法能有效解决跨视图地理定位问题，提升特征表示的一致性和鲁棒性。

Abstract: Cross-view geo-localization aims to determine the geographical location of a
query image by matching it against a gallery of images. This task is
challenging due to the significant appearance variations of objects observed
from variable views, along with the difficulty in extracting discriminative
features. Existing approaches often rely on extracting features through feature
map segmentation while neglecting spatial and semantic information. To address
these issues, we propose the EVA02-based Multi-scale Frequency Attention Fusion
(MFAF) method. The MFAF method consists of Multi-Frequency Branch-wise Block
(MFB) and the Frequency-aware Spatial Attention (FSA) module. The MFB block
effectively captures both low-frequency structural features and high-frequency
edge details across multiple scales, improving the consistency and robustness
of feature representations across various viewpoints. Meanwhile, the FSA module
adaptively focuses on the key regions of frequency features, significantly
mitigating the interference caused by background noise and viewpoint
variability. Extensive experiments on widely recognized benchmarks, including
University-1652, SUES-200, and Dense-UAV, demonstrate that the MFAF method
achieves competitive performance in both drone localization and drone
navigation tasks.

</details>


### [239] [A Comparative Study of YOLOv8 to YOLOv11 Performance in Underwater Vision Tasks](https://arxiv.org/abs/2509.12682)
*Gordon Hung,Ivan Felipe Rodriguez*

Main category: cs.CV

TL;DR: 本文针对水下图像特点，用两个数据集对比不同版本YOLO模型在水下图像的性能，发现精度在YOLOv9后饱和，YOLOv10速度与精度平衡好。


<details>
  <summary>Details</summary>
Motivation: 水下图像有光衰减等问题且AUV计算资源有限，陆基基准未明确YOLO系列在海洋领域表现。

Method: 整理两个数据集，设置四种训练机制，用相同超参数训练YOLOv8 - s到YOLOv11 - s，评估多项指标，用Grad - CAM可视化。

Result: 精度在YOLOv9后饱和，推理速度明显提升。

Conclusion: 首次对比YOLO变体在水下图像性能，YOLOv10适合AUV部署，提供开放可复现基准和代码库促进海洋视觉研究。

Abstract: Autonomous underwater vehicles (AUVs) increasingly rely on on-board
computer-vision systems for tasks such as habitat mapping, ecological
monitoring, and infrastructure inspection. However, underwater imagery is
hindered by light attenuation, turbidity, and severe class imbalance, while the
computational resources available on AUVs are limited. One-stage detectors from
the YOLO family are attractive because they fuse localization and
classification in a single, low-latency network; however, their terrestrial
benchmarks (COCO, PASCAL-VOC, Open Images) leave open the question of how
successive YOLO releases perform in the marine domain. We curate two openly
available datasets that span contrasting operating conditions: a Coral Disease
set (4,480 images, 18 classes) and a Fish Species set (7,500 images, 20
classes). For each dataset, we create four training regimes (25 %, 50 %, 75 %,
100 % of the images) while keeping balanced validation and test partitions
fixed. We train YOLOv8-s, YOLOv9-s, YOLOv10-s, and YOLOv11-s with identical
hyperparameters (100 epochs, 640 px input, batch = 16, T4 GPU) and evaluate
precision, recall, mAP50, mAP50-95, per-image inference time, and
frames-per-second (FPS). Post-hoc Grad-CAM visualizations probe feature
utilization and localization faithfulness. Across both datasets, accuracy
saturates after YOLOv9, suggesting architectural innovations primarily target
efficiency rather than accuracy. Inference speed, however, improves markedly.
Our results (i) provide the first controlled comparison of recent YOLO variants
on underwater imagery, (ii) show that lightweight YOLOv10 offers the best
speed-accuracy trade-off for embedded AUV deployment, and (iii) deliver an
open, reproducible benchmark and codebase to accelerate future marine-vision
research.

</details>


### [240] [Defense-to-Attack: Bypassing Weak Defenses Enables Stronger Jailbreaks in Vision-Language Models](https://arxiv.org/abs/2509.12724)
*Yunhan Zhao,Xiang Zheng,Xingjun Ma*

Main category: cs.CV

TL;DR: 本文揭示将弱防御融入攻击流程可提升视觉语言模型越狱攻击的效果和效率，提出Defense2Attack方法，经实验验证其性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型越狱攻击的有效性和效率有待提高。

Method: 提出Defense2Attack方法，包含视觉优化器、文本优化器和红队后缀生成器三个关键组件。

Result: 在四个视觉语言模型和四个安全基准上评估，Defense2Attack单次尝试就取得了优越的越狱性能，超越了需要多次尝试的现有攻击方法。

Conclusion: 为视觉语言模型越狱提供了新视角。

Abstract: Despite their superb capabilities, Vision-Language Models (VLMs) have been
shown to be vulnerable to jailbreak attacks. While recent jailbreaks have
achieved notable progress, their effectiveness and efficiency can still be
improved. In this work, we reveal an interesting phenomenon: incorporating weak
defense into the attack pipeline can significantly enhance both the
effectiveness and the efficiency of jailbreaks on VLMs. Building on this
insight, we propose Defense2Attack, a novel jailbreak method that bypasses the
safety guardrails of VLMs by leveraging defensive patterns to guide jailbreak
prompt design. Specifically, Defense2Attack consists of three key components:
(1) a visual optimizer that embeds universal adversarial perturbations with
affirmative and encouraging semantics; (2) a textual optimizer that refines the
input using a defense-styled prompt; and (3) a red-team suffix generator that
enhances the jailbreak through reinforcement fine-tuning. We empirically
evaluate our method on four VLMs and four safety benchmarks. The results
demonstrate that Defense2Attack achieves superior jailbreak performance in a
single attempt, outperforming state-of-the-art attack methods that often
require multiple tries. Our work offers a new perspective on jailbreaking VLMs.

</details>


### [241] [Uncertainty-Aware Hourly Air Temperature Mapping at 2 km Resolution via Physics-Guided Deep Learning](https://arxiv.org/abs/2509.12329)
*Shengjie Kris Liu,Siqin Wang,Lu Zhang*

Main category: cs.CV

TL;DR: 提出Amplifier Air - Transformer方法生成美国本土2公里分辨率的每小时气温数据，精度达1.93°C，可扩展用于无缝气温监测。


<details>
  <summary>Details</summary>
Motivation: 现有数据源无法在时空上提供无缝气温数据，需新方法生成高时空分辨率气温数据。

Method: 提出Amplifier Air - Transformer方法，先重建被云遮挡的GOES - 16地表温度数据，再将重建的地表温度转换为气温，并用深度集成学习估计预测不确定性。

Result: 基于777亿地表温度像素和1.55亿气象站气温记录进行构建和测试，站基验证中每小时气温映射精度达1.93°C。

Conclusion: 该方法简化了地表温度重建和气温预测，可扩展到其他卫星源进行高时空分辨率的无缝气温监测。

Abstract: Near-surface air temperature is a key physical property of the Earth's
surface. Although weather stations offer continuous monitoring and satellites
provide broad spatial coverage, no single data source offers seamless data in a
spatiotemporal fashion. Here, we propose a data-driven, physics-guided deep
learning approach to generate hourly air temperature data at 2 km resolution
over the contiguous United States. The approach, called Amplifier
Air-Transformer, first reconstructs GOES-16 surface temperature data obscured
by clouds. It does so through a neural network encoded with the annual
temperature cycle, incorporating a linear term to amplify ERA5 temperature
values at finer scales and convolutional layers to capture spatiotemporal
variations. Then, another neural network transforms the reconstructed surface
temperature into air temperature by leveraging its latent relationship with key
Earth surface properties. The approach is further enhanced with predictive
uncertainty estimation through deep ensemble learning to improve reliability.
The proposed approach is built and tested on 77.7 billion surface temperature
pixels and 155 million air temperature records from weather stations across the
contiguous United States (2018-2024), achieving hourly air temperature mapping
accuracy of 1.93 C in station-based validation. The proposed approach
streamlines surface temperature reconstruction and air temperature prediction,
and it can be extended to other satellite sources for seamless air temperature
monitoring at high spatiotemporal resolution. The generated data of this study
can be downloaded at https://doi.org/10.5281/zenodo.15252812, and the project
webpage can be found at https://skrisliu.com/HourlyAirTemp2kmUSA/.

</details>


### [242] [Cott-ADNet: Lightweight Real-Time Cotton Boll and Flower Detection Under Field Conditions](https://arxiv.org/abs/2509.12442)
*Rui-Feng Wang,Mingrui Xu,Matthew C Bauer,Iago Beffart Schardong,Xiaowen Ma,Kangning Cui*

Main category: cs.CV

TL;DR: 本文提出用于复杂田间条件下棉花铃和花识别的轻量级实时检测器Cott - ADNet，实验证明其准确高效，为自动棉花收获和表型分析提供基础。


<details>
  <summary>Details</summary>
Motivation: 棉花人工采摘劳动强度大、效率低且易错过最佳收获期，准确识别棉铃及其成熟度对自动化、产量估计和育种研究至关重要。

Method: 基于YOLOv11n提出Cott - ADNet，改进卷积设计，引入NeLU增强的全局注意力机制和扩张感受野SPPF模块；整理4966张标注图像数据集并发布1216张外部验证集。

Result: Cott - ADNet在仅7.5 GFLOPs计算量下，精确率91.5%、召回率89.8%、mAP50为93.3%、mAP为71.3%、F1分数为90.6%，在多尺度和旋转变化下性能稳定。

Conclusion: Cott - ADNet是适用于田间部署的准确高效解决方案，为自动棉花收获和高通量表型分析提供可靠依据。

Abstract: Cotton is one of the most important natural fiber crops worldwide, yet
harvesting remains limited by labor-intensive manual picking, low efficiency,
and yield losses from missing the optimal harvest window. Accurate recognition
of cotton bolls and their maturity is therefore essential for automation, yield
estimation, and breeding research. We propose Cott-ADNet, a lightweight
real-time detector tailored to cotton boll and flower recognition under complex
field conditions. Building on YOLOv11n, Cott-ADNet enhances spatial
representation and robustness through improved convolutional designs, while
introducing two new modules: a NeLU-enhanced Global Attention Mechanism to
better capture weak and low-contrast features, and a Dilated Receptive Field
SPPF to expand receptive fields for more effective multi-scale context modeling
at low computational cost. We curate a labeled dataset of 4,966 images, and
release an external validation set of 1,216 field images to support future
research. Experiments show that Cott-ADNet achieves 91.5% Precision, 89.8%
Recall, 93.3% mAP50, 71.3% mAP, and 90.6% F1-Score with only 7.5 GFLOPs,
maintaining stable performance under multi-scale and rotational variations.
These results demonstrate Cott-ADNet as an accurate and efficient solution for
in-field deployment, and thus provide a reliable basis for automated cotton
harvesting and high-throughput phenotypic analysis. Code and dataset is
available at https://github.com/SweefongWong/Cott-ADNet.

</details>


### [243] [CECT-Mamba: a Hierarchical Contrast-enhanced-aware Model for Pancreatic Tumor Subtyping from Multi-phase CECT](https://arxiv.org/abs/2509.12777)
*Zhifang Gong,Shuo Gao,Ben Zhao,Yingjing Xu,Yijun Yang,Shenghong Ju,Guangquan Zhou*

Main category: cs.CV

TL;DR: 提出结合多期CECT数据区分胰腺肿瘤亚型的自动方法，实验效果良好。


<details>
  <summary>Details</summary>
Motivation: 胰腺肿瘤异质性高，以往方法难以有效利用多期CECT的上下文信息，限制诊断性能。

Method: 首次结合多期CECT数据，用Mamba进行时空建模；提出双层次增强感知Mamba模块探索病灶期内和期间对比变化；加入相似性引导细化模块；设计空间互补积分器和多粒度融合模块。

Result: 在270例临床病例数据集上，区分胰腺导管腺癌和胰腺神经内分泌肿瘤的准确率达97.4%，AUC为98.6%。

Conclusion: 该方法可作为更准确高效的胰腺肿瘤亚型分类工具。

Abstract: Contrast-enhanced computed tomography (CECT) is the primary imaging technique
that provides valuable spatial-temporal information about lesions, enabling the
accurate diagnosis and subclassification of pancreatic tumors. However, the
high heterogeneity and variability of pancreatic tumors still pose substantial
challenges for precise subtyping diagnosis. Previous methods fail to
effectively explore the contextual information across multiple CECT phases
commonly used in radiologists' diagnostic workflows, thereby limiting their
performance. In this paper, we introduce, for the first time, an automatic way
to combine the multi-phase CECT data to discriminate between pancreatic tumor
subtypes, among which the key is using Mamba with promising learnability and
simplicity to encourage both temporal and spatial modeling from multi-phase
CECT. Specifically, we propose a dual hierarchical contrast-enhanced-aware
Mamba module incorporating two novel spatial and temporal sampling sequences to
explore intra and inter-phase contrast variations of lesions. A
similarity-guided refinement module is also imposed into the temporal scanning
modeling to emphasize the learning on local tumor regions with more obvious
temporal variations. Moreover, we design the space complementary integrator and
multi-granularity fusion module to encode and aggregate the semantics across
different scales, achieving more efficient learning for subtyping pancreatic
tumors. The experimental results on an in-house dataset of 270 clinical cases
achieve an accuracy of 97.4% and an AUC of 98.6% in distinguishing between
pancreatic ductal adenocarcinoma (PDAC) and pancreatic neuroendocrine tumors
(PNETs), demonstrating its potential as a more accurate and efficient tool.

</details>


### [244] [Data Scaling Laws for Radiology Foundation Models](https://arxiv.org/abs/2509.12818)
*Maximilian Ilse,Harshita Sharma,Anton Schwaighofer,Sam Bond-Taylor,Fernando Pérez-García,Olesya Melnichenko,Anne-Marie G. Sykes,Kelly K. Horst,Ashish Khandelwal,Maxwell Reynolds,Maria T. Wetscherek,Noel C. F. Codella,Javier Alvarez-Valle,Korfiatis Panagiotis,Valentina Salvatelli*

Main category: cs.CV

TL;DR: 研究在单机构多达350万张胸部X光片上对两种视觉编码器持续预训练，评估多任务表现，发现不同编码器优势任务及结构化监督价值，少量样本可超开源模型，凸显中心特定持续预训练效用。


<details>
  <summary>Details</summary>
Motivation: 医学影像基础模型受小数据集限制，需研究数据规模和预训练范式对性能的影响。

Method: 在单机构350万张胸部X光片上对MedImageInsight (MI2)和RAD - DINO持续预训练，保持计算和评估协议不变，评估分类、分割和报告生成任务。

Result: MI2在与发现相关任务上扩展更有效，RAD - DINO在与管相关任务上更强；用UniCL对MI2结合报告和结构化标签持续预训练可提升性能；部分任务3万张域内样本就能超越开源模型。

Conclusion: 中心特定的持续预训练很有用，医疗机构利用域内数据可显著提升性能。

Abstract: Foundation vision encoders such as CLIP and DINOv2, trained on web-scale
data, exhibit strong transfer performance across tasks and datasets. However,
medical imaging foundation models remain constrained by smaller datasets,
limiting our understanding of how data scale and pretraining paradigms affect
performance in this setting. In this work, we systematically study continual
pretraining of two vision encoders, MedImageInsight (MI2) and RAD-DINO
representing the two major encoder paradigms CLIP and DINOv2, on up to 3.5M
chest x-rays from a single institution, holding compute and evaluation
protocols constant. We evaluate on classification (radiology findings, lines
and tubes), segmentation (lines and tubes), and radiology report generation.
While prior work has primarily focused on tasks related to radiology findings,
we include lines and tubes tasks to counterbalance this bias and evaluate a
model's ability to extract features that preserve continuity along elongated
structures. Our experiments show that MI2 scales more effectively for
finding-related tasks, while RAD-DINO is stronger on tube-related tasks.
Surprisingly, continually pretraining MI2 with both reports and structured
labels using UniCL improves performance, underscoring the value of structured
supervision at scale. We further show that for some tasks, as few as 30k
in-domain samples are sufficient to surpass open-weights foundation models.
These results highlight the utility of center-specific continual pretraining,
enabling medical institutions to derive significant performance gains by
utilizing in-domain data.

</details>


### [245] [Runge-Kutta Approximation and Decoupled Attention for Rectified Flow Inversion and Semantic Editing](https://arxiv.org/abs/2509.12888)
*Weiming Chen,Zhihan Zhu,Yijia Wang,Zhihai He*

Main category: cs.CV

TL;DR: 本文指出整流流（RF）模型在实际应用中的两个挑战，提出高效高阶反演方法和DDTA机制，实验表明方法在保真度和可编辑性上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 解决整流流模型在实际应用中存在的低反演精度和多模态注意力纠缠问题。

Method: 提出基于微分方程龙格 - 库塔求解器的高效高阶反演方法；引入解耦扩散变压器注意力（DDTA）机制。

Result: 在图像重建和文本引导编辑任务的大量实验中，方法在保真度和可编辑性方面达到了最先进性能。

Conclusion: 所提出的方法有效解决了整流流模型面临的挑战，具有良好的性能。

Abstract: Rectified flow (RF) models have recently demonstrated superior generative
performance compared to DDIM-based diffusion models. However, in real-world
applications, they suffer from two major challenges: (1) low inversion accuracy
that hinders the consistency with the source image, and (2) entangled
multimodal attention in diffusion transformers, which hinders precise attention
control. To address the first challenge, we propose an efficient high-order
inversion method for rectified flow models based on the Runge-Kutta solver of
differential equations. To tackle the second challenge, we introduce Decoupled
Diffusion Transformer Attention (DDTA), a novel mechanism that disentangles
text and image attention inside the multimodal diffusion transformers, enabling
more precise semantic control. Extensive experiments on image reconstruction
and text-guided editing tasks demonstrate that our method achieves
state-of-the-art performance in terms of fidelity and editability. Code is
available at https://github.com/wmchen/RKSovler_DDTA.

</details>


### [246] [Cross-Layer Vision Smoothing: Enhancing Visual Understanding via Sustained Focus on Key Objects in Large Vision-Language Models](https://arxiv.org/abs/2509.12897)
*Jianfei Zhao,Feng Zhang,Xin Sun,Lingxing Kong,Zhixing Tan,Chong Feng*

Main category: cs.CV

TL;DR: 针对大视觉语言模型对关键对象注意力短暂问题，提出跨层视觉平滑方法CLVS，经实验验证有效且具泛化性，在多种视觉理解任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型对关键对象注意力短暂，假设持续关注关键对象可提升其视觉能力，由此提出改进方法。

Method: 提出Cross - Layer Vision Smoothing (CLVS)，引入视觉记忆平滑各层注意力分布，用无位置偏差视觉注意力初始化第一层记忆，后续层联合考虑前层记忆并迭代更新，用不确定性指示视觉理解完成并终止平滑过程。

Result: 在三个大视觉语言模型的四个基准测试中，CLVS方法有效且具泛化性，在多种视觉理解任务中达最优性能，在关系和属性理解方面提升显著。

Conclusion: CLVS方法能有效提升大视觉语言模型的视觉能力，在视觉理解任务中有出色表现。

Abstract: Large Vision-Language Models (LVLMs) can accurately locate key objects in
images, yet their attention to these objects tends to be very brief. Motivated
by the hypothesis that sustained focus on key objects can improve LVLMs' visual
capabilities, we propose Cross-Layer Vision Smoothing (CLVS). The core idea of
CLVS is to incorporate a vision memory that smooths the attention distribution
across layers. Specifically, we initialize this vision memory with
position-unbiased visual attention in the first layer. In subsequent layers,
the model's visual attention jointly considers the vision memory from previous
layers, while the memory is updated iteratively, thereby maintaining smooth
attention on key objects. Given that visual understanding primarily occurs in
the early and middle layers of the model, we use uncertainty as an indicator of
completed visual understanding and terminate the smoothing process accordingly.
Experiments on four benchmarks across three LVLMs confirm the effectiveness and
generalizability of our method. CLVS achieves state-of-the-art performance on a
variety of visual understanding tasks, with particularly significant
improvements in relation and attribute understanding.

</details>


### [247] [BATR-FST: Bi-Level Adaptive Token Refinement for Few-Shot Transformers](https://arxiv.org/abs/2509.12768)
*Mohammed Al-Habib,Zuping Zhang,Abdulrahman Noman*

Main category: cs.CV

TL;DR: 提出BATR - FST方法解决ViTs在少样本学习中的问题，实验表明该方法在少样本分类中效果优越。


<details>
  <summary>Details</summary>
Motivation: ViTs在少样本学习中存在细化token交互、利用有限训练数据和建立强归纳偏置等挑战，现有方法有局限性。

Method: 提出两阶段的BATR - FST方法，预训练阶段用MIM提供可迁移的patch级表示，元微调阶段采用双层自适应token细化模块、图token传播和类分离惩罚。

Result: 在三个基准少样本数据集的1 - shot和5 - shot场景中取得优越结果。

Conclusion: BATR - FST能提升基于transformer的少样本分类性能。

Abstract: Vision Transformers (ViTs) have shown significant promise in computer vision
applications. However, their performance in few-shot learning is limited by
challenges in refining token-level interactions, struggling with limited
training data, and developing a strong inductive bias. Existing methods often
depend on inflexible token matching or basic similarity measures, which limit
the effective incorporation of global context and localized feature refinement.
To address these challenges, we propose Bi-Level Adaptive Token Refinement for
Few-Shot Transformers (BATR-FST), a two-stage approach that progressively
improves token representations and maintains a robust inductive bias for
few-shot classification. During the pre-training phase, Masked Image Modeling
(MIM) provides Vision Transformers (ViTs) with transferable patch-level
representations by recreating masked image regions, providing a robust basis
for subsequent adaptation. In the meta-fine-tuning phase, BATR-FST incorporates
a Bi-Level Adaptive Token Refinement module that utilizes Token Clustering to
capture localized interactions, Uncertainty-Aware Token Weighting to prioritize
dependable features, and a Bi-Level Attention mechanism to balance
intra-cluster and inter-cluster relationships, thereby facilitating thorough
token refinement. Furthermore, Graph Token Propagation ensures semantic
consistency between support and query instances, while a Class Separation
Penalty preserves different class borders, enhancing discriminative capability.
Extensive experiments on three benchmark few-shot datasets demonstrate that
BATR-FST achieves superior results in both 1-shot and 5-shot scenarios and
improves the few-shot classification via transformers.

</details>


### [248] [Dual-Stage Reweighted MoE for Long-Tailed Egocentric Mistake Detection](https://arxiv.org/abs/2509.12990)
*Boyu Han,Qianqian Xu,Shilong Bao,Zhiyong Yang,Sicong Li,Qingming Huang*

Main category: cs.CV

TL;DR: 提出DR - MoE框架解决从第一人称视角视频数据判断用户动作是否错误的问题，性能表现好且代码开源。


<details>
  <summary>Details</summary>
Motivation: 解决从第一人称视角视频数据判断用户动作是否错误时，处理细微和不常见错误带来的挑战。

Method: 提出Dual - Stage Reweighted Mixture - of - Experts (DR - MoE)框架，第一阶段用冻结的ViViT模型和LoRA调优的ViViT模型提取特征并通过特征级专家模块组合；第二阶段用不同目标训练三个分类器，其预测结果通过分类级专家模块融合。

Result: 所提方法取得了良好性能，尤其在识别罕见和模糊的错误实例方面表现出色。

Conclusion: DR - MoE框架能有效解决从第一人称视角视频数据判断用户动作错误的问题。

Abstract: In this report, we address the problem of determining whether a user performs
an action incorrectly from egocentric video data. To handle the challenges
posed by subtle and infrequent mistakes, we propose a Dual-Stage Reweighted
Mixture-of-Experts (DR-MoE) framework. In the first stage, features are
extracted using a frozen ViViT model and a LoRA-tuned ViViT model, which are
combined through a feature-level expert module. In the second stage, three
classifiers are trained with different objectives: reweighted cross-entropy to
mitigate class imbalance, AUC loss to improve ranking under skewed
distributions, and label-aware loss with sharpness-aware minimization to
enhance calibration and generalization. Their predictions are fused using a
classification-level expert module. The proposed method achieves strong
performance, particularly in identifying rare and ambiguous mistake instances.
The code is available at https://github.com/boyuh/DR-MoE.

</details>


### [249] [MMMS: Multi-Modal Multi-Surface Interactive Segmentation](https://arxiv.org/abs/2509.12963)
*Robin Schön,Julian Lorenz,Katja Ludwig,Daniel Kienzle,Rainer Lienhart*

Main category: cs.CV

TL;DR: 提出基于用户点击交互式创建分割掩码的方法，用于多表面分割，有新评估指标和多模态输入，展示多模态融合策略有效性。


<details>
  <summary>Details</summary>
Motivation: 解决同一图像中多个表面分割难题，考虑表面纠缠相邻情况。

Method: 构建网络架构，输入RGB图像、非RGB模态、错误掩码和编码点击，预测改进分割掩码，架构满足特定条件。

Result: 多模态融合策略有效，使用额外模态减少点击数，RGB基线在单掩码分割场景有竞争力。

Conclusion: 所提多模态多表面交互式分割方法有效，RGB基线在特定场景表现良好。

Abstract: In this paper, we present a method to interactively create segmentation masks
on the basis of user clicks. We pay particular attention to the segmentation of
multiple surfaces that are simultaneously present in the same image. Since
these surfaces may be heavily entangled and adjacent, we also present a novel
extended evaluation metric that accounts for the challenges of this scenario.
Additionally, the presented method is able to use multi-modal inputs to
facilitate the segmentation task. At the center of this method is a network
architecture which takes as input an RGB image, a number of non-RGB modalities,
an erroneous mask, and encoded clicks. Based on this input, the network
predicts an improved segmentation mask. We design our architecture such that it
adheres to two conditions: (1) The RGB backbone is only available as a
black-box. (2) To reduce the response time, we want our model to integrate the
interaction-specific information after the image feature extraction and the
multi-modal fusion. We refer to the overall task as Multi-Modal Multi-Surface
interactive segmentation (MMMS). We are able to show the effectiveness of our
multi-modal fusion strategy. Using additional modalities, our system reduces
the NoC@90 by up to 1.28 clicks per surface on average on DeLiVER and up to
1.19 on MFNet. On top of this, we are able to show that our RGB-only baseline
achieves competitive, and in some cases even superior performance when tested
in a classical, single-mask interactive segmentation scenario.

</details>


### [250] [Improving Accuracy and Efficiency of Implicit Neural Representations: Making SIREN a WINNER](https://arxiv.org/abs/2509.12980)
*Hemanth Chandravamsi,Dhanush V. Shenoy,Steven H. Frankel*

Main category: cs.CV

TL;DR: 本文指出并解决了正弦表示网络（SIREN）的基本局限，提出WINNER方法，在音频、图像和3D形状拟合任务中取得先进成果。


<details>
  <summary>Details</summary>
Motivation: SIREN在未恰当初始化时，难以拟合频率支持范围外的信号，会出现“频谱瓶颈”现象。

Method: 提出WINNER方法，用高斯噪声扰动SIREN的均匀初始化权重，噪声尺度由目标信号的频谱质心自适应确定。

Result: 在音频拟合上达到了先进水平，在图像和3D形状拟合任务中比基础SIREN有显著提升。

Conclusion: WINNER为优化深度神经网络训练的自适应、目标感知初始化策略提供了新途径。

Abstract: We identify and address a fundamental limitation of sinusoidal representation
networks (SIRENs), a class of implicit neural representations. SIRENs Sitzmann
et al. (2020), when not initialized appropriately, can struggle at fitting
signals that fall outside their frequency support. In extreme cases, when the
network's frequency support misaligns with the target spectrum, a 'spectral
bottleneck' phenomenon is observed, where the model yields to a near-zero
output and fails to recover even the frequency components that are within its
representational capacity. To overcome this, we propose WINNER - Weight
Initialization with Noise for Neural Representations. WINNER perturbs uniformly
initialized weights of base SIREN with Gaussian noise - whose noise scales are
adaptively determined by the spectral centroid of the target signal. Similar to
random Fourier embeddings, this mitigates 'spectral bias' but without
introducing additional trainable parameters. Our method achieves
state-of-the-art audio fitting and significant gains in image and 3D shape
fitting tasks over base SIREN. Beyond signal fitting, WINNER suggests new
avenues in adaptive, target-aware initialization strategies for optimizing deep
neural network training. For code and data visit
cfdlabtechnion.github.io/siren_square/.

</details>


### [251] [Perception Before Reasoning: Two-Stage Reinforcement Learning for Visual Reasoning in Vision-Language Models](https://arxiv.org/abs/2509.13031)
*Yan Chen,Long Li,Teng Xi,Long Zeng,Jingdong Wang*

Main category: cs.CV

TL;DR: 提出两阶段强化学习框架提升视觉语言模型感知和推理能力，实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 直接将大语言模型的强化学习方法移植到视觉语言模型效果不佳，因视觉语言模型任务更复杂，需先准确感知理解视觉输入。

Method: 提出两阶段强化学习框架，先进行数据集级采样，第一阶段提升视觉感知，第二阶段增强推理能力。

Result: 得到PeBR - R1模型，在七个基准数据集实验显示该方法有效，PeBR - R1在多种视觉推理任务中表现优越。

Conclusion: 所提两阶段强化学习框架能有效提升视觉语言模型的感知和推理能力。

Abstract: Reinforcement learning (RL) has proven highly effective in eliciting the
reasoning capabilities of large language models (LLMs). Inspired by this
success, recent studies have explored applying similar techniques to
vision-language models (VLMs), aiming to enhance their reasoning performance.
However, directly transplanting RL methods from LLMs to VLMs is suboptimal, as
the tasks faced by VLMs are inherently more complex. Specifically, VLMs must
first accurately perceive and understand visual inputs before reasoning can be
effectively performed. To address this challenge, we propose a two-stage
reinforcement learning framework designed to jointly enhance both the
perceptual and reasoning capabilities of VLMs. To mitigate the vanishing
advantage issue commonly observed in RL training, we first perform
dataset-level sampling to selectively strengthen specific capabilities using
distinct data sources. During training, the first stage focuses on improving
the model's visual perception through coarse- and fine-grained visual
understanding, while the second stage targets the enhancement of reasoning
abilities. After the proposed two-stage reinforcement learning process, we
obtain PeBR-R1, a vision-language model with significantly enhanced perceptual
and reasoning capabilities. Experimental results on seven benchmark datasets
demonstrate the effectiveness of our approach and validate the superior
performance of PeBR-R1 across diverse visual reasoning tasks.

</details>


### [252] [TFANet: Three-Stage Image-Text Feature Alignment Network for Robust Referring Image Segmentation](https://arxiv.org/abs/2509.13070)
*Qianqi Lu,Yuxiang Xie,Jing Zhang,Shiwei Zou,Yan Chen,Xidao Luan*

Main category: cs.CV

TL;DR: 提出TFANet解决指称图像分割中多模态不对齐和语言语义损失问题，分三阶段增强多模态对齐。


<details>
  <summary>Details</summary>
Motivation: 现有指称图像分割方法在复杂场景下存在多模态不对齐和语言语义损失，目标定位错误或分割不完整。

Method: 提出TFANet，分知识增强（KPS）、知识融合（KFS）、知识强化（KIS）三阶段。KPS设计MLAM模块实现多尺度双向语义交换；KFS通过CFSM模块加强特征对齐；KIS提出WFDM模块补偿语义损失。

Result: 未提及具体结果。

Conclusion: 未提及具体结论。

Abstract: Referring Image Segmentation (RIS) is a task that segments image regions
based on language expressions, requiring fine-grained alignment between two
modalities. However, existing methods often struggle with multimodal
misalignment and language semantic loss, especially in complex scenes
containing multiple visually similar objects, where uniquely described targets
are frequently mislocalized or incompletely segmented. To tackle these
challenges, this paper proposes TFANet, a Three-stage Image-Text Feature
Alignment Network that systematically enhances multimodal alignment through a
hierarchical framework comprising three stages: Knowledge Plus Stage (KPS),
Knowledge Fusion Stage (KFS), and Knowledge Intensification Stage (KIS). In the
first stage, we design the Multiscale Linear Cross-Attention Module (MLAM),
which facilitates bidirectional semantic exchange between visual features and
textual representations across multiple scales. This establishes rich and
efficient alignment between image regions and different granularities of
linguistic descriptions. Subsequently, the KFS further strengthens feature
alignment through the Cross-modal Feature Scanning Module (CFSM), which applies
multimodal selective scanning to capture long-range dependencies and construct
a unified multimodal representation. This is essential for modeling long-range
cross-modal dependencies and enhancing alignment accuracy in complex scenes.
Finally, in the KIS, we propose the Word-level Linguistic Feature-guided
Semantic Deepening Module (WFDM) to compensate for semantic degradation
introduced in earlier stages.

</details>


### [253] [Curriculum Multi-Task Self-Supervision Improves Lightweight Architectures for Onboard Satellite Hyperspectral Image Segmentation](https://arxiv.org/abs/2509.13229)
*Hugo Carlesso,Josiane Mothe,Radu Tudor Ionescu*

Main category: cs.CV

TL;DR: 提出用于高光谱成像分析的课程多任务自监督学习框架CMTSSL，在四个公开数据集验证效果好，代码开源。


<details>
  <summary>Details</summary>
Motivation: 高光谱成像数据维度高、卫星系统数据传输慢，需紧凑高效模型支持星载处理并减少冗余数据传输。

Method: 引入CMTSSL框架，结合掩码图像建模与解耦的空间和光谱拼图，采用课程学习策略逐步增加数据复杂度。

Result: 在四个公开基准数据集验证，下游分割任务有持续提升，所用架构比一些先进模型轻超16000倍。

Conclusion: CMTSSL在轻量级架构的通用表征学习方面对实际高光谱成像应用有潜力。

Abstract: Hyperspectral imaging (HSI) captures detailed spectral signatures across
hundreds of contiguous bands per pixel, being indispensable for remote sensing
applications such as land-cover classification, change detection, and
environmental monitoring. Due to the high dimensionality of HSI data and the
slow rate of data transfer in satellite-based systems, compact and efficient
models are required to support onboard processing and minimize the transmission
of redundant or low-value data, e.g. cloud-covered areas. To this end, we
introduce a novel curriculum multi-task self-supervised learning (CMTSSL)
framework designed for lightweight architectures for HSI analysis. CMTSSL
integrates masked image modeling with decoupled spatial and spectral jigsaw
puzzle solving, guided by a curriculum learning strategy that progressively
increases data complexity during self-supervision. This enables the encoder to
jointly capture fine-grained spectral continuity, spatial structure, and global
semantic features. Unlike prior dual-task SSL methods, CMTSSL simultaneously
addresses spatial and spectral reasoning within a unified and computationally
efficient design, being particularly suitable for training lightweight models
for onboard satellite deployment. We validate our approach on four public
benchmark datasets, demonstrating consistent gains in downstream segmentation
tasks, using architectures that are over 16,000x lighter than some
state-of-the-art models. These results highlight the potential of CMTSSL in
generalizable representation learning with lightweight architectures for
real-world HSI applications. Our code is publicly available at
https://github.com/hugocarlesso/CMTSSL.

</details>


### [254] [Hierarchical Deep Fusion Framework for Multi-dimensional Facial Forgery Detection -- The 2024 Global Deepfake Image Detection Challenge](https://arxiv.org/abs/2509.13107)
*Kohou Wang,Huan Hu,Xiang Liu,Zezhou Chen,Ping Chen,Zhaoxiang Liu,Shiguo Lian*

Main category: cs.CV

TL;DR: 本文提出用于面部伪造检测的分层深度融合框架HDFF，结合多个预训练子模型，在比赛中取得良好成绩。


<details>
  <summary>Details</summary>
Motivation: 先进的深度伪造技术对数字安全和真实性构成挑战，需要强大且通用的模型来检测各种伪造手法。

Method: 引入基于集成的深度学习架构HDFF，集成Swin - MLP、CoAtNet、EfficientNetV2和DaViT四个预训练子模型，在MultiFFDI数据集上多阶段微调，拼接特征并训练最终分类器层。

Result: 在比赛私有排行榜上获得0.96852的分数，在184支队伍中排名第20。

Conclusion: 分层融合方法对复杂图像分类任务有效。

Abstract: The proliferation of sophisticated deepfake technology poses significant
challenges to digital security and authenticity. Detecting these forgeries,
especially across a wide spectrum of manipulation techniques, requires robust
and generalized models. This paper introduces the Hierarchical Deep Fusion
Framework (HDFF), an ensemble-based deep learning architecture designed for
high-performance facial forgery detection. Our framework integrates four
diverse pre-trained sub-models, Swin-MLP, CoAtNet, EfficientNetV2, and DaViT,
which are meticulously fine-tuned through a multi-stage process on the
MultiFFDI dataset. By concatenating the feature representations from these
specialized models and training a final classifier layer, HDFF effectively
leverages their collective strengths. This approach achieved a final score of
0.96852 on the competition's private leaderboard, securing the 20th position
out of 184 teams, demonstrating the efficacy of hierarchical fusion for complex
image classification tasks.

</details>


### [255] [Intelligent Vacuum Thermoforming Process](https://arxiv.org/abs/2509.13250)
*Andi Kuswoyo,Christos Margadji,Sebastian W. Pattinson*

Main category: cs.CV

TL;DR: 研究引入基于视觉的质量控制系统，用少量数据预测和优化真空热成型工艺参数，提高零件质量。


<details>
  <summary>Details</summary>
Motivation: 真空热成型因材料特性和模具配置变化，难以保证产品质量一致性。

Method: 开发综合数据集，结合图像增强技术，用k近邻算法将低质量零件映射到高质量对应物以确定参数调整。

Result: 模型在调整加热功率、加热时间和真空时间方面表现出色，减少缺陷，提高生产效率。

Conclusion: 基于视觉的质量控制系统能有效预测和优化真空热成型工艺参数，提高产品质量。

Abstract: Ensuring consistent quality in vacuum thermoforming presents challenges due
to variations in material properties and tooling configurations. This research
introduces a vision-based quality control system to predict and optimise
process parameters, thereby enhancing part quality with minimal data
requirements. A comprehensive dataset was developed using visual data from
vacuum-formed samples subjected to various process parameters, supplemented by
image augmentation techniques to improve model training. A k-Nearest Neighbour
algorithm was subsequently employed to identify adjustments needed in process
parameters by mapping low-quality parts to their high-quality counterparts. The
model exhibited strong performance in adjusting heating power, heating time,
and vacuum time to reduce defects and improve production efficiency.

</details>


### [256] [RadGame: An AI-Powered Platform for Radiology Education](https://arxiv.org/abs/2509.13270)
*Mohammed Baharoon,Siavash Raissi,John S. Jun,Thibault Heintz,Mahmoud Alabbad,Ali Alburkani,Sung Eun Kim,Kent Kleinschmidt,Abdulrahman O. Alhumaydhi,Mohannad Mohammed G. Alghamdi,Jeremy Francis Palacio,Mohammed Bukhaytan,Noah Michael Prudlo,Rithvik Akula,Brady Chrisler,Benjamin Galligos,Mohammed O. Almutairi,Mazeen Mohammed Alanazi,Nasser M. Alrashdi,Joel Jihwan Hwang,Sri Sai Dinesh Jaliparthi,Luke David Nelson,Nathaniel Nguyen,Sathvik Suryadevara,Steven Kim,Mohammed F. Mohammed,Yevgeniy R. Semenov,Kun-Hsing Yu,Abdulrhman Aljouie,Hassan AlOmaish,Adam Rodman,Pranav Rajpurkar*

Main category: cs.CV

TL;DR: 介绍了AI驱动的放射学教育游戏化平台RadGame，对比传统方法证明其能提升学习效果，强调AI游戏化教学潜力。


<details>
  <summary>Details</summary>
Motivation: 传统放射学训练难以提供即时和可扩展反馈，需新方法解决。

Method: 将游戏化与大规模公共数据集、AI驱动反馈结合，设计RadGame Localize和RadGame Report两个模块。

Result: 使用RadGame参与者定位准确率提升68%，报告撰写准确率提升31%，远高于传统方法。

Conclusion: AI驱动的游戏化能提供可扩展、反馈丰富的放射学训练，可重新设想医学AI资源在教育中的应用。

Abstract: We introduce RadGame, an AI-powered gamified platform for radiology education
that targets two core skills: localizing findings and generating reports.
Traditional radiology training is based on passive exposure to cases or active
practice with real-time input from supervising radiologists, limiting
opportunities for immediate and scalable feedback. RadGame addresses this gap
by combining gamification with large-scale public datasets and automated,
AI-driven feedback that provides clear, structured guidance to human learners.
In RadGame Localize, players draw bounding boxes around abnormalities, which
are automatically compared to radiologist-drawn annotations from public
datasets, and visual explanations are generated by vision-language models for
user missed findings. In RadGame Report, players compose findings given a chest
X-ray, patient age and indication, and receive structured AI feedback based on
radiology report generation metrics, highlighting errors and omissions compared
to a radiologist's written ground truth report from public datasets, producing
a final performance and style score. In a prospective evaluation, participants
using RadGame achieved a 68% improvement in localization accuracy compared to
17% with traditional passive methods and a 31% improvement in report-writing
accuracy compared to 4% with traditional methods after seeing the same cases.
RadGame highlights the potential of AI-driven gamification to deliver scalable,
feedback-rich radiology training and reimagines the application of medical AI
resources in education.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [257] [Between proportionnality and envy-freeness: k-proportionality](https://arxiv.org/abs/2509.12903)
*Guillaume Chèze*

Main category: cs.MA

TL;DR: 本文研究蛋糕切割问题，引入k - 比例性概念构建比例分割和无嫉妒分割间的尺度，证明了一些不可能结果并实现定理统一表述。


<details>
  <summary>Details</summary>
Motivation: 理解比例分割和无嫉妒分割相关陈述之间的差距，明确公平分割中的困难所在。

Method: 引入k - 比例性概念，当k = n对应比例分割，k = 2对应无嫉妒分割，以此开展研究。

Result: 证明在k ≤ n - 1时，存在圆形蛋糕无法进行k - 比例且公平的连通块分割以及蛋糕无法进行帕累托最优的k - 比例连通块分割的情况；实现关于强无嫉妒和强比例分割定理的统一表述。

Conclusion: 即使不考虑无嫉妒分割，采用更弱的概念也能得到不可能结果，k - 比例性有助于理解公平分割困难并统一相关定理表述。

Abstract: This article deals with the cake cutting problem. In this setting, there
exists two notions of fair division: proportional division (when there are n
players, each player thinks to get at least 1/n of the cake) and envy-free
division (each player wants to keep his or her share because he or she does not
envy the portion given to another player). Some results are valid for
proportional division but not for envy-free division. Here, we introduce and
study a scale between the proportional division and the envy-free division. The
goal is to understand where is the gap between statements about proportional
division and envy-free division. This scale comes from the notion introduced in
this article: k-proportionality. When k = n this notion corresponds to the
proportional division and when k = 2 it corresponds to envy-free division. With
k-proportionality we can understand where some difficulties in fair division
lie. First, we show that there are situations in which there is no
k-proportional and equitable division of a pie with connected pieces when k
$\le$ n -1. This result was known only for envy-free division, ie k = 2. Next,
we prove that there are situations in which there is no Pareto-optimal
k-proportional division of a cake with connected pieces when k $\le$ n -1. This
result was known only for k = 2. These theorems say that we can get an
impossibility result even if we do not consider an envy-free division but a
weaker notion. Finally, k-proportionality allows to give a generalization with
a uniform statement of theorems about strong envy-free and strong proportional
divisions.

</details>


### [258] [PromptSculptor: Multi-Agent Based Text-to-Image Prompt Optimization](https://arxiv.org/abs/2509.12446)
*Dawei Xiang,Wenyan Xu,Kexin Chu,Zixu Shen,Tianqi Ding,Wei Zhang*

Main category: cs.MA

TL;DR: 提出PromptSculptor框架自动优化文本到图像模型的提示词，实验显示其提升输出质量、减少迭代次数且可与多种模型集成。


<details>
  <summary>Details</summary>
Motivation: 文本到图像模型生成高质量图像需用户精心设计提示词并多次优化，为解决该问题提出PromptSculptor框架。

Method: 将任务分解为四个专业代理，利用思维链推理推断隐藏上下文并丰富细节，通过自我评估代理和反馈调整代理迭代优化提示词。

Result: 显著提升输出质量，减少用户满意所需的迭代次数，可与多种T2I模型无缝集成。

Conclusion: PromptSculptor框架有效且具有模型无关性，为工业应用铺平道路。

Abstract: The rapid advancement of generative AI has democratized access to powerful
tools such as Text-to-Image models. However, to generate high-quality images,
users must still craft detailed prompts specifying scene, style, and
context-often through multiple rounds of refinement. We propose PromptSculptor,
a novel multi-agent framework that automates this iterative prompt optimization
process. Our system decomposes the task into four specialized agents that work
collaboratively to transform a short, vague user prompt into a comprehensive,
refined prompt. By leveraging Chain-of-Thought reasoning, our framework
effectively infers hidden context and enriches scene and background details. To
iteratively refine the prompt, a self-evaluation agent aligns the modified
prompt with the original input, while a feedback-tuning agent incorporates user
feedback for further refinement. Experimental results demonstrate that
PromptSculptor significantly enhances output quality and reduces the number of
iterations needed for user satisfaction. Moreover, its model-agnostic design
allows seamless integration with various T2I models, paving the way for
industrial applications.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [259] [Generalizable Holographic Reconstruction via Amplitude-Only Diffusion Priors](https://arxiv.org/abs/2509.12728)
*Jeongsol Kim,Chanseok Lee,Jong Chul Ye,Mooseok Jang*

Main category: physics.optics

TL;DR: 提出利用仅在物体振幅上训练的扩散模型从衍射强度中恢复振幅和相位的方法，经实验验证其有效性和适应性，为计算成像中的非线性逆问题提供通用解决方案。


<details>
  <summary>Details</summary>
Motivation: 内联全息术中的相位恢复是基本但病态的逆问题，需解决振幅和相位非线性耦合问题。

Method: 利用仅在物体振幅上训练的扩散模型，采用预测 - 校正采样框架，分别计算振幅和相位的似然梯度，无需真实相位数据训练。

Result: 通过大量模拟和实验验证，方法在不同物体形状、成像系统配置和模式下有良好泛化能力，能重建复杂生物组织结构。

Conclusion: 该框架为计算成像中的非线性逆问题提供了经济有效、可推广的解决方案，为相干成像应用奠定基础。

Abstract: Phase retrieval in inline holography is a fundamental yet ill-posed inverse
problem due to the nonlinear coupling between amplitude and phase in coherent
imaging. We present a novel off-the-shelf solution that leverages a diffusion
model trained solely on object amplitude to recover both amplitude and phase
from diffraction intensities. Using a predictor-corrector sampling framework
with separate likelihood gradients for amplitude and phase, our method enables
complex field reconstruction without requiring ground-truth phase data for
training. We validate the proposed approach through extensive simulations and
experiments, demonstrating robust generalization across diverse object shapes,
imaging system configurations, and modalities, including lensless setups.
Notably, a diffusion prior trained on simple amplitude data (e.g., polystyrene
beads) successfully reconstructs complex biological tissue structures,
highlighting the method's adaptability. This framework provides a
cost-effective, generalizable solution for nonlinear inverse problems in
computational imaging, and establishes a foundation for broader coherent
imaging applications beyond holography.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [260] [Towards Trustworthy Agentic IoEV: AI Agents for Explainable Cyberthreat Mitigation and State Analytics](https://arxiv.org/abs/2509.12233)
*Meryem Malak Dif,Mouhamed Amine Bouchiha,Abdelaziz Amara Korba,Yacine Ghamri-Doudane*

Main category: cs.CR

TL;DR: 本文介绍适用于IoEV的新型AAI框架，经实验验证可提升安全性和预测准确性，相关数据和代码将公开。


<details>
  <summary>Details</summary>
Motivation: IoEV易受网络攻击、电池状态预测不可靠和决策过程不透明等问题影响，侵蚀信任和性能，需要解决这些挑战。

Method: 设计含专用代理的AAI架构，开发可解释的威胁缓解机制，提出弹性的SoC和SoH模型，实现三代理管道。

Result: 通过在不同IoEV场景的综合实验，验证框架在安全性和预测准确性上有显著提升。

Conclusion: 所提出的AAI框架能有效应对IoEV面临的挑战，提升系统的安全性和预测准确性。

Abstract: The Internet of Electric Vehicles (IoEV) envisions a tightly coupled
ecosystem of electric vehicles (EVs), charging infrastructure, and grid
services, yet it remains vulnerable to cyberattacks, unreliable battery-state
predictions, and opaque decision processes that erode trust and performance. To
address these challenges, we introduce a novel Agentic Artificial Intelligence
(AAI) framework tailored for IoEV, where specialized agents collaborate to
deliver autonomous threat mitigation, robust analytics, and interpretable
decision support. Specifically, we design an AAI architecture comprising
dedicated agents for cyber-threat detection and response at charging stations,
real-time State of Charge (SoC) estimation, and State of Health (SoH) anomaly
detection, all coordinated through a shared, explainable reasoning layer;
develop interpretable threat-mitigation mechanisms that proactively identify
and neutralize attacks on both physical charging points and learning
components; propose resilient SoC and SoH models that leverage continuous and
adversarial-aware learning to produce accurate, uncertainty-aware forecasts
with human-readable explanations; and implement a three-agent pipeline, where
each agent uses LLM-driven reasoning and dynamic tool invocation to interpret
intent, contextualize tasks, and execute formal optimizations for user-centric
assistance. Finally, we validate our framework through comprehensive
experiments across diverse IoEV scenarios, demonstrating significant
improvements in security and prediction accuracy. All datasets, models, and
code will be released publicly.

</details>


### [261] [Amulet: a Python Library for Assessing Interactions Among ML Defenses and Risks](https://arxiv.org/abs/2509.12386)
*Asim Waheed,Vasisht Duddu,Rui Zhang,Sebastian Szyller,N. Asokan*

Main category: cs.CR

TL;DR: 本文提出Python库AMULET，用于评估机器学习模型风险的意外交互，满足多项理想要求，可用于评估、比较和扩展攻防手段。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型面临安全、隐私和公平性风险，现有防御可能产生意外交互，且多地监管要求评估模型风险，需要一个评估意外交互的库。

Method: 开发了满足全面性、可扩展性、一致性和适用性要求的Python库AMULET。

Result: 创建了AMULET库，可评估未探索的意外交互、比较攻防效果以及纳入新的攻防手段。

Conclusion: AMULET库能有效满足评估机器学习模型意外交互的需求。

Abstract: ML models are susceptible to risks to security, privacy, and fairness.
Several defenses are designed to protect against their intended risks, but can
inadvertently affect susceptibility to other unrelated risks, known as
unintended interactions. Several jurisdictions are preparing ML regulatory
frameworks that require ML practitioners to assess the susceptibility of ML
models to different risks. A library for valuating unintended interactions that
can be used by (a) practitioners to evaluate unintended interactions at scale
prior to model deployment and (b) researchers to design defenses which do not
suffer from an unintended increase in unrelated risks. Ideally, such a library
should be i) comprehensive by including representative attacks, defenses and
metrics for different risks, ii) extensible to new modules due to its modular
design, iii) consistent with a user-friendly API template for inputs and
outputs, iv) applicable to evaluate previously unexplored unintended
interactions. We present AMULET, a Python library that covers risks to
security, privacy, and fairness, which satisfies all these requirements. AMULET
can be used to evaluate unexplored unintended interactions, compare
effectiveness between defenses or attacks, and include new attacks and
defenses.

</details>


### [262] [A Systematic Evaluation of Parameter-Efficient Fine-Tuning Methods for the Security of Code LLMs](https://arxiv.org/abs/2509.12649)
*Kiho Lee,Jungkon Kim,Doowon Kim,Hyoungshick Kim*

Main category: cs.CR

TL;DR: 研究评估七种参数高效微调技术提升代码生成大语言模型的安全代码生成能力，发现提示调优最有效，优化解码策略可进一步提升安全性，研究结果具有跨语言通用性。


<details>
  <summary>Details</summary>
Motivation: 代码生成大语言模型常生成不安全代码，存在严重风险，需提升其安全代码生成能力。

Method: 对七种参数高效微调技术进行综合评估，优化解码策略，进行TrojanPuzzle评估。

Result: 提示调优是最有效PEFT方法，在CodeGen2 16B上整体安全率达80.86%，优化解码策略后达87.65%，减少约203,700个每百万生成的易受攻击代码片段，提示和前缀调优提高对中毒攻击的鲁棒性，研究结果跨Python和Java有效。

Conclusion: 为使用大语言模型构建更具弹性的软件系统提供重要见解和实践指导。

Abstract: Code-generating Large Language Models (LLMs) significantly accelerate
software development. However, their frequent generation of insecure code
presents serious risks. We present a comprehensive evaluation of seven
parameter-efficient fine-tuning (PEFT) techniques, demonstrating substantial
gains in secure code generation without compromising functionality. Our
research identifies prompt-tuning as the most effective PEFT method, achieving
an 80.86% Overall-Secure-Rate on CodeGen2 16B, a 13.5-point improvement over
the 67.28% baseline. Optimizing decoding strategies through sampling
temperature further elevated security to 87.65%. This equates to a reduction of
approximately 203,700 vulnerable code snippets per million generated. Moreover,
prompt and prefix tuning increase robustness against poisoning attacks in our
TrojanPuzzle evaluation, with strong performance against CWE-79 and CWE-502
attack vectors. Our findings generalize across Python and Java, confirming
prompt-tuning's consistent effectiveness. This study provides essential
insights and practical guidance for building more resilient software systems
with LLMs.

</details>


### [263] [A Graph-Based Approach to Alert Contextualisation in Security Operations Centres](https://arxiv.org/abs/2509.12923)
*Magnus Wiik Eckhoff,Peter Marius Flydal,Siem Peters,Martin Eian,Jonas Halvorsen,Vasileios Mavroeidis,Gudmund Grov*

Main category: cs.CR

TL;DR: 本文提出基于图的方法增强安全运营中心（SOC）的警报上下文关联，通过聚合警报成图组并使用图匹配网络关联历史事件。


<details>
  <summary>Details</summary>
Motivation: 解读大量安全警报是SOC的重大挑战，有效上下文关联可区分威胁和良性活动，对需进一步分析的内容进行优先级排序。

Method: 提出基于图的方法，将警报聚合为基于图的警报组，用图匹配网络（GMNs）关联新警报组与历史事件。

Result: 通过分组相关警报，能在更高抽象级别进行分析，比单个警报更有效地捕捉攻击步骤，为分析师提供额外见解。

Conclusion: 基于图的方法适合增强SOC的警报上下文关联，可辅助分析师工作。

Abstract: Interpreting the massive volume of security alerts is a significant challenge
in Security Operations Centres (SOCs). Effective contextualisation is
important, enabling quick distinction between genuine threats and benign
activity to prioritise what needs further analysis.This paper proposes a
graph-based approach to enhance alert contextualisation in a SOC by aggregating
alerts into graph-based alert groups, where nodes represent alerts and edges
denote relationships within defined time-windows. By grouping related alerts,
we enable analysis at a higher abstraction level, capturing attack steps more
effectively than individual alerts. Furthermore, to show that our format is
well suited for downstream machine learning methods, we employ Graph Matching
Networks (GMNs) to correlate incoming alert groups with historical incidents,
providing analysts with additional insights.

</details>


### [264] [Jailbreaking Large Language Models Through Content Concretization](https://arxiv.org/abs/2509.12937)
*Johan Wahréus,Ahmed Hussain,Panos Papadimitratos*

Main category: cs.CR

TL;DR: 本文介绍了一种名为内容具体化（CC）的新型越狱技术，可将抽象恶意请求转化为具体可执行实现，经评估能大幅提高越狱成功率，揭示了当前大语言模型安全框架的关键漏洞。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的安全机制易被不同越狱技术绕过，需要新的越狱技术来揭示其安全漏洞。

Method: 内容具体化（CC）是一个两阶段过程，先使用安全过滤较宽松的低层级模型生成初始响应，再通过高层级模型结合初步输出和原始提示进行细化。

Result: 使用350个网络安全特定提示评估，越狱成功率从7%提升到62%，每次提示成本7.5美分；A/B测试表明额外细化步骤的输出被评为更恶意和技术上更优；手动代码分析显示生成的输出只需最小修改即可执行。

Conclusion: 当前大语言模型安全框架存在关键漏洞，随着有害代码生成能力的提高，漏洞问题将更突出。

Abstract: Large Language Models (LLMs) are increasingly deployed for task automation
and content generation, yet their safety mechanisms remain vulnerable to
circumvention through different jailbreaking techniques. In this paper, we
introduce \textit{Content Concretization} (CC), a novel jailbreaking technique
that iteratively transforms abstract malicious requests into concrete,
executable implementations. CC is a two-stage process: first, generating
initial LLM responses using lower-tier, less constrained safety filters models,
then refining them through higher-tier models that process both the preliminary
output and original prompt. We evaluate our technique using 350
cybersecurity-specific prompts, demonstrating substantial improvements in
jailbreak Success Rates (SRs), increasing from 7\% (no refinements) to 62\%
after three refinement iterations, while maintaining a cost of 7.5\textcent~per
prompt. Comparative A/B testing across nine different LLM evaluators confirms
that outputs from additional refinement steps are consistently rated as more
malicious and technically superior. Moreover, manual code analysis reveals that
generated outputs execute with minimal modification, although optimal
deployment typically requires target-specific fine-tuning. With eventual
improved harmful code generation, these results highlight critical
vulnerabilities in current LLM safety frameworks.

</details>


### [265] [xOffense: An AI-driven autonomous penetration testing framework with offensive knowledge-enhanced LLMs and multi agent systems](https://arxiv.org/abs/2509.13021)
*Phung Duc Luong,Le Tran Gia Bao,Nguyen Vu Khai Tam,Dong Huu Nguyen Khoa,Nguyen Huu Quyen,Van-Hau Pham,Phan The Duy*

Main category: cs.CR

TL;DR: 介绍AI驱动的多代理渗透测试框架xOffense，用微调的Qwen3 - 32B驱动，表现超当代方法。


<details>
  <summary>Details</summary>
Motivation: 将渗透测试从人工劳动密集型转为全自动、机器可执行的工作流。

Method: 利用微调的Qwen3 - 32B驱动推理决策，分配专门代理执行不同任务，用编排层协调，在思维链渗透测试数据上微调。

Result: 在两个基准测试中，xOffense始终优于当代方法，子任务完成率达79.17%，超越VulnBot和PentestGPT。

Conclusion: 结构化多代理编排中嵌入领域适应的中规模大语言模型，可为自主渗透测试提供更好、经济且可重复的解决方案。

Abstract: This work introduces xOffense, an AI-driven, multi-agent penetration testing
framework that shifts the process from labor-intensive, expert-driven manual
efforts to fully automated, machine-executable workflows capable of scaling
seamlessly with computational infrastructure. At its core, xOffense leverages a
fine-tuned, mid-scale open-source LLM (Qwen3-32B) to drive reasoning and
decision-making in penetration testing. The framework assigns specialized
agents to reconnaissance, vulnerability scanning, and exploitation, with an
orchestration layer ensuring seamless coordination across phases. Fine-tuning
on Chain-of-Thought penetration testing data further enables the model to
generate precise tool commands and perform consistent multi-step reasoning. We
evaluate xOffense on two rigorous benchmarks: AutoPenBench and
AI-Pentest-Benchmark. The results demonstrate that xOffense consistently
outperforms contemporary methods, achieving a sub-task completion rate of
79.17%, decisively surpassing leading systems such as VulnBot and PentestGPT.
These findings highlight the potential of domain-adapted mid-scale LLMs, when
embedded within structured multi-agent orchestration, to deliver superior,
cost-efficient, and reproducible solutions for autonomous penetration testing.

</details>


### [266] [MIA-EPT: Membership Inference Attack via Error Prediction for Tabular Data](https://arxiv.org/abs/2509.13046)
*Eyal German,Daniel Samira,Yuval Elovici,Asaf Shabtai*

Main category: cs.CR

TL;DR: 本文提出针对表格扩散模型的新型黑盒成员推理攻击MIA - EPT，验证其有效性，表明合成表格数据存在成员信息泄露问题。


<details>
  <summary>Details</summary>
Motivation: 现有表格扩散模型可能记忆训练记录并泄露敏感信息，针对表格扩散模型的成员推理攻击研究不足。

Method: 提出MIA - EPT攻击，通过掩盖和重构目标记录属性构建基于误差的特征向量，利用合成数据输出来揭示成员信号。

Result: 在三个基于扩散的合成器上验证，内部测试AUC - ROC达0.599，TPR@10% FPR为22.0%；在MIDST 2025竞赛条件下获黑盒多表赛道第二名。

Conclusion: 方法能揭示合成表格数据中大量成员信息泄露，质疑合成数据天然保护隐私的假设。

Abstract: Synthetic data generation plays an important role in enabling data sharing,
particularly in sensitive domains like healthcare and finance. Recent advances
in diffusion models have made it possible to generate realistic, high-quality
tabular data, but they may also memorize training records and leak sensitive
information. Membership inference attacks (MIAs) exploit this vulnerability by
determining whether a record was used in training. While MIAs have been studied
in images and text, their use against tabular diffusion models remains
underexplored despite the unique risks of structured attributes and limited
record diversity. In this paper, we introduce MIAEPT, Membership Inference
Attack via Error Prediction for Tabular Data, a novel black-box attack
specifically designed to target tabular diffusion models. MIA-EPT constructs
errorbased feature vectors by masking and reconstructing attributes of target
records, disclosing membership signals based on how well these attributes are
predicted. MIA-EPT operates without access to the internal components of the
generative model, relying only on its synthetic data output, and was shown to
generalize across multiple state-of-the-art diffusion models. We validate
MIA-EPT on three diffusion-based synthesizers, achieving AUC-ROC scores of up
to 0.599 and TPR@10% FPR values of 22.0% in our internal tests. Under the MIDST
2025 competition conditions, MIA-EPT achieved second place in the Black-box
Multi-Table track (TPR@10% FPR = 20.0%). These results demonstrate that our
method can uncover substantial membership leakage in synthetic tabular data,
challenging the assumption that synthetic data is inherently
privacy-preserving. Our code is publicly available at
https://github.com/eyalgerman/MIA-EPT.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [267] [Jackknife Variance Estimation for Hájek-Dominated Generalized U-Statistics](https://arxiv.org/abs/2509.12356)
*Jakob R. Juergens*

Main category: math.ST

TL;DR: 证明广义U统计量的刀切法方差估计量及某些变体的比率一致性，应用于两尺度分布最近邻回归估计量获一致方差估计。


<details>
  <summary>Details</summary>
Motivation: 统一和推广现有文献中关于广义U统计量刀切法方差估计的标准，使简单非参数刀切法与无穷小刀切法在广义设置下有相同地位。

Method: 证明满足Hájek投影主导条件下广义U统计量刀切法方差估计的比率一致性。

Result: 证明了刀切法方差估计的比率一致性，应用于两尺度分布最近邻回归估计量在更弱条件下获一致方差估计。

Conclusion: Hájek投影主导条件统一和推广了现有标准，刀切法方差估计可在较弱条件下应用。

Abstract: We prove ratio-consistency of the jackknife variance estimator, and certain
variants, for a broad class of generalized U-statistics whose variance is
asymptotically dominated by their H\'ajek projection, with the classical
fixed-order case recovered as a special instance. This H\'ajek projection
dominance condition unifies and generalizes several criteria in the existing
literature, placing the simple nonparametric jackknife on the same footing as
the infinitesimal jackknife in the generalized setting. As an illustration, we
apply our result to the two-scale distributional nearest-neighbor regression
estimator, obtaining consistent variance estimates under substantially weaker
conditions than previously required.

</details>


### [268] [Gaussian Mixture Model with unknown diagonal covariances via continuous sparse regularization](https://arxiv.org/abs/2509.12889)
*Romane Giard,Yohann de Castro,Clément Marteau*

Main category: math.ST

TL;DR: 本文使用Beurling - LASSO框架估计含未知对角协方差的高斯混合模型（GMMs），拓展方法至多元GMMs，建立非渐近恢复保证和明确分离条件。


<details>
  <summary>Details</summary>
Motivation: 解决从独立同分布样本中对含未知对角协方差的高斯混合模型进行统计估计的问题，且比之前需已知相同协方差的方法更灵活。

Method: 采用Beurling - LASSO（BLASSO）凸优化框架同时估计分量数量及其参数。

Result: 为分量均值、对角协方差、权重和密度预测建立了具有近参数收敛率的非渐近恢复保证，确定了混合分量的明确分离条件以构建非退化对偶证书。

Conclusion: 分析利用统计模型的Fisher - Rao几何，引入新的半距离，为分量分离、参数空间几何和可实现的统计恢复之间的相互作用提供了新见解。

Abstract: This paper addresses the statistical estimation of Gaussian Mixture Models
(GMMs) with unknown diagonal covariances from independent and identically
distributed samples. We employ the Beurling-LASSO (BLASSO), a convex
optimization framework that promotes sparsity in the space of measures, to
simultaneously estimate the number of components and their parameters. Our main
contribution extends the BLASSO methodology to multivariate GMMs with
component-specific unknown diagonal covariance matrices-a significantly more
flexible setting than previous approaches requiring known and identical
covariances. We establish non-asymptotic recovery guarantees with nearly
parametric convergence rates for component means, diagonal covariances, and
weights, as well as for density prediction. A key theoretical contribution is
the identification of an explicit separation condition on mixture components
that enables the construction of non-degenerate dual certificates-essential
tools for establishing statistical guarantees for the BLASSO. Our analysis
leverages the Fisher-Rao geometry of the statistical model and introduces a
novel semi-distance adapted to our framework, providing new insights into the
interplay between component separation, parameter space geometry, and
achievable statistical recovery.

</details>


### [269] [Optimal Conformal Prediction, E-values, Fuzzy Prediction Sets and Subsequent Decisions](https://arxiv.org/abs/2509.13130)
*Nick W. Koning,Sam van Meer*

Main category: math.ST

TL;DR: 本文对共形预测做出三项贡献，包括提出模糊共形置信集、推导最优共形置信集、将保证继承性推广到模糊置信集，结果可推广到任意模型预测集。


<details>
  <summary>Details</summary>
Motivation: 推动共形预测的发展，解决经典置信集二元包含/排除的局限性等问题。

Method: 提出模糊共形置信集并将其与e值联系，将置信集期望测度最小化解释为最优测试问题，对保证继承性进行推广。

Result: 模糊置信集是具有更合适误差保证的预测分布；明确传统共形预测的最优意义；保证继承性可推广到模糊置信集；任意有效测试（e值）可定义（模糊）预测置信集。

Conclusion: 研究成果能将共形预测相关结论推广到任意模型的预测集。

Abstract: We make three contributions to conformal prediction. First, we propose fuzzy
conformal confidence sets that offer a degree of exclusion, generalizing beyond
the binary inclusion/exclusion offered by classical confidence sets. We connect
fuzzy confidence sets to e-values to show this degree of exclusion is
equivalent to an exclusion at different confidence levels, capturing precisely
what e-values bring to conformal prediction. We show that a fuzzy confidence
set is a predictive distribution with a more appropriate error guarantee.
Second, we derive optimal conformal confidence sets by interpreting the
minimization of the expected measure of the confidence set as an optimal
testing problem against a particular alternative. We use this to characterize
exactly in what sense traditional conformal prediction is optimal. Third, we
generalize the inheritance of guarantees by subsequent minimax decisions from
confidence sets to fuzzy confidence sets. All our results generalize beyond the
exchangeable conformal setting to prediction sets for arbitrary models. In
particular, we find that any valid test (e-value) for a hypothesis
automatically defines a (fuzzy) prediction confidence set.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [270] [A comparison of pipelines for the translation of a low resource language based on transformers](https://arxiv.org/abs/2509.12514)
*Chiara Bonfanti,Michele Colombino,Giulia Coucourde,Faeze Memari,Stefano Pinardi,Rosa Meo*

Main category: cs.CL

TL;DR: 本文比较三种训练基于Transformer的神经网络的管道，以构建班巴拉语机器翻译器，第一种管道效果最佳。


<details>
  <summary>Details</summary>
Motivation: 为班巴拉语构建机器翻译器，比较不同训练管道的效果。

Method: 第一种管道训练简单Transformer；第二种微调LLaMA3指导模型；第三种使用语言蒸馏和BERT扩展预训练LaBSE模型。所有管道用不同超参数组合训练，并在不同数据集测试。

Result: 第一种管道翻译准确率最高，指导模型在单数据集表现更好。

Conclusion: 第一种管道虽简单，但在班巴拉语翻译中效果最佳，符合低资源翻译结果。

Abstract: This work compares three pipelines for training transformer-based neural
networks to produce machine translators for Bambara, a Mand\`e language spoken
in Africa by about 14,188,850 people. The first pipeline trains a simple
transformer to translate sentences from French into Bambara. The second
fine-tunes LLaMA3 (3B-8B) instructor models using decoder-only architectures
for French-to-Bambara translation. Models from the first two pipelines were
trained with different hyperparameter combinations to improve BLEU and chrF
scores, evaluated on both test sentences and official Bambara benchmarks. The
third pipeline uses language distillation with a student-teacher dual neural
network to integrate Bambara into a pre-trained LaBSE model, which provides
language-agnostic embeddings. A BERT extension is then applied to LaBSE to
generate translations. All pipelines were tested on Dokotoro (medical) and
Bayelemagaba (mixed domains). Results show that the first pipeline, although
simpler, achieves the best translation accuracy (10% BLEU, 21% chrF on
Bayelemagaba), consistent with low-resource translation results. On the Yiri
dataset, created for this work, it achieves 33.81% BLEU and 41% chrF.
Instructor-based models perform better on single datasets than on aggregated
collections, suggesting they capture dataset-specific patterns more
effectively.

</details>


### [271] [Automated Generation of Research Workflows from Academic Papers: A Full-text Mining Framework](https://arxiv.org/abs/2509.12955)
*Heng Zhang,Chengzhi Zhang*

Main category: cs.CL

TL;DR: 本文提出端到端框架，通过挖掘学术论文全文生成研究工作流，在NLP领域验证效果良好，还揭示研究范式转变。


<details>
  <summary>Details</summary>
Motivation: 现有方法只能提取碎片化程序组件，无法捕捉完整研究工作流，为解决此问题开展研究。

Method: 提出端到端框架，采用PU学习与SciBERT识别段落，用Flan - T5生成工作流短语，用ChatGPT分类短语，最后生成可视化流程图。

Result: 识别段落F1分数0.9772，生成短语ROUGE - 1、ROUGE - 2、ROUGE - L分数分别为0.4543、0.2877、0.4427，分类精度0.958，还揭示NLP领域方法转变。

Conclusion: 提供了自动化工作流生成的有效技术框架，为研究科学范式演变提供新视角。

Abstract: The automated generation of research workflows is essential for improving the
reproducibility of research and accelerating the paradigm of "AI for Science".
However, existing methods typically extract merely fragmented procedural
components and thus fail to capture complete research workflows. To address
this gap, we propose an end-to-end framework that generates comprehensive,
structured research workflows by mining full-text academic papers. As a case
study in the Natural Language Processing (NLP) domain, our paragraph-centric
approach first employs Positive-Unlabeled (PU) Learning with SciBERT to
identify workflow-descriptive paragraphs, achieving an F1-score of 0.9772.
Subsequently, we utilize Flan-T5 with prompt learning to generate workflow
phrases from these paragraphs, yielding ROUGE-1, ROUGE-2, and ROUGE-L scores of
0.4543, 0.2877, and 0.4427, respectively. These phrases are then systematically
categorized into data preparation, data processing, and data analysis stages
using ChatGPT with few-shot learning, achieving a classification precision of
0.958. By mapping categorized phrases to their document locations in the
documents, we finally generate readable visual flowcharts of the entire
research workflows. This approach facilitates the analysis of workflows derived
from an NLP corpus and reveals key methodological shifts over the past two
decades, including the increasing emphasis on data analysis and the transition
from feature engineering to ablation studies. Our work offers a validated
technical framework for automated workflow generation, along with a novel,
process-oriented perspective for the empirical investigation of evolving
scientific paradigms. Source code and data are available at:
https://github.com/ZH-heng/research_workflow.

</details>


### [272] [MORABLES: A Benchmark for Assessing Abstract Moral Reasoning in LLMs with Fables](https://arxiv.org/abs/2509.12371)
*Matteo Marcuzzo,Alessandro Zangari,Andrea Albarelli,Jose Camacho-Collados,Mohammad Taher Pilehvar*

Main category: cs.CL

TL;DR: 提出基于寓言和故事的MORABLES基准测试评估大模型道德推理能力，发现大模型易受对抗操纵，规模而非推理能力是性能主因。


<details>
  <summary>Details</summary>
Motivation: 大模型在标准阅读理解基准测试表现出色，需评估其复杂抽象推理和推断能力，文学基准可用于评估更深层次理解技能。

Method: 构建MORABLES基准测试，以多项选择题评估道德推理，设置干扰项和对抗变体。

Result: 大模型优于小模型，但易受对抗操纵，依赖表面模式，存在自相矛盾，推理增强模型未能缩小差距。

Conclusion: 规模是大模型性能的主要驱动因素，而非推理能力。

Abstract: As LLMs excel on standard reading comprehension benchmarks, attention is
shifting toward evaluating their capacity for complex abstract reasoning and
inference. Literature-based benchmarks, with their rich narrative and moral
depth, provide a compelling framework for evaluating such deeper comprehension
skills. Here, we present MORABLES, a human-verified benchmark built from fables
and short stories drawn from historical literature. The main task is structured
as multiple-choice questions targeting moral inference, with carefully crafted
distractors that challenge models to go beyond shallow, extractive question
answering. To further stress-test model robustness, we introduce adversarial
variants designed to surface LLM vulnerabilities and shortcuts due to issues
such as data contamination. Our findings show that, while larger models
outperform smaller ones, they remain susceptible to adversarial manipulation
and often rely on superficial patterns rather than true moral reasoning. This
brittleness results in significant self-contradiction, with the best models
refuting their own answers in roughly 20% of cases depending on the framing of
the moral choice. Interestingly, reasoning-enhanced models fail to bridge this
gap, suggesting that scale - not reasoning ability - is the primary driver of
performance.

</details>


### [273] [MedFact: Benchmarking the Fact-Checking Capabilities of Large Language Models on Chinese Medical Texts](https://arxiv.org/abs/2509.12440)
*Jiayi He,Yangmin Huang,Qianyun Du,Xiangying Zhou,Zhiyang He,Jiaxue Hu,Xiaodong Tao,Lixian Lai*

Main category: cs.CL

TL;DR: 提出用于中文医疗事实核查的基准MedFact，评估20个大语言模型，揭示模型定位错误及‘过度批评’问题，为模型开发提供资源。


<details>
  <summary>Details</summary>
Motivation: 现有医疗大语言模型事实可靠性评估基准数据领域狭窄，无法反映真实医疗信息复杂性，需新基准。

Method: 构建含2116个实例的MedFact基准，采用人机混合框架；对20个大语言模型进行全面评估。

Result: 模型能判断文本是否有错，但精确定位错误困难，且存在‘过度批评’现象，高级推理技术会加剧此问题。

Conclusion: MedFact突出了大语言模型在医疗应用中的挑战，为开发更可靠和有医疗意识的模型提供有力资源。

Abstract: The increasing deployment of Large Language Models (LLMs) in healthcare
necessitates a rigorous evaluation of their factual reliability. However,
existing benchmarks are often limited by narrow domains of data, failing to
capture the complexity of real-world medical information. To address this
critical gap, we introduce MedFact, a new and challenging benchmark for Chinese
medical fact-checking. MedFact comprises 2,116 expert-annotated instances
curated from diverse real-world texts, spanning 13 medical specialties, 8
fine-grained error types, 4 writing styles, and multiple difficulty levels. Its
construction employs a hybrid AI-human framework where iterative expert
feedback refines an AI-driven, multi-criteria filtering process, ensuring both
high data quality and difficulty. We conduct a comprehensive evaluation of 20
leading LLMs, benchmarking their performance on veracity classification and
error localization against a human expert baseline. Our results reveal that
while models can often determine if a text contains an error, precisely
localizing it remains a substantial challenge, with even top-performing models
falling short of human performance. Furthermore, our analysis uncovers a
frequent ``over-criticism'' phenomenon, a tendency for models to misidentify
correct information as erroneous, which is exacerbated by advanced reasoning
techniques such as multi-agent collaboration and inference-time scaling. By
highlighting these critical challenges for deploying LLMs in medical
applications, MedFact provides a robust resource to drive the development of
more factually reliable and medically aware models.

</details>


### [274] [FunAudio-ASR Technical Report](https://arxiv.org/abs/2509.12508)
*Keyu An,Yanni Chen,Chong Deng,Changfeng Gao,Zhifu Gao,Bo Gong,Xiangang Li,Yabin Li,Xiang Lv,Yunjie Ji,Yiheng Jiang,Bin Ma,Haoneng Luo,Chongjia Ni,Zexu Pan,Yiping Peng,Zhendong Peng,Peiyao Wang,Hao Wang,Wen Wang,Wupeng Wang,Biao Tian,Zhentao Tan,Nan Yang,Bin Yuan,Jieping Ye,Jixing Yu,Qinglin Zhang,Kun Zou,Han Zhao,Shengkui Zhao,Jingren Zhou*

Main category: cs.CL

TL;DR: 本文介绍了基于大语言模型的大规模自动语音识别系统FunAudio - ASR，结合多种技术实现跨场景先进性能，经优化满足实际部署需求，实验显示其在实际应用数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型易产生幻觉，影响自动语音识别在现实应用中的用户体验，需要开发更优的系统。

Method: 提出FunAudio - ASR系统，将海量数据、大模型容量、大语言模型集成和强化学习相结合，并针对实际部署进行优化。

Result: 多数基于大语言模型的自动语音识别系统在开源基准测试表现好，但在实际行业评估集表现不佳；FunAudio - ASR在实际应用数据集上达到了最优性能。

Conclusion: FunAudio - ASR系统有效且鲁棒，能满足实际应用需求。

Abstract: In recent years, automatic speech recognition (ASR) has witnessed
transformative advancements driven by three complementary paradigms: data
scaling, model size scaling, and deep integration with large language models
(LLMs). However, LLMs are prone to hallucination, which can significantly
degrade user experience in real-world ASR applications. In this paper, we
present FunAudio-ASR, a large-scale, LLM-based ASR system that synergistically
combines massive data, large model capacity, LLM integration, and reinforcement
learning to achieve state-of-the-art performance across diverse and complex
speech recognition scenarios. Moreover, FunAudio-ASR is specifically optimized
for practical deployment, with enhancements in streaming capability, noise
robustness, code-switching, hotword customization, and satisfying other
real-world application requirements. Experimental results show that while most
LLM-based ASR systems achieve strong performance on open-source benchmarks,
they often underperform on real industry evaluation sets. Thanks to
production-oriented optimizations, FunAudio-ASR achieves SOTA performance on
real application datasets, demonstrating its effectiveness and robustness in
practical settings.

</details>


### [275] [EconProver: Towards More Economical Test-Time Scaling for Automated Theorem Proving](https://arxiv.org/abs/2509.12603)
*Mukai Li,Linfeng Song,Zhenwen Liang,Jiahao Xu,Shansan Gong,Qi Liu,Haitao Mi,Dong Yu*

Main category: cs.CL

TL;DR: 本文系统比较ATP模型测试时扩展策略效率，指出当前开源方法低效，提出EconRL管道的两种方法降低计算成本，实验表明EconProver以12%成本达类似性能。


<details>
  <summary>Details</summary>
Motivation: 现有ATP模型测试时扩展策略计算开销大，且成本分析忽略不同策略采样成本差异，需降低成本同时保持性能。

Method: 提出可集成到EconRL管道的两种互补方法：动态CoT切换机制减少令牌消耗；带可训练前缀的多样化并行扩展强化学习提高受限采样通过率。

Result: 在miniF2F和ProofNet上实验，EconProver以12%计算成本达到与基线方法相当的性能。

Conclusion: 为部署轻量级ATP模型且不牺牲性能提供可行见解。

Abstract: Large Language Models (LLMs) have recently advanced the field of Automated
Theorem Proving (ATP), attaining substantial performance gains through widely
adopted test-time scaling strategies, notably reflective Chain-of-Thought (CoT)
reasoning and increased sampling passes. However, they both introduce
significant computational overhead for inference. Moreover, existing cost
analyses typically regulate only the number of sampling passes, while
neglecting the substantial disparities in sampling costs introduced by
different scaling strategies. In this paper, we systematically compare the
efficiency of different test-time scaling strategies for ATP models and
demonstrate the inefficiency of the current state-of-the-art (SOTA) open-source
approaches. We then investigate approaches to significantly reduce token usage
and sample passes while maintaining the original performance. Specifically, we
propose two complementary methods that can be integrated into a unified EconRL
pipeline for amplified benefits: (1) a dynamic Chain-of-Thought (CoT) switching
mechanism designed to mitigate unnecessary token consumption, and (2) Diverse
parallel-scaled reinforcement learning (RL) with trainable prefixes to enhance
pass rates under constrained sampling passes. Experiments on miniF2F and
ProofNet demonstrate that our EconProver achieves comparable performance to
baseline methods with only 12% of the computational cost. This work provides
actionable insights for deploying lightweight ATP models without sacrificing
performance.

</details>


### [276] [Positional Encoding via Token-Aware Phase Attention](https://arxiv.org/abs/2509.12635)
*Yu,Wang,Sheng Shen,Rémi Munos,Hongyuan Zhan,Yuandong Tian*

Main category: cs.CL

TL;DR: 指出RoPE在长上下文建模有局限，提出新的位置编码方法TAPA，其在长上下文表现更优。


<details>
  <summary>Details</summary>
Motivation: RoPE在长上下文建模中存在距离相关偏差的局限，其扩展方法需预训练后调整。

Method: 引入Token - Aware Phase Attention (TAPA)，将可学习的相位函数融入注意力机制。

Result: TAPA能保持长距离的token交互，可通过直接和轻微调扩展到更长上下文，能外推到未见长度，且在长上下文上困惑度显著低于RoPE系列。

Conclusion: TAPA是一种比RoPE更适合长上下文建模的位置编码方法。

Abstract: We prove under practical assumptions that Rotary Positional Embedding (RoPE)
introduces an intrinsic distance-dependent bias in attention scores that limits
RoPE's ability to model long-context. RoPE extension methods may alleviate this
issue, but they typically require post-hoc adjustments after pretraining, such
as rescaling or hyperparameters retuning. This paper introduces Token-Aware
Phase Attention (TAPA), a new positional encoding method that incorporates a
learnable phase function into the attention mechanism. TAPA preserves token
interactions over long range, extends to longer contexts with direct and light
fine-tuning, extrapolates to unseen lengths, and attains significantly lower
perplexity on long-context than RoPE families.

</details>


### [277] [Don't Change My View: Ideological Bias Auditing in Large Language Models](https://arxiv.org/abs/2509.12652)
*Paul Kröger,Emilio Barkett*

Main category: cs.CL

TL;DR: 本文提出一种统计方法检测大语言模型的意识形态引导，通过分析输出分布变化，实验验证了其适用性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型输出会影响公众舆论，需检测其是否被引导至特定意识形态立场。

Method: 将之前提出的统计方法应用于意识形态偏见审计，分析主题相关提示下模型输出的分布变化。

Result: 通过一系列实验验证了方法的实际适用性和支持事后审计大语言模型行为的潜力。

Conclusion: 该方法适合审计专有黑盒系统，可检测大语言模型的意识形态引导尝试。

Abstract: As large language models (LLMs) become increasingly embedded in products used
by millions, their outputs may influence individual beliefs and, cumulatively,
shape public opinion. If the behavior of LLMs can be intentionally steered
toward specific ideological positions, such as political or religious views,
then those who control these systems could gain disproportionate influence over
public discourse. Although it remains an open question whether LLMs can
reliably be guided toward coherent ideological stances and whether such
steering can be effectively prevented, a crucial first step is to develop
methods for detecting when such steering attempts occur. In this work, we adapt
a previously proposed statistical method to the new context of ideological bias
auditing. Our approach carries over the model-agnostic design of the original
framework, which does not require access to the internals of the language
model. Instead, it identifies potential ideological steering by analyzing
distributional shifts in model outputs across prompts that are thematically
related to a chosen topic. This design makes the method particularly suitable
for auditing proprietary black-box systems. We validate our approach through a
series of experiments, demonstrating its practical applicability and its
potential to support independent post hoc audits of LLM behavior.

</details>


### [278] [SENTRA: Selected-Next-Token Transformer for LLM Text Detection](https://arxiv.org/abs/2509.12385)
*Mitchell Plyler,Yilun Zhang,Alexander Tuzhilin,Saoud Khalifah,Sen Tian*

Main category: cs.CL

TL;DR: 本文提出通用的大语言模型文本检测器SENTRA，在跨领域检测中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 大语言模型能力提升且应用广泛，其生成文本未声明的滥用问题需解决，要检测这类文本。

Method: 提出基于Transformer的编码器SENTRA，利用选定下一个词概率序列，并在大量未标记数据上进行对比预训练。

Result: 在三个流行公共数据集、24个文本领域的实验表明，SENTRA在跨领域设置中显著优于流行基线。

Conclusion: SENTRA是一个通用的分类器，能有效检测大语言模型生成的未声明文本。

Abstract: LLMs are becoming increasingly capable and widespread. Consequently, the
potential and reality of their misuse is also growing. In this work, we address
the problem of detecting LLM-generated text that is not explicitly declared as
such. We present a novel, general-purpose, and supervised LLM text detector,
SElected-Next-Token tRAnsformer (SENTRA). SENTRA is a Transformer-based encoder
leveraging selected-next-token-probability sequences and utilizing contrastive
pre-training on large amounts of unlabeled data. Our experiments on three
popular public datasets across 24 domains of text demonstrate SENTRA is a
general-purpose classifier that significantly outperforms popular baselines in
the out-of-domain setting.

</details>


### [279] [The LLM Already Knows: Estimating LLM-Perceived Question Difficulty via Hidden Representations](https://arxiv.org/abs/2509.12886)
*Yubo Zhu,Dongrui Liu,Zecheng Lin,Wei Tong,Sheng Zhong,Jing Shao*

Main category: cs.CL

TL;DR: 提出一种仅利用目标大语言模型隐藏表示的难度估计新方法，实验证明其优于现有基线，还可指导自适应推理策略提高效率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型输入问题难度估计方法存在计算成本高或通用性不足的问题。

Method: 将token级生成过程建模为马尔可夫链，定义价值函数根据初始隐藏状态估计难度。

Result: 在文本和多模态任务中，该方法在难度估计上始终优于现有基线，应用于自适应推理策略可提高推理效率。

Conclusion: 所提方法能高效准确地估计大语言模型输入问题的难度，并可用于指导自适应推理策略。

Abstract: Estimating the difficulty of input questions as perceived by large language
models (LLMs) is essential for accurate performance evaluation and adaptive
inference. Existing methods typically rely on repeated response sampling,
auxiliary models, or fine-tuning the target model itself, which may incur
substantial computational costs or compromise generality. In this paper, we
propose a novel approach for difficulty estimation that leverages only the
hidden representations produced by the target LLM. We model the token-level
generation process as a Markov chain and define a value function to estimate
the expected output quality given any hidden state. This allows for efficient
and accurate difficulty estimation based solely on the initial hidden state,
without generating any output tokens. Extensive experiments across both textual
and multimodal tasks demonstrate that our method consistently outperforms
existing baselines in difficulty estimation. Moreover, we apply our difficulty
estimates to guide adaptive reasoning strategies, including Self-Consistency,
Best-of-N, and Self-Refine, achieving higher inference efficiency with fewer
generated tokens.

</details>


### [280] [Conan-Embedding-v2: Training an LLM from Scratch for Text Embeddings](https://arxiv.org/abs/2509.12892)
*Shiyu Li,Yang Tang,Ruijie Liu,Shi-Zhe Chen,Xi Chen*

Main category: cs.CL

TL;DR: 提出1.4B参数的Conan - embedding - v2文本嵌入模型，通过新方法缩小数据和训练差距，在MTEB和中文MTEB上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 解决现有用LoRA微调大语言模型做文本嵌入任务时存在的数据和训练差距问题。

Method: 增加新闻数据和多语言对用于预训练；提出跨语言检索数据集；引入软掩码机制；提出动态硬负采样方法。

Result: Conan - embedding - v2在Massive Text Embedding Benchmark (MTEB)和中文MTEB (2025年5月19日)上取得SOTA性能。

Conclusion: Conan - embedding - v2模型直观有效，仅约1.4B参数就取得良好效果。

Abstract: Large language models (LLMs) have recently demonstrated excellent performance
in text embedding tasks. Previous work usually use LoRA to fine-tune existing
LLMs, which are limited by the data and training gap between LLMs and embedding
models. In this work, we introduce Conan-embedding-v2, a new 1.4B-parameter LLM
trained from scratch and fine-tuned as a text embedder. First, we add news data
and multilingual pairs for LLM pretraining to bridge the data gap. Based on
this, we propose a cross-lingual retrieval dataset that enables the LLM to
better integrate embeddings across different languages. Second, whereas LLMs
use a causal mask with token-level loss, embedding models use a bidirectional
mask with sentence-level loss. This training gap makes full fine-tuning less
effective than LoRA. We introduce a soft-masking mechanism to gradually
transition between these two types of masks, enabling the model to learn more
comprehensive representations. Based on this, we propose a dynamic hard
negative mining method that exposes the model to more difficult negative
examples throughout the training process. Being intuitive and effective, with
only approximately 1.4B parameters, Conan-embedding-v2 achieves SOTA
performance on both the Massive Text Embedding Benchmark (MTEB) and Chinese
MTEB (May 19, 2025).

</details>


### [281] [All Roads Lead to Rome: Graph-Based Confidence Estimation for Large Language Model Reasoning](https://arxiv.org/abs/2509.12908)
*Caiqi Zhang,Chang Shu,Ehsan Shareghi,Nigel Collier*

Main category: cs.CL

TL;DR: 提出适用于推理任务的免训练、基于图的置信度估计方法，实验显示有改进。


<details>
  <summary>Details</summary>
Motivation: 现有置信度估计方法主要用于事实问答任务，难以泛化到推理任务。

Method: 将推理路径建模为有向图，利用图的中心性、路径收敛性和路径加权等属性估计置信度。

Result: 在三个推理数据集上使用两个大语言模型的实验表明，置信度估计得到改进，两个下游任务的性能得到提升。

Conclusion: 所提出的基于图的置信度估计方法适用于推理任务，能有效提高置信度估计和下游任务性能。

Abstract: Confidence estimation is essential for the reliable deployment of large
language models (LLMs). Existing methods are primarily designed for factual QA
tasks and often fail to generalize to reasoning tasks. To address this gap, we
propose a set of training-free, graph-based confidence estimation methods
tailored to reasoning tasks. Our approach models reasoning paths as directed
graphs and estimates confidence by exploiting graph properties such as
centrality, path convergence, and path weighting. Experiments with two LLMs on
three reasoning datasets demonstrate improved confidence estimation and
enhanced performance on two downstream tasks.

</details>


### [282] [Investigating ReLoRA: Effects on the Learning Dynamics of Small Language Models](https://arxiv.org/abs/2509.12960)
*Yuval Weiss,David Demitri Africa,Paula Buttery,Richard Diehl Martinez*

Main category: cs.CL

TL;DR: 研究ReLoRA在小语言模型（SLMs）预训练中的表现，发现其性能不如标准训练，低秩更新策略可能难用于SLM预训练。


<details>
  <summary>Details</summary>
Motivation: 现有对通过ReLoRA将参数高效方法扩展到预训练的研究较少，尤其是针对小语言模型，其计算和环境成本较低。

Method: 对11M - 66M参数的SLMs进行系统研究，通过消融实验评估性能和学习动态。

Result: ReLoRA在损失、Paloma困惑度和BLiMP上总体表现比标准训练差，模型越大差距越大，且强化了小模型的秩缺陷。

Conclusion: 低秩更新策略可能不易转移到SLM预训练，需要在低计算场景下开展更多研究。

Abstract: Parameter-efficient methods such as LoRA have revolutionised the fine-tuning
of LLMs. Still, their extension to pretraining via ReLoRA is less well
understood, especially for small language models (SLMs), which offer lower
computational and environmental costs. This work is the first systematic study
of ReLoRA in SLMs (11M-66M parameters), evaluating both performance and
learning dynamics. Through ablation experiments, we find that ReLoRA generally
performs worse than standard training on loss, Paloma perplexity and BLiMP,
with the gap widening for the larger models. Further analysis of the learning
dynamics of the models indicates that ReLoRA reinforces the rank deficiencies
found in smaller models. These results indicate that low-rank update strategies
may not transfer easily to SLM pretraining, highlighting the need for more
research in the low-compute regime.

</details>


### [283] [Multi-Model Synthetic Training for Mission-Critical Small Language Models](https://arxiv.org/abs/2509.13047)
*Nolan Platt,Pragyansmita Nayak*

Main category: cs.CL

TL;DR: 提出用大语言模型作一次性教师的方法，实现海事情报成本261倍降低，微调后的小模型有较高准确率，为专业AI合成数据集生成领域做贡献。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在专业领域应用受特定领域训练数据稀缺和复杂性的限制。

Method: 将32亿条船舶跟踪记录通过多模型生成转化为21,543个合成问答对，微调Qwen2.5 - 7B模型。

Result: 微调后的Qwen2.5 - 7B模型在海事任务中准确率达75%，成本大幅降低。

Conclusion: 适当微调的小模型可提供与昂贵大模型相近的准确率，该方法有广泛应用。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
many domains, yet their application to specialized fields remains constrained
by the scarcity and complexity of domain-specific training data. We present a
novel approach that achieves a 261x cost reduction for maritime intelligence by
using LLMs as one-time teachers rather than using them directly for inference.
Our method transforms 3.2 billion Automatic Identification System (AIS) vessel
tracking records into 21,543 synthetic question and answer pairs through
multi-model generation (GPT-4o and o3-mini), preventing overfitting and
ensuring accurate reasoning. The resulting fine-tuned Qwen2.5-7B model achieves
75% accuracy on maritime tasks, while being substantially cheaper than using a
larger model for inference. We show that smaller, cheaper models -- when fine
tuned properly -- can provide similar accuracy compared to larger models that
are prohibitively expensive. Our work contributes to the growing field of
synthetic dataset generation for specialized AI applications and presents a
highly reproducible framework for domains where manual annotation is
infeasible. Beyond expanding research in the growing field of specialized small
language models, our approach has immediate applications in maritime safety,
security operations, and vessel traffic management systems in various
industries.

</details>


### [284] [Shaping Explanations: Semantic Reward Modeling with Encoder-Only Transformers for GRPO](https://arxiv.org/abs/2509.13081)
*Francesco Pappone,Ruggero Marino Lazzaroni,Federico Califano,Niccolò Gentile,Roberto Marras*

Main category: cs.CL

TL;DR: 本文提出在GRPO框架中使用小型编码器变压器作为语义奖励模型的新方法，应用于意大利医学院入学考试模型训练，结果显示该方法能提升解释的忠实性和清晰度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型输出与复杂定性目标对齐存在挑战，标准强化学习技术有局限性。

Method: 在GRPO框架中使用小型编码器变压器作为语义奖励模型，基于余弦相似度提供奖励信号，应用于意大利医学院入学考试模型训练，结合标准领域自适应持续预训练和监督微调。

Result: GRPO结合提出的语义奖励方法相比强监督微调基线显著提高了解释的忠实性和清晰度。

Conclusion: 轻量级编码器模型用于复杂生成任务的细致奖励塑造是有效的。

Abstract: While Large Language Models (LLMs) excel at generating human-like text,
aligning their outputs with complex, qualitative goals like pedagogical
soundness remains a significant challenge. Standard reinforcement learning
techniques often rely on slow and expensive LLM-as-a-judge evaluations or on
brittle, keyword-based metrics like ROUGE, which fail to capture the semantic
essence of a high-quality explanation. In this work, we introduce a novel
approach to reward shaping within the Group Relative Policy Optimisation (GRPO)
framework. Our central contribution is the use of a small, efficient
encoder-only transformer as a semantic reward model. This model provides a
dense, semantically rich reward signal based on the cosine similarity between a
generated explanation and a ground-truth reference, guiding the policy towards
explanations that are not just factually correct but also structurally and
conceptually aligned with expert reasoning. We apply this method to the task of
training a model for the Italian medical-school entrance examinations,
following standard domain-adaptive continued pre-training (CPT) and supervised
fine-tuning (SFT). Our results demonstrate that GRPO with our proposed semantic
reward significantly improves explanation faithfulness and clarity over a
strong SFT baseline, showcasing the power of using lightweight encoder models
for nuanced reward shaping in complex generation tasks

</details>


### [285] [ChartGaze: Enhancing Chart Understanding in LVLMs with Eye-Tracking Guided Attention Refinement](https://arxiv.org/abs/2509.13282)
*Ali Salamatian,Amirhossein Abaskohi,Wan-Cyuan Fan,Mir Rayat Imtiaz Hossain,Leonid Sigal,Giuseppe Carenini*

Main category: cs.CL

TL;DR: 提出ChartGaze数据集，对比人类与模型注意力，提出引导注意力细化方法提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型在图表问答任务中，关注无关区域，导致可解释性和准确性下降。

Method: 创建ChartGaze数据集，对比人类与模型注意力，提出引导注意力细化方法。

Result: 提出的方法使多个模型答案准确率和注意力对齐度提升最高达2.56个百分点。

Conclusion: 引入人类注视可提升图表大视觉语言模型推理质量和可解释性。

Abstract: Charts are a crucial visual medium for communicating and representing
information. While Large Vision-Language Models (LVLMs) have made progress on
chart question answering (CQA), the task remains challenging, particularly when
models attend to irrelevant regions of the chart. In this work, we present
ChartGaze, a new eye-tracking dataset that captures human gaze patterns during
chart reasoning tasks. Through a systematic comparison of human and model
attention, we find that LVLMs often diverge from human gaze, leading to reduced
interpretability and accuracy. To address this, we propose a gaze-guided
attention refinement that aligns image-text attention with human fixations. Our
approach improves both answer accuracy and attention alignment, yielding gains
of up to 2.56 percentage points across multiple models. These results
demonstrate the promise of incorporating human gaze to enhance both the
reasoning quality and interpretability of chart-focused LVLMs.

</details>


### [286] [Do Natural Language Descriptions of Model Activations Convey Privileged Information?](https://arxiv.org/abs/2509.13316)
*Millicent Li,Alberto Mario Ceballos Arroyo,Giordano Rogers,Naomi Saphra,Byron C. Wallace*

Main category: cs.CL

TL;DR: 本文评估大语言模型激活语言化方法，发现现有数据集不适合评估，语言化常反映生成器参数知识，需针对性基准和实验控制。


<details>
  <summary>Details</summary>
Motivation: 探究激活语言化方法是否能提供目标模型内部工作的特权知识，还是仅传达输入信息。

Method: 对先前工作中使用的数据集上的流行语言化方法进行批判性评估，并进行对照实验。

Result: 这些方法在不访问目标模型内部的情况下就能在基准测试中成功，语言化常反映语言化器大语言模型的参数知识而非目标大语言模型的激活。

Conclusion: 需要有针对性的基准和实验控制来严格评估语言化方法是否能为大语言模型的操作提供有意义的见解。

Abstract: Recent interpretability methods have proposed to translate LLM internal
representations into natural language descriptions using a second verbalizer
LLM. This is intended to illuminate how the target model represents and
operates on inputs. But do such activation verbalization approaches actually
provide privileged knowledge about the internal workings of the target model,
or do they merely convey information about its inputs? We critically evaluate
popular verbalization methods across datasets used in prior work and find that
they succeed at benchmarks without any access to target model internals,
suggesting that these datasets are not ideal for evaluating verbalization
methods. We then run controlled experiments which reveal that verbalizations
often reflect the parametric knowledge of the verbalizer LLM which generated
them, rather than the activations of the target LLM being decoded. Taken
together, our results indicate a need for targeted benchmarks and experimental
controls to rigorously assess whether verbalization methods provide meaningful
insights into the operations of LLMs.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [287] [Identifying Information Technology Research Trends through Text Mining of NSF Awards](https://arxiv.org/abs/2509.12245)
*Said Varlioglu,Hazem Said,Murat Ozer,Nelly Elsayed*

Main category: cs.DL

TL;DR: 本文通过对1985 - 2024年NSF CISE获奖资助项目摘要进行文本挖掘分析，探索信息技术研究领域根源，发现其更关注用户、组织和社会需求。


<details>
  <summary>Details</summary>
Motivation: 信息技术虽为独立研究领域，但难以与其他相近领域区分，故探索其研究领域根源。

Method: 对50780篇NSF CISE获奖资助项目摘要进行大规模文本挖掘分析，根据项目内容分类，依据文献定义标记以人为中心的为信息技术研究项目，以基础设施为中心的为其他研究项目。

Result: 信息技术与其他相近领域的区别在于更关注用户、组织和社会的需求。

Conclusion: 信息技术研究领域有其独特性，核心在于关注用户、组织和社会需求。

Abstract: Information Technology (IT) is recognized as an independent and unique
research field. However, there has been ambiguity and difficulty in identifying
and differentiating IT research from other close variations. Given this
context, this paper aimed to explore the roots of the Information Technology
(IT) research domain by conducting a large-scale text mining analysis of 50,780
abstracts from awarded NSF CISE grants from 1985 to 2024. We categorized the
awards based on their program content, labeling human-centric programs as IT
research programs and infrastructure-centric programs as other research
programs based on the IT definitions in the literature. This novel approach
helped us identify the core concepts of IT research and compare the
similarities and differences between IT research and other research areas. The
results showed that IT differentiates itself from other close variations by
focusing more on the needs of users, organizations, and societies.

</details>


### [288] [Layout-Aware OCR for Black Digital Archives with Unsupervised Evaluation](https://arxiv.org/abs/2509.13236)
*Fitsum Sileshi Beyene,Christopher L. Dancy*

Main category: cs.DL

TL;DR: 本文提出适用于黑人报纸档案的布局感知OCR管道和无监督评估框架，评估显示其有一定优势，强调尊重文化布局逻辑的重要性。


<details>
  <summary>Details</summary>
Motivation: 黑人数字档案在AI研究和基础设施中代表性不足，数字化黑人历史报纸时存在诸多问题阻碍准确转录。

Method: 提出布局感知OCR管道，集成合成布局生成、增强数据上的模型预训练和YOLO检测器；使用SCS、RE和TRS三个无注释评估指标。

Result: 在400页数据集上评估表明，布局感知OCR相比全页基线提高了结构多样性、减少了冗余，连贯性有适度权衡。

Conclusion: 强调在AI驱动的文档理解中尊重文化布局逻辑的重要性，为未来社区驱动和符合伦理的档案AI系统奠定基础。

Abstract: Despite their cultural and historical significance, Black digital archives
continue to be a structurally underrepresented area in AI research and
infrastructure. This is especially evident in efforts to digitize historical
Black newspapers, where inconsistent typography, visual degradation, and
limited annotated layout data hinder accurate transcription, despite the
availability of various systems that claim to handle optical character
recognition (OCR) well. In this short paper, we present a layout-aware OCR
pipeline tailored for Black newspaper archives and introduce an unsupervised
evaluation framework suited to low-resource archival contexts. Our approach
integrates synthetic layout generation, model pretraining on augmented data,
and a fusion of state-of-the-art You Only Look Once (YOLO) detectors. We used
three annotation-free evaluation metrics, the Semantic Coherence Score (SCS),
Region Entropy (RE), and Textual Redundancy Score (TRS), which quantify
linguistic fluency, informational diversity, and redundancy across OCR regions.
Our evaluation on a 400-page dataset from ten Black newspaper titles
demonstrates that layout-aware OCR improves structural diversity and reduces
redundancy compared to full-page baselines, with modest trade-offs in
coherence. Our results highlight the importance of respecting cultural layout
logic in AI-driven document understanding and lay the foundation for future
community-driven and ethically grounded archival AI systems.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [289] [Out of Distribution Detection in Self-adaptive Robots with AI-powered Digital Twins](https://arxiv.org/abs/2509.12982)
*Erblin Isaku,Hassan Sartaj,Shaukat Ali,Beatriz Sanguino,Tongtong Wang,Guoyuan Li,Houxiang Zhang,Thomas Peyrucain*

Main category: cs.RO

TL;DR: 提出数字孪生方法ODiSAR用于自适应机器人的OOD检测，评估显示检测性能高且有可解释性。


<details>
  <summary>Details</summary>
Motivation: 自适应机器人在复杂不确定环境需主动检测和处理异常行为，数字孪生可用于OOD检测。

Method: 使用基于Transformer的数字孪生预测机器人状态，用重建误差和蒙特卡罗丢弃法进行不确定性量化，结合重建误差和预测方差检测OOD行为，包含可解释层。

Result: 在两个工业机器人数字孪生评估中，ODiSAR能预测行为并检测OOD事件，实现高达98% AUROC、96% TNR@TPR95和95% F1 - score的检测性能。

Conclusion: ODiSAR检测性能高，能提供可解释见解支持自适应。

Abstract: Self-adaptive robots (SARs) in complex, uncertain environments must
proactively detect and address abnormal behaviors, including
out-of-distribution (OOD) cases. To this end, digital twins offer a valuable
solution for OOD detection. Thus, we present a digital twin-based approach for
OOD detection (ODiSAR) in SARs. ODiSAR uses a Transformer-based digital twin to
forecast SAR states and employs reconstruction error and Monte Carlo dropout
for uncertainty quantification. By combining reconstruction error with
predictive variance, the digital twin effectively detects OOD behaviors, even
in previously unseen conditions. The digital twin also includes an
explainability layer that links potential OOD to specific SAR states, offering
insights for self-adaptation. We evaluated ODiSAR by creating digital twins of
two industrial robots: one navigating an office environment, and another
performing maritime ship navigation. In both cases, ODiSAR forecasts SAR
behaviors (i.e., robot trajectories and vessel motion) and proactively detects
OOD events. Our results showed that ODiSAR achieved high detection performance
-- up to 98\% AUROC, 96\% TNR@TPR95, and 95\% F1-score -- while providing
interpretable insights to support self-adaptation.

</details>


### [290] [An integrated process for design and control of lunar robotics using AI and simulation](https://arxiv.org/abs/2509.12367)
*Daniel Lindmark,Jonas Andersson,Kenneth Bodin,Tora Bodin,Hugo Börjesson,Fredrik Nordfeldth,Martin Servin*

Main category: cs.RO

TL;DR: 提出支持月球建设设备开发集成过程的技术框架并展示案例


<details>
  <summary>Details</summary>
Motivation: 设想并行探索物理设计与控制的月球建设设备开发集成过程，为此提出技术框架

Method: 借助OpenPLX语言，将CAD模型、自主系统与高保真实时3D模拟相连接

Result: 通过两个案例展示框架能力，如结合视觉语言模型和强化学习控制策略的自主月球车

Conclusion: 所提出的技术框架对月球建设设备开发有支持作用

Abstract: We envision an integrated process for developing lunar construction
equipment, where physical design and control are explored in parallel. In this
paper, we describe a technical framework that supports this process. It relies
on OpenPLX, a readable/writable declarative language that links CAD-models and
autonomous systems to high-fidelity, real-time 3D simulations of contacting
multibody dynamics, machine regolith interaction forces, and non-ideal sensors.
To demonstrate its capabilities, we present two case studies, including an
autonomous lunar rover that combines a vision-language model for navigation
with a reinforcement learning-based control policy for locomotion.

</details>


### [291] [Geometric Red-Teaming for Robotic Manipulation](https://arxiv.org/abs/2509.12379)
*Divyam Goel,Yufei Wang,Tiancheng Wu,Guixiu Qiao,Pavel Piliptchak,David Held,Zackory Erickson*

Main category: cs.RO

TL;DR: 提出几何红队框架GRT用于探测机器人操作策略鲁棒性，发现静态基准遗漏的失效模式，蓝队微调可提升策略性能，且结果在真实机械臂上得到验证。


<details>
  <summary>Details</summary>
Motivation: 现有机器人操作评估协议对系统在合理变化下的失效情况洞察有限，需要新的鲁棒性评估方法。

Method: 引入GRT框架，结合基于雅可比场的变形模型与无梯度、模拟器在环的优化策略，通过对象中心的几何扰动探测鲁棒性；还提出蓝队微调方法。

Result: GRT持续发现使策略性能崩溃的变形，蓝队微调使任务成功率最高提升60个百分点，在真实机械臂上模拟的崩溃形状使成功率从90%降至22.5%，蓝队微调后恢复至90%。

Conclusion: GRT是一个通用的、以对象为中心的机器人操作鲁棒性评估框架，红队发现的几何形状对策略优化有实用价值。

Abstract: Standard evaluation protocols in robotic manipulation typically assess policy
performance over curated, in-distribution test sets, offering limited insight
into how systems fail under plausible variation. We introduce Geometric
Red-Teaming (GRT), a red-teaming framework that probes robustness through
object-centric geometric perturbations, automatically generating CrashShapes --
structurally valid, user-constrained mesh deformations that trigger
catastrophic failures in pre-trained manipulation policies. The method
integrates a Jacobian field-based deformation model with a gradient-free,
simulator-in-the-loop optimization strategy. Across insertion, articulation,
and grasping tasks, GRT consistently discovers deformations that collapse
policy performance, revealing brittle failure modes missed by static
benchmarks. By combining task-level policy rollouts with constraint-aware shape
exploration, we aim to build a general purpose framework for structured,
object-centric robustness evaluation in robotic manipulation. We additionally
show that fine-tuning on individual CrashShapes, a process we refer to as
blue-teaming, improves task success by up to 60 percentage points on those
shapes, while preserving performance on the original object, demonstrating the
utility of red-teamed geometries for targeted policy refinement. Finally, we
validate both red-teaming and blue-teaming results with a real robotic arm,
observing that simulated CrashShapes reduce task success from 90% to as low as
22.5%, and that blue-teaming recovers performance to up to 90% on the
corresponding real-world geometry -- closely matching simulation outcomes.
Videos and code can be found on our project website:
https://georedteam.github.io/ .

</details>


### [292] [Pre-trained Visual Representations Generalize Where it Matters in Model-Based Reinforcement Learning](https://arxiv.org/abs/2509.12531)
*Scott Jones,Liyou Zhou,Sebastian W. Pattinson*

Main category: cs.RO

TL;DR: 研究预训练视觉模型（PVMs）在基于模型的强化学习（MBRL）中的有效性，发现其在视觉领域转移场景下比从头训练的模型表现好，部分微调可保持最高平均任务性能，表明PVMs能提升视觉策略学习的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 典型的视觉运动策略学习方法泛化能力差，预训练视觉模型在无模型强化学习中能提升鲁棒性，但在MBRL中效果不佳，因此研究PVMs在MBRL中的有效性。

Method: 研究PVMs在MBRL中视觉领域转移下的泛化能力，并探究不同程度的PVMs微调效果。

Result: 在严重转移场景下，PVMs比从头训练的基线模型表现好，部分微调能在最极端的分布转移下保持最高平均任务性能。

Conclusion: PVMs在促进视觉策略学习的鲁棒性方面非常成功，建议在基于模型的机器人学习应用中更广泛地采用。

Abstract: In visuomotor policy learning, the control policy for the robotic agent is
derived directly from visual inputs. The typical approach, where a policy and
vision encoder are trained jointly from scratch, generalizes poorly to novel
visual scene changes. Using pre-trained vision models (PVMs) to inform a policy
network improves robustness in model-free reinforcement learning (MFRL). Recent
developments in Model-based reinforcement learning (MBRL) suggest that MBRL is
more sample-efficient than MFRL. However, counterintuitively, existing work has
found PVMs to be ineffective in MBRL. Here, we investigate PVM's effectiveness
in MBRL, specifically on generalization under visual domain shifts. We show
that, in scenarios with severe shifts, PVMs perform much better than a baseline
model trained from scratch. We further investigate the effects of varying
levels of fine-tuning of PVMs. Our results show that partial fine-tuning can
maintain the highest average task performance under the most extreme
distribution shifts. Our results demonstrate that PVMs are highly successful in
promoting robustness in visual policy learning, providing compelling evidence
for their wider adoption in model-based robotic learning applications.

</details>


### [293] [ActiveVLN: Towards Active Exploration via Multi-Turn RL in Vision-and-Language Navigation](https://arxiv.org/abs/2509.12618)
*Zekai Zhang,Weiye Zhu,Hewei Pan,Xiangchen Wang,Rongtao Xu,Xing Sun,Feng Zheng*

Main category: cs.RO

TL;DR: 提出ActiveVLN框架用于视觉语言导航任务，通过多轮强化学习实现主动探索，实验显示性能提升显著且模型更小。


<details>
  <summary>Details</summary>
Motivation: 现有基于MLLM的VLN方法数据收集和训练成本高，先前的强化学习方法缺乏与环境的动态交互和主动探索，限制了代理发现多样导航路线的能力。

Method: 提出ActiveVLN框架，先使用少量专家轨迹进行模仿学习启动代理，再通过多轮强化学习让代理迭代预测和执行动作、自动收集多样轨迹并通过GRPO目标优化多个滚动，还引入动态早停策略和工程优化。

Result: ActiveVLN相比基于DAgger和先前基于强化学习的后训练方法，在模仿学习基线模型上取得最大性能提升，使用更小模型也能达到与最先进方法相当的性能。

Conclusion: ActiveVLN框架在视觉语言导航任务中有效，能解决现有方法的局限，提升性能且模型更优，代码和数据即将发布。

Abstract: The Vision-and-Language Navigation (VLN) task requires an agent to follow
natural language instructions and navigate through complex environments.
Existing MLLM-based VLN methods primarily rely on imitation learning (IL) and
often use DAgger for post-training to mitigate covariate shift. While
effective, these approaches incur substantial data collection and training
costs. Reinforcement learning (RL) offers a promising alternative. However,
prior VLN RL methods lack dynamic interaction with the environment and depend
on expert trajectories for reward shaping, rather than engaging in open-ended
active exploration. This restricts the agent's ability to discover diverse and
plausible navigation routes. To address these limitations, we propose
ActiveVLN, a VLN framework that explicitly enables active exploration through
multi-turn RL. In the first stage, a small fraction of expert trajectories is
used for IL to bootstrap the agent. In the second stage, the agent iteratively
predicts and executes actions, automatically collects diverse trajectories, and
optimizes multiple rollouts via the GRPO objective. To further improve RL
efficiency, we introduce a dynamic early-stopping strategy to prune long-tail
or likely failed trajectories, along with additional engineering optimizations.
Experiments show that ActiveVLN achieves the largest performance gains over IL
baselines compared to both DAgger-based and prior RL-based post-training
methods, while reaching competitive performance with state-of-the-art
approaches despite using a smaller model. Code and data will be released soon.

</details>


### [294] [Deep Learning for Model-Free Prediction of Thermal States of Robot Joint Motors](https://arxiv.org/abs/2509.12739)
*Trung Kien La,Eric Guiffo Kaigom*

Main category: cs.RO

TL;DR: 本文采用无模型可扩展方法，用含LSTM和前馈层的深度神经网络，基于关节扭矩预测机器人关节电机热行为，取得不错结果。


<details>
  <summary>Details</summary>
Motivation: 应对近似模型大量参数推导、识别和验证的复杂性与不确定性挑战，因近似模型难以获取。

Method: 训练由多个隐藏LSTM和前馈层组成的深度神经网络，收集并处理关节扭矩数据。

Result: 对七关节冗余机器人关节电机温度动态的机器学习预测结果很有前景。

Conclusion: 基于机器学习的方法可有效预测机器人关节电机热行为。

Abstract: In this work, deep neural networks made up of multiple hidden Long Short-Term
Memory (LSTM) and Feedforward layers are trained to predict the thermal
behavior of the joint motors of robot manipulators. A model-free and scalable
approach is adopted. It accommodates complexity and uncertainty challenges
stemming from the derivation, identification, and validation of a large number
of parameters of an approximation model that is hardly available. To this end,
sensed joint torques are collected and processed to foresee the thermal
behavior of joint motors. Promising prediction results of the machine learning
based capture of the temperature dynamics of joint motors of a redundant robot
with seven joints are presented.

</details>


### [295] [Deep Generative and Discriminative Digital Twin endowed with Variational Autoencoder for Unsupervised Predictive Thermal Condition Monitoring of Physical Robots in Industry 6.0 and Society 6.0](https://arxiv.org/abs/2509.12740)
*Eric Guiffo Kaigom*

Main category: cs.RO

TL;DR: 本文提出利用带生成式AI的智能数字孪生管理机器人热异常，用热难度分数预测热可行性，满足新兴应用需求。


<details>
  <summary>Details</summary>
Motivation: 在抗脆弱制造和以人为本的社会任务中，机器人需自主应对热饱和与过热问题，传统关机策略影响生产力和舒适度，冷却策略难实施。

Method: 利用带生成式AI（变分自编码器）的智能数字孪生管理热异常，从变分自编码器的重建误差中得出热难度概念。

Result: 机器人可使用热难度分数预测、预估并共享所需运动轮廓的热可行性。

Conclusion: 该方法能满足工业6.0和社会6.0新兴应用的需求。

Abstract: Robots are unrelentingly used to achieve operational efficiency in Industry
4.0 along with symbiotic and sustainable assistance for the work-force in
Industry 5.0. As resilience, robustness, and well-being are required in
anti-fragile manufacturing and human-centric societal tasks, an autonomous
anticipation and adaption to thermal saturation and burns due to motors
overheating become instrumental for human safety and robot availability. Robots
are thereby expected to self-sustain their performance and deliver user
experience, in addition to communicating their capability to other agents in
advance to ensure fully automated thermally feasible tasks, and prolong their
lifetime without human intervention. However, the traditional robot shutdown,
when facing an imminent thermal saturation, inhibits productivity in factories
and comfort in the society, while cooling strategies are hard to implement
after the robot acquisition. In this work, smart digital twins endowed with
generative AI, i.e., variational autoencoders, are leveraged to manage
thermally anomalous and generate uncritical robot states. The notion of thermal
difficulty is derived from the reconstruction error of variational
autoencoders. A robot can use this score to predict, anticipate, and share the
thermal feasibility of desired motion profiles to meet requirements from
emerging applications in Industry 6.0 and Society 6.0.

</details>


### [296] [Force-Modulated Visual Policy for Robot-Assisted Dressing with Arm Motions](https://arxiv.org/abs/2509.12741)
*Alexis Yihong Hao,Yufei Wang,Navin Sriram Ravie,Bharath Hegde,David Held,Zackory Erickson*

Main category: cs.RO

TL;DR: 本文开发了能处理视觉遮挡的部分观测、适应穿衣过程中手臂动作的机器人辅助穿衣系统，经仿真和真人实验验证性能优。


<details>
  <summary>Details</summary>
Motivation: 以往机器人辅助穿衣工作存在简化假设，限制了现实应用，为实现有效舒适的穿衣体验，需开发能适应手臂动作的系统。

Method: 在有部分观测的仿真环境中训练策略，利用少量数据和视觉与力觉的多模态反馈在现实中微调策略。

Result: 在仿真和12名参与者264次真人实验中，策略成功为参与者穿上两件长袖日常衣物，能适应多种手臂动作，在任务完成度和用户反馈上远超先前基线。

Conclusion: 所提出的机器人辅助穿衣系统具有良好的适应性和安全性，能有效提升穿衣体验，优于现有方法。

Abstract: Robot-assisted dressing has the potential to significantly improve the lives
of individuals with mobility impairments. To ensure an effective and
comfortable dressing experience, the robot must be able to handle challenging
deformable garments, apply appropriate forces, and adapt to limb movements
throughout the dressing process. Prior work often makes simplifying assumptions
-- such as static human limbs during dressing -- which limits real-world
applicability. In this work, we develop a robot-assisted dressing system
capable of handling partial observations with visual occlusions, as well as
robustly adapting to arm motions during the dressing process. Given a policy
trained in simulation with partial observations, we propose a method to
fine-tune it in the real world using a small amount of data and multi-modal
feedback from vision and force sensing, to further improve the policy's
adaptability to arm motions and enhance safety. We evaluate our method in
simulation with simplified articulated human meshes and in a real world human
study with 12 participants across 264 dressing trials. Our policy successfully
dresses two long-sleeve everyday garments onto the participants while being
adaptive to various kinds of arm motions, and greatly outperforms prior
baselines in terms of task completion and user feedback. Video are available at
https://dressing-motion.github.io/.

</details>


### [297] [Toward Ownership Understanding of Objects: Active Question Generation with Large Language Model and Probabilistic Generative Model](https://arxiv.org/abs/2509.12754)
*Saki Hashimoto,Shoichi Hasegawa,Tomochika Ishikawa,Akira Taniguchi,Yoshinobu Hagiwara,Lotfi El Hafi,Tadahiro Taniguchi*

Main category: cs.RO

TL;DR: 提出主动所有权学习框架ActOwL，结合主动推理与大语言模型常识推理，提升机器人获取所有权知识能力。


<details>
  <summary>Details</summary>
Motivation: 家庭和办公室环境中机器人需理解物体所有权以执行指令，但仅靠视觉特征无法可靠推断所有权。

Method: 提出ActOwL框架，用概率生成模型选问题以最大化信息增益，借助大语言模型常识知识预分类物体，仅针对私有物体提问。

Result: 在模拟家庭环境和真实实验室环境实验中，ActOwL比基线方法用更少问题实现更高所有权聚类准确率。

Conclusion: 结合主动推理与大语言模型引导的常识推理有效，能提升机器人获取所有权知识能力，以执行实际和社交上合适的任务。

Abstract: Robots operating in domestic and office environments must understand object
ownership to correctly execute instructions such as ``Bring me my cup.''
However, ownership cannot be reliably inferred from visual features alone. To
address this gap, we propose Active Ownership Learning (ActOwL), a framework
that enables robots to actively generate and ask ownership-related questions to
users. ActOwL employs a probabilistic generative model to select questions that
maximize information gain, thereby acquiring ownership knowledge efficiently to
improve learning efficiency. Additionally, by leveraging commonsense knowledge
from Large Language Models (LLM), objects are pre-classified as either shared
or owned, and only owned objects are targeted for questioning. Through
experiments in a simulated home environment and a real-world laboratory
setting, ActOwL achieved significantly higher ownership clustering accuracy
with fewer questions than baseline methods. These findings demonstrate the
effectiveness of combining active inference with LLM-guided commonsense
reasoning, advancing the capability of robots to acquire ownership knowledge
for practical and socially appropriate task execution.

</details>


### [298] [Learning to Generate Pointing Gestures in Situated Embodied Conversational Agents](https://arxiv.org/abs/2509.12507)
*Anna Deichler,Siyang Wang,Simon Alexanderson,Jonas Beskow*

Main category: cs.RO

TL;DR: 提出结合模仿和强化学习为具身智能体生成指向手势的框架，评估显示比现有监督模型更自然准确。


<details>
  <summary>Details</summary>
Motivation: 机器人和智能体研究需实现与人类自然交流，非语言交流对灵活交互至关重要。

Method: 结合模仿和强化学习生成指向手势，使用小运动捕捉数据集学习运动控制策略。

Result: 在客观指标和虚拟现实指称游戏中，系统比现有监督模型有更高自然度和准确性。

Conclusion: 模仿 - 强化学习在交际手势生成中有前景，有应用于机器人的潜力。

Abstract: One of the main goals of robotics and intelligent agent research is to enable
natural communication with humans in physically situated settings. While recent
work has focused on verbal modes such as language and speech, non-verbal
communication is crucial for flexible interaction. We present a framework for
generating pointing gestures in embodied agents by combining imitation and
reinforcement learning. Using a small motion capture dataset, our method learns
a motor control policy that produces physically valid, naturalistic gestures
with high referential accuracy. We evaluate the approach against supervised
learning and retrieval baselines in both objective metrics and a virtual
reality referential game with human users. Results show that our system
achieves higher naturalness and accuracy than state-of-the-art supervised
models, highlighting the promise of imitation-RL for communicative gesture
generation and its potential application to robots.

</details>


### [299] [Multi-Robot Task Planning for Multi-Object Retrieval Tasks with Distributed On-Site Knowledge via Large Language Models](https://arxiv.org/abs/2509.12838)
*Kento Murata,Shoichi Hasegawa,Tomochika Ishikawa,Yoshinobu Hagiwara,Akira Taniguchi,Lotfi El Hafi,Tadahiro Taniguchi*

Main category: cs.RO

TL;DR: 本文提出利用大语言模型和空间概念的任务规划框架，将自然语言指令分解为子任务并分配给多机器人，实验效果良好。


<details>
  <summary>Details</summary>
Motivation: 解决每个机器人拥有不同现场情境知识时，任务分配给哪个机器人的难题。

Method: 提出利用大语言模型和空间概念的任务规划框架，设计新颖的少样本提示策略。

Result: 实验中该方法成功分配47/50，优于随机（28/50）和基于常识的分配（26/50），定性评估表明框架能处理多种指令。

Conclusion: 所提出的框架能有效处理自然语言指令，完成任务分解、分配、顺序规划和执行。

Abstract: It is crucial to efficiently execute instructions such as "Find an apple and
a banana" or "Get ready for a field trip," which require searching for multiple
objects or understanding context-dependent commands. This study addresses the
challenging problem of determining which robot should be assigned to which part
of a task when each robot possesses different situational on-site
knowledge-specifically, spatial concepts learned from the area designated to it
by the user. We propose a task planning framework that leverages large language
models (LLMs) and spatial concepts to decompose natural language instructions
into subtasks and allocate them to multiple robots. We designed a novel
few-shot prompting strategy that enables LLMs to infer required objects from
ambiguous commands and decompose them into appropriate subtasks. In our
experiments, the proposed method achieved 47/50 successful assignments,
outperforming random (28/50) and commonsense-based assignment (26/50).
Furthermore, we conducted qualitative evaluations using two actual mobile
manipulators. The results demonstrated that our framework could handle
instructions, including those involving ad hoc categories such as "Get ready
for a field trip," by successfully performing task decomposition, assignment,
sequential planning, and execution.

</details>


### [300] [Towards Context-Aware Human-like Pointing Gestures with RL Motion Imitation](https://arxiv.org/abs/2509.12880)
*Anna Deichler,Siyang Wang,Simon Alexanderson,Jonas Beskow*

Main category: cs.RO

TL;DR: 提出人类指向手势动作捕捉数据集，用强化学习训练策略实现上下文感知指向行为。


<details>
  <summary>Details</summary>
Motivation: 以往研究多关注指向识别而非生成，要解决生成问题。

Method: 使用运动捕捉获取人类指向手势数据，结合运动模仿的强化学习训练策略。

Result: 在模拟中实现上下文感知的指向行为，平衡任务性能和自然动态。

Conclusion: 所提方法能使机器人实现上下文感知指向行为，兼顾性能与自然性。

Abstract: Pointing is a key mode of interaction with robots, yet most prior work has
focused on recognition rather than generation. We present a motion capture
dataset of human pointing gestures covering diverse styles, handedness, and
spatial targets. Using reinforcement learning with motion imitation, we train
policies that reproduce human-like pointing while maximizing precision. Results
show our approach enables context-aware pointing behaviors in simulation,
balancing task performance with natural dynamics.

</details>


### [301] [A Design Co-Pilot for Task-Tailored Manipulators](https://arxiv.org/abs/2509.13077)
*Jonathan Külz,Sehoon Ha,Matthias Althoff*

Main category: cs.RO

TL;DR: 本文提出自动设计和优化特定环境下机器人形态的方法，加速专业设计生成，经实验验证有效且具现实适用性。


<details>
  <summary>Details</summary>
Motivation: 通用机器人设计难以适应特定任务，定制机器人开发周期长、成本高，需新方法解决。

Method: 学习不同机械臂逆运动学，用全可微框架对设计的机器人和逆运动学解进行基于梯度的微调。

Result: 该方法能快速生成设计，找到可在复杂环境导航的机器人、在指定工作空间表现良好的机械臂，还能适应不同硬件约束，并在现实中验证可行。

Conclusion: 所提方法可加速专业设计生成，实现即时适应和人机有效协作，有实际应用价值。

Abstract: Although robotic manipulators are used in an ever-growing range of
applications, robot manufacturers typically follow a ``one-fits-all''
philosophy, employing identical manipulators in various settings. This often
leads to suboptimal performance, as general-purpose designs fail to exploit
particularities of tasks. The development of custom, task-tailored robots is
hindered by long, cost-intensive development cycles and the high cost of
customized hardware. Recently, various computational design methods have been
devised to overcome the bottleneck of human engineering. In addition, a surge
of modular robots allows quick and economical adaptation to changing industrial
settings. This work proposes an approach to automatically designing and
optimizing robot morphologies tailored to a specific environment. To this end,
we learn the inverse kinematics for a wide range of different manipulators. A
fully differentiable framework realizes gradient-based fine-tuning of designed
robots and inverse kinematics solutions. Our generative approach accelerates
the generation of specialized designs from hours with optimization-based
methods to seconds, serving as a design co-pilot that enables instant
adaptation and effective human-AI collaboration. Numerical experiments show
that our approach finds robots that can navigate cluttered environments,
manipulators that perform well across a specified workspace, and can be adapted
to different hardware constraints. Finally, we demonstrate the real-world
applicability of our method by setting up a modular robot designed in
simulation that successfully moves through an obstacle course.

</details>


### [302] [An Uncertainty-Weighted Decision Transformer for Navigation in Dense, Complex Driving Scenarios](https://arxiv.org/abs/2509.13132)
*Zhihao Zhang,Chengyang Peng,Minghao Zhu,Ekim Yurtsever,Keith A. Redmill*

Main category: cs.RO

TL;DR: 提出用于复杂环岛场景战术驾驶的新框架，含Uncertainty - Weighted Decision Transformer (UWDT)，实验表明其在奖励、碰撞率和行为稳定性上优于基线，证明不确定性感知时空变换器在自动驾驶决策中的优势。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶在密集动态环境中需能利用空间结构和长时依赖且对不确定性有鲁棒性的决策系统。

Method: 提出新框架，将多通道鸟瞰占用网格与基于Transformer的序列建模集成；提出UWDT，用冻结的教师Transformer估计每个标记的预测熵，作为学生模型损失函数的权重。

Result: 在环岛模拟器不同交通密度实验中，UWDT在奖励、碰撞率和行为稳定性方面始终优于其他基线。

Conclusion: 不确定性感知的时空变换器能为复杂交通环境中的自动驾驶提供更安全高效的决策。

Abstract: Autonomous driving in dense, dynamic environments requires decision-making
systems that can exploit both spatial structure and long-horizon temporal
dependencies while remaining robust to uncertainty. This work presents a novel
framework that integrates multi-channel bird's-eye-view occupancy grids with
transformer-based sequence modeling for tactical driving in complex roundabout
scenarios. To address the imbalance between frequent low-risk states and rare
safety-critical decisions, we propose the Uncertainty-Weighted Decision
Transformer (UWDT). UWDT employs a frozen teacher transformer to estimate
per-token predictive entropy, which is then used as a weight in the student
model's loss function. This mechanism amplifies learning from uncertain,
high-impact states while maintaining stability across common low-risk
transitions. Experiments in a roundabout simulator, across varying traffic
densities, show that UWDT consistently outperforms other baselines in terms of
reward, collision rate, and behavioral stability. The results demonstrate that
uncertainty-aware, spatial-temporal transformers can deliver safer and more
efficient decision-making for autonomous driving in complex traffic
environments.

</details>


### [303] [HARMONIC: A Content-Centric Cognitive Robotic Architecture](https://arxiv.org/abs/2509.13279)
*Sanjay Oruganti,Sergei Nirenburg,Marjorie McShane,Jesse English,Michael K. Roberts,Christian Arndt,Carlos Gonzalez,Mingyo Seo,Luis Sentis*

Main category: cs.RO

TL;DR: 介绍用于人机团队机器人的认知机器人架构HARMONIC，展示两个基于其的机器人系统。


<details>
  <summary>Details</summary>
Motivation: 解决人机团队中机器人的数据稀缺、可解释性和安全性问题，提升结果的安全性和质量，促进透明度和信任。

Method: 设计HARMONIC架构，支持语义感知解释、类人决策和有意语言交流，并在高保真模拟环境和物理机器人平台上实现两个基于该架构的概念验证机器人系统。

Result: 成功展示两个基于HARMONIC的机器人系统。

Conclusion: HARMONIC架构在人机团队机器人应用中具有一定的可行性和价值。

Abstract: This paper introduces HARMONIC, a cognitive-robotic architecture designed for
robots in human-robotic teams. HARMONIC supports semantic perception
interpretation, human-like decision-making, and intentional language
communication. It addresses the issues of safety and quality of results; aims
to solve problems of data scarcity, explainability, and safety; and promotes
transparency and trust. Two proof-of-concept HARMONIC-based robotic systems are
demonstrated, each implemented in both a high-fidelity simulation environment
and on physical robotic platforms.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [304] [A hidden benefit of incomplete round-robin tournaments: Encouraging offensive play](https://arxiv.org/abs/2509.13141)
*László Csató*

Main category: econ.GN

TL;DR: 本文探讨赛事设计对参赛者激励的影响，以欧冠改革为例，发现新赛制对进攻有更强激励。


<details>
  <summary>Details</summary>
Motivation: 探究赛事设计对参赛者激励的影响。

Method: 开发模拟框架，根据达到关键排名阈值的概率变化量化进攻的潜在得失，并应用于2024/25 UEFA欧冠改革研究。

Result: 新的不完整循环赛联赛阶段比之前小组赛对进攻有更强激励，头奖和二等奖激励平均分别增加119%和58%。

Conclusion: 赛事形式本身能强烈影响体育团队行为。

Abstract: This paper aims to explore the impact of tournament design on the incentives
of the contestants. We develop a simulation framework to quantify the potential
gain and loss from attacking based on changes in the probability of reaching
the critical ranking thresholds. The model is applied to investigate the
2024/25 UEFA Champions League reform. The novel incomplete round-robin league
phase is found to create more powerful incentives for offensive play than the
previous group stage, with an average increase of 119\% (58\%) regarding the
first (second) prize. Our study provides the first demonstration that the
tournament format itself can strongly influence team behaviour in sports.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [305] [Neural-Quantum-States Impurity Solver for Quantum Embedding Problems](https://arxiv.org/abs/2509.12431)
*Yinzhanghao Zhou,Tsung-Han Lee,Ao Chen,Nicola Lanatà,Hong Guo*

Main category: cond-mat.str-el

TL;DR: 本文为量子嵌入方法设计并测试了基于神经量子态（NQS）的杂质求解器，在安德森晶格模型上验证了准确性，指出方法主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 神经量子态在求解二次量子化哈密顿量方面有可扩展性和灵活性，本文旨在为量子嵌入方法设计NQS杂质求解器。

Method: 设计基于图变压器的NQS框架表示任意连接的杂质轨道，开发误差控制机制稳定量子嵌入循环的迭代更新。

Result: 在安德森晶格模型的基准gGA计算中验证了方法准确性，结果与精确对角化杂质求解器高度一致。

Conclusion: 方法的主要瓶颈是嵌入循环所需的物理可观测量的高精度采样，而非NQS变分优化，强调需要更高效的推理技术。

Abstract: Neural quantum states (NQS) have emerged as a promising approach to solve
second-quantised Hamiltonians, because of their scalability and flexibility. In
this work, we design and benchmark an NQS impurity solver for the quantum
embedding methods, focusing on the ghost Gutzwiller Approximation (gGA)
framework. We introduce a graph transformer-based NQS framework able to
represent arbitrarily connected impurity orbitals and develop an error control
mechanism to stabilise iterative updates throughout the quantum embedding
loops. We validate the accuracy of our approach with benchmark gGA calculations
of the Anderson Lattice Model, yielding results in excellent agreement with the
exact diagonalisation impurity solver. Finally, our analysis of the
computational budget reveals the method's principal bottleneck to be the
high-accuracy sampling of physical observables required by the embedding loop,
rather than the NQS variational optimisation, directly highlighting the
critical need for more efficient inference techniques.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [306] [Introducing the A2AJ's Canadian Legal Data: An open-source alternative to CanLII for the era of computational law](https://arxiv.org/abs/2509.13032)
*Simon Wallace,Sean Rehaag*

Main category: cs.CY

TL;DR: 文章介绍A2AJ项目作为CanLII的开源替代方案，指出CanLII限制法律数据访问问题，介绍A2AJ项目成果及开放法律数据的作用。


<details>
  <summary>Details</summary>
Motivation: CanLII限制加拿大法律数据的批量和编程访问，阻碍法律免费获取和司法公正运动，形成数字鸿沟。

Method: 将CanLII置于历史背景中分析，介绍A2AJ的加拿大法律数据项目，通过具体例子说明开放法律数据的作用。

Result: A2AJ项目通过多种渠道提供超11.6万份法院判决和5000部法规的开放访问。

Conclusion: 开放法律数据能让法院进行循证评估，让开发者为低收入群体开发工具。

Abstract: The Access to Algorithmic Justice project (A2AJ) is an open-source
alternative to the Canadian Legal Information Institute (CanLII). At a moment
when technology promises to enable new ways of working with law, CanLII is
becoming an impediment to the free access of law and access to justice
movements because it restricts bulk and programmatic access to Canadian legal
data. This means that Canada is staring down a digital divide: well-resourced
actors have the best new technological tools and, because CanLII has disclaimed
leadership, the public only gets second-rate tools. This article puts CanLII in
its larger historical context and shows how long and deep efforts to
democratize access to Canadian legal data are, and how often they are thwarted
by private industry. We introduce the A2AJ's Canadian Legal Data project, which
provides open access to over 116,000 court decisions and 5,000 statutes through
multiple channels including APIs, machine learning datasets, and AI integration
protocols. Through concrete examples, we demonstrate how open legal data
enables courts to conduct evidence-based assessments and allows developers to
create tools for practitioners serving low-income communities.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [307] [Private Markovian Equilibrium in Stackelberg Markov Games for Smart Grid Demand Response](https://arxiv.org/abs/2509.12225)
*Siying Huang,Yifen Mu,Ge Chen*

Main category: eess.SY

TL;DR: 本文针对可再生能源并网挑战，构建Stackelberg Markov博弈，引入私有状态等概念，开发可扩展解决方案框架，模拟验证方法有效性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决可再生能源大量接入给电网供需平衡带来的挑战。

Method: 构建Stackelberg Markov博弈，引入私有状态、私有马尔可夫策略和私有马尔可夫均衡概念，开发结合集中和分散算法的可扩展解决方案框架。

Result: 证明下层马尔可夫博弈中纯PME存在且可多项式时间计算，50用户的数值模拟验证方法有效性和可扩展性。

Conclusion: 所提方法能有效应对可再生能源并网挑战，具有良好可扩展性。

Abstract: The increasing integration of renewable energy introduces a great challenge
to the supply and demand balance of the power grid. To address this challenge,
this paper formulates a Stackelberg Markov game (SMG) between an aggregator and
multiple users, where the aggregator sets electricity prices and users make
demand and storage decisions. Considering that users' storage levels are
private information, we introduce private states and propose the new concepts
of private Markovian strategies (PMS) and private Markovian equilibrium (PME).
We establish the existence of a pure PME in the lower-level Markov game and
prove that it can be computed in polynomial time. Notably, computing
equilibrium in general Markov games is hard, and polynomial-time algorithms are
rarely available. Based on these theoretical results, we develop a scalable
solution framework combining centralized and decentralized algorithms for the
lower-level PME computation with upper-level pricing optimization. Numerical
simulations with up to 50 users based on real data validate the effectiveness
and scalability of the proposed methods, whereas prior studies typically
consider no more than 5 users.

</details>


### [308] [A Cost-Optimization Model for EV Charging Stations Utilizing Solar Energy and Variable Pricing](https://arxiv.org/abs/2509.12214)
*An Nguyen,Hung Pham,Cuong Do*

Main category: eess.SY

TL;DR: 本文提出电动汽车充电站成本优化框架，结合光伏并考虑电价不确定性，评估显示有成本节省，有实时可行性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 优化电动汽车充电站成本，考虑现场光伏发电和电价不确定性。

Method: 采用Bertsimas - Sim鲁棒公式，将模型制定为线性规划，满足车辆能源需求、充电和电网容量约束，最小化采购成本。

Result: 与先到先得基线相比平均节省约12%，峰值月减少达19.2%；名义成本增加约5%可使最坏情况暴露降低14%；能在标准笔记本上5秒内解决最多50辆并发电动汽车的实例。

Conclusion: 该方法为未来电动汽车充电运营提供实用、友好电网且可扩展的解决方案。

Abstract: This paper presents a cost optimization framework for electric vehicle (EV)
charging stations that leverages on-site photovoltaic (PV) generation and
explicitly accounts for electricity price uncertainty through a Bertsimas--Sim
robust formulation. The model is formulated as a linear program that satisfies
vehicle energy demands, respects charging and grid capacity constraints, and
minimizes procurement cost. Evaluations on real charging data from the Caltech
ACN dataset show average savings of about 12\% compared to a
first-come--first-served baseline, with peak monthly reductions up to 19.2\%. A
lightweight sensitivity analysis indicates that a modest $\sim$5\% increase in
nominal cost can reduce worst-case exposure by 14\%. Computational tests
confirm real-time feasibility, with instances of up to 50 concurrent EVs solved
in under 5 seconds on a standard laptop. The proposed method provides a
practical, grid-friendly, and scalable solution for future EV charging
operations.

</details>


### [309] [Meta-model Neural Process for Probabilistic Power Flow under Varying N-1 System Topologies](https://arxiv.org/abs/2509.12281)
*Sel Ly,Kapil Chauhan,Anshuman Singh,Hung Dinh Nguyen*

Main category: eess.SY

TL;DR: 本文提出基于元模型神经过程（MMNP）的拓扑自适应方法解决不同N - 1拓扑下概率潮流（PPF）问题，经IEEE 9 - 母线和118 - 母线系统验证，填补PPF方法空白。


<details>
  <summary>Details</summary>
Motivation: 传统PPF问题考虑固定拓扑，拓扑变化需重新求解，带来不便和计算负担，随着高可再生能源和大量电动汽车接入，更多意外情况出现，需要新方法。

Method: 提出基于MMNP的拓扑自适应方法，利用基于上下文集的拓扑表示和函数学习技术上的条件分布。

Result: 在IEEE 9 - 母线系统和IEEE 118 - 母线系统上进行仿真，9 - 母线和118 - 母线的最大%L1 - 相对误差范数分别为1.11%和0.77%。

Conclusion: 该自适应方法填补了电网波动性增加时代PPF方法的关键空白。

Abstract: The probabilistic power flow (PPF) problem is essential to quantifying the
distribution of the nodal voltages due to uncertain injections. The
conventional PPF problem considers a fixed topology, and the solutions to such
a PPF problem are associated with this topology. A change in the topology might
alter the power flow patterns and thus require the PPF problem to be solved
again. The previous PPF model and its solutions are no longer valid for the new
topology. This practice incurs both inconvenience and computation burdens as
more contingencies are foreseen due to high renewables and a large share of
electric vehicles. This paper presents a novel topology-adaptive approach,
based on the meta-model Neural Process (MMNP), for finding the solutions to PPF
problems under varying N-1 topologies, particularly with one-line failures. By
leveraging context set-based topology representation and conditional
distribution over function learning techniques, the proposed MMNP enhances the
robustness of PPF models to topology variations, mitigating the need for
retraining PPF models on a new configuration. Simulations on an IEEE 9-bus
system and IEEE 118-bus system validate the model's performance. The maximum
%L1-relative error norm was observed as 1.11% and 0.77% in 9-bus and 118-bus,
respectively. This adaptive approach fills a critical gap in PPF methodology in
an era of increasing grid volatility.

</details>


### [310] [Concentration inequalities for semidefinite least squares based on data](https://arxiv.org/abs/2509.13166)
*Filippo Fabiani,Andrea Simonetto*

Main category: eess.SY

TL;DR: 研究带半定约束的数据驱动最小二乘问题，推导约束松弛时最优解谱的有限样本保证，给出高置信界，还建立了梯度下降迭代误差界。


<details>
  <summary>Details</summary>
Motivation: 解决带半定约束的数据驱动最小二乘问题，简化求解过程。

Method: 推导最优解谱的有限样本保证，给出高置信界。

Result: 得到的证书易计算、无分布依赖，且随数据量增加而收缩；建立了梯度下降迭代误差界。

Conclusion: 可通过求解更简单的程序替代完整的半定最小二乘问题，同时保证解的特征值接近半定约束要求。

Abstract: We study data-driven least squares (LS) problems with semidefinite (SD)
constraints and derive finite-sample guarantees on the spectrum of their
optimal solutions when these constraints are relaxed. In particular, we provide
a high confidence bound allowing one to solve a simpler program in place of the
full SDLS problem, while ensuring that the eigenvalues of the resulting
solution are $\varepsilon$-close of those enforced by the SD constraints. The
developed certificate, which consistently shrinks as the number of data
increases, turns out to be easy-to-compute, distribution-free, and only
requires independent and identically distributed samples. Moreover, when the
SDLS is used to learn an unknown quadratic function, we establish bounds on the
error between a gradient descent iterate minimizing the surrogate cost obtained
with no SD constraints and the true minimizer.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [311] [Wave Function Collapse Set Covering and the Hill Climbing Algorithm: A New, Fast Heuristic and Metaheuristic Pairing for the Minimum Set Cover Problem](https://arxiv.org/abs/2509.12236)
*David Oprea,David Perkins*

Main category: math.OC

TL;DR: 本文提出针对最小集合覆盖问题的启发式算法WFC - SC，结合爬山元启发式优化，经基准测试显示在最优性和时间上有更好平衡。


<details>
  <summary>Details</summary>
Motivation: 解决最小集合覆盖问题的优化问题。

Method: 提出Wave Function Collapse Set Covering (WFC - SC)算法，经过观察、传播和坍缩步骤，还将其与爬山元启发式结合优化。

Result: 使用布鲁内尔大学的OR库进行基准测试，发现算法在最优性和时间上有更好平衡。

Conclusion: WFC - SC算法结合爬山元启发式在最小集合覆盖问题上表现良好，能在最优性和时间上取得较好平衡。

Abstract: In this paper, we present a new heuristic that focuses on the optimization
problem for the Minimum Set Cover Problem. Our new heuristic involves using
Wave Function Collapse and is called Wave Function Collapse Set Covering
(WFC-SC). This algorithm goes through observation, propagation, and collapsing,
which will be explained more later on in this paper. We optimize this algorithm
to quickly find optimal coverings that are close to the minimum needed. To
further optimize this algorithm, we pair it with the Hill Climbing
metaheuristic to help WFC-SC find a better solution than its "local optimum."
We benchmark our algorithm using the well known OR library from Brunel
University against other proficient algorithms. We find that our algorithm has
a better balance between both optimality and time.

</details>


### [312] [Exact alternative optima for nonlinear optimization problems defined with maximum component objective function constrained by the Sugeno-Weber fuzzy relational inequalities](https://arxiv.org/abs/2509.12669)
*Amin Ghodousian,Sara Zal,Minoo Ahmadi*

Main category: math.OC

TL;DR: 研究含模糊关系不等式约束的格优化问题，考虑Sugeno - Weber t - 范数族，给出可行性条件及求解算法并证明可找到精确最优解。


<details>
  <summary>Details</summary>
Motivation: Sugeno - Weber t - 范数族在模糊建模问题中应用广泛，研究含其模糊复合的格优化问题。

Method: 先研究问题可行域，给出可行性的充要条件，再基于问题理论性质提出算法。

Result: 证明算法能找到精确最优解，并给出示例说明算法。

Conclusion: 所提算法可有效解决含Sugeno - Weber t - 范数族模糊复合的格优化问题。

Abstract: In this paper, we study a latticized optimization problem with fuzzy
relational inequality constraints where the feasible region is formed as the
intersection of two inequality fuzzy systems and Sugeno-Weber family of t-norms
is considered as fuzzy composition. Sugeno-Weber family of t-norms and
t-conorms is one of the most applied one in various fuzzy modelling problems.
This family of t-norms and t-conorms was suggested by Weber for modeling
intersection and union of fuzzy sets. Also, the t-conorms were suggested as
addition rules by Sugeno for so-called alpha-fuzzy measures. The resolution of
the feasible region of the problem is firstly investigated when it is defined
with max-Sugeno-Weber composition and a necessary and sufficient condition is
presented for determining the feasibility. Then, based on some theoretical
properties of the problem, an algorithm is presented for solving this nonlinear
problem. It is proved that the algorithm can find the exact optimal solution
and an example is presented to illustrate the proposed algorithm.

</details>


### [313] [Rich Vehicle Routing Problem with diverse Vertices allowing Hierarchical and Multimodal Time-Dependant Transhipment of multiple Node- Vehicle- compatible Cargo with Cascaded Time-Minimization Objective for Emergency Decision Support Systems](https://arxiv.org/abs/2509.13227)
*Santanu Banerjee,Goutam Sen,Siddhartha Mukhopadhyay*

Main category: math.OC

TL;DR: 本文考虑丰富的车辆路径问题，提出级联最小化方法，设计启发式算法求解，在新数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 优化灾害响应/准备时间，最小化车辆路线持续时间。

Method: 提出级联最小化方法，通过MILP公式展示优势；设计基于决策树的启发式算法，考虑兼容性，生成小路线元素并集成，采用多种逻辑集成和扰动方案。

Result: PSR - GIP启发式算法在新数据集上能快速为MILP无法解决的大整数实例问题给出良好解决方案。

Conclusion: 提出的级联最小化方法优于现有方法，启发式算法能有效解决实际问题。

Abstract: A rich vehicle routing problem is considered allowing multiple trips of
heterogeneous vehicles stationed at distributed vehicle depots spread across
diverse geographies having access to different modes of transportation. The
problem arises from the real world requirement of optimizing the disaster
response/preparedness time and minimizes the route duration of the vehicles to
achieve the solution with the minimum highest-vehicle-route-duration. Multiple
diversely-functional vertices are considered including the concept of
Transhipment Ports as inter-modal resource transfer stations. Both simultaneous
and split pickup and transferring of different types of delivery and pickup
cargo is considered, along with Vehicle-Cargo and Transhipment Port-Cargo
Compatibility. The superiority of the proposed cascaded minimization approach
is shown over existing makespan minimization approaches through the developed
MILP formulation. To solve the problem quickly for practical implementation
within Disaster Management-specific Decision Support Systems, an extensive
Heuristic Algorithm is devised. The Heuristic utilizes Decision Tree based
structuring of possible routes and is able to inherently consider the
compatibility issues. Preferential generation of small route elements are
performed, which are integrated into route clusters; we consider multiple
different logical integration approaches, as well as shuffling the logics to
simultaneously produce multiple independent solutions. Finally perturbation of
the different solutions are done to find better neighbouring solutions. The
computational performance of the PSR-GIP Heuristic, on our created novel
datasets, indicate that it is able to give good solutions swiftly for practical
problems involving large integer instances which the MILP is unable to solve.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [314] [Genome-Factory: An Integrated Library for Tuning, Deploying, and Interpreting Genomic Models](https://arxiv.org/abs/2509.12266)
*Weimin Wu,Xuefeng Song,Yibo Wen,Qinjie Lin,Zhihan Zhou,Jerry Yao-Chieh Hu,Zhong Wang,Han Liu*

Main category: q-bio.GN

TL;DR: 介绍Genome - Factory库，可简化统一基因组模型开发工作流，有多种功能和接口，经多维度验证有实用价值。


<details>
  <summary>Details</summary>
Motivation: 简化和统一基因组模型开发的工作流，提高其可访问性。

Method: 提供自动化数据收集和预处理流程，支持三种模型调优方法，具备推理、基准测试功能，引入基于稀疏自动编码器的生物解释器，有命令行和网页两种接口。

Result: Genome - Factory经兼容性、基准测试和生物解释三个维度验证，展现出端到端可用性和实用价值。

Conclusion: Genome - Factory对实际基因组分析有实用价值。

Abstract: We introduce Genome-Factory, an integrated Python library for tuning,
deploying, and interpreting genomic models. Our core contribution is to
simplify and unify the workflow for genomic model development: data collection,
model tuning, inference, benchmarking, and interpretability. For data
collection, Genome-Factory offers an automated pipeline to download genomic
sequences and preprocess them. It also includes quality control, such as GC
content normalization. For model tuning, Genome-Factory supports three
approaches: full-parameter, low-rank adaptation, and adapter-based fine-tuning.
It is compatible with a wide range of genomic models. For inference,
Genome-Factory enables both embedding extraction and DNA sequence generation.
For benchmarking, we include two existing benchmarks and provide a flexible
interface for users to incorporate additional benchmarks. For interpretability,
Genome-Factory introduces the first open-source biological interpreter based on
a sparse auto-encoder. This module disentangles embeddings into sparse,
near-monosemantic latent units and links them to interpretable genomic features
by regressing on external readouts. To improve accessibility, Genome-Factory
features both a zero-code command-line interface and a user-friendly web
interface. We validate the utility of Genome-Factory across three dimensions:
(i) Compatibility with diverse models and fine-tuning methods; (ii)
Benchmarking downstream performance using two open-source benchmarks; (iii)
Biological interpretation of learned representations with DNABERT-2. These
results highlight its end-to-end usability and practical value for real-world
genomic analysis.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [315] [Digital Voices of Survival: From Social Media Disclosures to Support Provisions for Domestic Violence Victims](https://arxiv.org/abs/2509.12288)
*Kanlun Wang,Zhe Fu,Wangjiaxuan Xin,Lina Zhou,Shashi Kiran Chandrappa*

Main category: cs.SI

TL;DR: 本文提出计算框架研究家暴求助行为与社区支持机制，用社交媒体数据评估，成果推动相关知识发展并利于数字干预。


<details>
  <summary>Details</summary>
Motivation: 社交媒体使家暴受害者披露经历增加，但在线自我披露作为求助途径未充分研究，且现有研究缺乏对家暴自我披露、支持提供及其联系的全面理解。

Method: 提出由自我披露检测、帖子聚类、主题总结、支持提取和映射四个关键部分组成的计算框架，用社交媒体数据实施并评估。

Result: 研究成果推动了家暴自我披露和在线支持提供的现有知识发展。

Conclusion: 该框架有利于以受害者为中心的数字干预。

Abstract: Domestic Violence (DV) is a pervasive public health problem characterized by
patterns of coercive and abusive behavior within intimate relationships. With
the rise of social media as a key outlet for DV victims to disclose their
experiences, online self-disclosure has emerged as a critical yet underexplored
avenue for support-seeking. In addition, existing research lacks a
comprehensive and nuanced understanding of DV self-disclosure, support
provisions, and their connections. To address these gaps, this study proposes a
novel computational framework for modeling DV support-seeking behavior
alongside community support mechanisms. The framework consists of four key
components: self-disclosure detection, post clustering, topic summarization,
and support extraction and mapping. We implement and evaluate the framework
with data collected from relevant social media communities. Our findings not
only advance existing knowledge on DV self-disclosure and online support
provisions but also enable victim-centered digital interventions.

</details>


### [316] [Structured Information Loss in Network Embeddings](https://arxiv.org/abs/2509.12396)
*Gabriel Chuang,Augustin Chaintreau*

Main category: cs.SI

TL;DR: 分析网络嵌入算法，刻画表示图生成模型的条件，研究信息损失情况及等价类，探讨对社区检测和链接预测的影响。


<details>
  <summary>Details</summary>
Motivation: 研究简单网络嵌入算法所学习的表示对图生成模型的编码情况及对社区检测和链接预测的影响。

Method: 分析简单网络嵌入算法，刻画不同编码情况的条件，研究信息损失时图论等价类。

Result: 发现信息损失时等价类保留社区结构但丢失密度信息，表明仅基于嵌入的链接预测有效性有局限，朴素链接预测会加重或减轻结构偏差。

Conclusion: 仅基于嵌入的链接预测有强局限性，朴素链接预测在常见条件下会影响结构偏差。

Abstract: We analyze a simple algorithm for network embedding, explicitly
characterizing conditions under which the learned representation encodes the
graph's generative model fully, partially, or not at all. In cases where the
embedding loses some information (i.e., is not invertible), we describe the
equivalence classes of graphons that map to the same embedding, finding that
these classes preserve community structure but lose substantial density
information. Finally, we show implications for community detection and link
prediction. Our results suggest strong limitations on the effectiveness of link
prediction based on embeddings alone, and we show common conditions under which
naive link prediction adds edges in a disproportionate manner that can either
mitigate or exacerbate structural biases.

</details>


### [317] [A Pressure-Based Diffusion Model for Influence Maximization on Social Networks](https://arxiv.org/abs/2509.12822)
*Curt Stutsman,Eliot W. Robson,Abhishek K. Umrawal*

Main category: cs.SI

TL;DR: 提出压力阈值模型（PT）模拟社交网络影响力传播，研究影响力最大化问题，实验显示PT模型种子节点选择与LT模型不同，且密集网络压力效应更显著。


<details>
  <summary>Details</summary>
Motivation: 考虑个体局部社交网络对观点形成和传播的影响，解决影响力最大化问题。

Method: 提出PT模型扩展LT模型，调整节点输出影响力；借助CyNetDiff库在真实网络上实验。

Result: PT模型有独特的种子节点选择；密集网络压力效应比稀疏网络更显著。

Conclusion: PT模型可用于动态模拟社交网络影响力传播，不同网络结构对压力效应有不同影响。

Abstract: In many real-world scenarios, an individual's local social network carries
significant influence over the opinions they form and subsequently propagate to
others. In this paper, we propose a novel diffusion model -- the Pressure
Threshold model (PT) -- for dynamically simulating the spread of influence
through a social network. This new model extends the popular Linear Threshold
Model (LT) by adjusting a node's outgoing influence proportional to the
influence it receives from its activated neighbors. We address the Influence
Maximization (IM) problem, which involves selecting the most effective seed
nodes to achieve maximal graph coverage after a diffusion process, and how the
problem manifests with the PT Model. Experiments conducted on real-world
networks, facilitated by enhancements to the open-source network-diffusion
Python library, CyNetDiff, demonstrate unique seed node selection for the PT
Model when compared to the LT Model. Moreover, analyses demonstrate that
densely connected networks amplify pressure effects more significantly than
sparse networks.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [318] [Reduced Order Modeling of Energetic Materials Using Physics-Aware Recurrent Convolutional Neural Networks in a Latent Space (LatentPARC)](https://arxiv.org/abs/2509.12401)
*Zoë J. Gray,Joseph B. Choi,Youngsoo Choi,H. Keo Springer,H. S. Udaykumar,Stephen S. Baek*

Main category: cond-mat.mtrl-sci

TL;DR: 通过将物理感知深度学习（PADL）的复杂场演化学习任务解耦，结合并改进物理感知循环卷积（PARC）方法，投影到低维潜空间，加速模型并降低计算成本，实现快速预测。


<details>
  <summary>Details</summary>
Motivation: 解决PADL方法在学习复杂场演化问题时面临的挑战，实现对含能材料（EM）热力学的快速预测和全应用规模的结构 - 属性 - 性能关联表征。

Method: 将学习任务解耦为学习复杂几何特征和在低维特征空间建模动力学；基于PARC方法，将原始动力学投影到低维不变流形（潜空间）。

Result: 训练和推理时间显著减少，推理结果与PARC相当。

Conclusion: 该工作朝着实现更大尺度上EM热力学的快速预测和全应用规模的结构 - 属性 - 性能关联表征迈出了一步。

Abstract: Physics-aware deep learning (PADL) has gained popularity for use in complex
spatiotemporal dynamics (field evolution) simulations, such as those that arise
frequently in computational modeling of energetic materials (EM). Here, we show
that the challenge PADL methods face while learning complex field evolution
problems can be simplified and accelerated by decoupling it into two tasks:
learning complex geometric features in evolving fields and modeling dynamics
over these features in a lower dimensional feature space. To accomplish this,
we build upon our previous work on physics-aware recurrent convolutions (PARC).
PARC embeds knowledge of underlying physics into its neural network
architecture for more robust and accurate prediction of evolving physical
fields. PARC was shown to effectively learn complex nonlinear features such as
the formation of hotspots and coupled shock fronts in various initiation
scenarios of EMs, as a function of microstructures, serving effectively as a
microstructure-aware burn model. In this work, we further accelerate PARC and
reduce its computational cost by projecting the original dynamics onto a
lower-dimensional invariant manifold, or 'latent space.' The projected latent
representation encodes the complex geometry of evolving fields (e.g.
temperature and pressure) in a set of data-driven features. The reduced
dimension of this latent space allows us to learn the dynamics during the
initiation of EM with a lighter and more efficient model. We observe a
significant decrease in training and inference time while maintaining results
comparable to PARC at inference. This work takes steps towards enabling rapid
prediction of EM thermomechanics at larger scales and characterization of EM
structure-property-performance linkages at a full application scale.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [319] [Joint AoI and Handover Optimization in Space-Air-Ground Integrated Network](https://arxiv.org/abs/2509.12716)
*Zifan Lang,Guixia Liu,Geng Sun,Jiahui Li,Jiacheng Wang,Weijie Yuan,Dusit Niyato,Dong In Kim*

Main category: cs.NI

TL;DR: 本文提出AoI感知的SAGIN架构，用HAP作中继，结合FSO和RF链路，提出DD3QN - AS算法优化，仿真显示性能优越。


<details>
  <summary>Details</summary>
Motivation: 地面网络在偏远地区通信和应急连通方面有挑战，LEO卫星星座有间歇性覆盖问题，需解决。

Method: 引入HAP作为LEO卫星和地面终端的智能中继，采用三层设计结合FSO和RF链路；提出联合优化问题，用DD3QN - AS算法解决。

Result: 仿真结果表明，所提方法比基于策略的方法和其他DRL基准表现更优。

Conclusion: 所提出的架构和算法能有效解决LEO卫星覆盖的时间不连续性问题，性能优越。

Abstract: Despite the widespread deployment of terrestrial networks, providing reliable
communication services to remote areas and maintaining connectivity during
emergencies remains challenging. Low Earth orbit (LEO) satellite constellations
offer promising solutions with their global coverage capabilities and reduced
latency, yet struggle with intermittent coverage and limited communication
windows due to orbital dynamics. This paper introduces an age of information
(AoI)-aware space-air-ground integrated network (SAGIN) architecture that
leverages a high-altitude platform (HAP) as intelligent relay between the LEO
satellites and ground terminals. Our three-layer design employs hybrid
free-space optical (FSO) links for high-capacity satellite-to-HAP communication
and reliable radio frequency (RF) links for HAP-to-ground transmission, and
thus addressing the temporal discontinuity in LEO satellite coverage while
serving diverse user priorities. Specifically, we formulate a joint
optimization problem to simultaneously minimize the AoI and satellite handover
frequency through optimal transmit power distribution and satellite selection
decisions. This highly dynamic, non-convex problem with time-coupled
constraints presents significant computational challenges for traditional
approaches. To address these difficulties, we propose a novel diffusion model
(DM)-enhanced dueling double deep Q-network with action decomposition and state
transformer encoder (DD3QN-AS) algorithm that incorporates transformer-based
temporal feature extraction and employs a DM-based latent prompt generative
module to refine state-action representations through conditional denoising.
Simulation results highlight the superior performance of the proposed approach
compared with policy-based methods and some other deep reinforcement learning
(DRL) benchmarks.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [320] [A Computational Pipeline for Patient-Specific Modeling of Thoracic Aortic Aneurysm: From Medical Image to Finite Element Analysis](https://arxiv.org/abs/2509.12596)
*Jiasong Chen,Linchen Qian,Ruonan Gong,Christina Sun,Tongran Qin,Thuy Pham,Caitlin Martin,Mohammad Zafar,John Elefteriades,Wei Sun,Liang Liang*

Main category: eess.IV

TL;DR: 本文介绍胸主动脉瘤（TAA）危害，提及3D CT是诊断金标准，阐述基于深度学习的图像分割、六面体网格等在TAA有限元模型构建中的应用及患者特异性建模的重要性。


<details>
  <summary>Details</summary>
Motivation: 胸主动脉瘤是临床上重要疾病且是成人主要死因之一，需要建立患者特异性、基于生物力学的框架来预测TAA风险。

Method: 利用深度学习进行医学图像分割，将体素分割掩码转换为结构化网格表示，采用六面体网格进行有限元模拟，开展患者特异性建模。

Result: 未提及具体结果。

Conclusion: 开发准确的有限元模型是建立患者特异性、基于生物力学的TAA风险预测框架的关键初始步骤。

Abstract: The aorta is the body's largest arterial vessel, serving as the primary
pathway for oxygenated blood within the systemic circulation. Aortic aneurysms
consistently rank among the top twenty causes of mortality in the United
States. Thoracic aortic aneurysm (TAA) arises from abnormal dilation of the
thoracic aorta and remains a clinically significant disease, ranking as one of
the leading causes of death in adults. A thoracic aortic aneurysm ruptures when
the integrity of all aortic wall layers is compromised due to elevated blood
pressure. Currently, three-dimensional computed tomography (3D CT) is
considered the gold standard for diagnosing TAA. The geometric characteristics
of the aorta, which can be quantified from medical imaging, and stresses on the
aortic wall, which can be obtained by finite element analysis (FEA), are
critical in evaluating the risk of rupture and dissection. Deep learning based
image segmentation has emerged as a reliable method for extracting anatomical
regions of interest from medical images. Voxel based segmentation masks of
anatomical structures are typically converted into structured mesh
representation to enable accurate simulation. Hexahedral meshes are commonly
used in finite element simulations of the aorta due to their computational
efficiency and superior simulation accuracy. Due to anatomical variability,
patient specific modeling enables detailed assessment of individual anatomical
and biomechanics behaviors, supporting precise simulations, accurate diagnoses,
and personalized treatment strategies. Finite element (FE) simulations provide
valuable insights into the biomechanical behaviors of tissues and organs in
clinical studies. Developing accurate FE models represents a crucial initial
step in establishing a patient-specific, biomechanically based framework for
predicting the risk of TAA.

</details>


### [321] [Physics-Informed Neural Networks vs. Physics Models for Non-Invasive Glucose Monitoring: A Comparative Study Under Realistic Synthetic Conditions](https://arxiv.org/abs/2509.12253)
*Riyaadh Gani*

Main category: eess.IV

TL;DR: 本文提出超现实近红外模拟器，用其对六种方法进行基准测试，Beer - Lambert法表现最佳，结果颠覆了深层PINNs占优的假设并提供嵌入式光学葡萄糖传感器原型参考。


<details>
  <summary>Details</summary>
Motivation: 现有数据集忽略硬件噪声、环境漂移和个体生理差异，导致无创葡萄糖监测仪在实验室外常失效，需要更真实的模拟平台。

Method: 引入超现实近红外模拟器，注入多种影响因素，用该平台对六种方法（Enhanced Beer - Lambert、三种物理信息神经网络、选择性辐射传输PINN和浅层DNN）进行基准测试。

Result: Beer - Lambert法RMSE为13.6 mg/dL，Clarke - A准确率95.8%，±15%准确率93.8%，优于最佳PINN（14.6 mg/dL）和SDNN基线（35.1 mg/dL）。

Conclusion: 结果颠覆了深层PINNs占优的假设，为嵌入式光学葡萄糖传感器的快速原型设计提供了开放的端到端参考框架。

Abstract: Non-invasive glucose monitors often fail outside the lab because existing
datasets ignore hardware noise, environmental drift, and person-to-person
physiology. We introduce the first ultra-realistic near-infrared (NIR)
simulator that injects 12-bit ADC quantisation, +/-0.1% LED ageing, photodiode
dark noise, 15-45 C temperature, 30-90% relative humidity, contact-pressure
variation, Fitzpatrick I-VI melanin, and diurnal glucose excursions (dawn
phenomenon). Using this platform (rho glucose-NIR = 0.21), we benchmark six
methods: Enhanced Beer-Lambert (physics-engineered ridge regression), three
physics-informed neural networks (PINNs), a selective radiative-transfer PINN,
and a shallow DNN. Beer-Lambert achieves 13.6 mg/dL RMSE, 95.8% Clarke-A and
93.8% +/-15% accuracy with only 56 parameters and 0.01 ms inference,
outperforming the best PINN (14.6 mg/dL) and the SDNN baseline (35.1 mg/dL).
Results overturn the assumption that deeper PINNs dominate and supply an open,
end-to-end reference stack for rapid prototyping of embedded optical glucose
sensors.

</details>


### [322] [DinoAtten3D: Slice-Level Attention Aggregation of DinoV2 for 3D Brain MRI Anomaly Classification](https://arxiv.org/abs/2509.12512)
*Fazle Rafsani,Jay Shah,Catherine D. Chong,Todd J. Schwedt,Teresa Wu*

Main category: eess.IV

TL;DR: 提出基于注意力的全局聚合框架用于3D医学图像异常分类，结合预训练模型和复合损失函数，在有限数据和类别不平衡下表现良好。


<details>
  <summary>Details</summary>
Motivation: 医学影像异常检测和分类面临标注数据有限、类别不平衡和专家标注成本高的挑战，预训练视觉基础模型有潜力解决这些问题。

Method: 提出基于注意力的全局聚合框架，使用自监督DINOv2模型提取特征，通过软注意力机制分配切片权重，采用结合监督对比学习和类方差正则化的复合损失函数。

Result: 在ADNI数据集和机构多类头痛队列上验证框架，在有限数据和类别不平衡情况下展现出强异常分类性能。

Conclusion: 利用预训练2D基础模型结合基于注意力的切片聚合对医学影像中稳健的体积异常检测有效。

Abstract: Anomaly detection and classification in medical imaging are critical for
early diagnosis but remain challenging due to limited annotated data, class
imbalance, and the high cost of expert labeling. Emerging vision foundation
models such as DINOv2, pretrained on extensive, unlabeled datasets, offer
generalized representations that can potentially alleviate these limitations.
In this study, we propose an attention-based global aggregation framework
tailored specifically for 3D medical image anomaly classification. Leveraging
the self-supervised DINOv2 model as a pretrained feature extractor, our method
processes individual 2D axial slices of brain MRIs, assigning adaptive
slice-level importance weights through a soft attention mechanism. To further
address data scarcity, we employ a composite loss function combining supervised
contrastive learning with class-variance regularization, enhancing inter-class
separability and intra-class consistency. We validate our framework on the ADNI
dataset and an institutional multi-class headache cohort, demonstrating strong
anomaly classification performance despite limited data availability and
significant class imbalance. Our results highlight the efficacy of utilizing
pretrained 2D foundation models combined with attention-based slice aggregation
for robust volumetric anomaly detection in medical imaging. Our implementation
is publicly available at https://github.com/Rafsani/DinoAtten3D.git.

</details>


### [323] [DeepEyeNet: Generating Medical Report for Retinal Images](https://arxiv.org/abs/2509.12534)
*Jia-Hong Huang*

Main category: eess.IV

TL;DR: 论文探讨AI自动化生成视网膜图像医疗报告的潜力，提出方法并评估，证明其能提升临床效率和诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 视网膜疾病增多，眼科医生人力不足，传统人工解读图像生成报告耗时易错，需新方法提升效率。

Method: 采用多模态深度学习方法、改进医学关键词表示、克服RNN模型局限、增强系统可解释性。

Result: 所提方法经多种指标评估，达到了当前最优性能。

Conclusion: AI自动化生成医疗报告可革新视网膜疾病诊断，提高临床效率、诊断准确性和患者护理质量。

Abstract: The increasing prevalence of retinal diseases poses a significant challenge
to the healthcare system, as the demand for ophthalmologists surpasses the
available workforce. This imbalance creates a bottleneck in diagnosis and
treatment, potentially delaying critical care. Traditional methods of
generating medical reports from retinal images rely on manual interpretation,
which is time-consuming and prone to errors, further straining
ophthalmologists' limited resources. This thesis investigates the potential of
Artificial Intelligence (AI) to automate medical report generation for retinal
images. AI can quickly analyze large volumes of image data, identifying subtle
patterns essential for accurate diagnosis. By automating this process, AI
systems can greatly enhance the efficiency of retinal disease diagnosis,
reducing doctors' workloads and enabling them to focus on more complex cases.
The proposed AI-based methods address key challenges in automated report
generation: (1) A multi-modal deep learning approach captures interactions
between textual keywords and retinal images, resulting in more comprehensive
medical reports; (2) Improved methods for medical keyword representation
enhance the system's ability to capture nuances in medical terminology; (3)
Strategies to overcome RNN-based models' limitations, particularly in capturing
long-range dependencies within medical descriptions; (4) Techniques to enhance
the interpretability of the AI-based report generation system, fostering trust
and acceptance in clinical practice. These methods are rigorously evaluated
using various metrics and achieve state-of-the-art performance. This thesis
demonstrates AI's potential to revolutionize retinal disease diagnosis by
automating medical report generation, ultimately improving clinical efficiency,
diagnostic accuracy, and patient care.

</details>


### [324] [Enhancing Radiographic Disease Detection with MetaCheX, a Context-Aware Multimodal Model](https://arxiv.org/abs/2509.12287)
*Nathan He,Cody Chen*

Main category: eess.IV

TL;DR: 提出MetaCheX框架集成胸部X光图像和患者元数据，在数据集上表现优于仅用X光的基线模型，提升诊断准确性、减少偏差和增强泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有胸部放射学深度学习模型常忽略患者元数据，限制诊断准确性和公平性。

Method: 引入MetaCheX框架，结合CNN骨干网络与多层感知机处理的元数据，通过共享分类器整合。

Result: 在CheXpert Plus数据集上，MetaCheX在多种CNN架构中始终优于仅用X光的基线模型，整体诊断准确性（AUROC）显著提高。

Conclusion: 元数据可减少算法偏差，增强模型在不同患者群体中的泛化性，MetaCheX推动临床人工智能实现鲁棒、上下文感知的放射疾病检测。

Abstract: Existing deep learning models for chest radiology often neglect patient
metadata, limiting diagnostic accuracy and fairness. To bridge this gap, we
introduce MetaCheX, a novel multimodal framework that integrates chest X-ray
images with structured patient metadata to replicate clinical decision-making.
Our approach combines a convolutional neural network (CNN) backbone with
metadata processed by a multilayer perceptron through a shared classifier.
Evaluated on the CheXpert Plus dataset, MetaCheX consistently outperformed
radiograph-only baseline models across multiple CNN architectures. By
integrating metadata, the overall diagnostic accuracy was significantly
improved, measured by an increase in AUROC. The results of this study
demonstrate that metadata reduces algorithmic bias and enhances model
generalizability across diverse patient populations. MetaCheX advances clinical
artificial intelligence toward robust, context-aware radiographic disease
detection.

</details>


### [325] [MEGAN: Mixture of Experts for Robust Uncertainty Estimation in Endoscopy Videos](https://arxiv.org/abs/2509.12772)
*Damola Agbelese,Krishna Chaitanya,Pushpak Pati,Chaitanya Parmar,Pooya Mobadersany,Shreyas Fadnavis,Lindsey Surace,Shadi Yarandi,Louis R. Ghanem,Molly Lucas,Tommaso Mansi,Oana Gabriela Cula,Pablo F. Damasceno,Kristopher Standish*

Main category: eess.IV

TL;DR: 提出MEGAN网络解决医疗AI中模型训练依赖单一专家标注问题，在UC疾病严重程度估计任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统医疗AI不确定性量化方法训练依赖单一专家标注，忽略医疗领域评分者间的差异。

Method: 提出MEGAN，通过EDL模型聚合多个AI专家的不确定性估计和预测，其门控网络优化组合各EDL模型的结果。

Result: 在大规模前瞻性UC临床试验中，F1分数提升3.5%，预期校准误差降低30.5%，还能进行不确定性引导的样本分层。

Conclusion: MEGAN能增强整体预测置信度和校准度，减少标注负担，提高UC试验效率和一致性。

Abstract: Reliable uncertainty quantification (UQ) is essential in medical AI.
Evidential Deep Learning (EDL) offers a computationally efficient way to
quantify model uncertainty alongside predictions, unlike traditional methods
such as Monte Carlo (MC) Dropout and Deep Ensembles (DE). However, all these
methods often rely on a single expert's annotations as ground truth for model
training, overlooking the inter-rater variability in healthcare. To address
this issue, we propose MEGAN, a Multi-Expert Gating Network that aggregates
uncertainty estimates and predictions from multiple AI experts via EDL models
trained with diverse ground truths and modeling strategies. MEGAN's gating
network optimally combines predictions and uncertainties from each EDL model,
enhancing overall prediction confidence and calibration. We extensively
benchmark MEGAN on endoscopy videos for Ulcerative colitis (UC) disease
severity estimation, assessed by visual labeling of Mayo Endoscopic Subscore
(MES), where inter-rater variability is prevalent. In large-scale prospective
UC clinical trial, MEGAN achieved a 3.5% improvement in F1-score and a 30.5%
reduction in Expected Calibration Error (ECE) compared to existing methods.
Furthermore, MEGAN facilitated uncertainty-guided sample stratification,
reducing the annotation burden and potentially increasing efficiency and
consistency in UC trials.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [326] [VADER: A Variational Autoencoder to Infer Planetary Masses and Gas-Dust Disk Properties Around Young Stars](https://arxiv.org/abs/2509.12324)
*Sayed Shafaat Mahmud,Sayantan Auddy,Neal Turner,Jeffrey S. Bary*

Main category: astro-ph.EP

TL;DR: 介绍VADER模型用于从原行星盘图像推断行星质量和盘属性，模型预测效果好，可用于天体物理推断。


<details>
  <summary>Details</summary>
Motivation: 从高分辨率ALMA尘埃连续谱图像中推断行星质量和全局盘属性。

Method: 使用概率深度学习模型VADER，在超10万个由FARGO3D模拟并经RADMC3D后处理生成的原行星盘合成图像上训练。

Result: 训练模型预测物理参数R²>0.9，应用于23个真实盘，质量估计与文献值一致，揭示潜在相关性。

Conclusion: 基于VAE的生成模型是概率天体物理推断的可靠工具，可用于解释原行星盘子结构。

Abstract: We present \textbf{VADER} (Variational Autoencoder for Disks Embedded with
Rings), for inferring both planet mass and global disk properties from
high-resolution ALMA dust continuum images of protoplanetary disks (PPDs).
VADER, a probabilistic deep learning model, enables uncertainty-aware inference
of planet masses, $\alpha$-viscosity, dust-to-gas ratio, Stokes number, flaring
index, and the number of planets directly from protoplanetary disk images.
VADER is trained on over 100{,}000 synthetic images of PPDs generated from
\texttt{FARGO3D} simulations post-processed with \texttt{RADMC3D}. Our trained
model predicts physical planet and disk parameters with $R^2 > 0.9$ from dust
continuum images of PPDs. Applied to 23 real disks, VADER's mass estimates are
consistent with literature values and reveal latent correlations that reflect
known disk physics. Our results establish VAE-based generative models as robust
tools for probabilistic astrophysical inference, with direct applications to
interpreting protoplanetary disk substructures in the era of large
interferometric surveys.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [327] [Evolution of Programmers' Trust in Generative AI Programming Assistants](https://arxiv.org/abs/2509.13253)
*Anshul Shah,Thomas Rexin,Elena Tomson,Leo Porter,William G. Griswold,Adalbert Gerald Soosai Raj*

Main category: cs.HC

TL;DR: 研究调查学生使用GitHub Copilot后信任度的变化，发现平均信任度上升，并给出教学建议。


<details>
  <summary>Details</summary>
Motivation: 信任程度影响程序员使用生成式AI编程助手，过度或不足信任都有问题，需了解信任的动态变化。

Method: 对71名计算机专业高年级学生进行调查，定量测量信任水平，定性探究信任变化原因。

Result: 学生平均信任度随时间上升，完成项目后学生认为需程序员手动完成部分任务，看到正确性、理解代码上下文利用和学习自然语言处理基础提升了信任。

Conclusion: 为教师和行业经理理解学生校准对AI助手信任的因素提供帮助，并给出四条教学建议。

Abstract: Motivation. Trust in generative AI programming assistants is a vital attitude
that impacts how programmers use those programming assistants. Programmers that
are over-trusting may be too reliant on their tools, leading to incorrect or
vulnerable code; programmers that are under-trusting may avoid using tools that
can improve their productivity and well-being.
  Methods. Since trust is a dynamic attitude that may change over time, this
study aims to understand programmers' evolution of trust after immediate (one
hour) and extended (10 days) use of GitHub Copilot. We collected survey data
from 71 upper-division computer science students working on a legacy code base,
representing a population that is about to enter the workforce. In this study,
we quantitatively measure student trust levels and qualitatively uncover why
student trust changes.
  Findings. Student trust, on average, increased over time. After completing a
project with Copilot, however, students felt that Copilot requires a competent
programmer to complete some tasks manually. Students mentioned that seeing
Copilot's correctness, understanding how Copilot uses context from the code
base, and learning some basics of natural language processing contributed to
their elevated trust.
  Implications. Our study helps instructors and industry managers understand
the factors that influence how students calibrate their trust with AI
assistants. We make four pedagogical recommendations, which are that CS
educators should 1) provide opportunities for students to work with Copilot on
challenging software engineering tasks to calibrate their trust, 2) teach
traditional skills of comprehending, debugging, and testing so students can
verify output, 3) teach students about the basics of natural language
processing, and 4) explicitly introduce and demonstrate the range of features
available in Copilot.

</details>


### [328] [DoubleAgents: Exploring Mechanisms of Building Trust with Proactive AI](https://arxiv.org/abs/2509.12626)
*Tao Long,Xuanming Zhang,Sitong Wang,Zhou Yu,Lydia B Chilton*

Main category: cs.HC

TL;DR: 介绍代理式规划工具DoubleAgents，通过实验室研究和部署评估，展示其能增强用户信任及在现实中的实用性，并贡献信任设计模式和机制。


<details>
  <summary>Details</summary>
Motivation: 代理式工作流效率高，但采用取决于人们是否信任代理系统，旨在解决用户信任问题。

Method: 开发DoubleAgents工具，嵌入透明度和控制权相关机制，内置响应者模拟；进行两天实验室研究（n=10）、两次部署（n=2）和技术评估。

Result: 参与者起初犹豫委托，随体验透明度、控制权和自适应学习，变得更依赖；部署结果显示其与现实相关且有用，所需努力与任务复杂度和上下文数据相匹配。

Conclusion: 为主动式AI贡献信任设计模式和机制，模拟是建立和校准信任的安全途径。

Abstract: Agentic workflows promise efficiency, but adoption hinges on whether people
actually trust systems that act on their behalf. We present DoubleAgents, an
agentic planning tool that embeds transparency and control through user
intervention, value-reflecting policies, rich state visualizations, and
uncertainty flagging for human coordination tasks. A built-in respondent
simulation generates realistic scenarios, allowing users to rehearse, refine
policies, and calibrate their reliance before live use. We evaluate
DoubleAgents in a two-day lab study (n=10), two deployments (n=2), and a
technical evaluation. Results show that participants initially hesitated to
delegate but grew more reliant as they experienced transparency, control, and
adaptive learning during simulated cases. Deployment results demonstrate
DoubleAgents' real-world relevance and usefulness, showing that the effort
required scaled appropriately with task complexity and contextual data. We
contribute trust-by-design patterns and mechanisms for proactive AI --
consistency, controllability, and explainability -- along with simulation as a
safe path to build and calibrate trust over time.

</details>


### [329] [Gesture Evaluation in Virtual Reality](https://arxiv.org/abs/2509.12816)
*Axel Wiebe Werner,Jonas Beskow,Anna Deichler*

Main category: cs.HC

TL;DR: 本文对比评估VR和2D中计算机生成手势，发现VR中手势评分略高，VR影响整体感知且有独特优势。


<details>
  <summary>Details</summary>
Motivation: 现有对AI生成手势的评估大多局限于2D，而VR可能影响手势感知，因此进行对比评估。

Method: 对2023 GENEA挑战赛的三个模型在VR和2D中生成的手势进行对比评估。

Result: VR中观看的手势平均评分略高，对动作捕捉的“真实运动”效果最强，模型排名在不同环境下一致。

Conclusion: VR影响参与者的整体感知，相比传统2D评估有独特好处。

Abstract: Gestures are central to human communication, enriching interactions through
non-verbal expression. Virtual avatars increasingly use AI-generated gestures
to enhance life-likeness, yet evaluations have largely been confined to 2D.
Virtual Reality (VR) provides an immersive alternative that may affect how
gestures are perceived. This paper presents a comparative evaluation of
computer-generated gestures in VR and 2D, examining three models from the 2023
GENEA Challenge. Results show that gestures viewed in VR were rated slightly
higher on average, with the strongest effect observed for motion-capture "true
movement." While model rankings remained consistent across settings, VR
influenced participants' overall perception and offered unique benefits over
traditional 2D evaluation.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [330] [Sustainable LSTM-Based Precoding for RIS-Aided mmWave MIMO Systems with Implicit CSI](https://arxiv.org/abs/2509.12658)
*Po-Heng Chou,Jiun-Jia Wu,Wan-Jen Huang,Ronald Y. Chang*

Main category: eess.SP

TL;DR: 提出基于LSTM的预编码框架用于RIS辅助毫米波MIMO系统，减少开销和复杂度，能耗低且性能好。


<details>
  <summary>Details</summary>
Motivation: 解决RIS辅助毫米波MIMO系统中显式信道状态信息估计开销大、复杂度高的问题，设计可持续的预编码框架。

Method: 利用上行导频序列隐式学习信道特征，考虑RIS元件相位相关幅度模型，采用多标签训练策略。

Result: 设计方案达到穷举搜索90%以上的频谱效率，仅用2.2%的计算时间，能耗降低近两个数量级，在分布失配时有韧性且可扩展。

Conclusion: 该方法是6G无线网络实用且节能的解决方案。

Abstract: In this paper, we propose a sustainable long short-term memory (LSTM)-based
precoding framework for reconfigurable intelligent surface (RIS)-assisted
millimeter-wave (mmWave) MIMO systems. Instead of explicit channel state
information (CSI) estimation, the framework exploits uplink pilot sequences to
implicitly learn channel characteristics, reducing both pilot overhead and
inference complexity. Practical hardware constraints are addressed by
incorporating the phase-dependent amplitude model of RIS elements, while a
multi-label training strategy improves robustness when multiple near-optimal
codewords yield comparable performance. Simulations show that the proposed
design achieves over 90% of the spectral efficiency of exhaustive search (ES)
with only 2.2% of its computation time, cutting energy consumption by nearly
two orders of magnitude. The method also demonstrates resilience under
distribution mismatch and scalability to larger RIS arrays, making it a
practical and energy-efficient solution for sustainable 6G wireless networks.

</details>
