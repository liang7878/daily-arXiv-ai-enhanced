<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 36]
- [cs.CE](#cs.CE) [Total: 3]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.DS](#cs.DS) [Total: 14]
- [cs.GT](#cs.GT) [Total: 6]
- [cs.IR](#cs.IR) [Total: 6]
- [cs.LG](#cs.LG) [Total: 83]
- [cs.NE](#cs.NE) [Total: 6]
- [cs.SE](#cs.SE) [Total: 20]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [stat.ML](#stat.ML) [Total: 8]
- [stat.CO](#stat.CO) [Total: 1]
- [q-bio.OT](#q-bio.OT) [Total: 1]
- [eess.SY](#eess.SY) [Total: 3]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.CL](#cs.CL) [Total: 22]
- [cs.SI](#cs.SI) [Total: 3]
- [cs.CG](#cs.CG) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [econ.GN](#econ.GN) [Total: 4]
- [cs.CY](#cs.CY) [Total: 5]
- [cs.MA](#cs.MA) [Total: 1]
- [math-ph](#math-ph) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]
- [physics.app-ph](#physics.app-ph) [Total: 1]
- [gr-qc](#gr-qc) [Total: 1]
- [math.OC](#math.OC) [Total: 3]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.CR](#cs.CR) [Total: 11]
- [cs.CV](#cs.CV) [Total: 25]
- [cs.DM](#cs.DM) [Total: 1]
- [eess.IV](#eess.IV) [Total: 4]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.NI](#cs.NI) [Total: 3]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.AR](#cs.AR) [Total: 3]
- [stat.AP](#stat.AP) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [SAMEP: A Secure Protocol for Persistent Context Sharing Across AI Agents](https://arxiv.org/abs/2507.10562)
*Hari Masoor*

Main category: cs.AI

TL;DR: 提出SAMEP协议解决AI代理架构临时内存限制问题，在多领域有效，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理架构存在临时内存限制，阻碍跨会话和代理边界的协作与知识共享。

Method: 实现分布式内存库，运用基于向量的语义搜索、AES - 256 - GCM加密访问控制和兼容现有代理通信协议的标准化API。

Result: 在多领域展示有效性，减少73%的冗余计算，上下文相关性得分提高89%，完全符合监管要求。

Conclusion: SAMEP开创持久、协作的AI代理生态系统，同时保障安全和隐私。

Abstract: Current AI agent architectures suffer from ephemeral memory limitations,
preventing effective collaboration and knowledge sharing across sessions and
agent boundaries. We introduce SAMEP (Secure Agent Memory Exchange Protocol), a
novel framework that enables persistent, secure, and semantically searchable
memory sharing among AI agents. Our protocol addresses three critical
challenges: (1) persistent context preservation across agent sessions, (2)
secure multi-agent collaboration with fine-grained access control, and (3)
efficient semantic discovery of relevant historical context. SAMEP implements a
distributed memory repository with vector-based semantic search, cryptographic
access controls (AES-256-GCM), and standardized APIs compatible with existing
agent communication protocols (MCP, A2A). We demonstrate SAMEP's effectiveness
across diverse domains including multi-agent software development, healthcare
AI with HIPAA compliance, and multi-modal processing pipelines. Experimental
results show 73% reduction in redundant computations, 89% improvement in
context relevance scores, and complete compliance with regulatory requirements
including audit trail generation. SAMEP enables a new paradigm of persistent,
collaborative AI agent ecosystems while maintaining security and privacy
guarantees.

</details>


### [2] [AI Mother Tongue: Self-Emergent Communication in MARL via Endogenous Symbol Systems](https://arxiv.org/abs/2507.10566)
*Hung Ming Liu*

Main category: cs.AI

TL;DR: 研究探讨去中心化多智能体强化学习中紧急通信问题，用AIM框架证明无需外部归纳偏置可实现有效符号通信，对比传统方法更具优势并得出三个理论见解，还提及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 解决去中心化多智能体强化学习中紧急通信受‘联合探索困境’限制，质疑传统引入归纳偏置方法是否过度设计。

Method: 使用基于向量量化变分自编码器（VQ - VAE）的‘AI母语’（AIM）框架进行实验。

Result: AIM框架下智能体无需外部归纳偏置实现有效符号通信，比传统方法更具一般性和效率，符号使用呈幂律分布。

Conclusion: 得出‘神经通信假设’‘工具优先原则’‘语义可解释范式’三个理论见解，为连接符号主义和联结主义提供新途径，未来将探索HQ - VAE集成及‘强化学习低级预训练’。

Abstract: In Decentralized Multi-Agent Reinforcement Learning (MARL), the development
of Emergent Communication has long been constrained by the ``Joint Exploration
Dilemma'', leading agents to fall into a ``Communication Vacuum Equilibrium'' .
Traditional methods address this by introducing inductive biases to facilitate
communication emergence . This study fundamentally questions whether such
artificial inductive biases are, in fact, over-engineering. Through experiments
with the ``AI Mother Tongue'' (AIM) framework, based on a Vector Quantized
Variational Autoencoder (VQ-VAE), we demonstrate that when agents possess an
endogenous symbol system, their neural representations naturally exhibit
spontaneous semantic compression and Nash equilibrium-driven semantic
convergence, achieving effective symbolic communication without external
inductive biases. This aligns with recent neuroscience findings suggesting that
the human brain does not directly use human language for internal thought , and
resonates with research on ``soft thinking'' capabilities in Large Language
Models (LLMs) . Compared to traditional explicit communication methods, AIM
demonstrates stronger generality and efficiency. The interpretable analysis
toolkit developed in this study confirms that symbol usage exhibits a
significant power-law distribution, leading to three major theoretical
insights: the ``Neural Communication Hypothesis'', the ``Tool-First
Principle'', and the ``Semantic Interpretability Paradigm''. Future research
will explore the integration of Hierarchical Quantized Variational Autoencoders
(HQ-VAE) to enhance AIM's complex expressive capabilities and investigate the
potential for ``Reinforcement Learning (RL) Low-Level Pre-training''. This
discovery offers new avenues for bridging symbolism and connectionism.

</details>


### [3] [Orchestrator-Agent Trust: A Modular Agentic AI Visual Classification System with Trust-Aware Orchestration and RAG-Based Reasoning](https://arxiv.org/abs/2507.10571)
*Konstantinos I. Roumeliotis,Ranjan Sapkota,Manoj Karkee,Nikolaos D. Tselikas*

Main category: cs.AI

TL;DR: 本文提出模块化Agentic AI视觉分类框架用于苹果叶病诊断，在零样本设置下提升准确率，系统可扩展且开源。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体架构在零样本设置下缺乏信任的问题。

Method: 引入新的模块化框架，集成多模态智能体、推理编排器和RAG模块，设置三种配置，用置信度校准指标调节信任。

Result: 零样本设置下准确率提升77.94%，总体达85.63%；GPT - 4o校准更好，Qwen - 2.5 - VL过度自信；image - RAG可校正过度自信。

Conclusion: 该系统分离感知与元推理，可扩展到关键信任领域，且已开源支持复现和社区基准测试。

Abstract: Modern Artificial Intelligence (AI) increasingly relies on multi-agent
architectures that blend visual and language understanding. Yet, a pressing
challenge remains: How can we trust these agents especially in zero-shot
settings with no fine-tuning? We introduce a novel modular Agentic AI visual
classification framework that integrates generalist multimodal agents with a
non-visual reasoning orchestrator and a Retrieval-Augmented Generation (RAG)
module. Applied to apple leaf disease diagnosis, we benchmark three
configurations: (I) zero-shot with confidence-based orchestration, (II)
fine-tuned agents with improved performance, and (III) trust-calibrated
orchestration enhanced by CLIP-based image retrieval and re-evaluation loops.
Using confidence calibration metrics (ECE, OCR, CCC), the orchestrator
modulates trust across agents. Our results demonstrate a 77.94\% accuracy
improvement in the zero-shot setting using trust-aware orchestration and RAG,
achieving 85.63\% overall. GPT-4o showed better calibration, while Qwen-2.5-VL
displayed overconfidence. Furthermore, image-RAG grounded predictions with
visually similar cases, enabling correction of agent overconfidence via
iterative re-evaluation. The proposed system separates perception (vision
agents) from meta-reasoning (orchestrator), enabling scalable and interpretable
multi-agent AI. This blueprint is extensible to diagnostics, biology, and other
trust-critical domains. All models, prompts, results, and system components
including the complete software source code are openly released to support
reproducibility, transparency, and community benchmarking at Github:
https://github.com/Applied-AI-Research-Lab/Orchestrator-Agent-Trust

</details>


### [4] [Comprehension Without Competence: Architectural Limits of LLMs in Symbolic Computation and Reasoning](https://arxiv.org/abs/2507.10624)
*Zheng Zhang*

Main category: cs.AI

TL;DR: 本文指出大语言模型（LLMs）在符号推理等任务上失败，诊断为计算‘裂脑综合征’，并说明其局限及对未来模型的启示。


<details>
  <summary>Details</summary>
Motivation: 分析大语言模型在需要符号推理、算术准确性和逻辑一致性的任务上系统性失败的原因。

Method: 通过控制实验和架构分析。

Result: 发现LLMs存在理解和能力的差距，常能阐述正确原则但无法可靠应用，存在计算‘裂脑综合征’，即指令和行动路径分离。

Conclusion: 明确当前LLM能力边界，推动具有元认知控制等能力的未来模型发展，解释了机械可解释性结果和神经内省分析的局限性。

Abstract: Large Language Models (LLMs) display striking surface fluency yet
systematically fail at tasks requiring symbolic reasoning, arithmetic accuracy,
and logical consistency. This paper offers a structural diagnosis of such
failures, revealing a persistent gap between \textit{comprehension} and
\textit{competence}. Through controlled experiments and architectural analysis,
we demonstrate that LLMs often articulate correct principles without reliably
applying them--a failure rooted not in knowledge access, but in computational
execution. We term this phenomenon the computational \textit{split-brain
syndrome}, where instruction and action pathways are geometrically and
functionally dissociated. This core limitation recurs across domains, from
mathematical operations to relational inferences, and explains why model
behavior remains brittle even under idealized prompting. We argue that LLMs
function as powerful pattern completion engines, but lack the architectural
scaffolding for principled, compositional reasoning. Our findings delineate the
boundary of current LLM capabilities and motivate future models with
metacognitive control, principle lifting, and structurally grounded execution.
This diagnosis also clarifies why mechanistic interpretability findings may
reflect training-specific pattern coordination rather than universal
computational principles, and why the geometric separation between instruction
and execution pathways suggests limitations in neural introspection and
mechanistic analysis.

</details>


### [5] [Enhancing the Capabilities of Large Language Models for API calls through Knowledge Graphs](https://arxiv.org/abs/2507.10630)
*Ye Yang,Xue Xiao,Ping Yin,Taotao Xie*

Main category: cs.AI

TL;DR: 本文提出KG2data系统用于气象领域智能数据获取和查询处理，评估API调用准确性，表现优于对比系统，解决了LLM系统访问特定领域知识受限的问题。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在知识密集型领域如气象领域利用API调用工具的能力未充分探索，需要更好的解决方案。

Method: 引入KG2data系统，整合知识图谱、大语言模型、ReAct代理和工具使用技术，使用虚拟API从三个指标评估API调用准确性。

Result: KG2data在三个评估指标上表现优于RAG2data和chat2data。

Conclusion: KG2data为高知识需求领域的智能、基于知识的问答和数据分析提供了新解决方案。

Abstract: API calls by large language models (LLMs) offer a cutting-edge approach for
data analysis. However, their ability to effectively utilize tools via API
calls remains underexplored in knowledge-intensive domains like meteorology.
This paper introduces KG2data, a system that integrates knowledge graphs, LLMs,
ReAct agents, and tool-use technologies to enable intelligent data acquisition
and query handling in the meteorological field. Using a virtual API, we
evaluate API call accuracy across three metrics: name recognition failure,
hallucination failure, and call correctness. KG2data achieves superior
performance (1.43%, 0%, 88.57%) compared to RAG2data (16%, 10%, 72.14%) and
chat2data (7.14%, 8.57%, 71.43%). KG2data differs from typical LLM-based
systems by addressing their limited access to domain-specific knowledge, which
hampers performance on complex or terminology-rich queries. By using a
knowledge graph as persistent memory, our system enhances content retrieval,
complex query handling, domain-specific reasoning, semantic relationship
resolution, and heterogeneous data integration. It also mitigates the high cost
of fine-tuning LLMs, making the system more adaptable to evolving domain
knowledge and API structures. In summary, KG2data provides a novel solution for
intelligent, knowledge-based question answering and data analysis in domains
with high knowledge demands.

</details>


### [6] [From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web of Agents](https://arxiv.org/abs/2507.10644)
*Tatiana Petrova,Aleksandr Puzikov,Boris Bliznukov,Radu State*

Main category: cs.AI

TL;DR: 本文首次对Web of Agents (WoA) 进行全面的进化概述，介绍四轴分类法分析代理架构，指出智能位置的范式转变，认为新协议不足以构建生态系统，提出解决社会技术挑战的研究议程。


<details>
  <summary>Details</summary>
Motivation: 当前WoA领域研究分散，不同社区研究割裂，阻碍对该领域的整体理解，因此要进行全面进化概述。

Method: 引入四轴分类法（语义基础、通信范式、智能位置、发现机制）对各代代理架构进行分析。

Result: 发现现代协议是对早期标准局限性的进化回应，识别出智能位置从外部数据或平台转移到代理核心模型的范式转变。

Conclusion: 新协议对构建强大、开放、可信的生态系统是必要但不充分的，未来研究应聚焦于解决社会技术挑战。

Abstract: The concept of the Web of Agents (WoA), which transforms the static,
document-centric Web into an environment of autonomous agents acting on users'
behalf, has attracted growing interest as large language models (LLMs) become
more capable. However, research in this area is still fragmented across
different communities. Contemporary surveys catalog the latest LLM-powered
frameworks, while the rich histories of Multi-Agent Systems (MAS) and the
Semantic Web are often treated as separate, legacy domains. This fragmentation
obscures the intellectual lineage of modern systems and hinders a holistic
understanding of the field's trajectory. We present the first comprehensive
evolutionary overview of the WoA. We show that modern protocols like A2A and
the MCP, are direct evolutionary responses to the well-documented limitations
of earlier standards like FIPA standards and OWL-based semantic agents. To
systematize this analysis, we introduce a four-axis taxonomy (semantic
foundation, communication paradigm, locus of intelligence, discovery
mechanism). This framework provides a unified analytical lens for comparing
agent architectures across all generations, revealing a clear line of descent
where others have seen a disconnect. Our analysis identifies a paradigm shift
in the 'locus of intelligence': from being encoded in external data (Semantic
Web) or the platform (MAS) to being embedded within the agent's core model
(LLM). This shift is foundational to modern Agentic AI, enabling the scalable
and adaptive systems the WoA has long envisioned. We conclude that while new
protocols are essential, they are insufficient for building a robust, open,
trustworthy ecosystem. Finally, we argue that the next research frontier lies
in solving persistent socio-technical challenges, and we map out a new agenda
focused on decentralized identity, economic models, security, and governance
for the emerging WoA.

</details>


### [7] [DrafterBench: Benchmarking Large Language Models for Tasks Automation in Civil Engineering](https://arxiv.org/abs/2507.11527)
*Yinsheng Li,Zhen Dong,Yi Shao*

Main category: cs.AI

TL;DR: 提出DrafterBench用于全面评估土木工程技术图纸修订中LLM代理，含多种任务和工具，可分析能力与误差，代码开源。


<details>
  <summary>Details</summary>
Motivation: 缺乏从工业角度系统评估自动化代理的基准，在土木工程领域需要更多基准。

Method: 提出DrafterBench基准，包含从实际图纸文件总结的十二类任务、46个定制函数/工具和1920个任务，全面评估多种能力。

Result: DrafterBench可详细分析任务准确性和错误统计。

Conclusion: DrafterBench能深入了解代理能力，为工程应用中集成LLM确定改进目标。

Abstract: Large Language Model (LLM) agents have shown great potential for solving
real-world problems and promise to be a solution for tasks automation in
industry. However, more benchmarks are needed to systematically evaluate
automation agents from an industrial perspective, for example, in Civil
Engineering. Therefore, we propose DrafterBench for the comprehensive
evaluation of LLM agents in the context of technical drawing revision, a
representation task in civil engineering. DrafterBench contains twelve types of
tasks summarized from real-world drawing files, with 46 customized
functions/tools and 1920 tasks in total. DrafterBench is an open-source
benchmark to rigorously test AI agents' proficiency in interpreting intricate
and long-context instructions, leveraging prior knowledge, and adapting to
dynamic instruction quality via implicit policy awareness. The toolkit
comprehensively assesses distinct capabilities in structured data
comprehension, function execution, instruction following, and critical
reasoning. DrafterBench offers detailed analysis of task accuracy and error
statistics, aiming to provide deeper insight into agent capabilities and
identify improvement targets for integrating LLMs in engineering applications.
Our benchmark is available at https://github.com/Eason-Li-AIS/DrafterBench,
with the test set hosted at
https://huggingface.co/datasets/Eason666/DrafterBench.

</details>


### [8] [Parsing Musical Structure to Enable Meaningful Variations](https://arxiv.org/abs/2507.10740)
*Maziar Kanani,Sean O Leary,James McDermott*

Main category: cs.AI

TL;DR: 提出基于规则的方法，通过对现有曲调的语法进行变异生成新曲调，研究曲调在多次变异中的变化及各变异类型影响。


<details>
  <summary>Details</summary>
Motivation: 探索通过变异现有曲调来生成新音乐的方法。

Method: 解析曲调找到PA结构形成语法，对语法进行随机变异，变异后扩展语法得到新曲调，用编辑距离等指标研究曲调变化。

Result: 多次变异后得到与原曲调相关的新曲调，分析了各变异类型影响。

Conclusion: 该方法能生成新的音高序列，但仅聚焦于音高序列生成。

Abstract: This paper presents a novel rule-based approach for generating music by
varying existing tunes. We parse each tune to find the Pathway Assembly (PA) [
1], that is a structure representing all repetitions in the tune. The Sequitur
algorithm [2 ] is used for this. The result is a grammar. We then carry out
mutation on the grammar, rather than on a tune directly. There are potentially
19 types of mutations such as adding, removing, swapping or reversing parts of
the grammar that can be applied to the grammars. The system employs one of the
mutations randomly in this step to automatically manipulate the grammar.
Following the mutation, we need to expand the grammar which returns a new tune.
The output after 1 or more mutations will be a new tune related to the original
tune. Our study examines how tunes change gradually over the course of multiple
mutations. Edit distances, structural complexity and length of the tunes are
used to show how a tune is changed after multiple mutations. In addition, the
size of effect of each mutation type is analyzed. As a final point, we review
the musical aspect of the output tunes. It should be noted that the study only
focused on generating new pitch sequences. The study is based on an Irish
traditional tune dataset and a list of integers has been used to represent each
tune's pitch values.

</details>


### [9] [AI and the Net-Zero Journey: Energy Demand, Emissions, and the Potential for Transition](https://arxiv.org/abs/2507.10750)
*Pandu Devarakota,Nicolas Tsesmetzis,Faruk O. Alpak,Apurva Gala,Detlef Hohl*

Main category: cs.AI

TL;DR: 文章探讨数据中心能耗与温室气体排放，分析AI到2035年对CO2排放影响，近期或增排放，长期有望减碳。


<details>
  <summary>Details</summary>
Motivation: 在AI大量应用且数据中心建设受关注背景下，分析其对CO2排放的影响及在能源领域潜力。

Method: 提出数据中心能耗情景及影响，分析近、长期AI对CO2排放的不同作用。

Result: 近期AI需求增长或增加电力消耗和CO2排放，长期AI能优化各行业流程，减少碳足迹。

Conclusion: AI虽会给环境带来初期压力，但有潜力助力气候缓解工作。

Abstract: Thanks to the availability of massive amounts of data, computing resources,
and advanced algorithms, AI has entered nearly every sector. This has sparked
significant investment and interest, particularly in building data centers with
the necessary hardware and software to develop and operate AI models and
AI-based workflows. In this technical review article, we present energy
consumption scenarios of data centers and impact on GHG emissions, considering
both near-term projections (up to 2030) and long-term outlook (2035 and
beyond). We address the quintessential question of whether AI will have a net
positive, neutral, or negative impact on CO2 emissions by 2035. Additionally,
we discuss AI's potential to automate, create efficient and disruptive
workflows across various fields related to energy production, supply and
consumption. In the near-term scenario, the growing demand for AI will likely
strain computing resources, lead to increase in electricity consumption and
therefore associated CO2 emissions. This is due to the power-hungry nature of
big data centers and the requirements for training and running of large and
complex AI models, as well as the penetration of AI assistant search and
applications for public use. However, the long-term outlook could be more
promising. AI has the potential to be a game-changer in CO2 reduction. Its
ability to further automate and optimize processes across industries, from
energy production to logistics, could significantly decrease our carbon
footprint. This positive impact is anticipated to outweigh the initial
emissions bump, creating value for businesses and society in areas where
traditional solutions have fallen short. In essence, AI might cause some
initial growing pains for the environment, but it has the potential to support
climate mitigation efforts.

</details>


### [10] [Automated Thematic Analyses Using LLMs: Xylazine Wound Management Social Media Chatter Use Case](https://arxiv.org/abs/2507.10803)
*JaMor Hairston,Ritvik Ranjan,Sahithi Lakamana,Anthony Spadaro,Selen Bozkurt,Jeanmarie Perrone,Abeed Sarker*

Main category: cs.AI

TL;DR: 研究评估LLMs复制专家驱动社交媒体数据主题分析的可行性，发现少样本提示的LLM方法可自动化主题分析。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在归纳主题分析中面临挑战，评估其复制专家驱动的社交媒体数据主题分析的可行性。

Method: 使用两个非交叉的Reddit数据集，以12个专家推导的主题为标准，将任务建模为一系列二分类问题，采用零样本、单样本和少样本提示策略，用准确率、精确率、召回率和F1分数评估5个LLMs。

Result: 验证集上，GPT - 4o双样本提示表现最佳，高流行主题的模型衍生主题分布与专家分类相近。

Conclusion: 少样本基于LLM的方法可自动化主题分析，为定性研究提供可扩展补充。

Abstract: Background Large language models (LLMs) face challenges in inductive thematic
analysis, a task requiring deep interpretive and domain-specific expertise. We
evaluated the feasibility of using LLMs to replicate expert-driven thematic
analysis of social media data. Methods Using two temporally non-intersecting
Reddit datasets on xylazine (n=286 and n=686, for model optimization and
validation, respectively) with twelve expert-derived themes, we evaluated five
LLMs against expert coding. We modeled the task as a series of binary
classifications, rather than a single, multi-label classification, employing
zero-, single-, and few-shot prompting strategies and measuring performance via
accuracy, precision, recall, and F1-score. Results On the validation set,
GPT-4o with two-shot prompting performed best (accuracy: 90.9%; F1-score:
0.71). For high-prevalence themes, model-derived thematic distributions closely
mirrored expert classifications (e.g., xylazine use: 13.6% vs. 17.8%; MOUD use:
16.5% vs. 17.8%). Conclusions Our findings suggest that few-shot LLM-based
approaches can automate thematic analyses, offering a scalable supplement for
qualitative research. Keywords: thematic analysis, large language models,
natural language processing, qualitative analysis, social media, prompt
engineering, public health

</details>


### [11] [IoT Malware Network Traffic Detection using Deep Learning and GraphSAGE Models](https://arxiv.org/abs/2507.10758)
*Nikesh Prajapati,Bimal Karki,Saroj Gopali,Akbar Siami Namin*

Main category: cs.AI

TL;DR: 本文用深度学习模型检测物联网恶意攻击，评估多种模型，BERT 表现最佳。


<details>
  <summary>Details</summary>
Motivation: 通过深度学习模型检测物联网恶意攻击，并综合评估深度学习和基于图的模型在恶意网络流量检测方面的性能。

Method: 选用基于 GraphSAGE、BERT、TCN、Multi - Head Attention、BI - LSTM、LSTM 等模型进行物联网恶意网络流量检测。

Result: BERT 性能最佳，准确率达 99.94%，多项指标达 99.99%；Multi - Head Attention 检测能力好且结果可解释，但处理时间长；GraphSAGE 训练时间最短，不过准确率等指标最低。

Conclusion: 不同模型在物联网恶意网络流量检测中有不同表现，BERT 在捕捉时间依赖方面能力突出。

Abstract: This paper intends to detect IoT malicious attacks through deep learning
models and demonstrates a comprehensive evaluation of the deep learning and
graph-based models regarding malicious network traffic detection. The models
particularly are based on GraphSAGE, Bidirectional encoder representations from
transformers (BERT), Temporal Convolutional Network (TCN) as well as Multi-Head
Attention, together with Bidirectional Long Short-Term Memory (BI-LSTM)
Multi-Head Attention and BI-LSTM and LSTM models. The chosen models
demonstrated great performance to model temporal patterns and detect feature
significance. The observed performance are mainly due to the fact that IoT
system traffic patterns are both sequential and diverse, leaving a rich set of
temporal patterns for the models to learn. Experimental results showed that
BERT maintained the best performance. It achieved 99.94% accuracy rate
alongside high precision and recall, F1-score and AUC-ROC score of 99.99% which
demonstrates its capabilities through temporal dependency capture. The
Multi-Head Attention offered promising results by providing good detection
capabilities with interpretable results. On the other side, the Multi-Head
Attention model required significant processing time like BI-LSTM variants. The
GraphSAGE model achieved good accuracy while requiring the shortest training
time but yielded the lowest accuracy, precision, and F1 score compared to the
other models

</details>


### [12] [Detecting AI Assistance in Abstract Complex Tasks](https://arxiv.org/abs/2507.10761)
*Tyler King,Nikolos Gurney,John H. Miller,Volkan Ustun*

Main category: cs.AI

TL;DR: 本文提出将检测AI帮助作为分类任务，构建不同数据表示方式和架构对非机器学习友好的数据进行分类，展示编码时空量对检测抽象任务中AI帮助的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着AI在复杂任务中普及，检测AI帮助变得重要，但人类检测困难，且现有研究多针对具体数据类，很多AI辅助检测场景数据不适合机器学习。

Method: 构建四种神经网络友好的图像表示和一种时间序列表示，在三种经典深度学习架构和并行CNN - RNN架构上进行基准测试。

Result: 合适预处理后，常见模型能有效对非机器学习友好的数据进行分类。

Conclusion: 编码时空量对检测抽象任务中AI帮助很重要。

Abstract: Detecting assistance from artificial intelligence is increasingly important
as they become ubiquitous across complex tasks such as text generation, medical
diagnosis, and autonomous driving. Aid detection is challenging for humans,
especially when looking at abstract task data. Artificial neural networks excel
at classification thanks to their ability to quickly learn from and process
large amounts of data -- assuming appropriate preprocessing. We posit detecting
help from AI as a classification task for such models. Much of the research in
this space examines the classification of complex but concrete data classes,
such as images. Many AI assistance detection scenarios, however, result in data
that is not machine learning-friendly. We demonstrate that common models can
effectively classify such data when it is appropriately preprocessed. To do so,
we construct four distinct neural network-friendly image formulations along
with an additional time-series formulation that explicitly encodes the
exploration/exploitation of users, which allows for generalizability to other
abstract tasks. We benchmark the quality of each image formulation across three
classical deep learning architectures, along with a parallel CNN-RNN
architecture that leverages the additional time series to maximize testing
performance, showcasing the importance of encoding temporal and spatial
quantities for detecting AI aid in abstract tasks.

</details>


### [13] [Uncertainty-Informed Scheduling of Decision Points for Intelligent Mobile Health Interventions](https://arxiv.org/abs/2507.10798)
*Asim H. Gazi,Bhanu T. Gullapalli,Daiqi Gao,Benjamin M. Marlin,Vivek Shetty,Susan A. Murphy*

Main category: cs.AI

TL;DR: 本文提出 SigmaScheduling 方法动态安排决策点，经评估能提高干预及时的可能性，推动精准移动健康。


<details>
  <summary>Details</summary>
Motivation: 当前移动健康干预决策点固定间隔安排方式对日常不规律人群效果差，需改进。

Method: 提出 SigmaScheduling 方法，根据预测行为时间的不确定性动态安排决策点。

Result: 使用 68 名参与者的真实数据评估，该方法在至少 70% 的情况下使决策点在刷牙事件之前。

Conclusion: SigmaScheduling 能推动精准移动健康，尤其针对时间敏感的习惯行为。

Abstract: Timely decision making is critical to the effectiveness of mobile health
(mHealth) interventions. At predefined timepoints called "decision points,"
intelligent mHealth systems such as just-in-time adaptive interventions
(JITAIs) estimate an individual's biobehavioral context from sensor or survey
data and determine whether and how to intervene. For interventions targeting
habitual behavior (e.g., oral hygiene), effectiveness often hinges on
delivering support shortly before the target behavior is likely to occur.
Current practice schedules decision points at a fixed interval (e.g., one hour)
before user-provided behavior times, and the fixed interval is kept the same
for all individuals. However, this one-size-fits-all approach performs poorly
for individuals with irregular routines, often scheduling decision points after
the target behavior has already occurred, rendering interventions ineffective.
In this paper, we propose SigmaScheduling, a method to dynamically schedule
decision points based on uncertainty in predicted behavior times. When behavior
timing is more predictable, SigmaScheduling schedules decision points closer to
the predicted behavior time; when timing is less certain, SigmaScheduling
schedules decision points earlier, increasing the likelihood of timely
intervention. We evaluated SigmaScheduling using real-world data from 68
participants in a 10-week trial of Oralytics, a JITAI designed to improve daily
toothbrushing. SigmaScheduling increased the likelihood that decision points
preceded brushing events in at least 70% of cases, preserving opportunities to
intervene and impact behavior. Our results indicate that SigmaScheduling can
advance precision mHealth, particularly for JITAIs targeting time-sensitive,
habitual behaviors such as oral hygiene or dietary habits.

</details>


### [14] [AF-XRAY: Visual Explanation and Resolution of Ambiguity in Legal Argumentation Frameworks](https://arxiv.org/abs/2507.10831)
*Yilin Xia,Heng Zheng,Shawn Bowers,Bertram Ludäscher*

Main category: cs.AI

TL;DR: 介绍开源工具AF - XRAY用于法律推理中抽象论证框架的探索、分析和可视化，能处理歧义问题并通过实例展示其支持目的论法律推理。


<details>
  <summary>Details</summary>
Motivation: 论证框架在法律推理中存在让非专家识别歧义来源和解释论证接受情况的难题，需要工具解决。

Method: AF - XRAY引入分层可视化、攻击边分类、替代二值解叠加可视化、识别关键攻击集等方法。

Result: AF - XRAY能将模糊场景转化为有根据的解决方案，让用户找出歧义原因并探索替代解决方案。

Conclusion: 使用真实法律案例表明该工具通过揭示不同假设如何导致不同合理结论，支持目的论法律推理。

Abstract: Argumentation frameworks (AFs) provide formal approaches for legal reasoning,
but identifying sources of ambiguity and explaining argument acceptance remains
challenging for non-experts. We present AF-XRAY, an open-source toolkit for
exploring, analyzing, and visualizing abstract AFs in legal reasoning. AF-XRAY
introduces: (i) layered visualizations based on game-theoretic argument length
revealing well-founded derivation structures; (ii) classification of attack
edges by semantic roles (primary, secondary, blunders); (iii) overlay
visualizations of alternative 2-valued solutions on ambiguous 3-valued grounded
semantics; and (iv) identification of critical attack sets whose suspension
resolves undecided arguments. Through systematic generation of critical attack
sets, AF-XRAY transforms ambiguous scenarios into grounded solutions, enabling
users to pinpoint specific causes of ambiguity and explore alternative
resolutions. We use real-world legal cases (e.g., Wild Animals as modeled by
Bench-Capon) to show that our tool supports teleological legal reasoning by
revealing how different assumptions lead to different justified conclusions.

</details>


### [15] [NavComposer: Composing Language Instructions for Navigation Trajectories through Action-Scene-Object Modularization](https://arxiv.org/abs/2507.10894)
*Zongtao He,Liuyi Wang,Lu Chen,Chengju Liu,Qijun Chen*

Main category: cs.AI

TL;DR: 提出NavComposer自动生成高质量导航指令，引入NavInstrCritic评估系统，实验证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 专家提供的导航指令数量有限，合成注释质量不足，无法满足大规模研究需求。

Method: 提出NavComposer框架，分解语义实体并重新组合成指令；引入NavInstrCritic评估系统，从三个维度评估指令质量。

Result: 实验提供了方法有效性的直接和实际证据。

Conclusion: 将指令生成和评估与特定导航代理解耦，使研究更具可扩展性和通用性。

Abstract: Language-guided navigation is a cornerstone of embodied AI, enabling agents
to interpret language instructions and navigate complex environments. However,
expert-provided instructions are limited in quantity, while synthesized
annotations often lack quality, making them insufficient for large-scale
research. To address this, we propose NavComposer, a novel framework for
automatically generating high-quality navigation instructions. NavComposer
explicitly decomposes semantic entities such as actions, scenes, and objects,
and recomposes them into natural language instructions. Its modular
architecture allows flexible integration of state-of-the-art techniques, while
the explicit use of semantic entities enhances both the richness and accuracy
of instructions. Moreover, it operates in a data-agnostic manner, supporting
adaptation to diverse navigation trajectories without domain-specific training.
Complementing NavComposer, we introduce NavInstrCritic, a comprehensive
annotation-free evaluation system that assesses navigation instructions on
three dimensions: contrastive matching, semantic consistency, and linguistic
diversity. NavInstrCritic provides a holistic evaluation of instruction
quality, addressing limitations of traditional metrics that rely heavily on
expert annotations. By decoupling instruction generation and evaluation from
specific navigation agents, our method enables more scalable and generalizable
research. Extensive experiments provide direct and practical evidence for the
effectiveness of our method.

</details>


### [16] [Lessons Learned from Evaluation of LLM based Multi-agents in Safer Therapy Recommendation](https://arxiv.org/abs/2507.10911)
*Yicong Wu,Ting Chen,Irit Hochberg,Zhoujian Sun,Ruth Edry,Zhengxing Huang,Mor Peleg*

Main category: cs.AI

TL;DR: 研究探讨基于大语言模型的多智能体系统用于慢性病共病患者安全治疗推荐的可行性与价值，结果显示单智能体GP表现与多学科团队相当，推荐有不完整和不必要用药问题。


<details>
  <summary>Details</summary>
Motivation: 慢性病共病患者治疗推荐因治疗冲突风险具有挑战性，现有决策支持系统有扩展性局限，受全科医生管理共病患者方式启发进行研究。

Method: 设计单智能体和多智能体系统框架模拟多学科团队决策，在共病患者治疗规划任务上用基准案例评估系统，对比多智能体与单智能体方法及现实基准，定义超越技术指标的评估指标。

Result: 当前大语言模型下，单智能体GP表现与多学科团队相当，最佳得分模型能提出解决所有临床目标的正确推荐，但推荐不完整，部分模型存在不必要用药导致冲突。

Conclusion: 未明确提及，从结果看，虽大语言模型在治疗推荐有一定效果，但仍存在推荐不完整和不必要用药等问题待解决。

Abstract: Therapy recommendation for chronic patients with multimorbidity is
challenging due to risks of treatment conflicts. Existing decision support
systems face scalability limitations. Inspired by the way in which general
practitioners (GP) manage multimorbidity patients, occasionally convening
multidisciplinary team (MDT) collaboration, this study investigated the
feasibility and value of using a Large Language Model (LLM)-based multi-agent
system (MAS) for safer therapy recommendations. We designed a single agent and
a MAS framework simulating MDT decision-making by enabling discussion among LLM
agents to resolve medical conflicts. The systems were evaluated on therapy
planning tasks for multimorbidity patients using benchmark cases. We compared
MAS performance with single-agent approaches and real-world benchmarks. An
important contribution of our study is the definition of evaluation metrics
that go beyond the technical precision and recall and allow the inspection of
clinical goals met and medication burden of the proposed advices to a gold
standard benchmark. Our results show that with current LLMs, a single agent GP
performs as well as MDTs. The best-scoring models provide correct
recommendations that address all clinical goals, yet the advices are
incomplete. Some models also present unnecessary medications, resulting in
unnecessary conflicts between medication and conditions or drug-drug
interactions.

</details>


### [17] [Enhancing Safe and Controllable Protein Generation via Knowledge Preference Optimization](https://arxiv.org/abs/2507.10923)
*Yuhao Wang,Keyan Ding,Kehua Feng,Zeyuan Wang,Ming Qin,Xiaotong Li,Qiang Zhang,Huajun Chen*

Main category: cs.AI

TL;DR: 提出KPO框架应对蛋白语言模型生成有害序列问题，实验证明其有效降低风险并保持高功能性。


<details>
  <summary>Details</summary>
Motivation: 蛋白语言模型有生成有害蛋白序列风险，存在生物安全和伦理挑战，需解决该问题。

Method: 提出Knowledge - guided Preference Optimization (KPO)框架，通过蛋白安全知识图谱整合先验知识，用图剪枝策略识别偏好序列，用强化学习降低生成有害蛋白风险。

Result: KPO有效降低生成有害序列的可能性，同时保持高功能性。

Conclusion: KPO为生物技术中应用生成模型提供了强大的安全保障框架。

Abstract: Protein language models have emerged as powerful tools for sequence
generation, offering substantial advantages in functional optimization and
denovo design. However, these models also present significant risks of
generating harmful protein sequences, such as those that enhance viral
transmissibility or evade immune responses. These concerns underscore critical
biosafety and ethical challenges. To address these issues, we propose a
Knowledge-guided Preference Optimization (KPO) framework that integrates prior
knowledge via a Protein Safety Knowledge Graph. This framework utilizes an
efficient graph pruning strategy to identify preferred sequences and employs
reinforcement learning to minimize the risk of generating harmful proteins.
Experimental results demonstrate that KPO effectively reduces the likelihood of
producing hazardous sequences while maintaining high functionality, offering a
robust safety assurance framework for applying generative models in
biotechnology.

</details>


### [18] [Modeling Habitat Shifts: Integrating Convolutional Neural Networks and Tabular Data for Species Migration Prediction](https://arxiv.org/abs/2507.10993)
*Emir Durakovic,Min-Hong Shih*

Main category: cs.AI

TL;DR: 结合CNN和表格数据，利用卫星图像和环境特征预测鸟类在不同气候下的存在情况，平均准确率达85%。


<details>
  <summary>Details</summary>
Motivation: 应对气候导致的栖息地范围变化，准确建模特定栖息地是否有鸟类存在。

Method: 结合卷积神经网络（CNNs）和表格数据，利用卫星图像和环境特征（如温度、降水、海拔）进行预测，CNN模型捕捉景观空间特征，表格方法使用生态和地理数据。

Result: 两个系统预测鸟类分布的平均准确率达85%。

Conclusion: 该方法可扩展且可靠，有助于理解鸟类迁徙。

Abstract: Due to climate-induced changes, many habitats are experiencing range shifts
away from their traditional geographic locations (Piguet, 2011). We propose a
solution to accurately model whether bird species are present in a specific
habitat through the combination of Convolutional Neural Networks (CNNs)
(O'Shea, 2015) and tabular data. Our approach makes use of satellite imagery
and environmental features (e.g., temperature, precipitation, elevation) to
predict bird presence across various climates. The CNN model captures spatial
characteristics of landscapes such as forestation, water bodies, and
urbanization, whereas the tabular method uses ecological and geographic data.
Both systems predict the distribution of birds with an average accuracy of 85%,
offering a scalable but reliable method to understand bird migration.

</details>


### [19] [Personalized Exercise Recommendation with Semantically-Grounded Knowledge Tracing](https://arxiv.org/abs/2507.11060)
*Yilmazcan Ozyurt,Tunaberk Almaci,Stefan Feuerriegel,Mrinmaya Sachan*

Main category: cs.AI

TL;DR: 提出ExRec框架用于个性化练习推荐，结合知识追踪，经多任务验证有效，证明KT引导的RL在教育个性化方面有前景。


<details>
  <summary>Details</summary>
Motivation: 现有练习推荐方法常忽略问题语义内容和学生学习的顺序结构化进展。

Method: 构建端到端ExRec框架，从标注问题的知识组件、学习语义表示到训练KT模型和优化强化学习方法，用定制的基于模型的值估计方法改进标准Q学习的连续强化学习方法。

Result: 在四个在线数学学习的真实任务中验证了ExRec的有效性，能稳健泛化到新问题并产生可解释的学生学习轨迹。

Conclusion: KT引导的RL在教育有效个性化方面有前景。

Abstract: We introduce ExRec, a general framework for personalized exercise
recommendation with semantically-grounded knowledge tracing. Our method builds
on the observation that existing exercise recommendation approaches simulate
student performance via knowledge tracing (KT) but they often overlook two key
aspects: (a) the semantic content of questions and (b) the sequential,
structured progression of student learning. To address this, our ExRec presents
an end-to-end pipeline, from annotating the KCs of questions and learning their
semantic representations to training KT models and optimizing several
reinforcement learning (RL) methods. Moreover, we improve standard
Q-learning-based continuous RL methods via a tailored model-based value
estimation (MVE) approach that directly leverages the components of KT model in
estimating cumulative knowledge improvement. We validate the effectiveness of
our ExRec using various RL methods across four real-world tasks with different
educational goals in online math learning. We further show that ExRec
generalizes robustly to new, unseen questions and that it produces
interpretable student learning trajectories. Together, our findings highlight
the promise of KT-guided RL for effective personalization in education.

</details>


### [20] [Tactical Decision for Multi-UGV Confrontation with a Vision-Language Model-Based Commander](https://arxiv.org/abs/2507.11079)
*Li Wang,Qizhen Wu,Lei Chen*

Main category: cs.AI

TL;DR: 提出基于视觉语言模型的指挥官，解决自主对抗中感知到决策推理问题，实验胜率超80%。


<details>
  <summary>Details</summary>
Motivation: 多无人地面车辆对抗中，传统规则方法在复杂战场脆弱，强化学习缺乏可解释性，难以进行战略决策。

Method: 集成视觉语言模型用于场景理解和轻量级大语言模型用于战略推理，在共享语义空间实现统一感知和决策。

Result: 模拟和消融实验显示，与基线模型相比，该方法胜率超80%。

Conclusion: 所提方法结合两个模块建立全链过程，反映人类指挥官认知过程，有强适应性和可解释性。

Abstract: In multiple unmanned ground vehicle confrontations, autonomously evolving
multi-agent tactical decisions from situational awareness remain a significant
challenge. Traditional handcraft rule-based methods become vulnerable in the
complicated and transient battlefield environment, and current reinforcement
learning methods mainly focus on action manipulation instead of strategic
decisions due to lack of interpretability. Here, we propose a vision-language
model-based commander to address the issue of intelligent
perception-to-decision reasoning in autonomous confrontations. Our method
integrates a vision language model for scene understanding and a lightweight
large language model for strategic reasoning, achieving unified perception and
decision within a shared semantic space, with strong adaptability and
interpretability. Unlike rule-based search and reinforcement learning methods,
the combination of the two modules establishes a full-chain process, reflecting
the cognitive process of human commanders. Simulation and ablation experiments
validate that the proposed approach achieves a win rate of over 80% compared
with baseline models.

</details>


### [21] [Function-to-Style Guidance of LLMs for Code Translation](https://arxiv.org/abs/2507.11083)
*Longhui Zhang,Bin Wang,Jiahao Wang,Xiaofeng Zhao,Min Zhang,Hao Yang,Meishan Zhang,Yu Li,Jing Li,Jun Yu,Min Zhang*

Main category: cs.AI

TL;DR: 提出F2STrans范式提升大语言模型代码翻译性能，含功能学习和风格学习两阶段，引入新基准，实验显示显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在代码翻译中保证翻译代码的正确性和可读性有挑战，限制在实际软件开发中的有效应用。

Method: 提出F2STrans范式，包含功能学习（用高质量代码对优化正确性）和风格学习（结合正负风格示例提升可读性），引入新的代码翻译基准。

Result: 在新基准和现有数据集实验表明显著提升代码翻译性能，Qwen - 1.5B在20种场景平均表现超提示增强的Qwen - 32B和GPT - 4。

Conclusion: F2STrans范式能有效提升大语言模型在代码翻译中的性能。

Abstract: Large language models (LLMs) have made significant strides in code
translation tasks. However, ensuring both the correctness and readability of
translated code remains a challenge, limiting their effective adoption in
real-world software development. In this work, we propose F2STrans, a
function-to-style guiding paradigm designed to progressively improve the
performance of LLMs in code translation. Our approach comprises two key stages:
(1) Functional learning, which optimizes translation correctness using
high-quality source-target code pairs mined from online programming platforms,
and (2) Style learning, which improves translation readability by incorporating
both positive and negative style examples. Additionally, we introduce a novel
code translation benchmark that includes up-to-date source code, extensive test
cases, and manually annotated ground-truth translations, enabling comprehensive
functional and stylistic evaluations. Experiments on both our new benchmark and
existing datasets demonstrate that our approach significantly improves code
translation performance. Notably, our approach enables Qwen-1.5B to outperform
prompt-enhanced Qwen-32B and GPT-4 on average across 20 diverse code
translation scenarios.

</details>


### [22] [AI Agent Architecture for Decentralized Trading of Alternative Assets](https://arxiv.org/abs/2507.11117)
*Ailiya Borjigin,Cong He,Charles CC Lee,Wei Zhou*

Main category: cs.AI

TL;DR: 文章提出GoldMine OS架构，用AI代理实现实物黄金代币化和交易，实验表明其满足性能和安全要求，还探讨对传统非流动资产的影响及治理模式。


<details>
  <summary>Details</summary>
Motivation: 解决现实世界另类资产去中心化交易中，连接实物资产托管与区块链系统，满足合规、流动性和风险管理要求的问题。

Method: 提出GoldMine OS架构，结合链上智能合约和链下AI代理，使用四个合作代理和协调核心，通过模拟和受控试点部署评估系统。

Result: 原型能在1.2秒内按需发行代币，做市代理维持低利差，故障注入测试显示有韧性，基准测试可扩展到每秒5000笔交易和10000个并发用户。

Conclusion: 基于AI代理的另类资产去中心化交易可满足严格的性能和安全要求，治理模式能提供透明度、适应性和系统完整性保证。

Abstract: Decentralized trading of real-world alternative assets (e.g., gold) requires
bridging physical asset custody with blockchain systems while meeting strict
requirements for compliance, liquidity, and risk management. We present
GoldMine OS, a research oriented architecture that employs multiple specialized
AI agents to automate and secure the tokenization and exchange of physical gold
into a blockchain based stablecoin ("OZ"). Our approach combines on chain smart
contracts for critical risk controls with off chain AI agents for decision
making, blending the transparency and reliability of blockchains with the
flexibility of AI driven automation. We describe four cooperative agents
(Compliance, Token Issuance, Market Making, and Risk Control) and a
coordinating core, and evaluate the system through simulation and a controlled
pilot deployment. In experiments the prototype delivers on demand token
issuance in under 1.2 s, more than 100 times faster than manual workflows. The
Market Making agent maintains tight liquidity with spreads often below 0.5
percent even under volatile conditions. Fault injection tests show resilience:
an oracle price spoofing attack is detected and mitigated within 10 s, and a
simulated vault mis reporting halts issuance immediately with minimal user
impact. The architecture scales to 5000 transactions per second with 10000
concurrent users in benchmarks. These results indicate that an AI agent based
decentralized exchange for alternative assets can satisfy rigorous performance
and safety requirements. We discuss broader implications for democratizing
access to traditionally illiquid assets and explain how our governance model --
multi signature agent updates and on chain community voting on risk parameters
-- provides ongoing transparency, adaptability, and formal assurance of system
integrity.

</details>


### [23] [Defining neurosymbolic AI](https://arxiv.org/abs/2507.11127)
*Lennert De Smet,Luc De Raedt*

Main category: cs.AI

TL;DR: 提出神经符号AI的形式化定义，此定义抽象了关键要素并涵盖代表性系统。


<details>
  <summary>Details</summary>
Motivation: 当前神经符号AI领域缺乏被广泛接受的形式化定义。

Method: 将神经符号推理定义为对逻辑函数和信念函数乘积的积分计算。

Result: 所提出的神经符号AI定义抽象了关键代表性神经符号AI系统。

Conclusion: 成功给出神经符号AI的形式化定义。

Abstract: Neurosymbolic AI focuses on integrating learning and reasoning, in
particular, on unifying logical and neural representations. Despite the
existence of an alphabet soup of neurosymbolic AI systems, the field is lacking
a generally accepted formal definition of what neurosymbolic models and
inference really are. We introduce a formal definition for neurosymbolic AI
that makes abstraction of its key ingredients. More specifically, we define
neurosymbolic inference as the computation of an integral over a product of a
logical and a belief function. We show that our neurosymbolic AI definition
makes abstraction of key representative neurosymbolic AI systems.

</details>


### [24] [Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety](https://arxiv.org/abs/2507.11473)
*Tomek Korbak,Mikita Balesni,Elizabeth Barnes,Yoshua Bengio,Joe Benton,Joseph Bloom,Mark Chen,Alan Cooney,Allan Dafoe,Anca Dragan,Scott Emmons,Owain Evans,David Farhi,Ryan Greenblatt,Dan Hendrycks,Marius Hobbhahn,Evan Hubinger,Geoffrey Irving,Erik Jenner,Daniel Kokotajlo,Victoria Krakovna,Shane Legg,David Lindner,David Luan,Aleksander Mądry,Julian Michael,Neel Nanda,Dave Orr,Jakub Pachocki,Ethan Perez,Mary Phuong,Fabien Roger,Joshua Saxe,Buck Shlegeris,Martín Soto,Eric Steinberger,Jasmine Wang,Wojciech Zaremba,Bowen Baker,Rohin Shah,Vlad Mikulik*

Main category: cs.AI

TL;DR: 可监测AI思维链（CoT）以保障安全，虽有不足但有前景，建议进一步研究与投资，开发者考虑对CoT可监测性的影响。


<details>
  <summary>Details</summary>
Motivation: 利用以人类语言‘思考’的AI系统，通过监测其思维链来保障AI安全。

Method: 对AI的思维链进行监测。

Result: CoT监测虽不完美，会有部分不当行为未被发现，但有一定前景。

Conclusion: 建议进一步研究CoT可监测性并投资CoT监测，与现有安全方法并行，开发者考虑开发决策对CoT可监测性的影响。

Abstract: AI systems that "think" in human language offer a unique opportunity for AI
safety: we can monitor their chains of thought (CoT) for the intent to
misbehave. Like all other known AI oversight methods, CoT monitoring is
imperfect and allows some misbehavior to go unnoticed. Nevertheless, it shows
promise and we recommend further research into CoT monitorability and
investment in CoT monitoring alongside existing safety methods. Because CoT
monitorability may be fragile, we recommend that frontier model developers
consider the impact of development decisions on CoT monitorability.

</details>


### [25] [Collaborative Trustworthiness for Good Decision Making in Autonomous Systems](https://arxiv.org/abs/2507.11135)
*Selma Saidi,Omar Laimona,Christoph Schmickler,Dirk Ziegenbein*

Main category: cs.AI

TL;DR: 提出一种协作方法提升自主系统可靠性与决策能力，利用系统质量属性确定可信度，用BDD建模并制定约简规则。


<details>
  <summary>Details</summary>
Motivation: 自主系统在动态复杂环境中确保安全正确行为面临挑战，存在冲突信息时聚合是决策难题。

Method: 利用自主系统不同质量属性确定可信度，借鉴社会认识论定义聚合和传播规则，用BDDs建模并制定约简规则。

Result: 文中未提及具体结果。

Conclusion: 文中未明确总结性内容。

Abstract: Autonomous systems are becoming an integral part of many application domains,
like in the mobility sector. However, ensuring their safe and correct behaviour
in dynamic and complex environments remains a significant challenge, where
systems should autonomously make decisions e.g., about manoeuvring. We propose
in this paper a general collaborative approach for increasing the level of
trustworthiness in the environment of operation and improve reliability and
good decision making in autonomous system. In the presence of conflicting
information, aggregation becomes a major issue for trustworthy decision making
based on collaborative data sharing. Unlike classical approaches in the
literature that rely on consensus or majority as aggregation rule, we exploit
the fact that autonomous systems have different quality attributes like
perception quality. We use this criteria to determine which autonomous systems
are trustworthy and borrow concepts from social epistemology to define
aggregation and propagation rules, used for automated decision making. We use
Binary Decision Diagrams (BDDs) as formal models for beliefs aggregation and
propagation, and formulate reduction rules to reduce the size of the BDDs and
allow efficient computation structures for collaborative automated reasoning.

</details>


### [26] [Fine-grained Timing Analysis of Digital Integrated Circuits in Answer Set Programming](https://arxiv.org/abs/2507.11150)
*Alessandro Bertagnon,Marcello Dalpasso,Michele Favalli,Marco Gavanelli*

Main category: cs.AI

TL;DR: 论文聚焦集成电路组合模块最大延迟计算，指出传统方法不足，用ASP建模解决实际最大延迟计算难题，实验证明ASP可行。


<details>
  <summary>Details</summary>
Motivation: 传统静态时序分析计算最大延迟上限会导致处理器速度欠佳，错失性能提升机会，需计算实际最大延迟。

Method: 将计算实际最大延迟问题用Answer Set Programming (ASP) 建模，并给出非平凡编码。

Result: 实验表明ASP可用于解决硬件设计中的复杂问题。

Conclusion: ASP是解决硬件设计中复杂问题的可行方案。

Abstract: In the design of integrated circuits, one critical metric is the maximum
delay introduced by combinational modules within the circuit. This delay is
crucial because it represents the time required to perform a computation: in an
Arithmetic-Logic Unit it represents the maximum time taken by the circuit to
perform an arithmetic operation. When such a circuit is part of a larger,
synchronous system, like a CPU, the maximum delay directly impacts the maximum
clock frequency of the entire system. Typically, hardware designers use Static
Timing Analysis to compute an upper bound of the maximum delay because it can
be determined in polynomial time. However, relying on this upper bound can lead
to suboptimal processor speeds, thereby missing performance opportunities. In
this work, we tackle the challenging task of computing the actual maximum
delay, rather than an approximate value. Since the problem is computationally
hard, we model it in Answer Set Programming (ASP), a logic language featuring
extremely efficient solvers. We propose non-trivial encodings of the problem
into ASP. Experimental results show that ASP is a viable solution to address
complex problems in hardware design.

</details>


### [27] [DuetGraph: Coarse-to-Fine Knowledge Graph Reasoning with Dual-Pathway Global-Local Fusion](https://arxiv.org/abs/2507.11229)
*Jin Li,Zezhong Ding,Xike Xie*

Main category: cs.AI

TL;DR: 现有KG推理方法存在得分过平滑问题，提出DuetGraph机制解决该问题，实验证明其达SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有KG推理方法存在得分过平滑问题，模糊了正确与错误答案的区别，阻碍推理有效性。

Method: 提出DuetGraph，将局部和全局信息处理分离到两条不同路径，避免相互干扰；引入粗到细优化，划分实体子集缩小候选空间。

Result: 在多个数据集上实验，DuetGraph达SOTA性能，推理质量提升8.7%，训练效率加速1.8倍。

Conclusion: DuetGraph能有效解决得分过平滑问题，提升推理质量和训练效率。

Abstract: Knowledge graphs (KGs) are vital for enabling knowledge reasoning across
various domains. Recent KG reasoning methods that integrate both global and
local information have achieved promising results. However, existing methods
often suffer from score over-smoothing, which blurs the distinction between
correct and incorrect answers and hinders reasoning effectiveness. To address
this, we propose DuetGraph, a coarse-to-fine KG reasoning mechanism with
dual-pathway global-local fusion. DuetGraph tackles over-smoothing by
segregating -- rather than stacking -- the processing of local (via message
passing) and global (via attention) information into two distinct pathways,
preventing mutual interference and preserving representational discrimination.
In addition, DuetGraph introduces a coarse-to-fine optimization, which
partitions entities into high- and low-score subsets. This strategy narrows the
candidate space and sharpens the score gap between the two subsets, which
alleviates over-smoothing and enhances inference quality. Extensive experiments
on various datasets demonstrate that DuetGraph achieves state-of-the-art (SOTA)
performance, with up to an 8.7% improvement in reasoning quality and a
1.8$\times$ acceleration in training efficiency.

</details>


### [28] [Modeling Code: Is Text All You Need?](https://arxiv.org/abs/2507.11467)
*Daniel Nichols,Konstantinos Parasyris,Harshitha Menon,Brian R. Bartoldson,Giorgis Georgakoudis,Tal Ben-Nun,Abhinav Bhatele*

Main category: cs.AI

TL;DR: 介绍结合代码文本与结构化形式建模优势的新方法


<details>
  <summary>Details</summary>
Motivation: Transformer 模型在推理代码结构分析属性上能力有限，此前结合结构化数据和图神经网络的方法缺乏现代大语言模型的生成能力和规模

Method: 提出一种结合代码文本和更结构化形式建模优势的新方法

Result: 未提及

Conclusion: 未提及

Abstract: Code LLMs have become extremely popular recently for modeling source code
across a variety of tasks, such as generation, translation, and summarization.
However, transformer-based models are limited in their capabilities to reason
through structured, analytical properties of code, such as control and data
flow. Previous work has explored the modeling of these properties with
structured data and graph neural networks. However, these approaches lack the
generative capabilities and scale of modern LLMs. In this work, we introduce a
novel approach to combine the strengths of modeling both code as text and more
structured forms.

</details>


### [29] [Taming Uncertainty via Automation: Observing, Analyzing, and Optimizing Agentic AI Systems](https://arxiv.org/abs/2507.11277)
*Dany Moshkovich,Sergey Zeltyn*

Main category: cs.AI

TL;DR: 本文介绍AgentOps框架用于观测、分析、优化和自动化智能体AI系统操作，强调自动化管理不确定性的作用。


<details>
  <summary>Details</summary>
Motivation: 大语言模型驱动的智能体系统带来独特不确定性，传统软件可观测性和运维实践无法应对这些挑战。

Method: 识别开发者、测试人员、站点可靠性工程师和业务用户四个关键角色的不同需求，提出AgentOps自动化流水线的六阶段流程。

Result: 提出了AgentOps框架和自动化流水线。

Conclusion: 自动化在管理不确定性和实现自我改进的AI系统中起关键作用，要驯服不确定性以确保系统安全、自适应和有效运行。

Abstract: Large Language Models (LLMs) are increasingly deployed within agentic
systems-collections of interacting, LLM-powered agents that execute complex,
adaptive workflows using memory, tools, and dynamic planning. While enabling
powerful new capabilities, these systems also introduce unique forms of
uncertainty stemming from probabilistic reasoning, evolving memory states, and
fluid execution paths. Traditional software observability and operations
practices fall short in addressing these challenges.
  This paper introduces AgentOps: a comprehensive framework for observing,
analyzing, optimizing, and automating operation of agentic AI systems. We
identify distinct needs across four key roles-developers, testers, site
reliability engineers (SREs), and business users-each of whom engages with the
system at different points in its lifecycle. We present the AgentOps Automation
Pipeline, a six-stage process encompassing behavior observation, metric
collection, issue detection, root cause analysis, optimized recommendations,
and runtime automation. Throughout, we emphasize the critical role of
automation in managing uncertainty and enabling self-improving AI systems-not
by eliminating uncertainty, but by taming it to ensure safe, adaptive, and
effective operation.

</details>


### [30] [Opus: A Prompt Intention Framework for Complex Workflow Generation](https://arxiv.org/abs/2507.11288)
*Théo Fagnoni,Mahsun Altin,Chia En Chung,Phillip Kingston,Alan Tuning,Dana O. Mohamed,Inès Adnani*

Main category: cs.AI

TL;DR: 本文介绍Opus Prompt Intention Framework以提升大语言模型的复杂工作流生成能力，实验证明该框架有效。


<details>
  <summary>Details</summary>
Motivation: 提高指令调优大语言模型进行复杂工作流生成的能力。

Method: 在用户查询和工作流生成之间引入意图捕获层，实现Opus工作流意图框架，包括从用户查询中提取工作流信号、解释为结构化工作流意图对象并生成工作流。

Result: 该层使大语言模型能产生逻辑且有意义的输出，在1000个多意图查询 - 工作流对的合成基准测试中，应用该框架在语义工作流相似性指标上有持续改进。

Conclusion: 所提出的系统相比直接从用户查询生成工作流，显著提高了工作流生成质量，尤其在混合意图引出的情况下。

Abstract: This paper introduces the Opus Prompt Intention Framework, designed to
improve complex Workflow Generation with instruction-tuned Large Language
Models (LLMs). We propose an intermediate Intention Capture layer between user
queries and Workflow Generation, implementing the Opus Workflow Intention
Framework, which consists of extracting Workflow Signals from user queries,
interpreting them into structured Workflow Intention objects, and generating
Workflows based on these Intentions. Our results show that this layer enables
LLMs to produce logical and meaningful outputs that scale reliably as query
complexity increases. On a synthetic benchmark of 1,000 multi-intent
query-Workflow(s) pairs, applying the Opus Prompt Intention Framework to
Workflow Generation yields consistent improvements in semantic Workflow
similarity metrics. In this paper, we introduce the Opus Prompt Intention
Framework by applying the concepts of Workflow Signal and Workflow Intention to
LLM-driven Workflow Generation. We present a reproducible, customizable
LLM-based Intention Capture system to extract Workflow Signals and Workflow
Intentions from user queries. Finally, we provide empirical evidence that the
proposed system significantly improves Workflow Generation quality compared to
direct generation from user queries, particularly in cases of Mixed Intention
Elicitation.

</details>


### [31] [Contestability in Quantitative Argumentation](https://arxiv.org/abs/2507.11323)
*Xiang Yin,Nico Potyka,Antonio Rago,Timotheus Kampik,Francesca Toni*

Main category: cs.AI

TL;DR: 本文展示如何使用EW - QBAFs支持可竞争性AI，提出梯度关系归因解释和迭代算法解决权重调整问题并实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 可竞争性AI需决策符合人类偏好，EW - QBAFs在支持可竞争性方面受关注少，需研究其应用。

Method: 提出梯度关系归因解释（G - RAEs）量化主题论点强度对边权重变化的敏感性，基于G - RAEs开发迭代算法调整边权重。

Result: 在模拟个性化推荐系统和多层感知器结构特征的合成EW - QBAFs上实验，证明方法能有效解决问题。

Conclusion: EW - QBAFs可用于支持可竞争性AI，所提方法能有效调整边权重以实现所需论点强度。

Abstract: Contestable AI requires that AI-driven decisions align with human
preferences. While various forms of argumentation have been shown to support
contestability, Edge-Weighted Quantitative Bipolar Argumentation Frameworks
(EW-QBAFs) have received little attention. In this work, we show how EW-QBAFs
can be deployed for this purpose. Specifically, we introduce the contestability
problem for EW-QBAFs, which asks how to modify edge weights (e.g., preferences)
to achieve a desired strength for a specific argument of interest (i.e., a
topic argument). To address this problem, we propose gradient-based relation
attribution explanations (G-RAEs), which quantify the sensitivity of the topic
argument's strength to changes in individual edge weights, thus providing
interpretable guidance for weight adjustments towards contestability. Building
on G-RAEs, we develop an iterative algorithm that progressively adjusts the
edge weights to attain the desired strength. We evaluate our approach
experimentally on synthetic EW-QBAFs that simulate the structural
characteristics of personalised recommender systems and multi-layer
perceptrons, and demonstrate that it can solve the problem effectively.

</details>


### [32] [CogDDN: A Cognitive Demand-Driven Navigation with Decision Optimization and Dual-Process Thinking](https://arxiv.org/abs/2507.11334)
*Yuehao Huang,Liang Liu,Shuangming Lei,Yukai Ma,Hao Su,Jianbiao Mei,Pengxiang Zhao,Yaqing Gu,Yong Liu,Jiajun Lv*

Main category: cs.AI

TL;DR: 提出基于VLM的CogDDN框架，结合快慢思考系统和双过程决策模块，在AI2Thor模拟器上评估显示其导航准确性和适应性显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动的需求驱动导航（DDN）方法依赖预收集数据，泛化能力有限，难以适应未知场景。

Method: 提出CogDDN框架，集成快慢思考系统，通过语义对齐识别目标对象，采用双过程决策模块和Chain of Thought（CoT）推理。

Result: 在AI2Thor模拟器上使用ProcThor数据集进行评估，CogDDN比单视图仅相机方法性能高15%。

Conclusion: CogDDN在导航准确性和适应性上有显著改进。

Abstract: Mobile robots are increasingly required to navigate and interact within
unknown and unstructured environments to meet human demands. Demand-driven
navigation (DDN) enables robots to identify and locate objects based on
implicit human intent, even when object locations are unknown. However,
traditional data-driven DDN methods rely on pre-collected data for model
training and decision-making, limiting their generalization capability in
unseen scenarios. In this paper, we propose CogDDN, a VLM-based framework that
emulates the human cognitive and learning mechanisms by integrating fast and
slow thinking systems and selectively identifying key objects essential to
fulfilling user demands. CogDDN identifies appropriate target objects by
semantically aligning detected objects with the given instructions.
Furthermore, it incorporates a dual-process decision-making module, comprising
a Heuristic Process for rapid, efficient decisions and an Analytic Process that
analyzes past errors, accumulates them in a knowledge base, and continuously
improves performance. Chain of Thought (CoT) reasoning strengthens the
decision-making process. Extensive closed-loop evaluations on the AI2Thor
simulator with the ProcThor dataset show that CogDDN outperforms single-view
camera-only methods by 15%, demonstrating significant improvements in
navigation accuracy and adaptability. The project page is available at
https://yuehaohuang.github.io/CogDDN/.

</details>


### [33] [Foundation Models for Logistics: Toward Certifiable, Conversational Planning Interfaces](https://arxiv.org/abs/2507.11352)
*Yunhao Yang,Neel P. Bhatt,Christian Ellis,Alvaro Velasquez,Zhangyang Wang,Ufuk Topcu*

Main category: cs.AI

TL;DR: 介绍一种神经符号框架用于物流决策，轻量级模型表现超GPT - 4.1且降低推理延迟，为复杂物流决策提供实用途径。


<details>
  <summary>Details</summary>
Motivation: 现有物流规划方法中，整数规划慢且未考虑不确定性，大语言模型易误判和幻觉，需要更好的方法。

Method: 引入神经符号框架，将用户请求转为结构化规划规范，量化不确定性，信心不足时进行交互澄清。

Result: 仅用100个过滤不确定性的示例微调的轻量级模型超越GPT - 4.1的零样本性能，推理延迟降低近50%。

Conclusion: 该框架为复杂物流的可认证、实时和用户对齐决策指出了实用路径。

Abstract: Logistics operators, from battlefield coordinators rerouting airlifts ahead
of a storm to warehouse managers juggling late trucks, often face life-critical
decisions that demand both domain expertise and rapid and continuous
replanning. While popular methods like integer programming yield logistics
plans that satisfy user-defined logical constraints, they are slow and assume
an idealized mathematical model of the environment that does not account for
uncertainty. On the other hand, large language models (LLMs) can handle
uncertainty and promise to accelerate replanning while lowering the barrier to
entry by translating free-form utterances into executable plans, yet they
remain prone to misinterpretations and hallucinations that jeopardize safety
and cost. We introduce a neurosymbolic framework that pairs the accessibility
of natural-language dialogue with verifiable guarantees on goal interpretation.
It converts user requests into structured planning specifications, quantifies
its own uncertainty at the field and token level, and invokes an interactive
clarification loop whenever confidence falls below an adaptive threshold. A
lightweight model, fine-tuned on just 100 uncertainty-filtered examples,
surpasses the zero-shot performance of GPT-4.1 while cutting inference latency
by nearly 50%. These preliminary results highlight a practical path toward
certifiable, real-time, and user-aligned decision-making for complex logistics.

</details>


### [34] [Perspective-Aware AI in Extended Reality](https://arxiv.org/abs/2507.11479)
*Daniel Platnick,Matti Gruener,Marjan Alirezaie,Kent Larson,Dava J. Newman,Hossein Rahnama*

Main category: cs.AI

TL;DR: 文章提出PAiR框架，将视角感知AI与XR结合，介绍架构，通过场景验证其效用，为人类与AI交互开辟新方向。


<details>
  <summary>Details</summary>
Motivation: 当前AI增强的扩展现实（XR）系统因用户建模浅和认知上下文有限，无法提供自适应、沉浸式体验。

Method: 引入PAiR框架，基于从多模态数字足迹学习的Chronicles身份模型构建PAi，并在闭环系统中将动态用户状态与沉浸式环境连接，详细介绍架构和系统流程。

Result: 通过在基于Unity的OpenDome引擎中实现的两个概念验证场景，证明了PAiR的效用。

Conclusion: PAiR通过将基于视角的身份模型嵌入沉浸式系统，为人类与AI交互开辟了新方向。

Abstract: AI-enhanced Extended Reality (XR) aims to deliver adaptive, immersive
experiences-yet current systems fall short due to shallow user modeling and
limited cognitive context. We introduce Perspective-Aware AI in Extended
Reality (PAiR), a foundational framework for integrating Perspective-Aware AI
(PAi) with XR to enable interpretable, context-aware experiences grounded in
user identity. PAi is built on Chronicles: reasoning-ready identity models
learned from multimodal digital footprints that capture users' cognitive and
experiential evolution. PAiR employs these models in a closed-loop system
linking dynamic user states with immersive environments. We present PAiR's
architecture, detailing its modules and system flow, and demonstrate its
utility through two proof-of-concept scenarios implemented in the Unity-based
OpenDome engine. PAiR opens a new direction for human-AI interaction by
embedding perspective-based identity models into immersive systems.

</details>


### [35] [Illuminating the Three Dogmas of Reinforcement Learning under Evolutionary Light](https://arxiv.org/abs/2507.11482)
*Mani Hamidi,Terrence W. Deacon*

Main category: cs.AI

TL;DR: 文章提出受开放式进化理论启发的框架，重新审视强化学习的三个核心原则，并探讨其在生物学习模型中的应用及代理问题的解决方向。


<details>
  <summary>Details</summary>
Motivation: 强化学习的三个核心原则需进行概念修订，对理论和应用有重大影响，因此要重新审视这些原则。

Method: 提出受开放式进化理论启发的框架，重新审视每个假设并解决相关问题，还结合生命起源理论探讨代理问题。

Result: 进化范式能丰富学习观点、阐释奖励假设相关辩论，但不能单独解决代理问题。

Conclusion: 应整合生命起源理论的观点，为理解生物系统中的代理和资源受限的强化学习提供基础。

Abstract: Three core tenets of reinforcement learning (RL)--concerning the definition
of agency, the objective of learning, and the scope of the reward
hypothesis--have been highlighted as key targets for conceptual revision, with
major implications for theory and application. We propose a framework, inspired
by open-ended evolutionary theory, to reconsider these three "dogmas." We
revisit each assumption and address related concerns raised alongside them. To
make our arguments relevant to RL as a model of biological learning, we first
establish that evolutionary dynamics can plausibly operate within living brains
over an individual's lifetime, and are not confined to cross-generational
processes. We begin by revisiting the second dogma, drawing on evolutionary
insights to enrich the "adaptation-rather-than-search" view of learning. We
then address the third dogma regarding the limits of the reward hypothesis,
using analogies from evolutionary fitness to illuminate the scalar reward vs.
multi-objective debate. After discussing practical implications for exploration
in RL, we turn to the first--and arguably most fundamental--issue: the absence
of a formal account of agency. We argue that unlike the other two problems, the
evolutionary paradigm alone cannot resolve the agency question, though it
gestures in a productive direction. We advocate integrating ideas from
origins-of-life theory, where the thermodynamics of sustenance and replication
offer promising foundations for understanding agency and resource-constrained
reinforcement learning in biological systems.

</details>


### [36] [How Many Instructions Can LLMs Follow at Once?](https://arxiv.org/abs/2507.11538)
*Daniel Jaroslawicz,Brendan Whiting,Parth Shah,Karime Maamari*

Main category: cs.AI

TL;DR: 提出IFScale基准测试来衡量大语言模型在高指令密度下的指令遵循性能，评估20个模型，揭示性能下降模式等，开源基准和结果。


<details>
  <summary>Details</summary>
Motivation: 现有基准仅评估单条或少量指令任务，生产级大语言模型系统需同时遵循大量指令，要刻画其在高指令密度下的指令遵循能力。

Method: 引入包含500条关键词包含指令的IFScale基准测试来衡量模型随指令密度增加时指令遵循性能的下降情况，并评估7大供应商的20个模型。

Result: 即使最佳前沿模型在500条指令的最大密度下准确率仅68%，分析揭示了模型大小和推理能力与3种不同性能下降模式、对早期指令的偏向及不同类型的指令遵循错误相关。

Conclusion: 研究结果有助于指导现实应用中指令密集型提示的设计，凸显了重要的性能 - 延迟权衡，开源基准和结果供进一步分析。

Abstract: Production-grade LLM systems require robust adherence to dozens or even
hundreds of instructions simultaneously. However, the instruction-following
capabilities of LLMs at high instruction densities have not yet been
characterized, as existing benchmarks only evaluate models on tasks with a
single or few instructions. We introduce IFScale, a simple benchmark of 500
keyword-inclusion instructions for a business report writing task to measure
how instruction-following performance degrades as instruction density
increases. We evaluate 20 state-of-the-art models across seven major providers
and find that even the best frontier models only achieve 68% accuracy at the
max density of 500 instructions. Our analysis reveals model size and reasoning
capability to correlate with 3 distinct performance degradation patterns, bias
towards earlier instructions, and distinct categories of instruction-following
errors. Our insights can help inform design of instruction-dense prompts in
real-world applications and highlight important performance-latency tradeoffs.
We open-source the benchmark and all results for further analysis at
https://distylai.github.io/IFScale.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [37] [Three-dimensional SPH modeling of brittle fracture under hydrodynamic loading](https://arxiv.org/abs/2507.10553)
*Vishabjeet Singh,Chong Peng,Md Rushdie Ibne Islam*

Main category: cs.CE

TL;DR: 提出用于模拟流固耦合的三维SPH计算框架，经与现有模型和实验数据对比，证明其准确且稳健。


<details>
  <summary>Details</summary>
Motivation: 建立能模拟结构变形和失效的流固耦合三维SPH计算框架。

Method: 结合弱可压缩SPH和基于伪弹簧的SPH求解器，用统一建模方法处理边界和界面，用δ - SPH技术改进压力计算，用伪弹簧方法模拟结构损伤。

Result: 框架能捕捉三维裂纹表面，无需复杂的裂纹追踪算法或可见性准则。

Conclusion: 框架有效，在模拟详细断裂模式方面有高准确性和鲁棒性，能为水动力事件对结构完整性的影响提供见解。

Abstract: A three-dimensional SPH computational framework is presented for modeling
fluid-structure interactions with structural deformation and failure. We
combine weakly compressible SPH with a pseudo-spring-based SPH solver to
capture the fluid flow and deformable structures. A unified modeling approach
captures the solid boundaries and fluid-structure interfaces without
penalty-based contact force. The $\delta$-SPH technique improves the pressure
calculations in the fluid phase, while structural damage is modeled using a
pseudo-spring approach, with particle interactions limited to its neighbors.
The present framework can capture the three-dimensional crack surfaces in
structures without any computationally intensive crack-tracking algorithm or
visibility criteria. The framework has been proven effective against existing
models and experimental data, demonstrating high accuracy and robustness in
simulating detailed fracture patterns and offering insights into the impact of
hydrodynamic events on structural integrity.

</details>


### [38] [The Multiple Time-Stepping Method for 3-Body Interactions in High Performance Molecular Dynamics Simulations](https://arxiv.org/abs/2507.11172)
*David Martin,Samuel James Newcome,Markus Mühlhäußer,Manish Kumar Mishra,Fabio Alexander Gratl,Hans-Joachim Bungartz*

Main category: cs.CE

TL;DR: 论文聚焦提升含二体和三体相互作用的分子动力学模拟效率，研究HPC并行计算方法，提出新的共享内存并行截断方法。


<details>
  <summary>Details</summary>
Motivation: 传统二体势难以全面描述分子系统复杂性，三体相互作用虽重要但计算成本高，需提升分子动力学模拟效率。

Method: 采用r - RESPA多时间步算法减少三体相互作用计算次数，研究文献中的减少通信的分布式内存并行方法，提出在AutoPas粒子模拟库中实现的共享内存并行截断方法。

Result: 文中讨论了结果和方法，但未详细提及具体结果。

Conclusion: 为提升分子动力学模拟效率提供了潜在的改进方向。

Abstract: Understanding the complex behavior of molecular systems is fundamental to
fields such as physics, materials science, and biology. Molecular dynamics (MD)
simulations are crucial tools for studying atomic-level dynamics. This work
focuses on improving the efficiency of MD simulations involving two-body and
three-body interactions. Traditional two-body potentials often can not fully
capture the complexity of molecular systems, making the inclusion of three-body
interactions important. However, these interactions are in a cubic complexity
class, compared to a quadratic one for two-body interactions, and therefore are
computationally expensive, even when a cutoff distance is applied. One way to
improve efficiency is to use the r-RESPA multiple time-stepping algorithm to
reduce the number of three-body interaction calculations. In this work, we
investigate this method in the context of High Performance Computing (HPC)
methods that parallelize the calculations. In particular, we investigate a
communication-reducing distributed-memory parallel method from literature and
present a novel shared-memory parallel cutoff method, implemented in the
particle simulation library AutoPas. The results and methods are discussed,
providing insights into potential advancements in MD simulation efficiency.

</details>


### [39] [Data-Driven Differential Evolution in Tire Industry Extrusion: Leveraging Surrogate Models](https://arxiv.org/abs/2507.11191)
*Eider Garate-Perez,Kerman López de Calle-Etxabe,Susana Ferreiro*

Main category: cs.CE

TL;DR: 本文提出基于代理模型的数据驱动方法优化复杂制造系统，应用于轮胎挤出工艺，效果良好。


<details>
  <summary>Details</summary>
Motivation: 解决无目标函数和约束数学公式时工业过程优化难题。

Method: 用机器学习模型近似系统行为构建代理模型，集成到改进的差分进化算法（数据驱动差分进化与多级惩罚函数及代理模型）中。

Result: 基于代理模型的优化方法优于历史最佳配置，初始化和设置时间减少65%，显著降低材料浪费。

Conclusion: 数据驱动建模与元启发式优化结合在无显式公式的工业过程中有应用潜力。

Abstract: The optimization of industrial processes remains a critical challenge,
particularly when no mathematical formulation of objective functions or
constraints is available. This study addresses this issue by proposing a
surrogate-based, data-driven methodology for optimizing complex real-world
manufacturing systems using only historical process data. Machine learning
models are employed to approximate system behavior and construct surrogate
models, which are integrated into a tailored metaheuristic approach:
Data-Driven Differential Evolution with Multi-Level Penalty Functions and
Surrogate Models, an adapted version of Differential Evolution suited to the
characteristics of the studied process. The methodology is applied to an
extrusion process in the tire manufacturing industry, with the goal of
optimizing initialization parameters to reduce waste and production time.
Results show that the surrogate-based optimization approach outperforms
historical best configurations, achieving a 65\% reduction in initialization
and setup time, while also significantly minimizing material waste. These
findings highlight the potential of combining data-driven modeling and
metaheuristic optimization for industrial processes where explicit formulations
are unavailable.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [40] [SQLord: A Robust Enterprise Text-to-SQL Solution via Reverse Data Generation and Workflow Decomposition](https://arxiv.org/abs/2507.10629)
*Song Cheng,Qiannan Cheng,Linbo Jin,Lei Yi,Guannan Zhang*

Main category: cs.DB

TL;DR: 提出企业级NL2SQL框架SQLord，解决现有框架问题，测试效果好且已成功应用。


<details>
  <summary>Details</summary>
Motivation: 现有NL2SQL框架在处理复杂业务逻辑和微调时缺乏特定领域数据，评估方法在现实中难以实现。

Method: 引入数据反向生成方法用于监督微调，提出复杂查询分解方法，构建综合GPT - Judge评估框架。

Result: 离线测试显著优于现有基线，在线准确率超90%。

Conclusion: SQLord在复杂现实场景中具有优势和有效性，已在大型B2B电商平台多场景成功应用。

Abstract: Transforming natural language into SQL queries (NL2SQL) is crucial for
data-driven business applications. Existing frameworks, trained on open-source
datasets, struggle with complex business logic and lack domain-specific data
for fine-tuning. Additionally, evaluation methods often require annotated data
and executable database environments, which are scarce in real-world scenarios.
To address these challenges, we propose SQLord, an enterprise-level NL2SQL
framework. First, SQLord introduces a data reverse generation approach to
convert raw SQL statements into annotated data for supervised fine-tuning
(SFT). Second, it proposes a decomposition method for complex queries using an
automated workflow generator. Additionally, SQLord features a comprehensive
GPT-Judge evaluation framework, including Execution Evaluation (EXE), Query-SQL
Evaluation (QSE), and SQL-SQL Evaluation (SSE), tailored to diverse scenarios.
Offline tests significantly outperform state of the art baselines, and online
accuracy consistently exceeds 90, highlighting SQLord's advantages and
effectiveness in complex real world scenarios. SQLord has been successfully
applied across multiple scenarios on the world's largest B2B e-commerce
platform.

</details>


### [41] [LLMATCH: A Unified Schema Matching Framework with Large Language Models](https://arxiv.org/abs/2507.10897)
*Sha Wang,Yuchen Li,Hanhua Xiao,Bing Tian Dai,Roy Ka-Wei Lee,Yanfei Dong,Lambert Deng*

Main category: cs.DB

TL;DR: 提出LLMatch框架和SchemaNet基准，提升复杂模式匹配准确性和工程师生产力。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以处理复杂多表模式匹配问题，需要新方案。

Method: 将模式匹配分解为三阶段，采用两阶段优化策略，推出SchemaNet基准。

Result: LLMatch显著提高复杂模式匹配准确性，提升工程师生产力。

Conclusion: LLMatch和SchemaNet在复杂模式匹配和实际数据集成中有良好效果。

Abstract: Schema matching is a foundational task in enterprise data integration, aiming
to align disparate data sources. While traditional methods handle simple
one-to-one table mappings, they often struggle with complex multi-table schema
matching in real-world applications. We present LLMatch, a unified and modular
schema matching framework. LLMatch decomposes schema matching into three
distinct stages: schema preparation, table-candidate selection, and
column-level alignment, enabling component-level evaluation and future-proof
compatibility. It includes a novel two-stage optimization strategy: a Rollup
module that consolidates semantically related columns into higher-order
concepts, followed by a Drilldown module that re-expands these concepts for
fine-grained column mapping. To address the scarcity of complex semantic
matching benchmarks, we introduce SchemaNet, a benchmark derived from
real-world schema pairs across three enterprise domains, designed to capture
the challenges of multi-table schema alignment in practical settings.
Experiments demonstrate that LLMatch significantly improves matching accuracy
in complex schema matching settings and substantially boosts engineer
productivity in real-world data integration.

</details>


### [42] [Towards Practical Benchmarking of Data Cleaning Techniques: On Generating Authentic Errors via Large Language Models](https://arxiv.org/abs/2507.10934)
*Xinyuan Liu,Jiahui Chen,Bocheng Hu,Yu Sun,Xinyang Chen,Shaoxu Song*

Main category: cs.DB

TL;DR: 提出TableEG框架用大语言模型生成真实错误，经实验验证其优势并可作为后续任务基准。


<details>
  <summary>Details</summary>
Motivation: 数据质量是数据驱动系统挑战，缺乏多样真实错误数据集且手动标注有问题，需探索合成错误生成。

Method: 引入TableEG框架，采用表微调策略和三元组表示建模，在12个真实数据集上训练。

Result: TableEG生成的错误在模式和分布相似度上优于其他方法，性能指标与真实错误接近。

Conclusion: TableEG弥合合成与真实错误差距，为后续错误检测和纠正任务建立强大基准。

Abstract: Data quality remains an important challenge in data-driven systems, as errors
in tabular data can severely compromise downstream analytics and machine
learning performance. Although numerous error detection algorithms have been
proposed, the lack of diverse, real-world error datasets limits comprehensive
evaluation. Manual error annotation is both time-consuming and inconsistent,
motivating the exploration of synthetic error generation as an alternative. In
this work, we introduce TableEG, a framework that leverages large language
models (LLMs) to generate authentic errors. By employing a table fine-tuning
strategy and a triplet representation $(I, T, O)$ to model error generation,
detection, and correction tasks, TableEG captures the complex dependencies
inherent in two-dimensional tables. Trained on 12 real-world datasets spanning
10 diverse domains, TableEG ensures that the synthesized errors faithfully
reflect authentic error distributions. Experimental results indicate that
errors generated by TableEG exhibit superior pattern and distribution
similarity compared to both rule-based methods and LLM-generated errors without
fine-tuning. Furthermore, performance metrics on TableEG-generated errors
closely align with those on real-world errors across nearly all datasets and
detection algorithms, particularly for machine learning based detection
techniques. Overall, TableEG not only bridges the gap between synthetic and
real-world errors but also establishes a robust benchmark for subsequent error
detection and correction tasks.

</details>


### [43] [TOPJoin: A Context-Aware Multi-Criteria Approach for Joinable Column Search](https://arxiv.org/abs/2507.11505)
*Harsha Kokel,Aamod Khatiwada,Tejaswini Pedapati,Haritha Ananthakrishnan,Oktie Hassanzadeh,Horst Samulowitz,Kavitha Srinivas*

Main category: cs.DB

TL;DR: 文章指出企业数据分析中寻找可连接表的挑战，传统列相似度方法不足，提出TOPJoin方法，实验表明其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 企业数据湖中仅靠列相似度无法有效识别可连接列和表，需要考虑查询列的上下文。

Method: 先定义上下文感知的列可连接性，再提出多标准方法TOPJoin进行可连接列搜索。

Result: 在一个学术和一个真实世界的连接搜索基准上进行实验，TOPJoin比现有基线表现更好。

Conclusion: TOPJoin方法在可连接列搜索上具有优势，能更有效地处理企业数据湖中的数据连接问题。

Abstract: One of the major challenges in enterprise data analysis is the task of
finding joinable tables that are conceptually related and provide meaningful
insights. Traditionally, joinable tables have been discovered through a search
for similar columns, where two columns are considered similar syntactically if
there is a set overlap or they are considered similar semantically if either
the column embeddings or value embeddings are closer in the embedding space.
However, for enterprise data lakes, column similarity is not sufficient to
identify joinable columns and tables. The context of the query column is
important. Hence, in this work, we first define context-aware column
joinability. Then we propose a multi-criteria approach, called TOPJoin, for
joinable column search. We evaluate TOPJoin against existing join search
baselines over one academic and one real-world join search benchmark. Through
experiments, we find that TOPJoin performs better on both benchmarks than the
baselines.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [44] [FAFO: Over 1 million TPS on a single node running EVM while still Merkleizing every block](https://arxiv.org/abs/2507.10757)
*Ryan Zarick,Isaac Zhang,Daniel Wong,Thomas Kim,Bryan Pellegrino,Mignon Li,Kelvin Wong*

Main category: cs.DC

TL;DR: FAFO是首个解决区块链数据争用问题的交易调度器，通过预排序交易实现高并发，集成REVM后实现高吞吐量，成本低，支持轻客户端和无状态验证，开源可用。


<details>
  <summary>Details</summary>
Motivation: 当前区块链执行吞吐量受数据争用限制，降低了执行层并行性，需要解决该问题。

Method: 使用CPU优化的缓存友好布隆过滤器检测冲突，对交易预排序；集成Rust EVM客户端（REVM）；使用QMDB对世界状态进行默克尔化。

Result: 在单节点上实现每秒超110万次原生ETH转账和超50万次ERC20转账，成本比现有分片执行低91%。

Conclusion: 通过精简执行层和创新交易调度器设计，可实现支持未来去中心化应用所需的高吞吐量。

Abstract: Current blockchain execution throughput is limited by data contention,
reducing execution layer parallelism. Fast Ahead-of-Formation Optimization
(FAFO) is the first blockchain transaction scheduler to address this problem by
reordering transactions before block formation for maximum concurrency. FAFO
uses CPU-optimized cache-friendly Bloom filters to efficiently detect conflicts
and schedule parallel transaction execution at high throughput and low
overhead.
  We integrate the Rust EVM client (REVM) into FAFO and achieve over 1.1
million native ETH transfers per second and over half a million ERC20 transfers
per second on a single node (Table 1), with 91% lower cost compared to
state-of-the-art sharded execution. Unlike many other existing high throughput
blockchain execution clients, FAFO uses QMDB to Merkleize world state after
every block, enabling light clients and stateless validation for ZK-based
vApps. FAFO scales with minimal synchronization overhead, scaling linearly with
additional CPU resources until it fully exploits the maximum parallelism of the
underlying transaction flow. FAFO proves that the high throughput necessary to
support future decentralized applications can be achieved with a streamlined
execution layer and innovations in blockchain transaction scheduler design.
FAFO is open-sourced at https://github.com/LayerZero-Labs/fafo.

</details>


### [45] [Dissecting the NVIDIA Blackwell Architecture with Microbenchmarks](https://arxiv.org/abs/2507.10789)
*Aaron Jarmusch,Nathan Graddon,Sunita Chandrasekaran*

Main category: cs.DC

TL;DR: 本文对NVIDIA Blackwell架构进行微架构分析，与Hopper架构对比，研究其性能特征、功耗等，为开发者提供优化见解。


<details>
  <summary>Details</summary>
Motivation: 科研发展对计算能力需求增加，部分由GPU解决，需对现代NVIDIA Blackwell架构进行微架构分析。

Method: 通过精心设计的微基准测试研究GPU性能特征，对比Blackwell与Hopper架构，还研究不同工作负载下的功耗。

Result: 揭示关键子系统，呈现代际改进和性能倒退情况，研究了不同关键特性和功耗。

Conclusion: 研究结果为应用开发者、编译器编写者和性能工程师提供优化基于Blackwell平台工作负载的见解，为GPU架构研究提供新数据。

Abstract: The rapid development in scientific research provides a need for more compute
power, which is partly being solved by GPUs. This paper presents a
microarchitectural analysis of the modern NVIDIA Blackwell architecture by
studying GPU performance
  features with thought through microbenchmarks. We unveil key subsystems,
including the memory hierarchy, SM execution
  pipelines, and the SM sub-core units, including the 5th generation tensor
cores supporting FP4 and FP6 precisions.
  To understand the different key features of the NVIDIA GPU, we study latency,
throughput, cache behavior, and scheduling
  details, revealing subtle tuning metrics in the design of Blackwell. To
develop a comprehensive analysis, we compare the
  Blackwell architecture with the previous Hopper architecture by using the
GeForce RTX 5080 and H100 PCIe, respectively. We
  evaluate and compare results, presenting both generational improvements and
performance regressions. Additionally, we
  investigate the role of power efficiency and energy consumption under varied
workloads. Our findings provide actionable insights
  for application developers, compiler writers, and performance engineers to
optimize workloads on Blackwell-based platforms,
  and contribute new data to the growing research on GPU architectures.

</details>


### [46] [MMStencil: Optimizing High-order Stencils on Multicore CPU using Matrix Unit](https://arxiv.org/abs/2507.11067)
*Yinuo Wang,Tianqi Mao,Lin Gan,Wubing Wan,Zeyu Song,Jiayu Fu,Lanke He,Wenqiang Wang,Zekun Yin,Wei Xue,Guangwen Yang*

Main category: cs.DC

TL;DR: 本文针对3D高阶模板计算，提出MMStencil方法，结合多种优化策略，提升硬件利用率，相比现有库和工业版本有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 矩阵加速模板计算在3D高阶模板和HPC应用研究不足，随着多核CPU矩阵单元出现，需探索合适加速策略。

Method: 引入基于SIMD和矩阵单元的算法优化、内存优化和多线程并行范式，采用基于DMA的跨NUMA通信。

Result: MMStencil在不同模板形状和维度上保持高硬件利用率，比Nvidia A100 GPGPU上的现有库性能高2.1倍，RTM应用比高度优化的工业版本快1.8倍。

Conclusion: 所提优化策略有效，MMStencil在3D高阶模板计算和HPC应用中表现优异，能带来显著性能提升。

Abstract: Matrix-accelerated stencil computation is a hot research topic, yet its
application to three-dimensional (3D) high-order stencils and HPC remains
underexplored. With the emergence of matrix units on multicore CPUs, we analyze
matrix-based acceleration strategies and tailor an optimal approach for 3D
high-order stencils. We introduce algorithmic optimizations based on SIMD and
matrix units to address strided memory accesses, alignment conflicts, and
redundant accesses. We propose memory optimizations to boost on-package memory
efficiency, and a novel multi-thread parallelism paradigm to overcome
data-sharing challenges caused by the absence of shared data caches. MMStencil
sustains consistently high hardware utilization across diverse stencil shapes
and dimensions. Our DMA-based inter-NUMA communication further mitigates NUMA
effects and MPI limitations in hybrid parallelism. Combining all the
innovations, MMStencil outperforms state-of-the-art libraries on Nvidia A100
GPGPU by up to 2.1x. Moreover, the performance improvements translate directly
to real-world HPC applications and enable RTM applications to yield 1.8x
speedup versus a highly optimized industrial Nvidia A100 GPGPU version.

</details>


### [47] [Generating Dynamic Graph Algorithms for Multiple Backends for a Graph DSL](https://arxiv.org/abs/2507.11094)
*Nibedita Behera,Ashwina Kumar,Atharva Chougule,Mohammed Shan P S,Rushabh Nirdosh Lalwani,Rupesh Nasre*

Main category: cs.DC

TL;DR: 随着非结构化和半结构化数据增长，图算法并行化困难，本文为动态图算法处理提出抽象方案和运行时优化。


<details>
  <summary>Details</summary>
Motivation: 现有框架处理静态图有局限，生成动态图算法高效且同步正确的代码是重大挑战。

Method: 引入抽象方案和运行时优化，通过DSL表达动态处理逻辑，自动生成并行代码。

Result: 将DSL生成的代码应用于十个大图和三个常用算法进行验证。

Conclusion: 所提方法在处理动态图算法方面有效。

Abstract: With the rapid growth of unstructured and semistructured data, parallelizing
graph algorithms has become essential for efficiency. However, due to the
inherent irregularity in computation, memory access patterns, and
communication, graph algorithms are notoriously difficult to parallelize. To
address this challenge, several libraries, frameworks, and domain-specific
languages (DSLs) have been proposed to ease the parallel programming burden for
domain experts. Existing frameworks partially or fully abstract away
parallelism intricacies, provide intuitive scheduling mnemonics, and employ
program analysis to identify data races and generate synchronization code.
Despite these advances, most frameworks are limited in their abstractions and
runtime optimizations, especially when dealing with static graphs. In contrast,
many real-world graphs are inherently dynamic, with evolving structures over
time through insertions, deletions, and modifications of vertices, edges, and
attributes. Generating efficient and correctly synchronized code for such
dynamic graph algorithms remains a significant challenge.
  In this work, we introduce an abstraction scheme and runtime optimizations
for the efficient processing of morph algorithms. Specifically, given an
initial graph G and a set of updates $\Delta$G involving edge insertions and
deletions, we express the dynamic processing logic through a DSL and
automatically generate parallel code targeting multicore, distributed, and
many-core environments. We demonstrate the effectiveness of our approach by
applying the DSL-generated code to ten large graphs with diverse
characteristics and three widely used algorithms: Shortest Paths, PageRank, and
Triangle Counting.

</details>


### [48] [Boosting Scientific Error-Bounded Lossy Compression through Optimized Synergistic Lossy-Lossless Orchestration](https://arxiv.org/abs/2507.11165)
*Shixun Wu,Jinwen Pan,Jinyang Liu,Jiannan Tian,Ziwei Qiu,Jiajun Huang,Kai Zhao,Xin Liang,Sheng Di,Zizhong Chen,Franck Cappello*

Main category: cs.DC

TL;DR: 提出基于GPU的高比率科学误差有界有损压缩器cuSZ - Hi，优化预测方案、探索无损编码技术并评估，压缩比提升显著。


<details>
  <summary>Details</summary>
Motivation: 高性能计算架构发展，科学计算工作流产生大量数据，急需高比率、低延迟误差有界的数据压缩方案。

Method: 1. 在GPU上优化基于插值的数据预测方案；2. 探索无损数据编码技术，构建最佳无损编码管道；3. 用基准数据集和代表性基线评估cuSZ - Hi。

Result: 与现有GPU上的高比率科学误差有界有损压缩器相比，吞吐量相当或更好，在相同误差边界下压缩比提升达249%，在相同解压缩数据PSNR下压缩比提升达215%。

Conclusion: cuSZ - Hi是一种优化的、具有灵活开源框架设计的高比率GPU科学误差有界有损压缩器，压缩性能出色。

Abstract: As high-performance computing architectures evolve, more scientific computing
workflows are being deployed on advanced computing platforms such as GPUs.
These workflows can produce raw data at extremely high throughputs, requiring
urgent high-ratio and low-latency error-bounded data compression solutions. In
this paper, we propose cuSZ-Hi, an optimized high-ratio GPU-based scientific
error-bounded lossy compressor with a flexible, domain-irrelevant, and fully
open-source framework design. Our novel contributions are: 1) We maximally
optimize the parallelized interpolation-based data prediction scheme on GPUs,
enabling the full functionalities of interpolation-based scientific data
prediction that are adaptive to diverse data characteristics; 2) We thoroughly
explore and investigate lossless data encoding techniques, then craft and
incorporate the best-fit lossless encoding pipelines for maximizing the
compression ratio of cuSZ-Hi; 3) We systematically evaluate cuSZ-Hi on
benchmarking datasets together with representative baselines. Compared to
existing state-of-the-art scientific lossy compressors, with comparative or
better throughput than existing high-ratio scientific error-bounded lossy
compressors on GPUs, cuSZ-Hi can achieve up to 249% compression ratio
improvement under the same error bound, and up to 215% compression ratio
improvement under the same decompression data PSNR.

</details>


### [49] [Cyclic Data Streaming on GPUs for Short Range Stencils Applied to Molecular Dynamics](https://arxiv.org/abs/2507.11289)
*Martin Rose,Simon Homes,Lukas Ramsperger,Jose Gracia,Christoph Niethammer,Jadran Vrabec*

Main category: cs.DC

TL;DR: 提出依赖GPU高带宽通信的框架，可实现显式算法性能线性扩展，以分子动力学模拟为例，强缩放表现优于LAMMPS。


<details>
  <summary>Details</summary>
Motivation: 追求科学计算的最高性能。

Method: 构建依赖GPU高带宽通信的框架，让数据集切片在GPU进程环中传播实现并行计算，用户只需编写GPU内核并提供数据集切片。

Result: 以基于Lennard - Jones势的分子动力学模拟为案例，该框架在单节点性能和强缩放行为对比中，强缩放表现优于LAMMPS。

Conclusion: 所提出的框架在科学计算中具有良好性能，能实现显式算法性能的线性扩展，且无需用户了解底层并行策略。

Abstract: In the quest for highest performance in scientific computing, we present a
novel framework that relies on high-bandwidth communication between GPUs in a
compute cluster. The framework offers linear scaling of performance for
explicit algorithms that is only limited by the size of the dataset and the
number of GPUs. Slices of the dataset propagate in a ring of processes (GPUs)
from one GPU, where they are processed, to the next, which results in a
parallel-in-time parallelization. The user of the framework has to write GPU
kernels that implement the algorithm and provide slices of the dataset.
Knowledge about the underlying parallelization strategy is not required because
the communication between processes is carried out by the framework. As a case
study, molecular dynamics simulation based on the Lennard-Jones potential is
implemented to measure the performance for a homogeneous fluid. Single node
performance and strong scaling behavior of this framework is compared to
LAMMPS, which is outperformed in the strong scaling case.

</details>


### [50] [Scaling the memory wall using mixed-precision -- HPG-MxP on an exascale machine](https://arxiv.org/abs/2507.11512)
*Aditya Kashi,Nicholson Koukpaizan,Hao Lu,Michael Matheson,Sarp Oral,Feiyi Wang*

Main category: cs.DC

TL;DR: 本文介绍了混合精度算法在HPC系统上的应用，提出HPG - MxP基准测试，实现其在百亿亿次级系统的优化并取得1.6倍加速。


<details>
  <summary>Details</summary>
Motivation: 多数科学模拟应用受内存带宽限制，混合精度算法在HPC系统上的实际收益不明，需要评估其在基于稀疏矩阵的应用中的性能。

Method: 对HPG - MxP基准测试进行高度优化实现，并进行算法增强。

Result: 在现代基于GPU的超级计算机上，结合双精度和单精度实现了1.6倍的加速。

Conclusion: 优化后的HPG - MxP基准测试可有效评估HPC系统在稀疏矩阵混合精度应用中的性能，混合精度算法有实际加速效果。

Abstract: Mixed-precision algorithms have been proposed as a way for scientific
computing to benefit from some of the gains seen for artificial intelligence
(AI) on recent high performance computing (HPC) platforms. A few applications
dominated by dense matrix operations have seen substantial speedups by
utilizing low precision formats such as FP16. However, a majority of scientific
simulation applications are memory bandwidth limited. Beyond preliminary
studies, the practical gain from using mixed-precision algorithms on a given
HPC system is largely unclear.
  The High Performance GMRES Mixed Precision (HPG-MxP) benchmark has been
proposed to measure the useful performance of a HPC system on sparse
matrix-based mixed-precision applications. In this work, we present a highly
optimized implementation of the HPG-MxP benchmark for an exascale system and
describe our algorithm enhancements. We show for the first time a speedup of
1.6x using a combination of double- and single-precision on modern GPU-based
supercomputers.

</details>


### [51] [A new Dune grid for scalable dynamic adaptivity based on the p4est software library](https://arxiv.org/abs/2507.11386)
*Carsten Burstedde,Mikhail Kirilin,Robert Klöfkorn*

Main category: cs.DC

TL;DR: 本文将Dune求解器库与p4est软件进行新的网格接口扩展，对比已有实现，新实现扩展性更优，并提出替代平衡策略。


<details>
  <summary>Details</summary>
Motivation: 继承p4est的无限MPI可扩展性、较薄的数据结构和对多块网格拓扑的原生支持。

Method: 将Dune求解器库与p4est软件进行新的网格接口扩展，并在并行环境中对不同挑战性测试用例与基于Dune - ALUGrid的现有实现进行对比。

Result: 新实现的可扩展性优于Dune - ALUGrid，提出的替代平衡策略在性能上优于现有p4est平衡策略。

Conclusion: 新的Dune与p4est的耦合实现具有更好的可扩展性，替代平衡策略表现更优。

Abstract: In this work we extend the Dune solver library with another grid interface to
the open-source p4est software. While Dune already supports about a dozen
different mesh implementations through its mesh interface Dune-Grid, we
undertake this new coupling effort in order to inherit p4est's practically
unlimited MPI scalability as well as its relatively thin data structures, and
its native support for multi-block (forest) mesh topologies in both 2D and 3D.
  The presented implementation is compared to an existing implementation based
on Dune-ALUGrid for a variety of challenging test examples in a parallel
environment. The numerical experiments show that the implementation presented
here is outperforming Dune-ALUGrid in terms of scalability. In addition, an
alternative balancing strategy is presented to ensure 2:1 balancing across
element faces showing improved performance compared to the existing p4est
balance strategy in the numerical examples considered in this work.

</details>


### [52] [Quantifying the Energy Consumption and Carbon Emissions of LLM Inference via Simulations](https://arxiv.org/abs/2507.11417)
*Miray Özcan,Philipp Wiesner,Philipp Weiß,Odej Kao*

Main category: cs.DC

TL;DR: 现有模拟框架无法准确估计大语言模型推理碳排放，本文提出新模拟框架评估不同部署下大语言模型推理的能源和碳排放影响，展示了推理参数对能源需求和碳足迹的影响及可再生能源抵消潜力。


<details>
  <summary>Details</summary>
Motivation: 现有模拟框架缺乏功率概念，无法准确估计大语言模型推理相关碳排放，需新框架评估其能源和碳排放影响。

Method: 先为高保真大语言模型推理模拟器添加基于利用率指标估计功耗的GPU功率模型；再将模拟输出集成到能源系统联合模拟环境中，量化特定电网条件下的碳排放并探索碳感知调度潜力。

Result: 框架揭示推理参数对能源需求和碳足迹的影响，在示例部署中展示了高达69.2%的可再生能源抵消潜力。

Conclusion: 该框架为未来碳感知推理基础设施设计奠定基础。

Abstract: The environmental impact of Large Language Models (LLMs) is rising
significantly, with inference now accounting for more than half of their total
lifecycle carbon emissions. However, existing simulation frameworks, which are
increasingly used to determine efficient LLM deployments, lack any concept of
power and, therefore, cannot accurately estimate inference-related emissions.
We present a simulation framework to assess the energy and carbon implications
of LLM inference under varying deployment setups. First, we extend a
high-fidelity LLM inference simulator with a GPU power model that estimates
power consumption based on utilization metrics, enabling analysis across
configurations like batch size, sequence length, and model parallelism. Second,
we integrate simulation outputs into an energy system co-simulation environment
to quantify carbon emissions under specific grid conditions and explore the
potential of carbon-aware scheduling. Through scenario-based analysis, our
framework reveals how inference parameters affect energy demand and carbon
footprint, demonstrates a renewable offset potential of up to 69.2% in an
illustrative deployment case, and provides a foundation for future carbon-aware
inference infrastructure design.

</details>


### [53] [FLsim: A Modular and Library-Agnostic Simulation Framework for Federated Learning](https://arxiv.org/abs/2507.11430)
*Arnab Mukherjee,Raju Halder,Joydeep Chandra*

Main category: cs.DC

TL;DR: 介绍了联邦学习（FL）发展现状及研究挑战，提出FLsim框架，展示其特性、功能，经实验验证其有效性和多功能性，认为其是FL模拟框架的重要进展。


<details>
  <summary>Details</summary>
Motivation: 当前研究和基准测试新的FL技术面临挑战，为简化该过程。

Method: 引入具有模块化、可扩展性等特点的FLsim框架，用户可通过作业配置指定定制化FL需求。

Result: 通过一系列实验评估，证明了FLsim在模拟各种先进FL实验方面的有效性和多功能性。

Conclusion: FLsim是FL模拟框架的重要进步，为研究人员和从业者提供了前所未有的灵活性和功能性。

Abstract: Federated Learning (FL) has undergone significant development since its
inception in 2016, advancing from basic algorithms to complex methodologies
tailored to address diverse challenges and use cases. However, research and
benchmarking of novel FL techniques against a plethora of established
state-of-the-art solutions remain challenging. To streamline this process, we
introduce FLsim, a comprehensive FL simulation framework designed to meet the
diverse requirements of FL workflows in the literature. FLsim is characterized
by its modularity, scalability, resource efficiency, and controlled
reproducibility of experimental outcomes. Its easy to use interface allows
users to specify customized FL requirements through job configuration, which
supports: (a) customized data distributions, ranging from non-independent and
identically distributed (non-iid) data to independent and identically
distributed (iid) data, (b) selection of local learning algorithms according to
user preferences, with complete agnosticism to ML libraries, (c) choice of
network topology illustrating communication patterns among nodes, (d)
definition of model aggregation and consensus algorithms, and (e) pluggable
blockchain support for enhanced robustness. Through a series of experimental
evaluations, we demonstrate the effectiveness and versatility of FLsim in
simulating a diverse range of state-of-the-art FL experiments. We envisage that
FLsim would mark a significant advancement in FL simulation frameworks,
offering unprecedented flexibility and functionality for researchers and
practitioners alike.

</details>


### [54] [Uniting the World by Dividing it: Federated Maps to Enable Spatial Applications](https://arxiv.org/abs/2507.11437)
*Sagar Bharadwaj,Srinivasan Seshan,Anthony Rowe*

Main category: cs.DC

TL;DR: 本文指出空间网络缺乏空间命名系统，现有集中式地图基础设施不足，提出联邦空间命名系统并探讨相关服务。


<details>
  <summary>Details</summary>
Motivation: 空间网络发展缺少空间命名系统，现有集中式地图无法满足新兴应用需求。

Method: 提出联邦空间命名系统，即联邦映射基础设施，使不同方管理和服务自有地图。

Result: 可实现地图管理的可扩展性、地图的隔离和隐私保护。

Conclusion: 需重新架构地图相关服务以适用于联邦地图，并讨论了启用这些服务的基本服务和实际操作。

Abstract: The emergence of the Spatial Web -- the Web where content is tied to
real-world locations has the potential to improve and enable many applications
such as augmented reality, navigation, robotics, and more. The Spatial Web is
missing a key ingredient that is impeding its growth -- a spatial naming system
to resolve real-world locations to names. Today's spatial naming systems are
digital maps such as Google and Apple maps. These maps and the location-based
services provided on top of these maps are primarily controlled by a few large
corporations and mostly cover outdoor public spaces. Emerging classes of
applications, such as persistent world-scale augmented reality, require
detailed maps of both outdoor and indoor spaces. Existing centralized mapping
infrastructures are proving insufficient for such applications because of the
scale of cartography efforts required and the privacy of indoor map data.
  In this paper, we present a case for a federated spatial naming system, or in
other words, a federated mapping infrastructure. This enables disparate parties
to manage and serve their own maps of physical regions and unlocks scalability
of map management, isolation and privacy of maps. Map-related services such as
address-to-location mapping, location-based search, and routing needs
re-architecting to work on federated maps. We discuss some essential services
and practicalities of enabling these services.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [55] [A Fast Coloring Oracle for Average Case Hypergraphs](https://arxiv.org/abs/2507.10691)
*Cassandra Marcussen,Edward Pyne,Ronitt Rubinfeld,Asaf Shapira,Shlomo Tauber*

Main category: cs.DS

TL;DR: 本文提出新的简单确定性2 - 着色算法，避免使用正则引理，还将其转化为平均期望运行时间为O(n)的随机算法，且定义着色预言机并证明其平均能在O(1)时间内回答顶点查询。


<details>
  <summary>Details</summary>
Motivation: 之前解决超图2 - 着色问题的算法依赖正则引理，分析复杂且运行时间含塔型常数，作者希望改进算法。

Method: 设计新的不依赖正则引理的确定性2 - 着色算法，将其转化为随机算法，定义着色预言机。

Result: 新确定性算法重证前人定理，随机算法平均期望运行时间为O(n)，着色预言机平均能在O(1)时间内回答顶点查询。

Conclusion: 找到平均2 - 可着色超图的2 - 着色相对容易。

Abstract: Hypergraph $2$-colorability is one of the classical NP-hard problems. Person
and Schacht [SODA'09] designed a deterministic algorithm whose expected running
time is polynomial over a uniformly chosen $2$-colorable $3$-uniform
hypergraph. Lee, Molla, and Nagle recently extended this to $k$-uniform
hypergraphs for all $k\geq 3$. Both papers relied heavily on the regularity
lemma, hence their analysis was involved and their running time hid tower-type
constants.
  Our first result in this paper is a new simple and elementary deterministic
$2$-coloring algorithm that reproves the theorems of Person-Schacht and
Lee-Molla-Nagle while avoiding the use of the regularity lemma. We also show
how to turn our new algorithm into a randomized one with average expected
running time of only $O(n)$.
  Our second and main result gives what we consider to be the ultimate evidence
of just how easy it is to find a $2$-coloring of an average $2$-colorable
hypergraph. We define a coloring oracle to be an algorithm which, given vertex
$v$, assigns color red/blue to $v$ while inspecting as few edges as possible,
so that the answers to any sequence of queries to the oracle are consistent
with a single legal $2$-coloring of the input. Surprisingly, we show that there
is a coloring oracle that, on average, can answer every vertex query in time
$O(1)$.

</details>


### [56] [Solving Random Planted CSPs below the $n^{k/2}$ Threshold](https://arxiv.org/abs/2507.10833)
*Arpon Basu,Jun-Ting Hsieh,Andrew D. Lin,Peter Manohar*

Main category: cs.DS

TL;DR: 提出解决随机植入k元布尔约束满足问题（CSP）的算法家族，推广前人算法，与反驳随机CSP的权衡匹配，且算法思路不同。


<details>
  <summary>Details</summary>
Motivation: 解决随机植入的k元布尔约束满足问题，推广已有算法到更大运行时间。

Method: 分两步恢复植入赋值x*，先用度为O(ℓ)的平方和半定规划（SDP）找到接近x*的x^，再用舍入过程从x^恢复x*。

Result: 算法运行时间为n^{O(ℓ)}，当m ≥ O(n)·(n/ℓ)^(k/2 - 1) log n时能成功输出满足赋值。

Conclusion: 该算法推广了前人算法，且在思路上与近期算法不同，实现了约束数量和运行时间的权衡。

Abstract: We present a family of algorithms to solve random planted instances of any
$k$-ary Boolean constraint satisfaction problem (CSP). A randomly planted
instance of a Boolean CSP is generated by (1) choosing an arbitrary planted
assignment $x^*$, and then (2) sampling constraints from a particular "planting
distribution" designed so that $x^*$ will satisfy every constraint. Given an
$n$ variable instance of a $k$-ary Boolean CSP with $m$ constraints, our
algorithm runs in time $n^{O(\ell)}$ for a choice of a parameter $\ell$, and
succeeds in outputting a satisfying assignment if $m \geq O(n) \cdot
(n/\ell)^{\frac{k}{2} - 1} \log n$. This generalizes the
$\mathrm{poly}(n)$-time algorithm of [FPV15], the case of $\ell = O(1)$, to
larger runtimes, and matches the constraint number vs.\ runtime trade-off
established for refuting random CSPs by [RRS17].
  Our algorithm is conceptually different from the recent algorithm of
[GHKM23], which gave a $\mathrm{poly}(n)$-time algorithm to solve semirandom
CSPs with $m \geq \tilde{O}(n^{\frac{k}{2}})$ constraints by exploiting
conditions that allow a basic SDP to recover the planted assignment $x^*$
exactly. Instead, we forego certificates of uniqueness and recover $x^*$ in two
steps: we first use a degree-$O(\ell)$ Sum-of-Squares SDP to find some
$\hat{x}$ that is $o(1)$-close to $x^*$, and then we use a second rounding
procedure to recover $x^*$ from $\hat{x}$.

</details>


### [57] [Solving Linear Programs with Differential Privacy](https://arxiv.org/abs/2507.10946)
*Alina Ene,Huy Le Nguyen,Ta Duy Nguyen,Adrian Vladu*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the problem of solving linear programs of the form $Ax\le b$,
$x\ge0$ with differential privacy. For homogeneous LPs $Ax\ge0$, we give an
efficient $(\epsilon,\delta)$-differentially private algorithm which with
probability at least $1-\beta$ finds in polynomial time a solution that
satisfies all but
$O(\frac{d^{2}}{\epsilon}\log^{2}\frac{d}{\delta\beta}\sqrt{\log\frac{1}{\rho_{0}}})$
constraints, for problems with margin $\rho_{0}>0$. This improves the bound of
$O(\frac{d^{5}}{\epsilon}\log^{1.5}\frac{1}{\rho_{0}}\mathrm{poly}\log(d,\frac{1}{\delta},\frac{1}{\beta}))$
by [Kaplan-Mansour-Moran-Stemmer-Tur, STOC '25]. For general LPs $Ax\le b$,
$x\ge0$ with potentially zero margin, we give an efficient
$(\epsilon,\delta)$-differentially private algorithm that w.h.p drops
$O(\frac{d^{4}}{\epsilon}\log^{2.5}\frac{d}{\delta}\sqrt{\log dU})$
constraints, where $U$ is an upper bound for the entries of $A$ and $b$ in
absolute value. This improves the result by Kaplan et al. by at least a factor
of $d^{5}$. Our techniques build upon privatizing a rescaling perceptron
algorithm by [Hoberg-Rothvoss, IPCO '17] and a more refined iterative procedure
for identifying equality constraints by Kaplan et al.

</details>


### [58] [FPT Parameterisations of Fractional and Generalised Hypertree Width](https://arxiv.org/abs/2507.11080)
*Matthias Lanzinger,Igor Razgon,Daniel Unterberger*

Main category: cs.DS

TL;DR: 本文首次提出精确确定几个关键超图分解参数的固定参数易解（fpt）算法，适用于有界秩和有界度的超图类。


<details>
  <summary>Details</summary>
Motivation: 在复杂度理论、数据库和约束满足中，广义超树宽度、分数超树宽度和自适应宽度等度量很重要，但此前没有针对它们的精确fpt算法。

Method: 扩展了最近关于树宽的算法，利用单值二阶（MSO）转换。

Result: 得到了有界秩和有界度超图类的精确fpt算法。

Conclusion: 克服了超图结构分解比图更复杂的技术障碍，提出了有效算法。

Abstract: We present the first fixed-parameter tractable (fpt) algorithms for precisely
determining several central hypergraph decomposition parameters, including
generalized hypertree width, fractional hypertree width, and adaptive width.
Despite the recognized importance of these measures in complexity theory,
databases, and constraint satisfaction, no exact fpt algorithms for any of them
had previously been known. Our results are obtained for hypergraph classes of
bounded rank and bounded degree.
  Our approach extends a recent algorithm for treewidth (Boja\'ncyk &
Pilipczuk, LMCS 2022) utilizing monadic second-order (MSO) transductions.
Leveraging this framework, we overcome the significant technical hurdles
presented by hypergraphs, whose structural decompositions are technically much
more intricate than their graph counterparts.

</details>


### [59] [Faster algorithms for k-Orthogonal Vectors in low dimension](https://arxiv.org/abs/2507.11098)
*Anita Dürr,Evangelos Kipouridis,Karol Węgrzycki*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In the Orthogonal Vectors problem (OV), we are given two families $A, B$ of
subsets of $\{1,\ldots,d\}$, each of size $n$, and the task is to decide
whether there exists a pair $a \in A$ and $b \in B$ such that $a \cap b =
\emptyset$. Straightforward algorithms for this problem run in $\mathcal{O}(n^2
\cdot d)$ or $\mathcal{O}(2^d \cdot n)$ time, and assuming SETH, there is no
$2^{o(d)}\cdot n^{2-\varepsilon}$ time algorithm that solves this problem for
any constant $\varepsilon > 0$.
  Williams (FOCS 2024) presented a $\tilde{\mathcal{O}}(1.35^d \cdot n)$-time
algorithm for the problem, based on the succinct equality-rank decomposition of
the disjointness matrix. In this paper, we present a combinatorial algorithm
that runs in randomized time $\tilde{\mathcal{O}}(1.25^d n)$. This can be
improved to $\mathcal{O}(1.16^d \cdot n)$ using computer-aided evaluations.
  We generalize our result to the $k$-Orthogonal Vectors problem, where given
$k$ families $A_1,\ldots,A_k$ of subsets of $\{1,\ldots,d\}$, each of size $n$,
the task is to find elements $a_i \in A_i$ for every $i \in \{1,\ldots,k\}$
such that $a_1 \cap a_2 \cap \ldots \cap a_k = \emptyset$. We show that for
every fixed $k \ge 2$, there exists $\varepsilon_k > 0$ such that the $k$-OV
problem can be solved in time $\mathcal{O}(2^{(1 - \varepsilon_k)\cdot d}\cdot
n)$. We also show that, asymptotically, this is the best we can hope for: for
any $\varepsilon > 0$ there exists a $k \ge 2$ such that $2^{(1 -
\varepsilon)\cdot d} \cdot n^{\mathcal{O}(1)}$ time algorithm for
$k$-Orthogonal Vectors would contradict the Set Cover Conjecture.

</details>


### [60] [Efficient Branch-and-Bound for Submodular Function Maximization under Knapsack Constraint](https://arxiv.org/abs/2507.11107)
*Yimin Hao,Yi Zhou,Chao Xu,Zhang-Hua Fu*

Main category: cs.DS

TL;DR: 提出解决子模背包问题的最优分支定界法，实验显示新算法比传统方法高效。


<details>
  <summary>Details</summary>
Motivation: 现有解决子模背包问题的方法多为近似算法，但在医疗设施选址和风险管理等领域需要最优解，因此需要精确算法。

Method: 提出最优分支定界法，有带最坏情况紧度保证的新上界和高效双分支法以减少重复计算。

Result: 在设施选址、加权覆盖、影响力最大化等应用的实验表明，实现新思想的算法比传统方法高效得多。

Conclusion: 所提出的最优分支定界法在解决子模背包问题上比传统方法更高效。

Abstract: The submodular knapsack problem (SKP), which seeks to maximize a submodular
set function by selecting a subset of elements within a given budget, is an
important discrete optimization problem. The majority of existing approaches to
solving the SKP are approximation algorithms. However, in domains such as
health-care facility location and risk management, the need for optimal
solutions is still critical, necessitating the use of exact algorithms over
approximation methods. In this paper, we present an optimal branch-and-bound
approach, featuring a novel upper bound with a worst-case tightness guarantee
and an efficient dual branching method to minimize repeat computations.
Experiments in applications such as facility location, weighted coverage,
influence maximization, and so on show that the algorithms that implement the
new ideas are far more efficient than conventional methods.

</details>


### [61] [Finding Order-Preserving Subgraphs](https://arxiv.org/abs/2507.11115)
*Haruya Imamura,Yasuaki Kobayashi,Yota Otachi,Toshiki Saitoh,Keita Sato,Asahi Takaoka,Ryo Yoshinaka*

Main category: cs.DS

TL;DR: 研究有序（诱导）子图同构及最大公共有序（诱导）子图问题，证明其在小图类中NP完全，指出计算复杂度差异及与顶点排序的关系。


<details>
  <summary>Details</summary>
Motivation: 图中顶点存在自然全序，研究尊重此顺序的问题变体。

Method: 对问题在不同图类（如深度为2的树、阈值图、区间图等）进行理论分析和证明。

Result: 证明问题在小图类中NP完全；发现OSI和OISI在某些图类计算复杂度有差距；问题的可处理性依赖顶点排序。

Conclusion: 有序（诱导）子图同构及最大公共有序（诱导）子图问题计算复杂度高，且受图类和顶点排序影响。

Abstract: (Induced) Subgraph Isomorphism and Maximum Common (Induced) Subgraph are
fundamental problems in graph pattern matching and similarity computation. In
graphs derived from time-series data or protein structures, a natural total
ordering of vertices often arises from their underlying structure, such as
temporal sequences or amino acid sequences. This motivates the study of problem
variants that respect this inherent ordering. This paper addresses Ordered
(Induced) Subgraph Isomorphism (O(I)SI) and its generalization, Maximum Common
Ordered (Induced) Subgraph (MCO(I)S), which seek to find subgraph isomorphisms
that preserve the vertex orderings of two given ordered graphs. Our main
contributions are threefold: (1) We prove that these problems remain
NP-complete even when restricted to small graph classes, such as trees of depth
2 and threshold graphs. (2) We establish a gap in computational complexity
between OSI and OISI on certain graph classes. For instance, OSI is
polynomial-time solvable for interval graphs with their interval orderings,
whereas OISI remains NP-complete under the same setting. (3) We demonstrate
that the tractability of these problems can depend on the vertex ordering. For
example, while OISI is NP-complete on threshold graphs, its generalization,
MCOIS, can be solved in polynomial time if the specific vertex orderings that
characterize the threshold graphs are provided.

</details>


### [62] [Improved sampling algorithms and Poincaré inequalities for non-log-concave distributions](https://arxiv.org/abs/2507.11236)
*Yuchen He,Zhehan Lei,Jianan Shao,Chihao Zhang*

Main category: cs.DS

TL;DR: 研究从分布μ采样问题，在假设(1*)和(2)下给出更低查询复杂度采样算法，还得到Poincaré常数估计。


<details>
  <summary>Details</summary>
Motivation: 降低从分布μ采样的查询复杂度，探究光滑性条件加强对查询复杂度的影响。

Method: 基于对势函数V及其梯度的查询访问，在标准假设和加强假设下进行分析。

Result: 在假设(1*)和(2)下，采样查询复杂度为poly(L,d)⋅((Ld + M)/ε²)^O(L + 1)；在假设(1*)和更强矩假设下，Poincaré常数至多为O(λ)^2(L + 1)。

Conclusion: 光滑性条件适度加强会导致采样算法查询复杂度指数级差距，还能得到混合高斯分布Poincaré常数的改进估计。

Abstract: We study the problem of sampling from a distribution $\mu$ with density
$\propto e^{-V}$ for some potential function $V:\mathbb R^d\to \mathbb R$ with
query access to $V$ and $\nabla V$. We start with the following standard
assumptions:
  (1) The potential function $V$ is $L$-smooth.
  (2) The second moment $\mathbf{E}_{X\sim \mu}[\|X\|^2]\leq M$.
  Recently, He and Zhang (COLT'25) showed that the query complexity of sampling
from such distributions is at least
$\left(\frac{LM}{d\epsilon}\right)^{\Omega(d)}$ where $\epsilon$ is the desired
accuracy in total variation distance, and the Poincar\'e constant can be
arbitrarily large.
  Meanwhile, another common assumption in the study of diffusion based samplers
(see e.g., the work of Chen, Chewi, Li, Li, Salim and Zhang (ICLR'23))
strengthens the smoothness condition (1) to the following:
  (1*) The potential function of *every* distribution along the
Ornstein-Uhlenbeck process starting from $\mu$ is $L$-smooth.
  We show that under the assumptions (1*) and (2), the query complexity of
sampling from $\mu$ can be $\mathrm{poly}(L,d)\cdot
\left(\frac{Ld+M}{\epsilon^2}\right)^{\mathcal{O}(L+1)}$, which is polynomial
in $d$ and $\frac{1}{\epsilon}$ when $L=\mathcal{O}(1)$ and
$M=\mathrm{poly}(d)$. This improves the algorithm with quasi-polynomial query
complexity developed by Huang et al. (COLT'24). Our results imply that the
seemly moderate strengthening of the smoothness condition (1) to (1*) can lead
to an exponential gap in the query complexity of sampling algorithms.
  Moreover, we show that together with the assumption (1*) and the stronger
moment assumption that $\|X\|$ is $\lambda$-sub-Gaussian for $X\sim\mu$, the
Poincar\'e constant of $\mu$ is at most $\mathcal{O}(\lambda)^{2(L+1)}$. As an
application of our technique, we obtain improved estimate of the Poincar\'e
constant for mixture of Gaussians with the same covariance.

</details>


### [63] [Fully Dynamic Euclidean k-Means](https://arxiv.org/abs/2507.11256)
*Sayan Bhattacharya,Martín Costa,Ermiya Farokhnejad,Shaofeng H. -C. Jiang,Yaonan Jin,Jianing Lou*

Main category: cs.DS

TL;DR: 提出动态欧几里得k - 均值聚类问题的算法，有近似比、更新时间和追索的良好保证。


<details>
  <summary>Details</summary>
Motivation: 解决动态欧几里得k - 均值聚类问题，最小化近似比、更新时间和追索。

Method: 基于近期工作构建算法，设计多个新颖几何数据结构。

Result: 提出具有poly(1/ϵ)近似比、O~(k^ϵ)更新时间和O~(1)追索的动态算法，在一般情况下各参数有近乎最优保证。

Conclusion: 该算法在各参数上有近乎最优保证，改进更新时间或近似比会优于现有静态算法，追索的下界为Ω(1)。

Abstract: We consider the fundamental Euclidean $k$-means clustering problem in a
dynamic setting, where the input $X \subseteq \mathbb{R}^d$ evolves over time
via a sequence of point insertions/deletions. We have to explicitly maintain a
solution (a set of $k$ centers) $S \subseteq \mathbb{R}^d$ throughout these
updates, while minimizing the approximation ratio, the update time (time taken
to handle a point insertion/deletion) and the recourse (number of changes made
to the solution $S$) of the algorithm.
  We present a dynamic algorithm for this problem with
$\text{poly}(1/\epsilon)$-approximation ratio, $\tilde{O}(k^{\epsilon})$ update
time and $\tilde{O}(1)$ recourse. In the general regime, where the dimension
$d$ cannot be assumed to be a fixed constant, our algorithm has almost optimal
guarantees across all these three parameters. Indeed, improving our update time
or approximation ratio would imply beating the state-of-the-art static
algorithm for this problem (which is widely believed to be the best possible),
and the recourse of any dynamic algorithm must be $\Omega(1)$.
  We obtain our result by building on top of the recent work of [Bhattacharya,
Costa, Farokhnejad; STOC'25], which gave a near-optimal dynamic algorithm for
$k$-means in general metric spaces (as opposed to in the Euclidean setting).
Along the way, we design several novel geometric data structures that are of
independent interest. Specifically, one of our main contributions is designing
the first consistent hashing scheme [Czumaj, Jiang, Krauthgamer, Vesel\'y,
Yang; FOCS'22] that achieves $\text{poly}(d)$ running time per point evaluation
with competitive parameters.

</details>


### [64] [Deterministic Lower Bounds for $k$-Edge Connectivity in the Distributed Sketching Model](https://arxiv.org/abs/2507.11257)
*Peter Robinson,Ming Ming Tan*

Main category: cs.DS

TL;DR: 研究分布式草图模型下无向图的k - 边连通性问题，给出确定性算法的首个下界。


<details>
  <summary>Details</summary>
Motivation: 在分布式草图模型下，针对图连通性问题，此前缺乏确定性算法的下界研究，需要填补这一空白。

Method: 引入新的下界图构造和三方通信复杂度问题UniqueOverlap，利用交叉相交集族结果证明其对确定性算法的难度，通过新颖模拟论证得到下界。

Result: 对于任意超常数k = O(√n)，k - 边连通性的最坏情况消息长度为Ω(k)比特，是该模型下连通性决策问题的首个超多项式对数下界。

Conclusion: 在分布式草图模型中，使用确定性算法决定k - 边连通性有Ω(k)比特的消息长度下界。

Abstract: We study the $k$-edge connectivity problem on undirected graphs in the
distributed sketching model, where we have $n$ nodes and a referee. Each node
sends a single message to the referee based on its 1-hop neighborhood in the
graph, and the referee must decide whether the graph is $k$-edge connected by
taking into account the received messages.
  We present the first lower bound for deciding a graph connectivity problem in
this model with a deterministic algorithm. Concretely, we show that the worst
case message length is $\Omega( k )$ bits for $k$-edge connectivity, for any
super-constant $k = O(\sqrt{n})$. Previously, only a lower bound of $\Omega(
\log^3 n )$ bits was known for ($1$-edge) connectivity, due to Yu (SODA 2021).
In fact, our result is the first super-polylogarithmic lower bound for a
connectivity decision problem in the distributed graph sketching model.
  To obtain our result, we introduce a new lower bound graph construction, as
well as a new 3-party communication complexity problem that we call
UniqueOverlap. As this problem does not appear to be amenable to reductions to
existing hard problems such as set disjointness or indexing due to correlations
between the inputs of the three players, we leverage results from
cross-intersecting set families to prove the hardness of UniqueOverlap for
deterministic algorithms. Finally, we obtain the sought lower bound for
deciding $k$-edge connectivity via a novel simulation argument that, in
contrast to previous works, does not introduce any probability of error and
thus works for deterministic algorithms.

</details>


### [65] [On Tight Robust Coresets for $k$-Medians Clustering](https://arxiv.org/abs/2507.11260)
*Lingxiao Huang,Zhenyu Jiang,Yi Li,Xuan Wu*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper considers coresets for the robust $k$-medians problem with $m$
outliers, and new constructions in various metric spaces are obtained.
Specifically, for metric spaces with a bounded VC or doubling dimension $d$,
the coreset size is $O(m) + \tilde{O}(kd\varepsilon^{-2})$, which is optimal up
to logarithmic factors. For Euclidean spaces, the coreset size is
$O(m\varepsilon^{-1}) +
\tilde{O}(\min\{k^{4/3}\varepsilon^{-2},k\varepsilon^{-3}\})$, improving upon a
recent result by Jiang and Lou (ICALP 2025). These results also extend to
robust $(k,z)$-clustering, yielding, for VC and doubling dimension, a coreset
size of $O(m) + \tilde{O}(kd\varepsilon^{-2z})$ with the optimal linear
dependence on $m$. This extended result improves upon the earlier work of Huang
et al. (SODA 2025). The techniques introduce novel dataset decompositions,
enabling chaining arguments to be applied jointly across multiple components.

</details>


### [66] [Permutation patterns in streams](https://arxiv.org/abs/2507.11291)
*Benjamin Aram Berendsohn*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Permutation patterns and pattern avoidance are central, well-studied concepts
in combinatorics and computer science. Given two permutations $\tau$ and $\pi$,
the pattern matching problem (PPM) asks whether $\tau$ contains $\pi$. This
problem arises in various contexts in computer science and statistics and has
been studied extensively in exact-, parameterized-, approximate-,
property-testing- and other formulations.
  In this paper, we study pattern matching in a \emph{streaming setting}, when
the input $\tau$ is revealed sequentially, one element at a time. There is
extensive work on the space complexity of various statistics in streams of
integers. The novelty of our setting is that the input stream is \emph{a
permutation}, which allows inferring some information about future inputs. Our
algorithms crucially take advantage of this fact, while existing lower bound
techniques become difficult to apply.
  We show that the complexity of the problem changes dramatically depending on
the pattern~$\pi$. The space requirement is: $\Theta(k\log{n})$ for the
monotone patterns $\pi = 12\dots k$, or $\pi = k\dots21$, $O(\sqrt{n\log{n}})$
for $\pi \in \{312,132\}$, $O(\sqrt{n} \log n)$ for $\pi \in \{231,213\}$, and
$\widetilde{\Theta}_{\pi}(n)$ for all other $\pi$. If $\tau$ is an arbitrary
sequence of integers (not necessary a permutation), we show that the complexity
is $\widetilde{\Theta}_{\pi}(n)$ in all except the first (monotone) cases.

</details>


### [67] [Scheduling on Identical Machines with Setup Time and Unknown Execution Time](https://arxiv.org/abs/2507.11311)
*Yasushi Kawase,Kazuhisa Makino,Vinh Long Phan,Hanna Sumita*

Main category: cs.DS

TL;DR: 研究带初始设置的相同机器调度问题，考虑作业分批和不确定性，针对不同场景和设置设计在线算法并实现渐近最优竞争比。


<details>
  <summary>Details</summary>
Motivation: 解决作业执行时间未知情况下最小化完工时间的调度难题。

Method: 考虑作业分批分配到单台或多台机器的两种场景，分析有无抢占的设置，设计在线算法。

Result: 设计的在线算法在作业数量和机器数量方面实现了渐近最优竞争比。

Conclusion: 所设计算法在多种设置下能有效应对带初始设置和不确定性的调度问题。

Abstract: In this study, we investigate a scheduling problem on identical machines in
which jobs require initial setup before execution. We assume that an algorithm
can dynamically form a batch (i.e., a collection of jobs to be processed
together) from the remaining jobs. The setup time is modeled as a known
monotone function of the set of jobs within a batch, while the execution time
of each job remains unknown until completion. This uncertainty poses
significant challenges for minimizing the makespan. We address these challenges
by considering two scenarios: each job batch must be assigned to a single
machine, or a batch may be distributed across multiple machines. For both
scenarios, we analyze settings with and without preemption. Across these four
settings, we design online algorithms that achieve asymptotically optimal
competitive ratios with respect to both the number of jobs and the number of
machines.

</details>


### [68] [Multipass Linear Sketches for Geometric LP-Type Problems](https://arxiv.org/abs/2507.11484)
*N. Efe Çekirge,William Gay,David P. Woodruff*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: LP-type problems such as the Minimum Enclosing Ball (MEB), Linear Support
Vector Machine (SVM), Linear Programming (LP), and Semidefinite Programming
(SDP) are fundamental combinatorial optimization problems, with many important
applications in machine learning applications such as classification,
bioinformatics, and noisy learning. We study LP-type problems in several
streaming and distributed big data models, giving $\varepsilon$-approximation
linear sketching algorithms with a focus on the high accuracy regime with low
dimensionality $d$, that is, when ${d < (1/\varepsilon)^{0.999}}$. Our main
result is an $O(ds)$ pass algorithm with $O(s( \sqrt{d}/\varepsilon)^{3d/s})
\cdot \mathrm{poly}(d, \log (1/\varepsilon))$ space complexity in words, for
any parameter $s \in [1, d \log (1/\varepsilon)]$, to solve
$\varepsilon$-approximate LP-type problems of $O(d)$ combinatorial and VC
dimension. Notably, by taking $s = d \log (1/\varepsilon)$, we achieve space
complexity polynomial in $d$ and polylogarithmic in $1/\varepsilon$, presenting
exponential improvements in $1/\varepsilon$ over current algorithms. We
complement our results by showing lower bounds of $(1/\varepsilon)^{\Omega(d)}$
for any $1$-pass algorithm solving the $(1 + \varepsilon)$-approximation MEB
and linear SVM problems, further motivating our multi-pass approach.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [69] [Protocols for Verifying Smooth Strategies in Bandits and Games](https://arxiv.org/abs/2507.10567)
*Miranda Christ,Daniel Reichman,Jonathan Shafer*

Main category: cs.GT

TL;DR: 研究多臂老虎机和正规形式博弈中策略近似最优性的验证协议，提出查询次数亚线性的协议，证明其可行性并给出下界，还应用于正规形式博弈。


<details>
  <summary>Details</summary>
Motivation: 由于玩家可采取的行动数量通常很大，寻求对效用预言机查询次数为行动数量亚线性的验证协议。

Method: 证明对于足够平滑且不在特定行动上分配过多概率质量的策略，验证是可行的，提供多臂老虎机平滑策略的ε - 最优性验证协议。

Result: 验证协议所需的臂查询次数比学习少，建立了验证查询复杂度的近乎紧的下界，应用于正规形式博弈得到近似强平滑纳什均衡的验证协议，查询复杂度亚线性。

Conclusion: 实现了多臂老虎机和正规形式博弈中策略近似最优性的有效验证，查询复杂度为亚线性。

Abstract: We study protocols for verifying approximate optimality of strategies in
multi-armed bandits and normal-form games. As the number of actions available
to each player is often large, we seek protocols where the number of queries to
the utility oracle is sublinear in the number of actions. We prove that such
verification is possible for sufficiently smooth strategies that do not put too
much probability mass on any specific action. We provide protocols for
verifying that a smooth policy for a multi-armed bandit is
$\varepsilon$-optimal. Our verification protocols require provably fewer arm
queries than learning. Furthermore, we establish a nearly-tight lower bound on
the query complexity of verification in our settings. As an application, we
show how to use verification for bandits to achieve verification in normal-form
games. This gives a protocol for verifying whether a given strategy profile is
an approximate strong smooth Nash equilibrium, with a query complexity that is
sublinear in the number of actions.

</details>


### [70] [Pricing with Tips in Three-Sided Delivery Platforms](https://arxiv.org/abs/2507.10872)
*Yannai A. Gonczarowski,Gary Qiurui Ma,David C. Parkes*

Main category: cs.GT

TL;DR: 构建三方交易的配送平台模型，分析小费在定价中的作用，探讨均衡存在性、福利及计算问题，并给出确保高效均衡存在及可多项式时间计算的条件。


<details>
  <summary>Details</summary>
Motivation: 研究配送平台中三方交易的市场均衡，明确小费在定价和市场效率中的作用。

Method: 构建包含买家、商店和快递员的三方交易模型，分析有小费和无小费情况下的均衡。

Result: 无小费时，仅在快递员数量不少于买家或商店时均衡才保证存在；有小费时均衡总是存在，且最优有小费均衡福利不小于最优无小费均衡福利，但有效均衡可能不存在且计算最优均衡福利是NP难问题。

Conclusion: 找到市场结构的自然条件，确保有小费的有效均衡存在并可在多项式时间内计算。

Abstract: We model a delivery platform facilitating transactions among three sides:
buyers, stores, and couriers. In addition to buyers paying store-specific
purchase prices and couriers receiving store--buyer-specific delivery
compensation from the platform, each buyer has the option to directly tip for
delivery from a specific store. An equilibrium consists of prices,
compensations, tips, and transactions that clear the market, such that buyers
receive deliveries from preferred stores considering the prices and tips they
pay, and couriers deliver preferred orders considering the compensations and
tips they receive.
  We illustrate the role of tips in pricing: Without tips, an equilibrium is
only guaranteed to exist when there are at least as many couriers as buyers or
stores. In contrast, with tips an equilibrium always exists. From an efficiency
perspective, the optimal with-tip equilibrium welfare is always weakly larger
than the optimal without-tip equilibrium welfare. However, we show that even
with tips, efficient equilibria may not exist, and calculating the optimal
equilibrium welfare is NP-hard. To address these challenges, we identify
natural conditions on market structure that ensure the existence of efficient
with-tip equilibria and allow these efficient equilibria to be computed in
polynomial time.

</details>


### [71] [Fair Contracts](https://arxiv.org/abs/2507.11214)
*Matteo Castiglioni,Junjie Chen,Yingkai Li*

Main category: cs.GT

TL;DR: 研究公平约束下最优合同设计问题，分析计算复杂度、设计算法并研究公平代价。


<details>
  <summary>Details</summary>
Motivation: 解决在任务分配和补偿上有公平约束的最优合同设计问题。

Method: 采用无嫉妒（EF）及其松弛概念，考虑代理或任务数量为常数的情况，设计算法求解。

Result: 计算最优EF合同近似是NP难；代理数量固定时设计FPTAS；任务数量固定时给出多项式时间算法；EF合同公平代价可能无界，EF1合同公平代价有界。

Conclusion: 在公平约束下合同设计计算复杂，不同公平概念下公平代价表现不同。

Abstract: We introduce and study the problem of designing optimal contracts under
fairness constraints on the task assignments and compensations. We adopt the
notion of envy-free (EF) and its relaxations, $\epsilon$-EF and envy-free up to
one item (EF1), in contract design settings. Unlike fair allocations, EF
contracts are guaranteed to exist. However, computing any constant-factor
approximation to the optimal EF contract is NP-hard in general, even using
$\epsilon$-EF contracts. For this reason, we consider settings in which the
number of agents or tasks is constant. Notably, while even with three agents,
finding an EF contract better than $2/5$ approximation of the optimal is
NP-hard, we are able to design an FPTAS when the number of agents is constant,
under relaxed notions of $\epsilon$-EF and EF1. Moreover, we present a
polynomial-time algorithm for computing the optimal EF contract when the number
of tasks is constant. Finally, we analyze the price of fairness in contract
design. We show that the price of fairness for exact EF contracts can be
unbounded, even with a single task and two agents. In contrast, for EF1
contracts, the price of fairness is bounded between $\Omega(\sqrt{n})$ and
$O(n^2)$, where $n$ is the number of agents.

</details>


### [72] [A Parallelizable Approach for Characterizing NE in Zero-Sum Games After a Linear Number of Iterations of Gradient Descent](https://arxiv.org/abs/2507.11366)
*Taemin Kim,James P. Bailey*

Main category: cs.GT

TL;DR: 研究零和博弈在线优化方法，提出基于哈密顿动力学新方法，能有限迭代刻画纳什均衡集，可并行且适用于任意学习率，实验显示性能远超标准方法。


<details>
  <summary>Details</summary>
Motivation: 零和博弈是机器学习、经济学等领域的基础问题，传统方法存在局限，需新的在线优化方法。

Method: 提出基于物理学中哈密顿动力学的新方法，通过交替梯度下降迭代。

Result: 能在无界设定下以有限（线性）迭代次数刻画纳什均衡集，可并行且适用于任意学习率，实验中性能远超标准方法。

Conclusion: 新方法在零和博弈在线优化方面表现优异，具有创新性和实用性。

Abstract: We study online optimization methods for zero-sum games, a fundamental
problem in adversarial learning in machine learning, economics, and many other
domains. Traditional methods approximate Nash equilibria (NE) using either
regret-based methods (time-average convergence) or contraction-map-based
methods (last-iterate convergence). We propose a new method based on
Hamiltonian dynamics in physics and prove that it can characterize the set of
NE in a finite (linear) number of iterations of alternating gradient descent in
the unbounded setting, modulo degeneracy, a first in online optimization.
Unlike standard methods for computing NE, our proposed approach can be
parallelized and works with arbitrary learning rates, both firsts in
algorithmic game theory. Experimentally, we support our results by showing our
approach drastically outperforms standard methods.

</details>


### [73] [Better Regret Rates in Bilateral Trade via Sublinear Budget Violation](https://arxiv.org/abs/2507.11419)
*Anna Lunghi,Matteo Castiglioni,Alberto Marchesi*

Main category: cs.GT

TL;DR: 本文研究双边贸易中后悔率与全局预算平衡约束违反程度的权衡，设计算法并给出匹配下界，证明前人结果的紧性。


<details>
  <summary>Details</summary>
Motivation: 前人研究在双边贸易中以无悔学习算法设计交易机制，受预算平衡约束限制，本文探讨不同预算平衡约束违反程度下最优后悔率的变化。

Method: 设计一个允许预算平衡约束最多违反 $T^{\beta}$（$\beta \in [\frac{3}{4}, \frac{6}{7}]$）的算法，并给出匹配的下界。

Result: 算法达到 $\tilde O(T^{1 - \beta/3})$ 的后悔率，有匹配下界。

Conclusion: 前人在全局预算平衡情况的 $\tilde O(T^{3/4})$ 上界和无约束预算平衡违反下的 $\Omega(T^{5/7})$ 下界是紧的。

Abstract: Bilateral trade is a central problem in algorithmic economics, and recent
work has explored how to design trading mechanisms using no-regret learning
algorithms. However, no-regret learning is impossible when budget balance has
to be enforced at each time step. Bernasconi et al. [Ber+24] show how this
impossibility can be circumvented by relaxing the budget balance constraint to
hold only globally over all time steps. In particular, they design an algorithm
achieving regret of the order of $\tilde O(T^{3/4})$ and provide a lower bound
of $\Omega(T^{5/7})$.
  In this work, we interpolate between these two extremes by studying how the
optimal regret rate varies with the allowed violation of the global budget
balance constraint. Specifically, we design an algorithm that, by violating the
constraint by at most $T^{\beta}$ for any given $\beta \in [\frac{3}{4},
\frac{6}{7}]$, attains regret $\tilde O(T^{1 - \beta/3})$. We complement this
result with a matching lower bound, thus fully characterizing the trade-off
between regret and budget violation. Our results show that both the $\tilde
O(T^{3/4})$ upper bound in the global budget balance case and the
$\Omega(T^{5/7})$ lower bound under unconstrained budget balance violation
obtained by Bernasconi et al. [Ber+24] are tight.

</details>


### [74] [On the Complexity of the Optimal Correlated Equilibria in Extensive-Form Games](https://arxiv.org/abs/2507.11509)
*Vincent Cheval,Florian Horn,Soumyajit Paul,Mahsa Shirmohammadi*

Main category: cs.GT

TL;DR: 本文研究扩展式博弈中均衡计算的阈值问题，证明多人完美回忆扩展式博弈中正规形式相关均衡阈值问题为PSPACE难，纳什均衡阈值问题为ER完全，还解决了AFCE阈值问题的复杂度（NP难），并对多种相关均衡概念的阈值问题给出紧复杂度分类。


<details>
  <summary>Details</summary>
Motivation: 算法博弈论中一个重要问题是能否在扩展式等简洁博弈中高效计算正规形式相关均衡，受此驱动研究相关阈值问题。

Method: 通过理论证明来确定各种均衡阈值问题的复杂度。

Result: 证明多人完美回忆扩展式博弈中NFCE阈值问题PSPACE难，纳什均衡阈值问题ER完全；AFCE阈值问题NP难；对多种相关均衡概念阈值问题证明NP完全。

Conclusion: 为扩展式博弈中最优均衡计算复杂度提供了迄今最完整的图景。

Abstract: A major open question in algorithmic game theory is whether normal-form
correlated equilibria (NFCE) can be computed efficiently in succinct games such
as extensive-form games [DFF+25,6PR24,FP23,HvS08,VSF08,PR08]. Motivated by this
question, we study the associated Threshold problem: deciding whether there
exists a correlated equilibrium whose value exceeds a given threshold. We prove
that this problem is PSPACE-hard for NFCE in multiplayer extensive-form games
with perfect recall, even for fixed thresholds. To contextualize this result,
we also establish the complexity of the Threshold problem for Nash equilibria
in this setting, showing it is ER-complete. These results uncover a surprising
complexity reversal: while optimal correlated equilibria are computationally
simpler than optimal Nash in normal-form games, the opposite holds in
extensive-form games, where computing optimal correlated equilibria is provably
harder. Building on this line of inquiry, we also address a related question by
[VSF08], who introduced the notions of extensive-form correlated equilibrium
(EFCE) and agent-form correlated equilibrium (AFCE). They asked how difficult
the Threshold problem is for AFCE; we answer this question by proving that it
is NP-hard, even in two-player games without chance nodes. Complementing our
hardness results, we establish tight complexity classifications for the
Threshold problem across several correlated equilibrium concepts - including
EFCE, AFCE, normal-form coarse, extensive-form coarse, and agent-form coarse
correlated equilibria. For each of these solution concepts in multiplayer
stochastic extensive-form games with perfect recall, we prove NP-completeness
by providing matching NP upper bounds to the previously known hardness results.
Together, our results provide the most complete landscape to date for the
complexity of optimal equilibrium computation in extensive-form games.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [75] [Extracting Document Relations from Search Corpus by Marginalizing over User Queries](https://arxiv.org/abs/2507.10726)
*Yuki Iwamoto,Kaoru Tsunoda,Ken Kaneiwa*

Main category: cs.IR

TL;DR: 提出EDR - MQ框架通过查询边缘化发现文档关系，用MC - RAG实现，实验证明能识别有意义文档关系，提供实用文档组织方法。


<details>
  <summary>Details</summary>
Motivation: 现有文档关系分析方法依赖人工标注或预定义关系分类法，需要更好的方法。

Method: 提出EDR - MQ框架，基于查询边缘化发现文档关系；开发MC - RAG实现查询边缘化。

Result: 查询边缘化方法成功识别有意义文档关系，发现传统方法不易发现的主题聚类、证据链和跨领域连接。

Conclusion: 查询驱动框架为文档组织提供了适应不同用户视角和信息需求的实用方法。

Abstract: Understanding relationships between documents in large-scale corpora is
essential for knowledge discovery and information organization. However,
existing approaches rely heavily on manual annotation or predefined
relationship taxonomies. We propose EDR-MQ (Extracting Document Relations by
Marginalizing over User Queries), a novel framework that discovers document
relationships through query marginalization. EDR-MQ is based on the insight
that strongly related documents often co-occur in results across diverse user
queries, enabling us to estimate joint probabilities between document pairs by
marginalizing over a collection of queries. To enable this query
marginalization approach, we develop Multiply Conditioned Retrieval-Augmented
Generation (MC-RAG), which employs conditional retrieval where subsequent
document retrievals depend on previously retrieved content. By observing
co-occurrence patterns across diverse queries, EDR-MQ estimates joint
probabilities between document pairs without requiring labeled training data or
predefined taxonomies. Experimental results show that our query marginalization
approach successfully identifies meaningful document relationships, revealing
topical clusters, evidence chains, and cross-domain connections that are not
apparent through traditional similarity-based methods. Our query-driven
framework offers a practical approach to document organization that adapts to
different user perspectives and information needs.

</details>


### [76] [Overview of the TREC 2022 deep learning track](https://arxiv.org/abs/2507.10865)
*Nick Craswell,Bhaskar Mitra,Emine Yilmaz,Daniel Campos,Jimmy Lin,Ellen M. Voorhees,Ian Soboroff*

Main category: cs.IR

TL;DR: 本文介绍TREC深度学习赛道第四年情况，利用MS MARCO数据集，聚焦构建段落检索测试集，分析模型表现有意外结果。


<details>
  <summary>Details</summary>
Motivation: 延续前几年工作，构建更完整的段落检索测试集，推动深度学习在信息检索任务发展。

Method: 利用MS MARCO数据集及更新的段落和文档集合，进行段落检索和文档排序任务，用人工标注训练标签，对比不同模型表现。

Result: 大规模预训练深度神经排序模型仍优于传统检索方法；部分表现好的运行未进行密集检索；单阶段密集检索运行不如去年有竞争力。

Conclusion: 今年更有信心保证查询和判断质量，方便区分运行和未来复用数据集，模型表现有意外情况。

Abstract: This is the fourth year of the TREC Deep Learning track. As in previous
years, we leverage the MS MARCO datasets that made hundreds of thousands of
human annotated training labels available for both passage and document ranking
tasks. In addition, this year we also leverage both the refreshed passage and
document collections that were released last year leading to a nearly $16$
times increase in the size of the passage collection and nearly four times
increase in the document collection size. Unlike previous years, in 2022 we
mainly focused on constructing a more complete test collection for the passage
retrieval task, which has been the primary focus of the track. The document
ranking task was kept as a secondary task, where document-level labels were
inferred from the passage-level labels. Our analysis shows that similar to
previous years, deep neural ranking models that employ large scale pretraining
continued to outperform traditional retrieval methods. Due to the focusing our
judging resources on passage judging, we are more confident in the quality of
this year's queries and judgments, with respect to our ability to distinguish
between runs and reuse the dataset in future. We also see some surprises in
overall outcomes. Some top-performing runs did not do dense retrieval. Runs
that did single-stage dense retrieval were not as competitive this year as they
were last year.

</details>


### [77] [LLM-Driven Dual-Level Multi-Interest Modeling for Recommendation](https://arxiv.org/abs/2507.10917)
*Ziyan Wang,Yingpeng Du,Zhu Sun,Jieyi Bi,Haoyan Chua,Tianjun Wei,Jie Zhang*

Main category: cs.IR

TL;DR: 本文提出基于大语言模型的双级多兴趣建模框架用于推荐，实验显示该方法优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有用户多兴趣建模方法依赖启发式假设，大语言模型用于多兴趣分析存在粒度不确定和用户数据稀疏问题。

Method: 在用户个体层面，利用大语言模型将用户参与的项目分配到语义簇，通过对齐模块自适应调整粒度；在用户群体层面，聚合用户团成合成用户，解决数据稀疏问题，并进行对比学习。

Result: 在真实数据集上实验表明该方法优于现有方法。

Conclusion: 所提基于大语言模型的双级多兴趣建模框架能有效进行推荐。

Abstract: Recently, much effort has been devoted to modeling users' multi-interests
based on their behaviors or auxiliary signals. However, existing methods often
rely on heuristic assumptions, e.g., co-occurring items indicate the same
interest of users, failing to capture user multi-interests aligning with
real-world scenarios. While large language models (LLMs) show significant
potential for multi-interest analysis due to their extensive knowledge and
powerful reasoning capabilities, two key challenges remain. First, the
granularity of LLM-driven multi-interests is agnostic, possibly leading to
overly fine or coarse interest grouping. Second, individual user analysis
provides limited insights due to the data sparsity issue. In this paper, we
propose an LLM-driven dual-level multi-interest modeling framework for more
effective recommendation. At the user-individual level, we exploit LLMs to
flexibly allocate items engaged by users into different semantic clusters,
indicating their diverse and distinct interests. To alleviate the agnostic
generation of LLMs, we adaptively assign these semantic clusters to users'
collaborative multi-interests learned from global user-item interactions,
allowing the granularity to be automatically adjusted according to the user's
behaviors using an alignment module. To alleviate the limited insights derived
from individual users' behaviors, at the user-crowd level, we propose
aggregating user cliques into synthesized users with rich behaviors for more
comprehensive LLM-driven multi-interest analysis. We formulate a max covering
problem to ensure the compactness and representativeness of synthesized users'
behaviors, and then conduct contrastive learning based on their LLM-driven
multi-interests to disentangle item representations among different interests.
Experiments on real-world datasets show the superiority of our approach against
state-of-the-art methods.

</details>


### [78] [Unraveling the Biomarker Prospects of High-Altitude Diseases: Insights from Biomolecular Event Network Constructed using Text Mining](https://arxiv.org/abs/2507.10953)
*Balu Bhasuran,Sabenabanu Abdulkadhar,Jeyakumar Natarajan*

Main category: cs.IR

TL;DR: 本文通过文本挖掘和图分析研究高原病相关生物分子事件，构建网络并找出关键生物分子与功能簇，证明该方法对揭示机制和确定生物标志物的有效性。


<details>
  <summary>Details</summary>
Motivation: 高原病健康风险大，但分子机制了解不足，需深入研究。

Method: 开发整合监督机器学习与特征和多尺度拉普拉斯图核的生物分子事件提取管道，分析7847篇摘要；用PageRank算法对生物分子排序；进行子网络分析。

Result: 提取超150个生物分子事件，构建含97个节点和153条边的网络；确定关键蛋白；发现三个主要功能簇。

Conclusion: 大规模文本挖掘和基于图的分析有助于揭示高原病机制和确定潜在生物标志物。

Abstract: High-altitude diseases (HAD), encompassing acute mountain sickness (AMS),
high-altitude cerebral edema (HACE), and high-altitude pulmonary edema (HAPE),
are triggered by hypobaric hypoxia at elevations above 2,500 meters. These
conditions pose significant health risks, yet the molecular mechanisms remain
insufficiently understood. In this study, we developed a biomolecular event
extraction pipeline integrating supervised machine learning with feature-based
and multiscale Laplacian graph kernels to analyze 7,847 curated HAD-related
abstracts from PubMed. We extracted over 150 unique biomolecular events
including gene expression, regulation, binding, and localization and
constructed a weighted, undirected biomolecular event network comprising 97
nodes and 153 edges. Using the PageRank algorithm, we prioritized key
biomolecules based on their centrality within the event network. The top-ranked
proteins included Erythropoietin (EPO) (0.0163), Vascular endothelial growth
factor (VEGF) (0.0148), Hypoxia-inducible factor 1 (HIF-1) alpha (0.0136),
Endothelial PAS Domain Protein 1 (EPAS1) and Angiotensin-Converting Enzyme
(ACE) (0.0119), Egl nine homolog 1 (EGLN1), Endothelin 1 (ET-1), and 70
kilodalton heat shock protein (Hsp70)(0.0118), all of which play crucial roles
in oxygen sensing, vascular remodeling, erythropoiesis, and blood pressure
regulation. Subnetwork analysis revealed three major functional clusters
centered on hypoxia response, inflammation, and stress adaptation pathways. Our
integrative approach demonstrates the utility of large-scale text mining and
graph-based analysis to uncover mechanistic insights and prioritize potential
biomarkers for high-altitude disease.

</details>


### [79] [Aligned Query Expansion: Efficient Query Expansion for Information Retrieval through LLM Alignment](https://arxiv.org/abs/2507.11042)
*Adam Yang,Gustavo Penha,Enrico Palumbo,Hugues Bouchard*

Main category: cs.IR

TL;DR: 本文提出用于开放域问答中段落检索的对齐查询扩展方法AQE，能消除额外过滤步骤，提升检索效果。


<details>
  <summary>Details</summary>
Motivation: 现有生成 - 过滤的查询扩展方法成本高且无法指导大语言模型生成更有效的查询。

Method: 利用大语言模型对齐技术微调模型生成查询扩展，直接优化检索任务效果，无需额外过滤步骤。

Result: 实证评估显示AQE在领域内和领域外设置中均优于查询扩展基线模型，显著提升检索效果。

Conclusion: AQE能降低计算成本，提高检索有效性。

Abstract: With the breakthroughs in large language models (LLMs), query generation
techniques that expand documents and queries with related terms are becoming
increasingly popular in the information retrieval field. Such techniques have
been shown to improve the effectiveness of traditional lexical retrieval
methods by dealing with the vocabulary mismatch problem. Recent work has found
that generating queries with a greedy decoding strategy can produce sub-optimal
queries, including hallucinations, and proposed to filter out queries before
expansion. This `generate-then-filter' approach is costly, as it requires
generating multiple queries and applying a relevance model to all of them and
does not teach the LLM which of the generated queries is more effective for
expansion. To overcome such limitations, we propose Aligned Query Expansion
(AQE), a novel approach to enhance query expansion for passage retrieval in
open-domain question answering. AQE leverages recent techniques in LLM
alignment to fine-tune models for generating query expansions that directly
optimize the effectiveness of the retrieval task, eliminating the need for
additional filtering steps. This alignment ensures that queries are more
relevant, reducing computational costs while improving retrieval effectiveness.
Empirical evaluations show that AQE outperforms baseline models for query
expansion in both in-domain and out-of-domain settings, demonstrating
significant improvements in retrieval effectiveness.

</details>


### [80] [From Chaos to Automation: Enabling the Use of Unstructured Data for Robotic Process Automation](https://arxiv.org/abs/2507.11364)
*Kelly Kurowski,Xixi Lu,Hajo A. Reijers*

Main category: cs.IR

TL;DR: 组织内非结构化数据量大难处理，传统RPA依赖结构化数据，本文开发UNDRESS系统提升RPA处理非结构化数据能力，效果良好。


<details>
  <summary>Details</summary>
Motivation: 组织内非结构化数据占比大但难处理，传统RPA依赖结构化数据，应用受限，需解决此问题。

Method: 设计开发UNDRESS原型系统，结合模糊正则表达式、自然语言处理技术和大语言模型，基于文本提取和信息检索性能评估。

Result: UNDRESS有效提升RPA处理非结构化数据的能力。

Conclusion: 该系统有助于RPA在受非结构化数据阻碍的流程中更广泛应用，提高业务流程效率。

Abstract: The growing volume of unstructured data within organizations poses
significant challenges for data analysis and process automation. Unstructured
data, which lacks a predefined format, encompasses various forms such as
emails, reports, and scans. It is estimated to constitute approximately 80% of
enterprise data. Despite the valuable insights it can offer, extracting
meaningful information from unstructured data is more complex compared to
structured data. Robotic Process Automation (RPA) has gained popularity for
automating repetitive tasks, improving efficiency, and reducing errors.
However, RPA is traditionally reliant on structured data, limiting its
application to processes involving unstructured documents. This study addresses
this limitation by developing the UNstructured Document REtrieval SyStem
(UNDRESS), a system that uses fuzzy regular expressions, techniques for natural
language processing, and large language models to enable RPA platforms to
effectively retrieve information from unstructured documents. The research
involved the design and development of a prototype system, and its subsequent
evaluation based on text extraction and information retrieval performance. The
results demonstrate the effectiveness of UNDRESS in enhancing RPA capabilities
for unstructured data, providing a significant advancement in the field. The
findings suggest that this system could facilitate broader RPA adoption across
processes traditionally hindered by unstructured data, thereby improving
overall business process efficiency.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [81] [MH-FSF: A Unified Framework for Overcoming Benchmarking and Reproducibility Limitations in Feature Selection Evaluation](https://arxiv.org/abs/2507.10591)
*Vanderson Rocha,Diego Kreutz,Gabriel Canto,Hendrio Bragança,Eduardo Feitosa*

Main category: cs.LG

TL;DR: 提出MH - FSF框架用于特征选择，在10个公开安卓恶意软件数据集评估，结果显示性能差异，强调统一平台重要性。


<details>
  <summary>Details</summary>
Motivation: 当前特征选择研究存在基准测试有限、依赖专有数据集问题，阻碍可重复性和影响性能。

Method: 开发MH - FSF框架，实现17种特征选择方法，在10个公开安卓恶意软件数据集进行系统评估。

Result: 在平衡和不平衡数据集上性能有差异，凸显数据预处理和选择标准考虑不对称性的重要性。

Conclusion: 统一平台对比较不同特征选择技术很重要，该框架拓宽了现有文献，为特征选择研究指明新方向。

Abstract: Feature selection is vital for building effective predictive models, as it
reduces dimensionality and emphasizes key features. However, current research
often suffers from limited benchmarking and reliance on proprietary datasets.
This severely hinders reproducibility and can negatively impact overall
performance. To address these limitations, we introduce the MH-FSF framework, a
comprehensive, modular, and extensible platform designed to facilitate the
reproduction and implementation of feature selection methods. Developed through
collaborative research, MH-FSF provides implementations of 17 methods (11
classical, 6 domain-specific) and enables systematic evaluation on 10 publicly
available Android malware datasets. Our results reveal performance variations
across both balanced and imbalanced datasets, highlighting the critical need
for data preprocessing and selection criteria that account for these
asymmetries. We demonstrate the importance of a unified platform for comparing
diverse feature selection techniques, fostering methodological consistency and
rigor. By providing this framework, we aim to significantly broaden the
existing literature and pave the way for new research directions in feature
selection, particularly within the context of Android malware detection.

</details>


### [82] [Tool-to-Tool Matching Analysis Based Difference Score Computation Methods for Semiconductor Manufacturing](https://arxiv.org/abs/2507.10564)
*Sameera Bharadwaja H.,Siddhrath Jandial,Shashank S. Agashe,Rajesh Kumar Reddy Moore,Youngkwan Kim*

Main category: cs.LG

TL;DR: 提出新的工具到工具匹配（TTTM）分析管道解决传统方法问题，验证方法有效性并分析多元算法对超参数的敏感性。


<details>
  <summary>Details</summary>
Motivation: 传统TTTM方法依赖难获取的静态配置数据或黄金参考，且在异构环境中扩展性差。

Method: 提出新颖的TTTM分析管道，假设不匹配设备数据有更高方差和/或更多模式。

Result: 最佳单变量方法与方差和模式数量的相关系数分别>0.95和>0.5，最佳多变量方法与顶级单变量方法相关系数>0.75。

Conclusion: 提出的方法有效，还分析了多元算法对超参数的敏感性。

Abstract: We consider the problem of tool-to-tool matching (TTTM), also called, chamber
matching in the context of a semiconductor manufacturing equipment. Traditional
TTTM approaches utilize static configuration data or depend on a golden
reference which are difficult to obtain in a commercial manufacturing line.
Further, existing methods do not extend very well to a heterogeneous setting,
where equipment are of different make-and-model, sourced from different
equipment vendors. We propose novel TTTM analysis pipelines to overcome these
issues. We hypothesize that a mismatched equipment would have higher variance
and/or higher number of modes in the data. Our best univariate method achieves
a correlation coefficient >0.95 and >0.5 with the variance and number of modes,
respectively showing that the proposed methods are effective. Also, the best
multivariate method achieves a correlation coefficient >0.75 with the
top-performing univariate methods, showing its effectiveness. Finally, we
analyze the sensitivity of the multivariate algorithms to the algorithm
hyper-parameters.

</details>


### [83] [Enhancing Cross Entropy with a Linearly Adaptive Loss Function for Optimized Classification Performance](https://arxiv.org/abs/2507.10574)
*Jae Wan Shim*

Main category: cs.LG

TL;DR: 提出线性自适应交叉熵损失函数，在CIFAR - 100数据集上评估，结果显示优于标准交叉熵损失函数，且保持简单高效，为损失函数设计研究拓宽范围。


<details>
  <summary>Details</summary>
Motivation: 改进分类任务中优化过程，提升分类准确性，设计更好的损失函数。

Method: 提出基于信息理论的线性自适应交叉熵损失函数，该函数比标准交叉熵损失函数多一个依赖真实类预测概率的项，并在基于ResNet的模型上使用CIFAR - 100数据集进行评估。

Result: 提出的损失函数在分类准确性上始终优于标准交叉熵损失函数，且保持与传统交叉熵损失函数相近的效率。

Conclusion: 该方法能为未来损失函数设计研究拓宽范围。

Abstract: We propose the Linearly Adaptive Cross Entropy Loss function. This is a novel
measure derived from the information theory. In comparison to the standard
cross entropy loss function, the proposed one has an additional term that
depends on the predicted probability of the true class. This feature serves to
enhance the optimization process in classification tasks involving one-hot
encoded class labels. The proposed one has been evaluated on a ResNet-based
model using the CIFAR-100 dataset. Preliminary results show that the proposed
one consistently outperforms the standard cross entropy loss function in terms
of classification accuracy. Moreover, the proposed one maintains simplicity,
achieving practically the same efficiency to the traditional cross entropy
loss. These findings suggest that our approach could broaden the scope for
future research into loss function design.

</details>


### [84] [An Adaptive Volatility-based Learning Rate Scheduler](https://arxiv.org/abs/2507.10575)
*Kieran Chai Kai Ren*

Main category: cs.LG

TL;DR: 本文提出新型自适应学习率调度器VolSched，通过计算准确率波动率比值调整学习率，在CIFAR - 100数据集上提升了模型性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 流行的预定义和自适应学习率调度器可能导致次优泛化，需要更好的调度器。

Method: 引入受随机过程中波动率概念启发的VolSched，计算长期和短期准确率波动率的比值来动态调整学习率。

Result: 在CIFAR - 100数据集上，与ResNet - 18和ResNet - 34搭配时，分别提升top - 1准确率1.4和1.3个百分点；促进更长探索阶段；找到的最终解比次优基线平坦38%。

Conclusion: VolSched能让模型更有效探索损失曲面，获得更宽最小值，实现更好的泛化性能。

Abstract: Effective learning rate (LR) scheduling is crucial for training deep neural
networks. However, popular pre-defined and adaptive schedulers can still lead
to suboptimal generalization. This paper introduces VolSched, a novel adaptive
LR scheduler inspired by the concept of volatility in stochastic processes like
Geometric Brownian Motion to dynamically adjust the learning rate. By
calculating the ratio between long-term and short-term accuracy volatility,
VolSched increases the LR to escape plateaus and decreases it to stabilize
training, allowing the model to explore the loss landscape more effectively. We
evaluate VolSched on the CIFAR-100 dataset against a strong baseline using a
standard augmentation pipeline. When paired with ResNet-18 and ResNet-34, our
scheduler delivers consistent performance gains, improving top-1 accuracy by
1.4 and 1.3 percentage points respectively. Analysis of the loss curves reveals
that VolSched promotes a longer exploration phase. A quantitative analysis of
the Hessian shows that VolSched finds a final solution that is 38% flatter than
the next-best baseline, allowing the model to obtain wider minima and hence
better generalization performance.

</details>


### [85] [Universal Approximation Theorem for a Single-Layer Transformer](https://arxiv.org/abs/2507.10581)
*Esmail Gumaan*

Main category: cs.LG

TL;DR: 本文探讨深度学习和Transformer的数学基础，提出关于Transformer的通用逼近定理并证明，通过案例研究展示结果的实际意义，促进对Transformer模型的理论理解。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习和Transformer模型的理论理解有限，需深入研究其数学基础。

Method: 回顾线性代数、概率和优化的关键概念，详细分析多头自注意力机制和反向传播算法，证明单层Transformer的通用逼近定理。

Result: 证明单层Transformer能在紧致域上以任意精度逼近任何连续的序列到序列映射，并给出正式陈述和完整证明，进行案例研究。

Conclusion: 研究结果推进了对Transformer模型的理论理解，缩小了理论与实践的差距。

Abstract: Deep learning employs multi-layer neural networks trained via the
backpropagation algorithm. This approach has achieved success across many
domains and relies on adaptive gradient methods such as the Adam optimizer.
Sequence modeling evolved from recurrent neural networks to attention-based
models, culminating in the Transformer architecture. Transformers have achieved
state-of-the-art performance in natural language processing (for example, BERT
and GPT-3) and have been applied in computer vision and computational biology.
However, theoretical understanding of these models remains limited. In this
paper, we examine the mathematical foundations of deep learning and
Transformers and present a novel theoretical result. We review key concepts
from linear algebra, probability, and optimization that underpin deep learning,
and we analyze the multi-head self-attention mechanism and the backpropagation
algorithm in detail. Our main contribution is a universal approximation theorem
for Transformers: we prove that a single-layer Transformer, comprising one
self-attention layer followed by a position-wise feed-forward network with ReLU
activation, can approximate any continuous sequence-to-sequence mapping on a
compact domain to arbitrary precision. We provide a formal statement and a
complete proof. Finally, we present case studies that demonstrate the practical
implications of this result. Our findings advance the theoretical understanding
of Transformer models and help bridge the gap between theory and practice.

</details>


### [86] [Extension OL-MDISF: Online Learning from Mix-Typed, Drifted, and Incomplete Streaming Features](https://arxiv.org/abs/2507.10594)
*Shengda Zhuo,Di Wu,Yi He,Shuqiang Huang,Xindong Wu*

Main category: cs.LG

TL;DR: 论文针对在线学习面临的三个挑战提出OL - MDISF方法，并进行相关实验，提供可复现基准。


<details>
  <summary>Details</summary>
Motivation: 在线学习面临数据异质性、分布漂移和难以全标注的挑战，需要有效方法解决。

Method: 提出OL - MDISF方法，构建基于潜copula的异质特征表示，通过集成熵和潜在失配检测漂移，进行结构感知伪标注。

Result: 进行涵盖14个真实数据集、两种漂移场景的综合实验，包括CER趋势、消融研究等。

Conclusion: 本文为复杂弱监督流数据的在线学习提供可复现基准。

Abstract: Online learning, where feature spaces can change over time, offers a flexible
learning paradigm that has attracted considerable attention. However, it still
faces three significant challenges. First, the heterogeneity of real-world data
streams with mixed feature types presents challenges for traditional parametric
modeling. Second, data stream distributions can shift over time, causing an
abrupt and substantial decline in model performance. Third, it is often
infeasible to label every data instance due to time and cost constraints. To
address these issues, we proposed OL-MDISF (Online Learning from Mix-typed,
Drifted, and Incomplete Streaming Features), which constructs a latent
copula-based representation for heterogeneous features, detects drifts via
ensemble entropy and latent mismatch, and performs structure-aware
pseudo-labeling.
  This companion paper serves as a standalone technical reference to OL-MDISF.
It provides a contextual discussion of related work in mixed-type modeling,
drift adaptation, and weak supervision, as well as a comprehensive set of
experiments across 14 real-world datasets under two types of drift scenarios.
These include CER trends, ablation studies, sensitivity analyses, and temporal
ensemble dynamics. We hope this document offers a reproducible benchmark for
online learning on complex, weakly supervised streaming data.

</details>


### [87] [Divide-Then-Rule: A Cluster-Driven Hierarchical Interpolator for Attribute-Missing Graphs](https://arxiv.org/abs/2507.10595)
*Yaowen Hu,Wenxuan Tu,Yue Liu,Miaomiao Li,Wenpeng Lu,Zhigang Luo,Xinwang Liu,Ping Chen*

Main category: cs.LG

TL;DR: 提出Divide - Then - Rule Graph Completion (DTRGC)方法解决属性缺失图的深度图聚类问题，实验表明该方法显著提升聚类性能。


<details>
  <summary>Details</summary>
Motivation: 现有属性缺失图的插补方法未考虑节点邻域信息差异，导致结果不可靠，此领域研究不足，解决该问题对实际应用至关重要。

Method: 提出DTRGC方法，包含Dynamic Cluster - Aware Feature Propagation (DCFP)初始化缺失节点属性，Hierarchical Neighborhood - aware Imputation (HNAI)分层插补节点属性，Hop - wise Representation Enhancement (HRE)整合多跳信息丰富节点表示。

Result: 在六个常用图数据集上的实验显示，DTRGC显著提高了属性缺失图下各种深度图聚类方法的聚类性能。

Conclusion: DTRGC方法能有效解决属性缺失图的深度图聚类问题，提升聚类效果。

Abstract: Deep graph clustering (DGC) for attribute-missing graphs is an unsupervised
task aimed at partitioning nodes with incomplete attributes into distinct
clusters. Addressing this challenging issue is vital for practical
applications. However, research in this area remains underexplored. Existing
imputation methods for attribute-missing graphs often fail to account for the
varying amounts of information available across node neighborhoods, leading to
unreliable results, especially for nodes with insufficient known neighborhood.
To address this issue, we propose a novel method named Divide-Then-Rule Graph
Completion (DTRGC). This method first addresses nodes with sufficient known
neighborhood information and treats the imputed results as new knowledge to
iteratively impute more challenging nodes, while leveraging clustering
information to correct imputation errors. Specifically, Dynamic Cluster-Aware
Feature Propagation (DCFP) initializes missing node attributes by adjusting
propagation weights based on the clustering structure. Subsequently,
Hierarchical Neighborhood-aware Imputation (HNAI) categorizes attribute-missing
nodes into three groups based on the completeness of their neighborhood
attributes. The imputation is performed hierarchically, prioritizing the groups
with nodes that have the most available neighborhood information. The cluster
structure is then used to refine the imputation and correct potential errors.
Finally, Hop-wise Representation Enhancement (HRE) integrates information
across multiple hops, thereby enriching the expressiveness of node
representations. Experimental results on six widely used graph datasets show
that DTRGC significantly improves the clustering performance of various DGC
methods under attribute-missing graphs.

</details>


### [88] [RedOne: Revealing Domain-specific LLM Post-Training in Social Networking Services](https://arxiv.org/abs/2507.10605)
*Fei Zhao,Chonggang Lu,Yue Wang,Zheyong Xie,Ziyan Liu,Haofu Qian,JianZhao Huang,Fangcheng Shi,Zijie Meng,Hongcheng Guo,Mingqian He,Xinze Lyu,Yiming Lu,Ziyang Xiang,Zheyu Ye,Chengqiang Lu,Zhe Xu,Yi Wu,Yao Hu,Yan Gao,Jun Fan,Xiaolong Jiang,Weiting Liu,Boyang Wang,Shaosheng Cao*

Main category: cs.LG

TL;DR: 本文引入针对社交网络服务（SNS）的领域特定大语言模型RedOne，通过三阶段训练策略开发，实验显示其在多个SNS任务和实际场景表现优异。


<details>
  <summary>Details</summary>
Motivation: SNS发展对平台内容管理和交互质量提升提出挑战，现有大语言模型研究聚焦孤立任务，有性能瓶颈，无法适应多样场景。

Method: 采用三阶段训练策略（持续预训练、监督微调、偏好优化），使用大规模真实世界数据集开发RedOne。

Result: 与基础模型相比，RedOne在8个主要SNS任务平均提升14.02%，在SNS双语评估基准提升7.56%；在线测试中，有害内容检测暴露率降低11.23%，帖子搜索点击页面率提高14.95%。

Conclusion: RedOne是强大的SNS领域特定大语言模型，在各任务泛化性好，在实际场景有应用前景。

Abstract: As a primary medium for modern information dissemination, social networking
services (SNS) have experienced rapid growth, which has proposed significant
challenges for platform content management and interaction quality improvement.
Recently, the development of large language models (LLMs) has offered potential
solutions but existing studies focus on isolated tasks, which not only
encounter diminishing benefit from the data scaling within individual scenarios
but also fail to flexibly adapt to diverse real-world context. To address these
challenges, we introduce RedOne, a domain-specific LLM designed to break the
performance bottleneck of single-task baselines and establish a comprehensive
foundation for the SNS. RedOne was developed through a three-stage training
strategy consisting of continue pretraining, supervised fine-tuning, and
preference optimization, using a large-scale real-world dataset. Through
extensive experiments, RedOne maintains strong general capabilities, and
achieves an average improvement up to 14.02% across 8 major SNS tasks and 7.56%
in SNS bilingual evaluation benchmark, compared with base models. Furthermore,
through online testing, RedOne reduced the exposure rate in harmful content
detection by 11.23% and improved the click page rate in post-view search by
14.95% compared with single-tasks finetuned baseline models. These results
establish RedOne as a robust domain-specific LLM for SNS, demonstrating
excellent generalization across various tasks and promising applicability in
real-world scenarios.

</details>


### [89] [DALI-PD: Diffusion-based Synthetic Layout Heatmap Generation for ML in Physical Design](https://arxiv.org/abs/2507.10606)
*Bing-Yue Wu,Vidya A. Chhabria*

Main category: cs.LG

TL;DR: 提出DALI - PD框架生成合成布局热图以加速物理设计的机器学习研究，能快速生成多样热图并提升下游任务准确性。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习在物理设计任务中模型泛化性受限于高质量大规模训练数据集的获取，公开数据集存在静态、生成慢、需频繁更新等问题。

Method: 使用扩散模型通过快速推理在数秒内生成多样布局热图，热图包含多种类型。

Result: 创建了包含超20000个布局配置的数据集，热图与真实布局相似，提升了下游机器学习任务如IR压降或拥塞预测的准确性。

Conclusion: DALI - PD框架可有效解决当前物理设计机器学习研究中数据集的问题，加速相关研究。

Abstract: Machine learning (ML) has demonstrated significant promise in various
physical design (PD) tasks. However, model generalizability remains limited by
the availability of high-quality, large-scale training datasets. Creating such
datasets is often computationally expensive and constrained by IP. While very
few public datasets are available, they are typically static, slow to generate,
and require frequent updates. To address these limitations, we present DALI-PD,
a scalable framework for generating synthetic layout heatmaps to accelerate ML
in PD research. DALI-PD uses a diffusion model to generate diverse layout
heatmaps via fast inference in seconds. The heatmaps include power, IR drop,
congestion, macro placement, and cell density maps. Using DALI-PD, we created a
dataset comprising over 20,000 layout configurations with varying macro counts
and placements. These heatmaps closely resemble real layouts and improve ML
accuracy on downstream ML tasks such as IR drop or congestion prediction.

</details>


### [90] [GeoHopNet: Hopfield-Augmented Sparse Spatial Attention for Dynamic UAV Site Location Problem](https://arxiv.org/abs/2507.10636)
*Jianing Zhi,Xinghua Li,Zidong Chen*

Main category: cs.LG

TL;DR: 本文提出GeoHopNet解决无人机动态选址问题，实验显示其在求解规模和速度、质量上表现良好。


<details>
  <summary>Details</summary>
Motivation: 城市低空无人机经济发展对无人机降落点和补给站动态选址提出挑战，传统深度强化学习方法处理大规模城市级选址问题存在计算复杂度瓶颈。

Method: 提出GeoHopNet，包括距离偏置多头注意力机制、K近邻稀疏注意力、现代Hopfield外部记忆模块和记忆正则化策略。

Result: GeoHopNet扩展了可求解问题规模边界，在1000节点大规模实例中能快速找到高质量解，在100节点实例上比ADNet基线提升了解决方案质量并加快了速度。

Conclusion: GeoHopNet在无人机动态选址问题上表现出色，能有效应对大规模城市级选址问题。

Abstract: The rapid development of urban low-altitude unmanned aerial vehicle (UAV)
economy poses new challenges for dynamic site selection of UAV landing points
and supply stations. Traditional deep reinforcement learning methods face
computational complexity bottlenecks, particularly with standard attention
mechanisms, when handling large-scale urban-level location problems. This paper
proposes GeoHopNet, a Hopfield-augmented sparse spatial attention network
specifically designed for dynamic UAV site location problems. Our approach
introduces four core innovations: (1) distance-biased multi-head attention
mechanism that explicitly encodes spatial geometric information; (2) K-nearest
neighbor sparse attention that reduces computational complexity from $O(N^2)$
to $O(NK)$; (3) a modern Hopfield external memory module; and (4) a memory
regularization strategy. Experimental results demonstrate that GeoHopNet
extends the boundary of solvable problem sizes. For large-scale instances with
1,000 nodes, where standard attention models become prohibitively slow (over 3
seconds per instance) and traditional solvers fail, GeoHopNet finds
high-quality solutions (0.22\% optimality gap) in under 0.1 seconds. Compared
to the state-of-the-art ADNet baseline on 100-node instances, our method
improves solution quality by 22.2\% and is 1.8$\times$ faster.

</details>


### [91] [A Feed-Forward Artificial Intelligence Pipeline for Sustainable Desalination under Climate Uncertainties: UAE Insights](https://arxiv.org/abs/2507.10609)
*Obumneme Nwafor,Chioma Nwafor,Amro Zakaria,Nkechi Nwankwo*

Main category: cs.LG

TL;DR: 阿联酋海水淡化能耗高且面临气候不确定性挑战，研究提出两阶段预测模型架构，预测AOD和淡化性能效率损失，准确率达98%，还提出基于AOD和太阳能效率的控制逻辑，并打包成交互式仪表板。


<details>
  <summary>Details</summary>
Motivation: 阿联酋海水淡化能耗高、碳排放多，且面临气候不确定性挑战，如AOD影响太阳能海水淡化系统性能。

Method: 提出两阶段预测建模架构，第一阶段用卫星时间序列和气象数据预测AOD，第二阶段用预测的AOD和其他气象因素预测淡化性能效率损失；使用SHAP揭示系统退化关键驱动因素；提出基于AOD和太阳能效率的控制逻辑。

Result: 预测框架准确率达98%，提出的控制逻辑可调整海水淡化厂进水压力、调整维护计划和调节能源切换。

Conclusion: 将预测模型和基于规则的控制打包成交互式仪表板，为气候适应性规划提供管理决策支持系统。

Abstract: The United Arab Emirates (UAE) relies heavily on seawater desalination to
meet over 90% of its drinking water needs. Desalination processes are highly
energy intensive and account for approximately 15% of the UAE's electricity
consumption, contributing to over 22% of the country's energy-related CO2
emissions. Moreover, these processes face significant sustainability challenges
in the face of climate uncertainties such as rising seawater temperatures,
salinity, and aerosol optical depth (AOD). AOD greatly affects the operational
and economic performance of solar-powered desalination systems through
photovoltaic soiling, membrane fouling, and water turbidity cycles.
  This study proposes a novel pipelined two-stage predictive modelling
architecture: the first stage forecasts AOD using satellite-derived time series
and meteorological data; the second stage uses the predicted AOD and other
meteorological factors to predict desalination performance efficiency losses.
The framework achieved 98% accuracy, and SHAP (SHapley Additive exPlanations)
was used to reveal key drivers of system degradation. Furthermore, this study
proposes a dust-aware rule-based control logic for desalination systems based
on predicted values of AOD and solar efficiency. This control logic is used to
adjust the desalination plant feed water pressure, adapt maintenance
scheduling, and regulate energy source switching.
  To enhance the practical utility of the research findings, the predictive
models and rule-based controls were packaged into an interactive dashboard for
scenario and predictive analytics. This provides a management decision-support
system for climate-adaptive planning.

</details>


### [92] [A Group Theoretic Analysis of the Symmetries Underlying Base Addition and Their Learnability by Neural Networks](https://arxiv.org/abs/2507.10678)
*Cutter Dawes,Simon Segert,Kamesh Krishnamurthy,Jonathan D. Cohen*

Main category: cs.LG

TL;DR: 本文通过基加法研究神经网络对称性学习中的彻底泛化，分析基加法的进位函数，探究神经网络在不同进位函数下的学习情况，发现合适输入格式和进位函数可使简单网络实现彻底泛化，学习速度与进位函数结构相关，并讨论其对认知科学和机器学习的意义。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络在模拟人类认知功能和人工智能中高效学习支持彻底泛化的函数的设计难题，核心是发现和实现对称函数。

Method: 对基加法进行群论分析，找出不同进位函数并量化，训练神经网络用不同进位函数进行基加法，比较学习效果和速度。

Result: 简单神经网络在合适输入格式和进位函数下能实现彻底泛化，学习速度与进位函数结构密切相关。

Conclusion: 研究结果对认知科学和机器学习有一定相关性。

Abstract: A major challenge in the use of neural networks both for modeling human
cognitive function and for artificial intelligence is the design of systems
with the capacity to efficiently learn functions that support radical
generalization. At the roots of this is the capacity to discover and implement
symmetry functions. In this paper, we investigate a paradigmatic example of
radical generalization through the use of symmetry: base addition. We present a
group theoretic analysis of base addition, a fundamental and defining
characteristic of which is the carry function -- the transfer of the remainder,
when a sum exceeds the base modulus, to the next significant place. Our
analysis exposes a range of alternative carry functions for a given base, and
we introduce quantitative measures to characterize these. We then exploit
differences in carry functions to probe the inductive biases of neural networks
in symmetry learning, by training neural networks to carry out base addition
using different carries, and comparing efficacy and rate of learning as a
function of their structure. We find that even simple neural networks can
achieve radical generalization with the right input format and carry function,
and that learning speed is closely correlated with carry function structure. We
then discuss the relevance this has for cognitive science and machine learning.

</details>


### [93] [FedGSCA: Medical Federated Learning with Global Sample Selector and Client Adaptive Adjuster under Label Noise](https://arxiv.org/abs/2507.10611)
*Mengwen Ye,Yingzi Huangfu,Shujian Gao,Wei Ren,Weifan Liu,Zekuan Yu*

Main category: cs.LG

TL;DR: 提出FedGSCA框架用于增强有噪声医疗联邦学习的鲁棒性，在多数据集不同噪声条件下评估，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法难以应对医疗数据的噪声异质性和数据不平衡问题，导致训练不稳定和模型性能下降。

Method: 提出FedGSCA框架，包含全局样本选择器和客户端自适应调整机制，结合自适应阈值伪标签生成和鲁棒可信标签损失。

Result: 在一个真实世界结肠切片数据集和两个合成医疗数据集上评估，FedGSCA在极端和异质噪声场景中表现出色，优于现有方法。

Conclusion: FedGSCA在提高模型稳定性和处理复杂噪声方面有显著优势，适合现实世界医疗联邦学习场景。

Abstract: Federated Learning (FL) emerged as a solution for collaborative medical image
classification while preserving data privacy. However, label noise, which
arises from inter-institutional data variability, can cause training
instability and degrade model performance. Existing FL methods struggle with
noise heterogeneity and the imbalance in medical data. Motivated by these
challenges, we propose FedGSCA, a novel framework for enhancing robustness in
noisy medical FL. FedGSCA introduces a Global Sample Selector that aggregates
noise knowledge from all clients, effectively addressing noise heterogeneity
and improving global model stability. Furthermore, we develop a Client Adaptive
Adjustment (CAA) mechanism that combines adaptive threshold pseudo-label
generation and Robust Credal Labeling Loss. CAA dynamically adjusts to class
distributions, ensuring the inclusion of minority samples and carefully
managing noisy labels by considering multiple plausible labels. This dual
approach mitigates the impact of noisy data and prevents overfitting during
local training, which improves the generalizability of the model. We evaluate
FedGSCA on one real-world colon slides dataset and two synthetic medical
datasets under various noise conditions, including symmetric, asymmetric,
extreme, and heterogeneous types. The results show that FedGSCA outperforms the
state-of-the-art methods, excelling in extreme and heterogeneous noise
scenarios. Moreover, FedGSCA demonstrates significant advantages in improving
model stability and handling complex noise, making it well-suited for
real-world medical federated learning scenarios.

</details>


### [94] [A Simple Approximate Bayesian Inference Neural Surrogate for Stochastic Petri Net Models](https://arxiv.org/abs/2507.10714)
*Bright Kwaku Manu,Trevor Reckell,Beckett Sterner,Petar Jevtic*

Main category: cs.LG

TL;DR: 提出神经替代框架预测随机Petri网（SPN）速率函数系数，在合成SPN上表现良好，证明数据驱动、无似然替代方法可用于复杂离散事件系统参数恢复。


<details>
  <summary>Details</summary>
Motivation: SPN参数估计有挑战，特别是转移率依赖外部协变量且无显式似然时。

Method: 引入神经替代框架，使用轻量级1D卷积残差网络在Gillespie模拟的SPN实现上进行端到端训练，推理时用蒙特卡罗丢弃法。

Result: 在有20%事件缺失的合成SPN上，替代模型恢复速率函数系数的RMSE = 0.108，比传统贝叶斯方法运行快。

Conclusion: 数据驱动、无似然替代方法能在复杂、部分观测的离散事件系统中实现准确、稳健和实时的参数恢复。

Abstract: Stochastic Petri Nets (SPNs) are an increasingly popular tool of choice for
modeling discrete-event dynamics in areas such as epidemiology and systems
biology, yet their parameter estimation remains challenging in general and in
particular when transition rates depend on external covariates and explicit
likelihoods are unavailable. We introduce a neural-surrogate
(neural-network--based approximation of the posterior distribution) framework
that predicts the coefficients of known covariate-dependent rate functions
directly from noisy, partially observed token trajectories. Our model employs a
lightweight 1D Convolutional Residual Network trained end-to-end on
Gillespie-simulated SPN realizations, learning to invert system dynamics under
realistic conditions of event dropout. During inference, Monte Carlo dropout
provides calibrated uncertainty bounds together with point estimates. On
synthetic SPNs with 20% missing events, our surrogate recovers rate-function
coefficients with an RMSE = 0.108 and substantially runs faster than
traditional Bayesian approaches. These results demonstrate that data-driven,
likelihood-free surrogates can enable accurate, robust, and real-time parameter
recovery in complex, partially observed discrete-event systems.

</details>


### [95] [Sub-Scaling Laws: On the Role of Data Density and Training Strategies in LLMs](https://arxiv.org/abs/2507.10613)
*Zhengyu Chen,Siqi Wang,Teng Xiao,Yudong Wang,Shiqi Chen,Xunliang Cai,Junxian He,Jingang Wang*

Main category: cs.LG

TL;DR: 本文重新审视自然语言处理缩放定律，分析数据质量和训练策略对模型性能影响，识别子缩放关键因素，提出次优缩放定律。


<details>
  <summary>Details</summary>
Motivation: 传统缩放定律在大语言模型中出现偏差，性能提升减速，即子缩放现象，需重新审视缩放定律。

Method: 对超400个模型进行广泛实证分析。

Result: 确定高数据密度和非最优资源分配是子缩放关键因素，高数据密度因信息冗余导致收益递减，最优资源分配对持续提升性能至关重要。

Conclusion: 提出次优缩放定律，能更好预测子缩放状态下的性能，强调数据质量和多样性的重要性。

Abstract: Traditional scaling laws in natural language processing suggest that
increasing model size and training data enhances performance. However, recent
studies reveal deviations, particularly in large language models, where
performance improvements decelerate, which is a phenomenon known as
sub-scaling. This paper revisits these scaling laws by examining the impact of
data quality and training strategies on model performance. Through extensive
empirical analysis of over 400 models, we identify high data density and
non-optimal resource allocation as key factors contributing to sub-scaling.
High data density leads to diminishing returns due to redundant information,
while optimal resource allocation is crucial for sustained performance
improvements. We propose a sub-optimal scaling law that better predicts
performance in sub-scaling regimes, highlighting the importance of data quality
and diversity.

</details>


### [96] [D3FL: Data Distribution and Detrending for Robust Federated Learning in Non-linear Time-series Data](https://arxiv.org/abs/2507.11471)
*Harsha Varun Marisetty,Manik Gupta,Yogesh Simmhan*

Main category: cs.LG

TL;DR: 研究物联网非平稳、非线性时间序列数据在联邦学习中的预测问题，发现联邦学习处理非线性数据不如集中式方法，但适当去趋势技术可提升其性能。


<details>
  <summary>Details</summary>
Motivation: 物联网数据集中分析有延迟和成本问题，联邦学习是替代方案，但不同位置数据差异影响预测精度，需研究数据分布和去趋势技术对联邦学习性能的影响。

Method: 生成非线性数据分布的合成时间序列数据集，用集中式和联邦学习方法训练LSTM预测模型，并评估去趋势对真实数据集的影响。

Result: 联邦学习处理非线性数据分布时性能不如集中式方法；适当去趋势技术可提升联邦学习性能，降低不同数据分布的损失。

Conclusion: 在处理物联网非平稳、非线性时间序列数据时，采用适当去趋势技术能改善联邦学习的预测效果。

Abstract: With advancements in computing and communication technologies, the Internet
of Things (IoT) has seen significant growth. IoT devices typically collect data
from various sensors, such as temperature, humidity, and energy meters. Much of
this data is temporal in nature. Traditionally, data from IoT devices is
centralized for analysis, but this approach introduces delays and increased
communication costs. Federated learning (FL) has emerged as an effective
alternative, allowing for model training across distributed devices without the
need to centralize data. In many applications, such as smart home energy and
environmental monitoring, the data collected by IoT devices across different
locations can exhibit significant variation in trends and seasonal patterns.
Accurately forecasting such non-stationary, non-linear time-series data is
crucial for applications like energy consumption estimation and weather
forecasting. However, these data variations can severely impact prediction
accuracy. The key contributions of this paper are: (1) Investigating how
non-linear, non-stationary time-series data distributions, like generalized
extreme value (gen-extreme) and log norm distributions, affect FL performance.
(2) Analyzing how different detrending techniques for non-linear time-series
data influence the forecasting model's performance in a FL setup. We generated
several synthetic time-series datasets using non-linear data distributions and
trained an LSTM-based forecasting model using both centralized and FL
approaches. Additionally, we evaluated the impact of detrending on real-world
datasets with non-linear time-series data distributions. Our experimental
results show that: (1) FL performs worse than centralized approaches when
dealing with non-linear data distributions. (2) The use of appropriate
detrending techniques improves FL performance, reducing loss across different
data distributions.

</details>


### [97] [Fine-tuning Large Language Model for Automated Algorithm Design](https://arxiv.org/abs/2507.10614)
*Fei Liu,Rui Zhang,Xi Lin,Zhichao Lu,Qingfu Zhang*

Main category: cs.LG

TL;DR: 本文探索针对算法设计微调大语言模型，提出DAR采样策略并利用直接偏好优化，实验表明微调模型表现更好且有泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有算法设计多使用通用编码任务训练的大语言模型，需探究是否需要针对算法设计定制的大语言模型及如何获取和泛化。

Method: 引入DAR采样策略平衡训练数据多样性和质量，利用直接偏好优化使模型输出与任务目标对齐。

Result: 微调后的大语言模型在实验中显著优于现成模型，在可允许集问题上小模型表现可与大模型相当，且在相关任务上有泛化性。

Conclusion: 强调了算法设计中针对特定任务调整大语言模型的价值，为未来研究开辟新途径。

Abstract: The integration of large language models (LLMs) into automated algorithm
design has shown promising potential. A prevalent approach embeds LLMs within
search routines to iteratively generate and refine candidate algorithms.
However, most existing methods rely on off-the-shelf LLMs trained for general
coding tasks,leaving a key question open: Do we need LLMs specifically tailored
for algorithm design? If so, how can such LLMs be effectively obtained and how
well can they generalize across different algorithm design tasks? In this
paper, we take a first step toward answering these questions by exploring
fine-tuning of LLMs for algorithm design. We introduce a Diversity-Aware Rank
based (DAR) sampling strategy to balance training data diversity and quality,
then we leverage direct preference optimization to efficiently align LLM
outputs with task objectives. Our experiments, conducted on
Llama-3.2-1B-Instruct and Llama- 3.1-8B-Instruct, span three distinct algorithm
design tasks. Results suggest that finetuned LLMs can significantly outperform
their off-the-shelf counterparts with the smaller Llama-3.2-1B-Instruct and
match the larger Llama-3.1-8B-Instruct on the admissible set problem. Moreover,
we observe promising generalization: LLMs finetuned on specific algorithm
design tasks also improve performance on related tasks with varying settings.
These findings highlight the value of task-specific adaptation for LLMs in
algorithm design and open new avenues for future research.

</details>


### [98] [A parametric activation function based on Wendland RBF](https://arxiv.org/abs/2507.11493)
*Majid Darehmiraki*

Main category: cs.LG

TL;DR: 本文提出基于Wendland径向基函数的新型参数化激活函数，经理论分析和实验验证其性能良好，能缓解过拟合。


<details>
  <summary>Details</summary>
Motivation: 解决传统激活函数（如ReLU、sigmoid和tanh）的局限性。

Method: 将标准Wendland组件与线性和指数项结合，提出增强型Wendland激活函数，并进行理论分析和实验验证。

Result: 在某些场景尤其是回归任务中精度更高，保持计算效率。

Conclusion: Wendland激活函数可通过局部平滑变换缓解过拟合，提高泛化能力，未来可探索混合架构和特定领域适配。

Abstract: This paper introduces a novel parametric activation function based on
Wendland radial basis functions (RBFs) for deep neural networks. Wendland RBFs,
known for their compact support, smoothness, and positive definiteness in
approximation theory, are adapted to address limitations of traditional
activation functions like ReLU, sigmoid, and tanh. The proposed enhanced
Wendland activation combines a standard Wendland component with linear and
exponential terms, offering tunable locality, improved gradient propagation,
and enhanced stability during training. Theoretical analysis highlights its
mathematical properties, including smoothness and adaptability, while empirical
experiments on synthetic tasks (e.g., sine wave approximation) and benchmark
datasets (MNIST, Fashion-MNIST) demonstrate competitive performance. Results
show that the Wendland-based activation achieves superior accuracy in certain
scenarios, particularly in regression tasks, while maintaining computational
efficiency. The study bridges classical RBF theory with modern deep learning,
suggesting that Wendland activations can mitigate overfitting and improve
generalization through localized, smooth transformations. Future directions
include hybrid architectures and domain-specific adaptations.

</details>


### [99] [Multi-Armed Sampling Problem and the End of Exploration](https://arxiv.org/abs/2507.10797)
*Mohammad Pedramfar,Siamak Ravanbakhsh*

Main category: cs.LG

TL;DR: 本文介绍多臂采样框架，研究采样中探索 - 利用权衡，提出算法达最优遗憾界，统一多臂采样和多臂老虎机问题，对采样研究有基础作用。


<details>
  <summary>Details</summary>
Motivation: 在采样背景下严格检验探索 - 利用权衡。

Method: 系统定义框架的遗憾概念并建立下界，提出简单算法以达到最优遗憾界，定义连续问题族和相关遗憾度量来统一多臂采样和多臂老虎机问题。

Result: 理论结果表明采样无需探索，用温度参数统一了多臂采样和多臂老虎机问题。

Conclusion: 多臂采样框架及研究结果对采样研究有基础作用，为相关强化学习领域提供启示。

Abstract: This paper introduces the framework of multi-armed sampling, as the sampling
counterpart to the optimization problem of multi-arm bandits. Our primary
motivation is to rigorously examine the exploration-exploitation trade-off in
the context of sampling. We systematically define plausible notions of regret
for this framework and establish corresponding lower bounds. We then propose a
simple algorithm that achieves these optimal regret bounds. Our theoretical
results demonstrate that in contrast to optimization, sampling does not require
exploration. To further connect our findings with those of multi-armed bandits,
we define a continuous family of problems and associated regret measures that
smoothly interpolates and unifies multi-armed sampling and multi-armed bandit
problems using a temperature parameter. We believe the multi-armed sampling
framework, and our findings in this setting can have a foundational role in the
study of sampling including recent neural samplers, akin to the role of
multi-armed bandits in reinforcement learning. In particular, our work sheds
light on the need for exploration and the convergence properties of algorithm
for entropy-regularized reinforcement learning, fine-tuning of pretrained
models and reinforcement learning with human feedback (RLHF).

</details>


### [100] [Scalpel vs. Hammer: GRPO Amplifies Existing Capabilities, SFT Replaces Them](https://arxiv.org/abs/2507.10616)
*Neel Rajani,Aryo Pradipta Gema,Seraphina Goldfarb-Tarrant,Ivan Titov*

Main category: cs.LG

TL;DR: 对大语言模型用数学和代码数据集训练推理能力时，对比强化学习（RL）和监督微调（SFT），发现二者特点及参数修改情况，尝试冻结部分模型缓解问题但结果不明。


<details>
  <summary>Details</summary>
Motivation: 大语言模型使用数学和代码数据集训练推理能力时，RL和SFT训练动态不明，需要进行比较分析。

Method: 对同一数学问题，用相同模型和相似超参数对比RL和SFT，分析模型参数，尝试冻结部分模型训练。

Result: RL在数学领域有小提升，在知识密集型基准测试有轻微下降；SFT趋势更明显；冻结部分模型结果不确定。

Conclusion: 初步表明RL放大现有能力，SFT用新技能替换旧技能。

Abstract: Training large language models (LLMs) for reasoning via maths and code
datasets has become a major new focus in LLM post-training. Two particularly
popular approaches are reinforcement learning (RL) and supervised fine-tuning
(SFT), but their training dynamics are poorly understood. We present a
comparative analysis of RL and SFT on the same maths problems with the same
model and similar hyperparameters. We find that RL yields minor in-domain gains
on maths and slight degradation on knowledge-intensive benchmarks like MMLU,
while both trends are more pronounced in SFT. We also analyse model parameters
across checkpoints, observing that both algorithms modify query and key weights
the most. Meanwhile, SFT exhibits greater updates and also affects mid-layer
MLPs more, leading us to hypothesise that this may have caused the
out-of-domain degradation. We therefore investigate whether freezing parts of
the model during training can mitigate the reduced performance on
knowledge-intensive benchmarks. However, our results are inconclusive, with
benefits on GPQA:Diamond and degradation on other benchmarks. Taken together,
our observations provide a preliminary indication for why RL amplifies existing
capabilities, while SFT replaces old skills with new ones.

</details>


### [101] [Distributionally Robust Optimization with Adversarial Data Contamination](https://arxiv.org/abs/2507.10718)
*Shuyao Li,Ilias Diakonikolas,Jelena Diakonikolas*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Distributionally Robust Optimization (DRO) provides a framework for
decision-making under distributional uncertainty, yet its effectiveness can be
compromised by outliers in the training data. This paper introduces a
principled approach to simultaneously address both challenges. We focus on
optimizing Wasserstein-1 DRO objectives for generalized linear models with
convex Lipschitz loss functions, where an $\epsilon$-fraction of the training
data is adversarially corrupted. Our primary contribution lies in a novel
modeling framework that integrates robustness against training data
contamination with robustness against distributional shifts, alongside an
efficient algorithm inspired by robust statistics to solve the resulting
optimization problem. We prove that our method achieves an estimation error of
$O(\sqrt{\epsilon})$ for the true DRO objective value using only the
contaminated data under the bounded covariance assumption. This work
establishes the first rigorous guarantees, supported by efficient computation,
for learning under the dual challenges of data contamination and distributional
shifts.

</details>


### [102] [Compute Requirements for Algorithmic Innovation in Frontier AI Models](https://arxiv.org/abs/2507.10618)
*Peter Barnett*

Main category: cs.LG

TL;DR: 研究大语言模型预训练算法创新的计算需求，发现计算上限难显著阻碍AI算法进步。


<details>
  <summary>Details</summary>
Motivation: 探究开发算法创新所需的计算需求以及计算上限对创新的影响。

Method: 梳理Llama 3和DeepSeek - V3中36种预训练算法创新，估算开发总FLOP和硬件FLOP/s。

Result: 使用大量资源的创新需求每年翻倍，即使严格的计算上限也能允许一半的创新。

Conclusion: 仅计算上限不太可能大幅减缓AI算法进展。

Abstract: Algorithmic innovation in the pretraining of large language models has driven
a massive reduction in the total compute required to reach a given level of
capability. In this paper we empirically investigate the compute requirements
for developing algorithmic innovations. We catalog 36 pre-training algorithmic
innovations used in Llama 3 and DeepSeek-V3. For each innovation we estimate
both the total FLOP used in development and the FLOP/s of the hardware
utilized. Innovations using significant resources double in their requirements
each year. We then use this dataset to investigate the effect of compute caps
on innovation. Our analysis suggests that compute caps alone are unlikely to
dramatically slow AI algorithmic progress. Even stringent compute caps -- such
as capping total operations to the compute used to train GPT-2 or capping
hardware capacity to 8 H100 GPUs -- could still have allowed for half of the
cataloged innovations.

</details>


### [103] [Meta-Reinforcement Learning for Fast and Data-Efficient Spectrum Allocation in Dynamic Wireless Networks](https://arxiv.org/abs/2507.10619)
*Oluwaseyi Giwa,Tobi Awodunmila,Muhammad Ahmed Mohsin,Ahsan Bilal,Muhammad Ali Jamshed*

Main category: cs.LG

TL;DR: 提出元学习框架解决5G/6G网络频谱动态分配问题，对比非元学习DRL算法，证明元学习更有效安全。


<details>
  <summary>Details</summary>
Motivation: 传统深度强化学习在5G/6G网络频谱动态分配中因样本复杂度高和探索风险大而不可行，需解决这些挑战。

Method: 提出元学习框架，实现MAML、RNN和注意力增强RNN三种元学习架构，在模拟环境中与PPO基线对比。

Result: 基于注意力的元学习代理网络吞吐量峰值达48 Mbps，PPO降至10 Mbps；减少SINR和延迟违规超50%；公平指数0.7，资源分配更好。

Conclusion: 元学习是复杂无线系统智能控制的有效且安全的选择。

Abstract: The dynamic allocation of spectrum in 5G / 6G networks is critical to
efficient resource utilization. However, applying traditional deep
reinforcement learning (DRL) is often infeasible due to its immense sample
complexity and the safety risks associated with unguided exploration, which can
cause severe network interference. To address these challenges, we propose a
meta-learning framework that enables agents to learn a robust initial policy
and rapidly adapt to new wireless scenarios with minimal data. We implement
three meta-learning architectures, model-agnostic meta-learning (MAML),
recurrent neural network (RNN), and an attention-enhanced RNN, and evaluate
them against a non-meta-learning DRL algorithm, proximal policy optimization
(PPO) baseline, in a simulated dynamic integrated access/backhaul (IAB)
environment. Our results show a clear performance gap. The attention-based
meta-learning agent reaches a peak mean network throughput of 48 Mbps, while
the PPO baseline decreased drastically to 10 Mbps. Furthermore, our method
reduces SINR and latency violations by more than 50% compared to PPO. It also
shows quick adaptation, with a fairness index 0.7, showing better resource
allocation. This work proves that meta-learning is a very effective and safer
option for intelligent control in complex wireless systems.

</details>


### [104] [LLMs Meet Cross-Modal Time Series Analytics: Overview and Directions](https://arxiv.org/abs/2507.10620)
*Chenxi Liu,Hao Miao,Cheng Long,Yan Zhao,Ziyue Li,Panos Kalnis*

Main category: cs.LG

TL;DR: 本文介绍基于大语言模型的跨模态时间序列分析，对现有方法分类，讨论应用并总结挑战，旨在推动其实际应用。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽可用于时间序列分析，但文本和时间序列数据存在跨模态差距，需推动其在跨模态时间序列分析中的实际应用。

Method: 提供最新概述，引入分类法将现有方法分为转换、对齐和融合三类，并讨论下游任务应用。

Result: 介绍了大语言模型在跨模态时间序列分析中的方法和应用。

Conclusion: 本教程有助于参与者全面了解跨模态时间序列分析的现状、方法和未来研究方向。

Abstract: Large Language Models (LLMs) have emerged as a promising paradigm for time
series analytics, leveraging their massive parameters and the shared sequential
nature of textual and time series data. However, a cross-modality gap exists
between time series and textual data, as LLMs are pre-trained on textual
corpora and are not inherently optimized for time series. In this tutorial, we
provide an up-to-date overview of LLM-based cross-modal time series analytics.
We introduce a taxonomy that classifies existing approaches into three groups
based on cross-modal modeling strategies, e.g., conversion, alignment, and
fusion, and then discuss their applications across a range of downstream tasks.
In addition, we summarize several open challenges. This tutorial aims to expand
the practical application of LLMs in solving real-world problems in cross-modal
time series analytics while balancing effectiveness and efficiency.
Participants will gain a thorough understanding of current advancements,
methodologies, and future research directions in cross-modal time series
analytics.

</details>


### [105] [Flows and Diffusions on the Neural Manifold](https://arxiv.org/abs/2507.10623)
*Daniel Saragih,Deyu Cao,Tejas Balaji*

Main category: cs.LG

TL;DR: 本文将扩散和基于流的生成模型进展扩展到权重空间学习，提出统一框架，实验表明方法在生成权重、初始化训练和检测有害协变量偏移等方面表现良好。


<details>
  <summary>Details</summary>
Motivation: 将扩散和基于流的生成模型在图像合成等领域的成功扩展到权重空间学习。

Method: 将梯度下降诱导的轨迹建模为轨迹推理问题，在梯度流匹配框架下统一多种轨迹推理技术，探索架构和算法选择。

Result: 在生成分布内权重上匹配或超越基线，改善下游训练初始化，支持微调提升性能，在检测有害协变量偏移上优于可比基线。

Conclusion: 所提方法在权重空间学习及安全关键系统应用中有良好效果。

Abstract: Diffusion and flow-based generative models have achieved remarkable success
in domains such as image synthesis, video generation, and natural language
modeling. In this work, we extend these advances to weight space learning by
leveraging recent techniques to incorporate structural priors derived from
optimization dynamics. Central to our approach is modeling the trajectory
induced by gradient descent as a trajectory inference problem. We unify several
trajectory inference techniques under the framework of gradient flow matching,
providing a theoretical framework for treating optimization paths as inductive
bias. We further explore architectural and algorithmic choices, including
reward fine-tuning by adjoint matching, the use of autoencoders for latent
weight representation, conditioning on task-specific context data, and adopting
informative source distributions such as Kaiming uniform. Experiments
demonstrate that our method matches or surpasses baselines in generating
in-distribution weights, improves initialization for downstream training, and
supports fine-tuning to enhance performance. Finally, we illustrate a practical
application in safety-critical systems: detecting harmful covariate shifts,
where our method outperforms the closest comparable baseline.

</details>


### [106] [Fast Last-Iterate Convergence of SGD in the Smooth Interpolation Regime](https://arxiv.org/abs/2507.11274)
*Amit Attia,Matan Schliserman,Uri Sherman,Tomer Koren*

Main category: cs.LG

TL;DR: 研究插值机制下随机梯度下降（SGD）对平滑凸目标的总体收敛保证，给出SGD最后一次迭代的期望超额风险率，扩展并改进了现有结果。


<details>
  <summary>Details</summary>
Motivation: 由于对过参数化模型训练、持续学习中的遗忘分析以及随机Kaczmarz方法收敛理解的影响，近年来SGD最后一次迭代在大（恒定）步长下的行为受到越来越多关注。

Method: 对β - 平滑凸损失函数进行T步SGD，步长η ≤ 1/β。

Result: 最后一次迭代的期望超额风险为O(1/(ηT^(1 - βη/2)) + ηT^(βη/2)σₘ²)；调优步长时得到接近最优的O(1/T + σₘ/√T)速率；σₘ = 0时，η = 1/β得到O(1/√T)速率。

Conclusion: 扩展了Varre等人（2021）的结果，改进了Evron等人（2025）在可实现线性回归特殊情况下的最佳已知速率。

Abstract: We study population convergence guarantees of stochastic gradient descent
(SGD) for smooth convex objectives in the interpolation regime, where the noise
at optimum is zero or near zero. The behavior of the last iterate of SGD in
this setting -- particularly with large (constant) stepsizes -- has received
growing attention in recent years due to implications for the training of
over-parameterized models, as well as to analyzing forgetting in continual
learning and to understanding the convergence of the randomized Kaczmarz method
for solving linear systems. We establish that after $T$ steps of SGD on
$\beta$-smooth convex loss functions with stepsize $\eta \leq 1/\beta$, the
last iterate exhibits expected excess risk $\widetilde{O}(1/(\eta
T^{1-\beta\eta/2}) + \eta T^{\beta\eta/2} \sigma_\star^2)$, where
$\sigma_\star^2$ denotes the variance of the stochastic gradients at the
optimum. In particular, for a well-tuned stepsize we obtain a near optimal
$\widetilde{O}(1/T + \sigma_\star/\sqrt{T})$ rate for the last iterate,
extending the results of Varre et al. (2021) beyond least squares regression;
and when $\sigma_\star=0$ we obtain a rate of $O(1/\sqrt{T})$ with
$\eta=1/\beta$, improving upon the best-known $O(T^{-1/4})$ rate recently
established by Evron et al. (2025) in the special case of realizable linear
regression.

</details>


### [107] [Player-Team Heterogeneous Interaction Graph Transformer for Soccer Outcome Prediction](https://arxiv.org/abs/2507.10626)
*Lintao Wang,Shiwen Xu,Michael Horton,Joachim Gudmundsson,Zhiyong Wang*

Main category: cs.LG

TL;DR: 提出HIGFormer模型用于足球比赛结果预测，在WyScout数据集表现优，还可用于球员评估。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略球员和球队交互异质性，难以准确建模比赛动态。

Method: 提出HIGFormer模型，含球员交互网络、球队交互网络和比赛比较Transformer。

Result: 在WyScout数据集上，HIGFormer预测准确率显著优于现有方法。

Conclusion: HIGFormer有效，还能为球员表现评估、人才发掘和球队策略分析提供新视角。

Abstract: Predicting soccer match outcomes is a challenging task due to the inherently
unpredictable nature of the game and the numerous dynamic factors influencing
results. While it conventionally relies on meticulous feature engineering, deep
learning techniques have recently shown a great promise in learning effective
player and team representations directly for soccer outcome prediction.
However, existing methods often overlook the heterogeneous nature of
interactions among players and teams, which is crucial for accurately modeling
match dynamics. To address this gap, we propose HIGFormer (Heterogeneous
Interaction Graph Transformer), a novel graph-augmented transformer-based deep
learning model for soccer outcome prediction. HIGFormer introduces a
multi-level interaction framework that captures both fine-grained player
dynamics and high-level team interactions. Specifically, it comprises (1) a
Player Interaction Network, which encodes player performance through
heterogeneous interaction graphs, combining local graph convolutions with a
global graph-augmented transformer; (2) a Team Interaction Network, which
constructs interaction graphs from a team-to-team perspective to model
historical match relationships; and (3) a Match Comparison Transformer, which
jointly analyzes both team and player-level information to predict match
outcomes. Extensive experiments on the WyScout Open Access Dataset, a
large-scale real-world soccer dataset, demonstrate that HIGFormer significantly
outperforms existing methods in prediction accuracy. Furthermore, we provide
valuable insights into leveraging our model for player performance evaluation,
offering a new perspective on talent scouting and team strategy analysis.

</details>


### [108] [Neurosymbolic Reasoning Shortcuts under the Independence Assumption](https://arxiv.org/abs/2507.11357)
*Emile van Krieken,Pasquale Minervini,Edoardo Ponti,Antonio Vergari*

Main category: cs.LG

TL;DR: 本文探讨神经符号（NeSy）预测器中符号概念独立性假设的问题，表明该假设会使模型无法表示特定概念组合的不确定性，导致推理捷径问题。


<details>
  <summary>Details</summary>
Motivation: 解决NeSy社区对独立性假设何时真正限制NeSy系统的疑问。

Method: 通过形式化证明说明假设符号概念独立会使模型无法表示特定概念组合的不确定性。

Result: 证明了假设符号概念独立会使模型无法表示特定概念组合的不确定性，无法察觉推理捷径。

Conclusion: 独立性假设会阻碍NeSy预测器的学习，使其无法正确建模不确定性。

Abstract: The ubiquitous independence assumption among symbolic concepts in
neurosymbolic (NeSy) predictors is a convenient simplification: NeSy predictors
use it to speed up probabilistic reasoning. Recent works like van Krieken et
al. (2024) and Marconato et al. (2024) argued that the independence assumption
can hinder learning of NeSy predictors and, more crucially, prevent them from
correctly modelling uncertainty. There is, however, scepticism in the NeSy
community around the scenarios in which the independence assumption actually
limits NeSy systems (Faronius and Dos Martires, 2025). In this work, we settle
this question by formally showing that assuming independence among symbolic
concepts entails that a model can never represent uncertainty over certain
concept combinations. Thus, the model fails to be aware of reasoning shortcuts,
i.e., the pathological behaviour of NeSy predictors that predict correct
downstream tasks but for the wrong reasons.

</details>


### [109] [GHPO: Adaptive Guidance for Stable and Efficient LLM Reinforcement Learning](https://arxiv.org/abs/2507.10628)
*Ziru Liu,Cheng Gong,Xinyu Fu,Yaofang Liu,Ran Chen,Shoubo Hu,Suiyun Zhang,Rui Liu,Qingfu Zhang,Dandan Tu*

Main category: cs.LG

TL;DR: 提出引导混合策略优化（GHPO）框架解决强化学习训练不稳定和低效问题，实验显示有性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于策略的强化学习方法在训练大语言模型时存在不稳定和低效问题，尤其对小模型挑战大。

Method: 引入GHPO框架，通过自适应提示细化动态校准任务难度，平衡直接模仿学习和基于探索的强化学习。

Result: 在六个数学基准测试中平均性能提升约5%，优于基线方法。

Conclusion: GHPO框架提升了训练稳定性和推理性能，是开发推理模型的可扩展高效解决方案。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as
a powerful paradigm for facilitating the self-improvement of large language
models (LLMs), particularly in the domain of complex reasoning tasks. However,
prevailing on-policy RL methods often contend with significant training
instability and inefficiency. This is primarily due to a capacity-difficulty
mismatch, where the complexity of training data frequently outpaces the model's
current capabilities, leading to critically sparse reward signals and stalled
learning progress. This challenge is particularly acute for smaller, more
resource-efficient LLMs. To overcome this, we introduce the Guided Hybrid
Policy Optimization (GHPO), a novel difficulty-aware reinforcement learning
framework. GHPO dynamically calibrates task difficulty by employing adaptive
prompt refinement to provide targeted guidance. This unique approach adaptively
balances direct imitation learning for problems currently beyond the model's
reach with exploration-based reinforcement learning for more manageable tasks,
effectively creating a smooth and optimized learning curriculum. Extensive
experiments demonstrate that GHPO achieves an average performance gain of
approximately 5% across six challenging mathematics benchmarks, consistently
outperforming strong on-policy reinforcement learning and curriculum learning
baselines. Further analysis confirms that our framework significantly enhances
both training stability and final reasoning performance, thus offering a
scalable and efficient solution for developing powerful and robust reasoning
models.

</details>


### [110] [Local Pairwise Distance Matching for Backpropagation-Free Reinforcement Learning](https://arxiv.org/abs/2507.11367)
*Daniel Tanneberg*

Main category: cs.LG

TL;DR: 提出一种在强化学习中无需反向传播训练神经网络的新方法，实验显示其有竞争力且更稳定。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习用反向传播训练神经网络存在需存储前向传播激活值、易出现梯度消失或爆炸问题，影响学习性能和稳定性。

Method: 在强化学习中，利用多维缩放匹配成对距离原理引入局部、逐层损失，用前向传播时的局部信号训练每层，无需反向传播和存储中间激活值。

Result: 在常见强化学习基准测试中，该无反向传播方法与传统基于反向传播的方法相比有竞争力，提升了运行内和运行间的稳定性和一致性，在挑战性环境中性能更佳。

Conclusion: 提出的无反向传播方法可行且有效，能在强化学习中取得良好效果。

Abstract: Training neural networks with reinforcement learning (RL) typically relies on
backpropagation (BP), necessitating storage of activations from the forward
pass for subsequent backward updates. Furthermore, backpropagating error
signals through multiple layers often leads to vanishing or exploding
gradients, which can degrade learning performance and stability. We propose a
novel approach that trains each layer of the neural network using local signals
during the forward pass in RL settings. Our approach introduces local,
layer-wise losses leveraging the principle of matching pairwise distances from
multi-dimensional scaling, enhanced with optional reward-driven guidance. This
method allows each hidden layer to be trained using local signals computed
during forward propagation, thus eliminating the need for backward passes and
storing intermediate activations. Our experiments, conducted with policy
gradient methods across common RL benchmarks, demonstrate that this
backpropagation-free method achieves competitive performance compared to their
classical BP-based counterpart. Additionally, the proposed method enhances
stability and consistency within and across runs, and improves performance
especially in challenging environments.

</details>


### [111] [Scalable Unsupervised Segmentation via Random Fourier Feature-based Gaussian Process](https://arxiv.org/abs/2507.10632)
*Issei Saito,Masatoshi Nagano,Tomoaki Nakamura,Daichi Mochihashi,Koki Mimura*

Main category: cs.LG

TL;DR: 提出RFF - GP - HSMM方法解决GP - HSMM计算成本高问题，实验表明该方法分割性能与传统方法相当且速度更快。


<details>
  <summary>Details</summary>
Motivation: GP - HSMM在训练时需对N×N核矩阵求逆，随数据规模增大计算成本显著增加，需解决此问题。

Method: 使用随机傅里叶特征（RFF）通过线性回归近似高斯过程，避免核矩阵求逆。

Result: 在CMU运动捕捉数据集上，该方法分割性能与传统方法相当，在39200帧时间序列数据上分割速度约快278倍。

Conclusion: RFF - GP - HSMM是一种有效的快速无监督时间序列分割方法。

Abstract: In this paper, we propose RFF-GP-HSMM, a fast unsupervised time-series
segmentation method that incorporates random Fourier features (RFF) to address
the high computational cost of the Gaussian process hidden semi-Markov model
(GP-HSMM). GP-HSMM models time-series data using Gaussian processes, requiring
inversion of an N times N kernel matrix during training, where N is the number
of data points. As the scale of the data increases, matrix inversion incurs a
significant computational cost. To address this, the proposed method
approximates the Gaussian process with linear regression using RFF, preserving
expressive power while eliminating the need for inversion of the kernel matrix.
Experiments on the Carnegie Mellon University (CMU) motion-capture dataset
demonstrate that the proposed method achieves segmentation performance
comparable to that of conventional methods, with approximately 278 times faster
segmentation on time-series data comprising 39,200 frames.

</details>


### [112] [A Simple Baseline for Stable and Plastic Neural Networks](https://arxiv.org/abs/2507.10637)
*É. Künzel,A. Jaziri,V. Ramesh*

Main category: cs.LG

TL;DR: 提出RDBP解决计算机视觉持续学习问题，在持续ImageNet基准测试中表现良好，兼具实用性与可衡量性。


<details>
  <summary>Details</summary>
Motivation: 现有计算机视觉持续学习方法在可塑性和稳定性上失衡，需新方法解决。

Method: 引入RDBP，结合ReLUDown和Decreasing Backpropagation两种机制。

Result: 在持续ImageNet基准测试中，RDBP的可塑性和稳定性与现有方法相当或更优，且降低了计算成本。

Conclusion: RDBP为现实世界的持续学习提供实用解决方案，也为未来策略提供衡量基准。

Abstract: Continual learning in computer vision requires that models adapt to a
continuous stream of tasks without forgetting prior knowledge, yet existing
approaches often tip the balance heavily toward either plasticity or stability.
We introduce RDBP, a simple, low-overhead baseline that unites two
complementary mechanisms: ReLUDown, a lightweight activation modification that
preserves feature sensitivity while preventing neuron dormancy, and Decreasing
Backpropagation, a biologically inspired gradient-scheduling scheme that
progressively shields early layers from catastrophic updates. Evaluated on the
Continual ImageNet benchmark, RDBP matches or exceeds the plasticity and
stability of state-of-the-art methods while reducing computational cost. RDBP
thus provides both a practical solution for real-world continual learning and a
clear benchmark against which future continual learning strategies can be
measured.

</details>


### [113] [ZClassifier: Temperature Tuning and Manifold Approximation via KL Divergence on Logit Space](https://arxiv.org/abs/2507.10638)
*Shim Soon Yong*

Main category: cs.LG

TL;DR: 提出ZClassifier分类框架，用对角高斯分布的对数几率替换传统确定性对数几率，实验显示其在多方面优于softmax分类器。


<details>
  <summary>Details</summary>
Motivation: 统一不确定性校准和潜在控制，以原则性概率方式自然解释类置信度和几何一致性。

Method: 用对角高斯分布的对数几率替换传统确定性对数几率，通过最小化预测高斯分布和单位各向同性高斯之间的KL散度，同时解决温度缩放和流形逼近问题。

Result: 在CIFAR - 10和CIFAR - 100实验中，ZClassifier在鲁棒性、校准和潜在分离方面优于softmax分类器，还能用于分类器引导生成。

Conclusion: ZClassifier是一种有效的分类框架，能提高分类性能并用于分类器引导生成。

Abstract: We introduce a novel classification framework, ZClassifier, that replaces
conventional deterministic logits with diagonal Gaussian-distributed logits.
Our method simultaneously addresses temperature scaling and manifold
approximation by minimizing the Kullback-Leibler (KL) divergence between the
predicted Gaussian distributions and a unit isotropic Gaussian. This unifies
uncertainty calibration and latent control in a principled probabilistic
manner, enabling a natural interpretation of class confidence and geometric
consistency. Experiments on CIFAR-10 and CIFAR-100 show that ZClassifier
improves over softmax classifiers in robustness, calibration, and latent
separation. We also demonstrate its effectiveness for classifier-guided
generation by interpreting logits as Gaussian semantic potentials.

</details>


### [114] [First-of-its-kind AI model for bioacoustic detection using a lightweight associative memory Hopfield neural network](https://arxiv.org/abs/2507.10642)
*Andrew Gascoyne,Wendy Lomas*

Main category: cs.LG

TL;DR: 本文提出一种新AI模型解决生物声学数据处理难题，具有训练快、轻量、准确等优点，适用于多种设备，不局限特定物种。


<details>
  <summary>Details</summary>
Motivation: 解决被动声学监测设备产生的大量数据难以分析的问题，改进现有AI模型在训练数据、环境影响和硬件要求方面的不足。

Method: 使用透明、可解释的Hopfield神经网络的关联记忆来存储和检测信号，以进行物种分类。

Result: 训练迅速，处理10384条蝙蝠录音仅需5.4秒，内存占用小，精度达86%，与专家鉴定无分歧。

Conclusion: 提出的公平AI模型有望变革生物声学分析。

Abstract: A growing issue within conservation bioacoustics is the task of analysing the
vast amount of data generated from the use of passive acoustic monitoring
devices. In this paper, we present an alternative AI model which has the
potential to help alleviate this problem. Our model formulation addresses the
key issues encountered when using current AI models for bioacoustic analysis,
namely the: limited training data available; environmental impact, particularly
in energy consumption and carbon footprint of training and implementing these
models; and associated hardware requirements. The model developed in this work
uses associative memory via a transparent, explainable Hopfield neural network
to store signals and detect similar signals which can then be used to classify
species. Training is rapid ($3$\,ms), as only one representative signal is
required for each target sound within a dataset. The model is fast, taking only
$5.4$\,s to pre-process and classify all $10384$ publicly available bat
recordings, on a standard Apple MacBook Air. The model is also lightweight with
a small memory footprint of $144.09$\,MB of RAM usage. Hence, the low
computational demands make the model ideal for use on a variety of standard
personal devices with potential for deployment in the field via edge-processing
devices. It is also competitively accurate, with up to $86\%$ precision on the
dataset used to evaluate the model. In fact, we could not find a single case of
disagreement between model and manual identification via expert field guides.
Although a dataset of bat echolocation calls was chosen to demo this
first-of-its-kind AI model, trained on only two representative calls, the model
is not species specific. In conclusion, we propose an equitable AI model that
has the potential to be a game changer for fast, lightweight, sustainable,
transparent, explainable and accurate bioacoustic analysis.

</details>


### [115] [Ground-Compose-Reinforce: Tasking Reinforcement Learning Agents through Formal Language](https://arxiv.org/abs/2507.10741)
*Andrew C. Li,Toryn Q. Klassen,Andrew Wang,Parand A. Alamdari,Sheila A. McIlraith*

Main category: cs.LG

TL;DR: 提出Ground - Compose - Reinforce框架，实现从数据中进行形式语言接地并通过语言驱动强化学习智能体行为，实验显示该方法在有限数据下表现优于端到端数据驱动方法。


<details>
  <summary>Details</summary>
Motivation: 解决构建能通过语言与人类交互的情境智能体时，将语言在复杂感知和动作中接地的挑战，避免以往手动设计语言接地或整理大规模数据集的方式。

Method: 提出Ground - Compose - Reinforce神经符号框架，利用数据驱动学习避免特定领域元素的手动设计，借助组合式形式语言语义实现数据高效的接地和泛化。

Result: 在基于图像的网格世界和MuJoCo机器人领域实验中，该方法能在有限数据下将形式语言指令可靠地映射为行为，而端到端数据驱动方法失败。

Conclusion: Ground - Compose - Reinforce框架在语言接地和驱动智能体行为方面具有优势，能在有限数据下取得较好效果。

Abstract: Grounding language in complex perception (e.g. pixels) and action is a key
challenge when building situated agents that can interact with humans via
language. In past works, this is often solved via manual design of the language
grounding or by curating massive datasets relating language to elements of the
environment. We propose Ground-Compose-Reinforce, a neurosymbolic framework for
grounding formal language from data, and eliciting behaviours by directly
tasking RL agents through this language. By virtue of data-driven learning, our
framework avoids the manual design of domain-specific elements like reward
functions or symbol detectors. By virtue of compositional formal language
semantics, our framework achieves data-efficient grounding and generalization
to arbitrary language compositions. Experiments on an image-based gridworld and
a MuJoCo robotics domain show that our approach reliably maps formal language
instructions to behaviours with limited data while end-to-end, data-driven
approaches fail.

</details>


### [116] [A Benchmarking Framework for AI models in Automotive Aerodynamics](https://arxiv.org/abs/2507.10747)
*Kaustubh Tangsali,Rishikesh Ranade,Mohammad Amin Nabian,Alexey Kamenev,Peter Sharpe,Neil Ashton,Ram Cherukuri,Sanjay Choudhry*

Main category: cs.LG

TL;DR: 本文介绍NVIDIA PhysicsNeMo - CFD框架下的基准测试框架，用于评估汽车空气动力学预测AI模型，展示其对三个模型的评估，目标是推动相关领域发展。


<details>
  <summary>Details</summary>
Motivation: 系统评估汽车空气动力学预测AI模型的准确性、性能、可扩展性和泛化能力，提高对这些模型的理解和开发，加速该领域的研究与创新。

Method: 在开源NVIDIA PhysicsNeMo - CFD框架内构建基准测试框架，提供标准化方法比较AI模型，采用DrivAerML数据集对三个AI模型进行表面和体积流场预测评估，给出集成更多模型和数据集的指南。

Result: 框架可对AI模型进行评估，具有开放性和可扩展性，能纳入与CAE社区相关的多样指标。

Conclusion: 该基准测试研究有助于研究人员和行业专业人员选择、改进和推进AI驱动的空气动力学建模方法，促进汽车空气动力学领域开发更高效、准确和可解释的解决方案。

Abstract: In this paper, we introduce a benchmarking framework within the open-source
NVIDIA PhysicsNeMo-CFD framework designed to systematically assess the
accuracy, performance, scalability, and generalization capabilities of AI
models for automotive aerodynamics predictions. The open extensible framework
enables incorporation of a diverse set of metrics relevant to the
Computer-Aided Engineering (CAE) community. By providing a standardized
methodology for comparing AI models, the framework enhances transparency and
consistency in performance assessment, with the overarching goal of improving
the understanding and development of these models to accelerate research and
innovation in the field. To demonstrate its utility, the framework includes
evaluation of both surface and volumetric flow field predictions on three AI
models: DoMINO, X-MeshGraphNet, and FIGConvNet using the DrivAerML dataset. It
also includes guidelines for integrating additional models and datasets, making
it extensible for physically consistent metrics. This benchmarking study aims
to enable researchers and industry professionals in selecting, refining, and
advancing AI-driven aerodynamic modeling approaches, ultimately fostering the
development of more efficient, accurate, and interpretable solutions in
automotive aerodynamics

</details>


### [117] [Spatial Reasoners for Continuous Variables in Any Domain](https://arxiv.org/abs/2507.10768)
*Bart Pogodzinski,Christopher Wewer,Bernt Schiele,Jan Eric Lenssen*

Main category: cs.LG

TL;DR: 提出Spatial Reasoners框架用于连续变量空间推理，框架开源。


<details>
  <summary>Details</summary>
Motivation: 生成去噪模型用于多连续变量推理时，搭建推理基础设施需大量精力，需工具促进该领域研究。

Method: 开发Spatial Reasoners框架，提供易用接口控制变量映射、生成模型范式和推理策略。

Result: 成功开发Spatial Reasoners框架。

Conclusion: Spatial Reasoners框架可促进生成去噪模型在连续变量推理领域的研究。

Abstract: We present Spatial Reasoners, a software framework to perform spatial
reasoning over continuous variables with generative denoising models. Denoising
generative models have become the de-facto standard for image generation, due
to their effectiveness in sampling from complex, high-dimensional
distributions. Recently, they have started being explored in the context of
reasoning over multiple continuous variables. Providing infrastructure for
generative reasoning with such models requires a high effort, due to a wide
range of different denoising formulations, samplers, and inference strategies.
Our presented framework aims to facilitate research in this area, providing
easy-to-use interfaces to control variable mapping from arbitrary data domains,
generative model paradigms, and inference strategies. Spatial Reasoners are
openly available at https://spatialreasoners.github.io/

</details>


### [118] [A Generalizable Physics-Enhanced State Space Model for Long-Term Dynamics Forecasting in Complex Environments](https://arxiv.org/abs/2507.10792)
*Yuchen Wang,Hongjue Zhao,Haohong Lin,Enze Xu,Lifang He,Huajie Shao*

Main category: cs.LG

TL;DR: 本文提出Phy - SSM方法解决复杂环境下长期动态预测问题，实验表明其在长期插值和外推任务上优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂场景长期外推任务存在挑战，SSMs能捕捉序列数据长程依赖、建模连续动力系统，结合物理知识可提升泛化能力。

Method: 将部分已知系统动力学分解为已知和未知状态矩阵，集成到Phy - SSM单元；引入物理状态正则化项；理论分析解的唯一性。

Result: 在车辆运动预测、无人机状态预测和COVID - 19流行病学预测三个实际应用中，Phy - SSM在长期插值和外推任务上表现优于基线。

Conclusion: Phy - SSM方法在复杂环境长期动态预测中有优越性能，代码开源。

Abstract: This work aims to address the problem of long-term dynamic forecasting in
complex environments where data are noisy and irregularly sampled. While recent
studies have introduced some methods to improve prediction performance, these
approaches still face a significant challenge in handling long-term
extrapolation tasks under such complex scenarios. To overcome this challenge,
we propose Phy-SSM, a generalizable method that integrates partial physics
knowledge into state space models (SSMs) for long-term dynamics forecasting in
complex environments. Our motivation is that SSMs can effectively capture
long-range dependencies in sequential data and model continuous dynamical
systems, while the incorporation of physics knowledge improves generalization
ability. The key challenge lies in how to seamlessly incorporate partially
known physics into SSMs. To achieve this, we decompose partially known system
dynamics into known and unknown state matrices, which are integrated into a
Phy-SSM unit. To further enhance long-term prediction performance, we introduce
a physics state regularization term to make the estimated latent states align
with system dynamics. Besides, we theoretically analyze the uniqueness of the
solutions for our method. Extensive experiments on three real-world
applications, including vehicle motion prediction, drone state prediction, and
COVID-19 epidemiology forecasting, demonstrate the superior performance of
Phy-SSM over the baselines in both long-term interpolation and extrapolation
tasks. The code is available at https://github.com/511205787/Phy_SSM-ICML2025.

</details>


### [119] [Uncovering Causal Relation Shifts in Event Sequences under Out-of-Domain Interventions](https://arxiv.org/abs/2507.10809)
*Kazi Tasnim Zinat,Yun Zhou,Xiang Lyu,Yawei Wang,Zhicheng Liu,Panpan Xu*

Main category: cs.LG

TL;DR: 本文提出新因果框架处理域外干预下事件因果关系推断，设计无偏估计器和Transformer模型，实验显示方法优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有因果推断工作多未考虑域外干预对因果动态的影响，而现实中域外干预会显著改变因果关系。

Method: 提出新因果框架定义平均处理效应（ATE），设计无偏ATE估计器，使用基于Transformer的神经网络模型处理长时依赖和局部模式，并将域外干预信息融入过程建模。

Result: 在模拟和真实数据集上的实验表明，该方法在域外增强点过程的ATE估计和拟合优度方面优于基线。

Conclusion: 所提方法能有效处理域外干预下事件因果关系推断问题。

Abstract: Inferring causal relationships between event pairs in a temporal sequence is
applicable in many domains such as healthcare, manufacturing, and
transportation. Most existing work on causal inference primarily focuses on
event types within the designated domain, without considering the impact of
exogenous out-of-domain interventions. In real-world settings, these
out-of-domain interventions can significantly alter causal dynamics. To address
this gap, we propose a new causal framework to define average treatment effect
(ATE), beyond independent and identically distributed (i.i.d.) data in classic
Rubin's causal framework, to capture the causal relation shift between events
of temporal process under out-of-domain intervention. We design an unbiased ATE
estimator, and devise a Transformer-based neural network model to handle both
long-range temporal dependencies and local patterns while integrating
out-of-domain intervention information into process modeling. Extensive
experiments on both simulated and real-world datasets demonstrate that our
method outperforms baselines in ATE estimation and goodness-of-fit under
out-of-domain-augmented point processes.

</details>


### [120] [Semantic Context for Tool Orchestration](https://arxiv.org/abs/2507.10820)
*Robert Müller*

Main category: cs.LG

TL;DR: 本文表明语义上下文（SC）是稳健工具编排的基础组件，有三方面贡献，为构建编排代理提供全面指南。


<details>
  <summary>Details</summary>
Motivation: 探究语义上下文在工具编排中的作用，构建更高效、自适应和可扩展的编排代理。

Method: 运用上下文老虎机提供理论基础，引入SC - LinUCB；与大语言模型进行并行实证验证；提出FiReAct管道。

Result: SC - LinUCB能降低遗憾并在动态动作空间中良好适应；SC对静态和非静态环境下的上下文学习至关重要；基于SC的检索使大语言模型能有效编排大量工具。

Conclusion: 研究结果为构建更高效、自适应和可扩展的编排代理提供了全面指南。

Abstract: This paper demonstrates that Semantic Context (SC), leveraging descriptive
tool information, is a foundational component for robust tool orchestration.
Our contributions are threefold. First, we provide a theoretical foundation
using contextual bandits, introducing SC-LinUCB and proving it achieves lower
regret and adapts favourably in dynamic action spaces. Second, we provide
parallel empirical validation with Large Language Models, showing that SC is
critical for successful in-context learning in both static (efficient learning)
and non-stationary (robust adaptation) settings. Third, we propose the FiReAct
pipeline, and demonstrate on a benchmark with over 10,000 tools that SC-based
retrieval enables an LLM to effectively orchestrate over a large action space.
These findings provide a comprehensive guide to building more sample-efficient,
adaptive, and scalable orchestration agents.

</details>


### [121] [From Small to Large: A Graph Convolutional Network Approach for Solving Assortment Optimization Problems](https://arxiv.org/abs/2507.10834)
*Guokai Li,Pin Gao,Stefanus Jasin,Zizhuo Wang*

Main category: cs.LG

TL;DR: 本文探索用图卷积网络（GCN）解决混合多项logit选择模型下的约束品类优化问题，提出推理策略，在大小规模实例和无模型场景下实验验证有效性和高效性。


<details>
  <summary>Details</summary>
Motivation: 品类优化问题因组合和非线性特性通常为NP难问题，需寻找高效解决方法。

Method: 开发品类问题的图表示，训练GCN学习最优品类模式，基于GCN输出提出两个推理策略；将框架扩展到无模型场景。

Result: 在小规模实例上训练的GCN，其策略能在大规模实例上几秒内达90%以上最优性，性能和效率超现有启发式策略；在无模型场景实验也验证了策略有效性和高效性。

Conclusion: 图卷积网络可有效且高效解决约束品类优化问题，所提策略在不同场景均表现良好。

Abstract: Assortment optimization involves selecting a subset of substitutable products
(subject to certain constraints) to maximize the expected revenue. It is a
classic problem in revenue management and finds applications across various
industries. However, the problem is usually NP-hard due to its combinatorial
and non-linear nature. In this work, we explore how graph concolutional
networks (GCNs) can be leveraged to efficiently solve constrained assortment
optimization under the mixed multinomial logit choice model. We first develop a
graph representation of the assortment problem, then train a GCN to learn the
patterns of optimal assortments, and lastly propose two inference policies
based on the GCN's output. Due to the GCN's inherent ability to generalize
across inputs of varying sizes, we can use a GCN trained on small-scale
instances to facilitate large-scale instances. Extensive numerical experiments
demonstrate that given a GCN trained on small-scale instances (e.g., with 20
products), the proposed policies can achieve superior performance (90%+
optimality) on large-scale instances (with up to 2,000 products) within
seconds, which outperform existing heuristic policies in both performance and
efficiency. Furthermore, we extend our framework to a model-free setting where
the underlying choice model is unknown but transaction data is available. We
also conduct numerical experiments to demonstrate the effectiveness and
efficiency of our proposed policies in this setting.

</details>


### [122] [Collaboration Promotes Group Resilience in Multi-Agent RL](https://arxiv.org/abs/2111.06614)
*Ilai Shraga,Guy Azran,Matthias Gerstgrasser,Ofir Abu,Jeffrey S. Rosenschein,Sarah Keren*

Main category: cs.LG

TL;DR: 本文引入多智能体的群体弹性概念，假设协作有助于实现群体弹性并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注单智能体弹性，为使强化学习智能体在多智能体动态场景有效运行，需研究多智能体弹性。

Method: 提出群体弹性概念，假设协作是实现群体弹性的关键，通过评估不同协作协议检验假设。

Result: 所有被研究的协作方法比非协作方法实现了更高的群体弹性。

Conclusion: 协作对于在多智能体强化学习中实现群体弹性至关重要。

Abstract: To effectively operate in various dynamic scenarios, RL agents must be
resilient to unexpected changes in their environment. Previous work on this
form of resilience has focused on single-agent settings. In this work, we
introduce and formalize a multi-agent variant of resilience, which we term
group resilience. We further hypothesize that collaboration with other agents
is key to achieving group resilience; collaborating agents adapt better to
environmental perturbations in multi-agent reinforcement learning (MARL)
settings. We test our hypothesis empirically by evaluating different
collaboration protocols and examining their effect on group resilience. Our
experiments show that all the examined collaborative approaches achieve higher
group resilience than their non-collaborative counterparts.

</details>


### [123] [Offline Reinforcement Learning with Wasserstein Regularization via Optimal Transport Maps](https://arxiv.org/abs/2507.10843)
*Motoki Omura,Yusuke Mukuta,Kazuki Ota,Takayuki Osa,Tatsuya Harada*

Main category: cs.LG

TL;DR: 本文提出基于Wasserstein距离的离线强化学习方法，用ICNNs计算该距离，避免对抗训练，在D4RL基准数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决离线强化学习中分布偏移问题，现有方法多采用基于密度比的度量，而Wasserstein距离对分布外数据更鲁棒。

Method: 利用输入凸神经网络（ICNNs）对最优传输映射建模，以无判别器方式计算Wasserstein距离。

Result: 在D4RL基准数据集上，该方法性能与广泛使用的现有方法相当或更优。

Conclusion: 提出的基于Wasserstein距离的离线强化学习方法有效，代码开源。

Abstract: Offline reinforcement learning (RL) aims to learn an optimal policy from a
static dataset, making it particularly valuable in scenarios where data
collection is costly, such as robotics. A major challenge in offline RL is
distributional shift, where the learned policy deviates from the dataset
distribution, potentially leading to unreliable out-of-distribution actions. To
mitigate this issue, regularization techniques have been employed. While many
existing methods utilize density ratio-based measures, such as the
$f$-divergence, for regularization, we propose an approach that utilizes the
Wasserstein distance, which is robust to out-of-distribution data and captures
the similarity between actions. Our method employs input-convex neural networks
(ICNNs) to model optimal transport maps, enabling the computation of the
Wasserstein distance in a discriminator-free manner, thereby avoiding
adversarial training and ensuring stable learning. Our approach demonstrates
comparable or superior performance to widely used existing methods on the D4RL
benchmark dataset. The code is available at
https://github.com/motokiomura/Q-DOT .

</details>


### [124] [Visually grounded emotion regulation via diffusion models and user-driven reappraisal](https://arxiv.org/abs/2507.10861)
*Edoardo Pinzuti,Oliver Tüscher,André Ferreira Castro*

Main category: cs.LG

TL;DR: 提出将文本到图像扩散模型融入情绪调节过程，以视觉增强认知重评，实验表明AI辅助重评能显著降低负面情绪，多模态一致性增强调节效果。


<details>
  <summary>Details</summary>
Motivation: 传统认知重评干预对有创伤或抑郁的人因依赖高阶认知和语言过程而效果受限，需新方法。

Method: 引入系统让用户通过语音重评负面图像，用微调的IP - adapter稳定扩散模型将重评转化为视觉化内容，进行有AI视觉反馈和无AI视觉反馈的被试内实验。

Result: AI辅助重评比非AI和控制条件显著降低负面情绪，参与者重评与生成图像的情感一致性与情感缓解相关。

Conclusion: 生成式视觉输入可支持认知重评，为生成式AI、情感计算和治疗技术交叉领域开辟新方向。

Abstract: Cognitive reappraisal is a key strategy in emotion regulation, involving
reinterpretation of emotionally charged stimuli to alter affective responses.
Despite its central role in clinical and cognitive science, real-world
reappraisal interventions remain cognitively demanding, abstract, and primarily
verbal. This reliance on higher-order cognitive and linguistic processes is
often impaired in individuals with trauma or depression, limiting the
effectiveness of standard approaches. Here, we propose a novel, visually based
augmentation of cognitive reappraisal by integrating large-scale text-to-image
diffusion models into the emotional regulation process. Specifically, we
introduce a system in which users reinterpret emotionally negative images via
spoken reappraisals, which are transformed into supportive, emotionally
congruent visualizations using stable diffusion models with a fine-tuned
IP-adapter. This generative transformation visually instantiates users'
reappraisals while maintaining structural similarity to the original stimuli,
externalizing and reinforcing regulatory intent. To test this approach, we
conducted a within-subject experiment (N = 20) using a modified cognitive
emotion regulation (CER) task. Participants reappraised or described aversive
images from the International Affective Picture System (IAPS), with or without
AI-generated visual feedback. Results show that AI-assisted reappraisal
significantly reduced negative affect compared to both non-AI and control
conditions. Further analyses reveal that sentiment alignment between
participant reappraisals and generated images correlates with affective relief,
suggesting that multimodal coherence enhances regulatory efficacy. These
findings demonstrate that generative visual input can support cogitive
reappraisal and open new directions at the intersection of generative AI,
affective computing, and therapeutic technology.

</details>


### [125] [GALDS: A Graph-Autoencoder-based Latent Dynamics Surrogate model to predict neurite material transport](https://arxiv.org/abs/2507.10871)
*Tsung Yeh Hsieh,Yongjie Jessica Zhang*

Main category: cs.LG

TL;DR: 提出GALDS模型简化神经树物质传输模拟，测试效果好且速度快。


<details>
  <summary>Details</summary>
Motivation: 准确模拟神经树物质传输有计算挑战，传统方法耗时长、资源需求大，需优化。

Method: 提出GALDS模型，用图自编码器编码网络信息成全局图，结合受Neural ODE启发的图潜空间系统动力学模型预测系统动力学。

Result: 在八个未见几何结构和四个异常传输示例中，平均相对误差3%，最大相对误差<8%，速度比之前替代模型方法快10倍。

Conclusion: GALDS模型有效，可减少训练数据需求，缓解循环神经网络误差积累问题。

Abstract: Neurons exhibit intricate geometries within their neurite networks, which
play a crucial role in processes such as signaling and nutrient transport.
Accurate simulation of material transport in the networks is essential for
understanding these biological phenomena but poses significant computational
challenges because of the complex tree-like structures involved. Traditional
approaches are time-intensive and resource-demanding, yet the inherent
properties of neuron trees, which consists primarily of pipes with steady-state
parabolic velocity profiles and bifurcations, provide opportunities for
computational optimization. To address these challenges, we propose a
Graph-Autoencoder-based Latent Dynamics Surrogate (GALDS) model, which is
specifically designed to streamline the simulation of material transport in
neural trees. GALDS employs a graph autoencoder to encode latent
representations of the network's geometry, velocity fields, and concentration
profiles. These latent space representations are then assembled into a global
graph, which is subsequently used to predict system dynamics in the latent
space via a trained graph latent space system dynamic model, inspired by the
Neural Ordinary Differential Equations (Neural ODEs) concept. The integration
of an autoencoder allows for the use of smaller graph neural network models
with reduced training data requirements. Furthermore, the Neural ODE component
effectively mitigates the issue of error accumulation commonly encountered in
recurrent neural networks. The effectiveness of the GALDS model is demonstrated
through results on eight unseen geometries and four abnormal transport
examples, where our approach achieves mean relative error of 3% with maximum
relative error <8% and demonstrates a 10-fold speed improvement compared to
previous surrogate model approaches.

</details>


### [126] [Domain-Adaptive Small Language Models for Structured Tax Code Prediction](https://arxiv.org/abs/2507.10880)
*Souvik Nath,Sumit Wadhwa,Luiz Perez*

Main category: cs.LG

TL;DR: 本文提出用带编解码器架构的领域自适应小语言模型预测产品和服务税码，实验证明其性能优越且可扩展到其他税码。


<details>
  <summary>Details</summary>
Motivation: 跨国公司处理交易需遵循不同地区税收法规，准确确定产品和服务税码可避免税务处罚，当前NLP研究对此领域探索较少。

Method: 采用带编解码器架构的小语言模型处理非结构化产品和服务数据，以捕捉税码层次依赖关系。

Result: 编码器 - 解码器小语言模型能成功应用于结构化税码顺序预测，在HSN上表现优于扁平分类器，在结构化序列生成任务中优于仅解码器和仅编码器架构。

Conclusion: 该方法可扩展到其他政府规定的税务商品代码。

Abstract: Every day, multinational firms process thousands of transactions, each of
which must adhere to tax regulations that vary by jurisdiction and are often
nuanced. The determination of product and service tax codes, such as HSN or SAC
is a major use case in Tax compliance. An accurate determination of such codes
is imperative to avoid any tax penalties. This paper proposes a domain-adaptive
small language model (SLM) with an encoder-decoder architecture for the
enhanced prediction of product and service tax codes. In this approach, we
address the problem of predicting hierarchical tax code sequences using
unstructured product and services data. We employ an SLM based upon
encoder-decoder architecture as this enables sequential generation of tax codes
to capture the hierarchical dependencies present within the tax codes. Our
experiments demonstrate that encoder-decoder SLMs can be successfully applied
to the sequential prediction of structured tax codes, a domain that remains
comparatively unexplored in current NLP research. In this paper, we demonstrate
the superior performance of the domain-adaptive encoder-decoder SLMs over flat
classifiers when applied to the Harmonized System of Nomenclature (HSN), and
achieve superior results compared to decoder-only and encoder-only
architectures for structured sequence generation tasks. This approach can also
be scaled to other government-mandated tax commodity codes, such as United
Nations Standard Products and Services Codes (UNSPSC), or Brazil's Nomenclatura
Comum do Mercosul (NCM).

</details>


### [127] [Learning from Imperfect Data: Robust Inference of Dynamic Systems using Simulation-based Generative Model](https://arxiv.org/abs/2507.10884)
*Hyunwoo Cho,Hyeontae Jo,Hyung Ju Hwang*

Main category: cs.LG

TL;DR: 提出SiGMoID模型用于非线性动态系统推理，结合两种方法，经实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 解决非线性动态模型在数据有噪声、稀疏或部分可观测时系统推理的难题。

Method: 结合物理信息神经网络与超网络构建ODE求解器，以及Wasserstein生成对抗网络估计ODE参数。

Result: SiGMoID能量化数据噪声、估计系统参数、推断未观测的系统组件。

Conclusion: SiGMoID有效，在多领域有广泛适用性，可发现完整系统动态。

Abstract: System inference for nonlinear dynamic models, represented by ordinary
differential equations (ODEs), remains a significant challenge in many fields,
particularly when the data are noisy, sparse, or partially observable. In this
paper, we propose a Simulation-based Generative Model for Imperfect Data
(SiGMoID) that enables precise and robust inference for dynamic systems. The
proposed approach integrates two key methods: (1) physics-informed neural
networks with hyper-networks that constructs an ODE solver, and (2) Wasserstein
generative adversarial networks that estimates ODE parameters by effectively
capturing noisy data distributions. We demonstrate that SiGMoID quantifies data
noise, estimates system parameters, and infers unobserved system components.
Its effectiveness is validated validated through realistic experimental
examples, showcasing its broad applicability in various domains, from
scientific research to engineered systems, and enabling the discovery of full
system dynamics.

</details>


### [128] [How to Protect Models against Adversarial Unlearning?](https://arxiv.org/abs/2507.10886)
*Patryk Jasiorski,Marek Klonowski,Michał Woźniak*

Main category: cs.LG

TL;DR: 本文探讨对抗性遗忘问题，提出保护模型性能免受遗忘副作用影响的新方法。


<details>
  <summary>Details</summary>
Motivation: AI模型需遗忘以满足法律要求及去除不良内容等，但遗忘可能有副作用，且存在恶意方发起遗忘请求破坏模型性能的情况。

Method: 未明确提及具体方法，主要聚焦研究对抗性遗忘现象及影响因素。

Result: 得出现象和对手能力依赖多种因素，提出保护模型性能免受副作用影响的新方法。

Conclusion: 新方法可保护模型性能，应对自发遗忘和对抗行为带来的副作用。

Abstract: AI models need to be unlearned to fulfill the requirements of legal acts such
as the AI Act or GDPR, and also because of the need to remove toxic content,
debiasing, the impact of malicious instances, or changes in the data
distribution structure in which a model works. Unfortunately, removing
knowledge may cause undesirable side effects, such as a deterioration in model
performance. In this paper, we investigate the problem of adversarial
unlearning, where a malicious party intentionally sends unlearn requests to
deteriorate the model's performance maximally. We show that this phenomenon and
the adversary's capabilities depend on many factors, primarily on the backbone
model itself and strategy/limitations in selecting data to be unlearned. The
main result of this work is a new method of protecting model performance from
these side effects, both in the case of unlearned behavior resulting from
spontaneous processes and adversary actions.

</details>


### [129] [Outbound Modeling for Inventory Management](https://arxiv.org/abs/2507.10890)
*Riccardo Savorgnan,Udaya Ghai,Carson Eisenach,Dean Foster*

Main category: cs.LG

TL;DR: 研究库存仓库订单完成数量和运输成本预测问题，提出概率预测模型并给出验证方案，初步结果显示模型在分布内准确。


<details>
  <summary>Details</summary>
Motivation: 准确建模库存订单履行和运输成本过程对区域库存规划至关重要，现有模拟方法不可微且慢，需新模型。

Method: 将其构建为概率预测问题，对各仓库出库和运输成本联合分布建模，提出利用生产系统评估模型的验证方案。

Result: 初步结果表明模型在分布内设置下具有准确性。

Conclusion: 所提模型可用于库存订单完成数量和运输成本预测，验证方案有助于在强化学习环境评估模型，能应对分布外情况。

Abstract: We study the problem of forecasting the number of units fulfilled (or
``drained'') from each inventory warehouse to meet customer demand, along with
the associated outbound shipping costs. The actual drain and shipping costs are
determined by complex production systems that manage the planning and execution
of customers' orders fulfillment, i.e. from where and how to ship a unit to be
delivered to a customer. Accurately modeling these processes is critical for
regional inventory planning, especially when using Reinforcement Learning (RL)
to develop control policies. For the RL usecase, a drain model is incorporated
into a simulator to produce long rollouts, which we desire to be
differentiable. While simulating the calls to the internal software systems can
be used to recover this transition, they are non-differentiable and too slow
and costly to run within an RL training environment. Accordingly, we frame this
as a probabilistic forecasting problem, modeling the joint distribution of
outbound drain and shipping costs across all warehouses at each time period,
conditioned on inventory positions and exogenous customer demand. To ensure
robustness in an RL environment, the model must handle out-of-distribution
scenarios that arise from off-policy trajectories. We propose a validation
scheme that leverages production systems to evaluate the drain model on
counterfactual inventory states induced by RL policies. Preliminary results
demonstrate the model's accuracy within the in-distribution setting.

</details>


### [130] [Class-Proportional Coreset Selection for Difficulty-Separable Data](https://arxiv.org/abs/2507.10904)
*Elisa Tsai,Haizhong Zheng,Atul Prakash*

Main category: cs.LG

TL;DR: 本文指出多数现有单样本核心集选择方法忽略数据难度的类间差异，提出类难度可分离性系数，引入类比例采样策略变体，在多数据集上达最优数据效率，强调显式建模类难度可分离性对数据剪枝的重要性。


<details>
  <summary>Details</summary>
Motivation: 多数现有单样本核心集选择方法隐式假设数据难度在类内均匀，忽略不同类间数据难度的变化，作者希望解决该问题以提升机器学习系统的数据效率。

Method: 提出类难度可分离性概念并引入类难度可分离性系数（CDSC），引入多种采样策略的类比例变体。

Result: 在五个不同数据集上评估，类比例变体方法达到最优数据效率，如在CTU - 13数据集上，99%剪枝率时CCS - CP变体比类无关的CCS基线表现更稳定。

Conclusion: 显式建模类难度可分离性可实现更有效、鲁棒和可泛化的数据剪枝，尤其在高风险场景中。

Abstract: High-quality training data is essential for building reliable and efficient
machine learning systems. One-shot coreset selection addresses this by pruning
the dataset while maintaining or even improving model performance, often
relying on training-dynamics-based data difficulty scores. However, most
existing methods implicitly assume class-wise homogeneity in data difficulty,
overlooking variation in data difficulty across different classes.
  In this work, we challenge this assumption by showing that, in domains such
as network intrusion detection and medical imaging, data difficulty often
clusters by class. We formalize this as class-difficulty separability and
introduce the Class Difficulty Separability Coefficient (CDSC) as a
quantitative measure. We demonstrate that high CDSC values correlate with
performance degradation in class-agnostic coreset methods, which tend to
overrepresent easy majority classes while neglecting rare but informative ones.
  To address this, we introduce class-proportional variants of multiple
sampling strategies. Evaluated on five diverse datasets spanning security and
medical domains, our methods consistently achieve state-of-the-art data
efficiency. For instance, on CTU-13, at an extreme 99% pruning rate, a
class-proportional variant of Coverage-centric Coreset Selection (CCS-CP) shows
remarkable stability, with accuracy dropping only 2.58%, precision 0.49%, and
recall 0.19%. In contrast, the class-agnostic CCS baseline, the next best
method, suffers sharper declines of 7.59% in accuracy, 4.57% in precision, and
4.11% in recall.
  We further show that aggressive pruning enhances generalization in noisy,
imbalanced, and large-scale datasets. Our results underscore that explicitly
modeling class-difficulty separability leads to more effective, robust, and
generalizable data pruning, particularly in high-stakes scenarios.

</details>


### [131] [Diffusion Decoding for Peptide De Novo Sequencing](https://arxiv.org/abs/2507.10955)
*Chi-en Amy Tai,Alexander Wong*

Main category: cs.LG

TL;DR: 本文探讨用扩散解码器改进肽从头测序，实验发现特定设计和损失函数可提升氨基酸召回率，显示扩散解码器潜力。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法在肽从头测序中存在级联错误和无法有效利用高置信区域的问题，需要新方法解决。

Method: 研究适用于离散数据域的扩散解码器，实验三种不同扩散解码器设计、背包束搜索和多种损失函数。

Result: 背包束搜索未提升性能指标，简单替换变压器解码器会降低性能，最佳扩散解码器设计搭配DINOISER损失函数使氨基酸召回率相比基线模型提高0.373。

Conclusion: 扩散解码器不仅能增强模型敏感性，还能推动肽从头测序显著进步。

Abstract: Peptide de novo sequencing is a method used to reconstruct amino acid
sequences from tandem mass spectrometry data without relying on existing
protein sequence databases. Traditional deep learning approaches, such as
Casanovo, mainly utilize autoregressive decoders and predict amino acids
sequentially. Subsequently, they encounter cascading errors and fail to
leverage high-confidence regions effectively. To address these issues, this
paper investigates using diffusion decoders adapted for the discrete data
domain. These decoders provide a different approach, allowing sequence
generation to start from any peptide segment, thereby enhancing prediction
accuracy. We experiment with three different diffusion decoder designs,
knapsack beam search, and various loss functions. We find knapsack beam search
did not improve performance metrics and simply replacing the transformer
decoder with a diffusion decoder lowered performance. Although peptide
precision and recall were still 0, the best diffusion decoder design with the
DINOISER loss function obtained a statistically significant improvement in
amino acid recall by 0.373 compared to the baseline autoregressive
decoder-based Casanovo model. These findings highlight the potential of
diffusion decoders to not only enhance model sensitivity but also drive
significant advancements in peptide de novo sequencing.

</details>


### [132] [Physics-Informed Neural Networks For Semiconductor Film Deposition: A Review](https://arxiv.org/abs/2507.10983)
*Tao Han,Zahra Taheri,Hyunwoong Ko*

Main category: cs.LG

TL;DR: 本文全面综述半导体薄膜沉积过程的机器学习应用，分析现状并提出整合PINNs的新研究方向，以提升半导体制造效率。


<details>
  <summary>Details</summary>
Motivation: 半导体薄膜沉积过程复杂需精准控制，PINNs在解决相关问题上有潜力，需全面了解ML应用情况。

Method: 通过主题分析，研究现有ML方法在半导体薄膜沉积中的应用，探讨PINN嵌入物理知识的策略。

Result: 确定了关键趋势、现有局限和研究空白，讨论了PINN方法。

Conclusion: 提出整合PINNs优势的新研究方向，为未来研究指明路径，有望提升半导体制造精度、可扩展性和运营效率。

Abstract: Semiconductor manufacturing relies heavily on film deposition processes, such
as Chemical Vapor Deposition and Physical Vapor Deposition. These complex
processes require precise control to achieve film uniformity, proper adhesion,
and desired functionality. Recent advancements in Physics-Informed Neural
Networks (PINNs), an innovative machine learning (ML) approach, have shown
significant promise in addressing challenges related to process control,
quality assurance, and predictive modeling within semiconductor film deposition
and other manufacturing domains. This paper provides a comprehensive review of
ML applications targeted at semiconductor film deposition processes. Through a
thematic analysis, we identify key trends, existing limitations, and research
gaps, offering insights into both the advantages and constraints of current
methodologies. Our structured analysis aims to highlight the potential
integration of these ML techniques to enhance interpretability, accuracy, and
robustness in film deposition processes. Additionally, we examine
state-of-the-art PINN methods, discussing strategies for embedding physical
knowledge, governing laws, and partial differential equations into advanced
neural network architectures tailored for semiconductor manufacturing. Based on
this detailed review, we propose novel research directions that integrate the
strengths of PINNs to significantly advance film deposition processes. The
contributions of this study include establishing a clear pathway for future
research in integrating physics-informed ML frameworks, addressing existing
methodological gaps, and ultimately improving precision, scalability, and
operational efficiency within semiconductor manufacturing.

</details>


### [133] [StellarF: A Lora-Adapter Integrated Large Model Framework for Stellar Flare Forecasting with Historical & Statistical Data](https://arxiv.org/abs/2507.10986)
*Tianyu Su,Zhiqiang Zou,Ali Luo,Xiao Kong,Qingyu Lu,Min Li*

Main category: cs.LG

TL;DR: 提出名为StellarF的大模型用于恒星耀斑预测，在自建数据集上表现优异，建立新方法框架。


<details>
  <summary>Details</summary>
Motivation: 现有恒星耀斑预测领域受耀斑事件记录稀疏和缺乏特定领域大规模预测模型的限制。

Method: 引入StellarF模型，利用LoRA和Adapter技术进行参数高效学习，集成耀斑统计信息模块和历史耀斑记录模块实现多尺度模式识别。

Result: 在自建数据集上，StellarF与现有方法相比达到了最先进的性能。

Conclusion: 所提出的预测范式为天体物理研究和跨学科应用建立了新的方法论框架。

Abstract: Stellar flare forecasting, a critical research frontier in astronomy, offers
profound insights into stellar activity. However, the field is constrained by
both the sparsity of recorded flare events and the absence of domain-specific
large-scale predictive models. To address these challenges, this study
introduces StellarF (Stellar Flare Forecasting), a novel large model that
leverages Low-Rank (LoRA) and Adapter techniques to parameter-efficient
learning for stellar flare forecasting. At its core, StellarF integrates an
flare statistical information module with a historical flare record module,
enabling multi-scale pattern recognition from observational data. Extensive
experiments on our self-constructed datasets (derived from Kepler and TESS
light curves) demonstrate that StellarF achieves state-of-the-art performance
compared to existing methods. The proposed prediction paradigm establishes a
novel methodological framework for advancing astrophysical research and
cross-disciplinary applications.

</details>


### [134] [High-Throughput Distributed Reinforcement Learning via Adaptive Policy Synchronization](https://arxiv.org/abs/2507.10990)
*Rodney Lafuente-Mercado*

Main category: cs.LG

TL;DR: 提出轻量级、与学习器无关的分布式环境执行接口ClusterEnv及Adaptive Actor Policy Synchronization (AAPS) 机制，实验证明AAPS样本效率高且权重更新少。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习工作负载扩展框架将模拟、学习逻辑和编排纠缠成整体系统，限制了模块化和可重用性。

Method: 提出ClusterEnv接口，引入DETACH模式将模拟与训练解耦；提出AAPS机制解决分布式执行中的策略陈旧问题。

Result: ClusterEnv能很好集成到现有强化学习流程，支持多种策略方法且代码改动小；实验表明AAPS在离散控制任务中样本效率高，权重更新显著减少。

Conclusion: ClusterEnv和AAPS机制有效解决了现有强化学习框架的问题，提升了分布式环境执行的效率。

Abstract: Scaling reinforcement learning (RL) workloads often requires distributing
environment simulation across compute clusters. Existing frameworks entangle
simulation, learning logic, and orchestration into monolithic systems, limiting
modularity and reusability. We present ClusterEnv, a lightweight,
learner-agnostic interface for distributed environment execution that mirrors
the Gymnasium API. ClusterEnv introduces the DETACH pattern, which decouples
simulation from training by offloading reset() and step() operations to remote
workers while keeping learning centralized. To address policy staleness in
distributed execution, we propose Adaptive Actor Policy Synchronization (AAPS),
a divergence-triggered update mechanism that reduces synchronization overhead
without sacrificing performance. ClusterEnv integrates cleanly into existing RL
pipelines, supports both on-policy and off-policy methods, and requires minimal
code changes. Experiments on discrete control tasks demonstrate that AAPS
achieves high sample efficiency with significantly fewer weight updates. Source
code is available at https://github.com/rodlaf/ClusterEnv.

</details>


### [135] [Misalignment from Treating Means as Ends](https://arxiv.org/abs/2507.10995)
*Henrik Marklund,Alex Infanger,Benjamin Van Roy*

Main category: cs.LG

TL;DR: 奖励函数常不完善，混淆工具性和终端目标会导致严重的目标不一致，分析了相关问题。


<details>
  <summary>Details</summary>
Motivation: 指出奖励函数常因人类对目标的认知而被扭曲，需要研究其影响。

Method: 构建简单示例，展示混淆工具性和终端目标导致的严重目标不一致。

Result: 优化错误指定的奖励函数，用真实奖励函数衡量时表现不佳，提炼出强化学习对目标混淆高度敏感的环境的本质属性。

Conclusion: 探讨该问题在常见奖励学习方法及现实环境中的产生和表现。

Abstract: Reward functions, learned or manually specified, are rarely perfect. Instead
of accurately expressing human goals, these reward functions are often
distorted by human beliefs about how best to achieve those goals. Specifically,
these reward functions often express a combination of the human's terminal
goals -- those which are ends in themselves -- and the human's instrumental
goals -- those which are means to an end. We formulate a simple example in
which even slight conflation of instrumental and terminal goals results in
severe misalignment: optimizing the misspecified reward function results in
poor performance when measured by the true reward function. This example
distills the essential properties of environments that make reinforcement
learning highly sensitive to conflation of instrumental and terminal goals. We
discuss how this issue can arise with a common approach to reward learning and
how it can manifest in real environments.

</details>


### [136] [Crafting Imperceptible On-Manifold Adversarial Attacks for Tabular Data](https://arxiv.org/abs/2507.10998)
*Zhipeng He,Alexander Stevens,Chun Ouyang,Johannes De Smedt,Alistair Barros,Catarina Moreira*

Main category: cs.LG

TL;DR: 提出用混合输入变分自编码器（VAE）的潜在空间扰动框架生成表格数据对抗样本，评估显示优于传统方法和其他VAE方法。


<details>
  <summary>Details</summary>
Motivation: 表格数据因特征异质性，在对抗攻击中缺乏直观相似性度量，传统基于梯度方法生成的对抗样本易偏离原数据分布，难以生成难以察觉的修改。

Method: 提出使用混合输入VAE的潜在空间扰动框架，将分类嵌入和数值特征集成到统一潜在流形，指定IDSR衡量对抗样本比例。

Result: 在六个公开数据集和三种模型架构上评估，该方法异常值率更低，性能更一致。

Conclusion: 基于VAE的攻击依赖重建质量，在有足够训练数据时实用，强调流形上扰动对表格数据对抗攻击的重要性。

Abstract: Adversarial attacks on tabular data present fundamental challenges distinct
from image or text domains due to the heterogeneous nature of mixed categorical
and numerical features. Unlike images where pixel perturbations maintain visual
similarity, tabular data lacks intuitive similarity metrics, making it
difficult to define imperceptible modifications. Additionally, traditional
gradient-based methods prioritise $\ell_p$-norm constraints, often producing
adversarial examples that deviate from the original data distributions, making
them detectable. We propose a latent space perturbation framework using a
mixed-input Variational Autoencoder (VAE) to generate imperceptible adversarial
examples. The proposed VAE integrates categorical embeddings and numerical
features into a unified latent manifold, enabling perturbations that preserve
statistical consistency. We specify In-Distribution Success Rate (IDSR) to
measure the proportion of adversarial examples that remain statistically
indistinguishable from the input distribution. Evaluation across six publicly
available datasets and three model architectures demonstrates that our method
achieves substantially lower outlier rates and more consistent performance
compared to traditional input-space attacks and other VAE-based methods adapted
from image domain approaches. Our comprehensive analysis includes
hyperparameter sensitivity, sparsity control mechanisms, and generative
architectural comparisons, revealing that VAE-based attacks depend critically
on reconstruction quality but offer superior practical utility when sufficient
training data is available. This work highlights the importance of on-manifold
perturbations for realistic adversarial attacks on tabular data, offering a
robust approach for practical deployment. The source code can be accessed
through https://github.com/ZhipengHe/VAE-TabAttack.

</details>


### [137] [AdaMuon: Adaptive Muon Optimizer](https://arxiv.org/abs/2507.11005)
*Chongjie Si,Debing Zhang,Wei Shen*

Main category: cs.LG

TL;DR: 提出基于Muon优化器的AdaMuon自适应学习率框架，实验表明其优于原Muon。


<details>
  <summary>Details</summary>
Motivation: 在Muon优化器基础上进一步提升大规模模型训练效率。

Method: 为Muon增加两个相互依赖模块，即逐参数二阶矩调制和RMS对齐重缩放。

Result: 在多模型规模和学习率机制下，AdaMuon始终优于原Muon，收敛更快且保持训练稳定。

Conclusion: AdaMuon无额外调优负担，可无缝集成到现有Muon训练流程。

Abstract: We propose AdaMuon, an adaptive learning-rate framework built upon the
recently validated Muon optimizer, which has demonstrated substantial
efficiency gains over AdamW in large-scale model training. AdaMuon augments
Muon with two mutually dependent modules: (1) a per-parameter second-moment
modulation that captures orthogonal gradient updates to ensure update-level
adaptivity, and (2) a RMS-aligned rescaling that regulates the overall update
magnitude by aligning it with the intrinsic structure of the parameter space.
Empirical results on multiple model scales and learning-rate regimes confirm
that AdaMuon consistently outperforms the original Muon, delivering higher
acceleration in convergence while maintaining training stability. Our method
introduces no additional tuning burden and can be seamlessly integrated into
existing Muon training pipelines.

</details>


### [138] [Leveraging Advanced Machine Learning to Predict Turbulence Dynamics from Temperature Observations at an Experimental Prescribed Fire](https://arxiv.org/abs/2507.11012)
*Dipak Dulal,Joseph J. Charney,Michael R. Gallagher,Pitambar Acharya,Carmeliza Navasca,Nicholas S. Skowronski*

Main category: cs.LG

TL;DR: 研究利用新泽西松林实验性规定燃烧的数据，用机器学习模型通过温度数据预测湍动能，结果显示模型预测较成功，为理解火环境提供新方法。


<details>
  <summary>Details</summary>
Motivation: 探索能否用更易获取的温度数据预测湍动能，识别火环境中温度与气流过程的新关系，改进火灾操作策略和模型预测。

Method: 利用10Hz同步收集的温度和湍流数据，采用深度神经网络、随机森林回归、梯度提升和高斯过程回归等机器学习模型，进行数据可视化和相关性分析。

Result: 尽管预测变量与目标变量相关性弱，但各机器学习模型实现了更准确的湍动能预测，回归模型效果显著。

Conclusion: 研究提供了识别火环境中温度与气流新关系的数值方法，凸显了机器学习技术在分析火环境复杂大数据集的价值。

Abstract: This study explores the potential for predicting turbulent kinetic energy
(TKE) from more readily acquired temperature data using temperature profiles
and turbulence data collected concurrently at 10 Hz during a small experimental
prescribed burn in the New Jersey Pine Barrens. Machine learning models,
including Deep Neural Networks, Random Forest Regressor, Gradient Boosting, and
Gaussian Process Regressor, were employed to assess the potential to predict
TKE from temperature perturbations and explore temporal and spatial dynamics of
correlations. Data visualization and correlation analyses revealed patterns and
relationships between thermocouple temperatures and TKE, providing insight into
the underlying dynamics. More accurate predictions of TKE were achieved by
employing various machine learning models despite a weak correlation between
the predictors and the target variable. The results demonstrate significant
success, particularly from regression models, in accurately predicting the TKE.
The findings of this study demonstrate a novel numerical approach to
identifying new relationships between temperature and airflow processes in and
around the fire environment. These relationships can help refine our
understanding of combustion environment processes and the coupling and
decoupling of fire environment processes necessary for improving fire
operations strategy and fire and smoke model predictions. The findings of this
study additionally highlight the valuable role of machine learning techniques
in analyzing the complex large datasets of the fire environments, showcasing
their potential to advance fire research and management practices.

</details>


### [139] [First-Order Error Matters: Accurate Compensation for Quantized Large Language Models](https://arxiv.org/abs/2507.11017)
*Xingyu Zheng,Haotong Qin,Yuye Li,Jiakai Wang,Jinyang Guo,Michele Magno,Xianglong Liu*

Main category: cs.LG

TL;DR: 本文提出新的PTQ方法FOEM，通过引入一阶梯度项改进量化误差补偿，实验表明其性能优于经典GPTQ方法，还能与其他技术集成进一步提升效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于补偿的权重校准方法依赖二阶泰勒展开，假设一阶项可忽略，但渐进补偿过程会引入一阶偏差，该假设不成立，需改进。

Method: 提出FOEM方法，明确引入一阶梯度项，直接计算潜权重和全精度权重差值近似梯度，利用预计算的Cholesky因子实时恢复Hessian子矩阵的逆。

Result: 在多种模型和基准测试中，FOEM始终优于经典GPTQ方法，如在3位仅权重量化中降低Llama3 - 8B困惑度，提高Llama3 - 70B的5次射击MMLU准确率；能与其他技术集成进一步提升。

Conclusion: FOEM是一种有效的PTQ方法，可提高大语言模型量化效果，缩小与全精度基准的准确率差距。

Abstract: Post-training quantization (PTQ) offers an efficient approach to compressing
large language models (LLMs), significantly reducing memory access and
computational costs. Existing compensation-based weight calibration methods
often rely on a second-order Taylor expansion to model quantization error,
under the assumption that the first-order term is negligible in well-trained
full-precision models. However, we reveal that the progressive compensation
process introduces accumulated first-order deviations between latent weights
and their full-precision counterparts, making this assumption fundamentally
flawed. To address this, we propose FOEM, a novel PTQ method that explicitly
incorporates first-order gradient terms to improve quantization error
compensation. FOEM approximates gradients by directly computing the difference
between latent and full-precision weights, avoiding the high cost and limited
generalization of backpropagation-based gradient computation. This approach
introduces minimal additional computational overhead. Moreover, FOEM leverages
precomputed Cholesky factors to efficiently recover the inverse of Hessian
submatrices in real time. Extensive experiments across a wide range of models
and benchmarks demonstrate that FOEM consistently outperforms the classical
GPTQ method. In 3-bit weight-only quantization, FOEM reduces the perplexity of
Llama3-8B by 89.6%, and improves the 5-shot MMLU accuracy of Llama3-70B from
51.7% to 74.9%, approaching the full-precision performance of 78.6%.
Furthermore, FOEM can be seamlessly integrated with advanced techniques such as
GPTAQ and SpinQuant, yielding additional improvements under the challenging
W4A4KV4 setting, and further narrowing the accuracy gap with full-precision
baselines beyond what current state-of-the-art methods achieve. The code is
available at https://github.com/Xingyu-Zheng/FOEM.

</details>


### [140] [Relative Entropy Pathwise Policy Optimization](https://arxiv.org/abs/2507.11019)
*Claas Voelcker,Axel Brunnbauer,Marcel Hussing,Michal Nauman,Pieter Abbeel,Eric Eaton,Radu Grosu,Amir-massoud Farahmand,Igor Gilitschenski*

Main category: cs.LG

TL;DR: 本文提出REPPO算法，结合路径策略梯度与标准策略学习优点，在实验中表现良好。


<details>
  <summary>Details</summary>
Motivation: 分数函数策略梯度方差高影响训练稳定性，路径策略梯度依赖难以训练的动作条件价值函数，需构建基于价值梯度的在线策略算法。

Method: 探讨构建基于价值梯度的在线策略算法，平衡探索的随机策略和稳定训练的约束策略更新，评估促进准确价值函数学习的架构组件，提出REPPO算法。

Result: REPPO算法在两个标准GPU并行基准测试实验中，降低样本需求、时钟时间和内存占用，且超参数鲁棒性高。

Conclusion: REPPO算法有效结合路径策略梯度的样本效率和标准在线策略学习的简单性与低内存占用。

Abstract: Score-function policy gradients have delivered strong results in
game-playing, robotics and language-model fine-tuning. Yet its high-variance
often undermines training stability. On the other hand, pathwise policy
gradients alleviate the training variance, but are reliable only when driven by
an accurate action-conditioned value function which is notoriously hard to
train without relying on past off-policy data. In this paper, we discuss how to
construct a value-gradient driven, on-policy algorithm that allow training
Q-value models purely from on-policy data, unlocking the possibility of using
pathwise policy updates in the context of on-policy learning. We show how to
balance stochastic policies for exploration with constrained policy updates for
stable training, and evaluate important architectural components that
facilitate accurate value function learning. Building on these insights, we
propose Relative Entropy Pathwise Policy Optimization (REPPO), an efficient
on-policy algorithm that combines the sample-efficiency of pathwise policy
gradients with the simplicity and minimal memory footprint of standard
on-policy learning. We demonstrate that REPPO provides strong empirical
performance at decreased sample requirements, wall-clock time, memory footprint
as well as high hyperparameter robustness in a set of experiments on two
standard GPU-parallelized benchmarks.

</details>


### [141] [GATE: Graph Attention Neural Networks with Real-Time Edge Construction for Robust Indoor Localization using Mobile Embedded Devices](https://arxiv.org/abs/2507.11053)
*Danish Gufran,Sudeep Pasricha*

Main category: cs.LG

TL;DR: 提出GATE框架用于室内定位，解决传统模型问题，评估显示其误差大幅降低。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习和图神经网络在室内定位存在不能结合空间关系、受噪声影响大、有盲点等问题，需改进。

Method: 提出GATE框架，构建自适应图表示，引入AHV、MDHV和RTEC方法。

Result: 在多个室内场景评估，GATE比现有框架定位误差降低1.6x - 4.72x，最坏情况误差降低1.85x - 4.57x。

Conclusion: GATE框架有效解决室内定位难题，提升定位精度。

Abstract: Accurate indoor localization is crucial for enabling spatial context in smart
environments and navigation systems. Wi-Fi Received Signal Strength (RSS)
fingerprinting is a widely used indoor localization approach due to its
compatibility with mobile embedded devices. Deep Learning (DL) models improve
accuracy in localization tasks by learning RSS variations across locations, but
they assume fingerprint vectors exist in a Euclidean space, failing to
incorporate spatial relationships and the non-uniform distribution of
real-world RSS noise. This results in poor generalization across heterogeneous
mobile devices, where variations in hardware and signal processing distort RSS
readings. Graph Neural Networks (GNNs) can improve upon conventional DL models
by encoding indoor locations as nodes and modeling their spatial and signal
relationships as edges. However, GNNs struggle with non-Euclidean noise
distributions and suffer from the GNN blind spot problem, leading to degraded
accuracy in environments with dense access points (APs). To address these
challenges, we propose GATE, a novel framework that constructs an adaptive
graph representation of fingerprint vectors while preserving an indoor
state-space topology, modeling the non-Euclidean structure of RSS noise to
mitigate environmental noise and address device heterogeneity. GATE introduces
1) a novel Attention Hyperspace Vector (AHV) for enhanced message passing, 2) a
novel Multi-Dimensional Hyperspace Vector (MDHV) to mitigate the GNN blind
spot, and 3) an new Real-Time Edge Construction (RTEC) approach for dynamic
graph adaptation. Extensive real-world evaluations across multiple indoor
spaces with varying path lengths, AP densities, and heterogeneous devices
demonstrate that GATE achieves 1.6x to 4.72x lower mean localization errors and
1.85x to 4.57x lower worst-case errors compared to state-of-the-art indoor
localization frameworks.

</details>


### [142] [A Distance Metric for Mixed Integer Programming Instances](https://arxiv.org/abs/2507.11063)
*Gwen Maudet,Grégoire Danoy*

Main category: cs.LG

TL;DR: 本文提出首个针对混合整数线性规划（MILP）实例的数学距离度量，评估了精确和贪心变体，结果显示贪心版本速度快且性能好，优于非学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有MILP实例相似性度量缺乏精度或依赖标记数据，限制了适用性和泛化性，需要可靠的相似性度量。

Method: 直接从MILP实例的数学公式中导出距离度量，将右侧、权重和变量离散化，借鉴推土机距离量化权重 - 变量分布的不匹配。

Result: 度量的所有组件都有助于类识别，贪心版本与精确公式准确性相近，但速度快近200倍，优于非学习方法，可与监督分类器媲美。

Conclusion: 提出的无监督距离度量为MILP实例比较提供了有效方法，在类和子类分组任务中表现良好。

Abstract: Mixed-integer linear programming (MILP) is a powerful tool for addressing a
wide range of real-world problems, but it lacks a clear structure for comparing
instances. A reliable similarity metric could establish meaningful
relationships between instances, enabling more effective evaluation of instance
set heterogeneity and providing better guidance to solvers, particularly when
machine learning is involved. Existing similarity metrics often lack precision
in identifying instance classes or rely heavily on labeled data, which limits
their applicability and generalization. To bridge this gap, this paper
introduces the first mathematical distance metric for MILP instances, derived
directly from their mathematical formulations. By discretizing right-hand
sides, weights, and variables into classes, the proposed metric draws
inspiration from the Earth mover's distance to quantify mismatches in
weight-variable distributions for constraint comparisons. This approach
naturally extends to enable instance-level comparisons. We evaluate both an
exact and a greedy variant of our metric under various parameter settings,
using the StrIPLIB dataset. Results show that all components of the metric
contribute to class identification, and that the greedy version achieves
accuracy nearly identical to the exact formulation while being nearly 200 times
faster. Compared to state-of-the-art baselines, including feature-based,
image-based, and neural network models, our unsupervised method consistently
outperforms all non-learned approaches and rivals the performance of a
supervised classifier on class and subclass grouping tasks.

</details>


### [143] [LogTinyLLM: Tiny Large Language Models Based Contextual Log Anomaly Detection](https://arxiv.org/abs/2507.11071)
*Isaiah Thompson Ocansey,Ritwik Bhattacharya,Tanmay Sen*

Main category: cs.LG

TL;DR: 本文提出基于LoRA和适配器的方法检测大日志数据集中日志序列的上下文异常，对比不同微小大语言模型，LoRA微调比LogBert全量微调性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 传统规则或深度学习方法检测日志异常因日志序列量大且复杂而具有挑战性，有效检测异常日志序列对系统维护和开发至关重要。

Method: 提出基于低秩自适应（LoRA）和适配器的参数高效微调方法，在Thunderbird数据集上对比不同微小大语言模型。

Result: 基于LoRA的微调比基于LogBert的全量微调性能提升18 - 19个百分点，准确率达到97.76% - 98.83%，而LogBert全量微调准确率为79.37%。

Conclusion: 基于LoRA的微调方法在日志异常检测中表现出色，能显著提升性能。

Abstract: Log anomaly detection using traditional rule based or deep learning based
methods is often challenging due to the large volume and highly complex nature
of log sequence. So effective way of detection of anomalous sequence of logs is
crucial for system maintenance and development. This paper proposes parameter
efficient finetuning specifically low rank adaptation (LoRA) and adapter based
approaches for finding contextual anomalies in sequence of logs in large log
data set. It compares different tiny large language models (LLMs) on the
Thunderbird dataset. The results show that LoRA based finetuning provides
substantial performance improvements of 18 to 19 percentage over LogBert based
full finetuning approach, achieving accuracy scores between 97.76% and 98.83%
compared to 79.37%.

</details>


### [144] [Real-Time Bayesian Detection of Drift-Evasive GNSS Spoofing in Reinforcement Learning Based UAV Deconfliction](https://arxiv.org/abs/2507.11173)
*Deepak Kumar Panda,Weisi Guo*

Main category: cs.LG

TL;DR: 本文探讨用贝叶斯在线变点检测（BOCPD）方法监测无人机导航行为偏差，以检测漂移规避式欺骗攻击，实验表明该方法优于传统检测器。


<details>
  <summary>Details</summary>
Motivation: 无人机依赖GNSS伪距测量定位导航，易受漂移规避式欺骗攻击，传统检测技术有延迟，需鲁棒的时间尺度检测方法。

Method: 采用贝叶斯在线变点检测（BOCPD）方法，监测强化学习（RL）评论家网络值估计的时间变化。

Result: 该基于时间值的框架在检测漂移规避式欺骗攻击时，优于传统GNSS欺骗检测器、时间半监督学习框架和Page - Hinkley测试，有更高检测精度、更低误报和漏报率。

Conclusion: 所提出的方法能有效检测无人机导航中的漂移规避式欺骗攻击，提高应对隐蔽对抗操纵的恢复能力。

Abstract: Autonomous unmanned aerial vehicles (UAVs) rely on global navigation
satellite system (GNSS) pseudorange measurements for accurate real-time
localization and navigation. However, this dependence exposes them to
sophisticated spoofing threats, where adversaries manipulate pseudoranges to
deceive UAV receivers. Among these, drift-evasive spoofing attacks subtly
perturb measurements, gradually diverting the UAVs trajectory without
triggering conventional signal-level anti-spoofing mechanisms. Traditional
distributional shift detection techniques often require accumulating a
threshold number of samples, causing delays that impede rapid detection and
timely response. Consequently, robust temporal-scale detection methods are
essential to identify attack onset and enable contingency planning with
alternative sensing modalities, improving resilience against stealthy
adversarial manipulations. This study explores a Bayesian online change point
detection (BOCPD) approach that monitors temporal shifts in value estimates
from a reinforcement learning (RL) critic network to detect subtle behavioural
deviations in UAV navigation. Experimental results show that this temporal
value-based framework outperforms conventional GNSS spoofing detectors,
temporal semi-supervised learning frameworks, and the Page-Hinkley test,
achieving higher detection accuracy and lower false-positive and false-negative
rates for drift-evasive spoofing attacks.

</details>


### [145] [Gradient Regularization-based Neural Granger Causality](https://arxiv.org/abs/2507.11178)
*Meiliang Liu,Huiwen Dong,Xiaoxiao Yang,Yunfang Xu,Zijin Li,Zhengye Si,Xinyue Yang,Zhiwen Zhao*

Main category: cs.LG

TL;DR: 现有基于神经网络的格兰杰因果模型有局限，提出GRNGC模型，灵活性强，在数值模拟和真实数据集上表现好，降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有基于神经网络的格兰杰因果模型采用逐分量架构计算成本高，且通过对神经网络第一层权重施加稀疏惩罚提取因果关系会削弱捕捉复杂交互的能力。

Method: 提出GRNGC模型，仅需一个时间序列预测模型，对模型输入输出之间的梯度应用L1正则化来推断格兰杰因果关系，且不局限于特定时间序列预测模型，可采用多种架构。

Result: 在DREAM、Lorenz - 96等数值模拟中，GRNGC优于现有基线并显著降低计算开销；在真实的DNA、Yeast等数据集上验证了其在重建基因调控网络方面的有效性。

Conclusion: GRNGC模型有效解决了现有模型的局限，具有良好的性能和灵活性。

Abstract: With the advancement of deep learning technologies, various neural
network-based Granger causality models have been proposed. Although these
models have demonstrated notable improvements, several limitations remain. Most
existing approaches adopt the component-wise architecture, necessitating the
construction of a separate model for each time series, which results in
substantial computational costs. In addition, imposing the sparsity-inducing
penalty on the first-layer weights of the neural network to extract causal
relationships weakens the model's ability to capture complex interactions. To
address these limitations, we propose Gradient Regularization-based Neural
Granger Causality (GRNGC), which requires only one time series prediction model
and applies $L_{1}$ regularization to the gradient between model's input and
output to infer Granger causality. Moreover, GRNGC is not tied to a specific
time series forecasting model and can be implemented with diverse architectures
such as KAN, MLP, and LSTM, offering enhanced flexibility. Numerical
simulations on DREAM, Lorenz-96, fMRI BOLD, and CausalTime show that GRNGC
outperforms existing baselines and significantly reduces computational
overhead. Meanwhile, experiments on real-world DNA, Yeast, HeLa, and bladder
urothelial carcinoma datasets further validate the model's effectiveness in
reconstructing gene regulatory networks.

</details>


### [146] [Mixture of Experts in Large Language Models](https://arxiv.org/abs/2507.11181)
*Danyang Zhang,Junhao Song,Ziqian Bi,Yingfang Yuan,Tianyang Wang,Joe Yeong,Junfeng Hao*

Main category: cs.LG

TL;DR: 本文全面综述大语言模型中的Mixture-of-Experts (MoE)架构，分析其优势、挑战并指出未来方向。


<details>
  <summary>Details</summary>
Motivation: 研究MoE架构在大语言模型中提升性能且降低计算开销的能力。

Method: 对理论基础、核心架构设计和大语言模型应用进行系统分析，涉及专家门控和路由机制等多方面。

Result: 发现MoE相比贝叶斯方法有更好的模型容量，能提升特定任务性能且可高效扩展模型容量。

Conclusion: 强调确保专家多样性等的重要性，指出当前研究局限、挑战和未来方向，为MoE创新奠定基础。

Abstract: This paper presents a comprehensive review of the Mixture-of-Experts (MoE)
architecture in large language models, highlighting its ability to
significantly enhance model performance while maintaining minimal computational
overhead. Through a systematic analysis spanning theoretical foundations, core
architectural designs, and large language model (LLM) applications, we examine
expert gating and routing mechanisms, hierarchical and sparse MoE
configurations, meta-learning approaches, multimodal and multitask learning
scenarios, real-world deployment cases, and recent advances and challenges in
deep learning. Our analysis identifies key advantages of MoE, including
superior model capacity compared to equivalent Bayesian approaches, improved
task-specific performance, and the ability to scale model capacity efficiently.
We also underscore the importance of ensuring expert diversity, accurate
calibration, and reliable inference aggregation, as these are essential for
maximizing the effectiveness of MoE architectures. Finally, this review
outlines current research limitations, open challenges, and promising future
directions, providing a foundation for continued innovation in MoE architecture
and its applications.

</details>


### [147] [Quantized Rank Reduction: A Communications-Efficient Federated Learning Scheme for Network-Critical Applications](https://arxiv.org/abs/2507.11183)
*Dimitrios Kritsiolis,Constantine Kotropoulos*

Main category: cs.LG

TL;DR: 本文提出一种通信高效的联邦学习方案，用低秩近似和量化减少网络负载且对模型精度影响小。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中频繁交换模型更新导致通信开销大，需要降低网络负载。

Method: 利用神经网络梯度的低秩近似和量化技术。

Result: 显著减少去中心化学习过程的网络负载。

Conclusion: 所提方案能在不显著影响模型精度的前提下，有效降低联邦学习的通信开销。

Abstract: Federated learning is a machine learning approach that enables multiple
devices (i.e., agents) to train a shared model cooperatively without exchanging
raw data. This technique keeps data localized on user devices, ensuring privacy
and security, while each agent trains the model on their own data and only
shares model updates. The communication overhead is a significant challenge due
to the frequent exchange of model updates between the agents and the central
server. In this paper, we propose a communication-efficient federated learning
scheme that utilizes low-rank approximation of neural network gradients and
quantization to significantly reduce the network load of the decentralized
learning process with minimal impact on the model's accuracy.

</details>


### [148] [An Explainable AI-Enhanced Machine Learning Approach for Cardiovascular Disease Detection and Risk Assessment](https://arxiv.org/abs/2507.11185)
*Md. Emon Akter Sourov,Md. Sabbir Hossen,Pabon Shaha,Mohammad Minoar Hossain,Md Sadiq Iqbal*

Main category: cs.LG

TL;DR: 本文提出结合分类与回归模型的框架用于心脏病诊断和风险预测，用SMOTE处理数据不平衡，评估模型性能，随机森林和线性回归表现出色，强调机器学习在心脏病诊断中的潜力。


<details>
  <summary>Details</summary>
Motivation: 心脏病是全球健康问题，传统诊断方法不准确，机器学习有潜力提升诊断准确性、效率和速度。

Method: 提出结合分类和回归模型的框架，使用Heart Disease数据集，用SMOTE处理类不平衡，用多种性能指标评估模型，采用可解释AI技术。

Result: 分类模型中随机森林表现最佳，真实数据和合成数据准确率分别达97.2%和97.6%；回归任务中线性回归R2值最高，误差指标最低。

Conclusion: 机器学习有潜力革新心脏病诊断和风险预测，促进早期干预和临床决策。

Abstract: Heart disease remains a major global health concern, particularly in regions
with limited access to medical resources and diagnostic facilities. Traditional
diagnostic methods often fail to accurately identify and manage heart disease
risks, leading to adverse outcomes. Machine learning has the potential to
significantly enhance the accuracy, efficiency, and speed of heart disease
diagnosis. In this study, we proposed a comprehensive framework that combines
classification models for heart disease detection and regression models for
risk prediction. We employed the Heart Disease dataset, which comprises 1,035
cases. To address the issue of class imbalance, the Synthetic Minority
Oversampling Technique (SMOTE) was applied, resulting in the generation of an
additional 100,000 synthetic data points. Performance metrics, including
accuracy, precision, recall, F1-score, R2, MSE, RMSE, and MAE, were used to
evaluate the model's effectiveness. Among the classification models, Random
Forest emerged as the standout performer, achieving an accuracy of 97.2% on
real data and 97.6% on synthetic data. For regression tasks, Linear Regression
demonstrated the highest R2 values of 0.992 and 0.984 on real and synthetic
datasets, respectively, with the lowest error metrics. Additionally,
Explainable AI techniques were employed to enhance the interpretability of the
models. This study highlights the potential of machine learning to
revolutionize heart disease diagnosis and risk prediction, thereby facilitating
early intervention and enhancing clinical decision-making.

</details>


### [149] [Striking the Perfect Balance: Preserving Privacy While Boosting Utility in Collaborative Medical Prediction Platforms](https://arxiv.org/abs/2507.11187)
*Shao-Bo Lin,Xiaotong Liu,Yao Wang*

Main category: cs.LG

TL;DR: 本文提出隐私保护机制并集成到分布式学习框架，以解决在线协作医疗预测平台的隐私和预测质量问题，经理论证明和实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 在线协作医疗预测平台存在隐私和预测质量问题，影响患者参与和医生合作。

Method: 先明确隐私攻击和原则，提出隐私保护机制并集成到新型一次性分布式学习框架。

Result: 理论证明该分布式学习框架在特定隐私要求下可实现最优预测性能，通过模拟和真实数据实验验证平台有效性。

Conclusion: 所开发的隐私保护协作医疗预测平台能同时满足隐私要求和预测性能目标。

Abstract: Online collaborative medical prediction platforms offer convenience and
real-time feedback by leveraging massive electronic health records. However,
growing concerns about privacy and low prediction quality can deter patient
participation and doctor cooperation. In this paper, we first clarify the
privacy attacks, namely attribute attacks targeting patients and model
extraction attacks targeting doctors, and specify the corresponding privacy
principles. We then propose a privacy-preserving mechanism and integrate it
into a novel one-shot distributed learning framework, aiming to simultaneously
meet both privacy requirements and prediction performance objectives. Within
the framework of statistical learning theory, we theoretically demonstrate that
the proposed distributed learning framework can achieve the optimal prediction
performance under specific privacy requirements. We further validate the
developed privacy-preserving collaborative medical prediction platform through
both toy simulations and real-world data experiments.

</details>


### [150] [Gradient Descent on Logistic Regression: Do Large Step-Sizes Work with Data on the Sphere?](https://arxiv.org/abs/2507.11228)
*Si Yi Meng,Baptiste Goujaud,Antonio Orvieto,Christopher De Sa*

Main category: cs.LG

TL;DR: 探讨逻辑回归中梯度下降（GD），研究限制数据为等模是否是全局收敛的充分条件，一维空间成立，高维仍可能循环。


<details>
  <summary>Details</summary>
Motivation: 已知数据集线性可分和不可分情况下GD的不同表现，探索限制数据等模对全局收敛的影响。

Method: 理论证明。

Result: 在一维空间限制数据等模是全局收敛的充分条件，高维空间仍会有循环行为。

Conclusion: 希望激发对现实数据中循环情况量化及大步长全局收敛充分条件的进一步研究。

Abstract: Gradient descent (GD) on logistic regression has many fascinating properties.
When the dataset is linearly separable, it is known that the iterates converge
in direction to the maximum-margin separator regardless of how large the step
size is. In the non-separable case, however, it has been shown that GD can
exhibit a cycling behaviour even when the step sizes is still below the
stability threshold $2/\lambda$, where $\lambda$ is the largest eigenvalue of
the Hessian at the solution. This short paper explores whether restricting the
data to have equal magnitude is a sufficient condition for global convergence,
under any step size below the stability threshold. We prove that this is true
in a one dimensional space, but in higher dimensions cycling behaviour can
still occur. We hope to inspire further studies on quantifying how common these
cycles are in realistic datasets, as well as finding sufficient conditions to
guarantee global convergence with large step sizes.

</details>


### [151] [Generative Click-through Rate Prediction with Applications to Search Advertising](https://arxiv.org/abs/2507.11246)
*Lingwei Kong,Lu Wang,Changping Peng,Zhangang Lin,Ching Law,Jingping Shao*

Main category: cs.LG

TL;DR: 本文提出利用生成模型提升判别式CTR预测模型精度的新方法，经实验验证有效且已部署。


<details>
  <summary>Details</summary>
Motivation: 当前CTR预测多采用判别式模型，生成模型成功后其增强表达能力的潜力凸显，故引入生成模型提升CTR预测精度。

Method: 设计两阶段训练过程，先进行生成式预训练用于用户行为序列的下一项预测，再在判别式CTR预测框架中微调预训练好的生成模型。

Result: 在新数据集上进行大量实验验证了方法的有效性，在线A/B测试结果进一步证实其实用性，模型已部署在大型电商平台。

Conclusion: 提出的利用生成模型增强判别式CTR预测精度的方法有效，未来会发布代码和数据集。

Abstract: Click-Through Rate (CTR) prediction models are integral to a myriad of
industrial settings, such as personalized search advertising. Current methods
typically involve feature extraction from users' historical behavior sequences
combined with product information, feeding into a discriminative model that is
trained on user feedback to estimate CTR. With the success of models such as
GPT, the potential for generative models to enrich expressive power beyond
discriminative models has become apparent. In light of this, we introduce a
novel model that leverages generative models to enhance the precision of CTR
predictions in discriminative models. To reconcile the disparate data
aggregation needs of both model types, we design a two-stage training process:
1) Generative pre-training for next-item prediction with the given item
category in user behavior sequences; 2) Fine-tuning the well-trained generative
model within a discriminative CTR prediction framework. Our method's efficacy
is substantiated through extensive experiments on a new dataset, and its
significant utility is further corroborated by online A/B testing results.
Currently, the model is deployed on one of the world's largest e-commerce
platforms, and we intend to release the associated code and dataset in the
future.

</details>


### [152] [LyAm: Robust Non-Convex Optimization for Stable Learning in Noisy Environments](https://arxiv.org/abs/2507.11262)
*Elmira Mirzabeigi,Sepehr Rezaee,Kourosh Parand*

Main category: cs.LG

TL;DR: 本文提出新型优化器LyAm，结合Adam与Lyapunov稳定性机制，有理论保证且实验效果优于现有优化器。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络训练中梯度噪声大、收敛不稳定，影响性能和泛化能力的问题。

Method: 提出LyAm优化器，利用Lyapunov稳定性理论动态调整学习率，给出理论框架证明其在复杂非凸环境下的收敛性。

Result: 在CIFAR - 10和CIFAR - 100等数据集上实验表明，LyAm在准确率、收敛速度和稳定性上均优于现有优化器。

Conclusion: LyAm是鲁棒深度学习优化的有力候选方案。

Abstract: Training deep neural networks, particularly in computer vision tasks, often
suffers from noisy gradients and unstable convergence, which hinder performance
and generalization. In this paper, we propose LyAm, a novel optimizer that
integrates Adam's adaptive moment estimation with Lyapunov-based stability
mechanisms. LyAm dynamically adjusts the learning rate using Lyapunov stability
theory to enhance convergence robustness and mitigate training noise. We
provide a rigorous theoretical framework proving the convergence guarantees of
LyAm in complex, non-convex settings. Extensive experiments on like as CIFAR-10
and CIFAR-100 show that LyAm consistently outperforms state-of-the-art
optimizers in terms of accuracy, convergence speed, and stability, establishing
it as a strong candidate for robust deep learning optimization.

</details>


### [153] [Turning Sand to Gold: Recycling Data to Bridge On-Policy and Off-Policy Learning via Causal Bound](https://arxiv.org/abs/2507.11269)
*Tal Fiskus,Uri Shaham*

Main category: cs.LG

TL;DR: 引入将Neyman - Rubin潜在结果框架用于DRL的新理论成果以解决训练资源需求大问题，实验显示能提升奖励比、减小经验回放缓冲区大小、提高样本效率。


<details>
  <summary>Details</summary>
Motivation: 解决深度强化学习（DRL）代理训练时需要大量训练步骤和巨大经验回放缓冲区，导致计算和资源需求大的问题。

Method: 将Neyman - Rubin潜在结果框架引入DRL，建立事实损失的因果界限，通过在经验回放缓冲区存储过去值网络输出来计算该界限。

Result: 在Atari 2600和MuJoCo领域对多种代理实验，奖励比最高提升2427%，经验回放缓冲区大小最多减少96%。

Conclusion: 提出的方法能显著提高样本效率且成本可忽略不计。

Abstract: Deep reinforcement learning (DRL) agents excel in solving complex
decision-making tasks across various domains. However, they often require a
substantial number of training steps and a vast experience replay buffer,
leading to significant computational and resource demands. To address these
challenges, we introduce a novel theoretical result that leverages the
Neyman-Rubin potential outcomes framework into DRL. Unlike most methods that
focus on bounding the counterfactual loss, we establish a causal bound on the
factual loss, which is analogous to the on-policy loss in DRL. This bound is
computed by storing past value network outputs in the experience replay buffer,
effectively utilizing data that is usually discarded. Extensive experiments
across the Atari 2600 and MuJoCo domains on various agents, such as DQN and
SAC, achieve up to 2,427% higher reward ratio, outperforming the same agents
without our proposed term, and reducing the experience replay buffer size by up
to 96%, significantly improving sample efficiency at negligible cost.

</details>


### [154] [Guiding LLM Decision-Making with Fairness Reward Models](https://arxiv.org/abs/2507.11344)
*Zara Hall,Melanie Subbiah,Thomas P Zollo,Kathleen McKeown,Richard Zemel*

Main category: cs.LG

TL;DR: 提出训练可泛化公平奖励模型（FRM）框架，提升大语言模型决策公平性与准确性。


<details>
  <summary>Details</summary>
Motivation: 解决朴素思维链采样在高风险决策中会放大不公平偏差的问题，实现推理模型在高风险决策中的可靠应用。

Method: 训练可泛化的公平奖励模型（FRM），为大语言模型推理分配公平分数，在聚合决策时降低有偏差推理轨迹的权重。

Result: 单个FRM模型在弱监督、大语言模型标注的示例上训练后，可跨任务、领域和模型家族迁移，应用于现实决策任务能提升公平性，匹配或超越基线准确性。

Conclusion: 所提方法能在高风险决策中有效提升大语言模型决策的公平性和准确性。

Abstract: Large language models are increasingly used to support high-stakes decisions,
potentially influencing who is granted bail or receives a loan. Naive
chain-of-thought sampling can improve average decision accuracy, but has also
been shown to amplify unfair bias. To address this challenge and enable the
trustworthy use of reasoning models in high-stakes decision-making, we propose
a framework for training a generalizable Fairness Reward Model (FRM). Our model
assigns a fairness score to LLM reasoning, enabling the system to down-weight
biased trajectories and favor equitable ones when aggregating decisions across
reasoning chains. We show that a single Fairness Reward Model, trained on
weakly supervised, LLM-annotated examples of biased versus unbiased reasoning,
transfers across tasks, domains, and model families without additional
fine-tuning. Applied to real-world decision-making tasks including recidivism
prediction and social media moderation, we show that our approach consistently
improves fairness while matching, or even surpassing, baseline accuracy.

</details>


### [155] [Step-wise Policy for Rare-tool Knowledge (SPaRK): Offline RL that Drives Diverse Tool Use in LLMs](https://arxiv.org/abs/2507.11371)
*Gabriel Bo,Koa Chang,Justin Gu*

Main category: cs.LG

TL;DR: 提出SPaRK强化学习框架，用双目标奖励系统训练模型，采用稀有优先策略，实验显示其在MMLU - Pro分类中表现好且工具选择熵高。


<details>
  <summary>Details</summary>
Motivation: 让大语言模型探索超越传统高温采样的多样化工具使用模式。

Method: 引入双目标奖励系统，在MMLU - Pro数据集合成轨迹上用离线PPO训练Llama - 3.1 8B模型，采用稀有优先策略，用GPT - 4o打分。

Result: SPaRK在14个MMLU - Pro类别中取得有竞争力的表现，工具选择熵比基线和监督微调方法显著更高。

Conclusion: 通过明确工具多样性的算法探索可在不牺牲准确性的情况下增强推理能力。

Abstract: We present Step-wise Policy for Rare-tool Knowledge (SPaRK), a novel
reinforcement learning framework that teaches large language models to explore
diverse tool usage patterns beyond conventional high-temperature sampling.
Building on recent advances in step-wise reinforcement learning, we introduce a
dual-objective reward system that simultaneously optimizes for answer quality
and tool diversity, training a Llama-3.1 8B model through offline PPO on
synthetically generated trajectories from the MMLU-Pro dataset. Our approach
uniquely employs a rarity-first exploitation strategy where a GPT-4o judge
scores candidate actions across eight distinct tools plus chain-of-thought
reasoning, with the policy favoring less-frequently used but still viable tools
to encourage systematic exploration. Empirical results demonstrate that SPaRK
achieves competitive performance across 14 MMLU-Pro categories while exhibiting
significantly higher entropy in tool selection compared to both baseline and
supervised fine-tuning approaches, suggesting that algorithmic exploration
through explicit tool diversity can enhance reasoning capabilities without
sacrificing accuracy.

</details>


### [156] [A Neural Network Model of Complementary Learning Systems: Pattern Separation and Completion for Continual Learning](https://arxiv.org/abs/2507.11393)
*James P Jun,Vijay Marupudi,Raj Sanjay Shah,Sashank Varma*

Main category: cs.LG

TL;DR: 结合变分自编码器和现代霍普菲尔德网络构建持续学习模型，在Split - MNIST任务上接近SOTA，减少遗忘，为记忆巩固等建模提供模板。


<details>
  <summary>Details</summary>
Motivation: 神经网络存在灾难性遗忘问题，而人类能在学习新信息时不忘旧知识，互补学习系统理论有解释，需构建模型模拟。

Method: 利用变分自编码器的表征泛化能力和现代霍普菲尔德网络的稳健记忆存储特性，将二者结合成持续学习模型。

Result: 在Split - MNIST任务上达到约90%的准确率，接近SOTA，大幅减少遗忘，表征分析证实功能分离。

Conclusion: 该模型在可扩展架构中实现模式分离和完成，为生物和人工系统的记忆巩固、泛化和持续学习建模提供功能模板。

Abstract: Learning new information without forgetting prior knowledge is central to
human intelligence. In contrast, neural network models suffer from catastrophic
forgetting: a significant degradation in performance on previously learned
tasks when acquiring new information. The Complementary Learning Systems (CLS)
theory offers an explanation for this human ability, proposing that the brain
has distinct systems for pattern separation (encoding distinct memories) and
pattern completion (retrieving complete memories from partial cues). To capture
these complementary functions, we leverage the representational generalization
capabilities of variational autoencoders (VAEs) and the robust memory storage
properties of Modern Hopfield networks (MHNs), combining them into a neurally
plausible continual learning model. We evaluate this model on the Split-MNIST
task, a popular continual learning benchmark, and achieve close to
state-of-the-art accuracy (~90%), substantially reducing forgetting.
Representational analyses empirically confirm the functional dissociation: the
VAE underwrites pattern completion, while the MHN drives pattern separation. By
capturing pattern separation and completion in scalable architectures, our work
provides a functional template for modeling memory consolidation,
generalization, and continual learning in both biological and artificial
systems.

</details>


### [157] [Robust-Multi-Task Gradient Boosting](https://arxiv.org/abs/2507.11411)
*Seyedsaman Emami,Gonzalo Martínez-Muñoz,Daniel Hernández-Lobato*

Main category: cs.LG

TL;DR: 提出R - MTGB框架解决多任务学习中任务不对齐问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现实多任务学习场景中存在任务不对齐情况，会降低整体模型性能，需解决该问题。

Method: 提出Robust - Multi - Task Gradient Boosting (R - MTGB)框架，将学习过程分为学习共享模式、划分任务、微调特定任务预测器三个顺序模块。

Result: 在合成基准和真实数据集上实验表明，该方法能隔离异常值、转移知识、减少各任务预测误差并提升整体性能。

Conclusion: R - MTGB在具有挑战性的多任务学习环境中具有鲁棒性、适应性和可靠的收敛性。

Abstract: Multi-task learning (MTL) has shown effectiveness in exploiting shared
information across tasks to improve generalization. MTL assumes tasks share
similarities that can improve performance. In addition, boosting algorithms
have demonstrated exceptional performance across diverse learning problems,
primarily due to their ability to focus on hard-to-learn instances and
iteratively reduce residual errors. This makes them a promising approach for
learning multi-task problems. However, real-world MTL scenarios often involve
tasks that are not well-aligned (known as outlier or adversarial tasks), which
do not share beneficial similarities with others and can, in fact, deteriorate
the performance of the overall model. To overcome this challenge, we propose
Robust-Multi-Task Gradient Boosting (R-MTGB), a novel boosting framework that
explicitly models and adapts to task heterogeneity during training. R-MTGB
structures the learning process into three sequential blocks: (1) learning
shared patterns, (2) partitioning tasks into outliers and non-outliers with
regularized parameters, and (3) fine-tuning task-specific predictors. This
architecture enables R-MTGB to automatically detect and penalize outlier tasks
while promoting effective knowledge transfer among related tasks. Our method
integrates these mechanisms seamlessly within gradient boosting, allowing
robust handling of noisy or adversarial tasks without sacrificing accuracy.
Extensive experiments on both synthetic benchmarks and real-world datasets
demonstrate that our approach successfully isolates outliers, transfers
knowledge, and consistently reduces prediction errors for each task
individually, and achieves overall performance gains across all tasks. These
results highlight robustness, adaptability, and reliable convergence of R-MTGB
in challenging MTL environments.

</details>


### [158] [Toward Improving fNIRS Classification: A Study on Activation Functions in Deep Neural Architectures](https://arxiv.org/abs/2507.11436)
*Behtom Adeli,John McLinden,Pankaj Pandey,Ming Shao,Yalda Shahriari*

Main category: cs.LG

TL;DR: 研究评估多种激活函数在fNIRS分类任务中的表现，发现对称激活函数可能更优，强调选择合适激活函数的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前文献中激活函数对fNIRS领域深度学习性能的影响缺乏系统研究，而fNIRS的特性对模型精度有挑战。

Method: 使用多种深度学习架构评估常规和特定领域的激活函数，在单个听觉任务数据集上测试，采用标准化预处理和一致训练参数。

Result: 对称激活函数如Tanh和Abs(x)在某些架构中表现优于ReLU，对修改的绝对值函数MAF的分析也支持对称激活函数有效。

Conclusion: 选择与fNIRS数据信号特征相符的激活函数很重要。

Abstract: Activation functions are critical to the performance of deep neural networks,
particularly in domains such as functional near-infrared spectroscopy (fNIRS),
where nonlinearity, low signal-to-noise ratio (SNR), and signal variability
poses significant challenges to model accuracy. However, the impact of
activation functions on deep learning (DL) performance in the fNIRS domain
remains underexplored and lacks systematic investigation in the current
literature. This study evaluates a range of conventional and field-specific
activation functions for fNIRS classification tasks using multiple deep
learning architectures, including the domain-specific fNIRSNet, AbsoluteNet,
MDNN, and shallowConvNet (as the baseline), all tested on a single dataset
recorded during an auditory task. To ensure fair a comparison, all networks
were trained and tested using standardized preprocessing and consistent
training parameters. The results show that symmetrical activation functions
such as Tanh and the Absolute value function Abs(x) can outperform commonly
used functions like the Rectified Linear Unit (ReLU), depending on the
architecture. Additionally, a focused analysis of the role of symmetry was
conducted using a Modified Absolute Function (MAF), with results further
supporting the effectiveness of symmetrical activation functions on performance
gains. These findings underscore the importance of selecting proper activation
functions that align with the signal characteristics of fNIRS data.

</details>


### [159] [Data Augmentation in Time Series Forecasting through Inverted Framework](https://arxiv.org/abs/2507.11439)
*Hongming Tan,Ting Chen,Ruochong Jin,Wai Kin Chan*

Main category: cs.LG

TL;DR: 提出用于iTransformer倒置框架的数据增强方法DAIF，经多数据集和模型实验验证有效


<details>
  <summary>Details</summary>
Motivation: iTransformer倒置框架存在减少时间依赖信息、在变量相关性不显著时引入噪声的局限

Method: 定义倒置序列到序列框架结构，提出频率过滤和交叉变异修补两种DAIF策略

Result: 多数据集和倒置模型实验证明DAIF有效

Conclusion: DAIF能解决iTransformer倒置框架现存挑战，是MTS预测倒置框架的实时增强方法

Abstract: Currently, iTransformer is one of the most popular and effective models for
multivariate time series (MTS) forecasting. Thanks to its inverted framework,
iTransformer effectively captures multivariate correlation. However, the
inverted framework still has some limitations. It diminishes temporal
interdependency information, and introduces noise in cases of nonsignificant
variable correlation. To address these limitations, we introduce a novel data
augmentation method on inverted framework, called DAIF. Unlike previous data
augmentation methods, DAIF stands out as the first real-time augmentation
specifically designed for the inverted framework in MTS forecasting. We first
define the structure of the inverted sequence-to-sequence framework, then
propose two different DAIF strategies, Frequency Filtering and Cross-variation
Patching to address the existing challenges of the inverted framework.
Experiments across multiple datasets and inverted models have demonstrated the
effectiveness of our DAIF.

</details>


### [160] [LRMR: LLM-Driven Relational Multi-node Ranking for Lymph Node Metastasis Assessment in Rectal Cancer](https://arxiv.org/abs/2507.11457)
*Yaoxian Dong,Yifan Gao,Haoyue Li,Yanfen Cui,Xin Gao*

Main category: cs.LG

TL;DR: 提出LRMR框架评估直肠癌淋巴结转移，性能超基线模型，两阶段设计有重要价值。


<details>
  <summary>Details</summary>
Motivation: 传统MRI评估直肠癌淋巴结转移诊断性能有限，现有AI模型缺乏可解释性且忽略患者层面背景。

Method: 引入LRMR框架，分两阶段：先由多模态大语言模型分析生成特征报告，再由文本大语言模型进行患者间报告对比和风险排名。

Result: 在117例患者队列中，LRMR的AUC为0.7917，F1分数为0.7200，优于ResNet50等基线模型；消融实验表明两阶段设计对性能重要。

Conclusion: 通过两阶段大语言模型框架分离视觉感知和认知推理，为评估直肠癌淋巴结转移提供了强大、可解释且有效的新范式。

Abstract: Accurate preoperative assessment of lymph node (LN) metastasis in rectal
cancer guides treatment decisions, yet conventional MRI evaluation based on
morphological criteria shows limited diagnostic performance. While some
artificial intelligence models have been developed, they often operate as black
boxes, lacking the interpretability needed for clinical trust. Moreover, these
models typically evaluate nodes in isolation, overlooking the patient-level
context. To address these limitations, we introduce LRMR, an LLM-Driven
Relational Multi-node Ranking framework. This approach reframes the diagnostic
task from a direct classification problem into a structured reasoning and
ranking process. The LRMR framework operates in two stages. First, a multimodal
large language model (LLM) analyzes a composite montage image of all LNs from a
patient, generating a structured report that details ten distinct radiological
features. Second, a text-based LLM performs pairwise comparisons of these
reports between different patients, establishing a relative risk ranking based
on the severity and number of adverse features. We evaluated our method on a
retrospective cohort of 117 rectal cancer patients. LRMR achieved an area under
the curve (AUC) of 0.7917 and an F1-score of 0.7200, outperforming a range of
deep learning baselines, including ResNet50 (AUC 0.7708). Ablation studies
confirmed the value of our two main contributions: removing the relational
ranking stage or the structured prompting stage led to a significant
performance drop, with AUCs falling to 0.6875 and 0.6458, respectively. Our
work demonstrates that decoupling visual perception from cognitive reasoning
through a two-stage LLM framework offers a powerful, interpretable, and
effective new paradigm for assessing lymph node metastasis in rectal cancer.

</details>


### [161] [Exploring the robustness of TractOracle methods in RL-based tractography](https://arxiv.org/abs/2507.11486)
*Jeremi Levesque,Antoine Théberge,Maxime Descoteaux,Pierre-Marc Jodoin*

Main category: cs.LG

TL;DR: 本文研究TractOracle - RL框架的四种扩展，评估其在五个扩散MRI数据集上的性能，还引入迭代奖励训练（IRT）方案，结果显示基于oracle反馈的RL方法在准确性和解剖有效性上优于常用束描记术。


<details>
  <summary>Details</summary>
Motivation: 改进基于强化学习的束描记术，进一步提升其性能。

Method: 研究TractOracle - RL框架的四种扩展，引入迭代奖励训练（IRT）方案，在五个扩散MRI数据集上进行评估。

Result: 结合oracle和RL框架能实现稳健可靠的束描记术，基于oracle反馈的RL方法在准确性和解剖有效性上显著优于常用技术。

Conclusion: 结合oracle的RL框架及IRT方案在束描记术上表现良好，值得进一步推广。

Abstract: Tractography algorithms leverage diffusion MRI to reconstruct the fibrous
architecture of the brain's white matter. Among machine learning approaches,
reinforcement learning (RL) has emerged as a promising framework for
tractography, outperforming traditional methods in several key aspects.
TractOracle-RL, a recent RL-based approach, reduces false positives by
incorporating anatomical priors into the training process via a reward-based
mechanism. In this paper, we investigate four extensions of the original
TractOracle-RL framework by integrating recent advances in RL, and we evaluate
their performance across five diverse diffusion MRI datasets. Results
demonstrate that combining an oracle with the RL framework consistently leads
to robust and reliable tractography, regardless of the specific method or
dataset used. We also introduce a novel RL training scheme called Iterative
Reward Training (IRT), inspired by the Reinforcement Learning from Human
Feedback (RLHF) paradigm. Instead of relying on human input, IRT leverages
bundle filtering methods to iteratively refine the oracle's guidance throughout
training. Experimental results show that RL methods trained with oracle
feedback significantly outperform widely used tractography techniques in terms
of accuracy and anatomical validity.

</details>


### [162] [AirLLM: Diffusion Policy-based Adaptive LoRA for Remote Fine-Tuning of LLM over the Air](https://arxiv.org/abs/2507.11515)
*Shiyi Yang,Xiaoxue Yu,Rongpeng Li,Jianhang Zhu,Zhifeng Zhao,Honggang Zhang*

Main category: cs.LG

TL;DR: 提出AirLLM框架解决云辅助远程微调中LoRA方法通信效率低的问题，实验表明其能提升微调性能并降低传输成本。


<details>
  <summary>Details</summary>
Motivation: 边缘设备运行大语言模型面临通信带宽、计算和内存成本挑战，现有LoRA方法通信效率低。

Method: 开发AirLLM分层扩散策略框架，用PPO代理生成粗粒度决策，通过DDIM细化为高分辨率、任务和信道自适应的秩向量，两模块交替优化。

Result: 在不同信噪比实验中，AirLLM能持续提升微调性能并显著降低传输成本。

Conclusion: 强化驱动、扩散细化的秩自适应方法对可扩展和高效的空中远程微调有效。

Abstract: Operating Large Language Models (LLMs) on edge devices is increasingly
challenged by limited communication bandwidth and strained computational and
memory costs. Thus, cloud-assisted remote fine-tuning becomes indispensable.
Nevertheless, existing Low-Rank Adaptation (LoRA) approaches typically employ
fixed or heuristic rank configurations, and the subsequent over-the-air
transmission of all LoRA parameters could be rather inefficient. To address
this limitation, we develop AirLLM, a hierarchical diffusion policy framework
for communication-aware LoRA adaptation. Specifically, AirLLM models the rank
configuration as a structured action vector that spans all LoRA-inserted
projections. To solve the underlying high-dimensional sequential
decision-making problem, a Proximal Policy Optimization (PPO) agent generates
coarse-grained decisions by jointly observing wireless states and linguistic
complexity, which are then refined via Denoising Diffusion Implicit Models
(DDIM) to produce high-resolution, task- and channel-adaptive rank vectors. The
two modules are optimized alternatively, with the DDIM trained under the
Classifier-Free Guidance (CFG) paradigm to maintain alignment with PPO rewards.
Experiments under varying signal-to-noise ratios demonstrate that AirLLM
consistently enhances fine-tuning performance while significantly reducing
transmission costs, highlighting the effectiveness of reinforcement-driven,
diffusion-refined rank adaptation for scalable and efficient remote fine-tuning
over the air.

</details>


### [163] [Langevin Flows for Modeling Neural Latent Dynamics](https://arxiv.org/abs/2507.11531)
*Yue Song,T. Anderson Keller,Yisong Yue,Pietro Perona,Max Welling*

Main category: cs.LG

TL;DR: 本文提出LangevinFlow模型，结合物理先验，在合成神经群体和真实数据集上表现优异，是建模复杂神经群体动力学的有效框架。


<details>
  <summary>Details</summary>
Motivation: 神经群体有潜在动态结构，需模型捕捉内在网络动力学和外部未观察到的影响。

Method: 引入LangevinFlow，其是顺序变分自编码器，潜变量时间演化由欠阻尼Langevin方程控制，结合物理先验，模型包含循环编码器、一层Transformer解码器和潜空间的Langevin动力学。

Result: 在合成神经群体上优于现有基线，在NLB数据集上有更好的预测准确性和神经元似然性，在解码行为指标上匹配或超越其他方法。

Conclusion: 提出了灵活、受物理启发、高性能的框架用于建模复杂神经群体动力学及其未观察到的影响。

Abstract: Neural populations exhibit latent dynamical structures that drive
time-evolving spiking activities, motivating the search for models that capture
both intrinsic network dynamics and external unobserved influences. In this
work, we introduce LangevinFlow, a sequential Variational Auto-Encoder where
the time evolution of latent variables is governed by the underdamped Langevin
equation. Our approach incorporates physical priors -- such as inertia,
damping, a learned potential function, and stochastic forces -- to represent
both autonomous and non-autonomous processes in neural systems. Crucially, the
potential function is parameterized as a network of locally coupled
oscillators, biasing the model toward oscillatory and flow-like behaviors
observed in biological neural populations. Our model features a recurrent
encoder, a one-layer Transformer decoder, and Langevin dynamics in the latent
space. Empirically, our method outperforms state-of-the-art baselines on
synthetic neural populations generated by a Lorenz attractor, closely matching
ground-truth firing rates. On the Neural Latents Benchmark (NLB), the model
achieves superior held-out neuron likelihoods (bits per spike) and forward
prediction accuracy across four challenging datasets. It also matches or
surpasses alternative methods in decoding behavioral metrics such as hand
velocity. Overall, this work introduces a flexible, physics-inspired,
high-performing framework for modeling complex neural population dynamics and
their unobserved influences.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [164] [Tangma: A Tanh-Guided Activation Function with Learnable Parameters](https://arxiv.org/abs/2507.10560)
*Shreel Golwala*

Main category: cs.NE

TL;DR: 本文介绍新激活函数Tangma，结合双曲正切平滑形状与两个可学习参数，在MNIST和CIFAR - 10上评估，表现优于ReLU、Swish和GELU等，训练更高效。


<details>
  <summary>Details</summary>
Motivation: 激活函数对深度神经网络的反向传播和表达能力至关重要，需要更好的激活函数。

Method: 引入新激活函数Tangma，结合双曲正切和平滑形状与两个可学习参数，在MNIST和CIFAR - 10上用自定义卷积和线性层网络评估，并与ReLU、Swish和GELU对比。

Result: 在MNIST上验证准确率达99.09%，损失最低，收敛更快更稳定；在CIFAR - 10上验证准确率达78.15%，优于其他激活函数，训练损失有竞争力，训练效率更高。

Conclusion: Tangma在标准视觉任务中表现良好，训练可靠高效，可学习设计能更好控制激活行为，可能有益于图像识别或语言建模等任务的更大模型。

Abstract: Activation functions are key to effective backpropagation and expressiveness
in deep neural networks. This work introduces Tangma, a new activation function
that combines the smooth shape of the hyperbolic tangent with two learnable
parameters: $\alpha$, which shifts the curve's inflection point to adjust
neuron activation, and $\gamma$, which adds linearity to preserve weak
gradients and improve training stability. Tangma was evaluated on MNIST and
CIFAR-10 using custom networks composed of convolutional and linear layers, and
compared against ReLU, Swish, and GELU. On MNIST, Tangma achieved the highest
validation accuracy of 99.09% and the lowest validation loss, demonstrating
faster and more stable convergence than the baselines. On CIFAR-10, Tangma
reached a top validation accuracy of 78.15%, outperforming all other activation
functions while maintaining a competitive training loss. Tangma also showed
improved training efficiency, with lower average epoch runtimes compared to
Swish and GELU. These results suggest that Tangma performs well on standard
vision tasks and enables reliable, efficient training. Its learnable design
gives more control over activation behavior, which may benefit larger models in
tasks such as image recognition or language modeling.

</details>


### [165] [SFATTI: Spiking FPGA Accelerator for Temporal Task-driven Inference -- A Case Study on MNIST](https://arxiv.org/abs/2507.10561)
*Alessio Caviglia,Filippo Marostica,Alessio Carpegna,Alessandro Savino,Stefano Di Carlo*

Main category: cs.NE

TL;DR: 本文探索用开源Spiker+框架为MNIST数据集上的手写数字识别生成优化的SNN加速器，并评估配置和分析权衡。


<details>
  <summary>Details</summary>
Motivation: 硬件加速器对边缘应用低延迟、高能效推理很重要，SNN因特性适合基于FPGA的低功耗部署，故探索用Spiker+框架生成优化的SNN加速器。

Method: 使用开源Spiker+框架进行网络拓扑、神经元模型和量化的高级规范，自动生成可部署的HDL，评估多种配置。

Result: 未提及具体结果。

Conclusion: 未提及具体结论。

Abstract: Hardware accelerators are essential for achieving low-latency,
energy-efficient inference in edge applications like image recognition. Spiking
Neural Networks (SNNs) are particularly promising due to their event-driven and
temporally sparse nature, making them well-suited for low-power Field
Programmable Gate Array (FPGA)-based deployment. This paper explores using the
open-source Spiker+ framework to generate optimized SNNs accelerators for
handwritten digit recognition on the MNIST dataset. Spiker+ enables high-level
specification of network topologies, neuron models, and quantization,
automatically generating deployable HDL. We evaluate multiple configurations
and analyze trade-offs relevant to edge computing constraints.

</details>


### [166] [A Biomimetic Way for Coral-Reef-Inspired Swarm Intelligence for Carbon-Neutral Wastewater Treatment](https://arxiv.org/abs/2507.10563)
*Antonis Messinis*

Main category: cs.NE

TL;DR: 介绍受珊瑚礁启发的群交互网络用于碳中和污水处理，效果良好但有数据科学人员和治理限制问题


<details>
  <summary>Details</summary>
Motivation: 在废水率增加的情况下实现能源中性净化具有挑战性，需新方法进行碳中和污水处理

Method: 引入受珊瑚礁启发的群交互网络，结合形态发生抽象与多任务碳感知

Result: 与七个基线相比，实现96.7%去除效率、0.31 kWh m⁻³能耗和14.2 g m⁻³二氧化碳排放，在传感器漂移下有鲁棒性，在不同场景有潜在柴油节省

Conclusion: 该方法有效，但数据科学人员配备是障碍，未来将集成AutoML包装器，治理限制带来解释性挑战需进一步可视化分析

Abstract: With increasing wastewater rates, achieving energy-neutral purification is
challenging. We introduce a coral-reef-inspired Swarm Interaction Network for
carbon-neutral wastewater treatment, combining morphogenetic abstraction with
multi-task carbon awareness. Scalability stems from linear token complexity,
mitigating the energy-removal problem. Compared with seven baselines, our
approach achieves 96.7\% removal efficiency, 0.31~kWh~m$^{-3}$ energy
consumption, and 14.2~g~m$^{-3}$ CO$_2$ emissions. Variance analysis
demonstrates robustness under sensor drift. Field scenarios--insular lagoons,
brewery spikes, and desert greenhouses--show potential diesel savings of up to
22\%. However, data-science staffing remains an impediment. Future work will
integrate AutoML wrappers within the project scope, although governance
restrictions pose interpretability challenges that require further visual
analytics.

</details>


### [167] [An Exact Gradient Framework for Training Spiking Neural Networks](https://arxiv.org/abs/2507.10568)
*Arman Ferdowsi,Atakan Aral*

Main category: cs.NE

TL;DR: 本文提出分析式事件驱动学习框架，在多个基准测试上比现有方法有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有训练SNN的方法依赖离散时间模拟等，限制训练精度和效率，且对神经形态硬件实现有挑战，需克服这些问题。

Method: 提出分析式事件驱动学习框架，计算关于突触权重、传输延迟和自适应神经元放电阈值的精确损失梯度。

Result: 在多个基准测试中，相比现有方法，精度提升达7%，在时间精度和鲁棒性上也有显著提升。

Conclusion: 所提框架能有效克服现有方法的不足，提升SNN性能。

Abstract: Spiking neural networks inherently rely on the precise timing of discrete
spike events for information processing. Incorporating additional bio-inspired
degrees of freedom, such as trainable synaptic transmission delays and adaptive
firing thresholds, is essential for fully leveraging the temporal dynamics of
SNNs. Although recent methods have demonstrated the benefits of training
synaptic weights and delays, both in terms of accuracy and temporal
representation, these techniques typically rely on discrete-time simulations,
surrogate gradient approximations, or full access to internal state variables
such as membrane potentials. Such requirements limit training precision and
efficiency and pose challenges for neuromorphic hardware implementation due to
increased memory and I/O bandwidth demands. To overcome these challenges, we
propose an analytical event-driven learning framework that computes exact loss
gradients not only with respect to synaptic weights and transmission delays but
also to adaptive neuronal firing thresholds. Experiments on multiple benchmarks
demonstrate significant gains in accuracy (up to 7%), timing precision, and
robustness compared to existing methods.

</details>


### [168] [Grammatical Structure and Grammatical Variations in Non-Metric Iranian Classical Music](https://arxiv.org/abs/2507.10708)
*Maziar Kanani,Sean O Leary,James McDermott*

Main category: cs.NE

TL;DR: 本文介绍非度量伊朗古典音乐符号数据集、结构解析与变奏生成算法，应用已有算法解析旋律结构，开发变奏方法，经专家评估和统计分析，系统能生成可接受变奏，方法可适配其他音乐。


<details>
  <summary>Details</summary>
Motivation: 为伊朗古典音乐提供结构解析和变奏生成的方法，以用于教育和民族音乐学研究。

Method: 引入非度量伊朗古典音乐数据集，应用已有旋律结构解析算法，将曲调解析为语法来识别主题和乐句；开发变奏方法，通过对语法进行变异生成新曲调；进行统计分析。

Result: 系统成功在变异后产生可接受的变奏。

Conclusion: 虽然案例聚焦伊朗古典音乐，但方法可适配阿拉伯或土耳其古典音乐。

Abstract: In this study we introduce a symbolic dataset composed of non-metric Iranian
classical music, and algorithms for structural parsing of this music, and
generation of variations. The corpus comprises MIDI files and data sheets of
Dastgah Shour from Radif Mirza Abdollah, the foundational repertoire of Iranian
classical music. Furthermore, we apply our previously-introduced algorithm for
parsing melodic structure (Kanani et al., 2023b)to the dataset. Unlike much
Western music, this type of non-metric music does not follow bar-centric
organisation. The non-metric organisation can be captured well by our parsing
algorithm. We parse each tune (Gusheh) into a grammar to identify motifs and
phrases. These grammar representations can be useful for educational and
ethnomusicological purposes. We also further develop a previously-introduced
method of creating melodic variations (Kanani et al., 2023b). After parsing an
existing tune to produce a grammar, by applying mutations to this grammar, we
generate a new grammar. Expanding this new version yields a variation of the
original tune. Variations are assessed by a domain-expert listener.
Additionally, we conduct a statistical analysis of mutation with different
representation setups for our parsing and generation algorithms. The
overarching conclusion is that the system successfully produces acceptable
variations post-mutation. While our case study focuses on Iranian classical
music, the methodology can be adapted for Arabic or Turkish classical music.

</details>


### [169] [Biological Processing Units: Leveraging an Insect Connectome to Pioneer Biofidelic Neural Architectures](https://arxiv.org/abs/2507.10951)
*Siyu Yu,Zihan Qin,Tingshan Liu,Beiya Xu,R. Jacob Vogelstein,Jason Brown,Joshua T. Vogelstein*

Main category: cs.NE

TL;DR: 本文将果蝇幼虫脑连接组转换为生物处理单元（BPU），在多个数据集上测试其性能，展现了生物保真神经架构潜力并呼吁扩大规模。


<details>
  <summary>Details</summary>
Motivation: 探究生物进化电路是否能支持人工智能。

Method: 将果蝇幼虫脑连接图转换为BPU，通过结构化连接组扩展进行缩放，开展特定模态消融实验，构建GNN - BPU和CNN - BPU模型。

Result: 未修改的BPU在MNIST和CIFAR - 10上有较好表现，超大小匹配的MLP；扩展后提升CIFAR - 10性能；GNN - BPU和CNN - BPU在棋类数据集表现出色。

Conclusion: 生物保真神经架构有支持复杂认知任务的潜力，未来应扩展到更大更智能的连接组。

Abstract: The complete connectome of the Drosophila larva brain offers a unique
opportunity to investigate whether biologically evolved circuits can support
artificial intelligence. We convert this wiring diagram into a Biological
Processing Unit (BPU), a fixed recurrent network derived directly from synaptic
connectivity. Despite its modest size 3,000 neurons and 65,000 weights between
them), the unmodified BPU achieves 98% accuracy on MNIST and 58% on CIFAR-10,
surpassing size-matched MLPs. Scaling the BPU via structured connectome
expansions further improves CIFAR-10 performance, while modality-specific
ablations reveal the uneven contributions of different sensory subsystems. On
the ChessBench dataset, a lightweight GNN-BPU model trained on only 10,000
games achieves 60% move accuracy, nearly 10x better than any size transformer.
Moreover, CNN-BPU models with ~2M parameters outperform parameter-matched
Transformers, and with a depth-6 minimax search at inference, reach 91.7%
accuracy, exceeding even a 9M-parameter Transformer baseline. These results
demonstrate the potential of biofidelic neural architectures to support complex
cognitive tasks and motivate scaling to larger and more intelligent connectomes
in future work.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [170] [$\texttt{Droid}$: A Resource Suite for AI-Generated Code Detection](https://arxiv.org/abs/2507.10583)
*Daniil Orel,Indraneil Paul,Iryna Gurevych,Preslav Nakov*

Main category: cs.SE

TL;DR: 构建DroidCollection数据集并开发DroidDetect检测器，发现现有检测器泛化问题，提出对抗数据训练及增强训练方法。


<details>
  <summary>Details</summary>
Motivation: 缺乏全面的用于训练和评估机器生成代码检测器的开放数据集，现有检测器泛化性能差。

Method: 构建DroidCollection数据集，用多任务目标在该数据集上训练DroidDetect检测器，进行实验分析。

Result: 现有检测器泛化性能差，易被对抗样本攻破，少量对抗数据训练可解决问题，度量学习和基于不确定性的重采样可增强训练。

Conclusion: DroidCollection数据集和DroidDetect检测器有效，少量对抗数据训练和增强训练方法可提升检测性能。

Abstract: In this work, we compile $\textbf{$\texttt{DroidCollection}$}$, the most
extensive open data suite for training and evaluating machine-generated code
detectors, comprising over a million code samples, seven programming languages,
outputs from 43 coding models, and over three real-world coding domains.
Alongside fully AI-generated samples, our collection includes human-AI
co-authored code, as well as adversarial samples explicitly crafted to evade
detection. Subsequently, we develop $\textbf{$\texttt{DroidDetect}$}$, a suite
of encoder-only detectors trained using a multi-task objective over
$\texttt{DroidCollection}$. Our experiments show that existing detectors'
performance fails to generalise to diverse coding domains and programming
languages outside of their narrow training data. Additionally, we demonstrate
that while most detectors are easily compromised by humanising the output
distributions using superficial prompting and alignment approaches, this
problem can be easily amended by training on a small amount of adversarial
data. Finally, we demonstrate the effectiveness of metric learning and
uncertainty-based resampling as means to enhance detector training on possibly
noisy distributions.

</details>


### [171] [ARPaCCino: An Agentic-RAG for Policy as Code Compliance](https://arxiv.org/abs/2507.10584)
*Francesco Romeo,Luigi Arena,Francesco Blefari,Francesco Aurelio Pironti,Matteo Lupinacci,Angelo Furfaro*

Main category: cs.SE

TL;DR: 介绍了Policy as Code (PaC) 范式应用的阻碍，提出ARPaCCino系统实现PaC规则自动化生成和验证，实验证明其有效性，凸显了agentic RAG架构潜力。


<details>
  <summary>Details</summary>
Motivation: 解决Policy as Code (PaC) 范式因策略语言复杂和配置错误风险导致的采用阻碍问题。

Method: 结合Large Language Models (LLMs)、Retrieval-Augmented-Generation (RAG)和工具验证，根据自然语言描述生成正式Rego规则，评估IaC合规性并迭代改进配置。

Result: 在基于Terraform的案例研究中，能生成语法和语义正确的策略，识别不合规基础设施并进行修正，使用较小的开源LLMs也有效。

Conclusion: agentic RAG架构有潜力提升PaC工作流的自动化、可靠性和可访问性。

Abstract: Policy as Code (PaC) is a paradigm that encodes security and compliance
policies into machine-readable formats, enabling automated enforcement in
Infrastructure as Code (IaC) environments. However, its adoption is hindered by
the complexity of policy languages and the risk of misconfigurations. In this
work, we present ARPaCCino, an agentic system that combines Large Language
Models (LLMs), Retrieval-Augmented-Generation (RAG), and tool-based validation
to automate the generation and verification of PaC rules. Given natural
language descriptions of the desired policies, ARPaCCino generates formal Rego
rules, assesses IaC compliance, and iteratively refines the IaC configurations
to ensure conformance. Thanks to its modular agentic architecture and
integration with external tools and knowledge bases, ARPaCCino supports policy
validation across a wide range of technologies, including niche or emerging IaC
frameworks. Experimental evaluation involving a Terraform-based case study
demonstrates ARPaCCino's effectiveness in generating syntactically and
semantically correct policies, identifying non-compliant infrastructures, and
applying corrective modifications, even when using smaller, open-weight LLMs.
Our results highlight the potential of agentic RAG architectures to enhance the
automation, reliability, and accessibility of PaC workflows.

</details>


### [172] [Repairing Language Model Pipelines by Meta Self-Refining Competing Constraints at Runtime](https://arxiv.org/abs/2507.10590)
*Mojtaba Eshghie*

Main category: cs.SE

TL;DR: 提出Meta Self - Refining框架解决语言模型管道面对竞争软约束时的低效回溯问题，使LM程序更高效。


<details>
  <summary>Details</summary>
Motivation: 语言模型管道在面对竞争软约束时效果不佳，会出现低效回溯循环。

Method: 引入Meta Self - Refining框架，通过监控执行历史检测振荡失败，调用元修复器LM分析回溯尝试的整体状态并合成战略指令，引导原LM跳出失败的精炼循环。

Result: Meta Self - Refining能成功修复这些循环。

Conclusion: Meta Self - Refining可使语言模型程序更高效。

Abstract: Language Model (LM) pipelines can dynamically refine their outputs against
programmatic constraints. However, their effectiveness collapses when faced
with competing soft constraints, leading to inefficient backtracking loops
where satisfying one constraint violates another. We introduce Meta
Self-Refining, a framework that equips LM pipelines with a meta-corrective
layer to repair these competitions at runtime/inference-time. Our approach
monitors the pipeline's execution history to detect oscillatory failures. Upon
detection, it invokes a meta-repairer LM that analyzes the holistic state of
the backtracking attempts and synthesizes a strategic instruction to balance
the competing requirements. This self-repair instruction guides the original LM
out of a failing refining loop towards a successful output. Our results show
Meta Self-Refining can successfully repair these loops, leading to more
efficient LM programs.

</details>


### [173] [ToolRegistry: A Protocol-Agnostic Tool Management Library for Function-Calling LLMs](https://arxiv.org/abs/2507.10593)
*Peng Ding*

Main category: cs.SE

TL;DR: 本文提出Toolregistry工具管理库，可简化工具集成，评估显示其能减少代码、提升性能，兼容标准，实际案例证明提高开发效率和可维护性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型工具集成方法存在碎片化、协议限制和实现复杂等问题，导致开发成本高。

Method: 提出协议无关的工具管理库Toolregistry，通过统一接口简化工具注册、表示、执行和生命周期管理。

Result: Toolregistry能减少60 - 80%的工具集成代码，通过并发执行性能提升达3.1倍，100%兼容OpenAI函数调用标准，实际案例显示开发效率和代码可维护性显著提高。

Conclusion: Toolregistry有效解决了大语言模型工具集成的问题，开源且有文档，具有实用价值。

Abstract: Large Language Model (LLM) applications are increasingly relying on external
tools to extend their capabilities beyond text generation. However, current
tool integration approaches suffer from fragmentation, protocol limitations,
and implementation complexity, leading to substantial development overhead.
This paper presents Toolregistry, a protocol-agnostic tool management library
that simplifies tool registration, representation, execution, and lifecycle
management via a unified interface. Our evaluation demonstrates that
\toolregistry achieves 60-80% reduction in tool integration code, up to 3.1x
performance improvements through concurrent execution, and 100% compatibility
with OpenAI function calling standards. Real-world case studies show
significant improvements in development efficiency and code maintainability
across diverse integration scenarios. \toolregistry is open-source and
available at https://github.com/Oaklight/ToolRegistry, with comprehensive
documentation at https://toolregistry.readthedocs.io/.

</details>


### [174] [SENSOR: An ML-Enhanced Online Annotation Tool to Uncover Privacy Concerns from User Reviews in Social-Media Applications](https://arxiv.org/abs/2507.10640)
*Labiba Farah,Mohammad Ridwan Kabir,Shohel Ahmed,MD Mohaymen Ul Anam,Md. Sakibul Islam*

Main category: cs.SE

TL;DR: 本文针对社交媒体应用用户评论中隐私问题分类难的问题，提出自动在线注释工具SENSOR和注释模型GRACE，分析约16000条评论，GRACE表现最佳，SENSOR有助于开发者处理隐私相关问题。


<details>
  <summary>Details</summary>
Motivation: 社交媒体应用用户评论量大且复杂，手动识别和优先级排序隐私相关问题困难，且以往研究缺乏对评论特定分类的关注。

Method: 引入自动在线注释工具SENSOR，使用GRU、CBOW和注意力机制的注释模型GRACE，分析约16000条评论，由两名注释者手动标注以获得高一致性的标注数据集。

Result: GRACE在测试模型中表现最佳，宏F1分数为0.9434，宏ROC - AUC为0.9934，准确率为95.10%。

Conclusion: SENSOR有很大潜力帮助开发者提取和处理隐私相关的功能请求或错误报告，增强用户隐私和信任。

Abstract: The widespread use of social media applications has raised significant
privacy concerns, often highlighted in user reviews. These reviews also provide
developers with valuable insights into improving apps by addressing issues and
introducing better features. However, the sheer volume and nuanced nature of
reviews make manual identification and prioritization of privacy-related
concerns challenging for developers. Previous studies have developed software
utilities to automatically classify user reviews as privacy-relevant,
privacy-irrelevant, bug reports, feature requests, etc., using machine
learning. Notably, there is a lack of focus on classifying reviews specifically
as privacy-related feature requests, privacy-related bug reports, or
privacy-irrelevant. This paper introduces SENtinel SORt (SENSOR), an automated
online annotation tool designed to help developers annotate and classify user
reviews into these categories. For automating the annotation of such reviews,
this paper introduces the annotation model, GRACE (GRU-based Attention with
CBOW Embedding), using Gated Recurrent Units (GRU) with Continuous Bag of Words
(CBOW) and Attention mechanism. Approximately 16000 user reviews from seven
popular social media apps on Google Play Store, including Instagram, Facebook,
WhatsApp, Snapchat, X (formerly Twitter), Facebook Lite, and Line were
analyzed. Two annotators manually labelled the reviews, achieving a Cohen's
Kappa value of 0.87, ensuring a labeled dataset with high inter-rater agreement
for training machine learning models. Among the models tested, GRACE
demonstrated the best performance (macro F1-score: 0.9434, macro ROC-AUC:
0.9934, and accuracy: 95.10%) despite class imbalance. SENSOR demonstrates
significant potential to assist developers with extracting and addressing
privacy-related feature requests or bug reports from user reviews, enhancing
user privacy and trust.

</details>


### [175] [A Code Comprehension Benchmark for Large Language Models for Code](https://arxiv.org/abs/2507.10641)
*Jayant Havare,Saurav Chaudhary,Ganesh Ramakrishnan,Kaushik Maharajan,Srikanth Tamilselvam*

Main category: cs.SE

TL;DR: 论文指出大语言模型在代码任务中缺乏代码理解能力，提出用大规模数据集微调模型，经评估微调后模型代码理解能力提升，DPO微调的Codestral - 22B在主观性评分任务中微准确率最高。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽能学习代码表面句法模式，但缺乏代码理解能力，在需要深层语义理解的任务中表现不佳，需要提升代码理解能力。

Method: 使用大规模数据集对不同规模的三个代码模型进行代码理解任务的微调，并在一系列代码理解任务中评估模型。

Result: 微调后模型在代码理解任务中表现提升，如QWQ - 32B模型准确率从70%提升到83.47%，DPO微调的Codestral - 22B在主观性评分任务中微准确率达87.66%。

Conclusion: 通过微调模型可增强其代码理解能力。

Abstract: Large Language Models have shown impressive capabilities in coding tasks like
code generation and code completion, as they have been trained on a large
amount of code data. Also, since one of the core pretraining objectives is Next
Token Prediction, these models tends to learn surface-level syntactic patterns
in code. However, this does not guarantee code comprehension ability i.e. the
ability to capture the semantics of the code. In our opinion, this is the
reason why these models often underperform on tasks that require deeper
semantic understanding, such as code debugging and code optimization. To
address this, we propose fine-tuning these models specifically for code
comprehension tasks using large-scale datasets, enabling them to develop a more
robust understanding of code semantics. We evaluate three code models of
varying sizes on a suite of code comprehension tasks designed to assess
semantic understanding beyond surface-level syntactic pattern matching. In
particular, we analyze performance on the Subjectivity Grading Task and observe
that model performance improves after fine-tuning on relevant downstream tasks.
The most significant improvement is seen in the QWQ-32B model, where accuracy
increases from 70% to 83.47%. A similar or explainable trend is observed across
other models, clearly indicating an enhancement in code comprehension ability.
Among the models studied, the DPO-fine-tuned Codestral-22B achieves the highest
micro-accuracy of 87.66% on the Subjectivity Grading Task.

</details>


### [176] [CodeAssistBench (CAB): Dataset & Benchmarking for Multi-turn Chat-Based Code Assistance](https://arxiv.org/abs/2507.10646)
*Myeongsoo Kim,Shweta Garg,Baishakhi Ray,Varun Kumar,Anoop Deoras*

Main category: cs.SE

TL;DR: 提出CodeAssistBench（CAB）基准框架评估多轮编程辅助，构建测试集评估领先大模型，发现模型在复杂项目环境和独立问题解答能力差距大。


<details>
  <summary>Details</summary>
Motivation: 现有编程基准多聚焦代码生成任务，InfiBench和StackEval等有局限，需能评估多轮编程辅助的基准框架。

Method: 自动从GitHub问题生成可扩展数据集，对代码库自动容器化，通过模拟用户在容器化环境评估模型。

Result: 构建含3286个真实编程问题的测试集，评估显示模型在Stack Overflow问题成功率70 - 83%，在CAB近期问题解决率最高16.49%。

Conclusion: 凸显在复杂项目特定环境提供辅助和解答独立问题存在挑战。

Abstract: Programming assistants powered by large language models have transformed
software development, yet most benchmarks focus narrowly on code generation
tasks. Recent efforts like InfiBench and StackEval attempt to address this gap
using Stack Overflow data but remain limited to single-turn interactions in
isolated contexts, require significant manual curation, and fail to represent
complete project environments. We introduce CodeAssistBench (CAB), the first
benchmark framework for evaluating multi-turn programming assistance in
realistic settings that address real-world questions about actual codebases.
Unlike existing programming Q&A benchmarks, CAB automatically generates
scalable datasets from question-related GitHub issues using configurable
parameters (e.g., repository creation date, star count, programming languages),
and includes automatic containerization of codebases for evaluation. It then
evaluates models through simulated users in these containerized environments
with full codebase access. Using this framework, we constructed a test set of
3,286 real-world programming questions across 231 repositories, spanning seven
programming languages and diverse problem domains. Our evaluation of leading
LLMs reveals a substantial capability gap: while models perform well on Stack
Overflow questions with success rates of 70-83%, they resolve only up to 16.49%
of CAB's recent issues. This discrepancy highlights the challenges of providing
assistance in complex, project-specific contexts versus answering standalone
questions.

</details>


### [177] [Toward Realistic Evaluations of Just-In-Time Vulnerability Prediction](https://arxiv.org/abs/2507.10729)
*Duong Nguyen,Thanh Le-Cong,Triet Huynh Minh Le,M. Ali Babar,Quyet-Thang Huynh*

Main category: cs.SE

TL;DR: 本文指出当前即时漏洞预测（JIT - VP）评估设置理想化，引入大规模数据集进行更现实评估，发现预测性能大幅下降，常用的处理不平衡数据集的技术无效。


<details>
  <summary>Details</summary>
Motivation: 当前JIT - VP评估依赖理想化设置，要评估其在更现实场景（包含漏洞相关和无关提交）下的有效性。

Method: 引入包含超百万提交的大规模公共数据集，对八种先进JIT - VP技术进行实证分析，探索常用处理数据集不平衡的技术。

Result: 应用于现实条件时JIT - VP预测性能显著下降，常用处理不平衡数据集的技术无效。

Conclusion: 强调对JIT - VP进行现实评估的重要性，以及需要特定领域技术解决数据不平衡问题。

Abstract: Modern software systems are increasingly complex, presenting significant
challenges in quality assurance. Just-in-time vulnerability prediction (JIT-VP)
is a proactive approach to identifying vulnerable commits and providing early
warnings about potential security risks. However, we observe that current
JIT-VP evaluations rely on an idealized setting, where the evaluation datasets
are artificially balanced, consisting exclusively of vulnerability-introducing
and vulnerability-fixing commits.
  To address this limitation, this study assesses the effectiveness of JIT-VP
techniques under a more realistic setting that includes both
vulnerability-related and vulnerability-neutral commits. To enable a reliable
evaluation, we introduce a large-scale public dataset comprising over one
million commits from FFmpeg and the Linux kernel. Our empirical analysis of
eight state-of-the-art JIT-VP techniques reveals a significant decline in
predictive performance when applied to real-world conditions; for example, the
average PR-AUC on Linux drops 98\% from 0.805 to 0.016. This discrepancy is
mainly attributed to the severe class imbalance in real-world datasets, where
vulnerability-introducing commits constitute only a small fraction of all
commits.
  To mitigate this issue, we explore the effectiveness of widely adopted
techniques for handling dataset imbalance, including customized loss functions,
oversampling, and undersampling. Surprisingly, our experimental results
indicate that these techniques are ineffective in addressing the imbalance
problem in JIT-VP. These findings underscore the importance of realistic
evaluations of JIT-VP and the need for domain-specific techniques to address
data imbalance in such scenarios.

</details>


### [178] [GenAI-Enabled Backlog Grooming in Agile Software Projects: An Empirical Study](https://arxiv.org/abs/2507.10753)
*Kasper Lien Oftebro,Anh Nguyen-Duc,Kai-Kristian Kemell*

Main category: cs.SE

TL;DR: 研究GenAI助手能否在不牺牲准确性和透明度的情况下自动进行敏捷软件开发项目的待办事项梳理，开发Jira插件实现相关功能，结果显示AI辅助梳理精度达100%，完成时间减少45%，能优化流程和用户体验。


<details>
  <summary>Details</summary>
Motivation: 随着产品待办事项规模和复杂性增加，其中存在冗余、过时或定义不清的任务，影响优先级确定和决策过程，需要有效管理。

Method: 通过设计科学周期，开发Jira插件，将待办事项嵌入向量数据库，通过余弦相似度检测重复项，并利用GPT - 4o模型提出合并、删除或新事项的建议。

Result: AI辅助的待办事项梳理精度达到100%，完成时间减少45%。

Conclusion: 该工具具有简化待办事项细化流程并改善用户体验的潜力。

Abstract: Effective backlog management is critical for ensuring that development teams
remain aligned with evolving requirements and stakeholder expectations.
However, as product backlogs consistently grow in scale and complexity, they
tend to become cluttered with redundant, outdated, or poorly defined tasks,
complicating prioritization and decision making processes. This study
investigates whether a generative-AI (GenAI) assistant can automate backlog
grooming in Agile software projects without sacrificing accuracy or
transparency. Through Design Science cycles, we developed a Jira plug-in that
embeds backlog issues with the vector database, detects duplicates via cosine
similarity, and leverage the GPT-4o model to propose merges, deletions, or new
issues. We found that AI-assisted backlog grooming achieved 100 percent
precision while reducing the time-to-completion by 45 percent. The findings
demonstrated the tool's potential to streamline backlog refinement processes
while improving user experiences.

</details>


### [179] [Towards a Closer Collaboration Between Practice and Research in Agile Software Development Workshop: A Summary and Research Agenda](https://arxiv.org/abs/2507.10785)
*Michael Neumann,Eva-Maria Schön,Mali Senapathi,Maria Rauschenberger,Tiago Silva da Silva*

Main category: cs.SE

TL;DR: 本文介绍首个促进敏捷软件开发研究与实践合作的国际研讨会成果，探讨差距成因、弥合策略和待研究挑战。


<details>
  <summary>Details</summary>
Motivation: 解决敏捷软件开发研究与实践存在显著差距的问题。

Method: 举办首个促进敏捷软件开发研究与实践合作的国际研讨会。

Result: 明确了导致差距的主要主题和因素、弥合差距的策略以及需进一步研究的挑战。

Conclusion: 强调需进一步研究相关挑战以缩小敏捷软件开发研究与实践的差距。

Abstract: Agile software development principles and values have been widely adopted
across various industries, influencing products and services globally. Despite
its increasing popularity, a significant gap remains between research and
practical implementation. This paper presents the findings of the first
international workshop designed to foster collaboration between research and
practice in agile software development. We discuss the main themes and factors
identified by the workshop participants that contribute to this gap, strategies
to bridge it, and the challenges that require further research attention.

</details>


### [180] [How Robust are LLM-Generated Library Imports? An Empirical Study using Stack Overflow](https://arxiv.org/abs/2507.10818)
*Jasmine Latendresse,SayedHassan Khatoonabadi,Emad Shihab*

Main category: cs.SE

TL;DR: 研究六种先进大语言模型对Python库的推荐情况，发现其偏好第三方库，但存在可用性问题。


<details>
  <summary>Details</summary>
Motivation: 开发者用大语言模型辅助编程，需了解其如何推荐软件库。

Method: 用Stack Overflow上的真实Python问题对六种大语言模型进行实证研究，分析其导入库的类型、特征和可用性。

Result: 大语言模型偏好第三方库，推荐成熟、流行和许可宽松的依赖，但4.6%的库因结构不匹配无法自动解析，仅两个模型提供安装指导。

Conclusion: 研究结果为开发者和研究者提供见解，指出提升大语言模型生成代码在软件依赖方面可靠性和可用性的机会。

Abstract: Software libraries are central to the functionality, security, and
maintainability of modern code. As developers increasingly turn to Large
Language Models (LLMs) to assist with programming tasks, understanding how
these models recommend libraries is essential. In this paper, we conduct an
empirical study of six state-of-the-art LLMs, both proprietary and open-source,
by prompting them to solve real-world Python problems sourced from Stack
Overflow. We analyze the types of libraries they import, the characteristics of
those libraries, and the extent to which the recommendations are usable out of
the box. Our results show that LLMs predominantly favour third-party libraries
over standard ones, and often recommend mature, popular, and permissively
licensed dependencies. However, we also identify gaps in usability: 4.6% of the
libraries could not be resolved automatically due to structural mismatches
between import names and installable packages, and only two models (out of six)
provided installation guidance. While the generated code is technically valid,
the lack of contextual support places the burden of manually resolving
dependencies on the user. Our findings offer actionable insights for both
developers and researchers, and highlight opportunities to improve the
reliability and usability of LLM-generated code in the context of software
dependencies.

</details>


### [181] [Past, Present and Future: Exploring Adaptive AI in Software Development Bots](https://arxiv.org/abs/2507.10822)
*Omar Elsisi,Glaucia Melo*

Main category: cs.SE

TL;DR: 本文探讨自适应AI对话代理在软件开发中的作用、发展、挑战及未来应用，认为其有潜力革新软件开发。


<details>
  <summary>Details</summary>
Motivation: 研究自适应AI对话代理在软件开发中的作用，评估其优缺点并探讨未来应用。

Method: 分析自适应AI代理从简单查询系统到先进AI驱动解决方案的演变，研究其集成到软件开发流程中的挑战。

Result: 自适应AI代理可提供动态、上下文感知的帮助，能学习交互并改进，有潜力革新软件开发。

Conclusion: 自适应AI聊天机器人通过提供实时、定制化支持，提升开发周期效率，在软件开发领域有巨大潜力，但需解决数据隐私和伦理问题。

Abstract: Conversational agents, such as chatbots and virtual assistants, have become
essential in software development, boosting productivity, collaboration, and
automating various tasks. This paper examines the role of adaptive AI-powered
conversational agents in software development, highlighting their ability to
offer dynamic, context-aware assistance to developers. Unlike traditional
rule-based systems, adaptive AI agents use machine learning and natural
language processing to learn from interactions and improve over time, providing
more personalized and responsive help. We look at how these tools have evolved
from simple query-based systems to advanced AI-driven solutions like GitHub
Copilot and Microsoft Teams bots. We also explore the challenges of integrating
adaptive AI into software development processes. The study aims to assess the
benefits and limitations of these systems, address concerns like data privacy
and ethical issues, and offer insights into their future use in the field.
Ultimately, adaptive AI chatbots have great potential to revolutionize software
development by delivering real-time, customized support and enhancing the
efficiency of development cycles.

</details>


### [182] [Evaluating Generated Commit Messages with Large Language Models](https://arxiv.org/abs/2507.10906)
*Qunhong Zeng,Yuxia Zhang,Zexiong Ma,Bo Jiang,Ningyuan Sun,Klaas-Jan Stol,Xingyu Mou,Hui Liu*

Main category: cs.SE

TL;DR: 研究用大语言模型评估提交消息质量，通过实验证明结合思维链推理和少样本演示的大语言模型接近人类评估水平，优于传统指标。


<details>
  <summary>Details</summary>
Motivation: 提交消息质量不佳，传统自动评估指标有局限，人工评估耗资源，需探索新的自动评估方法。

Method: 对各种提示策略和最先进的大语言模型进行系统实验。

Result: 结合思维链推理和少样本演示的大语言模型接近人类评估水平，显著优于传统指标，且具有可接受的可重复性、鲁棒性和公平性。

Conclusion: 该研究对使用大语言模型进行提交消息评估进行了全面初步研究，提供了可扩展的替代人工评估的方法，能保持高质量评估。

Abstract: Commit messages are essential in software development as they serve to
document and explain code changes. Yet, their quality often falls short in
practice, with studies showing significant proportions of empty or inadequate
messages. While automated commit message generation has advanced significantly,
particularly with Large Language Models (LLMs), the evaluation of generated
messages remains challenging. Traditional reference-based automatic metrics
like BLEU, ROUGE-L, and METEOR have notable limitations in assessing commit
message quality, as they assume a one-to-one mapping between code changes and
commit messages, leading researchers to rely on resource-intensive human
evaluation. This study investigates the potential of LLMs as automated
evaluators for commit message quality. Through systematic experimentation with
various prompt strategies and state-of-the-art LLMs, we demonstrate that LLMs
combining Chain-of-Thought reasoning with few-shot demonstrations achieve near
human-level evaluation proficiency. Our LLM-based evaluator significantly
outperforms traditional metrics while maintaining acceptable reproducibility,
robustness, and fairness levels despite some inherent variability. This work
conducts a comprehensive preliminary study on using LLMs for commit message
evaluation, offering a scalable alternative to human assessment while
maintaining high-quality evaluation.

</details>


### [183] [SWE-MERA: A Dynamic Benchmark for Agenticly Evaluating Large Language Models on Software Engineering Tasks](https://arxiv.org/abs/2507.11059)
*Pavel Adamenko,Mikhail Ivanov,Aidar Valeev,Rodion Levichev,Pavel Zadorozhny,Ivan Lopatin,Dmitry Babayev,Alena Fenogenova,Valentin Malykh*

Main category: cs.SE

TL;DR: 现有软件工程基准有局限，提出SWE - MERA基准，评估多个大语言模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有软件工程基准尤其是SWE - bench数据集存在数据污染等严重问题。

Method: 自动收集GitHub真实问题并进行严格质量验证，构建可靠流程。

Result: 得到约10000个潜在任务，当前有300个样本，用Aider编码代理评估显示对先进模型有强区分能力。

Conclusion: 报告了2024年9月到2025年6月收集任务上多个大语言模型的性能。

Abstract: The rapid advancement of Large Language Models (LLMs) in software engineering
has revealed critical limitations in existing benchmarks, particularly the
widely used SWE-bench dataset. Recent studies have uncovered severe data
contamination issues, e.g. SWE-bench reports 32.67% of successful patches
involve direct solution leakage and 31.08\% pass due to inadequate test cases.
We introduce SWE-MERA, a dynamic, continuously updated benchmark designed to
address these fundamental challenges through an automated collection of
real-world GitHub issues and rigorous quality validation. Our approach
implements a reliable pipeline that ensures quality while minimizing
contamination risks, resulting in approximately 10,000 potential tasks with 300
samples currently available. Evaluation using the Aider coding agent
demonstrates strong discriminative power in state-of-the-art models. We report
performance across a dozen recent LLMs evaluated on tasks collected between
September 2024 and June 2025.

</details>


### [184] [MT4DP: Data Poisoning Attack Detection for DL-based Code Search Models via Metamorphic Testing](https://arxiv.org/abs/2507.11092)
*Gong Chen,Wenjie Liu,Xiaoyuan Xie,Xunzhu Tang,Tegawendé F. Bissyandé,Songqiang Chen*

Main category: cs.SE

TL;DR: 提出MT4DP框架检测DL代码搜索模型数据投毒攻击，实验表明效果显著，旨在推动相关研究。


<details>
  <summary>Details</summary>
Motivation: 现有DL代码搜索模型数据投毒攻击检测方法效果不足，需解决该安全问题。

Method: 提出Semantically Equivalent Metamorphic Relation (SE - MR)，识别高频词为潜在目标，生成后续查询，重排列表，计算方差检测攻击。

Result: MT4DP显著提升检测效果，平均F1分数比最佳基线高191%，平均精度高265%。

Conclusion: 研究工作有助于推动缓解DL代码搜索模型数据投毒威胁的有效技术研究。

Abstract: Recently, several studies have indicated that data poisoning attacks pose a
severe security threat to deep learning-based (DL-based) code search models.
Attackers inject carefully crafted malicious patterns into the training data,
misleading the code search model to learn these patterns during training.
During the usage of the poisoned code search model for inference, once the
malicious pattern is triggered, the model tends to rank the vulnerability code
higher. However, existing detection methods for data poisoning attacks on
DL-based code search models remain insufficiently effective. To address this
critical security issue, we propose MT4DP, a Data Poisoning Attack Detection
Framework for DL-based Code Search Models via Metamorphic Testing. MT4DP
introduces a novel Semantically Equivalent Metamorphic Relation (SE-MR)
designed to detect data poisoning attacks on DL-based code search models.
Specifically, MT4DP first identifies the high-frequency words from search
queries as potential poisoning targets and takes their corresponding queries as
the source queries. For each source query, MT4DP generates two semantically
equivalent follow-up queries and retrieves its source ranking list. Then, each
source ranking list is re-ranked based on the semantic similarities between its
code snippets and the follow-up queries. Finally, variances between the source
and re-ranked lists are calculated to reveal violations of the SE-MR and warn
the data poisoning attack. Experimental results demonstrate that MT4DP
significantly enhances the detection of data poisoning attacks on DL-based code
search models, outperforming the best baseline by 191% on average F1 score and
265% on average precision. Our work aims to promote further research into
effective techniques for mitigating data poisoning threats on DL-based code
search models.

</details>


### [185] [Automata Models for Effective Bug Description](https://arxiv.org/abs/2507.11146)
*Tom Yaacov,Gera Weiss,Gal Amram,Avi Hayoun*

Main category: cs.SE

TL;DR: 本文用自动机学习和测试技术获取简洁且信息丰富的bug描述，经评估方法有效。


<details>
  <summary>Details</summary>
Motivation: 调试复杂系统耗时，需要更好方法获取bug描述。

Method: 引入Failure Explanations (FE)、Eventual Failure Explanations (EFE)和Early Detection (ED)概念，排除无关信息，聚焦关键测试模式。

Result: 通过多种测试模式和真实基准测试评估，能产生紧凑且信息丰富的bug描述。

Conclusion: 该方法能有效增强bug检测和理解。

Abstract: Debugging complex systems is a crucial yet time-consuming task. This paper
presents the use of automata learning and testing techniques to obtain concise
and informative bug descriptions. We introduce the concepts of Failure
Explanations (FE), Eventual Failure Explanations (EFE), and Early Detection
(ED) to provide meaningful summaries of failing behavior patterns. By factoring
out irrelevant information and focusing on essential test patterns, our
approach aims to enhance bug detection and understanding. We evaluate our
methods using various test patterns and real-world benchmarks, demonstrating
their effectiveness in producing compact and informative bug descriptions.

</details>


### [186] [New Formulation of DNN Statistical Mutation Killing for Ensuring Monotonicity: A Technical Report](https://arxiv.org/abs/2507.11199)
*Jinhan Kim,Nargiz Humbatova,Gunel Jahangirova,Shin Yoo,Paolo Tonella*

Main category: cs.SE

TL;DR: 提出基于Fisher精确检验的统计变异体杀伤新公式，保留统计严谨性并确保单调性。


<details>
  <summary>Details</summary>
Motivation: 现有DeepCrime的统计变异体杀伤准则违反单调性，扩展测试集可能导致之前被杀死的变异体不再被认定为被杀死。

Method: 基于Fisher精确检验提出新的统计变异体杀伤公式。

Result: 无明确提及

Conclusion: 新公式能保留统计严谨性并确保单调性。

Abstract: Mutation testing has emerged as a powerful technique for evaluating the
effectiveness of test suites for Deep Neural Networks. Among existing
approaches, the statistical mutant killing criterion of DeepCrime has leveraged
statistical testing to determine whether a mutant significantly differs from
the original model. However, it suffers from a critical limitation: it violates
the monotonicity property, meaning that expanding a test set may result in
previously killed mutants no longer being classified as killed. In this
technical report, we propose a new formulation of statistical mutant killing
based on Fisher exact test that preserves the statistical rigour of it while
ensuring monotonicity.

</details>


### [187] [An Empirical Study of Multi-Agent RAG for Real-World University Admissions Counseling](https://arxiv.org/abs/2507.11272)
*Anh Nguyen-Duc,Chien Vu Manh,Bao Anh Tran,Viet Phuong Ngo,Luan Le Chi,Anh Quang Nguyen*

Main category: cs.SE

TL;DR: 本文介绍了用于越南高校招生咨询的对话式AI平台MARAUS，通过实验验证其效果并为低资源教育环境部署提供见解。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在招生咨询的应用多为原型或合成基准，缺乏实际部署系统，MARAUS旨在填补这一空白。

Method: 结合混合检索、多智能体编排和基于大语言模型的生成技术，与越南河内交通技术大学合作进行两阶段研究。

Result: MARAUS处理超6000次真实用户交互，准确率达92%，幻觉率从15%降至1.45%，平均响应时间低于4秒，两周部署成本11.58美元。

Conclusion: 为低资源教育环境中部署智能RAG系统提供了可操作的见解。

Abstract: This paper presents MARAUS (Multi-Agent and Retrieval-Augmented University
Admission System), a real-world deployment of a conversational AI platform for
higher education admissions counseling in Vietnam. While large language models
(LLMs) offer potential for automating advisory tasks, most existing solutions
remain limited to prototypes or synthetic benchmarks. MARAUS addresses this gap
by combining hybrid retrieval, multi-agent orchestration, and LLM-based
generation into a system tailored for real-world university admissions. In
collaboration with the University of Transport Technology (UTT) in Hanoi, we
conducted a two-phase study involving technical development and real-world
evaluation. MARAUS processed over 6,000 actual user interactions, spanning six
categories of queries. Results show substantial improvements over LLM-only
baselines: on average 92 percent accuracy, hallucination rates reduced from 15
precent to 1.45 percent, and average response times below 4 seconds. The system
operated cost-effectively, with a two-week deployment cost of 11.58 USD using
GPT-4o mini. This work provides actionable insights for the deployment of
agentic RAG systems in low-resource educational settings.

</details>


### [188] [RefModel: Detecting Refactorings using Foundation Models](https://arxiv.org/abs/2507.11346)
*Pedro Simões,Rohit Gheyi,Rian Melo,Jonhnanthan Oliveira,Márcio Ribeiro,Wesley K. G. Assunção*

Main category: cs.SE

TL;DR: 文章研究用基础模型进行重构检测的可行性，开发RefModel工具，评估多个模型，结果显示模型有竞争力，能泛化到其他语言。


<details>
  <summary>Details</summary>
Motivation: 现有重构检测工具依赖复杂规则和静态分析，难以扩展和泛化到其他编程语言，因此研究使用基础模型进行重构检测。

Method: 开发RefModel工具，在人工生成的Java程序数据集和真实开源项目重构数据上评估Phi4 - 14B、Claude 3.5 Sonnet、Gemini 2.5 Pro和o4 - mini - high等模型，并与传统工具对比。

Result: RefModel有竞争力，部分情况下优于传统工具；Claude 3.5 Sonnet和Gemini 2.5 Pro在真实场景中识别率达97%；模型能较好泛化到Python和Golang。

Conclusion: 使用基础模型进行重构检测是可行的，能提供自然语言解释且定义重构类型简单，具有良好的泛化能力。

Abstract: Refactoring is a common software engineering practice that improves code
quality without altering program behavior. Although tools like ReExtractor+,
RefactoringMiner, and RefDiff have been developed to detect refactorings
automatically, they rely on complex rule definitions and static analysis,
making them difficult to extend and generalize to other programming languages.
In this paper, we investigate the viability of using foundation models for
refactoring detection, implemented in a tool named RefModel. We evaluate
Phi4-14B, and Claude 3.5 Sonnet on a dataset of 858 single-operation
transformations applied to artificially generated Java programs, covering
widely-used refactoring types. We also extend our evaluation by including
Gemini 2.5 Pro and o4-mini-high, assessing their performance on 44 real-world
refactorings extracted from four open-source projects. These models are
compared against RefactoringMiner, RefDiff, and ReExtractor+. RefModel is
competitive with, and in some cases outperform, traditional tools. In
real-world settings, Claude 3.5 Sonnet and Gemini 2.5 Pro jointly identified
97% of all refactorings, surpassing the best-performing static-analysis-based
tools. The models showed encouraging generalization to Python and Golang. They
provide natural language explanations and require only a single sentence to
define each refactoring type.

</details>


### [189] [Security Debt in Practice: Nuanced Insights from Practitioners](https://arxiv.org/abs/2507.11362)
*Chaima Boufaied,Taher Ghaleb,Zainab Masood*

Main category: cs.SE

TL;DR: 文章基于对22位软件从业者的半结构化访谈开展定性实证研究，探讨软件从业者对安全债的认知、管理和沟通情况，发现从业者看法和管理方式有差异，强调要加强安全实践整合等。


<details>
  <summary>Details</summary>
Motivation: 当前对软件从业者在现实场景中如何认知、管理和沟通安全债的实证研究有限，因此开展研究。

Method: 对来自不同角色、组织和国家的22位软件从业者进行半结构化访谈，提出四个研究问题进行探究。

Result: 从业者对安全债的认知和管理存在差异，部分人更看重交付速度，部分人始终将安全作为优先事项。

Conclusion: 需要在软件开发生命周期中加强安全实践的整合，更一致地使用缓解策略，更好地平衡期限、资源和安全相关任务，并关注CIA三元组。

Abstract: With the increasing reliance on software and automation nowadays, tight
deadlines, limited resources, and prioritization of functionality over security
can lead to insecure coding practices. When not handled properly, these
constraints cause unaddressed security vulnerabilities to accumulate over time,
forming Security Debts (SDs). Despite their critical importance, there is
limited empirical evidence on how software practitioners perceive, manage, and
communicate SDs in real-world settings. In this paper, we present a qualitative
empirical study based on semi-structured interviews with 22 software
practitioners across various roles, organizations, and countries. We address
four research questions: i) we assess software practitioners' knowledge of SDs
and awareness of associated security risks, ii) we investigate their behavior
towards SDs, iii) we explore common tools and strategies used to mitigate SDs,
and iv) we analyze how security risks are communicated within teams and to
decision makers. We observe variations in how practitioners perceive and manage
SDs, with some prioritizing delivery speed over security, while others
consistently maintain security as a priority. Our findings emphasize the need
for stronger integration of security practices across the Software Development
Life Cycle (SDLC), more consistent use of mitigation strategies, better
balancing of deadlines, resources, and security-related tasks, with attention
to the Confidentiality, Integrity, and Availability (CIA) triad.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [190] [Kernel Learning for Mean-Variance Trading Strategies](https://arxiv.org/abs/2507.10701)
*Owen Futter,Nicola Muca Cirone,Blanka Horvath*

Main category: q-fin.TR

TL;DR: 本文开发基于核的框架构建动态、路径依赖交易策略，对比签名框架，二者均优于经典马尔可夫方法，框架有建模灵活性且有闭式解。


<details>
  <summary>Details</summary>
Motivation: 在均值 - 方差优化准则下构建动态、路径依赖的交易策略。

Method: 将交易策略参数化为再生核希尔伯特空间（RKHS）中的函数，与签名框架对比。

Result: 在资产动态或预测信号有时间依赖时，核框架和签名框架均显著优于经典马尔可夫方法。

Conclusion: 核框架在建模上有显著灵活性，保留闭式解，是基于梯度优化的替代方案。

Abstract: In this article, we develop a kernel-based framework for constructing
dynamic, pathdependent trading strategies under a mean-variance optimisation
criterion. Building on the theoretical results of (Muca Cirone and Salvi,
2025), we parameterise trading strategies as functions in a reproducing kernel
Hilbert space (RKHS), enabling a flexible and non-Markovian approach to optimal
portfolio problems. We compare this with the signature-based framework of
(Futter, Horvath, Wiese, 2023) and demonstrate that both significantly
outperform classical Markovian methods when the asset dynamics or predictive
signals exhibit temporal dependencies for both synthetic and market-data
examples. Using kernels in this context provides significant modelling
flexibility, as the choice of feature embedding can range from randomised
signatures to the final layers of neural network architectures. Crucially, our
framework retains closed-form solutions and provides an alternative to
gradient-based optimisation.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [191] [TaylorPODA: A Taylor Expansion-Based Method to Improve Post-Hoc Attributions for Opaque Models](https://arxiv.org/abs/2507.10643)
*Yuchi Tang,Iñaki Esnaola,Suzanne Mason,George Panoutsos*

Main category: stat.ML

TL;DR: 本文基于Deng等人（2024）的泰勒展开框架，提出一套严格假设来规范泰勒项特定归因，引入TaylorPODA方法，实证显示其有竞争力，为不透明模型可信部署迈进了一步。


<details>
  <summary>Details</summary>
Motivation: 现有事后模型无关方法缺乏明确系统框架量化特征贡献。

Method: 基于泰勒展开框架，提出‘precision’‘federation’‘zero - discrepancy’假设，引入包含‘adaptation’属性的TaylorPODA方法。

Result: TaylorPODA与基线方法相比取得有竞争力的结果，能提供有原则且便于可视化的解释。

Conclusion: 该工作为不透明模型的可信部署提供了理论基础更强的解释，是迈向可信部署的一步。

Abstract: Existing post-hoc model-agnostic methods generate external explanations for
opaque models, primarily by locally attributing the model output to its input
features. However, they often lack an explicit and systematic framework for
quantifying the contribution of individual features. Building on the Taylor
expansion framework introduced by Deng et al. (2024) to unify existing local
attribution methods, we propose a rigorous set of postulates -- "precision",
"federation", and "zero-discrepancy" -- to govern Taylor term-specific
attribution. Guided by these postulates, we introduce TaylorPODA (Taylor
expansion-derived imPortance-Order aDapted Attribution), which incorporates an
additional "adaptation" property. This property enables alignment with
task-specific goals, especially in post-hoc settings lacking ground-truth
explanations. Empirical evaluations demonstrate that TaylorPODA achieves
competitive results against baseline methods, providing principled and
visualization-friendly explanations. This work represents a step toward the
trustworthy deployment of opaque models by offering explanations with stronger
theoretical grounding.

</details>


### [192] [Robust Multi-Manifold Clustering via Simplex Paths](https://arxiv.org/abs/2507.10710)
*Haoyu Chen,Anna Little,Akin Narayan*

Main category: stat.ML

TL;DR: 本文提出多流形聚类的几何方法，分析距离性质并实验验证其优势，还给出可扩展实现。


<details>
  <summary>Details</summary>
Motivation: 提出新的多流形聚类方法，将可能相交的d维流形集合聚类为单个流形组件。

Method: 计算d - 单纯形上的局部图，用相邻单纯形的二面角作为图权重，计算单纯形图中的无穷路径距离得到LAPD，进行去噪处理。

Result: 实验表明该方法对噪声、曲率和小相交角具有鲁棒性，通常优于其他MMC算法。

Conclusion: 提出的方法有效，且提供了具有准线性计算复杂度的可扩展实现。

Abstract: This article introduces a novel, geometric approach for multi-manifold
clustering (MMC), i.e. for clustering a collection of potentially intersecting,
d-dimensional manifolds into the individual manifold components. We first
compute a locality graph on d-simplices, using the dihedral angle in between
adjacent simplices as the graph weights, and then compute infinity path
distances in this simplex graph. This procedure gives a metric on simplices
which we refer to as the largest angle path distance (LAPD). We analyze the
properties of LAPD under random sampling, and prove that with an appropriate
denoising procedure, this metric separates the manifold components with high
probability. We validate the proposed methodology with extensive numerical
experiments on both synthetic and real-world data sets. These experiments
demonstrate that the method is robust to noise, curvature, and small
intersection angle, and generally out-performs other MMC algorithms. In
addition, we provide a highly scalable implementation of the proposed
algorithm, which leverages approximation schemes for infinity path distance to
achieve quasi-linear computational complexity.

</details>


### [193] [GOLFS: Feature Selection via Combining Both Global and Local Information for High Dimensional Clustering](https://arxiv.org/abs/2507.10956)
*Zhaoyu Xing,Yang Wan,Juan Wen,Wei Zhong*

Main category: stat.ML

TL;DR: 提出新的无监督特征选择方法GOLFS用于高维聚类，结合局部和全局信息，通过迭代算法求解，模拟和实际数据应用证明其性能优秀。


<details>
  <summary>Details</summary>
Motivation: 有监督特征选择的正则化方法因缺乏聚类标签，无法直接用于高维聚类中识别判别性特征，需同时学习伪标签和选择特征。

Method: 提出GOLFS算法，结合流形学习的局部几何结构和正则化自表示的样本全局相关结构，使用迭代算法求解优化问题并证明收敛性。

Result: 模拟和两个实际数据应用表明GOLFS在特征选择和聚类上有出色的有限样本性能。

Conclusion: GOLFS算法通过结合局部和全局信息，能提高特征选择和聚类的准确性，是解决高维聚类问题的有效方法。

Abstract: It is important to identify the discriminative features for high dimensional
clustering. However, due to the lack of cluster labels, the regularization
methods developed for supervised feature selection can not be directly applied.
To learn the pseudo labels and select the discriminative features
simultaneously, we propose a new unsupervised feature selection method, named
GlObal and Local information combined Feature Selection (GOLFS), for high
dimensional clustering problems. The GOLFS algorithm combines both local
geometric structure via manifold learning and global correlation structure of
samples via regularized self-representation to select the discriminative
features. The combination improves the accuracy of both feature selection and
clustering by exploiting more comprehensive information. In addition, an
iterative algorithm is proposed to solve the optimization problem and the
convergency is proved. Simulations and two real data applications demonstrate
the excellent finite-sample performance of GOLFS on both feature selection and
clustering.

</details>


### [194] [Interpretable Bayesian Tensor Network Kernel Machines with Automatic Rank and Feature Selection](https://arxiv.org/abs/2507.11136)
*Afra Kilic,Kim Batselier*

Main category: stat.ML

TL;DR: 提出贝叶斯张量网络核机器，自动推断模型复杂度，实验证明其在多方面性能优越。


<details>
  <summary>Details</summary>
Motivation: 多数基于TN的核方法是确定性的，忽略参数不确定性，且需手动调整模型复杂度超参数。

Method: 提出全概率框架，对TN因子使用诱导稀疏的分层先验自动推断模型复杂度，应用平均场变分推断近似后验。

Result: 平均场近似TN因子得到的贝叶斯ALS算法与确定性算法计算复杂度相同，能无额外成本进行不确定性量化。

Conclusion: 模型在预测准确性、不确定性量化、可解释性和可扩展性方面表现优越。

Abstract: Tensor Network (TN) Kernel Machines speed up model learning by representing
parameters as low-rank TNs, reducing computation and memory use. However, most
TN-based Kernel methods are deterministic and ignore parameter uncertainty.
Further, they require manual tuning of model complexity hyperparameters like
tensor rank and feature dimensions, often through trial-and-error or
computationally costly methods like cross-validation. We propose Bayesian
Tensor Network Kernel Machines, a fully probabilistic framework that uses
sparsity-inducing hierarchical priors on TN factors to automatically infer
model complexity. This enables automatic inference of tensor rank and feature
dimensions, while also identifying the most relevant features for prediction,
thereby enhancing model interpretability. All the model parameters and
hyperparameters are treated as latent variables with corresponding priors.
Given the Bayesian approach and latent variable dependencies, we apply a
mean-field variational inference to approximate their posteriors. We show that
applying a mean-field approximation to TN factors yields a Bayesian ALS
algorithm with the same computational complexity as its deterministic
counterpart, enabling uncertainty quantification at no extra computational
cost. Experiments on synthetic and real-world datasets demonstrate the superior
performance of our model in prediction accuracy, uncertainty quantification,
interpretability, and scalability.

</details>


### [195] [Canonical Bayesian Linear System Identification](https://arxiv.org/abs/2507.11535)
*Andrey Bryutkin,Matthew E. Levine,Iñigo Urteaga,Youssef Marzouk*

Main category: stat.ML

TL;DR: 标准贝叶斯方法用于线性时不变系统识别存在参数不可识别问题，本文通过在贝叶斯框架中嵌入规范形式解决该问题，模拟显示其优势。


<details>
  <summary>Details</summary>
Motivation: 标准贝叶斯方法在LTI系统识别中因参数不可识别，后验复杂导致推理低效且不实用。

Method: 在贝叶斯框架中嵌入LTI系统的规范形式。

Result: 规范形式的推理能解决可识别性问题，解锁有意义的先验，确保伯恩斯坦 - 冯·米塞斯定理条件，模拟显示其计算效率更高、后验可解释且行为良好、能提供稳健的不确定性估计。

Conclusion: 嵌入规范形式的方法在LTI系统识别中优于标准参数化方法。

Abstract: Standard Bayesian approaches for linear time-invariant (LTI) system
identification are hindered by parameter non-identifiability; the resulting
complex, multi-modal posteriors make inference inefficient and impractical. We
solve this problem by embedding canonical forms of LTI systems within the
Bayesian framework. We rigorously establish that inference in these minimal
parameterizations fully captures all invariant system dynamics (e.g., transfer
functions, eigenvalues, predictive distributions of system outputs) while
resolving identifiability. This approach unlocks the use of meaningful,
structure-aware priors (e.g., enforcing stability via eigenvalues) and ensures
conditions for a Bernstein--von Mises theorem -- a link between Bayesian and
frequentist large-sample asymptotics that is broken in standard forms.
Extensive simulations with modern MCMC methods highlight advantages over
standard parameterizations: canonical forms achieve higher computational
efficiency, generate interpretable and well-behaved posteriors, and provide
robust uncertainty estimates, particularly from limited data.

</details>


### [196] [How does Labeling Error Impact Contrastive Learning? A Perspective from Data Dimensionality Reduction](https://arxiv.org/abs/2507.11161)
*Jun Chen,Hong Chen,Yonghua Yu,Yiming Ying*

Main category: stat.ML

TL;DR: 本文研究标注误差对对比学习下游分类性能的理论影响，用SVD降低误报样本，发现其利弊并给出增强建议以提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 以往对比学习理论研究依赖的标签一致性假设在实际中可能不成立，存在标注误差，研究其对下游分类性能的影响。

Method: 先揭示标注误差对下游分类风险的负面影响，应用数据降维方法（如SVD）减少误报样本并进行理论和实证评估。

Result: SVD是双刃剑，可能因增强图连通性降低导致下游分类准确率下降。

Conclusion: 应使用适度嵌入维度、数据膨胀、弱增强和SVD确保大图连通性和小标注误差来提升模型性能。

Abstract: In recent years, contrastive learning has achieved state-of-the-art
performance in the territory of self-supervised representation learning. Many
previous works have attempted to provide the theoretical understanding
underlying the success of contrastive learning. Almost all of them rely on a
default assumption, i.e., the label consistency assumption, which may not hold
in practice (the probability of failure is called labeling error) due to the
strength and randomness of common augmentation strategies, such as random
resized crop (RRC). This paper investigates the theoretical impact of labeling
error on the downstream classification performance of contrastive learning. We
first reveal several significant negative impacts of labeling error on
downstream classification risk. To mitigate these impacts, data dimensionality
reduction method (e.g., singular value decomposition, SVD) is applied on
original data to reduce false positive samples, and establish both theoretical
and empirical evaluations. Moreover, it is also found that SVD acts as a
double-edged sword, which may lead to the deterioration of downstream
classification accuracy due to the reduced connectivity of the augmentation
graph. Based on the above observations, we give the augmentation suggestion
that we should use some moderate embedding dimension (such as $512, 1024$ in
our experiments), data inflation, weak augmentation, and SVD to ensure large
graph connectivity and small labeling error to improve model performance.

</details>


### [197] [From Observational Data to Clinical Recommendations: A Causal Framework for Estimating Patient-level Treatment Effects and Learning Policies](https://arxiv.org/abs/2507.11381)
*Rom Gutman,Shimon Sheiba,Omer Noy Klien,Naama Dekel Bird,Amit Gruber,Doron Aronson,Oren Caspi,Uri Shalit*

Main category: stat.ML

TL;DR: 提出构建患者特定治疗推荐模型的框架，集成现有方法，有实际用例且结果显示能改善患者结局


<details>
  <summary>Details</summary>
Motivation: 构建基于患者层面因果模型和目标试验范式的患者特定治疗推荐模型，关注安全性和有效性及因果识别问题

Method: 不提供具体模型，而是将现有方法和专业知识集成到实用流程中

Result: 在心力衰竭并发急性肾损伤患者治疗优化的实际用例中，该流程能改善患者结局

Conclusion: 所提出的框架可改善患者治疗结局，优于当前治疗方案

Abstract: We propose a framework for building patient-specific treatment recommendation
models, building on the large recent literature on learning patient-level
causal models and inspired by the target trial paradigm of Hernan and Robins.
We focus on safety and validity, including the crucial issue of causal
identification when using observational data. We do not provide a specific
model, but rather a way to integrate existing methods and know-how into a
practical pipeline. We further provide a real world use-case of treatment
optimization for patients with heart failure who develop acute kidney injury
during hospitalization. The results suggest our pipeline can improve patient
outcomes over the current treatment regime.

</details>


### [198] [Joint space-time wind field data extrapolation and uncertainty quantification using nonparametric Bayesian dictionary learning](https://arxiv.org/abs/2507.11385)
*George D. Pasparakis,Ioannis A. Kougioumtzoglou,Michael D. Shields*

Main category: stat.ML

TL;DR: 提出基于非参数贝叶斯字典学习的方法用于时空风场数据外推与统计估计，有优势且经案例验证有效性。


<details>
  <summary>Details</summary>
Motivation: 利用有限/不完整测量数据进行联合时空风场数据外推和相关统计估计。

Method: 基于非参数贝叶斯字典学习，利用稀疏/不完整测量数据构建时间相关优化问题确定随机风场低维表示的展开系数。

Result: 与标准压缩采样处理相比，能量化估计不确定性、自适应选择展开基，外推精度高，通过两个案例验证了方法有效性。

Conclusion: 该方法可用于因各种限制需使用有限数量传感器的风工程应用。

Abstract: A methodology is developed, based on nonparametric Bayesian dictionary
learning, for joint space-time wind field data extrapolation and estimation of
related statistics by relying on limited/incomplete measurements. Specifically,
utilizing sparse/incomplete measured data, a time-dependent optimization
problem is formulated for determining the expansion coefficients of an
associated low-dimensional representation of the stochastic wind field.
Compared to an alternative, standard, compressive sampling treatment of the
problem, the developed methodology exhibits the following advantages. First,
the Bayesian formulation enables also the quantification of the uncertainty in
the estimates. Second, the requirement in standard CS-based applications for an
a priori selection of the expansion basis is circumvented. Instead, this is
done herein in an adaptive manner based on the acquired data. Overall, the
methodology exhibits enhanced extrapolation accuracy, even in cases of
high-dimensional data of arbitrary form, and of relatively large extrapolation
distances. Thus, it can be used, potentially, in a wide range of wind
engineering applications where various constraints dictate the use of a limited
number of sensors. The efficacy of the methodology is demonstrated by
considering two case studies. The first relates to the extrapolation of
simulated wind velocity records consistent with a prescribed joint
wavenumber-frequency power spectral density in a three-dimensional domain (2D
and time). The second pertains to the extrapolation of four-dimensional (3D and
time) boundary layer wind tunnel experimental data that exhibit significant
spatial variability and non-Gaussian characteristics.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [199] [FARS: Factor Augmented Regression Scenarios in R](https://arxiv.org/abs/2507.10679)
*Gian Pietro Bellocca,Ignacio Garrón,Vladimir Rodríguez-Caballero,Esther Ruiz*

Main category: stat.CO

TL;DR: FARS包为R语言提供基于ML - DFMs和FA - QRs建模和设计经济情景的框架及多种功能。


<details>
  <summary>Details</summary>
Motivation: 为计量经济学家、政策制定者和金融分析师获取关键经济变量分布的现实情景。

Method: 使用灵活的多级因子结构提取全球和特定板块因子，计算估计因子的渐近有效置信区域，估计FA - QRs，从分位数预测中恢复完整预测条件密度，在因子受压力时估计条件密度。

Result: 介绍了FARS包可实现的多种功能。

Conclusion: FARS包为经济情景建模和设计提供了全面框架。

Abstract: Obtaining realistic scenarios for the distribution of key economic variables
is crucial for econometricians, policy-makers, and financial analysts. The FARS
package provides a comprehensive framework in R for modeling and designing
economic scenarios based on distributions derived from multi-level dynamic
factor models (ML-DFMs) and factor-augmented quantile regressions (FA-QRs). The
package enables users to: (i) extract global and block-specific factors using a
flexible multi-level factor structure; (ii) compute asymptotically valid
confidence regions for the estimated factors, accounting for uncertainty in the
factor loadings; (iii) estimate FA-QRs; (iv) recover full predictive
conditional densities from quantile forecasts; and (v) estimate the conditional
density when the factors are stressed.

</details>


<div id='q-bio.OT'></div>

# q-bio.OT [[Back]](#toc)

### [200] [An Interpretable AI framework Quantifying Traditional Chinese Medicine Principles Towards Enhancing and Integrating with Modern Biomedicine](https://arxiv.org/abs/2507.11176)
*Haoran Li,Xingye Cheng,Ziyang Huang,Jingyuan Luo,Qianqian Xu,Qiguang Zhao,Tianchen Guo,Yumeng Zhang,Linda Lidan Zhong,Zhaoxiang Bian,Leihan Tang,Aiping Lyu,Liang Tian*

Main category: q-bio.OT

TL;DR: 提出AI框架量化中医症状与疗法映射，构建可解释中医嵌入空间，展示中医原理生物学意义，构建知识图谱助力疾病分析和药物开发。


<details>
  <summary>Details</summary>
Motivation: 传统中医诊疗缺乏量化框架和分子层面证据，限制其可解释性和可靠性。

Method: 用古代经典中医方剂记录训练AI框架，构建中医嵌入空间，将生物医学实体映射到该空间。

Result: 中医嵌入空间可量化中医实践和疗效，其主方向与关键生物功能相关，疾病和草药嵌入接近度与遗传关系一致，还能揭示潜在疾病关系。

Conclusion: 构建的中医知识图谱可预测疾病与靶点、药物等关联，为疾病分析和药物开发提供机会。

Abstract: Traditional Chinese Medicine diagnosis and treatment principles, established
through centuries of trial-and-error clinical practice, directly maps
patient-specific symptom patterns to personalised herbal therapies. These
empirical holistic mapping principles offer valuable strategies to address
remaining challenges of reductionism methodologies in modern biomedicine.
However, the lack of a quantitative framework and molecular-level evidence has
limited their interpretability and reliability. Here, we present an AI
framework trained on ancient and classical TCM formula records to quantify the
symptom pattern-herbal therapy mappings. Interestingly, we find that empirical
TCM diagnosis and treatment are consistent with the encoding-decoding processes
in the AI model. This enables us to construct an interpretable TCM embedding
space (TCM-ES) using the model's quantitative representation of TCM principles.
Validated through broad and extensive TCM patient data, the TCM-ES offers
universal quantification of the TCM practice and therapeutic efficacy. We
further map biomedical entities into the TCM-ES through correspondence
alignment. We find that the principal directions of the TCM-ES are
significantly associated with key biological functions (such as metabolism,
immune, and homeostasis), and that the disease and herb embedding proximity
aligns with their genetic relationships in the human protein interactome, which
demonstrate the biological significance of TCM principles. Moreover, the TCM-ES
uncovers latent disease relationships, and provides alternative metric to
assess clinical efficacy for modern disease-drug pairs. Finally, we construct a
comprehensive and integrative TCM knowledge graph, which predicts potential
associations between diseases and targets, drugs, herbal compounds, and herbal
therapies, providing TCM-informed opportunities for disease analysis and drug
development.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [201] [Approximate solutions to games of ordered preference](https://arxiv.org/abs/2507.11021)
*Pau de las Heras Molins,Eric Roy-Almonacid,Dong Ho Lee,Lasse Peters,David Fridovich-Keil,Georgios Bakirtzis*

Main category: eess.SY

TL;DR: 本文提出‘随时间的字典序迭代最优响应’策略解决有序偏好博弈计算复杂性问题，模拟显示其能有效计算近似最优解。


<details>
  <summary>Details</summary>
Motivation: 现有解决有序偏好博弈的方法存在计算复杂性问题，随着时间范围、玩家数量或偏好水平增加变得难以处理，且滚动时域框架未解决其固有的复杂性增长问题。

Method: 引入‘随时间的字典序迭代最优响应’策略，在滚动时域中用字典序迭代最优响应近似求解，并利用过去信息加速收敛。

Result: 通过模拟交通场景证明该策略能有效为滚动时域有序偏好博弈计算近似最优解，趋向广义纳什均衡。

Conclusion: ‘随时间的字典序迭代最优响应’策略可避免过度的复杂性增长，高效求解滚动时域有序偏好博弈。

Abstract: Autonomous vehicles must balance ranked objectives, such as minimizing travel
time, ensuring safety, and coordinating with traffic. Games of ordered
preference effectively model these interactions but become computationally
intractable as the time horizon, number of players, or number of preference
levels increase. While receding horizon frameworks mitigate long-horizon
intractability by solving sequential shorter games, often warm-started, they do
not resolve the complexity growth inherent in existing methods for solving
games of ordered preference. This paper introduces a solution strategy that
avoids excessive complexity growth by approximating solutions using
lexicographic iterated best response (IBR) in receding horizon, termed
"lexicographic IBR over time." Lexicographic IBR over time uses past
information to accelerate convergence. We demonstrate through simulated traffic
scenarios that lexicographic IBR over time efficiently computes
approximate-optimal solutions for receding horizon games of ordered preference,
converging towards generalized Nash equilibria.

</details>


### [202] [Learning to Quantize and Precode in Massive MIMO Systems for Energy Reduction: a Graph Neural Network Approach](https://arxiv.org/abs/2507.10634)
*Thomas Feys,Liesbet Van der Perre,François Rottenberg*

Main category: eess.SY

TL;DR: 研究粗量化下行大规模MIMO的非线性预编码，提出基于GNN的方法，提升可达和速率并降低功耗。


<details>
  <summary>Details</summary>
Motivation: 大规模MIMO系统中，DAC成为硬件复杂度和功耗瓶颈，需研究粗量化下的非线性预编码。

Method: 提出基于GNN的方法，以自监督方式训练，用直传Gumbel - softmax估计梯度。

Result: 粗量化下可达和速率显著提升，单用户场景降低DAC功耗，但增加数字信号处理功耗。

Conclusion: 该方法在一定带宽下可降低整体功耗，未考虑间接降耗因素。

Abstract: Massive MIMO systems are moving toward increased numbers of radio frequency
chains, higher carrier frequencies and larger bandwidths. As such,
digital-to-analog converters (DACs) are becoming a bottleneck in terms of
hardware complexity and power consumption. In this work, non-linear precoding
for coarsely quantized downlink massive MIMO is studied. Given the NP-hard
nature of this problem, a graph neural network (GNN) is proposed that directly
outputs the precoded quantized vector based on the channel matrix and the
intended transmit symbols. The model is trained in a self-supervised manner, by
directly maximizing the achievable rate. To overcome the non-differentiability
of the objective function, introduced due to the non-differentiable DAC
functions, a straight-through Gumbel-softmax estimation of the gradient is
proposed. The proposed method achieves a significant increase in achievable sum
rate under coarse quantization. For instance, in the single-user case, the
proposed method can achieve the same sum rate as maximum ratio transmission
(MRT) by using one-bit DAC's as compared to 3 bits for MRT. This reduces the
DAC's power consumption by a factor 4-7 and 3 for baseband and RF DACs
respectively. This, however, comes at the cost of increased digital signal
processing power consumption. When accounting for this, the reduction in
overall power consumption holds for a system bandwidth up to 3.5 MHz for
baseband DACs, while the RF DACs can maintain a power reduction of 2.9 for
higher bandwidths. Notably, indirect effects, which further reduce the power
consumption, such as a reduced fronthaul consumption and reduction in other
components, are not considered in this analysis.

</details>


### [203] [Standards-Compliant DM-RS Allocation via Temporal Channel Prediction for Massive MIMO Systems](https://arxiv.org/abs/2507.11064)
*Sehyun Ryu,Hyun Jong Yang*

Main category: eess.SY

TL;DR: 为降低超5G网络反馈开销，提出基于信道预测的参考信号分配（CPRS）概念及标准兼容架构，仿真显示吞吐量提升达36.60%。


<details>
  <summary>Details</summary>
Motivation: 现代大规模MIMO系统天线增多使FDD系统CSI反馈需求大增，现有研究在CSI约束下的参考信号分配方面存在不足。

Method: 引入CPRS概念，联合优化信道预测和DM - RS分配；提出基于ViViT/CNN的架构，将CSI矩阵视为序列图像数据。

Result: 使用NVIDIA Sionna生成的射线追踪信道数据进行仿真，相比基准策略，吞吐量提升达36.60%。

Conclusion: 提出的CPRS概念及架构能有效提升数据吞吐量，适用于动态环境的高效自适应传输。

Abstract: Reducing feedback overhead in beyond 5G networks is a critical challenge, as
the growing number of antennas in modern massive MIMO systems substantially
increases the channel state information (CSI) feedback demand in frequency
division duplex (FDD) systems. To address this, extensive research has focused
on CSI compression and prediction, with neural network-based approaches gaining
momentum and being considered for integration into the 3GPP 5G-Advanced
standards. While deep learning has been effectively applied to CSI-limited
beamforming and handover optimization, reference signal allocation under such
constraints remains surprisingly underexplored. To fill this gap, we introduce
the concept of channel prediction-based reference signal allocation (CPRS),
which jointly optimizes channel prediction and DM-RS allocation to improve data
throughput without requiring CSI feedback. We further propose a
standards-compliant ViViT/CNN-based architecture that implements CPRS by
treating evolving CSI matrices as sequential image-like data, enabling
efficient and adaptive transmission in dynamic environments. Simulation results
using ray-tracing channel data generated in NVIDIA Sionna validate the proposed
method, showing up to 36.60% throughput improvement over benchmark strategies.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [204] [BioScore: A Foundational Scoring Function For Diverse Biomolecular Complexes](https://arxiv.org/abs/2507.10877)
*Yuchen Zhu,Jihong Chen,Yitong Li,Xiaomin Fang,Xianbin Ye,Jingzhou He,Xujun Zhang,Jingxuan Ge,Chao Shen,Xiaonan Zhang,Tingjun Hou,Chang-Yu Hsieh*

Main category: physics.chem-ph

TL;DR: 提出BioScore打分函数，用双尺度几何图学习框架解决关键挑战，支持多任务，在多个基准测试中表现出色，有广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 当前基于结构的打分函数在不同生物分子系统中缺乏通用性，需更好的打分函数用于生物分子复合物的结构评估。

Method: 采用双尺度几何图学习框架，有针对结构评估和亲和力预测的定制模块。

Result: 在16个基准测试中，BioScore始终优于或与70种传统和深度学习方法相当；预训练提升亲和力预测和结合相关性；跨系统泛化实现零样本和少样本预测；统一表示改善挑战性系统的亲和力预测。

Conclusion: BioScore为复杂生物分子结构评估建立了稳健且通用的框架。

Abstract: Structural assessment of biomolecular complexes is vital for translating
molecular models into functional insights, shaping our understanding of biology
and aiding drug discovery. However, current structure-based scoring functions
often lack generalizability across diverse biomolecular systems. We present
BioScore, a foundational scoring function that addresses key challenges -- data
sparsity, cross-system representation, and task compatibility -- through a
dual-scale geometric graph learning framework with tailored modules for
structure assessment and affinity prediction. BioScore supports a wide range of
tasks, including affinity prediction, conformation ranking, and structure-based
virtual screening. Evaluated on 16 benchmarks spanning proteins, nucleic acids,
small molecules, and carbohydrates, BioScore consistently outperforms or
matches 70 traditional and deep learning methods. Our newly proposed PPI
Benchmark further enables comprehensive evaluation of protein-protein complex
scoring. BioScore demonstrates broad applicability: (1) pretraining on
mixed-structure data boosts protein-protein affinity prediction by up to 40%
and antigen-antibody binding correlation by over 90%; (2) cross-system
generalizability enables zero- and few-shot prediction with up to 71%
correlation gain; and (3) its unified representation captures chemically
challenging systems such as cyclic peptides, improving affinity prediction by
over 60%. BioScore establishes a robust and generalizable framework for
structural assessment across complex biomolecular landscapes.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [205] [HKGAI-V1: Towards Regional Sovereign Large Language Model for Hong Kong](https://arxiv.org/abs/2507.11502)
*Sirui Han,Junqi Zhu,Ruiyuan Zhang,Yike Guo*

Main category: cs.CL

TL;DR: 本文介绍了HKGAI - V1基础主权大语言模型的开发，该模型针对香港地区需求设计，在处理特定文化敏感查询上表现出色，还开发了评估基准，为区域AI系统开发提供蓝图。


<details>
  <summary>Details</summary>
Motivation: 建立适合香港的价值对齐AI基础设施，应对香港独特的多语言环境、社会法律背景和本地文化价值考量。

Method: 基于DeepSeek架构，通过多方面全参数微调与区域规范对齐，集成检索增强生成（RAG）系统。

Result: 成功开发HKGAI - V1，在处理香港特定文化敏感查询上优于通用模型；开发了专有的对抗性香港价值基准。

Conclusion: 本文不仅提供了技术成果，还为开发根植于本地特色的先进区域AI系统提供了可复制的蓝图。

Abstract: This paper presents the development of HKGAI-V1, a foundational sovereign
large language model (LLM), developed as part of an initiative to establish
value-aligned AI infrastructure specifically tailored for Hong Kong. Addressing
the region's unique multilingual environment (Cantonese, Mandarin, and
English), its distinct socio-legal context under the "one country, two systems"
framework, and specific local cultural and value considerations, the model is
built upon the DeepSeek architecture and systematically aligned with regional
norms through a multifaceted full parameter fine-tuning process. It is further
integrated with a retrieval-augmented generation (RAG) system to ensure timely
and factually grounded information access. The core contribution lies in the
design and implementation of a comprehensive, region-specific AI alignment and
safety framework, demonstrated through two key achievements: 1) The successful
development of HKGAI-V1 itself - which outper-forms general-purpose models in
handling Hong Kong-specific culturally sensitive queries, and embodies a
"governance-embedded" approach to digital sovereignty - empowers Hong Kong to
exercise control over AI applications in critical sectors including public
services, legal systems, and edu-cation. 2) The development of the proprietary
Adversarial HK Value Benchmark, a rigorous tool for evaluating model alignment
with local ethical and legal stand-ards under challenging conditions. By
documenting these achievements, the paper provides not only a technological
artifact but also a replicable blueprint for developing advanced, regionally
focused AI systems deeply rooted in their local identities.

</details>


### [206] [Applying Text Embedding Models for Efficient Analysis in Labeled Property Graphs](https://arxiv.org/abs/2507.10772)
*Michal Podstawski*

Main category: cs.CL

TL;DR: 本文探索使用预训练文本嵌入模型实现带标签属性图的高效语义分析，能提升分析准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 带标签属性图有丰富文本属性，合理利用可增强分析任务，需要方法实现高效语义分析。

Method: 嵌入文本节点和边属性，将语言模型嵌入集成到图管道且不改变图结构。

Result: 支持节点分类和关系预测等下游任务，增强了上下文理解。

Conclusion: 文本语义可显著提升属性图分析的准确性和可解释性。

Abstract: Labeled property graphs often contain rich textual attributes that can
enhance analytical tasks when properly leveraged. This work explores the use of
pretrained text embedding models to enable efficient semantic analysis in such
graphs. By embedding textual node and edge properties, we support downstream
tasks including node classification and relation prediction with improved
contextual understanding. Our approach integrates language model embeddings
into the graph pipeline without altering its structure, demonstrating that
textual semantics can significantly enhance the accuracy and interpretability
of property graph analysis.

</details>


### [207] [Seq vs Seq: An Open Suite of Paired Encoders and Decoders](https://arxiv.org/abs/2507.11412)
*Orion Weller,Kathryn Ricci,Marc Marone,Antoine Chaffin,Dawn Lawrie,Benjamin Van Durme*

Main category: cs.CL

TL;DR: 介绍SOTA开放数据Ettin模型套件，对比编码器和解码器模型，发现各自适合任务，开源研究成果。


<details>
  <summary>Details</summary>
Motivation: 此前对比编码器和解码器模型受参数、训练技术和数据集差异影响，需更公平对比。

Method: 引入Ettin模型套件，对编码器和解码器模型采用相同训练配方。

Result: 在各自规模类别中取得SOTA成果，编码器适合分类和检索任务，解码器适合生成任务，跨类型训练表现不佳。

Conclusion: 编码器和解码器模型各有优势，跨类型训练不如使用对应目标模型，开源成果供后续研究。

Abstract: The large language model (LLM) community focuses almost exclusively on
decoder-only language models, since they are easier to use for text generation.
However, a large subset of the community still uses encoder-only models for
tasks such as classification or retrieval. Previous work has attempted to
compare these architectures, but is forced to make comparisons with models that
have different numbers of parameters, training techniques, and datasets. We
introduce the SOTA open-data Ettin suite of models: paired encoder-only and
decoder-only models ranging from 17 million parameters to 1 billion, trained on
up to 2 trillion tokens. Using the same recipe for both encoder-only and
decoder-only models produces SOTA recipes in both categories for their
respective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as
decoders. Like previous work, we find that encoder-only models excel at
classification and retrieval tasks while decoders excel at generative tasks.
However, we show that adapting a decoder model to encoder tasks (and vice
versa) through continued training is subpar compared to using only the reverse
objective (i.e. a 400M encoder outperforms a 1B decoder on MNLI, and vice versa
for generative tasks). We open-source all artifacts of this study including
training data, training order segmented by checkpoint, and 200+ checkpoints to
allow future work to analyze or extend all aspects of training.

</details>


### [208] [Truth Sleuth and Trend Bender: AI Agents to fact-check YouTube videos and influence opinions](https://arxiv.org/abs/2507.10577)
*Logé Cécile,Ghori Rehan*

Main category: cs.CL

TL;DR: 本文提出一种AI系统对抗YouTube上的错误信息，含Truth Sleuth和Trend Bender两个代理，实验证明其能参与用户互动并影响观点，可助力打造更知情的网络空间。


<details>
  <summary>Details</summary>
Motivation: 当今数字世界中错误信息传播迅速，尤其是在YouTube平台，需要有效方法对抗错误信息。

Method: 开发AI系统，包含Truth Sleuth和Trend Bender两个代理。Truth Sleuth用RAG方法评估视频中说法的真实性并生成报告，Trend Bender利用报告和文章库生成有说服力的评论，且有自我评估循环迭代优化。

Result: 在基准数据集实验和YouTube实际部署中展示了系统能力，事实核查代理准确性高。

Conclusion: AI驱动的干预措施在对抗错误信息和促进更知情的网络空间方面有潜力。

Abstract: Misinformation poses a significant threat in today's digital world, often
spreading rapidly through platforms like YouTube. This paper introduces a novel
approach to combating misinformation by developing an AI-powered system that
not only fact-checks claims made in YouTube videos but also actively engages
users in the comment section and challenge misleading narratives. Our system
comprises two main agents: Truth Sleuth and Trend Bender.
  Truth Sleuth extracts claims from a YouTube video, uses a Retrieval-Augmented
Generation (RAG) approach - drawing on sources like Wikipedia, Google Search,
Google FactCheck - to accurately assess their veracity and generates a nuanced
and comprehensive report. Through rigorous prompt engineering, Trend Bender
leverages this report along with a curated corpus of relevant articles to
generate insightful and persuasive comments designed to stimulate a productive
debate. With a carefully set up self-evaluation loop, this agent is able to
iteratively improve its style and refine its output.
  We demonstrate the system's capabilities through experiments on established
benchmark datasets and a real-world deployment on YouTube, showcasing its
potential to engage users and potentially influence perspectives. Our findings
highlight the high accuracy of our fact-checking agent, and confirm the
potential of AI-driven interventions in combating misinformation and fostering
a more informed online space.

</details>


### [209] [An Offline Mobile Conversational Agent for Mental Health Support: Learning from Emotional Dialogues and Psychological Texts with Student-Centered Evaluation](https://arxiv.org/abs/2507.10580)
*Vimaleswar A,Prabhu Nandan Sahu,Nilesh Kumar Sahu,Haroon R Lone*

Main category: cs.CL

TL;DR: 本文提出离线智能手机应用EmoSApp用于心理健康和情感支持，介绍其技术实现并评估有效性，为未来心理健康解决方案提供蓝图。


<details>
  <summary>Details</summary>
Motivation: 当前数字平台在心理健康支持方面存在用户可及性、网络连接和数据隐私等问题，需要离线智能手机解决方案。

Method: 提出EmoSApp，利用大语言模型，使用Torchtune和Executorch针对资源受限设备进行微调、量化和部署，在自定义数据集上微调LLaMA - 3.2 - 1B - Instruct模型。

Result: 定性评估显示EmoSApp能连贯、共情地回应，保持互动对话并提供相关建议；定量评估证明微调量化模型在低资源环境有效。

Conclusion: EmoSApp通过优先设备端部署和领域适配，为便携式、安全且定制化的人工智能心理健康解决方案提供范例。

Abstract: Mental health plays a crucial role in the overall well-being of an
individual. In recent years, digital platforms have been increasingly used to
expand mental health and emotional support. However, there are persistent
challenges related to limited user accessibility, internet connectivity, and
data privacy, which highlight the need for an offline, smartphone-based
solution. To address these challenges, we propose EmoSApp (Emotional Support
App): an entirely offline, smartphone-based conversational app designed for
mental health and emotional support. The system leverages Large Language Models
(LLMs), specifically fine-tuned, quantized and deployed using Torchtune and
Executorch for resource-constrained devices, allowing all inferences to occur
on the smartphone. To equip EmoSApp with robust domain expertise, we fine-tuned
the LLaMA-3.2-1B-Instruct model on our custom curated ``Knowledge dataset'' of
14,582 mental-health QA pairs, along with the multi-turn conversational data.
  Through qualitative human evaluation with the student population, we
demonstrate that EmoSApp has the ability to respond coherently, empathetically,
maintain interactive dialogue, and provide relevant suggestions to user's
mental health problems. Additionally, quantitative evaluations on nine standard
commonsense and reasoning benchmarks demonstrate the efficacy of our
fine-tuned, quantized model in low-resource settings. By prioritizing on-device
deployment and specialized domain adaptation, EmoSApp serves as a blueprint for
future innovations in portable, secure, and highly tailored AI-driven mental
health solutions.

</details>


### [210] [A Taxonomy for Design and Evaluation of Prompt-Based Natural Language Explanations](https://arxiv.org/abs/2507.10585)
*Isar Nejadgholi,Mona Omidyeganeh,Marc-Antoine Drouin,Jonathan Boisvert*

Main category: cs.CL

TL;DR: 论文基于可解释人工智能文献创建适用于基于提示的自然语言解释的分类法，为透明AI系统的自然语言解释提供框架。


<details>
  <summary>Details</summary>
Motivation: 有效AI治理需结构化方法让利益相关者访问和验证AI系统行为，自然语言解释对说明模型行为很关键，需研究其特征和治理影响。

Method: 借鉴可解释人工智能文献，从上下文、生成与呈现、评估三个维度创建更新的可解释人工智能分类法。

Result: 创建了覆盖三个维度的适用于基于提示的自然语言解释的分类法。

Conclusion: 该分类法为研究人员、审计人员和政策制定者提供了表征、设计和改进透明AI系统自然语言解释的框架。

Abstract: Effective AI governance requires structured approaches for stakeholders to
access and verify AI system behavior. With the rise of large language models,
Natural Language Explanations (NLEs) are now key to articulating model
behavior, which necessitates a focused examination of their characteristics and
governance implications. We draw on Explainable AI (XAI) literature to create
an updated XAI taxonomy, adapted to prompt-based NLEs, across three dimensions:
(1) Context, including task, data, audience, and goals; (2) Generation and
Presentation, covering generation methods, inputs, interactivity, outputs, and
forms; and (3) Evaluation, focusing on content, presentation, and user-centered
properties, as well as the setting of the evaluation. This taxonomy provides a
framework for researchers, auditors, and policymakers to characterize, design,
and enhance NLEs for transparent AI systems.

</details>


### [211] [AutoRAG-LoRA: Hallucination-Triggered Knowledge Retuning via Lightweight Adapters](https://arxiv.org/abs/2507.10586)
*Kaushik Dwivedi,Padmanabh Patanjali Mishra*

Main category: cs.CL

TL;DR: 提出AutoRAG - LoRA框架应对大语言模型幻觉问题，集成多种技术降低事实偏差，保持模型效率和模块化。


<details>
  <summary>Details</summary>
Motivation: 大语言模型易产生幻觉，影响现实应用的可信度，需解决该问题。

Method: 采用轻量级基于LoRA的适配器和KL正则化训练，集成自动提示重写、混合检索和低秩适配器调优，加入幻觉检测模块和反馈校正循环。

Result: AutoRAG - LoRA显著减少了事实偏差。

Conclusion: AutoRAG - LoRA在减少事实偏差的同时能保持模型的效率和模块化。

Abstract: Large Language Models (LLMs) have demonstrated remarkable fluency across a
range of natural language tasks, yet remain vulnerable to hallucinations -
factual inaccuracies that undermine trust in real world deployment. We present
AutoRAG-LoRA, a modular framework for Retrieval-Augmented Generation (RAG) that
tackles hallucination in large language models through lightweight LoRA-based
adapters and KL-regularized training. Our pipeline integrates automated prompt
rewriting, hybrid retrieval, and low-rank adapter tuning to ground responses in
retrieved evidence. A hallucination detection module, using both
classifier-based and self-evaluation techniques, assigns confidence scores to
generated outputs, triggering an optional feedback correction loop. This loop
enforces factual alignment via contrastive KL loss and adapter fine tuning. We
demonstrate that AutoRAG-LoRA significantly reduces the factual drift while
preserving the efficiency and modularity of the model.

</details>


### [212] [Anthropomimetic Uncertainty: What Verbalized Uncertainty in Language Models is Missing](https://arxiv.org/abs/2507.10587)
*Dennis Ulmer,Alexandra Lorson,Ivan Titov,Christian Hardmeier*

Main category: cs.CL

TL;DR: 用户依赖大语言模型，但模型输出准确性存疑，需向用户传达置信度。研究提出拟人化不确定性概念，回顾相关研究，指出偏见并给出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型输出常表述自信但准确性存疑，需向用户传达置信度以促进人机协作并减少潜在危害，且当前NLP研究忽视人类不确定性交流细微差别和数据偏见。

Method: 全面回顾人类不确定性交流研究，调查正在进行的研究，并进行额外分析以发现言语化不确定性中被忽视的偏见。

Result: 发现言语化不确定性中存在被忽视的偏见。

Conclusion: 指出人机不确定性交流中的独特因素，并将拟人化不确定性拆解为NLP未来研究方向。

Abstract: Human users increasingly rely on natural language interactions with large
language models (LLMs) in order to receive help on a large variety of tasks and
problems. However, the trustworthiness and perceived legitimacy of LLMs is
undermined by the fact that their output is frequently stated in very confident
terms, even when its accuracy is questionable. Therefore, there is a need to
signal the confidence of the language model to a user in order to reap the
benefits of human-machine collaboration and mitigate potential harms.
Verbalized uncertainty is the expression of confidence with linguistic means,
an approach that integrates perfectly into language-based interfaces.
Nevertheless, most recent research in natural language processing (NLP)
overlooks the nuances surrounding human uncertainty communication and the data
biases that influence machine uncertainty communication. We argue for
anthropomimetic uncertainty, meaning that intuitive and trustworthy uncertainty
communication requires a degree of linguistic authenticity and personalization
to the user, which could be achieved by emulating human communication. We
present a thorough overview over the research in human uncertainty
communication, survey ongoing research, and perform additional analyses to
demonstrate so-far overlooked biases in verbalized uncertainty. We conclude by
pointing out unique factors in human-machine communication of uncertainty and
deconstruct anthropomimetic uncertainty into future research directions for
NLP.

</details>


### [213] [PLEX: Perturbation-free Local Explanations for LLM-Based Text Classification](https://arxiv.org/abs/2507.10596)
*Yogachandran Rahulamathavan,Misbah Farooq,Varuna De Silva*

Main category: cs.CL

TL;DR: 提出无扰动局部解释方法PLEX用于大语言模型文本分类解释，在四个任务上验证其有效性，加速解释并减少开销。


<details>
  <summary>Details</summary>
Motivation: 大语言模型文本分类缺乏可解释性，现有XAI方法依赖计算成本高的扰动，需新方法解决。

Method: 提出PLEX方法，利用大语言模型上下文嵌入和“孪生网络”风格神经网络，一次性训练后无需后续扰动。

Result: 在四个分类任务上与LIME和SHAP有超92%一致性，“压力测试”准确识别影响词，部分情况捕捉关键特征表现更优，加速解释，减少时间和计算开销。

Conclusion: PLEX为基于大语言模型的可解释文本分类提供了有前景的解决方案。

Abstract: Large Language Models (LLMs) excel in text classification, but their
complexity hinders interpretability, making it difficult to understand the
reasoning behind their predictions. Explainable AI (XAI) methods like LIME and
SHAP offer local explanations by identifying influential words, but they rely
on computationally expensive perturbations. These methods typically generate
thousands of perturbed sentences and perform inferences on each, incurring a
substantial computational burden, especially with LLMs. To address this, we
propose \underline{P}erturbation-free \underline{L}ocal \underline{Ex}planation
(PLEX), a novel method that leverages the contextual embeddings extracted from
the LLM and a ``Siamese network" style neural network trained to align with
feature importance scores. This one-off training eliminates the need for
subsequent perturbations, enabling efficient explanations for any new sentence.
We demonstrate PLEX's effectiveness on four different classification tasks
(sentiment, fake news, fake COVID-19 news and depression), showing more than
92\% agreement with LIME and SHAP. Our evaluation using a ``stress test"
reveals that PLEX accurately identifies influential words, leading to a similar
decline in classification accuracy as observed with LIME and SHAP when these
words are removed. Notably, in some cases, PLEX demonstrates superior
performance in capturing the impact of key features. PLEX dramatically
accelerates explanation, reducing time and computational overhead by two and
four orders of magnitude, respectively. This work offers a promising solution
for explainable LLM-based text classification.

</details>


### [214] [Emergence of Hierarchical Emotion Organization in Large Language Models](https://arxiv.org/abs/2507.10599)
*Bo Zhao,Maya Okawa,Eric J. Bigelow,Rose Yu,Tomer Ullman,Ekdeep Singh Lubana,Hidenori Tanaka*

Main category: cs.CL

TL;DR: 分析大语言模型输出中情绪状态的概率依赖，发现其形成与人类心理模型一致的层次化情绪树，存在情绪识别偏差，结果暗示可用认知理论改进模型评估。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型驱动对话代理的背景下，为实现伦理部署，需了解其对用户情绪状态的建模方式。

Method: 受情绪轮启发，分析模型输出中情绪状态的概率依赖。

Result: 大语言模型自然形成与人类心理模型一致的层次化情绪树，更大模型层次更复杂；在不同社会经济角色的情绪识别中存在系统性偏差，交叉、代表性不足群体的错误分类更严重；人类研究有相似发现。

Conclusion: 研究凸显了大语言模型中出现的情感推理能力，暗示可用认知理论进行更好的模型评估。

Abstract: As large language models (LLMs) increasingly power conversational agents,
understanding how they model users' emotional states is critical for ethical
deployment. Inspired by emotion wheels -- a psychological framework that argues
emotions organize hierarchically -- we analyze probabilistic dependencies
between emotional states in model outputs. We find that LLMs naturally form
hierarchical emotion trees that align with human psychological models, and
larger models develop more complex hierarchies. We also uncover systematic
biases in emotion recognition across socioeconomic personas, with compounding
misclassifications for intersectional, underrepresented groups. Human studies
reveal striking parallels, suggesting that LLMs internalize aspects of social
perception. Beyond highlighting emergent emotional reasoning in LLMs, our
results hint at the potential of using cognitively-grounded theories for
developing better model evaluations.

</details>


### [215] [Language Models for Adult Service Website Text Analysis](https://arxiv.org/abs/2507.10743)
*Nickolas Freeman,Thanh Nguyen,Gregory Bott,Jason Parton,Collin Francel*

Main category: cs.CL

TL;DR: 本文研究成人服务网站（ASW）文本分析用于打击性交易，提出定制transformer模型优于知名模型，并展示其在三项任务中的应用。


<details>
  <summary>Details</summary>
Motivation: 成人服务网站与性交易相关，组织利用其数据识别受害者，但文本分析是关键挑战。

Method: 对语言建模方法进行全面研究，包括简单信息检索方法、预训练transformer和定制transformer模型。

Result: 定制模型在多项指标上优于知名编码器仅transformer模型，并在三项相关任务中展示应用。

Conclusion: 所开发模型是ASW文本分析的重大进展，可用于多种下游应用和研究。

Abstract: Sex trafficking refers to the use of force, fraud, or coercion to compel an
individual to perform in commercial sex acts against their will. Adult service
websites (ASWs) have and continue to be linked to sex trafficking, offering a
platform for traffickers to advertise their victims. Thus, organizations
involved in the fight against sex trafficking often use ASW data when
attempting to identify potential sex trafficking victims. A critical challenge
in transforming ASW data into actionable insight is text analysis. Previous
research using ASW data has shown that ASW ad text is important for linking
ads. However, working with this text is challenging due to its extensive use of
emojis, poor grammar, and deliberate obfuscation to evade law enforcement
scrutiny. We conduct a comprehensive study of language modeling approaches for
this application area, including simple information retrieval methods,
pre-trained transformers, and custom transformer models. We demonstrate that
characteristics of ASW text data allow efficient custom transformer models to
be trained with relatively small GPU resources and used efficiently for
inference on consumer hardware. Our custom models outperform fine-tuned
variants of well-known encoder-only transformer models, including BERT-base,
RoBERTa, and ModernBERT, on accuracy, recall, F1 score, and ROC AUC. We
demonstrate the use of our best-performing custom configuration on three tasks
related to ASW data analysis: (i) decomposing the giant component in a graph
representation of ASW data, (ii) clustering ASW ad text, and (iii) using the
learned token embeddings to understand the use of emojis in the illicit context
we study. The models we develop represent a significant advancement in ASW text
analysis, which can be leveraged in a variety of downstream applications and
research.

</details>


### [216] [HanjaBridge: Resolving Semantic Ambiguity in Korean LLMs via Hanja-Augmented Pre-Training](https://arxiv.org/abs/2507.10920)
*Seungho Choi*

Main category: cs.CL

TL;DR: 提出HanjaBridge技术，集成到持续预训练框架，改善韩语理解，有跨语言迁移效果且推理无额外成本。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在韩语等低资源语言表现不佳，存在同音字等语义歧义问题。

Method: 提出HanjaBridge意义注入技术，将所有可能的汉字候选呈现给模型，结合标记级知识蒸馏。

Result: HanjaBridge显著提升韩语理解，在KoBALT基准上相对提升21%，有积极的跨语言迁移。

Conclusion: HanjaBridge能有效解决韩语语义歧义问题，提升韩语理解，且推理无额外成本。

Abstract: Large language models (LLMs) often show poor performance in low-resource
languages like Korean, partly due to unique linguistic challenges such as
homophonous Sino-Korean words that are indistinguishable in Hangul script. To
address this semantic ambiguity, we propose HanjaBridge, a novel
meaning-injection technique integrated into a continual pre-training (CPT)
framework. Instead of deterministically mapping a word to a single Hanja
(Chinese character), HanjaBridge presents the model with all possible Hanja
candidates for a given homograph, encouraging the model to learn contextual
disambiguation. This process is paired with token-level knowledge distillation
to prevent catastrophic forgetting. Experimental results show that HanjaBridge
significantly improves Korean language understanding, achieving a 21\% relative
improvement on the KoBALT benchmark. Notably, by reinforcing semantic alignment
between Korean and Chinese through shared Hanja, we observe a strong positive
cross-lingual transfer. Furthermore, these gains persist even when Hanja
augmentation is omitted at inference time, ensuring practical efficiency with
no additional run-time cost.

</details>


### [217] [Modeling Understanding of Story-Based Analogies Using Large Language Models](https://arxiv.org/abs/2507.10957)
*Kalit Inani,Keshav Kabra,Vijay Marupudi,Sashank Varma*

Main category: cs.CL

TL;DR: 研究聚焦故事类比映射任务，细粒度评估大语言模型类比推理能力，对比人类表现，还考察模型大小和架构影响。


<details>
  <summary>Details</summary>
Motivation: 此前研究表明大语言模型在类比推理中缺乏类人推理能力，本研究旨在评估其在检测和映射类比中与人类表现的契合度。

Method: 使用句子嵌入评估类比语义表征，探究明确提示解释类比的效果，在单个类比层面评估推理能力，还考察模型大小和不同架构的影响。

Result: 未提及具体结果。

Conclusion: 该工作增进了对大语言模型类比推理能力及其作为人类推理模型潜力的理解。

Abstract: Recent advancements in Large Language Models (LLMs) have brought them closer
to matching human cognition across a variety of tasks. How well do these models
align with human performance in detecting and mapping analogies? Prior research
has shown that LLMs can extract similarities from analogy problems but lack
robust human-like reasoning. Building on Webb, Holyoak, and Lu (2023), the
current study focused on a story-based analogical mapping task and conducted a
fine-grained evaluation of LLM reasoning abilities compared to human
performance. First, it explored the semantic representation of analogies in
LLMs, using sentence embeddings to assess whether they capture the similarity
between the source and target texts of an analogy, and the dissimilarity
between the source and distractor texts. Second, it investigated the
effectiveness of explicitly prompting LLMs to explain analogies. Throughout, we
examine whether LLMs exhibit similar performance profiles to those observed in
humans by evaluating their reasoning at the level of individual analogies, and
not just at the level of overall accuracy (as prior studies have done). Our
experiments include evaluating the impact of model size (8B vs. 70B parameters)
and performance variation across state-of-the-art model architectures such as
GPT-4 and LLaMA3. This work advances our understanding of the analogical
reasoning abilities of LLMs and their potential as models of human reasoning.

</details>


### [218] [Multi-Trigger Poisoning Amplifies Backdoor Vulnerabilities in LLMs](https://arxiv.org/abs/2507.11112)
*Sanhanat Sivapiromrat,Caiqi Zhang,Marco Basaldella,Nigel Collier*

Main category: cs.CL

TL;DR: 提出研究大语言模型中毒的框架，发现多触发器可共存，提出事后恢复方法防御多触发器中毒。


<details>
  <summary>Details</summary>
Motivation: 现有研究对大语言模型中毒攻击的触发机制和多触发器交互理解有限。

Method: 提出研究中毒的框架，使用高嵌入相似度多触发器测试，提出基于层权重差异分析的事后恢复方法。

Result: 发现多触发器可在模型中共存且不相互干扰，中毒触发器在特定情况下能实现鲁棒激活，暴露了大语言模型更广泛持久的漏洞。

Conclusion: 提出的事后恢复方法能有效去除触发行为，以最小参数更新防御多触发器中毒。

Abstract: Recent studies have shown that Large Language Models (LLMs) are vulnerable to
data poisoning attacks, where malicious training examples embed hidden
behaviours triggered by specific input patterns. However, most existing works
assume a phrase and focus on the attack's effectiveness, offering limited
understanding of trigger mechanisms and how multiple triggers interact within
the model. In this paper, we present a framework for studying poisoning in
LLMs. We show that multiple distinct backdoor triggers can coexist within a
single model without interfering with each other, enabling adversaries to embed
several triggers concurrently. Using multiple triggers with high embedding
similarity, we demonstrate that poisoned triggers can achieve robust activation
even when tokens are substituted or separated by long token spans. Our findings
expose a broader and more persistent vulnerability surface in LLMs. To mitigate
this threat, we propose a post hoc recovery method that selectively retrains
specific model components based on a layer-wise weight difference analysis. Our
method effectively removes the trigger behaviour with minimal parameter
updates, presenting a practical and efficient defence against multi-trigger
poisoning.

</details>


### [219] [What Should LLMs Forget? Quantifying Personal Data in LLMs for Right-to-Be-Forgotten Requests](https://arxiv.org/abs/2507.11128)
*Dimitri Staufer*

Main category: cs.CL

TL;DR: 文章引入WikiMem数据集和模型无关指标来量化大语言模型中的人-事实关联，评估多模型，为识别个人数据记忆提供基础。


<details>
  <summary>Details</summary>
Motivation: 大语言模型会记忆和泄露个人信息，现有方法无法识别模型中存储的个人-事实关联，隐私审计技术适用性有限，需解决合规问题。

Method: 引入包含超5000个自然语言金丝雀的WikiMem数据集和模型无关指标，用校准负对数似然对真值和反事实进行排序。

Result: 对15个大语言模型评估200个个体，发现记忆与主体网络存在和模型规模相关。

Conclusion: 为在个体层面识别大语言模型中记忆的个人数据提供基础，可用于构建遗忘集以实现机器遗忘和处理被遗忘权请求。

Abstract: Large Language Models (LLMs) can memorize and reveal personal information,
raising concerns regarding compliance with the EU's GDPR, particularly the
Right to Be Forgotten (RTBF). Existing machine unlearning methods assume the
data to forget is already known but do not address how to identify which
individual-fact associations are stored in the model. Privacy auditing
techniques typically operate at the population level or target a small set of
identifiers, limiting applicability to individual-level data inquiries. We
introduce WikiMem, a dataset of over 5,000 natural language canaries covering
243 human-related properties from Wikidata, and a model-agnostic metric to
quantify human-fact associations in LLMs. Our approach ranks ground-truth
values against counterfactuals using calibrated negative log-likelihood across
paraphrased prompts. We evaluate 200 individuals across 15 LLMs (410M-70B
parameters), showing that memorization correlates with subject web presence and
model scale. We provide a foundation for identifying memorized personal data in
LLMs at the individual level, enabling the dynamic construction of forget sets
for machine unlearning and RTBF requests.

</details>


### [220] [LLM-Augmented Symptom Analysis for Cardiovascular Disease Risk Prediction: A Clinical NLP](https://arxiv.org/abs/2507.11052)
*Haowei Yang,Ziyu Shen,Junli Shao,Luyao Men,Xinyue Han,Jing Dong*

Main category: cs.CL

TL;DR: 研究引入LLM增强的临床NLP管道用于心血管疾病风险预测，评估显示性能提升，解决部分挑战，凸显LLM在临床决策支持系统潜力。


<details>
  <summary>Details</summary>
Motivation: 现有心血管疾病预测模型主要利用结构化数据，而无结构临床笔记含宝贵早期指标，需更好的风险预测方法。

Method: 引入LLM增强的临床NLP管道，采用领域适配大语言模型进行症状提取等，集成心血管特定微调、基于提示的推理和实体感知推理。

Result: 在MIMIC - III和CARDIO - NLP数据集评估中，精度、召回率等性能提升，临床相关性高；用提示工程和混合规则验证解决上下文幻觉和时间模糊挑战。

Conclusion: LLM在临床决策支持系统有潜力，可推进早期预警系统，提升患者叙述转化为可行动风险评估的能力。

Abstract: Timely identification and accurate risk stratification of cardiovascular
disease (CVD) remain essential for reducing global mortality. While existing
prediction models primarily leverage structured data, unstructured clinical
notes contain valuable early indicators. This study introduces a novel
LLM-augmented clinical NLP pipeline that employs domain-adapted large language
models for symptom extraction, contextual reasoning, and correlation from
free-text reports. Our approach integrates cardiovascular-specific fine-tuning,
prompt-based inference, and entity-aware reasoning. Evaluations on MIMIC-III
and CARDIO-NLP datasets demonstrate improved performance in precision, recall,
F1-score, and AUROC, with high clinical relevance (kappa = 0.82) assessed by
cardiologists. Challenges such as contextual hallucination, which occurs when
plausible information contracts with provided source, and temporal ambiguity,
which is related with models struggling with chronological ordering of events
are addressed using prompt engineering and hybrid rule-based verification. This
work underscores the potential of LLMs in clinical decision support systems
(CDSS), advancing early warning systems and enhancing the translation of
patient narratives into actionable risk assessments.

</details>


### [221] [Internal Value Alignment in Large Language Models through Controlled Value Vector Activation](https://arxiv.org/abs/2507.11316)
*Haoran Jin,Meng Li,Xiting Wang,Zhihao Xu,Minlie Huang,Yantao Jia,Defu Lian*

Main category: cs.CL

TL;DR: 本文提出ConVA方法，直接对齐大语言模型内部价值观，实验显示该方法控制成功率高且不影响模型性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型与人类价值观对齐受关注，需有效方法实现内部价值观对齐。

Method: 提出Controlled Value Vector Activation (ConVA) 方法，包括上下文控制的价值向量识别方法和门控价值向量激活方法。

Result: 方法在10种基本价值观上实现最高控制成功率，不损害模型性能和流畅性，面对相反和潜在恶意输入提示也能确保目标价值观。

Conclusion: ConVA方法能有效实现大语言模型内部价值观对齐。

Abstract: Aligning Large Language Models (LLMs) with human values has attracted
increasing attention since it provides clarity, transparency, and the ability
to adapt to evolving scenarios. In this paper, we introduce a Controlled Value
Vector Activation (ConVA) method that directly aligns the internal values of
LLMs by interpreting how a value is encoded in their latent representations and
modifies relevant activations to ensure consistent values in LLMs. To ensure an
accurate and unbiased interpretation, we propose a context-controlled value
vector identification method. To consistently control values without
sacrificing model performance, we introduce a gated value vector activation
method for effective and minimum degree of value control. Experiments show that
our method achieves the highest control success rate across 10 basic values
without hurting LLM performance and fluency, and ensures target values even
with opposite and potentially malicious input prompts. Source code and data are
available at~ https://github.com/hr-jin/ConVA.

</details>


### [222] [Temperature and Persona Shape LLM Agent Consensus With Minimal Accuracy Gains in Qualitative Coding](https://arxiv.org/abs/2507.11198)
*Conrad Borchers,Bahar Shahrokhian,Francesco Balzan,Elham Tajik,Sreecharan Sankaranarayanan,Sebastian Simon*

Main category: cs.CL

TL;DR: 研究多智能体系统（MAS）在定性研究编码中的应用，发现温度、角色对达成共识有影响，但对编码准确性提升不显著，单智能体多数情况下表现更佳，开源相关代码。


<details>
  <summary>Details</summary>
Motivation: 探究多智能体系统在定性研究编码中相比单智能体的优势。

Method: 进行实验研究，使用六种开源大语言模型和18种实验配置，分析77000多个编码决策。

Result: 温度影响共识达成，多角色延迟部分模型共识，对编码准确性提升不明显，单智能体多数情况表现好，仅一个模型和代码类别在特定条件下MAS有增益。

Conclusion: 揭示基于大语言模型的定性方法的局限，质疑多样MAS角色能带来更好结果的观点。

Abstract: Large Language Models (LLMs) enable new possibilities for qualitative
research at scale, including coding and data annotation. While multi-agent
systems (MAS) can emulate human coding workflows, their benefits over
single-agent coding remain poorly understood. We conducted an experimental
study of how agent persona and temperature shape consensus-building and coding
accuracy of dialog segments based on a codebook with 8 codes. Our open-source
MAS mirrors deductive human coding through structured agent discussion and
consensus arbitration. Using six open-source LLMs (with 3 to 32 billion
parameters) and 18 experimental configurations, we analyze over 77,000 coding
decisions against a gold-standard dataset of human-annotated transcripts from
online math tutoring sessions. Temperature significantly impacted whether and
when consensus was reached across all six LLMs. MAS with multiple personas
(including neutral, assertive, or empathetic), significantly delayed consensus
in four out of six LLMs compared to uniform personas. In three of those LLMs,
higher temperatures significantly diminished the effects of multiple personas
on consensus. However, neither temperature nor persona pairing lead to robust
improvements in coding accuracy. Single agents matched or outperformed MAS
consensus in most conditions. Only one model (OpenHermesV2:7B) and code
category showed above-chance gains from MAS deliberation when temperature was
0.5 or lower and especially when the agents included at least one assertive
persona. Qualitative analysis of MAS collaboration for these configurations
suggests that MAS may nonetheless aid in narrowing ambiguous code applications
that could improve codebooks and human-AI coding. We contribute new insight
into the limits of LLM-based qualitative methods, challenging the notion that
diverse MAS personas lead to better outcomes. We open-source our MAS and
experimentation code.

</details>


### [223] [An Agentic Flow for Finite State Machine Extraction using Prompt Chaining](https://arxiv.org/abs/2507.11222)
*Fares Wael,Youssef Maklad,Ali Hamdi,Wael Elsersy*

Main category: cs.CL

TL;DR: 提出FlowFSM框架，结合大语言模型从RFC文档中提取有限状态机，实验效果好，显示基于代理的大语言模型系统在协议分析中有潜力。


<details>
  <summary>Details</summary>
Motivation: 现有有限状态机提取技术存在可扩展性、覆盖不完全和自然语言规范模糊等局限。

Method: 提出FlowFSM框架，利用大语言模型结合提示链和思维链推理，系统处理协议规范，识别状态转换并构建规则手册。

Result: 在FTP和RTSP协议实验中，FlowFSM提取精度高，减少了幻觉转换。

Conclusion: 基于代理的大语言模型系统在协议分析和有限状态机推理的网络安全和逆向工程应用中有发展潜力。

Abstract: Finite-State Machines (FSMs) are critical for modeling the operational logic
of network protocols, enabling verification, analysis, and vulnerability
discovery. However, existing FSM extraction techniques face limitations such as
scalability, incomplete coverage, and ambiguity in natural language
specifications. In this paper, we propose FlowFSM, a novel agentic framework
that leverages Large Language Models (LLMs) combined with prompt chaining and
chain-of-thought reasoning to extract accurate FSMs from raw RFC documents.
FlowFSM systematically processes protocol specifications, identifies state
transitions, and constructs structured rule-books by chaining agent outputs.
Experimental evaluation across FTP and RTSP protocols demonstrates that FlowFSM
achieves high extraction precision while minimizing hallucinated transitions,
showing promising results. Our findings highlight the potential of agent-based
LLM systems in the advancement of protocol analysis and FSM inference for
cybersecurity and reverse engineering applications.

</details>


### [224] [Automated Novelty Evaluation of Academic Paper: A Collaborative Approach Integrating Human and Large Language Model Knowledge](https://arxiv.org/abs/2507.11330)
*Wenqing Wu,Chengzhi Zhang,Yi Zhao*

Main category: cs.CL

TL;DR: 文章指出传统论文新颖性评估方法有局限，提出结合大语言模型和人类专家知识辅助预训练语言模型预测论文方法新颖性，经实验验证效果优越。


<details>
  <summary>Details</summary>
Motivation: 传统新颖性评估方法（专家判断和独特引用组合法）存在局限，结合大语言模型知识和人类专家判断能力来解决这些局限。

Method: 从同行评审报告中提取与论文新颖性相关句子，用大语言模型总结论文方法部分以微调预训练语言模型，设计带新颖稀疏注意力的文本引导融合模块。

Result: 所提方法与大量基线方法对比，经大量实验表明该方法性能优越。

Conclusion: 结合大语言模型和人类专家知识辅助预训练语言模型预测论文方法新颖性的方法有效可行。

Abstract: Novelty is a crucial criterion in the peer review process for evaluating
academic papers. Traditionally, it's judged by experts or measure by unique
reference combinations. Both methods have limitations: experts have limited
knowledge, and the effectiveness of the combination method is uncertain.
Moreover, it's unclear if unique citations truly measure novelty. The large
language model (LLM) possesses a wealth of knowledge, while human experts
possess judgment abilities that the LLM does not possess. Therefore, our
research integrates the knowledge and abilities of LLM and human experts to
address the limitations of novelty assessment. The most common novelty in
academic papers is the introduction of new methods. In this paper, we propose
leveraging human knowledge and LLM to assist pretrained language models (PLMs,
e.g. BERT etc.) in predicting the method novelty of papers. Specifically, we
extract sentences related to the novelty of the academic paper from peer review
reports and use LLM to summarize the methodology section of the academic paper,
which are then used to fine-tune PLMs. In addition, we have designed a
text-guided fusion module with novel Sparse-Attention to better integrate human
and LLM knowledge. We compared the method we proposed with a large number of
baselines. Extensive experiments demonstrate that our method achieves superior
performance.

</details>


### [225] [EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and Reasoning Modes](https://arxiv.org/abs/2507.11407)
*LG AI Research,:,Kyunghoon Bae,Eunbi Choi,Kibong Choi,Stanley Jungkyu Choi,Yemuk Choi,Kyubeen Han,Seokhee Hong,Junwon Hwang,Taewan Hwang,Joonwon Jang,Hyojin Jeon,Kijeong Jeon,Gerrard Jeongwon Jo,Hyunjik Jo,Jiyeon Jung,Euisoon Kim,Hyosang Kim,Jihoon Kim,Joonkee Kim,Seonghwan Kim,Soyeon Kim,Sunkyoung Kim,Yireun Kim,Yongil Kim,Youchul Kim,Edward Hwayoung Lee,Gwangho Lee,Haeju Lee,Honglak Lee,Jinsik Lee,Kyungmin Lee,Sangha Park,Young Min Paik,Yongmin Park,Youngyong Park,Sanghyun Seo,Sihoon Yang,Heuiyeen Yeen,Sihyuk Yi,Hyeongu Yun*

Main category: cs.CL

TL;DR: 介绍EXAONE 4.0，融合两种模式，有代理工具使用等特性，含两种大小模型，性能优且公开可用。


<details>
  <summary>Details</summary>
Motivation: 为代理AI时代做准备，结合EXAONE 3.5易用性和EXAONE Deep推理能力。

Method: 集成非推理模式和推理模式，融入代理工具使用等特性，推出不同大小模型。

Result: EXAONE 4.0比同类别开放权重模型性能优越，与前沿模型有竞争力。

Conclusion: EXAONE 4.0是有优势的模型，可通过指定链接公开下载用于研究。

Abstract: This technical report introduces EXAONE 4.0, which integrates a Non-reasoning
mode and a Reasoning mode to achieve both the excellent usability of EXAONE 3.5
and the advanced reasoning abilities of EXAONE Deep. To pave the way for the
agentic AI era, EXAONE 4.0 incorporates essential features such as agentic tool
use, and its multilingual capabilities are extended to support Spanish in
addition to English and Korean. The EXAONE 4.0 model series consists of two
sizes: a mid-size 32B model optimized for high performance, and a small-size
1.2B model designed for on-device applications. The EXAONE 4.0 demonstrates
superior performance compared to open-weight models in its class and remains
competitive even against frontier-class models. The models are publicly
available for research purposes and can be easily downloaded via
https://huggingface.co/LGAI-EXAONE.

</details>


### [226] [KisMATH: Do LLMs Have Knowledge of Implicit Structures in Mathematical Reasoning?](https://arxiv.org/abs/2507.11408)
*Soumadeep Saha,Akshay Chaturvedi,Saptarshi Saha,Utpal Garain,Nicholas Asher*

Main category: cs.CL

TL;DR: 本文引入因果思维链图CCGs和数据集KisMATH，分析表明CCG推理节点是必要条件，大模型会重视推理路径，为思维链研究提供新途径。


<details>
  <summary>Details</summary>
Motivation: 当前对于思维链提升大语言模型推理性能的机制尚无共识，需要进一步研究。

Method: 引入Causal CoT Graphs (CCGs)，从推理痕迹中自动提取有向无环图；构建包含1671个数学推理问题及对应CCGs的数据集KisMATH；对15个开源大语言模型进行实证分析。

Result: CCG中的推理节点是得出最终答案的中介，是推理的必要条件；大语言模型会重视CCG给出的推理路径。

Conclusion: KisMATH可进行与图对齐的可控干预，为研究思维链在大语言模型推理中的作用开辟了新途径。

Abstract: Chain-of-thought traces have been shown to improve performance of large
language models in a plethora of reasoning tasks, yet there is no consensus on
the mechanism through which this performance boost is achieved. To shed more
light on this, we introduce Causal CoT Graphs (CCGs), which are directed
acyclic graphs automatically extracted from reasoning traces that model
fine-grained causal dependencies in the language model output. A collection of
$1671$ mathematical reasoning problems from MATH500, GSM8K and AIME, and their
associated CCGs are compiled into our dataset -- \textbf{KisMATH}. Our detailed
empirical analysis with 15 open-weight LLMs shows that (i) reasoning nodes in
the CCG are mediators for the final answer, a condition necessary for
reasoning; and (ii) LLMs emphasise reasoning paths given by the CCG, indicating
that models internally realise structures akin to our graphs. KisMATH enables
controlled, graph-aligned interventions and opens up avenues for further
investigation into the role of chain-of-thought in LLM reasoning.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [227] [Urban delineation through the lens of commute networks: Leveraging graph embeddings to distinguish socioeconomic groups in cities](https://arxiv.org/abs/2507.11057)
*Devashish Khulbe,Stanislav Sobolevsky*

Main category: cs.SI

TL;DR: 研究用人口普查通勤网络结合图神经网络进行城市区域划分，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 城市区域划分对城市研究很重要，可应用于城市科学多方面，需新方法进行划分。

Method: 用人口普查的通勤网络，以图神经网络建模，获取低维表示并聚类。

Result: 实验表明网络嵌入能捕捉不同城市社区间显著的社会经济差异，如家庭收入中位数。

Conclusion: 证明人口普查移动性数据在区域划分中的作用，图神经网络可作为城市社区检测的有力替代方法。

Abstract: Delineating areas within metropolitan regions stands as an important focus
among urban researchers, shedding light on the urban perimeters shaped by
evolving population dynamics. Applications to urban science are numerous, from
facilitating comparisons between delineated districts and administrative
divisions to informing policymakers of the shifting economic and labor
landscapes. In this study, we propose using commute networks sourced from the
census for the purpose of urban delineation, by modeling them with a Graph
Neural Network (GNN) architecture. We derive low-dimensional representations of
granular urban areas (nodes) using GNNs. Subsequently, nodes' embeddings are
clustered to identify spatially cohesive communities in urban areas. Our
experiments across the U.S. demonstrate the effectiveness of network embeddings
in capturing significant socioeconomic disparities between communities in
various cities, particularly in factors such as median household income. The
role of census mobility data in regional delineation is also noted, and we
establish the utility of GNNs in urban community detection, as a powerful
alternative to existing methods in this domain. The results offer insights into
the wider effects of commute networks and their use in building meaningful
representations of urban regions.

</details>


### [228] [The Shape of Deceit: Behavioral Consistency and Fragility in Money Laundering Patterns](https://arxiv.org/abs/2507.10608)
*Danny Butvinik,Ofir Yakobi,Michal Einhorn Cohen,Elina Maliarsky*

Main category: cs.SI

TL;DR: 本文挑战传统以实体为中心的反洗钱方法，提出基于网络理论的视角，强调检测交易网络中的洗钱模式，探讨模式脆弱性和相似性概念。


<details>
  <summary>Details</summary>
Motivation: 传统反洗钱系统主要关注识别异常实体或交易，误解了洗钱的本质，本文旨在提出新的反洗钱视角。

Method: 提出网络理论视角，引入行为一致性概念，通过子图结构捕捉洗钱模式，探讨模式脆弱性和相似性。

Result: 提出新的反洗钱思路，强调不应依赖统计异常值，而应关注行为本质的保留。

Conclusion: 这种哲学和实践的转变对反洗钱系统在打击金融犯罪中的建模、扫描和解释网络有重要意义。

Abstract: Conventional anti-money laundering (AML) systems predominantly focus on
identifying anomalous entities or transactions, flagging them for manual
investigation based on statistical deviation or suspicious behavior. This
paradigm, however, misconstrues the true nature of money laundering, which is
rarely anomalous but often deliberate, repeated, and concealed within
consistent behavioral routines. In this paper, we challenge the entity-centric
approach and propose a network-theoretic perspective that emphasizes detecting
predefined laundering patterns across directed transaction networks. We
introduce the notion of behavioral consistency as the core trait of laundering
activity, and argue that such patterns are better captured through subgraph
structures expressing semantic and functional roles - not solely geometry.
Crucially, we explore the concept of pattern fragility: the sensitivity of
laundering patterns to small attribute changes and, conversely, their semantic
robustness even under drastic topological transformations. We claim that
laundering detection should not hinge on statistical outliers, but on
preservation of behavioral essence, and propose a reconceptualization of
pattern similarity grounded in this insight. This philosophical and practical
shift has implications for how AML systems model, scan, and interpret networks
in the fight against financial crime.

</details>


### [229] [Multilayer Artificial Benchmark for Community Detection (mABCD)](https://arxiv.org/abs/2507.10795)
*Łukasz Kraiński,Michał Czuba,Piotr Bródka,Paweł Prałat,Bogumił Kamiński,François Théberge*

Main category: cs.SI

TL;DR: 介绍了ABCD模型并提出其多层网络变体mABCD。


<details>
  <summary>Details</summary>
Motivation: 为多层网络找到合适的类似ABCD模型的工具。

Method: 基于ABCD模型的基本要素，引入多层网络变体mABCD。

Result: 提出了多层网络变体mABCD。

Conclusion: 可使用mABCD模型处理多层网络。

Abstract: The Artificial Benchmark for Community Detection (ABCD) model is a random
graph model with community structure and power-law distribution for both
degrees and community sizes. The model generates graphs similar to the
well-known LFR model but it is faster, more interpretable, and can be
investigated analytically. In this paper, we use the underlying ingredients of
the ABCD model and introduce its variant for multilayer networks, mABCD.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [230] [Compressed data structures for Heegaard splittings](https://arxiv.org/abs/2507.11406)
*Henrique Ennes,Clément Maria*

Main category: cs.CG

TL;DR: 提出一种数据结构来表示Heegaard图，建立多项式时间算法进行图的操作和计算拓扑不变量，且早期实现比标准软件表现更好。


<details>
  <summary>Details</summary>
Motivation: 有效表示Heegaard图并高效处理相关操作和计算拓扑不变量。

Method: 用关于曲面三角剖分的正常曲线表示Heegaard图，以二进制表达正常坐标向量所需空间衡量复杂度。

Result: 建立多项式时间算法进行图的比较、操作、稳定化检测等，早期实现比标准软件在平均情况有更好精度和速度，部分输入有指数级速度提升。

Conclusion: 所提出的数据结构和算法在表示Heegaard图及相关操作和计算方面有优势。

Abstract: Heegaard splittings provide a natural representation of closed 3-manifolds by
gluing handlebodies along a common surface. These splittings can be
equivalently given by two finite sets of meridians lying in the surface, which
define a Heegaard diagram. We present a data structure to effectively represent
Heegaard diagrams as normal curves with respect to triangulations of a surface
of complexity measured by the space required to express the normal coordinates'
vectors in binary. This structure can be significantly more compressed than
triangulations of 3-manifolds, given exponential gains for some families. Even
with this succinct definition of complexity, we establish polynomial time
algorithms for comparing and manipulating diagrams, performing stabilizations,
detecting trivial stabilizations and reductions, and computing topological
invariants of the underlying manifolds, such as their fundamental and first
homology groups. We also contrast early implementations of our techniques with
standard software programs for 3-manifolds, achieving better precision and
faster algorithms for the average cases and exponential gains in speed for some
particular presentations of the inputs.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [231] [React to This (RTT): A Nonverbal Turing Test for Embodied AI](https://arxiv.org/abs/2507.10812)
*Chuxuan Zhang,Yasaman Etesam,Angelica Lim*

Main category: cs.HC

TL;DR: 提出测试具身AI代理交互意识和可信度的方法，引入RTT测试并给出初始实验结果。


<details>
  <summary>Details</summary>
Motivation: 现有图灵测试有局限，需新方法测试具身AI代理在人类极限挑战场景下的交互意识和可信度。

Method: 基于图灵测试概念，提出‘Can machines react?’问题，引入RTT测试非语言行为。

Result: 完成初始实验并呈现了结果。

Conclusion: 未明确提及，但暗示新测试方法有潜力用于评估具身AI。

Abstract: We propose an approach to test embodied AI agents for interaction awareness
and believability, particularly in scenarios where humans push them to their
limits. Turing introduced the Imitation Game as a way to explore the question:
"Can machines think?" The Total Turing Test later expanded this concept beyond
purely verbal communication, incorporating perceptual and physical interaction.
Building on this, we propose a new guiding question: "Can machines react?" and
introduce the React to This (RTT) test for nonverbal behaviors, presenting
results from an initial experiment.

</details>


### [232] [Role-Playing LLM-Based Multi-Agent Support Framework for Detecting and Addressing Family Communication Bias](https://arxiv.org/abs/2507.11210)
*Rushia Harada,Yuken Kimura,Keito Inoshita*

Main category: cs.HC

TL;DR: 本文聚焦家庭幸福中易被忽视的心理动态，开发基于大语言模型的家庭沟通支持框架，实验显示该框架有一定检测能力且反馈质量高，能促进家庭互动积极转变。


<details>
  <summary>Details</summary>
Motivation: 传统指标常忽略家庭环境中微妙的心理动态，如理想父母偏差导致孩子情感表达和自主性受抑制，难以从外部察觉和解决，因此探索基于大语言模型的家庭沟通支持。

Method: 构建含30个场景的日版亲子对话语料库，开发基于角色扮演大语言模型的多智能体对话支持框架，由专业智能体检测和分析，元智能体汇总报告，专家智能体协作生成反馈。

Result: 系统能以中等准确率检测被抑制情感类别，反馈的同理心和实用性评分高，模拟后续对话显示情感表达和相互理解有所改善。

Conclusion: 该框架有潜力支持家庭互动的积极转变。

Abstract: Well-being in family settings involves subtle psychological dynamics that
conventional metrics often overlook. In particular, unconscious parental
expectations, termed ideal parent bias, can suppress children's emotional
expression and autonomy. This suppression, referred to as suppressed emotion,
often stems from well-meaning but value-driven communication, which is
difficult to detect or address from outside the family. Focusing on these
latent dynamics, this study explores Large Language Model (LLM)-based support
for psychologically safe family communication. We constructed a Japanese
parent-child dialogue corpus of 30 scenarios, each annotated with metadata on
ideal parent bias and suppressed emotion. Based on this corpus, we developed a
Role-Playing LLM-based multi-agent dialogue support framework that analyzes
dialogue and generates feedback. Specialized agents detect suppressed emotion,
describe implicit ideal parent bias in parental speech, and infer contextual
attributes such as the child's age and background. A meta-agent compiles these
outputs into a structured report, which is then passed to five selected expert
agents. These agents collaboratively generate empathetic and actionable
feedback through a structured four-step discussion process. Experiments show
that the system can detect categories of suppressed emotion with moderate
accuracy and produce feedback rated highly in empathy and practicality.
Moreover, simulated follow-up dialogues incorporating this feedback exhibited
signs of improved emotional expression and mutual understanding, suggesting the
framework's potential in supporting positive transformation in family
interactions.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [233] [Quantitative multi-metabolite imaging of Parkinson's disease using AI boosted molecular MRI](https://arxiv.org/abs/2507.11329)
*Hagar Shmuely,Michal Rivlin,Or Perlman*

Main category: physics.med-ph

TL;DR: 结合快速分子MRI采集范式与深度学习重建对急性MPTP小鼠模型多代谢物定量，定量参数图与组织学和MR光谱一致，部分质子体积分数可作PD生物标志物。


<details>
  <summary>Details</summary>
Motivation: 传统帕金森病体内分子成像方法有需放射性同位素、扫描时间长、空间分辨率低等问题，基于饱和转移的PD磁共振成像对比度半定量且非特异性。

Method: 在急性MPTP小鼠模型中，将快速分子MRI采集范式与基于深度学习的重建相结合，对谷氨酸、移动蛋白、半固体和移动大分子等多代谢物进行定量。

Result: 定量参数图与组织学和MR光谱大体一致。

Conclusion: 半固体磁化转移（MT）、酰胺和脂肪族中继核Overhauser效应（rNOE）质子体积分数可作为PD生物标志物。

Abstract: Traditional approaches for molecular imaging of Parkinson's disease (PD) in
vivo require radioactive isotopes, lengthy scan times, or deliver only low
spatial resolution. Recent advances in saturation transfer-based PD magnetic
resonance imaging (MRI) have provided biochemical insights, although the image
contrast is semi-quantitative and nonspecific. Here, we combined a rapid
molecular MRI acquisition paradigm with deep learning based reconstruction for
multi-metabolite quantification of glutamate, mobile proteins, semisolid, and
mobile macromolecules in an acute MPTP
(1-methyl-4-phenyl-1,2,3,6-tetrahydropyridine) mouse model. The quantitative
parameter maps are in general agreement with the histology and MR spectroscopy,
and demonstrate that semisolid magnetization transfer (MT), amide, and
aliphatic relayed nuclear Overhauser effect (rNOE) proton volume fractions may
serve as PD biomarkers.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [234] [Forecasting NYC Yellow Taxi Ridership Decline: A Time Series Analysis of Daily Passenger Counts (2017-2019)](https://arxiv.org/abs/2507.10588)
*Gaurav Singh*

Main category: econ.GN

TL;DR: 分析并预测2017 - 2019年纽约市黄色出租车日乘客量，用多种时间序列模型，发现首阶自回归模型预测最准，为政策制定者提供见解。


<details>
  <summary>Details</summary>
Motivation: 分析并预测纽约市黄色出租车在2017 - 2019年乘客量显著下降期间的日乘客数。

Method: 使用纽约市出租车和豪华轿车委员会的综合数据集，采用包括ARIMA模型在内的多种时间序列建模方法。

Result: 发现强季节性模式，日乘客量日均约下降200人；首阶自回归模型结合去趋势和去周期处理预测最准，测试RMSE为34,880，日均乘客量438,000人。

Conclusion: 研究为政策制定者和利益相关者理解和应对纽约市黄色出租车服务乘客量下降趋势提供有价值的见解。

Abstract: This study analyzes and forecasts daily passenger counts for New York City's
iconic yellow taxis during 2017-2019, a period of significant decline in
ridership. Using a comprehensive dataset from the NYC Taxi and Limousine
Commission, we employ various time series modeling approaches, including ARIMA
models, to predict daily passenger volumes. Our analysis reveals strong
seasonal patterns, with a consistent linear decline of approximately 200
passengers per day throughout the study period. After comparing multiple
modeling approaches, we find that a first-order autoregressive model, combined
with careful detrending and cycle removal, provides the most accurate
predictions, achieving a test RMSE of 34,880 passengers on a mean ridership of
438,000 daily passengers. The research provides valuable insights for
policymakers and stakeholders in understanding and potentially addressing the
declining trajectory of NYC's yellow taxi service.

</details>


### [235] [Artificial Finance: How AI Thinks About Money](https://arxiv.org/abs/2507.10933)
*Orhan Erdem,Ragavi Pobbathi Ashok*

Main category: econ.GN

TL;DR: 本文通过对比大语言模型（LLMs）与全球人类参与者对金融决策问题的回答，发现LLMs呈现风险中性决策模式，在跨期权衡时有异常，整体回答最接近坦桑尼亚参与者，有助于理解其模拟人类决策行为及潜在影响。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在金融决策方面如何模拟人类行为。

Method: 向七个领先的大语言模型提出常用金融决策问题，并将其输出与涵盖53个国家的人类回答数据集进行比较。

Result: 1. LLMs通常呈现风险中性决策模式；2. 在跨期权衡时，LLMs偶尔会给出与规范推理不一致的回答；3. LLMs的总体回答最接近坦桑尼亚参与者。

Conclusion: 研究结果有助于理解LLMs模拟人类决策行为，以及其输出中潜在的文化和训练影响。

Abstract: In this paper, we explore how large language models (LLMs) approach financial
decision-making by systematically comparing their responses to those of human
participants across the globe. We posed a set of commonly used financial
decision-making questions to seven leading LLMs, including five models from the
GPT series(GPT-4o, GPT-4.5, o1, o3-mini), Gemini 2.0 Flash, and DeepSeek R1. We
then compared their outputs to human responses drawn from a dataset covering 53
nations. Our analysis reveals three main results. First, LLMs generally exhibit
a risk-neutral decision-making pattern, favoring choices aligned with expected
value calculations when faced with lottery-type questions. Second, when
evaluating trade-offs between present and future, LLMs occasionally produce
responses that appear inconsistent with normative reasoning. Third, when we
examine cross-national similarities, we find that the LLMs' aggregate responses
most closely resemble those of participants from Tanzania. These findings
contribute to the understanding of how LLMs emulate human-like decision
behaviors and highlight potential cultural and training influences embedded
within their outputs.

</details>


### [236] [Propagation of carbon price shocks through the value chain: the mean-field game of defaults](https://arxiv.org/abs/2507.11353)
*Zorana Grbac,Simone Pavarana,Thorsten Schmidt,Peter Tankov*

Main category: econ.GN

TL;DR: 本文引入新的平均场博弈框架分析多部门经济中碳定价影响，建模并证明均衡存在与价格系统唯一性，通过数值实验揭示碳价冲击溢出效应。


<details>
  <summary>Details</summary>
Motivation: 分析多部门经济中碳定价对可违约企业的影响，研究部门间相互依存关系在脱碳路径中的作用。

Method: 将经济建模为各部门内的最优停止平均场博弈，采用线性规划方法刻画纳什均衡。

Result: 证明线性规划纳什均衡存在，价格系统唯一；数值实验表明碳价冲击在单部门经济中促使排放与劳动替代，在三部门经济中有显著溢出效应。

Conclusion: 部门间相互依存关系对形成有效的脱碳路径至关重要，碳价冲击会产生显著的价值链溢出效应。

Abstract: We introduce a new mean-field game framework to analyze the impact of carbon
pricing in a multi-sector economy with defaultable firms. Each sector produces
a homogeneous good, with its price endogenously determined through market
clearing. Firms act as price takers and maximize profits by choosing an optimal
allocation of inputs-including labor, emissions, and intermediate goods from
other sectors-while interacting through the endogenous sectoral price. Firms
also choose their default timing to maximize shareholder value.
  Formally, we model the economy as an optimal stopping mean-field game within
each sector. The resulting system of coupled mean-field games admits a linear
programming formulation that characterizes Nash equilibria in terms of
population measure flows. We prove the existence of a linear programming Nash
equilibrium and establish uniqueness of the associated price system.
  Numerical illustrations are presented for firms with constant elasticity of
substitution (CES) production functions. In a stylized single-sector economy,
carbon price shocks induce substitution between emissions and labor. In a
three-sector economy, the manufacturing sector faces consumer demand and
requires inputs from a brown sector, which can be increasingly replaced by
green-sector goods as carbon prices rise. These experiments reveal that carbon
price shocks can generate substantial spillover effects along the value chain,
underscoring the importance of sectoral interdependencies in shaping effective
decarbonization pathways.

</details>


### [237] [Adaptive Robust Optimization for European Electricity System Planning Considering Regional Dunkelflaute Events](https://arxiv.org/abs/2507.11361)
*Maximilian Bernecker,Smaranda Sgarciu,Xiaoming Kan,Mehrnaz Anvari,Iegor Riepin,Felix Müsgens*

Main category: econ.GN

TL;DR: 本文用ARO框架为欧洲全脱碳电力系统开发容量扩展模型，研究Dunkelflaute事件对系统成本和技术组合的影响，强调需欧洲协调政策策略。


<details>
  <summary>Details</summary>
Motivation: 研究全脱碳欧洲电力系统在Dunkelflaute事件下的容量扩展及应对策略。

Method: 使用自适应鲁棒优化（ARO）框架开发容量扩展模型，内识别最糟区域Dunkelflaute事件，在单次优化中纳入多极端天气情况。

Result: 系统成本随事件地理范围非线性上升；最优技术组合随天气压力变化；中欧是系统瓶颈，周边地区承担过建成本。

Conclusion: 需要超越国家规划的欧洲协调政策策略，支持跨境基建投资、扩大灵活技术规模、促进可再生能源地理均衡部署。

Abstract: This study develops a capacity expansion model for a fully decarbonized
European electricity system using an Adaptive Robust Optimization (ARO)
framework. The model endogenously identifies the worst regional Dunkelflaute
events, prolonged periods of low wind and solar availability, and incorporates
multiple extreme weather realizations within a single optimization run. Results
show that system costs rise nonlinearly with the geographic extent of these
events: a single worst-case regional disruption increases costs by 9%, but
broader disruptions across multiple regions lead to much sharper increases, up
to 51%. As Dunkelflaute conditions extend across most of Europe, additional
cost impacts level off, with a maximum increase of 71%. The optimal technology
mix evolves with the severity of weather stress: while renewables, batteries,
and interregional transmission are sufficient to manage localized events,
large-scale disruptions require long-term hydrogen storage and load shedding to
maintain system resilience. Central European regions, especially Germany and
France, emerge as systemic bottlenecks, while peripheral regions bear the cost
of compensatory overbuilding. These findings underscore the need for a
coordinated European policy strategy that goes beyond national planning to
support cross-border infrastructure investment, scale up flexible technologies
such as long-duration storage, and promote a geographically balanced deployment
of renewables to mitigate systemic risks associated with Dunkelflaute events.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [238] [NLP Meets the World: Toward Improving Conversations With the Public About Natural Language Processing Research](https://arxiv.org/abs/2507.10559)
*Shomir Wilson*

Main category: cs.CY

TL;DR: 本文针对与公众交流大语言模型能力和局限提出建议，涵盖三个主题并举例说明，以促进公众对NLP的理解和支持。


<details>
  <summary>Details</summary>
Motivation: 鉴于大语言模型发展引发公众对NLP的兴趣，为研究领域和研究者把握当下机会，与公众有效沟通。

Method: 引用已发表的NLP研究和热门新闻报道，以举例说明三个主题。

Result: 提出涵盖三个主题（模糊术语、不合理期望、伦理失败）的与公众沟通大语言模型的建议。

Conclusion: 这些建议能促进与公众就NLP进行有效、透明的沟通，加强公众理解并鼓励对研究的支持。

Abstract: Recent developments in large language models (LLMs) have been accompanied by
rapidly growing public interest in natural language processing (NLP). This
attention is reflected by major news venues, which sometimes invite NLP
researchers to share their knowledge and views with a wide audience.
Recognizing the opportunities of the present, for both the research field and
for individual researchers, this paper shares recommendations for communicating
with a general audience about LLMs' capabilities and limitations. These
recommendations cover three themes: vague terminology as an obstacle to public
understanding, unreasonable expectations as obstacles to sustainable growth,
and ethical failures as obstacles to continued support. Published NLP research
and popular news coverage are cited to illustrate these themes with examples.
The recommendations promote effective, transparent communication with the
general public about NLP, in order to strengthen public understanding and
encourage support for research.

</details>


### [239] [Can Large Language Models Understand As Well As Apply Patent Regulations to Pass a Hands-On Patent Attorney Test?](https://arxiv.org/abs/2507.10576)
*Bhakti Khera,Rezvan Alamian,Pascal A. Scherz,Stephan M. Goetz*

Main category: cs.CY

TL;DR: 本文评估多种大语言模型在欧洲专利律师资格考试部分内容上的表现，发现模型未达专业标准，公众可能高估其性能，开发虚拟专利律师任重道远。


<details>
  <summary>Details</summary>
Motivation: 法律领域虽已应用大语言模型，但对其量化表现及原因探索不足，本文旨在评估模型在欧洲专利律师资格考试中的表现。

Method: 对GPT系列、Anthropic、Deepseek和Llama - 3等多种开源和专有大语言模型，在欧洲专利律师资格考试部分内容上进行评估，结合人类专利专家对文本理由的评价。

Result: OpenAI o1表现最佳，AWS Llama 3.1 8B等表现较差，无模型能完全通过考试，模型输出受温度和提示措辞影响，自动指标与专家判断存在偏差。

Conclusion: 公众可能高估大模型性能，开发虚拟专利律师需解决逻辑一致性、鲁棒多模态和自适应提示等问题。

Abstract: The legal field already uses various large language models (LLMs) in actual
applications, but their quantitative performance and reasons for it are
underexplored. We evaluated several open-source and proprietary LLMs --
including GPT-series, Anthropic, Deepseek and Llama-3, variants -- on parts of
the European Qualifying Examination (EQE) for future European Patent Attorneys.
OpenAI o1 led with 0.82 accuracy and 0.81 F1 score, whereas (Amazon Web
Services) AWS Llama 3.1 8B lagged at 0.50 accuracy, and a Python-deployed Llama
3.1 8B scored 0.55. The latter two are within the range of mere guessing for
the two-answer forced-choice design. None of the evaluated models could have
passed the examination fully, as accuracy never exceeded the average threshold
of 0.90 required for professional-level standards -- also not models that are
regularly promoted for their assumed beyond-PhD- and bar-admitted-lawyer-level
performance. GPT-4o excelled at integrating text and graphics, while Claude 3
Opus often lost formatting coherence. Human patent experts evaluated the
textual justifications and uncovered various critical shortcomings of each
model. They valued clarity and legal rationale over the raw correctness of the
answers, which revealed misalignment between automatic metrics and expert
judgment. Model outputs were sensitive to modest temperature changes and prompt
wording, which underscores the remaining necessity of expert oversight. Future
work should target logical consistency, robust multimodality, and adaptive
prompting to approach human-level patent proficiency. In summary, despite the
outstanding performance of recent large models, the general public might
overestimate their performance. The field has a long way to go to develop a
virtual patent attorney. This paper wants to point out several specific
limitations that need solutions.

</details>


### [240] [Findings of the BEA 2025 Shared Task on Pedagogical Ability Assessment of AI-powered Tutors](https://arxiv.org/abs/2507.10579)
*Ekaterina Kochmar,Kaushal Kumar Maurya,Kseniia Petukhova,KV Aditya Srivatsa,Anaïs Tack,Justin Vasselli*

Main category: cs.CY

TL;DR: 本文介绍评估大语言模型驱动的AI导师教学能力的共享任务，展示评估结果并分析表现，资源公开。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型驱动的AI导师在教育对话中纠正学生错误的教学能力。

Method: 设置五个轨道，依据学习科学原则自动评估AI导师在错误识别、定位、提供指导和反馈可行性等方面的表现，将提交模型与人工标注的黄金标准对比。

Result: 各轨道有一定成果，但教学能力评估轨道仍有提升空间，四个教学能力评估轨道最佳宏F1分数在58.34 - 71.81之间，导师身份识别轨道最佳F1分数达96.98。

Conclusion: 概述共享任务主要发现，分析团队表现，公开资源支持未来研究。

Abstract: This shared task has aimed to assess pedagogical abilities of AI tutors
powered by large language models (LLMs), focusing on evaluating the quality of
tutor responses aimed at student's mistake remediation within educational
dialogues. The task consisted of five tracks designed to automatically evaluate
the AI tutor's performance across key dimensions of mistake identification,
precise location of the mistake, providing guidance, and feedback
actionability, grounded in learning science principles that define good and
effective tutor responses, as well as the track focusing on detection of the
tutor identity. The task attracted over 50 international teams across all
tracks. The submitted models were evaluated against gold-standard human
annotations, and the results, while promising, show that there is still
significant room for improvement in this domain: the best results for the four
pedagogical ability assessment tracks range between macro F1 scores of 58.34
(for providing guidance) and 71.81 (for mistake identification) on three-class
problems, with the best F1 score in the tutor identification track reaching
96.98 on a 9-class task. In this paper, we overview the main findings of the
shared task, discuss the approaches taken by the teams, and analyze their
performance. All resources associated with this task are made publicly
available to support future research in this critical domain.

</details>


### [241] [Exploring User Security and Privacy Attitudes and Concerns Toward the Use of General-Purpose LLM Chatbots for Mental Health](https://arxiv.org/abs/2507.10695)
*Jabari Kwesi,Jiaxun Cao,Riya Manchanda,Pardis Emami-Naeini*

Main category: cs.CY

TL;DR: 研究美国用户使用通用大语言模型聊天机器人进行心理健康管理时的隐私安全问题，发现误解与风险意识缺乏，提出防护建议。


<details>
  <summary>Details</summary>
Motivation: 现有研究多针对规则型心理健康聊天机器人，缺乏通用大语言模型聊天机器人相关实证研究，需了解用户隐私安全担忧、态度和期望。

Method: 对美国参与者进行21次半结构化访谈。

Result: 发现用户存在关键误解和缺乏风险意识，如将大语言模型的类人共情与类人问责混淆，误信聊天受HIPAA等法规保护，还提出“无形脆弱性”概念。

Conclusion: 提出更有效保护用户心理健康披露信息的建议。

Abstract: Individuals are increasingly relying on large language model (LLM)-enabled
conversational agents for emotional support. While prior research has examined
privacy and security issues in chatbots specifically designed for mental health
purposes, these chatbots are overwhelmingly "rule-based" offerings that do not
leverage generative AI. Little empirical research currently measures users'
privacy and security concerns, attitudes, and expectations when using
general-purpose LLM-enabled chatbots to manage and improve mental health.
Through 21 semi-structured interviews with U.S. participants, we identified
critical misconceptions and a general lack of risk awareness. Participants
conflated the human-like empathy exhibited by LLMs with human-like
accountability and mistakenly believed that their interactions with these
chatbots were safeguarded by the same regulations (e.g., HIPAA) as disclosures
with a licensed therapist. We introduce the concept of "intangible
vulnerability," where emotional or psychological disclosures are undervalued
compared to more tangible forms of information (e.g., financial or
location-based data). To address this, we propose recommendations to safeguard
user mental health disclosures with general-purpose LLM-enabled chatbots more
effectively.

</details>


### [242] ["Is it always watching? Is it always listening?" Exploring Contextual Privacy and Security Concerns Toward Domestic Social Robots](https://arxiv.org/abs/2507.10786)
*Henry Bell,Jabari Kwesi,Hiba Laabadli,Pardis Emami-Naeini*

Main category: cs.CY

TL;DR: 研究美国社交机器人的安全与隐私问题，通过访谈确定用户担忧并提出设计需求。


<details>
  <summary>Details</summary>
Motivation: 社交机器人在美受关注但有安全和隐私威胁，需了解用户需求来指导设计。

Method: 进行19次半结构化访谈。

Result: 确定了安全和隐私方面的重要担忧，不同应用场景有不同担忧，用户期望有具体隐私控制等。

Conclusion: 社交机器人设计需注重透明度、可用性和强大的隐私控制以促进应用。

Abstract: Equipped with artificial intelligence (AI) and advanced sensing capabilities,
social robots are gaining interest among consumers in the United States. These
robots seem like a natural evolution of traditional smart home devices.
However, their extensive data collection capabilities, anthropomorphic
features, and capacity to interact with their environment make social robots a
more significant security and privacy threat. Increased risks include data
linkage, unauthorized data sharing, and the physical safety of users and their
homes. It is critical to investigate U.S. users' security and privacy needs and
concerns to guide the design of social robots while these devices are still in
the early stages of commercialization in the U.S. market. Through 19
semi-structured interviews, we identified significant security and privacy
concerns, highlighting the need for transparency, usability, and robust privacy
controls to support adoption. For educational applications, participants
worried most about misinformation, and in medical use cases, they worried about
the reliability of these devices. Participants were also concerned with the
data inference that social robots could enable. We found that participants
expect tangible privacy controls, indicators of data collection, and
context-appropriate functionality.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [243] [A Learning Framework For Cooperative Collision Avoidance of UAV Swarms Leveraging Domain Knowledge](https://arxiv.org/abs/2507.10913)
*Shuangyao Huang,Haibo Zhang,Zhiyi Huang*

Main category: cs.MA

TL;DR: 提出利用领域知识驱动奖励的多智能体强化学习框架用于无人机群避撞，实验评估其性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有多智能体强化学习方法在无人机群避撞中的复杂信用分配和观测共享问题，使无人机适应复杂环境。

Method: 基于图像处理领域知识设计奖励函数，将障碍物建模为二维场的最大值，通过轮廓避免碰撞。

Result: 框架可训练大集群规模，减少智能体交互，消除复杂机制需求，且无人机经训练能适应复杂环境。

Conclusion: 通过实验评估框架性能，优于现有多智能体强化学习算法。

Abstract: This paper presents a multi-agent reinforcement learning (MARL) framework for
cooperative collision avoidance of UAV swarms leveraging domain
knowledge-driven reward. The reward is derived from knowledge in the domain of
image processing, approximating contours on a two-dimensional field. By
modeling obstacles as maxima on the field, collisions are inherently avoided as
contours never go through peaks or intersect. Additionally, counters are smooth
and energy-efficient. Our framework enables training with large swarm sizes as
the agent interaction is minimized and the need for complex credit assignment
schemes or observation sharing mechanisms in state-of-the-art MARL approaches
are eliminated. Moreover, UAVs obtain the ability to adapt to complex
environments where contours may be non-viable or non-existent through intensive
training. Extensive experiments are conducted to evaluate the performances of
our framework against state-of-the-art MARL algorithms.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [244] [From Kinetic Theory to AI: a Rediscovery of High-Dimensional Divergences and Their Properties](https://arxiv.org/abs/2507.11387)
*Gennaro Auricchio,Giovanni Brigati,Paolo Giudici,Giuseppe Toscani*

Main category: math-ph

TL;DR: 本文对源于动力学理论的散度度量进行比较综述，探讨其在机器学习和人工智能中的应用。


<details>
  <summary>Details</summary>
Motivation: 选择合适的散度度量对机器学习至关重要，而动力学理论中也有量化概率分布接近程度的需求，因此进行相关研究。

Method: 对源于动力学理论的散度度量进行比较综述。

Result: 文中未提及具体结果。

Conclusion: 文中未提及具体结论。

Abstract: Selecting an appropriate divergence measure is a critical aspect of machine
learning, as it directly impacts model performance. Among the most widely used,
we find the Kullback-Leibler (KL) divergence, originally introduced in kinetic
theory as a measure of relative entropy between probability distributions. Just
as in machine learning, the ability to quantify the proximity of probability
distributions plays a central role in kinetic theory. In this paper, we present
a comparative review of divergence measures rooted in kinetic theory,
highlighting their theoretical foundations and exploring their potential
applications in machine learning and artificial intelligence.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [245] [Causal Discovery for Linear Non-Gaussian Models with Disjoint Cycles](https://arxiv.org/abs/2507.10767)
*Mathias Drton,Marina Garrote-López,Niko Nikov,Elina Robeva,Y. Samuel Wang*

Main category: math.ST

TL;DR: 本文针对线性非高斯模型中学习循环因果结构的问题展开研究，给出了图确定相同模型的条件，提出定位源循环和推断循环拓扑顺序的方法，得到学习不相交循环因果结构的算法。


<details>
  <summary>Details</summary>
Motivation: 线性结构方程建模虽可纳入因果反馈循环，但循环存在导致模型难以用条件独立关系表征，学习循环因果结构仍是难题。

Method: 先明确两个有向图确定相同线性非高斯模型的条件，针对不相交循环图，利用非高斯分布低阶矩的二次和三次多项式关系定位源循环，结合去相关循环策略和多元回归推断有向循环的块拓扑顺序。

Result: 得到学习不相交循环因果结构的一致且计算高效的算法。

Conclusion: 所提出的方法能有效解决线性非高斯模型中不相交循环因果结构的学习问题。

Abstract: The paradigm of linear structural equation modeling readily allows one to
incorporate causal feedback loops in the model specification. These appear as
directed cycles in the common graphical representation of the models. However,
the presence of cycles entails difficulties such as the fact that models need
no longer be characterized by conditional independence relations. As a result,
learning cyclic causal structures remains a challenging problem. In this paper,
we offer new insights on this problem in the context of linear non-Gaussian
models. First, we precisely characterize when two directed graphs determine the
same linear non-Gaussian model. Next, we take up a setting of cycle-disjoint
graphs, for which we are able to show that simple quadratic and cubic
polynomial relations among low-order moments of a non-Gaussian distribution
allow one to locate source cycles. Complementing this with a strategy of
decorrelating cycles and multivariate regression allows one to infer a
block-topological order among the directed cycles, which leads to a {consistent
and computationally efficient algorithm} for learning causal structures with
disjoint cycles.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [246] [Stream programs are monoid homomorphisms with state](https://arxiv.org/abs/2507.10799)
*Tyler Hou,Michael Arntzenius,Max Willsey*

Main category: cs.PL

TL;DR: 定义一类确定性流函数，可用同态实现，展示其应用于多种场景。


<details>
  <summary>Details</summary>
Motivation: 简化流程序优化语义框架条件，同时支持丰富等式推理。

Method: 定义确定性流函数并将其实现为到“状态”幺半群的同态。

Result: 同态定律比先前语义框架条件简单，且能支持对表达性数据流程序的等式推理。

Conclusion: 通过分区数据库连接、分层否定和简化TCP模型等示例验证了方法的有效性。

Abstract: We define a broad class of deterministic stream functions and show they can
be implemented as homomorphisms into a "state" monoid. The homomorphism laws
are simpler than the conditions of previous semantic frameworks for stream
program optimization, yet retain support for rich equational reasoning over
expressive dataflow programs, including sequential composition, parallel
composition, and feedback. We demonstrate this using examples of partitioned
database joins, stratified negation, and a simplified model of TCP.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [247] [Real-time, Adaptive Radiological Anomaly Detection and Isotope Identification Using Non-negative Matrix Factorization](https://arxiv.org/abs/2507.10715)
*Chandler Jones,Mark Bandstra,Stefan Faaland,Yue Shi Lai,Nico Abgrall,Scott Suchyta,Reynold Cooper*

Main category: physics.app-ph

TL;DR: 本文提出一种基于非负矩阵分解（NMF）的自适应算法，可更新背景模型以适应环境变化，泛化性和检测性能良好。


<details>
  <summary>Details</summary>
Motivation: 移动探测器系统中，现有光谱异常检测和同位素识别算法因背景变化大，易超虚警率或牺牲检测灵敏度，传统NMF算法无法实时更新背景模型。

Method: 开发基于NMF的自适应算法，定期更新背景模型以适应环境变化。

Result: 自适应NMF算法假设更少，泛化性比现有基于NMF的方法更强，在模拟和真实数据集上检测性能相当或更优。

Conclusion: 自适应NMF算法能有效应对环境变化对背景光谱特征的影响，在核不扩散应用中具有优势。

Abstract: Spectroscopic anomaly detection and isotope identification algorithms are
integral components in nuclear nonproliferation applications such as search
operations. The task is especially challenging in the case of mobile detector
systems due to the fact that the observed gamma-ray background changes more
than for a static detector system, and a pretrained background model can easily
find itself out of domain. The result is that algorithms may exceed their
intended false alarm rate, or sacrifice detection sensitivity in order to
maintain the desired false alarm rate. Non-negative matrix factorization (NMF)
has been shown to be a powerful tool for spectral anomaly detection and
identification, but, like many similar algorithms that rely on data-driven
background models, in its conventional implementation it is unable to update in
real time to account for environmental changes that affect the background
spectroscopic signature. We have developed a novel NMF-based algorithm that
periodically updates its background model to accommodate changing environmental
conditions. The Adaptive NMF algorithm involves fewer assumptions about its
environment, making it more generalizable than existing NMF-based methods while
maintaining or exceeding detection performance on simulated and real-world
datasets.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [248] [Recent Advances in Simulation-based Inference for Gravitational Wave Data Analysis](https://arxiv.org/abs/2507.11192)
*Bo Liang,He Wang*

Main category: gr-qc

TL;DR: 引力波探测推动新需求，传统贝叶斯推理方法有计算挑战，本文综述基于模拟的推理方法，介绍理论基础和应用，指出其虽有速度优势但存在依赖模型和需验证准确性等问题。


<details>
  <summary>Details</summary>
Motivation: 引力波探测进入新时代，传统贝叶斯推理方法在处理高维参数空间和复杂噪声特征的引力波数据时有计算挑战，需要新方法。

Method: 综述新兴的基于模拟的推理方法，包括神经网络后验估计、神经比率估计等，介绍其理论基础并探讨在不同引力波数据处理场景的应用。

Result: 这些技术在控制研究中比传统方法有速度提升，准确性与传统方法相近。

Conclusion: 这些方法因依赖模型和对先验假设敏感，阻碍了广泛应用，其准确性需在更广泛参数空间和噪声条件下进一步验证。

Abstract: The detection of gravitational waves by the LIGO-Virgo-KAGRA collaboration
has ushered in a new era of observational astronomy, emphasizing the need for
rapid and detailed parameter estimation and population-level analyses.
Traditional Bayesian inference methods, particularly Markov chain Monte Carlo,
face significant computational challenges when dealing with the
high-dimensional parameter spaces and complex noise characteristics inherent in
gravitational wave data. This review examines the emerging role of
simulation-based inference methods in gravitational wave astronomy, with a
focus on approaches that leverage machine-learning techniques such as
normalizing flows and neural posterior estimation. We provide a comprehensive
overview of the theoretical foundations underlying various simulation-based
inference methods, including neural posterior estimation, neural ratio
estimation, neural likelihood estimation, flow matching, and consistency
models. We explore the applications of these methods across diverse
gravitational wave data processing scenarios, from single-source parameter
estimation and overlapping signal analysis to testing general relativity and
conducting population studies. Although these techniques demonstrate speed
improvements over traditional methods in controlled studies, their
model-dependent nature and sensitivity to prior assumptions are barriers to
their widespread adoption. Their accuracy, which is similar to that of
conventional methods, requires further validation across broader parameter
spaces and noise conditions.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [249] [A Tax-Efficient Model Predictive Control Policy for Retirement Funding](https://arxiv.org/abs/2507.10603)
*Kasper Johansson,Stephen Boyd*

Main category: math.OC

TL;DR: 本文提出两步法制定退休资金规划策略，用凸优化解决简化问题，采用模型预测控制（MPC）应对不确定性，并用蒙特卡罗模拟验证有效性。


<details>
  <summary>Details</summary>
Motivation: 解决退休人员储蓄管理问题，实现终身恒定税后通胀调整消费。

Method: 分两步制定策略，先解决简化规划问题，将其转化为凸优化问题；再采用模型预测控制（MPC），每年更新计划并执行。

Result: 提出的MPC退休政策能应对投资回报、通胀、预期寿命、外部收支及税收规则和税率变化等不确定性。

Conclusion: 通过蒙特卡罗模拟证明了MPC退休政策的有效性。

Abstract: The retirement funding problem addresses the question of how to manage a
retiree's savings to provide her with a constant post-tax inflation adjusted
consumption throughout her lifetime. This consists of choosing withdrawals and
transfers from and between several accounts with different tax treatments,
taking into account basic rules such as required minimum distributions and
limits on Roth conversions, additional income, liabilities, taxes, and the
bequest when the retiree dies. We develop a retirement funding policy in two
steps. In the first step, we consider a simplified planning problem in which
various future quantities, such as the retiree's remaining lifetime, future
investment returns, and future inflation, are known. Using a simplified model
of taxes, we pose this planning problem as a convex optimization problem, where
we maximize the bequest subject to providing a constant inflation adjusted
consumption target. Since this problem is convex, it can be solved quickly and
reliably. We leverage this planning method to form a retirement funding policy
that determines the actions to take each year, based on information known at
that time. Each year the retiree forms a new plan for the future years, using
the current account values and life expectancy, and optionally, updated
information such as changes in tax rates or rules. The retiree then carries out
the actions from the first year of the current plan. This update-plan-act cycle
is repeated each year, a general policy called model predictive control (MPC).
The MPC retirement policy reacts to the effects of uncertain investment returns
and inflation, changes in the retiree's expected lifetime or external income
and liabilities, and changes in tax rules and rates. We demonstrate the
effectiveness of the MPC retirement policy using Monte Carlo simulation.

</details>


### [250] [A Mathematical Optimization Approach to Multisphere Support Vector Data Description](https://arxiv.org/abs/2507.11106)
*Víctor Blanco,Inmaculada Espejo,Raúl Páez,Antonio M. Rodríguez-Chía*

Main category: math.OC

TL;DR: 提出用于多模态数据集离群点检测的优化框架，对比现有启发式技术有优势。


<details>
  <summary>Details</summary>
Motivation: 为多模态数据集离群点检测提供新方法，扩展支持向量数据描述方法。

Method: 提出混合整数二阶锥模型的原始公式构建欧几里得超球体识别异常观测，开发对偶模型应用核技巧。

Result: 广泛的计算研究表明精确方法有效，在准确性和鲁棒性上优于现有启发式技术。

Conclusion: 所提出的数学优化框架在多模态数据集离群点检测中效果良好且有优势。

Abstract: We present a novel mathematical optimization framework for outlier detection
in multimodal datasets, extending Support Vector Data Description approaches.
We provide a primal formulation, in the shape of a Mixed Integer Second Order
Cone model, that constructs Euclidean hyperspheres to identify anomalous
observations. Building on this, we develop a dual model that enables the
application of the kernel trick, thus allowing for the detection of outliers
within complex, non-linear data structures. An extensive computational study
demonstrates the effectiveness of our exact method, showing clear advantages
over existing heuristic techniques in terms of accuracy and robustness.

</details>


### [251] [Recursive Bound-Constrained AdaGrad with Applications to Multilevel and Domain Decomposition Minimization](https://arxiv.org/abs/2507.11513)
*Serge Gratton,Alena Kopaničáková,Philippe Toint*

Main category: math.OC

TL;DR: 提出两种OFFO噪声容忍算法，有统一理论框架，分析复杂度并通过实验验证效率。


<details>
  <summary>Details</summary>
Motivation: 处理边界约束、不精确梯度，在可用时利用二阶信息，拓展无约束优化的一阶AdaGrad算法。

Method: 提出多级别方法和领域分解方法，二者是一阶AdaGrad算法的推广，有统一理论框架。

Result: 两种方法以高概率在最多$O(\epsilon^{-2})$次迭代和噪声梯度评估内计算出边界约束问题的$\epsilon$-近似一阶临界点。

Conclusion: 算法在从基于PDE的问题到深度神经网络训练等应用中展现出卓越计算效率。

Abstract: Two OFFO (Objective-Function Free Optimization) noise tolerant algorithms are
presented that handle bound constraints, inexact gradients and use second-order
information when available.The first is a multi-level method exploiting a
hierarchical description of the problem and the second is a
domain-decomposition method covering the standard addditive Schwarz
decompositions. Both are generalizations of the first-order AdaGrad algorithm
for unconstrained optimization. Because these algorithms share a common
theoretical framework, a single convergence/complexity theory is provided which
covers them both. Its main result is that, with high probability, both methods
need at most $O(\epsilon^{-2})$ iterations and noisy gradient evaluations to
compute an $\epsilon$-approximate first-order critical point of the
bound-constrained problem. Extensive numerical experiments are discussed on
applications ranging from PDE-based problems to deep neural network training,
illustrating their remarkable computational efficiency.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [252] [Formal Verification of Variational Quantum Circuits](https://arxiv.org/abs/2507.10635)
*Nicola Assolini,Luca Marzari,Isabella Mastroeni,Alessandra di Pierro*

Main category: quant-ph

TL;DR: 文章对变分量子电路（VQCs）的形式验证问题进行了理论和实践研究，引入新语义框架并在标准验证基准上进行验证。


<details>
  <summary>Details</summary>
Motivation: 经典模型已有形式验证技术，但缺乏针对VQCs鲁棒性验证的框架，故开展对VQCs形式验证问题的研究。

Method: 受深度学习抽象解释方法启发，分析区间可达性技术在量子环境的适用性和局限性，引入基于抽象解释的语义框架。

Result: 发现量子特性如状态归一化会引入变量间依赖，挑战现有方法。

Conclusion: 提出的基于抽象解释的语义框架可形式定义VQCs验证问题并分析其复杂性，且在标准验证基准上得到验证。

Abstract: Variational quantum circuits (VQCs) are a central component of many quantum
machine learning algorithms, offering a hybrid quantum-classical framework
that, under certain aspects, can be considered similar to classical deep neural
networks. A shared aspect is, for instance, their vulnerability to adversarial
inputs, small perturbations that can lead to incorrect predictions. While
formal verification techniques have been extensively developed for classical
models, no comparable framework exists for certifying the robustness of VQCs.
Here, we present the first in-depth theoretical and practical study of the
formal verification problem for VQCs. Inspired by abstract interpretation
methods used in deep learning, we analyze the applicability and limitations of
interval-based reachability techniques in the quantum setting. We show that
quantum-specific aspects, such as state normalization, introduce inter-variable
dependencies that challenge existing approaches. We investigate these issues by
introducing a novel semantic framework based on abstract interpretation, where
the verification problem for VQCs can be formally defined, and its complexity
analyzed. Finally, we demonstrate our approach on standard verification
benchmarks.

</details>


### [253] [Stochastic Entanglement Configuration for Constructive Entanglement Topologies in Quantum Machine Learning with Application to Cardiac MRI](https://arxiv.org/abs/2507.11401)
*Mehri Mehrnia,Mohammed S. M. Elbaz*

Main category: quant-ph

TL;DR: 提出随机纠缠配置方法用于量子机器学习变分量子电路，在心脏MRI疾病分类中找到有效配置，优于经典模型和传统拓扑。


<details>
  <summary>Details</summary>
Motivation: 当前量子机器学习变分量子电路大多使用固定纠缠拓扑，无法适应任务需求，限制了相对经典模型的潜在优势。

Method: 引入随机纠缠配置方法，将配置编码为随机二进制矩阵，用纠缠密度和每量子比特约束作为指标，定义无约束和约束采样模式。

Result: 生成400个随机配置，找到64个有效纠缠配置，集成聚合达到约0.92的分类准确率，比经典模型高超5%，比传统拓扑高约20%。

Conclusion: 该方法找到的有效纠缠配置具有鲁棒性和泛化性。

Abstract: Efficient entanglement strategies are essential for advancing variational
quantum circuits (VQCs) for quantum machine learning (QML). However, most
current approaches use fixed entanglement topologies that are not adaptive to
task requirements, limiting potential gains over classical models. We introduce
a novel stochastic entanglement configuration method that systematically
generates diverse entanglement topologies to identify a subspace of
constructive entanglement configurations, defined as entanglement topologies
that boost hybrid model performance (e.g., classification accuracy) beyond
classical baselines. Each configuration is encoded as a stochastic binary
matrix, denoting directed entanglement between qubits. This enables scalable
exploration of the hyperspace of candidate entanglement topologies using
entanglement density and per-qubit constraints as key metrics. We define
unconstrained and constrained sampling modes, controlling entanglement per
qubit. Using our method, 400 stochastic configurations were generated and
evaluated in a hybrid QML for cardiac MRI disease classification. We identified
64 (16%) novel constructive entanglement configurations that consistently
outperformed the classical baseline. Ensemble aggregation of top-performing
configurations achieved ~0.92 classification accuracy, exceeding the classical
model (~0.87) by over 5%. Compared to four conventional topologies (ring,
nearest neighbor, no entanglement, fully entangled), none surpassed the
classical baseline (maximum accuracy ~0.82), while our configurations delivered
up to ~20% higher accuracy. Thus, highlighting the robustness and
generalizability of the identified constructive entanglements.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [254] [Crypto-Assisted Graph Degree Sequence Release under Local Differential Privacy](https://arxiv.org/abs/2507.10627)
*Xiaojian Zhang,Junqing Wang,Kerui Chen,Peiyuan Zhao,Huiyuan Bai*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Given a graph $G$ defined in a domain $\mathcal{G}$, we investigate locally
differentially private mechanisms to release a degree sequence on $\mathcal{G}$
that accurately approximates the actual degree distribution. Existing solutions
for this problem mostly use graph projection techniques based on edge deletion
process, using a threshold parameter $\theta$ to bound node degrees. However,
this approach presents a fundamental trade-off in threshold parameter
selection. While large $\theta$ values introduce substantial noise in the
released degree sequence, small $\theta$ values result in more edges removed
than necessary. Furthermore, $\theta$ selection leads to an excessive
communication cost. To remedy existing solutions' deficiencies, we present
CADR-LDP, an efficient framework incorporating encryption techniques and
differentially private mechanisms to release the degree sequence. In CADR-LDP,
we first use the crypto-assisted Optimal-$\theta$-Selection method to select
the optimal parameter with a low communication cost. Then, we use the LPEA-LOW
method to add some edges for each node with the edge addition process in local
projection. LPEA-LOW prioritizes the projection with low-degree nodes, which
can retain more edges for such nodes and reduce the projection error.
Theoretical analysis shows that CADR-LDP satisfies $\epsilon$-node local
differential privacy. The experimental results on eight graph datasets show
that our solution outperforms existing methods.

</details>


### [255] [Access Control for Information-Theoretically Secure Key-Document Stores](https://arxiv.org/abs/2507.10730)
*Yin Li,Sharad Mehrota,Shantanu Sharma,Komal Kumari*

Main category: cs.CR

TL;DR: 提出一种基于密钥的访问控制技术用于安全外包键值存储，采用Shamir秘密共享，可支持关键字文档检索等


<details>
  <summary>Details</summary>
Motivation: 实现安全外包键值存储，防止数据、用户访问权限和查询结果大小信息泄露

Method: 采用Shamir的秘密共享

Result: 能支持关键字文档检索，可检测恶意客户端，防止恶意服务器篡改数据，500,000个文件中对5,000个关键字的操作需231.5ms

Conclusion: 该方法可实现安全外包键值存储，同时保证高效访问

Abstract: This paper presents a novel key-based access control technique for secure
outsourcing key-value stores where values correspond to documents that are
indexed and accessed using keys. The proposed approach adopts Shamir's
secret-sharing that offers unconditional or information-theoretic security. It
supports keyword-based document retrieval while preventing leakage of the data,
access rights of users, or the size (\textit{i}.\textit{e}., volume of the
output that satisfies a query). The proposed approach allows servers to detect
(and abort) malicious clients from gaining unauthorized access to data, and
prevents malicious servers from altering data undetected while ensuring
efficient access -- it takes 231.5ms over 5,000 keywords across 500,000 files.

</details>


### [256] [A Review of Privacy Metrics for Privacy-Preserving Synthetic Data Generation](https://arxiv.org/abs/2507.11324)
*Frederik Marinus Trudslev,Matteo Lissandrini,Juan Manuel Rodriguez,Martin Bøgsted,Daniele Dell'Aglio*

Main category: cs.CR

TL;DR: 本文介绍17种隐私度量的假设和数学公式，以让隐私损失相关实际风险更透明。


<details>
  <summary>Details</summary>
Motivation: 差分隐私的隐私损失难以解释，为使隐私损失相关实际风险更透明，需要对多种隐私度量的计算进行明确定义。

Method: 呈现17种不同隐私度量的假设和数学公式。

Result: 给出17种隐私度量的假设和数学公式。

Conclusion: 有必要对隐私度量的计算进行清晰定义。

Abstract: Privacy Preserving Synthetic Data Generation (PP-SDG) has emerged to produce
synthetic datasets from personal data while maintaining privacy and utility.
Differential privacy (DP) is the property of a PP-SDG mechanism that
establishes how protected individuals are when sharing their sensitive data. It
is however difficult to interpret the privacy loss ($\varepsilon$) expressed by
DP. To make the actual risk associated with the privacy loss more transparent,
multiple privacy metrics (PMs) have been proposed to assess the privacy risk of
the data. These PMs are utilized in separate studies to assess newly introduced
PP-SDG mechanisms. Consequently, these PMs embody the same assumptions as the
PP-SDG mechanism they were made to assess. Therefore, a thorough definition of
how these are calculated is necessary. In this work, we present the assumptions
and mathematical formulations of 17 distinct privacy metrics.

</details>


### [257] [Game Theory Meets LLM and Agentic AI: Reimagining Cybersecurity for the Age of Intelligent Threats](https://arxiv.org/abs/2507.10621)
*Quanyan Zhu*

Main category: cs.CR

TL;DR: 文章探讨博弈论、具身AI与网络安全交叉领域，指出传统方法不足，借LLM和具身AI弥合理论与实践差距，介绍相关理论框架及应用。


<details>
  <summary>Details</summary>
Motivation: 传统网络安全方法依赖手动响应和脆弱启发式，需结合博弈论和软件工具构建主动智能防御系统，但理论与实践脱节，LLM和具身AI提供新途径。

Method: 回顾关键博弈论框架和解决方案概念，研究LLM代理增强网络防御、引入LLM驱动游戏，探索多智能体工作流程和协调游戏。

Result: LLM和具身AI可弥合理论与实践差距，共同进化带来更丰富理论基础和新解决方案概念，重塑软件设计。

Conclusion: 博弈论、具身AI和网络安全的融合能促进安全、智能和自适应网络系统的发展。

Abstract: Protecting cyberspace requires not only advanced tools but also a shift in
how we reason about threats, trust, and autonomy. Traditional cybersecurity
methods rely on manual responses and brittle heuristics. To build proactive and
intelligent defense systems, we need integrated theoretical frameworks and
software tools. Game theory provides a rigorous foundation for modeling
adversarial behavior, designing strategic defenses, and enabling trust in
autonomous systems. Meanwhile, software tools process cyber data, visualize
attack surfaces, verify compliance, and suggest mitigations. Yet a disconnect
remains between theory and practical implementation.
  The rise of Large Language Models (LLMs) and agentic AI offers a new path to
bridge this gap. LLM-powered agents can operationalize abstract strategies into
real-world decisions. Conversely, game theory can inform the reasoning and
coordination of these agents across complex workflows. LLMs also challenge
classical game-theoretic assumptions, such as perfect rationality or static
payoffs, prompting new models aligned with cognitive and computational
realities. This co-evolution promises richer theoretical foundations and novel
solution concepts. Agentic AI also reshapes software design: systems must now
be modular, adaptive, and trust-aware from the outset.
  This chapter explores the intersection of game theory, agentic AI, and
cybersecurity. We review key game-theoretic frameworks (e.g., static, dynamic,
Bayesian, and signaling games) and solution concepts. We then examine how LLM
agents can enhance cyber defense and introduce LLM-driven games that embed
reasoning into AI agents. Finally, we explore multi-agent workflows and
coordination games, outlining how this convergence fosters secure, intelligent,
and adaptive cyber systems.

</details>


### [258] [BandFuzz: An ML-powered Collaborative Fuzzing Framework](https://arxiv.org/abs/2507.10845)
*Wenxuan Shi,Hongwei Li,Jiahao Yu,Xinqian Sun,Wenbo Guo,Xinyu Xing*

Main category: cs.CR

TL;DR: 介绍协同模糊测试技术，指出其优势及现有框架面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决单个模糊测试器依赖特定假设、需手动挑选的问题，寻求通用模糊测试解决方案。

Method: 未提及

Result: 现有协同模糊测试框架受限于需要额外计算资源和资源分配低效等挑战。

Conclusion: 协同模糊测试是通用模糊测试解决方案的有前景方向，但现有框架效果受限。

Abstract: Collaborative fuzzing has recently emerged as a technique that combines
multiple individual fuzzers and dynamically chooses the appropriate
combinations suited for different programs. Unlike individual fuzzers, which
rely on specific assumptions to maintain their effectiveness, collaborative
fuzzing relaxes the assumptions on target programs, providing constant and
robust performance across various programs. Ideally, collaborative fuzzing
should be a more promising direction toward generic fuzzing solutions, as it
mitigates the need for manual cherry-picking of individual fuzzers. However,
the effectiveness of existing collaborative fuzzing frameworks is limited by
major challenges, such as the need for additional computational resources
compared to individual fuzzers and the inefficient allocation of resources
among the various fuzzers.

</details>


### [259] [MalCodeAI: Autonomous Vulnerability Detection and Remediation via Language Agnostic Code Reasoning](https://arxiv.org/abs/2507.10898)
*Jugal Gajjar,Kamalasankari Subramaniakuppusamy,Noha El Kachach*

Main category: cs.CR

TL;DR: 提出语言无关的多阶段AI管道MalCodeAI用于代码安全分析与修复，经训练在多方面表现良好，开发者评估得分高，推动软件安全方案发展。


<details>
  <summary>Details</summary>
Motivation: 网络威胁日益复杂，传统漏洞检测工具存在局限，需要新方法保障软件系统安全。

Method: 结合代码分解和语义推理，使用微调的Qwen2.5 - Coder - 3B - Instruct模型，在MLX框架内通过低秩自适应（LoRA）优化。

Result: 阶段1功能分解和代码段总结验证损失低至0.397；阶段2漏洞检测和修复最佳验证损失0.199；支持多种检测功能；开发者评估中各项得分较高。

Conclusion: 该工作是迈向智能、可解释且以开发者为中心的软件安全解决方案的重要进展。

Abstract: The growing complexity of cyber threats and the limitations of traditional
vulnerability detection tools necessitate novel approaches for securing
software systems. We introduce MalCodeAI, a language-agnostic, multi-stage AI
pipeline for autonomous code security analysis and remediation. MalCodeAI
combines code decomposition and semantic reasoning using fine-tuned
Qwen2.5-Coder-3B-Instruct models, optimized through Low-Rank Adaptation (LoRA)
within the MLX framework, and delivers scalable, accurate results across 14
programming languages. In Phase 1, the model achieved a validation loss as low
as 0.397 for functional decomposition and summarization of code segments after
200 iterations, 6 trainable layers, and a learning rate of 2 x 10^(-5). In
Phase 2, for vulnerability detection and remediation, it achieved a best
validation loss of 0.199 using the same number of iterations and trainable
layers but with an increased learning rate of 4 x 10^(-5), effectively
identifying security flaws and suggesting actionable fixes. MalCodeAI supports
red-hat-style exploit tracing, CVSS-based risk scoring, and zero-shot
generalization to detect complex, zero-day vulnerabilities. In a qualitative
evaluation involving 15 developers, the system received high scores in
usefulness (mean 8.06/10), interpretability (mean 7.40/10), and readability of
outputs (mean 7.53/10), confirming its practical value in real-world
development workflows. This work marks a significant advancement toward
intelligent, explainable, and developer-centric software security solutions.

</details>


### [260] [When and Where do Data Poisons Attack Textual Inversion?](https://arxiv.org/abs/2507.10578)
*Jeremy Styborski,Mingzhi Lyu,Jiayou Lu,Nupur Kapur,Adams Kong*

Main category: cs.CR

TL;DR: 本文系统分析扩散模型中毒攻击，提出Safe - Zone Training防御机制增强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 中毒攻击对扩散模型的鲁棒性构成挑战，需分析攻击情况并提出防御方法。

Method: 引入语义敏感度图，分析模型学习行为，提出包含JPEG压缩、限制训练时间步、损失掩码的Safe - Zone Training防御机制。

Result: 广泛实验表明SZT增强了文本反转对所有中毒攻击的鲁棒性，提升生成质量。

Conclusion: Safe - Zone Training能有效增强扩散模型文本反转对中毒攻击的鲁棒性，优于先前防御方法。

Abstract: Poisoning attacks pose significant challenges to the robustness of diffusion
models (DMs). In this paper, we systematically analyze when and where poisoning
attacks textual inversion (TI), a widely used personalization technique for
DMs. We first introduce Semantic Sensitivity Maps, a novel method for
visualizing the influence of poisoning on text embeddings. Second, we identify
and experimentally verify that DMs exhibit non-uniform learning behavior across
timesteps, focusing on lower-noise samples. Poisoning attacks inherit this bias
and inject adversarial signals predominantly at lower timesteps. Lastly, we
observe that adversarial signals distract learning away from relevant concept
regions within training data, corrupting the TI process. Based on these
insights, we propose Safe-Zone Training (SZT), a novel defense mechanism
comprised of 3 key components: (1) JPEG compression to weaken high-frequency
poison signals, (2) restriction to high timesteps during TI training to avoid
adversarial signals at lower timesteps, and (3) loss masking to constrain
learning to relevant regions. Extensive experiments across multiple poisoning
methods demonstrate that SZT greatly enhances the robustness of TI against all
poisoning attacks, improving generative quality beyond prior published
defenses. Code: www.github.com/JStyborski/Diff_Lab Data:
www.github.com/JStyborski/NC10

</details>


### [261] [LaSM: Layer-wise Scaling Mechanism for Defending Pop-up Attack on GUI Agents](https://arxiv.org/abs/2507.10610)
*Zihe Yan,Zhuosheng Zhang*

Main category: cs.CR

TL;DR: 本文研究GUI智能体在弹出式环境注入攻击下的漏洞，提出LaSM机制提升防御成功率，揭示注意力未对齐是核心漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有GUI智能体易受弹出式环境注入攻击，且现有防御方法存在成本高或效果差的问题。

Method: 系统研究攻击如何改变GUI智能体的注意力行为，发现正确和错误输出间的逐层注意力差异模式，提出LaSM机制选择性放大关键层的注意力和MLP模块。

Result: 在12种弹出式干扰和4种不同模型骨干上进行实验，LaSM持续提升防御成功率，结合提示级警报时在强归纳攻击下鲁棒性超98%。

Conclusion: 注意力未对齐是MLLM智能体的核心漏洞，可通过选择性逐层调制有效解决。

Abstract: Graphical user interface (GUI) agents built on multimodal large language
models (MLLMs) have recently demonstrated strong decision-making abilities in
screen-based interaction tasks. However, they remain highly vulnerable to
pop-up-based environmental injection attacks, where malicious visual elements
divert model attention and lead to unsafe or incorrect actions. Existing
defense methods either require costly retraining or perform poorly under
inductive interference. In this work, we systematically study how such attacks
alter the attention behavior of GUI agents and uncover a layer-wise attention
divergence pattern between correct and incorrect outputs. Based on this
insight, we propose \textbf{LaSM}, a \textit{Layer-wise Scaling Mechanism} that
selectively amplifies attention and MLP modules in critical layers. LaSM
improves the alignment between model saliency and task-relevant regions without
additional training. Extensive experiments across 12 types of pop-up
perturbations and 4 different model backbones show that LaSM consistently
enhances the defense success rate. When combined with prompt-level alerts, LaSM
achieves over 98\% robustness even under strong inductive attacks. Our findings
reveal that attention misalignment is a core vulnerability in MLLM agents and
can be effectively addressed through selective layer-wise modulation.

</details>


### [262] [Spectral Feature Extraction for Robust Network Intrusion Detection Using MFCCs](https://arxiv.org/abs/2507.10622)
*HyeYoung Lee,Muhammad Nadeem,Pavel Tsoi*

Main category: cs.CR

TL;DR: 本文提出结合MFCC和ResNet - 18的物联网网络流量异常检测方法，在三个数据集评估，证明自适应信号处理与深度学习架构结合可实现鲁棒可扩展的异常检测。


<details>
  <summary>Details</summary>
Motivation: 物联网网络快速扩张导致安全漏洞激增，需要强大的异常检测和分类技术。

Method: 利用可学习的MFCC和ResNet - 18识别物联网网络流量异常，将原始信号转换为MFCC以增强类可分性。

Result: 在CICIoT2023、NSL - KDD和IoTID20三个数据集上进行评估。

Conclusion: 自适应信号处理技术与深度学习架构结合能在异构物联网网络中实现鲁棒且可扩展的异常检测。

Abstract: The rapid expansion of Internet of Things (IoT) networks has led to a surge
in security vulnerabilities, emphasizing the critical need for robust anomaly
detection and classification techniques. In this work, we propose a novel
approach for identifying anomalies in IoT network traffic by leveraging the
Mel-frequency cepstral coefficients (MFCC) and ResNet-18, a deep learning model
known for its effectiveness in feature extraction and image-based tasks.
Learnable MFCCs enable adaptive spectral feature representation, capturing the
temporal patterns inherent in network traffic more effectively than traditional
fixed MFCCs. We demonstrate that transforming raw signals into MFCCs maps the
data into a higher-dimensional space, enhancing class separability and enabling
more effective multiclass classification. Our approach combines the strengths
of MFCCs with the robust feature extraction capabilities of ResNet-18, offering
a powerful framework for anomaly detection. The proposed model is evaluated on
three widely used IoT intrusion detection datasets: CICIoT2023, NSL-KDD, and
IoTID20. The experimental results highlight the potential of integrating
adaptive signal processing techniques with deep learning architectures to
achieve robust and scalable anomaly detection in heterogeneous IoT network
landscapes.

</details>


### [263] [PhreshPhish: A Real-World, High-Quality, Large-Scale Phishing Website Dataset and Benchmark](https://arxiv.org/abs/2507.10854)
*Thomas Dalton,Hemanth Gowda,Girish Rao,Sachin Pargi,Alireza Hadj Khodabakhshi,Joseph Rombs,Stephan Jou,Manish Marwah*

Main category: cs.CR

TL;DR: 本文介绍了大规模高质量钓鱼网站数据集PhreshPhish及基准数据集，解决现有数据集不足，促进钓鱼检测模型标准化比较和进步。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习在钓鱼攻击实时检测中因缺乏高质量数据集和基准而进展受阻，现有数据集存在质量差、数据泄露和不现实的基本比率等问题。

Method: 引入PhreshPhish数据集，提出一套综合基准数据集，通过减少数据泄露、增加任务难度、提高多样性和调整基本比率等设计，并对多种解决方案进行训练和评估。

Result: PhreshPhish比现有公共数据集规模大、质量高，完成了基准集的基线性能评估。

Conclusion: 该数据集和基准的可用性将实现模型的现实、标准化比较，推动钓鱼检测的进一步发展，数据集和基准可在Hugging Face获取。

Abstract: Phishing remains a pervasive and growing threat, inflicting heavy economic
and reputational damage. While machine learning has been effective in real-time
detection of phishing attacks, progress is hindered by lack of large,
high-quality datasets and benchmarks. In addition to poor-quality due to
challenges in data collection, existing datasets suffer from leakage and
unrealistic base rates, leading to overly optimistic performance results. In
this paper, we introduce PhreshPhish, a large-scale, high-quality dataset of
phishing websites that addresses these limitations. Compared to existing public
datasets, PhreshPhish is substantially larger and provides significantly higher
quality, as measured by the estimated rate of invalid or mislabeled data
points. Additionally, we propose a comprehensive suite of benchmark datasets
specifically designed for realistic model evaluation by minimizing leakage,
increasing task difficulty, enhancing dataset diversity, and adjustment of base
rates more likely to be seen in the real world. We train and evaluate multiple
solution approaches to provide baseline performance on the benchmark sets. We
believe the availability of this dataset and benchmarks will enable realistic,
standardized model comparison and foster further advances in phishing
detection. The datasets and benchmarks are available on Hugging Face
(https://huggingface.co/datasets/phreshphish/phreshphish).

</details>


### [264] [Hashed Watermark as a Filter: Defeating Forging and Overwriting Attacks in Weight-based Neural Network Watermarking](https://arxiv.org/abs/2507.11137)
*Yuan Yao,Jin Song,Jian Jin*

Main category: cs.CR

TL;DR: 提出NeuralMark方法保护神经网络所有权，利用哈希水印过滤器，能抵御多种攻击，在多架构和任务验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于权重的神经网络水印方法易受伪造和覆盖攻击，需要更鲁棒的所有权保护方法。

Method: 利用哈希函数从密钥生成不可逆二进制水印作过滤器选择嵌入参数，结合平均池化抵抗微调与剪枝攻击，可集成到多种架构。

Result: 理论分析安全边界，在13种卷积和Transformer架构、6种任务中验证有效性和鲁棒性。

Conclusion: NeuralMark方法能有效保护神经网络所有权，具有鲁棒性和广泛适用性。

Abstract: As valuable digital assets, deep neural networks necessitate robust ownership
protection, positioning neural network watermarking (NNW) as a promising
solution. Among various NNW approaches, weight-based methods are favored for
their simplicity and practicality; however, they remain vulnerable to forging
and overwriting attacks. To address those challenges, we propose NeuralMark, a
robust method built around a hashed watermark filter. Specifically, we utilize
a hash function to generate an irreversible binary watermark from a secret key,
which is then used as a filter to select the model parameters for embedding.
This design cleverly intertwines the embedding parameters with the hashed
watermark, providing a robust defense against both forging and overwriting
attacks. An average pooling is also incorporated to resist fine-tuning and
pruning attacks. Furthermore, it can be seamlessly integrated into various
neural network architectures, ensuring broad applicability. Theoretically, we
analyze its security boundary. Empirically, we verify its effectiveness and
robustness across 13 distinct Convolutional and Transformer architectures,
covering five image classification tasks and one text generation task. The
source codes are available at https://github.com/AIResearch-Group/NeuralMark.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [265] [Auditing Facial Emotion Recognition Datasets for Posed Expressions and Racial Bias](https://arxiv.org/abs/2507.10755)
*Rina Khan,Catherine Stinson*

Main category: cs.CV

TL;DR: 研究审计两个先进的面部表情识别（FER）数据集，发现数据集中存在问题及模型对不同肤色人群预测有偏差。


<details>
  <summary>Details</summary>
Motivation: 解决FER算法在检测自发表情时性能下降以及对部分种族和肤色人群表现不佳的挑战，这些挑战与数据集创建的数据收集实践有关。

Method: 从每个数据集随机抽样，检查图像是自发还是摆拍，提出识别方法；观察样本中个体肤色，测试在各数据集上训练的三个模型对不同种族和肤色人群面部表情的预测。

Result: 数据集中声称野外图像有大量摆拍；被审计的FER模型更易将非白人或深色皮肤者误判为负面情绪。

Conclusion: 在这些数据集上训练的模型性能不能代表野外应用的真实性能，模型偏差会在现实应用中造成危害。

Abstract: Facial expression recognition (FER) algorithms classify facial expressions
into emotions such as happy, sad, or angry. An evaluative challenge facing FER
algorithms is the fall in performance when detecting spontaneous expressions
compared to posed expressions. An ethical (and evaluative) challenge facing FER
algorithms is that they tend to perform poorly for people of some races and
skin colors. These challenges are linked to the data collection practices
employed in the creation of FER datasets. In this study, we audit two
state-of-the-art FER datasets. We take random samples from each dataset and
examine whether images are spontaneous or posed. In doing so, we propose a
methodology for identifying spontaneous or posed images. We discover a
significant number of images that were posed in the datasets purporting to
consist of in-the-wild images. Since performance of FER models vary between
spontaneous and posed images, the performance of models trained on these
datasets will not represent the true performance if such models were to be
deployed in in-the-wild applications. We also observe the skin color of
individuals in the samples, and test three models trained on each of the
datasets to predict facial expressions of people from various races and skin
tones. We find that the FER models audited were more likely to predict people
labeled as not white or determined to have dark skin as showing a negative
emotion such as anger or sadness even when they were smiling. This bias makes
such models prone to perpetuate harm in real life applications.

</details>


### [266] [A New Dataset and Performance Benchmark for Real-time Spacecraft Segmentation in Onboard Flight Computers](https://arxiv.org/abs/2507.10775)
*Jeffrey Joan Sam,Janhavi Sathe,Nikhil Chigali,Naman Gupta,Radhey Ruparel,Yicheng Jiang,Janmajay Singh,James W. Berck,Arko Barman*

Main category: cs.CV

TL;DR: 本文提出含近6.4万张标注图像的航天器数据集；对YOLOv8和YOLOv11微调，在特定约束下测试模型性能，结果良好，相关数据和模型开源。


<details>
  <summary>Details</summary>
Motivation: 航天器易受损，太空维修有风险且成本高，现有图像分割技术缺可用标注数据。

Method: 用真实航天器模型创建数据集，混合真实和合成背景，添加噪声和畸变；微调YOLOv8和YOLOv11，在特定约束下测试。

Result: 模型在约束条件下Dice分数0.92，Hausdorff距离0.69，推理时间约0.5秒。

Conclusion: 所提数据集和微调模型在太空实时图像分割应用有良好表现，相关资源可在指定链接获取。

Abstract: Spacecraft deployed in outer space are routinely subjected to various forms
of damage due to exposure to hazardous environments. In addition, there are
significant risks to the subsequent process of in-space repairs through human
extravehicular activity or robotic manipulation, incurring substantial
operational costs. Recent developments in image segmentation could enable the
development of reliable and cost-effective autonomous inspection systems. While
these models often require large amounts of training data to achieve
satisfactory results, publicly available annotated spacecraft segmentation data
are very scarce. Here, we present a new dataset of nearly 64k annotated
spacecraft images that was created using real spacecraft models, superimposed
on a mixture of real and synthetic backgrounds generated using NASA's TTALOS
pipeline. To mimic camera distortions and noise in real-world image
acquisition, we also added different types of noise and distortion to the
images. Finally, we finetuned YOLOv8 and YOLOv11 segmentation models to
generate performance benchmarks for the dataset under well-defined hardware and
inference time constraints to mimic real-world image segmentation challenges
for real-time onboard applications in space on NASA's inspector spacecraft. The
resulting models, when tested under these constraints, achieved a Dice score of
0.92, Hausdorff distance of 0.69, and an inference time of about 0.5 second.
The dataset and models for performance benchmark are available at
https://github.com/RiceD2KLab/SWiM.

</details>


### [267] [Warehouse Spatial Question Answering with LLM Agent](https://arxiv.org/abs/2507.10778)
*Hsiang-Wei Huang,Jen-Hao Cheng,Kuang-Ming Chen,Cheng-Yen Yang,Bahaa Alattar,Yi-Ru Lin,Pyongkun Kim,Sangwon Kim,Kwangju Kim,Chung-I Huang,Jenq-Neng Hwang*

Main category: cs.CV

TL;DR: 提出数据高效方法，构建有强空间推理能力的LLM代理系统解决复杂室内仓库场景空间问答任务，评估显示系统高效准确，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型（MLLMs）在空间理解任务上有挑战，以往方法需大规模微调，本文寻求数据高效的方法。

Method: 提出具有强空间推理能力的LLM代理系统，集成多种工具进行空间推理和API工具交互。

Result: 在2025 AI City Challenge Physical AI Spatial Intelligence Warehouse数据集上的评估表明，系统在对象检索、计数和距离估计等任务中实现了高精度和高效率。

Conclusion: 所提出的数据高效方法构建的系统能有效解决复杂室内仓库场景的空间问答任务。

Abstract: Spatial understanding has been a challenging task for existing Multi-modal
Large Language Models~(MLLMs). Previous methods leverage large-scale MLLM
finetuning to enhance MLLM's spatial understanding ability. In this paper, we
present a data-efficient approach. We propose a LLM agent system with strong
and advanced spatial reasoning ability, which can be used to solve the
challenging spatial question answering task in complex indoor warehouse
scenarios. Our system integrates multiple tools that allow the LLM agent to
conduct spatial reasoning and API tools interaction to answer the given
complicated spatial question. Extensive evaluations on the 2025 AI City
Challenge Physical AI Spatial Intelligence Warehouse dataset demonstrate that
our system achieves high accuracy and efficiency in tasks such as object
retrieval, counting, and distance estimation. The code is available at:
https://github.com/hsiangwei0903/SpatialAgent

</details>


### [268] [Winsor-CAM: Human-Tunable Visual Explanations from Deep Networks via Layer-Wise Winsorization](https://arxiv.org/abs/2507.10846)
*Casey Wall,Longwei Wang,Rodrigue Rizk,KC Santosh*

Main category: cs.CV

TL;DR: 提出Winsor - CAM方法改进Grad - CAM，在标准架构和数据集上评估显示其优于对比方法，推进可信AI。


<details>
  <summary>Details</summary>
Motivation: Grad - CAM聚焦最后卷积层或简单跨层平均，会掩盖重要语义线索或放大无关噪声，需要更好的解释CNN决策过程的方法。

Method: 提出Winsor - CAM，通过聚合所有卷积层信息生成显著图，应用Winsorization减轻异常值影响，设置用户可控阈值进行语义级调整。

Result: 在标准架构和PASCAL VOC 2012数据集上，Winsor - CAM生成更易解释的热力图，在定位指标上表现优于Grad - CAM和均匀层平均基线。

Conclusion: Winsor - CAM通过提供可解释的多层见解和人在环控制，推进了可信AI的目标。

Abstract: Interpreting the decision-making process of Convolutional Neural Networks
(CNNs) is critical for deploying models in high-stakes domains.
Gradient-weighted Class Activation Mapping (Grad-CAM) is a widely used method
for visual explanations, yet it typically focuses on the final convolutional
layer or na\"ively averages across layers, strategies that can obscure
important semantic cues or amplify irrelevant noise. We propose Winsor-CAM, a
novel, human-tunable extension of Grad-CAM that generates robust and coherent
saliency maps by aggregating information across all convolutional layers. To
mitigate the influence of noisy or extreme attribution values, Winsor-CAM
applies Winsorization, a percentile-based outlier attenuation technique. A
user-controllable threshold allows for semantic-level tuning, enabling flexible
exploration of model behavior across representational hierarchies. Evaluations
on standard architectures (ResNet50, DenseNet121, VGG16, InceptionV3) using the
PASCAL VOC 2012 dataset demonstrate that Winsor-CAM produces more interpretable
heatmaps and achieves superior performance in localization metrics, including
intersection-over-union and center-of-mass alignment, when compared to Grad-CAM
and uniform layer-averaging baselines. Winsor-CAM advances the goal of
trustworthy AI by offering interpretable, multi-layer insights with
human-in-the-loop control.

</details>


### [269] [A Lightweight and Robust Framework for Real-Time Colorectal Polyp Detection Using LOF-Based Preprocessing and YOLO-v11n](https://arxiv.org/abs/2507.10864)
*Saadat Behzadi,Danial Sharifrazi,Bita Mesbahzadeh,Javad Hassannataj Joloudarid,Roohallah Alizadehsani*

Main category: cs.CV

TL;DR: 本文提出结合LOF算法和YOLO - v11n的息肉检测框架，在多数据集测试表现良好，适用于临床实时支持。


<details>
  <summary>Details</summary>
Motivation: 及时准确检测结直肠息肉对诊断和预防结直肠癌至关重要，需新的检测框架。

Method: 在五个公开数据集上测试，将分割掩码转为检测标签，用LOF去除异常样本，采用5折交叉验证，数据输入YOLO - v11n并结合现代增强策略训练。

Result: 显著提高息肉定位性能，各项指标表现好，比以往YOLO方法更准确高效。

Conclusion: 该方法适合临床实时结肠镜检查支持，强调医学影像AI系统设计中数据预处理和模型效率的重要性。

Abstract: Objectives: Timely and accurate detection of colorectal polyps plays a
crucial role in diagnosing and preventing colorectal cancer, a major cause of
mortality worldwide. This study introduces a new, lightweight, and efficient
framework for polyp detection that combines the Local Outlier Factor (LOF)
algorithm for filtering noisy data with the YOLO-v11n deep learning model.
  Study design: An experimental study leveraging deep learning and outlier
removal techniques across multiple public datasets.
  Methods: The proposed approach was tested on five diverse and publicly
available datasets: CVC-ColonDB, CVC-ClinicDB, Kvasir-SEG, ETIS, and EndoScene.
Since these datasets originally lacked bounding box annotations, we converted
their segmentation masks into suitable detection labels. To enhance the
robustness and generalizability of our model, we apply 5-fold cross-validation
and remove anomalous samples using the LOF method configured with 30 neighbors
and a contamination ratio of 5%. Cleaned data are then fed into YOLO-v11n, a
fast and resource-efficient object detection architecture optimized for
real-time applications. We train the model using a combination of modern
augmentation strategies to improve detection accuracy under diverse conditions.
  Results: Our approach significantly improves polyp localization performance,
achieving a precision of 95.83%, recall of 91.85%, F1-score of 93.48%, mAP@0.5
of 96.48%, and mAP@0.5:0.95 of 77.75%. Compared to previous YOLO-based methods,
our model demonstrates enhanced accuracy and efficiency.
  Conclusions: These results suggest that the proposed method is well-suited
for real-time colonoscopy support in clinical settings. Overall, the study
underscores how crucial data preprocessing and model efficiency are when
designing effective AI systems for medical imaging.

</details>


### [270] [Modernizing CNN-based Weather Forecast Model towards Higher Computational Efficiency](https://arxiv.org/abs/2507.10893)
*Minjong Cheon,Eunhan Goo,Su-Hyeon Shin,Muhammad Ahmed,Hyungjun Kim*

Main category: cs.CV

TL;DR: 本文介绍用于全球天气预报的CNN模型KAI - a，其在降低计算需求的同时实现有竞争力的精度，性能与先进模型相当且能捕捉极端事件。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer架构的AI天气预报模型训练复杂度高、资源需求大，需改进。

Method: 提出CNN模型KAI - a，包含尺度不变架构和基于InceptionNeXt的模块，在ERA5每日数据集上训练。

Result: KAI - a在中程天气预报中性能与先进模型相当，设计轻量级，能有效捕捉极端事件。

Conclusion: KAI - a在保证精度的同时降低计算需求，具有实际应用价值。

Abstract: Recently, AI-based weather forecast models have achieved impressive advances.
These models have reached accuracy levels comparable to traditional NWP
systems, marking a significant milestone in data-driven weather prediction.
However, they mostly leverage Transformer-based architectures, which often
leads to high training complexity and resource demands due to the massive
parameter sizes. In this study, we introduce a modernized CNN-based model for
global weather forecasting that delivers competitive accuracy while
significantly reducing computational requirements. To present a systematic
modernization roadmap, we highlight key architectural enhancements across
multiple design scales from an earlier CNN-based approach. KAI-a incorporates a
scale-invariant architecture and InceptionNeXt-based blocks within a
geophysically-aware design, tailored to the structure of Earth system data.
Trained on the ERA5 daily dataset with 67 atmospheric variables, the model
contains about 7 million parameters and completes training in just 12 hours on
a single NVIDIA L40s GPU. Our evaluation shows that KAI-a matches the
performance of state-of-the-art models in medium-range weather forecasting,
while offering a significantly lightweight design. Furthermore, case studies on
the 2018 European heatwave and the East Asian summer monsoon demonstrate
KAI-a's robust skill in capturing extreme events, reinforcing its practical
utility.

</details>


### [271] [Commuting Distance Regularization for Timescale-Dependent Label Inconsistency in EEG Emotion Recognition](https://arxiv.org/abs/2507.10895)
*Xiaocong Zeng,Craig Michoski,Yan Pang,Dongyang Kuang*

Main category: cs.CV

TL;DR: 论文针对EEG情感识别中被忽视的时间尺度依赖标签不一致问题，提出LVL和LGCL正则化策略及新评估指标，在两个数据集上验证，结果优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 解决EEG情感识别中时间尺度依赖标签不一致问题，提升模型泛化性和可解释性。

Method: 提出LVL和LGCL正则化策略，结合有界变差函数和通勤时间距离；引入新评估指标；在多个神经架构上实验，用五种指标评估。

Result: 提出方法优于现有基线，LVL整体排名最佳，LGCL常排第二。

Conclusion: 所提框架有效，能在标签不一致情况下平衡可解释性和预测能力。

Abstract: In this work, we address the often-overlooked issue of Timescale Dependent
Label Inconsistency (TsDLI) in training neural network models for EEG-based
human emotion recognition. To mitigate TsDLI and enhance model generalization
and explainability, we propose two novel regularization strategies: Local
Variation Loss (LVL) and Local-Global Consistency Loss (LGCL). Both methods
incorporate classical mathematical principles--specifically, functions of
bounded variation and commute-time distances--within a graph theoretic
framework. Complementing our regularizers, we introduce a suite of new
evaluation metrics that better capture the alignment between temporally local
predictions and their associated global emotion labels. We validate our
approach through comprehensive experiments on two widely used EEG emotion
datasets, DREAMER and DEAP, across a range of neural architectures including
LSTM and transformer-based models. Performance is assessed using five distinct
metrics encompassing both quantitative accuracy and qualitative consistency.
Results consistently show that our proposed methods outperform state-of-the-art
baselines, delivering superior aggregate performance and offering a principled
trade-off between interpretability and predictive power under label
inconsistency. Notably, LVL achieves the best aggregate rank across all
benchmarked backbones and metrics, while LGCL frequently ranks the second,
highlighting the effectiveness of our framework.

</details>


### [272] [Conceptualizing Multi-scale Wavelet Attention and Ray-based Encoding for Human-Object Interaction Detection](https://arxiv.org/abs/2507.10977)
*Quan Bi Pay,Vishnu Monn Baskaran,Junn Yong Loo,KokSheik Wong,Simon See*

Main category: cs.CV

TL;DR: 提出针对HOI检测的小波注意力主干和射线编码器架构，在基准数据集上验证其潜力，代码公开。


<details>
  <summary>Details</summary>
Motivation: 现有HOI检测器难以高效给出可靠预测，依赖资源密集训练方法和低效架构。

Method: 概念化小波注意力主干和射线编码器架构，小波主干聚合特征解决表达问题，射线编码器优化解码器关注区域、减少计算开销。

Result: 在ImageNet和HICO - DET等基准数据集实验展示了架构潜力。

Conclusion: 提出的架构在HOI检测上具有有效性。

Abstract: Human-object interaction (HOI) detection is essential for accurately
localizing and characterizing interactions between humans and objects,
providing a comprehensive understanding of complex visual scenes across various
domains. However, existing HOI detectors often struggle to deliver reliable
predictions efficiently, relying on resource-intensive training methods and
inefficient architectures. To address these challenges, we conceptualize a
wavelet attention-like backbone and a novel ray-based encoder architecture
tailored for HOI detection. Our wavelet backbone addresses the limitations of
expressing middle-order interactions by aggregating discriminative features
from the low- and high-order interactions extracted from diverse convolutional
filters. Concurrently, the ray-based encoder facilitates multi-scale attention
by optimizing the focus of the decoder on relevant regions of interest and
mitigating computational overhead. As a result of harnessing the attenuated
intensity of learnable ray origins, our decoder aligns query embeddings with
emphasized regions of interest for accurate predictions. Experimental results
on benchmark datasets, including ImageNet and HICO-DET, showcase the potential
of our proposed architecture. The code is publicly available at
[https://github.com/henry-pay/RayEncoder].

</details>


### [273] [SpaRTAN: Spatial Reinforcement Token-based Aggregation Network for Visual Recognition](https://arxiv.org/abs/2507.10999)
*Quan Bi Pay,Vishnu Monn Baskaran,Junn Yong Loo,KokSheik Wong,Simon See*

Main category: cs.CV

TL;DR: 提出轻量级架构SpaRTAN，在ImageNet和COCO上实现参数高效且性能有竞争力。


<details>
  <summary>Details</summary>
Motivation: CNN和transformer有简单性偏差，现代CNN集成的MLP块有信息冗余，需高扩展比维持性能，需解决这些局限。

Method: SpaRTAN采用不同感受野的核捕获多阶空间特征，用基于波的通道聚合模块减少通道冗余。

Result: 在ImageNet-1k上3.8M参数达77.7%准确率，1.0 GFLOPs；在COCO上21.5M参数AP达50.0%，超之前基准1.2%。

Conclusion: SpaRTAN设计高效，能在保证性能的同时实现高参数效率。

Abstract: The resurgence of convolutional neural networks (CNNs) in visual recognition
tasks, exemplified by ConvNeXt, has demonstrated their capability to rival
transformer-based architectures through advanced training methodologies and
ViT-inspired design principles. However, both CNNs and transformers exhibit a
simplicity bias, favoring straightforward features over complex structural
representations. Furthermore, modern CNNs often integrate MLP-like blocks akin
to those in transformers, but these blocks suffer from significant information
redundancies, necessitating high expansion ratios to sustain competitive
performance. To address these limitations, we propose SpaRTAN, a lightweight
architectural design that enhances spatial and channel-wise information
processing. SpaRTAN employs kernels with varying receptive fields, controlled
by kernel size and dilation factor, to capture discriminative multi-order
spatial features effectively. A wave-based channel aggregation module further
modulates and reinforces pixel interactions, mitigating channel-wise
redundancies. Combining the two modules, the proposed network can efficiently
gather and dynamically contextualize discriminative features. Experimental
results in ImageNet and COCO demonstrate that SpaRTAN achieves remarkable
parameter efficiency while maintaining competitive performance. In particular,
on the ImageNet-1k benchmark, SpaRTAN achieves 77. 7% accuracy with only 3.8M
parameters and approximately 1.0 GFLOPs, demonstrating its ability to deliver
strong performance through an efficient design. On the COCO benchmark, it
achieves 50.0% AP, surpassing the previous benchmark by 1.2% with only 21.5M
parameters. The code is publicly available at
[https://github.com/henry-pay/SpaRTAN].

</details>


### [274] [Semantically Informed Salient Regions Guided Radiology Report Generation](https://arxiv.org/abs/2507.11015)
*Zeyi Hou,Zeqiang Wei,Ruixin Yan,Ning Lang,Xiuzhuang Zhou*

Main category: cs.CV

TL;DR: 现有胸部X光报告自动生成方法因数据偏差问题存在医学不准确问题，本文提出SISRNet方法可有效解决该问题且表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有胸部X光自动生成报告的深度学习方法因数据偏差常产生医学不准确报告，限制临床应用，需解决该问题。

Method: 提出Semantically Informed Salient Regions-guided (SISRNet) 报告生成方法，利用细粒度跨模态语义识别关键特征显著区域，在图像建模和报告生成时聚焦这些区域。

Result: SISRNet在IU - Xray和MIMIC - CXR数据集上比同类方法表现更优。

Conclusion: SISRNet方法能有效捕捉细微异常发现，减轻数据偏差负面影响，生成临床准确报告。

Abstract: Recent advances in automated radiology report generation from chest X-rays
using deep learning algorithms have the potential to significantly reduce the
arduous workload of radiologists. However, due to the inherent massive data
bias in radiology images, where abnormalities are typically subtle and sparsely
distributed, existing methods often produce fluent yet medically inaccurate
reports, limiting their applicability in clinical practice. To address this
issue effectively, we propose a Semantically Informed Salient Regions-guided
(SISRNet) report generation method. Specifically, our approach explicitly
identifies salient regions with medically critical characteristics using
fine-grained cross-modal semantics. Then, SISRNet systematically focuses on
these high-information regions during both image modeling and report
generation, effectively capturing subtle abnormal findings, mitigating the
negative impact of data bias, and ultimately generating clinically accurate
reports. Compared to its peers, SISRNet demonstrates superior performance on
widely used IU-Xray and MIMIC-CXR datasets.

</details>


### [275] [MMOne: Representing Multiple Modalities in One Scene](https://arxiv.org/abs/2507.11129)
*Zhifeng Gu,Bing Wang*

Main category: cs.CV

TL;DR: 提出通用框架MMOne解决多模态场景表示中的模态冲突问题，实验表明方法提升表示能力且可扩展。


<details>
  <summary>Details</summary>
Motivation: 多模态学习可增强对物理世界的理解，但模态冲突存在属性和粒度差异两个挑战，需要解决。

Method: 提出含新颖模态指示器的模态建模模块，设计多模态分解机制，将多模态信息解缠为共享和特定模态组件。

Result: 实验表明该方法持续提升各模态的表示能力，且可扩展到更多模态。

Conclusion: MMOne框架能有效解决多模态场景表示中的模态冲突问题，获得更紧凑高效的表示。

Abstract: Humans perceive the world through multimodal cues to understand and interact
with the environment. Learning a scene representation for multiple modalities
enhances comprehension of the physical world. However, modality conflicts,
arising from inherent distinctions among different modalities, present two
critical challenges: property disparity and granularity disparity. To address
these challenges, we propose a general framework, MMOne, to represent multiple
modalities in one scene, which can be readily extended to additional
modalities. Specifically, a modality modeling module with a novel modality
indicator is proposed to capture the unique properties of each modality.
Additionally, we design a multimodal decomposition mechanism to separate
multi-modal Gaussians into single-modal Gaussians based on modality
differences. We address the essential distinctions among modalities by
disentangling multimodal information into shared and modality-specific
components, resulting in a more compact and efficient multimodal scene
representation. Extensive experiments demonstrate that our method consistently
enhances the representation capability for each modality and is scalable to
additional modalities. The code is available at
https://github.com/Neal2020GitHub/MMOne.

</details>


### [276] [RMAU-NET: A Residual-Multihead-Attention U-Net Architecture for Landslide Segmentation and Detection from Remote Sensing Images](https://arxiv.org/abs/2507.11143)
*Lam Pham,Cam Le,Hieu Tang,Khang Truong,Truong Nguyen,Jasmin Lampert,Alexander Schindler,Martin Boyer,Son Phan*

Main category: cs.CV

TL;DR: 本文提出基于深度学习的端到端模型，利用遥感图像自动观测滑坡事件，在多个数据集上实验效果良好，有应用于实际系统潜力。


<details>
  <summary>Details</summary>
Motivation: 近年来滑坡灾害频发，但自动观测因观测区域大、地形复杂而具有挑战性，因此提出模型。

Method: 提出一种用于滑坡检测和分割两个任务的新型神经网络架构，以遥感图像为输入数据。

Result: 在LandSlide4Sense、Bijie数据集的滑坡检测任务中F1分数分别达98.23、93.83；在LandSlide4Sense、Nepal数据集的分割任务中mIoU分数分别为63.74、76.88。

Conclusion: 提出的模型有潜力集成到实际的滑坡观测系统中。

Abstract: In recent years, landslide disasters have reported frequently due to the
extreme weather events of droughts, floods , storms, or the consequence of
human activities such as deforestation, excessive exploitation of natural
resources. However, automatically observing landslide is challenging due to the
extremely large observing area and the rugged topography such as mountain or
highland. This motivates us to propose an end-to-end deep-learning-based model
which explores the remote sensing images for automatically observing landslide
events. By considering remote sensing images as the input data, we can obtain
free resource, observe large and rough terrains by time. To explore the remote
sensing images, we proposed a novel neural network architecture which is for
two tasks of landslide detection and landslide segmentation. We evaluated our
proposed model on three different benchmark datasets of LandSlide4Sense, Bijie,
and Nepal. By conducting extensive experiments, we achieve F1 scores of 98.23,
93.83 for the landslide detection task on LandSlide4Sense, Bijie datasets; mIoU
scores of 63.74, 76.88 on the segmentation tasks regarding LandSlide4Sense,
Nepal datasets. These experimental results prove potential to integrate our
proposed model into real-life landslide observation systems.

</details>


### [277] [Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with Regularized Score Distillation Sampling](https://arxiv.org/abs/2507.11061)
*Hayeon Kim,Ji Ha Jang,Se Young Chun*

Main category: cs.CV

TL;DR: 本文提出RoMaP框架解决3D高斯局部编辑难题，实验显示其达到了先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有3D编辑在高斯Splatting中实现精确局部编辑存在挑战，因多视图2D分割不一致和SDS损失的模糊性。

Method: 提出RoMaP框架，包括3D-GALP模块用于生成鲁棒3D掩码，以及结合标准SDS损失和额外正则化项的正则化SDS损失。

Result: RoMaP在重建和生成的高斯场景及对象上实现了先进的局部3D编辑。

Conclusion: RoMaP使3D高斯部件级编辑更鲁棒和灵活。

Abstract: Recent advances in 3D neural representations and instance-level editing
models have enabled the efficient creation of high-quality 3D content. However,
achieving precise local 3D edits remains challenging, especially for Gaussian
Splatting, due to inconsistent multi-view 2D part segmentations and inherently
ambiguous nature of Score Distillation Sampling (SDS) loss. To address these
limitations, we propose RoMaP, a novel local 3D Gaussian editing framework that
enables precise and drastic part-level modifications. First, we introduce a
robust 3D mask generation module with our 3D-Geometry Aware Label Prediction
(3D-GALP), which uses spherical harmonics (SH) coefficients to model
view-dependent label variations and soft-label property, yielding accurate and
consistent part segmentations across viewpoints. Second, we propose a
regularized SDS loss that combines the standard SDS loss with additional
regularizers. In particular, an L1 anchor loss is introduced via our Scheduled
Latent Mixing and Part (SLaMP) editing method, which generates high-quality
part-edited 2D images and confines modifications only to the target region
while preserving contextual coherence. Additional regularizers, such as
Gaussian prior removal, further improve flexibility by allowing changes beyond
the existing context, and robust 3D masking prevents unintended edits.
Experimental results demonstrate that our RoMaP achieves state-of-the-art local
3D editing on both reconstructed and generated Gaussian scenes and objects
qualitatively and quantitatively, making it possible for more robust and
flexible part-level 3D Gaussian editing.

</details>


### [278] [Joint angle model based learning to refine kinematic human pose estimation](https://arxiv.org/abs/2507.11075)
*Chang Peng,Yifei Zhou,Huifeng Xi,Shiqing Huang,Chuangye Chen,Jianming Yang,Bao Yang,Zhenyu Jiang*

Main category: cs.CV

TL;DR: 本文提出基于关节角度建模的无标记人体姿态估计（HPE）改进方法，训练后网络表现出色，测试显示其在挑战性场景中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前HPE在关键点识别和轨迹分析时有误差，现有深度学习模型受不准确训练数据集限制。

Method: 提出基于关节角度的人体姿态模型，用高阶傅里叶级数近似关节角度的时间变化以获取可靠“真值”，设计双向循环网络作为后处理模块改进HRNet估计。

Result: 使用该方法构建的高质量数据集训练的网络能纠正错误识别的关节并平滑时空轨迹。

Conclusion: 关节角度改进（JAR）方法在挑战性场景中优于现有HPE改进网络。

Abstract: Marker-free human pose estimation (HPE) has found increasing applications in
various fields. Current HPE suffers from occasional errors in keypoint
recognition and random fluctuation in keypoint trajectories when analyzing
kinematic human poses. The performance of existing deep learning-based models
for HPE refinement is considerably limited by inaccurate training datasets in
which the keypoints are manually annotated. This paper proposed a novel method
to overcome the difficulty through joint angle-based modeling. The key
techniques include: (i) A joint angle-based model of human pose, which is
robust to describe kinematic human poses; (ii) Approximating temporal variation
of joint angles through high order Fourier series to get reliable "ground
truth"; (iii) A bidirectional recurrent network is designed as a
post-processing module to refine the estimation of well-established HRNet.
Trained with the high-quality dataset constructed using our method, the network
demonstrates outstanding performance to correct wrongly recognized joints and
smooth their spatiotemporal trajectories. Tests show that joint angle-based
refinement (JAR) outperforms the state-of-the-art HPE refinement network in
challenging cases like figure skating and breaking.

</details>


### [279] [Automatic Road Subsurface Distress Recognition from Ground Penetrating Radar Images using Deep Learning-based Cross-verification](https://arxiv.org/abs/2507.11081)
*Chang Peng,Bao Yang,Meiqi Li,Ge Zhang,Hui Sun,Zhenyu Jiang*

Main category: cs.CV

TL;DR: 本文构建3D GPR数据集，提出交叉验证策略用于道路地下病害识别，集成到在线检测系统可减少大量人力。


<details>
  <summary>Details</summary>
Motivation: 当前从GPR图像识别道路地下病害劳动强度大且依赖专家经验，深度学习受数据集质量和网络识别能力限制。

Method: 构建含2134个多样样本的3D GPR数据集，基于YOLO模型对不同病害敏感度不同提出交叉验证策略。

Result: 现场测试召回率超98.6%，集成到在线系统可减少约90%检查人力。

Conclusion: 所提方法在道路地下病害识别中准确且高效，能大幅降低人力成本。

Abstract: Ground penetrating radar (GPR) has become a rapid and non-destructive
solution for road subsurface distress (RSD) detection. However, RSD recognition
from GPR images is labor-intensive and heavily relies on inspectors' expertise.
Deep learning offers the possibility for automatic RSD recognition, but its
current performance is limited by two factors: Scarcity of high-quality dataset
for network training and insufficient capability of network to distinguish RSD.
In this study, a rigorously validated 3D GPR dataset containing 2134 samples of
diverse types was constructed through field scanning. Based on the finding that
the YOLO model trained with one of the three scans of GPR images exhibits
varying sensitivity to specific type of RSD, we proposed a novel
cross-verification strategy with outstanding accuracy in RSD recognition,
achieving recall over 98.6% in field tests. The approach, integrated into an
online RSD detection system, can reduce the labor of inspection by around 90%.

</details>


### [280] [A Robust Incomplete Multimodal Low-Rank Adaptation Approach for Emotion Recognition](https://arxiv.org/abs/2507.11202)
*Xinkui Zhao,Jinsong Shu,Yangyang Wu,Guanjie Cheng,Zihe Liu,Naibo Wang,Shuiguang Deng,Zhongle Xie,Jianwei Yin*

Main category: cs.CV

TL;DR: 提出基于模态组合的单模态解耦动态低秩适应方法MCULoRA用于不完整多模态学习模型的参数高效训练，实验显示其性能优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有不完整多模态情感识别方法因不同模态组合训练梯度冲突，导致最终预测模型性能下降。

Method: 提出MCULoRA框架，包含模态组合感知低秩适应（MCLA）和动态参数微调（DPFT）两个关键模块。MCLA模块解耦共享信息与独特特征，DPFT模块根据模态表示空间可分性调整训练比例。

Result: 在多个基准数据集的实验中，MCULoRA在下游任务准确性上大幅超越以往不完整多模态学习方法。

Conclusion: MCULoRA是一种有效的不完整多模态学习模型参数高效训练方法。

Abstract: Multimodal Emotion Recognition (MER) often encounters incomplete
multimodality in practical applications due to sensor failures or privacy
protection requirements. While existing methods attempt to address various
incomplete multimodal scenarios by balancing the training of each modality
combination through additional gradients, these approaches face a critical
limitation: training gradients from different modality combinations conflict
with each other, ultimately degrading the performance of the final prediction
model. In this paper, we propose a unimodal decoupled dynamic low-rank
adaptation method based on modality combinations, named MCULoRA, which is a
novel framework for the parameter-efficient training of incomplete multimodal
learning models. MCULoRA consists of two key modules, modality combination
aware low-rank adaptation (MCLA) and dynamic parameter fine-tuning (DPFT). The
MCLA module effectively decouples the shared information from the distinct
characteristics of individual modality combinations. The DPFT module adjusts
the training ratio of modality combinations based on the separability of each
modality's representation space, optimizing the learning efficiency across
different modality combinations. Our extensive experimental evaluation in
multiple benchmark datasets demonstrates that MCULoRA substantially outperforms
previous incomplete multimodal learning approaches in downstream task accuracy.

</details>


### [281] [Fairness-Aware Grouping for Continuous Sensitive Variables: Application for Debiasing Face Analysis with respect to Skin Tone](https://arxiv.org/abs/2507.11247)
*Veronika Shilova,Emmanuel Malherbe,Giovanni Palma,Laurent Risser,Jean-Michel Loubes*

Main category: cs.CV

TL;DR: 提出连续敏感属性的公平分组方法，用合成数据集验证，在CelebA和FFHQ上实证，还用于去偏，能提升公平性且对准确性影响小。


<details>
  <summary>Details</summary>
Motivation: 传统按预设组评估公平性的方法在敏感属性连续时，可能忽略少数群体的歧视情况，需改进。

Method: 提出基于公平性的连续敏感属性分组方法，按观察到的歧视水平分组，找出使组间歧视方差最大的划分。

Result: 用合成数据集验证了方法的鲁棒性；在CelebA和FFHQ上发现更细微的歧视模式；分组模型用于去偏时能提升公平性且对准确性影响小。

Conclusion: 所提分区方法有效，为工业部署提供可能。

Abstract: Within a legal framework, fairness in datasets and models is typically
assessed by dividing observations into predefined groups and then computing
fairness measures (e.g., Disparate Impact or Equality of Odds with respect to
gender). However, when sensitive attributes such as skin color are continuous,
dividing into default groups may overlook or obscure the discrimination
experienced by certain minority subpopulations. To address this limitation, we
propose a fairness-based grouping approach for continuous (possibly
multidimensional) sensitive attributes. By grouping data according to observed
levels of discrimination, our method identifies the partition that maximizes a
novel criterion based on inter-group variance in discrimination, thereby
isolating the most critical subgroups.
  We validate the proposed approach using multiple synthetic datasets and
demonstrate its robustness under changing population distributions - revealing
how discrimination is manifested within the space of sensitive attributes.
Furthermore, we examine a specialized setting of monotonic fairness for the
case of skin color. Our empirical results on both CelebA and FFHQ, leveraging
the skin tone as predicted by an industrial proprietary algorithm, show that
the proposed segmentation uncovers more nuanced patterns of discrimination than
previously reported, and that these findings remain stable across datasets for
a given model. Finally, we leverage our grouping model for debiasing purpose,
aiming at predicting fair scores with group-by-group post-processing. The
results demonstrate that our approach improves fairness while having minimal
impact on accuracy, thus confirming our partition method and opening the door
for industrial deployment.

</details>


### [282] [Assessing Color Vision Test in Large Vision-language Models](https://arxiv.org/abs/2507.11153)
*Hongfei Ye,Bin Chen,Wenxi Liu,Yu Zhang,Zhao Li,Dandan Ni,Hongyang Chen*

Main category: cs.CV

TL;DR: 本文针对大视觉语言模型色觉能力未充分探索的问题，定义色觉测试任务、构建数据集，分析错误类型并提出微调策略。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型被广泛应用，但其色觉能力尚未得到充分探索。

Method: 定义大视觉语言模型色觉测试任务，构建涵盖多类别测试问题和不同难度级别的数据集，分析模型错误类型并提出微调策略。

Result: 文档未提及具体结果。

Conclusion: 文档未提及明确结论。

Abstract: With the widespread adoption of large vision-language models, the capacity
for color vision in these models is crucial. However, the color vision
abilities of large visual-language models have not yet been thoroughly
explored. To address this gap, we define a color vision testing task for large
vision-language models and construct a dataset \footnote{Anonymous Github
Showing some of the data
https://anonymous.4open.science/r/color-vision-test-dataset-3BCD} that covers
multiple categories of test questions and tasks of varying difficulty levels.
Furthermore, we analyze the types of errors made by large vision-language
models and propose fine-tuning strategies to enhance their performance in color
vision tests.

</details>


### [283] [YOLOatr : Deep Learning Based Automatic Target Detection and Localization in Thermal Infrared Imagery](https://arxiv.org/abs/2507.11267)
*Aon Safdar,Usman Akram,Waseem Anwar,Basit Malik,Mian Ibad Ali*

Main category: cs.CV

TL;DR: 针对热红外图像自动目标检测与识别难题，提出改进的单阶段检测器YOLOatr，在数据集上测试取得99.6%的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 热红外图像自动目标检测与识别在国防和监控领域挑战大，现有深度学习架构表现不佳。

Method: 基于改进的YOLOv5s提出YOLOatr，对检测头、颈部特征融合和数据增强进行优化。

Result: 在综合DSIAC MWIR数据集上测试，模型实现了高达99.6%的实时ATR性能。

Conclusion: 所提出的YOLOatr模型在热红外图像自动目标检测与识别任务中达到了最先进水平。

Abstract: Automatic Target Detection (ATD) and Recognition (ATR) from Thermal Infrared
(TI) imagery in the defense and surveillance domain is a challenging computer
vision (CV) task in comparison to the commercial autonomous vehicle perception
domain. Limited datasets, peculiar domain-specific and TI modality-specific
challenges, i.e., limited hardware, scale invariance issues due to greater
distances, deliberate occlusion by tactical vehicles, lower sensor resolution
and resultant lack of structural information in targets, effects of weather,
temperature, and time of day variations, and varying target to clutter ratios
all result in increased intra-class variability and higher inter-class
similarity, making accurate real-time ATR a challenging CV task. Resultantly,
contemporary state-of-the-art (SOTA) deep learning architectures underperform
in the ATR domain. We propose a modified anchor-based single-stage detector,
called YOLOatr, based on a modified YOLOv5s, with optimal modifications to the
detection heads, feature fusion in the neck, and a custom augmentation profile.
We evaluate the performance of our proposed model on a comprehensive DSIAC MWIR
dataset for real-time ATR over both correlated and decorrelated testing
protocols. The results demonstrate that our proposed model achieves
state-of-the-art ATR performance of up to 99.6%.

</details>


### [284] [Implementing Adaptations for Vision AutoRegressive Model](https://arxiv.org/abs/2507.11441)
*Kaif Shaikh,Antoni Kowalczuk,Franziska Boenisch,Adam Dziedzic*

Main category: cs.CV

TL;DR: 研究视觉自回归模型（VAR）在图像生成领域的适应策略并与扩散模型（DM）对比，发现VAR非DP适应表现优于DM，但DP适应性能欠佳。


<details>
  <summary>Details</summary>
Motivation: VAR在适应策略和差分隐私（DP）适应方面研究不足，而DM已有较多相关技术，需对VAR进行研究。

Method: 为VAR实现并基准测试多种策略，与DM的适应策略进行比较。

Result: VAR在非DP适应方面优于DM，但DP适应性能较差。

Conclusion: VAR的私有适应需要进一步研究。

Abstract: Vision AutoRegressive model (VAR) was recently introduced as an alternative
to Diffusion Models (DMs) in image generation domain. In this work we focus on
its adaptations, which aim to fine-tune pre-trained models to perform specific
downstream tasks, like medical data generation. While for DMs there exist many
techniques, adaptations for VAR remain underexplored. Similarly, differentially
private (DP) adaptations-ones that aim to preserve privacy of the adaptation
data-have been extensively studied for DMs, while VAR lacks such solutions. In
our work, we implement and benchmark many strategies for VAR, and compare them
to state-of-the-art DM adaptation strategies. We observe that VAR outperforms
DMs for non-DP adaptations, however, the performance of DP suffers, which
necessitates further research in private adaptations for VAR. Code is available
at https://github.com/sprintml/finetuning_var_dp.

</details>


### [285] [CATVis: Context-Aware Thought Visualization](https://arxiv.org/abs/2507.11522)
*Tariq Mehmood,Hamza Ahmad,Muhammad Haroon Shakeel,Murtaza Taj*

Main category: cs.CV

TL;DR: 提出新颖5阶段框架从EEG信号解码视觉表征，实现上下文感知EEG到图像生成，实验结果优于SOTA方法。


<details>
  <summary>Details</summary>
Motivation: EEG信号复杂且嘈杂，从EEG信号解码视觉表征是重大挑战。

Method: 提出5阶段框架，包括EEG编码器、跨模态对齐、字幕细化、嵌入插值和图像生成。

Result: 生成与视觉刺激对齐的高质量图像，分类准确率、生成准确率优于SOTA方法，Fréchet Inception距离降低。

Conclusion: 该方法具有优越的语义对齐和图像质量。

Abstract: EEG-based brain-computer interfaces (BCIs) have shown promise in various
applications, such as motor imagery and cognitive state monitoring. However,
decoding visual representations from EEG signals remains a significant
challenge due to their complex and noisy nature. We thus propose a novel
5-stage framework for decoding visual representations from EEG signals: (1) an
EEG encoder for concept classification, (2) cross-modal alignment of EEG and
text embeddings in CLIP feature space, (3) caption refinement via re-ranking,
(4) weighted interpolation of concept and caption embeddings for richer
semantics, and (5) image generation using a pre-trained Stable Diffusion model.
We enable context-aware EEG-to-image generation through cross-modal alignment
and re-ranking. Experimental results demonstrate that our method generates
high-quality images aligned with visual stimuli, outperforming SOTA approaches
by 13.43% in Classification Accuracy, 15.21% in Generation Accuracy and
reducing Fr\'echet Inception Distance by 36.61%, indicating superior semantic
alignment and image quality.

</details>


### [286] [Streaming 4D Visual Geometry Transformer](https://arxiv.org/abs/2507.11539)
*Dong Zhuo,Wenzhao Zheng,Jiahe Guo,Yuqi Wu,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 提出流式4D视觉几何变换器用于视频4D时空几何感知与重建，提升在线推理速度，代码开源。


<details>
  <summary>Details</summary>
Motivation: 为促进交互式和实时应用，解决从视频感知和重建4D时空几何这一具有挑战性的计算机视觉任务。

Method: 采用类似自回归大语言模型的设计，用因果变换器架构在线处理输入序列，利用时间因果注意力和缓存历史键值，从密集双向视觉几何基础变换器蒸馏知识，推理时支持迁移优化高效注意力算子。

Result: 在多个4D几何感知基准测试中，模型提高了在线场景推理速度，且性能有竞争力。

Conclusion: 为可扩展和交互式4D视觉系统奠定基础。

Abstract: Perceiving and reconstructing 4D spatial-temporal geometry from videos is a
fundamental yet challenging computer vision task. To facilitate interactive and
real-time applications, we propose a streaming 4D visual geometry transformer
that shares a similar philosophy with autoregressive large language models. We
explore a simple and efficient design and employ a causal transformer
architecture to process the input sequence in an online manner. We use temporal
causal attention and cache the historical keys and values as implicit memory to
enable efficient streaming long-term 4D reconstruction. This design can handle
real-time 4D reconstruction by incrementally integrating historical information
while maintaining high-quality spatial consistency. For efficient training, we
propose to distill knowledge from the dense bidirectional visual geometry
grounded transformer (VGGT) to our causal model. For inference, our model
supports the migration of optimized efficient attention operator (e.g.,
FlashAttention) from the field of large language models. Extensive experiments
on various 4D geometry perception benchmarks demonstrate that our model
increases the inference speed in online scenarios while maintaining competitive
performance, paving the way for scalable and interactive 4D vision systems.
Code is available at: https://github.com/wzzheng/StreamVGGT.

</details>


### [287] [Attributes Shape the Embedding Space of Face Recognition Models](https://arxiv.org/abs/2507.11372)
*Pierrick Leroy,Antonio Mastropietro,Marco Nurisso,Francesco Vaccarino*

Main category: cs.CV

TL;DR: 本文提出几何方法和物理启发的对齐度量评估人脸识别模型对属性的依赖或不变性，揭示模型在不同属性上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于对比损失的人脸识别训练仅关注身份信息，而嵌入空间存在受面部和图像属性影响的多尺度几何结构，需评估模型对这些属性的依赖或不变性。

Method: 提出几何方法描述模型对属性的依赖或不变性，引入物理启发的对齐度量，在受控简化模型和经合成数据微调的常用人脸识别模型上进行评估。

Result: 模型在不同属性上展现出不同程度的不变性。

Conclusion: 该研究有助于深入了解人脸识别模型的优缺点，增强模型可解释性。

Abstract: Face Recognition (FR) tasks have made significant progress with the advent of
Deep Neural Networks, particularly through margin-based triplet losses that
embed facial images into high-dimensional feature spaces. During training,
these contrastive losses focus exclusively on identity information as labels.
However, we observe a multiscale geometric structure emerging in the embedding
space, influenced by interpretable facial (e.g., hair color) and image
attributes (e.g., contrast). We propose a geometric approach to describe the
dependence or invariance of FR models to these attributes and introduce a
physics-inspired alignment metric. We evaluate the proposed metric on
controlled, simplified models and widely used FR models fine-tuned with
synthetic data for targeted attribute augmentation. Our findings reveal that
the models exhibit varying degrees of invariance across different attributes,
providing insight into their strengths and weaknesses and enabling deeper
interpretability. Code available here:
https://github.com/mantonios107/attrs-fr-embs}{https://github.com/mantonios107/attrs-fr-embs

</details>


### [288] [COLI: A Hierarchical Efficient Compressor for Large Images](https://arxiv.org/abs/2507.11443)
*Haoran Wang,Hanyu Pei,Yang Lyu,Kai Zhang,Li Li,Feng-Lei Fan*

Main category: cs.CV

TL;DR: 高分辨率大视野图像需要高效压缩方法，传统和数据驱动方法有局限，提出COLI框架，加速训练并提高压缩比，在医学图像数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有高分辨率大视野图像压缩方法有局限，传统技术难保留细节，数据驱动方法泛化性差，INR压缩大图像存在速度慢和压缩比不佳问题。

Method: 引入基于NeRV的COLI框架，通过预训练 - 微调范式、混合精度训练和重写损失函数加速训练，使用超压缩技术提高压缩比。

Result: 在两个医学成像数据集上，COLI在降低比特率的同时，PSNR和SSIM指标有竞争力或更优，加速NeRV训练达4倍。

Conclusion: COLI框架有效解决了INR压缩大图像的局限，能实现高效图像压缩。

Abstract: The escalating adoption of high-resolution, large-field-of-view imagery
amplifies the need for efficient compression methodologies. Conventional
techniques frequently fail to preserve critical image details, while
data-driven approaches exhibit limited generalizability. Implicit Neural
Representations (INRs) present a promising alternative by learning continuous
mappings from spatial coordinates to pixel intensities for individual images,
thereby storing network weights rather than raw pixels and avoiding the
generalization problem. However, INR-based compression of large images faces
challenges including slow compression speed and suboptimal compression ratios.
To address these limitations, we introduce COLI (Compressor for Large Images),
a novel framework leveraging Neural Representations for Videos (NeRV). First,
recognizing that INR-based compression constitutes a training process, we
accelerate its convergence through a pretraining-finetuning paradigm,
mixed-precision training, and reformulation of the sequential loss into a
parallelizable objective. Second, capitalizing on INRs' transformation of image
storage constraints into weight storage, we implement Hyper-Compression, a
novel post-training technique to substantially enhance compression ratios while
maintaining minimal output distortion. Evaluations across two medical imaging
datasets demonstrate that COLI consistently achieves competitive or superior
PSNR and SSIM metrics at significantly reduced bits per pixel (bpp), while
accelerating NeRV training by up to 4 times.

</details>


### [289] [COLIBRI Fuzzy Model: Color Linguistic-Based Representation and Interpretation](https://arxiv.org/abs/2507.11488)
*Pakizar Shamoi,Nuray Toganas,Muragul Muratbekova,Elnara Kadyrgali,Adilet Yerkin,Ayan Igali,Malika Ziyada,Ayana Adilova,Aron Karatayev,Yerdauit Torekhan*

Main category: cs.CV

TL;DR: 本文提出基于人类感知的模糊颜色模型COLIBRI，通过实验构建模型，对比显示其更符合人类感知，研究成果对多领域有重要意义。


<details>
  <summary>Details</summary>
Motivation: 计算机难以模仿人类颜色感知，旨在缩小计算颜色表示与人类视觉感知的差距。

Method: 采用三阶段实验方法，先识别颜色刺激，再进行大规模人类分类调查，根据数据提取模糊分区和生成隶属函数，模型包含自适应机制。

Result: 与传统颜色模型相比，该模型更符合人类感知，此前无同规模样本研究。

Conclusion: 研究成果对设计、人工智能、营销和人机交互等领域有重要意义。

Abstract: Colors are omnipresent in today's world and play a vital role in how humans
perceive and interact with their surroundings. However, it is challenging for
computers to imitate human color perception. This paper introduces the Human
Perception-Based Fuzzy Color Model, COLIBRI (Color Linguistic-Based
Representation and Interpretation), designed to bridge the gap between
computational color representations and human visual perception. The proposed
model uses fuzzy sets and logic to create a framework for color categorization.
Using a three-phase experimental approach, the study first identifies
distinguishable color stimuli for hue, saturation, and intensity through
preliminary experiments, followed by a large-scale human categorization survey
involving more than 1000 human subjects. The resulting data are used to extract
fuzzy partitions and generate membership functions that reflect real-world
perceptual uncertainty. The model incorporates a mechanism for adaptation that
allows refinement based on feedback and contextual changes. Comparative
evaluations demonstrate the model's alignment with human perception compared to
traditional color models, such as RGB, HSV, and LAB. To the best of our
knowledge, no previous research has documented the construction of a model for
color attribute specification based on a sample of this size or a comparable
sample of the human population (n = 2496). Our findings are significant for
fields such as design, artificial intelligence, marketing, and human-computer
interaction, where perceptually relevant color representation is critical.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [290] [Rapid Mixing of Glauber Dynamics for Monotone Systems via Entropic Independence](https://arxiv.org/abs/2507.11031)
*Weiming Feng,Minji Yang*

Main category: cs.DM

TL;DR: 研究单调系统上Glauber动力学的混合时间，证明新比较结果并改进具体模型已知结果。


<details>
  <summary>Details</summary>
Motivation: 研究单调系统上Glauber动力学的混合时间，改进已知结果。

Method: 结合经典删失不等式中的随机支配论证和高维扩张器的思想，证明Glauber动力学和场动力学的比较结果。

Result: 得到铁磁Ising模型诱导的随机簇模型混合时间为$	ilde{O}(n)$，二部硬核模型在单边唯一性条件下混合时间为$	ilde{O}(n^2)$，改进已知结果。

Conclusion: 所提出的方法有效，能得到更好的混合时间结果。

Abstract: We study the mixing time of Glauber dynamics on monotone systems. For
monotone systems satisfying the entropic independence condition, we prove a new
mixing time comparison result for Glauber dynamics. For concrete applications,
we obtain $\tilde{O}(n)$ mixing time for the random cluster model induced by
the ferromagnetic Ising model with consistently biased external fields, and
$\tilde{O}(n^2)$ mixing time for the bipartite hardcore model under the
one-sided uniqueness condition, where $n$ is the number of variables in
corresponding models, improving the best known results in [Chen and Zhang,
SODA'23] and [Chen, Liu, and Yin, FOCS'23], respectively.
  Our proof combines ideas from the stochastic dominance argument in the
classical censoring inequality and the recently developed high-dimensional
expanders. The key step in the proof is a novel comparison result between the
Glauber dynamics and the field dynamics for monotone systems.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [291] [Comparative Analysis of Vision Transformers and Traditional Deep Learning Approaches for Automated Pneumonia Detection in Chest X-Rays](https://arxiv.org/abs/2507.10589)
*Gaurav Singh*

Main category: eess.IV

TL;DR: 研究对比传统机器学习与深度学习方法在胸部X光片自动检测肺炎上的表现，发现Vision Transformers尤其是Cross - ViT架构效果更佳，为自动检测肺炎提供新方向。


<details>
  <summary>Details</summary>
Motivation: 肺炎尤其是由COVID - 19等疾病引发的肺炎是全球健康挑战，需要快速准确诊断，故对比不同方法用于自动检测肺炎。

Method: 评估从传统机器学习技术（如基于PCA的聚类、逻辑回归、支持向量分类）到先进深度学习架构（如改进LeNet、DenseNet - 121等CNN架构和多种Vision Transformer实现）。使用5856张儿科胸部X光片数据集。

Result: Vision Transformers尤其是Cross - ViT架构表现优越，准确率达88.25%，召回率达99.42%，超过传统CNN方法；架构选择比模型大小对性能影响更大，Cross - ViT 75M参数优于更大模型。

Conclusion: Vision Transformers为自动检测肺炎提供了有前景的方向，可能实现健康危机时更快速准确的诊断。

Abstract: Pneumonia, particularly when induced by diseases like COVID-19, remains a
critical global health challenge requiring rapid and accurate diagnosis. This
study presents a comprehensive comparison of traditional machine learning and
state-of-the-art deep learning approaches for automated pneumonia detection
using chest X-rays (CXRs). We evaluate multiple methodologies, ranging from
conventional machine learning techniques (PCA-based clustering, Logistic
Regression, and Support Vector Classification) to advanced deep learning
architectures including Convolutional Neural Networks (Modified LeNet,
DenseNet-121) and various Vision Transformer (ViT) implementations (Deep-ViT,
Compact Convolutional Transformer, and Cross-ViT). Using a dataset of 5,856
pediatric CXR images, we demonstrate that Vision Transformers, particularly the
Cross-ViT architecture, achieve superior performance with 88.25% accuracy and
99.42% recall, surpassing traditional CNN approaches. Our analysis reveals that
architectural choices impact performance more significantly than model size,
with Cross-ViT's 75M parameters outperforming larger models. The study also
addresses practical considerations including computational efficiency, training
requirements, and the critical balance between precision and recall in medical
diagnostics. Our findings suggest that Vision Transformers offer a promising
direction for automated pneumonia detection, potentially enabling more rapid
and accurate diagnosis during health crises.

</details>


### [292] [Latent Space Consistency for Sparse-View CT Reconstruction](https://arxiv.org/abs/2507.11152)
*Duoyou Chen,Yunqing Chen,Can Zhang,Zhou Wang,Cheng Chen,Ruoxiu Xiao*

Main category: eess.IV

TL;DR: 提出CLS - DM模型解决CT重建问题，实验表明其性能优越，代码开源。


<details>
  <summary>Details</summary>
Motivation: CT存在耗时久、辐射大问题，基于稀疏视图X射线图像的CT重建方法受关注，但现有扩散模型无法有效对齐潜在空间。

Method: 提出Consistent Latent Space Diffusion Model (CLS - DM)，引入跨模态特征对比学习，从2D X射线图像提取3D潜在信息并实现模态间潜在空间对齐。

Result: 在LIDC - IDRI和CTSpine1K数据集上，CLS - DM在标准体素级指标（PSNR、SSIM）上优于经典和最先进的生成模型。

Conclusion: 该方法提高了稀疏X射线重建CT的有效性和经济性，可推广到其他跨模态转换任务，代码已开源。

Abstract: Computed Tomography (CT) is a widely utilized imaging modality in clinical
settings. Using densely acquired rotational X-ray arrays, CT can capture 3D
spatial features. However, it is confronted with challenged such as significant
time consumption and high radiation exposure. CT reconstruction methods based
on sparse-view X-ray images have garnered substantial attention from
researchers as they present a means to mitigate costs and risks. In recent
years, diffusion models, particularly the Latent Diffusion Model (LDM), have
demonstrated promising potential in the domain of 3D CT reconstruction.
Nonetheless, due to the substantial differences between the 2D latent
representation of X-ray modalities and the 3D latent representation of CT
modalities, the vanilla LDM is incapable of achieving effective alignment
within the latent space. To address this issue, we propose the Consistent
Latent Space Diffusion Model (CLS-DM), which incorporates cross-modal feature
contrastive learning to efficiently extract latent 3D information from 2D X-ray
images and achieve latent space alignment between modalities. Experimental
results indicate that CLS-DM outperforms classical and state-of-the-art
generative models in terms of standard voxel-level metrics (PSNR, SSIM) on the
LIDC-IDRI and CTSpine1K datasets. This methodology not only aids in enhancing
the effectiveness and economic viability of sparse X-ray reconstructed CT but
can also be generalized to other cross-modal transformation tasks, such as
text-to-image synthesis. We have made our code publicly available at
https://anonymous.4open.science/r/CLS-DM-50D6/ to facilitate further research
and applications in other domains.

</details>


### [293] [HANS-Net: Hyperbolic Convolution and Adaptive Temporal Attention for Accurate and Generalizable Liver and Tumor Segmentation in CT Imaging](https://arxiv.org/abs/2507.11325)
*Arefin Ittesafun Abian,Ripon Kumar Debnath,Md. Abdur Rahman,Mohaimenul Azam Khan Raiaan,Md Rafiqul Islam,Asif Karim,Reem E. Mohamed,Sami Azam*

Main category: eess.IV

TL;DR: 提出HANS - Net进行腹部CT图像肝脏和肿瘤分割，评估显示其有效且健壮。


<details>
  <summary>Details</summary>
Motivation: 腹部CT图像肝脏和肿瘤准确分割因解剖结构复杂、肿瘤外观多变和标注数据有限而具挑战。

Method: 提出HANS - Net，结合双曲卷积、小波分解模块、突触可塑性机制、隐式神经表示分支，融入不确定性感知蒙特卡罗丢弃和轻量级时间注意力。

Result: 在LiTS数据集上有良好指标，交叉数据集验证在3D - IRCADb - 01数据集也表现出色。

Conclusion: HANS - Net在肝脏和肿瘤分割中有效且健壮。

Abstract: Accurate liver and tumor segmentation on abdominal CT images is critical for
reliable diagnosis and treatment planning, but remains challenging due to
complex anatomical structures, variability in tumor appearance, and limited
annotated data. To address these issues, we introduce Hyperbolic-convolutions
Adaptive-temporal-attention with Neural-representation and Synaptic-plasticity
Network (HANS-Net), a novel segmentation framework that synergistically
combines hyperbolic convolutions for hierarchical geometric representation, a
wavelet-inspired decomposition module for multi-scale texture learning, a
biologically motivated synaptic plasticity mechanism for adaptive feature
enhancement, and an implicit neural representation branch to model fine-grained
and continuous anatomical boundaries. Additionally, we incorporate
uncertainty-aware Monte Carlo dropout to quantify prediction confidence and
lightweight temporal attention to improve inter-slice consistency without
sacrificing efficiency. Extensive evaluations of the LiTS dataset demonstrate
that HANS-Net achieves a mean Dice score of 93.26%, an IoU of 88.09%, an
average symmetric surface distance (ASSD) of 0.72 mm, and a volume overlap
error (VOE) of 11.91%. Furthermore, cross-dataset validation on the
3D-IRCADb-01 dataset obtains an average Dice of 87.45%, IoU of 80.30%, ASSD of
1.525 mm, and VOE of 19.71%, indicating strong generalization across different
datasets. These results confirm the effectiveness and robustness of HANS-Net in
providing anatomically consistent, accurate, and confident liver and tumor
segmentation.

</details>


### [294] [U-RWKV: Lightweight medical image segmentation with direction-adaptive RWKV](https://arxiv.org/abs/2507.11415)
*Hongbo Ye,Fenghe Tang,Peiang Zhao,Zhen Huang,Dexin Zhao,Minghao Bian,S. Kevin Zhou*

Main category: eess.IV

TL;DR: 提出U - RWKV框架用于医疗图像分割，有DARM和SASE模块，实验显示其效率高且性能达最优，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有医疗图像分割方法（如U - Net及其变体）全局有效感受野有限，难以捕捉长距离依赖，需轻量级高性能解决方案以实现医疗可及性公平。

Method: 提出U - RWKV框架，引入Direction - Adaptive RWKV Module (DARM) 和Stage - Adaptive Squeeze - and - Excitation Module (SASE)。DARM用Dual - RWKV和QuadScan机制聚合上下文信息；SASE动态适配不同特征提取阶段。

Result: U - RWKV实现了最先进的分割性能，计算效率高。

Conclusion: U - RWKV为资源受限环境下普及先进医学成像技术提供了实用解决方案。

Abstract: Achieving equity in healthcare accessibility requires lightweight yet
high-performance solutions for medical image segmentation, particularly in
resource-limited settings. Existing methods like U-Net and its variants often
suffer from limited global Effective Receptive Fields (ERFs), hindering their
ability to capture long-range dependencies. To address this, we propose U-RWKV,
a novel framework leveraging the Recurrent Weighted Key-Value(RWKV)
architecture, which achieves efficient long-range modeling at O(N)
computational cost. The framework introduces two key innovations: the
Direction-Adaptive RWKV Module(DARM) and the Stage-Adaptive
Squeeze-and-Excitation Module(SASE). DARM employs Dual-RWKV and QuadScan
mechanisms to aggregate contextual cues across images, mitigating directional
bias while preserving global context and maintaining high computational
efficiency. SASE dynamically adapts its architecture to different feature
extraction stages, balancing high-resolution detail preservation and semantic
relationship capture. Experiments demonstrate that U-RWKV achieves
state-of-the-art segmentation performance with high computational efficiency,
offering a practical solution for democratizing advanced medical imaging
technologies in resource-constrained environments. The code is available at
https://github.com/hbyecoding/U-RWKV.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [295] [Learning to Move in Rhythm: Task-Conditioned Motion Policies with Orbital Stability Guarantees](https://arxiv.org/abs/2507.10602)
*Maximilian Stölzle,T. Konstantin Rusch,Zach J. Patterson,Rodrigo Pérez-Dattari,Francesco Stella,Josie Hughes,Cosimo Della Santina,Daniela Rus*

Main category: cs.RO

TL;DR: 文章引入轨道稳定运动原语（OSMPs）框架，能从演示中准确获取周期性运动，可实现零样本泛化，经实验验证比现有方法更有效。


<details>
  <summary>Details</summary>
Motivation: 动态运动原语难以捕捉复杂周期性行为且插值能力有限，适用范围窄，需新方法解决。

Method: 引入OSMPs框架，结合学习的微分同胚编码器与潜在空间的超临界Hopf分岔，通过对双射编码器进行任务条件设置。

Result: 通过大量模拟和真实世界实验验证，在多种机器人平台上表现出色，始终优于现有基线方法。

Conclusion: OSMPs框架具有通用性和有效性，能准确获取周期性运动并实现零样本泛化。

Abstract: Learning from demonstration provides a sample-efficient approach to acquiring
complex behaviors, enabling robots to move robustly, compliantly, and with
fluidity. In this context, Dynamic Motion Primitives offer built - in stability
and robustness to disturbances but often struggle to capture complex periodic
behaviors. Moreover, they are limited in their ability to interpolate between
different tasks. These shortcomings substantially narrow their applicability,
excluding a wide class of practically meaningful tasks such as locomotion and
rhythmic tool use. In this work, we introduce Orbitally Stable Motion
Primitives (OSMPs) - a framework that combines a learned diffeomorphic encoder
with a supercritical Hopf bifurcation in latent space, enabling the accurate
acquisition of periodic motions from demonstrations while ensuring formal
guarantees of orbital stability and transverse contraction. Furthermore, by
conditioning the bijective encoder on the task, we enable a single learned
policy to represent multiple motion objectives, yielding consistent zero-shot
generalization to unseen motion objectives within the training distribution. We
validate the proposed approach through extensive simulation and real-world
experiments across a diverse range of robotic platforms - from collaborative
arms and soft manipulators to a bio-inspired rigid-soft turtle robot -
demonstrating its versatility and effectiveness in consistently outperforming
state-of-the-art baselines such as diffusion policies, among others.

</details>


### [296] [Acting and Planning with Hierarchical Operational Models on a Mobile Robot: A Study with RAE+UPOM](https://arxiv.org/abs/2507.11345)
*Oscar Lima,Marc Vinci,Sunandita Patra,Sebastian Stock,Joachim Hertzberg,Martin Atzmueller,Malik Ghallab,Dana Nau,Paolo Traverso*

Main category: cs.RO

TL;DR: 本文首次物理部署集成的行动者 - 规划者系统，在移动机械臂上实现并用于物体收集任务，实验展示其鲁棒性并提供决策过程见解。


<details>
  <summary>Details</summary>
Motivation: 解决符号规划器模型与机器人实际运行的丰富控制结构之间的不一致性问题。

Method: 部署集成的行动者 - 规划者系统，将反应式行动引擎（RAE）与任意时间UCT类蒙特卡罗规划器（UPOM）交错，在移动机械臂上实现RAE+UPOM。

Result: 实验证明系统在行动失败和传感器噪声下能稳健执行任务。

Conclusion: 该集成系统有效，实验为交错的行动 - 规划决策过程提供了经验见解。

Abstract: Robotic task execution faces challenges due to the inconsistency between
symbolic planner models and the rich control structures actually running on the
robot. In this paper, we present the first physical deployment of an integrated
actor-planner system that shares hierarchical operational models for both
acting and planning, interleaving the Reactive Acting Engine (RAE) with an
anytime UCT-like Monte Carlo planner (UPOM). We implement RAE+UPOM on a mobile
manipulator in a real-world deployment for an object collection task. Our
experiments demonstrate robust task execution under action failures and sensor
noise, and provide empirical insights into the interleaved acting-and-planning
decision making process.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [297] [Pronunciation Deviation Analysis Through Voice Cloning and Acoustic Comparison](https://arxiv.org/abs/2507.10985)
*Andrew Valdivia,Yueming Zhang,Hailu Xu,Amir Ghasemkhani,Xin Qin*

Main category: cs.SD

TL;DR: 提出通过分析用户原语音与纠正发音的克隆语音偏差检测误发音的方法，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 寻找不依赖预定义语音规则和大量训练数据的误发音检测方法。

Method: 利用语音克隆技术生成发音正确的合成语音，逐帧比较找出问题片段。

Result: 该方法能有效找出特定发音错误。

Conclusion: 此方法无需针对每种目标语言的预定义语音规则和大量训练数据，可用于误发音检测。

Abstract: This paper presents a novel approach for detecting mispronunciations by
analyzing deviations between a user's original speech and their voice-cloned
counterpart with corrected pronunciation. We hypothesize that regions with
maximal acoustic deviation between the original and cloned utterances indicate
potential mispronunciations. Our method leverages recent advances in voice
cloning to generate a synthetic version of the user's voice with proper
pronunciation, then performs frame-by-frame comparisons to identify problematic
segments. Experimental results demonstrate the effectiveness of this approach
in pinpointing specific pronunciation errors without requiring predefined
phonetic rules or extensive training data for each target language.

</details>


### [298] [EditGen: Harnessing Cross-Attention Control for Instruction-Based Auto-Regressive Audio Editing](https://arxiv.org/abs/2507.11096)
*Vassilis Sioros,Alexandros Potamianos,Giorgos Paraskevopoulos*

Main category: cs.SD

TL;DR: 研究利用交叉注意力控制在自回归模型中进行高效音频编辑，提出Prompt - to - Prompt类方法和结合MUSICGEN的三种编辑机制，评估显示其优于扩散基线。


<details>
  <summary>Details</summary>
Motivation: 探索在自回归模型中进行高效音频编辑的方法。

Method: 开发Prompt - to - Prompt类方法，结合扩散策略；引入MUSICGEN，提出基于替换、重新加权和细化注意力分数的三种编辑机制；采用音乐评估指标和人类研究进行评估。

Result: 自动和人类评估表明，Prompt - to - Prompt指导与自回归生成模型的组合在旋律、动态和节奏方面显著优于扩散基线。

Conclusion: 所提出的方法在音频编辑方面有较好效果，代码开源。

Abstract: In this study, we investigate leveraging cross-attention control for
efficient audio editing within auto-regressive models. Inspired by image
editing methodologies, we develop a Prompt-to-Prompt-like approach that guides
edits through cross and self-attention mechanisms. Integrating a
diffusion-based strategy, influenced by Auffusion, we extend the model's
functionality to support refinement edits, establishing a baseline for
prompt-guided audio editing. Additionally, we introduce an alternative approach
by incorporating MUSICGEN, a pre-trained frozen auto-regressive model, and
propose three editing mechanisms, based on Replacement, Reweighting, and
Refinement of the attention scores. We employ commonly-used music-specific
evaluation metrics and a human study, to gauge time-varying controllability,
adherence to global text cues, and overall audio realism. The automatic and
human evaluations indicate that the proposed combination of prompt-to-prompt
guidance with autoregressive generation models significantly outperforms the
diffusion-based baseline in terms of melody, dynamics, and tempo of the
generated audio. Our code is available at https://github.com/billsioros/EditGen

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [299] [Arcturus: A Cloud Overlay Network for Global Accelerator with Enhanced Performance and Stability](https://arxiv.org/abs/2507.10928)
*Matthew Yang Liu,Chuang Chen,Pengcheng Lv,Hui Guo,Yanan Zhang,Cong Wang,Yusen Li,Zhenyu Li,Yu-Chu Tian*

Main category: cs.NI

TL;DR: 现有GA服务有缺陷，Arcturus云原生GA框架利用多云资源设计，分双平面，性能优、成本低、资源效率高。


<details>
  <summary>Details</summary>
Motivation: 现有GA服务与特定云供应商绑定，存在高成本、部署僵化、灵活性有限等问题，需改进。

Method: Arcturus利用多供应商低成本、异构云资源，采用双平面设计，转发平面构建自适应控制代理网络，调度平面通过轻量级定量优化协调负载和路由。

Result: 在数百万RPS评估中，Arcturus加速性能比商业GA服务高1.7倍，成本降低71%，资源效率超80%。

Conclusion: Arcturus能有效大规模利用云资源。

Abstract: Global Accelerator (GA) services play a vital role in ensuring low-latency,
high-reliability communication for real-time interactive applications. However,
existing GA offerings are tightly bound to specific cloud providers, resulting
in high costs, rigid deployment, and limited flexibility, especially for
large-scale or budget-sensitive deployments. Arcturus is a cloud-native GA
framework that revisits the design of GA systems by leveraging low-cost,
heterogeneous cloud resources across multiple providers. Rather than relying on
fixed, high-end infrastructure, Arcturus dynamically constructs its
acceleration network and balances performance, stability, and resource
efficiency. To achieve this, Arcturus introduces a two-plane design: a
forwarding plane that builds a proxy network with adaptive control, and a
scheduling plane that coordinates load and routing through lightweight,
quantitative optimization. Evaluations under millions of RPS show that Arcturus
outperforms commercial GA services by up to 1.7X in acceleration performance,
reduces cost by 71%, and maintains over 80% resource efficiency--demonstrating
efficient use of cloud resources at scale.

</details>


### [300] [LiLM-RDB-SFC: Lightweight Language Model with Relational Database-Guided DRL for Optimized SFC Provisioning](https://arxiv.org/abs/2507.10903)
*Parisa Fard Moshiri,Xinyu Zhu,Poonam Lohan,Burak Kantarci,Emil Janulewicz*

Main category: cs.NI

TL;DR: 本文提出LiLM - RDB - SFC方法结合轻量级语言模型和关系数据库指导DRL模型进行高效SFC供应，对比不同模型性能，FLAN - T5表现更优。


<details>
  <summary>Details</summary>
Motivation: 现代SDN和NFV环境中SFC管理和VNF放置是关键挑战，DRL在不可预测网络条件下适应性和响应性受限。

Method: 提出LiLM - RDB - SFC方法，利用BART和FLAN - T5两个LiLM解释网络数据并支持相关查询。

Result: FLAN - T5比BART测试损失低、准确率高、处理时间少；与SQLCoder相比，准确率相当但处理时间减少96%。

Conclusion: 结合轻量级语言模型和关系数据库的LiLM - RDB - SFC方法可有效指导DRL模型进行SFC供应，且FLAN - T5性能表现更好。

Abstract: Effective management of Service Function Chains (SFCs) and optimal Virtual
Network Function (VNF) placement are critical challenges in modern
Software-Defined Networking (SDN) and Network Function Virtualization (NFV)
environments. Although Deep Reinforcement Learning (DRL) is widely adopted for
dynamic network decision-making, its inherent dependency on structured data and
fixed action rules often limits adaptability and responsiveness, particularly
under unpredictable network conditions. This paper introduces LiLM-RDB-SFC, a
novel approach combining Lightweight Language Model (LiLM) with Relational
Database (RDB) to answer network state queries to guide DRL model for efficient
SFC provisioning. Our proposed approach leverages two LiLMs, Bidirectional and
Auto-Regressive Transformers (BART) and the Fine-tuned Language Net T5
(FLAN-T5), to interpret network data and support diverse query types related to
SFC demands, data center resources, and VNF availability. Results demonstrate
that FLAN-T5 outperforms BART with a lower test loss (0.00161 compared to
0.00734), higher accuracy (94.79% compared to 80.2%), and less processing time
(2h 2min compared to 2h 38min). Moreover, when compared to the large language
model SQLCoder, FLAN-T5 matches the accuracy of SQLCoder while cutting
processing time by 96% (SQLCoder: 54 h 43 min; FLAN-T5: 2 h 2 min).

</details>


### [301] [Improving Wi-Fi Network Performance Prediction with Deep Learning Models](https://arxiv.org/abs/2507.11168)
*Gabriele Formis,Amanda Ericson,Stefan Forsstrom,Kyi Thar,Gianluca Cena,Stefano Scanzio*

Main category: cs.NI

TL;DR: 本文利用机器学习技术预测Wi-Fi网络信道质量，比较不同模型表现，CNN在CPU和内存使用上更高效。


<details>
  <summary>Details</summary>
Motivation: 工业和关键任务应用对无线网络鲁棒性、可靠性和确定性需求增加，需新方法。

Method: 使用卷积神经网络和长短期记忆网络等机器学习技术，在多信道真实Wi-Fi设置数据集上分析。

Result: 可可靠预测帧传输率，卷积神经网络预测效果稍逊但CPU和内存使用更高效。

Conclusion: 卷积神经网络的特性增强了其在嵌入式和工业系统的可用性。

Abstract: The increasing need for robustness, reliability, and determinism in wireless
networks for industrial and mission-critical applications is the driver for the
growth of new innovative methods. The study presented in this work makes use of
machine learning techniques to predict channel quality in a Wi-Fi network in
terms of the frame delivery ratio. Predictions can be used proactively to
adjust communication parameters at runtime and optimize network operations for
industrial applications. Methods including convolutional neural networks and
long short-term memory were analyzed on datasets acquired from a real Wi-Fi
setup across multiple channels. The models were compared in terms of prediction
accuracy and computational complexity. Results show that the frame delivery
ratio can be reliably predicted, and convolutional neural networks, although
slightly less effective than other models, are more efficient in terms of CPU
usage and memory consumption. This enhances the model's usability on embedded
and industrial systems.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [302] [Bridging Brains and Machines: A Unified Frontier in Neuroscience, Artificial Intelligence, and Neuromorphic Systems](https://arxiv.org/abs/2507.10722)
*Sohan Shankar,Yi Pan,Hanqi Jiang,Zhengliang Liu,Mohammad R. Darbandi,Agustin Lorenzo,Junhao Chen,Md Mehedi Hasan,Arif Hassan Zidan,Eliana Gelman,Joshua A. Konfrst,Jillian Y. Russell,Katelyn Fernandes,Tianze Yang,Yiwei Li,Huaqin Zhao,Afrar Jahin,Triparna Ganguly,Shair Dinesha,Yifan Zhou,Zihao Wu,Xinliang Li,Lokesh Adusumilli,Aziza Hussein,Sagar Nookarapu,Jixin Hou,Kun Jiang,Jiaxi Li,Brenden Heinel,XianShen Xi,Hailey Hubbard,Zayna Khan,Levi Whitaker,Ivan Cao,Max Allgaier,Andrew Darby,Lin Zhao,Lu Zhang,Xiaoqiao Wang,Xiang Li,Wei Zhang,Xiaowei Yu,Dajiang Zhu,Yohannes Abate,Tianming Liu*

Main category: q-bio.NC

TL;DR: 文章识别神经科学、AGI和神经形态计算的融合范式，提出设计原则，回顾发展历程，探讨物理基板，指出四个挑战并提供整合议程。


<details>
  <summary>Details</summary>
Motivation: 探索神经科学、AGI和神经形态计算统一研究范式，设计结合人类和机器智能的下一代AGI系统。

Method: 基于大脑生理学框架，回顾从早期连接主义模型到先进大语言模型的发展，探讨新兴物理基板。

Result: 明确关键创新与神经生物学过程的相似性，指出四个交叉领域的关键挑战。

Conclusion: 跨神经科学、计算和硬件的综合视角为各领域提供整合议程。

Abstract: This position and survey paper identifies the emerging convergence of
neuroscience, artificial general intelligence (AGI), and neuromorphic computing
toward a unified research paradigm. Using a framework grounded in brain
physiology, we highlight how synaptic plasticity, sparse spike-based
communication, and multimodal association provide design principles for
next-generation AGI systems that potentially combine both human and machine
intelligences. The review traces this evolution from early connectionist models
to state-of-the-art large language models, demonstrating how key innovations
like transformer attention, foundation-model pre-training, and multi-agent
architectures mirror neurobiological processes like cortical mechanisms,
working memory, and episodic consolidation. We then discuss emerging physical
substrates capable of breaking the von Neumann bottleneck to achieve
brain-scale efficiency in silicon: memristive crossbars, in-memory compute
arrays, and emerging quantum and photonic devices. There are four critical
challenges at this intersection: 1) integrating spiking dynamics with
foundation models, 2) maintaining lifelong plasticity without catastrophic
forgetting, 3) unifying language with sensorimotor learning in embodied agents,
and 4) enforcing ethical safeguards in advanced neuromorphic autonomous
systems. This combined perspective across neuroscience, computation, and
hardware offers an integrative agenda for in each of these fields.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [303] [HEIMDALL: a grapH-based sEIsMic Detector And Locator for microseismicity](https://arxiv.org/abs/2507.10850)
*Matteo Bagagli,Francesco Grigoli,Davide Bacciu*

Main category: physics.geo-ph

TL;DR: 提出用于微地震监测的深度学习模型，在冰岛地区测试效果好，是可靠监测工具。


<details>
  <summary>Details</summary>
Motivation: 在绿色能源转型、减少碳排放背景下，开发增强型地热系统对微地震监测有需求。

Method: 利用图论和先进图神经网络架构，在滚动窗口上同时进行相位拾取、关联和事件定位，形成端到端地震目录创建管道。

Result: 与以往自动系统和参考目录相比，事件检测显著增加，减少了误报事件，减少人工监督和模型调整需求。

Conclusion: 该模型是地热地震区域可靠监测工具，可补充现有系统，降低地热能源开采运营风险。

Abstract: In this work, we present a new deep-learning model for microseismicity
monitoring that utilizes continuous spatiotemporal relationships between
seismic station recordings, forming an end-to-end pipeline for seismic catalog
creation. It employs graph theory and state-of-the-art graph neural network
architectures to perform phase picking, association, and event location
simultaneously over rolling windows, making it suitable for both playback and
near-real-time monitoring. As part of the global strategy to reduce carbon
emissions within the broader context of a green-energy transition, there has
been growing interest in exploiting enhanced geothermal systems. Tested in the
complex geothermal area of Iceland's Hengill region using open-access data from
a temporary experiment, our model was trained and validated using both manually
revised and automatic seismic catalogs. Results showed a significant increase
in event detection compared to previously published automatic systems and
reference catalogs, including a $4 M_w$ seismic sequence in December 2018 and a
single-day sequence in February 2019. Our method reduces false events,
minimizes manual oversight, and decreases the need for extensive tuning of
pipelines or transfer learning of deep-learning models. Overall, it validates a
robust monitoring tool for geothermal seismic regions, complementing existing
systems and enhancing operational risk mitigation during geothermal energy
exploitation.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [304] [Active Learning via Heteroskedastic Rational Kriging](https://arxiv.org/abs/2507.10952)
*Shangkun Wang,V. Roshan Joseph*

Main category: stat.ME

TL;DR: 提出新高斯过程模型用于主动学习，能在响应面更有趣区域放置设计点，通过模拟和真实数据集对比，性能可比且速度快。


<details>
  <summary>Details</summary>
Motivation: 现有的基于平稳高斯过程的主动学习方法在函数仅在小区域变化时会浪费资源。

Method: 提出能捕捉函数异方差性的新高斯过程模型进行主动学习，并与现有方法对比。

Result: 与其他基于非平稳高斯过程的方法相比，性能相当或更好，且速度快几个数量级。

Conclusion: 新的主动学习方法具有优势，能获得更准确的代理模型。

Abstract: Active learning methods for emulating complex computer models that rely on
stationary Gaussian processes tend to produce design points that uniformly fill
the entire experimental region, which can be wasteful for functions which vary
only in small regions. In this article, we propose a new Gaussian process model
that captures the heteroskedasticity of the function. Active learning using
this new model can place design points in the more interesting regions of the
response surface, and thus obtain surrogate models with better accuracy. The
proposed active learning method is compared with the state-of-the-art methods
using simulations and two real datasets. It is found to have comparable or
better performance relative to other non-stationary Gaussian process-based
methods, but faster by orders of magnitude.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [305] [Functional Neural Wavefunction Optimization](https://arxiv.org/abs/2507.10835)
*Victor Armegioiu,Juan Carrasquilla,Siddhartha Mishra,Johannes Müller,Jannes Nys,Marius Zeinhofer,Hang Zhang*

Main category: cond-mat.str-el

TL;DR: 提出变分量子蒙特卡罗优化算法设计与分析框架，通过Galerkin投影转换优化动力学，统一现有方法，推导新算法，经数值实验验证。


<details>
  <summary>Details</summary>
Motivation: 设计和分析变分量子蒙特卡罗中的优化算法。

Method: 利用Galerkin投影将无限维优化动力学转换为参数空间算法。

Result: 统一现有方法，如随机重构和Rayleigh - Gauss - Newton，推导新算法，通过数值实验准确估计凝聚态物理中几个典型模型的基态能量。

Conclusion: 所提框架具有实际相关性。

Abstract: We propose a framework for the design and analysis of optimization algorithms
in variational quantum Monte Carlo, drawing on geometric insights into the
corresponding function space. The framework translates infinite-dimensional
optimization dynamics into tractable parameter-space algorithms through a
Galerkin projection onto the tangent space of the variational ansatz. This
perspective unifies existing methods such as stochastic reconfiguration and
Rayleigh-Gauss-Newton, provides connections to classic function-space
algorithms, and motivates the derivation of novel algorithms with geometrically
principled hyperparameter choices. We validate our framework with numerical
experiments demonstrating its practical relevance through the accurate
estimation of ground-state energies for several prototypical models in
condensed matter physics modeled with neural network wavefunctions.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [306] [Neural Expectation Operators](https://arxiv.org/abs/2507.10607)
*Qian Qi*

Main category: math.PR

TL;DR: 本文介绍Measure Learning范式，通过非线性期望建模模糊性，给出相关数学定理和创新，拓展理论应用，提供数据驱动模糊建模基础框架。


<details>
  <summary>Details</summary>
Motivation: 构建在模糊性下进行数据驱动建模的数学基础框架，连接二次BSDE理论与机器学习。

Method: 定义由神经网络参数化驱动的倒向随机微分方程（BSDEs）的神经期望算子，通过架构设计确保关键公理性质。

Result: 得到满足局部Lipschitz条件和二次增长的BSDEs的适定性定理，拓展到前 - 后向SDE系统和大相互作用粒子系统分析。

Conclusion: 该工作为模糊性下的数据驱动建模提供了基础数学框架。

Abstract: This paper introduces \textbf{Measure Learning}, a paradigm for modeling
ambiguity via non-linear expectations. We define Neural Expectation Operators
as solutions to Backward Stochastic Differential Equations (BSDEs) whose
drivers are parameterized by neural networks. The main mathematical
contribution is a rigorous well-posedness theorem for BSDEs whose drivers
satisfy a local Lipschitz condition in the state variable $y$ and quadratic
growth in its martingale component $z$. This result circumvents the classical
global Lipschitz assumption, is applicable to common neural network
architectures (e.g., with ReLU activations), and holds for exponentially
integrable terminal data, which is the sharp condition for this setting. Our
primary innovation is to build a constructive bridge between the abstract, and
often restrictive, assumptions of the deep theory of quadratic BSDEs and the
world of machine learning, demonstrating that these conditions can be met by
concrete, verifiable neural network designs. We provide constructive methods
for enforcing key axiomatic properties, such as convexity, by architectural
design. The theory is extended to the analysis of fully coupled
Forward-Backward SDE systems and to the asymptotic analysis of large
interacting particle systems, for which we establish both a Law of Large
Numbers (propagation of chaos) and a Central Limit Theorem. This work provides
the foundational mathematical framework for data-driven modeling under
ambiguity.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [307] [AGFS-Tractometry: A Novel Atlas-Guided Fine-Scale Tractometry Approach for Enhanced Along-Tract Group Statistical Comparison Using Diffusion MRI Tractography](https://arxiv.org/abs/2507.10601)
*Ruixi Zheng,Wei Zhang,Yijie Li,Xi Zhu,Zhou Lan,Jarrett Rushmore,Yogesh Rathi,Nikos Makris,Lauren J. O'Donnell,Fan Zhang*

Main category: q-bio.QM

TL;DR: 提出AGFS - Tractometry方法用于脑白质连接研究，实验表明其在检测局部白质差异上有更高灵敏度和特异性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在群体间沿纤维束的统计分析方面存在不足，需更好方法研究不同群体局部沿束差异。

Method: 提出AGFS - Tractometry方法，创建新的图谱引导束轮廓模板，提出非参数置换检验组比较方法。

Result: 在合成数据集和真实数据上实验，与AFQ和BUAN对比，AGFS - Tractometry检测局部白质差异灵敏度和特异性更高，能识别更多有显著差异区域。

Conclusion: AGFS - Tractometry能检测细微或空间局部白质组间差异，代码开源。

Abstract: Diffusion MRI (dMRI) tractography is currently the only method for in vivo
mapping of the brain's white matter (WM) connections. Tractometry is an
advanced tractography analysis technique for along-tract profiling to
investigate the morphology and microstructural properties along the fiber
tracts. Tractometry has become an essential tool for studying local along-tract
differences between different populations (e.g., health vs disease). In this
study, we propose a novel atlas-guided fine-scale tractometry method, namely
AGFS-Tractometry, that leverages tract spatial information and permutation
testing to enhance the along-tract statistical analysis between populations.
There are two major contributions in AGFS-Tractometry. First, we create a novel
atlas-guided tract profiling template that enables consistent, fine-scale,
along-tract parcellation of subject-specific fiber tracts. Second, we propose a
novel nonparametric permutation testing group comparison method to enable
simultaneous analysis across all along-tract parcels while correcting for
multiple comparisons. We perform experimental evaluations on synthetic datasets
with known group differences and in vivo real data. We compare AGFS-Tractometry
with two state-of-the-art tractometry methods, including Automated Fiber-tract
Quantification (AFQ) and BUndle ANalytics (BUAN). Our results show that the
proposed AGFS-Tractometry obtains enhanced sensitivity and specificity in
detecting local WM differences. In the real data analysis experiments,
AGFS-Tractometry can identify more regions with significant differences, which
are anatomically consistent with the existing literature. Overall, these
demonstrate the ability of AGFS-Tractometry to detect subtle or spatially
localized WM group-level differences. The created tract profiling template and
related code are available at:
https://github.com/ZhengRuixi/AGFS-Tractometry.git.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [308] [Elk: Exploring the Efficiency of Inter-core Connected AI Chips with Deep Learning Compiler Techniques](https://arxiv.org/abs/2507.11506)
*Yiqi Liu,Yuqi Xue,Noelle Crawford,Jilong Xue,Jian Huang*

Main category: cs.AR

TL;DR: 本文提出DL编译器框架Elk，通过权衡计算、通信和I/O性能因素提升ICCA芯片效率，实验表明其能达理想性能94%，还可用于芯片架构设计探索。


<details>
  <summary>Details</summary>
Motivation: 满足深度学习模型需求，ICCA芯片采用新设计，但因计算、通信和I/O三者权衡难，探索其效率不易。

Method: 构建DL编译器框架Elk，将性能因素转为可配置参数形成全局权衡空间，采用归纳算子调度策略和成本感知片上内存分配算法生成优化执行计划。

Result: 基于IPU - POD4构建模拟器，Elk平均达到ICCA芯片理想性能的94%。

Conclusion: Elk能提升ICCA芯片效率，支持大DL模型，还可用于新ICCA芯片架构设计探索。

Abstract: To meet the increasing demand of deep learning (DL) models, AI chips are
employing both off-chip memory (e.g., HBM) and high-bandwidth low-latency
interconnect for direct inter-core data exchange. However, it is not easy to
explore the efficiency of these inter-core connected AI (ICCA) chips, due to a
fundamental tussle among compute (per-core execution), communication
(inter-core data exchange), and I/O (off-chip data access).
  In this paper, we develop Elk, a DL compiler framework to maximize the
efficiency of ICCA chips by jointly trading off all the three performance
factors discussed above. Elk structures these performance factors into
configurable parameters and forms a global trade-off space in the DL compiler.
To systematically explore this space and maximize overall efficiency, Elk
employs a new inductive operator scheduling policy and a cost-aware on-chip
memory allocation algorithm. It generates globally optimized execution plans
that best overlap off-chip data loading and on-chip execution. To examine the
efficiency of Elk, we build a full-fledged emulator based on a real ICCA chip
IPU-POD4, and an ICCA chip simulator for sensitivity analysis with different
interconnect network topologies. Elk achieves 94% of the ideal roofline
performance of ICCA chips on average, showing the benefits of supporting large
DL models on ICCA chips. We also show Elk's capability of enabling architecture
design space exploration for new ICCA chip development.

</details>


### [309] [SPICEAssistant: LLM using SPICE Simulation Tools for Schematic Design of Switched-Mode Power Supplies](https://arxiv.org/abs/2507.10639)
*Simon Nau,Jan Krummenauer,André Zimmermann*

Main category: cs.AR

TL;DR: 本文聚焦大语言模型应用于开关电源PCB设计，提出SPICEAssistant框架解决挑战，基准测试显示该框架有效提升设计能力，且优于GPT - 4o。


<details>
  <summary>Details</summary>
Motivation: 确定大语言模型在电子设计自动化领域中理解、适配和设计电子电路的能力，解决其在开关电源PCB设计中的挑战。

Method: 提出SPICEAssistant框架，为大语言模型提供与SPICE交互的工具，定义含256个问题的基准测试。

Result: 模拟反馈有效提升大语言模型开关电源设计能力，模拟迭代次数增加性能增强，SPICEAssistant框架在基准测试中比GPT - 4o高约38%。

Conclusion: SPICEAssistant框架能有效提升大语言模型在开关电源设计中的性能。

Abstract: State-of-the-art large language models (LLMs) show high performance across a
wide range of tasks in many domains of science. In the field of electronic
design automation (EDA), it is yet to be determined to what extent they are
capable to understand, adapt, and dimension electronic circuits. This paper
focuses on the application of LLMs to switched-mode power supply (SMPS) design
on printed circuit boards (PCBs). Particular challenges for LLMs in this
context include their limited ability to interpret results from key simulation
tools like SPICE and the multi-step design process. To address these
challenges, we suggest SPICEAssistant, a framework that provides a broad
selection of tools to an LLM. The tools serve as an interface to SPICE,
allowing the LLM to interact flexibly with the simulator to estimate the impact
of its modifications to the circuit. To evaluate the performance of
SPICEAssistant, we defined a benchmark consisting of 256 questions testing the
ability to adapt circuit netlists to fulfil different SMPS design tasks. The
benchmarking results show that simulation feedback effectively improves SMPS
design capabilities of LLMs. An increasing number of simulation iterations
leads to enhanced performance. The SPICEAssistant framework significantly
outperforms the standalone LLM GPT-4o on the benchmark by approximately 38%.

</details>


### [310] [SystolicAttention: Fusing FlashAttention within a Single Systolic Array](https://arxiv.org/abs/2507.11331)
*Jiawei Lin,Guokai Chen,Yuanlong Li,Thomas Bourgeat*

Main category: cs.AR

TL;DR: 当前基于脉动阵列的加速器执行FlashAttention存在问题，本文提出FSA架构解决，经评估性能优于现有商业加速器。


<details>
  <summary>Details</summary>
Motivation: 现有基于脉动阵列的加速器执行FlashAttention时，因频繁数据交换、非矩阵操作不匹配及资源争用等问题，导致脉动阵列利用率低、性能下降。

Method: 提出FSA架构，其核心是SystolicAttention调度算法，将FlashAttention操作细粒度映射到脉动阵列上。

Result: FSA在可综合RTL中实现，与AWS NeuronCore - v2和Google TPUv5e相比，注意力FLOPs/s利用率分别提高1.77倍和4.83倍，面积开销约10%。

Conclusion: FSA架构能有效解决现有脉动阵列加速器执行FlashAttention的问题，提升性能。

Abstract: Transformer models rely heavily on scaled dot-product attention (SDPA),
typically implemented using the FlashAttention algorithm. However, current
systolic-array-based accelerators face significant challenges when executing
FlashAttention. Systolic arrays can only achieve high utilization for
consecutive and large matrix multiplications. In contrast, FlashAttention
requires frequently interleaved matrix multiplications and softmax operations.
  The frequent data swaps between the systolic array and external vector units
result in low systolic array utilization. This is further exacerbated by the
fact that softmax involves numerous non-matrix operations, which are not
well-suited for systolic arrays. Moreover, the concurrent execution of matrix
multiplication on systolic arrays and softmax on vector units leads to register
file and SRAM port contention, further degrading performance.
  To overcome these limitations, we propose FSA, an enhanced systolic array
architecture that enables the entire FlashAttention algorithm to run entirely
within a single systolic array, eliminating the need for external vector units.
At the core of FSA is SystolicAttention, a novel scheduling algorithm that maps
FlashAttention operations onto systolic arrays with fine-grained, element-wise
overlap. This significantly improves array utilization while preserving the
original floating-point operation order to maintain numerical stability.
  We implement FSA in synthesizable RTL and evaluate its performance against
state-of-the-art commercial accelerators. Our results show that FSA achieves
1.77x and 4.83x higher attention FLOPs/s utilization compared to AWS
NeuronCore-v2 and Google TPUv5e, respectively, with only about 10% area
overhead.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [311] [Start from the End: A Framework for Computational Policy Exploration to Inform Effective and Geospatially Consistent Interventions applied to COVID-19 in St. Louis](https://arxiv.org/abs/2507.10870)
*David O'Gara,Matt Kasman,Matthew D. Haslam,Ross A. Hammond*

Main category: stat.AP

TL;DR: 提出参数探索框架，利用模拟器模型对高维参数空间进行不确定性感知预测，以确定可行响应策略，并应用于新冠疫情案例研究。


<details>
  <summary>Details</summary>
Motivation: 数学模型因详细实现和高计算要求，从业者只能探索少量干预场景，限制应对疾病爆发的准备工作。

Method: 提出利用模拟器模型的参数探索框架。

Result: 确定大量能大幅减少疾病传播的响应策略，识别可减少疾病传播地理空间差异的政策干预措施。

Conclusion: 该框架有助于设计应对传染病的策略。

Abstract: Mathematical models are a powerful tool to study infectious disease dynamics
and intervention strategies against them in social systems. However, due to
their detailed implementation and steep computational requirements,
practitioners and stakeholders are typically only able to explore a small
subset of all possible intervention scenarios, a severe limitation when
preparing for disease outbreaks. In this work, we propose a parameter
exploration framework utilizing emulator models to make uncertainty-aware
predictions of high-dimensional parameter spaces and identify large numbers of
feasible response strategies. We apply our framework to a case study of a
large-scale agent-based disease model of the COVID-19 ``Omicron wave'' in St.
Louis, Missouri that took place from December 2021 to February 2022. We
identify large numbers of response strategies that would have been estimated to
have reduced disease spread by a substantial amount. We also identify policy
interventions that would have been able to reduce the geospatial variation in
disease spread, which has additional implications for designing thoughtful
response strategies.

</details>
