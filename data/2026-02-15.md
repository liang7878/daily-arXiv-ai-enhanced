<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 51]
- [cs.CE](#cs.CE) [Total: 5]
- [cs.DB](#cs.DB) [Total: 6]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.DS](#cs.DS) [Total: 8]
- [cs.GT](#cs.GT) [Total: 18]
- [cs.IR](#cs.IR) [Total: 18]
- [cs.LG](#cs.LG) [Total: 58]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.SE](#cs.SE) [Total: 25]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [stat.ML](#stat.ML) [Total: 7]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [stat.AP](#stat.AP) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.CC](#cs.CC) [Total: 1]
- [cs.DM](#cs.DM) [Total: 1]
- [econ.TH](#econ.TH) [Total: 1]
- [cs.CR](#cs.CR) [Total: 8]
- [cs.AR](#cs.AR) [Total: 1]
- [econ.EM](#econ.EM) [Total: 3]
- [q-fin.GN](#q-fin.GN) [Total: 1]
- [econ.GN](#econ.GN) [Total: 4]
- [math.ST](#math.ST) [Total: 1]
- [cs.CG](#cs.CG) [Total: 2]
- [cs.CL](#cs.CL) [Total: 2]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [q-fin.MF](#q-fin.MF) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Explaining AI Without Code: A User Study on Explainable AI](https://arxiv.org/abs/2602.11159)
*Natalia Abarca,Andrés Carvallo,Claudia López Moncada,Felipe Bravo-Marquez*

Main category: cs.AI

TL;DR: 提出DashAI无代码机器学习平台的以人为中心的XAI模块，集成三种技术用于表格分类，用户研究评估其可用性和解释效果，指出XAI在无代码ML中需兼顾新手和专家需求。


<details>
  <summary>Details</summary>
Motivation: 机器学习在敏感领域应用引发自动化决策透明度担忧，可解释AI方法多需技术专长，无代码ML平台缺少可解释性，要解决新手和专家对解释的不同需求问题。

Method: 在DashAI无代码ML平台中引入XAI模块，集成PDP、PFI和KernelSHAP三种技术到表格分类工作流，开展有20名新手和专家参与的用户研究。

Result: 所有可解释性任务任务成功率≥80%；新手认为解释有用、准确和可信，专家更关注充分性和完整性；解释提升了对自动化的可预测性和信心感知，新手信任度高于专家。

Conclusion: 可解释AI在无代码机器学习中面临挑战，需使解释对新手易获取且对专家足够详细。

Abstract: The increasing use of Machine Learning (ML) in sensitive domains such as healthcare, finance, and public policy has raised concerns about the transparency of automated decisions. Explainable AI (XAI) addresses this by clarifying how models generate predictions, yet most methods demand technical expertise, limiting their value for novices. This gap is especially critical in no-code ML platforms, which seek to democratize AI but rarely include explainability. We present a human-centered XAI module in DashAI, an open-source no-code ML platform. The module integrates three complementary techniques, which are Partial Dependence Plots (PDP), Permutation Feature Importance (PFI), and KernelSHAP, into DashAI's workflow for tabular classification. A user study (N = 20; ML novices and experts) evaluated usability and the impact of explanations. Results show: (i) high task success ($\geq80\%$) across all explainability tasks; (ii) novices rated explanations as useful, accurate, and trustworthy on the Explanation Satisfaction Scale (ESS, Cronbach's $α$ = 0.74, a measure of internal consistency), while experts were more critical of sufficiency and completeness; and (iii) explanations improved perceived predictability and confidence on the Trust in Automation scale (TiA, $α$ = 0.60), with novices showing higher trust than experts. These findings highlight a central challenge for XAI in no-code ML, making explanations both accessible to novices and sufficiently detailed for experts.

</details>


### [2] [Latent Generative Solvers for Generalizable Long-Term Physics Simulation](https://arxiv.org/abs/2602.11229)
*Zituo Chen,Haixu Wu,Sili Deng*

Main category: cs.AI

TL;DR: 研究跨异构PDE系统的长时替代模拟，提出Latent Generative Solvers (LGS)框架，减少长时预测漂移，FLOPs低，可适应分布外数据。


<details>
  <summary>Details</summary>
Motivation: 实现跨异构PDE系统的长时模拟，提升神经PDE求解器的泛化性和长期预测可靠性。

Method: 提出两阶段LGS框架，用预训练VAE将PDE状态映射到共享潜在物理空间，用Transformer学习潜在动力学，引入不确定性旋钮和流强制机制。

Result: LGS在短时模拟上与确定性神经算子基线相当，长时减少漂移，FLOPs比非生成式基线低70倍，能在有限微调下适应分布外数据。

Conclusion: LGS为构建可泛化、考虑不确定性的神经PDE求解器提供了实用途径。

Abstract: We study long-horizon surrogate simulation across heterogeneous PDE systems. We introduce Latent Generative Solvers (LGS), a two-stage framework that (i) maps diverse PDE states into a shared latent physics space with a pretrained VAE, and (ii) learns probabilistic latent dynamics with a Transformer trained by flow matching. Our key mechanism is an uncertainty knob that perturbs latent inputs during training and inference, teaching the solver to correct off-manifold rollout drift and stabilizing autoregressive prediction. We further use flow forcing to update a system descriptor (context) from model-generated trajectories, aligning train/test conditioning and improving long-term stability. We pretrain on a curated corpus of $\sim$2.5M trajectories at $128^2$ resolution spanning 12 PDE families. LGS matches strong deterministic neural-operator baselines on short horizons while substantially reducing rollout drift on long horizons. Learning in latent space plus efficient architectural choices yields up to \textbf{70$\times$} lower FLOPs than non-generative baselines, enabling scalable pretraining. We also show efficient adaptation to an out-of-distribution $256^2$ Kolmogorov flow dataset under limited finetuning budgets. Overall, LGS provides a practical route toward generalizable, uncertainty-aware neural PDE solvers that are more reliable for long-term forecasting and downstream scientific workflows.

</details>


### [3] [On Decision-Valued Maps and Representational Dependence](https://arxiv.org/abs/2602.11295)
*Gil Raitses*

Main category: cs.AI

TL;DR: 本文形式化决策值映射，介绍DecisionDB基础设施，可记录、回放和审计相关关系，将表示空间分区并将决策重用作为可机械检查条件。


<details>
  <summary>Details</summary>
Motivation: 解决计算引擎对同一数据不同表示产生不同离散结果的问题，分析哪些表示保留结果、哪些改变结果。

Method: 形式化决策值映射，构建DecisionDB基础设施，利用内容计算的标识符和一次写入形式存储的工件来记录、回放和审计关系。

Result: 确定性回放能从存储工件中精确恢复每个记录的决策标识符，三个识别字段都与持久值匹配。

Conclusion: 将表示空间划分为持久区域和边界，把决策重用作为可机械检查的条件。

Abstract: A computational engine applied to different representations of the same data can produce different discrete outcomes, with some representations preserving the result and others changing it entirely. A decision-valued map records which representations preserve the outcome and which change it, associating each member of a declared representation family with the discrete result it produces. This paper formalizes decision-valued maps and describes DecisionDB, an infrastructure that logs, replays and audits these relationships using identifiers computed from content and artifacts stored in write-once form. Deterministic replay recovers each recorded decision identifier exactly from stored artifacts, with all three identifying fields matching their persisted values. The contribution partitions representation space into persistence regions and boundaries, and treats decision reuse as a mechanically checkable condition.

</details>


### [4] [Voxtral Realtime](https://arxiv.org/abs/2602.11298)
*Alexander H. Liu,Andy Ehrenberg,Andy Lo,Chen-Yo Sun,Guillaume Lample,Jean-Malo Delignon,Khyathi Raghavi Chandu,Patrick von Platen,Pavankumar Reddy Muddireddy,Rohin Arora,Sanchit Gandhi,Sandeep Subramanian,Soham Ghosh,Srijan Mishra,Abhinav Rastogi,Alan Jeffares,Albert Jiang,Alexandre Sablayrolles,Amélie Héliou,Andrew Bai,Angele Lenglemetz,Anmol Agarwal,Anton Eliseev,Antonia Calvi,Arjun Majumdar,Baptiste Bout,Baptiste Rozière,Baudouin De Monicault,Benjamin Tibi,Clémence Lanfranchi,Connor Chen,Corentin Barreau,Corentin Sautier,Cyprien Courtot,Darius Dabert,Diego de las Casas,Elliot Chane-Sane,Enguerrand Paquin,Faruk Ahmed,Federico Baldassarre,Gabrielle Berrada,Gaëtan Ecrepont,Gauthier Guinet,Genevieve Hayes,Georgii Novikov,Giada Pistilli,Guillaume Martin,Gunjan Dhanuka,Gunshi Gupta,Han Zhou,Indraneel Mukherjee,Irene Zhang,Jaeyoung Kim,Jan Ludziejewski,Jason Rute,Joachim Studnia,John Harvill,Jonas Amar,Josselin Somerville Roberts,Julien Tauran,Karmesh Yadav,Kartik Khandelwal,Kush Jain,Laurence Aitchison,Léonard Blier,Lingxiao Zhao,Louis Martin,Lucile Saulnier,Luyu Gao,Maarten Buyl,Manan Sharma,Margaret Jennings,Marie Pellat,Mark Prins,Mathieu Poirée,Mathilde Guillaumin,Matthieu Dinot,Matthieu Futeral,Maxime Darrin,Maximilian Augustin,Mert Unsal,Mia Chiquier,Nathan Grinsztajn,Neha Gupta,Olivier Bousquet,Olivier Duchenne,Patricia Wang,Paul Jacob,Paul Wambergue,Paula Kurylowicz,Philomène Chagniot,Pierre Stock,Piotr Miłoś,Prateek Gupta,Pravesh Agrawal,Quentin Torroba,Ram Ramrakhya,Rishi Shah,Romain Sauvestre,Roman Soletskyi,Rosalie Millner,Sagar Vaze,Samuel Humeau,Siddharth Gandhi,Sumukh Aithal,Szymon Antoniak,Teven Le Scao,Théo Cachet,Theo Simon Sorg,Thibaut Lavril,Thomas Chabal,Thomas Foubert,Thomas Robert,Thomas Wang,Tim Lawson,Tom Bewley,Tom Edwards,Tyler Wang,Valeriia Nemychnikova,Van Phung,Vedant Nanda,Victor Jouault,Virgile Richard,Vladislav Bataev,Wassim Bouaziz,Wen-Ding Li,William Marshall,Xinghui Li,Xingran Guo,Xinyu Yang,Yannic Neuhaus,Yihan Wang,Zaccharie Ramzi,Zhenlin Xu*

Main category: cs.AI

TL;DR: 介绍Voxtral Realtime，一种原生流式自动语音识别模型，在亚秒级延迟下达到离线转录质量，发布模型权重。


<details>
  <summary>Details</summary>
Motivation: 开发能在亚秒级延迟下达到离线转录质量的流式自动语音识别模型。

Method: 基于Delayed Streams Modeling框架，引入新的因果音频编码器和Ada RMS - Norm，在13种语言的大规模数据集上进行预训练。

Result: 在480ms延迟下，Voxtral Realtime性能与广泛使用的离线转录系统Whisper相当。

Conclusion: Voxtral Realtime能有效实现流式语音识别且达到较好效果，并开源模型权重。

Abstract: We introduce Voxtral Realtime, a natively streaming automatic speech recognition model that matches offline transcription quality at sub-second latency. Unlike approaches that adapt offline models through chunking or sliding windows, Voxtral Realtime is trained end-to-end for streaming, with explicit alignment between audio and text streams. Our architecture builds on the Delayed Streams Modeling framework, introducing a new causal audio encoder and Ada RMS-Norm for improved delay conditioning. We scale pretraining to a large-scale dataset spanning 13 languages. At a delay of 480ms, Voxtral Realtime achieves performance on par with Whisper, the most widely deployed offline transcription system. We release the model weights under the Apache 2.0 license.

</details>


### [5] [The PBSAI Governance Ecosystem: A Multi-Agent AI Reference Architecture for Securing Enterprise AI Estates](https://arxiv.org/abs/2602.11301)
*John M. Willis*

Main category: cs.AI

TL;DR: 企业快速部署AI系统形成AI地产，现有框架缺可实施架构，本文提出PBSAI治理生态系统架构并说明其特点与应用。


<details>
  <summary>Details</summary>
Motivation: 企业部署AI系统形成AI地产，但现有的治理和安全框架缺乏可实施的多智能体、支持AI的网络防御架构。

Method: 引入PBSAI治理生态系统，将职责分为十二个领域分类法，定义有界智能体家族，采用轻量级形式化模型。

Result: PBSAI架构展示了与NIST AI RMF功能的一致性，并说明了在企业SOC和超大规模防御环境中的应用。

Conclusion: PBSAI可作为开放生态系统发展和未来实证验证的结构化、以证据为中心的基础。

Abstract: Enterprises are rapidly deploying large language models, retrieval augmented generation pipelines, and tool using agents into production, often on shared high performance computing clusters and cloud accelerator platforms that also support defensive analytics. These systems increasingly function not as isolated models but as AI estates: socio technical systems spanning models, agents, data pipelines, security tooling, human workflows, and hyperscale infrastructure. Existing governance and security frameworks, including the NIST AI Risk Management Framework and systems security engineering guidance, articulate principles and risk functions but do not provide implementable architectures for multi agent, AI enabled cyber defense.
  This paper introduces the Practitioners Blueprint for Secure AI (PBSAI) Governance Ecosystem, a multi agent reference architecture for securing enterprise and hyperscale AI estates. PBSAI organizes responsibilities into a twelve domain taxonomy and defines bounded agent families that mediate between tools and policy through shared context envelopes and structured output contracts. The architecture assumes baseline enterprise security capabilities and encodes key systems security techniques, including analytic monitoring, coordinated defense, and adaptive response. A lightweight formal model of agents, context envelopes, and ecosystem level invariants clarifies the traceability, provenance, and human in the loop guarantees enforced across domains. We demonstrate alignment with NIST AI RMF functions and illustrate application in enterprise SOC and hyperscale defensive environments. PBSAI is proposed as a structured, evidence centric foundation for open ecosystem development and future empirical validation.

</details>


### [6] [Dissecting Subjectivity and the "Ground Truth" Illusion in Data Annotation](https://arxiv.org/abs/2602.11318)
*Sheza Munir,Benjamin Mah,Krisha Kalsi,Shivani Kapania,Julian Posada,Edith Law,Ding Wang,Syed Ishtiaque Ahmed*

Main category: cs.AI

TL;DR: 该论文指出机器学习‘地面真值’范式存在实证主义谬误，通过文献综述分析‘共识陷阱’机制，揭示系统问题，批判‘噪声传感器’谬误，提出多元注释基础设施路线图。


<details>
  <summary>Details</summary>
Motivation: 指出机器学习中‘地面真值’范式将人类分歧视为技术噪声的实证主义谬误，分析数据标注实践中导致‘共识陷阱’的机制。

Method: 进行系统的文献综述，对2020 - 2025年七个顶级会议的研究进行分析，采用分层关键词过滤、人工筛选和反思性主题分析等方法。

Result: 发现位置可读性的系统失败以及向人类验证者模型的架构转变带来锚定偏差和去除人类声音，地理霸权强加西方规范，统计模型误判文化多元主义为随机误差。

Conclusion: 批判‘噪声传感器’谬误，主张将分歧作为高保真信号，提出构建多元注释基础设施的路线图，从寻找单一正确答案转向映射人类经验的多样性。

Abstract: In machine learning, "ground truth" refers to the assumed correct labels used to train and evaluate models. However, the foundational "ground truth" paradigm rests on a positivistic fallacy that treats human disagreement as technical noise rather than a vital sociotechnical signal. This systematic literature review analyzes research published between 2020 and 2025 across seven premier venues: ACL, AIES, CHI, CSCW, EAAMO, FAccT, and NeurIPS, investigating the mechanisms in data annotation practices that facilitate this "consensus trap". Our identification phase captured 30,897 records, which were refined via a tiered keyword filtration schema to a high-recall corpus of 3,042 records for manual screening, resulting in a final included corpus of 346 papers for qualitative synthesis. Our reflexive thematic analysis reveals that systemic failures in positional legibility, combined with the recent architectural shift toward human-as-verifier models, specifically the reliance on model-mediated annotations, introduce deep-seated anchoring bias and effectively remove human voices from the loop. We further demonstrate how geographic hegemony imposes Western norms as universal benchmarks, often enforced by the performative alignment of precarious data workers who prioritize requester compliance over honest subjectivity to avoid economic penalties. Critiquing the "noisy sensor" fallacy, where statistical models misdiagnose cultural pluralism as random error, we argue for reclaiming disagreement as a high-fidelity signal essential for building culturally competent models. To address these systemic tensions, we propose a roadmap for pluralistic annotation infrastructures that shift the objective from discovering a singular "right" answer to mapping the diversity of human experience.

</details>


### [7] [Bi-Level Prompt Optimization for Multimodal LLM-as-a-Judge](https://arxiv.org/abs/2602.11340)
*Bo Pan,Xuan Kan,Kaitai Zhang,Yan Yan,Shunwen Tan,Zihao He,Zixin Ding,Junjie Wu,Liang Zhao*

Main category: cs.AI

TL;DR: 当前将基于大语言模型的评估与人类判断对齐有挑战，现有自动提示优化方法在多模态场景探索不足，本文提出BLPO框架用于多模态大语言模型作为评估器，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有自动提示优化方法主要针对纯文本评估，在多模态场景研究不足，多模态模型因上下文窗口限制难以高效优化提示。

Method: 提出BLPO双级提示优化框架，将图像转换为文本表示并联合优化评估提示和图像到文本的提示。

Result: 在四个数据集和三个大语言模型评估器上的实验证明了方法的有效性。

Conclusion: 提出的BLPO框架能有效解决多模态大语言模型作为评估器时因上下文窗口限制导致的提示优化难题。

Abstract: Large language models (LLMs) have become widely adopted as automated judges for evaluating AI-generated content. Despite their success, aligning LLM-based evaluations with human judgments remains challenging. While supervised fine-tuning on human-labeled data can improve alignment, it is costly and inflexible, requiring new training for each task or dataset. Recent progress in auto prompt optimization (APO) offers a more efficient alternative by automatically improving the instructions that guide LLM judges. However, existing APO methods primarily target text-only evaluations and remain underexplored in multimodal settings. In this work, we study auto prompt optimization for multimodal LLM-as-a-judge, particularly for evaluating AI-generated images. We identify a key bottleneck: multimodal models can only process a limited number of visual examples due to context window constraints, which hinders effective trial-and-error prompt refinement. To overcome this, we propose BLPO, a bi-level prompt optimization framework that converts images into textual representations while preserving evaluation-relevant visual cues. Our bi-level optimization approach jointly refines the judge prompt and the I2T prompt to maintain fidelity under limited context budgets. Experiments on four datasets and three LLM judges demonstrate the effectiveness of our method.

</details>


### [8] [AgentNoiseBench: Benchmarking Robustness of Tool-Using LLM Agents Under Noisy Condition](https://arxiv.org/abs/2602.11348)
*Ruipeng Wang,Yuxin Chen,Yukai Wang,Chang Wu,Junfeng Fang,Xiaodong Cai,Qi Gu,Hui Su,An Zhang,Xiang Wang,Xunliang Cai,Tat-Seng Chua*

Main category: cs.AI

TL;DR: 现有大语言模型在基准测试表现佳，但现实部署表现不佳，本文提出 AgentNoiseBench 框架评估智能体模型在噪声环境下的鲁棒性并进行评估，揭示模型对环境扰动的敏感性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在基准测试和现实部署中表现存在差异，主要因现有训练和评估范式基于理想化假设，忽略现实交互的随机性和噪声，需评估模型在噪声环境下的鲁棒性。

Method: 先分析现实场景中的偏差和不确定性，将环境噪声分为用户噪声和工具噪声；开发自动化管道，在现有以智能体为中心的基准测试中注入可控噪声；对多种架构和参数规模的模型进行评估。

Result: 不同噪声条件下模型性能有一致变化，表明当前智能体模型对现实环境扰动敏感。

Conclusion: AgentNoiseBench 框架可有效评估智能体模型在噪声环境下的鲁棒性，当前模型对现实环境扰动较敏感。

Abstract: Recent advances in large language models have enabled LLM-based agents to achieve strong performance on a variety of benchmarks. However, their performance in real-world deployments often that observed on benchmark settings, especially in complex and imperfect environments. This discrepancy largely arises because prevailing training and evaluation paradigms are typically built on idealized assumptions, overlooking the inherent stochasticity and noise present in real-world interactions. To bridge this gap, we introduce AgentNoiseBench, a framework for systematically evaluating the robustness of agentic models under noisy environments. We first conduct an in-depth analysis of biases and uncertainties in real-world scenarios and categorize environmental noise into two primary types: user-noise and tool-noise. Building on this analysis, we develop an automated pipeline that injects controllable noise into existing agent-centric benchmarks while preserving task solvability. Leveraging this pipeline, we perform extensive evaluations across a wide range of models with diverse architectures and parameter scales. Our results reveal consistent performance variations under different noise conditions, highlighting the sensitivity of current agentic models to realistic environmental perturbations.

</details>


### [9] [Pushing Forward Pareto Frontiers of Proactive Agents with Behavioral Agentic Optimization](https://arxiv.org/abs/2602.11351)
*Yihang Yao,Zhepeng Cen,Haohong Lin,Shiqi Liu,Zuxin Liu,Jiacheng Zhu,Zhang-Wei Hong,Laixi Shi,Ding Zhao*

Main category: cs.AI

TL;DR: 提出BAO框架解决主动大语言模型代理在多轮设置中的任务性能与用户参与度平衡问题，在多任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有主动大语言模型代理训练管道难以平衡任务性能和用户参与度。

Method: 提出BAO框架，结合行为增强和行为正则化。

Result: BAO在多个任务上大幅超越主动代理强化学习基线，性能与商业大语言模型代理相当甚至更优。

Conclusion: BAO在复杂多轮场景中训练主动、符合用户期望的大语言模型代理方面很有效。

Abstract: Proactive large language model (LLM) agents aim to actively plan, query, and interact over multiple turns, enabling efficient task completion beyond passive instruction following and making them essential for real-world, user-centric applications. Agentic reinforcement learning (RL) has recently emerged as a promising solution for training such agents in multi-turn settings, allowing interaction strategies to be learned from feedback. However, existing pipelines face a critical challenge in balancing task performance with user engagement, as passive agents can not efficiently adapt to users' intentions while overuse of human feedback reduces their satisfaction. To address this trade-off, we propose BAO, an agentic RL framework that combines behavior enhancement to enrich proactive reasoning and information-gathering capabilities with behavior regularization to suppress inefficient or redundant interactions and align agent behavior with user expectations. We evaluate BAO on multiple tasks from the UserRL benchmark suite, and demonstrate that it substantially outperforms proactive agentic RL baselines while achieving comparable or even superior performance to commercial LLM agents, highlighting its effectiveness for training proactive, user-aligned LLM agents in complex multi-turn scenarios. Our website: https://proactive-agentic-rl.github.io/.

</details>


### [10] [ReplicatorBench: Benchmarking LLM Agents for Replicability in Social and Behavioral Sciences](https://arxiv.org/abs/2602.11354)
*Bang Nguyen,Dominik Soós,Qian Ma,Rochana R. Obadage,Zack Ranjan,Sai Koneru,Timothy M. Errington,Shakhlo Nematova,Sarah Rajtmajer,Jian Wu,Meng Jiang*

Main category: cs.AI

TL;DR: 提出ReplicatorBench基准测试评估AI代理研究复制能力，开发ReplicatorAgent框架并评估，发现当前LLM代理检索资源有困难。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注计算方面，未考虑新数据可用性、缺乏真实多样性且只评估结果，不能评估识别不可复制研究的能力。

Method: 引入ReplicatorBench，涵盖社会和行为科学中可复制和不可复制研究声明；开发ReplicatorAgent框架；在四个大语言模型上评估，考虑编程语言设计选择和代码访问级别。

Result: 当前LLM代理能有效设计和执行计算实验，但在检索复制所需资源（如新数据）方面存在困难。

Conclusion: 提出的新基准可更好评估AI代理研究复制能力，指出当前LLM代理在资源检索上的不足。

Abstract: The literature has witnessed an emerging interest in AI agents for automated assessment of scientific papers. Existing benchmarks focus primarily on the computational aspect of this task, testing agents' ability to reproduce or replicate research outcomes when having access to the code and data. This setting, while foundational, (1) fails to capture the inconsistent availability of new data for replication as opposed to reproduction, and (2) lacks ground-truth diversity by focusing only on reproducible papers, thereby failing to evaluate an agent's ability to identify non-replicable research. Furthermore, most benchmarks only evaluate outcomes rather than the replication process. In response, we introduce ReplicatorBench, an end-to-end benchmark, including human-verified replicable and non-replicable research claims in social and behavioral sciences for evaluating AI agents in research replication across three stages: (1) extraction and retrieval of replication data; (2) design and execution of computational experiments; and (3) interpretation of results, allowing a test of AI agents' capability to mimic the activities of human replicators in real world. To set a baseline of AI agents' capability, we develop ReplicatorAgent, an agentic framework equipped with necessary tools, like web search and iterative interaction with sandboxed environments, to accomplish tasks in ReplicatorBench. We evaluate ReplicatorAgent across four underlying large language models (LLMs), as well as different design choices of programming language and levels of code access. Our findings reveal that while current LLM agents are capable of effectively designing and executing computational experiments, they struggle with retrieving resources, such as new data, necessary to replicate a claim. All code and data are publicly available at https://github.com/CenterForOpenScience/llm-benchmarking.

</details>


### [11] [Causal-JEPA: Learning World Models through Object-Level Latent Interventions](https://arxiv.org/abs/2602.11389)
*Heejeong Nam,Quentin Le Lidec,Lucas Maes,Yann LeCun,Randall Balestriero*

Main category: cs.AI

TL;DR: 提出C - JEPA对象中心世界模型，扩展了掩码联合嵌入预测，在视觉问答和代理控制任务取得成果，并证明其有因果归纳偏差。


<details>
  <summary>Details</summary>
Motivation: 现有以对象为中心表示不足以捕捉依赖交互的动态，世界模型需强大关系理解。

Method: 提出C - JEPA，应用对象级掩码，从其他对象推断对象状态，诱导潜在干预。

Result: 在视觉问答中持续提升，反事实推理绝对提升约20%；在代理控制任务中仅用基于补丁世界模型所需1%潜在输入特征就有可比性能。

Conclusion: 对象级掩码通过潜在干预诱导因果归纳偏差。

Abstract: World models require robust relational understanding to support prediction, reasoning, and control. While object-centric representations provide a useful abstraction, they are not sufficient to capture interaction-dependent dynamics. We therefore propose C-JEPA, a simple and flexible object-centric world model that extends masked joint embedding prediction from image patches to object-centric representations. By applying object-level masking that requires an object's state to be inferred from other objects, C-JEPA induces latent interventions with counterfactual-like effects and prevents shortcut solutions, making interaction reasoning essential. Empirically, C-JEPA leads to consistent gains in visual question answering, with an absolute improvement of about 20\% in counterfactual reasoning compared to the same architecture without object-level masking. On agent control tasks, C-JEPA enables substantially more efficient planning by using only 1\% of the total latent input features required by patch-based world models, while achieving comparable performance. Finally, we provide a formal analysis demonstrating that object-level masking induces a causal inductive bias via latent interventions. Our code is available at https://github.com/galilai-group/cjepa.

</details>


### [12] [GHOST: Unmasking Phantom States in Mamba2 via Grouped Hidden-state Output-aware Selection & Truncation](https://arxiv.org/abs/2602.11408)
*Michael Menezes,Anastasios Kyrillidis*

Main category: cs.AI

TL;DR: 提出GHOST结构化剪枝框架解决Mamba2推理开销大问题，在多参数模型上实现降维且增加少量困惑度。


<details>
  <summary>Details</summary>
Motivation: Mamba2扩展状态维度提升时间建模能力，但自回归生成时推理开销大，标准剪枝方法无法解决此瓶颈。

Method: 引入GHOST结构化剪枝框架，仅用前向传播统计近似控制理论平衡截断，联合测量可控性和可观测性。

Result: 在130M到2.7B参数模型上，实现50%状态维度降低，WikiText - 2上困惑度约增加1点。

Conclusion: GHOST框架在不使用反向传播的情况下，能达到接近基于梯度方法的保真度，有效解决Mamba2推理开销问题。

Abstract: While Mamba2's expanded state dimension enhances temporal modeling, it incurs substantial inference overhead that saturates bandwidth during autoregressive generation. Standard pruning methods fail to address this bottleneck: unstructured sparsity leaves activations dense, magnitude-based selection ignores runtime dynamics, and gradient-based methods impose prohibitive costs. We introduce GHOST (Grouped Hidden-state Output-aware Selection and Truncation), a structured pruning framework that approximates control-theoretic balanced truncation using only forward-pass statistics. By jointly measuring controllability and observability, GHOST rivals the fidelity of gradient-based methods without requiring backpropagation. As a highlight, on models ranging from 130M to 2.7B parameters, our approach achieves a 50\% state-dimension reduction with approximately 1 perplexity point increase on WikiText-2. Code is available at https://anonymous.4open.science/r/mamba2_ghost-7BCB/.

</details>


### [13] [TRACER: Trajectory Risk Aggregation for Critical Episodes in Agentic Reasoning](https://arxiv.org/abs/2602.11409)
*Sina Tayebati,Divake Kumar,Nastaran Darabi,Davide Ettori,Ranganath Krishnan,Amit Ranjan Trivedi*

Main category: cs.AI

TL;DR: 引入轨迹级不确定性度量TRACER用于人机多轮工具交互，能更好检测不确定性，代码和基准已开源。


<details>
  <summary>Details</summary>
Motivation: 现有不确定性代理聚焦单轮文本生成，无法捕捉多轮交互中轨迹级故障信号，难以估计人机多轮工具交互中AI智能体的不确定性。

Method: 引入TRACER，结合内容感知意外性、情境感知信号等，并使用尾风险函数聚合。

Result: 在$τ^2$-bench上评估，相比基线，TRACER使AUROC最多提高37.1%，AUARC最多提高55%。

Conclusion: TRACER能在复杂对话工具使用场景中更早、更准确检测不确定性。

Abstract: Estimating uncertainty for AI agents in real-world multi-turn tool-using interaction with humans is difficult because failures are often triggered by sparse critical episodes (e.g., looping, incoherent tool use, or user-agent miscoordination) even when local generation appears confident. Existing uncertainty proxies focus on single-shot text generation and therefore miss these trajectory-level breakdown signals. We introduce TRACER, a trajectory-level uncertainty metric for dual-control Tool-Agent-User interaction. TRACER combines content-aware surprisal with situational-awareness signals, semantic and lexical repetition, and tool-grounded coherence gaps, and aggregates them using a tail-focused risk functional with a MAX-composite step risk to surface decisive anomalies. We evaluate TRACER on $τ^2$-bench by predicting task failure and selective task execution. To this end, TRACER improves AUROC by up to 37.1% and AUARC by up to 55% over baselines, enabling earlier and more accurate detection of uncertainty in complex conversational tool-use settings. Our code and benchmark are available at https://github.com/sinatayebati/agent-tracer.

</details>


### [14] [Distributionally Robust Cooperative Multi-Agent Reinforcement Learning via Robust Value Factorization](https://arxiv.org/abs/2602.11437)
*Chengrui Qu,Christopher Yeh,Kishan Panaganti,Eric Mazumdar,Adam Wierman*

Main category: cs.AI

TL;DR: 提出Distributionally robust IGM (DrIGM)原则改进多智能体强化学习，在多个环境提升分布外性能。


<details>
  <summary>Details</summary>
Motivation: 现有合作多智能体强化学习在现实场景因环境不确定性不可靠。

Method: 引入DrIGM原则，推导现有价值分解架构的DrIGM兼容鲁棒变体，训练基于鲁棒Q目标。

Result: 在高保真SustainGym模拟器和星际争霸游戏环境中，方法持续提升分布外性能。

Conclusion: DrIGM原则可行，能为系统提供鲁棒性保证，改进后的方法有良好表现。

Abstract: Cooperative multi-agent reinforcement learning (MARL) commonly adopts centralized training with decentralized execution, where value-factorization methods enforce the individual-global-maximum (IGM) principle so that decentralized greedy actions recover the team-optimal joint action. However, the reliability of this recipe in real-world settings remains unreliable due to environmental uncertainties arising from the sim-to-real gap, model mismatch, and system noise. We address this gap by introducing Distributionally robust IGM (DrIGM), a principle that requires each agent's robust greedy action to align with the robust team-optimal joint action. We show that DrIGM holds for a novel definition of robust individual action values, which is compatible with decentralized greedy execution and yields a provable robustness guarantee for the whole system. Building on this foundation, we derive DrIGM-compliant robust variants of existing value-factorization architectures (e.g., VDN/QMIX/QTRAN) that (i) train on robust Q-targets, (ii) preserve scalability, and (iii) integrate seamlessly with existing codebases without bespoke per-agent reward shaping. Empirically, on high-fidelity SustainGym simulators and a StarCraft game environment, our methods consistently improve out-of-distribution performance. Code and data are available at https://github.com/crqu/robust-coMARL.

</details>


### [15] [Credit Where It is Due: Cross-Modality Connectivity Drives Precise Reinforcement Learning for MLLM Reasoning](https://arxiv.org/abs/2602.11455)
*Zhengbo Jiao,Shaobo Wang,Zifan Zhang,Wei Wang,Bing Zhao,Hu Wei,Linfeng Zhang*

Main category: cs.AI

TL;DR: 研究多模态强化学习中视觉证据整合的问题，发现高连通性标记的作用，提出 AT - RL 框架，证明推理质量取决于跨模态锚定的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大型语言模型（MLLMs）在强化学习中对视觉证据整合方式了解不足，需探索其在推理时的视觉证据整合机制。

Method: 从跨模态注意力连接的角度探索多模态强化学习，提出基于注意力拓扑图聚类的 Anchor - Token 强化学习（AT - RL）框架，选择性地强化高连通性标记。

Result: AT - RL 引入仅 1.2% 的开销，使 32B 模型在 MathVista 上超过 72B - Instruct 基线，在 STEM、视频和通用任务中均有提升；仅训练低连通性标记会导致性能严重下降。

Conclusion: 多模态推理质量由跨模态锚定的准确性决定，而非标记数量。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has significantly advanced the reasoning capabilities of Multimodal Large Language Models (MLLMs), yet how visual evidence is integrated during reasoning remains poorly understood. We explore multimodal RLVR through the lens of cross-modal attention connectivity and find that only a small fraction of tokens (approximately 15%) exhibit strong visual-textual coupling. These high-connectivity tokens act as anchors that ground reasoning in the image, while the majority follow linguistic patterns. During RLVR training, credit assignment naturally concentrates on these anchors, sharpening their visual grounding over time. Building on this insight, we propose Anchor-Token Reinforcement Learning (AT-RL), a lightweight framework that selectively reinforces high-connectivity tokens via graph-based clustering of attention topology. Evaluated across the series (3B-32B), AT-RL introduces only 1.2% overhead yet enables the 32B model to surpass the 72B-Instruct baseline on MathVista (80.2), with consistent gains observed across STEM, video and general tasks. Conversely, training solely on low-connectivity tokens causes severe degradation, confirming that effective multimodal RL hinges on precise credit assignment to visual anchors. Our work reveals that reasoning quality is governed not by token quantity but by the fidelity of cross-modal anchoring.

</details>


### [16] [AgentLeak: A Full-Stack Benchmark for Privacy Leakage in Multi-Agent LLM Systems](https://arxiv.org/abs/2602.11510)
*Faouzi El Yagoubi,Ranwa Al Mallah,Godwin Badu-Marfo*

Main category: cs.AI

TL;DR: 提出AgentLeak基准测试多智能体大语言模型隐私泄漏，发现多智能体配置虽减少输出泄漏但增加内部泄漏，Claude 3.5 Sonnet泄漏率低，强调需内部隐私保护框架。


<details>
  <summary>Details</summary>
Motivation: 当前基准无法衡量多智能体大语言模型系统的隐私风险。

Method: 引入AgentLeak基准，含1000个场景、32类攻击分类和三层检测管道，测试5种模型。

Result: 多智能体配置降低单通道输出泄漏但增加系统总暴露，内部通道泄漏严重，Claude 3.5 Sonnet泄漏率最低，C2 > C1模式一致。

Conclusion: 需要纳入内部通道隐私保护的协调框架，加强智能体间通信的隐私控制。

Abstract: Multi-agent Large Language Model (LLM) systems create privacy risks that current benchmarks cannot measure. When agents coordinate on tasks, sensitive data passes through inter-agent messages, shared memory, and tool arguments; pathways that output-only audits never inspect. We introduce AgentLeak, to the best of our knowledge the first full-stack benchmark for privacy leakage covering internal channels, spanning 1,000 scenarios across healthcare, finance, legal, and corporate domains, paired with a 32-class attack taxonomy and three-tier detection pipeline. Testing GPT-4o, GPT-4o-mini, Claude 3.5 Sonnet, Mistral Large, and Llama 3.3 70B across 4,979 traces reveals that multi-agent configurations reduce per-channel output leakage (C1: 27.2% vs 43.2% in single-agent) but introduce unmonitored internal channels that raise total system exposure to 68.9% (OR-aggregated across C1, C2, C5). Internal channels account for most of this gap: inter-agent messages (C2) leak at 68.8%, compared to 27.2% on C1 (output channel). This means that output-only audits miss 41.7% of violations. Claude 3.5 Sonnet, which emphasizes safety alignment in its design, achieves the lowest leakage rates on both external (3.3%) and internal (28.1%) channels, suggesting that model-level safety training may transfer to internal channel protection. Across all five models and four domains, the pattern C2 > C1 holds consistently, confirming that inter-agent communication is the primary vulnerability. These findings underscore the need for coordination frameworks that incorporate internal-channel privacy protections and enforce privacy controls on inter-agent communication.

</details>


### [17] [Human-Inspired Continuous Learning of Internal Reasoning Processes: Learning How to Think for Adaptive AI Systems](https://arxiv.org/abs/2602.11516)
*Hong Su*

Main category: cs.AI

TL;DR: 提出受人类启发的连续学习框架，将推理等统一，以内部思维过程为学习对象，实验显示在任务中减少平均运行时间。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视内部推理结构等持续优化，为开发能在动态环境持续适应的AI系统，需学习内部推理过程。

Method: 提出人类启发的连续学习框架，统一推理、行动、反思和验证，将内部思维过程作为学习对象，记录推理轨迹和交互为学习材料，支持替换预定义逻辑，引入分层学习机制。

Result: 在温度传感器异常检测任务实验中，引入内部过程学习使平均运行时间减少23.9%。

Conclusion: 该框架能让系统在保持运行稳定的同时，逐步进化内部认知架构。

Abstract: Learning internal reasoning processes is crucial for developing AI systems capable of sustained adaptation in dynamic real-world environments. However, most existing approaches primarily emphasize learning task-specific outputs or static knowledge representations, while overlooking the continuous refinement of internal reasoning structures, action scheduling policies, and learning mechanisms themselves. In this paper, we propose a human-inspired continuous learning framework that unifies reasoning, action, reflection, and verification within a sequential reasoning model enhanced by parallel learning. The framework explicitly treats internal thinking processes as primary learning objects. It systematically records internal reasoning trajectories and environmental interactions as structured learning material, enabling the system to optimize not only task-level content but also the organization, scheduling, and evolution of reasoning activities. This design realizes learning alongside processing, allowing cognitive structures to improve during execution. Furthermore, the framework supports controlled replacement of predefined logic with learned procedures and introduces a hierarchical learning-to-learn mechanism that jointly adapts task-level parameters and learning strategies. As a result, the system progressively evolves its internal cognitive architecture while preserving operational stability. Experimental results on a temperature sensor abnormality detection task show that incorporating internal-process learning reduces average runtime by 23.9%.

</details>


### [18] [CausalAgent: A Conversational Multi-Agent System for End-to-End Causal Inference](https://arxiv.org/abs/2602.11527)
*Jiawei Zhu,Wei Chen,Ruichu Cai*

Main category: cs.AI

TL;DR: 针对传统因果分析流程存在技术障碍的问题，提出因果智能体CausalAgent，通过多智能体系统等创新集成实现端到端因果推理，降低因果分析门槛。


<details>
  <summary>Details</summary>
Motivation: 传统因果分析流程存在显著技术壁垒，要求研究者具备统计学和计算机科学双重背景，需手动选择算法、处理数据质量问题和解释复杂结果。

Method: 提出CausalAgent，创新性地集成Multi - Agent Systems (MAS)、Retrieval - Augmented Generation (RAG)和Model Context Protocol (MCP)，通过自然语言交互实现从数据清理到报告生成的自动化。

Result: 用户只需上传数据集并以自然语言提问，即可获得严谨、交互式的分析报告。

Conclusion: CausalAgent作为一种以用户为中心的人机协作新范式，明确建模分析流程，通过交互式可视化显著降低因果分析的入门门槛，同时保证过程的严谨性和可解释性。

Abstract: Causal inference holds immense value in fields such as healthcare, economics, and social sciences. However, traditional causal analysis workflows impose significant technical barriers, requiring researchers to possess dual backgrounds in statistics and computer science, while manually selecting algorithms, handling data quality issues, and interpreting complex results. To address these challenges, we propose CausalAgent, a conversational multi-agent system for end-to-end causal inference. The system innovatively integrates Multi-Agent Systems (MAS), Retrieval-Augmented Generation (RAG), and the Model Context Protocol (MCP) to achieve automation from data cleaning and causal structure learning to bias correction and report generation through natural language interaction. Users need only upload a dataset and pose questions in natural language to receive a rigorous, interactive analysis report. As a novel user-centered human-AI collaboration paradigm, CausalAgent explicitly models the analysis workflow. By leveraging interactive visualizations, it significantly lowers the barrier to entry for causal analysis while ensuring the rigor and interpretability of the process.

</details>


### [19] [Budget-Constrained Agentic Large Language Models: Intention-Based Planning for Costly Tool Use](https://arxiv.org/abs/2602.11541)
*Hanbing Liu,Chunhao Tian,Nan An,Ziyuan Wang,Pinyan Lu,Changyuan Yu,Qi Qi*

Main category: cs.AI

TL;DR: 研究预算约束下工具增强代理，提出INTENT框架解决相关挑战，在成本增强基准测试中表现良好且具鲁棒性


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在严格货币预算下调用外部工具解决多步任务的难题，传统直接规划因各种因素难以实施

Method: 提出INTENT推理时间规划框架，利用意图感知分层世界模型预测工具使用、进行风险校准成本并在线引导决策

Result: 在成本增强的StableToolBench上，INTENT严格满足预算可行性，大幅提高任务成功率，在动态市场变化下保持鲁棒性

Conclusion: INTENT能有效应对预算约束下工具增强代理相关问题，在任务执行和鲁棒性上表现出色

Abstract: We study budget-constrained tool-augmented agents, where a large language model must solve multi-step tasks by invoking external tools under a strict monetary budget. We formalize this setting as sequential decision making in context space with priced and stochastic tool executions, making direct planning intractable due to massive state-action spaces, high variance of outcomes and prohibitive exploration cost. To address these challenges, we propose INTENT, an inference-time planning framework that leverages an intention-aware hierarchical world model to anticipate future tool usage, risk-calibrated cost, and guide decisions online. Across cost-augmented StableToolBench, INTENT strictly enforces hard budget feasibility while substantially improving task success over baselines, and remains robust under dynamic market shifts such as tool price changes and varying budgets.

</details>


### [20] [SemaPop: Semantic-Persona Conditioned Population Synthesis](https://arxiv.org/abs/2602.11569)
*Zhenlin Qin,Yancheng Ling,Leizhen Wang,Francisco Câmara Pereira,Zhenliang Ma*

Main category: cs.AI

TL;DR: 提出SemaPop模型用于人口合成，以大语言模型结合生成式建模，通过实验证明其性能优势及有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在语义条件人口生成上存在不足，需捕捉调查数据中的抽象行为模式。

Method: 提出SemaPop模型，结合大语言模型与生成式人口建模，以Wasserstein GAN为骨干实现SemaPop - GAN，引入边际正则化。

Result: SemaPop - GAN生成性能提升，与目标分布更匹配，保持样本可行性和多样性；消融实验证实语义角色条件和架构设计的作用。

Conclusion: SemaPop - GAN能通过有效融合语义和统计信息实现可控、可解释的人口合成，为生成式人口预测系统提供基础。

Abstract: Population synthesis is a critical component of individual-level socio-economic simulation, yet remains challenging due to the need to jointly represent statistical structure and latent behavioral semantics. Existing population synthesis approaches predominantly rely on structured attributes and statistical constraints, leaving a gap in semantic-conditioned population generation that can capture abstract behavioral patterns implicitly in survey data. This study proposes SemaPop, a semantic-statistical population synthesis model that integrates large language models (LLMs) with generative population modeling. SemaPop derives high-level persona representations from individual survey records and incorporates them as semantic conditioning signals for population generation, while marginal regularization is introduced to enforce alignment with target population marginals. In this study, the framework is instantiated using a Wasserstein GAN with gradient penalty (WGAN-GP) backbone, referred to as SemaPop-GAN. Extensive experiments demonstrate that SemaPop-GAN achieves improved generative performance, yielding closer alignment with target marginal and joint distributions while maintaining sample-level feasibility and diversity under semantic conditioning. Ablation studies further confirm the contribution of semantic persona conditioning and architectural design choices to balancing marginal consistency and structural realism. These results demonstrate that SemaPop-GAN enables controllable and interpretable population synthesis through effective semantic-statistical information fusion. SemaPop-GAN also provides a promising modular foundation for developing generative population projection systems that integrate individual-level behavioral semantics with population-level statistical constraints.

</details>


### [21] [Learning to Configure Agentic AI Systems](https://arxiv.org/abs/2602.11574)
*Aditya Taparia,Som Sagar,Ransalu Senanayake*

Main category: cs.AI

TL;DR: 本文提出ARC学习轻量级分层策略动态配置大语言模型代理系统，在多个基准测试中表现优于基线，证明逐查询配置代理的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型代理系统配置采用固定模板或手动启发式方法，导致行为脆弱和计算资源浪费。

Method: 将代理配置问题表述为逐查询决策问题，使用强化学习学习轻量级分层策略动态配置。

Result: 在推理和工具增强问答的多个基准测试中，学习到的策略始终优于手动设计和其他基线，任务准确率最高提升25%，并降低了令牌和运行时成本。

Conclusion: 逐查询学习代理配置是“一刀切”设计的有力替代方案。

Abstract: Configuring LLM-based agent systems involves choosing workflows, tools, token budgets, and prompts from a large combinatorial design space, and is typically handled today by fixed large templates or hand-tuned heuristics. This leads to brittle behavior and unnecessary compute, since the same cumbersome configuration is often applied to both easy and hard input queries. We formulate agent configuration as a query-wise decision problem and introduce ARC (Agentic Resource & Configuration learner), which learns a light-weight hierarchical policy using reinforcement learning to dynamically tailor these configurations. Across multiple benchmarks spanning reasoning and tool-augmented question answering, the learned policy consistently outperforms strong hand-designed and other baselines, achieving up to 25% higher task accuracy while also reducing token and runtime costs. These results demonstrate that learning per-query agent configurations is a powerful alternative to "one size fits all" designs.

</details>


### [22] [The Five Ws of Multi-Agent Communication: Who Talks to Whom, When, What, and Why -- A Survey from MARL to Emergent Language and LLMs](https://arxiv.org/abs/2602.11583)
*Jingdi Chen,Hanqing Yang,Zongjun Liu,Carlee Joe-Wong*

Main category: cs.AI

TL;DR: 本文综述多智能体通信（MA - Comm），追溯三种范式下通信方法演变，指出不同选择对通信设计的影响，提炼设计模式与挑战。


<details>
  <summary>Details</summary>
Motivation: 多智能体顺序决策在动态、部分可观测环境中，通信重要但现有通信协议存在任务特定、难解释、难泛化等问题，需要进行研究综述。

Method: 通过五个W（谁与谁通信、通信内容、通信时间、通信为何有益）来综述多智能体通信，追溯其在多智能体强化学习（MARL）、新兴语言（EL）和大语言模型（LLM）三种范式下的演变。

Result: 明确不同范式下通信方法的特点、面临问题，指出不同选择对通信设计的影响及主要权衡点。

Conclusion: 提炼出实用设计模式和开放挑战，以支持结合学习、语言和控制的可扩展且可解释的多智能体协作混合系统。

Abstract: Multi-agent sequential decision-making powers many real-world systems, from autonomous vehicles and robotics to collaborative AI assistants. In dynamic, partially observable environments, communication is often what reduces uncertainty and makes collaboration possible. This survey reviews multi-agent communication (MA-Comm) through the Five Ws: who communicates with whom, what is communicated, when communication occurs, and why communication is beneficial. This framing offers a clean way to connect ideas across otherwise separate research threads. We trace how communication approaches have evolved across three major paradigms. In Multi-Agent Reinforcement Learning (MARL), early methods used hand-designed or implicit protocols, followed by end-to-end learned communication optimized for reward and control. While successful, these protocols are frequently task-specific and hard to interpret, motivating work on Emergent Language (EL), where agents can develop more structured or symbolic communication through interaction. EL methods, however, still struggle with grounding, generalization, and scalability, which has fueled recent interest in large language models (LLMs) that bring natural language priors for reasoning, planning, and collaboration in more open-ended settings. Across MARL, EL, and LLM-based systems, we highlight how different choices shape communication design, where the main trade-offs lie, and what remains unsolved. We distill practical design patterns and open challenges to support future hybrid systems that combine learning, language, and control for scalable and interpretable multi-agent collaboration.

</details>


### [23] [MAPLE: Modality-Aware Post-training and Learning Ecosystem](https://arxiv.org/abs/2602.11596)
*Nikhil Verma,Minjung Kim,JooYoung Yoo,Kyung-Min Jin,Manasa Bharadwaj,Kevin Ferreira,Ko Keun Kim,Youngjoon Kim*

Main category: cs.AI

TL;DR: 提出MAPLE生态系统用于多模态强化学习后训练，缩小了单/多模态准确性差距，加速收敛并保持稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习后训练管道忽略任务对模态的实际需求，导致策略梯度方差增大、收敛缓慢和鲁棒性下降。

Method: 引入MAPLE生态系统，包括MAPLE - bench基准、MAPO优化框架、自适应加权和课程调度。

Result: MAPLE缩小单/多模态精度差距30.24%，收敛速度提高3.18倍，在不同信号组合下保持稳定。

Conclusion: MAPLE是适用于多模态强化学习后训练的完整解决方案。

Abstract: Multimodal language models now integrate text, audio, and video for unified reasoning. Yet existing RL post-training pipelines treat all input signals as equally relevant, ignoring which modalities each task actually requires. This modality-blind training inflates policy-gradient variance, slows convergence, and degrades robustness to real-world distribution shifts where signals may be missing, added, or reweighted. We introduce MAPLE, a complete modality-aware post-training and learning ecosystem comprising: (1) MAPLE-bench, the first benchmark explicitly annotating minimal signal combinations required per task; (2) MAPO, a modality-aware policy optimization framework that stratifies batches by modality requirement to reduce gradient variance from heterogeneous group advantages; (3) Adaptive weighting and curriculum scheduling that balances and prioritizes harder signal combinations. Systematic analysis across loss aggregation, clipping, sampling, and curriculum design establishes MAPO's optimal training strategy. Adaptive weighting and curriculum focused learning further boost performance across signal combinations. MAPLE narrows uni/multi-modal accuracy gaps by 30.24%, converges 3.18x faster, and maintains stability across all modality combinations under realistic reduced signal access. MAPLE constitutes a complete recipe for deployment-ready multimodal RL post-training.

</details>


### [24] [scPilot: Large Language Model Reasoning Toward Automated Single-Cell Analysis and Discovery](https://arxiv.org/abs/2602.11609)
*Yiming Gao,Zhen Wang,Jefferson Chen,Mark Antkowiak,Mengzhou Hu,JungHo Kong,Dexter Pratt,Jieyuan Liu,Enze Ma,Zhiting Hu,Eric P. Xing*

Main category: cs.AI

TL;DR: 提出scPilot框架进行组学原生推理，发布scBench评估，实验显示可提升准确率并实现可审计单细胞分析。


<details>
  <summary>Details</summary>
Motivation: 为实现大语言模型直接检查单细胞RNA测序数据和生物信息学工具进行自然语言交流，解决核心单细胞分析问题。

Method: 将核心单细胞分析转化为逐步推理问题，发布scBench评估推理能力。

Result: 迭代组学原生推理提升细胞类型注释平均准确率11%，Gemini - 2.5 - Pro减少轨迹图编辑距离30%，生成透明推理轨迹。

Conclusion: scPilot让大语言模型基于原始组学数据，实现可审计、可解释和有诊断信息的单细胞分析。

Abstract: We present scPilot, the first systematic framework to practice omics-native reasoning: a large language model (LLM) converses in natural language while directly inspecting single-cell RNA-seq data and on-demand bioinformatics tools. scPilot converts core single-cell analyses, i.e., cell-type annotation, developmental-trajectory reconstruction, and transcription-factor targeting, into step-by-step reasoning problems that the model must solve, justify, and, when needed, revise with new evidence.
  To measure progress, we release scBench, a suite of 9 expertly curated datasets and graders that faithfully evaluate the omics-native reasoning capability of scPilot w.r.t various LLMs. Experiments with o1 show that iterative omics-native reasoning lifts average accuracy by 11% for cell-type annotation and Gemini-2.5-Pro cuts trajectory graph-edit distance by 30% versus one-shot prompting, while generating transparent reasoning traces explain marker gene ambiguity and regulatory logic. By grounding LLMs in raw omics data, scPilot enables auditable, interpretable, and diagnostically informative single-cell analyses.
  Code, data, and package are available at https://github.com/maitrix-org/scPilot

</details>


### [25] [When Agents Disagree With Themselves: Measuring Behavioral Consistency in LLM-Based Agents](https://arxiv.org/abs/2602.11619)
*Aman Mehta*

Main category: cs.AI

TL;DR: 运行同一个大语言模型（LLM）智能体执行相同任务，行为常不一致。研究发现其行为方差与失败相关，方差源于早期决策，监测行为一致性可提升智能体可靠性。


<details>
  <summary>Details</summary>
Motivation: 探究同一LLM智能体在相同任务上是否会有相同行为，以及行为一致性与任务结果的关系。

Method: 在HotpotQA上对三个模型（Llama 3.1 70B、GPT - 4o和Claude Sonnet 4.5）进行3000次智能体运行实验。

Result: ReAct风格智能体平均每10次运行产生2.0 - 4.2个不同动作序列；行为一致的任务准确率80 - 92%，高度不一致的任务准确率仅25 - 60%；69%的分歧发生在第二步（首次搜索查询）。

Conclusion: 执行期间监测行为一致性可实现早期错误检测，提高智能体可靠性。

Abstract: Run the same LLM agent on the same task twice: do you get the same behavior? We find the answer is often no. In a study of 3,000 agent runs across three models (Llama 3.1 70B, GPT-4o, and Claude Sonnet 4.5) on HotpotQA, we observe that ReAct-style agents produce 2.0--4.2 distinct action sequences per 10 runs on average, even with identical inputs. More importantly, this variance predicts failure: tasks with consistent behavior ($\leq$2 unique paths) achieve 80--92% accuracy, while highly inconsistent tasks ($\geq$6 unique paths) achieve only 25--60%, a 32--55 percentage point gap depending on model. We trace variance to early decisions: 69% of divergence occurs at step 2, the first search query. Our results suggest that monitoring behavioral consistency during execution could enable early error detection and improve agent reliability.

</details>


### [26] [Neuro-Symbolic Multitasking: A Unified Framework for Discovering Generalizable Solutions to PDE Families](https://arxiv.org/abs/2602.11630)
*Yipeng Huang,Dejun Xu,Zexin Lin,Zhenzhong Wang,Min Jiang*

Main category: cs.AI

TL;DR: 提出神经辅助多任务符号PDE求解器框架NMIPS解决PDE族问题，实验显示精度提升且有可解释解。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法求解PDE族计算成本高，机器学习PDE求解器缺乏可解释性。

Method: 提出NMIPS框架，用多因子优化同时发现PDE解析解，设计仿射转移方法避免重复求解。

Result: 实验结果显示比现有基线有显著改进，精度提升达约35.7%，且提供可解释解析解。

Conclusion: NMIPS框架能有效解决PDE族问题，在精度和可解释性上有优势。

Abstract: Solving Partial Differential Equations (PDEs) is fundamental to numerous scientific and engineering disciplines. A common challenge arises from solving the PDE families, which are characterized by sharing an identical mathematical structure but varying in specific parameters. Traditional numerical methods, such as the finite element method, need to independently solve each instance within a PDE family, which incurs massive computational cost. On the other hand, while recent advancements in machine learning PDE solvers offer impressive computational speed and accuracy, their inherent ``black-box" nature presents a considerable limitation. These methods primarily yield numerical approximations, thereby lacking the crucial interpretability provided by analytical expressions, which are essential for deeper scientific insight. To address these limitations, we propose a neuro-assisted multitasking symbolic PDE solver framework for PDE family solving, dubbed NMIPS. In particular, we employ multifactorial optimization to simultaneously discover the analytical solutions of PDEs. To enhance computational efficiency, we devise an affine transfer method by transferring learned mathematical structures among PDEs in a family, avoiding solving each PDE from scratch. Experimental results across multiple cases demonstrate promising improvements over existing baselines, achieving up to a $\sim$35.7% increase in accuracy while providing interpretable analytical solutions.

</details>


### [27] [Do MLLMs Really Understand Space? A Mathematical Reasoning Evaluation](https://arxiv.org/abs/2602.11635)
*Shuo Lu,Jianjie Cheng,Yinuo Xu,Yongcan Yu,Lijun Sheng,Peijie Wang,Siru Jiang,Yongguan Hu,Run Ling,Yihua Shao,Ao Ma,Wei Feng,Lingxiao He,Meng Wang,Qianlong Xie,Xingxing Wang,Ran He,Jian Liang*

Main category: cs.AI

TL;DR: 本文指出多模态大语言模型在数学空间推理上表现不佳，提出MathSpatial框架评估和改进其空间推理能力，实验表明微调模型效果良好。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在数学空间推理能力不明，与人类表现存在较大差距，需研究改进。

Method: 构建MathSpatial框架，包含MathSpatial - Bench基准、MathSpatial - Corpus训练数据集和MathSpatial - SRT推理模型。

Result: 在MathSpatial上微调Qwen2.5 - VL - 7B模型达到有竞争力的准确率，同时减少25%的token。

Conclusion: MathSpatial是首个分离感知和推理的大规模资源，能精确测量和全面理解多模态大语言模型的数学空间推理能力。

Abstract: Multimodal large language models (MLLMs) have achieved strong performance on perception-oriented tasks, yet their ability to perform mathematical spatial reasoning, defined as the capacity to parse and manipulate two- and three-dimensional relations, remains unclear. Humans easily solve textbook-style spatial reasoning problems with over 95\% accuracy, but we find that most leading MLLMs fail to reach even 60\% on the same tasks. This striking gap highlights spatial reasoning as a fundamental weakness of current models. To investigate this gap, we present MathSpatial, a unified framework for evaluating and improving spatial reasoning in MLLMs. MathSpatial includes three complementary components: (i) MathSpatial-Bench, a benchmark of 2K problems across three categories and eleven subtypes, designed to isolate reasoning difficulty from perceptual noise; (ii) MathSpatial-Corpus, a training dataset of 8K additional problems with verified solutions; and (iii) MathSpatial-SRT, which models reasoning as structured traces composed of three atomic operations--Correlate, Constrain, and Infer. Experiments show that fine-tuning Qwen2.5-VL-7B on MathSpatial achieves competitive accuracy while reducing tokens by 25\%. MathSpatial provides the first large-scale resource that disentangles perception from reasoning, enabling precise measurement and comprehensive understanding of mathematical spatial reasoning in MLLMs.

</details>


### [28] [Quark Medical Alignment: A Holistic Multi-Dimensional Alignment and Collaborative Optimization Paradigm](https://arxiv.org/abs/2602.11661)
*Tianxiang Xu,Jiayi Liu,Yixuan Tong,Jialu Xu,Yunqing Wei,Kaiwen Feng,PanPan Hou,Kangping Yin,Jiyuan Hu,Hao Zhou,Zhenxin Ma,Jian Xu,Guanjun Jiang*

Main category: cs.AI

TL;DR: 现有强化学习范式用于医学问答存在不匹配问题，本文提出鲁棒医学对齐范式并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习范式用于医学问答存在范式不匹配问题，如人类反馈强化学习成本高、可验证奖励强化学习缺乏有效验证器，且医学对齐多目标奖励信号存在问题。

Method: 构建整体多维医学对齐矩阵，为迭代优化提供细粒度监督信号；提出统一优化机制，采用参考冻结归一化对齐奖励尺度，实施三因素自适应动态加权策略。

Result: 实验结果证明所提范式在现实医学场景评估中有效。

Conclusion: 所提范式为垂直领域复杂对齐建立了新范式。

Abstract: While reinforcement learning for large language model alignment has progressed rapidly in recent years, transferring these paradigms to high-stakes medical question answering reveals a fundamental paradigm mismatch. Reinforcement Learning from Human Feedback relies on preference annotations that are prohibitively expensive and often fail to reflect the absolute correctness of medical facts. Reinforcement Learning from Verifiable Rewards lacks effective automatic verifiers and struggles to handle complex clinical contexts. Meanwhile, medical alignment requires the simultaneous optimization of correctness, safety, and compliance, yet multi-objective heterogeneous reward signals are prone to scale mismatch and optimization conflicts.To address these challenges, we propose a robust medical alignment paradigm. We first construct a holistic multi-dimensional medical alignment matrix that decomposes alignment objectives into four categories: fundamental capabilities, expert knowledge, online feedback, and format specifications. Within each category, we establish a closed loop of where observable metrics inform attributable diagnosis, which in turn drives optimizable rewards, thereby providing fine-grained, high-resolution supervision signals for subsequent iterative optimization. To resolve gradient domination and optimization instability problem caused by heterogeneous signals, we further propose a unified optimization mechanism. This mechanism employs Reference-Frozen Normalization to align reward scales and implements a Tri-Factor Adaptive Dynamic Weighting strategy to achieve collaborative optimization that is weakness-oriented, risk-prioritized, and redundancy-reducing. Experimental results demonstrate the effectiveness of our proposed paradigm in real-world medical scenario evaluations, establishing a new paradigm for complex alignment in vertical domains.

</details>


### [29] [Cross-Architecture Model Diffing with Crosscoders: Unsupervised Discovery of Differences Between LLMs](https://arxiv.org/abs/2602.11729)
*Thomas Jiralerspong,Trenton Bricken*

Main category: cs.AI

TL;DR: 本文首次将crosscoders应用于跨架构模型diffing，并引入DFCs技术，发现不同模型的特征，推动跨架构crosscoder模型diffing成为识别AI模型差异的有效方法。


<details>
  <summary>Details</summary>
Motivation: 模型diffing在新模型安全关键行为发现上有前景，但现有应用多在基础模型和微调模型比较，新LLM常为新架构，因此需要跨架构方法。

Method: 首次将crosscoders应用于跨架构模型diffing，引入Dedicated Feature Crosscoders (DFCs)技术。

Result: 以无监督方式发现Qwen3 - 8B等模型的特定特征。

Conclusion: 跨架构crosscoder模型diffing是识别AI模型有意义行为差异的有效方法。

Abstract: Model diffing, the process of comparing models' internal representations to identify their differences, is a promising approach for uncovering safety-critical behaviors in new models. However, its application has so far been primarily focused on comparing a base model with its finetune. Since new LLM releases are often novel architectures, cross-architecture methods are essential to make model diffing widely applicable. Crosscoders are one solution capable of cross-architecture model diffing but have only ever been applied to base vs finetune comparisons. We provide the first application of crosscoders to cross-architecture model diffing and introduce Dedicated Feature Crosscoders (DFCs), an architectural modification designed to better isolate features unique to one model. Using this technique, we find in an unsupervised fashion features including Chinese Communist Party alignment in Qwen3-8B and Deepseek-R1-0528-Qwen3-8B, American exceptionalism in Llama3.1-8B-Instruct, and a copyright refusal mechanism in GPT-OSS-20B. Together, our results work towards establishing cross-architecture crosscoder model diffing as an effective method for identifying meaningful behavioral differences between AI models.

</details>


### [30] [PhyNiKCE: A Neurosymbolic Agentic Framework for Autonomous Computational Fluid Dynamics](https://arxiv.org/abs/2602.11666)
*E Fan,Lisong Shi,Zhengtong Li,Chih-yung Wen*

Main category: cs.AI

TL;DR: 本文提出PhyNiKCE框架解决大语言模型在CFD中应用的问题，经实验验证能提升性能并适用于更广泛工业自动化。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的概率性及语义-物理脱节问题限制了自主代理在CFD中的应用，需要解决这些问题以实现更可靠的工程应用。

Method: 引入PhyNiKCE框架，将神经规划与符号验证分离，使用符号知识引擎将模拟设置视为约束满足问题，通过确定性RAG引擎执行物理约束。

Result: 在OpenFOAM实验中，相比现有基线有96%的相对改进，减少59%的自主自我修正循环，降低17%的LLM令牌消耗。

Conclusion: 将神经生成与符号约束执行分离可显著提高鲁棒性和效率，该架构为更广泛工业自动化中的可信人工智能提供了可扩展、可审计的范式。

Abstract: The deployment of autonomous agents for Computational Fluid Dynamics (CFD), is critically limited by the probabilistic nature of Large Language Models (LLMs), which struggle to enforce the strict conservation laws and numerical stability required for physics-based simulations. Reliance on purely semantic Retrieval Augmented Generation (RAG) often leads to "context poisoning," where agents generate linguistically plausible but physically invalid configurations due to a fundamental Semantic-Physical Disconnect. To bridge this gap, this work introduces PhyNiKCE (Physical and Numerical Knowledgeable Context Engineering), a neurosymbolic agentic framework for trustworthy engineering. Unlike standard black-box agents, PhyNiKCE decouples neural planning from symbolic validation. It employs a Symbolic Knowledge Engine that treats simulation setup as a Constraint Satisfaction Problem, rigidly enforcing physical constraints via a Deterministic RAG Engine with specialized retrieval strategies for solvers, turbulence models, and boundary conditions. Validated through rigorous OpenFOAM experiments on practical, non-tutorial CFD tasks using Gemini-2.5-Pro/Flash, PhyNiKCE demonstrates a 96% relative improvement over state-of-the-art baselines. Furthermore, by replacing trial-and-error with knowledge-driven initialization, the framework reduced autonomous self-correction loops by 59% while simultaneously lowering LLM token consumption by 17%. These results demonstrate that decoupling neural generation from symbolic constraint enforcement significantly enhances robustness and efficiency. While validated on CFD, this architecture offers a scalable, auditable paradigm for Trustworthy Artificial Intelligence in broader industrial automation.

</details>


### [31] [Benchmark Health Index: A Systematic Framework for Benchmarking the Benchmarks of LLMs](https://arxiv.org/abs/2602.11674)
*Longyuan Zhu,Hairan Hua,Linlin Miao,Bing Zhao*

Main category: cs.AI

TL;DR: 现有大语言模型基准测试不可靠，提出基准健康指数（BHI）框架审计评估集，系统刻画评估格局，可用于基准选择和管理。


<details>
  <summary>Details</summary>
Motivation: 当前衡量大语言模型进展的基准测试因分数膨胀和选择性报告变得不可靠，社区难以确定哪些评估结果可信。

Method: 引入BHI框架，从能力区分、抗饱和、影响三个正交互补轴审计评估集，提炼2025年91个代表性模型技术报告中的106个有效基准。

Result: 系统刻画了评估格局。

Conclusion: BHI是首个宏观量化基准健康的框架，为基准选择提供原则基础，可实现下一代评估协议的动态生命周期管理。

Abstract: Large Language Models (LLMs) are advancing rapidly, yet the benchmarks used to measure this progress are becoming increasingly unreliable. Score inflation and selective reporting have eroded the authority of standard benchmarks, leaving the community uncertain about which evaluation results remain trustworthy. We introduce the Benchmark Health Index (BHI), a pure data-driven framework for auditing evaluation sets along three orthogonal and complementary axes: (1) Capability Discrimination, measuring how sharply a benchmark separates model performance beyond noise; (2) Anti-Saturation, estimating remaining headroom before ceiling effects erode resolution and thus the benchmark's expected longevity; and (3) Impact, quantifying influence across academic and industrial ecosystems via adoption breadth and practice-shaping power. By distilling 106 validated benchmarks from the technical reports of 91 representative models in 2025, we systematically characterize the evaluation landscape. BHI is the first framework to quantify benchmark health at a macro level, providing a principled basis for benchmark selection and enabling dynamic lifecycle management for next-generation evaluation protocols.

</details>


### [32] [Right for the Wrong Reasons: Epistemic Regret Minimization for Causal Rung Collapse in LLMs](https://arxiv.org/abs/2602.11675)
*Edward Y. Chang*

Main category: cs.AI

TL;DR: 指出机器学习系统‘因错误原因而正确’问题的因果根源，提出认知后悔最小化（ERM）方法及相关架构，证明理论结果并通过实验验证效果。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习系统通过捷径获高性能但在分布变化时崩溃的问题，明确其因果根源并改进。

Method: 提出Epistemic Regret Minimization（ERM）目标，构建三层架构，包括证明物理基础定理、将ERM作为满足AGM公理的因果信念修正算子、建立故障模式分类法。

Result: 证明了真实干预分布的渐近恢复及有限样本界；实验表明Rung Collapse在推理增强模型中仍存在，高级模型难纠正，ERM反馈能恢复大部分根深蒂固的错误。

Conclusion: 所提方法能有效解决机器学习系统‘因错误原因而正确’的问题，在多种场景下有良好效果。

Abstract: Machine learning systems that are "right for the wrong reasons" achieve high performance through shortcuts that collapse under distributional shift. We show this pathology has a precise causal origin: autoregressive training provides no gradient signal to distinguish association P(Y|X) from intervention P(Y|do(X)), a failure we formalize as Rung Collapse. When outcome-based learning reinforces correct answers obtained through incorrect causal models, the agent becomes entrenched in flawed reasoning, a phenomenon we term Aleatoric Entrenchment. We propose Epistemic Regret Minimization (ERM), a belief revision objective that penalizes errors in causal reasoning independently of task success, and embed it within a three-layer architecture with three contributions grounded in knowledge representation: (1) a Physical Grounding Theorem proving that actions satisfying actuator independence implement valid do-operations, bridging action languages and do-calculus; (2) ERM as a causal belief revision operator satisfying AGM postulates, preventing entrenchment even when the agent succeeds for the wrong reasons; and (3) a failure mode taxonomy that classifies recurring reasoning errors and injects domain-independent guards, enabling cross-domain transfer. We prove asymptotic recovery of the true interventional distribution with finite-sample bounds. Experiments on 1,360 causal trap scenarios across six frontier LLMs reveal that Rung Collapse persists even in reasoning-enhanced models (3.7% for GPT-5.2), that steerability exhibits inverse scaling where advanced models resist generic correction, and that targeted ERM feedback recovers 53-59% of entrenched errors where outcome-level feedback fails.

</details>


### [33] [FlowMind: Execute-Summarize for Structured Workflow Generation from LLM Reasoning](https://arxiv.org/abs/2602.11782)
*Yihao Liu,Ziyun Zhang,Zile He,Huaqian Cai*

Main category: cs.AI

TL;DR: 提出ES框架将任务执行和工作流构建解耦，通过实验证明其能将大语言模型推理转化为结构化工作流且效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽能通过推理和工具使用解决复杂任务，但将解决方案转化为结构化工作流具有挑战，且现有工作流构建方法存在不准确问题。

Method: 提出Execute - Summarize(ES)框架，先使用可用工具完成任务，再从执行轨迹独立重建结构化工作流。

Result: 引入FlowBench，通过大量实验表明该方法优于现有方法。

Conclusion: 该方法为将大语言模型自由形式的推理转化为结构化工作流提供了可靠范例，提高了工作流的准确性和鲁棒性。

Abstract: LLMs can solve complex tasks through reasoning and tool use, but accurately translating these solutions into structured workflows remains challenging. We model workflows as sequences of tool use and reformulate the problem as designing a mechanism that can both solve tasks and reliably construct workflows. Prior approaches that build workflows during execution often suffer from inaccuracies due to interference between the two processes. We propose an Execute-Summarize(ES) framework that decouples task execution from workflow construction: the model first completes the task using available tools, then independently reconstructs a structured workflow from execution traces. This separation improves workflow accuracy and robustness. We introduce FlowBench and show through extensive experiments that our approach outperforms existing methods, providing a reliable paradigm for grounding free-form LLM reasoning into structured workflows.

</details>


### [34] [Beyond Pixels: Vector-to-Graph Transformation for Reliable Schematic Auditing](https://arxiv.org/abs/2602.11678)
*Chengwei Ma,Zhen Tian,Zhou Zhou,Zhixian Xu,Xiaowei Zhu,Xia Hua,Si Shi,F. Richard Yu*

Main category: cs.AI

TL;DR: MLLMs有结构盲区，提出V2G管道转换CAD图为属性图，在电气合规检查基准测试中效果好，凸显像素方法不足，发布基准和代码。


<details>
  <summary>Details</summary>
Motivation: 解决MLLMs在视觉理解中存在的结构盲区问题，使其能处理工程示意图中的拓扑和符号逻辑。

Method: 提出Vector-to-Graph (V2G) 管道，将CAD图转换为属性图，使结构依赖关系明确。

Result: 在电气合规检查的诊断基准测试中，V2G在所有错误类别上都有大幅准确率提升，而领先的MLLMs接近随机水平。

Conclusion: 像素基方法存在系统性不足，结构感知表示为多模态AI在工程领域的实际应用提供了可靠途径。

Abstract: Multimodal Large Language Models (MLLMs) have shown remarkable progress in visual understanding, yet they suffer from a critical limitation: structural blindness. Even state-of-the-art models fail to capture topology and symbolic logic in engineering schematics, as their pixel-driven paradigm discards the explicit vector-defined relations needed for reasoning. To overcome this, we propose a Vector-to-Graph (V2G) pipeline that converts CAD diagrams into property graphs where nodes represent components and edges encode connectivity, making structural dependencies explicit and machine-auditable. On a diagnostic benchmark of electrical compliance checks, V2G yields large accuracy gains across all error categories, while leading MLLMs remain near chance level. These results highlight the systemic inadequacy of pixel-based methods and demonstrate that structure-aware representations provide a reliable path toward practical deployment of multimodal AI in engineering domains. To facilitate further research, we release our benchmark and implementation at https://github.com/gm-embodied/V2G-Audit.

</details>


### [35] [ThinkRouter: Efficient Reasoning via Routing Thinking between Latent and Discrete Spaces](https://arxiv.org/abs/2602.11683)
*Xin Xu,Tong Yu,Xiang Chen,Haoliang Wang,Julian McAuley,Saayan Mitra*

Main category: cs.AI

TL;DR: 本文分析潜在推理中模型置信度动态，提出推理时置信度感知路由机制ThinkRouter，实验表明其在准确性上优于基线方法，能校准错误并加速生成。


<details>
  <summary>Details</summary>
Motivation: 现有潜在推理有效性因设置而异，低置信度思考替代方案的软嵌入可能引入并传播噪声，导致不可靠推理轨迹的高置信度。

Method: 提出ThinkRouter，在模型置信度低时将思考路由到离散令牌空间，否则路由到潜在空间。

Result: 在STEM推理和编码基准测试中，ThinkRouter在准确性上优于显式CoT、随机路由和潜在推理基线，Pass@1平均提高19.70点，生成长度最多减少15.55%。

Conclusion: ThinkRouter能校准显式CoT和潜在推理产生的误差，通过全局降低模型置信度加速思考结束令牌的生成。

Abstract: Recent work explores latent reasoning to improve reasoning efficiency by replacing explicit reasoning trajectories with continuous representations in a latent space, yet its effectiveness varies across settings. Analysis of model confidence dynamics under latent reasoning reveals that thinking trajectories ending in incorrect answers contain fewer low-confidence steps than those ending in correct answers. Meanwhile, we suggest that soft embeddings aggregated by multiple low-confidence thinking alternatives may introduce and propagate noise, leading to high confidence in unreliable reasoning trajectories. Motivated by these observations, ThinkRouter, an inference-time confidence-aware routing mechanism is proposed to avoid high confidence and noise for efficient reasoning. ThinkRouter routes thinking to the discrete token space when model confidence is low, and to the latent space otherwise. Extensive experiments on STEM reasoning and coding benchmarks across diverse large reasoning models demonstrate that ThinkRouter outperforms explicit CoT, random routing, and latent reasoning baselines in terms of accuracy, achieving an average improvement of 19.70 points in Pass@1, while reducing generation length by up to 15.55%. Further comprehensive analysis reveals that ThinkRouter can calibrate errors arising from explicit CoT and latent reasoning, and accelerates end-of-thinking token generation by globally lowering model confidence.

</details>


### [36] [Beyond Parameter Arithmetic: Sparse Complementary Fusion for Distribution-Aware Model Merging](https://arxiv.org/abs/2602.11717)
*Weihong Lin,Lin Sun,Qilong Shi,Aomufei Yuan,Yuxuan Tian,Zhengyang Wang,Guangxiang Zhao,Xiangzheng Zhang,Tong Yang*

Main category: cs.AI

TL;DR: 提出SCF - RKL模型合并框架，通过稀疏、分布感知更新控制功能干扰，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法依赖参数空间启发式，会引入严重干扰，导致泛化能力下降和生成行为不稳定。

Method: 提出SCF - RKL框架，用反向Kullback - Leibler散度衡量模型功能差异，选择性合并互补参数。

Result: 在24个基准测试上，SCF - RKL始终优于现有模型合并方法，且保持强泛化和生成稳定性。

Conclusion: SCF - RKL是一种有效控制功能干扰的模型合并框架，能在集成新能力时保留稳定表示。

Abstract: Model merging has emerged as a promising paradigm for composing the capabilities of large language models by directly operating in weight space, enabling the integration of specialized models without costly retraining. However, existing merging methods largely rely on parameter-space heuristics, which often introduce severe interference, leading to degraded generalization and unstable generation behaviors such as repetition and incoherent outputs. In this work, we propose Sparse Complementary Fusion with reverse KL (SCF-RKL), a novel model merging framework that explicitly controls functional interference through sparse, distribution-aware updates. Instead of assuming linear additivity in parameter space, SCF-RKL measures the functional divergence between models using reverse Kullback-Leibler divergence and selectively incorporates complementary parameters. This mode-seeking, sparsity-inducing design effectively preserves stable representations while integrating new capabilities. We evaluate SCF-RKL across a wide range of model scales and architectures, covering both reasoning-focused and instruction-tuned models. Extensive experiments on 24 benchmarks spanning advanced reasoning, general reasoning and knowledge, instruction following, and safety demonstrate, vision classification that SCF-RKL consistently outperforms existing model merging methods while maintaining strong generalization and generation stability.

</details>


### [37] [Text2GQL-Bench: A Text to Graph Query Language Benchmark [Experiment, Analysis & Benchmark]](https://arxiv.org/abs/2602.11745)
*Songlin Lyu,Lujie Ban,Zihang Wu,Tianqi Luo,Jirong Liu,Chenhao Ma,Yuyu Luo,Nan Tang,Shipeng Qi,Heng Lin,Yongchao Liu,Chuntao Hong*

Main category: cs.AI

TL;DR: 提出Text2GQL - Bench统一基准，含多GQL数据集与可扩展框架，引入新评估方法，评估发现ISO - GQL生成存在方言差距。


<details>
  <summary>Details</summary>
Motivation: 现有Text - to - GQL系统数据集在领域覆盖、支持的图查询语言和评估范围有限，缺乏高质量基准数据集和评估方法。

Method: 构建Text2GQL - Bench，包含多GQL数据集和可扩展框架，引入综合评估方法。

Result: 评估发现ISO - GQL生成有方言差距，零样本执行准确率至多4%，3 - shot提示提升至约50%，语法有效性低于70%；微调的8B模型EX达45.1%，语法有效性达90.8%。

Conclusion: 接触足够ISO - GQL示例可大幅提升性能。

Abstract: Graph models are fundamental to data analysis in domains rich with complex relationships. Text-to-Graph-Query-Language (Text-to-GQL) systems act as a translator, converting natural language into executable graph queries. This capability allows Large Language Models (LLMs) to directly analyze and manipulate graph data, posi-tioning them as powerful agent infrastructures for Graph Database Management System (GDBMS). Despite recent progress, existing datasets are often limited in domain coverage, supported graph query languages, or evaluation scope. The advancement of Text-to-GQL systems is hindered by the lack of high-quality benchmark datasets and evaluation methods to systematically compare model capabilities across different graph query languages and domains. In this work, we present Text2GQL-Bench, a unified Text-to-GQL benchmark designed to address these limitations. Text2GQL-Bench couples a multi-GQL dataset that has 178,184 (Question, Query) pairs spanning 13 domains, with a scalable construction framework that generates datasets in different domains, question abstraction levels, and GQLs with heterogeneous resources. To support compre-hensive assessment, we introduce an evaluation method that goes beyond a single end-to-end metric by jointly reporting grammatical validity, similarity, semantic alignment, and execution accuracy. Our evaluation uncovers a stark dialect gap in ISO-GQL generation: even strong LLMs achieve only at most 4% execution accuracy (EX) in zero-shot settings, though a fixed 3-shot prompt raises accuracy to around 50%, the grammatical validity remains lower than 70%. Moreover, a fine-tuned 8B open-weight model reaches 45.1% EX, and 90.8% grammatical validity, demonstrating that most of the performance jump is unlocked by exposure to sufficient ISO-GQL examples.

</details>


### [38] [AIR: Improving Agent Safety through Incident Response](https://arxiv.org/abs/2602.11749)
*Zibo Xiao,Jun Sun,Junjie Chen*

Main category: cs.AI

TL;DR: 本文提出首个针对大语言模型（LLM）代理系统的事件响应框架AIR，评估显示其检测、修复和根除成功率超90%，证明事件响应可有效提升代理安全性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理安全机制侧重于事前预防，对事件发生后的响应、控制和恢复能力有限，需要新的解决方案。

Method: 定义特定领域语言，将其集成到代理执行循环，通过语义检查检测事件，引导代理执行控制和恢复动作，在根除阶段合成护栏规则。

Result: 在三种代表性代理类型上评估，检测、修复和根除成功率超90%，实验证实关键设计组件的必要性、及时性和适度开销，LLM生成规则接近开发者编写规则的有效性。

Conclusion: 事件响应作为提升代理安全性的一级机制是可行且必要的。

Abstract: Large Language Model (LLM) agents are increasingly deployed in practice across a wide range of autonomous applications. Yet current safety mechanisms for LLM agents focus almost exclusively on preventing failures in advance, providing limited capabilities for responding to, containing, or recovering from incidents after they inevitably arise. In this work, we introduce AIR, the first incident response framework for LLM agent systems. AIR defines a domain-specific language for managing the incident response lifecycle autonomously in LLM agent systems, and integrates it into the agent's execution loop to (1) detect incidents via semantic checks grounded in the current environment state and recent context, (2) guide the agent to execute containment and recovery actions via its tools, and (3) synthesize guardrail rules during eradication to block similar incidents in future executions. We evaluate AIR on three representative agent types. Results show that AIR achieves detection, remediation, and eradication success rates all exceeding 90%. Extensive experiments further confirm the necessity of AIR's key design components, show the timeliness and moderate overhead of AIR, and demonstrate that LLM-generated rules can approach the effectiveness of developer-authored rules across domains. These results show that incident response is both feasible and essential as a first-class mechanism for improving agent safety.

</details>


### [39] [TSR: Trajectory-Search Rollouts for Multi-Turn RL of LLM Agents](https://arxiv.org/abs/2602.11767)
*Aladin Djuhera,Swanand Ravindra Kadhe,Farhan Ahmed,Holger Boche*

Main category: cs.AI

TL;DR: 提出TSR方法提高多轮强化学习中每轮的滚动生成质量，在多个任务上取得性能提升和更稳定学习。


<details>
  <summary>Details</summary>
Motivation: 多轮强化学习存在奖励稀疏、延迟，环境随机等问题，朴素轨迹采样有弊端，需要改进方法。

Method: 提出TSR方法，进行轻量级树状搜索，利用特定任务反馈选择高得分动作构建高质量轨迹，可与不同搜索策略和优化器搭配。

Result: 在Sokoban、FrozenLake和WebShop任务上实现最多15%的性能提升，学习更稳定，训练计算有一次性增加。

Conclusion: TSR将搜索从推理阶段移到训练的滚动阶段，为多轮智能体学习提供简单通用机制，与现有框架和方法互补。

Abstract: Advances in large language models (LLMs) are driving a shift toward using reinforcement learning (RL) to train agents from iterative, multi-turn interactions across tasks. However, multi-turn RL remains challenging as rewards are often sparse or delayed, and environments can be stochastic. In this regime, naive trajectory sampling can hinder exploitation and induce mode collapse. We propose TSR (Trajectory-Search Rollouts), a training-time approach that repurposes test-time scaling ideas for improved per-turn rollout generation. TSR performs lightweight tree-style search to construct high-quality trajectories by selecting high-scoring actions at each turn using task-specific feedback. This improves rollout quality and stabilizes learning while leaving the underlying optimization objective unchanged, making TSR optimizer-agnostic. We instantiate TSR with best-of-N, beam, and shallow lookahead search, and pair it with PPO and GRPO, achieving up to 15% performance gains and more stable learning on Sokoban, FrozenLake, and WebShop tasks at a one-time increase in training compute. By moving search from inference time to the rollout stage of training, TSR provides a simple and general mechanism for stronger multi-turn agent learning, complementary to existing frameworks and rejection-sampling-style selection methods.

</details>


### [40] [How to Optimize Multispecies Set Predictions in Presence-Absence Modeling ?](https://arxiv.org/abs/2602.11771)
*Sébastien Gigot--Léandri,Gaétan Morand,Alexis Joly,François Munoz,David Mouillot,Christophe Botella,Maximilien Servajean*

Main category: cs.AI

TL;DR: 本文提出MaxExp与SSE方法用于物种分布模型二值化，在多个案例中表现良好。


<details>
  <summary>Details</summary>
Motivation: 传统物种分布模型二值化步骤是启发式的，会扭曲物种流行率和群落组成估计。

Method: 提出MaxExp决策驱动二值化框架，直接最大化所选评估指标；引入Set Size Expectation (SSE)方法，基于预期物种丰富度预测组合。

Result: 在三个案例研究中，MaxExp表现优于常用方法，SSE是更简单且有竞争力的选择。

Conclusion: 这两种方法为多物种SDM二值化提供了稳健且可重复的工具。

Abstract: Species distribution models (SDMs) commonly produce probabilistic occurrence predictions that must be converted into binary presence-absence maps for ecological inference and conservation planning. However, this binarization step is typically heuristic and can substantially distort estimates of species prevalence and community composition. We present MaxExp, a decision-driven binarization framework that selects the most probable species assemblage by directly maximizing a chosen evaluation metric. MaxExp requires no calibration data and is flexible across several scores. We also introduce the Set Size Expectation (SSE) method, a computationally efficient alternative that predicts assemblages based on expected species richness. Using three case studies spanning diverse taxa, species counts, and performance metrics, we show that MaxExp consistently matches or surpasses widely used thresholding and calibration methods, especially under strong class imbalance and high rarity. SSE offers a simpler yet competitive option. Together, these methods provide robust, reproducible tools for multispecies SDM binarization.

</details>


### [41] [RELATE: A Reinforcement Learning-Enhanced LLM Framework for Advertising Text Generation](https://arxiv.org/abs/2602.11780)
*Jinfang Wang,Jiajie Liu,Jianwei Wu,Ziqin Luo,Zhen Chen,Chunlei Li,Biao Han,Tao Deng,Yi Li,Shuanglong Li,Lin Liu*

Main category: cs.AI

TL;DR: 现有在线广告文本生成系统两阶段范式有局限，提出 RELATE 框架统一生成与目标对齐，实验证明其效果好。


<details>
  <summary>Details</summary>
Motivation: 现有工业系统两阶段范式导致优化目标不一致和漏斗效率低，限制全局最优性。

Method: 提出基于强化学习的端到端框架 RELATE，将性能和合规目标通过策略学习集成到生成过程，纳入转化指标并作为多维度奖励联合建模。

Result: 在大规模工业数据集实验中 RELATE 始终优于基线，在生产广告平台在线部署使点击率转化率显著提升。

Conclusion: RELATE 框架具有鲁棒性和实际有效性。

Abstract: In online advertising, advertising text plays a critical role in attracting user engagement and driving advertiser value. Existing industrial systems typically follow a two-stage paradigm, where candidate texts are first generated and subsequently aligned with online performance metrics such as click-through rate(CTR). This separation often leads to misaligned optimization objectives and low funnel efficiency, limiting global optimality.
  To address these limitations, we propose RELATE, a reinforcement learning-based end-to-end framework that unifies generation and objective alignment within a single model. Instead of decoupling text generation from downstream metric alignment, RELATE integrates performance and compliance objectives directly into the generation process via policy learning. To better capture ultimate advertiser value beyond click-level signals, We incorporate conversion-oriented metrics into the objective and jointly model them with compliance constraints as multi-dimensional rewards, enabling the model to generate high-quality ad texts that improve conversion performance under policy constraints.
  Extensive experiments on large-scale industrial datasets demonstrate that RELATE consistently outperforms baselines. Furthermore, online deployment on a production advertising platform yields statistically significant improvements in click-through conversion rate(CTCVR) under strict policy constraints, validating the robustness and real-world effectiveness of the proposed framework.

</details>


### [42] [Beyond End-to-End Video Models: An LLM-Based Multi-Agent System for Educational Video Generation](https://arxiv.org/abs/2602.11790)
*Lingyong Yan,Jiulong Wu,Dong Xie,Weixian Shi,Deguo Xia,Jizhou Huang*

Main category: cs.AI

TL;DR: 提出基于大语言模型的多智能体系统 LAVES 用于从教育问题生成高质量教学视频，实现自动化端到端生产，成本大幅降低且吞吐量高。


<details>
  <summary>Details</summary>
Motivation: 现有端到端视频生成模型在需要严格逻辑和精确知识表示的场景（如教学和教育媒体）中存在局限。

Method: 将教育视频生成作为多目标任务，将生成工作流分解为多个专业智能体，由中央协调智能体监督并设置质量门和迭代批判机制，构建可执行视频脚本。

Result: 在大规模部署中，LAVES 日产量超百万视频，成本比当前行业标准方法降低超 95%，且保持高接受率。

Conclusion: LAVES 能有效解决现有模型在教育视频生成中的局限，实现高效、低成本的自动化生产。

Abstract: Although recent end-to-end video generation models demonstrate impressive performance in visually oriented content creation, they remain limited in scenarios that require strict logical rigor and precise knowledge representation, such as instructional and educational media. To address this problem, we propose LAVES, a hierarchical LLM-based multi-agent system for generating high-quality instructional videos from educational problems. The LAVES formulates educational video generation as a multi-objective task that simultaneously demands correct step-by-step reasoning, pedagogically coherent narration, semantically faithful visual demonstrations, and precise audio--visual alignment. To address the limitations of prior approaches--including low procedural fidelity, high production cost, and limited controllability--LAVES decomposes the generation workflow into specialized agents coordinated by a central Orchestrating Agent with explicit quality gates and iterative critique mechanisms. Specifically, the Orchestrating Agent supervises a Solution Agent for rigorous problem solving, an Illustration Agent that produces executable visualization codes, and a Narration Agent for learner-oriented instructional scripts. In addition, all outputs from the working agents are subject to semantic critique, rule-based constraints, and tool-based compilation checks. Rather than directly synthesizing pixels, the system constructs a structured executable video script that is deterministically compiled into synchronized visuals and narration using template-driven assembly rules, enabling fully automated end-to-end production without manual editing. In large-scale deployments, LAVES achieves a throughput exceeding one million videos per day, delivering over a 95% reduction in cost compared to current industry-standard approaches while maintaining a high acceptance rate.

</details>


### [43] [Detecting RLVR Training Data via Structural Convergence of Reasoning](https://arxiv.org/abs/2602.11792)
*Hongbo Zhang,Yue Yang,Jianhao Yan,Guangsheng Bao,Yue Zhang,Yue Zhang*

Main category: cs.AI

TL;DR: 本文指出基于可验证奖励的强化学习（RLVR）训练数据未公开引发基准污染担忧，提出Min - kNN Distance检测器，实验表明其能有效区分RL训练中见过和未见过的示例，且优于现有方法。


<details>
  <summary>Details</summary>
Motivation: RLVR训练数据未公开引发基准污染担忧，传统基于似然的检测方法对RLVR效果不佳。

Method: 引入Min - kNN Distance，一种简单的黑盒检测器，通过对给定提示采样多个补全结果并计算k个最小最近邻编辑距离的平均值来量化生成的收敛性，无需访问参考模型或标记概率。

Result: 在多个RLVR训练的推理模型上的实验表明，Min - kNN Distance能可靠地区分RL训练中见过和未见过的示例，且优于现有的成员推理和RL污染检测基线。

Conclusion: Min - kNN Distance是一种有效的检测RLVR训练数据污染的方法。

Abstract: Reinforcement learning with verifiable rewards (RLVR) is central to training modern reasoning models, but the undisclosed training data raises concerns about benchmark contamination. Unlike pretraining methods, which optimize models using token-level probabilities, RLVR fine-tunes models based on reward feedback from self-generated reasoning trajectories, making conventional likelihood-based detection methods less effective. We show that RLVR induces a distinctive behavioral signature: prompts encountered during RLVR training result in more rigid and similar generations, while unseen prompts retain greater diversity. We introduce Min-$k$NN Distance, a simple black-box detector that quantifies this collapse by sampling multiple completions for a given prompt and computing the average of the $k$ smallest nearest-neighbor edit distances. Min-$k$NN Distance requires no access to the reference model or token probabilities. Experiments across multiple RLVR-trained reasoning models show that Min-$k$NN Distance reliably distinguishes RL-seen examples from unseen ones and outperforms existing membership inference and RL contamination detection baselines.

</details>


### [44] [Hi-SAM: A Hierarchical Structure-Aware Multi-modal Framework for Large-Scale Recommendation](https://arxiv.org/abs/2602.11799)
*Pingjun Pan,Tingting Zhou,Peiyao Lu,Tingting Fei,Hongxiang Chen,Chuanjiang Luo*

Main category: cs.AI

TL;DR: 现有多模态推荐语义ID方法存在分词欠佳和架构与数据不匹配问题，提出Hi - SAM框架，实验显示效果优于SOTA基线，上线平台核心指标提升6.55%。


<details>
  <summary>Details</summary>
Motivation: 解决现有语义ID方法在多模态推荐中存在的分词欠佳（缺乏跨模态语义和特定模态细节解耦）和架构与数据不匹配（忽略用户交互、物品和令牌层次结构）问题。

Method: 提出Hi - SAM框架，包含解纠缠语义分词器（DST）统一模态并量化，以及分层记忆锚定变压器（HMAT）通过分层RoPE恢复层次结构。

Result: 在真实数据集实验中效果优于SOTA基线，在冷启动场景表现突出；在大规模社交平台上线核心在线指标提升6.55%。

Conclusion: Hi - SAM框架有效解决了现有多模态推荐语义ID方法的问题，提升了推荐效果。

Abstract: Multi-modal recommendation has gained traction as items possess rich attributes like text and images. Semantic ID-based approaches effectively discretize this information into compact tokens. However, two challenges persist: (1) Suboptimal Tokenization: existing methods (e.g., RQ-VAE) lack disentanglement between shared cross-modal semantics and modality-specific details, causing redundancy or collapse; (2) Architecture-Data Mismatch: vanilla Transformers treat semantic IDs as flat streams, ignoring the hierarchy of user interactions, items, and tokens. Expanding items into multiple tokens amplifies length and noise, biasing attention toward local details over holistic semantics. We propose Hi-SAM, a Hierarchical Structure-Aware Multi-modal framework with two designs: (1) Disentangled Semantic Tokenizer (DST): unifies modalities via geometry-aware alignment and quantizes them via a coarse-to-fine strategy. Shared codebooks distill consensus while modality-specific ones recover nuances from residuals, enforced by mutual information minimization; (2) Hierarchical Memory-Anchor Transformer (HMAT): splits positional encoding into inter- and intra-item subspaces via Hierarchical RoPE to restore hierarchy. It inserts Anchor Tokens to condense items into compact memory, retaining details for the current item while accessing history only through compressed summaries. Experiments on real-world datasets show consistent improvements over SOTA baselines, especially in cold-start scenarios. Deployed on a large-scale social platform serving millions of users, Hi-SAM achieved a 6.55% gain in the core online metric.

</details>


### [45] [PuYun-LDM: A Latent Diffusion Model for High-Resolution Ensemble Weather Forecasts](https://arxiv.org/abs/2602.11807)
*Lianjun Wu,Shengchen Zhu,Yuxuan Liu,Liuyu Kai,Xiaoduan Feng,Duomin Wang,Wenshuo Liu,Jingxuan Zhang,Kelvin Li,Bin Wang*

Main category: cs.AI

TL;DR: 提出PuYun - LDM模型以解决潜在扩散模型在高分辨率集合天气预报中的可扩散性问题，加速预报效率。


<details>
  <summary>Details</summary>
Motivation: 潜在扩散模型在高分辨率集合天气预报中可扩散性有限，气象领域缺少适用基础模型，现有频率方法在多变量数据中存在正则化强度不均问题。

Method: 提出3D - MAE编码天气状态演变特征作为扩散模型额外条件，采用VA - MFM策略根据各变量频谱能量分布自适应选择阈值，结合提出PuYun - LDM。

Result: PuYun - LDM增强了潜在可扩散性，短期预报性能超ENS，长期与ENS相当，能在单GPU上高效生成15天全球预报。

Conclusion: PuYun - LDM能有效解决潜在扩散模型在高分辨率集合天气预报中的问题，提高预报效率。

Abstract: Latent diffusion models (LDMs) suffer from limited diffusability in high-resolution (<=0.25°) ensemble weather forecasting, where diffusability characterizes how easily a latent data distribution can be modeled by a diffusion process. Unlike natural image fields, meteorological fields lack task-agnostic foundation models and explicit semantic structures, making VFM-based regularization inapplicable. Moreover, existing frequency-based approaches impose identical spectral regularization across channels under a homogeneity assumption, which leads to uneven regularization strength under the inter-variable spectral heterogeneity in multivariate meteorological data. To address these challenges, we propose a 3D Masked AutoEncoder (3D-MAE) that encodes weather-state evolution features as an additional conditioning for the diffusion model, together with a Variable-Aware Masked Frequency Modeling (VA-MFM) strategy that adaptively selects thresholds based on the spectral energy distribution of each variable. Together, we propose PuYun-LDM, which enhances latent diffusability and achieves superior performance to ENS at short lead times while remaining comparable to ENS at longer horizons. PuYun-LDM generates a 15-day global forecast with a 6-hour temporal resolution in five minutes on a single NVIDIA H200 GPU, while ensemble forecasts can be efficiently produced in parallel.

</details>


### [46] [Predicting LLM Output Length via Entropy-Guided Representations](https://arxiv.org/abs/2602.11812)
*Huanyi Xie,Yubin Chen,Liangyu Wang,Lijie Hu,Di Wang*

Main category: cs.AI

TL;DR: 现有大语言模型服务和强化学习采样中序列长度长尾分布导致计算浪费，本文提出轻量级框架预测长度，在新基准测试中表现出色，提升了端到端吞吐量。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型服务和强化学习采样中序列长度长尾分布因填充导致的计算浪费问题，现有方法存在开销高、泛化性差等不足。

Method: 引入轻量级框架，利用主模型内部隐藏状态预测长度，包含熵引导令牌池化（EGTP）和渐进长度预测（PLP）两个核心组件，构建并发布综合基准ForeLen。

Result: EGTP在ForeLen上达到了最先进的准确率，比最佳基线降低了29.16%的平均绝对误差，与长度感知调度器集成带来了显著的端到端吞吐量提升。

Conclusion: 该工作为高效大语言模型推理提供了新的技术和评估基线。

Abstract: The long-tailed distribution of sequence lengths in LLM serving and reinforcement learning (RL) sampling causes significant computational waste due to excessive padding in batched inference. Existing methods rely on auxiliary models for static length prediction, but they incur high overhead, generalize poorly, and fail in stochastic "one-to-many" sampling scenarios. We introduce a lightweight framework that reuses the main model's internal hidden states for efficient length prediction. Our framework features two core components: 1) Entropy-Guided Token Pooling (EGTP), which uses on-the-fly activations and token entropy for highly accurate static prediction with negligible cost, and 2) Progressive Length Prediction (PLP), which dynamically estimates the remaining length at each decoding step to handle stochastic generation. To validate our approach, we build and release ForeLen, a comprehensive benchmark with long-sequence, Chain-of-Thought, and RL data. On ForeLen, EGTP achieves state-of-the-art accuracy, reducing MAE by 29.16\% over the best baseline. Integrating our methods with a length-aware scheduler yields significant end-to-end throughput gains. Our work provides a new technical and evaluation baseline for efficient LLM inference.

</details>


### [47] [Revis: Sparse Latent Steering to Mitigate Object Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2602.11824)
*Jialin Wu,Wei Shi,Han Shen,Peigui Qi,Kunsheng Tang,Zhicong Huang,Binghao Wang,Zhou Yang*

Main category: cs.AI

TL;DR: 提出REVIS框架解决大视觉语言模型的物体幻觉问题，通过数学方法减少幻觉率


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型常出现物体幻觉问题，原因是视觉特征和预训练文本表征在深层网络层交织，需解决该问题

Method: 提出训练无关的REVIS框架，基于潜在空间几何，通过正交投影提取纯视觉信息向量，采用校准策略在信息被抑制的深度进行稀疏干预

Result: 在标准基准测试中，REVIS相比最先进的基线将物体幻觉率降低了约19%，同时保留了一般推理能力

Conclusion: REVIS能以最小计算成本有效恢复视觉信息，减少物体幻觉率

Abstract: Despite the advanced capabilities of Large Vision-Language Models (LVLMs), they frequently suffer from object hallucination. One reason is that visual features and pretrained textual representations often become intertwined in the deeper network layers. To address this, we propose REVIS, a training-free framework designed to explicitly re-activate this suppressed visual information. Rooted in latent space geometry, REVIS extracts the pure visual information vector via orthogonal projection and employs a calibrated strategy to perform sparse intervention only at the precise depth where suppression occurs. This surgical approach effectively restores visual information with minimal computational cost. Empirical evaluations on standard benchmarks demonstrate that REVIS reduces object hallucination rates by approximately 19% compared to state-of-the-art baselines, while preserving general reasoning capabilities.

</details>


### [48] [Prototype Transformer: Towards Language Model Architectures Interpretable by Design](https://arxiv.org/abs/2602.11852)
*Yordan Yordanov,Matteo Forasassi,Bayar Menzat,Ruizhi Wang,Chang Qi,Markus Kaltenberger,Amine M'Charrak,Tommaso Salvatori,Thomas Lukasiewicz*

Main category: cs.AI

TL;DR: 提出Prototype Transformer (ProtoT) 架构，可解释推理过程、有计算扩展性优势，性能接近SOTA且能展现鲁棒性来源。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型推理过程不透明，存在欺骗和幻觉等风险，需要可解释的模型架构。

Method: 引入基于原型（参数向量）的自回归语言模型ProtoT，通过输入序列和原型的双向通信工作。

Result: ProtoT可自动捕捉可命名概念，计算可扩展性为线性，在文本生成和下游任务表现良好，对输入扰动有鲁棒性且能展示鲁棒性和敏感性来源。

Conclusion: ProtoT为创建可解释的高性能自回归语言模型铺平道路。

Abstract: While state-of-the-art language models (LMs) surpass the vast majority of humans in certain domains, their reasoning remains largely opaque, undermining trust in their output. Furthermore, while autoregressive LMs can output explicit reasoning, their true reasoning process is opaque, which introduces risks like deception and hallucination. In this work, we introduce the Prototype Transformer (ProtoT) -- an autoregressive LM architecture based on prototypes (parameter vectors), posed as an alternative to the standard self-attention-based transformers. ProtoT works by means of two-way communication between the input sequence and the prototypes, and we show that this leads to the prototypes automatically capturing nameable concepts (e.g. "woman") during training. They provide the potential to interpret the model's reasoning and allow for targeted edits of its behavior. Furthermore, by design, the prototypes create communication channels that aggregate contextual information at different time scales, aiding interpretability. In terms of computation scalability, ProtoT scales linearly with sequence length vs the quadratic scalability of SOTA self-attention transformers. Compared to baselines, ProtoT scales well with model and data size, and performs well on text generation and downstream tasks (GLUE). ProtoT exhibits robustness to input perturbations on par or better than some baselines, but differs from them by providing interpretable pathways showing how robustness and sensitivity arises. Reaching close to the performance of state-of-the-art architectures, ProtoT paves the way to creating well-performing autoregressive LMs interpretable by design.

</details>


### [49] [Talk2DM: Enabling Natural Language Querying and Commonsense Reasoning for Vehicle-Road-Cloud Integrated Dynamic Maps with Large Language Models](https://arxiv.org/abs/2602.11860)
*Lu Tao,Jinxuan Luo,Yousuke Watanabe,Zhengshu Zhou,Yuhuan Lu,Shen Ying,Pan Zhang,Fei Zhao,Hiroaki Takada*

Main category: cs.AI

TL;DR: 本文介绍VRCsim模拟框架生成数据，构建VRC - QA数据集，提出Talk2DM模块，实验显示其有良好准确率和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有动态地图（DM）系统缺乏自然语言支持（NLS）的人机界面，需增强人机交互。

Method: 引入VRCsim框架，构建VRC - QA数据集，提出基于CoP机制的Talk2DM模块。

Result: Talk2DM能切换不同大语言模型，由Qwen3:8B等模型驱动时，NLS查询准确率超93%，平均响应时间2 - 5秒。

Conclusion: Talk2DM具有较强泛化能力和实际应用潜力，大模型虽准确率高但效率低。

Abstract: Dynamic maps (DM) serve as the fundamental information infrastructure for vehicle-road-cloud (VRC) cooperative autonomous driving in China and Japan. By providing comprehensive traffic scene representations, DM overcome the limitations of standalone autonomous driving systems (ADS), such as physical occlusions. Although DM-enhanced ADS have been successfully deployed in real-world applications in Japan, existing DM systems still lack a natural-language-supported (NLS) human interface, which could substantially enhance human-DM interaction. To address this gap, this paper introduces VRCsim, a VRC cooperative perception (CP) simulation framework designed to generate streaming VRC-CP data. Based on VRCsim, we construct a question-answering data set, VRC-QA, focused on spatial querying and reasoning in mixed-traffic scenes. Building upon VRCsim and VRC-QA, we further propose Talk2DM, a plug-and-play module that extends VRC-DM systems with NLS querying and commonsense reasoning capabilities. Talk2DM is built upon a novel chain-of-prompt (CoP) mechanism that progressively integrates human-defined rules with the commonsense knowledge of large language models (LLMs). Experiments on VRC-QA show that Talk2DM can seamlessly switch across different LLMs while maintaining high NLS query accuracy, demonstrating strong generalization capability. Although larger models tend to achieve higher accuracy, they incur significant efficiency degradation. Our results reveal that Talk2DM, powered by Qwen3:8B, Gemma3:27B, and GPT-oss models, achieves over 93\% NLS query accuracy with an average response time of only 2-5 seconds, indicating strong practical potential.

</details>


### [50] [Intelligent AI Delegation](https://arxiv.org/abs/2602.11865)
*Nenad Tomašev,Matija Franklin,Simon Osindero*

Main category: cs.AI

TL;DR: 提出用于智能AI委托的自适应框架，适用于复杂委托网络中的人类和AI。


<details>
  <summary>Details</summary>
Motivation: 现有任务分解和委托方法依赖简单启发式，无法适应环境变化和处理意外故障，为让AI代理实现更宏伟目标，需更好的方法。

Method: 提出一个自适应框架，包含任务分配决策、权限责任转移、明确角色边界、意图说明和建立信任机制。

Result: 无明确提及具体结果。

Conclusion: 该框架适用于复杂委托网络中的人类和AI委托方与被委托方，有助于新兴代理网络协议的发展。

Abstract: AI agents are able to tackle increasingly complex tasks. To achieve more ambitious goals, AI agents need to be able to meaningfully decompose problems into manageable sub-components, and safely delegate their completion across to other AI agents and humans alike. Yet, existing task decomposition and delegation methods rely on simple heuristics, and are not able to dynamically adapt to environmental changes and robustly handle unexpected failures. Here we propose an adaptive framework for intelligent AI delegation - a sequence of decisions involving task allocation, that also incorporates transfer of authority, responsibility, accountability, clear specifications regarding roles and boundaries, clarity of intent, and mechanisms for establishing trust between the two (or more) parties. The proposed framework is applicable to both human and AI delegators and delegatees in complex delegation networks, aiming to inform the development of protocols in the emerging agentic web.

</details>


### [51] [From Atoms to Trees: Building a Structured Feature Forest with Hierarchical Sparse Autoencoders](https://arxiv.org/abs/2602.11881)
*Yifan Luo,Yang Zhan,Jiedong Jiang,Tianyang Liu,Mingrui Wu,Zhennan Zhou,Bin Dong*

Main category: cs.AI

TL;DR: 为捕捉大语言模型特征层次结构，提出分层稀疏自编码器HSAE，实验证明其能恢复语义层次结构且保留标准SAEs特性。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏自编码器孤立识别特征，而大语言模型有层次化的内在结构，需新方法捕捉。

Method: 提出HSAE，通过结构约束损失和随机特征扰动机制加强父子特征对齐。

Result: 在不同大语言模型和层的实验中，HSAE能恢复语义层次结构，保留标准SAEs重建保真度和可解释性。

Conclusion: HSAE是发现和分析大语言模型表示中多尺度概念结构的强大、可扩展工具。

Abstract: Sparse autoencoders (SAEs) have proven effective for extracting monosemantic features from large language models (LLMs), yet these features are typically identified in isolation. However, broad evidence suggests that LLMs capture the intrinsic structure of natural language, where the phenomenon of "feature splitting" in particular indicates that such structure is hierarchical. To capture this, we propose the Hierarchical Sparse Autoencoder (HSAE), which jointly learns a series of SAEs and the parent-child relationships between their features. HSAE strengthens the alignment between parent and child features through two novel mechanisms: a structural constraint loss and a random feature perturbation mechanism. Extensive experiments across various LLMs and layers demonstrate that HSAE consistently recovers semantically meaningful hierarchies, supported by both qualitative case studies and rigorous quantitative metrics. At the same time, HSAE preserves the reconstruction fidelity and interpretability of standard SAEs across different dictionary sizes. Our work provides a powerful, scalable tool for discovering and analyzing the multi-scale conceptual structures embedded in LLM representations.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [52] [A physics-informed data-driven framework for modeling hyperelastic materials with progressive damage and failure](https://arxiv.org/abs/2602.11414)
*Kshitiz Upadhyay*

Main category: cs.CE

TL;DR: 提出一种两阶段物理驱动与数据驱动结合的超弹性软材料本构模型框架，验证了其准确性和泛化能力，适用于有限实验数据下的软材料失效建模。


<details>
  <summary>Details</summary>
Motivation: 为超弹性软材料在渐进损伤和失效情况下建立本构模型，结合分析模型和机器学习优势以应对有限实验数据的建模需求。

Method: 基于能量限制超弹性概念，用高斯过程回归（GPR）分两阶段分别学习完整弹性响应和损伤演化，第二阶段通过惩罚优化施加约束保证热力学允许性。

Result: 在合成数据集上验证高内部分布精度，从有限训练数据到未训练模式有良好泛化能力，应用于实验脑组织数据可推断损伤演化和临界失效能量。

Conclusion: 该框架结合分析模型和机器学习优点，是有限实验数据下软材料失效建模的有效方法。

Abstract: This work presents a two-stage physics-informed, data-driven constitutive modeling framework for hyperelastic soft materials undergoing progressive damage and failure. The framework is grounded in the concept of hyperelasticity with energy limiters and employs Gaussian Process Regression (GPR) to separately learn the intact (undamaged) elastic response and damage evolution directly from data. In Stage I, GPR models learn the intact hyperelastic response through volumetric and isochoric response functions (or only the isochoric response under incompressibility), ensuring energetic consistency of the intact response and satisfaction of fundamental principles such as material frame indifference and balance of angular momentum. In Stage II, damage is modeled via a separate GPR model that learns the mapping between the intact strain energy density predicted by Stage I models and a stress-reduction factor governing damage and failure, with monotonicity, non-negativity, and complete-failure constraints enforced through penalty-based optimization to ensure thermodynamic admissibility. Validation on synthetic datasets, including benchmarking against analytical constitutive models and competing data-driven approaches, demonstrates high in-distribution accuracy under uniaxial tension and robust generalization from limited training data to compression and shear modes not used during training. Application to experimental brain tissue data demonstrates the practical applicability of the framework and enables inference of damage evolution and critical failure energy. Overall, the proposed framework combines the physical consistency, interpretability, and generalizability of analytical models with the flexibility, predictive accuracy, and automation of machine learning, offering a powerful approach for modeling failure in soft materials under limited experimental data.

</details>


### [53] [Quantum-Enhanced Temporal Embeddings via a Hybrid Seq2Seq Architecture](https://arxiv.org/abs/2602.11578)
*Tien-Ching Hsieh,Yun-Cheng Tsai,Samuel Yen-Chi Chen*

Main category: cs.CE

TL;DR: 研究浅量子层对时序数据表示学习的作用，开发QLSTM Seq2Seq自动编码器，在标准普尔500数据上表现优于经典LSTM，其潜在几何结构可指导投资组合分配策略。


<details>
  <summary>Details</summary>
Motivation: 探究浅的、与NISQ兼容的量子层如何改善现实世界顺序数据的时间表示学习。

Method: 开发QLSTM Seq2Seq自动编码器，将深度为1的变分量子电路嵌入每个循环门。

Result: 量子增强编码器在标准普尔500数据集上产生更优的轨迹、状态转换和聚类，支持使用RBF核进行投资组合分配且策略表现更优，潜在几何结构可作为状态指标。

Conclusion: 浅的混合量子和经典层在NISQ时代序列建模中有实际作用，为金融等领域改善时间嵌入提供可复制途径。

Abstract: This work investigates how shallow, NISQ-compatible quantum layers can improve temporal representation learning in real-world sequential data. We develop a QLSTM Seq2Seq autoencoder in which a depth-1 variational quantum circuit is embedded inside each recurrent gate, shaping the geometry of the learned latent manifold. Evaluated on fourteen rolling S and P 500 windows from 2022 to 2025, the quantum-enhanced encoder produces smoother trajectories, clearer regime transitions, and more stable, sector-coherent clusters than a classical LSTM baseline. These geometric properties support the use of a Radial Basis Function (RBF) kernel for downstream portfolio allocation, where both RBF-Graph and RBF-DivMom strategies consistently outperform their classical counterparts in risk-adjusted terms. Analysis across periods shows that compressed manifolds favor concentrated allocation, while dispersed manifolds favor diversification, demonstrating that latent geometry serves as a regime indicator. The results highlight a practical role for shallow hybrid quantum and classical layers in NISQ-era sequence modeling, offering a reproducible pathway for improving temporal embeddings in finance and other data-limited, noise-sensitive domains.

</details>


### [54] [Systematic Trend-Following with Adaptive Portfolio Construction: Enhancing Risk-Adjusted Alpha in Cryptocurrency Markets](https://arxiv.org/abs/2602.11708)
*Duc Bui,Thanh Nguyen*

Main category: cs.CE

TL;DR: 提出AdaptiveTrend交易框架，结合高频趋势跟踪、自适应投资组合构建和非对称多空资本分配，回测表现超基准策略，且经稳健性分析证明策略在不同市场条件下有韧性。


<details>
  <summary>Details</summary>
Motivation: 加密货币市场有动量效应和制度依赖波动性，为系统交易策略带来机遇和挑战，需开发有效交易框架。

Method: 提出AdaptiveTrend框架，包含动态止损机制、基于滚动夏普比率的资产选择程序和70/30非对称多空分配方案。

Result: 在超150个加密货币对36个月回测中，年化夏普比率2.41，最大回撤-12.7%，卡尔玛比率3.18，显著优于基准策略和等权重买入持有组合。

Conclusion: AdaptiveTrend策略有效，在不同市场条件下有韧性。

Abstract: Cryptocurrency markets exhibit pronounced momentum effects and regime-dependent volatility, presenting both opportunities and challenges for systematic trading strategies. We propose AdaptiveTrend, a multi-component algorithmic trading framework that integrates high-frequency trend-following on 6-hour intervals with monthly adaptive portfolio construction and asymmetric long-short capital allocation. Our framework introduces three key innovations: (1) a dynamic trailing stop mechanism calibrated to intra-day volatility regimes, (2) a rolling Sharpe-ratio-based asset selection procedure with market-capitalization-aware filtering, and (3) a theoretically motivated asymmetric 70/30 long-short allocation scheme grounded in the empirical positive drift of crypto markets. Through extensive out-of-sample backtesting across 150+ cryptocurrency pairs over a 36-month evaluation window (2022-2024), AdaptiveTrend achieves an annualized Sharpe ratio of 2.41, a maximum drawdown of -12.7%, and a Calmar ratio of 3.18, significantly outperforming benchmark trend-following strategies (TSMOM, time-series momentum) and equal-weighted buy-and-hold portfolios. We further conduct rigorous robustness analyses including parameter sensitivity, transaction cost modeling, and regime-conditional performance decomposition, demonstrating the strategy's resilience across bull, bear, and sideways market conditions.

</details>


### [55] [Systematic Analysis of Penalty-Optimised Illumination Design for Tomographic Volumetric Additive Manufacturing via the Extendable Framework TVAM AID Using the Core Imaging Library](https://arxiv.org/abs/2602.12178)
*Nicole Pellizzon,Richard Huber,Jon Spangenberg,Jakob Sauer Jørgensen*

Main category: cs.CE

TL;DR: 本文研究了断层体积增材制造（TVAM）中惩罚函数对打印指标的影响，并系统研究阈值参数，还介绍了可扩展框架TVAM AID。


<details>
  <summary>Details</summary>
Motivation: 为TVAM设计照明方案，优化打印效果。

Method: 选择惩罚函数，系统研究阈值参数，使用开源Core Imaging Library构建TVAM AID框架。

Result: 明确惩罚函数形状与光能剂量水平范围的联系，可表征阈值参数对打印指标的影响并选择默认值。

Conclusion: TVAM AID框架可利用CIL现有功能，降低入门门槛，鼓励使用重建优化策略。

Abstract: Tomographic Volumetric Additive Manufacturing(TVAM) is a novel manufacturing method that allows for the fast creation of objects of complex geometry in layerless fashion. The process is based on the solidification of photopolymer that occurs when a sufficient threshold dose of light-energy is absorbed. In order to create complex shapes, an illumination plan must be designed to force solidification in some desired areas while leaving other regions liquid. Determining an illumination plan can be considered as an optimisation problem where a variety of objective functionals (penalties) can be used. This work considers a selection of penalty functions and their impact on selected printing metrics; linking the shape of penalty functions to ranges of light-energy dose levels in in-part regions that should be printed and out-of-part regions that should remain liquid. Further, the threshold parameters that are typically used to demarcate minimum light-energy for in-part regions and maximum light-energy for out-of-part regions are investigated systematically as design parameters on both existing and new methods. This enables the characterisation of their effects on some selected printing metrics as well as informed selection for default values. This work is underpinned by a reproducible and extensible framework, TVAM Adaptive Illumination Design(TVAM AID), which makes use of the open-source Core Imaging Library(CIL) that is designed for tomographic imaging with an emphasis on reconstruction. The foundation of TVAM AID which is presented here can hence be easily enhanced by existing functionality in CIL thus lowering the barrier to entry and encouraging use of strategies that already exist for reconstruction optimisation.

</details>


### [56] [MagneX: A High-Performance, GPU-Enabled, Data-Driven Micromagnetics Solver for Spintronics](https://arxiv.org/abs/2602.12242)
*Andy Nonaka,Yingheng Tang,Julian C. LePelch,Prabhat Kumar,Weiqun Zhang,Jorge A. Munoz,Christian Fernandez-Soria,Cesar Diaz,David J. Gardner,Zhi Jackie Yao*

Main category: cs.CE

TL;DR: 开发开源微磁学建模工具MagneX，利用多种技术，验证性能和功能，展示数据驱动能力，助力自旋电子系统研究。


<details>
  <summary>Details</summary>
Motivation: 全面研究自旋电子器件中的多物理场耦合，需并行化和GPU加速，利用先进时间积分库和机器学习方法提升建模能力。

Method: 利用Exascale Computing Project软件框架AMReX、SUNDIALS时间积分库和基于Python的机器学习工作流开发MagneX，包含多种磁耦合机制。

Result: 展示代码的GPU性能和可扩展性，用标准问题和基准验证功能，用神经网络库替代计算昂贵的退磁物理展示数据驱动能力。

Conclusion: 该创新方法能探索完整物理相互作用，为理解和开发自旋电子与电子集成系统提供途径。

Abstract: In order to comprehensively investigate the multiphysics coupling in spintronic devices, it is essential to parallelize and utilize GPU-acceleration to address the spatial and temporal disparities inherent in the relevant physics. Additionally, the use of cutting-edge time integration libraries as well as machine learning (ML) approaches to replace and potentially accelerate expensive computational routines are attractive capabilities to enhance modeling capabilities moving forward. Leveraging the Exascale Computing Project software framework AMReX, as well as SUNDIALS time-integration libraries and python-based ML workflows, we have developed an open-source micromagnetics modeling tool called MagneX. This tool incorporates various crucial magnetic coupling mechanisms, including Zeeman coupling, demagnetization coupling, crystalline anisotropy interaction, exchange coupling, and Dzyaloshinskii-Moriya interaction (DMI) coupling. We demonstrate the GPU performance and scalability of the code and rigorously validate MagneX's functionality using the mumag standard problems and widely-accepted DMI benchmarks. Furthermore, we demonstrate the data-driven capability of MagneX by replacing the computationally-expensive demagnetization physics with neural network libraries trained from our simulation data. With the capacity to explore complete physical interactions, this innovative approach offers a promising pathway to better understand and develop fully integrated spintronic and electronic systems.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [57] [Filtered Approximate Nearest Neighbor Search in Vector Databases: System Design and Performance Analysis](https://arxiv.org/abs/2602.11443)
*Abylay Amanbayev,Brian Tsan,Tri Dang,Florin Rusu*

Main category: cs.DB

TL;DR: 文章系统化过滤策略分类，引入新数据集，提出GLS指标评估FANNS过滤策略，实验有新发现并给出实用指南。


<details>
  <summary>Details</summary>
Motivation: 缺乏对向量数据库中通用过滤策略表现的理解。

Method: 系统化过滤策略分类，引入新数据集MoReVec；提出GLS指标；扩展ANN - Benchmarks支持过滤向量搜索。

Result: 引擎内算法调整常覆盖原始索引性能；Milvus召回稳定性好；pgvector查询优化器常选次优方案；低选择性查询时IVFFlat优于HNSW。

Conclusion: 给出混合搜索工作负载选择索引类型和配置查询优化器的实用指南。

Abstract: Retrieval-Augmented Generation (RAG) applications increasingly rely on Filtered Approximate Nearest Neighbor Search (FANNS) to combine semantic retrieval with metadata constraints. While algorithmic innovations for FANNS have been proposed, there remains a lack of understanding regarding how generic filtering strategies perform within Vector Databases. In this work, we systematize the taxonomy of filtering strategies and evaluate their integration into FAISS, Milvus, and pgvector. To provide a robust benchmarking framework, we introduce a new relational dataset, \textit{MoReVec}, consisting of two tables, featuring 768-dimensional text embeddings and a rich schema of metadata attributes. We further propose the \textit{Global-Local Selectivity (GLS)} correlation metric to quantify the relationship between filters and query vectors.
  Our experiments reveal that algorithmic adaptations within the engine often override raw index performance. Specifically, we find that: (1) \textit{Milvus} achieves superior recall stability through hybrid approximate/exact execution; (2) \textit{pgvector}'s cost-based query optimizer frequently selects suboptimal execution plans, favoring approximate index scans even when exact sequential scans would yield perfect recall at comparable latency; and (3) partition-based indexes (IVFFlat) outperform graph-based indexes (HNSW) for low-selectivity queries. To facilitate this analysis, we extend the widely-used \textit{ANN-Benchmarks} to support filtered vector search and make it available online. Finally, we synthesize our findings into a set of practical guidelines for selecting index types and configuring query optimizers for hybrid search workloads.

</details>


### [58] [Fast Tuning the Index Construction Parameters of Proximity Graphs in Vector Databases](https://arxiv.org/abs/2602.11573)
*Wenyang Zhou,Jiadong Xie,Yingfan Liu,Zhihao Yin,Jeffrey Xu Yu,Hui Li,Zhangqian Mu,Xiaotian Qiao,Jiangtao Cui*

Main category: cs.DB

TL;DR: 本文提出高效框架FastPGT用于调整邻近图（PG）构建参数，通过同时构建多个PG加速参数估计，实验显示比SOTA方法VDTuner提速达2.37倍且不降低调优质量。


<details>
  <summary>Details</summary>
Motivation: 邻近图（PG）是高维向量空间k-近似最近邻搜索（k-ANNS）的SOTA方法，但PG构建参数显著影响搜索性能，调优时构建和评估图索引成本高，且目前无方法对此过程进行优化。

Method: 引入FastPGT框架，通过同时构建多个PG加速参数估计，并修改SOTA调优模型以一次性推荐多个参数。

Result: 在真实数据集上的大量实验表明，FastPGT比SOTA方法VDTuner提速达2.37倍，且不降低调优质量。

Conclusion: FastPGT是一种高效的PG构建参数调优框架，能在不损失调优质量的前提下显著提高调优速度。

Abstract: k-approximate nearest neighbor search (k-ANNS) in high-dimensional vector spaces is a fundamental problem across many fields. With the advent of vector databases and retrieval-augmented generation, k-ANNS has garnered increasing attention. Among existing methods, proximity graphs (PG) based approaches are the state-of-the-art (SOTA) methods. However, the construction parameters of PGs significantly impact their search performance. Before constructing a PG for a given dataset, it is essential to tune these parameters, which first recommends a set of promising parameters and then estimates the quality of each parameter by building the corresponding PG and then testing its k-ANNS performance. Given that the construction complexity of PGs is superlinear, building and evaluating graph indexes accounts for the primary cost of parameter tuning. Unfortunately, there is currently no method considered and optimized this process.In this paper, we introduce FastPGT, an efficient framework for tuning the PG construction parameters. FastPGT accelerates parameter estimation by building multiple PGs simultaneously, thereby reducing repeated computations. Moreover, we modify the SOTA tuning model to recommend multiple parameters at once, which can be efficiently estimated using our method of building multiple PGs simultaneously. Through extensive experiments on real-world datasets, we demonstrate that FastPGT achieves up to 2.37x speedup over the SOTA method VDTuner, without compromising tuning quality.

</details>


### [59] [Towards a theory of Façade-X data access: satisfiability of SPARQL basic graph patterns](https://arxiv.org/abs/2602.11756)
*Luigi Asprino,Enrico Daga*

Main category: cs.DB

TL;DR: 本文巩固了Façade - X方法，研究基本图模式的可满足性，给出判定算法并通过实验验证可行性，为研究查询执行策略和构建数据集成系统奠定基础。


<details>
  <summary>Details</summary>
Motivation: Façade - X虽能访问多种数据格式，但并非所有SPARQL查询在其图上可满足，需研究基本图模式的可满足性。

Method: 对Façade - X进行巩固研究，提出判定基本图模式可满足性的算法，并进行概念验证实现和广泛实验。

Result: 通过实验验证了算法在实际中的可行性，包括处理真实世界查询。

Conclusion: 研究结果为研究Façade - X数据访问的查询执行策略和构建更高效的知识图谱数据集成系统铺平道路。

Abstract: Data integration is the primary use case for knowledge graphs. However, integrated data are not typically graphs but come in different formats, for example, CSV, XML, or a relational database. Façade-X is a recently proposed method for providing direct access to an open-ended set of data formats. The method includes a meta-model that specialises RDF to fit general data structures. This model allows to express SPARQL queries targeting data sources with those structures. Previous work formalised Façade-X and demonstrated how it can theoretically represent any format expressible with a context-free grammar, as well as the relational model. A reference implementation, SPARQL Anything, demonstrates the feasibility of the approach in practice. It is noteworthy that Façade-X utilises a fraction of RDF, and, consequently, not all SPARQL queries yield a solution (i.e. are satisfiable) when evaluated over a Façade-X graph. In this article, we consolidate Façade-X, and we study the satisfiability of basic graph patterns. The theory is accompanied by an algorithm for deciding the satisfiability of basic graph patterns on Façade-X data sources. Furthermore, we provide extensive experiments with a proof-of-concept implementation, demonstrating practical feasibility, including with real-world queries. Our results pave the way for studying query execution strategies for Façade-X data access with SPARQL and supporting developers to build more efficient data integration systems for knowledge graphs.

</details>


### [60] [Data-Driven Trajectory Imputation for Vessel Mobility Analysis](https://arxiv.org/abs/2602.11890)
*Giannis Spiliopoulos,Alexandros Troupiotis-Kapeliaris,Kostas Patroumpas,Nikolaos Liapis,Dimitrios Skoutas,Dimitris Zissis,Nikos Bikakis*

Main category: cs.DB

TL;DR: 本文提出用于船舶轨迹的轻量级、可配置的HABIT插补框架，在精度上与基线方法相当，在延迟方面表现更好。


<details>
  <summary>Details</summary>
Motivation: 使用AIS数据时存在船舶轨迹有大的空白的问题，现有插补方法主要针对车辆轨迹，而船舶运动模式差异大，需要新的插补方法。

Method: 提出HABIT，一个基于H3聚合的船舶轨迹插补框架，从历史AIS数据中提取、分析和索引运动模式以插补缺失轨迹段。

Result: 在不同时间范围、密度和船舶类型的AIS数据上实证研究发现，HABIT在精度上与基线方法相当，延迟方面表现更好，同时考虑了船舶特征和运动模式。

Conclusion: HABIT是一种有效的船舶轨迹插补框架，能更好地处理船舶轨迹插补问题。

Abstract: Modeling vessel activity at sea is critical for a wide range of applications, including route planning, transportation logistics, maritime safety, and environmental monitoring. Over the past two decades, the Automatic Identification System (AIS) has enabled real-time monitoring of hundreds of thousands of vessels, generating huge amounts of data daily. One major challenge in using AIS data is the presence of large gaps in vessel trajectories, often caused by coverage limitations or intentional transmission interruptions. These gaps can significantly degrade data quality, resulting in inaccurate or incomplete analysis. State-of-the-art imputation approaches have mainly been devised to tackle gaps in vehicle trajectories, even when the underlying road network is not considered. But the motion patterns of sailing vessels differ substantially, e.g., smooth turns, maneuvering near ports, or navigating in adverse weather conditions. In this application paper, we propose HABIT, a lightweight, configurable H3 Aggregation-Based Imputation framework for vessel Trajectories. This data-driven framework provides a valuable means to impute missing trajectory segments by extracting, analyzing, and indexing motion patterns from historical AIS data. Our empirical study over AIS data across various timeframes, densities, and vessel types reveals that HABIT produces maritime trajectory imputations performing comparably to baseline methods in terms of accuracy, while performing better in terms of latency while accounting for vessel characteristics and their motion patterns.

</details>


### [61] [Designing and Comparing RPQ Semantics](https://arxiv.org/abs/2602.11949)
*Victor Marsault,Antoine Meyer*

Main category: cs.DB

TL;DR: 提出框架对正则路径查询（RPQ）语义进行分类和比较，指出一些属性相互排斥或无法满足，还给出新的RPQ语义示例。


<details>
  <summary>Details</summary>
Motivation: 为更好地理解、选择和设计RPQ语义，现有研究几乎只关注评估效率。

Method: 提出框架，根据其他标准对RPQ语义进行分类和比较，形式化相关属性。

Result: 发现部分属性相互排斥或无法满足，给出新的RPQ语义示例。

Conclusion: 新的RPQ语义示例或为未来图数据库查询语言语义设计提供思路。

Abstract: Modern property graph database query languages such as Cypher, PGQL, GSQL, and the standard GQL draw inspiration from the formalism of regular path queries (RPQs). In order to output walks explicitly, they depart from the classical and well-studied homomorphism semantics. However, it then becomes difficult to present results to users because RPQs may match infinitely many walks. The aforementioned languages use ad-hoc criteria to select a finite subset of those matches. For instance, Cypher uses trail semantics, discarding walks with repeated edges; PGQL and GSQL use shortest walk semantics, retaining only the walks of minimal length among all matched walks; and GQL allows users to choose from several semantics. Even though there is academic research on these semantics, it focuses almost exclusively on evaluation efficiency.
  In an attempt to better understand, choose and design RPQ semantics, we present a framework to categorize and compare them according to other criteria. We formalize several possible properties, pertaining to the study of RPQ semantics seen as mathematical functions mapping a database and a query to a finite set of walks. We show that some properties are mutually exclusive, or cannot be met. We also give several new RPQ semantics as examples. Some of them may provide ideas for the design of new semantics for future graph database query languages.

</details>


### [62] [DIVER: A Robust Text-to-SQL System with Dynamic Interactive Value Linking and Evidence Reasoning](https://arxiv.org/abs/2602.12064)
*Yafeng Nan,Haifeng Sun,Zirui Zhuang,Qi Qi,Guojun Chu,Jianxin Liao,Dan Pei,Jingyu Wang*

Main category: cs.DB

TL;DR: 现有Text - to - SQL模型在无专家帮助时性能不佳，提出DIVER系统通过自动证据推理和动态交互值链接提升模型鲁棒性，实验证明效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有Text - to - SQL模型性能依赖专家编写的证据，在无专家帮助的现实场景中性能严重下降，缺乏鲁棒性。

Method: 提出DIVER系统，利用兼容工具箱探测数据库，通过结构化工作区（CoTF）根据探测结果反思并选择新工具进行下一轮探测，自动迭代识别现有方法遗漏的模式和值链接。

Result: DIVER系统显著提升了各种Text - to - SQL模型的鲁棒性，执行准确率（EX）最高提升10.82%，有效效率得分（VES）最高提升16.09%；动态交互值链接提高了现有系统的鲁棒性和模式与值链接的准确性。

Conclusion: DIVER系统能够在无专家帮助的情况下实现鲁棒的Text - to - SQL。

Abstract: In the era of large language models, Text-to-SQL, as a natural language interface for databases, is playing an increasingly important role. The sota Text-to-SQL models have achieved impressive accuracy, but their performance critically relies on expert-written evidence, which typically clarifies schema and value linking that existing models struggle to identify. Such limitations stem from the ambiguity of user queries and, more importantly, the complexity of comprehending large-scale and dynamic database values. Consequently, in real-world scenarios where expert assistance is unavailable, existing methods suffer a severe performance collapse, with execution accuracy dropping by over 10%. This underscores their lack of robustness. To address this, we propose DIVER, a robust system that automates evidence reasoning with dynamic interactive value linking. It leverages a compatible toolbox containing diverse tools to probe the database. Then, restricted by a structured workspace (CoTF, Chain of Thoughts and Facts), it reflects based on probe results and selects a new tool for next round of probing. Through this automatically iterative process, DIVER identifies schema and value linking missed by existing methods. Based on these accurate linkings, DIVER is able to infer correct usage of SQL functions and formulas and generate high-quality evidence, achieving robust Text-to-SQL without expert assistance. Extensive experiments demonstrate that: 1) The DIVER system significantly enhances the robustness of various Text-to-SQL models, improving performance by up to 10.82% in Execution Accuracy (EX) and 16.09% in Valid Efficiency Score (VES). 2) Our dynamic interactive value linking significantly improves the robustness of existing systems and the accuracy of schema and value linking, especially when confronted with challenges posed by large-scale, dynamic database values.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [63] [Real Life Is Uncertain. Consensus Should Be Too!](https://arxiv.org/abs/2602.11362)
*Reginald Frank,Soujanya Ponnapalli,Octavio Lomeli,Neil Giridharan,Marcos K Aguilera,Natacha Crooks*

Main category: cs.DC

TL;DR: 指出传统共识协议的f阈值故障模型过于简化，提出概率故障模型可使系统更优。


<details>
  <summary>Details</summary>
Motivation: 传统共识协议的f阈值故障模型简化现实世界，限制成本和性能优化机会。

Method: 提出采用概率故障模型的概率共识协议，利用机器故障曲线。

Result: 可使系统避免传统瓶颈，更可靠、高效、经济且可持续。

Conclusion: 概率故障模型更适合现代分布式系统优化。

Abstract: Modern distributed systems rely on consensus protocols to build a fault-tolerant-core upon which they can build applications. Consensus protocols are correct under a specific failure model, where up to $f$ machines can fail. We argue that this $f$-threshold failure model oversimplifies the real world and limits potential opportunities to optimize for cost or performance. We argue instead for a probabilistic failure model that captures the complex and nuanced nature of faults observed in practice. Probabilistic consensus protocols can explicitly leverage individual machine \textit{failure curves} and explore side-stepping traditional bottlenecks such as majority quorum intersection, enabling systems that are more reliable, efficient, cost-effective, and sustainable.

</details>


### [64] [RL over Commodity Networks: Overcoming the Bandwidth Barrier with Lossless Sparse Deltas](https://arxiv.org/abs/2602.11456)
*Chaoyi Ruan,Geng Luo,Xinyi Wan,Long Zhao,Qinghe Wang,Jiaan Zhu,Duling Xu,Guanbin Xu,Dehui Wei,Xiang Liu,Cheng Li,Haifeng Sun,Congcong Miao,Jialin Li*

Main category: cs.DC

TL;DR: 提出SparrowRL系统，利用RL微调更新稀疏性，用于普通网络GPU资源，减少传输负载、提高吞吐量和性价比。


<details>
  <summary>Details</summary>
Motivation: 传统LLM的RL后训练依赖专用RDMA HPC集群，成本高，普通网络同步大模型参数慢，需改进。

Method: SparrowRL将每一步表示为稀疏增量检查点，将增量提取与多流传输进行流水线处理，将传输与滚动生成重叠，通过吞吐量和带宽感知调度和基于租约的容错协调异构工作节点。

Result: 在Qwen3模型上，SparrowRL减少了传输负载，提高了吞吐量，缩小了与理想RDMA单数据中心基线的差距，且性价比更高。

Conclusion: SparrowRL能有效利用普通网络的GPU资源进行高性能RL训练，具有成本效益。

Abstract: LLM post-training with reinforcement learning (RL) requires frequent synchronization of large model parameters between the trainer and distributed rollout actors. High-throughput RL post-training therefore relies on dedicated RDMA HPC clusters, an infrastructure cost most organizations cannot absorb. A natural alternative is to aggregate loosely-coupled GPUs over standard Ethernet and WAN links, but this commodity connectivity cannot sustain full-weight broadcasts: synchronizing an 8B model can take over 100~seconds on bandwidth-limited links, while rollout generation typically takes tens of seconds.
  Toward making RL practical in this regime, we observe that RL fine-tuning yields highly sparse per-step updates, with only around 1\% of parameter elements changing. Atop this insight, we present SparrowRL, a novel high-performance RL training system that preserves bit-exact updates without dropping or quantizing information, designed for commodity-networked, loosely-coupled GPU resources. SparrowRL represents each step as a sparse delta checkpoint, pipelines delta extraction with multi-stream transmission, overlaps transfer with rollout generation, and coordinates heterogeneous workers with throughput- and bandwidth-aware scheduling plus lease-based fault tolerance. On Qwen3 models from 4B to 14B deployed across up to four geographic regions, SparrowRL reduces per-step transfer payload by 79$\times$ for Qwen3-8B and improves throughput by 2.4--9.5$\times$ over full-weight broadcast across WAN, narrowing the throughput gap relative to an ideal RDMA single-datacenter baseline to within 8.91\%. By leveraging on-demand, cross-cloud GPUs over commodity links, SparrowRL delivers 1.21--1.59$\times$ higher tokens per dollar than reserved RDMA clusters at comparable throughput.

</details>


### [65] [Designing Scalable Rate Limiting Systems: Algorithms, Architecture, and Distributed Solutions](https://arxiv.org/abs/2602.11741)
*Bo Guan*

Main category: cs.DC

TL;DR: 本文提出生产级分布式限流系统架构，用Redis及Sorted Set，量化Rolling Window算法权衡，用Lua脚本消除竞态，提出三层架构，分析Redis Cluster部署并采用AP策略。


<details>
  <summary>Details</summary>
Motivation: 设计同时具备准确性、可用性和可扩展性的限流系统在分布式系统中是一个挑战，存在算法精度、可用性、一致性和分区容错性之间的权衡。

Method: 选择Redis及其Sorted Set数据结构；量化Rolling Window算法与Token Bucket和Fixed Window算法的准确性和内存成本权衡；使用服务器端Lua脚本将清理、计数和插入操作捆绑为原子操作；提出三层架构管理限流规则；分析在Redis Cluster上的部署并采用AP策略。

Result: 未明确提及具体实验或实际运行结果，但架构可利用Redis特性实现高效低延迟操作，消除并发环境竞态条件，规则可灵活更改，通过Redis Cluster提供可用性和可扩展性。

Conclusion: 该分布式限流系统架构通过合理选择技术和策略，能在生产级环境中平衡准确性、可用性和可扩展性，接受AP作为工程权衡。

Abstract: Designing a rate limiter that is simultaneously accurate, available, and scalable presents a fundamental challenge in distributed systems, primarily due to the trade-offs between algorithmic precision, availability, consistency, and partition tolerance. This article presents a concrete architecture for a distributed rate limiting system in a production-grade environment. Our design chooses the in-memory cache database, the Redis, along with its Sorted Set data structure, which provides $O(log (N))$ time complexity operation for the key-value pair dataset with efficiency and low latency, and maintains precision. The core contribution is quantifying the accuracy and memory cost trade-off of the chosen Rolling Window as the implemented rate limiting algorithm against the Token Bucket and Fixed Window algorithms. In addition, we explain how server-side Lua scripting is critical to bundling cleanup, counting, and insertion into a single atomic operation, thereby eliminating race conditions in concurrent environments. In the system architecture, we propose a three-layer architecture that manages the storage and updating of the limit rules. Through script load by hashing the rule parameters, rules can be changed without modifying the cached scripts. Furthermore, we analyze the deployment of this architecture on a Redis Cluster, which provides the availability and scalability by data sharding and replication. We explain the acceptance of AP (Availability and Partition Tolerance) from the CAP theorem as the pragmatic engineering trade-off for this use case.

</details>


### [66] [Differentially Private Perturbed Push-Sum Protocol and Its Application in Non-Convex Optimization](https://arxiv.org/abs/2602.11544)
*Yiming Zhou,Kaiping Xue,Enhong Chen*

Main category: cs.DC

TL;DR: 本文提出DPPS协议用于去中心化通信保护隐私，设计PartPSP算法用于非凸优化，理论证明和实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有去中心化网络隐私保护方法多针对特定下游任务，缺乏通用协议级解决方案。

Method: 提出DPPS协议并引入新的敏感度估计机制；设计PartPSP算法，将模型参数分区，对共享参数用DPPS。

Result: 理论证明PartPSP在非凸目标下收敛，实验验证DPPS隐私保护效果和PartPSP性能优于现有算法。

Conclusion: DPPS可作为即插即用的隐私保护方案，PartPSP能在相同隐私预算下提升优化性能。

Abstract: In decentralized networks, nodes cannot ensure that their shared information will be securely preserved by their neighbors, making privacy vulnerable to inference by curious nodes. Adding calibrated random noise before communication to satisfy differential privacy offers a proven defense; however, most existing methods are tailored to specific downstream tasks and lack a general, protocol-level privacy-preserving solution. To bridge this gap, we propose Differentially Private Perturbed Push-Sum (DPPS), a lightweight differential privacy protocol for decentralized communication. Since protocol-level differential privacy introduces the unique challenge of obtaining the sensitivity for each communication round, DPPS introduces a novel sensitivity estimation mechanism that requires each node to compute and broadcast only one scalar per round, enabling rigorous differential privacy guarantees. This design allows DPPS to serve as a plug-and-play, low-cost privacy-preserving solution for downstream applications built on it. To provide a concrete instantiation of DPPS and better balance the privacy-utility trade-off, we design PartPSP, a privacy-preserving decentralized algorithm for non-convex optimization that integrates a partial communication mechanism. By partitioning model parameters into local and shared components and applying DPPS only to the shared parameters, PartPSP reduces the dimensionality of consensus data, thereby lowering the magnitude of injected noise and improving optimization performance. We theoretically prove that PartPSP converges under non-convex objectives and, with partial communication, achieves better optimization performance under the same privacy budget. Experimental results validate the effectiveness of DPPS's privacy-preserving and demonstrate that PartPSP outperforms existing privacy-preserving decentralized optimization algorithms.

</details>


### [67] [LAER-MoE: Load-Adaptive Expert Re-layout for Efficient Mixture-of-Experts Training](https://arxiv.org/abs/2602.11686)
*Xinyi Liu,Yujie Wang,Fangcheng Fu,Xuefeng Xiao,Huixia Li,Jiashi Li,Bin Cui*

Main category: cs.DC

TL;DR: 提出LAER - MoE框架解决MoE模型专家并行训练负载不均衡问题，实验显示有加速效果。


<details>
  <summary>Details</summary>
Motivation: 专家并行训练中动态路由导致专家间负载不均衡，成为训练瓶颈。

Method: 引入LAER - MoE框架，采用FSEP并行范式，对通信操作细粒度调度，开发负载均衡规划器。

Result: 在A100集群实验中，相比现有最优训练系统实现最高1.69倍加速。

Conclusion: LAER - MoE框架能有效解决MoE模型专家并行训练的负载不均衡问题，提升训练效率。

Abstract: Expert parallelism is vital for effectively training Mixture-of-Experts (MoE) models, enabling different devices to host distinct experts, with each device processing different input data. However, during expert parallel training, dynamic routing results in significant load imbalance among experts: a handful of overloaded experts hinder overall iteration, emerging as a training bottleneck.
  In this paper, we introduce LAER-MoE, an efficient MoE training framework. The core of LAER-MoE is a novel parallel paradigm, Fully Sharded Expert Parallel (FSEP), which fully partitions each expert parameter by the number of devices and restores partial experts at expert granularity through All-to-All communication during training. This allows for flexible re-layout of expert parameters during training to enhance load balancing. In particular, we perform fine-grained scheduling of communication operations to minimize communication overhead. Additionally, we develop a load balancing planner to formulate re-layout strategies of experts and routing schemes for tokens during training. We perform experiments on an A100 cluster, and the results indicate that our system achieves up to 1.69x acceleration compared to the current state-of-the-art training systems. Source code available at https://github.com/PKU-DAIR/Hetu-Galvatron/tree/laer-moe.

</details>


### [68] [An Auction-Based Mechanism for Optimal Task Allocation and Resource Aware Containerization](https://arxiv.org/abs/2602.11998)
*Ramakant kumar*

Main category: cs.DC

TL;DR: 本文提出AUC - RAC机制用于物联网设备多本地服务器间计算任务高效卸载，利用Docker swarm和容器化，通过拍卖机制优化任务分配，实验证明可改善卸载和计算服务。


<details>
  <summary>Details</summary>
Motivation: 在基于云的物联网容器化中，资源管理和成本优化对任务成功执行构成挑战，需要高效的计算任务卸载方法。

Method: 提出AUC - RAC机制，利用Docker swarm连接本地服务器，采用Manager Node（MN）和Worker Nodes（WNs）形式，通过Docker容器化并行执行任务，MN接收任务后让WNs参与拍卖式竞价过程优化任务分配。

Result: 实验分析表明该方法通过实现本地服务器间的合作，为物联网设备提供了更好的卸载和计算密集型服务。

Conclusion: AUC - RAC机制能高效卸载计算任务，优化任务在多系统中的分配，对物联网设备计算服务有积极意义。

Abstract: Distributed computing has enabled cooperation between multiple computing devices for the simultaneous execution of resource-hungry tasks. Such execution also plays a pivotal role in the parallel execution of numerous tasks in the Internet of Things (IoT) environment. Leveraging the computing resources of multiple devices, the offloading and processing of computationintensive tasks can be carried out more efficiently. However, managing resources and optimizing costs remain challenging for successfully executing tasks in cloud-based containerization for IoT. This paper proposes AUC-RAC, an auction-based mechanism for efficient offloading of computation tasks among multiple local servers in the context of IoT devices. The approach leverages the concept of Docker swarm, which connects multiple local servers in the form of Manager Node (MN) and Worker Nodes (WNs). It uses Docker containerization to execute tasks simultaneously. In this system, IoT devices send tasks to the MN, which then sends the task details to all its WNs to participate in the auction-based bidding process. The auctionbased bidding process optimizes the allocation of computation tasks among multiple systems, considering their resource sufficiency. The experimental analysis establishes that the approach offers improved offloading and computation-intensive services for IoT devices by enabling cooperation between local servers.

</details>


### [69] [Contention Resolution, With and Without a Global Clock](https://arxiv.org/abs/2602.12070)
*Zixi Cai,Kuowen Chen,Shengquan Du,Tsvi Kopelowitz,Seth Pettie,Ben Plosk*

Main category: cs.DC

TL;DR: 本文研究争用解决问题，考虑全局时钟假设，给出新协议，分析复杂度差距及证明同时优化两种指标的不可能性。


<details>
  <summary>Details</summary>
Motivation: 以往争用解决问题研究多假设无全局时钟，而全局时钟假设在技术上现实且算法上有趣，能丰富问题和引入新方法。

Method: 设计新的争用解决协议，对不同假设下的随机争用解决协议进行分析。

Result: 1. 新协议保证特定期望和高概率下的延迟；2. 证明无记忆协议两种目标间存在log n复杂度差距；3. 证明无法同时优化两种指标。

Conclusion: 全局时钟假设下争用解决问题有新的复杂度结果，且无法同时在两种指标下达到最优。

Abstract: In the Contention Resolution problem $n$ parties each wish to have exclusive use of a shared resource for one unit of time. The problem has been studied since the early 1970s, under a variety of assumptions on feedback given to the parties, how the parties wake up, knowledge of $n$, and so on. The most consistent assumption is that parties do not have access to a global clock, only their local time since wake-up. This is surprising because the assumption of a global clock is both technologically realistic and algorithmically interesting. It enriches the problem, and opens the door to entirely new techniques. Our primary results are: [1] We design a new Contention Resolution protocol that guarantees latency $$O\left(\left(n\log\log n\log^{(3)} n\log^{(4)} n\cdots \log^{(\log^* n)} n\right)\cdot 2^{\log^* n}\right) \le n(\log\log n)^{1+o(1)}$$ in expectation and with high probability. This already establishes at least a roughly $\log n$ complexity gap between randomized protocols in GlobalClock and LocalClock. [2] Prior analyses of randomized ContentionResolution protocols in LocalClock guaranteed a certain latency with high probability, i.e., with probability $1-1/\text{poly}(n)$. We observe that it is just as natural to measure expected latency, and prove a $\log n$-factor complexity gap between the two objectives for memoryless protocols. The In-Expectation complexity is $Θ(n \log n/\log\log n)$ whereas the With-High-Probability latency is $Θ(n\log^2 n/\log\log n)$. Three of these four upper and lower bounds are new. [3] Given the complexity separation above, one would naturally want a ContentionResolution protocol that is optimal under both the In-Expectation and With-High-Probability metrics. This is impossible! It is even impossible to achieve In-Expectation latency $o(n\log^2 n/(\log\log n)^2)$ and With-High-Probability latency $n\log^{O(1)} n$ simultaneously.

</details>


### [70] [OServe: Accelerating LLM Serving via Spatial-Temporal Workload Orchestration](https://arxiv.org/abs/2602.12151)
*Youhe Jiang,Fangcheng Fu,Taiyi Wang,Guoliang He,Eiko Yoneki*

Main category: cs.DC

TL;DR: 现有大语言模型服务系统假设工作负载均匀稳定，与实际不符，OServe 系统通过新调度算法和工作负载自适应切换方法解决时空异质性问题，性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型服务系统假设工作负载均匀稳定，与实际时空异质性不匹配，导致性能不佳。

Method: 引入工作负载感知调度算法，根据实时工作负载特征优化异构模型部署；提出工作负载自适应切换方法，根据预测的工作负载变化迁移模型部署。

Result: 在真实轨迹实验中，与现有最先进的服务系统相比，OServe 性能提升最高达 2 倍，平均 1.5 倍。

Conclusion: OServe 系统能够有效解决大语言模型服务中的时空异质性问题，提升系统性能。

Abstract: Serving Large Language Models (LLMs) can benefit immensely from parallelizing both the model and input requests across multiple devices, but incoming workloads exhibit substantial spatial and temporal heterogeneity. Spatially, workloads comprise heterogeneous requests with varying compute and memory demands. Temporally, workload composition varies over time. Nevertheless, existing systems typically assume spatially uniform and temporally stable workloads, employing a homogeneous, static model deployment. This mismatch between the assumption and real-world spatial-temporal heterogeneity results in suboptimal performance. We present OServe, an LLM serving system with heterogeneous and flexible model deployment that addresses both spatial and temporal heterogeneity. First, OServe introduces a novel workload-aware scheduling algorithm that optimizes heterogeneous model deployments according to real-time workload characteristics. Second, OServe proposes an efficient workload-adaptive switching method that migrates model deployments in response to predicted workload changes. Experiments on real-world traces show that OServe improves performance by up to 2$\times$ (average: 1.5$\times$) compared to state-of-the-art serving systems.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [71] [Time-Optimal Construction of String Synchronizing Sets](https://arxiv.org/abs/2602.11324)
*Jonas Ellert,Tomasz Kociumaka*

Main category: cs.DS

TL;DR: 本文提出新方法，可在O(n logσ / log n)时间预处理字符串，以最优时间构造τ - 同步集，还能实现高效查询。


<details>
  <summary>Details</summary>
Motivation: 改进字符串τ - 同步集的构造时间，提升相关算法效率。

Method: 采用自定义变长编码处理稀疏整数序列的新框架，改进van Emde Boas树。

Result: 实现预处理时间O(n logσ / log n)，构造时间O((n logτ) / (τ log n))，支持高效查询。

Conclusion: 提出的方法在字RAM模型下时间复杂度最优，能高效构造和查询τ - 同步集。

Abstract: A key principle in string processing is local consistency: using short contexts to handle matching fragments of a string consistently. String synchronizing sets [Kempa, Kociumaka; STOC 2019] are an influential instantiation of this principle. A $τ$-synchronizing set of a length-$n$ string is a set of $O(n/τ)$ positions, chosen via their length-$2τ$ contexts, such that (outside highly periodic regions) at least one position in every length-$τ$ window is selected. Among their applications are faster algorithms for data compression, text indexing, and string similarity in the word RAM model.
  We show how to preprocess any string $T \in [0..σ)^n$ in $O(n\logσ/\log n)$ time so that, for any $τ\in[1..n]$, a $τ$-synchronizing set of $T$ can be constructed in $O((n\logτ)/(τ\log n))$ time. Both bounds are optimal in the word RAM model with word size $w=Θ(\log n)$. Previously, the construction time was $O(n/τ)$, either after an $O(n)$-time preprocessing [Kociumaka, Radoszewski, Rytter, Waleń; SICOMP 2024], or without preprocessing if $τ<0.2\log_σn$ [Kempa, Kociumaka; STOC 2019].
  A simple version of our method outputs the set as a sorted list in $O(n/τ)$ time, or as a bitmask in $O(n/\log n)$ time. Our optimal construction produces a compact fully indexable dictionary, supporting select queries in $O(1)$ time and rank queries in $O(\log(\tfrac{\logτ}{\log\log n}))$ time, matching unconditional cell-probe lower bounds for $τ\le n^{1-Ω(1)}$.
  We achieve this via a new framework for processing sparse integer sequences in a custom variable-length encoding. For rank and select queries, we augment the optimal variant of van Emde Boas trees [Pătraşcu, Thorup; STOC 2006] with a deterministic linear-time construction. The above query-time guarantees hold after preprocessing time proportional to the encoding size (in words).

</details>


### [72] [Preprocessed 3SUM for Unknown Universes with Subquadratic Space](https://arxiv.org/abs/2602.11363)
*Yael Kirkpatrick,John Kuszmaul,Surya Mathialagan,Virginia Vassilevska Williams*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider the classic 3SUM problem: given sets of integers $A, B, C $, determine whether there is a tuple $(a, b, c) \in A \times B \times C$ satisfying $a + b + c = 0$. The 3SUM Hypothesis, central in fine-grained complexity, states that there does not exist a truly subquadratic time 3SUM algorithm. Given this long-standing barrier, recent work over the past decade has explored 3SUM from a data structural perspective. Specifically, in the 3SUM in preprocessed universes regime, we are tasked with preprocessing sets $A, B$ of size $n$, to create a space-efficient data structure that can quickly answer queries, each of which is a 3SUM problem of the form $A', B', C'$, where $A' \subseteq A$ and $B' \subseteq B$. A series of results have achieved $\tilde{O}(n^2)$ preprocessing time, $\tilde{O}(n^2)$ space, and query time improving progressively from $\tilde{O}(n^{1.9})$ [CL15] to $\tilde{O}(n^{11/6})$ [CVX23] to $\tilde{O}(n^{1.5})$ [KPS25]. Given these series of works improving query time, a natural open question has emerged: can one achieve both truly subquadratic space and truly subquadratic query time for 3SUM in preprocessed universes?
  We resolve this question affirmatively, presenting a tradeoff curve between query and space complexity. Specifically, we present a simple randomized algorithm achieving $\tilde{O}(n^{1.5 + \varepsilon})$ query time and $\tilde{O}(n^{2 - 2\varepsilon/3})$ space complexity. Furthermore, our algorithm has $\tilde{O}(n^2)$ preprocessing time, matching past work. Notably, quadratic preprocessing is likely necessary for our tradeoff as either the preprocessing or the query time must be at least $n^{2-o(1)}$ under the 3SUM Hypothesis.

</details>


### [73] [Adaptive Power Iteration Method for Differentially Private PCA](https://arxiv.org/abs/2602.11454)
*Ta Duy Nguyem,Alina Ene,Huy Le Nguyen*

Main category: cs.DS

TL;DR: 研究矩阵近似计算最大奇异向量的(ε,δ)-差分隐私算法，基于过滤技术改进私有幂迭代法。


<details>
  <summary>Details</summary>
Motivation: 为矩阵近似计算最大奇异向量问题设计差分隐私算法，且与已有模型的工作形成互补。

Method: 研究私有幂迭代法，采用适应输入矩阵相干参数的过滤技术。

Result: 算法在低相干参数矩阵上的效用超越最坏情况保证。

Conclusion: 本文工作与Hardt - Roth的工作不同且相互补充。

Abstract: We study $(ε,δ)$-differentially private algorithms for the problem of approximately computing the top singular vector of a matrix $A\in\mathbb{R}^{n\times d}$ where each row of $A$ is a datapoint in $\mathbb{R}^{d}$. In our privacy model, neighboring inputs differ by one single row/datapoint. We study the private variant of the power iteration method, which is widely adopted in practice. Our algorithm is based on a filtering technique which adapts to the coherence parameter of the input matrix. This technique provides a utility that goes beyond the worst-case guarantees for matrices with low coherence parameter. Our work departs from and complements the work by Hardt-Roth (STOC 2013) which designed a private power iteration method for the privacy model where neighboring inputs differ in one single entry by at most 1.

</details>


### [74] [Gray Codes With Constant Delay and Constant Auxiliary Space](https://arxiv.org/abs/2602.11791)
*Antoine Amarilli,Claire David,Nadime Francis,Victor Marsault,Mikaël Monet,Yann Strozecki*

Main category: cs.DS

TL;DR: 本文提出两种新算法在磁带机和双端队列机模型下枚举所有二进制字，保证延迟和辅助空间为常数，还研究受限模型并证明其不能以常数辅助空间枚举。双端队列机的研究为有向图中枚举路径奠定基础。


<details>
  <summary>Details</summary>
Motivation: 寻找能以常数延迟和常数辅助空间枚举所有二进制字的算法，且双端队列机研究与图数据库查询处理相关。

Method: 在磁带机和双端队列机两种计算模型下设计算法，磁带机通过编辑工作带，双端队列机通过端点的推和弹出操作。

Result: 在磁带机模型下构造了实现斜容错准格雷码和格雷码的机器；在双端队列机模型下构造了能以常数延迟枚举所有二进制字的机器。

Conclusion: 磁带机和双端队列机能以常数延迟和常数辅助空间枚举所有二进制字，队列机和栈机不能；双端队列机的研究是有向图常数延迟和常数辅助空间枚举路径的第一步。

Abstract: We give the first two algorithms to enumerate all binary words of $\{0,1\}^\ell$ (like Gray codes) while ensuring that the delay and the auxiliary space is independent from $\ell$, i.e., constant time for each word, and constant memory in addition to the $\ell$ bits storing the current word. Our algorithms are given in two new computational models: tape machines and deque machines. We also study more restricted models, queue machines and stack machines, and show that they cannot enumerate all binary words with constant auxiliary space, even with unrestricted delay.
  A tape machine is a Turing machine that stores the current binary word on a single working tape of length $\ell$. The machine has a single head and must edit its tape to reach all possible words of $\{0,1\}^{\ell}$ , and output them (in unit time, by entering special output states), with no duplicates. We construct a tape machine that achieves this task with constant delay between consecutive outputs, which implies that the machine implements a so-called skew-tolerant quasi-Gray code. We then construct a more involved tape machine that implements a Gray code.
  A deque machine stores the current binary word on a double-ended queue of length $\ell$, and stores a constant-size internal state. It works as a tape machine, except that it modifies the content of the deque by performing push and pop operations on the endpoints. We construct deque machines that enumerate all words of $\{0,1\}^\ell$ with constant-delay. The main technical challenge in this model is to correctly detect when enumeration has finished.
  Our work on deque machine is also motivated by other contexts in which endpoint modifications occur naturally. In particular, our result is a first step towards enumerating walks in directed graphs with constant delay and constant auxiliary space, addressing a core task in modern graph database query processing.

</details>


### [75] [Combinatorial Perpetual Scheduling](https://arxiv.org/abs/2602.11826)
*Mirabel Mendoza-Cadena,Arturo Merino,Mads Anker Nielsen,Kevin Schewior*

Main category: cs.DS

TL;DR: 本文提出组合永久调度问题框架，针对组合竹花园修剪和组合风车调度问题给出高度界限及算法。


<details>
  <summary>Details</summary>
Motivation: 解决组合永久调度问题，满足元素出现频率要求，最小化元素最大高度。

Method: 利用拟阵交多面体的整性证明结果，为特定拟阵类设计高效算法。

Result: 当(E,ℐ)为拟阵时可保证最大高度至多为2；对特定拟阵类给出不同最大高度；一般集系最优保证高度为Θ(log |E|)；对组合风车调度给出可调度性的密度界限。

Conclusion: 为组合永久调度问题提供框架、算法及高度界限，不同情况有不同结果。

Abstract: This paper introduces a framework for combinatorial variants of perpetual-scheduling problems. Given a set system $(E,\mathcal{I})$, a schedule consists of an independent set $I_t \in \mathcal{I}$ for every time step $t \in \mathbb{N}$, with the objective of fulfilling frequency requirements on the occurrence of elements in $E$. We focus specifically on combinatorial bamboo garden trimming, where elements accumulate height at growth rates $g(e)$ for $e \in E$ given as a convex combination of incidence vectors of $\mathcal{I}$ and are reset to zero when scheduled, with the goal of minimizing the maximum height attained by any element.
  Using the integrality of the matroid-intersection polytope, we prove that, when $(E,\mathcal{I})$ is a matroid, it is possible to guarantee a maximum height of at most 2, which is optimal. We complement this existential result with efficient algorithms for specific matroid classes, achieving a maximum height of 2 for uniform and partition matroids, and 4 for graphic and laminar matroids. In contrast, we show that for general set systems, the optimal guaranteed height is $Θ(\log |E|)$ and can be achieved by an efficient algorithm. For combinatorial pinwheel scheduling, where each element $e\in E$ needs to occur in the schedule at least every $a_e \in \mathbb{N}$ time steps, our results imply bounds on the density sufficient for schedulability.

</details>


### [76] [History-Independent Load Balancing](https://arxiv.org/abs/2602.11953)
*Michael A. Bender,William Kuszmaul,Elaine Shi,Rose Silver*

Main category: cs.DS

TL;DR: 提出一种（强）历史无关的双选择球与箱子算法，支持插入和删除操作，有负载和预期追索保证。


<details>
  <summary>Details</summary>
Motivation: 寻找能支持插入和删除操作，在球与箱子问题上有较好负载和追索性能的算法。

Method: 设计了一个（强）历史无关的双选择球与箱子算法。

Result: 保证最大负载为 $m / n + O(1)$ 的高概率，每次操作的预期追索为 $O(\log \log (m/n))$。

Conclusion: 这是首个在 $m/n \ge \omega(1)$ 时实现非平凡保证，且首次实现 $O(1)$ 过载和 $o(m/n)$ 预期追索的全动态解决方案。

Abstract: We give a (strongly) history-independent two-choice balls-and-bins algorithm on $n$ bins that supports both insertions and deletions on a set of up to $m$ balls, while guaranteeing a maximum load of $m / n + O(1)$ with high probability, and achieving an expected recourse of $O(\log \log (m/n))$ per operation. To the best of our knowledge, this is the first history-independent solution to achieve nontrivial guarantees of any sort for $m/n \ge ω(1)$ and is the first fully dynamic solution (history independent or not) to achieve $O(1)$ overload with $o(m/n)$ expected recourse.

</details>


### [77] [Optimizing Distances for Multi-Broadcast in Temporal Graphs](https://arxiv.org/abs/2602.12126)
*Daniele Carnevale,Gianlorenzo D'Angelo*

Main category: cs.DS

TL;DR: 引入D - Temporal Multi - Broadcast (D - TMB) 问题，分析其在不同时间距离定义下的计算复杂度和可近似性。


<details>
  <summary>Details</summary>
Motivation: 受物流、多智能体信息传播和无线网络应用的启发，引入D - TMB问题。

Method: 分析D - TMB在六种时间距离定义（EA、LD、FT、ST、MH和MW）下的情况，给出单源和多源的复杂度分析及近似算法。

Result: 单源下，EA和LD可多项式时间求解，其他是NP难且近似困难；多源下，若无先验可行性假设，问题不可近似，还找出了EA和LD可处理的结构条件。

Conclusion: 全面刻画了D - TMB问题在不同场景和距离定义下的计算复杂度和可近似性。

Abstract: Temporal graphs represent networks in which connections change over time, with edges available only at specific moments. Motivated by applications in logistics, multi-agent information spreading, and wireless networks, we introduce the D-Temporal Multi-Broadcast (D-TMB) problem, which asks for scheduling the availability of edges so that a predetermined subset of sources reach all other vertices while optimizing the worst-case temporal distance D from any source. We show that D-TMB generalizes ReachFast (arXiv:2112.08797). We then characterize the computational complexity and approximability of D-TMB under six definitions of temporal distance D, namely Earliest-Arrival (EA), Latest-Departure (LD), Fastest-Time (FT), Shortest-Traveling (ST), Minimum-Hop (MH), and Minimum-Waiting (MW). For a single source, we show that D-TMB can be solved in polynomial time for EA and LD, while for the other temporal distances it is NP-hard and hard to approximate within a factor that depends on the adopted distance function. We give approximation algorithms for FT and MW. For multiple sources, if feasibility is not assumed a priori, the problem is inapproximable within any factor unless P = NP, even with just two sources. We complement this negative result by identifying structural conditions that guarantee tractability for EA and LD for any number of sources.

</details>


### [78] [Improved Online Algorithms for Inventory Management Problems with Holding and Delay Costs: Riding the Wave Makes Things Simpler, Stronger, & More General](https://arxiv.org/abs/2602.12175)
*David Shmoys,Varun Suriyanarayana,Seeun William Umboh*

Main category: cs.DS

TL;DR: 本文针对联合补货问题，改进了在线补货算法的竞争比，处理了任意单调的特定需求持有和延迟成本函数，还为单物品批量规模问题提供了新算法。


<details>
  <summary>Details</summary>
Motivation: Moseley等人的算法有竞争比和成本函数一致性假设的局限，本文旨在改进。

Method: 提出5 - 竞争原对偶算法，借鉴前人保持波前对偶解的方法，通过按需求延迟成本达到当前持有成本的时间排序决定提前服务的请求；为单物品批量规模问题提供新算法。

Result: 5 - 竞争算法能处理任意单调需求特定成本函数，单物品批量规模问题新算法竞争比约为2.681。

Conclusion: 本文算法在联合补货问题和单物品批量规模问题上都有优势，改进了竞争比并放松了一致性假设。

Abstract: The Joint Replenishment Problem (JRP) is a classical inventory management problem, that aims to model the trade-off between coordinating orders for multiple commodities (and their cost) with holding costs incurred by meeting demand in advance. Moseley, Niaparast and Ravi introduced a natural online generalization of the JRP in which inventory corresponding to demands may be replenished late, for a delay cost, or early, for a holding cost. They established that when the holding and delay costs are monotone and uniform across demands, there is a 30-competitive algorithm that employs a greedy strategy and a dual-fitting based analysis.
  We develop a 5-competitive algorithm that handles arbitrary monotone demand-specific holding and delay cost functions, thus simultaneously improving upon the competitive ratio and relaxing the uniformity assumption. Our primal-dual algorithm is in the spirit of the work Buchbinder, Kimbrel, Levi, Makarychev, and Sviridenko, which maintains a wavefront dual solution to decide when to place an order and which items to order. The main twist is in deciding which requests to serve early. In contrast to the work of Moseley et al., which ranks early requests in ascending order of desired service time and serves them until their total holding cost matches the ordering cost incurred for that item, we extend to the non-uniform case by instead ranking in ascending order of when the delay cost of a demand would reach its current holding cost. An important special case of the JRP is the single-item lot-sizing problem. Here, Moseley et al. gave a 3-competitive algorithm when the holding and delay costs are uniform across demands. We provide a new algorithm for which the competitive ratio is $φ+1 \approx 2.681$, where $φ$ is the golden ratio, which again holds for arbitrary monotone holding-delay costs.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [79] [Computing stable limit cycles of learning in games](https://arxiv.org/abs/2602.11315)
*Oliver Biggar,Christos Papadimitriou*

Main category: cs.GT

TL;DR: 本文针对虚构博弈/最优响应动态和复制者动态，给出了周期序列是否稳定的完整计算答案，证明两种动态下稳定性的等价性，提供多项式时间谱稳定测试方法，并给出稳定性的充分条件。


<details>
  <summary>Details</summary>
Motivation: 许多学习动态在N人博弈中不收敛，循环是基本现象，但哪种循环在博弈动态下稳定这一核心问题未解决，本文旨在回答该问题。

Method: 对虚构博弈/最优响应动态和复制者动态进行研究，给出多项式时间谱稳定测试，分析偏好图的汇均衡。

Result: （1）一个周期序列在一种动态下稳定当且仅当在另一种动态下稳定；（2）提供多项式时间谱稳定测试判断周期序列稳定性；（3）博弈偏好图的汇均衡循环是稳定的，且是复制者动态的吸引子。

Conclusion: 本文结果推广了Shapley（1964）和Jordan（1993）的著名定理，拓展了偏好图与复制者吸引子相关研究的边界。

Abstract: Many well-studied learning dynamics, such as fictitious play and the replicator, are known to not converge in general $N$-player games. The simplest mode of non-convergence is cyclical or periodic behavior. Such cycles are fundamental objects, and have inspired a number of significant insights in the field, beginning with the pioneering work of Shapley (1964). However a central question remains unanswered: which cycles are stable under game dynamics? In this paper we give a complete and computational answer to this question for the two best-studied dynamics, fictitious play/best-response dynamics and the replicator dynamic. We show (1) that a periodic sequence of profiles is stable under one of these dynamics if and only it is stable under the other, and (2) we provide a polynomial-time spectral stability test to determine whether a given periodic sequence is stable under either dynamic. Finally, we give an entirely `structural' sufficient condition for stability: every cycle that is a sink equilibrium of the preference graph of the game is stable, and moreover it is an attractor of the replicator dynamic. This result generalizes the famous theorems of Shapley (1964) and Jordan (1993), and extends the frontier of recent work relating the preference graph to the replicator attractors.

</details>


### [80] [When agents choose bundles autonomously: guarantees beyond discrepancy](https://arxiv.org/abs/2602.11330)
*Sushmita Gupta,Pallavi Jain,Sanjay Seetharaman,Meirav Zehavi*

Main category: cs.GT

TL;DR: 研究不可分割物品公平分配，证明分区价值有渐近限制，提出方法克服障碍，实现指数级改进，对不同估值类别有更好保证。


<details>
  <summary>Details</summary>
Motivation: 在具有加性非负归一化估值的n个代理人之间进行不可分割物品的公平分配，目标是获得高价值保证，克服现有分区的渐近限制。

Method: 创建动态分区，让代理人按顺序自主理性选择部分；针对三种受限估值类别进行研究。

Result: 实现了对差异障碍的指数级改进，保证每个代理人获得价值至少为PROP - O(log n)的部分；对三种受限估值类别有更好保证。

Conclusion: 通过提出的方法可以克服现有分区的渐近限制，为每个代理人实现更强的个体保证。

Abstract: We consider the fair division of indivisible items among $n$ agents with additive non-negative normalized valuations, with the goal of obtaining high value guarantees, that is, close to the proportional share for each agent.
  We prove that partitions where \emph{every} part yields high value for each agent are asymptotically limited by a discrepancy barrier of $Θ(\sqrt{n})$. Guided by this, our main objective is to overcome this barrier and achieve stronger individual guarantees for each agent in polynomial time.
  Towards this, we are able to exhibit an exponential improvement over the discrepancy barrier. In particular, we can create partitions on-the-go such that when agents arrive sequentially (representing a previously-agreed priority order) and pick a part autonomously and rationally (i.e., one of highest value), then each is guaranteed a part of value at least $\mathsf{PROP} - \mathcal{O}{(\log n)}$. Moreover, we show even better guarantees for three restricted valuation classes such as those defined by: a common ordering on items, a bound on the multiplicity of values, and a hypergraph with a bound on the \emph{influence} of any agent. Specifically, we study instances where: (1) the agents are ``close'' to unanimity in their relative valuation of the items -- a generalization of the ordered additive setting; (2) the valuation functions do not assign the same positive value to more than $t$ items; and (3) the valuation functions respect a hypergraph, a setting introduced by Christodoulou et al. [EC'23], where agents are vertices and items are hyperedges. While the sizes of the hyperedges and neighborhoods can be arbitrary, the influence of any agent $a$, defined as the number of its neighbors who value at least one item positively that $a$ also values positively, is bounded.

</details>


### [81] [Maximizing Index Diversity in Committee Elections](https://arxiv.org/abs/2602.11400)
*Paula Böhm,Robert Bredereck,Till Fluschnik*

Main category: cs.GT

TL;DR: 引入考虑委员会多样性的多获胜者选举模型，用多种多样性指数，分析计算复杂度并实证研究约束弱化对多样性的影响。


<details>
  <summary>Details</summary>
Motivation: 考虑委员会多样性，提出更合理的多获胜者选举模型。

Method: 引入两个考虑多样性的多获胜者选举模型，使用生态学多样性指数并引入新指数，定义指数属性，分析计算复杂度并进行实证研究。

Result: 完成了模型构建、指数定义与测试、复杂度分析和实证研究。

Conclusion: 提出的模型和方法可用于解决多获胜者选举中的多样性问题，约束弱化会影响多样性。

Abstract: We introduce two models of multiwinner elections with approval preferences and labelled candidates that take the committee's diversity into account. One model aims to find a committee with maximal diversity given a scoring function (e.g. of a scoring-based voting rule) and a lower bound for the score to be respected. The second model seeks to maximize the diversity given a minimal satisfaction for each agent to be respected. To measure the diversity of a committee, we use multiple diversity indices used in ecology and introduce one new index. We define (desirable) properties of diversity indices, test the indices considered against these properties, and characterize the new index. We analyze the computational complexity of computing a committee for both models and scoring functions of well-known voting rules, and investigate the influence of weakening the score or satisfaction constraints on the diversity empirically.

</details>


### [82] [The Distortion of Prior-Independent b-Matching Mechanisms](https://arxiv.org/abs/2602.11404)
*Ioannis Caragiannis,Vasilis Gkatzelis,Sebastian Homrighausen*

Main category: cs.GT

TL;DR: 文章评估分配物品机制在随机偏好下的失真（distortion），给出下界，提出达到最优失真与近乎最优失真差距的机制，还评估单步结构机制。


<details>
  <summary>Details</summary>
Motivation: 现有文献对序数机制的失真评估多为最坏情况分析，结果过于悲观，本文从随机生成偏好的预期性能角度评估失真。

Method: 先证明即使在特定条件下，序数机制失真无法优于e/(e - 1)；再提出能达到该最优失真的机制；优化失真差距并提出相应机制；评估单步结构机制的失真和失真差距。

Result: 证明序数机制失真下界为e/(e - 1)；提出达到最优失真e/(e - 1)和近最优失真差距1.076的机制；评估了单步结构机制的失真和失真差距。

Conclusion: 从随机偏好角度评估机制失真，得到理论下界，提出有效机制，还可进一步研究单步结构机制。

Abstract: In a setting where $m$ items need to be partitioned among $n$ agents, we evaluate the performance of mechanisms that take as input each agent's \emph{ordinal preferences}, i.e., their ranking of the items from most- to least-preferred. The standard measure for evaluating ordinal mechanisms is the \emph{distortion}, and the vast majority of the literature on distortion has focused on worst-case analysis, leading to some overly pessimistic results. We instead evaluate the distortion of mechanisms with respect to their expected performance when the agents' preferences are generated stochastically. We first show that no ordinal mechanism can achieve a distortion better than $e/(e-1)\approx 1.582$, even if each agent needs to receive exactly one item (i.e., $m=n$) and every agent's values for different items are drawn i.i.d.\ from the same known distribution. We then complement this negative result by proposing an ordinal mechanism that achieves the optimal distortion of $e/(e-1)$ even if each agent's values are drawn from an agent-specific distribution that is unknown to the mechanism. To further refine our analysis, we also optimize the \emph{distortion gap}, i.e., the extent to which an ordinal mechanism approximates the optimal distortion possible for the instance at hand, and we propose a mechanism with a near-optimal distortion gap of $1.076$. Finally, we also evaluate the distortion and distortion gap of simple mechanisms that have a one-pass structure.

</details>


### [83] [Fair Data-Exchange Mechanisms](https://arxiv.org/abs/2602.11417)
*Rashida Hakim,Christos Papadimitriou,Mihalis Yannakakis*

Main category: cs.GT

TL;DR: 研究无金钱转移下战略主体间的数据交换，提出公平交换合同，分析其博弈特性及均衡情况，证明公平交换是可行且激励相容的机制。


<details>
  <summary>Details</summary>
Motivation: 在研究联盟和医疗合作等无法或限制支付的领域，解决数据共享中搭便车问题，实现数据共享的利益。

Method: 引入公平交换合同，对策略空间进行变换，分析合同诱导的博弈。

Result: 合同诱导的博弈在策略空间变换下是超模的，存在纯纳什均衡且形成格，可在二次时间内计算；在图限制模型中经调整仍有可有效计算的纯纳什均衡和帕累托最优结果。

Conclusion: 公平交换为无支付情况下的数据交换提供了易处理且激励相容的机制。

Abstract: We study data exchange among strategic agents without monetary transfers, motivated by domains such as research consortia and healthcare collaborations where payments are infeasible or restricted. The central challenge is to reap the benefits of data-sharing while preventing free-riding that would otherwise lead agents to under invest in data collection. We introduce a simple fair-exchange contract in which, for every pair of agents, each agent receives exactly as many data points as it provides, equal to the minimum of their two collection levels. We show that the game induced by this contract is supermodular under a transformation of the strategy space. This results in a clean structure: pure Nash equilibria exist, they form a lattice, and can be computed in time quadratic in the number of agents. In addition, the maximal equilibrium is truthfully implementable under natural enforcement assumptions and is globally Pareto-optimal across all strategy profiles. In a graph-restricted variant of the model supermodularity fails, but an adaptation of the construction still yields efficiently computable pure Nash equilibria and Pareto-optimal outcomes. Overall, fair exchange provides a tractable and incentive-aligned mechanism for data exchange in the absence of payments.

</details>


### [84] [Dueling over Multiple Pieces of Dessert](https://arxiv.org/abs/2602.11486)
*Simina Brânzei,Reed Phillips*

Main category: cs.GT

TL;DR: 研究两人重复公平分配蛋糕动态，分析不同分区情况下 Alice 相对 Stackelberg 值的后悔值，还得出 Robertson - Webb 模型中近似 Stackelberg 分配的随机查询复杂性。


<details>
  <summary>Details</summary>
Motivation: 研究重复公平分配中，Alice 如何最小化其相对于 Stackelberg 值的后悔值。

Method: 分析 Alice 使用任意可测分区以及最多 k 次切割的情况，根据 Bob 学习率是否公开进行不同分析。

Result: 任意可测分区时达到强次线性后悔值不可能；最多 k 次切割时，Bob 学习率公开有多项式后悔界层级，私有则能保证一定后悔值但难获多项式速率。

Conclusion: 刻画 Robertson - Webb 模型中用固定次数切割找到近似 Stackelberg 分配的随机查询复杂性。

Abstract: We study the dynamics of repeated fair division between two players, Alice and Bob, where Alice partitions a cake into two subsets and Bob chooses his preferred one over $T$ rounds. Alice aims to minimize her regret relative to the Stackelberg value -- the maximum utility she could achieve if she knew Bob's private valuation.
  We show that if Alice uses arbitrary measurable partitions, achieving strongly sublinear regret is impossible; she suffers a regret of $Ω\Bigl(\frac{T}{\log^2 T}\Bigr)$ regret even against a myopic Bob. However, when Alice uses at most $k$ cuts, the learning landscape becomes tractable. We analyze Alice's performance based on her knowledge of Bob's strategic sophistication (his regret budget). When Bob's learning rate is public, we establish a hierarchy of polynomial regret bounds determined by $k$ and Bob's regret budget. In contrast, when this learning rate is private, Alice can universally guarantee $O\Bigl(\frac{T}{\log T}\Bigr)$ regret, but any attempt to secure a polynomial rate $O(T^β)$ (for $β< 1$) leaves her vulnerable to incurring strictly linear regret against some Bob.
  Finally, as a corollary of our online learning dynamics, we characterize the randomized query complexity of finding approximate Stackelberg allocations with a constant number of cuts in the Robertson-Webb model.

</details>


### [85] [Searching for Optimal Prices in Two-Sided Markets](https://arxiv.org/abs/2602.11691)
*Yiding Feng,Mengfan Ma,Bo Peng,Zongqi Wan*

Main category: cs.GT

TL;DR: 研究双边市场在线定价，分析不同机制下收益和贸易增益最大化的遗憾值，设计算法并给出界限，扩展到上下文场景。


<details>
  <summary>Details</summary>
Motivation: 在双边市场中，平台依据二元反馈反复定价以最大化贸易增益或利润，需研究不同机制下可实现的遗憾值。

Method: 针对收益和贸易增益最大化问题，设计使用双价格机制的算法；引入分段价格机制；将结果扩展到上下文场景。

Result: 设计出收益最大化的双价格机制算法，得到不同市场下贸易增益最大化的遗憾值界限，扩展场景中也得到相应遗憾值界限。

Conclusion: 明确了双边动态定价中可学习和不可学习状态的界限，适度增加定价表达能力可突破基本困难障碍。

Abstract: We investigate online pricing in two-sided markets where a platform repeatedly posts prices based on binary accept/reject feedback to maximize gains-from-trade (GFT) or profit. We characterize the regret achievable across three mechanism classes: Single-Price, Two-Price, and Segmented-Price.
  For profit maximization, we design an algorithm using Two-Price Mechanisms that achieves $O(n^2 \log\log T)$ regret, where $n$ is the number of traders.
  For GFT maximization, the optimal regret depends critically on both market size and mechanism expressiveness. Constant regret is achievable in bilateral trade, but this guarantee breaks down as the market grows: even in a one-seller, two-buyer market, any algorithm using Single-Price Mechanisms suffers regret at least $Ω\!\big(\frac{\log\log T}{\log\log\log\log T}\big)$, and we provide a nearly matching $O(\log\log T)$ upper bound for general one-to-many markets. In full many-to-many markets, we prove that Two-Price Mechanisms inevitably incur linear regret $Ω(T)$ due to a \emph{mismatch phenomenon}, wherein inefficient pairings prevent near-optimal trade. To overcome this barrier, we introduce \emph{Segmented-Price Mechanisms}, which partition traders into groups and assign distinct prices per group. Using this richer mechanism, we design an algorithm achieving $O(n^2 \log\log T + n^3)$ regret for GFT maximization.
  Finally, we extend our results to the contextual setting, where traders' costs and values depend linearly on observed $d$-dimensional features that vary across rounds, obtaining regret bounds of $O(n^2 d \log\log T + n^2 d \log d)$ for profit and $O(n^2 d^2 \log T)$ for GFT. Our work delineates sharp boundaries between learnable and unlearnable regimes in two-sided dynamic pricing and demonstrates how modest increases in pricing expressiveness can circumvent fundamental hardness barriers.

</details>


### [86] [Achieving EF1 and Epistemic EFX Guarantees Simultaneously](https://arxiv.org/abs/2602.11732)
*Hannaneh Akrami,Ryoga Mahara,Kurt Mehlhorn,Nidhi Rathi*

Main category: cs.GT

TL;DR: 研究不可分割物品公平分配问题，证明存在对可加估值同时满足EF1和EEFX的分配，引入强EEFX份额概念解决开放问题。


<details>
  <summary>Details</summary>
Motivation: 解决公平分配中EFX存在性这一重要开放问题，探讨EF1和EEFX能否同时满足。

Method: 引入新的基于份额的公平概念——强EEFX份额，证明其与EF1兼容。

Result: 证明存在对可加估值同时满足EF1（更强的EFL）和EEFX的分配。

Conclusion: 解决了Akrami和Rathi (2025)提出的主要开放问题，向解决EFX问题迈进了一步。

Abstract: We study the fundamental problem of fairly dividing a set of indivisible goods among agents with additive valuations. Here, envy-freeness up to any good (EFX) is a central fairness notion and resolving its existence is regarded as one of the most important open problems in this area of research. Two prominent relaxations of EFX are envy-freeness up to one good (EF1) and epistemic EFX (EEFX). While allocations satisfying each of these notions individually are known to exist even for general monotone valuations, whether both can be satisfied simultaneously remains open for all instances in which the EFX problem is itself unresolved.
  In this work, we show that there always exists an allocation that is both EF1 (in fact, the stronger notion EFL) and EEFX for additive valuations, thereby resolving the primary open question raised by Akrami and Rathi (2025) and bringing us one step closer to resolving the elusive EFX problem. We introduce a new share-based fairness notion, termed strong EEFX share, which may be of independent interest and which implies EEFX feasibility of bundles. We show that this notion is compatible with EF1, leading to the desired existence result.

</details>


### [87] [Global Convergence to Nash Equilibrium in Nonconvex General-Sum Games under the $n$-Sided PL Condition](https://arxiv.org/abs/2602.11835)
*Yutong Chao,Jalal Etesami*

Main category: cs.GT

TL;DR: 研究一般和博弈中寻找纳什均衡问题，引入n边PL条件分析梯度下降算法收敛性，提出适应变体并实验评估。


<details>
  <summary>Details</summary>
Motivation: 解决一般和博弈中寻找纳什均衡问题，分析现有算法收敛性及处理不收敛情况。

Method: 引入n边PL条件，分析梯度下降及其变体算法，提出适应变体算法。

Result: 可分析各种梯度下降算法收敛性，提出的适应变体算法朝NE收敛。

Conclusion: 所提算法在求解纳什均衡问题上有一定效果，通过实验性能评估。

Abstract: We consider the problem of finding a Nash equilibrium (NE) in a general-sum game, where player $i$'s objective is $f_i(x)=f_i(x_1,...,x_n)$, with $x_j\in\mathbb{R}^{d_j}$ denoting the strategy variables of player $j$. Our focus is on investigating first-order gradient-based algorithms and their variations, such as the block coordinate descent (BCD) algorithm, for tackling this problem. We introduce a set of conditions, called the $n$-sided PL condition, which extends the well-established gradient dominance condition a.k.a Polyak-Łojasiewicz (PL) condition and the concept of multi-convexity. This condition, satisfied by various classes of non-convex functions, allows us to analyze the convergence of various gradient descent (GD) algorithms. Moreover, our study delves into scenarios where the standard gradient descent methods fail to converge to NE. In such cases, we propose adapted variants of GD that converge towards NE and analyze their convergence rates. Finally, we evaluate the performance of the proposed algorithms through several experiments.

</details>


### [88] [Scale-Invariant Fast Convergence in Games](https://arxiv.org/abs/2602.11857)
*Taira Tsuchiya,Haipeng Luo,Shinji Ito*

Main category: cs.GT

TL;DR: 本文提出了无尺度且尺度不变的学习动态，在不同类型博弈中实现快速收敛，给出收敛率，介绍了基于乐观正则化领导者和新分析方法及加倍裁剪技术。


<details>
  <summary>Details</summary>
Motivation: 多数博弈学习的快速收敛保证需效用尺度先验知识，为解决此问题开展研究。

Method: 基于乐观正则化领导者，采用自适应学习率结合对手梯度向量平方路径长度，运用新的停止时间分析，在一般和博弈中使用加倍裁剪技术。

Result: 在两人零和博弈中得到外部遗憾有界的无尺度及尺度不变动态，收敛到纳什均衡；在多人一般和博弈中得到交换遗憾有界的动态，收敛到相关均衡，并给出收敛率。

Conclusion: 所提出的学习动态能在无效用先验信息且尺度不变的情况下实现快速收敛。

Abstract: Scale-invariance in games has recently emerged as a widely valued desirable property. Yet, almost all fast convergence guarantees in learning in games require prior knowledge of the utility scale. To address this, we develop learning dynamics that achieve fast convergence while being both scale-free, requiring no prior information about utilities, and scale-invariant, remaining unchanged under positive rescaling of utilities. For two-player zero-sum games, we obtain scale-free and scale-invariant dynamics with external regret bounded by $\tilde{O}(A_{\mathrm{diff}})$, where $A_{\mathrm{diff}}$ is the payoff range, which implies an $\tilde{O}(A_{\mathrm{diff}} / T)$ convergence rate to Nash equilibrium after $T$ rounds. For multiplayer general-sum games with $n$ players and $m$ actions, we obtain scale-free and scale-invariant dynamics with swap regret bounded by $O(U_{\mathrm{max}} \log T)$, where $U_{\mathrm{max}}$ is the range of the utilities, ignoring the dependence on the number of players and actions. This yields an $O(U_{\mathrm{max}} \log T / T)$ convergence rate to correlated equilibrium. Our learning dynamics are based on optimistic follow-the-regularized-leader with an adaptive learning rate that incorporates the squared path length of the opponents' gradient vectors, together with a new stopping-time analysis that exploits negative terms in regret bounds without scale-dependent tuning. For general-sum games, scale-free learning is enabled also by a technique called doubling clipping, which clips observed gradients based on past observations.

</details>


### [89] [Incentive Effects of a Cut-Off Score: Optimal Contest Design with Transparent Pre-Selection](https://arxiv.org/abs/2602.11914)
*Hanbing Liu,Ningyuan Li,Weian Li,Qi Qi,Changyuan Yu*

Main category: cs.GT

TL;DR: 研究带入围筛选和分数线公布的排名赛，分析入围选手均衡行为，得出最优竞赛格式、入围规模及有无入围筛选下最高个人表现对比结果。


<details>
  <summary>Details</summary>
Motivation: 研究在竞争环境中带入围筛选和分数线公布的排名赛，以明确不同目标下的最优竞赛设置。

Method: 对给定奖品结构和入围规模，全面刻画入围选手的均衡行为，考察最高个人表现和总表现两个目标函数。

Result: 两个目标下最优竞赛都是赢家通吃格式；最高个人表现目标下最优入围规模为 2 人，总表现目标下入围规模不影响结果；有入围筛选的最高个人表现是无入围筛选的 4/3 倍。

Conclusion: 确定了不同目标下排名赛的最优结构和入围规模的影响，强调入围筛选对提高最高个人表现有积极作用。

Abstract: Shortlisting is a common and effective method for pre-selecting participants in competitive settings. To ensure fairness, a cut-off score is typically announced, allowing only contestants who exceed it to enter the contest, while others are eliminated. In this paper, we study rank-order contests with shortlisting and cut-off score disclosure. We fully characterize the equilibrium behavior of shortlisted contestants for any given prize structure and shortlist size. We examine two objective functions: the highest individual performance and total performance. For both objectives, the optimal contest is in a winner-take-all format. For the highest individual performance, the optimal shortlist size is exactly two contestants, but, in contrast, for total performance, the shortlist size does not affect the outcome, i.e., any size yields the same total performance. Furthermore, we compare the highest individual performance achieved with and without shortlisting, and show that the former is 4/3 times greater than the latter.

</details>


### [90] [Strengthening Bulow-Klemperer-Style Results for Multi-Unit Auctions](https://arxiv.org/abs/2602.11959)
*Moshe Babaioff,Yiding Feng,Zihan Luo*

Main category: cs.GT

TL;DR: 本文研究VCG拍卖竞争复杂度，在更强分布假设下其竞争复杂度大幅下降，且提出供应限制变体机制能减少额外买家数量以实现近最优收益。


<details>
  <summary>Details</summary>
Motivation: 改进经典结果中VCG拍卖竞争复杂度高的问题，探索更好的竞争复杂度结果。

Method: 在更强分布假设（如MHR分布）下分析，提出供应限制变体的VCG拍卖机制，统一归结到截断广义帕累托分布进行分析。

Result: 在MHR分布平衡市场，只需添加约0.4447n个额外买家；供应限制变体机制严格优于标准VCG拍卖。

Conclusion: 更强分布假设和对VCG拍卖的简单改进可大幅减少实现近最优收益所需额外买家数量。

Abstract: The classic result of Bulow and Klemperer (1996) shows that in multi-unit auctions with $m$ units and $n\geq m$ buyers whose values are sampled i.i.d. from a regular distribution, the revenue of the VCG auction with $m$ additional buyers is at least as large as the optimal revenue. Unfortunately, for regular distributions, adding $m$ additional buyers is sometimes indeed necessary, so the "competition complexity" of the VCG auction is $m$. We seek proving better competition complexity results in two dimensions.
  First, under stronger distributional assumptions, the competition complexity of VCG auction drops dramatically. In balanced markets (where $m=n$) with MHR distributions, it is sufficient to only add $(e^{1/e} - 1 + o(1))n \approx 0.4447n$ additional buyers to match the optimal revenue -- less than half the number that is necessary under regularity -- and this bound is asymptotically tight. We provide both exact finite-market results for small value of $n$, and closed-form asymptotic formulas for general market with any $m\leq n$, and any target fraction of the optimal revenue.
  Second, we analyze a supply-limiting variant of VCG auction that caps the number of units sold in a prior-independent way. Whenever the goal is to achieve almost the optimal revenue, this mechanism strictly improves upon standard VCG auction, requiring significantly fewer additional buyers.
  Together, our results show that both stronger distributional assumptions, as well as a simple prior-independent refinement to the VCG auction, can each substantially reduce the number of additional buyers that is sufficient to achieve (near-)optimal revenue. Our analysis hinges on a unified worst-case reduction to truncated generalized Pareto distributions, enabling both numerical computation and analytical tractability.

</details>


### [91] [Pareto-Efficient Multi-Buyer Mechanisms: Characterization, Fairness and Welfare](https://arxiv.org/abs/2602.11967)
*Moshe Babaioff,Sijin Chen,Zhaohua Chen,Yiding Feng*

Main category: cs.GT

TL;DR: 研究贝叶斯单物品拍卖中买卖双方事前效用的帕累托前沿，分析两种合作议价理论的帕累托最优解，揭示公平 - 效率权衡对分布结构的敏感性。


<details>
  <summary>Details</summary>
Motivation: 研究所有真实机制下，买家价值独立同分布时买卖双方事前效用的帕累托前沿。

Method: 先对帕累托前沿进行完整结构刻画，再将机制选择问题视为双边议价博弈，分析两种合作议价理论的解。

Result: 正则且反MHR分布下，大市场中两种解接近最优福利；最坏情况MHR分布下，KS解保证一半最优福利，Nash解表现不佳。

Conclusion: 公平 - 效率权衡对分布结构敏感，KS解是不对称双边市场更稳健的公平概念。

Abstract: A truthful mechanism for a Bayesian single-item auction results with some ex-ante revenue for the seller, and some ex-ante total surplus for the buyers. We study the Pareto frontier of the set of seller-buyers ex-ante utilities, generated by all truthful mechanisms when buyers values are sampled independently and identically (i.i.d.). We first provide a complete structural characterization of the Pareto frontier under natural distributional assumptions. For example, when valuations are drawn i.i.d. from a distribution that is both regular and anti-MHR, every Pareto-optimal mechanism is a second-price auction with a reserve no larger than the monopoly reserve.
  Building on this, we interpret the problem of picking a mechanism as a two-sided bargaining game, and analyze two canonical Pareto-optimal solutions from cooperative bargaining theory: the Kalai-Smorodinsky (KS) solution, and the Nash solution. We prove that when values are drawn i.i.d. from a distribution that is both regular and anti-MHR, in large markets both solutions yield near-optimal welfare. In contrast, under worst-case MHR distributions, their performance diverges sharply: the KS solution guarantees one-half of the optimal welfare, while the Nash solution might only achieve an arbitrarily small fraction of it. These results highlight the sensitivity of fairness-efficiency tradeoffs to distributional structure, and affirm the KS solution as the more robust notion of fairness for asymmetric two-sided markets.

</details>


### [92] [Choose Your Agent: Tradeoffs in Adopting AI Advisors, Coaches, and Delegates in Multi-Party Negotiation](https://arxiv.org/abs/2602.12089)
*Kehang Zhu,Lithium Thain,Vivian Tsai,James Wexler,Crystal Qian*

Main category: cs.GT

TL;DR: 通过在线实验研究AI辅助模式在多人议价游戏中的效果，发现Delegate模式收益最高但用户偏好Advisor模式，揭示代理能力与群体福利间差距，强调设计与采用兼容的交互规则重要性。


<details>
  <summary>Details</summary>
Motivation: 随着AI在社交场景使用更普遍，理解代理 - 用户交互对设计改善个人和群体结果的系统至关重要。

Method: 进行在线行为实验（N = 243），参与者进行三轮三人一组的多轮议价游戏，随机分配Advisor、Coach、Delegate三种AI辅助模式，参与者在每轮决定手动行动或使用AI。

Result: 参与者虽偏好Advisor模式，但Delegate模式下个人平均收益最高，出现偏好 - 绩效不一致；Delegate产生正外部性，非采用用户也受益；Delegate像做市商，注入理性、帕累托改进提案重构交易环境。

Conclusion: 代理能力与实现的群体福利存在差距，自主代理虽有超人类战略表现，但福利增益受界面、用户认知和采用障碍限制；辅助模式应设计为内生产生参与的机制，采用兼容的交互规则是自动化辅助改善人类福利的前提。

Abstract: As AI usage becomes more prevalent in social contexts, understanding agent-user interaction is critical to designing systems that improve both individual and group outcomes. We present an online behavioral experiment (N = 243) in which participants play three multi-turn bargaining games in groups of three. Each game, presented in randomized order, grants \textit{access to} a single LLM assistance modality: proactive recommendations from an \textit{Advisor}, reactive feedback from a \textit{Coach}, or autonomous execution by a \textit{Delegate}; all modalities are powered by an underlying LLM that achieves superhuman performance in an all-agent environment. On each turn, participants privately decide whether to act manually or use the AI modality available in that game. Despite preferring the \textit{Advisor} modality, participants achieve the highest mean individual gains with the \textit{Delegate}, demonstrating a preference-performance misalignment. Moreover, delegation generates positive externalities; even non-adopting users in \textit{access-to-delegate} treatment groups benefit by receiving higher-quality offers. Mechanism analysis reveals that the \textit{Delegate} agent acts as a market maker, injecting rational, Pareto-improving proposals that restructure the trading environment. Our research reveals a gap between agent capabilities and realized group welfare. While autonomous agents can exhibit super-human strategic performance, their impact on realized welfare gains can be constrained by interfaces, user perceptions, and adoption barriers. Assistance modalities should be designed as mechanisms with endogenous participation; adoption-compatible interaction rules are a prerequisite to improving human welfare with automated assistance.

</details>


### [93] [Anonymous Contracts](https://arxiv.org/abs/2602.12118)
*Johannes Brustle,Paul Duetting,Stefano Leonardi,Tomasz Ponitka,Matteo Russo*

Main category: cs.GT

TL;DR: 研究多智能体契约问题，引入匿名契约，分析其均衡、效率等特性，探讨有限责任对绩效的影响。


<details>
  <summary>Details</summary>
Motivation: 传统差别契约可能被认为不公平，引入匿名契约确保对智能体一视同仁。

Method: 先证明匿名契约存在纯纳什均衡，确定统一匿名契约子类；分析效率时对比有限责任和无有限责任情况。

Result: 统一匿名契约保证唯一均衡；有限责任下匿名契约近似社会福利有对数因子限制，统一契约可达到此理论极限；无有限责任时匿名契约绩效提升，概率不同时可提取全部福利。

Conclusion: 有限责任影响匿名契约绩效，概率分布在不同责任情况下对绩效的影响有结构反转。

Abstract: We study a multi-agent contracting problem where agents exert costly effort to achieve individually observable binary outcomes. While the principal can theoretically extract the full social welfare using a discriminatory contract that tailors payments to individual costs, such contracts may be perceived as unfair. In this work, we introduce and analyze anonymous contracts, where payments depend solely on the total number of successes, ensuring identical treatment of agents.
  We first establish that every anonymous contract admits a pure Nash equilibrium. However, because general anonymous contracts can suffer from multiple equilibria with unbounded gaps in principal utility, we identify uniform anonymous contracts as a desirable subclass. We prove that uniform anonymous contracts guarantee a unique equilibrium, thereby providing robust performance guarantees.
  In terms of efficiency, we prove that under limited liability, anonymous contracts cannot generally approximate the social welfare better than a factor logarithmic in the spread of agent success probabilities. We show that uniform contracts are sufficient to match this theoretical limit. Finally, we demonstrate that removing limited liability significantly boosts performance: anonymous contracts generally achieve an $O(\log n)$ approximation to the social welfare and, surprisingly, can extract the full welfare whenever agents' success probabilities are distinct. This reveals a structural reversal: widely spread probabilities are the hardest case under limited liability, whereas identical probabilities become the hardest case when limited liability is removed.

</details>


### [94] [Convex Markov Games and Beyond: New Proof of Existence, Characterization and Learning Algorithms for Nash Equilibria](https://arxiv.org/abs/2602.12181)
*Anas Barakat,Ioannis Panageas,Antonios Varvitsiotis*

Main category: cs.GT

TL;DR: 本文研究了广义效用马尔可夫博弈（GUMGs），证明纳什均衡与投影伪梯度动力学不动点重合，给出存在性证明、策略梯度定理，设计算法并给出复杂度保证，拓展了先前零和凸马尔可夫博弈的研究。


<details>
  <summary>Details</summary>
Motivation: 凸马尔可夫博弈（cMGs）理论基础，特别是纳什均衡结构和学习算法保证尚不明确，需要对其扩展GUMGs进行研究。

Method: 证明GUMGs中纳什均衡与投影伪梯度动力学不动点重合，利用布劳威尔不动点定理证明NE存在性，建立策略梯度定理，设计无模型策略梯度算法，给出迭代和样本复杂度保证。

Result: 证明了纳什均衡特性、NE和马尔可夫完美均衡的存在性，建立策略梯度定理和算法，给出复杂度保证。

Conclusion: 结果扩展了先前零和cMGs的工作，首次对共同利益cMGs进行理论分析。

Abstract: Convex Markov Games (cMGs) were recently introduced as a broad class of multi-agent learning problems that generalize Markov games to settings where strategic agents optimize general utilities beyond additive rewards. While cMGs expand the modeling frontier, their theoretical foundations, particularly the structure of Nash equilibria (NE) and guarantees for learning algorithms, are not yet well understood. In this work, we address these gaps for an extension of cMGs, which we term General Utility Markov Games (GUMGs), capturing new applications requiring coupling between agents' occupancy measures. We prove that in GUMGs, Nash equilibria coincide with the fixed points of projected pseudo-gradient dynamics (i.e., first-order stationary points), enabled by a novel agent-wise gradient domination property. This insight also yields a simple proof of NE existence using Brouwer's fixed-point theorem. We further show the existence of Markov perfect equilibria. Building on this characterization, we establish a policy gradient theorem for GUMGs and design a model-free policy gradient algorithm. For potential GUMGs, we establish iteration complexity guarantees for computing approximate-NE under exact gradients and provide sample complexity bounds in both the generative model and on-policy settings. Our results extend beyond prior work restricted to zero-sum cMGs, providing the first theoretical analysis of common-interest cMGs.

</details>


### [95] [Adjusted Winner: from Splitting to Selling](https://arxiv.org/abs/2602.12231)
*Robert Bredereck,Bin Sun,Eyal Briman,Nimrod Talmon*

Main category: cs.GT

TL;DR: 提出AW方法扩展允许按预算约束出售资源并重新分配，进行公理分析，定义组合问题，确定复杂度并设计FPTAS，辅以模拟。


<details>
  <summary>Details</summary>
Motivation: 原AW方法依赖分割资源会带来实际复杂性，需改进。

Method: 提出允许按预算约束出售资源并重新分配的AW扩展框架，进行公理分析，定义组合问题，设计FPTAS，用计算机模拟。

Result: 确定组合问题的计算复杂度，得到理论结果并通过模拟验证。

Conclusion: 扩展的AW方法有潜力提供更公平的不可分割资源分配方案，减小原方法局限性。

Abstract: The Adjusted Winner (AW) method is a fundamental procedure for the fair division of indivisible resources between two agents. However, its reliance on splitting resources can lead to practical complications. To address this limitation, we propose an extension of AW that allows the sale of selected resources under a budget constraint, with the proceeds subsequently redistributed, thereby aiming for allocations that remain as equitable as possible. Alongside developing this extended framework, we provide an axiomatic analysis that examines how equitability and envy-freeness are modified in our setting. We then formally define the resulting combinatorial problems, establish their computational complexity, and design a fully polynomial-time approximation scheme (FPTAS) to mitigate their inherent intractability. Finally, we complement our theoretical results with computer-based simulations.

</details>


### [96] [Is Online Linear Optimization Sufficient for Strategic Robustness?](https://arxiv.org/abs/2602.12253)
*Yang Cai,Haipeng Luo,Chen-Yu Wei,Weiqiang Zheng*

Main category: cs.GT

TL;DR: 本文探索简单在线线性优化算法用于具有战略鲁棒性和低后悔值的投标算法的可行性，构造黑盒约减方法将OLO算法转化为战略鲁棒的无后悔投标算法并给出不同情况下的后悔值结果。


<details>
  <summary>Details</summary>
Motivation: 已有达到最优后悔值的投标算法在应对卖家操纵的战略鲁棒性研究不足，基于无交换后悔算法的投标算法在统计和计算效率上不佳，因此探索简单在线线性优化 (OLO) 算法是否能实现理想属性。

Method: 构建简单的黑盒约减方法，将任何OLO算法转换为具有战略鲁棒性的无后悔投标算法。

Result: 在已知和未知价值分布设置下均能实现，已知分布时达O(√(T log K))后悔值和战略鲁棒性，未知分布时具高概率O(√(T (log K + log(T/δ))))后悔值和战略鲁棒性，且去除假设。

Conclusion: 次线性线性化后悔足以实现战略鲁棒性，简单OLO算法可用于构造具有这两个理想属性的投标算法。

Abstract: We consider bidding in repeated Bayesian first-price auctions. Bidding algorithms that achieve optimal regret have been extensively studied, but their strategic robustness to the seller's manipulation remains relatively underexplored. Bidding algorithms based on no-swap-regret algorithms achieve both desirable properties, but are suboptimal in terms of statistical and computational efficiency. In contrast, online gradient ascent is the only algorithm that achieves $O(\sqrt{TK})$ regret and strategic robustness [KSS24], where $T$ denotes the number of auctions and $K$ the number of bids.
  In this paper, we explore whether simple online linear optimization (OLO) algorithms suffice for bidding algorithms with both desirable properties. Our main result shows that sublinear linearized regret is sufficient for strategic robustness. Specifically, we construct simple black-box reductions that convert any OLO algorithm into a strategically robust no-regret bidding algorithm, in both known and unknown value distribution settings. For the known value distribution case, our reduction yields a bidding algorithm that achieves $O(\sqrt{T \log K})$ regret and strategic robustness (with exponential improvement on the $K$-dependence compared to [KSS24]). For the unknown value distribution case, our reduction gives a bidding algorithm with high-probability $O(\sqrt{T (\log K+\log(T/δ)})$ regret and strategic robustness, while removing the bounded density assumption made in [KSS24].

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [97] [MTFM: A Scalable and Alignment-free Foundation Model for Industrial Recommendation in Meituan](https://arxiv.org/abs/2602.11235)
*Xin Song,Zhilin Guan,Ruidong Han,Binghao Tang,Tianwen Chen,Bing Li,Zihao Li,Han Zhang,Fei Jiang,Chaolin Xie,Chi Ma,Chunyang Jiang,Chunzhen Jing,Dengxuan Li,Fengyi Li,Lei Yu,Mengyao Sun,Pu Wang,Qing Wang,Rui Fan,Shangyu Chen,Shifeng Du,Siyuan Bai,Wei Lin,Wentao Zhu,Zhou Han,Zhuo Chen,Zikang Xu*

Main category: cs.IR

TL;DR: 提出基于Transformer的推荐框架MTFM，通过无对齐方式捕获多场景知识，进行系统优化，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有跨域和多场景推荐方法需大量资源和严格输入对齐，扩展性受限。

Method: 将跨域数据转换为异构令牌，采用多场景用户级样本聚合，集成Grouped - Query Attention和定制的Hybrid Target Attention，进行系统级优化。

Result: 离线和在线实验表明，扩大模型容量和多场景训练数据可显著提升性能。

Conclusion: MTFM能有效解决现有推荐方法的问题，提升推荐性能。

Abstract: Industrial recommendation systems typically involve multiple scenarios, yet existing cross-domain (CDR) and multi-scenario (MSR) methods often require prohibitive resources and strict input alignment, limiting their extensibility. We propose MTFM (Meituan Foundation Model for Recommendation), a transformer-based framework that addresses these challenges. Instead of pre-aligning inputs, MTFM transforms cross-domain data into heterogeneous tokens, capturing multi-scenario knowledge in an alignment-free manner. To enhance efficiency, we first introduce a multi-scenario user-level sample aggregation that significantly enhances training throughput by reducing the total number of instances. We further integrate Grouped-Query Attention and a customized Hybrid Target Attention to minimize memory usage and computational complexity. Furthermore, we implement various system-level optimizations, such as kernel fusion and the elimination of CPU-GPU blocking, to further enhance both training and inference throughput. Offline and online experiments validate the effectiveness of MTFM, demonstrating that significant performance gains are achieved by scaling both model capacity and multi-scenario training data.

</details>


### [98] [From Noise to Order: Learning to Rank via Denoising Diffusion](https://arxiv.org/abs/2602.11453)
*Sajad Ebrahimi,Bhaskar Mitra,Negar Arabzadeh,Ye Yuan,Haolun Wu,Fattane Zarrinkalam,Ebrahim Bagheri*

Main category: cs.IR

TL;DR: 提出基于去噪扩散的深度生成式方法DiffusionRank用于信息检索排序，效果优于传统判别式方法，为未来研究提供方向。


<details>
  <summary>Details</summary>
Motivation: 传统信息检索排序的判别式机器学习方法有局限，过参数化的排序模型在判别式设置下可能有不同拟合方式，希望通过生成式方法找到能解释完整数据分布的更鲁棒排序模型。

Method: 将现有的基于去噪扩散的表格数据集生成模型TabDiff扩展，创建经典判别式逐点和逐点学习排序目标的生成式等价形式，提出DiffusionRank。

Result: DiffusionRank模型相比判别式模型有显著改进。

Conclusion: 为利用深度生成式建模方法（如扩散）进行信息检索排序的未来研究提供了丰富空间。

Abstract: In information retrieval (IR), learning-to-rank (LTR) methods have traditionally limited themselves to discriminative machine learning approaches that model the probability of the document being relevant to the query given some feature representation of the query-document pair. In this work, we propose an alternative denoising diffusion-based deep generative approach to LTR that instead models the full joint distribution over feature vectors and relevance labels. While in the discriminative setting, an over-parameterized ranking model may find different ways to fit the training data, we hypothesize that candidate solutions that can explain the full data distribution under the generative setting produce more robust ranking models. With this motivation, we propose DiffusionRank that extends TabDiff, an existing denoising diffusion-based generative model for tabular datasets, to create generative equivalents of classical discriminative pointwise and pairwise LTR objectives. Our empirical results demonstrate significant improvements from DiffusionRank models over their discriminative counterparts. Our work points to a rich space for future research exploration on how we can leverage ongoing advancements in deep generative modeling approaches, such as diffusion, for learning-to-rank in IR.

</details>


### [99] [KuaiSearch: A Large-Scale E-Commerce Search Dataset for Recall, Ranking, and Relevance](https://arxiv.org/abs/2602.11518)
*Yupeng Li,Ben Chen,Mingyue Cheng,Zhiding Liu,Xuxin Zhang,Chenyi Lei,Wenwu Ou*

Main category: cs.IR

TL;DR: 本文指出电商搜索面临挑战，现有数据集有局限，构建并发布最大电商搜索数据集KuaiSearch，经实验证明其对电商搜索研究有价值。


<details>
  <summary>Details</summary>
Motivation: 解决电商搜索面临高度模糊查询、嘈杂产品文本等挑战，以及现有电商搜索数据集的局限性问题。

Method: 基于快手平台真实用户搜索交互构建并发布KuaiSearch数据集，对其从多视角分析并开展基准实验。

Result: 实验结果表明KuaiSearch为现实电商搜索研究提供了有价值基础。

Conclusion: KuaiSearch数据集能推动基于大语言模型的电商搜索研究。

Abstract: E-commerce search serves as a central interface, connecting user demands with massive product inventories and plays a vital role in our daily lives. However, in real-world applications, it faces challenges, including highly ambiguous queries, noisy product texts with weak semantic order, and diverse user preferences, all of which make it difficult to accurately capture user intent and fine-grained product semantics. In recent years, significant advances in large language models (LLMs) for semantic representation and contextual reasoning have created new opportunities to address these challenges. Nevertheless, existing e-commerce search datasets still suffer from notable limitations: queries are often heuristically constructed, cold-start users and long-tail products are filtered out, query and product texts are anonymized, and most datasets cover only a single stage of the search pipeline. Collectively, these issues constrain research on LLM-based e-commerce search. To address these challenges, we construct and release KuaiSearch. To the best of our knowledge, it is the largest e-commerce search dataset currently available. KuaiSearch is built upon real user search interactions from the Kuaishou platform, preserving authentic user queries and natural-language product texts, covering cold-start users and long-tail products, and systematically spanning three key stages of the search pipeline: recall, ranking, and relevance judgment. We conduct a comprehensive analysis of KuaiSearch from multiple perspectives, including products, users, and queries, and establish benchmark experiments across several representative search tasks. Experimental results demonstrate that KuaiSearch provides a valuable foundation for research on real-world e-commerce search.

</details>


### [100] [LASER: An Efficient Target-Aware Segmented Attention Framework for End-to-End Long Sequence Modeling](https://arxiv.org/abs/2602.11562)
*Tianhe Lin,Ziwei Xiong,Baoyuan Ou,Yingjie Qin,Lai Xu,Xiaocheng Zhong,Yao Hu,Zhiyong Wang,Tao Zhou,Yubin Xu,Di Wu*

Main category: cs.IR

TL;DR: 提出LASER框架打破长序列建模的延迟瓶颈，结合系统和算法优化，离线评估和在线测试效果好。


<details>
  <summary>Details</summary>
Motivation: 现代推荐系统长序列建模在实时工业环境面临高I/O延迟和计算复杂度瓶颈。

Method: 提出LASER框架，包含SeqVault服务基础设施降低检索延迟和CPU使用，提出STA机制和GSTA模块降低计算开销。

Result: 离线评估优于基线，在线A/B测试使ADVV提升2.36%，收入提升2.08%。

Conclusion: LASER框架有效打破延迟瓶颈，有良好可扩展性和商业影响。

Abstract: Modeling ultra-long user behavior sequences is pivotal for capturing evolving and lifelong interests in modern recommendation systems. However, deploying such models in real-time industrial environments faces a strict "Latency Wall", constrained by two distinct bottlenecks: the high I/O latency of retrieving massive user histories and the quadratic computational complexity of standard attention mechanisms. To break these bottlenecks, we present LASER, a full-stack optimization framework developed and deployed at Xiaohongshu (RedNote). Our approach tackles the challenges through two complementary innovations: (1) System efficiency: We introduce SeqVault, a unified schema-aware serving infrastructure for long user histories. By implementing a hybrid DRAM-SSD indexing strategy, SeqVault reduces retrieval latency by 50% and CPU usage by 75%, ensuring millisecond-level access to full real-time and life-cycle user histories. (2) Algorithmic efficiency: We propose a Segmented Target Attention (STA) mechanism to address the computational overhead. Motivated by the inherent sparsity of user interests, STA employs a sigmoid-based gating strategy that acts as a silence mechanism to filter out noisy items. Subsequently, a lightweight Global Stacked Target Attention (GSTA) module refines these compressed segments to capture cross-segment dependencies without incurring high computational costs. This design performs effective sequence compression, reducing the complexity of long-sequence modeling while preserving critical signals. Extensive offline evaluations demonstrate that LASER consistently outperforms state-of-the-art baselines. In large-scale online A/B testing serving over 100 million daily active users, LASER achieved a 2.36% lift in ADVV and a 2.08% lift in revenue, demonstrating its scalability and significant commercial impact.

</details>


### [101] [Analytical Search](https://arxiv.org/abs/2602.11581)
*Yiteng Tu,Shuo Miao,Weihang Su,Yiqun Liu,Qingyao Ai*

Main category: cs.IR

TL;DR: 现有信息检索范式难以满足分析性信息需求，本文提出分析性搜索范式，介绍框架并探讨研究方向，呼吁开发支持分析需求的下一代搜索引擎。


<details>
  <summary>Details</summary>
Motivation: 现有信息检索范式难以满足各领域分析信息需求，如趋势分析和因果影响评估等任务的端到端要求。

Method: 提出分析性搜索范式，将搜索重构为证据驱动、面向过程的分析工作流，构建包含查询理解、召回式检索、推理感知融合和自适应验证的统一系统框架。

Result: 提出分析性搜索范式及统一系统框架，探讨潜在研究方向。

Conclusion: 强调分析性搜索的概念和实践重要性，呼吁开发支持分析信息需求的下一代搜索引擎。

Abstract: Analytical information needs, such as trend analysis and causal impact assessment, are prevalent across various domains including law, finance, science, and much more. However, existing information retrieval paradigms, whether based on relevance-oriented document ranking or retrieval-augmented generation (RAG) with large language models (LLMs), often struggle to meet the end-to-end requirements of such tasks at the corpus scale. They either emphasize information finding rather than end-to-end problem solving, or simply treat everything as naive question answering, offering limited control over reasoning, evidence usage, and verifiability. As a result, they struggle to support analytical queries that have diverse utility concepts and high accountability requirements.
  In this paper, we propose analytical search as a distinct and emerging search paradigm designed to fulfill these analytical information needs. Analytical search reframes search as an evidence-governed, process-oriented analytical workflow that explicitly models analytical intent, retrieves evidence for fusion, and produces verifiable conclusions through structured, multi-step inference. We position analytical search in contrast to existing paradigms, and present a unified system framework that integrates query understanding, recall-oriented retrieval, reasoning-aware fusion, and adaptive verification. We also discuss potential research directions for the construction of analytical search engines. In this way, we highlight the conceptual significance and practical importance of analytical search and call on efforts toward the next generation of search engines that support analytical information needs.

</details>


### [102] [Recurrent Preference Memory for Efficient Long-Sequence Generative Recommendation](https://arxiv.org/abs/2602.11605)
*Yixiao Chen,Yuan Wang,Yue Liu,Qiyao Wang,Ke Cheng,Xin Xu,Juntong Yan,Shuojin Yang,Menghao Guo,Jun Zhang,Huan Yu,Jie Jiang*

Main category: cs.IR

TL;DR: 提出框架Rec2PM，压缩用户交互历史，采用新策略实现并行训练，提升存储效率，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 解决生成推荐模型在终身序列扩展时计算成本高和噪声累积的问题。

Method: 引入Rec2PM框架，采用自引用教师强制策略生成参考记忆，以标记嵌入表示记忆。

Result: 在大规模基准测试中，Rec2PM显著降低推理延迟和内存占用，且准确率更高。

Conclusion: 偏好记忆可作为去噪信息瓶颈，有效过滤交互噪声，捕捉长期兴趣。

Abstract: Generative recommendation (GenRec) models typically model user behavior via full attention, but scaling to lifelong sequences is hindered by prohibitive computational costs and noise accumulation from stochastic interactions. To address these challenges, we introduce Rec2PM, a framework that compresses long user interaction histories into compact Preference Memory tokens. Unlike traditional recurrent methods that suffer from serial training, Rec2PM employs a novel self-referential teacher-forcing strategy: it leverages a global view of the history to generate reference memories, which serve as supervision targets for parallelized recurrent updates. This allows for fully parallel training while maintaining the capability for iterative updates during inference. Additionally, by representing memory as token embeddings rather than extensive KV caches, Rec2PM achieves extreme storage efficiency. Experiments on large-scale benchmarks show that Rec2PM significantly reduces inference latency and memory footprint while achieving superior accuracy compared to full-sequence models. Analysis reveals that the Preference Memory functions as a denoising Information Bottleneck, effectively filtering interaction noise to capture robust long-term interests.

</details>


### [103] [Evolutionary Router Feature Generation for Zero-Shot Graph Anomaly Detection with Mixture-of-Experts](https://arxiv.org/abs/2602.11622)
*Haiyang Jiang,Tong Chen,Xinyi Gao,Guansong Pang,Quoc Viet Hung Nguyen,Hongzhi Yin*

Main category: cs.IR

TL;DR: 提出用于零样本图异常检测的MoE框架EvoFG，实验显示其性能优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有单GNN方法在零样本图异常检测中因图的异质性表达不足，MoE架构受分布偏移限制，路由存在挑战。

Method: 提出进化特征生成方案，通过基于大语言模型的生成器和Shapley引导评估构建和选择特征；设计具有不变学习目标的记忆增强路由器。

Result: 在六个基准测试上，EvoFG始终优于现有基线，实现了强大且稳定的零样本图异常检测性能。

Conclusion: EvoFG在解决零样本图异常检测的路由挑战方面有效，能提升性能。

Abstract: Zero-shot graph anomaly detection (GAD) has attracted increasing attention recent years, yet the heterogeneity of graph structures, features, and anomaly patterns across graphs make existing single GNN methods insufficiently expressive to model diverse anomaly mechanisms. In this regard, Mixture-of-experts (MoE) architectures provide a promising paradigm by integrating diverse GNN experts with complementary inductive biases, yet their effectiveness in zero-shot GAD is severely constrained by distribution shifts, leading to two key routing challenges. First, nodes often carry vastly different semantics across graphs, and straightforwardly performing routing based on their features is prone to generating biased or suboptimal expert assignments. Second, as anomalous graphs often exhibit pronounced distributional discrepancies, existing router designs fall short in capturing domain-invariant routing principles that generalize beyond the training graphs. To address these challenges, we propose a novel MoE framework with evolutionary router feature generation (EvoFG) for zero-shot GAD. To enhance MoE routing, we propose an evolutionary feature generation scheme that iteratively constructs and selects informative structural features via an LLM-based generator and Shapley-guided evaluation. Moreover, a memory-enhanced router with an invariant learning objective is designed to capture transferable routing patterns under distribution shifts. Extensive experiments on six benchmarks show that EvoFG consistently outperforms state-of-the-art baselines, achieving strong and stable zero-shot GAD performance.

</details>


### [104] [IntTravel: A Real-World Dataset and Generative Framework for Integrated Multi-Task Travel Recommendation](https://arxiv.org/abs/2602.11664)
*Huimin Yan,Longfei Xu,Junjie Sun,Zheng Liu,Wei Luo,Kaikui Liu,Xiangxiang Chu*

Main category: cs.IR

TL;DR: 提出大规模公共数据集IntTravel用于综合旅行推荐，并构建端到端生成框架，在多数据集表现出色且已部署应用。


<details>
  <summary>Details</summary>
Motivation: 现有研究受碎片化数据集限制，仅关注下一兴趣点推荐，忽略出发时间、出行方式和情境需求，且数据集规模有限影响性能评估。

Method: 引入IntTravel数据集，构建端到端、仅解码器的生成框架用于多任务推荐，融入信息保存、选择和分解。

Result: 框架在IntTravel数据集和非旅行基准测试中表现达到最优，IntTravel在高德地图部署使CTR提升1.09%。

Conclusion: IntTravel数据集和生成框架有效解决现有研究问题，具有良好性能和可推广性。

Abstract: Next Point of Interest (POI) recommendation is essential for modern mobility and location-based services. To provide a smooth user experience, models must understand several components of a journey holistically: "when to depart", "how to travel", "where to go", and "what needs arise via the route". However, current research is limited by fragmented datasets that focus merely on next POI recommendation ("where to go"), neglecting the departure time, travel mode, and situational requirements along the journey. Furthermore, the limited scale of these datasets impedes accurate evaluation of performance. To bridge this gap, we introduce IntTravel, the first large-scale public dataset for integrated travel recommendation, including 4.1 billion interactions from 163 million users with 7.3 million POIs. Built upon this dataset, we introduce an end-to-end, decoder-only generative framework for multi-task recommendation. It incorporates information preservation, selection, and factorization to balance task collaboration with specialized differentiation, yielding substantial performance gains. The framework's generalizability is highlighted by its state-of-the-art performance across both IntTravel dataset and an additional non-travel benchmark. IntTravel has been successfully deployed on Amap serving hundreds of millions of users, leading to a 1.09% increase in CTR. IntTravel is available at https://github.com/AMAP-ML/IntTravel.

</details>


### [105] [EpicCBR: Item-Relation-Enhanced Dual-Scenario Contrastive Learning for Cold-Start Bundle Recommendation](https://arxiv.org/abs/2602.11680)
*Yihang Li,Zhuo Liu,Wei Wei*

Main category: cs.IR

TL;DR: 提出EpicCBR框架解决冷启动捆绑推荐问题，实验显示其性能远超现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有捆绑推荐模型依赖观察到的用户 - 捆绑交互，难以探索新捆绑，且未充分利用用户 - 物品和捆绑 - 物品关系。

Method: 提出多视图对比学习框架EpicCBR，精确挖掘和利用物品关系构建用户画像，提出基于流行度的方法刻画新捆绑特征，引入多视图图对比学习框架确保模型泛化能力。

Result: 在三个流行基准上实验，EpicCBR比现有技术大幅领先（最高达387%）。

Conclusion: EpicCBR在冷启动场景中具有优越性。

Abstract: Bundle recommendation aims to recommend a set of items to users for overall consumption. Existing bundle recommendation models primarily depend on observed user-bundle interactions, limiting exploration of newly-emerged bundles that are constantly created. It pose a critical representation challenge for current bundle methods, as they usually treat each bundle as an independent instance, while neglecting to fully leverage the user-item (UI) and bundle-item (BI) relations over popular items. To alleviate it, in this paper we propose a multi-view contrastive learning framework for cold-start bundle recommendation, named EpicCBR. Specifically, it precisely mine and utilize the item relations to construct user profiles, identifying users likely to engage with bundles. Additionally, a popularity-based method that characterizes the features of new bundles through historical bundle information and user preferences is proposed. To build a framework that demonstrates robustness in both cold-start and warm-start scenarios, a multi-view graph contrastive learning framework capable of integrating these diverse scenarios is introduced to ensure the model's generalization capability. Extensive experiments conducted on three popular benchmarks showed that EpicCBR outperforms state-of-the-art by a large margin (up to 387%), sufficiently demonstrating the superiority of the proposed method in cold-start scenario. The code and dataset can be found in the GitHub repository: https://github.com/alexlovecoding/EpicCBR.

</details>


### [106] [Uncertainty-aware Generative Recommendation](https://arxiv.org/abs/2602.11719)
*Chenxiao Fan,Chongming Gao,Yaxin Gong,Haoyan Liu,Fuli Feng,Xiangnan He*

Main category: cs.IR

TL;DR: 提出不确定性感知生成推荐框架UGR，解决现有方法不确定性盲视问题，实验表明UGR性能优且稳定，还能支持下游风险感知应用。


<details>
  <summary>Details</summary>
Motivation: 现有偏好优化方法存在不确定性盲视问题，导致训练动态不稳定和决策风险无法量化。

Method: 提出UGR框架，协同不确定性加权奖励、难度感知优化动态和显式置信度对齐三种机制。

Result: UGR推荐性能优越，稳定训练，避免标准方法常见的性能下降，学习到的置信度支持下游风险感知应用。

Conclusion: UGR能有效解决现有生成推荐方法的不确定性问题，具有良好性能和稳定性，可支持下游应用。

Abstract: Generative Recommendation has emerged as a transformative paradigm, reformulating recommendation as an end-to-end autoregressive sequence generation task. Despite its promise, existing preference optimization methods typically rely on binary outcome correctness, suffering from a systemic limitation we term uncertainty blindness. This issue manifests in the neglect of the model's intrinsic generation confidence, the variation in sample learning difficulty, and the lack of explicit confidence expression, directly leading to unstable training dynamics and unquantifiable decision risks. In this paper, we propose Uncertainty-aware Generative Recommendation (UGR), a unified framework that leverages uncertainty as a critical signal for adaptive optimization. UGR synergizes three mechanisms: (1) an uncertainty-weighted reward to penalize confident errors; (2) difficulty-aware optimization dynamics to prevent premature convergence; and (3) explicit confidence alignment to empower the model with confidence expression capabilities. Extensive experiments demonstrate that UGR not only yields superior recommendation performance but also fundamentally stabilizes training, preventing the performance degradation often observed in standard methods. Furthermore, the learned confidence enables reliable downstream risk-aware applications.

</details>


### [107] [ULTRA:Urdu Language Transformer-based Recommendation Architecture](https://arxiv.org/abs/2602.11836)
*Alishbah Bashir,Fatima Qaiser,Ijaz Hussain*

Main category: cs.IR

TL;DR: 针对乌尔都语低资源语言缺乏有效语义内容推荐系统的问题，提出ULTRA框架，实验证明其能提升推荐相关性。


<details>
  <summary>Details</summary>
Motivation: 乌尔都语作为低资源语言，现有的内容推荐方法依赖词汇匹配或通用技术，难以捕捉语义意图，导致推荐的相关性和适应性不足。

Method: 提出ULTRA框架，采用双嵌入架构和查询长度感知路由机制，根据阈值将用户查询路由到专门的语义管道，利用基于Transformer的嵌入和优化池化策略进行上下文感知的相似性搜索。

Result: 在大规模乌尔都语新闻语料库上的实验表明，该架构在不同查询类型下持续提高推荐相关性，与单管道基线相比，精度提高到90%以上。

Conclusion: ULTRA是一个强大且可推广的内容推荐架构，为低资源语言环境下的语义检索系统提供了实用的设计见解。

Abstract: Urdu, as a low-resource language, lacks effective semantic content recommendation systems, particularly in the domain of personalized news retrieval. Existing approaches largely rely on lexical matching or language-agnostic techniques, which struggle to capture semantic intent and perform poorly under varying query lengths and information needs. This limitation results in reduced relevance and adaptability in Urdu content recommendation. We propose ULTRA (Urdu Language Transformer-based Recommendation Architecture),an adaptive semantic recommendation framework designed to address these challenges. ULTRA introduces a dual-embedding architecture with a query-length aware routing mechanism that dynamically distinguishes between short, intent-focused queries and longer, context-rich queries. Based on a threshold-driven decision process, user queries are routed to specialized semantic pipelines optimized for either title/headline-level or full-content/document level representations, ensuring appropriate semantic granularity during retrieval. The proposed system leverages transformer-based embeddings and optimized pooling strategies to move beyond surface-level keyword matching and enable context-aware similarity search. Extensive experiments conducted on a large-scale Urdu news corpus demonstrate that the proposed architecture consistently improves recommendation relevance across diverse query types. Results show gains in precision above 90% compared to single-pipeline baselines, highlighting the effectiveness of query-adaptive semantic alignment for low-resource languages. The findings establish ULTRA as a robust and generalizable content recommendation architecture, offering practical design insights for semantic retrieval systems in low-resource language settings.

</details>


### [108] [Improving Neural Retrieval with Attribution-Guided Query Rewriting](https://arxiv.org/abs/2602.11841)
*Moncef Garouani,Josiane Mothe*

Main category: cs.IR

TL;DR: 提出归因引导的查询重写方法，在BEIR数据集上提升检索效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅部分解决神经检索器的脆性问题，如LLM重写查询无检索器反馈，可解释性方法用于事后分析。

Method: 计算基于梯度的标记归因，将分数作为软指导用于结构化提示来引导LLM重写查询。

Result: 在BEIR集合上，重写查询在强基线基础上持续提高检索效果，对隐式或模糊信息需求提升更大。

Conclusion: 提出的归因引导查询重写方法有效，能改善神经检索器的脆性问题。

Abstract: Neural retrievers are effective but brittle: underspecified or ambiguous queries can misdirect ranking even when relevant documents exist. Existing approaches address this brittleness only partially: LLMs rewrite queries without retriever feedback, and explainability methods identify misleading tokens but are used for post-hoc analysis. We close this loop and propose an attribution-guided query rewriting method that uses token-level explanations to guide query rewriting. For each query, we compute gradient-based token attributions from the retriever and then use these scores as soft guidance in a structured prompt to an LLM that clarifies weak or misleading query components while preserving intent. Evaluated on BEIR collections, the resulting rewrites consistently improve retrieval effectiveness over strong baselines, with larger gains for implicit or ambiguous information needs.

</details>


### [109] [Efficient Crawling for Scalable Web Data Acquisition (Extended Version)](https://arxiv.org/abs/2602.11874)
*Antoine Gauquier,Ioana Manolescu,Pierre Senellart*

Main category: cs.IR

TL;DR: 为提高开放统计数据可访问性，提出基于强化学习的聚焦网络爬虫算法，实验证明其高效。


<details>
  <summary>Details</summary>
Motivation: 新闻事实核查和社会经济研究需分析高质量统计数据集，但大规模检索困难，为提高开放统计数据可访问性开展研究。

Method: 提出基于强化学习（睡眠多臂老虎机）的方法，设计SB - CLASSIFIER爬虫，根据网页中链接路径学习哪些超链接指向含多目标的页面。

Result: 在数百万网页的网站上实验表明，爬虫高效，只爬取小部分网站就能获取高比例目标。

Conclusion: 所提聚焦网络爬虫算法能高效获取目标，提高开放统计数据的可访问性。

Abstract: Journalistic fact-checking, as well as social or economic research, require analyzing high-quality statistics datasets (SDs, in short). However, retrieving SD corpora at scale may be hard, inefficient, or impossible, depending on how they are published online. To improve open statistics data accessibility, we present a focused Web crawling algorithm that retrieves as many targets, i.e., resources of certain types, as possible, from a given website, in an efficient and scalable way, by crawling (much) less than the full website. We show that optimally solving this problem is intractable, and propose an approach based on reinforcement learning, namely using sleeping bandits. We propose SB-CLASSIFIER, a crawler that efficiently learns which hyperlinks lead to pages that link to many targets, based on the paths leading to the links in their enclosing webpages. Our experiments on websites with millions of webpages show that our crawler is highly efficient, delivering high fractions of a site's targets while crawling only a small part.

</details>


### [110] [IncompeBench: A Permissively Licensed, Fine-Grained Benchmark for Music Information Retrieval](https://arxiv.org/abs/2602.11941)
*Benjamin Clavié,Atoof Shakir,Jonah Turner,Sean Lee,Aamir Shakir,Makoto P. Kato*

Main category: cs.IR

TL;DR: 现有音乐检索缺乏高质量评估基准，引入IncompeBench基准及公开相关数据集和提示。


<details>
  <summary>Details</summary>
Motivation: 当前音乐信息检索缺乏高质量评估基准。

Method: 构建包含1574个音乐片段、500个查询和超125000个相关性判断的多阶段标注管道。

Result: 创建了高一致性的标注，数据集和提示公开。

Conclusion: IncompeBench可用于评估音乐检索性能。

Abstract: Multimodal Information Retrieval has made significant progress in recent years, leveraging the increasingly strong multimodal abilities of deep pre-trained models to represent information across modalities. Music Information Retrieval (MIR), in particular, has considerably increased in quality, with neural representations of music even making its way into everyday life products. However, there is a lack of high-quality benchmarks for evaluating music retrieval performance. To address this issue, we introduce \textbf{IncompeBench}, a carefully annotated benchmark comprising $1,574$ permissively licensed, high-quality music snippets, $500$ diverse queries, and over $125,000$ individual relevance judgements. These annotations were created through the use of a multi-stage pipeline, resulting in high agreement between human annotators and the generated data. The resulting datasets are publicly available at https://huggingface.co/datasets/mixedbread-ai/incompebench-strict and https://huggingface.co/datasets/mixedbread-ai/incompebench-lenient with the prompts available at https://github.com/mixedbread-ai/incompebench-programs.

</details>


### [111] [Compress, Cross and Scale: Multi-Level Compression Cross Networks for Efficient Scaling in Recommender Systems](https://arxiv.org/abs/2602.12041)
*Heng Yu,Xiangjun Zhou,Jie Xia,Heng Zhao,Anxin Wu,Yu Zhao,Dongying Kong*

Main category: cs.IR

TL;DR: 提出MLCC及MC - MLCC模型捕捉高阶特征依赖，实验显示其优于基线模型，有良好扩展性，在线测试验证有效性并在B站广告系统应用。


<details>
  <summary>Details</summary>
Motivation: 现有交互模块难以同时实现强交互能力、高计算效率和良好扩展性，在生产约束下模型扩展ROI有限。

Method: 提出MLCC结构化特征交互架构，通过分层压缩和动态组合组织特征交叉；引入MC - MLCC多通道扩展，将特征交互分解到并行子空间。

Result: 在三个公共基准和大规模工业数据集上，模型优于基线模型，降低参数和FLOPs；扩展性分析显示基于通道扩展效率更高。

Conclusion: 提出的模型有效，能在严格延迟和资源约束下应用，已被B站广告系统广泛采用。

Abstract: Modeling high-order feature interactions efficiently is a central challenge in click-through rate and conversion rate prediction. Modern industrial recommender systems are predominantly built upon deep learning recommendation models, where the interaction backbone plays a critical role in determining both predictive performance and system efficiency. However, existing interaction modules often struggle to simultaneously achieve strong interaction capacity, high computational efficiency, and good scalability, resulting in limited ROI when models are scaled under strict production constraints. In this work, we propose MLCC, a structured feature interaction architecture that organizes feature crosses through hierarchical compression and dynamic composition, which can efficiently capture high-order feature dependencies while maintaining favorable computational complexity. We further introduce MC-MLCC, a Multi-Channel extension that decomposes feature interactions into parallel subspaces, enabling efficient horizontal scaling with improved representation capacity and significantly reduced parameter growth. Extensive experiments on three public benchmarks and a large-scale industrial dataset show that our proposed models consistently outperform strong DLRM-style baselines by up to 0.52 AUC, while reducing model parameters and FLOPs by up to 26$\times$ under comparable performance. Comprehensive scaling analyses demonstrate stable and predictable scaling behavior across embedding dimension, head number, and channel count, with channel-based scaling achieving substantially better efficiency than conventional embedding inflation. Finally, online A/B testing on a real-world advertising platform validates the practical effectiveness of our approach, which has been widely adopted in Bilibili advertising system under strict latency and resource constraints.

</details>


### [112] [Towards Personalized Bangla Book Recommendation: A Large-Scale Multi-Entity Book Graph Dataset](https://arxiv.org/abs/2602.12129)
*Rahin Arefin Ahmed,Md. Anik Chowdhury,Sakil Ahmed Sheikh Reza,Devnil Bhattacharjee,Muhammad Abdullah Adnan,Nafis Sadeq*

Main category: cs.IR

TL;DR: 文章介绍用于孟加拉语图书推荐的大规模数据集RokomariBG，并对多种推荐模型进行基准测试。


<details>
  <summary>Details</summary>
Motivation: 解决孟加拉语文学个性化图书推荐缺乏结构化、大规模公开数据集的问题。

Method: 创建多实体异构图书图数据集RokomariBG，并在Top - N推荐任务上对多种推荐模型进行基准测试。

Result: 基准测试结果显示利用多关系结构和文本辅助信息很重要，神经检索模型表现最佳，NDCG@10 = 0.204。

Conclusion: 为孟加拉语图书推荐研究建立基础基准和公开资源，便于可重复性评估和未来低资源文化领域推荐研究。

Abstract: Personalized book recommendation in Bangla literature has been constrained by the lack of structured, large-scale, and publicly available datasets. This work introduces RokomariBG, a large-scale, multi-entity heterogeneous book graph dataset designed to support research on personalized recommendation in a low-resource language setting. The dataset comprises 127,302 books, 63,723 users, 16,601 authors, 1,515 categories, 2,757 publishers, and 209,602 reviews, connected through eight relation types and organized as a comprehensive knowledge graph.
  To demonstrate the utility of the dataset, we provide a systematic benchmarking study on the Top-N recommendation task, evaluating a diverse set of representative recommendation models, including classical collaborative filtering methods, matrix factorization models, content-based approaches, graph neural networks, a hybrid matrix factorization model with side information, and a neural two-tower retrieval architecture. The benchmarking results highlight the importance of leveraging multi-relational structure and textual side information, with neural retrieval models achieving the strongest performance (NDCG@10 = 0.204). Overall, this work establishes a foundational benchmark and a publicly available resource for Bangla book recommendation research, enabling reproducible evaluation and future studies on recommendation in low-resource cultural domains. The dataset and code are publicly available at https://github.com/backlashblitz/Bangla-Book-Recommendation-Dataset

</details>


### [113] [SAGEO Arena: A Realistic Environment for Evaluating Search-Augmented Generative Engine Optimization](https://arxiv.org/abs/2602.12187)
*Sunghwan Kim,Wooseok Jeong,Serin Kim,Sangam Lee,Dongha Lee*

Main category: cs.IR

TL;DR: 本文介绍SAGEO Arena用于SAGEO分析，发现现有方法在现实条件下不实用，结构信息可缓解局限，有效SAGEO需针对各阶段优化。


<details>
  <summary>Details</summary>
Motivation: 现有评估环境无法支持全面的SAGEO研究，缺乏端到端可见性评估且忽略结构信息。

Method: 整合大规模含丰富结构信息的网页文档的生成式搜索管道。

Result: 现有方法在现实条件下不实用，常降低检索和重排序性能，结构信息可缓解局限。

Conclusion: 该基准为现实条件下的SAGEO评估和优化奠定基础。

Abstract: Search-Augmented Generative Engines (SAGE) have emerged as a new paradigm for information access, bridging web-scale retrieval with generative capabilities to deliver synthesized answers. This shift has fundamentally reshaped how web content gains exposure online, giving rise to Search-Augmented Generative Engine Optimization (SAGEO), the practice of optimizing web documents to improve their visibility in AI-generated responses. Despite growing interest, no evaluation environment currently supports comprehensive investigation of SAGEO. Specifically, existing benchmarks lack end-to-end visibility evaluation of optimization strategies, operating on pre-determined candidate documents that abstract away retrieval and reranking preceding generation. Moreover, existing benchmarks discard structural information (e.g., schema markup) present in real web documents, overlooking the rich signals that search systems actively leverage in practice. Motivated by these gaps, we introduce SAGEO Arena, a realistic and reproducible environment for stage-level SAGEO analysis. Our objective is to jointly target search-oriented optimization (SEO) and generation-centric optimization (GEO). To achieve this, we integrate a full generative search pipeline over a large-scale corpus of web documents with rich structural information. Our findings reveal that existing approaches remain largely impractical under realistic conditions and often degrade performance in retrieval and reranking. We also find that structural information helps mitigate these limitations, and that effective SAGEO requires tailoring optimization to each pipeline stage. Overall, our benchmark paves the way for realistic SAGEO evaluation and optimization beyond simplified settings.

</details>


### [114] [AttentionRetriever: Attention Layers are Secretly Long Document Retrievers](https://arxiv.org/abs/2602.12278)
*David Jiahao Fu,Lam Thanh Do,Jiayu Li,Kevin Chen-Chuan Chang*

Main category: cs.IR

TL;DR: 提出AttentionRetriever用于长文档检索，实验显示其性能远超现有模型且效率与密集检索模型相当。


<details>
  <summary>Details</summary>
Motivation: 现有检索模型未针对长文档检索设计，无法应对上下文感知、因果依赖和检索范围等关键挑战。

Method: 提出AttentionRetriever，利用注意力机制和基于实体的检索为长文档构建上下文感知嵌入并确定检索范围。

Result: AttentionRetriever在长文档检索数据集上大幅超越现有检索模型，且效率与密集检索模型相当。

Conclusion: AttentionRetriever是一种有效的长文档检索模型。

Abstract: Retrieval augmented generation (RAG) has been widely adopted to help Large Language Models (LLMs) to process tasks involving long documents. However, existing retrieval models are not designed for long document retrieval and fail to address several key challenges of long document retrieval, including context-awareness, causal dependence, and scope of retrieval. In this paper, we proposed AttentionRetriever, a novel long document retrieval model that leverages attention mechanism and entity-based retrieval to build context-aware embeddings for long document and determine the scope of retrieval. With extensive experiments, we found AttentionRetriever is able to outperform existing retrieval models on long document retrieval datasets by a large margin while remaining as efficient as dense retrieval models.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [115] [RooflineBench: A Benchmarking Framework for On-Device LLMs via Roofline Analysis](https://arxiv.org/abs/2602.11506)
*Zhen Bi,Xueshu Chen,Luoyang Sun,Yuhang Yao,Qing Shen,Jungang Lou,Cheng Deng*

Main category: cs.LG

TL;DR: 提出基于Roofline模型的系统框架，通过操作强度衡量小语言模型在资源受限边缘硬件上的性能，揭示性能与操作强度受序列长度影响，还提出应对硬件异构问题的方法。


<details>
  <summary>Details</summary>
Motivation: 小语言模型向本地智能过渡，需要在资源受限边缘硬件上严格表征性能，但客观衡量不同架构理论性能上限是挑战。

Method: 提出基于Roofline模型的系统框架，统一架构原语和硬件约束；定义推理潜力区域，引入相对推理潜力指标。

Result: 性能和操作强度受序列长度影响；模型深度增加操作强度会退化；硬件异构存在效率陷阱，多头潜在注意力可释放推理潜力。

Conclusion: 研究结果为硬件 - 软件协同设计提供可操作方向，使神经结构与物理约束相匹配。

Abstract: The transition toward localized intelligence through Small Language Models (SLMs) has intensified the need for rigorous performance characterization on resource-constrained edge hardware. However, objectively measuring the theoretical performance ceilings of diverse architectures across heterogeneous platforms remains a formidable challenge. In this work, we propose a systematic framework based on the Roofline model that unifies architectural primitives and hardware constraints through the lens of operational intensity (OI). By defining an inference-potential region, we introduce the Relative Inference Potential as a novel metric to compare efficiency differences between Large Language Models (LLMs) on the same hardware substrate. Extensive empirical analysis across diverse compute tiers reveals that variations in performance and OI are significantly influenced by sequence length. We further identify a critical regression in OI as model depth increases. Additionally, our findings highlight an efficiency trap induced by hardware heterogeneity and demonstrate how structural refinements, such as Multi-head Latent Attention (M LA), can effectively unlock latent inference potential across various hardware substrates. These insights provide actionable directions for hardware-software co-design to align neural structures with physical constraints in on-device intelligence. The released code is available in the Appendix C.

</details>


### [116] [Automated Optimization Modeling via a Localizable Error-Driven Perspective](https://arxiv.org/abs/2602.11164)
*Weiting Liu,Han Wu,Yufei Kuang,Xiongwei Han,Tao Zhong,Jianfeng Feng,Wenlian Lu*

Main category: cs.LG

TL;DR: 本文指出大语言模型自动优化建模中后训练数据不足的问题，提出MIND框架解决现有方法局限，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型自动优化建模中后训练因高质量数据稀缺和利用不足，导致性能受限，需解决相关问题。

Method: 提出MIND框架，基于优化建模误差传播的局部化模式，构建聚焦的高密度训练语料，提出DFPO策略处理难题。

Result: 在六个基准测试中，MIND始终优于所有现有自动优化建模方法。

Conclusion: MIND框架能有效解决现有自动优化建模方法的局限，提升大语言模型在特定领域后训练的性能。

Abstract: Automated optimization modeling via Large Language Models (LLMs) has emerged as a promising approach to assist complex human decision-making. While post-training has become a pivotal technique to enhance LLMs' capabilities in this domain, its effectiveness is severely constrained by the scarcity and underutilization of high-quality training data. However, through a detailed profiling of error patterns across various problem-response pairs drawn from post-training, we identify two fundamental limitations of existing automated optimization modeling approaches: (L1) the sparsity of error-specific problems and (L2) the sparse rewards associated with difficult problems. We demonstrate that these limitations can result in suboptimal performance in domain-specific post-training for LLMs. To tackle the above two limitations, we propose a novel error-driven learning framework -- namely, auto\textbf{m}ated opt\textbf{i}mization modeli\textbf{n}g via a localizable error-\textbf{d}riven perspective (MIND) -- that customizes the whole model training framework from data synthesis to post-training. MIND is based on our key observation of the unique localizable patterns in error propagation of optimization modelings, that is, modeling errors may remain localized to specific semantic segments and do not propagate throughout the entire solution. Thus, in contrast to holistic reasoning tasks such as mathematical proofs, MIND leverages the construction of a focused, high-density training corpus and proposes \textbf{D}ynamic Supervised \textbf{F}ine-Tuning \textbf{P}olicy \textbf{O}ptimization (DFPO) to tackle difficult problems through localized refinement. Experiments on six benchmarks demonstrate that MIND consistently outperforms all the state-of-the-art automated optimization modeling approaches.

</details>


### [117] [KBVQ-MoE: KLT-guided SVD with Bias-Corrected Vector Quantization for MoE Large Language Models](https://arxiv.org/abs/2602.11184)
*Zukang Xu,Zhixiong Zhao,Xing Hu,Zhixuan Chen,Dawei Yang*

Main category: cs.LG

TL;DR: 本文针对MoE模型部署挑战，提出KBVQ - MoE框架解决VQ应用问题，实验表明其比现有量化方法能更好保持准确性。


<details>
  <summary>Details</summary>
Motivation: MoE模型参数大、内存需求高，在资源受限环境部署有挑战，直接应用VQ到MoE会导致性能下降。

Method: 提出KBVQ - MoE框架，集成输入驱动冗余消除（KLT引导SVD提取主导权重分量并跨专家共享）和偏差校正输出稳定化（仅对专家特定表示进行VQ并通过通道仿射补偿校正量化输出）。

Result: 在各种MoE大语言模型上实验，KBVQ - MoE比现有量化方法更好保持准确性，如3位量化Qwen1.5 - MoE - A2.7B平均准确率达67.99，接近FP16基线的68.07。

Conclusion: KBVQ - MoE有潜力在边缘设备等资源受限平台高效部署。

Abstract: Mixture of Experts (MoE) models have achieved great success by significantly improving performance while maintaining computational efficiency through sparse expert activation. However, their enormous parameter sizes and memory demands pose major challenges for deployment in resource-constrained environments. Vector Quantization (VQ) offers a promising approach for ultra-low-bit compression in Large Language Models (LLMs) by leveraging a codebook, where weight vectors are mapped to the most similar discrete codewords. Yet, directly applying VQ to MoEs often leads to substantial performance degradation due to two critical obstacles: (1) redundant representations among experts cause VQ to repeatedly quantize similar representations for each expert, resulting in inefficient use of limited codebook capacity; and (2) cumulative output bias is amplified by expert aggregation in MoE layers, leading to distributional shifts in the quantized outputs. To address these issues, we propose KBVQ-MoE, a novel VQ framework to enhance extremely low-bit quantization for MoE-based LLMs. KBVQ-MoE integrates two techniques: (1) input-driven redundancy elimination, where a Karhunen-Loeve Transform (KLT) guided singular value decomposition (SVD) extracts dominant weight components and shares them across experts; and (2) bias-corrected output stabilization, where vector quantization is applied only to expert-specific (non-redundant) representations and the quantized outputs are corrected via channel-wise affine compensation. Experiments on various MoE LLMs demonstrate that KBVQ-MoE preserves accuracy substantially better than existing quantization methods. For example, 3-bit quantization of Qwen1.5-MoE-A2.7B achieves an average accuracy of 67.99, nearly identical to the FP16 baseline of 68.07, underscoring KBVQ-MoE's potential for efficient deployment on edge devices and other resource-constrained platforms.

</details>


### [118] [Predictive Associative Memory: Retrieval Beyond Similarity Through Temporal Co-occurrence](https://arxiv.org/abs/2602.11322)
*Jason Dury*

Main category: cs.LG

TL;DR: 针对现有神经记忆系统不足提出PAM架构，评估显示其在关联回忆上表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有神经记忆系统基于相似性检索，无法捕捉生物记忆的时间共现关联特性，需改进。

Method: 提出PAM架构，引入Inward JEPA，将其作为关联回忆系统评估。

Result: 在合成基准测试中，预测器多项指标表现好，如Association Precision@1 = 0.970等，打乱时间顺序后性能大幅下降，结果稳定。

Conclusion: PAM架构能有效利用时间共现结构进行关联回忆，优于基于相似性的检索方法。

Abstract: Current approaches to memory in neural systems rely on similarity-based retrieval: given a query, find the most representationally similar stored state. This assumption -- that useful memories are similar memories -- fails to capture a fundamental property of biological memory: association through temporal co-occurrence. We propose Predictive Associative Memory (PAM), an architecture in which a JEPA-style predictor, trained on temporal co-occurrence within a continuous experience stream, learns to navigate the associative structure of an embedding space. We introduce an Inward JEPA that operates over stored experience (predicting associatively reachable past states) as the complement to the standard Outward JEPA that operates over incoming sensory data (predicting future states). We evaluate PAM as an associative recall system -- testing faithfulness of recall for experienced associations -- rather than as a retrieval system evaluated on generalisation to unseen associations. On a synthetic benchmark, the predictor's top retrieval is a true temporal associate 97% of the time (Association Precision@1 = 0.970); it achieves cross-boundary Recall@20 = 0.421 where cosine similarity scores zero; and it separates experienced-together from never-experienced-together states with a discrimination AUC of 0.916 (cosine: 0.789). Even restricted to cross-room pairs where embedding similarity is uninformative, the predictor achieves AUC = 0.849 (cosine: 0.503, chance). A temporal shuffle control confirms the signal is genuine temporal co-occurrence structure, not embedding geometry: shuffling collapses cross-boundary recall by 90%, replicated across training seeds. All results are stable across seeds (SD < 0.006) and query selections (SD $\leq$ 0.012).

</details>


### [119] [Spectra: Rethinking Optimizers for LLMs Under Spectral Anisotropy](https://arxiv.org/abs/2602.11185)
*Zhendong Huang,Hengjie Cao,Fang Dong,Ruijun Huang,Mengyi Chen,Yifeng Yang,Xin Zhang,Anrui Chen,Mingzhi Dong,Yujiang Wang,Jinlong Hou,Qin Lv,Robert P. Dick,Yuan Cheng,Fan Yang,Tun Lu,Li Shang*

Main category: cs.LG

TL;DR: 论文指出LLM训练中梯度信号各向异性，提出Spectra优化器，在LLaMA3 8B上效果优于AdamW和Muon。


<details>
  <summary>Details</summary>
Motivation: LLM训练中梯度信号的各向异性导致主导方向抑制尾部学习，需改进优化器。

Method: 提出Spectra优化器，通过缓存、热启动幂迭代跟踪尖峰子空间并进行低秩谱整形。

Result: 在LLaMA3 8B上，Spectra比AdamW训练快30%，减少开销和内存，提高准确率；比Muon处理速度快5.1倍，损失更低，准确率提高。

Conclusion: Spectra优化器有效改善了LLM训练的效率和效果。

Abstract: Gradient signals in LLM training are highly anisotropic: recurrent linguistic structure concentrates energy into a small set of dominant spectral directions, while context specific information resides in a long tail. We show that this spike tail separation persists throughout training, with the spike occupying only about 1.5% of directions yet dominating optimizer statistics. This dominance suppresses tail learning by contracting tail updates through second moment normalization and tightening the globally stable learning rate bound. Motivated by this analysis, we propose Spectra, a spike aware optimizer that suppresses the dominant low rank spike subspace without amplifying the noise sensitive spectral tail. Spectra tracks the spike subspace via cached, warm started power iteration and applies low rank spectral shaping with negligible overhead and substantially reduced optimizer state memory. On LLaMA3 8B trained on 50B tokens, Spectra reaches the same target loss 30% faster than AdamW, reduces per step end to end overhead by 0.7%, cuts optimizer state memory by 49.25%, and improves average downstream accuracy by 1.62%. Compared to Muon, Spectra is 5.1x faster in optimizer processing time, achieves a lower final loss, and improves average accuracy by 0.66%.

</details>


### [120] [GAC-KAN: An Ultra-Lightweight GNSS Interference Classifier for GenAI-Powered Consumer Edge Devices](https://arxiv.org/abs/2602.11186)
*Zhihan Zeng,Kaihe Wang,Zhongpei Zhang,Yue Xiu*

Main category: cs.LG

TL;DR: 本文提出GAC - KAN框架解决GenAI时代数据稀缺和效率问题，在保证高准确率同时具有极轻量级特点。


<details>
  <summary>Details</summary>
Motivation: GenAI应用给边缘硬件带来计算负担，且真实干扰数据稀缺，影响安全任务如GNSS信号保护和分类器训练。

Method: 采用物理引导模拟方法合成数据集；设计MS - GAC骨干网络；引入KAN决策头。

Result: GAC - KAN整体准确率达98.0%，模型仅含0.13万个参数，比ViT基线少约660倍。

Conclusion: GAC - KAN是理想的“始终在线”安全伴侣，能确保GNSS可靠性且不与GenAI主要任务争夺计算资源。

Abstract: The integration of Generative AI (GenAI) into Consumer Electronics (CE)--from AI-powered assistants in wearables to generative planning in autonomous Uncrewed Aerial Vehicles (UAVs)--has revolutionized user experiences. However, these GenAI applications impose immense computational burdens on edge hardware, leaving strictly limited resources for fundamental security tasks like Global Navigation Satellite System (GNSS) signal protection. Furthermore, training robust classifiers for such devices is hindered by the scarcity of real-world interference data. To address the dual challenges of data scarcity and the extreme efficiency required by the GenAI era, this paper proposes a novel framework named GAC-KAN. First, we adopt a physics-guided simulation approach to synthesize a large-scale, high-fidelity jamming dataset, mitigating the data bottleneck. Second, to reconcile high accuracy with the stringent resource constraints of GenAI-native chips, we design a Multi-Scale Ghost-ACB-Coordinate (MS-GAC) backbone. This backbone combines Asymmetric Convolution Blocks (ACB) and Ghost modules to extract rich spectral-temporal features with minimal redundancy. Replacing the traditional Multi-Layer Perceptron (MLP) decision head, we introduce a Kolmogorov-Arnold Network (KAN), which employs learnable spline activation functions to achieve superior non-linear mapping capabilities with significantly fewer parameters. Experimental results demonstrate that GAC-KAN achieves an overall accuracy of 98.0\%, outperforming state-of-the-art baselines. Significantly, the model contains only 0.13 million parameter--approximately 660 times fewer than Vision Transformer (ViT) baselines. This extreme lightweight characteristic makes GAC-KAN an ideal "always-on" security companion, ensuring GNSS reliability without contending for the computational resources required by primary GenAI tasks.

</details>


### [121] [TDPNavigator-Placer: Thermal- and Wirelength-Aware Chiplet Placement in 2.5D Systems Through Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.11187)
*Yubo Hou,Furen Zhuang,Partha Pratim Kundu,Sezin Ata Kircali,Jie Wang,Mihai Dragos Rotaru,Dutta Rahul,Ashish James*

Main category: cs.LG

TL;DR: 提出TDPNavigator - Placer多智能体强化学习框架优化2.5D集成电路芯片小核布局，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有芯片小核布局方法在处理线长和热管理等冲突设计需求时能力有限，难以实际应用。

Method: 提出TDPNavigator - Placer多智能体强化学习框架，将冲突目标分配给专门智能体，在统一布局范式下按不同奖励机制和环境约束运行。

Result: TDPNavigator - Placer比现有方法显著改善了帕累托前沿，能在线长和热性能间实现更平衡的权衡。

Conclusion: TDPNavigator - Placer在2.5D集成电路芯片小核布局中表现更优，可应对设计需求冲突问题。

Abstract: The rapid growth of electronics has accelerated the adoption of 2.5D integrated circuits, where effective automated chiplet placement is essential as systems scale to larger and more heterogeneous chiplet assemblies. Existing placement methods typically focus on minimizing wirelength or transforming multi-objective optimization into a single objective through weighted sum, which limits their ability to handle competing design requirements. Wirelength reduction and thermal management are inherently conflicting objectives, making prior approaches inadequate for practical deployment. To address this challenge, we propose TDPNavigator-Placer, a novel multi-agent reinforcement learning framework that dynamically optimizes placement based on chiplet's thermal design power (TDP). This approach explicitly assigns these inherently conflicting objectives to specialized agents, each operating under distinct reward mechanisms and environmental constraints within a unified placement paradigm. Experimental results demonstrate that TDPNavigator-Placer delivers a significantly improved Pareto front over state-of-the-art methods, enabling more balanced trade-offs between wirelength and thermal performance.

</details>


### [122] [Time-TK: A Multi-Offset Temporal Interaction Framework Combining Transformer and Kolmogorov-Arnold Networks for Time Series Forecasting](https://arxiv.org/abs/2602.11190)
*Fan Zhang,Shiming Fan,Hua Wang*

Main category: cs.LG

TL;DR: 文章指出现有时间序列预测方法存在信息瓶颈，提出新时间序列嵌入视角，设计MOTE方法和Time - TK架构，实验显示其预测精度达最优。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测方法在处理长序列时，独立令牌嵌入破坏多偏移时间相关性，产生信息瓶颈，需新解决方案。

Method: 提出新的时间序列嵌入视角，给出令牌嵌入近似重建性能上限，设计MOTE方法，基于此设计Time - TK架构，包括多偏移交互式KAN学习时间模式和多偏移时间交互机制实现信息集成。

Result: 在14个真实基准数据集上实验表明，Time - TK显著优于所有基线模型。

Conclusion: Time - TK架构能有效解决现有方法的信息瓶颈问题，实现了最先进的预测精度。

Abstract: Time series forecasting is crucial for the World Wide Web and represents a core technical challenge in ensuring the stable and efficient operation of modern web services, such as intelligent transportation and website throughput. However, we have found that existing methods typically employ a strategy of embedding each time step as an independent token. This paradigm introduces a fundamental information bottleneck when processing long sequences, the root cause of which is that independent token embedding destroys a crucial structure within the sequence - what we term as multi-offset temporal correlation. This refers to the fine-grained dependencies embedded within the sequence that span across different time steps, which is especially prevalent in regular Web data. To fundamentally address this issue, we propose a new perspective on time series embedding. We provide an upper bound on the approximate reconstruction performance of token embedding, which guides our design of a concise yet effective Multi-Offset Time Embedding method to mitigate the performance degradation caused by standard token embedding. Furthermore, our MOTE can be integrated into various existing models and serve as a universal building block. Based on this paradigm, we further design a novel forecasting architecture named Time-TK. This architecture first utilizes a Multi-Offset Interactive KAN to learn and represent specific temporal patterns among multiple offset sub-sequences. Subsequently, it employs an efficient Multi-Offset Temporal Interaction mechanism to effectively capture the complex dependencies between these sub-sequences, achieving global information integration. Extensive experiments on 14 real-world benchmark datasets, covering domains such as traffic flow and BTC/USDT throughput, demonstrate that Time-TK significantly outperforms all baseline models, achieving state-of-the-art forecasting accuracy.

</details>


### [123] [Toward Adaptive Non-Intrusive Reduced-Order Models: Design and Challenges](https://arxiv.org/abs/2602.11378)
*Amirpasha Hedayat,Alberto Padovan,Karthik Duraisamy*

Main category: cs.LG

TL;DR: 本文提出三种自适应非侵入式降阶模型（ROM）公式，分析其在瞬态扰动驱动腔流中的表现，并强调ROM预测应考虑成本。


<details>
  <summary>Details</summary>
Motivation: 传统基于投影的静态ROM在系统离开训练流形时实用性受限，需研究自适应非侵入式ROM。

Method: 基于静态非侵入式ROM的思想，提出Adaptive OpInf、Adaptive NiTROM和混合公式三种自适应ROM公式，并描述相关窗口和计算预算，分析成本缩放。

Result: 在瞬态扰动驱动腔流中，静态ROM预测超出训练范围时会漂移或失稳，而自适应ROM各有优势，如Adaptive OpInf能抑制幅度漂移，Adaptive NiTROM能实现近精确能量跟踪，混合公式在工况变化和离线数据少的情况下更可靠。

Conclusion: ROM的预测声明应考虑成本，本文为构建自校正、非侵入式ROM提供了实用模板。

Abstract: Projection-based Reduced Order Models (ROMs) are often deployed as static surrogates, which limits their practical utility once a system leaves the training manifold. We formalize and study adaptive non-intrusive ROMs that update both the latent subspace and the reduced dynamics online. Building on ideas from static non-intrusive ROMs, specifically, Operator Inference (OpInf) and the recently-introduced Non-intrusive Trajectory-based optimization of Reduced-Order Models (NiTROM), we propose three formulations: Adaptive OpInf (sequential basis/operator refits), Adaptive NiTROM (joint Riemannian optimization of encoder/decoder and polynomial dynamics), and a hybrid that initializes NiTROM with an OpInf update. We describe the online data window, adaptation window, and computational budget, and analyze cost scaling. On a transiently perturbed lid-driven cavity flow, static Galerkin/OpInf/NiTROM drift or destabilize when forecasting beyond training. In contrast, Adaptive OpInf robustly suppresses amplitude drift with modest cost; Adaptive NiTROM is shown to attain near-exact energy tracking under frequent updates but is sensitive to its initialization and optimization depth; the hybrid is most reliable under regime changes and minimal offline data, yielding physically coherent fields and bounded energy. We argue that predictive claims for ROMs must be cost-aware and transparent, with clear separation of training/adaptation/deployment regimes and explicit reporting of online budgets and full-order model queries. This work provides a practical template for building self-correcting, non-intrusive ROMs that remain effective as the dynamics evolve well beyond the initial manifold.

</details>


### [124] [MELINOE: Fine-Tuning Enables Memory-Efficient Inference for Mixture-of-Experts Models](https://arxiv.org/abs/2602.11192)
*Arian Raje,Anupam Nayak,Gauri Joshi*

Main category: cs.LG

TL;DR: 提出MELINOE方法优化MoE模型推理效率，提升吞吐量，不损失下游任务性能。


<details>
  <summary>Details</summary>
Motivation: MoE模型虽计算高效，但因参数多难以用于资源受限场景，现有解决内存瓶颈方法有I/O延迟问题。

Method: 微调MoE模型，使其倾向激活少量专家，将这些专家缓存到GPU内存。

Result: 相比高效基线吞吐量提升1.2 - 3倍，相比高传输基线提升达14.7倍，下游任务性能不变或提升。

Conclusion: MELINOE是提升MoE推理效率的可靠方法。

Abstract: Mixture-of-Experts (MoE) model architectures can significantly reduce the number of activated parameters per token, enabling computationally efficient training and inference. However, their large overall parameter counts and model sizes have precluded their widespread usage in resource-constrained settings as all of the parameters must still be loaded into GPU memory. Prior works aim to address this memory bottleneck by offloading certain experts into CPU memory and porting them to GPU memory only when they are activated. In practice, these methods suffer from the significant I/O latency incurred by expert transfer. We present MELINOE, a method that fine-tunes an MoE model to more strongly prefer activating a smaller number of experts per sequence. Caching these preferred experts in GPU memory reduces expert churn and CPU-GPU transfer overhead. MELINOE increases throughput by $1.2-3\times$ over efficient baselines and up to $14.7\times$ over transfer-heavy baselines while retaining or even improving the performance of the model on a downstream task, making it a reliable method for improving MoE inference efficiency.

</details>


### [125] [Predicting the post-wildfire mudflow onset using machine learning models on multi-parameter experimental data](https://arxiv.org/abs/2602.11194)
*Mahta Movasat,Ingrid Tomac*

Main category: cs.LG

TL;DR: 研究运用多种机器学习算法预测和分类模拟实验结果，发现其对野火后灾害评估有潜力


<details>
  <summary>Details</summary>
Motivation: 野火后泥石流危害增大，需理解泥石流发生时间和条件，机器学习可建模复杂系统

Method: 应用多元线性回归、逻辑回归、支持向量分类器、K - 均值聚类和主成分分析等算法，对模拟实验结果进行预测和分类

Result: 多元线性回归预测总流量较有效，逻辑回归和支持向量分类器分类失效结果准确性高，细砂易侵蚀，高强度降雨前10分钟关键

Conclusion: 机器学习在野火后灾害评估和应急响应规划中有潜力

Abstract: Post-wildfire mudflows are increasingly hazardous due to the prevalence of wildfires, including those on the wildland-urban interface. Upon burning, soil on the surface or immediately beneath becomes hydrophobic, a phenomenon that occurs predominantly on sand-based hillslopes. Rainwater and eroded soil blanket the downslope, leading to catastrophic debris flows. Soil hydrophobicity enhances erosion, resulting in post-wildfire debris flows that differ from natural mudflows in intensity, duration, and destructiveness. Thus, it is crucial to understand the timing and conditions of debris-flow onset, driven by the coupled effects of critical parameters: varying rain intensities (RI), slope gradients, water-entry values, and grain sizes (D50). Machine Learning (ML) techniques have become increasingly valuable in geotechnical engineering due to their ability to model complex systems without predefined assumptions. This study applies multiple ML algorithms: multiple linear regression (MLR), logistic regression (LR), support vector classifier (SVC), K-means clustering, and principal component analysis (PCA) to predict and classify outcomes from laboratory experiments that model field conditions using a rain device on various soils in sloped flumes. While MLR effectively predicted total discharge, erosion predictions were less accurate, especially for coarse sand. LR and SVC achieved good accuracy in classifying failure outcomes, supported by clustering and dimensionality reduction. Sensitivity analysis revealed that fine sand is highly susceptible to erosion, particularly under low-intensity, long-duration rainfall. Results also show that the first 10 minutes of high-intensity rain are most critical for discharge and failure. These findings highlight the potential of ML for post-wildfire hazard assessment and emergency response planning.

</details>


### [126] [Potential-energy gating for robust state estimation in bistable stochastic systems](https://arxiv.org/abs/2602.11712)
*Luigi Simeone*

Main category: cs.LG

TL;DR: 提出势能门控方法用于双阱随机动力学系统的鲁棒状态估计，在多种滤波器实现，合成基准测试显示有显著RMSE改善，且对模型误设有鲁棒性，还应用于冰芯记录分析。


<details>
  <summary>Details</summary>
Motivation: 为双阱随机动力学系统开发一种更有效的鲁棒状态估计方法，区别于传统统计鲁棒滤波器和约束滤波器。

Method: 通过已知或假设的势能函数的局部值调制贝叶斯滤波器的观测噪声协方差，在多种卡尔曼滤波器和粒子滤波器中实现门控，仅需两个额外超参数。

Result: 在含10%异常值污染的Ginzburg - Landau双阱过程合成基准测试中，比标准扩展卡尔曼滤波器有57 - 80%的RMSE改善；对模型误设鲁棒，偏差50%时改善仍不低于47%；应用于冰芯记录分析，估计出不对称参数并表明异常值比例解释91%的滤波器改善方差。

Conclusion: 势能门控方法是一种有效的双阱随机动力学系统鲁棒状态估计方法，对噪声和模型误设具有较好鲁棒性，在实际应用中有效果。

Abstract: We introduce potential-energy gating, a method for robust state estimation in systems governed by double-well stochastic dynamics. The observation noise covariance of a Bayesian filter is modulated by the local value of a known or assumed potential energy function: observations are trusted when the state is near a potential minimum and progressively discounted as it approaches the barrier separating metastable wells. This physics-based mechanism differs from purely statistical robust filters, which treat all regions of state space identically, and from constrained filters, which impose hard bounds on states rather than modulating observation trust. We implement the gating within Extended, Unscented, Ensemble, and Adaptive Kalman filters and particle filters, requiring only two additional hyperparameters. Synthetic benchmarks on a Ginzburg-Landau double-well process with 10% outlier contamination and Monte Carlo validation over 100 replications show 57-80% RMSE improvement over the standard Extended Kalman Filter, all statistically significant (p < 10^{-15}, Wilcoxon signed-rank test). A naive topological baseline using only distance to the nearest well achieves 57%, confirming that the continuous energy landscape adds an additional ~21 percentage points. The method is robust to misspecification: even when assumed potential parameters deviate by 50% from their true values, improvement never falls below 47%. Comparing externally forced and spontaneous Kramers-type transitions, gating retains 68% improvement under noise-induced transitions whereas the naive baseline degrades to 30%. As an empirical illustration, we apply the framework to Dansgaard-Oeschger events in the NGRIP delta-18O ice-core record, estimating asymmetry parameter gamma = -0.109 (bootstrap 95% CI: [-0.220, -0.011], excluding zero) and demonstrating that outlier fraction explains 91% of the variance in filter improvement.

</details>


### [127] [Bootstrapping-based Regularisation for Reducing Individual Prediction Instability in Clinical Risk Prediction Models](https://arxiv.org/abs/2602.11360)
*Sara Matijevic,Christopher Yau*

Main category: cs.LG

TL;DR: 本文提出基于自助法的正则化框架，在多个数据集上验证其能提高预测稳定性，且不牺牲可解释性，为医疗场景提供可靠深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的临床预测模型不稳定，影响可靠性和临床应用，需提高稳定性。

Method: 提出将自助法过程直接嵌入深度神经网络训练的正则化框架，约束重采样数据集的预测变异性。

Result: 在多个数据集上，模型预测稳定性提高，平均绝对差异降低，显著偏差预测减少，保持了判别性能和特征重要性一致性。集成模型虽更稳定但牺牲可解释性。

Conclusion: 该方法能开发出更鲁棒、可重复且不牺牲可解释性的预测模型，对数据有限的医疗场景有价值。

Abstract: Clinical prediction models are increasingly used to support patient care, yet many deep learning-based approaches remain unstable, as their predictions can vary substantially when trained on different samples from the same population. Such instability undermines reliability and limits clinical adoption. In this study, we propose a novel bootstrapping-based regularisation framework that embeds the bootstrapping process directly into the training of deep neural networks. This approach constrains prediction variability across resampled datasets, producing a single model with inherent stability properties. We evaluated models constructed using the proposed regularisation approach against conventional and ensemble models using simulated data and three clinical datasets: GUSTO-I, Framingham, and SUPPORT. Across all datasets, our model exhibited improved prediction stability, with lower mean absolute differences (e.g., 0.019 vs. 0.059 in GUSTO-I; 0.057 vs. 0.088 in Framingham) and markedly fewer significantly deviating predictions. Importantly, discriminative performance and feature importance consistency were maintained, with high SHAP correlations between models (e.g., 0.894 for GUSTO-I; 0.965 for Framingham). While ensemble models achieved greater stability, we show that this came at the expense of interpretability, as each constituent model used predictors in different ways. By regularising predictions to align with bootstrapped distributions, our approach allows prediction models to be developed that achieve greater robustness and reproducibility without sacrificing interpretability. This method provides a practical route toward more reliable and clinically trustworthy deep learning models, particularly valuable in data-limited healthcare settings.

</details>


### [128] [AM-FM: A Foundation Model for Ambient Intelligence Through WiFi](https://arxiv.org/abs/2602.11200)
*Guozhen Zhu,Yuqian Hu,Sakila Jayaweera,Weihang Gao,Wei-Hsiang Wang,Jiaxuan Zhang,Beibei Wang,Chenshu Wu,K. J. Ray Liu*

Main category: cs.LG

TL;DR: 提出首个通过WiFi进行环境智能感知的基础模型AM - FM，用大量未标注数据预训练，在多个下游任务中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有无线感知依赖特定任务模型，需大量标注数据且限制实际部署，WiFi潜力未充分挖掘。

Method: 在920万个未标注的信道状态信息样本上用对比学习、掩码重建和物理信息目标进行预训练。

Result: 在九个下游任务的公共基准测试中，AM - FM表现出强大的跨任务性能和更高的数据效率。

Conclusion: 基础模型可利用现有无线基础设施实现可扩展的环境智能。

Abstract: Ambient intelligence, continuously understanding human presence, activity, and physiology in physical spaces, is fundamental to smart environments, health monitoring, and human-computer interaction. WiFi infrastructure provides a ubiquitous, always-on, privacy-preserving substrate for this capability across billions of IoT devices. Yet this potential remains largely untapped, as wireless sensing has typically relied on task-specific models that require substantial labeled data and limit practical deployment. We present AM-FM, the first foundation model for ambient intelligence and sensing through WiFi. AM-FM is pre-trained on 9.2 million unlabeled Channel State Information (CSI) samples collected over 439 days from 20 commercial device types deployed worldwide, learning general-purpose representations via contrastive learning, masked reconstruction, and physics-informed objectives tailored to wireless signals. Evaluated on public benchmarks spanning nine downstream tasks, AM-FM shows strong cross-task performance with improved data efficiency, demonstrating that foundation models can enable scalable ambient intelligence using existing wireless infrastructure.

</details>


### [129] [Zero-Sacrifice Persistent-Robustness Adversarial Defense for Pre-Trained Encoders](https://arxiv.org/abs/2602.11204)
*Zhuxin Lei,Ziyuan Yang,Yi Zhang*

Main category: cs.LG

TL;DR: 提出Zero - Sacrifice Persistent - Robustness Adversarial Defense (ZePAD)防御方法抵御下游无关对抗样本，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法依赖任务特定对抗微调，存在泛化性差、灾难性遗忘和良性性能下降问题，需更严格防御目标。

Method: 提出ZePAD，采用双分支结构，包括MPAE - Branch增强对抗抵抗，BMP - Branch保证良性性能，通过评估分支置信度检测DAEs。

Result: 在11种SSL方法和6个数据集上实验，某些情况下良性性能提升29.20%，对抗鲁棒性提升73.86%。

Conclusion: ZePAD能通过一次对抗微调抵御下游任务DAEs，实现持久鲁棒性，具有零牺牲特性。

Abstract: The widespread use of publicly available pre-trained encoders from self-supervised learning (SSL) has exposed a critical vulnerability: their susceptibility to downstream-agnostic adversarial examples (DAEs), which are crafted without knowledge of the downstream tasks but capable of misleading downstream models. While several defense methods have been explored recently, they rely primarily on task-specific adversarial fine-tuning, which inevitably limits generalizability and causes catastrophic forgetting and deteriorates benign performance. Different with previous works, we propose a more rigorous defense goal that requires only a single tuning for diverse downstream tasks to defend against DAEs and preserve benign performance. To achieve this defense goal, we introduce Zero-Sacrifice Persistent-Robustness Adversarial Defense (ZePAD), which is inspired by the inherent sensitivity of neural networks to data characteristics. Specifically, ZePAD is a dual-branch structure, which consists of a Multi-Pattern Adversarial Enhancement Branch (MPAE-Branch) that uses two adversarially fine-tuned encoders to strengthen adversarial resistance. The Benign Memory Preservation Branch (BMP-Branch) is trained on local data to ensure adversarial robustness does not compromise benign performance. Surprisingly, we find that ZePAD can directly detect DAEs by evaluating branch confidence, without introducing any adversarial exsample identification task during training. Notably, by enriching feature diversity, our method enables a single adversarial fine-tuning to defend against DAEs across downstream tasks, thereby achieving persistent robustness. Extensive experiments on 11 SSL methods and 6 datasets validate its effectiveness. In certain cases, it achieves a 29.20% improvement in benign performance and a 73.86% gain in adversarial robustness, highlighting its zero-sacrifice property.

</details>


### [130] [UltraLIF: Fully Differentiable Spiking Neural Networks via Ultradiscretization and Max-Plus Algebra](https://arxiv.org/abs/2602.11206)
*Jose Marie Antonio Miñoza*

Main category: cs.LG

TL;DR: 本文提出UltraLIF框架，用超离散化替代代理梯度，推导两种神经元模型，理论分析有收敛性，实验显示优于代理梯度基线。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络因脉冲生成不可微依赖启发式代理梯度，需更好方法。

Method: 引入UltraLIF框架，用超离散化替代代理梯度，从不同动力学系统推导神经元模型，通过标准反向传播训练。

Result: 理论分析有收敛性和有界非零梯度，实验在六个基准上优于代理梯度基线，可选稀疏惩罚可降低能耗。

Conclusion: UltraLIF框架有效，能提升脉冲神经网络性能，在单时间步设置和降低能耗方面表现良好。

Abstract: Spiking Neural Networks (SNNs) offer energy-efficient, biologically plausible computation but suffer from non-differentiable spike generation, necessitating reliance on heuristic surrogate gradients. This paper introduces UltraLIF, a principled framework that replaces surrogate gradients with ultradiscretization, a mathematical formalism from tropical geometry providing continuous relaxations of discrete dynamics. The central insight is that the max-plus semiring underlying ultradiscretization naturally models neural threshold dynamics: the log-sum-exp function serves as a differentiable soft-maximum that converges to hard thresholding as a learnable temperature parameter $\eps \to 0$. Two neuron models are derived from distinct dynamical systems: UltraLIF from the LIF ordinary differential equation (temporal dynamics) and UltraDLIF from the diffusion equation modeling gap junction coupling across neuronal populations (spatial dynamics). Both yield fully differentiable SNNs trainable via standard backpropagation with no forward-backward mismatch. Theoretical analysis establishes pointwise convergence to classical LIF dynamics with quantitative error bounds and bounded non-vanishing gradients. Experiments on six benchmarks spanning static images, neuromorphic vision, and audio demonstrate improvements over surrogate gradient baselines, with gains most pronounced in single-timestep ($T{=}1$) settings on neuromorphic and temporal datasets. An optional sparsity penalty enables significant energy reduction while maintaining competitive accuracy.

</details>


### [131] [Adaptive Physics Transformer with Fused Global-Local Attention for Subsurface Energy Systems](https://arxiv.org/abs/2602.11208)
*Xin Ju,Nok Hei,Fung,Yuyan Zhang,Carl Jacquemyn,Matthew Jackson,Randolph Settgast,Sally M. Benson,Gege Wen*

Main category: cs.LG

TL;DR: 提出Adaptive Physics Transformer (APT)解决地球地下系统全物理数值模拟计算成本高的问题，其表现优于现有架构，可用于跨数据集学习和大规模地下基础模型开发。


<details>
  <summary>Details</summary>
Motivation: 地球地下系统全物理数值模拟因地质异质性、高分辨率要求和物理过程耦合等问题计算成本高。

Method: 提出几何、网格和物理无关的神经算子APT，融合基于图的编码器和全局注意力机制。

Result: APT在规则和不规则网格的地下任务中表现优于现有架构，具有强大超分辨率能力，可从自适应网格细化模拟中直接学习，能进行跨数据集学习。

Conclusion: APT是开发大规模地下基础模型的强大且可扩展的骨干架构。

Abstract: The Earth's subsurface is a cornerstone of modern society, providing essential energy resources like hydrocarbons, geothermal, and minerals while serving as the primary reservoir for $CO_2$ sequestration. However, full physics numerical simulations of these systems are notoriously computationally expensive due to geological heterogeneity, high resolution requirements, and the tight coupling of physical processes with distinct propagation time scales. Here we propose the \textbf{Adaptive Physics Transformer} (APT), a geometry-, mesh-, and physics-agnostic neural operator that explicitly addresses these challenges. APT fuses a graph-based encoder to extract high-resolution local heterogeneous features with a global attention mechanism to resolve long-range physical impacts. Our results demonstrate that APT outperforms state-of-the-art architectures in subsurface tasks across both regular and irregular grids with robust super-resolution capabilities. Notably, APT is the first architecture that directly learns from adaptive mesh refinement simulations. We also demonstrate APT's capability for cross-dataset learning, positioning it as a robust and scalable backbone for large-scale subsurface foundation model development.

</details>


### [132] [MUSE: Multi-Tenant Model Serving With Seamless Model Updates](https://arxiv.org/abs/2602.11776)
*Cláudio Correia,Alberto E. A. Ferreira,Lucas Martins,Miguel P. Bento,Sofia Guerreiro,Ricardo Ribeiro Pereira,Ana Sofia Gomes,Jacopo Bono,Hugo Ferreira,Pedro Bizarro*

Main category: cs.LG

TL;DR: 介绍了MUSE模型服务框架，可在多租户环境中实现无缝模型更新，降低模型更新时间，节省成本。


<details>
  <summary>Details</summary>
Motivation: 在二进制分类系统中，模型重训练会使分数分布变化，导致现有阈值失效，多租户环境中重新校准阈值存在严重瓶颈，需要新方法解决。

Method: 引入MUSE框架，通过将模型分数与客户决策边界解耦实现无缝更新，采用动态基于意图的路由共享模型，结合两级分数转换将模型输出映射到稳定参考分布。

Result: Feedzai大规模部署后，每秒处理超千个事件，过去12个月处理超550亿个事件，跨数十个租户，保持高可用性和低延迟。

Conclusion: MUSE将模型前置时间从数周缩短至数分钟，增强模型抗攻击能力，节省数百万美元欺诈损失和运营成本。

Abstract: In binary classification systems, decision thresholds translate model scores into actions. Choosing suitable thresholds relies on the specific distribution of the underlying model scores but also on the specific business decisions of each client using that model. However, retraining models inevitably shifts score distributions, invalidating existing thresholds. In multi-tenant Score-as-a-Service environments, where decision boundaries reside in client-managed infrastructure, this creates a severe bottleneck: recalibration requires coordinating threshold updates across hundreds of clients, consuming excessive human hours and leading to model stagnation. We introduce MUSE, a model serving framework that enables seamless model updates by decoupling model scores from client decision boundaries. Designed for multi-tenancy, MUSE optimizes infrastructure re-use by sharing models via dynamic intent-based routing, combined with a two-level score transformation that maps model outputs to a stable, reference distribution. Deployed at scale by Feedzai, MUSE processes over a thousand events per second, and over 55 billion events in the last 12 months, across several dozens of tenants, while maintaining high-availability and low-latency guarantees. By reducing model lead time from weeks to minutes, MUSE promotes model resilience against shifting attacks, saving millions of dollars in fraud losses and operational costs.

</details>


### [133] [Learning Conditional Averages](https://arxiv.org/abs/2602.11920)
*Marco Bressan,Nataly Brukhim,Nicolo Cesa-Bianchi,Emmanuel Esposito,Yishay Mansour,Shay Moran,Maximilian Thiessen*

Main category: cs.LG

TL;DR: 本文在PAC框架下引入学习条件均值的问题，将PAC学习扩展到多个领域，给出条件均值可学习性的完整刻画及样本复杂度界。


<details>
  <summary>Details</summary>
Motivation: 将标准PAC学习扩展到能涵盖可解释性、公平性和推荐系统等多个领域的学习任务。

Method: 引入学习条件均值的问题，通过分析两个与概念类和邻域系统相关的组合参数的联合有限性进行研究。

Result: 得到条件均值可学习性的完整刻画，以及在对数因子范围内的紧样本复杂度界。

Conclusion: 学习条件均值问题扩展了PAC学习，其可学习性取决于两个组合参数的联合有限性。

Abstract: We introduce the problem of learning conditional averages in the PAC framework. The learner receives a sample labeled by an unknown target concept from a known concept class, as in standard PAC learning. However, instead of learning the target concept itself, the goal is to predict, for each instance, the average label over its neighborhood -- an arbitrary subset of points that contains the instance. In the degenerate case where all neighborhoods are singletons, the problem reduces exactly to classic PAC learning. More generally, it extends PAC learning to a setting that captures learning tasks arising in several domains, including explainability, fairness, and recommendation systems. Our main contribution is a complete characterization of when conditional averages are learnable, together with sample complexity bounds that are tight up to logarithmic factors. The characterization hinges on the joint finiteness of two novel combinatorial parameters, which depend on both the concept class and the neighborhood system, and are closely related to the independence number of the associated neighborhood graph.

</details>


### [134] [Towards Compressive and Scalable Recurrent Memory](https://arxiv.org/abs/2602.11212)
*Yunchong Song,Jushi Kai,Liming Lu,Kaixi Qiu,Zhouhan Lin*

Main category: cs.LG

TL;DR: 提出Elastic Memory内存架构用于Transformer处理长文本，在多领域数据集上超越基线模型且性能更佳，设计灵活。


<details>
  <summary>Details</summary>
Motivation: 解决Transformers在处理长上下文时注意力的二次瓶颈以及现有引入循环内存方法在理论原则和实际可扩展性间的权衡问题。

Method: 引入基于HiPPO框架的Elastic Memory，将历史序列视为连续信号样本，用最优在线压缩编码为固定大小内存状态，用多项式采样机制检索。

Result: 在三个领域的长上下文(32k+)数据集上均超越基线模型，在参数相同或模型扩展时表现优秀，速度更快。

Conclusion: Elastic Memory有效解决长上下文处理问题，解耦设计可在测试时注入归纳偏置提升性能。

Abstract: Transformers face a quadratic bottleneck in attention when scaling to long contexts. Recent approaches introduce recurrent memory to extend context beyond the current window, yet these often face a fundamental trade-off between theoretical principles and practical scalability. To address this, we introduce Elastic Memory, a novel memory architecture grounded in the HiPPO framework for online function approximation. Elastic Memory treats historical sequence as samples from continuous signals, applying optimal online compression to encode them into a fixed-size memory state. For retrieval, we propose a flexible \textit{polynomial sampling} mechanism that reconstructs a history summary from this compressed state. Elastic Memory consistently outperformed baselines on long-context (32k+) datasets across three domains. With equal parameters, it beat Memorizing Transformer by 16x memory and outperformed Melodi at all memory sizes, even when Melodi had 30% more parameters. When scaling model size, Elastic Memory stayed ahead of all baselines and was significantly faster than Melodi at 4x size. Furthermore, its decoupled design allows for injecting inductive biases at test-time to boost performance.

</details>


### [135] [PrefillShare: A Shared Prefill Module for KV Reuse in Multi-LLM Disaggregated Serving](https://arxiv.org/abs/2602.12029)
*Sunghyeon Woo,Hoseung Kim,Sunghwan Shim,Minjung Jo,Hyunjoon Jeong,Jeongtae Lee,Joonghoon Kim,Sungjae Lee,Baeseong Park,Se Jung Kwon,Dongsoo Lee*

Main category: cs.LG

TL;DR: 提出 PrefillShare 算法解决多智能体系统中多语言模型执行时的冗余问题，在多模型代理工作负载中降低延迟、提高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统中多个语言模型的执行模式存在冗余计算和存储问题，增加负载和延迟。

Method: 提出 PrefillShare 算法，将模型分解为预填充和解码模块，冻结预填充模块，仅微调解码模块，并引入路由机制。

Result: PrefillShare 在广泛任务和模型上达到全微调的准确率，在多模型代理工作负载中 p95 延迟降低 4.5 倍，吞吐量提高 3.9 倍。

Conclusion: PrefillShare 能有效解决多智能体多语言模型执行中的冗余问题，提升性能。

Abstract: Multi-agent systems increasingly orchestrate multiple specialized language models to solve complex real-world problems, often invoking them over a shared context. This execution pattern repeatedly processes the same prompt prefix across models. Consequently, each model redundantly executes the prefill stage and maintains its own key-value (KV) cache, increasing aggregate prefill load and worsening tail latency by intensifying prefill-decode interference in existing LLM serving stacks. Disaggregated serving reduces such interference by placing prefill and decode on separate GPUs, but disaggregation does not fundamentally eliminate inter-model redundancy in computation and KV storage for the same prompt. To address this issue, we propose PrefillShare, a novel algorithm that enables sharing the prefill stage across multiple models in a disaggregated setting. PrefillShare factorizes the model into prefill and decode modules, freezes the prefill module, and fine-tunes only the decode module. This design allows multiple task-specific models to share a prefill module and the KV cache generated for the same prompt. We further introduce a routing mechanism that enables effective prefill sharing across heterogeneous models in a vLLM-based disaggregated system. PrefillShare not only matches full fine-tuning accuracy on a broad range of tasks and models, but also delivers 4.5x lower p95 latency and 3.9x higher throughput in multi-model agent workloads.

</details>


### [136] [Charting Empirical Laws for LLM Fine-Tuning in Scientific Multi-Discipline Learning](https://arxiv.org/abs/2602.11215)
*Lintao Wang,Zhuqiang Lu,Yilin Zhu,Kun Hu,Zhenfei Yin,Shixiang Tang,Zhiyong Wang,Wanli Ouyang,Xinzhu Ma*

Main category: cs.LG

TL;DR: 本文对多学科大语言模型微调进行系统研究，构建五学科语料库分析不同微调方法学习模式，总结出四条经验法则，为多学科微调及通用科学大语言模型开发提供指导。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在单学科微调表现良好，但多学科情境下学习动态仍不清楚，期望通过跨领域知识协同提升泛化能力和适用性。

Method: 构建五学科语料库，分析全微调、LoRA、LoRA - MoE和LoRA组合的学习模式。

Result: 多学科学习比单学科训练更具可变性，总结出四条经验法则，即Balance - then - Diversity、Merge - then - Align、Optimize - then - Scale、Share - then - Specialize。

Conclusion: 这些法则为多学科微调提供实用方案，为开发可泛化的科学大语言模型提供可行指导。

Abstract: While large language models (LLMs) have achieved strong performance through fine-tuning within individual scientific domains, their learning dynamics in multi-disciplinary contexts remains poorly understood, despite the promise of improved generalization and broader applicability through cross-domain knowledge synergy. In this work, we present the first systematic study of multi-disciplinary LLM fine-tuning, constructing a five-discipline corpus and analyzing learning patterns of full fine-tuning, LoRA, LoRA-MoE, and LoRA compositions. Particularly, our study shows that multi-disciplinary learning is substantially more variable than single-discipline training and distills four consistent empirical laws: (1) Balance-then-Diversity: low-resource disciplines degrade performance unless mitigated via diversity-aware upsampling; (2) Merge-then-Align: restoring instruction-following ability is critical for cross-discipline synergy; (3) Optimize-then-Scale: parameter scaling offers limited gains without prior design optimization; and (4) Share-then-Specialize: asymmetric LoRA-MoE yields robust gains with minimal trainable parameters via shared low-rank projection. Together, these laws form a practical recipe for principled multi-discipline fine-tuning and provide actionable guidance for developing generalizable scientific LLMs.

</details>


### [137] [Protein Language Model Embeddings Improve Generalization of Implicit Transfer Operators](https://arxiv.org/abs/2602.11216)
*Panagiotis Antoniadis,Beatrice Pavesi,Simon Olsson,Ole Winther*

Main category: cs.LG

TL;DR: 本文指出传统分子动力学计算成本高，生成式分子动力学可转移性有限，提出结合辅助信息改进TITO，PLaTITO在蛋白质系统平衡采样基准测试中表现优异，并研究额外条件信号对模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 传统分子动力学计算成本高，生成式分子动力学可转移性有限，需改进方法提高数据效率和泛化能力。

Method: 结合辅助信息改进可转移隐式转移算子（TITO），使用粗粒度TITO模型并融入蛋白质语言模型（pLM）嵌入。

Result: 粗粒度TITO模型比玻尔兹曼模拟器更具数据效率，融入pLM嵌入进一步提高分布外泛化能力，PLaTITO在分布外蛋白质系统平衡采样基准测试中达到了最先进的性能。

Conclusion: 结合辅助信息能提高分子动力学TITO的数据效率和泛化能力，额外条件信号对模型性能有影响。

Abstract: Molecular dynamics (MD) is a central computational tool in physics, chemistry, and biology, enabling quantitative prediction of experimental observables as expectations over high-dimensional molecular distributions such as Boltzmann distributions and transition densities. However, conventional MD is fundamentally limited by the high computational cost required to generate independent samples. Generative molecular dynamics (GenMD) has recently emerged as an alternative, learning surrogates of molecular distributions either from data or through interaction with energy models. While these methods enable efficient sampling, their transferability across molecular systems is often limited. In this work, we show that incorporating auxiliary sources of information can improve the data efficiency and generalization of transferable implicit transfer operators (TITO) for molecular dynamics. We find that coarse-grained TITO models are substantially more data-efficient than Boltzmann Emulators, and that incorporating protein language model (pLM) embeddings further improves out-of-distribution generalization. Our approach, PLaTITO, achieves state-of-the-art performance on equilibrium sampling benchmarks for out-of-distribution protein systems, including fast-folding proteins. We further study the impact of additional conditioning signals -- such as structural embeddings, temperature, and large-language-model-derived embeddings -- on model performance.

</details>


### [138] [The Magic Correlations: Understanding Knowledge Transfer from Pretraining to Supervised Fine-Tuning](https://arxiv.org/abs/2602.11217)
*Simin Fan,Dimitris Paparas,Natasha Noy,Binbin Xiong,Noveen Sachdeva,Berivan Isik*

Main category: cs.LG

TL;DR: 研究语言模型能力从预训练到监督微调的转移，通过实验揭示转移可靠性因能力类别、基准和规模而异，为模型开发等提供指导。


<details>
  <summary>Details</summary>
Motivation: 理解语言模型能力从预训练到监督微调的转移对高效模型开发和数据管理至关重要。

Method: 通过一系列相关协议，对不同数据混合和模型规模下的准确性和置信度指标进行分析。

Result: 转移可靠性在能力类别、基准和规模上差异巨大，准确性和置信度有不同的缩放动态。

Conclusion: 研究揭示了预训练决策和下游结果之间的复杂相互作用，为基准选择、数据管理和高效模型开发提供了可操作的指导。

Abstract: Understanding how language model capabilities transfer from pretraining to supervised fine-tuning (SFT) is fundamental to efficient model development and data curation. In this work, we investigate four core questions: RQ1. To what extent do accuracy and confidence rankings established during pretraining persist after SFT? RQ2. Which benchmarks serve as robust cross-stage predictors and which are unreliable? RQ3. How do transfer dynamics shift with model scale? RQ4. How well does model confidence align with accuracy, as a measure of calibration quality? Does this alignment pattern transfer across training stages? We address these questions through a suite of correlation protocols applied to accuracy and confidence metrics across diverse data mixtures and model scales. Our experiments reveal that transfer reliability varies dramatically across capability categories, benchmarks, and scales -- with accuracy and confidence exhibiting distinct, sometimes opposing, scaling dynamics. These findings shed light on the complex interplay between pretraining decisions and downstream outcomes, providing actionable guidance for benchmark selection, data curation, and efficient model development.

</details>


### [139] [Empirical Gaussian Processes](https://arxiv.org/abs/2602.12082)
*Jihao Andreas Lin,Sebastian Ament,Louis C. Tiao,David Eriksson,Maximilian Balandat,Eytan Bakshy*

Main category: cs.LG

TL;DR: 研究经验高斯过程，从历史数据估计均值和协方差函数，理论有收敛性，并有算法可处理不同数据集，在学习曲线外推和时间序列预测有竞争力。


<details>
  <summary>Details</summary>
Motivation: 高斯过程实践中效果受核函数选择限制，传统核函数需专家知识、适应性有限、假设强。

Method: 从历史观测数据中经验性估计均值和协方差函数；将从独立数据集学习GP先验问题转化为似然估计，并推导期望最大化算法。

Result: 经验高斯过程在学习曲线外推和时间序列预测基准测试中取得有竞争力的表现。

Conclusion: 经验高斯过程构建了灵活、数据驱动的GP先验，克服了传统高斯过程的局限性。

Abstract: Gaussian processes (GPs) are powerful and widely used probabilistic regression models, but their effectiveness in practice is often limited by the choice of kernel function. This kernel function is typically handcrafted from a small set of standard functions, a process that requires expert knowledge, results in limited adaptivity to data, and imposes strong assumptions on the hypothesis space. We study Empirical GPs, a principled framework for constructing flexible, data-driven GP priors that overcome these limitations. Rather than relying on standard parametric kernels, we estimate the mean and covariance functions empirically from a corpus of historical observations, enabling the prior to reflect rich, non-trivial covariance structures present in the data. Theoretically, we show that the resulting model converges to the GP that is closest (in KL-divergence sense) to the real data generating process. Practically, we formulate the problem of learning the GP prior from independent datasets as likelihood estimation and derive an Expectation-Maximization algorithm with closed-form updates, allowing the model handle heterogeneous observation locations across datasets. We demonstrate that Empirical GPs achieve competitive performance on learning curve extrapolation and time series forecasting benchmarks.

</details>


### [140] [Credal Concept Bottleneck Models: Structural Separation of Epistemic and Aleatoric Uncertainty](https://arxiv.org/abs/2602.11219)
*Tanmoy Mukherjee,Marius Kloft,Pierre Marquis,Zied Bouraoui*

Main category: cs.LG

TL;DR: 本文提出用credal - set公式化方法分离认知不确定性和偶然不确定性，在多标注基准上效果良好。


<details>
  <summary>Details</summary>
Motivation: 多数方法从相同预测分布估计认知和偶然不确定性，二者估计通常强相关，模糊了语义，影响可靠决策。

Method: 提出credal - set公式化方法，在变分可信概念瓶颈模型中用两个不相交的不确定性头，通过不相交的目标和非重叠的梯度路径进行训练。

Result: 与标准方法相比，该方法在多标注基准上使认知和偶然不确定性的相关性降低一个数量级以上，同时改善了认知不确定性与预测误差、偶然不确定性与真实模糊性的一致性。

Conclusion: 所提方法能有效分离认知和偶然不确定性，提升决策可靠性。

Abstract: Decomposing predictive uncertainty into epistemic (model ignorance) and aleatoric (data ambiguity) components is central to reliable decision making, yet most methods estimate both from the same predictive distribution. Recent empirical and theoretical results show these estimates are typically strongly correlated, so changes in predictive spread simultaneously affect both components and blur their semantics. We propose a credal-set formulation in which uncertainty is represented as a set of predictive distributions, so that epistemic and aleatoric uncertainty correspond to distinct geometric properties: the size of the set versus the noise within its elements. We instantiate this idea in a Variational Credal Concept Bottleneck Model with two disjoint uncertainty heads trained by disjoint objectives and non-overlapping gradient paths, yielding separation by construction rather than post hoc decomposition. Across multi-annotator benchmarks, our approach reduces the correlation between epistemic and aleatoric uncertainty by over an order of magnitude compared to standard methods, while improving the alignment of epistemic uncertainty with prediction error and aleatoric uncertainty with ground-truth ambiguity.

</details>


### [141] [On the Complexity of Offline Reinforcement Learning with $Q^\star$-Approximation and Partial Coverage](https://arxiv.org/abs/2602.12107)
*Haolin Liu,Braham Snyder,Chen-Yu Wei*

Main category: cs.LG

TL;DR: 研究Q*-近似和部分覆盖下离线强化学习，给出信息论下界，引入通用框架，有多项改进和首次分析。


<details>
  <summary>Details</summary>
Motivation: 回答Q*-可实现性和Bellman完备性在部分覆盖下对样本高效离线RL是否充分的开放问题，该设置有实际算法但理论关注少。

Method: 建立信息论下界，引入受在线RL模型无关决策估计系数启发的通用框架，开发二阶性能差异引理。

Result: 否定回答开放问题；框架恢复并改进已有保证的相关量；软Q学习样本复杂度从ε⁻⁴提升到ε⁻²；去除额外在线交互需求；首次刻画无Bellman完备性的一般低Bellman秩MDP的离线可学习性；首次对非表格情形下CQL分析。

Conclusion: 提出的框架和方法在离线强化学习多个方面有改进和拓展，推动该领域发展。

Abstract: We study offline reinforcement learning under $Q^\star$-approximation and partial coverage, a setting that motivates practical algorithms such as Conservative $Q$-Learning (CQL; Kumar et al., 2020) but has received limited theoretical attention. Our work is inspired by the following open question: "Are $Q^\star$-realizability and Bellman completeness sufficient for sample-efficient offline RL under partial coverage?"
  We answer in the negative by establishing an information-theoretic lower bound. Going substantially beyond this, we introduce a general framework that characterizes the intrinsic complexity of a given $Q^\star$ function class, inspired by model-free decision-estimation coefficients (DEC) for online RL (Foster et al., 2023b; Liu et al., 2025b). This complexity recovers and improves the quantities underlying the guarantees of Chen and Jiang (2022) and Uehara et al. (2023), and extends to broader settings. Our decision-estimation decomposition can be combined with a wide range of $Q^\star$ estimation procedures, modularizing and generalizing existing approaches.
  Beyond the general framework, we make further contributions: By developing a novel second-order performance difference lemma, we obtain the first $ε^{-2}$ sample complexity under partial coverage for soft $Q$-learning, improving the $ε^{-4}$ bound of Uehara et al. (2023). We remove Chen and Jiang's (2022) need for additional online interaction when the value gap of $Q^\star$ is unknown. We also give the first characterization of offline learnability for general low-Bellman-rank MDPs without Bellman completeness (Jiang et al., 2017; Du et al., 2021; Jin et al., 2021), a canonical setting in online RL that remains unexplored in offline RL except for special cases. Finally, we provide the first analysis for CQL under $Q^\star$-realizability and Bellman completeness beyond the tabular case.

</details>


### [142] [Patch the Distribution Mismatch: RL Rewriting Agent for Stable Off-Policy SFT](https://arxiv.org/abs/2602.11220)
*Jiacheng Wang,Ping Jian,Zhen Yang,Zirong Chen,Keren Liao,Zhongbin Guo*

Main category: cs.LG

TL;DR: 本文提出基于强化学习的数据重写代理解决大语言模型微调时的灾难性遗忘问题，实验显示可减少非下游基准遗忘。


<details>
  <summary>Details</summary>
Motivation: 大语言模型下游微调时数据分布偏移会导致灾难性遗忘，现有数据重写方法存在目标与模型自然问答生成分布不匹配、多样性坍塌问题。

Method: 将数据重写视为策略学习问题，用强化学习优化重写分布，提出基于强化学习的数据重写代理。

Result: 方法在下游任务增益与标准监督微调相当，平均减少非下游基准遗忘12.34%。

Conclusion: 所提方法能构建高质量重写数据集用于下游微调，减少灾难性遗忘。

Abstract: Large language models (LLMs) have made rapid progress, yet adapting them to downstream scenarios still commonly relies on supervised fine-tuning (SFT). When downstream data exhibit a substantial distribution shift from the model's prior training distribution, SFT can induce catastrophic forgetting. To narrow this gap, data rewriting has been proposed as a data-centric approach that rewrites downstream training data prior to SFT. However, existing methods typically sample rewrites from a prompt-induced conditional distribution, so the resulting targets are not necessarily aligned with the model's natural QA-style generation distribution. Moreover, reliance on fixed templates can lead to diversity collapse. To address these issues, we cast data rewriting as a policy learning problem and learn a rewriting policy that better matches the backbone's QA-style generation distribution while preserving diversity. Since distributional alignment, diversity and task consistency are automatically evaluable but difficult to optimize end-to-end with differentiable objectives, we leverage reinforcement learning to optimize the rewrite distribution under reward feedback and propose an RL-based data-rewriting agent. The agent jointly optimizes QA-style distributional alignment and diversity under a hard task-consistency gate, thereby constructing a higher-quality rewritten dataset for downstream SFT. Extensive experiments show that our method achieves downstream gains comparable to standard SFT while reducing forgetting on non-downstream benchmarks by 12.34% on average. Our code is available at https://anonymous.4open.science/r/Patch-the-Prompt-Gap-4112 .

</details>


### [143] [Learning Glioblastoma Tumor Heterogeneity Using Brain Inspired Topological Neural Networks](https://arxiv.org/abs/2602.11234)
*Ankita Paul,Wenyi Wang*

Main category: cs.LG

TL;DR: 提出TopoGBM学习框架，从多参数3D MRI捕获异质性保留、扫描仪鲁棒的表征，在多个队列表现良好，证明结合拓扑先验可学习形态忠实嵌入。


<details>
  <summary>Details</summary>
Motivation: GBM精确预后受肿瘤异质性和MRI采集协议不一致影响，传统方法难以捕捉多尺度形态多样性。

Method: 提出TopoGBM框架，用拓扑正则化的3D卷积自动编码器，在压缩潜在空间保留肿瘤流形的复杂非欧不变量。

Result: 在多个队列评估中，TopoGBM表现优于基线模型，重建残差高度定位在病理异质性区域，约50%预后信号定位在肿瘤及周围微环境。

Conclusion: 结合拓扑先验能学习捕获肿瘤异质性且保持跨机构鲁棒性的形态忠实嵌入。

Abstract: Accurate prognosis for Glioblastoma (GBM) using deep learning (DL) is hindered by extreme spatial and structural heterogeneity. Moreover, inconsistent MRI acquisition protocols across institutions hinder generalizability of models. Conventional transformer and DL pipelines often fail to capture the multi-scale morphological diversity such as fragmented necrotic cores, infiltrating margins, and disjoint enhancing components leading to scanner-specific artifacts and poor cross-site prognosis. We propose TopoGBM, a learning framework designed to capture heterogeneity-preserved, scanner-robust representations from multi-parametric 3D MRI. Central to our approach is a 3D convolutional autoencoder regularized by a topological regularization that preserves the complex, non-Euclidean invariants of the tumor's manifold within a compressed latent space. By enforcing these topological priors, TopoGBM explicitly models the high-variance structural signatures characteristic of aggressive GBM. Evaluated across heterogeneous cohorts (UPENN, UCSF, RHUH) and external validation on TCGA, TopoGBM achieves better performance (C-index 0.67 test, 0.58 validation), outperforming baselines that degrade under domain shift. Mechanistic interpretability analysis reveals that reconstruction residuals are highly localized to pathologically heterogeneous zones, with tumor-restricted and healthy tissue error significantly low (Test: 0.03, Validation: 0.09). Furthermore, occlusion-based attribution localizes approximately 50% of the prognostic signal to the tumor and the diverse peritumoral microenvironment advocating clinical reliability of the unsupervised learning method. Our findings demonstrate that incorporating topological priors enables the learning of morphology-faithful embeddings that capture tumor heterogeneity while maintaining cross-institutional robustness.

</details>


### [144] [AI-Driven Clinical Decision Support System for Enhanced Diabetes Diagnosis and Management](https://arxiv.org/abs/2602.11237)
*Mujeeb Ur Rehman,Imran Rehan,Sohail Khalid*

Main category: cs.LG

TL;DR: 研究评估用于诊断2型糖尿病的AI - CDSS，结果显示其准确率高，或可成诊断有用工具。


<details>
  <summary>Details</summary>
Motivation: 识别2型糖尿病有挑战，AI - CDSS可助力医疗人员准确诊断，本研究旨在评估特定的AI - CDSS。

Method: 采用结合专家见解与机器学习技术的混合方法，用1298例患者数据开发和测试AI - CDSS，利用关键特征预测，开展105例患者的临床试点研究。

Result: AI - CDSS预测准确率高，测试数据集里与内分泌专家一致性达98.8%，试点研究中与糖尿病专家一致性达98.5%，远超非内分泌专家的85% 。

Conclusion: AI - CDSS有潜力成为准确识别2型糖尿病的有用工具，尤其在缺乏糖尿病专家的情况下。

Abstract: Identifying type 2 diabetes mellitus can be challenging, particularly for primary care physicians. Clinical decision support systems incorporating artificial intelligence (AI-CDSS) can assist medical professionals in diagnosing type 2 diabetes with high accuracy. This study aimed to assess an AI-CDSS specifically developed for the diagnosis of type 2 diabetes by employing a hybrid approach that integrates expert-driven insights with machine learning techniques. The AI-CDSS was developed (training dataset: n = 650) and tested (test dataset: n = 648) using a dataset of 1298 patients with and without type 2 diabetes. To generate predictions, the algorithm utilized key features such as body mass index, plasma fasting glucose, and hemoglobin A1C. Furthermore, a clinical pilot study involving 105 patients was conducted to assess the diagnostic accuracy of the system in comparison to non-endocrinology specialists. The AI-CDSS showed a high degree of accuracy, with 99.8% accuracy in predicting diabetes, 99.3% in predicting prediabetes, 99.2% in identifying at-risk individuals, and 98.8% in predicting no diabetes. The test dataset revealed a 98.8% agreement between endocrinology specialists and the AI-CDSS. Type 2 diabetes was identified in 45% of 105 individuals in the pilot study. Compared with diabetes specialists, the AI-CDSS scored a 98.5% concordance rate, greatly exceeding that of nonendocrinology specialists, who had an 85% agreement rate. These findings indicate that the AI-CDSS has the potential to be a useful tool for accurately identifying type 2 diabetes, especially in situations in which diabetes specialists are not readily available.

</details>


### [145] [Towards Sustainable Investment Policies Informed by Opponent Shaping](https://arxiv.org/abs/2602.11829)
*Juan Agustin Duque,Razvan Ciuca,Ayoub Echchahed,Hugo Larochelle,Aaron Courville*

Main category: cs.LG

TL;DR: 本文研究InvestESG模拟中的跨期社会困境，应用Advantage Alignment算法影响代理学习，证明策略性塑造学习过程可带来更好结果以助力政策制定。


<details>
  <summary>Details</summary>
Motivation: 应对气候变化需全球协调，但经济主体常重短期利益致社会困境，要研究InvestESG模拟中的社会困境及解决办法。

Method: 对InvestESG出现跨期社会困境的条件进行形式化刻画，推导理论阈值，应用Advantage Alignment算法影响代理学习。

Result: Advantage Alignment能使学习动态偏向合作结果，策略性塑造经济主体学习过程可带来更好结果。

Conclusion: 策略性塑造经济主体学习过程的结果可为政策机制提供信息，使市场激励与长期可持续目标更好对齐。

Abstract: Addressing climate change requires global coordination, yet rational economic actors often prioritize immediate gains over collective welfare, resulting in social dilemmas. InvestESG is a recently proposed multi-agent simulation that captures the dynamic interplay between investors and companies under climate risk. We provide a formal characterization of the conditions under which InvestESG exhibits an intertemporal social dilemma, deriving theoretical thresholds at which individual incentives diverge from collective welfare. Building on this, we apply Advantage Alignment, a scalable opponent shaping algorithm shown to be effective in general-sum games, to influence agent learning in InvestESG. We offer theoretical insights into why Advantage Alignment systematically favors socially beneficial equilibria by biasing learning dynamics toward cooperative outcomes. Our results demonstrate that strategically shaping the learning processes of economic agents can result in better outcomes that could inform policy mechanisms to better align market incentives with long-term sustainability goals.

</details>


### [146] [Evaluating Memory Structure in LLM Agents](https://arxiv.org/abs/2602.11243)
*Alina Shutova,Alexandra Olenina,Ivan Vinogradov,Anton Sinitsin*

Main category: cs.LG

TL;DR: 提出StructMemEval基准测试LLM代理组织长期记忆能力，初始实验显示简单检索增强LLM有困难，现代LLM在无提示时不能识别记忆结构。


<details>
  <summary>Details</summary>
Motivation: 现有长期记忆基准难以分析复杂记忆架构能力，无法测试复杂记忆层次，需新基准。

Method: 提出StructMemEval基准，收集人类通过特定结构组织知识解决的任务。

Result: 简单检索增强LLM难以完成任务，记忆代理在提示下可解决，现代LLM无提示时不识别记忆结构。

Conclusion: 指出LLM训练和记忆框架未来改进的重要方向。

Abstract: Modern LLM-based agents and chat assistants rely on long-term memory frameworks to store reusable knowledge, recall user preferences, and augment reasoning. As researchers create more complex memory architectures, it becomes increasingly difficult to analyze their capabilities and guide future memory designs. Most long-term memory benchmarks focus on simple fact retention, multi-hop recall, and time-based changes. While undoubtedly important, these capabilities can often be achieved with simple retrieval-augmented LLMs and do not test complex memory hierarchies. To bridge this gap, we propose StructMemEval - a benchmark that tests the agent's ability to organize its long-term memory, not just factual recall. We gather a suite of tasks that humans solve by organizing their knowledge in a specific structure: transaction ledgers, to-do lists, trees and others. Our initial experiments show that simple retrieval-augmented LLMs struggle with these tasks, whereas memory agents can reliably solve them if prompted how to organize their memory. However, we also find that modern LLMs do not always recognize the memory structure when not prompted to do so. This highlights an important direction for future improvements in both LLM training and memory frameworks.

</details>


### [147] [How Sampling Shapes LLM Alignment: From One-Shot Optima to Iterative Dynamics](https://arxiv.org/abs/2602.12180)
*Yurong Chen,Yu He,Michael I. Jordan,Fan Yao*

Main category: cs.LG

TL;DR: 研究大语言模型偏好对齐中采样和参考选择的影响，从理论和实验验证相关结论。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型与人类偏好对齐的标准方法中，采样和参考选择的影响缺乏理论理解。

Method: 通过Identity Preference Optimization框架研究采样和参考选择的影响，分析迭代对齐动态，证明特定参数下的情况及稳定性区域。

Result: 适当的实例相关采样能有更强排名保证，偏斜的策略采样在结构化偏好下会导致过度集中，迭代对齐动态在特定参数下有持续振荡或熵坍塌情况。

Conclusion: 理论见解可扩展到Direct Preference Optimization，实验验证发现的现象在更广泛偏好对齐方法中常见。

Abstract: Standard methods for aligning large language models with human preferences learn from pairwise comparisons among sampled candidate responses and regularize toward a reference policy. Despite their effectiveness, the effects of sampling and reference choices are poorly understood theoretically. We investigate these effects through Identity Preference Optimization, a widely used preference alignment framework, and show that proper instance-dependent sampling can yield stronger ranking guarantees, while skewed on-policy sampling can induce excessive concentration under structured preferences. We then analyze iterative alignment dynamics in which the learned policy feeds back into future sampling and reference policies, reflecting a common practice of model-generated preference data. We prove that these dynamics can exhibit persistent oscillations or entropy collapse for certain parameter choices, and characterize regimes that guarantee stability. Our theoretical insights extend to Direct Preference Optimization, indicating the phenomena we captured are common to a broader class of preference-alignment methods. Experiments on real-world preference data validate our findings.

</details>


### [148] [How Many Features Can a Language Model Store Under the Linear Representation Hypothesis?](https://arxiv.org/abs/2602.11246)
*Nikhil Garg,Jon Kleinberg,Kenny Peng*

Main category: cs.LG

TL;DR: 本文为线性表示假设引入数学框架，分离出线性表示和线性可访问两个主张，给出线性压缩感知的上下界，证明线性可访问更强，并为'叠加假设'提供理论依据。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型中间层线性表示特征时，确定线性表示和线性访问m个特征所需的最少神经元数量。

Method: 上界证明使用标准随机构造列近似正交的矩阵，下界证明结合近单位矩阵的秩界和图论的Turán定理，还将结果拓展到激活函数和偏置的解码器。

Result: 证明线性压缩感知中，下界 $d = Ω_ε(\frac{k^2}{\log k}\log (m/k))$，上界 $d = O_ε(k^2\log m)$。

Conclusion: 线性可访问比线性表示假设更强，神经元在LRH下可存储指数数量的特征，且展示了结果对特征表示几何的约束情况。

Abstract: We introduce a mathematical framework for the linear representation hypothesis (LRH), which asserts that intermediate layers of language models store features linearly. We separate the hypothesis into two claims: linear representation (features are linearly embedded in neuron activations) and linear accessibility (features can be linearly decoded). We then ask: How many neurons $d$ suffice to both linearly represent and linearly access $m$ features? Classical results in compressed sensing imply that for $k$-sparse inputs, $d = O(k\log (m/k))$ suffices if we allow non-linear decoding algorithms (Candes and Tao, 2006; Candes et al., 2006; Donoho, 2006). However, the additional requirement of linear decoding takes the problem out of the classical compressed sensing, into linear compressed sensing.
  Our main theoretical result establishes nearly-matching upper and lower bounds for linear compressed sensing. We prove that $d = Ω_ε(\frac{k^2}{\log k}\log (m/k))$ is required while $d = O_ε(k^2\log m)$ suffices. The lower bound establishes a quantitative gap between classical and linear compressed setting, illustrating how linear accessibility is a meaningfully stronger hypothesis than linear representation alone. The upper bound confirms that neurons can store an exponential number of features under the LRH, giving theoretical evidence for the "superposition hypothesis" (Elhage et al., 2022).
  The upper bound proof uses standard random constructions of matrices with approximately orthogonal columns. The lower bound proof uses rank bounds for near-identity matrices (Alon, 2003) together with Turán's theorem (bounding the number of edges in clique-free graphs). We also show how our results do and do not constrain the geometry of feature representations and extend our results to allow decoders with an activation function and bias.

</details>


### [149] [HiFloat4 Format for Language Model Inference](https://arxiv.org/abs/2602.11287)
*Yuanyong Luo,Jing Huang,Yu Cheng,Ziwei Yu,Kaihua Zhang,Kehong Hong,Xinda Ma,Xin Wang,Anping Tong,Guipeng Hu,Yun Xu,Mehran Taghian,Peng Wu,Guanglin Li,Yunke Peng,Tianchi Hu,Minqi Chen,Michael Bi Mi,Hu Liu,Xiping Zhou,Junsong Wang,Qiang Lin,Heng Liao*

Main category: cs.LG

TL;DR: 论文介绍了为深度学习定制的HiFloat4数据格式，经实验在多模型和任务上准确率高于NVFP4。


<details>
  <summary>Details</summary>
Motivation: 提出适合深度学习、能提高数据表示空间利用率、降低硬件能耗的数据格式。

Method: 提出HiFloat4数据格式，每个单元包含64个4位元素和32位共享缩放元数据，元数据指定三级缩放层次；对LLaMA、Qwen等多个语言模型进行推理实验。

Result: HiF4在多个模型和不同下游任务上比NVFP4有更高的平均准确率。

Conclusion: HiFloat4是一种性能良好的适合深度学习的数据格式。

Abstract: This paper introduces HiFloat4 (HiF4), a block floating-point data format tailored for deep learning. Each HiF4 unit packs 64 4-bit elements with 32 bits of shared scaling metadata, averaging 4.5 bits per value. The metadata specifies a three-level scaling hierarchy, capturing inter- and intra-group dynamic range while improving the utilization of the representational space. In addition, the large 64-element group size enables matrix multiplications to be executed in a highly fixed-point manner, significantly reducing hardware area and power consumption. To evaluate the proposed format, we conducted inference experiments on several language models, including LLaMA, Qwen, Mistral, DeepSeek-V3.1 and LongCat. Results show that HiF4 achieves higher average accuracy than the state-of-the-art NVFP4 format across multiple models and diverse downstream tasks.

</details>


### [150] [Efficient Analysis of the Distilled Neural Tangent Kernel](https://arxiv.org/abs/2602.11320)
*Jamie Mahowald,Brian Bell,Alex Ho,Michael Geyer*

Main category: cs.LG

TL;DR: 提出蒸馏神经切线核 (DNTK) 结合数据集蒸馏和投影方法降低 NTK 计算复杂度并保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有 NTK 方法受大规模雅可比矩阵计算限制，需降低计算成本。

Method: 使用 NTK 调优的数据集蒸馏压缩数据维度，结合最先进投影方法，提出 DNTK。

Result: 数据集蒸馏使雅可比计算量减少 20 - 100 倍，DNTK 最多降低 NTK 五阶计算复杂度。

Conclusion: DNTK 在降低复杂度同时能保持核结构和预测性能。

Abstract: Neural tangent kernel (NTK) methods are computationally limited by the need to evaluate large Jacobians across many data points. Existing approaches reduce this cost primarily through projecting and sketching the Jacobian. We show that NTK computation can also be reduced by compressing the data dimension itself using NTK-tuned dataset distillation. We demonstrate that the neural tangent space spanned by the input data can be induced by dataset distillation, yielding a 20-100$\times$ reduction in required Jacobian calculations. We further show that per-class NTK matrices have low effective rank that is preserved by this reduction. Building on these insights, we propose the distilled neural tangent kernel (DNTK), which combines NTK-tuned dataset distillation with state-of-the-art projection methods to reduce up NTK computational complexity by up to five orders of magnitude while preserving kernel structure and predictive performance.

</details>


### [151] [Divide and Learn: Multi-Objective Combinatorial Optimization at Scale](https://arxiv.org/abs/2602.11346)
*Esha Singh,Dongxia Wu,Chien-Yi Yang,Tajana Rosing,Rose Yu,Yi-An Ma*

Main category: cs.LG

TL;DR: 将多目标组合优化问题重新表述为在线学习问题，在分解决策空间上解决，有较好性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有多目标组合优化方法牺牲了通用性、可扩展性或理论保证。

Method: 将问题重新表述为在线学习问题，在分解决策空间上通过自适应专家引导的顺序构造解决位置式多臂老虎机子问题。

Result: 在标准基准测试中达到专业求解器80 - 98%的性能，样本和计算效率比贝叶斯优化方法提高两到三个数量级；在实际硬件 - 软件协同设计中，在固定评估预算下优于竞争方法。

Conclusion: 在分解决策空间上进行多臂老虎机优化是多目标优化中替代代理建模或离线训练的原则性方法。

Abstract: Multi-objective combinatorial optimization seeks Pareto-optimal solutions over exponentially large discrete spaces, yet existing methods sacrifice generality, scalability, or theoretical guarantees. We reformulate it as an online learning problem over a decomposed decision space, solving position-wise bandit subproblems via adaptive expert-guided sequential construction. This formulation admits regret bounds of $O(d\sqrt{T \log T})$ depending on subproblem dimensionality \(d\) rather than combinatorial space size. On standard benchmarks, our method achieves 80--98\% of specialized solvers performance while achieving two to three orders of magnitude improvement in sample and computational efficiency over Bayesian optimization methods. On real-world hardware-software co-design for AI accelerators with expensive simulations, we outperform competing methods under fixed evaluation budgets. The advantage grows with problem scale and objective count, establishing bandit optimization over decomposed decision spaces as a principled alternative to surrogate modeling or offline training for multi-objective optimization.

</details>


### [152] [Structured Hybrid Mechanistic Models for Robust Estimation of Time-Dependent Intervention Outcomes](https://arxiv.org/abs/2602.11350)
*Tomer Meir,Ori Linial,Danny Eytan,Uri Shalit*

Main category: cs.LG

TL;DR: 提出混合机制 - 数据驱动方法估计干预结果，在周期摆和丙泊酚推注场景表现优于纯数据驱动和机制方法。


<details>
  <summary>Details</summary>
Motivation: 数据驱动模型在分布外表现不佳，机制模型可能过于简化，需更好方法估计动力系统干预效果以实现结果优化。

Method: 将动力系统的转移算子分解为参数和非参数组件，区分干预相关和无关动态，对机制参数未知情况采用两阶段程序。

Result: 混合方法在分布外表现优于纯数据驱动和机制方法。

Conclusion: 混合机制 - 数据驱动模型在复杂现实动力系统的干预优化中有潜力。

Abstract: Estimating intervention effects in dynamical systems is crucial for outcome optimization. In medicine, such interventions arise in physiological regulation (e.g., cardiovascular system under fluid administration) and pharmacokinetics, among others. Propofol administration is an anesthetic intervention, where the challenge is to estimate the optimal dose required to achieve a target brain concentration for anesthesia, given patient characteristics, while avoiding under- or over-dosing. The pharmacokinetic state is characterized by drug concentrations across tissues, and its dynamics are governed by prior states, patient covariates, drug clearance, and drug administration. While data-driven models can capture complex dynamics, they often fail in out-of-distribution (OOD) regimes. Mechanistic models on the other hand are typically robust, but might be oversimplified. We propose a hybrid mechanistic-data-driven approach to estimate time-dependent intervention outcomes. Our approach decomposes the dynamical system's transition operator into parametric and nonparametric components, further distinguishing between intervention-related and unrelated dynamics. This structure leverages mechanistic anchors while learning residual patterns from data. For scenarios where mechanistic parameters are unknown, we introduce a two-stage procedure: first, pre-training an encoder on simulated data, and subsequently learning corrections from observed data. Two regimes with incomplete mechanistic knowledge are considered: periodic pendulum and Propofol bolus injections. Results demonstrate that our hybrid approach outperforms purely data-driven and mechanistic approaches, particularly OOD. This work highlights the potential of hybrid mechanistic-data-driven models for robust intervention optimization in complex, real-world dynamical systems.

</details>


### [153] [Retrieval-Aware Distillation for Transformer-SSM Hybrids](https://arxiv.org/abs/2602.11374)
*Aviv Bick,Eric P. Xing,Albert Gu*

Main category: cs.LG

TL;DR: 提出检索感知蒸馏方法，将预训练Transformer转换为混合模型，保留少量关键注意力头恢复教师模型性能，降低内存成本。


<details>
  <summary>Details</summary>
Motivation: 状态空间模型（SSMs）在需要上下文检索的基准测试中落后于Transformer，解决SSMs难以重现G&A注意力头的问题。

Method: 提出检索感知蒸馏方法，通过合成检索任务的消融实验确定关键注意力头，将预训练Transformer转换为混合学生模型。

Result: 保留2%的注意力头可在检索密集任务上恢复超95%的教师模型性能，混合模型内存效率提高5 - 6倍。

Conclusion: 该方法缩小了Transformer和SSM之间的差距，以较低的内存成本实现了较好的性能。

Abstract: State-space models (SSMs) offer efficient sequence modeling but lag behind Transformers on benchmarks that require in-context retrieval. Prior work links this gap to a small set of attention heads, termed Gather-and-Aggregate (G&A), which SSMs struggle to reproduce. We propose *retrieval-aware distillation*, which converts a pretrained Transformer into a hybrid student by preserving only these retrieval-critical heads and distilling the rest into recurrent heads. We identify the essential heads via ablation on a synthetic retrieval task, producing a hybrid with sparse, non-uniform attention placement. We show that preserving **just 2% of attention heads recovers over 95% of teacher performance on retrieval-heavy tasks** (10 heads in a 1B model), requiring far fewer heads than hybrids that retain at least 25%. We further find that large recurrent states often compensate for missing retrieval: once retrieval is handled by these heads, the SSM backbone can be simplified with limited loss, even with an $8\times$ reduction in state dimension. By reducing both the attention cache and the SSM state, the resulting hybrid is $5$--$6\times$ more memory-efficient than comparable hybrids, closing the Transformer--SSM gap at a fraction of the memory cost.

</details>


### [154] [WSBD: Freezing-Based Optimizer for Quantum Neural Networks](https://arxiv.org/abs/2602.11383)
*Christopher Kverne,Mayur Akewar,Yuqian Huo,Tirthak Patel,Janki Bhimani*

Main category: cs.LG

TL;DR: 提出WSBD优化器解决量子神经网络训练问题，比Adam收敛快，有收敛证明。


<details>
  <summary>Details</summary>
Motivation: 解决量子神经网络训练中梯度估计计算成本高和贫瘠高原问题。

Method: 引入WSBD优化器，采用动态、逐参数冻结策略，根据梯度重要性分数识别并临时冻结影响较小的参数。

Result: WSBD在基态能量问题上比Adam平均收敛快63.9%，且优势随网络规模增大而增加。

Conclusion: WSBD在保持全表达能力的同时能有效导航优化空间，逐参数冻结优于传统逐层方法。

Abstract: The training of Quantum Neural Networks (QNNs) is hindered by the high computational cost of gradient estimation and the barren plateau problem, where optimization landscapes become intractably flat. To address these challenges, we introduce Weighted Stochastic Block Descent (WSBD), a novel optimizer with a dynamic, parameter-wise freezing strategy. WSBD intelligently focuses computational resources by identifying and temporarily freezing less influential parameters based on a gradient-derived importance score. This approach significantly reduces the number of forward passes required per training step and helps navigate the optimization landscape more effectively. Unlike pruning or layer-wise freezing, WSBD maintains full expressive capacity while adapting throughout training. Our extensive evaluation shows that WSBD converges on average 63.9% faster than Adam for the popular ground-state-energy problem, an advantage that grows with QNN size. We provide a formal convergence proof for WSBD and show that parameter-wise freezing outperforms traditional layer-wise approaches in QNNs. Project page: https://github.com/Damrl-lab/WSBD-Stochastic-Freezing-Optimizer.

</details>


### [155] [Provably Efficient Algorithms for S- and Non-Rectangular Robust MDPs with General Parameterization](https://arxiv.org/abs/2602.11387)
*Anirudh Satheesh,Ziyi Chen,Furong Huang,Heng Huang*

Main category: cs.LG

TL;DR: 研究带通用策略参数化的鲁棒马尔可夫决策过程，解决现有方法问题，通过方法改进样本复杂度和算法复杂度。


<details>
  <summary>Details</summary>
Motivation: 过往研究大多局限于表格策略，缺乏样本复杂度保证或计算成本高。

Method: 将平均奖励 RMDPs 转化为熵正则化折扣鲁棒 MDPs，证明通用策略参数化特性，引入多级蒙特卡洛梯度估计器，设计投影梯度下降和 Frank - Wolfe 算法。

Result: 多级蒙特卡洛梯度估计器样本复杂度提升，算法在多种情况下复杂度显著改进。

Conclusion: 首次为非 (s, a) - 矩形的通用策略参数化 RMDPs 提供样本复杂度保证，在平均奖励和折扣设置上都有突破。

Abstract: We study robust Markov decision processes (RMDPs) with general policy parameterization under s-rectangular and non-rectangular uncertainty sets. Prior work is largely limited to tabular policies, and hence either lacks sample complexity guarantees or incurs high computational cost. Our method reduces the average reward RMDPs to entropy-regularized discounted robust MDPs, restoring strong duality and enabling tractable equilibrium computation. We prove novel Lipschitz and Lipschitz-smoothness properties for general policy parameterizations that extends to infinite state spaces. To address infinite-horizon gradient estimation, we introduce a multilevel Monte Carlo gradient estimator with $\tilde{\mathcal{O}}(ε^{-2})$ sample complexity, a factor of $\mathcal{O}(ε^{-2})$ improvement over prior work. Building on this, we design a projected gradient descent algorithm for s-rectangular uncertainty ($\mathcal{O}(ε^{-5})$) and a Frank--Wolfe algorithm for non-rectangular uncertainty ($\mathcal{O}(ε^{-4})$ discounted, $\mathcal{O}(ε^{-10.5})$ average reward), significantly improving prior results in both the discounted setting and average reward setting. Our work is the first one to provide sample complexity guarantees for RMDPs with general policy parameterization beyond $(s, a)$-rectangularity. It also provides the first such guarantees in the average reward setting and improves existing bounds for discounted robust MDPs.

</details>


### [156] [Sparse Semantic Dimension as a Generalization Certificate for LLMs](https://arxiv.org/abs/2602.11388)
*Dibyanayan Bandyopadhyay,Asif Ekbal*

Main category: cs.LG

TL;DR: 标准统计学习理论认为大语言模型会过拟合，但实际能稳健泛化。本文提出有效容量由模型内部表征几何决定，引入SSD衡量复杂度，在GPT - 2和Gemma - 2B上验证，发现“特征锐度”定律，还可作安全监测工具。


<details>
  <summary>Details</summary>
Motivation: 解释大语言模型参数远超训练token数却能稳健泛化的现象。

Method: 引入Sparse Semantic Dimension (SSD)，结合训练的Sparse Autoencoder (SAE)，将大语言模型和SAE视为固定的预言机进行分析。

Result: 在GPT - 2 Small和Gemma - 2B上验证框架有效，发现Gemma - 2B比GPT - 2识别活跃流形所需校准样本少，还发现分布外输入触发“特征爆炸”。

Conclusion: 模型泛化能力取决于字典稀疏性而非参数总数，框架可作为安全监测工具。

Abstract: Standard statistical learning theory predicts that Large Language Models (LLMs) should overfit because their parameter counts vastly exceed the number of training tokens. Yet, in practice, they generalize robustly. We propose that the effective capacity controlling generalization lies in the geometry of the model's internal representations: while the parameter space is high-dimensional, the activation states lie on a low-dimensional, sparse manifold. To formalize this, we introduce the Sparse Semantic Dimension (SSD), a complexity measure derived from the active feature vocabulary of a Sparse Autoencoder (SAE) trained on the model's layers. Treating the LLM and SAE as frozen oracles, we utilize this framework to attribute the model's generalization capabilities to the sparsity of the dictionary rather than the total parameter count. Empirically, we validate this framework on GPT-2 Small and Gemma-2B, demonstrating that our bound provides non-vacuous certificates at realistic sample sizes. Crucially, we uncover a counter-intuitive "feature sharpness" scaling law: despite being an order of magnitude larger, Gemma-2B requires significantly fewer calibration samples to identify its active manifold compared to GPT-2, suggesting that larger models learn more compressible, distinct semantic structures. Finally, we show that this framework functions as a reliable safety monitor: out-of-distribution inputs trigger a measurable "feature explosion" (a sharp spike in active features), effectively signaling epistemic uncertainty through learned feature violation. Code is available at: https://github.com/newcodevelop/sparse-semantic-dimension.

</details>


### [157] [General and Efficient Steering of Unconditional Diffusion](https://arxiv.org/abs/2602.11395)
*Qingsong Wang,Mikhail Belkin,Yusu Wang*

Main category: cs.LG

TL;DR: 提出无需梯度引导高效控制无条件扩散模型的方法，实验显示效果优于基于梯度引导且推理加速。


<details>
  <summary>Details</summary>
Motivation: 现有引导无条件扩散模型方法计算开销大，需高效控制方法。

Method: 基于噪声对齐和可转移概念向量两个观察，用递归特征机（RFM）识别概念方向。

Result: 在CIFAR - 10、ImageNet和CelebA上实验，精度/质量优于基于梯度引导，且推理显著加速。

Conclusion: 所提方法能高效引导无条件扩散模型，实现快速可控生成。

Abstract: Guiding unconditional diffusion models typically requires either retraining with conditional inputs or per-step gradient computations (e.g., classifier-based guidance), both of which incur substantial computational overhead. We present a general recipe for efficiently steering unconditional diffusion {without gradient guidance during inference}, enabling fast controllable generation. Our approach is built on two observations about diffusion model structure: Noise Alignment: even in early, highly corrupted stages, coarse semantic steering is possible using a lightweight, offline-computed guidance signal, avoiding any per-step or per-sample gradients. Transferable concept vectors: a concept direction in activation space once learned transfers across both {timesteps} and {samples}; the same fixed steering vector learned near low noise level remains effective when injected at intermediate noise levels for every generation trajectory, providing refined conditional control with efficiency. Such concept directions can be efficiently and reliably identified via Recursive Feature Machine (RFM), a light-weight backpropagation-free feature learning method. Experiments on CIFAR-10, ImageNet, and CelebA demonstrate improved accuracy/quality over gradient-based guidance, while achieving significant inference speedups.

</details>


### [158] [Can We Really Learn One Representation to Optimize All Rewards?](https://arxiv.org/abs/2602.11399)
*Chongyi Zheng,Royina Karegoudra Jayanth,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: 本文揭开FB表征学习神秘面纱，提出一步FB表征学习方法，实验证明其能降低误差、提升零样本性能。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习使用大模型作为先验解决强化学习问题，但FB表征学习的训练目标和学习行为未知，需深入了解。

Method: 分析FB表征存在条件、训练目标及收敛情况，提出一步FB表征学习方法，与现有方法建立联系。

Result: 一步FB在多个连续控制领域中收敛误差小10^5，平均零样本性能提升24%。

Conclusion: 一步FB表征学习方法有效，能在强化学习中实现更好的预训练效果。

Abstract: As machine learning has moved towards leveraging large models as priors for downstream tasks, the community has debated the right form of prior for solving reinforcement learning (RL) problems. If one were to try to prefetch as much computation as possible, they would attempt to learn a prior over the policies for some yet-to-be-determined reward function. Recent work (forward-backward (FB) representation learning) has tried this, arguing that an unsupervised representation learning procedure can enable optimal control over arbitrary rewards without further fine-tuning. However, FB's training objective and learning behavior remain mysterious. In this paper, we demystify FB by clarifying when such representations can exist, what its objective optimizes, and how it converges in practice. We draw connections with rank matching, fitted Q-evaluation, and contraction mapping. Our analysis suggests a simplified unsupervised pre-training method for RL that, instead of enabling optimal control, performs one step of policy improvement. We call our proposed method $\textbf{one-step forward-backward representation learning (one-step FB)}$. Experiments in didactic settings, as well as in $10$ state-based and image-based continuous control domains, demonstrate that one-step FB converges to errors $10^5$ smaller and improves zero-shot performance by $+24\%$ on average. Our project website is available at https://chongyi-zheng.github.io/onestep-fb.

</details>


### [159] [CADET: Context-Conditioned Ads CTR Prediction With a Decoder-Only Transformer](https://arxiv.org/abs/2602.11410)
*David Pardoe,Neil Daftary,Miro Furtado,Aditya Aiyer,Yu Wang,Liuqing Li,Tao Song,Lars Hertel,Young Jin Yun,Senthil Radhakrishnan,Zhiwei Wang,Tommy Li,Khai Tran,Ananth Nagarajan,Ali Naqvi,Yue Zhang,Renpeng Fang,Avi Romascanu,Arjun Kulothungun,Deepak Kumar,Praneeth Boda,Fedor Borisyuk,Ruoyan Wang*

Main category: cs.LG

TL;DR: 提出CADET用于广告CTR预测，有多项创新，线上A/B测试CTR提升11.04%，已在领英广告平台部署。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer架构应用于广告CTR预测面临处理评分后上下文信号、保持离线在线一致性和扩展到工业工作负载等挑战。

Method: 提出CADET模型，有上下文条件解码架构、自门控注意力机制、基于时间戳的旋转位置嵌入、会话掩码策略和生产工程技术等创新。

Result: 线上A/B测试中，CADET较生产LiRank基线模型CTR提升11.04%。

Conclusion: CADET模型在广告CTR预测方面表现良好，已成功部署在领英广告平台。

Abstract: Click-through rate (CTR) prediction is fundamental to online advertising systems. While Deep Learning Recommendation Models (DLRMs) with explicit feature interactions have long dominated this domain, recent advances in generative recommenders have shown promising results in content recommendation. However, adapting these transformer-based architectures to ads CTR prediction still presents unique challenges, including handling post-scoring contextual signals, maintaining offline-online consistency, and scaling to industrial workloads. We present CADET (Context-Conditioned Ads Decoder-Only Transformer), an end-to-end decoder-only transformer for ads CTR prediction deployed at LinkedIn. Our approach introduces several key innovations: (1) a context-conditioned decoding architecture with multi-tower prediction heads that explicitly model post-scoring signals such as ad position, resolving the chicken-and-egg problem between predicted CTR and ranking; (2) a self-gated attention mechanism that stabilizes training by adaptively regulating information flow at both representation and interaction levels; (3) a timestamp-based variant of Rotary Position Embedding (RoPE) that captures temporal relationships across timescales from seconds to months; (4) session masking strategies that prevent the model from learning dependencies on unavailable in-session events, addressing train-serve skew; and (5) production engineering techniques including tensor packing, sequence chunking, and custom Flash Attention kernels that enable efficient training and serving at scale. In online A/B testing, CADET achieves a 11.04\% CTR lift compared to the production LiRank baseline model, a hybrid ensemble of DCNv2 and sequential encoders. The system has been successfully deployed on LinkedIn's advertising platform, serving the main traffic for homefeed sponsored updates.

</details>


### [160] [TimeSynth: A Framework for Uncovering Systematic Biases in Time Series Forecasting](https://arxiv.org/abs/2602.11413)
*Md Rakibul Haque,Vishwa Goudar,Shireen Elhabian,Warren Woodrich Pettine*

Main category: cs.LG

TL;DR: 本文通过TimeSynth框架重新探讨时间序列预测中复杂非线性架构与简单线性模型的性能对比，发现线性模型存在系统偏差，非线性模型在信号复杂度增加时更具优势。


<details>
  <summary>Details</summary>
Motivation: 近期关于时间序列预测中复杂非线性架构是否真的优于简单线性模型存在争议，以往线性模型占优的说法常源于缺乏多样时间动态和有偏差的评估协议。

Method: 使用TimeSynth框架，模拟现实世界时间序列的关键属性，创建合成信号对线性、多层感知机（MLP）、卷积神经网络（CNNs）和Transformer四种模型族进行评估。

Result: 线性模型存在系统偏差，不论信号复杂度如何都会退化为简单振荡；非线性模型避免了这种情况，且随信号复杂度增加优势明显；Transformers和基于CNN的模型对复杂调制信号的适应性略强于MLP。此外，该框架凸显了分布和噪声变化下的鲁棒性差异，消除了先前基准的偏差。

Conclusion: TimeSynth为理解不同预测方法何时成功或失败提供了有原则的基础，超越了对模型等价性的过度简化论断。

Abstract: Time series forecasting is a fundamental tool with wide ranging applications, yet recent debates question whether complex nonlinear architectures truly outperform simple linear models. Prior claims of dominance of the linear model often stem from benchmarks that lack diverse temporal dynamics and employ biased evaluation protocols. We revisit this debate through TimeSynth, a structured framework that emulates key properties of real world time series,including non-stationarity, periodicity, trends, and phase modulation by creating synthesized signals whose parameters are derived from real-world time series. Evaluating four model families Linear, Multi Layer Perceptrons (MLP), Convolutional Neural Networks (CNNs), and Transformers, we find a systematic bias in linear models: they collapse to simple oscillation regardless of signal complexity. Nonlinear models avoid this collapse and gain clear advantages as signal complexity increases. Notably, Transformers and CNN based models exhibit slightly greater adaptability to complex modulated signals compared to MLPs. Beyond clean forecasting, the framework highlights robustness differences under distribution and noise shifts and removes biases of prior benchmarks by using independent instances for train, test, and validation for each signal family. Collectively, TimeSynth provides a principled foundation for understanding when different forecasting approaches succeed or fail, moving beyond oversimplified claims of model equivalence.

</details>


### [161] [Multi-Level Strategic Classification: Incentivizing Improvement through Promotion and Relegation Dynamics](https://arxiv.org/abs/2602.11439)
*Ziyuan Huang,Lina Alkarmi,Mingyan Liu*

Main category: cs.LG

TL;DR: 不同于现有顺序战略分类重权重优化的研究，本文分析多级升降框架下分类阈值和难度进展设计，证明在此机制下代理可通过努力达到任意高水平。


<details>
  <summary>Details</summary>
Motivation: 现有顺序战略分类研究主要关注动态分类器权重优化，本文希望从分类器阈值和难度进展设计的角度进行研究。

Method: 在多级升降框架下分析分类器阈值和难度进展设计，建立模型捕捉代理人的跨期激励。

Result: 刻画了代理人的最优长期策略，证明委托人能设计阈值序列有效激励诚实努力。

Conclusion: 在温和条件下，该机制能使代理人仅通过真正的改进努力达到任意高的水平。

Abstract: Strategic classification studies the problem where self-interested individuals or agents manipulate their response to obtain favorable decision outcomes made by classifiers, typically turning to dishonest actions when they are less costly than genuine efforts. While existing studies on sequential strategic classification primarily focus on optimizing dynamic classifier weights, we depart from these weight-centric approaches by analyzing the design of classifier thresholds and difficulty progression within a multi-level promotion-relegation framework. Our model captures the critical inter-temporal incentives driven by an agent's farsightedness, skill retention, and a leg-up effect where qualification and attainment can be self-reinforcing. We characterize the agent's optimal long-term strategy and demonstrate that a principal can design a sequence of thresholds to effectively incentivize honest effort. Crucially, we prove that under mild conditions, this mechanism enables agents to reach arbitrarily high levels solely through genuine improvement efforts.

</details>


### [162] [Hierarchical Concept Embedding & Pursuit for Interpretable Image Classification](https://arxiv.org/abs/2602.11448)
*Nghia Nguyen,Tianjiao Ding,René Vidal*

Main category: cs.LG

TL;DR: 提出Hierarchical Concept Embedding & Pursuit (HCEP)框架，将层次结构引入稀疏编码，在概念精度、召回率和分类准确率上表现出色，证明该方法能带来更可靠和可解释的图像分类模型。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏概念恢复方法忽略概念层次结构，可能导致预测正确但解释与层次结构不一致，需要改进。

Method: 提出HCEP框架，在潜在空间中引入概念嵌入的层次结构，使用层次稀疏编码来恢复图像中的概念。

Result: 实验表明HCEP在概念精度和召回率上优于基线，样本有限时分类准确率和概念恢复能力更优。

Conclusion: 将层次结构融入稀疏编码可产生更可靠和可解释的图像分类模型。

Abstract: Interpretable-by-design models are gaining traction in computer vision because they provide faithful explanations for their predictions. In image classification, these models typically recover human-interpretable concepts from an image and use them for classification. Sparse concept recovery methods leverage the latent space of vision-language models to represent image embeddings as a sparse combination of concept embeddings. However, because such methods ignore the hierarchical structure of concepts, they can produce correct predictions with explanations that are inconsistent with the hierarchy. In this work, we propose Hierarchical Concept Embedding \& Pursuit (HCEP), a framework that induces a hierarchy of concept embeddings in the latent space and uses hierarchical sparse coding to recover the concepts present in an image. Given a hierarchy of semantic concepts, we construct a corresponding hierarchy of concept embeddings and, assuming the correct concepts for an image form a rooted path in the hierarchy, derive desirable conditions for identifying them in the embedded space. We show that hierarchical sparse coding reliably recovers hierarchical concept embeddings, whereas vanilla sparse coding fails. Our experiments on real-world datasets demonstrate that HCEP outperforms baselines in concept precision and recall while maintaining competitive classification accuracy. Moreover, when the number of samples is limited, HCEP achieves superior classification accuracy and concept recovery. These results show that incorporating hierarchical structures into sparse coding yields more reliable and interpretable image classification models.

</details>


### [163] [Assessing Low Back Movement with Motion Tape Sensor Data Through Deep Learning](https://arxiv.org/abs/2602.11465)
*Jared Levy,Aarti Lalwani,Elijah Wyckoff,Kenneth J. Loh,Sara P. Gombatto,Rose Yu,Emilia Farcas*

Main category: cs.LG

TL;DR: 提出MT - AIM模型对下背部运动分类，解决运动监测难题。


<details>
  <summary>Details</summary>
Motivation: 背痛普遍，需评估下背部运动辅助理疗，但远程监测难，现有传感器不适用于日常生活，MT传感器虽有优势但数据有局限。

Method: 提出MT - AIM深度学习分类管道，利用条件生成模型生成合成MT数据，预测关节运动学特征作为补充。

Result: MT - AIM在对下背部运动分类中达到了最先进的准确率。

Conclusion: MT - AIM弥合了生理传感和运动分析之间的差距。

Abstract: Back pain is a pervasive issue affecting a significant portion of the population, often worsened by certain movements of the lower back. Assessing these movements is important for helping clinicians prescribe appropriate physical therapy. However, it can be difficult to monitor patients' movements remotely outside the clinic. High-fidelity data from motion capture sensors can be used to classify different movements, but these sensors are costly and impractical for use in free-living environments. Motion Tape (MT), a new fabric-based wearable sensor, addresses these issues by being low cost and portable. Despite these advantages, novelty and variability in sensor stability make the MT dataset small scale and inherent to noise. In this work, we propose the Motion-Tape Augmentation Inference Model (MT-AIM), a deep learning classification pipeline trained on MT data. In order to address the challenges of limited sample size and noise present within the MT dataset, MT-AIM leverages conditional generative models to generate synthetic MT data of a desired movement, as well as predicting joint kinematics as additional features. This combination of synthetic data generation and feature augmentation enables MT-AIM to achieve state-of-the-art accuracy in classifying lower back movements, bridging the gap between physiological sensing and movement analysis.

</details>


### [164] [PRISM: A 3D Probabilistic Neural Representation for Interpretable Shape Modeling](https://arxiv.org/abs/2602.11467)
*Yining Jiao,Sreekalyani Bhamidi,Carlton Jude Zdanski,Julia S Kimbell,Andrew Prince,Cameron P Worden,Samuel Kirse,Christopher Rutter,Benjamin H Shields,Jisan Mahmud,Marc Niethammer*

Main category: cs.LG

TL;DR: 提出PRISM框架用于解剖形状演变分析，在多数据集实验表现好。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略空间异质动态，需新方法理解解剖形状演变及量化不确定性。

Method: 引入PRISM框架，结合隐式神经表征与不确定性感知统计形状分析，给出闭形式Fisher信息度量。

Result: 在三个合成数据集和一个临床数据集实验中，PRISM在统一框架不同任务有强性能，给出可解释且有临床意义的不确定性估计。

Conclusion: PRISM框架能有效解决解剖形状演变分析问题，新方法可行且有效。

Abstract: Understanding how anatomical shapes evolve in response to developmental covariates and quantifying their spatially varying uncertainties is critical in healthcare research. Existing approaches typically rely on global time-warping formulations that ignore spatially heterogeneous dynamics. We introduce PRISM, a novel framework that bridges implicit neural representations with uncertainty-aware statistical shape analysis. PRISM models the conditional distribution of shapes given covariates, providing spatially continuous estimates of both the population mean and covariate-dependent uncertainty at arbitrary locations. A key theoretical contribution is a closed-form Fisher Information metric that enables efficient, analytically tractable local temporal uncertainty quantification via automatic differentiation. Experiments on three synthetic datasets and one clinical dataset demonstrate PRISM's strong performance across diverse tasks within a unified framework, while providing interpretable and clinically meaningful uncertainty estimates.

</details>


### [165] [External Division of Two Bregman Proximity Operators for Poisson Inverse Problems](https://arxiv.org/abs/2602.11482)
*Kazuki Haishima,Kyohei Suzuki,Konstantinos Slavakis*

Main category: cs.LG

TL;DR: 提出从含泊松噪声线性模型恢复稀疏向量的新方法，嵌入新算子，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决从含泊松噪声线性模型恢复稀疏向量问题，减轻经典l1范数正则化估计偏差。

Method: 引入外部除法定义的算子促进稀疏解，将其嵌入NoLips算法；通过两种互补重表述阐明算子几何结构。

Result: 新方法收敛更稳定，在合成数据和图像恢复问题上性能显著优于传统基于KL散度的方法。

Conclusion: 所提方法在恢复含泊松噪声线性模型的稀疏向量方面表现出色，具有更好的收敛性和性能。

Abstract: This paper presents a novel method for recovering sparse vectors from linear models corrupted by Poisson noise. The contribution is twofold. First, an operator defined via the external division of two Bregman proximity operators is introduced to promote sparse solutions while mitigating the estimation bias induced by classical $\ell_1$-norm regularization. This operator is then embedded into the already established NoLips algorithm, replacing the standard Bregman proximity operator in a plug-and-play manner. Second, the geometric structure of the proposed external-division operator is elucidated through two complementary reformulations, which provide clear interpretations in terms of the primal and dual spaces of the Poisson inverse problem. Numerical tests show that the proposed method exhibits more stable convergence behavior than conventional Kullback-Leibler (KL)-based approaches and achieves significantly superior performance on synthetic data and an image restoration problem.

</details>


### [166] [Exploring Multiple High-Scoring Subspaces in Generative Flow Networks](https://arxiv.org/abs/2602.11491)
*Xuan Yu,Xu Wang,Rui Zhu,Yudong Zhang,Yang Wang*

Main category: cs.LG

TL;DR: 本文提出CMAB - GFN，将组合多臂老虎机框架与GFlowNet策略集成，实验表明其能生成更高奖励的候选方案。


<details>
  <summary>Details</summary>
Motivation: 现有Generative Flow Networks在庞大状态空间过度探索，导致低奖励区域过采样和收敛到次优分布，需要有效引导GFlowNets寻找高奖励解决方案。

Method: 提出CMAB - GFN，用组合多臂老虎机组件修剪低质量动作，得到紧凑的高分数子空间供Generative Flow Networks探索。

Result: 在多个任务上的实验结果显示，CMAB - GFN比现有方法能生成更高奖励的候选方案。

Conclusion: CMAB - GFN能有效引导Generative Flow Networks找到高奖励解决方案，且不牺牲多样性。

Abstract: As a probabilistic sampling framework, Generative Flow Networks (GFlowNets) show strong potential for constructing complex combinatorial objects through the sequential composition of elementary components. However, existing GFlowNets often suffer from excessive exploration over vast state spaces, leading to over-sampling of low-reward regions and convergence to suboptimal distributions. Effectively biasing GFlowNets toward high-reward solutions remains a non-trivial challenge. In this paper, we propose CMAB-GFN, which integrates a combinatorial multi-armed bandit (CMAB) framework with GFlowNet policies. The CMAB component prunes low-quality actions, yielding compact high-scoring subspaces for exploration. Restricting GFNs to these compact high-scoring subspaces accelerates the discovery of high-value candidates, while the exploration of different subspaces ensures that diversity is not sacrificed. Experimental results on multiple tasks demonstrate that CMAB-GFN generates higher-reward candidates than existing approaches.

</details>


### [167] [Partial GFlowNet: Accelerating Convergence in Large State Spaces via Strategic Partitioning](https://arxiv.org/abs/2602.11498)
*Xuan Yu,Xu Wang,Rui Zhu,Yudong Zhang,Yang Wang*

Main category: cs.LG

TL;DR: 本文提出限制生成流网络（GFlowNets）行动者探索范围的方法，能使模型在大状态空间中更快收敛，生成更高奖励和更多样的候选结果。


<details>
  <summary>Details</summary>
Motivation: 现有GFlowNets在大状态空间中自由探索时存在显著收敛挑战。

Method: 引入规划器将整个状态空间划分为重叠的部分状态空间，让行动者在其中高效识别高奖励子区域；采用启发式策略切换部分区域，避免行动者浪费时间。

Result: 在多个广泛使用的数据集上的实验表明，该模型在大状态空间中比现有方法收敛更快，生成的候选结果奖励更高且多样性显著提升。

Conclusion: 所提出的限制行动者探索的方法能有效解决GFlowNets在大状态空间中的收敛问题。

Abstract: Generative Flow Networks (GFlowNets) have shown promising potential to generate high-scoring candidates with probability proportional to their rewards. As existing GFlowNets freely explore in state space, they encounter significant convergence challenges when scaling to large state spaces. Addressing this issue, this paper proposes to restrict the exploration of actor. A planner is introduced to partition the entire state space into overlapping partial state spaces. Given their limited size, these partial state spaces allow the actor to efficiently identify subregions with higher rewards. A heuristic strategy is introduced to switch partial regions thus preventing the actor from wasting time exploring fully explored or low-reward partial regions. By iteratively exploring these partial state spaces, the actor learns to converge towards the high-reward subregions within the entire state space. Experiments on several widely used datasets demonstrate that \modelname converges faster than existing works on large state spaces. Furthermore, \modelname not only generates candidates with higher rewards but also significantly improves their diversity.

</details>


### [168] [A Generic Framework for Fair Consensus Clustering in Streams](https://arxiv.org/abs/2602.11500)
*Diptarka Chakraborty,Kushagra Chatterjee,Debarati Das,Tien-Long Nguyen*

Main category: cs.LG

TL;DR: 研究流式模型下的公平共识聚类，设计首个常数因子算法，提出通用框架并扩展到k - 中值共识聚类问题。


<details>
  <summary>Details</summary>
Motivation: Chakraborty等人的离线公平共识聚类方法存储所有输入聚类代价高，不适用于大规模应用，需要在流式模型下研究公平共识聚类。

Method: 设计首个常数因子算法处理流数据，仅存储对数数量的输入；引入新的通用算法框架，将最接近公平聚类与聚类拟合相结合。

Result: 提出的算法和框架在流式和离线场景下都有改进的近似保证，且框架对公平性定义不敏感。

Conclusion: 可以将方法扩展到更一般的k - 中值共识聚类问题。

Abstract: Consensus clustering seeks to combine multiple clusterings of the same dataset, potentially derived by considering various non-sensitive attributes by different agents in a multi-agent environment, into a single partitioning that best reflects the overall structure of the underlying dataset. Recent work by Chakraborty et al, introduced a fair variant under proportionate fairness and obtained a constant-factor approximation by naively selecting the best closest fair input clustering; however, their offline approach requires storing all input clusterings, which is prohibitively expensive for most large-scale applications.
  In this paper, we initiate the study of fair consensus clustering in the streaming model, where input clusterings arrive sequentially and memory is limited. We design the first constant-factor algorithm that processes the stream while storing only a logarithmic number of inputs. En route, we introduce a new generic algorithmic framework that integrates closest fair clustering with cluster fitting, yielding improved approximation guarantees not only in the streaming setting but also when revisited offline. Furthermore, the framework is fairness-agnostic: it applies to any fairness definition for which an approximately close fair clustering can be computed efficiently. Finally, we extend our methods to the more general k-median consensus clustering problem.

</details>


### [169] [Calibrating an Imperfect Auxiliary Predictor for Unobserved No-Purchase Choice](https://arxiv.org/abs/2602.11505)
*Jiangkai Xiong,Kalyan Talluri,Hanzhao Wang*

Main category: cs.LG

TL;DR: 论文针对企业缺失消费者外部选择信息问题，研究利用有偏差的外部选择概率预测器，开发校准方法将其转化为有效无购买估计，并分析对下游决策的影响，数值实验证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 企业通常无法获取关键消费者行为信息，现有方法依赖辅助数据，而本文研究利用有偏差的黑盒辅助预测器的情况。

Method: 提出两种校准方法，一是在对数空间仿射失准下用简单回归识别外选效用参数；二是在近单调条件下提出基于排名的校准方法并推导有限样本误差界。

Result: 数值实验表明改进了无购买估计和下游品类决策，还讨论了多预测器的稳健聚合扩展。

Conclusion: 所提出的校准方法能将有偏差预测转化为有效无购买估计，分析了估计误差对下游决策质量的影响，明确了各误差来源的主导情况。

Abstract: Firms typically cannot observe key consumer actions: whether customers buy from a competitor, choose not to buy, or even fully consider the firm's offer. This missing outside-option information makes market-size and preference estimation difficult even in simple multinomial logit (MNL) models, and it is a central obstacle in practice when only transaction data are recorded. Existing approaches often rely on auxiliary market-share, aggregated, or cross-market data. We study a complementary setting in which a black-box auxiliary predictor provides outside-option probabilities, but is potentially biased or miscalibrated because it was trained in a different channel, period, or population, or produced by an external machine-learning system. We develop calibration methods that turn such imperfect predictions into statistically valid no-purchase estimates using purchase-only data from the focal environment. First, under affine miscalibration in logit space, we show that a simple regression identifies outside-option utility parameters and yields consistent recovery of no-purchase probabilities without collecting new labels for no-purchase events. Second, under a weaker nearly monotone condition, we propose a rank-based calibration method and derive finite-sample error bounds that cleanly separate auxiliary-predictor quality from first-stage utility-learning error over observed in-set choices. Our analysis also translates estimation error into downstream decision quality for assortment optimization, quantifying how calibration accuracy affects revenue performance. The bounds provide explicit dependence on predictor alignment and utility-learning error, clarifying when each source dominates. Numerical experiments demonstrate improvements in no-purchase estimation and downstream assortment decisions, and we discuss robust aggregation extensions for combining multiple auxiliary predictors.

</details>


### [170] [Unifying Stable Optimization and Reference Regularization in RLHF](https://arxiv.org/abs/2602.11523)
*Li He,Qiang Qu,He Zhao,Stephen Wan,Dadong Wang,Lina Yao,Tongliang Liu*

Main category: cs.LG

TL;DR: 提出统一正则化方法平衡防止奖励破解和稳定策略更新目标，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前RLHF存在奖励破解和稳定优化两大挑战，且同时对$π_0$和$π_t$正则化的隐式权衡未充分研究。

Method: 引入统一正则化方法，得到加权监督微调损失，明确平衡目标。

Result: 实验表明该方法在不同基准测试中始终优于RLHF和在线偏好学习方法。

Conclusion: 此统一方法能提升对齐性能和稳定性，改善对齐结果和实现复杂度。

Abstract: Reinforcement Learning from Human Feedback (RLHF) has advanced alignment capabilities significantly but remains hindered by two core challenges: \textbf{reward hacking} and \textbf{stable optimization}. Current solutions independently address these issues through separate regularization strategies, specifically a KL-divergence penalty against a supervised fine-tuned model ($π_0$) to mitigate reward hacking, and policy ratio clipping towards the current policy ($π_t$) to promote stable alignment. However, the implicit trade-off arising from simultaneously regularizing towards both $π_0$ and $π_t$ remains under-explored. In this paper, we introduce a unified regularization approach that explicitly balances the objectives of preventing reward hacking and maintaining stable policy updates. Our simple yet principled alignment objective yields a weighted supervised fine-tuning loss with a superior trade-off, which demonstrably improves both alignment results and implementation complexity. Extensive experiments across diverse benchmarks validate that our method consistently outperforms RLHF and online preference learning methods, achieving enhanced alignment performance and stability.

</details>


### [171] [Adaptive Milestone Reward for GUI Agents](https://arxiv.org/abs/2602.11524)
*Congmin Zheng,Xiaoyun Mo,Xinbei Ma,Qiqiang Lin,Yin Zhao,Jiachen Zhu,Xingyu Lou,Jun Wang,Zhaoxiang Wang,Weiwen Liu,Zhuosheng Zhang,Yong Yu,Weinan Zhang*

Main category: cs.LG

TL;DR: 针对强化学习中奖励保真度和密度的权衡问题，提出ADMIRE机制，实验显示能提升成功率且泛化性强


<details>
  <summary>Details</summary>
Motivation: 强化学习在长时程任务的时间信用分配上有问题，奖励保真度和密度难以权衡

Method: 提出ADMIRE机制，通过将轨迹锚定到动态提炼的里程碑来构建可验证、自适应的奖励系统，并集成非对称信用分配策略

Result: 在AndroidWorld上不同基础模型的成功率有超10%的绝对提升，在多种RL算法和异构环境中表现良好

Conclusion: ADMIRE机制有效解决奖励权衡问题，具有良好性能和泛化性

Abstract: Reinforcement Learning (RL) has emerged as a mainstream paradigm for training Mobile GUI Agents, yet it struggles with the temporal credit assignment problem inherent in long-horizon tasks. A primary challenge lies in the trade-off between reward fidelity and density: outcome reward offers high fidelity but suffers from signal sparsity, while process reward provides dense supervision but remains prone to bias and reward hacking. To resolve this conflict, we propose the Adaptive Milestone Reward (ADMIRE) mechanism. ADMIRE constructs a verifiable, adaptive reward system by anchoring trajectory to milestones, which are dynamically distilled from successful explorations. Crucially, ADMIRE integrates an asymmetric credit assignment strategy that denoises successful trajectories and scaffolds failed trajectories. Extensive experiments demonstrate that ADMIRE consistently yields over 10% absolute improvement in success rate across different base models on AndroidWorld. Moreover, the method exhibits robust generalizability, achieving strong performance across diverse RL algorithms and heterogeneous environments such as web navigation and embodied tasks.

</details>


### [172] [PASCAL: A Phase-Aware Scheduling Algorithm for Serving Reasoning-based Large Language Models](https://arxiv.org/abs/2602.11530)
*Eunyeong Cho,Jehyeon Bang,Ranggi Hwang,Minsoo Rhu*

Main category: cs.LG

TL;DR: 现有LLM服务框架在处理推理型大模型时性能不佳，提出PASCAL调度算法，可降低TTFT并保持QoE，在基准测试中效果显著。


<details>
  <summary>Details</summary>
Motivation: 推理型大语言模型采用CoT推理带来服务挑战，现有LLM服务框架无法区分推理和回答阶段，在GPU内存受限下性能下降。

Method: 提出PASCAL相位感知调度算法，优先处理推理以降低TTFT，在回答阶段使用可控抢占和令牌 pacing 来保证QoE；采用分层调度器结合实例级放置和实例内执行，并在阶段边界进行动态迁移。

Result: 在使用DeepSeek - R1 - Distill - Qwen - 32B的基准测试中，PASCAL最多可将尾部TTFT降低72%，并保持回答阶段SLO达标。

Conclusion: 相位感知调度对于基于推理的大语言模型部署很重要。

Abstract: The emergence of reasoning-based LLMs leveraging Chain-of-Thought (CoT) inference introduces new serving challenges, as their extended reasoning phases delay user-visible output and inflate Time-To-First-Token (TTFT). Existing LLM serving frameworks fail to distinguish between reasoning and answering phases, leading to performance degradation under GPU memory constraints. We present PASCAL, a phase-aware scheduling algorithm that prioritizes reasoning to reduce TTFT while using controlled preemption and token pacing during answering to preserve Quality-of-Experience (QoE). Our hierarchical scheduler combines instance-level placement with intra-instance execution and enables dynamic migration at phase boundaries to balance load and reduce interference. Across benchmarks using DeepSeek-R1-Distill-Qwen-32B, PASCAL reduces tail TTFT by up to 72% while maintaining answering phase SLO attainment, demonstrating the importance of phase-aware scheduling for reasoning-based LLM deployment.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [173] [Evolution With Purpose: Hierarchy-Informed Optimization of Whole-Brain Models](https://arxiv.org/abs/2602.11398)
*Hormoz Shahrzad,Niharika Gajawell,Kaitlin Maile,Manish Saggar,Risto Miikkulainen*

Main category: cs.NE

TL;DR: 本文探讨用生物知识引导进化搜索优化全脑动态平均场模型，对比不同策略，发现遵循脑网络层次的课程式方法（HICO）能泛化到新对象并可预测行为能力，展示领域知识在优化中的作用。


<details>
  <summary>Details</summary>
Motivation: 标准进化方法在大尺度生物物理脑模型优化中存在过拟合个体对象、预测能力有限的问题，研究用生物知识引导进化是否有帮助。

Method: 聚焦全脑动态平均场模型，对比参数共享的基线模型和不同脑区用不同参数集的异质模型，用四种策略优化异质模型，包括一次性优化、遵循脑网络层次的课程式方法（HICO）、反向课程式方法和随机打乱的课程式方法。

Result: 所有异质策略都能很好拟合数据，但只有课程式方法能泛化到新对象，且只有 HICO 能用参数集预测对象的行为能力。

Conclusion: 通过用大脑区域层次的生物知识引导进化，HICO 展示了如何利用领域知识服务于现实世界的优化。

Abstract: Evolutionary search is well suited for large-scale biophysical brain modeling, where many parameters with nonlinear interactions and no tractable gradients need to be optimized. Standard evolutionary approaches achieve an excellent fit to MRI data; however, among many possible such solutions, it finds ones that overfit to individual subjects and provide limited predictive power. This paper investigates whether guiding evolution with biological knowledge can help. Focusing on whole-brain Dynamic Mean Field (DMF) models, a baseline where 20 parameters were shared across the brain was compared against a heterogeneous formulation where different sets of 20 parameters were used for the seven canonical brain regions. The heterogeneous model was optimized using four strategies: optimizing all parameters at once, a curricular approach following the hierarchy of brain networks (HICO), a reversed curricular approach, and a randomly shuffled curricular approach. While all heterogeneous strategies fit the data well, only curricular approaches generalized to new subjects. Most importantly, only HICO made it possible to use the parameter sets to predict the subjects' behavioral abilities as well. Thus, by guiding evolution with biological knowledge about the hierarchy of brain regions, HICO demonstrated how domain knowledge can be harnessed to serve the purpose of optimization in real-world domains.

</details>


### [174] [Energy-Aware Spike Budgeting for Continual Learning in Spiking Neural Networks for Neuromorphic Vision](https://arxiv.org/abs/2602.12236)
*Anika Tabassum Meem,Muntasir Hossain Nadid,Md Zesun Ahmed Mia*

Main category: cs.NE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Neuromorphic vision systems based on spiking neural networks (SNNs) offer ultra-low-power perception for event-based and frame-based cameras, yet catastrophic forgetting remains a critical barrier to deployment in continually evolving environments. Existing continual learning methods, developed primarily for artificial neural networks, seldom jointly optimize accuracy and energy efficiency, with particularly limited exploration on event-based datasets. We propose an energy-aware spike budgeting framework for continual SNN learning that integrates experience replay, learnable leaky integrate-and-fire neuron parameters, and an adaptive spike scheduler to enforce dataset-specific energy constraints during training. Our approach exhibits modality-dependent behavior: on frame-based datasets (MNIST, CIFAR-10), spike budgeting acts as a sparsity-inducing regularizer, improving accuracy while reducing spike rates by up to 47\%; on event-based datasets (DVS-Gesture, N-MNIST, CIFAR-10-DVS), controlled budget relaxation enables accuracy gains up to 17.45 percentage points with minimal computational overhead. Across five benchmarks spanning both modalities, our method demonstrates consistent performance improvements while minimizing dynamic power consumption, advancing the practical viability of continual learning in neuromorphic vision systems.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [175] [SAFuzz: Semantic-Guided Adaptive Fuzzing for LLM-Generated Code](https://arxiv.org/abs/2602.11209)
*Ziyi Yang,Kalit Inani,Keshav Kabra,Vima Gupta,Anand Padmanabha Iyer*

Main category: cs.SE

TL;DR: 现有测试框架难适配AI生成代码，提出SAFuzz混合测试框架，提升漏洞检测精度、降低时间成本，与单元测试结合有互补效果。


<details>
  <summary>Details</summary>
Motivation: 当前测试框架难以跟上AI生成代码的数量，传统模糊测试技术资源分配均匀、缺乏语义感知，导致资源使用低效和漏洞遗漏。

Method: 提出混合测试框架SAFuzz，集成基于提示的行为多样化、利用特定问题预言机的线束生成和基于大语言模型的预测器，实现自适应资源分配和动态早期停止。

Result: 在CSES算法问题上，将漏洞判别精度从77.9%提高到85.7%，时间成本比SOTA GreenFuzz降低1.71倍，与现有单元测试生成方法结合使漏洞检测召回率从67.3%提高到79.5%。

Conclusion: SAFuzz能有效检测算法漏洞，与现有单元测试生成方法结合有互补优势。

Abstract: While AI-coding assistants accelerate software development, current testing frameworks struggle to keep pace with the resulting volume of AI-generated code. Traditional fuzzing techniques often allocate resources uniformly and lack semantic awareness of algorithmic vulnerability patterns, leading to inefficient resource usage and missed vulnerabilities. To address these limitations, we present a hybrid testing framework that leverages LLM-guided adaptive fuzzing to detect algorithmic vulnerabilities efficiently. Our system SAFuzz integrates prompt-based behavioral diversification, harness generation with problem-specific oracles, and an LLM-based predictor to enable adaptive resource allocation and dynamic early stopping. Evaluating SAFuzz on CSES algorithmic problems, we improve vulnerability discrimination precision from 77.9% to 85.7% and achieve a 1.71x reduction in time cost compared to SOTA GreenFuzz while maintaining comparable recall. We further observe that combining our approach with existing unit test generation methods yields complementary gains, increasing the bug detection recall from 67.3% to 79.5%.

</details>


### [176] [SWE-MiniSandbox: Container-Free Reinforcement Learning for Building Software Engineering Agents](https://arxiv.org/abs/2602.11210)
*Danlong Yuan,Wei Wu,Zhengren Wang,Xueliang Zhao,Huishuai Zhang,Dongyan Zhao*

Main category: cs.SE

TL;DR: 提出SWE - MiniSandbox，一种轻量级无容器方法用于SWE代理的可扩展强化学习训练，降低开销且性能相当。


<details>
  <summary>Details</summary>
Motivation: 现有依赖容器隔离的强化学习训练管道存在存储开销大、环境设置慢和需容器管理权限等问题。

Method: 使用内核级机制支持的隔离工作区执行任务，利用轻量级环境预缓存技术。

Result: 将磁盘使用量降至基于容器管道的约5%，环境准备时间降至约25%，评估性能与基于容器的管道相当。

Conclusion: SWE - MiniSandbox去除对重型容器基础设施的依赖，为扩展基于强化学习的SWE代理提供实用基础，尤其适用于资源受限的研究环境。

Abstract: Reinforcement learning (RL) has become a key paradigm for training software engineering (SWE) agents, but existing pipelines typically rely on per-task containers for isolation. At scale, pre-built container images incur substantial storage overhead, slow environment setup, and require container-management privileges. We propose SWE-MiniSandbox, a lightweight, container-free method that enables scalable RL training of SWE agents without sacrificing isolation. Instead of relying on per-instance containers, SWE-MiniSandbox executes each task in an isolated workspace backed by kernel-level mechanisms, substantially reducing system overhead. It leverages lightweight environment pre-caching techniques to eliminate the need for bulky container images. As a result, our approach lowers disk usage to approximately 5\% of that required by container-based pipelines and reduces environment preparation time to about 25\% of the container baseline. Empirical results demonstrate that SWE-MiniSandbox achieves evaluation performance comparable to standard container-based pipelines. By removing the dependency on heavy container infrastructure, SWE-MiniSandbox offers a practical and accessible foundation for scaling RL-based SWE agents, particularly in resource-constrained research environments.

</details>


### [177] [Patient Digital Twins for Chronic Care: Technical Hurdles, Lessons Learned, and the Road Ahead](https://arxiv.org/abs/2602.11223)
*Micheal P. Papazoglou,Bernd J. Krämer,Mira Raheem,Amal Elgammal*

Main category: cs.SE

TL;DR: 慢性病负担重，现有医疗系统不足，介绍患者医疗数字双胞胎（PMDTs）早期实施，分析可行性、挑战与技术收获，给出行动见解和发展机会。


<details>
  <summary>Details</summary>
Motivation: 慢性病是全球发病、死亡和医疗成本的主要负担，现有健康系统碎片化且被动，PMDTs 可带来范式转变。

Method: 通过本体驱动建模和联合分析试点进行 PMDTs 早期实施。

Result: QUALITOP 肿瘤学研究和分布式 AI 平台证实了 PMDTs 的可行性与挑战，也有技术收获。

Conclusion: 为软件工程师提供行动见解，指出如 DSLs 和模型驱动工程等推进 PMDTs 发展的机会。

Abstract: Chronic diseases constitute the principal burden of morbidity, mortality, and healthcare costs worldwide, yet current health systems remain fragmented and predominantly reactive. Patient Medical Digital Twins (PMDTs) offer a paradigm shift: holistic, continuously updated digital counterparts of patients that integrate clinical, genomic, lifestyle, and quality-of-life data. We report early implementations of PMDTs via ontology-driven modeling and federated analytics pilots. Insights from the QUALITOP oncology study and a distributed AI platform confirm both feasibility and challenges: aligning with HL7 FHIR and OMOP standards, embedding privacy governance, scaling federated queries, and designing intuitive clinician interfaces. We also highlight technical gains, such as automated reasoning over multimodal blueprints and predictive analytics for patient outcomes. By reflecting on these experiences, we outline actionable insights for software engineers and identify opportunities, such as DSLs and model-driven engineering, to advance PMDTs toward trustworthy, adaptive chronic care ecosystems.

</details>


### [178] [Agent-Diff: Benchmarking LLM Agents on Enterprise API Tasks via Code Execution with State-Diff-Based Evaluation](https://arxiv.org/abs/2602.11224)
*Hubert M. Pysklo,Artem Zhuravel,Patrick D. Watson*

Main category: cs.SE

TL;DR: 提出Agent - Diff基准框架评估能通过外部API执行代码的代理式大语言模型，介绍框架创新点并进行多模型多任务基准测试及消融实验。


<details>
  <summary>Details</summary>
Motivation: 代理式大语言模型性能受多种因素影响，现有基准测试在沙盒方法和生态有效方法间需权衡，需新框架评估。

Method: 提出状态差异合约分离过程与结果，定义任务成功标准；创建标准化脚本层沙盒供模型调用外部API；用框架对9个大语言模型在224个任务上进行基准测试，开展消融实验。

Result: 完成对9个大语言模型在224个使用企业软件工作流任务上的基准测试，进行了框架鲁棒性消融实验。

Conclusion: Agent - Diff框架可在统一沙盒下按标准化合约评估不同代理式大语言模型在真实服务接口上的性能。

Abstract: We present Agent-Diff, a novel benchmarking framework for evaluating agentic Large Language Models (LLMs) on real-world tasks that execute code via external APIs. Agentic LLM performance varies due to differences in models, external tool access, prompt structures, and agentic frameworks. Benchmarks must make fundamental trade-offs between a sandboxed approach that controls for variation in software environments and more ecologically valid approaches employing real services. Agent-Diff attempts to capture the desirable features of both of these approaches by including access to the real API interfaces for software services while sandboxing the environment in which calls are made, processed, and evaluated. This approach relies on two key innovations. The first is a novel state-diff contract, which separates process from outcome - rather than fuzzy trace or parameter matching, we define task success as whether the expected change in environment state was achieved. The second is a novel sandbox that provides a standardized scripting layer that all models use to execute code against external APIs (Slack, Box, Linear, Google Calendar). Thus, we can evaluate different agentic LLMs against a standardized set of contracts using a unified sandbox while still evaluating their performance on real-world service interfaces. Using the Agent-Diff framework, we provide benchmarks for nine LLMs across 224 tasks utilizing enterprise software workflows. In addition, we evaluate the robustness of the framework with ablation experiments to assess the contribution of access to API documentation on benchmark performance. Code and data: https://github.com/agent-diff-bench/agent-diff.

</details>


### [179] [Improving the Robustness of Large Language Models for Code Tasks via Fine-tuning with Perturbed Data](https://arxiv.org/abs/2602.11411)
*Yang Liu,Armstrong Foundjem,Xingfang Wu,Heng Li,Foutse Khomh*

Main category: cs.SE

TL;DR: 研究通过微调大语言模型（LLM）来提高编码任务中模型对潜在对抗输入的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 软件开发中，大语言模型处理多样输入时可能存在脆弱性，确保其鲁棒性很关键。

Method: 系统评估LLM鲁棒性，分别用字符级、单词级和句子级扰动的数据集进行微调，并与基础模型和未扰动数据集微调的模型对比。

Result: 使用扰动数据集微调显著提高模型鲁棒性，但性能略有下降，不过也有时会有性能提升。

Conclusion: 用扰动数据微调LLM能有效增强鲁棒性，但会有轻微性能损失，需平衡鲁棒性和性能。

Abstract: Context: In the fast-paced evolution of software development, Large Language Models (LLMs) have become indispensable tools for tasks such as code generation, completion, analysis, and bug fixing. Ensuring the robustness of these models against potential vulnerabilities from handling diverse inputs is critical, as variations in input can lead to incorrect or insecure code outputs.
  Objective: This work aims to improve the robustness of LLMs for coding-related tasks against potential adversarial inputs. Specifically, we investigate how fine-tuning LLMs with perturbed datasets impacts their robustness against input perturbations.
  Method: We systematically evaluated LLM robustness by fine-tuning models using datasets perturbed at character-level, word-level, and sentence-level, comparing results against base models and models fine-tuned on unperturbed datasets.
  Results: Fine-tuning LLMs with perturbed datasets significantly improves model robustness (RD usually drops around 4\% - 6\%), especially for models with relatively weak robustness. However, this fine-tuning process typically results in a slight performance decrease (pass@1 usually drops around 1\% - 3\%) compared to fine-tuning with unperturbed datasets, although occasional performance improvements are observed.
  Conclusion \& Implications: Fine-tuning LLMs for coding tasks with perturbed data effectively enhances their robustness at the cost of a minor performance reduction, emphasizing the importance of balancing the robustness and performance of LLMs for coding applications.

</details>


### [180] [A Grounded Theory of Debugging in Professional Software Engineering Practice](https://arxiv.org/abs/2602.11435)
*Haolin Li,Michael Coblenz*

Main category: cs.SE

TL;DR: 本文采用扎根理论方法研究专业开发者调试策略，提出调试是结构化迭代诊断过程的理论，对工具设计和教育有启示。


<details>
  <summary>Details</summary>
Motivation: 以往研究缺乏对专业开发者在大型真实代码库中调试推理方式的理论解释，旨在填补此空白。

Method: 采用扎根理论方法，观察7名专业开发者和5名专业直播编码者在自有代码库中的17个调试任务。

Result: 调试是结构化迭代诊断过程，开发者通过导航和执行策略交替收集信息，运用前后追踪推理模式并依情况调整，还收集外部资源，经验助其构建心智模型。

Conclusion: 提出了专业调试的扎根理论，体现该实践以人类为中心的维度，对工具设计和软件工程教育有意义。

Abstract: Debugging is a central yet complex activity in software engineering. Prior studies have documented debugging strategies and tool usage, but little theory explains how experienced developers reason about bugs in large, real-world codebases. We conducted a qualitative study using a grounded theory approach. We observed seven professional developers and five professional live-coding streamers working on 17 debugging tasks in their own codebases, capturing diverse contexts of debugging. We theorize debugging as a structured, iterative diagnostic process in which programmers update a mental model of the system to guide information gathering. Developers gather information by alternating between navigation and execution strategies, employing forward and backward tracing modes of reasoning and adapting these approaches according to codebase context, complexity, and familiarity. Developers also gather external resources to complement code-based evidence, with their experience enabling them to systematically construct a mental model. We contribute a grounded theory of professional debugging that surfaces the human-centered dimensions of the practice, with implications for tool design and software engineering education.

</details>


### [181] [Addressing OSS Community Managers' Challenges in Contributor Retention](https://arxiv.org/abs/2602.11447)
*Zixuan Feng,Katie Kimura,Bianca Trinkenreich,Igor Steinmacher,Marco Gerosa,Anita Sarma*

Main category: cs.SE

TL;DR: 开源软件社区管理者在保留贡献者方面面临挑战，现有工具不足，本文用混合方法设计解决方案、开发原型并评估，提供见解、策略和框架。


<details>
  <summary>Details</summary>
Motivation: 当前用于管理贡献者保留的工具只能提供回顾性见解，无法提前识别潜在的退出情况，需要新的解决方案来支持保留管理。

Method: 遵循设计科学研究范式，采用混合方法，包括半结构化访谈、多视角文献综述、社区调查，通过迭代构建 - 评估周期开发和完善策略，开发基于网络的原型并纳入反馈，进行实地评估。

Result: 提供了关于开源软件贡献者保留管理挑战的实证见解、支持管理者保留工作的可行策略，以及用于未来研究开源软件可持续性理论的实用框架。

Conclusion: 所提出的方法和成果对开源软件贡献者保留管理及相关研究具有重要意义。

Abstract: Open-source software (OSS) community managers face significant challenges in retaining contributors, as they must monitor activity and engagement while navigating complex dynamics of collaboration. Current tools designed for managing contributor retention (e.g., dashboards) fall short by providing retrospective rather than predictive insights to identify potential disengagement early. Without understanding how to anticipate and prevent disengagement, new solutions risk burdening community managers rather than supporting retention management. Following the Design Science Research paradigm, we employed a mixed-methods approach for problem identification and solution design to address contributor retention. To identify the challenges hindering retention management in OSS, we conducted semi-structured interviews, a multi-vocal literature review, and community surveys. Then through an iterative build-evaluate cycle, we developed and refined strategies for diagnosing retention risks and informing engagement efforts. We operationalized these strategies into a web-based prototype, incorporating feedback from 100+ OSS practitioners, and conducted an in situ evaluation across two OSS communities. Our study offers (1) empirical insights into the challenges of contributor retention management in OSS, (2) actionable strategies that support OSS community managers' retention efforts, and (3) a practical framework for future research in developing or validating theories about OSS sustainability.

</details>


### [182] [Search-Based Quantum Program Testing via Commuting Pauli String](https://arxiv.org/abs/2602.11487)
*Asmar Muqeet,Shaukat Ali,Paolo Arcaini*

Main category: cs.SE

TL;DR: 提出SB - QOPS搜索式量子程序测试方法，通过大规模实验证明其优于QOPS且有跨平台可移植性。


<details>
  <summary>Details</summary>
Motivation: 现有量子软件测试方法依赖简单输入、统计预言机、昂贵程序规范，在真实量子计算机上验证有限，需改进。

Method: 提出SB - QOPS方法，以泡利字符串重新定义测试用例，引入以测量为中心的预言机，用基于期望值的适应度函数探索搜索空间。

Result: 在最多29个量子比特的量子电路上实验，评估三种搜索策略，结果显示SB - QOPS显著优于QOPS，对最多29个量子比特的电路故障检测得分达100%。

Conclusion: SB - QOPS能有效测试量子程序，减少对完整程序规范的需求，提高测试预算利用率，且具有跨量子平台的可移植性。

Abstract: Quantum software testing is important for reliable quantum software engineering. Despite recent advances, existing quantum software testing approaches rely on simple test inputs and statistical oracles, costly program specifications, and limited validation on real quantum computers. To address these challenges, we propose SB-QOPS, a search-based quantum program testing approach via commuting Pauli strings. SB-QOPS, as a direct extension to a previously proposed QOPS approach, redefines test cases in terms of Pauli strings and introduces a measurement-centric oracle that exploits their commutation properties, enabling effective testing of quantum programs while reducing the need for full program specifications. By systematically exploring the search space through an expectation-value-based fitness function, SB-QOPS improves test budget utilization and increases the likelihood of uncovering subtle faults. We conduct a large-scale empirical evaluation on quantum circuits of up to 29 qubits on real quantum computers and emulators. We assess three search strategies: Genetic Algorithm, Hill Climbing, and the (1+1) Evolutionary Algorithm, and evaluate SB-QOPS under both simulated and real noisy conditions. Experiments span three quantum computing platforms: IBM, IQM, and Quantinuum. Results show that SB-QOPS significantly outperforms QOPS, achieving a fault-detection score of 100% for circuits up to 29 qubits, and demonstrating portability across quantum platforms.

</details>


### [183] [How Smart Is Your GUI Agent? A Framework for the Future of Software Interaction](https://arxiv.org/abs/2602.11514)
*Sidong Feng,Chunyang Chen*

Main category: cs.SE

TL;DR: 提出GUI Agent Autonomy Levels (GAL)框架以明确GUI代理自主性程度


<details>
  <summary>Details</summary>
Motivation: 当前‘代理’自主性描述差异大，掩盖能力、责任和风险，缺乏概念清晰度

Method: 构建一个六级框架GAL

Result: 创建了GAL框架

Conclusion: GAL框架能使自主性明确，有助于衡量向可信软件交互的进展

Abstract: GUI agents are rapidly becoming a new interaction to software, allowing people to navigate web, desktop and mobile rather than execute them click by click. Yet ``agent'' is described with radically different degrees of autonomy, obscuring capability, responsibility and risk. We call for conceptual clarity through GUI Agent Autonomy Levels (GAL), a six-level framework that makes autonomy explicit and helps benchmark progress toward trustworthy software interaction.

</details>


### [184] [Do Not Treat Code as Natural Language: Implications for Repository-Level Code Generation and Beyond](https://arxiv.org/abs/2602.11671)
*Minh Le-Anh,Huyen Nguyen,Khanh An Tran,Nam Le Hai,Linh Ngo Van,Nghi D. Q. Bui,Bach Le*

Main category: cs.SE

TL;DR: 现有代码大语言模型在仓库级代码生成效果不佳，本文介绍Hydra框架，能处理代码结构与依赖，实验显示其在相关基准测试中表现达最优。


<details>
  <summary>Details</summary>
Motivation: 现有代码大语言模型在仓库级任务中，因现有检索增强生成（RAG）方法的局限，效果变差。

Method: 提出Hydra框架，有结构感知索引策略、轻量级依赖感知检索器及混合检索机制。

Result: 在DevEval和RepoExec基准测试中，Hydra达到最优性能，Pass@1超最强基线5%以上，小模型也能表现很出色。

Conclusion: Hydra在仓库级代码生成上建立了新的最优水平。

Abstract: Large language models for code (CodeLLMs) have demonstrated remarkable success in standalone code completion and generation, sometimes even surpassing human performance, yet their effectiveness diminishes in repository-level settings where cross-file dependencies and structural context are essential. Existing Retrieval-Augmented Generation (RAG) approaches often borrow strategies from NLP, relying on chunking-based indexing and similarity-based retrieval. Chunking results in the loss of coherence between code units and overlooks structural relationships, while similarity-driven methods frequently miss functionally relevant dependencies such as helper functions, classes, or global variables. To address these limitations, we present Hydra, a repository-level code generation framework that treats code as structured code rather than natural language. Our approach introduces (i) a structure-aware indexing strategy that represents repositories as hierarchical trees of functions, classes, and variables, preserving code structure and dependencies, (ii) a lightweight dependency-aware retriever (DAR) that explicitly identifies and retrieves the true dependencies required by a target function, and (iii) a hybrid retrieval mechanism that combines DAR with similarity-based retrieval to provide both essential building blocks and practical usage examples. Extensive experiments on the challenging DevEval and RepoExec benchmarks, both requiring function implementation from real-world repositories with complex large repository context, show that Hydra achieves state-of-the-art performance across open- and closed-source CodeLLMs. Notably, our method establishes a new state of the art in repository-level code generation, surpassing strongest baseline by over 5% in Pass@1 and even enabling smaller models to match or exceed the performance of much larger ones that rely on existing retrievers.

</details>


### [185] [Beyond Code: Empirical Insights into How Team Dynamics Influence OSS Project Selection](https://arxiv.org/abs/2602.11692)
*Shashiwadana Nirmani,Hourieh Khalajzadeh,Mojtaba Shahin,Xiao Liu*

Main category: cs.SE

TL;DR: 现有OSS项目推荐系统忽视协作与社区因素，研究调查团队动态对项目选择的影响及不同动机贡献者的偏好差异，结果表明沟通相关团队动态受重视且重要性因动机而异，研究成果可用于设计更优推荐系统。


<details>
  <summary>Details</summary>
Motivation: 当前OSS项目推荐系统主要关注技术属性，忽略协作和社区方面影响，本研究旨在探究OSS社区团队动态对项目选择的影响及不同动机贡献者的偏好差异。

Method: 对198名OSS从业者进行在线调查，结合定量和定性分析来捕捉贡献者对团队动态的看法。

Result: 沟通相关团队动态受从业者一致重视，但重要性因贡献者动机而异，如追求声誉或社交的从业者偏好鼓励多元参与的项目社区。

Conclusion: 了解团队动态与贡献者动机的契合度能为从业者项目选择行为提供有价值的见解，可用于设计更考虑社会协作质量和动机匹配的项目推荐系统。

Abstract: Open-source software (OSS) development relies on effective collaboration among distributed contributors. Yet, current OSS project recommendation systems primarily emphasize technical attributes, overlooking the collaboration and community aspects that influence contributors' decisions to join and remain in projects. This study investigates how team dynamics within OSS communities influence project selection and how these preferences vary across contributors' motivations. We conducted an online survey with 198 OSS practitioners, combining quantitative and qualitative analyses to capture contributors' perceptions of team dynamics. The results reveal that communication-related team dynamics such as responsiveness, tone, and clarity of replies are consistently prioritized across practitioners. However, the relative importance of these team dynamics differs according to contributors' motivations. For instance, practitioners motivated by gaining reputation or networking preferred inclusive project communities that encouraged diverse participation. These findings highlight that understanding how team dynamics align with contributors' motivations provides valuable insights into practitioners' project selection behaviour. Those insights can inform the design of future human-aware project recommendation systems that better account for social collaboration quality and motivational fit.

</details>


### [186] [WebTestPilot: Agentic End-to-End Web Testing against Natural Language Specification by Inferring Oracles with Symbolized GUI Elements](https://arxiv.org/abs/2602.11724)
*Xiwen Teoh,Yun Lin,Duc-Minh Nguyen,Ruofei Ren,Wenjie Zhang,Jin Song Dong*

Main category: cs.SE

TL;DR: 现有基于大语言模型的方法在端到端网页测试中存在问题，本文提出WebTestPilot解决挑战，在测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型代理用于端到端网页测试时，语言模型存在幻觉问题，难以区分是幻觉还是真实应用程序错误，且现有方法无法捕捉隐式预言机。

Method: 引入WebTestPilot，使用符号化层将关键GUI元素符号化，并将自然语言规范转换为带预条件和后条件的步骤序列作为预言机，同时构建含漏洞网页应用的基准测试。

Result: WebTestPilot任务完成率达99%，漏洞检测精度和召回率均为96%，优于最佳基线。

Conclusion: WebTestPilot解决了网页测试存在的问题，能处理多样化自然语言输入和不同模型规模。

Abstract: Visual language model (VLM) agents show great promise in automating end-to-end (E2E) web testing against requirements in natural language. However, the probabilistic nature of language models can have inherent hallucinations. Therefore, given a detected inconsistency between the requirement and the web application, it is hard to distinguish whether it stems from the hallucination or a real application bug. Addressing this issue presents two core technical challenges: the implicit oracle inference challenge, where the agent must act as its own oracle to implicitly decide if the application's behavior is correct without guidance, and the probabilistic inference challenge, where an LLM's inconsistent reasoning undermines its trustworthiness as an oracle. Existing LLM-based approaches fail to capture such implicit oracles, either by treating any page navigation that doesn't crash as a success, or by checking each state in isolation, thus missing bugs dependent on context from prior steps.
  We introduce WebTestPilot, an LLM-based agent designed to address these challenges. WebTestPilot uses (1) a symbolization layer which detects and symbolizes critical GUI elements on the web application into symbols (i.e., variables) and (2) translates natural language specification into a sequence of steps, each of which is equipped with inferred pre- and post-conditions over the symbols as an oracle. This oracle captures data, temporal, and causal dependencies, enabling the validation of implicit requirements. To advance research in this area, we build a benchmark of bug-injected web apps for evaluating NL-to-E2E testing. The results show that WebTestPilot achieves a task completion rate of 99%, with 96% precision and 96% recall in bug detection, outperforming the best baseline (+70 precision, +27 recall). The agent generalizes across diverse natural language inputs and model scales.

</details>


### [187] [Leveraging Language Models to Discover Evidence-Based Actions for OSS Sustainability](https://arxiv.org/abs/2602.11746)
*Nafiz Imtiaz Khan,Vladimir Filkov*

Main category: cs.SE

TL;DR: 利用LLMs从软件工程文献中挖掘可操作建议，将分散研究成果转化为指导开源软件项目可持续性的结构化行动。


<details>
  <summary>Details</summary>
Motivation: 现有开源软件可持续性预测模型无法为维护者提供可操作建议，而软件工程研究积累了大量改善项目健康的具体实践证据未被充分利用。

Method: 设计RAG - pipeline和两层提示策略，从软件工程文献中提取可操作建议（ReACTs），筛选并组织这些建议。

Result: 得到1922条ReACTs，其中1312条通过严格质量标准，可与工具中的项目信号关联。

Conclusion: 提出了一种可重现、可扩展的方法，将分散研究成果转化为基于证据的结构化行动，指导开源软件项目实现可持续性。

Abstract: When successful, Open Source Software (OSS) projects create enormous value, but most never reach a sustainable state. Recent work has produced accurate models that forecast OSS sustainability, yet these models rarely tell maintainers what to do: their features are often high-level socio-technical signals that are not directly actionable. Decades of empirical software engineering research have accumulated a large but underused body of evidence on concrete practices that improve project health.
  We close this gap by using LLMs as evidence miners over the SE literature. We design a RAG-pipeline and a two-layer prompting strategy that extract researched actionables (ReACTs): concise, evidence-linked recommendations mapping to specific OSS practices. In the first layer, we systematically explore open LLMs and prompting techniques, selecting the best-performing combination to derive candidate ReACTs from 829 ICSE and FSE papers. In the second layer, we apply follow-up prompting to filter hallucinations, extract impact and evidence, and assess soundness and precision.
  Our pipeline yields 1,922 ReACTs, of which 1,312 pass strict quality criteria and are organized into practice-oriented categories connectable to project signals from tools like APEX. The result is a reproducible, scalable approach turning scattered research findings into structured, evidence-based actions guiding OSS projects toward sustainability.

</details>


### [188] [AmbiBench: Benchmarking Mobile GUI Agents Beyond One-Shot Instructions in the Wild](https://arxiv.org/abs/2602.11750)
*Jiazheng Sun,Mingxuan Li,Yingying Zhang,Jiayang Niu,Yachen Wu,Ruihan Jin,Shuyu Lei,Pengrongrui Tan,Zongyu Zhang,Ruoyi Wang,Jiachen Yang,Boyu Yang,Jiacheng Liu,Xin Peng*

Main category: cs.SE

TL;DR: 现有移动GUI代理基准存在局限，本文引入AmbiBench和MUSE来评估代理能力，重新定义评估标准。


<details>
  <summary>Details</summary>
Motivation: 现有基准假定用户指令完整明确，忽视了代理的意图对齐能力，需要改进。

Method: 引入含指令清晰度分类的AmbiBench，分为四个清晰度等级，构建240个任务的数据集，还开发了用MLLM-as-a-judge架构的MUSE自动评估框架。

Result: 揭示了不同清晰度下SoTA代理的性能边界，量化了主动交互的收益，验证了MUSE与人类判断的强相关性。

Conclusion: 重新定义评估标准，为理解用户意图的下一代代理奠定基础。

Abstract: Benchmarks are paramount for gauging progress in the domain of Mobile GUI Agents. In practical scenarios, users frequently fail to articulate precise directives containing full task details at the onset, and their expressions are typically ambiguous. Consequently, agents are required to converge on the user's true intent via active clarification and interaction during execution. However, existing benchmarks predominantly operate under the idealized assumption that user-issued instructions are complete and unequivocal. This paradigm focuses exclusively on assessing single-turn execution while overlooking the alignment capability of the agent. To address this limitation, we introduce AmbiBench, the first benchmark incorporating a taxonomy of instruction clarity to shift evaluation from unidirectional instruction following to bidirectional intent alignment. Grounded in Cognitive Gap theory, we propose a taxonomy of four clarity levels: Detailed, Standard, Incomplete, and Ambiguous. We construct a rigorous dataset of 240 ecologically valid tasks across 25 applications, subject to strict review protocols. Furthermore, targeting evaluation in dynamic environments, we develop MUSE (Mobile User Satisfaction Evaluator), an automated framework utilizing an MLLM-as-a-judge multi-agent architecture. MUSE performs fine-grained auditing across three dimensions: Outcome Effectiveness, Execution Quality, and Interaction Quality. Empirical results on AmbiBench reveal the performance boundaries of SoTA agents across different clarity levels, quantify the gains derived from active interaction, and validate the strong correlation between MUSE and human judgment. This work redefines evaluation standards, laying the foundation for next-generation agents capable of truly understanding user intent.

</details>


### [189] [Verifiable Provenance of Software Artifacts with Zero-Knowledge Compilation](https://arxiv.org/abs/2602.11887)
*Javier Ron,Martin Monperrus*

Main category: cs.SE

TL;DR: 提出基于零知识虚拟机（zkVM）的可验证源代码出处新方法，经测试可应用于实际软件并提供强安全保证。


<details>
  <summary>Details</summary>
Motivation: 实现可验证的源代码出处在实践中具有挑战性，现有流行技术需困难的构建工具链和环境匹配与重新执行。

Method: 在zkVM内执行编译器，生成编译输出和加密证明，证明编译是在声称的源代码和编译器上进行的。

Result: 使用RISC Zero zkVM和ChibiCC C编译器实现概念验证，在200个合成程序、31个OpenSSL和21个libsodium源文件上评估，成功阻止针对编译器替换、源篡改、输出操纵和重放攻击的对抗测试。

Conclusion: zk编译适用于现实世界软件并提供强安全保证。

Abstract: Verifying that a compiled binary originates from its claimed source code is a fundamental security requirement, called source code provenance. Achieving verifiable source code provenance in practice remains challenging. The most popular technique, called reproducible builds, requires difficult matching and reexecution of build toolchains and environments. We propose a novel approach to verifiable provenance based on compiling software with zero-knowledge virtual machines (zkVMs). By executing a compiler within a zkVM, our system produces both the compiled output and a cryptographic proof attesting that the compilation was performed on the claimed source code with the claimed compiler. We implement a proof-of-concept implementation using the RISC Zero zkVM and the ChibiCC C compiler, and evaluate it on 200 synthetic programs as well as 31 OpenSSL and 21 libsodium source files. Our results show that zk-compilation is applicable to real-world software and provides strong security guarantees: all adversarial tests targeting compiler substitution, source tampering, output manipulation, and replay attacks are successfully blocked.

</details>


### [190] [Leveraging LLMs to support co-evolution between definitions and instances of textual DSLs: A Systematic Evaluation](https://arxiv.org/abs/2602.11904)
*Weixing Zhang,Bowen Jiang,Yuhong Fu,Anne Koziolek,Regina Hebig,Daniel Strüber*

Main category: cs.SE

TL;DR: 研究评估大语言模型（LLMs）对文本领域特定语言（DSLs）语法和实例协同演化的潜力，结果表明小尺度案例表现好，大尺度性能下降。


<details>
  <summary>Details</summary>
Motivation: 现有模型驱动工程技术不适合文本DSLs，且可能丢失布局和注释等人类相关信息，因此评估LLMs在此方面的潜力。

Method: 使用Claude Sonnet 4.5和GPT - 5.2在十种案例语言上各进行十次运行，评估正确性和人类导向信息的保留情况。

Result: 小尺度案例表现好（精确率和召回率≥94%），大尺度性能下降，Claude在40行时保持85%召回率，GPT在最大实例上失败，响应时间随实例大小显著增加，语法演化复杂性和删除粒度对性能影响大于变更类型。

Conclusion: 明确了基于LLM的协同演化何时有效以及当前的局限性所在。

Abstract: Software languages evolve over time for reasons such as feature additions. When grammars evolve, textual instances that originally conformed to them may become outdated. While model-driven engineering provides many techniques for co-evolving models with metamodel changes, these approaches are not designed for textual DSLs and may lose human-relevant information such as layout and comments. This study systematically evaluates the potential of large language models (LLMs) for co-evolving grammars and instances of textual DSLs. Using Claude Sonnet 4.5 and GPT-5.2 across ten case languages with ten runs each, we assess both correctness and preservation of human-oriented information. Results show strong performance on small-scale cases ($\geq$94% precision and recall for instances requiring fewer than 20 modified lines), but performance degraded with scale: Claude maintains 85% recall at 40 lines, while GPT fails on the largest instances. Response time increases substantially with instance size, and grammar evolution complexity and deletion granularity affect performance more than change type. These findings clarify when LLM-based co-evolution is effective and where current limitations remain.

</details>


### [191] [Improving Code Generation via Small Language Model-as-a-judge](https://arxiv.org/abs/2602.11911)
*Giuseppe Crupi,Rosalia Tufano,Gabriele Bavota*

Main category: cs.SE

TL;DR: 研究解决现有代码生成研究局限，训练现代SLMs作代码正确性判断器，表现优于RankEF且成本低。


<details>
  <summary>Details</summary>
Motivation: 现有研究未评估T5排序器分类准确性，实验模型旧，不清楚能否助公司低成本训练代码生成器。

Method: 训练多个最先进的SLMs作为代码正确性判断器，评估其区分正确和错误实现的能力。

Result: 现代SLMs即使不利用基于执行的信息也优于RankEF，作为代码排序器时性能提升更高，能以低成本与大得多的LLMs竞争。

Conclusion: 现代SLMs可有效解决现有研究局限，为公司低成本训练代码生成器提供可能。

Abstract: Large language models (LLMs) have shown remarkable capabilities in automated code generation. While effective for mainstream languages, they may underperform on less common or domain-specific languages, prompting companies to develop in-house code generators. While open-source models can be trained for this, only LLMs with tens of billions of parameters match the performance of commercial tools, demanding costly training and deployment. Recent work proposed supporting code generation with smaller models (SLMs) by generating multiple candidate solutions and using another SLM to select the most likely correct one. The most recent work in this area is the one by Sun et al. [29] presenting RankEF, a T5 model trained to rank code solutions using both execution-based and non-execution-based information. However, Sun et al. do not assess the T5 ranker's classification accuracy, that is, how often it misjudges correct implementations as incorrect or vice versa, leaving open questions about the reliability of LMs as code correctness judges for other tasks (e.g., automated code review). Moreover, their experiments involve relatively old models, making it unclear the extent to which such a methodology would still help companies in cheaply training their own code generators with performance comparable to those of massive LLMs. We present a study addressing these limitations. We train several state-of-the-art SLMs as code correctness judges and assess their ability to discriminate between correct and wrong implementations. We show that modern SLMs outperform RankEF, even without exploiting execution-based information. When used as code rankers, they achieve higher performance gains than RankEF and perform competitively with LLMs 5-25x larger, at a fraction of the cost.

</details>


### [192] [Studying Quality Improvements Recommended via Manual and Automated Code Review](https://arxiv.org/abs/2602.11925)
*Giuseppe Crupi,Rosalia Tufano,Gabriele Bavota*

Main category: cs.SE

TL;DR: 研究人类与ChatGPT-4代码审查差异，发现二者互补，DL技术可作额外质量检查但不能替代人类。


<details>
  <summary>Details</summary>
Motivation: 探究DL方法在代码审查中推荐质量改进的程度。

Method: 收集240个PR中人类审查者的739条评论，让ChatGPT审查相同PR并对比推荐结果。

Result: ChatGPT平均推荐代码更改数是人类的2.4倍，但仅能发现人类报告质量问题的10%，约40%额外评论指出有意义问题。

Conclusion: DL代码审查可作人类审查的额外检查，不能替代人类，也不能节省审查时间。

Abstract: Several Deep Learning (DL)-based techniques have been proposed to automate code review. Still, it is unclear the extent to which these approaches can recommend quality improvements as a human reviewer. We study the similarities and differences between code reviews performed by humans and those automatically generated by DL models, using ChatGPT-4 as representative of the latter. In particular, we run a mining-based study in which we collect and manually inspect 739 comments posted by human reviewers to suggest code changes in 240 PRs. The manual inspection aims at classifying the type of quality improvement recommended by human reviewers (e.g., rename variable/constant). Then, we ask ChatGPT to perform a code review on the same PRs and we compare the quality improvements it recommends against those suggested by the human reviewers. We show that while, on average, ChatGPT tends to recommend a higher number of code changes as compared to human reviewers (~2.4x more), it can only spot 10% of the quality issues reported by humans. However, ~40% of the additional comments generated by the LLM point to meaningful quality issues. In short, our findings show the complementarity of manual and AI-based code review. This finding suggests that, in its current state, DL-based code review can be used as a further quality check on top of the one performed by humans, but should not be considered as a valid alternative to them nor as a mean to save code review time, since human reviewers would still need to perform their manual inspection while also validating the quality issues reported by the DL-based technique.

</details>


### [193] [Evaluating AGENTS.md: Are Repository-Level Context Files Helpful for Coding Agents?](https://arxiv.org/abs/2602.11988)
*Thibaud Gloaguen,Niels Mündler,Mark Müller,Veselin Raychev,Martin Vechev*

Main category: cs.SE

TL;DR: 研究代码代理使用上下文文件的效果，发现其降低任务成功率、增加推理成本，建议上下文文件仅描述最低要求。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对代码代理使用上下文文件在实际任务中有效性的严格研究。

Method: 在两种互补设置下评估代码代理的任务完成性能，包括流行仓库的既定SWE - bench任务和包含开发者提交上下文文件仓库的新问题集。

Result: 上下文文件相比不提供仓库上下文会降低任务成功率，增加推理成本超20%，会促使更广泛探索且代码代理倾向遵循指令。

Conclusion: 上下文文件的不必要要求使任务更难，人类编写的上下文文件应仅描述最低要求。

Abstract: A widespread practice in software development is to tailor coding agents to repositories using context files, such as AGENTS.md, by either manually or automatically generating them. Although this practice is strongly encouraged by agent developers, there is currently no rigorous investigation into whether such context files are actually effective for real-world tasks. In this work, we study this question and evaluate coding agents' task completion performance in two complementary settings: established SWE-bench tasks from popular repositories, with LLM-generated context files following agent-developer recommendations, and a novel collection of issues from repositories containing developer-committed context files.
  Across multiple coding agents and LLMs, we find that context files tend to reduce task success rates compared to providing no repository context, while also increasing inference cost by over 20%. Behaviorally, both LLM-generated and developer-provided context files encourage broader exploration (e.g., more thorough testing and file traversal), and coding agents tend to respect their instructions. Ultimately, we conclude that unnecessary requirements from context files make tasks harder, and human-written context files should describe only minimal requirements.

</details>


### [194] [An Empirical Study of the Imbalance Issue in Software Vulnerability Detection](https://arxiv.org/abs/2602.12038)
*Yuejun Guo,Qiang Hu,Qiang Tang,Yves Le Traon*

Main category: cs.SE

TL;DR: 本文研究深度学习漏洞检测中模型表现不稳定的问题，通过实验验证是数据不平衡导致，并研究现有解决方案的效果，发现各方案表现不同且无全面优秀者，还探讨外部影响以开发新方案。


<details>
  <summary>Details</summary>
Motivation: 深度学习用于漏洞检测虽有前景，但模型在不同数据集上表现不稳定，推测是数据不平衡问题导致，因此开展研究。

Method: 对九个开源数据集和两个最先进的深度学习模型进行综合实证研究。

Result: 证实数据不平衡是模型表现差异的核心原因；不同的不平衡解决方案在不同数据集和评估指标上表现不同，如Focal loss适合提高精度等，且无方案在所有指标上都出色。

Conclusion: 数据不平衡是深度学习漏洞检测模型性能差异的关键因素，现有不平衡解决方案有局限性，需探索外部影响以开发新方案。

Abstract: Vulnerability detection is crucial to protect software security. Nowadays, deep learning (DL) is the most promising technique to automate this detection task, leveraging its superior ability to extract patterns and representations within extensive code volumes. Despite its promise, DL-based vulnerability detection remains in its early stages, with model performance exhibiting variability across datasets. Drawing insights from other well-explored application areas like computer vision, we conjecture that the imbalance issue (the number of vulnerable code is extremely small) is at the core of the phenomenon. To validate this, we conduct a comprehensive empirical study involving nine open-source datasets and two state-of-the-art DL models. The results confirm our conjecture. We also obtain insightful findings on how existing imbalance solutions perform in vulnerability detection. It turns out that these solutions perform differently as well across datasets and evaluation metrics. Specifically: 1) Focal loss is more suitable to improve the precision, 2) mean false error and class-balanced loss encourages the recall, and 3) random over-sampling facilitates the F1-measure. However, none of them excels across all metrics. To delve deeper, we explore external influences on these solutions and offer insights for developing new solutions.

</details>


### [195] [ModelWisdom: An Integrated Toolkit for TLA+ Model Visualization, Digest and Repair](https://arxiv.org/abs/2602.12058)
*Zhiyong Chen,Jialun Cao,Chang Xu,Shing-Chi Cheung*

Main category: cs.SE

TL;DR: 提出交互式环境ModelWisdom，用可视化和大语言模型让TLA+模型检查更易解释和操作，改善输出解读和调试效果。


<details>
  <summary>Details</summary>
Motivation: 当前TLA+模型检查中，从业者在解读反例、理解状态转换图和修复模型方面面临挑战，原始输出可解释性有限，手动回溯困难，现有工具功能不足。

Method: 开发ModelWisdom，具备模型可视化、图优化、模型摘要和模型修复等功能。

Result: 将原始模型检查输出转化为可交互、可解释的工作流程，改善对非平凡TLA+规范的理解并减少调试工作量。

Conclusion: ModelWisdom提升了TLA+模型检查的可解释性和可操作性，有助于解决实际应用中的问题。

Abstract: Model checking in TLA+ provides strong correctness guarantees, yet practitioners continue to face significant challenges in interpreting counterexamples, understanding large state-transition graphs, and repairing faulty models. These difficulties stem from the limited explainability of raw model-checker output and the substantial manual effort required to trace violations back to source specifications. Although the TLA+ Toolbox includes a state diagram viewer, it offers only a static, fully expanded graph without folding, color highlighting, or semantic explanations, which limits its scalability and interpretability. We present ModelWisdom, an interactive environment that uses visualization and large language models to make TLA+ model checking more interpretable and actionable. ModelWisdom offers: (i) Model Visualization, with colorized violation highlighting, click-through links from transitions to TLA+ code, and mapping between violating states and broken properties; (ii) Graph Optimization, including tree-based structuring and node/edge folding to manage large models; (iii) Model Digest, which summarizes and explains subgraphs via large language models (LLMs) and performs preprocessing and partial explanations; and (iv) Model Repair, which extracts error information and supports iterative debugging. Together, these capabilities turn raw model-checker output into an interactive, explainable workflow, improving understanding and reducing debugging effort for nontrivial TLA+ specifications. The website to ModelWisdom is available: https://model-wisdom.pages.dev. A demonstrative video can be found at https://www.youtube.com/watch?v=plyZo30VShA.

</details>


### [196] [Performance Antipatterns: Angel or Devil for Power Consumption?](https://arxiv.org/abs/2602.12079)
*Alessandro Aneggi,Vincenzo Stoico,Andrea Janes*

Main category: cs.SE

TL;DR: 本文实证研究了性能反模式对微服务系统能耗的影响，发现部分反模式会显著增加能耗。


<details>
  <summary>Details</summary>
Motivation: 探究广泛研究的性能反模式是否会对微服务系统的能耗产生负面影响。

Method: 将十种反模式实现为独立微服务，在可控负载条件下进行评估，多次收集性能、CPU和DRAM能耗及资源利用率数据。

Result: 所有反模式都会降低性能，但只有部分反模式的响应时间与能耗增加有显著关系，一些反模式达到CPU饱和，能耗不再随响应时间上升。

Conclusion: 为识别兼具能耗反模式特征的性能反模式提供了系统基础，为设计更节能的微服务架构提供了可行建议。

Abstract: Performance antipatterns are known to degrade the responsiveness of microservice-based systems, but their impact on energy consumption remains largely unexplored. This paper empirically investigates whether widely studied performance antipatterns defined by Smith and Williams also negatively influence power usage. We implement ten antipatterns as isolated microservices and evaluate them under controlled load conditions, collecting synchronized measurements of performance, CPU and DRAM power consumption, and resource utilization across 30 repeated runs per antipattern. The results show that while all antipatterns degrade performance as expected, only a subset exhibit a statistically significant relationship between response time and increased power consumption. Specifically, several antipatterns reach CPU saturation, capping power draw regardless of rising response time, whereas others (\eg Unnecessary Processing, The Ramp) demonstrate energy-performance coupling indicative of inefficiency. Our results show that, while all injected performance antipatterns increase response time as expected, only a subset also behaves as clear energy antipatterns, with several cases reaching a nearly constant CPU power level where additional slowdowns mainly translate into longer execution time rather than higher instantaneous power consumption. The study provides a systematic foundation for identifying performance antipatterns that also behave as energy antipatterns and offers actionable insights for designing more energy-efficient microservices architectures.

</details>


### [197] [PPTAM$η$: Energy Aware CI/CD Pipeline for Container Based Applications](https://arxiv.org/abs/2602.12081)
*Alessandro Aneggi,Xiaozhou Li,Andrea Janes*

Main category: cs.SE

TL;DR: 提出PPTAM$η$自动化管道，将功率和能耗测量集成到GitLab CI中，对JWT认证API进行评估。


<details>
  <summary>Details</summary>
Motivation: 现代基于容器的微服务快速部署，但CI/CD管道很少测量能耗，而设计模式等会影响能效。

Method: 构建PPTAM$η$自动化管道，协调负载生成、容器监控和硬件功率探针，在每次提交时收集可比指标。

Result: 在JWT认证API的四个提交上评估PPTAM$η$，收集性能和能耗指标。

Conclusion: 该管道使开发人员可见能耗，支持测试工程师进行版本比较，便于研究人员进行趋势分析。

Abstract: Modern container-based microservices evolve through rapid deployment cycles, but CI/CD pipelines still rarely measure energy consumption, even though prior work shows that design patterns, code smells and refactorings affect energy efficiency. We present PPTAM$η$, an automated pipeline that integrates power and energy measurement into GitLab CI for containerised API systems, coordinating load generation, container monitoring and hardware power probes to collect comparable metrics at each commit. The pipeline makes energy visible to developers, supports version comparison for test engineers and enables trend analysis for researchers. We evaluate PPTAM$η$ on a JWT-authenticated API across four commits, collecting performance and energy metrics and summarising the architecture, measurement methodology and validation.

</details>


### [198] [On the Adoption of AI Coding Agents in Open-source Android and iOS Development](https://arxiv.org/abs/2602.12144)
*Muhammad Ahmad Khan,Hasnain Ali,Muneeb Rana,Muhammad Saqib Ilyas,Abdul Ali Bangash*

Main category: cs.SE

TL;DR: 对开源移动应用项目中AI生成代码进行类别级实证研究，分析不同平台、代理和任务类别的PR接受行为。


<details>
  <summary>Details</summary>
Motivation: AI编码代理对移动开发的影响缺乏实证研究，本文进行相关实证研究。

Method: 分析AIDev数据集中193个经过验证的安卓和iOS开源GitHub仓库中2901个AI编写的拉取请求（PR）。

Result: 安卓项目收到的AI编写PR更多，接受率更高；常规任务PR接受率最高；安卓PR解决时间在2025年年中有所改善后又下降。

Conclusion: 为AI代理对开源移动项目的影响提供了基于证据的特征描述，为评估代理生成的贡献建立了实证基线。

Abstract: AI coding agents are increasingly contributing to software development, yet their impact on mobile development has received little empirical attention. In this paper, we present the first category-level empirical study of agent-generated code in open-source mobile app projects. We analyzed PR acceptance behaviors across mobile platforms, agents, and task categories using 2,901 AI-authored pull requests (PRs) in 193 verified Android and iOS open-source GitHub repositories in the AIDev dataset. We find that Android projects have received 2x more AI-authored PRs and have achieved higher PR acceptance rate (71%) than iOS (63%), with significant agent-level variation on Android. Across task categories, PRs with routine tasks (feature, fix, and ui) achieve the highest acceptance, while structural changes like refactor and build achieve lower success and longer resolution times. Furthermore, our evolution analysis shows improvement in PR resolution time on Android through mid-2025 before it declined again. Our findings offer the first evidence-based characterization of AI agents effects on OSS mobile projects and establish empirical baselines for evaluating agent-generated contributions to design platform aware agentic systems.

</details>


### [199] [Automated Test Suite Enhancement Using Large Language Models with Few-shot Prompting](https://arxiv.org/abs/2602.12256)
*Alex Chudic,Gül Çalıklı*

Main category: cs.SE

TL;DR: 本文实证研究不同测试工件源的少样本提示对大语言模型生成单元测试质量的影响，实验表明大语言模型可通过少样本提示生成高质量测试，人工编写示例效果最佳。


<details>
  <summary>Details</summary>
Motivation: 传统工具生成的单元测试缺乏可读性等，大语言模型零样本能力被广泛研究，但少样本学习潜力未充分探索，研究不同测试源的少样本提示对生成单元测试质量的影响。

Method: 在HumanEval和ClassEval数据集上使用GPT - 4o进行实验，评估基于检索的示例选择方法。

Result: 大语言模型可通过少样本提示生成高质量测试，人工编写示例产生最佳覆盖率和正确性，基于问题描述和代码组合相似度选择示例能得到最有效的少样本提示。

Conclusion: 大语言模型在少样本提示下能生成高质量单元测试，不同测试源的少样本提示效果有差异，人工编写示例更优。

Abstract: Unit testing is essential for verifying the functional correctness of code modules (e.g., classes, methods), but manually writing unit tests is often labor-intensive and time-consuming. Unit tests generated by tools that employ traditional approaches, such as search-based software testing (SBST), lack readability, naturalness, and practical usability. LLMs have recently provided promising results and become integral to developers' daily practices. Consequently, software repositories now include a mix of human-written tests, LLM-generated tests, and those from tools employing traditional approaches such as SBST. While LLMs' zero-shot capabilities have been widely studied, their few-shot learning potential for unit test generation remains underexplored. Few-shot prompting enables LLMs to learn from examples in the prompt, and automatically retrieving such examples could enhance test suites. This paper empirically investigates how few-shot prompting with different test artifact sources, comprising human, SBST, or LLM, affects the quality of LLM-generated unit tests as program comprehension artifacts and their contribution to improving existing test suites by evaluating not only correctness and coverage but also readability, cognitive complexity, and maintainability in hybrid human-AI codebases. We conducted experiments on HumanEval and ClassEval datasets using GPT-4o, which is integrated into GitHub Copilot and widely used among developers. We also assessed retrieval-based methods for selecting relevant examples. Our results show that LLMs can generate high-quality tests via few-shot prompting, with human-written examples producing the best coverage and correctness. Additionally, selecting examples based on the combined similarity of problem description and code consistently yields the most effective few-shot prompts.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [200] [Time-Inhomogeneous Volatility Aversion for Financial Applications of Reinforcement Learning](https://arxiv.org/abs/2602.12030)
*Federico Cacciamani,Roberto Daluiso,Marco Pinciroli,Michele Trapletti,Edoardo Vittori*

Main category: q-fin.CP

TL;DR: 论文指出经典强化学习不适用于金融序贯决策中收益与风险权衡问题，提出新风险度量，研究对应目标性质及学习算法，并给出玩具示例数值结果。


<details>
  <summary>Details</summary>
Motivation: 经典强化学习目标是期望累积奖励，而金融应用需收益与风险权衡，且关注总回报的时间拆分，现有基于风险度量优化的强化学习方法不适用。

Method: 提出新的风险度量，该度量惩罚单奖励的不确定性并允许任意规划目标水平，研究由此产生的目标性质和学习算法的泛化。

Result: 给出了玩具示例的数值结果。

Conclusion: 新的风险度量适用于关注总回报时间拆分的金融序贯决策问题。

Abstract: In finance, sequential decision problems are often faced, for which reinforcement learning (RL) emerges as a promising tool for optimisation without the need of analytical tractability. However, the objective of classical RL is the expected cumulated reward, while financial applications typically require a trade-off between return and risk. In this work, we focus on settings where one cares about the time split of the total return, ruling out most risk-aware generalisations of RL which optimise a risk measure defined on the latter. We notice that a preference for homogeneous splits, which we found satisfactory for hedging, can be unfit for other problems, and therefore propose a new risk metric which still penalises uncertainty of the single rewards, but allows for an arbitrary planning of their target levels. We study the properties of the resulting objective and the generalisation of learning algorithms to optimise it. Finally, we show numerical results on toy examples.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [201] [Amortised and provably-robust simulation-based inference](https://arxiv.org/abs/2602.11325)
*Ayush Bharti,Charita Dellaporta,Yuga Hikida,François-Xavier Briol*

Main category: stat.ML

TL;DR: 提出基于广义贝叶斯推断和加权得分匹配损失神经近似的模拟推理新方法，对异常值稳健，有计算优势。


<details>
  <summary>Details</summary>
Motivation: 现有推理方法常无法处理因测量仪器故障或人为误差导致的数据中的异常值和极值。

Method: 引入基于广义贝叶斯推断和加权得分匹配损失神经近似的模拟推理新方法，选用条件密度模型。

Result: 得到一种既摊销又对异常值稳健的方法，无需马尔可夫链蒙特卡罗抽样，计算复杂度远低于现有最先进方法。

Conclusion: 新方法解决了现有推理方法的不足，具有重要计算优势。

Abstract: Complex simulator-based models are now routinely used to perform inference across the sciences and engineering, but existing inference methods are often unable to account for outliers and other extreme values in data which occur due to faulty measurement instruments or human error. In this paper, we introduce a novel approach to simulation-based inference grounded in generalised Bayesian inference and a neural approximation of a weighted score-matching loss. This leads to a method that is both amortised and provably robust to outliers, a combination not achieved by existing approaches. Furthermore, through a carefully chosen conditional density model, we demonstrate that inference can be further simplified and performed without the need for Markov chain Monte Carlo sampling, thereby offering significant computational advantages, with complexity that is only a small fraction of that of current state-of-the-art approaches.

</details>


### [202] [The Cost of Learning under Multiple Change Points](https://arxiv.org/abs/2602.11406)
*Tomer Gafni,Garud Iyengar,Assaf Zeevi*

Main category: stat.ML

TL;DR: 本文探讨多变化点环境下的在线学习问题，揭示传统方法弊端，提出Anytime Tracking CUSUM（ATC）算法，证明其近极小极大最优性，并通过实验验证理论结果。


<details>
  <summary>Details</summary>
Motivation: 多变化点环境给在线学习带来新挑战，经典方法可能因内生混淆现象导致灾难性失败，需新的学习算法。

Method: 提出Anytime Tracking CUSUM（ATC）算法，该算法是无时间范围的在线算法，采用选择性检测原则。

Result: 证明了适当调优的ATC算法近极小极大最优，其遗憾值接近信息论下界；合成和真实数据实验验证了理论结果。

Conclusion: ATC算法能有效解决多变化点环境下的在线学习问题，性能接近最佳。

Abstract: We consider an online learning problem in environments with multiple change points. In contrast to the single change point problem that is widely studied using classical "high confidence" detection schemes, the multiple change point environment presents new learning-theoretic and algorithmic challenges. Specifically, we show that classical methods may exhibit catastrophic failure (high regret) due to a phenomenon we refer to as endogenous confounding. To overcome this, we propose a new class of learning algorithms dubbed Anytime Tracking CUSUM (ATC). These are horizon-free online algorithms that implement a selective detection principle, balancing the need to ignore "small" (hard-to-detect) shifts, while reacting "quickly" to significant ones. We prove that the performance of a properly tuned ATC algorithm is nearly minimax-optimal; its regret is guaranteed to closely match a novel information-theoretic lower bound on the achievable performance of any learning algorithm in the multiple change point problem. Experiments on synthetic as well as real-world data validate the aforementioned theoretical findings.

</details>


### [203] [Provable Offline Reinforcement Learning for Structured Cyclic MDPs](https://arxiv.org/abs/2602.11679)
*Kyungbok Lee,Angelica Cristello Sarteau,Michael R. Kosorok*

Main category: stat.ML

TL;DR: 提出循环MDP框架解决多步决策问题，提出CycleFQI方法，建立误差界和收敛率，通过实验验证有效性


<details>
  <summary>Details</summary>
Motivation: 异构阶段特定动态、转移和折扣因子的多步决策问题中，离线学习有挑战，优化一个阶段策略会影响后续阶段状态分布

Method: 提出模块化结构框架将循环过程分解为阶段子问题，实例化CycleFQI方法，使用特定阶段Q函数向量，还提出基于筛子的渐近推断方法

Result: 建立有限样本次优性误差界，推导了全局收敛率，CycleFQI减轻了维度灾难，实验证明CycleFQI有效

Conclusion: CycleFQI方法能有效解决异构阶段多步决策问题

Abstract: We introduce a novel cyclic Markov decision process (MDP) framework for multi-step decision problems with heterogeneous stage-specific dynamics, transitions, and discount factors across the cycle. In this setting, offline learning is challenging: optimizing a policy at any stage shifts the state distributions of subsequent stages, propagating mismatch across the cycle. To address this, we propose a modular structural framework that decomposes the cyclic process into stage-wise sub-problems. While generally applicable, we instantiate this principle as CycleFQI, an extension of fitted Q-iteration enabling theoretical analysis and interpretation. It uses a vector of stage-specific Q-functions, tailored to each stage, to capture within-stage sequences and transitions between stages. This modular design enables partial control, allowing some stages to be optimized while others follow predefined policies. We establish finite-sample suboptimality error bounds and derive global convergence rates under Besov regularity, demonstrating that CycleFQI mitigates the curse of dimensionality compared to monolithic baselines. Additionally, we propose a sieve-based method for asymptotic inference of optimal policy values under a margin condition. Experiments on simulated and real-world Type 1 Diabetes data sets demonstrate CycleFQI's effectiveness.

</details>


### [204] [Estimation of instrument and noise parameters for inverse problem based on prior diffusion model](https://arxiv.org/abs/2602.11711)
*Jean-François Giovannelli*

Main category: stat.ML

TL;DR: 文章聚焦贝叶斯框架下正则化且先验由扩散过程建模时逆问题中观测参数（响应和误差参数）的估计，提出策略估计参数和量化不确定性，经实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 解决贝叶斯框架下正则化且先验由扩散过程建模时逆问题中观测参数估计的难题，尤其是后验采样的棘手问题。

Method: 提出一种新策略来定义观测参数和感兴趣图像的最优估计器，使用MCMC算法计算后验估计和性质。

Result: 所提策略能有效估计观测参数，可量化不确定性，数值实验证实了计算效率、估计质量和不确定性量化的有效性。

Conclusion: 提出的策略在逆问题观测参数估计中具有计算效率高、估计质量好、能有效量化不确定性的优点。

Abstract: This article addresses the issue of estimating observation parameters (response and error parameters) in inverse problems. The focus is on cases where regularization is introduced in a Bayesian framework and the prior is modeled by a diffusion process. In this context, the issue of posterior sampling is well known to be thorny, and a recent paper proposes a notably simple and effective solution. Consequently, it offers an remarkable additional flexibility when it comes to estimating observation parameters. The proposed strategy enables us to define an optimal estimator for both the observation parameters and the image of interest. Furthermore, the strategy provides a means of quantifying uncertainty. In addition, MCMC algorithms allow for the efficient computation of estimates and properties of posteriors, while offering some guarantees. The paper presents several numerical experiments that clearly confirm the computational efficiency and the quality of both estimates and uncertainties quantification.

</details>


### [205] [PAC-Bayesian Generalization Guarantees for Fairness on Stochastic and Deterministic Classifiers](https://arxiv.org/abs/2602.11722)
*Julien Bastian,Benjamin Leblanc,Pascal Germain,Amaury Habrard,Christine Largeron,Guillaume Metzler,Emilie Morvant,Paul Viallard*

Main category: stat.ML

TL;DR: 提出用于推导公平性泛化界限的PAC - 贝叶斯框架，适用于随机和确定性分类器，有两大优势并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 经典PAC泛化界限无法为学习平衡预测风险和公平性约束的模型提供公平性理论保证。

Method: 对随机分类器，用标准PAC - 贝叶斯技术推导公平性界限；对确定性分类器，利用PAC - 贝叶斯新进展扩展公平性界限。

Result: 框架适用于多种可表示为风险差异的公平性度量，能产生自界算法。通过三种经典公平性度量进行实验评估。

Conclusion: 框架有用且界限紧密。

Abstract: Classical PAC generalization bounds on the prediction risk of a classifier are insufficient to provide theoretical guarantees on fairness when the goal is to learn models balancing predictive risk and fairness constraints. We propose a PAC-Bayesian framework for deriving generalization bounds for fairness, covering both stochastic and deterministic classifiers. For stochastic classifiers, we derive a fairness bound using standard PAC-Bayes techniques. Whereas for deterministic classifiers, as usual PAC-Bayes arguments do not apply directly, we leverage a recent advance in PAC-Bayes to extend the fairness bound beyond the stochastic setting. Our framework has two advantages: (i) It applies to a broad class of fairness measures that can be expressed as a risk discrepancy, and (ii) it leads to a self-bounding algorithm in which the learning procedure directly optimizes a trade-off between generalization bounds on the prediction risk and on the fairness. We empirically evaluate our framework with three classical fairness measures, demonstrating not only its usefulness but also the tightness of our bounds.

</details>


### [206] [Aggregate Models, Not Explanations: Improving Feature Importance Estimation](https://arxiv.org/abs/2602.11760)
*Joseph Paillard,Angel Reyero Lobo,Denis A. Engemann,Bertrand Thirion*

Main category: stat.ML

TL;DR: 特征重要性方法在机器学习科学发现中有潜力，但模型不稳定影响估计准确性，研究表明模型层面集成对表达性模型能提供更准确变量重要性估计并验证。


<details>
  <summary>Details</summary>
Motivation: 解决表达性模型不稳定导致变量重要性估计不准确，以及集成时选择解释单个集成模型还是聚合个体模型解释困难的问题。

Method: 进行理论分析，在考虑复杂先进机器学习模型的假设下展开，研究选择主要受模型超额风险的驱动情况。

Result: 模型层面集成能减少主要误差项，为表达性模型提供更准确的变量重要性估计。

Conclusion: 模型层面集成对表达性模型的变量重要性估计更准确，在经典基准和大规模蛋白质组学研究中得到验证。

Abstract: Feature-importance methods show promise in transforming machine learning models from predictive engines into tools for scientific discovery. However, due to data sampling and algorithmic stochasticity, expressive models can be unstable, leading to inaccurate variable importance estimates and undermining their utility in critical biomedical applications. Although ensembling offers a solution, deciding whether to explain a single ensemble model or aggregate individual model explanations is difficult due to the nonlinearity of importance measures and remains largely understudied. Our theoretical analysis, developed under assumptions accommodating complex state-of-the-art ML models, reveals that this choice is primarily driven by the model's excess risk. In contrast to prior literature, we show that ensembling at the model level provides more accurate variable-importance estimates, particularly for expressive models, by reducing this leading error term. We validate these findings on classical benchmarks and a large-scale proteomic study from the UK Biobank.

</details>


### [207] [The Implicit Bias of Logit Regularization](https://arxiv.org/abs/2602.12039)
*Alon Beck,Yohai Bar Sinai,Noam Levi*

Main category: stat.ML

TL;DR: 本文分析线性分类中logit正则化器，揭示其诱导logit聚类隐式偏差，证明特定条件下权重向量与Fisher线性判别对齐，研究信号加噪声模型体现其效果，拓展理论理解。


<details>
  <summary>Details</summary>
Motivation: 现代分类器中广泛使用的logit正则化方法虽能改善校准和泛化，但机制尚待探索。

Method: 分析线性分类中一般类别的logit正则化器，研究高斯数据及logit充分聚类情况，探讨信号加噪声模型。

Result: logit正则化诱导logit聚类，在特定条件下使权重向量与Fisher线性判别对齐，在信号加噪声模型中减半临界样本复杂度、诱导小噪声极限下的学习并使泛化对噪声鲁棒。

Conclusion: 研究拓展了对标签平滑的理论理解，凸显更广泛logit正则化方法的有效性。

Abstract: Logit regularization, the addition a convex penalty directly in logit space, is widely used in modern classifiers, with label smoothing as a prominent example. While such methods often improve calibration and generalization, their mechanism remains under-explored. In this work, we analyze a general class of such logit regularizers in the context of linear classification, and demonstrate that they induce an implicit bias of logit clustering around finite per-sample targets. For Gaussian data, or whenever logits are sufficiently clustered, we prove that logit clustering drives the weight vector to align exactly with Fisher's Linear Discriminant. To demonstrate the consequences, we study a simple signal-plus-noise model in which this transition has dramatic effects: Logit regularization halves the critical sample complexity and induces grokking in the small-noise limit, while making generalization robust to noise. Our results extend the theoretical understanding of label smoothing and highlight the efficacy of a broader class of logit-regularization methods.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [208] [Bounded Local Generator Classes for Deterministic State Evolution](https://arxiv.org/abs/2602.11476)
*R. Jay Martin*

Main category: cs.OS

TL;DR: 本文形式化了作用于图索引状态系统的局部保持确定性算子的构造性子类，定义BLGC类，证明在该类假设下每步算子工作量的情况并得到相关结果。


<details>
  <summary>Details</summary>
Motivation: 对作用于图索引状态系统的局部保持确定性算子的构造性子类进行形式化研究。

Method: 定义Bounded Local Generator Classes (BLGC)类，在该类假设下进行分析证明。

Result: 证明在BLGC假设下，当节点数趋于无穷时，每步算子工作量满足W_t = O(1)，框架可进行希尔伯特空间嵌入并在允许子空间上有有界算子范数。

Conclusion: 结果适用于定义的子类，在所述局部性和有界性约束之外不具有普遍性。

Abstract: We formalize a constructive subclass of locality-preserving deterministic operators acting on graph-indexed state systems. We define the class of Bounded Local Generator Classes (BLGC), consisting of finite-range generators operating on bounded state spaces under deterministic composition. Within this class, incremental update cost is independent of total system dimension. We prove that, under the BLGC assumptions, per-step operator work satisfies W_t = O(1) as the number of nodes M \to \infty, establishing a structural decoupling between global state size and incremental computational effort. The framework admits a Hilbert-space embedding in \ell^2(V; \mathbb{R}^d) and yields bounded operator norms on admissible subspaces. The result applies specifically to the defined subclass and does not claim universality beyond the stated locality and boundedness constraints.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [209] [BIRD: A Museum Open Dataset Combining Behavior Patterns and Identity Types to Better Model Visitors' Experience](https://arxiv.org/abs/2602.11160)
*Alexanne Worm,Florian Marchal,Sylvain Castagnos*

Main category: cs.HC

TL;DR: 研究解决文化遗产领域人工智能数据缺乏问题，构建开放数据集以改善博物馆推荐路径。


<details>
  <summary>Details</summary>
Motivation: 人工智能训练和验证模型需大量数据，但文化遗产领域开放数据集有限，现有数据难以全面建模游客体验。

Method: 2019年2 - 3月对51名参与者使用眼动追踪眼镜收集数据，让其自由参观博物馆，然后结合语境、行为和反馈数据构建开放数据集。

Result: 可重新构建游客身份，重现Veron和Levasseur档案。

Conclusion: 该数据集能基于游客兴趣个性化调整，提高博物馆推荐路径的质量。

Abstract: Lack of data is a recurring problem in Artificial Intelligence, as it is essential for training and validating models. This is particularly true in the field of cultural heritage, where the number of open datasets is relatively limited and where the data collected does not always allow for holistic modeling of visitors' experience due to the fact that data are ad hoc (i.e. restricted to the sole characteristics required for the evaluation of a specific model). To overcome this lack, we conducted a study between February and March 2019 aimed at obtaining comprehensive and detailed information about visitors, their visit experience and their feedback. We equipped 51 participants with eye-tracking glasses, leaving them free to explore the 3 floors of the museum for an average of 57 minutes, and to discover an exhibition of more than 400 artworks. On this basis, we built an open dataset combining contextual data (demographic data, preferences, visiting habits, motivations, social context. . . ), behavioral data (spatiotemporal trajectories, gaze data) and feedback (satisfaction, fatigue, liked artworks, verbatim. . . ). Our analysis made it possible to re-enact visitor identities combining the majority of characteristics found in the literature and to reproduce the Veron and Levasseur profiles. This dataset will ultimately make it possible to improve the quality of recommended paths in museums by personalizing the number of points of interest (POIs), the time spent at these different POIs, and the amount of information to be provided to each visitor based on their level of interest.

</details>


### [210] [V-SHiNE: A Virtual Smart Home Framework for Explainability Evaluation](https://arxiv.org/abs/2602.11775)
*Mersedeh Sadeghi,Simon Scholz,Max Unterbusch,Andreas Vogelsang*

Main category: cs.HC

TL;DR: V-SHiNE是用于智能家居解释评估的浏览器模拟框架，经研究验证可行，为可解释智能系统评估提供平台。


<details>
  <summary>Details</summary>
Motivation: 评估智能家居解释的质量和影响在方法上存在困难。

Method: 构建浏览器端智能家居模拟框架V - SHiNE，可配置环境、模拟行为、接入自定义解释引擎，有灵活交付模式和丰富交互日志。

Result: 159名参与者的研究证明了V - SHiNE的可行性。

Conclusion: V - SHiNE为以用户为中心的可解释智能系统评估提供轻量级、可重复的平台。

Abstract: Explanations are essential for helping users interpret and trust autonomous smart-home decisions, yet evaluating their quality and impact remains methodologically difficult in this domain. V-SHiNE addresses this gap: a browser-based smarthome simulation framework for scalable and realistic assessment of explanations. It allows researchers to configure environments, simulate behaviors, and plug in custom explanation engines, with flexible delivery modes and rich interaction logging. A study with 159 participants demonstrates its feasibility. V-SHiNE provides a lightweight, reproducible platform for advancing user-centered evaluation of explainable intelligent systems

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [211] [Regularized Ensemble Forecasting for Learning Weights from Historical and Current Forecasts](https://arxiv.org/abs/2602.11379)
*Han Su,Xiaojia Guo,Xiaoke Zhang*

Main category: stat.AP

TL;DR: 文章提出一种新的正则化集成方法，结合当前预测和历史表现设置权重，在沃尔玛销售和宏观经济预测中表现优于基准模型，并探讨方法优势及信息来源。


<details>
  <summary>Details</summary>
Motivation: 结合多专家预测通常更准确，现有方法仅依赖当前预测或过去准确性，本文希望同时考虑两者因素。

Method: 提出一种新的正则化集成方法，通过最小化组合预测的方差并结合历史表现的正则化项来学习权重，且该方法有贝叶斯解释。

Result: 在沃尔玛销售和宏观经济预测的实证研究中，该集成方法在专家完整历史预测记录和不完整历史记录情况下都优于领先的基准模型。

Conclusion: 给出了确定最优权重的示例，讨论了该框架的优势，以及专家过去和当前预测分别何时更具信息性。

Abstract: Combining forecasts from multiple experts often yields more accurate results than relying on a single expert. In this paper, we introduce a novel regularized ensemble method that extends the traditional linear opinion pool by leveraging both current forecasts and historical performances to set the weights. Unlike existing approaches that rely only on either the current forecasts or past accuracy, our method accounts for both sources simultaneously. It learns weights by minimizing the variance of the combined forecast (or its transformed version) while incorporating a regularization term informed by historical performances. We also show that this approach has a Bayesian interpretation. Different distributional assumptions within this Bayesian framework yield different functional forms for the variance component and the regularization term, adapting the method to various scenarios. In empirical studies on Walmart sales and macroeconomic forecasting, our ensemble outperforms leading benchmark models both when experts' full forecasting histories are available and when experts enter and exit over time, resulting in incomplete historical records. Throughout, we provide illustrative examples that show how the optimal weights are determined and, based on the empirical results, we discuss where the framework's strengths lie and when experts' past versus current forecasts are more informative.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [212] [CL API: Real-Time Closed-Loop Interactions with Biological Neural Networks](https://arxiv.org/abs/2602.11632)
*David Hogan,Andrew Doherty,Boon Kien Khoo,Johnson Zhou,Richard Salib,James Stewart,Kiaran Lawson,Alon Loeffler,Brett Kagan*

Main category: q-bio.NC

TL;DR: 本文介绍了现有与生物神经网络（BNNs）交互方法的不足，提出Cortical Labs应用程序编程接口（CL API），它能实现与BNNs的实时亚毫秒级闭环交互，为其实验提供基础。


<details>
  <summary>Details</summary>
Motivation: 现有与BNNs交互的方法存在依赖底层硬件机制导致迭代复杂，或牺牲时间和结构控制影响一致性和可重复性的问题，需要更好的解决方案。

Method: 采用基于契约的API设计方法，通过声明式Python接口呈现契约，让非专业程序员能表达复杂行为而无需管理底层细节。

Result: CL API能实现与BNNs的实时亚毫秒级闭环交互，提供精确刺激语义、事务性准入、确定性排序和显式同步保证。

Conclusion: CL API为BNNs的实时实验提供了可访问和可重现的基础，支持基础生物学研究和新兴神经计算应用。

Abstract: Biological neural networks (BNNs) are increasingly explored for their rich dynamics, parallelism, and adaptive behavior. Beyond understanding their function as a scientific endeavour, a key focus has been using these biological systems as a novel computing substrate. However, BNNs can only function as reliable information-processing systems if inputs are delivered in a temporally and structurally consistent manner. In practice, this requires stimulation with precisely controlled structure, microsecond-scale timing, multi-channel synchronization, and the ability to observe and respond to neural activity in real-time. Existing approaches to interacting with BNNs face a fundamental trade-off: they either depend on low-level hardware mechanisms, imposing prohibitive complexity for rapid iteration, or they sacrifice temporal and structural control, undermining consistency and reproducibility - particularly in closed-loop experiments. The Cortical Labs Application Programming Interface (CL API) enables real-time, sub-millisecond closed-loop interactions with BNNs. Taking a contract-based API design approach, the CL API provides users with precise stimulation semantics, transactional admission, deterministic ordering, and explicit synchronization guarantees. This contract is presented through a declarative Python interface, enabling non-expert programmers to express complex stimulation and closed-loop behavior without managing low-level scheduling or hardware details. Ultimately, the CL API provides an accessible and reproducible foundation for real-time experimentation with BNNs, supporting both fundamental biological research and emerging neurocomputing applications.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [213] [Compositionality of Systems and Partially Ordered Runs](https://arxiv.org/abs/2602.11203)
*Peter Fettke,Wolfgang Reisig*

Main category: cs.LO

TL;DR: 本文提出处理Petri网及其运行的统一框架，展示组合网运行集和原网运行组合的关系。


<details>
  <summary>Details</summary>
Motivation: 找到合适模型描述分布式系统个体演化，需处理Petri网及其运行的组合与分解。

Method: 提出一个统一框架来处理Petri网及其运行，进行组合和分解操作。

Result: 对于网M和N，组合网M • N的运行集等于M和N运行的组合。

Conclusion: 所提出的统一框架能有效处理Petri网及其运行的组合与分解。

Abstract: In the late 1970s, C.A. Petri introduced partially ordered event occurrences (runs), then called \emph{processes}, as the appropriate model to describe the individual evolutions of distributed systems. Here, we present a unified framework for handling Petri nets and their runs, specifically to compose and decompose them. It is shown that, for nets $M$ and $N$, the set of runs of the composed net $M \bullet N$ equals the composition of the runs of $M$ and $N$.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [214] [Self-referential instances of the dominating set problem are irreducible](https://arxiv.org/abs/2602.10559)
*Guangyan Zhou*

Main category: cs.CC

TL;DR: 研究Erdos - Renyi随机图模型中支配数的算法可判定性，表明支配问题有强不可约性，极端困难源于解空间性质。


<details>
  <summary>Details</summary>
Motivation: 研究Erdos - Renyi随机图模型中支配数的算法可判定性。

Method: 选取合适的边概率，证明检查小阶诱导子图的算法无法确定图是否含特定大小支配集，通过局部对称映射展示产生难以区分的随机图实例。

Result: 对于精心选择的边概率，检查至多n^c阶诱导子图的算法无法确定G(n,p)是否含大小为ln n的支配集；存在局部对称映射可翻转支配集存在性。

Conclusion: 随机图中支配集问题的极端困难性并非源于局部结构，而是解空间的自指性和近独立性。

Abstract: We study the algorithmic decidability of the domination number in the Erdos-Renyi random graph model $G(n,p)$. We show that for a carefully chosen edge probability $p=p(n)$, the domination problem exhibits a strong irreducible property. Specifically, for any constant $0<c<1$, no algorithm that inspects only an induced subgraph of order at most $n^c$ can determine whether $G(n,p)$ contains a dominating set of size $k=\ln n$. We demonstrate that the existence of such a dominating set can be flipped by a local symmetry mapping that alters only a constant number of edges, thereby producing indistinguishable random graph instances which require exhaustive search. These results demonstrate that the extreme hardness of the dominating set problem in random graphs cannot be attributed to local structure, but instead arises from the self-referential nature and near-independence structure of the entire solution space.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [215] [Markovian protocols and an upper bound on the extension complexity of the matching polytope](https://arxiv.org/abs/2602.11382)
*M. Szusterman*

Main category: cs.DM

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper investigates the extension complexity of polytopes by exploiting the correspondence between non-negative factorizations of slack matrices and randomized communication protocols. We introduce a geometric characterization of extension complexity based on the width of Markovian protocols, as a variant of the framework introduced by Faenza et al. This enables us to derive a new upper bound of $\tilde{O}(n^3\cdot 1.5^n)$ for the extension complexity of the matching polytope $P_{\text{match}}(n)$, improving upon the standard $2^n$-bound given by Edmonds' description. Additionally, we recover Goemans' compact formulation for the permutahedron using a one-round protocol based on sorting networks.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [216] [Creative Ownership in the Age of AI](https://arxiv.org/abs/2602.12270)
*Annie Liang,Jay Lu*

Main category: econ.TH

TL;DR: 现行版权法聚焦实质性相似，但生成式AI可模仿风格不复制内容，本文提出新侵权判定标准并分析许可生成特性。


<details>
  <summary>Details</summary>
Motivation: 现有版权侵权定义不适合生成式AI情况，需要新判定标准。

Method: 将生成系统建模为闭包算子，将现有作品语料映射到新作品输出。

Result: 描述了许可生成的结构特性，揭示了渐近二分法：有机创作轻尾时，AI生成不受限；重尾时，监管持续约束。

Conclusion: 提出的新判定标准能用于分析生成式AI在版权下的生成情况。

Abstract: Copyright law focuses on whether a new work is "substantially similar" to an existing one, but generative AI can closely imitate style without copying content, a capability now central to ongoing litigation. We argue that existing definitions of infringement are ill-suited to this setting and propose a new criterion: a generative AI output infringes on an existing work if it could not have been generated without that work in its training corpus. To operationalize this definition, we model generative systems as closure operators mapping a corpus of existing works to an output of new works. AI generated outputs are \emph{permissible} if they do not infringe on any existing work according to our criterion. Our results characterize structural properties of permissible generation and reveal a sharp asymptotic dichotomy: when the process of organic creations is light-tailed, dependence on individual works eventually vanishes, so that regulation imposes no limits on AI generation; with heavy-tailed creations, regulation can be persistently constraining.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [217] [Future Mining: Learning for Safety and Security](https://arxiv.org/abs/2602.11472)
*Md Sazedur Rahman,Mizanur Rahman Jewel,Sanjay Madria*

Main category: cs.CR

TL;DR: 本文提出统一智能安全架构应对矿业环境挑战，介绍五个核心模块，构建有弹性、可信的智能矿业系统。


<details>
  <summary>Details</summary>
Motivation: 现实矿业环境有诸多约束，新兴网络物理威胁和能源受限问题影响矿业安全与可靠性，需构建新架构。

Method: 提出统一智能安全架构，整合多模态感知、安全联邦学习等技术，引入五个核心模块。

Result: 形成端到端框架，可引导矿工、识别受损模型或传感器、确保关键设备可靠性。

Conclusion: 该架构为构建在对抗条件下能保持运营连续性的智能矿业系统提供全面研究愿景。

Abstract: Mining is rapidly evolving into an AI driven cyber physical ecosystem where safety and operational reliability depend on robust perception, trustworthy distributed intelligence, and continuous monitoring of miners and equipment. However, real world mining environments impose severe constraints, including poor illumination, GPS denied conditions, irregular underground topologies and intermittent connectivity. These factors degrade perception accuracy, disrupt situational awareness and weaken distributed learning systems. At the same time, emerging cyber physical threats such as backdoor triggers, sensor spoofing, label flipping attacks, and poisoned model updates further jeopardize operational safety as mines adopt autonomous vehicles, humanoid assistance, and federated learning for collaborative intelligence. Energy constrained sensors also experience uneven battery depletion, creating blind spots in safety coverage and disrupting hazard detection pipelines. This paper presents a vision for a Unified Smart Safety and Security Architecture that integrates multimodal perception, secure federated learning, reinforcement learning, DTN enabled communication, and energy aware sensing into a cohesive safety framework. We introduce five core modules: Miner Finder, Multimodal Situational Awareness, Backdoor Attack Monitor, TrustFed LFD, and IoT driven Equipment Health Monitoring. These modules collectively address miner localization, hazard understanding, federated robustness, and predictive maintenance. Together, they form an end to end framework capable of guiding miners through obstructed pathways, identifying compromised models or sensors, and ensuring mission critical equipment reliability. This work outlines a comprehensive research vision for building a resilient and trustworthy intelligent mining system capable of maintaining operational continuity under adversarial conditions.

</details>


### [218] [LoRA-based Parameter-Efficient LLMs for Continuous Learning in Edge-based Malware Detection](https://arxiv.org/abs/2602.11655)
*Christian Rondanini,Barbara Carminati,Elena Ferrari,Niccolò Lardo,Ashish Kundu*

Main category: cs.CR

TL;DR: 论文提出基于边缘的恶意软件检测的连续学习架构，结合本地适应与全局知识共享，实验显示该方法有准确率提升且适配边缘设备


<details>
  <summary>Details</summary>
Motivation: 边缘设备需实时检测恶意软件，现有静态、集中重训练及本地训练模型有局限

Method: 提出结合本地适应与全局知识共享的连续学习架构，用轻量级变压器模型在边缘节点微调，聚合并重新分发LoRA模块

Result: 在两个公共数据集上实验，基于LoRA交换在遇到未知攻击时准确率提升20 - 25%，损失和F1稳定，LoRA增加模型大小不到1%

Conclusion: 该架构能实现跨设备泛化且适用于资源受限的边缘硬件

Abstract: The proliferation of edge devices has created an urgent need for security solutions capable of detecting malware in real time while operating under strict computational and memory constraints. Recently, Large Language Models (LLMs) have demonstrated remarkable capabilities in recognizing complex patterns, yet their deployment on edge devices remains impractical due to their resource demands. However, in edge malware detection, static or centrally retrained models degrade under evolving threats and heterogeneous traffic; locally trained models become siloed and fail to transfer across domains. To overcome these limitations, in this paper, we present a continuous learning architecture for edge-based malware detection that combines local adaptation on each device with global knowledge sharing through parameter-efficient LoRA adapters. Lightweight transformer models (DistilBERT, DistilGPT-2, TinyT5) run on edge nodes and are incrementally fine-tuned on device-specific traffic; only the resulting LoRA modules are aggregated by a lightweight coordinator and redistributed, enabling cross-device generalization without exchanging raw data. We evaluate on two public IoT security datasets, Edge-IIoTset and TON-IoT, under multi-round learning to simulate evolving threats. Compared to isolated fine-tuning, the LoRA-based exchange yields up to 20-25% accuracy gains when models encounter previously unseen attacks from another domain, while maintaining stable loss and F1 across rounds. LoRA adds less than 1% to model size (~0.6-1.8 MB), making updates practical for constrained edge hardware.

</details>


### [219] [Legitimate Overrides in Decentralized Protocols](https://arxiv.org/abs/2602.12260)
*Oghenekaro Elem,Nimrod Talmon*

Main category: cs.CR

TL;DR: 本文针对去中心化协议中的紧急机制设计问题，提出了Scope × Authority分类法，进行成本最小化分析并得出预测，结合事件评估后给出设计原则。


<details>
  <summary>Details</summary>
Motivation: 去中心化协议的紧急机制虽重要，但当前设计缺乏系统性且存在争议，约100亿美元的技术漏洞损失需有效机制应对。

Method: 开发Scope × Authority分类法绘制紧急架构设计空间；将权衡问题形式化为随机成本最小化问题并得出预测；结合705个漏洞事件评估预测。

Result: 发现遏制时间随触发权威类型系统变化；损失呈重尾分布；社区情绪会调节维持干预能力的有效成本。

Conclusion: 分析得出具体设计原则，推动紧急治理从意识形态辩论转向定量工程。

Abstract: Decentralized protocols claim immutable, rule-based execution, yet many embed emergency mechanisms such as chain-level freezes, protocol pauses, and account quarantines. These overrides are crucial for responding to exploits and systemic failures, but they expose a core tension: when does intervention preserve trust and when is it perceived as illegitimate discretion? With approximately $10$ billion in technical exploit losses potentially addressable by onchain intervention (2016--2026), the design of these mechanisms has high practical stakes, but current approaches remain ad hoc and ideologically charged. We address this gap by developing a Scope $\times$ Authority taxonomy that maps the design space of emergency architectures along two dimensions: the precision of the intervention and the concentration of trigger authority. We formalize the resulting tradeoffs of a standing centralization cost versus containment speed and collateral disruption as a stochastic cost-minimization problem; and derive three testable predictions. Assessing these predictions against 705 documented exploit incidents, we find that containment time varies systematically by authority type; that losses follow a heavy-tailed distribution ($α\approx 1.33$) concentrating risk in rare catastrophic events; and that community sentiment measurably modulates the effective cost of maintaining intervention capability. The analysis yields concrete design principles that move emergency governance from ideological debate towards quantitative engineering.

</details>


### [220] [Keeping a Secret Requires a Good Memory: Space Lower-Bounds for Private Algorithms](https://arxiv.org/abs/2602.12209)
*Alessandro Epasto,Xin Lyu,Pasin Manurangsi*

Main category: cs.CR

TL;DR: 本文从内存效率角度研究差分隐私计算成本，用多玩家通信游戏证明用户级差分隐私空间下界，解决公开问题并拓展到多类问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究对差分隐私在内存使用方面的固有成本探索不足，本文旨在填补该研究空白。

Method: 引入基于多玩家通信游戏的证明技术，将低内存私有算法的难度与“贡献上限”联系起来，通过分析游戏获胜所需传输信息得出内存下界。

Result: 以流中不同元素数量估计问题为例，证明私有算法达到特定错误率所需空间，解决文献中公开问题，建立私有与非私有算法空间复杂度的指数分离，还将技术推广到其他问题。

Conclusion: 该通信理论技术可用于解决多种自然统计估计任务中私有算法的空间复杂度问题，为差分隐私内存成本研究提供了新方法。

Abstract: We study the computational cost of differential privacy in terms of memory efficiency. While the trade-off between accuracy and differential privacy is well-understood, the inherent cost of privacy regarding memory use remains largely unexplored. This paper establishes for the first time an unconditional space lower bound for user-level differential privacy by introducing a novel proof technique based on a multi-player communication game.
  Central to our approach, this game formally links the hardness of low-memory private algorithms to the necessity of ``contribution capping'' -- tracking and limiting the users who disproportionately impact the dataset. We demonstrate that winning this communication game requires transmitting information proportional to the number of over-active users, which translates directly to memory lower bounds.
  We apply this framework, as an example, to the fundamental problem of estimating the number of distinct elements in a stream and we prove that any private algorithm requires almost $\widetildeΩ(T^{1/3})$ space to achieve certain error rates in a promise variant of the problem. This resolves an open problem in the literature (by Jain et al. NeurIPS 2023 and Cummings et al. ICML 2025) and establishes the first exponential separation between the space complexity of private algorithms and their non-private $\widetilde{O}(1)$ counterparts for a natural statistical estimation task. Furthermore, we show that this communication-theoretic technique generalizes to broad classes of problems, yielding lower bounds for private medians, quantiles, and max-select.

</details>


### [221] [Reliable and Private Anonymous Routing for Satellite Constellations](https://arxiv.org/abs/2602.11764)
*Nilesh Vyas,Fabien Geyer,Svetoslav Duhovnikov*

Main category: cs.CR

TL;DR: 本文针对共享动态网络基础设施对元数据隐私的威胁，提出增强匿名架构，介绍三项主要贡献，经模拟验证其有效性，为高匿名通信系统提供可行方案。


<details>
  <summary>Details</summary>
Motivation: 共享动态网络基础设施对元数据隐私构成威胁，尤其是在混合信任环境下，需要增强的安全架构。

Method: 提出增强的匿名架构，包括使用(n, k)擦除码的多路径传输协议、集成计算高效的PIR协议、引入自适应基于中心性的延迟策略。

Result: 多路径传输实现近乎零消息丢失，量化了可靠性与带宽开销的权衡；PIR协议的微基准测试证实其实践部署的可行性。

Conclusion: 提供了可部署的高匿名通信系统的有效蓝图，证明在大规模商业网络中安全复用敏感操作的可行性。

Abstract: Shared, dynamic network infrastructures, such as dual-use LEO satellite constellations, pose critical threats to metadata privacy, particularly for state actors operating in mixed-trust environments. This work proposes an enhanced anonymity architecture, evolving the Loopix mix-network, to provide robust security and reliability in these volatile topologies. We introduce three primary contributions: (1) A multi-path transport protocol utilizing $(n, k)$ erasure codes, which is demonstrated to counteract the high link volatility and intermittent connectivity that renders standard mix-networks unreliable. (2) The integration of a computationally efficient Private Information Retrieval (PIR) protocol during route discovery. (3) The introduction of adaptive, centrality-based delay strategies that efficiently mitigate the inherent topological bias of LEO networks, providing a superior anonymity-to-latency trade-off. This mechanism provably prevents metadata leakage at the user-provider directory, mitigating profiling and correlation attacks. We validate this architecture via high-fidelity, packet-level simulations of a LEO constellation. Empirical results show our multi-path transport achieves near-zero message loss, establishing a quantifiable trade-off between reliability and bandwidth overhead. Furthermore, microbenchmarks of the PIR protocol quantify its computational and latency overheads, confirming its feasibility for practical deployment. This work provides a validated blueprint for deployable high-anonymity communication systems, demonstrating the viability of securely multiplexing sensitive operations within large-scale commercial network infrastructures.

</details>


### [222] [Transferable Backdoor Attacks for Code Models via Sharpness-Aware Adversarial Perturbation](https://arxiv.org/abs/2602.11213)
*Shuyu Chang,Haiping Huang,Yanjun Zhang,Yujin Huang,Fu Xiao,Leo Yu Zhang*

Main category: cs.CR

TL;DR: 现有代码模型后门攻击在可迁移性和隐蔽性上存在权衡问题，本文提出STAB方法，实验显示其在可迁移性和隐蔽性上优于先前攻击。


<details>
  <summary>Details</summary>
Motivation: 现有基于静态触发器和动态触发器的代码模型后门攻击存在可迁移性与隐蔽性难以兼顾的问题，且动态攻击依赖不现实的数据分布假设。

Method: 提出Sharpness - aware Transferable Adversarial Backdoor (STAB) 攻击，使用Sharpness - Aware Minimization训练替代模型引导参数到平坦损失区域，用Gumbel - Softmax优化搜索离散触发器令牌生成上下文感知的对抗触发器。

Result: 在三个数据集和两个代码模型的实验表明，STAB在可迁移性和隐蔽性上优于先前攻击，防御后平均攻击成功率达73.2%，跨数据集攻击成功率比最佳动态触发器攻击高12.4%，且不影响干净输入上的性能。

Conclusion: STAB无需完整受害者数据，能同时实现可迁移性和隐蔽性，是一种有效的代码模型后门攻击方法。

Abstract: Code models are increasingly adopted in software development but remain vulnerable to backdoor attacks via poisoned training data. Existing backdoor attacks on code models face a fundamental trade-off between transferability and stealthiness. Static trigger-based attacks insert fixed dead code patterns that transfer well across models and datasets but are easily detected by code-specific defenses. In contrast, dynamic trigger-based attacks adaptively generate context-aware triggers to evade detection but suffer from poor cross-dataset transferability. Moreover, they rely on unrealistic assumptions of identical data distributions between poisoned and victim training data, limiting their practicality. To overcome these limitations, we propose Sharpness-aware Transferable Adversarial Backdoor (STAB), a novel attack that achieves both transferability and stealthiness without requiring complete victim data. STAB is motivated by the observation that adversarial perturbations in flat regions of the loss landscape transfer more effectively across datasets than those in sharp minima. To this end, we train a surrogate model using Sharpness-Aware Minimization to guide model parameters toward flat loss regions, and employ Gumbel-Softmax optimization to enable differentiable search over discrete trigger tokens for generating context-aware adversarial triggers. Experiments across three datasets and two code models show that STAB outperforms prior attacks in terms of transferability and stealthiness. It achieves a 73.2% average attack success rate after defense, outperforming static trigger-based attacks that fail under defense. STAB also surpasses the best dynamic trigger-based attack by 12.4% in cross-dataset attack success rate and maintains performance on clean inputs.

</details>


### [223] [Yaksha-Prashna: Understanding eBPF Bytecode Network Function Behavior](https://arxiv.org/abs/2602.11232)
*Animesh Singh,K Shiv Kumar,S. VenkataKeerthy,Pragna Mamidipaka,R V B R N Aaseesh,Sayandeep Sen,Palanivel Kodeswaran,Theophilus A. Benson,Ramakrishna Upadrasta,Praveen Tammana*

Main category: cs.CR

TL;DR: 设计Yaksha - Prashna系统用于断言和查询eBPF字节码的规范一致性和依赖关系，比现有工作快200 - 1000倍。


<details>
  <summary>Details</summary>
Motivation: 第三方eBPF网络功能以字节码形式提供，云运营商难以了解其功能正确性及与其他网络功能的交互，开发者想在不暴露源码的情况下提供功能正确性证明。

Method: 设计Yaksha - Prashna系统，构建特定领域模型，采用可扩展程序分析来提取和建模eBPF程序。

Result: 使用Yaksha - Prashna语言在标准和非标准eBPF网络功能上表达24个属性，比现有工作快200 - 1000倍。

Conclusion: Yaksha - Prashna系统可有效帮助运营商和开发者处理eBPF字节码的相关问题。

Abstract: Many cloud infrastructure organizations increasingly rely on third-party eBPF-based network functions for use cases like security, observability, and load balancing, so that not everyone requires a team of highly skilled eBPF experts. However, the network functions from third parties (e.g., F5, Palo Alto) are available in bytecode format to cloud operators, giving little or no understanding of their functional correctness and interaction with other network functions in a chain. Also, eBPF developers want to provide proof of functional correctness for their developed network functions without disclosing the source code to the operators. We design Yaksha-Prashna, a system that allows operators/developers to assert and query bytecode's conformance to its specification and dependencies on other bytecodes. Our work builds domain-specific models that enable us to employ scalable program analysis to extract and model eBPF programs. Using Yaksha-Prashna language, we express 24 properties on standard and non-standard eBPF-based network functions with 200-1000x speedup over the state-of-the-art work.

</details>


### [224] [Unknown Attack Detection in IoT Networks using Large Language Models: A Robust, Data-efficient Approach](https://arxiv.org/abs/2602.12183)
*Shan Ali,Feifei Niu,Paria Shirani,Lionel C. Briand*

Main category: cs.CR

TL;DR: 现有物联网入侵检测方法应对未知攻击有局限，提出SiamXBERT框架检测未知攻击，实验表明其表现优且所需训练数据少。


<details>
  <summary>Details</summary>
Motivation: 网络攻击演变导致未知威胁出现，现有机器学习和深度学习方法在应对数据稀缺、加密流量和分布变化时效果受限，检测未知攻击困难。

Method: 提出SiamXBERT框架，结合流级和包级信息构建双模态特征表示，通过元学习实现对新攻击类型的快速适应和泛化。

Result: 在代表性物联网入侵数据集实验中，SiamXBERT在数据集内和跨数据集设置下均优于现有基线，未知F1分数提升高达78.8%，且所需训练数据少。

Conclusion: SiamXBERT适用于现实物联网环境中对未知攻击的鲁棒检测。

Abstract: The rapid evolution of cyberattacks continues to drive the emergence of unknown (zero-day) threats, posing significant challenges for network intrusion detection systems in Internet of Things (IoT) networks. Existing machine learning and deep learning approaches typically rely on large labeled datasets, payload inspection, or closed-set classification, limiting their effectiveness under data scarcity, encrypted traffic, and distribution shifts. Consequently, detecting unknown attacks in realistic IoT deployments remains difficult. To address these limitations, we propose SiamXBERT, a robust and data-efficient Siamese meta-learning framework empowered by a transformer-based language model for unknown attack detection. The proposed approach constructs a dual-modality feature representation by integrating flow-level and packet-level information, enabling richer behavioral modeling while remaining compatible with encrypted traffic. Through meta-learning, the model rapidly adapts to new attack types using only a small number of labeled samples and generalizes to previously unseen behaviors. Extensive experiments on representative IoT intrusion datasets demonstrate that SiamXBERT consistently outperforms state-of-the-art baselines under both within-dataset and cross-dataset settings while requiring significantly less training data, achieving up to \num{78.8}\% improvement in unknown F1-score. These results highlight the practicality of SiamXBERT for robust unknown attack detection in real-world IoT environments.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [225] [PAM: Processing Across Memory Hierarchy for Efficient KV-centric LLM Serving System](https://arxiv.org/abs/2602.11521)
*Lian Liu,Shixin Zhao,Yutian Zhou,Yintao He,Mengdi Wang,Yinhe Han,Ying Wang*

Main category: cs.AR

TL;DR: 针对大语言模型服务系统中KV操作瓶颈，提出PAM系统，可提升效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型对高效服务系统需求增加，现有系统无法有效处理KV相关内存密集型操作。

Method: 提出Processing Across Memory (PAM)系统，利用上下文局部性分布KV令牌，引入PAMattention算法，结合KV映射、迁移接口和调度算法平衡计算负载。

Result: PAM能同时满足带宽和容量需求。

Conclusion: PAM显著提升了大语言模型服务系统的效率和可扩展性，为大规模AI时代提供了高性价比、高性能解决方案。

Abstract: The widespread adoption of Large Language Models (LLMs) has exponentially increased the demand for efficient serving systems. With growing requests and context lengths, key-value (KV)-related operations, including attention computation and KV cache storage, have emerged as critical bottlenecks. They require massive memory bandwidth and capacity. Unfortunately, existing LLM serving systems, optimized for compute-bound workloads, fail to handle these memory-intensive operations effectively. Even with Processing-In-Memory (PIM) technology, current single-level memory designs cannot simultaneously satisfy the bandwidth and capacity requirements.
  To address these challenges, we propose Processing Across Memory (PAM), a KV-centric LLM serving system that coordinates heterogeneous PIM-enabled memory devices within a hierarchical architecture. PAM introduces a novel computing paradigm to balance high memory bandwidth with scalable capacity. First, PAM exploits the inherent context locality in KV access patterns to intelligently distribute KV tokens across the memory hierarchy. Second, to further exploit context locality, it introduces the PAMattention algorithm, enabling fine-grained parallel attention computation across heterogeneous PIM devices. Finally, PAM incorporates an intra-device KV mapping, inter-device KV migration interface, and an inter-device online KV scheduling algorithm to dynamically balance computational workloads. By addressing both bandwidth and capacity demands simultaneously, PAM significantly enhances the efficiency and scalability of LLM serving systems, paving the way for cost-effective, high-performance solutions in the era of large-scale AI.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [226] [Cross-Fitting-Free Debiased Machine Learning with Multiway Dependence](https://arxiv.org/abs/2602.11333)
*Kaicheng Chen,Harold D. Chiang*

Main category: econ.EM

TL;DR: 本文为广义矩估计（GMM）模型中两步去偏机器学习（DML）估计量开发了渐近理论，无需交叉拟合。


<details>
  <summary>Details</summary>
Motivation: 交叉拟合在复杂一阶学习器和有效样本量受独立聚类数影响时，统计效率低且计算负担重，需要新方法。

Method: 结合Neyman正交矩条件与基于局部化的经验过程方法，不进行样本分割。

Result: 得到的DML - GMM估计量在多向聚类依赖下渐近线性和渐近正态。

Conclusion: 推导出新的全局和局部最大不等式，为理论论证提供支撑。

Abstract: This paper develops an asymptotic theory for two-step debiased machine learning (DML) estimators in generalised method of moments (GMM) models with general multiway clustered dependence, without relying on cross-fitting. While cross-fitting is commonly employed, it can be statistically inefficient and computationally burdensome when first-stage learners are complex and the effective sample size is governed by the number of independent clusters. We show that valid inference can be achieved without sample splitting by combining Neyman-orthogonal moment conditions with a localisation-based empirical process approach, allowing for an arbitrary number of clustering dimensions. The resulting DML-GMM estimators are shown to be asymptotically linear and asymptotically normal under multiway clustered dependence. A central technical contribution of the paper is the derivation of novel global and local maximal inequalities for general classes of functions of sums of separately exchangeable arrays, which underpin our theoretical arguments and are of independent interest.

</details>


### [227] [Decomposition of Spillover Effects Under Misspecification:Pseudo-true Estimands and a Local--Global Extension](https://arxiv.org/abs/2602.12023)
*Yechan Park,Xiaodong Yang*

Main category: econ.EM

TL;DR: 本文探讨基于暴露的估计量隐含的政策目标及解释其直接和溢出效应，提出边际政策效应分解，表明现有方法更稳健且模拟显示可在现实实验设计中恢复相关成分。


<details>
  <summary>Details</summary>
Motivation: 应用工作中基于暴露建模可能存在错误设定，需明确基于暴露的估计量隐含的政策目标及如何解释其直接和溢出效应。

Method: 以边际政策效应为基础，推导其在不同情况下的分解，包括基于暴露的分解和渐近分解。

Result: 边际政策效应可分解为直接和溢出效应，在特定情况下可进一步渐近分解；许多现有方法更稳健；模拟和半合成实验表明可在现实实验中恢复相关成分。

Conclusion: 重新解释基于暴露估计量的目标后，现有方法比之前理解的更具稳健性，且可在现实实验设计中恢复分解成分。

Abstract: Applied work with interference typically models outcomes as functions of own treatment and a low-dimensional exposure mapping of others' treatments, even when that mapping may be misspecified. This raises a basic question: what policy object are exposure-based estimands implicitly targeting, and how should we interpret their direct and spillover components relative to the underlying policy question? We take as primitive the marginal policy effect, defined as the effect of a small change in the treatment probability under the actual experimental design, and show that any researcher-chosen exposure mapping induces a unique pseudo-true outcome model. This model is the best approximation to the underlying potential outcomes that depends only on the user-chosen exposure. Utilizing that representation, the marginal policy effect admits a canonical decomposition into exposure-based direct and spillover effects, and each component provides its optimal approximation to the corresponding oracle objects that would be available if interference were fully known. We then focus on a setting that nests important empirical and theoretical applications in which both local network spillovers and global spillovers, such as market equilibrium, operate. There, the marginal policy effect further decomposes asymptotically into direct, local, and global channels. An important implication is that many existing methods are more robust than previously understood once we reinterpret their targets as channel-specific components of this pseudo-true policy estimand. Simulations and a semi-synthetic experiment calibrated to a large cash-transfer experiment show that these components can be recovered in realistic experimental designs.

</details>


### [228] [Improved Inference for CSDID Using the Cluster Jackknife](https://arxiv.org/abs/2602.12043)
*Sunny R. Karim,Morten Ørregaard Nielsen,James G. MacKinnon,Matthew D. Webb*

Main category: econ.EM

TL;DR: 传统双重差分法获可靠推断有困难，近年关注转移到处理效应估计，流行的CSDID仍有过拒绝问题，本文提出用聚类刀切法改善推断并提供软件包。


<details>
  <summary>Details</summary>
Motivation: 传统双重差分法在多种情况下难获可靠推断，流行的CSDID在少量聚类或处理组聚类数少时有过拒绝问题。

Method: 为CSDID方法提出聚类刀切法进行推断，并进行模拟实验。

Result: 模拟显示聚类刀切法能大大改善推断。

Conclusion: 聚类刀切法有助于解决CSDID方法的推断问题，且提供了方便计算聚类刀切法标准误差的软件包。

Abstract: Obtaining reliable inferences with traditional difference-in-differences (DiD) methods can be difficult. Problems can arise when both outcomes and errors are serially correlated, when there are few clusters or few treated clusters, when cluster sizes vary greatly, and in various other cases. In recent years, recognition of the ``staggered adoption'' problem has shifted the focus away from inference towards consistent estimation of treatment effects. One of the most popular new estimators is the CSDID procedure of Callaway and Sant'Anna (2021). We find that the issues of over-rejection with few clusters and/or few treated clusters are at least as severe for CSDID as for traditional DiD methods. We also propose using a cluster jackknife for inference with CSDID, which simulations suggest greatly improves inference. We provide software packages in Stata csdidjack and R didjack to calculate cluster-jackknife standard errors easily.

</details>


<div id='q-fin.GN'></div>

# q-fin.GN [[Back]](#toc)

### [229] [Exact Value Solution to the Equity Premium Puzzle](https://arxiv.org/abs/2602.11687)
*Atilla Aras*

Main category: q-fin.GN

TL;DR: 本文在不使用校准值的情况下解决股权溢价之谜，新模型从4个方程计算4个未知变量，所得结果与实证研究相符，证明模型有效。


<details>
  <summary>Details</summary>
Motivation: 在不使用校准值的情况下解决股权溢价之谜，改进先前使用校准值的模型。

Method: 构建新模型，从4个不同方程计算4个未知变量，求解方程组得到主观时间贴现因子和相对风险厌恶系数。

Result: 主观时间贴现因子为0.9581，相对风险厌恶系数为1.0319，与实证研究兼容，微观和宏观关于CRRA值的研究首次相互印证，投资者被确定为风险规避行为。

Conclusion: 新推导的模型有效，能使CCAPM在与先前模型相同假设下运行。

Abstract: The aim of this article is to provide the solution to the equity premium puzzle without using calibrated values. Calibrated values of subjective time discount factor were used in the prior derived models because 4 variables were determined from 3 different equations. Furthermore, calculated values and risk behavior determination of prior models were compatible with empirical literature. 4 unknown variables are now calculated from 4 different equations in the new derived model in this article. Subjective time discount factor and coefficient of relative risk aversion are found 0.9581 and 1.0319, respectively from the system of equations which are compatible with empirical studies. Micro and macro studies about CRRA value affirm each other for the first time in the literature. Furthermore, equity and risk-free asset investors are pinned down to be insufficient risk-loving, which can be considered a type of risk-averse behavior. Hence it can be said that calculated values and risk attitude determination align with empirical literature. This shows that derived model is valid and make CCAPM work under the same assumptions with those of prior derived models.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [230] [Interpolation and Prewar-Postwar Output Volatility and Shock-Persistence Debate: A Closer Look and New Results](https://arxiv.org/abs/2602.11334)
*Hashem Dezhbakhsh,Daniel Levy*

Main category: econ.GN

TL;DR: 研究表明常用线性内插法对序列冲击持续性和波动性的作用与认为的不同，实际战前和战后产出差异或更大，战后产出更稳定的观点有价值。


<details>
  <summary>Details</summary>
Motivation: 解释美国战前产出比战后更具波动性和更低冲击持续性常被归因于数据内插法这一现象。

Method: 进行分析得出常用线性内插法对序列冲击持续性和波动性的影响。

Result: 常用线性内插法增加冲击持续性并降低波动性，实际战前和战后产出差异可能更大。

Conclusion: 战后产出因政策和制度变化更稳定的观点有相当价值，结果适用于常用宏观时间序列模型。

Abstract: It is well established that the US prewar output was more volatile and less shock persistent than the postwar output. This is often attributed to the data interpolation employed to construct the prewar series. Our analytical results, however, indicate that commonly used linear interpolation has the opposite effect on shock persistence and volatility of a series - it increases shock persistence and reduces volatility. The surprising implication of this finding is that the actual differences between the volatility and shock persistence of the prewar and postwar output series are likely greater than the existing literature recognizes, and interpolation has dampened rather than magnified this difference. Consequently, the view that postwar output was more stable than prewar output because of the effectiveness of the postwar stabilization policies and institutional changes has considerable merit. Our results hold for parsimonious stationary and nonstationary time series commonly used to model macroeconomic time series

</details>


### [231] [Ecosystem service demand relationship and trade-off patterns in urban parks across China](https://arxiv.org/abs/2602.11442)
*Shuyao Wu,Delong Li,Zhonghao Zhang*

Main category: econ.GN

TL;DR: 研究通过2万多份调查分析中国城市公园九种生态服务需求关系，发现对空气净化和休闲服务偏好高，有三种需求组合，强调要进行定制化城市公园设计。


<details>
  <summary>Details</summary>
Motivation: 目前人们对城市公园生态系统服务的价值评估了解有限，需探究不同生态服务需求关系。

Method: 开展大规模调查，收集20075份回复，并进行点分配实验。

Result: 中国城市居民对空气净化和休闲服务偏好高，存在三种需求组合，社会经济和环境因素影响服务需求间的权衡强度。

Conclusion: 有必要进行定制化城市公园设计，以满足不同服务需求，可持续地提升城市生活质量。

Abstract: Urban parks play a vital role in delivering various essential ecosystem services that significantly contribute to the well-being of urban populations. However, there is quite a limited understanding of how people value these ecosystem services differently. Here, we investigated the relationships among nine ecosystem service demands in urban parks across China using a large-scale survey with 20,075 responses and a point-allotment experiment. We found particularly high preferences for air purification and recreation services at the expense of other services among urban residents in China. These preferences were further reflected in three distinct demand bundles: air purification-dominated, recreation-dominated, and balanced demands. Each bundle delineated a typical group of people with different representative characteristics. Socio-economic and environmental factors, such as environmental interest and vegetation coverage, were found to significantly influence the trade-off intensity among service demands. These results underscore the necessity for tailored urban park designs that address diverse service demands with the aim of enhancing the quality of urban life in China and beyond sustainably.

</details>


### [232] [Labor Supply under Temporary Wage Increases: Evidence from a Randomized Field Experiment](https://arxiv.org/abs/2602.11992)
*Mats Ekman,Niklas Jakobsson,Andreas Kotsadam*

Main category: econ.GN

TL;DR: 对瑞典街头报纸卖家进行预注册随机对照试验，测试劳动供给决策中的收入目标设定，发现处理组卖家表现不同，用类似观察性研究策略有一致模式。


<details>
  <summary>Details</summary>
Motivation: 测试瑞典街头报纸卖家劳动供给决策中的收入目标设定。

Method: 进行预注册随机对照试验，给处理组个体每卖出一份报纸提供25%的奖金。

Result: 处理组卖家卖出更多报纸、工作时间更长、休假天数更少，与跨期劳动供给研究结果不同，用类似观察性研究策略有一致模式。

Conclusion: 在该研究情境下存在与以往研究不同的劳动供给表现及特定模式。

Abstract: We conduct a pre-registered randomized controlled trial to test for income targeting in labor supply decisions among sellers of a Swedish street paper. These workers face liquidity constraints, high income volatility, and discretion over hours. Treated individuals received a 25 percent bonus per copy sold for the duration of an issue, simulating an increase in earnings potential. Treated sellers sold more papers, worked longer hours, and took fewer days off. These findings contrast with studies on intertemporal labor supply that find small substitution effects. Notably, when we apply strategies similar to observational studies, we recover patterns consistent with income targeting.

</details>


### [233] [Chaos and Misallocation under Price Controls](https://arxiv.org/abs/2602.12066)
*Brian C. Albrecht,Alex Tabarrok,Mark Whitmeyer*

Main category: econ.GN

TL;DR: 价格管制会消除套利动机，在价格上限约束下会出现极端市场分配，产生跨市场错配，论文推导错配界限并校准数据得出损失范围。


<details>
  <summary>Details</summary>
Motivation: 研究价格管制下的市场分配情况以及由此产生的福利变化和错配问题。

Method: 证明混沌定理，推导不需要参数假设的错配界限，将福利问题转化为一维搜索。

Result: 价格管制下经济会出现极端市场分配，产生跨市场错配，校准数据显示错配损失是哈伯格三角的1 - 9倍。

Conclusion: 价格管制会导致市场分配出现极端情况和跨市场错配，福利依赖于距观察到的均衡较远的需求。

Abstract: Price controls kill the incentive for arbitrage. We prove a Chaos Theorem: under a binding price ceiling, suppliers are indifferent across destinations, so arbitrarily small cost differences can determine the entire allocation. The economy tips to corner outcomes in which some markets are fully served while others are starved; small parameter changes flip the identity of the corners, generating discontinuous welfare jumps. These corner allocations create a distinct source of cross-market misallocation, separate from the aggregate quantity loss (the Harberger triangle) and from within-market misallocation emphasized in prior work. They also create an identification problem: welfare depends on demand far from the observed equilibrium. We derive sharp bounds on misallocation that require no parametric assumptions. In an efficient allocation, shadow prices are equalized across markets; combined with the adding-up constraint, this collapses the infinite-dimensional welfare problem to a one-dimensional search over a common shadow price, with extremal losses achieved by piecewise-linear demand schedules. Calibrating the bounds to station-level AAA survey data from the 1973-74 U.S. gasoline crisis, misallocation losses range from roughly 1 to 9 times the Harberger triangle.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [234] [High-Probability Minimax Adaptive Estimation in Besov Spaces via Online-to-Batch](https://arxiv.org/abs/2602.11747)
*Paul Liautaud,Pierre Gaillard,Olivier Wintenberger*

Main category: math.ST

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study nonparametric regression over Besov spaces from noisy observations under sub-exponential noise, aiming to achieve minimax-optimal guarantees on the integrated squared error that hold with high probability and adapt to the unknown noise level. To this end, we propose a wavelet-based online learning algorithm that dynamically adjusts to the observed gradient noise by adaptively clipping it at an appropriate level, eliminating the need to tune parameters such as the noise variance or gradient bounds. As a by-product of our analysis, we derive high-probability adaptive regret bounds that scale with the $\ell_1$-norm of the competitor. Finally, in the batch statistical setting, we obtain adaptive and minimax-optimal estimation rates for Besov spaces via a refined online-to-batch conversion. This approach carefully exploits the structure of the squared loss in combination with self-normalized concentration inequalities.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [235] [An Improved Upper Bound for the Euclidean TSP Constant Using Band Crossovers](https://arxiv.org/abs/2602.11250)
*Julia Gaudio,Charlie K. Guan*

Main category: cs.CG

TL;DR: 本文探讨单位正方形内随机点最优旅行商路径长度常数β的上下界，指出当前上界0.90380的改进有限至约0.88，提出启发式方法或可将上界提升至约0.85，严格数值分析将上界改进到0.90367。


<details>
  <summary>Details</summary>
Motivation: 进一步改进单位正方形中随机点最优旅行商路径长度常数β的上界。

Method: 通过模拟和集中分析，基于之前的带遍历策略，允许路径跨带并利用相邻带中相近点对，进行严格数值分析。

Result: 指出当前上界0.90380未来改进有限至约0.88；提出启发式方法或可将上界提升至约0.85；严格分析将上界改进到0.90367。

Conclusion: 当前上界改进空间有限，提出的启发式方法和严格数值分析可改进上界。

Abstract: Consider $n$ points generated uniformly at random in the unit square, and let $L_n$ be the length of their optimal traveling salesman tour. Beardwood, Halton, and Hammersley (1959) showed $L_n / \sqrt n \to β$ almost surely as $n\to \infty$ for some constant $β$. The exact value of $β$ is unknown but estimated to be approximately $0.71$ (Applegate, Bixby, Chvátal, Cook 2011). Beardwood et al. further showed that $0.625 \leq β\leq 0.92116.$ Currently, the best known bounds are $0.6277 \leq β\leq 0.90380$, due to Gaudio and Jaillet (2019) and Carlsson and Yu (2023), respectively. The upper bound was derived using a computer-aided approach that is amenable to lower bounds with improved computation speed. In this paper, we show via simulation and concentration analysis that future improvement of the $0.90380$ is limited to $\sim0.88$. Moreover, we provide an alternative tour-constructing heuristic that, via simulation, could potentially improve the upper bound to $\sim0.85$. Our approach builds on a prior \emph{band-traversal} strategy, initially proposed by Beardwood et al. (1959) and subsequently refined by Carlsson and Yu (2023): divide the unit square into bands of height $Θ(1/\sqrt{n})$, construct paths within each band, and then connect the paths to create a TSP tour. Our approach allows paths to cross bands, and takes advantage of pairs of points in adjacent bands which are close to each other. A rigorous numerical analysis improves the upper bound to $0.90367$.

</details>


### [236] [An Improved FPT Algorithm for Computing the Interleaving Distance between Merge Trees via Path-Preserving Maps](https://arxiv.org/abs/2602.12028)
*Althaf P,Amit Chattopadhyay,Osamu Saeki*

Main category: cs.CG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: A merge tree is a fundamental topological structure used to capture the sub-level set (and similarly, super-level set) topology in scalar data analysis. The interleaving distance is a theoretically sound, stable metric for comparing merge trees. However, computing this distance exactly is NP-hard. First fixed-parameter tractable (FPT) algorithm for it's exact computation introduces the concept of an $\varepsilon$-good map between two merge trees, where $\varepsilon$ is a candidate value for the interleaving distance. The complexity of their algorithm is $O(2^{2τ}(2τ)^{2τ+2}\cdot n^2\log^3n)$ where $τ$ is the degree-bound parameter and $n$ is the total number of nodes in both the merge trees. Their algorithm exhibits exponential complexity in $τ$, which increases with the increasing value of $\varepsilon$. In the current paper, we propose an improved FPT algorithm for computing the $\varepsilon$-good map between two merge trees. Our algorithm introduces two new parameters, $η_f$ and $η_g$, corresponding to the numbers of leaf nodes in the merge trees $M_f$ and $M_g$, respectively. This parametrization is motivated by the observation that a merge tree can be decomposed into a collection of unique leaf-to-root paths. The proposed algorithm achieves a complexity of $O\!\left(n^2\log n+η_g^{η_f}(η_f+η_g)\, n \log n \right)$. To obtain this reduced complexity, we assume that number of possible $\varepsilon$-good maps from $M_f$ to $M_g$ does not exceed that from $M_g$ to $M_f$. Notably, the parameters $η_f$ and $η_g$ are independent of the choice of $\varepsilon$. Compared to their algorithm, our approach substantially reduces the search space for computing an optimal $\varepsilon$-good map. We also provide a formal proof of correctness for the proposed algorithm.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [237] [Evaluating Few-Shot Temporal Reasoning of LLMs for Human Activity Prediction in Smart Environments](https://arxiv.org/abs/2602.11176)
*Maral Doctorarastoo,Katherine A. Flanigan,Mario Bergés,Christopher McComb*

Main category: cs.CL

TL;DR: 本文探索大语言模型能否通过紧凑的上下文线索推理日常活动，以填补现有数据驱动的基于代理模型在低数据环境中的不足，实验表明大语言模型有较强的人类行为时间理解能力，可作为有前景的时间推理器。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动的基于代理模型在低数据环境中存在困难，限制了其实用性，需探索新方法。

Method: 采用检索增强提示策略，整合时间、空间、行为历史和人物角色四种上下文来源，并在CASAS Aruba智能家居数据集上进行评估，测试不同数量的少样本示例。

Result: 大语言模型即使在零样本设置下也能产生连贯的日常活动预测，添加一两个示例可进一步完善持续时间校准和分类准确性，超过几个示例后性能饱和。

Conclusion: 预训练语言模型可作为有前景的时间推理器，能捕捉重复例程和依赖上下文的行为变化，加强基于代理模型的行为模块。

Abstract: Anticipating human activities and their durations is essential in applications such as smart-home automation, simulation-based architectural and urban design, activity-based transportation system simulation, and human-robot collaboration, where adaptive systems must respond to human activities. Existing data-driven agent-based models--from rule-based to deep learning--struggle in low-data environments, limiting their practicality. This paper investigates whether large language models, pre-trained on broad human knowledge, can fill this gap by reasoning about everyday activities from compact contextual cues. We adopt a retrieval-augmented prompting strategy that integrates four sources of context--temporal, spatial, behavioral history, and persona--and evaluate it on the CASAS Aruba smart-home dataset. The evaluation spans two complementary tasks: next-activity prediction with duration estimation, and multi-step daily sequence generation, each tested with various numbers of few-shot examples provided in the prompt. Analyzing few-shot effects reveals how much contextual supervision is sufficient to balance data efficiency and predictive accuracy, particularly in low-data environments. Results show that large language models exhibit strong inherent temporal understanding of human behavior: even in zero-shot settings, they produce coherent daily activity predictions, while adding one or two demonstrations further refines duration calibration and categorical accuracy. Beyond a few examples, performance saturates, indicating diminishing returns. Sequence-level evaluation confirms consistent temporal alignment across few-shot conditions. These findings suggest that pre-trained language models can serve as promising temporal reasoners, capturing both recurring routines and context-dependent behavioral variations, thereby strengthening the behavioral modules of agent-based models.

</details>


### [238] [HybridRAG: A Practical LLM-based ChatBot Framework based on Pre-Generated Q&A over Raw Unstructured Documents](https://arxiv.org/abs/2602.11156)
*Sungmoon Kim,Hyuna Jeon,Dahye Kim,Mingyu Kim,Dong-Kyu Chae,Jiwoong Kim*

Main category: cs.CL

TL;DR: 提出HybridRAG框架处理非结构化PDF文档，实验显示其比标准RAG基线质量更高、延迟更低，适用于现实聊天机器人应用。


<details>
  <summary>Details</summary>
Motivation: 现有RAG研究假定文本源结构良好，在查询时进行检索和生成，限制了其在现实聊天机器人场景中的适用性。

Method: 通过OCR和布局分析处理原始非结构化PDF文档，转化为分层文本块，用大语言模型预生成问答知识库，查询时先匹配知识库，无匹配再即时生成。

Result: 在OHRBench上的实验表明，HybridRAG比标准RAG基线答案质量更高、延迟更低。

Conclusion: HybridRAG是处理大量非结构化文档和大量用户、计算资源有限的现实聊天机器人应用的实用解决方案。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful approach for grounding Large Language Model (LLM)-based chatbot responses on external knowledge. However, existing RAG studies typically assume well-structured textual sources (e.g. Wikipedia or curated datasets) and perform retrieval and generation at query time, which can limit their applicability in real-world chatbot scenarios. In this paper, we present HybridRAG, a novel and practical RAG framework towards more accurate and faster chatbot responses. First, HybridRAG ingests raw, unstructured PDF documents containing complex layouts (text, tables, figures) via Optical Character Recognition (OCR) and layout analysis, and convert them into hierarchical text chunks. Then, it pre-generates a plausible question-answer (QA) knowledge base from the organized chunks using an LLM. At query time, user questions are matched against this QA bank to retrieve immediate answers when possible, and only if no suitable QA match is found does our framework fall back to an on-the-fly response generation. Experiments on OHRBench demonstrate that our HybridRAG provides higher answer quality and lower latency compared to a standard RAG baseline. We believe that HybridRAG could be a practical solution for real-world chatbot applications that must handle large volumes of unstructured documents and lots of users under limited computational resources.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [239] [SLD-L2S: Hierarchical Subspace Latent Diffusion for High-Fidelity Lip to Speech Synthesis](https://arxiv.org/abs/2602.11477)
*Yifan Liang,Andong Li,Kang Yang,Guochen Yu,Fangkun Liu,Lingling Dai,Xiaodong Li,Chengshi Zheng*

Main category: eess.AS

TL;DR: 本文提出基于分层子空间潜在扩散模型的SLD - L2S唇语合成框架，避免传统中间表征信息损失，实验显示其在多基准数据集上达最优。


<details>
  <summary>Details</summary>
Motivation: 当前唇语合成方法依赖中间表征，潜在扩散模型在该任务潜力未充分挖掘，需避免传统中间表征信息损失。

Method: 构建基于分层子空间潜在扩散模型的SLD - L2S框架，用分层架构和扩散卷积块（DiCB）处理视觉表征，采用重参数化流匹配技术生成目标潜在向量。

Result: SLD - L2S在多个基准数据集上达到最优生成质量，在客观和主观评估中超越现有方法。

Conclusion: 所提出的SLD - L2S框架有效，能提升唇语合成的语音质量。

Abstract: Although lip-to-speech synthesis (L2S) has achieved significant progress in recent years, current state-of-the-art methods typically rely on intermediate representations such as mel-spectrograms or discrete self-supervised learning (SSL) tokens. The potential of latent diffusion models (LDMs) in this task remains largely unexplored. In this paper, we introduce SLD-L2S, a novel L2S framework built upon a hierarchical subspace latent diffusion model. Our method aims to directly map visual lip movements to the continuous latent space of a pre-trained neural audio codec, thereby avoiding the information loss inherent in traditional intermediate representations. The core of our method is a hierarchical architecture that processes visual representations through multiple parallel subspaces, initiated by a subspace decomposition module. To efficiently enhance interactions within and between these subspaces, we design the diffusion convolution block (DiCB) as our network backbone. Furthermore, we employ a reparameterized flow matching technique to directly generate the target latent vectors. This enables a principled inclusion of speech language model (SLM) and semantic losses during training, moving beyond conventional flow matching objectives and improving synthesized speech quality. Our experiments show that SLD-L2S achieves state-of-the-art generation quality on multiple benchmark datasets, surpassing existing methods in both objective and subjective evaluations.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [240] [GORGO: Maximizing KV-Cache Reuse While Minimizing Network Latency in Cross-Region LLM Load Balancing](https://arxiv.org/abs/2602.11688)
*Alessio Ricci Toniolo,Abinaya Dinesh,Rome Thorstenson*

Main category: cs.NI

TL;DR: 提出GORGO方法优化多区域LLM推理的TTFT，经与三个基线对比，证明GORGO可降低P99 TTFT，GORGO - proxy在TTFT中位数上快2.5倍。


<details>
  <summary>Details</summary>
Motivation: 现有多区域负载均衡器在路由决策时忽略集群网络延迟，而分布式LLM推理需优化TTFT。

Method: 引入GORGO方法，通过优化总服务成本（考虑可用计算、网络延迟和前缀缓存）来最小化TTFT，同时与三个基线进行对比测试。

Result: GORGO通过网络感知路由降低P99 TTFT，防止跨区域转发问题提高平均TTFT，GORGO - proxy克服同步开销，TTFT中位数快2.5倍。

Conclusion: GORGO方法有效，集中式路由器GORGO - proxy表现出色。

Abstract: Distributing LLM inference across geographical regions can improve Time-to-First-Token (TTFT) by regionalizing service deployments. While existing multi-region load balancers save prefill computation by prioritizing Key--Value (KV) Cache hit rate, they ignore cluster networking latency, a critical factor in routing decisions. We introduce GORGO, a method for minimizing TTFT by optimizing a total serving cost as a function of available compute, network latency, and prefix caching. Using extensive profiling on custom infrastructure, we analyze component-level latency bottlenecks and benchmark GORGO against three baselines: (1) naive least-load routing, which ignores prefix-cache overlap; (2) prefix-similarity routing, which selectively pushes requests to the replica with the highest cached-prefix overlap; and (3) a centralized HTTP proxy that runs the GORGO policy while tracking requests across all nodes. We demonstrate that GORGO reduces P99 TTFT through network-aware routing and improves average TTFT by preventing pathological cross-region forwarding. Additionally, we find that GORGO-proxy overcomes synchronization overhead in previous methods and is 2.5x faster on median TTFT, demonstrating the success of a centralized router.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [241] [Mixed-Integer Programming for Change-point Detection](https://arxiv.org/abs/2602.11947)
*Apoorva Narula,Santanu S. Dey,Yao Xie*

Main category: math.OC

TL;DR: 提出离线多变化点检测的混合整数规划方法，加强了MIP公式，扩展到两个研究方向，实验表明在求解时间上有优势。


<details>
  <summary>Details</summary>
Motivation: 解决离线多变化点检测问题，提供比现有方法更优的方案。

Method: 将问题转化为全局最优分段线性拟合问题，提出加强的MIP公式，并扩展到多维和稀疏变化点检测场景。

Result: 在真实数据集上的广泛计算实验表明，在L1和L2损失函数下，所提公式相比现有技术减少了解决方案时间。

Conclusion: 提出的离线多变化点检测方法在求解效率上优于现有方法，适用于多维和稀疏变化点检测场景。

Abstract: We present a new mixed-integer programming (MIP) approach for offline multiple change-point detection by casting the problem as a globally optimal piecewise linear (PWL) fitting problem. Our main contribution is a family of strengthened MIP formulations whose linear programming (LP) relaxations admit integral projections onto the segment assignment variables, which encode the segment membership of each data point. This property yields provably tighter relaxations than existing formulations for offline multiple change-point detection. We further extend the framework to two settings of active research interest: (i) multidimensional PWL models with shared change-points, and (ii) sparse change-point detection, where only a subset of dimensions undergo structural change. Extensive computational experiments on benchmark real-world datasets demonstrate that the proposed formulations achieve reductions in solution times under both $\ell_1$ and $\ell_2$ loss functions in comparison to the state-of-the-art.

</details>


<div id='q-fin.MF'></div>

# q-fin.MF [[Back]](#toc)

### [242] [Liquidation Dynamics in DeFi and the Role of Transaction Fees](https://arxiv.org/abs/2602.12104)
*Agathe Sadeghi,Zachary Feinstein*

Main category: q-fin.MF

TL;DR: 本文研究去中心化金融借贷协议清算机制，刻画最优清算策略，揭示CPMM交易费用可抵御操纵。


<details>
  <summary>Details</summary>
Motivation: 借贷协议清算机制易受价格操纵和MEV影响，需研究最优清算策略及抵御方法。

Method: 通过动态规划刻画最优清算策略，明确建模OEV，推导清算边界。

Result: 得出封闭形式的清算边界，证明CPMM交易费用是关键安全参数，可使操纵无利可图。

Conclusion: CPMM交易费用有双重作用，补偿流动性提供者且可抵御操纵。

Abstract: Liquidation of collateral are the primary safeguard for solvency of lending protocols in decentralized finance. However, the mechanics of liquidations expose these protocols to predatory price manipulations and other forms of Maximal Extractable Value (MEV). In this paper, we characterize the optimal liquidation strategy, via a dynamic program, from the perspective of a profit-maximizing liquidator when the spot oracle is given by a Constant Product Market Maker (CPMM). We explicitly model Oracle Extractable Value (OEV) where liquidators manipulate the CPMM with sandwich attacks to trigger profitable liquidation events. We derive closed-form liquidation bounds and prove that CPMM transaction fees act as a critical security parameter. Crucially, we demonstrate that fees do not merely reduce attacker profits, but can make such manipulations unprofitable for an attacker. Our findings suggest that CPMM transaction fees serve a dual purpose: compensating liquidity providers and endogenously hardening CPMM oracles against manipulation without the latency of time-weighted averages or medianization.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [243] [Locally Interpretable Individualized Treatment Rules for Black-Box Decision Models](https://arxiv.org/abs/2602.11520)
*Yasin Khadem Charvadeh,Katherine S. Panageas,Yuan Chen*

Main category: stat.ME

TL;DR: 提出Locally Interpretable Individualized Treatment Rule (LI - ITR)方法优化医疗个性化治疗规则，模拟研究和实际应用验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有个性化治疗规则方法存在依赖可解释但不灵活模型或牺牲可解释性的黑盒方法，且多采用单一全局决策规则的问题。

Method: 结合灵活机器学习模型和局部可解释近似，用变分自编码器生成局部合成样本，通过可解释专家混合学习个体决策规则。

Result: 模拟研究显示LI - ITR能准确恢复真实主体局部系数和最佳治疗策略；乳腺癌副作用精准管理应用体现其灵活预测建模的必要性和实际效用。

Conclusion: LI - ITR在估计最佳治疗规则时能提供透明、临床可解释的解释，具有实用价值。

Abstract: Individualized treatment rules (ITRs) aim to optimize healthcare by tailoring treatment decisions to patient-specific characteristics. Existing methods typically rely on either interpretable but inflexible models or highly flexible black-box approaches that sacrifice interpretability; moreover, most impose a single global decision rule across patients. We introduce the Locally Interpretable Individualized Treatment Rule (LI-ITR) method, which combines flexible machine learning models to accurately learn complex treatment outcomes with locally interpretable approximations to construct subject-specific treatment rules. LI-ITR employs variational autoencoders to generate realistic local synthetic samples and learns individualized decision rules through a mixture of interpretable experts. Simulation studies show that LI-ITR accurately recovers true subject-specific local coefficients and optimal treatment strategies. An application to precision side-effect management in breast cancer illustrates the necessity of flexible predictive modeling and highlights the practical utility of LI-ITR in estimating optimal treatment rules while providing transparent, clinically interpretable explanations.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [244] [Cooperation Breakdown in LLM Agents Under Communication Delays](https://arxiv.org/abs/2602.11754)
*Keita Nishimoto,Kimitaka Asatani,Ichiro Sakata*

Main category: cs.MA

TL;DR: 提出FLCOA框架研究LLM - MAS中协作问题，引入带通信延迟的连续囚徒困境模拟，发现延迟与合作呈U形关系，指出MAS研究新方向。


<details>
  <summary>Details</summary>
Motivation: 为使LLM - MAS能在现实中部署，需让智能体在计算和通信约束下实现合作与协调。

Method: 提出FLCOA框架理论化智能体协作过程；引入带通信延迟的连续囚徒困境进行模拟实验。

Result: 延迟增加时智能体开始利用反应慢来获利；过度延迟减少剥削循环，延迟幅度与相互合作呈U形关系。

Conclusion: 促进合作不仅要关注高层制度设计，还需关注通信延迟和资源分配等底层因素。

Abstract: LLM-based multi-agent systems (LLM-MAS), in which autonomous AI agents cooperate to solve tasks, are gaining increasing attention. For such systems to be deployed in society, agents must be able to establish cooperation and coordination under real-world computational and communication constraints. We propose the FLCOA framework (Five Layers for Cooperation/Coordination among Autonomous Agents) to conceptualize how cooperation and coordination emerge in groups of autonomous agents, and highlight that the influence of lower-layer factors - especially computational and communication resources - has been largely overlooked. To examine the effect of communication delay, we introduce a Continuous Prisoner's Dilemma with Communication Delay and conduct simulations with LLM-based agents. As delay increases, agents begin to exploit slower responses even without explicit instructions. Interestingly, excessive delay reduces cycles of exploitation, yielding a U-shaped relationship between delay magnitude and mutual cooperation. These results suggest that fostering cooperation requires attention not only to high-level institutional design but also to lower-layer factors such as communication delay and resource allocation, pointing to new directions for MAS research.

</details>
