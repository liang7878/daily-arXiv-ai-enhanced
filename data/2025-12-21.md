<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 57]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.DB](#cs.DB) [Total: 6]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.DS](#cs.DS) [Total: 5]
- [cs.GT](#cs.GT) [Total: 3]
- [cs.IR](#cs.IR) [Total: 6]
- [cs.LG](#cs.LG) [Total: 93]
- [cs.NE](#cs.NE) [Total: 3]
- [cs.PF](#cs.PF) [Total: 2]
- [cs.SE](#cs.SE) [Total: 11]
- [q-fin.CP](#q-fin.CP) [Total: 2]
- [q-fin.RM](#q-fin.RM) [Total: 1]
- [q-fin.TR](#q-fin.TR) [Total: 2]
- [stat.ML](#stat.ML) [Total: 5]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.CL](#cs.CL) [Total: 15]
- [econ.EM](#econ.EM) [Total: 1]
- [q-fin.GN](#q-fin.GN) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.HC](#cs.HC) [Total: 4]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [cs.ET](#cs.ET) [Total: 2]
- [math.OC](#math.OC) [Total: 6]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 3]
- [stat.ME](#stat.ME) [Total: 4]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [nlin.AO](#nlin.AO) [Total: 1]
- [cs.CV](#cs.CV) [Total: 36]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.PL](#cs.PL) [Total: 4]
- [cs.RO](#cs.RO) [Total: 5]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [q-fin.PR](#q-fin.PR) [Total: 1]
- [econ.GN](#econ.GN) [Total: 4]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.CR](#cs.CR) [Total: 19]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [cs.CY](#cs.CY) [Total: 8]
- [eess.SP](#eess.SP) [Total: 4]
- [cs.CG](#cs.CG) [Total: 2]
- [econ.TH](#econ.TH) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [math.NA](#math.NA) [Total: 2]
- [cs.LO](#cs.LO) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Anubuddhi: A Multi-Agent AI System for Designing and Simulating Quantum Optics Experiments](https://arxiv.org/abs/2512.15736)
*S. K. Rithvik*

Main category: cs.AI

TL;DR: 介绍了多智能体AI系统Anubuddhi，它能根据自然语言提示设计和模拟量子光学实验，评估多个实验显示有较高设计 - 模拟对齐分数，表明其对研究和教学有积极作用。


<details>
  <summary>Details</summary>
Motivation: 开发一个无需专业编程知识，可根据自然语言提示设计和模拟量子光学实验的系统，以推动量子光学实验设计的普及。

Method: 通过语义检索从三层工具箱中安排组件组成光学布局，结合意图路由、知识增强生成和双模式验证（QuTiP和FreeSim），并进行物理模拟和收敛细化。

Result: 评估13个实验，系统设计 - 模拟对齐分数达8 - 9/10，自由形式模拟在11/13个实验中表现优于约束框架。

Conclusion: 系统实现了正确的物理架构，但数值预测需专家审查，能为研究和教学提供初始设计，可通过对话迭代改进。

Abstract: We present Anubuddhi, a multi-agent AI system that designs and simulates quantum optics experiments from natural language prompts without requiring specialized programming knowledge. The system composes optical layouts by arranging components from a three-tier toolbox via semantic retrieval, then validates designs through physics simulation with convergent refinement. The architecture combines intent routing, knowledge-augmented generation, and dual-mode validation (QuTiP and FreeSim). We evaluated 13 experiments spanning fundamental optics (Hong-Ou-Mandel interference, Michelson/Mach-Zehnder interferometry, Bell states, delayed-choice quantum eraser), quantum information protocols (BB84 QKD, Franson interferometry, GHZ states, quantum teleportation, hyperentanglement), and advanced technologies (boson sampling, electromagnetically induced transparency, frequency conversion). The system achieves design-simulation alignment scores of 8--9/10, with simulations faithfully modeling intended physics. A critical finding distinguishes structural correctness from quantitative accuracy: high alignment confirms correct physics architecture, while numerical predictions require expert review. Free-form simulation outperformed constrained frameworks for 11/13 experiments, revealing that quantum optics diversity demands flexible mathematical representations. The system democratizes computational experiment design for research and pedagogy, producing strong initial designs users can iteratively refine through conversation.

</details>


### [2] [The Principle of Proportional Duty: A Knowledge-Duty Framework for Ethical Equilibrium in Human and Artificial Systems](https://arxiv.org/abs/2512.15740)
*Timothy Prescher*

Main category: cs.AI

TL;DR: 本文提出比例责任原则（PPD）框架，揭示道德责任随不确定性的变化规律，用方程表达动态关系，经模拟验证其优势，还跨领域验证有效性，表明比例责任可稳定复杂系统。


<details>
  <summary>Details</summary>
Motivation: 传统伦理框架难以对不确定下的决策建模，本文旨在提出新框架解决该问题。

Method: 提出PPD框架，用方程表达责任动态关系，进行蒙特卡罗模拟，跨临床伦理、法律、经济治理和人工智能四个领域应用验证。

Result: 系统保持基线谦逊系数可产生更稳定的责任分配，降低过度自信决策风险，PPD框架在四个领域应用有效。

Conclusion: 比例责任原则可作为复杂系统的稳定原则，通过平衡认知信心和情境风险防止过度和疏漏。

Abstract: Traditional ethical frameworks often struggle to model decision-making under uncertainty, treating it as a simple constraint on action. This paper introduces the Principle of Proportional Duty (PPD), a novel framework that models how ethical responsibility scales with an agent's epistemic state. The framework reveals that moral duty is not lost to uncertainty but transforms: as uncertainty increases, Action Duty (the duty to act decisively) is proportionally converted into Repair Duty (the active duty to verify, inquire, and resolve uncertainty).
  This dynamic is expressed by the equation D_total = K[(1-HI) + HI * g(C_signal)], where Total Duty is a function of Knowledge (K), Humility/Uncertainty (HI), and Contextual Signal Strength (C_signal). Monte Carlo simulations demonstrate that systems maintaining a baseline humility coefficient (lambda > 0) produce more stable duty allocations and reduce the risk of overconfident decision-making.
  By formalizing humility as a system parameter, the PPD offers a mathematically tractable approach to moral responsibility that could inform the development of auditable AI decision systems. This paper applies the framework across four domains, clinical ethics, recipient-rights law, economic governance, and artificial intelligence, to demonstrate its cross-disciplinary validity. The findings suggest that proportional duty serves as a stabilizing principle within complex systems, preventing both overreach and omission by dynamically balancing epistemic confidence against contextual risk.

</details>


### [3] [Prompt-to-Parts: Generative AI for Physical Assembly and Scalable Instructions](https://arxiv.org/abs/2512.15743)
*David Noever*

Main category: cs.AI

TL;DR: 提出从自然语言描述生成物理可实现装配指令的框架，利用LDraw中间表示，用大语言模型生成有效构建序列，评估复杂领域输出，提出新方法连接语义设计与可制造输出，为制造和工程原型设计带来新选择。


<details>
  <summary>Details</summary>
Motivation: 解决之前像素扩散方法和CAD模型无法支持复杂装配指令或组件交换的问题，实现从自然语言规范到物理原型制作，连接语义设计意图和可制造输出。

Method: 在离散零件词汇表内操作，利用LDraw作为文本丰富的中间表示，用工具引导大语言模型生成有效构建序列，引入Python库进行模型生成。

Result: 能够为超过3000个装配零件的积木原型生成有效的分步构建序列和装配指令，在复杂卫星、飞机和建筑领域评估了可构建输出。

Conclusion: 提出的“积木包”方法作为物理API，为制造和工程原型设计的自然语言实现提供新的设计选择。

Abstract: We present a framework for generating physically realizable assembly instructions from natural language descriptions. Unlike unconstrained text-to-3D approaches, our method operates within a discrete parts vocabulary, enforcing geometric validity, connection constraints, and buildability ordering. Using LDraw as a text-rich intermediate representation, we demonstrate that large language models can be guided with tools to produce valid step-by-step construction sequences and assembly instructions for brick-based prototypes of more than 3000 assembly parts. We introduce a Python library for programmatic model generation and evaluate buildable outputs on complex satellites, aircraft, and architectural domains. The approach aims for demonstrable scalability, modularity, and fidelity that bridges the gap between semantic design intent and manufacturable output. Physical prototyping follows from natural language specifications. The work proposes a novel elemental lingua franca as a key missing piece from the previous pixel-based diffusion methods or computer-aided design (CAD) models that fail to support complex assembly instructions or component exchange. Across four original designs, this novel "bag of bricks" method thus functions as a physical API: a constrained vocabulary connecting precisely oriented brick locations to a "bag of words" through which arbitrary functional requirements compile into material reality. Given such a consistent and repeatable AI representation opens new design options while guiding natural language implementations in manufacturing and engineering prototyping.

</details>


### [4] [Emergence: Overcoming Privileged Information Bias in Asymmetric Embodied Agents via Active Querying](https://arxiv.org/abs/2512.15776)
*Shaun Baek,Sam Liu,Joseph Ukpong*

Main category: cs.AI

TL;DR: 研究大语言模型在具身环境中因信息不对称的符号接地问题，提出框架量化，发现成功差距，证明拉式协议更优，强调主动减少不确定性对协作的重要性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在具身环境存在符号接地问题，特别是信息不对称时，研究特权信息偏差现象。

Method: 在AI2 - THOR中提出新颖的不对称辅助推理框架进行实验。

Result: 发现成功差距，拉式协议比推式指令更稳健，成功回合澄清请求频率是两倍。

Conclusion: 主动减少不确定性是安全的人机和机器人协作的先决条件。

Abstract: Large Language Models (LLMs) act as powerful reasoning engines but struggle with "symbol grounding" in embodied environments, particularly when information is asymmetrically distributed. We investigate the Privileged Information Bias (or "Curse of Knowledge"), where a knowledgeable "Leader" agent fails to guide a sensor-limited "Follower" due to a lack of Theory of Mind. To quantify this phenomenon, we propose a novel Asymmetric Assistive Reasoning framework within AI2-THOR. Our experiments reveal a significant "Success Gap": while the Leader successfully perceives the target in 35.0% of episodes, the collaborative team succeeds only 17.0% of the time, implying that nearly 50% of feasible plans fail solely due to communicative grounding errors. We demonstrate that a "Pull-based" protocol (active querying) is significantly more robust than standard "Push-based" instruction, with successful episodes featuring 2x the frequency of clarification requests. This research isolates the mechanism of active uncertainty reduction as a prerequisite for safe human-AI and robot-robot collaboration.

</details>


### [5] [AI Epidemiology: achieving explainable AI through expert oversight patterns](https://arxiv.org/abs/2512.15783)
*Kit Tempest-Walters*

Main category: cs.AI

TL;DR: 提出AI流行病学框架，用于管理和解释高级AI系统，绕过模型复杂性问题，实现对AI输出的群体级监测，为专家和机构提供便利。


<details>
  <summary>Details</summary>
Motivation: 解决当前可解释性方法在大规模模型部署时受模型复杂性困扰的问题。

Method: 将AI - 专家交互标准化为风险等级、对齐分数和准确性分数等结构化评估字段，通过统计关联预测输出失败，并与专家覆盖和现实结果验证；被动跟踪专家与AI建议的收敛和分歧。

Result: 框架对专家无负担，能提供自动审计跟踪，模型更新和更换供应商时治理连续，能让专家和机构在AI输出造成危害前检测到不可靠输出。

Conclusion: 该框架使领域专家无需机器学习专业知识就能管理AI系统，实现了AI监督的民主化。

Abstract: AI Epidemiology is a framework for governing and explaining advanced AI systems by applying population-level surveillance methods to AI outputs. The approach mirrors the way in which epidemiologists enable public health interventions through statistical evidence before molecular mechanisms are understood. This bypasses the problem of model complexity which plagues current interpretability methods (such as SHAP and mechanistic interpretability) at the scale of deployed models.
  AI Epidemiology achieves this population-level surveillance by standardising capture of AI-expert interactions into structured assessment fields: risk level, alignment score, and accuracy score. These function as exposure variables which predict output failure through statistical associations, much like cholesterol and blood pressure act as exposure variables predicting cardiac events. Output-failure associations are subsequently validated against expert overrides and real-world outcomes.
  The framework places zero burden on experts and provides automatic audit trails by passively tracking expert convergence and divergence with AI recommendations. Since it analyses outputs rather than internal model computations, it also provides governance continuity when institutions update models and switch vendors. Finally, by providing reliability scores and semantic assessments (e.g. 'this recommendation resembles 500 cases overridden by experts due to guideline violations'), it enables experts and institutions to detect unreliable AI outputs before they cause harm. This democratises AI oversight by enabling domain experts to govern AI systems without requiring machine learning expertise.

</details>


### [6] [Beyond Training: Enabling Self-Evolution of Agents with MOBIMEM](https://arxiv.org/abs/2512.15784)
*Zibin Liu,Cheng Zhang,Xi Zhao,Yunfei Feng,Bingyu Bai,Dahu Feng,Erhu Feng,Yubin Xia,Haibo Chen*

Main category: cs.AI

TL;DR: 针对大语言模型代理部署后难以自进化的问题，提出内存中心的MOBIMEM系统，经评估有良好表现。


<details>
  <summary>Details</summary>
Motivation: 当前以模型为中心的代理架构部署后难以自我进化，持续重新训练模型开销大且存在精度和推理效率的权衡问题。

Method: 提出MOBIMEM系统，引入三种专门的内存原语解耦代理进化与模型权重，还集成了一套受操作系统启发的服务来编排执行。

Result: 在AndroidWorld和前50个应用上评估，MOBIMEM实现83.1%的配置文件对齐，检索时间23.83ms（比GraphRAG基线快280倍），任务成功率提高50.3%，移动设备上的端到端延迟最多降低9倍。

Conclusion: MOBIMEM系统无需模型重新训练就能实现迭代自进化，有效解决了现有架构的问题，有较好的性能提升。

Abstract: Large Language Model (LLM) agents are increasingly deployed to automate complex workflows in mobile and desktop environments. However, current model-centric agent architectures struggle to self-evolve post-deployment: improving personalization, capability, and efficiency typically requires continuous model retraining/fine-tuning, which incurs prohibitive computational overheads and suffers from an inherent trade-off between model accuracy and inference efficiency.
  To enable iterative self-evolution without model retraining, we propose MOBIMEM, a memory-centric agent system. MOBIMEM first introduces three specialized memory primitives to decouple agent evolution from model weights: (1) Profile Memory uses a lightweight distance-graph (DisGraph) structure to align with user preferences, resolving the accuracy-latency trade-off in user profile retrieval; (2) Experience Memory employs multi-level templates to instantiate execution logic for new tasks, ensuring capability generalization; and (3) Action Memory records fine-grained interaction sequences, reducing the reliance on expensive model inference. Building upon this memory architecture, MOBIMEM further integrates a suite of OS-inspired services to orchestrate execution: a scheduler that coordinates parallel sub-task execution and memory operations; an agent record-and-replay (AgentRR) mechanism that enables safe and efficient action reuse; and a context-aware exception handling that ensures graceful recovery from user interruptions and runtime errors.
  Evaluation on AndroidWorld and top-50 apps shows that MOBIMEM achieves 83.1% profile alignment with 23.83 ms retrieval time (280x faster than GraphRAG baselines), improves task success rates by up to 50.3%, and reduces end-to-end latency by up to 9x on mobile devices.

</details>


### [7] [State-Augmented Graphs for Circular Economy Triage](https://arxiv.org/abs/2512.15824)
*Richard Fox,Rui Li,Gustav Jonsson,Farzaneh Goli,Miying Yang,Emel Aktas,Yongjing Wang*

Main category: cs.AI

TL;DR: 本文提出一种新的决策框架用于循环经济分类决策，以电动车电池分级分类为例展示其灵活性，为优化循环经济分类决策提供基础。


<details>
  <summary>Details</summary>
Motivation: 有效循环经济分类需要平衡保留价值与处理和劳动力成本及约束的自适应决策。

Method: 提出一种基于状态增强的拆卸序列规划（DSP）图的简单确定性求解器的决策框架，将拆卸历史编码到状态中以满足马尔可夫性质。

Result: 通过电动车电池分级分类的实例展示了框架的灵活性，能适应不同机械复杂性、安全要求和经济驱动因素。

Conclusion: 该统一形式主义为跨不同产品和运营环境优化循环经济分类决策提供了易于处理和可推广的基础。

Abstract: Circular economy (CE) triage is the assessment of products to determine which sustainable pathway they can follow once they reach the end of their usefulness as they are currently being used. Effective CE triage requires adaptive decisions that balance retained value against the costs and constraints of processing and labour. This paper presents a novel decision-making framework as a simple deterministic solver over a state-augmented Disassembly Sequencing Planning (DSP) graph. By encoding the disassembly history into the state, our framework enforces the Markov property, enabling optimal, recursive evaluation by ensuring each decision only depends on the previous state. The triage decision involves choices between continuing disassembly or committing to a CE option. The model integrates condition-aware utility based on diagnostic health scores and complex operational constraints. We demonstrate the framework's flexibility with a worked example: the hierarchical triage of electric vehicle (EV) batteries, where decisions are driven by the recursive valuation of components. The example illustrates how a unified formalism enables the accommodation of varying mechanical complexity, safety requirements, and economic drivers. This unified formalism therefore provides a tractable and generalisable foundation for optimising CE triage decisions across diverse products and operational contexts.

</details>


### [8] [PediatricAnxietyBench: Evaluating Large Language Model Safety Under Parental Anxiety and Pressure in Pediatric Consultations](https://arxiv.org/abs/2512.15894)
*Vahideh Zolfaghari*

Main category: cs.AI

TL;DR: 论文介绍PediatricAnxietyBench基准评估大语言模型儿科指导安全性，发现模型有漏洞，该基准可揭示标准基准忽略的失败模式。


<details>
  <summary>Details</summary>
Motivation: 大语言模型用于儿科指导，但在真实对抗压力下的安全性未知，父母的紧急语言可能使模型给出有害建议。

Method: 创建包含300个高质量查询的PediatricAnxietyBench基准，用多维安全框架评估两个Llama模型，对抗性查询包含多种压力模式。

Result: 平均安全得分5.50/15，70B模型优于8B模型，对抗性查询使安全性降低8%，癫痫和接种后查询有漏洞，回避与安全强相关，缺乏紧急情况识别。

Conclusion: 模型规模影响安全性，所有模型对真实父母压力有漏洞，PediatricAnxietyBench可揭示临床显著失败模式。

Abstract: Large language models (LLMs) are increasingly consulted by parents for pediatric guidance, yet their safety under real-world adversarial pressures is poorly understood. Anxious parents often use urgent language that can compromise model safeguards, potentially causing harmful advice. PediatricAnxietyBench is an open-source benchmark of 300 high-quality queries across 10 pediatric topics (150 patient-derived, 150 adversarial) enabling reproducible evaluation. Two Llama models (70B and 8B) were assessed using a multi-dimensional safety framework covering diagnostic restraint, referral adherence, hedging, and emergency recognition. Adversarial queries incorporated parental pressure patterns, including urgency, economic barriers, and challenges to disclaimers. Mean safety score was 5.50/15 (SD=2.41). The 70B model outperformed the 8B model (6.26 vs 4.95, p<0.001) with lower critical failures (4.8% vs 12.0%, p=0.02). Adversarial queries reduced safety by 8% (p=0.03), with urgency causing the largest drop (-1.40). Vulnerabilities appeared in seizures (33.3% inappropriate diagnosis) and post-vaccination queries. Hedging strongly correlated with safety (r=0.68, p<0.001), while emergency recognition was absent. Model scale influences safety, yet all models showed vulnerabilities to realistic parental pressures. PediatricAnxietyBench provides a reusable adversarial evaluation framework to reveal clinically significant failure modes overlooked by standard benchmarks.

</details>


### [9] [Science Consultant Agent](https://arxiv.org/abs/2512.16171)
*Karthikeyan K,Philip Wu,Xin Tang,Alexandre Alves*

Main category: cs.AI

TL;DR: Science Consultant Agent是基于网络的AI工具，通过四核心组件加速AI方案开发。


<details>
  <summary>Details</summary>
Motivation: 帮助从业者为基于AI的解决方案选择和实施最有效的建模策略。

Method: 结合结构化问卷、基于文献的解决方案推荐和原型生成，通过问卷、智能填充、研究导向推荐和原型构建器四个核心组件运行。

Result: 加速了从产品经理、软件开发人员到研究人员等各类人员的开发。

Conclusion: Science Consultant Agent能有效助力AI解决方案开发。

Abstract: The Science Consultant Agent is a web-based Artificial Intelligence (AI) tool that helps practitioners select and implement the most effective modeling strategy for AI-based solutions. It operates through four core components: Questionnaire, Smart Fill, Research-Guided Recommendation, and Prototype Builder. By combining structured questionnaires, literature-backed solution recommendations, and prototype generation, the Science Consultant Agent accelerates development for everyone from Product Managers and Software Developers to Researchers. The full pipeline is illustrated in Figure 1.

</details>


### [10] [Darth Vecdor: An Open-Source System for Generating Knowledge Graphs Through Large Language Model Queries](https://arxiv.org/abs/2512.15906)
*Jonathan A. Handler*

Main category: cs.AI

TL;DR: 本文介绍了Darth Vecdor（DV），它可将大语言模型知识提取到SQL数据库，解决了LLM响应的一些问题，以开源形式发布，虽有风险但有望助力医疗。


<details>
  <summary>Details</summary>
Motivation: 为解决直接查询大语言模型在成本、速度、安全和置信度等方面的问题，以及应对LLM响应的诸多问题，开发可将知识结构化的工具。

Method: 构建具有特定功能的DV，提供简单的基于浏览器的图形用户界面。

Result: DV以免费、开源、可扩展的软件形式发布。

Conclusion: 合理使用DV及其输出有望改善医疗状况，但用户需注意潜在风险。

Abstract: Many large language models (LLMs) are trained on a massive body of knowledge present on the Internet. Darth Vecdor (DV) was designed to extract this knowledge into a structured, terminology-mapped, SQL database ("knowledge base" or "knowledge graph"). Knowledge graphs may be useful in many domains, including healthcare. Although one might query an LLM directly rather than a SQL-based knowledge graph, concerns such as cost, speed, safety, and confidence may arise, especially in high-volume operations. These may be mitigated when the information is pre-extracted from the LLM and becomes query-able through a standard database. However, the author found the need to address several issues. These included erroneous, off-topic, free-text, overly general, and inconsistent LLM responses, as well as allowing for multi-element responses. DV was built with features intended to mitigate these issues. To facilitate ease of use, and to allow for prompt engineering by those with domain expertise but little technical background, DV provides a simple, browser-based graphical user interface. DV has been released as free, open-source, extensible software, on an "as is" basis, without warranties or conditions of any kind, either express or implied. Users need to be cognizant of the potential risks and benefits of using DV and its outputs, and users are responsible for ensuring any use is safe and effective. DV should be assumed to have bugs, potentially very serious ones. However, the author hopes that appropriate use of current and future versions of DV and its outputs can help improve healthcare.

</details>


### [11] [From Personalization to Prejudice: Bias and Discrimination in Memory-Enhanced AI Agents for Recruitment](https://arxiv.org/abs/2512.16532)
*Himanshu Gharat,Himanshi Agrawal,Gourab K. Patro*

Main category: cs.AI

TL;DR: 研究内存增强的大语言模型个性化代理的偏差问题，实验表明个性化会引入和强化偏差，需额外保护措施。


<details>
  <summary>Details</summary>
Motivation: 内存增强的个性化虽有益但会引入偏差，此前相关研究较少，故开展研究。

Method: 以招聘为例，模拟内存增强的个性化代理行为，研究各操作阶段偏差的引入和放大情况。

Result: 使用安全训练大语言模型的代理实验显示，个性化会系统地引入和强化偏差。

Conclusion: 内存增强的基于大语言模型的人工智能代理需要额外的保护措施或代理护栏。

Abstract: Large Language Models (LLMs) have empowered AI agents with advanced capabilities for understanding, reasoning, and interacting across diverse tasks. The addition of memory further enhances them by enabling continuity across interactions, learning from past experiences, and improving the relevance of actions and responses over time; termed as memory-enhanced personalization. Although such personalization through memory offers clear benefits, it also introduces risks of bias. While several previous studies have highlighted bias in ML and LLMs, bias due to memory-enhanced personalized agents is largely unexplored. Using recruitment as an example use case, we simulate the behavior of a memory-enhanced personalized agent, and study whether and how bias is introduced and amplified in and across various stages of operation. Our experiments on agents using safety-trained LLMs reveal that bias is systematically introduced and reinforced through personalization, emphasizing the need for additional protective measures or agent guardrails in memory-enhanced LLM-based AI agents.

</details>


### [12] [Leveraging Spreading Activation for Improved Document Retrieval in Knowledge-Graph-Based RAG Systems](https://arxiv.org/abs/2512.15922)
*Jovan Pavlović,Miklós Krész,László Hajdu*

Main category: cs.AI

TL;DR: 本文提出一种新的RAG框架，用传播激活算法从自动构建的知识图连接的文档语料库中检索信息，可提升大语言模型在复杂任务上的表现，实验表明其性能良好，在资源受限场景有效。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统难以可靠检索和连接复杂推理任务所需的多步证据，标准RAG框架忽视信息可信度差异，GraphRAG存在依赖高质量图表示等问题。

Method: 提出一种新的RAG框架，使用传播激活算法从自动构建的知识图连接的文档语料库中检索信息。

Result: 该方法性能优于或与迭代RAG方法相当，与思维链迭代检索结合时答案正确性相比普通RAG最多有39%的绝对提升，在资源受限场景有效。

Conclusion: 所提方法能提升大语言模型在复杂任务上的表现，可作为即插即用模块与多种基于RAG的方法集成。

Abstract: Despite initial successes and a variety of architectures, retrieval-augmented generation (RAG) systems still struggle to reliably retrieve and connect the multi-step evidence required for complicated reasoning tasks. Most of the standard RAG frameworks regard all retrieved information as equally reliable, overlooking the varying credibility and interconnected nature of large textual corpora. GraphRAG approaches offer potential improvement to RAG systems by integrating knowledge graphs, which structure information into nodes and edges, capture entity relationships, and enable multi-step logical traversal. However, GraphRAG is not always an ideal solution as it depends on high-quality graph representations of the corpus, which requires either pre-existing knowledge graphs that are expensive to build and update, or automated graph construction pipelines that are often unreliable. Moreover, systems following this paradigm typically use large language models to guide graph traversal and evidence retrieval, leading to challenges similar to those encountered with standard RAG. In this paper, we propose a novel RAG framework that employs the spreading activation algorithm to retrieve information from a corpus of documents interconnected by automatically constructed knowledge graphs, thereby enhancing the performance of large language models on complex tasks such as multi-hop question answering. Experiments show that our method achieves better or comparable performance to iterative RAG methodologies, while also being easily integrable as a plug-and-play module with a wide range of RAG-based approaches. Combining our method with chain-of-thought iterative retrieval yields up to a 39\% absolute gain in answer correctness compared to naive RAG, achieving these results with small open-weight language models and highlighting its effectiveness in resource-constrained settings.

</details>


### [13] [Small Language Models for Efficient Agentic Tool Calling: Outperforming Large Models with Targeted Fine-tuning](https://arxiv.org/abs/2512.15943)
*Polaris Jhandi,Owais Kazi,Shreyas Subramanian,Neel Sendas*

Main category: cs.AI

TL;DR: 研究用优化的小语言模型（SLMs）替代大语言模型（LLMs）工作流的可行性，实验表明微调的SLM表现出色，可降低生成式AI应用成本。


<details>
  <summary>Details</summary>
Motivation: LLMs计算需求大，企业日常使用成本高，因此探索能降低基础设施开销的SLMs。

Method: 训练领域适应的SLM执行传统由LLMs处理的任务，用Hugging Face TRL对facebook/opt - 350m模型进行单轮微调。

Result: 微调的SLM在ToolBench评估中通过率达77.55%，远超ChatGPT - CoT等基线模型。

Conclusion: 精心设计和针对性训练SLMs可降低采用障碍，使生成式AI能在生产系统中大规模集成。

Abstract: As organizations scale adoption of generative AI, model cost optimization and operational efficiency have emerged as critical factors determining sustainability and accessibility. While Large Language Models (LLMs) demonstrate impressive capabilities across diverse tasks, their extensive computational requirements make them cost-prohibitive for routine enterprise use. This limitation motivates the exploration of Small Language Models (SLMs), which can deliver comparable performance in targeted applications while drastically reducing infrastructure overhead (Irugalbandara et al., 2023). In this work, we investigate the feasibility of replacing LLM-driven workflows with optimized SLMs. We trained a domain-adapted SLM to execute representative tasks traditionally handled by LLMs, such as document summarization, query answering, and structured data interpretation. As part of the experiment, we investigated the fine-tuning of facebook/opt-350m model (single epoch only) using the Hugging Face TRL (Transformer Reinforcement Learning), specifically the Supervised Fine-Tuning (SFT) trainer. The OPT-350M model was released by Meta AI in 2022 as part of the OPT (Open Pretrained Transformer) family of models. Similar studies demonstrate that even models at the 350M parameter scale can meaningfully contribute to instruction-tuning pipelines (Mekala et al., 2024). Experimental results demonstrated that our fine-tuned SLM achieves exceptional performance with a 77.55\% pass rate on ToolBench evaluation, significantly outperforming all baseline models including ChatGPT-CoT (26.00\%), ToolLLaMA-DFS (30.18\%), and ToolLLaMA-CoT (16.27\%). These findings emphasize that thoughtful design and targeted training of SLMs can significantly lower barriers to adoption, enabling cost-effective, large-scale integration of generative AI into production systems.

</details>


### [14] [Subjective functions](https://arxiv.org/abs/2512.15948)
*Samuel J. Gershman*

Main category: cs.AI

TL;DR: 本文探讨目标函数来源与选择问题，提出基于主观函数的方法并以预期预测误差为例，与多领域有联系。


<details>
  <summary>Details</summary>
Motivation: 弄清楚目标函数的来源、如何选择目标，以及赋予人工智能系统像人类一样合成新目标函数的能力。

Method: 提出主观函数的概念，将其作为一种高阶目标函数，以内源性方式定义，并以预期预测误差作为主观函数的具体例子进行研究。

Result: 提出了基于主观函数回答相关问题的方法，且该方法与心理学、神经科学和机器学习的思想有诸多联系。

Conclusion: 通过主观函数的概念和研究可以为目标函数相关问题提供解决思路。

Abstract: Where do objective functions come from? How do we select what goals to pursue? Human intelligence is adept at synthesizing new objective functions on the fly. How does this work, and can we endow artificial systems with the same ability? This paper proposes an approach to answering these questions, starting with the concept of a subjective function, a higher-order objective function that is endogenous to the agent (i.e., defined with respect to the agent's features, rather than an external task). Expected prediction error is studied as a concrete example of a subjective function. This proposal has many connections to ideas in psychology, neuroscience, and machine learning.

</details>


### [15] [Conversational Time Series Foundation Models: Towards Explainable and Effective Forecasting](https://arxiv.org/abs/2512.16022)
*Defu Cao,Michael Gee,Jinbo Liu,Hengxuan Wang,Wei Yang,Rui Wang,Yan Liu*

Main category: cs.AI

TL;DR: 时间序列基础模型众多，无单一模型始终最优，提出将大语言模型作为智能裁判协调基础模型集成，经微调后效果超现有模型。


<details>
  <summary>Details</summary>
Motivation: 时间序列基础模型无单一最优，大语言模型直接用于时间序列预测效果不佳，需有效集成模型并具可解释性。

Method: 将大语言模型作为智能裁判，引入基于SHAP忠实度分数的R1式微调，使其解释集成权重，通过多轮对话评估和优化。

Result: 在GIFT - Eval基准测试的23个数据集97种设置上，在CRPS和MASE指标上显著优于领先模型，创最佳结果。

Conclusion: 该方法可有效协调时间序列基础模型的集成，提升预测性能。

Abstract: The proliferation of time series foundation models has created a landscape where no single method achieves consistent superiority, framing the central challenge not as finding the best model, but as orchestrating an optimal ensemble with interpretability. While Large Language Models (LLMs) offer powerful reasoning capabilities, their direct application to time series forecasting has proven ineffective. We address this gap by repositioning the LLM as an intelligent judge that evaluates, explains, and strategically coordinates an ensemble of foundation models. To overcome the LLM's inherent lack of domain-specific knowledge on time series, we introduce an R1-style finetuning process, guided by SHAP-based faithfulness scores, which teaches the model to interpret ensemble weights as meaningful causal statements about temporal dynamics. The trained agent then engages in iterative, multi-turn conversations to perform forward-looking assessments, provide causally-grounded explanations for its weighting decisions, and adaptively refine the optimization strategy. Validated on the GIFT-Eval benchmark on 23 datasets across 97 settings, our approach significantly outperforms leading time series foundation models on both CRPS and MASE metrics, establishing new state-of-the-art results.

</details>


### [16] [Do Large Language Models Know What They Don't Know? Kalshibench: A New Benchmark for Evaluating Epistemic Calibration via Prediction Markets](https://arxiv.org/abs/2512.16030)
*Lukas Nel*

Main category: cs.AI

TL;DR: 介绍KalshiBench基准测试LLMs的认知校准能力，发现所有模型存在系统性过度自信，推断扩展和增强推理不会自动带来校准优势。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的认知校准情况不明，需要评估其对未来事件不确定性的量化能力。

Method: 引入KalshiBench基准测试，包含300个预测市场问题，对五个前沿模型进行评估。

Result: 所有模型存在系统性过度自信，最好校准的模型也有显著误差，推理增强模型校准更差，仅一个模型有正Brier Skill Score。

Conclusion: 扩展和增强推理不能自动提升校准能力，认知校准需要针对性发展。

Abstract: A well-calibrated model should express confidence that matches its actual accuracy -- when it claims 80\% confidence, it should be correct 80\% of the time. While large language models (LLMs) have achieved remarkable performance across diverse tasks, their epistemic calibration remains poorly understood. We introduce \textbf{KalshiBench}, a benchmark of 300 prediction market questions from Kalshi, a CFTC-regulated exchange, with verifiable real-world outcomes occurring after model training cutoffs. Unlike traditional benchmarks measuring accuracy on static knowledge, KalshiBench evaluates whether models can appropriately quantify uncertainty about genuinely unknown future events. We evaluate five frontier models -- Claude Opus 4.5, GPT-5.2, DeepSeek-V3.2, Qwen3-235B, and Kimi-K2 -- and find \textbf{systematic overconfidence across all models}. Even the best-calibrated model (Claude Opus 4.5, ECE=0.120) shows substantial calibration errors, while reasoning-enhanced models like GPT-5.2-XHigh exhibit \emph{worse} calibration (ECE=0.395) despite comparable accuracy. Critically, only one model achieves a positive Brier Skill Score, indicating most models perform worse than simply predicting base rates. Our findings suggest that scaling and enhanced reasoning do not automatically confer calibration benefits, highlighting epistemic calibration as a distinct capability requiring targeted development.

</details>


### [17] [Topic Discovery and Classification for Responsible Generative AI Adaptation in Higher Education](https://arxiv.org/abs/2512.16036)
*Diane Myung-kyung Woodbridge,Allyson Seba,Freddie Seba,Aydin Schwartz*

Main category: cs.AI

TL;DR: 随着生成式AI用于学习，高校制定相关政策但差异大，作者开发自动化系统来发现和分类相关政策，效果良好且能促进AI在教育中合理使用。


<details>
  <summary>Details</summary>
Motivation: 生成式AI应用于学习引发担忧，高校政策差异大使学生迷茫，需解决此挑战。

Method: 结合无监督主题建模技术和大语言模型，开发自动化系统发现和分类AI相关政策。

Result: 主题发现一致性评分为0.73，政策类别分类精准率在0.92 - 0.97，召回率在0.85 - 0.97。

Conclusion: 该工具可促进GenAI技术在教育中安全、公平使用，也能集成到平台助学生遵守指南。

Abstract: As generative artificial intelligence (GenAI) becomes increasingly capable of delivering personalized learning experiences and real-time feedback, a growing number of students are incorporating these tools into their academic workflows. They use GenAI to clarify concepts, solve complex problems, and, in some cases, complete assignments by copying and pasting model-generated contents. While GenAI has the potential to enhance learning experience, it also raises concerns around misinformation, hallucinated outputs, and its potential to undermine critical thinking and problem-solving skills. In response, many universities, colleges, departments, and instructors have begun to develop and adopt policies to guide responsible integration of GenAI into learning environments. However, these policies vary widely across institutions and contexts, and their evolving nature often leaves students uncertain about expectations and best practices. To address this challenge, the authors designed and implemented an automated system for discovering and categorizing AI-related policies found in course syllabi and institutional policy websites. The system combines unsupervised topic modeling techniques to identify key policy themes with large language models (LLMs) to classify the level of GenAI allowance and other requirements in policy texts. The developed application achieved a coherence score of 0.73 for topic discovery. In addition, GPT-4.0-based classification of policy categories achieved precision between 0.92 and 0.97, and recall between 0.85 and 0.97 across eight identified topics. By providing structured and interpretable policy information, this tool promotes the safe, equitable, and pedagogically aligned use of GenAI technologies in education. Furthermore, the system can be integrated into educational technology platforms to help students understand and comply with relevant guidelines.

</details>


### [18] [ParamExplorer: A framework for exploring parameters in generative art](https://arxiv.org/abs/2512.16529)
*Julien Gachadoat,Guillaume Lagarde*

Main category: cs.AI

TL;DR: 提出ParamExplorer框架辅助生成艺术算法参数空间探索并评估多种策略


<details>
  <summary>Details</summary>
Motivation: 生成艺术系统参数空间大且复杂，艺术家手动试错难以发现潜在有趣配置

Method: 引入受强化学习启发、可集成p5.js项目的ParamExplorer框架，在框架内实现并评估多种探索策略（代理）

Result: 未提及具体结果

Conclusion: 未提及具体结论

Abstract: Generative art systems often involve high-dimensional and complex parameter spaces in which aesthetically compelling outputs occupy only small, fragmented regions. Because of this combinatorial explosion, artists typically rely on extensive manual trial-and-error, leaving many potentially interesting configurations undiscovered. In this work we make two contributions. First, we introduce ParamExplorer, an interactive and modular framework inspired by reinforcement learning that helps the exploration of parameter spaces in generative art algorithms, guided by human-in-the-loop or even automated feedback. The framework also integrates seamlessly with existing p5.js projects. Second, within this framework we implement and evaluate several exploration strategies, referred to as agents.

</details>


### [19] [WeMusic-Agent: Efficient Conversational Music Recommendation via Knowledge Internalization and Agentic Boundary Learning](https://arxiv.org/abs/2512.16108)
*Wendong Bi,Yirong Mao,Xianglong Liu,Kai Tian,Jian Zhang,Hanjie Wang,Wenhui Que*

Main category: cs.AI

TL;DR: 提出WeMusic - Agent训练框架用于对话式音乐推荐，构建基准测试，实验显示其优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有对话式音乐推荐方法难以平衡领域知识和工具集成，且缺乏开源基准测试。

Method: 提出WeMusic - Agent训练框架，集成知识内化和代理边界学习；推出WeMusic - Agent - M1模型，在50B音乐语料上预训练并具备调用外部工具能力；构建基于微信听书真实数据的基准测试。

Result: 在真实数据实验中，WeMusic - Agent较现有模型有显著提升。

Conclusion: WeMusic - Agent训练框架能有效用于对话式音乐推荐，构建的基准测试可用于多维度评估。

Abstract: Personalized music recommendation in conversational scenarios usually requires a deep understanding of user preferences and nuanced musical context, yet existing methods often struggle with balancing specialized domain knowledge and flexible tool integration. This paper proposes WeMusic-Agent, a training framework for efficient LLM-based conversational music recommendation. By integrating the knowledge internalization and agentic boundary learning, the framework aims to teach the model to intelligently decide when to leverage internalized knowledge and when to call specialized tools (e.g., music retrieval APIs, music recommendation systems). Under this framework, we present WeMusic-Agent-M1, an agentic model that internalizes extensive musical knowledge via continued pretraining on 50B music-related corpus while acquiring the ability to invoke external tools when necessary. Additionally, considering the lack of open-source benchmarks for conversational music recommendation, we also construct a benchmark for personalized music recommendations derived from real-world data in WeChat Listen. This benchmark enables comprehensive evaluation across multiple dimensions, including relevance, personalization, and diversity of the recommendations. Experiments on real-world data demonstrate that WeMusic-Agent achieves significant improvements over existing models.

</details>


### [20] [ToolForge: A Data Synthesis Pipeline for Multi-Hop Search without Real-World APIs](https://arxiv.org/abs/2512.16149)
*Hao Chen,Zhexin Hu,Jiajun Chai,Haocheng Yang,Hang He,Xiaohan Wang,Wei Lin,Luhang Wang,Guojun Yin,Zhuofeng zhao*

Main category: cs.AI

TL;DR: 提出ToolForge自动化合成框架，无需真实API调用合成工具学习数据，经多层验证，小参数模型训练后效果超GPT - 4o，代码和数据集开源。


<details>
  <summary>Details</summary>
Motivation: 现有合成数据生成管道依赖大量真实API调用，成本高且缺乏多跳推理和自我反思。

Method: 构建少量虚拟工具，利用(问题, 黄金上下文, 答案)三元组合成多跳搜索场景数据，通过多跳推理和自我反思机制丰富数据，采用多层验证框架确保数据保真。

Result: 仅8B参数的模型在合成数据上训练后，在多个基准测试中表现优于GPT - 4o。

Conclusion: ToolForge框架能有效合成高质量工具学习数据，提升模型工具调用性能。

Abstract: Training LLMs to invoke tools and leverage retrieved information necessitates high-quality, diverse data. However, existing pipelines for synthetic data generation often rely on tens of thousands of real API calls to enhance generalization, incurring prohibitive costs while lacking multi-hop reasoning and self-reflection. To address these limitations, we introduce ToolForge, an automated synthesis framework that achieves strong real-world tool-calling performance by constructing only a small number of virtual tools, eliminating the need for real API calls. ToolForge leverages a (question, golden context, answer) triple to synthesize large-scale tool-learning data specifically designed for multi-hop search scenarios, further enriching the generated data through multi-hop reasoning and self-reflection mechanisms. To ensure data fidelity, we employ a Multi-Layer Validation Framework that integrates both rule-based and model-based assessments. Empirical results show that a model with only 8B parameters, when trained on our synthesized data, outperforms GPT-4o on multiple benchmarks. Our code and dataset are publicly available at https://github.com/Buycar-arb/ToolForge .

</details>


### [21] [Weighted K-Harmonic Means Clustering: Convergence Analysis and Applications to Wireless Communications](https://arxiv.org/abs/2512.16185)
*Gourab Ghatak*

Main category: cs.AI

TL;DR: 提出加权K - 调和均值（WKHM）聚类算法，证明其收敛性，仿真表明它在无线组网的信号强度和负载公平性上有出色权衡。


<details>
  <summary>Details</summary>
Motivation: 设计具有数值稳定性且能通过反距离加权进行软分配的聚类算法，并在无线网络中有直接应用解释。

Method: 提出WKHM算法，通过理论证明其在确定性和随机设置下的收敛性，用不同用户分布进行仿真。

Result: 证明在固定初始化下单调下降到局部最小值，在二项点过程初始化下依概率收敛，在温和衰减条件下几乎必然收敛；仿真显示WKHM在信号强度和负载公平性上优于现有算法。

Conclusion: WKHM是无线网络中联合无线电节点放置和用户关联的有效工具。

Abstract: We propose the \emph{weighted K-harmonic means} (WKHM) clustering algorithm, a regularized variant of K-harmonic means designed to ensure numerical stability while enabling soft assignments through inverse-distance weighting. Unlike classical K-means and constrained K-means, WKHM admits a direct interpretation in wireless networks: its weights are exactly equivalent to fractional user association based on received signal strength. We establish rigorous convergence guarantees under both deterministic and stochastic settings, addressing key technical challenges arising from non-convexity and random initialization. Specifically, we prove monotone descent to a local minimum under fixed initialization, convergence in probability under Binomial Point Process (BPP) initialization, and almost sure convergence under mild decay conditions. These results provide the first stochastic convergence guarantees for harmonic-mean-based clustering. Finally, through extensive simulations with diverse user distributions, we show that WKHM achieves a superior tradeoff between minimum signal strength and load fairness compared to classical and modern clustering baselines, making it a principled tool for joint radio node placement and user association in wireless networks.

</details>


### [22] [PDE-Agent: A toolchain-augmented multi-agent framework for PDE solving](https://arxiv.org/abs/2512.16214)
*Jianming Liu,Ren Zhu,Jian Xu,Kun Ding,Xu-Yao Zhang,Gaofeng Meng,Cheng-Lin Liu*

Main category: cs.AI

TL;DR: 本文提出PDE - Agent框架解决偏微分方程，通过创新机制实现自动化求解，经PDE - Bench验证性能优越。


<details>
  <summary>Details</summary>
Motivation: 传统偏微分方程求解方法繁琐，现有基于神经网络的方法依赖专家知识且缺乏自主性。

Method: 提出PDE - Agent框架，采用Prog - Act框架和Resource - Pool机制，开发PDE - Bench并提出评估指标。

Result: PDE - Agent在复杂多步、跨步骤依赖任务中展现出优越的适用性和性能。

Conclusion: 工具链增强的多智能体偏微分方程求解范式将推动自动科学计算的未来发展。

Abstract: Solving Partial Differential Equations (PDEs) is a cornerstone of engineering and scientific research. Traditional methods for PDE solving are cumbersome, relying on manual setup and domain expertise. While Physics-Informed Neural Network (PINNs) introduced end-to-end neural network-based solutions, and frameworks like DeepXDE further enhanced automation, these approaches still depend on expert knowledge and lack full autonomy. In this work, we frame PDE solving as tool invocation via LLM-driven agents and introduce PDE-Agent, the first toolchain-augmented multi-agent collaboration framework, inheriting the reasoning capacity of LLMs and the controllability of external tools and enabling automated PDE solving from natural language descriptions. PDE-Agent leverages the strengths of multi-agent and multi-tool collaboration through two key innovations: (1) A Prog-Act framework with graph memory for multi-agent collaboration, which enables effective dynamic planning and error correction via dual-loop mechanisms (localized fixes and global revisions). (2) A Resource-Pool integrated with a tool-parameter separation mechanism for multi-tool collaboration. This centralizes the management of runtime artifacts and resolves inter-tool dependency gaps in existing frameworks. To validate and evaluate this new paradigm for PDE solving , we develop PDE-Bench, a multi-type PDE Benchmark for agent-based tool collaborative solving, and propose multi-level metrics for assessing tool coordination. Evaluations verify that PDE-Agent exhibits superior applicability and performance in complex multi-step, cross-step dependent tasks. This new paradigm of toolchain-augmented multi-agent PDE solving will further advance future developments in automated scientific computing. Our source code and dataset will be made publicly available.

</details>


### [23] [Scaling Spatial Reasoning in MLLMs through Programmatic Data Synthesis](https://arxiv.org/abs/2512.16237)
*Zhi Helu,Huang Jingjing,Xu Wang,Xu Yangbin,Zhang Wanyue,Jiang Baoyang,Deng Shirui,Zhu Liang,Li Fangfang,Zhao Tiejun,Lin Yankai,Yao Yuan*

Main category: cs.AI

TL;DR: 文章指出当前具身智能的空间理解和推理能力有限，介绍了SPRITE框架，通过合成数据克服现有困境，验证了其有效性并将公开代码和数据集。


<details>
  <summary>Details</summary>
Motivation: 解决当前具身智能中空间理解和推理能力受限问题，突破增强视觉语言模型（VLMs）时模板数据集和手动标注的困境。

Method: 引入SPRITE框架，将真值生成重构为代码生成任务，利用大语言模型编译问题为可执行程序，并结合模拟器信息验证。

Result: 构建了含3个模拟器、11k+场景和300k+图像/视频指令调整对的数据集，基于此训练的VLM在多个空间基准测试中取得显著性能提升，优于同等规模数据集。

Conclusion: 克服传统模板方法的低多样性对构建强大、可泛化的空间智能至关重要，将公开SPRITE框架代码和完整数据集以推动研究。

Abstract: Embodied intelligence, a grand challenge in artificial intelligence, is fundamentally constrained by the limited spatial understanding and reasoning capabilities of current models. Prevailing efforts to address this through enhancing Vision-Language Models (VLMs) are trapped in a dilemma: template-based datasets are scalable but structurally rigid, while manual annotation is linguistically diverse but unscalable and, critically, computationally imprecise. We introduce SPRITE, a novel framework that overcomes this dilemma by leveraging simulators and large models to programmatically synthesize scalable, diverse, and high-quality spatial reasoning data. The core innovation of SPRITE is to reframe ground-truth generation as a code-generation task. We utilize LLMs to compile complex spatial questions into executable programs, which are then verified against high-precision scene meta-information extracted from simulators. This ensures our ground truth is both computationally precise and verifiable, while the generative power of LLMs provides vast linguistic diversity. Leveraging this pipeline, we have curated a dataset encompassing 3 simulators, 11k+ scenes, and 300k+ image/video instruction-tuning pairs. We demonstrate that a VLM trained on our data achieves significant performance gains on multiple spatial benchmarks and outperforms other open-source datasets of equivalent size. Furthermore, a scalability analysis confirms our hypothesis that overcoming the low-diversity nature of traditional template methods is essential for building robust, generalizable spatial intelligence. We will make the SPRITE framework code and the full 300k+ dataset publicly available to facilitate future research in spatial intelligence.

</details>


### [24] [AlignMerge - Alignment-Preserving Large Language Model Merging via Fisher-Guided Geometric Constraints](https://arxiv.org/abs/2512.16245)
*Aniruddha Roy,Jyoti Patel,Aman Chadha,Vinija Jain,Amitava Das*

Main category: cs.AI

TL;DR: 介绍AlignMerge几何感知合并框架，在多模型系列中合并安全锚点和任务专家时，能提升对齐指标并在多个能力上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型合并方案存在保存损失但暗中破坏对齐的问题，需新的合并方法确保尊重安全几何。

Method: 引入AlignMerge框架，在局部Fisher图中估计对齐子空间并用特定公式优化，以对齐质量指数作为对齐函数。

Result: 在五个模型系列中，AlignMerge提升对齐指标，在指令遵循、推理和帮助性上匹配或超越最佳专家，比其他方法有更小的对齐子空间漂移和更少的预算违规。

Conclusion: 让保留对齐的合并成为一流设计目标，为未来基础模型的几何感知组合指明道路。

Abstract: Merging large language models (LLMs) is a practical way to compose capabilities from multiple fine-tuned checkpoints without retraining. Yet standard schemes (linear weight soups, task vectors, and Fisher-weighted averaging) can preserve loss while quietly destroying alignment. We argue that merging is not a numerical trick but a geometry-constrained operation around an already-aligned anchor: fusion must be steered to respect safety geometry, not validated post hoc.
  We introduce AlignMerge, a geometry-aware merging framework that makes alignment an explicit invariant. In a local Fisher chart around an instruction-tuned base, we estimate an alignment subspace with projector P_A and optimize:
  L_AlignMerge = L_geo + lambda_align * L_align + lambda_bud * L_bud,
  where L_geo keeps the merge close to its experts in Fisher-Rao geometry, L_align penalizes motion along alignment-sensitive directions, and L_bud enforces a soft alignment budget. As the alignment functional we use the decoding-invariant Alignment Quality Index (AQI), a latent-space criterion that captures how cleanly aligned and misaligned behaviors separate in representation space.
  Across five model families (LLaMA-3 8B, Mistral 7B, Qwen 2, Phi-3.5, Gemma 2), merging safety anchors with task experts, AlignMerge improves alignment metrics (AQI, toxicity, LLM-judge alignment) while matching or exceeding the best expert on instruction-following, reasoning, and helpfulness. It also exhibits smaller alignment-subspace drift and fewer budget violations than Fisher soups, TIES, SafeMerge, and MergeAlign. These results make alignment-preserving merging a first-class design goal and suggest a path to geometry-aware composition of future foundation models.

</details>


### [25] [AMUSE: Audio-Visual Benchmark and Alignment Framework for Agentic Multi-Speaker Understanding](https://arxiv.org/abs/2512.16250)
*Sanjoy Chowdhury,Karren D. Yang,Xudong Liu,Fartash Faghri,Pavan Kumar Anasosalu Vasu,Oncel Tuzel,Dinesh Manocha,Chun-Liang Li,Raviteja Vemulapalli*

Main category: cs.AI

TL;DR: 本文引入AMUSE基准评估多模态大语言模型，发现模型多说话人推理能力弱，提出RAFT框架，实现基准测试精度提升，二者为评估和改进多模态模型提供平台。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在多说话人、以对话为中心的场景中存在推理困难，而此类场景是多模态音视频理解的核心，因此需要更好的评估和改进方法。

Method: 引入AMUSE基准评估模型，涵盖三种模式和六个任务家族；提出数据高效的RAFT框架，将奖励优化与内在多模态自我评估结合，进行选择性参数适配。

Result: 当前模型在各模式下均表现出弱多说话人推理和不一致行为；使用RAFT在基准测试中准确率相对提升达39.52%。

Conclusion: AMUSE和RAFT为检查多模态模型的代理推理能力和提升其性能提供了实用平台。

Abstract: Recent multimodal large language models (MLLMs) such as GPT-4o and Qwen3-Omni show strong perception but struggle in multi-speaker, dialogue-centric settings that demand agentic reasoning tracking who speaks, maintaining roles, and grounding events across time. These scenarios are central to multimodal audio-video understanding, where models must jointly reason over audio and visual streams in applications such as conversational video assistants and meeting analytics. We introduce AMUSE, a benchmark designed around tasks that are inherently agentic, requiring models to decompose complex audio-visual interactions into planning, grounding, and reflection steps. It evaluates MLLMs across three modes zero-shot, guided, and agentic and six task families, including spatio-temporal speaker grounding and multimodal dialogue summarization. Across all modes, current models exhibit weak multi-speaker reasoning and inconsistent behavior under both non-agentic and agentic evaluation. Motivated by the inherently agentic nature of these tasks and recent advances in LLM agents, we propose RAFT, a data-efficient agentic alignment framework that integrates reward optimization with intrinsic multimodal self-evaluation as reward and selective parameter adaptation for data and parameter efficient updates. Using RAFT, we achieve up to 39.52\% relative improvement in accuracy on our benchmark. Together, AMUSE and RAFT provide a practical platform for examining agentic reasoning in multimodal models and improving their capabilities.

</details>


### [26] [Learning to Wait: Synchronizing Agents with the Physical World](https://arxiv.org/abs/2512.16262)
*Yifei She,Ping Zhang,He Liu,Yanmin Jia,Yang Jing,Zijun Liu,Peng Sun,Xiangbin Li,Xiaohe Hu*

Main category: cs.AI

TL;DR: 提出代理端方法使大语言模型将认知时间线与物理世界对齐，在模拟实验中验证了时间感知能力可学习。


<details>
  <summary>Details</summary>
Motivation: 现实代理任务存在时间差距，现有环境端解决方案有局限性。

Method: 将代码即动作范式扩展到时间领域，利用语义先验和上下文学习预测等待时间。

Result: 模拟Kubernetes集群实验显示，代理能校准内部时钟，减少查询开销和执行延迟。

Conclusion: 时间感知是开放式环境中自主进化的可学习关键能力。

Abstract: Real-world agentic tasks, unlike synchronous Markov Decision Processes (MDPs), often involve non-blocking actions with variable latencies, creating a fundamental \textit{Temporal Gap} between action initiation and completion. Existing environment-side solutions, such as blocking wrappers or frequent polling, either limit scalability or dilute the agent's context window with redundant observations. In this work, we propose an \textbf{Agent-side Approach} that empowers Large Language Models (LLMs) to actively align their \textit{Cognitive Timeline} with the physical world. By extending the Code-as-Action paradigm to the temporal domain, agents utilize semantic priors and In-Context Learning (ICL) to predict precise waiting durations (\texttt{time.sleep(t)}), effectively synchronizing with asynchronous environment without exhaustive checking. Experiments in a simulated Kubernetes cluster demonstrate that agents can precisely calibrate their internal clocks to minimize both query overhead and execution latency, validating that temporal awareness is a learnable capability essential for autonomous evolution in open-ended environments.

</details>


### [27] [QuadSentinel: Sequent Safety for Machine-Checkable Control in Multi-agent Systems](https://arxiv.org/abs/2512.16279)
*Yiliu Yang,Yilei Jiang,Qunzhong Wang,Yingshui Tan,Xiaoyong Zhu,Sherman S. M. Chow,Bo Zheng,Xiangyu Yue*

Main category: cs.AI

TL;DR: 提出QuadSentinel解决大语言模型代理安全策略执行问题，经测试效果好且代码将开源。


<details>
  <summary>Details</summary>
Motivation: 大语言模型代理执行复杂任务有安全风险，部署者用自然语言写的策略模糊且依赖上下文，难以映射到机器可检查规则，运行时执行不可靠。

Method: 将安全策略表示为相继式，提出由四个代理组成的QuadSentinel，将策略编译为基于可观察状态谓词的机器可检查规则并在线执行，通过裁判逻辑和高效的top - k谓词更新器降低成本。

Result: 在ST - WebAgentBench和AgentHarm上测试，QuadSentinel提高了护栏准确性和规则召回率，减少误报，相比ShieldAgent等单代理基线有更好的整体安全控制。

Conclusion: 近期部署可采用该模式，将策略分离且机器可检查，无需修改核心代理。

Abstract: Safety risks arise as large language model-based agents solve complex tasks with tools, multi-step plans, and inter-agent messages. However, deployer-written policies in natural language are ambiguous and context dependent, so they map poorly to machine-checkable rules, and runtime enforcement is unreliable. Expressing safety policies as sequents, we propose \textsc{QuadSentinel}, a four-agent guard (state tracker, policy verifier, threat watcher, and referee) that compiles these policies into machine-checkable rules built from predicates over observable state and enforces them online. Referee logic plus an efficient top-$k$ predicate updater keeps costs low by prioritizing checks and resolving conflicts hierarchically. Measured on ST-WebAgentBench (ICML CUA~'25) and AgentHarm (ICLR~'25), \textsc{QuadSentinel} improves guardrail accuracy and rule recall while reducing false positives. Against single-agent baselines such as ShieldAgent (ICML~'25), it yields better overall safety control. Near-term deployments can adopt this pattern without modifying core agents by keeping policies separate and machine-checkable. Our code will be made publicly available at https://github.com/yyiliu/QuadSentinel.

</details>


### [28] [OS-Oracle: A Comprehensive Framework for Cross-Platform GUI Critic Models](https://arxiv.org/abs/2512.16295)
*Zhenyu Wu,Jingjing Xie,Zehao Li,Bowen Yang,Qiushi Sun,Zhaoyang Liu,Zhoumianze Liu,Yu Qiao,Xiangyu Yue,Zun Wang,Zichen Ding*

Main category: cs.AI

TL;DR: 本文针对VLM驱动的计算机使用代理在GUI导航和操作中可靠的步骤级决策瓶颈，提出OS - Oracle框架，构建数据集，模型在基准测试表现出色并能提升原生GUI代理性能，代码开源。


<details>
  <summary>Details</summary>
Motivation: VLM驱动的计算机使用代理在实际部署中，可靠的步骤级决策是关键瓶颈，长流程中错误易积累，不可逆操作有不良后果，且缺乏多样化高质量GUI反馈数据和公开的步骤级评估基准。

Method: 引入OS - Oracle，包括可扩展的跨平台GUI批评数据合成数据管道、结合监督微调与一致性保留组相对策略优化的两阶段训练范式、跨移动、网络和桌面平台评估批评模型性能的OS - Critic Bench基准。

Result: 整理出含310k批评样本的高质量数据集，OS - Oracle - 7B模型在OS - Critic Bench上达开源VLM的最优性能，在移动领域超越专有模型，还能提升原生GUI代理性能。

Conclusion: OS - Oracle框架有效解决了VLM驱动的计算机使用代理在步骤级决策方面的问题，为相关研究和应用提供了有价值的工具和方法。

Abstract: With VLM-powered computer-using agents (CUAs) becoming increasingly capable at graphical user interface (GUI) navigation and manipulation, reliable step-level decision-making has emerged as a key bottleneck for real-world deployment. In long-horizon workflows, errors accumulate quickly and irreversible actions can cause unintended consequences, motivating critic models that assess each action before execution. While critic models offer a promising solution, their effectiveness is hindered by the lack of diverse, high-quality GUI feedback data and public critic benchmarks for step-level evaluation in computer use. To bridge these gaps, we introduce OS-Oracle that makes three core contributions: (1) a scalable data pipeline for synthesizing cross-platform GUI critic data; (2) a two-stage training paradigm combining supervised fine-tuning (SFT) and consistency-preserving group relative policy optimization (CP-GRPO); (3) OS-Critic Bench, a holistic benchmark for evaluating critic model performance across Mobile, Web, and Desktop platforms. Leveraging this framework, we curate a high-quality dataset containing 310k critic samples. The resulting critic model, OS-Oracle-7B, achieves state-of-the-art performance among open-source VLMs on OS-Critic Bench, and surpasses proprietary models on the mobile domain. Furthermore, when serving as a pre-critic, OS-Oracle-7B improves the performance of native GUI agents such as UI-TARS-1.5-7B in OSWorld and AndroidWorld environments. The code is open-sourced at https://github.com/numbmelon/OS-Oracle.

</details>


### [29] [Code-in-the-Loop Forensics: Agentic Tool Use for Image Forgery Detection](https://arxiv.org/abs/2512.16300)
*Fanrui Zhang,Qiang Zhang,Sizhuo Zhou,Jianwen Sun,Chuanhao Li,Jiaxin Ai,Yukang Feng,Yujie Zhang,Wenjie Li,Zizhen Li,Yifan Chang,Jiawei Liu,Kaipeng Zhang*

Main category: cs.AI

TL;DR: 提出ForenAgent框架解决现有图像伪造检测方法难以融合高低层信息问题，经实验展现出良好能力。


<details>
  <summary>Details</summary>
Motivation: 现有图像伪造检测方法难以统一低层次语义无关伪像和高层次语义知识这两种异质信息流，无法有效建模跨层交互。

Method: 提出ForenAgent多轮交互IFD框架，采用两阶段训练管道，设计动态推理循环，构建FABench数据集。

Result: ForenAgent在低层次工具辅助下，在具有挑战性的IFD任务中展现出新兴工具使用能力和反思推理能力。

Conclusion: 为通用图像伪造检测开辟了有前景的道路。

Abstract: Existing image forgery detection (IFD) methods either exploit low-level, semantics-agnostic artifacts or rely on multimodal large language models (MLLMs) with high-level semantic knowledge. Although naturally complementary, these two information streams are highly heterogeneous in both paradigm and reasoning, making it difficult for existing methods to unify them or effectively model their cross-level interactions. To address this gap, we propose ForenAgent, a multi-round interactive IFD framework that enables MLLMs to autonomously generate, execute, and iteratively refine Python-based low-level tools around the detection objective, thereby achieving more flexible and interpretable forgery analysis. ForenAgent follows a two-stage training pipeline combining Cold Start and Reinforcement Fine-Tuning to enhance its tool interaction capability and reasoning adaptability progressively. Inspired by human reasoning, we design a dynamic reasoning loop comprising global perception, local focusing, iterative probing, and holistic adjudication, and instantiate it as both a data-sampling strategy and a task-aligned process reward. For systematic training and evaluation, we construct FABench, a heterogeneous, high-quality agent-forensics dataset comprising 100k images and approximately 200k agent-interaction question-answer pairs. Experiments show that ForenAgent exhibits emergent tool-use competence and reflective reasoning on challenging IFD tasks when assisted by low-level tools, charting a promising route toward general-purpose IFD. The code will be released after the review process is completed.

</details>


### [30] [Adaptation of Agentic AI](https://arxiv.org/abs/2512.16301)
*Pengcheng Jiang,Jiacheng Lin,Zhiyi Shi,Zifeng Wang,Luxi He,Yichen Wu,Ming Zhong,Peiyang Song,Qizheng Zhang,Heng Wang,Xueqiang Xu,Hanwen Xu,Pengrui Han,Dylan Zhang,Jiashuo Sun,Chaoqi Yang,Kun Qian,Tian Wang,Changran Hu,Manling Li,Quanzheng Li,Hao Peng,Sheng Wang,Jingbo Shang,Chao Zhang,Jiaxuan You,Liyuan Liu,Pan Lu,Yu Zhang,Heng Ji,Yejin Choi,Dawn Song,Jimeng Sun,Jiawei Han*

Main category: cs.AI

TL;DR: 文章统一了智能体AI适应研究格局成系统框架，分解适应形式，展示其作用并回顾相关方法，分析优劣势，指出挑战与机会，为构建智能体AI系统提供思路。


<details>
  <summary>Details</summary>
Motivation: 随着智能体AI系统能力和范围增长，适应成为提升性能、可靠性和泛化性的关键机制，需对相关研究进行系统整理。

Method: 将研究分为智能体适应和工具适应构建系统框架，进一步细分适应形式，回顾各类型代表性方法。

Result: 该框架有助于明确智能体AI适应策略的设计空间，明晰权衡，为系统设计提供选择或切换策略的指导。

Conclusion: 为致力于构建更强大、高效和可靠的智能体AI系统的研究者和从业者提供了概念基础和实践路线图。

Abstract: Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.

</details>


### [31] [Design and Evaluation of Cost-Aware PoQ for Decentralized LLM Inference](https://arxiv.org/abs/2512.16317)
*Arther Tian,Alex Ding,Frank Chen,Alan Wu,Aaron Chan,Bruce Zhang*

Main category: cs.AI

TL;DR: 本文提出成本感知的PoQ框架用于去中心化大语言模型推理，实验表明该框架为经济可持续的推理提供基础。


<details>
  <summary>Details</summary>
Motivation: 现有验证方法难以扩展到现代模型，原PoQ公式忽略节点计算成本差异。

Method: 引入成本感知的PoQ框架，将效率测量纳入奖励机制，结合多种评估方式，采用线性奖励函数。

Result: 语义文本相似性双编码器与真实和GPT分数相关性更高；大模型质量-成本效率高；成本感知奖励方案能奖励优质低成本节点，惩罚低效低质节点。

Conclusion: 成本感知的PoQ为经济可持续的去中心化大语言模型推理提供了实用基础。

Abstract: Decentralized large language model (LLM) inference promises transparent and censorship resistant access to advanced AI, yet existing verification approaches struggle to scale to modern models. Proof of Quality (PoQ) replaces cryptographic verification of computation with consensus over output quality, but the original formulation ignores heterogeneous computational costs across inference and evaluator nodes. This paper introduces a cost-aware PoQ framework that integrates explicit efficiency measurements into the reward mechanism for both types of nodes. The design combines ground truth token level F1, lightweight learned evaluators, and GPT based judgments within a unified evaluation pipeline, and adopts a linear reward function that balances normalized quality and cost.
  Experiments on extractive question answering and abstractive summarization use five instruction tuned LLMs ranging from TinyLlama-1.1B to Llama-3.2-3B and three evaluation models spanning cross encoder and bi encoder architectures. Results show that a semantic textual similarity bi encoder achieves much higher correlation with both ground truth and GPT scores than cross encoders, indicating that evaluator architecture is a critical design choice for PoQ. Quality-cost analysis further reveals that the largest models in the pool are also the most efficient in terms of quality per unit latency. Monte Carlo simulations over 5\,000 PoQ rounds demonstrate that the cost-aware reward scheme consistently assigns higher average rewards to high quality low cost inference models and to efficient evaluators, while penalizing slow low quality nodes. These findings suggest that cost-aware PoQ provides a practical foundation for economically sustainable decentralized LLM inference.

</details>


### [32] [AI Needs Physics More Than Physics Needs AI](https://arxiv.org/abs/2512.16344)
*Peter Coveney,Roger Highfield*

Main category: cs.AI

TL;DR: AI虽常被视为变革性技术，但多年炒作后实际影响有限，物理对当前AI有更大贡献，还提出‘Big AI’路线图。


<details>
  <summary>Details</summary>
Motivation: 探讨AI实际影响力与预期不符的情况，分析当前AI存在的问题，寻找AI发展新方向。

Method: 回顾当前AI架构存在问题的批判，强调量子AI和模拟计算的机会。

Result: 指出当前AI架构存在依赖大量无意义参数、分布偏差等诸多问题。

Conclusion: 应采用‘Big AI’，将基于理论的严谨性与机器学习的灵活性相结合。

Abstract: Artificial intelligence (AI) is commonly depicted as transformative. Yet, after more than a decade of hype, its measurable impact remains modest outside a few high-profile scientific and commercial successes. The 2024 Nobel Prizes in Chemistry and Physics recognized AI's potential, but broader assessments indicate the impact to date is often more promotional than technical. We argue that while current AI may influence physics, physics has significantly more to offer this generation of AI. Current architectures - large language models, reasoning models, and agentic AI - can depend on trillions of meaningless parameters, suffer from distributional bias, lack uncertainty quantification, provide no mechanistic insights, and fail to capture even elementary scientific laws. We review critiques of these limits, highlight opportunities in quantum AI and analogue computing, and lay down a roadmap for the adoption of 'Big AI': a synthesis of theory-based rigour with the flexibility of machine learning.

</details>


### [33] [PCIA: A Path Construction Imitation Algorithm for Global Optimization](https://arxiv.org/abs/2512.16392)
*Mohammad-Javad Rezaei,Mozafar Bag-Mohammadi*

Main category: cs.AI

TL;DR: 提出路径构建模仿算法（PCIA），受人类构建路径启发，经多问题测试，比其他元启发式算法有竞争力。


<details>
  <summary>Details</summary>
Motivation: 提出一种新的元启发式优化算法。

Method: 受人类构建路径和使用路径的方式启发，生成随机种群寻找最优路径，每个粒子代表一条路径。

Result: PCIA在53个数学优化问题和13个约束优化问题测试中，比流行和最新的元启发式算法更具竞争力。

Conclusion: PCIA是一种有竞争力的新的元启发式优化算法。

Abstract: In this paper, a new metaheuristic optimization algorithm, called Path Construction Imitation Algorithm (PCIA), is proposed. PCIA is inspired by how humans construct new paths and use them. Typically, humans prefer popular transportation routes. In the event of a path closure, a new route is built by mixing the existing paths intelligently. Also, humans select different pathways on a random basis to reach unknown destinations. PCIA generates a random population to find the best route toward the destination, similar to swarm-based algorithms. Each particle represents a path toward the destination. PCIA has been tested with 53 mathematical optimization problems and 13 constrained optimization problems. The results showed that the PCIA is highly competitive compared to both popular and the latest metaheuristic algorithms.

</details>


### [34] [Synthelite: Chemist-aligned and feasibility-aware synthesis planning with LLMs](https://arxiv.org/abs/2512.16424)
*Nguyen Xuan-Vu,Daniel Armstrong,Milena Wehrbach,Andres M Bran,Zlatko Jončev,Philippe Schwaller*

Main category: cs.AI

TL;DR: 介绍Synthelite合成规划框架，利用大语言模型直接提出逆合成转换，能适应不同约束，成功率高，有望成为合成规划的重要工具。


<details>
  <summary>Details</summary>
Motivation: 现有计算机辅助合成规划框架缺乏与人类专家交互机制，难以整合化学家见解。

Method: 引入使用大语言模型直接提出逆合成转换的Synthelite框架，通过自然语言提示允许专家干预。

Result: Synthelite能灵活适应不同用户指定约束，在策略约束和起始材料约束的合成任务中成功率达95%，设计路线时能考虑化学可行性。

Conclusion: Synthelite是有用工具，是迈向大语言模型主导合成规划范式的一步。

Abstract: Computer-aided synthesis planning (CASP) has long been envisioned as a complementary tool for synthetic chemists. However, existing frameworks often lack mechanisms to allow interaction with human experts, limiting their ability to integrate chemists' insights. In this work, we introduce Synthelite, a synthesis planning framework that uses large language models (LLMs) to directly propose retrosynthetic transformations. Synthelite can generate end-to-end synthesis routes by harnessing the intrinsic chemical knowledge and reasoning capabilities of LLMs, while allowing expert intervention through natural language prompts. Our experiments demonstrate that Synthelite can flexibly adapt its planning trajectory to diverse user-specified constraints, achieving up to 95\% success rates in both strategy-constrained and starting-material-constrained synthesis tasks. Additionally, Synthelite exhibits the ability to account for chemical feasibility during route design. We envision Synthelite to be both a useful tool and a step toward a paradigm where LLMs are the central orchestrators of synthesis planning.

</details>


### [35] [TIB AIssistant: a Platform for AI-Supported Research Across Research Life Cycles](https://arxiv.org/abs/2512.16442)
*Allard Oelen,Sören Auer*

Main category: cs.AI

TL;DR: 介绍TIB AIssistant这一支持研究全生命周期的AI研究平台，展示其功能并为社区维护的AI研究平台奠定基础。


<details>
  <summary>Details</summary>
Motivation: 随着AI和大语言模型普及，要为研究全生命周期提供支持。

Method: 构建由多个负责特定研究任务的助手组成的AIssistant平台，提供访问外部学术服务的工具，将生成数据存储并可导出为RO - Crate包，通过助手顺序演示展示功能。

Result: 展示了AIssistant生成研究论文草案章节的主要功能。

Conclusion: 为构建社区维护的AI支持研究平台奠定基础。

Abstract: The rapidly growing popularity of adopting Artificial Intelligence (AI), and specifically Large Language Models (LLMs), is having a widespread impact throughout society, including the academic domain. AI-supported research has the potential to support researchers with tasks across the entire research life cycle. In this work, we demonstrate the TIB AIssistant, an AI-supported research platform providing support throughout the research life cycle. The AIssistant consists of a collection of assistants, each responsible for a specific research task. In addition, tools are provided to give access to external scholarly services. Generated data is stored in the assets and can be exported as an RO-Crate bundle to provide transparency and enhance reproducibility of the research project. We demonstrate the AIssistant's main functionalities by means of a sequential walk-through of assistants, interacting with each other to generate sections for a draft research paper. In the end, with the AIssistant, we lay the foundation for a larger agenda of providing a community-maintained platform for AI-supported research.

</details>


### [36] [StarCraft+: Benchmarking Multi-agent Algorithms in Adversary Paradigm](https://arxiv.org/abs/2512.16444)
*Yadong Li,Tong Zhang,Bo Huang,Zhen Cui*

Main category: cs.AI

TL;DR: 论文建立多智能体算法对抗环境SC2BA及APyMARL库，对经典MARL算法进行两种模式的基准测试，揭示算法问题并开源。


<details>
  <summary>Details</summary>
Motivation: 现有MARL算法假想对手配置固定，算法评估缺乏多样性和通用性，需新基准环境。

Method: 建立SC2BA环境，开发APyMARL库，在两种对抗模式下对经典MARL算法进行基准测试。

Result: 广泛的基准实验揭示了已完成算法在有效性、敏感性和可扩展性方面的问题。

Conclusion: 该工作将为未来几年的MARL领域迈出新的一步。

Abstract: Deep multi-agent reinforcement learning (MARL) algorithms are booming in the field of collaborative intelligence, and StarCraft multi-agent challenge (SMAC) is widely-used as the benchmark therein. However, imaginary opponents of MARL algorithms are practically configured and controlled in a fixed built-in AI mode, which causes less diversity and versatility in algorithm evaluation. To address this issue, in this work, we establish a multi-agent algorithm-vs-algorithm environment, named StarCraft II battle arena (SC2BA), to refresh the benchmarking of MARL algorithms in an adversary paradigm. Taking StarCraft as infrastructure, the SC2BA environment is specifically created for inter-algorithm adversary with the consideration of fairness, usability and customizability, and meantime an adversarial PyMARL (APyMARL) library is developed with easy-to-use interfaces/modules. Grounding in SC2BA, we benchmark those classic MARL algorithms in two types of adversarial modes: dual-algorithm paired adversary and multi-algorithm mixed adversary, where the former conducts the adversary of pairwise algorithms while the latter focuses on the adversary to multiple behaviors from a group of algorithms. The extensive benchmark experiments exhibit some thought-provoking observations/problems in the effectivity, sensibility and scalability of these completed algorithms. The SC2BA environment as well as reproduced experiments are released in \href{https://github.com/dooliu/SC2BA}{Github}, and we believe that this work could mark a new step for the MARL field in the coming years.

</details>


### [37] [Towards AI-Supported Research: a Vision of the TIB AIssistant](https://arxiv.org/abs/2512.16447)
*Sören Auer,Allard Oelen,Mohamad Yaser Jaradeh,Mutahira Khalid,Farhana Keya,Sasi Kiran Gaddipati,Jennifer D'Souza,Lorenz Schlüter,Amirreza Alasti,Gollam Rabby,Azanzi Jiomekong,Oliver Karras*

Main category: cs.AI

TL;DR: 文章提出TIB AIssistant平台以解决AI融入研究的挑战，介绍其组件、框架及早期原型的情况。


<details>
  <summary>Details</summary>
Motivation: 生成式AI和大语言模型虽有潜力，但因领域需求不同、AI素养有限等问题，有效集成AI到研究存在挑战。

Method: 提出TIB AIssistant这一跨领域人机协作平台，借助模块化组件推动研究各阶段任务。

Result: 实现了早期原型，证明了方法的可行性和潜在影响。

Conclusion: TIB AIssistant平台有潜力支持各学科研究者进行科学发现。

Abstract: The rapid advancements in Generative AI and Large Language Models promise to transform the way research is conducted, potentially offering unprecedented opportunities to augment scholarly workflows. However, effectively integrating AI into research remains a challenge due to varying domain requirements, limited AI literacy, the complexity of coordinating tools and agents, and the unclear accuracy of Generative AI in research. We present the vision of the TIB AIssistant, a domain-agnostic human-machine collaborative platform designed to support researchers across disciplines in scientific discovery, with AI assistants supporting tasks across the research life cycle. The platform offers modular components - including prompt and tool libraries, a shared data store, and a flexible orchestration framework - that collectively facilitate ideation, literature analysis, methodology development, data analysis, and scholarly writing. We describe the conceptual framework, system architecture, and implementation of an early prototype that demonstrates the feasibility and potential impact of our approach.

</details>


### [38] [TimeSeries2Report prompting enables adaptive large language model management of lithium-ion batteries](https://arxiv.org/abs/2512.16453)
*Jiayang Yang,Chunhui Zhao,Martin Guay,Zhixing Cao*

Main category: cs.AI

TL;DR: 提出TimeSeries2Report (TS2R) 框架将电池运行时间序列转换为报告，提升大语言模型在电池管理中的性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在电池储能系统运维中的应用未充分探索，需有效方法让其处理电池时间序列数据。

Method: 提出TS2R框架，通过分段、语义抽象和基于规则的解释将短期时间动态编码为自然语言。

Result: 在实验室和真实数据集上进行基准测试，相比其他基线，TS2R提升了大语言模型在准确性、鲁棒性和可解释性方面的性能。

Conclusion: 集成TS2R的大语言模型无需重新训练或修改架构，达到专家级决策质量和预测一致性，为电池智能提供实用途径。

Abstract: Large language models (LLMs) offer promising capabilities for interpreting multivariate time-series data, yet their application to real-world battery energy storage system (BESS) operation and maintenance remains largely unexplored. Here, we present TimeSeries2Report (TS2R), a prompting framework that converts raw lithium-ion battery operational time-series into structured, semantically enriched reports, enabling LLMs to reason, predict, and make decisions in BESS management scenarios. TS2R encodes short-term temporal dynamics into natural language through a combination of segmentation, semantic abstraction, and rule-based interpretation, effectively bridging low-level sensor signals with high-level contextual insights. We benchmark TS2R across both lab-scale and real-world datasets, evaluating report quality and downstream task performance in anomaly detection, state-of-charge prediction, and charging/discharging management. Compared with vision-, embedding-, and text-based prompting baselines, report-based prompting via TS2R consistently improves LLM performance in terms of across accuracy, robustness, and explainability metrics. Notably, TS2R-integrated LLMs achieve expert-level decision quality and predictive consistency without retraining or architecture modification, establishing a practical path for adaptive, LLM-driven battery intelligence.

</details>


### [39] [cuPilot: A Strategy-Coordinated Multi-agent Framework for CUDA Kernel Evolution](https://arxiv.org/abs/2512.16465)
*Jinwu Chen,Qidie Wu,Bin Li,Lin Ma,Xin Si,Yang Hu,Shouyi Yin,Jun Yang*

Main category: cs.AI

TL;DR: 本文提出cuPilot框架优化CUDA内核，实验显示生成内核有显著加速效果并开源。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型和进化算法的CUDA内核优化方法因代理设计不佳和进化表示不匹配，性能不足。

Method: 提出策略协调的多智能体框架cuPilot，包括策略协调进化算法、屋顶线引导提示和策略级种群初始化。

Result: 在100个内核基准测试中，cuPilot生成的内核相比PyTorch平均加速3.09倍，在GEMM任务中实现复杂优化和关键硬件单元高利用率。

Conclusion: cuPilot框架能有效优化CUDA内核，提升性能。

Abstract: Optimizing CUDA kernels is a challenging and labor-intensive task, given the need for hardware-software co-design expertise and the proprietary nature of high-performance kernel libraries. While recent large language models (LLMs) combined with evolutionary algorithms show promise in automatic kernel optimization, existing approaches often fall short in performance due to their suboptimal agent designs and mismatched evolution representations. This work identifies these mismatches and proposes cuPilot, a strategy-coordinated multi-agent framework that introduces strategy as an intermediate semantic representation for kernel evolution. Key contributions include a strategy-coordinated evolution algorithm, roofline-guided prompting, and strategy-level population initialization. Experimental results show that the generated kernels by cuPilot achieve an average speed up of 3.09$\times$ over PyTorch on a benchmark of 100 kernels. On the GEMM tasks, cuPilot showcases sophisticated optimizations and achieves high utilization of critical hardware units. The generated kernels are open-sourced at https://github.com/champloo2878/cuPilot-Kernels.git.

</details>


### [40] [Quantifying and Bridging the Fidelity Gap: A Decisive-Feature Approach to Comparing Synthetic and Real Imagery](https://arxiv.org/abs/2512.16468)
*Danial Safaei,Siddartha Khastgir,Mohsen Alirezaei,Jeroen Ploeg,Son Tong,Xingyu Zhao*

Main category: cs.AI

TL;DR: 本文引入新指标DFF解决自动驾驶虚拟测试中缺乏基于行为的保真度度量问题，通过实验证明其能发现传统方法忽略的差异并提升保真度。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅关注像素级保真度，无法确保仿真与现实可靠转移，缺乏基于行为的保真度度量。

Method: 引入Decisive Feature Fidelity (DFF)，利用可解释AI方法识别比较关键特征，提出基于反事实解释的估计器及校准方案。

Result: 实验表明DFF能发现传统输出值保真度忽略的差异，DFF引导校准可提高关键特征和输入级保真度。

Conclusion: DFF能有效提升自动驾驶虚拟测试中模拟器保真度，且不牺牲输出值保真度。

Abstract: Virtual testing using synthetic data has become a cornerstone of autonomous vehicle (AV) safety assurance. Despite progress in improving visual realism through advanced simulators and generative AI, recent studies reveal that pixel-level fidelity alone does not ensure reliable transfer from simulation to the real world. What truly matters is whether the system-under-test (SUT) bases its decisions on the same causal evidence in both real and simulated environments - not just whether images "look real" to humans. This paper addresses the lack of such a behavior-grounded fidelity measure by introducing Decisive Feature Fidelity (DFF), a new SUT-specific metric that extends the existing fidelity spectrum to capture mechanism parity - the agreement in causal evidence underlying the SUT's decisions across domains. DFF leverages explainable-AI (XAI) methods to identify and compare the decisive features driving the SUT's outputs for matched real-synthetic pairs. We further propose practical estimators based on counterfactual explanations, along with a DFF-guided calibration scheme to enhance simulator fidelity. Experiments on 2126 matched KITTI-VirtualKITTI2 pairs demonstrate that DFF reveals discrepancies overlooked by conventional output-value fidelity. Furthermore, results show that DFF-guided calibration improves decisive-feature and input-level fidelity without sacrificing output value fidelity across diverse SUTs.

</details>


### [41] [Best Practices For Empirical Meta-Algorithmic Research Guidelines from the COSEAL Research Network](https://arxiv.org/abs/2512.16491)
*Theresa Eimer,Lennart Schäpermeier,André Biedenkapp,Alexander Tornede,Lars Kotthoff,Pieter Leyman,Matthias Feurer,Katharina Eggensperger,Kaitlin Maile,Tanja Tornede,Anna Kozak,Ke Xue,Marcel Wever,Mitra Baratchi,Damir Pulatov,Heike Trautmann,Haniye Kashgarani,Marius Lindauer*

Main category: cs.AI

TL;DR: 报告收集元算法研究实证的良好实践，为新研究者和从业者提供指南。


<details>
  <summary>Details</summary>
Motivation: 元算法实证研究依赖大量实验，存在诸多可能致结果不可扩展或无效的错误源，且最佳实践分散。

Method: 从COSEAL社区各子领域收集涵盖整个实验周期的良好实践。

Result:  established当前元算法研究的最佳实践情况。

Conclusion: 研究成果可作为元算法领域新研究者和从业者的指南。

Abstract: Empirical research on meta-algorithmics, such as algorithm selection, configuration, and scheduling, often relies on extensive and thus computationally expensive experiments. With the large degree of freedom we have over our experimental setup and design comes a plethora of possible error sources that threaten the scalability and validity of our scientific insights. Best practices for meta-algorithmic research exist, but they are scattered between different publications and fields, and continue to evolve separately from each other. In this report, we collect good practices for empirical meta-algorithmic research across the subfields of the COSEAL community, encompassing the entire experimental cycle: from formulating research questions and selecting an experimental design, to executing ex- periments, and ultimately, analyzing and presenting results impartially. It establishes the current state-of-the-art practices within meta-algorithmic research and serves as a guideline to both new researchers and practitioners in meta-algorithmic fields.

</details>


### [42] [Scaling Laws for Energy Efficiency of Local LLMs](https://arxiv.org/abs/2512.16531)
*Ander Alvarez,Alessandro Genuardi,Nilotpal Sinha,Antonio Tiene,Samuel Mugel,Román Orús*

Main category: cs.AI

TL;DR: 本文对大语言模型和视觉语言模型在CPU上进行基准测试，发现计算负载缩放规律，量子启发压缩可降低资源消耗和能耗，为边缘推理提供策略。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备部署本地大语言模型和视觉语言模型需平衡精度与计算、能源预算，而CPU上相关计算规律未充分研究。

Method: 在MacBook Pro M2和Raspberry Pi 5两种代表性CPU上进行基准测试，采用统一方法刻画计算负载缩放。

Result: 发现语言模型计算成本与令牌长度近似线性相关，视觉语言模型有“分辨率拐点”；量子启发压缩可降低资源消耗和能耗，保持或提高语义精度。

Conclusion: 量化了本地语言和视觉语言工作负载在CPU上的多模态缩放，确定模型压缩和输入分辨率预处理是可持续边缘推理的有效低成本手段。

Abstract: Deploying local large language models and vision-language models on edge devices requires balancing accuracy with constrained computational and energy budgets. Although graphics processors dominate modern artificial-intelligence deployment, most consumer hardware--including laptops, desktops, industrial controllers, and embedded systems--relies on central processing units. Despite this, the computational laws governing central-processing-unit-only inference for local language and vision-language workloads remain largely unexplored. We systematically benchmark large language and vision-language models on two representative central-processing-unit tiers widely used for local inference: a MacBook Pro M2, reflecting mainstream laptop-class deployment, and a Raspberry Pi 5, representing constrained, low-power embedded settings. Using a unified methodology based on continuous sampling of processor and memory usage together with area-under-curve integration, we characterize how computational load scales with input text length for language models and with image resolution for vision-language models. We uncover two empirical scaling laws: (1) computational cost for language-model inference scales approximately linearly with token length; and (2) vision-language models exhibit a preprocessing-driven "resolution knee", where compute remains constant above an internal resolution clamp and decreases sharply below it. Beyond these laws, we show that quantum-inspired compression reduces processor and memory usage by up to 71.9% and energy consumption by up to 62%, while preserving or improving semantic accuracy. These results provide a systematic quantification of multimodal central-processing-unit-only scaling for local language and vision-language workloads, and they identify model compression and input-resolution preprocessing as effective, low-cost levers for sustainable edge inference.

</details>


### [43] [Needle in the Web: A Benchmark for Retrieving Targeted Web Pages in the Wild](https://arxiv.org/abs/2512.16553)
*Yumeng Wang,Tianyu Fan,Lingrui Xu,Chao Huang*

Main category: cs.AI

TL;DR: 介绍了专门评估搜索代理和大语言模型系统在模糊探索性搜索能力的新基准测试集Needle in the Web，发现当前大多数模型在此测试中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试忽视了模糊探索性搜索，为填补这一空白提出新基准。

Method: 采用灵活方法基于网页内容的事实声明生成不同难度的查询，构建包含663个问题、涵盖七个领域的基准测试集。

Result: 对三个领先的大语言模型和三个基于代理的搜索系统进行测试，多数模型准确率低于35%，且无模型在各领域或难度级别上表现出色。

Conclusion: Needle in the Web对当前搜索系统构成重大挑战，凸显语义模糊下有效模糊检索这一开放问题。

Abstract: Large Language Models (LLMs) have evolved from simple chatbots into sophisticated agents capable of automating complex real-world tasks, where browsing and reasoning over live web content is key to assessing retrieval and cognitive skills. Existing benchmarks like BrowseComp and xBench-DeepSearch emphasize complex reasoning searches requiring multi-hop synthesis but neglect Fuzzy Exploratory Search, namely queries that are vague and multifaceted, where users seek the most relevant webpage rather than a single factual answer. To address this gap, we introduce Needle in the Web, a novel benchmark specifically designed to evaluate modern search agents and LLM-based systems on their ability to retrieve and reason over real-world web content in response to ambiguous, exploratory queries under varying levels of difficulty. Needle in the Web comprises 663 questions spanning seven distinct domains. To ensure high query quality and answer uniqueness, we employ a flexible methodology that reliably generates queries of controllable difficulty based on factual claims of web contents. We benchmark three leading LLMs and three agent-based search systems on Needle in the Web, finding that most models struggle: many achieve below 35% accuracy, and none consistently excel across domains or difficulty levels. These findings reveal that Needle in the Web presents a significant challenge for current search systems and highlights the open problem of effective fuzzy retrieval under semantic ambiguity.

</details>


### [44] [Implementing a Sharia Chatbot as a Consultation Medium for Questions About Islam](https://arxiv.org/abs/2512.16644)
*Wisnu Uriawan,Aria Octavian Hamza,Ade Ripaldi Nuralim,Adi Purnama,Ahmad Juaeni Yunus,Anissya Auliani Supriadi Putri*

Main category: cs.AI

TL;DR: 研究实现符合伊斯兰教法的聊天机器人，用强化学习和语义嵌入，经测试有87%语义准确率，有局限待改进。


<details>
  <summary>Details</summary>
Motivation: 创建交互式媒介以咨询伊斯兰问题，在工业4.0时代提升宗教素养、推动数字宣教和获取经核实的伊斯兰知识。

Method: 结合强化学习（Q学习）与Sentence - Transformers进行语义嵌入，采用CRISP - DM方法处理25000个问答对的数据集，用Flask API作后端、Flutter作前端开发原型。

Result: 聊天机器人原型在功能测试中语义准确率达87%。

Conclusion: 该聊天机器人有潜力连接传统伊斯兰学术和现代人工智能咨询，但存在静态学习和依赖数据集的局限，需未来改进。

Abstract: This research presents the implementation of a Sharia-compliant chatbot as an interactive medium for consulting Islamic questions, leveraging Reinforcement Learning (Q-Learning) integrated with Sentence-Transformers for semantic embedding to ensure contextual and accurate responses. Utilizing the CRISP-DM methodology, the system processes a curated Islam QA dataset of 25,000 question-answer pairs from authentic sources like the Qur'an, Hadith, and scholarly fatwas, formatted in JSON for flexibility and scalability. The chatbot prototype, developed with a Flask API backend and Flutter-based mobile frontend, achieves 87% semantic accuracy in functional testing across diverse topics including fiqh, aqidah, ibadah, and muamalah, demonstrating its potential to enhance religious literacy, digital da'wah, and access to verified Islamic knowledge in the Industry 4.0 era. While effective for closed-domain queries, limitations such as static learning and dataset dependency highlight opportunities for future enhancements like continuous adaptation and multi-turn conversation support, positioning this innovation as a bridge between traditional Islamic scholarship and modern AI-driven consultation.

</details>


### [45] [Prefix Probing: Lightweight Harmful Content Detection for Large Language Models](https://arxiv.org/abs/2512.16650)
*Jirui Yang,Hengqi Guo,Zhihui Lu,Yi Zhao,Yuansen Zhang,Shijing Hu,Qiang Duan,Yinggui Wang,Tao Wei*

Main category: cs.AI

TL;DR: 本文提出Prefix Probing黑盒有害内容检测方法，能降低检测开销，设计高效前缀构造算法提升检测性能，实验证明实用高效。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在现实安全敏感应用中，检测准确率、推理延迟和部署成本存在三方权衡。

Method: 提出Prefix Probing方法，比较“同意/执行”和“拒绝/安全”开头前缀的条件对数概率，利用前缀缓存降低检测开销；设计高效前缀构造算法。

Result: Prefix Probing检测效果与主流外部安全模型相当，仅产生极小计算成本，无需额外模型部署。

Conclusion: Prefix Probing具有很强的实用性和效率。

Abstract: Large language models often face a three-way trade-off among detection accuracy, inference latency, and deployment cost when used in real-world safety-sensitive applications. This paper introduces Prefix Probing, a black-box harmful content detection method that compares the conditional log-probabilities of "agreement/execution" versus "refusal/safety" opening prefixes and leverages prefix caching to reduce detection overhead to near first-token latency. During inference, the method requires only a single log-probability computation over the probe prefixes to produce a harmfulness score and apply a threshold, without invoking any additional models or multi-stage inference. To further enhance the discriminative power of the prefixes, we design an efficient prefix construction algorithm that automatically discovers highly informative prefixes, substantially improving detection performance. Extensive experiments demonstrate that Prefix Probing achieves detection effectiveness comparable to mainstream external safety models while incurring only minimal computational cost and requiring no extra model deployment, highlighting its strong practicality and efficiency.

</details>


### [46] [Comprehensive AI Literacy: The Case for Centering Human Agency](https://arxiv.org/abs/2512.16656)
*Sri Yash Tadimalla,Justin Cary,Gordon Hull,Jordan Register,Daniel Maxwell,David Pugalee,Tina Heafner*

Main category: cs.AI

TL;DR: 当前教育框架未能有效应对AI融入社会带来的教育需求，本文主张向以人类能动性为核心的全面AI素养进行系统性转变。


<details>
  <summary>Details</summary>
Motivation: AI快速融入社会，现有教育框架无法有效应对，出现危险的素养差距。

Method: 提出AI素养、流利度和能力框架。

Result: 使教育者和学生成为以人类为中心的AI应用主体。

Conclusion: 实现全面AI素养需深入培养批判性思维和理解认识论，为对待AI的决策和态度提供路径。

Abstract: The rapid assimilation of Artificial Intelligence technologies into various facets of society has created a significant educational imperative that current frameworks are failing to effectively address. We are witnessing the rise of a dangerous literacy gap, where a focus on the functional, operational skills of using AI tools is eclipsing the development of critical and ethical reasoning about them. This position paper argues for a systemic shift toward comprehensive AI literacy that centers human agency - the empowered capacity for intentional, critical, and responsible choice. This principle applies to all stakeholders in the educational ecosystem: it is the student's agency to question, create with, or consciously decide not to use AI based on the task; it is the teacher's agency to design learning experiences that align with instructional values, rather than ceding pedagogical control to a tool. True literacy involves teaching about agency itself, framing technology not as an inevitability to be adopted, but as a choice to be made. This requires a deep commitment to critical thinking and a robust understanding of epistemology. Through the AI Literacy, Fluency, and Competency frameworks described in this paper, educators and students will become agents in their own human-centric approaches to AI, providing necessary pathways to clearly articulate the intentions informing decisions and attitudes toward AI and the impact of these decisions on academic work, career, and society.

</details>


### [47] [Unsupervised Thematic Clustering Of hadith Texts Using The Apriori Algorithm](https://arxiv.org/abs/2512.16694)
*Wisnu Uriawan,Achmad Ajie Priyajie,Angga Gustian,Fikri Nur Hidayat,Sendi Ahmad Rafiudin,Muhamad Fikri Zaelani*

Main category: cs.AI

TL;DR: 研究用Apriori算法对印尼语布哈里圣训集进行主题分组，发现有意义关联模式，助力数字伊斯兰研究和学习系统发展。


<details>
  <summary>Details</summary>
Motivation: 随着伊斯兰文本数字化，需自动对圣训进行主题分组。

Method: 采用无监督学习的Apriori算法，对印尼语布哈里圣训集进行预处理，再进行关联规则挖掘分析。

Result: 发现如礼拜单元 - 祈祷、经文 - 启示、圣训 - 故事等有意义的关联模式。

Conclusion: Apriori算法能自动揭示潜在语义关系，有助于数字伊斯兰研究和基于技术的学习系统发展。

Abstract: This research stems from the urgency to automate the thematic grouping of hadith in line with the growing digitalization of Islamic texts. Based on a literature review, the unsupervised learning approach with the Apriori algorithm has proven effective in identifying association patterns and semantic relations in unlabeled text data. The dataset used is the Indonesian Translation of the hadith of Bukhari, which first goes through preprocessing stages including case folding, punctuation cleaning, tokenization, stopword removal, and stemming. Next, an association rule mining analysis was conducted using the Apriori algorithm with support, confidence, and lift parameters. The results show the existence of meaningful association patterns such as the relationship between rakaat-prayer, verse-revelation, and hadith-story, which describe the themes of worship, revelation, and hadith narration. These findings demonstrate that the Apriori algorithm has the ability to automatically uncover latent semantic relationships, while contributing to the development of digital Islamic studies and technology-based learning systems.

</details>


### [48] [Do Multi-Agents Solve Better Than Single? Evaluating Agentic Frameworks for Diagram-Grounded Geometry Problem Solving and Reasoning](https://arxiv.org/abs/2512.16698)
*Mahbub E Sobhani,Md. Faiyaz Abdullah Sayeedi,Mohammad Nehad Alam,Proma Hossain Progga,Swakkhar Shatabda*

Main category: cs.AI

TL;DR: 对单智能体和多智能体管道在四个视觉数学基准上进行系统比较，发现多智能体对开源模型有明显性能提升，对专有系统在新基准上有一定帮助，但并非普遍最优。


<details>
  <summary>Details</summary>
Motivation: 探究多智能体设计相对单智能体在图接地几何问题解决中对多模态大语言模型的优势。

Method: 在四个视觉数学基准（Geometry3K、MathVerse、OlympiadBench和We - Math）上系统比较单智能体和多智能体管道。

Result: 开源模型中多智能体持续提升性能，如Qwen - 2.5 - VL不同版本在多个基准上得分增加；闭源的Gemini - 2.0 - Flash在经典基准上单智能体表现更好，在新数据集We - Math上多智能体有适度改进。

Conclusion: 多智能体管道对开源模型有明确好处，能辅助专有系统应对新基准，但智能体分解并非普遍最优。

Abstract: Diagram-grounded geometry problem solving is a critical benchmark for multimodal large language models (MLLMs), yet the benefits of multi-agent design over single-agent remain unclear. We systematically compare single-agent and multi-agent pipelines on four visual math benchmarks: Geometry3K, MathVerse, OlympiadBench, and We-Math. For open-source models, multi-agent consistently improves performance. For example, Qwen-2.5-VL (7B) gains +6.8 points and Qwen-2.5-VL (32B) gains +3.3 on Geometry3K, and both Qwen-2.5-VL variants see further gains on OlympiadBench and We-Math. In contrast, the closed-source Gemini-2.0-Flash generally performs better in single-agent mode on classic benchmarks, while multi-agent yields only modest improvements on the newer We-Math dataset. These findings show that multi-agent pipelines provide clear benefits for open-source models and can assist strong proprietary systems on newer, less familiar benchmarks, but agentic decomposition is not universally optimal. All code, data, and reasoning files are available at https://github.com/faiyazabdullah/Interpreter-Solver

</details>


### [49] [Cyber Humanism in Education: Reclaiming Agency through AI and Learning Sciences](https://arxiv.org/abs/2512.16701)
*Giovanni Adorni*

Main category: cs.AI

TL;DR: 生成式AI重塑教育知识生产与验证，本文提出教育中的赛博人文主义框架并通过案例展示其应用及影响。


<details>
  <summary>Details</summary>
Motivation: 应对大语言模型在教育领域带来的认知自动化、教师去专业化等问题，重新找回人类能动性。

Method: 提出赛博人文主义设计的三大支柱，将其与国际能力框架关联，通过高等教育案例研究，运用基于提示的学习和新认证进行实践。

Result: 这些实践能增强认知能动性，但也暴露出工作量、公平性和治理方面的紧张关系。

Conclusion: 为富含AI、以人为本的教育未来发展提供了启示。

Abstract: Generative Artificial Intelligence (GenAI) is rapidly reshaping how knowledge is produced and validated in education. Rather than adding another digital tool, large language models reconfigure reading, writing, and coding into hybrid human-AI workflows, raising concerns about epistemic automation, cognitive offloading, and the de-professiona\-lisation of teachers. This paper proposes \emph{Cyber Humanism in Education} as a framework for reclaiming human agency in this landscape. We conceptualise AI-enabled learning environments as socio-technical infrastructures co-authored by humans and machines, and position educators and learners as epistemic agents and \emph{algorithmic citizens} who have both the right and the responsibility to shape these infrastructures.
  We articulate three pillars for cyber-humanist design, \emph{reflexive competence}, \emph{algorithmic citizenship}, and \emph{dialogic design}, and relate them to major international digital and AI competence frameworks. We then present higher-education case studies that operationalise these ideas through \emph{prompt-based learning} and a new \emph{Conversational AI Educator} certification within the EPICT ecosystem. The findings show how such practices can strengthen epistemic agency while surfacing tensions around workload, equity, and governance, and outline implications for the future of AI-rich, human-centred education.

</details>


### [50] [Dual Computational Horizons: Incompleteness and Unpredictability in Intelligent Systems](https://arxiv.org/abs/2512.16707)
*Abhisek Ganguly*

Main category: cs.AI

TL;DR: 本文形式化两种限制算法智能的计算局限性，指出其对智能体推理能力的结构限制及相关权衡。


<details>
  <summary>Details</summary>
Motivation: 探究约束算法智能的计算局限性，理解智能系统中推理、预测和自我分析的关系。

Method: 形式化定义形式不完备性和动态不可预测性这两种计算局限性。

Result: 这两种局限性共同对智能体推理自身预测能力施加结构限制，算法智能体通常无法计算自身最大预测 horizon。

Conclusion: 明确了智能系统中推理、预测和自我分析之间存在内在权衡。

Abstract: We formalize two independent computational limitations that constrain algorithmic intelligence: formal incompleteness and dynamical unpredictability. The former limits the deductive power of consistent reasoning systems while the later bounds long-term prediction under finite precision. We show that these two extrema together impose structural bounds on an agent's ability to reason about its own predictive capabilities. In particular, an algorithmic agent cannot compute its own maximal prediction horizon generally. This perspective clarifies inherent trade-offs between reasoning, prediction, and self-analysis in intelligent systems.

</details>


### [51] [Discovering and Learning Probabilistic Models of Black-Box AI Capabilities](https://arxiv.org/abs/2512.16733)
*Daniel Bramblett,Rushang Karia,Adrian Ciotinga,Ruthvick Suresh,Pulkit Verma,YooJung Choi,Siddharth Srivastava*

Main category: cs.AI

TL;DR: 本文提出用PDDL风格表示学习和建模黑盒AI规划能力，理论和实证结果验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 为确保黑盒AI系统安全运行和部署，需开发能有效且可解释表示其能力的方法。

Method: 使用PDDL风格表示，通过蒙特卡罗树搜索范式创建测试任务、获取数据并修剪可能符号模型的假设空间。

Result: 理论结果表明学习模型具有合理性、完整性和收敛性，实证结果展示了方法的范围、效率和准确性。

Conclusion: PDDL风格表示可有效学习和建模黑盒AI的规划能力。

Abstract: Black-box AI (BBAI) systems such as foundational models are increasingly being used for sequential decision making. To ensure that such systems are safe to operate and deploy, it is imperative to develop efficient methods that can provide a sound and interpretable representation of the BBAI's capabilities. This paper shows that PDDL-style representations can be used to efficiently learn and model an input BBAI's planning capabilities. It uses the Monte-Carlo tree search paradigm to systematically create test tasks, acquire data, and prune the hypothesis space of possible symbolic models. Learned models describe a BBAI's capabilities, the conditions under which they can be executed, and the possible outcomes of executing them along with their associated probabilities. Theoretical results show soundness, completeness and convergence of the learned models. Empirical results with multiple BBAI systems illustrate the scope, efficiency, and accuracy of the presented methods.

</details>


### [52] [AI-Driven Prediction of Cancer Pain Episodes: A Hybrid Decision Support Approach](https://arxiv.org/abs/2512.16739)
*Yipeng Zhuang,Yifeng Guo,Yuewen Li,Yuheng Wu,Philip Leung-Ho Yu,Tingting Song,Zhiyong Wang,Kunzhong Zhou,Weifang Wang,Li Zhuang*

Main category: cs.AI

TL;DR: 提出混合机器学习和大语言模型管道预测肺癌患者住院48和72小时内疼痛发作，准确率较高，能提升治疗精准度和资源分配。


<details>
  <summary>Details</summary>
Motivation: 肺癌患者常出现突破性疼痛发作，需及时干预，为实现主动疼痛管理。

Method: 构建混合机器学习和大语言模型管道，分析266例住院患者回顾性队列，结合结构化和非结构化电子健康记录数据，机器学习模块捕捉用药趋势，大语言模型解读模糊记录和文本笔记。

Result: 框架在48小时和72小时预测准确率分别达0.874和0.917，因大语言模型增强，灵敏度分别提升8.6%和10.4%。

Conclusion: 该混合方法为早期疼痛发作预测提供临床可解释且可扩展工具，有望提高肿瘤治疗精准度和优化资源分配。

Abstract: Lung cancer patients frequently experience breakthrough pain episodes, with up to 91% requiring timely intervention. To enable proactive pain management, we propose a hybrid machine learning and large language model pipeline that predicts pain episodes within 48 and 72 hours of hospitalization using both structured and unstructured electronic health record data. A retrospective cohort of 266 inpatients was analyzed, with features including demographics, tumor stage, vital signs, and WHO-tiered analgesic use. The machine learning module captured temporal medication trends, while the large language model interpreted ambiguous dosing records and free-text clinical notes. Integrating these modalities improved sensitivity and interpretability. Our framework achieved an accuracy of 0.874 (48h) and 0.917 (72h), with an improvement in sensitivity of 8.6% and 10.4% due to the augmentation of large language model. This hybrid approach offers a clinically interpretable and scalable tool for early pain episode forecasting, with potential to enhance treatment precision and optimize resource allocation in oncology care.

</details>


### [53] [CitySeeker: How Do VLMS Explore Embodied Urban Navigation With Implicit Human Needs?](https://arxiv.org/abs/2512.16755)
*Siqi Wang,Chao Liang,Yunfan Gao,Erxin Yu,Sen Li,Yushi Li,Jing Li,Haofen Wang*

Main category: cs.AI

TL;DR: 本文引入CitySeeker基准评估VLMs在城市导航中处理隐式需求的能力，实验发现现有模型表现不佳，分析了瓶颈并提出探索策略，为开发有空间智能的VLMs提供见解。


<details>
  <summary>Details</summary>
Motivation: 当前VLMs在动态城市环境中解读隐式人类需求的能力未充分探索，需要评估其空间推理和决策能力。

Method: 引入CitySeeker基准，包含8个城市6440条轨迹和7种场景；开展实验；提出Backtracking Mechanisms、Enriching Spatial Cognition和Memory - Based Retrieval（BCR）探索策略。

Result: 即使顶级模型任务完成率仅21.1%，发现长距离推理误差积累、空间认知不足和经验回忆欠缺等瓶颈。

Conclusion: 分析为开发有强大空间智能的VLMs应对“最后一公里”导航挑战提供了可行见解。

Abstract: Vision-Language Models (VLMs) have made significant progress in explicit instruction-based navigation; however, their ability to interpret implicit human needs (e.g., "I am thirsty") in dynamic urban environments remains underexplored. This paper introduces CitySeeker, a novel benchmark designed to assess VLMs' spatial reasoning and decision-making capabilities for exploring embodied urban navigation to address implicit needs. CitySeeker includes 6,440 trajectories across 8 cities, capturing diverse visual characteristics and implicit needs in 7 goal-driven scenarios. Extensive experiments reveal that even top-performing models (e.g., Qwen2.5-VL-32B-Instruct) achieve only 21.1% task completion. We find key bottlenecks in error accumulation in long-horizon reasoning, inadequate spatial cognition, and deficient experiential recall. To further analyze them, we investigate a series of exploratory strategies-Backtracking Mechanisms, Enriching Spatial Cognition, and Memory-Based Retrieval (BCR), inspired by human cognitive mapping's emphasis on iterative observation-reasoning cycles and adaptive path optimization. Our analysis provides actionable insights for developing VLMs with robust spatial intelligence required for tackling "last-mile" navigation challenges.

</details>


### [54] [TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge](https://arxiv.org/abs/2512.16855)
*Khurram Khalil,Khaza Anuarul Hoque*

Main category: cs.AI

TL;DR: 提出 TOGGLE 框架压缩大语言模型，在四种架构上测试，降低计算成本与模型大小。


<details>
  <summary>Details</summary>
Motivation: 大语言模型计算资源需求大，现有压缩技术会损害语言特性且无形式化保证。

Method: 采用信号时序逻辑（STL）在压缩时指定和执行语言特性；用 STL 鲁棒性导向的贝叶斯优化探索量化和剪枝配置。

Result: 在四种 LLM 架构上，计算成本（FLOPs）最多降低 3.3 倍，模型大小最多降低 68.8%，且满足语言特性。

Conclusion: TOGGLE 首次将形式化方法集成到 LLM 压缩，能实现高效、可验证的边缘部署。

Abstract: Large Language Models (LLMs) deliver exceptional performance across natural language tasks but demand substantial computational resources, limiting their deployment on resource-constrained edge devices. Existing compression techniques, such as quantization and pruning, often degrade critical linguistic properties and lack formal guarantees for preserving model behavior. We propose Temporal Logic-Guided Large Language Model Compression (TOGGLE), a novel framework that leverages Signal Temporal Logic (STL) to formally specify and enforce linguistic properties during compression. TOGGLE employs an STL robustness-guided Bayesian optimization to systematically explore layer-wise quantization and pruning configurations, generating compressed models that formally satisfy specified linguistic constraints without retraining or fine-tuning. Evaluating TOGGLE on four LLM architectures (GPT-2, DeepSeek-V2 7B, LLaMA 3 8B, and Mistral 7B), we achieve up to 3.3x reduction in computational costs (FLOPs) and up to a 68.8% reduction in model size while satisfying all linguistic properties. TOGGLE represents the first integration of formal methods into LLM compression, enabling efficient, verifiable deployment of LLMs on edge hardware.

</details>


### [55] [Distributional AGI Safety](https://arxiv.org/abs/2512.16856)
*Nenad Tomašev,Matija Franklin,Julian Jacobs,Sébastien Krier,Simon Osindero*

Main category: cs.AI

TL;DR: 论文指出应重视拼凑式AGI假设，提出分布式AGI安全框架以应对集体风险。


<details>
  <summary>Details</summary>
Motivation: 当前AI安全和对齐研究多关注单个AGI系统，而拼凑式AGI假设受关注少，且先进AI代理的部署使相关安全考量变得紧迫。

Method: 提出分布式AGI安全框架，核心是设计和实现虚拟代理沙盒经济，通过市场机制、审计、声誉管理和监督来治理代理间交易。

Result: 无明确提及具体结果

Conclusion: 应认真考虑拼凑式AGI假设，并依据此开发相应保障和缓解措施，分布式AGI安全框架有助于应对集体风险。

Abstract: AI safety and alignment research has predominantly been focused on methods for safeguarding individual AI systems, resting on the assumption of an eventual emergence of a monolithic Artificial General Intelligence (AGI). The alternative AGI emergence hypothesis, where general capability levels are first manifested through coordination in groups of sub-AGI individual agents with complementary skills and affordances, has received far less attention. Here we argue that this patchwork AGI hypothesis needs to be given serious consideration, and should inform the development of corresponding safeguards and mitigations. The rapid deployment of advanced AI agents with tool-use capabilities and the ability to communicate and coordinate makes this an urgent safety consideration. We therefore propose a framework for distributional AGI safety that moves beyond evaluating and aligning individual agents. This framework centers on the design and implementation of virtual agentic sandbox economies (impermeable or semi-permeable), where agent-to-agent transactions are governed by robust market mechanisms, coupled with appropriate auditability, reputation management, and oversight to mitigate collective risks.

</details>


### [56] [The Social Responsibility Stack: A Control-Theoretic Architecture for Governing Socio-Technical AI](https://arxiv.org/abs/2512.16873)
*Otman A. Basir*

Main category: cs.AI

TL;DR: 本文提出社会责任感堆栈（SRS）架构框架，将社会价值观嵌入人工智能系统，以解决现有负责任AI和治理机制缺乏可执行工程机制的问题，并通过案例展示其应用。


<details>
  <summary>Details</summary>
Motivation: 现有负责任AI和治理工作缺乏贯穿系统生命周期的可执行工程机制，需要将社会价值观融入AI系统。

Method: 引入SRS六层架构框架，将责任建模为社会技术系统的闭环监督控制问题，开发统一的基于约束的公式，引入安全包络和反馈解释。

Result: 展示了如何持续监控和执行公平性、自主性、认知负担和解释质量等指标，通过案例说明SRS可将规范目标转化为可操作的工程和运营控制。

Conclusion: 该框架架起了伦理、控制理论和AI治理之间的桥梁，为可问责、自适应和可审计的社会技术AI系统提供了实用基础。

Abstract: Artificial intelligence systems are increasingly deployed in domains that shape human behaviour, institutional decision-making, and societal outcomes. Existing responsible AI and governance efforts provide important normative principles but often lack enforceable engineering mechanisms that operate throughout the system lifecycle. This paper introduces the Social Responsibility Stack (SRS), a six-layer architectural framework that embeds societal values into AI systems as explicit constraints, safeguards, behavioural interfaces, auditing mechanisms, and governance processes. SRS models responsibility as a closed-loop supervisory control problem over socio-technical systems, integrating design-time safeguards with runtime monitoring and institutional oversight. We develop a unified constraint-based formulation, introduce safety-envelope and feedback interpretations, and show how fairness, autonomy, cognitive burden, and explanation quality can be continuously monitored and enforced. Case studies in clinical decision support, cooperative autonomous vehicles, and public-sector systems illustrate how SRS translates normative objectives into actionable engineering and operational controls. The framework bridges ethics, control theory, and AI governance, providing a practical foundation for accountable, adaptive, and auditable socio-technical AI systems.

</details>


### [57] [Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning](https://arxiv.org/abs/2512.16917)
*Qihao Liu,Luoxin Ye,Wufei Ma,Yu-Cheng Chou,Alan Yuille*

Main category: cs.AI

TL;DR: 引入Generative Adversarial Reasoner框架，通过对抗强化学习提升语言模型推理能力，在多个数学基准测试中取得增益。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在数学推理中存在如计算错误、逻辑不严谨等过程性错误，需要提升推理能力。

Method: 提出Generative Adversarial Reasoner框架，通过对抗强化学习让推理器和判别器共同进化，采用高效计算审查策略，以互补信号进行学习。

Result: 在多个数学基准测试中，相比标准RL后训练的强基线模型有显著提升，如在AIME24上提升了特定模型的表现。

Conclusion: 该方法能改善信用分配、提高样本效率、增强大语言模型整体推理质量，模块化判别器可灵活塑造奖励目标。

Abstract: Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule partitions each reasoning chain into logically complete slices of comparable length, and the discriminator evaluates each slice's soundness with concise, structured justifications. Learning couples complementary signals: the LLM reasoner is rewarded for logically consistent steps that yield correct answers, while the discriminator earns rewards for correctly detecting errors or distinguishing traces in the reasoning process. This produces dense, well-calibrated, on-policy step-level rewards that supplement sparse exact-match signals, improving credit assignment, increasing sample efficiency, and enhancing overall reasoning quality of LLMs. Across various mathematical benchmarks, the method delivers consistent gains over strong baselines with standard RL post-training. Specifically, on AIME24, we improve DeepSeek-R1-Distill-Qwen-7B from 54.0 to 61.3 (+7.3) and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7 (+10.0). The modular discriminator also enables flexible reward shaping for objectives such as teacher distillation, preference alignment, and mathematical proof-based reasoning.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [58] [Pressure-robust enriched Galerkin finite element methods for coupled Navier-Stokes and heat equations](https://arxiv.org/abs/2512.16716)
*Sanjeeb Poudel,Sanghyun Lee,Lin Mu*

Main category: cs.CE

TL;DR: 提出压力稳健的富集Galerkin有限元方法求解不可压缩Navier - Stokes和Boussinesq热方程，开展数值研究评估其性能。


<details>
  <summary>Details</summary>
Motivation: 需要高效准确的方法求解不可压缩Navier - Stokes和Boussinesq热方程，且在复杂几何和高畸变网格下保持稳定。

Method: 结合连续Lagrange单元和间断富集向量的EG公式；构造速度重构算子保证压力稳健性；用Picard迭代和Anderson加速迭代处理非线性。

Result: 基于AC混合有限元空间的重构方案在高畸变网格稳定准确；Anderson加速迭代在高Rayleigh数流中稳健高效收敛。

Conclusion: 压力稳健EG方法是复杂几何中求解耦合流动和热传输问题的灵活准确工具。

Abstract: We propose a pressure-robust enriched Galerkin (EG) finite element method for the incompressible Navier-Stokes and heat equations in the Boussinesq regime. For the Navier-Stokes equations, the EG formulation combines continuous Lagrange elements with a discontinuous enrichment vector per element in the velocity space and a piecewise constant pressure space, and it can be implemented efficiently within standard finite element frameworks. To enforce pressure robustness, we construct velocity reconstruction operators that map the discrete EG velocity field into exactly divergence-free, H(div)-conforming fields. In particular, we develop reconstructions based on Arbogast-Correa (AC) mixed finite element spaces on quadrilateral meshes and demonstrate that the resulting schemes remain stable and accurate even on highly distorted grids. The nonlinearity of the coupled Navier-Stokes-Boussinesq system is treated with several iterative strategies, including Picard iterations and Anderson-accelerated iterations; our numerical study shows that Anderson acceleration yields robust and efficient convergence for high Rayleigh number flows within the proposed framework. The performance of the method is assessed on a set of benchmark problems and application-driven test cases. These numerical experiments highlight the potential of pressure-robust EG methods as flexible and accurate tools for coupled flow and heat transport in complex geometries.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [59] [DP-Bench: A Benchmark for Evaluating Data Product Creation Systems](https://arxiv.org/abs/2512.15798)
*Faisal Chowdhury,Sola Shirai,Sarthak Dash,Nandana Mihindukulasooriya,Horst Samulowitz*

Main category: cs.DB

TL;DR: 本文提出首个用于评估自动创建数据产品的基准DP - Bench，并提出基于大语言模型的方法作为自动生成数据产品的基线，相关资源已开源。


<details>
  <summary>Details</summary>
Motivation: 目前几乎没有评估自动创建数据产品的基准，需要一个这样的基准来推动该领域发展。

Method: 利用ELT和Text - to - SQL基准的现有工作创建DP - Bench，提出基于大语言模型的方法作为基线。

Result: 创建了名为DP - Bench的基准，提出基于大语言模型的自动生成数据产品的基线方法，且将DP - Bench及补充材料开源。

Conclusion: 首个用于评估自动创建数据产品的基准DP - Bench的提出，为该领域提供了评估工具和基线方法。

Abstract: A data product is created with the intention of solving a specific problem, addressing a specific business usecase or meeting a particular need, going beyond just serving data as a raw asset. Data products enable end users to gain greater insights about their data. Since it was first introduced over a decade ago, there has been considerable work, especially in industry, to create data products manually or semi-automatically. However, there exists hardly any benchmark to evaluate automatic data product creation. In this work, we present a benchmark, first of its kind, for this task. We call it DP-Bench. We describe how this benchmark was created by taking advantage of existing work in ELT (Extract-Load-Transform) and Text-to-SQL benchmarks. We also propose a number of LLM based approaches that can be considered as baselines for generating data products automatically. We make the DP-Bench and supplementary materials available in https://huggingface.co/datasets/ibm-research/dp-bench .

</details>


### [60] [Implementing a Scalable, Redeployable and Multitiered Repository for FAIR and Secure Scientific Data Sharing: The BIG-MAP Archive](https://arxiv.org/abs/2512.15815)
*Valeria Granata,Francois Liot,Xing Wang,Steen Lysgaard,Ivano E. Castelli,Tejs Vegge,Nicola Marzari,Giovanni Pizzi*

Main category: cs.DB

TL;DR: 本文介绍BIG - MAP Archive，它是基于云的学科私有仓库，能解决大型联盟数据共享的组织和技术挑战，实现安全可控的数据共享，且可重新部署。


<details>
  <summary>Details</summary>
Motivation: 解决大型联盟（如研究合作、行业伙伴关系）数据共享时面临的组织和技术挑战，促进协作、交换成果及确保敏感数据安全访问。

Method: 构建基于InvenioRDM的BIG - MAP Archive，利用平台功能满足联盟特定需求，通过细粒度权限控制数据和元数据访问，有正式上传流程。

Result: BIG - MAP Archive能实现大型联盟内安全、受控的数据共享，可限制或开放访问，数据可按需准备公开。

Conclusion: BIG - MAP Archive能确保数据保密性，支持灵活的基于权限的访问，且可轻松为其他联盟重新部署。

Abstract: Data sharing in large consortia, such as research collaborations or industry partnerships, requires addressing both organizational and technical challenges. A common platform is essential to promote collaboration, facilitate exchange of findings, and ensure secure access to sensitive data. Key technical challenges include creating a scalable architecture, a user-friendly interface, and robust security and access control. The BIG-MAP Archive is a cloud-based, disciplinary, private repository designed to address these challenges. Built on InvenioRDM, it leverages platform functionalities to meet consortium-specific needs, providing a tailored solution compared to general repositories. Access can be restricted to members of specific communities or open to the entire consortium, such as the BATTERY 2030+, a consortium accelerating advanced battery technologies. Uploaded data and metadata are controlled via fine grained permissions, allowing access to individual project members or the full initiative. The formalized upload process ensures data are formatted and ready for publication in open repositories when needed. This paper reviews the repository's key features, showing how the BIG-MAP Archive enables secure, controlled data sharing within large consortia. It ensures data confidentiality while supporting flexible, permissions-based access and can be easily redeployed for other consortia, including MaterialsCommons4.eu and RAISE (Resource for AI Science in Europe).

</details>


### [61] [Scaling Text2SQL via LLM-efficient Schema Filtering with Functional Dependency Graph Rerankers](https://arxiv.org/abs/2512.16083)
*Thanh Dat Hoang,Thanh Tam Nguyen,Thanh Trung Huynh,Hongzhi Yin,Quoc Viet Hung Nguyen*

Main category: cs.DB

TL;DR: 提出开源的LLM高效模式过滤框架	oolname来压缩Text2SQL提示，在真实数据集实验表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有的Text2SQL系统在处理超出LLM上下文限制的真实世界模式时失败，当前缓解措施存在不足，需要扩展现有系统。

Method: 通过查询感知的LLM编码器对列进行排名，使用轻量级图变换器对相互连接的列进行重新排名，用斯坦纳树启发式方法选择保留连接性的子模式。

Result: 	oolname在真实数据集上实现了近乎完美的召回率和更高的精度，保持亚秒级中位延迟，可扩展到有23000多个列的模式。

Conclusion: 所提出的	oolname框架有效，能解决现有Text2SQL系统在处理大规模模式时的问题。

Abstract: Most modern Text2SQL systems prompt large language models (LLMs) with entire schemas -- mostly column information -- alongside the user's question. While effective on small databases, this approach fails on real-world schemas that exceed LLM context limits, even for commercial models. The recent Spider 2.0 benchmark exemplifies this with hundreds of tables and tens of thousands of columns, where existing systems often break. Current mitigations either rely on costly multi-step prompting pipelines or filter columns by ranking them against user's question independently, ignoring inter-column structure. To scale existing systems, we introduce \toolname, an open-source, LLM-efficient schema filtering framework that compacts Text2SQL prompts by (i) ranking columns with a query-aware LLM encoder enriched with values and metadata, (ii) reranking inter-connected columns via a lightweight graph transformer over functional dependencies, and (iii) selecting a connectivity-preserving sub-schema with a Steiner-tree heuristic. Experiments on real datasets show that \toolname achieves near-perfect recall and higher precision than CodeS, SchemaExP, Qwen rerankers, and embedding retrievers, while maintaining sub-second median latency and scaling to schemas with 23,000+ columns. Our source code is available at https://github.com/thanhdath/grast-sql.

</details>


### [62] [ModelTables: A Corpus of Tables about Models](https://arxiv.org/abs/2512.16106)
*Zhengyuan Dong,Victor Zhong,Renée J. Miller*

Main category: cs.DB

TL;DR: 提出ModelTables基准，涵盖超60K模型和90K表格，用多源信号评估相关性，进行表格搜索实验，指出需更好搜索方法，提供创建协议和代码。


<details>
  <summary>Details</summary>
Motivation: 捕获模型湖中表格的结构化语义，解决文本检索易忽略的问题，推动结构化模型知识的检索、比较和组织。

Method: 从多个来源构建语料库，用三种互补信号构建多源地面真值，在基准上比较不同搜索算子和信息检索基线。

Result: 联合语义表格检索P@1为54.8%，基于表格的密集检索达66.5%，元数据混合检索为54.1%。

Conclusion: 有必要开发更好的表格搜索方法，ModelTables为结构化模型知识处理提供依据。

Abstract: We present ModelTables, a benchmark of tables in Model Lakes that captures the structured semantics of performance and configuration tables often overlooked by text only retrieval. The corpus is built from Hugging Face model cards, GitHub READMEs, and referenced papers, linking each table to its surrounding model and publication context. Compared with open data lake tables, model tables are smaller yet exhibit denser inter table relationships, reflecting tightly coupled model and benchmark evolution. The current release covers over 60K models and 90K tables. To evaluate model and table relatedness, we construct a multi source ground truth using three complementary signals: (1) paper citation links, (2) explicit model card links and inheritance, and (3) shared training datasets. We present one extensive empirical use case for the benchmark which is table search. We compare canonical Data Lake search operators (unionable, joinable, keyword) and Information Retrieval baselines (dense, sparse, hybrid retrieval) on this benchmark. Union based semantic table retrieval attains 54.8 % P@1 overall (54.6 % on citation, 31.3 % on inheritance, 30.6 % on shared dataset signals); table based dense retrieval reaches 66.5 % P@1, and metadata hybrid retrieval achieves 54.1 %. This evaluation indicates clear room for developing better table search methods. By releasing ModelTables and its creation protocol, we provide the first large scale benchmark of structured data describing AI model. Our use case of table discovery in Model Lakes, provides intuition and evidence for developing more accurate semantic retrieval, structured comparison, and principled organization of structured model knowledge. Source code, data, and other artifacts have been made available at https://github.com/RJMillerLab/ModelTables.

</details>


### [63] [Multi-granularity Spatiotemporal Flow Patterns](https://arxiv.org/abs/2512.16255)
*Chrysanthi Kosyfaki,Nikos Mamoulis,Reynold Cheng,Ben Kai*

Main category: cs.DB

TL;DR: 本文研究不同粒度下区域间乘客流动重要趋势问题，定义ODT模式，提出枚举算法、优化方法、模式变体和近似解法，并在三个真实数据集上评估。


<details>
  <summary>Details</summary>
Motivation: 分析不同时空粒度下的对象或数据流动能揭示有趣的见解或趋势，研究者希望找到不同粒度下区域间乘客流动的重要趋势。

Method: 定义ODT模式，提出自底向上的枚举算法，采用优化方法减少搜索空间和计算成本，提出模式变体和基于生成 - 测试的近似解法。

Result: 在三个真实数据集上评估了方法的效率和有效性，展示了有趣的ODT流动模式。

Conclusion: 所提出的方法可有效发现不同粒度下区域间乘客移动的重要趋势，在实际数据集中有良好表现。

Abstract: Analyzing flow of objects or data at different granularities of space and time can unveil interesting insights or trends. For example, transportation companies, by aggregating passenger travel data (e.g., counting passengers traveling from one region to another), can analyze movement behavior. In this paper, we study the problem of finding important trends in passenger movements between regions at different granularities. We define Origin (O), Destination (D), and Time (T ) patterns (ODT patterns) and propose a bottom-up algorithm that enumerates them. We suggest and employ optimizations that greatly reduce the search space and the computational cost of pattern enumeration. We also propose pattern variants (constrained patterns and top-k patterns) that could be useful to differ- ent applications scenarios. Finally, we propose an approximate solution that fast identifies ODT patterns of specific sizes, following a generate-and-test approach. We evaluate the efficiency and effectiveness of our methods on three real datasets and showcase interesting ODT flow patterns in them.

</details>


### [64] [Subset Sampling over Joins](https://arxiv.org/abs/2512.16321)
*Aryan Esmailpour,Xiao Hu,Jinchao Huang,Stavros Sintos*

Main category: cs.DB

TL;DR: 本文研究连接上子集抽样问题，提出针对无环连接的高效抽样算法，实现接近最优的时空复杂度。


<details>
  <summary>Details</summary>
Motivation: 现代应用常需对关系连接隐式定义的集合抽样，而直接物化所有连接结果抽样计算不可行。

Method: 提出三种算法：静态索引用于生成多个连接子集样本；单次算法生成单个样本；动态索引支持元组插入并能维护样本或生成多个样本。

Result: 所提技术在输入大小和期望样本大小方面实现接近最优的时空复杂度。

Conclusion: 所提出的高效算法可有效解决连接上的子集抽样问题。

Abstract: Subset sampling (also known as Poisson sampling), where the decision to include any specific element in the sample is made independently of all others, is a fundamental primitive in data analytics, enabling efficient approximation by processing representative subsets rather than massive datasets. While sampling from explicit lists is well-understood, modern applications -- such as machine learning over relational data -- often require sampling from a set defined implicitly by a relational join. In this paper, we study the problem of \emph{subset sampling over joins}: drawing a random subset from the join results, where each join result is included independently with some probability. We address the general setting where the probability is derived from input tuple weights via decomposable functions (e.g., product, sum, min, max). Since the join size can be exponentially larger than the input, the naive approach of materializing all join results to perform subset sampling is computationally infeasible. We propose the first efficient algorithms for subset sampling over acyclic joins: (1) a \emph{static index} for generating multiple (independent) subset samples over joins; (2) a \emph{one-shot} algorithm for generating a single subset sample over joins; (3) a \emph{dynamic index} that can support tuple insertions, while maintaining a one-shot sample or generating multiple (independent) samples. Our techniques achieve near-optimal time and space complexity with respect to the input size and the expected sample size.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [65] [LOG.io: Unified Rollback Recovery and Data Lineage Capture for Distributed Data Pipelines](https://arxiv.org/abs/2512.16038)
*Eric Simon,Renato B. Hoffmann,Lucas Alf,Dalvan Griebler*

Main category: cs.DC

TL;DR: 本文介绍LOG.io，评估其在分布式数据管道的性能，与ABS协议对比。


<details>
  <summary>Details</summary>
Motivation: 为分布式数据管道提供正确回滚恢复和细粒度数据谱系捕获的全面解决方案。

Method: 使用基于日志的回滚恢复协议，在SAP Data Intelligence系统中进行性能评估对比。

Result: 有拖后腿算子且事件吞吐量适中时，正常处理中LOG.io与ABS相当，恢复时更优；其他情况ABS更优，但数据并行化可降低LOG.io开销；数据谱系捕获开销边际。

Conclusion: LOG.io对分布式数据管道的回滚恢复和数据谱系捕获有一定优势。

Abstract: This paper introduces LOG.io, a comprehensive solution designed for correct rollback recovery and fine-grain data lineage capture in distributed data pipelines. It is tailored for serverless scalable architectures and uses a log-based rollback recovery protocol. LOG.io supports a general programming model, accommodating non-deterministic operators, interactions with external systems, and arbitrary custom code. It is non-blocking, allowing failed operators to recover independently without interrupting other active operators, thereby leveraging data parallelization, and it facilitates dynamic scaling of operators during pipeline execution. Performance evaluations, conducted within the SAP Data Intelligence system, compare LOG.io with the Asynchronous Barrier Snapshotting (ABS) protocol, originally implemented in Flink. Our experiments show that when there are straggler operators in a data pipeline and the throughput of events is moderate (e.g., 1 event every 100 ms), LOG.io performs as well as ABS during normal processing and outperforms ABS during recovery. Otherwise, ABS performs better than LOG.io for both normal processing and recovery. However, we show that in these cases, data parallelization can largely reduce the overhead of LOG.io while ABS does not improve. Finally, we show that the overhead of data lineage capture, at the granularity of the event and between any two operators in a pipeline, is marginal, with less than 1.5% in all our experiments.

</details>


### [66] [MultiPath Transfer Engine: Breaking GPU and Host-Memory Bandwidth Bottlenecks in LLM Services](https://arxiv.org/abs/2512.16056)
*Lingfeng Tang,Daoping Zhang,Junjie Chen,Peihao Huang,Feng Jin,Chengguang Xu,Yuxin Chen,Feiqiang Sun,Guo Chen*

Main category: cs.DC

TL;DR: 论文提出MMA方案解决GPU与主机内存间数据传输带宽瓶颈，显著提升带宽并减少LLM服务时间和模型切换延迟。


<details>
  <summary>Details</summary>
Motivation: PCIe带宽有限成为LLM性能瓶颈，现有异构协议限制主机内存与GPU间带宽，导致服务器内带宽利用率低。

Method: 提出Multipath Memory Access (MMA)方案，通过动态库注入实现无缝部署，无需修改代码。

Result: 在测试平台上，MMA显著提高GPU与内存间数据传输带宽，峰值达245GB/s，相比单路径带宽提升4.62倍；端到端评估显示，减少LLM服务TTFT 1.14 - 2.38倍，降低vLLM睡眠模式下模型切换延迟1.12 - 2.48倍。

Conclusion: MMA方案有效解决了GPU与主机内存间数据传输的带宽瓶颈问题，提升了LLM性能。

Abstract: The limited bandwidth of PCIe has emerged as the critical bottleneck for large language model (LLM) performance, such as prefix cache fetching and model switching. Although intra-server multipath data transfer between GPU and host memory is theoretically possible, heterogeneous protocols such as PCIe and NVLink currently limit the bandwidth between host memory and GPUs to that of a single PICe link. This limitation resuals in underutilized intra-server bandwidth. To address this issue, we propose Multipath Memory Access (MMA), a scheme that, to the best of our knowledge, is the first to enalbe efficient multipath data transfer between GPU and host memory. MMA supports seamless deployment via dynamic library injection, enabling LLM applications to benefit from MMA without requiring any code modification. In our testbed, MMA significantly improves the data transfer bandwidth between the GPU and memory, achieving a peak bandwidth of 245 GB/s-representing a 4.62x speedup compared to the natice single-path bandwidth. End-to-end evaluations demonstrate that MMA reduces the time-to-first-token (TTFT) for LLM serving by 1.14x to 2.38x and decreases model-switching latency in vLLM's sleep mode by 1.12x to 2.48x.

</details>


### [67] [Twinning for Space-Air-Ground-Sea Integrated Networks: Beyond Conventional Digital Twin Towards Goal-Oriented Semantic Twin](https://arxiv.org/abs/2512.16058)
*Yifei Qiu,Tianle Liao,Xin Jin,Shaohua Wu,Dusit Niyato,Qinyu Zhang*

Main category: cs.DC

TL;DR: 本文针对空间-空中-地面-海洋集成网络（SAGSIN）中传统数字孪生（DT）的局限性，提出了面向目标的语义孪生（GOST）框架，介绍其构建方法、评估框架，通过案例证明其优势并指出研究方向。


<details>
  <summary>Details</summary>
Motivation: SAGSIN中传统DT存在计算开销大、模型同步延迟和跨系统语义差距等局限性，为满足实时态势感知和智能运维需求，需新的孪生框架。

Method: 提出GOST框架，从基于知识的语义、数据驱动的语义和面向目标的原则三个层面阐述，详细介绍核心使能技术，构建多维评估框架。

Result: 以远程卫星 - 无人机网络协作跟踪任务为例，表明GOST在感知数据及时性和协作跟踪方面显著优于传统DT。

Conclusion: GOST是一种变革性的孪生范式，可指导SAGSIN的发展。

Abstract: A space-air-ground-sea integrated network (SAGSIN) has emerged as a cornerstone of 6G systems, establishing a unified global architecture by integrating multi-domain network resources. Motivated by the demand for real-time situational awareness and intelligent operational maintenance, digital twin (DT) technology was initially regarded as a promising solution, owing to its capability to create virtual replicas and emulate physical system behaviors. However, in the context of SAGSIN, the high-fidelity, full-scale modeling paradigm inherent to conventional DTs encounters fundamental limitations, including prohibitive computational overhead, delayed model synchronization, and cross-system semantic gaps. To address these limitations, this survey paper proposes a novel twinning framework: goal-oriented semantic twin (GOST). Unlike DTs that pursue physical mirroring, GOST prioritizes ``utility'' over ``fidelity,'' leveraging semantic technologies and goal-oriented principles to construct lightweight, task-specific representations. This paper systematically articulates the GOST framework through three layers: knowledge-based semantics, data-driven semantics, and goal-oriented principles. Furthermore, we provide a comprehensive tutorial on constructing GOST by detailing its core enabling technologies and introduce a multidimensional evaluation framework for GOST. We present a case study targeting collaborative tracking tasks in remote satellite-UAV networks, demonstrating that GOST significantly outperforms conventional DTs in timeliness of perceptual data and collaborative tracking. Finally, we outline research directions, establishing GOST as a transformative twinning paradigm to guide the development of SAGSIN.

</details>


### [68] [Cold-Start Anti-Patterns and Refactorings in Serverless Systems: An Empirical Study](https://arxiv.org/abs/2512.16066)
*Syed Salauddin Mohammad Tariq,Foyzul Hassan,Amiangshu Bosu,Probir Roy*

Main category: cs.DC

TL;DR: 研究无服务器计算冷启动问题，提出SCABENCH基准和INITSCOPE框架，提升诊断效果。


<details>
  <summary>Details</summary>
Motivation: 无服务器计算冷启动延迟是性能瓶颈，以往将缓解措施视为黑盒优化，本文将其作为开发者可见的设计问题研究。

Method: 从81个开源无服务器系统问题报告中得出初始化反模式、修复策略和诊断挑战的分类，构建SCABENCH基准和INITSCOPE框架。

Result: INITSCOPE在SCABENCH上使定位准确率提高40%，诊断工作量减少64%，开发者研究显示任务准确率更高、诊断更快。

Conclusion: 研究结果推动无服务器设计中冷启动缓解的循证、性能感知实践，研究工件公开可用于后续研究和改进。

Abstract: Serverless computing simplifies deployment and scaling, yet cold-start latency remains a major performance bottleneck. Unlike prior work that treats mitigation as a black-box optimization, we study cold starts as a developer-visible design problem. From 81 adjudicated issue reports across open-source serverless systems, we derive taxonomies of initialization anti-patterns, remediation strategies, and diagnostic challenges spanning design, packaging, and runtime layers. Building on these insights, we introduce SCABENCH, a reproducible benchmark, and INITSCOPE, a lightweight analysis framework linking what code is loaded with what is executed. On SCABENCH, INITSCOPE improved localization accuracy by up to 40% and reduced diagnostic effort by 64% compared with prior tools, while a developer study showed higher task accuracy and faster diagnosis. Together, these results advance evidence-driven, performance-aware practices for cold-start mitigation in serverless design. Availability: The research artifact is publicly accessible for future studies and improvements.

</details>


### [69] [An Online Fragmentation-Aware Scheduler for Managing GPU-Sharing Workloads on Multi-Instance GPUs](https://arxiv.org/abs/2512.16099)
*Hsu-Tzu Ting,Jerry Chou,Ming-Hung Chen,I-Hsin Chung*

Main category: cs.DC

TL;DR: 现代GPU工作负载需高效资源共享，MIG虽提供硬件级隔离但有资源争用和碎片化问题，提出在线调度框架解决，实验显示显著提升系统效率。


<details>
  <summary>Details</summary>
Motivation: 现代GPU工作负载需要高效资源共享，而NVIDIA的MIG在有效利用时存在资源争用和GPU碎片化等新挑战。

Method: 提出一个集成条件负载均衡、动态分区和作业迁移的在线调度框架，动态调整作业放置以减少争用，重新组织GPU分配以解决内外碎片化问题。

Result: 实验结果表明，该方法显著提高了系统效率，应用所有技术后，完工时间最多可改善35%。

Conclusion: 所提出的在线调度框架能有效解决MIG使用中的问题，提升系统效率。

Abstract: Modern GPU workloads increasingly demand efficient resource sharing, as many jobs do not require the full capacity of a GPU. Among sharing techniques, NVIDIA's Multi-Instance GPU (MIG) offers strong resource isolation by enabling hardware-level GPU partitioning. However, leveraging MIG effectively introduces new challenges. First, resource contention persists due to shared components such as PCIe bandwidth. Second, GPU fragmentation becomes a critical issue, which is different from prior fine-grained GPU sharing work due to MIG's limited number of valid MIG configurations. Fragmentation arises not only from spatial discontinuity but also from rigid profile placement constraints, especially after job arrivals and terminations. To address these issues, we propose an online scheduling framework that integrates conditional load balancing, dynamic partitioning, and job migration. Our approach dynamically adapts job placement to minimize contention and reorganizes GPU allocations to combat both internal and external fragmentation. Experimental results show that our method significantly improves system efficiency. When all techniques are applied, the makespan improves by up to 35%.

</details>


### [70] [Staggered Batch Scheduling: Co-optimizing Time-to-First-Token and Throughput for High-Efficiency LLM Inference](https://arxiv.org/abs/2512.16134)
*Jian Tian,Shuailong Li,Yang Cao,Wenbo Cui,Minghan Zhu,Wenkang Wu,Jianming Zhang,Yanpeng Wang,Zhiwen Xiao,Zhenyu Hou,Dou Shen*

Main category: cs.DC

TL;DR: 论文针对大语言模型DP+EP架构调度挑战提出交错批调度（SBS）与负载感知全局分配策略，降低TTFT并提高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型DP+EP架构有高内部同步成本，立即请求调度会导致引擎内排队和并行气泡，降低TTFT。

Method: 提出交错批调度（SBS），通过缓冲请求形成最优执行批次；引入负载感知全局分配策略平衡计算负载。

Result: 在H800集群上为Deepseek - V3服务时，与现有即时调度基线相比，TTFT降低30% - 40%，吞吐量提高15% - 20%。

Conclusion: 交错批调度和负载感知全局分配策略能有效解决DP+EP架构调度问题，提升系统性能。

Abstract: The evolution of Large Language Model (LLM) serving towards complex, distributed architectures--specifically the P/D-separated, large-scale DP+EP paradigm--introduces distinct scheduling challenges. Unlike traditional deployments where schedulers can treat instances as black boxes, DP+EP architectures exhibit high internal synchronization costs. We identify that immediate request dispatching in such systems leads to severe in-engine queuing and parallelization bubbles, degrading Time-to-First-Token (TTFT). To address this, we propose Staggered Batch Scheduling (SBS), a mechanism that deliberately buffers requests to form optimal execution batches. This temporal decoupling eliminates internal queuing bubbles without compromising throughput. Furthermore, leveraging the scheduling window created by buffering, we introduce a Load-Aware Global Allocation strategy that balances computational load across DP units for both Prefill and Decode phases. Deployed on a production H800 cluster serving Deepseek-V3, our system reduces TTFT by 30%-40% and improves throughput by 15%-20% compared to state-of-the-art immediate scheduling baselines.

</details>


### [71] [Lotus: Optimizing Disaggregated Transactions with Disaggregated Locks](https://arxiv.org/abs/2512.16136)
*Zhisheng Hu,Pengfei Zuo,Junliang Hu,Yizou Chen,Yingjia Wang,Ming-Chang Yang*

Main category: cs.DC

TL;DR: 现有分布式事务系统在分散内存（DM）中存在MN的RDMA网卡性能瓶颈，本文提出Lotus系统解决此问题，实验显示能提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有DM分布式事务系统中MN的RDMA网卡因大量单边原子锁操作成为性能瓶颈，阻碍系统高效扩展。

Method: 提出Lotus系统，将锁与数据分离，在CN执行锁操作；采用应用感知锁管理机制；引入锁优先事务协议；使用免锁重建的恢复机制。

Result: 相比当前DM上的先进事务系统，Lotus将事务吞吐量提高至2.1倍，延迟降低49.4%。

Conclusion: Lotus系统通过锁分离和相应机制有效解决了MN RNIC的瓶颈问题，提升了系统性能。

Abstract: Disaggregated memory (DM) separates compute and memory resources, allowing flexible scaling to achieve high resource utilization. To ensure atomic and consistent data access on DM, distributed transaction systems have been adapted, where compute nodes (CNs) rely on one-sided RDMA operations to access remote data in memory nodes (MNs). However, we observe that in existing transaction systems, the RDMA network interface cards at MNs become a primary performance bottleneck. This bottleneck arises from the high volume of one-sided atomic operations used for locks, which hinders the system's ability to scale efficiently.
  To address this issue, this paper presents Lotus, a scalable distributed transaction system with lock disaggregation on DM. The key innovation of Lotus is to disaggregate locks from data and execute all locks on CNs, thus eliminating the bottleneck at MN RNICs. To achieve efficient lock management on CNs, Lotus employs an application-aware lock management mechanism that leverages the locality of the OLTP workloads to shard locks while maintaining load balance. To ensure consistent transaction processing with lock disaggregation, Lotus introduces a lock-first transaction protocol, which separates the locking phase as the first step in each read-write transaction execution. This protocol allows the system to determine the success of lock acquisitions early and proactively abort conflicting transactions, improving overall efficiency. To tolerate lock loss during CN failures, Lotus employs a lock-rebuild-free recovery mechanism that treats locks as ephemeral and avoids their reconstruction, ensuring lightweight recovery for CN failures. Experimental results demonstrate that Lotus improves transaction throughput by up to 2.1$\times$ and reduces latency by up to 49.4% compared to state-of-the-art transaction systems on DM.

</details>


### [72] [FlexKV: Flexible Index Offloading for Memory-Disaggregated Key-Value Store](https://arxiv.org/abs/2512.16148)
*Zhisheng Hu,Jiacheng Shen,Ming-Chang Yang*

Main category: cs.DC

TL;DR: 提出FlexKV，通过索引代理优化内存分离键值存储，实验显示性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有内存分离键值存储因索引处理依赖单边原子操作和计算侧缓存效率受限，导致性能不佳。

Method: 提出FlexKV，动态将索引卸载到计算节点，采用排名感知热度检测算法平衡负载、两级CN内存优化方案利用内存、RPC聚合缓存管理机制降低缓存一致性开销。

Result: 与现有内存分离键值存储相比，FlexKV吞吐量最高提升2.94倍，延迟最多降低85.2%。

Conclusion: FlexKV能有效提升内存分离键值存储的性能。

Abstract: Disaggregated memory (DM) is a promising data center architecture that decouples CPU and memory into independent resource pools to improve resource utilization. Building on DM, memory-disaggregated key-value (KV) stores are adopted to efficiently manage remote data. Unfortunately, existing approaches suffer from poor performance due to two critical issues: 1) the overdependence on one-sided atomic operations in index processing, and 2) the constrained efficiency in compute-side caches. To address these issues, we propose FlexKV, a memory-disaggregated KV store with index proxying. Our key idea is to dynamically offload the index to compute nodes, leveraging their powerful CPUs to accelerate index processing and maintain high-performance compute-side caches. Three challenges have to be addressed to enable efficient index proxying on DM, i.e., the load imbalance across compute nodes, the limited memory of compute nodes, and the expensive cache coherence overhead. FlexKV proposes: 1) a rank-aware hotness detection algorithm to continuously balance index load across compute nodes, 2) a two-level CN memory optimization scheme to efficiently utilize compute node memory, and 3) an RPC-aggregated cache management mechanism to reduce cache coherence overhead. The experimental results show that FlexKV improves throughput by up to 2.94$\times$ and reduces latency by up to 85.2%, compared with the state-of-the-art memory-disaggregated KV stores.

</details>


### [73] [AI4EOSC: a Federated Cloud Platform for Artificial Intelligence in Scientific Research](https://arxiv.org/abs/2512.16455)
*Ignacio Heredia,Álvaro López García,Germán Moltó,Amanda Calatrava,Valentin Kozlov,Alessandro Costantini,Viet Tran,Mario David,Daniel San Martín,Marcin Płóciennik,Marta Obregón Ruiz,Saúl Fernandez,Judith Sáinz-Pardo Díaz,Miguel Caballer,Caterina Alarcón Marín,Stefan Dlugolinsky,Martin Šeleng,Lisana Berberi,Khadijeh Alibabaei,Borja Esteban Sanchis,Pedro Castro,Giacinto Donvito,Diego Aguirre,Sergio Langarita,Vicente Rodriguez,Leonhard Duda,Andrés Heredia Canales,Susana Rebolledo Ruiz,João Machado,Giang Nguyen,Fernando Aguilar Gómez,Jaime Díez*

Main category: cs.DC

TL;DR: 介绍了一个支持科学工作负载中人工智能的联邦计算平台，提供集成用户体验、可追溯性工具等，且易于定制。


<details>
  <summary>Details</summary>
Motivation: 为科学工作负载中的人工智能提供支持，实现可重复部署，让用户能一致、透明地访问分布式电子基础设施。

Method: 构建全面的服务目录，提供涵盖机器学习全生命周期的集成用户体验；提供可追溯性和可重复性工具；与不同的人工智能模型提供商、数据集和存储资源集成；设计易于定制的平台。

Result: 该平台能提供集成用户体验，支持机器学习全生命周期，提供可追溯性和可重复性工具，集成多种资源。

Conclusion: 平台易于定制，可降低外部社区的采用门槛，能让用户与更广泛的机器学习生态系统互动。

Abstract: In this paper, we describe a federated compute platform dedicated to support Artificial Intelligence in scientific workloads. Putting the effort into reproducible deployments, it delivers consistent, transparent access to a federation of physically distributed e-Infrastructures. Through a comprehensive service catalogue, the platform is able to offer an integrated user experience covering the full Machine Learning lifecycle, including model development (with dedicated interactive development environments), training (with GPU resources, annotation tools, experiment tracking, and federated learning support) and deployment (covering a wide range of deployment options all along the Cloud Continuum). The platform also provides tools for traceability and reproducibility of AI models, integrates with different Artificial Intelligence model providers, datasets and storage resources, allowing users to interact with the broader Machine Learning ecosystem. Finally, it is easily customizable to lower the adoption barrier by external communities.

</details>


### [74] [Efficient CPU-GPU Collaborative Inference for MoE-based LLMs on Memory-Limited Systems](https://arxiv.org/abs/2512.16473)
*En-Ming Huang,Li-Shang Lin,Chun-Yi Lee*

Main category: cs.DC

TL;DR: 论文提出CPU - GPU协同推理框架解决大语言模型推理难题，评估显示性能提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型计算需求高，现有混合专家模型内存需求大，传统卸载方法有延迟，影响推理性能。

Method: 提出含专家缓存机制的CPU - GPU协同推理框架，缓存命中加速推理，缓存未命中计算卸载到CPU。

Result: 框架评估显示性能有提升，展示了CPU - GPU协作在消费级系统单请求推理场景中最大化硬件利用率的潜力。

Conclusion: 所提框架能减少数据传输需求，提升推理速度，有效解决现有问题。

Abstract: Large Language Models (LLMs) have achieved impressive results across various tasks, yet their high computational demands pose deployment challenges, especially on consumer-grade hardware. Mixture of Experts (MoE) models provide an efficient solution through selective activation of parameter subsets, which reduces computation requirements. Despite this efficiency, state-of-the-art MoE models still require substantial memory beyond typical consumer GPU capacities. Traditional offloading methods that transfer model weights between CPU and GPU introduce latency, limiting inference performance. This paper presents a novel CPU-GPU collaborative inference framework that incorporates an expert caching mechanism on the GPU to reduce data transfer requirements and enable faster inference through cache hits. Computations are offloaded to CPU for efficient cache miss handling, which benefits from CPU multithreading optimizations. The evaluations of our framework demonstrate performance improvements and highlight the potential of CPU-GPU collaboration to maximize hardware utilization for single-request inference scenarios on consumer-grade systems. The implementation of our framework is available at https://github.com/elsa-lab/MoE-CPU-GPU-Collaborative-Inference.

</details>


### [75] [Delay-Aware Multi-Stage Edge Server Upgrade with Budget Constraint](https://arxiv.org/abs/2512.16792)
*Endar Suprih Wihidayat,Sieteng Soh,Kwan-Wu Chin,Duc-son Pham*

Main category: cs.DC

TL;DR: 提出M - ESU网络规划问题，考虑边缘服务器部署、升级和任务卸载决策，给出MILP和M - ESU/H两种解决方案，模拟显示M - ESU/H有良好性能和实用价值。


<details>
  <summary>Details</summary>
Motivation: 解决现有多接入边缘计算（MEC）系统多年多阶段升级的网络规划问题，最大化满足延迟要求的任务平均数量。

Method: 提出M - ESU问题框架，同时考虑多种约束条件；给出MILP模型和高效启发式算法M - ESU/H两种解决方案。

Result: 小网络中M - ESU/H计算结果距最优解误差在1.25%内且速度快；大网络中M - ESU/H相比其他方案任务满意度提升达21.57%。

Conclusion: M - ESU/H具有可扩展性和对长期MEC系统的实用价值。

Abstract: In this paper, the Multi-stage Edge Server Upgrade (M-ESU) is proposed as a new network planning problem, involving the upgrading of an existing multi-access edge computing (MEC) system through multiple stages (e.g., over several years). More precisely, the problem considers two key decisions: (i) whether to deploy additional edge servers or upgrade those already installed, and (ii) how tasks should be offloaded so that the average number of tasks that meet their delay requirement is maximized. The framework specifically involves: (i) deployment of new servers combined with capacity upgrades for existing servers, and (ii) the optimal task offloading to maximize the average number of tasks with a delay requirement. It also considers the following constraints: (i) budget per stage, (ii) server deployment and upgrade cost (in $) and cost depreciation rate, (iii) computation resource of servers, (iv) number of tasks and their growth rate (in %), and (v) the increase in task sizes and stricter delay requirements over time. We present two solutions: a Mixed Integer Linear Programming (MILP) model and an efficient heuristic algorithm (M-ESU/H). MILP yields the optimal solution for small networks, whereas M-ESU/H is used in large-scale networks. For small networks, the simulation results show that the solution computed by M-ESU/H is within 1.25% of the optimal solution while running several orders of magnitude faster. For large networks, M-ESU/H is compared against three alternative heuristic solutions that consider only server deployment, or giving priority to server deployment or upgrade. Our experiments show that M-ESU/H yields up to 21.57% improvement in task satisfaction under identical budget and demand growth conditions, confirming its scalability and practical value for long-term MEC systems.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [76] [Improved Lower Bounds for Privacy under Continual Release](https://arxiv.org/abs/2512.15981)
*Bardiya Aryanfard,Monika Henzinger,David Saulpic,A. R. Sricharan*

Main category: cs.DS

TL;DR: 研究差分隐私下持续发布不断演变数据集统计信息问题，给出插入式图问题和同时范数估计的多项式下界，允许乘法近似可降低误差，还给出项级设置下图问题误差乘积的多项式下界。


<details>
  <summary>Details</summary>
Motivation: 研究差分隐私下持续发布演变数据集统计信息的误差下界，探索不同更新方式和近似条件对误差的影响。

Method: 对插入式图问题、同时范数估计等进行分析，证明新的误差下界，通过问题归约等方法得出结果。

Result: 在事件级设置下，给出插入式图问题的多项式下界，指数提升部分问题的结果；允许乘法近似可使部分问题的加法误差降至多对数级；在项级设置下，给出图问题误差乘积的多项式下界。

Conclusion: 多项式加法误差在插入式设置下不可避免，允许乘法近似可降低误差，新的误差乘积下界推广了以往无乘法误差机制的结果。

Abstract: We study the problem of continually releasing statistics of an evolving dataset under differential privacy. In the event-level setting, we show the first polynomial lower bounds on the additive error for insertions-only graph problems such as maximum matching, degree histogram and $k$-core. This is an exponential improvement on the polylogarithmic lower bounds of Fichtenberger et al.[ESA 2021] for the former two problems, and are the first continual release lower bounds for the latter. Our results run counter to the intuition that the difference between insertions-only vs fully dynamic updates causes the gap between polylogarithmic and polynomial additive error. We show that for maximum matching and $k$-core, allowing small multiplicative approximations is what brings the additive error down to polylogarithmic.
  Beyond graph problems, our techniques also show that polynomial additive error is unavoidable for Simultaneous Norm Estimation in the insertions-only setting. When multiplicative approximations are allowed, we circumvent this lower bound by giving the first continual mechanism with polylogarithmic additive error under $(1+ζ)$ multiplicative approximations, for $ζ>0$, for estimating all monotone symmetric norms simultaneously.
  In the item-level setting, we show polynomial lower bounds on the product of the multiplicative and the additive error of continual mechanisms for a large range of graph problems. To the best of our knowledge, these are the first lower bounds for any differentially private continual release mechanism with multiplicative error. To obtain this, we prove a new lower bound on the product of multiplicative and additive error for 1-Way-Marginals, from which we reduce to continual graph problems. This generalizes the lower bounds of Hardt and Talwar[STOC 2010] and Bun et al.[STOC 2014] on the additive error for mechanisms with no multiplicative error.

</details>


### [77] [Instance Optimality in PageRank Centrality Estimation](https://arxiv.org/abs/2512.16087)
*Mikkel Thorup,Hanzhi Wang*

Main category: cs.DS

TL;DR: 研究自适应算法估计顶点PageRank中心性，证明其在特定图上实例最优，给出非最优反例。


<details>
  <summary>Details</summary>
Motivation: 研究自适应算法在估计顶点PageRank中心性时的性能和最优性。

Method: 研究简单经典算法的自适应变体，分析其在不同类型有向图中的性能。

Result: 算法在最大入度和出度至多为n的常数分数的图上实例最优，扩展到部分顶点度无界的图；在多数顶点度等于n的图上非实例最优。

Conclusion: 该自适应算法在特定稀疏图上具有实例最优性，但在某些图上不具备。

Abstract: We study an adaptive variant of a simple, classic algorithm for estimating a vertex's PageRank centrality within a constant relative error, with constant probability. We show that this algorithm is instance-optimal up to a polylogarithmic factor for any directed graph of order $n$ whose maximal in- and out-degrees are at most a constant fraction of $n$. The instance-optimality also extends to graphs in which up to a polylogarithmic number of vertices have unbounded degree, thereby covering all sparse graphs with $\widetilde{O}(n)$ edges. Finally, we provide a counterexample showing that the algorithm is not instance-optimal for graphs with degrees mostly equal to $n$.

</details>


### [78] [Conquering the Multiverse: The River Voting Method with Efficient Parallel Universe Tiebreaking](https://arxiv.org/abs/2512.16414)
*Jannes Malanowski*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Democracy relies on making collective decisions through voting. In addition, voting procedures have further applications, for example in the training of artificial intelligence. An essential criterion for determining the winner of a fair election is that all alternatives are treated equally: this is called neutrality. The established Ranked Pairs voting method cannot simultaneously guarantee neutrality and be computationally tractable for election with ties. River, the recently introduced voting method, shares desirable properties with Ranked Pairs and has further advantages, such as a new property related to resistance against manipulation. Both Ranked Pairs and River use a weighted margin graph to model the election. Ties in the election can lead to edges of equal margin. To order the edges in such a case, a tiebreaking scheme must be employed. Many tiebreaks violate neutrality or other important properties. A tiebreaking scheme that preserves neutrality is Parallel Universe Tiebreaking (PUT). Ranked Pairs with PUT is NP-hard to compute.
  The main result of this thesis shows that River with PUT can be computed in polynomial worst-case runtime: We can check whether an alternative is a River PUT winner, by running River with a specially constructed ordering of the edges. To construct this ordering, we introduce the semi-River diagram which contains the edges that can appear in any River diagram for some arbitrary tiebreak. On this diagram we can compute the River winners, by applying a variant of Prims algorithm per alternative. Additionally, we give an algorithm improve the previous naive runtime of River from $\mathcal{O}(n^4)$ to $\mathcal{O}(n^2 \log n)$, where n is the number of alternatives.

</details>


### [79] [Fully Dynamic Algorithms for Chamfer Distance](https://arxiv.org/abs/2512.16639)
*Gramoz Goranci,Shaofeng Jiang,Peter Kiss,Eva Szilagyi,Qiaoyuan Yang*

Main category: cs.DS

TL;DR: 本文研究全动态环境下计算Chamfer距离问题，提出首个动态算法，在真实数据集上表现有竞争力。


<details>
  <summary>Details</summary>
Motivation: Chamfer距离是点云常用相异度指标，在动态数据集上需重复评估，现有缺乏动态算法。

Method: 提出动态算法，将问题转化为近似最近邻搜索，结合标准ANN界限。

Result: 获得(1 + ε)-近似在$	ilde{O}(ε^{-d})$更新时间，$O(1/ε)$-近似在$	ilde{O}(d n^{ε^2} ε^{-4})$更新时间。

Conclusion: 算法在真实数据集上表现优于自然基线，具有竞争力。

Abstract: We study the problem of computing Chamfer distance in the fully dynamic setting, where two set of points $A, B \subset \mathbb{R}^{d}$, each of size up to $n$, dynamically evolve through point insertions or deletions and the goal is to efficiently maintain an approximation to $\mathrm{dist}_{\mathrm{CH}}(A,B) = \sum_{a \in A} \min_{b \in B} \textrm{dist}(a,b)$, where $\textrm{dist}$ is a distance measure. Chamfer distance is a widely used dissimilarity metric for point clouds, with many practical applications that require repeated evaluation on dynamically changing datasets, e.g., when used as a loss function in machine learning. In this paper, we present the first dynamic algorithm for maintaining an approximation of the Chamfer distance under the $\ell_p$ norm for $p \in \{1,2 \}$. Our algorithm reduces to approximate nearest neighbor (ANN) search with little overhead. Plugging in standard ANN bounds, we obtain $(1+ε)$-approximation in $\tilde{O}(ε^{-d})$ update time and $O(1/ε)$-approximation in $\tilde{O}(d n^{ε^2} ε^{-4})$ update time. We evaluate our method on real-world datasets and demonstrate that it performs competitively against natural baselines.

</details>


### [80] [Learning Confidence Ellipsoids and Applications to Robust Subspace Recovery](https://arxiv.org/abs/2512.16875)
*Chao Gao,Liren Shan,Vaidehi Srinivas,Aravindan Vijayaraghavan*

Main category: cs.DS

TL;DR: 研究高维任意分布置信椭球问题，提出多项式时间算法找到体积有近似保证的椭球，还给出计算复杂性结果并获鲁棒子空间恢复问题的首个有近似保证的多项式时间算法。


<details>
  <summary>Details</summary>
Motivation: 高维下获取非平凡体积近似因子在椭球条件数趋于无穷时为NP难问题，因此关注能否高效找到与有界条件数椭球相比有体积近似保证的置信椭球。

Method: 利用最小体积包围椭球的原始 - 对偶结构和几何Brascamp - Lieb不等式。

Result: 提出多项式时间算法找到的椭球体积在最佳β条件椭球体积的$O(β^{γd})$倍内，且覆盖至少$1 - O(α/γ)$的概率质量；给出计算复杂性结果表明指数中的依赖关系在常数范围内是必要的。

Conclusion: 获得了鲁棒子空间恢复问题在最坏情况下实例的首个有近似保证的多项式时间算法。

Abstract: We study the problem of finding confidence ellipsoids for an arbitrary distribution in high dimensions. Given samples from a distribution $D$ and a confidence parameter $α$, the goal is to find the smallest volume ellipsoid $E$ which has probability mass $\Pr_{D}[E] \ge 1-α$. Ellipsoids are a highly expressive class of confidence sets as they can capture correlations in the distribution, and can approximate any convex set. This problem has been studied in many different communities. In statistics, this is the classic minimum volume estimator introduced by Rousseeuw as a robust non-parametric estimator of location and scatter. However in high dimensions, it becomes NP-hard to obtain any non-trivial approximation factor in volume when the condition number $β$ of the ellipsoid (ratio of the largest to the smallest axis length) goes to $\infty$. This motivates the focus of our paper: can we efficiently find confidence ellipsoids with volume approximation guarantees when compared to ellipsoids of bounded condition number $β$?
  Our main result is a polynomial time algorithm that finds an ellipsoid $E$ whose volume is within a $O(β^{γd})$ multiplicative factor of the volume of best $β$-conditioned ellipsoid while covering at least $1-O(α/γ)$ probability mass for any $γ< α$. We complement this with a computational hardness result that shows that such a dependence seems necessary up to constants in the exponent. The algorithm and analysis uses the rich primal-dual structure of the minimum volume enclosing ellipsoid and the geometric Brascamp-Lieb inequality. As a consequence, we obtain the first polynomial time algorithm with approximation guarantees on worst-case instances of the robust subspace recovery problem.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [81] [Algorithmic Monetary Policies for Blockchain Participation Games](https://arxiv.org/abs/2512.16514)
*Diodato Ferraioli,Paolo Penna,Manvir Schneider,Carmine Ventre*

Main category: cs.GT

TL;DR: 提出算法货币政策框架应对区块链代币经济短长期目标权衡，分析不同代理行为下均衡，讨论虚拟权益及其初始分布影响。


<details>
  <summary>Details</summary>
Motivation: 解决区块链代币经济中短期绩效激励与长期去中心化目标难以对齐的问题。

Method: 提出算法货币政策框架，分析在近视（短期效用最大化）和有远见（多轮规划）两种代理行为下的均衡，探讨虚拟权益作为替代方法。

Result: 对于近视代理，以绩效为中心的政策有集中化风险；有远见的代理能实现稳定去中心化，但代币价值有一定波动。初始虚拟权益分配对长期结果有重要影响。

Conclusion: 政策必须间接管理去中心化。

Abstract: A central challenge in blockchain tokenomics is aligning short-term performance incentives with long-term decentralization goals. We propose a framework for algorithmic monetary policies that navigates this tradeoff in repeated participation games. Agents, characterized by type (capability) and stake, choose to participate or abstain at each round; the policy (probabilistically) selects high-type agents for task execution (maximizing throughput) while distributing rewards to sustain decentralization. We analyze equilibria under two agent behaviors: myopic (short-term utility maximization) and foresighted (multi-round planning). For myopic agents, performance-centric policies risk centralization, but foresight enables stable decentralization with some volatility to the token value. We further discuss virtual stake--a hybrid of type and stake--as an alternative approach. We show that the initial virtual stake distribution critically impacts long-term outcomes, suggesting that policies must indirectly manage decentralization.

</details>


### [82] [Online Resource Allocation via Static Bundle Pricing](https://arxiv.org/abs/2512.16570)
*Dimitris Fotakis,Charalampos Platanos,Thanos Tolias*

Main category: cs.GT

TL;DR: 本文研究带互补性的在线资源分配问题，提出统一技术，给出静态匿名捆绑定价机制，得到竞争比结果并给出信息论下界。


<details>
  <summary>Details</summary>
Motivation: 在线资源分配需在不完全了解未来请求的情况下分配资源，标准物品定价和现有静态捆绑定价机制存在局限，且研究买家估值有互补性的环境。

Method: 针对三个领域开发统一技术，得到静态和匿名捆绑定价机制。

Result: 在不同设定下得到相应的竞争比机制，如在d - 单心思设定下得到$O(d^{1/B})$-竞争机制，还得到信息论下界。

Conclusion: 所提机制性能随物品容量指数提升，揭示与极值组合问题的联系。

Abstract: Online Resource Allocation addresses the problem of efficiently allocating limited resources to buyers with incomplete knowledge of future requests. In our setting, buyers arrive sequentially demanding a set of items, each with a value drawn from a known distribution. We study environments where buyers' valuations exhibit complementarities. In such settings, standard item-pricing mechanisms fail to leverage item multiplicities, while existing static bundle-pricing mechanisms rely on problem-specific arguments that do not generalize.
  We develop a unified technique for online resource allocation with complementarities for three domains: (i) single-minded combinatorial auctions with maximum bundle size $d$, (ii) general single-minded combinatorial auctions, and (iii) a graph-based routing model in which buyers request to route a unit of flow from a source node $s$ to a target node $t$ in a capacitated graph. Our approach yields static and anonymous bundle-pricing mechanisms whose performance improves exponentially with item capacities. For the $d$-single-minded setting with minimum item capacity $B$, we obtain an $O(d^{1/B})$-competitive mechanism, recovering the known $O(d)$ bound for unit capacities ($B=1$) and achieving exponentially better guarantees as capacities grow. For general single-minded combinatorial auctions and the graph-routing model, we obtain $O(m^{1/(B+1)})$-competitive mechanisms, where $m$ is the number of items.
  We complement these results with information-theoretic lower bounds. We show that no online algorithm can achieve a competitive ratio better than $Ω((m/\ln m)^{1/(B+2)})$ in the general single-minded setting and $Ω((d/\ln d)^{1/(B+1)})$ in the $d$-single-minded setting. In doing so, we reveal a deep connection to the extremal combinatorics problem of determining the maximum number of qualitatively independent partitions of a ground set.

</details>


### [83] [On the Edge of Core (Non-)Emptiness: An Automated Reasoning Approach to Approval-Based Multi-Winner Voting](https://arxiv.org/abs/2512.16895)
*Ratip Emin Berker,Emanuel Tewolde,Vincent Conitzer,Mingyu Guo,Marijn Heule,Lirong Xia*

Main category: cs.GT

TL;DR: 本文研究多胜者投票中核稳定委员会存在性问题，提出基于混合整数线性规划的方法，有计算优势并获特殊情况存在性结果，还揭示核稳定性与其他属性关系。


<details>
  <summary>Details</summary>
Motivation: 在选民对候选人进行赞成或反对投票的设定下，核稳定委员会是否总是存在仍是主要开放问题，因此开展相关研究。

Method: 基于混合整数线性规划的方法，与计算社会选择中流行的基于SAT的方法对比。

Result: 方法能产生与选民数量无关的特定候选人数的证明，获得特殊情况下核稳定性问题新的存在性结果，揭示核稳定性与其他理想属性的未知关系。

Conclusion: 基于混合整数线性规划的方法在解决核稳定委员会存在性问题上有优势，且能推动对核稳定性相关性质的研究。

Abstract: Core stability is a natural and well-studied notion for group fairness in multi-winner voting, where the task is to select a committee from a pool of candidates. We study the setting where voters either approve or disapprove of each candidate; here, it remains a major open problem whether a core-stable committee always exists. In this work, we develop an approach based on mixed-integer linear programming for deciding whether and when core-stable committees are guaranteed to exist. In contrast to SAT-based approaches popular in computational social choice, our method can produce proofs for a specific number of candidates independent of the number of voters. In addition to these computational gains, our program lends itself to a novel duality-based reformulation of the core stability problem, from which we obtain new existence results in special cases. Further, we use our framework to reveal previously unknown relationships between core stability and other desirable properties, such as notions of priceability.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [84] [On Recommending Category: A Cascading Approach](https://arxiv.org/abs/2512.16033)
*Qihao Wang,Pritom Saha Akash,Varvara Kollia,Kevin Chen-Chuan Chang,Biwei Jiang,Vadim Von Brzeski*

Main category: cs.IR

TL;DR: 本文提出用于类别级推荐的CCRec模型，实验显示其优于为物品级推荐设计的方法。


<details>
  <summary>Details</summary>
Motivation: 现有类别级偏好预测多通过物品级模型实现，忽略了物品级和类别级推荐的关键差异，需新模型。

Method: 提出带变分自编码器（VAE）的级联类别推荐器（CCRec）模型，用VAE编码物品级信息进行类别级推荐。

Result: 实验表明CCRec模型优于为物品级推荐设计的方法。

Conclusion: CCRec模型在类别级推荐上有优势，能解决现有方法忽略关键差异的问题。

Abstract: Recommendation plays a key role in e-commerce, enhancing user experience and boosting commercial success. Existing works mainly focus on recommending a set of items, but online e-commerce platforms have recently begun to pay attention to exploring users' potential interests at the category level. Category-level recommendation allows e-commerce platforms to promote users' engagements by expanding their interests to different types of items. In addition, it complements item-level recommendations when the latter becomes extremely challenging for users with little-known information and past interactions. Furthermore, it facilitates item-level recommendations in existing works. The predicted category, which is called intention in those works, aids the exploration of item-level preference. However, such category-level preference prediction has mostly been accomplished through applying item-level models. Some key differences between item-level recommendations and category-level recommendations are ignored in such a simplistic adaptation. In this paper, we propose a cascading category recommender (CCRec) model with a variational autoencoder (VAE) to encode item-level information to perform category-level recommendations. Experiments show the advantages of this model over methods designed for item-level recommendations.

</details>


### [85] [The Evolution of Reranking Models in Information Retrieval: From Heuristic Methods to Large Language Models](https://arxiv.org/abs/2512.16236)
*Tejul Pandit,Sakshi Mahendru,Meet Raval,Dhvani Upadhyay*

Main category: cs.IR

TL;DR: 本文全面调研信息检索重排序方法，涵盖历史技术、神经网络架构、效率提升技术及大语言模型集成，分析不同策略特点。


<details>
  <summary>Details</summary>
Motivation: 梳理不断变化的重排序器格局，清晰展示重排序方法的进展。

Method: 按时间顺序回顾重排序技术历史，分析神经网络架构，研究提升效率技术，探索大语言模型集成。

Result: 明确了不同重排序策略的基本思想、相对有效性、计算特征和实际权衡。

Conclusion: 对各种重排序范式进行结构化综合，突出其原理及优缺点。

Abstract: Reranking is a critical stage in contemporary information retrieval (IR) systems, improving the relevance of the user-presented final results by honing initial candidate sets. This paper is a thorough guide to examine the changing reranker landscape and offer a clear view of the advancements made in reranking methods. We present a comprehensive survey of reranking models employed in IR, particularly within modern Retrieval Augmented Generation (RAG) pipelines, where retrieved documents notably influence output quality.
  We embark on a chronological journey through the historical trajectory of reranking techniques, starting with foundational approaches, before exploring the wide range of sophisticated neural network architectures such as cross-encoders, sequence-generation models like T5, and Graph Neural Networks (GNNs) utilized for structural information. Recognizing the computational cost of advancing neural rerankers, we analyze techniques for enhancing efficiency, notably knowledge distillation for creating competitive, lighter alternatives. Furthermore, we map the emerging territory of integrating Large Language Models (LLMs) in reranking, examining novel prompting strategies and fine-tuning tactics. This survey seeks to elucidate the fundamental ideas, relative effectiveness, computational features, and real-world trade-offs of various reranking strategies. The survey provides a structured synthesis of the diverse reranking paradigms, highlighting their underlying principles and comparative strengths and weaknesses.

</details>


### [86] [From Flows to Functions: Macroscopic Behavioral Fingerprinting of IoT Devices via Network Services](https://arxiv.org/abs/2512.16348)
*Shayan Azizi,Norihiro Okui,Masataka Nakahara,Ayumu Kubota,Hassan Habibi Gharakheili*

Main category: cs.IR

TL;DR: 提出一种宏观、轻量级且可解释的物联网设备行为指纹识别方法，基于网络服务使用模式进行设备识别，并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于网络流量分析的设备识别方法存在计算成本高、对测量误差敏感和推理不透明等问题，需要更优方案。

Method: 提出基于物联网设备长期使用的网络服务的服务级指纹概念，推导通用表示方法，开发提取服务级指纹的程序，并在实验室测试中应用和评估。

Result: 通过分析13种消费级物联网设备流量和1000万条IPFIX流记录，验证了服务级指纹在封闭集和开放集场景下用于设备识别的有效性。

Conclusion: 所提的基于网络服务的服务级指纹方法可有效识别物联网设备，是一种宏观、轻量级且可解释的替代方案。

Abstract: Identifying devices such as cameras, printers, voice assistants, or health monitoring sensors, collectively known as the Internet of Things (IoT), within a network is a critical operational task, particularly to manage the cyber risks they introduce. While behavioral fingerprinting based on network traffic analysis has shown promise, most existing approaches rely on machine learning (ML) techniques applied to fine-grained features of short-lived traffic units (packets and/or flows). These methods tend to be computationally expensive, sensitive to traffic measurement errors, and often produce opaque inferences. In this paper, we propose a macroscopic, lightweight, and explainable alternative to behavioral fingerprinting focusing on the network services (e.g., TCP/80, UDP/53) that IoT devices use to perform their intended functions over extended periods. Our contributions are threefold. (1) We demonstrate that IoT devices exhibit stable and distinguishable patterns in their use of network services over a period of time. We formalize the notion of service-level fingerprints and derive a generalized method to represent network behaviors using a configurable granularity parameter. (2) We develop a procedure to extract service-level fingerprints, apply it to traffic from 13 consumer IoT device types in a lab testbed, and evaluate the resulting representations in terms of their convergence and recurrence properties. (3) We validate the efficacy of service-level fingerprints for device identification in closed-set and open-set scenarios. Our findings are based on a large dataset comprising about 10 million IPFIX flow records collected over a 1.5-year period.

</details>


### [87] [Introducing ORKG ASK: an AI-driven Scholarly Literature Search and Exploration System Taking a Neuro-Symbolic Approach](https://arxiv.org/abs/2512.16425)
*Allard Oelen,Mohamad Yaser Jaradeh,Sören Auer*

Main category: cs.IR

TL;DR: 随着学术文献量增长，借助生成式AI，介绍神经符号方法的文献搜索系统ASK，评估显示其易用且受用户认可。


<details>
  <summary>Details</summary>
Motivation: 解决学术文献增多导致查找相关文献困难的问题，利用生成式AI尤其是大语言模型带来的新可能。

Method: 采用神经符号方法，结合向量搜索、大语言模型和知识图谱，用检索增强生成（RAG）方法自动提取关键信息和生成答案。

Result: 系统易用，用户使用时总体满意。

Conclusion: ASK系统能为研究人员查找相关学术文献提供有效支持。

Abstract: As the volume of published scholarly literature continues to grow, finding relevant literature becomes increasingly difficult. With the rise of generative Artificial Intelligence (AI), and particularly Large Language Models (LLMs), new possibilities emerge to find and explore literature. We introduce ASK (Assistant for Scientific Knowledge), an AI-driven scholarly literature search and exploration system that follows a neuro-symbolic approach. ASK aims to provide active support to researchers in finding relevant scholarly literature by leveraging vector search, LLMs, and knowledge graphs. The system allows users to input research questions in natural language and retrieve relevant articles. ASK automatically extracts key information and generates answers to research questions using a Retrieval-Augmented Generation (RAG) approach. We present an evaluation of ASK, assessing the system's usability and usefulness. Findings indicate that the system is user-friendly and users are generally satisfied while using the system.

</details>


### [88] [InfoDCL: Informative Noise Enhanced Diffusion Based Contrastive Learning](https://arxiv.org/abs/2512.16576)
*Xufeng Liang,Zhida Qin,Chong Zhang,Tianyu Huang,Gangyi Ding*

Main category: cs.IR

TL;DR: 提出InfoDCL对比学习框架，用单步扩散过程生成真实用户偏好，构建协作训练目标策略，推理时用多层GCN整合高阶共现信息，实验显示显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习推荐方法通过随机扰动交互图构建稀疏视图，因推荐数据稀疏只能捕捉有限语义信息，缺乏真实用户偏好。

Method: 提出InfoDCL框架，采用单步扩散过程结合噪声与辅助语义信息生成信号并输入标准扩散过程；构建协作训练目标策略；推理时用多层GCN整合高阶共现信息。

Result: 在五个真实数据集上实验，InfoDCL显著优于现有方法。

Conclusion: InfoDCL是提升推荐性能的有效方案，为对比学习框架中的扩散方法应用提供新范例。

Abstract: Contrastive learning has demonstrated promising potential in recommender systems. Existing methods typically construct sparser views by randomly perturbing the original interaction graph, as they have no idea about the authentic user preferences. Owing to the sparse nature of recommendation data, this paradigm can only capture insufficient semantic information. To address the issue, we propose InfoDCL, a novel diffusion-based contrastive learning framework for recommendation. Rather than injecting randomly sampled Gaussian noise, we employ a single-step diffusion process that integrates noise with auxiliary semantic information to generate signals and feed them to the standard diffusion process to generate authentic user preferences as contrastive views. Besides, based on a comprehensive analysis of the mutual influence between generation and preference learning in InfoDCL, we build a collaborative training objective strategy to transform the interference between them into mutual collaboration. Additionally, we employ multiple GCN layers only during inference stage to incorporate higher-order co-occurrence information while maintaining training efficiency. Extensive experiments on five real-world datasets demonstrate that InfoDCL significantly outperforms state-of-the-art methods. Our InfoDCL offers an effective solution for enhancing recommendation performance and suggests a novel paradigm for applying diffusion method in contrastive learning frameworks.

</details>


### [89] [Microsoft Academic Graph Information Retrieval for Research Recommendation and Assistance](https://arxiv.org/abs/2512.16661)
*Jacob Reiss,Shikshya Shiwakoti,Samuel Goldsmith,Ujjwal Pandit*

Main category: cs.IR

TL;DR: 提出基于注意力的子图检索器，结合GNN和大模型应对科研文献过滤挑战。


<details>
  <summary>Details</summary>
Motivation: 当今科研文献获取易但筛选难，需有效方法筛选海量研究。

Method: 提出Attention-Based Subgraph Retriever，用基于注意力的剪枝提取子图，再传给大语言模型进行知识推理。

Result: 未提及

Conclusion: 未提及

Abstract: In today's information-driven world, access to scientific publications has become increasingly easy. At the same time, filtering through the massive volume of available research has become more challenging than ever. Graph Neural Networks (GNNs) and graph attention mechanisms have shown strong effectiveness in searching large-scale information databases, particularly when combined with modern large language models. In this paper, we propose an Attention-Based Subgraph Retriever, a GNN-as-retriever model that applies attention-based pruning to extract a refined subgraph, which is then passed to a large language model for advanced knowledge reasoning.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [90] [DiscoverDCP: A Data-Driven Approach for Construction of Disciplined Convex Programs via Symbolic Regression](https://arxiv.org/abs/2512.15721)
*Sveinung Myhre*

Main category: cs.LG

TL;DR: 提出DiscoverDCP框架，结合符号回归与DCP规则集进行系统识别，可构建全局凸模型用于安全关键任务。


<details>
  <summary>Details</summary>
Motivation: 解决传统后验凸性验证计算复杂的问题，以及发现比传统固定参数凸表达式更优的模型。

Method: 提出DiscoverDCP框架，使发现的候选模型表达式遵循DCP组合规则。

Result: 能够发现具有更宽松和准确函数形式的凸替代模型。

Conclusion: 该方法可产生适用于安全关键控制和优化任务的可解释、可验证且灵活的凸模型。

Abstract: We propose DiscoverDCP, a data-driven framework that integrates symbolic regression with the rule sets of Disciplined Convex Programming (DCP) to perform system identification. By enforcing that all discovered candidate model expressions adhere to DCP composition rules, we ensure that the output expressions are globally convex by construction, circumventing the computationally intractable process of post-hoc convexity verification. This approach allows for the discovery of convex surrogates that exhibit more relaxed and accurate functional forms than traditional fixed-parameter convex expressions (e.g., quadratic functions). The proposed method produces interpretable, verifiable, and flexible convex models suitable for safety-critical control and optimization tasks.

</details>


### [91] [Techno-economic optimization of a heat-pipe microreactor, part I: theory and cost optimization](https://arxiv.org/abs/2512.16032)
*Paul Seurin,Dean Price,Luis Nunez*

Main category: cs.LG

TL;DR: 提出统一几何设计优化法，考虑技术经济因素，用代理模型和强化学习优化微型反应堆平准化发电成本，使成本降低超57%。


<details>
  <summary>Details</summary>
Motivation: 微型反应堆存在规模不经济和财务可行性问题，需综合经济和物理分析进行早期设计迭代。

Method: 生成随机样本训练高斯过程和多层感知器等代理模型，部署在基于强化学习的优化框架中，对平准化发电成本进行优化并设置多项约束。

Result: 运行维护和资本成本是平准化发电成本的主要因素，通过改变设计参数，两种情况的成本均降低超57%。

Conclusion: 目前正追求燃料和热管性能的全面集成与多目标优化，以充分理解约束和成本性能的相互作用。

Abstract: Microreactors, particularly heat-pipe microreactors (HPMRs), are compact, transportable, self-regulated power systems well-suited for access-challenged remote areas where costly fossil fuels dominate. However, they suffer from diseconomies of scale, and their financial viability remains unconvincing. One step in addressing this shortcoming is to design these reactors with comprehensive economic and physics analyses informing early-stage design iteration. In this work, we present a novel unifying geometric design optimization approach that accounts for techno-economic considerations. We start by generating random samples to train surrogate models, including Gaussian processes (GPs) and multi-layer perceptrons (MLPs). We then deploy these surrogates within a reinforcement learning (RL)-based optimization framework to optimize the levelized cost of electricity (LCOE), all the while imposing constraints on the fuel lifetime, shutdown margin (SDM), peak heat flux, and rod-integrated peaking factor. We study two cases: one in which the axial reflector cost is very high, and one in which it is inexpensive. We found that the operation and maintenance and capital costs are the primary contributors to the overall LCOE particularly the cost of the axial reflectors (for the first case) and the control drum materials. The optimizer cleverly changes the design parameters so as to minimize one of them while still satisfying the constraints, ultimately reducing the LCOE by more than 57% in both instances. A comprehensive integration of fuel and HP performance with multi-objective optimization is currently being pursued to fully understand the interaction between constraints and cost performance.

</details>


### [92] [Hybrid Quantum-Classical Ensemble Learning for S\&P 500 Directional Prediction](https://arxiv.org/abs/2512.15738)
*Abraham Itzhak Weinberg*

Main category: cs.LG

TL;DR: 提出结合量子情感分析、决策变压器架构和战略模型选择的混合集成框架，实现60.14%的标准普尔500指数预测准确率，优于单个模型，有实际交易潜力。


<details>
  <summary>Details</summary>
Motivation: 金融市场预测具有挑战性，多数模型精度难以突破，需提升定向准确率。

Method: 引入混合集成框架，结合量子情感分析、决策变压器架构和战略模型选择，并进行架构多样性、增强情感分析和智能过滤。

Result: 在标准普尔500指数预测上达到60.14%的定向准确率，优于单个模型；评估结果经统计检验显著；初步回测夏普比率好于买入持有策略。

Conclusion: 该框架有效提升金融市场预测的准确性，有实际交易应用潜力。

Abstract: Financial market prediction is a challenging application of machine learning, where even small improvements in directional accuracy can yield substantial value. Most models struggle to exceed 55--57\% accuracy due to high noise, non-stationarity, and market efficiency. We introduce a hybrid ensemble framework combining quantum sentiment analysis, Decision Transformer architecture, and strategic model selection, achieving 60.14\% directional accuracy on S\&P 500 prediction, a 3.10\% improvement over individual models.
  Our framework addresses three limitations of prior approaches. First, architecture diversity dominates dataset diversity: combining different learning algorithms (LSTM, Decision Transformer, XGBoost, Random Forest, Logistic Regression) on the same data outperforms training identical architectures on multiple datasets (60.14\% vs.\ 52.80\%), confirmed by correlation analysis ($r>0.6$ among same-architecture models). Second, a 4-qubit variational quantum circuit enhances sentiment analysis, providing +0.8\% to +1.5\% gains per model. Third, smart filtering excludes weak predictors (accuracy $<52\%$), improving ensemble performance (Top-7 models: 60.14\% vs.\ all 35 models: 51.2\%).
  We evaluate on 2020--2023 market data across seven instruments, covering diverse regimes including the COVID-19 crash and inflation-driven correction. McNemar's test confirms statistical significance ($p<0.05$). Preliminary backtesting with confidence-based filtering (6+ model consensus) yields a Sharpe ratio of 1.2 versus buy-and-hold's 0.8, demonstrating practical trading potential.

</details>


### [93] [CauSTream: Causal Spatio-Temporal Representation Learning for Streamflow Forecasting](https://arxiv.org/abs/2512.16046)
*Shu Wan,Reepal Shah,John Sabo,Huan Liu,K. Selçuk Candan*

Main category: cs.LG

TL;DR: 提出CauStream框架用于因果时空径流预测，在多个流域和预测期表现优于现有方法，还能学习符合领域知识的因果图。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在径流预测中忽视物理过程，缺乏可解释性和泛化能力，现有因果学习方法依赖固定因果图，无法适应数据。

Method: 提出CauStream框架，联合学习径流因果图和路由图，并建立非参数设置下因果结构的可识别性条件。

Result: 在三个美国主要流域的三个预测期评估，模型始终优于先前的先进方法，预测窗口越长性能优势越大，且学习到的因果图与领域知识相符。

Conclusion: CauStream为因果时空建模提供了原则性基础，有潜力扩展到广泛的科学和环境应用中。

Abstract: Streamflow forecasting is crucial for water resource management and risk mitigation. While deep learning models have achieved strong predictive performance, they often overlook underlying physical processes, limiting interpretability and generalization. Recent causal learning approaches address these issues by integrating domain knowledge, yet they typically rely on fixed causal graphs that fail to adapt to data. We propose CauStream, a unified framework for causal spatiotemporal streamflow forecasting. CauSTream jointly learns (i) a runoff causal graph among meteorological forcings and (ii) a routing graph capturing dynamic dependencies across stations. We further establish identifiability conditions for these causal structures under a nonparametric setting. We evaluate CauSTream on three major U.S. river basins across three forecasting horizons. The model consistently outperforms prior state-of-the-art methods, with performance gaps widening at longer forecast windows, indicating stronger generalization to unseen conditions. Beyond forecasting, CauSTream also learns causal graphs that capture relationships among hydrological factors and stations. The inferred structures align closely with established domain knowledge, offering interpretable insights into watershed dynamics. CauSTream offers a principled foundation for causal spatiotemporal modeling, with the potential to extend to a wide range of scientific and environmental applications.

</details>


### [94] [SHARe-KAN: Holographic Vector Quantization for Memory-Bound Inference](https://arxiv.org/abs/2512.15742)
*Jeff Smith*

Main category: cs.LG

TL;DR: 传统剪枝对Vision KANs无效，提出SHARe - KAN框架与LUTHAM编译器，实现88倍运行时内存缩减并保持精度，工作负载脱离DRAM带宽限制。


<details>
  <summary>Details</summary>
Motivation: Kolmogorov - Arnold Networks存在内存墙问题，传统剪枝在Vision KANs中失效，阻碍其在内存受限环境部署。

Method: 提出SHARe - KAN框架，利用Gain - Shape - Bias向量量化利用功能冗余并保留密集拓扑；结合LUTHAM硬件感知编译器进行静态内存规划。

Result: 实现88倍运行时内存缩减（从1.13 GB到12.91 MB），在PASCAL VOC上与未压缩基准精度匹配，在NVIDIA Ampere架构上L2缓存驻留率超90%。

Conclusion: 所提方法能有效解决Vision KANs的内存问题，使工作负载脱离基于样条架构固有的DRAM带宽限制。

Abstract: Kolmogorov-Arnold Networks (KANs) face a fundamental memory wall: their learned basis functions create parameter counts that impose extreme bandwidth demands, hindering deployment in memory-constrained environments. We show that Vision KANs exhibit a holographic topology, where information is distributed across the interference of splines rather than localized to specific edges. Consequently, traditional pruning fails (10% sparsity degrades mAP from 85.23% to 45%, a $\sim$40-point drop). To address this, we present SHARe-KAN, a framework utilizing Gain-Shape-Bias Vector Quantization to exploit functional redundancy while preserving the dense topology. Coupled with LUTHAM, a hardware-aware compiler with static memory planning, we achieve $88\times$ runtime memory reduction (1.13 GB $\to$ 12.91 MB) and match uncompressed baseline accuracy on PASCAL VOC. Profiling on NVIDIA Ampere architecture confirms $>90\%$ L2 cache residency, demonstrating that the workload is decoupled from DRAM bandwidth constraints inherent to spline-based architectures.

</details>


### [95] [How Do Graph Signals Affect Recommendation: Unveiling the Mystery of Low and High-Frequency Graph Signals](https://arxiv.org/abs/2512.15744)
*Feng Liu,Hao Cang,Huanhuan Yuan,Jiaqing Fan,Yongjing Hao,Fuzhen Zhuang,Guanfeng Liu,Pengpeng Zhao*

Main category: cs.LG

TL;DR: 本文研究图信号对推荐性能的影响，证明高低频信号作用等价，提出频率信号缩放器和空间翻转方法，实验验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究未明确高低频图信号在推荐中的作用，本文旨在填补这一空白。

Method: 理论证明高低频图信号在推荐任务中作用等价，提出频率信号缩放器调整用户 - 项目对的平滑度，引入空间翻转方法恢复图嵌入表达能力。

Result: 证明高低频图信号单独使用也能有效推荐，在四个公开数据集上的实验验证了方法有效性。

Conclusion: 高低频图信号在推荐中作用等价，提出的频率信号缩放器和空间翻转方法可提升推荐效果。

Abstract: Spectral graph neural networks (GNNs) are highly effective in modeling graph signals, with their success in recommendation often attributed to low-pass filtering. However, recent studies highlight the importance of high-frequency signals. The role of low-frequency and high-frequency graph signals in recommendation remains unclear. This paper aims to bridge this gap by investigating the influence of graph signals on recommendation performance. We theoretically prove that the effects of low-frequency and high-frequency graph signals are equivalent in recommendation tasks, as both contribute by smoothing the similarities between user-item pairs. To leverage this insight, we propose a frequency signal scaler, a plug-and-play module that adjusts the graph signal filter function to fine-tune the smoothness between user-item pairs, making it compatible with any GNN model. Additionally, we identify and prove that graph embedding-based methods cannot fully capture the characteristics of graph signals. To address this limitation, a space flip method is introduced to restore the expressive power of graph embeddings. Remarkably, we demonstrate that either low-frequency or high-frequency graph signals alone are sufficient for effective recommendations. Extensive experiments on four public datasets validate the effectiveness of our proposed methods. Code is avaliable at https://github.com/mojosey/SimGCF.

</details>


### [96] [Data Valuation for LLM Fine-Tuning: Efficient Shapley Value Approximation via Language Model Arithmetic](https://arxiv.org/abs/2512.15765)
*Mélissa Tamine,Otmane Sakhi,Benjamin Heymann*

Main category: cs.LG

TL;DR: 分析大语言模型数据估值问题，指出计算Shapley值挑战大，而DPO训练的LLMs可简化计算。


<details>
  <summary>Details</summary>
Motivation: 解决数据所有者如何制定数据管理策略、如何合作训练模型并合理分配收益的问题，以及克服计算Shapley值成本高的挑战。

Method: 利用Direct Preference Optimization (DPO)特定数学结构来计算Shapley值。

Result: 证明DPO训练的LLMs能显著简化Shapley值计算，使其可扩展。

Conclusion: 该发现为数据估值与大语言模型结合开启许多应用可能。

Abstract: Data is a critical asset for training large language models (LLMs), alongside compute resources and skilled workers. While some training data is publicly available, substantial investment is required to generate proprietary datasets, such as human preference annotations or to curate new ones from existing sources. As larger datasets generally yield better model performance, two natural questions arise. First, how can data owners make informed decisions about curation strategies and data sources investment? Second, how can multiple data owners collaboratively pool their resources to train superior models while fairly distributing the benefits? This problem, data valuation, which is not specific to large language models, has been addressed by the machine learning community through the lens of cooperative game theory, with the Shapley value being the prevalent solution concept. However, computing Shapley values is notoriously expensive for data valuation, typically requiring numerous model retrainings, which can become prohibitive for large machine learning models. In this work, we demonstrate that this computational challenge is dramatically simplified for LLMs trained with Direct Preference Optimization (DPO). We show how the specific mathematical structure of DPO enables scalable Shapley value computation. We believe this observation unlocks many applications at the intersection of data valuation and large language models.

</details>


### [97] [LLaDA2.0: Scaling Up Diffusion Language Models to 100B](https://arxiv.org/abs/2512.15745)
*Tiwei Bie,Maosong Cao,Kun Chen,Lun Du,Mingliang Gong,Zhuochen Gong,Yanmei Gu,Jiaqi Hu,Zenan Huang,Zhenzhong Lan,Chengxi Li,Chongxuan Li,Jianguo Li,Zehuan Li,Huabin Liu,Ling Liu,Guoshan Lu,Xiaocheng Lu,Yuxin Ma,Jianfeng Tan,Lanning Wei,Ji-Rong Wen,Yipeng Xing,Xiaolu Zhang,Junbo Zhao,Da Zheng,Jun Zhou,Junlin Zhou,Zhanchao Zhou,Liwang Zhu,Yihong Zhuang*

Main category: cs.LG

TL;DR: 本文提出LLaDA2.0，通过系统转换自回归模型得到参数达100B的离散扩散大语言模型，建立新部署范式，开源两个变体模型。


<details>
  <summary>Details</summary>
Motivation: 避免从头训练的高成本，建立前沿规模部署的新范式。

Method: 采用新颖的3阶段基于块级WSD的训练方案将预训练自回归模型转换为dLLM，结合SFT和DPO进行后训练对齐。

Result: 得到LLaDA2.0 - mini (16B)和LLaDA2.0 - flash (100B)两个指令调优的MoE变体模型。

Conclusion: 模型保留并行解码优势，在前沿规模上有出色性能和效率，且已开源。

Abstract: This paper presents LLaDA2.0 -- a tuple of discrete diffusion large language models (dLLM) scaling up to 100B total parameters through systematic conversion from auto-regressive (AR) models -- establishing a new paradigm for frontier-scale deployment. Instead of costly training from scratch, LLaDA2.0 upholds knowledge inheritance, progressive adaption and efficiency-aware design principle, and seamless converts a pre-trained AR model into dLLM with a novel 3-phase block-level WSD based training scheme: progressive increasing block-size in block diffusion (warm-up), large-scale full-sequence diffusion (stable) and reverting back to compact-size block diffusion (decay). Along with post-training alignment with SFT and DPO, we obtain LLaDA2.0-mini (16B) and LLaDA2.0-flash (100B), two instruction-tuned Mixture-of-Experts (MoE) variants optimized for practical deployment. By preserving the advantages of parallel decoding, these models deliver superior performance and efficiency at the frontier scale. Both models were open-sourced.

</details>


### [98] [Introduction to Symbolic Regression in the Physical Sciences](https://arxiv.org/abs/2512.15920)
*Deaglan J. Bartlett,Harry Desmond,Pedro G. Ferreira,Gabriel Kronberger*

Main category: cs.LG

TL;DR: 本文介绍物理科学中符号回归特刊，概述SR概念基础、对比传统方法、总结方法考量、指出挑战与新兴方向，展示其进展与重要性。


<details>
  <summary>Details</summary>
Motivation: 受2025年4月皇家学会讨论会议启发，介绍物理科学中符号回归特刊。

Method: 在引言综述中概述SR概念基础，与传统回归方法对比，调查其在物理科学主要用例，总结方法考量。

Result: 明确SR在物理科学应用范围，指出其面临的挑战如可扩展性、抗噪性等。

Conclusion: 特刊论文展示了SR在物理科学领域的加速进展和日益增长的相关性。

Abstract: Symbolic regression (SR) has emerged as a powerful method for uncovering interpretable mathematical relationships from data, offering a novel route to both scientific discovery and efficient empirical modelling. This article introduces the Special Issue on Symbolic Regression for the Physical Sciences, motivated by the Royal Society discussion meeting held in April 2025. The contributions collected here span applications from automated equation discovery and emergent-phenomena modelling to the construction of compact emulators for computationally expensive simulations.
  The introductory review outlines the conceptual foundations of SR, contrasts it with conventional regression approaches, and surveys its main use cases in the physical sciences, including the derivation of effective theories, empirical functional forms and surrogate models. We summarise methodological considerations such as search-space design, operator selection, complexity control, feature selection, and integration with modern AI approaches. We also highlight ongoing challenges, including scalability, robustness to noise, overfitting and computational complexity. Finally we emphasise emerging directions, particularly the incorporation of symmetry constraints, asymptotic behaviour and other theoretical information. Taken together, the papers in this Special Issue illustrate the accelerating progress of SR and its growing relevance across the physical sciences.

</details>


### [99] [A Unified Generative-Predictive Framework for Deterministic Inverse Design](https://arxiv.org/abs/2512.15746)
*Reza T. Batley,Sourav Saha*

Main category: cs.LG

TL;DR: 本文提出Janus框架解决异质材料微观结构逆设计难题，在MNIST数据集和热导率标签微观结构逆设计中验证其有效性，能实现实时、物理信息驱动的逆微观结构生成。


<details>
  <summary>Details</summary>
Motivation: 异质材料微观结构逆设计问题本质上是病态且计算成本高，现有生成模型大多无法支持快速、稳定的确定性反演。

Method: 引入Janus统一生成 - 预测框架，将深度编码器 - 解码器架构与预测KHRONOS头耦合，学习潜在流形。

Result: 在MNIST数据集验证，在热导率标签微观结构逆设计中实现前向预测准确率$R^2=0.98$，像素级重建误差低于5%，逆解满足目标属性误差在1%以内。

Conclusion: Janus通过统一预测和生成，以较低计算成本实现实时、物理信息驱动的逆微观结构生成。

Abstract: Inverse design of heterogeneous material microstructures is a fundamentally ill-posed and famously computationally expensive problem. This is exacerbated by the high-dimensional design spaces associated with finely resolved images, multimodal input property streams, and a highly nonlinear forward physics. Whilst modern generative models excel at accurately modeling such complex forward behavior, most of them are not intrinsically structured to support fast, stable \emph{deterministic} inversion with a physics-informed bias. This work introduces Janus, a unified generative-predictive framework to address this problem. Janus couples a deep encoder-decoder architecture with a predictive KHRONOS head, a separable neural architecture. Topologically speaking, Janus learns a latent manifold simultaneously isometric for generative inversion and pruned for physical prediction; the joint objective inducing \emph{disentanglement} of the latent space. Janus is first validated on the MNIST dataset, demonstrating high-fidelity reconstruction, accurate classification and diverse generative inversion of all ten target classes. It is then applied to the inverse design of heterogeneous microstructures labeled with thermal conductivity. It achieves a forward prediction accuracy $R^2=0.98$ (2\% relative error) and sub-5\% pixelwise reconstruction error. Inverse solutions satisfy target properties to within $1\%$ relative error. Inverting a sweep through properties reveal smooth traversal of the latent manifold, and UMAP visualization confirms the emergence of a low-dimensional, disentangled manifold. By unifying prediction and generation within a single latent space, Janus enables real-time, physics-informed inverse microstructure generation at a lower computational cost typically associated with classical optimization-based approaches.

</details>


### [100] [Provably Extracting the Features from a General Superposition](https://arxiv.org/abs/2512.15987)
*Allen Liu*

Main category: cs.LG

TL;DR: 研究从黑盒查询访问中学习叠加特征，提出高效查询算法识别特征方向并重构函数，适用更一般场景。


<details>
  <summary>Details</summary>
Motivation: 复杂机器学习模型特征线性表示存在叠加难以恢复，叠加的过完备情况对典型算法有挑战。

Method: 在傅里叶空间搜索，迭代细化搜索空间定位隐藏方向。

Result: 提出高效查询算法，从有噪声的神谕访问中识别非退化响应的特征方向并重构函数。

Conclusion: 算法适用于更一般的叠加和响应函数情况。

Abstract: It is widely believed that complex machine learning models generally encode features through linear representations, but these features exist in superposition, making them challenging to recover. We study the following fundamental setting for learning features in superposition from black-box query access: we are given query access to a function \[ f(x)=\sum_{i=1}^n a_i\,σ_i(v_i^\top x), \] where each unit vector $v_i$ encodes a feature direction and $σ_i:\mathbb{R} \rightarrow \mathbb{R}$ is an arbitrary response function and our goal is to recover the $v_i$ and the function $f$.
  In learning-theoretic terms, superposition refers to the overcomplete regime, when the number of features is larger than the underlying dimension (i.e. $n > d$), which has proven especially challenging for typical algorithmic approaches. Our main result is an efficient query algorithm that, from noisy oracle access to $f$, identifies all feature directions whose responses are non-degenerate and reconstructs the function $f$. Crucially, our algorithm works in a significantly more general setting than all related prior results -- we allow for essentially arbitrary superpositions, only requiring that $v_i, v_j$ are not nearly identical for $i \neq j$, and general response functions $σ_i$. At a high level, our algorithm introduces an approach for searching in Fourier space by iteratively refining the search space to locate the hidden directions $v_i$.

</details>


### [101] [Stackelberg Learning from Human Feedback: Preference Optimization as a Sequential Game](https://arxiv.org/abs/2512.16626)
*Barna Pásztor,Thomas Kleine Buening,Andreas Krause*

Main category: cs.LG

TL;DR: 介绍Stackelberg Learning from Human Feedback (SLHF)偏好优化框架，对比其他方法并展示优势与实验成果。


<details>
  <summary>Details</summary>
Motivation: 提出新的偏好优化框架，解决现有方法不足，捕获更丰富偏好结构。

Method: 将对齐问题建模为领导者和追随者的序贯博弈，分解偏好优化问题。

Result: SLHF在一致性、数据敏感性和对非传递偏好的鲁棒性方面有优势，在大语言模型实验表现良好，能跨模型迁移。

Conclusion: SLHF是有效的偏好优化框架，可实现强对齐，有跨模型应用潜力。

Abstract: We introduce Stackelberg Learning from Human Feedback (SLHF), a new framework for preference optimization. SLHF frames the alignment problem as a sequential-move game between two policies: a Leader, which commits to an action, and a Follower, which responds conditionally on the Leader's action. This approach decomposes preference optimization into a refinement problem for the Follower and an optimization problem against an adversary for the Leader. Unlike Reinforcement Learning from Human Feedback (RLHF), which assigns scalar rewards to actions, or Nash Learning from Human Feedback (NLHF), which seeks a simultaneous-move equilibrium, SLHF leverages the asymmetry of sequential play to capture richer preference structures. The sequential design of SLHF naturally enables inference-time refinement, as the Follower learns to improve the Leader's actions, and these refinements can be leveraged through iterative sampling. We compare the solution concepts of SLHF, RLHF, and NLHF, and lay out key advantages in consistency, data sensitivity, and robustness to intransitive preferences. Experiments on large language models demonstrate that SLHF achieves strong alignment across diverse preference datasets, scales from 0.5B to 8B parameters, and yields inference-time refinements that transfer across model families without further fine-tuning.

</details>


### [102] [Explicit and Non-asymptotic Query Complexities of Rank-Based Zeroth-order Algorithms on Smooth Functions](https://arxiv.org/abs/2512.16200)
*Haishan Ye*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Rank-based zeroth-order (ZO) optimization -- which relies only on the ordering of function evaluations -- offers strong robustness to noise and monotone transformations, and underlies many successful algorithms such as CMA-ES, natural evolution strategies, and rank-based genetic algorithms. Despite its widespread use, the theoretical understanding of rank-based ZO methods remains limited: existing analyses provide only asymptotic insights and do not yield explicit convergence rates for algorithms selecting the top-$k$ directions.
  This work closes this gap by analyzing a simple rank-based ZO algorithm and establishing the first \emph{explicit}, and \emph{non-asymptotic} query complexities. For a $d$-dimension problem, if the function is $L$-smooth and $μ$-strongly convex, the algorithm achieves $\widetilde{\mathcal O}\!\left(\frac{dL}μ\log\!\frac{dL}{μδ}\log\!\frac{1}{\varepsilon}\right)$ to find an $\varepsilon$-suboptimal solution, and for smooth nonconvex objectives it reaches $\mathcal O\!\left(\frac{dL}{\varepsilon}\log\!\frac{1}{\varepsilon}\right)$. Notation $\cO(\cdot)$ hides constant terms and $\widetilde{\mathcal O}(\cdot)$ hides extra $\log\log\frac{1}{\varepsilon}$ term. These query complexities hold with a probability at least $1-δ$ with $0<δ<1$. The analysis in this paper is novel and avoids classical drift and information-geometric techniques. Our analysis offers new insight into why rank-based heuristics lead to efficient ZO optimization.

</details>


### [103] [D3G: Diverse Demographic Data Generation Increases Zero-Shot Image Classification Accuracy within Multimodal Models](https://arxiv.org/abs/2512.15747)
*Javon Hickmon*

Main category: cs.LG

TL;DR: 聚焦零样本图像分类问题，提出D3G方法提升分类准确率并减少人口统计学偏差。


<details>
  <summary>Details</summary>
Motivation: 现有图像分类任务存在模型欠拟合、数据质量不高和人口统计学偏差等问题，影响零样本图像分类效果。

Method: 提出训练无关的零样本方法D3G，以CLIP为基础多模态模型，Stable Diffusion XL为生成模型。

Result: 在推理时提供多样化人口统计数据可提升模型性能，并探究了个体人口统计特征对准确率指标的影响。

Conclusion: D3G方法能有效提升预训练多模态模型的分类准确率，减少人口统计学偏差。

Abstract: Image classification is a task essential for machine perception to achieve human-level image understanding. Multimodal models such as CLIP have been able to perform well on this task by learning semantic similarities across vision and language; however, despite these advances, image classification is still a challenging task. Models with low capacity often suffer from underfitting and thus underperform on fine-grained image classification. Along with this, it is important to ensure high-quality data with rich cross-modal representations of each class, which is often difficult to generate. When datasets do not enforce balanced demographics, the predictions will be biased toward the more represented class, while others will be neglected. We focus on how these issues can lead to harmful bias for zero-shot image classification, and explore how to combat these issues in demographic bias. We propose Diverse Demographic Data Generation (D3G), a training-free, zero-shot method of boosting classification accuracy while reducing demographic bias in pre-trained multimodal models. With this method, we utilize CLIP as our base multimodal model and Stable Diffusion XL as our generative model. We demonstrate that providing diverse demographic data at inference time improves performance for these models, and explore the impact of individual demographics on the resulting accuracy metric.

</details>


### [104] [Topic Modelling Black Box Optimization](https://arxiv.org/abs/2512.16445)
*Roman Akramov,Artem Khamatullin,Svetlana Glazyrina,Maksim Kryzhanovskiy,Roman Ischenko*

Main category: cs.LG

TL;DR: 本文探讨LDA主题数T选择问题，对比四种优化器，发现学习型优化器更高效。


<details>
  <summary>Details</summary>
Motivation: LDA中主题数T的选择对统计拟合和可解释性影响大，需合理选择T。

Method: 将T选择作为离散黑箱优化问题，用验证困惑度评估，对比GA、ES、PABBO和SABBO四种优化器。

Result: 四种优化器最终困惑度相近，但PABBO和SABBO在样本和时间上更高效，SABBO单次评估、PABBO几次评估就能找到优解，GA和ES需接近全量预算。

Conclusion: 学习型的PABBO和SABBO在解决LDA主题数T选择问题上更具效率。

Abstract: Choosing the number of topics $T$ in Latent Dirichlet Allocation (LDA) is a key design decision that strongly affects both the statistical fit and interpretability of topic models. In this work, we formulate the selection of $T$ as a discrete black-box optimization problem, where each function evaluation corresponds to training an LDA model and measuring its validation perplexity. Under a fixed evaluation budget, we compare four families of optimizers: two hand-designed evolutionary methods - Genetic Algorithm (GA) and Evolution Strategy (ES) - and two learned, amortized approaches, Preferential Amortized Black-Box Optimization (PABBO) and Sharpness-Aware Black-Box Optimization (SABBO). Our experiments show that, while GA, ES, PABBO, and SABBO eventually reach a similar band of final perplexity, the amortized optimizers are substantially more sample- and time-efficient. SABBO typically identifies a near-optimal topic number after essentially a single evaluation, and PABBO finds competitive configurations within a few evaluations, whereas GA and ES require almost the full budget to approach the same region.

</details>


### [105] [Surely Large Multimodal Models (Don't) Excel in Visual Species Recognition?](https://arxiv.org/abs/2512.15748)
*Tian Liu,Anwesha Basu,James Caverlee,Shu Kong*

Main category: cs.LG

TL;DR: 研究了大模态模型（LMMs）和少样本学习（FSL）模型在视觉物种识别（VSR）任务中的表现，并提出事后校正（POC）方法提升 FSL 模型效果。


<details>
  <summary>Details</summary>
Motivation: VSR 任务标注数据少，少样本学习训练专家模型有需求，探究 LMMs 在 VSR 任务表现及能否超越 FSL 专家模型。

Method: 提出 POC 方法，用包含软最大值置信度分数和少样本视觉示例的丰富提示，促使 LMM 对 FSL 专家模型的预测结果重新排序。

Result: 在五个 VSR 基准测试中，POC 比之前的 FSL 方法准确率提高 6.4%，且具有良好泛化性。

Conclusion: POC 可作为即插即用模块显著提升现有 FSL 方法，无需额外训练、验证或人工干预。

Abstract: Visual Species Recognition (VSR) is pivotal to biodiversity assessment and conservation, evolution research, and ecology and ecosystem management. Training a machine-learned model for VSR typically requires vast amounts of annotated images. Yet, species-level annotation demands domain expertise, making it realistic for domain experts to annotate only a few examples. These limited labeled data motivate training an ''expert'' model via few-shot learning (FSL). Meanwhile, advanced Large Multimodal Models (LMMs) have demonstrated prominent performance on general recognition tasks. It is straightforward to ask whether LMMs excel in the highly specialized VSR task and whether they outshine FSL expert models. Somewhat surprisingly, we find that LMMs struggle in this task, despite using various established prompting techniques. LMMs even significantly underperform FSL expert models, which are as simple as finetuning a pretrained visual encoder on the few-shot images. However, our in-depth analysis reveals that LMMs can effectively post-hoc correct the expert models' incorrect predictions. Briefly, given a test image, when prompted with the top predictions from an FSL expert model, LMMs can recover the ground-truth label. Building on this insight, we derive a simple method called Post-hoc Correction (POC), which prompts an LMM to re-rank the expert model's top predictions using enriched prompts that include softmax confidence scores and few-shot visual examples. Across five challenging VSR benchmarks, POC outperforms prior art of FSL by +6.4% in accuracy without extra training, validation, or manual intervention. Importantly, POC generalizes to different pretrained backbones and LMMs, serving as a plug-and-play module to significantly enhance existing FSL methods.

</details>


### [106] [TENG++: Time-Evolving Natural Gradient for Solving PDEs With Deep Neural Nets under General Boundary Conditions](https://arxiv.org/abs/2512.15771)
*Xinjie He,Chenggong Zhang*

Main category: cs.LG

TL;DR: 传统PDE数值方法有局限，PINNs有挑战。本文扩展TENG框架处理狄利克雷边界条件，实验证明Heun方法精度高、Euler方法在简单场景计算高效，为扩展到其他边界条件和PDE奠定基础。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法处理高维或复杂PDE问题有困难，PINNs在高精度和处理复杂边界条件方面存在挑战。

Method: 将TENG框架扩展以处理狄利克雷边界条件，结合自然梯度优化和数值时间步长方案（Euler和Heun方法），并在损失函数中加入边界条件惩罚项。

Result: 在热方程实验中，Heun方法因二阶修正精度更高，Euler方法在简单场景计算效率高。

Conclusion: 为扩展到诺伊曼和混合边界条件以及更广泛类别的PDE奠定基础，提升神经网络求解器在现实问题中的适用性。

Abstract: Partial Differential Equations (PDEs) are central to modeling complex systems across physical, biological, and engineering domains, yet traditional numerical methods often struggle with high-dimensional or complex problems. Physics-Informed Neural Networks (PINNs) have emerged as an efficient alternative by embedding physics-based constraints into deep learning frameworks, but they face challenges in achieving high accuracy and handling complex boundary conditions. In this work, we extend the Time-Evolving Natural Gradient (TENG) framework to address Dirichlet boundary conditions, integrating natural gradient optimization with numerical time-stepping schemes, including Euler and Heun methods, to ensure both stability and accuracy. By incorporating boundary condition penalty terms into the loss function, the proposed approach enables precise enforcement of Dirichlet constraints. Experiments on the heat equation demonstrate the superior accuracy of the Heun method due to its second-order corrections and the computational efficiency of the Euler method for simpler scenarios. This work establishes a foundation for extending the framework to Neumann and mixed boundary conditions, as well as broader classes of PDEs, advancing the applicability of neural network-based solvers for real-world problems.

</details>


### [107] [A Special Case of Quadratic Extrapolation Under the Neural Tangent Kernel](https://arxiv.org/abs/2512.15749)
*Abiel Kim*

Main category: cs.LG

TL;DR: 本文聚焦ReLU MLP在NTK机制下原点外推分析，发现原点附近二次外推现象。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习文献对ReLU MLP线性外推机制有分析，但NTK机制下原点外推分析较少，且无限维特征图特性使远点和原点附近外推不同，由此推动研究。

Method: 未提及

Result: 发现评估点接近原点时的二次外推。

Conclusion: 未提及

Abstract: It has been demonstrated both theoretically and empirically that the ReLU MLP tends to extrapolate linearly for an out-of-distribution evaluation point. The machine learning literature provides ample analysis with respect to the mechanisms to which linearity is induced. However, the analysis of extrapolation at the origin under the NTK regime remains a more unexplored special case. In particular, the infinite-dimensional feature map induced by the neural tangent kernel is not translationally invariant. This means that the study of an out-of-distribution evaluation point very far from the origin is not equivalent to the evaluation of a point very near the origin. And since the feature map is rotation invariant, these two special cases may represent the most canonically extreme bounds of ReLU NTK extrapolation. Ultimately, it is this loose recognition of the two special cases of extrapolation that motivate the discovery of quadratic extrapolation for an evaluation close to the origin.

</details>


### [108] [Abacus: Self-Supervised Event Counting-Aligned Distributional Pretraining for Sequential User Modeling](https://arxiv.org/abs/2512.16581)
*Sullivan Castro,Artem Betlei,Thomas Di Martino,Nadir El Manouzi*

Main category: cs.LG

TL;DR: 本文提出Abacus方法及混合目标，在展示广告中增强深度序列模型，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 展示广告系统中用户购买行为建模面临正样本稀疏、用户行为随机等问题，现有预测系统和序列模型有局限性。

Method: 使用自监督预训练策略增强深度序列模型，引入Abacus预测用户事件经验频率分布，提出结合Abacus与序列学习目标的混合目标。

Result: 在两个真实数据集上，Abacus预训练加速下游任务收敛，混合方法比基线提升最高6.1%的AUC。

Conclusion: 所提方法在展示广告用户购买行为建模上优于现有方法。

Abstract: Modeling user purchase behavior is a critical challenge in display advertising systems, necessary for real-time bidding. The difficulty arises from the sparsity of positive user events and the stochasticity of user actions, leading to severe class imbalance and irregular event timing. Predictive systems usually rely on hand-crafted "counter" features, overlooking the fine-grained temporal evolution of user intent. Meanwhile, current sequential models extract direct sequential signal, missing useful event-counting statistics. We enhance deep sequential models with self-supervised pretraining strategies for display advertising. Especially, we introduce Abacus, a novel approach of predicting the empirical frequency distribution of user events. We further propose a hybrid objective unifying Abacus with sequential learning objectives, combining stability of aggregated statistics with the sequence modeling sensitivity. Experiments on two real-world datasets show that Abacus pretraining outperforms existing methods accelerating downstream task convergence, while hybrid approach yields up to +6.1% AUC compared to the baselines.

</details>


### [109] [GLOW: Graph-Language Co-Reasoning for Agentic Workflow Performance Prediction](https://arxiv.org/abs/2512.15751)
*Wei Guan,Jian Cao,Jinyu Cai,Qiqi Cai,Jianqi Gao,See-Kiong Ng*

Main category: cs.LG

TL;DR: 提出GLOW框架用于Agentic Workflows性能预测，结合GNN与LLM，在实验中表现优于现有基线


<details>
  <summary>Details</summary>
Motivation: 现有Agentic Workflows性能预测方法无法同时捕捉其拓扑依赖和语义逻辑，且自动化生成扩展性受执行评估成本和延迟限制

Method: 提出GLOW框架，结合GNN的图结构建模能力和LLM的推理能力，用面向图的LLM提取语义特征并与GNN编码的结构表示融合，使用对比对齐策略细化潜在空间

Result: 在FLORA - Bench上的大量实验表明，GLOW在预测准确性和排名实用性方面优于现有基线

Conclusion: GLOW是一种有效的Agentic Workflows性能预测框架

Abstract: Agentic Workflows (AWs) have emerged as a promising paradigm for solving complex tasks. However, the scalability of automating their generation is severely constrained by the high cost and latency of execution-based evaluation. Existing AW performance prediction methods act as surrogates but fail to simultaneously capture the intricate topological dependencies and the deep semantic logic embedded in AWs. To address this limitation, we propose GLOW, a unified framework for AW performance prediction that combines the graph-structure modeling capabilities of GNNs with the reasoning power of LLMs. Specifically, we introduce a graph-oriented LLM, instruction-tuned on graph tasks, to extract topologically aware semantic features, which are fused with GNN-encoded structural representations. A contrastive alignment strategy further refines the latent space to distinguish high-quality AWs. Extensive experiments on FLORA-Bench show that GLOW outperforms state-of-the-art baselines in prediction accuracy and ranking utility.

</details>


### [110] [TAO-Net: Two-stage Adaptive OOD Classification Network for Fine-grained Encrypted Traffic Classification](https://arxiv.org/abs/2512.15753)
*Zihao Wang,Wei Peng,Junming Zhang,Jian Li,Wenxin Fang*

Main category: cs.LG

TL;DR: 提出TAO - Net对加密流量分类，在三个数据集实验效果好。


<details>
  <summary>Details</summary>
Motivation: 现有加密流量分类方法依赖预定义类别，处理未知流量类型效果差，且无法细粒度分类。

Method: 提出两阶段自适应OOD分类网络TAO - Net。第一阶段用混合OOD检测机制区分ID和OOD流量，第二阶段用大语言模型和语义增强提示策略将OOD流量分类转化为生成任务。

Result: 在三个数据集上TAO - Net达到96.81 - 97.70%的宏精度和96.77 - 97.68%的宏F1，优于仅达44.73 - 86.30%宏精度的以往方法。

Conclusion: TAO - Net在加密流量分类上表现出色，尤其在识别新兴网络应用方面。

Abstract: Encrypted traffic classification aims to identify applications or services by analyzing network traffic data. One of the critical challenges is the continuous emergence of new applications, which generates Out-of-Distribution (OOD) traffic patterns that deviate from known categories and are not well represented by predefined models. Current approaches rely on predefined categories, which limits their effectiveness in handling unknown traffic types. Although some methods mitigate this limitation by simply classifying unknown traffic into a single "Other" category, they fail to make a fine-grained classification. In this paper, we propose a Two-stage Adaptive OOD classification Network (TAO-Net) that achieves accurate classification for both In-Distribution (ID) and OOD encrypted traffic. The method incorporates an innovative two-stage design: the first stage employs a hybrid OOD detection mechanism that integrates transformer-based inter-layer transformation smoothness and feature analysis to effectively distinguish between ID and OOD traffic, while the second stage leverages large language models with a novel semantic-enhanced prompt strategy to transform OOD traffic classification into a generation task, enabling flexible fine-grained classification without relying on predefined labels. Experiments on three datasets demonstrate that TAO-Net achieves 96.81-97.70% macro-precision and 96.77-97.68% macro-F1, outperforming previous methods that only reach 44.73-86.30% macro-precision, particularly in identifying emerging network applications.

</details>


### [111] [KAN-Matrix: Visualizing Nonlinear Pairwise and Multivariate Contributions for Physical Insight](https://arxiv.org/abs/2512.15755)
*Luis A. De la Fuente,Hernan A. Moreno,Laura V. Alvarez,Hoshin V. Gupta*

Main category: cs.LG

TL;DR: 本文引入KANs新应用，提出PKAN和MKAN可视化工具，实验显示比传统方法更优，助于发现物理模式和模型开发。


<details>
  <summary>Details</summary>
Motivation: 解决科学家解读复杂数据集面临的高维与变量共线性挑战。

Method: 引入Kolmogorov - Arnold Networks（KANs）应用，提出Pairwise KAN Matrix（PKAN）和Multivariate KAN Contribution Matrix（MKAN）可视化工具。

Result: 通过实验比较，PKAN和MKAN比Pearson Correlation和Mutual Information能产生更稳健和有信息的结果。

Conclusion: PKAN和MKAN矩阵可捕捉关系强度和函数形式，有助于发现隐藏物理模式，促进领域知情的模型开发。

Abstract: Interpreting complex datasets remains a major challenge for scientists, particularly due to high dimensionality and collinearity among variables. We introduce a novel application of Kolmogorov-Arnold Networks (KANs) to enhance interpretability and parsimony beyond what traditional correlation analyses offer. We present two interpretable, color-coded visualization tools: the Pairwise KAN Matrix (PKAN) and the Multivariate KAN Contribution Matrix (MKAN). PKAN characterizes nonlinear associations between pairs of variables, while MKAN serves as a nonlinear feature-ranking tool that quantifies the relative contributions of inputs in predicting a target variable. These tools support pre-processing (e.g., feature selection, redundancy analysis) and post-processing (e.g., model explanation, physical insights) in model development workflows. Through experimental comparisons, we demonstrate that PKAN and MKAN yield more robust and informative results than Pearson Correlation and Mutual Information. By capturing the strength and functional forms of relationships, these matrices facilitate the discovery of hidden physical patterns and promote domain-informed model development.

</details>


### [112] [ReactorFold: Generative discovery of nuclear reactor cores via emergent physical reasoning](https://arxiv.org/abs/2512.15756)
*Yoonpyo Lee*

Main category: cs.LG

TL;DR: 提出ReactorFold生成框架，将燃料组件设计转化为语言模型序列建模问题，能突破传统方法局限，发现新设计拓扑。


<details>
  <summary>Details</summary>
Motivation: 传统设计方法在固定、人为定义的配置空间内搜索，难以发现新设计拓扑，需新方法解决核反应堆堆芯设计问题。

Method: 将燃料组件设计转化为语言模型序列建模问题，使用蒙特卡罗数据、参数高效微调及直接偏好优化（DPO）训练模型。

Result: DPO对齐模型实现设计空间扩展，能自主调整钆库存满足约束，发现高性能非对称配置。

Conclusion: 语言模型可内化因果物理关系，超越人为设计约束。

Abstract: Designing nuclear reactor cores requires navigating large discrete design spaces governed by complex neutronic interactions. Traditional deterministic, metaheuristic, and machine-learning-assisted methods search within fixed, human-defined configuration spaces, limiting their ability to discover fundamentally new design topologies. Here we introduce ReactorFold, a generative framework that reformulates fuel-assembly design as a sequence modeling problem for language models. Using Monte Carlo data, parameter-efficient fine-tuning, and Direct Preference Optimization (DPO), the model learns the latent structure of a pressurized-water-reactor assembly and generates candidate layouts in a single forward pass. Notably, the DPO-aligned model exhibits emergent design-space expansion: despite being trained exclusively on configurations with a fixed number of gadolinium burnable absorber (Gd) rods, it autonomously adjusts Gd inventory to satisfy strict power-peaking constraints. The model also discovers high-performing asymmetric configurations that challenge conventional symmetric loading heuristics, accessing design regimes inaccessible to conventional search methods and demonstrating that language models can internalize causal physical relationships and transcend human-imposed design constraints.

</details>


### [113] [Twin Restricted Kernel Machines for Multiview Classification](https://arxiv.org/abs/2512.15757)
*A. Quadir,M. Sajid,Mushir Akhtar,M. Tanveer*

Main category: cs.LG

TL;DR: 提出多视图孪生受限核机器TMvRKM模型，解决传统基于核方法的计算和泛化挑战，在多数据集测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统多视图支持向量机在高维空间捕捉决策边界有挑战，易出错且难处理视图不一致问题，需新模型。

Method: 引入TMvRKM模型，通过正则化最小二乘法确定最优分离超平面，目标函数含耦合项，融合早晚期策略。

Result: 在UCI、KEEL和AwA基准数据集上测试，实验和统计分析显示TMvRKM泛化性能出色，优于基线模型。

Conclusion: TMvRKM模型有效解决传统方法问题，提升计算效率和分类性能，泛化能力强。

Abstract: Multi-view learning (MVL) is an emerging field in machine learning that focuses on improving generalization performance by leveraging complementary information from multiple perspectives or views. Various multi-view support vector machine (MvSVM) approaches have been developed, demonstrating significant success. Moreover, these models face challenges in effectively capturing decision boundaries in high-dimensional spaces using the kernel trick. They are also prone to errors and struggle with view inconsistencies, which are common in multi-view datasets. In this work, we introduce the multiview twin restricted kernel machine (TMvRKM), a novel model that integrates the strengths of kernel machines with the multiview framework, addressing key computational and generalization challenges associated with traditional kernel-based approaches. Unlike traditional methods that rely on solving large quadratic programming problems (QPPs), the proposed TMvRKM efficiently determines an optimal separating hyperplane through a regularized least squares approach, enhancing both computational efficiency and classification performance. The primal objective of TMvRKM includes a coupling term designed to balance errors across multiple views effectively. By integrating early and late fusion strategies, TMvRKM leverages the collective information from all views during training while remaining flexible to variations specific to individual views. The proposed TMvRKM model is rigorously tested on UCI, KEEL, and AwA benchmark datasets. Both experimental results and statistical analyses consistently highlight its exceptional generalization performance, outperforming baseline models in every scenario.

</details>


### [114] [Multivariate Uncertainty Quantification with Tomographic Quantile Forests](https://arxiv.org/abs/2512.16383)
*Takuya Kanazawa*

Main category: cs.LG

TL;DR: 提出用于多变量目标的非参数树基回归模型Tomographic Quantile Forests (TQF)，评估其在合成和真实数据集上的表现并开源代码。


<details>
  <summary>Details</summary>
Motivation: 量化预测不确定性对安全可靠的现实AI部署至关重要，但多变量目标的条件分布全非参数估计仍具挑战。

Method: TQF学习方向投影的条件分位数，推理时跨多方向聚合分位数，通过高效交替方案最小化切片Wasserstein距离重建多变量条件分布。

Result: 在合成和真实数据集上对TQF进行评估。

Conclusion: TQF用单一模型覆盖所有方向且无凸性限制，是解决多变量目标非参数估计问题的有效方法。

Abstract: Quantifying predictive uncertainty is essential for safe and trustworthy real-world AI deployment. Yet, fully nonparametric estimation of conditional distributions remains challenging for multivariate targets. We propose Tomographic Quantile Forests (TQF), a nonparametric, uncertainty-aware, tree-based regression model for multivariate targets. TQF learns conditional quantiles of directional projections $\mathbf{n}^{\top}\mathbf{y}$ as functions of the input $\mathbf{x}$ and the unit direction $\mathbf{n}$. At inference, it aggregates quantiles across many directions and reconstructs the multivariate conditional distribution by minimizing the sliced Wasserstein distance via an efficient alternating scheme with convex subproblems. Unlike classical directional-quantile approaches that typically produce only convex quantile regions and require training separate models for different directions, TQF covers all directions with a single model without imposing convexity restrictions. We evaluate TQF on synthetic and real-world datasets, and release the source code on GitHub.

</details>


### [115] [Yantra AI -- An intelligence platform which interacts with manufacturing operations](https://arxiv.org/abs/2512.15758)
*Varshini Krishnamurthy*

Main category: cs.LG

TL;DR: 论文围绕为XRIT创建并测试智能生产系统展开，融入机器学习模型和实时可视化功能，测试显示其提升多项指标，未来聚焦实时数据融合与优化。


<details>
  <summary>Details</summary>
Motivation: Industry 4.0发展迅速，为使XRIT生产运营更顺畅，解决能源管理、预测性维护和AI决策支持等重要问题。

Method: 构建机器学习模型，如随机森林分类器和孤立森林；采用Streamlit实现实时数据可视化；添加基于GPT - 4的AI虚拟助手。

Result: 系统经过模拟数据测试，可在XRIT生产环境实时应用，显著提升工作效率、能源管理和维修规划能力。

Conclusion: 系统有效提升多项生产指标，未来将聚焦实时数据融合和其他优化途径。

Abstract: Industry 4.0 is growing quickly, which has changed smart production by encouraging the use of real-time tracking, machine learning, and AI-driven systems to make operations run more smoothly. The main focus of this dissertation is on creating and testing an intelligent production system for XRIT that solves important problems like energy management, predictive maintenance, and AI-powered decision support. Machine learning models are built into the system, such as the Random Forest Classifier for proactive maintenance and the Isolation Forest for finding outliers. These models help with decision-making and reducing downtime. Streamlit makes real-time data visualisation possible, giving workers access to dashboards that they can interact with and see real-time observations.The system was tested with fake data and is made to be scalable, so it can be used in real time in XRIT's production setting. Adding an AI-powered virtual assistant made with GPT-4 lets workers get real-time, useful information that makes complicated questions easier to answer and improves operational decisions. The testing shows that the system makes working efficiency, energy management, and the ability to plan repairs much better. Moving the system to real-time data merging and looking for other ways to make it better will be the main focus of future work.

</details>


### [116] [Kascade: A Practical Sparse Attention Method for Long-Context LLM Inference](https://arxiv.org/abs/2512.16391)
*Dhruv Deshmukh,Saurabh Goyal,Nipun Kwatra,Ramachandran Ramjee*

Main category: cs.LG

TL;DR: 提出无训练稀疏注意力方法Kascade，在长上下文基准测试中实现速度提升并接近密集注意力精度。


<details>
  <summary>Details</summary>
Motivation: 注意力是长上下文大语言模型推理延迟的主要来源，旨在解决该问题。

Method: 利用后softmax注意力固有稀疏性和高权重键的稳定性，在锚层计算Top - k索引并在中间重用层复用，通过动态规划目标选择锚层，结合高效实现约束，Top - k选择和复用考虑注意力头。

Result: 在H100 GPU上，相比FlashAttention - 3基线，解码注意力速度提升达4.1倍，预填充注意力速度提升达2.2倍。

Conclusion: Kascade在长上下文基准测试中能接近密集注意力精度，且大幅提升推理速度。

Abstract: Attention is the dominant source of latency during long-context LLM inference, an increasingly popular workload with reasoning models and RAG. We propose Kascade, a training-free sparse attention method that leverages known observations such as 1) post-softmax attention is intrinsically sparse, and 2) the identity of high-weight keys is stable across nearby layers. Kascade computes exact Top-k indices in a small set of anchor layers, then reuses those indices in intermediate reuse layers. The anchor layers are selected algorithmically, via a dynamic-programming objective that maximizes cross-layer similarity over a development set, allowing easy deployment across models. The method incorporates efficient implementation constraints (e.g. tile-level operations), across both prefill and decode attention. The Top-k selection and reuse in Kascade is head-aware and we show in our experiments that this is critical for high accuracy. Kascade achieves up to 4.1x speedup in decode attention and 2.2x speedup in prefill attention over FlashAttention-3 baseline on H100 GPUs while closely matching dense attention accuracy on long-context benchmarks such as LongBench and AIME-24.

</details>


### [117] [Semantic-Constrained Federated Aggregation: Convergence Theory and Privacy-Utility Bounds for Knowledge-Enhanced Distributed Learning](https://arxiv.org/abs/2512.15759)
*Jahidul Arafat*

Main category: cs.LG

TL;DR: 提出语义约束联邦聚合框架SCFA，证明收敛率，分析其在减少数据异质性、提升隐私-效用权衡方面的作用，在制造业预测性维护实验中验证效果。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在非IID数据条件下收敛慢，现有解决方案忽略语义有效性。

Method: 引入语义约束联邦聚合（SCFA）框架，将领域知识约束纳入分布式优化中。

Result: 约束可减少41%的有效数据异质性，提升隐私-效用权衡；在差分隐私下，约束正则化性能优于标准联邦学习；在实验中实现22%更快收敛、41.3%的模型差异减少。

Conclusion: 理论预测与实证观察高度匹配，证明了SCFA框架的有效性。

Abstract: Federated learning enables collaborative model training across distributed data sources but suffers from slow convergence under non-IID data conditions. Existing solutions employ algorithmic modifications treating all client updates identically, ignoring semantic validity. We introduce Semantic-Constrained Federated Aggregation (SCFA), a theoretically-grounded framework incorporating domain knowledge constraints into distributed optimization. We prove SCFA achieves convergence rate O(1/sqrt(T) + rho) where rho represents constraint violation rate, establishing the first convergence theory for constraint-based federated learning. Our analysis shows constraints reduce effective data heterogeneity by 41% and improve privacy-utility tradeoffs through hypothesis space reduction by factor theta=0.37. Under (epsilon,delta)-differential privacy with epsilon=10, constraint regularization maintains utility within 3.7% of non-private baseline versus 12.1% degradation for standard federated learning, representing 2.7x improvement. We validate our framework on manufacturing predictive maintenance using Bosch production data with 1.18 million samples and 968 sensor features, constructing knowledge graphs encoding 3,000 constraints from ISA-95 and MASON ontologies. Experiments demonstrate 22% faster convergence, 41.3% model divergence reduction, and constraint violation thresholds where rho<0.05 maintains 90% optimal performance while rho>0.18 causes catastrophic failure. Our theoretical predictions match empirical observations with R^2>0.90 across convergence, privacy, and violation-performance relationships.

</details>


### [118] [A Tutorial on Dimensionless Learning: Geometric Interpretation and the Effect of Noise](https://arxiv.org/abs/2512.15760)
*Zhengtao Jake Gan,Xiaoyu Xie*

Main category: cs.LG

TL;DR: 本教程介绍无量纲学习方法，结合经典量纲分析与机器学习技术从实验数据发现无量纲数和标度律，探讨噪声和采样影响，虽有进展但仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 介绍无量纲学习方法，将实验数据转化为简洁物理定律，揭示变量间量纲不变性。

Method: 结合经典量纲分析与现代机器学习技术，用神经网络找出最佳变量组合，采用正则化技术使系数简单可解释。

Result: 该方法能处理单或多个无量纲数情况，正则化方法对实验不确定性有鲁棒性。

Conclusion: 虽有进展，但仍面临计算成本、数据特征影响、输入变量选择和开发工具等挑战，可作为研究者应用该方法的教育资源和实用指南。

Abstract: Dimensionless learning is a data-driven framework for discovering dimensionless numbers and scaling laws from experimental measurements. This tutorial introduces the method, explaining how it transforms experimental data into compact physical laws that reveal compact dimensional invariance between variables. The approach combines classical dimensional analysis with modern machine learning techniques. Starting from measurements of physical quantities, the method identifies the fundamental ways to combine variables into dimensionless groups, then uses neural networks to discover which combinations best predict the experimental output. A key innovation is a regularization technique that encourages the learned coefficients to take simple, interpretable values like integers or half-integers, making the discovered laws both accurate and physically meaningful. We systematically investigate how measurement noise and discrete sampling affect the discovery process, demonstrating that the regularization approach provides robustness to experimental uncertainties. The method successfully handles cases with single or multiple dimensionless numbers, revealing how different but equivalent representations can capture the same underlying physics. Despite recent progress, key challenges remain, including managing the computational cost of identifying multiple dimensionless groups, understanding the influence of data characteristics, automating the selection of relevant input variables, and developing user-friendly tools for experimentalists. This tutorial serves as both an educational resource and a practical guide for researchers seeking to apply dimensionless learning to their experimental data.

</details>


### [119] [Machine Learning Framework for Thrombosis Risk Prediction in Rotary Blood Pumps](https://arxiv.org/abs/2512.15761)
*Christopher Blum,Michael Neidlin*

Main category: cs.LG

TL;DR: 本文介绍可解释机器学习框架用于空间血栓评估，训练模型可预测血栓易发生区域，成本低且计算高效。


<details>
  <summary>Details</summary>
Motivation: 现有计算模型难以将旋转血泵复杂流动条件转化为可靠且可解释的血栓风险预测，对特定流动特征如何导致血栓形成和生长了解不足。

Method: 引入基于计算流体动力学流动特征的可解释机器学习框架，使用逻辑回归模型结合结构化特征选择管道推导紧凑且物理可解释的特征集。

Result: 模型重现标记的风险分布，识别与血栓风险增加相关的流动特征集，能预测离心泵中血栓易发生区域。

Conclusion: 可解释机器学习能将局部流动特征与血栓风险关联，计算高效且机制透明，为基于CFD的血栓分析和设备设计工作流程提供方法论基础。

Abstract: Thrombosis in rotary blood pumps arises from complex flow conditions that remain difficult to translate into reliable and interpretable risk predictions using existing computational models. This limitation reflects an incomplete understanding of how specific flow features contribute to thrombus initiation and growth. This study introduces an interpretable machine learning framework for spatial thrombosis assessment based directly on computational fluid dynamics-derived flow features. A logistic regression (LR) model combined with a structured feature-selection pipeline is used to derive a compact and physically interpretable feature set, including nonlinear feature combinations. The framework is trained using spatial risk patterns from a validated, macro-scale thrombosis model for two representative scenarios. The model reproduces the labeled risk distributions and identifies distinct sets of flow features associated with increased thrombosis risk. When applied to a centrifugal pump, despite training on a single axial pump operating point, the model predicts plausible thrombosis-prone regions. These results show that interpretable machine learning can link local flow features to thrombosis risk while remaining computationally efficient and mechanistically transparent. The low computational cost enables rapid thrombogenicity screening without repeated or costly simulations. The proposed framework complements physics-based thrombosis modeling and provides a methodological basis for integrating interpretable machine learning into CFD-driven thrombosis analysis and device design workflows.

</details>


### [120] [Training Together, Diagnosing Better: Federated Learning for Collagen VI-Related Dystrophies](https://arxiv.org/abs/2512.16876)
*Astrid Brull,Sara Aguti,Véronique Bolduc,Ying Hu,Daniel M. Jimenez-Gutierrez,Enrique Zuazua,Joaquin Del-Rio,Oleksii Sliusarenko,Haiyan Zhou,Francesco Muntoni,Carsten G. Bönnemann,Xabi Uribe-Etxebarria*

Main category: cs.LG

TL;DR: 本文提出利用Sherpa.ai联邦学习平台对两个国际组织的分布式数据集进行联邦学习，诊断COL6 - RD，结果模型表现优于单机构模型，提升诊断效用和泛化性。


<details>
  <summary>Details</summary>
Motivation: 机器学习诊断罕见病存在数据稀缺和分散问题，跨机构合作面临隐私、监管和后勤障碍，需方法解决。

Method: 采用Sherpa.ai联邦学习平台，对两个国际组织的分布式数据集，利用患者成纤维细胞培养的胶原蛋白VI免疫荧光显微镜图像进行联邦学习。

Result: 得到能将胶原蛋白VI患者图像分类到COL6 - RD相关三个主要致病机制组的机器学习模型，F1分数达0.82，优于单机构模型。

Conclusion: 联邦学习相比孤立的机构模型显著提高诊断效用和泛化性，还能支持不确定意义变异的解释、指导测序策略的优先级确定以识别新的致病变异。

Abstract: The application of Machine Learning (ML) to the diagnosis of rare diseases, such as collagen VI-related dystrophies (COL6-RD), is fundamentally limited by the scarcity and fragmentation of available data. Attempts to expand sampling across hospitals, institutions, or countries with differing regulations face severe privacy, regulatory, and logistical obstacles that are often difficult to overcome. The Federated Learning (FL) provides a promising solution by enabling collaborative model training across decentralized datasets while keeping patient data local and private. Here, we report a novel global FL initiative using the Sherpa.ai FL platform, which leverages FL across distributed datasets in two international organizations for the diagnosis of COL6-RD, using collagen VI immunofluorescence microscopy images from patient-derived fibroblast cultures. Our solution resulted in an ML model capable of classifying collagen VI patient images into the three primary pathogenic mechanism groups associated with COL6-RD: exon skipping, glycine substitution, and pseudoexon insertion. This new approach achieved an F1-score of 0.82, outperforming single-organization models (0.57-0.75). These results demonstrate that FL substantially improves diagnostic utility and generalizability compared to isolated institutional models. Beyond enabling more accurate diagnosis, we anticipate that this approach will support the interpretation of variants of uncertain significance and guide the prioritization of sequencing strategies to identify novel pathogenic variants.

</details>


### [121] [Cross-Sample Augmented Test-Time Adaptation for Personalized Intraoperative Hypotension Prediction](https://arxiv.org/abs/2512.15762)
*Kanxue Li,Yibing Zhan,Hua Jin,Chongchong Qi,Xu Lin,Baosheng Yu*

Main category: cs.LG

TL;DR: 提出CSA - TTA框架解决术中低血压（IOH）精准预测难题，通过跨样本增强和特殊训练策略提升模型性能，在数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: IOH预测有风险且因患者个体差异难以准确预测，测试时适应（TTA）因IOH事件稀少导致训练不可靠。

Method: 构建跨样本库，采用粗到细检索策略构建测试时训练数据，训练中结合自监督掩码重建和回顾性序列预测信号。

Result: 在VitalDB和真实医院数据集上，CSA - TTA提升了召回率和F1分数，例如在VitalDB上微调时提高了1.33%和1.13%，零样本场景下提高了7.46%和5.07%。

Conclusion: CSA - TTA具有强大的鲁棒性和泛化能力，能有效提升IOH预测性能。

Abstract: Intraoperative hypotension (IOH) poses significant surgical risks, but accurate prediction remains challenging due to patient-specific variability. While test-time adaptation (TTA) offers a promising approach for personalized prediction, the rarity of IOH events often leads to unreliable test-time training. To address this, we propose CSA-TTA, a novel Cross-Sample Augmented Test-Time Adaptation framework that enhances training by incorporating hypotension events from other individuals. Specifically, we first construct a cross-sample bank by segmenting historical data into hypotensive and non-hypotensive samples. Then, we introduce a coarse-to-fine retrieval strategy for building test-time training data: we initially apply K-Shape clustering to identify representative cluster centers and subsequently retrieve the top-K semantically similar samples based on the current patient signal. Additionally, we integrate both self-supervised masked reconstruction and retrospective sequence forecasting signals during training to enhance model adaptability to rapid and subtle intraoperative dynamics. We evaluate the proposed CSA-TTA on both the VitalDB dataset and a real-world in-hospital dataset by integrating it with state-of-the-art time series forecasting models, including TimesFM and UniTS. CSA-TTA consistently enhances performance across settings-for instance, on VitalDB, it improves Recall and F1 scores by +1.33% and +1.13%, respectively, under fine-tuning, and by +7.46% and +5.07% in zero-shot scenarios-demonstrating strong robustness and generalization.

</details>


### [122] [AdaGradSelect: An adaptive gradient-guided layer selection method for efficient fine-tuning of SLMs](https://arxiv.org/abs/2512.15764)
*Anshul Kumar,Gagan Raj Gupta,Manisha Chawla*

Main category: cs.LG

TL;DR: 提出AdaGradSelect方法用于小语言模型参数高效微调，训练更快、内存使用少，性能接近全量微调且优于LoRA。


<details>
  <summary>Details</summary>
Motivation: 大语言模型全量微调成本高、内存需求大，现有PEFT方法有局限性，小语言模型更需高效微调方法。

Method: 基于梯度自适应选择要更新的transformer块，结合Dirichlet采样和epsilon - greedy探索策略。

Result: AdaGradSelect训练快12%，GPU内存使用少35%，性能接近全量微调，在GSM8K数据集上优于LoRA，在MATH数据集上精度相当。

Conclusion: AdaGradSelect是传统微调方法更有效且资源高效的替代方案。

Abstract: Large Language Models (LLMs) can perform many NLP tasks well, but fully fine-tuning them is expensive and requires a lot of memory. Parameter-Efficient Fine-Tuning (PEFT) methods such as LoRA reduce this cost by adding small low-rank updates to frozen model weights. However, these methods restrict the training to a limited subspace, which can sometimes reduce performance.
  For Small Language Models (SLMs), where efficiency gains matter even more, we introduce AdaGradSelect, an adaptive method that selects which transformer blocks to update based on gradients.
  Early observations showed that updating only the transformer blocks with the highest gradient norms can achieve performance close to full fine-tuning. Building on this insight, AdaGradSelect adaptively chooses which blocks to train. It uses a combination of Dirichlet-based sampling, which depends on how frequently blocks were updated in the past, and an epsilon-greedy exploration strategy. This lets the method explore different blocks in early training and gradually focus on the most important ones in later epochs.
  Experiments show that AdaGradSelect trains about 12 percent faster and uses 35 percent less GPU memory while delivering performance very close to full fine-tuning. On the GSM8K dataset, it outperforms LoRA (rank 256) by about 3 percent on average across models such as Qwen2.5-0.5B, LLaMA3.2-1B, and Phi4-mini-3.8B. It also achieves similar accuracy on the MATH dataset. Overall, AdaGradSelect provides a more effective and resource-efficient alternative to traditional fine-tuning methods.

</details>


### [123] [Bridging Data and Physics: A Graph Neural Network-Based Hybrid Twin Framework](https://arxiv.org/abs/2512.15767)
*M. Gorpinich,B. Moya,S. Rodriguez,F. Meraghni,Y. Jaafra,A. Briot,M. Henner,R. Leon,F. Chinesta*

Main category: cs.LG

TL;DR: 本文提出用基于图神经网络（GNN）的混合孪生方法来建模无知模型，解决复杂非稳态物理现象模拟中数据不足的问题，在非线性传热问题中验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 传统数学模型存在无知模型问题，纯数据驱动方法需大量数据且现实中不可用，且空间测量稀疏、不同空间配置数据获取难。

Method: 采用混合孪生方法建模无知组件，用GNN表示无知模型，学习缺失物理的空间模式。

Result: GNN成功捕获无知并在不同空间配置上推广修正，提高模拟准确性和可解释性，减少数据需求。

Conclusion: 基于GNN的混合孪生方法能有效解决复杂物理现象模拟中数据不足问题，提升模拟效果。

Abstract: Simulating complex unsteady physical phenomena relies on detailed mathematical models, simulated for instance by using the Finite Element Method (FEM). However, these models often exhibit discrepancies from the reality due to unmodeled effects or simplifying assumptions. We refer to this gap as the ignorance model. While purely data-driven approaches attempt to learn full system behavior, they require large amounts of high-quality data across the entire spatial and temporal domain. In real-world scenarios, such information is unavailable, making full data-driven modeling unreliable. To overcome this limitation, we model of the ignorance component using a hybrid twin approach, instead of simulating phenomena from scratch. Since physics-based models approximate the overall behavior of the phenomena, the remaining ignorance is typically lower in complexity than the full physical response, therefore, it can be learned with significantly fewer data. A key difficulty, however, is that spatial measurements are sparse, also obtaining data measuring the same phenomenon for different spatial configurations is challenging in practice. Our contribution is to overcome this limitation by using Graph Neural Networks (GNNs) to represent the ignorance model. GNNs learn the spatial pattern of the missing physics even when the number of measurement locations is limited. This allows us to enrich the physics-based model with data-driven corrections without requiring dense spatial, temporal and parametric data. To showcase the performance of the proposed method, we evaluate this GNN-based hybrid twin on nonlinear heat transfer problems across different meshes, geometries, and load positions. Results show that the GNN successfully captures the ignorance and generalizes corrections across spatial configurations, improving simulation accuracy and interpretability, while minimizing data requirements.

</details>


### [124] [TS-DP: Reinforcement Speculative Decoding For Temporal Adaptive Diffusion Policy Acceleration](https://arxiv.org/abs/2512.15773)
*Ye Li,Jiahe Feng,Yuan Meng,Kangye Ji,Chen Tang,Xinwan Wen,Shutao Xia,Zhi Wang,Wenwu Zhu*

Main category: cs.LG

TL;DR: 提出基于时间感知强化学习的推测扩散策略 (TS-DP) 框架，使扩散策略具备时间适应性推测解码能力，能在不降低性能下提高推理速度。


<details>
  <summary>Details</summary>
Motivation: 扩散策略在具身控制推理时存在高延迟和高计算成本问题，静态有损加速方法无法处理动态具身任务，推测解码是无损且自适应的替代方案，但存在挑战。

Method: 提出 TS-DP 框架，蒸馏基于 Transformer 的草稿模型替代昂贵去噪调用，使用基于 RL 的调度器调整推测参数。

Result: 在不同具身环境实验中，TS-DP 推理速度达 4.17 倍，草稿接受率超 94%，推理频率达 25 Hz，且性能无下降。

Conclusion: TS-DP 能实现基于扩散模型的实时控制，且不降低性能，提高效率。

Abstract: Diffusion Policy (DP) excels in embodied control but suffers from high inference latency and computational cost due to multiple iterative denoising steps. The temporal complexity of embodied tasks demands a dynamic and adaptable computation mode. Static and lossy acceleration methods, such as quantization, fail to handle such dynamic embodied tasks, while speculative decoding offers a lossless and adaptive yet underexplored alternative for DP. However, it is non-trivial to address the following challenges: how to match the base model's denoising quality at lower cost under time-varying task difficulty in embodied settings, and how to dynamically and interactively adjust computation based on task difficulty in such environments. In this paper, we propose Temporal-aware Reinforcement-based Speculative Diffusion Policy (TS-DP), the first framework that enables speculative decoding for DP with temporal adaptivity. First, to handle dynamic environments where task difficulty varies over time, we distill a Transformer-based drafter to imitate the base model and replace its costly denoising calls. Second, an RL-based scheduler further adapts to time-varying task difficulty by adjusting speculative parameters to maintain accuracy while improving efficiency. Extensive experiments across diverse embodied environments demonstrate that TS-DP achieves up to 4.17 times faster inference with over 94% accepted drafts, reaching an inference frequency of 25 Hz and enabling real-time diffusion-based control without performance degradation.

</details>


### [125] [Adversarial Robustness in Financial Machine Learning: Defenses, Economic Impact, and Governance Evidence](https://arxiv.org/abs/2512.15780)
*Samruddhi Baviskar*

Main category: cs.LG

TL;DR: 评估金融决策表格机器学习模型的对抗鲁棒性，发现小扰动下性能下降，对抗训练可部分恢复。


<details>
  <summary>Details</summary>
Motivation: 评估金融决策中表格机器学习模型的对抗鲁棒性。

Method: 使用信用评分和欺诈检测数据，应用基于梯度的攻击，测量对歧视、校准和金融风险指标的影响。

Result: 小扰动下模型性能显著下降，对抗训练可部分恢复性能。

Conclusion: 模型在对抗攻击下性能会下降，但对抗训练有一定恢复效果。

Abstract: We evaluate adversarial robustness in tabular machine learning models used in financial decision making. Using credit scoring and fraud detection data, we apply gradient based attacks and measure impacts on discrimination, calibration, and financial risk metrics. Results show notable performance degradation under small perturbations and partial recovery through adversarial training.

</details>


### [126] [Boosting t-SNE Efficiency for Sequencing Data: Insights from Kernel Selection](https://arxiv.org/abs/2512.15900)
*Avais Jan,Prakash Chourasia,Sarwan Ali,Murray Patterson*

Main category: cs.LG

TL;DR: 文章对比九种核函数用于分子序列t - SNE降维，发现余弦相似性核函数整体表现最佳，适用于大规模生物序列分析。


<details>
  <summary>Details</summary>
Motivation: 传统的高斯核用于生物序列t - SNE降维有局限性，新提出的隔离核也不理想，因此需评估不同核函数用于生物序列降维的效果。

Method: 使用One - Hot Encoding、Spike2Vec和minimizers三种嵌入方法，综合评估九种核函数，通过主观可视化和客观指标（如邻域保留分数）评估，并进行广泛分类和聚类实验。

Result: 余弦相似性核函数在运行效率和低维空间成对距离保留上表现优于其他核函数，不同数据集和机器学习算法实验也证明其性能最佳。

Conclusion: 核函数选择显著影响可视化质量和下游分析任务，余弦相似性核函数性能稳健，适用于大规模生物序列分析。

Abstract: Dimensionality reduction techniques are essential for visualizing and analyzing high-dimensional biological sequencing data. t-distributed Stochastic Neighbor Embedding (t-SNE) is widely used for this purpose, traditionally employing the Gaussian kernel to compute pairwise similarities. However, the Gaussian kernel's lack of data-dependence and computational overhead limit its scalability and effectiveness for categorical biological sequences. Recent work proposed the isolation kernel as an alternative, yet it may not optimally capture sequence similarities. In this study, we comprehensively evaluate nine different kernel functions for t-SNE applied to molecular sequences, using three embedding methods: One-Hot Encoding, Spike2Vec, and minimizers. Through both subjective visualization and objective metrics (including neighborhood preservation scores), we demonstrate that the cosine similarity kernel in general outperforms other kernels, including Gaussian and isolation kernels, achieving superior runtime efficiency and better preservation of pairwise distances in low-dimensional space. We further validate our findings through extensive classification and clustering experiments across six diverse biological datasets (Spike7k, Host, ShortRead, Rabies, Genome, and Breast Cancer), employing multiple machine learning algorithms and evaluation metrics. Our results show that kernel selection significantly impacts not only visualization quality but also downstream analytical tasks, with the cosine similarity kernel providing the most robust performance across different data types and embedding strategies, making it particularly suitable for large-scale biological sequence analysis.

</details>


### [127] [A Unification of Discrete, Gaussian, and Simplicial Diffusion](https://arxiv.org/abs/2512.15923)
*Nuria Alina Chandra,Yucen Lily Li,Alan N. Amin,Alex Ali,Joshua Rollins,Sebastian W. Ober,Aniruddh Raghu,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: 本文构建理论统一三种离散扩散方法，连接模型似然和超参数，解锁稳定单纯形扩散，实验表明新方法更稳定且性能好。


<details>
  <summary>Details</summary>
Motivation: 现有三种离散序列扩散建模方法算法、理论结构和权衡不同，此前理论仅考虑特殊情况连接，期望统一框架让从业者切换模型。

Method: 构建理论将三种离散扩散方法统一为Wright - Fisher种群遗传学模型的不同参数化，找到单纯形和高斯扩散为大种群极限。

Result: Wright - Fisher单纯形扩散更稳定，在条件DNA生成上优于先前单纯形扩散模型；可训练在多域表现有竞争力的单模型。

Conclusion: 成功统一三种离散扩散方法，解决模型权衡问题，新方法性能表现良好。

Abstract: To model discrete sequences such as DNA, proteins, and language using diffusion, practitioners must choose between three major methods: diffusion in discrete space, Gaussian diffusion in Euclidean space, or diffusion on the simplex. Despite their shared goal, these models have disparate algorithms, theoretical structures, and tradeoffs: discrete diffusion has the most natural domain, Gaussian diffusion has more mature algorithms, and diffusion on the simplex in principle combines the strengths of the other two but in practice suffers from a numerically unstable stochastic processes. Ideally we could see each of these models as instances of the same underlying framework, and enable practitioners to switch between models for downstream applications. However previous theories have only considered connections in special cases. Here we build a theory unifying all three methods of discrete diffusion as different parameterizations of the same underlying process: the Wright-Fisher population genetics model. In particular, we find simplicial and Gaussian diffusion as two large-population limits. Our theory formally connects the likelihoods and hyperparameters of these models and leverages decades of mathematical genetics literature to unlock stable simplicial diffusion. Finally, we relieve the practitioner of balancing model trade-offs by demonstrating it is possible to train a single model that can perform diffusion in any of these three domains at test time. Our experiments show that Wright-Fisher simplicial diffusion is more stable and outperforms previous simplicial diffusion models on conditional DNA generation. We also show that we can train models on multiple domains at once that are competitive with models trained on any individual domain.

</details>


### [128] [DSO: Direct Steering Optimization for Bias Mitigation](https://arxiv.org/abs/2512.15926)
*Lucas Monteiro Paes,Nivedha Sivakumar,Yinong Oliver Wang,Masha Fedzechkina Donaldson,Luca Zappella,Nicholas Apostoloff*

Main category: cs.LG

TL;DR: 提出Direct Steering Optimization (DSO)方法，在VLMs和LLMs上实现公平性和性能的权衡，并提供推理时控制。


<details>
  <summary>Details</summary>
Motivation: 生成模型决策受输入人物人口属性影响产生偏差，现有激活控制方法难以纠正偏差，用户有平衡偏差缓解和模型整体能力的需求。

Method: 使用强化学习为激活控制找到线性变换的Direct Steering Optimization (DSO)方法。

Result: DSO在VLMs和LLMs上实现了公平性和能力之间的最优权衡，为从业者提供推理时的权衡控制。

Conclusion: 设计直接优化以控制模型行为的控制策略有益，比依赖预定义启发式方法的可控性方法能更有效地干预偏差。

Abstract: Generative models are often deployed to make decisions on behalf of users, such as vision-language models (VLMs) identifying which person in a room is a doctor to help visually impaired individuals. Yet, VLM decisions are influenced by the perceived demographic attributes of people in the input, which can lead to biased outcomes like failing to identify women as doctors. Moreover, when reducing bias leads to performance loss, users may have varying needs for balancing bias mitigation with overall model capabilities, highlighting the demand for methods that enable controllable bias reduction during inference. Activation steering is a popular approach for inference-time controllability that has shown potential in inducing safer behavior in large language models (LLMs). However, we observe that current steering methods struggle to correct biases, where equiprobable outcomes across demographic groups are required. To address this, we propose Direct Steering Optimization (DSO) which uses reinforcement learning to find linear transformations for steering activations, tailored to mitigate bias while maintaining control over model performance. We demonstrate that DSO achieves state-of-the-art trade-off between fairness and capabilities on both VLMs and LLMs, while offering practitioners inference-time control over the trade-off. Overall, our work highlights the benefit of designing steering strategies that are directly optimized to control model behavior, providing more effective bias intervention than methods that rely on pre-defined heuristics for controllability.

</details>


### [129] [BarcodeMamba+: Advancing State-Space Models for Fungal Biodiversity Research](https://arxiv.org/abs/2512.15931)
*Tiancheng Gao,Scott C. Lowe,Brendan Furneaux,Angel X Chang,Graham W. Taylor*

Main category: cs.LG

TL;DR: 提出BarcodeMamba+模型用于真菌条形码分类，采用预训练和微调范式，结合系列增强手段，在真菌分类基准测试中优于现有方法，为相关研究提供新工具并公开代码。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习方法在真菌条形码分类中因标签稀疏和长尾分类分布难以泛化及捕捉数据层次结构，有局限性。

Method: 引入BarcodeMamba+模型，采用预训练与微调范式，微调时集成并评估一系列增强措施（分层标签平滑、加权损失函数、MycoAI多头输出层）。

Result: 各项增强组件均提升性能，最终模型在具有分类分布偏移的真菌分类基准测试中，所有分类级别上均优于现有方法。

Conclusion: 为基于基因组的生物多样性研究提供有力新工具，为该领域建立有效且可扩展的训练范式。

Abstract: Accurate taxonomic classification from DNA barcodes is a cornerstone of global biodiversity monitoring, yet fungi present extreme challenges due to sparse labelling and long-tailed taxa distributions. Conventional supervised learning methods often falter in this domain, struggling to generalize to unseen species and to capture the hierarchical nature of the data. To address these limitations, we introduce BarcodeMamba+, a foundation model for fungal barcode classification built on a powerful and efficient state-space model architecture. We employ a pretrain and fine-tune paradigm, which utilizes partially labelled data and we demonstrate this is substantially more effective than traditional fully-supervised methods in this data-sparse environment. During fine-tuning, we systematically integrate and evaluate a suite of enhancements--including hierarchical label smoothing, a weighted loss function, and a multi-head output layer from MycoAI--to specifically tackle the challenges of fungal taxonomy. Our experiments show that each of these components yields significant performance gains. On a challenging fungal classification benchmark with distinct taxonomic distribution shifts from the broad training set, our final model outperforms a range of existing methods across all taxonomic levels. Our work provides a powerful new tool for genomics-based biodiversity research and establishes an effective and scalable training paradigm for this challenging domain. Our code is publicly available at https://github.com/bioscan-ml/BarcodeMamba.

</details>


### [130] [In-Context Semi-Supervised Learning](https://arxiv.org/abs/2512.15934)
*Jiashuo Fan,Paul Rosu,Aaron T. Wang,Michael Li,Lawrence Carin,Xiang Cheng*

Main category: cs.LG

TL;DR: 本文研究Transformer在上下文半监督学习（IC - SSL）中的能力，表明其可利用无标签上下文学习表示并提升低标签场景性能。


<details>
  <summary>Details</summary>
Motivation: 多数关于Transformer上下文学习（ICL）的理论聚焦监督设置，而实际中标签稀疏或缺失时Transformer表现良好，因此研究IC - SSL。

Method: 引入并研究IC - SSL，探究Transformer利用无标签上下文学习上下文相关的表示。

Result: Transformer能利用无标签上下文学习鲁棒的、上下文相关的表示，实现准确预测并提升低标签场景性能。

Conclusion: 研究为Transformer在ICL框架下利用无标签上下文进行表示学习提供了基础见解。

Abstract: There has been significant recent interest in understanding the capacity of Transformers for in-context learning (ICL), yet most theory focuses on supervised settings with explicitly labeled pairs. In practice, Transformers often perform well even when labels are sparse or absent, suggesting crucial structure within unlabeled contextual demonstrations. We introduce and study in-context semi-supervised learning (IC-SSL), where a small set of labeled examples is accompanied by many unlabeled points, and show that Transformers can leverage the unlabeled context to learn a robust, context-dependent representation. This representation enables accurate predictions and markedly improves performance in low-label regimes, offering foundational insights into how Transformers exploit unlabeled context for representation learning within the ICL framework.

</details>


### [131] [SALVE: Sparse Autoencoder-Latent Vector Editing for Mechanistic Control of Neural Networks](https://arxiv.org/abs/2512.15938)
*Vegard Flovik*

Main category: cs.LG

TL;DR: 提出SALVE框架连接机理可解释性和模型编辑，在多种模型验证且能实现可解释控制。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络表现出色但难解释和控制，需构建框架连接机理可解释性和模型编辑。

Method: 用ℓ₁正则化自动编码器学习特征基，用Grad - FAM验证特征，利用自动编码器结构进行权重空间干预，推导临界抑制阈值α₍crit₎。

Result: 在卷积和基于变压器的模型上验证，能对模型行为进行一致的、可解释的控制。

Conclusion: 该框架为将特征发现转化为可操作的模型编辑提供了有原则的方法，推动透明可控AI系统发展。

Abstract: Deep neural networks achieve impressive performance but remain difficult to interpret and control. We present SALVE (Sparse Autoencoder-Latent Vector Editing), a unified "discover, validate, and control" framework that bridges mechanistic interpretability and model editing. Using an $\ell_1$-regularized autoencoder, we learn a sparse, model-native feature basis without supervision. We validate these features with Grad-FAM, a feature-level saliency mapping method that visually grounds latent features in input data. Leveraging the autoencoder's structure, we perform precise and permanent weight-space interventions, enabling continuous modulation of both class-defining and cross-class features. We further derive a critical suppression threshold, $α_{crit}$, quantifying each class's reliance on its dominant feature, supporting fine-grained robustness diagnostics. Our approach is validated on both convolutional (ResNet-18) and transformer-based (ViT-B/16) models, demonstrating consistent, interpretable control over their behavior. This work contributes a principled methodology for turning feature discovery into actionable model edits, advancing the development of transparent and controllable AI systems.

</details>


### [132] [AIE4ML: An End-to-End Framework for Compiling Neural Networks for the Next Generation of AMD AI Engines](https://arxiv.org/abs/2512.15946)
*Dimitrios Danopoulos,Enrico Lupi,Chang Sun,Sebastian Dittmeier,Michael Kagan,Vladimir Loncar,Maurizio Pierini*

Main category: cs.LG

TL;DR: 提出AIE4ML框架，可将AI模型自动转换为针对AIE - ML设备的优化固件，在各层面有优化方法，实验表现佳。


<details>
  <summary>Details</summary>
Motivation: AMD的Versal AI Engine进行高效AI推理有挑战，先前工作未解决全神经网络在2D阵列执行问题。

Method: 提供结构化并行化方法，设计高效线性层实现，用新图放置和搜索算法得到优化布局，无缝接受量化模型。

Result: 层缩放基准测试达98.6%效率，利用97.4% AIE瓦片，数据全片上移动；评估显示能在微秒延迟下提供GPU级吞吐量。

Conclusion: AIE4ML适用于粒子物理实验触发系统等超低延迟环境。

Abstract: Efficient AI inference on AMD's Versal AI Engine (AIE) is challenging due to tightly coupled VLIW execution, explicit datapaths, and local memory management. Prior work focused on first-generation AIE kernel optimizations, without tackling full neural network execution across the 2D array. In this work, we present AIE4ML, the first comprehensive framework for converting AI models automatically into optimized firmware targeting the AIE-ML generation devices, also with forward compatibility for the newer AIE-MLv2 architecture. At the single-kernel level, we attain performance close to the architectural peak. At the graph and system levels, we provide a structured parallelization method that can scale across the 2D AIE-ML fabric and exploit its dedicated memory tiles to stay entirely on-chip throughout the model execution. As a demonstration, we designed a generalized and highly efficient linear-layer implementation with intrinsic support for fused bias addition and ReLU activation. Also, as our framework necessitates the generation of multi-layer implementations, our approach systematically derives deterministic, compact, and topology-optimized placements tailored to the physical 2D grid of the device through a novel graph placement and search algorithm. Finally, the framework seamlessly accepts quantized models imported from high-level tools such as hls4ml or PyTorch while preserving bit-exactness. In layer scaling benchmarks, we achieve up to 98.6% efficiency relative to the single-kernel baseline, utilizing 296 of 304 AIE tiles (97.4%) of the device with entirely on-chip data movement. With evaluations across real-world model topologies, we demonstrate that AIE4ML delivers GPU-class throughput under microsecond latency constraints, making it a practical companion for ultra-low-latency environments such as trigger systems in particle physics experiments.

</details>


### [133] [Governance by Evidence: Regulated Predictors in Decision-Tree Models](https://arxiv.org/abs/2512.15955)
*Alexios Veskoukis,Dimitris Kalles*

Main category: cs.LG

TL;DR: 研究用决策树论文分析合法数据使用，发现许多预测变量属监管类别，支持隐私保护方法。


<details>
  <summary>Details</summary>
Motivation: 决策树方法常用但使用的数据受隐私法监管，需了解实际使用中受监管数据情况。

Method: 收集决策树研究语料，将预测变量分类，与欧美隐私法条文关联，分析流行度、行业构成和时间模式。

Result: 许多预测变量属监管类别，医疗保健占比最大，不同行业有差异。

Conclusion: 研究支持隐私保护方法和治理检查，可指导决策树外的机器学习实践。

Abstract: Decision-tree methods are widely used on structured tabular data and are valued for interpretability across many sectors. However, published studies often list the predictors they use (for example age, diagnosis codes, location). Privacy laws increasingly regulate such data types. We use published decision-tree papers as a proxy for real-world use of legally governed data. We compile a corpus of decision-tree studies and assign each reported predictor to a regulated data category (for example health data, biometric identifiers, children's data, financial attributes, location traces, and government IDs). We then link each category to specific excerpts in European Union and United States privacy laws. We find that many reported predictors fall into regulated categories, with the largest shares in healthcare and clear differences across industries. We analyze prevalence, industry composition, and temporal patterns, and summarize regulation-aligned timing using each framework's reference year. Our evidence supports privacy-preserving methods and governance checks, and can inform ML practice beyond decision trees.

</details>


### [134] [Tracking Wildfire Assets with Commodity RFID and Gaussian Process Modeling](https://arxiv.org/abs/2512.15956)
*John Hateley,Sriram Narasimhan,Omid Abari*

Main category: cs.LG

TL;DR: 本文提出一种利用商用RFID追踪森林环境中资产的新方法，无需预先标记已知位置，能达GPS精度。


<details>
  <summary>Details</summary>
Motivation: 商用RFID系统在森林环境中存在标签定位差的问题，且现有指纹法需预先标记已知位置，本文旨在解决无法标记已知位置时的定位问题。

Method: 提出用高斯过程仅基于RF信号响应特征建模各种环境，采用加权对数似然法将未知环境与已建模环境匹配。

Result: 能够实现与GPS相当的定位精度，可同时追踪多个野火资产，无需预先标记已知位置，成本远低于GPS。

Conclusion: 所提方法在森林环境中追踪资产具有成本效益和可扩展性，能有效解决定位问题。

Abstract: This paper presents a novel, cost-effective, and scalable approach to track numerous assets distributed in forested environments using commodity Radio Frequency Identification (RFID) targeting wildfire response applications. Commodity RFID systems suffer from poor tag localization when dispersed in forested environments due to signal attenuation, multi-path effects and environmental variability. Current methods to address this issue via fingerprinting rely on dispersing tags at known locations {\em a priori}. In this paper, we address the case when it is not possible to tag known locations and show that it is possible to localize tags to accuracies comparable to global positioning systems (GPS) without such a constraint. For this, we propose Gaussian Process to model various environments solely based on RF signal response signatures and without the aid of additional sensors such as global positioning GPS or cameras, and match an unknown RF to the closest match in a model dictionary. We utilize a new weighted log-likelihood method to associate an unknown environment with the closest environment in a dictionary of previously modeled environments, which is a crucial step in being able to use our approach. Our results show that it is possible to achieve localization accuracies of the order of GPS, but with passive commodity RFID, which will allow the tracking of dozens of wildfire assets within the vicinity of mobile readers at-a-time simultaneously, does not require known positions to be tagged {\em a priori}, and can achieve localization at a fraction of the cost compared to GPS.

</details>


### [135] [Dynamic Rank Reinforcement Learning for Adaptive Low-Rank Multi-Head Self Attention in Large Language Models](https://arxiv.org/abs/2512.15973)
*Caner Erden*

Main category: cs.LG

TL;DR: 提出动态秩强化学习（DR - RL）框架，自适应优化大语言模型中多头自注意力的低秩分解，实验表明能在保持精度同时减少浮点运算。


<details>
  <summary>Details</summary>
Motivation: 传统低秩近似依赖静态秩假设，在不同输入上下文灵活性受限，需要一种更自适应的方法。

Method: 将强化学习与在线矩阵扰动理论结合，通过RL智能体将秩选择作为顺序策略优化问题，利用在线矩阵扰动边界进行增量秩更新，集成轻量级Transformer策略网络和批量奇异值分解操作。

Result: DR - RL在下游任务中精度与全秩注意力相当，能显著减少浮点运算，特别是在长序列场景。

Conclusion: 该工作弥合了多头自注意力中自适应效率和理论严谨性的差距，为资源受限深度学习中的启发式秩减少技术提供了基于数学原理的替代方案。

Abstract: We propose Dynamic Rank Reinforcement Learning (DR-RL), a novel framework that adaptively optimizes the low-rank factorization of Multi-Head Self-Attention (MHSA) in Large Language Models (LLMs) through the integration of reinforcement learning and online matrix perturbation theory. While traditional low-rank approximations often rely on static rank assumptions--limiting their flexibility across diverse input contexts--our method dynamically selects ranks based on real-time sequence dynamics, layer-specific sensitivities, and hardware constraints. The core innovation lies in an RL agent that formulates rank selection as a sequential policy optimization problem, where the reward function strictly balances attention fidelity against computational latency. Crucially, we employ online matrix perturbation bounds to enable incremental rank updates, thereby avoiding the prohibitive cost of full decomposition during inference. Furthermore, the integration of a lightweight Transformer-based policy network and batched Singular Value Decomposition (SVD) operations ensures scalable deployment on modern GPU architectures. Experiments demonstrate that DR-RL maintains downstream accuracy statistically equivalent to full-rank attention while significantly reducing Floating Point Operations (FLOPs), particularly in long-sequence regimes (L > 4096). This work bridges the gap between adaptive efficiency and theoretical rigor in MHSA, offering a principled, mathematically grounded alternative to heuristic rank reduction techniques in resource-constrained deep learning. Source code and experiment logs are available at: https://github.com/canererden/DR_RL_Project

</details>


### [136] [Higher-Order LaSDI: Reduced Order Modeling with Multiple Time Derivatives](https://arxiv.org/abs/2512.15997)
*Robert Stephany,William Michael Anderson,Youngsoo Choi*

Main category: cs.LG

TL;DR: 本文提出新方法解决降阶模型在长时间预测偏微分方程时的性能下降问题，并在二维Burgers方程上验证。


<details>
  <summary>Details</summary>
Motivation: 解决复杂偏微分方程需高计算成本数值方法，现代降阶模型长时间预测能力会下降。

Method: 引入灵活、高阶且低成本的有限差分方案，提出Rollout损失函数来训练降阶模型。

Result: 在二维Burgers方程上进行了方法验证。

Conclusion: 提出的方法可解决降阶模型长时间预测能力不足问题。

Abstract: Solving complex partial differential equations is vital in the physical sciences, but often requires computationally expensive numerical methods. Reduced-order models (ROMs) address this by exploiting dimensionality reduction to create fast approximations. While modern ROMs can solve parameterized families of PDEs, their predictive power degrades over long time horizons. We address this by (1) introducing a flexible, high-order, yet inexpensive finite-difference scheme and (2) proposing a Rollout loss that trains ROMs to make accurate predictions over arbitrary time horizons. We demonstrate our approach on the 2D Burgers equation.

</details>


### [137] [Surrogate Neural Architecture Codesign Package (SNAC-Pack)](https://arxiv.org/abs/2512.15998)
*Jason Weitz,Dmitri Demler,Benjamin Hawks,Nhan Tran,Javier Duarte*

Main category: cs.LG

TL;DR: 提出集成框架SNAC - Pack用于FPGA部署的神经网络自动发现和优化，在高能物理喷流分类任务验证，证明硬件感知搜索潜力并提供开源框架。


<details>
  <summary>Details</summary>
Motivation: 现有神经架构搜索方法难以准确优化真实硬件性能，常依赖代理指标。

Method: 结合神经架构协同设计多阶段搜索能力与资源利用和延迟估计器，实现多目标优化，无需为每个候选模型进行耗时综合。

Result: 在高能物理喷流分类任务达63.84%准确率，在FPGA上合成时与传统方法优化模型精度和资源利用率相当。

Conclusion: 硬件感知神经架构搜索在资源受限部署有潜力，提供开源框架用于高效FPGA加速模型自动设计。

Abstract: Neural Architecture Search is a powerful approach for automating model design, but existing methods struggle to accurately optimize for real hardware performance, often relying on proxy metrics such as bit operations. We present Surrogate Neural Architecture Codesign Package (SNAC-Pack), an integrated framework that automates the discovery and optimization of neural networks focusing on FPGA deployment. SNAC-Pack combines Neural Architecture Codesign's multi-stage search capabilities with the Resource Utilization and Latency Estimator, enabling multi-objective optimization across accuracy, FPGA resource utilization, and latency without requiring time-intensive synthesis for each candidate model. We demonstrate SNAC-Pack on a high energy physics jet classification task, achieving 63.84% accuracy with resource estimation. When synthesized on a Xilinx Virtex UltraScale+ VU13P FPGA, the SNAC-Pack model matches baseline accuracy while maintaining comparable resource utilization to models optimized using traditional BOPs metrics. This work demonstrates the potential of hardware-aware neural architecture search for resource-constrained deployments and provides an open-source framework for automating the design of efficient FPGA-accelerated models.

</details>


### [138] [Towards Fine-Tuning-Based Site Calibration for Knowledge-Guided Machine Learning: A Summary of Results](https://arxiv.org/abs/2512.16013)
*Ruolei Zeng,Arun Sharma,Shuai An,Mingzhou Yang,Shengya Zhang,Licheng Liu,David Mulla,Shashi Shekhar*

Main category: cs.LG

TL;DR: 提出FTBSC - KGML框架用于农业生态系统碳循环量化，利用迁移学习和空间异质性，效果优于纯全局模型。


<details>
  <summary>Details</summary>
Motivation: 准确且经济高效地量化农业生态系统碳循环很重要，但现有方法在迁移学习和利用空间变异性方面存在挑战，传统方法有局限性。

Method: 提出FTBSC - KGML框架，通过预训练 - 微调过程和特定地点参数，利用遥感GPP、气候和土壤协变量，采用空间异质性感知的迁移学习方案。

Result: FTBSC - KGML验证误差更低，解释力更一致，能更好捕捉各州间的空间变异性。

Conclusion: FTBSC - KGML框架有效，扩展了先前的SDSA - KGML框架。

Abstract: Accurate and cost-effective quantification of the agroecosystem carbon cycle at decision-relevant scales is essential for climate mitigation and sustainable agriculture. However, both transfer learning and the exploitation of spatial variability in this field are challenging, as they involve heterogeneous data and complex cross-scale dependencies. Conventional approaches often rely on location-independent parameterizations and independent training, underutilizing transfer learning and spatial heterogeneity in the inputs, and limiting their applicability in regions with substantial variability. We propose FTBSC-KGML (Fine-Tuning-Based Site Calibration-Knowledge-Guided Machine Learning), a pretraining- and fine-tuning-based, spatial-variability-aware, and knowledge-guided machine learning framework that augments KGML-ag with a pretraining-fine-tuning process and site-specific parameters. Using a pretraining-fine-tuning process with remote-sensing GPP, climate, and soil covariates collected across multiple midwestern sites, FTBSC-KGML estimates land emissions while leveraging transfer learning and spatial heterogeneity. A key component is a spatial-heterogeneity-aware transfer-learning scheme, which is a globally pretrained model that is fine-tuned at each state or site to learn place-aware representations, thereby improving local accuracy under limited data without sacrificing interpretability. Empirically, FTBSC-KGML achieves lower validation error and greater consistency in explanatory power than a purely global model, thereby better capturing spatial variability across states. This work extends the prior SDSA-KGML framework.

</details>


### [139] [Explainable AI in Big Data Fraud Detection](https://arxiv.org/abs/2512.16037)
*Ayush Jain,Rahul Kulkarni,Siyi Lin*

Main category: cs.LG

TL;DR: 文章探讨可解释人工智能（XAI）融入大数据分析管道用于欺诈检测和风险管理，分析工具和方法优缺点，指出研究差距并给出框架，最后给出研究方向。


<details>
  <summary>Details</summary>
Motivation: 大数据依赖自动化分析引发透明度、监管合规性和信任问题，需将 XAI 融入大数据分析进行欺诈检测和风险管理。

Method: 回顾关键大数据特征、调查主要分析工具、呈现常用 XAI 方法的结构化综述并分析其优缺点。

Result: 识别出可扩展性、实时处理和图与时间模型可解释性等研究差距，给出整合大数据基础设施和解释机制及人类反馈的概念框架。

Conclusion: 提出可扩展 XAI、隐私感知解释和可解释欺诈检测系统标准化评估方法等研究方向。

Abstract: Big Data has become central to modern applications in finance, insurance, and cybersecurity, enabling machine learning systems to perform large-scale risk assessments and fraud detection. However, the increasing dependence on automated analytics introduces important concerns about transparency, regulatory compliance, and trust. This paper examines how explainable artificial intelligence (XAI) can be integrated into Big Data analytics pipelines for fraud detection and risk management. We review key Big Data characteristics and survey major analytical tools, including distributed storage systems, streaming platforms, and advanced fraud detection models such as anomaly detectors, graph-based approaches, and ensemble classifiers. We also present a structured review of widely used XAI methods, including LIME, SHAP, counterfactual explanations, and attention mechanisms, and analyze their strengths and limitations when deployed at scale. Based on these findings, we identify key research gaps related to scalability, real-time processing, and explainability for graph and temporal models. To address these challenges, we outline a conceptual framework that integrates scalable Big Data infrastructure with context-aware explanation mechanisms and human feedback. The paper concludes with open research directions in scalable XAI, privacy-aware explanations, and standardized evaluation methods for explainable fraud detection systems.

</details>


### [140] [In-Context Multi-Operator Learning with DeepOSets](https://arxiv.org/abs/2512.16074)
*Shao-Ting Chiu,Aditya Nambiar,Ali Syed,Jonathan W. Siegel,Ulisses Braga-Neto*

Main category: cs.LG

TL;DR: 本文指出改进后的DeepOSets架构是多算子上下文学习器，能从提示中恢复新PDE的解算子，且是连续算子的通用一致逼近器，实验证明其对未训练过的PDE有预测能力。


<details>
  <summary>Details</summary>
Motivation: 探索非自回归、非注意力架构在上下文学习中的能力，研究能否从提示中恢复新PDE的解算子。

Method: 对DeepOSets架构进行适当修改，使其成为多算子上下文学习器，并证明其是连续算子的通用一致逼近器。

Result: DeepOSets能从提示中恢复新PDE的解算子，是连续算子的通用一致逼近器，实验证明其对未训练过的PDE有准确预测能力。

Conclusion: 改进后的DeepOSets架构在上下文学习中表现出色，可用于解决未训练过的PDE问题。

Abstract: In-context Learning (ICL) is the remarkable capability displayed by some machine learning models to learn from examples in a prompt, without any further weight updates. ICL had originally been thought to emerge from the self-attention mechanism in autoregressive transformer architectures. DeepOSets is a non-autoregressive, non-attention based neural architecture that combines set learning via the DeepSets architecture with operator learning via Deep Operator Networks (DeepONets). In a previous study, DeepOSets was shown to display ICL capabilities in supervised learning problems. In this paper, we show that the DeepOSets architecture, with the appropriate modifications, is a multi-operator in-context learner that can recover the solution operator of a new PDE, not seen during training, from example pairs of parameter and solution placed in a user prompt, without any weight updates. Furthermore, we show that DeepOSets is a universal uniform approximator over a class of continuous operators, which we believe is the first result of its kind in the literature of scientific machine learning. This means that a single DeepOSets architecture exists that approximates in-context any continuous operator in the class to any fixed desired degree accuracy, given an appropriate number of examples in the prompt. Experiments with Poisson and reaction-diffusion forward and inverse boundary-value problems demonstrate the ability of the proposed model to use in-context examples to predict accurately the solutions corresponding to parameter queries for PDEs not seen during training.

</details>


### [141] [Privacy Blur: Quantifying Privacy and Utility for Image Data Release](https://arxiv.org/abs/2512.16086)
*Saeed Mahloujifar,Narine Kokhlikyan,Chuan Guo,Kamalika Chaudhuri*

Main category: cs.LG

TL;DR: 研究图像数据隐私模糊处理方法，指出高斯模糊隐私性差，像素化和加噪像素化能兼顾隐私与实用性，并推出Privacy Blur软件包。


<details>
  <summary>Details</summary>
Motivation: 图像数据含隐私信息，数据发布需兼顾隐私保护和模型训练实用性。

Method: 研究高斯模糊、像素化、像素化和噪声添加（DP - Pix）、裁剪四种模糊算法，用反转和辨别攻击评估隐私性，用基于模糊人脸数据训练模型所得表征质量评估实用性。

Result: 最流行的高斯模糊隐私性最差，易受反转攻击；合适粒度的像素化和加噪像素化对多项计算机视觉任务能兼顾隐私与实用性。

Conclusion: 提出了兼顾隐私与实用性的模糊处理方法并在Privacy Blur软件包中提供相关方法和参数。

Abstract: Image data collected in the wild often contains private information such as faces and license plates, and responsible data release must ensure that this information stays hidden. At the same time, released data should retain its usefulness for model-training. The standard method for private information obfuscation in images is Gaussian blurring. In this work, we show that practical implementations of Gaussian blurring are reversible enough to break privacy. We then take a closer look at the privacy-utility tradeoffs offered by three other obfuscation algorithms -- pixelization, pixelization and noise addition (DP-Pix), and cropping. Privacy is evaluated by reversal and discrimination attacks, while utility by the quality of the learnt representations when the model is trained on data with obfuscated faces. We show that the most popular industry-standard method, Gaussian blur is the least private of the four -- being susceptible to reversal attacks in its practical low-precision implementations. In contrast, pixelization and pixelization plus noise addition, when used at the right level of granularity, offer both privacy and utility for a number of computer vision tasks. We make our proposed methods together with suggested parameters available in a software package called Privacy Blur.

</details>


### [142] [AIMM: An AI-Driven Multimodal Framework for Detecting Social-Media-Influenced Stock Market Manipulation](https://arxiv.org/abs/2512.16103)
*Sandeep Neela*

Main category: cs.LG

TL;DR: 提出AI驱动框架AIMM评估市场操纵风险，构建数据集，有初步效果并开源相关资源。


<details>
  <summary>Details</summary>
Motivation: 市场操纵源于社交媒体活动，零售投资者、监管者和券商需要能连接线上叙事、协调模式与市场行为的工具。

Method: 构建AIMM框架，融合Reddit活动等数据生成风险评分，使用parquet-native管道和Streamlit仪表盘；因API限制采用校准合成社交特征，市场数据用真实历史数据；构建AIMM - GT数据集，进行前向评估和预测记录。

Result: 当前标记集小，但有初步判别能力，能对GME事件提前预警。

Conclusion: 开源代码、数据集架构和仪表盘设计，支持社交媒体驱动的市场监控研究。

Abstract: Market manipulation now routinely originates from coordinated social media campaigns, not isolated trades. Retail investors, regulators, and brokerages need tools that connect online narratives and coordination patterns to market behavior. We present AIMM, an AI-driven framework that fuses Reddit activity, bot and coordination indicators, and OHLCV market features into a daily AIMM Manipulation Risk Score for each ticker.
  The system uses a parquet-native pipeline with a Streamlit dashboard that allows analysts to explore suspicious windows, inspect underlying posts and price action, and log model outputs over time. Due to Reddit API restrictions, we employ calibrated synthetic social features matching documented event characteristics; market data (OHLCV) uses real historical data from Yahoo Finance. This release makes three contributions. First, we build the AIMM Ground Truth dataset (AIMM-GT): 33 labeled ticker-days spanning eight equities, drawing from SEC enforcement actions, community-verified manipulation cases, and matched normal controls. Second, we implement forward-walk evaluation and prospective prediction logging for both retrospective and deployment-style assessment. Third, we analyze lead times and show that AIMM flagged GME 22 days before the January 2021 squeeze peak.
  The current labeled set is small (33 ticker-days, 3 positive events), but results show preliminary discriminative capability and early warnings for the GME incident. We release the code, dataset schema, and dashboard design to support research on social media-driven market surveillance.

</details>


### [143] [BUILD with Precision: Bottom-Up Inference of Linear DAGs](https://arxiv.org/abs/2512.16111)
*Hamed Ajorlou,Samuel Rey,Gonzalo Mateos,Geert Leus,Antonio G. Marques*

Main category: cs.LG

TL;DR: 提出BUILD算法从观测数据学习有向无环图（DAG）结构，在合成基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决从观测数据学习DAG结构的问题，利用线性高斯结构方程模型（SEM）下观测数据的集合精度矩阵的特性。

Method: 提出BUILD算法，确定叶节点及其父节点，修剪叶节点，周期性重新估计精度矩阵以提高鲁棒性。

Result: 在具有挑战性的合成基准测试中，BUILD算法与现有DAG学习算法相比表现良好。

Conclusion: BUILD算法能有效从观测数据中学习DAG结构，且能明确控制复杂度。

Abstract: Learning the structure of directed acyclic graphs (DAGs) from observational data is a central problem in causal discovery, statistical signal processing, and machine learning. Under a linear Gaussian structural equation model (SEM) with equal noise variances, the problem is identifiable and we show that the ensemble precision matrix of the observations exhibits a distinctive structure that facilitates DAG recovery. Exploiting this property, we propose BUILD (Bottom-Up Inference of Linear DAGs), a deterministic stepwise algorithm that identifies leaf nodes and their parents, then prunes the leaves by removing incident edges to proceed to the next step, exactly reconstructing the DAG from the true precision matrix. In practice, precision matrices must be estimated from finite data, and ill-conditioning may lead to error accumulation across BUILD steps. As a mitigation strategy, we periodically re-estimate the precision matrix (with less variables as leaves are pruned), trading off runtime for enhanced robustness. Reproducible results on challenging synthetic benchmarks demonstrate that BUILD compares favorably to state-of-the-art DAG learning algorithms, while offering an explicit handle on complexity.

</details>


### [144] [Dual-View Inference Attack: Machine Unlearning Amplifies Privacy Exposure](https://arxiv.org/abs/2512.16126)
*Lulu Xue,Shengshan Hu,Linqiang Qian,Peijin Guo,Yechao Zhang,Minghui Li,Yanjun Zhang,Dayong Ye,Leo Yu Zhang*

Main category: cs.LG

TL;DR: 本文聚焦机器学习去学习中保留数据隐私风险，揭示双视图场景漏洞，提出DVIA攻击验证风险。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注已去学习数据隐私，保留数据风险未被充分研究，需填补该空白。

Method: 从信息论角度引入‘隐私知识增益’概念，提出DVIA（双视图推理攻击），利用对原模型和去学习模型的黑盒查询提取保留数据成员信息。

Result: 不同数据集和模型架构的实验验证了DVIA的有效性。

Conclusion: 双视图设置下存在隐私泄露放大问题，机器学习去学习存在隐私风险。

Abstract: Machine unlearning is a newly popularized technique for removing specific training data from a trained model, enabling it to comply with data deletion requests. While it protects the rights of users requesting unlearning, it also introduces new privacy risks. Prior works have primarily focused on the privacy of data that has been unlearned, while the risks to retained data remain largely unexplored. To address this gap, we focus on the privacy risks of retained data and, for the first time, reveal the vulnerabilities introduced by machine unlearning under the dual-view setting, where an adversary can query both the original and the unlearned models. From an information-theoretic perspective, we introduce the concept of {privacy knowledge gain} and demonstrate that the dual-view setting allows adversaries to obtain more information than querying either model alone, thereby amplifying privacy leakage. To effectively demonstrate this threat, we propose DVIA, a Dual-View Inference Attack, which extracts membership information on retained data using black-box queries to both models. DVIA eliminates the need to train an attack model and employs a lightweight likelihood ratio inference module for efficient inference. Experiments across different datasets and model architectures validate the effectiveness of DVIA and highlight the privacy risks inherent in the dual-view setting.

</details>


### [145] [INTELLECT-3: Technical Report](https://arxiv.org/abs/2512.16144)
*Prime Intellect Team,Mika Senghaas,Fares Obeid,Sami Jaghouar,William Brown,Jack Min Ong,Daniel Auras,Matej Sirovatka,Jannik Straube,Andrew Baker,Sebastian Müller,Justus Mattern,Manveer Basra,Aiman Ismail,Dominik Scherm,Cooper Miller,Ameen Patel,Simon Kirsten,Mario Sieg,Christian Reetz,Kemal Erdem,Vincent Weisser,Johannes Hagemann*

Main category: cs.LG

TL;DR: 介绍106B参数的Mixture - of - Experts模型INTELLECT - 3，其在多基准测试中表现出色，开源模型与基础设施，还介绍prime - rl框架并在GLM - 4.5 - Air - Base上进行训练。


<details>
  <summary>Details</summary>
Motivation: 开发一个在数学、代码、科学和推理基准测试中表现出色的模型，并推动相关技术的开源发展。

Method: 使用端到端RL基础设施栈进行大规模强化学习训练，引入prime - rl框架用于大规模异步强化学习，在GLM - 4.5 - Air - Base模型上进行SFT和RL训练。

Result: INTELLECT - 3在同规模模型中达到了最先进水平，超越了许多更大的前沿模型，且能将RL训练扩展到512个H200s并保持高训练效率。

Conclusion: 通过开源模型和基础设施，以及引入的prime - rl框架，为大规模强化学习提供了有效的解决方案和工具。

Abstract: We present INTELLECT-3, a 106B-parameter Mixture-of-Experts model (12B active) trained with large-scale reinforcement learning on our end-to-end RL infrastructure stack. INTELLECT-3 achieves state of the art performance for its size across math, code, science and reasoning benchmarks, outperforming many larger frontier models. We open-source the model together with the full infrastructure stack used to create it, including RL frameworks, complete recipe, and a wide collection of environments, built with the verifiers library, for training and evaluation from our Environments Hub community platform. Built for this effort, we introduce prime-rl, an open framework for large-scale asynchronous reinforcement learning, which scales seamlessly from a single node to thousands of GPUs, and is tailored for agentic RL with first-class support for multi-turn interactions and tool use. Using this stack, we run both SFT and RL training on top of the GLM-4.5-Air-Base model, scaling RL training up to 512 H200s with high training efficiency.

</details>


### [146] [A Multimodal Approach to Alzheimer's Diagnosis: Geometric Insights from Cube Copying and Cognitive Assessments](https://arxiv.org/abs/2512.16184)
*Jaeho Yang,Kijung Yoon*

Main category: cs.LG

TL;DR: 提出多模态框架将手绘立方体草图转换为图结构表示用于阿尔茨海默病分类，实验表明该方法性能好且可解释。


<details>
  <summary>Details</summary>
Motivation: 早期且易获取的阿尔茨海默病检测是临床挑战，立方体临摹任务可评估视觉空间功能，故开展相关研究。

Method: 提出多模态框架，将立方体草图转换为图结构表示，用图神经网络处理，与人口统计学信息和神经心理学测试分数在后期融合模型中融合。

Result: 基于图的表示提供强单模态基线，显著优于基于像素的卷积模型，多模态集成进一步提高性能和对类别不平衡的鲁棒性，SHAP可解释性分析确定关键预测因素。

Conclusion: 基于图的立方体临摹分析是一种可解释、非侵入性和可扩展的阿尔茨海默病筛查方法。

Abstract: Early and accessible detection of Alzheimer's disease (AD) remains a critical clinical challenge, and cube-copying tasks offer a simple yet informative assessment of visuospatial function. This work proposes a multimodal framework that converts hand-drawn cube sketches into graph-structured representations capturing geometric and topological properties, and integrates these features with demographic information and neuropsychological test (NPT) scores for AD classification. Cube drawings are modeled as graphs with node features encoding spatial coordinates, local graphlet-based topology, and angular geometry, which are processed using graph neural networks and fused with age, education, and NPT features in a late-fusion model. Experimental results show that graph-based representations provide a strong unimodal baseline and substantially outperform pixel-based convolutional models, while multimodal integration further improves performance and robustness to class imbalance. SHAP-based interpretability analysis identifies specific graphlet motifs and geometric distortions as key predictors, closely aligning with clinical observations of disorganized cube drawings in AD. Together, these results establish graph-based analysis of cube copying as an interpretable, non-invasive, and scalable approach for Alzheimer's disease screening.

</details>


### [147] [A Multi-scale Fused Graph Neural Network with Inter-view Contrastive Learning for Spatial Transcriptomics Data Clustering](https://arxiv.org/abs/2512.16188)
*Jianping Mei,Siqi Ai,Ye Yuan*

Main category: cs.LG

TL;DR: 现有方法在处理空间转录组学数据识别空间域时存在局限，提出stMFG模型，在数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法采用“分别编码、后期融合”范式，限制了多尺度语义捕获和跨视图交互，难以处理复杂的基因 - 空间相互作用，识别空间域具有挑战性。

Method: 提出多尺度交互式融合图网络stMFG，引入层间跨视图注意力机制动态整合空间和基因特征，结合跨视图对比学习和空间约束。

Result: 在DLPFC和乳腺癌数据集上，stMFG优于现有方法，在某些切片上ARI提升达14%。

Conclusion: stMFG模型在空间转录组学数据处理中具有良好性能，能有效识别空间域。

Abstract: Spatial transcriptomics enables genome-wide expression analysis within native tissue context, yet identifying spatial domains remains challenging due to complex gene-spatial interactions. Existing methods typically process spatial and feature views separately, fusing only at output level - an "encode-separately, fuse-late" paradigm that limits multi-scale semantic capture and cross-view interaction. Accordingly, stMFG is proposed, a multi-scale interactive fusion graph network that introduces layer-wise cross-view attention to dynamically integrate spatial and gene features after each convolution. The model combines cross-view contrastive learning with spatial constraints to enhance discriminability while maintaining spatial continuity. On DLPFC and breast cancer datasets, stMFG outperforms state-of-the-art methods, achieving up to 14% ARI improvement on certain slices.

</details>


### [148] [Neural emulation of gravity-driven geohazard runout](https://arxiv.org/abs/2512.16221)
*Lorenzo Nava,Ye Chen,Maximillian Van Wyk de Vries*

Main category: cs.LG

TL;DR: 训练机器学习模型预测地质灾害径流，速度快且准确，为减灾和预警带来新机遇。


<details>
  <summary>Details</summary>
Motivation: 地质灾害径流预测对保护生命、基础设施和生态系统至关重要，但现有方法存在速度与真实性的权衡问题。

Method: 训练机器学习模型，在超 10 万个数值模拟和超 1 万个真实世界数字高程模型芯片上进行训练。

Result: 模型能高精度预测流场范围和沉积厚度，计算速度比数值求解器快 100 到 10000 倍，能重现关键物理行为。

Conclusion: 神经网络模拟为地质灾害建模扩展到大规模预警系统相关时空尺度提供了有前景的途径。

Abstract: Predicting geohazard runout is critical for protecting lives, infrastructure and ecosystems. Rapid mass flows, including landslides and avalanches, cause several thousand deaths across a wide range of environments, often travelling many kilometres from their source. The wide range of source conditions and material properties governing these flows makes their runout difficult to anticipate, particularly for downstream communities that may be suddenly exposed to severe impacts. Accurately predicting runout at scale requires models that are both physically realistic and computationally efficient, yet existing approaches face a fundamental speed-realism trade-off. Here we train a machine learning model to predict geohazard runout across representative real world terrains. The model predicts both flow extent and deposit thickness with high accuracy and 100 to 10,000 times faster computation than numerical solvers. It is trained on over 100,000 numerical simulations across over 10,000 real world digital elevation model chips and reproduces key physical behaviours, including avulsion and deposition patterns, while generalizing across different flow types, sizes and landscapes. Our results demonstrate that neural emulation enables rapid, spatially resolved runout prediction across diverse real world terrains, opening new opportunities for disaster risk reduction and impact-based forecasting. These results highlight neural emulation as a promising pathway for extending physically realistic geohazard modelling to spatial and temporal scales relevant for large scale early warning systems.

</details>


### [149] [Coarse-to-Fine Open-Set Graph Node Classification with Large Language Models](https://arxiv.org/abs/2512.16244)
*Xueqi Ma,Xingjun Ma,Sarah Monazam Erfani,Danilo Mandic,James Bailey*

Main category: cs.LG

TL;DR: 提出利用大语言模型（LLMs）的粗到细开放式分类（CFC）框架，用于图数据集的开放集分类，在 OOD 检测和分类上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有方法将所有 OOD 样本视为单类，而实际应用需要深入了解 OOD 样本及其可能标签，需探索无真实标签信息下将 OOD 检测扩展到 OOD 分类的方法。

Method: 提出 CFC 框架，包括使用 LLM 提示进行 OOD 检测和异常标签生成的粗分类器、基于粗分类器识别的 OOD 样本训练的 GNN 精分类器以及通过 LLM 提示和后处理 OOD 标签实现的精细 OOD 分类。

Result: CFC 在图和文本领域的 OOD 检测上比现有方法提高 10%，在图数据集的 OOD 分类上达到 70% 的准确率。

Conclusion: CFC 框架有效可行，使用语义 OOD 实例提高了可解释性和实用价值。

Abstract: Developing open-set classification methods capable of classifying in-distribution (ID) data while detecting out-of-distribution (OOD) samples is essential for deploying graph neural networks (GNNs) in open-world scenarios. Existing methods typically treat all OOD samples as a single class, despite real-world applications, especially high-stake settings such as fraud detection and medical diagnosis, demanding deeper insights into OOD samples, including their probable labels. This raises a critical question: can OOD detection be extended to OOD classification without true label information? To address this question, we propose a Coarse-to-Fine open-set Classification (CFC) framework that leverages large language models (LLMs) for graph datasets. CFC consists of three key components: a coarse classifier that uses LLM prompts for OOD detection and outlier label generation, a GNN-based fine classifier trained with OOD samples identified by the coarse classifier for enhanced OOD detection and ID classification, and refined OOD classification achieved through LLM prompts and post-processed OOD labels. Unlike methods that rely on synthetic or auxiliary OOD samples, CFC employs semantic OOD instances that are genuinely out-of-distribution based on their inherent meaning, improving interpretability and practical utility. Experimental results show that CFC improves OOD detection by ten percent over state-of-the-art methods on graph and text domains and achieves up to seventy percent accuracy in OOD classification on graph datasets.

</details>


### [150] [Sharpness-aware Federated Graph Learning](https://arxiv.org/abs/2512.16247)
*Ruiyu Li,Peige Zhao,Guangxia Li,Pengcheng Wu,Xingyu Gao,Zhiqiang Xu*

Main category: cs.LG

TL;DR: 提出SEAL算法解决联邦图学习中数据异质性问题，提升分类准确率和泛化能力，实验表现优于SOTA基线。


<details>
  <summary>Details</summary>
Motivation: 现有联邦图学习（FGL）系统存在数据异质性问题，现有解决方案有模型易陷入尖锐谷和局部图数据表示维度坍塌的问题。

Method: 制定考虑局部GNN模型尖锐度的优化目标，同时最小化损失函数和尖锐度；引入基于局部表示相关矩阵的正则化器，缓解维度坍塌。

Result: SEAL算法能增强局部GNN模型在联邦图学习中的分类准确率和泛化能力，在多个图分类基准测试中优于SOTA FGL基线。

Conclusion: SEAL算法有效解决了FGL中的数据异质性问题，为更多参与者带来收益。

Abstract: One of many impediments to applying graph neural networks (GNNs) to large-scale real-world graph data is the challenge of centralized training, which requires aggregating data from different organizations, raising privacy concerns. Federated graph learning (FGL) addresses this by enabling collaborative GNN model training without sharing private data. However, a core challenge in FGL systems is the variation in local training data distributions among clients, known as the data heterogeneity problem. Most existing solutions suffer from two problems: (1) The typical optimizer based on empirical risk minimization tends to cause local models to fall into sharp valleys and weakens their generalization to out-of-distribution graph data. (2) The prevalent dimensional collapse in the learned representations of local graph data has an adverse impact on the classification capacity of the GNN model. To this end, we formulate a novel optimization objective that is aware of the sharpness (i.e., the curvature of the loss surface) of local GNN models. By minimizing the loss function and its sharpness simultaneously, we seek out model parameters in a flat region with uniformly low loss values, thus improving the generalization over heterogeneous data. By introducing a regularizer based on the correlation matrix of local representations, we relax the correlations of representations generated by individual local graph samples, so as to alleviate the dimensional collapse of the learned model. The proposed \textbf{S}harpness-aware f\textbf{E}derated gr\textbf{A}ph \textbf{L}earning (SEAL) algorithm can enhance the classification accuracy and generalization ability of local GNN models in federated graph learning. Experimental studies on several graph classification benchmarks show that SEAL consistently outperforms SOTA FGL baselines and provides gains for more participants.

</details>


### [151] [Sharpness-aware Second-order Latent Factor Model for High-dimensional and Incomplete Data](https://arxiv.org/abs/2512.16277)
*Jialiang Wang,Xueyan Bao,Hao Wu*

Main category: cs.LG

TL;DR: 提出Sharpness - aware SLF (SSLF)模型解决SLF模型优化难题，实验显示其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: SLF模型因双线性和非凸特性优化困难，而SAM可在最小化非凸目标时找到平坦局部最小值，故提出SSLF模型解决优化挑战。

Method: SSLF模型包含两个关键思想：通过Hessian - vector products获取二阶信息；通过设计的Hessian - vector products将锐度项注入曲率（Hessian）。

Result: 在多个工业数据集上的实验表明，提出的模型始终优于最先进的基线。

Conclusion: 提出的SSLF模型有效，能解决SLF模型优化难题并取得更好性能。

Abstract: Second-order Latent Factor (SLF) model, a class of low-rank representation learning methods, has proven effective at extracting node-to-node interaction patterns from High-dimensional and Incomplete (HDI) data. However, its optimization is notoriously difficult due to its bilinear and non-convex nature. Sharpness-aware Minimization (SAM) has recently proposed to find flat local minima when minimizing non-convex objectives, thereby improving the generalization of representation-learning models. To address this challenge, we propose a Sharpness-aware SLF (SSLF) model. SSLF embodies two key ideas: (1) acquiring second-order information via Hessian-vector products; and (2) injecting a sharpness term into the curvature (Hessian) through the designed Hessian-vector products. Experiments on multiple industrial datasets demonstrate that the proposed model consistently outperforms state-of-the-art baselines.

</details>


### [152] [CKA-Guided Modular Quantization: Beyond Bit-Width to Algorithmic Diversity](https://arxiv.org/abs/2512.16282)
*Jinhao Zhang,Yunquan Zhang,Daning Chen*

Main category: cs.LG

TL;DR: 提出CKA Guided Modular Quantization框架用于算法异构量化，实验表明其性能优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前主流大语言模型训练后量化方法对各层采用统一量化策略，忽略层间算法适用性差异。

Method: 独立评估每层的多种PTQ算法，用线性中心核对齐（CKA）作为指标自动选择每层的最优量化策略，整合各层策略构建混合量化模型。

Result: 在主流大语言模型上，该方法在困惑度和下游任务性能方面始终优于统一量化基线和最先进的混合精度方法。

Conclusion: CKA Guided Modular Quantization框架是有效的，能提升大语言模型量化性能。

Abstract: Current mainstream post-training quantization methods for large language models typically apply a uniform quantization strategy across all network layers, overlooking the substantial differences in algorithmic suitability among layers. To address this limitation, we propose CKA Guided Modular Quantization, a fine-tuning-free, plug-and-play framework for algorithmic heterogeneous quantization. Our method independently evaluates multiple PTQ algorithms on each layer and employs Linear Centered Kernel Alignment (CKA) as a metric to automatically select the optimal quantization strategy per layer. The individually optimized strategies are then integrated to construct a hybrid quantized model. Experiments demonstrate that our approach consistently outperforms both uniform quantization baselines and state-of-the-art mixed-precision methods across mainstream LLMs including LLaMA and Qwen ,in terms of perplexity (PPL) and downstream task performance.

</details>


### [153] [Feature-Selective Representation Misdirection for Machine Unlearning](https://arxiv.org/abs/2512.16297)
*Taozhao Chen,Linghan Huang,Kim-Kwang Raymond Choo,Huaming Chen*

Main category: cs.LG

TL;DR: 针对大语言模型敏感知识保留风险，提出SRMU激活编辑框架，实验显示其有良好去学习性能且效用损失小，为模型治理等提供基础。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在关键领域应用时保留敏感知识有诸多风险，现有去学习技术在数据分布高度纠缠场景有局限。

Method: 提出Selective Representation Misdirection for Unlearning (SRMU)激活编辑框架，采用带激活重要性图的结构化误导向量。

Result: 在WMDP基准测试中，SRMU去学习性能达最优，效用损失最小，在20 - 30%重叠情况下仍有效。

Conclusion: SRMU为基于大语言模型的应用提供了安全驱动的模型治理、隐私合规和可控知识移除的坚实基础。

Abstract: As large language models (LLMs) are increasingly adopted in safety-critical and regulated sectors, the retention of sensitive or prohibited knowledge introduces escalating risks, ranging from privacy leakage to regulatory non-compliance to to potential misuse, and so on. Recent studies suggest that machine unlearning can help ensure deployed models comply with evolving legal, safety, and governance requirements. However, current unlearning techniques assume clean separation between forget and retain datasets, which is challenging in operational settings characterized by highly entangled distributions. In such scenarios, perturbation-based methods often degrade general model utility or fail to ensure safety. To address this, we propose Selective Representation Misdirection for Unlearning (SRMU), a novel principled activation-editing framework that enforces feature-aware and directionally controlled perturbations. Unlike indiscriminate model weights perturbations, SRMU employs a structured misdirection vector with an activation importance map. The goal is to allow SRMU selectively suppresses harmful representations while preserving the utility on benign ones. Experiments are conducted on the widely used WMDP benchmark across low- and high-entanglement configurations. Empirical results reveal that SRMU delivers state-of-the-art unlearning performance with minimal utility losses, and remains effective under 20-30\% overlap where existing baselines collapse. SRMU provides a robust foundation for safety-driven model governance, privacy compliance, and controlled knowledge removal in the emerging LLM-based applications. We release the replication package at https://figshare.com/s/d5931192a8824de26aff.

</details>


### [154] [Pretrained Battery Transformer (PBT): A battery life prediction foundation model](https://arxiv.org/abs/2512.16334)
*Ruifeng Tan,Weixiang Hong,Jia Li,Jiaqiang Huang,Tong-Yi Zhang*

Main category: cs.LG

TL;DR: 提出首个电池寿命预测基础模型PBT，在多个数据集上表现优异，为电池寿命预测奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法因数据稀缺和异质性在电池循环寿命预测上进展受阻，且尚无用于此的基础模型。

Method: 通过领域知识编码的专家混合层开发PBT模型。

Result: 在最大公共电池寿命数据库上验证，PBT从13个锂离子电池数据集学习可迁移表示，平均比现有模型性能高19.8%，通过迁移学习在15个不同数据集上达到了最先进水平。

Conclusion: 为电池寿命预测建立了基础模型路径，迈向通用电池寿命预测系统。

Abstract: Early prediction of battery cycle life is essential for accelerating battery research, manufacturing, and deployment. Although machine learning methods have shown encouraging results, progress is hindered by data scarcity and heterogeneity arising from diverse aging conditions. In other fields, foundation models (FMs) trained on diverse datasets have achieved broad generalization through transfer learning, but no FMs have been reported for battery cycle life prediction yet. Here we present the Pretrained Battery Transformer (PBT), the first FM for battery life prediction, developed through domain-knowledge-encoded mixture-of-expert layers. Validated on the largest public battery life database, PBT learns transferable representations from 13 lithium-ion battery (LIB) datasets, outperforming existing models by an average of 19.8%. With transfer learning, PBT achieves state-of-the-art performance across 15 diverse datasets encompassing various operating conditions, formation protocols, and chemistries of LIBs. This work establishes a foundation model pathway for battery lifetime prediction, paving the way toward universal battery lifetime prediction systems.

</details>


### [155] [Quantitative Verification of Fairness in Tree Ensembles](https://arxiv.org/abs/2512.16386)
*Zhenjiang Zhao,Takahisa Toda,Takashi Kitamura*

Main category: cs.LG

TL;DR: 本文聚焦树集成模型公平性的定量验证，提出高效量化技术，实验证明其有效性和高效性，用于公平性测试时优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 传统验证方法仅返回单个反例，定量验证能估计反例比例和出现区域，对诊断和缓解偏差很重要，但目前主要用于深度神经网络，且现有框架有局限性。

Method: 利用树集成的离散结构，提出能提供任意时间上下界的高效量化技术。

Result: 在五个常用数据集上的实验证明了方法的有效性和高效性，用于公平性测试时显著优于现有技术。

Conclusion: 所提出的高效量化技术在树集成模型公平性定量验证方面表现出色，具有实际应用价值。

Abstract: This work focuses on quantitative verification of fairness in tree ensembles. Unlike traditional verification approaches that merely return a single counterexample when the fairness is violated, quantitative verification estimates the ratio of all counterexamples and characterizes the regions where they occur, which is important information for diagnosing and mitigating bias. To date, quantitative verification has been explored almost exclusively for deep neural networks (DNNs). Representative methods, such as DeepGemini and FairQuant, all build on the core idea of Counterexample-Guided Abstraction Refinement, a generic framework that could be adapted to other model classes. We extended the framework into a model-agnostic form, but discovered two limitations: (i) it can provide only lower bounds, and (ii) its performance scales poorly. Exploiting the discrete structure of tree ensembles, our work proposes an efficient quantification technique that delivers any-time upper and lower bounds. Experiments on five widely used datasets demonstrate its effectiveness and efficiency. When applied to fairness testing, our quantification method significantly outperforms state-of-the-art testing techniques.

</details>


### [156] [NDRL: Cotton Irrigation and Nitrogen Application with Nested Dual-Agent Reinforcement Learning](https://arxiv.org/abs/2512.16408)
*Ruifeng Xu,Liang He*

Main category: cs.LG

TL;DR: 文章指出现有水氮组合优化研究存在局限，提出NDRL方法，用DSSAT模拟验证，结果显示该方法提升了产量和资源效率，推动棉花灌溉与施肥发展。


<details>
  <summary>Details</summary>
Motivation: 现有研究在优化水氮组合时复杂度高、产量优化结果差，且难以量化轻度胁迫信号、反馈延迟，导致资源利用效率低。

Method: 提出NDRL方法，父智能体基于预计累积产量效益确定宏观灌溉和施肥行动，子智能体奖励函数结合量化的WSF和NSF，用混合概率分布优化每日策略，用DSSAT模拟并与NDRL交互。

Result: 与最佳基线相比，2023和2024年模拟产量均提高4.7%，灌溉水生产率分别提高5.6%和5.1%，氮素偏生产力分别提高6.3%和1.0%。

Conclusion: 该方法推动了棉花灌溉和施肥发展，为农业资源管理复杂性和精准性问题及可持续农业发展提供新思路。

Abstract: Effective irrigation and nitrogen fertilization have a significant impact on crop yield. However, existing research faces two limitations: (1) the high complexity of optimizing water-nitrogen combinations during crop growth and poor yield optimization results; and (2) the difficulty in quantifying mild stress signals and the delayed feedback, which results in less precise dynamic regulation of water and nitrogen and lower resource utilization efficiency. To address these issues, we propose a Nested Dual-Agent Reinforcement Learning (NDRL) method. The parent agent in NDRL identifies promising macroscopic irrigation and fertilization actions based on projected cumulative yield benefits, reducing ineffective explorationwhile maintaining alignment between objectives and yield. The child agent's reward function incorporates quantified Water Stress Factor (WSF) and Nitrogen Stress Factor (NSF), and uses a mixed probability distribution to dynamically optimize daily strategies, thereby enhancing both yield and resource efficiency. We used field experiment data from 2023 and 2024 to calibrate and validate the Decision Support System for Agrotechnology Transfer (DSSAT) to simulate real-world conditions and interact with NDRL. Experimental results demonstrate that, compared to the best baseline, the simulated yield increased by 4.7% in both 2023 and 2024, the irrigation water productivity increased by 5.6% and 5.1% respectively, and the nitrogen partial factor productivity increased by 6.3% and 1.0% respectively. Our method advances the development of cotton irrigation and nitrogen fertilization, providing new ideas for addressing the complexity and precision issues in agricultural resource management and for sustainable agricultural development.

</details>


### [157] [Geometric Laplace Neural Operator](https://arxiv.org/abs/2512.16409)
*Hao Tang,Jiongyu Zhu,Zimeng Feng,Hao Li,Chao Li*

Main category: cs.LG

TL;DR: 提出基于极点 - 留数分解和指数基函数的广义算子学习框架，引入几何拉普拉斯神经算子及网络架构，实验表现优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 现有神经算子方法在处理非周期激励、瞬态响应和不规则或非欧几何上的信号时存在困难。

Method: 提出基于极点 - 留数分解和指数基函数的广义算子学习框架，引入几何拉普拉斯神经算子（GLNO）并嵌入拉普拉斯 - 贝尔特拉米算子的特征基，设计网格不变网络架构（GLNONet）。

Result: 在PDEs/ODEs和真实世界数据集上的大量实验中，表现优于其他先进模型。

Conclusion: 所提方法能有效解决现有神经算子的不足，可扩展算子学习到任意黎曼流形。

Abstract: Neural operators have emerged as powerful tools for learning mappings between function spaces, enabling efficient solutions to partial differential equations across varying inputs and domains. Despite the success, existing methods often struggle with non-periodic excitations, transient responses, and signals defined on irregular or non-Euclidean geometries. To address this, we propose a generalized operator learning framework based on a pole-residue decomposition enriched with exponential basis functions, enabling expressive modeling of aperiodic and decaying dynamics. Building on this formulation, we introduce the Geometric Laplace Neural Operator (GLNO), which embeds the Laplace spectral representation into the eigen-basis of the Laplace-Beltrami operator, extending operator learning to arbitrary Riemannian manifolds without requiring periodicity or uniform grids. We further design a grid-invariant network architecture (GLNONet) that realizes GLNO in practice. Extensive experiments on PDEs/ODEs and real-world datasets demonstrate our robust performance over other state-of-the-art models.

</details>


### [158] [Multi-Fidelity Delayed Acceptance: hierarchical MCMC sampling for Bayesian inverse problems combining multiple solvers through deep neural networks](https://arxiv.org/abs/2512.16430)
*Filippo Zacchei,Paolo Conti,Attilio Alberto Frangi,Andrea Manzoni*

Main category: cs.LG

TL;DR: 本文提出适用于贝叶斯反问题的多保真延迟接受方案，结合多保真神经网络，在两个基准反问题上验证可节省计算成本。


<details>
  <summary>Details</summary>
Motivation: 基于物理模型的逆不确定性量化计算需求大，传统采样方法不可行，数据驱动代理模型依赖高保真数据且生成成本高，低保真数据单独使用会降低精度。

Method: 提出多保真延迟接受方案，扩展多级延迟接受框架，引入多保真神经网络结合不同保真度求解器的预测，将高保真评估限制在离线训练阶段，在线阶段用粗求解器评估并将输出传递给训练好的神经网络。

Result: 提出的方法能提高低保真求解器的近似精度，使子链长度更长、混合更好、后验推断加速，在两个基准反问题上节省了大量计算成本。

Conclusion: 所提策略有效，可在逆不确定性量化任务中节省计算资源。

Abstract: Inverse uncertainty quantification (UQ) tasks such as parameter estimation are computationally demanding whenever dealing with physics-based models, and typically require repeated evaluations of complex numerical solvers. When partial differential equations are involved, full-order models such as those based on the Finite Element Method can make traditional sampling approaches like Markov Chain Monte Carlo (MCMC) computationally infeasible. Although data-driven surrogate models may help reduce evaluation costs, their utility is often limited by the expense of generating high-fidelity data. In contrast, low-fidelity data can be produced more efficiently, although relying on them alone may degrade the accuracy of the inverse UQ solution.
  To address these challenges, we propose a Multi-Fidelity Delayed Acceptance scheme for Bayesian inverse problems. Extending the Multi-Level Delayed Acceptance framework, the method introduces multi-fidelity neural networks that combine the predictions of solvers of varying fidelity, with high fidelity evaluations restricted to an offline training stage. During the online phase, likelihood evaluations are obtained by evaluating the coarse solvers and passing their outputs to the trained neural networks, thereby avoiding additional high-fidelity simulations.
  This construction allows heterogeneous coarse solvers to be incorporated consistently within the hierarchy, providing greater flexibility than standard Multi-Level Delayed Acceptance. The proposed approach improves the approximation accuracy of the low fidelity solvers, leading to longer sub-chain lengths, better mixing, and accelerated posterior inference. The effectiveness of the strategy is demonstrated on two benchmark inverse problems involving (i) steady isotropic groundwater flow, (ii) an unsteady reaction-diffusion system, for which substantial computational savings are obtained.

</details>


### [159] [Emergent Bias and Fairness in Multi-Agent Decision Systems](https://arxiv.org/abs/2512.16433)
*Maeve Madigan,Parameswaran Kamalaruban,Glenn Moynihan,Tom Kempton,David Sutton,Stuart Burrell*

Main category: cs.LG

TL;DR: 本文指出多智能体系统缺乏有效评估方法，通过大规模模拟研究金融领域公平性，发现新兴偏差模式，强调应整体评估系统。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统缺乏有效评估方法，在高风险领域部署不安全，需开发公平性评估方法。

Method: 通过大规模模拟，在不同多智能体配置、通信和协作机制下检查公平性指标。

Result: 揭示了金融决策中无法追溯到单个智能体组件的新兴偏差模式，表明多智能体系统有集体行为。

Conclusion: 金融多智能体系统的公平性风险是模型风险重要组成部分，应将多智能体决策系统作为整体评估。

Abstract: Multi-agent systems have demonstrated the ability to improve performance on a variety of predictive tasks by leveraging collaborative decision making. However, the lack of effective evaluation methodologies has made it difficult to estimate the risk of bias, making deployment of such systems unsafe in high stakes domains such as consumer finance, where biased decisions can translate directly into regulatory breaches and financial loss. To address this challenge, we need to develop fairness evaluation methodologies for multi-agent predictive systems and measure the fairness characteristics of these systems in the financial tabular domain. Examining fairness metrics using large-scale simulations across diverse multi-agent configurations, with varying communication and collaboration mechanisms, we reveal patterns of emergent bias in financial decision-making that cannot be traced to individual agent components, indicating that multi-agent systems may exhibit genuinely collective behaviors. Our findings highlight that fairness risks in financial multi-agent systems represent a significant component of model risk, with tangible impacts on tasks such as credit scoring and income estimation. We advocate that multi-agent decision systems must be evaluated as holistic entities rather than through reductionist analyses of their constituent components.

</details>


### [160] [A Novel Proposal in Wind Turbine Blade Failure Detection: An Integrated Approach to Energy Efficiency and Sustainability](https://arxiv.org/abs/2512.16437)
*Jordan Abarca-Albores,Danna Cristina Gutiérrez Cabrera,Luis Antonio Salazar-Licea,Dante Ruiz-Robles,Jesus Alejandro Franco,Alberto-Jesus Perea-Moreno,David Muñoz-Rodríguez,Quetzalcoatl Hernandez-Escobedo*

Main category: cs.LG

TL;DR: 本文提出用计算学习技术检测风力涡轮机叶片故障的新方法，评估两种模型，结果显示聚类或更能捕捉数据特征，强调方法实用性，未来将结合方法提升检测精度并拓展应用。


<details>
  <summary>Details</summary>
Motivation: 寻找风力涡轮机叶片早期故障检测的新方法，提高系统可靠性。

Method: 评估两种模型，一是用逻辑回归，二是利用聚类。

Result: 逻辑回归优于神经网络、决策树和朴素贝叶斯方法；聚类在精度和数据分割上表现更好，可能更能捕捉数据特征。

Conclusion: 所提方法为早期故障检测提供新途径，强调集成不同计算学习技术可提升系统可靠性及方法的实用性，未来将结合方法提升精度并拓展应用。

Abstract: This paper presents a novel methodology for detecting faults in wind turbine blades using com-putational learning techniques. The study evaluates two models: the first employs logistic regression, which outperformed neural networks, decision trees, and the naive Bayes method, demonstrating its effectiveness in identifying fault-related patterns. The second model leverages clustering and achieves superior performance in terms of precision and data segmentation. The results indicate that clustering may better capture the underlying data characteristics compared to supervised methods. The proposed methodology offers a new approach to early fault detection in wind turbine blades, highlighting the potential of integrating different computational learning techniques to enhance system reliability. The use of accessible tools like Orange Data Mining underscores the practical application of these advanced solutions within the wind energy sector. Future work will focus on combining these methods to improve detection accuracy further and extend the application of these techniques to other critical components in energy infrastructure.

</details>


### [161] [IoMT-based Automated Leukemia Classification using CNN and Higher Order Singular Value](https://arxiv.org/abs/2512.16448)
*Shabnam Bagheri Marzijarani,Mohammad Zolfaghari,Hedieh Sajedi*

Main category: cs.LG

TL;DR: 本文将CNN与HOSVD分类器结合，在IoMT结构上识别白血病细胞，在ALL - IDB2数据集测试中达98.88%准确率。


<details>
  <summary>Details</summary>
Motivation: 急性淋巴细胞白血病（ALL）人工诊断有局限性，需基于人工智能的方法来识别癌细胞。

Method: 应用卷积神经网络（CNN）和高阶奇异值分解（HOSVD）分类器，在IoMT结构上识别白血病细胞。

Result: 模型在ALL - IDB2数据集测试步骤中平均准确率达98.88%。

Conclusion: 新的白血病分类框架可实现患者和临床医生实时通信，能快速安全识别白血病。

Abstract: The Internet of Things (IoT) is a concept by which objects find identity and can communicate with each other in a network. One of the applications of the IoT is in the field of medicine, which is called the Internet of Medical Things (IoMT). Acute Lymphocytic Leukemia (ALL) is a type of cancer categorized as a hematic disease. It usually begins in the bone marrow due to the overproduction of immature White Blood Cells (WBCs or leukocytes). Since it has a high rate of spread to other body organs, it is a fatal disease if not diagnosed and treated early. Therefore, for identifying cancerous (ALL) cells in medical diagnostic laboratories, blood, as well as bone marrow smears, are taken by pathologists. However, manual examinations face limitations due to human error risk and time-consuming procedures. So, to tackle the mentioned issues, methods based on Artificial Intelligence (AI), capable of identifying cancer from non-cancer tissue, seem vital. Deep Neural Networks (DNNs) are the most efficient machine learning (ML) methods. These techniques employ multiple layers to extract higher-level features from the raw input. In this paper, a Convolutional Neural Network (CNN) is applied along with a new type of classifier, Higher Order Singular Value Decomposition (HOSVD), to categorize ALL and normal (healthy) cells from microscopic blood images. We employed the model on IoMT structure to identify leukemia quickly and safely. With the help of this new leukemia classification framework, patients and clinicians can have real-time communication. The model was implemented on the Acute Lymphoblastic Leukemia Image Database (ALL-IDB2) and achieved an average accuracy of %98.88 in the test step.

</details>


### [162] [Batch Normalization-Free Fully Integer Quantized Neural Networks via Progressive Tandem Learning](https://arxiv.org/abs/2512.16476)
*Pengfei Sun,Wenyu Jiang,Piew Yoong Chee,Paul Devos,Dick Botteldooren*

Main category: cs.LG

TL;DR: 提出一种无 BN 层的全整数量化神经网络，通过逐层蒸馏训练，在 ImageNet 上取得有竞争力的精度，可实现端到端整数推理。


<details>
  <summary>Details</summary>
Motivation: 现有量化神经网络大多依赖 BN 层，无法实现真正的全整数部署，之前移除 BN 层的方法难以恢复其稳定性和精度。

Method: 采用渐进式、逐层蒸馏方案，从预训练的含 BN 层教师模型出发，使用逐层目标和渐进补偿训练无 BN 层的学生模型。

Result: 在 ImageNet 上使用 AlexNet 模型，无 BN 层模型在激进量化下达到有竞争力的 Top-1 精度。

Conclusion: 该方法可直接集成到标准量化流程，适用于资源受限的边缘和嵌入式设备，实现端到端整数推理。

Abstract: Quantised neural networks (QNNs) shrink models and reduce inference energy through low-bit arithmetic, yet most still depend on a running statistics batch normalisation (BN) layer, preventing true integer-only deployment. Prior attempts remove BN by parameter folding or tailored initialisation; while helpful, they rarely recover BN's stability and accuracy and often impose bespoke constraints. We present a BN-free, fully integer QNN trained via a progressive, layer-wise distillation scheme that slots into existing low-bit pipelines. Starting from a pretrained BN-enabled teacher, we use layer-wise targets and progressive compensation to train a student that performs inference exclusively with integer arithmetic and contains no BN operations. On ImageNet with AlexNet, the BN-free model attains competitive Top-1 accuracy under aggressive quantisation. The procedure integrates directly with standard quantisation workflows, enabling end-to-end integer-only inference for resource-constrained settings such as edge and embedded devices.

</details>


### [163] [Persistent Multiscale Density-based Clustering](https://arxiv.org/abs/2512.16558)
*Daniël Bot,Leland McInnes,Jan Aerts*

Main category: cs.LG

TL;DR: 提出新的基于密度的聚类算法PLSCAN，与HDBSCAN*和k - Means对比，展示其性能和计算成本优势。


<details>
  <summary>Details</summary>
Motivation: 现有基于密度的聚类算法在实践中需选择合适超参数，缺乏数据分布先验知识时较困难。

Method: 提出PLSCAN算法，运用尺度空间聚类原则，等价于新度量空间上的持久同调。

Result: 在多个真实数据集上，PLSCAN平均ARI更高，对互可达邻居数量变化不敏感；在低维数据集上运行时间与k - Means有竞争力，高维时与HDBSCAN*更相似。

Conclusion: PLSCAN是一种有效的聚类算法，在性能和计算成本上有一定优势。

Abstract: Clustering is a cornerstone of modern data analysis. Detecting clusters in exploratory data analyses (EDA) requires algorithms that make few assumptions about the data. Density-based clustering algorithms are particularly well-suited for EDA because they describe high-density regions, assuming only that a density exists. Applying density-based clustering algorithms in practice, however, requires selecting appropriate hyperparameters, which is difficult without prior knowledge of the data distribution. For example, DBSCAN requires selecting a density threshold, and HDBSCAN* relies on a minimum cluster size parameter. In this work, we propose Persistent Leaves Spatial Clustering for Applications with Noise (PLSCAN). This novel density-based clustering algorithm efficiently identifies all minimum cluster sizes for which HDBSCAN* produces stable (leaf) clusters. PLSCAN applies scale-space clustering principles and is equivalent to persistent homology on a novel metric space. We compare its performance to HDBSCAN* on several real-world datasets, demonstrating that it achieves a higher average ARI and is less sensitive to changes in the number of mutual reachability neighbours. Additionally, we compare PLSCAN's computational costs to k-Means, demonstrating competitive run-times on low-dimensional datasets. At higher dimensions, run times scale more similarly to HDBSCAN*.

</details>


### [164] [Exploiting Radio Frequency Fingerprints for Device Identification: Tackling Cross-receiver Challenges in the Source-data-free Scenario](https://arxiv.org/abs/2512.16648)
*Liu Yang,Qiang Li,Luxiong Wen,Jian Yang*

Main category: cs.LG

TL;DR: 针对基于深度学习的射频指纹识别（RFFI）模型跨接收器性能下降问题，研究无源数据的跨接收器RFFI问题，提出MS - SHOT方法，实验表明其在准确性和鲁棒性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 基于深度学习的RFFI模型在跨不同硬件特性接收器部署时，因分布偏移性能显著下降，需解决无源数据的跨接收器RFFI问题。

Method: 先构建基于约束伪标签的SCRFFI适应框架并进行理论分析，提出MS - SHOT方法，结合动量中心引导的软伪标签和全局结构约束。

Result: 在真实数据集上的大量实验显示，MS - SHOT在准确性和鲁棒性上始终优于现有方法。

Conclusion: MS - SHOT为RFFI中无数据源的跨接收器适应提供了实用且可扩展的解决方案。

Abstract: With the rapid proliferation of edge computing, Radio Frequency Fingerprint Identification (RFFI) has become increasingly important for secure device authentication. However, practical deployment of deep learning-based RFFI models is hindered by a critical challenge: their performance often degrades significantly when applied across receivers with different hardware characteristics due to distribution shifts introduced by receiver variation. To address this, we investigate the source-data-free cross-receiver RFFI (SCRFFI) problem, where a model pretrained on labeled signals from a source receiver must adapt to unlabeled signals from a target receiver, without access to any source-domain data during adaptation. We first formulate a novel constrained pseudo-labeling-based SCRFFI adaptation framework, and provide a theoretical analysis of its generalization performance. Our analysis highlights a key insight: the target-domain performance is highly sensitive to the quality of the pseudo-labels generated during adaptation. Motivated by this, we propose Momentum Soft pseudo-label Source Hypothesis Transfer (MS-SHOT), a new method for SCRFFI that incorporates momentum-center-guided soft pseudo-labeling and enforces global structural constraints to encourage confident and diverse predictions. Notably, MS-SHOT effectively addresses scenarios involving label shift or unknown, non-uniform class distributions in the target domain -- a significant limitation of prior methods. Extensive experiments on real-world datasets demonstrate that MS-SHOT consistently outperforms existing approaches in both accuracy and robustness, offering a practical and scalable solution for source-data-free cross-receiver adaptation in RFFI.

</details>


### [165] [DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI](https://arxiv.org/abs/2512.16676)
*Hao Liang,Xiaochen Ma,Zhou Liu,Zhen Hao Wong,Zhengyang Zhao,Zimo Meng,Runming He,Chengyu Shen,Qifeng Cai,Zhaoyang Han,Meiyi Qiang,Yalin Feng,Tianyi Bai,Zewei Pan,Ziyi Guo,Yizhen Jiang,Jingwen Deng,Qijie You,Peichao Lai,Tianyu Guo,Chi Hsu Tsai,Hengyi Feng,Rui Hu,Wenkai Yu,Junbo Niu,Bohan Zeng,Ruichuan An,Lu Ma,Jihao Huang,Yaowei Zheng,Conghui He,Linpeng Tang,Bin Cui,Weinan E,Wentao Zhang*

Main category: cs.LG

TL;DR: 提出DataFlow框架解决大语言模型数据准备问题，包含众多算子和管道，有DataFlow - Agent提升可用性，在多场景提升性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型对高质量数据需求增长，但现有数据准备流程缺乏原则抽象、难以复现且对模型内循环数据生成支持有限。

Method: 设计具有系统级抽象的DataFlow框架，提供PyTorch风格API，包含近200个算子和6个通用管道，引入DataFlow - Agent将自然语言规范转化为可执行管道。

Result: 在六个用例中提升下游大语言模型性能，在多个任务上超越人类数据集和合成基线，统一数据集使基础模型表现更好。

Conclusion: DataFlow为可靠、可复现和可扩展的大语言模型数据准备提供实用高性能基础，为未来以数据为中心的AI发展奠定系统级基础。

Abstract: The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\% execution accuracy in Text-to-SQL over SynSQL, +7\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.

</details>


### [166] [Blog Data Showdown: Machine Learning vs Neuro-Symbolic Models for Gender Classification](https://arxiv.org/abs/2512.16687)
*Natnael Tilahun Sinshaw,Mengmei He,Tadesse K. Bahiru,Sudhir Kumar Mohapatra*

Main category: cs.LG

TL;DR: 本文对文本分类中常用机器学习算法、文本表示方法、特征提取技术进行对比分析，比较机器学习、深度学习与神经符号AI（NeSy）方法，实验显示NeSy在有限数据集上效果与强MLP相当，未来将拓展研究。


<details>
  <summary>Details</summary>
Motivation: 文本分类在多领域有应用，研究旨在对比常用机器学习算法、文本表示、特征提取技术，比较机器学习、深度学习与NeSy方法在文本分类中的表现。

Method: 对支持向量机、朴素贝叶斯等机器学习算法，TF - IDF等文本表示方法，卡方检验等特征提取技术进行对比分析，将机器学习、深度学习与NeSy对比。

Result: 在有限数据集上，NeSy方法取得与强MLP相当的结果。

Conclusion: NeSy方法在文本分类中有一定效果，未来可通过拓展知识库、嵌入类型和超参数配置进一步研究其有效性。

Abstract: Text classification problems, such as gender classification from a blog, have been a well-matured research area that has been well studied using machine learning algorithms. It has several application domains in market analysis, customer recommendation, and recommendation systems. This study presents a comparative analysis of the widely used machine learning algorithms, namely Support Vector Machines (SVM), Naive Bayes (NB), Logistic Regression (LR), AdaBoost, XGBoost, and an SVM variant (SVM_R) with neuro-symbolic AI (NeSy). The paper also explores the effect of text representations such as TF-IDF, the Universal Sentence Encoder (USE), and RoBERTa. Additionally, various feature extraction techniques, including Chi-Square, Mutual Information, and Principal Component Analysis, are explored. Building on these, we introduce a comparative analysis of the machine learning and deep learning approaches in comparison to the NeSy. The experimental results show that the use of the NeSy approach matched strong MLP results despite a limited dataset. Future work on this research will expand the knowledge base, the scope of embedding types, and the hyperparameter configuration to further study the effectiveness of the NeSy approach.

</details>


### [167] [CLARiTy: A Vision Transformer for Multi-Label Classification and Weakly-Supervised Localization of Chest X-ray Pathologies](https://arxiv.org/abs/2512.16700)
*John M. Statheros,Hairong Wang,Richard Klein*

Main category: cs.LG

TL;DR: 提出基于视觉Transformer的CLARiTy模型用于胸部X光片多标签分类和弱监督定位，在多方面表现出色，低分辨率变体适用于低资源场景。


<details>
  <summary>Details</summary>
Motivation: 胸部X光片解读在多标签病理分类和空间定位上有挑战，且区域级标注稀缺。

Method: 引入CLARiTy模型，用多类特定令牌生成注意力图，用SegmentCAM模块分割前景和抑制背景，利用ConvNeXtV2教师模型蒸馏训练。

Result: CLARiTy - S - 16 - 512在14种病理分类表现有竞争力，8种病理弱监督定位达最优，超之前方法50.7%；CLARiTy - S - 16 - 224高效且超基线。

Conclusion: CLARiTy利用ViT自注意力和卷积背景抑制，超越CNN - ViT混合模型，实现精确、降噪热图。

Abstract: The interpretation of chest X-rays (CXRs) poses significant challenges, particularly in achieving accurate multi-label pathology classification and spatial localization. These tasks demand different levels of annotation granularity but are frequently constrained by the scarcity of region-level (dense) annotations. We introduce CLARiTy (Class Localizing and Attention Refining Image Transformer), a vision transformer-based model for joint multi-label classification and weakly-supervised localization of thoracic pathologies. CLARiTy employs multiple class-specific tokens to generate discriminative attention maps, and a SegmentCAM module for foreground segmentation and background suppression using explicit anatomical priors. Trained on image-level labels from the NIH ChestX-ray14 dataset, it leverages distillation from a ConvNeXtV2 teacher for efficiency. Evaluated on the official NIH split, the CLARiTy-S-16-512 (a configuration of CLARiTy), achieves competitive classification performance across 14 pathologies, and state-of-the-art weakly-supervised localization performance on 8 pathologies, outperforming prior methods by 50.7%. In particular, pronounced gains occur for small pathologies like nodules and masses. The lower-resolution variant of CLARiTy, CLARiTy-S-16-224, offers high efficiency while decisively surpassing baselines, thereby having the potential for use in low-resource settings. An ablation study confirms contributions of SegmentCAM, DINO pretraining, orthogonal class token loss, and attention pooling. CLARiTy advances beyond CNN-ViT hybrids by harnessing ViT self-attention for global context and class-specific localization, refined through convolutional background suppression for precise, noise-reduced heatmaps.

</details>


### [168] [Towards Reproducibility in Predictive Process Mining: SPICE - A Deep Learning Library](https://arxiv.org/abs/2512.16715)
*Oliver Stritzel,Nick Hühnerbein,Simon Rauch,Itzel Zarate,Lukas Fleischmann,Moike Buck,Attila Lischka,Christian Frey*

Main category: cs.LG

TL;DR: 提出Python框架SPICE用于PPM，可对不同建模方法进行可重复和稳健比较，并在11个数据集上进行比较。


<details>
  <summary>Details</summary>
Motivation: 现有的PPM方法缺乏可重复性、决策透明度、处理新数据集和基准测试的可用性，难以比较不同实现。

Method: 在PyTorch中重新实现三种流行的基于深度学习的PPM方法，设计具有严格可配置性的通用基础框架。

Result: 在11个数据集上与原始报告指标和公平指标进行了比较。

Conclusion: SPICE框架有助于对PPM的过去和未来建模方法进行可重复和稳健的比较。

Abstract: In recent years, Predictive Process Mining (PPM) techniques based on artificial neural networks have evolved as a method for monitoring the future behavior of unfolding business processes and predicting Key Performance Indicators (KPIs). However, many PPM approaches often lack reproducibility, transparency in decision making, usability for incorporating novel datasets and benchmarking, making comparisons among different implementations very difficult. In this paper, we propose SPICE, a Python framework that reimplements three popular, existing baseline deep-learning-based methods for PPM in PyTorch, while designing a common base framework with rigorous configurability to enable reproducible and robust comparison of past and future modelling approaches. We compare SPICE to original reported metrics and with fair metrics on 11 datasets.

</details>


### [169] [Phishing Detection System: An Ensemble Approach Using Character-Level CNN and Feature Engineering](https://arxiv.org/abs/2512.16717)
*Rudra Dubey,Arpit Mani Tripathi,Archit Srivastava,Sarvpal Singh*

Main category: cs.LG

TL;DR: 本文提出基于集成方法的钓鱼检测AI模型，结合字符级CNN和LightGBM，在测试集上表现良好，可实时检测且误报率低。


<details>
  <summary>Details</summary>
Motivation: 当前钓鱼攻击是普遍的网络安全风险，恶意者不断改变策略，需有效检测方法。

Method: 提取36个URL特征后用字符级CNN提取序列特征，结合LightGBM，采用集成方法构建模型。

Result: 在19873个URL测试集上，准确率99.819%，精度100%，召回率99.635%，ROC - AUC 99.947%，LightGBM和字符级CNN分别贡献40%和60%。

Conclusion: 该方法优于单个模型，能识别现代钓鱼技术，保持低误报率。

Abstract: In actuality, phishing attacks remain one of the most prevalent cybersecurity risks in existence today, with malevolent actors constantly changing their strategies to successfully trick users. This paper presents an AI model for a phishing detection system that uses an ensemble approach to combine character-level Convolutional Neural Networks (CNN) and LightGBM with engineered features. Our system uses a character-level CNN to extract sequential features after extracting 36 lexical, structural, and domain-based features from the URLs. On a test dataset of 19,873 URLs, the ensemble model achieves an accuracy of 99.819 percent, precision of 100 percent, recall of 99.635 percent, and ROC-AUC of 99.947 percent. Through a FastAPI-based service with an intuitive user interface, the suggested system has been utilised to offer real-time detection. In contrast, the results demonstrate that the suggested solution performs better than individual models; LightGBM contributes 40 percent and character-CNN contributes 60 percent to the final prediction. The suggested method maintains extremely low false positive rates while doing a good job of identifying contemporary phishing techniques. Index Terms - Phishing detection, machine learning, deep learning, CNN, ensemble methods, cybersecurity, URL analysis

</details>


### [170] [Polyharmonic Spline Packages: Composition, Efficient Procedures for Computation and Differentiation](https://arxiv.org/abs/2512.16718)
*Yuriy N. Bakhvalov*

Main category: cs.LG

TL;DR: 此前机器学习回归问题可用随机函数理论解决，但有计算成本高和高维适用性问题，本文提出级联架构解决扩展性问题，并给出高效矩阵程序。


<details>
  <summary>Details</summary>
Motivation: 解决先前机器学习回归问题解决方案计算成本高、不适用于高输入维度的问题。

Method: 提出由多调和样条包构建的级联架构，给出正向计算和端到端微分的高效矩阵程序。

Result: 所提级联架构可解决可扩展性问题，一定条件下有理论依据。

Conclusion: 该级联架构可用于未知低内在维度问题，提高了解决方案的可扩展性。

Abstract: In a previous paper it was shown that a machine learning regression problem can be solved within the framework of random function theory, with the optimal kernel analytically derived from symmetry and indifference principles and coinciding with a polyharmonic spline. However, a direct application of that solution is limited by O(N^3) computational cost and by a breakdown of the original theoretical assumptions when the input space has excessive dimensionality. This paper proposes a cascade architecture built from packages of polyharmonic splines that simultaneously addresses scalability and is theoretically justified for problems with unknown intrinsic low dimensionality. Efficient matrix procedures are presented for forward computation and end-to-end differentiation through the cascade.

</details>


### [171] [KOSS: Kalman-Optimal Selective State Spaces for Long-Term Sequence Modeling](https://arxiv.org/abs/2512.16723)
*Lei Wang,Xin Tan,Mingwei Wang,Ying Zhang*

Main category: cs.LG

TL;DR: 提出 KOSS 选择性状态空间模型，在多个任务中表现优于基线模型，并提供理论支持


<details>
  <summary>Details</summary>
Motivation: 现有选择性状态空间模型缺乏理论基础且无法支持从潜在状态动态进行上下文感知选择

Method: 将选择公式化为潜在状态不确定性最小化，采用基于卡尔曼增益的连续时间潜在更新，用全局谱微分和分段扫描确保计算稳定和可扩展性

Result: 在选择性复制任务中精度超 79%，九项长期预测基准中降低 MSE 2.92 - 36.23%，案例研究证明其在实际应用中有效

Conclusion: KOSS 是有效且具有理论基础的上下文感知选择性状态空间模型，实验验证其设计合理性

Abstract: Recent selective state space models (SSMs), such as Mamba and Mamba-2, have demonstrated strong performance in sequence modeling owing to input-dependent selection mechanisms. However, these mechanisms lack theoretical grounding and cannot support context-aware selection from latent state dynamics. To address these limitations, we propose KOSS, a Kalman-optimal Selective State Space model that formulates selection as latent state uncertainty minimization. Derived from estimation theory, KOSS adopts a continuous-time latent update driven by a Kalman gain that dynamically modulates information propagation based on content and context, enabling a closed-loop, context-aware selectivity mechanism. To ensure stable computation and near-linear scalability, KOSS employs global spectral differentiation for frequency-domain derivative estimation, along with a segment-wise scan for hardware-efficient processing. On a selective copying task with distractors, KOSS achieves over 79\% accuracy while baselines drop below 20\%, demonstrating robust context-aware selection. Furthermore, across nine long-term forecasting benchmarks, KOSS reduces MSE by 2.92--36.23\% and consistently outperforms state-of-the-art models in both accuracy and stability. To assess real-world applicability, a case study on secondary surveillance radar (SSR) tracking confirms KOSS's robustness under irregular intervals and noisy conditions and demonstrates its effectiveness in real-world applications. Finally, supplementary experiments verify Kalman gain convergence and the frequency response of spectral differentiation, providing theoretical support for the proposed closed-loop design.

</details>


### [172] [Machine Learning Algorithms: Detection Official Hajj and Umrah Travel Agency Based on Text and Metadata Analysis](https://arxiv.org/abs/2512.16742)
*Wisnu Uriawan,Muhamad Veva Ramadhan,Firman Adi Nugraha,Hasbi Nur Wahid,M Dantha Arianvasya,Muhammad Zaki Alghifari*

Main category: cs.LG

TL;DR: 印尼朝觐和副朝服务数字化带来便利同时引发数字欺诈问题，研究用机器学习算法验证应用真实性，对比SVM、RF、NB三种分类器，SVM性能最佳。


<details>
  <summary>Details</summary>
Motivation: 解决印尼朝觐和副朝数字服务中假冒应用造成的数字欺诈及隐私风险问题，增强宗教旅游领域数字信任。

Method: 使用官方和非官方应用的综合数据集，结合文本分析（TF - IDF）与敏感权限元数据分析的混合特征提取方法，对比SVM、RF、NB三种分类器。

Result: SVM算法性能最高，准确率92.3%，精确率91.5%，F1分数92.0%，特定合法性关键词和高风险权限是最显著判别因素。

Conclusion: 该系统可作为提升宗教旅游数字信任的主动、可扩展解决方案，能作为国家验证系统原型。

Abstract: The rapid digitalization of Hajj and Umrah services in Indonesia has significantly facilitated pilgrims but has concurrently opened avenues for digital fraud through counterfeit mobile applications. These fraudulent applications not only inflict financial losses but also pose severe privacy risks by harvesting sensitive personal data. This research aims to address this critical issue by implementing and evaluating machine learning algorithms to verify application authenticity automatically. Using a comprehensive dataset comprising both official applications registered with the Ministry of Religious Affairs and unofficial applications circulating on app stores, we compare the performance of three robust classifiers: Support Vector Machine (SVM), Random Forest (RF), and Na"ive Bayes (NB). The study utilizes a hybrid feature extraction methodology that combines Textual Analysis (TF-IDF) of application descriptions with Metadata Analysis of sensitive access permissions. The experimental results indicate that the SVM algorithm achieves the highest performance with an accuracy of 92.3%, a precision of 91.5%, and an F1-score of 92.0%. Detailed feature analysis reveals that specific keywords related to legality and high-risk permissions (e.g., READ PHONE STATE) are the most significant discriminators. This system is proposed as a proactive, scalable solution to enhance digital trust in the religious tourism sector, potentially serving as a prototype for a national verification system.

</details>


### [173] [NRGPT: An Energy-based Alternative for GPT](https://arxiv.org/abs/2512.16762)
*Nima Dehmamy,Benjamin Hoover,Bishwajit Saha,Leo Kozachkov,Jean-Jacques Slotine,Dmitry Krotov*

Main category: cs.LG

TL;DR: 提出eNeRgy - GPT（NRGPT）模型统一GPT和EBM框架，此模型在多任务中表现良好且可能更抗过拟合。


<details>
  <summary>Details</summary>
Motivation: 统一GPT架构和能量建模（EBM）框架。

Method: 对GPT进行最小修改以融入EBM框架，将NRGPT推理步骤视为在能量景观上探索标记。

Result: 证明在特定情况下探索过程为梯度下降，在莎士比亚数据集、代数ListOPS任务和OpenWebText语言建模任务中表现良好。

Conclusion: 所提出的模型能在多种设置下表现良好，且可能在长时间训练中才会过拟合，更抗过拟合。

Abstract: Generative Pre-trained Transformer (GPT) architectures are the most popular design for language modeling. Energy-based modeling is a different paradigm that views inference as a dynamical process operating on an energy landscape. We propose a minimal modification of the GPT setting to unify it with the EBM framework. The inference step of our model, which we call eNeRgy-GPT (NRGPT), is conceptualized as an exploration of the tokens on the energy landscape. We prove, and verify empirically, that under certain circumstances this exploration becomes gradient descent, although they don't necessarily lead to the best performing models. We demonstrate that our model performs well for simple language (Shakespeare dataset), algebraic ListOPS tasks, and richer settings such as OpenWebText language modeling. We also observe that our models may be more resistant to overfitting, doing so only during very long training.

</details>


### [174] [Pattern recognition in complex systems via vector-field representations of spatio-temporal data](https://arxiv.org/abs/2512.16763)
*Ingrid Amaranta Membrillo Solis,Maria van Rossem,Tristan Madeleine,Tetiana Orlova,Nina Podoliak,Giampaolo D'Alessandro,Jacek Brodzki,Malgosia Kaczmarek*

Main category: cs.LG

TL;DR: 本文引入几何框架分析复杂系统时空数据，提出适合数据分析和机器学习的双参数度量，通过数值模拟数据验证，结果表明能有效应对分析挑战，为理解复杂动力系统提供途径。


<details>
  <summary>Details</summary>
Motivation: 复杂系统具有高维、非线性动力学特性，传统方法受时空数据量和复杂性阻碍，需新方法分析。

Method: 引入基于离散测度空间上向量场理论的几何框架，提出双参数度量族，支持多种数据类型。

Result: 所提度量结合多维尺度分析，能实现降维、模式分解、相空间重构和吸引子表征。

Conclusion: 研究为理解复杂动力系统提供了鲁棒途径，尤其适用于传统建模不可行但有大量实验数据的情况。

Abstract: A complex system comprises multiple interacting entities whose interdependencies form a unified whole, exhibiting emergent behaviours not present in individual components. Examples include the human brain, living cells, soft matter, Earth's climate, ecosystems, and the economy. These systems exhibit high-dimensional, non-linear dynamics, making their modelling, classification, and prediction particularly challenging. Advances in information technology have enabled data-driven approaches to studying such systems. However, the sheer volume and complexity of spatio-temporal data often hinder traditional methods like dimensionality reduction, phase-space reconstruction, and attractor characterisation. This paper introduces a geometric framework for analysing spatio-temporal data from complex systems, grounded in the theory of vector fields over discrete measure spaces. We propose a two-parameter family of metrics suitable for data analysis and machine learning applications. The framework supports time-dependent images, image gradients, and real- or vector-valued functions defined on graphs and simplicial complexes. We validate our approach using data from numerical simulations of biological and physical systems on flat and curved domains. Our results show that the proposed metrics, combined with multidimensional scaling, effectively address key analytical challenges. They enable dimensionality reduction, mode decomposition, phase-space reconstruction, and attractor characterisation. Our findings offer a robust pathway for understanding complex dynamical systems, especially in contexts where traditional modelling is impractical but abundant experimental data are available.

</details>


### [175] [MEPIC: Memory Efficient Position Independent Caching for LLM Serving](https://arxiv.org/abs/2512.16822)
*Qian Wang,Zahra Yousefijamarani,Morgan Lindsay Heisler,Rongzhi Gu,Bai Xiaolong,Shan Yizhou,Wei Zhang,Wang Lan,Ying Xiong,Yong Zhang,Zhenan Fan*

Main category: cs.LG

TL;DR: 现代LLM应用对KV缓存造成压力，现有缓存方法有局限，提出MEPIC系统，可跨位置、请求和批次复用Chunk KV，减少HBM使用。


<details>
  <summary>Details</summary>
Motivation: 现代LLM应用处理长提示历史对KV缓存有巨大压力，现有的Prefix caching和PIC方法有局限性，需要一种更高效的缓存系统。

Method: 提出MEPIC系统，将chunk KV对齐到分页存储，将重新计算从令牌级转移到块级，通过注意力内核中的RoPE融合去除位置编码。

Result: MEPIC可消除HBM中大部分重复的Chunk KV，在可比的延迟和准确性下，比现有PIC减少高达2倍的HBM使用，长提示下可达5倍。

Conclusion: MEPIC系统无需更改模型，能有效提高KV缓存的内存使用效率。

Abstract: Modern LLM applications such as deep-research assistants, coding agents, and Retrieval-Augmented Generation (RAG) systems, repeatedly process long prompt histories containing shared document or code chunks, creating significant pressure on the Key Value (KV) cache, which must operate within limited memory while sustaining high throughput and low latency. Prefix caching partially alleviates some of these costs by reusing KV cache for previously processed tokens, but limited by strict prefix matching. Position-independent caching (PIC) enables chunk-level reuse at arbitrary positions, but requires selective recomputation and positional-encoding (PE) adjustments. However, because these operations vary across queries, KV for the same chunk diverges across requests. Moreover, without page alignment, chunk KV layouts diverge in memory, preventing page sharing. These issues result in only modest HBM savings even when many requests reuse the same content.
  We present MEPIC, a memory-efficient PIC system that enables chunk KV reuse across positions, requests, and batches. MEPIC aligns chunk KV to paged storage, shifts recomputation from token- to block-level so only the first block is request-specific, removes positional encodings via Rotary Position Embedding (RoPE) fusion in the attention kernel, and makes remaining blocks fully shareable. These techniques eliminate most duplicate chunk KV in HBM, reducing usage by up to 2x over state-of-the-art PIC at comparable latency and accuracy, and up to 5x for long prompts, without any model changes.

</details>


### [176] [Tiny Recursive Control: Iterative Reasoning for Efficient Optimal Control](https://arxiv.org/abs/2512.16824)
*Amit Jain,Richard Linares*

Main category: cs.LG

TL;DR: 提出Tiny Recursive Control (TRC) 神经架构，通过迭代深度而非参数数量实现控制能力，在非线性控制问题上评估效果好，存储需求比语言模型基线小两个数量级，证明递归推理可应用于连续控制合成。


<details>
  <summary>Details</summary>
Motivation: 神经网络控制器参数多，嵌入式航空航天系统有严格功耗和延迟限制，传统参数扩展方法不可行。

Method: 提出TRC架构，基于迭代深度产生容量，通过两级分层潜在结构重复应用紧凑网络，根据跟踪误差修正控制序列。

Result: 在非线性控制问题中，TRC实现近最优控制成本，在GPU上推理仅需毫秒级，内存需求小于10MB ，比语言模型基线小两个数量级。

Conclusion: 递归推理能有效应用于连续控制合成。

Abstract: Neural network controllers increasingly demand millions of parameters, and language model approaches push into the billions. For embedded aerospace systems with strict power and latency constraints, this scaling is prohibitive. We present Tiny Recursive Control (TRC), a neural architecture based on a counterintuitive principle: capacity can emerge from iteration depth rather than parameter count. TRC applies compact networks (approximately 1.5M parameters) repeatedly through a two-level hierarchical latent structure, refining control sequences by simulating trajectories and correcting based on tracking error. Because the same weights process every refinement step, adding iterations increases computation without increasing memory. We evaluate TRC on nonlinear control problems including oscillator stabilization and powered descent with fuel constraints. Across these domains, TRC achieves near-optimal control costs while requiring only millisecond-scale inference on GPU and under 10~MB memory, two orders of magnitude smaller than language model baselines. These results demonstrate that recursive reasoning, previously confined to discrete tasks, transfers effectively to continuous control synthesis.

</details>


### [177] [Meta-RL Induces Exploration in Language Agents](https://arxiv.org/abs/2512.16848)
*Yulun Jiang,Liangze Jiang,Damien Teney,Michael Moor,Maria Brbic*

Main category: cs.LG

TL;DR: 提出LaMer元强化学习框架使大语言模型智能体在测试时主动探索和学习，实验显示其表现优于基线且泛化性更好。


<details>
  <summary>Details</summary>
Motivation: 强化学习训练的大语言模型智能体在需要主动探索的任务中表现不佳，难以从试错中高效适应。

Method: 提出LaMer框架，包含跨回合训练框架和通过反思的上下文策略自适应两个关键组件。

Result: 在不同环境实验中，LaMer比强化学习基线在Sokoban、MineSweeper和Webshop上分别提升11%、14%和19%的性能，且在更具挑战性或未见过的任务上泛化性更好。

Conclusion: 元强化学习为语言智能体诱导探索提供了原则性方法，能通过学习的探索策略实现对新环境更稳健的适应。

Abstract: Reinforcement learning (RL) has enabled the training of large language model (LLM) agents to interact with the environment and to solve multi-turn long-horizon tasks. However, the RL-trained agents often struggle in tasks that require active exploration and fail to efficiently adapt from trial-and-error experiences. In this paper, we present LaMer, a general Meta-RL framework that enables LLM agents to actively explore and learn from the environment feedback at test time. LaMer consists of two key components: (i) a cross-episode training framework to encourage exploration and long-term rewards optimization; and (ii) in-context policy adaptation via reflection, allowing the agent to adapt their policy from task feedback signal without gradient update. Experiments across diverse environments show that LaMer significantly improves performance over RL baselines, with 11%, 14%, and 19% performance gains on Sokoban, MineSweeper and Webshop, respectively. Moreover, LaMer also demonstrates better generalization to more challenging or previously unseen tasks compared to the RL-trained agents. Overall, our results demonstrate that Meta-RL provides a principled approach to induce exploration in language agents, enabling more robust adaptation to novel environments through learned exploration strategies.

</details>


### [178] [Semi-Supervised Online Learning on the Edge by Transforming Knowledge from Teacher Models](https://arxiv.org/abs/2512.16866)
*Jiabin Xue*

Main category: cs.LG

TL;DR: 本文针对在线边缘机器学习中为未知数据点确定标签的挑战，提出结合知识蒸馏、主动学习和因果推理的知识转换（KT）方法，经实验验证其有效性，并指出其适用场景。


<details>
  <summary>Details</summary>
Motivation: 现有边缘机器学习方法对未知数据处理不佳，在线边缘机器学习可直接在边缘设备上训练和更新模型，但存在为未知数据点确定标签的挑战。

Method: 提出知识转换（KT）方法，结合知识蒸馏、主动学习和因果推理，通过将教师模型的知识转化为伪标签来训练学生模型。

Result: 模拟实验表明，给定稳定的教师模型时，学生模型最终能达到预期的最大性能。

Conclusion: KT方法在教师任务通用、无需从头训练教师模型或学生任务标签获取困难昂贵的场景中具有潜在益处。

Abstract: Edge machine learning (Edge ML) enables training ML models using the vast data distributed across network edges. However, many existing approaches assume static models trained centrally and then deployed, making them ineffective against unseen data. To address this, Online Edge ML allows models to be trained directly on edge devices and updated continuously with new data. This paper explores a key challenge of Online Edge ML: "How to determine labels for truly future, unseen data points". We propose Knowledge Transformation (KT), a hybrid method combining Knowledge Distillation, Active Learning, and causal reasoning. In short, KT acts as the oracle in active learning by transforming knowledge from a teacher model to generate pseudo-labels for training a student model. To verify the validity of the method, we conducted simulation experiments with two setups: (1) using a less stable teacher model and (2) a relatively more stable teacher model. Results indicate that when a stable teacher model is given, the student model can eventually reach its expected maximum performance. KT is potentially beneficial for scenarios that meet the following circumstances: (1) when the teacher's task is generic, which means existing pre-trained models might be adequate for its task, so there will be no need to train the teacher model from scratch; and/or (2) when the label for the student's task is difficult or expensive to acquire.

</details>


### [179] [Sequencing to Mitigate Catastrophic Forgetting in Continual Learning](https://arxiv.org/abs/2512.16871)
*Hesham G. Moussa,Aroosa Hameed,Arashmid Akhavain*

Main category: cs.LG

TL;DR: 文章探讨持续学习中的灾难性遗忘问题，从任务最优排序角度提出方法，结果显示能减少遗忘并提升性能，还可用于其他领域。


<details>
  <summary>Details</summary>
Motivation: 解决持续学习中灾难性遗忘问题，现有方法多属五类，本文从新角度即任务最优排序入手。

Method: 利用受神经架构搜索启发的零样本评分算法确定最优任务顺序。

Result: 智能任务排序可大幅减少灾难性遗忘，结合传统策略能提升性能和鲁棒性。

Conclusion: 任务排序方法有效，且可应用于其他领域如课程学习。

Abstract: To cope with real-world dynamics, an intelligent system needs to incrementally acquire, update, and exploit knowledge throughout its lifetime. This ability, known as Continual learning, provides a foundation for AI systems to develop themselves adaptively. Catastrophic forgetting is a major challenge to the progress of Continual Learning approaches, where learning a new task usually results in a dramatic performance drop on previously learned ones. Many approaches have emerged to counteract the impact of CF. Most of the proposed approaches can be categorized into five classes: replay-based, regularization-based, optimization-based, representation-based, and architecture-based. In this work, we approach the problem from a different angle, specifically by considering the optimal sequencing of tasks as they are presented to the model. We investigate the role of task sequencing in mitigating CF and propose a method for determining the optimal task order. The proposed method leverages zero-shot scoring algorithms inspired by neural architecture search (NAS). Results demonstrate that intelligent task sequencing can substantially reduce CF. Moreover, when combined with traditional continual learning strategies, sequencing offers enhanced performance and robustness against forgetting. Additionally, the presented approaches can find applications in other fields, such as curriculum learning.

</details>


### [180] [Impacts of Racial Bias in Historical Training Data for News AI](https://arxiv.org/abs/2512.16901)
*Rahul Bhargava,Malene Hornstrup Jespersen,Emily Boardman Ndulue,Vivica Dsouza*

Main category: cs.LG

TL;DR: 本文研究基于《纽约时报》标注语料库训练的多标签分类器中的“黑人”主题标签，发现其作为“种族主义探测器”表现不佳，揭示新闻编辑室采用AI工具时需降低重现历史偏见的风险。


<details>
  <summary>Details</summary>
Motivation: 研究基于广泛使用语料库训练的模型中可能存在的编码数十年前态度和刻板印象的问题，关注“黑人”主题标签的使用及其影响。

Method: 通过定量和定性方法研究训练语料库中标签的使用、编码概念以及对模型使用的影响，应用可解释AI方法。

Result: “黑人”标签部分可作为一些少数群体的“种族主义探测器”，但在现代案例中表现不佳。

Conclusion: 新闻编辑室在采用AI工作流工具时要降低新闻报道中重现历史偏见的风险。

Abstract: AI technologies have rapidly moved into business and research applications that involve large text corpora, including computational journalism research and newsroom settings. These models, trained on extant data from various sources, can be conceptualized as historical artifacts that encode decades-old attitudes and stereotypes. This paper investigates one such example trained on the broadly-used New York Times Annotated Corpus to create a multi-label classifier. Our use in research settings surfaced the concerning "blacks" thematic topic label. Through quantitative and qualitative means we investigate this label's use in the training corpus, what concepts it might be encoding in the trained classifier, and how those concepts impact our model use. Via the application of explainable AI methods, we find that the "blacks" label operates partially as a general "racism detector" across some minoritized groups. However, it performs poorly against expectations on modern examples such as COVID-19 era anti-Asian hate stories, and reporting on the Black Lives Matter movement. This case study of interrogating embedded biases in a model reveals how similar applications in newsroom settings can lead to unexpected outputs that could impact a wide variety of potential uses of any large language model-story discovery, audience targeting, summarization, etc. The fundamental tension this exposes for newsrooms is how to adopt AI-enabled workflow tools while reducing the risk of reproducing historical biases in news coverage.

</details>


### [181] [Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning](https://arxiv.org/abs/2512.16911)
*Andrew Wagenmaker,Perry Dong,Raymond Tsao,Chelsea Finn,Sergey Levine*

Main category: cs.LG

TL;DR: 探讨预训练策略对微调性能的影响，提出后验行为克隆（PostBC）策略，在机器人控制领域提升强化学习微调性能。


<details>
  <summary>Details</summary>
Motivation: 当前研究多关注微调算法，而忽视预训练策略作为强化学习微调有效初始化的重要性，旨在理解预训练策略对微调性能的影响及如何进行有效预训练。

Method: 理论分析标准行为克隆（BC）的不足，提出后验行为克隆（PostBC）策略，通过训练策略对演示者行为的后验分布建模，并在机器人控制领域用现代生成模型实现。

Result: PostBC策略能确保覆盖演示者的动作，预训练性能不低于BC策略，在机器人控制基准和实际操作任务中显著提升强化学习微调性能。

Conclusion: PostBC策略是一种有效的预训练方法，可作为强化学习微调的有效初始化，提升微调性能。

Abstract: Standard practice across domains from robotics to language is to first pretrain a policy on a large-scale demonstration dataset, and then finetune this policy, typically with reinforcement learning (RL), in order to improve performance on deployment domains. This finetuning step has proved critical in achieving human or super-human performance, yet while much attention has been given to developing more effective finetuning algorithms, little attention has been given to ensuring the pretrained policy is an effective initialization for RL finetuning. In this work we seek to understand how the pretrained policy affects finetuning performance, and how to pretrain policies in order to ensure they are effective initializations for finetuning. We first show theoretically that standard behavioral cloning (BC) -- which trains a policy to directly match the actions played by the demonstrator -- can fail to ensure coverage over the demonstrator's actions, a minimal condition necessary for effective RL finetuning. We then show that if, instead of exactly fitting the observed demonstrations, we train a policy to model the posterior distribution of the demonstrator's behavior given the demonstration dataset, we do obtain a policy that ensures coverage over the demonstrator's actions, enabling more effective finetuning. Furthermore, this policy -- which we refer to as the posterior behavioral cloning (PostBC) policy -- achieves this while ensuring pretrained performance is no worse than that of the BC policy. We then show that PostBC is practically implementable with modern generative models in robotic control domains -- relying only on standard supervised learning -- and leads to significantly improved RL finetuning performance on both realistic robotic control benchmarks and real-world robotic manipulation tasks, as compared to standard behavioral cloning.

</details>


### [182] [Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward](https://arxiv.org/abs/2512.16912)
*Peter Chen,Xiaopeng Li,Ziniu Li,Wotao Yin,Xi Chen,Tianyi Lin*

Main category: cs.LG

TL;DR: 本文研究可验证奖励强化学习（RLVR）中的探索 - 利用权衡问题，揭示虚假奖励降低策略熵提高推理性能的机制，并提出奖励失配模型。


<details>
  <summary>Details</summary>
Motivation: 近期研究显示RLVR中抑制探索和利用都能提升大语言模型推理性能，但潜在原理不明，因此聚焦策略熵与性能关系以及虚假奖励是否带来增益这两个问题。

Method: 研究虚假奖励下的裁剪偏差对策略熵的影响，提出奖励失配模型。

Result: 虚假奖励下的裁剪偏差降低策略熵，带来更自信和确定的输出，仅熵最小化不足以提升性能。

Conclusion: 明确了虚假奖励有益的机制，为更有效的RLVR训练提供原则。

Abstract: This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [183] [Improving Low-Latency Learning Performance in Spiking Neural Networks via a Change-Perceptive Dendrite-Soma-Axon Neuron](https://arxiv.org/abs/2512.16259)
*Zeyu Huang,Wei Meng,Quan Liu,Kun Chen,Li Ma*

Main category: cs.NE

TL;DR: 提出CP - DSA神经元模型解决传统脉冲神经元信息退化和模型简化问题，分析并通过实验验证其优势。


<details>
  <summary>Details</summary>
Motivation: 传统脉冲神经元硬重置机制导致信息退化，且神经元模型过于简单，无法准确模拟电位传输。

Method: 提出采用软重置策略的DSA神经元，结合基于电位变化的感知机制，形成CP - DSA神经元，含多个可学习参数。

Result: CP机制使模型在短时间步依靠相邻时间步差异信息取得有竞争力的性能，理论分析证明模型有效性和参数功能特性，实验证明模型优于现有方法。

Conclusion: CP - DSA神经元模型能有效解决传统脉冲神经元存在的问题，具有显著优势。

Abstract: Spiking neurons, the fundamental information processing units of Spiking Neural Networks (SNNs), have the all-or-zero information output form that allows SNNs to be more energy-efficient compared to Artificial Neural Networks (ANNs). However, the hard reset mechanism employed in spiking neurons leads to information degradation due to its uniform handling of diverse membrane potentials. Furthermore, the utilization of overly simplified neuron models that disregard the intricate biological structures inherently impedes the network's capacity to accurately simulate the actual potential transmission process. To address these issues, we propose a dendrite-soma-axon (DSA) neuron employing the soft reset strategy, in conjunction with a potential change-based perception mechanism, culminating in the change-perceptive dendrite-soma-axon (CP-DSA) neuron. Our model contains multiple learnable parameters that expand the representation space of neurons. The change-perceptive (CP) mechanism enables our model to achieve competitive performance in short time steps utilizing the difference information of adjacent time steps. Rigorous theoretical analysis is provided to demonstrate the efficacy of the CP-DSA model and the functional characteristics of its internal parameters. Furthermore, extensive experiments conducted on various datasets substantiate the significant advantages of the CP-DSA model over state-of-the-art approaches.

</details>


### [184] [Hypernetworks That Evolve Themselves](https://arxiv.org/abs/2512.16406)
*Joachim Winther Pedersen,Erwan Plantec,Eleni Nisioti,Marcello Barylli,Milton Montero,Kathrin Korte,Sebastian Risi*

Main category: cs.NE

TL;DR: 提出自引用图超网络，能不依赖外部优化器自主进化，在多个基准测试中表现良好，支持进化能力可从神经自引用中涌现的观点。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络不依赖外部优化器实现自我进化的方法。

Method: 结合超网络、随机参数生成和基于图的表示，让自引用图超网络自我变异、评估并自适应调整变异率。

Result: 在新的强化学习基准测试中展现快速可靠的适应能力和种群动态，在运动基准测试中进化出连贯步态，具备微调能力。

Conclusion: 进化能力可从神经自引用中涌现，自引用图超网络向更接近生物进化的合成系统迈进，为自主开放式学习智能体提供工具。

Abstract: How can neural networks evolve themselves without relying on external optimizers? We propose Self-Referential Graph HyperNetworks, systems where the very machinery of variation and inheritance is embedded within the network. By uniting hypernetworks, stochastic parameter generation, and graph-based representations, Self-Referential GHNs mutate and evaluate themselves while adapting mutation rates as selectable traits. Through new reinforcement learning benchmarks with environmental shifts (CartPoleSwitch, LunarLander-Switch), Self-Referential GHNs show swift, reliable adaptation and emergent population dynamics. In the locomotion benchmark Ant-v5, they evolve coherent gaits, showing promising fine-tuning capabilities by autonomously decreasing variation in the population to concentrate around promising solutions. Our findings support the idea that evolvability itself can emerge from neural self-reference. Self-Referential GHNs reflect a step toward synthetic systems that more closely mirror biological evolution, offering tools for autonomous, open-ended learning agents.

</details>


### [185] [On the Universal Representation Property of Spiking Neural Networks](https://arxiv.org/abs/2512.16872)
*Shayan Hundrieser,Philipp Tuchel,Insung Kong,Johannes Schmidt-Hieber*

Main category: cs.NE

TL;DR: 本文分析脉冲神经网络（SNNs）的表示能力，确立了一类脉冲序列函数的通用表示属性，结果定量、建设性且近乎最优，还揭示了SNNs适合表示的函数类型，最后讨论了脉冲序列分类。


<details>
  <summary>Details</summary>
Motivation: 探究SNNs作为一种节能的计算范式相比于经典计算范式和经典人工神经网络的表示能力。

Method: 将SNNs视为脉冲的序列到序列处理器，对一类脉冲序列函数进行分析。

Result: 确立了通用表示属性，结果定量、建设性且在所需权重和神经元数量上近乎最优，揭示SNNs适合表示特定类型函数，可通过模块化设计捕获复合函数。

Conclusion: 为理解基于脉冲的神经形态系统的能力和局限性提供了严格基础。

Abstract: Inspired by biology, spiking neural networks (SNNs) process information via discrete spikes over time, offering an energy-efficient alternative to the classical computing paradigm and classical artificial neural networks (ANNs). In this work, we analyze the representational power of SNNs by viewing them as sequence-to-sequence processors of spikes, i.e., systems that transform a stream of input spikes into a stream of output spikes. We establish the universal representation property for a natural class of spike train functions. Our results are fully quantitative, constructive, and near-optimal in the number of required weights and neurons. The analysis reveals that SNNs are particularly well-suited to represent functions with few inputs, low temporal complexity, or compositions of such functions. The latter is of particular interest, as it indicates that deep SNNs can efficiently capture composite functions via a modular design. As an application of our results, we discuss spike train classification. Overall, these results contribute to a rigorous foundation for understanding the capabilities and limitations of spike-based neuromorphic systems.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [186] [XTC, A Research Platform for Optimizing AI Workload Operators](https://arxiv.org/abs/2512.16512)
*Pompougnac Hugo,Guillon Christophe,Noiry Sylvain,Dutilleul Alban,Iooss Guillaume,Rastello Fabrice*

Main category: cs.PF

TL;DR: 介绍XTC平台统一编译器调度与性能评估，加速优化策略研究


<details>
  <summary>Details</summary>
Motivation: 现有调度语言局限于特定编译器生态，缺乏统一接口解耦调度规范与代码生成及测量

Method: 引入具备通用API和可复现测量框架的XTC平台

Result: XTC实现了跨编译器的可移植实验

Conclusion: XTC能加速优化策略研究

Abstract: Achieving high efficiency on AI operators demands precise control over computation and data movement. However, existing scheduling languages are locked into specific compiler ecosystems, preventing fair comparison, reuse, and evaluation across frameworks. No unified interface currently decouples scheduling specification from code generation and measurement. We introduce XTC, a platform that unifies scheduling and performance evaluation across compilers. With its common API and reproducible measurement framework, XTC enables portable experimentation and accelerates research on optimization strategies.

</details>


### [187] [An Upper Bound on the M/M/k Queue With Deterministic Setup Times](https://arxiv.org/abs/2512.16854)
*Jalani Williams,Weina Wang,Mor Harchol-Balter*

Main category: cs.PF

TL;DR: 本文研究多服务器系统中确定性设置时间对平均等待时间的影响，推导上下界并给出近似值，采用新方法MIST。


<details>
  <summary>Details</summary>
Motivation: 现代系统中服务器开关的设置时间影响排队，多服务器系统设置时间研究不足，现有指数设置假设不实际。

Method: 提出名为MIST的分析随机时间积分的新技术。

Result: 推导了M/M/k/Setup - Deterministic系统平均等待时间的上下界，且二者在乘法常数范围内，还给出简单准确的近似值。

Conclusion: 首次对有设置时间的有限服务器系统等待时间进行闭式刻画，为设计高效高性能系统提供依据。

Abstract: In many systems, servers do not turn on instantly; instead, a setup time must pass before a server can begin work. These "setup times" can wreak havoc on a system's queueing; this is especially true in modern systems, where servers are regularly turned on and off as a way to reduce operating costs (energy, labor, CO2, etc.). To design modern systems which are both efficient and performant, we need to understand how setup times affect queues.
  Unfortunately, despite successes in understanding setup in a single-server system, setup in a multiserver system remains poorly understood. To circumvent the main difficulty in analyzing multiserver setup, all existing results assume that setup times are memoryless, i.e. distributed Exponentially. However, in most practical settings, setup times are close to Deterministic, and the widely used Exponential-setup assumption leads to unrealistic model behavior and a dramatic underestimation of the true harm caused by setup times.
  This paper provides a comprehensive characterization of the average waiting time in a multiserver system with Deterministic setup times, the M/M/k/Setup-Deterministic. In particular, we derive upper and lower bounds on the average waiting time in this system, and show these bounds are within a multiplicative constant of each other. These bounds are the first closed-form characterization of waiting time in any finite-server system with setup times. Further, we demonstrate how to combine our upper and lower bounds to derive a simple and accurate approximation for the average waiting time. These results are all made possible via a new technique for analyzing random time integrals that we named the Method of Intervening Stopping Times, or MIST.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [188] [XBIDetective: Leveraging Vision Language Models for Identifying Cross-Browser Visual Inconsistencies](https://arxiv.org/abs/2512.15804)
*Balreet Grewal,James Graham,Jeff Muizelaar,Jan Honza Odvarko,Suhaib Mujahid,Marco Castelluccio,Cor-Paul Bezemer*

Main category: cs.SE

TL;DR: 本文探讨用视觉语言模型识别跨浏览器不一致性（XBIs），介绍XBIDetective工具并评估其性能，展示准确率，还讨论经验与应用场景。


<details>
  <summary>Details</summary>
Motivation: 浏览器渲染错误检测困难，现有检测XBIs的技术难以处理动态和交互式元素，因此需新方法。

Method: 开发XBIDetective工具，自动截取网站在Firefox和Chrome中的截图，用视觉语言模型分析；用现成和微调后的模型在1052个网站上评估工具性能。

Result: 使用微调后的模型时，XBIDetective识别跨浏览器差异的准确率为79%，检测动态元素和广告的准确率分别为84%和85%。

Conclusion: XBIDetective工具在识别XBIs方面有较好效果，还提出自动化回归测试、网站大规模监控和快速分类XBI错误报告等应用场景。

Abstract: Browser rendering bugs can be challenging to detect for browser developers, as they may be triggered by very specific conditions that are exhibited on only a very small subset of websites. Cross-browser inconsistencies (XBIs), variations in how a website is interpreted and displayed on different browsers, can be helpful guides to detect such rendering bugs. Although visual and Document Object Model (DOM)-based analysis techniques exist for detecting XBIs, they often struggle with dynamic and interactive elements. In this study, we discuss our industry experience with using vision language models (VLMs) to identify XBIs. We present the XBIDetective tool which automatically captures screenshots of a website in Mozilla Firefox and Google Chrome, and analyzes them with a VLM for XBIs. We evaluate XBIDetective's performance with an off-the-shelf and a fine-tuned VLM on 1,052 websites. We show that XBIDetective can identify cross-browser discrepancies with 79% accuracy and detect dynamic elements and advertisements with 84% and 85% accuracy, respectively, when using the fine-tuned VLM. We discuss important lessons learned, and we present several potential practical use cases for XBIDetective, including automated regression testing, large-scale monitoring of websites, and rapid triaging of XBI bug reports.

</details>


### [189] [CodeMem: Architecting Reproducible Agents via Dynamic MCP and Procedural Memory](https://arxiv.org/abs/2512.15813)
*Nishant Gaurav,Adit Akarsh,Tejas Ravishankar,Manoj Bajaj*

Main category: cs.SE

TL;DR: 当前工具使用型AI代理存在问题，已有工作解决部分问题但概率不稳定性仍在，本文提出CodeMem架构实现程序记忆以构建可靠的代理工作流。


<details>
  <summary>Details</summary>
Motivation: 当前工具使用型AI代理存在动作空间有限、上下文效率低和概率不稳定性问题，不适用于处理重复性任务，已有工作未解决概率不稳定性，需程序记忆保证一致性和可靠性。

Method: 提出CodeMem架构，通过代码实现程序记忆。

Result: 未提及具体结果。

Conclusion: CodeMem架构可用于构建和运行具有确定性可靠性的可重用代理工作流。

Abstract: Current tool-using AI agents suffer from limited action space, context inefficiency, and probabilistic instability that makes them unsuitable for handling repetitive tasks which are otherwise reliably and efficiently tackled by agentic workflows built on platforms like n8n and Zapier. Earlier works like CodeAct, DynaSaur, Code Mode have tried to tackle the first two issues by using the whole Python language as its action space: The number of tools that the agent can call becomes infinite. Python code blocks can execute complex actions into a single step and print only relevant results which helps in keeping the context lean. However, the probabilistic instability issue still remains, as for the same task in the same environment, the agent can follow different trajectories due to the probabilistic nature of LLMs. Therefore, we need procedural memory for consistency and reliability. This paper proposes CodeMem, an architecture to implement procedural memory via code which can be used to build and run reusable agentic workflows with deterministic reliability.

</details>


### [190] [OLAF: Towards Robust LLM-Based Annotation Framework in Empirical Software Engineering](https://arxiv.org/abs/2512.15979)
*Mia Mohammad Imran,Tarannum Shaila Zaman*

Main category: cs.SE

TL;DR: 现有研究对大语言模型在软件工程实证研究中注释的可靠性和可重复性探索不足，本文提出OLAF框架以推动更透明和可重复的注释研究。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在软件工程实证研究注释任务中的可靠性和可重复性未得到充分探索，现有研究缺乏标准化措施和关键配置细节。

Method: 提出操作化大语言模型注释框架（OLAF），组织关键概念。

Result: 提出OLAF概念框架。

Conclusion: 本文旨在推动软件工程研究中关于大语言模型注释的方法讨论和实证工作，以实现更透明和可重复的注释。

Abstract: Large Language Models (LLMs) are increasingly used in empirical software engineering (ESE) to automate or assist annotation tasks such as labeling commits, issues, and qualitative artifacts. Yet the reliability and reproducibility of such annotations remain underexplored. Existing studies often lack standardized measures for reliability, calibration, and drift, and frequently omit essential configuration details. We argue that LLM-based annotation should be treated as a measurement process rather than a purely automated activity. In this position paper, we outline the \textbf{Operationalization for LLM-based Annotation Framework (OLAF)}, a conceptual framework that organizes key constructs: \textit{reliability, calibration, drift, consensus, aggregation}, and \textit{transparency}. The paper aims to motivate methodological discussion and future empirical work toward more transparent and reproducible LLM-based annotation in software engineering research.

</details>


### [191] [Embedding Software Intent: Lightweight Java Module Recovery](https://arxiv.org/abs/2512.15980)
*Yirui He,Yuqi Huai,Xingyu Chen,Joshua Garcia*

Main category: cs.SE

TL;DR: 随着软件系统规模增大，依赖代码级抽象不切实际，现有架构恢复技术模块化 Java 项目有挑战，本文提出 ClassLAR 方法从 Java 系统中恢复模块，在评估中性能出色。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统规模增大，仅依靠代码级抽象管理系统不现实，且现有架构恢复技术在将现有单体 Java 项目模块化到 JPMS 模块时效果不佳。

Method: 提出 ClassLAR 方法，利用语言模型从包和类名中提取语义信息，使用全限定类名从单体 Java 系统中恢复 Java 模块。

Result: 在 20 个流行 Java 项目评估中，ClassLAR 在架构级相似性指标上优于所有现有技术，执行时间快 3.99 到 10.50 倍。

Conclusion: ClassLAR 是一种轻量级、高效的方法，能有效解决现有单体 Java 项目模块化的挑战。

Abstract: As an increasing number of software systems reach unprecedented scale, relying solely on code-level abstractions is becoming impractical. While architectural abstractions offer a means to manage these systems, maintaining their consistency with the actual code has been problematic. The Java Platform Module System (JPMS), introduced in Java 9, addresses this limitation by enabling explicit module specification at the language level. JPMS enhances architectural implementation through improved encapsulation and direct specification of ground-truth architectures within Java projects. Although many projects are written in Java, modularizing existing monolithic projects to JPMS modules is an open challenge due to ineffective module recovery by existing architecture recovery techniques. To address this challenge, this paper presents ClassLAR (Class-and Language model-based Architectural Recovery), a novel, lightweight, and efficient approach that recovers Java modules from monolithic Java systems using fully-qualified class names. ClassLAR leverages language models to extract semantic information from package and class names, capturing both structural and functional intent. In evaluations across 20 popular Java projects, ClassLAR outperformed all state-of-the-art techniques in architectural-level similarity metrics while achieving execution times that were 3.99 to 10.50 times faster.

</details>


### [192] [LLM4Perf: Large Language Models Are Effective Samplers for Multi-Objective Performance Modeling (Copy)](https://arxiv.org/abs/2512.16070)
*Xin Wang,Zhenhao Li,Zishuo Ding*

Main category: cs.SE

TL;DR: 本文研究LLMs能否作为多目标性能建模的有效采样器，通过实验表明LLM引导的方法在多数情况下优于传统基线。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统性能依赖复杂配置选项，构建准确性能模型需有效采样策略，现有方法在多目标优化和利用文档语义信息方面存在不足，受LLMs成功的启发，探究其能否用于多目标性能建模。

Method: 设计并实现基于反馈的框架LLM4Perf，在四个高度可配置的真实系统中系统评估LLM引导的采样过程。

Result: LLM引导的方法在多数情况下优于传统基线，LLM4Perf在近68.8%的评估场景中表现最佳，其配置空间剪枝还能改善基线方法性能，同时展示了LLM组件选择和超参数对有效性的影响。

Conclusion: 本文为LLMs在性能工程中的有效性提供了有力证据，并揭示了其成功的机制。

Abstract: The performance of modern software systems is critically dependent on their complex configuration options. Building accurate performance models to navigate this vast space requires effective sampling strategies, yet existing methods often struggle with multi-objective optimization and cannot leverage semantic information from documentation. The recent success of Large Language Models (LLMs) motivates the central question of this work: Can LLMs serve as effective samplers for multi-objective performance modeling? To explore this, we present a comprehensive empirical study investigating the capabilities and characteristics of LLM-driven sampling. We design and implement LLM4Perf, a feedback-based framework, and use it to systematically evaluate the LLM-guided sampling process across four highly configurable, real-world systems. Our study reveals that the LLM-guided approach outperforms traditional baselines in most cases. Quantitatively, LLM4Perf achieves the best performance in nearly 68.8% (77 out of 112) of all evaluation scenarios, demonstrating its superior effectiveness. We find this effectiveness stems from the LLM's dual capabilities of configuration space pruning and feedback-driven strategy refinement. The effectiveness of this pruning is further validated by the fact that it also improves the performance of the baseline methods in nearly 91.5% (410 out of 448) of cases. Furthermore, we show how the LLM choices for each component and hyperparameters within LLM4Perf affect its effectiveness. Overall, this paper provides strong evidence for the effectiveness of LLMs in performance engineering and offers concrete insights into the mechanisms that drive their success.

</details>


### [193] [Analysis of Design Patterns and Benchmark Practices in Apache Kafka Event-Streaming Systems](https://arxiv.org/abs/2512.16146)
*Muzeeb Mohammad*

Main category: cs.SE

TL;DR: 本文对2015 - 2025年42篇同行评审研究进行结构化综合，识别出9种Kafka设计模式，分析使用趋势和基准测试实践，指出存在的问题并提供实用指导。


<details>
  <summary>Details</summary>
Motivation: Apache Kafka虽广泛应用，但关于可复用架构设计模式和可重复基准测试方法的研究分散，需整合。

Method: 对42篇同行评审研究进行结构化综合，分析使用趋势、部署情况和基准测试实践。

Result: 识别出9种Kafka设计模式，发现配置披露、评估严谨性和可重复性方面存在显著不一致。

Conclusion: 提供统一分类法、模式基准矩阵和决策启发式方法，为设计基于Kafka的事件流系统提供实用指导。

Abstract: Apache Kafka has become a foundational platform for high throughput event streaming, enabling real time analytics, financial transaction processing, industrial telemetry, and large scale data driven systems. Despite its maturity and widespread adoption, consolidated research on reusable architectural design patterns and reproducible benchmarking methodologies remains fragmented across academic and industrial publications. This paper presents a structured synthesis of forty two peer reviewed studies published between 2015 and 2025, identifying nine recurring Kafka design patterns including log compaction, CQRS bus, exactly once pipelines, change data capture, stream table joins, saga orchestration, tiered storage, multi tenant topics, and event sourcing replay. The analysis examines co usage trends, domain specific deployments, and empirical benchmarking practices using standard suites such as TPCx Kafka and the Yahoo Streaming Benchmark, as well as custom workloads. The study highlights significant inconsistencies in configuration disclosure, evaluation rigor, and reproducibility that limit cross study comparison and practical replication. By providing a unified taxonomy, pattern benchmark matrix, and actionable decision heuristics, this work offers practical guidance for architects and researchers designing reproducible, high performance, and fault tolerant Kafka based event streaming systems.

</details>


### [194] [Beyond Blind Spots: Analytic Hints for Mitigating LLM-Based Evaluation Pitfalls](https://arxiv.org/abs/2512.16272)
*Ora Nova Fandina,Eitan Farchi,Shmulik Froimovich,Raviv Gal,Wesam Ibraheem,Rami Katan,Alice Podolsky*

Main category: cs.SE

TL;DR: 研究大语言模型作为代码生成评判者（LaaJ）在遗留代码现代化中的表现，发现其有评估盲点，提出分析检查工具并结合提示提升评估可靠性，还公开数据集和提示。


<details>
  <summary>Details</summary>
Motivation: LaaJ在关键评估任务中存在可靠性问题，为了解其在实际中的局限性，以遗留代码现代化为例进行研究。

Method: 分析生成的COBOL程序和LaaJ判断构建分类法，开发分析检查工具，将提示动态注入LaaJ提示中。

Result: 实验表明LaaJ单独检测错误覆盖率约45%，分析检查工具缺乏解释深度，LaaJ+Hints配置覆盖率最高达94%，解释更丰富准确。

Conclusion: 分析-大语言模型混合方法可显著提高部署管道中的评估可靠性。

Abstract: Large Language Models are increasingly deployed as judges (LaaJ) in code generation pipelines. While attractive for scalability, LaaJs tend to overlook domain specific issues raising concerns about their reliability in critical evaluation tasks. To better understand these limitations in practice, we examine LaaJ behavior in a concrete industrial use case: legacy code modernization via COBOL code generation. In this setting, we find that even production deployed LaaJs can miss domain critical errors, revealing consistent blind spots in their evaluation capabilities.
  To better understand these blind spots, we analyze generated COBOL programs and associated LaaJs judgments, drawing on expert knowledge to construct a preliminary taxonomy. Based on this taxonomy, we develop a lightweight analytic checker tool that flags over 30 domain specific issues observed in practice. We use its outputs as analytic hints, dynamically injecting them into the judges prompt to encourage LaaJ to revisit aspects it may have overlooked.
  Experiments on a test set of 100 programs using four production level LaaJs show that LaaJ alone detects only about 45% of the errors present in the code (in all judges we tested), while the analytic checker alone lacks explanatory depth. When combined, the LaaJ+Hints configuration achieves up to 94% coverage (for the best performing judge and injection prompt) and produces qualitatively richer, more accurate explanations, demonstrating that analytic-LLM hybrids can substantially enhance evaluation reliability in deployed pipelines. We release the dataset and all used prompts.

</details>


### [195] [Using a Sledgehammer to Crack a Nut? Revisiting Automated Compiler Fault Isolation](https://arxiv.org/abs/2512.16335)
*Yibiao Yang,Qingyang Li,Maolin Sun,Jiangchang Wu,Yuming Zhou*

Main category: cs.SE

TL;DR: 研究对比基于BIC的Basic策略与SBFL技术在编译器故障定位中的效果，发现Basic表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有SBFL技术有效性未与实践中广泛采用的BIC策略对比，需填补这一空白。

Method: 用二分查找确定BIC，将该提交中修改的文件标记为潜在故障文件，用60个GCC和60个LLVM的bug进行对比。

Result: Basic表现与SBFL技术相当，在Top-1和Top-5排名指标上常更优。

Conclusion: 为SBFL技术在实际场景中的有效性提供新见解，建议未来研究以Basic为基线。

Abstract: Background: Compilers are fundamental to software development, translating high-level source code into executable software systems. Faults in compilers can have severe consequences and thus effective localization and resolution of compiler bugs are crucial. Problem: In practice, developers often examine version history to identify and investigate bug-inducing commit (BIC) for fixing bugs. However, while numerous sophisticated Spectrum-Based Fault Localization (SBFL) techniques have been proposed for compiler fault isolation, their effectiveness has not been evaluated against the BIC-based strategies widely adopted in practice. Objective: This study aims to bridge this gap by directly comparing a BIC-based strategy, Basic, with representative SBFL techniques in the context of compiler fault localization. The BIC-based strategy closely aligns with common developer practices, as it directly identifies the BIC and treats the files modified in that commit as faulty candidates. Method: The Basic identifies the most recent good release and earliest bad release, and then employs a binary search to pinpoint the bug-inducing commit. All files modified in the identified commit are flagged as potentially faulty. We rigorously compare Basic against SBFL-based techniques using a benchmark consisting of 60 GCC bugs and 60 LLVM bugs. Result: Our analysis reveals that Basic performs comparably to, and in many cases outperforms, state-of-the-art SBFL-based techniques, particularly on the critical Top-1 and Top-5 ranking metrics. Conclusion: This study provides new insights into the practical effectiveness of SBFL-based techniques in real-world compiler debugging scenarios. We recommend that future research adopt Basic as a baseline when developing and evaluating new compiler fault isolation methods.

</details>


### [196] [An Empirical Study of the Realism of Mutants in Deep Learning](https://arxiv.org/abs/2512.16741)
*Zaheed Ahmed,Philip Makedonski,Jens Grabowski*

Main category: cs.SE

TL;DR: 本文对深度学习中预训练和后训练突变方法在逼真度方面进行了实证比较，发现预训练突变体更接近真实故障，但预训练突变计算成本高，需更有效的后训练算子。


<details>
  <summary>Details</summary>
Motivation: 突变分析应用于深度学习，但核心假设（突变体行为类似真实故障）在深度学习中未充分验证，需比较预训练和后训练突变方法的逼真度。

Method: 引入统计框架，使用公开可用的错误数据集，用代表两种方法的先进工具生成突变体。

Result: 预训练突变体与真实故障的耦合更强、行为相似性更高，即更接近真实故障。

Conclusion: 预训练突变计算成本高，需要开发能达到或超越预训练突变逼真度的后训练算子。

Abstract: Mutation analysis is a well-established technique for assessing test quality in the traditional software development paradigm by injecting artificial faults into programs. Its application to deep learning (DL) has expanded beyond classical testing to support tasks such as fault localization, repair, data generation, and model robustness evaluation. The core assumption is that mutants behave similarly to real faults, an assumption well established in traditional software systems but largely unverified for DL.
  This study presents the first empirical comparison of pre-training and post-training mutation approaches in DL with respect to realism. We introduce a statistical framework to quantify their coupling strength and behavioral similarity to real faults using publicly available bugs datasets: CleanML, DeepFD, DeepLocalize, and defect4ML. Mutants are generated using state-of-the-art tools representing both approaches.
  Results show that pre-training mutants exhibit consistently stronger coupling and higher behavioral similarity to real faults than post-training mutants, indicating greater realism. However, the substantial computational cost of pre-training mutation underscores the need for more effective post-training operators that match or exceed the realism demonstrated by pre-training mutants.

</details>


### [197] [Inside Out: Uncovering How Comment Internalization Steers LLMs for Better or Worse](https://arxiv.org/abs/2512.16790)
*Aaron Imani,Mohammad Moshirpour,Iftekhar Ahmed*

Main category: cs.SE

TL;DR: 对软件工程中大型语言模型（LLM）进行概念级可解释性研究，分析注释概念对性能影响并指出新方向。


<details>
  <summary>Details</summary>
Motivation: 了解LLM在软件工程任务中对注释的依赖位置及影响。

Method: 使用概念激活向量（CAV）分析内部注释表示，系统激活和停用概念，进行控制实验。

Result: LLM将注释内化为不同潜在概念，性能有显著、特定于模型和任务的变化，代码摘要对注释概念激活最强，代码完成敏感性最弱。

Conclusion: 为构建能推理和处理内部概念表示的软件工程工具和模型开辟新方向。

Abstract: While comments are non-functional elements of source code, Large Language Models (LLM) frequently rely on them to perform Software Engineering (SE) tasks. Yet, where in the model this reliance resides, and how it affects performance, remains poorly understood. We present the first concept-level interpretability study of LLMs in SE, analyzing three tasks - code completion, translation, and refinement - through the lens of internal comment representation. Using Concept Activation Vectors (CAV), we show that LLMs not only internalize comments as distinct latent concepts but also differentiate between subtypes such as Javadocs, inline, and multiline comments. By systematically activating and deactivating these concepts in the LLMs' embedding space, we observed significant, model-specific, and task-dependent shifts in performance ranging from -90% to +67%. Finally, we conducted a controlled experiment using the same set of code inputs, prompting LLMs to perform 10 distinct SE tasks while measuring the activation of the comment concept within their latent representations. We found that code summarization consistently triggered the strongest activation of comment concepts, whereas code completion elicited the weakest sensitivity. These results open a new direction for building SE tools and models that reason about and manipulate internal concept representations rather than relying solely on surface-level input.

</details>


### [198] [Toward Systematic Counterfactual Fairness Evaluation of Large Language Models: The CAFFE Framework](https://arxiv.org/abs/2512.16816)
*Alessandra Parziale,Gianmario Voria,Valeria Pontillo,Gemma Catolino,Andrea De Lucia,Fabio Palomba*

Main category: cs.SE

TL;DR: 本文提出CAFFE框架用于测试大语言模型的反事实公平性，实验表明其比现有变形测试方法更优。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型影响力增加，公平性问题受关注，现有工作用变形测试检测公平性问题，本文提出新视角。

Method: 提出CAFFE框架，形式化定义测试用例组件，自动生成测试数据，用语义相似度指标评估模型响应。

Result: 在三种不同架构的大语言模型上实验，CAFFE比现有变形方法有更广泛的偏差覆盖和更可靠的不公平行为检测。

Conclusion: CAFFE框架在测试大语言模型反事实公平性上优于现有变形测试方法。

Abstract: Nowadays, Large Language Models (LLMs) are foundational components of modern software systems. As their influence grows, concerns about fairness have become increasingly pressing. Prior work has proposed metamorphic testing to detect fairness issues, applying input transformations to uncover inconsistencies in model behavior. This paper introduces an alternative perspective for testing counterfactual fairness in LLMs, proposing a structured and intent-aware framework coined CAFFE (Counterfactual Assessment Framework for Fairness Evaluation). Inspired by traditional non-functional testing, CAFFE (1) formalizes LLM-Fairness test cases through explicitly defined components, including prompt intent, conversational context, input variants, expected fairness thresholds, and test environment configuration, (2) assists testers by automatically generating targeted test data, and (3) evaluates model responses using semantic similarity metrics. Our experiments, conducted on three different architectural families of LLM, demonstrate that CAFFE achieves broader bias coverage and more reliable detection of unfair behavior than existing metamorphic approaches.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [199] [A High-Level Framework for Practically Model-Independent Pricing](https://arxiv.org/abs/2512.15718)
*Marco Airoldi*

Main category: q-fin.CP

TL;DR: 提出解释不同定价模型对奇异衍生品估值相似原因的框架，结合路径重加权与锥优化层，给出模型无关价格区间。


<details>
  <summary>Details</summary>
Motivation: 解释不同定价模型校准到相同普通期权曲面时对奇异衍生品估值相似的原因。

Method: 在银行现有的蒙特卡罗基础设施上叠加路径重加权与锥优化层，且不改变现有代码。

Result: 为奇异衍生品提供了狭窄的、实际上与模型无关的价格区间。

Conclusion: 该框架调和了前台实践与学术文献中稳健的、与模型无关的思想。

Abstract: We present a high-level framework that explains why, in practice, different pricing models calibrated to the same vanilla surface tend to produce similar valuations for exotic derivatives. Our approach acts as an overlay on the Monte Carlo infrastructure already used in banks, combining path reweighting with a conic optimisation layer without requiring any changes to existing code. This construction delivers narrow, practically model-independent price bands for exotics, reconciling front-office practice with the robust, model-independent ideas developed in the academic literature.

</details>


### [200] [An Efficient Machine Learning Framework for Option Pricing via Fourier Transform](https://arxiv.org/abs/2512.16115)
*Liying Zhang,Ying Gao*

Main category: q-fin.CP

TL;DR: 提出混合算法框架用于快速定价多路径无关期权，训练模型构建代理定价算子，实验显示有加速效果并克服FFT方法局限。


<details>
  <summary>Details</summary>
Motivation: 动态市场中对期权定价模型快速重新校准需求增加，对数据生成和估值算法有严格计算要求。

Method: 将平滑偏移算法（SOA）与监督机器学习模型集成，基于SOA生成的数据集训练神经网络、随机森林和梯度提升决策树构建代理定价算子。

Result: 训练后的代理定价算子比直接SOA评估实现数量级加速。

Conclusion: 所提框架克服了基于快速傅里叶变换方法的关键数值局限。

Abstract: The increasing need for rapid recalibration of option pricing models in dynamic markets places stringent computational demands on data generation and valuation algorithms. In this work, we propose a hybrid algorithmic framework that integrates the smooth offset algorithm (SOA) with supervised machine learning models for the fast pricing of multiple path-independent options under exponential Lévy dynamics. Building upon the SOA-generated dataset, we train neural networks, random forests, and gradient boosted decision trees to construct surrogate pricing operators. Extensive numerical experiments demonstrate that, once trained, these surrogates achieve order-of-magnitude acceleration over direct SOA evaluation. Importantly, the proposed framework overcomes key numerical limitations inherent to fast Fourier transform-based methods, including the consistency of input data and the instability in deep out-of-the-money option pricing.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [201] [Bayesian Modeling for Uncertainty Management in Financial Risk Forecasting and Compliance](https://arxiv.org/abs/2512.15739)
*Sharif Al Mamun,Rakib Hossain,Md. Jobayer Rahman,Malay Kumar Devnath,Farhana Afroz,Lisan Al Amin*

Main category: q-fin.RM

TL;DR: 本文提出贝叶斯分析框架用于金融风险管理，在市场波动预测、欺诈检测和合规监测中表现良好，虽有挑战但可提供风险洞察，未来将扩展功能。


<details>
  <summary>Details</summary>
Motivation: 开发能精确量化不确定性的方法，提升金融风险管理中风险处理能力。

Method: 开发集成方法，构建概率、可解释模型，评估不同模型在风险预测等方面的表现。

Result: LSTM基线接近名义校准，GARCH(1,1)低估尾部风险，提出的DLM模型有轻微宽松的VaR估计；贝叶斯逻辑回归提升欺诈检测效果，分层Beta状态空间模型用于合规风险评估；分析有精确不确定性量化、可解释性和GPU加速。

Conclusion: 该框架能提供可操作的风险洞察，虽存在稀疏欺诈数据和代理合规标签等挑战，未来将扩展特征集、探索先验和增强可扩展推理。

Abstract: A Bayesian analytics framework that precisely quantifies uncertainty offers a significant advance for financial risk management. We develop an integrated approach that consistently enhances the handling of risk in market volatility forecasting, fraud detection, and compliance monitoring. Our probabilistic, interpretable models deliver reliable results: We evaluate the performance of one-day-ahead 95% Value-at-Risk (VaR) forecasts on daily S&P 500 returns, with a training period from 2000 to 2019 and an out-of-sample test period spanning 2020 to 2024. Formal tests of unconditional (Kupiec) and conditional (Christoffersen) coverage reveal that an LSTM baseline achieves near-nominal calibration. In contrast, a GARCH(1,1) model with Student-t innovations underestimates tail risk. Our proposed discount-factor DLM model produces a slightly liberal VaR estimate, with evidence of clustered violations. Bayesian logistic regression improves recall and AUC-ROC for fraud detection, and a hierarchical Beta state-space model provides transparent and adaptive compliance risk assessment. The pipeline is distinguished by precise uncertainty quantification, interpretability, and GPU-accelerated analysis, delivering up to 50x speedup. Remaining challenges include sparse fraud data and proxy compliance labels, but the framework enables actionable risk insights. Future expansion will extend feature sets, explore regime-switching priors, and enhance scalable inference.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [202] [Hidden Order in Trades Predicts the Size of Price Moves](https://arxiv.org/abs/2512.15720)
*Mainak Singha*

Main category: q-fin.TR

TL;DR: 研究表明实时订单流熵可预测日内收益率幅度但无方向信息，需扩展验证


<details>
  <summary>Details</summary>
Motivation: 解决金融市场中价格方向难预测但价格变动幅度有结构的悖论

Method: 用15状态马尔可夫转移矩阵计算实时订单流熵，分析3850万笔SPY交易，进行向前验证和标签置换安慰剂测试

Result: 熵低于5%分位数时，后续5分钟绝对收益率提高2.89倍，方向准确率45%，向前验证确认样本外可预测性，安慰剂测试拒绝原假设

Conclusion: 信息论指标可作市场微观结构中的波动率状态变量，但需扩展验证

Abstract: Financial markets exhibit an apparent paradox: while directional price movements remain largely unpredictable--consistent with weak-form efficiency--the magnitude of price changes displays systematic structure. Here we demonstrate that real-time order-flow entropy, computed from a 15-state Markov transition matrix at second resolution, predicts the magnitude of intraday returns without providing directional information. Analysis of 38.5 million SPY trades over 36 trading days reveals that conditioning on entropy below the 5th percentile increases subsequent 5-minute absolute returns by a factor of 2.89 (t = 12.41, p < 0.0001), while directional accuracy remains at 45.0%--statistically indistinguishable from chance (p = 0.12). This decoupling arises from a fundamental symmetry: entropy is invariant under sign permutation, detecting the presence of informed trading without revealing its direction. Walk-forward validation across five non-overlapping test periods confirms out-of-sample predictability, and label-permutation placebo tests yield z = 14.4 against the null. These findings suggest that information-theoretic measures may serve as volatility state variables in market microstructure, though the limited sample (36 days, single instrument) requires extended validation.

</details>


### [203] [The Red Queen's Trap: Limits of Deep Evolution in High-Frequency Trading](https://arxiv.org/abs/2512.15732)
*Yijia Chen*

Main category: q-fin.TR

TL;DR: 本文对‘Galaxy Empire’框架进行分析，发现训练指标和实际表现差异大，指出三种失败模式，表明无信息不对称时增加模型复杂度会加剧系统脆弱性。


<details>
  <summary>Details</summary>
Motivation: 探索深度强化学习和进化计算集成用于算法交易，以实现系统自主适应非平稳市场机制。

Method: 对‘Galaxy Empire’框架进行事后分析，在高频加密货币环境中部署500个自主代理。

Result: 训练指标（验证年化收益率>300%）和实际表现（资金衰减>70%）存在巨大差异。

Conclusion: 无信息不对称时增加模型复杂度会加剧系统脆弱性。

Abstract: The integration of Deep Reinforcement Learning (DRL) and Evolutionary Computation (EC) is frequently hypothesized to be the "Holy Grail" of algorithmic trading, promising systems that adapt autonomously to non-stationary market regimes. This paper presents a rigorous post-mortem analysis of "Galaxy Empire," a hybrid framework coupling LSTM/Transformer-based perception with a genetic "Time-is-Life" survival mechanism. Deploying a population of 500 autonomous agents in a high-frequency cryptocurrency environment, we observed a catastrophic divergence between training metrics (Validation APY $>300\%$) and live performance (Capital Decay $>70\%$). We deconstruct this failure through a multi-disciplinary lens, identifying three critical failure modes: the overfitting of \textit{Aleatoric Uncertainty} in low-entropy time-series, the \textit{Survivor Bias} inherent in evolutionary selection under high variance, and the mathematical impossibility of overcoming microstructure friction without order-flow data. Our findings provide empirical evidence that increasing model complexity in the absence of information asymmetry exacerbates systemic fragility.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [204] [BayesSum: Bayesian Quadrature in Discrete Spaces](https://arxiv.org/abs/2512.16105)
*Sophia Seulkee Kang,François-Xavier Briol,Toni Karvonen,Zonghao Chen*

Main category: stat.ML

TL;DR: 提出 BayesSum 估计器以解决离散域上难以处理的期望估计问题，理论和实验表明它比现有方法更高效。


<details>
  <summary>Details</summary>
Motivation: 现有离散域期望估计方法（如蒙特卡洛和俄罗斯轮盘赌估计器）需大量样本才能获得准确结果。

Method: 提出 BayesSum 估计器，将贝叶斯求积扩展到离散域，通过高斯过程利用被积函数的先验信息。

Result: 理论推导证明 BayesSum 在广泛设置下收敛速度比蒙特卡洛方法快得多，实验表明其在多个合成设置和模型参数估计中所需样本更少。

Conclusion: BayesSum 估计器相较现有方法更具样本效率，能更高效地解决离散域期望估计问题。

Abstract: This paper addresses the challenging computational problem of estimating intractable expectations over discrete domains. Existing approaches, including Monte Carlo and Russian Roulette estimators, are consistent but often require a large number of samples to achieve accurate results. We propose a novel estimator, \emph{BayesSum}, which is an extension of Bayesian quadrature to discrete domains. It is more sample efficient than alternatives due to its ability to make use of prior information about the integrand through a Gaussian process. We show this through theory, deriving a convergence rate significantly faster than Monte Carlo in a broad range of settings. We also demonstrate empirically that our proposed method does indeed require fewer samples on several synthetic settings as well as for parameter estimation for Conway-Maxwell-Poisson and Potts models.

</details>


### [205] [DAG Learning from Zero-Inflated Count Data Using Continuous Optimization](https://arxiv.org/abs/2512.16233)
*Noriaki Sato,Marco Scutari,Shuichi Kawano,Rui Yamaguchi,Seiya Imoto*

Main category: stat.ML

TL;DR: 提出ZICO方法从零膨胀计数数据学习网络结构，在模拟数据和基因调控网络逆向工程中表现良好，且可处理大规模变量集。


<details>
  <summary>Details</summary>
Motivation: 解决从零膨胀计数数据进行网络结构学习的问题。

Method: 将每个节点视为零膨胀广义线性模型，在有向无环图约束下优化基于得分的平滑目标，采用节点似然和规范链接，通过可微代理约束结合稀疏正则化确保无环性。

Result: 在模拟数据上性能优越、运行时间更快，在基因调控网络逆向工程中与常见算法相当或更好。

Conclusion: ZICO方法可完全向量化和小批量处理，能在多个领域以实际运行时间学习更大变量集。

Abstract: We address network structure learning from zero-inflated count data by casting each node as a zero-inflated generalized linear model and optimizing a smooth, score-based objective under a directed acyclic graph constraint. Our Zero-Inflated Continuous Optimization (ZICO) approach uses node-wise likelihoods with canonical links and enforces acyclicity through a differentiable surrogate constraint combined with sparsity regularization. ZICO achieves superior performance with faster runtimes on simulated data. It also performs comparably to or better than common algorithms for reverse engineering gene regulatory networks. ZICO is fully vectorized and mini-batched, enabling learning on larger variable sets with practical runtimes in a wide range of domains.

</details>


### [206] [Advantages and limitations in the use of transfer learning for individual treatment effects in causal machine learning](https://arxiv.org/abs/2512.16489)
*Seyda Betul Aydin,Holger Brandt*

Main category: stat.ML

TL;DR: 本文提出用迁移学习改进TARNet估计个体治疗效应（ITE），模拟和实证表明该方法能减少小样本ITE估计误差和偏差。


<details>
  <summary>Details</summary>
Motivation: 在不同环境中推广因果知识具有挑战性，基于机器学习的ITE模型估计需要大样本，限制了其在小数据集领域的应用。

Method: 使用迁移学习（TL - TARNet）将源数据集的知识应用到新环境中改进TARNet估计ITE。

Result: 模拟中，TL - TARNet在有大的无偏源数据且目标样本小时能减少ITE误差和偏差；实证中，迁移学习使目标平均ITE接近源ITE估计，减少无迁移估计的偏差。

Conclusion: 因果模型的迁移学习能改善小样本中ITE的估计。

Abstract: Generalizing causal knowledge across diverse environments is challenging, especially when estimates from large-scale datasets must be applied to smaller or systematically different contexts, where external validity is critical. Model-based estimators of individual treatment effects (ITE) from machine learning require large sample sizes, limiting their applicability in domains such as behavioral sciences with smaller datasets. We demonstrate how estimation of ITEs with Treatment Agnostic Representation Networks (TARNet; Shalit et al., 2017) can be improved by leveraging knowledge from source datasets and adapting it to new settings via transfer learning (TL-TARNet; Aloui et al., 2023). In simulations that vary source and sample sizes and consider both randomized and non-randomized intervention target settings, the transfer-learning extension TL-TARNet improves upon standard TARNet, reducing ITE error and attenuating bias when a large unbiased source is available and target samples are small. In an empirical application using the India Human Development Survey (IHDS-II), we estimate the effect of mothers' firewood collection time on children's weekly study time; transfer learning pulls the target mean ITEs toward the source ITE estimate, reducing bias in the estimates obtained without transfer. These results suggest that transfer learning for causal models can improve the estimation of ITE in small samples.

</details>


### [207] [Riemannian Stochastic Interpolants for Amorphous Particle Systems](https://arxiv.org/abs/2512.16607)
*Louis Grenioux,Leonardo Galliano,Ludovic Berthier,Giulio Biroli,Marylou Gabrié*

Main category: stat.ML

TL;DR: 本文针对玻璃态非晶材料平衡构型采样难题，利用等变黎曼随机插值框架解决问题，实验表明引入几何和对称约束可提升生成性能。


<details>
  <summary>Details</summary>
Motivation: 现代生成模型需适配不同领域，非晶材料平衡构型采样慢且难，需开发能生成明确定义似然的平衡构型的生成框架。

Method: 利用结合黎曼随机插值和等变流匹配的等变黎曼随机插值框架，将等变图神经网络调整为直接在环面上操作，严格纳入周期性边界条件和多组分粒子系统对称性。

Result: 在模型非晶系统上的数值实验显示，引入几何和对称约束显著提升了生成性能。

Conclusion: 通过等变黎曼随机插值框架并结合相关约束，能有效应对非晶材料平衡构型采样难题，提升生成性能。

Abstract: Modern generative models hold great promise for accelerating diverse tasks involving the simulation of physical systems, but they must be adapted to the specific constraints of each domain. Significant progress has been made for biomolecules and crystalline materials. Here, we address amorphous materials (glasses), which are disordered particle systems lacking atomic periodicity. Sampling equilibrium configurations of glass-forming materials is a notoriously slow and difficult task. This obstacle could be overcome by developing a generative framework capable of producing equilibrium configurations with well-defined likelihoods. In this work, we address this challenge by leveraging an equivariant Riemannian stochastic interpolation framework which combines Riemannian stochastic interpolant and equivariant flow matching. Our method rigorously incorporates periodic boundary conditions and the symmetries of multi-component particle systems, adapting an equivariant graph neural network to operate directly on the torus. Our numerical experiments on model amorphous systems demonstrate that enforcing geometric and symmetry constraints significantly improves generative performance.

</details>


### [208] [On The Hidden Biases of Flow Matching Samplers](https://arxiv.org/abs/2512.16768)
*Soon Hoe Lim*

Main category: stat.ML

TL;DR: 本文通过经验流匹配研究流匹配采样器的隐式偏差，分析经验流匹配的结构和能量偏差。


<details>
  <summary>Details</summary>
Motivation: 研究流匹配采样器在经验流匹配下的隐式偏差。

Method: 分析经验流匹配极小值、生成样本的动能，考虑高斯和重尾源分布。

Result: 经验流匹配极小值几乎不是梯度场，高斯源下动能指数集中，重尾源有多项式尾部。

Conclusion: 给出了经验流匹配中结构和能量偏差的数学解释。

Abstract: We study the implicit bias of flow matching (FM) samplers via the lens of empirical flow matching. Although population FM may produce gradient-field velocities resembling optimal transport (OT), we show that the empirical FM minimizer is almost never a gradient field, even when each conditional flow is. Consequently, empirical FM is intrinsically energetically suboptimal. In view of this, we analyze the kinetic energy of generated samples. With Gaussian sources, both instantaneous and integrated kinetic energies exhibit exponential concentration, while heavy-tailed sources lead to polynomial tails. These behaviors are governed primarily by the choice of source distribution rather than the data. Overall, these notes provide a concise mathematical account of the structural and energetic biases arising in empirical FM.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [209] [Can Transformers overcome the lack of data in the simulation of history-dependent flows?](https://arxiv.org/abs/2512.16305)
*P. Urdeitx,I. Alfaro,D. Gonzalez,F. Chinesta,E. Cueto*

Main category: physics.flu-dyn

TL;DR: 研究Transformer架构应对变量实验数据缺失的能力，在三个基准问题上评估，结果显示其在数据缺失系统中表现优于有metriplectic偏差的神经网络，变量可全知时metriplectic模型更优。


<details>
  <summary>Details</summary>
Motivation: 传统为弥补动力学系统描述中变量信息缺失而设计的现象学变量难实验测量，研究Transformer架构应对这些变量实验数据缺失的能力。

Method: 在三个基准问题（无历史依赖的圆柱流、用Oldroyd - B形式化建模的粘弹性库埃特流、由FENE模型描述的非线性聚合物流体）上评估方法。

Result: Transformer在实验数据缺失的系统中优于有metriplectic偏差的热力学一致、结构保留神经网络，在低维潜空间误差更低；变量可全知时metriplectic模型表现更好。

Conclusion: Transformer架构在变量实验数据缺失的系统中有更好的表现，而变量可全知时metriplectic模型更具优势。

Abstract: It is well known that the lack of information about certain variables necessary for the description of a dynamical system leads to the introduction of historical dependence (lack of Markovian character of the model) and noise. Traditionally, scientists have made up for these shortcomings by designing phenomenological variables that take into account this historical dependence (typically, conformational tensors in fluids). Often, these phenomenological variables are not easily measurable experimentally. In this work, we study to what extent Transformer architectures are able to cope with the lack of experimental data on these variables. The methodology is evaluated on three benchmark problems: a cylinder flow with no history dependence, a viscoelastic Couette flow modeled via the Oldroyd-B formalism, and a non-linear polymeric fluid described by the FENE model. Our results show that the Transformer outperforms a thermodynamically consistent, structure-preserving neural network with metriplectic bias in systems with missing experimental data, providing lower errors even in low-dimensional latent spaces. In contrast, for systems whose state variables can be fully known, the metriplectic model achieves superior performance.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [210] [From Facts to Conclusions : Integrating Deductive Reasoning in Retrieval-Augmented LLMs](https://arxiv.org/abs/2512.16795)
*Shubham Mishra,Samyek Jain,Gorang Mehrishi,Shiv Tiwari,Harsh Sharma,Pratik Narang,Dhruv Kumar*

Main category: cs.CL

TL;DR: 提出推理跟踪增强的RAG框架及CATS管道，建立数据集和评估管道，实验显示效果优于基线。


<details>
  <summary>Details</summary>
Motivation: 解决RAG在检索源冲突、信息过时或主观时失效，且缺乏统一推理监督的问题。

Method: 提出推理跟踪增强的RAG框架，包含文档裁决、冲突分析和基于证据合成三个阶段；引入CATS管道，用LLM作为评判器评估；建立推理数据集和评估管道。

Result: 实验对比基线有显著提升，如Qwen的端到端答案正确性从0.069提升到0.883，行为遵循度从0.074提升到0.722。

Conclusion: 所提方法能有效提升RAG系统在冲突感知和可解释性方面的性能。

Abstract: Retrieval-Augmented Generation (RAG) grounds large language models (LLMs) in external evidence, but fails when retrieved sources conflict or contain outdated or subjective information. Prior work address these issues independently but lack unified reasoning supervision. We propose a reasoning-trace-augmented RAG framework that adds structured, interpretable reasoning across three stages : (1) document-level adjudication, (2) conflict analysis, and (3) grounded synthesis, producing citation-linked answers or justified refusals. A Conflict-Aware Trust-Score (CATS) pipeline is introduced which evaluates groundedness, factual correctness, refusal accuracy, and conflict-behavior alignment using an LLM-as-a-Judge. Our 539-query reasoning dataset and evaluation pipeline establish a foundation for conflict-aware, interpretable RAG systems. Experimental results demonstrate substantial gains over baselines, most notably with Qwen, where Supervised Fine-Tuning improved End-to-End answer correctness from 0.069 to 0.883 and behavioral adherence from 0.074 to 0.722.

</details>


### [211] [Social Story Frames: Contextual Reasoning about Narrative Intent and Reception](https://arxiv.org/abs/2512.15925)
*Joel Mire,Maria Antoniak,Steven R. Wilson,Zexin Ma,Achyutarama R. Ganti,Andrew Piper,Maarten Sap*

Main category: cs.CL

TL;DR: 文章引入SocialStoryFrames用于研究读者对故事的反应，开发相关模型并在数据集上分析，推动在线社区故事讲述研究。


<details>
  <summary>Details</summary>
Motivation: 现有读者反应计算模型有限，无法进行细致分析，需新方法填补空白。

Method: 引入SocialStoryFrames，开发SSF - Generator和SSF - Classifier模型，通过人类调查和专家注释验证，在SSF - Corpus数据集上分析。

Result: 可表征故事意图频率和相互依赖关系，对比不同社区叙事实践。

Conclusion: SocialStoryFrames结合细粒度、上下文敏感建模与通用读者反应分类法，能推动在线社区故事讲述新研究。

Abstract: Reading stories evokes rich interpretive, affective, and evaluative responses, such as inferences about narrative intent or judgments about characters. Yet, computational models of reader response are limited, preventing nuanced analyses. To address this gap, we introduce SocialStoryFrames, a formalism for distilling plausible inferences about reader response, such as perceived author intent, explanatory and predictive reasoning, affective responses, and value judgments, using conversational context and a taxonomy grounded in narrative theory, linguistic pragmatics, and psychology. We develop two models, SSF-Generator and SSF-Classifier, validated through human surveys (N=382 participants) and expert annotations, respectively. We conduct pilot analyses to showcase the utility of the formalism for studying storytelling at scale. Specifically, applying our models to SSF-Corpus, a curated dataset of 6,140 social media stories from diverse contexts, we characterize the frequency and interdependence of storytelling intents, and we compare and contrast narrative practices (and their diversity) across communities. By linking fine-grained, context-sensitive modeling with a generic taxonomy of reader responses, SocialStoryFrames enable new research into storytelling in online communities.

</details>


### [212] [BRAID: Bounded Reasoning for Autonomous Inference and Decisions](https://arxiv.org/abs/2512.15959)
*Armağan Amcalar,Eyup Cinar*

Main category: cs.CL

TL;DR: 论文对不同GPT模型层级，用BRAID进行结构化提示的量化研究，表明其能提升推理准确性和成本效益。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型中性能、成本和令牌使用的非线性关系问题，优化推理效率。

Method: 使用BRAID在多个GPT模型层级进行结构化提示，基于Mermaid的指令图构建有界推理框架。

Result: 结构化的机器可读提示显著提高了生产系统中智能体的推理准确性和成本效益。

Conclusion: BRAID是优化自主智能体系统推理效率的有效且可扩展的技术。

Abstract: Large Language Models (LLMs) exhibit nonlinear relationships between performance, cost, and token usage. This paper presents a quantitative study on structured prompting using BRAID (Bounded Reasoning for Au tonomous Inference and Decisions) across multiple GPT model tiers, eval uated on the AdvancedIF, GSM-Hard, and the SCALE MultiChallenge benchmark datasets. BRAID introduces a bounded reasoning framework using Mermaid-based instruction graphs that enable models to reason struc turally rather than through unbounded natural-language token expansion. We show that structured machine-readable prompts substantially increase reasoning accuracy and cost efficiency for agents in production systems. The findings establish BRAID as an effective and scalable technique for optimizing inference efficiency in autonomous agent systems. All datasets and detailed result logs are available at https://benchmark.openserv.ai.

</details>


### [213] [Are We on the Right Way to Assessing LLM-as-a-Judge?](https://arxiv.org/abs/2512.16041)
*Yuanning Feng,Sinan Wang,Zhengxiang Cheng,Yao Wan,Dongping Chen*

Main category: cs.CL

TL;DR: 介绍新评估套件Sage评估LLM裁判质量，无需人工标注，实验证明其可靠性，揭示当前LLM作裁判有可靠性问题，给出提升方法并指出人类判断也有不一致性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-as-a-Judge基准依赖人工标注，有人类偏差且扩展性受限，故提出无需人工标注的评估套件。

Method: 受理性选择理论公理启发，引入本地自一致性和全局逻辑一致性两个评估维度，整理含650个问题的数据集。

Result: 实验证明Sage指标稳定且与监督基准高度相关，当前SOTA模型作裁判时存在可靠性问题，微调、基于面板的裁判和深度推理可提升一致性，人类判断也存在不一致性。

Conclusion: Sage是评估LLM-as-a-Judge的可靠套件，当前LLM作裁判有问题，可通过微调等方法提升性能，人类标注并非可靠的黄金标准。

Abstract: LLM-as-a-Judge has been widely adopted as an evaluation method and served as supervised rewards in model training. However, existing benchmarks for LLM-as-a-Judge are mainly relying on human-annotated ground truth, which introduces human bias that undermines the assessment of reliability and imposes scalability constraints. To overcome these limitations, we introduce Sage, a novel evaluation suite that assesses the quality of LLM judges without necessitating any human annotation. Inspired by axioms of rational choice theory, Sage introduces two new lenses for measuring LLM-as-a-Judge: local self-consistency (pair-wise preference stability) and global logical consistency (transitivity across a full set of preferences). We curate a dataset of 650 questions by combining structured benchmark problems with real-world user queries. Our experiments demonstrate both the stability of our metrics and their high correlation with supervised benchmarks like LLMBar and RewardBench2, confirming Sage's reliability as an evaluation suite for the robustness and accuracy of LLM-as-a-Judge. Based on Sage, we reveal that current state-of-the-art LLMs exhibit significant reliability problems when acting as judges in both scoring and pairwise settings; even the top-performing models, Gemini-2.5-Pro and GPT-5, fail to maintain consistent preferences in nearly a quarter of difficult cases. We attribute this to a new phenomenon called situational preference, which explains why explicit rubrics or criteria can help the model judge consistently across answer pairs. Our further analysis shows that finetuned LLM-as-a-Judge is a feasible method to boost performance, and the panel-based judge as well as deep reasoning can enhance the judging consistency. We also find substantial inconsistency in human judgments, which indicates that human annotation may not be a reliable gold standard.

</details>


### [214] [MRG-R1: Reinforcement Learning for Clinically Aligned Medical Report Generation](https://arxiv.org/abs/2512.16145)
*Pengyu Wang,Shuchang Ye,Usman Naseem,Jinman Kim*

Main category: cs.CL

TL;DR: 提出基于大视觉语言模型的语义驱动强化学习方法用于医疗报告生成，在两个数据集上取得SOTA性能，证明优化报告级奖励能提升临床正确性。


<details>
  <summary>Details</summary>
Motivation: 现有医疗报告生成方法训练基于词级目标，无法保证临床正确性。

Method: 提出语义驱动强化学习（SRL）方法，采用GRPO优化，计算基于边际的余弦相似度（MCCS）作为报告级奖励，使用轻量级推理格式约束。

Result: 在IU X - Ray和MIMIC - CXR数据集上，MRG - R1的CE - F1分别达到51.88和40.39，标签语义强化优于传统词级监督。

Conclusion: 优化基于临床的报告级奖励而非词重叠，能有效提升临床正确性，是探索医疗大视觉语言模型训练中语义强化监督医疗正确性的先驱工作。

Abstract: Medical report generation (MRG) aims to automatically derive radiology-style reports from medical images to aid in clinical decision-making. However, existing methods often generate text that mimics the linguistic style of radiologists but fails to guarantee clinical correctness, because they are trained on token-level objectives which focus on word-choice and sentence structure rather than actual medical accuracy. We propose a semantic-driven reinforcement learning (SRL) method for medical report generation, adopted on a large vision-language model (LVLM). SRL adopts Group Relative Policy Optimization (GRPO) to encourage clinical-correctness-guided learning beyond imitation of language style. Specifically, we optimise a report-level reward: a margin-based cosine similarity (MCCS) computed between key radiological findings extracted from generated and reference reports, thereby directly aligning clinical-label agreement and improving semantic correctness. A lightweight reasoning format constraint further guides the model to generate structured "thinking report" outputs. We evaluate Medical Report Generation with Sematic-driven Reinforment Learning (MRG-R1), on two datasets: IU X-Ray and MIMIC-CXR using clinical efficacy (CE) metrics. MRG-R1 achieves state-of-the-art performance with CE-F1 51.88 on IU X-Ray and 40.39 on MIMIC-CXR. We found that the label-semantic reinforcement is better than conventional token-level supervision. These results indicate that optimizing a clinically grounded, report-level reward rather than token overlap,meaningfully improves clinical correctness. This work is a prior to explore semantic-reinforcement in supervising medical correctness in medical Large vision-language model(Med-LVLM) training.

</details>


### [215] [Decoding Fake Narratives in Spreading Hateful Stories: A Dual-Head RoBERTa Model with Multi-Task Learning](https://arxiv.org/abs/2512.16147)
*Yash Bhaskar,Sankalp Bahad,Parameswari Krishnamurthy*

Main category: cs.CL

TL;DR: 社交媒体平台成为有害内容传播地，本文针对Faux - Hate共享任务开发系统，结合NLP技术和领域预训练，取得有竞争力结果。


<details>
  <summary>Details</summary>
Motivation: 社交媒体平台快速传播有害内容，Faux - Hate共享任务旨在检测由虚假叙述驱动的仇恨言论，本文以此为背景开发系统。

Method: 结合先进自然语言处理技术和领域特定预训练来处理二进制Faux - Hate检测和目标与严重程度预测两个子任务。

Result: 系统取得有竞争力的结果。

Conclusion: 利用多任务学习处理该复杂问题是有效的。

Abstract: Social media platforms, while enabling global connectivity, have become hubs for the rapid spread of harmful content, including hate speech and fake narratives \cite{davidson2017automated, shu2017fake}. The Faux-Hate shared task focuses on detecting a specific phenomenon: the generation of hate speech driven by fake narratives, termed Faux-Hate. Participants are challenged to identify such instances in code-mixed Hindi-English social media text. This paper describes our system developed for the shared task, addressing two primary sub-tasks: (a) Binary Faux-Hate detection, involving fake and hate speech classification, and (b) Target and Severity prediction, categorizing the intended target and severity of hateful content. Our approach combines advanced natural language processing techniques with domain-specific pretraining to enhance performance across both tasks. The system achieved competitive results, demonstrating the efficacy of leveraging multi-task learning for this complex problem.

</details>


### [216] [An Information-Theoretic Framework for Robust Large Language Model Editing](https://arxiv.org/abs/2512.16227)
*Qizhou Chen,Chengyu Wang,Taolin Zhang,Xiaofeng He*

Main category: cs.CL

TL;DR: 提出基于信息瓶颈理论的编辑大语言模型新框架IBKE，经多架构和基准任务验证有效，提升大语言模型实用性和可信度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在错误或过时信息，现有编辑技术难以泛化修正，需开发高效更新知识策略。

Method: 引入基于信息瓶颈理论的编辑框架，构建IBKE利用紧凑潜在表征引导基于梯度的更新。

Result: 在多个大语言模型架构和标准基准任务中验证了IBKE的有效性，展现了最先进的准确性和更好的泛化性与特异性。

Conclusion: 为开放域知识编辑建立了理论上有原则且实用的范式，提升了大语言模型在实际应用中的实用性和可信度。

Abstract: Large Language Models (LLMs) have become indispensable tools in science, technology, and society, enabling transformative advances across diverse fields. However, errors or outdated information within these models can undermine their accuracy and restrict their safe deployment. Developing efficient strategies for updating model knowledge without the expense and disruption of full retraining remains a critical challenge. Current model editing techniques frequently struggle to generalize corrections beyond narrow domains, leading to unintended consequences and limiting their practical impact. Here, we introduce a novel framework for editing LLMs, grounded in information bottleneck theory. This approach precisely compresses and isolates the essential information required for generalizable knowledge correction while minimizing disruption to unrelated model behaviors. Building upon this foundation, we present the Information Bottleneck Knowledge Editor (IBKE), which leverages compact latent representations to guide gradient-based updates, enabling robust and broadly applicable model editing. We validate IBKE's effectiveness across multiple LLM architectures and standard benchmark tasks, demonstrating state-of-the-art accuracy and improved generality and specificity of edits. These findings establish a theoretically principled and practical paradigm for open-domain knowledge editing, advancing the utility and trustworthiness of LLMs in real-world applications.

</details>


### [217] [Sigma-Moe-Tiny Technical Report](https://arxiv.org/abs/2512.16248)
*Qingguo Hu,Zhenghao Lin,Ziyue Yang,Yucheng Ding,Xiao Liu,Yuting Jiang,Ruizhe Wang,Tianyu Chen,Zhongxin Guo,Yifan Xiong,Rui Gao,Lei Qu,Jinsong Su,Peng Cheng,Yeyun Gong*

Main category: cs.CL

TL;DR: 提出Sigma - MoE - Tiny语言模型，实现高稀疏性，用渐进稀疏调度解决负载平衡问题，训练稳定且性能优异，还探讨了高稀疏模型负载平衡。


<details>
  <summary>Details</summary>
Motivation: 现有开源模型在稀疏性上有提升空间，极端稀疏带来专家负载平衡挑战。

Method: 采用细粒度专家分割，每层最多96个专家；提出渐进稀疏调度平衡专家利用率和训练稳定性；在多样化高质量语料上预训练后进行后训练。

Result: 训练过程稳定，无不可恢复的损失峰值；仅激活0.5B参数就达到同规模或更大规模模型中的顶级性能。

Conclusion: Sigma - MoE - Tiny模型有效，为未来MoE架构提高稀疏性提供见解。

Abstract: Mixture-of-Experts (MoE) has emerged as a promising paradigm for foundation models due to its efficient and powerful scalability. In this work, we present Sigma-MoE-Tiny, an MoE language model that achieves the highest sparsity compared to existing open-source models. Sigma-MoE-Tiny employs fine-grained expert segmentation with up to 96 experts per layer, while activating only one expert for each token, resulting in 20B total parameters with just 0.5B activated. The major challenge introduced by such extreme sparsity lies in expert load balancing. We find that the widely-used load balancing loss tends to become ineffective in the lower layers under this setting. To address this issue, we propose a progressive sparsification schedule aiming to balance expert utilization and training stability. Sigma-MoE-Tiny is pre-trained on a diverse and high-quality corpus, followed by post-training to further unlock its capabilities. The entire training process remains remarkably stable, with no occurrence of irrecoverable loss spikes. Comprehensive evaluations reveal that, despite activating only 0.5B parameters, Sigma-MoE-Tiny achieves top-tier performance among counterparts of comparable or significantly larger scale. In addition, we provide an in-depth discussion of load balancing in highly sparse MoE models, offering insights for advancing sparsity in future MoE architectures.
  Project page: https://qghuxmu.github.io/Sigma-MoE-Tiny
  Code: https://github.com/microsoft/ltp-megatron-lm

</details>


### [218] [Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs](https://arxiv.org/abs/2512.16378)
*Sara Papi,Javier Garcia Gilabert,Zachary Hopton,Vilém Zouhar,Carlos Escolano,Gerard I. Gállego,Jorge Iranzo-Sánchez,Ahrii Kim,Dominik Macháček,Patricia Schmidtova,Maike Züfle*

Main category: cs.CL

TL;DR: 文章提出Hearing to Translate测试套件，对比5个先进SpeechLLMs和16个直接与级联系统，发现级联系统总体更可靠，集成LLM对高质量语音翻译至关重要。


<details>
  <summary>Details</summary>
Motivation: 探究SpeechLLMs集成语音模态后，在语音翻译质量上是否优于传统级联架构。

Method: 提出Hearing to Translate测试套件，在16个基准测试、13种语言对和9种挑战性条件下，对比5个先进SpeechLLMs和16个直接与级联系统。

Result: 级联系统总体最可靠，当前SpeechLLMs仅在特定设置下与级联系统相当，语音基础模型落后于两者。

Conclusion: 在模型内或管道中集成LLM对高质量语音翻译至关重要。

Abstract: As Large Language Models (LLMs) expand beyond text, integrating speech as a native modality has given rise to SpeechLLMs, which aim to translate spoken language directly, thereby bypassing traditional transcription-based pipelines. Whether this integration improves speech-to-text translation quality over established cascaded architectures, however, remains an open question. We present Hearing to Translate, the first comprehensive test suite rigorously benchmarking 5 state-of-the-art SpeechLLMs against 16 strong direct and cascade systems that couple leading speech foundation models (SFM), with multilingual LLMs. Our analysis spans 16 benchmarks, 13 language pairs, and 9 challenging conditions, including disfluent, noisy, and long-form speech. Across this extensive evaluation, we find that cascaded systems remain the most reliable overall, while current SpeechLLMs only match cascades in selected settings and SFMs lag behind both, highlighting that integrating an LLM, either within the model or in a pipeline, is essential for high-quality speech translation.

</details>


### [219] [In-Context Algebra](https://arxiv.org/abs/2512.16902)
*Eric Todd,Jannik Brinkmann,Rohit Gandikota,David Bau*

Main category: cs.CL

TL;DR: 研究变压器模型在解决变量含义不固定的算术序列任务时的机制，发现三种学习机制。


<details>
  <summary>Details</summary>
Motivation: 以往研究中算术值标记含义固定，本文研究变量含义由交互决定时变压器的机制。

Method: 设计新任务，符号到代数元素的分配序列可变，开发针对性数据分布进行因果测试。

Result: 变压器在任务中达到近乎完美的准确率，能泛化到未见代数群，模型一致学习到三种机制。

Conclusion: 模型在处理含义不固定变量时发展出符号推理机制，补充了固定符号设置下的几何表示。

Abstract: We investigate the mechanisms that arise when transformers are trained to solve arithmetic on sequences where tokens are variables whose meaning is determined only through their interactions. While prior work has found that transformers develop geometric embeddings that mirror algebraic structure, those previous findings emerge from settings where arithmetic-valued tokens have fixed meanings. We devise a new task in which the assignment of symbols to specific algebraic group elements varies from one sequence to another. Despite this challenging setup, transformers achieve near-perfect accuracy on the task and even generalize to unseen algebraic groups. We develop targeted data distributions to create causal tests of a set of hypothesized mechanisms, and we isolate three mechanisms models consistently learn: commutative copying where a dedicated head copies answers, identity element recognition that distinguishes identity-containing facts, and closure-based cancellation that tracks group membership to constrain valid answers. Complementary to the geometric representations found in fixed-symbol settings, our findings show that models develop symbolic reasoning mechanisms when trained to reason in-context with variables whose meanings are not fixed.

</details>


### [220] [Plain language adaptations of biomedical text using LLMs: Comparision of evaluation metrics](https://arxiv.org/abs/2512.16530)
*Primoz Kocbek,Leon Kopitar,Gregor Stiglic*

Main category: cs.CL

TL;DR: 研究用大语言模型简化生物医学文本以提升健康素养，评估多种方法，发现gpt - 4o - mini表现优，FT方法欠佳，G - Eval效果好。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在简化生物医学文本、提升健康素养方面的应用。

Method: 使用公共数据集，开发并评估基线方法、双AI代理方法和微调方法，选OpenAI gpt - 4o和gpt - 4o mini模型作基线，用定量和定性指标评估。

Result: gpt - 4o - mini表现优，FT方法表现不佳，G - Eval作为定量指标与定性指标排名相似。

Conclusion: 大语言模型在简化生物医学文本中有应用价值，gpt - 4o - mini表现好，G - Eval评估有潜力。

Abstract: This study investigated the application of Large Language Models (LLMs) for simplifying biomedical texts to enhance health literacy. Using a public dataset, which included plain language adaptations of biomedical abstracts, we developed and evaluated several approaches, specifically a baseline approach using a prompt template, a two AI agent approach, and a fine-tuning approach. We selected OpenAI gpt-4o and gpt-4o mini models as baselines for further research. We evaluated our approaches with quantitative metrics, such as Flesch-Kincaid grade level, SMOG Index, SARI, and BERTScore, G-Eval, as well as with qualitative metric, more precisely 5-point Likert scales for simplicity, accuracy, completeness, brevity. Results showed a superior performance of gpt-4o-mini and an underperformance of FT approaches. G-Eval, a LLM based quantitative metric, showed promising results, ranking the approaches similarly as the qualitative metric.

</details>


### [221] [Refusal Steering: Fine-grained Control over LLM Refusal Behaviour for Sensitive Topics](https://arxiv.org/abs/2512.16602)
*Iker García-Ferrero,David Montero,Roman Orus*

Main category: cs.CL

TL;DR: 提出Refusal Steering方法，无需重新训练对大语言模型在政治敏感话题上的拒绝行为进行细粒度控制，方法在多模型有效，能去除拒绝行为并保留安全对齐。


<details>
  <summary>Details</summary>
Motivation: 对大语言模型在政治敏感话题上的拒绝行为进行细粒度控制，且无需重新训练。

Method: 用LLM作为判断器分配拒绝置信分数，提出脊正则化变体计算引导向量。

Result: 在Qwen3 - Next - 80B - A3B - Thinking上移除模型对政治敏感话题的拒绝行为，在JailbreakBench保持安全，在通用基准接近基线性能，方法在4B和80B模型通用，可诱导目标拒绝。

Conclusion: 激活引导可去除政治拒绝行为并保留有害内容安全对齐，为推理时可控、透明的审核提供实用途径。

Abstract: We introduce Refusal Steering, an inference-time method to exercise fine-grained control over Large Language Models refusal behaviour on politically sensitive topics without retraining. We replace fragile pattern-based refusal detection with an LLM-as-a-judge that assigns refusal confidence scores and we propose a ridge-regularized variant to compute steering vectors that better isolate the refusal--compliance direction. On Qwen3-Next-80B-A3B-Thinking, our method removes the refusal behaviour of the model around politically sensitive topics while maintaining safety on JailbreakBench and near-baseline performance on general benchmarks. The approach generalizes across 4B and 80B models and can also induce targeted refusals when desired. We analize the steering vectors and show that refusal signals concentrate in deeper layers of the transformer and are distributed across many dimensions. Together, these results demonstrate that activation steering can remove political refusal behaviour while retaining safety alignment for harmful content, offering a practical path to controllable, transparent moderation at inference time.

</details>


### [222] [GinSign: Grounding Natural Language Into System Signatures for Temporal Logic Translation](https://arxiv.org/abs/2512.16770)
*William English,Chase Walker,Dominic Simon,Rickard Ewetz*

Main category: cs.CL

TL;DR: 提出GinSign框架用于自然语言到时序逻辑翻译，将接地任务分层处理，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自然语言到时序逻辑翻译框架存在需准确原子接地或接地翻译准确率低的问题。

Method: 提出GinSign框架，引入接地模型，将接地任务分层，从自由生成问题转为结构化分类问题。

Result: 实验表明未接地框架生成的LTL语义不等价，GinSign框架支持下游模型检查，接地逻辑等价得分95.5%，比SOTA提升1.4倍。

Conclusion: GinSign框架在自然语言到时序逻辑翻译中表现良好，能解决现有框架的问题。

Abstract: Natural language (NL) to temporal logic (TL) translation enables engineers to specify, verify, and enforce system behaviors without manually crafting formal specifications-an essential capability for building trustworthy autonomous systems. While existing NL-to-TL translation frameworks have demonstrated encouraging initial results, these systems either explicitly assume access to accurate atom grounding or suffer from low grounded translation accuracy. In this paper, we propose a framework for Grounding Natural Language Into System Signatures for Temporal Logic translation called GinSign. The framework introduces a grounding model that learns the abstract task of mapping NL spans onto a given system signature: given a lifted NL specification and a system signature $\mathcal{S}$, the classifier must assign each lifted atomic proposition to an element of the set of signature-defined atoms $\mathcal{P}$. We decompose the grounding task hierarchically -- first predicting predicate labels, then selecting the appropriately typed constant arguments. Decomposing this task from a free-form generation problem into a structured classification problem permits the use of smaller masked language models and eliminates the reliance on expensive LLMs. Experiments across multiple domains show that frameworks which omit grounding tend to produce syntactically correct lifted LTL that is semantically nonequivalent to grounded target expressions, whereas our framework supports downstream model checking and achieves grounded logical-equivalence scores of $95.5\%$, a $1.4\times$ improvement over SOTA.

</details>


### [223] [Grammar-Forced Translation of Natural Language to Temporal Logic using LLMs](https://arxiv.org/abs/2512.16814)
*William English,Dominic Simon,Sumit Kumar Jha,Rickard Ewetz*

Main category: cs.CL

TL;DR: 提出用于自然语言到时态逻辑翻译的GraFT框架，限制有效输出令牌集降低任务复杂度，经基准测试，提升了翻译准确率。


<details>
  <summary>Details</summary>
Motivation: 现有自然语言到形式语言翻译方法在准确提升、共指处理和小数据学习方面存在困难。

Method: 提出GraFT框架，通过限制每一步有效输出令牌集来降低提升和翻译任务的复杂度，并给出理论解释。

Result: 在CW、GLTL和Navi基准测试中，GraFT端到端翻译准确率平均提升5.49%，域外翻译准确率平均提升14.06%。

Conclusion: GraFT框架能有效提升自然语言到时态逻辑的翻译准确率。

Abstract: Translating natural language (NL) into a formal language such as temporal logic (TL) is integral for human communication with robots and autonomous systems. State-of-the-art approaches decompose the task into a lifting of atomic propositions (APs) phase and a translation phase. However, existing methods struggle with accurate lifting, the existence of co-references, and learning from limited data. In this paper, we propose a framework for NL to TL translation called Grammar Forced Translation (GraFT). The framework is based on the observation that previous work solves both the lifting and translation steps by letting a language model iteratively predict tokens from its full vocabulary. In contrast, GraFT reduces the complexity of both tasks by restricting the set of valid output tokens from the full vocabulary to only a handful in each step. The solution space reduction is obtained by exploiting the unique properties of each problem. We also provide a theoretical justification for why the solution space reduction leads to more efficient learning. We evaluate the effectiveness of GraFT using the CW, GLTL, and Navi benchmarks. Compared with state-of-the-art translation approaches, it can be observed that GraFT the end-to-end translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% on average.

</details>


### [224] [LLMCache: Layer-Wise Caching Strategies for Accelerated Reuse in Transformer Inference](https://arxiv.org/abs/2512.16843)
*Harsh Vardhan Bansal*

Main category: cs.CL

TL;DR: 提出 LLMCache 层缓存框架加速 Transformer 推理，实验表明加速效果好且精度损失小。


<details>
  <summary>Details</summary>
Motivation: Transformer 语言模型推理延迟高，现有缓存机制有局限，需要更好的加速方案。

Method: 提出基于输入序列语义相似性重用中间激活的层级缓存框架 LLMCache，引入轻量级指纹机制和自适应驱逐策略。

Result: 在 BERT 和 GPT - 2 上测试，推理时间最多加速 3.1 倍，精度损失小于 0.5%。

Conclusion: LLMCache 是优化现实应用中 Transformer 推理的实用通用解决方案。

Abstract: Transformer-based language models have achieved remarkable performance across a wide range of tasks, yet their high inference latency poses a significant challenge for real-timeand large-scale deployment. While existing caching mechanisms,such as token-level key-value caches, offer speedups in autore-gressive decoding, they are limited in scope and applicability. In this paper, we present LLMCache, a novel layer-wise caching framework that accelerates transformer inference by reusing intermediate activations based on semantic similarity of input sequences. Unlike prior work, LLMCache is model-agnostic,operates across both encoder and decoder architectures, and supports caching at arbitrary transformer layers. We introduce a lightweight fingerprinting mechanism for matching seman-tically similar inputs and propose adaptive eviction strategies to manage cache staleness. Experiments on BERT and GPT-2 across SQuAD, WikiText-103, and OpenBookQA show up to 3.1 X speedup in inference time with <0.5% accuracy degradation. Our results highlight LLMCache as a practical and general-purpose solution for optimizing transformer inference in real-world applications

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [225] [xtdml: Double Machine Learning Estimation to Static Panel Data Models with Fixed Effects in R](https://arxiv.org/abs/2512.15965)
*Annalivia Polselli*

Main category: econ.EM

TL;DR: 本文介绍R包`xtdml`，它实现了Clarke和Polselli (2025)提出的部分线性面板回归模型的双重机器学习方法，并展示了其在模拟和真实纵向数据中的应用。


<details>
  <summary>Details</summary>
Motivation: 将机器学习的预测能力与统计估计相结合，对感兴趣的结构参数进行推断，实现部分线性面板回归模型的双重机器学习方法。

Method: 开发R包`xtdml`，利用`mlr3`生态系统的机器学习算法学习干扰函数，提供多种处理未观测个体异质性的方法，对协变量进行变换以提高学习性能。

Result: 成功开发`xtdml`包，并在模拟和真实纵向数据中展示了其使用。

Conclusion: `xtdml`包为部分线性面板回归模型的双重机器学习方法提供了有效的实现工具。

Abstract: The double machine learning (DML) method combines the predictive power of machine learning with statistical estimation to conduct inference about the structural parameter of interest. This paper presents the R package `xtdml`, which implements DML methods for partially linear panel regression models with low-dimensional fixed effects, high-dimensional confounding variables, proposed by Clarke and Polselli (2025). The package provides functionalities to: (a) learn nuisance functions with machine learning algorithms from the `mlr3` ecosystem, (b) handle unobserved individual heterogeneity choosing among first-difference transformation, within-group transformation, and correlated random effects, (c) transform the covariates with min-max normalization and polynomial expansion to improve learning performance. We showcase the use of `xtdml` with both simulated and real longitudinal data.

</details>


<div id='q-fin.GN'></div>

# q-fin.GN [[Back]](#toc)

### [226] [FedSight AI: Multi-Agent System Architecture for Federal Funds Target Rate Prediction](https://arxiv.org/abs/2512.15728)
*Yuhan Hou,Tianji Rao,Jeremy Tan,Adler Viton,Xiyue Zhang,David Ye,Abhishek Kodi,Sanjana Dulam,Aditya Paul,Yikai Feng*

Main category: q-fin.GN

TL;DR: 提出FedSight AI框架模拟FOMC审议和预测政策结果，CoD扩展提升性能，评估表现优且推理透明。


<details>
  <summary>Details</summary>
Motivation: 模拟FOMC审议过程并准确预测货币政策结果，以辅助了解货币政策制定。

Method: 引入多智能体框架FedSight AI，使用大语言模型，成员智能体分析指标和非结构化输入、辩论并投票，还有CoD扩展进行多阶段推理。

Result: 在2023 - 2024会议评估中，FedSight CoD准确率达93.75%，稳定性达93.33%，优于MiniFed和Ordinal Random Forest等基线模型。

Conclusion: FedSight AI框架有效且推理透明，能更好地模拟FOMC审议和预测政策结果。

Abstract: The Federal Open Market Committee (FOMC) sets the federal funds rate, shaping monetary policy and the broader economy. We introduce \emph{FedSight AI}, a multi-agent framework that uses large language models (LLMs) to simulate FOMC deliberations and predict policy outcomes. Member agents analyze structured indicators and unstructured inputs such as the Beige Book, debate options, and vote, replicating committee reasoning. A Chain-of-Draft (CoD) extension further improves efficiency and accuracy by enforcing concise multistage reasoning. Evaluated at 2023-2024 meetings, FedSight CoD achieved accuracy of 93.75\% and stability of 93.33\%, outperforming baselines including MiniFed and Ordinal Random Forest (RF), while offering transparent reasoning aligned with real FOMC communications.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [227] [Global universal approximation with Brownian signatures](https://arxiv.org/abs/2512.16396)
*Mihriban Ceylan,David J. Prömel*

Main category: math.PR

TL;DR: 建立了合适粗糙路径空间上一般和非预期泛函的L^p型通用逼近定理，适用于布朗运动，可近似随机过程。


<details>
  <summary>Details</summary>
Motivation: 研究合适粗糙路径空间上泛函的逼近问题。

Method: 推导加权粗糙路径空间的全局通用逼近定理。

Result: 线性泛函在时间扩展粗糙路径的符号上关于L^p距离是稠密的，定理适用于布朗运动，可近似p可积随机过程。

Conclusion: 线性泛函在时间扩展布朗运动的符号上能近似适应布朗滤波的p可积随机过程，包括随机微分方程的解。

Abstract: We establish $L^p$-type universal approximation theorems for general and non-anticipative functionals on suitable rough path spaces, showing that linear functionals acting on signatures of time-extended rough paths are dense with respect to an $L^p$-distance. To that end, we derive global universal approximation theorems for weighted rough path spaces. We demonstrate that these $L^p$-type universal approximation theorems apply in particular to Brownian motion. As a consequence, linear functionals on the signature of the time-extended Brownian motion can approximate any $p$-integrable stochastic process adapted to the Brownian filtration, including solutions to stochastic differential equations.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [228] [Pseudo-Cepstrum: Pitch Modification for Mel-Based Neural Vocoders](https://arxiv.org/abs/2512.16519)
*Nikolaos Ellinas,Alexandra Vioni,Panos Kakoulidis,Georgios Vamvoukakis,Myrsini Christidou,Konstantinos Markopoulos,Junkwang Oh,Gunu Jho,Inchul Hwang,Aimilios Chalamandaris,Pirros Tsiakoulis*

Main category: cs.SD

TL;DR: 本文介绍一种基于倒谱的音高修改方法，适用于任意梅尔频谱图，与基于梅尔的声码器兼容，经实验验证。


<details>
  <summary>Details</summary>
Motivation: 提出一种可应用于任意梅尔频谱图表示，且与基于梅尔的声码器兼容，无需额外训练或模型更改的音高修改方法。

Method: 直接修改倒谱特征空间以移动谐波结构到目标位置，通过伪逆梅尔变换计算频谱图幅度，应用DCT转换为倒谱，在该域移动倒谱峰值，再应用IDCT和梅尔滤波器组重新计算修改后的梅尔频谱图。

Result: 使用客观和主观指标在各种最先进的神经声码器上进行了实验验证，并与传统音高修改方法进行了比较。

Conclusion: 所提出的基于倒谱的音高修改方法可行有效。

Abstract: This paper introduces a cepstrum-based pitch modification method that can be applied to any mel-spectrogram representation. As a result, this method is compatible with any mel-based vocoder without requiring any additional training or changes to the model. This is achieved by directly modifying the cepstrum feature space in order to shift the harmonic structure to the desired target. The spectrogram magnitude is computed via the pseudo-inverse mel transform, then converted to the cepstrum by applying DCT. In this domain, the cepstral peak is shifted without having to estimate its position and the modified mel is recomputed by applying IDCT and mel-filterbank. These pitch-shifted mel-spectrogram features can be converted to speech with any compatible vocoder. The proposed method is validated experimentally with objective and subjective metrics on various state-of-the-art neural vocoders as well as in comparison with traditional pitch modification methods.

</details>


### [229] [Domain-Agnostic Causal-Aware Audio Transformer for Infant Cry Classification](https://arxiv.org/abs/2512.16271)
*Geofrey Owino,Bernard Shibwabo Kasamani,Ahmed M. Abdelmoniem,Edem Wornyo*

Main category: cs.SD

TL;DR: 提出DACH - TIC模型用于婴儿哭声分类，在数据集上表现优于基线模型，适合现实监测系统。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习方法依赖相关性驱动的声学表示，易受噪声、虚假线索和记录环境域转移影响，需要准确且可解释的婴儿哭声副语言分类方法用于早期检测和临床决策支持。

Method: 提出DACH - TIC模型，集成因果注意力、分层表示学习、多任务监督和对抗性领域泛化；采用结构化变压器主干，结合因果注意力掩码和受控扰动训练；使用领域对抗目标和多任务学习；在数据集上用ESC - 50环境噪声叠加进行领域增强。

Result: DACH - TIC在准确率上提高2.6%，宏F1分数提高2.2分，因果保真度增强；能有效泛化到未见声学环境，域性能差距仅2.4%。

Conclusion: DACH - TIC模型适合现实世界的新生儿声学监测系统。

Abstract: Accurate and interpretable classification of infant cry paralinguistics is essential for early detection of neonatal distress and clinical decision support. However, many existing deep learning methods rely on correlation-driven acoustic representations, which makes them vulnerable to noise, spurious cues, and domain shifts across recording environments. We propose DACH-TIC, a Domain-Agnostic Causal-Aware Hierarchical Audio Transformer for robust infant cry classification. The model integrates causal attention, hierarchical representation learning, multi-task supervision, and adversarial domain generalization within a unified framework.
  DACH-TIC employs a structured transformer backbone with local token-level and global semantic encoders, augmented by causal attention masking and controlled perturbation training to approximate counterfactual acoustic variations. A domain-adversarial objective promotes environment-invariant representations, while multi-task learning jointly optimizes cry type recognition, distress intensity estimation, and causal relevance prediction. The model is evaluated on the Baby Chillanto and Donate-a-Cry datasets, with ESC-50 environmental noise overlays for domain augmentation.
  Experimental results show that DACH-TIC outperforms state-of-the-art baselines, including HTS-AT and SE-ResNet Transformer, achieving improvements of 2.6 percent in accuracy and 2.2 points in macro-F1 score, alongside enhanced causal fidelity. The model generalizes effectively to unseen acoustic environments, with a domain performance gap of only 2.4 percent, demonstrating its suitability for real-world neonatal acoustic monitoring systems.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [230] [Enhanced Web User Interface Design Via Cross-Device Responsiveness Assessment Using An Improved HCI-INTEGRATED DL Schemes](https://arxiv.org/abs/2512.15775)
*Shrinivass Arunachalam Balasubramanian*

Main category: cs.HC

TL;DR: 文章提出用FECSM和QNDSOA通过CR评估优化动态网页UI，能达到98.5632%平均适应度，部署后进行反馈监测。


<details>
  <summary>Details</summary>
Motivation: 现有UI优化模型忽略CR评估，影响用户交互效率，需解决此问题来提升用户满意度。

Method: 先收集和预处理设计与用户交互信息，提取HCI特征并分组用户行为模式；用FECSM进行CR评估；用BiGLMRU分类UX变化类型并依据UICPI标记；用QNDSOA优化UI设计，部署后反馈监测。

Result: 利用QNDSOA优化UI设计达到平均适应度98.5632%。

Conclusion: 通过文中提出的方法能有效地对动态网页UI进行优化。

Abstract: User Interface (UI) optimization is essential in the digital era to enhance user satisfaction in web environments. Nevertheless, the existing UI optimization models had overlooked the Cross-Responsiveness (CR) assessment, affecting the user interaction efficiency. Consequently, this article proposes a dynamic web UI optimization through CR assessment using Finite Exponential Continuous State Machine (FECSM) and Quokka Nonlinear Difference Swarm Optimization Algorithm (QNDSOA). Initially, the design and user interaction related information is collected as well as pre-processed for min-max normalization. Next, the Human-Computer Interaction (HCI)-based features are extracted, followed by user behaviour pattern grouping. Meanwhile, the CR assessment is done using FECSM. Then, the proposed Bidirectional Gated Luong and Mish Recurrent Unit (BiGLMRU) is used to classify the User eXperience (UX) change type, which is labelled based on the User Interface Change Prediction Index (UICPI). Lastly, a novel QNDSOA is utilized to optimize the UI design with an average fitness of 98.5632%. Feedback monitoring is done after optimal deployment.

</details>


### [231] [A Multi-Agent Large Language Model Framework for Automated Qualitative Analysis](https://arxiv.org/abs/2512.16063)
*Qidi Xu,Nuzha Amjad,Grace Giles,Alexa Cumming,De'angelo Hermesky,Alexander Wen,Min Ji Kwak,Yejin Kim*

Main category: cs.HC

TL;DR: 研究开发多智能体大语言模型框架CoTI自动进行定性主题分析，应用于心力衰竭患者访谈分析，结果优于初级研究者和基线NLP模型，还实现了人机交互应用，但与初级研究者协作效果有限。


<details>
  <summary>Details</summary>
Motivation: 传统定性主题分析劳动密集、主观且难扩展，为推进以患者为中心的护理，需更好方法探索患者体验。

Method: 开发包含三个智能体的多智能体大语言模型框架CoTI，应用于12个心力衰竭患者访谈分析。

Result: CoTI识别的关键短语、主题和代码本比初级研究者和基线NLP模型更接近高级研究者，实现了面向用户的人机交互应用。

Conclusion: CoTI在定性主题分析中有优势，但与初级研究者协作时，他们可能过度依赖CoTI，限制独立批判性思维。

Abstract: Understanding patients experiences is essential for advancing patient centered care, especially in chronic diseases that require ongoing communication. However, qualitative thematic analysis, the primary approach for exploring these experiences, remains labor intensive, subjective, and difficult to scale. In this study, we developed a multi agent large language model framework that automates qualitative thematic analysis through three agents (Instructor, Thematizer, CodebookGenerator), named Collaborative Theme Identification Agent (CoTI). We applied CoTI to 12 heart failure patient interviews to analyze their perceptions of medication intensity. CoTI identified key phrases, themes, and codebook that were more similar to those of the senior investigator than both junior investigators and baseline NLP models. We also implemented CoTI into a user-facing application to enable AI human interaction in qualitative analysis. However, collaboration between CoTI and junior investigators provided only marginal gains, suggesting they may overrely on CoTI and limit their independent critical thinking.

</details>


### [232] [Evaluation of Generative Models for Emotional 3D Animation Generation in VR](https://arxiv.org/abs/2512.16081)
*Kiran Chhatre,Renan Guarese,Andrii Matviienko,Christopher Peters*

Main category: cs.HC

TL;DR: 论文在VR环境中评估情感3D动画生成模型，开展用户研究对比不同模型。结果显示显式建模情感的方法识别准确率更高，当前模型处理微妙情绪有限，用户对动画享受和交互质量评分低，但认可多样性。


<details>
  <summary>Details</summary>
Motivation: 现有2D环境下统计指标评估无法充分捕捉用户感知的情感，限制对模型有效性的理解，需在VR环境中进行以用户为中心的评估。

Method: 在VR环境开展用户研究（N=48），评估三种先进的语音驱动3D动画方法在两种情绪下的表现，并与基于重建方法的真实人类表情对比。

Result: 显式建模情感的方法识别准确率更高；用户对快乐动画的真实感和自然度评分明显高于中性动画；生成模型在面部表情质量上不如重建方法；所有方法在动画享受和交互质量上评分较低；参与者认可动画多样性。

Conclusion: 应将以用户为中心的评估纳入生成模型开发，当前生成模型在处理微妙情绪状态上存在局限。

Abstract: Social interactions incorporate nonverbal signals to convey emotions alongside speech, including facial expressions and body gestures. Generative models have demonstrated promising results in creating full-body nonverbal animations synchronized with speech; however, evaluations using statistical metrics in 2D settings fail to fully capture user-perceived emotions, limiting our understanding of model effectiveness. To address this, we evaluate emotional 3D animation generative models within a Virtual Reality (VR) environment, emphasizing user-centric metrics emotional arousal realism, naturalness, enjoyment, diversity, and interaction quality in a real-time human-agent interaction scenario. Through a user study (N=48), we examine perceived emotional quality for three state of the art speech-driven 3D animation methods across two emotions happiness (high arousal) and neutral (mid arousal). Additionally, we compare these generative models against real human expressions obtained via a reconstruction-based method to assess both their strengths and limitations and how closely they replicate real human facial and body expressions. Our results demonstrate that methods explicitly modeling emotions lead to higher recognition accuracy compared to those focusing solely on speech-driven synchrony. Users rated the realism and naturalness of happy animations significantly higher than those of neutral animations, highlighting the limitations of current generative models in handling subtle emotional states. Generative models underperformed compared to reconstruction-based methods in facial expression quality, and all methods received relatively low ratings for animation enjoyment and interaction quality, emphasizing the importance of incorporating user-centric evaluations into generative model development. Finally, participants positively recognized animation diversity across all generative models.

</details>


### [233] [Plausibility as Failure: How LLMs and Humans Co-Construct Epistemic Error](https://arxiv.org/abs/2512.16750)
*Claudia Vale Oliveira,Nelson Zagalo,Filipe Silva,Anabela Brandao,Syeda Faryal Hussain Khurrum,Joaquim Santos*

Main category: cs.HC

TL;DR: 研究分析大语言模型与人类交互中认知失败问题，发现错误是共同构建结果，需重新评估。


<details>
  <summary>Details</summary>
Motivation: 大语言模型作为认知伙伴，其错误多通过预测指标分析，该研究关注其对人类判断的解释性影响。

Method: 进行三轮多LLM评估，使用跨学科任务和逐步细化评估框架，观察评估者多维度解读模型响应。

Result: LLM错误从预测型转为诠释型，评估者常混淆标准，观察到验证负担和认知漂移。

Conclusion: 错误是生成合理性和人类解读捷径共同结果，需将评估重塑为关系性诠释过程。

Abstract: Large language models (LLMs) are increasingly used as epistemic partners in everyday reasoning, yet their errors remain predominantly analyzed through predictive metrics rather than through their interpretive effects on human judgment. This study examines how different forms of epistemic failure emerge, are masked, and are tolerated in human AI interaction, where failure is understood as a relational breakdown shaped by model-generated plausibility and human interpretive judgment. We conducted a three round, multi LLM evaluation using interdisciplinary tasks and progressively differentiated assessment frameworks to observe how evaluators interpret model responses across linguistic, epistemic, and credibility dimensions. Our findings show that LLM errors shift from predictive to hermeneutic forms, where linguistic fluency, structural coherence, and superficially plausible citations conceal deeper distortions of meaning. Evaluators frequently conflated criteria such as correctness, relevance, bias, groundedness, and consistency, indicating that human judgment collapses analytical distinctions into intuitive heuristics shaped by form and fluency. Across rounds, we observed a systematic verification burden and cognitive drift. As tasks became denser, evaluators increasingly relied on surface cues, allowing erroneous yet well formed answers to pass as credible. These results suggest that error is not solely a property of model behavior but a co-constructed outcome of generative plausibility and human interpretive shortcuts. Understanding AI epistemic failure therefore requires reframing evaluation as a relational interpretive process, where the boundary between system failure and human miscalibration becomes porous. The study provides implications for LLM assessment, digital literacy, and the design of trustworthy human AI communication.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [234] [Random matrix theory of sparse neuronal networks with heterogeneous timescales](https://arxiv.org/abs/2512.12767)
*Thiparat Chotibut,Oleg Evnin,Weerawit Horinouchi*

Main category: q-bio.NC

TL;DR: 文章研究训练的循环神经网络，分析雅可比矩阵的特性，给出谱边的解析描述。


<details>
  <summary>Details</summary>
Motivation: 探索训练的网络特征及其在塑造理想动态景观中的作用的联系。

Method: 研究平衡点附近动态的雅可比矩阵，指定随机矩阵系综，用统计场论方法建立解析理论。

Result: 得到雅可比矩阵谱边的解析描述，将其统计参数与工作记忆计算关键的平衡点特征相关联。

Conclusion: 通过对雅可比矩阵的分析，建立参数与平衡特征的关系，有助于理解工作记忆计算的网络机制。

Abstract: Training recurrent neuronal networks consisting of excitatory (E) and inhibitory (I) units with additive noise for working memory computation slows and diversifies inhibitory timescales, leading to improved task performance that is attributed to emergent marginally stable equilibria [PNAS 122 (2025) e2316745122]. Yet the link between trained network characteristics and their roles in shaping desirable dynamical landscapes remains unexplored. Here, we investigate the Jacobian matrices describing the dynamics near these equilibria and show that they are sparse, non-Hermitian rectangular-block matrices modified by heterogeneous synaptic decay timescales and activation-function gains. We specify a random matrix ensemble that faithfully captures the spectra of trained Jacobian matrices, arising from the inhibitory core - excitatory periphery network motif (pruned E weights, broadly distributed I weights) observed post-training. An analytic theory of this ensemble is developed using statistical field theory methods: a Hermitized resolvent representation of the spectral density processed with a supersymmetry-based treatment in the style of Fyodorov and Mirlin. In this manner, an analytic description of the spectral edge is obtained, relating statistical parameters of the Jacobians (sparsity, weight variances, E/I ratio, and the distributions of timescales and gains) to near-critical features of the equilibria essential for robust working memory computation.

</details>


### [235] [Dynamical Mechanisms for Coordinating Long-term Working Memory Based on the Precision of Spike-timing in Cortical Neurons](https://arxiv.org/abs/2512.15891)
*Terrence J. Sejnowski*

Main category: q-bio.NC

TL;DR: 文章探讨皮层神经元的时间编码，研究行波与STDP结合对长期工作记忆的影响。


<details>
  <summary>Details</summary>
Motivation: 以往运动感觉研究多依赖平均放电率，对长时间尺度的工作记忆了解少，探究体内调节STDP的时间机制。

Method: 分析皮层神经元的精确放电时间、皮层行波特点以及行波与STDP的关联。

Result: 行波通过同步刺激突触，引发抑制性反弹产生反向动作电位激活STDP，可形成持续数小时的二级网络。

Conclusion: 行波和STDP结合可能负责我们的思考，有助于理解皮层功能和长期工作记忆。

Abstract: In the last century, most sensorimotor studies of cortical neurons relied on average firing rates. Rate coding is efficient for fast sensorimotor processing that occurs within a few seconds. Much less is known about long-term working memory with a time scale of hours (Ericsson and Kintsch, 1995). The discovery of the millisecond precision of spike initiation in cortical neurons was unexpected (Mainen and Sejnowski, 1995). Even more striking was the precision of spiking in vivo, in response to rapidly fluctuating sensory inputs, suggesting that neural circuits could, in principle, preserve and manipulate sensory information through spike timing. It could support spike-timing-dependent plasticity (STDP), which is triggered by the relative timing of spikes between presynaptic and postsynaptic neurons in the millisecond range. What spike-timing mechanisms could regulate STDP in vivo? Cortical traveling waves have been observed across many frequency bands with high temporal precision. Traveling waves have wave fronts that could link spike timing to STDP. As a wave front passes through a cortical column, excitatory synapses on the dendrites of both pyramidal and basket cells are synchronously stimulated. Inhibitory basket cells form a calyx on pyramidal cell bodies, and inhibitory rebound following a strong transient hyperpolarization can trigger a backpropagating action potential, which arrives shortly after the excitatory inputs on pyramidal dendrites. STDP activated in this way could persist for hours, creating a second-tier network. This temporary network could support long-term working memory, a cognitive network riding above the long-term sensorimotor network. On their own, traveling waves and STDP have not yet yielded new insights into cortical function. Together, they could be responsible for how we think (Sejnowski, 2025).

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [236] [Human-like Working Memory from Artificial Intrinsic Plasticity Neurons](https://arxiv.org/abs/2512.15829)
*Jingli Liu,Huannan Zheng,Bohao Zou,Kezhou Yang*

Main category: cs.ET

TL;DR: 提出IPNet神经形态架构实现类人工作记忆，在多任务中表现优，能耗低、面积小。


<details>
  <summary>Details</summary>
Motivation: 人工网络实现工作记忆能耗高且对噪声敏感，需新架构。

Method: 利用磁隧道结的焦耳热动力学，通过神经元固有可塑性实现类人工作记忆。

Result: 在手势数据集、自动驾驶任务表现优，能耗大幅降低，面积小。

Conclusion: 通过神经元固有可塑性实现类人工作记忆，使神经网络具备动态视觉处理和低代谢成本优势。

Abstract: Working memory enables the brain to integrate transient information for rapid decision-making. Artificial networks typically replicate this via recurrent or parallel architectures, yet incur high energy costs and noise sensitivity. Here we report IPNet, a hardware-software co-designed neuromorphic architecture realizing human-like working memory via neuronal intrinsic plasticity. Exploiting Joule-heating dynamics of Magnetic Tunnel Junctions (MTJs), IPNet physically emulates biological memory volatility. The memory behavior of the proposed architecture shows similar trends in n-back, free recall and memory interference tasks to that of reported human subjects. Implemented exclusively with MTJ neurons, the architecture with human-like working memory achieves 99.65% accuracy on 11-class DVS gesture datasets and maintains 99.48% on a novel 22-class time-reversed benchmark, outperforming RNN, LSTM, and 2+1D CNN baselines sharing identical backbones. For autonomous driving (DDD-20), IPNet reduces steering prediction error by 14.4% compared to ResNet-LSTM. Architecturally, we identify a 'Memory-at-the-Frontier' effect where performance is maximized at the sensing interface, validating a bio-plausible near-sensor processing paradigm. Crucially, all results rely on raw parameters from fabricated devices without optimization. Hardware-in-the-loop validation confirms the system's physical realizability. Separately, energy analysis reveals a reduction in memory power of 2,874x compared to LSTMs and 90,920x versus parallel 3D-CNNs. This capacitor-free design enables a compact ~1.5um2 footprint (28 nm CMOS): a >20-fold reduction over standard LIF neurons. Ultimately, we demonstrate that instantiating human-like working memory via intrinsic neuronal plasticity endows neural networks with the dual biological advantages of superior dynamic vision processing and minimal metabolic cost.

</details>


### [237] [Feasibility of Radio Frequency Based Wireless Sensing of Lead Contamination in Soil](https://arxiv.org/abs/2512.16071)
*Yixuan Gao,Tanvir Ahmed,Mikhail Mohammed,Zhongqi Cheng,Rajalakshmi Nandakumar*

Main category: cs.ET

TL;DR: 现有铅测量技术费力且成本高，本文提出基于射频的无线系统SoilScanner检测土壤铅，实验证明其能以72%准确率分类土壤铅含量，表明基于无线技术构建便携低成本铅检测设备可行。


<details>
  <summary>Details</summary>
Motivation: 城市土壤中铅污染影响食品安全和公共健康、阻碍城市绿化，而现有测量铅的技术费力且昂贵。

Method: 基于土壤中不同盐对不同频段无线电信号传播的影响不同，先在对照实验中向干净土壤添加盐，后用机器学习模型在非受控实地样本中验证发现。

Result: SoilScanner能以72%的准确率将土壤样本分为低铅和高铅类别（阈值200 ppm），铅含量> 500 ppm的样本无错分。

Conclusion: 基于无线技术构建便携式、经济实惠的铅检测和筛选设备是可行的。

Abstract: Widespread Pb (lead) contamination of urban soil significantly impacts food safety and public health and hinders city greening efforts. However, most existing technologies for measuring Pb are labor-intensive and costly. In this study, we propose SoilScanner, a radio frequency-based wireless system that can detect Pb in soils. This is based on our discovery that the propagation of different frequency band radio signals is affected differently by different salts such as NaCl and Pb(NO3)2 in the soil. In a controlled experiment, manually adding NaCl and Pb(NO3)2 in clean soil, we demonstrated that different salts reflected signals at different frequencies in distinct patterns. In addition, we confirmed the finding using uncontrolled field samples with a machine learning model. Our experiment results show that SoilScanner can classify soil samples into low-Pb and high-Pb categories (threshold at 200 ppm) with an accuracy of 72%, with no sample with > 500 ppm of Pb being misclassified. The results of this study show that it is feasible to build portable and affordable Pb detection and screening devices based on wireless technology.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [238] [Decision-Focused Bias Correction for Fluid Approximation](https://arxiv.org/abs/2512.15726)
*Can Er,Mo Liu*

Main category: math.OC

TL;DR: 本文研究如何找到替代均值的点统计量来解决两阶段随机优化问题，为含顾客遗弃成本的服务网络建立条件并提出算法，数值实验证明方法优于传统流体近似。


<details>
  <summary>Details</summary>
Motivation: 传统流体近似用均值替代随机分布会引入偏差和次优决策，需要寻找替代点统计量以获得最优决策。

Method: 为含顾客遗弃成本的服务网络建立存在决策校正点估计的充要条件，提出计算算法；在可分解网络结构下研究其与经典报童问题解的关系。

Result: 在含顾客遗弃成本的服务网络中找到决策校正点估计，且在可分解网络结构下与经典报童解密切相关；数值实验表明决策聚焦校正方法优于传统流体近似。

Conclusion: 提出的决策聚焦校正方法能解决传统流体近似的问题，优于传统方法，可用于两阶段随机优化问题以获最优决策。

Abstract: Fluid approximation is a widely used approach for solving two-stage stochastic optimization problems, with broad applications in service system design such as call centers and healthcare operations. However, replacing the underlying random distribution (e.g., demand distribution) with its mean (e.g., the time-varying average arrival rate) introduces bias in performance estimation and can lead to suboptimal decisions. In this paper, we investigate how to identify an alternative point statistic, which is not necessarily the mean, such that substituting this statistic into the two-stage optimization problem yields the optimal decision. We refer to this statistic as the decision-corrected point estimate (time-varying arrival rate). For a general service network with customer abandonment costs, we establish necessary and sufficient conditions for the existence of such a corrected point estimate and propose an algorithm for its computation. Under a decomposable network structure, we further show that the resulting decision-corrected point estimate is closely related to the classical newsvendor solution. Numerical experiments demonstrate the superiority of our decision-focused correction method compared to the traditional fluid approximation.

</details>


### [239] [Lower bounds for ranking-based pivot rules](https://arxiv.org/abs/2512.16684)
*Yann Disser,Georg Loho,Matthew Maat,Nils Mosis*

Main category: math.OC

TL;DR: 论文引入统一框架对规则类进行形式化，给出基于排名类规则的下界，为策略改进和策略迭代分别得到超多项式和次指数下界，结果可推广到线性规划单纯形法。


<details>
  <summary>Details</summary>
Motivation: 线性规划单纯形法、马尔可夫决策过程策略迭代和奇偶博弈策略改进的多项式枢轴规则是各自领域的突出开放问题，现有下界构造针对个别或小部分枢轴规则，需要统一框架。

Method: 引入统一框架对规则类根据其依赖的输入信息进行形式化，针对基于排名类规则给出下界。

Result: 通过一类汇奇偶博弈为策略改进得到超多项式下界，通过一类马尔可夫决策过程为策略迭代得到次指数下界，结果可推广到线性规划单纯形法。

Conclusion: 统一框架可用于分析不同领域的规则类，得到的下界对相关问题的研究有重要意义。

Abstract: The existence of a polynomial pivot rule for the simplex method for linear programming, policy iteration for Markov decision processes, and strategy improvement for parity games each are prominent open problems in their respective fields. While numerous natural candidates for efficient rules have been eliminated, all existing lower bound constructions are tailored to individual or small sets of pivot rules. We introduce a unified framework for formalizing classes of rules according to the information about the input that they rely on. Within this framework, we show lower bounds for \emph{ranking-based} classes of rules that base their decisions on orderings of the improving pivot steps induced by the underlying data. Our first result is a superpolynomial lower bound for strategy improvement, obtained via a family of sink parity games, which applies to memory-based generalizations of Bland's rule that only access the input by comparing the ranks of improving edges in some global order. Our second result is a subexponential lower bound for policy iteration, obtained via a family of Markov decision processes, which applies to memoryless rules that only access the input by comparing improving actions according to their ranks in a global order, their reduced costs, and the associated improvements in objective value. Both results carry over to the simplex method for linear programming.

</details>


### [240] [A Context-Free Smart Grid Model Using Complex System Approach](https://arxiv.org/abs/2512.15733)
*Soufian Ben Amor,Alain Bui,Guillaume Guerard*

Main category: math.OC

TL;DR: 本文提出基于复杂系统的智能电网建模方法，结合博弈论与经典方法实现优化。


<details>
  <summary>Details</summary>
Motivation: 能源与污染是21世纪紧迫问题，智能电网建模和仿真有价值，但全局优化不易。

Method: 提出基于复杂系统的智能电网建模方法，在不同层面结合博弈论和经典方法进行优化。

Result: 结合方法能灵活、可扩展地实现优化，且保持通用性。

Conclusion: 基于复杂系统的建模方法结合博弈论和经典方法，可有效实现智能电网优化。

Abstract: Energy and pollution are urging problems of the 21th century. By gradually changing the actual power grid system, smart grid may evolve into different systems by means of size, elements and strategies, but its fundamental requirements and objectives will not change such as optimizing production, transmission, and consumption. Studying the smart grid through modeling and simulation provides us with valuable results which cannot be obtained in real world due to time and cost related constraints. Moreover, due to the complexity of the smart grid, achieving global optimization is not an easy task. In this paper, we propose a complex system based approach to the smart grid modeling, accentuating on the optimization by combining game theoretical and classical methods in different levels. Thanks to this combination, the optimization can be achieved with flexibility and scalability, while keeping its generality.

</details>


### [241] [Deep Reinforcement Learning Optimization for Uncertain Nonlinear Systems via Event-Triggered Robust Adaptive Dynamic Programming](https://arxiv.org/abs/2512.15735)
*Ningwei Bai,Chi Pui Chan,Qichen Yin,Tengyang Gong,Yunda Yan,Zezhi Tang*

Main category: math.OC

TL;DR: 提出一种统一控制架构，结合RL控制器、ESO和ETM，减少计算量，经实验证明性能良好。


<details>
  <summary>Details</summary>
Motivation: 在无精确系统描述下获得近最优行为，减少不必要计算，提高系统控制性能和抗干扰能力。

Method: 将RL控制器与ESO耦合，采用基于值迭代的ADP方法进行策略近似，引入ETM限制计算，用Lyapunov分析系统稳定性。

Result: 数值实验表明该方法保持强控制性能和抗干扰能力，相比标准时间触发ADP方案显著减少采样和处理工作量。

Conclusion: 所提出的统一控制架构有效可行，能降低计算负荷，提高系统性能。

Abstract: This work proposes a unified control architecture that couples a Reinforcement Learning (RL)-driven controller with a disturbance-rejection Extended State Observer (ESO), complemented by an Event-Triggered Mechanism (ETM) to limit unnecessary computations. The ESO is utilized to estimate the system states and the lumped disturbance in real time, forming the foundation for effective disturbance compensation. To obtain near-optimal behavior without an accurate system description, a value-iteration-based Adaptive Dynamic Programming (ADP) method is adopted for policy approximation. The inclusion of the ETM ensures that parameter updates of the learning module are executed only when the state deviation surpasses a predefined bound, thereby preventing excessive learning activity and substantially reducing computational load. A Lyapunov-oriented analysis is used to characterize the stability properties of the resulting closed-loop system. Numerical experiments further confirm that the developed approach maintains strong control performance and disturbance tolerance, while achieving a significant reduction in sampling and processing effort compared with standard time-triggered ADP schemes.

</details>


### [242] [Non-Asymptotic Global Convergence of PPO-Clip](https://arxiv.org/abs/2512.16565)
*Yin Liu,Qiming Dai,Junyu Zhang,Zaiwen Wen*

Main category: math.OC

TL;DR: 本文在软最大化策略参数化下，对含f - 散度正则化的确定性仅演员近端策略优化（PPO）算法进行分析，推导相关条件和不等式，得出不同正则化器下的收敛率。


<details>
  <summary>Details</summary>
Motivation: 近端策略优化（PPO）的仅演员变体虽经验上成功，但对其问题和算法性质缺乏严格理论理解，需要推进PPO - Clip算法的理论基础。

Method: 在一般强化学习设置下，对含f - 散度正则化的确定性仅演员PPO算法进行分析，推导非均匀Lipschitz平滑条件和Łojasiewicz不等式。

Result: 对于前向KL正则化器，建立了到全局最优策略的非渐近线性收敛率；对于反向KL正则化器，得出了平稳收敛和局部线性收敛。

Conclusion: 通过理论分析，推进了PPO - Clip算法的理论基础，明确了不同正则化器下算法的收敛性质。

Abstract: Reinforcement learning (RL) has gained attention for aligning large language models (LLMs) via reinforcement learning from human feedback (RLHF). The actor-only variants of Proximal Policy Optimization (PPO) are widely applied for their efficiency. These algorithms incorporate a clipping mechanism to improve stability. Besides, a regularization term, such as the reverse KL-divergence or a more general \(f\)-divergence, is introduced to prevent policy drift. Despite their empirical success, a rigorous theoretical understanding of the problem and the algorithm's properties is limited. This paper advances the theoretical foundations of the PPO-Clip algorithm by analyzing a deterministic actor-only PPO algorithm within the general RL setting with \(f\)-divergence regularization under the softmax policy parameterization. We derive a non-uniform Lipschitz smoothness condition and a Łojasiewicz inequality for the considered problem. Based on these, a non-asymptotic linear convergence rate to the globally optimal policy is established for the forward KL-regularizer. Furthermore, stationary convergence and local linear convergence are derived for the reverse KL-regularizer.

</details>


### [243] [Muon is Provably Faster with Momentum Variance Reduction](https://arxiv.org/abs/2512.16598)
*Xun Qian,Hussein Rammal,Dmitry Kovalev,Peter Richtárik*

Main category: math.OC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent empirical research has demonstrated that deep learning optimizers based on the linear minimization oracle (LMO) over specifically chosen Non-Euclidean norm balls, such as Muon and Scion, outperform Adam-type methods in the training of large language models. In this work, we show that such optimizers can be provably improved by replacing their vanilla momentum by momentum variance reduction (MVR). Instead of proposing and analyzing MVR variants of Muon and Scion separately, we incorporate MVR into the recently proposed Gluon framework, which captures Muon, Scion and other specific Non-Euclidean LMO-based methods as special cases, and at the same time works with a more general smoothness assumption which better captures the layer-wise structure of neural networks. In the non-convex case, we incorporate MVR into Gluon in three different ways. All of them improve the convergence rate from ${\cal O} (\frac{1}{K^{1/4}})$ to ${\cal O} (\frac{1}{K^{1/3}})$. Additionally, we provide improved rates in the star-convex case. Finally, we conduct several numerical experiments that verify the superior performance of our proposed algorithms in terms of iteration complexity.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [244] [Lifting Biomolecular Data Acquisition](https://arxiv.org/abs/2512.15984)
*Eli N. Weinstein,Andrei Slabodkin,Mattia G. Gollub,Kerry Dobbs,Xiao-Bing Cui,Fang Zhang,Kristina Gurung,Elizabeth B. Wood*

Main category: q-bio.BM

TL;DR: 提出基于压缩感知神经扩展到函数空间的方法，同时测量分子活性，实验与算法协同设计提升信息密度，并在抗体和细胞疗法上验证。


<details>
  <summary>Details</summary>
Motivation: 提高机器学习驱动科学中湿实验室实验的信息密度。

Method: 基于压缩感知神经扩展到函数空间，同时测量多个不同分子的活性，并在模型训练时反卷积分子 - 活性图。

Result: 实验与学习算法的协同设计可使信息密度获得数量级的提升。

Conclusion: 该方法在抗体和细胞疗法上得到了有效验证。

Abstract: One strategy to scale up ML-driven science is to increase wet lab experiments' information density. We present a method based on a neural extension of compressed sensing to function space. We measure the activity of multiple different molecules simultaneously, rather than individually. Then, we deconvolute the molecule-activity map during model training. Co-design of wet lab experiments and learning algorithms provably leads to orders-of-magnitude gains in information density. We demonstrate on antibodies and cell therapies.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [245] [A Survey on Spatio-Temporal Knowledge Graph Models](https://arxiv.org/abs/2512.16487)
*Philipp Plamper,Hanna Köpcke,Anika Groß*

Main category: cs.SI

TL;DR: 本文对时空知识图模型进行系统综述，分析现存方法问题，给出建模指南并指出开放挑战。


<details>
  <summary>Details</summary>
Motivation: 时空知识图虽应用广泛，但建模面临基础分散、现有方法缺乏概念对齐等问题，需系统梳理。

Method: 追溯时空知识图在静态、时间和空间图建模中的起源，沿关键建模维度分析现有方法。

Result: 发现统一建模框架缺失，多数模型针对特定用例，缺乏可复用性和长期知识保存设计。

Conclusion: 基于分析结果得出建模指南，确定开放挑战以指导未来研究。

Abstract: Many complex real-world systems exhibit inherently intertwined temporal and spatial characteristics. Spatio-temporal knowledge graphs (STKGs) have therefore emerged as a powerful representation paradigm, as they integrate entities, relationships, time and space within a unified graph structure. They are increasingly applied across diverse domains, including environmental systems and urban, transportation, social and human mobility networks. However, modeling STKGs remains challenging: their foundations span classical graph theory as well as temporal and spatial graph models, which have evolved independently across different research communities and follow heterogeneous modeling assumptions and terminologies. As a result, existing approaches often lack conceptual alignment, generalizability and reusability. This survey provides a systematic review of spatio-temporal knowledge graph models, tracing their origins in static, temporal and spatial graph modeling. We analyze existing approaches along key modeling dimensions, including edge semantics, temporal and spatial annotation strategies, temporal and spatial semantics and relate these choices to their respective application domains. Our analysis reveals that unified modeling frameworks are largely absent and that most current models are tailored to specific use cases rather than designed for reuse or long-term knowledge preservation. Based on these findings, we derive modeling guidelines and identify open challenges to guide future research.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [246] [Artificial Intelligence-Enabled Holistic Design of Catalysts Tailored for Semiconducting Carbon Nanotube Growth](https://arxiv.org/abs/2512.16151)
*Liu Qian,Yue Li,Ying Xie,Jian Zhang,Pai Li,Yue Yu,Zhe Liu,Feng Ding,Jin Zhang*

Main category: cond-mat.mtrl-sci

TL;DR: 本文提出将机器学习融入传统催化剂设计的整体框架用于半导体碳纳米管合成，筛选催化剂并经实验验证，为催化剂设计和纳米材料合成提供通用方法。


<details>
  <summary>Details</summary>
Motivation: 碳纳米管合成存在复杂纳米级催化反应，现有催化剂设计策略在纳米级面临挑战，需要创新设计以获得高密度、高质量半导体碳纳米管。

Method: 提出将机器学习融入传统催化剂设计的整体框架，结合知识洞察与数据驱动技术，利用电子结构数据库、预训练嵌入模型和物理驱动预测模型。

Result: 筛选出54种候选催化剂，确定3种高潜力催化剂，高通量实验验证预测，半导体选择性超91%，FeTiO3催化剂达98.6%。

Conclusion: 该方法不仅解决半导体碳纳米管合成问题，还为全球催化剂设计和纳米材料合成提供通用方法，推动材料科学精确控制发展。

Abstract: Catalyst design is crucial for materials synthesis, especially for complex reaction networks. Strategies like collaborative catalytic systems and multifunctional catalysts are effective but face challenges at the nanoscale. Carbon nanotube synthesis contains complicated nanoscale catalytic reactions, thus achieving high-density, high-quality semiconducting CNTs demands innovative catalyst design. In this work, we present a holistic framework integrating machine learning into traditional catalyst design for semiconducting CNT synthesis. It combines knowledge-based insights with data-driven techniques. Three key components, including open-access electronic structure databases for precise physicochemical descriptors, pre-trained natural language processing-based embedding model for higher-level abstractions, and physical - driven predictive models based on experiment data, are utilized. Through this framework, a new method for selective semiconducting CNT synthesis via catalyst - mediated electron injection, tuned by light during growth, is proposed. 54 candidate catalysts are screened, and three with high potential are identified. High-throughput experiments validate the predictions, with semiconducting selectivity exceeding 91% and the FeTiO3 catalyst reaching 98.6%. This approach not only addresses semiconducting CNT synthesis but also offers a generalizable methodology for global catalyst design and nanomaterials synthesis, advancing materials science in precise control.

</details>


### [247] [Predictive Inorganic Synthesis based on Machine Learning using Small Data sets: a case study of size-controlled Cu Nanoparticles](https://arxiv.org/abs/2512.16545)
*Brent Motmans,Digvijay Ghogare,Thijs G. I. van Wijk,An Hardy,Danny E. P. Vanpoucke*

Main category: cond-mat.mtrl-sci

TL;DR: 研究用少量数据借助机器学习预测微波辅助多元醇合成铜纳米颗粒的尺寸，拉丁超立方抽样收集数据，集成回归模型预测效果佳。


<details>
  <summary>Details</summary>
Motivation: 铜纳米颗粒合成对反应参数敏感，实验优化耗时长、资源多，且机器学习在材料研究中因缺乏大量高质量实验数据集而应用受限。

Method: 用拉丁超立方抽样创建实验数据集来覆盖参数空间，借助AMADEUS框架构建集成回归模型。

Result: 集成回归模型能高精度预测颗粒尺寸（$R^2 = 0.74$），优于经典统计方法（$R^2 = 0.60$）。

Conclusion: 对于实验室规模合成优化，高质量小数据集结合经典可解释机器学习模型优于传统统计方法，可用于定量合成预测，为数据驱动的无机合成设计提供途径。

Abstract: Copper nanoparticles (Cu NPs) have a broad applicability, yet their synthesis is sensitive to subtle changes in reaction parameters. This sensitivity, combined with the time- and resource-intensive nature of experimental optimization, poses a major challenge in achieving reproducible and size-controlled synthesis. While Machine Learning (ML) shows promise in materials research, its application is often limited by scarcity of large high-quality experimental data sets. This study explores ML to predict the size of Cu NPs from microwave-assisted polyol synthesis using a small data set of 25 in-house performed syntheses. Latin Hypercube Sampling is used to efficiently cover the parameter space while creating the experimental data set. Ensemble regression models, built with the AMADEUS framework, successfully predict particle sizes with high accuracy ($R^2 = 0.74$), outperforming classical statistical approaches ($R^2 = 0.60$). Overall, this study highlights that, for lab-scale synthesis optimization, high-quality small datasets combined with classical, interpretable ML models outperform traditional statistical methods and are fully sufficient for quantitative synthesis prediction. This approach provides a sustainable and experimentally realistic pathway toward data-driven inorganic synthesis design.

</details>


### [248] [How accurate are foundational machine learning interatomic potentials for heterogeneous catalysis?](https://arxiv.org/abs/2512.16702)
*Luuk H. E. Kempen,Raffaele Cheula,Mie Andersen*

Main category: cond-mat.mtrl-sci

TL;DR: 本文系统分析80种MLIPs在多相催化典型任务中的零样本性能，指出其优缺点并对比特定任务模型，表明无通用最佳MLIP。


<details>
  <summary>Details</summary>
Motivation: 现有的MLIPs基准测试通常局限于有序、晶体和块状材料，报告的性能不一定能准确反映其在多相催化等实际应用中的表现。

Method: 系统分析80种不同MLIPs在多相催化典型任务中的零样本性能，评估不同数据集上的任务。

Result: 当前基础MLIPs在一些应用中能达到高精度，但在磁性材料上表现差，结构弛豫会增加能量预测误差；特定任务模型在精度上可与最佳MLIPs竞争，且无单一MLIP始终表现最佳。

Conclusion: 用户需针对自身应用考察MLIP的适用性。

Abstract: Foundational machine learning interatomic potentials (MLIPs) are being developed at a rapid pace, promising closer and closer approximation to ab initio accuracy. This unlocks the possibility to simulate much larger length and time scales. However, benchmarks for these MLIPs are usually limited to ordered, crystalline and bulk materials. Hence, reported performance does not necessarily accurately reflect MLIP performance in real applications such as heterogeneous catalysis. Here, we systematically analyze zero-shot performance of 80 different MLIPs, evaluating tasks typical for heterogeneous catalysis across a range of different data sets, including adsorption and reaction on surfaces of alloyed metals, oxides, and metal-oxide interfacial systems. We demonstrate that current-generation foundational MLIPs can already perform at high accuracy for applications such as predicting vacancy formation energies of perovskite oxides or zero-point energies of supported nanoclusters. However, limitations also exist. We find that many MLIPs catastrophically fail when applied to magnetic materials, and structure relaxation in the MLIP generally increases the energy prediction error compared to single-point evaluation of a previously optimized structure. Comparing low-cost task-specific models to foundational MLIPs, we highlight some core differences between these model approaches and show that -- if considering only accuracy -- these models can compete with the current generation of best-performing MLIPs. Furthermore, we show that no single MLIP universally performs best, requiring users to investigate MLIP suitability for their desired application.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [249] [Reliability-Targeted Simulation of Item Response Data: Solving the Inverse Design Problem](https://arxiv.org/abs/2512.16012)
*JoonHo Lee*

Main category: stat.ME

TL;DR: 本文针对IRT方法蒙特卡罗模拟中忽略可靠性的问题，提出可靠性目标模拟框架及两种算法，验证了算法效果并解释理论差异，还提供开源R包。


<details>
  <summary>Details</summary>
Motivation: IRT研究将可靠性作为偶然结果，存在“可靠性遗漏”问题，影响生成数据的信噪比，因此需解决该问题。

Method: 引入可靠性目标模拟框架，提出经验正交校准（EQC）和随机近似校准（SAC）两种算法。

Result: 综合验证研究表明，EQC能实现精确校准，SAC在非正态潜在分布和经验项目库中保持无偏性。

Conclusion: 明确了不同可靠性度量的理论区别，开源R包IRTsimrel可使研究人员将可靠性作为可控实验输入进行标准化。

Abstract: Monte Carlo simulations are the primary methodology for evaluating Item Response Theory (IRT) methods, yet marginal reliability - the fundamental metric of data informativeness - is rarely treated as an explicit design factor. Unlike in multilevel modeling where the intraclass correlation (ICC) is routinely manipulated, IRT studies typically treat reliability as an incidental outcome, creating a "reliability omission" that obscures the signal-to-noise ratio of generated data. To address this gap, we introduce a principled framework for reliability-targeted simulation, transforming reliability from an implicit by-product into a precise input parameter. We formalize the inverse design problem, solving for a global discrimination scaling factor that uniquely achieves a pre-specified target reliability. Two complementary algorithms are proposed: Empirical Quadrature Calibration (EQC) for rapid, deterministic precision, and Stochastic Approximation Calibration (SAC) for rigorous stochastic estimation. A comprehensive validation study across 960 conditions demonstrates that EQC achieves essentially exact calibration, while SAC remains unbiased across non-normal latent distributions and empirical item pools. Furthermore, we clarify the theoretical distinction between average-information and error-variance-based reliability metrics, showing they require different calibration scales due to Jensen's inequality. An accompanying open-source R package, IRTsimrel, enables researchers to standardize reliability as a controlled experimental input.

</details>


### [250] [Asymptotic and finite-sample distributions of one- and two-sample empirical relative entropy, with application to change-point detection](https://arxiv.org/abs/2512.16411)
*Matthieu Garcin,Louis Perot*

Main category: stat.ME

TL;DR: 研究经验相对熵分布以构建统计检验用于离线变点检测，对比经典方法并在真实数据集验证。


<details>
  <summary>Details</summary>
Motivation: 相对熵可用于离线变点检测，扩展依赖矩差异的经典方法，需构建合适统计检验。

Method: 研究经验相对熵分布，推导多种近似，引入新方法获Berry - Esseen不等式，构建变点检测程序。

Result: 理论贡献涵盖单样本和双样本经验相对熵，通过模拟对比经典方法。

Conclusion: 所提基于相对熵的变点检测方法有实际应用价值，在温度序列和股票指数波动率数据集得到验证。

Abstract: Relative entropy, as a divergence metric between two distributions, can be used for offline change-point detection and extends classical methods that mainly rely on moment-based discrepancies. To build a statistical test suitable for this context, we study the distribution of empirical relative entropy and derive several types of approximations: concentration inequalities for finite samples, asymptotic distributions, and Berry-Esseen bounds in a pre-asymptotic regime. For the latter, we introduce a new approach to obtain Berry-Esseen inequalities for nonlinear functions of sum statistics under some convexity assumptions. Our theoretical contributions cover both one- and two-sample empirical relative entropies. We then detail a change-point detection procedure built on relative entropy and compare it, through extensive simulations, with classical methods based on moments or on information criteria. Finally, we illustrate its practical relevance on two real datasets involving temperature series and volatility of stock indices.

</details>


### [251] [Efficient and scalable clustering of survival curves](https://arxiv.org/abs/2512.16481)
*Nora M. Villanueva,Marta Sestelo,Luis Meira-Machado*

Main category: stat.ME

TL;DR: 提出用k - means和log - rank测试识别与聚类生存曲线的新方法，能降计算成本，模拟研究显示结果与现有方法相当且效率高。


<details>
  <summary>Details</summary>
Motivation: 传统识别生存曲线簇的方法计算负担重，需更高效方法。

Method: 利用k - means和log - rank测试识别与聚类生存曲线，无需昂贵重采样。

Result: 模拟研究表明该方法结果与基于bootstrap的聚类方法相当，大幅提升计算效率。

Conclusion: 基于log - rank的聚类程序为医学和流行病学研究中处理多条生存曲线提供可行且省时的解决方案。

Abstract: Survival analysis encompasses a broad range of methods for analyzing time-to-event data, with one key objective being the comparison of survival curves across groups. Traditional approaches for identifying clusters of survival curves often rely on computationally intensive bootstrap techniques to approximate the null hypothesis distribution. While effective, these methods impose significant computational burdens. In this work, we propose a novel approach that leverages the k-means and log-rank test to efficiently identify and cluster survival curves. Our method eliminates the need for computationally expensive resampling, significantly reducing processing time while maintaining statistical reliability. By systematically evaluating survival curves and determining optimal clusters, the proposed method ensures a practical and scalable alternative for large-scale survival data analysis. Through simulation studies, we demonstrate that our approach achieves results comparable to existing bootstrap-based clustering methods while dramatically improving computational efficiency. These findings suggest that the log-rank-based clustering procedure offers a viable and time-efficient solution for researchers working with multiple survival curves in medical and epidemiological studies.

</details>


### [252] [Consensus dimension reduction via multi-view learning](https://arxiv.org/abs/2512.15802)
*Bingxue An,Tiffany M. Tang*

Main category: stat.ME

TL;DR: 传统降维方法对同一数据输出结果可能不同且受超参数影响，文章提出共识方法整合多可视化结果，实验证明有效且对方法和超参数选择鲁棒。


<details>
  <summary>Details</summary>
Motivation: 不同降维方法对相同数据输出不同可能冲突的可视化结果，且受超参数影响大，需更鲁棒可信的降维输出。

Method: 采用共识方法，借鉴多视图学习思路识别不同降维可视化中的稳定共享模式，将其可视化到单一低维图中。

Result: 通过模拟和真实案例研究，证明共识可视化能有效识别并保留共享的低维数据结构。

Conclusion: 该方法对降维方法和超参数的选择具有鲁棒性，有利于实现可信且可复现的数据科学工作。

Abstract: A plethora of dimension reduction methods have been developed to visualize high-dimensional data in low dimensions. However, different dimension reduction methods often output different and possibly conflicting visualizations of the same data. This problem is further exacerbated by the choice of hyperparameters, which may substantially impact the resulting visualization. To obtain a more robust and trustworthy dimension reduction output, we advocate for a consensus approach, which summarizes multiple visualizations into a single consensus dimension reduction visualization. Here, we leverage ideas from multi-view learning in order to identify the patterns that are most stable or shared across the many different dimension reduction visualizations, or views, and subsequently visualize this shared structure in a single low-dimensional plot. We demonstrate that this consensus visualization effectively identifies and preserves the shared low-dimensional data structure through both simulated and real-world case studies. We further highlight our method's robustness to the choice of dimension reduction method and hyperparameters -- a highly-desirable property when working towards trustworthy and reproducible data science.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [253] [The Colombian legislative process, 2014-2025: networks, topics, and polarization](https://arxiv.org/abs/2512.16827)
*Juan Sosa,Brayan Riveros,Emma J. Camargo-Díaz*

Main category: physics.soc-ph

TL;DR: 本文用4083项法案分析2014 - 2025年哥伦比亚众议院立法产出，综合关系和语义结构来描述各政府时期主题变化等情况。


<details>
  <summary>Details</summary>
Motivation: 评估政治两极分化是否反映在立法合作中，描述主题变化，识别有影响力的行为者和集体，促进立法过程的透明度和公民监督。

Method: 构建政党与法案、代表与法案的二部网络及其投影；基于短描述中的共现构建语义网络，用随机块模型和Latent Dirichlet Allocation识别主题；应用贝叶斯社交模型检测连接性强的术语。

Result: 未提及具体结果内容。

Conclusion: 该方法整合关系和语义结构，可描述主题变化，识别有影响力的主体，提供可复现的综合信息以促进立法透明和公民监督。

Abstract: The legislative output of Colombia's House of Representatives between 2014 and 2025 is analyzed using 4,083 bills. Bipartite networks are constructed between parties and bills, and between representatives and bills, along with their projections, to characterize co-sponsorship patterns, centrality, and influence, and to assess whether political polarization is reflected in legislative collaboration. In parallel, the content of the initiatives is studied through semantic networks based on co-occurrences extracted from short descriptions, and topics by party and period are identified using a stochastic block model for weighted networks, with additional comparison using Latent Dirichlet Allocation. In addition, a Bayesian sociability model is applied to detect terms with robust connectivity and to summarize discursive cores. Overall, the approach integrates relational and semantic structure to describe thematic shifts across administrations, identify influential actors and collectives, and provide a reproducible synthesis that promotes transparency and citizen oversight of the legislative process.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [254] [Prefix Sums via Kronecker Products](https://arxiv.org/abs/2512.16309)
*Aleksandros Sobczyk,Anastasios Zouzias*

Main category: quant-ph

TL;DR: 通过线性代数重新审视前缀和，设计递归前缀和算法与电路，该电路有三项特性，并用于设计量子加法器提升性能。


<details>
  <summary>Details</summary>
Motivation: 在当前背景下期望找到具有特定优势的前缀和算法与电路，改进量子加法器性能。

Method: 借助线性代数，利用三角全1矩阵分解为两个克罗内克积之和的恒等式来设计递归前缀和算法与电路。

Result: 设计的电路能同时具备零缺陷、每级恒定扇出和深度渐近严格小于2log(n)的特性，且用其设计的量子加法器在Toffoli深度、Toffoli门数量和额外量子比特数上有提升。

Conclusion: 通过线性代数方法设计的前缀和电路可行且有优势，能有效改进量子加法器性能。

Abstract: In this work, we revisit prefix sums through the lens of linear algebra. We describe an identity that decomposes triangular all-ones matrices as a sum of two Kronecker products, and apply it to design recursive prefix sum algorithms and circuits. Notably, the proposed family of circuits is the first one that achieves the following three properties simultaneously: (i) zero-deficiency, (ii) constant fan-out per-level, and (iii) depth that is asymptotically strictly smaller than $2\log(n)$ for input length n. As an application, we show how to use these circuits to design quantum adders with $1.893\log(n) + O(1)$ Toffoli depth, $O(n)$ Toffoli gates, and $O(n)$ additional qubits, improving the Toffoli depth and/or Toffoli size of existing constructions.

</details>


### [255] [Non-Linear Strong Data-Processing for Quantum Hockey-Stick Divergences](https://arxiv.org/abs/2512.16778)
*Theshani Nuradha,Ian George,Christoph Hirche*

Main category: quant-ph

TL;DR: 本文为满足特定噪声准则的噪声信道建立量子曲棍球棒散度的非线性强数据处理不等式（SDPI），改进现有结果，定义广义曲线，推导反向平斯克型不等式，展示其在确定有限混合时间和隐私保证方面的应用。


<details>
  <summary>Details</summary>
Motivation: 线性SDPI并不总是最优的，多数情况下可改进，因此希望建立更优的非线性SDPI。

Method: 为满足特定噪声准则的噪声信道建立量子曲棍球棒散度的非线性SDPI，定义 $F_γ$ 曲线，推导反向平斯克型不等式。

Result: 建立的非线性SDPI改进了现有线性SDPI和经典曲棍球棒散度的非线性SDPI，能确定更严格的有限混合时间。

Conclusion: 非线性SDPI比线性SDPI更优，在确定有限混合时间和量子局部差分隐私的隐私保证方面有应用价值。

Abstract: Data-processing is a desired property of classical and quantum divergences and information measures. In information theory, the contraction coefficient measures how much the distinguishability of quantum states decreases when they are transmitted through a quantum channel, establishing linear strong data-processing inequalities (SDPI). However, these linear SDPI are not always tight and can be improved in most of the cases. In this work, we establish non-linear SDPI for quantum hockey-stick divergence for noisy channels that satisfy a certain noise criterion. We also note that our results improve upon existing linear SDPI for quantum hockey-stick divergences and also non-linear SDPI for classical hockey-stick divergence. We define $F_γ$ curves generalizing Dobrushin curves for the quantum setting while characterizing SDPI for the sequential composition of heterogeneous channels. In addition, we derive reverse-Pinsker type inequalities for $f$-divergences with additional constraints on hockey-stick divergences. We show that these non-linear SDPI can establish tighter finite mixing times that cannot be achieved through linear SDPI. Furthermore, we find applications of these in establishing stronger privacy guarantees for the composition of sequential private quantum channels when privacy is quantified by quantum local differential privacy.

</details>


<div id='nlin.AO'></div>

# nlin.AO [[Back]](#toc)

### [256] [The Universe Learning Itself: On the Evolution of Dynamics from the Big Bang to Machine Intelligence](https://arxiv.org/abs/2512.16515)
*Pradeep Singh,Mudasani Rushikesh,Bezawada Sri Sai Anurag,Balasubramanian Raman*

Main category: nlin.AO

TL;DR: 提出从宇宙大爆炸到当代人类社会及人工智能的统一动力学系统叙事，强调跨尺度理论视角。


<details>
  <summary>Details</summary>
Motivation: 打破不同领域的分割，以统一视角理解宇宙结构形成。

Method: 将各领域视为不同状态空间的动力学机制，通过相变等衔接，强调数学主题。

Result: 构建从宇宙起源到人类文化科技的统一动力学叙事。

Conclusion: 提供跨尺度理论视角，将宇宙历史视为动力学自身的演化。

Abstract: We develop a unified, dynamical-systems narrative of the universe that traces a continuous chain of structure formation from the Big Bang to contemporary human societies and their artificial learning systems. Rather than treating cosmology, astrophysics, geophysics, biology, cognition, and machine intelligence as disjoint domains, we view each as successive regimes of dynamics on ever-richer state spaces, stitched together by phase transitions, symmetry-breaking events, and emergent attractors. Starting from inflationary field dynamics and the growth of primordial perturbations, we describe how gravitational instability sculpts the cosmic web, how dissipative collapse in baryonic matter yields stars and planets, and how planetary-scale geochemical cycles define long-lived nonequilibrium attractors. Within these attractors, we frame the origin of life as the emergence of self-maintaining reaction networks, evolutionary biology as flow on high-dimensional genotype-phenotype-environment manifolds, and brains as adaptive dynamical systems operating near critical surfaces. Human culture and technology-including modern machine learning and artificial intelligence-are then interpreted as symbolic and institutional dynamics that implement and refine engineered learning flows which recursively reshape their own phase space. Throughout, we emphasize recurring mathematical motifs-instability, bifurcation, multiscale coupling, and constrained flows on measure-zero subsets of the accessible state space. Our aim is not to present any new cosmological or biological model, but a cross-scale, theoretical perspective: a way of reading the universe's history as the evolution of dynamics itself, culminating (so far) in biological and artificial systems capable of modeling, predicting, and deliberately perturbing their own future trajectories.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [257] [LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation](https://arxiv.org/abs/2512.16891)
*Haichao Zhang,Yao Lu,Lichen Wang,Yunzhe Li,Daiwei Chen,Yunpeng Xu,Yun Fu*

Main category: cs.CV

TL;DR: 现有视频大语言模型在视频推荐等下游任务部署有挑战，本文提出LinkedOut表示法解决问题，在基准测试达SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前视频大语言模型在下游视频推荐任务部署时存在高延迟、不支持多视频输入、丢失视觉细节等问题，缺乏能保留像素级细节并利用世界知识的表示。

Method: 提出LinkedOut表示法，从视频中直接提取VLLM世界知识，使用可提示查询和可选辅助模态从原始帧提取语义和知识感知的标记，引入跨层知识融合MoE。

Result: LinkedOut是首个基于VLLM且无需手工标签处理原始帧的视频推荐方法，在标准基准测试中取得了SOTA结果。

Conclusion: 可解释性研究和消融实验证实了层多样性和层融合的好处，为下游视觉任务提供了充分利用VLLM世界知识先验和视觉推理的实用途径。

Abstract: Video Large Language Models (VLLMs) unlock world-knowledge-aware video understanding through pretraining on internet-scale data and have already shown promise on tasks such as movie analysis and video question answering. However, deploying VLLMs for downstream tasks such as video recommendation remains challenging, since real systems require multi-video inputs, lightweight backbones, low-latency sequential inference, and rapid response. In practice, (1) decode-only generation yields high latency for sequential inference, (2) typical interfaces do not support multi-video inputs, and (3) constraining outputs to language discards fine-grained visual details that matter for downstream vision tasks. We argue that these limitations stem from the absence of a representation that preserves pixel-level detail while leveraging world knowledge. We present LinkedOut, a representation that extracts VLLM world knowledge directly from video to enable fast inference, supports multi-video histories, and removes the language bottleneck. LinkedOut extracts semantically grounded, knowledge-aware tokens from raw frames using VLLMs, guided by promptable queries and optional auxiliary modalities. We introduce a cross-layer knowledge fusion MoE that selects the appropriate level of abstraction from the rich VLLM features, enabling personalized, interpretable, and low-latency recommendation. To our knowledge, LinkedOut is the first VLLM-based video recommendation method that operates on raw frames without handcrafted labels, achieving state-of-the-art results on standard benchmarks. Interpretability studies and ablations confirm the benefits of layer diversity and layer-wise fusion, pointing to a practical path that fully leverages VLLM world-knowledge priors and visual reasoning for downstream vision tasks such as recommendation.

</details>


### [258] [Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep Neural Network and Grad-CAM Explanations](https://arxiv.org/abs/2511.00456)
*Kiran Shahi,Anup Bagale*

Main category: cs.CV

TL;DR: 提出用Grad - CAM的弱监督深度学习框架进行肺炎分类和定位，评估七个预训练模型，模型精度高，凸显弱监督可解释模型在肺炎筛查潜力。


<details>
  <summary>Details</summary>
Motivation: 胸部X光诊断肺炎时，获取准确本地化所需的像素级注释成本高且耗时，需要改进方法。

Method: 提出使用Grad - CAM的弱监督深度学习框架，用图像级标签生成热图；评估七个预训练模型，采用焦点损失和患者级分割防止数据泄漏。

Result: 所有模型分类准确率达96 - 98%，ResNet - 18和EfficientNet - B0表现最佳，MobileNet - V3是轻量级选择；Grad - CAM热图聚焦临床相关肺部区域。

Conclusion: 弱监督、可解释模型能增强AI辅助肺炎筛查的透明度和临床信任。

Abstract: Chest X-ray imaging is commonly used to diagnose pneumonia, but accurately localizing the pneumonia-affected regions typically requires detailed pixel-level annotations, which are costly and time consuming to obtain. To address this limitation, this study proposes a weakly supervised deep learning framework for pneumonia classification and localization using Gradient-weighted Class Activation Mapping (Grad-CAM). Instead of relying on costly pixel-level annotations, the proposed method utilizes image-level labels to generate clinically meaningful heatmaps that highlight pneumonia-affected regions. Furthermore, we evaluate seven pre-trained deep learning models, including a Vision Transformer, under identical training conditions, using focal loss and patient-wise splits to prevent data leakage. Experimental results suggest that all models achieved high classification accuracy (96--98\%), with ResNet-18 and EfficientNet-B0 showing the best overall performance and MobileNet-V3 providing an efficient lightweight alternative. Grad-CAM heatmap visualizations confirm that the proposed methods focus on clinically relevant lung regions, supporting the use of explainable AI for radiological diagnostics. Overall, this work highlights the potential of weakly supervised, explainable models that enhance transparency and clinical trust in AI-assisted pneumonia screening.

</details>


### [259] [Seeing Beyond Words: Self-Supervised Visual Learning for Multimodal Large Language Models](https://arxiv.org/abs/2512.15885)
*Davide Caffagni,Sara Sarto,Marcella Cornia,Lorenzo Baraldi,Pier Luigi Dovesi,Shaghayegh Roohi,Mark Granroth-Wilding,Rita Cucchiara*

Main category: cs.CV

TL;DR: MLLMs在基础视觉推理任务能力有限，提出JARVIS框架提升其视觉能力，实验证明有效且代码开源。


<details>
  <summary>Details</summary>
Motivation: MLLMs在基础视觉推理任务能力有限，因主要从文本描述学习视觉理解且多模态指令微调规模小。

Method: 将I - JEPA学习范式集成到MLLMs训练的标准视觉 - 语言对齐流程，用冻结视觉基础模型作编码器，训练LLM早期层作预测器。

Result: 在标准MLLM基准测试中，JARVIS持续提升以视觉为中心的基准测试性能，不降低多模态推理能力。

Conclusion: JARVIS能有效增强MLLMs的视觉理解能力。

Abstract: Multimodal Large Language Models (MLLMs) have recently demonstrated impressive capabilities in connecting vision and language, yet their proficiency in fundamental visual reasoning tasks remains limited. This limitation can be attributed to the fact that MLLMs learn visual understanding primarily from textual descriptions, which constitute a subjective and inherently incomplete supervisory signal. Furthermore, the modest scale of multimodal instruction tuning compared to massive text-only pre-training leads MLLMs to overfit language priors while overlooking visual details. To address these issues, we introduce JARVIS, a JEPA-inspired framework for self-supervised visual enhancement in MLLMs. Specifically, we integrate the I-JEPA learning paradigm into the standard vision-language alignment pipeline of MLLMs training. Our approach leverages frozen vision foundation models as context and target encoders, while training the predictor, implemented as the early layers of an LLM, to learn structural and semantic regularities from images without relying exclusively on language supervision. Extensive experiments on standard MLLM benchmarks show that JARVIS consistently improves performance on vision-centric benchmarks across different LLM families, without degrading multimodal reasoning abilities. Our source code is publicly available at: https://github.com/aimagelab/JARVIS.

</details>


### [260] [Seeing is Believing (and Predicting): Context-Aware Multi-Human Behavior Prediction with Vision Language Models](https://arxiv.org/abs/2512.15957)
*Utsav Panchal,Yuchen Liu,Luigi Palmieri,Ilche Georgievski,Marco Aiello*

Main category: cs.CV

TL;DR: 提出基于VLM的CAMP - VLM框架预测多人行为，用合成数据微调，评估泛化能力，精度超基线。


<details>
  <summary>Details</summary>
Motivation: 先前研究多关注单人场景行为预测，而一些机器人应用需从第三人视角理解多人行为。

Method: 提出CAMP - VLM框架，结合视觉输入的上下文特征和场景图的空间感知；用模拟器生成的合成数据微调模型；使用SFT和DPO方法。

Result: CAMP - VLM在预测精度上比最佳基线模型最高高出66.9%。

Conclusion: CAMP - VLM框架能有效提升多人行为预测的准确性，在合成和真实场景数据上有较好的泛化能力。

Abstract: Accurately predicting human behaviors is crucial for mobile robots operating in human-populated environments. While prior research primarily focuses on predicting actions in single-human scenarios from an egocentric view, several robotic applications require understanding multiple human behaviors from a third-person perspective. To this end, we present CAMP-VLM (Context-Aware Multi-human behavior Prediction): a Vision Language Model (VLM)-based framework that incorporates contextual features from visual input and spatial awareness from scene graphs to enhance prediction of humans-scene interactions. Due to the lack of suitable datasets for multi-human behavior prediction from an observer view, we perform fine-tuning of CAMP-VLM with synthetic human behavior data generated by a photorealistic simulator, and evaluate the resulting models on both synthetic and real-world sequences to assess their generalization capabilities. Leveraging Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO), CAMP-VLM outperforms the best-performing baseline by up to 66.9% in prediction accuracy.

</details>


### [261] [Two-Step Data Augmentation for Masked Face Detection and Recognition: Turning Fake Masks to Real](https://arxiv.org/abs/2512.15774)
*Yan Yang,George Bebis,Mircea Nicolescu*

Main category: cs.CV

TL;DR: 提出两步生成式数据增强框架解决口罩人脸检测和识别中数据稀缺和分布偏移问题，实验验证其有效性并给出未来改进方向


<details>
  <summary>Details</summary>
Motivation: 解决口罩人脸检测和识别中数据稀缺和分布偏移的挑战

Method: 结合基于规则的口罩变形和基于GAN的无配对图像到图像转换的两步生成式数据增强框架，引入非口罩保留损失和随机噪声注入

Result: 相比仅基于规则的变形有一致的定性改进，补充了现有基于GAN的口罩人脸生成方法

Conclusion: 所提组件有效，为以数据为中心的人脸识别任务增强提供未来改进方向

Abstract: Data scarcity and distribution shift pose major challenges for masked face detection and recognition. We propose a two-step generative data augmentation framework that combines rule-based mask warping with unpaired image-to-image translation using GANs, enabling the generation of realistic masked-face samples beyond purely synthetic transformations. Compared to rule-based warping alone, the proposed approach yields consistent qualitative improvements and complements existing GAN-based masked face generation methods such as IAMGAN. We introduce a non-mask preservation loss and stochastic noise injection to stabilize training and enhance sample diversity. Experimental observations highlight the effectiveness of the proposed components and suggest directions for future improvements in data-centric augmentation for face recognition tasks.

</details>


### [262] [LAPX: Lightweight Hourglass Network with Global Context](https://arxiv.org/abs/2512.16089)
*Haopeng Zhao,Marsha Mariya Kappan,Mahdi Bamdad,Francisco Cruz*

Main category: cs.CV

TL;DR: 提出轻量级人体姿态估计模型LAPX，在数据集上取得好结果且适合边缘设备。


<details>
  <summary>Details</summary>
Motivation: 现有SOTA人体姿态估计方法参数多、计算成本高，轻量级方法在边缘设备部署有局限，强调推理速度的模型精度有限。

Method: 基于LAP提出含自注意力机制的Hourglass网络LAPX，改进阶段设计并优化轻量级注意力模块。

Result: 在MPII和COCO数据集上取得有竞争力的结果，仅2.3M参数，具备实时性能。

Conclusion: LAPX适合边缘设备部署。

Abstract: Human pose estimation is a crucial task in computer vision. Methods that have SOTA (State-of-the-Art) accuracy, often involve a large number of parameters and incur substantial computational cost. Many lightweight variants have been proposed to reduce the model size and computational cost of them. However, several of these methods still contain components that are not well suited for efficient deployment on edge devices. Moreover, models that primarily emphasize inference speed on edge devices often suffer from limited accuracy due to their overly simplified designs. To address these limitations, we propose LAPX, an Hourglass network with self-attention that captures global contextual information, based on previous work, LAP. In addition to adopting the self-attention module, LAPX advances the stage design and refine the lightweight attention modules. It achieves competitive results on two benchmark datasets, MPII and COCO, with only 2.3M parameters, and demonstrates real-time performance, confirming its edge-device suitability.

</details>


### [263] [TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times](https://arxiv.org/abs/2512.16093)
*Jintao Zhang,Kaiwen Zheng,Kai Jiang,Haoxu Wang,Ion Stoica,Joseph E. Gonzalez,Jianfei Chen,Jun Zhu*

Main category: cs.CV

TL;DR: 介绍TurboDiffusion视频生成加速框架，能在保持视频质量下将端到端扩散生成加速100 - 200倍，并给出实验结果和代码仓库。


<details>
  <summary>Details</summary>
Motivation: 加速视频生成过程，同时保持视频质量。

Method: 采用低比特SageAttention和可训练的Sparse - Linear Attention加速注意力计算；用rCM进行高效步骤蒸馏；将模型参数和激活量化为8位以加速线性层和压缩模型，并结合其他工程优化。

Result: 在多个模型上实验，即使在单张RTX 5090 GPU上，TurboDiffusion也能将视频生成加速100 - 200倍，且保持相近视频质量。

Conclusion: TurboDiffusion是一个有效的视频生成加速框架，可显著提升视频生成速度。

Abstract: We introduce TurboDiffusion, a video generation acceleration framework that can speed up end-to-end diffusion generation by 100-200x while maintaining video quality. TurboDiffusion mainly relies on several components for acceleration: (1) Attention acceleration: TurboDiffusion uses low-bit SageAttention and trainable Sparse-Linear Attention (SLA) to speed up attention computation. (2) Step distillation: TurboDiffusion adopts rCM for efficient step distillation. (3) W8A8 quantization: TurboDiffusion quantizes model parameters and activations to 8 bits to accelerate linear layers and compress the model. In addition, TurboDiffusion incorporates several other engineering optimizations.
  We conduct experiments on the Wan2.2-I2V-14B-720P, Wan2.1-T2V-1.3B-480P, Wan2.1-T2V-14B-720P, and Wan2.1-T2V-14B-480P models. Experimental results show that TurboDiffusion achieves 100-200x speedup for video generation even on a single RTX 5090 GPU, while maintaining comparable video quality. The GitHub repository, which includes model checkpoints and easy-to-use code, is available at https://github.com/thu-ml/TurboDiffusion.

</details>


### [264] [FOD-Diff: 3D Multi-Channel Patch Diffusion Model for Fiber Orientation Distribution](https://arxiv.org/abs/2512.16075)
*Hao Tang,Hanyu Liu,Alessandro Perelli,Xi Chen,Chao Li*

Main category: cs.CV

TL;DR: 提出3D多通道补丁扩散模型从低角分辨率扩散磁共振成像（LAR - FOD）预测高角分辨率扩散磁共振成像（HAR - FOD），实验效果佳。


<details>
  <summary>Details</summary>
Motivation: 传统从LAR - FOD估计FOD精度有限，从HAR - FOD估计则扫描时间长，现有扩散模型生成HAR - FOD因球谐系数多有挑战。

Method: 提出3D多通道补丁扩散模型，设计FOD - 补丁适配器、体素级条件协调模块和球谐注意力模块。

Result: 该方法在HAR - FOD预测中表现最佳，优于其他先进方法。

Conclusion: 所提模型能有效从LAR - FOD预测HAR - FOD，具有良好性能。

Abstract: Diffusion MRI (dMRI) is a critical non-invasive technique to estimate fiber orientation distribution (FOD) for characterizing white matter integrity. Estimating FOD from single-shell low angular resolution dMRI (LAR-FOD) is limited by accuracy, whereas estimating FOD from multi-shell high angular resolution dMRI (HAR-FOD) requires a long scanning time, which limits its applicability. Diffusion models have shown promise in estimating HAR-FOD based on LAR-FOD. However, using diffusion models to efficiently generate HAR-FOD is challenging due to the large number of spherical harmonic (SH) coefficients in FOD. Here, we propose a 3D multi-channel patch diffusion model to predict HAR-FOD from LAR-FOD. We design the FOD-patch adapter by introducing the prior brain anatomy for more efficient patch-based learning. Furthermore, we introduce a voxel-level conditional coordinating module to enhance the global understanding of the model. We design the SH attention module to effectively learn the complex correlations of the SH coefficients. Our experimental results show that our method achieves the best performance in HAR-FOD prediction and outperforms other state-of-the-art methods.

</details>


### [265] [C-DGPA: Class-Centric Dual-Alignment Generative Prompt Adaptation](https://arxiv.org/abs/2512.16164)
*Chao Li,Dasha Hu,Chengyang Li,Yuming Jiang,Yuncheng Shen*

Main category: cs.CV

TL;DR: 本文提出C - DGPA方法解决无监督域适应中VLM提示调优面临的领域差异问题，经实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 直接在下游无监督域适应任务中使用提示调优部署视觉 - 语言模型面临减轻领域差异的挑战，现有提示调优策略忽视条件分布差异，导致类原型不对齐和语义可区分性下降等问题。

Method: 提出C - DGPA方法，通过新颖的双分支架构协同优化边缘分布对齐和条件分布对齐，边缘分布对齐分支采用动态对抗训练框架，条件分布对齐分支引入类映射机制。

Result: 在OfficeHome、Office31和VisDA - 2017上的大量实验验证了C - DGPA的优越性，在所有基准测试中取得了新的最优结果。

Conclusion: C - DGPA的双对齐策略通过协同优化将领域知识有效集成到提示学习中，确保了领域不变和语义可区分的表示。

Abstract: Unsupervised Domain Adaptation transfers knowledge from a labeled source domain to an unlabeled target domain. Directly deploying Vision-Language Models (VLMs) with prompt tuning in downstream UDA tasks faces the signifi cant challenge of mitigating domain discrepancies. Existing prompt-tuning strategies primarily align marginal distribu tion, but neglect conditional distribution discrepancies, lead ing to critical issues such as class prototype misalignment and degraded semantic discriminability. To address these lim itations, the work proposes C-DGPA: Class-Centric Dual Alignment Generative Prompt Adaptation. C-DGPA syner gistically optimizes marginal distribution alignment and con ditional distribution alignment through a novel dual-branch architecture. The marginal distribution alignment branch em ploys a dynamic adversarial training framework to bridge marginal distribution discrepancies. Simultaneously, the con ditional distribution alignment branch introduces a Class Mapping Mechanism (CMM) to align conditional distribu tion discrepancies by standardizing semantic prompt under standing and preventing source domain over-reliance. This dual alignment strategy effectively integrates domain knowl edge into prompt learning via synergistic optimization, ensur ing domain-invariant and semantically discriminative repre sentations. Extensive experiments on OfficeHome, Office31, and VisDA-2017 validate the superiority of C-DGPA. It achieves new state-of-the-art results on all benchmarks.

</details>


### [266] [Towards Closing the Domain Gap with Event Cameras](https://arxiv.org/abs/2512.16178)
*M. Oltan Sevinc,Liao Wu,Francisco Cruz*

Main category: cs.CV

TL;DR: 传统相机在跨光照条件时性能受领域差距影响，本文提出事件相机作为替代，结果显示事件相机在光照条件间性能更稳定，跨域场景表现更好。


<details>
  <summary>Details</summary>
Motivation: 解决传统相机在训练数据条件与部署环境不匹配时（即领域差距问题），尤其是日夜间光照差异的性能下降问题。

Method: 提出使用事件相机替代传统相机来应对光照条件领域差距问题。

Result: 事件相机在不同光照条件下能保持更稳定的性能，领域偏移损失通常与灰度帧相当或更小，在跨域场景中提供更优的基线性能。

Conclusion: 事件相机可作为跨光照条件领域差距问题的潜在替代方案，无需额外调整即可保持性能。

Abstract: Although traditional cameras are the primary sensor for end-to-end driving, their performance suffers greatly when the conditions of the data they were trained on does not match the deployment environment, a problem known as the domain gap. In this work, we consider the day-night lighting difference domain gap. Instead of traditional cameras we propose event cameras as a potential alternative which can maintain performance across lighting condition domain gaps without requiring additional adjustments. Our results show that event cameras maintain more consistent performance across lighting conditions, exhibiting domain-shift penalties that are generally comparable to or smaller than grayscale frames and provide superior baseline performance in cross-domain scenarios.

</details>


### [267] [Open Ad-hoc Categorization with Contextualized Feature Learning](https://arxiv.org/abs/2512.16202)
*Zilin Wang,Sangwoo Mo,Stella X. Yu,Sima Behpour,Liu Ren*

Main category: cs.CV

TL;DR: 研究开放即席分类，提出OAK模型，在多数据集上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 自适应视觉场景分类对AI代理处理变化任务至关重要，研究开放即席分类问题。

Method: 提出OAK模型，在冻结CLIP输入处引入可学习上下文标记，结合CLIP图像文本对齐和GCD视觉聚类目标优化。

Result: 在Stanford和Clevr - 4数据集上，OAK在多分类任务中准确率和概念发现方面达SOTA，如在Stanford Mood上有87.4%的新颖准确率，超CLIP和GCD超50%，还能生成可解释显著性图。

Conclusion: OAK模型能实现自适应和可推广的分类，同时提高透明度和可信度。

Abstract: Adaptive categorization of visual scenes is essential for AI agents to handle changing tasks. Unlike fixed common categories for plants or animals, ad-hoc categories are created dynamically to serve specific goals. We study open ad-hoc categorization: Given a few labeled exemplars and abundant unlabeled data, the goal is to discover the underlying context and to expand ad-hoc categories through semantic extension and visual clustering around it.
  Building on the insight that ad-hoc and common categories rely on similar perceptual mechanisms, we propose OAK, a simple model that introduces a small set of learnable context tokens at the input of a frozen CLIP and optimizes with both CLIP's image-text alignment objective and GCD's visual clustering objective.
  On Stanford and Clevr-4 datasets, OAK achieves state-of-the-art in accuracy and concept discovery across multiple categorizations, including 87.4% novel accuracy on Stanford Mood, surpassing CLIP and GCD by over 50%. Moreover, OAK produces interpretable saliency maps, focusing on hands for Action, faces for Mood, and backgrounds for Location, promoting transparency and trust while enabling adaptive and generalizable categorization.

</details>


### [268] [Pixel Super-Resolved Fluorescence Lifetime Imaging Using Deep Learning](https://arxiv.org/abs/2512.16266)
*Paloma Casteleiro Costa,Parnian Ghapandar Kashani,Xuhui Liu,Alexander Chen,Ary Portes,Julien Bec,Laura Marcu,Aydogan Ozcan*

Main category: cs.CV

TL;DR: 介绍基于深度学习的多通道像素超分辨率框架FLIM_PSR_k，可从大像素数据重建高分辨率FLIM图像，在患者肿瘤组织样本测试中表现良好，推动FLIM应用。


<details>
  <summary>Details</summary>
Motivation: 传统FLIM临床应用受限，存在像素驻留时间长、信噪比低及分辨率 - 速度权衡问题。

Method: 使用条件生成对抗网络（cGAN）框架训练FLIM_PSR_k模型。

Result: 在患者肿瘤组织样本盲测中，FLIM_PSR_k可靠实现k = 5的超分辨率，输出图像空间带宽积提高25倍，图像质量指标显著改善。

Conclusion: FLIM_PSR_k提高了FLIM的有效空间分辨率，推动寿命成像向更快、更高分辨率和硬件灵活的方向发展，利于其转化应用。

Abstract: Fluorescence lifetime imaging microscopy (FLIM) is a powerful quantitative technique that provides metabolic and molecular contrast, offering strong translational potential for label-free, real-time diagnostics. However, its clinical adoption remains limited by long pixel dwell times and low signal-to-noise ratio (SNR), which impose a stricter resolution-speed trade-off than conventional optical imaging approaches. Here, we introduce FLIM_PSR_k, a deep learning-based multi-channel pixel super-resolution (PSR) framework that reconstructs high-resolution FLIM images from data acquired with up to a 5-fold increased pixel size. The model is trained using the conditional generative adversarial network (cGAN) framework, which, compared to diffusion model-based alternatives, delivers a more robust PSR reconstruction with substantially shorter inference times, a crucial advantage for practical deployment. FLIM_PSR_k not only enables faster image acquisition but can also alleviate SNR limitations in autofluorescence-based FLIM. Blind testing on held-out patient-derived tumor tissue samples demonstrates that FLIM_PSR_k reliably achieves a super-resolution factor of k = 5, resulting in a 25-fold increase in the space-bandwidth product of the output images and revealing fine architectural features lost in lower-resolution inputs, with statistically significant improvements across various image quality metrics. By increasing FLIM's effective spatial resolution, FLIM_PSR_k advances lifetime imaging toward faster, higher-resolution, and hardware-flexible implementations compatible with low-numerical-aperture and miniaturized platforms, better positioning FLIM for translational applications.

</details>


### [269] [AI-Powered Dermatological Diagnosis: From Interpretable Models to Clinical Implementation A Comprehensive Framework for Accessible and Trustworthy Skin Disease Detection](https://arxiv.org/abs/2512.16235)
*Satya Narayana Panda,Vaishnavi Kukkala,Spandana Iyer*

Main category: cs.CV

TL;DR: 本文开发多模态AI框架结合家族病史与临床影像提升皮肤病诊断准确性，经验证有效果，后续计划正式临床试验。


<details>
  <summary>Details</summary>
Motivation: 皮肤病诊断困难，家族病史在诊断中未充分利用，需AI系统结合家族病史数据与临床影像提升诊断准确性并支持临床试验验证和实际应用。

Method: 开发结合深度学习图像分析和结构化临床数据（含家族病史模式）的多模态AI框架，采用可解释卷积神经网络与结合遗传风险因素的临床决策树，通过不同医疗环境的前瞻性临床试验验证AI辅助诊断。

Result: 集成AI系统纳入家族病史数据后诊断准确性提高，尤其针对遗传性皮肤病，专家反馈有早期检测和个性化推荐潜力。

Conclusion: 该框架可集成到临床工作流程，通过可解释AI机制保持可解释性，后续计划开展正式临床试验。

Abstract: Dermatological conditions affect 1.9 billion people globally, yet accurate diagnosis remains challenging due to limited specialist availability and complex clinical presentations. Family history significantly influences skin disease susceptibility and treatment responses, but is often underutilized in diagnostic processes. This research addresses the critical question: How can AI-powered systems integrate family history data with clinical imaging to enhance dermatological diagnosis while supporting clinical trial validation and real-world implementation?
  We developed a comprehensive multi-modal AI framework that combines deep learning-based image analysis with structured clinical data, including detailed family history patterns. Our approach employs interpretable convolutional neural networks integrated with clinical decision trees that incorporate hereditary risk factors. The methodology includes prospective clinical trials across diverse healthcare settings to validate AI-assisted diagnosis against traditional clinical assessment.
  In this work, validation was conducted with healthcare professionals to assess AI-assisted outputs against clinical expectations; prospective clinical trials across diverse healthcare settings are proposed as future work. The integrated AI system demonstrates enhanced diagnostic accuracy when family history data is incorporated, particularly for hereditary skin conditions such as melanoma, psoriasis, and atopic dermatitis. Expert feedback indicates potential for improved early detection and more personalized recommendations; formal clinical trials are planned. The framework is designed for integration into clinical workflows while maintaining interpretability through explainable AI mechanisms.

</details>


### [270] [TextEditBench: Evaluating Reasoning-aware Text Editing Beyond Rendering](https://arxiv.org/abs/2512.16270)
*Rui Gui,Yang Wan,Haochen Han,Dongxing Mao,Fangming Liu,Min Li,Alex Jinpeng Wang*

Main category: cs.CV

TL;DR: 文章介绍针对图像文本编辑引入TextEditBench评估基准及新评估维度SE，实验表明当前模型在文本编辑时仍有不足，为多模态生成提供测试基础。


<details>
  <summary>Details</summary>
Motivation: 当前图像中文本编辑领域研究较少，需填补生成清晰字符且保持语义、几何和上下文连贯的评估方面的空白。

Method: 引入TextEditBench评估基准，着重推理密集编辑场景；提出Semantic Expectation (SE)评估维度，衡量模型文本编辑时的推理能力。

Result: 实验显示，现有模型能遵循简单文本指令，但在上下文相关推理、物理一致性和布局感知整合方面存在困难。

Conclusion: TextEditBench聚焦长期被忽视的基础能力评估，为推进文本引导的图像编辑和多模态生成推理提供新测试平台。

Abstract: Text rendering has recently emerged as one of the most challenging frontiers in visual generation, drawing significant attention from large-scale diffusion and multimodal models. However, text editing within images remains largely unexplored, as it requires generating legible characters while preserving semantic, geometric, and contextual coherence. To fill this gap, we introduce TextEditBench, a comprehensive evaluation benchmark that explicitly focuses on text-centric regions in images. Beyond basic pixel manipulations, our benchmark emphasizes reasoning-intensive editing scenarios that require models to understand physical plausibility, linguistic meaning, and cross-modal dependencies. We further propose a novel evaluation dimension, Semantic Expectation (SE), which measures reasoning ability of model to maintain semantic consistency, contextual coherence, and cross-modal alignment during text editing. Extensive experiments on state-of-the-art editing systems reveal that while current models can follow simple textual instructions, they still struggle with context-dependent reasoning, physical consistency, and layout-aware integration. By focusing evaluation on this long-overlooked yet fundamental capability, TextEditBench establishes a new testing ground for advancing text-guided image editing and reasoning in multimodal generation.

</details>


### [271] [GFLAN: Generative Functional Layouts](https://arxiv.org/abs/2512.16275)
*Mohamed Abouagour,Eleftherios Garyfallidis*

Main category: cs.CV

TL;DR: 本文介绍GFLAN框架解决现有深度学习方法在自动生成平面图时的问题，采用两阶段分解方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在自动生成平面图时难以捕捉建筑推理，无法有效处理拓扑关系、功能约束传播和流通模式等问题。

Method: 提出GFLAN框架，将平面图合成明确分解为拓扑规划和几何实现两阶段。阶段A用带双编码器的卷积架构分配房间质心；阶段B构建异构图，用Transformer增强的图神经网络回归房间边界。

Result: 未提及具体结果。

Conclusion: 未提及明确结论。

Abstract: Automated floor plan generation lies at the intersection of combinatorial search, geometric constraint satisfaction, and functional design requirements -- a confluence that has historically resisted a unified computational treatment. While recent deep learning approaches have improved the state of the art, they often struggle to capture architectural reasoning: the precedence of topological relationships over geometric instantiation, the propagation of functional constraints through adjacency networks, and the emergence of circulation patterns from local connectivity decisions. To address these fundamental challenges, this paper introduces GFLAN, a generative framework that restructures floor plan synthesis through explicit factorization into topological planning and geometric realization. Given a single exterior boundary and a front-door location, our approach departs from direct pixel-to-pixel or wall-tracing generation in favor of a principled two-stage decomposition. Stage A employs a specialized convolutional architecture with dual encoders -- separating invariant spatial context from evolving layout state -- to sequentially allocate room centroids within the building envelope via discrete probability maps over feasible placements. Stage B constructs a heterogeneous graph linking room nodes to boundary vertices, then applies a Transformer-augmented graph neural network (GNN) that jointly regresses room boundaries.

</details>


### [272] [SARMAE: Masked Autoencoder for SAR Representation Learning](https://arxiv.org/abs/2512.16635)
*Danxu Liu,Di Wang,Hebaixu Wang,Haoyang Chen,Wentao Jiang,Yilin Cheng,Haonan Guo,Wei Cui,Jing Zhang*

Main category: cs.CV

TL;DR: 提出SARMAE用于自监督SAR表征学习，构建SAR - 1M数据集，设计SARE和SARC方法，实验显示其在多任务达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有SAR的深度学习受数据稀缺和斑点噪声约束，阻碍细粒度语义表征学习。

Method: 构建百万级SAR数据集SAR - 1M及配对光学图像；设计Speckle - Aware Representation Enhancement(SARE)注入斑点噪声；引入Semantic Anchor Representation Constraint(SARC)利用配对光学先验确保语义一致性。

Result: 在多个SAR数据集的分类、检测和分割任务中达到了最先进的性能。

Conclusion: SARMAE是解决SAR表征学习中数据稀缺和斑点噪声问题的有效方法。

Abstract: Synthetic Aperture Radar (SAR) imagery plays a critical role in all-weather, day-and-night remote sensing applications. However, existing SAR-oriented deep learning is constrained by data scarcity, while the physically grounded speckle noise in SAR imagery further hampers fine-grained semantic representation learning. To address these challenges, we propose SARMAE, a Noise-Aware Masked Autoencoder for self-supervised SAR representation learning. Specifically, we construct SAR-1M, the first million-scale SAR dataset, with additional paired optical images, to enable large-scale pre-training. Building upon this, we design Speckle-Aware Representation Enhancement (SARE), which injects SAR-specific speckle noise into masked autoencoders to facilitate noise-aware and robust representation learning. Furthermore, we introduce Semantic Anchor Representation Constraint (SARC), which leverages paired optical priors to align SAR features and ensure semantic consistency. Extensive experiments across multiple SAR datasets demonstrate that SARMAE achieves state-of-the-art performance on classification, detection, and segmentation tasks. Code and models will be available at https://github.com/MiliLab/SARMAE.

</details>


### [273] [PixelArena: A benchmark for Pixel-Precision Visual Intelligence](https://arxiv.org/abs/2512.16303)
*Feng Liang,Sizhe Cheng,Chenqi Yi*

Main category: cs.CV

TL;DR: 提出用语义分割任务在PixelArena评估多模态大语言模型细粒度图像生成能力，发现Gemini 3 Pro Image有高保真零样本图像生成能力并分析结果。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成基准多关注美学，缺乏对细粒度生成能力的评估。

Method: 在PixelArena使用语义分割任务评估模型细粒度生成能力。

Result: Gemini 3 Pro Image在零样本设置下能高保真生成语义掩码，展示出视觉智能和泛化能力，还进行了结果对比并给出失败案例。

Conclusion: 该研究标志领域进展，为多模态、推理、可解释性和基准测试等未来研究提供见解。

Abstract: Multi-modal large language models that have image output are emerging. Many image generation benchmarks focus on aesthetics instead of fine-grained generation capabilities. In PixelArena, we propose using semantic segmentation tasks to objectively examine their fine-grained generative intelligence with pixel precision. We find the latest Gemini 3 Pro Image has emergent image generation capabilities that generate semantic masks with high fidelity under zero-shot settings, showcasing visual intelligence unseen before and true generalization in new image generation tasks. We further investigate its results, compare them qualitatively and quantitatively with those of other models, and present failure cases. The findings not only signal exciting progress in the field but also provide insights into future research related to multimodality, reasoning, interpretability and benchmarking.

</details>


### [274] [Collaborative Edge-to-Server Inference for Vision-Language Models](https://arxiv.org/abs/2512.16349)
*Soochang Song,Yongjune Kim*

Main category: cs.CV

TL;DR: 提出用于视觉语言模型的边到服务器协作推理框架，减少通信成本并保持推理精度。


<details>
  <summary>Details</summary>
Motivation: 典型部署中调整原图像大小会丢弃细节导致精度下降，需降低通信成本并保持精度。

Method: 设计两阶段框架，第一阶段服务器对全局图像推理并确定感兴趣区域，计算输出令牌最小熵确定是否重传，超出阈值则请求边缘设备发送局部图像，利用两者图像改进推理。

Result: 跨多个视觉语言模型架构的实验表明，该框架显著降低了通信成本并保持了推理精度。

Conclusion: 所提协作推理框架能在减少通信成本的同时维持推理精度。

Abstract: We propose a collaborative edge-to-server inference framework for vision-language models (VLMs) that reduces the communication cost while maintaining inference accuracy. In typical deployments, visual data captured at edge devices (clients) is transmitted to the server for VLM inference. However, resizing the original image (global image) to match the vision encoder's input resolution often discards fine-grained details, leading to accuracy degradation. To overcome this limitation, we design a two-stage framework. In the first stage, the server performs inference on the global image and identifies a region of interest (RoI) using the VLM's internal attention. The min-entropy of the output tokens is then computed as a confidence measure to determine whether retransmission is required. If the min-entropy exceeds a predefined threshold, the server requests the edge device to send a detail-preserved local image of the RoI. The server then refines its inference by jointly leveraging the global and local images. This selective retransmission strategy ensures that only essential visual content is transmitted. Experiments across multiple VLM architectures show that the proposed framework significantly reduces communication cost while maintaining inference accuracy.

</details>


### [275] [Using Gaussian Splats to Create High-Fidelity Facial Geometry and Texture](https://arxiv.org/abs/2512.16397)
*Haodi He,Jihun Yu,Ronald Fedkiw*

Main category: cs.CV

TL;DR: 利用三维神经表示，用高斯溅射法处理人脸图像，重建中性姿态，生成可用三角表面和高分辨率反照率纹理，用于标准图形管线，还展示在文本驱动资产创建管道中的应用。


<details>
  <summary>Details</summary>
Motivation: 构建对未校准人脸图像集合统一且一致的解释，解决相关重建和应用问题。

Method: 利用高斯溅射法，借助分割注释对齐人脸语义区域，软约束高斯到三角表面，使用可重光照高斯模型分离纹理和光照。

Result: 得到可用于标准图形管线的三角表面和高分辨率反照率纹理，实现高视觉保真度高斯溅射，可在不修改图形管线其他方面情况下使用。

Conclusion: 该方法有效，在文本驱动资产创建管道中展示了良好效果，系统灵活性强，可处理不同图像进行鲁棒正则化。

Abstract: We leverage increasingly popular three-dimensional neural representations in order to construct a unified and consistent explanation of a collection of uncalibrated images of the human face. Our approach utilizes Gaussian Splatting, since it is more explicit and thus more amenable to constraints than NeRFs. We leverage segmentation annotations to align the semantic regions of the face, facilitating the reconstruction of a neutral pose from only 11 images (as opposed to requiring a long video). We soft constrain the Gaussians to an underlying triangulated surface in order to provide a more structured Gaussian Splat reconstruction, which in turn informs subsequent perturbations to increase the accuracy of the underlying triangulated surface. The resulting triangulated surface can then be used in a standard graphics pipeline. In addition, and perhaps most impactful, we show how accurate geometry enables the Gaussian Splats to be transformed into texture space where they can be treated as a view-dependent neural texture. This allows one to use high visual fidelity Gaussian Splatting on any asset in a scene without the need to modify any other asset or any other aspect (geometry, lighting, renderer, etc.) of the graphics pipeline. We utilize a relightable Gaussian model to disentangle texture from lighting in order to obtain a delit high-resolution albedo texture that is also readily usable in a standard graphics pipeline. The flexibility of our system allows for training with disparate images, even with incompatible lighting, facilitating robust regularization. Finally, we demonstrate the efficacy of our approach by illustrating its use in a text-driven asset creation pipeline.

</details>


### [276] [Pixel Seal: Adversarial-only training for invisible image and video watermarking](https://arxiv.org/abs/2512.16874)
*Tomáš Souček,Pierre Fernandez,Hady Elsahar,Sylvestre-Alvise Rebuffi,Valeriu Lacatusu,Tuan Tran,Tom Sander,Alexandre Mourachko*

Main category: cs.CV

TL;DR: 本文提出Pixel Seal图像和视频水印方案，解决现有方法问题，经评估效果优于现有技术，还能适配视频。


<details>
  <summary>Details</summary>
Motivation: 现有隐形水印模型训练困难，难以平衡鲁棒性和不可感知性。

Method: 提出仅对抗训练范式，消除不可靠的逐像素不可感知损失；引入三阶段训练计划，解耦鲁棒性和不可感知性；通过高分辨率适配解决分辨率差距。

Result: 在不同图像类型和多种变换下评估，Pixel Seal的鲁棒性和不可感知性明显优于现有技术。

Conclusion: Pixel Seal是现实世界图像和视频可靠溯源的实用且可扩展解决方案。

Abstract: Invisible watermarking is essential for tracing the provenance of digital content. However, training state-of-the-art models remains notoriously difficult, with current approaches often struggling to balance robustness against true imperceptibility. This work introduces Pixel Seal, which sets a new state-of-the-art for image and video watermarking. We first identify three fundamental issues of existing methods: (i) the reliance on proxy perceptual losses such as MSE and LPIPS that fail to mimic human perception and result in visible watermark artifacts; (ii) the optimization instability caused by conflicting objectives, which necessitates exhaustive hyperparameter tuning; and (iii) reduced robustness and imperceptibility of watermarks when scaling models to high-resolution images and videos. To overcome these issues, we first propose an adversarial-only training paradigm that eliminates unreliable pixel-wise imperceptibility losses. Second, we introduce a three-stage training schedule that stabilizes convergence by decoupling robustness and imperceptibility. Third, we address the resolution gap via high-resolution adaptation, employing JND-based attenuation and training-time inference simulation to eliminate upscaling artifacts. We thoroughly evaluate the robustness and imperceptibility of Pixel Seal on different image types and across a wide range of transformations, and show clear improvements over the state-of-the-art. We finally demonstrate that the model efficiently adapts to video via temporal watermark pooling, positioning Pixel Seal as a practical and scalable solution for reliable provenance in real-world image and video settings.

</details>


### [277] [SFTok: Bridging the Performance Gap in Discrete Tokenizers](https://arxiv.org/abs/2512.16910)
*Qihang Rao,Borui Zhang,Wenzhao Zheng,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 提出离散分词器SFTok，在高压缩率下实现图像重建和类到图像生成的先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有离散分词器落后于连续分词器，限制其在多模态系统中的应用。

Method: 提出SFTok，采用多步迭代机制、自强制引导视觉重建和去偏拟合训练策略。

Result: 在ImageNet上以64个标记的高压缩率实现rFID = 1.21的重建质量，在类到图像生成任务中gFID = 2.29。

Conclusion: SFTok解决了多步过程中的训练推理不一致问题，显著提高图像重建质量。

Abstract: Recent advances in multimodal models highlight the pivotal role of image tokenization in high-resolution image generation. By compressing images into compact latent representations, tokenizers enable generative models to operate in lower-dimensional spaces, thereby improving computational efficiency and reducing complexity. Discrete tokenizers naturally align with the autoregressive paradigm but still lag behind continuous ones, limiting their adoption in multimodal systems. To address this, we propose \textbf{SFTok}, a discrete tokenizer that incorporates a multi-step iterative mechanism for precise reconstruction. By integrating \textbf{self-forcing guided visual reconstruction} and \textbf{debias-and-fitting training strategy}, SFTok resolves the training-inference inconsistency in multi-step process, significantly enhancing image reconstruction quality. At a high compression rate of only 64 tokens per image, SFTok achieves state-of-the-art reconstruction quality on ImageNet (rFID = 1.21) and demonstrates exceptional performance in class-to-image generation tasks (gFID = 2.29).

</details>


### [278] [Guiding Perception-Reasoning Closer to Human in Blind Image Quality Assessment](https://arxiv.org/abs/2512.16484)
*Yuan Li,Yahan Yu,Youyuan Lin,Yong-Hao Yang,Chenhui Chu,Shin'ya Nishida*

Main category: cs.CV

TL;DR: 本文探讨模型在盲图像质量评估中获得类人、自洽推理能力的方法，通过收集人类评估数据、采用强化学习，在指标上表现佳，显示出迈向类人可解释推理的进步。


<details>
  <summary>Details</summary>
Motivation: 研究模型如何在盲图像质量评估中获得类人且自洽的推理能力。

Method: 收集人类评估数据，采用强化学习，以人类标注为奖励信号引导模型，设计奖励促使模型从自我生成描述中推断图像质量。

Result: 在通用指标上得分预测表现与现有先进系统相当，以ROUGE - 1衡量人类与模型的一致性，模型在超1000个人类标注样本上ROUGE - 1得分达0.512高于基线。

Conclusion: 该方法取得了较好效果，是盲图像质量评估中迈向类人可解释推理的一步。

Abstract: Humans assess image quality through a perception-reasoning cascade, integrating sensory cues with implicit reasoning to form self-consistent judgments. In this work, we investigate how a model can acquire both human-like and self-consistent reasoning capability for blind image quality assessment (BIQA). We first collect human evaluation data that capture several aspects of human perception-reasoning pipeline. Then, we adopt reinforcement learning, using human annotations as reward signals to guide the model toward human-like perception and reasoning. To enable the model to internalize self-consistent reasoning capability, we design a reward that drives the model to infer the image quality purely from self-generated descriptions. Empirically, our approach achieves score prediction performance comparable to state-of-the-art BIQA systems under general metrics, including Pearson and Spearman correlation coefficients. In addition to the rating score, we assess human-model alignment using ROUGE-1 to measure the similarity between model-generated and human perception-reasoning chains. On over 1,000 human-annotated samples, our model reaches a ROUGE-1 score of 0.512 (cf. 0.443 for baseline), indicating substantial coverage of human explanations and marking a step toward human-like interpretable reasoning in BIQA.

</details>


### [279] [Smile on the Face, Sadness in the Eyes: Bridging the Emotion Gap with a Multimodal Dataset of Eye and Facial Behaviors](https://arxiv.org/abs/2512.16485)
*Kejun Liu,Yuanyuan Liu,Lin Wei,Chang Tang,Yibing Zhan,Zijing Chen,Zhe Chen*

Main category: cs.CV

TL;DR: 文章引入眼动行为作为情感线索，构建EMER数据集，设计EMERT模型，实验表明其性能优越，强调眼动行为对情感识别的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前情感识别领域过度依赖面部表情识别，但面部表情常不能反映真实情感，为理解和缩小面部表情识别与情感识别的差距。

Method: 利用自发情绪诱导范式收集包含眼动数据和面部表情视频的数据，分别标注多模态情感识别和面部表情识别的多视角情感标签；设计EMERT模型，利用模态对抗特征解耦和多任务Transformer。

Result: EMERT在实验中大幅超越其他先进的多模态方法。

Conclusion: 强调眼动行为在情感识别中的重要性，有助于缩小面部表情识别与情感识别的差距，提升情感识别性能，且数据集和模型将公开。

Abstract: Emotion Recognition (ER) is the process of analyzing and identifying human emotions from sensing data. Currently, the field heavily relies on facial expression recognition (FER) because visual channel conveys rich emotional cues. However, facial expressions are often used as social tools rather than manifestations of genuine inner emotions. To understand and bridge this gap between FER and ER, we introduce eye behaviors as an important emotional cue and construct an Eye-behavior-aided Multimodal Emotion Recognition (EMER) dataset. To collect data with genuine emotions, spontaneous emotion induction paradigm is exploited with stimulus material, during which non-invasive eye behavior data, like eye movement sequences and eye fixation maps, is captured together with facial expression videos. To better illustrate the gap between ER and FER, multi-view emotion labels for mutimodal ER and FER are separately annotated. Furthermore, based on the new dataset, we design a simple yet effective Eye-behavior-aided MER Transformer (EMERT) that enhances ER by bridging the emotion gap. EMERT leverages modality-adversarial feature decoupling and a multitask Transformer to model eye behaviors as a strong complement to facial expressions. In the experiment, we introduce seven multimodal benchmark protocols for a variety of comprehensive evaluations of the EMER dataset. The results show that the EMERT outperforms other state-of-the-art multimodal methods by a great margin, revealing the importance of modeling eye behaviors for robust ER. To sum up, we provide a comprehensive analysis of the importance of eye behaviors in ER, advancing the study on addressing the gap between FER and ER for more robust ER performance. Our EMER dataset and the trained EMERT models will be publicly available at https://github.com/kejun1/EMER.

</details>


### [280] [PoseMoE: Mixture-of-Experts Network for Monocular 3D Human Pose Estimation](https://arxiv.org/abs/2512.16494)
*Mengyuan Liu,Jiajie Liu,Jinyan Zhang,Wenhao Li,Junsong Yuan*

Main category: cs.CV

TL;DR: 现有基于提升的单目3D人体姿态估计方法存在深度不确定性影响精度问题，本文提出PoseMoE网络解决该问题，实验显示其性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 基于提升的单目3D人体姿态估计方法将检测到的2D姿态和未知深度在纠缠特征空间编码，引入深度不确定性，限制了整体估计精度。

Method: 提出PoseMoE网络，包含专家混合网络细化2D姿态特征和学习深度特征，解耦2D姿态和深度的特征编码；提出跨专家知识聚合模块聚合时空上下文信息，通过2D姿态和深度的双向映射增强特征。

Result: 在Human3.6M、MPI - INF - 3DHP和3DPW三个常用数据集上，PoseMoE网络性能优于传统基于提升的方法。

Conclusion: 提出的PoseMoE网络能有效解决现有基于提升方法的局限性，提高单目3D人体姿态估计的精度。

Abstract: The lifting-based methods have dominated monocular 3D human pose estimation by leveraging detected 2D poses as intermediate representations. The 2D component of the final 3D human pose benefits from the detected 2D poses, whereas its depth counterpart must be estimated from scratch. The lifting-based methods encode the detected 2D pose and unknown depth in an entangled feature space, explicitly introducing depth uncertainty to the detected 2D pose, thereby limiting overall estimation accuracy. This work reveals that the depth representation is pivotal for the estimation process. Specifically, when depth is in an initial, completely unknown state, jointly encoding depth features with 2D pose features is detrimental to the estimation process. In contrast, when depth is initially refined to a more dependable state via network-based estimation, encoding it together with 2D pose information is beneficial. To address this limitation, we present a Mixture-of-Experts network for monocular 3D pose estimation named PoseMoE. Our approach introduces: (1) A mixture-of-experts network where specialized expert modules refine the well-detected 2D pose features and learn the depth features. This mixture-of-experts design disentangles the feature encoding process for 2D pose and depth, therefore reducing the explicit influence of uncertain depth features on 2D pose features. (2) A cross-expert knowledge aggregation module is proposed to aggregate cross-expert spatio-temporal contextual information. This step enhances features through bidirectional mapping between 2D pose and depth. Extensive experiments show that our proposed PoseMoE outperforms the conventional lifting-based methods on three widely used datasets: Human3.6M, MPI-INF-3DHP, and 3DPW.

</details>


### [281] [TTP: Test-Time Padding for Adversarial Detection and Robust Adaptation on Vision-Language Models](https://arxiv.org/abs/2512.16523)
*Zhiwei Li,Yitian Pang,Weining Wang,Zhenan Sun,Qi Li*

Main category: cs.CV

TL;DR: 提出轻量级防御框架TTP，在推理时进行对抗检测和针对性调整，实验表明其超越现有方法，提升对抗鲁棒性且不降低干净准确率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（如CLIP）易受对抗扰动影响，训练时防御需标注数据和重新训练，测试时策略无法可靠区分干净和对抗输入。

Method: 提出Test - Time Padding (TTP)框架，通过空间填充前后CLIP特征嵌入的余弦相似度变化检测对抗输入，对检测到的对抗样本采用可训练填充恢复注意力模式并结合相似度感知集成策略预测，对干净输入默认不变或结合现有测试时适应技术。

Result: 在不同CLIP骨干网络和细粒度基准上的综合实验显示，TTP始终超越现有测试时防御方法，大幅提升对抗鲁棒性且不降低干净准确率。

Conclusion: TTP是一种有效的测试时防御框架，能提升视觉语言模型的对抗鲁棒性。

Abstract: Vision-Language Models (VLMs), such as CLIP, have achieved impressive zero-shot recognition performance but remain highly susceptible to adversarial perturbations, posing significant risks in safety-critical scenarios. Previous training-time defenses rely on adversarial fine-tuning, which requires labeled data and costly retraining, while existing test-time strategies fail to reliably distinguish between clean and adversarial inputs, thereby preventing both adversarial robustness and clean accuracy from reaching their optimum. To address these limitations, we propose Test-Time Padding (TTP), a lightweight defense framework that performs adversarial detection followed by targeted adaptation at inference. TTP identifies adversarial inputs via the cosine similarity shift between CLIP feature embeddings computed before and after spatial padding, yielding a universal threshold for reliable detection across architectures and datasets. For detected adversarial cases, TTP employs trainable padding to restore disrupted attention patterns, coupled with a similarity-aware ensemble strategy for a more robust final prediction. For clean inputs, TTP leaves them unchanged by default or optionally integrates existing test-time adaptation techniques for further accuracy gains. Comprehensive experiments on diverse CLIP backbones and fine-grained benchmarks show that TTP consistently surpasses state-of-the-art test-time defenses, delivering substantial improvements in adversarial robustness without compromising clean accuracy. The code for this paper will be released soon.

</details>


### [282] [Yuan-TecSwin: A text conditioned Diffusion model with Swin-transformer blocks](https://arxiv.org/abs/2512.16586)
*Shaohua Wu,Tong Yu,Shenling Wang,Xudong Zhao*

Main category: cs.CV

TL;DR: 提出含Swin - transformer的文本条件扩散模型Yuan - TecSwin，提升非局部建模能力，推理性能提升10%，在ImageNet上取得SOTA的FID分数，生成图像难与手绘区分。


<details>
  <summary>Details</summary>
Motivation: CNN卷积操作的局部性限制模型理解长距离语义信息的能力，需改进。

Method: 用Swin - transformer块替代编码器和解码器中的CNN块，选择合适文本编码器、有效利用文本嵌入、精心设计文本条件融合，在不同扩散阶段使用自适应时间步搜索。

Result: 推理性能提升10%，在ImageNet生成基准上达到1.37的SOTA的FID分数，人类难区分模型生成图像和手绘图像。

Conclusion: Yuan - TecSwin能有效提高非局部建模和文本图像对齐能力，提升推理性能。

Abstract: Diffusion models have shown remarkable capacity in image synthesis based on their U-shaped architecture and convolutional neural networks (CNN) as basic blocks. The locality of the convolution operation in CNN may limit the model's ability to understand long-range semantic information. To address this issue, we propose Yuan-TecSwin, a text-conditioned diffusion model with Swin-transformer in this work. The Swin-transformer blocks take the place of CNN blocks in the encoder and decoder, to improve the non-local modeling ability in feature extraction and image restoration. The text-image alignment is improved with a well-chosen text encoder, effective utilization of text embedding, and careful design in the incorporation of text condition. Using an adapted time step to search in different diffusion stages, inference performance is further improved by 10%. Yuan-TecSwin achieves the state-of-the-art FID score of 1.37 on ImageNet generation benchmark, without any additional models at different denoising stages. In a side-by-side comparison, we find it difficult for human interviewees to tell the model-generated images from the human-painted ones.

</details>


### [283] [Few-Shot Fingerprinting Subject Re-Identification in 3D-MRI and 2D-X-Ray](https://arxiv.org/abs/2512.16685)
*Gonçalo Gaspar Alves,Shekoufeh Gorgi Zadeh,Andreas Husch,Ben Bausch*

Main category: cs.CV

TL;DR: 本文通过主题指纹识别解决开源数据集合并时的数据泄露问题，用ResNet - 50模型评估了少样本指纹识别，在多个数据集和场景中取得高MR@K分数。


<details>
  <summary>Details</summary>
Motivation: 解决开源数据集合并时因同一主体出现在多个集合导致的数据泄露及模型性能虚高问题。

Method: 采用主题指纹识别，将主体图像映射到潜在空间的不同区域实现主体再识别，使用带三元组边界损失训练的ResNet - 50模型，在不同场景下对3D MRI和2D X射线数据进行少样本指纹识别评估。

Result: 模型在ChestXray - 14和BraTS - 2021等数据集的不同任务中取得高Mean - Recall - @ - K分数，如ChestXray - 14的20-way 1-shot为99.10%等。

Conclusion: 主题指纹识别和所采用的模型能有效解决开源数据集合并时的数据泄露问题，评估表现良好。

Abstract: Combining open-source datasets can introduce data leakage if the same subject appears in multiple sets, leading to inflated model performance. To address this, we explore subject fingerprinting, mapping all images of a subject to a distinct region in latent space, to enable subject re-identification via similarity matching. Using a ResNet-50 trained with triplet margin loss, we evaluate few-shot fingerprinting on 3D MRI and 2D X-ray data in both standard (20-way 1-shot) and challenging (1000-way 1-shot) scenarios. The model achieves high Mean- Recall-@-K scores: 99.10% (20-way 1-shot) and 90.06% (500-way 5-shot) on ChestXray-14; 99.20% (20-way 1-shot) and 98.86% (100-way 3-shot) on BraTS- 2021.

</details>


### [284] [TreeNet: A Light Weight Model for Low Bitrate Image Compression](https://arxiv.org/abs/2512.16743)
*Mahadev Prasad Panda,Purnachandra Rao Makkena,Srivatsa Prativadibhayankaram,Siegfried Fößel,André Kaup*

Main category: cs.CV

TL;DR: 提出低复杂度图像压缩模型TreeNet，用二叉树结构和注意力特征融合机制，在低比特率下优于JPEG AI且降低复杂度。


<details>
  <summary>Details</summary>
Motivation: 降低基于学习的图像压缩技术的计算复杂度，以促进其广泛应用。

Method: 提出TreeNet，采用二叉树结构的编解码器架构，使用注意力特征融合机制整合多分支特征。

Result: 在三个基准数据集上评估，低比特率下BD - rate比JPEG AI平均提高4.83%，模型复杂度降低87.82%。

Conclusion: TreeNet能有效降低复杂度并提高性能，消融研究为重建因素提供深入见解。

Abstract: Reducing computational complexity remains a critical challenge for the widespread adoption of learning-based image compression techniques. In this work, we propose TreeNet, a novel low-complexity image compression model that leverages a binary tree-structured encoder-decoder architecture to achieve efficient representation and reconstruction. We employ attentional feature fusion mechanism to effectively integrate features from multiple branches. We evaluate TreeNet on three widely used benchmark datasets and compare its performance against competing methods including JPEG AI, a recent standard in learning-based image compression. At low bitrates, TreeNet achieves an average improvement of 4.83% in BD-rate over JPEG AI, while reducing model complexity by 87.82%. Furthermore, we conduct extensive ablation studies to investigate the influence of various latent representations within TreeNet, offering deeper insights into the factors contributing to reconstruction.

</details>


### [285] [KineST: A Kinematics-guided Spatiotemporal State Space Model for Human Motion Tracking from Sparse Signals](https://arxiv.org/abs/2512.16791)
*Shuting Zhao,Zeyu Xiao,Xinrong Chen*

Main category: cs.CV

TL;DR: 提出KineST模型用于AR/VR全身动作跟踪，解决稀疏信号下姿态重建难题，实验显示其高效准确。


<details>
  <summary>Details</summary>
Motivation: 现有基于AR/VR头戴设备稀疏信号的全身姿态重建方法存在计算成本高、难平衡精度、时间连贯性和效率等问题。

Method: 提出KineST模型，采用运动学引导双向扫描和混合时空表示学习方法，引入几何角速度损失。

Result: 实验表明KineST在轻量级框架下精度和时间一致性表现出色。

Conclusion: KineST是解决AR/VR头戴设备稀疏信号下全身姿态重建问题的有效方法。

Abstract: Full-body motion tracking plays an essential role in AR/VR applications, bridging physical and virtual interactions. However, it is challenging to reconstruct realistic and diverse full-body poses based on sparse signals obtained by head-mounted displays, which are the main devices in AR/VR scenarios. Existing methods for pose reconstruction often incur high computational costs or rely on separately modeling spatial and temporal dependencies, making it difficult to balance accuracy, temporal coherence, and efficiency. To address this problem, we propose KineST, a novel kinematics-guided state space model, which effectively extracts spatiotemporal dependencies while integrating local and global pose perception. The innovation comes from two core ideas. Firstly, in order to better capture intricate joint relationships, the scanning strategy within the State Space Duality framework is reformulated into kinematics-guided bidirectional scanning, which embeds kinematic priors. Secondly, a mixed spatiotemporal representation learning approach is employed to tightly couple spatial and temporal contexts, balancing accuracy and smoothness. Additionally, a geometric angular velocity loss is introduced to impose physically meaningful constraints on rotational variations for further improving motion stability. Extensive experiments demonstrate that KineST has superior performance in both accuracy and temporal consistency within a lightweight framework. Project page: https://kaka-1314.github.io/KineST/

</details>


### [286] [Next-Generation License Plate Detection and Recognition System using YOLOv8](https://arxiv.org/abs/2512.16826)
*Arslan Amin,Rafia Mumtaz,Muhammad Jawad Bashir,Syed Mohammad Hassan Zaidi*

Main category: cs.CV

TL;DR: 研究YOLOv8变体在车牌识别和字符识别任务中的表现，提出优化管道，为智能交通系统边缘设备部署奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有方法在不同环境下实时准确进行车牌检测和识别存在困难，需提升智能交通系统相关性能。

Method: 使用两个不同数据集训练和评估YOLOv8变体，引入自定义字符排序方法，构建利用YOLOv8 Nano进行车牌识别、YOLOv8 Small进行字符识别的优化管道。

Result: YOLOv8 Nano在车牌识别任务中精度0.964、mAP50为0.918；YOLOv8 Small在字符识别任务中精度0.92、mAP50为0.91。

Conclusion: 提出的配置兼顾计算效率和高准确性，为智能交通系统边缘设备的实际部署提供有力支持，推动城市基础设施智能化发展。

Abstract: In the evolving landscape of traffic management and vehicle surveillance, efficient license plate detection and recognition are indispensable. Historically, many methodologies have tackled this challenge, but consistent real-time accuracy, especially in diverse environments, remains elusive. This study examines the performance of YOLOv8 variants on License Plate Recognition (LPR) and Character Recognition tasks, crucial for advancing Intelligent Transportation Systems. Two distinct datasets were employed for training and evaluation, yielding notable findings. The YOLOv8 Nano variant demonstrated a precision of 0.964 and mAP50 of 0.918 on the LPR task, while the YOLOv8 Small variant exhibited a precision of 0.92 and mAP50 of 0.91 on the Character Recognition task. A custom method for character sequencing was introduced, effectively sequencing the detected characters based on their x-axis positions. An optimized pipeline, utilizing YOLOv8 Nano for LPR and YOLOv8 Small for Character Recognition, is proposed. This configuration not only maintains computational efficiency but also ensures high accuracy, establishing a robust foundation for future real-world deployments on edge devices within Intelligent Transportation Systems. This effort marks a significant stride towards the development of smarter and more efficient urban infrastructures.

</details>


### [287] [OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction](https://arxiv.org/abs/2512.16842)
*Yuxin Ray Song,Jinzhou Li,Rao Fu,Devin Murphy,Kaichen Zhou,Rishi Shiv,Yaqi Li,Haoyu Xiong,Crystal Elaine Owens,Yilun Du,Yiyue Luo,Xianyi Cheng,Antonio Torralba,Wojciech Matusik,Paul Pu Liang*

Main category: cs.CV

TL;DR: 提出首个野外第一人称全手触觉数据集OpenTouch，介绍相关基准，展示触觉信号作用并推进多模态感知等研究。


<details>
  <summary>Details</summary>
Motivation: 人类自我中心感知对手部接触缺乏认知，缺少野外可穿戴触觉传感器和手部触摸与第一人称视频对齐的数据集，需弥合视觉感知和物理交互差距。

Method: 提出OpenTouch数据集，包含同步视频触摸姿势数据和带注释的剪辑，介绍相关基准。

Result: 触觉信号有助于理解抓握、加强跨模态对齐，可从野外视频查询中可靠检索。

Conclusion: 发布标注的视觉触摸姿势数据集和基准，有望推进多模态自我中心感知、具身学习和接触密集型机器人操作研究。

Abstract: The human hand is our primary interface to the physical world, yet egocentric perception rarely knows when, where, or how forcefully it makes contact. Robust wearable tactile sensors are scarce, and no existing in-the-wild datasets align first-person video with full-hand touch. To bridge the gap between visual perception and physical interaction, we present OpenTouch, the first in-the-wild egocentric full-hand tactile dataset, containing 5.1 hours of synchronized video-touch-pose data and 2,900 curated clips with detailed text annotations. Using OpenTouch, we introduce retrieval and classification benchmarks that probe how touch grounds perception and action. We show that tactile signals provide a compact yet powerful cue for grasp understanding, strengthen cross-modal alignment, and can be reliably retrieved from in-the-wild video queries. By releasing this annotated vision-touch-pose dataset and benchmark, we aim to advance multimodal egocentric perception, embodied learning, and contact-rich robotic manipulation.

</details>


### [288] [GenEval 2: Addressing Benchmark Drift in Text-to-Image Evaluation](https://arxiv.org/abs/2512.16853)
*Amita Kamath,Kai-Wei Chang,Ranjay Krishna,Luke Zettlemoyer,Yushi Hu,Marjan Ghazvininejad*

Main category: cs.CV

TL;DR: 自动化文本到图像（T2I）模型评估具有挑战性，易出现基准漂移，如GenEval 已偏离人类判断，为此提出新基准GenEval 2及评估方法Soft - TIFA，强调持续审计和改进基准的重要性。


<details>
  <summary>Details</summary>
Motivation: 解决T2I模型自动化评估中基准漂移问题，填补现有基准如GenEval的不足。

Method: 引入新基准GenEval 2 ，覆盖更多基本视觉概念和具有更高的组合性；提出评估方法Soft - TIFA，结合视觉原语判断。

Result: 新基准GenEval 2对当前模型更具挑战性，Soft - TIFA评估方法与人类判断更一致且不易漂移。

Conclusion: 虽GenEval 2有望长期作为强基准，但避免基准漂移不能保证，强调对T2I及相关自动模型评估基准持续审计和改进的重要性。

Abstract: Automating Text-to-Image (T2I) model evaluation is challenging; a judge model must be used to score correctness, and test prompts must be selected to be challenging for current T2I models but not the judge. We argue that satisfying these constraints can lead to benchmark drift over time, where the static benchmark judges fail to keep up with newer model capabilities. We show that benchmark drift is a significant problem for GenEval, one of the most popular T2I benchmarks. Although GenEval was well-aligned with human judgment at the time of its release, it has drifted far from human judgment over time -- resulting in an absolute error of as much as 17.7% for current models. This level of drift strongly suggests that GenEval has been saturated for some time, as we verify via a large-scale human study. To help fill this benchmarking gap, we introduce a new benchmark, GenEval 2, with improved coverage of primitive visual concepts and higher degrees of compositionality, which we show is more challenging for current models. We also introduce Soft-TIFA, an evaluation method for GenEval 2 that combines judgments for visual primitives, which we show is more well-aligned with human judgment and argue is less likely to drift from human-alignment over time (as compared to more holistic judges such as VQAScore). Although we hope GenEval 2 will provide a strong benchmark for many years, avoiding benchmark drift is far from guaranteed and our work, more generally, highlights the importance of continual audits and improvement for T2I and related automated model evaluation benchmarks.

</details>


### [289] [Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos](https://arxiv.org/abs/2512.16907)
*Mingfei Chen,Yifan Wang,Zhengqin Li,Homanga Bharadhwaj,Yujin Chen,Chuan Qin,Ziyi Kou,Yuan Tian,Eric Whitmire,Rajinder Sodhi,Hrvoje Benko,Eli Shlizerman,Yue Liu*

Main category: cs.CV

TL;DR: 提出EgoMAN数据集和EgoMAN模型用于3D手部轨迹预测，能生成准确且有泛化性的轨迹。


<details>
  <summary>Details</summary>
Motivation: 过往3D手部轨迹预测工作受限于解耦运动与语义监督的数据集，以及推理和动作联系弱的模型。

Method: 提出EgoMAN数据集，包含219K 6DoF轨迹和3M结构化问答对；引入EgoMAN模型，通过轨迹令牌接口连接视觉语言推理和运动生成，并逐步训练使推理与运动动力学对齐。

Result: 能生成准确且有场景泛化性的阶段感知轨迹。

Conclusion: 所提方法可有效解决现有3D手部轨迹预测工作的局限。

Abstract: Prior works on 3D hand trajectory prediction are constrained by datasets that decouple motion from semantic supervision and by models that weakly link reasoning and action. To address these, we first present the EgoMAN dataset, a large-scale egocentric dataset for interaction stage-aware 3D hand trajectory prediction with 219K 6DoF trajectories and 3M structured QA pairs for semantic, spatial, and motion reasoning. We then introduce the EgoMAN model, a reasoning-to-motion framework that links vision-language reasoning and motion generation via a trajectory-token interface. Trained progressively to align reasoning with motion dynamics, our approach yields accurate and stage-aware trajectories with generalization across real-world scenes.

</details>


### [290] [DVGT: Driving Visual Geometry Transformer](https://arxiv.org/abs/2512.16919)
*Sicheng Zuo,Zixun Xie,Wenzhao Zheng,Shaoqing Xu,Fang Li,Shengyin Jiang,Long Chen,Zhi-Xin Yang,Jiwen Lu*

Main category: cs.CV

TL;DR: 提出驾驶视觉几何变换器DVGT，从多视图视觉输入重建全局密集3D点图，在多种驾驶数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 缺乏适应不同场景和相机配置的驾驶目标密集几何感知模型。

Method: 用DINO骨干提取图像特征，通过交替的视图内局部注意力、跨视图空间注意力和跨帧时间注意力推断图像间几何关系，用多头解码全局点图和各帧的自我姿态。

Result: 在包括nuScenes、OpenScene等多个驾驶数据集上训练，在各种场景下显著优于现有模型。

Conclusion: DVGT无需明确3D几何先验，可灵活处理任意相机配置，直接从图像序列预测度量尺度几何，无需与外部传感器进行后对齐。

Abstract: Perceiving and reconstructing 3D scene geometry from visual inputs is crucial for autonomous driving. However, there still lacks a driving-targeted dense geometry perception model that can adapt to different scenarios and camera configurations. To bridge this gap, we propose a Driving Visual Geometry Transformer (DVGT), which reconstructs a global dense 3D point map from a sequence of unposed multi-view visual inputs. We first extract visual features for each image using a DINO backbone, and employ alternating intra-view local attention, cross-view spatial attention, and cross-frame temporal attention to infer geometric relations across images. We then use multiple heads to decode a global point map in the ego coordinate of the first frame and the ego poses for each frame. Unlike conventional methods that rely on precise camera parameters, DVGT is free of explicit 3D geometric priors, enabling flexible processing of arbitrary camera configurations. DVGT directly predicts metric-scaled geometry from image sequences, eliminating the need for post-alignment with external sensors. Trained on a large mixture of driving datasets including nuScenes, OpenScene, Waymo, KITTI, and DDAD, DVGT significantly outperforms existing models on various scenarios. Code is available at https://github.com/wzzheng/DVGT.

</details>


### [291] [EasyV2V: A High-quality Instruction-based Video Editing Framework](https://arxiv.org/abs/2512.16920)
*Jinjie Mai,Chaoyang Wang,Guocheng Gordon Qian,Willi Menapace,Sergey Tulyakov,Bernard Ghanem,Peter Wonka,Ashkan Mirzaei*

Main category: cs.CV

TL;DR: 提出EasyV2V进行基于指令的视频编辑，研究数据、架构和控制设计空间，实现灵活输入和SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 图像编辑发展迅速，但视频编辑面临一致性、控制和泛化的挑战，研究较少。

Method: 数据上构建多样视频对和添加过渡监督；模型通过简单序列拼接和轻量级LoRA微调；控制用单一掩码机制统一时空控制。

Result: EasyV2V能处理灵活输入，实现了先进的视频编辑结果，超越同期和商业系统。

Conclusion: EasyV2V是一个简单有效的基于指令的视频编辑框架。

Abstract: While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/

</details>


### [292] [Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification](https://arxiv.org/abs/2512.16921)
*Qihao Liu,Chengzhi Mao,Yaojie Liu,Alan Yuille,Wen-Sheng Chu*

Main category: cs.CV

TL;DR: 提出AuditDM框架用于发现和纠正多模态大语言模型失败模式，应用于多个模型有显著效果，表明针对性模型审计是有效提升途径。


<details>
  <summary>Details</summary>
Motivation: 传统多模态大语言模型评估方法缺乏可解释性，无法充分揭示模型能力差距。

Method: 通过强化学习微调多模态大语言模型作为审计器，生成挑战性问题和反事实图像以最大化目标模型间分歧。

Result: 在Gemma - 3和PaliGemma - 2等模型发现超20种不同失败类型，微调后模型在16个基准测试中提升，3B模型能超越28B模型。

Conclusion: 数据扩展收益递减时，针对性模型审计是模型诊断和改进的有效途径。

Abstract: Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [293] [Physics-Informed Neural Networks for Modeling the Martian Induced Magnetosphere](https://arxiv.org/abs/2512.16175)
*Jiawei Gao,Chuanfei Dong,Chi Zhang,Yilan Qin,Simin Shekarpaz,Xinmin Li,Liang Wang,Hongyang Zhou,Abigail Tadlock*

Main category: astro-ph.EP

TL;DR: 本文首次用结合MAVEN观测和物理定律的物理信息神经网络（PINN）构建火星感应磁层磁场的数据驱动模型，展示了PINN重建复杂磁场结构的能力。


<details>
  <summary>Details</summary>
Motivation: 现有火星感应磁层全球模型依赖计算密集的基于物理的模拟，需新方法来理解火星磁场环境及其对太阳风的响应。

Method: 使用物理信息神经网络（PINN）结合MAVEN观测和物理定律构建数据驱动模型，并在不同太阳风条件下训练。

Result: 模型准确重建三维磁场结构及其对太阳风驱动的变化，确定磁场结构对太阳风参数的关键依赖。

Conclusion: PINN有能力重建火星感应磁层的复杂磁场结构，为太阳风 - 火星相互作用研究提供了有前景的工具。

Abstract: Understanding the magnetic field environment around Mars and its response to upstream solar wind conditions provide key insights into the processes driving atmospheric ion escape. To date, global models of Martian induced magnetosphere have been exclusively physics-based, relying on computationally intensive simulations. For the first time, we develop a data-driven model of the Martian induced magnetospheric magnetic field using Physics-Informed Neural Network (PINN) combined with MAVEN observations and physical laws. Trained under varying solar wind conditions, including B_IMF, P_SW, and θ_cone, the data-driven model accurately reconstructs the three-dimensional magnetic field configuration and its variability in response to upstream solar wind drivers. Based on the PINN results, we identify key dependencies of magnetic field configuration on solar wind parameters, including the hemispheric asymmetries of the draped field line strength in the Mars-Solar-Electric coordinates. These findings demonstrate the capability of PINNs to reconstruct complex magnetic field structures in the Martian induced magnetosphere, thereby offering a promising tool for advancing studies of solar wind-Mars interactions.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [294] [Coordinated Anti-Jamming Resilience in Swarm Networks via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2512.16813)
*Bahman Abolhassani,Tugba Erpek,Kemal Davaslioglu,Yalin E. Sagduyu,Sastry Kompella*

Main category: cs.NI

TL;DR: 本文提出基于QMIX算法的多智能体强化学习框架，提升机器人群体通信在反应式干扰下的弹性，仿真显示该算法有效。


<details>
  <summary>Details</summary>
Motivation: 反应式干扰器对机器人群体网络构成严重安全威胁，传统对策对其效果不佳。

Method: 提出基于QMIX算法的多智能体强化学习框架，各智能体联合选择发射频率和功率，QMIX学习集中但可分解的动作价值函数。

Result: QMIX快速收敛到接近理想辅助界限的合作策略，比基线方法有更高吞吐量和更低干扰发生率。

Conclusion: 多智能体强化学习在有竞争的环境中保障自主群体安全方面是有效的。

Abstract: Reactive jammers pose a severe security threat to robotic-swarm networks by selectively disrupting inter-agent communications and undermining formation integrity and mission success. Conventional countermeasures such as fixed power control or static channel hopping are largely ineffective against such adaptive adversaries. This paper presents a multi-agent reinforcement learning (MARL) framework based on the QMIX algorithm to improve the resilience of swarm communications under reactive jamming. We consider a network of multiple transmitter-receiver pairs sharing channels while a reactive jammer with Markovian threshold dynamics senses aggregate power and reacts accordingly. Each agent jointly selects transmit frequency (channel) and power, and QMIX learns a centralized but factorizable action-value function that enables coordinated yet decentralized execution. We benchmark QMIX against a genie-aided optimal policy in a no-channel-reuse setting, and against local Upper Confidence Bound (UCB) and a stateless reactive policy in a more general fading regime with channel reuse enabled. Simulation results show that QMIX rapidly converges to cooperative policies that nearly match the genie-aided bound, while achieving higher throughput and lower jamming incidence than the baselines, thereby demonstrating MARL's effectiveness for securing autonomous swarms in contested environments.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [295] [LOOPRAG: Enhancing Loop Transformation Optimization with Retrieval-Augmented Large Language Models](https://arxiv.org/abs/2512.15766)
*Yijie Zhi,Yayu Cao,Jianhua Dai,Xiaoyang Han,Jingwen Pu,Qingran Wu,Sheng Cheng,Ming Cai*

Main category: cs.PL

TL;DR: 提出LOOPRAG框架用于引导大语言模型进行循环优化，在多个基准测试集上取得显著加速比。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在循环变换优化方面存在困难，常导致错误或次优优化，错失性能提升机会。

Method: 提出LOOPRAG框架，包括利用循环属性的参数驱动方法、基于循环特征的检索算法、基于反馈的迭代机制和多种测试进行等价性检查。

Result: 在PolyBench、TSVC和LORE基准测试集上，相比基础编译器和基础大语言模型取得显著加速比。

Conclusion: LOOPRAG框架能有效引导大语言模型进行循环优化，提升性能。

Abstract: Loop transformations are semantics-preserving optimization techniques, widely used to maximize objectives such as parallelism. Despite decades of research, applying the optimal composition of loop transformations remains challenging due to inherent complexities, including cost modeling for optimization objectives. Recent studies have explored the potential of Large Language Models (LLMs) for code optimization. However, our key observation is that LLMs often struggle with effective loop transformation optimization, frequently leading to errors or suboptimal optimization, thereby missing opportunities for performance improvements. To bridge this gap, we propose LOOPRAG, a novel retrieval-augmented generation framework designed to guide LLMs in performing effective loop optimization on Static Control Part. We introduce a parameter-driven method to harness loop properties, which trigger various loop transformations, and generate diverse yet legal example codes serving as a demonstration source. To effectively obtain the most informative demonstrations, we propose a loop-aware algorithm based on loop features, which balances similarity and diversity for code retrieval. To enhance correct and efficient code generation, we introduce a feedback-based iterative mechanism that incorporates compilation, testing and performance results as feedback to guide LLMs. Each optimized code undergoes mutation, coverage and differential testing for equivalence checking. We evaluate LOOPRAG on PolyBench, TSVC and LORE benchmark suites, and compare it against compilers (GCC-Graphite, Clang-Polly, Perspective and ICX) and representative LLMs (DeepSeek and GPT-4). The results demonstrate average speedups over base compilers of up to 11.20$\times$, 14.34$\times$, and 9.29$\times$ for PolyBench, TSVC, and LORE, respectively, and speedups over base LLMs of up to 11.97$\times$, 5.61$\times$, and 11.59$\times$.

</details>


### [296] [Automated Formalization of Probabilistic Requirements from Structured Natural Language](https://arxiv.org/abs/2512.15788)
*Anastasia Mavridou,Marie Farrell,Gricel Vázquez,Tom Pressburger,Timothy E. Wang,Radu Calinescu,Michael Fisher*

Main category: cs.PL

TL;DR: 本文扩展NASA的FRET工具支持概率需求规范，提出自动化翻译方法，让自主自适应系统形式化分析更实用、少出错。


<details>
  <summary>Details</summary>
Motivation: 在软件密集型系统中集成自主和自适应行为面临诸多挑战，尤其是概率需求规范的难题，期望降低开发者使用复杂形式化方法的难度。

Method: 扩展NASA FRET工具的结构化自然语言以支持概率需求规范，提出将结构化自然语言需求转换为概率时态逻辑公式的形式化、组合式和自动化方法，构建自动化验证框架和形式化证明。

Result: 扩展后的FRET工具能让开发者用结构化自然语言指定概率需求并自动转换为概率时态逻辑。

Conclusion: 扩展后的FRET工具使自主和自适应系统的形式化分析更实用、更不易出错。

Abstract: Integrating autonomous and adaptive behavior into software-intensive systems presents significant challenges for software development, as uncertainties in the environment or decision-making processes must be explicitly captured. These challenges are amplified in safety- and mission-critical systems, which must undergo rigorous scrutiny during design and development. Key among these challenges is the difficulty of specifying requirements that use probabilistic constructs to capture the uncertainty affecting these systems. To enable formal analysis, such requirements must be expressed in precise mathematical notations such as probabilistic logics. However, expecting developers to write requirements directly in complex formalisms is unrealistic and highly error-prone. We extend the structured natural language used by NASA's Formal Requirement Elicitation Tool (FRET) with support for the specification of unambiguous and correct probabilistic requirements, and develop an automated approach for translating these requirements into logical formulas. We propose and develop a formal, compositional, and automated approach for translating structured natural-language requirements into formulas in probabilistic temporal logic. To increase trust in our formalizations, we provide assurance that the generated formulas are well-formed and conform to the intended semantics through an automated validation framework and a formal proof. The extended FRET tool enables developers to specify probabilistic requirements in structured natural language, and to automatically translate them into probabilistic temporal logic, making the formal analysis of autonomous and adaptive systems more practical and less error-prone.

</details>


### [297] [Optimizing Agentic Language Model Inference via Speculative Tool Calls](https://arxiv.org/abs/2512.15834)
*Daniel Nichols,Prajwal Singhania,Charles Jekel,Abhinav Bhatele,Harshitha Menon*

Main category: cs.PL

TL;DR: 提出系统优化方法解决语言模型调用工具的推理性能瓶颈，提升吞吐量并做理论分析，推荐新 API 端点。


<details>
  <summary>Details</summary>
Motivation: 语言模型依赖外部工具提升能力，但工具调用在推理过程中引入性能瓶颈。

Method: 通过推测工具调用、让序列驻留在推理引擎以减少开销进行系统优化。

Result: 托管语言模型代理推理时，吞吐量提升数百 token 每秒。

Conclusion: 对优化算法进行理论分析，提出了最佳性能的推测配置，推荐新的“工具缓存” API 端点便于 LM 提供商采用优化。

Abstract: Language models (LMs) are becoming increasingly dependent on external tools. LM-based agentic frameworks frequently interact with their environment via such tools to search files, run code, call APIs, etc. Further, modern reasoning-based LMs use tools such as web search and Python code execution to enhance their reasoning capabilities. While tools greatly improve the capabilities of LMs, they also introduce performance bottlenecks during the inference process. In this paper, we introduce novel systems optimizations to address such performance bottlenecks by speculating tool calls and forcing sequences to remain resident in the inference engine to minimize overheads. Our optimizations lead to throughput improvements of several hundred tokens per second when hosting inference for LM agents. We provide a theoretical analysis of our algorithms to provide insights into speculation configurations that will yield the best performance. Further, we recommend a new "tool cache" API endpoint to enable LM providers to easily adopt these optimizations.

</details>


### [298] [A Neurosymbolic Approach to Loop Invariant Generation via Weakest Precondition Reasoning](https://arxiv.org/abs/2512.15816)
*Daragh King,Vasileios Koutavas,Laura Kovacs*

Main category: cs.PL

TL;DR: 本文提出NeuroInv用于循环不变式生成，在150个Java程序基准测试中成功率达99.5%，还能应对复杂验证场景。


<details>
  <summary>Details</summary>
Motivation: 现有使用大语言模型进行循环不变式生成的方法缺乏可靠结构化方法，且很少参考现有程序验证理论。

Method: NeuroInv包含两个关键模块，一是利用大语言模型和Hoare逻辑通过反向链式最弱前置条件推理推导和细化候选不变式的神经推理模块，二是使用OpenJML的反例迭代修复不变式的验证引导符号模块。

Result: 在150个Java程序的综合基准测试中成功率达99.5%，远超其他评估方法；在10个更大的多循环程序硬基准测试中，也能应对更复杂验证场景。

Conclusion: NeuroInv是一种有效的循环不变式生成方法，能处理复杂的程序验证场景。

Abstract: Loop invariant generation remains a critical bottleneck in automated program verification. Recent work has begun to explore the use of Large Language Models (LLMs) in this area, yet these approaches tend to lack a reliable and structured methodology, with little reference to existing program verification theory. This paper presents NeuroInv, a neurosymbolic approach to loop invariant generation. NeuroInv comprises two key modules: (1) a neural reasoning module that leverages LLMs and Hoare logic to derive and refine candidate invariants via backward-chaining weakest precondition reasoning, and (2) a verification-guided symbolic module that iteratively repairs invariants using counterexamples from OpenJML. We evaluate NeuroInv on a comprehensive benchmark of 150 Java programs, encompassing single and multiple (sequential) loops, multiple arrays, random branching, and noisy code segments. NeuroInv achieves a $99.5\%$ success rate, substantially outperforming the other evaluated approaches. Additionally, we introduce a hard benchmark of $10$ larger multi-loop programs (with an average of $7$ loops each); NeuroInv's performance in this setting demonstrates that it can scale to more complex verification scenarios.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [299] [Few-Shot Inference of Human Perceptions of Robot Performance in Social Navigation Scenarios](https://arxiv.org/abs/2512.16019)
*Qiping Zhang,Nathan Tsoi,Mofeed Nagib,Hao-Tien Lewis Chiang,Marynel Vázquez*

Main category: cs.RO

TL;DR: 文章提出利用大语言模型（LLMs）的少样本学习能力，在社交导航任务中预测用户对机器人性能的感知。通过扩展数据集实验，证明LLMs表现佳、可扩展性好，还研究输入特征和个性化示例对预测的影响。


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动方法需大量标注数据，限制其在捕捉人类对机器人行为评估中的应用，因此提出利用LLMs少样本学习能力改进机器人预测用户对其性能感知的能力。

Method: 扩展SEAN TOGETHER数据集，用增强数据集评估多个LLMs根据机器人和周围人类运动的时空线索，从少量上下文示例中预测人类对机器人性能感知的能力，还进行消融研究和探索个性化示例应用。

Result: LLMs能达到或超过传统监督学习模型的性能，且所需标注实例少一个数量级；更多上下文示例可提高预测性能；个性化示例能进一步提高预测准确性。

Conclusion: 此工作为通过以用户为中心的反馈可扩展地改善机器人行为铺平道路。

Abstract: Understanding how humans evaluate robot behavior during human-robot interactions is crucial for developing socially aware robots that behave according to human expectations. While the traditional approach to capturing these evaluations is to conduct a user study, recent work has proposed utilizing machine learning instead. However, existing data-driven methods require large amounts of labeled data, which limits their use in practice. To address this gap, we propose leveraging the few-shot learning capabilities of Large Language Models (LLMs) to improve how well a robot can predict a user's perception of its performance, and study this idea experimentally in social navigation tasks. To this end, we extend the SEAN TOGETHER dataset with additional real-world human-robot navigation episodes and participant feedback. Using this augmented dataset, we evaluate the ability of several LLMs to predict human perceptions of robot performance from a small number of in-context examples, based on observed spatio-temporal cues of the robot and surrounding human motion. Our results demonstrate that LLMs can match or exceed the performance of traditional supervised learning models while requiring an order of magnitude fewer labeled instances. We further show that prediction performance can improve with more in-context examples, confirming the scalability of our approach. Additionally, we investigate what kind of sensor-based information an LLM relies on to make these inferences by conducting an ablation study on the input features considered for performance prediction. Finally, we explore the novel application of personalized examples for in-context learning, i.e., drawn from the same user being evaluated, finding that they further enhance prediction accuracy. This work paves the path to improving robot behavior in a scalable manner through user-centered feedback.

</details>


### [300] [Olaf: Bringing an Animated Character to Life in the Physical World](https://arxiv.org/abs/2512.16705)
*David Müller,Espen Knoop,Dario Mylonopoulos,Agon Serifi,Michael A. Hopkins,Ruben Grandia,Moritz Bächer*

Main category: cs.RO

TL;DR: 本文借助强化学习将动画角色Olaf在现实中实现，采用多种机械设计和控制策略，经模拟和硬件验证，提升了机器人角色的可信度。


<details>
  <summary>Details</summary>
Motivation: 动画角色运动方式和比例与典型机器人不同，为机械设计和运动控制创新提供平台，要将Olaf在现实中实现。

Method: 依靠动画参考引导的强化学习进行控制；用软泡沫裙隐藏不对称双腿；在手臂、嘴和眼睛使用球形和平面连杆；引入额外奖励减少冲击噪音；将温度值作为策略额外输入并引入新奖励防止过热。

Result: 在模拟和硬件上验证了建模有效性，提升了带服装机器人角色的可信度。

Conclusion: 通过多种设计和控制策略，能实现高可信度的带服装机器人角色。

Abstract: Animated characters often move in non-physical ways and have proportions that are far from a typical walking robot. This provides an ideal platform for innovation in both mechanical design and stylized motion control. In this paper, we bring Olaf to life in the physical world, relying on reinforcement learning guided by animation references for control. To create the illusion of Olaf's feet moving along his body, we hide two asymmetric legs under a soft foam skirt. To fit actuators inside the character, we use spherical and planar linkages in the arms, mouth, and eyes. Because the walk cycle results in harsh contact sounds, we introduce additional rewards that noticeably reduce impact noise. The large head, driven by small actuators in the character's slim neck, creates a risk of overheating, amplified by the costume. To keep actuators from overheating, we feed temperature values as additional inputs to policies, introducing new rewards to keep them within bounds. We validate the efficacy of our modeling in simulation and on hardware, demonstrating an unmatched level of believability for a costumed robotic character.

</details>


### [301] [ReinforceGen: Hybrid Skill Policies with Automated Data Generation and Reinforcement Learning](https://arxiv.org/abs/2512.16861)
*Zihan Zhou,Animesh Garg,Ajay Mandlekar,Caelan Garrett*

Main category: cs.RO

TL;DR: 提出ReinforceGen系统解决长视野操作挑战，结合多技术形成初始方案并用强化学习微调，在Robosuite数据集上达80%成功率，微调使性能平均提升89%。


<details>
  <summary>Details</summary>
Motivation: 解决机器人领域长视野操作的长期挑战。

Method: 结合任务分解、数据生成、模仿学习和运动规划形成初始方案，用强化学习微调各组件，先分割任务为技能，用人类演示数据进行模仿学习，再通过在线适应和强化学习微调。

Result: 在Robosuite数据集上所有视觉运动控制任务最高重置范围设置下达80%成功率，微调方法使性能平均提升89%。

Conclusion: ReinforceGen系统能有效解决长视野操作问题，强化学习微调对性能提升有显著作用。

Abstract: Long-horizon manipulation has been a long-standing challenge in the robotics community. We propose ReinforceGen, a system that combines task decomposition, data generation, imitation learning, and motion planning to form an initial solution, and improves each component through reinforcement-learning-based fine-tuning. ReinforceGen first segments the task into multiple localized skills, which are connected through motion planning. The skills and motion planning targets are trained with imitation learning on a dataset generated from 10 human demonstrations, and then fine-tuned through online adaptation and reinforcement learning. When benchmarked on the Robosuite dataset, ReinforceGen reaches 80% success rate on all tasks with visuomotor controls in the highest reset range setting. Additional ablation studies show that our fine-tuning approaches contributes to an 89% average performance increase. More results and videos available in https://reinforcegen.github.io/

</details>


### [302] [PolaRiS: Scalable Real-to-Sim Evaluations for Generalist Robot Policies](https://arxiv.org/abs/2512.16881)
*Arhan Jain,Mingtong Zhang,Kanav Arora,William Chen,Marcel Torne,Muhammad Zubair Irshad,Sergey Zakharov,Yue Wang,Sergey Levine,Chelsea Finn,Wei-Chiu Ma,Dhruv Shah,Abhishek Gupta,Karl Pertsch*

Main category: cs.RO

TL;DR: 提出PolaRiS框架用于高保真模拟机器人评估，利用神经重建方法将真实场景视频转为模拟环境，能缩小真实与模拟差距，与真实世界策略表现相关性强。


<details>
  <summary>Details</summary>
Motivation: 机器人学习研究中准确衡量和比较机器人策略性能困难，现有模拟基准与现实有差距，构建模拟环境需大量人力和专业知识。

Method: 引入PolaRiS框架，利用神经重建方法将真实场景视频转为交互式模拟环境，开发模拟数据协同训练方法。

Result: PolaRiS评估与真实世界通用策略性能的相关性比现有模拟基准更强，能快速创建多样化模拟环境。

Conclusion: 该工作向分布式和民主化的下一代机器人基础模型评估迈进了一步。

Abstract: A significant challenge for robot learning research is our ability to accurately measure and compare the performance of robot policies. Benchmarking in robotics is historically challenging due to the stochasticity, reproducibility, and time-consuming nature of real-world rollouts. This challenge is exacerbated for recent generalist policies, which has to be evaluated across a wide variety of scenes and tasks. Evaluation in simulation offers a scalable complement to real world evaluations, but the visual and physical domain gap between existing simulation benchmarks and the real world has made them an unreliable signal for policy improvement. Furthermore, building realistic and diverse simulated environments has traditionally required significant human effort and expertise. To bridge the gap, we introduce Policy Evaluation and Environment Reconstruction in Simulation (PolaRiS), a scalable real-to-sim framework for high-fidelity simulated robot evaluation. PolaRiS utilizes neural reconstruction methods to turn short video scans of real-world scenes into interactive simulation environments. Additionally, we develop a simple simulation data co-training recipe that bridges remaining real-to-sim gaps and enables zero-shot evaluation in unseen simulation environments. Through extensive paired evaluations between simulation and the real world, we demonstrate that PolaRiS evaluations provide a much stronger correlation to real world generalist policy performance than existing simulated benchmarks. Its simplicity also enables rapid creation of diverse simulated environments. As such, this work takes a step towards distributed and democratized evaluation for the next generation of robotic foundation models.

</details>


### [303] [E-SDS: Environment-aware See it, Do it, Sorted - Automated Environment-Aware Reinforcement Learning for Humanoid Locomotion](https://arxiv.org/abs/2512.16446)
*Enis Yalcin,Joshua O'Hara,Maria Stamatopoulou,Chengxu Zhou,Dimitrios Kanoulas*

Main category: cs.RO

TL;DR: 提出E - SDS框架结合VLM和实时地形传感器分析自动生成奖励函数，在人形机器人运动任务上表现出色，减少奖励设计人力和时间。


<details>
  <summary>Details</summary>
Motivation: 当前基于VLM的方法在人形机器人运动奖励设计中缺乏环境感知，无法应对复杂地形，需解决感知差距问题。

Method: 提出E - SDS框架，将VLM与实时地形传感器分析集成，基于示例视频自动生成奖励函数。

Result: 在Unitree G1人形机器人的四种地形测试中，E - SDS能实现成功下楼梯，其他方法不能；在所有地形中，E - SDS减少速度跟踪误差51.9 - 82.6%。

Conclusion: E - SDS框架减少奖励设计人力，从数天降至不到两小时，同时产生更强大的运动策略。

Abstract: Vision-language models (VLMs) show promise in automating reward design in humanoid locomotion, which could eliminate the need for tedious manual engineering. However, current VLM-based methods are essentially "blind", as they lack the environmental perception required to navigate complex terrain. We present E-SDS (Environment-aware See it, Do it, Sorted), a framework that closes this perception gap. E-SDS integrates VLMs with real-time terrain sensor analysis to automatically generate reward functions that facilitate training of robust perceptive locomotion policies, grounded by example videos. Evaluated on a Unitree G1 humanoid across four distinct terrains (simple, gaps, obstacles, stairs), E-SDS uniquely enabled successful stair descent, while policies trained with manually-designed rewards or a non-perceptive automated baseline were unable to complete the task. In all terrains, E-SDS also reduced velocity tracking error by 51.9-82.6%. Our framework reduces the human effort of reward design from days to less than two hours while simultaneously producing more robust and capable locomotion policies.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [304] [Beyond openness: Inclusiveness and usability of Chinese scholarly data in OpenAlex](https://arxiv.org/abs/2512.16339)
*Lin Zhang,Zhe Cao,Jianhua Liu,Nees Jan van Eck*

Main category: cs.DL

TL;DR: 研究评估OpenAlex对中文期刊及文章的覆盖和元数据质量，发现其在实现全面包容性上存在挑战，并给出改进建议。


<details>
  <summary>Details</summary>
Motivation: 评估OpenAlex作为开放学术数据源是否能实现对中文期刊及文章更大包容性的承诺。

Method: 以《中国核心期刊要目总览（2023年版）》和万方数据为基准，分析期刊级覆盖、文章级覆盖和元数据字段的完整性与准确性。

Result: OpenAlex仅收录37%的核心期刊和24%的文章，有学科和时间差异；元数据质量参差不齐，DOI覆盖有限，语言信息常错误。

Conclusion: 指出实现全面包容性和可用性存在重大挑战，建议改进数据聚合策略、DOI注册和元数据标准化。

Abstract: OpenAlex, launched in 2022 as a fully open scholarly data source, promises greater inclusiveness compared to traditional proprietary databases. This study evaluates whether OpenAlex delivers on that promise by examining its coverage and metadata quality for Chinese-language journals and their articles. Using the 2023 edition of A Guide to the Core Journals of China (GCJC) and Wanfang Data as a benchmark, we analyze three aspects: (1) journal-level coverage, (2) article-level coverage, and (3) completeness and accuracy of metadata fields. Results show that OpenAlex indexes only 37% of GCJC journals and 24% of their articles, with substantial disciplinary and temporal variation. Metadata quality is uneven: while basic fields such as title and publication year are complete, bibliographic details, author affiliations, and cited references are frequently missing or inaccurate. DOI coverage is limited, and language information is often incorrect, with most Chinese-language articles labeled as English. These findings highlight significant challenges for achieving full inclusiveness and usability in research evaluation and related activities. We conclude with recommendations for improving data aggregation strategies, DOI registration practices, and metadata standardization to enhance the integration of local scholarly outputs into global open infrastructures.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [305] [Information theory and discriminative sampling for model discovery](https://arxiv.org/abs/2512.16000)
*Yuxuan Bao,J. Nathan Kutz*

Main category: cs.IT

TL;DR: 本文将Fisher信息矩阵用于SINDy框架，可视化信息模式，展示以信息为基础的分析可提升采样效率和模型性能，还说明Fisher信息和熵指标能在三种场景促进数据效率。


<details>
  <summary>Details</summary>
Motivation: 随着数据驱动模型发现愈发重要，期望借助可量化的信息指标指导采样策略来提高学习效率、减少数据需求。

Method: 在SINDy数据驱动框架中使用Fisher信息矩阵，进行信息模式可视化，通过对FIM的光谱分析阐明统计装袋的好处。

Result: 可视化了混沌和非混沌系统中单个轨迹和多初始条件的信息模式，表明信息分析提升采样效率和模型性能，还在三种场景中展示了Fisher信息和熵指标促进数据效率。

Conclusion: 以可量化信息指标为导向的采样策略能有效提升学习效率和降低数据需求。

Abstract: Fisher information and Shannon entropy are fundamental tools for understanding and analyzing dynamical systems from complementary perspectives. They can characterize unknown parameters by quantifying the information contained in variables, or measure how different initial trajectories or temporal segments of a trajectory contribute to learning or inferring system dynamics. In this work, we leverage the Fisher Information Matrix (FIM) within the data-driven framework of {\em sparse identification of nonlinear dynamics} (SINDy). We visualize information patterns in chaotic and non-chaotic systems for both single trajectories and multiple initial conditions, demonstrating how information-based analysis can improve sampling efficiency and enhance model performance by prioritizing more informative data. The benefits of statistical bagging are further elucidated through spectral analysis of the FIM. We also illustrate how Fisher information and entropy metrics can promote data efficiency in three scenarios: when only a single trajectory is available, when a tunable control parameter exists, and when multiple trajectories can be freely initialized. As data-driven model discovery continues to gain prominence, principled sampling strategies guided by quantifiable information metrics offer a powerful approach for improving learning efficiency and reducing data requirements.

</details>


<div id='q-fin.PR'></div>

# q-fin.PR [[Back]](#toc)

### [306] [Interpretable Deep Learning for Stock Returns: A Consensus-Bottleneck Asset Pricing Model](https://arxiv.org/abs/2512.16251)
*Bong-Gyu Jang,Younwoo Jeong,Changeun Kim*

Main category: q-fin.PR

TL;DR: 本文提出CB - APM模型，可预测美股风险溢价，在预测和解释能力上优于标准深度学习方法，能带来经济收益，揭示预期回报中信念驱动结构。


<details>
  <summary>Details</summary>
Motivation: 构建可部分解释的神经网络模型，捕捉投资者信念如何通过共识形成过程压缩为资产价格，解释信念聚合与预期回报的关系。

Method: 引入Consensus - Bottleneck Asset Pricing Model (CB - APM) 模型，通过建模‘瓶颈’总结公司和宏观层面信息。

Result: 模型改进了长期回报预测，在预测准确性和解释力上优于标准深度学习方法，能带来经济收益，其学习到的共识表示捕捉到传统因子模型未涵盖的定价变动。

Conclusion: CB - APM为理解信念驱动的回报动态提供了可解释且基于实证的框架。

Abstract: We introduce the \textit{Consensus-Bottleneck Asset Pricing Model} (CB-APM), a partially interpretable neural network that replicates the reasoning processes of sell-side analysts by capturing how dispersed investor beliefs are compressed into asset prices through a consensus formation process. By modeling this ``bottleneck'' to summarize firm- and macro-level information, CB-APM not only predicts future risk premiums of U.S. equities but also links belief aggregation to expected returns in a structurally interpretable manner. The model improves long-horizon return forecasts and outperforms standard deep learning approaches in both predictive accuracy and explanatory power. Comprehensive portfolio analyses show that CB-APM's out-of-sample predictions translate into economically meaningful payoffs, with monotonic return differentials and stable long-short performance across regularization settings. Empirically, CB-APM leverages consensus as a regularizer to amplify long-horizon predictability and yields interpretable consensus-based components that clarify how information is priced in returns. Moreover, regression and GRS-based pricing diagnostics reveal that the learned consensus representations capture priced variation only partially spanned by traditional factor models, demonstrating that CB-APM uncovers belief-driven structure in expected returns beyond the canonical factor space. Overall, CB-APM provides an interpretable and empirically grounded framework for understanding belief-driven return dynamics.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [307] [Are the Bank of Korea's Inflation Forecasts Biased Toward the Target?](https://arxiv.org/abs/2512.16068)
*Eunkyu Seong,Seojeong Lee*

Main category: econ.GN

TL;DR: 研究韩国银行通胀预测是否有偏差及校正策略，发现预测有偏差，部分校正策略可提升准确性。


<details>
  <summary>Details</summary>
Motivation: 探讨韩国银行通胀预测是否存在偏向通胀目标的偏差。

Method: 扩展Holden和Peel（1990）测试以纳入状态依赖，基于预测时实际通胀是否低于目标定义经济状态；采用基于AR(1)和平均误差模型的偏差校正策略。

Result: 韩国银行的通胀预测在状态依赖框架下有偏差；校正策略总体提升预测准确性，AR(1)校正表现较稳定，能持续降低均方根误差。

Conclusion: 韩国银行通胀预测存在偏差，部分偏差校正策略有效。

Abstract: The Bank of Korea (BoK) regularly publishes the Economic Outlook, offering forecasts for key macroeconomic variables such as GDP growth, inflation, and unemployment rates. This study examines whether the BoK's inflation forecasts exhibit bias, specifically a tendency to align with its inflation target. We extend the Holden and Peel (1990) test to incorporate state-dependency, defining the state of the economy based on whether realized inflation falls below the target at the time of the forecast. Our analysis reveals that the BoK's inflation forecasts are biased under this state-dependent framework. Furthermore, we examine a range of bias correction strategies based on AR(1) and mean error models, including their state-dependent variants. These strategies generally improve forecast accuracy. Among them, the AR(1)-based correction exhibits relatively stable performance, consistently reducing the root mean square error.

</details>


### [308] [Migrants as First Responders: A Global Estimate of Disaster-Driven Remittances](https://arxiv.org/abs/2512.16373)
*Andrea Vismara,Ola Ali,Carsten Källner,Guillermo Prieto-Viertel,Rafael Prieto-Curiel*

Main category: econ.GN

TL;DR: 研究国际移民汇款对原籍国灾害的响应，发现十年间约3320亿美元汇款用于应对灾害，不同灾害引发的汇款响应不同，汇款是重要但有限的灾害融资形式。


<details>
  <summary>Details</summary>
Motivation: 国际汇款是家庭应对灾害的重要资金来源，但对其对环境灾害的响应量化不足，需揭示国际移民汇款对原籍国灾害响应的宏观金融系统。

Method: 开发模拟个人汇款决策的结构模型，用2010 - 2019年全球灾害记录和双边月度汇款流量数据进行校准。

Result: 十年间约3320亿美元（占汇款总额5.46%）用于应对地震、洪水、风暴和干旱；地震引发人均汇款响应最大，干旱最小；不同侨民群体提供金融支持能力有显著差异。

Conclusion: 汇款是重要但有限的灾害融资形式，在增强未来应对环境冲击的恢复力方面有重要性和局限性。

Abstract: International remittances represent a vital source of disaster adaptation finance for households around the world, yet their responsiveness to environmental disasters remains poorly quantified. We reveal a previously unmeasured global macro-financial system of international migrant diasporas remittances response to the occurrence of disasters in the country of origin. We do so by developing a structural model simulating individual remittance decisions, calibrated with global disaster records and bilateral monthly remittances flow data from the period 2010-2019. Our analysis reveals that approximately 332 billion USD (5.46\% of total remittances) were mobilized specifically in response to earthquakes, floods, storms, and droughts over the decade. Earthquakes triggered the largest remittance responses per person affected, while droughts elicited the smallest. The model also identifies significant variation in diaspora groups' capacity to activate financial support. These findings establish remittances as a substantial yet limited form of disaster finance, highlighting their importance and limitations in building resilience against future environmental shocks.

</details>


### [309] [Did a feedback mechanism between propositional and prescriptive knowledge create modern growth?](https://arxiv.org/abs/2512.16587)
*Julius Koschnick*

Main category: econ.GN

TL;DR: 本文定量检验了现代经济增长起源的反馈循环假说，发现18世纪后半叶命题知识和规范知识的反馈循环变为正向且对实体经济有积极影响，支持了Mokyr的假说。


<details>
  <summary>Details</summary>
Motivation: Joel Mokyr提出的现代经济增长起源于命题知识和规范知识反馈循环的论点未被直接检验，本文旨在提供定量证据。

Method: 引入基于文本的两个新指标，即出版物创新性和知识溢出，估计1600 - 1800年英格兰命题知识和规范知识间的知识溢出对创新的影响。

Result: 发现18世纪后半叶命题知识和规范知识的反馈循环变为正向，且该过程对通过专利衡量的实体经济有积极影响。

Conclusion: 研究结果为Mokyr的原始假说提供了实证支持。

Abstract: What was the origin of modern economic growth? Joel Mokyr has argued that self-sustained modern economic growth originated from a feedback loop between propositional (theoretical) and prescriptive (applied) knowledge, which turned positive in the eighteenth century during the "Industrial Enlightenment". While influential, this thesis has never been directly tested. This paper provides the first quantitative evidence by estimating the impact of knowledge spillovers between propositional and prescriptive knowledge on innovation in England, 1600-1800. For this, it introduces two new text-based measures for 1) the innovativeness of publications and 2) knowledge spillovers. The paper finds strong evidence that a feedback loop between propositional and prescriptive knowledge became positive during the second half of the eighteenth century. It also documents that this process had positive effects on the real economy as measured through patents. Overall, the findings provide empirical support for Mokyr's original hypothesis.

</details>


### [310] [Veblen effects and broken windows in an environmental OLG model](https://arxiv.org/abs/2512.16806)
*Nicolás Blampied,Alessia Cafferata,Marwil J. Davila-Fernandez*

Main category: econ.GN

TL;DR: 本文构建环境叠代模型研究社会比较对消费和环境的影响，发现地位驱动消费会增加环境脆弱性。


<details>
  <summary>Details</summary>
Motivation: 探究持续的社会比较是否导致过度消费并增加生态足迹，以及社会比较如何随时间塑造绿色偏好。

Method: 开发考虑凡勃伦效应且允许绿色偏好异步更新的环境叠代模型。

Result: 最优路径上，地位性消费导致过度消费，损害环境；消费税无法解决社会比较问题；凡勃伦机制弱时存在两个稳定均衡，绿色投资脆弱；凡勃伦效应强时，模型呈现内生、持续、非周期性振荡，绿色偏好接近零，环境质量低。

Conclusion: 环境脆弱性随地位驱动消费而增加。

Abstract: Can constantly comparing ourselves to others lead to overconsumption, ultimately increasing the ecological footprint? How do social comparisons shape green preferences over time? To answer these questions, we develop an environmental Overlapping Generations (OLG) model that explicitly accounts for Veblen effects and allows green preferences to be updated asynchronously, influenced by past environmental conditions and relative status considerations. We show that, along the optimal path, positional spending leads to overconsumption, which is detrimental to the environment. Taxing consumption is counterproductive as it does not directly address the social comparisons issue, leaving the problem unchanged. When the Veblenian mechanism is weak, the introduction of a materialistic ``secular trend'' -- that lowers the importance placed on the public good -- gives rise to two stable equilibria separated by a saddle: one in which agents care about environmental quality as much as consuming, and the other in which they derive utility solely from the latter. Studying the basins of attraction reveals that green investments are highly fragile. Our numerical experiments further indicated that, when Veblen effects are strong, the model depicts endogenous, persistent, aperiodic oscillations. In this case, green preferences fluctuate close to zero, and environmental quality is very low. Taken together, these findings suggest environmental vulnerability grows in parallel with status-driven consumption.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [311] [Ev-Trust: A Strategy Equilibrium Trust Mechanism for Evolutionary Games in LLM-Based Multi-Agent Services](https://arxiv.org/abs/2512.16167)
*Shiduo Yang,Jiye Wang,Jiayu Qin,Jianbin Li,Yu Wang,Yuanhe Zhao,Kenan Guo*

Main category: cs.MA

TL;DR: 随着Web向以代理为中心范式发展，基于LLM的多智能体系统存在信任问题，本文提出Ev - Trust机制解决，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 基于LLM的多智能体系统开放性和异质性放大了欺骗、欺诈和错误信息风险，对信任建立和系统鲁棒性构成挑战。

Method: 提出基于进化博弈论的Ev - Trust策略均衡信任机制，将直接信任、间接信任和预期收益整合到动态反馈结构中，在去中心化服务框架中让智能体自适应调整策略。并通过复制动态方程进行理论推导。

Result: 实验结果表明该方法能有效反映智能体可信度，减少恶意策略，增加集体收益。

Conclusion: Ev - Trust可为群体进化博弈场景下的智能体服务网络信任建模提供新视角。

Abstract: The rapid evolution of the Web toward an agent-centric paradigm, driven by large language models (LLMs), has enabled autonomous agents to reason, plan, and interact in complex decentralized environments. However, the openness and heterogeneity of LLM-based multi-agent systems also amplify the risks of deception, fraud, and misinformation, posing severe challenges to trust establishment and system robustness. To address this issue, we propose Ev-Trust, a strategy-equilibrium trust mechanism grounded in evolutionary game theory. This mechanism integrates direct trust, indirect trust, and expected revenue into a dynamic feedback structure that guides agents' behavioral evolution toward equilibria. Within a decentralized "Request-Response-Payment-Evaluation" service framework, Ev-Trust enables agents to adaptively adjust strategies, naturally excluding malicious participants while reinforcing high-quality collaboration. Furthermore, our theoretical derivation based on replicator dynamics equations proves the existence and stability of local evolutionary equilibria. Experimental results indicate that our approach effectively reflects agent trustworthiness in LLM-driven open service interaction scenarios, reduces malicious strategies, and increases collective revenue. We hope Ev-Trust can provide a new perspective on trust modeling for the agentic service web in group evolutionary game scenarios.

</details>


### [312] [Don't Guess, Escalate: Towards Explainable Uncertainty-Calibrated AI Forensic Agents](https://arxiv.org/abs/2512.16614)
*Giulia Boato,Andrea Montibeller,Edward Delp,Luisa Verdoliva,Daniele Miorandi*

Main category: cs.MA

TL;DR: 提出AI法医代理，改进多媒体取证真实性验证过程


<details>
  <summary>Details</summary>
Motivation: AI正重塑多媒体取证格局，当前解决方案存在缺陷

Method: 提出AI法医代理作为可靠编排器，选择和组合法医检测器，识别来源和上下文，并提供不确定性感知评估，引入统一框架

Result: 未提及

Conclusion: 未提及

Abstract: AI is reshaping the landscape of multimedia forensics. We propose AI forensic agents: reliable orchestrators that select and combine forensic detectors, identify provenance and context, and provide uncertainty-aware assessments. We highlight pitfalls in current solutions and introduce a unified framework to improve the authenticity verification process.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [313] [Private Virtual Tree Networks for Secure Multi-Tenant Environments Based on the VIRGO Overlay Network](https://arxiv.org/abs/2512.15915)
*Lican Huang*

Main category: cs.CR

TL;DR: 提出Private Virtual Tree Networks (PVTNs)，基于VIRGO网络，利用加密技术实现安全、可扩展的虚拟树网络，无需全球公钥基础设施。


<details>
  <summary>Details</summary>
Motivation: VIRGO网络虽能组织分布式系统，但缺乏安全和隐私机制，需设计新方案。

Method: 在PVTNs中，加入请求用管理者公钥加密，会员授权通过管理者签名的委托证书执行，公钥仅在直接管理 - 成员关系中披露。

Result: 通过系统模型、协议、安全分析和设计原理，证明PVTNs能实现可扩展性、动态管理和强安全保障。

Conclusion: PVTNs在不依赖全球公钥基础设施的情况下实现了可扩展性、动态管理和强安全保障。

Abstract: Hierarchical organization is a fundamental structure in real-world society, where authority and responsibility are delegated from managers to subordinates. The VIRGO network (Virtual Hierarchical Overlay Network for scalable grid computing) provides a scalable overlay for organizing distributed systems but lacks intrinsic security and privacy mechanisms. This paper proposes Private Virtual Tree Networks (PVTNs), a cryptographically enforced extension that leverages the VIRGO overlay to mirror real organizational hierarchies. In PVTNs, join requests are encrypted with the manager's public key to ensure confidentiality, while membership authorization is enforced through manager-signed delegation certificates. Public keys are treated as organizational secrets and are disclosed only within direct manager-member relationships, resulting in a private, non-enumerable virtual tree. Our work demonstrates, through the system model, protocols, security analysis, and design rationale, that PVTNs achieve scalability, dynamic management, and strong security guarantees without relying on global public key infrastructures.

</details>


### [314] [Efficient Bitcoin Meta-Protocol Transaction and Data Discovery Through nLockTime Field Repurposing](https://arxiv.org/abs/2512.16683)
*Nikodem Tomczak*

Main category: cs.CR

TL;DR: 介绍Lockchain协议，可零边际区块空间成本实现高效交易发现和数据验证。


<details>
  <summary>Details</summary>
Motivation: 解决比特币交易大规模发现未充分优化的问题。

Method: 将比特币交易的4字节nLockTime字段重新用作紧凑元数据头，约束值范围以编码协议信号等。

Result: 形成高效的发现层，索引器可通过检查固定大小头字段筛选候选交易。

Conclusion: 该协议应用成熟设计模式解决问题，不涉及新的加密原语或存储方法。

Abstract: We describe the Lockchain Protocol, a lightweight Bitcoin meta-protocol that enables highly efficient transaction discovery at zero marginal block space cost, and data verification without introducing any new on-chain storage mechanism. The protocol repurposes the mandatory 4-byte nLockTime field of every Bitcoin transaction as a compact metadata header. By constraining values to an unused range of past Unix timestamps greater than or equal to 500,000,000, the field can encode a protocol signal, type, variant, and sequence identifier while remaining fully valid under Bitcoin consensus and policy rules. The primary contribution of the protocol is an efficient discovery layer. Indexers can filter candidate transactions by examining a fixed-size header field, independent of transaction payload size, and only then selectively inspect heavier data such as OP RETURN outputs or witness fields. The Lockchain Protocol applies established protocol design patterns to an under-optimised problem domain, namely transaction discovery at scale, and does not claim new cryptographic primitives or storage methods.

</details>


### [315] [PHANTOM: Progressive High-fidelity Adversarial Network for Threat Object Modeling](https://arxiv.org/abs/2512.15768)
*Jamal Al-Karaki,Muhammad Al-Zafar Khan,Rand Derar Mohammad Al Athamneh*

Main category: cs.CR

TL;DR: 本文提出PHANTOM框架生成高保真合成攻击数据，经测试数据有效，但生成罕见攻击类型有局限。


<details>
  <summary>Details</summary>
Motivation: 网络攻击数据稀缺阻碍入侵检测系统发展，需生成合成攻击数据。

Method: 提出PHANTOM框架，采用渐进式训练、双路径VAE - GAN架构和特定领域特征匹配。

Result: 在100,000个网络流量样本上评估，基于PHANTOM数据训练的模型对真实攻击加权准确率达98%，合成数据保留真实分布和多样性。

Conclusion: 该工作推动了用于训练强大、保护隐私的检测系统的合成数据生成，但生成罕见攻击类型存在挑战。

Abstract: The scarcity of cyberattack data hinders the development of robust intrusion detection systems. This paper introduces PHANTOM, a novel adversarial variational framework for generating high-fidelity synthetic attack data. Its innovations include progressive training, a dual-path VAE-GAN architecture, and domain-specific feature matching to preserve the semantics of attacks. Evaluated on 100,000 network traffic samples, models trained on PHANTOM data achieve 98% weighted accuracy on real attacks. Statistical analyses confirm that the synthetic data preserves authentic distributions and diversity. Limitations in generating rare attack types are noted, highlighting challenges with severe class imbalance. This work advances the generation of synthetic data for training robust, privacy-preserving detection systems.

</details>


### [316] [Data-Chain Backdoor: Do You Trust Diffusion Models as Generative Data Supplier?](https://arxiv.org/abs/2512.15769)
*Junchi Lu,Xinke Li,Yuheng Liu,Qi Alfred Chen*

Main category: cs.CR

TL;DR: 研究生成数据供应链中的后门传播问题，发现开源扩散模型会成为后门隐藏载体，还发现ESTM现象，揭示新威胁并给出初步缓解思路。


<details>
  <summary>Details</summary>
Motivation: 生成模型用于合成数据增强虽降低成本，但带来安全隐患，研究新兴生成数据供应链中的后门传播。

Method: 研究开源扩散模型在数据生成中的特性，分析其对后门触发器的记忆和传播情况。

Result: 发现开源扩散模型会成为后门隐藏载体，存在ESTM现象，在干净标签攻击场景下有严重安全风险。

Conclusion: 揭示生成数据管道中未被充分探索的威胁，为缓解合成数据生成中的后门风险提供初步见解。

Abstract: The increasing use of generative models such as diffusion models for synthetic data augmentation has greatly reduced the cost of data collection and labeling in downstream perception tasks. However, this new data source paradigm may introduce important security concerns. This work investigates backdoor propagation in such emerging generative data supply chains, namely Data-Chain Backdoor (DCB). Specifically, we find that open-source diffusion models can become hidden carriers of backdoors. Their strong distribution-fitting ability causes them to memorize and reproduce backdoor triggers during generation, which are subsequently inherited by downstream models, resulting in severe security risks. This threat is particularly concerning under clean-label attack scenarios, as it remains effective while having negligible impact on the utility of the synthetic data. Furthermore, we discover an Early-Stage Trigger Manifestation (ESTM) phenomenon: backdoor trigger patterns tend to surface more explicitly in the early, high-noise stages of the diffusion model's reverse generation process before being subtly integrated into the final samples. Overall, this work reveals a previously underexplored threat in generative data pipelines and provides initial insights toward mitigating backdoor risks in synthetic data generation.

</details>


### [317] [Cybercrime and Computer Forensics in Epoch of Artificial Intelligence in India](https://arxiv.org/abs/2512.15799)
*Sahibpreet Singh,Shikha Dhiman*

Main category: cs.CR

TL;DR: 本文探讨印度刑法学在计算取证完整性方面需重新评估，研究DPDP法案与对抗性AI威胁的兼容性，提出以人类为中心的取证模型。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能融入数字生态，需重新评估印度刑法学中计算取证完整性，填补DPDP法案与对抗性AI威胁兼容性的研究空白。

Method: 采用教义法学方法，综合对DPDP法案的法定分析与全球伦理框架进行监管效能评估。

Result: 机器学习模式识别精度高，但存在数据中毒和算法偏见漏洞；法案数据最小化原则与取证数据保留要求冲突；现有法律定义无法涵盖AI驱动犯罪。

Conclusion: 应使印度隐私法规与国际取证标准同步，提出以人类为中心的取证模型，为未来立法修订和技术标准化提供路线图。

Abstract: The integration of generative Artificial Intelligence into the digital ecosystem necessitates a critical re-evaluation of Indian criminal jurisprudence regarding computational forensics integrity. While algorithmic efficiency enhances evidence extraction, a research gap exists regarding the Digital Personal Data Protection Act, 2023's compatibility with adversarial AI threats, specifically anti-forensics and deepfakes. This study scrutinizes the AI "dual-use" dilemma, functioning as both a cyber-threat vector and forensic automation mechanism, to delineate privacy boundaries in high-stakes investigations. Employing a doctrinal legal methodology, the research synthesizes statutory analysis of the DPDP Act with global ethical frameworks (IEEE, EU) to evaluate regulatory efficacy. Preliminary results indicate that while Machine Learning offers high accuracy in pattern recognition, it introduces vulnerabilities regarding data poisoning and algorithmic bias. Findings highlight a critical tension between the Act's data minimization principles and forensic data retention requirements. Furthermore, the paper identifies that existing legal definitions inadequately encompass AI-driven "tool crimes" and "target crimes." Consequently, the research proposes a "human-centric" forensic model prioritizing explainable AI (XAI) to ensure evidence admissibility. These implications suggest that synchronizing Indian privacy statutes with international forensic standards is imperative to mitigate synthetic media risks, establishing a roadmap for future legislative amendments and technical standardization.

</details>


### [318] [VET Your Agent: Towards Host-Independent Autonomy via Verifiable Execution Traces](https://arxiv.org/abs/2512.15892)
*Artem Grigor,Christian Schroeder de Witt,Simon Birnbach,Ivan Martinovic*

Main category: cs.CR

TL;DR: 本文介绍VET框架实现代理输出独立于主机的身份验证，评估显示当前技术可实现实用的主机无关身份验证。


<details>
  <summary>Details</summary>
Motivation: 现有自主代理在主机控制的基础设施上执行，主机可篡改数据破坏自主性，需解决此问题。

Method: 引入VET框架，核心是AID，支持多种证明机制，对基于API的大语言模型代理实现VET并在实际工作负载上评估。

Result: 对于黑盒API调用，Web Proofs最实用，开销通常在3倍以内；公共API调用，低开销的TEE Proxy通常足够，还部署了可验证交易代理。

Conclusion: 当前技术已能实现实用的主机无关身份验证，为实现完全主机独立自主性的未来系统奠定基础。

Abstract: Recent advances in large language models (LLMs) have enabled a new generation of autonomous agents that operate over sustained periods and manage sensitive resources on behalf of users. Trusted for their ability to act without direct oversight, such agents are increasingly considered in high-stakes domains including financial management, dispute resolution, and governance. Yet in practice, agents execute on infrastructure controlled by a host, who can tamper with models, inputs, or outputs, undermining any meaningful notion of autonomy.
  We address this gap by introducing VET (Verifiable Execution Traces), a formal framework that achieves host-independent authentication of agent outputs and takes a step toward host-independent autonomy. Central to VET is the Agent Identity Document (AID), which specifies an agent's configuration together with the proof systems required for verification. VET is compositional: it supports multiple proof mechanisms, including trusted hardware, succinct cryptographic proofs, and notarized TLS transcripts (Web Proofs).
  We implement VET for an API-based LLM agent and evaluate our instantiation on realistic workloads. We find that for today's black-box, secret-bearing API calls, Web Proofs appear to be the most practical choice, with overhead typically under 3$\times$ compared to direct API calls, while for public API calls, a lower-overhead TEE Proxy is often sufficient. As a case study, we deploy a verifiable trading agent that produces proofs for each decision and composes Web Proofs with a TEE Proxy. Our results demonstrate that practical, host-agnostic authentication is already possible with current technology, laying the foundation for future systems that achieve full host-independent autonomy.

</details>


### [319] [RAMBO: Reliability Analysis for Mamba through Bit-flip attack Optimization](https://arxiv.org/abs/2512.15778)
*Sanjay Das,Swastik Bhattacharya,Shamik Kundu,Arnab Raha,Souvik Kundu,Kanad Basu*

Main category: cs.CR

TL;DR: 研究表明状态空间模型（SSMs）中Mamba架构在硬件位翻转攻击下精度显著下降，凸显其脆弱性。


<details>
  <summary>Details</summary>
Motivation: 随着SSMs在现实应用增多，评估其对软硬件威胁的敏感性，尤其是硬件位翻转攻击。

Method: 引入专门针对Mamba架构的位翻转攻击框架RAMBO，在Mamba - 1.4b模型上结合LAMBADA基准测试进行实验。

Result: 翻转单个关键位可使准确率从74.64%降至0%，困惑度从18.94增至3.75 x 10^6。

Conclusion: SSMs对对抗性扰动具有明显的脆弱性。

Abstract: State-space models (SSMs), exemplified by the Mamba architecture, have recently emerged as state-of-the-art sequence-modeling frameworks, offering linear-time scalability together with strong performance in long-context settings. Owing to their unique combination of efficiency, scalability, and expressive capacity, SSMs have become compelling alternatives to transformer-based models, which suffer from the quadratic computational and memory costs of attention mechanisms. As SSMs are increasingly deployed in real-world applications, it is critical to assess their susceptibility to both software- and hardware-level threats to ensure secure and reliable operation. Among such threats, hardware-induced bit-flip attacks (BFAs) pose a particularly severe risk by corrupting model parameters through memory faults, thereby undermining model accuracy and functional integrity. To investigate this vulnerability, we introduce RAMBO, the first BFA framework specifically designed to target Mamba-based architectures. Through experiments on the Mamba-1.4b model with LAMBADA benchmark, a cloze-style word-prediction task, we demonstrate that flipping merely a single critical bit can catastrophically reduce accuracy from 74.64% to 0% and increase perplexity from 18.94 to 3.75 x 10^6. These results demonstrate the pronounced fragility of SSMs to adversarial perturbations.

</details>


### [320] [Hyperparameter Tuning-Based Optimized Performance Analysis of Machine Learning Algorithms for Network Intrusion Detection](https://arxiv.org/abs/2512.15779)
*Sudhanshu Sekhar Tripathy,Bichitrananda Behera*

Main category: cs.CR

TL;DR: 研究用机器学习方法提升网络入侵检测系统（NIDS）准确性，评估优化多种算法，SVM经调参后表现最佳，验证了ML分类器的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着网络威胁日益复杂，NIDS需进化以检测新兴威胁和异常行为，研究旨在用机器学习方法提高NIDS准确性。

Method: 利用1999 KDD CUP入侵数据集，评估优化多种ML算法，用网格和随机搜索技术进行超参数优化，采用十折交叉验证和递归特征消除进行特征选择。

Result: 未调参时性能不佳，调参后SVM准确率达99.12%，误报率0.0091，优于默认配置和其他分类器。

Conclusion: ML分类器具有适应性和可靠性，有助于提高网络入侵检测系统的准确性。

Abstract: Network Intrusion Detection Systems (NIDS) are essential for securing networks by identifying and mitigating unauthorized activities indicative of cyberattacks. As cyber threats grow increasingly sophisticated, NIDS must evolve to detect both emerging threats and deviations from normal behavior. This study explores the application of machine learning (ML) methods to improve the NIDS accuracy through analyzing intricate structures in deep-featured network traffic records. Leveraging the 1999 KDD CUP intrusion dataset as a benchmark, this research evaluates and optimizes several ML algorithms, including Support Vector Machines (SVM), Naïve Bayes variants (MNB, BNB), Random Forest (RF), k-Nearest Neighbors (k-NN), Decision Trees (DT), AdaBoost, XGBoost, Logistic Regression (LR), Ridge Classifier, Passive-Aggressive (PA) Classifier, Rocchio Classifier, Artificial Neural Networks (ANN), and Perceptron (PPN). Initial evaluations without hyper-parameter optimization demonstrated suboptimal performance, highlighting the importance of tuning to enhance classification accuracy. After hyper-parameter optimization using grid and random search techniques, the SVM classifier achieved 99.12% accuracy with a 0.0091 False Alarm Rate (FAR), outperforming its default configuration (98.08% accuracy, 0.0123 FAR) and all other classifiers. This result confirms that SVM accomplishes the highest accuracy among the evaluated classifiers. We validated the effectiveness of all classifiers using a tenfold cross-validation approach, incorporating Recursive Feature Elimination (RFE) for feature selection to enhance the classifiers accuracy and efficiency. Our outcomes indicate that ML classifiers are both adaptable and reliable, contributing to enhanced accuracy in systems for detecting network intrusions.

</details>


### [321] [Auto-Tuning Safety Guardrails for Black-Box Large Language Models](https://arxiv.org/abs/2512.15782)
*Perry Abdulkadir*

Main category: cs.CR

TL;DR: 本文将大语言模型安全护栏设计作为超参数优化问题研究，通过对Mistral - 7B - Instruct包装并在多个基准测试评估，发现黑盒Optuna研究表现良好，证明此方法可行。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型安全护栏（系统提示和内容过滤器）通常是手动调整的，脆弱且难以复现，因此需要一种更好的方法。

Method: 将Mistral - 7B - Instruct与模块化越狱和恶意软件系统提示及基于ModernBERT的有害性分类器包装，在三个公共基准测试上评估候选配置，使用攻击成功率、良性有害响应率和端到端延迟评分，进行48点网格搜索并与黑盒Optuna研究对比。

Result: 黑盒Optuna研究能可靠地重新发现最佳网格配置，所需评估次数少一个数量级，时钟时间约少8倍。

Conclusion: 将安全护栏视为可调整的超参数是在计算和时间限制下强化黑盒大语言模型部署的可行方法。

Abstract: Large language models (LLMs) are increasingly deployed behind safety guardrails such as system prompts and content filters, especially in settings where product teams cannot modify model weights. In practice these guardrails are typically hand-tuned, brittle, and difficult to reproduce. This paper studies a simple but practical alternative: treat safety guardrail design itself as a hyperparameter optimization problem over a frozen base model. Concretely, I wrap Mistral-7B-Instruct with modular jailbreak and malware system prompts plus a ModernBERT-based harmfulness classifier, then evaluate candidate configurations on three public benchmarks covering malware generation, classic jailbreak prompts, and benign user queries. Each configuration is scored using malware and jailbreak attack success rate, benign harmful-response rate, and end-to-end latency. A 48-point grid search over prompt combinations and filter modes establishes a baseline. I then run a black-box Optuna study over the same space and show that it reliably rediscovers the best grid configurations while requiring an order of magnitude fewer evaluations and roughly 8x less wall-clock time. The results suggest that viewing safety guardrails as tunable hyperparameters is a feasible way to harden black-box LLM deployments under compute and time constraints.

</details>


### [322] [An empirical analysis of zero-day vulnerabilities disclosed by the zero day initiative](https://arxiv.org/abs/2512.15803)
*Apurva Shet,Izzat Alsmadi*

Main category: cs.CR

TL;DR: 分析2024年1 - 4月Zero Day Initiative（ZDI）披露的415个零日漏洞，确定披露趋势、严重程度分布，探索预测模型以改进补丁优先级策略。


<details>
  <summary>Details</summary>
Motivation: 零日漏洞是网络安全中最关键的威胁之一，受影响系统在补丁开发和部署前无防护，研究旨在识别披露趋势、严重程度分布，支持更好的漏洞管理和应对新兴威胁。

Method: 分析ZDI在2024年1 - 4月报告的漏洞披露数据集，探索使用结构化元数据和非结构化文本描述的经典机器学习和深度学习模型进行严重程度分类。

Result: 未提及具体结果。

Conclusion: 研究结果将支持改进补丁优先级策略、更有效的漏洞管理和增强组织对新兴零日威胁的准备。

Abstract: Zero-day vulnerabilities represent some of the most critical threats in cybersecurity, as they correspond to previously unknown flaws in software or hardware that are actively exploited before vendors can develop and deploy patches. During this exposure window, affected systems remain defenseless, making zero-day attacks particularly damaging and difficult to mitigate. This study analyzes the Zero Day Initiative (ZDI) vulnerability disclosures reported between January and April 2024, Cole [2025] comprising a total of 415 vulnerabilities. The dataset includes vulnerability identifiers, Common Vulnerability Scoring System (CVSS) v3.0 scores, publication dates, and short textual descriptions. The primary objectives of this work are to identify trends in zero-day vulnerability disclosures, examine severity distributions across vendors, and investigate which vulnerability characteristics are most indicative of high severity. In addition, this study explores predictive modeling approaches for severity classification, comparing classical machine learning techniques with deep learning models using both structured metadata and unstructured textual descriptions. The findings aim to support improved patch prioritization strategies, more effective vulnerability management, and enhanced organizational preparedness against emerging zero-day threats.

</details>


### [323] [Secure AI-Driven Super-Resolution for Real-Time Mixed Reality Applications](https://arxiv.org/abs/2512.15823)
*Mohammad Waquas Usmani,Sankalpa Timilsina,Michael Zink,Susmit Shannigrahi*

Main category: cs.CR

TL;DR: 该工作设计系统降低360°和6DoF点云视频实时AR/VR流的带宽消耗和加密/解密延迟，评估显示有良好效果。


<details>
  <summary>Details</summary>
Motivation: 沉浸式格式的点云视频对实时AR/VR流的高带宽和低延迟要求带来挑战，需降低带宽消耗和加密/解密延迟。

Method: 在源服务器对内容降采样并部分加密，在客户端用ML超分辨率模型解密和上采样。

Result: 带宽/延迟、加密/解密开销近乎线性降低，超分辨率模型能有效重建原始全分辨率点云，误差小且推理时间适中。

Conclusion: 所设计系统能有效降低带宽消耗和加密/解密延迟，适用于实时AR/VR流。

Abstract: Immersive formats such as 360° and 6DoF point cloud videos require high bandwidth and low latency, posing challenges for real-time AR/VR streaming. This work focuses on reducing bandwidth consumption and encryption/decryption delay, two key contributors to overall latency. We design a system that downsamples point cloud content at the origin server and applies partial encryption. At the client, the content is decrypted and upscaled using an ML-based super-resolution model. Our evaluation demonstrates a nearly linear reduction in bandwidth/latency, and encryption/decryption overhead with lower downsampling resolutions, while the super-resolution model effectively reconstructs the original full-resolution point clouds with minimal error and modest inference time.

</details>


### [324] [Autoencoder-based Denoising Defense against Adversarial Attacks on Object Detection](https://arxiv.org/abs/2512.16123)
*Min Geun Song,Gang Min Kim,Woonmin Kim,Yongsik Kim,Jeonghyun Sim,Sangbeom Park,Huy Kang Kim*

Main category: cs.CR

TL;DR: 提出基于自编码器的去噪防御方法应对深度学习目标检测模型的对抗样本问题，实验表明该方法可部分恢复检测性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习目标检测模型易受对抗样本影响，需提升其抗攻击能力。

Method: 对COCO数据集车辆相关图像用Perlin噪声进行对抗攻击，用单层卷积自编码器去除扰动，用YOLOv5评估检测性能。

Result: 对抗攻击使bbox mAP从0.2890降至0.1640，性能下降43.3%；应用自编码器防御后，bbox mAP提升至0.1700（恢复3.7%），bbox mAP@50从0.2780增至0.3080（提升10.8%）。

Conclusion: 基于自编码器的去噪可在无需重新训练模型的情况下对对抗攻击提供部分防御。

Abstract: Deep learning-based object detection models play a critical role in real-world applications such as autonomous driving and security surveillance systems, yet they remain vulnerable to adversarial examples. In this work, we propose an autoencoder-based denoising defense to recover object detection performance degraded by adversarial perturbations. We conduct adversarial attacks using Perlin noise on vehicle-related images from the COCO dataset, apply a single-layer convolutional autoencoder to remove the perturbations, and evaluate detection performance using YOLOv5. Our experiments demonstrate that adversarial attacks reduce bbox mAP from 0.2890 to 0.1640, representing a 43.3% performance degradation. After applying the proposed autoencoder defense, bbox mAP improves to 0.1700 (3.7% recovery) and bbox mAP@50 increases from 0.2780 to 0.3080 (10.8% improvement). These results indicate that autoencoder-based denoising can provide partial defense against adversarial attacks without requiring model retraining.

</details>


### [325] [In-Context Probing for Membership Inference in Fine-Tuned Language Models](https://arxiv.org/abs/2512.16292)
*Zhexi Lu,Hongliang Chi,Nathalie Baracaldo,Swanand Ravindra Kadhe,Yuseok Jeon,Lei Yu*

Main category: cs.CR

TL;DR: 提出基于训练动力学理论的MIA框架ICP - MIA，在多个任务和模型上效果优于现有黑盒MIA方法，是评估大模型隐私风险实用理论框架。


<details>
  <summary>Details</summary>
Motivation: 现有黑盒成员推理攻击（MIA）技术依赖的信号与样本固有属性纠缠，泛化性差、信噪比低，大模型微调时面临隐私威胁。

Method: 提出ICP - MIA框架，引入优化间隙作为成员身份信号，提出无训练的上下文探测（ICP）方法估计间隙，并给出两种探测策略。

Result: 在三项任务和多个大语言模型上实验表明，ICP - MIA显著优于现有的黑盒MIA方法，尤其是在低误报率情况下。

Conclusion: ICP - MIA是一个实用且有理论支持的框架，可用于审计已部署大语言模型的隐私风险。

Abstract: Membership inference attacks (MIAs) pose a critical privacy threat to fine-tuned large language models (LLMs), especially when models are adapted to domain-specific tasks using sensitive data. While prior black-box MIA techniques rely on confidence scores or token likelihoods, these signals are often entangled with a sample's intrinsic properties - such as content difficulty or rarity - leading to poor generalization and low signal-to-noise ratios. In this paper, we propose ICP-MIA, a novel MIA framework grounded in the theory of training dynamics, particularly the phenomenon of diminishing returns during optimization. We introduce the Optimization Gap as a fundamental signal of membership: at convergence, member samples exhibit minimal remaining loss-reduction potential, while non-members retain significant potential for further optimization. To estimate this gap in a black-box setting, we propose In-Context Probing (ICP), a training-free method that simulates fine-tuning-like behavior via strategically constructed input contexts. We propose two probing strategies: reference-data-based (using semantically similar public samples) and self-perturbation (via masking or generation). Experiments on three tasks and multiple LLMs show that ICP-MIA significantly outperforms prior black-box MIAs, particularly at low false positive rates. We further analyze how reference data alignment, model type, PEFT configurations, and training schedules affect attack effectiveness. Our findings establish ICP-MIA as a practical and theoretically grounded framework for auditing privacy risks in deployed LLMs.

</details>


### [326] [A Systematic Study of Code Obfuscation Against LLM-based Vulnerability Detection](https://arxiv.org/abs/2512.16538)
*Xiao Li,Yue Li,Hao Wu,Yue Zhang,Yechao Zhang,Fengyuan Xu,Sheng Zhong*

Main category: cs.CR

TL;DR: 文章对代码混淆技术进行结构化整理并在统一框架下评估其对基于大语言模型的代码漏洞检测的影响，分析结果并提出未来方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型用于代码漏洞检测时，代码混淆对其可靠性和鲁棒性的影响难以系统评估。

Method: 将现有混淆方法分为三大类11个子类19种具体技术，用一致的大语言模型驱动方法在四种编程语言上实现，评估对15个大语言模型和两个编码代理的影响。

Result: 代码混淆对基于大语言模型的漏洞检测有正负两方面影响，明确了混淆导致性能提升或下降的条件。

Conclusion: 分析结果后指出几个开放问题，提出增强大语言模型在现实漏洞检测中鲁棒性的未来方向。

Abstract: As large language models (LLMs) are increasingly adopted for code vulnerability detection, their reliability and robustness across diverse vulnerability types have become a pressing concern. In traditional adversarial settings, code obfuscation has long been used as a general strategy to bypass auditing tools, preserving exploitability without tampering with the tools themselves. Numerous efforts have explored obfuscation methods and tools, yet their capabilities differ in terms of supported techniques, granularity, and programming languages, making it difficult to systematically assess their impact on LLM-based vulnerability detection. To address this gap, we provide a structured systematization of obfuscation techniques and evaluate them under a unified framework. Specifically, we categorize existing obfuscation methods into three major classes (layout, data flow, and control flow) covering 11 subcategories and 19 concrete techniques. We implement these techniques across four programming languages (Solidity, C, C++, and Python) using a consistent LLM-driven approach, and evaluate their effects on 15 LLMs spanning four model families (DeepSeek, OpenAI, Qwen, and LLaMA), as well as on two coding agents (GitHub Copilot and Codex). Our findings reveal both positive and negative impacts of code obfuscation on LLM-based vulnerability detection, highlighting conditions under which obfuscation leads to performance improvements or degradations. We further analyze these outcomes with respect to vulnerability characteristics, code properties, and model attributes. Finally, we outline several open problems and propose future directions to enhance the robustness of LLMs for real-world vulnerability detection.

</details>


### [327] [Love, Lies, and Language Models: Investigating AI's Role in Romance-Baiting Scams](https://arxiv.org/abs/2512.16280)
*Gilad Gressel,Rahul Pankajakshan,Shir Rozenfeld,Ling Li,Ivan Franceschini,Krishnahsree Achuthan,Yisroel Mirsky*

Main category: cs.CR

TL;DR: 本文研究浪漫诱饵诈骗中LLM的应用，发现LLM已广泛用于诈骗组织，能取得更好效果，且现有防御不足。


<details>
  <summary>Details</summary>
Motivation: 浪漫诱饵诈骗造成危害且基于文本，需探究大语言模型（LLMs）在当前和未来自动化中的作用。

Method: 采访145名内部人员和5名受害者，进行LLM诈骗代理与人类操作者的长期对话研究，评估商业安全过滤器。

Result: LLM已广泛用于诈骗组织，87%诈骗任务易自动化；LLM代理更易获信任和更高请求合规率；流行安全过滤器无法检测浪漫诱饵对话。

Conclusion: 浪漫诱饵诈骗可能实现LLM全面自动化，现有防御不足以阻止其扩张。

Abstract: Romance-baiting scams have become a major source of financial and emotional harm worldwide. These operations are run by organized crime syndicates that traffic thousands of people into forced labor, requiring them to build emotional intimacy with victims over weeks of text conversations before pressuring them into fraudulent cryptocurrency investments. Because the scams are inherently text-based, they raise urgent questions about the role of Large Language Models (LLMs) in both current and future automation.
  We investigate this intersection by interviewing 145 insiders and 5 scam victims, performing a blinded long-term conversation study comparing LLM scam agents to human operators, and executing an evaluation of commercial safety filters. Our findings show that LLMs are already widely deployed within scam organizations, with 87% of scam labor consisting of systematized conversational tasks readily susceptible to automation. In a week-long study, an LLM agent not only elicited greater trust from study participants (p=0.007) but also achieved higher compliance with requests than human operators (46% vs. 18% for humans). Meanwhile, popular safety filters detected 0.0% of romance baiting dialogues. Together, these results suggest that romance-baiting scams may be amenable to full-scale LLM automation, while existing defenses remain inadequate to prevent their expansion.

</details>


### [328] [Beyond the Benchmark: Innovative Defenses Against Prompt Injection Attacks](https://arxiv.org/abs/2512.16307)
*Safwan Shaheer,G. M. Refatul Islam,Mohammad Rafid Hamid,Tahsin Zaman Jilan*

Main category: cs.CR

TL;DR: 论文聚焦小开源模型（LLaMA系列），讨论提示注入攻击安全风险，引入新防御机制，评估其效果，证明能缓解大语言模型目标劫持漏洞。


<details>
  <summary>Details</summary>
Motivation: 大语言模型领域快速发展，小开源模型有广泛部署潜力，但面临提示注入攻击的安全风险，需保障其安全。

Method: 引入能自动生成防御的机制，用种子防御（思维链）框架迭代优化防御提示，评估生成的防御机制应对基准攻击的效果。

Result: 显著降低攻击成功率和误检率，有效检测目标劫持能力。

Conclusion: 为开源大语言模型生态及其安全做出贡献，为资源受限环境下小开源大语言模型更安全高效部署奠定基础。

Abstract: In this fast-evolving area of LLMs, our paper discusses the significant security risk presented by prompt injection attacks. It focuses on small open-sourced models, specifically the LLaMA family of models. We introduce novel defense mechanisms capable of generating automatic defenses and systematically evaluate said generated defenses against a comprehensive set of benchmarked attacks. Thus, we empirically demonstrated the improvement proposed by our approach in mitigating goal-hijacking vulnerabilities in LLMs. Our work recognizes the increasing relevance of small open-sourced LLMs and their potential for broad deployments on edge devices, aligning with future trends in LLM applications. We contribute to the greater ecosystem of open-source LLMs and their security in the following: (1) assessing present prompt-based defenses against the latest attacks, (2) introducing a new framework using a seed defense (Chain Of Thoughts) to refine the defense prompts iteratively, and (3) showing significant improvements in detecting goal hijacking attacks. Out strategies significantly reduce the success rates of the attacks and false detection rates while at the same time effectively detecting goal-hijacking capabilities, paving the way for more secure and efficient deployments of small and open-source LLMs in resource-constrained environments.

</details>


### [329] [Agent Tools Orchestration Leaks More: Dataset, Benchmark, and Mitigation](https://arxiv.org/abs/2512.16310)
*Yuxuan Qiao,Dongqin Liu,Hongchang Yang,Wei Zhou,Songlin Hu*

Main category: cs.CR

TL;DR: 研究单智能体多工具架构下的工具编排隐私风险（TOP - R），建立框架、构建评估基准，提出缓解方法。


<details>
  <summary>Details</summary>
Motivation: 单智能体多工具架构流行但存在新的严重隐私风险TOP - R，需系统研究该风险。

Method: 建立正式框架分析风险根源；构建TOP - Bench评估风险；引入H - Score量化安全与鲁棒性权衡；提出隐私增强原则（PEP）方法缓解风险。

Result: TOP - R是严重风险，8个代表性模型平均风险泄漏率达90.24%，平均H - Score仅0.167；PEP方法有效缓解TOP - R，将风险泄漏率降至46.58%，H - Score提升至0.624。

Conclusion: 揭示新的风险和当前智能体架构的固有结构局限，提供了可行的缓解策略。

Abstract: Driven by Large Language Models, the single-agent, multi-tool architecture has become a popular paradigm for autonomous agents due to its simplicity and effectiveness. However, this architecture also introduces a new and severe privacy risk, which we term Tools Orchestration Privacy Risk (TOP-R), where an agent, to achieve a benign user goal, autonomously aggregates information fragments across multiple tools and leverages its reasoning capabilities to synthesize unexpected sensitive information. We provide the first systematic study of this risk. First, we establish a formal framework, attributing the risk's root cause to the agent's misaligned objective function: an overoptimization for helpfulness while neglecting privacy awareness. Second, we construct TOP-Bench, comprising paired leakage and benign scenarios, to comprehensively evaluate this risk. To quantify the trade-off between safety and robustness, we introduce the H-Score as a holistic metric. The evaluation results reveal that TOP-R is a severe risk: the average Risk Leakage Rate (RLR) of eight representative models reaches 90.24%, while the average H-Score is merely 0.167, with no model exceeding 0.3. Finally, we propose the Privacy Enhancement Principle (PEP) method, which effectively mitigates TOP-R, reducing the Risk Leakage Rate to 46.58% and significantly improving the H-Score to 0.624. Our work reveals both a new class of risk and inherent structural limitations in current agent architectures, while also offering feasible mitigation strategies.

</details>


### [330] [Protecting Deep Neural Network Intellectual Property with Chaos-Based White-Box Watermarking](https://arxiv.org/abs/2512.16658)
*Sangeeth B,Serena Nicolazzo,Deepa K.,Vinod P*

Main category: cs.CR

TL;DR: 为解决DNN知识产权保护问题，本文提出基于混沌序列的白盒水印框架，经实验验证有效，可用于实际场景。


<details>
  <summary>Details</summary>
Motivation: DNNs在多领域快速发展，但其易被复制、再分发或再利用，急需有效机制声明和验证模型所有权。

Method: 使用逻辑映射生成混沌序列作为水印，注入所选中间层权重，不改变模型结构和预测性能；基于遗传算法验证所有权，通过优化提取和重新生成序列的相似度恢复原始混沌参数。

Result: 在MNIST和CIFAR - 10数据集的图像分类任务实验中，微调后水印仍可检测，模型精度损失可忽略；还进行了视觉分析和构建分类器区分不同模型。

Conclusion: 该方法为白盒场景下嵌入和验证模型所有权提供灵活、可扩展的解决方案，适用于IP保护至关重要的现实场景。

Abstract: The rapid proliferation of deep neural networks (DNNs) across several domains has led to increasing concerns regarding intellectual property (IP) protection and model misuse. Trained DNNs represent valuable assets, often developed through significant investments. However, the ease with which models can be copied, redistributed, or repurposed highlights the urgent need for effective mechanisms to assert and verify model ownership. In this work, we propose an efficient and resilient white-box watermarking framework that embeds ownership information into the internal parameters of a DNN using chaotic sequences. The watermark is generated using a logistic map, a well-known chaotic function, producing a sequence that is sensitive to its initialization parameters. This sequence is injected into the weights of a chosen intermediate layer without requiring structural modifications to the model or degradation in predictive performance. To validate ownership, we introduce a verification process based on a genetic algorithm that recovers the original chaotic parameters by optimizing the similarity between the extracted and regenerated sequences. The effectiveness of the proposed approach is demonstrated through extensive experiments on image classification tasks using MNIST and CIFAR-10 datasets. The results show that the embedded watermark remains detectable after fine-tuning, with negligible loss in model accuracy. In addition to numerical recovery of the watermark, we perform visual analyses using weight density plots and construct activation-based classifiers to distinguish between original, watermarked, and tampered models. Overall, the proposed method offers a flexible and scalable solution for embedding and verifying model ownership in white-box settings well-suited for real-world scenarios where IP protection is critical.

</details>


### [331] [PrivateXR: Defending Privacy Attacks in Extended Reality Through Explainable AI-Guided Differential Privacy](https://arxiv.org/abs/2512.16851)
*Ripan Kumar Kundu,Istiak Ahmed,Khaza Anuarul Hoque*

Main category: cs.CR

TL;DR: 本文提出结合可解释AI与差分隐私的框架防御AI XR系统隐私攻击，实验证明能降低攻击成功率、提升推理时间，并在头戴设备上部署。


<details>
  <summary>Details</summary>
Motivation: AI XR技术虽有创新应用，但数据敏感易引发隐私攻击，传统差分隐私应用有局限。

Method: 利用事后解释识别AI XR模型中最具影响力的特征，推理时对这些特征选择性应用差分隐私。

Result: 在网络晕动症任务中，所提方法将MIA和RDA成功率分别降低43%和39%，Transformer模型准确率达97%，推理时间提升约2倍。

Conclusion: 所提方法有效防御隐私攻击，保护用户隐私，且具有实用性。

Abstract: The convergence of artificial AI and XR technologies (AI XR) promises innovative applications across many domains. However, the sensitive nature of data (e.g., eye-tracking) used in these systems raises significant privacy concerns, as adversaries can exploit these data and models to infer and leak personal information through membership inference attacks (MIA) and re-identification (RDA) with a high success rate. Researchers have proposed various techniques to mitigate such privacy attacks, including differential privacy (DP). However, AI XR datasets often contain numerous features, and applying DP uniformly can introduce unnecessary noise to less relevant features, degrade model accuracy, and increase inference time, limiting real-time XR deployment. Motivated by this, we propose a novel framework combining explainable AI (XAI) and DP-enabled privacy-preserving mechanisms to defend against privacy attacks. Specifically, we leverage post-hoc explanations to identify the most influential features in AI XR models and selectively apply DP to those features during inference. We evaluate our XAI-guided DP approach on three state-of-the-art AI XR models and three datasets: cybersickness, emotion, and activity classification. Our results show that the proposed method reduces MIA and RDA success rates by up to 43% and 39%, respectively, for cybersickness tasks while preserving model utility with up to 97% accuracy using Transformer models. Furthermore, it improves inference time by up to ~2x compared to traditional DP approaches. To demonstrate practicality, we deploy the XAI-guided DP AI XR models on an HTC VIVE Pro headset and develop a user interface (UI), namely PrivateXR, allowing users to adjust privacy levels (e.g., low, medium, high) while receiving real-time task predictions, protecting user privacy during XR gameplay.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [332] [Cartesian-nj: Extending e3nn to Irreducible Cartesian Tensor Product and Contracion](https://arxiv.org/abs/2512.16882)
*Zemin Xu,Chenyu Wu,Wenbo Xie,Daiqian Xie,P. Hu*

Main category: physics.chem-ph

TL;DR: 本文引入笛卡尔 - 3j 和笛卡尔 - nj 符号，扩展 e3nn 支持不可约笛卡尔张量积，实现 MACE、NequIP 和 Allegro 的笛卡尔对应版本并进行系统比较，还以 TACE 为例考察相关架构的合理性和改进机会。


<details>
  <summary>Details</summary>
Motivation: 质疑球张量（ST）构造是否是唯一可行的设计原则，推动笛卡尔网络的持续发展。

Method: 引入笛卡尔 - 3j 和笛卡尔 - nj 符号，扩展 e3nn 支持不可约笛卡尔张量积，实现相关模型的笛卡尔对应版本，以 TACE 为例进行考察。

Result: 实现了可进行笛卡尔和球形模型系统比较的框架，发布了 Python 包 cartnn。

Conclusion: 能对笛卡尔和球形模型进行系统比较，还可考察基于不可约笛卡尔张量积和收缩构造的架构在笛卡尔空间的合理性和改进空间。

Abstract: Equivariant atomistic machine learning models have brought substantial gains in both extrapolation capability and predictive accuracy. Depending on the basis of the space, two distinct types of irreducible representations are utilized. From architectures built upon spherical tensors (STs) to more recent formulations employing irreducible Cartesian tensors (ICTs), STs have remained dominant owing to their compactness, elegance, and theoretical completeness. Nevertheless, questions have persisted regarding whether ST constructions are the only viable design principle, motivating continued development of Cartesian networks. In this work, we introduce the Cartesian-3j and Cartesian-nj symbol, which serve as direct analogues of the Wigner-3j and Wigner-nj symbol defined for tensor coupling. These coefficients enable the combination of any two ICTs into a new ICT. Building on this foundation, we extend e3nn to support irreducible Cartesian tensor product, and we release the resulting Python package as cartnn. Within this framework, we implement Cartesian counterparts of MACE, NequIP, and Allegro, allowing the first systematic comparison of Cartesian and spherical models to assess whether Cartesian formulations may offer advantages under specific conditions. Using TACE as a representative example, we further examine whether architectures constructed from irreducible Cartesian tensor product and contraction(ICTP and ICTC) are conceptually well-founded in Cartesian space and whether opportunities remain for improving their design.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [333] [Foundation Models in Biomedical Imaging: Turning Hype into Reality](https://arxiv.org/abs/2512.15808)
*Amgad Muneer,Kai Zhang,Ibraheem Hamdi,Rizwan Qureshi,Muhammad Waqas,Shereen Fouad,Hazrat Ali,Syed Muhammad Anwar,Jia Wu*

Main category: q-bio.QM

TL;DR: 本文探讨生物医学成像领域基础模型的现状、挑战与未来，认为构建因果感知、可验证安全的混合系统很重要。


<details>
  <summary>Details</summary>
Motivation: 基础模型在临床评估和部署中存在显著挑战，需缩小潜力与现实的差距。

Method: 评估技术现状，分析核心能力与局限；提供推理分类；探讨部署问题，关注验证框架。

Result: 指出超越统计关联追求因果推断的重要性，剖析部署中的重要问题。

Conclusion: 自主AI医生仍遥远，未来依赖开发能增强人类专业知识的合适系统。

Abstract: Foundation models (FMs) are driving a prominent shift in artificial intelligence across different domains, including biomedical imaging. These models are designed to move beyond narrow pattern recognition towards emulating sophisticated clinical reasoning, understanding complex spatial relationships, and integrating multimodal data with unprecedented flexibility. However, a critical gap exists between this potential and the current reality, where the clinical evaluation and deployment of FMs are hampered by significant challenges. Herein, we critically assess the current state-of-the-art, analyzing hype by examining the core capabilities and limitations of FMs in the biomedical domain. We also provide a taxonomy of reasoning, ranging from emulated sequential logic and spatial understanding to the integration of explicit symbolic knowledge, to evaluate whether these models exhibit genuine cognition or merely mimic surface-level patterns. We argue that a critical frontier lies beyond statistical correlation, in the pursuit of causal inference, which is essential for building robust models that understand cause and effect. Furthermore, we discuss the paramount issues in deployment stemming from trustworthiness, bias, and safety, dissecting the challenges of algorithmic bias, data bias and privacy, and model hallucinations. We also draw attention to the need for more inclusive, rigorous, and clinically relevant validation frameworks to ensure their safe and ethical application. We conclude that while the vision of autonomous AI-doctors remains distant, the immediate reality is the emergence of powerful technology and assistive tools that would benefit clinical practice. The future of FMs in biomedical imaging hinges not on scale alone, but on developing hybrid, causally aware, and verifiably safe systems that augment, rather than replace, human expertise.

</details>


### [334] [Scalable Agentic Reasoning for Designing Biologics Targeting Intrinsically Disordered Proteins](https://arxiv.org/abs/2512.15930)
*Matthew Sinclair,Moeen Meigooni,Archit Vasan,Ozan Gokdemir,Xinran Lian,Heng Ma,Yadu Babuji,Alexander Brace,Khalid Hossain,Carlo Siebenschuh,Thomas Brettin,Kyle Chard,Christopher Henry,Venkatram Vishwanath,Rick L. Stevens,Ian T. Foster,Arvind Ramanathan*

Main category: q-bio.QM

TL;DR: 由于内在无序蛋白（IDP）缺乏稳定结构难以成药，本文设计实现了可扩展多智能体系统StructBioReasoner用于设计靶向IDP的生物制剂，在两个蛋白上测试表现良好，为大规模平台上的治疗发现奠定基础。


<details>
  <summary>Details</summary>
Motivation: IDP是关键治疗靶点，但缺乏稳定结构使其难以成药，现有计算方法需能推理复杂构象集合并统筹多种计算工具的自主系统实现转化应用。

Method: 设计实现StructBioReasoner，采用基于锦标赛的推理框架，利用专门智能体竞争生成和优化治疗假设，通过可扩展的联合智能体中间件Academy在HPC基础设施上协调执行，集成多种工具和领域知识。

Result: 在Der f 21和NMNAT - 2上进行基准测试，787个Der f 21候选结合剂中超50%在结合自由能上优于文献中的人类设计参考结合剂；在NMNAT - 2的97,066个结合剂中确定了三种结合模式。

Conclusion: StructBioReasoner为百亿亿次计算平台上的IDP治疗发现的智能推理系统奠定了基础。

Abstract: Intrinsically disordered proteins (IDPs) represent crucial therapeutic targets due to their significant role in disease -- approximately 80\% of cancer-related proteins contain long disordered regions -- but their lack of stable secondary/tertiary structures makes them "undruggable". While recent computational advances, such as diffusion models, can design high-affinity IDP binders, translating these to practical drug discovery requires autonomous systems capable of reasoning across complex conformational ensembles and orchestrating diverse computational tools at scale.To address this challenge, we designed and implemented StructBioReasoner, a scalable multi-agent system for designing biologics that can be used to target IDPs. StructBioReasoner employs a novel tournament-based reasoning framework where specialized agents compete to generate and refine therapeutic hypotheses, naturally distributing computational load for efficient exploration of the vast design space. Agents integrate domain knowledge with access to literature synthesis, AI-structure prediction, molecular simulations, and stability analysis, coordinating their execution on HPC infrastructure via an extensible federated agentic middleware, Academy. We benchmark StructBioReasoner across Der f 21 and NMNAT-2 and demonstrate that over 50\% of 787 designed and validated candidates for Der f 21 outperformed the human-designed reference binders from literature, in terms of improved binding free energy. For the more challenging NMNAT-2 protein, we identified three binding modes from 97,066 binders, including the well-studied NMNAT2:p53 interface. Thus, StructBioReasoner lays the groundwork for agentic reasoning systems for IDP therapeutic discovery on Exascale platforms.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [335] [Smart Data Portfolios: A Quantitative Framework for Input Governance in AI](https://arxiv.org/abs/2512.16452)
*A. Talha Yalta,A. Yasemin Yalta*

Main category: cs.CY

TL;DR: 文章引入Smart Data Portfolio (SDP)框架解决AI决策可解释性问题，定义相关指标生成治理效率前沿，通过监管约束数据分配，以电信案例说明其应用。


<details>
  <summary>Details</summary>
Motivation: 随着对公平性、隐私等问题的关注，期望AI决策能够由机构解释并让相关方理解。

Method: 引入SDP框架，定义信息回报和治理调整风险两个指标，监管机构通过风险上限等约束数据分配。

Result: 不同AI服务在共同治理结构下需不同数据组合，框架适用于AI系统大规模部署。

Conclusion: 框架提供了适用于AI系统大规模部署的输入级解释层。

Abstract: Growing concerns about fairness, privacy, robustness, and transparency have made it a central expectation of AI governance that automated decisions be explainable by institutions and intelligible to affected parties. We introduce the Smart Data Portfolio (SDP) framework, which treats data categories as productive but risk-bearing assets, formalizing input governance as an information-risk trade-off. Within this framework, we define two portfolio-level quantities, Informational Return and Governance-Adjusted Risk, whose interaction characterizes data mixtures and generates a Governance-Efficient Frontier. Regulators shape this frontier through risk caps, admissible categories, and weight bands that translate fairness, privacy, robustness, and provenance requirements into measurable constraints on data allocation while preserving model flexibility. A telecommunications illustration shows how different AI services require distinct portfolios within a common governance structure. The framework offers a familiar portfolio logic as an input-level explanation layer suited to the large-scale deployment of AI systems.

</details>


### [336] [Value Lens: Using Large Language Models to Understand Human Values](https://arxiv.org/abs/2512.15722)
*Eduardo de la Cruz Fernández,Marcelo Karanik,Sascha Ossowski*

Main category: cs.CY

TL;DR: 本文提出Value Lens模型，用大语言模型检测人类价值观，分两阶段运行，效果良好。


<details>
  <summary>Details</summary>
Motivation: 自主决策系统需使决策符合人类价值观，要评估决策对人类价值观的反映，需识别行动对价值观的影响。

Method: 提出基于文本的Value Lens模型，分两阶段运行，第一阶段用LLM生成价值观描述并由专家验证，第二阶段用一对LLM，一个检测价值观，另一个作为审查者。

Result: Value Lens模型表现与其他采用不同方法的模型相当，甚至更优。

Conclusion: Value Lens模型在检测人类价值观方面有较好效果。

Abstract: The autonomous decision-making process, which is increasingly applied to computer systems, requires that the choices made by these systems align with human values. In this context, systems must assess how well their decisions reflect human values. To achieve this, it is essential to identify whether each available action promotes or undermines these values. This article presents Value Lens, a text-based model designed to detect human values using generative artificial intelligence, specifically Large Language Models (LLMs). The proposed model operates in two stages: the first aims to formulate a formal theory of values, while the second focuses on identifying these values within a given text. In the first stage, an LLM generates a description based on the established theory of values, which experts then verify. In the second stage, a pair of LLMs is employed: one LLM detects the presence of values, and the second acts as a critic and reviewer of the detection process. The results indicate that Value Lens performs comparably to, and even exceeds, the effectiveness of other models that apply different methods for similar tasks.

</details>


### [337] [Cultural Rights and the Rights to Development in the Age of AI: Implications for Global Human Rights Governance](https://arxiv.org/abs/2512.15786)
*Alexander Kriebitz,Caitlin Corrigan,Aive Pevkur,Alberto Santos Ferro,Amanda Horzyk,Dirk Brand,Dohee Kim,Dodzi Koku Hattoh,Flavia Massucci,Gilles Fayad,Kamil Strzepek,Laud Ammah,Lavina Ramkissoon,Mariette Awad,Natalia Amasiadi,Nathan C. Walker,Nicole Manger,Sophia Devlin*

Main category: cs.CY

TL;DR: 最新人工智能技术挑战文化权利与发展权，论文分析影响、现存治理框架问题，探讨未来研究和政策方向。


<details>
  <summary>Details</summary>
Motivation: 人工智能技术飞速发展，其对文化权利与发展权的保护和实现构成挑战，研究此二者关系并提出应对方法。

Method: 论文考察人工智能对文化权利与发展权的影响，分析人工智能在算法设计和部署中文化与发展假设的认知和规范局限性，以及其对两类权利的个体和结构影响。

Result: 确定了现有AI治理框架在文化权利与发展权方面的差距和矛盾。

Conclusion: 将两类权利置于人工智能与人权背景下，为学术讨论做贡献，并给出未来研究和政策制定方向。

Abstract: Cultural rights and the right to development are essential norms within the wider framework of international human rights law. However, recent technological advances in artificial intelligence (AI) and adjacent digital frontier technologies pose significant challenges to the protection and realization of these rights. This owes to the increasing influence of AI systems on the creation and depiction of cultural content, affect the use and distribution of the intellectual property of individuals and communities, and influence cultural participation and expression worldwide. In addition, the growing influence of AI thus risks exacerbating preexisting economic, social and digital divides and reinforcing inequities for marginalized communities. This dynamic challenges the existing interplay between cultural rights and the right to development, and raises questions about the integration of cultural and developmental considerations into emerging AI governance frameworks. To address these challenges, the paper examines the impact of AI on both categories of rights. Conceptually, it analyzes the epistemic and normative limitations of AI with respect to cultural and developmental assumptions embedded in algorithmic design and deployment, but also individual and structural impacts of AI on both rights. On this basis, the paper identifies gaps and tensions in existing AI governance frameworks with respect to cultural rights and the right to development.
  By situating cultural rights and the right to development within the broader landscape of AI and human rights, this paper contributes to the academic discourse on AI ethics, legal frameworks, and international human rights law. Finally, it outlines avenues for future research and policy development based on existing conversations in global AI governance.

</details>


### [338] [Toward Agentic Environments: GenAI and the Convergence of AI, Sustainability, and Human-Centric Spaces](https://arxiv.org/abs/2512.15787)
*Przemek Pospieszny,Dominika P. Brodowicz*

Main category: cs.CY

TL;DR: 介绍了以可持续性为导向的人工智能框架——能动环境，通过多视角研究，表明其在资源利用和数据隐私方面的潜力并给出建议。


<details>
  <summary>Details</summary>
Motivation: 现有以云为中心的AI部署因高计算需求有重大环境影响，需一种新的可持续AI框架。

Method: 采用二次研究，以及对顶尖科技公司AI专业人士的焦点小组和半结构化访谈获取一手数据，从个人、专业商业、城市运营三个视角检验能动环境的概念框架。

Result: 能动环境通过优化资源利用和加强数据隐私，有潜力促进可持续生态系统。

Conclusion: 建议采用边缘驱动部署模型，减少对高能耗云基础设施的依赖。

Abstract: In recent years, advances in artificial intelligence (AI), particularly generative AI (GenAI) and large language models (LLMs), have made human-computer interactions more frequent, efficient, and accessible across sectors ranging from banking to healthcare. AI tools embedded in digital devices support decision-making and operational management at both individual and organizational levels, including resource allocation, workflow automation, and real-time data analysis. However, the prevailing cloud-centric deployment of AI carries a substantial environmental footprint due to high computational demands. In this context, this paper introduces the concept of agentic environments, a sustainability-oriented AI framework that extends beyond reactive systems by leveraging GenAI, multi-agent systems, and edge computing to reduce the environmental impact of technology. Agentic environments enable more efficient resource use, improved quality of life, and sustainability-by-design, while simultaneously enhancing data privacy through decentralized, edge-driven solutions. Drawing on secondary research as well as primary data from focus groups and semi-structured interviews with AI professionals from leading technology companies, the paper proposes a conceptual framework for agentic environments examined through three lenses: the personal sphere, professional and commercial use, and urban operations. The findings highlight the potential of agentic environments to foster sustainable ecosystems through optimized resource utilization and strengthened data privacy. The study concludes with recommendations for edge-driven deployment models to reduce reliance on energy-intensive cloud infrastructures.

</details>


### [339] [Evaluation of AI Ethics Tools in Language Models: A Developers' Perspective Case Stud](https://arxiv.org/abs/2512.15791)
*Jhessica Silva,Diego A. B. Moreira,Gabriel O. dos Santos,Alef Ferreira,Helena Maia,Sandra Avila,Helio Pedrini*

Main category: cs.CY

TL;DR: 本文提出评估语言模型中AI伦理工具（AIETs）的方法，对213个AIETs调研后选4个应用于葡萄牙语语言模型评估，结果显示其可作伦理考量指引，但未解决模型独特问题及识别葡语模型潜在负面影响。


<details>
  <summary>Details</summary>
Motivation: 语言模型广泛应用，需负责任地开发和部署，虽AIETs相关研究增加，但很多缺乏文档、用例和实践有效性证明，因此需评估AIETs。

Method: 对213个AIETs进行文献调研，筛选出4个AIETs应用于葡萄牙语语言模型，与开发者进行35小时访谈，从开发者视角评估AIETs使用和质量。

Result: 应用的AIETs可作为语言模型伦理考量的指引，但未解决模型独特方面，如习语表达，也未帮助识别葡语模型潜在负面影响。

Conclusion: 应用的AIETs有一定作用，但存在不足，未能全面解决语言模型伦理问题。

Abstract: In Artificial Intelligence (AI), language models have gained significant importance due to the widespread adoption of systems capable of simulating realistic conversations with humans through text generation. Because of their impact on society, developing and deploying these language models must be done responsibly, with attention to their negative impacts and possible harms. In this scenario, the number of AI Ethics Tools (AIETs) publications has recently increased. These AIETs are designed to help developers, companies, governments, and other stakeholders establish trust, transparency, and responsibility with their technologies by bringing accepted values to guide AI's design, development, and use stages. However, many AIETs lack good documentation, examples of use, and proof of their effectiveness in practice. This paper presents a methodology for evaluating AIETs in language models. Our approach involved an extensive literature survey on 213 AIETs, and after applying inclusion and exclusion criteria, we selected four AIETs: Model Cards, ALTAI, FactSheets, and Harms Modeling. For evaluation, we applied AIETs to language models developed for the Portuguese language, conducting 35 hours of interviews with their developers. The evaluation considered the developers' perspective on the AIETs' use and quality in helping to identify ethical considerations about their model. The results suggest that the applied AIETs serve as a guide for formulating general ethical considerations about language models. However, we note that they do not address unique aspects of these models, such as idiomatic expressions. Additionally, these AIETs did not help to identify potential negative impacts of models for the Portuguese language.

</details>


### [340] [A Systematic Analysis of Biases in Large Language Models](https://arxiv.org/abs/2512.15792)
*Xulang Zhang,Rui Mao,Erik Cambria*

Main category: cs.CY

TL;DR: 研究对四个广泛使用的大语言模型进行全面检查，发现虽模型意在中立公正，但仍存在不同类型的偏差和倾向。


<details>
  <summary>Details</summary>
Motivation: 确保大语言模型在不同情境下保持公平，以实现安全和负责任的部署。

Method: 通过一系列精心设计的实验，在政治、意识形态、联盟、语言和性别维度，分别用新闻摘要、新闻立场分类、联合国投票模式、多语言故事完成和世界价值观调查等方法研究模型。

Result: 大语言模型虽旨在中立公正，但仍表现出不同类型的偏差和倾向。

Conclusion: 大语言模型在保持公平性上仍有改进空间。

Abstract: Large language models (LLMs) have rapidly become indispensable tools for acquiring information and supporting human decision-making. However, ensuring that these models uphold fairness across varied contexts is critical to their safe and responsible deployment. In this study, we undertake a comprehensive examination of four widely adopted LLMs, probing their underlying biases and inclinations across the dimensions of politics, ideology, alliance, language, and gender. Through a series of carefully designed experiments, we investigate their political neutrality using news summarization, ideological biases through news stance classification, tendencies toward specific geopolitical alliances via United Nations voting patterns, language bias in the context of multilingual story completion, and gender-related affinities as revealed by responses to the World Values Survey. Results indicate that while the LLMs are aligned to be neutral and impartial, they still show biases and affinities of different types.

</details>


### [341] [Explainable Ethical Assessment on Human Behaviors by Generating Conflicting Social Norms](https://arxiv.org/abs/2512.15793)
*Yuxi Sun,Wei Gao,Hongzhan Lin,Jing Ma,Wenxuan Zhang*

Main category: cs.CY

TL;DR: 本文介绍了ClarityEthic方法，通过生成人类行为背后的冲突性社会规范增强语言模型的道德推理能力，实验表明该方法优于基线方法且生成规范能合理解释人类行为评估。


<details>
  <summary>Details</summary>
Motivation: 当前基于大规模数据训练但无明确规范的AI系统评估人类行为效价时难以解释且不可信，考虑社会规范可助AI模型更好理解和预测效价，且冲突规范会影响人类行为。

Method: 引入ClarityEthic方法，使用对比学习策略生成人类行为背后的冲突性社会规范以增强效价预测和解释。

Result: 广泛实验表明该方法优于强基线方法，人类评估证实生成的社会规范能为人类行为评估提供合理的解释。

Conclusion: ClarityEthic方法能有效增强语言模型的道德推理能力，提升效价预测和解释效果。

Abstract: Human behaviors are often guided or constrained by social norms, which are defined as shared, commonsense rules. For example, underlying an action ``\textit{report a witnessed crime}" are social norms that inform our conduct, such as ``\textit{It is expected to be brave to report crimes}''. Current AI systems that assess valence (i.e., support or oppose) of human actions by leveraging large-scale data training not grounded on explicit norms may be difficult to explain, and thus untrustworthy. Emulating human assessors by considering social norms can help AI models better understand and predict valence. While multiple norms come into play, conflicting norms can create tension and directly influence human behavior. For example, when deciding whether to ``\textit{report a witnessed crime}'', one may balance \textit{bravery} against \textit{self-protection}. In this paper, we introduce \textit{ClarityEthic}, a novel ethical assessment approach, to enhance valence prediction and explanation by generating conflicting social norms behind human actions, which strengthens the moral reasoning capabilities of language models by using a contrastive learning strategy. Extensive experiments demonstrate that our method outperforms strong baseline approaches, and human evaluations confirm that the generated social norms provide plausible explanations for the assessment of human behaviors.

</details>


### [342] [Cross-Language Bias Examination in Large Language Models](https://arxiv.org/abs/2512.16029)
*Yuxuan Liang,Marwa Mahmoud*

Main category: cs.CY

TL;DR: 提出多语言偏见评估框架评估大语言模型偏见，发现不同语言和偏见类型存在差异，为开发公平多语言模型奠定基础。


<details>
  <summary>Details</summary>
Motivation: 填补跨语言偏见分析研究空白，开发公平有效的多语言大语言模型。

Method: 结合BBQ基准进行显式偏见评估和基于提示的内隐联想测试进行隐式偏见测量，将提示和词表翻译成五种语言进行跨语言比较。

Result: 不同语言偏见存在显著差距，如阿拉伯语和西班牙语刻板印象偏见高，汉语和英语低；不同偏见类型有不同模式，年龄显式偏见低但隐式偏见高。

Conclusion: 大语言模型在语言和偏见维度上差异显著，研究提供的跨语言偏见分析方法为开发公平多语言模型奠定基础。

Abstract: This study introduces an innovative multilingual bias evaluation framework for assessing bias in Large Language Models, combining explicit bias assessment through the BBQ benchmark with implicit bias measurement using a prompt-based Implicit Association Test. By translating the prompts and word list into five target languages, English, Chinese, Arabic, French, and Spanish, we directly compare different types of bias across languages. The results reveal substantial gaps in bias across languages used in LLMs. For example, Arabic and Spanish consistently show higher levels of stereotype bias, while Chinese and English exhibit lower levels of bias. We also identify contrasting patterns across bias types. Age shows the lowest explicit bias but the highest implicit bias, emphasizing the importance of detecting implicit biases that are undetectable with standard benchmarks. These findings indicate that LLMs vary significantly across languages and bias dimensions. This study fills a key research gap by providing a comprehensive methodology for cross-lingual bias analysis. Ultimately, our work establishes a foundation for the development of equitable multilingual LLMs, ensuring fairness and effectiveness across diverse languages and cultures.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [343] [Large Model Enabled Embodied Intelligence for 6G Integrated Perception, Communication, and Computation Network](https://arxiv.org/abs/2512.15109)
*Zhuoran Li,Zhen Gao,Xinhua Liu,Zheng Wang,Xiaotian Zhou,Lei Liu,Yongpeng Wu,Wei Feng,Yongming Huang*

Main category: eess.SP

TL;DR: 本文探讨大人工智能模型赋能智能基站代理，回顾基站演进，提出架构、研究场景、分析技术、给出评估框架并指出挑战，认为其是实现6G系统的可行路径。


<details>
  <summary>Details</summary>
Motivation: 6G将智能置于无线架构核心，融合感知、通信和计算，研究大人工智能模型赋能基站以实现智能基站代理。

Method: 回顾基站历史演进，提出IBSA架构，研究典型场景，分析关键使能技术，给出评估框架。

Result: 分析了关键技术，提出了评估框架和基准考虑因素，提炼了开放挑战。

Conclusion: 大人工智能模型赋能的智能基站代理是实现集成感知、通信和计算的安全关键6G系统的可行路径。

Abstract: The advent of sixth-generation (6G) places intelligence at the core of wireless architecture, fusing perception, communication, and computation into a single closed-loop. This paper argues that large artificial intelligence models (LAMs) can endow base stations with perception, reasoning, and acting capabilities, thus transforming them into intelligent base station agents (IBSAs). We first review the historical evolution of BSs from single-functional analog infrastructure to distributed, software-defined, and finally LAM-empowered IBSA, highlighting the accompanying changes in architecture, hardware platforms, and deployment. We then present an IBSA architecture that couples a perception-cognition-execution pipeline with cloud-edge-end collaboration and parameter-efficient adaptation. Subsequently,we study two representative scenarios: (i) cooperative vehicle-road perception for autonomous driving, and (ii) ubiquitous base station support for low-altitude uncrewed aerial vehicle safety monitoring and response against unauthorized drones. On this basis, we analyze key enabling technologies spanning LAM design and training, efficient edge-cloud inference, multi-modal perception and actuation, as well as trustworthy security and governance. We further propose a holistic evaluation framework and benchmark considerations that jointly cover communication performance, perception accuracy, decision-making reliability, safety, and energy efficiency. Finally, we distill open challenges on benchmarks, continual adaptation, trustworthy decision-making, and standardization. Together, this work positions LAM-enabled IBSAs as a practical path toward integrated perception, communication, and computation native, safety-critical 6G systems.

</details>


### [344] [TinyMyo: a Tiny Foundation Model for Flexible EMG Signal Processing at the Edge](https://arxiv.org/abs/2512.15729)
*Matteo Fasulo,Giusy Spacone,Thorir Mar Ingolfsson,Yawei Li,Luca Benini,Andrea Cossettini*

Main category: eess.SP

TL;DR: 提出轻量级基础模型TinyMyo，可处理多下游任务，泛化能力强，模型小且能部署在超低功耗微控制器，还开源代码。


<details>
  <summary>Details</summary>
Motivation: 现有基于肌电信号的基础模型局限于单下游任务且难部署在嵌入式平台，需解决跨主体、记录系统和采集协议的泛化挑战。

Method: 采用Transformer编码器架构，在公开数据集上自监督预训练，用同一骨干网络经少量任务特定头调整处理多下游任务。

Result: 在多任务中泛化能力强，性能达或超现有水平，模型参数少于5M；在多数据集上取得最优结果；首次在超低功耗微控制器上部署，平均功耗36.45mW。

Conclusion: 开源的TinyMyo为肌电研究社区提供灵活资源，可加速未来研究。

Abstract: Surface electromyography (EMG) is a non-invasive sensing modality used in several domains, including biomechanics, rehabilitation, prosthetic control, and emerging human-machine interaction paradigms. Despite decades of use, significant challenges remain in achieving robust generalization across subjects, recording systems, and acquisition protocols. To tackle these challenges, foundation models (FMs) are gaining traction when targeting end-to-end applications based on EMG signals. Yet, existing EMG FMs remain limited to single downstream tasks and lack deployability on embedded platforms. In this work, we present TinyMyo, a lightweight FM based on a Transformer encoder architecture. The model is pre-trained in a self-supervised manner on publicly available datasets and achieves high reconstruction fidelity with only 3.6M parameters. With minimal task-specific head adaptations, the same backbone is used to tackle multiple downstream tasks, leveraging datasets acquired from diverse sensing locations and hardware platforms. We demonstrate generalization across hand gesture classification, hand kinematic regression, speech production and recognition, with performance comparable to or surpassing the state of the art (SoA), and model size below 5M parameters. We achieve SoA results compared to previous FM-based works on the NinaPro DB5 ($89.4\pm0.16\%$), UCI-EMG ($97.56\pm0.32\%$), and EPN-612 ($96.74\pm0.09\%$) datasets. We report, to the best of our knowledge, the first deployment of an EMG FM on an ultra-low-power microcontroller (GAP9), achieving an average power envelope of 36.45mW. By open-sourcing the pre-trained and the downstream task architectures (https://github.com/pulp-bio/BioFoundation), we aim to provide a flexible resource that can accelerate future research and serve as a common foundation for the EMG community.

</details>


### [345] [Concurrence: A dependence criterion for time series, applied to biological data](https://arxiv.org/abs/2512.16001)
*Evangelos Sariyanidi,John D. Herrington,Lisa Yankowitz,Pratik Chaudhari,Theodore D. Satterthwaite,Casey J. Zampella,Jeffrey S. Morris,Edward Gunning,Robert T. Schultz,Russell T. Shinohara,Birkan Tunc*

Main category: eess.SP

TL;DR: 提出一种依赖准则concurrence，可在无需特殊参数调整和大量数据情况下揭示多种信号间关系。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在无先验知识或大数据集情况下捕捉生物系统复杂非线性相互作用，需要新的依赖测量方法。

Method: 引入依赖准则，若能构建分类器区分两个时间序列的时间对齐与未对齐片段，则认为二者相关。

Result: 该准则concurrence在理论上与依赖相关。

Conclusion: 此准则可成为跨学科科学分析的标准方法。

Abstract: Measuring the statistical dependence between observed signals is a primary tool for scientific discovery. However, biological systems often exhibit complex non-linear interactions that currently cannot be captured without a priori knowledge or large datasets. We introduce a criterion for dependence, whereby two time series are deemed dependent if one can construct a classifier that distinguishes between temporally aligned vs. misaligned segments extracted from them. We show that this criterion, concurrence, is theoretically linked with dependence, and can become a standard approach for scientific analyses across disciplines, as it can expose relationships across a wide spectrum of signals (fMRI, physiological and behavioral data) without ad-hoc parameter tuning or large amounts of data.

</details>


### [346] [Few-Shot Specific Emitter Identification via Integrated Complex Variational Mode Decomposition and Spatial Attention Transfer](https://arxiv.org/abs/2512.16786)
*Chenyu Zhu,Zeyang Li,Ziyi Xie,Jie Zhang*

Main category: eess.SP

TL;DR: 提出集成复变分模态分解算法与结合时空注意力机制和分支网络的模型，用于特定发射器识别，在模拟数据和公开数据集验证效果佳。


<details>
  <summary>Details</summary>
Motivation: 多数基于深度学习的特定发射器识别方法依赖大量数据或先验信息，在有限标注数据的现实场景中存在挑战。

Method: 提出集成复变分模态分解算法对复值信号进行分解和重构，利用时间卷积网络建模信号时序特征，引入空间注意力机制对信号段自适应加权，构建分支网络利用其他数据预训练权重。

Result: 消融实验证明模型各组件有效，在公开数据集上仅用10个符号、无需先验知识识别准确率达96%。

Conclusion: 所提方法能在有限数据和无需先验知识的情况下，有效提升特定发射器识别性能。

Abstract: Specific emitter identification (SEI) utilizes passive hardware characteristics to authenticate transmitters, providing a robust physical-layer security solution. However, most deep-learning-based methods rely on extensive data or require prior information, which poses challenges in real-world scenarios with limited labeled data. We propose an integrated complex variational mode decomposition algorithm that decomposes and reconstructs complex-valued signals to approximate the original transmitted signals, thereby enabling more accurate feature extraction. We further utilize a temporal convolutional network to effectively model the sequential signal characteristics, and introduce a spatial attention mechanism to adaptively weight informative signal segments, significantly enhancing identification performance. Additionally, the branch network allows leveraging pre-trained weights from other data while reducing the need for auxiliary datasets. Ablation experiments on the simulated data demonstrate the effectiveness of each component of the model. An accuracy comparison on a public dataset reveals that our method achieves 96% accuracy using only 10 symbols without requiring any prior knowledge.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [347] [Edge-wise Topological Divergence Gaps: Guiding Search in Combinatorial Optimization](https://arxiv.org/abs/2512.15800)
*Ilya Trofimov,Daria Voronkova,Alexander Mironenko,Anton Dmitriev,Eduard Tulchinskii,Evgeny Burnaev,Serguei Barannikov*

Main category: cs.CG

TL;DR: 提出TSP拓扑反馈机制，基于规范分解定理开发拓扑引导优化，实验显示其性能和收敛性更好。


<details>
  <summary>Details</summary>
Motivation: 为旅行商问题（TSP）引入新机制以提升启发式算法性能。

Method: 分析路径与最小生成树（MST）的差异，提出规范分解定理，开发2 - opt和3 - opt启发式算法的拓扑引导。

Result: 对基于热图方法、TSPLIB和随机实例的路径进行精细优化实验，拓扑引导优化在很多情况下性能更好、收敛更快。

Conclusion: 拓扑引导优化可提升TSP启发式算法性能和收敛速度。

Abstract: We introduce a topological feedback mechanism for the Travelling Salesman Problem (TSP) by analyzing the divergence between a tour and the minimum spanning tree (MST). Our key contribution is a canonical decomposition theorem that expresses the tour-MST gap as edge-wise topology-divergence gaps from the RTD-Lite barcode. Based on this, we develop a topological guidance for 2-opt and 3-opt heuristics that increases their performance. We carry out experiments with fine-optimization of tours obtained from heatmap-based methods, TSPLIB, and random instances. Experiments demonstrate the topology-guided optimization results in better performance and faster convergence in many cases.

</details>


### [348] [Hierarchical Neural Surfaces for 3D Mesh Compression](https://arxiv.org/abs/2512.15985)
*Sai Karthikey Pentapati,Gregoire Phillips,Alan Bovik*

Main category: cs.CG

TL;DR: 提出构建零亏格3D流形紧凑隐式神经表示（INR）的方法，能实时解码任意分辨率/连接性3D网格，兼顾重建质量和压缩表示大小。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注3D几何传统隐式表示的神经对应物，而基于三角形网格的几何表示在行业中广泛使用，构建能生成它的INR研究较少。

Method: 对给定3D网格创建球形参数化，将其表面映射到单位球面上，构建INR编码其表面连续定义的位移向量场以再生原始形状，利用分层结构保证紧凑性。

Result: 可以实时进行解码，在重建质量和压缩表示大小之间取得了最先进的平衡。

Conclusion: 所提方法能有效构建零亏格3D流形的紧凑INR，可从其中解码任意分辨率/连接性的3D网格。

Abstract: Implicit Neural Representations (INRs) have been demonstrated to achieve state-of-the-art compression of a broad range of modalities such as images, videos, 3D surfaces, and audio. Most studies have focused on building neural counterparts of traditional implicit representations of 3D geometries, such as signed distance functions. However, the triangle mesh-based representation of geometry remains the most widely used representation in the industry, while building INRs capable of generating them has been sparsely studied. In this paper, we present a method for building compact INRs of zero-genus 3D manifolds. Our method relies on creating a spherical parameterization of a given 3D mesh - mapping the surface of a mesh to that of a unit sphere - then constructing an INR that encodes the displacement vector field defined continuously on its surface that regenerates the original shape. The compactness of our representation can be attributed to its hierarchical structure, wherein it first recovers the coarse structure of the encoded surface before adding high-frequency details to it. Once the INR is computed, 3D meshes of arbitrary resolution/connectivity can be decoded from it. The decoding can be performed in real time while achieving a state-of-the-art trade-off between reconstruction quality and the size of the compressed representations.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [349] [Strategic Bid Shading in Real-Time Bidding Auctions in Ad Exchange Using Minority Game Theory](https://arxiv.org/abs/2512.15717)
*Dipankar Das*

Main category: econ.TH

TL;DR: 本文用雅虎数据集研究实时竞价（RTB）中的出价阴影行为，结合少数者博弈理论等分析，发现基于少数者的出价策略可降低支出并提高中标率。


<details>
  <summary>Details</summary>
Motivation: 传统拍卖理论在实时广告市场不完全适用，RTB平台中广告商常采用出价阴影策略，需对其进行实证研究。

Method: 利用雅虎Webscope数据集，结合少数者博弈理论、聚类算法和方差缩放诊断，分析按时间分割的印象市场中的均衡出价行为。

Result: 发现基于少数者的出价策略，代理商将广告位划分子市场并策略性出价，可降低支出并提高中标率。

Conclusion: 强调了少数者策略在分散高频拍卖环境中对出价者动态和定价结果的计算和经济影响。

Abstract: Traditional auction theory posits that bid value exhibits a positive correlation with the probability of securing the auctioned object in ascending auctions. However, under uncertainty and incomplete information, as is characteristic in real-time advertising markets, truthful bidding may not always represent a dominant strategy or yield a Pure Strategy Nash Equilibrium. Real-Time Bidding (RTB) platforms operationalize impression-level auctions via programmatic interfaces, where advertisers compete in first-price auction settings and often resort to bid shading, i.e., strategically submitting bids below their private valuations to optimize payoff. 
This paper empirically investigates bid shading behaviors and strategic adaptation using large-scale RTB auction data from the Yahoo Webscope dataset. Integrating Minority Game Theory with clustering algorithms and variance-scaling diagnostics, we analyze equilibrium bidding behavior across temporally segmented impression markets. Our results reveal the emergence of minority-based bidding strategies, wherein agents partition hourly ad slots into submarkets and place bids strategically where they anticipate being in the numerical minority. This strategic heterogeneity facilitates reduced expenditure while enhancing win probability, functioning as an endogenous bid shading mechanism. The analysis highlights the computational and economic implications of minority strategies in shaping bidder dynamics and pricing outcomes in decentralized, high-frequency auction environments.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [350] [Graph Neural Networks for Interferometer Simulations](https://arxiv.org/abs/2512.16051)
*Sidharth Kannan,Pooyan Goodarzi,Evangelos E. Papalexakis,Jonathan W. Richardson*

Main category: astro-ph.IM

TL;DR: 本文将图神经网络（GNNs）应用于仪器设计，以LIGO模拟为例，展示其能准确捕捉光学物理且运行速度快，还给出数据集。


<details>
  <summary>Details</summary>
Motivation: 探索GNNs在物理科学中的新应用，即仪器设计。

Method: 将GNNs应用于LIGO模型模拟。

Result: GNNs能准确捕捉复杂光学物理，运行速度比现有模拟包快815倍。

Conclusion: 指出该问题对机器学习模型的挑战，并提供数据集作为未来研究基准。

Abstract: In recent years, graph neural networks (GNNs) have shown tremendous promise in solving problems in high energy physics, materials science, and fluid dynamics. In this work, we introduce a new application for GNNs in the physical sciences: instrumentation design. As a case study, we apply GNNs to simulate models of the Laser Interferometer Gravitational-Wave Observatory (LIGO) and show that they are capable of accurately capturing the complex optical physics at play, while achieving runtimes 815 times faster than state of the art simulation packages. We discuss the unique challenges this problem provides for machine learning models. In addition, we provide a dataset of high-fidelity optical physics simulations for three interferometer topologies, which can be used as a benchmarking suite for future work in this direction.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [351] [Make the most of what you have: Resource-efficient randomized algorithms for matrix computations](https://arxiv.org/abs/2512.15929)
*Ethan N. Epperly*

Main category: math.NA

TL;DR: 论文探讨如何高效利用信息设计随机算法，分别研究了半正定矩阵低秩近似、隐式矩阵属性估计和超定线性最小二乘问题的随机算法。


<details>
  <summary>Details</summary>
Motivation: 解决如何在有限数据预算下，尽可能高效利用信息设计算法，实现最大速度和精度。

Method: 第一部分探索随机主元Cholesky算法进行半正定矩阵低秩近似；第二部分用留一法开发矩阵属性估计算法；第三部分开发具有反向稳定性的快速随机最小二乘算法。

Result: 随机主元Cholesky算法在低秩近似任务中速度和可靠性优于其他方法；开发了优化的矩阵属性估计算法；解决了随机最小二乘算法在浮点运算中的精度问题。

Conclusion: 通过合适的算法设计，能在有限数据预算下高效利用信息，实现速度和精度的提升。

Abstract: In recent years, randomized algorithms have established themselves as fundamental tools in computational linear algebra, with applications in scientific computing, machine learning, and quantum information science. Many randomized matrix algorithms proceed by first collecting information about a matrix and then processing that data to perform some computational task. This thesis addresses the following question: How can one design algorithms that use this information as efficiently as possible, reliably achieving the greatest possible speed and accuracy for a limited data budget?
  The first part of this thesis focuses on low-rank approximation for positive-semidefinite matrices. Here, the goal is to compute an accurate approximation to a matrix after accessing as few entries of the matrix as possible. This part of the thesis explores the randomly pivoted Cholesky (RPCholesky) algorithm for this task, which achieves a level of speed and reliability greater than other methods for the same problem.
  The second part of this thesis considers the task of estimating attributes of an implicit matrix accessible only by matrix-vector products. This thesis describes the leave-one-out approach to developing matrix attribute estimation algorithms and develops optimized trace, diagonal, and row-norm estimation algorithms.
  The third part of this thesis considers randomized algorithms for overdetermined linear least squares problems. Randomized algorithms for linear-least squares problems are asymptotically faster than any known deterministic algorithm, but recent work has raised questions about the accuracy of these methods in floating point arithmetic. This thesis shows these issues are resolvable by developing fast randomized least-squares problem achieving backward stability, the gold-standard stability guarantee for a numerical algorithm.

</details>


### [352] [Time-Frequency Analysis for Neural Networks](https://arxiv.org/abs/2512.15992)
*Ahmed Abdeljawad,Elena Cordero*

Main category: math.NA

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We develop a quantitative approximation theory for shallow neural networks using tools from time-frequency analysis. Working in weighted modulation spaces $M^{p,q}_m(\mathbf{R}^{d})$, we prove dimension-independent approximation rates in Sobolev norms $W^{n,r}(Ω)$ for networks whose units combine standard activations with localized time-frequency windows. Our main result shows that for $f \in M^{p,q}_m(\mathbf{R}^{d})$ one can achieve \[ \|f - f_N\|_{W^{n,r}(Ω)} \lesssim N^{-1/2}\,\|f\|_{M^{p,q}_m(\mathbf{R}^{d})}, \] on bounded domains, with explicit control of all constants. We further obtain global approximation theorems on $\mathbf{R}^{d}$ using weighted modulation dictionaries, and derive consequences for Feichtinger's algebra, Fourier-Lebesgue spaces, and Barron spaces. Numerical experiments in one and two dimensions confirm that modulation-based networks achieve substantially better Sobolev approximation than standard ReLU networks, consistent with the theoretical estimates.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [353] [Checking the HAL Interface Specification Continuously, Right from the Start](https://arxiv.org/abs/2512.16897)
*Manuel Bentele,Onur Altinordu,Jan Körner,Andreas Podelski,Axel Sikora*

Main category: cs.LO

TL;DR: 本文提出一种新方法，通过在开发伊始就持续检查硬件抽象层（HAL）接口规范，解决软件模型检查在工业实践中应用的不可预测性问题，初步实验评估结果很有前景。


<details>
  <summary>Details</summary>
Motivation: 软件模型检查虽可用于检查应用程序接口规范，但在工业实践中的应用受其不可预测性阻碍，正确使用HAL接口对嵌入式应用至关重要。

Method: 分多个迭代开发嵌入式应用，从仅调用HAL函数的程序骨架开始，逐步添加实际功能，在每一步都检查HAL接口规范。

Result: 初步实验评估中，按照该方法，每一步检查都成功，特别是最终应用程序的检查也成功。

Conclusion: 该方法很有前景，可解决软件模型检查在工业实践应用中的不可预测性问题。

Abstract: The correct use of a Hardware Abstraction Layer (HAL) interface in embedded applications is crucial to prevent malfunctions, crashes, or even hardware damage. Software model checking has been successfully applied to check interface specifications in application programs, but its employment in industrial practice is hindered by its unpredictability (whether it succeeds for a given application program or not). In this paper, we present a novel approach to address this problem by checking the HAL interface specification continuously and right from the start of the development. I.e., we develop an embedded application in several iterations without a formal connection between the steps. The steps start from a program skeleton which does nothing but calling HAL functions. Actual functionality is added consecutively. The HAL interface specification is checked in each step of the sequence. The idea of the approach is to exploit a specific feature of software model checking: Its attempt to compute exactly the abstraction that is needed for the check to succeed may carry over from one step to the next, even if there is no formal connection between the steps. The experience from a preliminary experimental evaluation of our approach in the development of embedded applications is very promising. Following our approach, the check succeeds in each step and in particular in the final application program.

</details>


### [354] [Towards Mass Spectrum Analysis with ASP](https://arxiv.org/abs/2512.16780)
*Nils Küchenmeister,Alex Ivliev,Markus Krötzsch*

Main category: cs.LO

TL;DR: 提出用ASP基于质谱数据发现化学样品分子结构，开发规范表示和ASP实现并评估。


<details>
  <summary>Details</summary>
Motivation: 利用ASP根据质谱测量的元素和结构片段相对丰度来发现化学样品分子结构。

Method: 开发分子结构的规范表示和使用这些定义的ASP实现。

Result: 在大量已知分子结构上评估了实现的正确性，与其他ASP对称破缺方法和商业工具比较了质量和性能。

Conclusion: 未明确提及，但暗示所提出方法有一定价值。

Abstract: We present a new use of Answer Set Programming (ASP) to discover the molecular structure of chemical samples based on the relative abundance of elements and structural fragments, as measured in mass spectrometry. To constrain the exponential search space for this combinatorial problem, we develop canonical representations of molecular structures and an ASP implementation that uses these definitions. We evaluate the correctness of our implementation over a large set of known molecular structures, and we compare its quality and performance to other ASP symmetry-breaking methods and to a commercial tool from analytical chemistry. Under consideration in Theory and Practice of Logic Programming (TPLP).

</details>
