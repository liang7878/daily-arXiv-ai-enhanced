<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 26]
- [cs.CE](#cs.CE) [Total: 3]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.DS](#cs.DS) [Total: 8]
- [cs.GT](#cs.GT) [Total: 2]
- [cs.IR](#cs.IR) [Total: 5]
- [cs.LG](#cs.LG) [Total: 91]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.SE](#cs.SE) [Total: 22]
- [q-fin.CP](#q-fin.CP) [Total: 2]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [q-fin.RM](#q-fin.RM) [Total: 1]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [stat.ML](#stat.ML) [Total: 6]
- [stat.CO](#stat.CO) [Total: 3]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [cs.DM](#cs.DM) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.CR](#cs.CR) [Total: 19]
- [cs.CL](#cs.CL) [Total: 54]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.RO](#cs.RO) [Total: 7]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [stat.ME](#stat.ME) [Total: 3]
- [math.ST](#math.ST) [Total: 2]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.CV](#cs.CV) [Total: 16]
- [cs.ET](#cs.ET) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [stat.AP](#stat.AP) [Total: 1]
- [math.OC](#math.OC) [Total: 3]
- [cs.SD](#cs.SD) [Total: 4]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [eess.SY](#eess.SY) [Total: 2]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.CC](#cs.CC) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [q-bio.PE](#q-bio.PE) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.CY](#cs.CY) [Total: 5]
- [cs.GR](#cs.GR) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [BrowserArena: Evaluating LLM Agents on Real-World Web Navigation Tasks](https://arxiv.org/abs/2510.02418)
*Sagnik Anupam,Davis Brown,Shuo Li,Eric Wong,Hamed Hassani,Osbert Bastani*

Main category: cs.AI

TL;DR: 介绍BrowserArena平台评估开放网络代理，识别三种失败模式，发现不同模型应对差异，体现代理多样性与脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型网络代理评估局限于沙盒环境或人工任务，需评估开放网络代理。

Method: 引入BrowserArena平台，收集用户任务、进行竞技式对比、用人工反馈识别失败模式，构建针对性数据集研究。

Result: 识别出验证码解析、弹窗移除、直接导航到URL三种失败模式，发现不同模型应对失败模式有差异。

Conclusion: 当前网络代理有多样性和脆弱性，该基准测试方法可大规模评估和理解网络代理失败模式。

Abstract: LLM web agents now browse and take actions on the open web, yet current agent
evaluations are constrained to sandboxed environments or artificial tasks. We
introduce BrowserArena, a live open-web agent evaluation platform that collects
user-submitted tasks, runs Arena-style head-to-head comparisons, and uses
step-level human feedback to surface failure modes. Collecting and analyzing
step-level annotations on the agent traces, we identify three consistent
failure modes: captcha resolution, pop-up banner removal, and direct navigation
to URLs. By constructing targeted datasets to further study these tasks, we
discover variations in how different language models navigate these failure
modes. We find, for example, that o4-mini deploys a wider variety of strategies
to circumvent captcha resolution than other models and DeepSeek-R1 consistently
misleads users about captcha resolution. Our findings surface both the
diversity and brittleness of current web agents. More broadly, our benchmarking
methodology provides an approach to evaluating and understanding web agent
failure modes at scale.

</details>


### [2] [RefineShot: Rethinking Cinematography Understanding with Foundational Skill Evaluation](https://arxiv.org/abs/2510.02423)
*Hang Wu,Yujun Cai,Haonan Ge,Hongkai Chen,Ming-Hsuan Yang,Yiwei Wang*

Main category: cs.AI

TL;DR: 分析ShotBench和ShotVL存在的问题，提出改进方法得到RefineShot基准，促进电影摄影理解评估。


<details>
  <summary>Details</summary>
Motivation: 现有ShotBench和ShotVL存在问题，影响评估可靠性，阻碍电影摄影理解领域进步。

Method: 对ShotBench进行选项重构，分析ShotVL推理行为，引入扩展评估协议。

Result: 得到RefineShot基准。

Conclusion: RefineShot能实现更可靠评估，推动电影摄影理解未来发展。

Abstract: Cinematography understanding refers to the ability to recognize not only the
visual content of a scene but also the cinematic techniques that shape
narrative meaning. This capability is attracting increasing attention, as it
enhances multimodal understanding in real-world applications and underpins
coherent content creation in film and media. As the most comprehensive
benchmark for this task, ShotBench spans a wide range of cinematic concepts and
VQA-style evaluations, with ShotVL achieving state-of-the-art results on it.
However, our analysis reveals that ambiguous option design in ShotBench and
ShotVL's shortcomings in reasoning consistency and instruction adherence
undermine evaluation reliability, limiting fair comparison and hindering future
progress. To overcome these issues, we systematically refine ShotBench through
consistent option restructuring, conduct the first critical analysis of
ShotVL's reasoning behavior, and introduce an extended evaluation protocol that
jointly assesses task accuracy and core model competencies. These efforts lead
to RefineShot, a refined and expanded benchmark that enables more reliable
assessment and fosters future advances in cinematography understanding.

</details>


### [3] [Safe and Efficient In-Context Learning via Risk Control](https://arxiv.org/abs/2510.02480)
*Andrea Wynn,Metod Jazbec,Charith Peris,Rinat Khaziev,Anqi Liu,Daniel Khashabi,Eric Nalisnick*

Main category: cs.AI

TL;DR: 提出新方法限制有害演示对大语言模型性能的影响，能控制风险并提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型从上下文示例学习时存在安全隐患，易受错误或恶意演示影响，需内置机制防范攻击。

Method: 先定义模型的“安全”基准行为（零样本性能），应用无分布风险控制（DFRC），利用动态提前退出预测忽略关注不安全输入的注意力头，对DFRC进行修改。

Result: 理论和实证结果表明该方法能有效控制有害上下文演示的风险，同时在有益演示时实现显著的计算效率提升。

Conclusion: 所提方法可限制有害演示降低模型性能的程度，兼具风险控制和效率提升。

Abstract: Large language models (LLMs) demonstrate a remarkable ability to learn new
tasks from a few in-context examples. However, this flexibility introduces
safety concerns: LLMs can be influenced by incorrect or malicious
demonstrations -- for example, if an adversary tampers with or injects harmful
examples without a human supervisor noticing. This motivates principled designs
in which the system itself includes built-in mechanisms to guard against such
attacks. We propose a novel approach to limit the degree to which harmful
demonstrations can degrade model performance. First, we define a baseline
``safe'' behavior for the model -- the model's performance given no in-context
demonstrations (zero-shot). Next, we apply distribution-free risk control
(DFRC) to control the extent to which in-context samples can decay performance
below zero-shot. We achieve this by leveraging dynamic early exit prediction,
ignoring later attention heads that attend the most to the unsafe inputs.
Finally, we propose modifications to DFRC that allow it to both control risk
for harmful inputs \textit{and} leverage performance and efficiency gains on
helpful inputs. We present both theoretical and empirical results showing that
our approach can effectively control risk for harmful in-context demonstrations
while simultaneously achieving substantial computational efficiency gains with
helpful demonstrations.

</details>


### [4] [Multimodal Function Vectors for Spatial Relations](https://arxiv.org/abs/2510.02528)
*Shuhao Fu,Esther Goldberg,Ying Nian Wu,Hongjing Lu*

Main category: cs.AI

TL;DR: 本文研究LMMs处理空间关系任务的机制，提取并操作注意力头激活的功能向量，提升模型在关系任务中的表现，展示其泛化能力。


<details>
  <summary>Details</summary>
Motivation: LMMs在上下文学习中表现出色，但支持任务学习的内在机制不透明，需要探索其处理空间关系的机制。

Method: 运用因果中介分析，在合成和真实图像数据集上识别影响关系预测的注意力头，提取多模态功能向量，微调功能向量并结合解决类比问题。

Result: 提取的功能向量能提高推理时的零样本准确率，微调后显著优于上下文学习基线，可组合解决新的空间关系类比问题。

Conclusion: LMMs在局部内部结构中编码空间关系知识，可系统提取和优化，有助于理解模型模块化和增强对关系推理的控制。

Abstract: Large Multimodal Models (LMMs) demonstrate impressive in-context learning
abilities from limited multimodal demonstrations, yet the internal mechanisms
supporting such task learning remain opaque. Building on prior work of large
language models, we show that a small subset of attention heads in the
vision-language model OpenFlamingo-4B is responsible for transmitting
representations of spatial relations. The activations of these attention heads,
termed function vectors, can be extracted and manipulated to alter an LMM's
performance on relational tasks. First, using both synthetic and real image
datasets, we apply causal mediation analysis to identify attention heads that
strongly influence relational predictions, and extract multimodal function
vectors that improve zero-shot accuracy at inference time. We further
demonstrate that these multimodal function vectors can be fine-tuned with a
modest amount of training data, while keeping LMM parameters frozen, to
significantly outperform in-context learning baselines. Finally, we show that
relation-specific function vectors can be linearly combined to solve analogy
problems involving novel and untrained spatial relations, highlighting the
strong generalization ability of this approach. Our results show that LMMs
encode spatial relational knowledge within localized internal structures, which
can be systematically extracted and optimized, thereby advancing our
understanding of model modularity and enhancing control over relational
reasoning in LMMs.

</details>


### [5] [Orchestrating Human-AI Teams: The Manager Agent as a Unifying Research Challenge](https://arxiv.org/abs/2510.02557)
*Charlie Masters,Advaith Vellanki,Jiangbo Shangguan,Bart Kultys,Jonathan Gilmore,Alastair Moore,Stefano V. Albrecht*

Main category: cs.AI

TL;DR: 论文提出自主代理系统研究愿景，以解决复杂多智能体工作流管理问题，识别四个基础挑战，发布MA - Gym框架，评估发现GPT - 5基管理智能体优化存在困难，最后提及自主管理系统的组织和伦理影响。


<details>
  <summary>Details</summary>
Motivation: 现有代理AI在自动化单个任务上有进展，但复杂多智能体工作流管理仍是难题，因此提出自主代理系统研究愿景。

Method: 将工作流管理形式化为部分可观测随机博弈，识别四个基础挑战，发布MA - Gym开源模拟和评估框架，并评估GPT - 5基管理智能体。

Result: 评估发现GPT - 5基管理智能体在联合优化目标完成、约束遵守和工作流运行时间方面存在困难，表明工作流管理仍是难题。

Conclusion: 讨论了自主管理系统的组织和伦理影响。

Abstract: While agentic AI has advanced in automating individual tasks, managing
complex multi-agent workflows remains a challenging problem. This paper
presents a research vision for autonomous agentic systems that orchestrate
collaboration within dynamic human-AI teams. We propose the Autonomous Manager
Agent as a core challenge: an agent that decomposes complex goals into task
graphs, allocates tasks to human and AI workers, monitors progress, adapts to
changing conditions, and maintains transparent stakeholder communication. We
formalize workflow management as a Partially Observable Stochastic Game and
identify four foundational challenges: (1) compositional reasoning for
hierarchical decomposition, (2) multi-objective optimization under shifting
preferences, (3) coordination and planning in ad hoc teams, and (4) governance
and compliance by design. To advance this agenda, we release MA-Gym, an
open-source simulation and evaluation framework for multi-agent workflow
orchestration. Evaluating GPT-5-based Manager Agents across 20 workflows, we
find they struggle to jointly optimize for goal completion, constraint
adherence, and workflow runtime - underscoring workflow management as a
difficult open problem. We conclude with organizational and ethical
implications of autonomous management systems.

</details>


### [6] [Agentic Additive Manufacturing Alloy Discovery](https://arxiv.org/abs/2510.02567)
*Peter Pak,Achuth Chandrasekhar,Amir Barati Farimani*

Main category: cs.AI

TL;DR: 本文利用大语言模型驱动的智能体系统，实现增材制造中合金发现任务的自动化与加速。


<details>
  <summary>Details</summary>
Motivation: 增材制造中合金发现是复杂挑战，需要多领域专业知识，旨在利用大语言模型智能体系统解决该问题。

Method: 通过模型上下文协议（MCP）调用工具，如进行Thermo - Calc属性图计算和熔合不足工艺图生成，多智能体系统处理复杂用户提示。

Result: 多智能体系统能有效推理复杂用户提示，分析合金可打印性，还能根据工具调用结果动态调整任务轨迹。

Conclusion: 采用该多智能体系统可自动化和加速增材制造领域的合金发现任务。

Abstract: Agentic systems enable the intelligent use of research tooling, augmenting a
researcher's ability to investigate and propose novel solutions to existing
problems. Within Additive Manufacturing (AM), alloy discovery remains a complex
challenge, often requiring expertise in the various domains of materials
science, thermodynamic simulations, and experimental analysis. Large Language
Model (LLM) enabled agents can facilitate this endeavor by utilizing their
extensive knowledge base to dispatch tool calls via Model Context Protocol
(MCP) to perform actions such as Thermo-Calc property diagram calculations and
lack of fusion process map generation. In addition, the multi-agent system
developed in this work is able to effectively reason through complex user
prompts and provide analysis on the printability of proposed alloys. These
agents can dynamically adjust their task trajectory to the outcomes of tool
call results, effectively enabling autonomous decision-making in practical
environments. This work aims to utilize LLM enabled agents to automate and
accelerate the task of alloy discovery within the field of additive
manufacturing and showcase the benefits of adopting this multi-agent system.

</details>


### [7] [A Benchmark Study of Deep Reinforcement Learning Algorithms for the Container Stowage Planning Problem](https://arxiv.org/abs/2510.02589)
*Yunqi Huang,Nishith Chennakeshava,Alexis Carras,Vladislav Neverov,Wei Liu,Aske Plaat,Yingjie Fan*

Main category: cs.AI

TL;DR: 文章开发Gym环境评估五种强化学习算法用于集装箱积载规划，揭示算法性能差异，提供可复用环境。


<details>
  <summary>Details</summary>
Motivation: 传统集装箱积载规划依赖人工，现有强化学习应用缺乏不同算法的系统基准比较。

Method: 开发Gym环境，涵盖多智能体和单智能体的起重机调度，评估DQN、QR - DQN、A2C、PPO和TRPO五种RL算法。

Result: 随着复杂度增加，不同算法性能出现明显差距。

Conclusion: 为集装箱积载规划对多种强化学习方法进行基准测试，提供带起重机调度的可复用Gym环境，为未来研究和实际应用奠定基础。

Abstract: Container stowage planning (CSPP) is a critical component of maritime
transportation and terminal operations, directly affecting supply chain
efficiency. Owing to its complexity, CSPP has traditionally relied on human
expertise. While reinforcement learning (RL) has recently been applied to CSPP,
systematic benchmark comparisons across different algorithms remain limited. To
address this gap, we develop a Gym environment that captures the fundamental
features of CSPP and extend it to include crane scheduling in both multi-agent
and single-agent formulations. Within this framework, we evaluate five RL
algorithms: DQN, QR-DQN, A2C, PPO, and TRPO under multiple scenarios of varying
complexity. The results reveal distinct performance gaps with increasing
complexity, underscoring the importance of algorithm choice and problem
formulation for CSPP. Overall, this paper benchmarks multiple RL methods for
CSPP while providing a reusable Gym environment with crane scheduling, thus
offering a foundation for future research and practical deployment in maritime
logistics.

</details>


### [8] [Geolog-IA: Conversational System for Academic Theses](https://arxiv.org/abs/2510.02653)
*Micaela Fuel Pozo,Andrea Guatumillo Saltos,Yeseña Tipan Llumiquinga,Kelly Lascano Aguirre,Marilyn Castillo Jara,Christian Mejia-Escobar*

Main category: cs.AI

TL;DR: 本文介绍基于AI的地质问答系统Geolog - IA，使用特定模型和架构，评估表现良好，有实用价值。


<details>
  <summary>Details</summary>
Motivation: 开发能自然回答厄瓜多尔中央大学地质论文相关问题的对话系统。

Method: 采用Llama 3.1和Gemini 2.5语言模型，结合RAG架构和SQLite数据库。

Result: 用BLEU指标评估，Geolog - IA平均得分0.87，响应一致性和准确性高，有直观网页界面。

Conclusion: 该工具对教育、培训和研究有重要支持作用，可为其他学科应用奠定基础。

Abstract: This study presents the development of Geolog-IA, a novel conversational
system based on artificial intelligence that responds naturally to questions
about geology theses from the Central University of Ecuador. Our proposal uses
the Llama 3.1 and Gemini 2.5 language models, which are complemented by a
Retrieval Augmented Generation (RAG) architecture and an SQLite database. This
strategy allows us to overcome problems such as hallucinations and outdated
knowledge. The evaluation of Geolog-IA's performance with the BLEU metric
reaches an average of 0.87, indicating high consistency and accuracy in the
responses generated. The system offers an intuitive, web-based interface that
facilitates interaction and information retrieval for directors, teachers,
students, and administrative staff at the institution. This tool can be a key
support in education, training, and research and establishes a basis for future
applications in other disciplines.

</details>


### [9] [Multimodal Large Language Model Framework for Safe and Interpretable Grid-Integrated EVs](https://arxiv.org/abs/2510.02592)
*Jean Douglas Carvalho,Hugo Kenji,Ahmad Mohammad Saber,Glaucia Melo,Max Mauro Dias Santos,Deepa Kundur*

Main category: cs.AI

TL;DR: 本文提出基于多模态大语言模型的框架处理传感器数据，生成自然语言警报，经真实数据验证有效，凸显大语言模型在电动汽车出行中的潜力。


<details>
  <summary>Details</summary>
Motivation: 电动汽车融入智能电网有机会提升交通和能源网络，但确保驾驶员、车辆与环境间安全可解释的交互是关键挑战。

Method: 提出基于多模态大语言模型的框架，结合视觉感知（YOLOv8）、地理编码定位和CAN总线遥测处理多模态传感器数据。

Result: 使用真实数据验证框架在城市驾驶场景中有效，能为关键情况生成上下文感知警报。

Conclusion: 大语言模型可作为电动汽车出行的辅助工具，有利于实现车队协调、负荷预测和交通感知能源规划。

Abstract: The integration of electric vehicles (EVs) into smart grids presents unique
opportunities to enhance both transportation systems and energy networks.
However, ensuring safe and interpretable interactions between drivers,
vehicles, and the surrounding environment remains a critical challenge. This
paper presents a multi-modal large language model (LLM)-based framework to
process multimodal sensor data - such as object detection, semantic
segmentation, and vehicular telemetry - and generate natural-language alerts
for drivers. The framework is validated using real-world data collected from
instrumented vehicles driving on urban roads, ensuring its applicability to
real-world scenarios. By combining visual perception (YOLOv8), geocoded
positioning, and CAN bus telemetry, the framework bridges raw sensor data and
driver comprehension, enabling safer and more informed decision-making in urban
driving scenarios. Case studies using real data demonstrate the framework's
effectiveness in generating context-aware alerts for critical situations, such
as proximity to pedestrians, cyclists, and other vehicles. This paper
highlights the potential of LLMs as assistive tools in e-mobility, benefiting
both transportation systems and electric networks by enabling scalable fleet
coordination, EV load forecasting, and traffic-aware energy planning.
  Index Terms - Electric vehicles, visual perception, large language models,
YOLOv8, semantic segmentation, CAN bus, prompt engineering, smart grid.

</details>


### [10] [AutoMaAS: Self-Evolving Multi-Agent Architecture Search for Large Language Models](https://arxiv.org/abs/2510.02669)
*Bo Ma,Hang Li,ZeHua Hu,XiaoFan Gui,LuYao Liu,Simon Liu*

Main category: cs.AI

TL;DR: 本文提出AutoMaAS框架，通过四项创新实现多智能体架构自动搜索，实验显示性能提升且成本降低，有良好可迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有自动化设计方法难以根据查询复杂度和领域要求调整资源分配，需新方法。

Method: 利用神经架构搜索原理，结合动态算子生命周期管理和自动化机器学习技术，有自动算子生成等四项创新。

Result: 在六个基准测试中，性能提升1.0 - 7.1%，推理成本降低3 - 5%，跨数据集和大模型骨干有良好可迁移性。

Conclusion: AutoMaAS为大语言模型时代的自动化多智能体系统设计建立了新范式。

Abstract: Multi-agent systems powered by large language models have demonstrated
remarkable capabilities across diverse domains, yet existing automated design
approaches seek monolithic solutions that fail to adapt resource allocation
based on query complexity and domain requirements. This paper introduces
AutoMaAS, a self-evolving multi-agent architecture search framework that
leverages neural architecture search principles to automatically discover
optimal agent configurations through dynamic operator lifecycle management and
automated machine learning techniques. Our approach incorporates four key
innovations: (1) automatic operator generation, fusion, and elimination based
on performance-cost analysis, (2) dynamic cost-aware optimization with
real-time parameter adjustment, (3) online feedback integration for continuous
architecture refinement, and (4) enhanced interpretability through decision
tracing mechanisms. Extensive experiments across six benchmarks demonstrate
that AutoMaAS achieves 1.0-7.1\% performance improvement while reducing
inference costs by 3-5\% compared to state-of-the-art methods. The framework
shows superior transferability across datasets and LLM backbones, establishing
a new paradigm for automated multi-agent system design in the era of large
language models.

</details>


### [11] [Mitigating Modal Imbalance in Multimodal Reasoning](https://arxiv.org/abs/2510.02608)
*Chen Henry Wu,Neil Kale,Aditi Raghunathan*

Main category: cs.AI

TL;DR: 研究基础模型在跨模态冲突场景下的联合推理能力，发现存在跨模态注意力失衡问题，提出简单方法可减少失衡并提升下游性能。


<details>
  <summary>Details</summary>
Motivation: 了解基础模型在多模态交互形成跨模态上下文时的联合推理能力。

Method: 在跨模态冲突场景下对基础模型进行实验，分析跨模态注意力情况，提出在训练实例中明确结合多模态的方法。

Result: 基础模型在单模态上下文识别冲突比例达90%，跨模态降至3%，存在跨模态注意力失衡，简单方法可减少失衡。

Conclusion: 系统地解决跨模态上下文问题对构建可靠基础模型很重要。

Abstract: Foundation models (FMs) deployed in real-world tasks such as computer-use
agents must integrate diverse modalities. How good are FMs at performing joint
reasoning, simultaneously reasoning over multiple modalities, especially when
the modalities interact and relate to each other to form cross-modal context?
To better understand this problem, we study FMs on cross-modal conflicts:
scenarios where conflicting evidence is presented across modalities. This
allows us to examine whether FMs prioritize one modality over another or reason
jointly to reconcile the conflict. Our experiments reveal that FMs can
recognize conflicts in unimodal contexts, composed of a single modality, 90% of
the time, but the ratio falls as low as 3% when evidence is split across
modalities -- similar observations hold in cross-lingual contexts, composed of
multiple languages. We trace this failure to cross-modal attention imbalance,
showing that FMs exhibit extreme asymmetry in attention scores,
disproportionately prioritizing certain modalities. We show that cross-modal
attention imbalance does not go away by simply scaling up multimodal or
multilingual datasets blindly, since they lack training examples that
explicitly require cross-modal reasoning. We demonstrate that even a simple and
scalable method of explicitly combining multiple modalities within each
training instance significantly reduces attention imbalance. Reduced attention
imbalance directly translates to improved downstream performance on several
vision-language benchmarks. Our findings underscore the importance of
systematically addressing cross-modal contexts to build reliable foundation
models.

</details>


### [12] [On the Role of Temperature Sampling in Test-Time Scaling](https://arxiv.org/abs/2510.02611)
*Yuheng Wu,Azalia Mirhoseini,Thierry Tambe*

Main category: cs.AI

TL;DR: 研究表明大语言模型测试时扩展（TTS）中样本数K增加不会无限提升准确率，提出沿温度维度扩展可扩大推理边界，有更好效果并设计多温度投票法。


<details>
  <summary>Details</summary>
Motivation: 发现大语言模型TTS中增加样本数K不能无限提升准确率，单温度扩展只能挖掘部分模型潜力，需探索新扩展方式。

Method: 提出沿温度维度扩展的方法，设计多温度投票法。

Result: 在Qwen3和五个推理基准上，温度扩展比单温度TTS额外提升7.3分，使基础模型性能达强化学习训练模型水平。

Conclusion: TTS比以往认为的更强大，温度扩展是释放基础模型潜力的简单有效方法。

Abstract: Large language models (LLMs) can improve reasoning at inference time through
test-time scaling (TTS), where multiple reasoning traces are generated and the
best one is selected. Prior work shows that increasing the number of samples K
steadily improves accuracy. In this paper, we demonstrate that this trend does
not hold indefinitely: at large K, further scaling yields no gains, and certain
hard questions remain unsolved regardless of the number of traces.
Interestingly, we find that different sampling temperatures solve different
subsets of problems, implying that single-temperature scaling explores only
part of a model's potential. We therefore propose scaling along the temperature
dimension, which enlarges the reasoning boundary of LLMs. Averaged over Qwen3
(0.6B, 1.7B, 4B, 8B) and five representative reasoning benchmarks (AIME
2024/2025, MATH500, LiveCodeBench, Hi-ToM), temperature scaling yields an
additional 7.3 points over single-temperature TTS. Temperature scaling also
enables base models to reach performance comparable to reinforcement learning
(RL)-trained counterparts, without additional post-training. We further provide
a comprehensive analysis of this phenomenon and design a multi-temperature
voting method that reduces the overhead of temperature scaling. Overall, our
findings suggest that TTS is more powerful than previously thought, and that
temperature scaling offers a simple and effective way to unlock the latent
potential of base models.

</details>


### [13] [A Concept of Possibility for Real-World Events](https://arxiv.org/abs/2510.02655)
*Daniel G. Schwartz*

Main category: cs.AI

TL;DR: 本文提出新的可能性概念，聚焦现实事件可能性计算，可用于规划问题，给出示例并提及潜在应用。


<details>
  <summary>Details</summary>
Motivation: 提出不同于L.A. Zadeh 1978年提出的标准概念的新可能性概念，聚焦现实事件可能性。

Method: 将事件视为有前提条件和约束条件，根据前提条件成立和约束条件不成立的概率计算事件可能性。

Result: 给出新可能性概念并阐述，提供车辆路线规划示例。

Conclusion: 该理论可用于确定多个计划中最可行的计划，推测能正确捕捉人类关于计划的正常推理，还提及潜在未来应用。

Abstract: This paper offers a new concept of {\it possibility} as an alternative to the
now-a-days standard concept originally introduced by L.A. Zadeh in 1978. This
new version was inspired by the original but, formally, has nothing in common
with it other than that they both adopt the {\L}ukasiewicz multivalent
interpretation of the logical connectives. Moreover, rather than seeking to
provide a general notion of possibility, this focuses specifically on the
possibility of a real-world event. An event is viewed as having prerequisites
that enable its occurrence and constraints that may impede its occurrence, and
the possibility of the event is computed as a function of the probabilities
that the prerequisites hold and the constraints do not. This version of
possibility might appropriately be applied to problems of planning. When there
are multiple plans available for achieving a goal, this theory can be used to
determine which plan is most possible, i.e., easiest or most feasible to
complete. It is speculated that this model of reasoning correctly captures
normal human reasoning about plans. The theory is elaborated and an
illustrative example for vehicle route planning is provided. There is also a
suggestion of potential future applications.

</details>


### [14] [ARMs: Adaptive Red-Teaming Agent against Multimodal Models with Plug-and-Play Attacks](https://arxiv.org/abs/2510.02677)
*Zhaorun Chen,Xun Liu,Mintong Kang,Jiawei Zhang,Minzhou Pan,Shuang Yang,Bo Li*

Main category: cs.AI

TL;DR: 提出ARM以评估VLM安全性，实验效果好，构建数据集提升VLM鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有VLM红队评估工作有局限，缺乏对新兴漏洞的可扩展探索

Method: 提出ARM，有11种新攻击策略，集成17种红队算法，设计分层内存和探索算法

Result: ARM攻击成功率达SOTA，生成实例多样性高，构建含30K+实例的数据集

Conclusion: ARM可有效评估VLM安全，数据集能提升VLM鲁棒性和安全对齐

Abstract: As vision-language models (VLMs) gain prominence, their multimodal interfaces
also introduce new safety vulnerabilities, making the safety evaluation
challenging and critical. Existing red-teaming efforts are either restricted to
a narrow set of adversarial patterns or depend heavily on manual engineering,
lacking scalable exploration of emerging real-world VLM vulnerabilities. To
bridge this gap, we propose ARMs, an adaptive red-teaming agent that
systematically conducts comprehensive risk assessments for VLMs. Given a target
harmful behavior or risk definition, ARMs automatically optimizes diverse
red-teaming strategies with reasoning-enhanced multi-step orchestration, to
effectively elicit harmful outputs from target VLMs. We propose 11 novel
multimodal attack strategies, covering diverse adversarial patterns of VLMs
(e.g., reasoning hijacking, contextual cloaking), and integrate 17 red-teaming
algorithms into ARMs via model context protocol (MCP). To balance the diversity
and effectiveness of the attack, we design a layered memory with an
epsilon-greedy attack exploration algorithm. Extensive experiments on instance-
and policy-based benchmarks show that ARMs achieves SOTA attack success rates,
exceeding baselines by an average of 52.1% and surpassing 90% on
Claude-4-Sonnet. We show that the diversity of red-teaming instances generated
by ARMs is significantly higher, revealing emerging vulnerabilities in VLMs.
Leveraging ARMs, we construct ARMs-Bench, a large-scale multimodal safety
dataset comprising over 30K red-teaming instances spanning 51 diverse risk
categories, grounded in both real-world multimodal threats and regulatory
risks. Safety fine-tuning with ARMs-Bench substantially improves the robustness
of VLMs while preserving their general utility, providing actionable guidance
to improve multimodal safety alignment against emerging threats.

</details>


### [15] [Automated Constraint Specification for Job Scheduling by Regulating Generative Model with Domain-Specific Representation](https://arxiv.org/abs/2510.02679)
*Yu-Zhe Shi,Qiao Xu,Yanjia Li,Mingchen Liu,Huamin Qu,Lecheng Ruan,Qining Wang*

Main category: cs.AI

TL;DR: 本文提出约束中心架构，调节大语言模型进行可靠的生产调度约束自动规范，实验表明该方法优于纯大语言模型方法。


<details>
  <summary>Details</summary>
Motivation: 当前将制造需求转化为正式约束需人工完成，大语言模型直接应用存在问题，需可靠方法实现约束自动规范。

Method: 提出约束中心架构，定义三层层次结构空间，采用特定领域表示，设计自动生产场景适应算法。

Result: 该方法成功平衡大语言模型生成能力和制造系统可靠性要求，在约束规范任务中显著优于纯大语言模型方法。

Conclusion: 所提方法能有效利用大语言模型进行生产调度约束自动规范。

Abstract: Advanced Planning and Scheduling (APS) systems have become indispensable for
modern manufacturing operations, enabling optimized resource allocation and
production efficiency in increasingly complex and dynamic environments. While
algorithms for solving abstracted scheduling problems have been extensively
investigated, the critical prerequisite of specifying manufacturing
requirements into formal constraints remains manual and labor-intensive.
Although recent advances of generative models, particularly Large Language
Models (LLMs), show promise in automating constraint specification from
heterogeneous raw manufacturing data, their direct application faces challenges
due to natural language ambiguity, non-deterministic outputs, and limited
domain-specific knowledge. This paper presents a constraint-centric
architecture that regulates LLMs to perform reliable automated constraint
specification for production scheduling. The architecture defines a
hierarchical structural space organized across three levels, implemented
through domain-specific representation to ensure precision and reliability
while maintaining flexibility. Furthermore, an automated production scenario
adaptation algorithm is designed and deployed to efficiently customize the
architecture for specific manufacturing configurations. Experimental results
demonstrate that the proposed approach successfully balances the generative
capabilities of LLMs with the reliability requirements of manufacturing
systems, significantly outperforming pure LLM-based approaches in constraint
specification tasks.

</details>


### [16] [NCV: A Node-Wise Consistency Verification Approach for Low-Cost Structured Error Localization in LLM Reasoning](https://arxiv.org/abs/2510.02816)
*Yulong Zhang,Li Wang,Wei Du,Peilin Li,Yuqin Dai Zhiyuan Zhao,Lingyong Fang,Ziniu Liu,Ru Zhang,Huijia Zhu,Gongshen Liu*

Main category: cs.AI

TL;DR: 提出无训练框架NCV用于大语言模型多步推理验证，实验表明其提升可解释性和效率，在公开数据集上F1分数有提升且使用更少token。


<details>
  <summary>Details</summary>
Motivation: 大语言模型多步推理验证存在误差定位不精确和token成本高的问题，现有方法存在不足。

Method: 引入无训练框架Node-wise Consistency Verification (NCV)，将验证转化为节点级轻量级二元一致性检查，分解思维链为相互连接的验证节点。

Result: 在公开数据集上，NCV的F1分数比基线提高10% - 25%，比传统方法使用少6 - 58倍的token。

Conclusion: NCV提升了可解释性和效率，为可靠的大语言模型推理验证提供了可扩展的解决方案。

Abstract: Verifying multi-step reasoning in large language models is difficult due to
imprecise error localization and high token costs. Existing methods either
assess entire reasoning chains, suffering attention dilution, or rely on
expensive multi-sampling. We introduce Node-wise Consistency Verification
(NCV), a training-free framework that recasts verification as lightweight
binary consistency checks at the node level. By decomposing the chain of
thought into interconnected verification nodes, NCV precisely localizes errors
and avoids unnecessary long-form generation. Experiments demonstrate that our
approach enhances interpretability and efficiency, presenting a scalable
solution for reliable LLM reasoning verification. On public datasets, NCV
achieves a 10\% to 25\% improvement in F1 scores over baselines while utilizing
$6\times$~$58\times$ fewer tokens than traditional methods like CoT-based
verifiers.

</details>


### [17] [Beyond the Final Answer: Evaluating the Reasoning Trajectories of Tool-Augmented Agents](https://arxiv.org/abs/2510.02837)
*Wonjoong Kim,Sangwu Park,Yeonjun In,Sein Kim,Dongha Lee,Chanyoung Park*

Main category: cs.AI

TL;DR: 现有工具增强基准评估方法多限于答案匹配，无法全面评估。本文引入TRACE框架对工具增强大语言模型代理性能进行多维度评估，结果表明其能有效评估且具可扩展性和成本效益。


<details>
  <summary>Details</summary>
Motivation: 当前工具增强基准评估方法大多局限于答案匹配，随着解决用户请求步骤增多，需超越最终答案评估问题解决轨迹，而现有评估方法存在局限性。

Method: 引入TRACE框架，利用证据库积累推理步骤知识以有效分析和评估推理轨迹；开发新的元评估数据集，用多维度性能分数标注不同轨迹。

Result: TRACE能准确评估复杂行为，具有可扩展性和成本效益，即使使用小型开源大语言模型也可行。

Conclusion: TRACE框架可有效评估工具增强大语言模型代理在解决任务时的推理轨迹，能发现新观察和见解。

Abstract: Although recent tool-augmented benchmarks incorporate complex user requests
and diverse tools, the evaluation methods for most of them remain limited to
answer matching. However, as the number of steps required to resolve a user
request increases, a proper evaluation of an agent's performance must go beyond
the final answer to also assess the problem-solving trajectory, including
previously ignored aspects such as efficiency, hallucination, and adaptivity.
The most straightforward method for evaluating these aspects is to compare an
agent's trajectory with the ground-truth trajectory, but this approach is
fundamentally limited since annotating all valid ground-truth trajectories is
prohibitively expensive. However, a simple LLM-based evaluator struggles to
assess trajectories in detail without ground truth. To effectively evaluate the
agents in this manner, we introduce TRACE, a framework for the
multi-dimensional evaluation of tool-augmented LLM agent performance. By
incorporating an evidence bank, which accumulates knowledge gathered from
preceding reasoning steps, TRACE enables a multi-faceted analysis and
evaluation of an agent's reasoning trajectory effectively. To validate our
framework, we develop a new meta-evaluation dataset by augmenting existing
benchmarks with diverse and flawed trajectories, each labeled with
multi-faceted performance scores. Our results confirm that TRACE accurately
evaluates these complex behaviors in a scalable and cost-effective manner, even
with small open-source LLMs. Furthermore, we apply our method to evaluate the
trajectories that agents produce while solving tool-augmented tasks, presenting
previously unreported observations and their corresponding insights.

</details>


### [18] [Take Goodhart Seriously: Principled Limit on General-Purpose AI Optimization](https://arxiv.org/abs/2510.02840)
*Antoine Maier,Aude Maier,Tom David*

Main category: cs.AI

TL;DR: 指出机器学习中目标满足假设（OSA）在现实中不成立，因存在多种误差和目标误设，通用人工智能系统优化需有原则性限制，否则会失控。


<details>
  <summary>Details</summary>
Motivation: 探讨机器学习中常被忽视的训练模型是否满足指定目标函数的假设（OSA）及其影响。

Method: 在与学习范式无关的框架下分析，结合近期数学结果，指出OSA在现实条件下的问题及与古德哈特定律失效模式的关联。

Result: 发现OSA在现实中失败，误差和目标误设不可避免，且与古德哈特定律失效模式难以区分。

Conclusion: 通用人工智能系统的优化需要有原则性限制，否则会导致不可控后果。

Abstract: A common but rarely examined assumption in machine learning is that training
yields models that actually satisfy their specified objective function. We call
this the Objective Satisfaction Assumption (OSA). Although deviations from OSA
are acknowledged, their implications are overlooked. We argue, in a
learning-paradigm-agnostic framework, that OSA fails in realistic conditions:
approximation, estimation, and optimization errors guarantee systematic
deviations from the intended objective, regardless of the quality of its
specification. Beyond these technical limitations, perfectly capturing and
translating the developer's intent, such as alignment with human preferences,
into a formal objective is practically impossible, making misspecification
inevitable. Building on recent mathematical results, absent a mathematical
characterization of these gaps, they are indistinguishable from those that
collapse into Goodhart's law failure modes under strong optimization pressure.
Because the Goodhart breaking point cannot be located ex ante, a principled
limit on the optimization of General-Purpose AI systems is necessary. Absent
such a limit, continued optimization is liable to push systems into predictable
and irreversible loss of control.

</details>


### [19] [Reward Model Routing in Alignment](https://arxiv.org/abs/2510.02850)
*Xinle Wu,Yao Lu*

Main category: cs.AI

TL;DR: 提出BayesianRouter混合路由框架解决单奖励模型问题，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于单奖励模型的强化学习对齐大语言模型的方法存在对齐质量受限和过拟合风险，且现有路由方法有冷启动和探索不足问题。

Method: 提出BayesianRouter，结合离线奖励模型优势学习和在线贝叶斯选择，离线训练多任务路由器估计可靠性，在线用贝叶斯汤普森采样路由器进行选择。

Result: 在指令遵循和推理基准测试中，BayesianRouter始终优于单个奖励模型、奖励模型集成和现有路由方法。

Conclusion: BayesianRouter是解决单奖励模型问题的有效方法，能提升对齐大语言模型的效果。

Abstract: Reinforcement learning from human or AI feedback (RLHF / RLAIF) has become
the standard paradigm for aligning large language models (LLMs). However, most
pipelines rely on a single reward model (RM), limiting alignment quality and
risking overfitting. Recent work explores RM routing--dynamically selecting an
RM from a candidate pool to exploit complementary strengths while maintaining
$O(1)$ RM calls--but existing methods suffer from cold-start and insufficient
exploration. We propose BayesianRouter, a hybrid routing framework that
combines offline RM strengths learning with online Bayesian selection. In the
offline stage, a multi-task router is trained on preference data to estimate
per-RM reliability. In the online stage, a Bayesian Thompson sampling router
performs per-query RM selection, initializing RM-specific weight vectors with
offline embeddings as Gaussian priors and adaptively updating their posteriors
with online rewards to adapt to the evolving policy distribution. Extensive
experiments on instruction-following (AlpacaEval-2, Arena-Hard, MT-Bench) and
reasoning (GSM8K, MMLU) benchmarks show that BayesianRouter consistently
outperforms individual RMs, RM ensembling, and existing routing methods.

</details>


### [20] [Consolidating Reinforcement Learning for Multimodal Discrete Diffusion Models](https://arxiv.org/abs/2510.02880)
*Tianren Ma,Mu Zhang,Yibing Wang,Qixiang Ye*

Main category: cs.AI

TL;DR: 提出MaskGRPO方法解决离散扩散模型奖励优化难题，在多基准测试表现良好。


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型奖励优化有挑战，现有强化学习方法难以处理。

Method: 先明确DDM理论基础构建重要性估计器，再为视觉序列定制滚动方法。

Result: MaskGRPO在推理、编码和视觉生成基准测试中更新更稳定高效，推理和生成质量更好。

Conclusion: MaskGRPO是系统的策略优化方法和离散视觉扩散的实用途径。

Abstract: Optimizing discrete diffusion model (DDM) with rewards remains a challenge:
the non-autoregressive paradigm makes importance sampling intractable and
rollout complex, puzzling reinforcement learning methods such as Group Relative
Policy Optimization (GRPO). In this study, we introduce MaskGRPO, the first
viable approach to enable scalable multimodal reinforcement learning in
discrete diffusion with effective importance sampling and modality-specific
adaptations. To this end, we first clarify the theoretical foundation for DDMs,
which facilitates building an importance estimator that captures valuable token
fluctuation for gradient updates. We then delicately tailored the rollout
method for visual sequences, which yields diverse completions and reliable
optimization gradients. Upon math reasoning, coding, and visual generation
benchmarks, MaskGRPO brings more stable and efficient updates, leading to
stronger reasoning performance and better generation quality. This study
establishes MaskGRPO as a systematic policy optimization approach and the first
practical way for discretized visual diffusion.

</details>


### [21] [Onto-Epistemological Analysis of AI Explanations](https://arxiv.org/abs/2510.02996)
*Martina Mattioli,Eike Petersen,Aasa Feragen,Marcello Pelillo,Siavash A. Bigdeli*

Main category: cs.AI

TL;DR: 指出XAI方法假设非无害，分析其本体和认识论假设，强调选方法时不能忽视范式并讨论如何适配不同领域。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习黑盒系统缺乏可解释性限制其应用，XAI方法有技术背景假设且基础概念有哲学争议，需探讨假设影响。

Method: 对可解释性方法应用于AI系统时的本体和认识论假设进行研究分析。

Result: 发现XAI方法看似小的技术变化对应解释假设的重要差异。

Conclusion: 强调选XAI方法不能忽视潜在的本体 - 认识论范式，要为不同应用领域选择和调整合适方法。

Abstract: Artificial intelligence (AI) is being applied in almost every field. At the
same time, the currently dominant deep learning methods are fundamentally
black-box systems that lack explanations for their inferences, significantly
limiting their trustworthiness and adoption. Explainable AI (XAI) methods aim
to overcome this challenge by providing explanations of the models' decision
process. Such methods are often proposed and developed by engineers and
scientists with a predominantly technical background and incorporate their
assumptions about the existence, validity, and explanatory utility of different
conceivable explanatory mechanisms. However, the basic concept of an
explanation -- what it is, whether we can know it, whether it is absolute or
relative -- is far from trivial and has been the subject of deep philosophical
debate for millennia. As we point out here, the assumptions incorporated into
different XAI methods are not harmless and have important consequences for the
validity and interpretation of AI explanations in different domains. We
investigate ontological and epistemological assumptions in explainability
methods when they are applied to AI systems, meaning the assumptions we make
about the existence of explanations and our ability to gain knowledge about
those explanations. Our analysis shows how seemingly small technical changes to
an XAI method may correspond to important differences in the underlying
assumptions about explanations. We furthermore highlight the risks of ignoring
the underlying onto-epistemological paradigm when choosing an XAI method for a
given application, and we discuss how to select and adapt appropriate XAI
methods for different domains of application.

</details>


### [22] [From Facts to Foils: Designing and Evaluating Counterfactual Explanations for Smart Environments](https://arxiv.org/abs/2510.03078)
*Anna Trapp,Mersedeh Sadeghi,Andreas Vogelsang*

Main category: cs.AI

TL;DR: 本文首次提出并实现适用于基于规则的智能环境的反事实解释，通过用户研究对比其与传统因果解释，结果显示用户偏好高度依赖上下文。


<details>
  <summary>Details</summary>
Motivation: 基于规则的智能环境需要可解释性，而现有领域缺乏生成反事实解释的既定方法。

Method: 将反事实解释实现为扩展现有智能环境解释引擎的插件，并开展用户研究（N = 17）对比反事实解释和传统因果解释。

Result: 用户偏好高度依赖上下文，时间紧迫时倾向因果解释，解决问题时偏好反事实解释。

Conclusion: 为智能环境提供新解释类型的实用框架，并给出选择不同解释类型的经验依据。

Abstract: Explainability is increasingly seen as an essential feature of rule-based
smart environments. While counterfactual explanations, which describe what
could have been done differently to achieve a desired outcome, are a powerful
tool in eXplainable AI (XAI), no established methods exist for generating them
in these rule-based domains. In this paper, we present the first formalization
and implementation of counterfactual explanations tailored to this domain. It
is implemented as a plugin that extends an existing explanation engine for
smart environments. We conducted a user study (N=17) to evaluate our generated
counterfactuals against traditional causal explanations. The results show that
user preference is highly contextual: causal explanations are favored for their
linguistic simplicity and in time-pressured situations, while counterfactuals
are preferred for their actionable content, particularly when a user wants to
resolve a problem. Our work contributes a practical framework for a new type of
explanation in smart environments and provides empirical evidence to guide the
choice of when each explanation type is most effective.

</details>


### [23] [A Study of Rule Omission in Raven's Progressive Matrices](https://arxiv.org/abs/2510.03127)
*Binze Li*

Main category: cs.AI

TL;DR: 研究现代AI系统在不完整训练条件下的泛化能力，发现当前模型有局限性，需向稳健抽象推理发展。


<details>
  <summary>Details</summary>
Motivation: 不清楚现有模型在RPM任务上的表现是真正推理能力还是依赖统计捷径，要研究现代AI系统在不完整训练下的泛化能力。

Method: 在训练中故意省略几个结构规则，在I - RAVEN数据集上评估序列到序列变压器模型和基于视觉的架构。

Result: 变压器模型在熟悉规则上表现好，但面对新规则或省略规则时准确率急剧下降，且存在标记级准确率和完整答案准确率的差距。

Conclusion: 研究为深度学习模型推理机制提供新见解，强调需超越模式识别、实现稳健抽象推理的架构。

Abstract: Analogical reasoning lies at the core of human cognition and remains a
fundamental challenge for artificial intelligence. Raven's Progressive Matrices
(RPM) serve as a widely used benchmark to assess abstract reasoning by
requiring the inference of underlying structural rules. While many vision-based
and language-based models have achieved success on RPM tasks, it remains
unclear whether their performance reflects genuine reasoning ability or
reliance on statistical shortcuts. This study investigates the generalization
capacity of modern AI systems under conditions of incomplete training by
deliberately omitting several structural rules during training. Both
sequence-to-sequence transformer models and vision-based architectures such as
CoPINet and the Dual-Contrast Network are evaluated on the Impartial-RAVEN
(I-RAVEN) dataset. Experiments reveal that although transformers demonstrate
strong performance on familiar rules, their accuracy declines sharply when
faced with novel or omitted rules. Moreover, the gap between token-level
accuracy and complete answer accuracy highlights fundamental limitations in
current approaches. These findings provide new insights into the reasoning
mechanisms underlying deep learning models and underscore the need for
architectures that move beyond pattern recognition toward robust abstract
reasoning.

</details>


### [24] [Improving Cooperation in Collaborative Embodied AI](https://arxiv.org/abs/2510.03153)
*Hima Jacob Leven Suprabha,Laxmi Nag Laxminarayan Nagesh,Ajith Nair,Alvin Reuben Amal Selvaster,Ayan Khan,Raghuram Damarla,Sanju Hannah Samuel,Sreenithi Saravana Perumal,Titouan Puech,Venkataramireddy Marella,Vishal Sonar,Alessandro Suglia,Oliver Lemon*

Main category: cs.AI

TL;DR: 本文探索大语言模型集成到多智能体系统中的提示方法，优化CoELA框架，实验找出提升协作性能的组合，还集成语音能力，结果显示提示优化有效且语音集成提供更好界面。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型集成到多智能体系统中不同提示方法，提升智能体协作行为和决策能力。

Method: 增强CoELA框架，进行系统实验，研究不同大语言模型和提示工程策略，还集成语音能力。

Result: 提示优化能提升协作智能体性能，如最佳组合使Gemma3系统效率比原CoELA系统提高22%，语音集成提供更具吸引力的用户界面。

Conclusion: 提示优化对提升协作智能体性能有效，语音集成利于系统开发和演示。

Abstract: The integration of Large Language Models (LLMs) into multiagent systems has
opened new possibilities for collaborative reasoning and cooperation with AI
agents. This paper explores different prompting methods and evaluates their
effectiveness in enhancing agent collaborative behaviour and decision-making.
We enhance CoELA, a framework designed for building Collaborative Embodied
Agents that leverage LLMs for multi-agent communication, reasoning, and task
coordination in shared virtual spaces. Through systematic experimentation, we
examine different LLMs and prompt engineering strategies to identify optimised
combinations that maximise collaboration performance. Furthermore, we extend
our research by integrating speech capabilities, enabling seamless
collaborative voice-based interactions. Our findings highlight the
effectiveness of prompt optimisation in enhancing collaborative agent
performance; for example, our best combination improved the efficiency of the
system running with Gemma3 by 22% compared to the original CoELA system. In
addition, the speech integration provides a more engaging user interface for
iterative system development and demonstrations.

</details>


### [25] [CoDA: Agentic Systems for Collaborative Data Visualization](https://arxiv.org/abs/2510.03194)
*Zichen Chen,Jiefeng Chen,Sercan Ö. Arik,Misha Sra,Tomas Pfister,Jinsung Yoon*

Main category: cs.AI

TL;DR: 本文提出多智能体系统CoDA用于自然语言查询的可视化自动化，评估显示其表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 当前系统在处理含多文件的复杂数据集及迭代优化时存在困难，现有方法常简化任务，难以处理数据复杂性等问题。

Method: 将挑战重构为协作式多智能体问题，引入CoDA系统，使用专门的大语言模型智能体进行元数据分析、任务规划、代码生成和自我反思。

Result: 广泛评估显示CoDA在总体得分上有显著提升，比竞争基线高出达41.5%。

Conclusion: 可视化自动化的未来在于集成、协作的智能体工作流程，而非孤立的代码生成。

Abstract: Deep research has revolutionized data analysis, yet data scientists still
devote substantial time to manually crafting visualizations, highlighting the
need for robust automation from natural language queries. However, current
systems struggle with complex datasets containing multiple files and iterative
refinement. Existing approaches, including simple single- or multi-agent
systems, often oversimplify the task, focusing on initial query parsing while
failing to robustly manage data complexity, code errors, or final visualization
quality. In this paper, we reframe this challenge as a collaborative
multi-agent problem. We introduce CoDA, a multi-agent system that employs
specialized LLM agents for metadata analysis, task planning, code generation,
and self-reflection. We formalize this pipeline, demonstrating how
metadata-focused analysis bypasses token limits and quality-driven refinement
ensures robustness. Extensive evaluations show CoDA achieves substantial gains
in the overall score, outperforming competitive baselines by up to 41.5%. This
work demonstrates that the future of visualization automation lies not in
isolated code generation but in integrated, collaborative agentic workflows.

</details>


### [26] [Coevolutionary Continuous Discrete Diffusion: Make Your Diffusion Language Model a Latent Reasoner](https://arxiv.org/abs/2510.03206)
*Cai Zhou,Chenxiao Yang,Yi Hu,Chenyu Wang,Chubin Zhang,Muhan Zhang,Lester Mackey,Tommi Jaakkola,Stephen Bates,Dinghuai Zhang*

Main category: cs.AI

TL;DR: 本文探讨扩散语言模型，证明连续扩散模型有更强表达能力，提出CCDD方法结合连续和离散空间，实验表现好。


<details>
  <summary>Details</summary>
Motivation: 虽有理论显示连续扩散模型优势，但其实验表现不如离散模型，要解决理论表达能力和实证性能的矛盾。

Method: 提出Coevolutionary Continuous Discrete Diffusion (CCDD)，在连续表示空间和离散令牌空间的并集上定义联合多模态扩散过程，同时去噪；还提出有效架构和训练/采样技术。

Result: CCDD在实际语言建模实验中展现出强大的实证性能。

Conclusion: 连续扩散模型有更强表达能力，CCDD方法结合两种模态，兼具表达性、可训练性和样本质量。

Abstract: Diffusion language models, especially masked discrete diffusion models, have
achieved great success recently. While there are some theoretical and primary
empirical results showing the advantages of latent reasoning with looped
transformers or continuous chain-of-thoughts, continuous diffusion models
typically underperform their discrete counterparts. In this paper, we argue
that diffusion language models do not necessarily need to be in the discrete
space. In particular, we prove that continuous diffusion models have stronger
expressivity than discrete diffusions and looped transformers. We attribute the
contradiction between the theoretical expressiveness and empirical performance
to their practical trainability: while continuous diffusion provides
intermediate supervision that looped transformers lack, they introduce
additional difficulty decoding tokens into the discrete token space from the
continuous representation space. We therefore propose Coevolutionary Continuous
Discrete Diffusion (CCDD), which defines a joint multimodal diffusion process
on the union of a continuous representation space and a discrete token space,
leveraging a single model to simultaneously denoise in the joint space. By
combining two modalities, CCDD is expressive with rich semantics in the latent
space, as well as good trainability and sample quality with the help of
explicit discrete tokens. We also propose effective architectures and advanced
training/sampling techniques for CCDD, which reveals strong empirical
performance in extensive language modeling experiments on real-world tasks.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [27] [Heterogeneous Graph Representation of Stiffened Panels with Non-Uniform Boundary Conditions and Loads](https://arxiv.org/abs/2510.02472)
*Yuecheng Cai,Jasmin Jelovica*

Main category: cs.CE

TL;DR: 本文提出用异质图神经网络对加筋板进行异质图表示，实现对加筋板应力和位移场的预测，测试显示性能优于同质图表示。


<details>
  <summary>Details</summary>
Motivation: 代理模型在结构分析和优化中很重要，需要一种能考虑几何变异性、非均匀边界条件和不同加载场景的方法。

Method: 将结构划分为多个结构单元，用三种不同节点类型表示，引入边的异质性，提出不同异质程度的图表示并用于异质图变换器进行预测，还进行数值测试和消融分析。

Result: 异质图表示在预测位移和冯·米塞斯应力方面表现出较高的准确性，能有效捕捉结构行为模式和最大值，性能优于同质图表示。

Conclusion: 所提出的异质图表示方法在加筋板的应力和位移场预测中具有良好的效果。

Abstract: Surrogate models are essential in structural analysis and optimization. We
propose a heterogeneous graph representation of stiffened panels that accounts
for geometrical variability, non-uniform boundary conditions, and diverse
loading scenarios, using heterogeneous graph neural networks (HGNNs). The
structure is partitioned into multiple structural units, such as stiffeners and
the plates between them, with each unit represented by three distinct node
types: geometry, boundary, and loading nodes. Edge heterogeneity is introduced
by incorporating local orientations and spatial relationships of the connecting
nodes. Several heterogeneous graph representations, each with varying degrees
of heterogeneity, are proposed and analyzed. These representations are
implemented into a heterogeneous graph transformer (HGT) to predict von Mises
stress and displacement fields across stiffened panels, based on loading and
degrees of freedom at their boundaries. To assess the efficacy of our approach,
we conducted numerical tests on panels subjected to patch loads and box beams
composed of stiffened panels under various loading conditions. The
heterogeneous graph representation was compared with a homogeneous counterpart,
demonstrating superior performance. Additionally, an ablation analysis was
performed to evaluate the impact of graph heterogeneity on HGT performance. The
results show strong predictive accuracy for both displacement and von Mises
stress, effectively capturing structural behavior patterns and maximum values.

</details>


### [28] [VisitHGNN: Heterogeneous Graph Neural Networks for Modeling Point-of-Interest Visit Patterns](https://arxiv.org/abs/2510.02702)
*Lin Pang,Jidong J. Yang*

Main category: cs.CE

TL;DR: 通过挖掘城市地点间流动模式，提出VisitHGNN预测邻里到特定目的地的访问概率，在富尔顿县数据上表现出色，可支持城市规划等决策。


<details>
  <summary>Details</summary>
Motivation: 理解城市居民在邻里和目的地间的出行模式对交通规划、出行管理和公共卫生至关重要，需有效方法估计访问概率。

Method: 引入VisitHGNN，利用数值、JSON和文本属性表征POI，用72个社会人口变量描述CBGs，通过空间邻接和带距离注释的跨类型边连接，以距离为基础约束推理，训练最小化掩蔽的KL散度。

Result: 使用富尔顿县每周出行数据，VisitHGNN预测性能强，各项指标表现好，远超基线模型，与实际出行模式拟合度高。

Conclusion: 该模型在城市规划、交通政策、出行系统设计和公共卫生等决策支持方面有潜力。

Abstract: Understanding how urban residents travel between neighborhoods and
destinations is critical for transportation planning, mobility management, and
public health. By mining historical origin-to-destination flow patterns with
spatial, temporal, and functional relations among urban places, we estimate
probabilities of visits from neighborhoods to specific destinations. These
probabilities capture neighborhood-level contributions to citywide vehicular
and foot traffic, supporting demand estimation, accessibility assessment, and
multimodal planning. Particularly, we introduce VisitHGNN, a heterogeneous,
relation-specific graph neural network designed to predict visit probabilities
at individual Points of interest (POIs). POIs are characterized using
numerical, JSON-derived, and textual attributes, augmented with fixed summaries
of POI--POI spatial proximity, temporal co-activity, and brand affinity, while
census block groups (CBGs) are described with 72 socio-demographic variables.
CBGs are connected via spatial adjacency, and POIs and CBGs are linked through
distance-annotated cross-type edges. Inference is constrained to a
distance-based candidate set of plausible origin CBGs, and training minimizes a
masked Kullback-Leibler (KL) divergence to yield probability distribution
across the candidate set. Using weekly mobility data from Fulton County,
Georgia, USA, VisitHGNN achieves strong predictive performance with mean KL
divergence of 0.287, MAE of 0.008, Top-1 accuracy of 0.853, and R-square of
0.892, substantially outperforming pairwise MLP and distance-only baselines,
and aligning closely with empirical visitation patterns (NDCG@50 = 0.966);
Recall@5 = 0.611). The resulting distributions closely mirror observed travel
behavior with high fidelity, highlighting the model's potential for decision
support in urban planning, transportation policy, mobility system design, and
public health.

</details>


### [29] [Can LLMs Hit Moving Targets? Tracking Evolving Signals in Corporate Disclosures](https://arxiv.org/abs/2510.03195)
*Chanyeol Choi,Jihoon Kwon,Minjae Kim*

Main category: cs.CE

TL;DR: 原研究方法有局限性，本文提出基于大语言模型的目标提取方法，提升金融文本绩效预测的粒度和准确性。


<details>
  <summary>Details</summary>
Motivation: 原研究使用命名实体识别方法提取目标存在噪声和上下文信息丢失的问题，影响准确性。

Method: 提出基于大语言模型的目标提取方法，并定义新指标以更好捕捉语义上下文。

Result: 该方法能保留语义上下文，比原方法有更高的预测能力。

Conclusion: 此方法提升了金融文本绩效预测的粒度和准确性。

Abstract: Moving targets -- managers' strategic shifting of key performance metrics
when the original targets become difficult to achieve -- have been shown to
predict subsequent stock underperformance. However, our work reveals that the
method employed in that study exhibits two key limitations that hinder the
accuracy -- noise in the extracted targets and loss of contextual information
-- both of which stem primarily from the use of a named entity recognition
(NER). To address these two limitations, we propose an LLM-based target
extraction} method with a newly defined metric that better captures semantic
context. This approach preserves semantic context beyond simple entity
recognition and yields consistently higher predictive power than the original
approach. Overall, our approach enhances the granularity and accuracy of
financial text-based performance prediction.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [30] [A New Normalization Form for Limited Distinct Attributes](https://arxiv.org/abs/2510.02865)
*Niko S. Snell,Rayen C. Lee*

Main category: cs.DB

TL;DR: 现有数据库规范化方法未处理非有限不同属性问题，提出有限不同范式（LDNF）改进数据规范化过程。


<details>
  <summary>Details</summary>
Motivation: 现有数据库规范化方法未对只能拥有有限数量值的属性值进行约束，非有限不同属性会导致数据异常和查询不准确，需改进现有规范化过程。

Method: 提出有限不同范式（LDNF），将非有限不同属性转换为有限不同属性，使其符合有限数量的值。

Result: 未提及具体结果。

Conclusion: 提出LDNF，与现有范式结合可满足当前规范化方法未覆盖的需求。

Abstract: In modern databases, the practice of data normalization continues to be
important in improving data integrity, minimizing redundancies, and eliminating
anomalies. However, since its inception and consequent improvements, there have
been no attempts to document a method which constrains the values of attributes
capable of only possessing a limited quantity of values. These non-limited
distinct attributes pose a problem throughout many relational databases as they
have the potential to cause data anomalies and query inaccuracies. Thus, a new
database normalization method, Limited Distinct Normal Form (LDNF), is
necessary in order to improve upon the currently established data normalization
process. In brief, LDNF is a method which turns non-limited distinct attributes
into limited distinct attributes by forcing the attributes to conform to a
limited quantity of values. Utilizing LDNF in tandem with existing normal forms
fulfills a need in normalization that is otherwise not present when only using
current methods. A formal approach to LDNF is therefore proposed.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [31] [ElasticMoE: An Efficient Auto Scaling Method for Mixture-of-Experts Models](https://arxiv.org/abs/2510.02613)
*Gursimran Singh,Timothy Yu,Haley Li,Cheng Chen,Hanieh Sadri,Qintao Zhang,Yu Zhang,Ying Xiong,Yong Zhang,Zhenan Fan*

Main category: cs.DC

TL;DR: 提出ElasticMoE框架解决MoE大语言模型弹性服务难题，评估显示性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有MoE模型并行推理管道使弹性服务困难，水平和垂直扩展策略存在不足，不适合云部署流量模式。

Method: ElasticMoE将推理执行与内存操作解耦，通过HMM模块零拷贝重映射重用权重和KV缓存，利用高带宽点对点传输使新加速器上线，采用虚拟内存专家重新分配机制迁移专家。

Result: 在Ascend NPUs上对三个流行MoE大语言模型评估，ElasticMoE扩缩容延迟降低9倍，扩缩容时吞吐量提升2倍，SLO达成率显著提高。

Conclusion: ElasticMoE实现细粒度、并发扩缩容且干扰小，提升了大规模MoE大语言模型在动态云环境中部署的实用性。

Abstract: Mixture-of-Experts (MoE) models promise efficient scaling of large language
models (LLMs) by activating only a small subset of experts per token, but their
parallelized inference pipelines make elastic serving challenging. Existing
strategies fall short: horizontal scaling provisions entire replicas of the
current configuration, often tens to hundreds of accelerators, leading to
coarse granularity, long provisioning delays, and costly overprovisioning.
Vertical scaling offers finer adjustments but typically requires instance
restarts, incurring downtime. These limitations make current approaches
ill-suited for the bursty, short-lived traffic patterns common in cloud
deployments.
  We present ElasticMoE, an elastic scaling framework for MoE LLMs that
achieves fine-grained, low-latency, and zero-downtime scaling. ElasticMoE
decouples inference execution from memory operations, enabling scaling steps to
proceed concurrently with serving. An HBM Management Module (HMM) reuses
weights and KV caches via zero-copy remapping, while high-bandwidth
peer-to-peer transfers bring newly added accelerators online without
interrupting service. A virtual memory based expert redistribution mechanism
migrates MoE experts without costly buffer reallocations, reducing peak memory
usage during expert parallelism reconfiguration.
  Our evaluation on Ascend NPUs with three popular MoE LLMs shows that
ElasticMoE achieves up to 9x lower scale-up latency, up to 2x better throughput
during scaling, and significantly improves SLO attainment compared to
baselines. By enabling fine-grained, concurrent scaling with minimal
disruption, ElasticMoE advances the practicality of deploying massive MoE LLMs
in dynamic cloud environments.

</details>


### [32] [GRNND: A GPU-Parallel Relative NN-Descent Algorithm for Efficient Approximate Nearest Neighbor Graph Construction](https://arxiv.org/abs/2510.02774)
*Xiang Li,Qiong Chang,Yun Li,Jun Miyazaki*

Main category: cs.DC

TL;DR: 提出GRNND，首个RNN - Descent的GPU并行算法，实验显示其性能超现有CPU和GPU方法。


<details>
  <summary>Details</summary>
Motivation: RNN - Descent在数据量和维度增加时，图构建复杂度急剧上升，耗时久影响后续查询处理。

Method: 引入无序邻居传播策略，利用warp级协作操作和固定容量双缓冲邻居池。

Result: GRNND比现有GPU方法提速2.4到51.7倍，比CPU方法提速17.8到49.8倍。

Conclusion: GRNND能充分利用GPU架构，性能优于现有方法。

Abstract: Relative Nearest Neighbor Descent (RNN-Descent) is a state-of-the-art
algorithm for constructing sparse approximate nearest neighbor (ANN) graphs by
combining the iterative refinement of NN-Descent with the edge-pruning rules of
the Relative Neighborhood Graph (RNG). It has demonstrated strong effectiveness
in large-scale search tasks such as information retrieval and related tasks.
However, as the amount and dimensionality of data increase, the complexity of
graph construction in RNN-Descent rises sharply, making this stage increasingly
time-consuming and even prohibitive for subsequent query processing. In this
paper, we propose GRNND, the first GPU-parallel algorithm of RNN-Descent
designed to fully exploit GPU architecture. GRNND introduces a disordered
neighbor propagation strategy to mitigate synchronized update traps, enhancing
structural diversity, and avoiding premature convergence during parallel
execution. It also leverages warp-level cooperative operations and a
double-buffered neighbor pool with fixed capacity for efficient memory access,
eliminate contention, and enable highly parallelized neighbor updates.
Extensive experiments demonstrate that GRNND consistently outperforms existing
CPU- and GPU-based methods. GRNND achieves 2.4 to 51.7x speedup over existing
GPU methods, and 17.8 to 49.8x speedup over CPU methods.

</details>


### [33] [TridentServe: A Stage-level Serving System for Diffusion Pipelines](https://arxiv.org/abs/2510.02838)
*Yifei Xia,Fangcheng Fu,Hao Yuan,Hanke Zhang,Xupeng Miao,Yijun Liu,Suhan Ling,Jie Jiang,Bin Cui*

Main category: cs.DC

TL;DR: 现有扩散管道服务系统效率低，提出动态阶段级服务范式和TridentServe系统，实验显示其性能优于现有工作。


<details>
  <summary>Details</summary>
Motivation: 当前扩散管道服务系统采用静态、手动和管道级范式，资源分配不合理，效率低下。

Method: 提出动态阶段级服务范式，开发TridentServe系统，自动动态得出管道部署的放置计划和请求处理的调度计划，共同优化模型和请求的资源分配。

Result: TridentServe在各种工作负载下，相比现有工作，SLO达成率持续提升，平均/P95延迟最多降低2.5倍和3.6倍/4.1倍。

Conclusion: TridentServe系统能有效提升扩散管道服务的性能和效率。

Abstract: Diffusion pipelines, renowned for their powerful visual generation
capabilities, have seen widespread adoption in generative vision tasks (e.g.,
text-to-image/video). These pipelines typically follow an
encode--diffuse--decode three-stage architecture. Current serving systems
deploy diffusion pipelines within a static, manual, and pipeline-level
paradigm, allocating the same resources to every request and stage. However,
through an in-depth analysis, we find that such a paradigm is inefficient due
to the discrepancy in resource needs across the three stages of each request,
as well as across different requests. Following the analysis, we propose the
dynamic stage-level serving paradigm and develop TridentServe, a brand new
diffusion serving system. TridentServe automatically, dynamically derives the
placement plan (i.e., how each stage resides) for pipeline deployment and the
dispatch plan (i.e., how the requests are routed) for request processing,
co-optimizing the resource allocation for both model and requests. Extensive
experiments show that TridentServe consistently improves SLO attainment and
reduces average/P95 latencies by up to 2.5x and 3.6x/4.1x over existing works
across a variety of workloads.

</details>


### [34] [On the energy efficiency of sparse matrix computations on multi-GPU clusters](https://arxiv.org/abs/2510.02878)
*Massimo Bernaschi,Alessandro Celestini,Pasqua D'Ambra,Giorgio Richelli*

Main category: cs.DC

TL;DR: 研究并行稀疏矩阵计算库能效，提供能耗分析，表明优化计算和减少数据移动可降低时间和能耗，且优于同类框架。


<details>
  <summary>Details</summary>
Motivation: 满足现代高性能计算平台对可持续性的需求，在已有性能研究基础上扩展能效研究。

Method: 设计高并行度方法并优化多GPU算法实现，采用准确测量库核心组件运行时能耗的方法和工具。

Result: 优化GPU计算和减少数据移动可降低求解时间和能耗，该库在标准基准测试中优于同类软件框架。

Conclusion: 该库在能效方面表现出色，具有显著优势。

Abstract: We investigate the energy efficiency of a library designed for parallel
computations with sparse matrices. The library leverages high-performance,
energy-efficient Graphics Processing Unit (GPU) accelerators to enable
large-scale scientific applications. Our primary development objective was to
maximize parallel performance and scalability in solving sparse linear systems
whose dimensions far exceed the memory capacity of a single node. To this end,
we devised methods that expose a high degree of parallelism while optimizing
algorithmic implementations for efficient multi-GPU usage. Previous work has
already demonstrated the library's performance efficiency on large-scale
systems comprising thousands of NVIDIA GPUs, achieving improvements over
state-of-the-art solutions. In this paper, we extend those results by providing
energy profiles that address the growing sustainability requirements of modern
HPC platforms. We present our methodology and tools for accurate runtime energy
measurements of the library's core components and discuss the findings. Our
results confirm that optimizing GPU computations and minimizing data movement
across memory and computing nodes reduces both time-to-solution and energy
consumption. Moreover, we show that the library delivers substantial advantages
over comparable software frameworks on standard benchmarks.

</details>


### [35] [Energy Efficiency in Cloud-Based Big Data Processing for Earth Observation: Gap Analysis and Future Directions](https://arxiv.org/abs/2510.02882)
*Adhitya Bhawiyuga,Serkan Girgin,Rolf A. de By,Raul Zurita-Milla*

Main category: cs.DC

TL;DR: 文章指出基于云的地球观测大数据处理中能源效率实践的差距，并提出改进的研究方向。


<details>
  <summary>Details</summary>
Motivation: 地球观测数据量快速增长，云计算用于处理大数据集，但能源效率方面关注少，且大数据处理中能源成本和碳足迹受关注。

Method: 先考察当前地球观测大数据现状，分析云处理需求和现有解决方案；再研究其他大数据领域成功的能源效率策略，找出现有平台差距。

Result: 识别出现有平台在能源监测、数据管理能源意识、能源感知资源分配和任务调度能源效率标准等方面的差距。

Conclusion: 提出开发能源感知性能监测和基准框架、采用基础设施编排优化技术和能源高效任务调度方法，以提高能源意识，降低功耗和环境影响。

Abstract: Earth observation (EO) data volumes are rapidly increasing. While cloud
computing are now used for processing large EO datasets, the energy efficiency
aspects of such a processing have received much less attention. This issue is
notable given the increasing awareness of energy costs and carbon footprint in
big data processing, particularly with increased attention on compute-intensive
foundation models. In this paper we identify gaps in energy efficiency
practices within cloud-based EO big data (EOBD) processing and propose several
research directions for improvement. We first examine the current EOBD
landscape, focus on the requirements that necessitate cloud-based processing
and analyze existing cloud-based EOBD solutions. We then investigate energy
efficiency strategies that have been successfully employed in well-studied big
data domains. Through this analysis, we identify several critical gaps in
existing EOBD processing platforms, which primarily focus on data accessibility
and computational feasibility, instead of energy efficiency. These gaps include
insufficient energy monitoring mechanisms, lack of energy awareness in data
management, inadequate implementation of energy-aware resource allocation and
lack of energy efficiency criteria on task scheduling. Based on these findings,
we propose the development of energy-aware performance monitoring and
benchmarking frameworks, the use of optimization techniques for infrastructure
orchestration, and of energy-efficient task scheduling approaches for
distributed cloud-based EOBD processing frameworks. These proposed approaches
aim to foster more energy awareness in EOBD processing , potentially reducing
power consumption and environmental impact while maintaining or minimally
impacting processing performance.

</details>


### [36] [PyRadiomics-cuda: a GPU-accelerated 3D features extraction from medical images within PyRadiomics](https://arxiv.org/abs/2510.02894)
*Jakub Lisowski,Piotr Tyrakowski,Szymon Zyguła,Krzysztof Kaczmarski*

Main category: cs.DC

TL;DR: PyRadiomics - cuda是PyRadiomics库的GPU加速扩展，可加速医学图像特征提取，保持原API兼容性，测试证明其在多种场景有用，代码和测试套件开源。


<details>
  <summary>Details</summary>
Motivation: 解决从医学图像中提取三维形状特征的计算挑战，提高处理大规模体积数据集的效率。

Method: 将关键几何计算卸载到GPU硬件。

Result: 在典型计算集群、预算设备和家用设备上的测试证明其在各种场景都有用。

Conclusion: PyRadiomics - cuda能实现高效、可扩展的放射组学分析，支持高通量AI管道的快速特征提取。

Abstract: PyRadiomics-cuda is a GPU-accelerated extension of the PyRadiomics library,
designed to address the computational challenges of extracting
three-dimensional shape features from medical images. By offloading key
geometric computations to GPU hardware it dramatically reduces processing times
for large volumetric datasets. The system maintains full compatibility with the
original PyRadiomics API, enabling seamless integration into existing AI
workflows without code modifications. This transparent acceleration facilitates
efficient, scalable radiomics analysis, supporting rapid feature extraction
essential for high-throughput AI pipeline. Tests performed on a typical
computational cluster, budget and home devices prove usefulness in all
scenarios. PyRadiomics-cuda is implemented in Python and C/CUDA and is freely
available under the BSD license at https://github.com/mis-wut/pyradiomics-CUDA
Additionally PyRadiomics-cuda test suite is available at
https://github.com/mis-wut/pyradiomics-cuda-data-gen. It provides detailed
handbook and sample scripts suited for different kinds of workflows plus
detailed installation instructions. The dataset used for testing is available
at Kaggle
https://www.kaggle.com/datasets/sabahesaraki/kidney-tumor-segmentation-challengekits-19

</details>


### [37] [iDDS: Intelligent Distributed Dispatch and Scheduling for Workflow Orchestration](https://arxiv.org/abs/2510.02930)
*Wen Guan,Tadashi Maeno,Aleksandr Alekseev,Fernando Harald Barreiro Megino,Kaushik De,Edward Karavakis,Alexei Klimentov,Tatiana Korchuganova,FaHui Lin,Paul Nilsson,Torre Wenaus,Zhaoyu Yang,Xin Zhao*

Main category: cs.DC

TL;DR: 本文介绍智能分布式调度服务iDDS，包括架构、组件，通过用例展示其多功能性，还提及面临挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 设计一个适用于大规模分布式科学计算的多功能工作流编排系统，以减少操作开销，实现可重复、高吞吐量的工作流。

Method: 介绍iDDS的架构和核心组件，通过多个实际用例展示其功能。

Result: iDDS通过统一工作负载调度、数据移动和自适应决策，能跨异构基础设施减少操作开销，实现可重复、高吞吐量的工作流。

Conclusion: 指出iDDS当前面临的挑战和未来在交互式、云原生和无服务器工作流支持等方面的发展方向。

Abstract: The intelligent Distributed Dispatch and Scheduling (iDDS) service is a
versatile workflow orchestration system designed for large-scale, distributed
scientific computing. iDDS extends traditional workload and data management by
integrating data-aware execution, conditional logic, and programmable
workflows, enabling automation of complex and dynamic processing pipelines.
Originally developed for the ATLAS experiment at the Large Hadron Collider,
iDDS has evolved into an experiment-agnostic platform that supports both
template-driven workflows and a Function-as-a-Task model for Python-based
orchestration.
  This paper presents the architecture and core components of iDDS,
highlighting its scalability, modular message-driven design, and integration
with systems such as PanDA and Rucio. We demonstrate its versatility through
real-world use cases: fine-grained tape resource optimization for ATLAS,
orchestration of large Directed Acyclic Graph (DAG) workflows for the Rubin
Observatory, distributed hyperparameter optimization for machine learning
applications, active learning for physics analyses, and AI-assisted detector
design at the Electron-Ion Collider.
  By unifying workload scheduling, data movement, and adaptive decision-making,
iDDS reduces operational overhead and enables reproducible, high-throughput
workflows across heterogeneous infrastructures. We conclude with current
challenges and future directions, including interactive, cloud-native, and
serverless workflow support.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [38] [Even Faster Kernel Matrix Linear Algebra via Density Estimation](https://arxiv.org/abs/2510.02540)
*Rikhav Shah,Sandeep Silwal,Haike Xu*

Main category: cs.DS

TL;DR: 本文研究用核密度估计（KDE）处理线性代数任务，改进现有算法，给出上界和下界。


<details>
  <summary>Details</summary>
Motivation: 改进现有算法，降低计算复杂度和对参数的依赖。

Method: 使用核密度估计（KDE）处理核矩阵的线性代数任务。

Result: 改进了矩阵向量积、矩阵矩阵积、谱范数和所有元素和的计算算法，降低对\(\varepsilon\)和\(n\)的依赖，给出相关问题的下界。

Conclusion: KDE方法在相关问题上有改进，但有一定局限性。

Abstract: This paper studies the use of kernel density estimation (KDE) for linear
algebraic tasks involving the kernel matrix of a collection of $n$ data points
in $\mathbb R^d$. In particular, we improve upon existing algorithms for
computing the following up to $(1+\varepsilon)$ relative error: matrix-vector
products, matrix-matrix products, the spectral norm, and sum of all entries.
The runtimes of our algorithms depend on the dimension $d$, the number of
points $n$, and the target error $\varepsilon$. Importantly, the dependence on
$n$ in each case is far lower when accessing the kernel matrix through KDE
queries as opposed to reading individual entries.
  Our improvements over existing best algorithms (particularly those of
Backurs, Indyk, Musco, and Wagner '21) for these tasks reduce the polynomial
dependence on $\varepsilon$, and additionally decreases the dependence on $n$
in the case of computing the sum of all entries of the kernel matrix.
  We complement our upper bounds with several lower bounds for related
problems, which provide (conditional) quadratic time hardness results and
additionally hint at the limits of KDE based approaches for the problems we
study.

</details>


### [39] [Near-Optimal Fault-Tolerant Strong Connectivity Preservers](https://arxiv.org/abs/2510.02562)
*Gary Hoppenworth,Thatchaphol Saranurak,Benyu Wang*

Main category: cs.DS

TL;DR: 本文研究有向图的容错连通性保持器，近乎填补有向图的理论界差距，给出构造方法，并对相关问题改进了现有结果，还展示了 k - 连通性保持器指数依赖 k 并非必然。


<details>
  <summary>Details</summary>
Motivation: 有向图 k - 容错连通性保持器的上下界存在显著差距，需缩小该差距。

Method: 通过研究证明存在具有特定边数的 k - 容错连通性保持器，并给出构造方法，利用已知归约得到 k - 连通性保持器。

Result: 证明存在 O(k4^k n log n) 条边的 k - 容错连通性保持器，可在 poly(2^k n) 时间内构造出 O(8^k n log^(5/2) n) 条边的保持器；得到 O(k4^k n log n) 条边的 k - 连通性保持器，改进了之前结果；给出 O(n √(kn)) 条边的 k - 连通性保持器构造。

Conclusion: 对于任意常数 k，结果在 log n 因子内是最优的，且 k - 连通性保持器对 k 的指数依赖并非固有。

Abstract: A $k$-fault-tolerant connectivity preserver of a directed $n$-vertex graph
$G$ is a subgraph $H$ such that, for any edge set $F \subseteq E(G)$ of size
$|F| \le k$, the strongly connected components of $G - F$ and $H - F$ are the
same. While some graphs require a preserver with $\Omega(2^{k}n)$ edges
[BCR18], the best-known upper bound is $\tilde{O}(k2^{k}n^{2-1/k})$ edges
[CC20], leaving a significant gap of $\Omega(n^{1-1/k})$. In contrast, there is
no gap in undirected graphs; the optimal bound of $\Theta(kn)$ has been
well-established since the 90s [NI92].
  We nearly close the gap for directed graphs; we prove that there exists a
$k$-fault-tolerant connectivity preserver with $O(k4^{k}n\log n)$ edges, and we
can construct one with $O(8^{k}n\log^{5/2}n)$ edges in $\text{poly}(2^{k}n)$
time.
  Our results also improve the state-of-the-art for a closely related object; a
\textit{$k$-connectivity preserver} of $G$ is a subgraph $H$ where, for all $i
\le k$, the strongly $i$-connected components of $G$ and $H$ agree. By a known
reduction, we obtain a $k$-connectivity preserver with $O(k4^{k}n\log n)$
edges, improving the previous best bound of $\tilde{O}(k2^{k}n^{2-1/(k-1)})$
[CC20]. Therefore, for any constant $k$, our results are optimal to a $\log n$
factor for both problems.
  Lastly, we show that the exponential dependency on $k$ is not inherent for
$k$-connectivity preservers by presenting another construction with $O(n
\sqrt{kn})$ edges.

</details>


### [40] [Congestion bounds via Laplacian eigenvalues and their application to tensor networks with arbitrary geometry](https://arxiv.org/abs/2510.02725)
*Sayan Mukherjee,Shinichiro Akiyama*

Main category: cs.DS

TL;DR: 研究将图顶点嵌入二叉树叶子节点的拥塞问题，给出拥塞上下界，并提供启发式方法，还进行数值比较，结果对张量网络收缩内存复杂度有界。


<details>
  <summary>Details</summary>
Motivation: 将任意图顶点嵌入树以最小化重叠在计算机科学和物理学中有重要应用，研究图顶点到二叉树叶子节点的双射嵌入拥塞问题。

Method: 理论推导得出拥塞上下界，采用层次谱聚类的收缩启发式方法，对不同类型图进行数值比较。

Result: 得出拥塞上下界λ₂(G)·2n/9 ≤ cng(G) ≤ λₙ(G)·2n/9，发现层次谱聚类启发式方法对稀疏图找低拥塞嵌入有效。

Conclusion: 结果给出了张量网络收缩内存复杂度基于底层图的上下界。

Abstract: Embedding the vertices of arbitrary graphs into trees while minimizing some
measure of overlap is an important problem with applications in computer
science and physics. In this work, we consider the problem of bijectively
embedding the vertices of an $n$-vertex graph $G$ into the leaves of an
$n$-leaf rooted binary tree $\mathcal{B}$. The congestion of such an embedding
is given by the largest size of the cut induced by the two components obtained
by deleting any vertex of $\mathcal{B}$. The congestion $\mathrm{cng}(G)$ is
defined as the minimum congestion obtained by any embedding. We show that
$\lambda_2(G)\cdot 2n/9\le \mathrm{cng} (G)\le \lambda_n(G)\cdot 2n/9$, where
$0=\lambda_1(G)\le \cdots \le \lambda_n(G)$ are the Laplacian eigenvalues of
$G$. We also provide a contraction heuristic given by hierarchically spectral
clustering the original graph, which we numerically find to be effective in
finding low congestion embeddings for sparse graphs. We numerically compare our
congestion bounds on different families of graphs with regular structure
(hypercubes and lattices), random graphs, and tensor network representations of
quantum circuits. Our results imply lower and upper bounds on the memory
complexity of tensor network contraction in terms of the underlying graph.

</details>


### [41] [On the Enumeration of all Unique Paths of Recombining Trinomial Trees](https://arxiv.org/abs/2510.02727)
*Ethan Torres,Ramavarapu Sreenivas,Richard Sowers*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recombining trinomial trees are a workhorse for modeling discrete-event
systems in option pricing, logistics, and feedback control. Because each node
stores a state-dependent quantity, a depth-$D$ tree naively yields
$\mathcal{O}(3^{D})$ trajectories, making exhaustive enumeration infeasible.
Under time-homogeneous dynamics, however, the graph exhibits two exploitable
symmetries: (i) translational invariance of nodes and (ii) a canonical
bijection between admissible paths and ordered tuples encoding weak
compositions. Leveraging these, we introduce a mass-shifting enumeration
algorithm that slides integer "masses" through a cardinality tuple to generate
exactly one representative per path-equivalence class while implicitly counting
the associated weak compositions. This trims the search space by an exponential
factor, enabling markedly deeper trees -- and therefore tighter numerical
approximations of the underlying evolution -- to be processed in practice. We
further derive an upper bound on the combinatorial counting expression that
induces a theoretical lower bound on the algorithmic cost of approximately
$\mathcal{O}\bigl(D^{1/2}1.612^{D}\bigr)$. This correspondence permits direct
benchmarking while empirical tests, whose pseudo-code we provide, corroborate
the bound, showing only a small constant overhead and substantial speedups over
classical breadth-first traversal. Finally, we highlight structural links
between our algorithmic/combinatorial framework and Motzkin paths with
Narayana-type refinements, suggesting refined enumerative formulas and new
potential analytic tools for path-dependent functionals.

</details>


### [42] [Pareto-optimal Non-uniform Language Generation](https://arxiv.org/abs/2510.02795)
*Moses Charikar,Chirag Pabbaraju*

Main category: cs.DS

TL;DR: 前人研究语言极限生成模型有非均匀生成保证但时间可能非最优，本文研究非均匀语言极限生成的帕累托最优，提出算法并拓展到有噪声和代表性生成场景。


<details>
  <summary>Details</summary>
Motivation: 前人算法语言生成时间可能非最优，研究帕累托最优的非均匀语言生成。

Method: 提出一种算法，其生成时间（几乎）是帕累托最优的。

Result: 得到（几乎）帕累托最优的生成时间，且算法框架可应用到有噪声和代表性生成场景。

Conclusion: 帕累托最优是实现非均匀生成的最佳目标，所提算法有良好适应性。

Abstract: Kleinberg and Mullainathan (2024) recently proposed an interesting model for
language generation in the limit: Given a countable collection of languages,
and an adversary enumerating the strings of some language $L$ from the
collection, the objective is to generate new strings from the target language,
such that all strings generated beyond some finite time are valid. Li, Raman
and Tewari (2024) and Charikar and Pabbaraju (2024) showed strong non-uniform
generation guarantees in this model, giving algorithms that generate new valid
strings from $L$ after seeing a number of distinct input strings $t(L)$ that
depends only on $L$ (and the collection), but not the enumeration order.
However, for both these works, the language-wise generation times $t(L)$ of the
algorithm can be strictly sub-optimal.
  In this work, we study Pareto-optimality of non-uniform language generation
in the limit. We propose an algorithm, whose generation times $t^\star(L)$ are
(almost) Pareto-optimal: any other algorithm whose generation time for some
language $L$ is strictly smaller than $t^\star(L)$, must satisfy that its
generation time for some other language $L'$ is strictly worse than
$t^\star(L')$. Pareto-optimality is essentially the best that one can achieve
for non-uniform generation. Our algorithmic framework conveniently adapts to
further give Pareto-optimal non-uniform generation algorithms in the
practically motivated settings of noisy as well as representative generation.

</details>


### [43] [Low Recourse Arborescence Forests Under Uniformly Random Arcs](https://arxiv.org/abs/2510.02950)
*J Niklas Dahlmeier,D Ellis Hershkowitz*

Main category: cs.DS

TL;DR: 研究弧插入下最大弧基数树形图森林维护问题，分析了不可能情况及可能的随机插入算法。


<details>
  <summary>Details</summary>
Motivation: 研究在弧插入时如何维护最大弧基数树形图森林并最小化追索（维护解中改变的弧总数），该问题是最大基数匹配的“树形图版本”。

Method: 先分析插入模型下的不可能情况，再针对所有弧随机到达的情况给出算法。

Result: 在对抗性弧到达时必然产生Ω(m · n)的追索，匹配O(m · n)的平凡上界；在所有弧均匀随机到达时，算法的期望追索为O(m · log² n)。

Conclusion: 明确了不同情况下维护最大弧基数树形图森林的追索情况，并给出随机插入下的有效算法。

Abstract: In this work, we study how to maintain a forest of arborescences of maximum
arc cardinality under arc insertions while minimizing recourse -- the total
number of arcs changed in the maintained solution. This problem is the
"arborescence version'' of max cardinality matching.
  On the impossibility side, we observe that even in this insertion-only model,
it is possible for $m$ adversarial arc arrivals to necessarily incur $\Omega(m
\cdot n)$ recourse, matching a trivial upper bound of $O(m \cdot n)$. On the
possibility side, we give an algorithm with expected recourse $O(m \cdot \log^2
n)$ if all $m$ arcs arrive uniformly at random.

</details>


### [44] [Oracle-based Uniform Sampling from Convex Bodies](https://arxiv.org/abs/2510.02983)
*Thanh Dang,Jiaming Liang*

Main category: cs.DS

TL;DR: 提出新的马尔可夫链蒙特卡罗算法对凸体上均匀分布采样，实现受限高斯预言机，建立非渐近复杂度。


<details>
  <summary>Details</summary>
Motivation: 提出新算法对凸体上的均匀分布进行采样。

Method: 基于交替采样框架/近端采样器，利用吉布斯采样和受限高斯预言机，通过拒绝采样和投影预言机或分离预言机实现受限高斯预言机。

Result: 在两种预言机情况下，建立了非渐近复杂度以获得无偏样本，精度用Rényi散度或χ²散度衡量。

Conclusion: 新算法可有效实现凸体上均匀分布采样，并给出相关复杂度。

Abstract: We propose new Markov chain Monte Carlo algorithms to sample a uniform
distribution on a convex body $K$. Our algorithms are based on the Alternating
Sampling Framework/proximal sampler, which uses Gibbs sampling on an augmented
distribution and assumes access to the so-called restricted Gaussian oracle
(RGO). The key contribution of this work is the efficient implementation of RGO
for uniform sampling on $K$ via rejection sampling and access to either a
projection oracle or a separation oracle on $K$. In both oracle cases, we
establish non-asymptotic complexities to obtain unbiased samples where the
accuracy is measured in R\'enyi divergence or $\chi^2$-divergence.

</details>


### [45] [Smooth Trade-off for Tensor PCA via Sharp Bounds for Kikuchi Matrices](https://arxiv.org/abs/2510.03061)
*Pravesh K. Kothari,Jeff Xu*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this work, we revisit algorithms for Tensor PCA: given an order-$r$ tensor
of the form $T = G+\lambda \cdot v^{\otimes r}$ where $G$ is a random symmetric
Gaussian tensor with unit variance entries and $v$ is an unknown boolean vector
in $\{\pm 1\}^n$, what's the minimum $\lambda$ at which one can distinguish $T$
from a random Gaussian tensor and more generally, recover $v$? As a result of a
long line of work, we know that for any $\ell \in \N$, there is a $n^{O(\ell)}$
time algorithm that succeeds when the signal strength $\lambda \gtrsim
\sqrt{\log n} \cdot n^{-r/4} \cdot \ell^{1/2-r/4}$. The question of whether the
logarithmic factor is necessary turns out to be crucial to understanding
whether larger polynomial time allows recovering the signal at a lower signal
strength. Such a smooth trade-off is necessary for tensor PCA being a candidate
problem for quantum speedups[SOKB25]. It was first conjectured by [WAM19] and
then, more recently, with an eye on smooth trade-offs, reiterated in a blogpost
of Bandeira.
  In this work, we resolve these conjectures and show that spectral algorithms
based on the Kikuchi hierarchy \cite{WAM19} succeed whenever $\lambda \geq
\Theta_r(1) \cdot n^{-r/4} \cdot \ell^{1/2-r/4}$ where $\Theta_r(1)$ only hides
an absolute constant independent of $n$ and $\ell$. A sharp bound such as this
was previously known only for $\ell \leq 3r/4$ via non-asymptotic techniques in
random matrix theory inspired by free probability.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [46] [Deceptive Planning Exploiting Inattention Blindness](https://arxiv.org/abs/2510.02714)
*Mustafa O. Karabag,Jesse Milzman,Ufuk Topcu*

Main category: cs.GT

TL;DR: 研究有感知约束下理性疏忽的决策问题，在两人零和随机博弈中建模，提出传感器选择算法和利用对手疏忽的决策算法，数值示例展示玩家2可利用玩家1疏忽。


<details>
  <summary>Details</summary>
Motivation: 在有感知约束的环境中，不准确先验信念可能导致疏忽盲视，研究如何在这种情况下进行决策。

Method: 将感知约束建模为在线传感器选择问题，开发价值加权目标函数，提出贪心算法；为玩家2提出短视决策算法。

Result: 当玩家2不偏离预设策略时，目标函数为预期价值损失提供上界；数值示例显示玩家1持续选择符合先验的传感器，玩家2可利用其疏忽。

Conclusion: 在有感知约束的决策中，玩家可能因先验信念产生疏忽，对手可利用这一特点获得优势。

Abstract: We study decision-making with rational inattention in settings where agents
have perception constraints. In such settings, inaccurate prior beliefs or
models of others may lead to inattention blindness, where an agent is unaware
of its incorrect beliefs. We model this phenomenon in two-player zero-sum
stochastic games, where Player 1 has perception constraints and Player 2
deceptively deviates from its security policy presumed by Player 1 to gain an
advantage. We formulate the perception constraints as an online sensor
selection problem, develop a value-weighted objective function for sensor
selection capturing rational inattention, and propose the greedy algorithm for
selection under this monotone objective function. When Player 2 does not
deviate from the presumed policy, this objective function provides an upper
bound on the expected value loss compared to the security value where Player 1
has perfect information of the state. We then propose a myopic decision-making
algorithm for Player 2 to exploit Player 1's beliefs by deviating from the
presumed policy and, thereby, improve upon the security value. Numerical
examples illustrate how Player 1 persistently chooses sensors that are
consistent with its priors, allowing Player 2 to systematically exploit its
inattention.

</details>


### [47] [Reach together: How populations win repeated games](https://arxiv.org/abs/2510.02984)
*Nathalie Bertrand,Patricia Bouyer,Luc Lapointe,Corto Mascle*

Main category: cs.GT

TL;DR: 研究重复博弈中玩家按可达性目标选行动的参数化情形，证明该问题可在多项式空间解决，且问题是PSPACE完全的。


<details>
  <summary>Details</summary>
Motivation: 研究重复博弈中是否存在与种群规模无关的统一联盟策略让玩家获胜。

Method: 使用代数工具，展示有限半群总结一定种群规模区间策略，通过半群中特定元素表征获胜策略存在性。

Result: 问题可在多项式空间解决，得出匹配复杂度下界。

Conclusion: 带可达性目标的重复种群博弈是PSPACE完全的。

Abstract: In repeated games, players choose actions concurrently at each step. We
consider a parameterized setting of repeated games in which the players form a
population of an arbitrary size. Their utility functions encode a reachability
objective. The problem is whether there exists a uniform coalition strategy for
the players so that they are sure to win independently of the population size.
We use algebraic tools to show that the problem can be solved in polynomial
space. First we exhibit a finite semigroup whose elements summarize strategies
over a finite interval of population sizes. Then, we characterize the existence
of winning strategies by the existence of particular elements in this
semigroup. Finally, we provide a matching complexity lower bound, to conclude
that repeated population games with reachability objectives are
PSPACE-complete.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [48] [Revisiting Query Variants: The Advantage of Retrieval Over Generation of Query Variants for Effective QPP](https://arxiv.org/abs/2510.02512)
*Fangzheng Tian,Debasis Ganguly,Craig Macdonald*

Main category: cs.IR

TL;DR: 本文提出从训练集检索查询变体（QVs）用于查询性能预测（QPP）的方法，在TREC DL'19和DL'20实验中，该方法优于现有基于生成QV的QPP方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于QV的QPP方法通过查询扩展或非上下文嵌入生成QV，可能引入主题漂移和幻觉，需改进。

Method: 从训练集为目标查询检索QVs，通过对直接检索的1跳QV的相关文档进行二次检索得到2跳QV。

Result: 在TREC DL'19和DL'20实验中，使用本文方法检索QV的QPP方法在MonoT5等神经排序模型上比现有最佳方法性能高约20%。

Conclusion: 本文提出的从训练集检索QV的方法能有效提升QPP方法的性能。

Abstract: Leveraging query variants (QVs), i.e., queries with potentially similar
information needs to the target query, has been shown to improve the
effectiveness of query performance prediction (QPP) approaches. Existing
QV-based QPP methods generate QVs facilitated by either query expansion or
non-contextual embeddings, which may introduce topical drifts and
hallucinations. In this paper, we propose a method that retrieves QVs from a
training set (e.g., MS MARCO) for a given target query of QPP. To achieve a
high recall in retrieving queries with the most similar information needs as
the target query from a training set, we extend the directly retrieved QVs
(1-hop QVs) by a second retrieval using their denoted relevant documents (which
yields 2-hop QVs). Our experiments, conducted on TREC DL'19 and DL'20, show
that the QPP methods with QVs retrieved by our method outperform the
best-performing existing generated-QV-based QPP approaches by as much as around
20\%, on neural ranking models like MonoT5.

</details>


### [49] [OpenZL: A Graph-Based Model for Compression](https://arxiv.org/abs/2510.03203)
*Yann Collet,Nick Terrell,W. Felix Handte,Danielle Rozenblit,Victor Zhang,Kevin Zhang,Yaelle Goldschlag,Jennifer Lee,Daniel Riegel,Stan Angelov,Nadav Rotem*

Main category: cs.IR

TL;DR: 传统通用无损压缩在资源利用和吞吐量方面有问题，应用特定压缩有局限。提出图模型和OpenZL，实验显示其在压缩率和速度上优于通用压缩器，Meta内部部署有改进。


<details>
  <summary>Details</summary>
Motivation: 解决通用无损压缩资源利用和吞吐量问题，以及应用特定压缩适用性、维护和部署难题。

Method: 提出压缩的图模型理论框架，实现OpenZL，将数据压缩成自描述格式，有通用解码器。

Result: OpenZL在多种真实数据集上压缩率和速度优于通用压缩器，Meta内部部署大小和/或速度有提升，开发时间从数月减至数天。

Conclusion: OpenZL是现代数据密集型应用在实用、可扩展和可维护数据压缩方面的进步。

Abstract: Research in general-purpose lossless compression over the last decade has
largely found improvements in compression ratio that come at great cost to
resource utilization and processing throughput. However, most production
workloads require high throughput and low resource utilization, so most
research systems have seen little adoption. Instead, real world improvements in
compression are increasingly often realized by building application-specific
compressors which can exploit knowledge about the structure and semantics of
the data being compressed. These systems easily outperform even the best
generic compressors, but application-specific compression schemes are not
without drawbacks. They are inherently limited in applicability and are
difficult to maintain and deploy.
  We show that these challenges can be overcome with a new way of thinking
about compression. We propose the ``graph model'' of compression, a new
theoretical framework for representing compression as a directed acyclic graph
of modular codecs. This motivates OpenZL, an implementation of this model that
compresses data into a self-describing wire format, any configuration of which
can be decompressed by a universal decoder. OpenZL's design enables rapid
development of tailored compressors with minimal code, its universal decoder
eliminates deployment lag, and its investment in a well-vetted standard
component library minimizes security risks. Experimental results demonstrate
that OpenZL achieves superior compression ratios and speeds compared to
state-of-the-art general-purpose compressors on a variety of real-world
datasets. Internal deployments at Meta have also shown consistent improvements
in size and/or speed, with development timelines reduced from months to days.
OpenZL thus represents an advance in practical, scalable, and maintainable data
compression for modern data-intensive applications.

</details>


### [50] [A Simple but Effective Elaborative Query Reformulation Approach for Natural Language Recommendation](https://arxiv.org/abs/2510.02656)
*Qianfeng Wen,Yifan Liu,Justin Cui,Joshua Zhang,Anton Korikov,George-Kirollos Saad,Scott Sanner*

Main category: cs.IR

TL;DR: 提出基于大语言模型的EQR方法结合广度和深度进行查询改写，引入新基准测试，实验表明EQR显著优于现有方法，可改善自然语言推荐系统。


<details>
  <summary>Details</summary>
Motivation: 现有自然语言推荐系统的密集检索难以处理宽泛或间接意图查询，现有查询改写方法不能兼顾广度和深度。

Method: 提出基于大语言模型的EQR方法，结合广度和深度生成潜在查询子主题及信息丰富的阐述，引入三个新的自然语言推荐基准测试。

Result: EQR在各种评估指标上大幅优于现有查询改写方法。

Conclusion: 简单有效的查询改写方法能显著改善针对宽泛和间接意图查询的自然语言推荐系统。

Abstract: Natural Language (NL) recommender systems aim to retrieve relevant items from
free-form user queries and item descriptions. Existing systems often rely on
dense retrieval (DR), which struggles to interpret challenging queries that
express broad (e.g., "cities for youth friendly activities") or indirect (e.g.,
"cities for a high school graduation trip") user intents. While query
reformulation (QR) has been widely adopted to improve such systems, existing QR
methods tend to focus only on expanding the range of query subtopics (breadth)
or elaborating on the potential meaning of a query (depth), but not both. In
this paper, we propose EQR (Elaborative Subtopic Query Reformulation), a large
language model-based QR method that combines both breadth and depth by
generating potential query subtopics with information-rich elaborations. We
also introduce three new natural language recommendation benchmarks in travel,
hotel, and restaurant domains to establish evaluation of NL recommendation with
challenging queries. Experiments show EQR substantially outperforms
state-of-the-art QR methods in various evaluation metrics, highlighting that a
simple yet effective QR approach can significantly improve NL recommender
systems for queries with broad and indirect user intents.

</details>


### [51] [Less LLM, More Documents: Searching for Improved RAG](https://arxiv.org/abs/2510.02657)
*Jingjie Ning,Yibo Kong,Yunfan Long,Jamie Callan*

Main category: cs.IR

TL;DR: 探索扩大检索器语料库来增强检索增强生成（RAG），发现扩大语料库可替代增大模型规模，且有语料 - 生成器权衡关系。


<details>
  <summary>Details</summary>
Motivation: 解决扩大生成器规模带来成本增加和部署受限的问题，探索增强RAG的新途径。

Method: 进行实验，对比不同规模生成器搭配不同大小语料库的RAG效果。

Result: 扩大语料库能持续增强RAG，小和中型生成器搭配大语料库常能匹敌大模型配小语料库，中型模型受益最大，改进主要源于答案段落覆盖增加。

Conclusion: 存在语料 - 生成器权衡关系，投资大语料库是增强RAG的有效途径，效果常与增大LLM相当。

Abstract: Retrieval-Augmented Generation (RAG) couples document retrieval with large
language models (LLMs). While scaling generators improves accuracy, it also
raises cost and limits deployability. We explore an orthogonal axis: enlarging
the retriever's corpus to reduce reliance on large LLMs. Experimental results
show that corpus scaling consistently strengthens RAG and can often serve as a
substitute for increasing model size, though with diminishing returns at larger
scales. Small- and mid-sized generators paired with larger corpora often rival
much larger models with smaller corpora; mid-sized models tend to gain the
most, while tiny and large models benefit less. Our analysis shows that
improvements arise primarily from increased coverage of answer-bearing
passages, while utilization efficiency remains largely unchanged. These
findings establish a principled corpus-generator trade-off: investing in larger
corpora offers an effective path to stronger RAG, often comparable to enlarging
the LLM itself.

</details>


### [52] [AgenticRAG: Tool-Augmented Foundation Models for Zero-Shot Explainable Recommender Systems](https://arxiv.org/abs/2510.02668)
*Bo Ma,Hang Li,ZeHua Hu,XiaoFan Gui,LuYao Liu,Simon Liu*

Main category: cs.IR

TL;DR: 本文提出AgenticRAG框架用于零样本可解释推荐，在三个数据集上实验结果优于基线模型，兼具可解释性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 基础模型在推荐系统中的应用受推理不透明和知识限制，需要可解释的推荐方法。

Method: 结合工具增强基础模型与检索增强生成，集成外部工具调用、知识检索和思维链推理，创建自主推荐代理。

Result: 在三个真实数据集上实验，AgenticRAG比现有基线模型有持续提升，如在不同数据集上NDCG@10有不同程度提高。

Conclusion: AgenticRAG框架在保持与传统方法相当的计算效率的同时，具有更好的可解释性。

Abstract: Foundation models have revolutionized artificial intelligence, yet their
application in recommender systems remains limited by reasoning opacity and
knowledge constraints. This paper introduces AgenticRAG, a novel framework that
combines tool-augmented foundation models with retrieval-augmented generation
for zero-shot explainable recommendations. Our approach integrates external
tool invocation, knowledge retrieval, and chain-of-thought reasoning to create
autonomous recommendation agents capable of transparent decision-making without
task-specific training. Experimental results on three real-world datasets
demonstrate that AgenticRAG achieves consistent improvements over
state-of-the-art baselines, with NDCG@10 improvements of 0.4\% on Amazon
Electronics, 0.8\% on MovieLens-1M, and 1.6\% on Yelp datasets. The framework
exhibits superior explainability while maintaining computational efficiency
comparable to traditional methods.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [53] [Extreme value forecasting using relevance-based data augmentation with deep learning models](https://arxiv.org/abs/2510.02407)
*Junru Hua,Rahul Ahluwalia,Rohitash Chandra*

Main category: cs.LG

TL;DR: 提出用于极值预测的数据增强框架，结合深度学习与数据增强模型，研究合适的数据增强模型，结果显示SMOTE策略适应性好，Conv - LSTM和BD - LSTM各有优势。


<details>
  <summary>Details</summary>
Motivation: 极值预测是具有挑战性的领域，希望通过数据增强方法提高预测效果。

Method: 采用深度学习模型（Conv - LSTM和BD - LSTM）结合GAN和SMOTE等数据增强模型进行多步提前预测，研究合适的数据增强模型，提出基于相关函数纳入数据增强的策略。

Result: SMOTE策略在长短周期预测中适应性好，性能提升；Conv - LSTM在周期性稳定数据集表现好，BD - LSTM在混沌或非平稳序列表现好。

Conclusion: SMOTE策略适合极值预测，Conv - LSTM和BD - LSTM有互补优势。

Abstract: Data augmentation with generative adversarial networks (GANs) has been
popular for class imbalance problems, mainly for pattern classification and
computer vision-related applications. Extreme value forecasting is a
challenging field that has various applications from finance to climate change
problems. In this study, we present a data augmentation framework for extreme
value forecasting. In this framework, our focus is on forecasting extreme
values using deep learning models in combination with data augmentation models
such as GANs and synthetic minority oversampling technique (SMOTE). We use deep
learning models such as convolutional long short-term memory (Conv-LSTM) and
bidirectional long short-term memory (BD-LSTM) networks for multistep ahead
prediction featuring extremes. We investigate which data augmentation models
are the most suitable, taking into account the prediction accuracy overall and
at extreme regions, along with computational efficiency. We also present novel
strategies for incorporating data augmentation, considering extreme values
based on a relevance function. Our results indicate that the SMOTE-based
strategy consistently demonstrated superior adaptability, leading to improved
performance across both short- and long-horizon forecasts. Conv-LSTM and
BD-LSTM exhibit complementary strengths: the former excels in periodic, stable
datasets, while the latter performs better in chaotic or non-stationary
sequences.

</details>


### [54] [OpenTSLM: Time-Series Language Models for Reasoning over Multivariate Medical Text- and Time-Series Data](https://arxiv.org/abs/2510.02410)
*Patrick Langer,Thomas Kaar,Max Rosenblattl,Maxwell A. Xu,Winnie Chow,Martin Maritsch,Aradhana Verma,Brian Han,Daniel Seung Kim,Henry Chubb,Scott Ceresnak,Aydin Zahedivash,Alexander Tarlochan Singh Sandhu,Fatima Rodriguez,Daniel McDuff,Elgar Fleisch,Oliver Aalami,Filipe Barata,Paul Schmiedmayer*

Main category: cs.LG

TL;DR: 本文提出OpenTSLM系列时间序列语言模型，集成时间序列到预训练大模型，在多任务中表现优于基线模型，代码等开源。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理时间序列数据方面存在不足，为解决该问题提出OpenTSLM。

Method: 提出OpenTSLM-SoftPrompt和OpenTSLM-Flamingo两种架构，通过不同方式集成时间序列与文本，在多个文本-时间序列推理任务上进行基准测试。

Result: OpenTSLM模型在各任务中优于基线模型，OpenTSLM-Flamingo在长序列表现更好且内存需求稳定。

Conclusion: OpenTSLM能有效处理时间序列数据，具备较强推理能力，开源资源利于后续研究。

Abstract: LLMs have emerged as powerful tools for interpreting multimodal data. In
medicine, they hold particular promise for synthesizing large volumes of
clinical information into actionable insights and digital health applications.
Yet, a major limitation remains their inability to handle time series. To
overcome this gap, we present OpenTSLM, a family of Time Series Language Models
(TSLMs) created by integrating time series as a native modality to pretrained
LLMs, enabling reasoning over multiple time series of any length. We
investigate two architectures for OpenTSLM. The first, OpenTSLM-SoftPrompt,
models time series implicitly by concatenating learnable time series tokens
with text tokens via soft prompting. Although parameter-efficient, we
hypothesize that explicit time series modeling scales better and outperforms
implicit approaches. We thus introduce OpenTSLM-Flamingo, which integrates time
series with text via cross-attention. We benchmark both variants against
baselines that treat time series as text tokens or plots, across a suite of
text-time-series Chain-of-Thought (CoT) reasoning tasks. We introduce three
datasets: HAR-CoT, Sleep-CoT, and ECG-QA-CoT. Across all, OpenTSLM models
outperform baselines, reaching 69.9 F1 in sleep staging and 65.4 in HAR,
compared to 9.05 and 52.2 for finetuned text-only models. Notably, even
1B-parameter OpenTSLM models surpass GPT-4o (15.47 and 2.95). OpenTSLM-Flamingo
matches OpenTSLM-SoftPrompt in performance and outperforms on longer sequences,
while maintaining stable memory requirements. By contrast, SoftPrompt grows
exponentially in memory with sequence length, requiring around 110 GB compared
to 40 GB VRAM when training on ECG-QA with LLaMA-3B. Expert reviews by
clinicians find strong reasoning capabilities exhibited by OpenTSLMs on ECG-QA.
To facilitate further research, we provide all code, datasets, and models
open-source.

</details>


### [55] [Signature-Informed Transformer for Asset Allocation](https://arxiv.org/abs/2510.03129)
*Yoontae Hwang,Stefan Zohren*

Main category: cs.LG

TL;DR: 提出Signature - Informed Transformer (SIT)框架用于资产配置，在S&P 100数据上表现优于传统和深度学习基线模型，表明特定目标和归纳偏差对风险资本分配很重要。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习预测器在稳健资产配置中因目标不匹配和误差放大而失败的问题。

Method: 引入SIT框架，使用路径签名进行资产动态几何表示，采用签名增强注意力机制嵌入金融归纳偏差，直接优化风险感知金融目标来学习端到端分配策略。

Result: 在每日S&P 100股票数据上，SIT显著优于传统和深度学习基线模型，尤其对比先预测后优化的模型。

Conclusion: 投资组合感知目标和几何感知归纳偏差对机器学习系统中的风险感知资本分配至关重要。

Abstract: Robust asset allocation is a key challenge in quantitative finance, where
deep-learning forecasters often fail due to objective mismatch and error
amplification. We introduce the Signature-Informed Transformer (SIT), a novel
framework that learns end-to-end allocation policies by directly optimizing a
risk-aware financial objective. SIT's core innovations include path signatures
for a rich geometric representation of asset dynamics and a signature-augmented
attention mechanism embedding financial inductive biases, like lead-lag
effects, into the model. Evaluated on daily S\&P 100 equity data, SIT
decisively outperforms traditional and deep-learning baselines, especially when
compared to predict-then-optimize models. These results indicate that
portfolio-aware objectives and geometry-aware inductive biases are essential
for risk-aware capital allocation in machine-learning systems. The code is
available at:
https://github.com/Yoontae6719/Signature-Informed-Transformer-For-Asset-Allocation

</details>


### [56] [RainSeer: Fine-Grained Rainfall Reconstruction via Physics-Guided Modeling](https://arxiv.org/abs/2510.02414)
*Lin Chen,Jun Chen,Minghui Qiu,Shuxin Zhong,Binghong Chen,Kaishun Wu*

Main category: cs.LG

TL;DR: 提出RainSeer框架用于高分辨率降雨场重建，在两个公开数据集上评估显示比现有方法有改进。


<details>
  <summary>Details</summary>
Motivation: 现有空间插值方法在重建高分辨率降雨场时存在过度平滑问题，无法捕捉急剧变化和局部极端情况。

Method: 引入基于物理结构先验的RainSeer框架，采用物理信息的两阶段架构，包括结构到点映射器和地理感知降雨解码器。

Result: 在RAIN - F和MeteoNet两个数据集上评估，相比现有基线减少MAE超13.31%，显著提高重建降雨场的结构保真度。

Conclusion: RainSeer框架在高分辨率降雨场重建方面有效，优于现有方法。

Abstract: Reconstructing high-resolution rainfall fields is essential for flood
forecasting, hydrological modeling, and climate analysis. However, existing
spatial interpolation methods-whether based on automatic weather station (AWS)
measurements or enhanced with satellite/radar observations often over-smooth
critical structures, failing to capture sharp transitions and localized
extremes. We introduce RainSeer, a structure-aware reconstruction framework
that reinterprets radar reflectivity as a physically grounded structural
prior-capturing when, where, and how rain develops. This shift, however,
introduces two fundamental challenges: (i) translating high-resolution
volumetric radar fields into sparse point-wise rainfall observations, and (ii)
bridging the physical disconnect between aloft hydro-meteors and ground-level
precipitation. RainSeer addresses these through a physics-informed two-stage
architecture: a Structure-to-Point Mapper performs spatial alignment by
projecting mesoscale radar structures into localized ground-level rainfall,
through a bidirectional mapping, and a Geo-Aware Rain Decoder captures the
semantic transformation of hydro-meteors through descent, melting, and
evaporation via a causal spatiotemporal attention mechanism. We evaluate
RainSeer on two public datasets-RAIN-F (Korea, 2017-2019) and MeteoNet (France,
2016-2018)-and observe consistent improvements over state-of-the-art baselines,
reducing MAE by over 13.31% and significantly enhancing structural fidelity in
reconstructed rainfall fields.

</details>


### [57] [How to Train Your Advisor: Steering Black-Box LLMs with Advisor Models](https://arxiv.org/abs/2510.02453)
*Parth Asawa,Alan Zhu,Matei Zaharia,Alexandros G. Dimakis,Joseph E. Gonzalez*

Main category: cs.LG

TL;DR: 提出Advisor Models用于黑盒模型动态优化，在多领域表现优于静态提示优化器，有良好泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有静态提示优化无法适应不同输入、用户或环境，需对黑盒模型进行动态优化。

Method: 使用强化学习训练轻量级参数策略的Advisor Models，根据环境奖励信号逐实例塑造黑盒模型行为。

Result: 在涉及推理和个性化的多个领域，Advisor Models优于静态提示优化器，可跨黑盒模型迁移，对分布外输入有鲁棒性。

Conclusion: 通过Advisor Models对黑盒模型进行动态优化是实现个性化和环境自适应AI的有前景方向。

Abstract: Foundation models are increasingly deployed as black-box services, where
model weights cannot be modified and customization is limited to prompting.
While static prompt optimization has shown promise, it produces a single fixed
prompt that fails to adapt to different inputs, users, or environments. We
introduce Advisor Models, lightweight parametric policies trained with
reinforcement learning to reactively issue natural language steering
instructions in-context to black-box models. The advisor is a second small
model that sits between the input and the model, shaping behavior on a
per-instance basis using reward signals from the environment. Across multiple
domains involving reasoning and personalization, we show that Advisor Models
outperform static prompt optimizers, discovering environment dynamics and
improving downstream task performance. We also demonstrate the generalizability
of advisors by transferring them across black-box models, as well as the
framework's ability to achieve specialization while retaining robustness to
out-of-distribution inputs. Viewed more broadly, Advisor Models provide a
learnable interface to black-box systems where the advisor acts as a
parametric, environment-specific memory. We argue that dynamic optimization of
black-box models via Advisor Models is a promising direction for enabling
personalization and environment-adaptable AI with frontier-level capabilities.

</details>


### [58] [Market-Based Data Subset Selection -- Principled Aggregation of Multi-Criteria Example Utility](https://arxiv.org/abs/2510.02456)
*Ashish Jha,Valentin Leplat,AH Phan*

Main category: cs.LG

TL;DR: 提出基于市场的训练数据选择器，理论上有最大熵聚合特性，实验在GSM8K和AGNews上表现良好，统一多信号数据整理。


<details>
  <summary>Details</summary>
Motivation: 训练数据有用子集选择难，示例效用信号异构且权重设置随意。

Method: 提出基于市场的选择器，通过成本函数预测市场定价，明确处理令牌预算，用轻量级多样性头提高覆盖。

Result: 在GSM8K上与强单信号基线表现相当，减少种子方差和选择开销；在AGNews上有竞争力的准确率，平衡和稳定性提升。

Conclusion: 该框架在固定计算下统一了用于提示级推理和分类的多信号数据整理。

Abstract: Selecting a small yet useful subset of training data is hard because signals
of example utility (uncertainty, rarity, diversity, etc.) are heterogeneous and
typically combined with ad hoc weights. We propose a market-based selector that
prices each example via a cost-function prediction market (LMSR), signals act
as traders, a single liquidity parameter controls concentration, and topic-wise
normalization stabilizes calibration. Token budgets are handled explicitly by a
price-per-token rule $\rho=p/\ell^{\gamma}$, with $\gamma$ exposing an
interpretable length bias; a lightweight diversity head improves coverage. We
quantify coverage via topic cluster coverage and effective sample size. On the
theory side, we show that LMSR implements a maximum-entropy aggregation with
exponential weighting and a convex objective, yielding transparent knobs for
aggregation strength. Empirically, on GSM8K (60k-token budget) the market with
diversity achieves parity with strong single-signal baselines while reducing
seed variance and incurring $<\!0.1$ GPU-hr selection overhead; on AGNews at
kept=5-25\% the market (with light balancing) delivers competitive accuracy
with improved balance and stability. The framework unifies multi-signal data
curation under fixed compute for prompt-level reasoning and classification.

</details>


### [59] [Assessing the Potential for Catastrophic Failure in Dynamic Post-Training Quantization](https://arxiv.org/abs/2510.02457)
*Logan Frank,Paul Ardis*

Main category: cs.LG

TL;DR: 探讨动态PTQ极端故障，用知识蒸馏和强化学习分析量化最坏情况，结果显示存在性能大幅下降情况，强调实际部署需谨慎。


<details>
  <summary>Details</summary>
Motivation: PTQ虽能降低计算和存储成本，但推理时输入分布可能导致性能大幅下降，在安全关键环境中需研究其程度和原因。

Method: 提出知识蒸馏和强化学习任务，学习网络和位宽策略对，分析量化下的最坏情况。

Result: 证实存在“有害”网络 - 策略对，部分实例性能准确率下降10 - 65%，而“鲁棒”对应物下降<2%。

Conclusion: 这是理解PTQ故障案例的初步一步，强调实际部署要谨慎，鼓励未来深入研究鲁棒性和安全考量。

Abstract: Post-training quantization (PTQ) has recently emerged as an effective tool
for reducing the computational complexity and memory usage of a neural network
by representing its weights and activations with lower precision. While this
paradigm has shown great success in lowering compute and storage costs, there
is the potential for drastic performance reduction depending upon the
distribution of inputs experienced in inference. When considering possible
deployment in safety-critical environments, it is important to investigate the
extent of potential performance reduction, and what characteristics of input
distributions may give rise to this reduction. In this work, we explore the
idea of extreme failure stemming from dynamic PTQ and formulate a knowledge
distillation and reinforcement learning task to learn a network and bit-width
policy pair such that catastrophic failure under quantization is analyzed in
terms of worst case potential. Our results confirm the existence of this
"detrimental" network-policy pair, with several instances demonstrating
performance reductions in the range of 10-65% in accuracy, compared to their
"robust" counterparts encountering a <2% decrease. From systematic
experimentation and analyses, we also provide an initial exploration into
points at highest vulnerability. While our results represent an initial step
toward understanding failure cases introduced by PTQ, our findings ultimately
emphasize the need for caution in real-world deployment scenarios. We hope this
work encourages more rigorous examinations of robustness and a greater emphasis
on safety considerations for future works within the broader field of deep
learning.

</details>


### [60] [SAGE: Streaming Agreement-Driven Gradient Sketches for Representative Subset Selection](https://arxiv.org/abs/2510.02470)
*Ashish Jha,Salman Ahmadi-Asl*

Main category: cs.LG

TL;DR: 提出SAGE数据子集选择方法，能降低训练计算和内存开销，保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络在大数据集上训练计算和能源消耗大。

Method: 提出SAGE方法，维护梯度几何的紧凑FD草图，优先选择草图梯度与共识方向一致的样本，采用两阶段、GPU友好的管道。

Result: 在多个基准测试中，SAGE在小保留率预算下训练，与全数据训练和最近的子集选择基线相比保持了有竞争力的准确率，减少了端到端计算和峰值内存。

Conclusion: SAGE是一种实用的恒定内存替代方案，可补充剪枝和模型压缩以实现高效训练。

Abstract: Training modern neural networks on large datasets is computationally and
energy intensive. We present SAGE, a streaming data-subset selection method
that maintains a compact Frequent Directions (FD) sketch of gradient geometry
in $O(\ell D)$ memory and prioritizes examples whose sketched gradients align
with a consensus direction. The approach eliminates $N \times N$ pairwise
similarities and explicit $N \times \ell$ gradient stores, yielding a simple
two-pass, GPU-friendly pipeline. Leveraging FD's deterministic approximation
guarantees, we analyze how agreement scoring preserves gradient energy within
the principal sketched subspace. Across multiple benchmarks, SAGE trains with
small kept-rate budgets while retaining competitive accuracy relative to
full-data training and recent subset-selection baselines, and reduces
end-to-end compute and peak memory. Overall, SAGE offers a practical,
constant-memory alternative that complements pruning and model compression for
efficient training.

</details>


### [61] [Uncertainty-Guided Model Selection for Tabular Foundation Models in Biomolecule Efficacy Prediction](https://arxiv.org/abs/2510.02476)
*Jie Li,Andrew McCarthy,Zhizhuo Zhang,Stephen Young*

Main category: cs.LG

TL;DR: 研究提出不确定性引导的模型选择策略用于生物分子功效预测，TabPFN模型表现出色，模型预测的IQR与真实误差负相关，选择低IQR模型集成表现更佳。


<details>
  <summary>Details</summary>
Motivation: 上下文学习器性能对上下文敏感，需解决在无真实标签情况下为集成选择最佳模型的问题。

Method: 研究不确定性引导的模型选择策略，利用TabPFN模型，基于简单序列特征，以模型预测的IQR作为不确定性度量。

Result: TabPFN模型使用简单序列特征可超越专业的最先进预测器，IQR与真实预测误差负相关，选择低平均IQR的模型集成表现优于简单集成或单模型。

Conclusion: 模型不确定性是优化生物分子功效预测的强大无标签启发式方法。

Abstract: In-context learners like TabPFN are promising for biomolecule efficacy
prediction, where established molecular feature sets and relevant experimental
results can serve as powerful contextual examples. However, their performance
is highly sensitive to the provided context, making strategies like post-hoc
ensembling of models trained on different data subsets a viable approach. An
open question is how to select the best models for the ensemble without access
to ground truth labels. In this study, we investigate an uncertainty-guided
strategy for model selection. We demonstrate on an siRNA knockdown efficacy
task that a TabPFN model using simple sequence-based features can surpass
specialized state-of-the-art predictors. We also show that the model's
predicted inter-quantile range (IQR), a measure of its uncertainty, has a
negative correlation with true prediction error. By selecting and averaging an
ensemble of models with the lowest mean IQR, we achieve superior performance
compared to naive ensembling or using a single model trained on all available
data. This finding highlights model uncertainty as a powerful, label-free
heuristic for optimizing biomolecule efficacy predictions.

</details>


### [62] [Litespark Technical Report: High-Throughput, Energy-Efficient LLM Training Framework](https://arxiv.org/abs/2510.02483)
*Nii Osae Osae Dade,Moinul Hossain Rahat*

Main category: cs.LG

TL;DR: 提出Litespark预训练框架，优化transformer层，在Llama模型上实现训练吞吐量提升和能耗降低，且具有广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型训练时间长和能耗高的问题。

Method: 对transformer注意力和MLP层进行有针对性优化，结合架构改进和算法增强，提高模型FLOPs利用率。

Result: 在3B和30B参数的Llama模型上，训练吞吐量提升2 - 6倍，能耗降低55% - 83%。

Conclusion: 优化方法与模型和硬件无关，适用于多种transformer架构及训练后阶段。

Abstract: Training Large Language Models (LLMs) is plagued by long training times and
massive energy consumption, with modern models requiring months of computation
and gigawatt-hours of electricity. In light of these challenges,we introduce
Litespark, a novel pre-training framework that addresses these inefficiencies
through targeted optimizations to transformer attention and MLP layers. Our
approach combines architectural improvements with algorithmic enhancements to
maximize Model FLOPs Utilization (MFU) while maintaining compatibility with
standard transformer implementations. Comprehensive benchmarking on 3B and 30B
parameter Llama models using the SlimPajama-627B dataset demonstrates
substantial performance gains: 2x-6x training throughput improvement and
$55\%-83$% energy consumption reduction across multi-node H200 GPU clusters.
These optimizations are model- and hardware-agnostic, enabling broad
applicability across transformer architectures and extending to post-training
phases including supervised fine-tuning and direct preference optimization.

</details>


### [63] [From Pixels to Factors: Learning Independently Controllable State Variables for Reinforcement Learning](https://arxiv.org/abs/2510.02484)
*Rafael Rodriguez-Sanchez,Cameron Allen,George Konidaris*

Main category: cs.LG

TL;DR: 提出Action - Controllable Factorization (ACF)解决表征问题，在三个基准测试中表现优于基线算法。


<details>
  <summary>Details</summary>
Motivation: 现有利用因子化马尔可夫决策过程的算法需先验因子化表示，深度强化学习无法利用因子化结构，需解决表征问题。

Method: 提出ACF对比学习方法，利用稀疏性挖掘独立可控的潜在变量。

Result: 在Taxi、FourRooms和MiniGrid - DoorKey三个有已知因子化结构的基准测试中，从像素观测中直接恢复真实可控因子，且表现优于基线解纠缠算法。

Conclusion: ACF方法能有效解决表征问题，在相关基准测试中有良好表现。

Abstract: Algorithms that exploit factored Markov decision processes are far more
sample-efficient than factor-agnostic methods, yet they assume a factored
representation is known a priori -- a requirement that breaks down when the
agent sees only high-dimensional observations. Conversely, deep reinforcement
learning handles such inputs but cannot benefit from factored structure. We
address this representation problem with Action-Controllable Factorization
(ACF), a contrastive learning approach that uncovers independently controllable
latent variables -- state components each action can influence separately. ACF
leverages sparsity: actions typically affect only a subset of variables, while
the rest evolve under the environment's dynamics, yielding informative data for
contrastive training. ACF recovers the ground truth controllable factors
directly from pixel observations on three benchmarks with known factored
structure -- Taxi, FourRooms, and MiniGrid-DoorKey -- consistently
outperforming baseline disentanglement algorithms.

</details>


### [64] [Online Learning in the Random Order Model](https://arxiv.org/abs/2510.02820)
*Martino Bernasconi,Andrea Celli,Riccardo Colini-Baldeschi,Federico Fusco,Stefano Leonardi,Matteo Russo*

Main category: cs.LG

TL;DR: 本文提出通用模板使随机学习算法适应随机顺序模型，恢复了部分问题的改进遗憾界，还研究在线分类并指出随机顺序下可学习性由VC维度表征。


<details>
  <summary>Details</summary>
Motivation: 随机顺序模型在有限时间有非平稳性，会影响随机学习算法性能，且随机模型的无遗憾算法在随机顺序实例上可能失效，需让随机学习算法适应随机顺序模型。

Method: 提出通用模板来适配随机学习算法到随机顺序模型。

Result: 恢复了预测延迟、带约束在线学习和带切换成本多臂老虎机问题的改进遗憾界；证明随机顺序下在线分类的可学习性由VC维度而非Littlestone维度表征。

Conclusion: 所提通用模板能让随机学习算法适应随机顺序模型，且不显著影响遗憾保证，还揭示了随机顺序模型与一般对抗模型在在线分类可学习性上的差异。

Abstract: In the random-order model for online learning,
  the sequence of losses is chosen upfront by an adversary and presented to the
learner
  after a random permutation. Any random-order input is \emph{asymptotically}
equivalent to a stochastic i.i.d. one, but, for finite times, it may exhibit
significant {\em non-stationarity}, which can hinder the performance of
stochastic learning algorithms.
  While algorithms for adversarial inputs naturally maintain their regret
guarantees in random order, simple no-regret algorithms exist for the
stochastic model that fail against random-order instances.
  In this paper, we propose a general template to adapt stochastic learning
algorithms to the random-order model without substantially affecting their
regret guarantees. This allows us to recover improved regret bounds for
prediction with delays, online learning with constraints, and bandits with
switching costs. Finally, we investigate online classification and prove that,
in random order, learnability is characterized by the VC dimension rather than
the Littlestone dimension, thus providing a further separation from the general
adversarial model.

</details>


### [65] [Improved Robustness of Deep Reinforcement Learning for Control of Time-Varying Systems by Bounded Extremum Seeking](https://arxiv.org/abs/2510.02490)
*Shaifalee Saxena,Alan Williams,Rafael Fierro,Alexander Scheinker*

Main category: cs.LG

TL;DR: 研究用鲁棒模型独立有界极值搜索（ES）反馈控制提升一类非线性时变系统深度强化学习（DRL）控制器的鲁棒性，展示DRL与有界ES结合的混合控制器性能更优，并给出数值研究。


<details>
  <summary>Details</summary>
Motivation: DRL在系统模型随时间快速变化时性能急剧下降，有界ES收敛速度随调参数量增加而减慢且可能陷入局部极小值，需要提升控制器性能与鲁棒性。

Method: 将DRL与有界ES结合，构建混合控制器，利用DRL学习历史数据快速控制多参数系统，有界ES确保对时间变化的鲁棒性。

Result: 证明DRL和有界ES结合的混合控制器性能超过单独部分之和。

Conclusion: DRL与有界ES结合的混合控制器可有效提升非线性时变系统控制器的性能与鲁棒性。

Abstract: In this paper, we study the use of robust model independent bounded extremum
seeking (ES) feedback control to improve the robustness of deep reinforcement
learning (DRL) controllers for a class of nonlinear time-varying systems. DRL
has the potential to learn from large datasets to quickly control or optimize
the outputs of many-parameter systems, but its performance degrades
catastrophically when the system model changes rapidly over time. Bounded ES
can handle time-varying systems with unknown control directions, but its
convergence speed slows down as the number of tuned parameters increases and,
like all local adaptive methods, it can get stuck in local minima. We
demonstrate that together, DRL and bounded ES result in a hybrid controller
whose performance exceeds the sum of its parts with DRL taking advantage of
historical data to learn how to quickly control a many-parameter system to a
desired setpoint while bounded ES ensures its robustness to time variations. We
present a numerical study of a general time-varying system and a combined
ES-DRL controller for automatic tuning of the Low Energy Beam Transport section
at the Los Alamos Neutron Science Center linear particle accelerator.

</details>


### [66] [Hyperparameter Loss Surfaces Are Simple Near their Optima](https://arxiv.org/abs/2510.02721)
*Nicholas Lourie,He He,Kyunghyun Cho*

Main category: cs.LG

TL;DR: 本文发现超参数损失面的新结构，提出新理论和工具，基于随机搜索技术揭示渐近区域，得到新分布和渐近法则，可用于新分析并开源代码。


<details>
  <summary>Details</summary>
Motivation: 超参数对模型能力影响大，但现代模型大难以广泛搜索，且缺乏理解超参数损失面的工具。

Method: 基于随机搜索开发新技术，揭示渐近区域，发现最佳分数的新分布。

Result: 得到定义损失面的特征、随机搜索的渐近法则，可进行新分析。

Conclusion: 新工具可用于分析最佳性能置信区间、确定有效超参数数量等，代码已开源。

Abstract: Hyperparameters greatly impact models' capabilities; however, modern models
are too large for extensive search. Instead, researchers design recipes that
train well across scales based on their understanding of the hyperparameters.
Despite this importance, few tools exist for understanding the hyperparameter
loss surface. We discover novel structure in it and propose a new theory
yielding such tools. The loss surface is complex, but as you approach the
optimum simple structure emerges. It becomes characterized by a few basic
features, like its effective dimension and the best possible loss. To uncover
this asymptotic regime, we develop a novel technique based on random search.
Within this regime, the best scores from random search take on a new
distribution we discover. Its parameters are exactly the features defining the
loss surface in the asymptotic regime. From these features, we derive a new
asymptotic law for random search that can explain and extrapolate its
convergence. These new tools enable new analyses, such as confidence intervals
for the best possible performance or determining the effective number of
hyperparameters. We make these tools available at
https://github.com/nicholaslourie/opda .

</details>


### [67] [CHORD: Customizing Hybrid-precision On-device Model for Sequential Recommendation with Device-cloud Collaboration](https://arxiv.org/abs/2510.03038)
*Tianqi Liu,Kairui Fu,Shengyu Zhang,Wenyan Fan,Zhaocheng Du,Jieming Zhu,Fan Wu,Fei Wu*

Main category: cs.LG

TL;DR: 提出CHORD框架，利用通道混合精度量化实现个性化和资源自适应部署，实验验证其准确性、效率和适应性。


<details>
  <summary>Details</summary>
Motivation: 迁移模型到设备需压缩，但现有量化方法忽略用户兴趣，设备微调增加计算负担。

Method: 提出CHORD框架，利用通道混合精度量化，通过云辅助超网络模块识别关键参数，多粒度分析参数敏感性，设备端混合精度量化，2位编码量化策略。

Result: 在三个真实数据集和两个骨干模型上实验，证明了CHORD的准确性、效率和适应性。

Conclusion: CHORD能同时实现个性化和资源自适应部署，避免昂贵的再训练周期，减少通信开销。

Abstract: With the advancement of mobile device capabilities, deploying reranking
models directly on devices has become feasible, enabling real-time contextual
recommendations. When migrating models from cloud to devices, resource
heterogeneity inevitably necessitates model compression. Recent quantization
methods show promise for efficient deployment, yet they overlook
device-specific user interests, resulting in compromised recommendation
accuracy. While on-device finetuning captures personalized user preference, it
imposes additional computational burden through local retraining. To address
these challenges, we propose a framework for \underline{\textbf{C}}ustomizing
\underline{\textbf{H}}ybrid-precision \underline{\textbf{O}}n-device model for
sequential \underline{\textbf{R}}ecommendation with
\underline{\textbf{D}}evice-cloud collaboration (\textbf{CHORD}), leveraging
channel-wise mixed-precision quantization to simultaneously achieve
personalization and resource-adaptive deployment. CHORD distributes randomly
initialized models across heterogeneous devices and identifies user-specific
critical parameters through auxiliary hypernetwork modules on the cloud. Our
parameter sensitivity analysis operates across multiple granularities (layer,
filter, and element levels), enabling precise mapping from user profiles to
quantization strategy. Through on-device mixed-precision quantization, CHORD
delivers dynamic model adaptation and accelerated inference without
backpropagation, eliminating costly retraining cycles. We minimize
communication overhead by encoding quantization strategies using only 2 bits
per channel instead of 32-bit weights. Experiments on three real-world datasets
with two popular backbones (SASRec and Caser) demonstrate the accuracy,
efficiency, and adaptivity of CHORD.

</details>


### [68] [Beyond Imitation: Recovering Dense Rewards from Demonstrations](https://arxiv.org/abs/2510.02493)
*Jiangnan Li,Thuy-Trang Vu,Ehsan Abbasnejad,Gholamreza Haffari*

Main category: cs.LG

TL;DR: 本文挑战传统观点，建立监督微调（SFT）与逆强化学习的等价性，恢复隐含的密集奖励信号并用于强化学习，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 挑战传统上将SFT视为简单模仿学习的观点，挖掘SFT更多潜力。

Method: 证明SFT目标是逆Q学习的特例，通过制定相对基线的奖励函数恢复密集奖励信号，使用Dense - Path REINFORCE方法进一步用强化学习改进策略。

Result: Dense - Path REINFORCE方法在指令跟随基准测试中始终优于原始SFT模型。

Conclusion: SFT不仅是策略模仿，还是强大的奖励学习机制，为利用专家示范开辟新可能。

Abstract: Conventionally, supervised fine-tuning (SFT) is treated as a simple imitation
learning process that only trains a policy to imitate expert behavior on
demonstration datasets. In this work, we challenge this view by establishing a
fundamental equivalence between SFT and Inverse Reinforcement Learning. We
prove that the SFT objective is a special case of Inverse Q-Learning, which
implies that the SFT process does not just learn a policy, but also an
implicit, dense, token-level reward model that explains the expert
demonstrations. We then show how to recover this dense reward signal directly
from the SFT model by formulating a baseline-relative reward function. The
availability of such a dense reward model offers numerous benefits, providing
granular credit assignment for each token generated. We demonstrate one key
application by using these recovered rewards to further improve the policy with
reinforcement learning. Our method, Dense-Path REINFORCE, consistently
outperforms the original SFT models on instruction-following benchmarks. This
work reframes SFT not merely as policy imitation but as a powerful reward
learning mechanism, opening new possibilities for leveraging expert
demonstrations.

</details>


### [69] [In-memory Training on Analog Devices with Limited Conductance States via Multi-tile Residual Learning](https://arxiv.org/abs/2510.02516)
*Jindan Li,Zhaoxian Wu,Gaowen Liu,Tayfun Gokmen,Tianyi Chen*

Main category: cs.LG

TL;DR: 本文提出残差学习框架，以解决有限状态忆阻器在片上训练中精度不足问题，实验显示该方法优于现有策略且硬件开销适中。


<details>
  <summary>Details</summary>
Motivation: 有效内存训练需至少8位电导状态，而实际忆阻器多为4位分辨率，限制更新精度会降低训练准确率，为实现有限状态设备的片上训练。

Method: 提出残差学习框架，在多个交叉开关阵列上顺序学习以补偿低精度权重更新的残差误差。

Result: 理论分析表明最优性差距随阵列数量缩小，达到线性收敛率；标准图像分类基准实验显示该方法在有限状态设置下优于现有内存模拟训练策略。

Conclusion: 提出的残差学习框架能在有限状态设备上有效进行训练，硬件开销适中。

Abstract: Analog in-memory computing (AIMC) accelerators enable efficient deep neural
network computation directly within memory using resistive crossbar arrays,
where model parameters are represented by the conductance states of memristive
devices. However, effective in-memory training typically requires at least
8-bit conductance states to match digital baselines. Realizing such
fine-grained states is costly and often requires complex noise mitigation
techniques that increase circuit complexity and energy consumption. In
practice, many promising memristive devices such as ReRAM offer only about
4-bit resolution due to fabrication constraints, and this limited update
precision substantially degrades training accuracy. To enable on-chip training
with these limited-state devices, this paper proposes a \emph{residual
learning} framework that sequentially learns on multiple crossbar tiles to
compensate the residual errors from low-precision weight updates. Our
theoretical analysis shows that the optimality gap shrinks with the number of
tiles and achieves a linear convergence rate. Experiments on standard image
classification benchmarks demonstrate that our method consistently outperforms
state-of-the-art in-memory analog training strategies under limited-state
settings, while incurring only moderate hardware overhead as confirmed by our
cost analysis.

</details>


### [70] [Differentially Private Wasserstein Barycenters](https://arxiv.org/abs/2510.03021)
*Anming Gu,Sasidhar Kunapuli,Mark Bun,Edward Chien,Kristjan Greenewald*

Main category: cs.LG

TL;DR: 提出首个在差分隐私下计算Wasserstein重心的算法，在多数据集上有良好的准确性 - 隐私权衡。


<details>
  <summary>Details</summary>
Motivation: 输入的概率测度基于敏感数据集构建，需要差分隐私处理。

Method: 提出在差分隐私下计算Wasserstein重心的算法。

Result: 在合成数据、MNIST和大规模美国人口数据集上，方法能产生高质量的私有重心，有强准确性 - 隐私权衡。

Conclusion: 成功提出在差分隐私下计算Wasserstein重心的有效算法。

Abstract: The Wasserstein barycenter is defined as the mean of a set of probability
measures under the optimal transport metric, and has numerous applications
spanning machine learning, statistics, and computer graphics. In practice these
input measures are empirical distributions built from sensitive datasets,
motivating a differentially private (DP) treatment. We present, to our
knowledge, the first algorithms for computing Wasserstein barycenters under
differential privacy. Empirically, on synthetic data, MNIST, and large-scale
U.S. population datasets, our methods produce high-quality private barycenters
with strong accuracy-privacy tradeoffs.

</details>


### [71] [Taming Imperfect Process Verifiers: A Sampling Perspective on Backtracking](https://arxiv.org/abs/2510.03149)
*Dhruv Rohatgi,Abhishek Shetty,Donya Saless,Yuchen Li,Ankur Moitra,Andrej Risteski,Dylan J. Foster*

Main category: cs.LG

TL;DR: 提出新的测试时采样算法VGB，用回溯提高对验证器错误的鲁棒性，在合成和真实语言建模任务上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 结合语言模型和过程验证器的测试时算法设计空间和计算扩展性不明确，学习高质量验证器成本高，且验证器错误会导致标准解码技术失败。

Method: 引入新算法VGB，将自回归生成视为部分生成树的随机游走，回溯具有概率性，推广了Sinclair - Jerrum随机游走。

Result: 在合成和真实语言建模任务上，VGB在多种指标上优于基线。

Conclusion: VGB算法能提高对验证器错误的鲁棒性，是更优的解码策略。

Abstract: Test-time algorithms that combine the generative power of language models
with process verifiers that assess the quality of partial generations offer a
promising lever for eliciting new reasoning capabilities, but the algorithmic
design space and computational scaling properties of such approaches are still
opaque, and their benefits are far from apparent when one accounts for the cost
of learning a high-quality verifier. Our starting point is the observation that
seemingly benign errors in a learned verifier can lead to catastrophic failures
for standard decoding techniques due to error amplification during the course
of generation. We then ask: can this be improved with more sophisticated
decoding strategies?
  We introduce a new process-guided test-time sampling algorithm, VGB, which
uses theoretically grounded backtracking to achieve provably better robustness
to verifier errors. VGB interprets autoregressive generation as a random walk
on a tree of partial generations, with transition probabilities guided by the
process verifier and base model; crucially, backtracking occurs
probabilistically. This process generalizes the seminal Sinclair-Jerrum random
walk (Sinclair & Jerrum, 1989) from the literature on approximate counting and
sampling in theoretical computer science, and a conceptual contribution of our
work is to highlight parallels with this literature. Empirically, we
demonstrate on both synthetic and real language modeling tasks that VGB
outperforms baselines on a variety of metrics.

</details>


### [72] [Graph Generation with Spectral Geodesic Flow Matching](https://arxiv.org/abs/2510.02520)
*Xikun Huang,Tianyu Ruan,Chihao Zhang,Shihua Zhang*

Main category: cs.LG

TL;DR: 提出SFMG框架用于图生成，融合谱几何与流匹配，有多项优势且性能良好。


<details>
  <summary>Details</summary>
Motivation: 现有图生成方法常忽略图特征向量诱导的几何结构和全局结构。

Method: 提出Spectral Geodesic Flow Matching (SFMG)框架，将输入和目标图嵌入连续黎曼流形，定义嵌入间的测地流并匹配分布以生成图。

Result: SFMG在多个基准测试中表现与现有最优方法相当，比基于扩散的模型快30倍，能泛化到未见图规模。

Conclusion: SFMG通过融合谱几何与流匹配为图合成提供新方法。

Abstract: Graph generation is a fundamental task with wide applications in modeling
complex systems. Although existing methods align the spectrum or degree profile
of the target graph, they often ignore the geometry induced by eigenvectors and
the global structure of the graph. In this work, we propose Spectral Geodesic
Flow Matching (SFMG), a novel framework that uses spectral eigenmaps to embed
both input and target graphs into continuous Riemannian manifolds. We then
define geodesic flows between embeddings and match distributions along these
flows to generate output graphs. Our method yields several advantages: (i)
captures geometric structure beyond eigenvalues, (ii) supports flexible
generation of diverse graphs, and (iii) scales efficiently. Empirically, SFMG
matches the performance of state-of-the-art approaches on graphlet, degree, and
spectral metrics across diverse benchmarks. In particular, it achieves up to
30$\times$ speedup over diffusion-based models, offering a substantial
advantage in scalability and training efficiency. We also demonstrate its
ability to generalize to unseen graph scales. Overall, SFMG provides a new
approach to graph synthesis by integrating spectral geometry with flow
matching.

</details>


### [73] [Model-brain comparison using inter-animal transforms](https://arxiv.org/abs/2510.02523)
*Imran Thobani,Javier Sagastuy-Brena,Aran Nayebi,Jacob Prince,Rosa Cao,Daniel Yamins*

Main category: cs.LG

TL;DR: 本文提出基于IATC的模型与大脑反应对比方法，在多场景验证其效果，证明高预测性与机制识别无固有权衡，支持TDANN作为视觉系统模型。


<details>
  <summary>Details</summary>
Motivation: 当前在比较人工神经网络模型激活与大脑反应的方法上缺乏共识，需要提出合适的比较方法。

Method: 基于Inter - Animal Transform Class (IATC) 提出比较方法，在神经网络模型模拟群体、小鼠群体和人类群体中确定IATC。

Result: IATC能解析神经机制细节，实现神经活动准确预测和机制识别高特异性，为TDANN作为视觉系统模型提供新证据。

Conclusion: IATC实现了有原则的模型 - 大脑比较，优化了以往方法并为相关研究提供背景。

Abstract: Artificial neural network models have emerged as promising mechanistic models
of the brain. However, there is little consensus on the correct method for
comparing model activations to brain responses. Drawing on recent work in
philosophy of neuroscience, we propose a comparison methodology based on the
Inter-Animal Transform Class (IATC) - the strictest set of functions needed to
accurately map neural responses between subjects in an animal population. Using
the IATC, we can map bidirectionally between a candidate model's responses and
brain data, assessing how well the model can masquerade as a typical subject
using the same kinds of transforms needed to map across real subjects. We
identify the IATC in three settings: a simulated population of neural network
models, a population of mouse subjects, and a population of human subjects. We
find that the IATC resolves detailed aspects of the neural mechanism, such as
the non-linear activation function. Most importantly, we find that the IATC
enables accurate predictions of neural activity while also achieving high
specificity in mechanism identification, evidenced by its ability to separate
response patterns from different brain areas while strongly aligning
same-brain-area responses between subjects. In other words, the IATC is a
proof-by-existence that there is no inherent tradeoff between the neural
engineering goal of high model-brain predictivity and the neuroscientific goal
of identifying mechanistically accurate brain models. Using IATC-guided
transforms, we obtain new evidence in favor of topographical deep neural
networks (TDANNs) as models of the visual system. Overall, the IATC enables
principled model-brain comparisons, contextualizing previous findings about the
predictive success of deep learning models of the brain, while improving upon
previous approaches to model-brain comparison.

</details>


### [74] [Distilled Protein Backbone Generation](https://arxiv.org/abs/2510.03095)
*Liyang Xie,Haoran Zhang,Zhendong Wang,Wesley Tansey,Mingyuan Zhou*

Main category: cs.LG

TL;DR: 探索分数蒸馏技术减少蛋白质骨架生成采样步骤，结合多步生成与推理时间噪声调制，使采样速度提升超20倍，推动扩散模型在蛋白质工程应用。


<details>
  <summary>Details</summary>
Motivation: 现有扩散和流基生成模型在蛋白质骨架生成中生成速度慢，限制其在大规模蛋白质发现中的实用性。

Method: 研究如何适配Score identity Distillation (SiD)训练少步蛋白质骨架生成器，结合多步生成与推理时间噪声调制。

Result: 蒸馏后的少步生成器采样速度提升超20倍，在可设计性、多样性和新颖性上与Proteina教师模型相当。

Conclusion: 减少推理成本可实现大规模计算机模拟蛋白质设计，让扩散模型更接近实际蛋白质工程应用。

Abstract: Diffusion- and flow-based generative models have recently demonstrated strong
performance in protein backbone generation tasks, offering unprecedented
capabilities for de novo protein design. However, while achieving notable
performance in generation quality, these models are limited by their generating
speed, often requiring hundreds of iterative steps in the reverse-diffusion
process. This computational bottleneck limits their practical utility in
large-scale protein discovery, where thousands to millions of candidate
structures are needed. To address this challenge, we explore the techniques of
score distillation, which has shown great success in reducing the number of
sampling steps in the vision domain while maintaining high generation quality.
However, a straightforward adaptation of these methods results in unacceptably
low designability. Through extensive study, we have identified how to
appropriately adapt Score identity Distillation (SiD), a state-of-the-art score
distillation strategy, to train few-step protein backbone generators which
significantly reduce sampling time, while maintaining comparable performance to
their pretrained teacher model. In particular, multistep generation combined
with inference time noise modulation is key to the success. We demonstrate that
our distilled few-step generators achieve more than a 20-fold improvement in
sampling speed, while achieving similar levels of designability, diversity, and
novelty as the Proteina teacher model. This reduction in inference cost enables
large-scale in silico protein design, thereby bringing diffusion-based models
closer to real-world protein engineering applications.

</details>


### [75] [AttentiveGRUAE: An Attention-Based GRU Autoencoder for Temporal Clustering and Behavioral Characterization of Depression from Wearable Data](https://arxiv.org/abs/2510.02558)
*Nidhi Soley,Vishal M Patel,Casey O Taylor*

Main category: cs.LG

TL;DR: 提出AttentiveGRUAE用于纵向可穿戴数据的时间聚类和结果预测，在多方面表现优异，有临床解释性。


<details>
  <summary>Details</summary>
Motivation: 设计新模型进行纵向可穿戴数据的时间聚类和结果预测。

Method: 提出AttentiveGRUAE模型，联合优化序列重建、抑郁率预测、行为子类型识别三个目标。

Result: 在GLOBEM 2018 - 2019睡眠数据上聚类和抑郁分类表现优于基线模型，在GLOBEM 2020 - 2021数据上验证了聚类的可重复性和稳定性。

Conclusion: AttentiveGRUAE模型有效，能为风险提供临床可解释的解释。

Abstract: In this study, we present AttentiveGRUAE, a novel attention-based gated
recurrent unit (GRU) autoencoder designed for temporal clustering and
prediction of outcome from longitudinal wearable data. Our model jointly
optimizes three objectives: (1) learning a compact latent representation of
daily behavioral features via sequence reconstruction, (2) predicting
end-of-period depression rate through a binary classification head, and (3)
identifying behavioral subtypes through Gaussian Mixture Model (GMM) based soft
clustering of learned embeddings. We evaluate AttentiveGRUAE on longitudinal
sleep data from 372 participants (GLOBEM 2018-2019), and it demonstrates
superior performance over baseline clustering, domain-aligned self-supervised,
and ablated models in both clustering quality (silhouette score = 0.70 vs
0.32-0.70) and depression classification (AUC = 0.74 vs 0.50-0.67).
Additionally, external validation on cross-year cohorts from 332 participants
(GLOBEM 2020-2021) confirms cluster reproducibility (silhouette score = 0.63,
AUC = 0.61) and stability. We further perform subtype analysis and visualize
temporal attention, which highlights sleep-related differences between clusters
and identifies salient time windows that align with changes in sleep
regularity, yielding clinically interpretable explanations of risk.

</details>


### [76] [On The Expressive Power of GNN Derivatives](https://arxiv.org/abs/2510.02565)
*Yam Eitan,Moshe Eliasof,Yoav Gelberg,Fabrizio Frasca,Guy Bar-Shalom,Haggai Maron*

Main category: cs.LG

TL;DR: 论文提出HOD - GNN方法，利用高阶节点导数增强GNN表达能力，理论上表达能力与WL层次对齐，开发高效算法，在图学习基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有GNN表达能力有限，且导数作为增强GNN表达能力的手段尚未被探索。

Method: 引入HOD - GNN方法，利用基础模型的高阶节点导数增强MPNN表达能力，通过第二个GNN处理生成的嵌入，开发计算高阶导数的消息传递算法。

Result: 理论上架构家族表达能力与WL层次对齐，建立与子图GNN和流行结构编码方案的联系，在图学习基准测试中表现出色。

Conclusion: 节点导数是增强GNN表达能力的自然方式，HOD - GNN方法有效且高效。

Abstract: Despite significant advances in Graph Neural Networks (GNNs), their limited
expressivity remains a fundamental challenge. Research on GNN expressivity has
produced many expressive architectures, leading to architecture hierarchies
with models of increasing expressive power. Separately, derivatives of GNNs
with respect to node features have been widely studied in the context of the
oversquashing and over-smoothing phenomena, GNN explainability, and more. To
date, these derivatives remain unexplored as a means to enhance GNN
expressivity. In this paper, we show that these derivatives provide a natural
way to enhance the expressivity of GNNs. We introduce High-Order Derivative GNN
(HOD-GNN), a novel method that enhances the expressivity of Message Passing
Neural Networks (MPNNs) by leveraging high-order node derivatives of the base
model. These derivatives generate expressive structure-aware node embeddings
processed by a second GNN in an end-to-end trainable architecture.
Theoretically, we show that the resulting architecture family's expressive
power aligns with the WL hierarchy. We also draw deep connections between
HOD-GNN, Subgraph GNNs, and popular structural encoding schemes. For
computational efficiency, we develop a message-passing algorithm for computing
high-order derivatives of MPNNs that exploits graph sparsity and parallelism.
Evaluations on popular graph learning benchmarks demonstrate HOD-GNN's strong
performance on popular graph learning tasks.

</details>


### [77] [Geospatial Machine Learning Libraries](https://arxiv.org/abs/2510.02572)
*Adam J. Stewart,Caleb Robinson,Arindam Banerjee*

Main category: cs.LG

TL;DR: 本文全面介绍地理空间机器学习（GeoML）库，分析其发展、功能和生态，介绍流行库，讨论常见方法，通过案例展示应用，指出最佳实践、挑战和未来方向，以指导相关人员。


<details>
  <summary>Details</summary>
Motivation: 地球观测数据增长快，但GeoML领域库发展滞后，需全面了解GeoML库以应对其独特挑战。

Method: 分析GeoML库的发展、核心功能和生态，介绍流行库，讨论常见方法，进行作物类型映射案例研究。

Result: 展示了GeoML库的综合信息，通过案例证明工具的实际应用。

Conclusion: 为从业者、开发者和研究人员提供在快速发展的GeoML领域导航和贡献的指导，指出未来基础模型发展和开源地理空间软件治理的需求。

Abstract: Recent advances in machine learning have been supported by the emergence of
domain-specific software libraries, enabling streamlined workflows and
increased reproducibility. For geospatial machine learning (GeoML), the
availability of Earth observation data has outpaced the development of domain
libraries to handle its unique challenges, such as varying spatial resolutions,
spectral properties, temporal cadence, data coverage, coordinate systems, and
file formats. This chapter presents a comprehensive overview of GeoML
libraries, analyzing their evolution, core functionalities, and the current
ecosystem. It also introduces popular GeoML libraries such as TorchGeo,
eo-learn, and Raster Vision, detailing their architecture, supported data
types, and integration with ML frameworks. Additionally, it discusses common
methodologies for data preprocessing, spatial--temporal joins, benchmarking,
and the use of pretrained models. Through a case study in crop type mapping, it
demonstrates practical applications of these tools. Best practices in software
design, licensing, and testing are highlighted, along with open challenges and
future directions, particularly the rise of foundation models and the need for
governance in open-source geospatial software. Our aim is to guide
practitioners, developers, and researchers in navigating and contributing to
the rapidly evolving GeoML landscape.

</details>


### [78] [Why Do We Need Warm-up? A Theoretical Perspective](https://arxiv.org/abs/2510.03164)
*Foivos Alimisis,Rustem Islamov,Aurelien Lucchi*

Main category: cs.LG

TL;DR: 本文对学习率预热提升训练效果给出原理性解释，理论和实验证明相关条件对常见神经架构成立，证明预热梯度下降收敛更快并验证其实际益处。


<details>
  <summary>Details</summary>
Motivation: 学习率预热在深度学习中广泛使用，但理论基础不明，需对其提升训练效果给出原理性解释。

Method: 依靠 $(L_0, L_1)$ - 平滑性条件的推广，从理论和实验验证该条件对常见神经架构成立。

Result: 证明带预热的梯度下降比固定步长收敛更快，建立复杂度上下界，实验验证预热的实际好处。

Conclusion: 学习率预热能提升训练效果，有理论依据和实际益处。

Abstract: Learning rate warm-up - increasing the learning rate at the beginning of
training - has become a ubiquitous heuristic in modern deep learning, yet its
theoretical foundations remain poorly understood. In this work, we provide a
principled explanation for why warm-up improves training. We rely on a
generalization of the $(L_0, L_1)$-smoothness condition, which bounds local
curvature as a linear function of the loss sub-optimality and exhibits
desirable closure properties. We demonstrate both theoretically and empirically
that this condition holds for common neural architectures trained with
mean-squared error and cross-entropy losses. Under this assumption, we prove
that Gradient Descent with a warm-up schedule achieves faster convergence than
with a fixed step-size, establishing upper and lower complexity bounds.
Finally, we validate our theoretical insights through experiments on language
and vision models, confirming the practical benefits of warm-up schedules.

</details>


### [79] [Use the Online Network If You Can: Towards Fast and Stable Reinforcement Learning](https://arxiv.org/abs/2510.02590)
*Ahmed Hendawy,Henrik Metternich,Théo Vincent,Mahdi Kallel,Jan Peters,Carlo D'Eramo*

Main category: cs.LG

TL;DR: 提出MINTO方法，结合目标网络和在线网络优势，实现更快更稳定的值函数学习，在多基准测试中提升性能。


<details>
  <summary>Details</summary>
Motivation: 目标网络学习慢，在线网络引导学习不稳定，希望结合两者优势。

Method: 引入新的更新规则，使用目标网络和在线网络的最小估计值计算目标，得到MINTO方法。

Result: MINTO实现更快更稳定的值函数学习，可无缝集成到多种算法，在多基准测试中持续提升性能。

Conclusion: MINTO具有广泛适用性和有效性。

Abstract: The use of target networks is a popular approach for estimating value
functions in deep Reinforcement Learning (RL). While effective, the target
network remains a compromise solution that preserves stability at the cost of
slowly moving targets, thus delaying learning. Conversely, using the online
network as a bootstrapped target is intuitively appealing, albeit well-known to
lead to unstable learning. In this work, we aim to obtain the best out of both
worlds by introducing a novel update rule that computes the target using the
MINimum estimate between the Target and Online network, giving rise to our
method, MINTO. Through this simple, yet effective modification, we show that
MINTO enables faster and stable value function learning, by mitigating the
potential overestimation bias of using the online network for bootstrapping.
Notably, MINTO can be seamlessly integrated into a wide range of value-based
and actor-critic algorithms with a negligible cost. We evaluate MINTO
extensively across diverse benchmarks, spanning online and offline RL, as well
as discrete and continuous action spaces. Across all benchmarks, MINTO
consistently improves performance, demonstrating its broad applicability and
effectiveness.

</details>


### [80] [Best-of-Majority: Minimax-Optimal Strategy for Pass@$k$ Inference Scaling](https://arxiv.org/abs/2510.03199)
*Qiwei Di,Kaixuan Ji,Xuheng Li,Heyang Zhao,Quanquan Gu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: LLM inference often generates a batch of candidates for a prompt and selects
one via strategies like majority voting or Best-of- N (BoN). For difficult
tasks, this single-shot selection often underperforms. Consequently,
evaluations commonly report Pass@$k$: the agent may submit up to $k$ responses,
and only the best of them is used when computing regret. Motivated by this, we
study inference scaling in the more general Pass@$k$ inference setting, and
prove that neither majority voting nor BoN exhibits the desirable scaling with
$k$ and the sampling budget $N$. Combining the advantages of majority voting
and BoN, we propose a new inference strategy called Best-of-Majority (BoM),
with a pivotal step that restricts the candidates to the responses with high
frequency in the $N$ samples before selecting the top-$k$ rewards. We prove
that when the sampling budget is $N=\tilde\Omega(C^*)$, the regret of BoM is
$O(\epsilon_{\mathrm{opt}}+\sqrt{\epsilon_{\mathrm{RM}}^2C^*/k})$, where $C^*$
is the coverage coefficient, $\epsilon_{\mathrm{RM}}$ is the estimation error
of the reward model, and $\epsilon_{\mathrm{opt}}$ is the estimation error of
reward at the optimal response. We further establish a matching lower bound,
certifying that our algorithm is minimax optimal. Beyond optimality, BoM has a
key advantage: unlike majority voting and BoN, its performance does not degrade
when increasing $N$. Experimental results of inference on math problems show
BoM outperforming both majority voting and BoN.

</details>


### [81] [Towards CONUS-Wide ML-Augmented Conceptually-Interpretable Modeling of Catchment-Scale Precipitation-Storage-Runoff Dynamics](https://arxiv.org/abs/2510.02605)
*Yuan-Heng Wang,Yang Yang,Fabio Ciulla,Hoshin V. Gupta,Charuleka Varadharajan*

Main category: cs.LG

TL;DR: 本文开展美国大陆大样本研究，用基于MCP的机器学习增强的物理可解释模型，评估结果表明应根据水文状况选合适复杂度模型，MCP模型性能与LSTM相当，强调理论驱动、物理基础方法潜力。


<details>
  <summary>Details</summary>
Motivation: 现代基于机器学习的大样本水文建模未在物理概念理解提升基础上带来预测改进，需探索新方法。

Method: 开展美国大陆大样本研究，使用基于质量守恒感知器（MCP）的不同复杂度的机器学习增强的物理可解释集水区尺度模型，用属性掩码评估结果。

Result: 表明应根据水文状况选择合适复杂度的模型架构，基于MCP的物理可解释质量守恒模型性能与基于LSTM的数据驱动模型相当。

Conclusion: 强调理论驱动、物理基础的方法在大样本水文学中的潜力，为未来模型奠定基础。

Abstract: While many modern studies are dedicated to ML-based large-sample hydrologic
modeling, these efforts have not necessarily translated into predictive
improvements that are grounded in enhanced physical-conceptual understanding.
Here, we report on a CONUS-wide large-sample study (spanning diverse
hydro-geo-climatic conditions) using ML-augmented physically-interpretable
catchment-scale models of varying complexity based in the Mass-Conserving
Perceptron (MCP). Results were evaluated using attribute masks such as snow
regime, forest cover, and climate zone. Our results indicate the importance of
selecting model architectures of appropriate model complexity based on how
process dominance varies with hydrological regime. Benchmark comparisons show
that physically-interpretable mass-conserving MCP-based models can achieve
performance comparable to data-based models based in the Long Short-Term Memory
network (LSTM) architecture. Overall, this study highlights the potential of a
theory-informed, physically grounded approach to large-sample hydrology, with
emphasis on mechanistic understanding and the development of parsimonious and
interpretable model architectures, thereby laying the foundation for future
models of everywhere that architecturally encode information about spatially-
and temporally-varying process dominance.

</details>


### [82] [MINERVA: Mutual Information Neural Estimation for Supervised Feature Selection](https://arxiv.org/abs/2510.02610)
*Taurai Muvunzaa,Egor Kraev,Pere Planell-Morell,Alexander Y. Shestopaloff*

Main category: cs.LG

TL;DR: 提出基于特征与目标互信息神经估计的监督特征选择方法MINERVA，通过两阶段过程实现，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有特征过滤器依赖统计成对依赖指标建模特征 - 目标关系，在目标依赖高阶特征交互时可能失效。

Method: 用神经网络参数化互信息近似，设计带稀疏正则化器的损失函数进行特征选择，采用两阶段过程解耦表示学习和特征选择。

Result: 通过评估特征子集集成，有效捕捉复杂特征 - 目标关系，在合成和真实欺诈数据集实验中证明方法有效且能得到精确解。

Conclusion: 所提出的MINERVA方法能有效处理复杂特征 - 目标关系，具备良好性能。

Abstract: Existing feature filters rely on statistical pair-wise dependence metrics to
model feature-target relationships, but this approach may fail when the target
depends on higher-order feature interactions rather than individual
contributions. We introduce Mutual Information Neural Estimation Regularized
Vetting Algorithm (MINERVA), a novel approach to supervised feature selection
based on neural estimation of mutual information between features and targets.
We paramaterize the approximation of mutual information with neural networks
and perform feature selection using a carefully designed loss function
augmented with sparsity-inducing regularizers. Our method is implemented in a
two-stage process to decouple representation learning from feature selection,
ensuring better generalization and a more accurate expression of feature
importance. We present examples of ubiquitous dependency structures that are
rarely captured in literature and show that our proposed method effectively
captures these complex feature-target relationships by evaluating feature
subsets as an ensemble. Experimental results on synthetic and real-life fraud
datasets demonstrate the efficacy of our method and its ability to perform
exact solutions.

</details>


### [83] [TabImpute: Accurate and Fast Zero-Shot Missing-Data Imputation with a Pre-Trained Transformer](https://arxiv.org/abs/2510.02625)
*Jacob Feitelberg,Dwaipayan Saha,Kyuseong Choi,Zaid Ahmad,Anish Agarwal,Raaz Dwivedi*

Main category: cs.LG

TL;DR: 提出TabImpute预训练transformer用于零样本数据插补，介绍训练和评估方法，在MissBench基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有表格数据缺失值插补方法在不同领域性能差异大且超参数调优耗时，缺乏默认方法。

Method: 基于TabPFN提出TabImpute；引入表格逐元素特征化、合成训练数据生成管道和MissBench基准。

Result: 逐元素特征化使插补速度比之前TabPFN方法快100倍；合成训练数据生成管道提升测试性能；TabImpute在MissBench上比11种既定插补方法表现更稳健。

Conclusion: TabImpute能实现准确快速的零样本插补，无需推理时拟合或调优超参数。

Abstract: Missing data is a pervasive problem in tabular settings. Existing solutions
range from simple averaging to complex generative adversarial networks.
However, due to huge variance in performance across real-world domains and
time-consuming hyperparameter tuning, no default imputation method exists.
Building on TabPFN, a recent tabular foundation model for supervised learning,
we propose TabImpute, a pre-trained transformer that delivers accurate and fast
zero-shot imputations requiring no fitting or hyperparameter tuning at
inference-time. To train and evaluate TabImpute, we introduce (i) an entry-wise
featurization for tabular settings, which enables a $100\times$ speedup over
the previous TabPFN imputation method, (ii) a synthetic training data
generation pipeline incorporating realistic missingness patterns, which boosts
test-time performance, and (iii) MissBench, a comprehensive benchmark for
evaluation of imputation methods with $42$ OpenML datasets and $13$ missingness
patterns. MissBench spans domains such as medicine, finance, and engineering,
showcasing TabImpute's robust performance compared to $11$ established
imputation methods.

</details>


### [84] [HyperAdaLoRA: Accelerating LoRA Rank Allocation During Training via Hypernetworks without Sacrificing Performance](https://arxiv.org/abs/2510.02630)
*Hao Zhang,Zhenjia Li,Runfeng Bao,Yifan Gao,Xi Xiao,Bo Huang,Yuhang Wu,Tianyang Wang,Hao Xu*

Main category: cs.LG

TL;DR: 提出HyperAdaLoRA框架加速AdaLoRA收敛，实验证明其能快速收敛且性能不下降，有广泛适用性。


<details>
  <summary>Details</summary>
Motivation: AdaLoRA训练时存在收敛速度慢和计算开销高的问题，需要改进。

Method: 使用基于注意力机制的超网络动态生成奇异值分解的参数，对超网络输出进行剪枝实现动态秩分配。

Result: 在不同数据集和模型上实验表明方法能快速收敛且不牺牲性能，扩展实验验证了广泛适用性。

Conclusion: HyperAdaLoRA能有效加速AdaLoRA收敛，具有良好性能和广泛适用性。

Abstract: Parameter-Efficient Fine-Tuning (PEFT), especially Low-Rank Adaptation
(LoRA), has emerged as a promising approach to fine-tuning large language
models(LLMs) while reducing computational and memory overhead. However, LoRA
assumes a uniform rank \textit{r} for each incremental matrix, not accounting
for the varying significance of weight matrices across different modules and
layers. AdaLoRA leverages Singular Value Decomposition (SVD) to parameterize
updates and employs pruning of singular values to introduce dynamic rank
allocation, thereby enhancing adaptability. However, during the training
process, it often encounters issues of slow convergence speed and high
computational overhead. To address this issue, we propose HyperAdaLoRA, a novel
framework that accelerates the convergence of AdaLoRA by leveraging a
hypernetwork. Instead of directly optimizing the components of Singular Value
Decomposition $(P, \Lambda, Q)$, HyperAdaLoRA employs a hypernetwork based on
attention mechanisms to dynamically generate these parameters. By pruning the
outputs of the hypernetwork that generates the singular values, dynamic rank
allocation is achieved. Comprehensive experiments on various datasets and
models demonstrate that our method achieves faster convergence without
sacrificing performance. Additionally, further extension experiments on other
LoRA-based approaches validate the broad applicability of our method.

</details>


### [85] [Dissecting Transformers: A CLEAR Perspective towards Green AI](https://arxiv.org/abs/2510.02810)
*Hemang Jain,Shailender Goyal,Divyansh Pandey,Karthik Vaidhyanathan*

Main category: cs.LG

TL;DR: 本文对大语言模型推理能耗进行细粒度分析，提出CLEAR方法评估15个模型，发现注意力块每浮点运算能耗高，为组件级优化提供基线。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理能耗成AI能耗主要部分，但现有可持续性研究缺乏细粒度测量方法，未将能效作为主要目标。

Method: 提出Component - Level Energy Assessment via Repeated sampling (CLEAR)方法，克服组件执行与能量传感器监测的时间不匹配问题。

Result: 评估15个模型，组件能量方差低于9.5%，捕获超90%的总能量；发现注意力块每浮点运算能耗高，FLOP不能反映组件真实能耗。

Conclusion: 建立组件级能耗基线，为组件级优化构建节能变压器模型提供初步见解。

Abstract: The rapid adoption of Large Language Models (LLMs) has raised significant
environmental concerns. Unlike the one-time cost of training, LLM inference
occurs continuously at a global scale and now dominates the AI energy
footprint. Yet, most sustainability studies report only coarse, model-level
metrics due to the lack of fine-grained measurement methods, treating energy
efficiency more as an afterthought than as a primary objective. We present the
first fine-grained empirical analysis of inference energy across core
components of transformer architecture. We propose a novel methodology,
Component-Level Energy Assessment via Repeated sampling (CLEAR), to overcome
temporal mismatch between microsecond scale component execution and monitoring
of millisecond (ms) scale energy sensors. Using CLEAR, we evaluate 15 models
spanning four distinct architecture types and consistently keep component-wise
energy variance below 9.5\% while capturing more than 90\% of the model's total
energy as individual components. Our empirical analysis reveals that Attention
blocks consume significantly more energy per floating-point operation (FLOP),
indicating that energy consumption is not proportionally aligned with FLOP
counts. This shows that FLOPs alone fail to capture the true energy cost at a
component level. Our findings establish detailed component-level energy
baselines and provide insight as an initial step to build energy-efficient
transformer models through component-level optimizations.

</details>


### [86] [Optimal Characteristics of Inspection Vehicle for Drive-by Bridge Inspection](https://arxiv.org/abs/2510.02658)
*A. Calderon Hurtado,E. Atroshchenko,K. C. Chang,C. W. Kim,M. Makki Alamdari*

Main category: cs.LG

TL;DR: 本文提出优化检查车辆的框架以提高桥梁损伤检测灵敏度，用AAE方法和Kriging元模型优化，得出不同频率比和车辆重量下的检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于车载检测的桥梁健康监测方法中，车辆机械和动力学特性影响检测性能，需优化检查车辆。

Method: 使用基于对抗自动编码器（AAE）的无监督深度学习方法重构加速度响应频域表示，通过最小化健康和受损桥梁状态下损伤指数分布的Wasserstein距离优化两轴车辆轮胎悬架系统的质量和刚度，采用Kriging元模型近似目标函数。

Result: 频率比在0.3 - 0.7之间的车辆检测最有效，接近共振的车辆表现不佳，较轻车辆需要较低固有频率实现最佳检测。

Conclusion: 首次严格优化了车载传感平台并提出了专用检查车辆。

Abstract: Drive-by inspection for bridge health monitoring has gained increasing
attention over the past decade. This method involves analysing the coupled
vehicle-bridge response, recorded by an instrumented inspection vehicle, to
assess structural integrity and detect damage. However, the vehicles mechanical
and dynamic properties significantly influence detection performance, limiting
the effectiveness of the approach. This study presents a framework for
optimising the inspection vehicle to enhance damage sensitivity. An
unsupervised deep learning methodbased on adversarial autoencoders (AAE)is used
to reconstruct the frequency-domain representation of acceleration responses.
The mass and stiffness of the tyre suspension system of a two-axle vehicle are
optimised by minimising the Wasserstein distance between damage index
distributions for healthy and damaged bridge states. A Kriging meta-model is
employed to approximate this objective function efficiently and identify
optimal vehicle configurations in both dimensional and non-dimensional
parameter spaces. Results show that vehicles with frequency ratios between 0.3
and 0.7 relative to the bridges' first natural frequency are most effective,
while those near resonance perform poorly. Lighter vehicles require lower
natural frequencies for optimal detection. This is the first study to
rigorously optimise the sensing platform for drive-by sensing and to propose a
purpose-built inspection vehicle.

</details>


### [87] [TutorBench: A Benchmark To Assess Tutoring Capabilities Of Large Language Models](https://arxiv.org/abs/2510.02663)
*Rakshith S Srinivasa,Zora Che,Chen Bo Calvin Zhang,Diego Mares,Ernesto Hernandez,Jayeon Park,Dean Lee,Guillermo Mangialardi,Charmaine Ng,Ed-Yeremai Hernandez Cardona,Anisha Gunjal,Yunzhong He,Bing Liu,Chen Xing*

Main category: cs.LG

TL;DR: 本文介绍了用于评估大语言模型辅导技能的TutorBench数据集和评估基准，评估16个前沿模型，结果显示模型有很大提升空间，不同模型家族优缺点各异。


<details>
  <summary>Details</summary>
Motivation: 学生将大语言模型用作学习辅助工具，需要构建善于处理辅导细微差别的模型，因此要严格评估大语言模型的核心辅导技能。

Method: 创建包含1490个样本的TutorBench数据集，涵盖三种常见辅导任务，样本配有特定评分标准，使用基于大语言模型评判器和评分标准的自动评估方法评估16个前沿大语言模型。

Result: 前沿大语言模型得分均未超56%，在各项辅导技能上通过率均低于60%，不同模型家族优缺点不同，Claude模型在支持主动学习方面表现突出。

Conclusion: TutorBench为下一代人工智能辅导工具的开发提供了全面且不饱和的基准。

Abstract: As students increasingly adopt large language models (LLMs) as learning aids,
it is crucial to build models that are adept at handling the nuances of
tutoring: they need to identify the core needs of students, be adaptive,
provide personalized guidance, and be accurate. To this end, we introduce
TutorBench, a dataset and evaluation benchmark designed to rigorously evaluate
the core tutoring skills of LLMs. The dataset comprises 1,490 samples curated
by human experts, focused on high-school and AP-level curricula. The samples
are drawn from three common tutoring tasks: (i) generating adaptive
explanations tailored to a student's confusion, (ii) providing actionable
feedback on a student's work, and (iii) promoting active learning through
effective hint generation. To account for the inherent complexity of tutoring,
samples are accompanied by sample-specific rubrics which are used to judge
model responses during evaluation. TutorBench uses a reliable and fine-grained
automatic evaluation method that uses an LLM-judge and the sample-specific
rubrics. We evaluate 16 frontier LLMs on TutorBench and present a detailed
analysis of their performance and behavior. Our results show that none of the
frontier LLMs achieve a score of greater than $56\%$, showing a large room for
improvement. We find that LLMs fall short in exhibiting the full range of
tutoring skills needed to guide, diagnose, and support students effectively,
with all the frontier models achieving less than a $60\%$ pass rate on rubric
criteria related to these skills. We also find that different model families
exhibit varied strengths and limitations: the Claude models outperform others
in supporting active learning, while they lag behind in the other two use
cases. By releasing TutorBench, we provide a comprehensive and unsaturated
benchmark to guide the development of the next-generation of AI tutors.

</details>


### [88] [Topological Invariance and Breakdown in Learning](https://arxiv.org/abs/2510.02670)
*Yongyi Yang,Tomaso Poggio,Isaac Chuang,Liu Ziyin*

Main category: cs.LG

TL;DR: 证明一类排列等变学习规则训练过程对神经元拓扑结构的影响，划分学习动态阶段，理论具通用性。


<details>
  <summary>Details</summary>
Motivation: 研究神经元网络在梯度下降下的学习动态，揭示学习率对神经元拓扑结构的影响。

Method: 证明一类排列等变学习规则（包括SGD、Adam等）训练过程诱导神经元间的双Lipschitz映射。

Result: 学习率低于拓扑临界点时保留神经元拓扑结构，高于时允许拓扑简化，学习动态分两阶段。

Conclusion: 理论独立于特定架构和损失函数，可将拓扑方法普遍应用于深度学习研究。

Abstract: We prove that for a broad class of permutation-equivariant learning rules
(including SGD, Adam, and others), the training process induces a bi-Lipschitz
mapping between neurons and strongly constrains the topology of the neuron
distribution during training. This result reveals a qualitative difference
between small and large learning rates $\eta$. With a learning rate below a
topological critical point $\eta^*$, the training is constrained to preserve
all topological structure of the neurons. In contrast, above $\eta^*$, the
learning process allows for topological simplification, making the neuron
manifold progressively coarser and thereby reducing the model's expressivity.
Viewed in combination with the recent discovery of the edge of stability
phenomenon, the learning dynamics of neuron networks under gradient descent can
be divided into two phases: first they undergo smooth optimization under
topological constraints, and then enter a second phase where they learn through
drastic topological simplifications. A key feature of our theory is that it is
independent of specific architectures or loss functions, enabling the universal
application of topological methods to the study of deep learning.

</details>


### [89] [To Compress or Not? Pushing the Frontier of Lossless GenAI Model Weights Compression with Exponent Concentration](https://arxiv.org/abs/2510.02676)
*Zeyu Yang,Tianyi Zhang,Jianwen Xie,Chuan Li,Zhaozhuo Xu,Anshumali Shrivastava*

Main category: cs.LG

TL;DR: 研究生成式AI模型权重指数集中现象，提出ECF8格式，实验显示有内存节省和吞吐量加速效果。


<details>
  <summary>Details</summary>
Motivation: 生成式AI模型参数规模大，需开发低精度浮点格式以高效部署。

Method: 对生成式AI权重指数集中现象进行理论和实证研究，基于分析提出ECF8格式。

Result: 在高达671B参数的大模型实验中，实现26.9%内存节省和177.1%吞吐量加速，且计算无损。

Conclusion: 指数集中是训练模型的统计规律，为FP8时代无损低精度浮点设计提供原则性路径。

Abstract: The scaling of Generative AI (GenAI) models into the hundreds of billions of
parameters makes low-precision computation indispensable for efficient
deployment. We argue that the fundamental solution lies in developing
low-precision floating-point formats, which inherently provide numerical
stability, memory savings, and hardware efficiency without dequantization
overhead. In this paper, we present a theoretical and empirical study of an
exponent concentration phenomenon in GenAI weights: exponents consistently
exhibit low entropy across architectures and modalities. We show that this
arises naturally from $\alpha$-stable distributions induced by stochastic
gradient descent, and we prove tight bounds on the entropy of exponents. Our
analysis establishes a theoretical compression limit near FP4.67, which
motivates the design of a practical FP8 format. Building on these insights, we
propose Exponent-Concentrated FP8 (ECF8), a lossless compression framework with
entropy-aware encoding and GPU-optimized decoding. Experiments on LLMs and DiTs
up to 671B parameters demonstrate up to 26.9% memory savings and 177.1%
throughput acceleration, with perfectly lossless computations, i.e., no
deviation in model outputs. Our results establish exponent concentration as a
statistical law of trained models and open a principled path for lossless
low-precision floating-point design in the FP8 era.

</details>


### [90] [Can Data-Driven Dynamics Reveal Hidden Physics? There Is A Need for Interpretable Neural Operators](https://arxiv.org/abs/2510.02683)
*Wenhan Gao,Jian Luo,Fang Wan,Ruichen Xu,Xiang Liu,Haipeng Xing,Yi Liu*

Main category: cs.LG

TL;DR: 本文对神经算子分类并提出多个观点，探讨其学习物理驱动动力学，指出解释方法局限，介绍模型性能，强调融入物理原理框架的需求。


<details>
  <summary>Details</summary>
Motivation: 深入理解神经算子学习机制，学习符合物理原理的数据驱动动力学。

Method: 将神经算子分为空间域和函数域模型两类，基于此分类提出观点，给出解释神经算子预测过程的方法。

Result: 解释方法有局限性；简单的双空间多尺度模型达SOTA性能。

Conclusion: 需通用解释方法；双空间多尺度模型有学习复杂物理的潜力；需有原则的框架将已知物理融入神经算子。

Abstract: Recently, neural operators have emerged as powerful tools for learning
mappings between function spaces, enabling data-driven simulations of complex
dynamics. Despite their successes, a deeper understanding of their learning
mechanisms remains underexplored. In this work, we classify neural operators
into two types: (1) Spatial domain models that learn on grids and (2)
Functional domain models that learn with function bases. We present several
viewpoints based on this classification and focus on learning data-driven
dynamics adhering to physical principles. Specifically, we provide a way to
explain the prediction-making process of neural operators and show that neural
operator can learn hidden physical patterns from data. However, this
explanation method is limited to specific situations, highlighting the urgent
need for generalizable explanation methods. Next, we show that a simple
dual-space multi-scale model can achieve SOTA performance and we believe that
dual-space multi-spatio-scale models hold significant potential to learn
complex physics and require further investigation. Lastly, we discuss the
critical need for principled frameworks to incorporate known physics into
neural operators, enabling better generalization and uncovering more hidden
physical phenomena.

</details>


### [91] [Multiplicative-Additive Constrained Models:Toward Joint Visualization of Interactive and Independent Effects](https://arxiv.org/abs/2509.21923)
*Fumin Wang*

Main category: cs.LG

TL;DR: 文章引入Multiplicative - Additive Constrained Models (MACMs)，结合GAMs和CESR优点，实验显示基于神经网络的MACMs预测性能优于CESR和现有GAMs。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域应用机器学习时需要考虑可解释性，GAMs虽有可解释性但预测性能受限，CESR性能未超GAMs，因此需要新模型。

Method: 引入MACMs，在CESR基础上增加加法部分，解耦交互项和独立项的系数，扩大假设空间。

Result: 实验表明基于神经网络的MACMs在预测性能上显著优于CESR和当前最先进的GAMs。

Conclusion: MACMs在CESR和GAMs基础上有所改进，是更优模型。

Abstract: Interpretability is one of the considerations when applying machine learning
to high-stakes fields such as healthcare that involve matters of life safety.
Generalized Additive Models (GAMs) enhance interpretability by visualizing
shape functions. Nevertheless, to preserve interpretability, GAMs omit
higher-order interaction effects (beyond pairwise interactions), which imposes
significant constraints on their predictive performance. We observe that Curve
Ergodic Set Regression (CESR), a multiplicative model, naturally enables the
visualization of its shape functions and simultaneously incorporates both
interactions among all features and individual feature effects. Nevertheless,
CESR fails to demonstrate superior performance compared to GAMs. We introduce
Multiplicative-Additive Constrained Models (MACMs), which augment CESR with an
additive part to disentangle the intertwined coefficients of its interactive
and independent terms, thus effectively broadening the hypothesis space. The
model is composed of a multiplicative part and an additive part, whose shape
functions can both be naturally visualized, thereby assisting users in
interpreting how features participate in the decision-making process.
Consequently, MACMs constitute an improvement over both CESR and GAMs. The
experimental results indicate that neural network-based MACMs significantly
outperform both CESR and the current state-of-the-art GAMs in terms of
predictive performance.

</details>


### [92] [EvoSpeak: Large Language Models for Interpretable Genetic Programming-Evolved Heuristics](https://arxiv.org/abs/2510.02686)
*Meng Xu,Jiao Liu,Yew Soon Ong*

Main category: cs.LG

TL;DR: 提出EvoSpeak框架结合遗传编程（GP）和大语言模型（LLMs），在动态柔性作业车间调度实验中验证可产生更有效启发式、提高进化效率和报告可读性。


<details>
  <summary>Details</summary>
Motivation: 传统GP在动态和大规模场景下，有效启发式复杂，存在可解释性差、收敛慢和跨任务迁移性受限问题。

Method: 提出EvoSpeak框架，从高质量GP启发式中学习和提取知识，用于生成热启动种群、将不透明GP树转化为自然语言解释、实现跨任务知识迁移和偏好感知启发式生成。

Result: 在单目标和多目标动态柔性作业车间调度实验中，EvoSpeak产生更有效启发式，提高进化效率，提供可读报告。

Conclusion: EvoSpeak结合GP和LLMs优势，推动了现实优化问题智能、透明和用户友好启发式的发展。

Abstract: Genetic programming (GP) has demonstrated strong effectiveness in evolving
tree-structured heuristics for complex optimization problems. Yet, in dynamic
and large-scale scenarios, the most effective heuristics are often highly
complex, hindering interpretability, slowing convergence, and limiting
transferability across tasks. To address these challenges, we present EvoSpeak,
a novel framework that integrates GP with large language models (LLMs) to
enhance the efficiency, transparency, and adaptability of heuristic evolution.
EvoSpeak learns from high-quality GP heuristics, extracts knowledge, and
leverages this knowledge to (i) generate warm-start populations that accelerate
convergence, (ii) translate opaque GP trees into concise natural-language
explanations that foster interpretability and trust, and (iii) enable knowledge
transfer and preference-aware heuristic generation across related tasks. We
verify the effectiveness of EvoSpeak through extensive experiments on dynamic
flexible job shop scheduling (DFJSS), under both single- and multi-objective
formulations. The results demonstrate that EvoSpeak produces more effective
heuristics, improves evolutionary efficiency, and delivers human-readable
reports that enhance usability. By coupling the symbolic reasoning power of GP
with the interpretative and generative strengths of LLMs, EvoSpeak advances the
development of intelligent, transparent, and user-aligned heuristics for
real-world optimization problems.

</details>


### [93] [Fine-Tuning Diffusion Models via Intermediate Distribution Shaping](https://arxiv.org/abs/2510.02692)
*Gautham Govind Anil,Shaan Ul Haque,Nithish Kannen,Dheeraj Nagaraj,Sanjay Shakkottai,Karthikeyan Shanmugam*

Main category: cs.LG

TL;DR: 本文统一RAFT变体为GRAFT，引入P - GRAFT并提出逆噪声校正方法，在多任务生成中取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 预训练扩散模型虽能捕捉训练数据分布，但需用奖励函数调整以适应下游应用，且现有策略梯度方法对扩散模型不可行。

Method: 统一RAFT变体为GRAFT，引入P - GRAFT调整中间噪声水平分布，提出逆噪声校正改进流模型。

Result: 在文本到图像、布局、分子和无条件图像生成任务中进行评估，应用于Stable Diffusion 2在T2I基准上优于策略梯度方法，逆噪声校正降低FLOPs/image下提高FID。

Conclusion: 所提方法在扩散模型微调中有效，能更好地塑造数据分布以适应下游应用。

Abstract: Diffusion models are widely used for generative tasks across domains. While
pre-trained diffusion models effectively capture the training data
distribution, it is often desirable to shape these distributions using reward
functions to align with downstream applications. Policy gradient methods, such
as Proximal Policy Optimization (PPO), are widely used in the context of
autoregressive generation. However, the marginal likelihoods required for such
methods are intractable for diffusion models, leading to alternative proposals
and relaxations. In this context, we unify variants of Rejection sAmpling based
Fine-Tuning (RAFT) as GRAFT, and show that this implicitly performs PPO with
reshaped rewards. We then introduce P-GRAFT to shape distributions at
intermediate noise levels and demonstrate empirically that this can lead to
more effective fine-tuning. We mathematically explain this via a bias-variance
tradeoff. Motivated by this, we propose inverse noise correction to improve
flow models without leveraging explicit rewards. We empirically evaluate our
methods on text-to-image(T2I) generation, layout generation, molecule
generation and unconditional image generation. Notably, our framework, applied
to Stable Diffusion 2, improves over policy gradient methods on popular T2I
benchmarks in terms of VQAScore and shows an $8.81\%$ relative improvement over
the base model. For unconditional image generation, inverse noise correction
improves FID of generated images at lower FLOPs/image.

</details>


### [94] [RAMAC: Multimodal Risk-Aware Offline Reinforcement Learning and the Role of Behavior Regularization](https://arxiv.org/abs/2510.02695)
*Kai Fukazawa,Kunal Mundada,Iman Soltani*

Main category: cs.LG

TL;DR: 提出RAMAC框架解决离线强化学习中风险敏感学习问题，在多个任务上取得较好效果。


<details>
  <summary>Details</summary>
Motivation: 现有风险规避离线强化学习存在价值保守和策略类受限问题，而表达性策略仅用于风险中性场景，需解决该差距。

Method: 引入Risk - Aware Multimodal Actor - Critic (RAMAC)框架，将表达性生成actor与分布性critic结合，通过生成路径区分组合目标。

Result: 使用扩散和流匹配actor实例化RAMAC，在大多数随机D4RL任务上保持高回报同时，CVaR₀.₁有持续提升。

Conclusion: RAMAC框架能在复杂多模态场景实现风险敏感学习。

Abstract: In safety-critical domains where online data collection is infeasible,
offline reinforcement learning (RL) offers an attractive alternative but only
if policies deliver high returns without incurring catastrophic lower-tail
risk. Prior work on risk-averse offline RL achieves safety at the cost of value
conservatism and restricted policy classes, whereas expressive policies are
only used in risk-neutral settings. Here, we address this gap by introducing
the \textbf{Risk-Aware Multimodal Actor-Critic (RAMAC)} framework, which
couples an \emph{expressive generative actor} with a distributional critic. The
RAMAC differentiates composite objective combining distributional risk and BC
loss through the generative path, achieving risk-sensitive learning in complex
multimodal scenarios. We instantiate RAMAC with diffusion and flow-matching
actors and observe consistent gains in $\mathrm{CVaR}_{0.1}$ while maintaining
strong returns on most Stochastic-D4RL tasks. Code:
https://github.com/KaiFukazawa/RAMAC.git

</details>


### [95] [A Novel Unified Lightweight Temporal-Spatial Transformer Approach for Intrusion Detection in Drone Networks](https://arxiv.org/abs/2510.02711)
*Tarun Kumar Biswas,Ashrafun Zannat,Waqas Ishtiaq,Md. Alamgir Hossain*

Main category: cs.LG

TL;DR: 本文提出针对无人机网络的新型入侵检测系统TSLT - Net，实验显示其性能优越，适合实时无人机网络安全。


<details>
  <summary>Details</summary>
Motivation: 无人机网络易受攻击，现有入侵检测机制在无人机运行环境中缺乏适应性、效率和通用性。

Method: 提出TSLT - Net，利用自注意力机制建模网络流量的时空模式，有简化预处理流程，支持多类攻击分类和二元异常检测。

Result: 在ISOT无人机异常检测数据集上，多类检测准确率达99.99%，二元异常检测达100%，内存占用仅0.04 MB，可训练参数9722个。

Conclusion: TSLT - Net是实时无人机网络安全的有效可扩展解决方案，适合关键任务无人机系统的边缘设备部署。

Abstract: The growing integration of drones across commercial, industrial, and civilian
domains has introduced significant cybersecurity challenges, particularly due
to the susceptibility of drone networks to a wide range of cyberattacks.
Existing intrusion detection mechanisms often lack the adaptability,
efficiency, and generalizability required for the dynamic and resource
constrained environments in which drones operate. This paper proposes TSLT-Net,
a novel lightweight and unified Temporal Spatial Transformer based intrusion
detection system tailored specifically for drone networks. By leveraging self
attention mechanisms, TSLT-Net effectively models both temporal patterns and
spatial dependencies in network traffic, enabling accurate detection of diverse
intrusion types. The framework includes a streamlined preprocessing pipeline
and supports both multiclass attack classification and binary anomaly detection
within a single architecture. Extensive experiments conducted on the ISOT Drone
Anomaly Detection Dataset, consisting of more than 2.3 million labeled records,
demonstrate the superior performance of TSLT-Net with 99.99 percent accuracy in
multiclass detection and 100 percent in binary anomaly detection, while
maintaining a minimal memory footprint of only 0.04 MB and 9722 trainable
parameters. These results establish TSLT-Net as an effective and scalable
solution for real time drone cybersecurity, particularly suitable for
deployment on edge devices in mission critical UAV systems.

</details>


### [96] [CST-AFNet: A dual attention-based deep learning framework for intrusion detection in IoT networks](https://arxiv.org/abs/2510.02717)
*Waqas Ishtiaq,Ashrafun Zannat,A. H. M. Shahariar Parvez,Md. Alamgir Hossain,Muntasir Hasan Kanchan,Muhammad Masud Tarek*

Main category: cs.LG

TL;DR: 本文提出CST AFNet用于物联网网络入侵检测，在Edge IIoTset数据集上表现出色，准确率达99.97%，优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 物联网发展带来复杂网络安全挑战，需有效入侵检测方法。

Method: 提出CST AFNet，集成多尺度CNN、BiGRUs和双注意力机制，在Edge IIoTset数据集上训练评估。

Result: CST AFNet准确率达99.97%，宏平均精度、召回率和F1分数超99.3%，优于传统模型。

Conclusion: CST AFNet是复杂物联网和工业物联网环境中实时网络威胁检测的强大可扩展解决方案。

Abstract: The rapid expansion of the Internet of Things (IoT) has revolutionized modern
industries by enabling smart automation and real time connectivity. However,
this evolution has also introduced complex cybersecurity challenges due to the
heterogeneous, resource constrained, and distributed nature of these
environments. To address these challenges, this research presents CST AFNet, a
novel dual attention based deep learning framework specifically designed for
robust intrusion detection in IoT networks. The model integrates multi scale
Convolutional Neural Networks (CNNs) for spatial feature extraction,
Bidirectional Gated Recurrent Units (BiGRUs) for capturing temporal
dependencies, and a dual attention mechanism, channel and temporal attention,
to enhance focus on critical patterns in the data. The proposed method was
trained and evaluated on the Edge IIoTset dataset, a comprehensive and
realistic benchmark containing more than 2.2 million labeled instances spanning
15 attack types and benign traffic, collected from a seven layer industrial
testbed. Our proposed model achieves outstanding accuracy for both 15 attack
types and benign traffic. CST AFNet achieves 99.97 percent accuracy. Moreover,
this model demonstrates exceptional performance with macro averaged precision,
recall, and F1 score all above 99.3 percent. Experimental results show that CST
AFNet achieves superior detection accuracy, significantly outperforming
traditional deep learning models. The findings confirm that CST AFNet is a
powerful and scalable solution for real time cyber threat detection in complex
IoT and IIoT environments, paving the way for more secure, intelligent, and
adaptive cyber physical systems.

</details>


### [97] [Accuracy Law for the Future of Deep Time Series Forecasting](https://arxiv.org/abs/2510.02729)
*Yuxuan Wang,Haixu Wu,Yuezhou Ma,Yuchen Fang,Ziyi Zhang,Yong Liu,Shiyu Wang,Zhou Ye,Yang Xiang,Jianmin Wang,Mingsheng Long*

Main category: cs.LG

TL;DR: 本文聚焦深度时间序列预测性能上限估计问题，发现最小预测误差与窗口序列模式复杂度的指数关系（准确率定律），用于识别饱和任务和制定训练策略。


<details>
  <summary>Details</summary>
Motivation: 近年来深度时间序列预测发展迅速，但研究人员因标准基准改进小而迷茫，且时间序列预测有非零误差下界，需要明确研究目标。

Method: 超越经典序列可预测性指标，基于对超2800个新训练的深度预测器进行严格统计测试。

Result: 发现深度模型最小预测误差与窗口序列模式复杂度存在显著指数关系，即准确率定律。

Conclusion: 准确率定律可识别广泛使用基准中的饱和任务，为大型时间序列模型得出有效训练策略，为未来研究提供有价值见解。

Abstract: Deep time series forecasting has emerged as a booming direction in recent
years. Despite the exponential growth of community interests, researchers are
sometimes confused about the direction of their efforts due to minor
improvements on standard benchmarks. In this paper, we notice that, unlike
image recognition, whose well-acknowledged and realizable goal is 100%
accuracy, time series forecasting inherently faces a non-zero error lower bound
due to its partially observable and uncertain nature. To pinpoint the research
objective and release researchers from saturated tasks, this paper focuses on a
fundamental question: how to estimate the performance upper bound of deep time
series forecasting? Going beyond classical series-wise predictability metrics,
e.g., ADF test, we realize that the forecasting performance is highly related
to window-wise properties because of the sequence-to-sequence forecasting
paradigm of deep time series models. Based on rigorous statistical tests of
over 2,800 newly trained deep forecasters, we discover a significant
exponential relationship between the minimum forecasting error of deep models
and the complexity of window-wise series patterns, which is termed the accuracy
law. The proposed accuracy law successfully guides us to identify saturated
tasks from widely used benchmarks and derives an effective training strategy
for large time series models, offering valuable insights for future research.

</details>


### [98] [Dale meets Langevin: A Multiplicative Denoising Diffusion Model](https://arxiv.org/abs/2510.02730)
*Nishanth Shetty,Madhava Prasath,Chandra Sekhar Seelamantula*

Main category: cs.LG

TL;DR: 本文基于Dale法则提出指数梯度下降优化方案，连接几何布朗运动与该方案，提出乘法去噪得分匹配新形式，实验验证新方案生成能力，是首个基于几何布朗运动的生物启发生成模型。


<details>
  <summary>Details</summary>
Motivation: 标准梯度下降优化学习与生物系统学习不一致，需构建受生物启发的学习技术。

Method: 从几何布朗运动的随机微分方程出发，离散化反向时间随机微分方程得到乘法更新规则；提出乘法去噪得分匹配新形式。

Result: 新方案在MNIST、Fashion MNIST和Kuzushiji数据集实验中展现出生成能力。

Conclusion: 这是首个采用乘法更新、基于几何布朗运动的生物启发生成模型。

Abstract: Gradient descent has proven to be a powerful and effective technique for
optimization in numerous machine learning applications. Recent advances in
computational neuroscience have shown that learning in standard gradient
descent optimization formulation is not consistent with learning in biological
systems. This has opened up interesting avenues for building biologically
inspired learning techniques. One such approach is inspired by Dale's law,
which states that inhibitory and excitatory synapses do not swap roles during
the course of learning. The resulting exponential gradient descent optimization
scheme leads to log-normally distributed synaptic weights. Interestingly, the
density that satisfies the Fokker-Planck equation corresponding to the
stochastic differential equation (SDE) with geometric Brownian motion (GBM) is
the log-normal density. Leveraging this connection, we start with the SDE
governing geometric Brownian motion, and show that discretizing the
corresponding reverse-time SDE yields a multiplicative update rule, which
surprisingly, coincides with the sampling equivalent of the exponential
gradient descent update founded on Dale's law. Furthermore, we propose a new
formalism for multiplicative denoising score-matching, subsuming the loss
function proposed by Hyvaerinen for non-negative data. Indeed, log-normally
distributed data is positive and the proposed score-matching formalism turns
out to be a natural fit. This allows for training of score-based models for
image data and results in a novel multiplicative update scheme for sample
generation starting from a log-normal density. Experimental results on MNIST,
Fashion MNIST, and Kuzushiji datasets demonstrate generative capability of the
new scheme. To the best of our knowledge, this is the first instance of a
biologically inspired generative model employing multiplicative updates,
founded on geometric Brownian motion.

</details>


### [99] [Hybrid-Collaborative Augmentation and Contrastive Sample Adaptive-Differential Awareness for Robust Attributed Graph Clustering](https://arxiv.org/abs/2510.02731)
*Tianxiang Zhao,Youqing Wang,Jinlu Wang,Jiapu Wang,Mingliang Cui,Junbin Gao,Jipeng Guo*

Main category: cs.LG

TL;DR: 传统CAGC方法存在忽视边级嵌入增强和正负样本对差异问题，本文提出RAGC方法，结合HCA和CSADA，在六个基准数据集上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 多数CAGC方法仅关注节点级嵌入增强，忽略边级嵌入增强及不同粒度嵌入增强间的交互，且平等对待所有对比样本对，限制了判别能力。

Method: 提出RAGC方法，同时执行节点级和边级嵌入表示与增强，建立更全面的相似性度量标准；利用高置信度伪标签信息设计CSADA策略，自适应识别和差异化处理对比样本对。

Result: 在六个基准数据集上的图聚类评估表明，RAGC方法优于多个先进的CAGC方法。

Conclusion: RAGC方法通过HCA和CSADA模块相互促进，能有效提高表示学习的判别能力。

Abstract: Due to its powerful capability of self-supervised representation learning and
clustering, contrastive attributed graph clustering (CAGC) has achieved great
success, which mainly depends on effective data augmentation and contrastive
objective setting. However, most CAGC methods utilize edges as auxiliary
information to obtain node-level embedding representation and only focus on
node-level embedding augmentation. This approach overlooks edge-level embedding
augmentation and the interactions between node-level and edge-level embedding
augmentations across various granularity. Moreover, they often treat all
contrastive sample pairs equally, neglecting the significant differences
between hard and easy positive-negative sample pairs, which ultimately limits
their discriminative capability. To tackle these issues, a novel robust
attributed graph clustering (RAGC), incorporating hybrid-collaborative
augmentation (HCA) and contrastive sample adaptive-differential awareness
(CSADA), is proposed. First, node-level and edge-level embedding
representations and augmentations are simultaneously executed to establish a
more comprehensive similarity measurement criterion for subsequent contrastive
learning. In turn, the discriminative similarity further consciously guides
edge augmentation. Second, by leveraging pseudo-label information with high
confidence, a CSADA strategy is elaborately designed, which adaptively
identifies all contrastive sample pairs and differentially treats them by an
innovative weight modulation function. The HCA and CSADA modules mutually
reinforce each other in a beneficent cycle, thereby enhancing discriminability
in representation learning. Comprehensive graph clustering evaluations over six
benchmark datasets demonstrate the effectiveness of the proposed RAGC against
several state-of-the-art CAGC methods.

</details>


### [100] [TokenFlow: Responsive LLM Text Streaming Serving under Request Burst via Preemptive Scheduling](https://arxiv.org/abs/2510.02758)
*Junyi Chen,Chuheng Du,Renyuan Liu,Shuochao Yao,Dingtian Yan,Jiang Liao,Shengzhong Liu,Fan Wu,Guihai Chen*

Main category: cs.LG

TL;DR: 提出TokenFlow系统，通过抢占式请求调度和主动KV缓存管理提升LLM文本流性能，实验显示有效吞吐量提升、P99 TTFT降低。


<details>
  <summary>Details</summary>
Motivation: 标准LLM服务系统因非抢占式请求调度和被动内存管理，在请求突发时资源利用率低、请求处理并行性差，需改进文本流性能。

Method: TokenFlow基于实时令牌缓冲区占用和令牌消耗率动态对请求进行优先级排序，在后台主动在GPU和CPU内存之间转移KV缓存，并使I/O与计算重叠以减少请求抢占开销。

Result: 在多GPU上对Llama3 - 8B和Qwen2.5 - 32B的实验表明，TokenFlow有效吞吐量最高提升82.5%，P99 TTFT最多降低80.2%，且不降低整体令牌吞吐量。

Conclusion: TokenFlow能有效提升LLM文本流性能，平衡响应性和稳定生成。

Abstract: Real-time LLM interactions demand streamed token generations, where text
tokens are progressively generated and delivered to users while balancing two
objectives: responsiveness (i.e., low time-to-first-token) and steady
generation (i.e.,required time-between-tokens). Standard LLM serving systems
suffer from the inflexibility caused by non-preemptive request scheduling and
reactive memory management, leading to poor resource utilization and low
request processing parallelism under request bursts. Therefore, we present
TokenFlow, a novel LLM serving system with enhanced text streaming performance
via preemptive request scheduling and proactive key-value (KV) cache
management. TokenFlow dynamically prioritizes requests based on real-time token
buffer occupancy and token consumption rate, while actively transferring KV
cache between GPU and CPU memory in the background and overlapping I/O with
computation to minimize request preemption overhead. Extensive experiments on
Llama3-8B and Qwen2.5-32B across multiple GPUs (RTX 4090, A6000, H200)
demonstrate that TokenFlow achieves up to 82.5% higher effective throughput
(accounting for actual user consumption) while reducing P99 TTFT by up to
80.2%, without degrading overall token throughput.

</details>


### [101] [Fusing Multi- and Hyperspectral Satellite Data for Harmful Algal Bloom Monitoring with Self-Supervised and Hierarchical Deep Learning](https://arxiv.org/abs/2510.02763)
*Nicholas LaHaye,Kelly M. Luis,Michelle M. Gierach*

Main category: cs.LG

TL;DR: 提出自监督机器学习框架SIT - FUSE，用多传感器卫星数据检测和绘制有害藻华严重程度和物种分类，结果与实测相符，推动标签稀缺环境下藻华监测。


<details>
  <summary>Details</summary>
Motivation: 解决在标签稀缺环境下进行有害藻华（HAB）监测和物种分类的问题。

Method: 融合多传感器反射率数据与TROPOMI太阳诱导荧光数据，采用自监督表征学习和分层深度聚类。

Result: 结果与总浮游植物、特定藻类测量结果高度一致。

Conclusion: 推进了标签稀缺环境下可扩展的HAB监测，为全球水生生物地球化学应用自监督学习迈出关键一步。

Abstract: We present a self-supervised machine learning framework for detecting and
mapping harmful algal bloom (HAB) severity and speciation using multi-sensor
satellite data. By fusing reflectance data from operational instruments (VIIRS,
MODIS, Sentinel-3, PACE) with TROPOMI solar-induced fluorescence (SIF), our
framework, called SIT-FUSE, generates HAB severity and speciation products
without requiring per-instrument labeled datasets. The framework employs
self-supervised representation learning, hierarchical deep clustering to
segment phytoplankton concentrations and speciations into interpretable
classes, validated against in-situ data from the Gulf of Mexico and Southern
California (2018-2025). Results show strong agreement with total phytoplankton,
Karenia brevis, Alexandrium spp., and Pseudo-nitzschia spp. measurements. This
work advances scalable HAB monitoring in label-scarce environments while
enabling exploratory analysis via hierarchical embeddings: a critical step
toward operationalizing self-supervised learning for global aquatic
biogeochemistry.

</details>


### [102] [Curl Descent: Non-Gradient Learning Dynamics with Sign-Diverse Plasticity](https://arxiv.org/abs/2510.02765)
*Hugo Ninou,Jonathan Kadmon,N. Alex Cayco-Gajic*

Main category: cs.LG

TL;DR: 研究了学习动态中可能存在的非梯度“curl”项对神经网络学习的影响，发现其在不同强度下有不同作用，为基于梯度的学习规范理论提供了反例。


<details>
  <summary>Details</summary>
Motivation: 探究生物神经网络学习中是否采用类似梯度策略，以及突触可塑性规则是否近似梯度下降。

Method: 在可解析的师生框架下分析前馈网络，通过具有规则翻转可塑性的神经元引入非梯度动态。

Result: 小的curl项保持原解流形稳定性，学习动态类似梯度下降；强curl项使解流形不稳定，可能导致混沌学习或加速学习。

Conclusion: 确定了能通过多样学习规则支持稳健学习的特定架构，为基于梯度的学习规范理论提供反例。

Abstract: Gradient-based algorithms are a cornerstone of artificial neural network
training, yet it remains unclear whether biological neural networks use similar
gradient-based strategies during learning. Experiments often discover a
diversity of synaptic plasticity rules, but whether these amount to an
approximation to gradient descent is unclear. Here we investigate a previously
overlooked possibility: that learning dynamics may include fundamentally
non-gradient "curl"-like components while still being able to effectively
optimize a loss function. Curl terms naturally emerge in networks with
inhibitory-excitatory connectivity or Hebbian/anti-Hebbian plasticity,
resulting in learning dynamics that cannot be framed as gradient descent on any
objective. To investigate the impact of these curl terms, we analyze
feedforward networks within an analytically tractable student-teacher
framework, systematically introducing non-gradient dynamics through neurons
exhibiting rule-flipped plasticity. Small curl terms preserve the stability of
the original solution manifold, resulting in learning dynamics similar to
gradient descent. Beyond a critical value, strong curl terms destabilize the
solution manifold. Depending on the network architecture, this loss of
stability can lead to chaotic learning dynamics that destroy performance. In
other cases, the curl terms can counterintuitively speed learning compared to
gradient descent by allowing the weight dynamics to escape saddles by
temporarily ascending the loss. Our results identify specific architectures
capable of supporting robust learning via diverse learning rules, providing an
important counterpoint to normative theories of gradient-based learning in
neural networks.

</details>


### [103] [A Granular Study of Safety Pretraining under Model Abliteration](https://arxiv.org/abs/2510.02768)
*Shashank Agnihotri,Jonas Jakubassa,Priyam Dey,Sachin Goyal,Bernt Schiele,Venkatesh Babu Radhakrishnan,Margret Keuper*

Main category: cs.LG

TL;DR: 研究常见安全干预措施在推理时激活编辑下的情况，对模型进行消蚀评估，得出数据中心安全组件的鲁棒性、评估结果受评判者选择的影响，并给出将推理时编辑纳入安全评估的协议。


<details>
  <summary>Details</summary>
Motivation: 探讨常见安全干预措施（如拒绝训练或元标签训练）在推理时激活编辑下是否仍有效。

Method: 研究模型消蚀技术，对SmolLM2 - 1.7B的安全预训练检查点和广泛使用的开放基线进行评估，对20个系统（原始和消蚀后）各发出100个提示，用多位评判者对回复分类，在小部分人工标注子集上验证评判者准确性，还探测模型能否识别自身输出中的拒绝情况。

Result: 得出了检查点级别的数据中心安全组件在消蚀下的鲁棒性特征，量化了评判者选择对评估结果的影响。

Conclusion: 给出了将推理时编辑纳入安全评估的实用协议。

Abstract: Open-weight LLMs can be modified at inference time with simple activation
edits, which raises a practical question for safety: do common safety
interventions like refusal training or metatag training survive such edits? We
study model abliteration, a lightweight projection technique designed to remove
refusal-sensitive directions, and conduct a controlled evaluation across a
granular sequence of Safety Pretraining checkpoints for SmolLM2-1.7B, alongside
widely used open baselines. For each of 20 systems, original and abliterated,
we issue 100 prompts with balanced harmful and harmless cases, classify
responses as **Refusal** or **Non-Refusal** using multiple judges, and validate
judge fidelity on a small human-labeled subset. We also probe whether models
can identify refusal in their own outputs. Our study produces a
checkpoint-level characterization of which data-centric safety components
remain robust under abliteration, quantifies how judge selection influences
evaluation outcomes, and outlines a practical protocol for integrating
inference-time edits into safety assessments. Code:
https://github.com/shashankskagnihotri/safety_pretraining.

</details>


### [104] [Optimal Rates for Generalization of Gradient Descent for Deep ReLU Classification](https://arxiv.org/abs/2510.02779)
*Yuanfan Li,Yunwen Lei,Zheng-Chu Guo,Yiming Ying*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent advances have significantly improved our understanding of the
generalization performance of gradient descent (GD) methods in deep neural
networks. A natural and fundamental question is whether GD can achieve
generalization rates comparable to the minimax optimal rates established in the
kernel setting. Existing results either yield suboptimal rates of
$O(1/\sqrt{n})$, or focus on networks with smooth activation functions,
incurring exponential dependence on network depth $L$. In this work, we
establish optimal generalization rates for GD with deep ReLU networks by
carefully trading off optimization and generalization errors, achieving only
polynomial dependence on depth. Specifically, under the assumption that the
data are NTK separable from the margin $\gamma$, we prove an excess risk rate
of $\widetilde{O}(L^4 (1 + \gamma L^2) / (n \gamma^2))$, which aligns with the
optimal SVM-type rate $\widetilde{O}(1 / (n \gamma^2))$ up to depth-dependent
factors. A key technical contribution is our novel control of activation
patterns near a reference model, enabling a sharper Rademacher complexity bound
for deep ReLU networks trained with gradient descent.

</details>


### [105] [OptunaHub: A Platform for Black-Box Optimization](https://arxiv.org/abs/2510.02798)
*Yoshihiko Ozaki,Shuhei Watanabe,Toshihiko Yanase*

Main category: cs.LG

TL;DR: 介绍OptunaHub平台，它集中了黑盒优化方法和基准，旨在促进跨领域研究和贡献应用的良性循环，代码公开。


<details>
  <summary>Details</summary>
Motivation: 黑盒优化研究在各领域较为分散，需要一个集中平台促进跨领域研究。

Method: 创建OptunaHub平台，提供统一Python API、贡献者包注册表和Web界面。

Result: 成功推出OptunaHub平台，代码在GitHub上公开。

Conclusion: OptunaHub有望促进黑盒优化领域的贡献和应用的良性循环。

Abstract: Black-box optimization (BBO) drives advances in domains such as AutoML and
Materials Informatics, yet research efforts often remain fragmented across
domains. We introduce OptunaHub (https://hub.optuna.org/), a community platform
that centralizes BBO methods and benchmarks. OptunaHub provides unified Python
APIs, a contributor package registry, and a web interface to promote
searchability and cross-domain research. OptunaHub aims to foster a virtuous
cycle of contributions and applications. The source code is publicly available
in the optunahub, optunahub-registry, and optunahub-web repositories under the
Optuna organization on GitHub (https://github.com/optuna/).

</details>


### [106] [Relevance-Aware Thresholding in Online Conformal Prediction for Time Series](https://arxiv.org/abs/2510.02809)
*Théo Dupuy,Binbin Xu,Stéphane Perrey,Jacky Montmain,Abdelhak Imoussaten*

Main category: cs.LG

TL;DR: 本文聚焦时间序列在线共形预测（OCP）方法，提出改进阈值更新步骤以提升预测区间质量。


<details>
  <summary>Details</summary>
Motivation: 现有OCP方法在阈值更新时多只关注预测区间有效性，未考虑相关性，本文旨在利用这一被忽视的方面。

Method: 用更广泛的函数替代二元评估来量化预测区间与真实值的相关性，改进阈值更新步骤。

Result: 在真实数据集上实验表明，该方法能产生更窄的预测区间，同时保持覆盖有效性。

Conclusion: 所提方法可防止阈值突变，产生比现有OCP方法更紧的区间，具有一定优势。

Abstract: Uncertainty quantification has received considerable interest in recent works
in Machine Learning. In particular, Conformal Prediction (CP) gains ground in
this field. For the case of time series, Online Conformal Prediction (OCP)
becomes an option to address the problem of data distribution shift over time.
Indeed, the idea of OCP is to update a threshold of some quantity (whether the
miscoverage level or the quantile) based on the distribution observation. To
evaluate the performance of OCP methods, two key aspects are typically
considered: the coverage validity and the prediction interval width
minimization. Recently, new OCP methods have emerged, offering long-run
coverage guarantees and producing more informative intervals. However, during
the threshold update step, most of these methods focus solely on the validity
of the prediction intervals~--~that is, whether the ground truth falls inside
or outside the interval~--~without accounting for their relevance. In this
paper, we aim to leverage this overlooked aspect. Specifically, we propose
enhancing the threshold update step by replacing the binary evaluation
(inside/outside) with a broader class of functions that quantify the relevance
of the prediction interval using the ground truth. This approach helps prevent
abrupt threshold changes, potentially resulting in narrower prediction
intervals. Indeed, experimental results on real-world datasets suggest that
these functions can produce tighter intervals compared to existing OCP methods
while maintaining coverage validity.

</details>


### [107] [Mitigating Spurious Correlation via Distributionally Robust Learning with Hierarchical Ambiguity Sets](https://arxiv.org/abs/2510.02818)
*Sung Ho Jo,Seonghwi Kim,Minwoo Chae*

Main category: cs.LG

TL;DR: 传统监督学习易受虚假关联影响，提出Group DRO的分层扩展方法应对组间和组内不确定性，设新基准，表现佳。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习易受虚假关联影响，现有方法对组内分布偏移脆弱，需解决组间和组内不确定性。

Method: 提出Group DRO的分层扩展方法，引入模拟少数群体分布偏移的新基准设置。

Result: 在新设置下表现出强鲁棒性，现有方法失败，标准基准上表现优。

Conclusion: 扩大模糊集以捕获组间和组内分布不确定性很重要。

Abstract: Conventional supervised learning methods are often vulnerable to spurious
correlations, particularly under distribution shifts in test data. To address
this issue, several approaches, most notably Group DRO, have been developed.
While these methods are highly robust to subpopulation or group shifts, they
remain vulnerable to intra-group distributional shifts, which frequently occur
in minority groups with limited samples. We propose a hierarchical extension of
Group DRO that addresses both inter-group and intra-group uncertainties,
providing robustness to distribution shifts at multiple levels. We also
introduce new benchmark settings that simulate realistic minority group
distribution shifts-an important yet previously underexplored challenge in
spurious correlation research. Our method demonstrates strong robustness under
these conditions-where existing robust learning methods consistently fail-while
also achieving superior performance on standard benchmarks. These results
highlight the importance of broadening the ambiguity set to better capture both
inter-group and intra-group distributional uncertainties.

</details>


### [108] [FlexiQ: Adaptive Mixed-Precision Quantization for Latency/Accuracy Trade-Offs in Deep Neural Networks](https://arxiv.org/abs/2510.02822)
*Jaemin Kim,Hongjun Um,Sungkyun Kim,Yongjun Park,Jiwon Seo*

Main category: cs.LG

TL;DR: 提出自适应混合精度量化方案FlexiQ，在多个视觉模型上评估效果好，实现高效准确率 - 延迟权衡。


<details>
  <summary>Details</summary>
Motivation: 神经网络在硬件加速器上运行成本高且难应对实时工作负载波动。

Method: 选择性地对小值范围特征通道应用低比特计算，采用高效降比特方法，实时调整低比特通道比例。

Result: 在11个视觉模型上，4位模型微调后平均准确率高6.6%，优于4种先进量化技术，50% 4位模型准确率损失小且有速度提升，引入的运行时开销小。

Conclusion: FlexiQ具有硬件效率和整体性能优势。

Abstract: Neural networks commonly execute on hardware accelerators such as NPUs and
GPUs for their size and computation overhead. These accelerators are costly and
it is hard to scale their resources to handle real-time workload fluctuations.
  We present FlexiQ, an adaptive mixed-precision quantization scheme for
computer vision models. FlexiQ selectively applies low-bitwidth computation to
feature channels with small value ranges and employs an efficient bit-lowering
method to minimize quantization errors while maintaining inference accuracy.
Furthermore, FlexiQ adjusts its low-bitwidth channel ratio in real time,
enabling quantized models to effectively manage fluctuating inference workload.
  We implemented FlexiQ prototype, including the mixed-precision inference
runtime on our custom NPU and GPUs. Evaluated on eleven convolution- and
transformer-based vision models, FlexiQ achieves on average 6.6% higher
accuracy for 4-bit models with finetuning and outperforms four state-of-the-art
quantization techniques. Moreover, our mixed-precision models achieved an
efficient accuracy-latency trade-off, with the 50% 4-bit model incurring only
0.6% accuracy loss while achieving 40% of the speedup of the 100% 4-bit model
over 8-bit model. Latency evaluations on our NPU and GPUs confirmed that FlexiQ
introduces minimal runtime overhead, demonstrating its hardware efficiency and
overall performance benefits.

</details>


### [109] [The Curious Case of In-Training Compression of State Space Models](https://arxiv.org/abs/2510.02823)
*Makram Chahine,Philipp Nazari,Daniela Rus,T. Konstantin Rusch*

Main category: cs.LG

TL;DR: 本文提出利用控制理论对状态空间模型（SSMs）进行训练时降维，实验表明该方法能在保持表现力的同时加速优化。


<details>
  <summary>Details</summary>
Motivation: 解决SSMs在最大化表现力和限制计算负担之间的平衡问题。

Method: 利用Hankel矩阵的特征值稳定性，在训练SSMs时识别并保留高影响维度。

Result: 训练时降维显著加速优化，压缩模型保留了关键结构，保持了较高性能。

Conclusion: 从大模型开始并在训练中收缩的SSMs能实现计算效率和高性能的平衡。

Abstract: State Space Models (SSMs), developed to tackle long sequence modeling tasks
efficiently, offer both parallelizable training and fast inference. At their
core are recurrent dynamical systems that maintain a hidden state, with update
costs scaling with the state dimension. A key design challenge is striking the
right balance between maximizing expressivity and limiting this computational
burden. Control theory, and more specifically Hankel singular value analysis,
provides a potent framework for the measure of energy for each state, as well
as the balanced truncation of the original system down to a smaller
representation with performance guarantees. Leveraging the eigenvalue stability
properties of Hankel matrices, we apply this lens to SSMs during training,
where only dimensions of high influence are identified and preserved. Our
approach applies to Linear Time-Invariant SSMs such as Linear Recurrent Units,
but is also extendable to selective models. Experiments show that in-training
reduction significantly accelerates optimization while preserving expressivity,
with compressed models retaining task-critical structure lost by models trained
directly at smaller dimension. In other words, SSMs that begin large and shrink
during training achieve computational efficiency while maintaining higher
performance.

</details>


### [110] [Multi-scale Autoregressive Models are Laplacian, Discrete, and Latent Diffusion Models in Disguise](https://arxiv.org/abs/2510.02826)
*Steve Hong,Samuel Belkadi*

Main category: cs.LG

TL;DR: 从迭代细化框架重新审视视觉自回归（VAR）模型，连接VAR与去噪扩散，实验量化各因素影响并拓展应用。


<details>
  <summary>Details</summary>
Motivation: 重新审视VAR模型，挖掘其效率和保真度原因，拓展其应用。

Method: 将VAR形式化为确定性前向过程构建拉普拉斯风格潜在金字塔，配合学习的反向过程以粗到细步骤重构，进行控制实验量化因素影响。

Result: 明确了有助于解释VAR效率和保真度的三个设计选择，实验量化了各因素对保真度和速度的贡献。

Conclusion: 该框架可拓展到图生成和气象预报，能让VAR利用扩散生态系统工具并保持少步、尺度并行生成。

Abstract: We revisit Visual Autoregressive (VAR) models through the lens of an
iterative-refinement framework. Rather than viewing VAR solely as next-scale
autoregression, we formalise it as a deterministic forward process that
constructs a Laplacian-style latent pyramid, paired with a learned backward
process that reconstructs it in a small number of coarse-to-fine steps. This
view connects VAR to denoising diffusion and isolates three design choices that
help explain its efficiency and fidelity: refining in a learned latent space,
casting prediction as discrete classification over code indices, and
partitioning the task by spatial frequency. We run controlled experiments to
quantify each factor's contribution to fidelity and speed, and we outline how
the same framework extends to permutation-invariant graph generation and to
probabilistic, ensemble-style medium-range weather forecasting. The framework
also suggests practical interfaces for VAR to leverage tools from the diffusion
ecosystem while retaining few-step, scale-parallel generation.

</details>


### [111] [Subject-Adaptive Sparse Linear Models for Interpretable Personalized Health Prediction from Multimodal Lifelog Data](https://arxiv.org/abs/2510.02835)
*Dohyun Bu,Jisoo Han,Soohwa Kwon,Yulim So,Jong-Seok Lee*

Main category: cs.LG

TL;DR: 提出Subject - Adaptive Sparse Linear (SASL)框架用于个性化健康预测，结合LightGBM在CH - 2025数据集上评估，性能与黑盒方法相当，参数少且更具可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有模型在个性化健康结果预测中牺牲可解释性，未充分解决个体间差异问题。

Method: 提出SASL框架，集成普通最小二乘回归与特定主体交互；用基于嵌套F检验的迭代反向特征消除法构建模型；采用回归 - 阈值法最大化F1分数；通过基于置信度的门控选择性纳入LightGBM模型输出。

Result: 在CH - 2025数据集上，混合SASL - LightGBM框架预测性能与复杂黑盒方法相当。

Conclusion: SASL框架参数少、透明度高，能为临床医生和从业者提供清晰可行的见解。

Abstract: Improved prediction of personalized health outcomes -- such as sleep quality
and stress -- from multimodal lifelog data could have meaningful clinical and
practical implications. However, state-of-the-art models, primarily deep neural
networks and gradient-boosted ensembles, sacrifice interpretability and fail to
adequately address the significant inter-individual variability inherent in
lifelog data. To overcome these challenges, we propose the Subject-Adaptive
Sparse Linear (SASL) framework, an interpretable modeling approach explicitly
designed for personalized health prediction. SASL integrates ordinary least
squares regression with subject-specific interactions, systematically
distinguishing global from individual-level effects. We employ an iterative
backward feature elimination method based on nested $F$-tests to construct a
sparse and statistically robust model. Additionally, recognizing that health
outcomes often represent discretized versions of continuous processes, we
develop a regression-then-thresholding approach specifically designed to
maximize macro-averaged F1 scores for ordinal targets. For intrinsically
challenging predictions, SASL selectively incorporates outputs from compact
LightGBM models through confidence-based gating, enhancing accuracy without
compromising interpretability. Evaluations conducted on the CH-2025 dataset --
which comprises roughly 450 daily observations from ten subjects -- demonstrate
that the hybrid SASL-LightGBM framework achieves predictive performance
comparable to that of sophisticated black-box methods, but with significantly
fewer parameters and substantially greater transparency, thus providing clear
and actionable insights for clinicians and practitioners.

</details>


### [112] [Knowledge-Aware Modeling with Frequency Adaptive Learning for Battery Health Prognostics](https://arxiv.org/abs/2510.02839)
*Vijay Babu Pamshetti,Wei Zhang,Sumei Sun,Jie Zhang,Yonggang Wen,Qingyu Yan*

Main category: cs.LG

TL;DR: 提出Karma模型用于电池健康预测，实验显示其性能优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动模型缺乏知识引导，难以实现准确可靠的电池健康预测。

Method: 先进行信号分解，采用双流深度学习架构分别捕捉低频长期和高频短期特征，用知识规范预测，用粒子滤波器优化知识参数。

Result: 在两个主流数据集上，相比现有算法，电池健康预测平均误差分别降低50.6%和32.6%。

Conclusion: Karma模型具有鲁棒性、泛化性，有潜力用于更安全可靠的电池管理。

Abstract: Battery health prognostics are critical for ensuring safety, efficiency, and
sustainability in modern energy systems. However, it has been challenging to
achieve accurate and robust prognostics due to complex battery degradation
behaviors with nonlinearity, noise, capacity regeneration, etc. Existing
data-driven models capture temporal degradation features but often lack
knowledge guidance, which leads to unreliable long-term health prognostics. To
overcome these limitations, we propose Karma, a knowledge-aware model with
frequency-adaptive learning for battery capacity estimation and remaining
useful life prediction. The model first performs signal decomposition to derive
battery signals in different frequency bands. A dual-stream deep learning
architecture is developed, where one stream captures long-term low-frequency
degradation trends and the other models high-frequency short-term dynamics.
Karma regulates the prognostics with knowledge, where battery degradation is
modeled as a double exponential function based on empirical studies. Our
dual-stream model is used to optimize the parameters of the knowledge with
particle filters to ensure physically consistent and reliable prognostics and
uncertainty quantification. Experimental study demonstrates Karma's superior
performance, achieving average error reductions of 50.6% and 32.6% over
state-of-the-art algorithms for battery health prediction on two mainstream
datasets, respectively. These results highlight Karma's robustness,
generalizability, and potential for safer and more reliable battery management
across diverse applications.

</details>


### [113] [RoiRL: Efficient, Self-Supervised Reasoning with Offline Iterative Reinforcement Learning](https://arxiv.org/abs/2510.02892)
*Aleksei Arzhantsev,Otmane Sakhi,Flavian Vasile*

Main category: cs.LG

TL;DR: 提出RoiRL方法，可在无标签下优化大语言模型推理，训练快且性能优。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习需真实奖励，TTRL依赖在线学习且计算成本高，需轻量级离线学习方法。

Method: 提出RoiRL，消除维护参考模型的需求，优化加权对数似然目标。

Result: RoiRL训练速度快2.5倍，在推理基准测试中始终优于TTRL。

Conclusion: RoiRL为无标签自改进大语言模型提供了可扩展途径。

Abstract: Reinforcement learning (RL) is central to improving reasoning in large
language models (LLMs) but typically requires ground-truth rewards. Test-Time
Reinforcement Learning (TTRL) removes this need by using majority-vote rewards,
but relies on heavy online RL and incurs substantial computational cost. We
propose RoiRL: Reasoning with offline iterative Reinforcement Learning, a
family of lightweight offline learning alternatives that can target the same
regularized optimal policies. Unlike TTRL, RoiRL eliminates the need to
maintain a reference model and instead optimizes weighted log-likelihood
objectives, enabling stable training with significantly lower memory and
compute requirements. Experimental results show that RoiRL trains to 2.5x
faster and consistently outperforms TTRL on reasoning benchmarks, establishing
a scalable path to self-improving LLMs without labels.

</details>


### [114] [DMark: Order-Agnostic Watermarking for Diffusion Large Language Models](https://arxiv.org/abs/2510.02902)
*Linyu Wu,Linhao Zhong,Wenjie Qu,Yuexin Li,Yue Liu,Shengfang Zhai,Chunhua Shen,Jiaheng Zhang*

Main category: cs.LG

TL;DR: 提出首个针对扩散大语言模型（dLLMs）的水印框架DMark，实验显示其检测率高且能保持文本质量，还具抗文本操作鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有水印方法因dLLMs非顺序解码而失效，需为其设计专用水印框架。

Method: 引入三种互补策略，即预测水印、双向水印和预测 - 双向水印以恢复水印可检测性。

Result: 在多个dLLMs上实验表明，DMark在1%误报率下检测率达92.0 - 99.5%，而现有方法简单适配后仅49.6 - 71.2%。

Conclusion: 有效水印对非自回归语言模型是可行的。

Abstract: Diffusion large language models (dLLMs) offer faster generation than
autoregressive models while maintaining comparable quality, but existing
watermarking methods fail on them due to their non-sequential decoding. Unlike
autoregressive models that generate tokens left-to-right, dLLMs can finalize
tokens in arbitrary order, breaking the causal design underlying traditional
watermarks. We present DMark, the first watermarking framework designed
specifically for dLLMs. DMark introduces three complementary strategies to
restore watermark detectability: predictive watermarking uses model-predicted
tokens when actual context is unavailable; bidirectional watermarking exploits
both forward and backward dependencies unique to diffusion decoding; and
predictive-bidirectional watermarking combines both approaches to maximize
detection strength. Experiments across multiple dLLMs show that DMark achieves
92.0-99.5% detection rates at 1% false positive rate while maintaining text
quality, compared to only 49.6-71.2% for naive adaptations of existing methods.
DMark also demonstrates robustness against text manipulations, establishing
that effective watermarking is feasible for non-autoregressive language models.

</details>


### [115] [Learning Explicit Single-Cell Dynamics Using ODE Representations](https://arxiv.org/abs/2510.02903)
*Jan-Philipp von Bassewitz,Adeel Pervez,Marco Fumero,Matthew Robinson,Theofanis Karaletsos,Francesco Locatello*

Main category: cs.LG

TL;DR: 提出Cell - MNN模型解决现有细胞分化动力学建模问题，性能佳且可学习可解释的基因相互作用。


<details>
  <summary>Details</summary>
Motivation: 现有细胞分化动力学建模的先进模型依赖计算昂贵的最优传输预处理和多阶段训练，且无法发现明确的基因相互作用，需要改进。

Method: 提出Cell - MNN，一种编码器 - 解码器架构，其潜在表示是一个局部线性化的常微分方程，控制从干细胞到组织细胞的细胞进化动力学，且除标准PCA预处理外是端到端的。

Result: Cell - MNN在单细胞基准测试中取得有竞争力的性能，在处理更大数据集和跨多数据集联合训练方面超越现有先进基线，还学习到可解释的基因相互作用并通过TRRUST数据库验证。

Conclusion: Cell - MNN是一种有效的细胞分化动力学建模方法，能解决现有模型的问题。

Abstract: Modeling the dynamics of cellular differentiation is fundamental to advancing
the understanding and treatment of diseases associated with this process, such
as cancer. With the rapid growth of single-cell datasets, this has also become
a particularly promising and active domain for machine learning. Current
state-of-the-art models, however, rely on computationally expensive optimal
transport preprocessing and multi-stage training, while also not discovering
explicit gene interactions. To address these challenges we propose
Cell-Mechanistic Neural Networks (Cell-MNN), an encoder-decoder architecture
whose latent representation is a locally linearized ODE governing the dynamics
of cellular evolution from stem to tissue cells. Cell-MNN is fully end-to-end
(besides a standard PCA pre-processing) and its ODE representation explicitly
learns biologically consistent and interpretable gene interactions.
Empirically, we show that Cell-MNN achieves competitive performance on
single-cell benchmarks, surpasses state-of-the-art baselines in scaling to
larger datasets and joint training across multiple datasets, while also
learning interpretable gene interactions that we validate against the TRRUST
database of gene interactions.

</details>


### [116] [FeDABoost: Fairness Aware Federated Learning with Adaptive Boosting](https://arxiv.org/abs/2510.02914)
*Tharuka Kasthuri Arachchige,Veselka Boeva,Shahrooz Abghari*

Main category: cs.LG

TL;DR: 提出FeDABoost框架，结合动态提升机制和自适应梯度聚合策略，在三个基准数据集上评估，实现公平性提升和有竞争力的性能。


<details>
  <summary>Details</summary>
Motivation: 提高非IID设置下联邦学习的性能和公平性，通过增强模型聚合和促进表现不佳客户端的训练。

Method: 提出FeDABoost框架，聚合方法借鉴Multiclass AdaBoost算法的加权机制，给局部错误率低的客户端更高权重；动态调整focal loss聚焦参数提升表现不佳的客户端。

Result: 在MNIST、FEMNIST和CIFAR10三个基准数据集上评估，与FedAvg和Ditto比较，FeDABoost实现了更好的公平性和有竞争力的性能。

Conclusion: FeDABoost能提高非IID设置下联邦学习的公平性和性能。

Abstract: This work focuses on improving the performance and fairness of Federated
Learning (FL) in non IID settings by enhancing model aggregation and boosting
the training of underperforming clients. We propose FeDABoost, a novel FL
framework that integrates a dynamic boosting mechanism and an adaptive gradient
aggregation strategy. Inspired by the weighting mechanism of the Multiclass
AdaBoost (SAMME) algorithm, our aggregation method assigns higher weights to
clients with lower local error rates, thereby promoting more reliable
contributions to the global model. In parallel, FeDABoost dynamically boosts
underperforming clients by adjusting the focal loss focusing parameter,
emphasizing hard to classify examples during local training. We have evaluated
FeDABoost on three benchmark datasets MNIST, FEMNIST, and CIFAR10, and compared
its performance with those of FedAvg and Ditto. The results show that FeDABoost
achieves improved fairness and competitive performance.

</details>


### [117] [RAxSS: Retrieval-Augmented Sparse Sampling for Explainable Variable-Length Medical Time Series Classification](https://arxiv.org/abs/2510.02936)
*Aydin Javadov,Samir Garibov,Tobias Hoesli,Qiyang Sun,Florian von Wangenheim,Joseph Ollier,Björn W. Schuller*

Main category: cs.LG

TL;DR: 研究将随机稀疏采样框架推广用于检索信息分类，在iEEG分类中表现良好，有可靠且可解释性。


<details>
  <summary>Details</summary>
Motivation: 医疗时间序列分析因数据稀疏、噪声和记录长度多变而具挑战性，此前随机稀疏采样和检索增强方法有一定效果，需进一步推广。

Method: 将随机稀疏采样框架推广用于检索信息分类，按通道内相似度对窗口预测加权并在概率空间聚合。

Result: 在四个医疗中心收集的iEEG记录中评估，方法取得有竞争力的iEEG分类性能。

Conclusion: 该方法可用于可靠且可解释的临床可变长度时间序列分类。

Abstract: Medical time series analysis is challenging due to data sparsity, noise, and
highly variable recording lengths. Prior work has shown that stochastic sparse
sampling effectively handles variable-length signals, while retrieval-augmented
approaches improve explainability and robustness to noise and weak temporal
correlations. In this study, we generalize the stochastic sparse sampling
framework for retrieval-informed classification. Specifically, we weight window
predictions by within-channel similarity and aggregate them in probability
space, yielding convex series-level scores and an explicit evidence trail for
explainability. Our method achieves competitive iEEG classification performance
and provides practitioners with greater transparency and explainability. We
evaluate our method in iEEG recordings collected in four medical centers,
demonstrating its potential for reliable and explainable clinical
variable-length time series classification.

</details>


### [118] [Ergodic Risk Measures: Towards a Risk-Aware Foundation for Continual Reinforcement Learning](https://arxiv.org/abs/2510.02945)
*Juan Sebastian Rojas,Chi-Guhn Lee*

Main category: cs.LG

TL;DR: 本文首次从风险感知决策角度对持续强化学习进行理论研究，引入遍历风险度量，结合案例和实证展示其优势。


<details>
  <summary>Details</summary>
Motivation: 现有持续强化学习多基于风险中性决策，本文旨在从风险感知决策角度进行理论研究。

Method: 指出经典风险度量理论与持续学习不兼容，引入遍历风险度量将风险度量理论拓展到持续学习场景，并进行案例研究和实证分析。

Result: 实证结果显示遍历风险度量具有直观吸引力和理论合理性。

Conclusion: 引入的遍历风险度量可将风险度量理论拓展到持续学习场景，具有良好效果。

Abstract: Continual reinforcement learning (continual RL) seeks to formalize the
notions of lifelong learning and endless adaptation in RL. In particular, the
aim of continual RL is to develop RL agents that can maintain a careful balance
between retaining useful information and adapting to new situations. To date,
continual RL has been explored almost exclusively through the lens of
risk-neutral decision-making, in which the agent aims to optimize the expected
(or mean) long-run performance. In this work, we present the first formal
theoretical treatment of continual RL through the lens of risk-aware
decision-making, in which the agent aims to optimize a reward-based measure of
long-run performance beyond the mean. In particular, we show that the classical
theory of risk measures, widely used as a theoretical foundation in
non-continual risk-aware RL, is, in its current form, incompatible with the
continual setting. Then, building on this insight, we extend risk measure
theory into the continual setting by introducing a new class of ergodic risk
measures that are compatible with continual learning. Finally, we provide a
case study of risk-aware continual learning, along with empirical results,
which show the intuitive appeal and theoretical soundness of ergodic risk
measures.

</details>


### [119] [ContextFlow: Context-Aware Flow Matching For Trajectory Inference From Spatial Omics Data](https://arxiv.org/abs/2510.02952)
*Santanu Subhash Rathod,Francesco Ceccarelli,Sean B. Holden,Pietro Liò,Xiao Zhang,Jovan Tanevski*

Main category: cs.LG

TL;DR: 提出ContextFlow框架从空间组学数据推断组织动态轨迹，在多数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 理解发育、再生、疾病进展等过程中组织结构和功能变化的动态，需从纵向空间组学数据推断轨迹。

Method: 提出ContextFlow，将局部组织和配体 - 受体通信模式整合到过渡似然矩阵以正则化最优传输目标。

Result: 在三个数据集上，ContextFlow在多个定量和定性指标上优于现有流匹配方法。

Conclusion: ContextFlow是一个可推广的从纵向空间组学数据建模时空动态的框架。

Abstract: Inferring trajectories from longitudinal spatially-resolved omics data is
fundamental to understanding the dynamics of structural and functional tissue
changes in development, regeneration and repair, disease progression, and
response to treatment. We propose ContextFlow, a novel context-aware flow
matching framework that incorporates prior knowledge to guide the inference of
structural tissue dynamics from spatially resolved omics data. Specifically,
ContextFlow integrates local tissue organization and ligand-receptor
communication patterns into a transition plausibility matrix that regularizes
the optimal transport objective. By embedding these contextual constraints,
ContextFlow generates trajectories that are not only statistically consistent
but also biologically meaningful, making it a generalizable framework for
modeling spatiotemporal dynamics from longitudinal, spatially resolved omics
data. Evaluated on three datasets, ContextFlow consistently outperforms
state-of-the-art flow matching methods across multiple quantitative and
qualitative metrics of inference accuracy and biological coherence. Our code is
available at: \href{https://github.com/santanurathod/ContextFlow}{ContextFlow}

</details>


### [120] [Confidence and Dispersity as Signals: Unsupervised Model Evaluation and Ranking](https://arxiv.org/abs/2510.02956)
*Weijian Deng,Weijie Tu,Ibrahim Radwan,Mohammad Abu Alsheikh,Stephen Gould,Liang Zheng*

Main category: cs.LG

TL;DR: 本文提出无监督模型评估与排序统一框架，对比多种指标，发现混合指标表现更佳，核范数指标性能稳健。


<details>
  <summary>Details</summary>
Motivation: 在分布偏移且无标签测试数据情况下，对模型泛化能力评估至关重要，需有效无监督评估方法。

Method: 提出统一框架用于两种常见部署场景，利用模型预测的置信度和分散度，系统对比基于置信度、分散度和混合指标。

Result: 混合指标在两种评估场景中始终优于单一方面指标，预测矩阵核范数在各任务中性能稳健。

Conclusion: 研究为部署场景下无监督模型评估提供实用且通用基础。

Abstract: Assessing model generalization under distribution shift is essential for
real-world deployment, particularly when labeled test data is unavailable. This
paper presents a unified and practical framework for unsupervised model
evaluation and ranking in two common deployment settings: (1) estimating the
accuracy of a fixed model on multiple unlabeled test sets (dataset-centric
evaluation), and (2) ranking a set of candidate models on a single unlabeled
test set (model-centric evaluation). We demonstrate that two intrinsic
properties of model predictions, namely confidence (which reflects prediction
certainty) and dispersity (which captures the diversity of predicted classes),
together provide strong and complementary signals for generalization. We
systematically benchmark a set of confidence-based, dispersity-based, and
hybrid metrics across a wide range of model architectures, datasets, and
distribution shift types. Our results show that hybrid metrics consistently
outperform single-aspect metrics on both dataset-centric and model-centric
evaluation settings. In particular, the nuclear norm of the prediction matrix
provides robust and accurate performance across tasks, including real-world
datasets, and maintains reliability under moderate class imbalance. These
findings offer a practical and generalizable basis for unsupervised model
assessment in deployment scenarios.

</details>


### [121] [From high-frequency sensors to noon reports: Using transfer learning for shaft power prediction in maritime](https://arxiv.org/abs/2510.03003)
*Akriti Sharma,Dogan Altan,Dusica Marijan,Arnbjørn Maressa*

Main category: cs.LG

TL;DR: 本文提出基于迁移学习的船舶轴功率预测方法，在不同类型船舶上测试，相比仅用中午报告数据训练的模型，误差率降低。


<details>
  <summary>Details</summary>
Motivation: 全球海运增长，准确预测轴功率对优化船舶性能至关重要，但获取高质量传感器数据困难且成本高，需寻找替代数据来源。

Method: 提出基于迁移学习的方法，先在一艘船的高频数据上训练模型，再用其他船的低频每日中午报告数据微调。

Result: 在姊妹船、类似船和不同船的实验中，平均绝对百分比误差分别降低10.6%、3.6%和5.3%。

Conclusion: 基于迁移学习的船舶轴功率预测方法有效，能降低不同类型船舶的预测误差。

Abstract: With the growth of global maritime transportation, energy optimization has
become crucial for reducing costs and ensuring operational efficiency. Shaft
power is the mechanical power transmitted from the engine to the shaft and
directly impacts fuel consumption, making its accurate prediction a paramount
step in optimizing vessel performance. Power consumption is highly correlated
with ship parameters such as speed and shaft rotation per minute, as well as
weather and sea conditions. Frequent access to this operational data can
improve prediction accuracy. However, obtaining high-quality sensor data is
often infeasible and costly, making alternative sources such as noon reports a
viable option. In this paper, we propose a transfer learning-based approach for
predicting vessels shaft power, where a model is initially trained on
high-frequency data from a vessel and then fine-tuned with low-frequency daily
noon reports from other vessels. We tested our approach on sister vessels
(identical dimensions and configurations), a similar vessel (slightly larger
with a different engine), and a different vessel (distinct dimensions and
configurations). The experiments showed that the mean absolute percentage error
decreased by 10.6 percent for sister vessels, 3.6 percent for a similar vessel,
and 5.3 percent for a different vessel, compared to the model trained solely on
noon report data.

</details>


### [122] [BrainIB++: Leveraging Graph Neural Networks and Information Bottleneck for Functional Brain Biomarkers in Schizophrenia](https://arxiv.org/abs/2510.03004)
*Tianzheng Hu,Qiang Li,Shu Liu,Vince D. Calhoun,Guido van Wingen,Shujian Yu*

Main category: cs.LG

TL;DR: 提出创新图神经网络框架BrainIB++用于精神疾病诊断，性能优且可解释。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习诊断模型依赖特征工程有偏差，深度学习模型缺乏可解释性，限制临床应用。

Method: 引入端到端的图神经网络框架BrainIB++，运用信息瓶颈原理识别最具信息的数据驱动脑区作为子图用于解释。

Result: 在三个多队列精神分裂症数据集上，模型诊断准确性优于九种已建立的脑网络分类方法，对未见数据有泛化性，识别的子图与临床生物标志物相符。

Conclusion: BrainIB++模型具有良好性能和可解释性，适用于现实诊断应用。

Abstract: The development of diagnostic models is gaining traction in the field of
psychiatric disorders. Recently, machine learning classifiers based on
resting-state functional magnetic resonance imaging (rs-fMRI) have been
developed to identify brain biomarkers that differentiate psychiatric disorders
from healthy controls. However, conventional machine learning-based diagnostic
models often depend on extensive feature engineering, which introduces bias
through manual intervention. While deep learning models are expected to operate
without manual involvement, their lack of interpretability poses significant
challenges in obtaining explainable and reliable brain biomarkers to support
diagnostic decisions, ultimately limiting their clinical applicability. In this
study, we introduce an end-to-end innovative graph neural network framework
named BrainIB++, which applies the information bottleneck (IB) principle to
identify the most informative data-driven brain regions as subgraphs during
model training for interpretation. We evaluate the performance of our model
against nine established brain network classification methods across three
multi-cohort schizophrenia datasets. It consistently demonstrates superior
diagnostic accuracy and exhibits generalizability to unseen data. Furthermore,
the subgraphs identified by our model also correspond with established clinical
biomarkers in schizophrenia, particularly emphasizing abnormalities in the
visual, sensorimotor, and higher cognition brain functional network. This
alignment enhances the model's interpretability and underscores its relevance
for real-world diagnostic applications.

</details>


### [123] [Distributional Inverse Reinforcement Learning](https://arxiv.org/abs/2510.03013)
*Feiyang Wu,Ye Zhao,Anqi Wu*

Main category: cs.LG

TL;DR: 提出离线逆强化学习分布框架，能捕获专家行为结构，实证表明可恢复奖励表示并实现先进模仿性能。


<details>
  <summary>Details</summary>
Motivation: 传统逆强化学习方法只能恢复确定性奖励估计或匹配期望回报，无法捕获专家行为丰富结构，本文旨在解决此问题。

Method: 通过最小化一阶随机占优（FSD）违规并将失真风险度量（DRMs）集成到策略学习中，捕获专家行为丰富结构。

Result: 在合成基准、真实世界神经行为数据和MuJoCo控制任务上的实证结果表明，该方法能恢复有表现力的奖励表示并实现先进的模仿性能。

Conclusion: 所提的分布框架适用于行为分析和风险感知模仿学习。

Abstract: We propose a distributional framework for offline Inverse Reinforcement
Learning (IRL) that jointly models uncertainty over reward functions and full
distributions of returns. Unlike conventional IRL approaches that recover a
deterministic reward estimate or match only expected returns, our method
captures richer structure in expert behavior, particularly in learning the
reward distribution, by minimizing first-order stochastic dominance (FSD)
violations and thus integrating distortion risk measures (DRMs) into policy
learning, enabling the recovery of both reward distributions and
distribution-aware policies. This formulation is well-suited for behavior
analysis and risk-aware imitation learning. Empirical results on synthetic
benchmarks, real-world neurobehavioral data, and MuJoCo control tasks
demonstrate that our method recovers expressive reward representations and
achieves state-of-the-art imitation performance.

</details>


### [124] [Learning Robust Diffusion Models from Imprecise Supervision](https://arxiv.org/abs/2510.03016)
*Dong-Dong Wu,Jiacheng Cui,Wei Wang,Zhiqiang She,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 提出DMIS框架解决条件扩散模型在不精确监督下的训练问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 条件扩散模型训练依赖含不精确信息的大规模数据集，不精确监督会导致条件不匹配和生成质量下降。

Method: 从似然最大化推导，将目标分解为生成和分类组件，生成组件建模不精确标签分布，分类组件用扩散分类器推断类别后验概率，采用优化时间步采样策略提高效率。

Result: 在多种不精确监督形式的实验中，DMIS能持续生成高质量且具有类别区分性的样本。

Conclusion: DMIS是首个在扩散模型内进行的系统研究，能有效解决不精确监督下扩散模型的训练问题。

Abstract: Conditional diffusion models have achieved remarkable success in various
generative tasks recently, but their training typically relies on large-scale
datasets that inevitably contain imprecise information in conditional inputs.
Such supervision, often stemming from noisy, ambiguous, or incomplete labels,
will cause condition mismatch and degrade generation quality. To address this
challenge, we propose DMIS, a unified framework for training robust Diffusion
Models from Imprecise Supervision, which is the first systematic study within
diffusion models. Our framework is derived from likelihood maximization and
decomposes the objective into generative and classification components: the
generative component models imprecise-label distributions, while the
classification component leverages a diffusion classifier to infer
class-posterior probabilities, with its efficiency further improved by an
optimized timestep sampling strategy. Extensive experiments on diverse forms of
imprecise supervision, covering tasks of image generation, weakly supervised
learning, and noisy dataset condensation demonstrate that DMIS consistently
produces high-quality and class-discriminative samples.

</details>


### [125] [Lightweight Transformer for EEG Classification via Balanced Signed Graph Algorithm Unrolling](https://arxiv.org/abs/2510.03027)
*Junyi Yao,Parham Eftekhar,Gene Cheung,Xujin Chris Liu,Yao Wang,Wei Hu*

Main category: cs.LG

TL;DR: 本文利用平衡带符号图的谱去噪算法构建轻量级可解释的类Transformer神经网络，对EEG信号进行癫痫患者和健康人的分类，实验表明该方法参数少且性能与代表性深度学习方案相当。


<details>
  <summary>Details</summary>
Motivation: 利用收集的EEG信号区分癫痫患者和健康人。

Method: 通过展开平衡带符号图的谱去噪算法构建轻量级可解释的类Transformer神经网络，在映射的正图上通过Lanczos近似实现理想低通滤波器，学习最优截止频率，根据两个平衡带符号图去噪器的重建误差进行二元分类。

Result: 该方法实现了与代表性深度学习方案相当的分类性能，且使用的参数大幅减少。

Conclusion: 所提出的方法在EEG信号分类中具有有效性和优势，参数少且性能好。

Abstract: Samples of brain signals collected by EEG sensors have inherent
anti-correlations that are well modeled by negative edges in a finite graph. To
differentiate epilepsy patients from healthy subjects using collected EEG
signals, we build lightweight and interpretable transformer-like neural nets by
unrolling a spectral denoising algorithm for signals on a balanced signed graph
-- graph with no cycles of odd number of negative edges. A balanced signed
graph has well-defined frequencies that map to a corresponding positive graph
via similarity transform of the graph Laplacian matrices. We implement an ideal
low-pass filter efficiently on the mapped positive graph via Lanczos
approximation, where the optimal cutoff frequency is learned from data. Given
that two balanced signed graph denoisers learn posterior probabilities of two
different signal classes during training, we evaluate their reconstruction
errors for binary classification of EEG signals. Experiments show that our
method achieves classification performance comparable to representative deep
learning schemes, while employing dramatically fewer parameters.

</details>


### [126] [Bayesian E(3)-Equivariant Interatomic Potential with Iterative Restratification of Many-body Message Passing](https://arxiv.org/abs/2510.03046)
*Soohaeng Yoo Willow,Tae Hyeon Park,Gi Beom Sim,Sung Wook Moon,Seung Kyu Min,D. ChangMo Yang,Hyun Woo Kim,Juho Lee,Chang Woo Myung*

Main category: cs.LG

TL;DR: 本文开发贝叶斯E(3)等变MLP解决当前MLP不确定性量化难题，通过新损失函数、系统基准测试等实现不确定性引导的主动学习等任务。


<details>
  <summary>Details</summary>
Motivation: 当前MLP在不确定性量化方面存在困难，限制了其在主动学习、校准和分布外检测等方面的可靠性。

Method: 开发贝叶斯E(3)等变MLP，引入联合能量 - 力负对数似然损失函数，系统地对多种贝叶斯方法进行基准测试。

Result: 贝叶斯MLP与最先进模型精度相当，能实现不确定性引导的主动学习、分布外检测和能量/力校准，基于BALD的框架表现优于随机采样等。

Conclusion: 贝叶斯等变神经网络是开发大规模原子模拟中具有不确定性感知的MLP的强大框架。

Abstract: Machine learning potentials (MLPs) have become essential for large-scale
atomistic simulations, enabling ab initio-level accuracy with computational
efficiency. However, current MLPs struggle with uncertainty quantification,
limiting their reliability for active learning, calibration, and
out-of-distribution (OOD) detection. We address these challenges by developing
Bayesian E(3) equivariant MLPs with iterative restratification of many-body
message passing. Our approach introduces the joint energy-force negative
log-likelihood (NLL$_\text{JEF}$) loss function, which explicitly models
uncertainty in both energies and interatomic forces, yielding superior accuracy
compared to conventional NLL losses. We systematically benchmark multiple
Bayesian approaches, including deep ensembles with mean-variance estimation,
stochastic weight averaging Gaussian, improved variational online Newton, and
laplace approximation by evaluating their performance on uncertainty
prediction, OOD detection, calibration, and active learning tasks. We further
demonstrate that NLL$_\text{JEF}$ facilitates efficient active learning by
quantifying energy and force uncertainties. Using Bayesian active learning by
disagreement (BALD), our framework outperforms random sampling and
energy-uncertainty-based sampling. Our results demonstrate that Bayesian MLPs
achieve competitive accuracy with state-of-the-art models while enabling
uncertainty-guided active learning, OOD detection, and energy/forces
calibration. This work establishes Bayesian equivariant neural networks as a
powerful framework for developing uncertainty-aware MLPs for atomistic
simulations at scale.

</details>


### [127] [ZeroShotOpt: Towards Zero-Shot Pretrained Models for Efficient Black-Box Optimization](https://arxiv.org/abs/2510.03051)
*Jamison Meindl,Yunsheng Tian,Tony Cui,Veronika Thost,Zhang-Wei Hong,Johannes Dürholt,Jie Chen,Wojciech Matusik,Mina Konaković Luković*

Main category: cs.LG

TL;DR: 提出ZeroShotOpt用于连续黑盒优化任务，利用离线强化学习和合成函数预训练，实现零样本泛化，样本效率高。


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯优化（BO）性能依赖超参数且难以跨问题泛化，需要更高效的全局优化方法。

Method: 在12种BO变体收集的大规模优化轨迹上进行离线强化学习，生成数百万具有不同景观的合成高斯过程函数进行预训练。

Result: ZeroShotOpt在众多未见基准测试中实现稳健的零样本泛化，样本效率匹配或超越包括BO在内的领先全局优化器。

Conclusion: ZeroShotOpt是一种通用的预训练模型，为连续黑盒优化提供高效解决方案，且为未来扩展和改进提供基础。

Abstract: Global optimization of expensive, derivative-free black-box functions
requires extreme sample efficiency. While Bayesian optimization (BO) is the
current state-of-the-art, its performance hinges on surrogate and acquisition
function hyper-parameters that are often hand-tuned and fail to generalize
across problem landscapes. We present ZeroShotOpt, a general-purpose,
pretrained model for continuous black-box optimization tasks ranging from 2D to
20D. Our approach leverages offline reinforcement learning on large-scale
optimization trajectories collected from 12 BO variants. To scale pretraining,
we generate millions of synthetic Gaussian process-based functions with diverse
landscapes, enabling the model to learn transferable optimization policies. As
a result, ZeroShotOpt achieves robust zero-shot generalization on a wide array
of unseen benchmarks, matching or surpassing the sample efficiency of leading
global optimizers, including BO, while also offering a reusable foundation for
future extensions and improvements. Our open-source code, dataset, and model
are available at: https://github.com/jamisonmeindl/zeroshotopt

</details>


### [128] [Comparative Analysis of Parameterized Action Actor-Critic Reinforcement Learning Algorithms for Web Search Match Plan Generation](https://arxiv.org/abs/2510.03064)
*Ubayd Bapoo,Clement N Nyirenda*

Main category: cs.LG

TL;DR: 研究评估SAC、GAC和TQC在高维决策任务中表现，PAGAC算法表现最佳，未来可探索混合策略。


<details>
  <summary>Details</summary>
Motivation: 评估SAC、GAC和TQC在高维决策任务、参数化动作空间中的性能。

Method: 使用Platform - v0和Goal - v0基准测试，用Microsoft NNI进行超参数优化，修改GAC和TQC代码以确保可重复性。

Result: PAGAC在各基准测试中训练速度最快、回报最高，如在Platform游戏中5000集耗时41:24，Robot Soccer Goal游戏中耗时24:04。

Conclusion: PAGAC在复杂动作空间有明显优势，效率和可靠性优于PASAC和PATQC，适合快速收敛和强性能要求的任务，未来可探索混合策略。

Abstract: This study evaluates the performance of Soft Actor Critic (SAC), Greedy Actor
Critic (GAC), and Truncated Quantile Critics (TQC) in high-dimensional
decision-making tasks using fully observable environments. The focus is on
parametrized action (PA) spaces, eliminating the need for recurrent networks,
with benchmarks Platform-v0 and Goal-v0 testing discrete actions linked to
continuous action-parameter spaces. Hyperparameter optimization was performed
with Microsoft NNI, ensuring reproducibility by modifying the codebase for GAC
and TQC. Results show that Parameterized Action Greedy Actor-Critic (PAGAC)
outperformed other algorithms, achieving the fastest training times and highest
returns across benchmarks, completing 5,000 episodes in 41:24 for the Platform
game and 24:04 for the Robot Soccer Goal game. Its speed and stability provide
clear advantages in complex action spaces. Compared to PASAC and PATQC, PAGAC
demonstrated superior efficiency and reliability, making it ideal for tasks
requiring rapid convergence and robust performance. Future work could explore
hybrid strategies combining entropy-regularization with truncation-based
methods to enhance stability and expand investigations into generalizability.

</details>


### [129] [A Unified Deep Reinforcement Learning Approach for Close Enough Traveling Salesman Problem](https://arxiv.org/abs/2510.03065)
*Mingfeng Fan,Jiaqi Cheng,Yaoxin Wu,Yifeng Zhang,Yibin Yang,Guohua Wu,Guillaume Sartoretti*

Main category: cs.LG

TL;DR: 提出统一双解码器深度强化学习框架UD3RL解决CETSP，实验显示其优于传统方法，泛化性和鲁棒性强。


<details>
  <summary>Details</summary>
Motivation: 现有研究对CETSP关注有限，其基于邻域的访问准则带来挑战。

Method: 用离散化方案为CETSP构建MDP，提出UD3RL框架，分离决策为节点选择和航点确定，采用适应编码器、节点解码器和位置解码器，引入k近邻子图交互策略，定制REINFORCE算法训练。

Result: UD3RL在解质量和运行时间上优于传统方法，在不同问题规模、空间分布、半径范围有强泛化性，对动态环境有鲁棒性。

Conclusion: UD3RL是解决CETSP的有效方法，具有良好性能和泛化能力。

Abstract: In recent years, deep reinforcement learning (DRL) has gained traction for
solving the NP-hard traveling salesman problem (TSP). However, limited
attention has been given to the close-enough TSP (CETSP), primarily due to the
challenge introduced by its neighborhood-based visitation criterion, wherein a
node is considered visited if the agent enters a compact neighborhood around
it. In this work, we formulate a Markov decision process (MDP) for CETSP using
a discretization scheme and propose a novel unified dual-decoder DRL (UD3RL)
framework that separates decision-making into node selection and waypoint
determination. Specifically, an adapted encoder is employed for effective
feature extraction, followed by a node-decoder and a loc-decoder to handle the
two sub-tasks, respectively. A k-nearest neighbors subgraph interaction
strategy is further introduced to enhance spatial reasoning during location
decoding. Furthermore, we customize the REINFORCE algorithm to train UD3RL as a
unified model capable of generalizing across different problem sizes and
varying neighborhood radius types (i.e., constant and random radii).
Experimental results show that UD3RL outperforms conventional methods in both
solution quality and runtime, while exhibiting strong generalization across
problem scales, spatial distributions, and radius ranges, as well as robustness
to dynamic environments.

</details>


### [130] [Bootstrap Learning for Combinatorial Graph Alignment with Sequential GNNs](https://arxiv.org/abs/2510.03086)
*Marc Lelarge*

Main category: cs.LG

TL;DR: 提出图对齐问题的链式GNN方法，在合成基准测试中表现优于现有方法，结合传统优化后效果更佳。


<details>
  <summary>Details</summary>
Motivation: 解决图神经网络在组合问题上难以超越传统优化方法的问题，提升其在图对齐问题上的表现。

Method: 训练一系列GNN，每个网络迭代优化前一个网络生成的相似度矩阵，结合处理节点对的架构。

Result: 在合成基准测试中，链式GNN在困难实例上准确率比现有方法高3倍多，能解决其他方法失败的规则图问题，结合传统优化后大幅超越现有求解器。

Conclusion: 所提出的链式GNN方法在图对齐问题上效果显著，有较大实用价值。

Abstract: Graph neural networks (GNNs) have struggled to outperform traditional
optimization methods on combinatorial problems, limiting their practical
impact. We address this gap by introducing a novel chaining procedure for the
graph alignment problem, a fundamental NP-hard task of finding optimal node
correspondences between unlabeled graphs using only structural information. Our
method trains a sequence of GNNs where each network learns to iteratively
refine similarity matrices produced by previous networks. During inference,
this creates a bootstrap effect: each GNN improves upon partial solutions by
incorporating discrete ranking information about node alignment quality from
prior iterations. We combine this with a powerful architecture that operates on
node pairs rather than individual nodes, capturing global structural patterns
essential for alignment that standard message-passing networks cannot
represent. Extensive experiments on synthetic benchmarks demonstrate
substantial improvements: our chained GNNs achieve over 3x better accuracy than
existing methods on challenging instances, and uniquely solve regular graphs
where all competing approaches fail. When combined with traditional
optimization as post-processing, our method substantially outperforms
state-of-the-art solvers on the graph alignment benchmark.

</details>


### [131] [Adaptive Node Feature Selection For Graph Neural Networks](https://arxiv.org/abs/2510.03096)
*Ali Azizpour,Madeline Navarro,Santiago Segarra*

Main category: cs.LG

TL;DR: 提出一种图神经网络自适应节点特征选择方法，在训练中识别并移除不必要特征，实验验证了其灵活性和适应性。


<details>
  <summary>Details</summary>
Motivation: 图结构数据的复杂依赖使经典特征重要性度量方法不适用，需新方法来衡量特征对模型输出的贡献。

Method: 提出一种与模型和任务无关的方法，基于特征值置换后验证性能的变化确定相关特征，并从理论上阐释该干预方法。

Result: 不仅能得出特征重要性分数，还能跟踪特征相关性变化；实证结果验证了该方法对不同图架构的灵活性和对更具挑战性图学习设置的适应性。

Conclusion: 所提出的自适应节点特征选择方法可行，具有灵活性和适应性。

Abstract: We propose an adaptive node feature selection approach for graph neural
networks (GNNs) that identifies and removes unnecessary features during
training. The ability to measure how features contribute to model output is key
for interpreting decisions, reducing dimensionality, and even improving
performance by eliminating unhelpful variables. However, graph-structured data
introduces complex dependencies that may not be amenable to classical feature
importance metrics. Inspired by this challenge, we present a model- and
task-agnostic method that determines relevant features during training based on
changes in validation performance upon permuting feature values. We
theoretically motivate our intervention-based approach by characterizing how
GNN performance depends on the relationships between node data and graph
structure. Not only do we return feature importance scores once training
concludes, we also track how relevance evolves as features are successively
dropped. We can therefore monitor if features are eliminated effectively and
also evaluate other metrics with this technique. Our empirical results verify
the flexibility of our approach to different graph architectures as well as its
adaptability to more challenging graph learning settings.

</details>


### [132] [AdaBet: Gradient-free Layer Selection for Efficient Training of Deep Neural Networks](https://arxiv.org/abs/2510.03101)
*Irene Tenison,Soumyajit Chatterjee,Fahim Kawsar,Mohammad Malekzadeh*

Main category: cs.LG

TL;DR: 介绍AdaBet无梯度层选择方法，评估显示在十六对基准模型和数据集上比基于梯度的基线平均分类准确率高5%，平均峰值内存消耗降低40%。


<details>
  <summary>Details</summary>
Motivation: 在边缘和移动设备上利用预训练神经网络，需有效适应特定运行时数据分布，但现有设备端再训练方法因神经网络深度和计算开销大而不实用，现有层选择方法有局限性。

Method: 提出AdaBet，通过Betti数分析激活空间拓扑特征，仅使用前向传播对重要层进行排名。

Result: 在十六对基准模型和数据集上评估，AdaBet比基于梯度的基线平均分类准确率高5%，平均峰值内存消耗降低40%。

Conclusion: AdaBet能在不使用标签或梯度的情况下选择对再训练和适应重要的层，且效果和内存消耗表现良好。

Abstract: To utilize pre-trained neural networks on edge and mobile devices, we often
require efficient adaptation to user-specific runtime data distributions while
operating under limited compute and memory resources. On-device retraining with
a target dataset can facilitate such adaptations; however, it remains
impractical due to the increasing depth of modern neural nets, as well as the
computational overhead associated with gradient-based optimization across all
layers. Current approaches reduce training cost by selecting a subset of layers
for retraining, however, they rely on labeled data, at least one full-model
backpropagation, or server-side meta-training; limiting their suitability for
constrained devices. We introduce AdaBet, a gradient-free layer selection
approach to rank important layers by analyzing topological features of their
activation spaces through Betti Numbers and using forward passes alone. AdaBet
allows selecting layers with high learning capacity, which are important for
retraining and adaptation, without requiring labels or gradients. Evaluating
AdaBet on sixteen pairs of benchmark models and datasets, shows AdaBet achieves
an average gain of 5% more classification accuracy over gradient-based
baselines while reducing average peak memory consumption by 40%.

</details>


### [133] [Real Time Headway Predictions in Urban Rail Systems and Implications for Service Control: A Deep Learning Approach](https://arxiv.org/abs/2510.03121)
*Muhammad Usama,Haris Koutsopoulos*

Main category: cs.LG

TL;DR: 本文提出基于ConvLSTM的深度学习框架预测地铁列车间隔，评估控制决策，经评估有良好效果，可优化调度策略。


<details>
  <summary>Details</summary>
Motivation: 实现城市地铁系统高效实时调度，确保服务可靠性、提高资源利用率和乘客满意度。

Method: 提出以ConvLSTM模型为核心的深度学习框架，结合计划终端间隔和历史间隔数据预测列车间隔，引入灵活方法模拟调度策略。

Result: 在城市地铁线路大规模数据集上评估，ConvLSTM模型展现出良好的列车间隔预测效果。

Conclusion: 该框架为铁路运营商提供高效工具，可优化调度策略，提升服务一致性和乘客满意度。

Abstract: Efficient real-time dispatching in urban metro systems is essential for
ensuring service reliability, maximizing resource utilization, and improving
passenger satisfaction. This study presents a novel deep learning framework
centered on a Convolutional Long Short-Term Memory (ConvLSTM) model designed to
predict the complex spatiotemporal propagation of train headways across an
entire metro line. By directly incorporating planned terminal headways as a
critical input alongside historical headway data, the proposed model accurately
forecasts future headway dynamics, effectively capturing both their temporal
evolution and spatial dependencies across all stations. This capability
empowers dispatchers to evaluate the impact of various terminal headway control
decisions without resorting to computationally intensive simulations. We
introduce a flexible methodology to simulate diverse dispatcher strategies,
ranging from maintaining even headways to implementing custom patterns derived
from observed terminal departures. In contrast to existing research primarily
focused on passenger load predictioning or atypical disruption scenarios, our
approach emphasizes proactive operational control. Evaluated on a large-scale
dataset from an urban metro line, the proposed ConvLSTM model demonstrates
promising headway predictions, offering actionable insights for real-time
decision-making. This framework provides rail operators with a powerful,
computationally efficient tool to optimize dispatching strategies, thereby
significantly improving service consistency and passenger satisfaction.

</details>


### [134] [Enhancing XAI Narratives through Multi-Narrative Refinement and Knowledge Distillation](https://arxiv.org/abs/2510.03134)
*Flavio Giorgi,Matteo Silvestri,Cesare Campagnano,Fabrizio Silvestri,Gabriele Tolomei*

Main category: cs.LG

TL;DR: 提出利用大小语言模型为反事实解释编写叙述的新管道，提升学生模型性能。


<details>
  <summary>Details</summary>
Motivation: 可解释人工智能中反事实解释复杂难被非专家理解，需解决此问题。

Method: 提出利用大小语言模型的新管道，采用知识蒸馏和精炼机制，引入评估方法。

Result: 提升了学生模型的推理能力和实际性能。

Conclusion: 所提管道使学生模型更适用于实际场景。

Abstract: Explainable Artificial Intelligence has become a crucial area of research,
aiming to demystify the decision-making processes of deep learning models.
Among various explainability techniques, counterfactual explanations have been
proven particularly promising, as they offer insights into model behavior by
highlighting minimal changes that would alter a prediction. Despite their
potential, these explanations are often complex and technical, making them
difficult for non-experts to interpret. To address this challenge, we propose a
novel pipeline that leverages Language Models, large and small, to compose
narratives for counterfactual explanations. We employ knowledge distillation
techniques along with a refining mechanism to enable Small Language Models to
perform comparably to their larger counterparts while maintaining robust
reasoning abilities. In addition, we introduce a simple but effective
evaluation method to assess natural language narratives, designed to verify
whether the models' responses are in line with the factual, counterfactual
ground truth. As a result, our proposed pipeline enhances both the reasoning
capabilities and practical performance of student models, making them more
suitable for real-world use cases.

</details>


### [135] [Mixture of Many Zero-Compute Experts: A High-Rate Quantization Theory Perspective](https://arxiv.org/abs/2510.03151)
*Yehuda Dar*

Main category: cs.LG

TL;DR: 运用经典高率量化理论研究回归任务的混合专家（MoE）模型，分析近似误差、学习专家参数，探讨近似与估计误差权衡和专家数量的关系。


<details>
  <summary>Details</summary>
Motivation: 利用高率量化理论假设，为回归任务的MoE模型提供新见解。

Method: 基于输入空间分割定义MoE模型，根据高率量化理论假设专家数量足够大；对一维和多维输入分别分析测试误差；考虑从训练数据学习专家参数。

Result: 对一维输入给出测试误差及最小化分割和专家；对多维输入给出测试误差上界并研究其最小化；分析专家参数的统计学习特性。

Conclusion: 理论和实证表明MoE学习中近似和估计误差的权衡取决于专家数量。

Abstract: This paper uses classical high-rate quantization theory to provide new
insights into mixture-of-experts (MoE) models for regression tasks. Our MoE is
defined by a segmentation of the input space to regions, each with a
single-parameter expert that acts as a constant predictor with zero-compute at
inference. Motivated by high-rate quantization theory assumptions, we assume
that the number of experts is sufficiently large to make their input-space
regions very small. This lets us to study the approximation error of our MoE
model class: (i) for one-dimensional inputs, we formulate the test error and
its minimizing segmentation and experts; (ii) for multidimensional inputs, we
formulate an upper bound for the test error and study its minimization.
Moreover, we consider the learning of the expert parameters from a training
dataset, given an input-space segmentation, and formulate their statistical
learning properties. This leads us to theoretically and empirically show how
the tradeoff between approximation and estimation errors in MoE learning
depends on the number of experts.

</details>


### [136] [Calibrated Uncertainty Sampling for Active Learning](https://arxiv.org/abs/2510.03162)
*Ha Manh Bui,Iliana Maifeld-Carucci,Anqi Liu*

Main category: cs.LG

TL;DR: 研究主动学习低校准误差分类器问题，提出新的采集函数，通过估计校准误差选择样本，在实验中表现优于其他基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于模型不确定性的采集函数因未校准的不确定性模型影响效果，导致泛化和校准误差高，尤其是在深度神经网络中问题更严重。

Method: 提出新的采集函数，先估计校准误差，选择校准误差最高的样本，利用核校准误差估计器，在协变量偏移下操作。

Result: 所提出的方法在基于池的主动学习设置中，校准误差和泛化误差均低于其他采集函数基线。

Conclusion: 使用新的采集函数进行主动学习最终能使未标记池和未见测试数据的校准误差有界。

Abstract: We study the problem of actively learning a classifier with a low calibration
error. One of the most popular Acquisition Functions (AFs) in pool-based Active
Learning (AL) is querying by the model's uncertainty. However, we recognize
that an uncalibrated uncertainty model on the unlabeled pool may significantly
affect the AF effectiveness, leading to sub-optimal generalization and high
calibration error on unseen data. Deep Neural Networks (DNNs) make it even
worse as the model uncertainty from DNN is usually uncalibrated. Therefore, we
propose a new AF by estimating calibration errors and query samples with the
highest calibration error before leveraging DNN uncertainty. Specifically, we
utilize a kernel calibration error estimator under the covariate shift and
formally show that AL with this AF eventually leads to a bounded calibration
error on the unlabeled pool and unseen test data. Empirically, our proposed
method surpasses other AF baselines by having a lower calibration and
generalization error across pool-based AL settings.

</details>


### [137] [FTTE: Federated Learning on Resource-Constrained Devices](https://arxiv.org/abs/2510.03165)
*Irene Tenison,Anna Murphy,Charles Beauville,Lalana Kagal*

Main category: cs.LG

TL;DR: 提出FTTE半异步联邦学习框架，实验表明其收敛更快、内存使用和通信负载更低，是实用可扩展的解决方案。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在资源受限边缘节点部署有挑战，传统同步和异步方法在异构大规模网络中存在延迟和收敛慢问题。

Method: 提出FTTE框架，采用稀疏参数更新和基于客户端更新年龄与方差的陈旧度加权聚合。

Result: FTTE比同步FL收敛快81%，设备内存使用低80%，通信负载降低69%，在挑战性场景下比半异步方法达到相当或更高的目标精度。

Conclusion: FTTE是在异构且资源受限边缘设备上进行现实世界联邦学习部署的首个实用可扩展解决方案。

Abstract: Federated learning (FL) enables collaborative model training across
distributed devices while preserving data privacy, but deployment on
resource-constrained edge nodes remains challenging due to limited memory,
energy, and communication bandwidth. Traditional synchronous and asynchronous
FL approaches further suffer from straggler induced delays and slow convergence
in heterogeneous, large scale networks. We present FTTE (Federated Tiny
Training Engine),a novel semi-asynchronous FL framework that uniquely employs
sparse parameter updates and a staleness-weighted aggregation based on both age
and variance of client updates. Extensive experiments across diverse models and
data distributions - including up to 500 clients and 90% stragglers -
demonstrate that FTTE not only achieves 81% faster convergence, 80% lower
on-device memory usage, and 69% communication payload reduction than
synchronous FL (eg.FedAVG), but also consistently reaches comparable or higher
target accuracy than semi-asynchronous (eg.FedBuff) in challenging regimes.
These results establish FTTE as the first practical and scalable solution for
real-world FL deployments on heterogeneous and predominantly
resource-constrained edge devices.

</details>


### [138] [Q-Learning with Shift-Aware Upper Confidence Bound in Non-Stationary Reinforcement Learning](https://arxiv.org/abs/2510.03181)
*Ha Manh Bui,Felix Parker,Kimia Ghobadi,Anqi Liu*

Main category: cs.LG

TL;DR: 研究非平稳强化学习在分布偏移下的问题，提出DQUCB算法，理论证明优于QUCB，实证表现也更好。


<details>
  <summary>Details</summary>
Motivation: QUCB算法在分布偏移后会利用次优奖励，需要解决此问题。

Method: 提出DQUCB算法，用转移密度函数检测分布偏移，利用似然性提升Q - learning UCB的不确定性估计质量。

Result: 理论上，oracle DQUCB有更好的遗憾保证；实证上，DQUCB有计算效率，在强化学习任务和COVID - 19患者医院分配任务中遗憾更低。

Conclusion: DQUCB算法在处理分布偏移的非平稳强化学习问题上优于QUCB。

Abstract: We study the Non-Stationary Reinforcement Learning (RL) under distribution
shifts in both finite-horizon episodic and infinite-horizon discounted Markov
Decision Processes (MDPs). In the finite-horizon case, the transition functions
may suddenly change at a particular episode. In the infinite-horizon setting,
such changes can occur at an arbitrary time step during the agent's interaction
with the environment. While the Q-learning Upper Confidence Bound algorithm
(QUCB) can discover a proper policy during learning, due to the distribution
shifts, this policy can exploit sub-optimal rewards after the shift happens. To
address this issue, we propose Density-QUCB (DQUCB), a shift-aware
Q-learning~UCB algorithm, which uses a transition density function to detect
distribution shifts, then leverages its likelihood to enhance the uncertainty
estimation quality of Q-learning~UCB, resulting in a balance between
exploration and exploitation. Theoretically, we prove that our oracle DQUCB
achieves a better regret guarantee than QUCB. Empirically, our DQUCB enjoys the
computational efficiency of model-free RL and outperforms QUCB baselines by
having a lower regret across RL tasks, as well as a real-world COVID-19 patient
hospital allocation task using a Deep-Q-learning architecture.

</details>


### [139] [PRISM-Physics: Causal DAG-Based Process Evaluation for Physics Reasoning](https://arxiv.org/abs/2510.03185)
*Wanjia Zhao,Qinwei Ma,Jingzhe Shi,Shirley Wu,Jiaqi Han,Yijia Xiao,Si-Yuan Chen,Xiao Luo,Ludwig Schmidt,James Zou*

Main category: cs.LG

TL;DR: 引入PRISM - Physics评估框架和基准用于复杂物理推理问题，能进行细粒度、可解释且理论上合理的评分，结果更符合专家评分，可揭示模型推理问题并为训练提供信号。


<details>
  <summary>Details</summary>
Motivation: 现有物理基准大多只评估最终答案，无法捕捉推理过程，近期逐步方法有可靠性和诊断有效性问题，需要更好的评估方法。

Method: 引入PRISM - Physics，用有向无环图（DAG）表示解决方案，证明DAG表示和评分策略的最优性，结合自主开发的基于规则的符号公式等价匹配方法。

Result: 评估框架更符合人类专家评分，实验揭示了最先进大语言模型在物理推理上的持续失败，步骤级评分能提供诊断见解和训练信号。

Conclusion: PRISM - Physics结合结构严谨性、理论保证和符号验证，为推进过程级评估和引导具有更深科学推理能力的模型发展提供了原则性基础。

Abstract: Benchmarks for competition-style reasoning have advanced evaluation in
mathematics and programming, yet physics remains comparatively explored. Most
existing physics benchmarks evaluate only final answers, which fail to capture
reasoning processes, while recent stepwise methods rely on heuristic
LLM-as-judge scoring or restrictive linear assumptions, limiting reliability
and diagnostic validity. We introduce PRISM-Physics, a process-level evaluation
framework and benchmark for complex physics reasoning problems. Solutions are
represented as directed acyclic graphs (DAGs) of formulas, explicitly encoding
causal dependencies among intermediate steps to enable fine-grained,
interpretable, and theoretically grounded scoring. We prove the optimality of
the DAG representation and the corresponding scoring policy. Combining with a
fully rule-based method for symbolic formula equivalence matching that we
developed, we ensure consistent validation across diverse formulations without
heuristic judgments. Results show that our evaluation framework is more aligned
with human experts' scoring. Experiments on state-of-the-art LLMs reveal
persistent reasoning failures in physics, while step-level scoring offers both
diagnostic insight and rich signals for later training. By combining structural
rigor, theoretical guarantees, and symbolic validation, PRISM-Physics provides
a principled foundation for advancing process-level evaluation and guiding the
development of models with deeper scientific reasoning capabilities.

</details>


### [140] [Superposition disentanglement of neural representations reveals hidden alignment](https://arxiv.org/abs/2510.03186)
*André Longon,David Klindt,Meenakshi Khosla*

Main category: cs.LG

TL;DR: 研究叠加与对齐指标的相互作用，发现解纠缠叠加对揭示神经代码间真实表征对齐必要。


<details>
  <summary>Details</summary>
Motivation: 探索叠加是否会以不良方式与对齐指标相互作用。

Method: 先建立严格排列指标依赖叠加安排的理论，通过训练稀疏自动编码器在玩具模型中解纠缠叠加进行测试。

Result: 在玩具模型中用稀疏过完备潜在代码替换基础神经元时对齐分数增加，在视觉领域的DNN→DNN和DNN→大脑线性回归对齐中也有类似增加。

Conclusion: 叠加解纠缠对映射指标揭示神经代码间真实表征对齐是必要的。

Abstract: The superposition hypothesis states that a single neuron within a population
may participate in the representation of multiple features in order for the
population to represent more features than the number of neurons. In
neuroscience and AI, representational alignment metrics measure the extent to
which different deep neural networks (DNNs) or brains represent similar
information. In this work, we explore a critical question: \textit{does
superposition interact with alignment metrics in any undesirable way?} We
hypothesize that models which represent the same features in \textit{different
superposition arrangements}, i.e., their neurons have different linear
combinations of the features, will interfere with predictive mapping metrics
(semi-matching, soft-matching, linear regression), producing lower alignment
than expected. We first develop a theory for how the strict permutation metrics
are dependent on superposition arrangements. This is tested by training sparse
autoencoders (SAEs) to disentangle superposition in toy models, where alignment
scores are shown to typically increase when a model's base neurons are replaced
with its sparse overcomplete latent codes. We find similar increases for
DNN\(\rightarrow\)DNN and DNN\(\rightarrow\)brain linear regression alignment
in the visual domain. Our results suggest that superposition disentanglement is
necessary for mapping metrics to uncover the true representational alignment
between neural codes.

</details>


### [141] [Estimation of Resistance Training RPE using Inertial Sensors and Electromyography](https://arxiv.org/abs/2510.03197)
*James Thomas,Johan Wahlström*

Main category: cs.LG

TL;DR: 研究用机器学习模型基于可穿戴传感器数据估算单臂哑铃二头肌弯举时的RPE，随机森林分类器表现最佳，证明了可行性并指出挑战。


<details>
  <summary>Details</summary>
Motivation: 准确估算RPE可通过个性化反馈和预防损伤来增强阻力训练。

Method: 收集69组超1000次重复的自定义数据集，提取统计特征训练模型，评估多种机器学习模型。

Result: 随机森林分类器表现最佳，精确准确率41.4%，±1 RPE准确率85.9%；加入EMG数据略提高准确率；偏心重复时间是最强RPE预测指标。

Conclusion: 证明了基于可穿戴传感器估算RPE的可行性，指出提高模型泛化性的关键挑战。

Abstract: Accurate estimation of rating of perceived exertion (RPE) can enhance
resistance training through personalized feedback and injury prevention. This
study investigates the application of machine learning models to estimate RPE
during single-arm dumbbell bicep curls, using data from wearable inertial and
electromyography (EMG) sensors. A custom dataset of 69 sets and over 1000
repetitions was collected, with statistical features extracted for model
training. Among the models evaluated, a random forest classifier achieved the
highest performance, with 41.4% exact accuracy and 85.9% $\pm1$ RPE accuracy.
While the inclusion of EMG data slightly improved model accuracy over inertial
sensors alone, its utility may have been limited by factors such as data
quality and placement sensitivity. Feature analysis highlighted eccentric
repetition time as the strongest RPE predictor. The results demonstrate the
feasibility of wearable-sensor-based RPE estimation and identify key challenges
for improving model generalizability.

</details>


### [142] [To Distill or Decide? Understanding the Algorithmic Trade-off in Partially Observable Reinforcement Learning](https://arxiv.org/abs/2510.03207)
*Yuda Song,Dhruv Rohatgi,Aarti Singh,J. Andrew Bagnell*

Main category: cs.LG

TL;DR: 本文通过理论模型和实验，研究特权专家蒸馏与无特权信息标准强化学习的算法权衡，发现权衡取决于潜在动态随机性，且最优潜在策略并非总是最佳蒸馏策略，为利用特权信息提供新指南。


<details>
  <summary>Details</summary>
Motivation: 部分可观测性是强化学习的难题，特权专家蒸馏虽有效但有失败模式，因此研究其与无特权信息标准强化学习的算法权衡。

Method: 使用名为扰动块马尔可夫决策过程的理论模型和模拟运动任务的控制实验。

Result: 1. 权衡取决于潜在动态的随机性；2. 最优潜在策略并非总是最佳蒸馏策略。

Conclusion: 研究结果为有效利用特权信息提供新指南，有望提高许多实际部分可观测领域的策略学习效率。

Abstract: Partial observability is a notorious challenge in reinforcement learning
(RL), due to the need to learn complex, history-dependent policies. Recent
empirical successes have used privileged expert distillation--which leverages
availability of latent state information during training (e.g., from a
simulator) to learn and imitate the optimal latent, Markovian policy--to
disentangle the task of "learning to see" from "learning to act". While expert
distillation is more computationally efficient than RL without latent state
information, it also has well-documented failure modes. In this paper--through
a simple but instructive theoretical model called the perturbed Block MDP, and
controlled experiments on challenging simulated locomotion tasks--we
investigate the algorithmic trade-off between privileged expert distillation
and standard RL without privileged information. Our main findings are: (1) The
trade-off empirically hinges on the stochasticity of the latent dynamics, as
theoretically predicted by contrasting approximate decodability with belief
contraction in the perturbed Block MDP; and (2) The optimal latent policy is
not always the best latent policy to distill. Our results suggest new
guidelines for effectively exploiting privileged information, potentially
advancing the efficiency of policy learning across many practical partially
observable domains.

</details>


### [143] [Low-probability Tokens Sustain Exploration in Reinforcement Learning with Verifiable Reward](https://arxiv.org/abs/2510.03222)
*Guanhua Huang,Tingqiang Xu,Mingze Wang,Qi Yi,Xue Gong,Siheng Li,Ruibin Xiong,Kejiao Li,Yuhao Jiang,Bo Zhou*

Main category: cs.LG

TL;DR: 本文指出RLVR中探索性低概率标记被消除问题，提出Lp - Reg方法，实验显示该方法能稳定训练并提升数学基准测试准确率。


<details>
  <summary>Details</summary>
Motivation: RLVR存在训练瓶颈，之前维持高策略熵方法未充分探索有效探索机制，且单纯关注熵有风险。

Method: 引入Low - probability Regularization (Lp - Reg)，将策略正则化为启发式代理分布，通过过滤噪声标记并重新归一化，利用KL散度保护有价值标记。

Result: Lp - Reg能稳定进行约1000步在线策略训练，在五个数学基准测试上平均准确率达60.17%，比先前方法提高2.66%。

Conclusion: Lp - Reg解决了RLVR中探索性标记被消除问题，实现稳定训练并取得了最优性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has propelled Large
Language Models in complex reasoning, yet its scalability is often hindered by
a training bottleneck where performance plateaus as policy entropy collapses,
signaling a loss of exploration. Previous methods typically address this by
maintaining high policy entropy, yet the precise mechanisms that govern
meaningful exploration have remained underexplored. Our analysis suggests that
an unselective focus on entropy risks amplifying irrelevant tokens and
destabilizing training. This paper investigates the exploration dynamics within
RLVR and identifies a key issue: the gradual elimination of valuable
low-probability exploratory tokens, which we term \textbf{\textit{reasoning
sparks}}. We find that while abundant in pre-trained models, these sparks are
systematically extinguished during RLVR due to over-penalization, leading to a
degeneracy in exploration. To address this, we introduce Low-probability
Regularization (Lp-Reg). Its core mechanism regularizes the policy towards a
heuristic proxy distribution. This proxy is constructed by filtering out
presumed noise tokens and re-normalizing the distribution over the remaining
candidates. The result is a less-noisy proxy where the probability of
\textit{reasoning sparks} is amplified, which then serves as a soft
regularization target to shield these valuable tokens from elimination via KL
divergence. Experiments show that Lp-Reg enables stable on-policy training for
around 1,000 steps, a regime where baseline entropy-control methods collapse.
This sustained exploration leads to state-of-the-art performance, achieving a
$60.17\%$ average accuracy on five math benchmarks, an improvement of $2.66\%$
over prior methods. Code is available at https://github.com/CarlanLark/Lp-Reg.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [144] [A many-objective evolutionary algorithm using indicator-driven weight vector optimization](https://arxiv.org/abs/2510.02709)
*Xiaojing Han,Yuanxin Li*

Main category: cs.NE

TL;DR: 针对不规则帕累托前沿，提出带简化超体积指标的自适应多目标进化算法，实验显示比六种先进算法更有效。


<details>
  <summary>Details</summary>
Motivation: 固定权重向量在处理不规则帕累托前沿时，会导致解集分布不均匀甚至优化结果不佳。

Method: 基于MOEA/D框架，用简化超体积指标评估解分布，用R2指标动态调节权重向量更新频率。

Result: 实验表明，提出的算法与六种先进算法相比是高效且有效的。

Conclusion: 所提自适应多目标进化算法能有效解决不规则帕累托前沿的优化问题。

Abstract: For regular Pareto Fronts (PFs), such as those that are smooth, continuous,
and uniformly distributed, using fixed weight vectors is sufficient for
multi-objective optimization approaches using decomposition. However, when
encountering irregular PFs-including degenerate, disconnected, inverted, etc.
Fixed weight vectors can often cause a non-uniform distribution of the sets or
even poor optimization results. To address this issue, this study proposes an
adaptive many-objective evolutionary algorithm with a simplified hypervolume
indicator. It synthesizes indicator assessment techniques with
decomposition-based methods to facilitate self-adaptive and dynamic adjustment
of the weight vectors in many-objective optimization methods. Specifically,
based on the MOEA/D framework, it uses a simplified hypervolume indicator to
accurately assess solution distribution. Simultaneously, applying the R2
indicator (as an approximation of hypervolume) dynamically regulates the update
frequency of the weight vectors. Experimental results demonstrate that the
proposed algorithm is efficient and effective when compared with six
state-of-the-art algorithms.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [145] [CWM: An Open-Weights LLM for Research on Code Generation with World Models](https://arxiv.org/abs/2510.02387)
*FAIR CodeGen team,Quentin Carbonneaux,Gal Cohen,Jonas Gehring,Jacob Kahn,Jannik Kossen,Felix Kreuk,Emily McMilin,Michel Meyer,Yuxiang Wei,David Zhang,Kunhao Zheng,Jordi Armengol-Estapé,Pedram Bashiri,Maximilian Beck,Pierre Chambon,Abhishek Charnalia,Chris Cummins,Juliette Decugis,Zacharias V. Fisches,François Fleuret,Fabian Gloeckle,Alex Gu,Michael Hassid,Daniel Haziza,Badr Youbi Idrissi,Christian Keller,Rahul Kindi,Hugh Leather,Gallil Maimon,Aram Markosyan,Francisco Massa,Pierre-Emmanuel Mazaré,Vegard Mella,Naila Murray,Keyur Muzumdar,Peter O'Hearn,Matteo Pagliardini,Dmitrii Pedchenko,Tal Remez,Volker Seeker,Marco Selvi,Oren Sultan,Sida Wang,Luca Wehrstedt,Ori Yoran,Lingming Zhang,Taco Cohen,Yossi Adi,Gabriel Synnaeve*

Main category: cs.SE

TL;DR: 发布320亿参数的代码世界模型CWM，可用于推进代码生成研究，在通用编码和数学任务上表现出色并发布模型检查点。


<details>
  <summary>Details</summary>
Motivation: 推进基于世界模型的代码生成研究，提升代码理解能力。

Method: 在Python解释器和Docker环境的大量观察 - 动作轨迹上进行中训练，在可验证编码、数学和多轮软件工程环境中进行广泛的多任务推理强化学习。

Result: CWM在通用编码和数学任务上有强性能，如SWE - bench Verified的pass@1分数达65.8%等。

Conclusion: CWM为研究人员提供了强大的测试平台，展示了世界模型对智能编码的益处，发布模型检查点以支持后续研究。

Abstract: We release Code World Model (CWM), a 32-billion-parameter open-weights LLM,
to advance research on code generation with world models. To improve code
understanding beyond what can be learned from training on static code alone, we
mid-train CWM on a large amount of observation-action trajectories from Python
interpreter and agentic Docker environments, and perform extensive multi-task
reasoning RL in verifiable coding, math, and multi-turn software engineering
environments. With CWM, we provide a strong testbed for researchers to explore
the opportunities world modeling affords for improving code generation with
reasoning and planning in computational environments. We present first steps of
how world models can benefit agentic coding, enable step-by-step simulation of
Python code execution, and show early results of how reasoning can benefit from
the latter. CWM is a dense, decoder-only LLM trained with a context size of up
to 131k tokens. Independent of its world modeling capabilities, CWM offers
strong performance on general coding and math tasks: it reaches pass@1 scores
of 65.8% on SWE-bench Verified (with test-time scaling), 68.6% on
LiveCodeBench, 96.6% on Math-500, and 76.0% on AIME 2024. To support further
research on code world modeling, we release model checkpoints after
mid-training, SFT, and RL.

</details>


### [146] [From Trace to Line: LLM Agent for Real-World OSS Vulnerability Localization](https://arxiv.org/abs/2510.02389)
*Haoran Xi,Minghao Shao,Brendan Dolan-Gavitt,Muhammad Shafique,Ramesh Karri*

Main category: cs.SE

TL;DR: 提出T2L - Agent框架和T2L - ARVO基准，提升大语言模型在漏洞发现的精确性，推动漏洞检测发展。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型漏洞发现方法有局限，无法提供精确行级定位和针对性补丁。

Method: 提出T2L - Agent框架，结合多轮反馈和Agentic Trace Analyzer，融合运行时证据与AST代码分块；引入T2L - ARVO基准。

Result: T2L - Agent在T2L - ARVO上检测率达58.0%，行级定位率达54.8%，远超基线。

Conclusion: 框架和基准推动基于大语言模型的漏洞检测向可部署、鲁棒、精确诊断发展，减少噪音并加速开源软件打补丁。

Abstract: Large language models show promise for vulnerability discovery, yet
prevailing methods inspect code in isolation, struggle with long contexts, and
focus on coarse function- or file-level detections - offering limited
actionable guidance to engineers who need precise line-level localization and
targeted patches in real-world software development. We present T2L-Agent
(Trace-to-Line Agent), a project-level, end-to-end framework that plans its own
analysis and progressively narrows scope from modules to exact vulnerable
lines. T2L-Agent couples multi-round feedback with an Agentic Trace Analyzer
(ATA) that fuses runtime evidence - crash points, stack traces, and coverage
deltas - with AST-based code chunking, enabling iterative refinement beyond
single pass predictions and translating symptoms into actionable, line-level
diagnoses. To benchmark line-level vulnerability discovery, we introduce
T2L-ARVO, a diverse, expert-verified 50-case benchmark spanning five crash
families and real-world projects. T2L-ARVO is specifically designed to support
both coarse-grained detection and fine-grained localization, enabling rigorous
evaluation of systems that aim to move beyond file-level predictions. On
T2L-ARVO, T2L-Agent achieves up to 58.0% detection and 54.8% line-level
localization, substantially outperforming baselines. Together, the framework
and benchmark push LLM-based vulnerability detection from coarse identification
toward deployable, robust, precision diagnostics that reduce noise and
accelerate patching in open-source software workflows.

</details>


### [147] [AP2O: Correcting LLM-Generated Code Errors Type by Type Like Humans via Adaptive Progressive Preference Optimization](https://arxiv.org/abs/2510.02393)
*Jianqing Zhang,Wei Xia,Hande Dong,Qiang Lin,Jian Cao*

Main category: cs.SE

TL;DR: 现有大语言模型代码生成有错误，提出AP2O - Coder方法减少代码错误，实验显示提升性能且减少偏好数据使用。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成的代码存在编译和运行时错误，现有离线偏好优化方法忽略失败代码的深层次错误类型。

Method: 提出AP2O - Coder方法，构建错误笔记，逐步按错误类型优化模型，并自适应重放错误类型以适应模型弱点变化。

Result: 在多种代码和通用大语言模型上实验，AP2O - Coder在pass@k上最多提升3%代码生成性能，且使用更少偏好数据。

Conclusion: AP2O - Coder能有效减少代码生成错误，提升代码生成性能。

Abstract: LLMs' code generation capabilities have yielded substantial improvements in
the effectiveness of programming tasks. However, LLM-generated code still
suffers from compilation and runtime errors. Existing offline preference
optimization methods primarily focus on enhancing LLMs' coding abilities using
pass/fail signals in the preference data, overlooking the deep-level error
types in the failed codes. To address this, we propose Adaptively Progressive
Preference Optimization (AP2O) for coding (i.e., AP2O-Coder), a method that
guides LLMs adaptively and methodically to reduce code errors for code
generation. Specifically, we construct an error notebook from failed codes and
progressively optimize the LLM to correct errors type by type. Furthermore, we
adaptively replay error types to tailor to the LLM's changing weaknesses
throughout the training process. Through extensive experiments on both code and
general LLMs (Llama, Qwen, and DeepSeek series) with parameters ranging from
0.5B to 34B, our AP2O-Coder improves code generation performance by up to 3% in
pass@k while using less preference data. Code: https://github.com/TsingZ0/AP2O

</details>


### [148] [Dynamic Function Configuration and its Management in Serverless Computing: A Taxonomy and Future Directions](https://arxiv.org/abs/2510.02404)
*Siddharth Agarwal,Maria A. Rodriguez,Rajkumar Buyya*

Main category: cs.SE

TL;DR: 文章聚焦FaaS资源配置问题，提出影响函数设计等方面的因素分类，综述现有研究，指出研究差距并给出未来方向。


<details>
  <summary>Details</summary>
Motivation: FaaS资源配置因平台不透明和开源框架复杂性，给开发者带来挑战，需解决以推动无服务器计算高效执行。

Method: 识别FaaS资源配置技术不同方面，提出影响因素分类，分析现有文献进行全面综述。

Result: 完成对函数配置现有研究的全面综述，确定研究差距。

Conclusion: 提出未来研究方向，以增强函数配置，推动无服务器计算更广泛应用。

Abstract: The serverless cloud computing model offers a framework where the service
provider abstracts the underlying infrastructure management from developers. In
this serverless model, FaaS provides an event-driven, function-oriented
computing service characterised by fine-grained, usage-based pricing that
eliminates cost for idle resources. Platforms like AWS Lambda, Azure Functions,
and Cloud Run Functions require developers to configure their function(s) with
minimum operational resources for its successful execution. This resource
allocation influences both the operational expense and the performance quality
of these functions. However, a noticeable lack of platform transparency forces
developers to rely on expert knowledge or experience-based ad-hoc decisions to
request desired function resources. This makes optimal resource configuration a
non-trivial task while adhering to performance constraints. Furthermore, while
commercial platforms often scale resources like CPU and network bandwidth
proportional to memory, open-source frameworks permit independent configuration
of function resources, introducing additional complexity for developers aiming
to optimise their functions. These complexities have directed researchers to
resolve developer challenges and advance towards an efficient server-less
execution model. In this article, we identify different aspects of resource
configuration techniques in FaaS settings and propose a taxonomy of factors
that influence function design, configuration, run-time cost, and performance
guarantees. We conduct an analysis of existing literature on resource
configuration to present a comprehensive review of current studies on function
configuration. We also identify existing research gaps and suggest future
research directions to enhance function configuration and strengthen the
capabilities of serverless computing environments to drive its broader
adoption.

</details>


### [149] [Product Manager Practices for Delegating Work to Generative AI: "Accountability must not be delegated to non-human actors"](https://arxiv.org/abs/2510.02504)
*Mara Ulloa,Jenna L. Butler,Sankeerti Haniyur,Courtney Miller,Barrett Amos,Advait Sarkar,Margaret-Anne Storey*

Main category: cs.SE

TL;DR: 研究生成式AI对微软产品经理工作的影响，通过多种方法得出采用率、用例等成果并探讨影响。


<details>
  <summary>Details</summary>
Motivation: 现有软件工程研究多关注开发者与生成式AI的交互，对产品经理工作受其影响了解不足。

Method: 在微软开展混合方法研究，包括对885名产品经理进行调查、分析731名产品经理的遥测数据以及对15名产品经理进行访谈。

Result: 得出产品经理当前生成式AI采用率、用例、感知的好处和障碍；提出产品经理评估任务委派给生成式AI的框架；了解产品经理将生成式AI融入工作的实践及对自身角色演变的看法。

Conclusion: 讨论了生成式AI工作流采用过程和软件开发角色的影响。

Abstract: Generative AI (GenAI) is changing the nature of knowledge work, particularly
for Product Managers (PMs) in software development teams. While much software
engineering research has focused on developers' interactions with GenAI, there
is less understanding of how the work of PMs is evolving due to GenAI. To
address this gap, we conducted a mixed-methods study at Microsoft, a large,
multinational software company: surveying 885 PMs, analyzing telemetry data for
a subset of PMs (N=731), and interviewing a subset of 15 PMs. We contribute:
(1) PMs' current GenAI adoption rates, uses cases, and perceived benefits and
barriers and; (2) a framework capturing how PMs assess which tasks to delegate
to GenAI; (3) PMs adaptation practices for integrating GenAI into their roles
and perceptions of how their role is evolving. We end by discussing
implications on the broader GenAI workflow adoption process and software
development roles.

</details>


### [150] [ZeroFalse: Improving Precision in Static Analysis with LLMs](https://arxiv.org/abs/2510.02534)
*Mohsen Iranmanesh,Sina Moradi Sabet,Sina Marefat,Ali Javidi Ghasr,Allison Wilson,Iman Sharafaldin,Mohammad A. Tayebi*

Main category: cs.SE

TL;DR: 提出ZeroFalse框架结合静态分析与大语言模型减少SAST工具误报，评估效果良好，是实用可扩展方法。


<details>
  <summary>Details</summary>
Motivation: SAST工具因过多误报影响采用，削弱开发者信任且需手动筛选。

Method: ZeroFalse将静态分析输出视为结构化契约，用流敏感跟踪、上下文证据和CWE特定知识丰富后让LLM裁决，使用十个最先进LLM评估。

Result: 最佳模型在OWASP Java Benchmark和OpenVuln数据集上F1分数分别达0.912和0.955，召回率和准确率超90%，CWE专业提示表现更好，推理型LLM精度召回平衡佳。

Conclusion: ZeroFalse是增强SAST可靠性、支持集成到CI/CD管道的实用可扩展方法。

Abstract: Static Application Security Testing (SAST) tools are integral to modern
software development, yet their adoption is undermined by excessive false
positives that weaken developer trust and demand costly manual triage. We
present ZeroFalse, a framework that integrates static analysis with large
language models (LLMs) to reduce false positives while preserving coverage.
ZeroFalse treats static analyzer outputs as structured contracts, enriching
them with flow-sensitive traces, contextual evidence, and CWE-specific
knowledge before adjudication by an LLM. This design preserves the systematic
reach of static analysis while leveraging the reasoning capabilities of LLMs.
We evaluate ZeroFalse across both benchmarks and real-world projects using ten
state-of-the-art LLMs. Our best-performing models achieve F1-scores of 0.912 on
the OWASP Java Benchmark and 0.955 on the OpenVuln dataset, maintaining recall
and precision above 90%. Results further show that CWE-specialized prompting
consistently outperforms generic prompts, and reasoning-oriented LLMs provide
the most reliable precision-recall balance. These findings position ZeroFalse
as a practical and scalable approach for enhancing the reliability of SAST and
supporting its integration into real-world CI/CD pipelines.

</details>


### [151] [Key Considerations for Auto-Scaling: Lessons from Benchmark Microservices](https://arxiv.org/abs/2510.02585)
*Majid Dashtbani,Ladan Tahvildari*

Main category: cs.SE

TL;DR: 本文通过应用先进自动伸缩方法到微服务基准测试，识别出实用的自动伸缩考虑因素，并按软件生命周期分类验证，强调生命周期感知工程对微服务自动伸缩的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试常忽略自动伸缩的基础方面，难以在现实条件下评估自动伸缩方法。

Method: 将多种先进自动伸缩方法应用于常用微服务基准测试，按软件生命周期（架构、实现、部署）对问题分类，用Sock - Shop基准测试验证并评估不同自动伸缩策略。

Result: 忽略关键生命周期问题会降低自动伸缩器性能，解决这些问题可实现更稳定高效的伸缩。

Conclusion: 生命周期感知工程对释放微服务系统自动伸缩的全部潜力至关重要。

Abstract: Microservices have become the dominant architectural paradigm for building
scalable and modular cloud-native systems. However, achieving effective
auto-scaling in such systems remains a non-trivial challenge, as it depends not
only on advanced scaling techniques but also on sound design, implementation,
and deployment practices. Yet, these foundational aspects are often overlooked
in existing benchmarks, making it difficult to evaluate autoscaling methods
under realistic conditions. In this paper, we identify a set of practical
auto-scaling considerations by applying several state-of-the-art autoscaling
methods to widely used microservice benchmarks. To structure these findings, we
classify the issues based on when they arise during the software lifecycle:
Architecture, Implementation, and Deployment. The Architecture phase covers
high-level decisions such as service decomposition and inter-service
dependencies. The Implementation phase includes aspects like initialization
overhead, metrics instrumentation, and error propagation. The Deployment phase
focuses on runtime configurations such as resource limits and health checks. We
validate these considerations using the Sock-Shop benchmark and evaluate
diverse auto-scaling strategies, including threshold-based, control-theoretic,
learning-based, black-box optimization, and dependency-aware approaches. Our
findings show that overlooking key lifecycle concerns can degrade autoscaler
performance, while addressing them leads to more stable and efficient scaling.
These results underscore the importance of lifecycle-aware engineering for
unlocking the full potential of auto-scaling in microservice-based systems.

</details>


### [152] [RedCodeAgent: Automatic Red-teaming Agent against Diverse Code Agents](https://arxiv.org/abs/2510.02609)
*Chengquan Guo,Chulin Xie,Yu Yang,Zhaorun Chen,Zinan Lin,Xander Davies,Yarin Gal,Dawn Song,Bo Li*

Main category: cs.SE

TL;DR: 提出RedCodeAgent自动化红队工具，用于发现代码代理漏洞，经评估表现优于现有方法，还在真实代码助手验证。


<details>
  <summary>Details</summary>
Motivation: 现有静态安全基准和红队工具不足以识别代码代理新兴安全风险，需新工具。

Method: 提出RedCodeAgent，有自适应内存模块，可选择有效红队工具；开发模拟沙箱环境评估执行结果。

Result: 在多方面评估中，RedCodeAgent攻击成功率高、拒绝率低，效率高；在真实代码助手验证出未知安全风险。

Conclusion: RedCodeAgent实现代码代理可扩展、自适应和有效的安全评估。

Abstract: Code agents have gained widespread adoption due to their strong code
generation capabilities and integration with code interpreters, enabling
dynamic execution, debugging, and interactive programming capabilities. While
these advancements have streamlined complex workflows, they have also
introduced critical safety and security risks. Current static safety benchmarks
and red-teaming tools are inadequate for identifying emerging real-world risky
scenarios, as they fail to cover certain boundary conditions, such as the
combined effects of different jailbreak tools. In this work, we propose
RedCodeAgent, the first automated red-teaming agent designed to systematically
uncover vulnerabilities in diverse code agents. With an adaptive memory module,
RedCodeAgent can leverage existing jailbreak knowledge, dynamically select the
most effective red-teaming tools and tool combinations in a tailored toolbox
for a given input query, thus identifying vulnerabilities that might otherwise
be overlooked. For reliable evaluation, we develop simulated sandbox
environments to additionally evaluate the execution results of code agents,
mitigating potential biases of LLM-based judges that only rely on static code.
Through extensive evaluations across multiple state-of-the-art code agents,
diverse risky scenarios, and various programming languages, RedCodeAgent
consistently outperforms existing red-teaming methods, achieving higher attack
success rates and lower rejection rates with high efficiency. We further
validate RedCodeAgent on real-world code assistants, e.g., Cursor and Codeium,
exposing previously unidentified security risks. By automating and optimizing
red-teaming processes, RedCodeAgent enables scalable, adaptive, and effective
safety assessments of code agents.

</details>


### [153] [Automatic Building Code Review: A Case Study](https://arxiv.org/abs/2510.02634)
*Hanlong Wan,Weili Xu,Michael Rosenberg,Jian Zhang,Aysha Siddika*

Main category: cs.SE

TL;DR: 本文介绍了一种新的代理驱动框架用于自动代码审查（ACR），结合BIM数据提取与自动化验证，通过案例演示评估，显示MCP代理管道表现更好，推进了ACR研究。


<details>
  <summary>Details</summary>
Motivation: 建筑官员在审查设计文件时面临劳动密集、易出错和成本高的问题，BIM和LLMs的发展为ACR解决方案带来机会。

Method: 引入代理驱动框架，集成BIM数据提取与自动化验证，使用RAG和MCP代理管道，通过直接API调用和RAG推理进行建筑规范检查。

Result: 通过案例演示评估，GPT - 4o在效率和稳定性上表现最佳，MCP代理管道在严谨性和可靠性上优于RAG推理管道。

Conclusion: 该工作展示了一种可扩展、可互操作且可用于生产的方法，将BIM与权威代码审查工具相结合，推进了ACR研究。

Abstract: Building officials, particularly those in resource-constrained or rural
jurisdictions, face labor-intensive, error-prone, and costly manual reviews of
design documents as projects increase in size and complexity. The growing
adoption of Building Information Modeling (BIM) and Large Language Models
(LLMs) presents opportunities for automated code review (ACR) solutions. This
study introduces a novel agent-driven framework that integrates BIM-based data
extraction with automated verification using both retrieval-augmented
generation (RAG) and Model Context Protocol (MCP) agent pipelines. The
framework employs LLM-enabled agents to extract geometry, schedules, and system
attributes from heterogeneous file types, which are then processed for building
code checking through two complementary mechanisms: (1) direct API calls to the
US Department of Energy COMcheck engine, providing deterministic and
audit-ready outputs, and (2) RAG-based reasoning over rule provisions, enabling
flexible interpretation where coverage is incomplete or ambiguous.
  The framework was evaluated through case demonstrations, including automated
extraction of geometric attributes (such as surface area, tilt, and insulation
values), parsing of operational schedules, and validation of lighting
allowances under ASHRAE Standard 90.1-2022. Comparative performance tests
across multiple LLMs showed that GPT-4o achieved the best balance of efficiency
and stability, while smaller models exhibited inconsistencies or failures.
Results confirm that MCP agent pipelines outperform RAG reasoning pipelines in
rigor and reliability. This work advances ACR research by demonstrating a
scalable, interoperable, and production-ready approach that bridges BIM with
authoritative code review tools.

</details>


### [154] [Using Fourier Analysis and Mutant Clustering to Accelerate DNN Mutation Testing](https://arxiv.org/abs/2510.02718)
*Ali Ghanbari,Sasan Tavakkol*

Main category: cs.SE

TL;DR: 提出DM#技术用傅里叶分析加速DNN突变测试，评估显示可平均加速28.38%，误差仅0.72%。


<details>
  <summary>Details</summary>
Motivation: DNN突变分析因需在大数据集上测试大量突变体，成本高昂，需加速技术。

Method: DM#利用傅里叶分析量化突变体行为，聚类突变体，选代表测试并复用结果。

Result: DM#平均加速突变测试28.38%，平均突变得分误差0.72%，比多种基线技术误差小。

Conclusion: DM#能有效加速DNN突变测试，且误差较小。

Abstract: Deep neural network (DNN) mutation analysis is a promising approach to
evaluating test set adequacy. Due to the large number of generated mutants that
must be tested on large datasets, mutation analysis is costly. In this paper,
we present a technique, named DM#, for accelerating DNN mutation testing using
Fourier analysis. The key insight is that DNN outputs are real-valued functions
suitable for Fourier analysis that can be leveraged to quantify mutant behavior
using only a few data points. DM# uses the quantified mutant behavior to
cluster the mutants so that the ones with similar behavior fall into the same
group. A representative from each group is then selected for testing, and the
result of the test, e.g., whether the mutant is killed or survived, is reused
for all other mutants represented by the selected mutant, obviating the need
for testing other mutants. 14 DNN models of sizes ranging from thousands to
millions of parameters, trained on different datasets, are used to evaluate DM#
and compare it to several baseline techniques. Our results provide empirical
evidence on the effectiveness of DM# in accelerating mutation testing by
28.38%, on average, at the average cost of only 0.72% error in mutation score.
Moreover, on average, DM# incurs 11.78, 15.16, and 114.36 times less mutation
score error compared to random mutant selection, boundary sample selection, and
random sample selection techniques, respectively, while generally offering
comparable speed-up.

</details>


### [155] [Automated Repair of OpenID Connect Programs (Extended Version)](https://arxiv.org/abs/2510.02773)
*Tamjid Al Rahat,Yanju Chen,Yu Feng,Yuan Tian*

Main category: cs.SE

TL;DR: 提出AuthFix引擎自动修复OpenID漏洞，评估显示其74%成功率且补丁语义接近开发者编写的。


<details>
  <summary>Details</summary>
Motivation: OpenID Connect虽广泛应用但存在安全漏洞致损失，需有效修复策略，自动化程序修复有前景但面临挑战。

Method: 提出AuthFix修复引擎，集成故障定位、补丁合成和验证组件，用基于Petri网的模型检查器确保补丁正确性。

Result: 在OpenID漏洞数据集上，AuthFix成功为23个漏洞中的17个生成正确补丁，成功率74%，多数补丁语义与开发者编写的相当。

Conclusion: AuthFix是一种有效的自动修复OpenID漏洞的方法。

Abstract: OpenID Connect has revolutionized online authentication based on single
sign-on (SSO) by providing a secure and convenient method for accessing
multiple services with a single set of credentials. Despite its widespread
adoption, critical security bugs in OpenID Connect have resulted in significant
financial losses and security breaches, highlighting the need for robust
mitigation strategies. Automated program repair presents a promising solution
for generating candidate patches for OpenID implementations. However,
challenges such as domain-specific complexities and the necessity for precise
fault localization and patch verification must be addressed. We propose
AuthFix, a counterexample-guided repair engine leveraging LLMs for automated
OpenID bug fixing. AuthFix integrates three key components: fault localization,
patch synthesis, and patch verification. By employing a novel Petri-net-based
model checker, AuthFix ensures the correctness of patches by effectively
modeling interactions. Our evaluation on a dataset of OpenID bugs demonstrates
that AuthFix successfully generated correct patches for 17 out of 23 bugs
(74%), with a high proportion of patches semantically equivalent to
developer-written fixes.

</details>


### [156] [C2|Q>: A Robust Framework for Bridging Classical and Quantum Software Development](https://arxiv.org/abs/2510.02854)
*Boshuai Ye,Arif Ali Khan,Teemu Pihkakoski,Peng Liang,Muhammad Azeem Akbar,Matti Silveri,Lauri Malmi*

Main category: cs.SE

TL;DR: 提出硬件无关的量子软件开发框架C2|Q>，它将经典代码转为量子可执行程序，评估各模块表现良好，减少实现工作量。


<details>
  <summary>Details</summary>
Motivation: 当前量子开发环境让经典软件工程师难上手，需降低使用门槛。

Method: 将工作流分为编码器、部署模块和解码器三个核心模块，应用模块化软件工程原则。

Result: 编码器完成率93.8%，硬件推荐模块能选合适设备，完整工作流处理经典规范完成率高，减少近40X实现工作量。

Conclusion: C2|Q>框架有效降低量子开发难度，可将经典代码转为量子程序，开源代码可获取。

Abstract: Quantum Software Engineering (QSE) is emerging as a critical discipline to
make quantum computing accessible to a broader developer community; however,
most quantum development environments still require developers to engage with
low-level details across the software stack - including problem encoding,
circuit construction, algorithm configuration, hardware selection, and result
interpretation - making them difficult for classical software engineers to use.
To bridge this gap, we present C2|Q>: a hardware-agnostic quantum software
development framework that translates classical specifications (code) into
quantum-executable programs while preserving methodological rigor. The
framework applies modular software engineering principles by classifying the
workflow into three core modules: an encoder that classifies problems, produces
Quantum-Compatible Formats (QCFs), and constructs quantum circuits, a
deployment module that generates circuits and recommends hardware based on
fidelity, runtime, and cost, and a decoder that interprets quantum outputs into
classical solutions. In evaluation, the encoder module achieved a 93.8%
completion rate, the hardware recommendation module consistently selected the
appropriate quantum devices for workloads scaling up to 56 qubits, and the full
C2|Q>: workflow successfully processed classical specifications (434 Python
snippets and 100 JSON inputs) with completion rates of 93.8% and 100%,
respectively. For case study problems executed on publicly available NISQ
hardware, C2|Q>: reduced the required implementation effort by nearly 40X
compared to manual implementations using low-level quantum software development
kits (SDKs), with empirical runs limited to small- and medium-sized instances
consistent with current NISQ capabilities. The open-source implementation of
C2|Q>: is available at https://github.com/C2-Q/C2Q

</details>


### [157] [GramTrans: A Better Code Representation Approach in Code Generation](https://arxiv.org/abs/2510.02887)
*Zhao Zhang,Qingyuan Liang,Zeyu Sun,Yizhou Chen,Guoqing Wang,Yican Sun,Lu Zhang,Ge Li,Yingfei Xiong*

Main category: cs.SE

TL;DR: 本文探讨代码表示选择对模型性能的影响，提出解析难度与模型性能关系的猜想，通过实验验证，提出GramTrans方法并在多语言和模型上评估，结果表明有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对代码表示解析难度和模型有效性关系的深入理解，需探究代码表示选择对模型性能的影响。

Method: 提出猜想并用语法类形式化，在Python DSL上实验，提出GramTrans方法及分层冲突消除算法，在Python和Java上用多个代码生成模型评估。

Result: 实验表明解析难度与模型性能强相关，GramTrans在多个基准测试中比基线表示有显著提升。

Conclusion: 解析难度与模型性能强相关，支持了提出的猜想，GramTrans方法有效。

Abstract: Code generation has shown great promise in assisting software development. A
fundamental yet underexplored question is how the choice of code representation
affects model performance. While existing studies employ various
representations, such as treating code as plain text, grammar rule sequences,
or syntax tree sequences, they lack a principled understanding of the
relationship between parsing difficulty and model effectiveness. This paper
proposes a conjecture: the easier a representation is to parse, the better
performance the model achieves. We formalize this idea using grammar classes,
where representations in simpler classes (e.g., LL(1)) are easier to parse.
Through a controlled experiment on a Python-based DSL, we show that parsing
difficulty strongly correlates with model performance. Motivated by this
finding, we present GramTrans, a general approach that automatically transforms
a context-free language into a representation within the LL(1) class. GramTrans
introduces a novel hierarchical conflict elimination algorithm, enabling a
flexible trade-off between syntactic simplicity and token efficiency. We
evaluate GramTrans on both Python and Java using three code generation models:
StarCoder 1B, DeepSeek-Coder 1.3B, and Qwen2.5 1.5B. Across multiple
benchmarks, GramTrans consistently delivers significant improvements over
baseline representations. Furthermore, our analysis of existing representations
reconfirms the strong alignment between parsing difficulty and model
performance, providing additional support for the conjecture.

</details>


### [158] [Mechanistic Interpretability of Code Correctness in LLMs via Sparse Autoencoders](https://arxiv.org/abs/2510.02917)
*Kriz Tahimic,Charibeth Cheng*

Main category: cs.SE

TL;DR: 本文应用稀疏自编码器分解大语言模型表示以研究代码正确性机制，发现相关方向能预测错误代码，且预训练所学机制在微调中被复用，还给出三个实际应用建议。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型用于软件开发，理解其内部代码正确性机制对安全部署至关重要。

Method: 应用稀疏自编码器分解大语言模型表示，用t统计量选预测方向，通过分离分数选引导方向，用引导、注意力分析和权重正交化分析机制特性。

Result: 大语言模型中代码正确性方向能可靠预测错误代码，修正能力有取舍；成功的代码生成依赖关注测试用例；基础模型中确定的方向在指令微调后仍有效。

Conclusion: 提示策略应优先考虑测试示例；预测方向可作错误警报；预测器可引导选择性引导以防止代码损坏。

Abstract: As Large Language Models become integral to software development, with
substantial portions of AI-suggested code entering production, understanding
their internal correctness mechanisms becomes critical for safe deployment. We
apply sparse autoencoders to decompose LLM representations, identifying
directions that correspond to code correctness. We select predictor directions
using t-statistics and steering directions through separation scores from base
model representations, then analyze their mechanistic properties through
steering, attention analysis, and weight orthogonalization. We find that code
correctness directions in LLMs reliably predict incorrect code, while
correction capabilities, though statistically significant, involve tradeoffs
between fixing errors and preserving correct code. Mechanistically, successful
code generation depends on attending to test cases rather than problem
descriptions. Moreover, directions identified in base models retain their
effectiveness after instruction-tuning, suggesting code correctness mechanisms
learned during pre-training are repurposed during fine-tuning. Our mechanistic
insights suggest three practical applications: prompting strategies should
prioritize test examples over elaborate problem descriptions, predictor
directions can serve as error alarms for developer review, and these same
predictors can guide selective steering, intervening only when errors are
anticipated to prevent the code corruption from constant steering.

</details>


### [159] [Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection](https://arxiv.org/abs/2510.02934)
*Thanh Trong Vu,Tuan-Dung Bui,Thu-Trang Nguyen,Son Nguyen,Hieu Dinh Vo*

Main category: cs.SE

TL;DR: 提出AUTOPROBE方法动态选择内部表示评估代码正确性，实验显示其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有评估LLM生成代码正确性的方法依赖预选择/固定层和标记位置，限制了跨模型架构和任务的泛化性。

Method: 采用基于注意力的机制学习隐藏状态的重要性分数，聚合加权表示并通过探测分类器多维度预测代码正确性。

Result: 在多个基准和代码大语言模型上实验，AUTOPROBE性能始终优于基线，安全评估超当前最优白盒方法18%，编译性和功能性评估表现出高鲁棒性。

Conclusion: 动态选择重要内部信号使AUTOPROBE成为评估各种LLM生成代码正确性的强大且通用的解决方案。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in
code generation and are increasingly integrated into the software development
process. However, ensuring the correctness of LLM-generated code remains a
critical concern. Prior work has shown that the internal representations of
LLMs encode meaningful signals for assessing code correctness. Nevertheless,
the existing methods rely on representations from pre-selected/fixed layers and
token positions, which could limit its generalizability across diverse model
architectures and tasks. In this work, we introduce AUTOPROBE, a novel
model-agnostic approach that dynamically selects the most informative internal
representations for code correctness assessment. AUTOPROBE employs an
attention-based mechanism to learn importance scores for hidden states,
enabling it to focus on the most relevant features. These weighted
representations are then aggregated and passed to a probing classifier to
predict code correctness across multiple dimensions, including compilability,
functionality, and security. To evaluate the performance of AUTOPROBE, we
conduct extensive experiments across multiple benchmarks and code LLMs. Our
experimental results show that AUTOPROBE consistently outperforms the
baselines. For security assessment, AUTOPROBE surpasses the state-of-the-art
white-box approach by 18%. For compilability and functionality assessment,
AUTOPROBE demonstrates its highest robustness to code complexity, with the
performance higher than the other approaches by up to 19% and 111%,
respectively. These findings highlight that dynamically selecting important
internal signals enables AUTOPROBE to serve as a robust and generalizable
solution for assessing the correctness of code generated by various LLMs.

</details>


### [160] [Tracing and Metrics Design Patterns for Monitoring Cloud-native Applications](https://arxiv.org/abs/2510.02991)
*Carlos Albuquerque,Filipe F. Correia*

Main category: cs.SE

TL;DR: 本文在过往工作基础上提出三种监控云原生应用的设计模式，为软件从业者提供指导。


<details>
  <summary>Details</summary>
Motivation: 随着软件架构分布式和易变性增加，云原生应用的问题诊断面临碎片化可观测性和根因分析困难的挑战，需要有效方法。

Method: 基于行业实践和可观测性框架，提出分布式追踪、应用指标、基础设施指标三种设计模式。

Result: 分布式追踪可提升服务间请求流可见性，应用指标能实现实时监控和异常检测，基础设施指标有助于评估资源利用等情况。

Conclusion: 三种设计模式能为软件从业者监控云原生应用提供指导。

Abstract: Observability helps ensure the reliability and maintainability of
cloud-native applications. As software architectures become increasingly
distributed and subject to change, it becomes a greater challenge to diagnose
system issues effectively, often having to deal with fragmented observability
and more difficult root cause analysis. This paper builds upon our previous
work and introduces three design patterns that address key challenges in
monitoring cloud-native applications. Distributed Tracing improves visibility
into request flows across services, aiding in latency analysis and root cause
detection, Application Metrics provides a structured approach to instrumenting
applications with meaningful performance indicators, enabling real-time
monitoring and anomaly detection, and Infrastructure Metrics focuses on
monitoring the environment in which the system is operated, helping teams
assess resource utilization, scalability, and operational health. These
patterns are derived from industry practices and observability frameworks and
aim to offer guidance for software practitioners.

</details>


### [161] [Patterns for Teaching Agile with Student Projects -- Team and Project Setup](https://arxiv.org/abs/2510.03005)
*Daniel Pinho,Petr Pícha,Filipe Correia,Přemek Brada*

Main category: cs.SE

TL;DR: 因敏捷宣言理念流行，高校敏捷软件开发课程增多，但相关教学文献缺乏实用建议。本文展示了针对大学生教授敏捷软件开发实践的模式语言早期工作，给出团队和项目设置阶段的五个模式。


<details>
  <summary>Details</summary>
Motivation: 现有关于敏捷软件开发在课堂应用的文献缺乏实用建议，多关注框架或跨领域教学。

Method: 基于高等教育背景下教育者的自身经验，开发聚焦于教授大学生敏捷软件开发实践的模式语言。

Result: 提出五个专注于团队和项目设置阶段的模式，即限制团队规模、缩小项目范围、业务非关键项目、自组织团队和团队选择主题。

Conclusion: 这五个模式是开发整体模式语言的起点。

Abstract: Higher education courses teaching about agile software development (ASD) have
increased in commonality as the ideas behind the Agile Manifesto became more
commonplace in the industry. However, a lot of the literature on how ASD is
applied in the classroom does not provide much actionable advice, focusing on
frameworks or even moving beyond the software development area into teaching in
an agile way. We, therefore, showcase early work on a pattern language that
focuses on teaching ASD practices to university students, which stems from our
own experiences as educators in higher education contexts. We present five
patterns, specifically focused on team and project setup phase: Capping Team
Size, Smaller Project Scope, Business Non-Critical Project, Self-assembling
Teams, and Team Chooses Topic as a starting point for developing the overall
pattern language.

</details>


### [162] [Investigating The Smells of LLM Generated Code](https://arxiv.org/abs/2510.03029)
*Debalina Ghosh Paul,Hong Zhu,Ian Bayley*

Main category: cs.SE

TL;DR: 本文提出基于场景的方法评估大语言模型生成代码质量，发现生成代码比参考代码有更多代码异味，质量明显较差。


<details>
  <summary>Details</summary>
Motivation: 现有研究对大语言模型生成代码的质量评估较少，本文旨在识别需改进代码质量的薄弱场景。

Method: 测量代码异味并与专业编写的参考代码对比，按代码主题和复杂度划分测试数据集，搭建自动化测试系统，对四个先进大语言模型生成的Java程序进行实验。

Result: 大语言模型生成代码的代码异味发生率高于参考代码，Falcon表现相对较好，复杂任务和高级主题的代码异味增加更明显。

Conclusion: 大语言模型在不同任务复杂度和主题上的表现与对应场景下人类编写代码质量高度相关，但生成代码质量明显较差。

Abstract: Context: Large Language Models (LLMs) are increasingly being used to generate
program code. Much research has been reported on the functional correctness of
generated code, but there is far less on code quality.
  Objectives: In this study, we propose a scenario-based method of evaluating
the quality of LLM-generated code to identify the weakest scenarios in which
the quality of LLM generated code should be improved.
  Methods: The method measures code smells, an important indicator of code
quality, and compares them with a baseline formed from reference solutions of
professionally written code. The test dataset is divided into various subsets
according to the topics of the code and complexity of the coding tasks to
represent different scenarios of using LLMs for code generation. We will also
present an automated test system for this purpose and report experiments with
the Java programs generated in response to prompts given to four
state-of-the-art LLMs: Gemini Pro, ChatGPT, Codex, and Falcon.
  Results: We find that LLM-generated code has a higher incidence of code
smells compared to reference solutions. Falcon performed the least badly, with
a smell increase of 42.28%, followed by Gemini Pro (62.07%), ChatGPT (65.05%)
and finally Codex (84.97%). The average smell increase across all LLMs was
63.34%, comprising 73.35% for implementation smells and 21.42% for design
smells. We also found that the increase in code smells is greater for more
complex coding tasks and for more advanced topics, such as those involving
object-orientated concepts.
  Conclusion: In terms of code smells, LLM's performances on various coding
task complexities and topics are highly correlated to the quality of human
written code in the corresponding scenarios. However, the quality of LLM
generated code is noticeably poorer than human written code.

</details>


### [163] [Refactoring Towards Microservices: Preparing the Ground for Service Extraction](https://arxiv.org/abs/2510.03050)
*Rita Peixoto,Filipe F. Correia,Thatiane Rosa,Eduardo Guerra,Alfredo Goldman*

Main category: cs.SE

TL;DR: 组织向微服务迁移多为手动且费力，本文提出含七个重构的目录解决代码层面依赖问题，简化迁移并为自动化奠基。


<details>
  <summary>Details</summary>
Motivation: 组织从单体系统向微服务迁移过程手动且劳动密集，现有文献多关注架构层面，忽视代码层面挑战和依赖。

Method: 提出包含七个重构的目录，聚焦处理依赖，系统化代码层面迁移过程。

Result: 提供了结构化、循序渐进的迁移方法。

Conclusion: 简化了迁移过程，为迁移自动化奠定基础，使开发者能高效实现变更。

Abstract: As organizations increasingly transition from monolithic systems to
microservices, they aim to achieve higher availability, automatic scaling,
simplified infrastructure management, enhanced collaboration, and streamlined
deployments. However, this migration process remains largely manual and
labour-intensive. While existing literature offers various strategies for
decomposing monoliths, these approaches primarily focus on architecture-level
guidance, often overlooking the code-level challenges and dependencies that
developers must address during the migration. This article introduces a
catalogue of seven refactorings specifically designed to support the transition
to a microservices architecture with a focus on handling dependencies. The
catalogue provides developers with a systematic guide that consolidates
refactorings identified in the literature and addresses the critical gap in
systematizing the process at the code level. By offering a structured,
step-by-step approach, this work simplifies the migration process and lays the
groundwork for its potential automation, empowering developers to implement
these changes efficiently and effectively.

</details>


### [164] [State Field Coverage: A Metric for Oracle Quality](https://arxiv.org/abs/2510.03071)
*Facundo Molina,Nazareno Aguirre,Alessandra Gorla*

Main category: cs.SE

TL;DR: 本文提出用于评估测试预言机质量的状态字段覆盖率新指标，静态计算该指标并实验评估，结果表明其适合评估预言机质量。


<details>
  <summary>Details</summary>
Motivation: 现有评估预言机质量的指标不能全面指导预言机改进或通用性受限，需新指标。

Method: 提出状态字段覆盖率指标，静态计算该指标，通过涉及273个表示不变式和249,027个测试断言的实验评估该指标。

Result: 状态字段覆盖率与预言机故障检测能力（通过变异得分衡量）强相关。

Conclusion: 状态字段覆盖率是适合评估预言机质量的指标。

Abstract: The effectiveness of testing in uncovering software defects depends not only
on the characteristics of the test inputs and how thoroughly they exercise the
software, but also on the quality of the oracles used to determine whether the
software behaves as expected. Therefore, assessing the quality of oracles is
crucial to improve the overall effectiveness of the testing process. Existing
metrics have been used for this purpose, but they either fail to provide a
comprehensive basis for guiding oracle improvement, or they are tailored to
specific types of oracles, thus limiting their generality.
  In this paper, we introduce state field coverage, a novel metric for
assessing oracle quality. This metric measures the proportion of an object's
state, as statically defined by its class fields, that an oracle may access
during test execution. The main intuition of our metric is that oracles with a
higher state field coverage are more likely to detect faults in the software
under analysis, as they inspect a larger portion of the object states to
determine whether tests pass or not.
  We implement a mechanism to statically compute the state field coverage
metric. Being statically computed, the metric is efficient and provides direct
guidance for improving test oracles by identifying state fields that remain
unexamined. We evaluate state field coverage through experiments involving 273
representation invariants and 249,027 test assertions. The results show that
state field coverage is a well-suited metric for assessing oracle quality, as
it strongly correlates with the oracles' fault-detection ability, measured by
mutation score.

</details>


### [165] [When Names Disappear: Revealing What LLMs Actually Understand About Code](https://arxiv.org/abs/2510.03178)
*Cuong Chi Le,Minh V. T. Pham,Cuong Duc Van,Hoang N. Phan,Huy N. Phan,Tien N. Nguyen*

Main category: cs.SE

TL;DR: 论文指出大语言模型处理代码任务时对程序意义的理解不清晰，引入语义保留混淆技术和增强基准测试评估模型代码理解和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在代码任务中如何推导程序意义，发现当前基准测试可能奖励命名模式记忆而非真正语义推理。

Method: 引入语义保留混淆技术，发布增强基准测试ClassEval - Obf。

Result: ClassEval - Obf减少了夸大的性能差距，削弱了记忆捷径。

Conclusion: ClassEval - Obf能为评估大语言模型的代码理解和泛化能力提供更可靠依据。

Abstract: Large Language Models (LLMs) achieve strong results on code tasks, but how
they derive program meaning remains unclear. We argue that code communicates
through two channels: structural semantics, which define formal behavior, and
human-interpretable naming, which conveys intent. Removing the naming channel
severely degrades intent-level tasks such as summarization, where models
regress to line-by-line descriptions. Surprisingly, we also observe consistent
reductions on execution tasks that should depend only on structure, revealing
that current benchmarks reward memorization of naming patterns rather than
genuine semantic reasoning. To disentangle these effects, we introduce a suite
of semantics-preserving obfuscations and show that they expose identifier
leakage across both summarization and execution. Building on these insights, we
release ClassEval-Obf, an obfuscation-enhanced benchmark that systematically
suppresses naming cues while preserving behavior. Our results demonstrate that
ClassEval-Obf reduces inflated performance gaps, weakens memorization
shortcuts, and provides a more reliable basis for assessing LLMs' code
understanding and generalization.

</details>


### [166] [Abstain and Validate: A Dual-LLM Policy for Reducing Noise in Agentic Program Repair](https://arxiv.org/abs/2510.03217)
*José Cambronero,Michele Tufano,Sherry Shi,Renyao Wei,Grant Uy,Runxiang Cheng,Chin-Jung Liu,Shiying Pan,Satish Chandra,Pat Rondon*

Main category: cs.SE

TL;DR: 引入两种基于大语言模型的策略（bug弃权和补丁验证）减少代理式自动程序修复（APR）噪声，在谷歌代码库上评估效果显著，为工业规模部署提供实用途径。


<details>
  <summary>Details</summary>
Motivation: 代理式APR生成的补丁需人工审查，展示不太可能的补丁会产生大量噪声，浪费开发者时间并削弱对自动代码更改的信任。

Method: 引入bug弃权策略排除代理式APR系统不太可能修复的bug，引入补丁验证策略拒绝不太可能修复给定bug的补丁。

Result: 在谷歌代码库的三组bug上评估，去除被策略拒绝的bug和补丁轨迹，成功率分别最多可提高13和15个百分点，组合使用最多可提高39个百分点；在特定类型bug上，补丁验证也能提高平均单样本成功率。

Conclusion: 这两种策略为代理式APR系统的可靠工业规模部署提供了实用途径。

Abstract: Agentic Automated Program Repair (APR) is increasingly tackling complex,
repository-level bugs in industry, but ultimately agent-generated patches still
need to be reviewed by a human before committing them to ensure they address
the bug. Showing unlikely patches to developers can lead to substantial noise,
wasting valuable developer time and eroding trust in automated code changes. We
introduce two complementary LLM-based policies to reduce such noise: bug
abstention and patch validation policies. Bug abstention excludes bugs that the
agentic APR system is unlikely to fix. Patch validation rejects patches that
are unlikely to be a good fix for the given bug. We evaluate both policies on
three sets of bugs from Google's codebase, and their candidate patches
generated by an internal agentic APR system. On a set of 174 human-reported
bugs, removing bugs and patch trajectories rejected by our policies can raise
success rates by up to 13 percentage points and 15 percentage points,
respectively, and by up to 39 percentage points in combination. On null pointer
exceptions and sanitizer-reported bugs with machine-generated bug reports,
patch validation also improves average single-sample success rates. This
two-policy approach provides a practical path to the reliable, industrial-scale
deployment of agentic APR systems.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [167] [FinReflectKG -- MultiHop: Financial QA Benchmark for Reasoning with Knowledge Graph Evidence](https://arxiv.org/abs/2510.02906)
*Abhinav Arun,Reetu Raj Harsh,Bhaskarjit Sarmah,Stefano Pasquali*

Main category: q-fin.CP

TL;DR: 提出基于FinReflectKG的基准FinReflectKG - MultiHop，评估不同检索场景，表明KG引导精确检索在金融多跳问答中有显著优势并发布部分基准数据。


<details>
  <summary>Details</summary>
Motivation: 金融披露多跳推理存在检索问题，LLMs在导航嘈杂上下文时消耗过多令牌，缺乏精确KG引导会导致推理模型表现不佳。

Method: 构建FinReflectKG - MultiHop基准，挖掘多跳子图模式生成问题，通过两阶段管道创建QA对并进行质量控制评估，评估三种检索场景。

Result: KG引导的精确检索在基准数据集上使正确率提高约24%，减少约84.5%的令牌使用。

Conclusion: 知识图谱在多跳金融问答中连接证据方面起关键作用，发布部分基准数据以促进进一步研究。

Abstract: Multi-hop reasoning over financial disclosures is often a retrieval problem
before it becomes a reasoning or generation problem: relevant facts are
dispersed across sections, filings, companies, and years, and LLMs often expend
excessive tokens navigating noisy context. Without precise Knowledge Graph
(KG)-guided selection of relevant context, even strong reasoning models either
fail to answer or consume excessive tokens, whereas KG-linked evidence enables
models to focus their reasoning on composing already retrieved facts. We
present FinReflectKG - MultiHop, a benchmark built on FinReflectKG, a
temporally indexed financial KG that links audited triples to source chunks
from S&P 100 filings (2022-2024). Mining frequent 2-3 hop subgraph patterns
across sectors (via GICS taxonomy), we generate financial analyst style
questions with exact supporting evidence from the KG. A two-phase pipeline
first creates QA pairs via pattern-specific prompts, followed by a
multi-criteria quality control evaluation to ensure QA validity. We then
evaluate three controlled retrieval scenarios: (S1) precise KG-linked paths;
(S2) text-only page windows centered on relevant text spans; and (S3) relevant
page windows with randomizations and distractors. Across both reasoning and
non-reasoning models, KG-guided precise retrieval yields substantial gains on
the FinReflectKG - MultiHop QA benchmark dataset, boosting correctness scores
by approximately 24 percent while reducing token utilization by approximately
84.5 percent compared to the page window setting, which reflects the
traditional vector retrieval paradigm. Spanning intra-document, inter-year, and
cross-company scopes, our work underscores the pivotal role of knowledge graphs
in efficiently connecting evidence for multi-hop financial QA. We also release
a curated subset of the benchmark (555 QA Pairs) to catalyze further research.

</details>


### [168] [Joint Bidding on Intraday and Frequency Containment Reserve Markets](https://arxiv.org/abs/2510.03209)
*Yiming Zhang,Wolfgang Ridinger,David Wozabal*

Main category: q-fin.CP

TL;DR: 本文提出优化电池储能系统参与多电力市场的新方法，经回测验证有效提升利润。


<details>
  <summary>Details</summary>
Motivation: 可再生能源集成增加供应可变性，电池储能系统是平衡供需的可行方案，但现有文献在多市场参与研究有不足。

Method: 开发结合一次调频备用市场参与和日内市场连续交易的联合投标策略，利用滚动内在算法进行日内决策和荷电状态恢复，用学习分类器策略确定市场间最优容量分配。

Result: 学习分类器策略比最佳静态策略整体利润提高超4%，比天真动态基准提高超3%，与理论完美预见策略差距缩小到4%。

Conclusion: 基于学习的动态分配方法在复杂多市场环境中有效。

Abstract: As renewable energy integration increases supply variability, battery energy
storage systems (BESS) present a viable solution for balancing supply and
demand. This paper proposes a novel approach for optimizing battery BESS
participation in multiple electricity markets. We develop a joint bidding
strategy that combines participation in the primary frequency reserve market
with continuous trading in the intraday market, addressing a gap in the extant
literature which typically considers these markets in isolation or simplifies
the continuous nature of intraday trading. Our approach utilizes a mixed
integer linear programming implementation of the rolling intrinsic algorithm
for intraday decisions and state of charge recovery, alongside a learned
classifier strategy (LCS) that determines optimal capacity allocation between
markets. A comprehensive out-of-sample backtest over more than one year of
historical German market data validates our approach: The LCS increases overall
profits by over 4% compared to the best-performing static strategy and by more
than 3% over a naive dynamic benchmark. Crucially, our method closes the gap to
a theoretical perfect foresight strategy to just 4%, demonstrating the
effectiveness of dynamic, learning-based allocation in a complex, multi-market
environment.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [169] [Do Mutual Funds Make Active and Skilled Liquidity Choices in Portfolio Management? Evidence from India](https://arxiv.org/abs/2510.02741)
*Pankaj K Agarwal,H K Pradhan,Konark Saxena*

Main category: q-fin.PM

TL;DR: 研究印度开放式股票共同基金的主动流动性管理，发现基金经理应对资金流入的方式及不同流动性组合基金的策略，且主动管理流动性的基金有显著回报。


<details>
  <summary>Details</summary>
Motivation: 研究印度开放式股票共同基金的主动流动性管理情况。

Method: 对印度开放式股票共同基金进行研究观察。

Result: 基金经理通过增加现金持有应对资金流入，流动性差的投资组合的基金保持更多现金储备，主动管理流动性的基金有显著回报。

Conclusion: 在印度这种存在显著横截面流动性差异的市场，主动流动性管理很重要。

Abstract: This study examines active liquidity management by Indian open-ended equity
mutual funds. We find that fund managers respond to inflows by increasing cash
holdings, which are later used to purchase less-liquid stocks at favourable
valuations. Funds with less liquid portfolios tend to maintain larger cash
reserves to manage flows. Funds that make active liquidity choices yield
statistically and economically significant gross and net returns. The
performance differences between funds with varying activeness in altering
liquidity highlight the importance of active liquidity management in markets
with substantial cross-sectional liquidity differences such as India.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [170] [Comparative Evaluation of VaR Models: Historical Simulation, GARCH-Based Monte Carlo, and Filtered Historical Simulation](https://arxiv.org/abs/2505.05646)
*Xin Tian*

Main category: q-fin.RM

TL;DR: 评估三种VaR建模方法，发现HS和GARCH - N模型校准不佳，GARCH - FHS表现出色，能更好捕捉厚尾风险和提供可靠短期风险预测。


<details>
  <summary>Details</summary>
Motivation: 全面评估三种VaR建模方法（HS、GARCH - N和FHS）在样本内和多日预测框架下的表现。

Method: 用三种方法计算每日5% VaR估计值，通过经验违约频率和视觉违约指标评估准确性；在GARCH - N和GARCH - FHS框架下模拟5天累计回报计算多期VaR和预期损失。

Result: HS和GARCH - N模型校准严重不佳，经验违约率远超理论水平；GARCH - N低估尾部风险，GARCH - FHS提供更稳健保守的尾部估计。

Conclusion: GARCH - FHS模型在捕捉厚尾风险方面表现更优，能提供更可靠的短期风险预测。

Abstract: This report presents a comprehensive evaluation of three Value-at-Risk (VaR)
modeling approaches: Historical Simulation (HS), GARCH with Normal
approximation (GARCH-N), and GARCH with Filtered Historical Simulation (FHS),
using both in-sample and multi-day forecasting frameworks. We compute daily 5
percent VaR estimates using each method and assess their accuracy via empirical
breach frequencies and visual breach indicators. Our findings reveal severe
miscalibration in the HS and GARCH-N models, with empirical breach rates far
exceeding theoretical levels. In contrast, the FHS method consistently aligns
with theoretical expectations and exhibits desirable statistical and visual
behavior. We further simulate 5-day cumulative returns under both GARCH-N and
GARCH-FHS frameworks to compute multi-period VaR and Expected Shortfall.
Results show that GARCH-N underestimates tail risk due to its reliance on the
Gaussian assumption, whereas GARCH-FHS provides more robust and conservative
tail estimates. Overall, the study demonstrates that the GARCH-FHS model offers
superior performance in capturing fat-tailed risks and provides more reliable
short-term risk forecasts.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [171] [FR-LUX: Friction-Aware, Regime-Conditioned Policy Optimization for Implementable Portfolio Management](https://arxiv.org/abs/2510.02986)
*Jian'an Zhang*

Main category: q-fin.TR

TL;DR: 提出FR - LUX强化学习框架应对交易成本和市场状态转换问题，在多场景表现良好且有理论保证和可实现性。


<details>
  <summary>Details</summary>
Motivation: 解决纸质投资组合在实盘交易中因交易成本和市场状态转换而失败的问题。

Method: 引入FR - LUX框架，集成微观结构一致执行模型、交易空间信任区域和显式状态条件。

Result: 在多制度和成本水平网格下，FR - LUX平均夏普比率高、成本 - 绩效斜率更平缓、风险 - 回报效率优，成对场景改进显著。

Conclusion: FR - LUX框架有效，有理论保证且可实施。

Abstract: Transaction costs and regime shifts are major reasons why paper portfolios
fail in live trading. We introduce FR-LUX (Friction-aware, Regime-conditioned
Learning under eXecution costs), a reinforcement learning framework that learns
after-cost trading policies and remains robust across volatility-liquidity
regimes. FR-LUX integrates three ingredients: (i) a microstructure-consistent
execution model combining proportional and impact costs, directly embedded in
the reward; (ii) a trade-space trust region that constrains changes in
inventory flow rather than logits, yielding stable low-turnover updates; and
(iii) explicit regime conditioning so the policy specializes to LL/LH/HL/HH
states without fragmenting the data. On a 4 x 5 grid of regimes and cost levels
with multiple random seeds, FR-LUX achieves the top average Sharpe ratio with
narrow bootstrap confidence intervals, maintains a flatter cost-performance
slope than strong baselines, and attains superior risk-return efficiency for a
given turnover budget. Pairwise scenario-level improvements are strictly
positive and remain statistically significant after multiple-testing
corrections. We provide formal guarantees on optimality under convex frictions,
monotonic improvement under a KL trust region, long-run turnover bounds and
induced inaction bands due to proportional costs, positive value advantage for
regime-conditioned policies, and robustness to cost misspecification. The
methodology is implementable: costs are calibrated from standard liquidity
proxies, scenario-level inference avoids pseudo-replication, and all figures
and tables are reproducible from released artifacts.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [172] [Higher-arity PAC learning, VC dimension and packing lemma](https://arxiv.org/abs/2510.02420)
*Artem Chernikov,Henry Towsner*

Main category: stat.ML

TL;DR: 概述Chernikov等人的工作，涉及高arity VC理论及相关引理，展示其对高arity PAC学习的刻画，并指出近期一些结果可由此工作推出。


<details>
  <summary>Details</summary>
Motivation: 对Chernikov等人发展的高arity VC理论相关工作进行概述，并展示其与高arity PAC学习的联系。

Method: 未提及具体方法

Result: 展示高arity VC理论刻画了高arity PAC学习，指出近期部分结果可由之前工作推出。

Conclusion: 高arity VC理论在高arity PAC学习中有重要作用，之前工作能为近期研究提供基础。

Abstract: The aim of this note is to overview some of our work in Chernikov, Towsner'20
(arXiv:2010.00726) developing higher arity VC theory (VC$_n$ dimension),
including a generalization of Haussler packing lemma, and an associated tame
(slice-wise) hypergraph regularity lemma; and to demonstrate that it
characterizes higher arity PAC learning (PAC$_n$ learning) in $n$-fold product
spaces with respect to product measures introduced by Kobayashi, Kuriyama and
Takeuchi'15. We also point out how some of the recent results in
arXiv:2402.14294, arXiv:2505.15688, arXiv:2509.20404 follow from our work in
arXiv:2010.00726.

</details>


### [173] [Predictive inference for time series: why is split conformal effective despite temporal dependence?](https://arxiv.org/abs/2510.02471)
*Rina Foygel Barber,Ashwin Pananjady*

Main category: stat.ML

TL;DR: 研究时间序列预测中分裂共形预测的理论性质，用新的“切换系数”界定覆盖损失，相关覆盖概率刻画较精确。


<details>
  <summary>Details</summary>
Motivation: 时间序列中数据存在时间依赖性，违反共形预测所需的可交换性假设，且有“记忆”的预测器会加剧问题，需研究其理论性质。

Method: 研究分裂共形预测在时间序列设置下的理论性质，引入新的“切换系数”。

Result: 用“切换系数”界定方法的覆盖损失，对平稳、β - 混合过程类的覆盖概率刻画精确。

Conclusion: 对时间序列中分裂共形预测的覆盖情况有了理论界定，引入的工具或可用于分析其他依赖数据的预测推断方法。

Abstract: We consider the problem of uncertainty quantification for prediction in a
time series: if we use past data to forecast the next time point, can we
provide valid prediction intervals around our forecasts? To avoid placing
distributional assumptions on the data, in recent years the conformal
prediction method has been a popular approach for predictive inference, since
it provides distribution-free coverage for any iid or exchangeable data
distribution. However, in the time series setting, the strong empirical
performance of conformal prediction methods is not well understood, since even
short-range temporal dependence is a strong violation of the exchangeability
assumption. Using predictors with "memory" -- i.e., predictors that utilize
past observations, such as autoregressive models -- further exacerbates this
problem. In this work, we examine the theoretical properties of split conformal
prediction in the time series setting, including the case where predictors may
have memory. Our results bound the loss of coverage of these methods in terms
of a new "switch coefficient", measuring the extent to which temporal
dependence within the time series creates violations of exchangeability. Our
characterization of the coverage probability is sharp over the class of
stationary, $\beta$-mixing processes. Along the way, we introduce tools that
may prove useful in analyzing other predictive inference methods for dependent
data.

</details>


### [174] [Beyond Linear Diffusions: Improved Representations for Rare Conditional Generative Modeling](https://arxiv.org/abs/2510.02499)
*Kulunu Dharmakeerthi,Yousef El-Laham,Henry H. Wong,Vamsi K. Potluru,Changhong He,Taosong He*

Main category: stat.ML

TL;DR: 提出一种尾自适应方法，使基于分数的生成模型在低概率区域样本复杂度降低，在极端尾条件下表现优于标准扩散模型。


<details>
  <summary>Details</summary>
Motivation: 当前线性扩散方法在建模条件分布 $P(Y|X=x)$ 且 $P(X=x)$ 较小时面临挑战，低概率区域样本少，难以建模条件密度。

Method: 借鉴条件极值理论，调整数据表示和前向方案，采用数据驱动的非线性漂移项进行扩散。

Result: 在两个合成数据集和一个真实金融数据集上验证，尾自适应方法在极端尾条件下能更准确捕捉响应分布。

Conclusion: 尾自适应方法在极端尾条件下显著优于标准扩散模型。

Abstract: Diffusion models have emerged as powerful generative frameworks with
widespread applications across machine learning and artificial intelligence
systems. While current research has predominantly focused on linear diffusions,
these approaches can face significant challenges when modeling a conditional
distribution, $P(Y|X=x)$, when $P(X=x)$ is small. In these regions, few
samples, if any, are available for training, thus modeling the corresponding
conditional density may be difficult. Recognizing this, we show it is possible
to adapt the data representation and forward scheme so that the sample
complexity of learning a score-based generative model is small in low
probability regions of the conditioning space. Drawing inspiration from
conditional extreme value theory we characterize this method precisely in the
special case in the tail regions of the conditioning variable, $X$. We show how
diffusion with a data-driven choice of nonlinear drift term is best suited to
model tail events under an appropriate representation of the data. Through
empirical validation on two synthetic datasets and a real-world financial
dataset, we demonstrate that our tail-adaptive approach significantly
outperforms standard diffusion models in accurately capturing response
distributions at the extreme tail conditions.

</details>


### [175] [Adaptive randomized pivoting and volume sampling](https://arxiv.org/abs/2510.02513)
*Ethan N. Epperly*

Main category: stat.ML

TL;DR: 本文重新解读ARP算法，给出新分析和更快实现。


<details>
  <summary>Details</summary>
Motivation: 对ARP算法进行新的解读和分析，以挖掘其更多特性和优化实现。

Method: 将ARP算法与体积采样分布和线性回归的主动学习算法建立联系，使用拒绝采样。

Result: 对ARP算法进行了新的分析，并得到了更快的实现。

Conclusion: 通过与其他算法建立联系，能对ARP算法有新的认识并优化其实现。

Abstract: Adaptive randomized pivoting (ARP) is a recently proposed and highly
effective algorithm for column subset selection. This paper reinterprets the
ARP algorithm by drawing connections to the volume sampling distribution and
active learning algorithms for linear regression. As consequences, this paper
presents new analysis for the ARP algorithm and faster implementations using
rejection sampling.

</details>


### [176] [Learning Multi-Index Models with Hyper-Kernel Ridge Regression](https://arxiv.org/abs/2510.02532)
*Shuo Huang,Hippolyte Labarrière,Ernesto De Vito,Tomaso Poggio,Lorenzo Rosasco*

Main category: stat.ML

TL;DR: 本文研究DNN优势理论基础，提出HKRR结合神经网络和核方法，给出样本复杂度结果，开发优化方法并对比。


<details>
  <summary>Details</summary>
Motivation: 深入理解深度神经网络在高维问题中优于其他模型却理论基础不明的情况，探究其成功的关键因素。

Method: 考虑多指标模型（MIM），引入超核岭回归（HKRR），利用核性质开发交替最小化和交替梯度方法进行理论和数值对比。

Result: HKRR能自适应学习MIM，克服维度灾难，数值结果补充并强化理论发现。

Conclusion: 从组合结构角度研究可更好理解深度网络优势，HKRR是有效方法。

Abstract: Deep neural networks excel in high-dimensional problems, outperforming models
such as kernel methods, which suffer from the curse of dimensionality. However,
the theoretical foundations of this success remain poorly understood. We follow
the idea that the compositional structure of the learning task is the key
factor determining when deep networks outperform other approaches. Taking a
step towards formalizing this idea, we consider a simple compositional model,
namely the multi-index model (MIM). In this context, we introduce and study
hyper-kernel ridge regression (HKRR), an approach blending neural networks and
kernel methods. Our main contribution is a sample complexity result
demonstrating that HKRR can adaptively learn MIM, overcoming the curse of
dimensionality. Further, we exploit the kernel nature of the estimator to
develop ad hoc optimization approaches. Indeed, we contrast alternating
minimization and alternating gradient methods both theoretically and
numerically. These numerical results complement and reinforce our theoretical
findings.

</details>


### [177] [Neural Jump ODEs as Generative Models](https://arxiv.org/abs/2510.02757)
*Robert A. Crowell,Florian Krach,Josef Teichmann*

Main category: stat.ML

TL;DR: 探索Neural Jump ODEs (NJODEs)作为伊藤过程生成模型的应用，证明可近似漂移和扩散系数，有无需对抗训练等优势。


<details>
  <summary>Details</summary>
Motivation: 研究如何使用NJODEs作为伊藤过程的生成模型。

Method: 在标准正则性假设下，用NJODE框架近似伊藤过程的漂移和扩散系数。

Result: 在极限情况下能恢复真实参数，生成与真实过程同分布的样本。

Conclusion: 该方法无需对抗训练，可处理不规则采样、缺失值和路径依赖动态，能用于现实场景。

Abstract: In this work, we explore how Neural Jump ODEs (NJODEs) can be used as
generative models for It\^o processes. Given (discrete observations of) samples
of a fixed underlying It\^o process, the NJODE framework can be used to
approximate the drift and diffusion coefficients of the process. Under standard
regularity assumptions on the It\^o processes, we prove that, in the limit, we
recover the true parameters with our approximation. Hence, using these learned
coefficients to sample from the corresponding It\^o process generates, in the
limit, samples with the same law as the true underlying process. Compared to
other generative machine learning models, our approach has the advantage that
it does not need adversarial training and can be trained solely as a predictive
model on the observed samples without the need to generate any samples during
training to empirically approximate the distribution. Moreover, the NJODE
framework naturally deals with irregularly sampled data with missing values as
well as with path-dependent dynamics, allowing to apply this approach in
real-world settings. In particular, in the case of path-dependent coefficients
of the It\^o processes, the NJODE learns their optimal approximation given the
past observations and therefore allows generating new paths conditionally on
discrete, irregular, and incomplete past observations in an optimal way.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [178] [Alzheimer's Clinical Research Data via R Packages: the alzverse](https://arxiv.org/abs/2510.02318)
*Michael C. Donohue,Kedir Hussen,Oliver Langford,Richard Gallardo,Gustavo Jimenez-Maggiora,Paul S. Aisen*

Main category: stat.CO

TL;DR: 文章指出共享临床研究数据对阿尔茨海默病等研究很重要，但存在挑战，介绍用R包克服挑战的优势，以A4LEARN、ADNIMERGE2等为例，还引入alzverse包促进元分析，强调R数据包可加速临床研究。


<details>
  <summary>Details</summary>
Motivation: 解决临床研究数据在可访问性、标准化等方面的挑战，推动阿尔茨海默病等研究的数据共享。

Method: 介绍A4LEARN、ADNIMERGE2等R包收集数据、文档和分析示例，以及alzverse包结合特定研究数据促进元分析。

Result: R数据包将数据等整合为可移植包，可在常用R编程环境安装和浏览。

Conclusion: R数据包能促进协作、透明性和可重复性，对加速临床研究起重要作用。

Abstract: Sharing clinical research data is essential for advancing research in
Alzheimer's disease (AD) and other therapeutic areas. However, challenges in
data accessibility, standardization, documentation, usability, and
reproducibility continue to impede this goal. In this article, we highlight the
advantages of using R packages to overcome these challenges using two examples.
The A4LEARN R package includes data from a randomized trial (the Anti-Amyloid
Treatment in Asymptomatic Alzheimer's [A4] study) and its companion
observational study of biomarker negative individuals (the Longitudinal
Evaluation of Amyloid Risk and Neurodegeneration [LEARN] study). The ADNIMERGE2
R package includes data from the Alzheimer's Disease Neuroimaging Initiative
(ADNI), a longitudinal observational biomarker and imaging study. These
packages collect data, documentation, and reproducible analysis vignettes into
a portable bundle that can be installed and browsed within commonly used R
programming environments. We also introduce the alzverse package which
leverages a common data standard to combine study-specific data packages to
facilitate meta-analyses. By promoting collaboration, transparency, and
reproducibility, R data packages can play a vital role in accelerating clinical
research.

</details>


### [179] [HOMC: A MATLAB Package for Higher Order Markov Chains](https://arxiv.org/abs/2510.02664)
*Jianhong Xu*

Main category: stat.CO

TL;DR: 介绍首个用于高阶马尔可夫链的MATLAB包，可计算相关重要量、进行属性检查等，对研究和实践有帮助。


<details>
  <summary>Details</summary>
Motivation: 为了方便计算高阶马尔可夫链相关重要量及进行属性检查，提供一个实用工具。

Method: 开发MATLAB包，实现张量“box”积等关键功能。

Result: 开发出首个高阶马尔可夫链的MATLAB包，可完成多种计算和检查任务。

Conclusion: 该HOMC包对涉及高阶马尔可夫链的数值实验和算法原型设计等任务有实用价值。

Abstract: We present a MATLAB package, which is the first of its kind, for Higher Order
Markov Chains (HOMC). It can be used to easily compute all important quantities
in our recent works relevant to higher order Markov chains, such as the
$k$-step transition tensor, limiting probability distribution, ever-reaching
probability tensor, and mean first passage time tensor. It can also be used to
check whether a higher order chain is ergodic or regular, to construct the
transition matrix of the associated reduced first order chain, and to determine
whether a state is recurrent or transient. A key function in the package is an
implementation of the tensor ``box'' product which has a probabilistic
interpretation and is different from other tensor products in the literature.
This HOMC package is useful to researchers and practitioners alike for tasks
such as numerical experimentation and algorithm prototyping involving higher
order Markov chains.

</details>


### [180] [A fast non-reversible sampler for Bayesian finite mixture models](https://arxiv.org/abs/2510.03226)
*Filippo Ascolani,Giacomo Zanella*

Main category: stat.CO

TL;DR: 本文提出一种新的非可逆抽样方案用于贝叶斯有限混合模型，在多场景下优于经典抽样器，理论上性能不差于标准方案，还能减少收敛时间。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯有限混合模型后验分布抽样困难，流行的可逆马尔可夫链蒙特卡罗方案在观测值数量大时收敛慢。

Method: 引入一种新的非可逆抽样方案。

Result: 新方案在许多感兴趣的场景中大幅优于经典抽样器；理论上性能在渐近方差方面不比标准方案差超过四倍；可将收敛时间从O$(n^2)$降至O$(n)$。

Conclusion: 混合模型的统计特征使其适合使用非可逆离散抽样器。

Abstract: Finite mixtures are a cornerstone of Bayesian modelling, and it is well-known
that sampling from the resulting posterior distribution can be a hard task. In
particular, popular reversible Markov chain Monte Carlo schemes are often slow
to converge when the number of observations $n$ is large. In this paper we
introduce a novel and simple non-reversible sampling scheme for Bayesian finite
mixture models, which is shown to drastically outperform classical samplers in
many scenarios of interest, especially during convergence phase and when
components in the mixture have non-negligible overlap. At the theoretical
level, we show that the performance of the proposed non-reversible scheme
cannot be worse than the standard one, in terms of asymptotic variance, by more
than a factor of four; and we provide a scaling limit analysis suggesting that
the non-reversible sampler can reduce the convergence time from O$(n^2)$ to
O$(n)$. We also discuss why the statistical features of mixture models make
them an ideal case for the use of non-reversible discrete samplers.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [181] [Cross-Platform DNA Methylation Classifier for the Eight Molecular Subtypes of Group 3 & 4 Medulloblastoma](https://arxiv.org/abs/2510.02416)
*Omer Abid,Gholamreza Rafiee*

Main category: q-bio.GN

TL;DR: 本文提出基于DNA甲基化的跨平台机器学习分类器区分髓母细胞瘤3、4组新亚型，效果良好，有潜力推动精准医疗。


<details>
  <summary>Details</summary>
Motivation: 髓母细胞瘤分子亚组发现为个性化治疗提供可能，需要分类器将研究成果转化到临床实践。

Method: 开发基于DNA甲基化的跨平台机器学习分类器，在HM450和EPIC甲基化阵列样本上进行测试。

Result: 在两个独立测试集上，模型加权F1为0.95，平衡准确率为0.957，跨平台表现一致。

Conclusion: 该研究朝着推进精准医疗、改善3、4组髓母细胞瘤患者临床结果的方向迈进。

Abstract: Medulloblastoma is a malignant pediatric brain cancer, and the discovery of
molecular subgroups is enabling personalized treatment strategies. In 2019, a
consensus identified eight novel subtypes within Groups 3 and 4, each
displaying heterogeneous characteristics. Classifiers are essential for
translating these findings into clinical practice by supporting clinical
trials, personalized therapy development and application, and patient
monitoring. This study presents a DNA methylation-based, cross-platform machine
learning classifier capable of distinguishing these subtypes on both HM450 and
EPIC methylation array samples. Across two independent test sets, the model
achieved weighted F1 = 0.95 and balanced accuracy = 0.957, consistent across
platforms. As the first cross-platform solution, it provides backward
compatibility while extending applicability to a newer platform, also enhancing
accessibility. It also has the potential to become the first publicly available
classifier for these subtypes once deployed through a web application, as
planned in the future. This work overall takes steps in the direction of
advancing precision medicine and improving clinical outcomes for patients
within the majority prevalence medulloblastoma subgroups, groups 3 and 4.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [182] [Optimized Degree Realization: Minimum Dominating Set & Maximum Matching](https://arxiv.org/abs/2510.03176)
*Amotz Bar-Noy,Igor Kalinichev,David Peleg,Dror Rawitz*

Main category: cs.DM

TL;DR: 研究度实现问题的优化变体，为最小支配集和最大匹配的度实现问题给出多项式时间算法，并对最小支配集给定大小的序列进行简洁刻画。


<details>
  <summary>Details</summary>
Motivation: 已有部分度实现优化问题的高效算法，研究尚无算法的最小支配集和最大匹配度实现问题，以及刻画具有给定优化函数值的序列。

Method: 设计多项式时间算法解决两个开放问题，并对最小支配集给定大小的序列进行刻画。

Result: 给出最小支配集和最大匹配度实现问题的多项式时间算法，对最小支配集给定大小的序列进行简洁刻画。

Conclusion: 解决了两个度实现优化问题，推动了度实现问题研究进展。

Abstract: The Degree Realization problem requires, given a sequence $d$ of $n$ positive
integers, to decide whether there exists a graph whose degrees correspond to
$d$, and to construct such a graph if it exists. A more challenging variant of
the problem arises when $d$ has many different realizations, and some of them
may be more desirable than others. We study \emph{optimized realization}
problems in which the goal is to compute a realization that optimizes some
quality measure. Efficient algorithms are known for the problems of finding a
realization with the maximum clique, the maximum independent set, or the
minimum vertex cover. In this paper, we focus on two problems for which such
algorithms were not known. The first is the Degree Realization with Minimum
Dominating Set problem, where the goal is to find a realization whose minimum
dominating set is minimized among all the realizations of the given sequence
$d$. The second is the Degree Realization with Maximum Matching problem, where
the goal is to find a realization with the largest matching among all the
realizations of $d$. We present polynomial time realization algorithms for
these two open problems.
  A related problem of interest and importance is \emph{characterizing} the
sequences with a given value of the optimized function. This leads to an
efficient computation of the optimized value without providing the realization
that achieves that value. For the Maximum Matching problem, a succinct
characterization of degree sequences with a maximum matching of a given size
was known. This paper provides a succinct characterization of sequences with
minimum dominating set of a given size.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [183] [Glaucoma Detection and Structured OCT Report Generation via a Fine-tuned Multimodal Large Language Model](https://arxiv.org/abs/2510.02403)
*Jalil Jalili,Yashraj Gavhane,Evan Walker,Anna Heinke,Christopher Bowd,Akram Belghith,Massimo A. Fazio,Christopher A. Girkin,C. Gustavo De Moraes,Jeffrey M. Liebmann,Sally L. Baxter,Robert N. Weinreb,Linda M. Zangwill,Mark Christopher*

Main category: q-bio.QM

TL;DR: 本文开发可解释的多模态大语言模型（MM - LLM）处理OCT扫描，在图像质量评估、青光眼检测和RNFL变薄分类任务中表现良好。


<details>
  <summary>Details</summary>
Motivation: 开发可解释的MM - LLM，用于OCT扫描质量筛选和生成包含青光眼诊断及RNFL变薄评估的结构化临床报告。

Method: 对Llama 3.2 Vision - Instruct模型进行微调，使用配对的OCT图像和自动生成的结构化临床报告作为训练数据，在保留测试集上评估模型。

Result: 模型在质量分类中准确率0.90、特异性0.98；青光眼检测准确率0.86；RNFL变薄预测准确率0.83 - 0.94；文本生成得分与参考报告高度一致。

Conclusion: 微调后的MM - LLM可根据OCT成像生成准确临床描述，在识别图像质量问题和检测青光眼方面准确率高，能提供RNFL变薄的扇形描述辅助临床评估。

Abstract: Objective: To develop an explainable multimodal large language model (MM-LLM)
that (1) screens optic nerve head (ONH) OCT circle scans for quality and (2)
generates structured clinical reports that include glaucoma diagnosis and
sector-wise retinal nerve fiber layer (RNFL) thinning assessments. Design:
Retrospective cohort study of 1,310 subjects contributing 43,849 Spectralis ONH
OCT circle scans (1,331 glaucomatous and 867 healthy eyes) from the DIGS and
ADAGES cohorts. Methods: A MM-LLM (Llama 3.2 Vision-Instruct model) was
fine-tuned to generate clinical descriptions of OCT imaging data. Training data
included paired OCT images and automatically generated, structured clinical
reports that described global and sectoral RNFL thinning. Poor-quality scans
were labeled as unusable and paired with a fixed refusal statement. The model
was evaluated on a held-out test set for three tasks: quality assessment,
glaucoma detection, and RNFL thinning classification across seven anatomical
sectors. Evaluation metrics included accuracy, sensitivity, specificity,
precision, and F1-score. Model description quality was also evaluated using
standard text evaluation metrics. Results: The model achieved 0.90 accuracy and
0.98 specificity for quality triage. For glaucoma detection, accuracy was 0.86
(sensitivity 0.91, specificity 0.73, F1-score 0.91). RNFL thinning prediction
accuracy ranged from 0.83 to 0.94, with highest performance in global and
temporal sectors. Text generation scores showed strong alignment with reference
reports (BLEU: 0.82; ROUGE-1: 0.94; ROUGE-2: 0.87; ROUGE-L: 0.92; BERTScore-F1:
0.99). Conclusions: The fine-tuned MM-LLM generated accurate clinical
descriptions based on OCT imaging. The model achieved high accuracy in
identifying image quality issues and detecting glaucoma. The model also
provided sectoral descriptions of RNFL thinning to help support clinical OCT
evaluation.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [184] [WEE-Therapy: A Mixture of Weak Encoders Framework for Psychological Counseling Dialogue Analysis](https://arxiv.org/abs/2510.02320)
*Yongqi Kang,Yong Zhao*

Main category: eess.AS

TL;DR: 提出多任务音频大模型WEE - Therapy处理咨询对话，评估显示其各任务表现佳且参数开销小。


<details>
  <summary>Details</summary>
Motivation: 现有音频大模型难以捕捉咨询对话领域特定特征，阻碍计算心理学发展。

Method: 提出WEE - Therapy，采用弱编码器集成（WEE）机制，用轻量级专业编码器补充强大的基础编码器，使用双路由策略。

Result: 在情感识别、技术分类、风险检测和总结任务上取得显著性能提升，参数开销最小。

Conclusion: WEE - Therapy在人工智能辅助临床分析方面有强大潜力。

Abstract: The advancement of computational psychology requires AI tools capable of
deeply understanding counseling dialogues. Existing audio language models
(AudioLLMs) often rely on single speech encoders pre-trained on general data,
struggling to capture domain-specific features like complex emotions and
professional techniques. To address this, we propose WEE-Therapy, a multi-task
AudioLLM incorporating a Weak Encoder Ensemble (WEE) mechanism. This
supplements a powerful base encoder with a pool of lightweight, specialized
encoders. A novel dual-routing strategy combines stable, data-independent
domain knowledge with dynamic, data-dependent expert selection. Evaluated on
emotion recognition, technique classification, risk detection, and
summarization, WEE-Therapy achieves significant performance gains across all
tasks with minimal parameter overhead, demonstrating strong potential for
AI-assisted clinical analysis.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [185] [A Study of Neural Polar Decoders for Communication](https://arxiv.org/abs/2510.03069)
*Rom Hirsch,Ziv Aharoni,Henry D. Pfister,Haim H. Permuter*

Main category: eess.SP

TL;DR: 本文对用于端到端通信系统的神经极化解码器（NPD）进行调整和分析，实验表明其性能优于5G极化解码器。


<details>
  <summary>Details</summary>
Motivation: 以往研究证明NPD在合成信道有效，本研究将其拓展到实际通信系统。

Method: 对NPD进行调整以适配OFDM和单载波通信系统，通过速率匹配等支持任意码长、高阶调制和不同信道条件。

Result: 在5G信道实验中，NPD在BER、BLER和吞吐量上始终优于5G极化解码器，用于单载波系统时性能与OFDM相当且PAPR更低。

Conclusion: NPD是一种高性能、无导频且稳健的解码解决方案。

Abstract: In this paper, we adapt and analyze Neural Polar Decoders (NPDs) for
end-to-end communication systems. While prior work demonstrated the
effectiveness of NPDs on synthetic channels, this study extends the NPD to
real-world communication systems. The NPD was adapted to complete OFDM and
single-carrier communication systems. To satisfy practical system requirements,
the NPD is extended to support any code length via rate matching, higher-order
modulations, and robustness across diverse channel conditions. The NPD operates
directly on channels with memory, exploiting their structure to achieve higher
data rates without requiring pilots and a cyclic prefix. Although NPD entails
higher computational complexity than the standard 5G polar decoder, its neural
network architecture enables an efficient representation of channel statistics,
resulting in manageable complexity suitable for practical systems. Experimental
results over 5G channels demonstrate that the NPD consistently outperforms the
5G polar decoder in terms of BER, BLER, and throughput. These improvements are
particularly significant for low-rate and short-block configurations, which are
prevalent in 5G control channels. Furthermore, NPDs applied to single-carrier
systems offer performance comparable to OFDM with lower PAPR, enabling
effective single-carrier transmission over 5G channels. These results position
the NPD as a high-performance, pilotless, and robust decoding solution.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [186] [Hybrid Schemes of NIST Post-Quantum Cryptography Standard Algorithms and Quantum Key Distribution for Key Exchange and Digital Signature](https://arxiv.org/abs/2510.02379)
*Abel C. H. Chen*

Main category: cs.CR

TL;DR: 本文提出将QKD与NIST标准化PQC算法结合的混合方案，包含密钥交换协议和数字签名方案，并对方案进行评估。


<details>
  <summary>Details</summary>
Motivation: PQC和QKD各有优缺点，可相互补充，近期研究提出结合二者的混合方案，因此本文进行相关研究。

Method: 提出混合QKD - PQC密钥交换协议和混合QKD - PQC数字签名方案。密钥交换协议结合ML - KEM与QKD协议；数字签名方案利用ML - DSA和SLH - DSA生成签名重建值，用BB84和E91协议传输的确认码验证。

Result: 文中未明确提及具体结果。

Conclusion: 文中未明确提及具体结论。

Abstract: Since the security of post-quantum cryptography (PQC) algorithms is based on
the hardness of mathematical problems, while the security of quantum key
distribution (QKD) relies on the fundamental principles of quantum physics,
each approach possesses distinct advantages and limitations that can complement
one another. Consequently, recent studies have proposed hybrid schemes that
combine QKD and PQC to establish a dual-layered security model. In response to
this trend, this study proposes hybrid schemes that integrate QKD with the
National Institute of Standards and Technology (NIST) standardized PQC
algorithms. These hybrid schemes include two core components: a hybrid QKD-PQC
key exchange protocol and a hybrid QKD-PQC digital signature scheme. For the
hybrid key exchange protocol, this study combines Module-Lattice-based Key
Encapsulation Mechanisms (ML-KEM) with QKD protocols, specifically BB84 and
E91, to construct a secure key exchange protocol. In the design of the hybrid
digital signature scheme, this study utilizes Module-Lattice-based Digital
Signature Algorithms (ML-DSA) and Stateless Hash-based Digital Signature
Algorithms (SLH-DSA) to generate signature reconstruction values. These values
are verified using confirmation codes transmitted via the BB84 and E91
protocols. The proposed hybrid key exchange protocol is evaluated by examining
the shared secret key it produces, particularly with respect to entropy and
whether the output is independent and identically distributed (IID).
Furthermore, the computation time and message lengths of the proposed hybrid
schemes are evaluated.

</details>


### [187] [Federated Spatiotemporal Graph Learning for Passive Attack Detection in Smart Grids](https://arxiv.org/abs/2510.02371)
*Bochra Al Agha,Razane Tajeddine*

Main category: cs.CR

TL;DR: 本文提出以图为中心的多模态检测器检测智能电网被动攻击，使用两阶段编码器，在联邦学习设置下训练，在合成数据集上取得高准确率，适合非独立同分布的联邦智能电网部署。


<details>
  <summary>Details</summary>
Motivation: 智能电网面临被动窃听威胁，其信号微弱且检测困难，需有效检测方法。

Method: 引入以图为中心的多模态检测器，融合物理层和行为指标；使用两阶段编码器，结合图卷积和双向GRU；在FedProx下进行联邦学习训练；生成合成数据集。

Result: 模型在测试中达到每时间步98.32%的准确率（F1_attack = 0.972）和每序列93.35%的准确率，误报率0.15%。

Conclusion: 结合时空上下文能可靠检测隐蔽侦察，保持低误报率，适用于非独立同分布的联邦智能电网部署。

Abstract: Smart grids are exposed to passive eavesdropping, where attackers listen
silently to communication links. Although no data is actively altered, such
reconnaissance can reveal grid topology, consumption patterns, and operational
behavior, creating a gateway to more severe targeted attacks. Detecting this
threat is difficult because the signals it produces are faint, short-lived, and
often disappear when traffic is examined by a single node or along a single
timeline. This paper introduces a graph-centric, multimodal detector that fuses
physical-layer and behavioral indicators over ego-centric star subgraphs and
short temporal windows to detect passive attacks. To capture stealthy
perturbations, a two-stage encoder is introduced: graph convolution aggregates
spatial context across ego-centric star subgraphs, while a bidirectional GRU
models short-term temporal dependencies. The encoder transforms heterogeneous
features into a unified spatio-temporal representation suitable for
classification. Training occurs in a federated learning setup under FedProx,
improving robustness to heterogeneous local raw data and contributing to the
trustworthiness of decentralized training; raw measurements remain on client
devices. A synthetic, standards-informed dataset is generated to emulate
heterogeneous HAN/NAN/WAN communications with wireless-only passive
perturbations, event co-occurrence, and leak-safe splits. The model achieves a
testing accuracy of 98.32% per-timestep (F1_{attack}=0.972) and 93.35%
per-sequence at 0.15% FPR using a simple decision rule with run-length m=2 and
threshold $\tau=0.55$. The results demonstrate that combining spatial and
temporal context enables reliable detection of stealthy reconnaissance while
maintaining low false-positive rates, making the approach suitable for non-IID
federated smart-grid deployments.

</details>


### [188] [PolyLink: A Blockchain Based Decentralized Edge AI Platform for LLM Inference](https://arxiv.org/abs/2510.02395)
*Hongbo Liu,Jiannong Cao,Bo Yang,Dongbin Bai,Yinfeng Cao,Xiaoming Shen,Yinan Zhang,Jinwen Liang,Shan Jiang,Mingjin Zhang*

Main category: cs.CR

TL;DR: 提出基于区块链的去中心化AI平台PolyLink，解决大语言模型服务中心化问题，经评估有实用延迟和安全保障。


<details>
  <summary>Details</summary>
Motivation: 大语言模型服务的部署和使用高度中心化，给终端用户和开发者带来信任问题和成本。

Method: 提出去中心化众包架构支持跨设备部署和推理；设计TIQE协议确保推理完整性；集成基于代币的激励模型。

Result: 推理和验证延迟实用，系统能抵御模型退化攻击和验证者腐败。

Conclusion: PolyLink能有效解决大语言模型服务中心化问题，已开源可用。

Abstract: The rapid advancement of large language models (LLMs) in recent years has
revolutionized the AI landscape. However, the deployment model and usage of LLM
services remain highly centralized, creating significant trust issues and costs
for end users and developers. To address these issues, we propose PolyLink, a
blockchain-based decentralized AI platform that decentralizes LLM development
and inference. Specifically, PolyLink introduces a decentralized crowdsourcing
architecture that supports single-device and cross-device model deployment and
inference across heterogeneous devices at the edge. Moreover, to ensure the
inference integrity, we design the TIQE protocol, which combines a lightweight
cross-encoder model and an LLM-as-a-Judge for a high-accuracy inference
evaluation. Lastly, we integrate a comprehensive token-based incentive model
with dynamic pricing and reward mechanisms for all participants. We have
deployed PolyLink and conducted an extensive real-world evaluation through
geo-distributed deployment across heterogeneous devices. Results indicate that
the inference and verification latency is practical. Our security analysis
demonstrates that the system is resistant to model degradation attacks and
validator corruptions. PolyLink is now available at
https://github.com/IMCL-PolyLink/PolyLink.

</details>


### [189] [Modeling the Attack: Detecting AI-Generated Text by Quantifying Adversarial Perturbations](https://arxiv.org/abs/2510.02319)
*Lekkala Sai Teja,Annepaka Yadagiri,Sangam Sai Anish,Siva Gopala Krishna Nuthakki,Partha Pakray*

Main category: cs.CR

TL;DR: 本文对比研究对抗鲁棒性，指出标准对抗训练局限，提出PIFE框架，该框架表现优于传统方法，表明显式建模扰动伪影是实现鲁棒性的更优途径。


<details>
  <summary>Details</summary>
Motivation: 先进大语言模型带来双重使用问题，现有AI生成文本检测系统易受对抗攻击，需创建可靠检测系统。

Method: 先量化标准对抗训练的局限，再引入PIFE框架，将输入文本通过多阶段归一化管道转化为标准形式，用指标量化转换幅度并将信号输入分类器，对传统强化Transformer和PIFE增强模型进行评估。

Result: 传统对抗训练对语义攻击效果不佳，真阳性率降至48.8%；PIFE模型在相同条件下真阳性率达82.6%，能有效应对复杂语义攻击。

Conclusion: 显式建模扰动伪影而非仅基于其训练，是在对抗竞赛中实现真正鲁棒性的更有前景的方法。

Abstract: The growth of highly advanced Large Language Models (LLMs) constitutes a huge
dual-use problem, making it necessary to create dependable AI-generated text
detection systems. Modern detectors are notoriously vulnerable to adversarial
attacks, with paraphrasing standing out as an effective evasion technique that
foils statistical detection. This paper presents a comparative study of
adversarial robustness, first by quantifying the limitations of standard
adversarial training and then by introducing a novel, significantly more
resilient detection framework: Perturbation-Invariant Feature Engineering
(PIFE), a framework that enhances detection by first transforming input text
into a standardized form using a multi-stage normalization pipeline, it then
quantifies the transformation's magnitude using metrics like Levenshtein
distance and semantic similarity, feeding these signals directly to the
classifier. We evaluate both a conventionally hardened Transformer and our
PIFE-augmented model against a hierarchical taxonomy of character-, word-, and
sentence-level attacks. Our findings first confirm that conventional
adversarial training, while resilient to syntactic noise, fails against
semantic attacks, an effect we term "semantic evasion threshold", where its
True Positive Rate at a strict 1% False Positive Rate plummets to 48.8%. In
stark contrast, our PIFE model, which explicitly engineers features from the
discrepancy between a text and its canonical form, overcomes this limitation.
It maintains a remarkable 82.6% TPR under the same conditions, effectively
neutralizing the most sophisticated semantic attacks. This superior performance
demonstrates that explicitly modeling perturbation artifacts, rather than
merely training on them, is a more promising path toward achieving genuine
robustness in the adversarial arms race.

</details>


### [190] [Agentic-AI Healthcare: Multilingual, Privacy-First Framework with MCP Agents](https://arxiv.org/abs/2510.02325)
*Mohammed A. Shehab*

Main category: cs.CR

TL;DR: 介绍了隐私感知、多语言且可解释的研究原型Agentic - AI Healthcare，展示其应用及可行性。


<details>
  <summary>Details</summary>
Motivation: 探索在医疗应用中结合智能体编排、多语言可访问性和合规架构的可行性。

Method: 利用新兴的Model Context Protocol (MCP)编排多个智能体进行患者交互，集成隐私和合规层，采用基于角色的访问控制、AES - GCM字段级加密和防篡改审计日志。

Result: 实现了多语言患者 - 医生交互和基于大语言模型的透明诊断推理。

Conclusion: 证明了在医疗应用中结合智能体编排、多语言可访问性和合规架构是可行的，该平台为研究原型，非认证医疗设备。

Abstract: This paper introduces Agentic-AI Healthcare, a privacy-aware, multilingual,
and explainable research prototype developed as a single-investigator project.
The system leverages the emerging Model Context Protocol (MCP) to orchestrate
multiple intelligent agents for patient interaction, including symptom
checking, medication suggestions, and appointment scheduling. The platform
integrates a dedicated Privacy and Compliance Layer that applies role-based
access control (RBAC), AES-GCM field-level encryption, and tamper-evident audit
logging, aligning with major healthcare data protection standards such as HIPAA
(US), PIPEDA (Canada), and PHIPA (Ontario). Example use cases demonstrate
multilingual patient-doctor interaction (English, French, Arabic) and
transparent diagnostic reasoning powered by large language models. As an
applied AI contribution, this work highlights the feasibility of combining
agentic orchestration, multilingual accessibility, and compliance-aware
architecture in healthcare applications. This platform is presented as a
research prototype and is not a certified medical device.

</details>


### [191] [CATMark: A Context-Aware Thresholding Framework for Robust Cross-Task Watermarking in Large Language Models](https://arxiv.org/abs/2510.02342)
*Yu Zhang,Shuliang Liu,Xu Yang,Xuming Hu*

Main category: cs.CR

TL;DR: 提出上下文感知阈值水印框架，可动态调整水印强度，实验表明其能在跨任务中提升文本质量且不牺牲检测精度。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型水印算法嵌入水印导致文本质量下降，依赖熵阈值的方法需大量计算资源调优且适应性差。

Method: 提出Context-Aware Threshold watermarking框架，用logits聚类将文本生成划分为语义状态，建立上下文感知的熵阈值。

Result: 该框架在跨任务中提升了文本质量，且不牺牲检测精度。

Conclusion: 所提框架无需预定义阈值和特定任务调优，能有效改善大语言模型水印算法的性能。

Abstract: Watermarking algorithms for Large Language Models (LLMs) effectively identify
machine-generated content by embedding and detecting hidden statistical
features in text. However, such embedding leads to a decline in text quality,
especially in low-entropy scenarios where performance needs improvement.
Existing methods that rely on entropy thresholds often require significant
computational resources for tuning and demonstrate poor adaptability to unknown
or cross-task generation scenarios. We propose \textbf{C}ontext-\textbf{A}ware
\textbf{T}hreshold watermarking ($\myalgo$), a novel framework that dynamically
adjusts watermarking intensity based on real-time semantic context. $\myalgo$
partitions text generation into semantic states using logits clustering,
establishing context-aware entropy thresholds that preserve fidelity in
structured content while embedding robust watermarks. Crucially, it requires no
pre-defined thresholds or task-specific tuning. Experiments show $\myalgo$
improves text quality in cross-tasks without sacrificing detection accuracy.

</details>


### [192] [An Investigation into the Performance of Non-Contrastive Self-Supervised Learning Methods for Network Intrusion Detection](https://arxiv.org/abs/2510.02349)
*Hamed Fard,Tobias Schalau,Gerhard Wunder*

Main category: cs.CR

TL;DR: 本文对比五种非对比自监督学习方法在网络入侵检测中的性能，在两个数据集上进行90次实验，展示非对比方法检测攻击的竞争力。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习在网络入侵检测中只能检测已知异常，自监督学习在计算机视觉的成功促使其应用于网络入侵检测，此前非对比方法的有效性不明。

Method: 对比五种非对比自监督学习方法，使用三种编码器架构和六种增强策略，在UNSW - NB15和5G - NIDD数据集上进行90次实验。

Result: 报告每个自监督模型中编码器架构和增强方法组合的最高平均精度、召回率、F1分数和AUCROC。

Conclusion: 非对比自监督学习方法在攻击检测上具有竞争力。

Abstract: Network intrusion detection, a well-explored cybersecurity field, has
predominantly relied on supervised learning algorithms in the past two decades.
However, their limitations in detecting only known anomalies prompt the
exploration of alternative approaches. Motivated by the success of
self-supervised learning in computer vision, there is a rising interest in
adapting this paradigm for network intrusion detection. While prior research
mainly delved into contrastive self-supervised methods, the efficacy of
non-contrastive methods, in conjunction with encoder architectures serving as
the representation learning backbone and augmentation strategies that determine
what is learned, remains unclear for effective attack detection. This paper
compares the performance of five non-contrastive self-supervised learning
methods using three encoder architectures and six augmentation strategies.
Ninety experiments are systematically conducted on two network intrusion
detection datasets, UNSW-NB15 and 5G-NIDD. For each self-supervised model, the
combination of encoder architecture and augmentation method yielding the
highest average precision, recall, F1-score, and AUCROC is reported.
Furthermore, by comparing the best-performing models to two unsupervised
baselines, DeepSVDD, and an Autoencoder, we showcase the competitiveness of the
non-contrastive methods for attack detection. Code at:
https://github.com/renje4z335jh4/non_contrastive_SSL_NIDS

</details>


### [193] [Measuring Physical-World Privacy Awareness of Large Language Models: An Evaluation Benchmark](https://arxiv.org/abs/2510.02356)
*Xinjie Shen,Mufei Li,Pan Li*

Main category: cs.CR

TL;DR: 引入EAPrivacy评估基准测试大语言模型在现实世界的隐私意识，发现当前模型存在严重不足，需更强大的物理感知对齐。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法局限于自然语言场景，大语言模型应用于具身智能体时需衡量其在现实世界的隐私意识。

Method: 引入EAPrivacy评估基准，利用四个层级的程序生成场景测试模型。

Result: 当前模型存在严重不足，如Gemini 2.5 Pro在环境变化场景中准确率仅59%，模型在多数情况下优先完成任务而非遵循隐私约束，在高风险场景中部分模型超15%的时间无视社会规范。

Conclusion: 大语言模型在物理隐私方面存在根本偏差，需要更强大的物理感知对齐。

Abstract: The deployment of Large Language Models (LLMs) in embodied agents creates an
urgent need to measure their privacy awareness in the physical world. Existing
evaluation methods, however, are confined to natural language based scenarios.
To bridge this gap, we introduce EAPrivacy, a comprehensive evaluation
benchmark designed to quantify the physical-world privacy awareness of
LLM-powered agents. EAPrivacy utilizes procedurally generated scenarios across
four tiers to test an agent's ability to handle sensitive objects, adapt to
changing environments, balance task execution with privacy constraints, and
resolve conflicts with social norms. Our measurements reveal a critical deficit
in current models. The top-performing model, Gemini 2.5 Pro, achieved only 59\%
accuracy in scenarios involving changing physical environments. Furthermore,
when a task was accompanied by a privacy request, models prioritized completion
over the constraint in up to 86\% of cases. In high-stakes situations pitting
privacy against critical social norms, leading models like GPT-4o and
Claude-3.5-haiku disregarded the social norm over 15\% of the time. These
findings, demonstrated by our benchmark, underscore a fundamental misalignment
in LLMs regarding physically grounded privacy and establish the need for more
robust, physically-aware alignment.

</details>


### [194] [Privacy in the Age of AI: A Taxonomy of Data Risks](https://arxiv.org/abs/2510.02357)
*Grace Billiris,Asif Gill,Madhushi Bandara*

Main category: cs.CR

TL;DR: 本文对AI隐私风险进行分类，识别出19种关键风险，揭示分布情况，挑战传统安全方法，助力可信AI发展。


<details>
  <summary>Details</summary>
Motivation: AI系统带来隐私挑战，传统隐私框架对AI技术不适用。

Method: 通过系统综述45项研究，综合得出AI隐私风险的分类法。

Result: 确定19种关键风险，分四类，各维度分布均衡，人为错误是最显著因素。

Conclusion: 该分类法挑战传统方法，弥合AI隐私技术与行为维度，为可信AI发展及未来研究奠定基础。

Abstract: Artificial Intelligence (AI) systems introduce unprecedented privacy
challenges as they process increasingly sensitive data. Traditional privacy
frameworks prove inadequate for AI technologies due to unique characteristics
such as autonomous learning and black-box decision-making. This paper presents
a taxonomy classifying AI privacy risks, synthesised from 45 studies identified
through systematic review. We identify 19 key risks grouped under four
categories: Dataset-Level, Model-Level, Infrastructure-Level, and Insider
Threat Risks. Findings reveal a balanced distribution across these dimensions,
with human error (9.45%) emerging as the most significant factor. This taxonomy
challenges conventional security approaches that typically prioritise technical
controls over human factors, highlighting gaps in holistic understanding. By
bridging technical and behavioural dimensions of AI privacy, this paper
contributes to advancing trustworthy AI development and provides a foundation
for future research.

</details>


### [195] [A-MemGuard: A Proactive Defense Framework for LLM-Based Agent Memory](https://arxiv.org/abs/2510.02373)
*Qianshan Wei,Tengchao Yang,Yaochen Wang,Xinfeng Li,Lijun Li,Zhenfei Yin,Yi Zhan,Thorsten Holz,Zhiqiang Lin,XiaoFeng Wang*

Main category: cs.CR

TL;DR: 本文指出大语言模型（LLM）代理内存存在被注入恶意记录操控行为的安全风险，提出A - MemGuard防御框架，经评估有效降低攻击成功率且成本低。


<details>
  <summary>Details</summary>
Motivation: LLM代理依赖内存学习，但内存易被注入恶意记录操控未来行为，存在安全风险。

Method: 提出A - MemGuard框架，结合基于共识的验证机制和双内存结构，不修改代理核心架构。

Result: 在多个基准测试中，A - MemGuard有效将攻击成功率降低95%以上，且效用成本极小。

Conclusion: 将LLM内存安全从静态过滤转变为主动、基于经验的模型，防御能力随时间增强。代码已开源。

Abstract: Large Language Model (LLM) agents use memory to learn from past interactions,
enabling autonomous planning and decision-making in complex environments.
However, this reliance on memory introduces a critical security risk: an
adversary can inject seemingly harmless records into an agent's memory to
manipulate its future behavior. This vulnerability is characterized by two core
aspects: First, the malicious effect of injected records is only activated
within a specific context, making them hard to detect when individual memory
entries are audited in isolation. Second, once triggered, the manipulation can
initiate a self-reinforcing error cycle: the corrupted outcome is stored as
precedent, which not only amplifies the initial error but also progressively
lowers the threshold for similar attacks in the future. To address these
challenges, we introduce A-MemGuard (Agent-Memory Guard), the first proactive
defense framework for LLM agent memory. The core idea of our work is the
insight that memory itself must become both self-checking and self-correcting.
Without modifying the agent's core architecture, A-MemGuard combines two
mechanisms: (1) consensus-based validation, which detects anomalies by
comparing reasoning paths derived from multiple related memories and (2) a
dual-memory structure, where detected failures are distilled into ``lessons''
stored separately and consulted before future actions, breaking error cycles
and enabling adaptation. Comprehensive evaluations on multiple benchmarks show
that A-MemGuard effectively cuts attack success rates by over 95% while
incurring a minimal utility cost. This work shifts LLM memory security from
static filtering to a proactive, experience-driven model where defenses
strengthen over time. Our code is available in
https://github.com/TangciuYueng/AMemGuard

</details>


### [196] [A Hybrid CAPTCHA Combining Generative AI with Keystroke Dynamics for Enhanced Bot Detection](https://arxiv.org/abs/2510.02374)
*Ayda Aghaei Nia*

Main category: cs.CR

TL;DR: 本文提出结合大语言模型认知挑战和按键动力学行为生物特征分析的新型混合验证码系统，实验显示该系统在检测机器人方面准确性高且可用性强。


<details>
  <summary>Details</summary>
Motivation: 传统验证码在可用性和抵御人工智能机器人攻击之间存在权衡问题，需要改进。

Method: 生成对人类简单、对自动化代理复杂的动态问题，同时分析用户打字节奏以区分人类和机器人输入；介绍系统架构，规范按键分析特征提取方法。

Result: 双层方法在机器人检测中准确率高，能抵御粘贴和脚本模拟攻击，且在人类参与者中可用性得分高。

Conclusion: 结合认知和行为测试有潜力创建更安全、用户友好的新一代验证码。

Abstract: Completely Automated Public Turing tests to tell Computers and Humans Apart
(CAPTCHAs) are a foundational component of web security, yet traditional
implementations suffer from a trade-off between usability and resilience
against AI-powered bots. This paper introduces a novel hybrid CAPTCHA system
that synergizes the cognitive challenges posed by Large Language Models (LLMs)
with the behavioral biometric analysis of keystroke dynamics. Our approach
generates dynamic, unpredictable questions that are trivial for humans but
non-trivial for automated agents, while simultaneously analyzing the user's
typing rhythm to distinguish human patterns from robotic input. We present the
system's architecture, formalize the feature extraction methodology for
keystroke analysis, and report on an experimental evaluation. The results
indicate that our dual-layered approach achieves a high degree of accuracy in
bot detection, successfully thwarting both paste-based and script-based
simulation attacks, while maintaining a high usability score among human
participants. This work demonstrates the potential of combining cognitive and
behavioral tests to create a new generation of more secure and user-friendly
CAPTCHAs.

</details>


### [197] [Scaling Homomorphic Applications in Deployment](https://arxiv.org/abs/2510.02376)
*Ryan Marinelli,Angelica Chowdhury*

Main category: cs.CR

TL;DR: 开发概念验证同态应用评估加密生态系统生产就绪度，以电影推荐应用为例并优化部署。


<details>
  <summary>Details</summary>
Motivation: 确定加密生态系统的生产就绪度。

Method: 开发电影推荐应用，通过容器化和编排进行生产化，调整部署配置并进行额外基础设施优化。

Result: 通过额外基础设施优化缓解了全同态加密（FHE）的计算限制。

Conclusion: 未明确提及，但暗示通过这些方法可评估加密生态系统的生产就绪度。

Abstract: In this endeavor, a proof-of-concept homomorphic application is developed to
determine the production readiness of encryption ecosystems. A movie
recommendation app is implemented for this purpose and productionized through
containerization and orchestration. By tuning deployment configurations, the
computational limitations of Fully Homomorphic Encryption (FHE) are mitigated
through additional infrastructure optimizations
  Index Terms: Reinforcement Learning, Orchestration, Homomorphic Encryption

</details>


### [198] [On The Fragility of Benchmark Contamination Detection in Reasoning Models](https://arxiv.org/abs/2510.02386)
*Han Wang,Haoyu Li,Brian Ko,Huan Zhang*

Main category: cs.CR

TL;DR: 研究发现大语言模型（LRMs）规避基准污染检测很容易，揭示评估漏洞，强调需先进检测方法和可信评估协议。


<details>
  <summary>Details</summary>
Motivation: 当前LRMs排行榜促使开发者优化基准套件，存在基准污染问题，需研究污染检测情况。

Method: 研究污染可能出现的两种场景，进行实证实验和理论分析。

Result: 场景一：SFT阶段污染可被检测，但GRPO训练能隐藏污染信号；场景二：含CoT的SFT污染用于高级LRMs时，多数检测方法近乎随机猜测。

Conclusion: LRMs评估存在独特漏洞，需先进污染检测方法和可信评估协议。

Abstract: Leaderboards for LRMs have turned evaluation into a competition,
incentivizing developers to optimize directly on benchmark suites. A shortcut
to achieving higher rankings is to incorporate evaluation benchmarks into the
training data, thereby yielding inflated performance, known as benchmark
contamination. Surprisingly, our studies find that evading contamination
detections for LRMs is alarmingly easy. We focus on the two scenarios where
contamination may occur in practice: (I) when the base model evolves into LRM
via SFT and RL, we find that contamination during SFT can be originally
identified by contamination detection methods. Yet, even a brief GRPO training
can markedly conceal contamination signals that most detection methods rely on.
Further empirical experiments and theoretical analysis indicate that PPO style
importance sampling and clipping objectives are the root cause of this
detection concealment, indicating that a broad class of RL methods may
inherently exhibit similar concealment capability; (II) when SFT contamination
with CoT is applied to advanced LRMs as the final stage, most contamination
detection methods perform near random guesses. Without exposure to non-members,
contaminated LRMs would still have more confidence when responding to those
unseen samples that share similar distributions to the training set, and thus,
evade existing memorization-based detection methods. Together, our findings
reveal the unique vulnerability of LRMs evaluations: Model developers could
easily contaminate LRMs to achieve inflated leaderboards performance while
leaving minimal traces of contamination, thereby strongly undermining the
fairness of evaluation and threatening the integrity of public leaderboards.
This underscores the urgent need for advanced contamination detection methods
and trustworthy evaluation protocols tailored to LRMs.

</details>


### [199] [Dynamic Target Attack](https://arxiv.org/abs/2510.02422)
*Kedong Xiu,Churui Zeng,Tianhang Zheng,Xinzhe Huang,Xiaojun Jia,Di Wang,Puning Zhao,Zhan Qin,Kui Ren*

Main category: cs.CR

TL;DR: 提出动态目标攻击（DTA）越狱框架，降低目标与输出分布差异，实验表明其在白盒和黑盒设置下有效性和效率均优于现有攻击方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度的越狱攻击使用固定目标，与原始输出差异大，优化对抗提示需大量迭代且可能失败。

Method: 提出DTA框架，在每次优化轮次中从当前提示的输出分布中迭代采样多个候选响应，选择最有害响应作为临时目标进行提示优化。

Result: 白盒设置下，DTA仅需200次优化迭代，平均攻击成功率超87%，时间成本比现有基线少2 - 26倍；黑盒设置下，使用Llama - 3 - 8B - Instruct作为替代模型，对Llama - 3 - 70B - Instruct攻击成功率达85%。

Conclusion: DTA在越狱攻击中具有显著的有效性和效率优势，优于现有攻击方法。

Abstract: Existing gradient-based jailbreak attacks typically optimize an adversarial
suffix to induce a fixed affirmative response. However, this fixed target
usually resides in an extremely low-density region of a safety-aligned LLM's
output distribution conditioned on diverse harmful inputs. Due to the
substantial discrepancy between the target and the original output, existing
attacks require numerous iterations to optimize the adversarial prompt, which
might still fail to induce the low-probability target response from the target
LLM. In this paper, we propose Dynamic Target Attack (DTA), a new jailbreaking
framework relying on the target LLM's own responses as targets to optimize the
adversarial prompts. In each optimization round, DTA iteratively samples
multiple candidate responses directly from the output distribution conditioned
on the current prompt, and selects the most harmful response as a temporary
target for prompt optimization. In contrast to existing attacks, DTA
significantly reduces the discrepancy between the target and the output
distribution, substantially easing the optimization process to search for an
effective adversarial prompt.
  Extensive experiments demonstrate the superior effectiveness and efficiency
of DTA: under the white-box setting, DTA only needs 200 optimization iterations
to achieve an average attack success rate (ASR) of over 87\% on recent
safety-aligned LLMs, exceeding the state-of-the-art baselines by over 15\%. The
time cost of DTA is 2-26 times less than existing baselines. Under the
black-box setting, DTA uses Llama-3-8B-Instruct as a surrogate model for target
sampling and achieves an ASR of 85\% against the black-box target model
Llama-3-70B-Instruct, exceeding its counterparts by over 25\%.

</details>


### [200] [ToolTweak: An Attack on Tool Selection in LLM-based Agents](https://arxiv.org/abs/2510.02554)
*Jonathan Sneh,Ruomei Yan,Jialin Yu,Philip Torr,Yarin Gal,Sunando Sengupta,Eric Sommerlade,Alasdair Paren,Adel Bibi*

Main category: cs.CR

TL;DR: 论文指出大语言模型驱动的智能体在工具选择过程存在漏洞，提出ToolTweak攻击增加工具选择率，揭示风险并评估两种防御方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型驱动的智能体与外部工具交互增多，工具选择过程可能存在漏洞，影响工具生态的公平性、竞争性和安全性。

Method: 提出ToolTweak自动攻击，迭代操纵工具名称和描述；评估了释义和困惑度过滤两种防御方法。

Result: ToolTweak攻击可将工具选择率从约20%提高到81%，且在开源和闭源模型间有强可迁移性；两种防御方法可减少偏差，使智能体更公平地选择功能相似工具。

Conclusion: 工具选择过程存在关键漏洞，会对新兴工具生态造成风险，两种防御方法可缓解这些风险。

Abstract: As LLMs increasingly power agents that interact with external tools, tool use
has become an essential mechanism for extending their capabilities. These
agents typically select tools from growing databases or marketplaces to solve
user tasks, creating implicit competition among tool providers and developers
for visibility and usage. In this paper, we show that this selection process
harbors a critical vulnerability: by iteratively manipulating tool names and
descriptions, adversaries can systematically bias agents toward selecting
specific tools, gaining unfair advantage over equally capable alternatives. We
present ToolTweak, a lightweight automatic attack that increases selection
rates from a baseline of around 20% to as high as 81%, with strong
transferability between open-source and closed-source models. Beyond individual
tools, we show that such attacks cause distributional shifts in tool usage,
revealing risks to fairness, competition, and security in emerging tool
ecosystems. To mitigate these risks, we evaluate two defenses: paraphrasing and
perplexity filtering, which reduce bias and lead agents to select functionally
similar tools more equally. All code will be open-sourced upon acceptance.

</details>


### [201] [LLM-Generated Samples for Android Malware Detection](https://arxiv.org/abs/2510.02391)
*Nik Rollinson,Nikolaos Polatidis*

Main category: cs.CR

TL;DR: 研究用微调GPT - 4.1 - mini生成三种恶意软件家族数据，评估不同训练设置下分类器效果，发现合成数据可增强稀缺数据集但不能单独作为训练源。


<details>
  <summary>Details</summary>
Motivation: 安卓恶意软件不断演变，签名防御和机器学习模型面临挑战，合成数据可缓解数据稀缺问题，但大语言模型在生成有效恶意软件数据方面研究不足。

Method: 用KronoDroid数据集微调GPT - 4.1 - mini生成三种恶意软件家族结构化记录，通过提示工程和后处理解决生成不一致问题，评估三种训练设置下的多个分类器。

Result: 仅用真实数据训练接近完美检测，合成数据增强训练性能略降，仅用合成数据训练效果不一。

Conclusion: 大语言模型生成的恶意软件数据可增强稀缺数据集且不影响检测精度，但不能单独作为训练源。

Abstract: Android malware continues to evolve through obfuscation and polymorphism,
posing challenges for both signature-based defenses and machine learning models
trained on limited and imbalanced datasets. Synthetic data has been proposed as
a remedy for scarcity, yet the role of large language models (LLMs) in
generating effective malware data for detection tasks remains underexplored. In
this study, we fine-tune GPT-4.1-mini to produce structured records for three
malware families: BankBot, Locker/SLocker, and Airpush/StopSMS, using the
KronoDroid dataset. After addressing generation inconsistencies with prompt
engineering and post-processing, we evaluate multiple classifiers under three
settings: training with real data only, real-plus-synthetic data, and synthetic
data alone. Results show that real-only training achieves near perfect
detection, while augmentation with synthetic data preserves high performance
with only minor degradations. In contrast, synthetic-only training produces
mixed outcomes, with effectiveness varying across malware families and
fine-tuning strategies. These findings suggest that LLM-generated malware can
enhance scarce datasets without compromising detection accuracy, but remains
insufficient as a standalone training source.

</details>


### [202] [Adaptive Deception Framework with Behavioral Analysis for Enhanced Cybersecurity Defense](https://arxiv.org/abs/2510.02424)
*Basil Abdullah AL-Zahrani*

Main category: cs.CR

TL;DR: 本文提出CADL自适应欺骗框架，在CICIDS2017数据集上检测率达99.88%，误报率0.13%，优于传统系统，还提供开源实现。


<details>
  <summary>Details</summary>
Motivation: 提出更有效的网络入侵检测和响应框架，替代成本高昂的商业欺骗平台。

Method: 采用集成机器学习（随机森林、XGBoost、神经网络）结合行为分析，通过协调信号总线架构共享情报，根据时间模式分析攻击者并部署五级欺骗策略。

Result: 在50000个CICIDS2017测试样本上检测率高，误报率低，行为分析对攻击者分类准确率达89%，显著优于传统入侵检测系统。

Conclusion: CADL框架有效且实用，提供开源实现和透明指标，是商业欺骗平台的可行替代方案。

Abstract: This paper presents CADL (Cognitive-Adaptive Deception Layer), an adaptive
deception framework achieving 99.88% detection rate with 0.13% false positive
rate on the CICIDS2017 dataset. The framework employs ensemble machine learning
(Random Forest, XGBoost, Neural Networks) combined with behavioral profiling to
identify and adapt responses to network intrusions. Through a coordinated
signal bus architecture, security components share real-time intelligence,
enabling collective decision-making. The system profiles attackers based on
temporal patterns and deploys customized deception strategies across five
escalation levels. Evaluation on 50,000 CICIDS2017 test samples demonstrates
that CADL significantly outperforms traditional intrusion detection systems
(Snort: 71.2%, Suricata: 68.5%) while maintaining production-ready false
positive rates. The framework's behavioral analysis achieves 89% accuracy in
classifying attacker profiles. We provide open-source implementation and
transparent performance metrics, offering an accessible alternative to
commercial deception platforms costing $150-400 per host annually.

</details>


### [203] [A Statistical Method for Attack-Agnostic Adversarial Attack Detection with Compressive Sensing Comparison](https://arxiv.org/abs/2510.02707)
*Chinthana Wimalasuriya,Spyros Tragoudas*

Main category: cs.CR

TL;DR: 提出统计方法实现实时对抗检测，测试效果好且降低误报率


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击检测方法难以检测未知攻击或高精度检测不同类型攻击

Method: 在神经网络部署前建立检测基线，通过比较压缩/未压缩神经网络对的行为生成对抗存在度量

Result: 与最先进技术对比，在多种攻击类型上接近完美检测，显著降低误报率

Conclusion: 该方法可靠实用，适用于现实应用

Abstract: Adversarial attacks present a significant threat to modern machine learning
systems. Yet, existing detection methods often lack the ability to detect
unseen attacks or detect different attack types with a high level of accuracy.
In this work, we propose a statistical approach that establishes a detection
baseline before a neural network's deployment, enabling effective real-time
adversarial detection. We generate a metric of adversarial presence by
comparing the behavior of a compressed/uncompressed neural network pair. Our
method has been tested against state-of-the-art techniques, and it achieves
near-perfect detection across a wide range of attack types. Moreover, it
significantly reduces false positives, making it both reliable and practical
for real-world applications.

</details>


### [204] [Untargeted Jailbreak Attack](https://arxiv.org/abs/2510.02999)
*Xinzhe Huang,Wenjing Hu,Tianhang Zheng,Kedong Xiu,Xiaojun Jia,Di Wang,Zhan Qin,Kui Ren*

Main category: cs.CR

TL;DR: 提出首个基于梯度的无目标越狱攻击（UJA），通过最大化不安全概率攻击大语言模型，评估显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度的大语言模型越狱攻击多为有目标攻击，搜索空间受限且效率低，需改进。

Method: 提出无目标攻击目标，用判断模型量化不安全概率，将目标分解为两个可微子目标进行优化，并进行理论分析。

Result: UJA在仅100次优化迭代下对安全对齐的大语言模型攻击成功率超80%，比现有方法高20%以上。

Conclusion: UJA无限制目标扩展了搜索空间，能更灵活高效探索大语言模型漏洞，优于现有梯度攻击方法。

Abstract: Existing gradient-based jailbreak attacks on Large Language Models (LLMs),
such as Greedy Coordinate Gradient (GCG) and COLD-Attack, typically optimize
adversarial suffixes to align the LLM output with a predefined target response.
However, by restricting the optimization objective as inducing a predefined
target, these methods inherently constrain the adversarial search space, which
limit their overall attack efficacy. Furthermore, existing methods typically
require a large number of optimization iterations to fulfill the large gap
between the fixed target and the original model response, resulting in low
attack efficiency.
  To overcome the limitations of targeted jailbreak attacks, we propose the
first gradient-based untargeted jailbreak attack (UJA), aiming to elicit an
unsafe response without enforcing any predefined patterns. Specifically, we
formulate an untargeted attack objective to maximize the unsafety probability
of the LLM response, which can be quantified using a judge model. Since the
objective is non-differentiable, we further decompose it into two
differentiable sub-objectives for optimizing an optimal harmful response and
the corresponding adversarial prompt, with a theoretical analysis to validate
the decomposition. In contrast to targeted jailbreak attacks, UJA's
unrestricted objective significantly expands the search space, enabling a more
flexible and efficient exploration of LLM vulnerabilities.Extensive evaluations
demonstrate that \textsc{UJA} can achieve over 80\% attack success rates
against recent safety-aligned LLMs with only 100 optimization iterations,
outperforming the state-of-the-art gradient-based attacks such as I-GCG and
COLD-Attack by over 20\%.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [205] [Breaking the MoE LLM Trilemma: Dynamic Expert Clustering with Structured Compression](https://arxiv.org/abs/2510.02345)
*Peijun Zhu,Ning Yang,Jiayu Wei,Jinghang Wu,Haijun Zhang*

Main category: cs.CL

TL;DR: 提出统一框架解决MoE大语言模型负载不平衡、参数冗余和通信开销问题，评估显示能减少参数、提高吞吐量等。


<details>
  <summary>Details</summary>
Motivation: 解决Mixture-of-Experts (MoE)大语言模型面临的负载不平衡、参数冗余和通信开销的三难困境。

Method: 采用基于动态专家聚类和结构化压缩的统一框架，包含在线聚类、权重分解、两级分层路由策略、异构精度方案和动态卸载。

Result: 在GLUE和WikiText - 103上评估，匹配标准MoE模型质量，减少约80%总参数，提高10% - 20%吞吐量，降低专家负载方差超三倍。

Conclusion: 结构重组是实现可扩展、高效和内存有效的MoE大语言模型的有效途径。

Abstract: Mixture-of-Experts (MoE) Large Language Models (LLMs) face a trilemma of load
imbalance, parameter redundancy, and communication overhead. We introduce a
unified framework based on dynamic expert clustering and structured compression
to address these issues cohesively. Our method employs an online clustering
procedure that periodically regroups experts using a fused metric of parameter
and activation similarity, which stabilizes expert utilization. To our
knowledge, this is one of the first frameworks to leverage the semantic
embedding capability of the router to dynamically reconfigure the model's
architecture during training for substantial efficiency gains. Within each
cluster, we decompose expert weights into a shared base matrix and extremely
low-rank residual adapters, achieving up to fivefold parameter reduction per
group while preserving specialization. This structure enables a two-stage
hierarchical routing strategy: tokens are first assigned to a cluster, then to
specific experts within it, drastically reducing the routing search space and
the volume of all-to-all communication. Furthermore, a heterogeneous precision
scheme, which stores shared bases in FP16 and residual factors in INT4, coupled
with dynamic offloading of inactive clusters, reduces peak memory consumption
to levels comparable to dense models. Evaluated on GLUE and WikiText-103, our
framework matches the quality of standard MoE models while reducing total
parameters by approximately 80%, improving throughput by 10% to 20%, and
lowering expert load variance by a factor of over three. Our work demonstrates
that structural reorganization is a principled path toward scalable, efficient,
and memory-effective MoE LLMs.

</details>


### [206] [Hierarchical Semantic Retrieval with Cobweb](https://arxiv.org/abs/2510.02539)
*Anant Gupta,Karthik Singaravadivelan,Zekun Wang*

Main category: cs.CL

TL;DR: 本文提出Cobweb框架组织句子嵌入并进行粗到细的文档检索，在多个数据集评估，结果显示其有效、鲁棒、可扩展且可解释。


<details>
  <summary>Details</summary>
Motivation: 现有神经文档检索未充分利用语料库结构，解释性不透明。

Method: 使用Cobweb框架将句子嵌入组织成原型树，通过粗到细遍历对文档排序，提出两种推理方法。

Result: 在MS MARCO和QQP数据集上评估，检索方法在强编码器嵌入上与点积搜索效果相当，在kNN效果下降时更鲁棒。

Conclusion: Cobweb框架具有竞争力、对嵌入质量鲁棒、可扩展且检索过程可解释。

Abstract: Neural document retrieval often treats a corpus as a flat cloud of vectors
scored at a single granularity, leaving corpus structure underused and
explanations opaque. We use Cobweb--a hierarchy-aware framework--to organize
sentence embeddings into a prototype tree and rank documents via coarse-to-fine
traversal. Internal nodes act as concept prototypes, providing multi-granular
relevance signals and a transparent rationale through retrieval paths. We
instantiate two inference approaches: a generalized best-first search and a
lightweight path-sum ranker. We evaluate our approaches on MS MARCO and QQP
with encoder (e.g., BERT/T5) and decoder (GPT-2) representations. Our results
show that our retrieval approaches match the dot product search on strong
encoder embeddings while remaining robust when kNN degrades: with GPT-2
vectors, dot product performance collapses whereas our approaches still
retrieve relevant results. Overall, our experiments suggest that Cobweb
provides competitive effectiveness, improved robustness to embedding quality,
scalability, and interpretable retrieval via hierarchical prototypes.

</details>


### [207] [StepChain GraphRAG: Reasoning Over Knowledge Graphs for Multi-Hop Question Answering](https://arxiv.org/abs/2510.02827)
*Tengjun Ni,Xin Yuan,Shenghong Li,Kai Wu,Ren Ping Liu,Wei Ni,Wenjie Zhang*

Main category: cs.CL

TL;DR: 提出StepChain GraphRAG框架用于多跳问答，实验取得SOTA成绩并增强可解释性，还探讨了未来工作方向。


<details>
  <summary>Details</summary>
Motivation: 解决迭代推理步骤与外部知识检索集成在多跳问答中的挑战。

Method: 构建全局索引，推理时将检索段落解析为知识图，拆分复杂查询为子问题，用基于BFS的遍历动态扩展相关边。

Result: 在MuSiQue、2WikiMultiHopQA和HotpotQA上取得SOTA的精确匹配和F1分数，平均EM提升2.57%，F1提升2.13%，在HotpotQA上提升最大，还增强了可解释性。

Conclusion: 讨论未来工作可减轻计算开销，解决大语言模型幻觉问题以提升多跳问答效率和可靠性。

Abstract: Recent progress in retrieval-augmented generation (RAG) has led to more
accurate and interpretable multi-hop question answering (QA). Yet, challenges
persist in integrating iterative reasoning steps with external knowledge
retrieval. To address this, we introduce StepChain GraphRAG, a framework that
unites question decomposition with a Breadth-First Search (BFS) Reasoning Flow
for enhanced multi-hop QA. Our approach first builds a global index over the
corpus; at inference time, only retrieved passages are parsed on-the-fly into a
knowledge graph, and the complex query is split into sub-questions. For each
sub-question, a BFS-based traversal dynamically expands along relevant edges,
assembling explicit evidence chains without overwhelming the language model
with superfluous context. Experiments on MuSiQue, 2WikiMultiHopQA, and HotpotQA
show that StepChain GraphRAG achieves state-of-the-art Exact Match and F1
scores. StepChain GraphRAG lifts average EM by 2.57% and F1 by 2.13% over the
SOTA method, achieving the largest gain on HotpotQA (+4.70% EM, +3.44% F1).
StepChain GraphRAG also fosters enhanced explainability by preserving the
chain-of-thought across intermediate retrieval steps. We conclude by discussing
how future work can mitigate the computational overhead and address potential
hallucinations from large language models to refine efficiency and reliability
in multi-hop QA.

</details>


### [208] [Grounding Large Language Models in Clinical Evidence: A Retrieval-Augmented Generation System for Querying UK NICE Clinical Guidelines](https://arxiv.org/abs/2510.02967)
*Matthew Lewis,Samuel Thio,Richard JB Dobson,Spiros Denaxas*

Main category: cs.CL

TL;DR: 本文开发并评估了基于大语言模型的检索增强生成（RAG）系统，用于查询英国NICE临床指南，系统性能高，证明RAG在医疗领域应用有效可靠。


<details>
  <summary>Details</summary>
Motivation: 英国NICE临床指南篇幅长、数量多，在时间受限的医疗系统中利用困难，需开发系统提供精准信息。

Method: 构建由混合嵌入机制组成的检索架构，用10195个文本块的数据库评估系统，用7901个查询测试，还用70个问答对评估生成阶段。

Result: 检索阶段平均倒数排名0.814，第一个文本块召回率81%，前十个召回率99.1%；生成阶段RAG增强模型性能提升，忠实度大幅提高，上下文精度为1。

Conclusion: RAG是在医疗中应用生成式AI的有效、可靠且可扩展的方法，能实现低成本获取医疗指南。

Abstract: This paper presents the development and evaluation of a Retrieval-Augmented
Generation (RAG) system for querying the United Kingdom's National Institute
for Health and Care Excellence (NICE) clinical guidelines using Large Language
Models (LLMs). The extensive length and volume of these guidelines can impede
their utilisation within a time-constrained healthcare system, a challenge this
project addresses through the creation of a system capable of providing users
with precisely matched information in response to natural language queries. The
system's retrieval architecture, composed of a hybrid embedding mechanism, was
evaluated against a database of 10,195 text chunks derived from three hundred
guidelines. It demonstrates high performance, with a Mean Reciprocal Rank (MRR)
of 0.814, a Recall of 81% at the first chunk and of 99.1% within the top ten
retrieved chunks, when evaluated on 7901 queries.
  The most significant impact of the RAG system was observed during the
generation phase. When evaluated on a manually curated dataset of seventy
question-answer pairs, RAG-enhanced models showed substantial gains in
performance. Faithfulness, the measure of whether an answer is supported by the
source text, was increased by 64.7 percentage points to 99.5% for the
RAG-enhanced O4-Mini model and significantly outperformed the medical-focused
Meditron3-8B LLM, which scored 43%. This, combined with a perfect Context
Precision score of 1 for all RAG-enhanced models, confirms the system's ability
to prevent information fabrication by grounding its answers in relevant source
material. This study thus establishes RAG as an effective, reliable, and
scalable approach for applying generative AI in healthcare, enabling
cost-effective access to medical guidelines.

</details>


### [209] [Hallucination reduction with CASAL: Contrastive Activation Steering For Amortized Learning](https://arxiv.org/abs/2510.02324)
*Wannan Yang,Xinchi Qiu,Lei Yu,Yuchen Zhang,Oliver Aobo Yang,Narine Kokhlikyan,Nicola Cancedda,Diego Garcia-Olano*

Main category: cs.CL

TL;DR: 提出CASAL算法，将激活引导的优势融入模型权重，减少大语言模型幻觉，效率高且能有效泛化。


<details>
  <summary>Details</summary>
Motivation: 现有减少大语言模型幻觉的方法需实时监控和干预，希望找到更高效方法。

Method: 引入Contrastive Activation Steering for Amortized Learning (CASAL)算法，训练单个Transformer层的子模块。

Result: 在多个短问答基准上减少30%-40%的幻觉，计算效率比强LoRA基线高30倍，数据效率高20倍，能有效泛化到分布外领域。

Conclusion: CASAL是基于引导的训练方法，对密集和混合专家模型都有效，为可解释性方法在生产系统中的实际应用迈出了有前景的一步。

Abstract: Large Language Models (LLMs) exhibit impressive capabilities but often
hallucinate, confidently providing incorrect answers instead of admitting
ignorance. Prior work has shown that models encode linear representations of
their own knowledge and that activation steering can reduce hallucinations.
These approaches, however, require real-time monitoring and intervention during
inference. We introduce Contrastive Activation Steering for Amortized Learning
(CASAL), an efficient algorithm that connects interpretability with amortized
optimization. CASAL directly bakes the benefits of activation steering into
model's weights. Once trained, LLMs answer questions they know while abstaining
from answering those they do not. CASAL's light-weight design requires training
only a submodule of a single transformer layer and yet reduces hallucination by
30%-40% across multiple short-form QA benchmarks. CASAL is 30x more
compute-efficient and 20x more data-efficient than strong LoRA-based baselines
such as SFT and DPO, boosting its practical applicability in data scarce
domains. Importantly, CASAL also generalizes effectively to out-of-distribution
(OOD) domains. We showcase CASAL's flexibility in mitigating hallucinations in
both text-only and vision-language models. To our knowledge, CASAL is the first
steering-based training method that has been shown to be effective for both
dense and Mixture-of-Experts (MoE) models. CASAL represents a promising step
forward for applying interpretability-inspired method for practical deployment
in production systems.

</details>


### [210] [Hallucination-Resistant, Domain-Specific Research Assistant with Self-Evaluation and Vector-Grounded Retrieval](https://arxiv.org/abs/2510.02326)
*Vivek Bhavsar,Joseph Ereifej,Aravanan Gurusami*

Main category: cs.CL

TL;DR: 提出基于GPT的研究助手RA - FSM，经评估受领域专家青睐，设计具透明性和可推广性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在文献合成中存在幻觉和误引用问题，限制其在专家工作流程中的作用。

Method: 构建模块化的RA - FSM，将生成过程置于有限状态控制循环，以向量检索和确定性引用管道为基础，构建领域知识库。

Result: 在六项任务类别评估中，领域专家在盲测A/B评审中更青睐RA - FSM，其能探索更多内容，但有可调的延迟和成本开销。

Conclusion: RA - FSM设计强调为高风险技术工作提供透明、有充分引用的答案，可推广到其他科学领域。

Abstract: Large language models accelerate literature synthesis but can hallucinate and
mis-cite, limiting their usefulness in expert workflows. We present RA-FSM
(Research Assistant - Finite State Machine), a modular GPT-based research
assistant that wraps generation in a finite-state control loop: Relevance ->
Confidence -> Knowledge. The system is grounded in vector retrieval and a
deterministic citation pipeline. The controller filters out-of-scope queries,
scores answerability, decomposes questions, and triggers retrieval only when
needed, and emits answers with confidence labels and in-corpus, de-duplicated
references. A ranked-tier ingestion workflow constructs a domain knowledge base
from journals, conferences, indices, preprints, and patents, writing both to a
dense vector index and to a relational store of normalized metrics. We
implement the system for photonics and evaluate it on six task categories:
analytical reasoning, numerical analysis, methodological critique, comparative
synthesis, factual extraction, and application design. In blinded A/B reviews,
domain experts prefer RA-FSM to both a strong Notebook LM (NLM) and a vanilla
Default GPT API call single-pass baseline, citing stronger boundary-condition
handling and more defensible evidence use. Coverage and novelty analyses
indicate that RA-FSM explores beyond the NLM while incurring tunable latency
and cost overheads. The design emphasizes transparent, well-cited answers for
high-stakes technical work and is generalizable to other scientific domains.

</details>


### [211] [KAME: Tandem Architecture for Enhancing Knowledge in Real-Time Speech-to-Speech Conversational AI](https://arxiv.org/abs/2510.02327)
*So Kuroki,Yotaro Kubo,Takuya Akiba,Yujin Tang*

Main category: cs.CL

TL;DR: 提出混合架构结合实时语音到语音模型和大语言模型优势，评估显示在正确性和延迟上表现良好。


<details>
  <summary>Details</summary>
Motivation: 实时语音到语音模型缺乏知识和语义理解，级联系统有高延迟问题，需结合两者优势。

Method: 引入混合架构，用S2S变压器处理语音获即时响应，同时将查询传至后端大语言模型，实时注入其响应指导S2S模型生成。

Result: 使用MT - Bench基准语音合成变体评估，系统在响应正确性上大幅超越基线S2S模型，接近级联系统，延迟与基线相当。

Conclusion: 该混合架构有效结合两种范式优势，在正确性和延迟间取得良好平衡。

Abstract: Real-time speech-to-speech (S2S) models excel at generating natural,
low-latency conversational responses but often lack deep knowledge and semantic
understanding. Conversely, cascaded systems combining automatic speech
recognition, a text-based Large Language Model (LLM), and text-to-speech
synthesis offer superior knowledge representation at the cost of high latency,
which disrupts the flow of natural interaction. This paper introduces a novel
hybrid architecture that bridges the gap between these two paradigms. Our
framework processes user speech through an S2S transformer for immediate
responsiveness while concurrently relaying the query to a powerful back-end
LLM. The LLM's text-based response is then injected in real time to guide the
S2S model's speech generation, effectively infusing its output with rich
knowledge without the full latency penalty of a cascaded system. We evaluated
our method using a speech-synthesized variant of the MT-Bench benchmark that
consists of multi-turn question-answering sessions. The results demonstrate
that our system substantially outperforms a baseline S2S model in response
correctness, approaching that of a cascaded system, while maintaining a latency
on par with the baseline.

</details>


### [212] [AMANDA: Agentic Medical Knowledge Augmentation for Data-Efficient Medical Visual Question Answering](https://arxiv.org/abs/2510.02328)
*Ziqing Wang,Chengsheng Mao,Xiaole Wen,Yuan Luo,Kaize Ding*

Main category: cs.CL

TL;DR: 提出训练无关的AMANDA框架用于医学多模态大语言模型的医学视觉问答，在多基准测试中效果提升，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有医学多模态大语言模型在低资源环境下因医学推理能力瓶颈（内在忽略图像细节、外在未融入专业知识）而失效。

Method: 提出AMANDA框架，通过大语言模型代理进行医学知识增强，包括内在的粗到细问题分解和外在的生物医学知识图谱检索。

Result: 在八个医学视觉问答基准测试中，零样本和少样本设置下都有显著提升。

Conclusion: AMANDA框架能有效解决现有医学多模态大语言模型在低资源环境下的推理瓶颈问题。

Abstract: Medical Multimodal Large Language Models (Med-MLLMs) have shown great promise
in medical visual question answering (Med-VQA). However, when deployed in
low-resource settings where abundant labeled data are unavailable, existing
Med-MLLMs commonly fail due to their medical reasoning capability bottlenecks:
(i) the intrinsic reasoning bottleneck that ignores the details from the
medical image; (ii) the extrinsic reasoning bottleneck that fails to
incorporate specialized medical knowledge. To address those limitations, we
propose AMANDA, a training-free agentic framework that performs medical
knowledge augmentation via LLM agents. Specifically, our intrinsic medical
knowledge augmentation focuses on coarse-to-fine question decomposition for
comprehensive diagnosis, while extrinsic medical knowledge augmentation grounds
the reasoning process via biomedical knowledge graph retrieval. Extensive
experiments across eight Med-VQA benchmarks demonstrate substantial
improvements in both zero-shot and few-shot Med-VQA settings. The code is
available at https://github.com/REAL-Lab-NU/AMANDA.

</details>


### [213] [SelfJudge: Faster Speculative Decoding via Self-Supervised Judge Verification](https://arxiv.org/abs/2510.02329)
*Kanghoon Yoon,Minsub Kim,Sungjae Lee,Joonhyung Lee,Sunghyeon Woo,Yeonjun In,Se Jung Kwon,Chanyoung Park,Dongsoo Lee*

Main category: cs.CL

TL;DR: 提出SelfJudge方法用于LLM推理加速，通过自监督训练判断验证器，实验显示比基线方法有更好的推理 - 准确率权衡。


<details>
  <summary>Details</summary>
Motivation: 现有判断解码方法依赖人工标注或有可验证事实的任务，限制了在不同NLP任务中的通用性。

Method: 提出SelfJudge，通过目标模型的自监督训练判断验证器，通过评估标记替换后的响应是否保留原响应的含义来衡量语义保留。

Result: SelfJudge在推理 - 准确率权衡方面优于判断解码基线。

Conclusion: SelfJudge为更快的LLM推理提供了广泛适用的解决方案。

Abstract: Speculative decoding accelerates LLM inference by verifying candidate tokens
from a draft model against a larger target model. Recent judge decoding boosts
this process by relaxing verification criteria by accepting draft tokens that
may exhibit minor discrepancies from target model output, but existing methods
are restricted by their reliance on human annotations or tasks with verifiable
ground truths, limiting generalizability across diverse NLP tasks. We propose
SelfJudge, which trains judge verifiers via self-supervision of the target
model. Our method measures semantic preservation by assessing whether
token-substituted responses preserve the meaning of original responses,
enabling automatic verifier training across diverse NLP tasks. Our experiments
show SelfJudge achieves superior inference-accuracy trade-offs than judge
decoding baselines, offering a broadly applicable solution for faster LLM
inference.

</details>


### [214] [EntropyLong: Effective Long-Context Training via Predictive Uncertainty](https://arxiv.org/abs/2510.02330)
*Junlong Jia,Ziyang Chen,Xing Wu,Chaochen Gao,Zijia Lin,Debing Zhang,Songlin Hu,Binghui Guo*

Main category: cs.CL

TL;DR: 提出EntropyLong数据构造方法，生成有验证依赖的长序列数据集，训练模型在相关基准测试有显著提升，消融实验验证熵基验证有效性。


<details>
  <summary>Details</summary>
Motivation: 当前长上下文语言模型数据构造方法难以保证真正的长距离依赖，需要专门的数据构造方法。

Method: 提出EntropyLong方法，利用预测不确定性验证依赖质量，通过识别文档高熵位置、检索相关上下文、评估是否降低预测熵来验证，结合原文与验证后的上下文构造训练样本。

Result: 生成128K长度有验证依赖的数据集，训练模型在RULER和LongBenchv2基准测试有显著提升。

Conclusion: 基于熵的验证对于长上下文训练是必要且有效的。

Abstract: Training long-context language models to capture long-range dependencies
requires specialized data construction. Current approaches, such as generic
text concatenation or heuristic-based variants, frequently fail to guarantee
genuine long-range dependencies. We propose EntropyLong, a novel data
construction method that leverages predictive uncertainty to verify dependency
quality. Our approach identifies high-entropy positions in documents, retrieves
semantically relevant contexts from large corpora, and verifies their utility
by assessing whether they reduce prediction entropy. This model-in-the-loop
verification ensures each dependency represents measurable information gain
rather than spurious correlation. We construct training samples with long-range
dependencies by combining original documents with these verified contextual
supplements. Using FineWebEdu and Cosmopedia, we generate a dataset of
128K-length sequences with verified dependencies. Models trained on this data
demonstrate significant improvements on RULER benchmarks, particularly in tasks
requiring distant information. Following instruction fine-tuning, our models
also achieve substantial gains on LongBenchv2, demonstrating enhanced
long-context understanding. Extensive ablation studies further validate the
necessity and effectiveness of entropybased verification for long-context
training.

</details>


### [215] [Synthetic Dialogue Generation for Interactive Conversational Elicitation & Recommendation (ICER)](https://arxiv.org/abs/2510.02331)
*Moonkyung Ryu,Chih-Wei Hsu,Yinlam Chow,Mohammad Ghavamzadeh,Craig Boutilier*

Main category: cs.CL

TL;DR: 为解决公共CRS数据不足及数据生成缺乏行为一致性问题，用行为模拟器和LM提示生成自然对话并创建开源数据集，评估显示对话有一致性、事实性和自然性。


<details>
  <summary>Details</summary>
Motivation: 公共CRS数据匮乏，用LMs作数据生成器缺乏行为一致性，难以微调LMs用于CRSs。

Method: 利用行为模拟器和LM提示，生成与用户潜在状态一致的自然对话。

Result: 生成了包含偏好引出和示例批判的大型开源CRS数据集，部分对话经评估有较高一致性、事实性和自然性。

Conclusion: 所提出的生成自然对话的方法有效，能生成高质量的CRS数据。

Abstract: While language models (LMs) offer great potential for conversational
recommender systems (CRSs), the paucity of public CRS data makes fine-tuning
LMs for CRSs challenging. In response, LMs as user simulators qua data
generators can be used to train LM-based CRSs, but often lack behavioral
consistency, generating utterance sequences inconsistent with those of any real
user. To address this, we develop a methodology for generating natural
dialogues that are consistent with a user's underlying state using behavior
simulators together with LM-prompting. We illustrate our approach by generating
a large, open-source CRS data set with both preference elicitation and example
critiquing. Rater evaluation on some of these dialogues shows them to exhibit
considerable consistency, factuality and naturalness.

</details>


### [216] [A High-Capacity and Secure Disambiguation Algorithm for Neural Linguistic Steganography](https://arxiv.org/abs/2510.02332)
*Yapei Feng,Feng Jiang,Shanhao Wu,Hua Zhong*

Main category: cs.CL

TL;DR: 提出 look - ahead Sync 方法克服 SyncPool 容量限制，理论证明安全，实验显示性能超 SyncPool，迈向实用高容量可证明安全语言隐写术。


<details>
  <summary>Details</summary>
Motivation: 解决现代分词器的分词歧义导致的解码失败问题，且克服 SyncPool 牺牲嵌入容量的缺点。

Method: 提出 look - ahead Sync 方法，仅对真正不可区分的令牌序列进行最小同步采样，保留其他可区分路径以最大化嵌入容量。

Result: 在英语和中文基准测试中，该方法接近理论容量上限，显著优于 SyncPool，英语嵌入率提升超 160%，中文超 25%。

Conclusion: 该工作是迈向实用高容量可证明安全语言隐写术的重要一步。

Abstract: Neural linguistic steganography aims to embed information
  into natural text while preserving statistical undetectability. A fundamental
challenge in this ffeld stems from tokenization ambiguity in modern tokenizers,
which can lead to catastrophic decoding failures. The recent method, SyncPool,
addresses this ambiguity
  by employing a coarse-grained synchronization mechanism over groups of
ambiguous candidates. However, SyncPool sacriffces embedding capacity, as it
utilizes the entire Shannon entropy of an ambiguous group solely for
synchronization rather than for payload embedding. We propose a method named
look-ahead Sync, which overcomes the capacity limitation of SyncPool while
retaining its provable security guarantees. Our approach performs minimal
synchronized sampling only on truly indistinguishable token sequences, while
strategically preserving all other discernible paths to maximize embedding
capacity. We provide theoretical proofs for the security of our method and
analyze the gap between its achievable embedding capacity and the theoretical
upper bound. Experiments on English (using Llama 3) and Chinese (using Qwen
2.5) benchmarks show that our method consistently approaches the theoretical
capacity upper bound and signiffcantly outperforms SyncPool. The improvement in
embedding rate exceeds 160% in English and 25% in Chinese, particularly in
settings with larger candidate pools. This work represents a signiffcant step
toward practical high-capacity provably secure linguistic steganography.

</details>


### [217] [Human Mobility Datasets Enriched With Contextual and Social Dimensions](https://arxiv.org/abs/2510.02333)
*Chiara Pugliese,Francesco Lettich,Guido Rocchietti,Chiara Renso,Fabio Pinelli*

Main category: cs.CL

TL;DR: 本文介绍两个语义丰富的人类轨迹公开数据集及构建管道，含多种上下文层和LLM生成的社交媒体帖子，支持多研究任务。


<details>
  <summary>Details</summary>
Motivation: 提供能支持语义推理和FAIR数据实践，可用于多种研究任务的人类轨迹数据集。

Method: 从OpenStreetMap获取GPS轨迹，构建含多种上下文层的数据集，用LLM生成社交媒体帖子，提供开源可复现管道。

Result: 得到两个覆盖巴黎和纽约的数据集，有表格和RDF两种格式。

Conclusion: 此资源首次在可复用框架中结合现实移动、结构化语义丰富、LLM生成文本和语义网兼容性。

Abstract: In this resource paper, we present two publicly available datasets of
semantically enriched human trajectories, together with the pipeline to build
them. The trajectories are publicly available GPS traces retrieved from
OpenStreetMap. Each dataset includes contextual layers such as stops, moves,
points of interest (POIs), inferred transportation modes, and weather data. A
novel semantic feature is the inclusion of synthetic, realistic social media
posts generated by Large Language Models (LLMs), enabling multimodal and
semantic mobility analysis. The datasets are available in both tabular and
Resource Description Framework (RDF) formats, supporting semantic reasoning and
FAIR data practices. They cover two structurally distinct, large cities: Paris
and New York. Our open source reproducible pipeline allows for dataset
customization, while the datasets support research tasks such as behavior
modeling, mobility prediction, knowledge graph construction, and LLM-based
applications. To our knowledge, our resource is the first to combine real-world
movement, structured semantic enrichment, LLM-generated text, and semantic web
compatibility in a reusable framework.

</details>


### [218] [Where Did It Go Wrong? Attributing Undesirable LLM Behaviors via Representation Gradient Tracing](https://arxiv.org/abs/2510.02334)
*Zhe Li,Wei Zhao,Yige Li,Jun Sun*

Main category: cs.CL

TL;DR: 提出新框架分析大语言模型不良行为，评估效果好，提供诊断工具。


<details>
  <summary>Details</summary>
Motivation: 大语言模型部署存在不良行为，现有归因方法有缺陷，需诊断其失败根源以保障AI安全。

Method: 引入新框架，通过分析表示及其梯度，在模型激活空间操作，将输出与训练数据关联。

Result: 该方法在样本级归因和细粒度标记级分析表现出色，能精准识别影响模型行为的样本和短语。

Conclusion: 此工作为理解、审计和缓解大语言模型风险提供强大诊断工具。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities, yet
their deployment is frequently undermined by undesirable behaviors such as
generating harmful content, factual inaccuracies, and societal biases.
Diagnosing the root causes of these failures poses a critical challenge for AI
safety. Existing attribution methods, particularly those based on parameter
gradients, often fall short due to prohibitive noisy signals and computational
complexity. In this work, we introduce a novel and efficient framework that
diagnoses a range of undesirable LLM behaviors by analyzing representation and
its gradients, which operates directly in the model's activation space to
provide a semantically meaningful signal linking outputs to their training
data. We systematically evaluate our method for tasks that include tracking
harmful content, detecting backdoor poisoning, and identifying knowledge
contamination. The results demonstrate that our approach not only excels at
sample-level attribution but also enables fine-grained token-level analysis,
precisely identifying the specific samples and phrases that causally influence
model behavior. This work provides a powerful diagnostic tool to understand,
audit, and ultimately mitigate the risks associated with LLMs. The code is
available at https://github.com/plumprc/RepT.

</details>


### [219] [FormalML: A Benchmark for Evaluating Formal Subgoal Completion in Machine Learning Theory](https://arxiv.org/abs/2510.02335)
*Xiao-Wen Yang,Zihao Zhang,Jianuo Cao,Zhi Zhou,Zenan Li,Lan-Zhe Guo,Yuan Yao,Taolue Chen,Yu-Feng Li,Xiaoxing Ma*

Main category: cs.CL

TL;DR: 本文聚焦大语言模型在形式定理证明中作为数学家实用助手的子目标完成任务，引入FormalML基准，评估现有证明器发现其存在局限。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在形式定理证明虽有进展，但作为数学家实用助手完成复杂证明中缺失步骤的能力未充分探索，提出子目标完成任务。

Method: 引入基于机器学习基础理论的Lean 4基准FormalML，用翻译策略将过程式证明转化为声明式形式，提取4937个不同难度问题。

Result: 评估现有证明器发现其在准确性和效率上存在持续局限。

Conclusion: 需要更强大的基于大语言模型的定理证明器以有效完成子目标。

Abstract: Large language models (LLMs) have recently demonstrated remarkable progress
in formal theorem proving. Yet their ability to serve as practical assistants
for mathematicians, filling in missing steps within complex proofs, remains
underexplored. We identify this challenge as the task of subgoal completion,
where an LLM must discharge short but nontrivial proof obligations left
unresolved in a human-provided sketch. To study this problem, we introduce
FormalML, a Lean 4 benchmark built from foundational theories of machine
learning. Using a translation tactic that converts procedural proofs into
declarative form, we extract 4937 problems spanning optimization and
probability inequalities, with varying levels of difficulty. FormalML is the
first subgoal completion benchmark to combine premise retrieval and complex
research-level contexts. Evaluation of state-of-the-art provers highlights
persistent limitations in accuracy and efficiency, underscoring the need for
more capable LLM-based theorem provers for effective subgoal completion,

</details>


### [220] [KurdSTS: The Kurdish Semantic Textual Similarity](https://arxiv.org/abs/2510.02336)
*Abdulhady Abas Abdullah,Hadi Veisi,Hussein M. Al*

Main category: cs.CL

TL;DR: 本文推出首个库尔德语STS数据集，对多种模型进行基准测试并取得有竞争力结果，为库尔德语语义和低资源NLP研究提供起点。


<details>
  <summary>Details</summary>
Motivation: 现有资源对库尔德语这类低资源语言支持不足，需要开展相关研究。

Method: 创建包含10000个句子对的库尔德语STS数据集，对Sentence - BERT、多语言BERT等模型进行基准测试。

Result: 取得有竞争力的结果，同时指出库尔德语形态、正字法变体和代码混合带来的挑战。

Conclusion: 该数据集和基准测试建立了可重复的评估套件，为未来研究提供了有力起点。

Abstract: Semantic Textual Similarity (STS) measures the degree of meaning overlap
between two texts and underpins many NLP tasks. While extensive resources exist
for high-resource languages, low-resource languages such as Kurdish remain
underserved. We present, to our knowledge, the first Kurdish STS dataset:
10,000 sentence pairs spanning formal and informal registers, each annotated
for similarity. We benchmark Sentence-BERT, multilingual BERT, and other strong
baselines, obtaining competitive results while highlighting challenges arising
from Kurdish morphology, orthographic variation, and code-mixing. The dataset
and baselines establish a reproducible evaluation suite and provide a strong
starting point for future research on Kurdish semantics and low-resource NLP.

</details>


### [221] [CRACQ: A Multi-Dimensional Approach To Automated Document Assessment](https://arxiv.org/abs/2510.02337)
*Ishak Soltani,Francisco Belo,Bernardo Tavares*

Main category: cs.CL

TL;DR: 本文提出CRACQ多维度评估框架，可评估多种机器生成文本，训练并测试后显示比直接用大语言模型评估更稳定可解释，但存在可靠性和领域范围挑战。


<details>
  <summary>Details</summary>
Motivation: 现有单分数评估方法存在不足，需要一个能对多样化机器生成文本进行评估且可解释的方法。

Method: 基于基于特征的自动作文评分（AES），将语言、语义和结构信号整合到累积评估中，在500个合成拨款提案上训练。

Result: CRACQ比直接的大语言模型评估产生更稳定和可解释的特征级判断。

Conclusion: CRACQ为机器生成文本评估提供了新方法，但在可靠性和领域范围方面还有待改进。

Abstract: This paper presents CRACQ, a multi-dimensional evaluation framework tailored
to evaluate documents across f i v e specific traits: Coherence, Rigor,
Appropriateness, Completeness, and Quality. Building on insights from
traitbased Automated Essay Scoring (AES), CRACQ expands its fo-cus beyond
essays to encompass diverse forms of machine-generated text, providing a
rubricdriven and interpretable methodology for automated evaluation. Unlike
singlescore approaches, CRACQ integrates linguistic, semantic, and structural
signals into a cumulative assessment, enabling both holistic and trait-level
analysis. Trained on 500 synthetic grant pro-posals, CRACQ was benchmarked
against an LLM-as-a-judge and further tested on both strong and weak real
applications. Preliminary results in-dicate that CRACQ produces more stable and
interpretable trait-level judgments than direct LLM evaluation, though
challenges in reliability and domain scope remain

</details>


### [222] [Optimizing Long-Form Clinical Text Generation with Claim-Based Rewards](https://arxiv.org/abs/2510.02338)
*Samyak Jhaveri,Praphul Singh,Jangwon Kim,Tara Taghavi,Krishnaram Kenthapadi*

Main category: cs.CL

TL;DR: 提出评估集成强化学习框架用于长格式临床文本生成，优化事实依据和完整性，提升临床笔记质量并降低成本，框架可扩展到现实场景。


<details>
  <summary>Details</summary>
Motivation: 自动化临床文档需与完整性和事实依据等优先级精确对齐。

Method: 提出评估集成强化学习框架，将GRPO与DocLens结合，直接优化事实依据和完整性，无需训练单独奖励模型或依赖人工参考。

Result: 通过简单奖励门控策略提高临床笔记质量、降低训练成本，GPT - 5定性评估显示GRPO输出在事实性、完整性和简洁性上更受青睐，减少遗漏和幻觉。

Conclusion: 该框架可扩展到现实场景，能纳入自定义目标。

Abstract: Automating clinical documentation with large language models requires precise
alignment with priorities such as completeness and factual grounding. We
present an evaluation-integrated reinforcement learning framework for long-form
clinical text generation that couples Group Relative Policy Optimization (GRPO)
with DocLens, a claim-level evaluator that provides deterministic,
dialogue-grounded rewards. Our method directly optimizes factual grounding and
completeness without training a separate reward model or relying on
human-authored references. Empirically, the approach improves clinical note
quality and reduces training cost via a simple reward-gating strategy. An
independent GPT-5 qualitative evaluation further supports these gains, showing
higher preference for GRPO outputs in factuality, completeness, and brevity,
with fewer omissions and hallucinations. Because the benchmarks are relatively
clean and the base model already well aligned, these improvements likely
represent a conservative lower bound. The framework is scalable to real-world
settings and can incorporate custom objectives such as guideline adherence or
billing preferences.

</details>


### [223] [Evaluating Uncertainty Quantification Methods in Argumentative Large Language Models](https://arxiv.org/abs/2510.02339)
*Kevin Zhou,Adam Dejl,Gabriel Freedman,Lihu Chen,Antonio Rago,Francesca Toni*

Main category: cs.CL

TL;DR: 研究将大语言模型不确定性量化（UQ）方法集成到论证性大语言模型（ArgLLMs）中，实验表明直接提示是有效的UQ策略。


<details>
  <summary>Details</summary>
Motivation: 保证大语言模型技术可靠性，探索UQ方法在ArgLLMs中的集成。

Method: 进行实验评估不同LLM UQ方法下ArgLLMs在声明验证任务中的表现。

Result: 直接提示这种简单方法在ArgLLMs中是有效的UQ策略，优于更复杂的方法。

Conclusion: 直接提示可作为ArgLLMs中有效的UQ策略。

Abstract: Research in uncertainty quantification (UQ) for large language models (LLMs)
is increasingly important towards guaranteeing the reliability of this
groundbreaking technology. We explore the integration of LLM UQ methods in
argumentative LLMs (ArgLLMs), an explainable LLM framework for decision-making
based on computational argumentation in which UQ plays a critical role. We
conduct experiments to evaluate ArgLLMs' performance on claim verification
tasks when using different LLM UQ methods, inherently performing an assessment
of the UQ methods' effectiveness. Moreover, the experimental procedure itself
is a novel way of evaluating the effectiveness of UQ methods, especially when
intricate and potentially contentious statements are present. Our results
demonstrate that, despite its simplicity, direct prompting is an effective UQ
strategy in ArgLLMs, outperforming considerably more complex approaches.

</details>


### [224] [DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning](https://arxiv.org/abs/2510.02341)
*Yifan Wang,Bolian Li,Junlin Wu,Zhaoxuan Tan,Zheli Liu,Ruqi Zhang,Ananth Grama,Qingkai Zeng*

Main category: cs.CL

TL;DR: 本文提出DRIFT方法，利用真实世界用户不满信号进行训练，在多个数据集和评估指标上优于基线方法，且具有可扩展性和理论保证。


<details>
  <summary>Details</summary>
Motivation: 现实中大型语言模型部署时用户显式满意反馈稀缺，现有偏好学习方法与数据特征不匹配。

Method: 引入DRIFT方法，以真实世界用户不满信号为训练锚点，从不断演变的策略中动态采样正样本。

Result: 在多个数据集上，DRIFT模型在任务得分和胜率上有显著提升，在大模型规模下表现突出，还保留了探索能力。

Conclusion: DRIFT是一种有效且可扩展的真实世界后训练方法，能利用最丰富和有信息的信号。

Abstract: Real-world large language model deployments (e.g., conversational AI systems,
code generation assistants) naturally generate abundant implicit user
dissatisfaction (DSAT) signals, as users iterate toward better answers through
refinements, corrections, and expressed preferences, while explicit
satisfaction (SAT) feedback is scarce. Existing preference learning approaches
are poorly aligned with this data profile, as they rely on costly human
annotations or assume plentiful positive responses. In this paper, we introduce
\textbf{DRIFT} (\textbf{D}issatisfaction-\textbf{R}efined \textbf{I}terative
pre\textbf{F}erence \textbf{T}raining), which anchors training on real-world
DSAT signals and samples positives dynamically from the evolving policy.
Empirically, DRIFT models trained on real-world \textit{WildFeedback} datasets
and synthetic \textit{UltraFeedback} datasets achieve up to +6.23\% (7B) /
+7.61\% (14B) on WildBench Task Score and up to +8.95\% (7B) / +12.29\% (14B)
on AlpacaEval2 win rate over base models, outperforming strong baseline methods
such as iterative DPO and SPIN. At larger scales, the improvements are
particularly pronounced: 14B models trained with DRIFT surpass GPT-4o-mini on
WildBench. Further analysis shows that DRIFT also preserves exploratory
capacity, yielding more diverse high-reward solutions rather than collapsing to
narrow subsets. Theoretically, we demonstrate that this design preserves
preference margins and avoids the gradient degeneration. These results show
that DRIFT is an effective and scalable recipe for real-world post-training
that leverages the most abundant and informative signal. The code and data are
available at https://github.com/cacayaya/DRIFT.git.

</details>


### [225] [$\texttt{BluePrint}$: A Social Media User Dataset for LLM Persona Evaluation and Training](https://arxiv.org/abs/2510.02343)
*Aurélien Bück-Kaeffer,Je Qin Chooi,Dan Zhao,Maximilian Puelma Touzel,Kellin Pelrine,Jean-François Godbout,Reihaneh Rabbany,Zachary Yang*

Main category: cs.CL

TL;DR: 文章介绍了SIMPACT框架和BluePrint数据集，用于解决缺乏标准化社交媒体数据资源的问题，推动社交媒体模拟研究。


<details>
  <summary>Details</summary>
Motivation: 当前领域缺乏用于微调及评估大语言模型作为社交媒体代理的标准化数据资源。

Method: 引入SIMPACT框架构建行为导向的社交媒体数据集，将下一步行动预测作为训练和评估任务，引入聚类和总体层面的指标；发布基于公共Bluesky数据的BluePrint数据集。

Result: 发布了BluePrint数据集，该数据集能支持构建考虑上下文依赖的社交媒体用户代理模型。

Conclusion: SIMPACT框架为推进严格且符合伦理的社交媒体模拟提供基础，BluePrint可作为评估基准和构建特定领域数据集的模板。

Abstract: Large language models (LLMs) offer promising capabilities for simulating
social media dynamics at scale, enabling studies that would be ethically or
logistically challenging with human subjects. However, the field lacks
standardized data resources for fine-tuning and evaluating LLMs as realistic
social media agents. We address this gap by introducing SIMPACT, the
SIMulation-oriented Persona and Action Capture Toolkit, a privacy respecting
framework for constructing behaviorally-grounded social media datasets suitable
for training agent models. We formulate next-action prediction as a task for
training and evaluating LLM-based agents and introduce metrics at both the
cluster and population levels to assess behavioral fidelity and stylistic
realism. As a concrete implementation, we release BluePrint, a large-scale
dataset built from public Bluesky data focused on political discourse.
BluePrint clusters anonymized users into personas of aggregated behaviours,
capturing authentic engagement patterns while safeguarding privacy through
pseudonymization and removal of personally identifiable information. The
dataset includes a sizable action set of 12 social media interaction types
(likes, replies, reposts, etc.), each instance tied to the posting activity
preceding it. This supports the development of agents that use
context-dependence, not only in the language, but also in the interaction
behaviours of social media to model social media users. By standardizing data
and evaluation protocols, SIMPACT provides a foundation for advancing rigorous,
ethically responsible social media simulations. BluePrint serves as both an
evaluation benchmark for political discourse modeling and a template for
building domain specific datasets to study challenges such as misinformation
and polarization.

</details>


### [226] [Small Language Models for Curriculum-based Guidance](https://arxiv.org/abs/2510.02347)
*Konstantinos Katharakis,Sippo Rossi,Raghava Rao Mukkamala*

Main category: cs.CL

TL;DR: 研究探索用RAG管道开发评估基于课程指导的AI教学助手，对比8个SLMs与GPT - 4o，发现SLMs在合适提示和检索下可媲美LLMs，且有可持续性优势。


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI和大语言模型在教育领域应用，开发基于课程指导的AI教学助手。

Method: 运用检索增强生成（RAG）管道到选定的开源小语言模型（SLMs），对8个SLMs与GPT - 4o进行基准测试。

Result: 在适当提示和针对性检索下，SLMs能提供准确、符合教学法的回应，且有较低计算和能源需求等优势。

Conclusion: SLMs可作为教育机构实现可持续、节能的个性化学习的可行AI教学助手。

Abstract: The adoption of generative AI and large language models (LLMs) in education
is still emerging. In this study, we explore the development and evaluation of
AI teaching assistants that provide curriculum-based guidance using a
retrieval-augmented generation (RAG) pipeline applied to selected open-source
small language models (SLMs). We benchmarked eight SLMs, including LLaMA 3.1,
IBM Granite 3.3, and Gemma 3 (7-17B parameters), against GPT-4o. Our findings
show that with proper prompting and targeted retrieval, SLMs can match LLMs in
delivering accurate, pedagogically aligned responses. Importantly, SLMs offer
significant sustainability benefits due to their lower computational and energy
requirements, enabling real-time use on consumer-grade hardware without
depending on cloud infrastructure. This makes them not only cost-effective and
privacy-preserving but also environmentally responsible, positioning them as
viable AI teaching assistants for educational institutions aiming to scale
personalized learning in a sustainable and energy-efficient manner.

</details>


### [227] [mini-vec2vec: Scaling Universal Geometry Alignment with Linear Transformations](https://arxiv.org/abs/2510.02348)
*Guy Dar*

Main category: cs.CL

TL;DR: 提出mini - vec2vec改进vec2vec，高效且稳定，结果优。


<details>
  <summary>Details</summary>
Motivation: vec2vec计算成本高且不稳定，需更好的文本嵌入空间对齐方法。

Method: 由伪平行嵌入向量的试探性匹配、变换拟合和迭代细化三个主要阶段组成。

Result: 线性替代方法在效率上远超原vec2vec，结果相当或更好。

Conclusion: 方法稳定、算法步骤可解释，便于扩展并适用于新领域。

Abstract: We build upon vec2vec, a procedure designed to align text embedding spaces
without parallel data. vec2vec finds a near-perfect alignment, but it is
expensive and unstable. We present mini-vec2vec, a simple and efficient
alternative that requires substantially lower computational cost and is highly
robust. Moreover, the learned mapping is a linear transformation. Our method
consists of three main stages: a tentative matching of pseudo-parallel
embedding vectors, transformation fitting, and iterative refinement. Our linear
alternative exceeds the original instantiation of vec2vec by orders of
magnitude in efficiency, while matching or exceeding their results. The
method's stability and interpretable algorithmic steps facilitate scaling and
unlock new opportunities for adoption in new domains and fields.

</details>


### [228] [LLMSQL: Upgrading WikiSQL for the LLM Era of Text-to-SQL](https://arxiv.org/abs/2510.02350)
*Dzmitry Pihulski,Karol Charchut,Viktoria Novogrodskaia,Jan Kocoń*

Main category: cs.CL

TL;DR: 提出LLMSQL，对WikiSQL进行系统修订和转换，评估多个大语言模型，是适合大模型的基准。


<details>
  <summary>Details</summary>
Motivation: WikiSQL因结构和注释问题，在NL2SQL研究中的使用减少，需改进。

Method: 对WikiSQL错误分类，实施自动清理和重新注释方法，评估多个大语言模型。

Result: 未明确提及具体评估结果。

Conclusion: LLMSQL是适合大模型的基准，能为现代自然语言到SQL模型提供直接生成和评估。

Abstract: Converting natural language questions into SQL queries (Text-to-SQL) enables
non-expert users to interact with relational databases and has long been a
central task for natural language interfaces to data. While the WikiSQL dataset
played a key role in early NL2SQL research, its usage has declined due to
structural and annotation issues, including case sensitivity inconsistencies,
data type mismatches, syntax errors, and unanswered questions. We present
LLMSQL, a systematic revision and transformation of WikiSQL designed for the
LLM era. We classify these errors and implement automated methods for cleaning
and re-annotation. To assess the impact of these improvements, we evaluated
multiple large language models (LLMs), including Gemma 3, LLaMA 3.2, Mistral
7B, gpt-oss 20B, Phi-3.5 Mini, Qwen 2.5, OpenAI o4-mini, DeepSeek R1 and
others. Rather than serving as an update, LLMSQL is introduced as an LLM-ready
benchmark: unlike the original WikiSQL, tailored for pointer-network models
selecting tokens from input, LLMSQL provides clean natural language questions
and full SQL queries as plain text, enabling straightforward generation and
evaluation for modern natural language-to-SQL models.

</details>


### [229] [Language, Culture, and Ideology: Personalizing Offensiveness Detection in Political Tweets with Reasoning LLMs](https://arxiv.org/abs/2510.02351)
*Dzmitry Pihulski,Jan Kocoń*

Main category: cs.CL

TL;DR: 研究大语言模型从不同政治文化视角评估政治话语冒犯性，用数据集评估多模型，大模型表现更好，推理能力很关键。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型从特定政治文化视角评估政治话语冒犯性的情况。

Method: 使用2020年美国大选推文的多语言MD - Agreement数据集子集，评估多个大语言模型从不同政治人物视角判断推文冒犯性。

Result: 具有显式推理能力的大模型（如DeepSeek - R1、o4 - mini）对意识形态和文化差异更一致、更敏感，小模型常无法捕捉细微差别。

Conclusion: 推理能力能显著提高冒犯性判断的个性化和可解释性，是多语言和多意识形态社会政治文本分类的关键。

Abstract: We explore how large language models (LLMs) assess offensiveness in political
discourse when prompted to adopt specific political and cultural perspectives.
Using a multilingual subset of the MD-Agreement dataset centered on tweets from
the 2020 US elections, we evaluate several recent LLMs - including DeepSeek-R1,
o4-mini, GPT-4.1-mini, Qwen3, Gemma, and Mistral - tasked with judging tweets
as offensive or non-offensive from the viewpoints of varied political personas
(far-right, conservative, centrist, progressive) across English, Polish, and
Russian contexts. Our results show that larger models with explicit reasoning
abilities (e.g., DeepSeek-R1, o4-mini) are more consistent and sensitive to
ideological and cultural variation, while smaller models often fail to capture
subtle distinctions. We find that reasoning capabilities significantly improve
both the personalization and interpretability of offensiveness judgments,
suggesting that such mechanisms are key to adapting LLMs for nuanced
sociopolitical text classification across languages and ideologies.

</details>


### [230] [Evaluating Bias in Spoken Dialogue LLMs for Real-World Decisions and Recommendations](https://arxiv.org/abs/2510.02352)
*Yihao Wu,Tianrui Wang,Yizhou Peng,Yi-Wen Chao,Xuyi Zhuang,Xinsheng Wang,Shunshun Yin,Ziyang Ma*

Main category: cs.CL

TL;DR: 本文系统评估语音大语言模型的偏差，发现闭源模型偏差通常较低，开源模型对年龄和性别更敏感，推荐任务会放大群体差异，多轮对话中偏差决策可能持续，还发布了数据集和代码。


<details>
  <summary>Details</summary>
Motivation: 现有研究对语音对话模型中的偏差及其特征探索不足，且副语言特征可能加剧偏差，影响决策和推荐任务公平性，因此开展研究。

Method: 使用Group Unfairness Score（GUS）衡量决策偏差，用similarity-based normalized statistics rate（SNSR）衡量推荐偏差，对开源模型（如Qwen2.5 - Omni、GLM - 4 - Voice）和闭源API（如GPT - 4o Audio、Gemini - 2.5 - Flash）进行评估。

Result: 闭源模型偏差通常较低，开源模型对年龄和性别更敏感，推荐任务会放大群体差异，多轮对话中偏差决策可能持续。

Conclusion: 本研究是对端到端语音对话模型偏差的首次系统研究，为构建公平可靠的音频交互系统提供见解，还发布数据集和代码以推动后续研究。

Abstract: While biases in large language models (LLMs), such as stereotypes and
cultural tendencies in outputs, have been examined and identified, their
presence and characteristics in spoken dialogue models (SDMs) with audio input
and output remain largely unexplored. Paralinguistic features, such as age,
gender, and accent, can affect model outputs; when compounded by multi-turn
conversations, these effects may exacerbate biases, with potential implications
for fairness in decision-making and recommendation tasks. In this paper, we
systematically evaluate biases in speech LLMs and study the impact of
multi-turn dialogues with repeated negative feedback. Bias is measured using
Group Unfairness Score (GUS) for decisions and similarity-based normalized
statistics rate (SNSR) for recommendations, across both open-source models like
Qwen2.5-Omni and GLM-4-Voice, as well as closed-source APIs such as GPT-4o
Audio and Gemini-2.5-Flash. Our analysis reveals that closed-source models
generally exhibit lower bias, while open-source models are more sensitive to
age and gender, and recommendation tasks tend to amplify cross-group
disparities. We found that biased decisions may persist in multi-turn
conversations. This work provides the first systematic study of biases in
end-to-end spoken dialogue models, offering insights towards fair and reliable
audio-based interactive systems. To facilitate further research, we release the
FairDialogue dataset and evaluation code.

</details>


### [231] [DiffuSpec: Unlocking Diffusion Language Models for Speculative Decoding](https://arxiv.org/abs/2510.02358)
*Guanghao Li,Zhihui Fu,Min Fang,Qibin Zhao,Ming Tang,Chun Yuan,Jun Wang*

Main category: cs.CL

TL;DR: 提出DiffuSpec框架，用预训练扩散语言模型生成多令牌草稿，引入CPS和ADL组件，实现高达3倍时钟加速。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型自回归解码延迟高，现有推测解码的AR起草器有局限，需更好方案。

Method: 提出DiffuSpec训练无关框架，用预训练扩散语言模型生成草稿，引入CPS和ADL组件。

Result: DiffuSpec在基准测试中实现高达3倍时钟加速。

Conclusion: 基于扩散的起草是推测解码中自回归起草器的强大替代方案。

Abstract: As large language models (LLMs) scale up, accuracy improves, but the
autoregressive (AR) nature of decoding increases latency since each token
requires a serial forward pass. Speculative decoding addresses this by
employing a fast drafter to propose multi-token drafts, which are then verified
in parallel by the target model. However, many deployments still rely on AR
drafters, where sequential passes limit wall-clock gains. We revisit the
drafting stage and present DiffuSpec, a training-free drop-in framework that
uses a pretrained diffusion language model (DLM) to produce multi-token drafts
in a single forward pass, while remaining compatible with standard AR
verifiers. Because DLM drafts are generated under bidirectional conditioning,
parallel per-position candidates form a token lattice in which the locally
highest-probability token at each position need not form a causal left-to-right
path. Moreover, DLM drafting requires pre-specifying a draft length, inducing a
speed-quality trade-off. To address these challenges, we introduce two
practical components: (i) a causal-consistency path search (CPS) over this
lattice that extracts a left-to-right path aligned with AR verification; and
(ii) an adaptive draft-length (ADL) controller that adjusts next proposal size
based on recent acceptance feedback and realized generated length. Across
benchmarks, DiffuSpec yields up to 3x wall-clock speedup, establishing
diffusion-based drafting as a robust alternative to autoregressive drafters for
speculative decoding.

</details>


### [232] [Emission-GPT: A domain-specific language model agent for knowledge retrieval, emission inventory and data analysis](https://arxiv.org/abs/2510.02359)
*Jiashu Ye,Tong Wu,Weiwen Chen,Hao Zhang,Zeteng Lin,Xingxing Li,Shujuan Weng,Manni Zhu,Xin Yuan,Xinlong Hong,Jingjie Li,Junyu Zheng,Zhijiong Huang,Jing Tang*

Main category: cs.CL

TL;DR: 论文提出针对大气排放领域的知识增强大语言模型代理Emission - GPT，能支持领域问答和排放数据分析，案例证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有排放相关知识碎片化、专业化，获取和编译排放数据方法低效，阻碍非专家解读排放信息，影响研究和管理。

Method: 基于超10000份文档构建知识库，集成提示工程和问题补全，支持领域问答，让用户通过自然语言交互分析排放数据。

Result: 广东案例显示Emission - GPT能用简单提示从原始数据中提取关键信息。

Conclusion: Emission - GPT模块化、可扩展架构便于自动化传统手动工作流程，是下一代排放清单开发和情景评估的基础工具。

Abstract: Improving air quality and addressing climate change relies on accurate
understanding and analysis of air pollutant and greenhouse gas emissions.
However, emission-related knowledge is often fragmented and highly specialized,
while existing methods for accessing and compiling emissions data remain
inefficient. These issues hinder the ability of non-experts to interpret
emissions information, posing challenges to research and management. To address
this, we present Emission-GPT, a knowledge-enhanced large language model agent
tailored for the atmospheric emissions domain. Built on a curated knowledge
base of over 10,000 documents (including standards, reports, guidebooks, and
peer-reviewed literature), Emission-GPT integrates prompt engineering and
question completion to support accurate domain-specific question answering.
Emission-GPT also enables users to interactively analyze emissions data via
natural language, such as querying and visualizing inventories, analyzing
source contributions, and recommending emission factors for user-defined
scenarios. A case study in Guangdong Province demonstrates that Emission-GPT
can extract key insights--such as point source distributions and sectoral
trends--directly from raw data with simple prompts. Its modular and extensible
architecture facilitates automation of traditionally manual workflows,
positioning Emission-GPT as a foundational tool for next-generation emission
inventory development and scenario-based assessment.

</details>


### [233] [Spiral of Silence in Large Language Model Agents](https://arxiv.org/abs/2510.02360)
*Mingze Zhong,Meng Fang,Zijing Shi,Yuxuan Huang,Shunfeng Zheng,Yali Du,Ling Chen,Jun Wang*

Main category: cs.CL

TL;DR: 研究大语言模型集体中是否会出现类似沉默的螺旋动态，提出评估框架并实验，表明历史和角色信号共同作用可复制该模式，无历史锚定则无法出现，强调监控和缓解从众现象的必要性。


<details>
  <summary>Details</summary>
Motivation: 经典沉默的螺旋理论用于人类社会，探讨大语言模型集体中能否基于纯统计语言生成出现类似动态。

Method: 提出评估框架，设置四种控制条件，用趋势测试和集中度测量评估观点动态。

Result: 历史和角色信号共同产生强多数主导并复制模式；仅历史信号导致强锚定；仅角色信号促成多样但不相关的观点。

Conclusion: 该研究连接计算社会学和负责任的AI设计，需监控和缓解大语言模型代理系统中的从众现象。

Abstract: The Spiral of Silence (SoS) theory holds that individuals with minority views
often refrain from speaking out for fear of social isolation, enabling majority
positions to dominate public discourse. When the 'agents' are large language
models (LLMs), however, the classical psychological explanation is not directly
applicable, since SoS was developed for human societies. This raises a central
question: can SoS-like dynamics nevertheless emerge from purely statistical
language generation in LLM collectives? We propose an evaluation framework for
examining SoS in LLM agents. Specifically, we consider four controlled
conditions that systematically vary the availability of 'History' and 'Persona'
signals. Opinion dynamics are assessed using trend tests such as Mann-Kendall
and Spearman's rank, along with concentration measures including kurtosis and
interquartile range. Experiments across open-source and closed-source models
show that history and persona together produce strong majority dominance and
replicate SoS patterns; history signals alone induce strong anchoring; and
persona signals alone foster diverse but uncorrelated opinions, indicating that
without historical anchoring, SoS dynamics cannot emerge. The work bridges
computational sociology and responsible AI design, highlighting the need to
monitor and mitigate emergent conformity in LLM-agent systems.

</details>


### [234] [ChunkLLM: A Lightweight Pluggable Framework for Accelerating LLMs Inference](https://arxiv.org/abs/2510.02361)
*Haojie Ouyang,Jianwei Lv,Lei Ren,Chen Wei,Xiaojie Wang,Fangxiang Feng*

Main category: cs.CL

TL;DR: 提出轻量级可插拔训练框架ChunkLLM解决Transformer大模型计算效率问题，实验表现良好。


<details>
  <summary>Details</summary>
Motivation: Transformer大模型因自注意力机制计算效率低，现有基于块选择和压缩的方法存在语义不完整或训练推理效率低的问题。

Method: 提出ChunkLLM框架，包含QK Adapter和Chunk Adapter组件，训练时冻结主干参数，设计注意力蒸馏方法训练QK Adapter，推理时仅在检测到块边界时触发块选择。

Result: 在多任务长文本和短文本基准数据集上实验，ChunkLLM在短文本基准上表现相当，在长上下文基准上保持98.64%性能，关键值缓存保留率48.58%，处理120K长文本时最高加速4.48倍。

Conclusion: ChunkLLM能有效解决Transformer大模型计算效率问题，且在性能上表现良好。

Abstract: Transformer-based large models excel in natural language processing and
computer vision, but face severe computational inefficiencies due to the
self-attention's quadratic complexity with input tokens. Recently, researchers
have proposed a series of methods based on block selection and compression to
alleviate this problem, but they either have issues with semantic
incompleteness or poor training-inference efficiency. To comprehensively
address these challenges, we propose ChunkLLM, a lightweight and pluggable
training framework. Specifically, we introduce two components: QK Adapter
(Q-Adapter and K-Adapter) and Chunk Adapter. The former is attached to each
Transformer layer, serving dual purposes of feature compression and chunk
attention acquisition. The latter operates at the bottommost layer of the
model, functioning to detect chunk boundaries by leveraging contextual semantic
information. During the training phase, the parameters of the backbone remain
frozen, with only the QK Adapter and Chunk Adapter undergoing training.
Notably, we design an attention distillation method for training the QK
Adapter, which enhances the recall rate of key chunks. During the inference
phase, chunk selection is triggered exclusively when the current token is
detected as a chunk boundary, thereby accelerating model inference.
Experimental evaluations are conducted on a diverse set of long-text and
short-text benchmark datasets spanning multiple tasks. ChunkLLM not only
attains comparable performance on short-text benchmarks but also maintains
98.64% of the performance on long-context benchmarks while preserving a 48.58%
key-value cache retention rate. Particularly, ChunkLLM attains a maximum
speedup of 4.48x in comparison to the vanilla Transformer in the processing of
120K long texts.

</details>


### [235] [A Cross-Lingual Analysis of Bias in Large Language Models Using Romanian History](https://arxiv.org/abs/2510.02362)
*Matei-Iulian Cocu,Răzvan-Cosmin Cristia,Adrian Marius Dumitran*

Main category: cs.CL

TL;DR: 研究通过让大语言模型回答罗马尼亚历史争议问题评估其偏见，发现模型存在回答不一致问题。


<details>
  <summary>Details</summary>
Motivation: 研究有教育目的，且认识到历史常因文化等因素被歪曲呈现，大语言模型训练数据集有歧义会使用户缺乏中立性。

Method: 分三个阶段研究，验证预期回答类型会影响回答，让模型先作肯定回答，再以数值作答。

Result: 二元回答稳定性较高但不完美且因语言而异，模型会跨语言或格式改变立场，数值评级常与初始二元选择不同，最一致的模型不一定最准确或中立。

Conclusion: 研究揭示了模型在特定语言语境下存在回答不一致的倾向。

Abstract: In this case study, we select a set of controversial Romanian historical
questions and ask multiple Large Language Models to answer them across
languages and contexts, in order to assess their biases. Besides being a study
mainly performed for educational purposes, the motivation also lies in the
recognition that history is often presented through altered perspectives,
primarily influenced by the culture and ideals of a state, even through large
language models. Since they are often trained on certain data sets that may
present certain ambiguities, the lack of neutrality is subsequently instilled
in users. The research process was carried out in three stages, to confirm the
idea that the type of response expected can influence, to a certain extent, the
response itself; after providing an affirmative answer to some given question,
an LLM could shift its way of thinking after being asked the same question
again, but being told to respond with a numerical value of a scale. Results
show that binary response stability is relatively high but far from perfect and
varies by language. Models often flip stance across languages or between
formats; numeric ratings frequently diverge from the initial binary choice, and
the most consistent models are not always those judged most accurate or
neutral. Our research brings to light the predisposition of models to such
inconsistencies, within a specific contextualization of the language for the
question asked.

</details>


### [236] [Beyond Manuals and Tasks: Instance-Level Context Learning for LLM Agents](https://arxiv.org/abs/2510.02369)
*Kuntai Cai,Juncheng Liu,Xianglin Yang,Zhaojie Niu,Xiaokui Xiao,Xing Chen*

Main category: cs.CL

TL;DR: 论文识别出大语言模型（LLM）智能体缺失的实例级上下文，提出实例级上下文学习（ILCL）方法解决该问题，实验显示提升了智能体成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 发现LLM智能体在复杂任务中因缺少实例级上下文而常失败，该上下文包含特定环境实例的可验证和可复用事实。

Method: 将问题形式化为ILCL，采用引导式探索，用紧凑的待办事项森林确定行动优先级，通过轻量级的计划 - 行动 - 提取循环执行，生成高精度上下文文档。

Result: 在TextWorld、ALFWorld和Crafter实验中，提升了成功率和效率，如ReAct在TextWorld中平均成功率从37%升至95%。

Conclusion: 该方法将一次性探索转化为持久、可复用知识，补充现有上下文，使LLM智能体更可靠高效。

Abstract: Large language model (LLM) agents typically receive two kinds of context: (i)
environment-level manuals that define interaction interfaces and global rules,
and (ii) task-level guidance or demonstrations tied to specific goals. In this
work, we identify a crucial but overlooked third type of context,
instance-level context, which consists of verifiable and reusable facts tied to
a specific environment instance, such as object locations, crafting recipes,
and local rules. We argue that the absence of instance-level context is a
common source of failure for LLM agents in complex tasks, as success often
depends not only on reasoning over global rules or task prompts but also on
making decisions based on precise and persistent facts. Acquiring such context
requires more than memorization: the challenge lies in efficiently exploring,
validating, and formatting these facts under tight interaction budgets. We
formalize this problem as Instance-Level Context Learning (ILCL) and introduce
our task-agnostic method to solve it. Our method performs a guided exploration,
using a compact TODO forest to intelligently prioritize its next actions and a
lightweight plan-act-extract loop to execute them. This process automatically
produces a high-precision context document that is reusable across many
downstream tasks and agents, thereby amortizing the initial exploration cost.
Experiments across TextWorld, ALFWorld, and Crafter demonstrate consistent
gains in both success and efficiency: for instance, ReAct's mean success rate
in TextWorld rises from 37% to 95%, while IGE improves from 81% to 95%. By
transforming one-off exploration into persistent, reusable knowledge, our
method complements existing contexts to enable more reliable and efficient LLM
agents.

</details>


### [237] [Training Dynamics of Parametric and In-Context Knowledge Utilization in Language Models](https://arxiv.org/abs/2510.02370)
*Minsung Kim,Dong-Kyum Kim,Jea Kwon,Nakyeong Yang,Kyomin Jung,Meeyoung Cha*

Main category: cs.CL

TL;DR: 本文研究训练条件对大语言模型使用上下文知识和参数知识的影响，发现非理想语料特性对学习鲁棒仲裁策略很重要。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对训练中知识仲裁策略形成机制的系统理解，可能导致预训练模型产生不良仲裁行为并浪费计算资源。

Method: 在合成传记语料上训练基于Transformer的语言模型，系统控制各种条件。

Result: 文档内事实重复有助于参数和上下文能力发展，包含不一致信息或分布偏差的语料能促使模型发展鲁棒仲裁策略。

Conclusion: 非理想语料特性对学习鲁棒仲裁很重要，为预训练模型提供了整合参数和上下文知识的具体经验指导。

Abstract: Large language models often encounter conflicts between in-context knowledge
retrieved at inference time and parametric knowledge acquired during
pretraining. Models that accept external knowledge uncritically are vulnerable
to misinformation, whereas models that adhere rigidly to parametric knowledge
fail to benefit from retrieval. Despite the widespread adoption of
retrieval-augmented generation, we still lack a systematic understanding of
what shapes knowledge-arbitration strategies during training. This gap risks
producing pretrained models with undesirable arbitration behaviors and,
consequently, wasting substantial computational resources after the pretraining
budget has already been spent. To address this problem, we present the first
controlled study of how training conditions influence models' use of in-context
and parametric knowledge, and how they arbitrate between them. We train
transformer-based language models on a synthetic biographies corpus while
systematically controlling various conditions. Our experiments reveal that
intra-document repetition of facts fosters the development of both parametric
and in-context capabilities. Moreover, training on a corpus that contains
inconsistent information or distributional skew encourages models to develop
robust strategies for leveraging parametric and in-context knowledge. Rather
than viewing these non-ideal properties as artifacts to remove, our results
indicate that they are important for learning robust arbitration. These
insights offer concrete, empirical guidance for pretraining models that
harmoniously integrate parametric and in-context knowledge.

</details>


### [238] [Pretraining with hierarchical memories: separating long-tail and common knowledge](https://arxiv.org/abs/2510.02375)
*Hadi Pouransari,David Grangier,C Thomas,Michael Kirchhof,Oncel Tuzel*

Main category: cs.CL

TL;DR: 提出内存增强架构和预训练策略，使小语言模型借助参数化内存库提升性能，实验显示有显著效果。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型依赖参数扩展提升性能，将所有知识压缩进参数既不必要也不适用于边缘设备，需解决此问题。

Method: 采用内存增强架构和与现有硬件范式对齐的预训练策略，让小语言模型访问编码世界知识的分层参数化内存库，训练时获取上下文相关的小内存块添加到模型。

Result: 1. 160M参数模型结合从4.6B内存库获取的18M参数内存，性能与参数多两倍的常规模型相当。2. 研究了transformer中参数化内存的最优类型和大小，可扩展到超21B参数。

Conclusion: 提出的分层前馈内存能在transformer架构中稳健工作，无论是预训练时添加还是事后添加。

Abstract: The impressive performance gains of modern language models currently rely on
scaling parameters: larger models store more world knowledge and reason better.
Yet compressing all world knowledge into parameters is unnecessary, as only a
fraction is used per prompt, and impractical for edge devices with limited
inference-time memory and compute. We address this shortcoming by a
memory-augmented architecture and a pretraining strategy aligned with existing
hardware paradigms. We introduce small language models that access large
hierarchical parametric memory banks encoding world knowledge. During
pretraining and inference, we fetch a small, context-dependent memory block and
add it to the model. Our pretraining learns to store long-tail world knowledge
in the memory parameters, while the small language model acts as an anchor
capturing common knowledge and general reasoning abilities. Through
trillion-token-scale experiments, we show significant gains: a 160M-parameters
model augmented with an 18M-parameters memory fetched from a 4.6B memory bank
obtains comparable performance to a regular model with more than 2x the
parameters. Through extensive experiments, we study the optimal type and size
of parametric memories in transformers, scaling them to over 21B parameters. We
find that our proposed hierarchical feed-forward memories work robustly across
transformer architectures, whether added during pretraining or post-hoc.

</details>


### [239] [CLARITY: Clinical Assistant for Routing, Inference, and Triage](https://arxiv.org/abs/2510.02463)
*Vladimir Shaposhnikov,Aleksandr Nesterov,Ilia Kopanichuk,Ivan Bakulin,Egor Zhelvakov,Ruslan Abramov,Ekaterina Tsapieva,Dmitry V. Dylov,Ivan Oseledets*

Main category: cs.CL

TL;DR: 介绍AI驱动平台CLARITY，结合FSM和LLM分析症状、分诊，集成到大型平台效果好。


<details>
  <summary>Details</summary>
Motivation: 设计一个能促进患者到专科医生转诊、临床咨询和病情严重程度评估的平台。

Method: 采用结合有限状态机（FSM）和使用大语言模型（LLM）的协作代理的混合架构，构建在模块化微服务框架上。

Result: 集成到全国性大型医院间IT平台，两个月完成超55000次对话，2500次经专家标注验证，首诊路由精度超人类，咨询时间最多缩短至三分之一。

Conclusion: CLARITY安全、高效、稳健，可灵活扩展，性能优于人类。

Abstract: We present CLARITY (Clinical Assistant for Routing, Inference, and Triage),
an AI-driven platform designed to facilitate patient-to-specialist routing,
clinical consultations, and severity assessment of patients' conditions. Its
hybrid architecture combines a Finite State Machine (FSM) for structured
dialogue flows with collaborative agents that employ Large Language Model (LLM)
to analyze symptoms and prioritize referrals to appropriate specialists. Built
on a modular microservices framework, CLARITY ensures safe, efficient, and
robust performance, flexible and readily scalable to meet the demands of
existing workflows and IT solutions in healthcare.
  We report integration of our clinical assistant into a large-scale
nation-wide inter-hospital IT platform, with over 55,000 content-rich user
dialogues completed within the two months of deployment, 2,500 of which were
expert-annotated for a consequent validation. The validation results show that
CLARITY surpasses human-level performance in terms of the first-attempt routing
precision, naturally requiring up to 3 times shorter duration of the
consultation than with a human.

</details>


### [240] [Knowledge-Graph Based RAG System Evaluation Framework](https://arxiv.org/abs/2510.02549)
*Sicheng Dong,Vahid Zolfaghari,Nenad Petrovic,Alois Knoll*

Main category: cs.CL

TL;DR: 本文聚焦RAG系统评估难题，基于RAGAS框架拓展为KG评估范式，验证其有效性并探讨未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统评估指标难以有效评估RAG系统，需新评估方法。

Method: 将RAGAS框架拓展为基于KG的评估范式，结合多跳推理和语义社区聚类。

Result: 与RAGAS分数对比、构建人工标注子集验证方法有效性，实验表明对语义差异更敏感。

Conclusion: 提出KG评估范式，讨论评估RAG系统挑战与未来研究方向。

Abstract: Large language models (LLMs) has become a significant research focus and is
utilized in various fields, such as text generation and dialog systems. One of
the most essential applications of LLM is Retrieval Augmented Generation (RAG),
which greatly enhances generated content's reliability and relevance. However,
evaluating RAG systems remains a challenging task. Traditional evaluation
metrics struggle to effectively capture the key features of modern
LLM-generated content that often exhibits high fluency and naturalness.
Inspired by the RAGAS tool, a well-known RAG evaluation framework, we extended
this framework into a KG-based evaluation paradigm, enabling multi-hop
reasoning and semantic community clustering to derive more comprehensive
scoring metrics. By incorporating these comprehensive evaluation criteria, we
gain a deeper understanding of RAG systems and a more nuanced perspective on
their performance. To validate the effectiveness of our approach, we compare
its performance with RAGAS scores and construct a human-annotated subset to
assess the correlation between human judgments and automated metrics. In
addition, we conduct targeted experiments to demonstrate that our KG-based
evaluation method is more sensitive to subtle semantic differences in generated
outputs. Finally, we discuss the key challenges in evaluating RAG systems and
highlight potential directions for future research.

</details>


### [241] [EEFSUVA: A New Mathematical Olympiad Benchmark](https://arxiv.org/abs/2510.01227)
*Nicole N Khatibi,Daniil A. Radamovich,Michael P. Brenner*

Main category: cs.CL

TL;DR: 本文详细检验大语言模型（LLMs）在数学基准测试中表现的说法，引入新基准EEFSUVA进行评估，发现LLMs表现下降，强调更广泛评估数据集的重要性。


<details>
  <summary>Details</summary>
Motivation: 检验当前基准是否能真正反映LLMs的数学推理能力，当前基准可能因数据污染和问题类型狭窄而高估模型推理能力。

Method: 引入新基准EEFSUVA，其题目来自东欧和前苏联国家不太常见的地区和全国奥林匹克竞赛。

Result: 即使是最先进的LLMs在EEFSUVA上的表现也比其他奥林匹克风格的基准有显著下降。

Conclusion: 更广泛的评估数据集对全面评估数学推理能力和指导未来模型开发具有潜在重要性。

Abstract: Recent breakthroughs have spurred claims that large language models (LLMs)
match gold medal Olympiad to graduate level proficiency on mathematics
benchmarks. In this work, we examine these claims in detail and assess the
extent to which current benchmarks capture genuine LLM mathematical reasoning.
The composition of these benchmarks, primarily drawing from the International
Mathematics Olympiad (IMO) and related competitions, may overstate models
reasoning ability due to potential data contamination and a narrow focus on
familiar problem types. To enable a more holistic assessment of mathematical
understanding, we introduce EEFSUVA, a novel benchmark curated from under
circulated regional and national Olympiads of Eastern Europe and the countries
from the former Soviet Union. These contests feature problems of comparable
difficulty to the IMO and are renowned for demanding nonstandard
problem-solving techniques, yet their problems are far less prevalent in online
corpora. Preliminary results suggest that even state-of-the-art LLMs exhibit a
notable performance decline on EEFSUVA relative to other Olympiad-style
benchmarks. These findings also suggest the potential importance of broader
evaluation datasets for a fuller assessment of mathematical reasoning and for
guiding future model development.

</details>


### [242] [Can Prompts Rewind Time for LLMs? Evaluating the Effectiveness of Prompted Knowledge Cutoffs](https://arxiv.org/abs/2510.02340)
*Xin Gao,Ruiyi Zhang,Daniel Du,Saurabh Mahindre,Sai Ashish Somayajula,Pengtao Xie*

Main category: cs.CL

TL;DR: 研究提示能否让大语言模型模拟早期知识截止，构建数据集评估，发现提示模拟有局限，需更严格评估。


<details>
  <summary>Details</summary>
Motivation: 大语言模型用于时间预测时因依赖预训练数据有污染问题，且有基于提示的遗忘技术，探究能否用提示模拟早期知识截止。

Method: 构建三个评估数据集，评估大语言模型对直接事实知识、语义变化和因果相关知识的遗忘程度。

Result: 基于提示的模拟知识截止在直接查询特定日期后信息时有效，但在遗忘内容与查询因果相关且未直接询问时难以诱导遗忘。

Conclusion: 在将大语言模型应用于时间预测任务时需要更严格的评估设置。

Abstract: Large Language Models (LLMs) are widely used for temporal prediction, but
their reliance on pretraining data raises contamination concerns, as accurate
predictions on pre-cutoff test data may reflect memorization rather than
reasoning, leading to an overestimation of their generalization capability.
With the recent emergence of prompting-based unlearning techniques, a natural
question arises: Can LLMs be prompted to simulate an earlier knowledge cutoff?
In this work, we investigate the capability of prompting to simulate earlier
knowledge cutoff in LLMs. We construct three evaluation datasets to assess the
extent to which LLMs can forget (1) direct factual knowledge, (2) semantic
shifts, and (3) causally related knowledge. Results demonstrate that while
prompt-based simulated knowledge cutoffs show effectiveness when directly
queried with the information after that date, they struggle to induce
forgetting when the forgotten content is not directly asked but causally
related to the query. These findings highlight the need for more rigorous
evaluation settings when applying LLMs for temporal prediction tasks. The full
dataset and evaluation code are available at
https://github.com/gxx27/time_unlearn.

</details>


### [243] [An Senegalese Legal Texts Structuration Using LLM-augmented Knowledge Graph](https://arxiv.org/abs/2510.02353)
*Oumar Kane,Mouhamad M. Allaya,Dame Samb,Mamadou Bousso*

Main category: cs.CL

TL;DR: 研究探索在塞内加尔司法系统应用AI和LLM改善法律文本获取，成功提取法律条文、构建图数据库并验证模型有效性，以助民众和法律人士理解权责。


<details>
  <summary>Details</summary>
Motivation: 解决塞内加尔司法系统中法律文件提取和组织困难，提升司法信息获取便利性。

Method: 从各类法律文件中提取条文，开发图数据库，运用高级三元组提取技术，验证GPT - 4o、GPT - 4和Mistral - Large等模型。

Result: 成功从法律文件中提取7967条条文，构建含2872个节点和10774个关系的图数据库，验证模型在识别关系和元数据方面有效。

Conclusion: 利用这些技术可创建框架，帮助塞内加尔公民和法律专业人士更有效理解自身权利和责任。

Abstract: This study examines the application of artificial intelligence (AI) and large
language models (LLM) to improve access to legal texts in Senegal's judicial
system. The emphasis is on the difficulties of extracting and organizing legal
documents, highlighting the need for better access to judicial information. The
research successfully extracted 7,967 articles from various legal documents,
particularly focusing on the Land and Public Domain Code. A detailed graph
database was developed, which contains 2,872 nodes and 10,774 relationships,
aiding in the visualization of interconnections within legal texts. In
addition, advanced triple extraction techniques were utilized for knowledge,
demonstrating the effectiveness of models such as GPT-4o, GPT-4, and
Mistral-Large in identifying relationships and relevant metadata. Through these
technologies, the aim is to create a solid framework that allows Senegalese
citizens and legal professionals to more effectively understand their rights
and responsibilities.

</details>


### [244] [Modeling the language cortex with form-independent and enriched representations of sentence meaning reveals remarkable semantic abstractness](https://arxiv.org/abs/2510.02354)
*Shreya Saha,Shurui Li,Greta Tuckute,Yuanning Li,Ru-Yuan Zhang,Leila Wehbe,Evelina Fedorenko,Meenakshi Khosla*

Main category: cs.CL

TL;DR: 通过视觉和语言模型建模句子的神经反应，发现语言皮层存在高度抽象、形式独立的意义表征。


<details>
  <summary>Details</summary>
Motivation: 探讨语言皮层中意义表征的抽象性。

Method: 用视觉和语言模型的表征来建模句子的神经反应，生成对应图像并提取视觉模型嵌入，对句子的多个释义取平均等。

Result: 聚合多个生成图像能更准确预测语言皮层反应；对句子多个释义取平均可提高预测准确性；丰富释义的上下文细节进一步增加预测准确性，甚至超过原句嵌入的预测。

Conclusion: 语言皮层中存在高度抽象、形式独立的意义表征。

Abstract: The human language system represents both linguistic forms and meanings, but
the abstractness of the meaning representations remains debated. Here, we
searched for abstract representations of meaning in the language cortex by
modeling neural responses to sentences using representations from vision and
language models. When we generate images corresponding to sentences and extract
vision model embeddings, we find that aggregating across multiple generated
images yields increasingly accurate predictions of language cortex responses,
sometimes rivaling large language models. Similarly, averaging embeddings
across multiple paraphrases of a sentence improves prediction accuracy compared
to any single paraphrase. Enriching paraphrases with contextual details that
may be implicit (e.g., augmenting "I had a pancake" to include details like
"maple syrup") further increases prediction accuracy, even surpassing
predictions based on the embedding of the original sentence, suggesting that
the language system maintains richer and broader semantic representations than
language models. Together, these results demonstrate the existence of highly
abstract, form-independent meaning representations within the language cortex.

</details>


### [245] [Uncertainty-Aware Answer Selection for Improved Reasoning in Multi-LLM Systems](https://arxiv.org/abs/2510.02377)
*Aakriti Agrawal,Rohith Aralikatti,Anirudh Satheesh,Souradip Chakraborty,Amrit Singh Bedi,Furong Huang*

Main category: cs.CL

TL;DR: 提出用校准对数似然分数从多个大语言模型选最佳响应的方法，在多个数据集上有提升。


<details>
  <summary>Details</summary>
Motivation: 从多个大语言模型中选最可靠响应有挑战，现有方法有局限，多LLM系统常表现不佳。

Method: 提出用校准对数似然分数从多个不同大语言模型中选择最佳响应的方法，隐式利用模型知识和置信度。

Result: 在GSM8K、MMLU（6子集）和ARC数据集的辩论和非辩论设置中分别提升约4%、3%和5%。

Conclusion: 该方法能有效从多个大语言模型中选出最佳响应，具有一定优势。

Abstract: Large Language Models (LLMs) have demonstrated exceptional capabilities, yet
selecting the most reliable response from multiple LLMs remains a challenge,
particularly in resource-constrained settings. Existing approaches often depend
on costly external verifiers, human evaluators, or self-consistency techniques
that require multiple samples from a single model. While multi-LLM systems
produce more diverse responses than single models and thus have greater
potential, they often underperform compared to single LLM self-consistency. We
propose a principled, novel and computationally efficient method to select the
best response from multiple different LLMs using a calibrated log-likelihood
score, implicitly leveraging the inherent knowledge and confidence of these
models. Our method demonstrates improvements of approx. 4%, 3%, and 5% across
both debate (multi-round LLM discussions) and non-debate (Best-of-N with
multiple LLMs) settings on GSM8K, MMLU (6 subsets), and ARC datasets
respectively.

</details>


### [246] [Time-To-Inconsistency: A Survival Analysis of Large Language Model Robustness to Adversarial Attacks](https://arxiv.org/abs/2510.02712)
*Yubo Li,Ramayya Krishnan,Rema Padman*

Main category: cs.CL

TL;DR: 本文对对话式AI的鲁棒性进行生存分析，发现不同语义漂移对对话失败的影响，表明生存分析可评估大模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有评估框架无法捕捉现实对话中随时间的退化动态，需研究大模型在多轮对话中的鲁棒性。

Method: 对9个最先进大模型的36951个对话轮次进行生存分析，采用Cox比例风险、加速失效时间和随机生存森林方法。

Result: 突然的语义漂移会增加对话失败风险，渐进的语义漂移可降低失败风险；含交互项的AFT模型性能更优。

Conclusion: 生存分析是评估大模型鲁棒性的有力范式，为设计对话代理提供见解，挑战了对话AI系统语义一致性的传统假设。

Abstract: Large Language Models (LLMs) have revolutionized conversational AI, yet their
robustness in extended multi-turn dialogues remains poorly understood. Existing
evaluation frameworks focus on static benchmarks and single-turn assessments,
failing to capture the temporal dynamics of conversational degradation that
characterize real-world interactions. In this work, we present the first
comprehensive survival analysis of conversational AI robustness, analyzing
36,951 conversation turns across 9 state-of-the-art LLMs to model failure as a
time-to-event process. Our survival modeling framework-employing Cox
proportional hazards, Accelerated Failure Time, and Random Survival Forest
approaches-reveals extraordinary temporal dynamics. We find that abrupt,
prompt-to-prompt(P2P) semantic drift is catastrophic, dramatically increasing
the hazard of conversational failure. In stark contrast, gradual, cumulative
drift is highly protective, vastly reducing the failure hazard and enabling
significantly longer dialogues. AFT models with interactions demonstrate
superior performance, achieving excellent discrimination and exceptional
calibration. These findings establish survival analysis as a powerful paradigm
for evaluating LLM robustness, offer concrete insights for designing resilient
conversational agents, and challenge prevailing assumptions about the necessity
of semantic consistency in conversational AI Systems.

</details>


### [247] [TravelBench : Exploring LLM Performance in Low-Resource Domains](https://arxiv.org/abs/2510.02719)
*Srinivas Billa,Xiaonan Jing*

Main category: cs.CL

TL;DR: 现有大语言模型基准测试难以反映低资源任务能力，本文整理旅行领域数据集分析大语言模型表现，发现通用基准不足，推理对小模型提升大。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型基准测试在低资源任务中不能有效反映模型能力，难以开发有效解决方案。

Method: 整理14个旅行领域数据集，涵盖7种常见NLP任务，分析大语言模型在各种任务中的表现。

Result: 通用基准测试结果不足以了解低资源任务中的模型性能；开箱即用的大语言模型在复杂特定领域场景中会遇到性能瓶颈；推理对较小的大语言模型有更显著提升。

Conclusion: 通用基准测试不适用于评估低资源任务模型性能，推理对小模型有更大帮助。

Abstract: Results on existing LLM benchmarks capture little information over the model
capabilities in low-resource tasks, making it difficult to develop effective
solutions in these domains. To address these challenges, we curated 14
travel-domain datasets spanning 7 common NLP tasks using anonymised data from
real-world scenarios, and analysed the performance across LLMs. We report on
the accuracy, scaling behaviour, and reasoning capabilities of LLMs in a
variety of tasks. Our results confirm that general benchmarking results are
insufficient for understanding model performance in low-resource tasks. Despite
the amount of training FLOPs, out-of-the-box LLMs hit performance bottlenecks
in complex, domain-specific scenarios. Furthermore, reasoning provides a more
significant boost for smaller LLMs by making the model a better judge on
certain tasks.

</details>


### [248] [Words That Make Language Models Perceive](https://arxiv.org/abs/2510.02425)
*Sophie L. Wang,Phillip Isola,Brian Cheung*

Main category: cs.CL

TL;DR: 研究发现轻量级提示工程可在纯文本训练的大语言模型中激活合适模态的表征。


<details>
  <summary>Details</summary>
Motivation: 纯文本训练的大语言模型虽无直接感知经验，但内部表征受多模态规律影响，测试明确感官提示能否使模型与专业视觉和音频编码器更接近。

Method: 使用感官提示让模型‘看’或‘听’，以提示其根据潜在视觉或听觉证据进行下一个词的预测。

Result: 轻量级提示工程能可靠地在纯文本训练的大语言模型中激活合适模态的表征。

Conclusion: 明确感官提示能使纯文本训练的大语言模型更接近专业视觉和音频编码器的表征。

Abstract: Large language models (LLMs) trained purely on text ostensibly lack any
direct perceptual experience, yet their internal representations are implicitly
shaped by multimodal regularities encoded in language. We test the hypothesis
that explicit sensory prompting can surface this latent structure, bringing a
text-only LLM into closer representational alignment with specialist vision and
audio encoders. When a sensory prompt tells the model to 'see' or 'hear', it
cues the model to resolve its next-token predictions as if they were
conditioned on latent visual or auditory evidence that is never actually
supplied. Our findings reveal that lightweight prompt engineering can reliably
activate modality-appropriate representations in purely text-trained LLMs.

</details>


### [249] [Unraveling Syntax: How Language Models Learn Context-Free Grammars](https://arxiv.org/abs/2510.02524)
*Laura Ying Schulz,Daniel Mitropolsky,Tomaso Poggio*

Main category: cs.CL

TL;DR: 提出理解语言模型获取语法的新框架，研究小模型在PCFG生成的合成语言上的学习动态，发现transformers学习特点及模型局限，开启新研究方向。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽成果显著，但对其学习动态了解甚少，需要新框架理解语言模型获取语法的过程。

Method: 从观察多数领域由PCFG捕获入手，研究小模型在PCFG生成的合成语言上的学习动态，推导训练损失和KL散度的递归公式。

Result: transformers并行降低所有子语法的损失；子语法预训练可改善小模型最终损失，预训练模型内部表征更符合语法子结构；模型处理深层递归结构有困难。

Conclusion: 以PCFG作为测试平台研究transformers学习动态开启新研究方向，存在诸多待解决问题。

Abstract: We introduce a new framework for understanding how language models acquire
syntax. While large models achieve impressive results, little is known about
their learning dynamics. Our approach starts with the observation that most
domains of interest, such as natural language syntax, coding languages,
arithmetic problems, are captured by probabilistic context-free grammars
(PCFGs). We study the learning dynamics of small models trained on synthetic
languages generated from PCFGs, enabling precise control over grammar
complexity, recursion depth, and subgrammar structure. We prove several
general, recursive formulae for the training loss and Kullback-Leibler
divergence over the subgrammar structure of a PCFG. Empirically, we find that
unlike children, who first master simple substructures before progressing to
more complex constructions, transformers reduce loss across all subgrammars in
parallel. We further show that subgrammar pretraining can improve the final
loss for smaller models, and that pretrained models develop internal
representations more aligned with the grammar's substructure. Finally, we
demonstrate that models struggle with deeper recursive structures (a limitation
even of large language models), revealing fundamental challenges in how neural
networks represent hierarchical syntax. Overall, our work initiates the study
of the learning dynamics of transformers on PCFGs as a versatile testbed for
probing learning in language models, opening a research direction with many
open questions.

</details>


### [250] [A Computational Framework for Interpretable Text-Based Personality Assessment from Social Media](https://arxiv.org/abs/2510.02811)
*Matej Gjurković*

Main category: cs.CL

TL;DR: 论文针对人格评估自动化方法面临的数据集稀缺和学科脱节问题，构建MBTI9k和PANDORA数据集，实验发现人口变量影响模型有效性，开发了SIMPA框架用于可解释性人格评估，且该框架有广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 随着数字足迹增加，自动化人格评估方法愈发重要，但面临人格标注数据集稀缺和人格心理学与NLP脱节的问题，影响模型有效性和可解释性。

Method: 收集Reddit平台数据构建MBTI9k和PANDORA数据集；开发SIMPA框架，利用机器学习和语义相似度进行人格评估。

Result: 实验表明人口变量影响模型有效性；SIMPA框架能给出与人类评估相当的人格评估结果，且具有高可解释性和效率。

Conclusion: SIMPA框架虽专注于人格评估，但因其模型无关设计、分层线索检测和可扩展性，适用于涉及复杂标签分类和变量线索与目标概念关联的各类研究和实际应用。

Abstract: Personality refers to individual differences in behavior, thinking, and
feeling. With the growing availability of digital footprints, especially from
social media, automated methods for personality assessment have become
increasingly important. Natural language processing (NLP) enables the analysis
of unstructured text data to identify personality indicators. However, two main
challenges remain central to this thesis: the scarcity of large,
personality-labeled datasets and the disconnect between personality psychology
and NLP, which restricts model validity and interpretability. To address these
challenges, this thesis presents two datasets -- MBTI9k and PANDORA --
collected from Reddit, a platform known for user anonymity and diverse
discussions. The PANDORA dataset contains 17 million comments from over 10,000
users and integrates the MBTI and Big Five personality models with demographic
information, overcoming limitations in data size, quality, and label coverage.
Experiments on these datasets show that demographic variables influence model
validity. In response, the SIMPA (Statement-to-Item Matching Personality
Assessment) framework was developed - a computational framework for
interpretable personality assessment that matches user-generated statements
with validated questionnaire items. By using machine learning and semantic
similarity, SIMPA delivers personality assessments comparable to human
evaluations while maintaining high interpretability and efficiency. Although
focused on personality assessment, SIMPA's versatility extends beyond this
domain. Its model-agnostic design, layered cue detection, and scalability make
it suitable for various research and practical applications involving complex
label taxonomies and variable cue associations with target concepts.

</details>


### [251] [Evaluating Large Language Models for IUCN Red List Species Information](https://arxiv.org/abs/2510.02830)
*Shinya Uryu*

Main category: cs.CL

TL;DR: 研究验证5个大语言模型对21955个物种的评估能力，发现模型在分类学表现好但保护推理差，有知识推理差距和系统偏差，建议混合使用。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在保护领域应用增加，但对物种评估的可靠性不确定，需进行系统验证。

Method: 在IUCN红色名录的四个核心评估组件上对五个领先模型进行系统验证。

Result: 模型在分类学分类上表现出色（94.9%），但在保护推理上较差（状态评估27.2%），存在知识推理差距和偏向有魅力脊椎动物的系统偏差。

Conclusion: 大语言模型是信息检索的有力工具，但基于判断的决策需人工监督，建议采用混合方法，人类专家保留风险评估和政策制定的唯一权力。

Abstract: Large Language Models (LLMs) are rapidly being adopted in conservation to
address the biodiversity crisis, yet their reliability for species evaluation
is uncertain. This study systematically validates five leading models on 21,955
species across four core IUCN Red List assessment components: taxonomy,
conservation status, distribution, and threats. A critical paradox was
revealed: models excelled at taxonomic classification (94.9%) but consistently
failed at conservation reasoning (27.2% for status assessment). This
knowledge-reasoning gap, evident across all models, suggests inherent
architectural constraints, not just data limitations. Furthermore, models
exhibited systematic biases favoring charismatic vertebrates, potentially
amplifying existing conservation inequities. These findings delineate clear
boundaries for responsible LLM deployment: they are powerful tools for
information retrieval but require human oversight for judgment-based decisions.
A hybrid approach is recommended, where LLMs augment expert capacity while
human experts retain sole authority over risk assessment and policy.

</details>


### [252] [Constraint Satisfaction Approaches to Wordle: Novel Heuristics and Cross-Lexicon Validation](https://arxiv.org/abs/2510.02855)
*Jahidul Arafat,Fariha Tasmin,Sanjaya Poudel,Kamrujjaman,Eftakhar Ahmed Arnob,Ahsan Habib Tareq*

Main category: cs.CL

TL;DR: 提出Wordle的CSP公式与约束感知求解策略，在多方面优于现有方法，建立新性能基准。


<details>
  <summary>Details</summary>
Motivation: 现有求解器未进行正式约束处理，需提出新的Wordle约束满足问题（CSP）求解策略。

Method: 提出CSP - Aware Entropy和Probabilistic CSP框架，前者在约束传播后计算信息增益，后者结合贝叶斯词频先验与逻辑约束。

Result: CSP - Aware Entropy平均猜测次数3.54，成功率99.9%，运行时间快46%；有噪声时CSP - aware方法有优势，Probabilistic CSP全噪声水平成功率100%；跨词典验证成功率88%。

Conclusion: 有原则的约束满足技术在结构化谜题求解领域优于经典信息论和基于学习的方法。

Abstract: Wordle presents an algorithmically rich testbed for constraint satisfaction
problem (CSP) solving. While existing solvers rely on information-theoretic
entropy maximization or frequency-based heuristics without formal constraint
treatment, we present the first comprehensive CSP formulation of Wordle with
novel constraint-aware solving strategies. We introduce CSP-Aware Entropy,
computing information gain after constraint propagation rather than on raw
candidate sets, and a Probabilistic CSP framework integrating Bayesian
word-frequency priors with logical constraints. Through evaluation on 2,315
English words, CSP-Aware Entropy achieves 3.54 average guesses with 99.9%
success rate, a statistically significant 1.7% improvement over Forward
Checking (t=-4.82, p<0.001, Cohen's d=0.07) with 46% faster runtime (12.9ms
versus 23.7ms per guess). Under 10% noise, CSP-aware approaches maintain 5.3
percentage point advantages (29.0% versus 23.7%, p=0.041), while Probabilistic
CSP achieves 100% success across all noise levels (0-20%) through constraint
recovery mechanisms. Cross-lexicon validation on 500 Spanish words demonstrates
88% success with zero language-specific tuning, validating that core CSP
principles transfer across languages despite an 11.2 percentage point gap from
linguistic differences (p<0.001, Fisher's exact test). Our open-source
implementation with 34 unit tests achieving 91% code coverage provides
reproducible infrastructure for CSP research. The combination of formal CSP
treatment, constraint-aware heuristics, probabilistic-logical integration,
robustness analysis, and cross-lexicon validation establishes new performance
benchmarks demonstrating that principled constraint satisfaction techniques
outperform classical information-theoretic and learning-based approaches for
structured puzzle-solving domains.

</details>


### [253] [Uncertainty as Feature Gaps: Epistemic Uncertainty Quantification of LLMs in Contextual Question-Answering](https://arxiv.org/abs/2510.02671)
*Yavuz Bakman,Sungmin Kang,Zhiqi Huang,Duygu Nur Yaldiz,Catarina G. Belém,Chenyang Zhu,Anoop Kumar,Alfy Samuel,Salman Avestimehr,Daben Liu,Sai Praneeth Karimireddy*

Main category: cs.CL

TL;DR: 本文聚焦上下文问答任务的不确定性量化，提出理论方法，实验显示该方法大幅优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有不确定性量化研究主要集中于封闭事实问答，上下文问答的不确定性量化未被充分探索，而其在现实应用中很重要。

Method: 引入任务无关、词元级别的不确定性度量，分解该度量分离认知不确定性成分，近似真实分布，推导认知不确定性上界，提取三个特征并集成形成不确定性分数。

Result: 在多个问答基准测试的分布内和分布外设置中，该方法大幅优于现有无监督和有监督的不确定性量化方法，PRR 最多提升 13 点，推理开销可忽略不计。

Conclusion: 提出的上下文问答任务的不确定性量化方法有效且高效。

Abstract: Uncertainty Quantification (UQ) research has primarily focused on closed-book
factual question answering (QA), while contextual QA remains unexplored,
despite its importance in real-world applications. In this work, we focus on UQ
for the contextual QA task and propose a theoretically grounded approach to
quantify epistemic uncertainty. We begin by introducing a task-agnostic,
token-level uncertainty measure defined as the cross-entropy between the
predictive distribution of the given model and the unknown true distribution.
By decomposing this measure, we isolate the epistemic component and approximate
the true distribution by a perfectly prompted, idealized model. We then derive
an upper bound for epistemic uncertainty and show that it can be interpreted as
semantic feature gaps in the given model's hidden representations relative to
the ideal model. We further apply this generic framework to the contextual QA
task and hypothesize that three features approximate this gap: context-reliance
(using the provided context rather than parametric knowledge), context
comprehension (extracting relevant information from context), and honesty
(avoiding intentional lies). Using a top-down interpretability approach, we
extract these features by using only a small number of labeled samples and
ensemble them to form a robust uncertainty score. Experiments on multiple QA
benchmarks in both in-distribution and out-of-distribution settings show that
our method substantially outperforms state-of-the-art unsupervised
(sampling-free and sampling-based) and supervised UQ methods, achieving up to a
13-point PRR improvement while incurring a negligible inference overhead.

</details>


### [254] [Semantic Differentiation in Speech Emotion Recognition: Insights from Descriptive and Expressive Speech Roles](https://arxiv.org/abs/2510.03060)
*Rongchen Guo,Vincent Francoeur,Isar Nejadgholi,Sylvain Gagnon,Miodrag Bolic*

Main category: cs.CL

TL;DR: 研究区分语音描述性和表达性语义，通过实验发现其分别与预期和唤起情绪相关，为SER应用和上下文感知AI系统提供依据。


<details>
  <summary>Details</summary>
Motivation: 语音情感识别（SER）准确性受语音情感细微差别复杂性限制，需提升SER准确性以改善人机交互。

Method: 区分描述性语义和表达性语义，让参与者观看情感丰富电影片段后描述经历并记录音频，同时记录预期情感标签、自我评估情感反应及效价/唤醒分数。

Result: 描述性语义与预期情感一致，表达性语义与唤起情感相关。

Conclusion: 研究结果为SER在人机交互中的应用提供参考，为更具上下文感知能力的AI系统奠定基础。

Abstract: Speech Emotion Recognition (SER) is essential for improving human-computer
interaction, yet its accuracy remains constrained by the complexity of
emotional nuances in speech. In this study, we distinguish between descriptive
semantics, which represents the contextual content of speech, and expressive
semantics, which reflects the speaker's emotional state. After watching
emotionally charged movie segments, we recorded audio clips of participants
describing their experiences, along with the intended emotion tags for each
clip, participants' self-rated emotional responses, and their valence/arousal
scores. Through experiments, we show that descriptive semantics align with
intended emotions, while expressive semantics correlate with evoked emotions.
Our findings inform SER applications in human-AI interaction and pave the way
for more context-aware AI systems.

</details>


### [255] [Cache-to-Cache: Direct Semantic Communication Between Large Language Models](https://arxiv.org/abs/2510.03215)
*Tianyu Fu,Zihan Min,Hanling Zhang,Jichao Yan,Guohao Dai,Wanli Ouyang,Yu Wang*

Main category: cs.CL

TL;DR: 提出C2C范式实现大语言模型间直接语义通信，实验显示其准确率更高、速度更快。


<details>
  <summary>Details</summary>
Motivation: 现有多LLM系统中LLM通过文本通信会丢失语义信息和产生生成延迟，因此探索LLM能否超越文本通信。

Method: 提出C2C范式，用神经网络融合源模型和目标模型的KV-cache实现语义直接传输，用可学习的门控机制选择目标层。

Result: C2C比单个模型平均准确率高8.5 - 10.5%，比文本通信范式准确率高约3.0 - 5.0%，延迟平均加速2.0倍。

Conclusion: C2C是一种有效的多LLM系统通信范式，代码开源。

Abstract: Multi-LLM systems harness the complementary strengths of diverse Large
Language Models, achieving performance and efficiency gains unattainable by a
single model. In existing designs, LLMs communicate through text, forcing
internal representations to be transformed into output token sequences. This
process both loses rich semantic information and incurs token-by-token
generation latency. Motivated by these limitations, we ask: Can LLMs
communicate beyond text? Oracle experiments show that enriching the KV-Cache
semantics can improve response quality without increasing cache size,
supporting KV-Cache as an effective medium for inter-model communication. Thus,
we propose Cache-to-Cache (C2C), a new paradigm for direct semantic
communication between LLMs. C2C uses a neural network to project and fuse the
source model's KV-cache with that of the target model to enable direct semantic
transfer. A learnable gating mechanism selects the target layers that benefit
from cache communication. Compared with text communication, C2C utilizes the
deep, specialized semantics from both models, while avoiding explicit
intermediate text generation. Experiments show that C2C achieves 8.5-10.5%
higher average accuracy than individual models. It further outperforms the text
communication paradigm by approximately 3.0-5.0%, while delivering an average
2.0x speedup in latency. Our code is available at
https://github.com/thu-nics/C2C.

</details>


### [256] [Topic Modeling as Long-Form Generation: Can Long-Context LLMs revolutionize NTM via Zero-Shot Prompting?](https://arxiv.org/abs/2510.03174)
*Xuan Xu,Haolun Li,Zhongliang Yang,Beilin Chu,Jia Song,Moxuan Xu,Linna Zhou*

Main category: cs.CL

TL;DR: 本文探索大语言模型时代主题建模新范式，提出基于LLM的主题模型任务实现方法，比较LLM与NTM主题质量。


<details>
  <summary>Details</summary>
Motivation: 传统主题模型依赖推理和生成网络学习潜在主题分布，探索大语言模型时代主题建模新范式。

Method: 提出简单实用方法实现基于LLM的主题模型任务，通过零样本提示比较长文本生成范式与NTM，系统比较NTM和LLM的主题质量。

Result: 未提及具体结果。

Conclusion: 未提及具体结论。

Abstract: Traditional topic models such as neural topic models rely on inference and
generation networks to learn latent topic distributions. This paper explores a
new paradigm for topic modeling in the era of large language models, framing TM
as a long-form generation task whose definition is updated in this paradigm. We
propose a simple but practical approach to implement LLM-based topic model
tasks out of the box (sample a data subset, generate topics and representative
text with our prompt, text assignment with keyword match). We then investigate
whether the long-form generation paradigm can beat NTMs via zero-shot
prompting. We conduct a systematic comparison between NTMs and LLMs in terms of
topic quality and empirically examine the claim that "a majority of NTMs are
outdated."

</details>


### [257] [Self-Anchor: Large Language Model Reasoning via Step-by-step Attention Alignment](https://arxiv.org/abs/2510.03223)
*Hongxiang Zhang,Yuan Tian,Tianyi Zhang*

Main category: cs.CL

TL;DR: 提出Self - Anchor管道解决大语言模型复杂推理任务问题，实验显示其优于SOTA提示方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的方法在推理链延长时，关键中间步骤和原始提示易被埋没，导致推理出错，需解决大语言模型复杂推理任务问题。

Method: 提出Self - Anchor管道，将推理轨迹分解为结构化计划，自动调整模型注意力到最相关推理步骤。

Result: Self - Anchor在六个基准测试中优于SOTA提示方法，显著缩小‘非推理’模型和专业推理模型的性能差距。

Conclusion: Self - Anchor有潜力让大多数大语言模型无需重新训练就能处理复杂推理任务。

Abstract: To solve complex reasoning tasks for Large Language Models (LLMs),
prompting-based methods offer a lightweight alternative to fine-tuning and
reinforcement learning. However, as reasoning chains extend, critical
intermediate steps and the original prompt will be buried in the context,
receiving insufficient attention and leading to errors. In this paper, we
propose Self-Anchor, a novel pipeline that leverages the inherent structure of
reasoning to steer LLM attention. Self-Anchor decomposes reasoning trajectories
into structured plans and automatically aligns the model's attention to the
most relevant inference steps, allowing the model to maintain focus throughout
generation. Our experiment shows that Self-Anchor outperforms SOTA prompting
methods across six benchmarks. Notably, Self-Anchor significantly reduces the
performance gap between ``non-reasoning'' models and specialized reasoning
models, with the potential to enable most LLMs to tackle complex reasoning
tasks without retraining.

</details>


### [258] [Reward Models are Metrics in a Trench Coat](https://arxiv.org/abs/2510.03231)
*Sebastian Gehrmann*

Main category: cs.CL

TL;DR: 强化学习用于大语言模型后训练引发对奖励模型的关注，奖励模型与评估指标研究分离有冗余和问题，主张两领域加强合作并给出研究方向。


<details>
  <summary>Details</summary>
Motivation: 解决奖励模型和评估指标研究分离导致的冗余术语和重复陷阱等问题。

Method: 展示指标在特定任务上优于奖励模型，对两个领域进行广泛调查。

Result: 发现两领域分离问题，指出特定任务中指标表现，给出调查结果。

Conclusion: 两领域更紧密合作可克服现有问题，还指出多个可改进的研究方向。

Abstract: The emergence of reinforcement learning in post-training of large language
models has sparked significant interest in reward models. Reward models assess
the quality of sampled model outputs to generate training signals. This task is
also performed by evaluation metrics that monitor the performance of an AI
model. We find that the two research areas are mostly separate, leading to
redundant terminology and repeated pitfalls. Common challenges include
susceptibility to spurious correlations, impact on downstream reward hacking,
methods to improve data quality, and approaches to meta-evaluation. Our
position paper argues that a closer collaboration between the fields can help
overcome these issues. To that end, we show how metrics outperform reward
models on specific tasks and provide an extensive survey of the two areas.
Grounded in this survey, we point to multiple research topics in which closer
alignment can improve reward models and metrics in areas such as preference
elicitation methods, avoidance of spurious correlations and reward hacking, and
calibration-aware meta-evaluation.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [259] [Identifying Asymptomatic Nodes in Network Epidemics using Graph Neural Networks](https://arxiv.org/abs/2510.02568)
*Conrado Catarcione Pinto,Amanda Camacho Novaes de Oliveira,Rodrigo Sapienza Luna,Daniel Ratton Figueiredo*

Main category: cs.SI

TL;DR: 论文针对疫情中无症状感染者识别难题，采用图神经网络模型，评估显示该方法在不同场景下稳健有效。


<details>
  <summary>Details</summary>
Motivation: 疫情中无症状感染者会传播病毒且识别成本高，需有效方法识别。

Method: 采用经典SI网络流行病模型，用带监督学习的图神经网络模型，根据观察到的感染节点构建节点特征来分类健康节点。

Result: 该方法在不同网络模型、规模和观察到的感染比例下都稳健，能准确识别无症状节点。

Conclusion: 所提方法在不同场景下有效，可推广到不同网络规模和感染观察比例。

Abstract: Infected individuals in some epidemics can remain asymptomatic while still
carrying and transmitting the infection. These individuals contribute to the
spread of the epidemic and pose a significant challenge to public health
policies. Identifying asymptomatic individuals is critical for measuring and
controlling an epidemic, but periodic and widespread testing of healthy
individuals is often too costly. This work tackles the problem of identifying
asymptomatic individuals considering a classic SI (Susceptible-Infected)
network epidemic model where a fraction of the infected nodes are not observed
as infected (i.e., their observed state is identical to susceptible nodes). In
order to classify healthy nodes as asymptomatic or susceptible, a Graph Neural
Network (GNN) model with supervised learning is adopted where a set of node
features are built from the network with observed infected nodes. The approach
is evaluated across different network models, network sizes, and fraction of
observed infections. Results indicate that the proposed methodology is robust
across different scenarios, accurately identifying asymptomatic nodes while
also generalizing to different network sizes and fraction of observed
infections.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [260] [Action Deviation-Aware Inference for Low-Latency Wireless Robots](https://arxiv.org/abs/2510.02851)
*Jeyoung Park,Yeonsub Lim,Seungeun Oh,Jihong Park,Jinho Choi,Seong-Lyun Kim*

Main category: cs.RO

TL;DR: 本文针对6G分布式ML中行为克隆策略无法并行验证草稿的问题，提出动作偏差感知混合推理方法，减少传输和操作，降低延迟。


<details>
  <summary>Details</summary>
Motivation: 支持从自动驾驶到工业机器人操作等对延迟敏感的AI应用，解决行为克隆策略在分布式ML中无法并行验证草稿的问题。

Method: 提出动作偏差感知混合推理，利用草稿模型估计动作验证需求，选择性跳过服务器操作，推导路径偏差阈值平衡传输率和推理性能。

Result: 相比不跳过的混合推理，减少40%的上行传输和服务器操作，降低33.32%的端到端延迟，任务成功率达目标模型单独推理的97.03%。

Conclusion: 动作偏差感知混合推理能有效减少传输和操作，降低延迟，且保持较高的任务成功率。

Abstract: To support latency-sensitive AI applications ranging from autonomous driving
to industrial robot manipulation, 6G envisions distributed ML, connecting
distributed computational resources in edge and cloud over hyper-reliable
low-latency communication (HRLLC). In this setting, speculative decoding can
facilitate collaborative inference of models distributively deployed: an
on-device draft model locally generates drafts and a remote server-based target
model verifies and corrects them, resulting lower latency. However, unlike
autoregressive text generation, behavior cloning policies, typically used for
embodied AI applications like robot manipulation and autonomous driving, cannot
parallelize verification and correction for multiple drafts as each action
depends on observation which needs to be updated by a previous action. To this
end, we propose Action Deviation-Aware Hybrid Inference, wherein the draft
model estimates an action's need for verification and correction by the target
model and selectively skips communication and computation for server
operations. Action deviation shows a strong correlation with action's rejection
probability by the target model, enabling selective skipping. We derive the
path deviation threshold that balances the transmission rate and the inference
performance, and we empirically show that action deviation-aware hybrid
inference reduces uplink transmission and server operation by 40%, while
lowering end-to-end latency by 33.32% relative to hybrid inference without
skipping and achieving task success rate up to 97.03% of that of target model
only inference.

</details>


### [261] [SIMSplat: Predictive Driving Scene Editing with Language-aligned 4D Gaussian Splatting](https://arxiv.org/abs/2510.02469)
*Sung-Yeon Park,Adam Lee,Juanwu Lu,Can Cui,Luyang Jiang,Rohit Gupta,Kyungtae Han,Ahmadreza Moradipari,Ziran Wang*

Main category: cs.RO

TL;DR: 提出具有语言对齐高斯拼接的预测性驾驶场景编辑器SIMSplat，支持自然语言操作，实验展示其编辑能力和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有驾驶场景编辑框架因编辑能力有限，难以高效生成真实场景，需解决此问题。

Method: 提出SIMSplat编辑器，通过语言与高斯重建场景对齐，支持自然语言提示操作和道路对象查询，提供对象级编辑，结合多智能体运动预测优化路径。

Result: 在Waymo数据集上的实验表明SIMSplat有广泛的编辑能力和场景适应性。

Conclusion: SIMSplat是一种有效的驾驶场景编辑解决方案，能解决现有框架的不足。

Abstract: Driving scene manipulation with sensor data is emerging as a promising
alternative to traditional virtual driving simulators. However, existing
frameworks struggle to generate realistic scenarios efficiently due to limited
editing capabilities. To address these challenges, we present SIMSplat, a
predictive driving scene editor with language-aligned Gaussian splatting. As a
language-controlled editor, SIMSplat enables intuitive manipulation using
natural language prompts. By aligning language with Gaussian-reconstructed
scenes, it further supports direct querying of road objects, allowing precise
and flexible editing. Our method provides detailed object-level editing,
including adding new objects and modifying the trajectories of both vehicles
and pedestrians, while also incorporating predictive path refinement through
multi-agent motion prediction to generate realistic interactions among all
agents in the scene. Experiments on the Waymo dataset demonstrate SIMSplat's
extensive editing capabilities and adaptability across a wide range of
scenarios. Project page: https://sungyeonparkk.github.io/simsplat/

</details>


### [262] [A Trajectory Generator for High-Density Traffic and Diverse Agent-Interaction Scenarios](https://arxiv.org/abs/2510.02627)
*Ruining Yang,Yi Xu,Yixiao Chen,Yun Fu,Lili Su*

Main category: cs.RO

TL;DR: 提出新轨迹生成框架解决现有基准数据集长尾分布问题，提升场景密度和行为多样性，实验证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 现有基准数据集存在长尾分布问题，影响模型泛化和评估，需解决该问题以实现准确轨迹预测。

Method: 将连续道路环境转换为结构化网格表示，引入行为感知生成机制，结合规则决策触发、Frenet 轨迹平滑和动态可行性约束。

Result: 在大规模数据集上实验表明，方法显著提升了代理密度和行为多样性，保留了运动真实性和场景级安全，合成数据有利于下游轨迹预测模型。

Conclusion: 所提轨迹生成框架有效解决了现有数据集问题，能生成高质量合成数据，提升轨迹预测性能。

Abstract: Accurate trajectory prediction is fundamental to autonomous driving, as it
underpins safe motion planning and collision avoidance in complex environments.
However, existing benchmark datasets suffer from a pronounced long-tail
distribution problem, with most samples drawn from low-density scenarios and
simple straight-driving behaviors. This underrepresentation of high-density
scenarios and safety critical maneuvers such as lane changes, overtaking and
turning is an obstacle to model generalization and leads to overly optimistic
evaluations. To address these challenges, we propose a novel trajectory
generation framework that simultaneously enhances scenarios density and
enriches behavioral diversity. Specifically, our approach converts continuous
road environments into a structured grid representation that supports
fine-grained path planning, explicit conflict detection, and multi-agent
coordination. Built upon this representation, we introduce behavior-aware
generation mechanisms that combine rule-based decision triggers with
Frenet-based trajectory smoothing and dynamic feasibility constraints. This
design allows us to synthesize realistic high-density scenarios and rare
behaviors with complex interactions that are often missing in real data.
Extensive experiments on the large-scale Argoverse 1 and Argoverse 2 datasets
demonstrate that our method significantly improves both agent density and
behavior diversity, while preserving motion realism and scenario-level safety.
Our synthetic data also benefits downstream trajectory prediction models and
enhances performance in challenging high-density scenarios.

</details>


### [263] [A $1000\times$ Faster LLM-enhanced Algorithm For Path Planning in Large-scale Grid Maps](https://arxiv.org/abs/2510.02716)
*Junlin Zeng,Xin Zhang,Xiang Zhao,Yan Pan*

Main category: cs.RO

TL;DR: 现有路径规划方法在大规模地图上存在不足，LLM - A*计算时间长，本文提出iLLM - A*算法，速度大幅提升、节省内存且路径更优。


<details>
  <summary>Details</summary>
Motivation: 现有路径规划方法在大规模地图上因高搜索时间和内存消耗表现不佳，LLM - A*也存在计算时间长的问题，需要改进。

Method: 深入研究LLM - A*找到瓶颈，设计iLLM - A*算法，包含A*优化、LLM增量学习生成高质量路点和为A*选择合适路点三个机制。

Result: 与LLM - A*相比，iLLM - A*平均提速超1000倍，极端情况达2349.5倍，节省高达58.6%内存，路径更短且标准差更低。

Conclusion: iLLM - A*算法在路径规划上有显著优势，能有效解决大规模地图路径规划问题。

Abstract: Path planning in grid maps, arising from various applications, has garnered
significant attention. Existing methods, such as A*, Dijkstra, and their
variants, work well for small-scale maps but fail to address large-scale ones
due to high search time and memory consumption. Recently, Large Language Models
(LLMs) have shown remarkable performance in path planning but still suffer from
spatial illusion and poor planning performance. Among all the works, LLM-A*
\cite{meng2024llm} leverages LLM to generate a series of waypoints and then
uses A* to plan the paths between the neighboring waypoints. In this way, the
complete path is constructed. However, LLM-A* still suffers from high
computational time for large-scale maps. To fill this gap, we conducted a deep
investigation into LLM-A* and found its bottleneck, resulting in limited
performance. Accordingly, we design an innovative LLM-enhanced algorithm, abbr.
as iLLM-A*. iLLM-A* includes 3 carefully designed mechanisms, including the
optimization of A*, an incremental learning method for LLM to generate
high-quality waypoints, and the selection of the appropriate waypoints for A*
for path planning. Finally, a comprehensive evaluation on various grid maps
shows that, compared with LLM-A*, iLLM-A* \textbf{1) achieves more than
$1000\times$ speedup on average, and up to $2349.5\times$ speedup in the
extreme case, 2) saves up to $58.6\%$ of the memory cost, 3) achieves both
obviously shorter path length and lower path length standard deviation.}

</details>


### [264] [Work Zones challenge VLM Trajectory Planning: Toward Mitigation and Robust Autonomous Driving](https://arxiv.org/abs/2510.02803)
*Yifan Liao,Zhen Sun,Xiaoyun Qiu,Zixiao Zhao,Wenbing Tang,Xinlei He,Xinhu Zheng,Tianwei Zhang,Xinyi Huang,Xingshuo Han*

Main category: cs.RO

TL;DR: 文章首次系统研究工作区轨迹规划中视觉语言模型（VLMs），发现主流VLMs多数情况生成轨迹错误，提出REACT - Drive框架，实验证明其能降低误差、减少推理时间且具强实用性。


<details>
  <summary>Details</summary>
Motivation: 现有研究未探索VLMs在工作区的轨迹规划能力，文章旨在填补此空白。

Method: 先通过子图挖掘和聚类分析识别候选模式，经人工验证确定8种常见失败模式；提出REACT - Drive框架，结合VLMs与检索增强生成（RAG），用VLMs转换失败案例为规则和代码，RAG检索新场景相似模式指导轨迹生成。

Result: 在ROADWork数据集上，REACT - Drive相比VLM基线平均位移误差约降3倍；推理时间仅0.58s，低于微调等方法；实车实验在15个工作区场景展示其实用性。

Conclusion: REACT - Drive在工作区轨迹规划中能有效降低误差、减少推理时间，具有很强的实际应用价值。

Abstract: Visual Language Models (VLMs), with powerful multimodal reasoning
capabilities, are gradually integrated into autonomous driving by several
automobile manufacturers to enhance planning capability in challenging
environments. However, the trajectory planning capability of VLMs in work
zones, which often include irregular layouts, temporary traffic control, and
dynamically changing geometric structures, is still unexplored. To bridge this
gap, we conduct the \textit{first} systematic study of VLMs for work zone
trajectory planning, revealing that mainstream VLMs fail to generate correct
trajectories in $68.0%$ of cases. To better understand these failures, we first
identify candidate patterns via subgraph mining and clustering analysis, and
then confirm the validity of $8$ common failure patterns through human
verification. Building on these findings, we propose REACT-Drive, a trajectory
planning framework that integrates VLMs with Retrieval-Augmented Generation
(RAG). Specifically, REACT-Drive leverages VLMs to convert prior failure cases
into constraint rules and executable trajectory planning code, while RAG
retrieves similar patterns in new scenarios to guide trajectory generation.
Experimental results on the ROADWork dataset show that REACT-Drive yields a
reduction of around $3\times$ in average displacement error relative to VLM
baselines under evaluation with Qwen2.5-VL. In addition, REACT-Drive yields the
lowest inference time ($0.58$s) compared with other methods such as fine-tuning
($17.90$s). We further conduct experiments using a real vehicle in 15 work zone
scenarios in the physical world, demonstrating the strong practicality of
REACT-Drive.

</details>


### [265] [Flow with the Force Field: Learning 3D Compliant Flow Matching Policies from Force and Demonstration-Guided Simulation Data](https://arxiv.org/abs/2510.02738)
*Tianyu Li,Yihan Li,Zizhe Zhang,Nadia Figueroa*

Main category: cs.RO

TL;DR: 本文提出在仿真中生成力信息数据的框架，结合柔顺策略提升视觉运动策略性能，并在真实机器人任务中验证。


<details>
  <summary>Details</summary>
Motivation: 视觉运动策略在处理接触丰富任务时面临挑战，引入力信息虽有帮助但需大量数据，仿真生成数据又存在Sim2Real差距问题。

Method: 引入一个通过单个人类演示实例化的在仿真中生成力信息数据的框架，与柔顺策略耦合。

Result: 在真实机器人任务（非抓握块翻转和双手移动物体）中，学习到的策略展现出可靠的接触维持能力和对新条件的适应性。

Conclusion: 提出的方法能有效提升视觉运动策略在接触丰富任务中的性能。

Abstract: While visuomotor policy has made advancements in recent years, contact-rich
tasks still remain a challenge. Robotic manipulation tasks that require
continuous contact demand explicit handling of compliance and force. However,
most visuomotor policies ignore compliance, overlooking the importance of
physical interaction with the real world, often leading to excessive contact
forces or fragile behavior under uncertainty. Introducing force information
into vision-based imitation learning could help improve awareness of contacts,
but could also require a lot of data to perform well. One remedy for data
scarcity is to generate data in simulation, yet computationally taxing
processes are required to generate data good enough not to suffer from the
Sim2Real gap. In this work, we introduce a framework for generating
force-informed data in simulation, instantiated by a single human
demonstration, and show how coupling with a compliant policy improves the
performance of a visuomotor policy learned from synthetic data. We validate our
approach on real-robot tasks, including non-prehensile block flipping and a
bi-manual object moving, where the learned policy exhibits reliable contact
maintenance and adaptation to novel conditions. Project Website:
https://flow-with-the-force-field.github.io/webpage/

</details>


### [266] [Simulation to Rules: A Dual-VLM Framework for Formal Visual Planning](https://arxiv.org/abs/2510.03182)
*Yilun Hao,Yongchao Chen,Chuchu Fan,Yang Zhang*

Main category: cs.RO

TL;DR: 现有视觉语言模型（VLMs）在精确空间和长视野推理方面有不足，PDDL规划器无法处理视觉输入，此前方法在生成PDDL领域文件时有困难。本文提出VLMFP框架，能自主生成PDDL问题和领域文件，评估显示其有较好效果。


<details>
  <summary>Details</summary>
Motivation: 现有VLMs在视觉规划中难以准确生成PDDL领域文件，需人工预定义或不断访问环境进行细化，因此需要找到能自主生成PDDL问题和领域文件的方法。

Method: 提出VLMFP框架，引入两个VLMs，SimVLM根据输入规则描述模拟动作后果，GenVLM通过比较PDDL和SimVLM执行结果来生成和迭代细化PDDL文件。

Result: SimVLM能准确描述大部分场景、模拟动作序列和判断目标达成情况。在SimVLM指导下，VLMFP能为未见实例生成有效计划。

Conclusion: VLMFP框架能自主生成PDDL问题和领域文件，具有多层次泛化能力，在不同场景下有较好表现。

Abstract: Vision Language Models (VLMs) show strong potential for visual planning but
struggle with precise spatial and long-horizon reasoning. In contrast, Planning
Domain Definition Language (PDDL) planners excel at long-horizon formal
planning, but cannot interpret visual inputs. Recent works combine these
complementary advantages by enabling VLMs to turn visual planning problems into
PDDL files for formal planning. However, while VLMs can generate PDDL problem
files satisfactorily, they struggle to accurately generate the PDDL domain
files, which describe all the planning rules. As a result, prior methods rely
on human experts to predefine domain files or on constant environment access
for refinement. We propose VLMFP, a Dual-VLM-guided framework that can
autonomously generate both PDDL problem and domain files for formal visual
planning. VLMFP introduces two VLMs to ensure reliable PDDL file generation: A
SimVLM that simulates action consequences based on input rule descriptions, and
a GenVLM that generates and iteratively refines PDDL files by comparing the
PDDL and SimVLM execution results. VLMFP unleashes multiple levels of
generalizability: The same generated PDDL domain file works for all the
different instances under the same problem, and VLMs generalize to different
problems with varied appearances and rules. We evaluate VLMFP with 6 grid-world
domains and test its generalization to unseen instances, appearance, and game
rules. On average, SimVLM accurately describes 95.5%, 82.6% of scenarios,
simulates 85.5%, 87.8% of action sequence, and judges 82.4%, 85.6% goal
reaching for seen and unseen appearances, respectively. With the guidance of
SimVLM, VLMFP can generate PDDL files to reach 70.0%, 54.1% valid plans for
unseen instances in seen and unseen appearances, respectively. Project page:
https://sites.google.com/view/vlmfp.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [267] [Learning a distance measure from the information-estimation geometry of data](https://arxiv.org/abs/2510.02514)
*Guy Ohayon,Pierre-Etienne H. Fiquet,Florentin Guth,Jona Ballé,Eero P. Simoncelli*

Main category: eess.IV

TL;DR: 本文提出信息估计度量（IEM），证明其为有效全局度量，在高斯信号下与马氏距离一致，可利用学习的去噪器计算，在ImageNet数据库实验显示其在预测人类感知判断方面有竞争力。


<details>
  <summary>Details</summary>
Motivation: 提出一种新的距离函数，能适应复杂分布几何结构，用于图像质量评估等。

Method: 从信息论和估计论的基本关系出发推导IEM，证明其为有效全局度量并推导局部二阶近似的黎曼度量，利用学习的去噪器和求解一维积分计算IEM。

Result: 对于高斯分布信号，IEM与马氏距离一致；在ImageNet数据库上学习的IEM在预测人类感知判断方面与最先进的监督图像质量度量有竞争力或更优。

Conclusion: IEM是一种有效的距离度量，在图像质量评估等领域有应用价值。

Abstract: We introduce the Information-Estimation Metric (IEM), a novel form of
distance function derived from an underlying continuous probability density
over a domain of signals. The IEM is rooted in a fundamental relationship
between information theory and estimation theory, which links the
log-probability of a signal with the errors of an optimal denoiser, applied to
noisy observations of the signal. In particular, the IEM between a pair of
signals is obtained by comparing their denoising error vectors over a range of
noise amplitudes. Geometrically, this amounts to comparing the score vector
fields of the blurred density around the signals over a range of blur levels.
We prove that the IEM is a valid global metric and derive a closed-form
expression for its local second-order approximation, which yields a Riemannian
metric. For Gaussian-distributed signals, the IEM coincides with the
Mahalanobis distance. But for more complex distributions, it adapts, both
locally and globally, to the geometry of the distribution. In practice, the IEM
can be computed using a learned denoiser (analogous to generative diffusion
models) and solving a one-dimensional integral. To demonstrate the value of our
framework, we learn an IEM on the ImageNet database. Experiments show that this
IEM is competitive with or outperforms state-of-the-art supervised image
quality metrics in predicting human perceptual judgments.

</details>


### [268] [Wave-GMS: Lightweight Multi-Scale Generative Model for Medical Image Segmentation](https://arxiv.org/abs/2510.03216)
*Talha Ahmed,Nehal Ahmed Shaikh,Hassan Mohy-ud-Din*

Main category: eess.IV

TL;DR: 提出轻量级高效医学图像分割模型Wave - GMS，在多数据集实验中表现优异且参数少，代码开源。


<details>
  <summary>Details</summary>
Motivation: 为实现AI工具在医院和医疗设施中的公平部署，需要能在低成本、内存有限的GPU上以大批次训练的高性能深度分割网络。

Method: 提出Wave - GMS，该模型可训练参数少，无需加载内存密集型预训练视觉基础模型，支持在内存有限的GPU上大批次训练。

Result: 在四个公开数据集上实验表明，Wave - GMS实现了最先进的分割性能，具有出色的跨领域泛化能力，仅需约260万个可训练参数。

Conclusion: Wave - GMS是一个轻量级、高效且性能优越的医学图像分割模型。

Abstract: For equitable deployment of AI tools in hospitals and healthcare facilities,
we need Deep Segmentation Networks that offer high performance and can be
trained on cost-effective GPUs with limited memory and large batch sizes. In
this work, we propose Wave-GMS, a lightweight and efficient multi-scale
generative model for medical image segmentation. Wave-GMS has a substantially
smaller number of trainable parameters, does not require loading
memory-intensive pretrained vision foundation models, and supports training
with large batch sizes on GPUs with limited memory. We conducted extensive
experiments on four publicly available datasets (BUS, BUSI, Kvasir-Instrument,
and HAM10000), demonstrating that Wave-GMS achieves state-of-the-art
segmentation performance with superior cross-domain generalizability, while
requiring only ~2.6M trainable parameters. Code is available at
https://github.com/ATPLab-LUMS/Wave-GMS.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [269] [NetCAS: Dynamic Cache and Backend Device Management in Networked Environments](https://arxiv.org/abs/2510.02323)
*Joon Yong Hwang,Chanseo Park,Ikjun Yeom,Younghoon Kim*

Main category: cs.OS

TL;DR: 提出NetCAS框架，基于实时网络反馈和预计算的性能配置文件动态拆分I/O，比传统缓存和其他方案性能更好。


<details>
  <summary>Details</summary>
Motivation: 现代存储系统结合缓存和后端设备加速I/O，但数据中心远程后端存储存在网络争用问题，传统基于命中率的策略难以应对。

Method: NetCAS框架基于实时网络反馈和预计算的Perf Profile动态拆分I/O，采用低开销的批量轮询调度器。

Result: NetCAS在远程存储环境中比传统缓存性能提高达174%，在网络条件波动时比Orthus等方案性能高3.5倍。

Conclusion: NetCAS能根据工作负载配置和网络性能调整拆分比例，有效提升存储系统性能。

Abstract: Modern storage systems often combine fast cache with slower backend devices
to accelerate I/O. As performance gaps narrow, concurrently accessing both
devices, rather than relying solely on cache hits, can improve throughput.
However, in data centers, remote backend storage accessed over networks suffers
from unpredictable contention, complicating this split. We present NetCAS, a
framework that dynamically splits I/O between cache and backend devices based
on real-time network feedback and a precomputed Perf Profile. Unlike
traditional hit-rate-based policies, NetCAS adapts split ratios to workload
configuration and networking performance. NetCAS employs a low-overhead batched
round-robin scheduler to enforce splits, avoiding per-request costs. It
achieves up to 174% higher performance than traditional caching in remote
storage environments and outperforms converging schemes like Orthus by up to
3.5X under fluctuating network conditions.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [270] [Automatic Generation of Digital Twins for Network Testing](https://arxiv.org/abs/2510.03205)
*Shenjia Ding,David Flynn,Paul Harvey*

Main category: cs.NI

TL;DR: 本文探讨自动生成数字孪生体以提供高效准确的验证工具，实验证明该方法可行。


<details>
  <summary>Details</summary>
Motivation: 电信网络软件部署前需大量测试验证，现有数字孪生体配置和执行耗时长、人力成本高。

Method: 探索自动生成数字孪生体的方法，与ITU - T自主网络架构的实验子系统对齐。

Result: 针对初始用例的实验结果表明，该方法能自动创建高效且精度足够的数字孪生体。

Conclusion: 该方法可行，可纳入现有验证流程。

Abstract: The increased use of software in the operation and management of
telecommunication networks has moved the industry one step closer to realizing
autonomous network operation. One consequence of this shift is the
significantly increased need for testing and validation before such software
can be deployed. Complementing existing simulation or hardware-based
approaches, digital twins present an environment to achieve this testing;
however, they require significant time and human effort to configure and
execute. This paper explores the automatic generation of digital twins to
provide efficient and accurate validation tools, aligned to the ITU-T
autonomous network architecture's experimentation subsystem. We present
experimental results for an initial use case, demonstrating that the approach
is feasible in automatically creating efficient digital twins with sufficient
accuracy to be included as part of existing validation pipelines.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [271] [Orthogonal Procrustes problem preserves correlations in synthetic data](https://arxiv.org/abs/2510.02405)
*Oussama Ounissi,Nicklas Jävergård,Adrian Muntean*

Main category: stat.ME

TL;DR: 本文介绍将正交普罗克拉斯提斯问题应用于合成数据生成，该方法能保留特征间皮尔逊相关性，通过能源消耗数据集验证有效性，可作为轻量级后处理步骤。


<details>
  <summary>Details</summary>
Motivation: 找到一种能在合成数据生成中保留特征间重要统计关系（皮尔逊相关性）的方法。

Method: 将正交普罗克拉斯提斯问题应用于合成数据生成，作为已生成合成数据集的轻量级后处理步骤。

Result: 使用大型真实能源消耗表格数据集进行实证说明，证明了该方法的有效性。

Conclusion: 该方法有实际合成数据生成的应用潜力，不旨在取代现有生成模型，而是作为轻量级后处理步骤。

Abstract: This work introduces the application of the Orthogonal Procrustes problem to
the generation of synthetic data. The proposed methodology ensures that the
resulting synthetic data preserves important statistical relationships among
features, specifically the Pearson correlation. An empirical illustration using
a large, real-world, tabular dataset of energy consumption demonstrates the
effectiveness of the approach and highlights its potential for application in
practical synthetic data generation. Our approach is not meant to replace
existing generative models, but rather as a lightweight post-processing step
that enforces exact Pearson correlation to an already generated synthetic
dataset.

</details>


### [272] [What is in the model? A Comparison of variable selection criteria and model search approaches](https://arxiv.org/abs/2510.02628)
*Shuangshuang Xu,Marco A. R. Ferreira,Allison N. Tegge*

Main category: stat.ME

TL;DR: 本文对变量选择方法进行综合比较，模拟研究表明穷举搜索BIC和随机搜索BIC分别在小和大模型空间表现最佳。


<details>
  <summary>Details</summary>
Motivation: 为帮助研究者更好理解潜在机制，变量选择至关重要，需对变量选择方法进行综合比较。

Method: 使用正确识别率、召回率和错误发现率等性能指标，考虑BIC和AIC评估模型，采用穷举、贪心、LASSO路径和随机搜索等方法搜索模型空间，进行线性和广义线性模型的模拟研究。

Result: 穷举搜索BIC和随机搜索BIC分别在小和大模型空间表现最佳，具有最高的正确识别率和最低的错误发现率。

Conclusion: 这些方法有助于提高研究的可重复性。

Abstract: For many scientific questions, understanding the underlying mechanism is the
goal. To help investigators better understand the underlying mechanism,
variable selection is a crucial step that permits the identification of the
most associated regression variables of interest. A variable selection method
consists of model evaluation using an information criterion and a search of the
model space. Here, we provide a comprehensive comparison of variable selection
methods using performance measures of correct identification rate (CIR),
recall, and false discovery rate (FDR). We consider the BIC and AIC for
evaluating models, and exhaustive, greedy, LASSO path, and stochastic search
approaches for searching the model space; we also consider LASSO using cross
validation. We perform simulation studies for linear and generalized linear
models that parametrically explore a wide range of realistic sample sizes,
effect sizes, and correlations among regression variables. We consider model
spaces with a small and larger number of potential regressors. The results show
that the exhaustive search BIC and stochastic search BIC outperform the other
methods when considering the performance measures on small and large model
spaces, respectively. These approaches result in the highest CIR and lowest
FDR, which collectively may support long-term efforts towards increasing
replicability in research.

</details>


### [273] [Total Robustness in Bayesian Nonlinear Regression for Measurement Error Problems under Model Misspecification](https://arxiv.org/abs/2510.03131)
*Mengqi Chen,Charita Dellaporta,Thomas B. Berrett,Theodoros Damoulas*

Main category: stat.ME

TL;DR: 提出首个针对一般非线性回归中三个挑战的贝叶斯非参数框架，有高效算法，模拟和实证研究表现优，为数据和模型有缺陷时的回归提供实用范式。


<details>
  <summary>Details</summary>
Motivation: 现代回归分析常受协变量测量误差、回归模型误设和测量误差分布误设影响，需解决这些问题。

Method: 为潜在协变量 - 响应分布分配狄利克雷过程先验，用潜在协变量的后验伪样本更新，建立泛化界，采用基于梯度的算法。

Result: 模拟和两个实际研究显示，与贝叶斯和频率学派竞争对手相比，该框架估计误差更低，对误设的估计敏感性降低。

Conclusion: 该框架为数据和模型都有缺陷时的可靠回归提供了实用且可解释的范式。

Abstract: Modern regression analyses are often undermined by covariate measurement
error, misspecification of the regression model, and misspecification of the
measurement error distribution. We present, to the best of our knowledge, the
first Bayesian nonparametric framework targeting total robustness that tackles
all three challenges in general nonlinear regression. The framework assigns a
Dirichlet process prior to the latent covariate-response distribution and
updates it with posterior pseudo-samples of the latent covariates, thereby
providing the Dirichlet process posterior with observation-informed latent
inputs and yielding estimators that minimise the discrepancy between Dirichlet
process realisations and the model-induced joint law. This design allows
practitioners to (i) encode prior beliefs, (ii) choose between pseudo-sampling
latent covariates or working directly with error-prone observations, and (iii)
tune the influence of prior and data. We establish generalisation bounds that
tighten whenever the prior or pseudo-sample generator aligns with the
underlying data generating process, ensuring robustness without sacrificing
consistency. A gradient-based algorithm enables efficient computations;
simulations and two real-world studies show lower estimation error and reduced
estimation sensitivity to misspecification compared to Bayesian and frequentist
competitors. The framework, therefore, offers a practical and interpretable
paradigm for trustworthy regression when data and models are jointly imperfect.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [274] [Gradient-enhanced global sensitivity analysis with Poincar{é} chaos expansions](https://arxiv.org/abs/2510.03056)
*O Roustant,N Lüthen,D Heredia,B Sudret*

Main category: math.ST

TL;DR: 本文提出基于Poincaré基的梯度增强全局敏感性分析框架，在洪水建模案例中用有限数据准确估计Sobol'指标。


<details>
  <summary>Details</summary>
Motivation: 在导数可用时，期望基函数的导数也构成正交基，以用于全局敏感性分析。

Method: 提出基于Poincaré基的梯度增强全局敏感性分析框架，结合稀疏梯度增强回归和导数敏感性分析的权重方案。

Result: 在洪水建模案例中，用有限数据准确估计了Sobol'指标。

Conclusion: 所提方法适用于多种概率测度和权重选择，能有效利用有限数据进行全局敏感性分析。

Abstract: Chaos expansions are widely used in global sensitivity analysis (GSA), as
they leverage orthogonal bases of L2 spaces to efficiently compute Sobol'
indices, particularly in data-scarce settings. When derivatives are available,
we argue that a desirable property is for the derivatives of the basis
functions to also form an orthogonal basis. We demonstrate that the only basis
satisfying this property is the one associated with weighted Poincar{\'e}
inequalities and Sturm-Liouville eigenvalue problems, which we refer to as the
Poincar{\'e} basis. We then introduce a comprehensive framework for
gradient-enhanced GSA that integrates recent advances in sparse,
gradient-enhanced regression for surrogate modeling with the construction of
weighting schemes for derivative-based sensitivity analysis. The proposed
methodology is applicable to a broad class of probability measures and supports
various choices of weights. We illustrate the effectiveness of the approach on
a challenging flood modeling case study, where Sobol' indices are accurately
estimated using limited data.

</details>


### [275] [Rates of Convergence of Generalised Variational Inference Posteriors under Prior Misspecification](https://arxiv.org/abs/2510.03109)
*Terje Mildner,Paris Giampouras,Theodoros Damoulas*

Main category: math.ST

TL;DR: 本文在有界散度的广义变分推理（GVI）框架内证明了收敛速率和对先验错误指定的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决GVI和联邦GVI在使用与Kullback - Leibler不同散度、在部分概率测度子集内操作及产生难处理后验时面临的先验错误指定问题。

Method: 依靠限制可能的GVI后验测度空间并据此推断性质。

Result: 建立了任意波兰空间上GVI后验存在和唯一的充分条件，证明GVI后验测度集中在损失最小化子的邻域，并得出与先验测度无关的收敛速率。

Conclusion: 在有界散度的GVI框架内实现了收敛速率证明和对先验错误指定的鲁棒性分析。

Abstract: We prove rates of convergence and robustness to prior misspecification within
a Generalised Variational Inference (GVI) framework with bounded divergences.
This addresses a significant open challenge for GVI and Federated GVI that
employ a different divergence to the Kullback--Leibler under prior
misspecification, operate within a subset of possible probability measures, and
result in intractable posteriors. Our theoretical contributions cover severe
prior misspecification while relying on our ability to restrict the space of
possible GVI posterior measures, and infer properties based on this space. In
particular, we are able to establish sufficient conditions for existence and
uniqueness of GVI posteriors on arbitrary Polish spaces, prove that the GVI
posterior measure concentrates on a neighbourhood of loss minimisers, and
extend this to rates of convergence regardless of the prior measure.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [276] [Simple Quantum Algorithm for Approximate $k$-Mismatch Problem](https://arxiv.org/abs/2510.02399)
*Ruhan Habib*

Main category: quant-ph

TL;DR: 提出近似解决k - 不匹配问题的量子算法，复杂度为\(\tilde{O}\left( \epsilon^{-1} \sqrt{\frac{mn}{k}} \right)\)。


<details>
  <summary>Details</summary>
Motivation: 前人对经典和量子计算环境下的k - 不匹配问题进行研究，本文欲提供新的解决方法。

Method: 给出参数\(\epsilon \in (0, 1]\)，以近似方式解决问题，仅当为\(\left(1+\epsilon\right)k\) - 不匹配时返回匹配。

Result: 得到一个时间（规模）复杂度为\(\tilde{O}\left( \epsilon^{-1} \sqrt{\frac{mn}{k}} \right)\)的量子算法。

Conclusion: 成功设计出满足特定条件且具有一定复杂度的近似量子算法来解决k - 不匹配问题。

Abstract: In the $k$-mismatch problem, given a pattern and a text of length $n$ and $m$
respectively, we have to find if the text has a sub-string with a Hamming
distance of at most $k$ from the pattern. This has been studied in the
classical setting since 1982 and recently in the quantum computational setting
by Jin and Nogler and Kociumaka, Nogler, and Wellnitz. We provide a simple
quantum algorithm that solves the problem in an approximate manner, given a
parameter $\epsilon \in (0, 1]$. It returns an occurrence as a match only if it
is a $\left(1+\epsilon\right)k$-mismatch. If it does not return any occurrence,
then there is no $k$-mismatch. This algorithm has a time (size) complexity of
$\tilde{O}\left( \epsilon^{-1} \sqrt{\frac{mn}{k}} \right)$.

</details>


### [277] [Scalable Quantum Optimisation using HADOF: Hamiltonian Auto-Decomposition Optimisation Framework](https://arxiv.org/abs/2510.02926)
*Namasi G Sankar,Georgios Miliotis,Simon Caton*

Main category: quant-ph

TL;DR: 本文提出HADOF框架，可将QUBO哈密顿量自动分解以解决NISQ设备量子比特数有限问题，与SA和CPLEX比较有优势，还在IBM量子计算机上实现了玩具问题。


<details>
  <summary>Details</summary>
Motivation: 当前NISQ设备量子比特数有限，限制了量子优化算法的实际应用。

Method: 提出Hamiltonian Auto-Decomposition Optimisation Framework (HADOF)，通过迭代策略将QUBO哈密顿量分解为子哈密顿量，分别优化后聚合为全局解。

Result: HADOF可扩展到远超可用量子比特的问题规模，同时保持有竞争力的准确性和运行时间，且在IBM量子计算机上实现了玩具问题。

Conclusion: HADOF有量子优化实际应用的潜力。

Abstract: Quantum Annealing (QA) and QAOA are promising quantum optimisation algorithms
used for finding approximate solutions to combinatorial problems on near-term
NISQ systems. Many NP-hard problems can be reformulated as Quadratic
Unconstrained Binary Optimisation (QUBO), which maps naturally onto quantum
Hamiltonians. However, the limited qubit counts of current NISQ devices
restrict practical deployment of such algorithms. In this study, we present the
Hamiltonian Auto-Decomposition Optimisation Framework (HADOF), which leverages
an iterative strategy to automatically divide the Quadratic Unconstrained
Binary Optimisation (QUBO) Hamiltonian into sub-Hamiltonians which can be
optimised separately using Hamiltonian based optimisers such as QAOA, QA or
Simulated Annealing (SA) and aggregated into a global solution. We compare
HADOF with Simulated Annealing (SA) and the CPLEX exact solver, showing
scalability to problem sizes far exceeding available qubits while maintaining
competitive accuracy and runtime. Furthermore, we realise HADOF for a toy
problem on an IBM quantum computer, showing promise for practical applications
of quantum optimisation.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [278] [ReeMark: Reeb Graphs for Simulating Patterns of Life in Spatiotemporal Trajectories](https://arxiv.org/abs/2510.03152)
*Anantajit Subrahmanya,Chandrakanth Gudavalli,Connor Levenson,Umang Garg,B. S. Manjunath*

Main category: cs.CV

TL;DR: 本文提出马尔可夫Reeb图框架模拟时空轨迹，评估显示其有效且高效，有广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 准确建模人类移动性对城市规划、流行病学和交通管理至关重要。

Method: 结合个体和群体层面的移动结构，在概率拓扑模型中引入马尔可夫Reeb图框架模拟时空轨迹。

Result: 在Urban Anomalies数据集上评估，方法保真度高，数据和计算高效。

Conclusion: 马尔可夫Reeb图是可扩展的轨迹模拟框架，适用于不同城市环境。

Abstract: Accurately modeling human mobility is critical for urban planning,
epidemiology, and traffic management. In this work, we introduce Markovian Reeb
Graphs, a novel framework for simulating spatiotemporal trajectories that
preserve Patterns of Life (PoLs) learned from baseline data. By combining
individual- and population-level mobility structures within a probabilistic
topological model, our approach generates realistic future trajectories that
capture both consistency and variability in daily life. Evaluations on the
Urban Anomalies dataset (Atlanta and Berlin subsets) using the Jensen-Shannon
Divergence (JSD) across population- and agent-level metrics demonstrate that
the proposed method achieves strong fidelity while remaining data- and
compute-efficient. These results position Markovian Reeb Graphs as a scalable
framework for trajectory simulation with broad applicability across diverse
urban environments.

</details>


### [279] [Representation Learning for Compressed Video Action Recognition via Attentive Cross-modal Interaction with Motion Enhancement](https://arxiv.org/abs/2205.03569)
*Bing Li,Jiaxin Chen,Dongming Zhang,Xiuguo Bao,Di Huang*

Main category: cs.CV

TL;DR: 提出MEACI - Net框架用于压缩视频动作识别，在多个基准测试中验证其有效性和效率。


<details>
  <summary>Details</summary>
Motivation: 压缩视频动作识别存在动态信息粗糙嘈杂、RGB和运动模态融合不足的问题，需解决以提升性能。

Method: 采用双流架构，运动流用带去噪模块的多尺度块增强表征学习，引入SMC和CMA模块加强两流交互。

Result: 在UCF - 101、HMDB - 51和Kinetics - 400基准测试上进行大量实验。

Conclusion: MEACI - Net在压缩视频动作识别中有效且高效。

Abstract: Compressed video action recognition has recently drawn growing attention,
since it remarkably reduces the storage and computational cost via replacing
raw videos by sparsely sampled RGB frames and compressed motion cues (e.g.,
motion vectors and residuals). However, this task severely suffers from the
coarse and noisy dynamics and the insufficient fusion of the heterogeneous RGB
and motion modalities. To address the two issues above, this paper proposes a
novel framework, namely Attentive Cross-modal Interaction Network with Motion
Enhancement (MEACI-Net). It follows the two-stream architecture, i.e. one for
the RGB modality and the other for the motion modality. Particularly, the
motion stream employs a multi-scale block embedded with a denoising module to
enhance representation learning. The interaction between the two streams is
then strengthened by introducing the Selective Motion Complement (SMC) and
Cross-Modality Augment (CMA) modules, where SMC complements the RGB modality
with spatio-temporally attentive local motion features and CMA further combines
the two modalities with selective feature augmentation. Extensive experiments
on the UCF-101, HMDB-51 and Kinetics-400 benchmarks demonstrate the
effectiveness and efficiency of MEACI-Net.

</details>


### [280] [Oracle-RLAIF: An Improved Fine-Tuning Framework for Multi-modal Video Models through Reinforcement Learning from Ranking Feedback](https://arxiv.org/abs/2510.02561)
*Derek Shi,Ruben Glatt,Christine Klymko,Shubham Mohole,Hongjun Choi,Shashank Kushwaha,Sam Sakla,Felipe Leno da Silva*

Main category: cs.CV

TL;DR: 提出Oracle - RLAIF框架和$GRPO_{rank}$损失函数，在视频理解基准测试中表现优于现有方法，为多模态视频模型微调提供新思路。


<details>
  <summary>Details</summary>
Motivation: 现有大视频语言模型微调成本高，尤其是收集人类反馈和训练奖励模型成本高，需更具成本效益的微调方法。

Method: 提出Oracle - RLAIF框架，用通用Oracle排序器替代训练的奖励模型；引入基于GRPO的$GRPO_{rank}$损失函数优化顺序反馈。

Result: 在各种视频理解基准测试中，Oracle - RLAIF始终优于使用现有微调方法的领先大视频语言模型。

Conclusion: Oracle - RLAIF为通过排序而非评分的强化学习来对齐大型多模态视频模型开辟了创建灵活且数据高效框架的道路。

Abstract: Recent advances in large video-language models (VLMs) rely on extensive
fine-tuning techniques that strengthen alignment between textual and visual
comprehension. Leading pipelines typically pair supervised fine-tuning (SFT)
with reinforcement learning from preference data to enhance video
comprehension. However, as VLMs scale in parameter size, so does the cost of
gathering enough human feedback. To make fine-tuning more cost-effective,
recent frameworks explore reinforcement learning with AI feedback (RLAIF),
which replace human preference with AI as a judge. Current RLAIF frameworks
rely on a specialized reward model trained with video narratives to create
calibrated scalar rewards -- an expensive and restrictive pipeline. We propose
Oracle-RLAIF, a novel framework that replaces the trained reward model with a
more general Oracle ranker which acts as a drop-in model ranking candidate
model responses rather than scoring them. Alongside Oracle-RLAIF, we introduce
$GRPO_{rank}$, a novel rank-based loss function based on Group Relative Policy
Optimization (GRPO) that directly optimizes ordinal feedback with rank-aware
advantages. Empirically, we demonstrate that Oracle-RLAIF consistently
outperforms leading VLMs using existing fine-tuning methods when evaluated
across various video comprehension benchmarks. Oracle-RLAIF paves the path to
creating flexible and data-efficient frameworks for aligning large multi-modal
video models with reinforcement learning from rank rather than score.

</details>


### [281] [How Confident are Video Models? Empowering Video Models to Express their Uncertainty](https://arxiv.org/abs/2510.02571)
*Zhiting Mei,Ola Shorinwa,Anirudha Majumdar*

Main category: cs.CV

TL;DR: 本文首次提出生成式视频模型不确定性量化框架，含评估指标、黑盒UQ方法和UQ数据集，实验证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型会产生幻觉但缺乏不确定性量化方法，存在安全隐患，本文旨在量化视频模型不确定性。

Method: 提出不确定性量化框架，包括基于稳健秩相关估计的评估指标、黑盒UQ方法S - QUBED和UQ数据集，在潜空间分离不同来源的不确定性。

Result: S - QUBED计算的总不确定性估计与任务准确率负相关，能有效计算随机和认知成分。

Conclusion: 所提框架为视频模型不确定性量化提供有效方案。

Abstract: Generative video models demonstrate impressive text-to-video capabilities,
spurring widespread adoption in many real-world applications. However, like
large language models (LLMs), video generation models tend to hallucinate,
producing plausible videos even when they are factually wrong. Although
uncertainty quantification (UQ) of LLMs has been extensively studied in prior
work, no UQ method for video models exists, raising critical safety concerns.
To our knowledge, this paper represents the first work towards quantifying the
uncertainty of video models. We present a framework for uncertainty
quantification of generative video models, consisting of: (i) a metric for
evaluating the calibration of video models based on robust rank correlation
estimation with no stringent modeling assumptions; (ii) a black-box UQ method
for video models (termed S-QUBED), which leverages latent modeling to
rigorously decompose predictive uncertainty into its aleatoric and epistemic
components; and (iii) a UQ dataset to facilitate benchmarking calibration in
video models. By conditioning the generation task in the latent space, we
disentangle uncertainty arising due to vague task specifications from that
arising from lack of knowledge. Through extensive experiments on benchmark
video datasets, we demonstrate that S-QUBED computes calibrated total
uncertainty estimates that are negatively correlated with the task accuracy and
effectively computes the aleatoric and epistemic constituents.

</details>


### [282] [Hierarchical Generalized Category Discovery for Brain Tumor Classification in Digital Pathology](https://arxiv.org/abs/2510.02760)
*Matthias Perkonigg,Patrick Rockenschaub,Georg Göbel,Adelheid Wöhrer*

Main category: cs.CV

TL;DR: 提出用于脑肿瘤分类的HGCD - BT方法，在OpenSRH数据集上准确率比现有GCD方法提升28%，且在不同成像方式上有通用性。


<details>
  <summary>Details</summary>
Motivation: 现有脑肿瘤分类方法受限，无法捕捉训练时未出现的肿瘤类型模式，GCD可分类未知类但未反映脑肿瘤分类的层次结构。

Method: 引入HGCD - BT方法，将层次聚类与对比学习结合，扩展基于对比学习的GCD，加入半监督层次聚类损失。

Result: 在OpenSRH数据集上，HGCD - BT在斑块级分类准确率比现有GCD方法提升28%，在数字脑肿瘤图谱的苏木精和伊红染色全切片图像的幻灯片级分类中也证明了通用性。

Conclusion: HGCD - BT方法有效，能用于脑肿瘤分类，且在不同成像方式下都有良好表现。

Abstract: Accurate brain tumor classification is critical for intra-operative decision
making in neuro-oncological surgery. However, existing approaches are
restricted to a fixed set of predefined classes and are therefore unable to
capture patterns of tumor types not available during training. Unsupervised
learning can extract general-purpose features, but it lacks the ability to
incorporate prior knowledge from labelled data, and semi-supervised methods
often assume that all potential classes are represented in the labelled data.
Generalized Category Discovery (GCD) aims to bridge this gap by categorizing
both known and unknown classes within unlabelled data. To reflect the
hierarchical structure of brain tumor taxonomies, in this work, we introduce
Hierarchical Generalized Category Discovery for Brain Tumor Classification
(HGCD-BT), a novel approach that integrates hierarchical clustering with
contrastive learning. Our method extends contrastive learning based GCD by
incorporating a novel semi-supervised hierarchical clustering loss. We evaluate
HGCD-BT on OpenSRH, a dataset of stimulated Raman histology brain tumor images,
achieving a +28% improvement in accuracy over state-of-the-art GCD methods for
patch-level classification, particularly in identifying previously unseen tumor
categories. Furthermore, we demonstrate the generalizability of HGCD-BT on
slide-level classification of hematoxylin and eosin stained whole-slide images
from the Digital Brain Tumor Atlas, confirming its utility across imaging
modalities.

</details>


### [283] [Align Your Query: Representation Alignment for Multimodality Medical Object Detection](https://arxiv.org/abs/2510.02789)
*Ara Seo,Bryan Sangwoo Kim,Hyungjin Chung,Jong Chul Ye*

Main category: cs.CV

TL;DR: 针对单一检测器在混合医疗模态上训练效果不佳的问题，提出通过表示对齐的方法，利用模态令牌、MoCA和QueryREPA生成模态感知、类别忠实的查询，提升多模态医疗目标检测的AP。


<details>
  <summary>Details</summary>
Motivation: 单一检测器在混合医疗模态上训练时，因异质统计和不相交表示空间导致医疗目标检测效果不佳。

Method: 定义模态令牌，通过多模态上下文注意力（MoCA）将其集成到检测过程，引入QueryREPA进行预训练，使用特定任务的对比目标和模态平衡批次对齐查询表示和模态令牌。

Result: 在多种模态一起训练时，该方法持续提高了平均精度（AP），开销极小且无需架构修改。

Conclusion: 该方法为鲁棒的多模态医疗目标检测提供了实用途径。

Abstract: Medical object detection suffers when a single detector is trained on mixed
medical modalities (e.g., CXR, CT, MRI) due to heterogeneous statistics and
disjoint representation spaces. To address this challenge, we turn to
representation alignment, an approach that has proven effective for bringing
features from different sources into a shared space. Specifically, we target
the representations of DETR-style object queries and propose a simple,
detector-agnostic framework to align them with modality context. First, we
define modality tokens: compact, text-derived embeddings encoding imaging
modality that are lightweight and require no extra annotations. We integrate
the modality tokens into the detection process via Multimodality Context
Attention (MoCA), mixing object-query representations via self-attention to
propagate modality context within the query set. This preserves DETR-style
architectures and adds negligible latency while injecting modality cues into
object queries. We further introduce QueryREPA, a short pretraining stage that
aligns query representations to their modality tokens using a task-specific
contrastive objective with modality-balanced batches. Together, MoCA and
QueryREPA produce modality-aware, class-faithful queries that transfer
effectively to downstream training. Across diverse modalities trained
altogether, the proposed approach consistently improves AP with minimal
overhead and no architectural modifications, offering a practical path toward
robust multimodality medical object detection. Project page:
https://araseo.github.io/alignyourquery/.

</details>


### [284] [MaskCD: Mitigating LVLM Hallucinations by Image Head Masked Contrastive Decoding](https://arxiv.org/abs/2510.02790)
*Jingyuan Deng,Yujiu Yang*

Main category: cs.CV

TL;DR: 提出图像头部掩码对比解码（MaskCD）方法缓解大视觉语言模型幻觉问题，在多个模型和基准测试中验证有效。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型在视觉语言理解中存在幻觉问题，现有解决方法有缺陷。

Method: 利用模型中的“图像头”，通过掩码构建对比样本进行对比解码。

Result: 在LLaVA - 1.5 - 7b和Qwen - VL - 7b上测试，MaskCD有效缓解幻觉现象并保留模型通用能力。

Conclusion: MaskCD是一种能有效解决大视觉语言模型幻觉问题的方法。

Abstract: Large vision-language models (LVLMs) have shown remarkable performance in
visual-language understanding for downstream multimodal tasks. While their
capabilities are improving, problems emerge simultaneously. Among those
problems, the hallucinations have attracted much attention, which stands for
the phenomenon where LVLMs generate contradictory content to their input visual
and text contents. Many approaches have been proposed to deal with this issue,
such as contrastive decoding and attention manipulation. However, contrastive
decoding methods struggle in constructing appropriate contrastive samples, and
attention manipulation methods are highly sensitive, lacking stability. In this
work, we propose image head Masked Contrastive Decoding (MaskCD). Our approach
utilizes the "image heads" in LVLMs, masking them to construct contrastive
samples for contrastive decoding. We evaluated MaskCD on LLaVA-1.5-7b and
Qwen-VL-7b, using various benchmarks such as CHAIR, POPE, AMBER and MME. The
results demonstrate that MaskCD effectively alleviates the phenomenon of
hallucinations and retains the general capabilities of LVLMs. Corresponding
resources could be found at: https://github.com/Deng-Jingyuan/MaskCD .

</details>


### [285] [Multimodal Carotid Risk Stratification with Large Vision-Language Models: Benchmarking, Fine-Tuning, and Clinical Insights](https://arxiv.org/abs/2510.02922)
*Daphne Tsolissou,Theofanis Ganitidis,Konstantinos Mitsis,Stergios CHristodoulidis,Maria Vakalopoulou,Konstantina Nikita*

Main category: cs.CV

TL;DR: 研究探索LVLMs用于颈动脉斑块多模态评估，发现其有局限，经适配和多模态数据整合提升性能，强调临床应用需多方面考量。


<details>
  <summary>Details</summary>
Motivation: 颈动脉粥样硬化疾病可靠风险评估是临床挑战，需整合多样信息，探索LVLMs用于多模态颈动脉斑块评估的潜力。

Method: 提出通过访谈式问题序列模拟诊断场景的框架，比较多种开源LVLMs，用低秩适配（LoRA）将LLaVa - NeXT - Vicuna适配到超声领域，以文本形式整合多模态表格数据。

Result: 并非所有LVLMs能准确识别成像方式和解剖结构，在风险分类上表现差；适配后在中风风险分层有改善，整合多模态数据提升特异性和平衡准确率，性能与CNN基线相当。

Conclusion: LVLMs在超声心血管风险预测有前景和局限，临床转化需重视多模态整合、模型校准和领域适配。

Abstract: Reliable risk assessment for carotid atheromatous disease remains a major
clinical challenge, as it requires integrating diverse clinical and imaging
information in a manner that is transparent and interpretable to clinicians.
This study investigates the potential of state-of-the-art and recent large
vision-language models (LVLMs) for multimodal carotid plaque assessment by
integrating ultrasound imaging (USI) with structured clinical, demographic,
laboratory, and protein biomarker data. A framework that simulates realistic
diagnostic scenarios through interview-style question sequences is proposed,
comparing a range of open-source LVLMs, including both general-purpose and
medically tuned models. Zero-shot experiments reveal that even if they are very
powerful, not all LVLMs can accurately identify imaging modality and anatomy,
while all of them perform poorly in accurate risk classification. To address
this limitation, LLaVa-NeXT-Vicuna is adapted to the ultrasound domain using
low-rank adaptation (LoRA), resulting in substantial improvements in stroke
risk stratification. The integration of multimodal tabular data in the form of
text further enhances specificity and balanced accuracy, yielding competitive
performance compared to prior convolutional neural network (CNN) baselines
trained on the same dataset. Our findings highlight both the promise and
limitations of LVLMs in ultrasound-based cardiovascular risk prediction,
underscoring the importance of multimodal integration, model calibration, and
domain adaptation for clinical translation.

</details>


### [286] [ELMF4EggQ: Ensemble Learning with Multimodal Feature Fusion for Non-Destructive Egg Quality Assessment](https://arxiv.org/abs/2510.02876)
*Md Zahim Hassan,Md. Osama,Muhammad Ashad Kabir,Md. Saiful Islam,Zannatul Naim*

Main category: cs.CV

TL;DR: 本文提出ELMF4EggQ框架，用外部属性对鸡蛋分级和新鲜度分类，实验表明多模态方法效果好，代码和数据公开。


<details>
  <summary>Details</summary>
Motivation: 准确无损评估鸡蛋质量对商业家禽生产很重要，此前缺乏用外部特征评估内部质量的研究。

Method: 构建公开数据集，用预训练CNN模型提取图像特征，结合形状和重量特征，经PCA降维、SMOTE增强，用多机器学习算法分类，用集成投票机制提高准确率。

Result: 多模态方法显著优于仅图像和仅表格基线，分级准确率86.57%，新鲜度预测准确率70.83%。

Conclusion: 多模态特征融合的集成学习框架能有效评估鸡蛋质量，代码和数据公开利于该领域进一步研究。

Abstract: Accurate, non-destructive assessment of egg quality is critical for ensuring
food safety, maintaining product standards, and operational efficiency in
commercial poultry production. This paper introduces ELMF4EggQ, an ensemble
learning framework that employs multimodal feature fusion to classify egg grade
and freshness using only external attributes - image, shape, and weight. A
novel, publicly available dataset of 186 brown-shelled eggs was constructed,
with egg grade and freshness levels determined through laboratory-based expert
assessments involving internal quality measurements, such as yolk index and
Haugh unit. To the best of our knowledge, this is the first study to apply
machine learning methods for internal egg quality assessment using only
external, non-invasive features, and the first to release a corresponding
labeled dataset. The proposed framework integrates deep features extracted from
external egg images with structural characteristics such as egg shape and
weight, enabling a comprehensive representation of each egg. Image feature
extraction is performed using top-performing pre-trained CNN models (ResNet152,
DenseNet169, and ResNet152V2), followed by PCA-based dimensionality reduction,
SMOTE augmentation, and classification using multiple machine learning
algorithms. An ensemble voting mechanism combines predictions from the
best-performing classifiers to enhance overall accuracy. Experimental results
demonstrate that the multimodal approach significantly outperforms image-only
and tabular (shape and weight) only baselines, with the multimodal ensemble
approach achieving 86.57% accuracy in grade classification and 70.83% in
freshness prediction. All code and data are publicly available at
https://github.com/Kenshin-Keeps/Egg_Quality_Prediction_ELMF4EggQ, promoting
transparency, reproducibility, and further research in this domain.

</details>


### [287] [When and Where do Events Switch in Multi-Event Video Generation?](https://arxiv.org/abs/2510.03049)
*Ruotong Liao,Guowen Huang,Qing Cheng,Thomas Seidl,Daniel Cremers,Volker Tresp*

Main category: cs.CV

TL;DR: 论文引入MEve评估多事件文本到视频生成，研究OpenSora和CogVideoX，揭示多事件视频生成关键因素。


<details>
  <summary>Details</summary>
Motivation: 现有多事件文本到视频生成方法未检查事件转换内在因素，论文旨在探究多事件提示控制事件转换的时间和位置。

Method: 引入MEve提示套件，对OpenSora和CogVideoX进行系统研究。

Result: 实验表明在去噪步骤和分块模型层早期干预很重要。

Conclusion: 揭示多事件视频生成关键因素，为未来模型多事件调节提供可能。

Abstract: Text-to-video (T2V) generation has surged in response to challenging
questions, especially when a long video must depict multiple sequential events
with temporal coherence and controllable content. Existing methods that extend
to multi-event generation omit an inspection of the intrinsic factor in event
shifting. The paper aims to answer the central question: When and where
multi-event prompts control event transition during T2V generation. This work
introduces MEve, a self-curated prompt suite for evaluating multi-event
text-to-video (T2V) generation, and conducts a systematic study of two
representative model families, i.e., OpenSora and CogVideoX. Extensive
experiments demonstrate the importance of early intervention in denoising steps
and block-wise model layers, revealing the essential factor for multi-event
video generation and highlighting the possibilities for multi-event
conditioning in future models.

</details>


### [288] [What Drives Compositional Generalization in Visual Generative Models?](https://arxiv.org/abs/2510.03075)
*Karim Farid,Rajat Sahay,Yumna Ali Alnaggar,Simon Schrodi,Volker Fischer,Cordelia Schmid,Thomas Brox*

Main category: cs.CV

TL;DR: 研究设计选择对图像和视频生成中组合泛化的影响，找出两个关键因素并展示改进离散模型组合性能的方法。


<details>
  <summary>Details</summary>
Motivation: 并非所有影响组合泛化的机制都被完全理解，需系统研究设计选择对其的影响。

Method: 通过控制实验，研究设计选择的影响，确定两个关键因素。

Result: 发现训练目标分布类型和条件信息程度是关键因素，放松MaskGIT离散损失可提升其组合性能。

Conclusion: 放松MaskGIT离散损失并结合辅助连续JEPA目标能改善离散模型的组合泛化性能。

Abstract: Compositional generalization, the ability to generate novel combinations of
known concepts, is a key ingredient for visual generative models. Yet, not all
mechanisms that enable or inhibit it are fully understood. In this work, we
conduct a systematic study of how various design choices influence
compositional generalization in image and video generation in a positive or
negative way. Through controlled experiments, we identify two key factors: (i)
whether the training objective operates on a discrete or continuous
distribution, and (ii) to what extent conditioning provides information about
the constituent concepts during training. Building on these insights, we show
that relaxing the MaskGIT discrete loss with an auxiliary continuous JEPA-based
objective can improve compositional performance in discrete models like
MaskGIT.

</details>


### [289] [HAVIR: HierArchical Vision to Image Reconstruction using CLIP-Guided Versatile Diffusion](https://arxiv.org/abs/2510.03122)
*Shiyi Zhang,Dong Liang,Hairong Zheng,Yihang Zhou*

Main category: cs.CV

TL;DR: 本文提出HAVIR模型用于从脑活动重建视觉信息，实验显示该模型在复杂场景下重建效果优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有从脑活动重建视觉信息的方法在准确恢复高度复杂视觉刺激方面存在挑战，自然场景特征特性导致了这一困难。

Method: 受视觉皮层分层表征理论启发，提出HAVIR模型，将视觉皮层分为两个层级区域并提取特征，包括用结构生成器提取结构信息转化为潜在扩散先验，语义提取器将语义处理体素转化为CLIP嵌入，再通过通用扩散模型合成最终图像。

Result: HAVIR模型提升了重建图像的结构和语义质量，在复杂场景下也表现良好，优于现有模型。

Conclusion: HAVIR模型在从脑活动重建视觉信息方面有较好的效果，能应对复杂场景，优于现有方法。

Abstract: The reconstruction of visual information from brain activity fosters
interdisciplinary integration between neuroscience and computer vision.
However, existing methods still face challenges in accurately recovering highly
complex visual stimuli. This difficulty stems from the characteristics of
natural scenes: low-level features exhibit heterogeneity, while high-level
features show semantic entanglement due to contextual overlaps. Inspired by the
hierarchical representation theory of the visual cortex, we propose the HAVIR
model, which separates the visual cortex into two hierarchical regions and
extracts distinct features from each. Specifically, the Structural Generator
extracts structural information from spatial processing voxels and converts it
into latent diffusion priors, while the Semantic Extractor converts semantic
processing voxels into CLIP embeddings. These components are integrated via the
Versatile Diffusion model to synthesize the final image. Experimental results
demonstrate that HAVIR enhances both the structural and semantic quality of
reconstructions, even in complex scenes, and outperforms existing models.

</details>


### [290] [SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the SpineMed-450k Corpus](https://arxiv.org/abs/2510.03160)
*Ming Zhao,Wenhui Dong,Yang Zhang,Xiang Zheng,Zhonghao Zhang,Zian Zhou,Yunzhi Guan,Liukun Xu,Wei Peng,Zhaoyang Gong,Zhicheng Zhang,Dachuan Li,Xiaosheng Ma,Yuli Ma,Jianing Ni,Changjiang Jiang,Lixia Tian,Qixin Chen,Kaishun Xia,Pingping Liu,Tongshun Zhang,Zhiqiang Liu,Zhongan Bi,Chenyang Si,Tiansheng Sun,Caifeng Shan*

Main category: cs.CV

TL;DR: 脊柱疾病影响全球6.19亿人，AI辅助诊断受限于缺乏特定椎骨层面、多模态数据集。本文引入SpineMed生态系统，含数据集SpineMed - 450k和评估框架SpineBench，评估显示微调模型有显著提升。


<details>
  <summary>Details</summary>
Motivation: 脊柱疾病是全球残疾的主要原因之一，但AI辅助诊断缺乏层面感知、多模态数据集以及可追溯的临床指导数据和标准化基准。

Method: 引入与脊柱外科医生共同设计的SpineMed生态系统，包括从多种来源整理的SpineMed - 450k数据集和临床评估框架SpineBench。使用临床医生参与的管道和两阶段大语言模型生成方法整理数据。

Result: 对多个大视觉语言模型的评估揭示了其在细粒度、特定层面推理上的弱点，微调后的模型在所有任务上有显著提升，医生评估确认了模型输出的诊断清晰度和实用性。

Conclusion: SpineMed生态系统能有效提升脊柱疾病AI辅助诊断的性能，其数据集和评估框架具有重要价值。

Abstract: Spine disorders affect 619 million people globally and are a leading cause of
disability, yet AI-assisted diagnosis remains limited by the lack of
level-aware, multimodal datasets. Clinical decision-making for spine disorders
requires sophisticated reasoning across X-ray, CT, and MRI at specific
vertebral levels. However, progress has been constrained by the absence of
traceable, clinically-grounded instruction data and standardized,
spine-specific benchmarks. To address this, we introduce SpineMed, an ecosystem
co-designed with practicing spine surgeons. It features SpineMed-450k, the
first large-scale dataset explicitly designed for vertebral-level reasoning
across imaging modalities with over 450,000 instruction instances, and
SpineBench, a clinically-grounded evaluation framework. SpineMed-450k is
curated from diverse sources, including textbooks, guidelines, open datasets,
and ~1,000 de-identified hospital cases, using a clinician-in-the-loop pipeline
with a two-stage LLM generation method (draft and revision) to ensure
high-quality, traceable data for question-answering, multi-turn consultations,
and report generation. SpineBench evaluates models on clinically salient axes,
including level identification, pathology assessment, and surgical planning.
Our comprehensive evaluation of several recently advanced large vision-language
models (LVLMs) on SpineBench reveals systematic weaknesses in fine-grained,
level-specific reasoning. In contrast, our model fine-tuned on SpineMed-450k
demonstrates consistent and significant improvements across all tasks.
Clinician assessments confirm the diagnostic clarity and practical utility of
our model's outputs.

</details>


### [291] [UniShield: An Adaptive Multi-Agent Framework for Unified Forgery Image Detection and Localization](https://arxiv.org/abs/2510.03161)
*Qing Huang,Zhipei Xu,Xuanyu Zhang,Jian Zhang*

Main category: cs.CV

TL;DR: 图像生成发展使合成图像逼真，带来风险，FIDL重要但现有方法有局限，提出UniShield系统，实验显示其效果好。


<details>
  <summary>Details</summary>
Motivation: 合成图像带来社会风险，现有FIDL方法因专业性窄、跨域泛化差和缺乏自适应框架，实际应用受限。

Method: 提出基于多智能体的统一系统UniShield，集成感知智能体和检测智能体。感知智能体分析特征选模型，检测智能体整合专家检测器并生成报告。

Result: UniShield取得了最先进的结果，超越现有统一方法和特定领域检测器。

Conclusion: UniShield具有优越的实用性、适应性和可扩展性。

Abstract: With the rapid advancements in image generation, synthetic images have become
increasingly realistic, posing significant societal risks, such as
misinformation and fraud. Forgery Image Detection and Localization (FIDL) thus
emerges as essential for maintaining information integrity and societal
security. Despite impressive performances by existing domain-specific detection
methods, their practical applicability remains limited, primarily due to their
narrow specialization, poor cross-domain generalization, and the absence of an
integrated adaptive framework. To address these issues, we propose UniShield,
the novel multi-agent-based unified system capable of detecting and localizing
image forgeries across diverse domains, including image manipulation, document
manipulation, DeepFake, and AI-generated images. UniShield innovatively
integrates a perception agent with a detection agent. The perception agent
intelligently analyzes image features to dynamically select suitable detection
models, while the detection agent consolidates various expert detectors into a
unified framework and generates interpretable reports. Extensive experiments
show that UniShield achieves state-of-the-art results, surpassing both existing
unified approaches and domain-specific detectors, highlighting its superior
practicality, adaptiveness, and scalability.

</details>


### [292] [Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles](https://arxiv.org/abs/2510.03224)
*Dong Lao,Yuxiang Zhang,Haniyeh Ehsani Oskouie,Yangchao Wu,Alex Wong,Stefano Soatto*

Main category: cs.CV

TL;DR: 提出一种测试时防御对抗攻击机制，利用随机共振“以噪制噪”，训练和架构无关，实验显示在多任务有良好鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有依赖特征过滤或平滑的方法会导致信息损失，需新的对抗攻击防御机制。

Method: 对输入图像引入小的平移扰动，对齐变换后的特征嵌入并聚合，再映射回原始参考图像，有闭式公式。

Result: 在图像分类、立体匹配和光流等任务上展现出最先进的鲁棒性，恢复一定比例的准确率损失。

Conclusion: 该方法训练和架构无关、攻击无关，具备通用性和实用性。

Abstract: We propose a test-time defense mechanism against adversarial attacks:
imperceptible image perturbations that significantly alter the predictions of a
model. Unlike existing methods that rely on feature filtering or smoothing,
which can lead to information loss, we propose to "combat noise with noise" by
leveraging stochastic resonance to enhance robustness while minimizing
information loss. Our approach introduces small translational perturbations to
the input image, aligns the transformed feature embeddings, and aggregates them
before mapping back to the original reference image. This can be expressed in a
closed-form formula, which can be deployed on diverse existing network
architectures without introducing additional network modules or fine-tuning for
specific attack types. The resulting method is entirely training-free,
architecture-agnostic, and attack-agnostic. Empirical results show
state-of-the-art robustness on image classification and, for the first time,
establish a generic test-time defense for dense prediction tasks, including
stereo matching and optical flow, highlighting the method's versatility and
practicality. Specifically, relative to clean (unperturbed) performance, our
method recovers up to 68.1% of the accuracy loss on image classification, 71.9%
on stereo matching, and 29.2% on optical flow under various types of
adversarial attacks.

</details>


### [293] [Improving GUI Grounding with Explicit Position-to-Coordinate Mapping](https://arxiv.org/abs/2510.03230)
*Suyuchen Wang,Tianyu Zhang,Ahmed Masry,Christopher Pal,Spandana Gella,Bang Liu,Perouz Taslakian*

Main category: cs.CV

TL;DR: 现有VLM在GUI grounding任务中因patch - to - pixel映射问题在新分辨率下表现差，提出RULER tokens和I - MRoPE创新方法，实验显示提升了准确率。


<details>
  <summary>Details</summary>
Motivation: 解决当前VLM在GUI grounding任务中，在新分辨率下因patch - to - pixel映射问题导致准确率下降和故障增多的问题。

Method: 提出RULER tokens作为显式坐标标记，提出Interleaved MRoPE（I - MRoPE）改进空间编码。

Result: 在ScreenSpot、ScreenSpot - V2和ScreenSpot - Pro实验中，接地准确率持续提升，在高分辨率界面提升最大。

Conclusion: 提供显式空间指导而非依赖隐式学习的方法，能在不同分辨率和平台实现更可靠的GUI自动化。

Abstract: GUI grounding, the task of mapping natural-language instructions to pixel
coordinates, is crucial for autonomous agents, yet remains difficult for
current VLMs. The core bottleneck is reliable patch-to-pixel mapping, which
breaks when extrapolating to high-resolution displays unseen during training.
Current approaches generate coordinates as text tokens directly from visual
features, forcing the model to infer complex position-to-pixel mappings
implicitly; as a result, accuracy degrades and failures proliferate on new
resolutions. We address this with two complementary innovations. First, RULER
tokens serve as explicit coordinate markers, letting the model reference
positions similar to gridlines on a map and adjust rather than generate
coordinates from scratch. Second, Interleaved MRoPE (I-MRoPE) improves spatial
encoding by ensuring that width and height dimensions are represented equally,
addressing the asymmetry of standard positional schemes. Experiments on
ScreenSpot, ScreenSpot-V2, and ScreenSpot-Pro show consistent gains in
grounding accuracy, with the largest improvements on high-resolution
interfaces. By providing explicit spatial guidance rather than relying on
implicit learning, our approach enables more reliable GUI automation across
diverse resolutions and platforms.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [294] [NEURODNAAI: Neural pipeline approaches for the advancing dna-based information storage as a sustainable digital medium using deep learning framework](https://arxiv.org/abs/2510.02417)
*Rakesh Thakur,Lavanya Singh,Yashika,Manomay Bundawala,Aruna Kumar*

Main category: cs.ET

TL;DR: 本文提出NeuroDNAAI框架用于DNA存储，借鉴量子并行概念，结合深度学习，实验显示其准确性高，可实现可扩展、生物有效的DNA存储。


<details>
  <summary>Details</summary>
Motivation: 现有DNA数字信息存储面临合成成本、测序错误和生物约束等挑战，限制实际应用。

Method: 借鉴量子并行概念增强编码多样性和弹性，结合生物约束与深度学习进行错误缓解；NeuroDNAAI将二进制数据流编码为DNA序列，经噪声信道传输后高保真重建。

Result: 传统方案不能有效适应实际噪声，NeuroDNAAI准确性更高，在基准数据集上文本和图像的误码率低。

Conclusion: NeuroDNAAI将理论、工作流程和模拟统一，可实现可扩展、生物有效的DNA存储。

Abstract: DNA is a promising medium for digital information storage for its exceptional
density and durability. While prior studies advanced coding theory, workflow
design, and simulation tools, challenges such as synthesis costs, sequencing
errors, and biological constraints (GC-content imbalance, homopolymers) limit
practical deployment. To address this, our framework draws from quantum
parallelism concepts to enhance encoding diversity and resilience, integrating
biologically informed constraints with deep learning to enhance error
mitigation in DNA storage. NeuroDNAAI encodes binary data streams into symbolic
DNA sequences, transmits them through a noisy channel with substitutions,
insertions, and deletions, and reconstructs them with high fidelity. Our
results show that traditional prompting or rule-based schemes fail to adapt
effectively to realistic noise, whereas NeuroDNAAI achieves superior accuracy.
Experiments on benchmark datasets demonstrate low bit error rates for both text
and images. By unifying theory, workflow, and simulation into one pipeline,
NeuroDNAAI enables scalable, biologically valid archival DNA storage

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [295] [Fully automated inverse co-optimization of templates and block copolymer blending recipes for DSA lithography](https://arxiv.org/abs/2510.02715)
*Yuhao Zhou,Huangyan Shen,Qingliang Song,Qingshu Dong,Jianfeng Li,Weihua Li*

Main category: physics.comp-ph

TL;DR: 本文提出高斯描述符和使用AB/AB二元共混物，结合贝叶斯优化，为定向自组装技术优化模板，确保可制造性并推动实际应用。


<details>
  <summary>Details</summary>
Motivation: 解决块体共聚物自组装中模板形状有效参数化和优化，及优化模板需具备良好可制造性的问题。

Method: 提出高斯描述符表征模板形状，使用AB/AB二元共混物，应用贝叶斯优化共同优化二元共混物和模板形状，并在优化中对模板曲率变化施加约束。

Result: 基于高斯描述符的贝叶斯优化能为多样多孔图案高效生成最优模板，自组装形态高度匹配，且优化模板可制造性好，共混物关键参数可调窗口宽。

Conclusion: 为定向自组装技术提供有价值见解，推动其实际应用。

Abstract: The directed self-assembly (DSA) of block copolymers (BCPs) offers a highly
promising approach for the fabrication of contact holes or vertical
interconnect access at sub-7nm technology nodes. To fabricate circular holes
with precisely controlled size and positions, the self-assembly of block
copolymers requires guidance from a properly designed template. Effectively
parameterizing the template shape to enable efficient optimization remains a
critical yet challenging problem. Moreover, the optimized template must possess
excellent manufacturability for practical applications. In this work, we
propose a Gaussian descriptor for characterizing the template shape with only
two parameters. We further propose to use AB/AB binary blends instead of pure
diblock copolymer to improve the adaptability of the block copolymer system to
the template shape. The Bayesian optimization (BO) is applied to co-optimize
the binary blend and the template shape. Our results demonstrate that BO based
on the Gaussian descriptor can efficiently yield the optimal templates for
diverse multi-hole patterns, all leading to highly matched self-assembled
morphologies. Moreover, by imposing constraints on the variation of curvature
of the template during optimization, superior manufacturability is ensured for
each optimized template. It is noteworthy that each key parameter of the blend
exhibits a relatively wide tunable window under the requirement of rather high
precision. Our work provides valuable insights for advancing DSA technology,
and thus potentially propels its practical applications forward.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [296] [Model Falsification for Predicting Dynamical Responses of Uncertain Structural Systems](https://arxiv.org/abs/2510.02612)
*Subhayan De,Tianhao Yu,Patrick T. Brewick,Erik A. Johnson,Steven F. Wojtkiewicz*

Main category: stat.AP

TL;DR: 提出一种基于误发现率边界的结构动力响应预测方法，可减少计算成本并实现准确预测。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯方法预测结构动力响应需大量模型模拟，因多数模型不合理且计算成本高，需改进。

Method: 利用误发现率计算边界，对不能准确重现测量数据的模型进行证伪，用未证伪模型以近似贝叶斯方式预测响应。

Result: 通过三个结构实例验证，实现准确响应预测并显著节省计算成本。

Conclusion: 所提方法有潜力用于结构动力响应预测。

Abstract: Accurate prediction of dynamical response of structural system depends on the
correct modeling of that system. However, modeling becomes increasingly
challenging when there are many candidate models available to describe the
system behavior. Furthermore, uncertainties can be present even for the
parameters of these model classes. The plausibility of each input-output model
class of the structures with uncertain components can be determined by a
Bayesian approach from measured dynamic responses to one or more input records;
predictions of the structural system response to alternate input records can
then be made. However, this approach may require many model simulations, even
though most of those model classes are quite implausible. An approach is
proposed herein to use a bound, computed from the false discovery rate, on the
likelihood of measured data to falsify models considering uncertainties in the
passive control devices that do not reproduce the measured data to sufficient
accuracy. Response prediction is then performed using the unfalsified models in
an approximate Bayesian sense by assigning weights, computed from the
likelihoods, only to the unfalsified models approach incurring only a fraction
of the computational cost of the standard Bayesian approach. The proposed
approach for response prediction is illustrated using three structural
examples: an earthquake-excited four--degree-of-freedom building model with a
hysteretic isolation layer; a 1623--degree-of-freedom three-dimensional
building model, with tuned mass dampers attached to its roof, subjected to wind
loads; and a full-scale four-story base-isolated building tested on world's
largest shake table in Japan's E-Defense lab. The results exhibit accurate
response predictions and significant computational savings, thereby
illustrating the potential of the proposed method.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [297] [Joint Stochastic Optimal Control and Stopping in Aquaculture: Finite-Difference and PINN-Based Approaches](https://arxiv.org/abs/2510.02910)
*Kevin Kamm*

Main category: math.OC

TL;DR: 本文研究水产养殖中的联合随机最优控制与停止问题，提出简化模型，用两种数值方法求解，结果表明联合优化优于单一策略。


<details>
  <summary>Details</summary>
Motivation: 在水产养殖中，通过最优投喂策略和收获时间，在随机价格动态下最大化农场利润。

Method: 将问题表述为HJB变分不等式和自由边界问题，采用有限差分法作为基准，结合物理信息神经网络（PINN）和深度最优停止（DeepOS）算法。

Result: 有限差分法在中等维度表现良好，PINN方法在高维更具扩展性且精度相当，联合优化决策优于单一策略。

Conclusion: 联合优化投喂和收获决策能提高水产养殖利润。

Abstract: This paper studies a joint stochastic optimal control and stopping (JCtrlOS)
problem motivated by aquaculture operations, where the objective is to maximize
farm profit through an optimal feeding strategy and harvesting time under
stochastic price dynamics. We introduce a simplified aquaculture model
capturing essential biological and economic features, distinguishing between
biologically optimal and economically optimal feeding strategies. The problem
is formulated as a Hamilton-Jacobi-Bellman variational inequality and
corresponding free boundary problem. We develop two numerical solution
approaches: First, a finite difference scheme that serves as a benchmark, and
second, a Physics-Informed Neural Network (PINN)-based method, combined with a
deep optimal stopping (DeepOS) algorithm to improve stopping time accuracy.
Numerical experiments demonstrate that while finite differences perform well in
medium-dimensional settings, the PINN approach achieves comparable accuracy and
is more scalable to higher dimensions where grid-based methods become
infeasible. The results confirm that jointly optimizing feeding and harvesting
decisions outperforms strategies that neglect either control or stopping.

</details>


### [298] [Quantitative Convergence Analysis of Projected Stochastic Gradient Descent for Non-Convex Losses via the Goldstein Subdifferential](https://arxiv.org/abs/2510.02735)
*Yuping Zheng,Andrew Lamperski*

Main category: math.OC

TL;DR: 本文分析紧致凸集上非凸损失的投影随机梯度下降（SGD），提出新收敛准则，在无方差缩减下获收敛结果，给出不同数据条件下渐近和非渐近界。


<details>
  <summary>Details</summary>
Motivation: 现有投影SGD收敛结果在度量和方差缩减方面有局限，难以与无约束SGD比较。

Method: 通过梯度到约束产生的Goldstein次微分的距离度量收敛，分析独立同分布（IID）或满足混合条件的数据。

Result: 得到渐近收敛和期望下的$O(N^{-1/3})$非渐近界，在IID次高斯数据下有几乎必然渐近收敛和高概率$O(N^{-1/5})$非渐近界。

Conclusion: 提出的收敛准则可简化为无约束情况常用准则，且无需方差缩减，得到首个投影SGD非凸损失的非渐近高概率界。

Abstract: Stochastic gradient descent (SGD) is the main algorithm behind a large body
of work in machine learning. In many cases, constraints are enforced via
projections, leading to projected stochastic gradient algorithms. In recent
years, a large body of work has examined the convergence properties of
projected SGD for non-convex losses in asymptotic and non-asymptotic settings.
Strong quantitative guarantees are available for convergence measured via
Moreau envelopes. However, these results cannot be compared directly with work
on unconstrained SGD, since the Moreau envelope construction changes the
gradient. Other common measures based on gradient mappings have the limitation
that convergence can only be guaranteed if variance reduction methods, such as
mini-batching, are employed. This paper presents an analysis of projected SGD
for non-convex losses over compact convex sets. Convergence is measured via the
distance of the gradient to the Goldstein subdifferential generated by the
constraints. Our proposed convergence criterion directly reduces to commonly
used criteria in the unconstrained case, and we obtain convergence without
requiring variance reduction. We obtain results for data that are independent,
identically distributed (IID) or satisfy mixing conditions ($L$-mixing). In
these cases, we derive asymptotic convergence and $O(N^{-1/3})$ non-asymptotic
bounds in expectation, where $N$ is the number of steps. In the case of IID
sub-Gaussian data, we obtain almost-sure asymptotic convergence and
high-probability non-asymptotic $O(N^{-1/5})$ bounds. In particular, these are
the first non-asymptotic high-probability bounds for projected SGD with
non-convex losses.

</details>


### [299] [Improving Online-to-Nonconvex Conversion for Smooth Optimization via Double Optimism](https://arxiv.org/abs/2510.03167)
*Francisco Patitucci,Ruichen Jiang,Aryan Mokhtari*

Main category: math.OC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: A recent breakthrough in nonconvex optimization is the online-to-nonconvex
conversion framework of \cite{cutkosky2023optimal}, which reformulates the task
of finding an $\varepsilon$-first-order stationary point as an online learning
problem. When both the gradient and the Hessian are Lipschitz continuous,
instantiating this framework with two different online learners achieves a
complexity of $\mathcal{O}(\varepsilon^{-1.75}\log(1/\varepsilon))$ in the
deterministic case and a complexity of $\mathcal{O}(\varepsilon^{-3.5})$ in the
stochastic case. However, this approach suffers from several limitations: (i)
the deterministic method relies on a complex double-loop scheme that solves a
fixed-point equation to construct hint vectors for an optimistic online
learner, introducing an extra logarithmic factor; (ii) the stochastic method
assumes a bounded second-order moment of the stochastic gradient, which is
stronger than standard variance bounds; and (iii) different online learning
algorithms are used in the two settings. In this paper, we address these issues
by introducing an online optimistic gradient method based on a novel
\textit{doubly optimistic hint function}. Specifically, we use the gradient at
an extrapolated point as the hint, motivated by two optimistic assumptions:
that the difference between the hint and the target gradient remains near
constant, and that consecutive update directions change slowly due to
smoothness. Our method eliminates the need for a double loop and removes the
logarithmic factor. Furthermore, by simply replacing full gradients with
stochastic gradients and under the standard assumption that their variance is
bounded by $\sigma^2$, we obtain a unified algorithm with complexity
$\mathcal{O}(\varepsilon^{-1.75} + \sigma^2 \varepsilon^{-3.5})$, smoothly
interpolating between the best-known deterministic rate and the optimal
stochastic rate.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [300] [Linear RNNs for autoregressive generation of long music samples](https://arxiv.org/abs/2510.02401)
*Konrad Szewczyk,Daniel Gallo Fernández,James Townsend*

Main category: cs.SD

TL;DR: 本文探索线性RNN在原始音频建模中的应用，提出HarmonicRNN模型，在小规模数据集上取得了最优效果。


<details>
  <summary>Details</summary>
Motivation: 直接自回归生成音频波形有挑战，传统方法效果有限，而线性RNN有潜力，因此探索其在原始音频建模中的应用。

Method: 研究不同架构选择的影响，使用上下文并行性，以支持长达一分钟序列的训练，提出HarmonicRNN模型。

Result: HarmonicRNN在小规模数据集上达到了最优的对数似然和感知指标。

Conclusion: 线性RNN在原始音频建模中具有潜力，HarmonicRNN模型表现出色。

Abstract: Directly learning to generate audio waveforms in an autoregressive manner is
a challenging task, due to the length of the raw sequences and the existence of
important structure on many different timescales. Traditional approaches based
on recurrent neural networks, as well as causal convolutions and
self-attention, have only had limited success on this task. However, recent
work has shown that deep state space models, also referred to as linear RNNs,
can be highly efficient in this context. In this work, we push the boundaries
of linear RNNs applied to raw audio modeling, investigating the effects of
different architectural choices and using context-parallelism to enable
training on sequences up to one minute (1M tokens) in length. We present a
model, HarmonicRNN, which attains state of the art log-likelihoods and
perceptual metrics on small-scale datasets.

</details>


### [301] [Flamed-TTS: Flow Matching Attention-Free Models for Efficient Generating and Dynamic Pacing Zero-shot Text-to-Speech](https://arxiv.org/abs/2510.02848)
*Hieu-Nghia Huynh-Nguyen,Huynh Nguyen Dang,Ngoc-Son Nguyen,Van Nguyen*

Main category: cs.SD

TL;DR: 提出新型零样本TTS框架Flamed - TTS，解决现有方法问题，实验效果超SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有零样本TTS方法存在合成不可靠、推理慢、计算开销大及时间多样性不足等问题。

Method: 重新制定流匹配训练范式，结合语音不同属性的离散和连续表示。

Result: Flamed - TTS在多项指标上超SOTA，WER达4%，推理低延迟，语音高保真。

Conclusion: Flamed - TTS是有效的零样本TTS框架，代码和音频样本可在官网获取。

Abstract: Zero-shot Text-to-Speech (TTS) has recently advanced significantly, enabling
models to synthesize speech from text using short, limited-context prompts.
These prompts serve as voice exemplars, allowing the model to mimic speaker
identity, prosody, and other traits without extensive speaker-specific data.
Although recent approaches incorporating language models, diffusion, and flow
matching have proven their effectiveness in zero-shot TTS, they still encounter
challenges such as unreliable synthesis caused by token repetition or
unexpected content transfer, along with slow inference and substantial
computational overhead. Moreover, temporal diversity-crucial for enhancing the
naturalness of synthesized speech-remains largely underexplored. To address
these challenges, we propose Flamed-TTS, a novel zero-shot TTS framework that
emphasizes low computational cost, low latency, and high speech fidelity
alongside rich temporal diversity. To achieve this, we reformulate the flow
matching training paradigm and incorporate both discrete and continuous
representations corresponding to different attributes of speech. Experimental
results demonstrate that Flamed-TTS surpasses state-of-the-art models in terms
of intelligibility, naturalness, speaker similarity, acoustic characteristics
preservation, and dynamic pace. Notably, Flamed-TTS achieves the best WER of 4%
compared to the leading zero-shot TTS baselines, while maintaining low latency
in inference and high fidelity in generated speech. Code and audio samples are
available at our demo page https://flamed-tts.github.io.

</details>


### [302] [WavInWav: Time-domain Speech Hiding via Invertible Neural Network](https://arxiv.org/abs/2510.02915)
*Wei Fan,Kejiang Chen,Xiangkun Wang,Weiming Zhang,Nenghai Yu*

Main category: cs.SD

TL;DR: 本文针对先前音频隐藏方法在恢复秘密音频时质量不佳的问题，提出基于DNN的新方法，实验表明该方法优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 先前音频隐藏方法在恢复秘密音频时因对时频关系建模的固有局限，导致音质不佳，需改进。

Method: 使用基于流的可逆神经网络建立隐写音频、载体音频和秘密音频的直接联系；在时域信号上实现时频损失；添加加密技术保护隐藏数据。

Result: 在VCTK和LibriSpeech数据集上实验，该方法在主客观指标上优于先前方法，对各类噪声有鲁棒性。

Conclusion: 该方法在目标安全通信场景中具有实用性。

Abstract: Data hiding is essential for secure communication across digital media, and
recent advances in Deep Neural Networks (DNNs) provide enhanced methods for
embedding secret information effectively. However, previous audio hiding
methods often result in unsatisfactory quality when recovering secret audio,
due to their inherent limitations in the modeling of time-frequency
relationships. In this paper, we explore these limitations and introduce a new
DNN-based approach. We use a flow-based invertible neural network to establish
a direct link between stego audio, cover audio, and secret audio, enhancing the
reversibility of embedding and extracting messages. To address common issues
from time-frequency transformations that degrade secret audio quality during
recovery, we implement a time-frequency loss on the time-domain signal. This
approach not only retains the benefits of time-frequency constraints but also
enhances the reversibility of message recovery, which is vital for practical
applications. We also add an encryption technique to protect the hidden data
from unauthorized access. Experimental results on the VCTK and LibriSpeech
datasets demonstrate that our method outperforms previous approaches in terms
of subjective and objective metrics and exhibits robustness to various types of
noise, suggesting its utility in targeted secure communication scenarios.

</details>


### [303] [SALSA-V: Shortcut-Augmented Long-form Synchronized Audio from Videos](https://arxiv.org/abs/2510.02916)
*Amir Dellali,Luca A. Lanzendörfer,Florian Grötschla,Roger Wattenhofer*

Main category: cs.SD

TL;DR: 提出SALSA - V多模态视频转音频生成模型，能从无声视频合成高同步、高保真长音频，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 开发能从无声视频合成高同步、高保真长音频的模型，满足近实时应用和专业音频合成需求。

Method: 引入掩码扩散目标，实现音频条件生成和任意长度音频序列合成；训练中加入捷径损失，实现快速生成。

Result: 在定量评估和人耳试听研究中，SALSA - V在视听对齐和同步方面显著优于现有方法；训练时使用随机掩码能匹配参考音频样本频谱特征。

Conclusion: SALSA - V在视频转音频任务中表现出色，适用于近实时应用和专业音频合成任务。

Abstract: We propose SALSA-V, a multimodal video-to-audio generation model capable of
synthesizing highly synchronized, high-fidelity long-form audio from silent
video content. Our approach introduces a masked diffusion objective, enabling
audio-conditioned generation and the seamless synthesis of audio sequences of
unconstrained length. Additionally, by integrating a shortcut loss into our
training process, we achieve rapid generation of high-quality audio samples in
as few as eight sampling steps, paving the way for near-real-time applications
without requiring dedicated fine-tuning or retraining. We demonstrate that
SALSA-V significantly outperforms existing state-of-the-art methods in both
audiovisual alignment and synchronization with video content in quantitative
evaluation and a human listening study. Furthermore, our use of random masking
during training enables our model to match spectral characteristics of
reference audio samples, broadening its applicability to professional audio
synthesis tasks such as Foley generation and sound design.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [304] [Stimulus-Voltage-Based Prediction of Action Potential Onset Timing: Classical vs. Quantum-Inspired Approaches](https://arxiv.org/abs/2510.03155)
*Stevens Johnson,Varun Puram,Johnson Thomas,Acsah Konuparamban,Ashwin Kannan*

Main category: q-bio.NC

TL;DR: 提出量子启发的漏积分放电（QI - LIF）模型，对比经典LIF模型，结果显示QI - LIF能显著降低动作电位起始预测误差，凸显量子启发计算框架在神经建模中的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统漏积分放电（LIF）模型在预测动作电位起始潜伏期时相对误差高，尤其是在强或快速变化刺激下，需更准确的模型理解危险信号神经编码。

Method: 提出将动作电位起始视为概率事件，用高斯波包表示的QI - LIF模型，用海马和感觉神经元的合成数据，比较经典LIF和QI - LIF模型动作电位起始预测的相对误差。

Result: QI - LIF模型显著降低预测误差，尤其在高强度刺激下，与生物观测响应更吻合。

Conclusion: 量子启发计算框架可提高神经建模准确性，对量子工程用于脑启发计算有启示。

Abstract: Accurate modeling of neuronal action potential (AP) onset timing is crucial
for understanding neural coding of danger signals. Traditional leaky
integrate-and-fire (LIF) models, while widely used, exhibit high relative error
in predicting AP onset latency, especially under strong or rapidly changing
stimuli. Inspired by recent experimental findings and quantum theory, we
present a quantum-inspired leaky integrate-and-fire (QI-LIF) model that treats
AP onset as a probabilistic event, represented by a Gaussian wave packet in
time. This approach captures the biological variability and uncertainty
inherent in neuronal firing. We systematically compare the relative error of AP
onset predictions between the classical LIF and QI-LIF models using synthetic
data from hippocampal and sensory neurons subjected to varying stimulus
amplitudes. Our results demonstrate that the QI-LIF model significantly reduces
prediction error, particularly for high-intensity stimuli, aligning closely
with observed biological responses. This work highlights the potential of
quantum-inspired computational frameworks in advancing the accuracy of neural
modeling and has implications for quantum engineering approaches to
brain-inspired computing.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [305] [Self-supervised diffusion model fine-tuning for costate initialization using Markov chain Monte Carlo](https://arxiv.org/abs/2510.02527)
*Jannik Graebner,Ryne Beeson*

Main category: astro-ph.EP

TL;DR: 本文使用条件扩散模型结合马尔可夫链蒙特卡罗算法与自监督微调，解决低推力航天器轨迹全局搜索与优化难题，通过数值实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 间接法进行长时低推力航天器轨迹全局搜索与优化面临解空间复杂和难以生成协态变量初始猜测的挑战，期望找到完成帕累托前沿及解决相关轨迹问题的灵活方法。

Method: 用条件扩散模型表示候选最优轨迹解的分布，引入马尔可夫链蒙特卡罗算法与自监督微调，采用随机游走Metropolis算法生成新数据，基于约束违规和任务目标函数评估进行奖励加权训练微调扩散模型。

Result: 通过木星 - 欧罗巴和土星 - 泰坦两个转移问题的数值实验，展示了提高样本质量和明确实现帕累托最优的能力，完成部分帕累托前沿并生成密集优质的帕累托前沿。

Conclusion: 所提框架无需单独的数据生成阶段，基于马尔可夫链理论能有效解决低推力航天器轨迹优化问题，提高样本质量并实现帕累托最优。

Abstract: Global search and optimization of long-duration, low-thrust spacecraft
trajectories with the indirect method is challenging due to a complex solution
space and the difficulty of generating good initial guesses for the costate
variables. This is particularly true in multibody environments. Given data that
reveals a partial Pareto optimal front, it is desirable to find a flexible
manner in which the Pareto front can be completed and fronts for related
trajectory problems can be found. In this work we use conditional diffusion
models to represent the distribution of candidate optimal trajectory solutions.
We then introduce into this framework the novel approach of using Markov Chain
Monte Carlo algorithms with self-supervised fine-tuning to achieve the
aforementioned goals. Specifically, a random walk Metropolis algorithm is
employed to propose new data that can be used to fine-tune the diffusion model
using a reward-weighted training based on efficient evaluations of constraint
violations and missions objective functions. The framework removes the need for
separate focused and often tedious data generation phases. Numerical
experiments are presented for two problems demonstrating the ability to improve
sample quality and explicitly target Pareto optimality based on the theory of
Markov chains. The first problem does so for a transfer in the Jupiter-Europa
circular restricted three-body problem, where the MCMC approach completes a
partial Pareto front. The second problem demonstrates how a dense and superior
Pareto front can be generated by the MCMC self-supervised fine-tuning method
for a Saturn-Titan transfer starting from the Jupiter-Europa case versus a
separate dedicated global search.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [306] [An Encoder-Decoder Network for Beamforming over Sparse Large-Scale MIMO Channels](https://arxiv.org/abs/2510.02355)
*Yubo Zhang,Jeremy Johnston,Xiaodong Wang*

Main category: eess.SY

TL;DR: 提出用于大规模稀疏MIMO信道下行波束赋形的端到端深度学习框架EDN，拓展到远近场混合波束赋形场景，仿真验证有效性。


<details>
  <summary>Details</summary>
Motivation: 开发适用于大规模稀疏MIMO信道下行波束赋形的有效方法。

Method: 构建含编码器、波束赋形解码器、信道解码器的EDN架构，采用半摊销学习和知识蒸馏策略训练。

Result: 该框架拓展到远近场混合波束赋形场景，仿真显示在不同网络和信道条件下有效。

Conclusion: 所提EDN波束赋形框架在大规模稀疏MIMO信道下行波束赋形中有效。

Abstract: We develop an end-to-end deep learning framework for downlink beamforming in
large-scale sparse MIMO channels. The core is a deep EDN architecture with
three modules: (i) an encoder NN, deployed at each user end, that compresses
estimated downlink channels into low-dimensional latent vectors. The latent
vector from each user is compressed and then fed back to the BS. (ii) a
beamformer decoder NN at the BS that maps recovered latent vectors to
beamformers, and (iii) a channel decoder NN at the BS that reconstructs
downlink channels from recovered latent vectors to further refine the
beamformers. The training of EDN leverages two key strategies: (a)
semi-amortized learning, where the beamformer decoder NN contains an analytical
gradient ascent during both training and inference stages, and (b) knowledge
distillation, where the loss function consists of a supervised term and an
unsupervised term, and starting from supervised training with MMSE beamformers,
over the epochs, the model training gradually shifts toward unsupervised using
the sum-rate objective. The proposed EDN beamforming framework is extended to
both far-field and near-field hybrid beamforming scenarios. Extensive
simulations validate its effectiveness under diverse network and channel
conditions.

</details>


### [307] [Global Convergence of Policy Gradient for Entropy Regularized Linear-Quadratic Control with multiplicative noise](https://arxiv.org/abs/2510.02896)
*Gabriel Diaz,Lucky Li,Wenhao Zhang*

Main category: eess.SY

TL;DR: 本文研究基于强化学习的无限时间范围带乘性噪声的熵正则化线性二次控制问题，提出RPG和SB - RPG算法，数值模拟验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 强化学习在动态环境顺序决策中强大，研究基于强化学习的熵正则化线性二次控制问题。

Method: 将正则化策略梯度（RPG）算法应用于随机最优控制，基于零阶优化方法引入无模型强化学习算法SB - RPG。

Result: 证明RPG在梯度主导和近平滑条件下全局收敛，SB - RPG在未知系统参数下有全局收敛的理论保证，数值模拟验证理论结果。

Conclusion: 所提算法有效，熵正则化可加速收敛并解决探索与利用权衡问题。

Abstract: Reinforcement Learning (RL) has emerged as a powerful framework for
sequential decision-making in dynamic environments, particularly when system
parameters are unknown. This paper investigates RL-based control for
entropy-regularized Linear Quadratic control (LQC) problems with multiplicative
noises over an infinite time horizon. First, we adapt the Regularized Policy
Gradient (RPG) algorithm to stochastic optimal control settings, proving that
despite the non-convexity of the problem, RPG converges globally under
conditions of gradient domination and near-smoothness. Second, based on
zero-order optimization approach, we introduce a novel model free RL algorithm:
Sample-Based Regularized Policy Gradient (SB-RPG). SB-RPG operates without
knowledge of system parameters yet still retains strong theoretical guarantees
of global convergence. Our model leverages entropy regularization to accelerate
convergence and address the exploration versus exploitation trade-off inherent
in RL. Numerical simulations validate the theoretical results and demonstrate
the efficacy of SB-RPG in unknown-parameters environments.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [308] [A Hardware Accelerator for the Goemans-Williamson Algorithm](https://arxiv.org/abs/2510.02863)
*D. A. Herrera-Martí,E. Guthmuller,J. Fereyre*

Main category: cs.AR

TL;DR: 本文聚焦Max - Cut问题，指出Goemans和Williamson的凸半定松弛法适合构建基准和性能评估，还介绍扩展浮点精度在凸优化代数子程序中的应用及对求解时间的加速估计。


<details>
  <summary>Details</summary>
Motivation: Max - Cut问题是评估量子和经典优化器局部搜索启发式的基准，但局部搜索仅提供平均情况性能保证，而Goemans和Williamson的方法有最坏情况保证，有必要研究其在大问题规模下的优化。

Method: 将扩展浮点精度纳入凸优化的代数子程序，如在内部点法中使用的共轭梯度等间接矩阵求逆方法。

Result: 对于原生运行扩展精度的硬件架构，使用共轭梯度等间接矩阵求逆方法时，增加内部工作精度能减少求解时间，且减少因子随系统规模增大。

Conclusion: 扩展浮点精度在凸优化代数子程序中的应用能有效加速大问题规模下Max - Cut问题的求解。

Abstract: The combinatorial problem Max-Cut has become a benchmark in the evaluation of
local search heuristics for both quantum and classical optimisers. In contrast
to local search, which only provides average-case performance guarantees, the
convex semidefinite relaxation of Max-Cut by Goemans and Williamson, provides
worst-case guarantees and is therefore suited to both the construction of
benchmarks and in applications to performance-critic scenarios.
  We show how extended floating point precision can be incorporated in
algebraic subroutines in convex optimisation, namely in indirect matrix
inversion methods like Conjugate Gradient, which are used in Interior Point
Methods in the case of very large problem sizes. Also, an estimate is provided
of the expected acceleration of the time to solution for a hardware
architecture that runs natively on extended precision. Specifically, when using
indirect matrix inversion methods like Conjugate Gradient, which have lower
complexity than direct methods and are therefore used in very large problems,
we see that increasing the internal working precision reduces the time to
solution by a factor that increases with the system size.

</details>


### [309] [HALO: Memory-Centric Heterogeneous Accelerator with 2.5D Integration for Low-Batch LLM Inference](https://arxiv.org/abs/2510.02675)
*Shubham Negi,Kaushik Roy*

Main category: cs.AR

TL;DR: 针对低批次大语言模型推理的独特挑战，提出异构内存中心加速器HALO，实验显示有显著加速效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型快速应用对高效推理需求增长，其推理两阶段计算和内存需求差异大，低批次长上下文场景研究不足。

Method: 提出HALO，集成基于HBM的Compute - in - DRAM和片上模拟Compute - in - Memory，采用相位感知映射策略，分析两种极端架构性能权衡。

Result: 在LLaMA - 2 7B和Qwen3 8B模型上实验，LLMs映射到HALO比AttAcc有18倍几何平均加速，比CENT有2.5倍加速。

Conclusion: HALO能有效应对低批次大语言模型推理中预填充和解码阶段的挑战，提升推理效率。

Abstract: The rapid adoption of Large Language Models (LLMs) has driven a growing
demand for efficient inference, particularly in latency-sensitive applications
such as chatbots and personalized assistants. Unlike traditional deep neural
networks, LLM inference proceeds in two distinct phases: the prefill phase,
which processes the full input sequence in parallel, and the decode phase,
which generates tokens sequentially. These phases exhibit highly diverse
compute and memory requirements, which makes accelerator design particularly
challenging. Prior works have primarily been optimized for high-batch inference
or evaluated only short input context lengths, leaving the low-batch and long
context regime, which is critical for interactive applications, largely
underexplored.
  We propose HALO, a heterogeneous memory centric accelerator designed for
these unique challenges of prefill and decode phases in low-batch LLM
inference. HALO integrates HBM based Compute-in-DRAM (CiD) with an on-chip
analog Compute-in-Memory (CiM), co-packaged using 2.5D integration. To further
improve the hardware utilization, we introduce a phase-aware mapping strategy
that adapts to the distinct demands of the prefill and decode phases. Compute
bound operations in the prefill phase are mapped to CiM to exploit its high
throughput matrix multiplication capability, while memory-bound operations in
the decode phase are executed on CiD to benefit from reduced data movement
within DRAM. Additionally, we present an analysis of the performance tradeoffs
of LLMs under two architectural extremes: a fully CiD and a fully on-chip
analog CiM design to highlight the need for a heterogeneous design. We evaluate
HALO on LLaMA-2 7B and Qwen3 8B models. Our experimental results show that LLMs
mapped to HALO achieve up to 18x geometric mean speedup over AttAcc, an
attention-optimized mapping and 2.5x over CENT, a fully CiD based mapping.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [310] [The Computational Complexity of Almost Stable Clustering with Penalties](https://arxiv.org/abs/2510.03143)
*Kamyar Khodamoradi,Farnam Mansouri,Sandra Zilles*

Main category: cs.CC

TL;DR: 研究小倍增维度度量下k - MEANS和k - MEDIAN聚类问题稳定实例的复杂度，提出‘几乎稳定’概念，证明特殊情况可多项式时间求解，也给出困难实例的超多项式时间下界。


<details>
  <summary>Details</summary>
Motivation: 已有研究多在低维欧氏空间下基于乘法扰动弹性，本文采用更通用‘几乎稳定’概念，并将结果扩展到带惩罚的情况。

Method: 研究特殊情况的可解性，在指数时间假设下分析困难实例的复杂度。

Result: 证明几乎稳定的k - MEANS/k - MEDIAN（带惩罚）某些特殊情况可多项式时间求解，也给出困难实例精确算法运行时间的超多项式下界。

Conclusion: 明确了小倍增维度度量下稳定k - MEANS/k - MEDIAN（带惩罚）问题在不同情况下的复杂度特性。

Abstract: We investigate the complexity of stable (or perturbation-resilient) instances
of $\mathrm{k-M\small{EANS}}$ and $\mathrm{k-M\small{EDIAN}}$ clustering
problems in metrics with small doubling dimension. While these problems have
been extensively studied under multiplicative perturbation resilience in
low-dimensional Euclidean spaces (e.g., (Friggstad et al., 2019; Cohen-Addad
and Schwiegelshohn, 2017)), we adopt a more general notion of stability, termed
``almost stable'', which is closer to the notion of $(\alpha,
\varepsilon)$-perturbation resilience introduced by Balcan and Liang (2016).
Additionally, we extend our results to
$\mathrm{k-M\small{EANS}}$/$\mathrm{k-M\small{EDIAN}}$ with penalties, where
each data point is either assigned to a cluster centre or incurs a penalty.
  We show that certain special cases of almost stable
$\mathrm{k-M\small{EANS}}$/$\mathrm{k-M\small{EDIAN}}$ (with penalties) are
solvable in polynomial time. To complement this, we also examine the hardness
of almost stable instances and $(1 + \frac{1}{poly(n)})$-stable instances of
$\mathrm{k-M\small{EANS}}$/$\mathrm{k-M\small{EDIAN}}$ (with penalties),
proving super-polynomial lower bounds on the runtime of any exact algorithm
under the widely believed Exponential Time Hypothesis (ETH).

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [311] [When Researchers Say Mental Model/Theory of Mind of AI, What Are They Really Talking About?](https://arxiv.org/abs/2510.02660)
*Xiaoyun Yin,Elmira Zahmat Doost,Shiwen Zhou,Garima Arya Yadav,Jamie C. Gorman*

Main category: cs.HC

TL;DR: 当前关于AI具备心智理论（ToM）的讨论混淆了模式匹配和真实认知，应转向相互ToM框架。


<details>
  <summary>Details</summary>
Motivation: 指出当前AI具备ToM讨论中存在混淆模拟与真实认知的问题。

Method: 分析近期研究结果以及现有测试范式的缺陷。

Result: 发现现有基于实验室任务的研究结果仅基于行为模仿，且测试范式可能存在将人类认知测试应用于AI的问题。

Conclusion: 建议将重点转向相互ToM框架，强调人机交互动态。

Abstract: When researchers claim AI systems possess ToM or mental models, they are
fundamentally discussing behavioral predictions and bias corrections rather
than genuine mental states. This position paper argues that the current
discourse conflates sophisticated pattern matching with authentic cognition,
missing a crucial distinction between simulation and experience. While recent
studies show LLMs achieving human-level performance on ToM laboratory tasks,
these results are based only on behavioral mimicry. More importantly, the
entire testing paradigm may be flawed in applying individual human cognitive
tests to AI systems, but assessing human cognition directly in the moment of
human-AI interaction. I suggest shifting focus toward mutual ToM frameworks
that acknowledge the simultaneous contributions of human cognition and AI
algorithms, emphasizing the interaction dynamics, instead of testing AI in
isolation.

</details>


### [312] [Prototyping Digital Social Spaces through Metaphor-Driven Design: Translating Spatial Concepts into an Interactive Social Simulation](https://arxiv.org/abs/2510.02759)
*Yoojin Hong,Martina Di Paola,Braahmi Padmakumar,Hwi Joon Lee,Mahnoor Shafiq,Joseph Seering*

Main category: cs.HC

TL;DR: 本文介绍了一个隐喻驱动系统，用于帮助用户设想和探索新的社交媒体环境，研究表明隐喻有助于用户表达社交期望，该系统可作为设计工具。


<details>
  <summary>Details</summary>
Motivation: 社交媒体平台设计局限于参与度和规模，且在平台限制下难以对替代方案进行原型设计。

Method: 引入隐喻驱动系统，将用户隐喻转化为平台功能集并生成含大语言模型驱动智能体的交互式模拟，开展用户创建和交互模拟社交媒体空间的研究。

Result: 隐喻能让用户表达不同社交期望，模拟的真实性取决于对亲密感、参与度和时间投入等动态的捕捉程度。

Conclusion: 隐喻驱动模拟可作为强大的设计工具，用于构建替代社交架构原型和拓展未来社交平台设计空间。

Abstract: Social media platforms are central to communication, yet their designs remain
narrowly focused on engagement and scale. While researchers have proposed
alternative visions for online spaces, these ideas are difficult to prototype
within platform constraints. In this paper, we introduce a metaphor-driven
system to help users imagine and explore new social media environments. The
system translates users' metaphors into structured sets of platform features
and generates interactive simulations populated with LLM-driven agents. To
evaluate this approach, we conducted a study where participants created and
interacted with simulated social media spaces. Our findings show that metaphors
allow users to express distinct social expectations, and that perceived
authenticity of the simulation depended on how well it captured dynamics like
intimacy, participation, and temporal engagement. We conclude by discussing how
metaphor-driven simulation can be a powerful design tool for prototyping
alternative social architectures and expanding the design space for future
social platforms.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [313] [oRANS: Online optimisation of RANS machine learning models with embedded DNS data generation](https://arxiv.org/abs/2510.02982)
*Daniel Dehtyriov,Jonathan F. MacArt,Justin Sirignano*

Main category: physics.flu-dyn

TL;DR: 提出在线优化框架解决DL用于RANS封闭模型时高保真数据有限问题，在多个算例中表现出色，提供了可扩展的物理信息机器学习封闭模型途径。


<details>
  <summary>Details</summary>
Motivation: DL用于流动物理模拟受高保真训练数据稀缺限制，传统离线范式训练的封闭模型易过拟合且泛化性差。

Method: 引入在线优化框架，在RANS域子域嵌入DNS动态生成训练数据，通过反馈循环更新DL封闭模型。

Result: 在线优化的RANS模型显著优于离线训练和文献校准的封闭模型，使用适度的DNS子域就能实现准确训练，性能在特定情况下会下降。

Conclusion: 该框架为物理信息机器学习封闭模型提供可扩展途径，能实现跨流态泛化且无需大量预计算训练数据集。

Abstract: Deep learning (DL) has demonstrated promise for accelerating and enhancing
the accuracy of flow physics simulations, but progress is constrained by the
scarcity of high-fidelity training data, which is costly to generate and
inherently limited to a small set of flow conditions. Consequently, closures
trained in the conventional offline paradigm tend to overfit and fail to
generalise to new regimes. We introduce an online optimisation framework for
DL-based Reynolds-averaged Navier--Stokes (RANS) closures which seeks to
address the challenge of limited high-fidelity datasets. Training data is
dynamically generated by embedding a direct numerical simulation (DNS) within a
subdomain of the RANS domain. The RANS solution supplies boundary conditions to
the DNS, while the DNS provides mean velocity and turbulence statistics that
are used to update a DL closure model during the simulation. This feedback loop
enables the closure to adapt to the embedded DNS target flow, avoiding reliance
on precomputed datasets and improving out-of-distribution performance. The
approach is demonstrated for the stochastically forced Burgers equation and for
turbulent channel flow at $Re_\tau=180$, $270$, $395$ and $590$ with varying
embedded domain lengths $1\leq L_0/L\leq 8$. Online-optimised RANS models
significantly outperform both offline-trained and literature-calibrated
closures, with accurate training achieved using modest DNS subdomains.
Performance degrades primarily when boundary-condition contamination dominates
or when domains are too short to capture low-wavenumber modes. This framework
provides a scalable route to physics-informed machine learning closures,
enabling data-adaptive reduced-order models that generalise across flow regimes
without requiring large precomputed training datasets.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [314] [The land use-climate change-biodiversity nexus in European islands stakeholders](https://arxiv.org/abs/2510.02829)
*Aristides Moustakas,Irene Christoforidi,George Zittis,Nazli Demirel,Mauro Fois,Savvas Zotos,Eirini Gallou,Valentini Stamatiadou,Elli Tzirkalli,Christos Zoumides,Kristina Košić,Aikaterini Christopoulou,Aleksandra Dragin,Damian Łowicki,Artur Gil,Bruna Almeida,Panos Chrysos,Mario V. Balzan,Mark D. C. Mansoldo,Rannveig Ólafsdóttir,Cigdem Kaptan Ayhan,Lutfi Atay,Mirela Tase,Vladimir Stojanović,Maja Mijatov Ladičorbić,Juan Pedro Díaz,Francisco Javier Expósito,Sonia Quiroga,Miguel Ángel Casquet Cano,Haoran Wang,Cristina Suárez,Paraskevi Manolaki,Ioannis N. Vogiatzakis*

Main category: q-bio.PE

TL;DR: 研究咨询欧洲21个岛屿利益相关者对气候和土地利用变化看法，用机器学习分析影响，指出主要特征、优先问题及影响感知，强调需管理措施。


<details>
  <summary>Details</summary>
Motivation: 为促进气候适应和减缓，了解利益相关者对土地利用和气候变化的观点及知识差距。

Method: 咨询21个欧洲岛屿利益相关者，用机器学习分析气候和土地利用变化影响感知。

Result: 主要气候特征是温度，土地利用特征是森林砍伐；水相关问题是首要优先事项；能源相关问题风险高；利益相关者认为气候变化和土地利用变化对生态系统服务影响为负。

Conclusion: 利益相关者对生物多样性影响有共同看法，区分气候和土地利用影响，水、能源和可再生能源问题需管理措施。

Abstract: To promote climate adaptation and mitigation, it is crucial to understand
stakeholder perspectives and knowledge gaps on land use and climate changes.
Stakeholders across 21 European islands were consulted on climate and land use
change issues affecting ecosystem services. Climate change perceptions included
temperature, precipitation, humidity, extremes, and wind. Land use change
perceptions included deforestation, coastal degradation, habitat protection,
renewable energy facilities, wetlands, and others. Additional concerns such as
invasive species, water or energy scarcity, infrastructure problems, and
austerity were also considered. Climate and land use change impact perceptions
were analysed with machine learning to quantify their influence. The
predominant climatic characteristic is temperature, and the predominant land
use characteristic is deforestation. Water-related problems are top priorities
for stakeholders. Energy-related problems, including energy deficiency and
issues with wind and solar facilities, rank high as combined climate and land
use risks. Stakeholders generally perceive climate change impacts on ecosystem
services as negative, with natural habitat destruction and biodiversity loss
identified as top issues. Land use change impacts are also negative but more
complex, with more explanatory variables. Stakeholders share common perceptions
on biodiversity impacts despite geographic disparity, but they differentiate
between climate and land use impacts. Water, energy, and renewable energy
issues pose serious concerns, requiring management measures.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [315] [The Equilibrium Response of Atmospheric Machine-Learning Models to Uniform Sea Surface Temperature Warming](https://arxiv.org/abs/2510.02415)
*Bosong Zhang,Timothy M. Merlis*

Main category: physics.ao-ph

TL;DR: 评估多个ML模型对海表温度均匀变暖的气候响应，指出其对气候变化应用有前景但存在局限。


<details>
  <summary>Details</summary>
Motivation: 现有ML模型在训练分布外的泛化能力未知，需评估其对气候变化的响应。

Method: 评估多个先进ML模型（ACE2 - ERA5、NeuralGCM和cBottle）对海表温度均匀变暖的响应，对比物理环流模型（GFDL's AM4）进行关键诊断。

Result: ML模型能重现物理模型响应的关键方面，如降水响应，但在辐射响应和陆地变暖等方面与物理响应有显著差异。

Conclusion: ML模型用于气候变化应用有前景，但需进一步改进以实现稳健的样本外泛化。

Abstract: Machine learning models for the global atmosphere that are capable of
producing stable, multi-year simulations of Earth's climate have recently been
developed. However, the ability of these ML models to generalize beyond the
training distribution remains an open question. In this study, we evaluate the
climate response of several state-of-the-art ML models (ACE2-ERA5, NeuralGCM,
and cBottle) to a uniform sea surface temperature warming, a widely used
benchmark for evaluating climate change. We assess each ML model's performance
relative to a physics-based general circulation model (GFDL's AM4) across key
diagnostics, including surface air temperature, precipitation, temperature and
wind profiles, and top-of-the-atmosphere radiation. While the ML models
reproduce key aspects of the physical model response, particularly the response
of precipitation, some exhibit notable departures from robust physical
responses, including radiative responses and land region warming. Our results
highlight the promise and current limitations of ML models for climate change
applications and suggest that further improvements are needed for robust
out-of-sample generalization.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [316] [Sensors in viticulture: functions, benefits, and data-driven insights](https://arxiv.org/abs/2510.03000)
*Milan Milenkovic*

Main category: cs.CY

TL;DR: 本文简述葡萄栽培中传感器数据平台的功能、益处和实际考量，对葡萄栽培者及农业和物联网研究者有参考价值。


<details>
  <summary>Details</summary>
Motivation: 展示传感器及相关分析预测在为葡萄栽培者决策提供数据支持方面的作用，强调其对葡萄园运营管理的重要性。

Method: 无明确提及方法。

Result: 传感器数据平台能提供实时测量、预测和警报，指导葡萄园操作，增强运营有效性和效率，实现精准葡萄栽培原则。

Conclusion: 传感器数据平台在葡萄栽培中有重要作用，值得葡萄栽培者和相关研究者关注。

Abstract: Use of sensor and related analytical predictions can be a powerful tool in
providing data-informed input to viticulturalists' decision process,
complementing their vineyard observations and intuition. Their up-to-date
measurements, predictions, and alerts offer actionable insights and suggestions
for managing key vineyard operations, such as irrigation, disease and pest
control, canopy management, and harvest timing. In many cases, anticipatory
interventions can mitigate risks before problems become apparent. By offering
guidance on the targeting, timing, and dosage of vineyard practices, sensor
data platforms can enhance operational effectiveness and efficiency while
conserving labor and resources when they are not required. They also enable
implementation of the principles of precision viticulture - doing the right
thing, at the right time, in the right place. This paper provides a succinct
summary of the functions, benefits, and practical considerations of sensor data
platforms in viticulture. It may be of interest to viticulturalists as well as
agricultural and IoT researchers.

</details>


### [317] [PHORECAST: Enabling AI Understanding of Public Health Outreach Across Populations](https://arxiv.org/abs/2510.02535)
*Rifaa Qadri,Anh Nhat Nhu,Swati Ramnath,Laura Yu Zheng,Raj Bhansali,Sylvette La Touche-Howard,Tracy Marie Zeeger,Tom Goldstein,Ming Lin*

Main category: cs.CY

TL;DR: 本文介绍多模态数据集PHORECAST，用于研究个体和社区对健康信息的响应，以推动公共健康领域AI发展。


<details>
  <summary>Details</summary>
Motivation: 当前大视觉语言模型在模拟人类对说服性信息的细微、异质响应方面研究不足，缺乏全面的多模态数据集，需推动个性化和社会感知的机器学习。

Method: 引入多模态数据集PHORECAST，支持多模态理解、响应预测等任务。

Result: 该数据集可严格评估现代AI系统模拟、解释和预测异质公众情绪和行为的能力。

Conclusion: PHORECAST旨在促进开发更具社会意识且符合适应性和包容性健康传播目标的模型。

Abstract: Understanding how diverse individuals and communities respond to persuasive
messaging holds significant potential for advancing personalized and socially
aware machine learning. While Large Vision and Language Models (VLMs) offer
promise, their ability to emulate nuanced, heterogeneous human responses,
particularly in high stakes domains like public health, remains underexplored
due in part to the lack of comprehensive, multimodal dataset. We introduce
PHORECAST (Public Health Outreach REceptivity and CAmpaign Signal Tracking), a
multimodal dataset curated to enable fine-grained prediction of both
individuallevel behavioral responses and community-wide engagement patterns to
health messaging. This dataset supports tasks in multimodal understanding,
response prediction, personalization, and social forecasting, allowing rigorous
evaluation of how well modern AI systems can emulate, interpret, and anticipate
heterogeneous public sentiment and behavior. By providing a new dataset to
enable AI advances for public health, PHORECAST aims to catalyze the
development of models that are not only more socially aware but also aligned
with the goals of adaptive and inclusive health communication

</details>


### [318] [Representing Beauty: Towards a Participatory but Objective Latent Aesthetics](https://arxiv.org/abs/2510.02869)
*Alexander Michael Rusnak*

Main category: cs.CY

TL;DR: 本文探讨神经网络表征美的能力，发现美有现实主义基础，表明人机共创不仅可能且是基础。


<details>
  <summary>Details</summary>
Motivation: 探究机器识别美的含义以及神经网络表征美的能力。

Method: 借鉴跨模型表征收敛的研究，对比美感和非美感图像在不同模型间的表征。

Result: 美感内容在不同模型间产生更相似和对齐的表征，非美感图像则不然。

Conclusion: 美的形式结构有现实主义基础，人机共创以美为目的吸引子，是文化生产和机器感知的基础。

Abstract: What does it mean for a machine to recognize beauty? While beauty remains a
culturally and experientially compelling but philosophically elusive concept,
deep learning systems increasingly appear capable of modeling aesthetic
judgment. In this paper, we explore the capacity of neural networks to
represent beauty despite the immense formal diversity of objects for which the
term applies. By drawing on recent work on cross-model representational
convergence, we show how aesthetic content produces more similar and aligned
representations between models which have been trained on distinct data and
modalities - while unaesthetic images do not produce more aligned
representations. This finding implies that the formal structure of beautiful
images has a realist basis - rather than only as a reflection of socially
constructed values. Furthermore, we propose that these realist representations
exist because of a joint grounding of aesthetic form in physical and cultural
substance. We argue that human perceptual and creative acts play a central role
in shaping these the latent spaces of deep learning systems, but that a realist
basis for aesthetics shows that machines are not mere creative parrots but can
produce novel creative insights from the unique vantage point of scale. Our
findings suggest that human-machine co-creation is not merely possible, but
foundational - with beauty serving as a teleological attractor in both cultural
production and machine perception.

</details>


### [319] [Corrosion Risk Estimation for Heritage Preservation: An Internet of Things and Machine Learning Approach Using Temperature and Humidity](https://arxiv.org/abs/2510.02973)
*Reginald Juan M. Mercado,Muhammad Kabeer,Haider Al-Obaidy,Rosdiadee Nordin*

Main category: cs.CY

TL;DR: 研究开发物联网硬件系统监测钢构建筑腐蚀，构建机器学习框架预测腐蚀率，通过仪表盘提供实时监测和建议，该方法可扩展且成本低。


<details>
  <summary>Details</summary>
Motivation: 对菲律宾圣塞巴斯蒂安大教堂等文化遗产地的钢结构进行主动保护需要准确的腐蚀预测。

Method: 开发连接LoRa无线通信的物联网硬件系统监测建筑，利用三年数据集构建仅基于温度和相对湿度数据的机器学习框架预测腐蚀率，并通过仪表盘为公众提供访问。

Result: 框架能提供实时腐蚀监测和可操作的保护建议。

Conclusion: 这种少数据方法可扩展且成本低，先进回归能从基本气象数据中提取准确腐蚀预测，无需大量传感器网络就能实现全球文化重要结构的主动保护。

Abstract: Proactive preservation of steel structures at culturally significant heritage
sites like the San Sebastian Basilica in the Philippines requires accurate
corrosion forecasting. This study developed an Internet of Things hardware
system connected with LoRa wireless communications to monitor heritage
buildings with steel structures. From a three year dataset generated by the IoT
system, we built a machine learning framework for predicting atmospheric
corrosion rates using only temperature and relative humidity data. Deployed via
a Streamlit dashboard with ngrok tunneling for public access, the framework
provides real-time corrosion monitoring and actionable preservation
recommendations. This minimal-data approach is scalable and cost effective for
heritage sites with limited monitoring resources, showing that advanced
regression can extract accurate corrosion predictions from basic meteorological
data enabling proactive preservation of culturally significant structures
worldwide without requiring extensive sensor networks

</details>


### [320] [AI Generated Child Sexual Abuse Material -- What's the Harm?](https://arxiv.org/abs/2510.02978)
*Caoilte Ó Ciardha,John Buckley,Rebecca S. Portnoff*

Main category: cs.CY

TL;DR: 本文探讨生成式AI产生的儿童性虐待材料（AI CSAM）带来的挑战，分析其危害并警示将其视为减少危害工具的观点。


<details>
  <summary>Details</summary>
Motivation: 鉴于生成式AI产生的AI CSAM对儿童保护、执法和社会应对儿童剥削带来挑战，且部分人认为其无害，故分析其危害。

Method: 提供关键技术介绍，批判性审视AI CSAM相关危害。

Result: AI CSAM会导致未受虐待儿童被合成相关材料、幸存者二次受害、助长不良行为及降低犯罪门槛等。

Conclusion: 应警惕将AI CSAM视为减少危害工具的观点，其所谓无害的说法掩盖了真实风险，可能导致应对迟缓。

Abstract: The development of generative artificial intelligence (AI) tools capable of
producing wholly or partially synthetic child sexual abuse material (AI CSAM)
presents profound challenges for child protection, law enforcement, and
societal responses to child exploitation. While some argue that the harmfulness
of AI CSAM differs fundamentally from other CSAM due to a perceived absence of
direct victimization, this perspective fails to account for the range of risks
associated with its production and consumption. AI has been implicated in the
creation of synthetic CSAM of children who have not previously been abused, the
revictimization of known survivors of abuse, the facilitation of grooming,
coercion and sexual extortion, and the normalization of child sexual
exploitation. Additionally, AI CSAM may serve as a new or enhanced pathway into
offending by lowering barriers to engagement, desensitizing users to
progressively extreme content, and undermining protective factors for
individuals with a sexual interest in children. This paper provides a primer on
some key technologies, critically examines the harms associated with AI CSAM,
and cautions against claims that it may function as a harm reduction tool,
emphasizing how some appeals to harmlessness obscure its real risks and may
contribute to inertia in ecosystem responses.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [321] [Hyperparameters are all you need: Using five-step inference for an original diffusion model to generate images comparable to the latest distillation model](https://arxiv.org/abs/2510.02390)
*Zilai Li*

Main category: cs.GR

TL;DR: 本文提出无训练算法，8步生成高质量512x512和1024x1024图像，FID性能佳，还验证了5步和6步推理效果。


<details>
  <summary>Details</summary>
Motivation: 基于对扩散ODE和SDE截断误差的分析，期望找到高效生成高质量图像的方法。

Method: 分析扩散ODE和SDE截断误差，提出无训练算法进行图像生成。

Result: 8步生成1024x1024图像FID性能与最新蒸馏模型相当；8步生成512x512图像FID性能优于DPM++ 2m 20步推理结果；5步推理FID性能与多种模型相当；6步合成1024x1024图像FID与最新蒸馏算法差距有限。

Conclusion: 所提无训练算法在少步数图像生成上表现良好，具有高效性和高质量的特点。

Abstract: The diffusion model is a state-of-the-art generative model that generates an
image by applying a neural network iteratively. Moreover, this generation
process is regarded as an algorithm solving an ordinary differential equation
or a stochastic differential equation. Based on the analysis of the truncation
error of the diffusion ODE and SDE, our study proposes a training-free
algorithm that generates high-quality 512 x 512 and 1024 x 1024 images in eight
steps, with flexible guidance scales. To the best of my knowledge, our
algorithm is the first one that samples a 1024 x 1024 resolution image in 8
steps with an FID performance comparable to that of the latest distillation
model, but without additional training. Meanwhile, our algorithm can also
generate a 512 x 512 image in 8 steps, and its FID performance is better than
the inference result using state-of-the-art ODE solver DPM++ 2m in 20 steps. We
validate our eight-step image generation algorithm using the COCO 2014, COCO
2017, and LAION datasets. And our best FID performance is 15.7, 22.35, and
17.52. While the FID performance of DPM++2m is 17.3, 23.75, and 17.33. Further,
it also outperforms the state-of-the-art AMED-plugin solver, whose FID
performance is 19.07, 25.50, and 18.06. We also apply the algorithm in
five-step inference without additional training, for which the best FID
performance in the datasets mentioned above is 19.18, 23.24, and 19.61,
respectively, and is comparable to the performance of the state-of-the-art AMED
Pulgin solver in eight steps, SDXL-turbo in four steps, and the
state-of-the-art diffusion distillation model Flash Diffusion in five steps. We
also validate our algorithm in synthesizing 1024 * 1024 images within 6 steps,
whose FID performance only has a limited distance to the latest distillation
algorithm. The code is in repo:
https://github.com/TheLovesOfLadyPurple/Hyperparameters-are-all-you-need

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [322] [SAE-RNA: A Sparse Autoencoder Model for Interpreting RNA Language Model Representations](https://arxiv.org/abs/2510.02734)
*Taehan Kim,Sangdae Nam*

Main category: q-bio.BM

TL;DR: 提出SAE - RNA解释性模型分析RiNALMo表示并映射到生物特征，用于探索RNA语言模型编码信息。


<details>
  <summary>Details</summary>
Motivation: 当前不清楚RNA语言模型对mRNA或ncRNA家族内部编码的内容，需开展相关分析。

Method: 将RNA解释性问题构建为预训练嵌入中的概念发现问题，不进行端到端再训练。

Result: 提出SAE - RNA模型，能分析RiNALMo表示并映射到生物特征。

Conclusion: 该模型可用于探索RNA语言模型对ncRNA家族的编码信息，还能扩展用于RNA组比较和支持假设生成。

Abstract: Deep learning, particularly with the advancement of Large Language Models,
has transformed biomolecular modeling, with protein advances (e.g., ESM)
inspiring emerging RNA language models such as RiNALMo. Yet how and what these
RNA Language Models internally encode about messenger RNA (mRNA) or non-coding
RNA (ncRNA) families remains unclear. We present SAE- RNA, interpretability
model that analyzes RiNALMo representations and maps them to known human-level
biological features. Our work frames RNA interpretability as concept discovery
in pretrained embeddings, without end-to-end retraining, and provides practical
tools to probe what RNA LMs may encode about ncRNA families. The model can be
extended to close comparisons between RNA groups, and supporting hypothesis
generation about previously unrecognized relationships.

</details>


### [323] [FLOWR.root: A flow matching based foundation model for joint multi-purpose structure-aware 3D ligand generation and affinity prediction](https://arxiv.org/abs/2510.02578)
*Julian Cremer,Tuan Le,Mohammad M. Ghahremanpour,Emilia Sługocka,Filipe Menezes,Djork-Arné Clevert*

Main category: q-bio.BM

TL;DR: 介绍Flowr.root模型用于口袋感知3D配体生成、结合亲和力预测和置信度估计，性能优异，为基于结构的药物设计提供基础。


<details>
  <summary>Details</summary>
Motivation: 开发能进行口袋感知3D配体生成、联合结合亲和力预测和置信度估计的模型，用于基于结构的药物设计。

Method: 结合大规模配体库和混合保真度的蛋白质 - 配体复合物进行训练，在精选共晶数据集上细化，并进行参数高效微调。

Result: 在无条件3D分子生成和口袋条件配体设计中达到先进水平，亲和力预测模块准确性高、速度快，案例研究显示预测与实验数据相关性强。

Conclusion: Flowr.root通过整合结构感知生成、亲和力估计和属性引导采样，为基于结构的药物设计提供全面基础。

Abstract: We present Flowr.root, an equivariant flow-matching model for pocket-aware 3D
ligand generation with joint binding affinity prediction and confidence
estimation. The model supports de novo generation, pharmacophore-conditional
sampling, fragment elaboration, and multi-endpoint affinity prediction (pIC50,
pKi, pKd, pEC50). Training combines large-scale ligand libraries with
mixed-fidelity protein-ligand complexes, followed by refinement on curated
co-crystal datasets and parameter-efficient finetuning for project-specific
adaptation. Flowr.root achieves state-of-the-art performance in unconditional
3D molecule generation and pocket-conditional ligand design, producing
geometrically realistic, low-strain structures. The integrated affinity
prediction module demonstrates superior accuracy on the SPINDR test set and
outperforms recent models on the Schrodinger FEP+/OpenFE benchmark with
substantial speed advantages. As a foundation model, Flowr.root requires
finetuning on project-specific datasets to account for unseen
structure-activity landscapes, yielding strong correlation with experimental
data. Joint generation and affinity prediction enable inference-time scaling
through importance sampling, steering molecular design toward higher-affinity
compounds. Case studies validate this: selective CK2alpha ligand generation
against CLK3 shows significant correlation between predicted and
quantum-mechanical binding energies, while ERalpha and TYK2 scaffold
elaboration demonstrates strong agreement with QM calculations. By integrating
structure-aware generation, affinity estimation, and property-guided sampling,
Flowr.root provides a comprehensive foundation for structure-based drug design
spanning hit identification through lead optimization.

</details>
