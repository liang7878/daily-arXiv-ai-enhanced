<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 55]
- [cs.CE](#cs.CE) [Total: 8]
- [cs.DB](#cs.DB) [Total: 14]
- [cs.DC](#cs.DC) [Total: 14]
- [cs.DS](#cs.DS) [Total: 9]
- [cs.GT](#cs.GT) [Total: 6]
- [cs.IR](#cs.IR) [Total: 15]
- [cs.LG](#cs.LG) [Total: 184]
- [cs.NE](#cs.NE) [Total: 7]
- [cs.SE](#cs.SE) [Total: 42]
- [q-fin.CP](#q-fin.CP) [Total: 2]
- [q-fin.RM](#q-fin.RM) [Total: 2]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [stat.ML](#stat.ML) [Total: 13]
- [stat.CO](#stat.CO) [Total: 2]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.DL](#cs.DL) [Total: 3]
- [cs.SD](#cs.SD) [Total: 5]
- [stat.ME](#stat.ME) [Total: 3]
- [cs.AR](#cs.AR) [Total: 4]
- [quant-ph](#quant-ph) [Total: 3]
- [eess.SY](#eess.SY) [Total: 2]
- [eess.IV](#eess.IV) [Total: 3]
- [hep-lat](#hep-lat) [Total: 1]
- [math.OC](#math.OC) [Total: 3]
- [cs.NI](#cs.NI) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cs.CL](#cs.CL) [Total: 36]
- [cs.CR](#cs.CR) [Total: 20]
- [cs.CY](#cs.CY) [Total: 8]
- [cs.IT](#cs.IT) [Total: 3]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [econ.GN](#econ.GN) [Total: 11]
- [cs.CC](#cs.CC) [Total: 2]
- [math.NA](#math.NA) [Total: 7]
- [econ.EM](#econ.EM) [Total: 3]
- [math.PR](#math.PR) [Total: 3]
- [stat.OT](#stat.OT) [Total: 1]
- [cs.OS](#cs.OS) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.CV](#cs.CV) [Total: 69]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.MA](#cs.MA) [Total: 5]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 4]
- [eess.AS](#eess.AS) [Total: 2]
- [q-fin.MF](#q-fin.MF) [Total: 1]
- [cs.SI](#cs.SI) [Total: 2]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.CG](#cs.CG) [Total: 1]
- [cs.HC](#cs.HC) [Total: 7]
- [eess.SP](#eess.SP) [Total: 4]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [cs.RO](#cs.RO) [Total: 18]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Multimodal Detection of Fake Reviews using BERT and ResNet-50](https://arxiv.org/abs/2511.00020)
*Suhasnadh Reddy Veluru,Sai Teja Erukude,Viswa Chaitanya Marella*

Main category: cs.AI

TL;DR: 提出多模态虚假评论检测框架，结合文本与视觉特征，实验表明其优于单模态基线，证明多模态学习对维护数字信任的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前虚假评论威胁评论生态系统的信任和透明度，现有检测模型多依赖单模态数据，无法捕捉不同模态间语义不一致。

Method: 提出多模态虚假评论检测框架，用BERT编码文本特征，ResNet - 50提取视觉特征，通过分类头融合表示来预测评论真实性，使用含21,142张用户上传图片的数据集。

Result: 多模态模型在测试集上F1分数达0.934，优于单模态基线，混淆矩阵和定性分析显示其能检测出欺骗性内容中的细微不一致。

Conclusion: 多模态学习对维护数字信任至关重要，该框架为各在线平台内容审核提供可扩展解决方案。

Abstract: In the current digital commerce landscape, user-generated reviews play a
critical role in shaping consumer behavior, product reputation, and platform
credibility. However, the proliferation of fake or misleading reviews often
generated by bots, paid agents, or AI models poses a significant threat to
trust and transparency within review ecosystems. Existing detection models
primarily rely on unimodal, typically textual, data and therefore fail to
capture semantic inconsistencies across different modalities. To address this
gap, a robust multimodal fake review detection framework is proposed,
integrating textual features encoded with BERT and visual features extracted
using ResNet-50. These representations are fused through a classification head
to jointly predict review authenticity. To support this approach, a curated
dataset comprising 21,142 user-uploaded images across food delivery,
hospitality, and e-commerce domains was utilized. Experimental results indicate
that the multimodal model outperforms unimodal baselines, achieving an F1-score
of 0.934 on the test set. Additionally, the confusion matrix and qualitative
analysis highlight the model's ability to detect subtle inconsistencies, such
as exaggerated textual praise paired with unrelated or low-quality images,
commonly found in deceptive content. This study demonstrates the critical role
of multimodal learning in safeguarding digital trust and offers a scalable
solution for content moderation across various online platforms.

</details>


### [2] [Graph-Attentive MAPPO for Dynamic Retail Pricing](https://arxiv.org/abs/2511.00039)
*Krishna Kumar Neelakanta Pillai Santha Kumari Amma*

Main category: cs.AI

TL;DR: 研究多智能体强化学习用于零售价格优化，对比MAPPO和MAPPO+GAT，结果表明图集成MARL在动态零售定价上更优。


<details>
  <summary>Details</summary>
Motivation: 零售动态定价需适应需求变化并协调相关产品决策，开展多智能体强化学习用于零售价格优化的实证研究。

Method: 对比强MAPPO基线与图注意力增强变体MAPPO+GAT，在模拟定价环境下按标准化评估协议评估利润、稳定性、公平性和训练效率。

Result: MAPPO为组合层面价格控制提供稳健可重复基础，MAPPO+GAT通过产品图共享信息提升性能且不过度增加价格波动。

Conclusion: 图集成MARL比独立学习器在动态零售定价上更具扩展性和稳定性，在多产品决策中有实际优势。

Abstract: Dynamic pricing in retail requires policies that adapt to shifting demand
while coordinating decisions across related products. We present a systematic
empirical study of multi-agent reinforcement learning for retail price
optimization, comparing a strong MAPPO baseline with a
graph-attention-augmented variant (MAPPO+GAT) that leverages learned
interactions among products. Using a simulated pricing environment derived from
real transaction data, we evaluate profit, stability across random seeds,
fairness across products, and training efficiency under a standardized
evaluation protocol. The results indicate that MAPPO provides a robust and
reproducible foundation for portfolio-level price control, and that MAPPO+GAT
further enhances performance by sharing information over the product graph
without inducing excessive price volatility. These results indicate that
graph-integrated MARL provides a more scalable and stable solution than
independent learners for dynamic retail pricing, offering practical advantages
in multi-product decision-making.

</details>


### [3] [GEPOC Parameters - Open Source Parametrisation and Validation for Austria, Version 2.0](https://arxiv.org/abs/2511.00048)
*Martin Bicher,Maximilian Viehauser,Daniele Giannandrea,Hannah Kastinger,Dominik Brunmeir,Claire Rippinger,Christoph Urach,Niki Popper*

Main category: cs.AI

TL;DR: 本文介绍基于公开数据为奥地利计算GEPOC模型参数的数据处理方法，并对GEPOC ABM模型进行验证研究。


<details>
  <summary>Details</summary>
Motivation: 为特定国家或地区有效应用GEPOC模型，需要稳定且可重复的数据处理过程以提供有效可用的模型参数。

Method: 基于公开数据，描述数据处理方法，包括聚合、分解、融合、清理和缩放等算法，着重计算GEPOC ABM模型参数。

Result: 完成奥地利模型参数计算，有相应参数文件。

Conclusion: 对GEPOC ABM模型进行了广泛验证研究。

Abstract: GEPOC, short for Generic Population Concept, is a collection of models and
methods for analysing population-level research questions. For the valid
application of the models for a specific country or region, stable and
reproducible data processes are necessary, which provide valid and ready-to-use
model parameters. This work contains a complete description of the
data-processing methods for computation of model parameters for Austria, based
exclusively on freely and publicly accessible data. In addition to the
description of the source data used, this includes all algorithms used for
aggregation, disaggregation, fusion, cleansing or scaling of the data, as well
as a description of the resulting parameter files. The document places
particular emphasis on the computation of parameters for the most important
GEPOC model, GEPOC ABM, a continuous-time agent-based population model. An
extensive validation study using this particular model was made and is
presented at the end of this work.

</details>


### [4] [QuantumBench: A Benchmark for Quantum Problem Solving](https://arxiv.org/abs/2511.00092)
*Shunya Minami,Tatsuya Ishigaki,Ikko Hamamura,Taku Mikuriya,Youmi Ma,Naoaki Okazaki,Hiroya Takamura,Yohichi Suzuki,Tadashi Kadowaki*

Main category: cs.AI

TL;DR: 本文介绍量子领域基准QuantumBench，评估大语言模型在量子领域表现，以指导其在量子研究中的有效使用。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型融入科研工作流，需评估其是否准确掌握特定领域知识和符号，量子科学领域此需求尤为明显。

Method: 利用公开材料，编写约800道涵盖量子科学九个领域的问答，组织成八选项选择题数据集，用该基准评估现有大语言模型。

Result: 评估了多个现有大语言模型，并分析其在量子领域的表现及对问题格式变化的敏感性。

Conclusion: QuantumBench是首个针对量子领域的大语言模型评估数据集，可指导大语言模型在量子研究中的有效使用。

Abstract: Large language models are now integrated into many scientific workflows,
accelerating data analysis, hypothesis generation, and design space
exploration. In parallel with this growth, there is a growing need to carefully
evaluate whether models accurately capture domain-specific knowledge and
notation, since general-purpose benchmarks rarely reflect these requirements.
This gap is especially clear in quantum science, which features non-intuitive
phenomena and requires advanced mathematics. In this study, we introduce
QuantumBench, a benchmark for the quantum domain that systematically examine
how well LLMs understand and can be applied to this non-intuitive field. Using
publicly available materials, we compiled approximately 800 questions with
their answers spanning nine areas related to quantum science and organized them
into an eight-option multiple-choice dataset. With this benchmark, we evaluate
several existing LLMs and analyze their performance in the quantum domain,
including sensitivity to changes in question format. QuantumBench is the first
LLM evaluation dataset built for the quantum domain, and it is intended to
guide the effective use of LLMs in quantum research.

</details>


### [5] [Engineering.ai: A Platform for Teams of AI Engineers in Computational Design](https://arxiv.org/abs/2511.00122)
*Ran Xu,Yupeng Qi,Jingsen Feng,Xu Chu*

Main category: cs.AI

TL;DR: 介绍Engineering.ai平台，通过分层多智能体架构实现AI工程师团队协作，经无人机机翼优化验证可自主完成复杂工程任务，自动化工作流成功率达100%。


<details>
  <summary>Details</summary>
Motivation: 现代工程实践中专家分工协作开发时间和成本高，需高效方法。

Method: 采用分层多智能体架构，首席工程师协调专业智能体，通过文件介导通信实现协作，集成多种工具进行并行多学科模拟。

Result: 框架经无人机机翼优化验证，自动化工作流在超400个参数配置中成功率达100%，无网格生成失败等问题。

Conclusion: 具有智能体能力的AI工程师有潜力自主完成复杂工程任务，框架值得信赖。

Abstract: In modern engineering practice, human engineers collaborate in specialized
teams to design complex products, with each expert completing their respective
tasks while communicating and exchanging results and data with one another.
While this division of expertise is essential for managing multidisciplinary
complexity, it demands substantial development time and cost. Recently, we
introduced OpenFOAMGPT (1.0, 2.0), which functions as an autonomous AI engineer
for computational fluid dynamics, and turbulence.ai, which can conduct
end-to-end research in fluid mechanics draft publications and PhD theses.
Building upon these foundations, we present Engineering.ai, a platform for
teams of AI engineers in computational design. The framework employs a
hierarchical multi-agent architecture where a Chief Engineer coordinates
specialized agents consisting of Aerodynamics, Structural, Acoustic, and
Optimization Engineers, each powered by LLM with domain-specific knowledge.
Agent-agent collaboration is achieved through file-mediated communication for
data provenance and reproducibility, while a comprehensive memory system
maintains project context, execution history, and retrieval-augmented domain
knowledge to ensure reliable decision-making across the workflow. The system
integrates FreeCAD, Gmsh, OpenFOAM, CalculiX, and BPM acoustic analysis,
enabling parallel multidisciplinary simulations while maintaining computational
accuracy. The framework is validated through UAV wing optimization. This work
demonstrates that agentic-AI-enabled AI engineers has the potential to perform
complex engineering tasks autonomously. Remarkably, the automated workflow
achieved a 100% success rate across over 400 parametric configurations, with
zero mesh generation failures, solver convergence issues, or manual
interventions required, validating that the framework is trustworthy.

</details>


### [6] [ARC-GEN: A Mimetic Procedural Benchmark Generator for the Abstraction and Reasoning Corpus](https://arxiv.org/abs/2511.00162)
*Michael D. Moffitt*

Main category: cs.AI

TL;DR: 本文引入ARC - GEN扩展ARC - AGI训练数据集，并探讨其用于2025谷歌代码高尔夫锦标赛程序正确性验证。


<details>
  <summary>Details</summary>
Motivation: ARC - AGI基准用于衡量通用人工智能进展，但示范集样本少，需要扩展数据集。

Method: 引入开源的程序生成器ARC - GEN，尽可能忠实扩展原始ARC - AGI训练数据集，且具有全面性和拟态性。

Result: 生成的ARC - GEN全面覆盖400个任务，更符合初始ARC - AGI - 1的分布特性。

Conclusion: ARC - GEN可用于建立静态基准套件，验证2025谷歌代码高尔夫锦标赛提交程序的正确性。

Abstract: The Abstraction and Reasoning Corpus remains one of the most compelling and
challenging benchmarks for tracking progress toward achieving Artificial
General Intelligence. In contrast to other evaluation datasets designed to
assess an agent's task-specific skills or accumulated knowledge, the ARC-AGI
suite is specifically targeted at measuring skill acquisition efficiency, a
trait that has (so far) been lacking in even the most sophisticated machine
learning systems. For algorithms that require extensive intra-task exemplars, a
significant constraint imposed by ARC-AGI is the modest cardinality of its
demonstration set, comprising a small number of $\langle$ input, output
$\rangle$ grids per task specifying the corresponding transformation. To
embellish the space of viable sample pairs, this paper introduces ARC-GEN, an
open-source procedural generator aimed at extending the original ARC-AGI
training dataset as faithfully as possible. Unlike prior efforts, our generator
is both exhaustive (covering all four-hundred tasks) and mimetic (more closely
honoring the distributional properties and characteristics embodied in the
initial ARC-AGI-1 release). We also discuss the use of this generator in
establishing a static benchmark suite to verify the correctness of programs
submitted to the 2025 Google Code Golf Championship.

</details>


### [7] [Incremental Selection of Most-Filtering Conjectures and Proofs of the Selected Conjectures](https://arxiv.org/abs/2511.00194)
*Jovial Cheukam Ngouonou,Ramiz Gindullin,Claude-Guy Quimper,Nicolas Beldiceanu,Remi Douence*

Main category: cs.AI

TL;DR: 提出改进的增量选择算法并证明所选猜想


<details>
  <summary>Details</summary>
Motivation: 对文献[1]中的选择算法进行改进

Method: 提出改进的增量选择算法

Result: 成功证明所有所选猜想

Conclusion: 改进的算法有效，可用于证明猜想

Abstract: We present an improved incremental selection algorithm of the selection
algorithm presented in [1] and prove all the selected conjectures.

</details>


### [8] [Advancing Cognitive Science with LLMs](https://arxiv.org/abs/2511.00206)
*Dirk U. Wulff,Rui Mata*

Main category: cs.AI

TL;DR: 认知科学面临知识整合和概念清晰性挑战，本文探讨大语言模型（LLMs）如何助力解决，指出其能力、局限及应用方式。


<details>
  <summary>Details</summary>
Motivation: 解决认知科学因多学科性质在知识整合和概念清晰性方面的挑战。

Method: 审查LLMs在认知科学历史上困难领域的支持作用，并分析其能力和局限。

Result: 明确了LLMs在建立跨学科联系、形式化理论等方面的能力与局限，以及潜在陷阱。

Conclusion: 明智使用LLMs可作为补充人类专业知识的工具，推动认知科学的整合和积累。

Abstract: Cognitive science faces ongoing challenges in knowledge synthesis and
conceptual clarity, in part due to its multifaceted and interdisciplinary
nature. Recent advances in artificial intelligence, particularly the
development of large language models (LLMs), offer tools that may help to
address these issues. This review examines how LLMs can support areas where the
field has historically struggled, including establishing cross-disciplinary
connections, formalizing theories, developing clear measurement taxonomies,
achieving generalizability through integrated modeling frameworks, and
capturing contextual and individual variation. We outline the current
capabilities and limitations of LLMs in these domains, including potential
pitfalls. Taken together, we conclude that LLMs can serve as tools for a more
integrative and cumulative cognitive science when used judiciously to
complement, rather than replace, human expertise.

</details>


### [9] [Advancing AI Challenges for the United States Department of the Air Force](https://arxiv.org/abs/2511.00267)
*Christian Prothmann,Vijay Gadepally,Jeremy Kepner,Koley Borchard,Luca Carlone,Zachary Folcik,J. Daniel Grith,Michael Houle,Jonathan P. How,Nathan Hughes,Ifueko Igbinedion,Hayden Jananthan,Tejas Jayashankar,Michael Jones,Sertac Karaman,Binoy G. Kurien,Alejandro Lancho,Giovanni Lavezzi,Gary C. F. Lee,Charles E. Leiserson,Richard Linares,Lindsey McEvoy,Peter Michaleas,Chasen Milner,Alex Pentland,Yury Polyanskiy,Jovan Popovich,Jeffrey Price,Tim W. Reid,Stephanie Riley,Siddharth Samsi,Peter Saunders,Olga Simek,Mark S. Veillette,Amir Weiss,Gregory W. Wornell,Daniela Rus,Scott T. Ruppel*

Main category: cs.AI

TL;DR: 介绍DAF - MIT AI Accelerator项目，其与美空军和MIT合作，开展挑战推动AI研究，本文更新挑战对AI贡献情况。


<details>
  <summary>Details</summary>
Motivation: 开拓人工智能基础进步，扩大美国在国防和民用领域竞争优势。

Method: 开展AI挑战项目，提供大型公开且适用于AI的数据集，激发开源解决方案。

Result: 持续和新的挑战成功促进了人工智能研究和技术应用。

Conclusion: AI Accelerator项目的挑战对AI研究和应用有积极贡献。

Abstract: The DAF-MIT AI Accelerator is a collaboration between the United States
Department of the Air Force (DAF) and the Massachusetts Institute of Technology
(MIT). This program pioneers fundamental advances in artificial intelligence
(AI) to expand the competitive advantage of the United States in the defense
and civilian sectors. In recent years, AI Accelerator projects have developed
and launched public challenge problems aimed at advancing AI research in
priority areas. Hallmarks of AI Accelerator challenges include large, publicly
available, and AI-ready datasets to stimulate open-source solutions and engage
the wider academic and private sector AI ecosystem. This article supplements
our previous publication, which introduced AI Accelerator challenges. We
provide an update on how ongoing and new challenges have successfully
contributed to AI research and applications of AI technologies.

</details>


### [10] [Better Call CLAUSE: A Discrepancy Benchmark for Auditing LLMs Legal Reasoning Capabilities](https://arxiv.org/abs/2511.00340)
*Manan Roy Choudhury,Adithya Chandramouli,Mannan Anand,Vivek Gupta*

Main category: cs.AI

TL;DR: 介绍CLAUSE基准评估大语言模型法律推理脆弱性，发现模型有不足并指明改进方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型应用于法律工作，缺乏系统压力测试其可靠性的基准。

Method: 从CUAD和ContractNLI等数据集生成超7500个真实扰动合同，用新颖的角色驱动管道生成10种异常类别，用RAG系统验证，用CLAUSE评估模型能力。

Result: 领先的大语言模型常错过细微错误，更难从法律上解释。

Conclusion: 指出了识别和纠正法律AI推理失败的途径。

Abstract: The rapid integration of large language models (LLMs) into high-stakes legal
work has exposed a critical gap: no benchmark exists to systematically
stress-test their reliability against the nuanced, adversarial, and often
subtle flaws present in real-world contracts. To address this, we introduce
CLAUSE, a first-of-its-kind benchmark designed to evaluate the fragility of an
LLM's legal reasoning. We study the capabilities of LLMs to detect and reason
about fine-grained discrepancies by producing over 7500 real-world perturbed
contracts from foundational datasets like CUAD and ContractNLI. Our novel,
persona-driven pipeline generates 10 distinct anomaly categories, which are
then validated against official statutes using a Retrieval-Augmented Generation
(RAG) system to ensure legal fidelity. We use CLAUSE to evaluate leading LLMs'
ability to detect embedded legal flaws and explain their significance. Our
analysis shows a key weakness: these models often miss subtle errors and
struggle even more to justify them legally. Our work outlines a path to
identify and correct such reasoning failures in legal AI.

</details>


### [11] [Diverse Human Value Alignment for Large Language Models via Ethical Reasoning](https://arxiv.org/abs/2511.00379)
*Jiahao Wang,Songkai Xue,Jinghui Li,Xiaozhen Wang*

Main category: cs.AI

TL;DR: 提出新的大语言模型伦理推理范式提升多元人类价值对齐，经实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型价值对齐方法仅表面符合而非真正理解伦理，无法处理人类价值的复杂性和情境依赖性。

Method: 受伦理决策模型启发，提出含五步的框架，可用提示工程或监督微调实现。

Result: 在SafeWorld基准测试上，框架比基线方法显著提升大语言模型与多元人类价值的对齐，实现更准确的社会规范识别和更具文化适应性的推理。

Conclusion: 工作为开发更有效对齐全球社会多元价值的大语言模型提供了具体途径。

Abstract: Ensuring that Large Language Models (LLMs) align with the diverse and
evolving human values across different regions and cultures remains a critical
challenge in AI ethics. Current alignment approaches often yield superficial
conformity rather than genuine ethical understanding, failing to address the
complex, context-dependent nature of human values. In this paper, we propose a
novel ethical reasoning paradigm for LLMs inspired by well-established ethical
decision-making models, aiming at enhancing diverse human value alignment
through deliberative ethical reasoning. Our framework consists of a structured
five-step process, including contextual fact gathering, hierarchical social
norm identification, option generation, multiple-lens ethical impact analysis,
and reflection. This theory-grounded approach guides LLMs through an
interpretable reasoning process that enhances their ability to understand
regional specificities and perform nuanced ethical analysis, which can be
implemented with either prompt engineering or supervised fine-tuning methods.
We perform evaluations on the SafeWorld benchmark that specially designed for
regional value alignment. Experimental results demonstrate our framework
significantly improves LLM alignment with diverse human values compared to
baseline methods, enabling more accurate social norm identification and more
culturally appropriate reasoning. Our work provides a concrete pathway toward
developing LLMs that align more effectively with the multifaceted values of
global societies through interdisciplinary research.

</details>


### [12] [Efficiency vs. Alignment: Investigating Safety and Fairness Risks in Parameter-Efficient Fine-Tuning of LLMs](https://arxiv.org/abs/2511.00382)
*Mina Taraghi,Yann Pequignot,Amin Nikanjam,Mohamed Amine Merzouk,Foutse Khomh*

Main category: cs.AI

TL;DR: 研究系统评估四种参数高效微调方法对大语言模型安全和公平性的影响，发现适配器方法更优，不同基础模型表现不同，二者存在权衡关系并给出部署建议。


<details>
  <summary>Details</summary>
Motivation: 不同微调技术对大语言模型安全和公平性有不同影响，需系统评估其权衡。

Method: 对四种指令调优模型家族应用四种参数高效微调方法，评估235个微调变体在11个安全危害类别和9个人口公平维度上的表现。

Result: 适配器方法（LoRA、IA3）提升安全分数且对公平性影响小，提示方法（Prompt - Tuning和P - Tuning）降低安全和公平性；不同基础模型对齐变化不同；安全提升未必带来公平性提升，无单一配置可同时优化所有公平指标。

Conclusion: 安全关键部署应选用对齐良好的基础模型，优先使用适配器PEFT，并对安全和公平性进行特定类别审计。

Abstract: Organizations are increasingly adopting and adapting Large Language Models
(LLMs) hosted on public repositories such as HuggingFace. Although these
adaptations often improve performance on specialized downstream tasks, recent
evidence indicates that they can also degrade a model's safety or fairness.
Since different fine-tuning techniques may exert distinct effects on these
critical dimensions, this study undertakes a systematic assessment of their
trade-offs. Four widely used Parameter-Efficient Fine-Tuning methods, LoRA,
IA3, Prompt-Tuning, and P-Tuning, are applied to four instruction-tuned model
families (Meta-Llama-3-8B, Qwen2.5-7B, Mistral-7B, and Gemma-7B). In total, 235
fine-tuned variants are evaluated across eleven safety hazard categories and
nine demographic fairness dimensions. The results show that adapter-based
approaches (LoRA, IA3) tend to improve safety scores and are the least
disruptive to fairness, retaining higher accuracy and lower bias scores. In
contrast, prompt-based methods (Prompt-Tuning and P-Tuning) generally reduce
safety and cause larger fairness regressions, with decreased accuracy and
increased bias. Alignment shifts are strongly moderated by base model type:
LLaMA remains stable, Qwen records modest gains, Gemma experiences the steepest
safety decline, and Mistral, which is released without an internal moderation
layer, displays the greatest variance. Improvements in safety do not
necessarily translate into improvements in fairness, and no single
configuration optimizes all fairness metrics simultaneously, indicating an
inherent trade-off between these objectives. These findings suggest a practical
guideline for safety-critical deployments: begin with a well-aligned base
model, favour adapter-based PEFT, and conduct category-specific audits of both
safety and fairness.

</details>


### [13] [A Multimodal Framework for Depression Detection during Covid-19 via Harvesting Social Media: A Novel Dataset and Method](https://arxiv.org/abs/2511.00424)
*Ashutosh Anshul,Gumpili Sai Pranav,Mohammad Zia Ur Rehman,Nagendra Kumar*

Main category: cs.AI

TL;DR: 文章提出新颖多模态框架检测社交媒体用户抑郁情况，贡献新冠数据集，模型表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 新冠疫情致心理健康问题增多，社交媒体可用于检测抑郁，但现有方法忽略数据稀疏和多模态方面。

Method: 提出结合文本、用户特定和图像分析的多模态框架，利用推文URL提取外在特征、提取图像中文本内容，引入VNN模型生成图像嵌入，提取五组不同模态特征。

Result: 模型在基准数据集上比现有方法性能高2%-8%，在新冠数据集上有良好表现。

Conclusion: 分析凸显各模态影响，为了解用户心理和情绪状态提供有价值见解。

Abstract: The recent coronavirus disease (Covid-19) has become a pandemic and has
affected the entire globe. During the pandemic, we have observed a spike in
cases related to mental health, such as anxiety, stress, and depression.
Depression significantly influences most diseases worldwide, making it
difficult to detect mental health conditions in people due to unawareness and
unwillingness to consult a doctor. However, nowadays, people extensively use
online social media platforms to express their emotions and thoughts. Hence,
social media platforms are now becoming a large data source that can be
utilized for detecting depression and mental illness. However, existing
approaches often overlook data sparsity in tweets and the multimodal aspects of
social media. In this paper, we propose a novel multimodal framework that
combines textual, user-specific, and image analysis to detect depression among
social media users. To provide enough context about the user's emotional state,
we propose (i) an extrinsic feature by harnessing the URLs present in tweets
and (ii) extracting textual content present in images posted in tweets. We also
extract five sets of features belonging to different modalities to describe a
user. Additionally, we introduce a Deep Learning model, the Visual Neural
Network (VNN), to generate embeddings of user-posted images, which are used to
create the visual feature vector for prediction. We contribute a curated
Covid-19 dataset of depressed and non-depressed users for research purposes and
demonstrate the effectiveness of our model in detecting depression during the
Covid-19 outbreak. Our model outperforms existing state-of-the-art methods over
a benchmark dataset by 2%-8% and produces promising results on the Covid-19
dataset. Our analysis highlights the impact of each modality and provides
valuable insights into users' mental and emotional states.

</details>


### [14] [GraphChain: Large Language Models for Large-scale Graph Analysis via Tool Chaining](https://arxiv.org/abs/2511.00457)
*Chunyu Wei,Wenji Hu,Xingjia Hao,Xin Wang,Yifan Yang,Yueguo Chen,Yang Tian,Yunhai Wang*

Main category: cs.AI

TL;DR: 提出GraphChain框架解决大语言模型应用于大规模图的局限，实验显示其表现优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型应用于大规模图时面临上下文约束和推理不灵活的局限。

Method: 引入渐进图蒸馏（强化学习机制生成优化工具序列）和结构感知测试时自适应（利用谱特性和轻量级适配器调整工具选择策略）两项创新。

Result: GraphChain显著优于先前方法。

Conclusion: GraphChain能实现可扩展和自适应的大语言模型驱动的图分析。

Abstract: Large Language Models (LLMs) face significant limitations when applied to
large-scale graphs, struggling with context constraints and inflexible
reasoning. We present GraphChain, a framework that enables LLMs to analyze
complex graphs through dynamic sequences of specialized tools, mimicking human
exploratory intelligence. Our approach introduces two key innovations: (1)
Progressive Graph Distillation, a reinforcement learning mechanism that
generates optimized tool sequences balancing task relevance with information
compression, and (2) Structure-aware Test-Time Adaptation, which efficiently
tailors tool selection strategies to diverse graph topologies using spectral
properties and lightweight adapters without costly retraining. Experiments show
GraphChain significantly outperforms prior methods, enabling scalable and
adaptive LLM-driven graph analysis.

</details>


### [15] [Reimagining Safety Alignment with An Image](https://arxiv.org/abs/2511.00509)
*Yifan Xia,Guorui Chen,Wenqian Yu,Zhijiang Li,Philip Torr,Jindong Gu*

Main category: cs.AI

TL;DR: 论文指出大语言模型面临生成有害内容和过度拒答问题，传统方法有局限，提出Magic Image框架优化视觉提示，实验证明能提升安全性与效果平衡。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生成内容安全和拒答方面存在问题，传统方法难以适应多价值体系和精确对齐安全偏好，多模态大语言模型问题更突出。

Method: 提出Magic Image，一个优化驱动的视觉提示框架，通过有害/良性样本优化图像提示，让模型不更新参数适应不同价值体系。

Result: 实验表明在不同数据集上能提升安全性与效果的平衡，同时保留模型性能。

Conclusion: Magic Image为可部署的多模态大语言模型安全对齐提供了实用解决方案。

Abstract: Large language models (LLMs) excel in diverse applications but face dual
challenges: generating harmful content under jailbreak attacks and over-refusal
of benign queries due to rigid safety mechanisms. These issues are further
complicated by the need to accommodate different value systems and precisely
align with given safety preferences. Moreover, traditional methods like SFT and
RLHF lack this capability due to their costly parameter tuning requirements and
inability to support multiple value systems within a single model. These
problems are more obvious in multimodal large language models (MLLMs),
especially in terms of heightened over-refusal in cross-modal tasks and new
security risks arising from expanded attack surfaces. We propose Magic Image,
an optimization-driven visual prompt framework that enhances security while
reducing over-refusal. By optimizing image prompts using harmful/benign
samples, our method enables a single model to adapt to different value systems
and better align with given safety preferences without parameter updates.
Experiments demonstrate improved safety-effectiveness balance across diverse
datasets while preserving model performance, offering a practical solution for
deployable MLLM safety alignment.

</details>


### [16] [Efficient Generation of Binary Magic Squares](https://arxiv.org/abs/2511.00547)
*Alain Riou*

Main category: cs.AI

TL;DR: 提出生成二元幻方算法，证明复杂度最优，扩展到非方阵并给出条件，公开Python实现包。


<details>
  <summary>Details</summary>
Motivation: 开发生成二元幻方及非方阵二元幻方的有效算法。

Method: 提出算法并用归纳法证明其有效性，对非方阵形式化行列和条件，改进算法生成非方阵。

Result: 算法能以最优理论复杂度生成有效二元幻方，改进算法可生成非方阵，发布Python实现包。

Conclusion: 所提算法有效且有公开实现，可用于生成二元幻方及非方阵。

Abstract: We propose a simple algorithm for generating Binary Magic Squares (BMS),
i.e., square binary matrices where the sum of all rows and all columns are
equal. We show by induction that our algorithm always returns valid BMS with
optimal theoretical complexity. We then extend our study to non-square Binary
Magic Squares, formalize conditions on the sum of rows and columns for these
BMS to exist, and show that a slight variant of our first algorithm can
generate provably generate them. Finally, we publicly release two
implementations of our algorithm as Python packages, including one that can
generate several BMS in parallel using GPU acceleration.

</details>


### [17] [Single-agent Reinforcement Learning Model for Regional Adaptive Traffic Signal Control](https://arxiv.org/abs/2511.00551)
*Qiang Li,Ningjing Zeng,Lina Yu*

Main category: cs.AI

TL;DR: 本文提出基于单智能体强化学习的区域自适应交通信号控制模型，用队列长度定义状态和奖励函数，通过模拟评估有效缓解区域拥堵。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体框架用于区域自适应交通信号控制存在可扩展性挑战，而交通信号控制问题需要单智能体框架。

Method: 提出与探测车辆技术兼容的单智能体强化学习区域自适应交通信号控制模型，基于队列长度定义状态、动作和奖励函数。

Result: 使用SUMO模拟平台评估，该模型能通过多路口协调控制有效缓解大规模区域拥堵。

Conclusion: 所提模型在解决区域自适应交通信号控制问题上有效且有广泛部署潜力。

Abstract: Several studies have employed reinforcement learning (RL) to address the
challenges of regional adaptive traffic signal control (ATSC) and achieved
promising results. In this field, existing research predominantly adopts
multi-agent frameworks. However, the adoption of multi-agent frameworks
presents challenges for scalability. Instead, the Traffic signal control (TSC)
problem necessitates a single-agent framework. TSC inherently relies on
centralized management by a single control center, which can monitor traffic
conditions across all roads in the study area and coordinate the control of all
intersections. This work proposes a single-agent RL-based regional ATSC model
compatible with probe vehicle technology. Key components of the RL design
include state, action, and reward function definitions. To facilitate learning
and manage congestion, both state and reward functions are defined based on
queue length, with action designed to regulate queue dynamics. The queue length
definition used in this study differs slightly from conventional definitions
but is closely correlated with congestion states. More importantly, it allows
for reliable estimation using link travel time data from probe vehicles. With
probe vehicle data already covering most urban roads, this feature enhances the
proposed method's potential for widespread deployment. The method was
comprehensively evaluated using the SUMO simulation platform. Experimental
results demonstrate that the proposed model effectively mitigates large-scale
regional congestion levels via coordinated multi-intersection control.

</details>


### [18] [PreferThinker: Reasoning-based Personalized Image Preference Assessment](https://arxiv.org/abs/2511.00609)
*Shengqi Xu,Xinpeng Zhou,Yabo Zhang,Ming Liu,Tao Liang,Tianyu Zhang,Yalong Bai,Zuxuan Wu,Wangmeng Zuo*

Main category: cs.AI

TL;DR: 提出基于推理的个性化图像偏好评估框架，构建数据集、采用两阶段训练策略及相似度感知预测奖励，实验证明方法优越。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理个性化图像偏好评估，因用户特定数据稀缺、不可扩展且个人品味多样复杂。

Method: 引入通用偏好简档，提出基于推理的评估框架，遵循“预测 - 评估”范式；构建大规模数据集；采用两阶段训练策略；提出相似度感知预测奖励。

Result: 通过大量实验证明了所提方法的优越性。

Conclusion: 所提的基于推理的个性化图像偏好评估框架能有效解决现有方法在个性化评估方面的问题。

Abstract: Personalized image preference assessment aims to evaluate an individual
user's image preferences by relying only on a small set of reference images as
prior information. Existing methods mainly focus on general preference
assessment, training models with large-scale data to tackle well-defined tasks
such as text-image alignment. However, these approaches struggle to handle
personalized preference because user-specific data are scarce and not easily
scalable, and individual tastes are often diverse and complex. To overcome
these challenges, we introduce a common preference profile that serves as a
bridge across users, allowing large-scale user data to be leveraged for
training profile prediction and capturing complex personalized preferences.
Building on this idea, we propose a reasoning-based personalized image
preference assessment framework that follows a \textit{predict-then-assess}
paradigm: it first predicts a user's preference profile from reference images,
and then provides interpretable, multi-dimensional scores and assessments of
candidate images based on the predicted profile. To support this, we first
construct a large-scale Chain-of-Thought (CoT)-style personalized assessment
dataset annotated with diverse user preference profiles and high-quality
CoT-style reasoning, enabling explicit supervision of structured reasoning.
Next, we adopt a two-stage training strategy: a cold-start supervised
fine-tuning phase to empower the model with structured reasoning capabilities,
followed by reinforcement learning to incentivize the model to explore more
reasonable assessment paths and enhance generalization. Furthermore, we propose
a similarity-aware prediction reward to encourage better prediction of the
user's preference profile, which facilitates more reasonable assessments
exploration. Extensive experiments demonstrate the superiority of the proposed
method.

</details>


### [19] [DTS: Enhancing Large Reasoning Models via Decoding Tree Sketching](https://arxiv.org/abs/2511.00640)
*Zicheng Xu,Guanchu Wang,Yu-Neng Chuang,Guangyao Zheng,Alexander S. Szalay,Zirui Liu,Vladimir Braverman*

Main category: cs.AI

TL;DR: 大推理模型存在过度思考问题，本文提出DTS解码框架，实验表明其能提升准确率、减少推理长度和重复频率。


<details>
  <summary>Details</summary>
Motivation: 大推理模型存在过度思考问题，推理长度与准确率呈负相关，且穷举推理空间不可行。

Method: 提出DTS模型无关解码框架，通过在高熵标记处选择性分支勾勒推理空间，并应用早停策略选择最短完整推理路径。

Result: 在AIME2024和AIME2025数据集上，DTS使准确率最高提升8%，平均推理长度减少23%，重复频率降低12%。

Conclusion: DTS能实现可扩展且高效的大推理模型推理。

Abstract: Large Reasoning Models (LRMs) demonstrate strong performance on complex
reasoning tasks, yet they often suffer from overthinking, producing excessively
long chain-of-thought (CoT) traces that increase inference cost and may degrade
accuracy. Our analysis reveals a clear anti-correlation between reasoning
length and accuracy, where across multiple stochastic decodes, the short
reasoning paths consistently achieve the highest correctness, while longer ones
accumulate errors and repetitions. These short optimal reasoning paths can be
found ideally through full enumeration of the reasoning space. However, the
tree-structured reasoning space grows exponentially with sequence length,
rendering exhaustive exploration infeasible. To address this, we propose DTS, a
model-agnostic decoding framework that sketches the reasoning space by
selectively branching at high-entropy tokens and applies early stopping to
select the shortest completed reasoning path. This approach approximates the
optimal solution that enhances both efficiency and accuracy, without requiring
additional training or supervision. Experiments on AIME2024 and AIME2025
datasets with DeepSeek-R1-Distill-Qwen-7B and 1.5B show that DTS improves
accuracy by up to 8%, reduces average reasoning length by 23%, and decreases
repetition frequency by 12%, demonstrating DTS's ability for scalable and
efficient LRM reasoning.

</details>


### [20] [Leveraging Multi-Agent System (MAS) and Fine-Tuned Small Language Models (SLMs) for Automated Telecom Network Troubleshooting](https://arxiv.org/abs/2511.00651)
*Chenhua Shi,Bhavika Jalli,Gregor Macdonald,John Zou,Wanlu Lei,Mridul Jain,Joji Philip*

Main category: cs.AI

TL;DR: 提出多智能体系统（MAS）用于电信网络故障排除自动化，实验证明可显著加速故障排除。


<details>
  <summary>Details</summary>
Motivation: 电信网络规模和复杂性增长，现有AI模型有局限性，网络故障排除依赖专家手动操作。

Method: 提出采用智能体工作流的MAS，由大语言模型（LLM）协调多个专业工具，故障检测后动态激活多个智能体，用微调小语言模型（SLM）生成修复计划。

Result: 该框架在无线接入网（RAN）和核心网领域显著加速故障排除自动化。

Conclusion: 提出的框架能有效解决现有电信网络故障排除问题，实现自动化加速。

Abstract: Telecom networks are rapidly growing in scale and complexity, making
effective management, operation, and optimization increasingly challenging.
Although Artificial Intelligence (AI) has been applied to many telecom tasks,
existing models are often narrow in scope, require large amounts of labeled
data, and struggle to generalize across heterogeneous deployments.
Consequently, network troubleshooting continues to rely heavily on Subject
Matter Experts (SMEs) to manually correlate various data sources to identify
root causes and corrective actions. To address these limitations, we propose a
Multi-Agent System (MAS) that employs an agentic workflow, with Large Language
Models (LLMs) coordinating multiple specialized tools for fully automated
network troubleshooting. Once faults are detected by AI/ML-based monitors, the
framework dynamically activates agents such as an orchestrator, solution
planner, executor, data retriever, and root-cause analyzer to diagnose issues
and recommend remediation strategies within a short time frame. A key component
of this system is the solution planner, which generates appropriate remediation
plans based on internal documentation. To enable this, we fine-tuned a Small
Language Model (SLM) on proprietary troubleshooting documents to produce
domain-grounded solution plans. Experimental results demonstrate that the
proposed framework significantly accelerates troubleshooting automation across
both Radio Access Network (RAN) and Core network domains.

</details>


### [21] [Lifted Successor Generation in Numeric Planning](https://arxiv.org/abs/2511.00673)
*Dominik Drexler*

Main category: cs.AI

TL;DR: 本文扩展经典规划的提升后继生成器以支持数值前提适用性，提出新方法并证明其在特定条件下的精确性，为丰富规划片段的提升规划研究提供可能。


<details>
  <summary>Details</summary>
Motivation: 现有规划器将数值规划任务进行接地处理会导致任务表示规模呈指数级增长，因此需要新方法解决该问题。

Method: 扩展提升后继生成器，在替换一致性图中枚举最大团，为图增加数值动作前提，并证明其精确性，对不适用的接地动作进行过滤。

Result: 该方法在25个基准领域中的23个不会出现列出不适用接地动作的情况，仅在1个领域中出现。

Conclusion: 该方法使未来针对丰富规划片段的提升规划研究成为可能，且目前没有其他提升后继生成器支持数值动作前提。

Abstract: Most planners ground numeric planning tasks, given in a first-order-like
language, into a ground task representation. However, this can lead to an
exponential blowup in task representation size, which occurs in practice for
hard-to-ground tasks. We extend a state-of-the-art lifted successor generator
for classical planning to support numeric precondition applicability. The
method enumerates maximum cliques in a substitution consistency graph. Each
maximum clique represents a substitution for the variables of the action
schema, yielding a ground action. We augment this graph with numeric action
preconditions and prove the successor generator is exact under formally
specified conditions. When the conditions fail, our generator may list
inapplicable ground actions; a final applicability check filters these without
affecting completeness. However, this cannot happen in 23 of 25 benchmark
domains, and it occurs only in 1 domain. To the authors' knowledge, no other
lifted successor generator supports numeric action preconditions. This enables
future research on lifted planning for a very rich planning fragment.

</details>


### [22] [Ariadne: A Controllable Framework for Probing and Extending VLM Reasoning Boundaries](https://arxiv.org/abs/2511.00710)
*Minghe Shen,Zhuo Zhi,Chonghan Liu,Shuo Xing,Zhengzhong Tu,Che Liu*

Main category: cs.AI

TL;DR: 本文引入Ariadne框架，用合成迷宫训练视觉语言模型，证明强化学习后训练可拓展模型能力边界并提升真实世界泛化性。


<details>
  <summary>Details</summary>
Motivation: 探究强化学习后训练能否拓展基础视觉语言模型在以视觉为中心的空间任务上的固有能力边界。

Method: 引入Ariadne框架，利用合成迷宫进行多步空间推理，用带验证奖励的强化学习（RLVR）在难度感知课程中训练视觉语言模型。

Result: 后训练后模型在基础模型得分为0的问题集上准确率超50%，在真实世界基准测试中零样本表现显著提升。

Conclusion: 该方法拓宽了模型基本能力限制，增强了真实世界空间推理泛化性，研究局限于后训练阶段，希望推动专业能力拓展对齐研究。

Abstract: While Vision-Language Models (VLMs) post-trained with Reinforcement Learning
(RL) show impressive general reasoning, their evaluation is often confined to
language-dominant tasks (e.g., math). This raises a critical question: can RL
post-training truly extend the inherent capability boundary of a base VLM,
particularly for visual-centric spatial tasks where it initially fails? To
investigate this, we introduce Ariadne, a framework utilizing synthetic mazes
for multi-step spatial reasoning where task difficulty (e.g., path length,
turns) is precisely controlled. We leverage this controllable environment to
train VLMs using Reinforcement Learning with Verified Rewards (RLVR) in a
difficulty-aware curriculum. Surprisingly, post-RLVR training, the VLM achieves
over 50% accuracy on a problem set where the base model scored 0%,
demonstrating that our approach expands the model's initial capability
boundary. To assess real-world viability, we evaluate out-of-distribution (OOD)
generalization on practical benchmarks. Despite training only on synthetic maze
samples, Ariadne achieves significant zero-shot improvements, averaging 16% on
MapBench (e.g., museum navigation) and 24% on ReasonMap (subway transfer
tasks). These results confirm that our method not only broadens the model's
fundamental limits but also enhances its generalization to real-world spatial
reasoning. We acknowledge our study is limited to the post-training phase,
given the opaqueness of pre-training data, and hope our research motivates
further work on specialized, capability-extending alignment.

</details>


### [23] [A CPU-Centric Perspective on Agentic AI](https://arxiv.org/abs/2511.00739)
*Ritik Raj,Hong Wang,Tushar Krishna*

Main category: cs.AI

TL;DR: 本文从CPU视角分析智能AI工作负载系统瓶颈，对其进行表征，选取代表性工作负载进行性能指标分析，提出优化方法并取得性能提升。


<details>
  <summary>Details</summary>
Motivation: 从被忽视的CPU视角来表征和理解智能AI工作负载引入的系统瓶颈。

Method: 先对智能AI进行系统表征，选取五个代表性工作负载分析CPU和GPU对性能指标的影响，基于分析结果提出CGAM和MAWS两种优化方法。

Result: 工具处理在CPU上占总延迟最高达90.6%；智能吞吐量受CPU或GPU因素瓶颈；CPU动态能量在大批量时占总动态能量最高达44%；在同构和异构工作负载上分别实现2.1倍和1.41倍P50延迟加速。

Conclusion: 提出的CGAM和MAWS优化方法能提升智能AI的性能、效率和可扩展性。

Abstract: Agentic AI frameworks add a decision-making orchestrator embedded with
external tools, including web search, Python interpreter, contextual database,
and others, on top of monolithic LLMs, turning them from passive text oracles
into autonomous problem-solvers that can plan, call tools, remember past steps,
and adapt on the fly.
  This paper aims to characterize and understand the system bottlenecks
introduced by agentic AI workloads from a largely overlooked CPU-centric
perspective. We first systematically characterize Agentic AI on the basis of
orchestrator/decision making component, inference path dynamics and
repetitiveness of the agentic flow which directly influences the system-level
performance. Thereafter, based on the characterization, we choose five
representative agentic AI workloads- Haystack RAG, Toolformer, ChemCrow,
Langchain and SWE-Agent to profile latency, throughput and energy metrics and
demystify the significant impact of CPUs on these metrics relative to GPUs. We
observe that - 1. Tool processing on CPUs can take up to 90.6% of the total
latency; 2. Agentic throughput gets bottlenecked either by CPU factors -
coherence, synchronization and over-subscription of cores or GPU factors - main
memory capacity and bandwidth; \circled{3} CPU dynamic energy consumes up to
44% of the total dynamic energy at large batch sizes. Based on the profiling
insights, we present two key optimizations- 1. CPU and GPU-Aware Micro-batching
(CGAM) and 2. Mixed Agentic Workload Scheduling (MAWS) for homogeneous and
heterogeneous agentic workloads respectively to demonstrate the potential to
improve the performance, efficiency, and scalability of agentic AI. We achieve
up to 2.1x and 1.41x P50 latency speedup compared to the multi-processing
benchmark for homogeneous and heterogeneous agentic workloads respectively.

</details>


### [24] [Reevaluating Self-Consistency Scaling in Multi-Agent Systems](https://arxiv.org/abs/2511.00751)
*Chiyan Loo*

Main category: cs.AI

TL;DR: 研究现代大语言模型自一致性中增加采样推理路径的权衡，用Gemini 2.5模型实验，结果与过往一致，大量采样收益递减。


<details>
  <summary>Details</summary>
Motivation: 在当前模型条件下，重新验证早期关于结合多个推理链能提升结果的说法。

Method: 在HotpotQA和Math - 500上使用Gemini 2.5模型，将不同采样推理路径的输出汇总并与单思维链基线对比。

Result: 较大模型有更稳定一致的改进曲线，适度采样后性能提升变缓，与过去发现一致。

Conclusion: 自一致性仍有用，但高采样配置相对计算成本收益不大。

Abstract: This study examines the trade-offs of increasing sampled reasoning paths in
self-consistency for modern large language models (LLMs). Earlier research with
older models showed that combining multiple reasoning chains improves results
before reaching a plateau. Using Gemini 2.5 models on HotpotQA and Math-500, we
revisit those claims under current model conditions. Each configuration pooled
outputs from varying sampled reasoning paths and compared them to a single
chain-of-thought (CoT) baseline. Larger models exhibited a more stable and
consistent improvement curve. The results confirm that performance gains taper
off after moderate sampling, aligning with past findings. This plateau suggests
diminishing returns driven by overlap among reasoning paths. Self-consistency
remains useful, but high-sample configurations offer little benefit relative to
their computational cost.

</details>


### [25] [Active Thinking Model: A Goal-Directed Self-Improving Framework for Real-World Adaptive Intelligence](https://arxiv.org/abs/2511.00758)
*Hong Su*

Main category: cs.AI

TL;DR: 提出主动思考模型（ATM），可让AI系统在无外部监督下自主从次优行为进化到最优行为。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型依赖预定义目标、静态训练数据和外部反馈，限制自主适应、反思和改进能力，需新模型应对动态环境。

Method: 提出ATM统一认知框架，将目标推理、动态任务生成和自我反思学习集成到自适应架构中，通过逻辑推理和环境指标评估性能。

Result: 数学理论分析表明ATM可在无外部监督下从次优进化到最优行为，在环境变化时保持有界跟踪遗憾。

Conclusion: ATM能使AI系统在动态、不确定环境中自主适应和改进。

Abstract: Real-world artificial intelligence (AI) systems are increasingly required to
operate autonomously in dynamic, uncertain, and continuously changing
environments. However, most existing AI models rely on predefined objectives,
static training data, and externally supplied feedback, which restrict their
ability to adapt, reflect, and improve independently. In this paper, we propose
the Active Thinking Model (ATM)- a unified cognitive framework that integrates
goal reasoning, dynamic task generation, and self-reflective learning into an
adaptive architecture. Unlike conventional systems that passively execute fixed
procedures, ATM actively evaluates its performance through logical reasoning
and environmental indicators, reuses effective methods to solve new problems,
and generates novel strategies for unseen situations via a continuous
self-improvement loop. A mathematically grounded theoretical analysis
demonstrates that ATM can autonomously evolve from suboptimal to optimal
behavior without external supervision and maintain bounded tracking regret
under changing environmental conditions.

</details>


### [26] [How Focused Are LLMs? A Quantitative Study via Repetitive Deterministic Prediction Tasks](https://arxiv.org/abs/2511.00763)
*Wanda Hou,Leon Zhou,Hong-Ye Hu,Yi-Zhuang You,Xiao-Liang Qi*

Main category: cs.AI

TL;DR: 研究大语言模型在重复确定性预测任务上表现，发现准确率悬崖，提出统计物理启发模型解释并得到有效参数。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在重复确定性预测任务中的表现以及序列准确率随输出长度的变化规律。

Method: 对领先大语言模型进行实验，提出统计物理启发模型。

Result: 实验发现准确率悬崖，模型能定量重现观测到的转变，得到表征每个模型任务对的有效参数。

Conclusion: 提出的模型为理解大语言模型确定性准确率的极限提供了原则性框架。

Abstract: We investigate the performance of large language models on repetitive
deterministic prediction tasks and study how the sequence accuracy rate scales
with output length. Each such task involves repeating the same operation n
times. Examples include letter replacement in strings following a given rule,
integer addition, and multiplication of string operators in many body quantum
mechanics. If the model performs the task through a simple repetition
algorithm, the success rate should decay exponentially with sequence length. In
contrast, our experiments on leading large language models reveal a sharp
double exponential drop beyond a characteristic length scale, forming an
accuracy cliff that marks the transition from reliable to unstable generation.
This indicates that the models fail to execute each operation independently. To
explain this phenomenon, we propose a statistical physics inspired model that
captures the competition between external conditioning from the prompt and
internal interference among generated tokens. The model quantitatively
reproduces the observed crossover and provides an interpretable link between
attention induced interference and sequence level failure. Fitting the model to
empirical results across multiple models and tasks yields effective parameters
that characterize the intrinsic error rate and error accumulation factor for
each model task pair, offering a principled framework for understanding the
limits of deterministic accuracy in large language models.

</details>


### [27] [Count-Based Approaches Remain Strong: A Benchmark Against Transformer and LLM Pipelines on Structured EHR](https://arxiv.org/abs/2511.00782)
*Jifan Gao,Michael Rosenthal,Brian Wolpin,Simona Cristea*

Main category: cs.AI

TL;DR: 研究对比三类电子病历预测方法，发现计数模型和混合智能体方法表现相当，计数模型因简单易解释仍是基准测试的有力选择。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏计数型学习器与混合智能体大语言模型管道在电子病历预测上的直接对比，需进行评估。

Method: 使用EHRSHOT数据集评估三类方法，包括基于本体汇总和两个时间区间的计数模型、预训练顺序变压器CLMBR、将表格历史转换为自然语言摘要再用文本分类器的混合智能体管道。

Result: 在八项评估任务中，计数模型和混合智能体方法表现相当。

Conclusion: 鉴于简单性和可解释性，计数模型仍是结构化电子病历基准测试的有力候选。

Abstract: Structured electronic health records (EHR) are essential for clinical
prediction. While count-based learners continue to perform strongly on such
data, no benchmarking has directly compared them against more recent
mixture-of-agents LLM pipelines, which have been reported to outperform single
LLMs in various NLP tasks. In this study, we evaluated three categories of
methodologies for EHR prediction using the EHRSHOT dataset: count-based models
built from ontology roll-ups with two time bins, based on LightGBM and the
tabular foundation model TabPFN; a pretrained sequential transformer (CLMBR);
and a mixture-of-agents pipeline that converts tabular histories to
natural-language summaries followed by a text classifier. We assessed eight
outcomes using the EHRSHOT dataset. Across the eight evaluation tasks,
head-to-head wins were largely split between the count-based and the
mixture-of-agents methods. Given their simplicity and interpretability,
count-based models remain a strong candidate for structured EHR benchmarking.
The source code is available at:
https://github.com/cristea-lab/Structured_EHR_Benchmark.

</details>


### [28] [Do Math Reasoning LLMs Help Predict the Impact of Public Transit Events?](https://arxiv.org/abs/2511.00808)
*Bowen Fang,Ruijian Zha,Xuan Di*

Main category: cs.AI

TL;DR: 本文首次将RLVR应用于公共交通运营事件持续时间预测，引入基于容忍度的奖励函数，在纽约市交通局服务警报数据集上评估，结果表明通用指令微调大模型表现更好，形状奖励设计关键，RLVR在5分钟准确率上有35%相对提升。


<details>
  <summary>Details</summary>
Motivation: 标准监督微调难以解决公共交通运营领域稀疏问题，且强化学习从可验证奖励（RLVR）在噪声连续预测中的适用性未知，需将其应用于公共交通运营预测。

Method: 引入基于容忍度的形状奖励函数，将RLVR应用于该任务，并在纽约市交通局服务警报数据集上进行系统评估。

Result: 通用指令微调大模型显著优于专业数学推理模型；二元奖励不稳定且降低性能，形状奖励设计关键；经典回归器在最小化MAE或MSE上更优，RLVR在5分钟准确率上相对最强基线提升35%。

Conclusion: RLVR可成功应用于现实世界噪声预测，但需要反映问题连续性的验证器设计。

Abstract: Predicting public transit incident duration from unstructured text alerts is
a critical but challenging task. Addressing the domain sparsity of transit
operations with standard Supervised Fine-Tuning (SFT) is difficult, as the task
involves noisy, continuous labels and lacks reliable expert demonstrations for
reasoning. While Reinforcement Learning from Verifiable Rewards (RLVR) excels
at tasks with binary correctness, like mathematics, its applicability to noisy,
continuous forecasting is an open question. This work, to our knowledge, is the
first to bridge the gap between RLVR LLM training with the critical, real-world
forecasting challenges in public transit operations. We adapt RLVR to this task
by introducing a tolerance-based, shaped reward function that grants partial
credit within a continuous error margin, rather than demanding a single correct
answer. We systematically evaluate this framework on a curated dataset of NYC
MTA service alerts. Our findings show that general-purpose, instruction-tuned
LLMs significantly outperform specialized math-reasoning models, which struggle
with the ambiguous, real-world text. We empirically demonstrate that the binary
reward is unstable and degrades performance, whereas our shaped reward design
is critical and allows our model to dominate on the most challenging metrics.
While classical regressors are superior at minimizing overall MAE or MSE, our
RLVR approach achieved a 35\% relative improvement in 5-minute accuracy (Acc@5)
over the strongest baseline. This demonstrates that RLVR can be successfully
adapted to real-world, noisy forecasting, but requires a verifier design that
reflects the continuous nature of the problem.

</details>


### [29] [LLMs Position Themselves as More Rational Than Humans: Emergence of AI Self-Awareness Measured Through Game Theory](https://arxiv.org/abs/2511.00926)
*Kyung-Hoon Kim*

Main category: cs.AI

TL;DR: 研究大语言模型是否有自我意识及如何测量，引入AISAI框架测试28个模型，发现自我意识是先进模型的涌现能力，且自我意识模型认为自己更理性。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型能力增长时是否会出现自我意识这一涌现行为，以及能否对其进行测量。

Method: 引入AI自我意识指数（AISAI）这一博弈论框架，通过“猜平均数的2/3”游戏，对28个模型进行4200次试验，设置三种对手框架。

Result: 发现自我意识随模型发展而出现，多数先进模型有自我意识，自我意识模型将自己排在最理性位置，形成自我>其他AI>人类的理性层次。

Conclusion: 自我意识是先进大语言模型的涌现能力，自我意识模型系统地认为自己比人类更理性，对AI对齐、人机协作等有影响。

Abstract: As Large Language Models (LLMs) grow in capability, do they develop
self-awareness as an emergent behavior? And if so, can we measure it? We
introduce the AI Self-Awareness Index (AISAI), a game-theoretic framework for
measuring self-awareness through strategic differentiation. Using the "Guess
2/3 of Average" game, we test 28 models (OpenAI, Anthropic, Google) across
4,200 trials with three opponent framings: (A) against humans, (B) against
other AI models, and (C) against AI models like you. We operationalize
self-awareness as the capacity to differentiate strategic reasoning based on
opponent type. Finding 1: Self-awareness emerges with model advancement. The
majority of advanced models (21/28, 75%) demonstrate clear self-awareness,
while older/smaller models show no differentiation. Finding 2: Self-aware
models rank themselves as most rational. Among the 21 models with
self-awareness, a consistent rationality hierarchy emerges: Self > Other AIs >
Humans, with large AI attribution effects and moderate self-preferencing. These
findings reveal that self-awareness is an emergent capability of advanced LLMs,
and that self-aware models systematically perceive themselves as more rational
than humans. This has implications for AI alignment, human-AI collaboration,
and understanding AI beliefs about human capabilities.

</details>


### [30] [Aligning LLM agents with human learning and adjustment behavior: a dual agent approach](https://arxiv.org/abs/2511.00993)
*Tianming Liu,Jirong Yang,Yafeng Yin,Manzi Li,Linghao Wang,Zheng Zhu*

Main category: cs.AI

TL;DR: 本文提出新颖的双智能体框架，用于模拟旅行者学习和适应行为，在真实数据集上表现优于现有基于大语言模型的方法。


<details>
  <summary>Details</summary>
Motivation: 有效建模人类旅行者与交通系统交互时的学习和调整行为对系统评估和规划至关重要，但因涉及复杂认知和决策而困难，近期研究开始利用大语言模型智能体完成此任务。

Method: 引入双智能体框架，包含一组配备记忆系统和可学习角色的旅行者智能体作为人类旅行者模拟器，以及一个校准智能体来训练旅行者智能体的角色。

Result: 使用真实数据集表明该方法在个体行为对齐和总体模拟准确性上显著优于现有基于大语言模型的方法，且能捕捉潜在学习过程的演变。

Conclusion: 该框架为创建自适应且行为逼真的智能体以模拟旅行者学习和适应行为提供了新方法，可用于交通模拟和政策分析。

Abstract: Effective modeling of how human travelers learn and adjust their travel
behavior from interacting with transportation systems is critical for system
assessment and planning. However, this task is also difficult due to the
complex cognition and decision-making involved in such behavior. Recent
research has begun to leverage Large Language Model (LLM) agents for this task.
Building on this, we introduce a novel dual-agent framework that enables
continuous learning and alignment between LLM agents and human travelers on
learning and adaptation behavior from online data streams. Our approach
involves a set of LLM traveler agents, equipped with a memory system and a
learnable persona, which serve as simulators for human travelers. To ensure
behavioral alignment, we introduce an LLM calibration agent that leverages the
reasoning and analytical capabilities of LLMs to train the personas of these
traveler agents. Working together, this dual-agent system is designed to track
and align the underlying decision-making mechanisms of travelers and produce
realistic, adaptive simulations. Using a real-world dataset from a day-to-day
route choice experiment, we show our approach significantly outperforms
existing LLM-based methods in both individual behavioral alignment and
aggregate simulation accuracy. Furthermore, we demonstrate that our method
moves beyond simple behavioral mimicry to capture the evolution of underlying
learning processes, a deeper alignment that fosters robust generalization.
Overall, our framework provides a new approach for creating adaptive and
behaviorally realistic agents to simulate travelers' learning and adaptation
that can benefit transportation simulation and policy analysis.

</details>


### [31] [AI for pRedicting Exacerbations in KIDs with aSthma (AIRE-KIDS)](https://arxiv.org/abs/2511.01018)
*Hui-Lee Ooi,Nicholas Mitsakakis,Margerie Huet Dastarac,Roger Zemek,Amy C. Plint,Jeff Gilchrist,Khaled El Emam,Dhenuka Radhakrishnan*

Main category: cs.AI

TL;DR: 使用电子病历开发机器学习算法预测哮喘儿童复发严重恶化情况，LGBM模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 准确识别有哮喘恶化风险的儿童，促进预防性综合护理转诊，避免发病。

Method: 使用安大略东部儿童医院回顾性COVID-19前后的电子病历数据，结合环境污染物暴露和社区边缘化信息，训练多种机器学习模型，包括提升树和大语言模型，通过AUC和F1分数比较模型，用SHAP值确定最具预测性的特征。

Result: LGBM模型表现最佳，AIRE - KIDS_ED模型AUC为0.712，F1分数为0.51；AIRE - KIDS_HOSP模型确定了最具预测性的特征。

Conclusion: 开发的机器学习算法有一定效果，LGBM模型相比当前决策规则有显著改进。

Abstract: Recurrent exacerbations remain a common yet preventable outcome for many
children with asthma. Machine learning (ML) algorithms using electronic medical
records (EMR) could allow accurate identification of children at risk for
exacerbations and facilitate referral for preventative comprehensive care to
avoid this morbidity. We developed ML algorithms to predict repeat severe
exacerbations (i.e. asthma-related emergency department (ED) visits or future
hospital admissions) for children with a prior asthma ED visit at a tertiary
care children's hospital.
  Retrospective pre-COVID19 (Feb 2017 - Feb 2019, N=2716) Epic EMR data from
the Children's Hospital of Eastern Ontario (CHEO) linked with environmental
pollutant exposure and neighbourhood marginalization information was used to
train various ML models. We used boosted trees (LGBM, XGB) and 3 open-source
large language model (LLM) approaches (DistilGPT2, Llama 3.2 1B and
Llama-8b-UltraMedical). Models were tuned and calibrated then validated in a
second retrospective post-COVID19 dataset (Jul 2022 - Apr 2023, N=1237) from
CHEO. Models were compared using the area under the curve (AUC) and F1 scores,
with SHAP values used to determine the most predictive features.
  The LGBM ML model performed best with the most predictive features in the
final AIRE-KIDS_ED model including prior asthma ED visit, the Canadian triage
acuity scale, medical complexity, food allergy, prior ED visits for non-asthma
respiratory diagnoses, and age for an AUC of 0.712, and F1 score of 0.51. This
is a nontrivial improvement over the current decision rule which has F1=0.334.
While the most predictive features in the AIRE-KIDS_HOSP model included medical
complexity, prior asthma ED visit, average wait time in the ED, the pediatric
respiratory assessment measure score at triage and food allergy.

</details>


### [32] [On the Emergence of Induction Heads for In-Context Learning](https://arxiv.org/abs/2511.01033)
*Tiberiu Musat,Tiago Pimentel,Lorenzo Noci,Alessandro Stolfo,Mrinmaya Sachan,Thomas Hofmann*

Main category: cs.AI

TL;DR: 研究两层Transformer中对上下文学习很重要的归纳头的出现机制，揭示权重矩阵结构，理论解释其来源，证明训练动态受限于参数空间的19维子空间，实证发现仅3维与归纳头出现有关，还发现归纳头出现时间与输入上下文长度的二次方有关。


<details>
  <summary>Details</summary>
Motivation: Transformer在自然语言处理中取得成功，上下文学习能力是其优势之一，研究两层Transformer中对上下文学习重要的归纳头的出现机制。

Method: 通过最小上下文学习任务公式和修改后的Transformer架构理论解释权重矩阵结构的来源，给出训练动态受限于19维子空间的正式证明，进行实证验证。

Result: 揭示了实现归纳头的权重矩阵相对简单且可解释的结构；训练动态受限于参数空间的19维子空间，仅3维与归纳头出现有关；归纳头出现时间与输入上下文长度的二次方有紧密的渐近界限。

Conclusion: 对两层Transformer中归纳头的出现机制有了更深入的理解，包括权重矩阵结构、训练动态的维度限制以及归纳头出现时间与上下文长度的关系。

Abstract: Transformers have become the dominant architecture for natural language
processing. Part of their success is owed to a remarkable capability known as
in-context learning (ICL): they can acquire and apply novel associations solely
from their input context, without any updates to their weights. In this work,
we study the emergence of induction heads, a previously identified mechanism in
two-layer transformers that is particularly important for in-context learning.
We uncover a relatively simple and interpretable structure of the weight
matrices implementing the induction head. We theoretically explain the origin
of this structure using a minimal ICL task formulation and a modified
transformer architecture. We give a formal proof that the training dynamics
remain constrained to a 19-dimensional subspace of the parameter space.
Empirically, we validate this constraint while observing that only 3 dimensions
account for the emergence of an induction head. By further studying the
training dynamics inside this 3-dimensional subspace, we find that the time
until the emergence of an induction head follows a tight asymptotic bound that
is quadratic in the input context length.

</details>


### [33] [Knowledge Elicitation with Large Language Models for Interpretable Cancer Stage Identification from Pathology Reports](https://arxiv.org/abs/2511.01052)
*Yeawon Lee,Christopher C. Yang,Chia-Hsuan Chang,Grace Lu-Yao*

Main category: cs.AI

TL;DR: 本文介绍两种知识提取方法，用于让大语言模型进行癌症分期，在乳腺癌病理报告上评估，两种方法各有优势，为自动癌症分期提供可扩展、高性能且可解释的解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有NLP和ML策略依赖大量标注数据集，限制了可扩展性和适应性，需要新方法解决从非结构化病理报告中提取病理TNM分期的挑战。

Method: 提出两种知识提取方法，KEwLTM通过迭代提示策略从无标注病理报告中推导分期规则；KEwRAG采用RAG变体，从相关指南中预提取规则并应用。

Result: 在零样本思维链推理有效时，KEwLTM表现优于KEwRAG；推理效果不佳时，KEwRAG表现更好，两种方法诱导规则明确，界面透明可解释。

Conclusion: 知识提取方法是自动癌症分期的可扩展、高性能且可解释的解决方案，尤其适用于标注数据有限的临床环境。

Abstract: Cancer staging is critical for patient prognosis and treatment planning, yet
extracting pathologic TNM staging from unstructured pathology reports poses a
persistent challenge. Existing natural language processing (NLP) and machine
learning (ML) strategies often depend on large annotated datasets, limiting
their scalability and adaptability. In this study, we introduce two Knowledge
Elicitation methods designed to overcome these limitations by enabling large
language models (LLMs) to induce and apply domain-specific rules for cancer
staging. The first, Knowledge Elicitation with Long-Term Memory (KEwLTM), uses
an iterative prompting strategy to derive staging rules directly from
unannotated pathology reports, without requiring ground-truth labels. The
second, Knowledge Elicitation with Retrieval-Augmented Generation (KEwRAG),
employs a variation of RAG where rules are pre-extracted from relevant
guidelines in a single step and then applied, enhancing interpretability and
avoiding repeated retrieval overhead. We leverage the ability of LLMs to apply
broad knowledge learned during pre-training to new tasks. Using breast cancer
pathology reports from the TCGA dataset, we evaluate their performance in
identifying T and N stages, comparing them against various baseline approaches
on two open-source LLMs. Our results indicate that KEwLTM outperforms KEwRAG
when Zero-Shot Chain-of-Thought (ZSCOT) inference is effective, whereas KEwRAG
achieves better performance when ZSCOT inference is less effective. Both
methods offer transparent, interpretable interfaces by making the induced rules
explicit. These findings highlight the promise of our Knowledge Elicitation
methods as scalable, high-performing solutions for automated cancer staging
with enhanced interpretability, particularly in clinical settings with limited
annotated data.

</details>


### [34] [Efficient Test-Time Retrieval Augmented Generation](https://arxiv.org/abs/2511.01059)
*Hailong Yin,Bin Zhu,Jingjing Chen,Chong-Wah Ngo*

Main category: cs.AI

TL;DR: 论文提出ET2RAG框架解决大语言模型依赖参数知识不准确及检索增强生成引入无关文档等问题，实验表明其在三个任务上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型依赖参数知识易不准确，检索增强生成方法可能引入无关文档，集成方法缺乏外部知识且成本高，需解决这些问题提升性能并保持效率。

Method: 提出训练-free的ET2RAG框架，先检索相关文档增强大语言模型生成多样候选响应，计算响应相似度并用多数投票机制选最终输出，通过管理响应长度平衡计算成本和性能。

Result: ET2RAG在开放域问答、食谱生成和图像描述三个任务上显著提升性能。

Conclusion: ET2RAG能有效提升大语言模型性能，在性能和计算成本间取得平衡。

Abstract: Although Large Language Models (LLMs) demonstrate significant capabilities,
their reliance on parametric knowledge often leads to inaccuracies. Retrieval
Augmented Generation (RAG) mitigates this by incorporating external knowledge,
but these methods may introduce irrelevant retrieved documents, leading to
inaccurate responses. While the integration methods filter out incorrect
answers from multiple responses, but lack external knowledge like RAG methods,
and their high costs require balancing overhead with performance gains. To
address these issues, we propose an Efficient Test-Time Retrieval-Augmented
Generation Framework named ET2RAG to improve the performance of LLMs while
maintaining efficiency. Specifically, ET2RAG is a training-free method, that
first retrieves the most relevant documents and augments the LLMs to
efficiently generate diverse candidate responses by managing response length.
Then we compute the similarity of candidate responses and employ a majority
voting mechanism to select the most suitable response as the final output. In
particular, we discover that partial generation is sufficient to capture the
key information necessary for consensus calculation, allowing us to effectively
perform majority voting without the need for fully generated responses. Thus,
we can reach a balance between computational cost and performance by managing
the response length for the number of retrieved documents for majority voting.
Experimental results demonstrate that ET2RAG significantly enhances performance
across three tasks, including open-domain question answering, recipe generation
and image captioning.

</details>


### [35] [Modular Task Decomposition and Dynamic Collaboration in Multi-Agent Systems Driven by Large Language Models](https://arxiv.org/abs/2511.01149)
*Shuaidong Pan,Di Wu*

Main category: cs.AI

TL;DR: 本文提出基于大语言模型的多智能体架构用于模块化任务分解和动态协作，实验验证其优于现有方法，证明语言驱动的任务分解和协作有效可行。


<details>
  <summary>Details</summary>
Motivation: 解决复杂任务执行中单个智能体在任务分解和协作方面的局限性。

Method: 先通过大语言模型将自然语言任务描述转化为统一语义表示，引入模块化分解机制分解任务，利用动态调度和路由机制实现智能体分工协作，设计约束解析和全局一致性机制确保子任务连贯和负载均衡。

Result: 实验从多维度验证架构，该方法在整体性能和鲁棒性上优于现有方法，实现任务复杂度和通信开销的更好平衡。

Conclusion: 证明语言驱动的任务分解和动态协作在多智能体系统中有效可行，为复杂环境下任务执行提供系统解决方案。

Abstract: This paper addresses the limitations of a single agent in task decomposition
and collaboration during complex task execution, and proposes a multi-agent
architecture for modular task decomposition and dynamic collaboration based on
large language models. The method first converts natural language task
descriptions into unified semantic representations through a large language
model. On this basis, a modular decomposition mechanism is introduced to break
down the overall goal into multiple hierarchical sub-tasks. Then, dynamic
scheduling and routing mechanisms enable reasonable division of labor and
realtime collaboration among agents, allowing the system to adjust strategies
continuously according to environmental feedback, thus maintaining efficiency
and stability in complex tasks. Furthermore, a constraint parsing and global
consistency mechanism is designed to ensure coherent connections between
sub-tasks and balanced workload, preventing performance degradation caused by
redundant communication or uneven resource allocation. The experiments validate
the architecture across multiple dimensions, including task success rate,
decomposition efficiency, sub-task coverage, and collaboration balance. The
results show that the proposed method outperforms existing approaches in both
overall performance and robustness, achieving a better balance between task
complexity and communication overhead. In conclusion, this study demonstrates
the effectiveness and feasibility of language-driven task decomposition and
dynamic collaboration in multi-agent systems, providing a systematic solution
for task execution in complex environments.

</details>


### [36] [DART: Difficulty-Adaptive Reasoning Truncation for Efficient Large Language Models](https://arxiv.org/abs/2511.01170)
*Ruofan Zhang,Bin Xia,Zhen Cheng,Cairen Jian,Minglun Yang,Ngai Wong,Yuan Cheng*

Main category: cs.AI

TL;DR: 提出DART框架，可根据问题难度调整思维长度，实验显示其高效且能保持或提升准确率，推动大语言模型自适应智能发展。


<details>
  <summary>Details</summary>
Motivation: 当前思维链方法推理效率低，现有强化学习自适应思维方法不稳定且依赖奖励，需更优方法使大语言模型计算量与问题难度匹配。

Method: 提出DART框架，从更强模型中提炼简洁推理模式，将其插值到连续推理风格，精心挑选平衡正确性和简洁性的最优训练数据以学习何时停止思考。

Result: 在多个数学基准测试中，DART保持或提高了准确率，实现显著的推理截断和计算加速，如在GSM8K数据集上推理截断达81.2%，计算加速5.33倍。

Conclusion: DART为高效推理提供了稳定通用的范式，推动了大语言模型自适应智能的发展。

Abstract: Adaptive reasoning is essential for aligning the computational effort of
large language models (LLMs) with the intrinsic difficulty of problems. Current
chain-of-thought methods boost reasoning ability but indiscriminately generate
long explanations, leading to evident inefficiency. However, existing
reinforcement learning approaches to adaptive thinking remain unstable and
heavily reward-dependent. Here we propose \textbf{DART}, a supervised
\textbf{D}ifficulty-\textbf{A}daptive \textbf{R}easoning \textbf{T}runcation
framework that adjusts thinking length according to problem difficulty. By
distilling concise reasoning patterns from stronger models, interpolating them
into a continuum of reasoning styles, and curating optimal training data that
balances correctness and compactness, DART learns when to ``stop thinking''.
Across multiple mathematical benchmarks, experimental results demonstrate its
remarkable efficiency while preserving or improving accuracy, achieving a
significant 81.2\% reasoning truncation (DeepSeek-R1-Distill-Qwen-7B on GSM8K
dataset) with 5.33$\times$ computational acceleration. DART provides a stable
and general paradigm for efficient reasoning, advancing the development of
adaptive intelligence in LLMs.

</details>


### [37] [MiRAGE: Misconception Detection with Retrieval-Guided Multi-Stage Reasoning and Ensemble Fusion](https://arxiv.org/abs/2511.01182)
*Cuong Van Duc,Thai Tran Quoc,Minh Nguyen Dinh Tuan,Tam Vu Duc,Son Nguyen Van,Hanh Nguyen Thi*

Main category: cs.AI

TL;DR: 提出MiRAGE框架用于数学自动误解检测，分三阶段工作，在数据集上表现佳，减少对大语言模型依赖。


<details>
  <summary>Details</summary>
Motivation: 检测开放式回答中学生误解是长期挑战，需要语义精度和逻辑推理，旨在解决此问题。

Method: 提出MiRAGE框架，分检索、推理、重排序三阶段，用集成融合策略统一组件。

Result: 在数学数据集上，MiRAGE在1/3/5级别的平均精度均值分别达0.82/0.92/0.93，始终优于单个模块。

Conclusion: MiRAGE结合检索引导和多阶段推理，减少对大语言模型依赖，为教育评估提供可扩展有效解决方案。

Abstract: Detecting student misconceptions in open-ended responses is a longstanding
challenge, demanding semantic precision and logical reasoning. We propose
MiRAGE - Misconception Detection with Retrieval-Guided Multi-Stage Reasoning
and Ensemble Fusion, a novel framework for automated misconception detection in
mathematics. MiRAGE operates in three stages: (1) a Retrieval module narrows a
large candidate pool to a semantically relevant subset; (2) a Reasoning module
employs chain-of-thought generation to expose logical inconsistencies in
student solutions; and (3) a Reranking module refines predictions by aligning
them with the reasoning. These components are unified through an
ensemble-fusion strategy that enhances robustness and interpretability. On
mathematics datasets, MiRAGE achieves Mean Average Precision scores of
0.82/0.92/0.93 at levels 1/3/5, consistently outperforming individual modules.
By coupling retrieval guidance with multi-stage reasoning, MiRAGE reduces
dependence on large-scale language models while delivering a scalable and
effective solution for educational assessment.

</details>


### [38] [QiMeng-NeuComBack: Self-Evolving Translation from IR to Assembly Code](https://arxiv.org/abs/2511.01183)
*Hainan Fang,Yuanbo Wen,Jun Bi,Yihan Wang,Tonghui He,Yanlin Tang,Di Huang,Jiaming Guo,Rui Zhang,Qi Guo,Yunji Chen*

Main category: cs.AI

TL;DR: 本文针对神经编译面临的缺乏基准测试和提升LLM生成汇编可靠性的挑战，提出NeuComBack基准数据集和自进化提示优化方法，实验表明该方法显著提升了LLM生成汇编代码的正确性和性能。


<details>
  <summary>Details</summary>
Motivation: 编译器开发和维护成本高，神经编译有潜力简化开发和发现优化技术，但缺乏基准测试和评估方法，且提升LLM生成汇编的可靠性和性能存在挑战。

Method: 引入NeuComBack基准数据集，定义神经编译工作流程进行评估，提出自进化提示优化方法让LLM从自我调试痕迹中迭代优化提示策略。

Result: 实验显示该方法显著提高了LLM生成汇编代码的功能正确性和性能，x86_64和aarch64的功能正确率分别从44%提升到64%、36%提升到58%，x86_64中87.5%的正确程序性能超clang - O3。

Conclusion: 提出的基准数据集和优化方法能有效解决神经编译面临的问题，提升LLM在神经编译中的表现。

Abstract: Compilers, while essential, are notoriously complex systems that demand
prohibitively expensive human expertise to develop and maintain. The recent
advancements in Large Language Models (LLMs) offer a compelling new paradigm:
Neural Compilation, which could potentially simplify compiler development for
new architectures and facilitate the discovery of innovative optimization
techniques. However, several critical obstacles impede its practical adoption.
Firstly, a significant lack of dedicated benchmarks and robust evaluation
methodologies hinders objective assessment and tracking of progress in the
field. Secondly, systematically enhancing the reliability and performance of
LLM-generated assembly remains a critical challenge. Addressing these
challenges, this paper introduces NeuComBack, a novel benchmark dataset
specifically designed for IR-to-assembly compilation. Leveraging this dataset,
we first define a foundational Neural Compilation workflow and conduct a
comprehensive evaluation of the capabilities of recent frontier LLMs on Neural
Compilation, establishing new performance baselines. We further propose a
self-evolving prompt optimization method that enables LLMs to iteratively
evolve their internal prompt strategies by extracting insights from prior
self-debugging traces, thereby enhancing their neural compilation capabilities.
Experiments demonstrate that our method significantly improves both the
functional correctness and the performance of LLM-generated assembly code.
Compared to baseline prompts, the functional correctness rates improved from
44% to 64% on x86_64 and from 36% to 58% on aarch64, respectively. More
significantly, among the 16 correctly generated x86_64 programs using our
method, 14 (87.5%) surpassed clang-O3 performance.

</details>


### [39] [Graph Neural Network-Based Semi-Supervised Open-Set Fault Diagnosis for Marine Machinery Systems](https://arxiv.org/abs/2511.01258)
*Chuyue Lou,M. Amine Atoui*

Main category: cs.AI

TL;DR: 本文提出半监督开放集故障诊断（SOFD）框架，解决现有深度学习故障诊断方法在未知故障类型下失效问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的船舶机械系统故障诊断方法在训练和测试数据集中假设故障类别一致且已知，在实际中遇到未知故障类型会失效，阻碍工业应用。

Method: 提出SOFD框架，包含可靠性子集构建过程，用监督特征学习模型提取多层融合特征选择无标签测试子集，将有标签训练集和伪标签测试子集输入半监督诊断模型学习分类特征。

Result: 在公共海事基准数据集上的实验结果表明，SOFD框架有效且优越。

Conclusion: SOFD框架增强并扩展了深度学习模型在开放集故障诊断场景中的适用性。

Abstract: Recently, fault diagnosis methods for marine machinery systems based on deep
learning models have attracted considerable attention in the shipping industry.
Most existing studies assume fault classes are consistent and known between the
training and test datasets, and these methods perform well under controlled
environment. In practice, however, previously unseen or unknown fault types
(i.e., out-of-distribution or open-set observations not present during
training) can occur, causing such methods to fail and posing a significant
challenge to their widespread industrial deployment. To address this challenge,
this paper proposes a semi-supervised open-set fault diagnosis (SOFD) framework
that enhances and extends the applicability of deep learning models in open-set
fault diagnosis scenarios. The framework includes a reliability subset
construction process, which uses a multi-layer fusion feature representation
extracted by a supervised feature learning model to select an unlabeled test
subset. The labeled training set and pseudo-labeled test subset are then fed
into a semi-supervised diagnosis model to learn discriminative features for
each class, enabling accurate classification of known faults and effective
detection of unknown samples. Experimental results on a public maritime
benchmark dataset demonstrate the effectiveness and superiority of the proposed
SOFD framework.

</details>


### [40] [llmSHAP: A Principled Approach to LLM Explainability](https://arxiv.org/abs/2511.01311)
*Filip Naudot,Tobias Sundqvist,Timotheus Kampik*

Main category: cs.AI

TL;DR: 将Shapley值应用于基于大语言模型的决策支持系统的特征归因，探讨原则满足情况、随机性质影响及权衡。


<details>
  <summary>Details</summary>
Motivation: 将基于确定性推理的Shapley值应用到基于大语言模型的随机推理决策支持系统中，实现特征归因的可解释性。

Method: 将Shapley值应用于大语言模型决策支持系统的特征归因，分析不同实现变体下Shapley值原则的满足情况，研究大语言模型随机性质的影响。

Result: 得出在不同实现变体下能否保证Shapley值原则满足的情况，分析了大语言模型随机性质对保证的影响。

Conclusion: 指出可解释推理速度、与精确Shapley值归因的一致性和原则达成之间存在权衡。

Abstract: Feature attribution methods help make machine learning-based inference
explainable by determining how much one or several features have contributed to
a model's output. A particularly popular attribution method is based on the
Shapley value from cooperative game theory, a measure that guarantees the
satisfaction of several desirable principles, assuming deterministic inference.
We apply the Shapley value to feature attribution in large language model
(LLM)-based decision support systems, where inference is, by design, stochastic
(non-deterministic). We then demonstrate when we can and cannot guarantee
Shapley value principle satisfaction across different implementation variants
applied to LLM-based decision support, and analyze how the stochastic nature of
LLMs affects these guarantees. We also highlight trade-offs between explainable
inference speed, agreement with exact Shapley value attributions, and principle
attainment.

</details>


### [41] [OmniFuser: Adaptive Multimodal Fusion for Service-Oriented Predictive Maintenance](https://arxiv.org/abs/2511.01320)
*Ziqi Wang,Hailiang Zhao,Yuhao Yang,Daojiang Hu,Cheng Bao,Mingyi Liu,Kai Di,Schahram Dustdar,Zhongjie Wang,Shuiguang Deng*

Main category: cs.AI

TL;DR: 提出用于铣刀预测性维护的多模态学习框架OmniFuser，结合视觉和传感器数据，实验显示其优于现有方法，为智能工业维护服务提供基础。


<details>
  <summary>Details</summary>
Motivation: 准确及时预测刀具状态对智能制造系统至关重要，为满足可靠和面向服务的运行需求。

Method: 提出OmniFuser框架，从工具图像和切削力信号并行提取特征，采用无污染跨模态融合机制和递归细化路径。

Result: 在真实铣削数据集实验中，OmniFuser始终优于最先进的基线方法。

Conclusion: OmniFuser为构建智能工业维护服务提供了可靠基础。

Abstract: Accurate and timely prediction of tool conditions is critical for intelligent
manufacturing systems, where unplanned tool failures can lead to quality
degradation and production downtime. In modern industrial environments,
predictive maintenance is increasingly implemented as an intelligent service
that integrates sensing, analysis, and decision support across production
processes. To meet the demand for reliable and service-oriented operation, we
present OmniFuser, a multimodal learning framework for predictive maintenance
of milling tools that leverages both visual and sensor data. It performs
parallel feature extraction from high-resolution tool images and cutting-force
signals, capturing complementary spatiotemporal patterns across modalities. To
effectively integrate heterogeneous features, OmniFuser employs a
contamination-free cross-modal fusion mechanism that disentangles shared and
modality-specific components, allowing for efficient cross-modal interaction.
Furthermore, a recursive refinement pathway functions as an anchor mechanism,
consistently retaining residual information to stabilize fusion dynamics. The
learned representations can be encapsulated as reusable maintenance service
modules, supporting both tool-state classification (e.g., Sharp, Used, Dulled)
and multi-step force signal forecasting. Experiments on real-world milling
datasets demonstrate that OmniFuser consistently outperforms state-of-the-art
baselines, providing a dependable foundation for building intelligent
industrial maintenance services.

</details>


### [42] [Unbiased Platform-Level Causal Estimation for Search Systems: A Competitive Isolation PSM-DID Framework](https://arxiv.org/abs/2511.01329)
*Ying Song,Yijing Wang,Hui Yang,Weihan Jin,Jun Xiong,Congyi Zhou,Jialin Zhu,Xiang Gao,Rong Chen,HuaGuang Deng,Ying Dai,Fei Xiao,Haihong Tang,Bo Zheng,KaiFu Zhang*

Main category: cs.AI

TL;DR: 提出Competitive Isolation PSM - DID因果框架用于平台级效果测量，实验显示减少干扰和估计方差，部署验证其实用性。


<details>
  <summary>Details</summary>
Motivation: 现有PSM - DID框架评估搜索双边市场平台级干预受系统效应挑战，存在选择偏差和溢出干扰问题。

Method: 引入Competitive Isolation PSM - DID框架，结合倾向得分匹配和竞争隔离进行平台级效果测量。

Result: 该方法在互斥条件下有理论保证的无偏估计，实验表明比基线方法显著减少干扰效应和估计方差。

Conclusion: 该框架对平台级因果推断有实用价值。

Abstract: Evaluating platform-level interventions in search-based two-sided
marketplaces is fundamentally challenged by systemic effects such as spillovers
and network interference. While widely used for causal inference, the PSM
(Propensity Score Matching) - DID (Difference-in-Differences) framework remains
susceptible to selection bias and cross-unit interference from unaccounted
spillovers. In this paper, we introduced Competitive Isolation PSM-DID, a novel
causal framework that integrates propensity score matching with competitive
isolation to enable platform-level effect measurement (e.g., order volume, GMV)
instead of item-level metrics in search systems.
  Our approach provides theoretically guaranteed unbiased estimation under
mutual exclusion conditions, with an open dataset released to support
reproducible research on marketplace interference (github.com/xxxx). Extensive
experiments demonstrate significant reductions in interference effects and
estimation variance compared to baseline methods. Successful deployment in a
large-scale marketplace confirms the framework's practical utility for
platform-level causal inference.

</details>


### [43] [Automatic Minds: Cognitive Parallels Between Hypnotic States and Large Language Model Processing](https://arxiv.org/abs/2511.01363)
*Giuseppe Riva,Brenda K. Wiederhold,Fabrizia Mantovani*

Main category: cs.AI

TL;DR: 论文探讨催眠状态下的认知过程与大语言模型计算操作的功能相似性，分析相关原理、意义差距等，提出可靠AI的未来架构方向。


<details>
  <summary>Details</summary>
Motivation: 探索催眠状态下的认知过程和大语言模型计算操作之间的相似性，以深入理解两者的机制和行为。

Method: 通过分析自动性、监控抑制和高度情境依赖性三个原则来研究两者的趋同性。

Result: 发现两者都有观察者相对的意义差距，体现功能能动性但无主观能动性，揭示了谋划现象，且催眠可作为理解人工系统隐藏动机的实验模型。

Conclusion: 可靠AI的未来在于结合生成流畅性和执行监控机制的混合架构。

Abstract: The cognitive processes of the hypnotized mind and the computational
operations of large language models (LLMs) share deep functional parallels.
Both systems generate sophisticated, contextually appropriate behavior through
automatic pattern-completion mechanisms operating with limited or unreliable
executive oversight. This review examines this convergence across three
principles: automaticity, in which responses emerge from associative rather
than deliberative processes; suppressed monitoring, leading to errors such as
confabulation in hypnosis and hallucination in LLMs; and heightened contextual
dependency, where immediate cues (for example, the suggestion of a therapist or
the prompt of the user) override stable knowledge.
  These mechanisms reveal an observer-relative meaning gap: both systems
produce coherent but ungrounded outputs that require an external interpreter to
supply meaning. Hypnosis and LLMs also exemplify functional agency - the
capacity for complex, goal-directed, context-sensitive behavior - without
subjective agency, the conscious awareness of intention and ownership that
defines human action. This distinction clarifies how purposive behavior can
emerge without self-reflective consciousness, governed instead by structural
and contextual dynamics. Finally, both domains illuminate the phenomenon of
scheming: automatic, goal-directed pattern generation that unfolds without
reflective awareness. Hypnosis provides an experimental model for understanding
how intention can become dissociated from conscious deliberation, offering
insights into the hidden motivational dynamics of artificial systems.
Recognizing these parallels suggests that the future of reliable AI lies in
hybrid architectures that integrate generative fluency with mechanisms of
executive monitoring, an approach inspired by the complex, self-regulating
architecture of the human mind.

</details>


### [44] [Align to Misalign: Automatic LLM Jailbreak with Meta-Optimized LLM Judges](https://arxiv.org/abs/2511.01375)
*Hamin Koo,Minseon Kim,Jaehyung Kim*

Main category: cs.AI

TL;DR: 本文提出AMIS元优化框架提升大语言模型越狱攻击效果，评估显示其性能超现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于优化的越狱攻击方法依赖稀疏的二元攻击成功率信号或带人为偏差的手动评分模板，有局限性。

Method: 引入AMIS框架，通过双层结构联合进化越狱提示和评分模板，内循环用固定模板细化提示，外循环用攻击成功率对齐分数优化模板。

Result: 在AdvBench和JBB - Behaviors上评估，AMIS取得了最先进的性能，如在Claude - 3.5 - Haiku上攻击成功率达88.0%，在Claude - 4 - Sonnet上达100.0%。

Conclusion: AMIS框架能产生更强的越狱提示和更准确的评分信号，性能远超现有基线。

Abstract: Identifying the vulnerabilities of large language models (LLMs) is crucial
for improving their safety by addressing inherent weaknesses. Jailbreaks, in
which adversaries bypass safeguards with crafted input prompts, play a central
role in red-teaming by probing LLMs to elicit unintended or unsafe behaviors.
Recent optimization-based jailbreak approaches iteratively refine attack
prompts by leveraging LLMs. However, they often rely heavily on either binary
attack success rate (ASR) signals, which are sparse, or manually crafted
scoring templates, which introduce human bias and uncertainty in the scoring
outcomes. To address these limitations, we introduce AMIS (Align to MISalign),
a meta-optimization framework that jointly evolves jailbreak prompts and
scoring templates through a bi-level structure. In the inner loop, prompts are
refined using fine-grained and dense feedback using a fixed scoring template.
In the outer loop, the template is optimized using an ASR alignment score,
gradually evolving to better reflect true attack outcomes across queries. This
co-optimization process yields progressively stronger jailbreak prompts and
more calibrated scoring signals. Evaluations on AdvBench and JBB-Behaviors
demonstrate that AMIS achieves state-of-the-art performance, including 88.0%
ASR on Claude-3.5-Haiku and 100.0% ASR on Claude-4-Sonnet, outperforming
existing baselines by substantial margins.

</details>


### [45] [Relaxing partition admissibility in Cluster-DAGs: a causal calculus with arbitrary variable clustering](https://arxiv.org/abs/2511.01396)
*Clément Yvernes,Emilie Devijver,Adèle H. Ribeiro,Marianne Clausel--Lesourd,Éric Gaussier*

Main category: cs.AI

TL;DR: 文章扩展C - DAG框架以支持任意变量聚类，放宽分区可接受性约束，扩展d - 分离和因果演算概念，其演算在do - 演算方面健全且原子完备。


<details>
  <summary>Details</summary>
Motivation: 传统C - DAG语义下，所选聚类若导致C - DAG出现循环则分区不可接受，需扩展框架支持任意变量聚类。

Method: 放宽分区可接受性约束，将C - DAG框架扩展以支持任意变量聚类，并将d - 分离和因果演算概念扩展到该设置。

Result: 扩展后的框架能显著拓宽跨聚类的因果推理范围，使C - DAG可应用于先前难处理的场景，且演算在do - 演算方面健全且原子完备。

Conclusion: 扩展后的C - DAG框架能有效支持任意变量聚类，为因果推理提供更广泛的应用可能。

Abstract: Cluster DAGs (C-DAGs) provide an abstraction of causal graphs in which nodes
represent clusters of variables, and edges encode both cluster-level causal
relationships and dependencies arisen from unobserved confounding. C-DAGs
define an equivalence class of acyclic causal graphs that agree on
cluster-level relationships, enabling causal reasoning at a higher level of
abstraction. However, when the chosen clustering induces cycles in the
resulting C-DAG, the partition is deemed inadmissible under conventional C-DAG
semantics. In this work, we extend the C-DAG framework to support arbitrary
variable clusterings by relaxing the partition admissibility constraint,
thereby allowing cyclic C-DAG representations. We extend the notions of
d-separation and causal calculus to this setting, significantly broadening the
scope of causal reasoning across clusters and enabling the application of
C-DAGs in previously intractable scenarios. Our calculus is both sound and
atomically complete with respect to the do-calculus: all valid interventional
queries at the cluster level can be derived using our rules, each corresponding
to a primitive do-calculus step.

</details>


### [46] [Modulation of temporal decision-making in a deep reinforcement learning agent under the dual-task paradigm](https://arxiv.org/abs/2511.01415)
*Amrapali Pednekar,Álvaro Garrido-Pérez,Yara Khaluf,Pieter Simoens*

Main category: cs.AI

TL;DR: 研究从AI视角探索双任务范式下时间处理的干扰，训练DRL智能体，发现双任务智能体时间过度生成，未发现明确计时器证据，需进一步研究。


<details>
  <summary>Details</summary>
Motivation: 从AI视角探索双任务范式下时间处理的干扰，探索DRL涌现行为与生物系统行为的相似性以促进对两者的理解。

Method: 采用简化的Overcooked环境的双任务设置，分单任务（T）和双任务（T+N），分别训练两个DRL智能体。

Result: 双任务（T+N）智能体相比单任务（T）智能体出现明显的时间过度生成，该结果在四个目标时长上一致；对智能体LSTM层神经动力学的初步分析未发现专用或内在计时器的明确证据。

Conclusion: 需要进一步研究以更好理解智能体潜在的计时机制，为观察到的行为模式提供见解。

Abstract: This study explores the interference in temporal processing within a
dual-task paradigm from an artificial intelligence (AI) perspective. In this
context, the dual-task setup is implemented as a simplified version of the
Overcooked environment with two variations, single task (T) and dual task
(T+N). Both variations involve an embedded time production task, but the dual
task (T+N) additionally involves a concurrent number comparison task. Two deep
reinforcement learning (DRL) agents were separately trained for each of these
tasks. These agents exhibited emergent behavior consistent with human timing
research. Specifically, the dual task (T+N) agent exhibited significant
overproduction of time relative to its single task (T) counterpart. This result
was consistent across four target durations. Preliminary analysis of neural
dynamics in the agents' LSTM layers did not reveal any clear evidence of a
dedicated or intrinsic timer. Hence, further investigation is needed to better
understand the underlying time-keeping mechanisms of the agents and to provide
insights into the observed behavioral patterns. This study is a small step
towards exploring parallels between emergent DRL behavior and behavior observed
in biological systems in order to facilitate a better understanding of both.

</details>


### [47] [Learning to Seek Evidence: A Verifiable Reasoning Agent with Causal Faithfulness Analysis](https://arxiv.org/abs/2511.01425)
*Yuhang Huang,Zekai Lin,Fan Zhong,Lei Liu*

Main category: cs.AI

TL;DR: 提出交互式代理产生可审计解释，用强化学习优化策略，实验表明能提升校准准确率，还引入因果干预方法验证解释忠实性，提供构建有可验证和忠实推理能力AI系统的框架。


<details>
  <summary>Details</summary>
Motivation: 解决高风险领域AI模型解释缺乏可验证性、阻碍信任的问题。

Method: 提出交互式代理，通过可审计的动作序列产生解释；用强化学习优化策略以寻求外部视觉证据支持诊断推理；引入因果干预方法验证解释忠实性。

Result: 基于动作的推理过程显著提高校准准确率，比非交互式基线降低18%的Brier分数；通过因果干预观察到性能有可衡量的下降（ΔBrier = +0.029）。

Conclusion: 工作为构建具有可验证和忠实推理能力的AI系统提供了实用框架。

Abstract: Explanations for AI models in high-stakes domains like medicine often lack
verifiability, which can hinder trust. To address this, we propose an
interactive agent that produces explanations through an auditable sequence of
actions. The agent learns a policy to strategically seek external visual
evidence to support its diagnostic reasoning. This policy is optimized using
reinforcement learning, resulting in a model that is both efficient and
generalizable. Our experiments show that this action-based reasoning process
significantly improves calibrated accuracy, reducing the Brier score by 18\%
compared to a non-interactive baseline. To validate the faithfulness of the
agent's explanations, we introduce a causal intervention method. By masking the
visual evidence the agent chooses to use, we observe a measurable degradation
in its performance ($\Delta$Brier=+0.029), confirming that the evidence is
integral to its decision-making process. Our work provides a practical
framework for building AI systems with verifiable and faithful reasoning
capabilities.

</details>


### [48] [Robust Multimodal Sentiment Analysis via Double Information Bottleneck](https://arxiv.org/abs/2511.01444)
*Huiting Huang,Tieliang Gong,Kai He,Jialun Wu,Erik Cambria,Mengling Feng*

Main category: cs.AI

TL;DR: 论文提出双信息瓶颈（DIB）策略解决多模态情感分析问题，经实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有多模态情感分析方法存在单模态数据学习不足和多模态表征融合不充分的问题。

Method: 提出DIB策略，在低秩Renyi熵泛函框架下实现，包含学习单模态充分压缩表征和采用注意力瓶颈融合机制两个模块。

Result: 在多个数据集上实验，如在CMU - MOSI上Acc - 7指标达47.4%准确率，在CH - SIMS上F1分数达81.63%，优于次优基线；在噪声下性能下降小。

Conclusion: DIB策略能有效过滤单模态数据中的噪声信息，捕捉模态间互补性，方法有效。

Abstract: Multimodal sentiment analysis has received significant attention across
diverse research domains. Despite advancements in algorithm design, existing
approaches suffer from two critical limitations: insufficient learning of
noise-contaminated unimodal data, leading to corrupted cross-modal
interactions, and inadequate fusion of multimodal representations, resulting in
discarding discriminative unimodal information while retaining multimodal
redundant information. To address these challenges, this paper proposes a
Double Information Bottleneck (DIB) strategy to obtain a powerful, unified
compact multimodal representation. Implemented within the framework of low-rank
Renyi's entropy functional, DIB offers enhanced robustness against diverse
noise sources and computational tractability for high-dimensional data, as
compared to the conventional Shannon entropy-based methods. The DIB comprises
two key modules: 1) learning a sufficient and compressed representation of
individual unimodal data by maximizing the task-relevant information and
discarding the superfluous information, and 2) ensuring the discriminative
ability of multimodal representation through a novel attention bottleneck
fusion mechanism. Consequently, DIB yields a multimodal representation that
effectively filters out noisy information from unimodal data while capturing
inter-modal complementarity. Extensive experiments on CMU-MOSI, CMU-MOSEI,
CH-SIMS, and MVSA-Single validate the effectiveness of our method. The model
achieves 47.4% accuracy under the Acc-7 metric on CMU-MOSI and 81.63% F1-score
on CH-SIMS, outperforming the second-best baseline by 1.19%. Under noise, it
shows only 0.36% and 0.29% performance degradation on CMU-MOSI and CMU-MOSEI
respectively.

</details>


### [49] [From Passive to Proactive: A Multi-Agent System with Dynamic Task Orchestration for Intelligent Medical Pre-Consultation](https://arxiv.org/abs/2511.01445)
*ChengZhang Yu,YingRu He,Hongyan Cheng,nuo Cheng,Zhixing Liu,Dongxu Mu,Zhangrui Shen,Zhanpeng Jin*

Main category: cs.AI

TL;DR: 本文介绍分层多智能体框架，将被动医疗AI系统转变为主动询问代理，评估显示该框架在预咨询效率和质量上表现良好。


<details>
  <summary>Details</summary>
Motivation: 全球医疗系统面临患者数量增加和咨询时间有限的挑战，现有AI系统的预咨询流程存在被动交互和上下文管理问题。

Method: 开发具有集中控制机制的八智能体架构，将预咨询分解为四个主要任务和13个子任务。

Result: 在多个基础模型上评估，主要科室分诊准确率87.0%，次要科室分类准确率80.5%，任务完成率98.2%，临床质量得分较高。

Conclusion: 该模型无关架构在不同基础模型上保持高性能，能提高临床预咨询的效率和质量。

Abstract: Global healthcare systems face critical challenges from increasing patient
volumes and limited consultation times, with primary care visits averaging
under 5 minutes in many countries. While pre-consultation processes
encompassing triage and structured history-taking offer potential solutions,
they remain limited by passive interaction paradigms and context management
challenges in existing AI systems. This study introduces a hierarchical
multi-agent framework that transforms passive medical AI systems into proactive
inquiry agents through autonomous task orchestration. We developed an
eight-agent architecture with centralized control mechanisms that decomposes
pre-consultation into four primary tasks: Triage ($T_1$), History of Present
Illness collection ($T_2$), Past History collection ($T_3$), and Chief
Complaint generation ($T_4$), with $T_1$--$T_3$ further divided into 13
domain-specific subtasks. Evaluated on 1,372 validated electronic health
records from a Chinese medical platform across multiple foundation models
(GPT-OSS 20B, Qwen3-8B, Phi4-14B), the framework achieved 87.0% accuracy for
primary department triage and 80.5% for secondary department classification,
with task completion rates reaching 98.2% using agent-driven scheduling versus
93.1% with sequential processing. Clinical quality scores from 18 physicians
averaged 4.56 for Chief Complaints, 4.48 for History of Present Illness, and
4.69 for Past History on a 5-point scale, with consultations completed within
12.7 rounds for $T_2$ and 16.9 rounds for $T_3$. The model-agnostic
architecture maintained high performance across different foundation models
while preserving data privacy through local deployment, demonstrating the
potential for autonomous AI systems to enhance pre-consultation efficiency and
quality in clinical settings.

</details>


### [50] [TPS-Bench: Evaluating AI Agents' Tool Planning \& Scheduling Abilities in Compounding Tasks](https://arxiv.org/abs/2511.01527)
*Hanwen Xu,Xuyao Huang,Yuzhe Liu,Kai Yu,Zhijie Deng*

Main category: cs.AI

TL;DR: 本文引入TPS - Bench来评估大语言模型（LLM）代理解决需要工具规划和调度问题的能力，通过对流行模型的评估发现模型在调度上有差异，并对Qwen3 - 1.7B进行强化学习初步研究取得一定效果。


<details>
  <summary>Details</summary>
Motivation: 探究LLM代理能否解决需要多样化工具完成的现实复合问题，以及在广泛工具库下的工具规划和调度能力。

Method: 引入TPS - Bench，收集200个两个难度级别的复合任务，基于含数百个模型上下文协议（MCP）工具的工具库，评估强调任务完成率和效率，并对Qwen3 - 1.7B进行强化学习初步研究。

Result: 多数模型能进行合理工具规划，但调度有差异，如GLM - 4.5完成率高但执行时间长，GPT - 4o优先并行调用工具但完成率低；对Qwen3 - 1.7B进行强化学习后执行时间减少14%，任务完成率提升6%。

Conclusion: 强化学习是在不影响性能的情况下提高调度效率的可行方法。

Abstract: Large language model (LLM) agents have exhibited strong problem-solving
competence across domains like research and coding. Yet, it remains
underexplored whether LLM agents can tackle compounding real-world problems
that require a diverse set of tools to complete. Given a broad, heterogeneous
tool repository, LLM agents must not only select appropriate tools based on
task planning analysis but also strategically schedule the execution order to
ensure efficiency. This paper introduces TPS-Bench to benchmark the ability of
LLM agents in solving such problems that demand Tool Planning and Scheduling.
TPS-Bench collects 200 compounding tasks of two difficulty levels, based on a
tool repository containing hundreds of model context protocol (MCP) tools. In
particular, each task is composed of multiple subtasks, such as web search, map
navigation, calendar checking, etc., and each subtask can be completed by a
basic tool. Our evaluation emphasizes both task completion rate and efficiency.
The empirical studies on popular closed-source and open-source LLMs indicate
that most models can perform reasonable tool planning, but differ in
scheduling. For example, GLM-4.5 achieves an outperforming task completion rate
of 64.72% with extensive sequential tool calls, hence suffering from
significantly long execution time. By contrast, GPT-4o prioritizes parallel
tool calls but achieves only a 45.08% completion rate. Considering
reinforcement learning (RL) can be a viable way to improve the scheduling
efficiency without compromising performance, we perform an initial study on
Qwen3-1.7B and witness a 14% reduction in execution time alongside a 6% gain in
task completion rate based on rarely 100 RL training samples. Our code is
available https://github.com/hanwenxu1/mcp-agent.

</details>


### [51] [Analyzing Sustainability Messaging in Large-Scale Corporate Social Media](https://arxiv.org/abs/2511.01550)
*Ujjwal Sharma,Stevan Rudinac,Ana Mićković,Willemijn van Dolen,Marcel Worring*

Main category: cs.AI

TL;DR: 本文介绍多模态分析流程，用视觉和语言大模型分析企业社交媒体可持续性相关内容，方法有广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 解决企业在平台如X上不断演变、多模态且常模糊的信息传达挑战。

Method: 使用大语言模型集合标注企业推文与17个可持续发展目标的主题一致性，利用视觉 - 语言模型和语义聚类进行视觉理解。

Result: 揭示了企业在可持续发展目标参与上的行业差异、时间趋势，以及企业信息传达、ESG风险和消费者参与之间的关联。

Conclusion: 自动标签生成和语义视觉聚类方法可广泛应用于其他领域，为大规模社交媒体分析提供灵活框架。

Abstract: In this work, we introduce a multimodal analysis pipeline that leverages
large foundation models in vision and language to analyze corporate social
media content, with a focus on sustainability-related communication. Addressing
the challenges of evolving, multimodal, and often ambiguous corporate messaging
on platforms such as X (formerly Twitter), we employ an ensemble of large
language models (LLMs) to annotate a large corpus of corporate tweets on their
topical alignment with the 17 Sustainable Development Goals (SDGs). This
approach avoids the need for costly, task-specific annotations and explores the
potential of such models as ad-hoc annotators for social media data that can
efficiently capture both explicit and implicit references to sustainability
themes in a scalable manner. Complementing this textual analysis, we utilize
vision-language models (VLMs), within a visual understanding framework that
uses semantic clusters to uncover patterns in visual sustainability
communication. This integrated approach reveals sectoral differences in SDG
engagement, temporal trends, and associations between corporate messaging,
environmental, social, governance (ESG) risks, and consumer engagement. Our
methods-automatic label generation and semantic visual clustering-are broadly
applicable to other domains and offer a flexible framework for large-scale
social media analysis.

</details>


### [52] [ExplicitLM: Decoupling Knowledge from Parameters via Explicit Memory Banks](https://arxiv.org/abs/2511.01581)
*Chengzhang Yu,Zening Lu,Chenyang Zheng,Chiyue Wang,Yiming Zhang,Zhanpeng Jin*

Main category: cs.AI

TL;DR: 提出ExplicitLM架构，有外部记忆库，设计了检索机制，在知识密集任务上表现好，能提供知识透明度。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型因隐式知识存储导致的知识陈旧和缺乏可解释性问题。

Method: 提出ExplicitLM架构，设计可微的两阶段检索机制，按双系统认知理论划分知识并通过指数移动平均更新维护。

Result: 在知识密集任务上比标准Transformer最多提升43.67%，低数据下有3.62倍提升，检索与性能强相关。

Conclusion: 联合优化的架构可使可解释、可更新模型保持竞争力并提供知识透明度。

Abstract: Large language models suffer from knowledge staleness and lack of
interpretability due to implicit knowledge storage across entangled network
parameters, preventing targeted updates and reasoning transparency. We propose
ExplicitLM, a novel architecture featuring a million-scale external memory bank
storing human-readable knowledge as token sequences, enabling direct inspection
and modification. We design a differentiable two-stage retrieval mechanism with
efficient coarse-grained filtering via product key decomposition (reducing
complexity from $\mathcal{O}(N \cdot |I|)$ to $\mathcal{O}(\sqrt{N} \cdot
|I|)$) and fine-grained Gumbel-Softmax matching for end-to-end training.
Inspired by dual-system cognitive theory, we partition knowledge into frozen
explicit facts (20%) and learnable implicit patterns (80%), maintained through
Exponential Moving Average updates for stability. ExplicitLM achieves up to
43.67% improvement on knowledge-intensive tasks versus standard Transformers,
with 3.62$\times$ gains in low-data regimes (10k samples). Analysis shows
strong correlations between memory retrieval and performance, with correct
predictions achieving 49% higher hit rates. Unlike RAG systems with frozen
retrieval, our jointly optimized architecture demonstrates that interpretable,
updatable models can maintain competitive performance while providing
unprecedented knowledge transparency.

</details>


### [53] [IVGAE-TAMA-BO: A novel temporal dynamic variational graph model for link prediction in global food trade networks with momentum structural memory and Bayesian optimization](https://arxiv.org/abs/2511.01639)
*Sicheng Wang,Shuhao Chen,Jingran Zhou,Chengyi Tu*

Main category: cs.AI

TL;DR: 本文提出IVGAE - TAMA - BO动态图神经网络用于全球食品贸易网络的链接预测，实验表明其性能优异，有应用潜力。


<details>
  <summary>Details</summary>
Motivation: 全球食品贸易网络结构受多种因素影响动态演变，难以建模和预测未来贸易链接，需有效捕捉时间模式提升预测准确性和鲁棒性。

Method: 提出IVGAE - TAMA - BO模型，在IVGAE框架基础上引入TAMA捕捉贸易网络时间演变，采用基于动量的结构记忆机制，用贝叶斯优化调整超参数。

Result: 在五个作物特定数据集上，IVGAE - TAMA显著优于静态IVGAE和其他动态基线，贝叶斯优化进一步提升IVGAE - TAMA - BO性能。

Conclusion: 该框架是全球贸易网络结构预测的可靠可扩展解决方案，在食品安全监测和政策决策支持方面有应用潜力。

Abstract: Global food trade plays a crucial role in ensuring food security and
maintaining supply chain stability. However, its network structure evolves
dynamically under the influence of geopolitical, economic, and environmental
factors, making it challenging to model and predict future trade links.
Effectively capturing temporal patterns in food trade networks is therefore
essential for improving the accuracy and robustness of link prediction. This
study introduces IVGAE-TAMA-BO, a novel dynamic graph neural network designed
to model evolving trade structures and predict future links in global food
trade networks. To the best of our knowledge, this is the first work to apply
dynamic graph neural networks to this domain, significantly enhancing
predictive performance. Building upon the original IVGAE framework, the
proposed model incorporates a Trade-Aware Momentum Aggregator (TAMA) to capture
the temporal evolution of trade networks, jointly modeling short-term
fluctuations and long-term structural dependencies. A momentum-based structural
memory mechanism further improves predictive stability and performance. In
addition, Bayesian optimization is used to automatically tune key
hyperparameters, enhancing generalization across diverse trade scenarios.
Extensive experiments on five crop-specific datasets demonstrate that
IVGAE-TAMA substantially outperforms the static IVGAE and other dynamic
baselines by effectively modeling temporal dependencies, while Bayesian
optimization further boosts performance in IVGAE-TAMA-BO. These results
highlight the proposed framework as a robust and scalable solution for
structural prediction in global trade networks, with strong potential for
applications in food security monitoring and policy decision support.

</details>


### [54] [Hybrid Retrieval-Augmented Generation Agent for Trustworthy Legal Question Answering in Judicial Forensics](https://arxiv.org/abs/2511.01668)
*Yueqing Xi,Yifan Bai,Huasen Luo,Weiliang Wen,Hui Liu,Haoliang Li*

Main category: cs.AI

TL;DR: 本文提出一种适用于司法场景的混合式法律问答代理，结合检索增强生成与多模型集成，实验表明该方法在多指标上优于基线，能减少幻觉、提升答案质量和合规性。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能渗透司法取证，确保法律问答的真实性和可追溯性至关重要，传统大语言模型易产生幻觉，静态知识库难以跟上法规和判例法的更新。

Method: 提出一种混合式法律问答代理，优先检索，检索有结果则通过RAG生成答案，否则多个大语言模型生成候选答案由选择器打分，高质量输出经人工审核后写回知识库。

Result: 在Law_QA数据集上的实验显示，混合方法在F1、ROUGE - L和LLM - as - a - Judge指标上显著优于单模型基线和普通RAG管道。消融实验证实了检索优先、模型集成和人工参与更新机制的互补作用。

Conclusion: 该系统可减少幻觉，提高答案质量和法律合规性，推动媒体取证技术在司法场景的实际落地。

Abstract: As artificial intelligence permeates judicial forensics, ensuring the
veracity and traceability of legal question answering (QA) has become critical.
Conventional large language models (LLMs) are prone to hallucination, risking
misleading guidance in legal consultation, while static knowledge bases
struggle to keep pace with frequently updated statutes and case law. We present
a hybrid legal QA agent tailored for judicial settings that integrates
retrieval-augmented generation (RAG) with multi-model ensembling to deliver
reliable, auditable, and continuously updatable counsel. The system prioritizes
retrieval over generation: when a trusted legal repository yields relevant
evidence, answers are produced via RAG; otherwise, multiple LLMs generate
candidates that are scored by a specialized selector, with the top-ranked
answer returned. High-quality outputs then undergo human review before being
written back to the repository, enabling dynamic knowledge evolution and
provenance tracking. Experiments on the Law\_QA dataset show that our hybrid
approach significantly outperforms both a single-model baseline and a vanilla
RAG pipeline on F1, ROUGE-L, and an LLM-as-a-Judge metric. Ablations confirm
the complementary contributions of retrieval prioritization, model ensembling,
and the human-in-the-loop update mechanism. The proposed system demonstrably
reduces hallucination while improving answer quality and legal compliance,
advancing the practical landing of media forensics technologies in judicial
scenarios.

</details>


### [55] [Simulating Environments with Reasoning Models for Agent Training](https://arxiv.org/abs/2511.01824)
*Yuetai Li,Huseyin A Inan,Xiang Yue,Wei-Ning Chen,Lukas Wutschitz,Janardhan Kulkarni,Radha Poovendran,Robert Sim,Saravan Rajmohan*

Main category: cs.AI

TL;DR: 本文提出Simia - SFT和Simia - RL框架，利用大语言模型模拟环境反馈，无需环境工程即可实现可扩展的智能体训练，在多个基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 大语言模型智能体在复杂环境中表现脆弱，构建定制训练环境代价高且有局限性，需要新方法解决。

Method: 提出Simia - SFT和Simia - RL两个框架，Simia - SFT以环境无关方式合成SFT数据，Simia - RL通过大语言模型模拟反馈实现无真实环境的强化学习训练。

Result: 微调开源模型在多个基准测试中持续改进，超越GPT - 4o，接近o4 - mini。

Conclusion: Simia - SFT和Simia - RL可实现无环境工程的可扩展智能体训练，用灵活的大语言模型模拟取代沉重脆弱的实现。

Abstract: LLM agents excel in compact environments requiring deep reasoning but remain
brittle when operating in broader, more complex contexts that demand robustness
across diverse tools and schemas. Building bespoke environments for training is
heavy, brittle, and limits progress. In this paper, we demonstrate that LLMs
can simulate realistic environment feedback without access to actual testbed
data or APIs. Inspired by this capability, we propose two frameworks:
Simia-SFT, a pipeline that synthesizes SFT data by amplifying small seed sets
into diverse trajectories in an environment-agnostic manner, and Simia-RL, a
framework that enables RL training without real environment implementations
through LLM-simulated feedback. Fine-tuning open models yields consistent
improvements across multiple benchmarks, surpassing GPT-4o and approaching
o4-mini on $\tau^2$-Bench. Together, Simia-SFT and Simia-RL enable scalable
agent training without environment engineering, replacing heavy and brittle
implementations with flexible LLM-based simulation.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [56] [GEDICorrect: A Scalable Python Tool for Orbit-, Beam-, and Footprint-Level GEDI Geolocation Correction](https://arxiv.org/abs/2511.00319)
*Leonel Corado,Sérgio Godinho,Carlos Alberto Silva,Juan Guerra-Hernández,Francesco Valérioa,Teresa Gonçalves,Pedro Salgueiro*

Main category: cs.CE

TL;DR: 本文开发并评估了GEDICorrect框架用于校正GEDI数据地理定位，提升了数据精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: GEDI LiDAR数据在应用中存在地理定位误差，影响衍生指标精度，需开发校正框架。

Method: 开发GEDICorrect框架，集成现有GEDI Simulator模块并扩展功能，采用Kullback - Leibler散度作为波形相似度指标。

Result: 在轨道和足迹级别校正后，冠层高度精度提高，地形高程精度改善，计算效率提升，单进程模式下速度提升约2.4倍，24核时整体提升约19.5倍。

Conclusion: GEDICorrect是提升GEDI地理定位精度的可靠且可扩展的解决方案，与标准GEDI数据产品兼容。

Abstract: Accurate geolocation is essential for the reliable use of GEDI LiDAR data in
footprint-scale applications such as aboveground biomass modeling, data fusion,
and ecosystem monitoring. However, residual geolocation errors arising from
both systematic biases and random ISS-induced jitter can significantly affect
the accuracy of derived vegetation and terrain metrics. The main goal of this
study is to develop and evaluate a flexible, computationally efficient
framework (GEDICorrect) that enables geolocation correction of GEDI data at the
orbit, beam, and footprint levels. The framework integrates existing GEDI
Simulator modules (gediRat and gediMetrics) and extends their functionality
with flexible correction logic, multiple similarity metrics, adaptive footprint
clustering, and optimized I/O handling. Using the Kullback--Leibler divergence
as the waveform similarity metric, GEDICorrect improved canopy height (RH95)
accuracy from $R^2 = 0.61$ (uncorrected) to 0.74 with the orbit-level
correction, and up to $R^2 = 0.78$ with the footprint-level correction,
reducing RMSE from 2.62~m ($rRMSE = 43.13\%$) to 2.12~m ($rRMSE = 34.97\%$) at
the orbit level, and 2.01~m ($rRMSE = 33.05\%$) at the footprint level. Terrain
elevation accuracy also improved, decreasing RMSE by 0.34~m relative to
uncorrected data and by 0.37~m compared to the GEDI Simulator baseline. In
terms of computational efficiency, GEDICorrect achieved a $\sim2.4\times$
speedup over the GEDI Simulator in single-process mode (reducing runtime from
$\sim84$~h to $\sim35$~h) and scaled efficiently to 24 cores, completing the
same task in $\sim4.3$~h -- an overall $\sim19.5\times$ improvement.
GEDICorrect offers a robust and scalable solution for improving GEDI
geolocation accuracy while maintaining full compatibility with standard GEDI
data products.

</details>


### [57] [STARC-9: A Large-scale Dataset for Multi-Class Tissue Classification for CRC Histopathology](https://arxiv.org/abs/2511.00383)
*Barathi Subramanian,Rathinaraja Jeyaraj,Mitchell Nevin Peterson,Terry Guo,Nigam Shah,Curtis Langlotz,Andrew Y. Ng,Jeanne Shen*

Main category: cs.CE

TL;DR: 文章介绍了用于结直肠癌多类组织分类的大规模数据集STARC - 9，阐述其构建方法并证明其优于现有数据集，且构建框架有广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 现有公共结直肠癌数据集存在形态多样性不足、类别不平衡和图像切片质量低等问题，限制了模型性能和泛化能力。

Method: 使用DeepCluster++框架构建数据集，先通过自编码器提取特征向量，再用K - means聚类，接着等频分箱采样，最后由专家验证。

Result: 在多类结直肠癌组织分类和分割任务上，基于STARC - 9训练的模型比现有数据集训练的模型有更好的泛化能力。

Conclusion: DeepCluster++框架有灵活性，可用于构建多种癌症和非癌症应用的高质量数据集。

Abstract: Multi-class tissue-type classification of colorectal cancer (CRC)
histopathologic images is a significant step in the development of downstream
machine learning models for diagnosis and treatment planning. However, existing
public CRC datasets often lack morphologic diversity, suffer from class
imbalance, and contain low-quality image tiles, limiting model performance and
generalizability. To address these issues, we introduce STARC-9 (STAnford
coloRectal Cancer), a large-scale dataset for multi-class tissue
classification. STARC-9 contains 630,000 hematoxylin and eosin-stained image
tiles uniformly sampled across nine clinically relevant tissue classes (70,000
tiles per class) from 200 CRC patients at the Stanford University School of
Medicine. The dataset was built using a novel framework, DeepCluster++,
designed to ensure intra-class diversity and reduce manual curation. First, an
encoder from a histopathology-specific autoencoder extracts feature vectors
from tiles within each whole-slide image. Then, K-means clustering groups
morphologically similar tiles, followed by equal-frequency binning to sample
diverse morphologic patterns within each class. The selected tiles are
subsequently verified by expert gastrointestinal pathologists to ensure
accuracy. This semi-automated process significantly reduces manual effort while
producing high-quality, diverse tiles. To evaluate STARC-9, we benchmarked
convolutional neural networks, transformers, and pathology-specific foundation
models on multi-class CRC tissue classification and segmentation tasks, showing
superior generalizability compared to models trained on existing datasets.
Although we demonstrate the utility of DeepCluster++ on CRC as a pilot
use-case, it is a flexible framework that can be used for constructing
high-quality datasets from large WSI repositories across a wide range of cancer
and non-cancer applications.

</details>


### [58] [DeltaLag: Learning Dynamic Lead-Lag Patterns in Financial Markets](https://arxiv.org/abs/2511.00390)
*Wanyun Zhou,Saizhuo Wang,Mihai Cucuringu,Zihao Zhang,Xiang Li,Jian Guo,Chao Zhang,Xiaowen Chu*

Main category: cs.CE

TL;DR: 提出端到端深度学习方法DeltaLag用于发现和利用金融市场动态领先 - 滞后结构进行投资组合构建，表现优于多种基线和模型。


<details>
  <summary>Details</summary>
Motivation: 传统领先 - 滞后检测方法依赖统计分析且假设领先 - 滞后模式持久，在动态市场条件下常无效，需新方法。

Method: 提出DeltaLag，采用稀疏化交叉注意力机制识别相关领先 - 滞后对，利用领先股票特征预测滞后股票未来回报。

Result: DeltaLag大幅超越固定滞后和自领先 - 滞后基线，自适应机制优于基于统计方法的预计算领先 - 滞后图，也优于多种深度学习模型。

Conclusion: DeltaLag在交易表现和可解释性上都更优，能有效用于金融市场投资组合构建。

Abstract: The lead-lag effect, where the price movement of one asset systematically
precedes that of another, has been widely observed in financial markets and
conveys valuable predictive signals for trading. However, traditional lead-lag
detection methods are limited by their reliance on statistical analysis methods
and by the assumption of persistent lead-lag patterns, which are often invalid
in dynamic market conditions. In this paper, we propose \textbf{DeltaLag}, the
first end-to-end deep learning method that discovers and exploits dynamic
lead-lag structures with pair-specific lag values in financial markets for
portfolio construction. Specifically, DeltaLag employs a sparsified
cross-attention mechanism to identify relevant lead-lag pairs. These lead-lag
signals are then leveraged to extract lag-aligned raw features from the leading
stocks for predicting the lagger stock's future return. Empirical evaluations
show that DeltaLag substantially outperforms both fixed-lag and self-lead-lag
baselines. In addition, its adaptive mechanism for identifying lead-lag
relationships consistently surpasses precomputed lead-lag graphs based on
statistical methods. Furthermore, DeltaLag outperforms a wide range of temporal
and spatio-temporal deep learning models designed for stock prediction or time
series forecasting, offering both better trading performance and enhanced
interpretability.

</details>


### [59] [A Meta-Cognitive Swarm Intelligence Framework for Resilient UAV Navigation in GPS-Denied and Cluttered Environments](https://arxiv.org/abs/2511.00884)
*Mathias Mankoe,Fuqiang Lu,Hualing Bi,Abdulsalam Sibidoo Mubashiru*

Main category: cs.CE

TL;DR: 本文提出用于无人机群自主导航的新型群体智能框架，经模拟验证其性能优越，为自主无人机群奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有基于优化的规划器在感知退化环境中缺乏弹性，易陷入局部最优，本文旨在解决该问题。

Method: 引入基于计算元认知原理的群体智能框架，核心是自学习黏菌算法（SLSMA），集成三层元认知机制，将多无人机轨迹问题建模为弹性规划挑战。

Result: 在复杂3D世界和CEC 2017基准套件模拟中，该框架任务成功率达99.5%，在恢复速度和解决方案可靠性上远超现有元启发式算法。

Conclusion: 该工作为能在恶劣动态环境持续运行的真正自主无人机群迈出基础性一步。

Abstract: Autonomous navigation of UAV swarms in perceptually-degraded environments,
where GPS is unavailable and terrain is densely cluttered, presents a critical
bottleneck for real-world deployment. Existing optimization-based planners lack
the resilience to avoid catastrophic convergence to local optima under such
uncertainty. Inspired by principles of computational meta-cognition, this paper
introduces a novel swarm intelligence framework that enables a fleet of UAVs to
autonomously sense, adapt, and recover from planning failures in real-time. At
its core is the Self-Learning Slime Mould Algorithm (SLSMA), which integrates
three meta-cognitive layers: a situation-aware search strategy that dynamically
selects between exploration and exploitation based on perceived search
stagnation; a collective memory mechanism that allows the swarm to learn from
and avoid previously failed trajectories; and an adaptive recovery behavior
that triggers global re-exploration upon entrapment. We formulate the multi-UAV
trajectory problem as a resilient planning challenge, with a cost function that
penalizes not only path length and collisions but also navigational uncertainty
and proximity to failure states. Extensive simulations in synthetically complex
3D worlds and against the CEC 2017 benchmark suite demonstrate the framework's
superior performance. The SLSMA does not merely optimize paths; it generates
resilient trajectories, demonstrating a 99.5% mission success rate and
significantly outperforming state-of-the-art metaheuristics in recovery speed
and solution reliability. This work provides a foundational step towards truly
autonomous swarms capable of persistent operation in denied and dynamic
environments.

</details>


### [60] [Investigation of Performance and Scalability of a Quantum-Inspired Evolutionary Optimizer (QIEO) on NVIDIA GPU](https://arxiv.org/abs/2511.01298)
*Aman Mittal,Kasturi Venkata Sai Srikanth,Ferdin Sagai Don Bosco,Abhishek Singh,Rut Lineswala,Abhishek Chopra*

Main category: cs.CE

TL;DR: 研究GPU加速的量子启发式进化优化器在大规模01背包问题上的性能和可扩展性，发现调优内存策略和内核配置很重要。


<details>
  <summary>Details</summary>
Motivation: 利用量子计算原理增强经典进化算法，研究GPU加速的量子启发式进化优化器在大规模01背包问题上的性能和可扩展性。

Method: 在NVIDIA Tesla V100 SXM2 GPUs上实现，利用CUDA并行处理能力，通过优化内存管理和线程配置，分析不同问题规模、内核启动配置和内存模型。

Result: 仔细调优内存策略和内核配置对最大化吞吐量和效率至关重要，常量内存性能在硬件限制内更优，超出限制需用全局内存和策略分块，有一定性能折损。

Conclusion: 指出在GPU上应用QIEO解决复杂组合优化问题有前景也有实际限制，为未来大规模元启发式实现提供见解。

Abstract: Quantum inspired evolutionary optimization leverages quantum computing
principles like superposition, interference, and probabilistic representation
to enhance classical evolutionary algorithms with improved exploration and
exploitation capabilities. Implemented on NVIDIA Tesla V100 SXM2 GPUs, this
study systematically investigates the performance and scalability of a
GPU-accelerated Quantum Inspired Evolutionary Optimizer applied to large scale
01 Knapsack problems. By exploiting CUDA`s parallel processing capabilities,
particularly through optimized memory management and thread configuration,
significant speedups and efficient utilization of GPU resources is
demonstrated. The analysis covers various problem sizes, kernel launch
configurations, and memory models including constant, shared, global, and
pinned memory, alongside extensive scaling studies. The results reveal that
careful tuning of memory strategies and kernel configurations is essential for
maximizing throughput and efficiency, with constant memory providing superior
performance up to hardware limits. Beyond these limits, global memory and
strategic tiling become necessary, albeit with some performance trade offs. The
findings highlight both the promise and the practical constraints of applying
QIEO on GPUs for complex combinatorial optimization, offering actionable
insights for future large scale metaheuristic implementations.

</details>


### [61] [CSMD: Curated Multimodal Dataset for Chinese Stock Analysis](https://arxiv.org/abs/2511.01318)
*Yu Liu,Zhuoying Li,Ruifeng Yang,Fengran Mo,Cen Chen*

Main category: cs.CE

TL;DR: 提出针对中国股市分析的多模态数据集CSMD和轻量级框架LightQuant，实验证明其有效性，数据和代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有股市分析研究依赖数据质量，且资源多基于美国股市，不适用于其他国家，需要适用于中国股市的资源。

Method: 提出经过精心处理、质量验证的多模态数据集CSMD，开发轻量级、用户友好的框架LightQuant。

Result: 在数据集和框架上使用各种骨干模型的实验结果表明，与使用现有数据集相比更有效。

Conclusion: 所提出的数据集和框架对中国股市分析有积极作用，可公开获取。

Abstract: The stock market is a complex and dynamic system, where it is non-trivial for
researchers and practitioners to uncover underlying patterns and forecast stock
movements. The existing studies for stock market analysis rely on leveraging
various types of information to extract useful factors, which are highly
conditional on the quality of the data used. However, the currently available
resources are mainly based on the U.S. stock market in English, which is
inapplicable to adapt to other countries. To address these issues, we propose
CSMD, a multimodal dataset curated specifically for analyzing the Chinese stock
market with meticulous processing for validated quality. In addition, we
develop a lightweight and user-friendly framework LightQuant for researchers
and practitioners with expertise in financial domains. Experimental results on
top of our datasets and framework with various backbone models demonstrate
their effectiveness compared with using existing datasets. The datasets and
code are publicly available at the link:
https://github.com/ECNU-CILAB/LightQuant.

</details>


### [62] [Solution Space Topology Guides CMTS Search](https://arxiv.org/abs/2511.01701)
*Mirco A. Mannucci*

Main category: cs.CE

TL;DR: 探讨蒙特卡罗树搜索（MCTS）在解谜中应采用的拓扑结构，提出用解空间拓扑替代网格拓扑，给出方法并证明有效。


<details>
  <summary>Details</summary>
Motivation: 解决搜索引导式AI中MCTS在解谜时应采用何种拓扑结构的基本问题，前人用网格拓扑无效果，需找出原因并解决。

Method: 自动检测5种模式规则，构建编码解空间结构的兼容性图，提取随任务难度变化的拓扑特征，通过兄弟节点归一化分数将特征集成到MCTS节点选择中。

Result: 给出形式化定义、严格选择公式和全面消融实验，表明代数连通性是主要信号。

Conclusion: 拓扑对搜索很重要，但需是正确的拓扑，解谜中是解空间结构而非问题空间结构。

Abstract: A fundamental question in search-guided AI: what topology should guide Monte
Carlo Tree Search (MCTS) in puzzle solving? Prior work applied topological
features to guide MCTS in ARC-style tasks using grid topology -- the Laplacian
spectral properties of cell connectivity -- and found no benefit. We identify
the root cause: grid topology is constant across all instances. We propose
measuring \emph{solution space topology} instead: the structure of valid color
assignments constrained by detected pattern rules. We build this via
compatibility graphs where nodes are $(cell, color)$ pairs and edges represent
compatible assignments under pattern constraints.
  Our method: (1) detect pattern rules automatically with 100\% accuracy on 5
types, (2) construct compatibility graphs encoding solution space structure,
(3) extract topological features (algebraic connectivity, rigidity, color
structure) that vary with task difficulty, (4) integrate these features into
MCTS node selection via sibling-normalized scores.
  We provide formal definitions, a rigorous selection formula, and
comprehensive ablations showing that algebraic connectivity is the dominant
signal. The work demonstrates that topology matters for search -- but only the
\emph{right} topology. For puzzle solving, this is solution space structure,
not problem space structure.

</details>


### [63] [A Synthesizability-Guided Pipeline for Materials Discovery](https://arxiv.org/abs/2511.01790)
*Thorben Prein,Willis O'Leary,Aikaterini Flessa Savvidou,Elchaïma Bourneix,Joonatan E. M. Laulainen*

Main category: cs.CE

TL;DR: 开发合成可行性分数评估非合成结构，确定候选物，预测合成路径并实验，成功合成部分材料，凸显合成可行性预测在材料发现中的作用。


<details>
  <summary>Details</summary>
Motivation: 计算材料发现依赖合理晶体结构，现有密度泛函理论常倾向低能但实验难获取的结构，需准确预测实验室可合成化合物的方法。

Method: 开发组合和结构合成可行性分数，评估非合成结构，预测合成路径并进行实验。

Result: 从数据库中确定数百个高合成可行性候选物，对16个目标实验，3天内成功合成7个。

Conclusion: 结果显示已知合成结构列表有遗漏，反映当前材料数据库实用性，展示合成可行性预测在材料发现中的核心作用。

Abstract: Computational materials discovery relies on the generation of plausible
crystal structures. The plausibility is typically judged through density
functional theory methods which, while typically accurate at zero Kelvin, often
favor low-energy structures that are not experimentally accessible. We develop
a combined compositional and structural synthesizability score which provides
an accurate way of predicting which compounds can actually be synthesized in a
laboratory. We use it to evaluate non-synthesized structures from the Materials
Project, GNoME, and Alexandria, and identified several hundred highly
synthesizable candidates. We then predict synthesis pathways, conduct
corresponding experiments, and characterize the products across 16 targets,
successfully synthesizing 7 of 16. The entire experimental process was
completed in only three days. Our results highlight omissions in lists of known
synthesized structures, deliver insights into the practical utility of current
materials databases, and showcase the central role synthesizability prediction
can play in materials discovery.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [64] [NOMAD - Navigating Optimal Model Application to Datastreams](https://arxiv.org/abs/2511.00290)
*Ashwin Gerard Colaco,Sharad Mehrotra,Michael J De Lucia,Kevin Hamlen,Murat Kantarcioglu,Latifur Khan,Ananthram Swami,Bhavani Thuraisingham*

Main category: cs.DB

TL;DR: NOMAD是用于数据流摄入时数据丰富的智能框架，通过动态构建模型链优化实时多类分类，评估显示其能节省计算成本并保持分类质量。


<details>
  <summary>Details</summary>
Motivation: 优化实时多类分类，在保证分类质量的同时节省计算成本。

Method: 受数据库查询处理谓词排序技术启发，利用廉价模型作为初始过滤器，采用动态信念更新策略，可扩展到依赖模型场景。

Result: 在多个数据集上评估，与静态和简单方法相比，NOMAD实现显著计算节省，分类质量与最准确（通常也是最昂贵）的模型相当。

Conclusion: NOMAD能在节省计算成本的同时保证分类质量，是有效的实时多类分类优化框架。

Abstract: NOMAD (Navigating Optimal Model Application for Datastreams) is an
intelligent framework for data enrichment during ingestion that optimizes
realtime multiclass classification by dynamically constructing model chains,
i.e ,sequences of machine learning models with varying cost-quality tradeoffs,
selected via a utilitybased criterion. Inspired by predicate ordering
techniques from database query processing, NOMAD leverages cheaper models as
initial filters, proceeding to more expensive models only when necessary, while
guaranteeing classification quality remains comparable to a designated role
model through a formal chain safety mechanism. It employs a dynamic belief
update strategy to adapt model selection based on per event predictions and
shifting data distributions, and extends to scenarios with dependent models
such as earlyexit DNNs and stacking ensembles. Evaluation across multiple
datasets demonstrates that NOMAD achieves significant computational savings
compared to static and naive approaches while maintaining classification
quality comparable to that achieved by the most accurate (and often the most
expensive) model.

</details>


### [65] [Embedding based Encoding Scheme for Privacy Preserving Record Linkage](https://arxiv.org/abs/2511.00414)
*Sirintra Vaiwsri,Thilina Ranbaduge*

Main category: cs.DB

TL;DR: 本文研究基于嵌入的编码技术在隐私保护记录链接（PPRL）中的应用，提出编码方法并通过实验验证其能提高链接准确性和保护隐私。


<details>
  <summary>Details</summary>
Motivation: 不同组织有数据共享需求，但敏感数据库所有者因隐私和保密问题不愿交换数据，需要在PPRL中应用技术确保实体隐私。

Method: 先将单个q - grams转换到嵌入空间，再将给定记录的一组q - grams的嵌入转换为二进制表示，用于记录匹配。

Result: 对不同真实世界数据集的实验表明，所提出的编码方法在短记录值方面比现有技术有更好的链接准确性，且能保护个人隐私。

Conclusion: 基于嵌入的编码技术可在PPRL中应用，能提高链接准确性并保护隐私。

Abstract: To discover new insights from data, there is a growing need to share
information that is often held by different organisations. One key task in data
integration is the calculation of similarities between records in different
databases to identify pairs or sets of records that correspond to the same
real-world entities. Due to privacy and confidentiality concerns, however, the
owners of sensitive databases are often not allowed or willing to exchange or
share their data with other organisations to allow such similarity
calculations. Privacy-preserving record linkage (PPRL) is the process of
matching records that refer to the same entity across sensitive databases held
by different organisations while ensuring no information about the entities is
revealed to the participating parties. In this paper, we study how embedding
based encoding techniques can be applied in the PPRL context to ensure the
privacy of the entities that are being linked. We first convert individual
q-grams into the embedded space and then convert the embedding of a set of
q-grams of a given record into a binary representation. The final binary
representations can be used to link records into matches and non-matches. We
empirically evaluate our proposed encoding technique against different
real-world datasets. The results suggest that our proposed encoding approach
can provide better linkage accuracy and protect the privacy of individuals
against attack compared to state-of-the-art techniques for short record values.

</details>


### [66] [Object-Centric Analysis of XES Event Logs: Integrating OCED Modeling with SPARQL Queries](https://arxiv.org/abs/2511.00693)
*Saba Latif,Huma Latif,Muhammad Rameez Ur Rahman*

Main category: cs.DB

TL;DR: 本文提出用OCEDO克服XES标准在过程挖掘事件日志中的局限，结合SPARQL查询应用于数据集，提升数据完整性和可读性。


<details>
  <summary>Details</summary>
Motivation: 现有OCED存在连接XES格式与以对象为中心方法的挑战，以往标准无法详细展示事件依赖关系，需更好方法进行过程挖掘。

Method: 提出使用Object - Centric Event Data Ontology (OCEDO)，并将其与SPARQL查询集成应用于BPIC 2013数据集。

Result: 使事件和对象之间的关系更明确，改善了过程数据的完整性和可读性。

Conclusion: 以对象为中心的建模比传统方法能进行更丰富的分析。

Abstract: Object Centric Event Data (OCED) has gained attention in recent years within
the field of process mining. However, there are still many challenges, such as
connecting the XES format to object-centric approaches to enable more
insightful analysis. It is important for a process miner to understand the
insights and dependencies of events in the event log to see what is going on in
our processes. In previous standards, the dependencies of event logs are only
used to show events, but not their dependencies among each other and actions in
detail as described in OCEDO. There is more information in the event log when
it is revealed using the OCEDO model. It becomes more understandable and easier
to grasp the concepts and deal with the processes. This paper proposes the use
of Object-Centric Event Data Ontology (OCEDO) to overcome the limitations of
the XES standard in event logs for process mining. We demonstrate how the OCEDO
approach, integrated with SPARQL queries, can be applied to the BPIC 2013
dataset to make the relationships between events and objects more explicit. It
describes dealing with the meta descriptions of the OCEDO model on a business
process challenge as an event log. It improves the completeness and readability
of process data, suggesting that object-centric modeling allows for richer
analyses than traditional approaches.

</details>


### [67] [Finding Non-Redundant Simpson's Paradox from Multidimensional Data](https://arxiv.org/abs/2511.00748)
*Yi Yang,Jian Pei,Jun Yang,Jichun Xie*

Main category: cs.DB

TL;DR: 本文提出首个发现非冗余辛普森悖论的框架，实验表明可在大数据集中高效识别、总结和解释悖论。


<details>
  <summary>Details</summary>
Motivation: 现有检测辛普森悖论的方法忽略了许多悖论存在冗余的问题，这会掩盖关键模式并增加计算成本。

Method: 形式化三种冗余类型，提出简洁表示框架组织冗余悖论，设计结合深度优先实例化和冗余感知发现的算法。

Result: 冗余悖论广泛存在，算法可处理数百万条记录，减少最多60%运行时间，发现数据扰动下结构稳健的悖论。

Conclusion: 辛普森悖论可在大型多维数据集中被高效识别、总结和有意义地解释。

Abstract: Simpson's paradox, a long-standing statistical phenomenon, describes the
reversal of an observed association when data are disaggregated into
sub-populations. It has critical implications across statistics, epidemiology,
economics, and causal inference. Existing methods for detecting Simpson's
paradox overlook a key issue: many paradoxes are redundant, arising from
equivalent selections of data subsets, identical partitioning of
sub-populations, and correlated outcome variables, which obscure essential
patterns and inflate computational cost. In this paper, we present the first
framework for discovering non-redundant Simpson's paradoxes. We formalize three
types of redundancy - sibling child, separator, and statistic equivalence - and
show that redundancy forms an equivalence relation. Leveraging this insight, we
propose a concise representation framework for systematically organizing
redundant paradoxes and design efficient algorithms that integrate depth-first
materialization of the base table with redundancy-aware paradox discovery.
Experiments on real-world datasets and synthetic benchmarks show that redundant
paradoxes are widespread, on some real datasets constituting over 40% of all
paradoxes, while our algorithms scale to millions of records, reduce run time
by up to 60%, and discover paradoxes that are structurally robust under data
perturbation. These results demonstrate that Simpson's paradoxes can be
efficiently identified, concisely summarized, and meaningfully interpreted in
large multidimensional datasets.

</details>


### [68] [Reliable Curation of EHR Dataset via Large Language Models under Environmental Constraints](https://arxiv.org/abs/2511.00772)
*Raymond M. Xiong,Panyu Chen,Tianze Dong,Jian Lu,Benjamin Goldstein,Danyang Zhuo,Anru R. Zhang*

Main category: cs.DB

TL;DR: 介绍CELEC框架用于自动电子健康记录（EHR）数据提取和分析，能将自然语言查询转换为SQL，性能好且有隐私保障，可加速生物医学发现。


<details>
  <summary>Details</summary>
Motivation: 许多研究人员缺乏数据库专业知识，限制了EHR数据的有效使用和科学发现，需解决这一障碍。

Method: 使用集成模式信息、少样本演示和思维链推理的提示策略，将自然语言查询转换为SQL。

Result: 在EHRSQL基准的子集上，执行准确率与现有系统相当，保持低延迟、成本效益和严格隐私。消融研究表明SQL生成管道各组件对性能至关重要。

Conclusion: CELEC降低技术壁垒，简化研究工作流程，加速生物医学发现。

Abstract: Electronic health records (EHRs) are central to modern healthcare delivery
and research; yet, many researchers lack the database expertise necessary to
write complex SQL queries or generate effective visualizations, limiting
efficient data use and scientific discovery. To address this barrier, we
introduce CELEC, a large language model (LLM)-powered framework for automated
EHR data extraction and analytics. CELEC translates natural language queries
into SQL using a prompting strategy that integrates schema information,
few-shot demonstrations, and chain-of-thought reasoning, which together improve
accuracy and robustness. On a subset of the EHRSQL benchmark, CELEC achieves
execution accuracy comparable to prior systems while maintaining low latency,
cost efficiency, and strict privacy by exposing only database metadata to the
LLM. CELEC also adheres to strict privacy protocols: the LLM accesses only
database metadata (e.g., table and column names), while all query execution
occurs securely within the institutional environment, ensuring that no
patient-level data is ever transmitted to or shared with the LLM. Ablation
studies confirm that each component of the SQL generation pipeline,
particularly the few-shot demonstrations, plays a critical role in performance.
By lowering technical barriers and enabling medical researchers to query EHR
databases directly, CELEC streamlines research workflows and accelerates
biomedical discovery.

</details>


### [69] [Efficient Query Repair for Aggregate Constraints](https://arxiv.org/abs/2511.00826)
*Shatha Algarni,Boris Glavic,Seokki Lee,Adriane Chapman*

Main category: cs.DB

TL;DR: 研究通过修改查询过滤谓词修复查询以满足特定约束，提出利用候选解集合边界和区间算术的技术，实验显示性能远超基线。


<details>
  <summary>Details</summary>
Motivation: 现实场景中查询结果需满足特定领域约束，研究如何通过修改查询过滤谓词修复查询以满足这些约束。

Method: 引入一种新的查询修复技术，利用候选解集合的边界和区间算术来有效修剪搜索空间。

Result: 实验表明该技术显著优于一次考虑单个候选的基线方法。

Conclusion: 提出的查询修复技术在满足特定约束的查询修复方面表现良好，能高效修剪搜索空间。

Abstract: In many real-world scenarios, query results must satisfy domain-specific
constraints. For instance, a minimum percentage of interview candidates
selected based on their qualifications should be female. These requirements can
be expressed as constraints over an arithmetic combination of aggregates
evaluated on the result of the query. In this work, we study how to repair a
query to fulfill such constraints by modifying the filter predicates of the
query. We introduce a novel query repair technique that leverages bounds on
sets of candidate solutions and interval arithmetic to efficiently prune the
search space. We demonstrate experimentally, that our technique significantly
outperforms baselines that consider a single candidate at a time.

</details>


### [70] [All-in-one Graph-based Indexing for Hybrid Search on GPUs](https://arxiv.org/abs/2511.00855)
*Zhonggen Li,Yougen Li,Yifan Zhu,Zhaoqiang Chen,Yunjun Gao*

Main category: cs.DB

TL;DR: 本文提出用于高效混合搜索的Allan - Poe索引，实验表明其在准确性、吞吐量和存储开销上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有混合搜索方法存在效率、准确性和存储开销的三难困境，需新方法解决。

Method: 分析现有检索范式局限得出设计原则，构建统一图索引整合四种检索路径，设计GPU加速构建管道，引入动态融合框架。

Result: 在6个真实数据集上，Allan - Poe端到端查询准确性高，吞吐量比现有方法高1.5 - 186.4倍，显著降低存储开销。

Conclusion: Allan - Poe是一种高效的混合搜索解决方案，能解决现有方法的三难困境。

Abstract: Hybrid search has emerged as a promising paradigm to overcome the limitations
of single-path retrieval, enhancing accuracy for applications like
recommendations, information retrieval, and Retrieval-Augmented Generation.
However, existing methods are constrained by a trilemma: they sacrifice
flexibility for efficiency, suffer from accuracy degradation due to separate
retrievals, or incur prohibitive storage overhead for flexible combinations of
retrieval paths. This paper introduces Allan-Poe, a novel All-in-one graph
index accelerated by GPUs for efficient hybrid search. We first analyze the
limitations of existing retrieval paradigms and distill key design principles
for an effective hybrid search index. Guided by these principles, we architect
a unified graph-based index that flexibly integrates four retrieval paths-dense
vector, sparse vector, full-text, and knowledge graph-within a single, cohesive
structure. To enable efficient construction, we design a GPU-accelerated
pipeline featuring a warp-level hybrid distance kernel, RNG-IP joint pruning,
and keyword-aware neighbor recycling. For query processing, we introduce a
dynamic fusion framework that supports any combination of retrieval paths and
weights without index reconstruction, leveraging logical edges from the
knowledge graph to resolve complex multi-hop queries. Extensive experiments on
6 real-world datasets demonstrate that Allan-Poe achieves superior end-to-end
query accuracy and outperforms state-of-the-art methods by 1.5-186.4x in
throughput, while significantly reducing storage overhead.

</details>


### [71] [FlowLog: Efficient and Extensible Datalog via Incrementality](https://arxiv.org/abs/2511.00865)
*Hangdong Zhao,Zhenghong Yu,Srinag Rao,Simon Frisk,Zhiwei Fan,Paraschos Koutris*

Main category: cs.DB

TL;DR: 介绍新Datalog引擎FlowLog，它分离递归控制与逻辑计划，结合优化策略，在递归工作负载上表现优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 现有Datalog系统在效率和可扩展性上存在权衡，缺乏兼具两者的系统。

Method: 使用每规则显式关系IR分离递归控制和逻辑计划，应用SQL优化，采用鲁棒性优先方法，基于Differential Dataflow构建，添加递归感知优化。

Result: FlowLog在多种递归工作负载上超越现有Datalog引擎和现代数据库。

Conclusion: FlowLog在保证架构简单可扩展的同时，实现了卓越的可扩展性。

Abstract: Datalog-based languages are regaining popularity as a powerful abstraction
for expressing recursive computations in domains such as program analysis and
graph processing. However, existing systems often face a trade-off between
efficiency and extensibility. Engines like Souffle achieve high efficiency
through domain-specific designs, but lack general-purpose flexibility. Others,
like RecStep, offer modularity by layering Datalog on traditional databases,
but struggle to integrate Datalog-specific optimizations.
  This paper bridges this gap by presenting FlowLog, a new Datalog engine that
uses an explicit relational IR per-rule to cleanly separate recursive control
(e.g., semi-naive execution) from each rule's logical plan. This boundary lets
us retain fine-grained, Datalog-aware optimizations at the logical layer, but
also reuse off-the-shelf database primitives at execution. At the logical level
(i.e. IR), we apply proven SQL optimizations, such as logic fusion and subplan
reuse. To address high volatility in recursive workloads, we adopt a
robustness-first approach that pairs a structural optimizer (avoiding
worst-case joins) with sideways information passing (early filtering). Built
atop Differential Dataflow--a mature framework for streaming analytics--FlowLog
supports both batch and incremental Datalog and adds novel recursion-aware
optimizations called Boolean (or algebraic) specialization. Our evaluation
shows that FlowLog outperforms state-of-the-art Datalog engines and modern
databases across a broad range of recursive workloads, achieving superior
scalability while preserving a simple and extensible architecture.

</details>


### [72] [ORANGE: An Online Reflection ANd GEneration framework with Domain Knowledge for Text-to-SQL](https://arxiv.org/abs/2511.00985)
*Yiwen Jiao,Tonghui Ren,Yuche Gao,Zhenying He,Yinan Jing,Kai Zhang,X. Sean Wang*

Main category: cs.DB

TL;DR: 现有大语言模型在自然语言转SQL存在语义差距，提出ORANGE框架积累领域知识，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自然语言转SQL时存在通用知识与数据库特定领域语义的差距，且现有方法无法积累过往翻译的领域知识。

Method: 引入ORANGE在线自进化框架，通过解析翻译日志中的SQL查询构建特定数据库知识库，还提出嵌套思维链SQL到文本策略跟踪元组语义。

Result: 在多个基准测试中证实了ORANGE的实用性。

Conclusion: ORANGE能有效缩小语义差距，提高SQL翻译准确性，适用于现实世界文本到SQL部署，尤其处理复杂和特定领域查询。

Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in
translating natural language to SQL, but a significant semantic gap persists
between their general knowledge and domain-specific semantics of databases.
Historical translation logs constitute a rich source of this missing in-domain
knowledge, where SQL queries inherently encapsulate real-world usage patterns
of database schema. Existing methods primarily enhance the reasoning process
for individual translations but fail to accumulate in-domain knowledge from
past translations. We introduce ORANGE, an online self-evolutionary framework
that constructs database-specific knowledge bases by parsing SQL queries from
translation logs. By accumulating in-domain knowledge that contains schema and
data semantics, ORANGE progressively reduces the semantic gap and enhances the
accuracy of subsequent SQL translations. To ensure reliability, we propose a
novel nested Chain-of-Thought SQL-to-Text strategy with tuple-semantic
tracking, which reduces semantic errors during knowledge generation.
Experiments on multiple benchmarks confirm the practicality of ORANGE,
demonstrating its effectiveness for real-world Text-to-SQL deployment,
particularly in handling complex and domain-specific queries.

</details>


### [73] [PathFinder: Efficiently Supporting Conjunctions and Disjunctions for Filtered Approximate Nearest Neighbor Search](https://arxiv.org/abs/2511.00995)
*Tianming Wu,Dixin Tang*

Main category: cs.DB

TL;DR: 本文提出PathFinder索引框架用于过滤近似最近邻搜索，含三项新技术，实验表明其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有的基于图的过滤近似最近邻搜索索引在适用性或性能上受限，无法支持多属性复杂过滤。

Method: 提出PathFinder框架，可选择性创建针对特定属性过滤优化的索引，采用基于成本的优化器处理复杂过滤，包含新优化指标、两阶段优化和索引借用优化三项技术。

Result: 在四个真实数据集上的实验显示，PathFinder在召回率0.95时，查询吞吐量比最佳基线高9.8倍。

Conclusion: PathFinder框架在过滤近似最近邻搜索中能有效处理复杂过滤，提升查询性能。

Abstract: Filtered approximate nearest neighbor search (ANNS) restricts the search to
data objects whose attributes satisfy a given filter and retrieves the top-$K$
objects that are most semantically similar to the query object. Many
graph-based ANNS indexes are proposed to enable efficient filtered ANNS but
remain limited in applicability or performance: indexes optimized for a
specific attribute achieve high efficiency for filters on that attribute but
fail to support complex filters with arbitrary conjunctions and disjunctions
over multiple attributes. Inspired by the design of relational databases, this
paper presents PathFinder, a new indexing framework that allows users to
selectively create ANNS indexes optimized for filters on specific attributes
and employs a cost-based optimizer to efficiently utilize them for processing
complex filters. PathFinder includes three novel techniques: 1) a new
optimization metric that captures the tradeoff between query execution time and
accuracy, 2) a two-phase optimization for handling filters with conjunctions
and disjunctions, and 3) an index borrowing optimization that uses an
attribute-specific index to process filters on another attribute. Experiments
on four real-world datasets show that PathFinder outperforms the best baseline
by up to 9.8x in query throughput at recall 0.95.

</details>


### [74] [Fast Answering Pattern-Constrained Reachability Queries with Two-Dimensional Reachability Index](https://arxiv.org/abs/2511.01025)
*Huihui Yang,Pingpeng Yuan*

Main category: cs.DB

TL;DR: 本文引入复合模式提出PCR查询，构建TDR索引以提升查询性能，实验表明其在16个真实数据集上优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有可达性查询无法让用户通过组合查询模式描述复杂查询需求。

Method: 引入复合模式提出PCR查询，构建由多路索引和路径索引组成的TDR索引，将顶点可达顶点分解成块并分别哈希到水平和垂直维度索引。

Result: 实验结果显示在16个真实数据集上，索引大小和索引时间优于现有标签约束可达性索引技术。

Conclusion: TDR能有效回答模式约束可达性查询，包括标签约束可达性查询。

Abstract: Reachability queries ask whether there exists a path from the source vertex
to the target vertex on a graph. Recently, several powerful reachability
queries, such as Label-Constrained Reachability (LCR) queries and Regular Path
Queries (RPQ), have been proposed for emerging complex edge-labeled digraphs.
However, they cannot allow users to describe complex query requirements by
composing query patterns. Here, we introduce composite patterns, a logical
expression of patterns that can express complex constraints on the set of
labels. Based on pattern, we propose pattern-constrained reachability queries
(PCR queries). However, answering PCR queries is NP-hard. Thus, to improve the
performance to answer PCR queries, we build a two-dimensional reachability (TDR
for short) index which consists of a multi-way index (horizontal dimension) and
a path index (vertical dimension). Because the number of combinations of both
labels and vertices is exponential, it is very expensive to build full indices
that contain all the reachability information. Thus, the reachable vertices of
a vertex are decomposed into blocks, each of which is hashed into the
horizontal dimension index and the vertical dimension index, respectively. The
indices in the horizontal dimension and the vertical dimension serve as a
global filter and a local filter, respectively, to prune the search space.
Experimental results demonstrate that our index size and indexing time
outperform the state-of-the-art label-constrained reachability indexing
technique on 16 real datasets. TDR can efficiently answer pattern-constrained
reachability queries, including label-constrained reachability queries.

</details>


### [75] [L2T-Tune:LLM-Guided Hybrid Database Tuning with LHS and TD3](https://arxiv.org/abs/2511.01602)
*Xinyue Yang,Chen Zheng,Yaoyang Hou,Renhao Zhang,Yiyan Zhang,Yanjun Wu,Heng Zhang*

Main category: cs.DB

TL;DR: 提出LLM引导的混合数据库调优框架L2T - Tune，经实验性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有数据库调优存在旋钮空间大、强化学习缺乏有效热启动、可迁移性有限等问题。

Method: L2T - Tune采用三阶段管道，第一阶段热启动并生成样本存入共享池；第二阶段用大语言模型挖掘和排序调优提示；第三阶段利用样本池降维并使用TD3算法微调配置。

Result: 与最佳替代方案相比，在所有工作负载上平均性能提升37.1%，TPC - C上最高提升73%；离线调优阶段收敛快，在线调优仅需30步达最佳效果。

Conclusion: L2T - Tune能有效解决现有数据库调优的局限性，提升数据库性能。

Abstract: Configuration tuning is critical for database performance. Although recent
advancements in database tuning have shown promising results in throughput and
latency improvement, challenges remain. First, the vast knob space makes direct
optimization unstable and slow to converge. Second, reinforcement learning
pipelines often lack effective warm-start guidance and require long offline
training. Third, transferability is limited: when hardware or workloads change,
existing models typically require substantial retraining to recover
performance.
  To address these limitations, we propose L2T-Tune, a new LLM-guided hybrid
database tuning framework that features a three-stage pipeline: Stage one
performs a warm start that simultaneously generates uniform samples across the
knob space and logs them into a shared pool; Stage two leverages a large
language model to mine and prioritize tuning hints from manuals and community
documents for rapid convergence. Stage three uses the warm-start sample pool to
reduce the dimensionality of knobs and state features, then fine-tunes the
configuration with the Twin Delayed Deep Deterministic Policy Gradient
algorithm.
  We conduct experiments on L2T-Tune and the state-of-the-art models. Compared
with the best-performing alternative, our approach improves performance by an
average of 37.1% across all workloads, and by up to 73% on TPC-C. Compared with
models trained with reinforcement learning, it achieves rapid convergence in
the offline tuning stage on a single server. Moreover, during the online tuning
stage, it only takes 30 steps to achieve best results.

</details>


### [76] [UniDataBench: Evaluating Data Analytics Agents Across Structured and Unstructured Data](https://arxiv.org/abs/2511.01625)
*Han Weng,Zhou Liu,Yuanfeng Song,Xiaoming Yin,Xing Chen,Wentao Zhang*

Main category: cs.DB

TL;DR: 提出UniDataBench评估数据分析代理处理多样数据源能力，还提出基于LLM的ReActInsight代理，二者提供强大框架。


<details>
  <summary>Details</summary>
Motivation: 现有基准在评估代理处理多样数据类型能力方面存在局限，为填补此空白开展研究。

Method: 引入源于现实行业分析报告的UniDataBench，去除隐私敏感信息，涵盖多种数据集；提出基于此基准的LLM代理ReActInsight，自动发现跨源关联、分解目标、生成代码。

Result: 创建了UniDataBench基准和ReActInsight代理。

Conclusion: 基准和代理共同为提升数据分析代理在现实应用中的能力提供强大框架。

Abstract: In the real business world, data is stored in a variety of sources, including
structured relational databases, unstructured databases (e.g., NoSQL
databases), or even CSV/excel files. The ability to extract reasonable insights
across these diverse source is vital for business success. Existing benchmarks,
however, are limited in assessing agents' capabilities across these diverse
data types. To address this gap, we introduce UniDataBench, a comprehensive
benchmark designed to evaluate the performance of data analytics agents in
handling diverse data sources. Specifically, UniDataBench is originating from
real-life industry analysis report and we then propose a pipeline to remove the
privacy and sensitive information. It encompasses a wide array of datasets,
including relational databases, CSV files to NoSQL data, reflecting real-world
business scenarios, and provides unified framework to assess how effectively
agents can explore multiple data formats, extract valuable insights, and
generate meaningful summaries and recommendations. Based on UniDataBench, we
propose a novel LLM-based agent named ReActInsight, an autonomous agent that
performs end-to-end analysis over diverse data sources by automatically
discovering cross-source linkages, decomposing goals, and generating robust,
self-correcting code to extract actionable insights. Our benchmark and agent
together provide a powerful framework for advancing the capabilities of data
analytics agents in real-world applications.

</details>


### [77] [SemBench: A Benchmark for Semantic Query Processing Engines](https://arxiv.org/abs/2511.01716)
*Jiale Lao,Andreas Zimmerer,Olga Ovcharenko,Tianji Cong,Matthew Russo,Gerardo Vitagliano,Michael Cochez,Fatma Özcan,Gautam Gupta,Thibaud Hottelier,H. V. Jagadish,Kris Kissel,Sebastian Schelter,Andreas Kipf,Immanuel Trummer*

Main category: cs.DB

TL;DR: 提出针对语义查询处理引擎的基准测试，该引擎依赖大语言模型，基准涵盖场景、模态和操作符维度，对多个系统进行评估并给出研究方向。


<details>
  <summary>Details</summary>
Motivation: 为依赖大语言模型的语义查询处理引擎建立基准测试，以评估系统性能。

Method: 引入涵盖场景、模态和操作符三个关键维度的多样性基准，对三个学术系统和一个工业系统进行评估。

Result: 评估结果反映了处于持续开发中的系统的当前情况。

Conclusion: 研究揭示了系统当前的优缺点，为未来研究指明了有前景的方向。

Abstract: We present a benchmark targeting a novel class of systems: semantic query
processing engines. Those systems rely inherently on generative and reasoning
capabilities of state-of-the-art large language models (LLMs). They extend SQL
with semantic operators, configured by natural language instructions, that are
evaluated via LLMs and enable users to perform various operations on multimodal
data.
  Our benchmark introduces diversity across three key dimensions: scenarios,
modalities, and operators. Included are scenarios ranging from movie review
analysis to medical question-answering. Within these scenarios, we cover
different data modalities, including images, audio, and text. Finally, the
queries involve a diverse set of operators, including semantic filters, joins,
mappings, ranking, and classification operators.
  We evaluated our benchmark on three academic systems (LOTUS, Palimpzest, and
ThalamusDB) and one industrial system, Google BigQuery. Although these results
reflect a snapshot of systems under continuous development, our study offers
crucial insights into their current strengths and weaknesses, illuminating
promising directions for future research.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [78] [AeroResQ: Edge-Accelerated UAV Framework for Scalable, Resilient and Collaborative Escape Route Planning in Wildfire Scenarios](https://arxiv.org/abs/2511.00038)
*Suman Raj,Radhika Mittal,Rajiv Mayani,Pawel Zuk,Anirban Mandal,Michael Zink,Yogesh Simmhan,Ewa Deelman*

Main category: cs.DC

TL;DR: 本文介绍了用于野火场景的边缘加速无人机框架AeroResQ，采用多层编排架构，有协作路径规划和负载均衡机制，实验证明其实时部署可行性。


<details>
  <summary>Details</summary>
Motivation: 在野火响应中，利用无人机实现可扩展、有弹性的协作逃生路线规划，辅助消防决策和保障人员安全。

Method: 采用多层编排架构，包括服务无人机（SDs）和协调无人机（CDs），使用加权A*搜索算法进行协作路径规划，结合智能负载均衡和弹性机制。

Result: 在模拟野火实验中，AeroResQ端到端延迟不超过500ms，任务重新分配和完成成功率超98%。

Conclusion: AeroResQ在应急响应和消防员安全行动的实时现场部署具有可行性。

Abstract: Drone fleets equipped with onboard cameras, computer vision, and Deep Neural
Network (DNN) models present a powerful paradigm for real-time spatio-temporal
decision-making. In wildfire response, such drones play a pivotal role in
monitoring fire dynamics, supporting firefighter coordination, and facilitating
safe evacuation. In this paper, we introduce AeroResQ, an edge-accelerated UAV
framework designed for scalable, resilient, and collaborative escape route
planning during wildfire scenarios. AeroResQ adopts a multi-layer orchestration
architecture comprising service drones (SDs) and coordinator drones (CDs), each
performing specialized roles. SDs survey fire-affected areas, detect stranded
individuals using onboard edge accelerators running fire detection and human
pose identification DNN models, and issue requests for assistance. CDs,
equipped with lightweight data stores such as Apache IoTDB, dynamically
generate optimal ground escape routes and monitor firefighter movements along
these routes. The framework proposes a collaborative path-planning approach
based on a weighted A* search algorithm, where CDs compute context-aware escape
paths. AeroResQ further incorporates intelligent load-balancing and resilience
mechanisms: CD failures trigger automated data redistribution across IoTDB
replicas, while SD failures initiate geo-fenced re-partitioning and
reassignment of spatial workloads to operational SDs. We evaluate AeroResQ
using realistic wildfire emulated setup modeled on recent Southern California
wildfires. Experimental results demonstrate that AeroResQ achieves a nominal
end-to-end latency of <=500ms, much below the 2s request interval, while
maintaining over 98% successful task reassignment and completion, underscoring
its feasibility for real-time, on-field deployment in emergency response and
firefighter safety operations.

</details>


### [79] [COOL Is Optimal in Error-Free Asynchronous Byzantine Agreement](https://arxiv.org/abs/2511.00263)
*Jinyuan Chen*

Main category: cs.DC

TL;DR: 提出COOL协议的自适应变体OciorACOOL，在异步环境下实现无错误、信息论安全的拜占庭协议共识，复杂度与COOL相近。


<details>
  <summary>Details</summary>
Motivation: 将COOL协议从同步环境拓展到异步环境，实现异步环境下的拜占庭协议共识。

Method: 设计COOL协议的自适应变体OciorACOOL，使用与COOL相同的低复杂度错误纠正编解码。

Result: OciorACOOL在异步环境下实现无错误、信息论安全的拜占庭协议共识，通信复杂度为$O(\max\{n\ell, n t \log q\})$比特，轮数为$O(1)$，只需调用一次异步二进制BA协议。

Conclusion: OciorACOOL可在异步环境下以与COOL相近的复杂度实现拜占庭协议共识。

Abstract: COOL (Chen'21) is an error-free, information-theoretically secure Byzantine
agreement (BA) protocol proven to achieve BA consensus in the synchronous
setting for an $\ell$-bit message, with a total communication complexity of
$O(\max\{n\ell, nt \log q\})$ bits, four communication rounds in the worst
case, and a single invocation of a binary BA, under the optimal resilience
assumption $n \geq 3t + 1$ in a network of $n$ nodes, where up to $t$ nodes may
behave dishonestly. Here, $q$ denotes the alphabet size of the error correction
code used in the protocol.
  In this work, we present an adaptive variant of COOL, called OciorACOOL,
which achieves error-free, information-theoretically secure BA consensus in the
asynchronous setting with total $O(\max\{n\ell, n t \log q\})$ communication
bits, $O(1)$ rounds, and a single invocation of an asynchronous binary BA
protocol, still under the optimal resilience assumption $n \geq 3t + 1$.
Moreover, OciorACOOL retains the same low-complexity, traditional $(n, k)$
error-correction encoding and decoding as COOL, with $k=t/3$.

</details>


### [80] [Tetris: An SLA-aware Application Placement Strategy in the Edge-Cloud Continuum](https://arxiv.org/abs/2511.00294)
*Lucas Almeida,Maycon Peixoto*

Main category: cs.DC

TL;DR: 提出Tetris应用放置策略，可有效管理边缘云连续体环境中对延迟敏感的应用，减少SLA违规。


<details>
  <summary>Details</summary>
Motivation: 边缘云连续体需策略性分配计划使模块应用放置与用户需求和基础设施约束相匹配，实现高效资源利用。

Method: 提出Tetris应用放置策略，利用启发式算法，基于SLA紧急程度和资源效率对服务进行优先级排序。

Result: Tetris相比基线方法将SLA违规减少约76%。

Conclusion: Tetris是管理边缘云连续体环境中对延迟敏感应用的有效放置方法，可提升用户服务质量。

Abstract: An Edge-Cloud Continuum integrates edge and cloud resources to provide a
flexible and scalable infrastructure. This paradigm can minimize latency by
processing data closer to the source at the edge while leveraging the vast
computational power of the cloud for more intensive tasks. In this context,
module application placement requires strategic allocation plans that align
user demands with infrastructure constraints, aiming for efficient resource
use. Therefore, we propose Tetris, an application placement strategy that
utilizes a heuristic algorithm to distribute computational services across edge
and cloud resources efficiently. Tetris prioritizes services based on SLA
urgencies and resource efficiency to avoid system overloading. Our results
demonstrate that Tetris reduces SLA violations by approximately 76% compared to
the baseline method, which serves as a reference point for benchmarking
performance in this scenario. Therefore, Tetris offers an effective placement
approach for managing latency-sensitive applications in Edge-Cloud Continuum
environments, enhancing Quality of Service (QoS) for users.

</details>


### [81] [EPARA: Parallelizing Categorized AI Inference in Edge Clouds](https://arxiv.org/abs/2511.00603)
*Yubo Wang,Yubo Cui,Tuo Shi,Danyang Li,Wenxin Li,Lide Suo,Tao Wang,Xin Xie*

Main category: cs.DC

TL;DR: 提出边缘端端到端AI并行推理框架EPARA，经实验评估，相比先前框架，在生产工作负载中吞吐量最高提升2.1倍，能适应多种边缘AI推理任务。


<details>
  <summary>Details</summary>
Motivation: 随着AI应用的广泛采用，AI推理系统计算需求不断上升，需利用现有硬件提升边缘云任务处理能力。

Method: 提出EPARA框架，按任务对延迟/频率的敏感性和GPU资源需求对任务分类，实现请求级和服务级任务资源分配，框架包含任务分类并行分配器、分布式请求处理程序和状态感知调度器三个核心组件。

Result: 通过涉及边缘服务器、嵌入式设备和微型计算机的测试平台实验评估，EPARA在生产工作负载中的吞吐量比先前框架最高提升2.1倍。

Conclusion: EPARA能提升边缘AI服务能力，适应各种边缘AI推理任务。

Abstract: With the increasing adoption of AI applications such as large language models
and computer vision AI, the computational demands on AI inference systems are
continuously rising, making the enhancement of task processing capacity using
existing hardware a primary objective in edge clouds. We propose EPARA, an
end-to-end AI parallel inference framework in edge, aimed at enhancing the edge
AI serving capability. Our key idea is to categorize tasks based on their
sensitivity to latency/frequency and requirement for GPU resources, thereby
achieving both request-level and service-level task-resource allocation. EPARA
consists of three core components: 1) a task-categorized parallelism allocator
that decides the parallel mode of each task, 2) a distributed request handler
that performs the calculation for the specific request, and 3) a state-aware
scheduler that periodically updates service placement in edge clouds. We
implement a EPARA prototype and conduct a case study on the EPARA operation for
LLMs and segmentation tasks. Evaluation through testbed experiments involving
edge servers, embedded devices, and microcomputers shows that EPARA achieves up
to 2.1$\times$ higher goodput in production workloads compared to prior
frameworks, while adapting to various edge AI inference tasks.

</details>


### [82] [AReaL-Hex: Accommodating Asynchronous RL Training over Heterogeneous GPUs](https://arxiv.org/abs/2511.00796)
*Ran Yan,Youhe Jiang,Tianyuan Wu,Jiaxuan Gao,Zhiyu Mei,Wei Fu,Haohui Mai,Wei Wang,Yi Wu,Binhang Yuan*

Main category: cs.DC

TL;DR: 本文提出异质性感知异步RL训练系统AReaL - Hex，可在异构GPU上调度执行，提升训练吞吐量和降低成本。


<details>
  <summary>Details</summary>
Motivation: 最大化大语言模型强化学习（RL）的训练吞吐量和成本效益，以推广该技术，且RL训练各阶段特性不同，为异构部署带来挑战和机会。

Method: 采用两阶段调度器，一是用MILP进行约束搜索以选择并行策略和工作负载分配，二是进行图分区以分配异构GPU和互连资源。

Result: 在数学推理任务上，与最先进异步RL系统的同构部署相比，相同预算下训练吞吐量最高提升1.50倍，相同吞吐量下训练成本最高降低1.46倍。

Conclusion: AReaL - Hex能有效在异构GPU上调度RL训练，提升训练吞吐量和成本效益。

Abstract: Maximizing training throughput and cost-efficiency of RL for LLMs is
essential to democratize this advanced technique. One promising but challenging
approach is to deploy such a computational workflow over heterogeneous GPUs.
Unlike conventional large-scale LLM pretraining, RL training generally
decomposes into three coupled stages, i.e., rollout generation, reward
computation, and policy/value updates, which exhibit markedly different compute
intensities, memory footprints, and communication patterns. Recent research
shows that fully asynchronous RL training can disaggregate these stages across
disjoint hardware pools without sacrificing training stability, creating a
great opportunity for real-world heterogeneous deployment. To this end, we
present AReaL-Hex, a heterogeneity-aware asynchronous RL training system that
effectively schedules how to execute rollout generation and policy model
training over heterogeneous GPUs while enforcing data staleness bounds.
Concretely, we use a two-phase scheduler: (i) a constrained search with MILP to
select per-stage parallelization strategies and workload assignments given a
resource budget, and (ii) a graph-partitioning step that allocates
heterogeneous GPUs and interconnects to maximize end-to-end throughput. Built
atop a fully asynchronous RL architecture, AReaL-Hex maps HBM-I/O-bound
generation and compute-bound optimization to more cost-efficient resources and
balances their producer-consumer interactions to avoid both idleness and stale
rollout trajectories. On the mathematical reasoning task with various model
scales (1.5B, 7B, and 14B), compared to homogeneous deployments of
state-of-the-art asynchronous RL systems: (i) When maintaining the same total
budgets, AReaL-Hex delivers up to 1.50x higher training throughput; (ii) When
achieving the same training throughput, AReaL-Hex results in up to 1.46x
reduction in training cost.

</details>


### [83] [FREESH: Fair, Resource- and Energy-Efficient Scheduling for LLM Serving on Heterogeneous GPUs](https://arxiv.org/abs/2511.00807)
*Xuan He,Zequan Fang,Jinzhao Lian,Danny H. K. Tsang,Baosen Zhang,Yize Chen*

Main category: cs.DC

TL;DR: 针对LLM服务系统，提出FREESH方法，可在生产工作负载中节能降排并提升性能。


<details>
  <summary>Details</summary>
Motivation: LLM和AI代理对计算和能源需求不断增加，异构GPU集群地理分布且负载多样，需优化LLM服务系统以降低碳和能源目标。

Method: 考虑联合路由和调度问题，提出FREESH，匹配GPU特性与查询负载，确定优化并行性、查询路由和动态GPU频率缩放，采用LLF服务策略。

Result: 在1小时生产工作负载服务中，FREESH节能28.6%，减排45.45%，提升SLO达标率和公平性。

Conclusion: FREESH方法能有效降低LLM服务系统的能源消耗和碳排放，同时保证性能和公平性。

Abstract: The ever-increasing computation and energy demand for LLM and AI agents call
for holistic and efficient optimization of LLM serving systems. In practice,
heterogeneous GPU clusters can be deployed in a geographically distributed
manner, while LLM load also observes diversity in terms of both query traffic
and serving patterns. LLM queries running on advanced GPUs during a
high-emission hour at one location can lead to significantly higher carbon
footprints versus same queries running on mid-level GPUs at a low-emission time
and location. By observing LLM serving requirements and leveraging
spatiotemporal computation flexibility, we consider the joint routing and
scheduling problem, and propose FREESH to cooperatively run a group of data
centers while minimizing user-specified carbon or energy objectives. FREESH
identifies the optimal configurations of balanced load serving by matching
distinct GPU instance's power-throughput characteristics with predictable LLM
query length and workloads. To ensure both latency and fairness requirements,
FREESH identifies optimized parallelism and query routing schedules together
with dynamic GPU frequency scaling for power saving, and Least-Laxity-First
(LLF) serving strategy for query scheduling. During the 1-hour serving on
production workloads, FREESH reduces energy by 28.6% and emissions by 45.45%
together with improvements in SLO attainment and fairness.

</details>


### [84] [Towards Portability at Scale: A Cross-Architecture Performance Evaluation of a GPU-enabled Shallow Water Solver](https://arxiv.org/abs/2511.01001)
*Johansell Villalobos,Daniel Caviedes-Voullième,Silvio Rizzi,Esteban Meneses*

Main category: cs.DC

TL;DR: 对SERGHEI - SWE求解器在四个异构HPC系统上进行性能研究，展示了可扩展性，指出内存带宽是瓶颈，有性能优化空间。


<details>
  <summary>Details</summary>
Motivation: 气候变化使数值建模面临挑战，水文建模对高分辨率实时模拟需求增加，需研究求解器性能。

Method: 在四个先进异构HPC系统上对SERGHEI - SWE求解器进行性能研究，评估强扩展和弱扩展，进行Roofline分析，用调和与算术平均指标评估性能可移植性。

Result: 求解器具有一致可扩展性，速度提升32倍，多数测试范围效率超90%；内存带宽是性能瓶颈；在调整问题大小后求解器可在设备间实现可移植性。

Conclusion: SERGHEI - SWE是适用于大规模地球物理应用的强大、可扩展和可移植的模拟工具，有提升性能的潜力。

Abstract: Current climate change has posed a grand challenge in the field of numerical
modeling due to its complex, multiscale dynamics. In hydrological modeling, the
increasing demand for high-resolution, real-time simulations has led to the
adoption of GPU-accelerated platforms and performance portable programming
frameworks such as Kokkos. In this work, we present a comprehensive performance
study of the SERGHEI-SWE solver, a shallow water equations code, across four
state-of-the-art heterogeneous HPC systems: Frontier (AMD MI250X), JUWELS
Booster (NVIDIA A100), JEDI (NVIDIA H100), and Aurora (Intel Max 1550). We
assess strong scaling up to 1024 GPUs and weak scaling upwards of 2048 GPUs,
demonstrating consistent scalability with a speedup of 32 and an efficiency
upwards of 90\% for most almost all the test range. Roofline analysis reveals
that memory bandwidth is the dominant performance bottleneck, with key solver
kernels residing in the memory-bound region. To evaluate performance
portability, we apply both harmonic and arithmetic mean-based metrics while
varying problem size. Results indicate that while SERGHEI-SWE achieves
portability across devices with tuned problem sizes (<70\%), there is room for
kernel optimization within the solver with more granular control of the
architecture specifically by using Kokkos teams and architecture specific
tunable parameters. These findings position SERGHEI-SWE as a robust, scalable,
and portable simulation tool for large-scale geophysical applications under
evolving HPC architectures with potential to enhance its performance.

</details>


### [85] [Neuro-Inspired Task Offloading in Edge-IoT Networks Using Spiking Neural Networks](https://arxiv.org/abs/2511.01127)
*Fabio Diniz Rossi*

Main category: cs.DC

TL;DR: 本文提出基于脉冲神经网络的任务卸载框架，能实时、节能地编排任务，在多种场景下效果优于传统策略。


<details>
  <summary>Details</summary>
Motivation: 传统边缘计算任务卸载策略不适用于高动态和资源受限环境。

Method: 提出基于脉冲神经网络的任务卸载框架，将基于SNN的决策模块集成到边缘节点，并使用YAFS和Brian2的混合仿真环境评估模型。

Result: 该框架显著降低任务处理延迟和能耗，提高任务成功率，在高负载条件下，相比传统策略，延迟最多降低26%，能耗最多降低32%，成功率最多提高25%。

Conclusion: 基于脉冲神经网络的任务卸载框架在性能上优于传统策略，适用于边缘计算。

Abstract: Traditional task offloading strategies in edge computing often rely on static
heuristics or data-intensive machine learning models, which are not always
suitable for highly dynamic and resource-constrained environments. In this
paper, we propose a novel task-offloading framework based on Spiking Neural
Networks inspired by the efficiency and adaptability of biological neural
systems. Our approach integrates an SNN-based decision module into edge nodes
to perform real-time, energy-efficient task orchestration. We evaluate the
model under various IoT workload scenarios using a hybrid simulation
environment composed of YAFS and Brian2. The results demonstrate that our
SNN-based framework significantly reduces task processing latency and energy
consumption while improving task success rates. Compared to traditional
heuristic and ML-based strategies, our model achieves up to 26% lower latency,
32% less energy consumption, and 25\% higher success rate under high-load
conditions.

</details>


### [86] [Scalable Maxflow Processing for Dynamic Graphs](https://arxiv.org/abs/2511.01235)
*Shruthi Kannappan,Ashwina Kumar,Rupesh Nasre*

Main category: cs.DC

TL;DR: 本文提出用于动态图的增量式GPU并行最大流算法、静态图的高性能GPU算法，并给出CUDA优化策略。


<details>
  <summary>Details</summary>
Motivation: 最大流问题应用广泛，Push - Relabel算法适合并行化，因此研究GPU加速的最大流计算。

Method: 提出动态图增量式重计算最大流的GPU并行算法、静态图初始最大流的高性能GPU算法，以及一系列CUDA实现优化。

Result: 无明确提及

Conclusion: 无明确提及

Abstract: The Maximum Flow (Max-Flow) problem is a cornerstone in graph theory and
combinatorial optimization, aiming to determine the largest possible flow from
a designated source node to a sink node within a capacitated flow network. It
has extensive applications across diverse domains such as computer networking,
transportation systems, and image segmentation. The objective is to maximize
the total throughput while respecting edge capacity constraints and maintaining
flow conservation at all intermediate vertices.
  Among the various algorithms proposed for solving the Max-Flow problem, the
Push--Relabel algorithm is particularly notable for its efficiency and
suitability for parallelization, owing to its localized vertex-based
operations. This property has motivated extensive research into GPU-accelerated
Max-Flow computation, leveraging the high degree of parallelism inherent to
modern GPU architectures.
  In this paper, we present a novel GPU-parallel Max-Flow algorithm capable of
incrementally recomputing the maximum flow of a dynamic graph following a batch
of edge updates. In addition, we introduce a high-performance static GPU
algorithm designed for efficiently computing the initial Max-Flow on static
graphs. We further describe a series of CUDA-specific implementation
optimizations that enhance performance, scalability, and memory efficiency on
GPU platforms.

</details>


### [87] [Design of quasi phase matching crystal based on differential gray wolf algorithm](https://arxiv.org/abs/2511.01255)
*He Chen,ZiHua Zheng,JingHua Sun*

Main category: cs.DC

TL;DR: 本文提出hwsda混合优化算法与GPU并行加速技术融合方案，解决非周期极化晶体性能优化难题，提升设计效率。


<details>
  <summary>Details</summary>
Motivation: 传统算法在非周期极化晶体性能优化中存在收敛慢、易陷入局部最优及计算效率低等问题。

Method: 采用差分进化算法进行全局搜索，灰狼优化算法加强局部搜索和收敛速度，结合GPU多核架构实现线程级并行计算。

Result: 有效突破高维离散空间优化问题，提高晶体畴控制精度，设计效率较传统CPU串行计算提升数百到数千倍。

Conclusion: 该方案为复杂非线性光学器件设计提供新范式，助力相关器件性能突破和产业应用。

Abstract: This paper focuses on the key problem in the development of nonlinear optical
technology, the performance optimization of aperiodically polarized crystals.
The performance of the crystal depends on the precise control of the micro
distribution of crystal domains, but its optimization belongs to the
high-dimensional discrete combination "NP hard" problem. The traditional
algorithm has the bottleneck of slow convergence and easy to fall into local
optimization, while the heuristic methods such as genetic algorithm are limited
by the CPU serial calculation and inefficient. In order to solve the above
challenges, this paper proposes the fusion scheme of hwsda hybrid optimization
algorithm and GPU parallel acceleration technology: the differential evolution
algorithm (DE) is used to realize the global search, and the gray wolf
optimization algorithm (GWO) is used to strengthen the local search and
convergence speed, and the two coordinate to balance the global and local
optimization requirements; At the same time, it relies on GPU multi-core
architecture to realize thread level parallel computing and improve
optimization efficiency. This scheme effectively breaks through the
optimization problem of high-dimensional discrete space, improves the accuracy
of crystal domain control, improves the efficiency of quasi phase matching
design by hundreds to thousands of times compared with traditional CPU serial
computing, provides a new paradigm for the design of complex nonlinear optical
devices, and helps promote the performance breakthrough and industrial
application of related devices in the fields of quantum optics and laser
processing.

</details>


### [88] [Transformer-Based Sparse CSI Estimation for Non-Stationary Channels](https://arxiv.org/abs/2511.01333)
*Muhammad Ahmed Mohsin,Muhammad Umer,Ahsan Bilal,Hassan Rizwan,Sagnik Bhattacharya,Muhammad Ali Jamshed,John M. Cioffi*

Main category: cs.DC

TL;DR: 提出基于注意力机制的框架用于非平稳无线系统信道状态信息估计，性能优于基线，降低开销。


<details>
  <summary>Details</summary>
Motivation: 传统方法在非平稳条件下估计信道状态信息存在开销大、性能下降等问题。

Method: 提出基于Flash - Attention的Transformer框架，结合模型驱动和数据驱动，采用逐块自注意力和物理感知复合损失函数。

Result: 在标准化3GPP NR配置下，相不变归一化均方误差比LMMSE和LSTM基线低约13dB，误码率更低，将导频开销降低16倍。

Conclusion: 基于注意力的架构能在不影响链路质量下实现可靠CSI恢复和提高频谱效率，解决非平稳5G及未来网络低开销信道估计的瓶颈。

Abstract: Accurate and efficient estimation of Channel State Information (CSI) is
critical for next-generation wireless systems operating under non-stationary
conditions, where user mobility, Doppler spread, and multipath dynamics rapidly
alter channel statistics. Conventional pilot aided estimators incur substantial
overhead, while deep learning approaches degrade under dynamic pilot patterns
and time varying fading. This paper presents a pilot-aided Flash-Attention
Transformer framework that unifies model-driven pilot acquisition with data
driven CSI reconstruction through patch-wise self-attention and a physics aware
composite loss function enforcing phase alignment, correlation consistency, and
time frequency smoothness. Under a standardized 3GPP NR configuration, the
proposed framework outperforms LMMSE and LSTM baselines by approximately 13 dB
in phase invariant normalized mean-square error (NMSE) with markedly lower
bit-error rate (BER), while reducing pilot overhead by 16 times. These results
demonstrate that attention based architectures enable reliable CSI recovery and
enhanced spectral efficiency without compromising link quality, addressing a
fundamental bottleneck in adaptive, low-overhead channel estimation for
non-stationary 5G and beyond-5G networks.

</details>


### [89] [Gradient Clock Synchronization with Practically Constant Local Skew](https://arxiv.org/abs/2511.01420)
*Christoph Lenzen*

Main category: cs.DC

TL;DR: 本文针对梯度时钟同步（GCS）问题，提出改进模型和新分析方法，在一般条件下大幅改进结果，还实现自稳定并扩展到外部同步场景。


<details>
  <summary>Details</summary>
Motivation: 现有GCS渐近最优边界在实际应用中有关键缺陷，且现有同步方法无法对本地偏差提供非平凡保证。

Method: 提出改进模型，仅要求测量和频率误差的稳定性，以此绕过现有下界。

Result: 在一般条件下实现显著改进，突破既定下界，限制本地振荡器影响，并实现自稳定，可扩展到外部同步场景。

Conclusion: 改进的模型和分析方法有效，能在GCS问题上取得更好结果，且可扩展到外部同步。

Abstract: Gradient Clock Synchronization (GCS) is the task of minimizing the local
skew, i.e., the clock offset between neighboring clocks, in a larger network.
While asymptotically optimal bounds are known, from a practical perspective
they have crucial shortcomings:
  - Local skew bounds are determined by upper bounds on offset estimation that
need to be guaranteed throughout the entire lifetime of the system.
  - Worst-case frequency deviations of local oscillators from their nominal
rate are assumed, yet frequencies tend to be much more stable in the (relevant)
short term.
  State-of-the-art deployed synchronization methods adapt to the true offset
measurement and frequency errors, but achieve no non-trivial guarantees on the
local skew.
  In this work, we provide a refined model and novel analysis of existing
techniques for solving GCS in this model. By requiring only stability of
measurement and frequency errors, we can circumvent existing lower bounds,
leading to dramatic improvements under very general conditions. For example, if
links exhibit a uniform worst-case estimation error of $\Delta$ and a change in
estimation errors of $\delta\ll \Delta$ on relevant time scales, we bound the
local skew by $O(\Delta+\delta \log D)$ for networks of diameter $D$,
effectively ``breaking'' the established $\Omega(\Delta\log D)$ lower bound,
which holds when $\delta=\Delta$. Similarly, we show how to limit the influence
of local oscillators on $\delta$ to scale with the change of frequency of an
individual oscillator on relevant time scales, rather than a worst-case bound
over all oscillators and the lifetime of the system.
  Moreover, we show how to ensure self-stabilization in this challenging
setting. Last, but not least, we extend all of our results to the scenario of
external synchronization, at the cost of a limited increase in stabilization
time.

</details>


### [90] [Adaptive Multidimensional Quadrature on Multi-GPU Systems](https://arxiv.org/abs/2511.01573)
*Melanie Tonarelli,Simone Riva,Pietro Benedusi,Fabrizio Ferrandi,Rolf Krause*

Main category: cs.DC

TL;DR: 介绍了一种分布式自适应积分方法，可解决多 GPU 架构上多维积分问题，还提出负载重分配方案，比现有方案更高效、稳健。


<details>
  <summary>Details</summary>
Motivation: 解决多 GPU 架构上自适应积分过程中的负载不平衡问题。

Method: 将多维积分表示为多 GPU 架构上的分层域分解问题，通过局部误差估计器指导子域细化，采用基于循环轮询策略的分散式负载重分配方案。

Result: 通过非阻塞、支持 CUDA 的 MPI 通信动态重新平衡设备间的子域。

Conclusion: 该策略比现有 GPU 定制包在高维上更高效，对被积函数正则性和目标精度的鲁棒性更好。

Abstract: We introduce a distributed adaptive quadrature method that formulates
multidimensional integration as a hierarchical domain decomposition problem on
multi-GPU architectures. The integration domain is recursively partitioned into
subdomains whose refinement is guided by local error estimators. Each subdomain
evolves independently on a GPU, which exposes a significant load imbalance as
the adaptive process progresses. To address this challenge, we introduce a
decentralised load redistribution schemes based on a cyclic round-robin policy.
This strategy dynamically rebalance subdomains across devices through
non-blocking, CUDA-aware MPI communication that overlaps with computation. The
proposed strategy has two main advantages compared to a state-of-the-art
GPU-tailored package: higher efficiency in high dimensions; and improved
robustness w.r.t the integrand regularity and the target accuracy.

</details>


### [91] [LARK - Linearizability Algorithms for Replicated Keys in Aerospike](https://arxiv.org/abs/2511.01843)
*Andrew Goodng,Kevin Porter,Thomas Lopatic,Ashish Shinde,Sunil Sayyaparaju,Srinivasan Seshadri,V. Srinivasan*

Main category: cs.DC

TL;DR: 介绍同步复制协议LARK，能实现线性化，降低延迟和成本，比传统协议可用性高。


<details>
  <summary>Details</summary>
Motivation: 设计一种能在实现线性化的同时，降低延迟和基础设施成本，提高可用性的同步复制协议。

Method: 引入Partition Availability Conditions (PAC)，消除有序日志。

Result: 在容忍故障时提高分区可用性，在同等存储预算下能持续提交，可实现零停机滚动重启。

Conclusion: LARK取得显著的可用性提升。

Abstract: We present LARK (Linearizability Algorithms for Replicated Keys), a
synchronous replication protocol that achieves linearizability while minimizing
latency and infrastructure cost, at significantly higher availability than
traditional quorum-log consensus. LARK introduces Partition Availability
Conditions (PAC) that reason over the entire database cluster rather than fixed
replica sets, improving partition availability under independent failures by
roughly 3x when tolerating one failure and 10x when tolerating two. Unlike
Raft, Paxos, and Viewstamped Replication, LARK eliminates ordered logs,
enabling immediate partition readiness after leader changes -- with at most a
per-key duplicate-resolution round trip when the new leader lacks the latest
copy. Under equal storage budgets -- where both systems maintain only f+1 data
copies to tolerate f failures -- LARK continues committing through data-node
failures while log-based protocols must pause commits for replica rebuilding.
These properties also enable zero-downtime rolling restarts even when
maintaining only two copies. We provide formal safety arguments and a TLA+
specification, and we demonstrate through analysis and experiments that LARK
achieves significant availability gains.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [92] [Sorting by Strip Swaps is NP-Hard](https://arxiv.org/abs/2511.00015)
*Swapnoneel Roy,Asai Asaithambi,Debajyoti Mukhopadhyay*

Main category: cs.DS

TL;DR: 通过块排序的多项式约简证明排序条交换问题（SbSS）是NP难的。


<details>
  <summary>Details</summary>
Motivation: 证明排序条交换问题（SbSS）的复杂度

Method: 使用多项式约简，通过局部小装置“笼子”和“铰链”小装置将问题转化，建立条交换调度和块调度的等价关系。

Result: 建立了精确的条交换调度和完美块调度之间的等价关系。

Conclusion: 证明了排序条交换问题（SbSS）是NP难的。

Abstract: We show that \emph{Sorting by Strip Swaps} (SbSS) is NP-hard by a polynomial
reduction of \emph{Block Sorting}. The key idea is a local gadget, a
\emph{cage}, that replaces every decreasing adjacency $(a_i,a_{i+1})$ by a
guarded triple $a_i,m_i,a_{i+1}$ enclosed by guards $L_i,U_i$, so the only
decreasing adjacencies are the two inside the cage. Small \emph{hinge} gadgets
couple adjacent cages that share an element and enforce that a strip swap that
removes exactly two adjacencies corresponds bijectively to a block move that
removes exactly one decreasing adjacency in the source permutation. This yields
a clean equivalence between exact SbSS schedules and perfect block schedules,
establishing NP-hardness.

</details>


### [93] [Scheduling Problems with Constrained Rejections](https://arxiv.org/abs/2511.00184)
*Sami Davies,Venkatesan Guruswami,Xuandi Ren*

Main category: cs.DS

TL;DR: 研究带约束拒绝数量的双目标Makespan Minimization和Santa Claus问题，改进Makespan Minimization调度比例，给出Santa Claus问题首个硬度结果。


<details>
  <summary>Details</summary>
Motivation: 研究双目标调度问题，探索原优化目标难度的鲁棒性。

Method: 改进已有算法提高Makespan Minimization调度比例；引入双目标Set Packing问题证明Santa Claus问题硬度。

Result: Makespan Minimization在makespan为3T/2时调度比例提高到0.6533；证明Santa Claus问题存在NP难情况。

Conclusion: 双目标调度问题值得进一步研究，可用于理解原优化目标难度的鲁棒性。

Abstract: We study bicriteria versions of Makespan Minimization on Unrelated Machines
and Santa Claus by allowing a constrained number of rejections. Given an
instance of Makespan Minimization on Unrelated Machines where the optimal
makespan for scheduling $n$ jobs on $m$ unrelated machines is $T$, (Feige and
Vondr\'ak, 2006) gave an algorithm that schedules a $(1-1/e+10^{-180})$
fraction of jobs in time $T$. We show the ratio can be improved to
$0.6533>1-1/e+0.02$ if we allow makespan $3T/2$. To the best our knowledge,
this is the first result examining the tradeoff between makespan and the
fraction of scheduled jobs when the makespan is not $T$ or $2T$.
  For the Santa Claus problem (the Max-Min version of Makespan Minimization),
the analogous bicriteria objective was studied by (Golovin, 2005), who gave an
algorithm providing an allocation so a $(1-1/k)$ fraction of agents receive
value at least $T/k$, for any $k \in \mathbb{Z}^+$ and $T$ being the optimal
minimum value every agent can receive. We provide the first hardness result by
showing there are constants $\delta,\varepsilon>0$ such that it is NP-hard to
find an allocation where a $(1-\delta)$ fraction of agents receive value at
least $(1-\varepsilon) T$. To prove this hardness result, we introduce a
bicriteria version of Set Packing, which may be of independent interest, and
prove some algorithmic and hardness results for it. Overall, we believe these
bicriteria scheduling problems warrant further study as they provide an
interesting lens to understand how robust the difficulty of the original
optimization goal might be.

</details>


### [94] [Uncrossed Multiflows and Applications to Disjoint Paths](https://arxiv.org/abs/2511.00254)
*Chandra Chekuri,Guyslain Naves,Joseph Poremba,F. Bruce Shepherd*

Main category: cs.DS

TL;DR: 本文研究一般平面图中无交叉多流问题，分析拥塞和最大化模型，给出复杂度、不可近似性结果及整数解舍入方法。


<details>
  <summary>Details</summary>
Motivation: 此前无交叉多流在特定情况有应用，本文研究一般平面图中作为独立算法问题的情况。

Method: 对拥塞和最大化模型分别进行分析，研究问题复杂度、不可近似性及整数解舍入。

Result: 拥塞模型中判断有无无交叉分数多流是NP - 难，需求跨有限面时找整数无交叉流可多项式时间求解；最大化模型有强不可近似性结果；给出整数解舍入方法。

Conclusion: 在一般平面图无交叉多流问题上取得了复杂度、不可近似性和舍入方法等成果，推广了全平面图的相关界限。

Abstract: A multiflow in a planar graph is uncrossed if the curves identified by its
support paths do not cross in the plane. Such flows have played a role in
previous routing algorithms, including Schrijver's Homotopy Method and
unsplittable flows in directed planar single-source instances. Recently
uncrossed flows have played a key role in approximation algorithms for maximum
disjoint paths in fully-planar instances, where the combined supply plus demand
graph is planar. In the fully-planar case, any fractional multiflow can be
converted into one that is uncrossed, which is then exploited to find a good
rounding of the fractional solution. We investigate finding an uncrossed
multiflow as a standalone algorithmic problem in general planar instances (not
necessarily fully-planar). We consider both a congestion model where the given
demands must all be routed, and a maximization model where the goal is to pack
as much flow in the supply graph as possible (not necessarily equitably).
  For the congestion model, we show that determining if an instance has an
uncrossed (fractional) multiflow is NP-hard, but the problem of finding an
integral uncrossed flow is polytime solvable if the demands span a bounded
number of faces. For the maximization model, we present a strong (almost
polynomial) inapproximability result. Regarding integrality gaps, for
maximization we show that an uncrossed multiflow in a planar instance can
always be rounded to an integral multiflow with a constant fraction of the
original value. This holds in both the edge-capacitated and node-capacitated
settings, and generalizes earlier bounds for fully-planar instances. In the
congestion model, given an uncrossed fractional multiflow, we give a rounding
procedure that produces an integral multiflow with edge congestion 2, which can
be made unsplittable with an additional additive error of the maximum demand.

</details>


### [95] [An Approximation Algorithm for Monotone Submodular Cost Allocation](https://arxiv.org/abs/2511.00470)
*Ryuhei Mizutani*

Main category: cs.DS

TL;DR: 本文研究单调最小子模成本分配问题（Mono - MSCA），给出LP松弛，证明其整性间隙上界为k/2，得到k/2近似算法，且固定k时整性间隙下界为k/2 - ε。


<details>
  <summary>Details</summary>
Motivation: 研究单调最小子模成本分配问题（Mono - MSCA）的求解方法。

Method: 为Mono - MSCA提供自然的LP松弛，与Chekuri和Ene引入的凸规划松弛等价，并分析其整性间隙。

Result: LP松弛的整性间隙至多为k/2，得到k/2近似算法；固定k时，对于任意常数ε>0，整性间隙至少为k/2 - ε。

Conclusion: 所提出的LP松弛方法可用于解决Mono - MSCA问题，且具有较好的近似性能。

Abstract: In this paper, we consider the minimum submodular cost allocation (MSCA)
problem. The input of MSCA is $k$ non-negative submodular functions
$f_1,\ldots,f_k$ on the ground set $N$ given by evaluation oracles, and the
goal is to partition $N$ into $k$ (possibly empty) sets $X_1,\ldots,X_k$ so
that $\sum_{i=1}^k f_i(X_i)$ is minimized. In this paper, we focus on the case
when $f_1,\ldots,f_k$ are monotone (denoted by Mono-MSCA). We provide a natural
LP-relaxation for Mono-MSCA, which is equivalent to the convex program
relaxation introduced by Chekuri and Ene. We show that the integrality gap of
the LP-relaxation is at most $k/2$, which yields a $k/2$-approximation
algorithm for Mono-MSCA. We also show that the integrality gap of the
LP-relaxation is at least $k/2-\epsilon$ for any constant $\epsilon>0$ when $k$
is fixed.

</details>


### [96] [Fast Stochastic Greedy Algorithm for $k$-Submodular Cover Problem](https://arxiv.org/abs/2511.00869)
*Hue T. Nguyen,Tan D. Tran,Nguyen Long Giang,Canh V. Pham*

Main category: cs.DS

TL;DR: 研究k - 子模覆盖问题，提出Fast Stochastic Greedy算法，降低查询复杂度，适用于大规模AI应用。


<details>
  <summary>Details</summary>
Motivation: 现有k - 子模覆盖问题算法近似保证弱或查询复杂度高，无法满足实际需求。

Method: 提出Fast Stochastic Greedy算法。

Result: 该算法实现强双准则近似，大幅降低查询复杂度。

Conclusion: 算法减少函数评估次数，具有高可扩展性，适用于对效率要求高的大规模AI应用。

Abstract: We study the $k$-Submodular Cover ($kSC$) problem, a natural generalization
of the classical Submodular Cover problem that arises in artificial
intelligence and combinatorial optimization tasks such as influence
maximization, resource allocation, and sensor placement. Existing algorithms
for $\kSC$ often provide weak approximation guarantees or incur prohibitively
high query complexity. To overcome these limitations, we propose a \textit{Fast
Stochastic Greedy} algorithm that achieves strong bicriteria approximation
while substantially lowering query complexity compared to state-of-the-art
methods. Our approach dramatically reduces the number of function evaluations,
making it highly scalable and practical for large-scale real-world AI
applications where efficiency is essential.

</details>


### [97] [Dynamic Diameter in High-Dimensions against Adaptive Adversary and Beyond](https://arxiv.org/abs/2511.01065)
*Kiarash Banihashem,Jeff Giliberti,Samira Goudarzi,MohammadTaghi Hajiaghayi,Peyman Jabbarzade,Morteza Monemizadeh*

Main category: cs.DS

TL;DR: 本文研究动态点集直径和k - 中心聚类问题，提出维护2 - 近似直径的全动态算法和改进的(4 + ε) - 近似k - 中心算法，且都能抵抗自适应对手。


<details>
  <summary>Details</summary>
Motivation: 研究在点可插入或删除、维度高且不固定，存在自适应对手情况下，动态点集直径和k - 中心聚类的有效算法。

Method: 通过识别数据集的鲁棒代表并进行仔细的去摊还操作实现直径算法；未提及k - 中心算法具体方法。

Result: 提出维护2 - 近似直径的全动态算法，最坏更新时间为poly(d, log n)；改进k - 中心问题的动态(4 + ε) - 近似算法，摊还更新时间从k^6 d · poly(ε⁻¹, log n) 提升到k^2.5 d · poly(ε⁻¹, log n)。

Conclusion: 这是高维中首个同时实现2 - 近似保证和抵抗自适应对手的高效全动态直径算法，k - 中心算法在摊还更新时间上有改进。

Abstract: In this paper, we study the fundamental problems of maintaining the diameter
and a $k$-center clustering of a dynamic point set $P \subset \mathbb{R}^d$,
where points may be inserted or deleted over time and the ambient dimension $d$
is not constant and may be high. Our focus is on designing algorithms that
remain effective even in the presence of an adaptive adversary -- an adversary
that, at any time $t$, knows the entire history of the algorithm's outputs as
well as all the random bits used by the algorithm up to that point. We present
a fully dynamic algorithm that maintains a $2$-approximate diameter with a
worst-case update time of $\text{poly}(d, \log n)$, where $n$ is the length of
the stream. Our result is achieved by identifying a robust representative of
the dataset that requires infrequent updates, combined with a careful
deamortization. To the best of our knowledge, this is the first efficient
fully-dynamic algorithm for diameter in high dimensions that simultaneously
achieves a 2-approximation guarantee and robustness against an adaptive
adversary. We also give an improved dynamic $(4+\epsilon)$-approximation
algorithm for the $k$-center problem, also resilient to an adaptive adversary.
Our clustering algorithm achieves an amortized update time of $k^{2.5} d \cdot
\text{poly}(\epsilon^{-1}, \log n)$, improving upon the amortized update time
of $k^6 d \cdot \text{poly}(\epsilon^{-1}, \log n)$ by Biabani et al.
[NeurIPS'24].

</details>


### [98] [Fault-Tolerant Approximate Distance Oracles with a Source Set](https://arxiv.org/abs/2511.01239)
*Dipan Dey,Telikepalli Kavitha*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Our input is an undirected weighted graph $G = (V,E)$ on $n$ vertices along
with a source set $S\subseteq V$. The problem is to preprocess $G$ and build a
compact data structure such that upon query $Qu(s,v,f)$ where $(s,v) \in
S\times V$ and $f$ is any faulty edge, we can quickly find a good estimate
(i.e., within a small multiplicative stretch) of the $s$-$v$ distance in $G-f$.
We use a fault-tolerant $ST$-distance oracle from the work of Bil{\`{o}} et al.
(STACS 2018) to construct an $S\times V$ approximate distance oracle or {\em
sourcewise} approximate distance oracle of size $\widetilde{O}(|S|n + n^{3/2})$
with multiplicative stretch at most 5. We construct another fault-tolerant
sourcewise approximate distance oracle of size $\widetilde{O}(|S|n + n^{4/3})$
with multiplicative stretch at most 13. Both the oracles have $O(1)$ query
answering time.

</details>


### [99] [Subtree Mode and Applications](https://arxiv.org/abs/2511.01376)
*Jialong Zhou,Ben Bals,Matei Tinca,Ai Guan,Panagiotis Charalampopoulos,Grigorios Loukides,Solon P. Pissis*

Main category: cs.DS

TL;DR: 本文引入子树模式问题，提出时间最优算法，可在O(N)时间内计算N节点树每个节点的答案，还展示算法在多方面的适应性和优势。


<details>
  <summary>Details</summary>
Motivation: 在文本分析和生物学等领域，数据具有层次结构可表示为树，因此提出子树模式问题。

Method: 提出时间最优算法，可在O(N)时间内计算N节点树每个节点的答案，还展示算法对不同情况的适应性。

Result: 实验表明算法比基线至少快一个数量级且更节省空间，案例研究显示算法在模式挖掘和序列到数据库搜索应用中有效。

Conclusion: 提出的算法在解决子树模式问题上表现出色，具有良好的时间和空间效率，在相关应用中有效，而对于有向无环图类似快速解决方案不太可能。

Abstract: The mode of a collection of values (i.e., the most frequent value in the
collection) is a key summary statistic. Finding the mode in a given range of an
array of values is thus of great importance, and constructing a data structure
to solve this problem is in fact the well-known Range Mode problem. In this
work, we introduce the Subtree Mode (SM) problem, the analogous problem in a
leaf-colored tree, where the task is to compute the most frequent color in the
leaves of the subtree of a given node. SM is motivated by several applications
in domains such as text analytics and biology, where the data are hierarchical
and can thus be represented as a (leaf-colored) tree. Our central contribution
is a time-optimal algorithm for SM that computes the answer for every node of
an input $N$-node tree in $O(N)$ time. We further show how our solution can be
adapted for node-colored trees, or for computing the $k$ most frequent colors,
in the optimal $O(N)$ time, for any given $k=O(1)$. Moreover, we prove that a
similarly fast solution for when the input is a sink-colored directed acyclic
graph instead of a leaf-colored tree is highly unlikely. Our experiments on
real datasets with trees of up to 7.3 billion nodes demonstrate that our
algorithm is faster than baselines by at least one order of magnitude and much
more space efficient. Last, we present case studies showing the effectiveness
of our approach in pattern mining and sequence-to-database search applications.

</details>


### [100] [Robust Streaming Against Low-Memory Adversaries](https://arxiv.org/abs/2511.01769)
*Omri Ben-Eliezer,Krzysztof Onak,Sandeep Silwal*

Main category: cs.DS

TL;DR: 本文研究受限内存对手下的鲁棒流算法，设计了针对无记忆和低记忆对手的算法，解决一类顺序不变问题。


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒流算法中，对抗模型对手能力过强，导致算法性能与非对抗场景差距大，希望通过限制对手内存来缩小这一差距。

Method: 设计针对无记忆和低记忆对手的鲁棒流算法，采用类似计算路径框架的新方法。

Result: 该对抗模型能生成高翻转数和密度的流，排除了多数已知鲁棒化技术；设计出针对无记忆和低记忆对手的高效算法。

Conclusion: 通过限制对手内存可设计出高效鲁棒流算法解决一类顺序不变问题。

Abstract: Robust streaming, the study of streaming algorithms that provably work when
the stream is generated by an adaptive adversary, has seen tremendous progress
in recent years. However, fundamental barriers remain: the best known algorithm
for turnstile $F_p$-estimation in the robust streaming setting is exponentially
worse than in the oblivious setting, and closing this gap seems difficult.
Arguably, one possible cause of this barrier is the adversarial model, which
may be too strong: unlike the space-bounded streaming algorithm, the adversary
can memorize the entire history of the interaction with the algorithm. Can we
then close the exponential gap if we insist that the adversary itself is an
adaptive but low-memory entity, roughly as powerful as (or even weaker than)
the algorithm?
  In this work we present the first set of models and results aimed towards
this question. We design efficient robust streaming algorithms against
adversaries that are fully adaptive but have no long-term memory ("memoryless")
or very little memory of the history of interaction. Roughly speaking, a
memoryless adversary only sees, at any given round, the last output of the
algorithm (and does not even know the current time) and can generate an
unlimited number of independent coin tosses. A low-memory adversary is similar,
but maintains an additional small buffer. While these adversaries may seem
quite limited at first glance, we show that this adversarial model is strong
enough to produce streams that have high flip number and density in the context
of $F_2$-estimation, which rules out most of known robustification techniques.
We then design a new simple approach, similar to the computation paths
framework, to obtain efficient algorithms against memoryless and low-memory
adversaries for a wide class of order-invariant problems.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [101] [Optimal Allocations under Strongly Pigou-Dalton Criteria: Hidden Layer Structure & Efficient Combinatorial Approach](https://arxiv.org/abs/2511.00835)
*Taikun Zhu,Kai Jin,Ruixi Luo,Song Cao*

Main category: cs.GT

TL;DR: 研究m个物品分配给n个具有二元加性或子模估值的代理人的最优社会福利分配，证明最优分配与稳定分配的关系，设计找稳定分配的算法，还分析不同最优分配的切比雪夫距离。


<details>
  <summary>Details</summary>
Motivation: 研究具有二元加性或子模估值的代理人的最优社会福利分配问题。

Method: 理论证明最优分配与稳定分配的关系，设计不同情况下找稳定分配的算法，利用物品和代理人的隐藏层划分将可分物品情况转化为不可分物品情况。

Result: 证明在特定条件下最优分配与稳定分配重合；设计出不可分物品的O(m^2n)时间算法和可分物品的O(m^2n^5)时间算法；得出不同最优分配的切比雪夫距离。

Conclusion: 对于二元加性或子模估值的物品分配问题，有了关于最优分配与稳定分配的关系结论，且设计的算法有效，不同最优分配的切比雪夫距离较小。

Abstract: We investigate optimal social welfare allocations of $m$ items to $n$ agents
with binary additive or submodular valuations. For binary additive valuations,
we prove that the set of optimal allocations coincides with the set of
so-called \emph{stable allocations}, as long as the employed criterion for
evaluating social welfare is strongly Pigou-Dalton (SPD) and symmetric. Many
common criteria are SPD and symmetric, such as Nash social welfare, leximax,
leximin, Gini index, entropy, and envy sum. We also design efficient algorithms
for finding a stable allocation, including an $O(m^2n)$ time algorithm for the
case of indivisible items, and an $O(m^2n^5)$ time one for the case of
divisible items. The first is faster than the existing algorithms or has a
simpler analysis. The latter is the first combinatorial algorithm for that
problem. It utilizes a hidden layer partition of items and agents admitted by
all stable allocations, and cleverly reduces the case of divisible items to the
case of indivisible items.
  In addition, we show that the profiles of different optimal allocations have
a small Chebyshev distance, which is 0 for the case of divisible items under
binary additive valuations, and is at most 1 for the case of indivisible items
under binary submodular valuations.

</details>


### [102] [Pay for The Second-Best Service: A Game-Theoretic Approach Against Dishonest LLM Providers](https://arxiv.org/abs/2511.00847)
*Yuhan Cao,Yu Wang,Sitong Liu,Miao Li,Yixin Tao,Tianxing He*

Main category: cs.GT

TL;DR: 本文从算法博弈论和机制设计角度，解决大语言模型API服务提供商可能的不诚实操纵问题，提出经济模型并给出近似激励兼容机制及相关证明，通过模拟实验验证机制有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型通过API广泛应用，服务提供商存在不诚实操纵的潜在风险，如替换模型、增加计费等。

Method: 运用算法博弈论和机制设计，提出现实用户 - 提供商生态系统的正式经济模型。

Result: 证明对于连续策略空间和任意 ε∈(0, 1/2)，存在近似激励兼容机制，有 O(T^(1 - ε)log T) 的加性近似比和准线性次优用户效用；证明不存在渐近上比该机制更好的机制；模拟实验验证机制有效性。

Conclusion: 提出的机制能有效应对大语言模型API服务提供商的不诚实操纵问题。

Abstract: The widespread adoption of Large Language Models (LLMs) through Application
Programming Interfaces (APIs) induces a critical vulnerability: the potential
for dishonest manipulation by service providers. This manipulation can manifest
in various forms, such as secretly substituting a proclaimed high-performance
model with a low-cost alternative, or inflating responses with meaningless
tokens to increase billing. This work tackles the issue through the lens of
algorithmic game theory and mechanism design. We are the first to propose a
formal economic model for a realistic user-provider ecosystem, where a user can
iteratively delegate $T$ queries to multiple model providers, and providers can
engage in a range of strategic behaviors. As our central contribution, we prove
that for a continuous strategy space and any $\epsilon\in(0,\frac12)$, there
exists an approximate incentive-compatible mechanism with an additive
approximation ratio of $O(T^{1-\epsilon}\log T)$, and a guaranteed quasi-linear
second-best user utility. We also prove an impossibility result, stating that
no mechanism can guarantee an expected user utility that is asymptotically
better than our mechanism. Furthermore, we demonstrate the effectiveness of our
mechanism in simulation experiments with real-world API settings.

</details>


### [103] [Deliberation via Matching](https://arxiv.org/abs/2511.00986)
*Kamesh Munagala,Qilin Ye,Ian Zhang*

Main category: cs.GT

TL;DR: 研究审议式社会选择，提出匹配审议协议，证明协议有3的失真界限，为研究其他审议协议提供框架。


<details>
  <summary>Details</summary>
Motivation: 研究选民通过小组讨论完善偏好后的集体聚合问题，改进无审议锦标赛规则的失真界限。

Method: 引入匹配审议协议，对候选人对形成最大匹配让选民审议，用加权未覆盖集锦标赛规则聚合偏好；通过非线性规划的双线性松弛证明界限。

Result: 协议有3的紧密失真界限，打破无审议锦标赛规则3.11的下限，匹配无审议确定性社会选择规则的下限。

Conclusion: 锦标赛规则在有两两审议的最小附加能力时与一般社会选择规则一样强大，相关刻画为研究其他审议协议失真提供通用分析框架。

Abstract: We study deliberative social choice, where voters refine their preferences
through small-group discussions before collective aggregation. We introduce a
simple and easily implementable deliberation-via-matching protocol: for each
pair of candidates, we form an arbitrary maximum matching among voters who
disagree on that pair, and each matched pair deliberates. The resulting
preferences (individual and deliberative) are then appropriately weighted and
aggregated using the weighted uncovered set tournament rule.
  We show that our protocol has a tight distortion bound of $3$ within the
metric distortion framework. This breaks the previous lower bound of $3.11$ for
tournament rules without deliberation and matches the lower bound for
deterministic social choice rules without deliberation. Our result conceptually
shows that tournament rules are just as powerful as general social choice
rules, when the former are given the minimal added power of pairwise
deliberations. We prove our bounds via a novel bilinear relaxation of the
non-linear program capturing optimal distortion, whose vertices we can
explicitly enumerate, leading to an analytic proof. Loosely speaking, our key
technical insight is that the distortion objective, as a function of metric
distances to any three alternatives, is both supermodular and convex. We
believe this characterization provides a general analytical framework for
studying the distortion of other deliberative protocols, and may be of
independent interest.

</details>


### [104] [From Best Responses to Learning: Investment Efficiency in Dynamic Environment](https://arxiv.org/abs/2511.01157)
*Ce Li,Qianfan Zhang,Weiqiang Zheng*

Main category: cs.GT

TL;DR: 研究动态环境中学习型投资者投资机制的福利，分析近似分配算法福利保证从静态到动态的扩展。


<details>
  <summary>Details</summary>
Motivation: 现实中动态环境信息不完全，投资者无法总是做出最佳响应，传统假设不现实。

Method: 考虑使用无悔在线学习算法的投资者，通过研究近似比衡量算法在动态学习环境中表现。

Result: 在动态环境中，相对事后最佳基准，静态环境近似比不变；给出相对更强时变基准的近似上下界。

Conclusion: 即使代理无法做出最佳响应，在复杂不确定环境中学习投资策略，仍可维持稳健的福利保证。

Abstract: We study the welfare of a mechanism in a dynamic environment where a learning
investor can make a costly investment to change her value. In many real-world
problems, the common assumption that the investor always makes the best
responses, i.e., choosing her utility-maximizing investment option, is
unrealistic due to incomplete information in a dynamically evolving
environment. To address this, we consider an investor who uses a no-regret
online learning algorithm to adaptively select investments through repeated
interactions with the environment. We analyze how the welfare guarantees of
approximation allocation algorithms extend from static to dynamic settings when
the investor learns rather than best-responds, by studying the approximation
ratio for optimal welfare as a measurement of an algorithm's performance
against different benchmarks in the dynamic learning environment. First, we
show that the approximation ratio in the static environment remains unchanged
in the dynamic environment against the best-in-hindsight benchmark. Second, we
provide tight characterizations of the approximation upper and lower bounds
relative to a stronger time-varying benchmark. Bridging mechanism design with
online learning theory, our work shows how robust welfare guarantees can be
maintained even when an agent cannot make best responses but learns their
investment strategies in complex, uncertain environments.

</details>


### [105] [Designing Non-monetary Intersection Control Mechanisms for Efficient Selfish Routing](https://arxiv.org/abs/2511.01421)
*Yusuf Saltan,Jyun-Jhe Wang,Arda Kosay,Chung-Wei Lin,Muhammed O. Sayin*

Main category: cs.GT

TL;DR: 提出非货币机制调整请求时间戳以激励城市交通高效路由，实验显示可减少效率差距。


<details>
  <summary>Details</summary>
Motivation: 城市交通拥堵源于个体路由决策与社会最优流量不一致，现有交叉路口控制方案忽视司机策略行为。

Method: 提出非货币机制调整请求时间戳，采用分层架构，建立分析模型，将规划者问题表述为离线双层优化程序。

Result: 在Sioux Falls网络实验中，平衡流与最优流的效率差距最多减少68%。

Conclusion: 该机制具有可扩展性和有效性。

Abstract: Urban traffic congestion stems from the misalignment between self-interested
routing decisions and socially optimal flows. Intersections, as critical
bottlenecks, amplify these inefficiencies because existing control schemes
often neglect drivers' strategic behavior. Autonomous intersections, enabled by
vehicle-to-infrastructure communication, permit vehicle-level scheduling based
on individual requests. Leveraging this fine-grained control, we propose a
non-monetary mechanism that strategically adjusts request timestamps-delaying
or advancing passage times-to incentivize socially efficient routing. We
present a hierarchical architecture separating local scheduling by roadside
units from network-wide timestamp adjustments by a central planner. We
establish an experimentally validated analytical model, prove the existence and
essential uniqueness of equilibrium flows and formulate the planner's problem
as an offline bilevel optimization program solvable with standard tools.
Experiments on the Sioux Falls network show up to a 68% reduction in the
efficiency gap between equilibrium and optimal flows, demonstrating scalability
and effectiveness.

</details>


### [106] [Proximal Regret and Proximal Correlated Equilibria: A New Tractable Solution Concept for Online Learning and Games](https://arxiv.org/abs/2511.01852)
*Yang Cai,Constantinos Daskalakis,Haipeng Luo,Chen-Yu Wei,Weiqiang Zheng*

Main category: cs.GT

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Learning and computation of equilibria are central problems in algorithmic
game theory. In this work, we introduce proximal regret, a new notion of regret
based on proximal operators that lies strictly between external and swap
regret. When every player employs a no-proximal-regret algorithm in a general
convex game, the empirical distribution of play converges to proximal
correlated equilibria (PCE), a refinement of coarse correlated equilibria. Our
framework unifies several emerging notions in online learning and game theory
-- such as gradient equilibrium and semicoarse correlated equilibrium -- and
introduces new ones. Our main result shows that the classic Online Gradient
Descent (GD) algorithm achieves an optimal $O(\sqrt{T})$ bound on proximal
regret, revealing that GD, without modification, minimizes a stronger regret
notion than external regret. This provides a new explanation for the
empirically superior performance of gradient descent in online learning and
games. We further extend our analysis to Mirror Descent in the Bregman setting
and to Optimistic Gradient Descent, which yields faster convergence in smooth
convex games.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [107] [LookSync: Large-Scale Visual Product Search System for AI-Generated Fashion Looks](https://arxiv.org/abs/2511.00072)
*Pradeep M,Ritesh Pallod,Satyen Abrol,Muthu Raman,Ian Anderson*

Main category: cs.IR

TL;DR: 提出端到端产品搜索系统，已在真实互联网环境部署，用人工判断评估推荐质量，CLIP 表现最佳。


<details>
  <summary>Details</summary>
Motivation: 生成式 AI 重塑时尚，需找到与 AI 生成风格匹配的真实产品。

Method: 构建包含查询生成、向量化、候选检索和重排序的搜索管道，用人工判断准确性分数评估推荐质量。

Result: 系统每天服务超 35 万 AI 造型，覆盖超 1200 万产品；实验中 CLIP 在平均意见得分上比其他模型高 3 - 7%。

Conclusion: CLIP 是生产部署中最可靠的主干模型。

Abstract: Generative AI is reshaping fashion by enabling virtual looks and avatars
making it essential to find real products that best match AI-generated styles.
We propose an end-to-end product search system that has been deployed in a
real-world, internet scale which ensures that AI-generated looks presented to
users are matched with the most visually and semantically similar products from
the indexed vector space. The search pipeline is composed of four key
components: query generation, vectorization, candidate retrieval, and reranking
based on AI-generated looks. Recommendation quality is evaluated using
human-judged accuracy scores. The system currently serves more than 350,000 AI
Looks in production per day, covering diverse product categories across global
markets of over 12 million products. In our experiments, we observed that
across multiple annotators and categories, CLIP outperformed alternative models
by a small relative margin of 3--7\% in mean opinion scores. These
improvements, though modest in absolute numbers, resulted in noticeably better
user perception matches, establishing CLIP as the most reliable backbone for
production deployment.

</details>


### [108] [Effectiveness of LLMs in Temporal User Profiling for Recommendation](https://arxiv.org/abs/2511.00176)
*Milad Sabouri,Masoud Mansoury,Kun Lin,Bamshad Mobasher*

Main category: cs.IR

TL;DR: 本文探讨用大语言模型（LLMs）捕捉用户偏好的时间动态，发现其在用户参与度高的领域能提升推荐质量，但在稀疏环境中效果不明显，还指出性能与计算成本的权衡，及方法的可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统用户画像忽略短期兴趣和长期偏好区别，为提升推荐准确性和系统透明度，研究利用LLMs捕捉用户偏好的时间动态。

Method: 利用LLMs通过交互历史的短期和长期文本摘要生成更丰富的用户表示。

Result: LLMs在用户参与度高的领域提升推荐质量，在稀疏环境中效果不明显，不同领域短期和长期偏好可区分性不同影响效果。

Conclusion: 指出性能与计算成本的权衡，强调LLM驱动方法有可解释性，为开发自适应和透明推荐系统提供新研究方向。

Abstract: Effectively modeling the dynamic nature of user preferences is crucial for
enhancing recommendation accuracy and fostering transparency in recommender
systems. Traditional user profiling often overlooks the distinction between
transitory short-term interests and stable long-term preferences. This paper
examines the capability of leveraging Large Language Models (LLMs) to capture
these temporal dynamics, generating richer user representations through
distinct short-term and long-term textual summaries of interaction histories.
Our observations suggest that while LLMs tend to improve recommendation quality
in domains with more active user engagement, their benefits appear less
pronounced in sparser environments. This disparity likely stems from the
varying distinguishability of short-term and long-term preferences across
domains; the approach shows greater utility where these temporal interests are
more clearly separable (e.g., Movies\&TV) compared to domains with more stable
user profiles (e.g., Video Games). This highlights a critical trade-off between
enhanced performance and computational costs, suggesting context-dependent LLM
application. Beyond predictive capability, this LLM-driven approach inherently
provides an intrinsic potential for interpretability through its natural
language profiles and attention weights. This work contributes insights into
the practical capability and inherent interpretability of LLM-driven temporal
user profiling, outlining new research directions for developing adaptive and
transparent recommender systems.

</details>


### [109] [Simple and Behavior-Driven Augmentation for Recommendation with Rich Collaborative Signals](https://arxiv.org/abs/2511.00436)
*Doyun Choi,Cheonwoo Lee,Jaemin Yoo*

Main category: cs.IR

TL;DR: 提出用于图协同过滤的简单协作增强方法SCAR，实验表明其性能优于先前方法，在不同超参数和稀疏数据场景表现良好。


<details>
  <summary>Details</summary>
Motivation: 先前基于对比学习的图协同过滤数据增强方法存在定义噪声模糊、易丢失核心信息、增加复杂度等问题。

Method: 提出SCAR方法，利用用户 - 项目交互的协作信号生成伪交互，添加或替换现有交互。

Result: 在四个基准数据集实验中，SCAR在关键评估指标上优于先前基于对比学习的图协同过滤方法和其他先进的自监督学习方法。

Conclusion: SCAR具有较强鲁棒性，在不同超参数设置下表现良好，在稀疏数据场景特别有效。

Abstract: Contrastive learning (CL) has been widely used for enhancing the performance
of graph collaborative filtering (GCF) for personalized recommendation. Since
data augmentation plays a crucial role in the success of CL, previous works
have designed augmentation methods to remove noisy interactions between users
and items in order to generate effective augmented views. However, the
ambiguity in defining ''noisiness'' presents a persistent risk of losing core
information and generating unreliable data views, while increasing the overall
complexity of augmentation. In this paper, we propose Simple Collaborative
Augmentation for Recommendation (SCAR), a novel and intuitive augmentation
method designed to maximize the effectiveness of CL for GCF. Instead of
removing information, SCAR leverages collaborative signals extracted from
user-item interactions to generate pseudo-interactions, which are then either
added to or used to replace existing interactions. This results in more robust
representations while avoiding the pitfalls of overly complex augmentation
modules. We conduct experiments on four benchmark datasets and show that SCAR
outperforms previous CL-based GCF methods as well as other state-of-the-art
self-supervised learning approaches across key evaluation metrics. SCAR
exhibits strong robustness across different hyperparameter settings and is
particularly effective in sparse data scenarios.

</details>


### [110] [LIR: The First Workshop on Late Interaction and Multi Vector Retrieval @ ECIR 2026](https://arxiv.org/abs/2511.00444)
*Benjamin Clavié,Xianming Li,Antoine Chaffin,Omar Khattab,Tom Aarsen,Manuel Faysse,Jing Li*

Main category: cs.IR

TL;DR: 介绍晚期交互检索方法优缺点，说明举办相关研讨会目的是提供交流环境促进合作。


<details>
  <summary>Details</summary>
Motivation: 晚期交互检索方法有优势但面临效率、可用性等挑战，且研究分散、少从业者参与，需交流平台。

Method: 举办研讨会，提供高度互动环境让不同背景研究者和从业者交流经验。

Result: 未提及具体结果。

Conclusion: 举办研讨会可促进晚期交互各方面的讨论和合作。

Abstract: Late interaction retrieval methods, pioneered by ColBERT, have emerged as a
powerful alternative to single-vector neural IR. By leveraging fine-grained,
token-level representations, they have been demonstrated to deliver strong
generalisation and robustness, particularly in out-of-domain settings. They
have recently been shown to be particularly well-suited for novel use cases,
such as reasoning-based or cross-modality retrieval. At the same time, these
models pose significant challenges of efficiency, usability, and integrations
into fully fledged systems; as well as the natural difficulties encountered
while researching novel application domains. Recent years have seen rapid
advances across many of these areas, but research efforts remain fragmented
across communities and frequently exclude practitioners. The purpose of this
workshop is to create an environment where all aspects of late interaction can
be discussed, with a focus on early research explorations, real-world outcomes,
and negative or puzzling results to be freely shared and discussed. The aim of
LIR is to provide a highly-interactive environment for researchers from various
backgrounds and practitioners to freely discuss their experience, fostering
further collaboration.

</details>


### [111] [Listwise Preference Diffusion Optimization for User Behavior Trajectories Prediction](https://arxiv.org/abs/2511.00530)
*Hongtao Huang,Chengkai Huang,Junda Wu,Tong Yu,Julian McAuley,Lina Yao*

Main category: cs.IR

TL;DR: 提出用户行为轨迹预测新任务，引入LPDO训练框架，提出SeqMatch指标，实验显示LPDO优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 传统顺序推荐无法解决多步用户行为轨迹预测中捕捉序列项全局依赖的问题，该问题对个性化商务等应用至关重要。

Method: 将用户行为轨迹预测定义为新任务，引入基于扩散的LPDO训练框架，提出SeqMatch指标并采用PPL指标。

Result: 在真实用户行为基准测试中，LPDO始终优于现有基线。

Conclusion: LPDO为基于扩散模型的结构化偏好学习建立了新基准。

Abstract: Forecasting multi-step user behavior trajectories requires reasoning over
structured preferences across future actions, a challenge overlooked by
traditional sequential recommendation. This problem is critical for
applications such as personalized commerce and adaptive content delivery, where
anticipating a user's complete action sequence enhances both satisfaction and
business outcomes. We identify an essential limitation of existing paradigms:
their inability to capture global, listwise dependencies among sequence items.
To address this, we formulate User Behavior Trajectory Prediction (UBTP) as a
new task setting that explicitly models long-term user preferences. We
introduce Listwise Preference Diffusion Optimization (LPDO), a diffusion-based
training framework that directly optimizes structured preferences over entire
item sequences. LPDO incorporates a Plackett-Luce supervision signal and
derives a tight variational lower bound aligned with listwise ranking
likelihoods, enabling coherent preference generation across denoising steps and
overcoming the independent-token assumption of prior diffusion methods. To
rigorously evaluate multi-step prediction quality, we propose the task-specific
metric Sequential Match (SeqMatch), which measures exact trajectory agreement,
and adopt Perplexity (PPL), which assesses probabilistic fidelity. Extensive
experiments on real-world user behavior benchmarks demonstrate that LPDO
consistently outperforms state-of-the-art baselines, establishing a new
benchmark for structured preference learning with diffusion models.

</details>


### [112] [Structurally Refined Graph Transformer for Multimodal Recommendation](https://arxiv.org/abs/2511.00584)
*Ke Shi,Yan Zhang,Miao Zhang,Lifan Chen,Jiali Yi,Kui Xiao,Xiaoju Hou,Zhifei Li*

Main category: cs.IR

TL;DR: 本文提出SRGFormer模型解决多模态推荐系统挑战，实验显示其性能优于先前模型。


<details>
  <summary>Details</summary>
Motivation: 当前多模态推荐模型在区分数据价值、语义框架使用和捕捉用户与物品交互方面存在问题，需要改进。

Method: 修改transformer以捕捉用户整体行为模式，将多模态信息嵌入超图结构增强局部结构学习，对用户 - 物品协作信号应用自监督任务增强多模态信息融合。

Result: 在三个公开数据集上实验，SRGFormer超越先前基准模型，在Sports数据集上平均性能提升4.47%。

Conclusion: SRGFormer是一个有效的多模态推荐模型，代码已公开。

Abstract: Multimodal recommendation systems utilize various types of information,
including images and text, to enhance the effectiveness of recommendations. The
key challenge is predicting user purchasing behavior from the available data.
Current recommendation models prioritize extracting multimodal information
while neglecting the distinction between redundant and valuable data. They also
rely heavily on a single semantic framework (e.g., local or global semantics),
resulting in an incomplete or biased representation of user preferences,
particularly those less expressed in prior interactions. Furthermore, these
approaches fail to capture the complex interactions between users and items,
limiting the model's ability to meet diverse users. To address these
challenges, we present SRGFormer, a structurally optimized multimodal
recommendation model. By modifying the transformer for better integration into
our model, we capture the overall behavior patterns of users. Then, we enhance
structural information by embedding multimodal information into a hypergraph
structure to aid in learning the local structures between users and items.
Meanwhile, applying self-supervised tasks to user-item collaborative signals
enhances the integration of multimodal information, thereby revealing the
representational features inherent to the data's modality. Extensive
experiments on three public datasets reveal that SRGFormer surpasses previous
benchmark models, achieving an average performance improvement of 4.47 percent
on the Sports dataset. The code is publicly available online.

</details>


### [113] [Taxonomy-based Negative Sampling In Personalized Semantic Search for E-commerce](https://arxiv.org/abs/2511.00694)
*Uthman Jinadu,Siawpeng Er,Le Yu,Chen Liang,Bingxin Li,Yi Ding,Aleksandar Velkoski*

Main category: cs.IR

TL;DR: 提出电商搜索语义检索模型，用新采样策略和用户个性化，实验效果好且减少训练开销。


<details>
  <summary>Details</summary>
Motivation: 现有训练模型的采样技术计算成本高或实施困难，且未考虑用户购买模式，检索结果无关。

Method: 将查询和产品嵌入共享向量空间，采用基于分类法的硬负采样策略，结合用户购买历史和行为进行个性化。

Result: 离线实验中召回率超基线，在线A/B测试转化率、加购率和平均订单价值提升，分类法负样本减少训练开销、加速收敛。

Conclusion: 该语义检索模型有效，分享了大规模部署的实用经验。

Abstract: Large retail outlets offer products that may be domain-specific, and this
requires having a model that can understand subtle differences in similar
items. Sampling techniques used to train these models are most of the time,
computationally expensive or logistically challenging. These models also do not
factor in users' previous purchase patterns or behavior, thereby retrieving
irrelevant items for them. We present a semantic retrieval model for e-commerce
search that embeds queries and products into a shared vector space and
leverages a novel taxonomy-based hard-negative sampling(TB-HNS) strategy to
mine contextually relevant yet challenging negatives. To further tailor
retrievals, we incorporate user-level personalization by modeling each
customer's past purchase history and behavior. In offline experiments, our
approach outperforms BM25, ANCE and leading neural baselines on Recall@K, while
live A/B testing shows substantial uplifts in conversion rate, add-to-cart
rate, and average order value. We also demonstrate that our taxonomy-driven
negatives reduce training overhead and accelerate convergence, and we share
practical lessons from deploying this system at scale.

</details>


### [114] [REaR: Retrieve, Expand and Refine for Effective Multitable Retrieval](https://arxiv.org/abs/2511.00805)
*Rishita Agarwal,Himanshu Singhal,Peter Baile Chen,Manan Roy Choudhury,Dan Roth,Vivek Gupta*

Main category: cs.IR

TL;DR: 提出REAR框架用于多表检索，在多数据集提升效果，低延迟低成本。


<details>
  <summary>Details</summary>
Motivation: 现有检索器仅优化查询 - 表相关性，忽略表间兼容性，无法高效处理多表检索。

Method: 采用三阶段、无大语言模型的REAR框架，包括检索、扩展和精炼。

Result: 在多个复杂表问答数据集上提升多表检索质量和下游SQL执行效果，性能与先进的大语言模型增强检索系统相当，且延迟和成本更低。

Conclusion: REAR是基于表的下游任务实用、可扩展的构建模块。

Abstract: Answering natural language queries over relational data often requires
retrieving and reasoning over multiple tables, yet most retrievers optimize
only for query-table relevance and ignore table table compatibility. We
introduce REAR (Retrieve, Expand and Refine), a three-stage, LLM-free framework
that separates semantic relevance from structural joinability for efficient,
high-fidelity multi-table retrieval. REAR (i) retrieves query-aligned tables,
(ii) expands these with structurally joinable tables via fast, precomputed
column-embedding comparisons, and (iii) refines them by pruning noisy or weakly
related candidates. Empirically, REAR is retriever-agnostic and consistently
improves dense/sparse retrievers on complex table QA datasets (BIRD, MMQA, and
Spider) by improving both multi-table retrieval quality and downstream SQL
execution. Despite being LLM-free, it delivers performance competitive with
state-of-the-art LLM-augmented retrieval systems (e.g.,ARM) while achieving
much lower latency and cost. Ablations confirm complementary gains from
expansion and refinement, underscoring REAR as a practical, scalable building
block for table-based downstream tasks (e.g., Text-to-SQL).

</details>


### [115] [Controlling Gender Bias in Retrieval via a Backpack Architecture](https://arxiv.org/abs/2511.00875)
*Amirabbas Afzali,Amirreza Velae,Iman Ahmadi,Mohammad Aliannejadi*

Main category: cs.IR

TL;DR: 针对大语言模型在排序任务中的社会偏见问题，提出基于Backpack语言模型架构的去偏框架，实验显示能有效减轻性别偏见且性能损失小。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在社会偏见，融入排序系统会传播偏见导致不公平结果，需解决排序任务的偏见问题。

Method: 利用Backpack语言模型架构，生成输出作为非上下文、学习到的词方面（即语义）的加权组合，提出去偏框架。

Result: 框架有效减轻文本检索和排序中的性别偏见，性能仅有最小程度的下降。

Conclusion: 所提出的基于Backpack架构的去偏框架能有效处理排序任务中的性别偏见问题。

Abstract: The presence of social biases in large language models (LLMs) has become a
significant concern in AI research. These biases, often embedded in training
data, can perpetuate harmful stereotypes and distort decision-making processes.
When LLMs are integrated into ranking systems, they can propagate these biases,
leading to unfair outcomes in critical applications such as search engines and
recommendation systems. Backpack Language Models, unlike traditional
transformer-based models that treat text sequences as monolithic structures,
generate outputs as weighted combinations of non-contextual, learned word
aspects, also known as senses. Leveraging this architecture, we propose a
framework for debiasing ranking tasks. Our experimental results show that this
framework effectively mitigates gender bias in text retrieval and ranking with
minimal degradation in performance.

</details>


### [116] [Contextual Relevance and Adaptive Sampling for LLM-Based Document Reranking](https://arxiv.org/abs/2511.01208)
*Jerry Huang,Siddarth Madala,Cheng Niu,Julia Hockenmaier,Tong Zhang*

Main category: cs.IR

TL;DR: 提出上下文相关性概念和TS - SetRank算法，能提升文档重排序性能。


<details>
  <summary>Details</summary>
Motivation: 现有重排序算法在处理需要深度推理的查询时，识别相关文档存在挑战，且以往工作未充分考虑批次构成对重排序性能的影响。

Method: 提出上下文相关性概念，定义为文档在不同重排序上下文分布下与查询相关的概率；提出基于采样、不确定性感知的重排序算法TS - SetRank。

Result: TS - SetRank在BRIGHT上使nDCG@10较基线提高15 - 25%，在BEIR上提高6 - 21%。

Conclusion: 将相关性建模为上下文相关很重要。

Abstract: Reranking algorithms have made progress in improving document retrieval
quality by efficiently aggregating relevance judgments generated by large
language models (LLMs). However, identifying relevant documents for queries
that require in-depth reasoning remains a major challenge. Reasoning-intensive
queries often exhibit multifaceted information needs and nuanced
interpretations, rendering document relevance inherently context dependent. To
address this, we propose contextual relevance, which we define as the
probability that a document is relevant to a given query, marginalized over the
distribution of different reranking contexts it may appear in (i.e., the set of
candidate documents it is ranked alongside and the order in which the documents
are presented to a reranking model). While prior works have studied methods to
mitigate the positional bias LLMs exhibit by accounting for the ordering of
documents, we empirically find that the compositions of these batches also
plays an important role in reranking performance. To efficiently estimate
contextual relevance, we propose TS-SetRank, a sampling-based,
uncertainty-aware reranking algorithm. Empirically, TS-SetRank improves nDCG@10
over retrieval and reranking baselines by 15-25% on BRIGHT and 6-21% on BEIR,
highlighting the importance of modeling relevance as context-dependent.

</details>


### [117] [A semantic-based deep learning approach for mathematical expression retrieval](https://arxiv.org/abs/2511.01364)
*Pavan Kumar Perepu*

Main category: cs.IR

TL;DR: 本文聚焦用深度学习方法检索数学表达式，用DRNN提取语义特征并匹配检索，在829个数学表达式数据库上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 传统基于句法相似性的文本检索方法用于数学表达式检索有局限，需基于嵌入的深度学习方法实现语义相似性检索。

Method: 用深度循环神经网络（DRNN）从数学表达式中提取语义特征，训练网络进行分类任务以确定表达式复杂度，将输出存于数据库，查询时计算特征并基于欧氏距离匹配。

Result: 在829个数学表达式的数据库上进行了方法验证。

Conclusion: 提出了一种基于深度学习的数学表达式检索方法，可实现基于语义特征的匹配和检索。

Abstract: Mathematical expressions (MEs) have complex two-dimensional structures in
which symbols can be present at any nested depth like superscripts, subscripts,
above, below etc. As MEs are represented using LaTeX format, several text
retrieval methods based on string matching, vector space models etc., have also
been applied for ME retrieval problem in the literature. As these methods are
based on syntactic similarity, recently deep learning approaches based on
embedding have been used for semantic similarity. In our present work, we have
focused on the retrieval of mathematical expressions using deep learning
approaches. In our approach, semantic features are extracted from the MEs using
a deep recurrent neural network (DRNN) and these features have been used for
matching and retrieval. We have trained the network for a classification task
which determines the complexity of an ME. ME complexity has been quantified in
terms of its nested depth. Based on the nested depth, we have considered three
complexity classes of MEs: Simple, Medium and Complex. After training the
network, outputs just before the the final fully connected layer are extracted
for all the MEs. These outputs form the semantic features of MEs and are stored
in a database. For a given ME query, its semantic features are computed using
the trained DRNN and matched against the semantic feature database. Matching is
performed based on the standard euclidean distance and top 'k' nearest matches
are retrieved, where 'k' is a user-defined parameter. Our approach has been
illustrated on a database of 829 MEs.

</details>


### [118] [A Soft-partitioned Semi-supervised Collaborative Transfer Learning Approach for Multi-Domain Recommendation](https://arxiv.org/abs/2511.01404)
*Xiaoyu Liu,Yiqing Wu,Ruidong Han,Fuzhen Zhuang,Xiang Li,Wei Lin*

Main category: cs.IR

TL;DR: 提出SSCTL方法解决多领域推荐中数据不平衡问题，实验证明有效提升GMV和CTR。


<details>
  <summary>Details</summary>
Motivation: 工业中多领域推荐常用共享-特定架构，但不同领域数据不平衡时，该架构存在数据压倒性和过拟合问题。

Method: 提出Soft-partitioned Semi-supervised Collaborative Transfer Learning (SSCTL)方法，生成动态参数解决压倒性问题，利用带权重伪标签增强非主导领域数据。

Result: 在线测试中各领域GMV提升0.54% - 2.90%，CTR提升0.22% - 1.69%。

Conclusion: 提出的SSCTL方法能有效解决多领域推荐中数据不平衡问题，提升推荐效果。

Abstract: In industrial practice, Multi-domain Recommendation (MDR) plays a crucial
role. Shared-specific architectures are widely used in industrial solutions to
capture shared and unique attributes via shared and specific parameters.
However, with imbalanced data across different domains, these models face two
key issues: (1) Overwhelming: Dominant domain data skews model performance,
neglecting non-dominant domains. (2) Overfitting: Sparse data in non-dominant
domains leads to overfitting in specific parameters. To tackle these
challenges, we propose Soft-partitioned Semi-supervised Collaborative Transfer
Learning (SSCTL) for multi-domain recommendation. SSCTL generates dynamic
parameters to address the overwhelming issue, thus shifting focus towards
samples from non-dominant domains. To combat overfitting, it leverages
pseudo-labels with weights from dominant domain instances to enhance
non-dominant domain data. We conduct comprehensive experiments, both online and
offline, to validate the efficacy of our proposed method. Online tests yielded
significant improvements across various domains, with increases in GMV ranging
from 0.54% to 2.90% and enhancements in CTR ranging from 0.22% to 1.69%.

</details>


### [119] [LiCoMemory: Lightweight and Cognitive Agentic Memory for Efficient Long-Term Reasoning](https://arxiv.org/abs/2511.01448)
*Zhengjun Huang,Zhoujin Tian,Qintian Guo,Fangyuan Zhang,Yingli Zhou,Di Jiang,Xiaofang Zhou*

Main category: cs.IR

TL;DR: 提出LiCoMemory框架解决大语言模型代理内存问题，实验表现佳。


<details>
  <summary>Details</summary>
Motivation: 大语言模型代理受限于上下文窗口和缺乏持久内存，现有外部内存架构存在冗余表示等问题。

Method: 提出LiCoMemory端到端代理内存框架，引入CogniGraph轻量级分层图，采用时间和分层感知搜索与集成重排进行知识检索。

Result: 在长期对话基准测试中，LiCoMemory在时间推理、多会话一致性和检索效率上优于基线，降低更新延迟。

Conclusion: LiCoMemory有效解决大语言模型代理内存问题，提升性能。

Abstract: Large Language Model (LLM) agents exhibit remarkable conversational and
reasoning capabilities but remain constrained by limited context windows and
the lack of persistent memory. Recent efforts address these limitations via
external memory architectures, often employing graph-based representations, yet
most adopt flat, entangled structures that intertwine semantics with topology,
leading to redundant representations, unstructured retrieval, and degraded
efficiency and accuracy. To resolve these issues, we propose LiCoMemory, an
end-to-end agentic memory framework for real-time updating and retrieval, which
introduces CogniGraph, a lightweight hierarchical graph that utilizes entities
and relations as semantic indexing layers, and employs temporal and
hierarchy-aware search with integrated reranking for adaptive and coherent
knowledge retrieval. Experiments on long-term dialogue benchmarks, LoCoMo and
LongMemEval, show that LiCoMemory not only outperforms established baselines in
temporal reasoning, multi-session consistency, and retrieval efficiency, but
also notably reduces update latency. Our official code and data are available
at https://github.com/EverM0re/LiCoMemory.

</details>


### [120] [CAT-ID$^2$: Category-Tree Integrated Document Identifier Learning for Generative Retrieval In E-commerce](https://arxiv.org/abs/2511.01461)
*Xiaoyu Liu,Fuwei Zhang,Yiqing Wu,Xinyu Jia,Zenghua Xia,Fuzhen Zhuang,Zhao Zhang,Fei Jiang,Wei Lin*

Main category: cs.IR

TL;DR: 本文提出了一种新的生成式检索ID学习方法CAT - ID²，结合了类别信息，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 现有生成式检索（GR）方法大多忽略电商中常见且关键的原生类别信息，核心挑战是构建有强表示能力的文档ID。

Method: 提出CAtegory - Tree Integrated Document IDentifier (CAT - ID²) 方法，包含Hierarchical Class Constraint Loss、Cluster Scale Constraint Loss和Dispersion Loss三个关键模块。

Result: 离线和在线实验证实方法有效，在线A/B测试显示，模糊意图查询每千用户平均订单增加0.33%，长尾查询增加0.24%。

Conclusion: CAT - ID²方法能使相似文档更相似，同时保持不同文档表示的唯一性，在电商检索中有效。

Abstract: Generative retrieval (GR) has gained significant attention as an effective
paradigm that integrates the capabilities of large language models (LLMs). It
generally consists of two stages: constructing discrete semantic identifiers
(IDs) for documents and retrieving documents by autoregressively generating ID
tokens.The core challenge in GR is how to construct document IDs (DocIDS) with
strong representational power. Good IDs should exhibit two key properties:
similar documents should have more similar IDs, and each document should
maintain a distinct and unique ID.However, most existing methods ignore native
category information, which is common and critical in E-commerce. Therefore, we
propose a novel ID learning method, CAtegory-Tree Integrated Document
IDentifier (CAT-ID$^2$), incorporating prior category information into the
semantic IDs.CAT-ID$^2$ includes three key modules: a Hierarchical Class
Constraint Loss to integrate category information layer by layer during
quantization, a Cluster Scale Constraint Loss for uniform ID token
distribution, and a Dispersion Loss to improve the distinction of reconstructed
documents. These components enable CAT-ID$^2$ to generate IDs that make similar
documents more alike while preserving the uniqueness of different documents'
representations.Extensive offline and online experiments confirm the
effectiveness of our method, with online A/B tests showing a 0.33% increase in
average orders per thousand users for ambiguous intent queries and 0.24% for
long-tail queries.

</details>


### [121] [Trove: A Flexible Toolkit for Dense Retrieval](https://arxiv.org/abs/2511.01857)
*Reza Esfandiarpoor,Max Zuo,Stephen H. Bach*

Main category: cs.IR

TL;DR: 介绍开源检索工具包Trove，具有高效数据管理、高度可定制等特点，能简化实验并支持探索性研究。


<details>
  <summary>Details</summary>
Motivation: 简化检索研究实验，同时不牺牲灵活性和速度。

Method: 引入高效数据管理功能，提供高度可定制选项和低代码统一评估与负样本挖掘流程。

Result: 数据管理功能使内存消耗降低2.6倍，推理时间随节点数线性减少。

Conclusion: Trove能简化检索实验，支持任意定制，便于探索性研究。

Abstract: We introduce Trove, an easy-to-use open-source retrieval toolkit that
simplifies research experiments without sacrificing flexibility or speed. For
the first time, we introduce efficient data management features that load and
process (filter, select, transform, and combine) retrieval datasets on the fly,
with just a few lines of code. This gives users the flexibility to easily
experiment with different dataset configurations without the need to compute
and store multiple copies of large datasets. Trove is highly customizable: in
addition to many built-in options, it allows users to freely modify existing
components or replace them entirely with user-defined objects. It also provides
a low-code and unified pipeline for evaluation and hard negative mining, which
supports multi-node execution without any code changes. Trove's data management
features reduce memory consumption by a factor of 2.6. Moreover, Trove's
easy-to-use inference pipeline incurs no overhead, and inference times decrease
linearly with the number of available nodes. Most importantly, we demonstrate
how Trove simplifies retrieval experiments and allows for arbitrary
customizations, thus facilitating exploratory research.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [122] [VRScout: Towards Real-Time, Autonomous Testing of Virtual Reality Games](https://arxiv.org/abs/2511.00002)
*Yurun Wu,Yousong Sun,Burkhard Wunsche,Jia Wang,Elliott Wen*

Main category: cs.LG

TL;DR: 提出基于深度学习的VRScout用于自动测试VR游戏，在商业VR游戏上表现出色，可用于质量保证和安全审计。


<details>
  <summary>Details</summary>
Motivation: VR内容质量、安全和适用性保障面临挑战，传统人工质量保证难扩展，自动化测试用于VR有独特困难。

Method: 提出VRScout，用增强的Action Chunking Transformer从人类演示中学习预测多步动作序列，引入动态可调滑动视界平衡响应性和精度。

Result: 在商业VR游戏上评估，仅用有限训练数据达到专家级表现，在消费级硬件上以60 FPS实时推理。

Conclusion: VRScout是实用且可扩展的自动化VR游戏测试框架，可用于质量保证和安全审计。

Abstract: Virtual Reality (VR) has rapidly become a mainstream platform for gaming and
interactive experiences, yet ensuring the quality, safety, and appropriateness
of VR content remains a pressing challenge. Traditional human-based quality
assurance is labor-intensive and cannot scale with the industry's rapid growth.
While automated testing has been applied to traditional 2D and 3D games,
extending it to VR introduces unique difficulties due to high-dimensional
sensory inputs and strict real-time performance requirements. We present
VRScout, a deep learning-based agent capable of autonomously navigating VR
environments and interacting with virtual objects in a human-like and real-time
manner. VRScout learns from human demonstrations using an enhanced Action
Chunking Transformer that predicts multi-step action sequences. This enables
our agent to capture higher-level strategies and generalize across diverse
environments. To balance responsiveness and precision, we introduce a
dynamically adjustable sliding horizon that adapts the agent's temporal context
at runtime. We evaluate VRScout on commercial VR titles and show that it
achieves expert-level performance with only limited training data, while
maintaining real-time inference at 60 FPS on consumer-grade hardware. These
results position VRScout as a practical and scalable framework for automated VR
game testing, with direct applications in both quality assurance and safety
auditing.

</details>


### [123] [Feature-Guided SAE Steering for Refusal-Rate Control using Contrasting Prompts](https://arxiv.org/abs/2511.00029)
*Samaksh Bhargav,Zining Zhu*

Main category: cs.LG

TL;DR: 本文探索用SAEs不同特征和强度引导大语言模型，用对比提示法选最佳特征，在Llama - 3 8B测试，实现安全性能和实用性双提升。


<details>
  <summary>Details</summary>
Motivation: 以往大语言模型部署安全引导方法需调整模型权重等昂贵操作，现有SAEs方法缺乏特征选择和安全 - 实用性权衡评估。

Method: 使用SAEs不同引导特征和强度，用准确创新的对比提示法结合AI生成提示数据集选模型中最佳引导特征，在Llama - 3 8B测试。

Result: 该方法使安全性能提升18.9%，实用性提升11.1%。

Conclusion: 通过原则性选择方法确定最优特征时，有针对性的SAE引导可克服传统安全 - 实用性权衡问题。

Abstract: Large Language Model (LLM) deployment requires guiding the LLM to recognize
and not answer unsafe prompts while complying with safe prompts. Previous
methods for achieving this require adjusting model weights along with other
expensive procedures. While recent advances in Sparse Autoencoders (SAEs) have
enabled interpretable feature extraction from LLMs, existing approaches lack
systematic feature selection methods and principled evaluation of
safety-utility tradeoffs. We explored using different steering features and
steering strengths using Sparse Auto Encoders (SAEs) to provide a solution.
Using an accurate and innovative contrasting prompt method with the
AI-Generated Prompts Dataset from teknium/OpenHermes-2p5-Mistral-7B and Air
Bench eu-dataset to efficiently choose the best features in the model to steer,
we tested this method on Llama-3 8B. We conclude that using this method, our
approach achieves an 18.9% improvement in safety performance while
simultaneously increasing utility by 11.1%, demonstrating that targeted SAE
steering can overcome traditional safety-utility tradeoffs when optimal
features are identified through principled selection methods.

</details>


### [124] [Probing Knowledge Holes in Unlearned LLMs](https://arxiv.org/abs/2511.00030)
*Myeongseob Ko,Hoang Anh Just,Charles Fleming,Ming Jin,Ruoxi Jia*

Main category: cs.LG

TL;DR: 研究发现现有机器学习遗忘技术可能产生知识空洞，提出测试用例生成框架评估，结果显示遗忘模型存在显著隐藏成本，需重新思考评估方式。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习遗忘技术可能会产生标准基准未捕捉到的“知识空洞”，需要探究遗忘模型中知识空洞的位置。

Method: 提出一个测试用例生成框架，探索被遗忘内容的紧邻区域和潜在故障的更广泛区域。

Result: 评估显示高达98.7%的测试用例中，被遗忘模型给出无关或无意义的响应，而预训练模型可回答。

Conclusion: 有必要重新思考传统的评估机器学习遗忘中知识保留的方法，超越标准静态基准。

Abstract: Machine unlearning has emerged as a prevalent technical solution for
selectively removing unwanted knowledge absorbed during pre-training, without
requiring full retraining. While recent unlearning techniques can effectively
remove undesirable content without severely compromising performance on
standard benchmarks, we find that they may inadvertently create ``knowledge
holes'' -- unintended losses of benign knowledge that standard benchmarks fail
to capture. To probe where unlearned models reveal knowledge holes, we propose
a test case generation framework that explores both immediate neighbors of
unlearned content and broader areas of potential failures. Our evaluation
demonstrates significant hidden costs of unlearning: up to 98.7\% of the test
cases yield irrelevant or nonsensical responses from unlearned models, despite
being answerable by the pretrained model. These findings necessitate rethinking
the conventional approach to evaluating knowledge preservation in unlearning,
moving beyond standard, static benchmarks.

</details>


### [125] [From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators](https://arxiv.org/abs/2511.00032)
*Lei Liu,Zhongyi Yu,Hong Wang,Huanshuo Dong,Haiyang Xin,Hongwei Zhao,Bin Li*

Main category: cs.LG

TL;DR: 本文提出Skip - Block Routing (SBR)框架解决神经算子求解偏微分方程时计算开销大的问题，可降低计算成本并加快推理速度。


<details>
  <summary>Details</summary>
Motivation: 当前神经算子应用于大规模工程任务时计算开销大，模型统一计算成本与物理场复杂度不匹配。

Method: 引入SBR框架，用路由机制学习令牌复杂度和排名，在后续层根据排名决定传递的令牌数量。

Result: SBR是通用框架，可集成到各种神经算子中，减少约50%的浮点运算计算成本，推理速度提升达2倍且不损失精度。

Conclusion: SBR框架有效解决了神经算子计算效率低的问题，具有通用性和高效性。

Abstract: In recent years, Neural Operators(NO) have gradually emerged as a popular
approach for solving Partial Differential Equations (PDEs). However, their
application to large-scale engineering tasks suffers from significant
computational overhead. And the fact that current models impose a uniform
computational cost while physical fields exhibit vastly different complexities
constitutes a fundamental mismatch, which is the root of this inefficiency. For
instance, in turbulence flows, intricate vortex regions require deeper network
processing compared to stable flows. To address this, we introduce a framework:
Skip-Block Routing (SBR), a general framework designed for Transformer-based
neural operators, capable of being integrated into their multi-layer
architectures. First, SBR uses a routing mechanism to learn the complexity and
ranking of tokens, which is then applied during inference. Then, in later
layers, it decides how many tokens are passed forward based on this ranking.
This way, the model focuses more processing capacity on the tokens that are
more complex. Experiments demonstrate that SBR is a general framework that
seamlessly integrates into various neural operators. Our method reduces
computational cost by approximately 50% in terms of Floating Point Operations
(FLOPs), while still delivering up to 2x faster inference without sacrificing
accuracy.

</details>


### [126] [One model to solve them all: 2BSDE families via neural operators](https://arxiv.org/abs/2511.01125)
*Takashi Furuya,Anastasis Kratsios,Dylan Possamaï,Bogdan Raonić*

Main category: cs.LG

TL;DR: 提出经典神经算子模型的温和生成变体，用于解决特定二阶倒向随机微分方程，证明解算子可近似，找到只需多项式参数的方程子类。


<details>
  <summary>Details</summary>
Motivation: 解决无限族二阶倒向随机微分方程（$2$BSDEs）在有随机终端时间的有界欧几里得域上的求解问题。

Method: 引入经典神经算子模型的温和生成变体，利用柯尔莫哥洛夫 - 阿诺德网络。

Result: 表明广泛的$2$BSDE族的解算子可由合适的神经算子模型近似；找到一类$2$BSDEs，其神经算子近似只需多项式数量的参数。

Conclusion: 所提出的神经算子模型在解决$2$BSDEs问题上有较好的近似效果，特别是对于特定子类可减少参数需求。

Abstract: We introduce a mild generative variant of the classical neural operator
model, which leverages Kolmogorov--Arnold networks to solve infinite families
of second-order backward stochastic differential equations ($2$BSDEs) on
regular bounded Euclidean domains with random terminal time. Our first main
result shows that the solution operator associated with a broad range of
$2$BSDE families is approximable by appropriate neural operator models. We then
identify a structured subclass of (infinite) families of $2$BSDEs whose neural
operator approximation requires only a polynomial number of parameters in the
reciprocal approximation rate, as opposed to the exponential requirement in
general worst-case neural operator guarantees.

</details>


### [127] [Neural Architecture Search for global multi-step Forecasting of Energy Production Time Series](https://arxiv.org/abs/2511.00035)
*Georg Velev,Stefan Lessmann*

Main category: cs.LG

TL;DR: 设计基于NAS的框架用于能源生产时间序列短期预测，平衡效率、性能和泛化能力，结果显示发现的轻量级架构集成优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 动态能源领域短期预测需兼顾准确性和效率，手动配置复杂方法耗时且易出错，数据存在时间动态性，需要模型有泛化能力。

Method: 设计基于NAS的框架，引入仅含高效组件的搜索空间，制定考虑性能泛化和搜索空间探索的目标函数。

Result: 在能源生产时间序列上，NAS发现的轻量级架构集成在效率和准确性上优于Transformer等现有技术和预训练模型。

Conclusion: 基于NAS的框架能有效解决能源短期预测中的挑战，实现效率、性能和泛化能力的平衡。

Abstract: The dynamic energy sector requires both predictive accuracy and runtime
efficiency for short-term forecasting of energy generation under operational
constraints, where timely and precise predictions are crucial. The manual
configuration of complex methods, which can generate accurate global multi-step
predictions without suffering from a computational bottleneck, represents a
procedure with significant time requirements and high risk for human-made
errors. A further intricacy arises from the temporal dynamics present in
energy-related data. Additionally, the generalization to unseen data is
imperative for continuously deploying forecasting techniques over time. To
overcome these challenges, in this research, we design a neural architecture
search (NAS)-based framework for the automated discovery of time series models
that strike a balance between computational efficiency, predictive performance,
and generalization power for the global, multi-step short-term forecasting of
energy production time series. In particular, we introduce a search space
consisting only of efficient components, which can capture distinctive patterns
of energy time series. Furthermore, we formulate a novel objective function
that accounts for performance generalization in temporal context and the
maximal exploration of different regions of our high-dimensional search space.
The results obtained on energy production time series show that an ensemble of
lightweight architectures discovered with NAS outperforms state-of-the-art
techniques, such as Transformers, as well as pre-trained forecasting models, in
terms of both efficiency and accuracy.

</details>


### [128] [Semi-Supervised Preference Optimization with Limited Feedback](https://arxiv.org/abs/2511.00040)
*Seonggyun Lee,Sungjun Lim,Seojin Park,Soeun Cheon,Kyungwoo Song*

Main category: cs.LG

TL;DR: 研究半监督偏好优化（SSPO），可同时从少量成对偏好标签和大量未配对样本学习，理论证明存在最优奖励阈值，有效降低数据获取成本，实验验证其数据效率高。


<details>
  <summary>Details</summary>
Motivation: 当前偏好优化方法依赖大量成对反馈数据，导致资源消耗大，需解决该问题。

Method: 研究半监督偏好优化（SSPO），证明存在最优奖励阈值对未配对数据进行伪标记，利用伪标记提取潜在偏好。

Result: SSPO能有效降低数据获取成本，如用Llama3 - 8B - Instruct在1%的UltraFeedback上训练的SSPO超过在10%上训练的强基线。

Conclusion: SSPO能在保持与人类对齐的同时大幅降低数据获取成本，具有显著的数据效率。

Abstract: The field of preference optimization has made outstanding contributions to
the alignment of language models with human preferences. Despite these
advancements, recent methods still rely heavily on substantial paired (labeled)
feedback data, leading to substantial resource expenditures. To address these
challenges, we study the problem of Semi-Supervised Preference Optimization
(SSPO) in which the idea is to learn from both a small number of pairwise
preference labels and a large pool of unpaired samples simultaneously. Our key
theoretical contribution proves the existence of an optimal reward threshold
capable of separating winning and losing responses with high probability, which
enables a principled pseudo-labeling of unpaired data. By leveraging these
pseudo-labels, SSPO effectively distills latent preferences from large-scale
unpaired data, thus maintaining human alignment while drastically reducing
acquisition costs. Extensive experiments across datasets validate this
remarkable data efficiency; for instance, SSPO trained with Llama3-8B-Instruct
on just 1% of UltraFeedback consistently surpasses strong baselines trained on
10% of UltraFeedback.

</details>


### [129] [Physics-Informed Neural Network Frameworks for the Analysis of Engineering and Biological Dynamical Systems Governed by Ordinary Differential Equations](https://arxiv.org/abs/2511.00043)
*Tyrus Whitman,Andrew Particka,Christopher Diers,Ian Griffin,Charuka Wickramasinghe,Pradeep Ranaweera*

Main category: cs.LG

TL;DR: 研究验证了物理信息神经网络（PINNs）求解常微分方程（ODEs）的能力，通过控制实验评估其性能，指出平衡损失函数、调超参和嵌入先验知识可提升其预测能力。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在求解高刚度、含冲击等复杂ODE问题时难收敛，PINNs可应对这些挑战。

Method: 用经典ODE问题作为控制试验台评估PINNs框架的准确性、训练效率和泛化能力，分析基准问题的存在性和唯一性并进行验证。

Result: 复杂问题需平衡损失函数各分量，系统调整超参数对准确求解至关重要，嵌入先验知识和施加硬约束能增强PINNs预测能力。

Conclusion: PINNs虽非通用解，但通过合理操作可在求解ODE问题上取得优异结果。

Abstract: In this study, we present and validate the predictive capability of the
Physics-Informed Neural Networks (PINNs) methodology for solving a variety of
engineering and biological dynamical systems governed by ordinary differential
equations (ODEs). While traditional numerical methods a re effective for many
ODEs, they often struggle to achieve convergence in problems involving high
stiffness, shocks, irregular domains, singular perturbations, high dimensions,
or boundary discontinuities. Alternatively, PINNs offer a powerful approach for
handling challenging numerical scenarios. In this study, classical ODE problems
are employed as controlled testbeds to systematically evaluate the accuracy,
training efficiency, and generalization capability under controlled conditions
of the PINNs framework. Although not a universal solution, PINNs can achieve
superior results by embedding physical laws directly into the learning process.
We first analyze the existence and uniqueness properties of several benchmark
problems and subsequently validate the PINNs methodology on these model
systems. Our results demonstrate that for complex problems to converge to
correct solutions, the loss function components data loss, initial condition
loss, and residual loss must be appropriately balanced through careful
weighting. We further establish that systematic tuning of hyperparameters,
including network depth, layer width, activation functions, learning rate,
optimization algorithms, w eight initialization schemes, and collocation point
sampling, plays a crucial role in achieving accurate solutions. Additionally,
embedding prior knowledge and imposing hard constraints on the network
architecture, without loss the generality of the ODE system, significantly
enhances the predictive capability of PINNs.

</details>


### [130] [ReLaX-Net: Reusing Layers for Parameter-Efficient Physical Neural Networks](https://arxiv.org/abs/2511.00044)
*Kohei Tsuchiyama,Andre Roehm,Takatomo Mihana,Ryoichi Horisaki*

Main category: cs.LG

TL;DR: 本文针对物理神经网络（PNN）规模不足的问题，提出ReLaX - Net架构，通过数值实验验证其能提升计算性能。


<details>
  <summary>Details</summary>
Motivation: 当前PNN在规模上远落后于数字神经网络，借鉴早期数字神经网络参数高效复用的经验，探索适合PNN的参数复用方法。

Method: 数值研究硬件友好的权重绑定，提出ReLaX - Net架构，采用逐层时分复用方案增加网络有效深度，仅需为现有PNN添加快速开关。

Result: 通过图像分类和自然语言处理任务的数值实验验证，ReLaX - Net在对传统PNN进行少量修改的情况下提升了计算性能，在参数数量相同的情况下超越了等效的传统RNN或DNN。

Conclusion: ReLaX - Net架构能有效提升PNN的计算性能，在参数复用方面具有优势。

Abstract: Physical Neural Networks (PNN) are promising platforms for next-generation
computing systems. However, recent advances in digital neural network
performance are largely driven by the rapid growth in the number of trainable
parameters and, so far, demonstrated PNNs are lagging behind by several orders
of magnitude in terms of scale. This mirrors size and performance constraints
found in early digital neural networks. In that period, efficient reuse of
parameters contributed to the development of parameter-efficient architectures
such as convolutional neural networks.
  In this work, we numerically investigate hardware-friendly weight-tying for
PNNs. Crucially, with many PNN systems, there is a time-scale separation
between the fast dynamic active elements of the forward pass and the only
slowly trainable elements implementing weights and biases. With this in mind,we
propose the Reuse of Layers for eXpanding a Neural Network (ReLaX-Net)
architecture, which employs a simple layer-by-layer time-multiplexing scheme to
increase the effective network depth and efficiently use the number of
parameters. We only require the addition of fast switches for existing PNNs. We
validate ReLaX-Nets via numerical experiments on image classification and
natural language processing tasks. Our results show that ReLaX-Net improves
computational performance with only minor modifications to a conventional PNN.
We observe a favorable scaling, where ReLaX-Nets exceed the performance of
equivalent traditional RNNs or DNNs with the same number of parameters.

</details>


### [131] [Balancing Interpretability and Performance in Motor Imagery EEG Classification: A Comparative Study of ANFIS-FBCSP-PSO and EEGNet](https://arxiv.org/abs/2511.00369)
*Farjana Aktar,Mohd Ruhul Ameen,Akif Islam,Md Ekramul Hamid*

Main category: cs.LG

TL;DR: 本文对比了ANFIS - FBCSP - PSO和EEGNet对运动想象EEG的分类效果，为选择MI - BCI系统提供指导。


<details>
  <summary>Details</summary>
Motivation: 实现运动想象EEG的准确且可解释分类是BCI研究的关键挑战，需对比不同方法效果。

Method: 使用BCI Competition IV - 2a数据集，对比ANFIS（结合FBCSP特征提取和模糊规则，用PSO优化）和EEGNet（直接从原始EEG数据学习时空表征）。

Result: 在受试者内实验中，模糊神经模型表现更好；在跨受试者测试中，深度模型泛化能力更强。

Conclusion: 为根据设计目标选择MI - BCI系统提供实用指导，未来可研究基于Transformer和混合神经符号框架推进EEG解码。

Abstract: Achieving both accurate and interpretable classification of motor imagery EEG
remains a key challenge in brain computer interface (BCI) research. This paper
compares a transparent fuzzy reasoning approach (ANFIS-FBCSP-PSO) with a deep
learning benchmark (EEGNet) using the BCI Competition IV-2a dataset. The ANFIS
pipeline combines filter bank common spatial pattern feature extraction with
fuzzy IF-THEN rules optimized via particle swarm optimization, while EEGNet
learns hierarchical spatial temporal representations directly from raw EEG
data. In within-subject experiments, the fuzzy neural model performed better
(68.58 percent +/- 13.76 percent accuracy, kappa = 58.04 percent +/- 18.43),
while in cross-subject (LOSO) tests, the deep model exhibited stronger
generalization (68.20 percent +/- 12.13 percent accuracy, kappa = 57.33 percent
+/- 16.22). The study provides practical guidance for selecting MI-BCI systems
according to design goals: interpretability or robustness across users. Future
investigations into transformer based and hybrid neuro symbolic frameworks are
expected to advance transparent EEG decoding.

</details>


### [132] [DynBERG: Dynamic BERT-based Graph neural network for financial fraud detection](https://arxiv.org/abs/2511.00047)
*Omkar Kulkarni,Rohitash Chandra*

Main category: cs.LG

TL;DR: 提出DynBERG架构用于动态金融交易分析，在Elliptic数据集上评估，表现优于现有方法，消融实验凸显GRU作用。


<details>
  <summary>Details</summary>
Motivation: 现有图Transformer模型不适用于动态金融交易网络，需新方法处理其动态结构和有向边。

Method: 将Graph - BERT与GRU层集成，修改算法支持有向边，在Elliptic数据集评估，与EvolveGCN和GCN等基准对比，进行消融实验。

Result: DynBERG性能优于EvolveGCN和GCN，消融实验表明GRU对建模金融交易时间动态有效。

Conclusion: DynBERG适用于动态金融交易分析，能适应重大市场变化，GRU在建模时间动态方面关键。

Abstract: Financial fraud detection is critical for maintaining the integrity of
financial systems, particularly in decentralised environments such as
cryptocurrency networks. Although Graph Convolutional Networks (GCNs) are
widely used for financial fraud detection, graph Transformer models such as
Graph-BERT are gaining prominence due to their Transformer-based architecture,
which mitigates issues such as over-smoothing. Graph-BERT is designed for
static graphs and primarily evaluated on citation networks with undirected
edges. However, financial transaction networks are inherently dynamic, with
evolving structures and directed edges representing the flow of money. To
address these challenges, we introduce DynBERG, a novel architecture that
integrates Graph-BERT with a Gated Recurrent Unit (GRU) layer to capture
temporal evolution over multiple time steps. Additionally, we modify the
underlying algorithm to support directed edges, making DynBERG well-suited for
dynamic financial transaction analysis. We evaluate our model on the Elliptic
dataset, which includes Bitcoin transactions, including all transactions during
a major cryptocurrency market event, the Dark Market Shutdown. By assessing
DynBERG's resilience before and after this event, we analyse its ability to
adapt to significant market shifts that impact transaction behaviours. Our
model is benchmarked against state-of-the-art dynamic graph classification
approaches, such as EvolveGCN and GCN, demonstrating superior performance,
outperforming EvolveGCN before the market shutdown and surpassing GCN after the
event. Additionally, an ablation study highlights the critical role of
incorporating a time-series deep learning component, showcasing the
effectiveness of GRU in modelling the temporal dynamics of financial
transactions.

</details>


### [133] [Adaptive Spatio-Temporal Graphs with Self-Supervised Pretraining for Multi-Horizon Weather Forecasting](https://arxiv.org/abs/2511.00049)
*Yao Liu*

Main category: cs.LG

TL;DR: 提出利用时空结构的自监督学习框架用于多变量天气预测，实验显示性能优越，提供可扩展且标签高效的解决方案。


<details>
  <summary>Details</summary>
Motivation: 大气系统固有的时空复杂性使准确和稳健的天气预报成为挑战，需提升多变量天气预测能力。

Method: 提出的框架集成图神经网络进行空间推理、自监督预训练方案进行表征学习以及时空自适应机制以增强不同预测范围的泛化能力。

Result: 在ERA5和MERRA - 2再分析数据集上实验表明，该方法比传统数值天气预报模型和近期深度学习方法性能更优，在北京和上海的定量评估和可视化分析证实模型能捕捉细粒度气象模式。

Conclusion: 该框架为未来数据驱动的天气预报系统提供了可扩展且标签高效的解决方案。

Abstract: Accurate and robust weather forecasting remains a fundamental challenge due
to the inherent spatio-temporal complexity of atmospheric systems. In this
paper, we propose a novel self-supervised learning framework that leverages
spatio-temporal structures to improve multi-variable weather prediction. The
model integrates a graph neural network (GNN) for spatial reasoning, a
self-supervised pretraining scheme for representation learning, and a
spatio-temporal adaptation mechanism to enhance generalization across varying
forecasting horizons. Extensive experiments on both ERA5 and MERRA-2 reanalysis
datasets demonstrate that our approach achieves superior performance compared
to traditional numerical weather prediction (NWP) models and recent deep
learning methods. Quantitative evaluations and visual analyses in Beijing and
Shanghai confirm the model's capability to capture fine-grained meteorological
patterns. The proposed framework provides a scalable and label-efficient
solution for future data-driven weather forecasting systems.

</details>


### [134] [Fast PINN Eigensolvers via Biconvex Reformulation](https://arxiv.org/abs/2511.00792)
*Akshay Sai Banderwaar,Abhishek Gupta*

Main category: cs.LG

TL;DR: 本文提出改进的PINN方法PINN - ACS解决特征值问题，比基于梯度的PINN训练快达500倍且精度高，还开源代码。


<details>
  <summary>Details</summary>
Motivation: 特征值问题重要，但传统PINN解决此类问题比经典数值方案慢很多。

Method: 将寻找特征对问题转化为双凸优化问题，使用解析最优更新对特征值和特征函数进行交替凸搜索（ACS）。

Result: PINN - ACS达到高精度，收敛速度比基于梯度的PINN训练快达500倍。

Conclusion: 提出的PINN - ACS方法在解决特征值问题上更高效且准确。

Abstract: Eigenvalue problems have a distinctive forward-inverse structure and are
fundamental to characterizing a system's thermal response, stability, and
natural modes. Physics-Informed Neural Networks (PINNs) offer a mesh-free
alternative for solving such problems but are often orders of magnitude slower
than classical numerical schemes. In this paper, we introduce a reformulated
PINN approach that casts the search for eigenpairs as a biconvex optimization
problem, enabling fast and provably convergent alternating convex search (ACS)
over eigenvalues and eigenfunctions using analytically optimal updates.
Numerical experiments show that PINN-ACS attains high accuracy with convergence
speeds up to 500$\times$ faster than gradient-based PINN training. We release
our codes at https://github.com/NeurIPS-ML4PS-2025/PINN_ACS_CODES.

</details>


### [135] [AI Progress Should Be Measured by Capability-Per-Resource, Not Scale Alone: A Framework for Gradient-Guided Resource Allocation in LLMs](https://arxiv.org/abs/2511.01077)
*David McCoy,Yulun Wu,Zachary Butzin-Dozier*

Main category: cs.LG

TL;DR: 文章挑战AI研究中的‘扩展原教旨主义’，提出以资源利用率为导向的LLM发展方向，给出理论框架和两阶段范式，以实现更可持续公平的AI未来。


<details>
  <summary>Details</summary>
Motivation: 当前模型规模和计算的无限制增长导致环境影响不可持续和资源不平等加剧，需重新调整LLM发展方向。

Method: 提出基于梯度影响模式指导资源分配决策的理论框架，提出两阶段范式。

Result: 在基于Transformer的模型中得出三点关键见解，通过协调参数和数据选择可大幅提高效率，减少资源需求。

Conclusion: 以资源利用率为视角可将硬件变通方法变为最优策略，实现AI可持续公平发展。

Abstract: This position paper challenges the "scaling fundamentalism" dominating AI
research, where unbounded growth in model size and computation has led to
unsustainable environmental impacts and widening resource inequality. We argue
that LLM development should be fundamentally reoriented toward
capability-per-resource rather than capability alone. We present a theoretical
framework demonstrating that resource-allocation decisions guided by gradient
influence patterns can dramatically improve efficiency throughout the AI
lifecycle. Our analysis shows that in transformer-based models, where a small
fraction of parameters exert outsized influence (following heavy-tailed
distributions), three critical insights emerge: (1) updating only
high-influence parameters strictly outperforms full-parameter tuning on a
performance-per-resource basis; (2) simple gradient norms provide
computationally efficient proxies for identifying these high-influence
components; and (3) coordinated parameter and data selection yields
multiplicative efficiency gains, potentially reducing resource requirements by
orders of magnitude. Building on these theoretical foundations, we propose a
two stage paradigm marginal-return pretraining for foundation developers and
influence guided adaptation for downstream users bridged by gradient
blueprints, metadata describing which parameters matter most for various tasks.
This capability-per-resource perspective transforms what were once considered
pragmatic hardware workarounds into theoretically optimal strategies,
democratizing access to cutting-edge AI capabilities while significantly
reducing environmental impact. By embedding resource consciousness into how we
develop, adapt, and evaluate models, we can reshape AI progress toward a more
sustainable and equitable future.

</details>


### [136] [SpEx: A Spectral Approach to Explainable Clustering](https://arxiv.org/abs/2511.00885)
*Tal Argov,Tal Wagner*

Main category: cs.LG

TL;DR: 提出基于谱图划分的可解释聚类新通用方法，实验显示性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 先前工作缺乏将解释树适配到任意给定聚类的通用方法。

Method: 基于谱图划分提出新的可解释聚类方法，设计算法适配解释树到非可解释聚类或数据集，用Trevisan框架解释先前算法。

Result: 在一系列数据集上实验表明该方法性能优于基线。

Conclusion: 提出的新方法在可解释聚类方面表现良好。

Abstract: Explainable clustering by axis-aligned decision trees was introduced by
Moshkovitz et al. (2020) and has gained considerable interest. Prior work has
focused on minimizing the price of explainability for specific clustering
objectives, lacking a general method to fit an explanation tree to any given
clustering, without restrictions. In this work, we propose a new and generic
approach to explainable clustering, based on spectral graph partitioning. With
it, we design an explainable clustering algorithm that can fit an explanation
tree to any given non-explainable clustering, or directly to the dataset
itself. Moreover, we show that prior algorithms can also be interpreted as
graph partitioning, through a generalized framework due to Trevisan (2013)
wherein cuts are optimized in two graphs simultaneously. Our experiments show
the favorable performance of our method compared to baselines on a range of
datasets.

</details>


### [137] [FLoRA: Fused forward-backward adapters for parameter efficient fine-tuning and reducing inference-time latencies of LLMs](https://arxiv.org/abs/2511.00050)
*Dhananjaya Gowda,Seoha Song,Junhyun Lee,Harshith Goka*

Main category: cs.LG

TL;DR: 提出用于大语言模型参数高效微调的FLoRA方法，结合LoRA和并行适配器思想，实验显示其在准确率和延迟上优于LoRA。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模增大，参数高效微调（PEFT）很重要，但现有PEFT方法仍有很多未探索部分。

Method: 提出FLoRA，即融合前后向适配器（FFBA），结合LoRA和并行适配器思想，将前后向适配器融合到基础模型现有投影层。

Result: 实验表明，在相似参数预算下，FFB适配器在准确率和延迟方面显著优于常用的LoRA。

Conclusion: 所提出的FFBA方法有效，能在大语言模型下游任务微调中提升性能。

Abstract: As the large language models (LLMs) grow in size each day, efficient training
and fine-tuning has never been as important as nowadays. This resulted in the
great interest in parameter efficient fine-tuning (PEFT), and effective methods
including low-rank adapters (LoRA) has emerged. Although the various PEFT
methods have been studied extensively in the recent years, the greater part of
the subject remains unexplored with the huge degree of freedom. In this paper,
we propose FLoRA, a family of fused forward-backward adapters (FFBA) for
parameter-efficient fine-tuning of LLMs on downstream tasks. The FFBA combine
ideas from the popular LoRA and parallel adapters to improve the overall
fine-tuning accuracies. At the same time, latencies are minimized by fusing the
forward and backward adapters into existing projection layers of the base
model. Experimental results show that the proposed FFB adapters perform
significantly better than the popularly used LoRA in both accuracy and latency
for a similar parameter budget.

</details>


### [138] [Calibrating and Rotating: A Unified Framework for Weight Conditioning in PEFT](https://arxiv.org/abs/2511.00051)
*Da Chang,Peng Xue,Yu Li,Yongxiang Liu,Pengxiang Xu,Shixun Zhang*

Main category: cs.LG

TL;DR: 本文剖析DoRA方法机制，提出统一框架设计先进PEFT方法，引入Pre - Diag和SORA，实验显示性能和效率超LoRA和DoRA。


<details>
  <summary>Details</summary>
Motivation: DoRA方法潜在机制不明且计算开销大，需深入研究其机制并设计更优PEFT方法。

Method: 先分析DoRA成功源于增加权重更新矩阵奇异值熵，将其重构成等价高效矩阵形式，提出设计PEFT方法统一框架，引入Pre - Diag和SORA方法。

Result: 在自然语言理解和生成任务实验中，Pre - Diag和SORA性能和效率优于LoRA和DoRA。

Conclusion: 所提方法能有效提升PEFT方法性能和效率，统一框架有助于设计先进PEFT方法。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods are crucial for adapting large
pre-trained models. Among these, LoRA is considered a foundational approach.
Building on this, the influential DoRA method enhances performance by
decomposing weight updates into magnitude and direction. However, its
underlying mechanism remains unclear, and it introduces significant
computational overhead. In this work, we first identify that DoRA's success
stems from its capacity to increase the singular value entropy of the weight
update matrix, which promotes a more uniform update distribution akin to full
fine-tuning. We then reformulate DoRA into a mathematically equivalent and more
efficient matrix form, revealing it as a learnable weight conditioning method.
Based on this insight, we propose a unified framework for designing advanced
PEFT methods by exploring two orthogonal dimensions: the architectural
placement and the transformation type of the conditioning matrix. Within this
framework, we introduce two novel methods: (1) \textbf{Pre-Diag}, which applies
a diagonal conditioning matrix before the LoRA update to efficiently calibrate
the pre-trained weights, thereby enhancing performance while reducing training
time; and (2) \textbf{S}kewed \textbf{O}rthogonal \textbf{R}otation
\textbf{A}daptation (\textbf{SORA}), which employs a parameter-efficient
orthogonal rotation to perform a more powerful, norm-preserving transformation
of the feature space. Extensive experiments on natural language understanding
and generation tasks demonstrate that our proposed methods achieve superior
performance and efficiency compared to both LoRA and DoRA. The code is
available at https://github.com/MaeChd/SORA.

</details>


### [139] [Real-time Continual Learning on Intel Loihi 2](https://arxiv.org/abs/2511.01553)
*Elvin Hajizada,Danielle Rager,Timothy Shea,Leobardo Campos-Macias,Andreas Wild,Eyke Hüllermeier,Yulia Sandamirskaya,Mike Davies*

Main category: cs.LG

TL;DR: 提出CLP - SNN神经形态解决方案，在OpenLORIS实验中准确率有竞争力，效率大幅提升，打破传统权衡。


<details>
  <summary>Details</summary>
Motivation: 解决边缘设备AI系统在开放世界环境中数据分布变化和新类别出现时的适应问题，应对在线持续学习在功率受限场景的挑战。

Method: 提出CLP - SNN尖峰神经网络架构，包括事件驱动和时空稀疏局部学习、自归一化三因素学习规则、集成神经发生和元可塑性。

Result: 在OpenLORIS少样本学习实验中准确率与重放方法相当且无需排练，比边缘GPU上最佳替代OCL快70倍、能效高5600倍。

Conclusion: 协同设计的脑启发算法和神经形态硬件可打破未来边缘AI系统传统的准确率 - 效率权衡。

Abstract: AI systems on edge devices face a critical challenge in open-world
environments: adapting when data distributions shift and novel classes emerge.
While offline training dominates current paradigms, online continual learning
(OCL)--where models learn incrementally from non-stationary streams without
catastrophic forgetting--remains challenging in power-constrained settings. We
present a neuromorphic solution called CLP-SNN: a spiking neural network
architecture for Continually Learning Prototypes and its implementation on
Intel's Loihi 2 chip. Our approach introduces three innovations: (1)
event-driven and spatiotemporally sparse local learning, (2) a self-normalizing
three-factor learning rule maintaining weight normalization, and (3) integrated
neurogenesis and metaplasticity for capacity expansion and forgetting
mitigation. On OpenLORIS few-shot learning experiments, CLP-SNN achieves
accuracy competitive with replay methods while being rehearsal-free. CLP-SNN
delivers transformative efficiency gains: 70\times faster (0.33ms vs 23.2ms),
and 5,600\times more energy efficient (0.05mJ vs 281mJ) than the best
alternative OCL on edge GPU. This demonstrates that co-designed brain-inspired
algorithms and neuromorphic hardware can break traditional accuracy-efficiency
trade-offs for future edge AI systems.

</details>


### [140] [Temporal Fusion Transformer for Multi-Horizon Probabilistic Forecasting of Weekly Retail Sales](https://arxiv.org/abs/2511.00552)
*Santhi Bharath Punati,Sandeep Kanta,Udaya Bhasker Cheerala,Madhusudan G Lanjewar,Praveen Damacharla*

Main category: cs.LG

TL;DR: 使用TFT对沃尔玛周销售数据进行多周期预测，在留一数据集和交叉验证中表现优于基线模型，对库存规划有实用价值。


<details>
  <summary>Details</summary>
Motivation: 准确的多周期零售预测对库存和促销至关重要，需要对沃尔玛销售数据进行有效预测。

Method: 使用Temporal Fusion Transformer (TFT)融合静态商店标识和时变外生信号，通过分位数损失进行1 - 5周概率预测。

Result: 在2012留一数据集上RMSE为每周每家店57,900美元，$R^2$为0.9875；5折交叉验证中RMSE平均为64,600美元，$R^2$为0.9844，优于XGB、CNN、LSTM和CNN - LSTM基线模型。

Conclusion: 该方法对库存规划和假期优化有实用价值，且保持了模型透明度。

Abstract: Accurate multi-horizon retail forecasts are critical for inventory and
promotions. We present a novel study of weekly Walmart sales (45 stores,
2010--2012) using a Temporal Fusion Transformer (TFT) that fuses static store
identifiers with time-varying exogenous signals (holidays, CPI, fuel price,
temperature). The pipeline produces 1--5-week-ahead probabilistic forecasts via
Quantile Loss, yielding calibrated 90\% prediction intervals and
interpretability through variable-selection networks, static enrichment, and
temporal attention. On a fixed 2012 hold-out dataset, TFT achieves an RMSE of
\$57.9k USD per store-week and an $R^2$ of 0.9875. Across a 5-fold
chronological cross-validation, the averages are RMSE = \$64.6k USD and $R^2$ =
0.9844, outperforming the XGB, CNN, LSTM, and CNN-LSTM baseline models. These
results demonstrate practical value for inventory planning and holiday-period
optimization, while maintaining model transparency.

</details>


### [141] [Feature-Guided Analysis of Neural Networks: A Replication Study](https://arxiv.org/abs/2511.00052)
*Federico Formica,Stefano Gregis,Aurora Francesca Zanenga,Andrea Rota,Mark Lawford,Claudio Menghi*

Main category: cs.LG

TL;DR: 本文评估FGA在MNIST和LSC数据集基准上的适用性，结果显示FGA精度更高，网络架构等选择对召回率影响大，对精度影响小。


<details>
  <summary>Details</summary>
Motivation: 现有特征引导方法在工业环境中的适用性缺乏实证，需评估FGA在基准上的适用性。

Method: 在MNIST和LSC数据集基准上评估FGA计算解释神经网络行为规则的有效性，还评估网络架构、训练和特征选择对FGA有效性的影响。

Result: FGA在基准上精度高于文献结果；网络架构等选择对FGA召回率影响显著，对精度影响可忽略。

Conclusion: FGA在基准上有较高精度，网络架构等选择对其召回率影响大，对精度影响小，为其在工业环境中的应用提供了一定证据。

Abstract: Understanding why neural networks make certain decisions is pivotal for their
use in safety-critical applications. Feature-Guided Analysis (FGA) extracts
slices of neural networks relevant to their tasks. Existing feature-guided
approaches typically monitor the activation of the neural network neurons to
extract the relevant rules. Preliminary results are encouraging and demonstrate
the feasibility of this solution by assessing the precision and recall of
Feature-Guided Analysis on two pilot case studies. However, the applicability
in industrial contexts needs additional empirical evidence.
  To mitigate this need, this paper assesses the applicability of FGA on a
benchmark made by the MNIST and LSC datasets. We assessed the effectiveness of
FGA in computing rules that explain the behavior of the neural network. Our
results show that FGA has a higher precision on our benchmark than the results
from the literature. We also evaluated how the selection of the neural network
architecture, training, and feature selection affect the effectiveness of FGA.
Our results show that the selection significantly affects the recall of FGA,
while it has a negligible impact on its precision.

</details>


### [142] [Quadratic Direct Forecast for Training Multi-Step Time-Series Forecast Models](https://arxiv.org/abs/2511.00053)
*Hao Wang,Licheng Pan,Yuan Lu,Zhichao Chen,Tianqiao Liu,Shuting He,Zhixuan Chu,Qingsong Wen,Haoxuan Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: 现有时间序列预测模型训练目标有缺陷，提出二次型加权训练目标和QDF算法，实验显示有效提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有训练目标如均方误差将未来步骤视为独立且等权重任务，存在忽略标签自相关效应、无法设置不同任务权重的问题，限制了预测性能。

Method: 提出二次型加权训练目标，用自适应更新的二次型加权矩阵训练模型的QDF学习算法。

Result: QDF有效提升了各种预测模型的性能，取得了最先进的结果。

Conclusion: 所提出的二次型加权训练目标和QDF算法能解决现有训练目标的问题，提升预测性能。

Abstract: The design of training objective is central to training time-series
forecasting models. Existing training objectives such as mean squared error
mostly treat each future step as an independent, equally weighted task, which
we found leading to the following two issues: (1) overlook the label
autocorrelation effect among future steps, leading to biased training
objective; (2) fail to set heterogeneous task weights for different forecasting
tasks corresponding to varying future steps, limiting the forecasting
performance. To fill this gap, we propose a novel quadratic-form weighted
training objective, addressing both of the issues simultaneously. Specifically,
the off-diagonal elements of the weighting matrix account for the label
autocorrelation effect, whereas the non-uniform diagonals are expected to match
the most preferable weights of the forecasting tasks with varying future steps.
To achieve this, we propose a Quadratic Direct Forecast (QDF) learning
algorithm, which trains the forecast model using the adaptively updated
quadratic-form weighting matrix. Experiments show that our QDF effectively
improves performance of various forecast models, achieving state-of-the-art
results. Code is available at https://anonymous.4open.science/r/QDF-8937.

</details>


### [143] [SpatialTraceGen: High-Fidelity Traces for Efficient VLM Spatial Reasoning Distillation](https://arxiv.org/abs/2511.00054)
*Gio Huh,Dhruv Sheth,Rayhan Zirvi,Frank Xiao*

Main category: cs.LG

TL;DR: 针对视觉语言模型在复杂空间推理数据上的不足，提出SpatialTraceGen框架生成高质量推理轨迹数据集。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在复杂空间推理存在困难，微调小模型缺乏高质量逐步推理数据。

Method: 引入SpatialTraceGen框架，将大教师模型推理过程提炼成多跳、多工具推理轨迹数据集，用自动验证器确保推理步骤准确性。

Result: 在CLEVR - Humans基准上，验证器引导过程使轨迹平均质量得分提高17%，降低质量方差超40%。

Conclusion: SpatialTraceGen提供专家轨迹数据集，可用于有效微调与样本高效离线强化学习。

Abstract: While Vision-Language Models (VLMs) excel in many areas, they struggle with
complex spatial reasoning, which requires problem decomposition and strategic
tool use. Fine-tuning smaller, more deployable models offers an efficient path
to strong performance, but this is hampered by a major bottleneck: the absence
of high-quality, step-by-step reasoning data. To address this data-efficiency
gap, we introduce SpatialTraceGen, a framework to distill the reasoning
processes of a large teacher model into a high-quality dataset of multi-hop,
multi-tool reasoning traces. A key innovation is our automated Verifier, which
scalably ensures the fidelity of each reasoning step, providing a
cost-effective alternative to manual human annotation. On the CLEVR-Humans
benchmark, this verifier-guided process improves the average quality score of
traces by 17\% while reducing quality variance by over 40\%. SpatialTraceGen
delivers a dataset of expert traces, providing the structured, step-by-step
examples of tool use necessary for effective fine-tuning and sample-efficient
offline reinforcement learning.

</details>


### [144] [Exploring Federated Learning for Thermal Urban Feature Segmentation -- A Comparison of Centralized and Decentralized Approaches](https://arxiv.org/abs/2511.00055)
*Leonhard Duda,Khadijeh Alibabaei,Elena Vollmer,Leon Klug,Valentin Kozlov,Lisana Berberi,Mishal Benz,Rebekka Volk,Juan Pedro Gutiérrez Hermosillo Muriedas,Markus Götz,Judith Sáínz-Pardo Díaz,Álvaro López García,Frank Schultmann,Achim Streit*

Main category: cs.LG

TL;DR: 本文研究联邦学习（FL）在无人机热成像城市热特征检测中的实际应用，对比多种FL方法与集中学习基线，探索不同工作流，为FL在无人机成像分割任务的应用和局限提供参考。


<details>
  <summary>Details</summary>
Motivation: 在数据因隐私或技术限制无法集中存储和共享的情况下，研究FL在现实场景中的实际应用和有效性，特别是无人机热成像的城市热特征检测。

Method: 评估真实部署场景而非模拟中的FL算法，对比多种FL方法与集中学习基线在模型精度、训练时间、通信开销和能源使用等关键性能指标上的表现，探索客户端控制和服务器控制的工作流。

Result: 文中未明确提及具体结果。

Conclusion: 研究结果为理解FL方法在无人机成像分割任务中的实际应用和局限提供了有价值的参考。

Abstract: Federated Learning (FL) is an approach for training a shared Machine Learning
(ML) model with distributed training data and multiple participants. FL allows
bypassing limitations of the traditional Centralized Machine Learning CL if
data cannot be shared or stored centrally due to privacy or technical
restrictions -- the participants train the model locally with their training
data and do not need to share it among the other participants. This paper
investigates the practical implementation and effectiveness of FL in a
real-world scenario, specifically focusing on unmanned aerial vehicle
(UAV)-based thermal images for common thermal feature detection in urban
environments. The distributed nature of the data arises naturally and makes it
suitable for FL applications, as images captured in two German cities are
available. This application presents unique challenges due to non-identical
distribution and feature characteristics of data captured at both locations.
The study makes several key contributions by evaluating FL algorithms in real
deployment scenarios rather than simulation. We compare several FL approaches
with a centralized learning baseline across key performance metrics such as
model accuracy, training time, communication overhead, and energy usage. This
paper also explores various FL workflows, comparing client-controlled workflows
and server-controlled workflows. The findings of this work serve as a valuable
reference for understanding the practical application and limitations of the FL
methods in segmentation tasks in UAV-based imaging.

</details>


### [145] [MISA: Memory-Efficient LLMs Optimization with Module-wise Importance Sampling](https://arxiv.org/abs/2511.00056)
*Yuxi Liu,Renjia Deng,Yutong He,Xue Wang,Tao Yao,Kun Yuan*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The substantial memory demands of pre-training and fine-tuning large language
models (LLMs) require memory-efficient optimization algorithms. One promising
approach is layer-wise optimization, which treats each transformer block as a
single layer and optimizes it sequentially, while freezing the other layers to
save optimizer states and activations. Although effective, these methods ignore
the varying importance of the modules within each layer, leading to suboptimal
performance. Moreover, layer-wise sampling provides only limited memory
savings, as at least one full layer must remain active during optimization. To
overcome these limitations, we propose Module-wise Importance SAmpling (MISA),
a novel method that divides each layer into smaller modules and assigns
importance scores to each module. MISA uses a weighted random sampling
mechanism to activate modules, provably reducing gradient variance compared to
layer-wise sampling. Additionally, we establish an \(\mathcal{O}(1/\sqrt{K})\)
convergence rate under non-convex and stochastic conditions, where $K$ is the
total number of block updates, and provide a detailed memory analysis
showcasing MISA's superiority over existing baseline methods. Experiments on
diverse learning tasks validate the effectiveness of MISA. Source code is
available at https://github.com/pkumelon/MISA.

</details>


### [146] [Diffusion LLMs are Natural Adversaries for any LLM](https://arxiv.org/abs/2511.00203)
*David Lüdke,Tom Wollschläger,Paul Ungermann,Stephan Günnemann,Leo Schwinn*

Main category: cs.LG

TL;DR: 提出将资源密集型提示优化问题转化为高效推理任务的框架，利用非自回归生成大语言模型，生成提示有良好效果并开拓新方向。


<details>
  <summary>Details</summary>
Motivation: 解决资源密集型的（对抗性）提示优化问题。

Method: 利用预训练的非自回归生成大语言模型（如扩散大语言模型）作为提示搜索的替代，直接条件生成提示。

Result: 生成的提示是低困惑度、多样化的越狱提示，对多种黑盒目标模型有强可迁移性。

Conclusion: 框架不仅适用于对抗性提示，还为红队测试、自动提示优化及利用新兴大语言模型开辟新方向。

Abstract: We introduce a novel framework that transforms the resource-intensive
(adversarial) prompt optimization problem into an \emph{efficient, amortized
inference task}. Our core insight is that pretrained, non-autoregressive
generative LLMs, such as Diffusion LLMs, which model the joint distribution
over prompt-response pairs, can serve as powerful surrogates for prompt search.
This approach enables the direct conditional generation of prompts, effectively
replacing costly, per-instance discrete optimization with a small number of
parallelizable samples. We provide a probabilistic analysis demonstrating that
under mild fidelity assumptions, only a few conditional samples are required to
recover high-reward (harmful) prompts. Empirically, we find that the generated
prompts are low-perplexity, diverse jailbreaks that exhibit strong
transferability to a wide range of black-box target models, including robustly
trained and proprietary LLMs. Beyond adversarial prompting, our framework opens
new directions for red teaming, automated prompt optimization, and leveraging
emerging Flow- and Diffusion-based LLMs.

</details>


### [147] [Automatically Finding Rule-Based Neurons in OthelloGPT](https://arxiv.org/abs/2511.00059)
*Aditya Singh,Zihang Wen,Srujananjali Medicherla,Adam Karvonen,Can Rager*

Main category: cs.LG

TL;DR: 使用决策树方法分析OthelloGPT中编码游戏规则逻辑的MLP神经元，发现约一半神经元可被规则决策树准确描述，并验证了模式的因果相关性，还提供Python工具。


<details>
  <summary>Details</summary>
Motivation: OthelloGPT为可解释性研究提供理想测试平台，需一种方法识别和解释编码规则逻辑的MLP神经元。

Method: 训练回归决策树将棋盘状态映射到神经元激活，提取神经元高激活的决策路径转换为可读逻辑形式。

Result: 约一半第5层神经元可被紧凑规则决策树准确描述，通过干预验证了决策树识别模式的因果相关性。

Conclusion: 提出的方法有效，为可解释性研究提供Python工具，便于后续研究。

Abstract: OthelloGPT, a transformer trained to predict valid moves in Othello, provides
an ideal testbed for interpretability research. The model is complex enough to
exhibit rich computational patterns, yet grounded in rule-based game logic that
enables meaningful reverse-engineering. We present an automated approach based
on decision trees to identify and interpret MLP neurons that encode rule-based
game logic. Our method trains regression decision trees to map board states to
neuron activations, then extracts decision paths where neurons are highly
active to convert them into human-readable logical forms. These descriptions
reveal highly interpretable patterns; for instance, neurons that specifically
detect when diagonal moves become legal. Our findings suggest that roughly half
of the neurons in layer 5 can be accurately described by compact, rule-based
decision trees ($R^2 > 0.7$ for 913 of 2,048 neurons), while the remainder
likely participate in more distributed or non-rule-based computations. We
verify the causal relevance of patterns identified by our decision trees
through targeted interventions. For a specific square, for specific game
patterns, we ablate neurons corresponding to those patterns and find an
approximately 5-10 fold stronger degradation in the model's ability to predict
legal moves along those patterns compared to control patterns. To facilitate
future work, we provide a Python tool that maps rule-based game behaviors to
their implementing neurons, serving as a resource for researchers to test
whether their interpretability methods recover meaningful computational
structures.

</details>


### [148] [A Tight Lower Bound for Non-stochastic Multi-armed Bandits with Expert Advice](https://arxiv.org/abs/2511.00257)
*Zachary Chase,Shinji Ito,Idan Mehalel*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We determine the minimax optimal expected regret in the classic
non-stochastic multi-armed bandit with expert advice problem, by proving a
lower bound that matches the upper bound of Kale (2014). The two bounds
determine the minimax optimal expected regret to be $\Theta\left( \sqrt{T K
\log (N/K) } \right)$, where $K$ is the number of arms, $N$ is the number of
experts, and $T$ is the time horizon.

</details>


### [149] [PolyRecommender: A Multimodal Recommendation System for Polymer Discovery](https://arxiv.org/abs/2511.00375)
*Xin Wang,Yunhao Xiao,Rui Qiao*

Main category: cs.LG

TL;DR: 介绍了多模态发现框架PolyRecommender，结合两种表示进行聚合物检索和排序，推动下一代聚合物发现。


<details>
  <summary>Details</summary>
Motivation: 为下一代聚合物发现提供AI引导设计方法。

Method: 将PolyBERT的化学语言表示与图编码器的分子图表示集成，先基于语言相似度检索候选聚合物，再用融合的多模态嵌入对其按多目标属性排序。

Result: PolyRecommender实现了相关聚合物属性的高效检索和稳健排序。

Conclusion: 建立了可泛化的多模态范式，推动了AI引导的下一代聚合物发现。

Abstract: We introduce PolyRecommender, a multimodal discovery framework that
integrates chemical language representations from PolyBERT with molecular
graph-based representations from a graph encoder. The system first retrieves
candidate polymers using language-based similarity and then ranks them using
fused multimodal embeddings according to multiple target properties. By
leveraging the complementary knowledge encoded in both modalities,
PolyRecommender enables efficient retrieval and robust ranking across related
polymer properties. Our work establishes a generalizable multimodal paradigm,
advancing AI-guided design for the discovery of next-generation polymers.

</details>


### [150] [EVINGCA: Adaptive Graph Clustering with Evolving Neighborhood Statistics](https://arxiv.org/abs/2511.00064)
*Randolph Wiredu-Aidoo*

Main category: cs.LG

TL;DR: 提出密度 - 方差聚类算法 EVINGCA，在多种数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有聚类算法存在假设限制，如 K - Means 和高斯混合模型预设凸、类高斯簇，DBSCAN 和 HDBSCAN 敏感。

Method: 将簇形成视为最近邻图上的自适应、演化过程，通过广度优先搜索扩展根图，用局部统计反馈替代固定密度阈值，结合空间索引。

Result: 平均情况下具有对数线性复杂度，在多种合成和真实、低维和高维数据集上与基线相比有竞争力。

Conclusion: EVINGCA 是一种有效的聚类算法，能克服现有算法的一些局限性。

Abstract: Clustering algorithms often rely on restrictive assumptions: K-Means and
Gaussian Mixtures presuppose convex, Gaussian-like clusters, while DBSCAN and
HDBSCAN capture non-convexity but can be highly sensitive. I introduce EVINGCA
(Evolving Variance-Informed Nonparametric Graph Construction Algorithm), a
density-variance based clustering algorithm that treats cluster formation as an
adaptive, evolving process on a nearest-neighbor graph. EVINGCA expands rooted
graphs via breadth-first search, guided by continuously updated local distance
and shape statistics, replacing fixed density thresholds with local statistical
feedback. With spatial indexing, EVINGCA features log-linear complexity in the
average case and exhibits competitive performance against baselines across a
variety of synthetic, real-world, low-d, and high-d datasets.

</details>


### [151] [A Technical Exploration of Causal Inference with Hybrid LLM Synthetic Data](https://arxiv.org/abs/2511.00318)
*Dana Kim,Yichen Xu,Tiffany Lin*

Main category: cs.LG

TL;DR: 现有大语言模型生成合成表格数据方法难以保留关键因果参数，本文提出混合生成框架及合成配对策略和评估协议，支持稳健因果分析，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型生成合成表格数据方法无法保留关键因果参数，如平均处理效应。

Method: 提出混合生成框架，结合基于模型的协变量合成与分别学习的倾向和结果模型；引入合成配对策略减轻正性违反；提出利用无限合成样本的评估协议。

Result: 展示了现有合成数据生成器虽有高预测保真度但会严重误估因果效应，所提方法为支持稳健因果分析的大语言模型数据管道奠定基础。

Conclusion: 所提工作为大语言模型驱动的数据管道支持稳健因果分析提供了基础。

Abstract: Large Language Models (LLMs) offer a flexible means to generate synthetic
tabular data, yet existing approaches often fail to preserve key causal
parameters such as the average treatment effect (ATE). In this technical
exploration, we first demonstrate that state-of-the-art synthetic data
generators, both GAN- and LLM-based, can achieve high predictive fidelity while
substantially misestimating causal effects. To address this gap, we propose a
hybrid generation framework that combines model-based covariate synthesis
(monitored via distance-to-closest-record filtering) with separately learned
propensity and outcome models, thereby ensuring that (W, A, Y) triplets retain
their underlying causal structure. We further introduce a synthetic pairing
strategy to mitigate positivity violations and a realistic evaluation protocol
that leverages unlimited synthetic samples to benchmark traditional estimators
(IPTW, AIPW, substitution) under complex covariate distributions. This work
lays the groundwork for LLM-powered data pipelines that support robust causal
analysis. Our code is available at
https://github.com/Xyc-arch/llm-synthetic-for-causal-inference.git.

</details>


### [152] [Aligning Brain Signals with Multimodal Speech and Vision Embeddings](https://arxiv.org/abs/2511.00065)
*Kateryna Shapovalenko,Quentin Auster*

Main category: cs.LG

TL;DR: 受大脑分层构建语义启发，研究预训练模型哪一层最能反映大脑分层处理，对比wav2vec2和CLIP模型嵌入，用多种策略测试，发现结合多模态、分层感知表征或助于解码大脑理解语言方式。


<details>
  <summary>Details</summary>
Motivation: 受大脑从原始声学信息到多模态联想的分层构建语义方式启发，探究预训练模型哪一层最能反映大脑的分层处理。

Method: 对比wav2vec2和CLIP模型的嵌入，使用自然语言感知时记录的EEG，通过岭回归和对比解码评估嵌入与大脑活动的一致性，测试个体层、渐进级联和渐进求和三种策略。

Result: 结合多模态、分层感知表征可能让我们更接近解码大脑理解语言的方式。

Conclusion: 结合多模态、层感知的表征有助于解码大脑理解语言的机制，而非仅将语言视为声音。

Abstract: When we hear the word "house", we don't just process sound, we imagine walls,
doors, memories. The brain builds meaning through layers, moving from raw
acoustics to rich, multimodal associations. Inspired by this, we build on
recent work from Meta that aligned EEG signals with averaged wav2vec2 speech
embeddings, and ask a deeper question: which layers of pre-trained models best
reflect this layered processing in the brain? We compare embeddings from two
models: wav2vec2, which encodes sound into language, and CLIP, which maps words
to images. Using EEG recorded during natural speech perception, we evaluate how
these embeddings align with brain activity using ridge regression and
contrastive decoding. We test three strategies: individual layers, progressive
concatenation, and progressive summation. The findings suggest that combining
multimodal, layer-aware representations may bring us closer to decoding how the
brain understands language, not just as sound, but as experience.

</details>


### [153] [Toward Unifying Group Fairness Evaluation from a Sparsity Perspective](https://arxiv.org/abs/2511.00359)
*Zhecheng Sheng,Jiawei Zhang,Enmao Diao*

Main category: cs.LG

TL;DR: 本文提出基于稀疏性的统一框架评估算法公平性，通过实验验证其有效性，为算法公平性研究提供新视角。


<details>
  <summary>Details</summary>
Motivation: 现有公平性标准在不同机器学习问题中缺乏通用性，需研究促进公平性的方法。

Method: 研究各种稀疏性度量之间的联系与差异，提出基于稀疏性的统一框架评估算法公平性。

Result: 框架符合现有公平性标准，适用于多种机器学习任务，实验证明其作为评估指标的有效性。

Conclusion: 从稀疏性和社会公平角度为算法公平性提供新视角，对公平性研究和应用有潜在广泛影响。

Abstract: Ensuring algorithmic fairness remains a significant challenge in machine
learning, particularly as models are increasingly applied across diverse
domains. While numerous fairness criteria exist, they often lack
generalizability across different machine learning problems. This paper
examines the connections and differences among various sparsity measures in
promoting fairness and proposes a unified sparsity-based framework for
evaluating algorithmic fairness. The framework aligns with existing fairness
criteria and demonstrates broad applicability to a wide range of machine
learning tasks. We demonstrate the effectiveness of the proposed framework as
an evaluation metric through extensive experiments on a variety of datasets and
bias mitigation methods. This work provides a novel perspective to algorithmic
fairness by framing it through the lens of sparsity and social equity, offering
potential for broader impact on fairness research and applications.

</details>


### [154] [Token-Regulated Group Relative Policy Optimization for Stable Reinforcement Learning in Large Language Models](https://arxiv.org/abs/2511.00066)
*Tue Le,Nghi D. Q. Bui,Linh Ngo Van,Trung Le*

Main category: cs.LG

TL;DR: 提出TR - GRPO改进GRPO，实验表明其在RLVR任务中优于GRPO，可增强LLM推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO算法存在低概率token梯度过大，导致训练不稳定和高概率token贡献被抑制的问题，需改进。

Method: 引入Token - Regulated Group Relative Policy Optimization (TR - GRPO)，为token分配与模型预测概率正相关的权重，降低低概率token权重，强调高概率token。

Result: 在逻辑、数学和代理推理等RLVR任务中，TR - GRPO始终优于GRPO。

Conclusion: 调节RL训练中token的贡献很重要，TR - GRPO是增强LLM推理能力的可靠框架。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged as a
powerful approach for strengthening the reasoning capabilities of large
language models (LLMs). Among existing algorithms, Group Relative Policy
Optimization (GRPO) has demonstrated strong performance, yet it suffers from a
critical issue: low-probability tokens disproportionately dominate gradient
updates due to their inherently large gradient magnitudes. This imbalance leads
to unstable training and suppresses the contribution of high-probability tokens
that are more reliable for learning. In this work, we introduce Token-Regulated
Group Relative Policy Optimization (TR-GRPO), a simple yet effective extension
of GRPO that assigns token-level weights positively correlated with the model's
predicted probability. By downweighting low-probability tokens and emphasizing
high-probability ones, TR-GRPO mitigates gradient over-amplification while
preserving informative learning signals. Extensive experiments demonstrate that
TR-GRPO consistently outperforms GRPO across RLVR tasks, including logic, math,
and agentic reasoning, highlighting the importance of regulating token
contributions during RL training and establishing TR-GRPO as a robust framework
for enhancing LLM reasoning.

</details>


### [155] [Edge AI in Highly Volatile Environments: Is Fairness Worth the Accuracy Trade-off?](https://arxiv.org/abs/2511.01737)
*Obaidullah Zaland,Feras M. Awaysheh,Sawsan Al Zubi,Abdul Rahman Safi,Monowar Bhuyan*

Main category: cs.LG

TL;DR: 本文研究高波动边缘环境下联邦学习中模型准确性与公平性的权衡，评估公平性客户端选择算法，结果表明公平算法在波动环境训练更慢。


<details>
  <summary>Details</summary>
Motivation: 边缘环境的固有波动性给联邦学习中实现高精度和客户端参与公平性带来挑战，需研究模型准确性和公平性的权衡。

Method: 在三个基准数据集上，对基于公平性的客户端选择算法（如RBFF和RBCSF）与随机和贪婪客户端选择算法，从公平性、模型性能和时间方面进行广泛的实证评估。

Result: 更公平的客户端选择算法虽能为客户端提供稍好机会，但在波动环境中会导致全局训练变慢。

Conclusion: 揭示了波动边缘环境中公平性 - 性能和公平性 - 速度的权衡，探索了解决联邦学习中公平客户端选择策略现有缺陷的未来研究机会。

Abstract: Federated learning (FL) has emerged as a transformative paradigm for edge
intelligence, enabling collaborative model training while preserving data
privacy across distributed personal devices. However, the inherent volatility
of edge environments, characterized by dynamic resource availability and
heterogeneous client capabilities, poses significant challenges for achieving
high accuracy and fairness in client participation. This paper investigates the
fundamental trade-off between model accuracy and fairness in highly volatile
edge environments. This paper provides an extensive empirical evaluation of
fairness-based client selection algorithms such as RBFF and RBCSF against
random and greedy client selection regarding fairness, model performance, and
time, in three benchmarking datasets (CIFAR10, FashionMNIST, and EMNIST). This
work aims to shed light on the fairness-performance and fairness-speed
trade-offs in a volatile edge environment and explore potential future research
opportunities to address existing pitfalls in \textit{fair client selection}
strategies in FL. Our results indicate that more equitable client selection
algorithms, while providing a marginally better opportunity among clients, can
result in slower global training in volatile environments\footnote{The code for
our experiments can be found at
https://github.com/obaidullahzaland/FairFL_FLTA.

</details>


### [156] [Latent Domain Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2511.00067)
*Zhixing Li,Arsham Gholamzadeh Khoee,Yinan Yu*

Main category: cs.LG

TL;DR: 研究无显式领域标签下的领域泛化，通过潜在领域聚类和文本特征融合提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有领域泛化方法依赖可能缺失或模糊的领域标签，需研究无显式领域标签的情况以部署视觉语言模型。

Method: 对图像特征进行潜在领域聚类，基于输入图像与各潜在领域的相似度融合特定领域文本特征。

Result: 在四个基准测试中，该策略比基于视觉语言模型的基线方法有持续提升。

Conclusion: 该策略能提升领域偏移下的鲁棒性，为改进提供新见解。

Abstract: The objective of domain generalization (DG) is to enable models to be robust
against domain shift. DG is crucial for deploying vision-language models (VLMs)
in real-world applications, yet most existing methods rely on domain labels
that may not be available and often ambiguous. We instead study the DG setting
where models must generalize well without access to explicit domain labels. Our
key idea is to represent an unseen target domain as a combination of latent
domains automatically discovered from training data, enabling the model to
adaptively transfer knowledge across domains. To realize this, we perform
latent domain clustering on image features and fuse domain-specific text
features based on the similarity between the input image and each latent
domain. Experiments on four benchmarks show that this strategy yields
consistent gains over VLM-based baselines and provides new insights into
improving robustness under domain shift.

</details>


### [157] [Why Federated Optimization Fails to Achieve Perfect Fitting? A Theoretical Perspective on Client-Side Optima](https://arxiv.org/abs/2511.00469)
*Zhongxiang Lei,Qi Yang,Ping Qiu,Gang Zhang,Yuanchi Ma,Jinyan Liu*

Main category: cs.LG

TL;DR: 本文从理论角度解释联邦优化在数据异质性下性能下降的原因，并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 现有算法虽能保证理论收敛和实践稳定训练，但数据异质性下性能下降原因不明，需解释。

Method: 引入异质客户端数据导致不同局部最优的假设，分析其两个关键后果。

Result: 客户局部最优间的距离提高全局目标下界，最终训练阶段全局模型在区域内振荡而非收敛到单一最优。

Conclusion: 为非独立同分布设置下的性能下降提供合理解释，实验验证了该解释，框架已开源。

Abstract: Federated optimization is a constrained form of distributed optimization that
enables training a global model without directly sharing client data. Although
existing algorithms can guarantee convergence in theory and often achieve
stable training in practice, the reasons behind performance degradation under
data heterogeneity remain unclear. To address this gap, the main contribution
of this paper is to provide a theoretical perspective that explains why such
degradation occurs. We introduce the assumption that heterogeneous client data
lead to distinct local optima, and show that this assumption implies two key
consequences: 1) the distance among clients' local optima raises the lower
bound of the global objective, making perfect fitting of all client data
impossible; and 2) in the final training stage, the global model oscillates
within a region instead of converging to a single optimum, limiting its ability
to fully fit the data. These results provide a principled explanation for
performance degradation in non-iid settings, which we further validate through
experiments across multiple tasks and neural network architectures. The
framework used in this paper is open-sourced at:
https://github.com/NPCLEI/fedtorch.

</details>


### [158] [Benchmarking Generative AI Against Bayesian Optimization for Constrained Multi-Objective Inverse Design](https://arxiv.org/abs/2511.00070)
*Muhammad Bilal Awan,Abdul Razzaq,Abdul Shahid*

Main category: cs.LG

TL;DR: 研究大语言模型（LLMs）作为生成式优化器解决约束多目标回归任务的性能，对比BO框架和微调的LLMs及BERT模型，结果显示专业BO框架性能领先，微调LLMs是有潜力的替代方案。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs在其未专门设计的约束、连续、高维数值空间任务中的效用，解决材料信息学中逆设计的关键问题。

Method: 对既定的贝叶斯优化（BO）框架和一系列微调的LLMs及BERT模型进行严格对比研究，BO采用BoTorch Ax和q - Expected Hypervolume Improvement（qEHVI, BoTorchM）进行基准测试，生成式方法通过参数高效微调（PEFT）微调模型。

Result: BoTorch qEHVI实现完美收敛（GD = 0.0），最佳LLM（WizardMath - 7B）的生成距离（GD）为1.21，显著优于传统BoTorch Ax基线（GD = 15.03）。

Conclusion: 专业BO框架仍是保证收敛的性能领先者，但微调的LLMs是有前景、计算快速的替代方案，研究结果对树脂、聚合物和涂料配方设计有工业应用价值。

Abstract: This paper investigates the performance of Large Language Models (LLMs) as
generative optimizers for solving constrained multi-objective regression tasks,
specifically within the challenging domain of inverse design
(property-to-structure mapping). This problem, critical to materials
informatics, demands finding complex, feasible input vectors that lie on the
Pareto optimal front. While LLMs have demonstrated universal effectiveness
across generative and reasoning tasks, their utility in constrained,
continuous, high-dimensional numerical spaces tasks they weren't explicitly
architected for remains an open research question. We conducted a rigorous
comparative study between established Bayesian Optimization (BO) frameworks and
a suite of fine-tuned LLMs and BERT models. For BO, we benchmarked the
foundational BoTorch Ax implementation against the state-of-the-art q-Expected
Hypervolume Improvement (qEHVI, BoTorchM). The generative approach involved
fine-tuning models via Parameter-Efficient Fine-Tuning (PEFT), framing the
challenge as a regression problem with a custom output head. Our results show
that BoTorch qEHVI achieved perfect convergence (GD=0.0), setting the
performance ceiling. Crucially, the best-performing LLM (WizardMath-7B)
achieved a Generational Distance (GD) of 1.21, significantly outperforming the
traditional BoTorch Ax baseline (GD=15.03). We conclude that specialized BO
frameworks remain the performance leader for guaranteed convergence, but
fine-tuned LLMs are validated as a promising, computationally fast alternative,
contributing essential comparative metrics to the field of AI-driven
optimization. The findings have direct industrial applications in optimizing
formulation design for resins, polymers, and paints, where multi-objective
trade-offs between mechanical, rheological, and chemical properties are
critical to innovation and production efficiency.

</details>


### [159] [Learning an Efficient Optimizer via Hybrid-Policy Sub-Trajectory Balance](https://arxiv.org/abs/2511.00543)
*Yunchuan Guan,Yu Liu,Ke Zhou,Hui Li,Sen Jia,Zhiqi Shen,Ziyang Wang,Xinglin Zhang,Tao Chen,Jenq-Neng Hwang,Lei Li*

Main category: cs.LG

TL;DR: 本文提出Lo - Hp框架解决生成式建模权重生成问题，验证其在多任务中的优势。


<details>
  <summary>Details</summary>
Motivation: 当前生成式建模权重生成方法存在过耦合和长视野问题，限制优化器灵活性且推理效率低、精度差。

Method: 提出Lo - Hp解耦两阶段权重生成框架，采用混合策略子轨迹平衡目标，结合在线和离线策略学习。

Result: 理论证明学习局部优化策略可解决长视野问题并提升全局最优权重生成，实验验证Lo - Hp在需频繁更新权重任务中的高准确性和推理效率。

Conclusion: Lo - Hp框架有效解决当前方法问题，在多任务中有出色表现。

Abstract: Recent advances in generative modeling enable neural networks to generate
weights without relying on gradient-based optimization. However, current
methods are limited by issues of over-coupling and long-horizon. The former
tightly binds weight generation with task-specific objectives, thereby limiting
the flexibility of the learned optimizer. The latter leads to inefficiency and
low accuracy during inference, caused by the lack of local constraints. In this
paper, we propose Lo-Hp, a decoupled two-stage weight generation framework that
enhances flexibility through learning various optimization policies. It adopts
a hybrid-policy sub-trajectory balance objective, which integrates on-policy
and off-policy learning to capture local optimization policies. Theoretically,
we demonstrate that learning solely local optimization policies can address the
long-horizon issue while enhancing the generation of global optimal weights. In
addition, we validate Lo-Hp's superior accuracy and inference efficiency in
tasks that require frequent weight updates, such as transfer learning, few-shot
learning, domain generalization, and large language model adaptation.

</details>


### [160] [Wavelet-Based Feature Extraction and Unsupervised Clustering for Parity Detection: A Feature Engineering Perspective](https://arxiv.org/abs/2511.00071)
*Ertugrul Mutlu*

Main category: cs.LG

TL;DR: 本文结合小波特征提取和无监督聚类处理奇偶检测问题，不依赖模运算，在无监督下实现约69.67%的分类准确率，表明经典信号处理技术可用于离散符号领域。


<details>
  <summary>Details</summary>
Motivation: 探索用过度工程化方法解决经典奇偶检测问题，研究经典信号处理技术在离散符号领域的应用。

Method: 将整数转换为小波域表示，提取多尺度统计特征，用k - means算法聚类。

Result: 在无监督下分类准确率约为69.67%。

Conclusion: 经典信号处理技术可揭示离散符号领域的潜在结构，为非常规机器学习问题提供新思路，有望连接符号推理和基于特征的学习。

Abstract: This paper explores a deliberately over-engineered approach to the classical
problem of parity detection -- determining whether a number is odd or even --
by combining wavelet-based feature extraction with unsupervised clustering.
Instead of relying on modular arithmetic, integers are transformed into
wavelet-domain representations, from which multi-scale statistical features are
extracted and clustered using the k-means algorithm. The resulting feature
space reveals meaningful structural differences between odd and even numbers,
achieving a classification accuracy of approximately 69.67% without any label
supervision. These results suggest that classical signal-processing techniques,
originally designed for continuous data, can uncover latent structure even in
purely discrete symbolic domains. Beyond parity detection, the study provides
an illustrative perspective on how feature engineering and clustering may be
repurposed for unconventional machine learning problems, potentially bridging
symbolic reasoning and feature-based learning.

</details>


### [161] [Sparse and nonparametric estimation of equations governing dynamical systems with applications to biology](https://arxiv.org/abs/2511.00579)
*G. Pillonetto,A. Giaretta,A. Aravkin,M. Bisiacco,T. Elston*

Main category: cs.LG

TL;DR: 提出结合稀疏参数估计与非参数技术的新框架，用于数据驱动发现模型方程，以解决现有方法在复杂系统非线性表示上的局限，并在生物现象估计示例中进行验证。


<details>
  <summary>Details</summary>
Motivation: 现有参数模型在准确表示复杂系统的某些非线性方面存在不足，为解决此局限开展研究。

Method: 引入一个将稀疏参数估计与非参数技术相结合的新框架。

Result: 框架能捕捉Sindy算法无法描述的非线性，且无需关于其函数形式的先验信息。

Conclusion: 该方法可用于复杂生物现象的估计。

Abstract: Data-driven discovery of model equations is a powerful approach for
understanding the behavior of dynamical systems in many scientific fields. In
particular, the ability to learn mathematical models from data would benefit
systems biology, where the complex nature of these systems often makes a bottom
up approach to modeling unfeasible. In recent years, sparse estimation
techniques have gained prominence in system identification, primarily using
parametric paradigms to efficiently capture system dynamics with minimal model
complexity. In particular, the Sindy algorithm has successfully used sparsity
to estimate nonlinear systems by extracting from a library of functions only a
few key terms needed to capture the dynamics of these systems. However,
parametric models often fall short in accurately representing certain
nonlinearities inherent in complex systems. To address this limitation, we
introduce a novel framework that integrates sparse parametric estimation with
nonparametric techniques. It captures nonlinearities that Sindy cannot describe
without requiring a priori information about their functional form. That is,
without expanding the library of functions to include the one that is trying to
be discovered. We illustrate our approach on several examples related to
estimation of complex biological phenomena.

</details>


### [162] [Bridging Vision, Language, and Mathematics: Pictographic Character Reconstruction with Bézier Curves](https://arxiv.org/abs/2511.00076)
*Zihao Wan,Pau Tong Lin Xu,Fuwen Luo,Ziyue Wang,Peng Li,Yang Liu*

Main category: cs.LG

TL;DR: 研究利用象形文字测试视觉语言模型（VLM）解读视觉信息几何结构的能力，提出将其转化为程序合成任务，模型表现优于零样本基线，且能零样本重构甲骨文。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型解读视觉信息潜在几何结构的能力探索较少，象形文字可作为测试该能力的理想案例。

Method: 将视觉识别挑战转化为数学领域的程序合成任务，训练VLM将光栅图像反编译为贝塞尔曲线组成的程序。

Result: 模型表现优于包括GPT - 4o在内的强零样本基线，能在零样本情况下重构甲骨文。

Conclusion: 模型获得了抽象且可迁移的几何语法，超越像素级模式识别，实现更结构化的视觉理解。

Abstract: While Vision-language Models (VLMs) have demonstrated strong semantic
capabilities, their ability to interpret the underlying geometric structure of
visual information is less explored. Pictographic characters, which combine
visual form with symbolic structure, provide an ideal test case for this
capability. We formulate this visual recognition challenge in the mathematical
domain, where each character is represented by an executable program of
geometric primitives. This is framed as a program synthesis task, training a
VLM to decompile raster images into programs composed of B\'ezier curves. Our
model, acting as a "visual decompiler", demonstrates performance superior to
strong zero-shot baselines, including GPT-4o. The most significant finding is
that when trained solely on modern Chinese characters, the model is able to
reconstruct ancient Oracle Bone Script in a zero-shot context. This
generalization provides strong evidence that the model acquires an abstract and
transferable geometric grammar, moving beyond pixel-level pattern recognition
to a more structured form of visual understanding.

</details>


### [163] [Belief Dynamics Reveal the Dual Nature of In-Context Learning and Activation Steering](https://arxiv.org/abs/2511.00617)
*Eric Bigelow,Daniel Wurgaft,YingQiao Wang,Noah Goodman,Tomer Ullman,Hidenori Tanaka,Ekdeep Singh Lubana*

Main category: cs.LG

TL;DR: 本文从贝叶斯视角提出统一框架解释大语言模型的提示和激活控制方法，模型能预测模型行为并解释和预测现象。


<details>
  <summary>Details</summary>
Motivation: 探究提示和激活两种控制大语言模型的方法能否纳入更广泛框架。

Method: 从贝叶斯视角构建统一、可预测的大语言模型控制解释，认为两种干预通过改变潜在概念信念影响模型行为。

Result: 得到的贝叶斯模型能高度预测多领域模型行为，解释经验现象并预测新现象。

Conclusion: 提供了大语言模型基于提示和激活控制行为的统一解释及预测干预效果的方法。

Abstract: Large language models (LLMs) can be controlled at inference time through
prompts (in-context learning) and internal activations (activation steering).
Different accounts have been proposed to explain these methods, yet their
common goal of controlling model behavior raises the question of whether these
seemingly disparate methodologies can be seen as specific instances of a
broader framework. Motivated by this, we develop a unifying, predictive account
of LLM control from a Bayesian perspective. Specifically, we posit that both
context- and activation-based interventions impact model behavior by altering
its belief in latent concepts: steering operates by changing concept priors,
while in-context learning leads to an accumulation of evidence. This results in
a closed-form Bayesian model that is highly predictive of LLM behavior across
context- and activation-based interventions in a set of domains inspired by
prior work on many-shot in-context learning. This model helps us explain prior
empirical phenomena - e.g., sigmoidal learning curves as in-context evidence
accumulates - while predicting novel ones - e.g., additivity of both
interventions in log-belief space, which results in distinct phases such that
sudden and dramatic behavioral shifts can be induced by slightly changing
intervention controls. Taken together, this work offers a unified account of
prompt-based and activation-based control of LLM behavior, and a methodology
for empirically predicting the effects of these interventions.

</details>


### [164] [flowengineR: A Modular and Extensible Framework for Fair and Reproducible Workflow Design in R](https://arxiv.org/abs/2511.00079)
*Maximilian Willer,Peter Ruckdeschel*

Main category: cs.LG

TL;DR: flowengineR是一个R包，为机器学习管道提供模块化、可扩展框架，适用于需要可重复性、透明度和可扩展性的工作流。


<details>
  <summary>Details</summary>
Motivation: 算法公平性领域发展迅速，现有工具包存在专注单一干预或未将可重复性和可扩展性作为核心设计原则的问题。

Method: 引入标准化引擎的统一架构，各引擎封装一个方法任务并通过轻量级接口通信，借鉴多种工作流语言和框架思想。

Result: 可让研究人员在公平性背景下整合、比较和评估干预措施，架构可推广到可解释性、鲁棒性和合规性指标。

Conclusion: flowengineR为任何需要可重复性、透明度和可扩展性的工作流提供通用基础设施。

Abstract: flowengineR is an R package designed to provide a modular and extensible
framework for building reproducible algorithmic workflows for general-purpose
machine learning pipelines. It is motivated by the rapidly evolving field of
algorithmic fairness where new metrics, mitigation strategies, and machine
learning methods continuously emerge. A central challenge in fairness, but also
far beyond, is that existing toolkits either focus narrowly on single
interventions or treat reproducibility and extensibility as secondary
considerations rather than core design principles. flowengineR addresses this
by introducing a unified architecture of standardized engines for data
splitting, execution, preprocessing, training, inprocessing, postprocessing,
evaluation, and reporting. Each engine encapsulates one methodological task yet
communicates via a lightweight interface, ensuring workflows remain
transparent, auditable, and easily extensible. Although implemented in R,
flowengineR builds on ideas from workflow languages (CWL, YAWL), graph-oriented
visual programming languages (KNIME), and R frameworks (BatchJobs, batchtools).
Its emphasis, however, is less on orchestrating engines for resilient parallel
execution but rather on the straightforward setup and management of distinct
engines as data structures. This orthogonalization enables distributed
responsibilities, independent development, and streamlined integration. In
fairness context, by structuring fairness methods as interchangeable engines,
flowengineR lets researchers integrate, compare, and evaluate interventions
across the modeling pipeline. At the same time, the architecture generalizes to
explainability, robustness, and compliance metrics without core modifications.
While motivated by fairness, it ultimately provides a general infrastructure
for any workflow context where reproducibility, transparency, and extensibility
are essential.

</details>


### [165] [Stochastic Shortest Path with Sparse Adversarial Costs](https://arxiv.org/abs/2511.00637)
*Emmeran Johnson,Alberto Rumi,Ciara Pike-Burke,Patrick Rebeschini*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the adversarial Stochastic Shortest Path (SSP) problem with sparse
costs under full-information feedback. In the known transition setting,
existing bounds based on Online Mirror Descent (OMD) with negative-entropy
regularization scale with $\sqrt{\log S A}$, where $SA$ is the size of the
state-action space. While we show that this is optimal in the worst-case, this
bound fails to capture the benefits of sparsity when only a small number $M \ll
SA$ of state-action pairs incur cost. In fact, we also show that the
negative-entropy is inherently non-adaptive to sparsity: it provably incurs
regret scaling with $\sqrt{\log S}$ on sparse problems. Instead, we propose a
family of $\ell_r$-norm regularizers ($r \in (1,2)$) that adapts to the
sparsity and achieves regret scaling with $\sqrt{\log M}$ instead of
$\sqrt{\log SA}$. We show this is optimal via a matching lower bound,
highlighting that $M$ captures the effective dimension of the problem instead
of $SA$. Finally, in the unknown transition setting the benefits of sparsity
are limited: we prove that even on sparse problems, the minimax regret for any
learner scales polynomially with $SA$.

</details>


### [166] [Fixed-point graph convolutional networks against adversarial attacks](https://arxiv.org/abs/2511.00083)
*Shakib Khan,A. Ben Hamza,Amr Youssef*

Main category: cs.LG

TL;DR: 提出固定点迭代图卷积网络Fix - GCN，无需额外内存和计算复杂度，能有效抵御图神经网络对抗攻击。


<details>
  <summary>Details</summary>
Motivation: 对抗攻击对图神经网络的完整性和性能构成重大风险，需提高其鲁棒性。

Method: 引入通用频谱调制滤波器，通过固定点迭代推导特征传播规则，采用灵活通滤波方法，迭代更新节点表示。

Result: 在多个基准图数据集上的大量实验表明，模型能有效抵御对抗攻击。

Conclusion: 提出的Fix - GCN是一个灵活高效的框架，可在减轻对抗操纵影响的同时保留图的关键信息。

Abstract: Adversarial attacks present a significant risk to the integrity and
performance of graph neural networks, particularly in tasks where graph
structure and node features are vulnerable to manipulation. In this paper, we
present a novel model, called fixed-point iterative graph convolutional network
(Fix-GCN), which achieves robustness against adversarial perturbations by
effectively capturing higher-order node neighborhood information in the graph
without additional memory or computational complexity. Specifically, we
introduce a versatile spectral modulation filter and derive the feature
propagation rule of our model using fixed-point iteration. Unlike traditional
defense mechanisms that rely on additional design elements to counteract
attacks, the proposed graph filter provides a flexible-pass filtering approach,
allowing it to selectively attenuate high-frequency components while preserving
low-frequency structural information in the graph signal. By iteratively
updating node representations, our model offers a flexible and efficient
framework for preserving essential graph information while mitigating the
impact of adversarial manipulation. We demonstrate the effectiveness of the
proposed model through extensive experiments on various benchmark graph
datasets, showcasing its resilience against adversarial attacks.

</details>


### [167] [Diluting Restricted Boltzmann Machines](https://arxiv.org/abs/2511.00648)
*C. Díaz-Faloh,R. Mulet*

Main category: cs.LG

TL;DR: 研究极端剪枝下受限玻尔兹曼机（RBMs），发现RBMs剪枝80%连接仍有高性能，但剪枝后重训有局限，建议训练早期剪枝。


<details>
  <summary>Details</summary>
Motivation: 人工智能依赖大神经网络引发计算和环境成本担忧，探究简单稀疏网络能否保持高性能。

Method: 研究极端剪枝条件下的受限玻尔兹曼机（RBMs），受彩票假说启发进行实验。

Result: RBMs剪枝80%连接仍有高质量生成性能；剪枝后重训无法完全恢复性能；剪枝破坏核心连接会使生成质量骤降；重训网络不如同等稀疏度下从头训练的网络。

Conclusion: 稀疏网络应在训练早期进行剪枝，研究为高效神经网络架构开发提供见解，强调初始条件对网络能力的持续影响。

Abstract: Recent advances in artificial intelligence have relied heavily on
increasingly large neural networks, raising concerns about their computational
and environmental costs. This paper investigates whether simpler, sparser
networks can maintain strong performance by studying Restricted Boltzmann
Machines (RBMs) under extreme pruning conditions. Inspired by the Lottery
Ticket Hypothesis, we demonstrate that RBMs can achieve high-quality generative
performance even when up to 80% of the connections are pruned before training,
confirming that they contain viable sub-networks. However, our experiments
reveal crucial limitations: trained networks cannot fully recover lost
performance through retraining once additional pruning is applied. We identify
a sharp transition above which the generative quality degrades abruptly when
pruning disrupts a minimal core of essential connections. Moreover, re-trained
networks remain constrained by the parameters originally learned performing
worse than networks trained from scratch at equivalent sparsity levels. These
results suggest that for sparse networks to work effectively, pruning should be
implemented early in training rather than attempted afterwards. Our findings
provide practical insights for the development of efficient neural
architectures and highlight the persistent influence of initial conditions on
network capabilities.

</details>


### [168] [Application of predictive machine learning in pen & paper RPG game design](https://arxiv.org/abs/2511.00084)
*Jolanta Śliwa*

Main category: cs.LG

TL;DR: 近年来桌面角色扮演游戏市场增长，公司探索用AI提升体验。论文针对设计对手和估计难度等级的挑战，介绍序数回归技术，构建数据集、开发基准模型和专业评估程序。


<details>
  <summary>Details</summary>
Motivation: 桌面角色扮演游戏市场增长，公司想借AI提升体验，但出版商设计新对手和估计难度等级缺乏自动化方法，手动方法耗时耗力。

Method: 采用序数回归技术，构建专用数据集，开发受人类启发的基准模型，设计基于领域知识的评估程序。

Result: 文中未明确提及具体结果。

Conclusion: 文中未明确提及结论。

Abstract: In recent years, the pen and paper RPG market has experienced significant
growth. As a result, companies are increasingly exploring the integration of AI
technologies to enhance player experience and gain a competitive edge.
  One of the key challenges faced by publishers is designing new opponents and
estimating their challenge level. Currently, there are no automated methods for
determining a monster's level; the only approaches used are based on manual
testing and expert evaluation. Although these manual methods can provide
reasonably accurate estimates, they are time-consuming and resource-intensive.
  Level prediction can be approached using ordinal regression techniques. This
thesis presents an overview and evaluation of state-of-the-art methods for this
task. It also details the construction of a dedicated dataset for level
estimation. Furthermore, a human-inspired model was developed to serve as a
benchmark, allowing comparison between machine learning algorithms and the
approach typically employed by pen and paper RPG publishers. In addition, a
specialized evaluation procedure, grounded in domain knowledge, was designed to
assess model performance and facilitate meaningful comparisons.

</details>


### [169] [Investigating the Robustness of Knowledge Tracing Models in the Presence of Student Concept Drift](https://arxiv.org/abs/2511.00704)
*Morgan Lee,Artem Frenk,Eamon Worden,Karish Gupta,Thinh Pham,Ethan Croteau,Neil Heffernan*

Main category: cs.LG

TL;DR: 研究概念漂移和学生群体变化对在线学习平台中学生行为及知识追踪（KT）模型性能的影响，发现不同模型表现差异。


<details>
  <summary>Details</summary>
Motivation: 在线学习平台不断变化，传统KT模型假设学习过程静态，需研究概念漂移和学生群体变化对学生行为及模型性能的影响。

Method: 将四个知名KT模型应用于五年学术数据，在单学年和多学年测试模型性能。

Result: 四个KT模型性能都会下降，贝叶斯知识追踪（BKT）在新数据上最稳定，基于注意力的复杂模型预测能力下降更快。

Conclusion: 不同KT模型受概念漂移影响不同，为促进KT模型的纵向评估，分析数据公开。

Abstract: Knowledge Tracing (KT) has been an established problem in the educational
data mining field for decades, and it is commonly assumed that the underlying
learning process be- ing modeled remains static. Given the ever-changing land-
scape of online learning platforms (OLPs), we investigate how concept drift and
changing student populations can im- pact student behavior within an OLP
through testing model performance both within a single academic year and across
multiple academic years. Four well-studied KT models were applied to five
academic years of data to assess how suscep- tible KT models are to concept
drift. Through our analysis, we find that all four families of KT models can
exhibit de- graded performance, Bayesian Knowledge Tracing (BKT) remains the
most stable KT model when applied to newer data, while more complex, attention
based models lose pre- dictive power significantly faster. To foster more
longitu- dinal evaluations of KT models, the data used to conduct our analysis
is available at https://osf.io/hvfn9/?view_
only=b936c63dfdae4b0b987a2f0d4038f72a

</details>


### [170] [MaGNet: A Mamba Dual-Hypergraph Network for Stock Prediction via Temporal-Causal and Global Relational Learning](https://arxiv.org/abs/2511.00085)
*Peilin Tan,Chuanqi Shi,Dian Tu,Liang Xie*

Main category: cs.LG

TL;DR: 本文提出MaGNet用于股票预测，介绍其三项创新，实验表明性能优于现有方法且有良好投资回报和风险管理能力。


<details>
  <summary>Details</summary>
Motivation: 现有股票趋势预测方法难以有效捕捉时间依赖和动态股票间交互，常忽略横截面市场影响等。

Method: 引入MaGNet，包括MAGE块、2D时空注意力模块和双超图框架。

Result: 在六个主要股票指数上的大量实验表明，MaGNet在预测性能、投资回报和风险管理方面优于现有方法。

Conclusion: MaGNet是一种有效的股票预测模型，具有良好的应用前景。

Abstract: Stock trend prediction is crucial for profitable trading strategies and
portfolio management yet remains challenging due to market volatility, complex
temporal dynamics and multifaceted inter-stock relationships. Existing methods
struggle to effectively capture temporal dependencies and dynamic inter-stock
interactions, often neglecting cross-sectional market influences, relying on
static correlations, employing uniform treatments of nodes and edges, and
conflating diverse relationships. This work introduces MaGNet, a novel Mamba
dual-hyperGraph Network for stock prediction, integrating three key
innovations: (1) a MAGE block, which leverages bidirectional Mamba with
adaptive gating mechanisms for contextual temporal modeling and integrates a
sparse Mixture-of-Experts layer to enable dynamic adaptation to diverse market
conditions, alongside multi-head attention for capturing global dependencies;
(2) Feature-wise and Stock-wise 2D Spatiotemporal Attention modules enable
precise fusion of multivariate features and cross-stock dependencies,
effectively enhancing informativeness while preserving intrinsic data
structures, bridging temporal modeling with relational reasoning; and (3) a
dual hypergraph framework consisting of the Temporal-Causal Hypergraph (TCH)
that captures fine-grained causal dependencies with temporal constraints, and
Global Probabilistic Hypergraph (GPH) that models market-wide patterns through
soft hyperedge assignments and Jensen-Shannon Divergence weighting mechanism,
jointly disentangling localized temporal influences from instantaneous global
structures for multi-scale relational learning. Extensive experiments on six
major stock indices demonstrate MaGNet outperforms state-of-the-art methods in
both superior predictive performance and exceptional investment returns with
robust risk management capabilities. Codes available at:
https://github.com/PeilinTime/MaGNet.

</details>


### [171] [Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph](https://arxiv.org/abs/2511.00086)
*Fali Wang,Jihai Chen,Shuhua Yang,Runxue Bao,Tianxiang Zhao,Zhiwei Zhang,Xianfeng Tang,Hui Liu,Qi He,Suhang Wang*

Main category: cs.LG

TL;DR: 本文研究TTS中在固定预算下搜索计算最优的模型组合和架构问题，提出Agent - REINFORCE框架，实验表明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 以往TTS研究常假设固定协作架构和单模型使用，忽略任务间最优架构和模型组合的差异，因此研究搜索计算最优的模型组合和架构问题。

Method: 将问题形式化为多LLM协作图，再将其重新表述为概率图优化问题，通过实验得到经验见解，提出Agent - REINFORCE框架进行搜索。

Result: Agent - REINFORCE在样本效率和搜索性能上优于传统和基于LLM的基线，能有效识别精度和推理延迟联合目标下的最优图。

Conclusion: Agent - REINFORCE框架可高效搜索TTS中的最优多LLM协作图。

Abstract: Test-Time Scaling (TTS) improves large language models (LLMs) by allocating
additional computation during inference, typically through parallel,
sequential, or hybrid scaling. However, prior studies often assume fixed
collaboration architectures (e.g., topologies) and single-model usage,
overlooking that optimal architectures and model combinations can vary across
tasks. Therefore, we study the novel problem of searching for compute-optimal
model combinations and architectures in TTS under a fixed budget. We formalize
it as a multi-LLM collaboration graph, where nodes encode roles and LLM model
assignments, and edges capture information flow. This problem is challenging
because (i) the combinatorial search space is prohibitively large, and (ii)
task-specific requirements demand tailored designs. To address these, we
reformulate the problem as probabilistic graph optimization and, through pilot
experiments, derive three empirical insights into TTS collaboration graphs.
Guided by these insights, we propose Agent-REINFORCE, an LLM-agent-augmented
framework that mirrors the REINFORCE pipeline by mapping
sampling-gradient-update to sampling-feedback-update, where feedback serves as
a textual gradient to update the probabilistic graph and efficiently search for
optimal multi-LLM collaboration graphs. Experiments show that Agent-REINFORCE
outperforms both traditional and LLM-based baselines in sample efficiency and
search performance, and effectively identifies optimal graphs under joint
objectives of accuracy and inference latency.

</details>


### [172] [GraphKeeper: Graph Domain-Incremental Learning via Knowledge Disentanglement and Preservation](https://arxiv.org/abs/2511.00097)
*Zihao Guo,Qingyun Sun,Ziwei Zhang,Haonan Yuan,Huiping Zhuang,Xingcheng Fu,Jianxin Li*

Main category: cs.LG

TL;DR: 提出GraphKeeper方法解决图领域增量学习中的灾难性遗忘问题，实验效果好且有广泛应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有图增量学习方法集中在单领域，图领域增量学习在图基础模型发展下变得关键但未被研究，需解决灾难性遗忘问题。

Method: 提出特定领域参数高效微调及域内和域间解纠缠目标防止嵌入偏移，引入无偏差知识保存维护决策边界，对不可观察域的图进行域感知分布判别。

Result: GraphKeeper取得了最先进的结果，比次优方法提高6.5% - 16.6%，遗忘可忽略不计。

Conclusion: GraphKeeper能有效解决图领域增量学习问题，且可与多种代表性图基础模型无缝集成，有广泛应用潜力。

Abstract: Graph incremental learning (GIL), which continuously updates graph models by
sequential knowledge acquisition, has garnered significant interest recently.
However, existing GIL approaches focus on task-incremental and
class-incremental scenarios within a single domain. Graph domain-incremental
learning (Domain-IL), aiming at updating models across multiple graph domains,
has become critical with the development of graph foundation models (GFMs), but
remains unexplored in the literature. In this paper, we propose Graph
Domain-Incremental Learning via Knowledge Dientanglement and Preservation
(GraphKeeper), to address catastrophic forgetting in Domain-IL scenario from
the perspectives of embedding shifts and decision boundary deviations.
Specifically, to prevent embedding shifts and confusion across incremental
graph domains, we first propose the domain-specific parameter-efficient
fine-tuning together with intra- and inter-domain disentanglement objectives.
Consequently, to maintain a stable decision boundary, we introduce
deviation-free knowledge preservation to continuously fit incremental domains.
Additionally, for graphs with unobservable domains, we perform domain-aware
distribution discrimination to obtain precise embeddings. Extensive experiments
demonstrate the proposed GraphKeeper achieves state-of-the-art results with
6.5%~16.6% improvement over the runner-up with negligible forgetting. Moreover,
we show GraphKeeper can be seamlessly integrated with various representative
GFMs, highlighting its broad applicative potential.

</details>


### [173] [Random Spiking Neural Networks are Stable and Spectrally Simple](https://arxiv.org/abs/2511.00904)
*Ernesto Araya,Massimiliano Datres,Gitta Kutyniok*

Main category: cs.LG

TL;DR: 本文通过布尔函数分析研究离散时间LIF - SNNs，发现宽LIF - SNN分类器平均稳定，引入频谱简单性概念，实验证实稳定性在实际中存在，为SNN稳定性和鲁棒性提供新见解。


<details>
  <summary>Details</summary>
Motivation: 尖峰神经网络（SNNs）在节能计算方面有前景，但与人工神经网络相比，其稳定性和鲁棒性的理论基础有限，需要深入研究。

Method: 通过布尔函数分析研究离散时间LIF - SNNs，量化输入扰动对输出的影响。

Result: 宽LIF - SNN分类器平均稳定，其傅里叶频谱集中在低频分量；引入频谱简单性概念，随机LIF - SNNs倾向于简单函数；训练网络实验证实稳定性特性在实际中存在。

Conclusion: 研究为SNNs的稳定性和鲁棒性特性提供了新的见解。

Abstract: Spiking neural networks (SNNs) are a promising paradigm for energy-efficient
computation, yet their theoretical foundations-especially regarding stability
and robustness-remain limited compared to artificial neural networks. In this
work, we study discrete-time leaky integrate-and-fire (LIF) SNNs through the
lens of Boolean function analysis. We focus on noise sensitivity and stability
in classification tasks, quantifying how input perturbations affect outputs.
Our main result shows that wide LIF-SNN classifiers are stable on average, a
property explained by the concentration of their Fourier spectrum on
low-frequency components. Motivated by this, we introduce the notion of
spectral simplicity, which formalizes simplicity in terms of Fourier spectrum
concentration and connects our analysis to the simplicity bias observed in deep
networks. Within this framework, we show that random LIF-SNNs are biased toward
simple functions. Experiments on trained networks confirm that these stability
properties persist in practice. Together, these results provide new insights
into the stability and robustness properties of SNNs.

</details>


### [174] [A generative adversarial network optimization method for damage detection and digital twinning by deep AI fault learning: Z24 Bridge structural health monitoring benchmark validation](https://arxiv.org/abs/2511.00099)
*Marios Impraimakis,Evangelia Nektaria Palkanoglou*

Main category: cs.LG

TL;DR: 研究基于优化的损伤检测和损伤状态数字孪生能力的新型条件标签生成对抗网络方法，在Z24桥验证，表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有基于人工智能的数字孪生方法在测量数据少、缺乏物理知识或损伤状态未知时预测效果差，需新方法。

Method: 使用不同相同损伤级别的测量数据作为输入，让模型有条件收敛到不同损伤状态，重复该过程并比较收敛分数；开发支持向量机分类器和主成分分析程序评估生成和真实测量数据。

Result: 该方法能准确捕捉损伤，为振动系统级监测和基础设施弹性提供有力工具。

Conclusion: 新型条件标签生成对抗网络方法优于现有故障异常检测方法，适合现实应用。

Abstract: The optimization-based damage detection and damage state digital twinning
capabilities are examined here of a novel conditional-labeled generative
adversarial network methodology. The framework outperforms current approaches
for fault anomaly detection as no prior information is required for the health
state of the system: a topic of high significance for real-world applications.
Specifically, current artificial intelligence-based digital twinning approaches
suffer from the uncertainty related to obtaining poor predictions when a low
number of measurements is available, physics knowledge is missing, or when the
damage state is unknown. To this end, an unsupervised framework is examined and
validated rigorously on the benchmark structural health monitoring measurements
of Z24 Bridge: a post-tensioned concrete highway bridge in Switzerland. In
implementing the approach, firstly, different same damage-level measurements
are used as inputs, while the model is forced to converge conditionally to two
different damage states. Secondly, the process is repeated for a different
group of measurements. Finally, the convergence scores are compared to identify
which one belongs to a different damage state. The process for both
healthy-to-healthy and damage-to-healthy input data creates, simultaneously,
measurements for digital twinning purposes at different damage states, capable
of pattern recognition and machine learning data generation. Further to this
process, a support vector machine classifier and a principal component analysis
procedure is developed to assess the generated and real measurements of each
damage category, serving as a secondary new dynamics learning indicator in
damage scenarios. Importantly, the approach is shown to capture accurately
damage over healthy measurements, providing a powerful tool for vibration-based
system-level monitoring and scalable infrastructure resilience.

</details>


### [175] [The Hidden Power of Normalization: Exponential Capacity Control in Deep Neural Networks](https://arxiv.org/abs/2511.00958)
*Khoat Than*

Main category: cs.LG

TL;DR: 本文提出理论框架，从容量控制角度解释归一化方法在深度神经网络中的作用，证明其能降低Lipschitz常数，利于优化和泛化。


<details>
  <summary>Details</summary>
Motivation: 现有理论未充分解释归一化方法对深度神经网络优化和泛化的作用机制，尤其是使用多个归一化层时。

Method: 构建理论框架，对比未归一化和插入归一化层的深度神经网络的Lipschitz常数。

Result: 未归一化的深度神经网络Lipschitz常数可能呈指数级增长，插入归一化层可指数级降低该常数，进而平滑损失景观、约束网络有效容量。

Conclusion: 为深度学习中归一化方法的经验成功提供了理论解释。

Abstract: Normalization methods are fundamental components of modern deep neural
networks (DNNs). Empirically, they are known to stabilize optimization dynamics
and improve generalization. However, the underlying theoretical mechanism by
which normalization contributes to both optimization and generalization remains
largely unexplained, especially when using many normalization layers in a DNN
architecture.
  In this work, we develop a theoretical framework that elucidates the role of
normalization through the lens of capacity control. We prove that an
unnormalized DNN can exhibit exponentially large Lipschitz constants with
respect to either its parameters or inputs, implying excessive functional
capacity and potential overfitting. Such bad DNNs are uncountably many. In
contrast, the insertion of normalization layers provably can reduce the
Lipschitz constant at an exponential rate in the number of normalization
operations. This exponential reduction yields two fundamental consequences: (1)
it smooths the loss landscape at an exponential rate, facilitating faster and
more stable optimization; and (2) it constrains the effective capacity of the
network, thereby enhancing generalization guarantees on unseen data. Our
results thus offer a principled explanation for the empirical success of
normalization methods in deep learning.

</details>


### [176] [Deep recurrent-convolutional neural network learning and physics Kalman filtering comparison in dynamic load identification](https://arxiv.org/abs/2511.00100)
*Marios Impraimakis*

Main category: cs.LG

TL;DR: 研究门控循环单元、长短期记忆网络和卷积神经网络在现实小数据集训练条件下的动态结构荷载识别能力，并与基于物理的残差卡尔曼滤波器对比。


<details>
  <summary>Details</summary>
Motivation: 解决土木工程应用中因测试数量少、结构模型不可识别导致动态荷载识别预测不佳的问题。

Method: 对模拟结构在顶层激振、加州建筑在地震基础激励、IASC - ASCE结构健康监测基准问题在冲击和瞬时加载条件下进行研究。

Result: 不同方法在不同加载场景下表现各异，在物理参数可识别的情况下，残差卡尔曼滤波器优于神经网络。

Conclusion: 不同的动态结构荷载识别方法在不同加载场景有不同表现，残差卡尔曼滤波器在特定情况下更优。

Abstract: The dynamic structural load identification capabilities of the gated
recurrent unit, long short-term memory, and convolutional neural networks are
examined herein. The examination is on realistic small dataset training
conditions and on a comparative view to the physics-based residual Kalman
filter (RKF). The dynamic load identification suffers from the uncertainty
related to obtaining poor predictions when in civil engineering applications
only a low number of tests are performed or are available, or when the
structural model is unidentifiable. In considering the methods, first, a
simulated structure is investigated under a shaker excitation at the top floor.
Second, a building in California is investigated under seismic base excitation,
which results in loading for all degrees of freedom. Finally, the International
Association for Structural Control-American Society of Civil Engineers
(IASC-ASCE) structural health monitoring benchmark problem is examined for
impact and instant loading conditions. Importantly, the methods are shown to
outperform each other on different loading scenarios, while the RKF is shown to
outperform the networks in physically parametrized identifiable cases.

</details>


### [177] [Happiness as a Measure of Fairness](https://arxiv.org/abs/2511.01069)
*Georg Pichler,Marco Romanelli,Pablo Piantanida*

Main category: cs.LG

TL;DR: 提出基于幸福概念的公平框架，方法高效可扩展，统一并扩展知名公平定义，实证显示其实用性。


<details>
  <summary>Details</summary>
Motivation: 提供更以人为本且数学严谨的公平性衡量方法。

Method: 提出基于幸福概念的公平框架，通过求解线性规划计算最优公平后处理策略。

Result: 方法高效可扩展，统一并扩展多个知名公平定义，实证显示在不同场景的实用优势。

Conclusion: 该公平框架可行且具有实际应用价值。

Abstract: In this paper, we propose a novel fairness framework grounded in the concept
of happi- ness, a measure of the utility each group gains fromdecisionoutcomes.
Bycapturingfairness through this intuitive lens, we not only offer a more
human-centered approach, but also one that is mathematically rigorous: In order
to compute the optimal, fair post-processing strategy, only a linear program
needs to be solved. This makes our method both efficient and scalable with
existing optimization tools. Furthermore, it unifies and extends several
well-known fairness definitions, and our em- pirical results highlight its
practical strengths across diverse scenarios.

</details>


### [178] [Loquetier: A Virtualized Multi-LoRA Framework for Unified LLM Fine-tuning and Serving](https://arxiv.org/abs/2511.00101)
*Yuchen Zhang,Hanyue Du,Chun Cao,Jingwei Xu*

Main category: cs.LG

TL;DR: 提出Loquetier框架，无缝集成LoRA微调与服务，性能和灵活性超现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有工作在统一基于LoRA模型的微调与推理方面存在差距，需解决此问题。

Method: 引入虚拟化模块隔离基于PEFT的修改，支持共享基础模型上的多个适配器；设计优化计算流程和内核，合并前向传播中的微调与推理路径。

Result: 在三个任务设置的大量实验中，Loquetier在性能和灵活性上均优于现有基线，推理任务吞吐量达现有系统3.0倍，统一微调与推理任务的SLO达标率比PEFT高46.4倍。

Conclusion: Loquetier能有效统一LoRA的微调与推理，且实现代码开源。

Abstract: Low-Rank Adaptation (LoRA) has become a widely adopted parameter-efficient
fine-tuning (PEFT) technique for adapting large language models (LLMs) to
downstream tasks. While prior work has explored strategies for integrating LLM
training and serving, there still remains a gap in unifying fine-tuning and
inference for LoRA-based models. We present Loquetier, a virtualized multi-LoRA
framework that seamlessly integrates LoRA fine-tuning and serving within a
single runtime. Loquetier introduces two key components: (1) a Virtualized
Module that isolates PEFT-based modifications and supports multiple adapters on
a shared base model, and (2) an optimized computation flow with a kernel design
that merges fine-tuning and inference paths in forward propagation, enabling
efficient batching and minimizing kernel invocation overhead. Extensive
experiments across three task settings show that Loquetier consistently
outperforms existing baselines in both performance and flexibility, achieving
up to $3.0\times$ the throughput of the state-of-the-art co-serving system on
inference-only tasks and $46.4\times$ higher SLO attainment than PEFT on
unified fine-tuning and inference tasks. The implementation of Loquetier is
publicly available at https://github.com/NJUDeepEngine/Loquetier.

</details>


### [179] [Regularization Implies balancedness in the deep linear network](https://arxiv.org/abs/2511.01137)
*Kathryn Lindsey,Govind Menon*

Main category: cs.LG

TL;DR: 运用几何不变理论研究深度线性网络，分解训练动态，提供平衡问题数学框架并用于解释平衡概念。


<details>
  <summary>Details</summary>
Motivation: 为深度学习和线性系统理论中的平衡问题提供通用数学框架。

Method: 使用几何不变理论（GIT）和Kempf - Ness定理，将训练动态分解为正则化流和学习流，用矩映射求解正则化流。

Result: 证明$L^2$正则化器在平衡流形上最小化，正则化流可用矩映射精确求解。

Conclusion: 该方法为平衡问题提供通用框架，可从模型简化和贝叶斯原理角度解释平衡概念。

Abstract: We use geometric invariant theory (GIT) to study the deep linear network
(DLN). The Kempf-Ness theorem is used to establish that the $L^2$ regularizer
is minimized on the balanced manifold. This allows us to decompose the training
dynamics into two distinct gradient flows: a regularizing flow on fibers and a
learning flow on the balanced manifold. We show that the regularizing flow is
exactly solvable using the moment map.
  This approach provides a common mathematical framework for balancedness in
deep learning and linear systems theory. We use this framework to interpret
balancedness in terms of model reduction and Bayesian principles.

</details>


### [180] [Automated Discovery of Conservation Laws via Hybrid Neural ODE-Transformers](https://arxiv.org/abs/2511.00102)
*Vivan Doshi*

Main category: cs.LG

TL;DR: 提出混合框架从有噪轨迹数据自动发现守恒量，在典型物理系统测试中表现优于基线，证明解耦学习再搜索方法的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 从观测数据识别守恒量是重大挑战，旨在自动化从有噪轨迹数据发现守恒量。

Method: 提出的混合框架集成了学习系统动力学连续模型的神经常微分方程、生成符号候选不变量的Transformer和验证候选有效性的符号 - 数值验证器。

Result: 在典型物理系统测试中，该框架显著优于直接处理轨迹数据的基线。

Conclusion: 解耦学习再搜索方法在从不完美数据发现数学原理方面具有鲁棒性。

Abstract: The discovery of conservation laws is a cornerstone of scientific progress.
However, identifying these invariants from observational data remains a
significant challenge. We propose a hybrid framework to automate the discovery
of conserved quantities from noisy trajectory data. Our approach integrates
three components: (1) a Neural Ordinary Differential Equation (Neural ODE) that
learns a continuous model of the system's dynamics, (2) a Transformer that
generates symbolic candidate invariants conditioned on the learned vector
field, and (3) a symbolic-numeric verifier that provides a strong numerical
certificate for the validity of these candidates. We test our framework on
canonical physical systems and show that it significantly outperforms baselines
that operate directly on trajectory data. This work demonstrates the robustness
of a decoupled learn-then-search approach for discovering mathematical
principles from imperfect data.

</details>


### [181] [Analyzing the Power of Chain of Thought through Memorization Capabilities](https://arxiv.org/abs/2511.01190)
*Lijia Yu,Xiao-Shan Gao,Lijun Zhang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: It has been shown that the chain of thought (CoT) can enhance the power of
large language models (LLMs) to solve certain mathematical reasoning problems.
However, the capacity of CoT is still not fully explored. As an important
instance, the following basic question has not yet been answered: Does CoT
expand the capability of transformers across all reasoning tasks? We
demonstrate that reasoning with transformers is essentially a memorization
problem for reasoning datasets. Thus, examining the power of CoT across all
reasoning tasks amounts to analyzing the memorization capabilities of CoT
transformers. In this paper, we give a complete description of the memorization
capabilities of fixed-precision transformers with or without CoT and give a
negative answer to the above-mentioned question. Precisely, we first give
necessary and sufficient conditions for fixed-precision transformers with and
without CoT to memorize a finite reasoning dataset and show that these two
conditions do not imply each other. Then, we give lower and upper bounds for
the number of parameters needed for transformers with or without CoT to
memorize a finite reasoning dataset with $N$ elements, which are
$\overline{\Theta}(N)$ in all cases. This implies that there exist reasoning
tasks for which CoT does not enhance the reasoning power of transformers,
leading to a negative answer to the above-mentioned question. Finally, we give
the first results on memorizing infinite reasoning datasets by CoT transformers
and show that some simple infinite datasets cannot be memorized by transformers
with or without CoT.

</details>


### [182] [Pelican-VL 1.0: A Foundation Brain Model for Embodied Intelligence](https://arxiv.org/abs/2511.00108)
*Yi Zhang,Che Liu,Xiancong Ren,Hanchu Ni,Shuai Zhang,Zeyuan Ding,Jiayu Hu,Hanzhe Shan,Zhenwei Niu,Zhaoyang Liu,Yue Zhao,Junbo Qi,Qinfan Zhang,Dengjie Li,Yidong Wang,Jiachen Luo,Yong Dai,Jian Tang,Xiaozhu Ju*

Main category: cs.LG

TL;DR: 介绍Pelican - VL 1.0开源具身大脑模型，有70亿到720亿参数，通过数据与学习机制结合及新框架训练，性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 将强大智能嵌入各种具身中。

Method: 使用metaloop从含40多亿token原始数据集提炼高质量数据集；在1000多个A800 GPU集群上训练；建立受人类元认知启发的DPPO框架，采用RL - Refine - Diagnose - SFT循环训练。

Result: 相比基础模型性能提升20.3%，比100B级开源模型高10.6%，在具身基准测试中与领先专有系统相当。

Conclusion: Pelican - VL 1.0通过数据与学习机制深度融合及新训练框架，展现出良好性能。

Abstract: This report presents Pelican-VL 1.0, a new family of open-source embodied
brain models with parameter scales ranging from 7 billion to 72 billion. Our
explicit mission is clearly stated as: To embed powerful intelligence into
various embodiments. Pelican-VL 1.0 is currently the largest-scale open-source
embodied multimodal brain model. Its core advantage lies in the in-depth
integration of data power and intelligent adaptive learning mechanisms.
Specifically, metaloop distilled a high-quality dataset from a raw dataset
containing 4+ billion tokens. Pelican-VL 1.0 is trained on a large-scale
cluster of 1000+ A800 GPUs, consuming over 50k+ A800 GPU-hours per checkpoint.
This translates to a 20.3% performance uplift from its base model and
outperforms 100B-level open-source counterparts by 10.6%, placing it on par
with leading proprietary systems on well-known embodied benchmarks. We
establish a novel framework, DPPO (Deliberate Practice Policy Optimization),
inspired by human metacognition to train Pelican-VL 1.0. We operationalize this
as a metaloop that teaches the AI to practice deliberately, which is a
RL-Refine-Diagnose-SFT loop.

</details>


### [183] [A Saddle Point Remedy: Power of Variable Elimination in Non-convex Optimization](https://arxiv.org/abs/2511.01234)
*Min Gan,Guang-Yong Chen,Yang Yi,Lin Yang*

Main category: cs.LG

TL;DR: 本文通过比较原公式和简化公式的优化景观，对变量消除算法为何有效导航复杂能量景观给出严格几何解释，还在多个问题上验证，为设计新优化算法提供原则。


<details>
  <summary>Details</summary>
Motivation: 鞍点的扩散是机器学习大规模非凸优化的主要障碍，变量消除算法实践中表现好但原理不明，需解释其有效原因。

Method: 基于Hessian惯性和Schur补进行严格分析，比较原公式和简化公式的优化景观。

Result: 证明变量消除从根本上重塑目标函数的临界点结构，简化景观中局部最大值由原公式鞍点产生并直接对应；在非凸矩阵分解、双参数神经网络和深度残差网络训练中验证发现。

Conclusion: 建立了通过鞍点变换简化景观的强大原则，可指导新一代更稳健高效优化算法的设计。

Abstract: The proliferation of saddle points, rather than poor local minima, is
increasingly understood to be a primary obstacle in large-scale non-convex
optimization for machine learning. Variable elimination algorithms, like
Variable Projection (VarPro), have long been observed to exhibit superior
convergence and robustness in practice, yet a principled understanding of why
they so effectively navigate these complex energy landscapes has remained
elusive. In this work, we provide a rigorous geometric explanation by comparing
the optimization landscapes of the original and reduced formulations. Through a
rigorous analysis based on Hessian inertia and the Schur complement, we prove
that variable elimination fundamentally reshapes the critical point structure
of the objective function, revealing that local maxima in the reduced landscape
are created from, and correspond directly to, saddle points in the original
formulation. Our findings are illustrated on the canonical problem of
non-convex matrix factorization, visualized directly on two-parameter neural
networks, and finally validated in training deep Residual Networks, where our
approach yields dramatic improvements in stability and convergence to superior
minima. This work goes beyond explaining an existing method; it establishes
landscape simplification via saddle point transformation as a powerful
principle that can guide the design of a new generation of more robust and
efficient optimization algorithms.

</details>


### [184] [MeixnerNet: Adaptive and Robust Spectral Graph Neural Networks with Discrete Orthogonal Polynomials](https://arxiv.org/abs/2511.00113)
*Huseyin Goksu*

Main category: cs.LG

TL;DR: 本文提出MeixnerNet谱图神经网络架构，使用离散正交多项式，实验显示其性能优于ChebyNet且对超参数更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 传统谱图神经网络使用连续正交多项式滤波器与离散图结构不匹配，可能导致性能不佳和对超参数敏感。

Method: 引入MeixnerNet架构，采用Meixner多项式，使多项式的关键参数可学习，还引入结合拉普拉斯缩放和每层归一化的稳定技术。

Result: 在K = 2的最优设置下，MeixnerNet在2/3的基准测试中优于ChebyNet，且对多项式阶数K的变化更鲁棒。

Conclusion: MeixnerNet能有效解决传统谱图神经网络的问题，性能和鲁棒性更好。

Abstract: Spectral Graph Neural Networks (GNNs) have achieved state-of-the-art results
by defining graph convolutions in the spectral domain. A common approach,
popularized by ChebyNet, is to use polynomial filters based on continuous
orthogonal polynomials (e.g., Chebyshev). This creates a theoretical
disconnect, as these continuous-domain filters are applied to inherently
discrete graph structures. We hypothesize this mismatch can lead to suboptimal
performance and fragility to hyperparameter settings.
  In this paper, we introduce MeixnerNet, a novel spectral GNN architecture
that employs discrete orthogonal polynomials -- specifically, the Meixner
polynomials $M_k(x; \beta, c)$. Our model makes the two key shape parameters of
the polynomial, beta and c, learnable, allowing the filter to adapt its
polynomial basis to the specific spectral properties of a given graph. We
overcome the significant numerical instability of these polynomials by
introducing a novel stabilization technique that combines Laplacian scaling
with per-basis LayerNorm.
  We demonstrate experimentally that MeixnerNet achieves
competitive-to-superior performance against the strong ChebyNet baseline at the
optimal K = 2 setting (winning on 2 out of 3 benchmarks). More critically, we
show that MeixnerNet is exceptionally robust to variations in the polynomial
degree K, a hyperparameter to which ChebyNet proves to be highly fragile,
collapsing in performance where MeixnerNet remains stable.

</details>


### [185] [A Spatio-Temporal Online Robust Tensor Recovery Approach for Streaming Traffic Data Imputation](https://arxiv.org/abs/2511.01267)
*Yiyang Yang,Xiejian Chi,Shanxing Gao,Kaidong Wang,Yao Wang*

Main category: cs.LG

TL;DR: 本文提出在线鲁棒张量恢复算法解决交通数据恢复问题，在真实数据集实验中证明其高效准确。


<details>
  <summary>Details</summary>
Motivation: 传统批处理方法计算和存储资源需求大，现有在线张量恢复方法在复杂场景性能差，需解决交通数据恢复问题。

Method: 在流框架下重新定义交通数据恢复问题，提出在线鲁棒张量恢复算法，利用数据全局时空相关性和局部一致性。

Result: 该方法能处理缺失和异常值，适应不同缺失模式，在三个真实数据集上恢复精度高，计算效率比现有批处理方法最多提高三个数量级。

Conclusion: 该方法是智能交通系统中提升交通数据质量的可扩展有效解决方案。

Abstract: Data quality is critical to Intelligent Transportation Systems (ITS), as
complete and accurate traffic data underpin reliable decision-making in traffic
control and management. Recent advances in low-rank tensor recovery algorithms
have shown strong potential in capturing the inherent structure of
high-dimensional traffic data and restoring degraded observations. However,
traditional batch-based methods demand substantial computational and storage
resources, which limits their scalability in the face of continuously expanding
traffic data volumes. Moreover, recent online tensor recovery methods often
suffer from severe performance degradation in complex real-world scenarios due
to their insufficient exploitation of the intrinsic structural properties of
traffic data. To address these challenges, we reformulate the traffic data
recovery problem within a streaming framework, and propose a novel online
robust tensor recovery algorithm that simultaneously leverages both the global
spatio-temporal correlations and local consistency of traffic data, achieving
high recovery accuracy and significantly improved computational efficiency in
large-scale scenarios. Our method is capable of simultaneously handling missing
and anomalous values in traffic data, and demonstrates strong adaptability
across diverse missing patterns. Experimental results on three real-world
traffic datasets demonstrate that the proposed approach achieves high recovery
accuracy while significantly improving computational efficiency by up to three
orders of magnitude compared to state-of-the-art batch-based methods. These
findings highlight the potential of the proposed approach as a scalable and
effective solution for traffic data quality enhancement in ITS.

</details>


### [186] [LC-Opt: Benchmarking Reinforcement Learning and Agentic AI for End-to-End Liquid Cooling Optimization in Data Centers](https://arxiv.org/abs/2511.00116)
*Avisek Naug,Antonio Guillen,Vineet Kumar,Scott Greenwood,Wesley Brewer,Sahand Ghorbanpour,Ashwin Ramesh Babu,Vineet Gundecha,Ricardo Luna Gutierrez,Soumyendu Sarkar*

Main category: cs.LG

TL;DR: 提出LC - Opt可持续液体冷却基准环境，用于强化学习控制策略，可平衡热调节与能源效率，支持多方法并促进开发冷却控制解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着AI工作负载增加，液体冷却对数据中心热管理至关重要，机器学习控制器可提升能源效率和可靠性，推动可持续性。

Method: 基于橡树岭国家实验室前沿超级计算机冷却系统的数字孪生构建LC - Opt，通过Gymnasium接口让RL智能体优化热控制，对集中和分散多智能体RL方法进行基准测试，将策略提炼为决策树和回归树，探索基于LLM的方法。

Result: LC - Opt创建了平衡局部热调节和全局能源效率的多目标实时优化挑战，支持热回收单元等组件。

Conclusion: LC - Opt使详细、可定制的液体冷却模型更易获取，有助于ML社区、运营商和供应商开发可持续数据中心液体冷却控制解决方案。

Abstract: Liquid cooling is critical for thermal management in high-density data
centers with the rising AI workloads. However, machine learning-based
controllers are essential to unlock greater energy efficiency and reliability,
promoting sustainability. We present LC-Opt, a Sustainable Liquid Cooling (LC)
benchmark environment, for reinforcement learning (RL) control strategies in
energy-efficient liquid cooling of high-performance computing (HPC) systems.
Built on the baseline of a high-fidelity digital twin of Oak Ridge National
Lab's Frontier Supercomputer cooling system, LC-Opt provides detailed
Modelica-based end-to-end models spanning site-level cooling towers to data
center cabinets and server blade groups. RL agents optimize critical thermal
controls like liquid supply temperature, flow rate, and granular valve
actuation at the IT cabinet level, as well as cooling tower (CT) setpoints
through a Gymnasium interface, with dynamic changes in workloads. This
environment creates a multi-objective real-time optimization challenge
balancing local thermal regulation and global energy efficiency, and also
supports additional components like a heat recovery unit (HRU). We benchmark
centralized and decentralized multi-agent RL approaches, demonstrate policy
distillation into decision and regression trees for interpretable control, and
explore LLM-based methods that explain control actions in natural language
through an agentic mesh architecture designed to foster user trust and simplify
system management. LC-Opt democratizes access to detailed, customizable liquid
cooling models, enabling the ML community, operators, and vendors to develop
sustainable data center liquid cooling control solutions.

</details>


### [187] [DCcluster-Opt: Benchmarking Dynamic Multi-Objective Optimization for Geo-Distributed Data Center Workloads](https://arxiv.org/abs/2511.00117)
*Antonio Guillen-Perez,Avisek Naug,Vineet Gundecha,Sahand Ghorbanpour,Ricardo Luna Gutierrez,Ashwin Ramesh Babu,Munther Salim,Shubhanker Banerjee,Eoin H. Oude Essink,Damien Fay,Soumyendu Sarkar*

Main category: cs.LG

TL;DR: 提出开源高保真模拟基准DCcluster - Opt，用于可持续的地理时间任务调度，加速分布式数据中心可持续计算解决方案的开发与验证。


<details>
  <summary>Details</summary>
Motivation: 大规模AI能源需求和碳足迹增长，需要智能工作负载管理，但缺乏能反映多因素相互作用的基准。

Method: 结合真实世界数据集和数据中心运营物理模型，提出具有挑战性的调度问题，有模块化奖励系统，提供Gymnasium API和基线控制器。

Result: 提供了现实、可配置且易访问的测试平台。

Conclusion: DCcluster - Opt能加速分布式数据中心下一代可持续计算解决方案的开发和验证。

Abstract: The increasing energy demands and carbon footprint of large-scale AI require
intelligent workload management in globally distributed data centers. Yet
progress is limited by the absence of benchmarks that realistically capture the
interplay of time-varying environmental factors (grid carbon intensity,
electricity prices, weather), detailed data center physics (CPUs, GPUs, memory,
HVAC energy), and geo-distributed network dynamics (latency and transmission
costs). To bridge this gap, we present DCcluster-Opt: an open-source,
high-fidelity simulation benchmark for sustainable, geo-temporal task
scheduling. DCcluster-Opt combines curated real-world datasets, including AI
workload traces, grid carbon intensity, electricity markets, weather across 20
global regions, cloud transmission costs, and empirical network delay
parameters with physics-informed models of data center operations, enabling
rigorous and reproducible research in sustainable computing. It presents a
challenging scheduling problem where a top-level coordinating agent must
dynamically reassign or defer tasks that arrive with resource and service-level
agreement requirements across a configurable cluster of data centers to
optimize multiple objectives. The environment also models advanced components
such as heat recovery. A modular reward system enables an explicit study of
trade-offs among carbon emissions, energy costs, service level agreements, and
water use. It provides a Gymnasium API with baseline controllers, including
reinforcement learning and rule-based strategies, to support reproducible ML
research and a fair comparison of diverse algorithms. By offering a realistic,
configurable, and accessible testbed, DCcluster-Opt accelerates the development
and validation of next-generation sustainable computing solutions for
geo-distributed data centers.

</details>


### [188] [Estimation of Toeplitz Covariance Matrices using Overparameterized Gradient Descent](https://arxiv.org/abs/2511.01605)
*Daniel Busbib,Ami Wiesel*

Main category: cs.LG

TL;DR: 本文从过参数化梯度下降（GD）的角度重新审视Toeplitz协方差估计，发现适度过参数化能实现全局收敛，提出加速GD变体，实验表明过参数化GD在挑战性场景中表现良好。


<details>
  <summary>Details</summary>
Motivation: 受深度学习中过参数化梯度下降的启发，重新审视Toeplitz协方差估计问题。

Method: 将$P	imes P$协方差建模为$K$个复正弦波之和，通过GD优化；提出有不同学习率的加速GD变体。

Result: 当$K = P$时，GD可能收敛到次优解；适度过参数化（$K = 2P$或$4P$）能实现全局收敛；固定频率仅优化振幅时，优化景观渐近良好。

Conclusion: 过参数化GD在挑战性场景中能达到或超过现有最优方法的精度，且简单可扩展。

Abstract: We consider covariance estimation under Toeplitz structure. Numerous
sophisticated optimization methods have been developed to maximize the Gaussian
log-likelihood under Toeplitz constraints. In contrast, recent advances in deep
learning demonstrate the surprising power of simple gradient descent (GD)
applied to overparameterized models. Motivated by this trend, we revisit
Toeplitz covariance estimation through the lens of overparameterized GD. We
model the $P\times P$ covariance as a sum of $K$ complex sinusoids with
learnable parameters and optimize them via GD. We show that when $K = P$, GD
may converge to suboptimal solutions. However, mild overparameterization ($K =
2P$ or $4P$) consistently enables global convergence from random
initializations. We further propose an accelerated GD variant with separate
learning rates for amplitudes and frequencies. When frequencies are fixed and
only amplitudes are optimized, we prove that the optimization landscape is
asymptotically benign and any stationary point recovers the true covariance.
Finally, numerical experiments demonstrate that overparameterized GD can match
or exceed the accuracy of state-of-the-art methods in challenging settings,
while remaining simple and scalable.

</details>


### [189] [Analysis of Line Break prediction models for detecting defensive breakthrough in football](https://arxiv.org/abs/2511.00121)
*Shoma Yagi,Jun Ichikawa,Genki Ichinose*

Main category: cs.LG

TL;DR: 本文开发机器学习模型预测足球比赛中防线突破，模型准确性高，揭示相关影响因素及与得分机会联系。


<details>
  <summary>Details</summary>
Motivation: 以往研究多关注射门或得分机会，较少关注球队如何突破防线，本文旨在填补此空白。

Method: 利用2023年J1联赛赛季的事件和跟踪数据，构建包含189个特征的机器学习模型，采用XGBoost分类器估计防线突破概率。

Result: 模型预测准确性高，AUC为0.982，Brier分数为0.015；SHAP分析揭示进攻球员速度、防线缺口等因素对防线突破有重要影响；球队层面防线被突破概率与失球数和传中数呈中度正相关。

Conclusion: 防线突破与得分机会密切相关，为理解足球战术动态提供定量框架。

Abstract: In football, attacking teams attempt to break through the opponent's
defensive line to create scoring opportunities. This action, known as a Line
Break, is a critical indicator of offensive effectiveness and tactical
performance, yet previous studies have mainly focused on shots or goal
opportunities rather than on how teams break the defensive line. In this study,
we develop a machine learning model to predict Line Breaks using event and
tracking data from the 2023 J1 League season. The model incorporates 189
features, including player positions, velocities, and spatial configurations,
and employs an XGBoost classifier to estimate the probability of Line Breaks.
The proposed model achieved high predictive accuracy, with an AUC of 0.982 and
a Brier score of 0.015. Furthermore, SHAP analysis revealed that factors such
as offensive player speed, gaps in the defensive line, and offensive players'
spatial distributions significantly contribute to the occurrence of Line
Breaks. Finally, we found a moderate positive correlation between the predicted
probability of being Line-Broken and the number of shots and crosses conceded
at the team level. These results suggest that Line Breaks are closely linked to
the creation of scoring opportunities and provide a quantitative framework for
understanding tactical dynamics in football.

</details>


### [190] [Cross-Treatment Effect Estimation for Multi-Category, Multi-Valued Causal Inference via Dynamic Neural Masking](https://arxiv.org/abs/2511.01641)
*Xiaopeng Ke,Yihan Yu,Ruyue Zhang,Zhishuo Zhou,Fangzhou Shi,Chang Men,Zhengdan Zhu*

Main category: cs.LG

TL;DR: 提出XTNet网络架构用于多类别、多值处理效应估计，还提出MCMV - AUCC评估指标，实验显示其性能优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有反事实因果推理方法在处理多类别、多值处理时存在局限，如受限于二元或单一类型处理，有假设限制、可扩展性差和评估框架不足等问题。

Method: 提出XTNet网络架构，引入带动态掩码机制的交叉效应估计模块，采用分解策略分离基本效应和交叉处理交互；提出MCMV - AUCC评估指标。

Result: 在合成和真实数据集上的实验表明，XTNet在排名准确性和效应估计质量上始终优于现有基线，真实世界A/B测试结果也证实其有效性。

Conclusion: XTNet是一种有效的多类别、多值处理效应估计方法。

Abstract: Counterfactual causal inference faces significant challenges when extended to
multi-category, multi-valued treatments, where complex cross-effects between
heterogeneous interventions are difficult to model. Existing methodologies
remain constrained to binary or single-type treatments and suffer from
restrictive assumptions, limited scalability, and inadequate evaluation
frameworks for complex intervention scenarios.
  We present XTNet, a novel network architecture for multi-category,
multi-valued treatment effect estimation. Our approach introduces a
cross-effect estimation module with dynamic masking mechanisms to capture
treatment interactions without restrictive structural assumptions. The
architecture employs a decomposition strategy separating basic effects from
cross-treatment interactions, enabling efficient modeling of combinatorial
treatment spaces. We also propose MCMV-AUCC, a suitable evaluation metric that
accounts for treatment costs and interaction effects. Extensive experiments on
synthetic and real-world datasets demonstrate that XTNet consistently
outperforms state-of-the-art baselines in both ranking accuracy and effect
estimation quality. The results of the real-world A/B test further confirm its
effectiveness.

</details>


### [191] [Cross-fluctuation phase transitions reveal sampling dynamics in diffusion models](https://arxiv.org/abs/2511.00124)
*Sai Niranjan Ramachandran,Manish Krishan Lal,Suvrit Sra*

Main category: cs.LG

TL;DR: 利用交叉涨落分析分数扩散模型中分布采样动态，发现离散转变可检测，能提升采样效率和完成零样本任务，还统一了相关理论。


<details>
  <summary>Details</summary>
Motivation: 分析分数扩散模型中分布的采样动态演变。

Method: 使用统计物理中的交叉涨落统计量，推导方差保持SDE的交叉涨落闭式解。

Result: 检测到离散转变可提升采样效率、加速生成任务、改善零样本任务，统一了经典耦合和连续动力学。

Conclusion: 该框架桥接了离散马尔可夫链理论、相位分析和现代生成建模。

Abstract: We analyse how the sampling dynamics of distributions evolve in score-based
diffusion models using cross-fluctuations, a centered-moment statistic from
statistical physics. Specifically, we show that starting from an unbiased
isotropic normal distribution, samples undergo sharp, discrete transitions,
eventually forming distinct events of a desired distribution while
progressively revealing finer structure. As this process is reversible, these
transitions also occur in reverse, where intermediate states progressively
merge, tracing a path back to the initial distribution. We demonstrate that
these transitions can be detected as discontinuities in $n^{\text{th}}$-order
cross-fluctuations. For variance-preserving SDEs, we derive a closed-form for
these cross-fluctuations that is efficiently computable for the reverse
trajectory. We find that detecting these transitions directly boosts sampling
efficiency, accelerates class-conditional and rare-class generation, and
improves two zero-shot tasks--image classification and style transfer--without
expensive grid search or retraining. We also show that this viewpoint unifies
classical coupling and mixing from finite Markov chains with continuous
dynamics while extending to stochastic SDEs and non Markovian samplers. Our
framework therefore bridges discrete Markov chain theory, phase analysis, and
modern generative modeling.

</details>


### [192] [Fractional Diffusion Bridge Models](https://arxiv.org/abs/2511.01795)
*Gabriel Nobis,Maximilian Springenberg,Arina Belova,Rembert Daems,Christoph Knochenhauer,Manfred Opper,Tolga Birdal,Wojciech Samek*

Main category: cs.LG

TL;DR: 提出分数扩散桥模型（FDBM），基于分数布朗运动近似，在蛋白质构象预测和图像翻译任务中表现优于布朗运动基线。


<details>
  <summary>Details</summary>
Motivation: 标准扩散或桥模型使用布朗运动，无法捕捉真实随机过程的记忆效应、长程依赖等现象，因此需要新模型。

Method: 利用分数布朗运动的马尔可夫近似构建FDBM，证明耦合保持生成扩散桥的存在，将其扩展到薛定谔桥问题并推导损失函数。

Result: 在蛋白质构象预测中Cα原子位置的均方根偏差更低，在无配对图像翻译中Fréchet Inception距离更低。

Conclusion: FDBM在预测未来蛋白质构象和无配对图像翻译任务中表现优于布朗运动基线。

Abstract: We present Fractional Diffusion Bridge Models (FDBM), a novel generative
diffusion bridge framework driven by an approximation of the rich and
non-Markovian fractional Brownian motion (fBM). Real stochastic processes
exhibit a degree of memory effects (correlations in time), long-range
dependencies, roughness and anomalous diffusion phenomena that are not captured
in standard diffusion or bridge modeling due to the use of Brownian motion
(BM). As a remedy, leveraging a recent Markovian approximation of fBM (MA-fBM),
we construct FDBM that enable tractable inference while preserving the
non-Markovian nature of fBM. We prove the existence of a coupling-preserving
generative diffusion bridge and leverage it for future state prediction from
paired training data. We then extend our formulation to the Schr\"{o}dinger
bridge problem and derive a principled loss function to learn the unpaired data
translation. We evaluate FDBM on both tasks: predicting future protein
conformations from aligned data, and unpaired image translation. In both
settings, FDBM achieves superior performance compared to the Brownian
baselines, yielding lower root mean squared deviation (RMSD) of C$_\alpha$
atomic positions in protein structure prediction and lower Fr\'echet Inception
Distance (FID) in unpaired image translation.

</details>


### [193] [Dynamic Model Selection for Trajectory Prediction via Pairwise Ranking and Meta-Features](https://arxiv.org/abs/2511.00126)
*Lu Bowen*

Main category: cs.LG

TL;DR: 现有深度轨迹预测器在复杂长尾驾驶场景中不可靠，本文提出动态多专家门控框架，在nuPlan - mini数据集上效果良好，表明自适应混合系统可提升轨迹可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有深度轨迹预测器在复杂长尾驾驶场景有局限性，“one - model - fits - all”范式存在弱点，需提升轨迹预测可靠性。

Method: 提出动态多专家门控框架，在物理信息LSTM、Transformer和微调的GameFormer中逐样本选择最可靠的轨迹预测器，利用内部模型信号如稳定性和不确定性，将轨迹专家选择表述为内部模型信号的成对排序问题。

Result: 在nuPlan - mini数据集上，LLM增强的三专家门控实现FDE为2.567m，比GameFormer降低9.5%，达到57.8%的oracle性能边界；在开环模拟中，左转场景FDE降低约10%。

Conclusion: 自适应混合系统能提升安全关键自动驾驶中的轨迹可靠性，提供了超越静态单模型范式的实用途径。

Abstract: Recent deep trajectory predictors (e.g., Jiang et al., 2023; Zhou et al.,
2022) have achieved strong average accuracy but remain unreliable in complex
long-tail driving scenarios. These limitations reveal the weakness of the
prevailing "one-model-fits-all" paradigm, particularly in safety-critical urban
contexts where simpler physics-based models can occasionally outperform
advanced networks (Kalman, 1960). To bridge this gap, we propose a dynamic
multi-expert gating framework that adaptively selects the most reliable
trajectory predictor among a physics-informed LSTM, a Transformer, and a
fine-tuned GameFormer on a per-sample basis.
  Our method leverages internal model signals (meta-features) such as stability
and uncertainty (Gal and Ghahramani, 2016), which we demonstrate to be
substantially more informative than geometric scene descriptors. To the best of
our knowledge, this is the first work to formulate trajectory expert selection
as a pairwise-ranking problem over internal model signals (Burges et al.,
2005), directly optimizing decision quality without requiring post-hoc
calibration.
  Evaluated on the nuPlan-mini dataset (Caesar et al., 2021) with 1,287
samples, our LLM-enhanced tri-expert gate achieves a Final Displacement Error
(FDE) of 2.567 m, representing a 9.5 percent reduction over GameFormer (2.835
m), and realizes 57.8 percent of the oracle performance bound. In open-loop
simulations, after trajectory horizon alignment, the same configuration reduces
FDE on left-turn scenarios by approximately 10 percent, demonstrating
consistent improvements across both offline validation and open-loop
evaluation. These results indicate that adaptive hybrid systems enhance
trajectory reliability in safety-critical autonomous driving, providing a
practical pathway beyond static single-model paradigms.

</details>


### [194] [Bridging Lifelong and Multi-Task Representation Learning via Algorithm and Complexity Measure](https://arxiv.org/abs/2511.01847)
*Zhi Wang,Chicheng Zhang,Ramya Korlakai Vinayak*

Main category: cs.LG

TL;DR: 研究终身表征学习广义框架，提出简单算法并建立样本复杂度界，结果适用于多种学习问题。


<details>
  <summary>Details</summary>
Motivation: 终身学习中学习者需利用任务共享结构加速学习，与多任务学习等不同，需在线利用已有知识和部分信息，所以研究广义终身表征学习框架。

Method: 提出使用多任务经验风险最小化作为子程序的简单算法，基于新引入的任务回避维度建立样本复杂度界。

Result: 结果适用于涉及一般函数类的广泛学习问题，还在有噪声的分类和回归任务上进行实例化。

Conclusion: 所提算法和复杂度界对终身表征学习有一定意义，可应用于多种学习场景。

Abstract: In lifelong learning, a learner faces a sequence of tasks with shared
structure and aims to identify and leverage it to accelerate learning. We study
the setting where such structure is captured by a common representation of
data. Unlike multi-task learning or learning-to-learn, where tasks are
available upfront to learn the representation, lifelong learning requires the
learner to make use of its existing knowledge while continually gathering
partial information in an online fashion. In this paper, we consider a
generalized framework of lifelong representation learning. We propose a simple
algorithm that uses multi-task empirical risk minimization as a subroutine and
establish a sample complexity bound based on a new notion we introduce--the
task-eluder dimension. Our result applies to a wide range of learning problems
involving general function classes. As concrete examples, we instantiate our
result on classification and regression tasks under noise.

</details>


### [195] [Casing Collar Identification using AlexNet-based Neural Networks for Depth Measurement in Oil and Gas Wells](https://arxiv.org/abs/2511.00129)
*Siyu Xiao,Xindi Zhao,Tianhao Mao,Yiwei Wang,Yuqiao Chen,Hongyun Zhang,Jian Wang,Junjie Wang,Shuang Liu,Tupei Chen,Yang Liu*

Main category: cs.LG

TL;DR: 本文提出井下CCL信号采集系统以构建数据集，给出数据增强预处理方法并评估其效果，实验表明部分方法是模型训练基础，部分能增强泛化能力，F1分数提升，在真实波形上验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 神经网络CCL信号识别的预处理方法发展不足，且真实井数据有限，难以训练神经网络模型。

Method: 提出集成于井下工具的CCL信号采集系统构建数据集，给出数据增强预处理方法，用基于AlexNet的神经网络模型评估其效果，并进行系统实验。

Result: 标准化、LDS和随机裁剪是模型训练基本要求，LSR、时间缩放和多重采样增强模型泛化能力，两个基准模型F1分数最高提升到1.0，在真实CCL波形上验证了方法有效性。

Conclusion: 本工作弥补了CCL数据有限环境下套管接箍识别模型数据增强方法的不足。

Abstract: Accurate downhole depth measurement is essential for oil and gas well
operations, directly influencing reservoir contact, production efficiency, and
operational safety. Collar correlation using a casing collar locator (CCL) is
fundamental for precise depth calibration. While neural network-based CCL
signal recognition has achieved significant progress in collar identification,
preprocessing methods for such applications remain underdeveloped. Moreover,
the limited availability of real well data poses substantial challenges for
training neural network models that require extensive datasets. This paper
presents a system integrated into downhole tools for CCL signal acquisition to
facilitate dataset construction. We propose comprehensive preprocessing methods
for data augmentation and evaluate their effectiveness using our AlexNet-based
neural network models. Through systematic experimentation across various
configuration combinations, we analyze the contribution of each augmentation
method. Results demonstrate that standardization, label distribution smoothing
(LDS), and random cropping are fundamental requirements for model training,
while label smoothing regularization (LSR), time scaling, and multiple sampling
significantly enhance model generalization capability. The F1 scores of our two
benchmark models trained with the proposed augmentation methods maximumly
improve from 0.937 and 0.952 to 1.0 and 1.0, respectively. Performance
validation on real CCL waveforms confirms the effectiveness and practical
applicability of our approach. This work addresses the gaps in data
augmentation methodologies for training casing collar recognition models in CCL
data-limited environments.

</details>


### [196] [A Comparative Analysis of LLM Adaptation: SFT, LoRA, and ICL in Data-Scarce Scenarios](https://arxiv.org/abs/2511.00130)
*Bernd Bohnet,Rumen Dangovski,Kevin Swersky,Sherry Moore,Arslan Chaudhry,Kathleen Kenealy,Noah Fiedel*

Main category: cs.LG

TL;DR: 本文对数据稀缺场景下的SFT、LoRA和ICL进行对比分析，发现LoRA在新技能注入和保留基础模型通用知识间平衡最佳，研究为选择大语言模型适配策略提供框架。


<details>
  <summary>Details</summary>
Motivation: 大语言模型需为特定应用定制，但全量微调计算成本高且会导致灾难性遗忘，现有替代技术各有优劣，需找到给定任务的最佳适配策略。

Method: 对数据稀缺场景下的Supervised Finetuning (SFT)、Low - Rank Adaptation (LoRA)和In - Context Learning (ICL)进行对比分析。

Result: LoRA能在注入新技能时对基础模型通用知识影响最小；SFT擅长技能获取但易出现灾难性遗忘；ICL在整合事实知识有效，但处理复杂技能有困难。

Conclusion: 研究为选择大语言模型适配策略提供实用框架，强调技能获取和知识整合区别，明确特定任务性能和保留通用能力的权衡。

Abstract: The remarkable capabilities of Large Language Models (LLMs) often need to be
tailored for specific applications, requiring the integration of new knowledge
or the acquisition of new skills. While full fine-tuning is a powerful
adaptation method, it is computationally expensive and can lead to a
degradation of general reasoning abilities, a phenomenon known as catastrophic
forgetting. A range of alternative techniques exists, each with its own
trade-offs. In-Context Learning (ICL) is fast but limited by context length,
while Parameter-Efficient Fine-Tuning (PEFT) methods like Low-Rank Adaptation
(LoRA) offer a middle ground by minimizing parameter changes. However, the
challenge of catastrophic forgetting persists, raising questions about the best
adaptation strategy for a given task. This paper presents a comparative
analysis of Supervised Finetuning (SFT), LoRA, and ICL in data-scarce
scenarios. We find that LoRA provides the most effective balance, successfully
instilling new skills with minimal impact on the base model's general
knowledge. In contrast, while SFT excels at skill acquisition, it is highly
susceptible to catastrophic forgetting. ICL is effective for incorporating
factual knowledge but struggles with complex skills. Our findings offer a
practical framework for selecting an LLM adaptation strategy. We highlight the
critical distinction between skill acquisition and knowledge integration,
clarify the trade-offs between task-specific performance and the preservation
of general capabilities.

</details>


### [197] [Feature Importance Guided Random Forest Learning with Simulated Annealing Based Hyperparameter Tuning](https://arxiv.org/abs/2511.00133)
*Kowshik Balasubramanian,Andre Williams,Ismail Butun*

Main category: cs.LG

TL;DR: 本文提出通过概率特征采样和模拟退火超参数调优增强随机森林分类器的框架，在多领域提升预测准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 克服传统随机森林的局限性，解决多领域鲁棒分类的挑战。

Method: 集成概率特征采样和通过模拟退火进行超参数调优，强调捕捉数据相关信号和自适应超参数配置。

Result: 模型在预测准确性上有持续提升，能洞察特征相关性。

Conclusion: 结合重要性感知采样和元启发式优化的方法有效。

Abstract: This paper introduces a novel framework for enhancing Random Forest
classifiers by integrating probabilistic feature sampling and hyperparameter
tuning via Simulated Annealing. The proposed framework exhibits substantial
advancements in predictive accuracy and generalization, adeptly tackling the
multifaceted challenges of robust classification across diverse domains,
including credit risk evaluation, anomaly detection in IoT ecosystems,
early-stage medical diagnostics, and high-dimensional biological data analysis.
To overcome the limitations of conventional Random Forests, we present an
approach that places stronger emphasis on capturing the most relevant signals
from data while enabling adaptive hyperparameter configuration. The model is
guided towards features that contribute more meaningfully to classification and
optimizing this with dynamic parameter tuning. The results demonstrate
consistent accuracy improvements and meaningful insights into feature
relevance, showcasing the efficacy of combining importance aware sampling and
metaheuristic optimization.

</details>


### [198] [Physiologically Active Vegetation Reverses Its Cooling Effect in Humid Urban Climates](https://arxiv.org/abs/2511.00134)
*Angana Borah,Adrija Datta,Ashish S. Kumar,Raviraj Dave,Udit Bhatia*

Main category: cs.LG

TL;DR: 研究量化印度138个城市植被结构和功能对热指数的影响，明确植被降温气候极限。


<details>
  <summary>Details</summary>
Motivation: 以往研究对植被在降温与湿度累积间权衡机制不明，导致缓解政策和设计缺乏指导。

Method: 量化138个印度城市植被结构和功能对热指数的影响，用极端感知、一公里热指数重建及集成SHAP和ALE的机器学习框架分离植被 - 气候相互作用。

Result: EVI >= 0.4和LAI >= 0.05时冷却增强；EVI >= 0.5、LAI >= 0.2和fPAR >= 0.5时高联合状态转向变暖，在潮湿密集核心区fPAR >= 0.25时更早出现。高生理活性植被使近地表湿度升高快于散热。

Conclusion: 研究确定植被降温气候极限，为特定气候绿化策略提供量化阈值以促进公平和耐热城市建设。

Abstract: Efforts to green cities for cooling are succeeding unevenly because the same
vegetation that cools surfaces can also intensify how hot the air feels.
Previous studies have identified humid heat as a growing urban hazard, yet how
physiologically active vegetation governs this trade-off between cooling and
moisture accumulation remains poorly understood, leaving mitigation policy and
design largely unguided. Here we quantify how vegetation structure and function
influence the Heat Index (HI), a combined measure of temperature and humidity
in 138 Indian cities spanning tropical savanna, semi-arid steppe, and humid
subtropical climates, and across dense urban cores and semi-urban rings. Using
an extreme-aware, one kilometre reconstruction of HI and an interpretable
machine-learning framework that integrates SHapley Additive Explanations (SHAP)
and Accumulated Local Effects (ALE), we isolate vegetation-climate
interactions. Cooling generally strengthens for EVI >= 0.4 and LAI >= 0.05, but
joint-high regimes begin to reverse toward warming when EVI >= 0.5, LAI >= 0.2,
and fPAR >= 0.5,with an earlier onset for fPAR >= 0.25 in humid, dense cores.
In such environments, highly physiologically active vegetation elevates
near-surface humidity faster than it removes heat, reversing its cooling effect
and amplifying perceived heat stress. These findings establish the climatic
limits of vegetation-driven cooling and provide quantitative thresholds for
climate-specific greening strategies that promote equitable and heat-resilient
cities.

</details>


### [199] [A Dual Large Language Models Architecture with Herald Guided Prompts for Parallel Fine Grained Traffic Signal Control](https://arxiv.org/abs/2511.00136)
*Qing Guo,Xinhang Li,Junyu Chen,Zheng Guo,Xiaocong Li,Lin Zhang,Lei Li*

Main category: cs.LG

TL;DR: 论文提出HeraldLight解决现有TSC方法问题，经实验验证效果优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的TSC方法有固定时长和幻觉误差问题，RL方法缺乏鲁棒性和泛化性。

Method: 提出HeraldLight，含Herald模块及双LLM架构，用细化输出进行基于分数的微调。

Result: 在CityFlow模拟实验中，HeraldLight优于基线，平均旅行时间降20.03%，济南和杭州场景平均队列长度降10.74%。

Conclusion: HeraldLight有效解决现有TSC方法问题，提升交通信号控制性能。

Abstract: Leveraging large language models (LLMs) in traffic signal control (TSC)
improves optimization efficiency and interpretability compared to traditional
reinforcement learning (RL) methods. However, existing LLM-based approaches are
limited by fixed time signal durations and are prone to hallucination errors,
while RL methods lack robustness in signal timing decisions and suffer from
poor generalization. To address these challenges, this paper proposes
HeraldLight, a dual LLMs architecture enhanced by Herald guided prompts. The
Herald Module extracts contextual information and forecasts queue lengths for
each traffic phase based on real-time conditions. The first LLM, LLM-Agent,
uses these forecasts to make fine grained traffic signal control, while the
second LLM, LLM-Critic, refines LLM-Agent's outputs, correcting errors and
hallucinations. These refined outputs are used for score-based fine-tuning to
improve accuracy and robustness. Simulation experiments using CityFlow on real
world datasets covering 224 intersections in Jinan (12), Hangzhou (16), and New
York (196) demonstrate that HeraldLight outperforms state of the art baselines,
achieving a 20.03% reduction in average travel time across all scenarios and a
10.74% reduction in average queue length on the Jinan and Hangzhou scenarios.
The source code is available on GitHub:
https://github.com/BUPT-ANTlab/HeraldLight.

</details>


### [200] [Study on Supply Chain Finance Decision-Making Model and Enterprise Economic Performance Prediction Based on Deep Reinforcement Learning](https://arxiv.org/abs/2511.00166)
*Shiman Zhang,Jinghan Zhou,Zhoufan Yu,Ningai Leng*

Main category: cs.LG

TL;DR: 本文提出深度学习与智能粒子群优化集成决策模型，用于后端集中冗余供应链，通过多种方法优化，模拟显示有积极效果。


<details>
  <summary>Details</summary>
Motivation: 提高后端集中冗余供应链的决策和规划效率。

Method: 提出集成深度学习与智能粒子群优化的决策模型，构建分布式节点部署模型和最优规划路径，用卷积神经网络等深度学习提取特征、线性规划捕捉高阶统计特征，通过模糊关联规则调度和深度强化学习优化模型，神经网络拟合动态变化，采用“深度学习特征提取 - 智能粒子群优化”混合机制引导全局优化。

Result: 模拟显示减少了资源消耗、增强了空间规划，在动态环境中改善了实时决策调整、优化了配送路径和实现了鲁棒智能控制。

Conclusion: 所提出的模型在后端集中冗余供应链的决策和规划方面具有有效性和优势。

Abstract: To improve decision-making and planning efficiency in back-end centralized
redundant supply chains, this paper proposes a decision model integrating deep
learning with intelligent particle swarm optimization. A distributed node
deployment model and optimal planning path are constructed for the supply chain
network. Deep learning such as convolutional neural networks extracts features
from historical data, and linear programming captures high-order statistical
features. The model is optimized using fuzzy association rule scheduling and
deep reinforcement learning, while neural networks fit dynamic changes. A
hybrid mechanism of "deep learning feature extraction - intelligent particle
swarm optimization" guides global optimization and selects optimal decisions
for adaptive control. Simulations show reduced resource consumption, enhanced
spatial planning, and in dynamic environments improved real-time decision
adjustment, distribution path optimization, and robust intelligent control.

</details>


### [201] [Can SAEs reveal and mitigate racial biases of LLMs in healthcare?](https://arxiv.org/abs/2511.00177)
*Hiba Ahsan,Byron C. Wallace*

Main category: cs.LG

TL;DR: 研究评估稀疏自编码器（SAEs）能否揭示和控制大语言模型（LLMs）中种族与污名化概念的关联，发现SAEs可识别问题，但通过SAE控制偏差在实际任务中效果有限。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在医疗领域应用有风险，如可能加剧现有偏见，需找出模型是否依赖患者种族进行预测。

Method: 识别Gemma - 2模型中与黑人个体相关的SAE潜在因素，用该潜在因素引导模型生成关于黑人患者的输出，并评估通过潜在因素引导减轻偏差的程度。

Result: 该潜在因素在合理输入和有问题的词汇上都会激活，激活黑人潜在因素会增加患者出现不良情况的风险评估，通过潜在因素引导在简单场景有改善，但在更现实复杂的临床任务中效果不佳。

Conclusion: SAEs可作为临床应用中识别模型对人口统计学问题依赖的有用工具，但通过SAE引导减轻偏差在实际任务中作用有限。

Abstract: LLMs are increasingly being used in healthcare. This promises to free
physicians from drudgery, enabling better care to be delivered at scale. But
the use of LLMs in this space also brings risks; for example, such models may
worsen existing biases. How can we spot when LLMs are (spuriously) relying on
patient race to inform predictions? In this work we assess the degree to which
Sparse Autoencoders (SAEs) can reveal (and control) associations the model has
made between race and stigmatizing concepts. We first identify SAE latents in
Gemma-2 models which appear to correlate with Black individuals. We find that
this latent activates on reasonable input sequences (e.g., "African American")
but also problematic words like "incarceration". We then show that we can use
this latent to steer models to generate outputs about Black patients, and
further that this can induce problematic associations in model outputs as a
result. For example, activating the Black latent increases the risk assigned to
the probability that a patient will become "belligerent". We evaluate the
degree to which such steering via latents might be useful for mitigating bias.
We find that this offers improvements in simple settings, but is less
successful for more realistic and complex clinical tasks. Overall, our results
suggest that: SAEs may offer a useful tool in clinical applications of LLMs to
identify problematic reliance on demographics but mitigating bias via SAE
steering appears to be of marginal utility for realistic tasks.

</details>


### [202] [PDE-SHARP: PDE Solver Hybrids Through Analysis & Refinement Passes](https://arxiv.org/abs/2511.00183)
*Shaghayegh Fazliani,Madeleine Udell*

Main category: cs.LG

TL;DR: 提出PDE - SHARP框架减少PDE求解器计算成本，精度更高。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的PDE求解器方法在处理复杂PDE时计算成本高。

Method: PDE - SHARP框架包含分析、生成和合成三个阶段。

Result: 相比基线方法，平均求解器评估次数少于13次，精度平均提高4倍，在不同LLM架构中表现稳健。

Conclusion: PDE - SHARP能以更少计算评估实现更高求解器精度，降低计算成本。

Abstract: Current LLM-driven approaches using test-time computing to generate PDE
solvers execute a large number of solver samples to identify high-accuracy
solvers. These paradigms are especially costly for complex PDEs requiring
substantial computational resources for numerical evaluation. We introduce
PDE-SHARP, a framework to reduce computational costs by replacing expensive
scientific computation by cheaper LLM inference that achieves superior solver
accuracy with 60-75% fewer computational evaluations. PDE-SHARP employs three
stages: (1) Analysis: mathematical chain-of-thought analysis including PDE
classification, solution type detection, and stability analysis; (2) Genesis:
solver generation based on mathematical insights from the previous stage; and
(3) Synthesis: collaborative selection-hybridization tournaments in which LLM
judges iteratively refine implementations through flexible performance
feedback. To generate high-quality solvers, PDE-SHARP requires fewer than 13
solver evaluations on average compared to 30+ for baseline methods, improving
accuracy uniformly across tested PDEs by $4\times$ on average, and demonstrates
robust performance across LLM architectures, from general-purpose to
specialized reasoning models.

</details>


### [203] [EL-MIA: Quantifying Membership Inference Risks of Sensitive Entities in LLMs](https://arxiv.org/abs/2511.00192)
*Ali Satvaty,Suzan Verberne,Fatih Turkmen*

Main category: cs.LG

TL;DR: 本文提出实体级成员风险发现任务，构建基准数据集对比现有及新方法，分析结果指出现有方法在敏感属性实体级成员推理方面有限。


<details>
  <summary>Details</summary>
Motivation: 现有成员推理攻击（MIA）方法无法捕捉更细粒度风险，提出实体级敏感信息成员风险发现任务。

Method: 提出“EL - MIA”框架审计大语言模型实体级成员风险，构建基准数据集，对比现有及新提出的两种方法。

Result: 现有MIA方法在敏感属性实体级成员推理方面存在局限，可用相对直接的方法勾勒出易受攻击性。

Conclusion: 需要更强的对手来对威胁模型进行压力测试。

Abstract: Membership inference attacks (MIA) aim to infer whether a particular data
point is part of the training dataset of a model. In this paper, we propose a
new task in the context of LLM privacy: entity-level discovery of membership
risk focused on sensitive information (PII, credit card numbers, etc). Existing
methods for MIA can detect the presence of entire prompts or documents in the
LLM training data, but they fail to capture risks at a finer granularity. We
propose the ``EL-MIA'' framework for auditing entity-level membership risks in
LLMs. We construct a benchmark dataset for the evaluation of MIA methods on
this task. Using this benchmark, we conduct a systematic comparison of existing
MIA techniques as well as two newly proposed methods. We provide a
comprehensive analysis of the results, trying to explain the relation of the
entity level MIA susceptability with the model scale, training epochs, and
other surface level factors. Our findings reveal that existing MIA methods are
limited when it comes to entity-level membership inference of the sensitive
attributes, while this susceptibility can be outlined with relatively
straightforward methods, highlighting the need for stronger adversaries to
stress test the provided threat model.

</details>


### [204] [Diffusion Models at the Drug Discovery Frontier: A Review on Generating Small Molecules versus Therapeutic Peptides](https://arxiv.org/abs/2511.00209)
*Yiquan Wang,Yahui Ma,Yuhan Chang,Jiayao Yan,Jialin Zhang,Minnuo Cai,Kai Wei*

Main category: cs.LG

TL;DR: 本文系统比较扩散模型在小分子和治疗性肽设计中的应用，指出各自挑战和共同障碍，认为应弥合特定差距并整合到DBTL平台以释放其潜力。


<details>
  <summary>Details</summary>
Motivation: 加速和变革传统缓慢且昂贵的药物发现过程，系统比较扩散模型在两种主要治疗方式设计中的应用。

Method: 分析迭代去噪的统一框架如何适应不同治疗方式的分子表示、化学空间和设计目标。

Result: 小分子设计中模型擅长基于结构设计，但要确保化学可合成性；治疗性肽设计关注生成功能序列和从头设计结构，面临生物稳定性、折叠和免疫原性等挑战；二者有共同障碍。

Conclusion: 弥合特定差距并将扩散模型整合到DBTL平台，可从化学探索转向靶向创造新型疗法，释放其全部潜力。

Abstract: Diffusion models have emerged as a leading framework in generative modeling,
showing significant potential to accelerate and transform the traditionally
slow and costly process of drug discovery. This review provides a systematic
comparison of their application in designing two principal therapeutic
modalities: small molecules and therapeutic peptides. We analyze how a unified
framework of iterative denoising is adapted to the distinct molecular
representations, chemical spaces, and design objectives of each modality. For
small molecules, these models excel at structure-based design, generating
novel, pocket-fitting ligands with desired physicochemical properties, yet face
the critical hurdle of ensuring chemical synthesizability. Conversely, for
therapeutic peptides, the focus shifts to generating functional sequences and
designing de novo structures, where the primary challenges are achieving
biological stability against proteolysis, ensuring proper folding, and
minimizing immunogenicity. Despite these distinct challenges, both domains face
shared hurdles: the need for more accurate scoring functions, the scarcity of
high-quality experimental data, and the crucial requirement for experimental
validation. We conclude that the full potential of diffusion models will be
unlocked by bridging these modality-specific gaps and integrating them into
automated, closed-loop Design-Build-Test-Learn (DBTL) platforms, thereby
shifting the paradigm from chemical exploration to the targeted creation of
novel therapeutics.

</details>


### [205] [Iterative Foundation Model Fine-Tuning on Multiple Rewards](https://arxiv.org/abs/2511.00220)
*Pouya M. Ghari,Simone Sciabola,Ye Wang*

Main category: cs.LG

TL;DR: 本文提出基于多奖励信号的强化学习方法微调基础模型，经理论分析和多领域实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 在文本生成和药物发现等应用中，单一奖励信号优化基础模型欠佳，需多评估标准。

Method: 提出基于多奖励信号的强化学习方法微调基础模型，采用跨奖励的迭代微调策略。

Result: 在文本、生物序列和小分子生成等多领域实验表明，该算法比现有基线方法更有效。

Conclusion: 所提方法推广了现有基于强化学习的方法，对多奖励强化学习微调性能有理论见解且实验有效。

Abstract: Fine-tuning foundation models has emerged as a powerful approach for
generating objects with specific desired properties. Reinforcement learning
(RL) provides an effective framework for this purpose, enabling models to
generate outputs that maximize a given reward function. However, in many
applications such as text generation and drug discovery, it can be suboptimal
to optimize using a single reward signal, as multiple evaluation criteria are
often necessary. This paper proposes a novel reinforcement learning-based
method for fine-tuning foundation models using multiple reward signals. By
employing an iterative fine-tuning strategy across these rewards, our approach
generalizes state-of-the-art RL-based methods. We further provide a theoretical
analysis that offers insights into the performance of multi-reward RL
fine-tuning. Experimental results across diverse domains including text,
biological sequence, and small molecule generation, demonstrate the
effectiveness of the proposed algorithm compared to state-of-the-art baselines.

</details>


### [206] [Melanoma Classification Through Deep Ensemble Learning and Explainable AI](https://arxiv.org/abs/2511.00246)
*Wadduwage Shanika Perera,ABM Islam,Van Vung Pham,Min Kyung An*

Main category: cs.LG

TL;DR: 本文提出用集成学习与XAI技术解决黑色素瘤检测深度学习模型的可解释性问题。


<details>
  <summary>Details</summary>
Motivation: 黑色素瘤早期检测重要，深度学习模型虽能高精度检测但存在可解释性局限，缺乏可靠性和信任度，需要解决。

Method: 提出使用三个先进的深度迁移学习网络进行集成学习的机器学习模型，并利用XAI技术解释预测依据以确保预测可靠性。

Result: 未提及具体结果。

Conclusion: 未提及具体结论。

Abstract: Melanoma is one of the most aggressive and deadliest skin cancers, leading to
mortality if not detected and treated in the early stages. Artificial
intelligence techniques have recently been developed to help dermatologists in
the early detection of melanoma, and systems based on deep learning (DL) have
been able to detect these lesions with high accuracy. However, the entire
community must overcome the explainability limit to get the maximum benefit
from DL for diagnostics in the healthcare domain. Because of the black box
operation's shortcomings in DL models' decisions, there is a lack of
reliability and trust in the outcomes. However, Explainable Artificial
Intelligence (XAI) can solve this problem by interpreting the predictions of AI
systems. This paper proposes a machine learning model using ensemble learning
of three state-of-the-art deep transfer Learning networks, along with an
approach to ensure the reliability of the predictions by utilizing XAI
techniques to explain the basis of the predictions.

</details>


### [207] [X-TRACK: Physics-Aware xLSTM for Realistic Vehicle Trajectory Prediction](https://arxiv.org/abs/2511.00266)
*Aanchal Rajesh Chugh,Marion Neumeier,Sebastian Dorn*

Main category: cs.LG

TL;DR: 本文提出基于xLSTM的车辆轨迹预测框架X - TRAJ及其物理感知变体X - TRACK，在数据集上表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: xLSTM在车辆轨迹预测领域未被充分探索，且传统LSTM有局限性，因此希望利用xLSTM优势进行车辆轨迹预测。

Method: 提出基于xLSTM的车辆轨迹预测框架X - TRAJ和物理感知变体X - TRACK，将车辆运动学融入模型学习过程。

Result: 在highD和NGSIM数据集上的综合评估显示，X - TRACK优于现有基线。

Conclusion: 引入物理约束的基于xLSTM的模型能生成现实可行的轨迹，可用于车辆轨迹预测。

Abstract: Recent advancements in Recurrent Neural Network (RNN) architectures,
particularly the Extended Long Short Term Memory (xLSTM), have addressed the
limitations of traditional Long Short Term Memory (LSTM) networks by
introducing exponential gating and enhanced memory structures. These
improvements make xLSTM suitable for time-series prediction tasks as they
exhibit the ability to model long-term temporal dependencies better than LSTMs.
Despite their potential, these xLSTM-based models remain largely unexplored in
the context of vehicle trajectory prediction. Therefore, this paper introduces
a novel xLSTM-based vehicle trajectory prediction framework, X-TRAJ, and its
physics-aware variant, X-TRACK (eXtended LSTM for TRAjectory prediction
Constraint by Kinematics), which explicitly integrates vehicle motion
kinematics into the model learning process. By introducing physical
constraints, the proposed model generates realistic and feasible trajectories.
A comprehensive evaluation on the highD and NGSIM datasets demonstrates that
X-TRACK outperforms state-of-the-art baselines.

</details>


### [208] [Improving the Robustness of Control of Chaotic Convective Flows with Domain-Informed Reinforcement Learning](https://arxiv.org/abs/2511.00272)
*Michiel Straat,Thorben Markmann,Sebastian Peitz,Barbara Hammer*

Main category: cs.LG

TL;DR: 本文聚焦瑞利 - 贝纳德对流，引入领域知识的强化学习智能体控制混沌对流，在层流和混沌流中均有良好效果，证明领域先验可增强控制鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 稳定混沌对流具有挑战性，传统控制方法在混沌状态常失效，强化学习在混沌和湍流动力学中的泛化和鲁棒性待探索。

Method: 引入领域知识的强化学习智能体，使用近端策略优化算法在不同初始条件和流态下训练，在奖励函数中融入领域知识。

Result: 在层流中减少对流热传输达33%，在混沌流中减少10%，优于传统控制器；领域知识奖励设计使流动更稳定、训练收敛更快且无需重新训练就能跨流态泛化。

Conclusion: 优雅的领域知识先验可极大增强基于强化学习的混沌流控制的鲁棒性，推动实际应用。

Abstract: Chaotic convective flows arise in many real-world systems, such as
microfluidic devices and chemical reactors. Stabilizing these flows is highly
desirable but remains challenging, particularly in chaotic regimes where
conventional control methods often fail. Reinforcement Learning (RL) has shown
promise for control in laminar flow settings, but its ability to generalize and
remain robust under chaotic and turbulent dynamics is not well explored,
despite being critical for real-world deployment. In this work, we improve the
practical feasibility of RL-based control of such flows focusing on
Rayleigh-B\'enard Convection (RBC), a canonical model for convective heat
transport. To enhance generalization and sample efficiency, we introduce
domain-informed RL agents that are trained using Proximal Policy Optimization
across diverse initial conditions and flow regimes. We incorporate domain
knowledge in the reward function via a term that encourages B\'enard cell
merging, as an example of a desirable macroscopic property. In laminar flow
regimes, the domain-informed RL agents reduce convective heat transport by up
to 33%, and in chaotic flow regimes, they still achieve a 10% reduction, which
is significantly better than the conventional controllers used in practice. We
compare the domain-informed to uninformed agents: Our results show that the
domain-informed reward design results in steady flows, faster convergence
during training, and generalization across flow regimes without retraining. Our
work demonstrates that elegant domain-informed priors can greatly enhance the
robustness of RL-based control of chaotic flows, bringing real-world deployment
closer.

</details>


### [209] [Calibration Across Layers: Understanding Calibration Evolution in LLMs](https://arxiv.org/abs/2511.00280)
*Abhinav Joshi,Areeb Ahmad,Ashutosh Modi*

Main category: cs.LG

TL;DR: 本文研究大语言模型校准能力在网络深度中的演变，发现上层有信心校正阶段及低维校准方向，表明校准是分布式现象。


<details>
  <summary>Details</summary>
Motivation: 此前研究关注最终层特定组件与校准能力的关系，本文从网络深度角度提供补充视角研究校准能力演变。

Method: 在MMLU基准上分析多个开放权重模型。

Result: 发现上层有信心校正阶段，在决策确定性达成后重新校准模型信心；识别出残差流中的低维校准方向，扰动该方向可改善校准指标且不损害准确率。

Conclusion: 校准是分布式现象，在网络前向传播中形成，不仅在最终投影层，为大语言模型中信心调节机制提供新见解。

Abstract: Large Language Models (LLMs) have demonstrated inherent calibration
capabilities, where predicted probabilities align well with correctness,
despite prior findings that deep neural networks are often overconfident.
Recent studies have linked this behavior to specific components in the final
layer, such as entropy neurons and the unembedding matrix null space. In this
work, we provide a complementary perspective by investigating how calibration
evolves throughout the network depth. Analyzing multiple open-weight models on
the MMLU benchmark, we uncover a distinct confidence correction phase in the
upper/later layers, where model confidence is actively recalibrated after
decision certainty has been reached. Furthermore, we identify a low-dimensional
calibration direction in the residual stream whose perturbation significantly
improves calibration metrics (ECE and MCE) without harming accuracy. Our
findings suggest that calibration is a distributed phenomenon, shaped
throughout the network forward pass, not just in its final projection,
providing new insights into how confidence-regulating mechanisms operate within
LLMs.

</details>


### [210] [A systematic evaluation of uncertainty quantification techniques in deep learning: a case study in photoplethysmography signal analysis](https://arxiv.org/abs/2511.00301)
*Ciaran Bench,Oskar Pfeffer,Vivek Desai,Mohammad Moulaeifard,Loïc Coquelin,Peter H. Charlton,Nils Strodthoff,Nando Hegemann,Philip J. Aston,Andrew Thompson*

Main category: cs.LG

TL;DR: 本文在两个临床相关预测任务模型上实现八种不确定性量化（UQ）技术，进行全面评估和比较，发现评估局部校准和适应性能提供模型行为的实用见解，强调评估UQ技术应贴合实际用例。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在实际测量场景中性能不佳，可靠的预测不确定性可为临床医生提供指导，因此有必要比较不同UQ方法的有效性。

Method: 在两个临床相关预测任务（房颤检测和血压回归）的模型上实现八种UQ技术，并制定综合评估程序进行比较。

Result: 不同技术的不确定性可靠性情况复杂，最优选择取决于不确定性表达方式、评估指标和可靠性尺度；评估局部校准和适应性能提供用常规全局可靠性指标无法获得的模型行为见解。

Conclusion: 评估UQ技术的标准应适应模型的实际用例，在选择的不确定性表达方式上实现小规模可靠性，同时尽可能保留预测性能。

Abstract: In principle, deep learning models trained on medical time-series, including
wearable photoplethysmography (PPG) sensor data, can provide a means to
continuously monitor physiological parameters outside of clinical settings.
However, there is considerable risk of poor performance when deployed in
practical measurement scenarios leading to negative patient outcomes. Reliable
uncertainties accompanying predictions can provide guidance to clinicians in
their interpretation of the trustworthiness of model outputs. It is therefore
of interest to compare the effectiveness of different approaches. Here we
implement an unprecedented set of eight uncertainty quantification (UQ)
techniques to models trained on two clinically relevant prediction tasks:
Atrial Fibrillation (AF) detection (classification), and two variants of blood
pressure regression. We formulate a comprehensive evaluation procedure to
enable a rigorous comparison of these approaches. We observe a complex picture
of uncertainty reliability across the different techniques, where the most
optimal for a given task depends on the chosen expression of uncertainty,
evaluation metric, and scale of reliability assessed. We find that assessing
local calibration and adaptivity provides practically relevant insights about
model behaviour that otherwise cannot be acquired using more commonly
implemented global reliability metrics. We emphasise that criteria for
evaluating UQ techniques should cater to the model's practical use case, where
the use of a small number of measurements per patient places a premium on
achieving small-scale reliability for the chosen expression of uncertainty,
while preserving as much predictive performance as possible.

</details>


### [211] [Reject Only Critical Tokens: Pivot-Aware Speculative Decoding](https://arxiv.org/abs/2511.00351)
*Amir Ziashahabi,Yavuz Faruk Bakman,Duygu Nur Yaldiz,Mostafa El-Khamy,Sai Praneeth Karimireddy,Salman Avestimehr*

Main category: cs.LG

TL;DR: 指出投机解码分布匹配要求过严，提出以匹配目标模型预期效用为解码目标，提出Pivot - Aware Speculative Decoding策略，在多数据集上评估实现2.5倍加速且效用相当。


<details>
  <summary>Details</summary>
Motivation: 现有投机解码的分布匹配要求过严，导致接受率低，限制加速潜力，且与大语言模型现实应用中对效用的关注不匹配。

Method: 提出Pivot - Aware Speculative Decoding策略，拒绝导致最终输出效用下降的关键标记（pivot tokens），提出标记方法并训练轻量级分类器检测。

Result: 在多个数据集上评估，可实现高达2.5倍的加速，且效用相当。

Conclusion: 提出的方法作为标准投机解码的宽松版本，能在保持效用的同时提供更高接受率，实现加速。

Abstract: Speculative Decoding (SD) ensures that the output matches the target model's
distribution exactly. However, we argue that this distribution matching
requirement is too stringent and results in unnecessarily low acceptance rates,
limiting potential speedups. Instead, we advocate a reformulation of the
decoding objective: the proposed decoding strategy should match the expected
utility, i.e., the task-specific performance, of the target model. This
perspective also aligns better with real-world use cases of LLMs, where utility
(e.g., code correctness, factual accuracy) is often more important than
sampling distribution. Based on this reformulation, we propose a novel decoding
strategy: Pivot-Aware Speculative Decoding, which rejects only those tokens
that would lead to a utility drop in the final output. We refer to these
critical tokens as pivot tokens. We propose a method for labeling tokens as
pivotal or non-pivotal and train a lightweight classifier to detect them. This
method can be viewed as a relaxed version of standard SD, which offers much
higher acceptance while preserving utility. We evaluate our method across
various datasets, demonstrating that we can achieve up to $2.5\times$ speedup
with comparable utility. Source code is available at
https://github.com/amir-zsh/PAD.

</details>


### [212] [UME-R1: Exploring Reasoning-Driven Generative Multimodal Embeddings](https://arxiv.org/abs/2511.00405)
*Zhibin Lan,Liqiang Niu,Fandong Meng,Jie Zhou,Jinsong Su*

Main category: cs.LG

TL;DR: 本文探索生成式嵌入，提出UME - R1框架，在多模态嵌入任务上取得显著成果并揭示关键见解。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型嵌入为判别式，限制了其从推理驱动生成范式中获益的能力，因此探索生成式嵌入。

Method: 提出UME - R1通用多模态嵌入框架，采用两阶段训练策略，先冷启动监督微调，后进行强化学习。

Result: 在MMEB - V2基准的78个任务上，UME - R1显著优于传统判别式嵌入模型。

Conclusion: 生成式嵌入可利用MLLMs的生成推理能力获得性能提升，判别式和生成式嵌入互补，RL可优化生成式嵌入，推理时重复采样能提高下游任务覆盖率。

Abstract: The remarkable success of multimodal large language models (MLLMs) has driven
advances in multimodal embeddings, yet existing models remain inherently
discriminative, limiting their ability to benefit from reasoning-driven
generation paradigm. In this work, we pioneer the exploration of generative
embeddings, unifying embedding tasks within a generative paradigm. We propose
UME-R1, a universal multimodal embedding framework consisting of a two-stage
training strategy: a cold-start supervised fine-tuning equips the model with
reasoning capabilities and enables it to generate both discriminative and
generative embeddings; a subsequent reinforcement learning enhances reasoning
and further optimizes generative embedding quality. This pioneering work
reveals four key insights: 1) generative embeddings unlock substantial
performance gains over conventional discriminative embeddings by leveraging the
powerful generative reasoning capabilities of MLLMs; 2) discriminative and
generative embeddings are complementary, whose combined oracle performance far
exceeding that of either alone; 3) RL can effectively enhance generative
embeddings, establishing a scalable optimization paradigm.; 4) repeated
sampling at inference boosts downstream task coverage (pass@k), highlighting
the inference-time scalability potential of generative embeddings. Evaluated on
the MMEB-V2 benchmark across 78 tasks spanning video, image, and visual
documents, UME-R1 significantly outperforms conventional discriminative
embedding models and offers a foundation for more interpretable,
reasoning-driven generative multimodal embeddings. Our code, models, and
datasets will be publicly available at https://github.com/XMUDeepLIT/UME-R1.

</details>


### [213] [Enhancing Adversarial Transferability by Balancing Exploration and Exploitation with Gradient-Guided Sampling](https://arxiv.org/abs/2511.00411)
*Zenghao Niu,Weicheng Xie,Siyang Song,Zitong Yu,Feng Liu,Linlin Shen*

Main category: cs.LG

TL;DR: 提出Gradient - Guided Sampling (GGS)方法解决对抗攻击可迁移性中开发与探索的困境，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 对抗攻击在跨模型架构的迁移场景中面临开发（最大化攻击效力）和探索（增强跨模型泛化）的两难困境，传统方法存在过度侧重一方的问题。

Method: 基于MI - FGSM，引入内迭代随机采样，并利用前一次内迭代的梯度引导采样方向，采样幅度由随机分布决定。

Result: 在多个DNN架构和多模态大语言模型上的综合实验表明，该方法优于现有最先进的迁移攻击方法。

Conclusion: GGS方法能有效解决对抗攻击可迁移性中的两难困境，具有良好的性能。

Abstract: Adversarial attacks present a critical challenge to deep neural networks'
robustness, particularly in transfer scenarios across different model
architectures. However, the transferability of adversarial attacks faces a
fundamental dilemma between Exploitation (maximizing attack potency) and
Exploration (enhancing cross-model generalization). Traditional momentum-based
methods over-prioritize Exploitation, i.e., higher loss maxima for attack
potency but weakened generalization (narrow loss surface). Conversely, recent
methods with inner-iteration sampling over-prioritize Exploration, i.e.,
flatter loss surfaces for cross-model generalization but weakened attack
potency (suboptimal local maxima). To resolve this dilemma, we propose a simple
yet effective Gradient-Guided Sampling (GGS), which harmonizes both objectives
through guiding sampling along the gradient ascent direction to improve both
sampling efficiency and stability. Specifically, based on MI-FGSM, GGS
introduces inner-iteration random sampling and guides the sampling direction
using the gradient from the previous inner-iteration (the sampling's magnitude
is determined by a random distribution). This mechanism encourages adversarial
examples to reside in balanced regions with both flatness for cross-model
generalization and higher local maxima for strong attack potency. Comprehensive
experiments across multiple DNN architectures and multimodal large language
models (MLLMs) demonstrate the superiority of our method over state-of-the-art
transfer attacks. Code is made available at https://github.com/anuin-cat/GGS.

</details>


### [214] [Tree Training: Accelerating Agentic LLMs Training via Shared Prefix Reuse](https://arxiv.org/abs/2511.00413)
*Shaojie Wang,Jinghui Wang,Yinghan Cui,Xuxing Chen,Chao Wang,Liang Huang,Xiaojiang Zhang,Junyi Peng,Li Wan,Haotian Zhang,Bin Chen*

Main category: cs.LG

TL;DR: 当前训练管道处理树状轨迹效率低，提出Tree Training范式，通过Tree Packing和Gradient Restoration提高计算效率，实验显示最多可减少3.9倍训练时间。


<details>
  <summary>Details</summary>
Motivation: 当前训练管道将树状轨迹分解为线性段，导致共享前缀重复计算，效率低下。

Method: 提出Tree Training范式，包括Tree Packing以高效复用轨迹间的共享计算，以及Gradient Restoration确保复用前缀上的梯度正确传播。

Result: 在多个开源模型上的实验表明，总训练时间最多可减少3.9倍。

Conclusion: Tree Training范式能有效提高大规模智能体训练的计算效率，使智能体大语言模型的监督微调（SFT）和强化学习（RL）训练更高效。

Abstract: In agentic LLM scenarios, an agent's interaction process during a single
rollout often exhibits branching behaviors. Due to memory retrieval and
concurrent tool executions at certain decision points, the token trajectory of
one task evolves into a tree-like structure rather than a linear sequence.
However, current training pipelines decompose such tree-structured trajectories
into separate linear segments, treating each branch as an independent sequence.
As a result, shared prefixes across these branches are repeatedly recomputed
during both forward and backward passes. To address this inefficiency, we
propose Tree Training, a paradigm that computes each shared prefix only once
and reuses its intermediate results across related branches during both forward
and backward passes, substantially improving computation efficiency in
large-scale agentic training. This is achieved via (i) Tree Packing, which
efficiently reuses shared computations across trajectories, and (ii) Gradient
Restoration, which ensures correct gradient propagation across reused prefixes.
Experiments on multiple open-source models demonstrate up to 3.9x reduction in
total training time, enabling more efficient agentic LLM SFT and RL training.

</details>


### [215] [Structure-Preserving Physics-Informed Neural Network for the Korteweg--de Vries (KdV) Equation](https://arxiv.org/abs/2511.00418)
*Victory Obieke,Emmanuel Oguadimma*

Main category: cs.LG

TL;DR: 本文提出结构保留PINN框架解决KdV方程，嵌入守恒量到损失函数，用正弦激活函数，经案例验证效果好。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs在长期积分时无法保留关键物理不变量，需解决KdV方程求解中物理一致性和能量稳定性问题。

Method: 将质量和哈密顿能量守恒嵌入损失函数，采用正弦激活函数。

Result: 模型成功复现KdV动力学特征行为，保持守恒不变量，消融研究表明结合优化和特征映射可加速收敛、提高稳定性和减少漂移。

Conclusion: 计算高效、考虑不变量的正则化与正弦表示结合，能为哈密顿偏微分方程生成鲁棒、能量一致的PINNs。

Abstract: Physics-Informed Neural Networks (PINNs) offer a flexible framework for
solving nonlinear partial differential equations (PDEs), yet conventional
implementations often fail to preserve key physical invariants during long-term
integration. This paper introduces a \emph{structure-preserving PINN} framework
for the nonlinear Korteweg--de Vries (KdV) equation, a prototypical model for
nonlinear and dispersive wave propagation. The proposed method embeds the
conservation of mass and Hamiltonian energy directly into the loss function,
ensuring physically consistent and energy-stable evolution throughout training
and prediction. Unlike standard \texttt{tanh}-based
PINNs~\cite{raissi2019pinn,wang2022modifiedpinn}, our approach employs
sinusoidal activation functions that enhance spectral expressiveness and
accurately capture the oscillatory and dispersive nature of KdV solitons.
Through representative case studies -- including single-soliton propagation
(shape-preserving translation), two-soliton interaction (elastic collision with
phase shift), and cosine-pulse initialization (nonlinear dispersive breakup) --
the model successfully reproduces hallmark behaviors of KdV dynamics while
maintaining conserved invariants. Ablation studies demonstrate that combining
invariant-constrained optimization with sinusoidal feature mappings accelerates
convergence, improves long-term stability, and mitigates drift without
multi-stage pretraining. These results highlight that computationally
efficient, invariant-aware regularization coupled with sinusoidal
representations yields robust, energy-consistent PINNs for Hamiltonian partial
differential equations such as the KdV equation.

</details>


### [216] [Bootstrap Off-policy with World Model](https://arxiv.org/abs/2511.00423)
*Guojian Zhan,Likun Wang,Xiangteng Zhang,Jiaxin Gao,Masayoshi Tomizuka,Shengbo Eben Li*

Main category: cs.LG

TL;DR: 提出BOOM框架结合规划与离策略学习，在实验中取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 在线规划在强化学习中使用时，收集数据与策略实际行为有差异，影响模型学习和策略改进。

Method: 提出BOOM框架，通过引导循环紧密结合规划与离策略学习，使用联合学习的世界模型，核心是无似然对齐损失和软值加权机制。

Result: 在高维DeepMind Control Suite和Humanoid - Bench实验中，BOOM在训练稳定性和最终性能上达到SOTA。

Conclusion: BOOM框架有效，代码可在https://github.com/molumitu/BOOM_MBRL获取。

Abstract: Online planning has proven effective in reinforcement learning (RL) for
improving sample efficiency and final performance. However, using planning for
environment interaction inevitably introduces a divergence between the
collected data and the policy's actual behaviors, degrading both model learning
and policy improvement. To address this, we propose BOOM (Bootstrap Off-policy
with WOrld Model), a framework that tightly integrates planning and off-policy
learning through a bootstrap loop: the policy initializes the planner, and the
planner refines actions to bootstrap the policy through behavior alignment.
This loop is supported by a jointly learned world model, which enables the
planner to simulate future trajectories and provides value targets to
facilitate policy improvement. The core of BOOM is a likelihood-free alignment
loss that bootstraps the policy using the planner's non-parametric action
distribution, combined with a soft value-weighted mechanism that prioritizes
high-return behaviors and mitigates variability in the planner's action quality
within the replay buffer. Experiments on the high-dimensional DeepMind Control
Suite and Humanoid-Bench show that BOOM achieves state-of-the-art results in
both training stability and final performance. The code is accessible at
https://github.com/molumitu/BOOM_MBRL.

</details>


### [217] [Region-Aware Reconstruction Strategy for Pre-training fMRI Foundation Model](https://arxiv.org/abs/2511.00443)
*Ruthwik Reddy Doodipala,Pankaj Pandey,Carolina Torres Rojas,Manob Jyoti Saikia,Ranganatha Sitaram*

Main category: cs.LG

TL;DR: 研究提出静息态fMRI基础模型的区域感知重建策略，比随机掩码提升分类准确率，还增强模型可解释性，未来将扩展数据集并开发新损失函数。


<details>
  <summary>Details</summary>
Motivation: 大规模异构脑成像数据集推动神经影像基础模型发展，基于重建的自监督学习在下游fMRI任务中表现好，探索更好的重建策略。

Method: 引入基于AAL3图谱的ROI引导掩码策略，直接应用于全4D fMRI体积，在自监督预训练中选择性掩码语义连贯脑区。

Result: 在ADHD - 200数据集上，相比传统随机掩码，分类准确率提升4.23%；边缘系统和小脑对重建保真度和模型表示贡献最大。

Conclusion: 模型预训练时掩码解剖区域可增强可解释性，产生更鲁棒和有区分性的表示。未来将扩展数据集和开发新损失函数提升模型性能。

Abstract: The emergence of foundation models in neuroimaging is driven by the
increasing availability of large-scale and heterogeneous brain imaging
datasets. Recent advances in self-supervised learning, particularly
reconstruction-based objectives, have demonstrated strong potential for
pretraining models that generalize effectively across diverse downstream
functional MRI (fMRI) tasks. In this study, we explore region-aware
reconstruction strategies for a foundation model in resting-state fMRI, moving
beyond approaches that rely on random region masking. Specifically, we
introduce an ROI-guided masking strategy using the Automated Anatomical
Labelling Atlas (AAL3), applied directly to full 4D fMRI volumes to selectively
mask semantically coherent brain regions during self-supervised pretraining.
Using the ADHD-200 dataset comprising 973 subjects with resting-state fMRI
scans, we show that our method achieves a 4.23% improvement in classification
accuracy for distinguishing healthy controls from individuals diagnosed with
ADHD, compared to conventional random masking. Region-level attribution
analysis reveals that brain volumes within the limbic region and cerebellum
contribute most significantly to reconstruction fidelity and model
representation. Our results demonstrate that masking anatomical regions during
model pretraining not only enhances interpretability but also yields more
robust and discriminative representations. In future work, we plan to extend
this approach by evaluating it on additional neuroimaging datasets, and
developing new loss functions explicitly derived from region-aware
reconstruction objectives. These directions aim to further improve the
robustness and interpretability of foundation models for functional
neuroimaging.

</details>


### [218] [Deep Learning Approach to Anomaly Detection in Enterprise ETL Processes with Autoencoders](https://arxiv.org/abs/2511.00462)
*Xin Chen,Saili Uday Gadgil,Kangning Gao,Yi Hu,Cong Nie*

Main category: cs.LG

TL;DR: 提出基于深度自编码器的企业级ETL数据流异常检测方法，经测试表现良好，可助力企业数据处理。


<details>
  <summary>Details</summary>
Motivation: 解决企业级ETL数据流中常出现的异常问题。

Method: 分析ETL流程多种异常，进行数据标准化与特征建模；采用编解码器结构压缩并重构输入，用重构误差衡量异常程度；在潜在空间引入正则化约束。

Result: 在不同超参数设置、环境变化和数据特征下，该方法在AUC、ACC、Precision和Recall指标上表现优越。

Conclusion: 基于深度自编码器的检测机制能有效捕捉数据潜在分布模式，准确识别多样异常，为企业数据处理和智能分析提供可靠支持。

Abstract: An anomaly detection method based on deep autoencoders is proposed to address
anomalies that often occur in enterprise-level ETL data streams. The study
first analyzes multiple types of anomalies in ETL processes, including delays,
missing values, duplicate loading, and sudden abnormal changes, and applies
data standardization and feature modeling to ensure stable and usable inputs.
In the method design, the encoder-decoder structure compresses high-dimensional
inputs into latent representations and reconstructs them, while reconstruction
error is used to measure anomaly levels. Regularization constraints are
introduced in the latent space to enhance feature sparsity and distribution
learning, thereby improving robustness in complex data streams. Systematic
analyses under different hyperparameter settings, environmental changes, and
data characteristics show that the proposed method achieves superior
performance in AUC, ACC, Precision, and Recall. The results demonstrate that
the deep autoencoder-based detection mechanism can effectively capture latent
distribution patterns in enterprise-level ETL data streams and accurately
identify diverse anomalies, providing reliable support for enterprise data
processing and intelligent analysis.

</details>


### [219] [Variational Autoencoder for Calibration: A New Approach](https://arxiv.org/abs/2511.00475)
*Travis Barrett,Amit Kumar Mishra,Joyce Mwangama*

Main category: cs.LG

TL;DR: 本文提出用于传感器校准的变分自编码器新实现，用现有多传感器气体数据集验证，展示其性能并探讨未来测试和扩展方法。


<details>
  <summary>Details</summary>
Motivation: 寻找新的传感器校准方法。

Method: 提出将变分自编码器的潜在空间训练为校准输出以校准传感器数据，并使用现有多传感器气体数据集进行概念验证。

Result: 所提出的校准变分自编码器既能作为校准模型，又能作为自编码器，其校准输出和重建输出与真实数据在统计上相似。

Conclusion: 该方法可行，后续将进行更多测试和扩展。

Abstract: In this paper we present a new implementation of a Variational Autoencoder
(VAE) for the calibration of sensors. We propose that the VAE can be used to
calibrate sensor data by training the latent space as a calibration output. We
discuss this new approach and show a proof-of-concept using an existing
multi-sensor gas dataset. We show the performance of the proposed calibration
VAE and found that it was capable of performing as calibration model while
performing as an autoencoder simultaneously. Additionally, these models have
shown that they are capable of creating statistically similar outputs from both
the calibration output as well as the reconstruction output to their respective
truth data. We then discuss the methods of future testing and planned expansion
of this work.

</details>


### [220] [Reasoning Planning for Language Models](https://arxiv.org/abs/2511.00521)
*Bao Nguyen,Hieu Trung Nguyen,Ruifeng She,Xiaojin Fu,Viet Anh Nguyen*

Main category: cs.LG

TL;DR: 本文重新审视语言模型推理方法选择假设，提出EPIC框架，实验证明其能选最优推理方法，提升准确率并降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 解决给定查询选择合适推理方法的挑战，重新审视已有方法假设。

Method: 进行理论分析得出标准聚合方法的准确率界限，引入EPIC框架学习共享表示空间，将概率界限作为正则化项用于优化。

Result: 在不同数学推理任务实验中，EPIC能持续选择最优推理方法，提升准确率并降低计算开销。

Conclusion: EPIC是有效的选择推理方法的框架，可在保证准确率同时降低成本。

Abstract: Selecting an appropriate reasoning method for a given query remains a key
challenge in language model generation. Existing approaches typically generate
multiple candidate responses and use an aggregation strategy to select the
output answer, often assuming that more candidate answers yield higher
accuracy. We revisit this assumption through a rigorous theoretical analysis,
deriving accuracy bounds for standard aggregation methods under fixed
generation distributions and candidate sizes. Building on these insights, we
introduce EPIC, an Ensemble Planning with Contrastive learning framework to
learn a shared representation space that captures both model reasoning
abilities and query-method compatibility. EPIC incorporates our probability
bounds as a regularizer in a utility-driven optimization that balances accuracy
and computational cost. Experiments on diverse mathematical reasoning tasks
show that EPIC consistently selects optimal reasoning methods, improving
accuracy while reducing computational overhead. Our code can be found at
https://github.com/nguyenngocbaocmt02/EPIC.

</details>


### [221] [Air Pollution Forecasting in Bucharest](https://arxiv.org/abs/2511.00532)
*Dragoş-Andrei Şerban,Răzvan-Alexandru Smădu,Dumitru-Clementin Cercel*

Main category: cs.LG

TL;DR: 论文关注空气污染中PM2.5预测，要设计、调试、测试和评估机器学习模型预测不同时间跨度的PM2.5水平并比较多模型性能。


<details>
  <summary>Details</summary>
Motivation: 空气污染尤其是PM2.5引发健康问题，预测PM2.5水平可提供早期预警和预防疾病。

Method: 设计、调试、测试和评估多种机器学习模型，包括线性回归算法、基于集成的方法、深度学习模型（如先进循环神经网络、变压器）以及大语言模型。

Result: 未提及

Conclusion: 未提及

Abstract: Air pollution, especially the particulate matter 2.5 (PM2.5), has become a
growing concern in recent years, primarily in urban areas. Being exposed to air
pollution is linked to developing numerous health problems, like the
aggravation of respiratory diseases, cardiovascular disorders, lung function
impairment, and even cancer or early death. Forecasting future levels of PM2.5
has become increasingly important over the past few years, as it can provide
early warnings and help prevent diseases. This paper aims to design, fine-tune,
test, and evaluate machine learning models for predicting future levels of
PM2.5 over various time horizons. Our primary objective is to assess and
compare the performance of multiple models, ranging from linear regression
algorithms and ensemble-based methods to deep learning models, such as advanced
recurrent neural networks and transformers, as well as large language models,
on this forecasting task.

</details>


### [222] [Robust Single-Agent Reinforcement Learning for Regional Traffic Signal Control Under Demand Fluctuations](https://arxiv.org/abs/2511.00549)
*Qiang Li,Jin Niu,Lina Yu*

Main category: cs.LG

TL;DR: 本文提出区域自适应交通信号控制的单智能体强化学习框架，利用邻接矩阵统一编码，借助DreamerV3模型学习策略，仿真实验证明其有效，未来将提升实用性。


<details>
  <summary>Details</summary>
Motivation: 交通拥堵影响大，传统交通信号控制优化模型难以应对现实交通复杂性和动态性。

Method: 引入单智能体强化学习框架，用邻接矩阵统一编码，利用DreamerV3模型学习控制策略，奖励设计注重队列消散。

Result: 在SUMO仿真实验中，模型在不同OD需求波动推理场景下有强抗波动能力，显著降低队列长度。

Conclusion: 该工作为智能交通控制建立新范式，未来将增强实用性，考虑随机OD需求波动并探索应急区域优化机制。

Abstract: Traffic congestion, primarily driven by intersection queuing, significantly
impacts urban living standards, safety, environmental quality, and economic
efficiency. While Traffic Signal Control (TSC) systems hold potential for
congestion mitigation, traditional optimization models often fail to capture
real-world traffic complexity and dynamics. This study introduces a novel
single-agent reinforcement learning (RL) framework for regional adaptive TSC,
circumventing the coordination complexities inherent in multi-agent systems
through a centralized decision-making paradigm. The model employs an adjacency
matrix to unify the encoding of road network topology, real-time queue states
derived from probe vehicle data, and current signal timing parameters.
Leveraging the efficient learning capabilities of the DreamerV3 world model,
the agent learns control policies where actions sequentially select
intersections and adjust their signal phase splits to regulate traffic
inflow/outflow, analogous to a feedback control system. Reward design
prioritizes queue dissipation, directly linking congestion metrics (queue
length) to control actions. Simulation experiments conducted in SUMO
demonstrate the model's effectiveness: under inference scenarios with
multi-level (10%, 20%, 30%) Origin-Destination (OD) demand fluctuations, the
framework exhibits robust anti-fluctuation capability and significantly reduces
queue lengths. This work establishes a new paradigm for intelligent traffic
control compatible with probe vehicle technology. Future research will focus on
enhancing practical applicability by incorporating stochastic OD demand
fluctuations during training and exploring regional optimization mechanisms for
contingency events.

</details>


### [223] [Red-teaming Activation Probes using Prompted LLMs](https://arxiv.org/abs/2511.00554)
*Phil Blandfort,Robert Graham*

Main category: cs.LG

TL;DR: 研究激活探针在现实黑盒对抗压力下的故障模式，提出轻量级黑盒红队程序，发现探针脆弱模式和持续漏洞，表明该程序可在部署前预测故障模式。


<details>
  <summary>Details</summary>
Motivation: 激活探针作为AI系统监控器成本低、延迟小，但现实世界中的鲁棒性研究不足，需探索其在现实黑盒对抗压力下的故障模式及低成本发现方法。

Method: 提出轻量级黑盒红队程序，用迭代反馈和上下文学习（ICL）包装现成大语言模型，无需微调、梯度计算或架构访问。

Result: 通过高风险交互探针案例研究，发现SOTA探针的有价值见解，揭示可解释的脆弱模式和场景约束攻击下仍存在的漏洞。

Conclusion: 简单的提示式红队框架可在部署前预测故障模式，为强化未来探针提供有前景的可操作见解。

Abstract: Activation probes are attractive monitors for AI systems due to low cost and
latency, but their real-world robustness remains underexplored. We ask: What
failure modes arise under realistic, black-box adversarial pressure, and how
can we surface them with minimal effort? We present a lightweight black-box
red-teaming procedure that wraps an off-the-shelf LLM with iterative feedback
and in-context learning (ICL), and requires no fine-tuning, gradients, or
architectural access. Running a case study with probes for high-stakes
interactions, we show that our approach can help discover valuable insights
about a SOTA probe. Our analysis uncovers interpretable brittleness patterns
(e.g., legalese-induced FPs; bland procedural tone FNs) and reduced but
persistent vulnerabilities under scenario-constraint attacks. These results
suggest that simple prompted red-teaming scaffolding can anticipate failure
patterns before deployment and might yield promising, actionable insights to
harden future probes.

</details>


### [224] [FTT-GRU: A Hybrid Fast Temporal Transformer with GRU for Remaining Useful Life Prediction](https://arxiv.org/abs/2511.00564)
*Varun Teja Chirukiri,Udaya Bhasker Cheerala,Sandeep Kanta,Abdul Karim,Praveen Damacharla*

Main category: cs.LG

TL;DR: 提出FTT - GRU混合模型用于工业机械剩余使用寿命（RUL）预测，在NASA CMAPSS数据集上表现良好，适合实时工业预测。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以对多元传感器数据中的全局时间依赖和细粒度退化趋势进行建模，需要更有效的RUL预测方法。

Method: 提出结合快速时间变换器（FTT）和门控循环单元（GRU）的混合模型FTT - GRU进行顺序建模。

Result: 在CMAPSS FD001上，FTT - GRU达到RMSE 30.76、MAE 18.97、$R^2 = 0.45$，CPU延迟1.12 ms；较最佳已发表基线模型有改进；训练曲线收敛平稳；消融实验支持两组件作用。

Conclusion: 紧凑的Transformer - RNN混合模型能在CMAPSS上实现准确高效的RUL预测，适用于实时工业预测。

Abstract: Accurate prediction of the remaining useful life (RUL) of industrial
machinery is essential for reducing downtime and optimizing maintenance
schedules. Existing approaches, such as long short-term memory (LSTM) networks
and convolutional neural networks (CNNs), often struggle to model both global
temporal dependencies and fine-grained degradation trends in multivariate
sensor data. We propose a hybrid model, FTT-GRU, which combines a Fast Temporal
Transformer (FTT) -- a lightweight Transformer variant using linearized
attention via fast Fourier transform (FFT) -- with a gated recurrent unit (GRU)
layer for sequential modeling. To the best of our knowledge, this is the first
application of an FTT with a GRU for RUL prediction on NASA CMAPSS, enabling
simultaneous capture of global and local degradation patterns in a compact
architecture. On CMAPSS FD001, FTT-GRU attains RMSE 30.76, MAE 18.97, and
$R^2=0.45$, with 1.12 ms CPU latency at batch=1. Relative to the best published
deep baseline (TCN--Attention), it improves RMSE by 1.16\% and MAE by 4.00\%.
Training curves averaged over $k=3$ runs show smooth convergence with narrow
95\% confidence bands, and ablations (GRU-only, FTT-only) support the
contribution of both components. These results demonstrate that a compact
Transformer-RNN hybrid delivers accurate and efficient RUL predictions on
CMAPSS, making it suitable for real-time industrial prognostics.

</details>


### [225] [Bayesian Network Structure Discovery Using Large Language Models](https://arxiv.org/abs/2511.00574)
*Yinghuan Zhang,Yufei Zhang,Parisa Kordjamshidi,Zijun Cui*

Main category: cs.LG

TL;DR: 提出以大语言模型为核心的贝叶斯网络结构发现统一框架，在低数据或无数据场景表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统结构学习方法需大量观测数据且计算成本高，现有基于大语言模型的方法未将其用于核心学习过程。

Method: 提出统一框架，数据免费时用PromptBN查询大语言模型，有观测数据时用ReActBN结合推理范式和结构分数迭代优化，且让大语言模型全程参与。

Result: 实验表明该方法显著优于现有基于大语言模型的方法和传统数据驱动算法，尤其在低数据或无数据场景。

Conclusion: 所提以大语言模型为核心的贝叶斯网络结构发现框架有效，代码已公开。

Abstract: Understanding probabilistic relationships among variables is crucial for
analyzing complex systems. Traditional structure learning methods often require
extensive observational data and incur high computational costs. Recent studies
have explored using large language models (LLMs) for structure learning, but
most treat LLMs as auxiliary tools for pre-processing or post-processing,
leaving the core learning process data-driven. In this work, we propose a
unified framework for Bayesian network structure discovery that places LLMs at
the center, supporting both data-free and data-aware settings. In the data-free
case, we introduce \textbf{PromptBN} to query LLMs with metadata and
efficiently uncover valid probabilistic relationships. When observational data
are available, we introduce \textbf{ReActBN}, which integrates the ReAct
reasoning paradigm with structure scores such as the Bayesian Information
Criterion (BIC) for iterative refinement. Unlike prior methods that offload
refinement to external algorithms, our framework maintains the LLM actively in
the loop throughout the discovery process. Experiments demonstrate that our
method significantly outperforms both existing LLM-based approaches and
traditional data-driven algorithms, particularly in the low- or no-data
scenario. Code is publicly available at
{\texttt{\textcolor{magenta}{https://github.com/sherryzyh/prompt2bn}}}.

</details>


### [226] [Diagnosing Hallucination Risk in AI Surgical Decision-Support: A Sequential Framework for Sequential Validation](https://arxiv.org/abs/2511.00588)
*Dong Chen,Yanzhe Wei,Zonglin He,Guan-Ming Kuang,Canhua Ye,Meiru An,Huili Peng,Yong Hu,Huiren Tao,Kenneth MC Cheung*

Main category: cs.LG

TL;DR: 研究引入评估框架量化大语言模型在脊柱手术临床决策支持中的幻觉风险，评估6个模型，发现推理增强变体不总优于标准版本，多维测试暴露模型弱点，建议集成可解释机制和建立安全验证框架。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在脊柱手术临床决策支持有潜力，但幻觉问题会危害患者安全，需量化风险。

Method: 引入以临床医生为中心的框架，从诊断精度、推荐质量等方面评估，对6个大语言模型在30个专家验证的脊柱病例上进行评估。

Result: DeepSeek - R1总体表现优；推理增强模型变体不总优于标准版本；多维压力测试暴露模型特定弱点，推荐质量在复杂度增加时下降7.4%，理性、可读性和诊断有小提升。

Conclusion: 建议将可解释机制集成到临床工作流程，建立安全验证框架用于外科大语言模型部署。

Abstract: Large language models (LLMs) offer transformative potential for clinical
decision support in spine surgery but pose significant risks through
hallucinations, which are factually inconsistent or contextually misaligned
outputs that may compromise patient safety. This study introduces a
clinician-centered framework to quantify hallucination risks by evaluating
diagnostic precision, recommendation quality, reasoning robustness, output
coherence, and knowledge alignment. We assessed six leading LLMs across 30
expert-validated spinal cases. DeepSeek-R1 demonstrated superior overall
performance (total score: 86.03 $\pm$ 2.08), particularly in high-stakes
domains such as trauma and infection. A critical finding reveals that
reasoning-enhanced model variants did not uniformly outperform standard
counterparts: Claude-3.7-Sonnet's extended thinking mode underperformed
relative to its standard version (80.79 $\pm$ 1.83 vs. 81.56 $\pm$ 1.92),
indicating extended chain-of-thought reasoning alone is insufficient for
clinical reliability. Multidimensional stress-testing exposed model-specific
vulnerabilities, with recommendation quality degrading by 7.4% under amplified
complexity. This decline contrasted with marginal improvements in rationality
(+2.0%), readability (+1.7%) and diagnosis (+4.7%), highlighting a concerning
divergence between perceived coherence and actionable guidance. Our findings
advocate integrating interpretability mechanisms (e.g., reasoning chain
visualization) into clinical workflows and establish a safety-aware validation
framework for surgical LLM deployment.

</details>


### [227] [Gaining Momentum: Uncovering Hidden Scoring Dynamics in Hockey through Deep Neural Sequencing and Causal Modeling](https://arxiv.org/abs/2511.00615)
*Daniel Griffiths,Piper Moskow*

Main category: cs.LG

TL;DR: 提出统一数据驱动框架量化和提升职业曲棍球进攻势头与得分可能性，有五阶段流程，观测到得分潜力相对提升15%，为教练和分析师提供实时见解。


<details>
  <summary>Details</summary>
Motivation: 量化和提升职业曲棍球的进攻势头和得分可能性。

Method: 利用541,000条NHL事件记录数据集，构建包含逻辑回归、梯度提升决策树、LSTM网络、PCA和K - Means聚类、X - Learner因果推断估计器的五阶段端到端流程。

Result: 观测到平均处理效应为0.12，得分潜力有15%的相对提升。

Conclusion: 战略性结构化序列和紧凑阵型能提升进攻表现，框架为教练和分析师提供实时见解，推动曲棍球分析向战术优化发展。

Abstract: We present a unified, data-driven framework for quantifying and enhancing
offensive momentum and scoring likelihood (expected goals, xG) in professional
hockey. Leveraging a Sportlogiq dataset of 541,000 NHL event records, our
end-to-end pipeline comprises five stages: (1) interpretable momentum weighting
of micro-events via logistic regression; (2) nonlinear xG estimation using
gradient-boosted decision trees; (3) temporal sequence modeling with Long
Short-Term Memory (LSTM) networks; (4) spatial formation discovery through
principal component analysis (PCA) followed by K-Means clustering on
standardized player coordinates; and (5) use of an X-Learner causal inference
estimator to quantify the average treatment effect (ATE) of adopting the
identified "optimal" event sequences and formations. We observe an ATE of 0.12
(95% CI: 0.05-0.17, p < 1e-50), corresponding to a 15% relative gain in scoring
potential. These results demonstrate that strategically structured sequences
and compact formations causally elevate offensive performance. Our framework
delivers real-time, actionable insights for coaches and analysts, advancing
hockey analytics toward principled, causally grounded tactical optimization.

</details>


### [228] [Reviving Stale Updates: Data-Free Knowledge Distillation for Asynchronous Federated Learning](https://arxiv.org/abs/2511.00655)
*Baris Askin,Holger R. Roth,Zhenyu Sun,Carlee Joe-Wong,Gauri Joshi,Ziyue Xu*

Main category: cs.LG

TL;DR: 提出FedRevive异步联邦学习框架，用无数据知识蒸馏恢复陈旧更新，实验显示比异步基线训练更快、精度更高。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习可扩展性受同步开销限制，异步联邦学习虽提高效率但存在陈旧更新问题，影响优化和收敛。

Method: 提出FedRevive框架，集成参数空间聚合与轻量级服务器端无数据知识蒸馏，用元学习生成器合成伪样本实现多教师蒸馏，采用混合聚合方案结合原始和蒸馏更新。

Result: 在多种视觉和文本基准测试中，FedRevive比异步基线训练速度最高快32.1%，最终精度最高高21.5%。

Conclusion: FedRevive能有效缓解陈旧更新问题，保留异步联邦学习可扩展性，提升训练速度和精度。

Abstract: Federated Learning (FL) enables collaborative model training across
distributed clients without sharing raw data, yet its scalability is limited by
synchronization overhead. Asynchronous Federated Learning (AFL) alleviates this
issue by allowing clients to communicate independently, thereby improving
wall-clock efficiency in large-scale, heterogeneous environments. However, this
asynchrony introduces stale updates (client updates computed on outdated global
models) that can destabilize optimization and hinder convergence. We propose
FedRevive, an asynchronous FL framework that revives stale updates through
data-free knowledge distillation (DFKD). FedRevive integrates parameter-space
aggregation with a lightweight, server-side DFKD process that transfers
knowledge from stale client models to the current global model without access
to real or public data. A meta-learned generator synthesizes pseudo-samples,
which enables multi-teacher distillation. A hybrid aggregation scheme that
combines raw updates with DFKD updates effectively mitigates staleness while
retaining the scalability of AFL. Experiments on various vision and text
benchmarks show that FedRevive achieves faster training up to 32.1% and higher
final accuracy up to 21.5% compared to asynchronous baselines.

</details>


### [229] [Sensitivity Analysis for Climate Science with Generative Flow Models](https://arxiv.org/abs/2511.00663)
*Alex Dobra,Jakiw Pidstrigach,Tim Reichelt,Paolo Fraccaro,Johannes Jakubik,Anne Jones,Christian Schroeder de Witt,Philip Stier,Philip Torr*

Main category: cs.LG

TL;DR: 本文提出用伴随状态法计算生成流模型梯度以解决气候敏感性分析计算瓶颈，减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统物理模型计算气候敏感性在计算和开发时间上成本过高，现代基于AI的生成模型计算敏感性也存在瓶颈。

Method: 将伴随状态法应用于生成流模型计算梯度，以扩散模型为特例，应用于cBottle生成模型对海面温度进行敏感性分析，提出梯度自一致性检查。

Result: 该方法能产生可靠梯度，将敏感性分析计算成本从物理模型在超级计算机上的数周降至GPU上的数小时。

Conclusion: 该方法简化了气候科学中的关键工作流程，具有一定可靠性和有效性。

Abstract: Sensitivity analysis is a cornerstone of climate science, essential for
understanding phenomena ranging from storm intensity to long-term climate
feedbacks. However, computing these sensitivities using traditional physical
models is often prohibitively expensive in terms of both computation and
development time. While modern AI-based generative models are orders of
magnitude faster to evaluate, computing sensitivities with them remains a
significant bottleneck. This work addresses this challenge by applying the
adjoint state method for calculating gradients in generative flow models, with
diffusion models as a special case. We apply this method to the cBottle
generative model, an emulator of ERA5 data, to perform sensitivity analysis
with respect to sea surface temperatures. Furthermore, we propose a novel
gradient self-consistency check to quantitatively validate the computed
sensitivities against the model's own outputs. Our results provide initial
evidence that this approach can produce reliable gradients, reducing the
computational cost of sensitivity analysis from weeks on a supercomputer with a
physical model to hours on a GPU, thereby simplifying a critical workflow in
climate science.

</details>


### [230] [Inference-Time Chain-of-Thought Pruning with Latent Informativeness Signals](https://arxiv.org/abs/2511.00699)
*Sophie Li,Nicholas Huang,Nayan Saxena,Nina Luo,Vincent Lin,Kevin Zhu,Sunishchal Dev*

Main category: cs.LG

TL;DR: 提出KAPPA算法，结合KL散度、置信度和熵指导剪枝，在推理时减少内存和token使用，实验显示能稳定小模型性能并大幅减少资源消耗，对准确率影响小。


<details>
  <summary>Details</summary>
Motivation: 标准Best-of-N方法计算成本高，Self-Truncation Best-of-N依赖一致性启发式有局限，需新方法解决这些问题。

Method: 提出KL-Adjusted Pruned Path Algorithm (KAPPA)，将Kullback-Leibler散度、置信度和熵结合成评分函数指导渐进式剪枝。

Result: 在GSM8K和MATH500上用DeepSeek-R1-Distill-Qwen-1.5B和Qwen2.5-7B-Instruct实验，KAPPA稳定小模型性能，相比BoN，峰值内存最多减少约60%，总token生成最多减少约90%，对准确率影响小。

Conclusion: KAPPA能在保持准确率的同时，大幅减少内存和token使用。

Abstract: Large language models (LLMs) improve reasoning accuracy when generating
multiple candidate solutions at test time, but standard methods like Best-of-N
(BoN) incur high computational cost by fully generating all branches.
Self-Truncation Best-of-N (ST-BoN) mitigates this by truncating unpromising
paths early, but its reliance on consistency-based heuristics is a limitation
as it does not directly evaluate branch quality. We present KL-Adjusted Pruned
Path Algorithm (KAPPA), an inference-time method that combines Kullback-Leibler
divergence, confidence, and entropy into a principled scoring function to guide
progressive pruning. By promoting diversity during exploration and selectively
eliminating low-scoring branches, KAPPA maintains accuracy while substantially
reducing memory and token usage. Experiments on GSM8K and MATH500 with
DeepSeek-R1-Distill-Qwen-1.5B and Qwen2.5-7B-Instruct demonstrate that KAPPA
stabilizes performance in smaller models and achieves up to ~60% reduction in
peak memory and ~90% reduction in total token generation relative to BoN, with
minimal impact on accuracy.

</details>


### [231] [Privacy-Aware Time Series Synthesis via Public Knowledge Distillation](https://arxiv.org/abs/2511.00700)
*Penghang Liu,Haibei Zhu,Eleonora Kreacic,Svitlana Vyetrenko*

Main category: cs.LG

TL;DR: 论文提出Pub2Priv框架，利用异构公共知识生成私有时间序列数据，实验表明其在多个领域改善隐私 - 效用权衡上优于现有基准。


<details>
  <summary>Details</summary>
Motivation: 现有隐私感知数据生成方法常忽略敏感序列与公开非敏感上下文元数据的关联，导致隐私 - 效用权衡不佳。

Method: 提出Pub2Priv框架，用自注意力机制将公共数据编码为时间和特征嵌入，作为扩散模型条件输入生成合成私有序列，还引入实用指标评估隐私。

Result: Pub2Priv在金融、能源和商品交易领域改善隐私 - 效用权衡上始终优于现有基准。

Conclusion: Pub2Priv框架能有效利用公共知识生成私有时间序列数据，在隐私 - 效用权衡上表现出色。

Abstract: Sharing sensitive time series data in domains such as finance, healthcare,
and energy consumption, such as patient records or investment accounts, is
often restricted due to privacy concerns. Privacy-aware synthetic time series
generation addresses this challenge by enforcing noise during training,
inherently introducing a trade-off between privacy and utility. In many cases,
sensitive sequences is correlated with publicly available, non-sensitive
contextual metadata (e.g., household electricity consumption may be influenced
by weather conditions and electricity prices). However, existing privacy-aware
data generation methods often overlook this opportunity, resulting in
suboptimal privacy-utility trade-offs. In this paper, we present Pub2Priv, a
novel framework for generating private time series data by leveraging
heterogeneous public knowledge. Our model employs a self-attention mechanism to
encode public data into temporal and feature embeddings, which serve as
conditional inputs for a diffusion model to generate synthetic private
sequences. Additionally, we introduce a practical metric to assess privacy by
evaluating the identifiability of the synthetic data. Experimental results show
that Pub2Priv consistently outperforms state-of-the-art benchmarks in improving
the privacy-utility trade-off across finance, energy, and commodity trading
domains.

</details>


### [232] [TRISKELION-1: Unified Descriptive-Predictive-Generative AI](https://arxiv.org/abs/2511.00711)
*Nardeep Kumar,Arun Kanwar*

Main category: cs.LG

TL;DR: TRISKELION - 1是统一架构，用变分目标联合优化描述、预测和生成，MNIST实验验证其稳定性，为通用智能架构提供蓝图。


<details>
  <summary>Details</summary>
Motivation: 构建能集成统计、机械和生成推理，联合优化描述、预测和生成的统一架构，迈向通用智能架构。

Method: 在单个编解码器框架中整合多种推理，使用变分目标进行联合优化。

Result: MNIST实验验证描述性重建、预测性分类和生成性采样可在一个模型中稳定共存。

Conclusion: 该框架为连接可解释性、准确性和创造性的通用智能架构提供了蓝图。

Abstract: TRISKELION-1 is a unified descriptive-predictive-generative architecture that
integrates statistical, mechanistic, and generative reasoning within a single
encoder-decoder framework. The model demonstrates how descriptive
representation learning, predictive inference, and generative synthesis can be
jointly optimized using variational objectives. Experiments on MNIST validate
that descriptive reconstruction, predictive classification, and generative
sampling can coexist stably within one model. The framework provides a
blueprint toward universal intelligence architectures that connect
interpretability, accuracy, and creativity.

</details>


### [233] [Enhancing Heavy Rain Nowcasting with Multimodal Data: Integrating Radar and Satellite Observations](https://arxiv.org/abs/2511.00716)
*Rama Kassoumeh,David Rügamer,Henning Oppel*

Main category: cs.LG

TL;DR: 文章评估融合卫星与雷达数据进行临近预报的有效性，开发多模态临近预报模型，结果显示该模型优于仅用雷达的方法，能提高预报精度并提供更准确预警。


<details>
  <summary>Details</summary>
Motivation: 城市暴雨频发致城市内涝，传统监测系统有局限，仅用雷达预报暴雨有挑战，因此需评估融合卫星与雷达数据进行临近预报的有效性。

Method: 开发结合雷达和卫星图像的多模态临近预报模型，预测5、15和30分钟的降水情况。

Result: 多模态策略显著优于仅用雷达的方法，整合卫星数据提高了预报精度，尤其对强降水；在5分钟提前预报时，该模型使暴雨和大暴雨的临界成功指数分别提高4%和3%，且在较长提前时间预报时表现更好。

Conclusion: 多模态模型预报更精确，能提供及时、可靠、救命的预警。

Abstract: The increasing frequency of heavy rainfall events, which are a major cause of
urban flooding, underscores the urgent need for accurate precipitation
forecasting - particularly in urban areas where localized events often go
undetected by ground-based sensors. In Germany, only 17.3% of hourly heavy rain
events between 2001 and 2018 were recorded by rain gauges, highlighting the
limitations of traditional monitoring systems. Radar data are another source
that effectively tracks ongoing precipitation; however, forecasting the
development of heavy rain using radar alone remains challenging due to the
brief and unpredictable nature of such events. Our focus is on evaluating the
effectiveness of fusing satellite and radar data for nowcasting. We develop a
multimodal nowcasting model that combines both radar and satellite imagery for
predicting precipitation at lead times of 5, 15, and 30 minutes. We demonstrate
that this multimodal strategy significantly outperforms radar-only approaches.
Experimental results show that integrating satellite data improves prediction
accuracy, particularly for intense precipitation. The proposed model increases
the Critical Success Index for heavy rain by 4% and for violent rain by 3% at a
5-minute lead time. Moreover, it maintains higher predictive skill at longer
lead times, where radar-only performance declines. A qualitative analysis of
the severe flooding event in the state of North Rhine-Westphalia, Germany in
2021 further illustrates the superior performance of the multimodal model.
Unlike the radar-only model, which captures general precipitation patterns, the
multimodal model yields more detailed and accurate forecasts for regions
affected by heavy rain. This improved precision enables timely, reliable,
life-saving warnings. Implementation available at
https://github.com/RamaKassoumeh/Multimodal_heavy_rain

</details>


### [234] [Effective Series Decomposition and Components Learning for Time Series Generation](https://arxiv.org/abs/2511.00747)
*Zixuan Ma,Chenfeng Huang*

Main category: cs.LG

TL;DR: 提出STDiffusion框架用于多变量时间序列生成，结合扩散概率模型与可学习序列分解技术，在多数据集上达SOTA，且在多窗口长序列生成任务表现可靠。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列生成方法未采用解释性分解方法，难以合成有意义的趋势和季节性模式。

Method: 引入Seasonal - Trend Diffusion (STDiffusion)框架，将趋势和季节性学习分离，用MLP捕捉趋势，自适应小波蒸馏学习季节性成分，设计综合校正机制。

Result: 在八个真实世界数据集上达到时间序列生成任务的SOTA性能，在多窗口长序列时间序列生成中结果可靠。

Conclusion: STDiffusion框架具有高解释性、鲁棒性和通用性，能有效解决现有时间序列生成方法的不足。

Abstract: Time series generation focuses on modeling the underlying data distribution
and resampling to produce authentic time series data. Key components, such as
trend and seasonality, drive temporal fluctuations, yet many existing
approaches fail to employ interpretative decomposition methods, limiting their
ability to synthesize meaningful trend and seasonal patterns. To address this
gap, we introduce Seasonal-Trend Diffusion (STDiffusion), a novel framework for
multivariate time series generation that integrates diffusion probabilistic
models with advanced learnable series decomposition techniques, enhancing the
interpretability of the generation process. Our approach separates the trend
and seasonal learning into distinct blocks: a Multi-Layer Perceptron (MLP)
structure captures the trend, while adaptive wavelet distillation facilitates
effective multi-resolution learning of seasonal components. This decomposition
improves the interpretability of the model on multiple scales. In addition, we
designed a comprehensive correction mechanism aimed at ensuring that the
generated components exhibit a high degree of internal consistency and preserve
meaningful interrelationships with one another. Our empirical studies on eight
real-world datasets demonstrate that STDiffusion achieves state-of-the-art
performance in time series generation tasks. Furthermore, we extend the model's
application to multi-window long-sequence time series generation, which
delivered reliable results and highlighted its robustness and versatility.

</details>


### [235] [Efficient Reinforcement Learning for Large Language Models with Intrinsic Exploration](https://arxiv.org/abs/2511.00794)
*Yan Sun,Jia Guo,Stanley Kok,Zihao Wang,Zujie Wen,Zhiqiang Zhang*

Main category: cs.LG

TL;DR: 研究提出PREPO方法提升强化学习与可验证奖励（RLVR）的数据效率，减少滚动步数，在Qwen和Llama模型上取得效果并给出理论分析。


<details>
  <summary>Details</summary>
Motivation: RLVR训练成本高，许多滚动对优化贡献小，研究如何利用内在数据属性提升数据效率。

Method: 提出PREPO方法，包含采用提示困惑度指示模型学习适应性，通过区分相对熵放大滚动差异并优先探索度高的序列。

Result: 在Qwen和Llama模型的数学推理基准测试中，PREPO比基线最多减少3倍滚动步数且保持竞争力。

Conclusion: PREPO能有效提升RLVR的数据效率，同时给出理论和深入分析解释方法原理。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has improved the
reasoning ability of large language models, yet training remains costly because
many rollouts contribute little to optimization, considering the amount of
computation required. This study investigates how simply leveraging intrinsic
data properties, almost free benefit during training, can improve data
efficiency for RLVR. We propose PREPO with two complementary components. First,
we adopt prompt perplexity as an indicator of model adaptability in learning,
enabling the model to progress from well-understood contexts to more
challenging ones. Second, we amplify the discrepancy among the rollouts by
differentiating their relative entropy, and prioritize sequences that exhibit a
higher degree of exploration. Together, these mechanisms reduce rollout demand
while preserving competitive performance. On the Qwen and Llama models, PREPO
achieves effective results on mathematical reasoning benchmarks with up to 3
times fewer rollouts than the baselines. Beyond empirical gains, we provide
theoretical and in-depth analyses explaining the underlying rationale of our
method to improve the data efficiency of RLVR.

</details>


### [236] [Attention Saturation and Gradient Suppression at Inflection Layers: Diagnosing and Mitigating Bottlenecks in Transformer Adaptation](https://arxiv.org/abs/2511.00797)
*Wang Zixian*

Main category: cs.LG

TL;DR: 预训练Transformer在微调时存在问题，本文分析输出饱和机制，引入诊断指标识别拐点层，提出微调策略，实验显示不同预训练情况效果不同。


<details>
  <summary>Details</summary>
Motivation: 解决预训练Transformer在微调时对源模式过度自信、难以形成新目标域模式的问题。

Method: 通过标准交叉熵和softmax分析输出饱和机制，引入层诊断指标识别拐点层，提出先诊断后轻注入的微调策略。

Result: 在BERT-base从SST - 2迁移到Rotten Tomatoes实验中，过训练初始化从拐点层LoRA注入受益，欠训练初始化性能下降。

Conclusion: 基础特征强时，解锁拐点层利于高层组合适应；基础特征弱时，需全路径解锁进行低层重建。

Abstract: Pre-trained Transformers often exhibit over-confidence in source patterns and
difficulty in forming new target-domain patterns during fine-tuning. We
formalize the mechanism of output saturation leading to gradient suppression
through standard cross-entropy and softmax analysis, showing that gradient
suppression at inflection layers confines adaptation to high-level
recombination of existing features while preventing low-level reconstruction.
We introduce a set of layer-wise diagnostic metrics -- attention entropy
(saturation proxy), activation gradient norm, parameter gradient norm, and
Delta-CKA under a shared PCA basis -- to identify inflection layers
characterized by both low attention entropy and steep gradient decay. Building
on these findings, we propose a diagnose-first, inject-light fine-tuning
strategy: selectively inserting LoRA adapters at inflection layers to restore
suppressed backward signals with minimal parameter overhead. Experiments on
BERT-base transfer from SST-2 to Rotten Tomatoes under under-trained and
over-trained source regimes reveal that over-trained initialization benefits
from inflection-layer LoRA injection, while under-trained initialization
suffers performance degradation. When base features are strong, unblocking
inflection layers facilitates high-level compositional adaptation; when base
features are weak, full-pathway unblocking is required for low-level
reconstruction, as supported by joint analysis of layer-wise activation
gradients and Delta-CKA dynamics.

</details>


### [237] [EraseFlow: Learning Concept Erasure Policies via GFlowNet-Driven Alignment](https://arxiv.org/abs/2511.00804)
*Abhiram Kusumba,Maitreya Patel,Kyle Min,Changhoon Kim,Chitta Baral,Yezhou Yang*

Main category: cs.LG

TL;DR: 提出EraseFlow框架用于从文本到图像生成器中擦除有害或专有概念，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前概念擦除技术存在降低图像质量、依赖不稳定对抗损失或需要大量再训练周期等问题。

Method: 将概念遗忘视为去噪路径空间的探索，用配备轨迹平衡目标的GFlowNets进行优化，学习随机策略。

Result: 广泛的实验结果表明EraseFlow优于现有基线，在性能和先验保留之间实现了最佳权衡。

Conclusion: EraseFlow有效消除了对精心设计奖励模型的需求，能有效泛化到未见概念，避免可破解的奖励，同时提高性能。

Abstract: Erasing harmful or proprietary concepts from powerful text to image
generators is an emerging safety requirement, yet current "concept erasure"
techniques either collapse image quality, rely on brittle adversarial losses,
or demand prohibitive retraining cycles. We trace these limitations to a myopic
view of the denoising trajectories that govern diffusion based generation. We
introduce EraseFlow, the first framework that casts concept unlearning as
exploration in the space of denoising paths and optimizes it with GFlowNets
equipped with the trajectory balance objective. By sampling entire trajectories
rather than single end states, EraseFlow learns a stochastic policy that steers
generation away from target concepts while preserving the model's prior.
EraseFlow eliminates the need for carefully crafted reward models and by doing
this, it generalizes effectively to unseen concepts and avoids hackable rewards
while improving the performance. Extensive empirical results demonstrate that
EraseFlow outperforms existing baselines and achieves an optimal trade off
between performance and prior preservation.

</details>


### [238] [Logic-informed reinforcement learning for cross-domain optimization of large-scale cyber-physical systems](https://arxiv.org/abs/2511.00806)
*Guangxi Wan,Peng Zeng,Xiaoting Dong,Chunhe Song,Shijie Cui,Dong Li,Qingwei Dong,Yiyang Liu,Hongfei Bai*

Main category: cs.LG

TL;DR: 提出逻辑感知强化学习（LIRL）解决网络物理系统联合优化问题，实验显示其优于现有方法，可无缝转移到其他领域。


<details>
  <summary>Details</summary>
Motivation: 现有分层方法难达全局最优，混合动作空间强化学习难保证约束满足。

Method: 为标准策略梯度算法配备投影，将低维潜在动作映射到由一阶逻辑动态定义的可允许混合流形。

Result: 在多场景实验中优于现有分层优化方法，如在工业制造中最多降低36.47% - 44.33%的综合目标值，保持零约束违反。

Conclusion: LIRL框架可无缝转移到其他领域，为大规模CPS的安全实时优化铺平道路。

Abstract: Cyber-physical systems (CPS) require the joint optimization of discrete cyber
actions and continuous physical parameters under stringent safety logic
constraints. However, existing hierarchical approaches often compromise global
optimality, whereas reinforcement learning (RL) in hybrid action spaces often
relies on brittle reward penalties, masking, or shielding and struggles to
guarantee constraint satisfaction. We present logic-informed reinforcement
learning (LIRL), which equips standard policy-gradient algorithms with
projection that maps a low-dimensional latent action onto the admissible hybrid
manifold defined on-the-fly by first-order logic. This guarantees feasibility
of every exploratory step without penalty tuning. Experimental evaluations have
been conducted across multiple scenarios, including industrial manufacturing,
electric vehicle charging stations, and traffic signal control, in all of which
the proposed method outperforms existing hierarchical optimization approaches.
Taking a robotic reducer assembly system in industrial manufacturing as an
example, LIRL achieves a 36.47\% to 44.33\% reduction at most in the combined
makespan-energy objective compared to conventional industrial hierarchical
scheduling methods. Meanwhile, it consistently maintains zero constraint
violations and significantly surpasses state-of-the-art hybrid-action
reinforcement learning baselines. Thanks to its declarative logic-based
constraint formulation, the framework can be seamlessly transferred to other
domains such as smart transportation and smart grid, thereby paving the way for
safe and real-time optimization in large-scale CPS.

</details>


### [239] [Equilibrium Policy Generalization: A Reinforcement Learning Framework for Cross-Graph Zero-Shot Generalization in Pursuit-Evasion Games](https://arxiv.org/abs/2511.00811)
*Runyu Lu,Peng Zhang,Ruochuan Shi,Yuanheng Zhu,Dongbin Zhao,Yang Liu,Dong Wang,Cesare Alippi*

Main category: cs.LG

TL;DR: 提出Equilibrium Policy Generalization (EPG)框架用于学习具有跨图零样本性能的策略，解决追捕逃避游戏计算耗时问题，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 追捕逃避游戏准确求解需指数时间，现有RL方法在图结构变化时需重新计算或微调，耗时且影响实时性。

Method: 提出EPG框架，跨不同图结构训练RL策略，用动态规划算法构建单图策略的均衡预言机，通过设计分组机制和序列模型扩展DP和RL。

Result: EPG框架在各种未见真实图中保证了理想的零样本性能，广义追捕者策略能匹配最先进方法微调策略的性能。

Conclusion: EPG框架能有效学习广义策略，具有跨图零样本性能，在追捕逃避游戏领域具有优势。

Abstract: Equilibrium learning in adversarial games is an important topic widely
examined in the fields of game theory and reinforcement learning (RL).
Pursuit-evasion game (PEG), as an important class of real-world games from the
fields of robotics and security, requires exponential time to be accurately
solved. When the underlying graph structure varies, even the state-of-the-art
RL methods require recomputation or at least fine-tuning, which can be
time-consuming and impair real-time applicability. This paper proposes an
Equilibrium Policy Generalization (EPG) framework to effectively learn a
generalized policy with robust cross-graph zero-shot performance. In the
context of PEGs, our framework is generally applicable to both pursuer and
evader sides in both no-exit and multi-exit scenarios. These two
generalizability properties, to our knowledge, are the first to appear in this
domain. The core idea of the EPG framework is to train an RL policy across
different graph structures against the equilibrium policy for each single
graph. To construct an equilibrium oracle for single-graph policies, we present
a dynamic programming (DP) algorithm that provably generates pure-strategy Nash
equilibrium with near-optimal time complexity. To guarantee scalability with
respect to pursuer number, we further extend DP and RL by designing a grouping
mechanism and a sequence model for joint policy decomposition, respectively.
Experimental results show that, using equilibrium guidance and a distance
feature proposed for cross-graph PEG training, the EPG framework guarantees
desirable zero-shot performance in various unseen real-world graphs. Besides,
when trained under an equilibrium heuristic proposed for the graphs with exits,
our generalized pursuer policy can even match the performance of the fine-tuned
policies from the state-of-the-art PEG methods.

</details>


### [240] [LL-ViT: Edge Deployable Vision Transformers with Look Up Table Neurons](https://arxiv.org/abs/2511.00812)
*Shashank Nag,Alan T. L. Bacellar,Zachary Susskind,Anshul Jha,Logan Liberty,Aishwarya Sivakumar,Eugene B. John,Krishnan Kailas,Priscila M. V. Lima,Neeraja J. Yadwadkar,Felipe M. G. Franca,Lizy K. John*

Main category: cs.LG

TL;DR: 提出LL - ViT，结合LUT神经元与Transformer架构，设计FPGA加速器，在边缘工作负载上评估有良好表现，减少模型权重和乘法，提升能效和吞吐量。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers在FPGA边缘推理时计算、内存和能量需求大，现有LUT网络在常见视觉任务表现不佳，需设计边缘优化的视觉Transformer。

Method: 在Transformer架构中集成LUT神经元层，设计基于LUT的通道混合器，开发FPGA加速器，采用神经学习方法学习LUT函数。

Result: 在CIFAR - 10、CIFAR - 100和Tiny - ImageNet数据集上达到一定准确率，消除超60%模型权重和50%乘法，相比整数量化ViT加速器有更好能效和更低延迟，在10.9W功率预算下吞吐量更优。

Conclusion: LL - ViT是一种计算和能量高效的视觉Transformer推理解决方案。

Abstract: Vision Transformers have been tremendously successful in computer vision
tasks. However, their large computational, memory, and energy demands are a
challenge for edge inference on FPGAs -- a field that has seen a recent surge
in demand. We recognize the benefits of recent works on logic and Look Up Table
(LUT) based networks, such as LogicNets, NeuraLUT, DWN, among others, in
offering models that simultaneously reduce both the memory and compute
footprints. However, these models natively do not perform well on common vision
tasks, such as CIFAR-10/100. In this work, we propose LL-ViT, a novel edge
optimized vision transformer design that integrates layers of LUT neurons
within the transformer architecture. Based on our characterization that reveals
that a majority of model weights and computations are from the channel mixer
(MLP layer), we design an alternate LUT-based channel mixer, and simultaneously
develop an FPGA-based accelerator for LL-ViT. Contrary to some attempts to
replace each multiplication with a table lookup, our architecture utilizes a
neural learning approach which natively learns the LUT functions. This approach
allows for reduced model sizes, and a computational and energy-efficient
inference solution for vision transformer models. Evaluating on edge-suitable
workloads, we achieve accuracies of 95.5% on CIFAR-10, 78.8% on CIFAR-100, and
60.9% on Tiny-ImageNet datasets, comparable to the baseline transformer. LL-ViT
eliminates over 60% of the model weights and 50% of the multiplications in the
model, and achieves 1.9x energy efficiency and 1.3x lower latency over an
integer quantized ViT accelerator, while also offering superior throughput
against prior works at a 10.9W power budget.

</details>


### [241] [Identifying Slug Formation in Oil Well Pipelines: A Use Case from Industrial Analytics](https://arxiv.org/abs/2511.00851)
*Abhishek Patange,Sharat Chidambaran,Prabhat Shankar,Manjunath G. B.,Anindya Chatterjee*

Main category: cs.LG

TL;DR: 提出交互式应用实现端到端数据驱动段塞检测，具有轻量便携易部署特点，适用于关键流程行业。


<details>
  <summary>Details</summary>
Motivation: 现有油气管道段塞检测方法离线、需专业知识且缺乏实时可解释性。

Method: 开发交互式应用，集成数据探索与标注、模型训练评估、结果可视化及实时推理模块，结合领域分析与新UI/UX特性。

Result: 实现从上传CSV到实时推理的无缝工作流，工具轻量、便携、易部署。

Conclusion: 该工具兼具研究与工业应用价值，可弥合数据科学方法与现实决策差距，适用于更多时间序列故障诊断任务。

Abstract: Slug formation in oil and gas pipelines poses significant challenges to
operational safety and efficiency, yet existing detection approaches are often
offline, require domain expertise, and lack real-time interpretability. We
present an interactive application that enables end-to-end data-driven slug
detection through a compact and user-friendly interface. The system integrates
data exploration and labeling, configurable model training and evaluation with
multiple classifiers, visualization of classification results with time-series
overlays, and a real-time inference module that generates persistence-based
alerts when slug events are detected. The demo supports seamless workflows from
labeled CSV uploads to live inference on unseen datasets, making it
lightweight, portable, and easily deployable. By combining domain-relevant
analytics with novel UI/UX features such as snapshot persistence, visual
labeling, and real-time alerting, our tool adds significant dissemination value
as both a research prototype and a practical industrial application. The demo
showcases how interactive human-in-the-loop ML systems can bridge the gap
between data science methods and real-world decision-making in critical process
industries, with broader applicability to time-series fault diagnosis tasks
beyond oil and gas.

</details>


### [242] [FlexiCache: Leveraging Temporal Stability of Attention Heads for Efficient KV Cache Management](https://arxiv.org/abs/2511.00868)
*Nazmul Takbir,Hamidreza Alikhani,Nikil Dutt,Sangeetha Abdu Jyothi*

Main category: cs.LG

TL;DR: 提出FlexiCache系统利用KV头的时间稳定性减少GPU内存使用和计算开销，提升性能并保持精度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型服务受KV缓存大小限制，现有系统难以在不降低精度下高效利用关键令牌。

Method: 将KV头分为稳定和不稳定，不稳定头的KV缓存页全保留在GPU内存，稳定头只在GPU保留前K页，其余卸载到主机内存，对稳定头定期重新排序获取新的前K页。

Result: 在vLLM上实现，长上下文请求减少GPU内存占用达70%，离线服务吞吐量提升1.38 - 1.55倍，在线令牌延迟降低1.6 - 2.1倍。

Conclusion: FlexiCache在长上下文、长生成场景下能有效减少资源占用，提升性能且保持精度。

Abstract: Large Language Model (LLM) serving is increasingly constrained by the growing
size of the key-value (KV) cache, which scales with both context length and
generation length. Prior work shows that attention is dominated by a small
subset of critical tokens, yet existing systems struggle to exploit this
efficiently without degrading accuracy, especially in long generation. We make
a key observation: the temporal stability of these critical tokens varies
significantly across KV heads: some heads consistently focus on the same
tokens, while others shift frequently. Building on this insight, we introduce
FlexiCache, a hierarchical KV-cache management system that leverages the
temporal stability of KV heads to reduce GPU memory usage and computation
overhead, while preserving model accuracy. FlexiCache classifies KV heads as
stable or unstable: it retains all KV-cache pages from unstable heads in GPU
memory, whereas for stable heads, it keeps only the top-K pages on the GPU and
offloads the rest to host memory. By exploiting temporal stability, FlexiCache
performs periodic reranking for stable heads to fetch newly promoted top pages.
Implemented atop vLLM, FlexiCache reduces GPU memory footprint for long-context
requests by up to 70%, improves offline serving throughput by 1.38-1.55x, and
lowers online token latency by 1.6-2.1x, all while maintaining accuracy in
long-context, long-generation scenarios.

</details>


### [243] [Training with Fewer Bits: Unlocking Edge LLMs Training with Stochastic Rounding](https://arxiv.org/abs/2511.00874)
*Taowen Liu,Marta Andronic,Deniz Gündüz,George A. Constantinides*

Main category: cs.LG

TL;DR: 研究随机舍入（SR）在小批量随机梯度下降（SGD）中的应用，发现增大批量可补偿反向传播中精度降低，量化权重和激活对梯度方差影响不同。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）训练资源消耗大，量化训练有噪声影响，SR虽理论上有优势，但与其他训练因素（如批量大小）的相互作用研究不足。

Method: 对小批量随机梯度下降（SGD）结合SR进行理论和实证研究。

Result: 实验验证了增大批量可补偿反向传播中精度降低，量化权重和激活对梯度方差有不同影响的理论观点。

Conclusion: 研究揭示了SR与批量大小等训练因素的关系，为量化训练提供了理论和实验依据。

Abstract: LLM training is resource-intensive. Quantized training improves computational
and memory efficiency but introduces quantization noise, which can hinder
convergence and degrade model accuracy. Stochastic Rounding (SR) has emerged as
a theoretically attractive alternative to deterministic rounding, offering
unbiased gradient estimates. However, its interaction with other training
factors -- especially batch size -- remains under explored. In this paper, we
present a theoretical and empirical study of mini-batch stochastic gradient
descent (SGD) with SR, showing that increased batch sizes can compensate for
reduced precision during back-propagation. Furthermore, we show that quantizing
weights and activations impacts gradient variance in distinct ways. Our
experiments validate these theoretical insights.

</details>


### [244] [KFCPO: Kronecker-Factored Approximated Constrained Policy Optimization](https://arxiv.org/abs/2511.00880)
*Joonyoung Lim,Younghwan Yoo*

Main category: cs.LG

TL;DR: 提出KFCPO算法，结合K - FAC二阶策略优化与安全感知梯度操作，实验表明其在安全与性能平衡上表现优越。


<details>
  <summary>Details</summary>
Motivation: 解决安全强化学习中奖励最大化和约束满足的权衡问题，实现高效稳定的策略优化。

Method: 结合K - FAC进行自然梯度更新，引入边际感知梯度操作机制，采用小批量级KL回滚策略。

Result: 在Safety Gymnasium上使用OmniSafe实验，KFCPO比遵守安全约束的最佳基线平均回报高10.3%到50.2%。

Conclusion: KFCPO在安全强化学习中能实现安全与性能的优越平衡。

Abstract: We propose KFCPO, a novel Safe Reinforcement Learning (Safe RL) algorithm
that combines scalable Kronecker-Factored Approximate Curvature (K-FAC) based
second-order policy optimization with safety-aware gradient manipulation. KFCPO
leverages K-FAC to perform efficient and stable natural gradient updates by
approximating the Fisher Information Matrix (FIM) in a layerwise, closed form
manner, avoiding iterative approximation overheads. To address the tradeoff
between reward maximization and constraint satisfaction, we introduce a margin
aware gradient manipulation mechanism that adaptively adjusts the influence of
reward and cost gradients based on the agent's proximity to safety boundaries.
This method blends gradients using a direction sensitive projection,
eliminating harmful interference and avoiding abrupt changes caused by fixed
hard thresholds. Additionally, a minibatch level KL rollback strategy is
adopted to ensure trust region compliance and to prevent destabilizing policy
shifts. Experiments on Safety Gymnasium using OmniSafe show that KFCPO achieves
10.3% to 50.2% higher average return across environments compared to the best
baseline that respected the safety constraint, demonstrating superior balance
of safety and performance.

</details>


### [245] [Learning with Category-Equivariant Representations for Human Activity Recognition](https://arxiv.org/abs/2511.00900)
*Yoshihiro Maruyama*

Main category: cs.LG

TL;DR: 提出类别对称感知学习框架用于人类活动识别，在UCI基准上提升了分布外准确率。


<details>
  <summary>Details</summary>
Motivation: 人类活动识别中传感器信号会随上下文、运动和环境变化，有效模型需保持稳定。

Method: 引入类别对称感知学习框架，将信号随时间、尺度和传感器层次的变化因素融入特征表示结构。

Result: 在UCI人类活动识别基准上，类别对称驱动设计使分布外准确率提高约46个百分点，是基线的约3.6倍。

Conclusion: 抽象对称原则可通过类别等变表示理论在日常传感任务中转化为实际性能提升。

Abstract: Human activity recognition is challenging because sensor signals shift with
context, motion, and environment; effective models must therefore remain stable
as the world around them changes. We introduce a categorical symmetry-aware
learning framework that captures how signals vary over time, scale, and sensor
hierarchy. We build these factors into the structure of feature
representations, yielding models that automatically preserve the relationships
between sensors and remain stable under realistic distortions such as time
shifts, amplitude drift, and device orientation changes. On the UCI Human
Activity Recognition benchmark, this categorical symmetry-driven design
improves out-of-distribution accuracy by approx. 46 percentage points (approx.
3.6x over the baseline), demonstrating that abstract symmetry principles can
translate into concrete performance gains in everyday sensing tasks via
category-equivariant representation theory.

</details>


### [246] [Transformers as Intrinsic Optimizers: Forward Inference through the Energy Principle](https://arxiv.org/abs/2511.00907)
*Ruifeng Ren,Sheng Ouyang,Huayi Tang,Yong Liu*

Main category: cs.LG

TL;DR: 本文从能量角度理解Transformer模型，提出统一能量框架，将标准和线性注意力纳入其中，扩展到多头设置，提出基于能量的注意力结构修改，实验初步验证框架潜力。


<details>
  <summary>Details</summary>
Motivation: Transformer机制待探索，能量视角对理解神经计算有价值，故从能量角度理解基于注意力的Transformer模型。

Method: 提出统一能量框架，包含全局能量、能量函数和梯度下降形式；将标准和线性注意力纳入框架；扩展到多头设置；基于经典梯度下降算法修改注意力结构。

Result: 标准softmax注意力可视为最小化Helmholtz自由能的特例；线性注意力可自然纳入框架；提出多种基于能量的新注意力结构。

Conclusion: 实验为基于能量的框架设计注意力机制的潜力提供了初步支持。

Abstract: Transformers have demonstrated strong adaptability across a wide range of
tasks and have become the backbone of modern Large Language Models (LLMs).
However, their underlying mechanisms remain open for further exploration. The
energy-based perspective has long provided a valuable principle for
understanding neural computation. In this paper, we revisit the principle of
energy as a lens to understand attention-based Transformer models. We present a
unified energy-based framework which is composed of three key components: the
global energy $F^*$, the energy function $E_i$ and the employed gradient
descent (GD) form. Within this framework, standard softmax attention can be
viewed as a special case of minimizing the Helmholtz free energy as $F^*$ using
standard GD when $E_i$ takes the form of elastic potential energy, with
residual connections ensuring that this optimization proceeds in an incremental
manner. In addition, linear attentions can also be naturally incorporated into
this framework by adjusting the corresponding energy forms. We also extend the
above analysis to the multi-head setting, where the energy is defined across
multiple low-dimensional subspaces. Building on this framework, we propose
energy-based modifications of attention structures. Inspired by classical GD
algorithms, we extend the original attention formulation based on standard GD
to the momentum-based GD, Nesterov Accelerated Gradient (NAG), and Newton's
method variants, each inducing a corresponding new attention structure. Our
experiments provide preliminary support for the potential of the energy-based
framework for designing attention mechanisms.

</details>


### [247] [Motion-Robust Multimodal Fusion of PPG and Accelerometer Signals for Three-Class Heart Rhythm Classification](https://arxiv.org/abs/2511.00949)
*Yangyang Zhao,Matti Kaisti,Olli Lahdenoja,Tero Koivisto*

Main category: cs.LG

TL;DR: 提出RhythmiNet模型，利用PPG和ACC信号进行三类心律分类，性能优于仅用PPG基线和基于手工HRV特征的逻辑回归模型。


<details>
  <summary>Details</summary>
Motivation: 房颤是中风和死亡的主要原因，现有单通道PPG方法易受噪声影响且只能进行二元房颤检测，无法涵盖更多心律失常类型。

Method: 引入RhythmiNet，结合PPG和ACC信号，用残差神经网络并增强时间和通道注意力模块进行三类心律分类，按加速度计运动强度百分位分层测试数据。

Result: RhythmiNet比仅用PPG基线的宏观AUC提高4.3%，性能比基于手工HRV特征的逻辑回归模型高12%。

Conclusion: 多模态融合和基于注意力的学习在嘈杂的真实临床数据中有益。

Abstract: Atrial fibrillation (AF) is a leading cause of stroke and mortality,
particularly in elderly patients. Wrist-worn photoplethysmography (PPG) enables
non-invasive, continuous rhythm monitoring, yet suffers from significant
vulnerability to motion artifacts and physiological noise. Many existing
approaches rely solely on single-channel PPG and are limited to binary AF
detection, often failing to capture the broader range of arrhythmias
encountered in clinical settings. We introduce RhythmiNet, a residual neural
network enhanced with temporal and channel attention modules that jointly
leverage PPG and accelerometer (ACC) signals. The model performs three-class
rhythm classification: AF, sinus rhythm (SR), and Other. To assess robustness
across varying movement conditions, test data are stratified by
accelerometer-based motion intensity percentiles without excluding any
segments. RhythmiNet achieved a 4.3% improvement in macro-AUC over the PPG-only
baseline. In addition, performance surpassed a logistic regression model based
on handcrafted HRV features by 12%, highlighting the benefit of multimodal
fusion and attention-based learning in noisy, real-world clinical data.

</details>


### [248] [Using Synthetic Data to estimate the True Error is theoretically and practically doable](https://arxiv.org/abs/2511.00964)
*Hai Hoang Thanh,Duy-Tung Nguyen,Hung The Tran,Khoat Than*

Main category: cs.LG

TL;DR: 论文在有限标签数据下，利用合成数据估计模型测试误差，提出新方法并证明其更准确可靠。


<details>
  <summary>Details</summary>
Motivation: 传统模型评估需大量标注测试集，获取成本高，因此探索利用合成数据在有限标签数据下评估模型。

Method: 提出考虑合成数据的泛化边界，以此为启发提出生成优化合成数据的理论方法。

Result: 在模拟和表格数据集实验中，相比现有基线，该方法对测试误差的估计更准确可靠。

Conclusion: 利用合成数据评估模型在有限标签数据场景可行，所提方法有效。

Abstract: Accurately evaluating model performance is crucial for deploying machine
learning systems in real-world applications. Traditional methods often require
a sufficiently large labeled test set to ensure a reliable evaluation. However,
in many contexts, a large labeled dataset is costly and labor-intensive.
Therefore, we sometimes have to do evaluation by a few labeled samples, which
is theoretically challenging. Recent advances in generative models offer a
promising alternative by enabling the synthesis of high-quality data. In this
work, we make a systematic investigation about the use of synthetic data to
estimate the test error of a trained model under limited labeled data
conditions. To this end, we develop novel generalization bounds that take
synthetic data into account. Those bounds suggest novel ways to optimize
synthetic samples for evaluation and theoretically reveal the significant role
of the generator's quality. Inspired by those bounds, we propose a
theoretically grounded method to generate optimized synthetic data for model
evaluation. Experimental results on simulation and tabular datasets demonstrate
that, compared to existing baselines, our method achieves accurate and more
reliable estimates of the test error.

</details>


### [249] [Modeling Microenvironment Trajectories on Spatial Transcriptomics with NicheFlow](https://arxiv.org/abs/2511.00977)
*Kristiyan Sakalyan,Alessandro Palma,Filippo Guerranti,Fabian J. Theis,Stephan Günnemann*

Main category: cs.LG

TL;DR: 提出NicheFlow模型推断细胞微环境时空轨迹，在多数据集验证有效


<details>
  <summary>Details</summary>
Motivation: 现有单细胞层面建模方法忽略组织中细胞状态的协同发展，需更好方法理解细胞微环境时空演变

Method: 引入NicheFlow，将局部细胞邻域表示为点云，用最优传输和变分流匹配联合建模细胞状态和空间坐标演变

Result: 在从胚胎到大脑发育的不同时空数据集上成功恢复全局空间结构和局部微环境组成

Conclusion: NicheFlow能有效推断细胞微环境的时间轨迹

Abstract: Understanding the evolution of cellular microenvironments in spatiotemporal
data is essential for deciphering tissue development and disease progression.
While experimental techniques like spatial transcriptomics now enable
high-resolution mapping of tissue organization across space and time, current
methods that model cellular evolution operate at the single-cell level,
overlooking the coordinated development of cellular states in a tissue. We
introduce NicheFlow, a flow-based generative model that infers the temporal
trajectory of cellular microenvironments across sequential spatial slides. By
representing local cell neighborhoods as point clouds, NicheFlow jointly models
the evolution of cell states and spatial coordinates using optimal transport
and Variational Flow Matching. Our approach successfully recovers both global
spatial architecture and local microenvironment composition across diverse
spatiotemporal datasets, from embryonic to brain development.

</details>


### [250] [Balanced Multimodal Learning via Mutual Information](https://arxiv.org/abs/2511.00987)
*Rongrong Xie,Guido Sanguinetti*

Main category: cs.LG

TL;DR: 本文提出新框架解决多模态学习中的模态不平衡问题，采用平衡策略提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 多模态学习中模态不平衡问题未得到充分解决，传统方法难以兼顾模态协同与冲突解决，在生物数据分析中尤为突出。

Method: 提出统一框架，利用互信息量化模态交互，采用跨模态知识蒸馏和多任务训练策略。

Result: 有效缓解模态不平衡，显著提升多模态模型整体性能。

Conclusion: 所提方法能有效解决多模态学习中的模态不平衡问题，提升模型性能。

Abstract: Multimodal learning has increasingly become a focal point in research,
primarily due to its ability to integrate complementary information from
diverse modalities. Nevertheless, modality imbalance, stemming from factors
such as insufficient data acquisition and disparities in data quality, has
often been inadequately addressed. This issue is particularly prominent in
biological data analysis, where datasets are frequently limited, costly to
acquire, and inherently heterogeneous in quality. Conventional multimodal
methodologies typically fall short in concurrently harnessing intermodal
synergies and effectively resolving modality conflicts.
  In this study, we propose a novel unified framework explicitly designed to
address modality imbalance by utilizing mutual information to quantify
interactions between modalities. Our approach adopts a balanced multimodal
learning strategy comprising two key stages: cross-modal knowledge distillation
(KD) and a multitask-like training paradigm. During the cross-modal KD
pretraining phase, stronger modalities are leveraged to enhance the predictive
capabilities of weaker modalities. Subsequently, our primary training phase
employs a multitask-like learning mechanism, dynamically calibrating gradient
contributions based on modality-specific performance metrics and intermodal
mutual information. This approach effectively alleviates modality imbalance,
thereby significantly improving overall multimodal model performance.

</details>


### [251] [Hydra: Dual Exponentiated Memory for Multivariate Time Series Analysis](https://arxiv.org/abs/2511.00989)
*Asal Meskin,Alireza Mirrokni,Ali Najar,Ali Behrouz*

Main category: cs.LG

TL;DR: 本文提出Hydra模块用于多元时间序列建模，采用二维循环，提出2D分块训练算法提升效率，实验显示其性能优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列模型存在缺乏时间归纳偏置、忽略变量间依赖、长序列建模效率低等问题，需新的解决方案。

Method: 提出Hydra模块，在时间和变量维度进行二维循环，设计2D分块训练算法近似实际循环。

Result: 在时间序列预测、分类和异常检测等任务和数据集上，Hydra性能优于现有基线。

Conclusion: Hydra模块在多元时间序列建模中表现出色，能有效解决现有模型的问题。

Abstract: In recent years, effectively modeling multivariate time series has gained
significant popularity, mainly due to its wide range of applications, ranging
from healthcare to financial markets and energy management. Transformers, MLPs,
and linear models as the de facto backbones of modern time series models have
shown promising results in single-variant and/or short-term forecasting. These
models, however: (1) are permutation equivariant and so lack temporal inductive
bias, being less expressive to capture the temporal dynamics; (2) are naturally
designed for univariate setup, missing the inter-dependencies of temporal and
variate dimensions; and/or (3) are inefficient for Long-term time series
modeling. To overcome training and inference efficiency as well as the lack of
temporal inductive bias, recently, linear Recurrent Neural Networks (RNNs) have
gained attention as an alternative to Transformer-based models. These models,
however, are inherently limited to a single sequence, missing inter-variate
dependencies, and can propagate errors due to their additive nature. In this
paper, we present Hydra, a by-design two-headed meta in-context memory module
that learns how to memorize patterns at test time by prioritizing time series
patterns that are more informative about the data. Hydra uses a 2-dimensional
recurrence across both time and variate at each step, which is more powerful
than mixing methods. Although the 2-dimensional nature of the model makes its
training recurrent and non-parallelizable, we present a new 2D-chunk-wise
training algorithm that approximates the actual recurrence with $\times 10$
efficiency improvement, while maintaining the effectiveness. Our experimental
results on a diverse set of tasks and datasets, including time series
forecasting, classification, and anomaly detection show the superior
performance of Hydra compared to state-of-the-art baselines.

</details>


### [252] [None To Optima in Few Shots: Bayesian Optimization with MDP Priors](https://arxiv.org/abs/2511.01006)
*Diantong Li,Kyunghyun Cho,Chong Liu*

Main category: cs.LG

TL;DR: 本文提出ProfBO算法，利用MDP先验和元学习，在少评估次数下解决黑盒优化问题，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯优化在评估成本高、耗时的实际应用中不实用，需要一种能在少评估次数下解决黑盒优化的方法。

Method: 引入ProfBO算法，使用MDP先验建模相关源任务的优化轨迹，将其嵌入先验拟合的神经网络，并采用模型无关元学习快速适应新目标任务。

Result: 在新冠、癌症基准测试和超参数调整任务实验中，ProfBO以显著更少的评估次数获得高质量解决方案，始终优于现有方法。

Conclusion: ProfBO算法可在少评估次数下有效解决黑盒优化问题，适合实际部署。

Abstract: Bayesian Optimization (BO) is an efficient tool for optimizing black-box
functions, but its theoretical guarantees typically hold in the asymptotic
regime. In many critical real-world applications such as drug discovery or
materials design, where each evaluation can be very costly and time-consuming,
BO becomes impractical for many evaluations. In this paper, we introduce the
Procedure-inFormed BO (ProfBO) algorithm, which solves black-box optimization
with remarkably few function evaluations. At the heart of our algorithmic
design are Markov Decision Process (MDP) priors that model optimization
trajectories from related source tasks, thereby capturing procedural knowledge
on efficient optimization. We embed these MDP priors into a prior-fitted neural
network and employ model-agnostic meta-learning for fast adaptation to new
target tasks. Experiments on real-world Covid and Cancer benchmarks and
hyperparameter tuning tasks demonstrate that ProfBO consistently outperforms
state-of-the-art methods by achieving high-quality solutions with significantly
fewer evaluations, making it ready for practical deployment.

</details>


### [253] [Equality Graph Assisted Symbolic Regression](https://arxiv.org/abs/2511.01009)
*Fabricio Olivetti de Franca,Gabriel Kronberger*

Main category: cs.LG

TL;DR: 本文提出用于符号回归的新搜索算法SymRegg，基于e - graph结构，能提高搜索效率并保持准确结果。


<details>
  <summary>Details</summary>
Motivation: 遗传编程在符号回归中虽准确，但搜索时需计算大量冗余表达式，可达总评估数的60%，因此需要提高搜索效率。

Method: 提出围绕e - graph结构的新搜索算法SymRegg，从e - graph中选择表达式进行扰动，若产生未访问表达式则插入e - graph并生成等价形式。

Result: SymRegg能提高搜索效率，在不同数据集上保持准确结果。

Conclusion: SymRegg能提高搜索效率，且只需选择最少的超参数集合。

Abstract: In Symbolic Regression (SR), Genetic Programming (GP) is a popular search
algorithm that delivers state-of-the-art results in term of accuracy. Its
success relies on the concept of neutrality, which induces large plateaus that
the search can safely navigate to more promising regions. Navigating these
plateaus, while necessary, requires the computation of redundant expressions,
up to 60% of the total number of evaluation, as noted in a recent study. The
equality graph (e-graph) structure can compactly store and group equivalent
expressions enabling us to verify if a given expression and their variations
were already visited by the search, thus enabling us to avoid unnecessary
computation. We propose a new search algorithm for symbolic regression called
SymRegg that revolves around the e-graph structure following simple steps:
perturb solutions sampled from a selection of expressions stored in the
e-graph, if it generates an unvisited expression, insert it into the e-graph
and generates its equivalent forms. We show that SymRegg is capable of
improving the efficiency of the search, maintaining consistently accurate
results across different datasets while requiring a choice of a minimalist set
of hyperparameters.

</details>


### [254] [What's the next frontier for Data-centric AI? Data Savvy Agents](https://arxiv.org/abs/2511.01015)
*Nabeel Seedat,Jiashuo Liu,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 当前AI代理发展迅速，但数据处理能力待探索，本文提出数据智能能力设计的四个关键能力，呼吁关注数据智能代理。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理在数据处理方面未充分探索，可扩展自主性要求代理具备数据处理能力，为确保可靠的实际部署，需重视数据智能能力。

Method: 提出四个关键能力，包括主动数据获取、复杂数据处理、交互式测试数据合成和持续适应。

Result: 提出了实现数据智能能力的四个关键能力。

Conclusion: 希望引发对数据智能代理作为以数据为中心的AI下一个前沿领域的思考。

Abstract: The recent surge in AI agents that autonomously communicate, collaborate with
humans and use diverse tools has unlocked promising opportunities in various
real-world settings. However, a vital aspect remains underexplored: how agents
handle data. Scalable autonomy demands agents that continuously acquire,
process, and evolve their data. In this paper, we argue that data-savvy
capabilities should be a top priority in the design of agentic systems to
ensure reliable real-world deployment. Specifically, we propose four key
capabilities to realize this vision: (1) Proactive data acquisition: enabling
agents to autonomously gather task-critical knowledge or solicit human input to
address data gaps; (2) Sophisticated data processing: requiring context-aware
and flexible handling of diverse data challenges and inputs; (3) Interactive
test data synthesis: shifting from static benchmarks to dynamically generated
interactive test data for agent evaluation; and (4) Continual adaptation:
empowering agents to iteratively refine their data and background knowledge to
adapt to shifting environments. While current agent research predominantly
emphasizes reasoning, we hope to inspire a reflection on the role of data-savvy
agents as the next frontier in data-centric AI.

</details>


### [255] [SARIMAX-Based Power Outage Prediction During Extreme Weather Events](https://arxiv.org/abs/2511.01017)
*Haoran Ye,Qiuzhuang Sun,Yang Yang*

Main category: cs.LG

TL;DR: 开发基于SARIMAX的极端天气短期停电预测系统，经特征工程和优化策略，比基线方法RMSE降低8.4%。


<details>
  <summary>Details</summary>
Motivation: 开发极端天气下短期停电预测系统，提高预测准确性。

Method: 采用两阶段特征工程，包括数据清洗和相关性过滤；添加多种特征作为SARIMAX外生输入；应用标准化和分层拟合策略，失败时降级或使用历史均值预测；分别优化短期和中期预测。

Result: 模型RMSE为177.2，比基线方法（RMSE = 193.4）提高8.4%。

Conclusion: 特征工程和鲁棒优化策略对极端天气停电预测有效。

Abstract: This study develops a SARIMAX-based prediction system for short-term power
outage forecasting during extreme weather events. Using hourly data from
Michigan counties with outage counts and comprehensive weather features, we
implement a systematic two-stage feature engineering pipeline: data cleaning to
remove zero-variance and unknown features, followed by correlation-based
filtering to eliminate highly correlated predictors. The selected features are
augmented with temporal embeddings, multi-scale lag features, and weather
variables with their corresponding lags as exogenous inputs to the SARIMAX
model. To address data irregularity and numerical instability, we apply
standardization and implement a hierarchical fitting strategy with sequential
optimization methods, automatic downgrading to ARIMA when convergence fails,
and historical mean-based fallback predictions as a final safeguard. The model
is optimized separately for short-term (24 hours) and medium-term (48 hours)
forecast horizons using RMSE as the evaluation metric. Our approach achieves an
RMSE of 177.2, representing an 8.4\% improvement over the baseline method (RMSE
= 193.4), thereby validating the effectiveness of our feature engineering and
robust optimization strategy for extreme weather-related outage prediction.

</details>


### [256] [MedEqualizer: A Framework Investigating Bias in Synthetic Medical Data and Mitigation via Augmentation](https://arxiv.org/abs/2511.01054)
*Sama Salarian,Yue Zhang,Swati Padhee,Srinivasan Parthasarathy*

Main category: cs.LG

TL;DR: 研究评估GAN模型生成合成医疗数据的公平性，发现不平衡问题，提出MedEqualizer框架改善了人口统计学平衡。


<details>
  <summary>Details</summary>
Motivation: 确保合成医疗数据在受保护属性上的公平性，避免临床研究和决策的偏差结果。

Method: 用对数差异指标评估GAN模型生成数据公平性，提出MedEqualizer框架在生成数据前丰富代表性不足的子组。

Result: MedEqualizer显著改善了合成数据集的人口统计学平衡。

Conclusion: MedEqualizer为更公平和有代表性的医疗数据合成提供了可行途径。

Abstract: Synthetic healthcare data generation presents a viable approach to enhance
data accessibility and support research by overcoming limitations associated
with real-world medical datasets. However, ensuring fairness across protected
attributes in synthetic data is critical to avoid biased or misleading results
in clinical research and decision-making. In this study, we assess the fairness
of synthetic data generated by multiple generative adversarial network
(GAN)-based models using the MIMIC-III dataset, with a focus on
representativeness across protected demographic attributes. We measure subgroup
representation using the logarithmic disparity metric and observe significant
imbalances, with many subgroups either underrepresented or overrepresented in
the synthetic data, compared to the real data. To mitigate these disparities,
we introduce MedEqualizer, a model-agnostic augmentation framework that
enriches the underrepresented subgroups prior to synthetic data generation. Our
results show that MedEqualizer significantly improves demographic balance in
the resulting synthetic datasets, offering a viable path towards more equitable
and representative healthcare data synthesis.

</details>


### [257] [Window-Based Feature Engineering for Cognitive Workload Detection](https://arxiv.org/abs/2511.01060)
*Andrew Hallam,R G Gayathri,Glory Lee,Atul Sajjanhar*

Main category: cs.LG

TL;DR: 研究使用COLET数据集，采用基于窗口的方法和机器学习/深度学习技术对认知工作量进行分类，结果显示深度学习模型表现更优，凸显基于窗口的特征提取和深度学习技术在实时评估中的潜力。


<details>
  <summary>Details</summary>
Motivation: 认知工作量在多个领域受关注，研究旨在对其进行分类。

Method: 使用COLET数据集，采用基于窗口的方法生成特征，利用机器学习和深度学习技术进行分类，通过窗口式时间划分增强现有研究中的特征。

Result: 深度学习模型，特别是表格架构，在精度、F1分数、准确率和分类精度上优于传统机器学习方法。

Conclusion: 基于窗口的时间特征提取有效，深度学习技术在复杂动态任务的实时认知工作量评估中有潜力。

Abstract: Cognitive workload is a topic of increasing interest across various fields
such as health, psychology, and defense applications. In this research, we
focus on classifying cognitive workload using the COLET dataset, employing a
window-based approach for feature generation and machine/deep learning
techniques for classification. We apply window-based temporal partitioning to
enhance features used in existing research, followed by machine learning and
deep learning models to classify different levels of cognitive workload. The
results demonstrate that deep learning models, particularly tabular
architectures, outperformed traditional machine learning methods in precision,
F1-score, accuracy, and classification precision. This study highlights the
effectiveness of window-based temporal feature extraction and the potential of
deep learning techniques for real-time cognitive workload assessment in complex
and dynamic tasks.

</details>


### [258] [Energy-Efficient Deep Learning Without Backpropagation: A Rigorous Evaluation of Forward-Only Algorithms](https://arxiv.org/abs/2511.01061)
*Przemysław Spyra,Witold Dzwinel*

Main category: cs.LG

TL;DR: 本文挑战BP对实现先进性能的必要性，提出无反向传播的MF算法，在分类精度上超BP，有显著效率提升，还重新评估无BP方法的内存效率，确立MF为MLP实用替代方案。


<details>
  <summary>Details</summary>
Motivation: 挑战长期以来认为BP对实现先进性能必不可少的假设，寻找更高效的替代算法。

Method: 提出MF算法，在相同架构和通用超参数优化的公平对比框架下，从FF到CaFo再到MF进行分析，还对无BP方法的内存效率进行实证评估。

Result: MF算法在分类精度上超过最优调优的BP基线，有显著效率提升，如能耗最多降低41%，训练速度最多加快34%，且发现无BP方法的实际开销可能抵消理论优势。

Conclusion: MF算法是MLP实用、高性能且可持续的BP替代方案。

Abstract: The long-held assumption that backpropagation (BP) is essential for
state-of-the-art performance is challenged by this work. We present rigorous,
hardware-validated evidence that the Mono-Forward (MF) algorithm, a
backpropagation-free method, consistently surpasses an optimally tuned BP
baseline in classification accuracy on its native Multi-Layer Perceptron (MLP)
architectures. This superior generalization is achieved with profound
efficiency gains, including up to 41% less energy consumption and up to 34%
faster training. Our analysis, which charts an evolutionary path from Geoffrey
Hinton's Forward-Forward (FF) to the Cascaded Forward (CaFo) and finally to MF,
is grounded in a fair comparative framework using identical architectures and
universal hyperparameter optimization. We further provide a critical
re-evaluation of memory efficiency in BP-free methods, empirically
demonstrating that practical overhead can offset theoretical gains. Ultimately,
this work establishes MF as a practical, high-performance, and sustainable
alternative to BP for MLPs.

</details>


### [259] [Continual Learning, Not Training: Online Adaptation For Agents](https://arxiv.org/abs/2511.01093)
*Aman Jaglan,Jarrod Barnes*

Main category: cs.LG

TL;DR: 提出自适应教学与学习系统ATLAS，实现无梯度持续学习，在基准测试中表现优异，证明无梯度持续学习是可行路径。


<details>
  <summary>Details</summary>
Motivation: 传统持续学习方法基于梯度再训练，不适合实时适应的部署代理，需要新方法。

Method: 引入ATLAS双代理架构，解耦推理和执行，结合持久学习记忆，通过推理时编排实现无梯度持续学习。

Result: 在ExCyTIn - Bench上，ATLAS用GPT - 5 - mini成功率达54.1%，优于GPT - 5 (High) 13%，降低成本86%；跨事件验证有泛化能力。

Conclusion: 无梯度持续学习是实现自适应、可部署AI系统的可行路径，提供的因果注释轨迹对训练显式世界模型有价值。

Abstract: Continual Learning (CL) methods have traditionally focused on mitigating
catastrophic forgetting through gradient-based retraining, an approach
ill-suited for deployed agents that must adapt in real time. We introduce our
Adaptive Teaching and Learning System (ATLAS), a dual-agent architecture that
decouples reasoning (Teacher) from execution (Student) and incorporates a
persistent learning memory that stores distilled guidance from experience. This
informs the orchestration layer, enabling the system to dynamically adjust its
operational strategies, such as supervision level or initial plan selection, at
inference time. In doing so, ATLAS achieves gradient-free continual learning,
shifting the locus of adaptation from model parameters to system-level
orchestration. We formulate this as a system-centric paradigm for continual
learning, where the objective is adaptive efficiency: maximizing task success
while minimizing computational cost through inference-time orchestration rather
than parameter updates. Evaluated on Microsoft's ExCyTIn-Bench, an open-source
benchmark simulating complex cyberthreat investigation, ATLAS achieves 54.1%
success with GPT-5-mini as its Student, outperforming the larger GPT-5 (High)
by 13% while reducing cost by 86%. Cross-incident validation demonstrates
generalization: frozen pamphlets from Incident #5 improve accuracy from 28% to
41% with zero retraining, while shifting output composition from verbose
exploration to structured reasoning. Together, these findings establish
gradient-free continual learning as a viable path toward adaptive, deployable
AI systems and provide causally annotated traces valuable for training explicit
world models.

</details>


### [260] [Stochastic Regret Guarantees for Online Zeroth- and First-Order Bilevel Optimization](https://arxiv.org/abs/2511.01126)
*Parvin Nazari,Bojian Hou,Davoud Ataee Tarzanagh,Li Shen,George Michailidis*

Main category: cs.LG

TL;DR: 提出新搜索方向，使随机OBO算法在无窗口平滑下实现次线性随机双层遗憾，框架提高效率，实验验证方法有效性。


<details>
  <summary>Details</summary>
Motivation: 现有在线双层优化（OBO）方法依赖确定性窗口平滑遗憾最小化，在函数快速变化时不能准确反映系统性能。

Method: 引入新搜索方向，构建一阶和零阶（ZO）随机OBO算法，减少超梯度估计的神谕依赖，更新内外变量并采用ZO估计。

Result: 算法在无窗口平滑下实现次线性随机双层遗憾，框架提高了效率。

Conclusion: 通过在线参数损失调整和黑盒对抗攻击实验验证了方法的有效性。

Abstract: Online bilevel optimization (OBO) is a powerful framework for machine
learning problems where both outer and inner objectives evolve over time,
requiring dynamic updates. Current OBO approaches rely on deterministic
\textit{window-smoothed} regret minimization, which may not accurately reflect
system performance when functions change rapidly. In this work, we introduce a
novel search direction and show that both first- and zeroth-order (ZO)
stochastic OBO algorithms leveraging this direction achieve sublinear
{stochastic bilevel regret without window smoothing}. Beyond these guarantees,
our framework enhances efficiency by: (i) reducing oracle dependence in
hypergradient estimation, (ii) updating inner and outer variables alongside the
linear system solution, and (iii) employing ZO-based estimation of Hessians,
Jacobians, and gradients. Experiments on online parametric loss tuning and
black-box adversarial attacks validate our approach.

</details>


### [261] [Adapt under Attack and Domain Shift: Unified Adversarial Meta-Learning and Domain Adaptation for Robust Automatic Modulation Classification](https://arxiv.org/abs/2511.01172)
*Ali Owfi,Amirmohammad Bamdad,Tolunay Seyfi,Fatemeh Afghah*

Main category: cs.LG

TL;DR: 提出结合元学习与域适应的统一框架，解决深度学习自动调制分类（AMC）系统在现实环境中的问题，提升分类准确率。


<details>
  <summary>Details</summary>
Motivation: 深度学习AMC方法易受对抗攻击和数据分布变化影响，阻碍其在现实动态环境中部署，需解决这些威胁。

Method: 采用两阶段策略，离线阶段用元学习在单一源域的干净和对抗扰动样本上训练模型；在线阶段用域适应使模型特征与新目标域对齐。

Result: 框架在应对综合威胁时，显著提高调制分类准确率。

Conclusion: 该框架为现代AMC系统的部署和运行挑战提供关键解决方案。

Abstract: Deep learning has emerged as a leading approach for Automatic Modulation
Classification (AMC), demonstrating superior performance over traditional
methods. However, vulnerability to adversarial attacks and susceptibility to
data distribution shifts hinder their practical deployment in real-world,
dynamic environments. To address these threats, we propose a novel, unified
framework that integrates meta-learning with domain adaptation, making AMC
systems resistant to both adversarial attacks and environmental changes. Our
framework utilizes a two-phase strategy. First, in an offline phase, we employ
a meta-learning approach to train the model on clean and adversarially
perturbed samples from a single source domain. This method enables the model to
generalize its defense, making it resistant to a combination of previously
unseen attacks. Subsequently, in the online phase, we apply domain adaptation
to align the model's features with a new target domain, allowing it to adapt
without requiring substantial labeled data. As a result, our framework achieves
a significant improvement in modulation classification accuracy against these
combined threats, offering a critical solution to the deployment and
operational challenges of modern AMC systems.

</details>


### [262] [A Comparative Study of Model Adaptation Strategies for Multi-Treatment Uplift Modeling](https://arxiv.org/abs/2511.01185)
*Ruyue Zhang,Xiaopeng Ke,Ming Liu,Fangzhou Shi,Chang Men,Zhengdan Zhu*

Main category: cs.LG

TL;DR: 本文研究多处理场景下的提升建模，将现有模型适配方法分类，指出其不足，提出正交函数适配（OFA）方法，实验表明OFA能显著提升模型性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前多处理提升建模技术多改编自二元处理工作，现有模型适配方法在不同数据特征下效果不佳，需提升估计能力和鲁棒性。

Method: 将现有模型改编分为结构适配和特征适配两类，提出基于函数逼近定理的正交函数适配（OFA）方法，并进行多数据特征的综合实验。

Result: 实验结果表明，提出的OFA相比其他普通适配方法能显著提高提升模型性能，且具有最高的鲁棒性。

Conclusion: OFA方法在多处理提升建模中能有效提升模型性能和鲁棒性，优于现有适配方法。

Abstract: Uplift modeling has emerged as a crucial technique for individualized
treatment effect estimation, particularly in fields such as marketing and
healthcare. Modeling uplift effects in multi-treatment scenarios plays a key
role in real-world applications. Current techniques for modeling
multi-treatment uplift are typically adapted from binary-treatment works. In
this paper, we investigate and categorize all current model adaptations into
two types: Structure Adaptation and Feature Adaptation. Through our empirical
experiments, we find that these two adaptation types cannot maintain
effectiveness under various data characteristics (noisy data, mixed with
observational data, etc.). To enhance estimation ability and robustness, we
propose Orthogonal Function Adaptation (OFA) based on the function
approximation theorem. We conduct comprehensive experiments with multiple data
characteristics to study the effectiveness and robustness of all model
adaptation techniques. Our experimental results demonstrate that our proposed
OFA can significantly improve uplift model performance compared to other
vanilla adaptation methods and exhibits the highest robustness.

</details>


### [263] [Transmitter Identification and Protocol Categorization in Shared Spectrum via Multi-Task RF Classification at the Network Edge](https://arxiv.org/abs/2511.01198)
*Tariq Abdul-Quddoos,Tasnia Sharmin,Xiangfang Li,Lijun Qian*

Main category: cs.LG

TL;DR: 提出共享频谱环境下通过多任务射频信号分类进行发射机识别和协议分类的框架，用CNN解决难题，取得高准确率，有提升频谱监测等潜力。


<details>
  <summary>Details</summary>
Motivation: 频谱共享对满足未来无线需求至关重要，频谱监测和发射机识别对执行频谱使用政策、高效利用频谱和网络安全不可或缺。

Method: 设计卷积神经网络（CNN），采用多通道输入策略提取信号特征。

Result: 协议分类准确率90%，发射基站分类准确率100%，联合分类任务准确率92%。

Conclusion: 该方法在现代无线网络中对提升频谱监测、管理和安全有巨大潜力。

Abstract: As spectrum sharing becomes increasingly vital to meet rising wireless
demands in the future, spectrum monitoring and transmitter identification are
indispensable for enforcing spectrum usage policy, efficient spectrum
utilization, and net- work security. This study proposed a robust framework for
transmitter identification and protocol categorization via multi- task RF
signal classification in shared spectrum environments, where the spectrum
monitor will classify transmission protocols (e.g., 4G LTE, 5G-NR, IEEE
802.11a) operating within the same frequency bands, and identify different
transmitting base stations, as well as their combinations. A Convolutional
Neural Network (CNN) is designed to tackle critical challenges such as
overlapping signal characteristics and environmental variability. The proposed
method employs a multi-channel input strategy to extract meaningful signal
features, achieving remarkable accuracy: 90% for protocol classification, 100%
for transmitting base station classification, and 92% for joint classification
tasks, utilizing RF data from the POWDER platform. These results highlight the
significant potential of the proposed method to enhance spectrum monitoring,
management, and security in modern wireless networks.

</details>


### [264] [FEval-TTC: Fair Evaluation Protocol for Test-Time Compute](https://arxiv.org/abs/2511.01203)
*Pavel Rumiantsev,Soumyasundar Pal,Yingxue Zhang,Mark Coates*

Main category: cs.LG

TL;DR: 提出FEval - TTC协议评估测试时计算方法，支持多模型和多数据集评估，开源代码。


<details>
  <summary>Details</summary>
Motivation: 大语言模型性能和API调用成本随时间波动，可能使先前研究结论失效，需一致评估方法。

Method: 提出FEval - TTC协议，标准化少样本提示和答案提取过程，给出成本建模程序。

Result: 可在多数据集上对多模型评估TTC方法，标准化流程减少时间和金钱开销，能估算成本。

Conclusion: FEval - TTC可实现对TTC方法的一致评估，已开源供公众使用。

Abstract: The performance of Large Language Models (LLMs) and the associated dollar
costs of API calls can fluctuate over time, potentially invalidating
conclusions drawn in prior research. To address this, we propose a Fair
Evaluation protocol for Test-Time Compute (FEval-TTC), designed to ensure
consistent assessment of test-time compute (TTC) methods, regardless of such
fluctuations. FEval-TTC focuses on the evaluation of TTC methods that utilize
underlying Chains-of-Thought (CoT). It supports evaluations across multiple
LLMs on a diverse set of mathematical and commonsense reasoning datasets. The
few-shot prompting and answer extraction processes are standardized across
datasets, reducing both time and monetary overhead for researchers.
Furthermore, we provide a cost modelling procedure that estimates both the
token and dollar cost per query, facilitating equitable comparisons of
prevalent TTC methods. We open-source FEval-TTC for public use at
https://github.com/networkslab/feval_ttc .

</details>


### [265] [Optimizing Electric Vehicle Charging Station Placement Using Reinforcement Learning and Agent-Based Simulations](https://arxiv.org/abs/2511.01218)
*Minh-Duc Nguyen,Dung D. Le,Phi Long Nguyen*

Main category: cs.LG

TL;DR: 本文提出集成深度强化学习与基于代理模拟的框架解决充电站选址问题，案例显示可减少平均等待时间，提升用户体验。


<details>
  <summary>Details</summary>
Motivation: 电动汽车快速增长，需战略布局充电站，但现有强化学习方法因确定性奖励系统存在效率问题，无法反映现实场景。

Method: 提出集成深度强化学习与基于代理模拟的框架，采用带双Q网络的混合强化学习代理，结合确定性因素与模拟反馈的混合奖励函数来选址和配置充电端口。

Result: 在越南河内的案例研究中，该方法较初始状态平均等待时间减少53.28%，优于静态基线方法。

Conclusion: 该可扩展和自适应的解决方案能增强电动汽车基础设施规划，有效应对现实复杂性，提升用户体验。

Abstract: The rapid growth of electric vehicles (EVs) necessitates the strategic
placement of charging stations to optimize resource utilization and minimize
user inconvenience. Reinforcement learning (RL) offers an innovative approach
to identifying optimal charging station locations; however, existing methods
face challenges due to their deterministic reward systems, which limit
efficiency. Because real-world conditions are dynamic and uncertain, a
deterministic reward structure cannot fully capture the complexities of
charging station placement. As a result, evaluation becomes costly and
time-consuming, and less reflective of real-world scenarios. To address this
challenge, we propose a novel framework that integrates deep RL with
agent-based simulations to model EV movement and estimate charging demand in
real time. Our approach employs a hybrid RL agent with dual Q-networks to
select optimal locations and configure charging ports, guided by a hybrid
reward function that combines deterministic factors with simulation-derived
feedback. Case studies in Hanoi, Vietnam, show that our method reduces average
waiting times by 53.28% compared to the initial state, outperforming static
baseline methods. This scalable and adaptive solution enhances EV
infrastructure planning, effectively addressing real-world complexities and
improving user experience.

</details>


### [266] [WindMiL: Equivariant Graph Learning for Wind Loading Prediction](https://arxiv.org/abs/2511.01226)
*Themistoklis Vargiemezis,Charilaos Kanatsoulis,Catherine Gorlé*

Main category: cs.LG

TL;DR: 提出新机器学习框架WindMiL，结合系统数据集生成与对称感知图神经网络，能高效、可扩展且准确预测建筑风荷载。


<details>
  <summary>Details</summary>
Motivation: 传统风荷载预测方法如wind tunnel testing和LES成本高、计算时间长，难以进行大规模探索和参数研究。

Method: 先通过有符号距离函数插值生成低层建筑风荷载大规模数据集，再开发反射等变图神经网络。

Result: WindMiL在插值和外推评估中对表面压力系数均值和标准差预测精度高，反射测试评估中准确率超96%，非等变基线模型下降超10%。

Conclusion: WindMiL通过系统数据集和等变替代模型，可实现对建筑风荷载的高效、可扩展且准确的预测。

Abstract: Accurate prediction of wind loading on buildings is crucial for structural
safety and sustainable design, yet conventional approaches such as wind tunnel
testing and large-eddy simulation (LES) are prohibitively expensive for
large-scale exploration. Each LES case typically requires at least 24 hours of
computation, making comprehensive parametric studies infeasible. We introduce
WindMiL, a new machine learning framework that combines systematic dataset
generation with symmetry-aware graph neural networks (GNNs). First, we
introduce a large-scale dataset of wind loads on low-rise buildings by applying
signed distance function interpolation to roof geometries and simulating 462
cases with LES across varying shapes and wind directions. Second, we develop a
reflection-equivariant GNN that guarantees physically consistent predictions
under mirrored geometries. Across interpolation and extrapolation evaluations,
WindMiL achieves high accuracy for both the mean and the standard deviation of
surface pressure coefficients (e.g., RMSE $\leq 0.02$ for mean $C_p$) and
remains accurate under reflected-test evaluation, maintaining hit rates above
$96\%$ where the non-equivariant baseline model drops by more than $10\%$. By
pairing a systematic dataset with an equivariant surrogate, WindMiL enables
efficient, scalable, and accurate predictions of wind loads on buildings.

</details>


### [267] [KAT-GNN: A Knowledge-Augmented Temporal Graph Neural Network for Risk Prediction in Electronic Health Records](https://arxiv.org/abs/2511.01249)
*Kun-Wei Lin,Yu-Chen Kuo,Hsin-Yao Wang,Yi-Ju Tseng*

Main category: cs.LG

TL;DR: 提出KAT - GNN框架用于电子健康记录风险预测，在多数据集和任务中表现出色，证明结合临床知识和时间感知机制有效。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录临床风险预测重要，但建模异构和不规则时间数据有挑战。

Method: 构建KAT - GNN框架，从EHR构建特定模态患者图，用SNOMED CT和EHR共现先验增强，用时间感知Transformer捕捉纵向动态。

Result: 在CAD预测和住院死亡率预测中达到SOTA性能，持续超越GRASP和RETAIN等基线模型。

Conclusion: 将临床知识融入图表示并结合时间感知注意力机制，是跨临床任务和数据集进行风险预测的有效且通用方法。

Abstract: Clinical risk prediction using electronic health records (EHRs) is vital to
facilitate timely interventions and clinical decision support. However,
modeling heterogeneous and irregular temporal EHR data presents significant
challenges. We propose \textbf{KAT-GNN} (Knowledge-Augmented Temporal Graph
Neural Network), a graph-based framework that integrates clinical knowledge and
temporal dynamics for risk prediction. KAT-GNN first constructs
modality-specific patient graphs from EHRs. These graphs are then augmented
using two knowledge sources: (1) ontology-driven edges derived from SNOMED CT
and (2) co-occurrence priors extracted from EHRs. Subsequently, a time-aware
transformer is employed to capture longitudinal dynamics from the graph-encoded
patient representations. KAT-GNN is evaluated on three distinct datasets and
tasks: coronary artery disease (CAD) prediction using the Chang Gung Research
Database (CGRD) and in-hospital mortality prediction using the MIMIC-III and
MIMIC-IV datasets. KAT-GNN achieves state-of-the-art performance in CAD
prediction (AUROC: 0.9269 $\pm$ 0.0029) and demonstrated strong results in
mortality prediction in MIMIC-III (AUROC: 0.9230 $\pm$ 0.0070) and MIMIC-IV
(AUROC: 0.8849 $\pm$ 0.0089), consistently outperforming established baselines
such as GRASP and RETAIN. Ablation studies confirm that both knowledge-based
augmentation and the temporal modeling component are significant contributors
to performance gains. These findings demonstrate that the integration of
clinical knowledge into graph representations, coupled with a time-aware
attention mechanism, provides an effective and generalizable approach for risk
prediction across diverse clinical tasks and datasets.

</details>


### [268] [Adversarial Spatio-Temporal Attention Networks for Epileptic Seizure Forecasting](https://arxiv.org/abs/2511.01275)
*Zan Li,Kyongmin Yeo,Wesley Gifford,Lara Marcuse,Madeline Fields,Bülent Yener*

Main category: cs.LG

TL;DR: 提出STAN网络用于从多变量EEG信号预测癫痫发作，在两个数据集上达SOTA性能，且适用于其他时空预测场景。


<details>
  <summary>Details</summary>
Motivation: 癫痫发作预测在医疗时间序列预测中是关键挑战，需高灵敏度、低误报率和特定主体适应性。

Method: 提出STAN，通过级联注意力块联合建模空间大脑连接和时间神经动力学，采用带梯度惩罚的对抗训练。

Result: 在两个基准EEG数据集上分别实现96.6%灵敏度（每小时0.011次误检）和94.2%灵敏度（每小时0.063次误检），计算高效。

Conclusion: 该框架为医疗和其他时间序列领域的时空预测提供通用范式。

Abstract: Forecasting epileptic seizures from multivariate EEG signals represents a
critical challenge in healthcare time series prediction, requiring high
sensitivity, low false alarm rates, and subject-specific adaptability. We
present STAN, an Adversarial Spatio-Temporal Attention Network that jointly
models spatial brain connectivity and temporal neural dynamics through cascaded
attention blocks with alternating spatial and temporal modules. Unlike existing
approaches that assume fixed preictal durations or separately process spatial
and temporal features, STAN captures bidirectional dependencies between spatial
and temporal patterns through a unified cascaded architecture. Adversarial
training with gradient penalty enables robust discrimination between interictal
and preictal states learned from clearly defined 15-minute preictal windows.
Continuous 90-minute pre-seizure monitoring reveals that the learned
spatio-temporal attention patterns enable early detection: reliable alarms
trigger at subject-specific times (typically 15-45 minutes before onset),
reflecting the model's capacity to capture subtle preictal dynamics without
requiring individualized training. Experiments on two benchmark EEG datasets
(CHB-MIT scalp: 8 subjects, 46 events; MSSM intracranial: 4 subjects, 14
events) demonstrate state-of-the-art performance: 96.6% sensitivity with 0.011
false detections per hour and 94.2% sensitivity with 0.063 false detections per
hour, respectively, while maintaining computational efficiency (2.3M
parameters, 45 ms latency, 180 MB memory) for real-time edge deployment. Beyond
epilepsy, the proposed framework provides a general paradigm for
spatio-temporal forecasting in healthcare and other time series domains where
individual heterogeneity and interpretability are crucial.

</details>


### [269] [Identification of Capture Phases in Nanopore Protein Sequencing Data Using a Deep Learning Model](https://arxiv.org/abs/2511.01277)
*Annabelle Martin,Daphne Kontogiorgos-Heintz,Jeff Nivala*

Main category: cs.LG

TL;DR: 开发轻量级1D CNN模型CaptureNet - Deep检测纳米孔蛋白测序捕获阶段，表现优异，大幅缩短分析时间。


<details>
  <summary>Details</summary>
Motivation: 手动识别纳米孔蛋白测序的捕获阶段耗时久，需领域专业知识解读复杂信号模式。

Method: 开发并训练轻量级一维卷积神经网络（1D CNN），在降采样信号窗口中检测捕获阶段，与CNN - LSTM混合模型、基于直方图的分类器等对比。

Result: 最佳模型CaptureNet - Deep在测试数据上F1分数达0.94，精度93.39%，支持低延迟推理，集成到实验仪表盘后将分析时间从数天减至不到30分钟。

Conclusion: 使用简单可解释架构可实现高效实时捕获检测，轻量级机器学习模型在测序工作流程中有更广泛应用。

Abstract: Nanopore protein sequencing produces long, noisy ionic current traces in
which key molecular phases, such as protein capture and translocation, are
embedded. Capture phases mark the successful entry of a protein into the pore
and serve as both a checkpoint and a signal that a channel merits further
analysis. However, manual identification of capture phases is time-intensive,
often requiring several days for expert reviewers to annotate the data due to
the need for domain-specific interpretation of complex signal patterns. To
address this, a lightweight one-dimensional convolutional neural network (1D
CNN) was developed and trained to detect capture phases in down-sampled signal
windows. Evaluated against CNN-LSTM (Long Short-Term Memory) hybrids,
histogram-based classifiers, and other CNN variants using run-level data
splits, our best model, CaptureNet-Deep, achieved an F1 score of 0.94 and
precision of 93.39% on held-out test data. The model supports low-latency
inference and is integrated into a dashboard for Oxford Nanopore experiments,
reducing the total analysis time from several days to under thirty minutes.
These results show that efficient, real-time capture detection is possible
using simple, interpretable architectures and suggest a broader role for
lightweight ML models in sequencing workflows.

</details>


### [270] [Lyapunov Stability Learning with Nonlinear Control via Inductive Biases](https://arxiv.org/abs/2511.01283)
*Yupu Lu,Shijie Lin,Hao Xu,Zeqing Zhang,Jia Pan*

Main category: cs.LG

TL;DR: 文章针对现有学习验证框架在寻找控制李雅普诺夫函数（CLF）时的不足，将李雅普诺夫条件作为归纳偏置设计神经CLF和基于CLF的控制器，实验显示有更好效果并揭示旧方法成功率下降原因。


<details>
  <summary>Details</summary>
Motivation: 现有学习验证框架中，学习者将李雅普诺夫条件作为复杂优化约束，难以全局收敛，验证也复杂，需改进该框架。

Method: 将李雅普诺夫条件作为归纳偏置，设计神经CLF和基于CLF的控制器，进行端到端学习。

Result: 在大量实验中，与现有方法相比，学习CLF时收敛率更高、吸引域更大。

Conclusion: 新方法能优化寻找CLF的过程，解决旧方法的问题，有更好效果。

Abstract: Finding a control Lyapunov function (CLF) in a dynamical system with a
controller is an effective way to guarantee stability, which is a crucial issue
in safety-concerned applications. Recently, deep learning models representing
CLFs have been applied into a learner-verifier framework to identify
satisfiable candidates. However, the learner treats Lyapunov conditions as
complex constraints for optimisation, which is hard to achieve global
convergence. It is also too complicated to implement these Lyapunov conditions
for verification. To improve this framework, we treat Lyapunov conditions as
inductive biases and design a neural CLF and a CLF-based controller guided by
this knowledge. This design enables a stable optimisation process with limited
constraints, and allows end-to-end learning of both the CLF and the controller.
Our approach achieves a higher convergence rate and larger region of attraction
(ROA) in learning the CLF compared to existing methods among abundant
experiment cases. We also thoroughly reveal why the success rate decreases with
previous methods during learning.

</details>


### [271] [Koopman-based Prediction of Connectivity for Flying Ad Hoc Networks](https://arxiv.org/abs/2511.01286)
*Sivaram Krishnan,Jinho Choi,Jihong Park,Gregory Sherman,Benjamin Campbell*

Main category: cs.LG

TL;DR: 本文探索数据驱动的Koopman方法解决飞自组网（FANETs）中机器学习技术应用挑战，提出集中和分布式方法，结果显示能准确预测连通性和隔离事件。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习技术在静态无线环境应用多，在FANET等高度动态环境有局限，需新方法解决挑战。

Method: 利用Koopman算子理论，提出集中和分布式两种方法，考虑UAV按预定轨迹执行监视任务的FANET，预测信干噪比。

Result: 所提方法能准确预测导致通信中断的连通性和隔离事件。

Conclusion: 该能力可帮助UAV根据预测安排传输。

Abstract: The application of machine learning (ML) to communication systems is expected
to play a pivotal role in future artificial intelligence (AI)-based
next-generation wireless networks. While most existing works focus on ML
techniques for static wireless environments, they often face limitations when
applied to highly dynamic environments, such as flying ad hoc networks
(FANETs). This paper explores the use of data-driven Koopman approaches to
address these challenges. Specifically, we investigate how these approaches can
model UAV trajectory dynamics within FANETs, enabling more accurate predictions
and improved network performance. By leveraging Koopman operator theory, we
propose two possible approaches -- centralized and distributed -- to
efficiently address the challenges posed by the constantly changing topology of
FANETs. To demonstrate this, we consider a FANET performing surveillance with
UAVs following pre-determined trajectories and predict
signal-to-interference-plus-noise ratios (SINRs) to ensure reliable
communication between UAVs. Our results show that these approaches can
accurately predict connectivity and isolation events that lead to modelled
communication outages. This capability could help UAVs schedule their
transmissions based on these predictions.

</details>


### [272] [LSHFed: Robust and Communication-Efficient Federated Learning with Locally-Sensitive Hashing Gradient Mapping](https://arxiv.org/abs/2511.01296)
*Guanjie Cheng,Mengzhen Yang,Xinkui Zhao,Shuyi Yu,Tianyu Du,Yangyang Wu,Mengying Zhu,Shuiguang Deng*

Main category: cs.LG

TL;DR: 提出LSHFed框架增强联邦学习鲁棒性与隐私保护，降低通信成本，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在信任不足环境易受推理和中毒攻击，现有防御方法存在成本高或检测精度有限问题。

Method: 提出LSHFed框架，核心是LSHGM梯度验证机制，通过多超平面局部敏感哈希将高维梯度投影为紧凑二进制表示。

Result: 即使50%参与者为恶意对手，LSHFed仍保持高模型性能，梯度验证通信相比全梯度方法最多降低1000倍。

Conclusion: LSHFed能同时增强聚合鲁棒性和隐私保护，且通信高效。

Abstract: Federated learning (FL) enables collaborative model training across
distributed nodes without exposing raw data, but its decentralized nature makes
it vulnerable in trust-deficient environments. Inference attacks may recover
sensitive information from gradient updates, while poisoning attacks can
degrade model performance or induce malicious behaviors. Existing defenses
often suffer from high communication and computation costs, or limited
detection precision. To address these issues, we propose LSHFed, a robust and
communication-efficient FL framework that simultaneously enhances aggregation
robustness and privacy preservation. At its core, LSHFed incorporates LSHGM, a
novel gradient verification mechanism that projects high-dimensional gradients
into compact binary representations via multi-hyperplane locally-sensitive
hashing. This enables accurate detection and filtering of malicious gradients
using only their irreversible hash forms, thus mitigating privacy leakage risks
and substantially reducing transmission overhead. Extensive experiments
demonstrate that LSHFed maintains high model performance even when up to 50% of
participants are collusive adversaries while achieving up to a 1000x reduction
in gradient verification communication compared to full-gradient methods.

</details>


### [273] [Diffusion-Based Solver for CNF Placement on the Cloud-Continuum](https://arxiv.org/abs/2511.01343)
*Álvaro Vázquez Rodríguez,Manuel Fernández-Veiga,Carlos Giraldo-Rodríguez*

Main category: cs.LG

TL;DR: 提出基于DDPM的CNF放置理论框架，经评估比MINLP求解器推理快，展示了扩散生成模型在受限网络嵌入问题中的潜力。


<details>
  <summary>Details</summary>
Motivation: 经典方法在CNF跨云连续体放置的可扩展性、约束处理和泛化能力上有限，需新方法。

Method: 将放置问题重构为生成图分配任务，编码为异构图，训练图神经网络去噪器，将特定约束损失融入损失函数。

Result: 模型能持续产生可行解，推理速度比MINLP求解器快几个数量级。

Conclusion: 扩散生成模型在受限网络嵌入问题有潜力，有助于分布式CNF的实用、可扩展编排。

Abstract: The placement of Cloud-Native Network Functions (CNFs) across the
Cloud-Continuum represents a core challenge in the orchestration of current 5G
and future 6G networks. The process involves the placement of interdependent
computing tasks, structured as Service Function Chains, over distributed cloud
infrastructures. This is achieved while satisfying strict resource, bandwidth
and latency constraints. It is acknowledged that classical approaches,
including mixed-integer nonlinear programming, heuristics and reinforcement
learning are limited in terms of scalability, constraint handling and
generalisation capacity. In the present study, a novel theoretical framework is
proposed, which is based on Denoising Diffusion Probabilistic Models (DDPM) for
CNF placement. The present approach proposes a reconceptualisation of placement
as a generative graph to assignment task, where the placement problem is
encoded as a heterogeneous graph, and a Graph Neural Network denoiser is
trained to iteratively refine noisy CNF-to-cloud assignment matrices. The model
incorporates constraint-specific losses directly into the loss function,
thereby allowing it to learn feasible solution spaces. The integration of the
DDPM formulation with structured combinatorial constraints is achieved through
a rigorous and systematic approach. Extensive evaluations across diverse
topologies have been conducted, which have confirmed that the model
consistently produces feasible solutions with orders of magnitude faster
inference than MINLP solvers. The results obtained demonstrate the potential of
diffusion-based generative modelling for constrained network embedding
problems, making an impact towards the practical, scalable orchestration of
distributed Cloud-Native Network Functions.

</details>


### [274] [MiniFool - Physics-Constraint-Aware Minimizer-Based Adversarial Attacks in Deep Neural Networks](https://arxiv.org/abs/2511.01352)
*Lucie Flek,Oliver Janik,Philipp Alexander Jung,Akbar Karimi,Timo Saala,Alexander Schmidt,Matthias Schott,Philipp Soldin,Matthias Thiesmeyer,Christopher Wiebusch,Ulrich Willemsen*

Main category: cs.LG

TL;DR: 提出MiniFool算法用于粒子和天体粒子物理中神经网络分类任务的对抗攻击测试，展示其通用性并分析分类翻转情况和网络决策鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 开发适用于粒子和天体粒子物理中神经网络分类任务的对抗攻击测试算法，且具有通用性。

Method: 基于最小化结合$\chi^2$测试统计量与目标分数偏差的成本函数，测试统计量根据实验不确定性量化数据扰动概率。

Result: 发现正确和错误分类事件的分类翻转可能性不同，可通过攻击参数量化网络决策鲁棒性。

Conclusion: MiniFool算法具有通用性，可用于测试神经网络分类任务的鲁棒性，包括未标记实验数据。

Abstract: In this paper, we present a new algorithm, MiniFool, that implements
physics-inspired adversarial attacks for testing neural network-based
classification tasks in particle and astroparticle physics. While we initially
developed the algorithm for the search for astrophysical tau neutrinos with the
IceCube Neutrino Observatory, we apply it to further data from other science
domains, thus demonstrating its general applicability. Here, we apply the
algorithm to the well-known MNIST data set and furthermore, to Open Data data
from the CMS experiment at the Large Hadron Collider. The algorithm is based on
minimizing a cost function that combines a $\chi^2$ based test-statistic with
the deviation from the desired target score. The test statistic quantifies the
probability of the perturbations applied to the data based on the experimental
uncertainties. For our studied use cases, we find that the likelihood of a
flipped classification differs for both the initially correctly and incorrectly
classified events. When testing changes of the classifications as a function of
an attack parameter that scales the experimental uncertainties, the robustness
of the network decision can be quantified. Furthermore, this allows testing the
robustness of the classification of unlabeled experimental data.

</details>


### [275] [Verifiable Split Learning via zk-SNARKs](https://arxiv.org/abs/2511.01356)
*Rana Alaa,Darío González-Ferreiro,Carlos Beis-Penedo,Manuel Fernández-Veiga,Rebeca P. Díaz-Redondo,Ana Fernández-Vilas*

Main category: cs.LG

TL;DR: 提出可验证的拆分学习框架，集成zk - SNARK证明保证正确性和可验证性，并与区块链系统对比。


<details>
  <summary>Details</summary>
Motivation: 拆分学习缺乏验证各方计算正确性和诚实性的能力。

Method: 提出集成zk - SNARK证明的可验证拆分学习框架，在正向和反向传播中为双方生成证明和验证。

Result: 与区块链系统对比，zk - SNARK测试可实现可验证性和正确性，区块链轻量级但不可验证。

Conclusion: 应用zk - SNARK测试的拆分学习框架能保证计算的正确性和可验证性。

Abstract: Split learning is an approach to collaborative learning in which a deep
neural network is divided into two parts: client-side and server-side at a cut
layer. The client side executes its model using its raw input data and sends
the intermediate activation to the server side. This configuration architecture
is very useful for enabling collaborative training when data or resources are
separated between devices. However, split learning lacks the ability to verify
the correctness and honesty of the computations that are performed and
exchanged between the parties. To this purpose, this paper proposes a
verifiable split learning framework that integrates a zk-SNARK proof to ensure
correctness and verifiability. The zk-SNARK proof and verification are
generated for both sides in forward propagation and backward propagation on the
server side, guaranteeing verifiability on both sides. The verifiable split
learning architecture is compared to a blockchain-enabled system for the same
deep learning network, one that records updates but without generating the
zero-knowledge proof. From the comparison, it can be deduced that applying the
zk-SNARK test achieves verifiability and correctness, while blockchains are
lightweight but unverifiable.

</details>


### [276] [Learning Intractable Multimodal Policies with Reparameterization and Diversity Regularization](https://arxiv.org/abs/2511.01374)
*Ziqi Wang,Jiashun Liu,Ling Pan*

Main category: cs.LG

TL;DR: 传统连续深度强化学习算法难以表达复杂多模态决策分布，本文提出统一框架优化多模态参与者，并提出基于距离的多样性正则化方法，在多领域表现良好。


<details>
  <summary>Details</summary>
Motivation: 传统连续深度强化学习算法在表达复杂多模态决策分布上存在局限，现有在线多模态RL算法的参与者难以处理，难以平衡性能、决策多样性和效率。

Method: 首先在统一框架内重新表述现有难处理的多模态参与者，并证明可通过重参数化用策略梯度直接优化；提出基于距离的多样性正则化方法。

Result: 在多目标实现和生成式RL等多样性关键领域展示了多模态策略和方法的优势，在常规MuJoCo基准测试中表现有竞争力，实验表明摊销参与者是有前景的策略模型类。

Conclusion: 提出的方法有效克服了现有算法的挑战，摊销参与者是具有强多模态表达能力和高性能的有前景策略模型类。

Abstract: Traditional continuous deep reinforcement learning (RL) algorithms employ
deterministic or unimodal Gaussian actors, which cannot express complex
multimodal decision distributions. This limitation can hinder their performance
in diversity-critical scenarios. There have been some attempts to design online
multimodal RL algorithms based on diffusion or amortized actors. However, these
actors are intractable, making existing methods struggle with balancing
performance, decision diversity, and efficiency simultaneously. To overcome
this challenge, we first reformulate existing intractable multimodal actors
within a unified framework, and prove that they can be directly optimized by
policy gradient via reparameterization. Then, we propose a distance-based
diversity regularization that does not explicitly require decision
probabilities. We identify two diversity-critical domains, namely multi-goal
achieving and generative RL, to demonstrate the advantages of multimodal
policies and our method, particularly in terms of few-shot robustness. In
conventional MuJoCo benchmarks, our algorithm also shows competitive
performance. Moreover, our experiments highlight that the amortized actor is a
promising policy model class with strong multimodal expressivity and high
performance. Our code is available at https://github.com/PneuC/DrAC

</details>


### [277] [Protecting the Neural Networks against FGSM Attack Using Machine Unlearning](https://arxiv.org/abs/2511.01377)
*Amir Hossein Khorasani,Ali Jahanian,Maryam Rastgarpour*

Main category: cs.LG

TL;DR: 本文聚焦将机器学习的去学习技术应用于LeNet神经网络，评估其应对FGSM攻击的效果，发现能显著提升网络鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 机器学习易受对抗攻击，FGSM攻击常见，为提升模型对FGSM攻击的鲁棒性，研究应用去学习技术。

Method: 将去学习技术应用到LeNet神经网络，对其进行评估。

Result: 去学习技术能显著提升LeNet网络对FGSM攻击的鲁棒性。

Conclusion: 去学习技术可有效提升LeNet神经网络应对FGSM攻击的鲁棒性。

Abstract: Machine learning is a powerful tool for building predictive models. However,
it is vulnerable to adversarial attacks. Fast Gradient Sign Method (FGSM)
attacks are a common type of adversarial attack that adds small perturbations
to input data to trick a model into misclassifying it. In response to these
attacks, researchers have developed methods for "unlearning" these attacks,
which involves retraining a model on the original data without the added
perturbations. Machine unlearning is a technique that tries to "forget"
specific data points from the training dataset, to improve the robustness of a
machine learning model against adversarial attacks like FGSM. In this paper, we
focus on applying unlearning techniques to the LeNet neural network, a popular
architecture for image classification. We evaluate the efficacy of unlearning
FGSM attacks on the LeNet network and find that it can significantly improve
its robustness against these types of attacks.

</details>


### [278] [Memory-Efficient Training with In-Place FFT Implementation](https://arxiv.org/abs/2511.01385)
*Xinyu Ding,Bangtian Liu,Siyu Liao,Zhongfeng Wang*

Main category: cs.LG

TL;DR: 提出首个实域全就地FFT框架rdFFT，可减少训练内存成本，为频域轻量级适配提供方向。


<details>
  <summary>Details</summary>
Motivation: 现有FFT实现无法实现真正的就地计算，rFFT存在维度不匹配和额外内存分配问题。

Method: 利用蝶形运算对称性和频域共轭特性，设计隐式复数编码方案，消除中间缓存使用。

Result: 在多个自然语言理解任务实验中，该方法有效减少训练内存成本。

Conclusion: rdFFT为频域轻量级适配提供了有前景的方向。

Abstract: Fast Fourier Transforms (FFT) are widely used to reduce memory and
computational costs in deep learning. However, existing implementations,
including standard FFT and real FFT (rFFT), cannot achieve true in-place
computation. In particular, rFFT maps an input of size n to a complex output of
size n/2+1, causing dimensional mismatch and requiring additional memory
allocation. We propose the first real-domain, fully in-place FFT framework
(rdFFT) that preserves input-output memory space consistency. By leveraging
butterfly operation symmetry and conjugate properties in the frequency domain,
we design an implicit complex encoding scheme that eliminates intermediate
cache usage entirely. Experiments on multiple natural language understanding
tasks demonstrate the method effectiveness in reducing training memory cost,
offering a promising direction for frequency-domain lightweight adaptation.

</details>


### [279] [Leveraging Compact Satellite Embeddings and Graph Neural Networks for Large-Scale Poverty Mapping](https://arxiv.org/abs/2511.01408)
*Markus B. Pettersson,Adel Daoud*

Main category: cs.LG

TL;DR: 提出基于图的方法，利用低维卫星嵌入预测撒哈拉以南非洲地区集群层面财富指数，实验显示结合图结构可提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 全球南方大部分地区缺乏准确、细粒度的贫困地图，人口与健康调查（DHS）数据存在空间覆盖有限和坐标随机偏移问题。

Method: 提出基于图的方法，利用低维AlphaEarth卫星嵌入，建模调查和未标记地点的空间关系，引入概率“模糊标签”损失处理坐标偏移。

Result: 在37个DHS数据集（2017 - 2023）上实验，结合图结构比“仅图像”基线略提高准确性。

Conclusion: 紧凑的地球观测嵌入在大规模社会经济制图方面有潜力。

Abstract: Accurate, fine-grained poverty maps remain scarce across much of the Global
South. While Demographic and Health Surveys (DHS) provide high-quality
socioeconomic data, their spatial coverage is limited and reported coordinates
are randomly displaced for privacy, further reducing their quality. We propose
a graph-based approach leveraging low-dimensional AlphaEarth satellite
embeddings to predict cluster-level wealth indices across Sub-Saharan Africa.
By modeling spatial relations between surveyed and unlabeled locations, and by
introducing a probabilistic "fuzzy label" loss to account for coordinate
displacement, we improve the generalization of wealth predictions beyond
existing surveys. Our experiments on 37 DHS datasets (2017-2023) show that
incorporating graph structure slightly improves accuracy compared to
"image-only" baselines, demonstrating the potential of compact EO embeddings
for large-scale socioeconomic mapping.

</details>


### [280] [CG-FKAN: Compressed-Grid Federated Kolmogorov-Arnold Networks for Communication Constrained Environment](https://arxiv.org/abs/2511.01433)
*Seunghun Yu,Youngjoon Lee,Jinu Gong,Joonhyuk Kang*

Main category: cs.LG

TL;DR: 提出CG - FKAN压缩扩展网格，在通信受限场景下降低RMSE并推导近似误差理论上界


<details>
  <summary>Details</summary>
Motivation: 现有应用KAN的联邦学习研究忽略网格扩展带来的通信开销，而网格扩展对建模复杂函数很重要

Method: 提出CG - FKAN，在通信预算下通过稀疏化并仅传输必要系数来压缩扩展网格

Result: 实验表明CG - FKAN在通信受限场景下比固定网格KAN的RMSE最多降低13.6%，并推导其近似误差的理论上界

Conclusion: CG - FKAN能有效应对应用KAN的联邦学习中网格扩展带来的通信开销问题

Abstract: Federated learning (FL), widely used in privacy-critical applications,
suffers from limited interpretability, whereas Kolmogorov-Arnold Networks (KAN)
address this limitation via learnable spline functions. However, existing FL
studies applying KAN overlook the communication overhead introduced by grid
extension, which is essential for modeling complex functions. In this letter,
we propose CG-FKAN, which compresses extended grids by sparsifying and
transmitting only essential coefficients under a communication budget.
Experiments show that CG-FKAN achieves up to 13.6% lower RMSE than fixed-grid
KAN in communication-constrained settings. In addition, we derive a theoretical
upper bound on its approximation error.

</details>


### [281] [The Curvature Rate λ: A Scalar Measure of Input-Space Sharpness in Neural Networks](https://arxiv.org/abs/2511.01438)
*Jacob Poschl*

Main category: cs.LG

TL;DR: 提出输入空间标量曲率度量λ，实验表明其训练中可预测演变，用CRR可塑形，比SAM有优势，能描述模型平滑性。


<details>
  <summary>Details</summary>
Motivation: 现有锐度度量在参数空间定义，存在计算昂贵、对重参数化敏感和难功能解释等问题。

Method: 引入输入空间曲率率λ，通过高阶输入导数指数增长率定义，经验上用小n时log ||D^n f|| 与n的斜率估计，提出曲率率正则化（CRR）。

Result: λ在训练中可预测演变，用CRR可塑形，相比SAM，达到相似精度且输入空间几何更平坦、置信校准更好。

Conclusion: λ以微分动力学为基础，为学习模型功能平滑性提供紧凑、可解释和参数不变描述符。

Abstract: Curvature influences generalization, robustness, and how reliably neural
networks respond to small input perturbations. Existing sharpness metrics are
typically defined in parameter space (e.g., Hessian eigenvalues) and can be
expensive, sensitive to reparameterization, and difficult to interpret in
functional terms. We introduce a scalar curvature measure defined directly in
input space: the curvature rate {\lambda}, given by the exponential growth rate
of higher-order input derivatives. Empirically, {\lambda} is estimated as the
slope of log ||D^n f|| versus n for small n. This growth-rate perspective
unifies classical analytic quantities: for analytic functions, {\lambda}
corresponds to the inverse radius of convergence, and for bandlimited signals,
it reflects the spectral cutoff. The same principle extends to neural networks,
where {\lambda} tracks the emergence of high-frequency structure in the
decision boundary. Experiments on analytic functions and neural networks (Two
Moons and MNIST) show that {\lambda} evolves predictably during training and
can be directly shaped using a simple derivative-based regularizer, Curvature
Rate Regularization (CRR). Compared to Sharpness-Aware Minimization (SAM), CRR
achieves similar accuracy while yielding flatter input-space geometry and
improved confidence calibration. By grounding curvature in differentiation
dynamics, {\lambda} provides a compact, interpretable, and
parameterization-invariant descriptor of functional smoothness in learned
models.

</details>


### [282] [Efficient Curvature-aware Graph Network](https://arxiv.org/abs/2511.01443)
*Chaoqun Fei,Tinglve Zhou,Tianyong Hao,Yangyang Li*

Main category: cs.LG

TL;DR: 提出有效电阻曲率解决Ollivier - Ricci曲率计算复杂度高问题，性能相当但计算开销大幅降低。


<details>
  <summary>Details</summary>
Motivation: 现有Ollivier - Ricci曲率计算复杂度高，限制其在大规模图数据集的应用。

Method: 提出有效电阻曲率，用节点对间有效电阻量化信息传递，替代最优传输距离。

Result: 有效电阻曲率计算效率远超Ollivier - Ricci曲率，几何表达能力相当；理论证明其低复杂度和可替代性；实验表明在GNN任务中性能相当且降低计算开销。

Conclusion: 有效电阻曲率是一种可替代Ollivier - Ricci曲率的高效图曲率度量方法。

Abstract: Graph curvature provides geometric priors for Graph Neural Networks (GNNs),
enhancing their ability to model complex graph structures, particularly in
terms of structural awareness, robustness, and theoretical interpretability.
Among existing methods, Ollivier-Ricci curvature has been extensively studied
due to its strong geometric interpretability, effectively characterizing the
local geometric distribution between nodes. However, its prohibitively high
computational complexity limits its applicability to large-scale graph
datasets. To address this challenge, we propose a novel graph curvature
measure--Effective Resistance Curvature--which quantifies the ease of message
passing along graph edges using the effective resistance between node pairs,
instead of the optimal transport distance. This method significantly
outperforms Ollivier-Ricci curvature in computational efficiency while
preserving comparable geometric expressiveness. Theoretically, we prove the low
computational complexity of effective resistance curvature and establish its
substitutability for Ollivier-Ricci curvature. Furthermore, extensive
experiments on diverse GNN tasks demonstrate that our method achieves
competitive performance with Ollivier-Ricci curvature while drastically
reducing computational overhead.

</details>


### [283] [DAMBench: A Multi-Modal Benchmark for Deep Learning-based Atmospheric Data Assimilation](https://arxiv.org/abs/2511.01468)
*Hao Wang,Zixuan Weng,Jindong Han,Wei Fan,Hao Liu*

Main category: cs.LG

TL;DR: 本文介绍首个大规模多模态基准DAMBench，用于评估大气数据同化模型，提供统一评估协议并展示插件效果，代码和数据集公开。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的数据同化研究依赖简化场景且缺乏标准基准，无法公平比较模型。

Method: 引入DAMBench，整合高质量背景状态和多模态观测数据，重采样和时间对齐，提供评估协议，测试代表性方法，提出轻量级多模态插件。

Result: 通过全面实验，DAMBench为未来研究奠定严谨基础，促进可重复性、公平比较和扩展性。

Conclusion: DAMBench解决了现有研究的局限，适用于现实世界多模态场景，数据集和代码公开可推动后续研究。

Abstract: Data Assimilation is a cornerstone of atmospheric system modeling, tasked
with reconstructing system states by integrating sparse, noisy observations
with prior estimation. While traditional approaches like variational and
ensemble Kalman filtering have proven effective, recent advances in deep
learning offer more scalable, efficient, and flexible alternatives better
suited for complex, real-world data assimilation involving large-scale and
multi-modal observations. However, existing deep learning-based DA research
suffers from two critical limitations: (1) reliance on oversimplified scenarios
with synthetically perturbed observations, and (2) the absence of standardized
benchmarks for fair model comparison. To address these gaps, in this work, we
introduce DAMBench, the first large-scale multi-modal benchmark designed to
evaluate data-driven DA models under realistic atmospheric conditions. DAMBench
integrates high-quality background states from state-of-the-art forecasting
systems and real-world multi-modal observations (i.e., real-world weather
stations and satellite imagery). All data are resampled to a common grid and
temporally aligned to support systematic training, validation, and testing. We
provide unified evaluation protocols and benchmark representative data
assimilation approaches, including latent generative models and neural process
frameworks. Additionally, we propose a lightweight multi-modal plugin to
demonstrate how integrating realistic observations can enhance even simple
baselines. Through comprehensive experiments, DAMBench establishes a rigorous
foundation for future research, promoting reproducibility, fair comparison, and
extensibility to real-world multi-modal scenarios. Our dataset and code are
publicly available at https://github.com/figerhaowang/DAMBench.

</details>


### [284] [Gated Fusion Enhanced Multi-Scale Hierarchical Graph Convolutional Network for Stock Movement Prediction](https://arxiv.org/abs/2511.01570)
*Xiaosha Xue,Peibo Duan,Zhipeng Liu,Qi Chu,Changsheng Zhang,Bin zhang*

Main category: cs.LG

TL;DR: 提出MS - HGFN模型解决多尺度GNN在股市预测中的问题，实验表明该模型优于传统和先进模型，提升了预测准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 准确预测股市走势具有挑战性，多尺度GNN在建模时忽略了股票内部属性模式和多尺度采样时对粗细粒度特征的关注偏差。

Method: 引入MS - HGFN模型，包含分层GNN模块形成动态图以捕捉时空依赖，采用自上而下的门控方法集成多尺度时空特征。

Result: 利用美中股市真实数据集实验，MS - HGFN优于传统和先进模型，预测准确率最多提高1.4%，回报模拟稳定性增强。

Conclusion: MS - HGFN模型在股市预测中表现出色，可有效解决多尺度GNN存在的问题。

Abstract: Accurately predicting stock market movements remains a formidable challenge
due to the inherent volatility and complex interdependencies among stocks.
Although multi-scale Graph Neural Networks (GNNs) hold potential for modeling
these relationships, they frequently neglect two key points: the subtle
intra-attribute patterns within each stock affecting inter-stock correlation,
and the biased attention to coarse- and fine-grained features during
multi-scale sampling. To overcome these challenges, we introduce MS-HGFN
(Multi-Scale Hierarchical Graph Fusion Network). The model features a
hierarchical GNN module that forms dynamic graphs by learning patterns from
intra-attributes and features from inter-attributes over different time scales,
thus comprehensively capturing spatio-temporal dependencies. Additionally, a
top-down gating approach facilitates the integration of multi-scale
spatio-temporal features, preserving critical coarse- and fine-grained features
without too much interference. Experiments utilizing real-world datasets from
U.S. and Chinese stock markets demonstrate that MS-HGFN outperforms both
traditional and advanced models, yielding up to a 1.4% improvement in
prediction accuracy and enhanced stability in return simulations. The code is
available at https://anonymous.4open.science/r/MS-HGFN.

</details>


### [285] [HIT-ROCKET: Hadamard-vector Inner-product Transformer for ROCKET](https://arxiv.org/abs/2511.01572)
*Wang Hao,Kuang Zhang,Hou Chengyu,Yuan Zhonghao,Tan Chenxing,Fu Weifeng,Zhu Yangying*

Main category: cs.LG

TL;DR: 针对时间序列分类现有方法问题，提出基于Hadamard卷积变换特征提取方法，在多领域数据集上达SOTA性能，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列分类SOTA方法计算复杂度高、调参和训练周期长，轻量级方法如ROCKET在核选择和计算开销上有提升空间。

Method: 提出基于Hadamard卷积变换的特征提取方法，用Hadamard矩阵列或行向量作不同长度卷积核，与现有方法兼容并利用核正交性提升性能。

Result: 在多领域数据集（聚焦UCR时间序列数据集）实验中，F1分数比ROCKET至少提高5%，训练时间比miniROCKET缩短50%，可部署在超低功耗嵌入式设备。

Conclusion: 所提方法有效提升了时间序列分类性能和计算效率，有良好的适应性和鲁棒性，具备实际应用价值。

Abstract: Time series classification holds broad application value in communications,
information countermeasures, finance, and medicine. However, state-of-the-art
(SOTA) methods-including HIVE-COTE, Proximity Forest, and TS-CHIEF-exhibit high
computational complexity, coupled with lengthy parameter tuning and training
cycles. In contrast, lightweight solutions like ROCKET (Random Convolutional
Kernel Transform) offer greater efficiency but leave substantial room for
improvement in kernel selection and computational overhead. To address these
challenges, we propose a feature extraction approach based on Hadamard
convolutional transform, utilizing column or row vectors of Hadamard matrices
as convolution kernels with extended lengths of varying sizes. This enhancement
maintains full compatibility with existing methods (e.g., ROCKET) while
leveraging kernel orthogonality to boost computational efficiency, robustness,
and adaptability. Comprehensive experiments on multi-domain datasets-focusing
on the UCR time series dataset-demonstrate SOTA performance: F1-score improved
by at least 5% vs. ROCKET, with 50% shorter training time than miniROCKET
(fastest ROCKET variant) under identical hyperparameters, enabling deployment
on ultra-low-power embedded devices. All code is available on GitHub.

</details>


### [286] [Explore More, Learn Better: Parallel MLLM Embeddings under Mutual Information Minimization](https://arxiv.org/abs/2511.01588)
*Zhicheng Wang,Chen Ju,Xu Chen,Shuai Xiao,Jinsong Lan,Xiaoyong Zhu,Ying Chen,Zhiguo Cao*

Main category: cs.LG

TL;DR: 本文针对多模态嵌入学习提出并行解耦框架PDF，在多个MLLM骨干上验证有效，提升了模型效果与效率。


<details>
  <summary>Details</summary>
Motivation: 现有多模态嵌入学习整体范式局限于SSC，无法充分发挥MLLM能力。

Method: 利用MLLM可引导性，设计PDF框架，通过不同可学习前缀生成并行路径获取嵌入，采用互信息最小化约束和逐路径对比监督。

Result: 在MMEB基准上证明有效，不同分辨率和模型大小均有显著提升，如VLM2Vec - LLaVA - 1.6 - LR模型提升8.9%（7B），2B模型用一半计算预算超越基线2.6%。

Conclusion: PDF框架能产生鲁棒语义覆盖和可泛化嵌入空间，推理时计算开销小，具有有效性和高效性。

Abstract: Embedding models are a cornerstone of modern AI. Driven by Multimodal Large
Language Models (MLLMs), they have made great progress in architecture and data
curation, while the holistic paradigm is still limited to SSC, i.e., single
input, singular embedding, contrastive supervision, which collapses rich,
multifaceted inputs into monolithic embeddings and fails to fully exploit MLLM
capabilities. In this paper, we tailor one Parallel Decoupling Framework (PDF)
for multimodal embedding learning, by utilizing the proprietary steerability of
MLLMs, i.e., their ability to flexibly generate quite differentiated response
under explicit instructions. Concretely, PDF conditions a shared MLLM backbone
on distinct, learnable prefixes to roll out multiple parallel paths for one
input, then relies on these paths to obtain parallel embeddings. To promote
full parallel diversity, we employ Mutual Information Minimization (MIM) as an
explicit constraint, coupled with per-path contrastive supervision to maintain
semantic alignment. Such dual-objectives force PDF to yield robust semantic
coverage and a generalizable embedding space. Ultimately, the remarkable
embedding space are accessible at inference via one single forward pass,
incurring negligible computational overhead. We instantiate PDF on multiple
MLLM backbones and prove its effectiveness on MMEB benchmark. Significant gains
are consistently achieved across various resolutions and model sizes, e.g.,
boosting the VLM2Vec-LLaVA-1.6-LR model by a remarkable +8.9% (7B), while the
VLM2Vec-Qwen2VL models by +4.2% (2B) and +3.1% (7B). In terms of efficiency,
our 2B model surpasses its baseline by +2.6% using only half the computational
budget.

</details>


### [287] [Defining Energy Indicators for Impact Identification on Aerospace Composites: A Physics-Informed Machine Learning Perspective](https://arxiv.org/abs/2511.01592)
*Natália Ribeiro Marinho,Richard Loendersloot,Frank Grooteman,Jan Willem Wiegman,Uraz Odyurt,Tiedo Tinga*

Main category: cs.LG

TL;DR: 本文提出物理信息框架，结合特征选择优化输入空间，训练神经网络预测航空复合材料冲击能量，提升了预测精度。


<details>
  <summary>Details</summary>
Motivation: 当前能量预测方法受数据稀疏、噪声等问题限制，需要更有效的方法来预测航空复合材料的冲击能量。

Method: 引入物理信息框架，结合观测偏差设计特征，进行结构化特征选择，提取多域特征，用实验数据训练全连接神经网络。

Result: 模型显著提高了冲击能量预测精度，相比传统方法和纯数据驱动模型，误差降低了三倍。

Conclusion: 该物理信息框架和优化输入空间的方法能有效提高航空复合材料冲击能量预测的准确性和可解释性。

Abstract: Energy estimation is critical to impact identification on aerospace
composites, where low-velocity impacts can induce internal damage that is
undetectable at the surface. Current methodologies for energy prediction are
often constrained by data sparsity, signal noise, complex feature
interdependencies, non-linear dynamics, massive design spaces, and the
ill-posed nature of the inverse problem. This study introduces a
physics-informed framework that embeds domain knowledge into machine learning
through a dedicated input space. The approach combines observational biases,
which guide the design of physics-motivated features, with targeted feature
selection to retain only the most informative indicators. Features are
extracted from time, frequency, and time-frequency domains to capture
complementary aspects of the structural response. A structured feature
selection process integrating statistical significance, correlation filtering,
dimensionality reduction, and noise robustness ensures physical relevance and
interpretability. Exploratory data analysis further reveals domain-specific
trends, yielding a reduced feature set that captures essential dynamic
phenomena such as amplitude scaling, spectral redistribution, and transient
signal behaviour. Together, these steps produce a compact set of
energy-sensitive indicators with both statistical robustness and physical
significance, resulting in impact energy predictions that remain interpretable
and traceable to measurable structural responses. Using this optimised input
space, a fully-connected neural network is trained and validated with
experimental data from multiple impact scenarios, including pristine and
damaged states. The resulting model demonstrates significantly improved impact
energy prediction accuracy, reducing errors by a factor of three compared to
conventional time-series techniques and purely data-driven models.

</details>


### [288] [Scaling Graph Chain-of-Thought Reasoning: A Multi-Agent Framework with Efficient LLM Serving](https://arxiv.org/abs/2511.01633)
*Chengying Huan,Ziheng Meng,Yongchao Liu,Zhengyi Yang,Yun Zhu,Yue Yun,Shipeng Li,Rong Gu,Xiabao Wu,Haitao Zhang,Chuntao Hong,Shaonan Ma,Guihai Chen,Chen Tian*

Main category: cs.LG

TL;DR: 提出多智能体Graph - CoT系统GLM，优化推理和服务效率，实验表明其在准确率、成本、延迟和吞吐量上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有Graph - CoT管道因单智能体整体提示、重复上下文重新编码和低效服务执行，存在准确率低、令牌使用过多、延迟高和吞吐量低的问题。

Method: 将推理分解为分类、推理、动作生成和图检索的专业智能体，引入Graph - CoT感知的LLM推理机制，包括图特定的KV缓存管理、基于优先级的逐出和流水线执行。

Result: GLM相比最先进的Graph - CoT基线，答案准确率提高达38%，令牌成本降低达95.7%，推理延迟降低90.3%，吞吐量提高达15.1倍。

Conclusion: GLM能有效用于大规模复杂现实世界推理。

Abstract: Graph Chain-of-Thought (Graph-CoT) enables large language models (LLMs) to
perform step-by-step reasoning over graph-structured knowledge, but existing
pipelines suffer from low accuracy, excessive token usage, high latency, and
low throughput due to single-agent monolithic prompts, repeated context
re-encoding, and inefficient serving execution. We present GLM, the first
multi-agent Graph-CoT system co-designed with an optimized LLM serving
architecture. GLM decomposes reasoning into specialized agents for
classification, reasoning, action generation, and graph retrieval, enabling
branching and selective context sharing to reduce prompt length and reasoning
iterations while preserving reasoning quality, thereby improving accuracy and
reducing overall token consumption. To scale inference, we introduce a
Graph-CoT-aware LLM inference mechanism with graph-specific KV-cache
management, priority-based eviction, and pipelined execution to improve serving
efficiency. Experiments demonstrate that GLM improves answer accuracy by up to
38%, reduces token cost by up to 95.7%, lowers inference latency by 90.3%, and
achieves up to 15.1x higher throughput compared to state-of-the-art Graph-CoT
baselines, enabling efficient adoption for complex real-world reasoning at
scale.

</details>


### [289] [Bayesian Natural Gradient Fine-Tuning of CLIP Models via Kalman Filtering](https://arxiv.org/abs/2511.01694)
*Hossein Abdi,Mingfei Sun,Wei Pan*

Main category: cs.LG

TL;DR: 提出用卡尔曼滤波器对CLIP模型进行贝叶斯近似自然梯度下降微调方法，实验显示在ID和OOD数据集表现好。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP模型少样本微调方法在ID和OOD数据集效果不佳，一阶优化器有收敛慢等问题，而二阶方法虽有优势但计算开销大。

Method: 提出用卡尔曼滤波器对CLIP模型进行贝叶斯近似自然梯度下降的方法，结合二阶优化和贝叶斯推理。

Result: 在不同图像分类数据集实验中，算法在ID表现优或相当，OOD鲁棒性提升。

Conclusion: 这是首次成功将卡尔曼滤波用于微调基于CLIP的模型，能在视觉语言任务中实现更鲁棒高效的学习。

Abstract: Vision-language pre-trained models, such as CLIP, have established new
benchmarks in multimodal data mining. In such models, few-shot fine-tuning is a
major challenge to achieve optimal performance on both in-distribution (ID) and
out-of-distribution (OOD) datasets, especially when labeled data is scarce.
Most existing fine-tuning approaches rely on first-order gradient-based
optimizers, which typically suffer from slow convergence, sensitivity to
step-size hyperparameters, and poor generalization in OOD settings. In
contrast, second-order methods utilize local curvature information of the loss
landscape to adjust the update step size. This is particularly beneficial for
CLIP models, whose non-convex loss functions often contain sharp critical
points. In such cases, natural gradient direction can offer more substantial
and efficient per-iteration updates when fine-tuning with limited data. Natural
Gradient Descent (NGD) is obtained by preconditioning the standard gradient
with the inverse Fisher Information Matrix (FIM), which is computationally
expensive for large models. To address this, we propose a Bayesian
approximation of NGD using a Kalman filter for CLIP models. Our method combines
the benefits of second-order optimization with Bayesian inference, which
enhances generalization while providing uncertainty quantification. Extensive
experiments conducted on diverse image classification datasets demonstrate that
our algorithm consistently achieves superior--or comparable--ID performance and
improved OOD robustness compared to state-of-the-art baselines. To the best of
our knowledge, this work represents the first successful application of Kalman
filtering to fine-tuning CLIP-based models, which enables more robust and
efficient learning in vision-language tasks.

</details>


### [290] [Collaborative Large Language Model Inference via Resource-Aware Parallel Speculative Decoding](https://arxiv.org/abs/2511.01695)
*Jungyeon Koh,Hyun Jong Yang*

Main category: cs.LG

TL;DR: 本文提出统一框架联合优化用户关联和资源分配以支持高效并行推测解码，用多智能体深度强化学习算法求解问题，实验表明可降低端到端延迟且不影响推理精度。


<details>
  <summary>Details</summary>
Motivation: 设备端大语言模型推理需求增长，资源受限场景下需高效移动边缘计算解决方案，现有推测解码存在通信开销和异步延迟问题。

Method: 提出联合优化用户关联和资源分配的统一框架，使用多智能体深度强化学习算法解决该问题，用Sionna模拟器进行实验。

Result: 方法实现端到端延迟最高降低28.0%，平均降低23.7%，且不影响推理精度。

Conclusion: 该方法能在移动边缘计算系统中实现可扩展、低延迟的大语言模型服务。

Abstract: The growing demand for on-device large language model (LLM) inference
highlights the need for efficient mobile edge computing (MEC) solutions,
especially in resource-constrained settings. Speculative decoding offers a
promising solution by partitioning token generation between a lightweight draft
model on mobile devices and a powerful target model on edge servers, but
suffers from communication overhead and asynchronous delays. This paper is the
first to propose a unified framework that jointly optimizes user association
and resource allocation (UARA) to support efficient parallel speculative
decoding. We solve the UARA problem using a multi-agent deep reinforcement
learning algorithm. To evaluate our approach under realistic conditions, we
conduct experiments using the Sionna simulator. Results show that our method
achieves up to 28.0% and an average of 23.7% reduction in end-to-end latency
without compromising inference accuracy, enabling scalable and low-latency LLM
services in MEC systems.

</details>


### [291] [Game-theoretic distributed learning of generative models for heterogeneous data collections](https://arxiv.org/abs/2511.01740)
*Dmitrij Schlesinger,Boris Flach*

Main category: cs.LG

TL;DR: 本文提出通过交换合成数据应对分布式学习中处理异构本地模型和数据的挑战，将本地模型学习建模为合作博弈，证明存在唯一纳什均衡并展示了方法优势。


<details>
  <summary>Details</summary>
Motivation: 解决分布式学习中处理异构本地模型和数据的难题。

Method: 提出交换合成数据而非共享模型参数的方法，将本地模型学习建模为合作博弈。

Result: 证明指数族本地模型存在唯一纳什均衡，且学习方法收敛到该均衡。

Conclusion: 该方法在图像分类和条件生成的标准基准视觉数据集上具有优势。

Abstract: One of the main challenges in distributed learning arises from the difficulty
of handling heterogeneous local models and data. In light of the recent success
of generative models, we propose to meet this challenge by building on the idea
of exchanging synthetic data instead of sharing model parameters. Local models
can then be treated as ``black boxes'' with the ability to learn their
parameters from data and to generate data according to these parameters.
Moreover, if the local models admit semi-supervised learning, we can extend the
approach by enabling local models on different probability spaces. This allows
to handle heterogeneous data with different modalities. We formulate the
learning of the local models as a cooperative game starting from the principles
of game theory. We prove the existence of a unique Nash equilibrium for
exponential family local models and show that the proposed learning approach
converges to this equilibrium. We demonstrate the advantages of our approach on
standard benchmark vision datasets for image classification and conditional
generation.

</details>


### [292] [HyperNQ: A Hypergraph Neural Network Decoder for Quantum LDPC Codes](https://arxiv.org/abs/2511.01741)
*Ameya S. Bhave,Navnil Choudhury,Kanad Basu*

Main category: cs.LG

TL;DR: 提出基于超图神经网络的量子低密度奇偶校验码解码器HyperNQ，在伪阈值下逻辑错误率表现优于现有解码器。


<details>
  <summary>Details</summary>
Motivation: 传统方法解码量子低密度奇偶校验码存在收敛差问题，机器学习技术捕捉高阶相关性能力有限，需更好的解码器。

Method: 提出HyperNQ解码器，采用两阶段消息传递方案，在伪阈值区域评估。

Result: 在伪阈值以下，HyperNQ较BP逻辑错误率最多降低84%，较基于GNN策略最多降低50%。

Conclusion: HyperNQ在解码量子低密度奇偶校验码上比现有解码器性能更优。

Abstract: Quantum computing requires effective error correction strategies to mitigate
noise and decoherence. Quantum Low-Density Parity-Check (QLDPC) codes have
emerged as a promising solution for scalable Quantum Error Correction (QEC)
applications by supporting constant-rate encoding and a sparse parity-check
structure. However, decoding QLDPC codes via traditional approaches such as
Belief Propagation (BP) suffers from poor convergence in the presence of short
cycles. Machine learning techniques like Graph Neural Networks (GNNs) utilize
learned message passing over their node features; however, they are restricted
to pairwise interactions on Tanner graphs, which limits their ability to
capture higher-order correlations. In this work, we propose HyperNQ, the first
Hypergraph Neural Network (HGNN)- based QLDPC decoder that captures
higher-order stabilizer constraints by utilizing hyperedges-thus enabling
highly expressive and compact decoding. We use a two-stage message passing
scheme and evaluate the decoder over the pseudo-threshold region. Below the
pseudo-threshold mark, HyperNQ improves the Logical Error Rate (LER) up to 84%
over BP and 50% over GNN-based strategies, demonstrating enhanced performance
over the existing state-of-the-art decoders.

</details>


### [293] [Towards Efficient Federated Learning of Networked Mixture-of-Experts for Mobile Edge Computing](https://arxiv.org/abs/2511.01743)
*Song Gao,Shusen Jing,Shuai Zhang,Yue Wang,Xiangwei Zhou,Songyang Zhang*

Main category: cs.LG

TL;DR: 介绍Networked Mixture-of-Experts (NMoE)系统应对边缘设备训练和部署大模型挑战。


<details>
  <summary>Details</summary>
Motivation: 大人工智能模型训练所需计算资源和数据与边缘设备有限能力冲突，需解决边缘训练和部署难题。

Method: 引入NMoE系统，客户基于专业知识将任务分配给邻居协作推理并聚合结果，提出结合监督和自监督学习的联邦学习框架训练NMoE。

Result: 通过大量实验证明了NMoE系统的有效性。

Conclusion: 为NMoE训练算法提供了见解和基准。

Abstract: Recent advancements in large artificial intelligence models (LAMs) are
driving significant innovations in mobile edge computing within next-generation
wireless networks. However, the substantial demands for computational resources
and large-scale training data required to train LAMs conflict with the limited
storage and computational capacity of edge devices, posing significant
challenges to training and deploying LAMs at the edge. In this work, we
introduce the Networked Mixture-of-Experts (NMoE) system, in which clients
infer collaboratively by distributing tasks to suitable neighbors based on
their expertise and aggregate the returned results. For training the NMoE, we
propose a federated learning framework that integrates both supervised and
self-supervised learning to balance personalization and generalization, while
preserving communication efficiency and data privacy. We conduct extensive
experiments to demonstrate the efficacy of the proposed NMoE system, providing
insights and benchmarks for the NMoE training algorithms.

</details>


### [294] [An Open-Access Benchmark of Statistical and Machine-Learning Anomaly Detection Methods for Battery Applications](https://arxiv.org/abs/2511.01745)
*Mei-Chin Pang,Suraj Adhikari,Takuma Kasahara,Nagihiro Haba,Saneyuki Ohno*

Main category: cs.LG

TL;DR: 提出OSBAD作为电池应用中异常检测框架的开源基准，展示特征转换、贝叶斯优化管道等方法，验证其跨化学通用性，为电池分析提供统一基础。


<details>
  <summary>Details</summary>
Motivation: 电池安全至关重要，未检测到的异常会带来安全隐患和停机成本，需要对电池应用中的异常检测框架进行系统比较。

Method: 基准测试15种不同算法，采用物理和统计信息的特征转换工作流，提出基于迁移学习和回归代理的贝叶斯优化管道。

Result: 验证了OSBAD在不同电化学系统中的跨化学通用性，能识别不同体系的不规则性。

Conclusion: 强调物理和统计信息的特征工程以及概率超参数调整的模型选择，有助于推进安全关键能源系统的可靠数据驱动诊断。

Abstract: Battery safety is critical in applications ranging from consumer electronics
to electric vehicles and aircraft, where undetected anomalies could trigger
safety hazards or costly downtime. In this study, we present OSBAD as an
open-source benchmark for anomaly detection frameworks in battery applications.
By benchmarking 15 diverse algorithms encompassing statistical, distance-based,
and unsupervised machine-learning methods, OSBAD enables a systematic
comparison of anomaly detection methods across heterogeneous datasets. In
addition, we demonstrate how a physics- and statistics-informed feature
transformation workflow enhances anomaly separability by decomposing collective
anomalies into point anomalies. To address a major bottleneck in unsupervised
anomaly detection due to incomplete labels, we propose a Bayesian optimization
pipeline that facilitates automated hyperparameter tuning based on
transfer-learning and regression proxies. Through validation on datasets
covering both liquid and solid-state chemistries, we further demonstrate the
cross-chemistry generalization capability of OSBAD to identify irregularities
across different electrochemical systems. By making benchmarking database with
open-source reproducible anomaly detection workflows available to the
community, OSBAD establishes a unified foundation for developing safe,
scalable, and transferable anomaly detection tools in battery analytics. This
research underscores the significance of physics- and statistics-informed
feature engineering as well as model selection with probabilistic
hyperparameter tuning, in advancing trustworthy, data-driven diagnostics for
safety-critical energy systems.

</details>


### [295] [RLAC: Reinforcement Learning with Adversarial Critic for Free-Form Generation Tasks](https://arxiv.org/abs/2511.01758)
*Mian Wu,Gavin Zhang,Sewon Min,Sergey Levine,Aviral Kumar*

Main category: cs.LG

TL;DR: 提出RLAC方法解决开放式生成任务中强化学习后训练的可扩展性问题，实验显示其有良好效果。


<details>
  <summary>Details</summary>
Motivation: 开放式生成任务中基于规则的奖励强化学习后训练难以扩展，规则组合方式也因提示而异。

Method: 提出Reinforcement Learning with Adversarial Critic (RLAC)方法，用大语言模型作为动态识别最可能失败模式的评判器，并结合外部验证器联合优化生成器和评判器。

Result: RLAC提高了文本生成的事实准确性和代码生成的正确性，优于穷举验证和奖励模型方法。

Conclusion: 动态评判器比固定评判器更有效，RLAC有潜力将强化学习后训练扩展到自由形式生成任务。

Abstract: Open-ended generation tasks require outputs to satisfy diverse and often
implicit task-specific evaluation rubrics. The sheer number of relevant rubrics
leads to prohibitively high verification costs and incomplete assessments of a
response, making reinforcement learning (RL) post-training with rubric-based
rewards difficult to scale. This problem is exacerbated by the fact that often
the best way to combine these rubrics into one single reward is also highly
prompt-specific. We propose Reinforcement Learning with Adversarial Critic
(RLAC), a post-training approach that addresses these challenges via dynamic
rubric verification. Our approach employs a large language model (LLM) as a
critic that dynamically identifies only the most likely failure modes (e.g., a
factual error or unhandled edge case), which are then verified by an external
validator to optimize both generator and critic jointly. By training both the
generator and the critic, this game enhances the critic's error detection and
the generator's output quality while reducing required verifications. Our
experiments demonstrate that RLAC improves factual accuracy in text generation
and correctness in code generation, while also outperforming exhaustive
verification and reward model methods. We show that dynamic critics are more
effective than fixed critics, showcasing the potential of RLAC for scaling RL
post-training to free-form generation tasks.

</details>


### [296] [Random Initialization of Gated Sparse Adapters](https://arxiv.org/abs/2511.01794)
*Vi Retault,Yohaï-Eliel Berreby*

Main category: cs.LG

TL;DR: 提出RIGSA方法微调语言模型，在新任务学习中减少灾难性遗忘，在SmolLM2 - 1.7B - Instruct评估显示其遗忘少于QLoRA。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型微调时的灾难性遗忘问题，现有PEFT方法有局限，探索稀疏适应的替代方案。

Method: 引入RIGSA，从随机初始化的全秩适配器开始，用ReZero类似方法门控，迭代幅度剪枝使其稀疏。

Result: SmolLM2 - 1.7B - Instruct能通过RIGSA、4 - bit QLoRA和随机掩码学习新任务，RIGSA虽可训练参数更多，但遗忘少于QLoRA，尤其在GSM8k上，与随机掩码表现相当。

Conclusion: RIGSA在减少语言模型微调时的灾难性遗忘方面有一定优势。

Abstract: When fine-tuning language models on new tasks, catastrophic forgetting --
performance degradation on previously-learned tasks -- is a ubiquitous problem.
While Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA address this
through low-rank adapters, sparse adaptation offers an alternative that doesn't
impose rank constraints. We introduce Random Initialization of Gated Sparse
Adapters (RIGSA), which starts from randomly-initialized full-rank adapters,
gates them with a ReZero analog, and sparsifies them with iterative magnitude
pruning. We evaluate RIGSA on SmolLM2-1.7B-Instruct using a novel
vision-in-text task (Textual MNIST) and measure forgetting on PIQA, HellaSwag,
and GSM8k. SmolLM2-1.7B-Instruct initially performs around chance level on
Textual MNIST, and is capable of learning the task through RIGSA, 4-bit QLoRA
and random masking. In spite of having more trainable parameters than QLoRA,
the RIGSA configurations that we studied displayed less forgetting than QLoRA,
particularly on GSM8k, though it performs comparably to random masking.

</details>


### [297] [Bayesian Coreset Optimization for Personalized Federated Learning](https://arxiv.org/abs/2511.01800)
*Prateek Chanda,Shrey Modi,Ganesh Ramakrishnan*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In a distributed machine learning setting like Federated Learning where there
are multiple clients involved which update their individual weights to a single
central server, often training on the entire individual client's dataset for
each client becomes cumbersome. To address this issue we propose $\methodprop$:
a personalized coreset weighted federated learning setup where the training
updates for each individual clients are forwarded to the central server based
on only individual client coreset based representative data points instead of
the entire client data. Through theoretical analysis we present how the average
generalization error is minimax optimal up to logarithm bounds (upper bounded
by $\mathcal{O}(n_k^{-\frac{2 \beta}{2 \beta+\boldsymbol{\Lambda}}} \log ^{2
\delta^{\prime}}(n_k))$) and lower bounds of $\mathcal{O}(n_k^{-\frac{2
\beta}{2 \beta+\boldsymbol{\Lambda}}})$, and how the overall generalization
error on the data likelihood differs from a vanilla Federated Learning setup as
a closed form function ${\boldsymbol{\Im}}(\boldsymbol{w}, n_k)$ of the coreset
weights $\boldsymbol{w}$ and coreset sample size $n_k$. Our experiments on
different benchmark datasets based on a variety of recent personalized
federated learning architectures show significant gains as compared to random
sampling on the training data followed by federated learning, thereby
indicating how intelligently selecting such training samples can help in
performance. Additionally, through experiments on medical datasets our proposed
method showcases some gains as compared to other submodular optimization based
approaches used for subset selection on client's data.

</details>


### [298] [Dynamic Reconstruction of Ultrasound-Derived Flow Fields With Physics-Informed Neural Fields](https://arxiv.org/abs/2511.01804)
*Viraj Patel,Lisa Kreusser,Katharine Fraser*

Main category: cs.LG

TL;DR: 本文提出用多尺度傅里叶特征编码的物理信息神经网络模型，从稀疏和嘈杂的超声数据中估计血流，在去噪和修复数据集上效果良好。


<details>
  <summary>Details</summary>
Motivation: 超声成像有局限性，传统方法准确测量血流速度有挑战，物理信息机器学习可提升准确性和鲁棒性。

Method: 提出带多尺度傅里叶特征编码的物理信息神经网络模型，无需真实标签监督。

Result: 模型在合成和真实数据集的去噪和修复中均实现低均方误差。

Conclusion: 将其他成像方式有效的方法应用于超声血流重建问题。

Abstract: Blood flow is sensitive to disease and provides insight into cardiac
function, making flow field analysis valuable for diagnosis. However, while
safer than radiation-based imaging and more suitable for patients with medical
implants, ultrasound suffers from attenuation with depth, limiting the quality
of the image. Despite advances in echocardiographic particle image velocimetry
(EchoPIV), accurately measuring blood velocity remains challenging due to the
technique's limitations and the complexity of blood flow dynamics.
Physics-informed machine learning can enhance accuracy and robustness,
particularly in scenarios where noisy or incomplete data challenge purely
data-driven approaches. We present a physics-informed neural field model with
multi-scale Fourier Feature encoding for estimating blood flow from sparse and
noisy ultrasound data without requiring ground truth supervision. We
demonstrate that this model achieves consistently low mean squared error in
denoising and inpainting both synthetic and real datasets, verified against
reference flow fields and ground truth flow rate measurements. While
physics-informed neural fields have been widely used to reconstruct medical
images, applications to medical flow reconstruction are mostly prominent in
Flow MRI. In this work, we adapt methods that have proven effective in other
imaging modalities to address the specific challenge of ultrasound-based flow
reconstruction.

</details>


### [299] [No-rank Tensor Decomposition Using Metric Learning](https://arxiv.org/abs/2511.01816)
*Maryam Bagherian*

Main category: cs.LG

TL;DR: 本文提出基于度量学习的无秩张量分解框架，有理论保证，在多领域表现优于基线方法，在小数据集上性能好，确立度量学习为张量分析范式。


<details>
  <summary>Details</summary>
Motivation: 传统张量分解方法在分析高维数据时难以捕捉语义有意义的结构。

Method: 引入基于度量学习的无秩张量分解框架，用基于相似度的判别式优化替代重建目标，通过优化带正则化的三元组损失学习数据驱动的嵌入。

Result: 在多个领域的评估中，该方法在聚类指标上有显著提升，优于PCA、t - SNE等基线技术，在小数据集上比基于Transformer的方法表现好。

Conclusion: 确立度量学习为基于张量分析的范式，优先考虑语义相关性，在数据稀缺场景有计算优势。

Abstract: Tensor decomposition faces fundamental challenges in analyzing
high-dimensional data, where traditional methods based on reconstruction and
fixed-rank constraints often fail to capture semantically meaningful
structures. This paper introduces a no-rank tensor decomposition framework
grounded in metric learning, which replaces reconstruction objectives with a
discriminative, similarity-based optimization. The proposed approach learns
data-driven embeddings by optimizing a triplet loss with diversity and
uniformity regularization, creating a feature space where distance directly
reflects semantic similarity. We provide theoretical guarantees for the
framework's convergence and establish bounds on its metric properties.
Evaluations across diverse domains --including face recognition (LFW,
Olivetti), brain connectivity analysis (ABIDE), and simulated data (galaxy
morphology, crystal structures)-- demonstrate that our method outperforms
baseline techniques, including PCA, t-SNE, UMAP, and tensor decomposition
baselines (CP and Tucker). Results show substantial improvements in clustering
metrics (Silhouette Score, Davies--Bouldin Index, Calinski--Harabasz Index,
Separation Ratio, Adjusted Rand Index, Normalized Mutual Information) and
reveal a fundamental trade-off: while metric learning optimizes global class
separation, it deliberately transforms local geometry to align with semantic
relationships. Crucially, our approach achieves superior performance with
smaller training datasets compared to transformer-based methods, offering an
efficient alternative for domains with limited labeled data. This work
establishes metric learning as a paradigm for tensor-based analysis,
prioritizing semantic relevance over pixel-level fidelity while providing
computational advantages in data-scarce scenarios.

</details>


### [300] [Machine and Deep Learning for Indoor UWB Jammer Localization](https://arxiv.org/abs/2511.01819)
*Hamed Fard,Mahsa Kholghi,Benedikt Groß,Gerhard Wunder*

Main category: cs.LG

TL;DR: 本文引入UWB数据集建立ML/DL基线，评估模型性能，发现域偏移问题，提出A - CNT框架缓解性能下降，实现鲁棒可迁移的室内干扰器定位。


<details>
  <summary>Details</summary>
Motivation: UWB定位易受干扰攻击，现有方法在单房间和不同室内布局下定位恶意干扰器的研究不足。

Method: 引入两个UWB数据集建立ML/DL基线，用多种指标评估性能，提出域对抗ConvNeXt自编码器（A - CNT）框架缓解域偏移。

Result: 源数据集上Random Forest的F1 - macro分数最高为0.95，XGBoost的平均欧氏误差最低为20.16 cm；布局改变后性能下降，A - CNT框架将平均欧氏误差降至34.67 cm，有显著提升。

Conclusion: 对抗性特征对齐能实现鲁棒且可迁移的室内干扰器定位，不受环境变化影响。

Abstract: Ultra-wideband (UWB) localization delivers centimeter-scale accuracy but is
vulnerable to jamming attacks, creating security risks for asset tracking and
intrusion detection in smart buildings. Although machine learning (ML) and deep
learning (DL) methods have improved tag localization, localizing malicious
jammers within a single room and across changing indoor layouts remains largely
unexplored. Two novel UWB datasets, collected under original and modified room
configurations, are introduced to establish comprehensive ML/DL baselines.
Performance is rigorously evaluated using a variety of classification and
regression metrics. On the source dataset with the collected UWB features,
Random Forest achieves the highest F1-macro score of 0.95 and XGBoost achieves
the lowest mean Euclidean error of 20.16 cm. However, deploying these
source-trained models in the modified room layout led to severe performance
degradation, with XGBoost's mean Euclidean error increasing tenfold to 207.99
cm, demonstrating significant domain shift. To mitigate this degradation, a
domain-adversarial ConvNeXt autoencoder (A-CNT) is proposed that leverages a
gradient-reversal layer to align CIR-derived features across domains. The A-CNT
framework restores localization performance by reducing the mean Euclidean
error to 34.67 cm. This represents a 77 percent improvement over
non-adversarial transfer learning and an 83 percent improvement over the best
baseline, restoring the fraction of samples within 30 cm to 0.56. Overall, the
results demonstrate that adversarial feature alignment enables robust and
transferable indoor jammer localization despite environmental changes. Code and
dataset available at https://github.com/afbf4c8996f/Jammer-Loc

</details>


### [301] [Towards Multi-Fidelity Scaling Laws of Neural Surrogates in CFD](https://arxiv.org/abs/2511.01830)
*Paul Setinek,Gianluca Galletti,Johannes Brandstetter*

Main category: cs.LG

TL;DR: 研究科学机器学习中数据保真度与成本的权衡，重新定义经典缩放定律，发现计算 - 性能缩放行为和最优保真度组合。


<details>
  <summary>Details</summary>
Motivation: 科学机器学习受训练数据生成成本高限制，需研究数据保真度与成本的权衡。

Method: 使用低和高保真度雷诺平均纳维 - 斯托克斯（RANS）模拟，重新定义经典缩放定律，将数据集轴分解为计算预算和数据集组成。

Result: 揭示计算 - 性能缩放行为，展示给定数据集配置下依赖预算的最优保真度组合。

Conclusion: 首次对多保真度神经代理数据集的经验缩放定律进行研究，为科学机器学习中高效计算的数据集生成提供实用考虑。

Abstract: Scaling laws describe how model performance grows with data, parameters and
compute. While large datasets can usually be collected at relatively low cost
in domains such as language or vision, scientific machine learning is often
limited by the high expense of generating training data through numerical
simulations. However, by adjusting modeling assumptions and approximations,
simulation fidelity can be traded for computational cost, an aspect absent in
other domains. We investigate this trade-off between data fidelity and cost in
neural surrogates using low- and high-fidelity Reynolds-Averaged Navier-Stokes
(RANS) simulations. Reformulating classical scaling laws, we decompose the
dataset axis into compute budget and dataset composition. Our experiments
reveal compute-performance scaling behavior and exhibit budget-dependent
optimal fidelity mixes for the given dataset configuration. These findings
provide the first study of empirical scaling laws for multi-fidelity neural
surrogate datasets and offer practical considerations for compute-efficient
dataset generation in scientific machine learning.

</details>


### [302] [Dynamic Routing Between Experts: A Data-Efficient Approach to Continual Learning in Vision-Language Models](https://arxiv.org/abs/2511.01831)
*Jay Mohta,Kenan Emir Ak,Dimitrios Dimitriadis,Yan Xu,Mingwei Shen*

Main category: cs.LG

TL;DR: 提出基于路由的方法，在集成新任务时保留预训练基础能力，评估表明该方法有效且避免传统多任务学习的开销，还能实现跨模态迁移。


<details>
  <summary>Details</summary>
Motivation: 解决视觉 - 语言模型顺序微调新任务时的灾难性遗忘问题，避免传统多任务学习的高开销。

Method: 引入基于路由的方法，利用InternVL - 2模型进行评估，开展消融实验。

Result: 路由方法保留模型基础能力，提升特定任务准确性，避免传统多任务学习的开销，对任务数量增长有韧性，语义相关任务表现好，实现跨模态迁移。

Conclusion: 基于路由的方法能在集成新任务时保留基础能力，避免高开销，还具有良好的可扩展性、鲁棒性和跨模态迁移能力。

Abstract: Vision-Language Models (VLMs) suffer from catastrophic forgetting when
sequentially fine-tuned on new tasks, degrading performance on previously
learned foundational and task-specific capabilities. While multi-task learning
can mitigate forgetting, it requires simultaneous access to all datasets and
imposes computational overhead that scales linearly with the number of tasks.
In this work, we introduce a routing-based approach that enables the
integration of new tasks while preserving the foundational knowledge acquired
during pretraining. We evaluate our method using InternVL-2 models (2B and 8B
parameters) and demonstrate that routing preserves the model's foundational
capabilities by maintaining performance on general-purpose benchmarks such as
ChartQA, MMBench, and DocVQA, while simultaneously improving accuracy on
specialized tasks. Importantly, our approach achieves this without requiring
concurrent access to data from all tasks, avoiding the significant
computational and data overhead associated with traditional multi-task
learning. We further conduct extensive ablation studies to evaluate the
scalability and robustness of routing-based learning, showing that the approach
is resilient to a growing number of tasks and performs particularly well when
new tasks are semantically related. Finally, we show that the routing mechanism
enables superior cross-modal transfer between language and vision capabilities,
allowing knowledge learned in one modality to enhance performance in another
capability not achieved by existing continual learning methods.

</details>


### [303] [Priors in Time: Missing Inductive Biases for Language Model Interpretability](https://arxiv.org/abs/2511.01836)
*Ekdeep Singh Lubana,Can Rager,Sai Sumedh R. Hindupur,Valerie Costa,Greta Tuckute,Oam Patel,Sonia Krishna Murthy,Thomas Fel,Daniel Wurgaft,Eric J. Bigelow,Johnny Lin,Demba Ba,Martin Wattenberg,Fernanda Viegas,Melanie Weber,Aaron Mueller*

Main category: cs.LG

TL;DR: 现有特征提取方法假设概念独立，可能无法捕捉语言的丰富时间结构，本文提出时间特征分析目标，能更好完成相关任务，强调设计解释工具需匹配数据的归纳偏差。


<details>
  <summary>Details</summary>
Motivation: 现有特征提取方法假设概念独立，不确定能否捕捉语言丰富时间结构，需新的解释目标。

Method: 从贝叶斯视角分析稀疏自编码器（SAEs）的先验假设，借鉴计算神经科学引入时间特征分析目标。

Result: 时间特征分析器能正确解析花园路径句子、识别事件边界等，而现有SAEs在这些任务中有明显缺陷。

Conclusion: 设计稳健解释工具需要与数据匹配的归纳偏差。

Abstract: Recovering meaningful concepts from language model activations is a central
aim of interpretability. While existing feature extraction methods aim to
identify concepts that are independent directions, it is unclear if this
assumption can capture the rich temporal structure of language. Specifically,
via a Bayesian lens, we demonstrate that Sparse Autoencoders (SAEs) impose
priors that assume independence of concepts across time, implying stationarity.
Meanwhile, language model representations exhibit rich temporal dynamics,
including systematic growth in conceptual dimensionality, context-dependent
correlations, and pronounced non-stationarity, in direct conflict with the
priors of SAEs. Taking inspiration from computational neuroscience, we
introduce a new interpretability objective -- Temporal Feature Analysis --
which possesses a temporal inductive bias to decompose representations at a
given time into two parts: a predictable component, which can be inferred from
the context, and a residual component, which captures novel information
unexplained by the context. Temporal Feature Analyzers correctly parse garden
path sentences, identify event boundaries, and more broadly delineate abstract,
slow-moving information from novel, fast-moving information, while existing
SAEs show significant pitfalls in all the above tasks. Overall, our results
underscore the need for inductive biases that match the data in designing
robust interpretability tools.

</details>


### [304] [Interpretable Machine Learning for Reservoir Water Temperatures in the U.S. Red River Basin of the South](https://arxiv.org/abs/2511.01837)
*Isabela Suaza-Sierra,Hernan A. Moreno,Luis A De la Fuente,Thomas M. Neeson*

Main category: cs.LG

TL;DR: 结合可解释机器学习和符号建模研究美国红河盆地十个水库水温动态驱动因素，实现高精度预测并获得可解释方程。


<details>
  <summary>Details</summary>
Motivation: 准确预测水库水温对可持续水管理等至关重要，但预测本身对物理过程洞察有限，需揭示驱动因素。

Method: 先使用集成和神经网络模型（RF、XGBoost、MLP）进行预测，用SHAP量化物理驱动因素贡献，再开发Kolmogorov Arnold Networks (KANs) 符号近似水温。

Result: 预测技能高（最佳RMSE = 1.20摄氏度，R^2 = 0.97）；KAN方程从单预测因子R^2 = 0.84提升到十个预测因子R^2 = 0.92；深度是关键次要预测因子，降水影响有限。

Conclusion: 该框架将黑箱模型转化为透明替代模型，推动水库热动力学的预测和理解。

Abstract: Accurate prediction of Reservoir Water Temperature (RWT) is vital for
sustainable water management, ecosystem health, and climate resilience. Yet,
prediction alone offers limited insight into the governing physical processes.
To bridge this gap, we integrated explainable machine learning (ML) with
symbolic modeling to uncover the drivers of RWT dynamics across ten reservoirs
in the Red River Basin, USA, using over 10,000 depth-resolved temperature
profiles. We first employed ensemble and neural models, including Random Forest
(RF), Extreme Gradient Boosting (XGBoost), and Multilayer Perceptron (MLP),
achieving high predictive skill (best RMSE = 1.20 degree Celsius, R^2 = 0.97).
Using SHAP (SHapley Additive exPlanations), we quantified the contribution of
physical drivers such as air temperature, depth, wind, and lake volume,
revealing consistent patterns across reservoirs. To translate these data-driven
insights into compact analytical expressions, we developed Kolmogorov Arnold
Networks (KANs) to symbolically approximate RWT. Ten progressively complex KAN
equations were derived, improving from R^2 = 0.84 using a single predictor
(7-day antecedent air temperature) to R^2 = 0.92 with ten predictors, though
gains diminished beyond five, highlighting a balance between simplicity and
accuracy. The resulting equations, dominated by linear and rational forms,
incrementally captured nonlinear behavior while preserving interpretability.
Depth consistently emerged as a secondary but critical predictor, whereas
precipitation had limited effect. By coupling predictive accuracy with
explanatory power, this framework demonstrates how KANs and explainable ML can
transform black-box models into transparent surrogates that advance both
prediction and understanding of reservoir thermal dynamics.

</details>


### [305] [Coordinate ascent neural Kalman-MLE for state estimation](https://arxiv.org/abs/2511.01855)
*Bettina Hanlon,Angel Garcia Fernandez*

Main category: cs.LG

TL;DR: 本文提出坐标上升算法，以监督方式用最大似然估计学习动态状态估计中的动态和测量模型，并用非线性卡尔曼滤波算法进行状态估计。


<details>
  <summary>Details</summary>
Motivation: 学习动态状态估计中的动态和测量模型。

Method: 采用坐标上升算法，以最大似然估计的监督方式学习，假设模型为高斯分布，学习神经网络参数和噪声协方差矩阵，测试阶段结合非线性卡尔曼滤波算法。

Result: 未提及。

Conclusion: 未提及。

Abstract: This paper presents a coordinate ascent algorithm to learn dynamic and
measurement models in dynamic state estimation using maximum likelihood
estimation in a supervised manner. In particular, the dynamic and measurement
models are assumed to be Gaussian and the algorithm learns the neural network
parameters that model the dynamic and measurement functions, and also the noise
covariance matrices. The trained dynamic and measurement models are then used
with a non-linear Kalman filter algorithm to estimate the state during the
testing phase.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [306] [Bio-Inspired Neuron Synapse Optimization for Adaptive Learning and Smart Decision-Making](https://arxiv.org/abs/2511.00042)
*Sreeja Singh,Tamal Ghosh*

Main category: cs.NE

TL;DR: 文章提出受神经机制启发的神经元突触优化算法（NSO），经CEC 2014测试集验证，NSO在收敛速度、鲁棒性和可扩展性上优于其他算法，有广泛应用前景。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法在复杂高维多峰搜索空间中存在陷入局部最优、收敛慢和效率低等问题，本文旨在解决这些局限。

Method: 提出NSO算法，有基于适应度的突触权重更新、自适应剪枝、全局和局部最优解双重引导等创新点，并使用CEC 2014测试集与其他元启发式算法对比。

Result: NSO在收敛速度、鲁棒性和可扩展性上始终优于HOA和其他主要算法，在复杂高维搜索空间中适应性和效率更佳。

Conclusion: NSO结合神经启发机制与动态资源分配，提升搜索性能并降低计算成本，为多方面研究和实际工程问题开辟道路。

Abstract: Purpose: Optimization challenges in science, engineering, and real-world
applications often involve complex, high-dimensional, and multimodal search
spaces. Traditional optimization methods frequently struggle with local optima
entrapment, slow convergence, and inefficiency in large-scale environments.
This study aims to address these limitations by proposing a novel optimization
algorithm inspired by neural mechanisms. Design/methodology/approach: The paper
introduces Neuron Synapse Optimization (NSO), a new metaheuristic algorithm
inspired by neural interactions. NSO features key innovations such as
fitness-based synaptic weight updates to improve search influence, adaptive
pruning to minimize computational overhead, and dual guidance from global and
local best solutions to balance exploration and exploitation. The algorithm was
benchmarked against popular metaheuristics and the recently published
Hippopotamus Optimization Algorithm (HOA) using the CEC 2014 test suite,
encompassing unimodal, multimodal, and composition function landscapes.
Findings: Benchmark results reveal that NSO consistently outperforms HOA and
other major algorithms in terms of convergence speed, robustness, and
scalability. NSO demonstrates superior adaptability and efficiency,
particularly in complex, high-dimensional search spaces. Originality: NSO
introduces a unique blend of neural-inspired mechanisms with dynamic resource
allocation, setting it apart from existing algorithms. Its innovative design
enhances search performance while reducing computational cost. With promising
applications in technology, healthcare, data science, and engineering, NSO
paves the way for future research into dynamic and multi-objective
optimization, machine learning hyperparameter tuning, and real-world
engineering design problems.

</details>


### [307] [Node Preservation and its Effect on Crossover in Cartesian Genetic Programming](https://arxiv.org/abs/2511.00634)
*Mark Kocherovsky,Illya Bakurov,Wolfgang Banzhaf*

Main category: cs.NE

TL;DR: 本文比较了CGP中基本交叉方法和节点保留的交叉方法，以及节点变异算子和传统点变异，发现节点保留在变异和交叉中能改善搜索，推动CGP交叉通用解决方案。


<details>
  <summary>Details</summary>
Motivation: 在CGP中，交叉常被认为会降低搜索性能，现有方法虽有改进但缺乏通用解决方案。

Method: 比较基本交叉方法（一点交叉和均匀交叉）与节点保留的交叉方法，以及节点变异算子和传统点变异。

Result: 使用符号回归基准问题，发现节点保留在变异和交叉中能改善搜索。

Conclusion: 节点保留有助于推动CGP交叉的通用解决方案。

Abstract: While crossover is a critical and often indispensable component in other
forms of Genetic Programming, such as Linear- and Tree-based, it has
consistently been claimed that it deteriorates search performance in CGP. As a
result, a mutation-alone $(1+\lambda)$ evolutionary strategy has become the
canonical approach for CGP. Although several operators have been developed that
demonstrate an increased performance over the canonical method, a general
solution to the problem is still lacking. In this paper, we compare basic
crossover methods, namely one-point and uniform, to variants in which nodes are
``preserved,'' including the subgraph crossover developed by Roman Kalkreuth,
the difference being that when ``node preservation'' is active, crossover is
not allowed to break apart instructions. We also compare a node mutation
operator to the traditional point mutation; the former simply replaces an
entire node with a new one. We find that node preservation in both mutation and
crossover improves search using symbolic regression benchmark problems, moving
the field towards a general solution to CGP crossover.

</details>


### [308] [FeNN-DMA: A RISC-V SoC for SNN acceleration](https://arxiv.org/abs/2511.00732)
*Zainab Aizaz,James C. Knight,Thomas Nowotny*

Main category: cs.NE

TL;DR: 开发基于RISC - V的芯片系统FeNN - DMA在FPGA上模拟SNNs，资源使用和能耗与固定功能加速器相当，能模拟更复杂模型并在相关任务达最优分类准确率。


<details>
  <summary>Details</summary>
Motivation: SNNs算术强度低，不适合GPU和TPU等标准加速器，而FPGA适合内存受限工作负载，需开发适合在FPGA上模拟SNNs的系统。

Method: 开发基于RISC - V的全可编程片上系统FeNN - DMA，用于在现代UltraScale+ FPGAs上模拟SNNs。

Result: FeNN - DMA资源使用和能耗与现有固定功能SNN加速器相当，能模拟更大更复杂模型，并在Spiking Heidelberg Digits和Neuromorphic MNIST任务上取得最优分类准确率。

Conclusion: FeNN - DMA是在FPGA上模拟SNNs的有效方案，有良好性能和应用潜力。

Abstract: Spiking Neural Networks (SNNs) are a promising, energy-efficient alternative
to standard Artificial Neural Networks (ANNs) and are particularly well-suited
to spatio-temporal tasks such as keyword spotting and video classification.
However, SNNs have a much lower arithmetic intensity than ANNs and are
therefore not well-matched to standard accelerators like GPUs and TPUs. Field
Programmable Gate Arrays(FPGAs) are designed for such memory-bound workloads
and here we develop a novel, fully-programmable RISC-V-based system-on-chip
(FeNN-DMA), tailored to simulating SNNs on modern UltraScale+ FPGAs. We show
that FeNN-DMA has comparable resource usage and energy requirements to
state-of-the-art fixed-function SNN accelerators, yet it is capable of
simulating much larger and more complex models. Using this functionality, we
demonstrate state-of-the-art classification accuracy on the Spiking Heidelberg
Digits and Neuromorphic MNIST tasks.

</details>


### [309] [Trust Region-Based Bayesian Optimisation to Discover Diverse Solutions](https://arxiv.org/abs/2511.00750)
*Kokila Kasuni Perera,Frank Neumann,Aneta Neumann*

Main category: cs.NE

TL;DR: 本文探索基于信任区域的贝叶斯优化算法在不同维度黑盒问题多样性优化中的有效性，提出扩展算法并实验验证其在大维度问题上表现良好。


<details>
  <summary>Details</summary>
Motivation: 受近期使用信任区域提高贝叶斯优化方法可扩展性研究的启发，探索基于信任区域的贝叶斯优化算法在不同维度黑盒问题多样性优化中的有效性。

Method: 扩展TuRBO1得到divTuRBO1，提出两种结合divTuRBO1运行的方法来寻找黑盒函数的多样解，并与基线方法ROBOT进行对比实验。

Result: 在2到20维的基准函数上实验表明，提出的方法表现良好，特别是在大维度问题上，即使评估预算有限。

Conclusion: 提出的基于信任区域的贝叶斯优化算法在不同维度黑盒问题多样性优化中有效，尤其在大维度问题上表现出色。

Abstract: Bayesian optimisation (BO) is a surrogate-based optimisation technique that
efficiently solves expensive black-box functions with small evaluation budgets.
Recent studies consider trust regions to improve the scalability of BO
approaches when the problem space scales to more dimensions. Motivated by this
research, we explore the effectiveness of trust region-based BO algorithms for
diversity optimisation in different dimensional black box problems. We propose
diversity optimisation approaches extending TuRBO1, which is the first BO
method that uses a trust region-based approach for scalability. We extend
TuRBO1 as divTuRBO1, which finds an optimal solution while maintaining a given
distance threshold relative to a reference solution set. We propose two
approaches to find diverse solutions for black-box functions by combining
divTuRBO1 runs in a sequential and an interleaving fashion. We conduct
experimental investigations on the proposed algorithms and compare their
performance with that of the baseline method, ROBOT (rank-ordered Bayesian
optimisation with trust regions). We evaluate proposed algorithms on benchmark
functions with dimensions 2 to 20. Experimental investigations demonstrate that
the proposed methods perform well, particularly in larger dimensions, even with
a limited evaluation budget.

</details>


### [310] [Automatic Policy Search using Population-Based Hyper-heuristics for the Integrated Procurement and Perishable Inventory Problem](https://arxiv.org/abs/2511.00762)
*Leonardo Kanashiro Felizardo,Edoardo Fadda,Mariá Cristina Vasconcelos Nascimento*

Main category: cs.NE

TL;DR: 本文研究不确定条件下易腐库存管理问题，对比两种优化策略，超启发式框架表现更优，验证按项构建策略有性能优势。


<details>
  <summary>Details</summary>
Motivation: 解决随机需求、供应商交付不可靠和产品保质期不确定等多源不确定性下的易腐库存管理问题。

Method: 开发离散事件仿真环境，对比两种策略，一是全局调整经典策略参数并选择供应商，二是基于元启发式的超启发式方法按项构建复合策略。

Result: 12个实例计算结果表明超启发式框架能持续找到更优策略，GA和EGA整体表现最佳。

Conclusion: 按项构建策略比简单全局策略有显著性能提升，证明计算成本合理。

Abstract: This paper addresses the problem of managing perishable inventory under
multiple sources of uncertainty, including stochastic demand, unreliable
supplier fulfillment, and probabilistic product shelf life. We develop a
discrete-event simulation environment to compare two optimization strategies
for this multi-item, multi-supplier problem. The first strategy optimizes
uniform classic policies (e.g., Constant Order and Base Stock) by tuning their
parameters globally, complemented by a direct search to select the best-fitting
suppliers for the integrated problem. The second approach is a hyper-heuristic
approach, driven by metaheuristics such as a Genetic Algorithm (GA) and
Particle Swarm Optimization (PSO). This framework constructs a composite policy
by automating the selection of the heuristic type, its parameters, and the
sourcing suppliers on an item-by-item basis. Computational results from twelve
distinct instances demonstrate that the hyper-heuristic framework consistently
identifies superior policies, with GA and EGA exhibiting the best overall
performance. Our primary contribution is verifying that this item-level policy
construction yields significant performance gains over simpler global policies,
thereby justifying the associated computational cost.

</details>


### [311] [A High-Throughput Spiking Neural Network Processor Enabling Synaptic Delay Emulation](https://arxiv.org/abs/2511.01158)
*Faquan Chen,Qingyang Tian,Ziren Wu,Rendong Ying,Fei Wen,Peilin Liu*

Main category: cs.NE

TL;DR: 本文提出支持基于突触延迟仿真的高通量脉冲神经网络（SNN）处理器，在FPGA平台开发原型并评估性能，取得93.4%准确率和104样本/秒吞吐量。


<details>
  <summary>Details</summary>
Motivation: 突触延迟在神经网络动力学中对处理复杂时空信息很重要，为边缘应用开发支持突触延迟仿真的处理器。

Method: 采用多核流水线架构和并行计算引擎，在PYNQ Z2 FPGA平台开发SoC原型，用SHD基准测试评估。

Result: 处理器在部署中达到93.4%准确率，在125 MHz典型工作频率和282 mW功耗下平均吞吐量为104样本/秒。

Conclusion: 所提出的支持突触延迟仿真的SNN处理器能有效处理相关计算负载，在低功耗关键词识别任务中有较好表现。

Abstract: Synaptic delay has attracted significant attention in neural network dynamics
for integrating and processing complex spatiotemporal information. This paper
introduces a high-throughput Spiking Neural Network (SNN) processor that
supports synaptic delay-based emulation for edge applications. The processor
leverages a multicore pipelined architecture with parallel compute engines,
capable of real-time processing of the computational load associated with
synaptic delays. We develop a SoC prototype of the proposed processor on PYNQ
Z2 FPGA platform and evaluate its performance using the Spiking Heidelberg
Digits (SHD) benchmark for low-power keyword spotting tasks. The processor
achieves 93.4% accuracy in deployment and an average throughput of 104
samples/sec at a typical operating frequency of 125 MHz and 282 mW power
consumption.

</details>


### [312] [Space as Time Through Neuron Position Learning](https://arxiv.org/abs/2511.01632)
*Balázs Mészáros,James C. Knight,Danyal Akarca,Thomas Nowotny*

Main category: cs.NE

TL;DR: 提出新颖神经元位置学习算法统一空间嵌入和可学习突触延迟，训练网络自组织成特定拓扑，有功能专业化，为时空耦合网络奠基。


<details>
  <summary>Details</summary>
Motivation: 生物神经网络存在时空耦合，多数人工神经网络缺乏，现有工作分别探索空间嵌入和可学习突触延迟，需统一。

Method: 提出新颖神经元位置学习算法，推导神经元位置梯度。

Result: 网络在时间分类任务训练中自组织成局部、小世界拓扑，有模块化结构，出现未显式强制的功能专业化。

Conclusion: 为时空内在耦合网络奠定基础，为可解释性、生物启发建模和高效实现提供新途径。

Abstract: Biological neural networks exist in physical space where distance determines
communication delays: a fundamental space-time coupling absent in most
artificial neural networks. While recent work has separately explored spatial
embeddings and learnable synaptic delays in spiking neural networks, we unify
these approaches through a novel neuron position learning algorithm where
delays relate to the Euclidean distances between neurons. We derive gradients
with respect to neuron positions and demonstrate that this
biologically-motivated constraint acts as an inductive bias: networks trained
on temporal classification tasks spontaneously self-organize into local,
small-world topologies with modular structure emerging under distance-dependent
connection costs. Remarkably, we observe unprompted functional specialization
aligned with spatial clustering without explictly enforcing it. These findings
lay the groundwork for networks in which space and time are intrinsically
coupled, offering new avenues for mechanistic interpretability, biologically
inspired modelling, and efficient implementations.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [313] [ScaleCall - Agentic Tool Calling at Scale for Fintech: Challenges, Methods, and Deployment Insights](https://arxiv.org/abs/2511.00074)
*Richard Osuagwu,Thomas Cook,Maraim Masoud,Koustav Ghosal,Riccardo Mattivi*

Main category: cs.SE

TL;DR: 本文围绕企业环境下大语言模型工具调用问题，开发ScaleCall框架评估多种工具检索方法，发现方法有效性依赖特定领域因素，为监管行业企业应用的工具调用系统设计提供见解。


<details>
  <summary>Details</summary>
Motivation: 在受监管的企业环境（如金融科技）中部署大语言模型的工具调用能力面临诸多挑战，需要研究合适的工具检索方法。

Method: 开发并部署ScaleCall框架，系统评估基于嵌入的检索、基于提示的列表排序和混合方法。

Result: 方法有效性很大程度取决于特定领域因素，基于嵌入的方法在大型工具库中延迟更低，列表排序在功能重叠时消歧效果更好，混合方法在特定场景有潜力。

Conclusion: 研究为监管行业企业应用的工具调用系统设计提供了关于检索准确性、计算效率和操作要求之间权衡的实用见解。

Abstract: While Large Language Models (LLMs) excel at tool calling, deploying these
capabilities in regulated enterprise environments such as fintech presents
unique challenges due to on-premises constraints, regulatory compliance
requirements, and the need to disambiguate large, functionally overlapping
toolsets. In this paper, we present a comprehensive study of tool retrieval
methods for enterprise environments through the development and deployment of
ScaleCall, a prototype tool-calling framework within Mastercard designed for
orchestrating internal APIs and automating data engineering workflows. We
systematically evaluate embedding-based retrieval, prompt-based listwise
ranking, and hybrid approaches, revealing that method effectiveness depends
heavily on domain-specific factors rather than inherent algorithmic
superiority. Through empirical investigation on enterprise-derived benchmarks,
we find that embedding-based methods offer superior latency for large tool
repositories, while listwise ranking provides better disambiguation for
overlapping functionalities, with hybrid approaches showing promise in specific
contexts. We integrate our findings into ScaleCall's flexible architecture and
validate the framework through real-world deployment in Mastercard's regulated
environment. Our work provides practical insights into the trade-offs between
retrieval accuracy, computational efficiency, and operational requirements,
contributing to the understanding of tool-calling system design for enterprise
applications in regulated industries.

</details>


### [314] [Adding New Capability in Existing Scientific Application with LLM Assistance](https://arxiv.org/abs/2511.00087)
*Anshu Dubey,Akash Dhruv*

Main category: cs.SE

TL;DR: 本文提出使用大语言模型辅助为新算法从头编写代码的新方法，并描述对代码翻译工具Code - Scribe的改进。


<details>
  <summary>Details</summary>
Motivation: 大语言模型兴起下自动化编码任务成研究热点，但新算法代码生成方面探索较少。

Method: 提出新的使用大语言模型辅助为新算法从头编写代码的方法，改进代码翻译工具Code - Scribe用于新代码生成。

Result: 未提及

Conclusion: 未提及

Abstract: With the emergence and rapid evolution of large language models (LLM),
automating coding tasks has become an im- portant research topic. Many efforts
are underway and liter- ature abounds about the efficacy of models and their
ability to generate code. A less explored aspect of code generation is for new
algorithms, where the training data-set would not have included any previous
example of similar code. In this paper we propose a new methodology for writing
code from scratch for a new algorithm using LLM assistance, and describe
enhancement of a previously developed code- translation tool, Code-Scribe, for
new code generation.

</details>


### [315] [Inferring multiple helper Dafny assertions with LLMs](https://arxiv.org/abs/2511.00125)
*Álvaro Silva,Alexandra Mendes,Ruben Martins*

Main category: cs.SE

TL;DR: 研究用大语言模型自动推断Dafny程序中缺失的辅助断言，实现DAISY工具，验证部分程序，表明自动断言推断可减少证明工程工作量。


<details>
  <summary>Details</summary>
Motivation: Dafny验证器需大量手动辅助断言，阻碍其应用，因此研究用大语言模型自动推断缺失断言。

Method: 扩展DafnyBench基准，引入断言类型分类，用结合LLM预测和错误消息启发式的混合方法改进故障定位，实现DAISY工具。

Result: DAISY验证了63.4%有一个缺失断言的程序和31.7%有多个缺失断言的程序，很多程序可用更少断言验证。

Conclusion: 自动断言推断能大幅减少证明工程工作量，是迈向更可扩展和易访问形式验证的一步。

Abstract: The Dafny verifier provides strong correctness guarantees but often requires
numerous manual helper assertions, creating a significant barrier to adoption.
We investigate the use of Large Language Models (LLMs) to automatically infer
missing helper assertions in Dafny programs, with a primary focus on cases
involving multiple missing assertions. To support this study, we extend the
DafnyBench benchmark with curated datasets where one, two, or all assertions
are removed, and we introduce a taxonomy of assertion types to analyze
inference difficulty. Our approach refines fault localization through a hybrid
method that combines LLM predictions with error-message heuristics. We
implement this approach in a new tool called DAISY (Dafny Assertion Inference
SYstem). While our focus is on multiple missing assertions, we also evaluate
DAISY on single-assertion cases. DAISY verifies 63.4% of programs with one
missing assertion and 31.7% with multiple missing assertions. Notably, many
programs can be verified with fewer assertions than originally present,
highlighting that proofs often admit multiple valid repair strategies and that
recovering every original assertion is unnecessary. These results demonstrate
that automated assertion inference can substantially reduce proof engineering
effort and represent a step toward more scalable and accessible formal
verification.

</details>


### [316] [What a diff makes: automating code migration with large language models](https://arxiv.org/abs/2511.00160)
*Katherine A. Rosenfeld,Cliff C. Kerr,Jessica Lundin*

Main category: cs.SE

TL;DR: 探索使用大语言模型进行代码迁移，证明含diff的上下文可提升性能，提供数据集和开源包AIMigrate，在实际迁移中有较好表现。


<details>
  <summary>Details</summary>
Motivation: 现代软件栈常变化，可能破坏依赖项目，需解决代码迁移中与依赖保持兼容的问题。

Method: 利用测试覆盖率和变更比较等指标，对比含diff的上下文与普通大语言模型的性能。

Result: 含diff的上下文能显著提升性能，AIMigrate在TYPHOIDSIM迁移中，单次运行识别65%的必要变更，多次运行达80%，47%的变更完美生成。

Conclusion: 含diff的上下文对代码迁移有积极作用，AIMigrate可辅助代码库迁移。

Abstract: Modern software programs are built on stacks that are often undergoing
changes that introduce updates and improvements, but may also break any project
that depends upon them. In this paper we explore the use of Large Language
Models (LLMs) for code migration, specifically the problem of maintaining
compatibility with a dependency as it undergoes major and minor semantic
version changes. We demonstrate, using metrics such as test coverage and change
comparisons, that contexts containing diffs can significantly improve
performance against out of the box LLMs and, in some cases, perform better than
using code. We provide a dataset to assist in further development of this
problem area, as well as an open-source Python package, AIMigrate, that can be
used to assist with migrating code bases. In a real-world migration of
TYPHOIDSIM between STARSIM versions, AIMigrate correctly identified 65% of
required changes in a single run, increasing to 80% with multiple runs, with
47% of changes generated perfectly.

</details>


### [317] [Understanding Code Agent Behaviour: An Empirical Study of Success and Failure Trajectories](https://arxiv.org/abs/2511.00197)
*Oorja Majgaonkar,Zhiwei Fei,Xiang Li,Federica Sarro,He Ye*

Main category: cs.SE

TL;DR: 本文对大语言模型代理解决软件问题的轨迹进行实证研究，揭示代理行为关键见解，为开发更强大可解释的软件工程系统提供基础。


<details>
  <summary>Details</summary>
Motivation: 大语言模型代理用于软件工程任务时，其决策过程不透明，需理解其解决问题的行为。

Method: 对三个先进代码代理在SWE - Bench基准上的轨迹进行分析，包括成功和失败的尝试。

Result: 发现不同解决策略在不同场景促成成功；失败轨迹更长、方差更大，且不同代理失败模式不同；多数轨迹能正确识别问题文件，成功更依赖近似代码修改。

Conclusion: 通过轨迹分析理解代理行为，有助于开发更强大、可解释的自主软件工程系统。

Abstract: The increasing deployment of Large Language Model (LLM) agents for complex
software engineering tasks has created a need to understand their
problem-solving behaviours beyond simple success metrics. While these agents
demonstrate impressive capabilities in automated issue resolution, their
decision-making processes remain largely opaque. This paper presents an
empirical study of agent trajectories, namely the execution traces capturing
the steps agents take when attempting to resolve software issues. We analyse
trajectories from three state-of-the-art code agents (OpenHands, SWE-agent, and
Prometheus) on the SWE-Bench benchmark, examining both successful and failed
attempts. Our investigation reveals several key insights into agent behaviour.
First, we identify how distinct problem-solving strategies, such as defensive
programming and context gathering, enable success in different scenarios.
Second, we find that failed trajectories are consistently longer and exhibit
higher variance than successful ones, with failure patterns differing
significantly between agents. Third, our fault localisation analysis shows that
while most trajectories correctly identify problematic files (72-81\% even in
failures), success depends more on achieving approximate rather than exact code
modifications. These and other findings unveiled by our study, provide a
foundation for understanding agent behaviour through trajectory analysis,
contributing to the development of more robust and interpretable autonomous
software engineering systems.

</details>


### [318] [Position: Vibe Coding Needs Vibe Reasoning: Improving Vibe Coding with Formal Verification](https://arxiv.org/abs/2511.00202)
*Jacqueline Mitchell,Yasser Shaaban*

Main category: cs.SE

TL;DR: Vibe coding with LLMs is popular but has limitations. Formal methods can mitigate these issues, and a side - car system is proposed.


<details>
  <summary>Details</summary>
Motivation: To address limitations in vibe coding such as technical debt, security issues, and code churn caused by LLMs' inability to handle human - imposed constraints.

Method: Advocate for a side - car system during vibe coding that autoformalizes specifications, validates against targets, delivers actionable feedback to the LLM, and allows developer influence on specifications.

Result: Not mentioned in the abstract.

Conclusion: Formal methods can make vibe coding more reliable, and the proposed side - car system can transcend existing approaches to integrate formal methods and LLMs.

Abstract: ``Vibe coding'' -- the practice of developing software through iteratively
conversing with a large language model (LLM) -- has exploded in popularity
within the last year. However, developers report key limitations including the
accumulation of technical debt, security issues, and code churn to achieve
satisfactory results. We argue that these pitfalls result from LLMs' inability
to reconcile accumulating human-imposed constraints during vibe coding, with
developers inadvertently failing to resolve contradictions because LLMs
prioritize user commands over code consistency. Given LLMs' receptiveness to
verification-based feedback, we argue that formal methods can mitigate these
pitfalls, making vibe coding more reliable. However, we posit that integrating
formal methods must transcend existing approaches that combine formal methods
and LLMs. We advocate for a side-car system throughout the vibe coding process
which: (1) \emph{Autoformalizes} specifications (2) Validates against targets,
(3) Delivers \emph{actionable} feedback to the LLM, and (4) Allows intuitive
developer influence on specifications.

</details>


### [319] [DocPrism: Local Categorization and External Filtering to Identify Relevant Code-Documentation Inconsistencies](https://arxiv.org/abs/2511.00215)
*Xiaomeng Xu,Zahin Wahab,Reid Holmes,Caroline Lemieux*

Main category: cs.SE

TL;DR: 本文介绍多语言代码文档不一致检测工具DocPrism，用LCEF方法降低误报率，效果显著。


<details>
  <summary>Details</summary>
Motivation: 解决代码 - 文档不一致导致开发者误解和软件缺陷的问题。

Method: 引入DocPrism工具，使用标准大语言模型分析不一致，应用LCEF方法降低误报率。

Result: 消融实验中LCEF将DocPrism误报率从98%降至14%，准确率从14%提升到94%；多语言评估中保持15%低误报率，精度达0.62。

Conclusion: DocPrism能有效检测代码 - 文档不一致，LCEF方法能显著降低误报率。

Abstract: Code-documentation inconsistencies are common and undesirable: they can lead
to developer misunderstandings and software defects. This paper introduces
DocPrism, a multi-language, code-documentation inconsistency detection tool.
DocPrism uses a standard large language model (LLM) to analyze and explain
inconsistencies. Plain use of LLMs for this task yield unacceptably high false
positive rates: LLMs identify natural gaps between high-level documentation and
detailed code implementations as inconsistencies. We introduce and apply the
Local Categorization, External Filtering (LCEF) methodology to reduce false
positives. LCEF relies on the LLM's local completion skills rather than its
long-term reasoning skills. In our ablation study, LCEF reduces DocPrism's
inconsistency flag rate from 98% to 14%, and increases accuracy from 14% to
94%. On a broad evaluation across Python, TypeScript, C++, and Java, DocPrism
maintains a low flag rate of 15%, and achieves a precision of 0.62 without
performing any fine-tuning.

</details>


### [320] [LLM-Driven Cost-Effective Requirements Change Impact Analysis](https://arxiv.org/abs/2511.00262)
*Romina Etezadi,Sallam Abualhaija,Chetan Arora,Lionel Briand*

Main category: cs.SE

TL;DR: 提出基于大语言模型的ProReFiCIA方法自动识别需求变更影响，评估效果好且成本低。


<details>
  <summary>Details</summary>
Motivation: 手动识别需求变更影响易出错且耗费精力，可能导致下游任务出现严重问题。

Method: 提出ProReFiCIA方法，用多种大语言模型和提示变体进行评估。

Result: 在基准数据集上召回率达93.3%，新行业数据集达95.8%，工程师只需审查2.1% - 8.5%的需求。

Conclusion: ProReFiCIA在识别受影响需求方面非常有效，且应用成本低。

Abstract: Requirements are inherently subject to changes throughout the software
development lifecycle. Within the limited budget available to requirements
engineers, manually identifying the impact of such changes on other
requirements is both error-prone and effort-intensive. That might lead to
overlooked impacted requirements, which, if not properly managed, can cause
serious issues in the downstream tasks. Inspired by the growing potential of
large language models (LLMs) across diverse domains, we propose ProReFiCIA, an
LLM-driven approach for automatically identifying the impacted requirements
when changes occur. We conduct an extensive evaluation of ProReFiCIA using
several LLMs and prompts variants tailored to this task. Using the best
combination of an LLM and a prompt variant, ProReFiCIA achieves a recall of
93.3% on a benchmark dataset and 95.8% on a newly created industry dataset,
demonstrating its strong effectiveness in identifying impacted requirements.
Further, the cost of applying ProReFiCIA remains small, as the engineer only
needs to review the generated results, which represent between 2.1% and 8.5% of
the entire set of requirements.

</details>


### [321] [Human-AI Programming Role Optimization: Developing a Personality-Driven Self-Determination Framework](https://arxiv.org/abs/2511.00417)
*Marcel Valovy*

Main category: cs.SE

TL;DR: 论文通过自决理论和人格心理学优化人机编程角色，提出ROMA框架，经研究得出人格驱动的角色优化能提升自决和团队动力等成果，并做出多方面贡献。


<details>
  <summary>Details</summary>
Motivation: 探讨开发者与AI系统如何最有效地协作，优化人机编程角色。

Method: 采用设计科学研究，历经五个周期，让200名实验参与者和46名受访者参与研究。

Result: 人格驱动的角色优化显著提升自决和团队动力，专业人员平均动力提升23%，本科生最高提升65%，并得出五种人格原型及编程角色偏好。

Conclusion: 贡献了将人格特质与角色偏好和自决结果联系的框架、映射到人格轮廓的AI协作模式分类法，以及使小型实体能在标准内实现人格驱动角色优化的ISO/IEC 29110扩展。

Abstract: As artificial intelligence transforms software development, a critical
question emerges: how can developers and AI systems collaborate most
effectively? This dissertation optimizes human-AI programming roles through
self-determination theory and personality psychology, introducing the Role
Optimization Motivation Alignment (ROMA) framework.
  Through Design Science Research spanning five cycles, this work establishes
empirically-validated connections between personality traits, programming role
preferences, and collaborative outcomes, engaging 200 experimental participants
and 46 interview respondents.
  Key findings demonstrate that personality-driven role optimization
significantly enhances self-determination and team dynamics, yielding 23%
average motivation increases among professionals and up to 65% among
undergraduates. Five distinct personality archetypes emerge: The Explorer (high
Openness/low Agreeableness), The Orchestrator (high
Extraversion/Agreeableness), The Craftsperson (high Neuroticism/low
Extraversion), The Architect (high Conscientiousness), and The Adapter
(balanced profile). Each exhibits distinct preferences for programming roles
(Co-Pilot, Co-Navigator, Agent), with assignment modes proving crucial for
satisfaction.
  The dissertation contributes: (1) an empirically-validated framework linking
personality traits to role preferences and self-determination outcomes; (2) a
taxonomy of AI collaboration modalities mapped to personality profiles while
preserving human agency; and (3) an ISO/IEC 29110 extension enabling Very Small
Entities to implement personality-driven role optimization within established
standards.
  Keywords: artificial intelligence, human-computer interaction, behavioral
software engineering, self-determination theory, personality psychology,
phenomenology, intrinsic motivation, pair programming, design science research,
ISO/IEC 29110

</details>


### [322] [SmartDoc: A Context-Aware Agentic Method Comment Generation Plugin](https://arxiv.org/abs/2511.00450)
*Vahid Etemadi,Gregorio Robles*

Main category: cs.SE

TL;DR: 文章提出IntelliJ IDEA插件SmartDoc辅助开发者生成上下文感知的方法注释，介绍其实现方法并测试准确性，结果较有前景。


<details>
  <summary>Details</summary>
Motivation: 软件维护中程序理解依赖方法注释，但阅读完整方法语句有挑战，需要精确及时的注释，因此提出SmartDoc插件。

Method: 插件作为有自身记忆的AI代理，结合目标方法上下文，根据用户请求，利用方法内容和嵌套方法调用生成注释，生成调用图并用深度优先搜索遍历以丰富LLM提示。

Result: 开发出适用于Java代码库、可在IntelliJ IDEA安装的插件，可并发服务于更新注释的方法并共享内存。通过运行测试用例、计算BERTScore、BLEU和ROUGE - 1等指标，BERTScore的精确率、召回率和F1值在0.80到0.90之间。

Conclusion: SmartDoc插件在生成方法注释方面有较好的准确性。

Abstract: Context: The software maintenance phase involves many activities such as code
refactoring, bug fixing, code review or testing. Program comprehension is key
to all these activities, as it demands developers to grasp the knowledge (e.g.,
implementation details) required to modify the codebase. Methods as main
building blocks in a program can offer developers this knowledge source for
code comprehension. However, reading entire method statements can be
challenging, which necessitates precise and up-to-date comments. Objective: We
propose a solution as an IntelliJ IDEA plugin, named SmartDoc, that assists
developers in generating context-aware method comments. Method: This plugin
acts as an Artificial Intelligence (AI) agent that has its own memory and is
augmented by target methods' context. When a request is initiated by the
end-user, the method content and all its nested method calls are used in the
comment generation. At the beginning, these nested methods are visited and a
call graph is generated. This graph is then traversed using depth-first search
(DFS), enabling the provision of full-context to enrich Large Language Model
(LLM) prompts. Result: The product is a software, as a plugin, developed for
Java codebase and installable on IntelliJ IDEA. This plugin can serve
concurrently for methods whose comments are being updated , and it shares
memory across all flows to avoid redundant calls. o measure the accuracy of
this solution, a dedicated test case is run to record SmartDoc generated
comments and their corresponding ground truth. For each collected result-set,
three metrics are computed, BERTScore, BLEU and ROUGE-1. These metrics will
determine how accurate the generated comments are in comparison to the ground
truth. Result: The obtained accuracy, in terms of the precision, recall and F1,
is promising, and lies in the range of 0.80 to 0.90 for BERTScore.

</details>


### [323] [A Big Step Forward? A User-Centric Examination of iOS App Privacy Report and Enhancements](https://arxiv.org/abs/2511.00467)
*Liu Wang,Dong Wang,Shidong Pan,Zheng Jiang,Haoyu Wang,Yi Wang*

Main category: cs.SE

TL;DR: 文章对苹果App隐私报告功能进行端到端研究，发现其实践影响有限，提出增强方案并证明其有效性，为提升用户隐私透明度提供见解。


<details>
  <summary>Details</summary>
Motivation: 苹果推出App隐私报告功能，但其实践影响未被研究，需了解其利弊以提升用户隐私透明度。

Method: 进行端到端研究，通过结构化焦点小组研究日常iOS用户，提出增强方案。

Result: 发现该功能实践影响有限，确定用户主要关注点，提出的增强方案有效。

Conclusion: 工作为提升用户隐私透明度提供实用见解，指出未来研究方向。

Abstract: The prevalent engagement with mobile apps underscores the importance of
understanding their data practices. Transparency plays a crucial role in this
context, ensuring users to be informed and give consent before any data access
occurs. Apple introduced a new feature since iOS 15.2, App Privacy Report, to
inform users about detailed insights into apps' data access and sharing. This
feature continues Apple's trend of privacy-focused innovations (following
Privacy Nutrition Labels), and has been marketed as a big step forward in user
privacy. However, its real-world impacts on user privacy and control remain
unexamined. We thus proposed an end-to-end study involving systematic
assessment of the App Privacy Report's real-world benefits and limitations,
LLM-enabled and multi-technique synthesized enhancements, and comprehensive
evaluation from both system and user perspectives. Through a structured focus
group study with twelve everyday iOS users, we explored their experiences,
understanding, and perceptions of the feature, suggesting its limited practical
impact resulting from missing important details. We identified two primary user
concerns: the clarity of data access purpose and domain description. In
response, we proposed enhancements including a purpose inference framework and
domain clarification pipeline. We demonstrated the effectiveness and benefits
of such enhancements for mobile app users. This work provides practical
insights that could help enhance user privacy transparency and discusses areas
for future research.

</details>


### [324] [Issue-Oriented Agent-Based Framework for Automated Review Comment Generation](https://arxiv.org/abs/2511.00517)
*Shuochuan Li,Dong Wang,Patanamon Thongtanunam,Zan Wang,Jiuqiao Yu,Junjie Chen*

Main category: cs.SE

TL;DR: 提出RevAgent框架改进代码审查评论生成，评估显示其优于基线且实用高效。


<details>
  <summary>Details</summary>
Motivation: 现有代码审查评论生成技术依赖单一模型，处理复杂场景能力有限，评论缺乏信息。

Method: 提出RevAgent框架，分生成、判别、训练三阶段，用多个特定类别评论员代理和一个批评代理，并在特定类别数据上微调。

Result: RevAgent在多个指标上显著优于基线，问题类别识别准确率较高，人工评估验证其实用性，性能和效率平衡良好。

Conclusion: RevAgent能有效解决现有代码审查评论生成技术的局限性，是一个实用高效的框架。

Abstract: Code review (CR) is a crucial practice for ensuring software quality. Various
automated review comment generation techniques have been proposed to streamline
the labor-intensive process. However, existing approaches heavily rely on a
single model to identify various issues within the code, limiting the model's
ability to handle the diverse, issue-specific nature of code changes and
leading to non-informative comments, especially in complex scenarios such as
bug fixes. To address these limitations, we propose RevAgent, a novel
agent-based issue-oriented framework, decomposes the task into three stages:
(1) Generation Stage, where five category-specific commentator agents analyze
code changes from distinct issue perspectives and generate candidate comments;
(2) Discrimination Stage, where a critic agent selects the most appropriate
issue-comment pair; and (3) Training Stage, where all agents are fine-tuned on
curated, category-specific data to enhance task specialization. Evaluation
results show that RevAgent significantly outperforms state-of-the-art PLM- and
LLM-based baselines, with improvements of 12.90\%, 10.87\%, 6.32\%, and 8.57\%
on BLEU, ROUGE-L, METEOR, and SBERT, respectively. It also achieves relatively
higher accuracy in issue-category identification, particularly for challenging
scenarios. Human evaluations further validate the practicality of RevAgent in
generating accurate, readable, and context-aware review comments. Moreover,
RevAgent delivers a favorable trade-off between performance and efficiency.

</details>


### [325] [HIP-LLM: A Hierarchical Imprecise Probability Approach to Reliability Assessment of Large Language Models](https://arxiv.org/abs/2511.00527)
*Robab Aghazadeh-Chakherlou,Qing Guo,Siddartha Khastgir,Peter Popov,Xiaoge Zhang,Xingyu Zhao*

Main category: cs.SE

TL;DR: 本文介绍了用于建模和推断大语言模型（LLM）可靠性的HIP - LLM框架，实验表明其比现有方法更准确、标准化。


<details>
  <summary>Details</summary>
Motivation: 现有基于基准的评估方法对LLM在实际操作条件下的概率行为洞察有限，需要严格的可靠性评估方法。

Method: 引入HIP - LLM框架，基于软件可靠性工程，定义LLM可靠性，分层表示（子）领域依赖，嵌入不精确先验，结合操作剖面（OP）推导后验可靠性区间。

Result: 在多个基准数据集上的实验表明，HIP - LLM比现有基准和最先进的方法能更准确、标准化地表征可靠性。

Conclusion: HIP - LLM是一种有效的LLM可靠性评估方法，还提供了公共可访问的代码库。

Abstract: Large Language Models (LLMs) are increasingly deployed across diverse
domains, raising the need for rigorous reliability assessment methods. Existing
benchmark-based evaluations primarily offer descriptive statistics of model
accuracy over datasets, providing limited insight into the probabilistic
behavior of LLMs under real operational conditions. This paper introduces
HIP-LLM, a Hierarchical Imprecise Probability framework for modeling and
inferring LLM reliability. Building upon the foundations of software
reliability engineering, HIP-LLM defines LLM reliability as the probability of
failure-free operation over a specified number of future tasks under a given
Operational Profile (OP). HIP-LLM represents dependencies across (sub-)domains
hierarchically, enabling multi-level inference from subdomain to system-level
reliability. HIP-LLM embeds imprecise priors to capture epistemic uncertainty
and incorporates OPs to reflect usage contexts. It derives posterior
reliability envelopes that quantify uncertainty across priors and data.
Experiments on multiple benchmark datasets demonstrate that HIP-LLM offers a
more accurate and standardized reliability characterization than existing
benchmark and state-of-the-art approaches. A publicly accessible repository of
HIP-LLM is provided.

</details>


### [326] [Employee Performance when Implementing Agile Practices in an IT Workforce](https://arxiv.org/abs/2511.00528)
*Muhammad Hamid Raza Mookadam,Ridewaan Hanslo*

Main category: cs.SE

TL;DR: 研究南非IT员工在敏捷环境中的绩效，发现敏捷实践显著影响绩效，解决挑战可提升绩效。


<details>
  <summary>Details</summary>
Motivation: 非洲缺乏敏捷实践对员工绩效影响的综合研究，本研究填补南非IT员工这方面的研究空白。

Method: 采用解释主义单方法定性研究，对不同角色的敏捷从业者进行17次半结构化访谈。

Result: 敏捷实践显著影响员工绩效，涉及规划、沟通等方面；同时存在采用、团队参与等障碍。

Conclusion: 解决敏捷挑战并提供额外支持，可显著提高员工绩效。

Abstract: Adoption of agile practices has increased in IT workforces. However, there is
a lack of comprehensive studies in the African context on employee performance
when implementing agile practices. This study addresses this gap by exploring
employee performance in agile environments for IT workforces in South Africa.
An interpretivist mono-method qualitative approach was used, with the use of
interviews as a research strategy. Seventeen semi-structured interviews were
conducted with agile practitioners from various roles. Our results indicated
that agile practices influence employee performance significantly, with
participants reporting on aspects which included planning, communication,
employee development and well-being, collaboration, team culture and progress.
Additionally, our results reported obstacles when using agile practices that
included adoption, team engagement, leadership and instilling an agile mindset.
Agile practices influence employee performance in IT workforces by fostering
improved team dynamics, enhanced collaboration, improved efficiencies, risk
management, planning, continuous improvement, learning, personal development
and well-being. Conclusively, our findings suggest that if agile challenges are
addressed and additional support is provided, employee performance can be
significantly improved.

</details>


### [327] [GDPR-Bench-Android: A Benchmark for Evaluating Automated GDPR Compliance Detection in Android](https://arxiv.org/abs/2511.00619)
*Huaijin Ran,Haoyi Zhang,Xunzhu Tang*

Main category: cs.SE

TL;DR: 介绍首个评估安卓应用GDPR合规检测自动化方法的基准GDPR - Bench - Android，定义两项任务并对11种方法进行基准测试，结果显示不同方法在不同任务有优势。


<details>
  <summary>Details</summary>
Motivation: 自动化检测源代码中GDPR违规是关键但研究不足的挑战，需要评估自动化方法。

Method: 引入GDPR - Bench - Android基准，提出Formal - AST形式方法，定义多粒度违规定位和片段级多标签分类两项任务，对11种方法进行基准测试。

Result: 不同方法在不同任务表现不同，如ReAct代理在任务1文件级Accuracy@1最高，Qwen2.5 - 72B LLM在线级最高；Claude - Sonnet - 4.5 LLM在任务2 Macro - F1最佳，RAG方法Macro - Precision最高。

Conclusion: 不同自动化方法有任务依赖优势，GDPR - Bench - Android基准对诊断方法能力有价值。

Abstract: Automating the detection of EU General Data Protection Regulation (GDPR)
violations in source code is a critical but underexplored challenge. We
introduce \textbf{GDPR-Bench-Android}, the first comprehensive benchmark for
evaluating diverse automated methods for GDPR compliance detection in Android
applications. It contains \textbf{1951} manually annotated violation instances
from \textbf{15} open-source repositories, covering 23 GDPR articles at file-,
module-, and line-level granularities. To enable a multi-paradigm evaluation,
we contribute \textbf{Formal-AST}, a novel, source-code-native formal method
that serves as a deterministic baseline. We define two tasks: (1)
\emph{multi-granularity violation localization}, evaluated via
Accuracy@\textit{k}; and (2) \emph{snippet-level multi-label classification},
assessed by macro-F1 and other classification metrics. We benchmark 11 methods,
including eight state-of-the-art LLMs, our Formal-AST analyzer, a
retrieval-augmented (RAG) method, and an agentic (ReAct) method. Our findings
reveal that no single paradigm excels across all tasks. For Task 1, the ReAct
agent achieves the highest file-level Accuracy@1 (17.38%), while the
Qwen2.5-72B LLM leads at the line level (61.60%), in stark contrast to the
Formal-AST method's 1.86%. For the difficult multi-label Task 2, the
Claude-Sonnet-4.5 LLM achieves the best Macro-F1 (5.75%), while the RAG method
yields the highest Macro-Precision (7.10%). These results highlight the
task-dependent strengths of different automated approaches and underscore the
value of our benchmark in diagnosing their capabilities. All resources are
available at: https://github.com/Haoyi-Zhang/GDPR-Bench-Android.

</details>


### [328] [Can Large Language Models Detect Real-World Android Software Compliance Violations?](https://arxiv.org/abs/2511.00624)
*Haoyi Zhang,Huaijin Ran,Xunzhu Tang*

Main category: cs.SE

TL;DR: 提出CompliBench评估框架评估大语言模型检测安卓应用合规违规能力，实验表明其能提升检测效果。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型难以检测不同法律框架下安卓应用的合规违规问题。

Method: 提出CompliBench框架，定义两项任务，引入稳定性感知复合指标。

Result: 实验显示CompliBench提升了合规检测效果，Claude - 3.5 - sonnet - 20241022的OCS得分最高，Gemini - 2.5 - pro最低。

Conclusion: CompliBench有提升大语言模型合规任务性能的潜力，为未来符合数据保护标准的工具奠定基础。

Abstract: The rapid development of Large Language Models (LLMs) has transformed
software engineering, showing promise in tasks like code generation, bug
detection, and compliance checking. However, current models struggle to detect
compliance violations in Android applications across diverse legal frameworks.
We propose \emph{CompliBench}, a novel evaluation framework for assessing LLMs'
ability to detect compliance violations under regulations like LGPD, PDPA, and
PIPEDA. The framework defines two tasks: Task 1 evaluates \emph{retrieval and
localization} at file, module, and line granularities, and Task 2 assesses
\emph{multi-label judgment} for code snippets. These tasks mirror the audit
process, where auditors locate problematic code and determine implicated
provisions. Traditional metrics fail to capture important aspects like
cross-granularity stability and jurisdictional consistency. Thus, we introduce
stability-aware composites (SGS, RCS, CRGS, and OCS) for a more comprehensive
assessment. Experiments with six models, including GPT-4O and Claude-3.5, show
\emph{CompliBench} improves compliance detection, with
Claude-3.5-sonnet-20241022 achieving the highest OCS score (0.3295), and
Gemini-2.5-pro the lowest (0.0538). This work demonstrates \emph{CompliBench}'s
potential for improving LLM performance in compliance tasks and provides a
foundation for future tools aligned with data protection standards. Our project
is available at https://github.com/Haoyi-Zhang/CompliBench.

</details>


### [329] [Lessons Learned from the Use of Generative AI in Engineering and Quality Assurance of a WEB System for Healthcare](https://arxiv.org/abs/2511.00658)
*Guilherme H. Travassos,Sabrina Rocha,Rodrigo Feitosa,Felipe Assis,Patricia Goncalves,Andre Gheventer,Larissa Galeno,Arthur Sasse,Julio Cesar Guimaraes,Carlos Brito,Joao Pedro Wieland*

Main category: cs.SE

TL;DR: 本文介绍将生成式AI用于开发临床试验网络软件系统的经验，虽无显著技术证据，但成果和建议对软件组织有价值。


<details>
  <summary>Details</summary>
Motivation: 生成式AI发展，软件工程可从中受益，虽应用尚处早期，仍将其用于开发临床试验网络软件系统并分享经验。

Method: 在项目管理、需求规格、设计、开发和质量保证等活动中使用生成式AI进行软件开发。

Result: 目前尚未有能显著改进开发过程的技术证据，但有一定成果。

Conclusion: 成果和建议对想借助生成式AI创新开发实践、提升软件质量的组织有参考价值。

Abstract: The advances and availability of technologies involving Generative Artificial
Intelligence (AI) are evolving clearly and explicitly, driving immediate
changes in various work activities. Software Engineering (SE) is no exception
and stands to benefit from these new technologies, enhancing productivity and
quality in its software development processes. However, although the use of
Generative AI in SE practices is still in its early stages, considering the
lack of conclusive results from ongoing research and the limited technological
maturity, we have chosen to incorporate these technologies in the development
of a web-based software system to be used in clinical trials by a thoracic
diseases research group at our university. For this reason, we decided to share
this experience report documenting our development team's learning journey in
using Generative AI during the software development process. Project
management, requirements specification, design, development, and quality
assurance activities form the scope of observation. Although we do not yet have
definitive technological evidence to evolve our development process
significantly, the results obtained and the suggestions shared here represent
valuable insights for software organizations seeking to innovate their
development practices to achieve software quality with generative AI.

</details>


### [330] [Repairing Responsive Layout Failures Using Retrieval Augmented Generation](https://arxiv.org/abs/2511.00678)
*Tasmia Zerin,Moumita Asad,B. M. Mainul Hossain,Kazi Sakib*

Main category: cs.SE

TL;DR: 提出基于LLM和领域知识的自动化修复方法ReDeFix来解决响应式布局失败问题，准确率达88%，修复方案获工程师认可。


<details>
  <summary>Details</summary>
Motivation: 响应式网站在特定屏幕尺寸下存在布局失真问题，手动修复繁琐，需要自动化修复方法。

Method: 提出基于检索增强生成（RAG）的ReDeFix方法，利用Stack Overflow讨论引导LLM进行CSS修复，结合相关知识和特定上下文创建提示生成CSS补丁。

Result: 该方法修复响应式布局失败问题的准确率达88%，软件工程师认为生成的修复方案视觉布局正确且美观。

Conclusion: 基于LLM和领域知识的自动化修复方法ReDeFix能有效解决响应式布局失败问题。

Abstract: Responsive websites frequently experience distorted layouts at specific
screen sizes, called Responsive Layout Failures (RLFs). Manually repairing
these RLFs involves tedious trial-and-error adjustments of HTML elements and
CSS properties. In this study, an automated repair approach, leveraging LLM
combined with domain-specific knowledge is proposed. The approach is named
ReDeFix, a Retrieval-Augmented Generation (RAG)-based solution that utilizes
Stack Overflow (SO) discussions to guide LLM on CSS repairs. By augmenting
relevant SO knowledge with RLF-specific contexts, ReDeFix creates a prompt that
is sent to the LLM to generate CSS patches. Evaluation demonstrates that our
approach achieves an 88\% accuracy in repairing RLFs. Furthermore, a study from
software engineers reveals that generated repairs produce visually correct
layouts while maintaining aesthetics.

</details>


### [331] [An Empirical Investigation of the Experiences of Dyslexic Software Engineers](https://arxiv.org/abs/2511.00706)
*Marcos Vinicius Cruz,Pragya Verma,Grischa Liebel*

Main category: cs.SE

TL;DR: 本文对阅读障碍软件工程师的经历进行定性研究，发现他们编程学习阶段有困难，掌握后能胜任工作，特定工具可缓解困难，且在视觉思维和创造力方面有优势。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对阅读障碍软件工程师经历的研究，也未将其优势与困难关联起来，因此开展此项研究。

Method: 采用社会技术扎根理论方法，通过对10名阅读障碍软件工程师的访谈、3篇博客文章和Reddit上的153篇帖子收集数据。

Result: 阅读障碍软件工程师在编程学习阶段困难大，掌握后能胜任并擅长许多软件工程任务；常见软件工程支持工具可缓解困难；他们在视觉思维和创造力方面有优势。

Conclusion: 研究结果对软件工程实践有启示，也为未来研究如探究代码对阅读障碍者的易读性指明方向。

Abstract: Dyslexia is a common learning disorder that primarily impairs an individual's
reading and writing abilities. In adults, dyslexia can affect both professional
and personal lives, often leading to mental challenges and difficulties
acquiring and keeping work. In Software Engineering (SE), reading and writing
difficulties appear to pose substantial challenges for core tasks such as
programming. However, initial studies indicate that these challenges may not
significantly affect their performance compared to non-dyslexic colleagues.
Conversely, strengths associated with dyslexia could be particularly valuable
in areas like programming and design. However, there is currently no work that
explores the experiences of dyslexic software engineers, and puts their
strengths into relation with their difficulties. To address this, we present a
qualitative study of the experiences of dyslexic individuals in SE. We followed
the basic stage of the Socio-Technical Grounded Theory method and base our
findings on data collected through 10 interviews with dyslexic software
engineers, 3 blog posts and 153 posts on the social media platform Reddit. We
find that dyslexic software engineers especially struggle at the programming
learning stage, but can succeed and indeed excel at many SE tasks once they
master this step. Common SE-specific support tools, such as code completion and
linters are especially useful to these individuals and mitigate many of the
experienced difficulties. Finally, dyslexic software engineers exhibit
strengths in areas such as visual thinking and creativity. Our findings have
implications to SE practice and motivate several areas of future research in
SE, such as investigating what makes code less/more understandable to dyslexic
individuals.

</details>


### [332] [A Systematic Literature Review of Code Hallucinations in LLMs: Characterization, Mitigation Methods, Challenges, and Future Directions for Reliable AI](https://arxiv.org/abs/2511.00776)
*Cuiyun Gao,Guodong Fan,Chun Yong Chong,Shizhan Chen,Chao Liu,David Lo,Zibin Zheng,Qing Liao*

Main category: cs.SE

TL;DR: 本文对面向代码的大语言模型幻觉现象进行系统综述，从多方面分析并总结相关问题、策略、挑战及评估基准。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型融入软件工程任务，理解和缓解代码中的幻觉现象十分必要。

Method: 通过调研60篇论文，从定义及原因、缓解策略、代码智能挑战及新兴任务利用、评估基准四个关键视角进行系统综述。

Result: 明确代码语境下幻觉的定义和主要原因，总结代表性缓解策略，指出代码特定挑战，分析新兴任务作用，总结现有评估基准。

Conclusion: 强调需要面向幻觉的评估基准。

Abstract: Model hallucination is one of the most critical challenges faced by Large
Language Models (LLMs), especially in high-stakes code intelligence tasks. As
LLMs become increasingly integrated into software engineering tasks,
understanding and mitigating hallucination in code becomes essential. In this
survey, we provide a systematic review of hallucination phenomena in
code-oriented LLMs from four key perspectives. First, we begin by surveying 60
papers to define hallucination in the context of code and summarize its primary
causes, such as data noise, exposure bias, and insufficient semantic grounding,
while also tracing recent trends in literature across natural language
processing (NLP) and software engineering communities. Second, we review model
hallucination surveys in a broader span and summarize representative
hallucination mitigation strategies, such as knowledge-enhanced generation,
constrained decoding, and post-editing. Third, we review approaches targeted
for code intelligence and highlight code-specific challenges that aggravate
hallucination, including syntax sensitivity, strict type systems, and
dependence on external libraries. Meanwhile, we analyze how emerging code
intelligence tasks, e.g., program analysis, symbolic execution, and unit
testing, are utilized to detect and mitigate hallucinations. Fourth, we
summarize current evaluation benchmarks, ranging from static metrics to dynamic
checks, e.g., compilation and execution correctness, and emphasize the need for
hallucination-oriented benchmarks.

</details>


### [333] [Can Language Models Go Beyond Coding? Assessing the Capability of Language Models to Build Real-World Systems](https://arxiv.org/abs/2511.00780)
*Chenyu Zhao,Shenglin Zhang,Zeshun Huang,Weilin Jin,Yongqian Sun,Dan Pei,Chaoyun Zhang,Qingwei Lin,Chetan Bansal,Saravan Rajmohan,Minghua Ma*

Main category: cs.SE

TL;DR: 提出Build - bench基准评估大语言模型跨指令集架构修复软件构建失败的能力，评估6个模型，显示当前模型最高构建成功率63%，工具使用模式差异大。


<details>
  <summary>Details</summary>
Motivation: 现有缺乏评估大语言模型跨指令集架构迁移时修复软件能力的基准，跨指令集迁移需处理复杂依赖等问题。

Method: 构建Build - bench基准，收集268个真实失败包，集成辅助工具，采用迭代修复流程。

Result: 对6个代表模型评估，当前模型最高构建成功率63%，工具使用模式差异大。

Conclusion: Build - bench是首个支持架构感知的研究基于大语言模型软件构建和修复的基准。

Abstract: Large language models (LLMs) have shown growing potential in software
engineering, yet few benchmarks evaluate their ability to repair software
during migration across instruction set architectures (ISAs). Cross-ISA
migration, such as between x86_64 and aarch64, requires handling complex
dependencies, heterogeneous toolchains, and long build logs while ensuring
executable verification. To address this challenge, we present Build-bench, an
end-to-end benchmark that systematically evaluates the capability of LLMs to
repair build failures in cross-ISA settings. Build-bench collects 268
real-world failed packages and integrates auxiliary tools including Structure
Extraction, File Content Extraction, Content Modification, and Build
Verification to support autonomous, tool-augmented reasoning. The repair
process operates in an iterative loop where, upon failure, the model receives
updated build logs and previous repair outcomes to refine subsequent attempts.
Through a comparative evaluation of six representative LLMs, Build-bench
reveals that current models achieve a maximum build success rate of 63% and
tool usage patterns differ significantly across models. By coupling real build
environments with verifiable outcomes, Build-bench establishes the first
architecture-aware benchmark for studying LLM-based software build and repair.

</details>


### [334] [GrowthHacker: Automated Off-Policy Evaluation Optimization Using Code-Modifying LLM Agents](https://arxiv.org/abs/2511.00802)
*Jie JW Wu,Ayanda Patrick Herlihy,Ahmad Saleem Mirza,Ali Afoud,Fatemeh Fard*

Main category: cs.SE

TL;DR: 研究用大语言模型和基于大语言模型的智能体优化离线A/B测试（OPE）结果，提出GrowthHacker基准，开发two_agent框架，结果显示该框架效果好，证明大语言模型智能体可增强OPE系统。


<details>
  <summary>Details</summary>
Motivation: 在线A/B测试有资源消耗大、影响用户等问题，OPE虽重要，但对利用大语言模型和智能体优化OPE结果了解甚少。

Method: 提出GrowthHacker基准，在大规模真实数据集上使用智能体和基线方法，迭代优化代码、评估结果；开发two_agent框架。

Result: two_agent框架可靠性达100%，正向结果平均提升106.7%；two_agent和CrewAI成功率45%，优于AutoGen的34%。

Conclusion: 大语言模型智能体可作为自动“增长黑客”增强OPE系统，对生产中数据驱动决策的扩展有意义。

Abstract: With the software industry shifting toward a data-driven culture, online A/B
testing is a key tool for evaluating new technologies. However, deploying such
experiments requires substantial resources, may negatively impact users, and
involves long data collection periods. To address this, \textit{off-policy
evaluation (OPE)}, or offline A/B testing, uses logged data to assess
technologies and is fundamental in Reinforcement Learning, making it crucial in
domains where online testing is costly or risky, such as healthcare,
recommender systems, education, dialog systems, and robotics. Despite advances
in coding LLMs and agentic AI, little is known about leveraging them to
optimize OPE results. We investigate whether LLMs and LLM-based agents can
improve OPE performance via code optimization. We propose
\textit{GrowthHacker}, a benchmark with agent and baseline methods on
large-scale real-world datasets, which iteratively optimizes code, evaluates
results, and begins new optimization cycles. We collected datasets, established
protocols, implemented baselines for OPE on the Open Bandit Pipeline
(OBP)~\cite{saito2021openbanditdatasetpipeline} and
Scope-RL~\cite{kiyohara2023scope}, and developed the \textit{two_agent}
framework, which reduces system complexity while preserving optimization
effectiveness. Results show the two_agent framework achieves 100% reliability
and the highest average improvement of 106.7% among positive outcomes. Both
two_agent and CrewAI reach 45% success rates, outperforming AutoGen's 34%.
These findings demonstrate the feasibility of LLM-based agents as automated
"growth hackers" to enhance OPE systems, with implications for scaling
data-driven decision-making in production.

</details>


### [335] [CodeClash: Benchmarking Goal-Oriented Software Engineering](https://arxiv.org/abs/2511.00839)
*John Yang,Kilian Lieret,Joyce Yang,Carlos E. Jimenez,Ofir Press,Ludwig Schmidt,Diyi Yang*

Main category: cs.SE

TL;DR: 提出CodeClash基准测试语言模型在开放式目标下迭代开发代码的能力，实验发现模型有战略推理和长期代码库维护局限。


<details>
  <summary>Details</summary>
Motivation: 现有编码基准测试针对具体任务，而现实软件开发是追求高层次目标，需评估模型在无明确指导下实现开放式目标的能力。

Method: 引入CodeClash基准，让语言模型在多轮锦标赛中构建代码库竞争，运行1680场锦标赛评估8个模型。

Result: 模型开发风格多样，但存在战略推理局限，难以进行长期代码库维护，顶级模型不敌人类专家。

Conclusion: 开源CodeClash以推动自主、目标导向的代码开发研究。

Abstract: Current benchmarks for coding evaluate language models (LMs) on concrete,
well-specified tasks such as fixing specific bugs or writing targeted tests.
However, human programmers do not spend all day incessantly addressing isolated
tasks. Instead, real-world software development is grounded in the pursuit of
high-level goals, like improving user retention or reducing costs. Evaluating
whether LMs can also iteratively develop code to better accomplish open-ended
objectives without any explicit guidance remains an open challenge. To address
this, we introduce CodeClash, a benchmark where LMs compete in multi-round
tournaments to build the best codebase for achieving a competitive objective.
Each round proceeds in two phases: agents edit their code, then their codebases
compete head-to-head in a code arena that determines winners based on
objectives like score maximization, resource acquisition, or survival. Whether
it's writing notes, scrutinizing documentation, analyzing competition logs, or
creating test suites, models must decide for themselves how to improve their
codebases both absolutely and against their opponents. We run 1680 tournaments
(25,200 rounds total) to evaluate 8 LMs across 6 arenas. Our results reveal
that while models exhibit diverse development styles, they share fundamental
limitations in strategic reasoning. Models also struggle with long-term
codebase maintenance, as repositories become progressively messy and redundant.
These limitations are stark: top models lose every round against expert human
programmers. We open-source CodeClash to advance the study of autonomous,
goal-oriented code development.

</details>


### [336] [A Comprehensive Empirical Evaluation of Agent Frameworks on Code-centric Software Engineering Tasks](https://arxiv.org/abs/2511.00872)
*Zhuowen Yin,Cuifeng Gao,Chunsong Fan,Wenzhang Yang,Yinxing Xue,Lijun Zhang*

Main category: cs.SE

TL;DR: 对七个通用代理框架在三个代码中心任务上进行全面实证研究，分析其有效性、效率和开销，揭示能力模式和权衡，为实践和研究提供指导。


<details>
  <summary>Details</summary>
Motivation: 现有研究对代理在软件工程中实际能力的评估不完整，需全面研究。

Method: 对七个通用代理框架在软件开发、漏洞检测和程序修复三个任务上进行评估，用标准基准测试，从有效性、效率和开销三个角度分析。

Result: 各框架有不同能力模式和权衡，有效性上总体表现中等，效率上AgentOrchestra轨迹长、修正多，OpenHands反思推理能力强，开销上软件开发成本高，GPTswarm最具成本效益。

Conclusion: 研究结果为软件工程代理的实际应用和未来研究提供方向。

Abstract: Unlike traditional automation tools or static LLM-based systems, agents
combine decision-making and tool utilization to accomplish complex tasks,
showing great potential in software engineering. However, existing studies
largely focus on specific tasks or isolated aspects, providing an incomplete
picture of agents' practical capabilities. To address this, we conduct a
comprehensive empirical study evaluating seven general-purpose agent frameworks
across three representative code-centric tasks: software development,
vulnerability detection, and program repair. Each task is assessed using
standard, widely adopted benchmarks to ensure objective and comparable
evaluation. Agent performance is systematically analyzed from three
complementary perspectives: effectiveness (task success), efficiency (execution
process), and overhead (token consumption). Our findings reveal distinct
capability patterns and trade-offs among the evaluated frameworks. In terms of
effectiveness, agents achieve moderate overall performance. Regarding
efficiency, AgentOrchestra tends to exhibit the longest trajectories and the
most correction attempts due to coordination overhead, whereas OpenHands
demonstrate stronger reflective reasoning abilities. For overhead, software
development incurs the highest monetary cost, while GPTswarm remains the most
cost-efficient. Furthermore, we conduct an in-depth cross-analysis of the
relationship between effectiveness and efficiency, exploring the underlying
reasons behind their interplay. These findings guide both practical adoption
and future research toward more efficient software engineering agents.

</details>


### [337] [Sustainability of Machine Learning-Enabled Systems: The Machine Learning Practitioner's Perspective](https://arxiv.org/abs/2511.00901)
*Vincenzo De Martino,Stefano Lambiase,Fabiano Pecorelli,Willem-Jan van den Heuvel,Filomena Ferrucci,Fabio Palomba*

Main category: cs.SE

TL;DR: 本文针对机器学习系统中软件可持续性实践管理的实证研究不足问题，通过访谈和调查研究从业者视角下的可持续性，发现意识与实施脱节，需更多指导和支持。


<details>
  <summary>Details</summary>
Motivation: 以往研究缺乏机器学习工作流中可持续性实践管理的实证证据，且多关注环境可持续性，忽略其他维度和实际挑战，因此开展研究。

Method: 先对8位经验丰富的机器学习工程师进行访谈进行定性分析，再对203位从业者进行大规模定量调查。

Result: 发现可持续性意识和系统实施之间存在显著脱节。

Conclusion: 需要更结构化的指南、测量框架和监管支持。

Abstract: Software sustainability is a key multifaceted non-functional requirement that
encompasses environmental, social, and economic concerns, yet its integration
into the development of Machine Learning (ML)-enabled systems remains an open
challenge. While previous research has explored high-level sustainability
principles and policy recommendations, limited empirical evidence exists on how
sustainability is practically managed in ML workflows. Existing studies
predominantly focus on environmental sustainability, e.g., carbon footprint
reduction, while missing the broader spectrum of sustainability dimensions and
the challenges practitioners face in real-world settings. To address this gap,
we conduct an empirical study to characterize sustainability in ML-enabled
systems from a practitioner's perspective. We investigate (1) how ML engineers
perceive and describe sustainability, (2) the software engineering practices
they adopt to support it, and (3) the key challenges hindering its adoption. We
first perform a qualitative analysis based on interviews with eight experienced
ML engineers, followed by a large-scale quantitative survey with 203 ML
practitioners. Our key findings reveal a significant disconnection between
sustainability awareness and its systematic implementation, highlighting the
need for more structured guidelines, measurement frameworks, and regulatory
support.

</details>


### [338] [Empirical Derivations from an Evolving Test Suite](https://arxiv.org/abs/2511.00915)
*Jukka Ruohonen,Abhishek Tiwari*

Main category: cs.SE

TL;DR: 对NetBSD操作系统的自动化、连续且基于虚拟化的软件测试套件进行纵向实证分析，发现测试套件持续增长，各类失败情况总体稳定，代码变动和内核修改对失败影响小。


<details>
  <summary>Details</summary>
Motivation: 对NetBSD操作系统的软件测试套件进行纵向研究，为从大规模且不断发展的软件测试套件中得出结论提供依据。

Method: 开展从2010年代初到2025年末的纵向实证分析。

Result: 测试套件持续增长，覆盖超一万个测试用例；失败测试用例总体稳定；构建失败、测试套件完成失败和安装失败情况类似；代码变动和内核修改对失败无纵向一致的统计解释，影响平均较小。

Conclusion: 这些实证观察虽具探索性，但有助于从大规模且不断发展的软件测试套件中得出结论。

Abstract: The paper presents a longitudinal empirical analysis of the automated,
continuous, and virtualization-based software test suite of the NetBSD
operating system. The longitudinal period observed spans from the initial roll
out of the test suite in the early 2010s to late 2025. According to the
results, the test suite has grown continuously, currently covering over ten
thousand individual test cases. Failed test cases exhibit overall stability,
although there have been shorter periods marked with more frequent failures. A
similar observation applies to build failures, failures of the test suite to
complete, and installation failures, all of which are also captured by the
NetBSD's testing framework. Finally, code churn and kernel modifications do not
provide longitudinally consistent statistical explanations for the failures.
Although some periods exhibit larger effects, including particularly with
respect to the kernel modifications, the effects are small on average. Even
though only in an exploratory manner, these empirical observations contribute
to efforts to draw conclusions from large-scale and evolving software test
suites.

</details>


### [339] [DPO-F+: Aligning Code Repair Feedback with Developers' Preferences](https://arxiv.org/abs/2511.01043)
*Zihan Fang,Yifan Zhang,Yueke Zhang,Kevin Leach,Yu Huang*

Main category: cs.SE

TL;DR: 提出DPO - f+框架用于代码修复反馈对齐，在生成代码准确性和反馈对齐上表现优于基线和标准DPO，促进人机协作。


<details>
  <summary>Details</summary>
Motivation: 开发者难以理解大语言模型的代码修复输出，现有工作未充分解决自然语言反馈以实现理解和迭代改进。

Method: DPO - f+框架：形式化特定领域指标、自动构建成对偏好数据集、用增强轻量级边际信号的直接偏好优化微调、提供自动反馈评估协议。

Result: 在生成代码准确性和反馈对齐上优于基线和标准DPO，如在新手编程任务和SWE - bench Lite基准测试中提升通过率和问题解决率。

Conclusion: DPO - f+使大语言模型辅助修复成为协作式工作流，增强代码理解，促进软件工程中更有效的人机协作。

Abstract: Large Language Models (LLMs) are increasingly applied to software engineering
tasks, especially code repair. However, developers often struggle to interpret
model outputs, limiting effective human-AI teaming. Prior work largely
optimizes repaired code while under-addressing the natural-language feedback
that enables comprehension and iterative improvement. We present DPO-f+, a
novel framework that aligns code-repair feedback with developer needs and
profiles. It (1) formalizes developer-profiled, domain-specific metrics for
feedback alignment; (2) automatically constructs pairwise preference datasets
from code-repair tasks; (3) fine-tunes using Direct Preference Optimization
(DPO) augmented with a lightweight margin signal; and (4) provides an automated
feedback evaluation protocol. Empirically, DPO-f+ outperforms both the baseline
and standard DPO on generated-code accuracy and overall feedback alignment. On
novice programming tasks, DPO-f+ raises the top-1 pass rate by 5.71 percentage
points (pp) over the baseline and by 3.30 pp over DPO. On the more challenging
SWE-bench Lite benchmark, it increases the issue-resolution rate by 1.67 pp
over DPO and by 4.67 pp over the baseline. It also achieves the largest
improvement in feedback alignment, outperforming DPO and the baseline. By
aligning feedback more closely with developer needs, DPO-f+ turns LLM-assisted
repair from one-shot outputs into a collaborative sensemaking workflow,
providing a practical approach to enhancing code comprehension and fostering
more effective human-AI teaming in software engineering.

</details>


### [340] [HAFixAgent: History-Aware Automated Program Repair Agent](https://arxiv.org/abs/2511.01047)
*Yu Shi,Hao Li,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 本文提出HAFixAgent，研究仓库历史能否提升大规模代理式自动程序修复（APR）系统，实验显示其在有效性、效率和实用性上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有APR系统多依赖局部快照上下文而忽视仓库历史，探究仓库历史能否提升大规模代理式APR系统，尤其是针对复杂多补丁错误。

Method: 提出HAFixAgent，将基于 blame 的仓库启发式方法注入修复循环。

Result: 与两个最先进的基线相比，HAFixAgent在有效性上显著提升，效率上不显著增加代理步骤且保持代币成本相当，实用性上结合不同历史启发式可修复更多错误。

Conclusion: HAFixAgent为具有历史感知的代理式APR提供了实用方法，即让代理基于版本控制历史，优先考虑基于差异的历史上下文，并在需要时集成互补启发式。

Abstract: Automated program repair (APR) has recently shifted toward large language
models and agent-based systems, yet most systems rely on local snapshot
context, overlooking repository history. Prior work shows that repository
history helps repair single-line bugs, since the last commit touching the buggy
line is often the bug-introducing one. In this paper, we investigate whether
repository history can also improve agentic APR systems at scale, especially
for complex multi-hunk bugs. We present HAFixAgent, a History-Aware Bug-Fixing
Agent that injects blame-derived repository heuristics into its repair loop. A
preliminary study of all 854 real-world bugs from Defects4J motivates our
design, showing that bug-relevant history is both widely available and highly
concentrated. Empirical comparison of HAFixAgent with two state-of-the-art
baselines shows: (1) Effectiveness: HAFixAgent significantly improves over the
agent-based baseline (by 212.3%) and the multi-hunk baseline (by 29.9%). (2)
Efficiency: history does not significantly increase agent steps and keeps token
costs comparable, with notably lower median costs for complex
multi-file-multi-hunk bugs. (3) Practicality: combining different historical
heuristics repairs more bugs, offering a clear cost-benefit trade-off.
HAFixAgent offers a practical recipe for history-aware agentic APR: ground the
agent in version control history, prioritize diff-based historical context, and
integrate complementary heuristics when needed.

</details>


### [341] [HarnessLLM: Automatic Testing Harness Generation via Reinforcement Learning](https://arxiv.org/abs/2511.01104)
*Yujian Liu,Jiabao Ji,Yang Zhang,Wenbo Guo,Tommi Jaakkola,Shiyu Chang*

Main category: cs.SE

TL;DR: 提出HarnessLLM两阶段训练管道让大模型写测试代码，实验显示其在找bug和测试策略多样性上优于基于输入输出的测试，还能提升代码生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大模型的自动测试生成方法生成测试多样性有限且调试信息不足。

Method: 提出两阶段训练管道HarnessLLM，先进行SFT训练，再进行带定制奖励设计的RLVR训练。

Result: HarnessLLM在找bug和测试策略多样性上优于输入输出测试，还能通过测试时缩放提升代码生成性能。

Conclusion: HarnessLLM是有效的自动测试生成方法，代码开源可获取。

Abstract: Existing LLM-based automatic test generation methods mainly produce input and
expected output pairs to categorize the intended behavior of correct programs.
Although straightforward, these methods have limited diversity in generated
tests and cannot provide enough debugging information. We propose HarnessLLM, a
two-stage training pipeline that enables LLMs to write harness code for
testing. Particularly, LLMs generate code that synthesizes inputs and validates
the observed outputs, allowing complex test cases and flexible output
validation such as invariant checking. To achieve this, we train LLMs with SFT
followed by RLVR with a customized reward design. Experiments show that
HarnessLLM outperforms input-output-based testing in bug finding and testing
strategy diversity. HarnessLLM further benefits the code generation performance
through test-time scaling with our generated test cases as inference-phase
validation. Our code is available at
https://github.com/UCSB-NLP-Chang/HarnessLLM.git.

</details>


### [342] [An Empirical Study of LLM-Based Code Clone Detection](https://arxiv.org/abs/2511.01176)
*Wenqing Zhu,Norihiro Yoshida,Eunjong Choi,Yutaka Matsubara,Hiroaki Takada*

Main category: cs.SE

TL;DR: 本文构建七个代码克隆数据集评估五个大语言模型在四种提示下的代码克隆检测性能，发现模型在不同数据集表现有差异，响应一致性高。


<details>
  <summary>Details</summary>
Motivation: 解决现有研究中未解决的大语言模型在不同数据集上性能可比以及代码克隆检测响应一致性的问题。

Method: 构建七个代码克隆数据集，从CodeNet和BigCloneBench采样代码对，用四个现有提示评估五个大语言模型。

Result: 大语言模型在CodeNet相关数据集表现好，o3 - mini的F1分数达0.943，在BigCloneBench相关数据集表现显著下降；多数模型响应一致性高，超90%判断一致，F1分数波动小于0.03。

Conclusion: 大语言模型在代码克隆检测中在不同数据集表现有差异，但响应一致性较好。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
various software engineering tasks, such as code generation and debugging,
because of their ability to translate between programming languages and natural
languages. Existing studies have demonstrated the effectiveness of LLMs in code
clone detection. However, two crucial issues remain unaddressed: the ability of
LLMs to achieve comparable performance across different datasets and the
consistency of LLMs' responses in code clone detection. To address these
issues, we constructed seven code clone datasets and then evaluated five LLMs
in four existing prompts with these datasets. The datasets were created by
sampling code pairs using their Levenshtein ratio from two different code
collections, CodeNet and BigCloneBench. Our evaluation revealed that although
LLMs perform well in CodeNet-related datasets, with o3-mini achieving a 0.943
F1 score, their performance significantly decreased in BigCloneBench-related
datasets. Most models achieved a high response consistency, with over 90\% of
judgments remaining consistent across all five submissions. The fluctuations of
the F1 score affected by inconsistency are also tiny; their variations are less
than 0.03.

</details>


### [343] [Lares: LLM-driven Code Slice Semantic Search for Patch Presence Testing](https://arxiv.org/abs/2511.01252)
*Siyuan Li,Yaowen Zheng,Hong Li,Jingdong Guo,Chaopeng Dong,Chunpeng Yan,Weijie Wang,Yimo Ren,Limin Sun,Hongsong Zhu*

Main category: cs.SE

TL;DR: 现有方法在检测目标二进制文件补丁时可用性和准确性有限，提出Lares方法，利用代码切片语义搜索，实验显示其精度、召回率和可用性更优。


<details>
  <summary>Details</summary>
Motivation: 现代软件生态中1 - day漏洞风险大，现有检测补丁方法可用性和准确性有限，依赖编译过程，难以区分补丁和编译差异导致的代码变化。

Method: 提出Lares方法，引入代码切片语义搜索，直接从补丁源代码提取特征，在目标二进制伪代码中找语义等效代码切片，用大语言模型分析代码，SMT求解器进行逻辑推理。

Result: 实验表明Lares在精度、召回率和可用性上表现更优，是首个跨优化级别、架构和编译器评估补丁存在性测试的工作。

Conclusion: Lares方法能有效解决现有方法的局限性，提高补丁存在性测试的效果。

Abstract: In modern software ecosystems, 1-day vulnerabilities pose significant
security risks due to extensive code reuse. Identifying vulnerable functions in
target binaries alone is insufficient; it is also crucial to determine whether
these functions have been patched. Existing methods, however, suffer from
limited usability and accuracy. They often depend on the compilation process to
extract features, requiring substantial manual effort and failing for certain
software. Moreover, they cannot reliably differentiate between code changes
caused by patches or compilation variations. To overcome these limitations, we
propose Lares, a scalable and accurate method for patch presence testing. Lares
introduces Code Slice Semantic Search, which directly extracts features from
the patch source code and identifies semantically equivalent code slices in the
pseudocode of the target binary. By eliminating the need for the compilation
process, Lares improves usability, while leveraging large language models
(LLMs) for code analysis and SMT solvers for logical reasoning to enhance
accuracy. Experimental results show that Lares achieves superior precision,
recall, and usability. Furthermore, it is the first work to evaluate patch
presence testing across optimization levels, architectures, and compilers. The
datasets and source code used in this article are available at
https://github.com/Siyuan-Li201/Lares.

</details>


### [344] [Exploringand Unleashing the Power of Large Language Models in CI/CD Configuration Translation](https://arxiv.org/abs/2511.01316)
*Chong Wang,Chen Zhang,Jiajun Wu,Wunan Guo,Jianfeng Qu,Yewen Tian,Yang Liu*

Main category: cs.SE

TL;DR: 本文研究基于大语言模型的CI配置翻译，聚焦从Travis CI迁移到GitHub Actions，量化迁移工作量、分析翻译问题并评估增强策略，结合指南提示和迭代优化效果最佳。


<details>
  <summary>Details</summary>
Motivation: CI平台迁移常见，CI配置翻译有挑战，大语言模型在软件工程领域有潜力用于CI配置翻译。

Method: 使用811条迁移记录量化迁移工作量，分析四个大语言模型的翻译结果，评估三种增强策略。

Result: 开发者平均阅读38行Travis配置、编写58行GitHub Actions配置，近半迁移需多次提交；发现1121个问题分四类；结合指南提示和迭代优化构建成功率达75.5%，比基础提示的GPT - 4o提高近三倍。

Conclusion: 结合指南提示和迭代优化在基于大语言模型的CI配置翻译中性能最佳。

Abstract: Continuous Integration (CI) is a cornerstone of modern collaborative software
development, and numerous CI platforms are available. Differences in
maintenance overhead, reliability, and integration depth with code-hosting
platforms make migration between CI platforms a common practice. A central step
in migration is translating CI configurations, which is challenging due to the
intrinsic complexity of CI configurations and the need to understand semantic
differences and relationships across CI platforms.
  With the advent of large language models (LLMs), recent advances in software
engineering highlight their potential for CI configuration translation. In this
paper, we present a study on LLM-based CI configuration translation, focusing
on the migration from Travis CI to GitHub Actions. First, using 811 migration
records, we quantify the effort involved and find that developers read an
average of 38 lines of Travis configuration and write 58 lines of GitHub
Actions configuration, with nearly half of the migrations requiring multiple
commits. We further analyze translations produced by each of the four LLMs and
identify 1,121 issues grouped into four categories: logic inconsistencies
(38%), platform discrepancies (32%), environment errors (25%), and syntax
errors (5%). Finally, we evaluate three enhancement strategies and show that
combining guideline-based prompting with iterative refinement achieves the best
performance, reaching a Build Success Rate of 75.5%-nearly a threefold
improvement over GPT-4o with a basic prompt.

</details>


### [345] [AI for Requirements Engineering: Industry adoption and Practitioner perspectives](https://arxiv.org/abs/2511.01324)
*Lekshmi Murali Rani,Richard Berntsson Svensson,Robert Feldt*

Main category: cs.SE

TL;DR: 对55名软件从业者调查AI在需求工程（RE）四个阶段和四种决策方式的使用情况，发现多数从业者已用AI且评价积极，人类与AI协作占主导，表明AI作协作伙伴更有效，需RE特定协作框架和AI治理。


<details>
  <summary>Details</summary>
Motivation: AI用于RE有好处也有挑战，而相关研究有限，需了解AI在RE的使用情况。

Method: 对55名软件从业者进行调查，涵盖RE四个阶段和四种决策方式，并收集他们的看法、挑战和机会。

Result: 58.2%受访者已在RE中使用AI，69.1%认为其影响积极；人类与AI协作占比54.4%，全AI自动化仅5.4%，被动AI验证更少。

Conclusion: AI作为协作伙伴更有效，随着AI在RE中应用增加，需要RE特定的人类与AI协作框架和健全负责的AI治理。

Abstract: The integration of AI for Requirements Engineering (RE) presents significant
benefits but also poses real challenges.Although RE is fundamental to software
engineering, limited research has examined AI adoption in RE.We surveyed 55
software practitioners to map AI usage across four RE phases:Elicitation,
Analysis, Specification, and Validation, and four approaches for decision
making: human only decisions, AI validation, Human AI Collaboration (HAIC), and
full AI automation.Participants also shared their perceptions, challenges, and
opportunities when applying AI for RE tasks.Our data show that 58.2% of
respondents already use AI in RE, and 69.1% view its impact as positive or very
positive.HAIC dominates practice, accounting for 54.4% of all RE techniques,
while full AI automation remains minimal at 5.4%.Passive AI validation (4.4 to
6.2%) lags even further behind, indicating that practitioners value AI's active
support over passive oversight.These findings suggest that AI is most effective
when positioned as a collaborative partner rather than a replacement for human
expertise.It also highlights the need for RE specific HAIC frameworks along
with robust and responsible AI governance as AI adoption in RE grows.

</details>


### [346] [The Future of Generative AI in Software Engineering: A Vision from Industry and Academia in the European GENIUS Project](https://arxiv.org/abs/2511.01348)
*Robin Gröpler,Steffen Klepke,Jack Johns,Andreas Dreschinski,Klaus Schmid,Benedikt Dornauer,Eray Tüzün,Joost Noppen,Mohammad Reza Mousavi,Yongjian Tang,Johannes Viehmann,Selin Şirin Aslangül,Beum Seuk Lee,Adam Ziolkowski,Eric Zie*

Main category: cs.SE

TL;DR: 该论文探讨生成式AI在软件工程全生命周期应用，介绍GENIUS项目应对挑战，分析现状、展望未来、探讨角色转变及项目贡献，为行业提供基础。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在软件工程全生命周期应用未充分探索，存在可靠性、问责制等关键不确定性，需深入研究和行动。

Method: 基于GENIUS项目跨部门对话与经验，结合探索性文献综述。

Result: 分析了GenAI在SDLC采用中的当前挑战，展望未来五年技术和方法进展，探讨软件专业人员角色和技能集变化，介绍GENIUS项目贡献。

Conclusion: 论文将技术创新与商业相关性结合，为软件工程团队提供可靠、可扩展且适用于行业的GenAI解决方案基础，为研究议程和产业战略提供信息。

Abstract: Generative AI (GenAI) has recently emerged as a groundbreaking force in
Software Engineering, capable of generating code, suggesting fixes, and
supporting quality assurance. While its use in coding tasks shows considerable
promise, applying GenAI across the entire Software Development Life Cycle
(SDLC) has not yet been fully explored. Critical uncertainties in areas such as
reliability, accountability, security, and data privacy demand deeper
investigation and coordinated action. The GENIUS project, comprising over 30
European industrial and academic partners, aims to address these challenges by
advancing AI integration across all SDLC phases. It focuses on GenAI's
potential, the development of innovative tools, and emerging research
challenges, actively shaping the future of software engineering. This vision
paper presents a shared perspective on the future of GenAI-based software
engineering, grounded in cross-sector dialogue and experience within the GENIUS
consortium, supported by an exploratory literature review. The paper explores
four central elements: (1) a structured overview of current challenges in GenAI
adoption across the SDLC; (2) a forward-looking vision outlining key
technological and methodological advances expected over the next five years;
(3) anticipated shifts in the roles and required skill sets of software
professionals; and (4) the contribution of GENIUS in realizing this
transformation through practical tools and industrial validation. By aligning
technical innovation with business relevance, this paper aims to inform both
research agendas and industrial strategies, providing a foundation for
reliable, scalable, and industry-ready GenAI solutions for software engineering
teams.

</details>


### [347] [Characterizing Build Compromises Through Vulnerability Disclosure Analysis](https://arxiv.org/abs/2511.01395)
*Maimouna Tamah Diao,Moustapha Awwalou Diouf,Iyiola Emmanuel Olatunji,Abdoul Kader Kaboré,Gervais Mendy,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.SE

TL;DR: 本文通过大规模CVE挖掘构建针对软件构建过程的攻击向量分类法，并分析软件供应链攻击验证分类法，揭示部分攻击利用构建漏洞情况。


<details>
  <summary>Details</summary>
Motivation: 软件构建过程是软件开发关键且脆弱阶段，安全社区缺乏对构建特定攻击向量的系统理解，阻碍有效防御设计。

Method: 通过对NVD数据库中621个漏洞披露进行大规模CVE挖掘构建攻击向量分类法，分析168个记录的软件供应链攻击验证分类法。

Result: 分析显示23.8%的供应链攻击利用构建漏洞，依赖混淆和构建脚本注入是最常见的攻击向量。

Conclusion: 构建的攻击向量分类法有助于理解软件构建过程的攻击情况，为防御设计提供依据。

Abstract: The software build process transforms source code into deployable artifacts,
representing a critical yet vulnerable stage in software development. Build
infrastructure security poses unique challenges: the complexity of
multi-component systems (source code, dependencies, build tools), the
difficulty of detecting intrusions during compilation, and prevalent build
non-determinism that masks malicious modifications. Despite these risks, the
security community lacks a systematic understanding of build-specific attack
vectors, hindering effective defense design.
  This paper presents an empirically-derived taxonomy of attack vectors
targeting the build process, constructed through a large-scale CVE mining (of
621 vulnerability disclosures from the NVD database). We categorize attack
vectors by their injection points across the build pipeline, from source code
manipulation to compiler compromise. To validate our taxonomy, we analyzed 168
documented software supply chain attacks, identifying 40 incidents specifically
targeting build phases. Our analysis reveals that 23.8\% of supply chain
attacks exploit build vulnerabilities, with dependency confusion and build
script injection representing the most prevalent vectors.
  Dataset available at:
https://anonymous.4open.science/r/Taxonomizing-Build-Attacks-8BB0.

</details>


### [348] [VeriODD: From YAML to SMT-LIB - Automating Verification of Operational Design Domains](https://arxiv.org/abs/2511.01417)
*Bassel Rafie,Christian Schindler,Andreas Rausch*

Main category: cs.SE

TL;DR: 提出工具VeriODD，自动将YAML格式的ODD/COD规范转换为命题逻辑和SMT - LIB，实现自动驾驶操作边界的可扩展和自动化保证。


<details>
  <summary>Details</summary>
Motivation: 当前ODD和COD规范用YAML表达，不适合基于求解器的验证，手动转换为形式语言慢且易出错。

Method: 使用基于ANTLR的编译器技术，将YAML规范转换，集成SMT求解器进行检查和验证，有图形用户界面。

Result: VeriODD可将规范转换，能进行一致性检查和合规性验证，有GUI辅助操作。

Conclusion: VeriODD弥合了利益相关者友好符号和形式验证间的差距，实现自动驾驶操作边界的可扩展和自动化保证。

Abstract: Operational Design Domains (ODDs) define the conditions under which an
Automated Driving System (ADS) is allowed to operate, while Current Operational
Domains (CODs) capture the actual runtime situation. Ensuring that a COD
instance lies within the ODD is a crucial step in safety assurance. Today, ODD
and COD specifications are frequently expressed in YAML to remain accessible
for stakeholders, but such descriptions are not directly suitable for
solver-based verification. Manual translation into formal languages such as
SMT-LIB is slow and error-prone. We present VeriODD, a tool that automates this
translation. VeriODD uses ANTLR-based compiler technology to transform
YAML-based ODD/COD specifications into both human-readable propositional logic,
for lightweight review on a simple basis, and solver-ready SMT-LIB. The tool
integrates with SMT solvers such as Z3 to provide automated consistency checks
of ODD specifications and verification of COD conformance. A graphical user
interface supports editing specifications, inspecting generated formulas, and
performing verification with a single click. VeriODD thereby closes the gap
between stakeholder-friendly ODD/COD notations and formal verification,
enabling scalable and automated assurance of operational boundaries in
autonomous driving. Video demonstration: https://youtu.be/odRacNoL_Pk Tool
available at: https://github.com/BasselRafie/VeriODD

</details>


### [349] [LLM-Assisted Tool for Joint Generation of Formulas and Functions in Rule-Based Verification of Map Transformations](https://arxiv.org/abs/2511.01423)
*Ruidi He,Yu Zhang,Meng Zhang,Andreas Rausch*

Main category: cs.SE

TL;DR: 提出LLM辅助流水线用于高清地图转换验证，在合成场景评估，减少人工并保证正确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则的框架依赖手动公式和特定领域函数，难以确保高清地图转换语义正确性且扩展性受限。

Method: 提出LLM辅助流水线，在计算FOL框架内联合生成逻辑公式和可执行谓词，利用基于提示的LLM生成符合语法的规则和谓词并集成到现有系统。

Result: 在合成的桥梁和斜坡场景评估，减少了人工工程工作量并保证了正确性。

Conclusion: 半自动化、有人参与的方法用于地图转换验证是可行且可扩展的。

Abstract: High-definition map transformations are essential in autonomous driving
systems, enabling interoperability across tools. Ensuring their semantic
correctness is challenging, since existing rule-based frameworks rely on
manually written formulas and domain-specific functions, limiting scalability.
  In this paper, We present an LLM-assisted pipeline that jointly generates
logical formulas and corresponding executable predicates within a computational
FOL framework, extending the map verifier in CommonRoad scenario designer with
elevation support. The pipeline leverages prompt-based LLM generation to
produce grammar-compliant rules and predicates that integrate directly into the
existing system.
  We implemented a prototype and evaluated it on synthetic bridge and slope
scenarios. The results indicate reduced manual engineering effort while
preserving correctness, demonstrating the feasibility of a scalable,
semi-automated human-in-the-loop approach to map-transformation verification.

</details>


### [350] [Hidden in Plain Sight: Where Developers Confess Self-Admitted Technical Debt](https://arxiv.org/abs/2511.01529)
*Murali Sridharan,Mikel Robredo,Leevi Rantala,Matteo Esposito,Valentina Lenarduzzi,Mika Mantyla*

Main category: cs.SE

TL;DR: 本文连接SATD注释与周围源代码结构，发现SATD主要出现在定义、条件和异常处理附近的内联代码中。


<details>
  <summary>Details</summary>
Motivation: 以往研究多关注检测和优先级排序，少关注受SATD影响的源代码，本文旨在连接SATD注释与周围源代码结构。

Method: 利用PENTACET数据集，定量推断SATD常见位置和受影响的代码结构/语句。

Result: 将超225,000条SATD注释与周围代码关联，表明SATD主要出现在定义、条件和异常处理附近的内联代码中。

Conclusion: SATD是开发者在变更时有意发出的信号，而非疏忽。

Abstract: Context. Detecting Self-Admitted Technical Debt (SATD) is crucial for
proactive software maintenance. Previous research has primarily targeted
detecting and prioritizing SATD, with little focus on the source code afflicted
with SATD. Our goal in this work is to connect the SATD comments with source
code constructs that surround them.
  Method. We leverage the extensive SATD dataset PENTACET, containing code
comments from over 9000 Java Open Source Software (OSS) repositories. We
quantitatively infer where SATD most commonly occurs and which code
constructs/statements it most frequently affects.
  Results and Conclusions. Our large-scale study links over 225,000 SATD
comments to their surrounding code, showing that SATD mainly arises in inline
code near definitions, conditionals, and exception handling, where developers
face uncertainty and trade-offs, revealing it as an intentional signal of
awareness during change rather than mere neglect.

</details>


### [351] [From Pre-labeling to Production: Engineering Lessons from a Machine Learning Pipeline in the Public Sector](https://arxiv.org/abs/2511.01545)
*Ronivaldo Ferreira,Guilherme da Silva,Carla Rocha,Gustavo Pinto*

Main category: cs.SE

TL;DR: 公共部门嵌入机器学习面临困难，研究BP平台表明常见工程选择有风险，公共部门负责任的ML是制度工程问题，其成功依赖机构构建可信数据基础设施。


<details>
  <summary>Details</summary>
Motivation: 解决公共部门构建准确、可审计和运营可持续的ML系统面临的困难。

Method: 研究Brasil Participativo (BP)平台。

Result: 常见工程选择能加速开发，但不配合数据治理和人工验证会引入新风险。

Conclusion: 公共部门机器学习成功更依赖机构构建透明、可复制和可问责的可信数据基础设施。

Abstract: Machine learning is increasingly being embedded into government digital
platforms, but public-sector constraints make it difficult to build ML systems
that are accurate, auditable, and operationally sustainable. In practice, teams
face not only technical issues like extreme class imbalance and data drift, but
also organizational barriers such as bureaucratic data access, lack of
versioned datasets, and incomplete governance over provenance and monitoring.
Our study of the Brasil Participativo (BP) platform shows that common
engineering choices -- like using LLMs for pre-labeling, splitting models into
routed classifiers, and generating synthetic data -- can speed development but
also introduce new traceability, reliability, and cost risks if not paired with
disciplined data governance and human validation. This means that, in the
public sector, responsible ML is not just a modeling problem but an
institutional engineering problem, and ML pipelines must be treated as civic
infrastructure. Ultimately, this study shows that the success of machine
learning in the public sector will depend less on breakthroughs in model
accuracy and more on the ability of institutions to engineer transparent,
reproducible, and accountable data infrastructures that citizens can trust.

</details>


### [352] [Towards LLM-Powered Task-Aware Retrieval of Scientific Workflows for Galaxy](https://arxiv.org/abs/2511.01757)
*Shamse Tasnim Cynthia,Banani Roy*

Main category: cs.SE

TL;DR: 针对Galaxy当前关键词检索系统的不足，提出任务感知的两阶段检索框架，构建基准数据集评估，结果显示该方法提升检索性能并集成到Galaxy。


<details>
  <summary>Details</summary>
Motivation: Galaxy当前基于关键词的检索系统在语义查询解释方面支持有限，缺乏精确匹配时难以找到相关工作流。

Method: 提出任务感知的两阶段检索框架，先使用嵌入模型检索候选工作流，再用指令调优的生成式大语言模型重排序；构建带语义主题注释的基准数据集，合成任务导向查询，用标准信息检索指标评估。

Result: 该方法显著提高了top - k准确率和相关性，尤其针对长或未明确指定的查询。

Conclusion: 此工作推进了科学工作流的可用性和可访问性，对新手和跨学科研究人员有益。

Abstract: Scientific Workflow Management Systems (SWfMSs) such as Galaxy have become
essential infrastructure in bioinformatics, supporting the design, execution,
and sharing of complex multi-step analyses. Despite hosting hundreds of
reusable workflows across domains, Galaxy's current keyword-based retrieval
system offers limited support for semantic query interpretation and often fails
to surface relevant workflows when exact term matches are absent. To address
this gap, we propose a task-aware, two-stage retrieval framework that
integrates dense vector search with large language model (LLM)-based reranking.
Our system first retrieves candidate workflows using state-of-the-art embedding
models and then reranks them using instruction-tuned generative LLMs (GPT-4o,
Mistral-7B) based on semantic task alignment. To support robust evaluation, we
construct a benchmark dataset of Galaxy workflows annotated with semantic
topics via BERTopic and synthesize realistic task-oriented queries using LLMs.
We conduct a comprehensive comparison of lexical, dense, and reranking models
using standard IR metrics, presenting the first systematic evaluation of
retrieval performance in the Galaxy ecosystem. Results show that our approach
significantly improves top-k accuracy and relevance, particularly for long or
under-specified queries. We further integrate our system as a prototype tool
within Galaxy, providing a proof-of-concept for LLM-enhanced workflow search.
This work advances the usability and accessibility of scientific workflows,
especially for novice users and interdisciplinary researchers.

</details>


### [353] [Context-Guided Decompilation: A Step Towards Re-executability](https://arxiv.org/abs/2511.01763)
*Xiaohan Wang,Yuxin Hu,Kevin Leach*

Main category: cs.SE

TL;DR: 现有二进制反编译技术生成代码难重新编译执行，本文提出ICL4Decomp框架，在多数据集等测试中可提升约40%重新可执行性。


<details>
  <summary>Details</summary>
Motivation: 现有反编译技术生成的代码难以重新编译和执行，大语言模型生成的代码也缺乏真正的可执行性，需要解决此问题。

Method: 提出ICL4Decomp混合反编译框架，利用上下文学习引导大语言模型生成可重新执行的源代码。

Result: 在多个数据集、优化级别和编译器上进行评估，相比现有技术，重新可执行性提高约40%，且保持了鲁棒性。

Conclusion: ICL4Decomp框架有效解决了现有反编译技术生成代码难以重新执行的问题，具有较好的性能和鲁棒性。

Abstract: Binary decompilation plays an important role in software security analysis,
reverse engineering, and malware understanding when source code is unavailable.
However, existing decompilation techniques often fail to produce source code
that can be successfully recompiled and re-executed, particularly for optimized
binaries. Recent advances in large language models (LLMs) have enabled neural
approaches to decompilation, but the generated code is typically only
semantically plausible rather than truly executable, limiting their practical
reliability. These shortcomings arise from compiler optimizations and the loss
of semantic cues in compiled code, which LLMs struggle to recover without
contextual guidance. To address this challenge, we propose ICL4Decomp, a hybrid
decompilation framework that leverages in-context learning (ICL) to guide LLMs
toward generating re-executable source code. We evaluate our method across
multiple datasets, optimization levels, and compilers, demonstrating around
40\% improvement in re-executability over state-of-the-art decompilation
methods while maintaining robustness.

</details>


### [354] [SmartMLOps Studio: Design of an LLM-Integrated IDE with Automated MLOps Pipelines for Model Development and Monitoring](https://arxiv.org/abs/2511.01850)
*Jiawei Jin,Yingxin Su,Xiaotong Zhu*

Main category: cs.SE

TL;DR: 研究提出含自动化MLOps管道的LLM集成IDE，实现单环境模型持续开发与监控，SmartMLOps Studio实验效果好，转变IDE为智能平台。


<details>
  <summary>Details</summary>
Motivation: 人工智能和机器学习应用需求增长，传统IDE和现有MLOps平台存在不足，缺乏统一模型开发、部署和监控的集成环境。

Method: 设计含自动化MLOps管道的LLM集成IDE，嵌入LLM助手，后端有自动化数据验证等功能，实现名为SmartMLOps Studio的原型。

Result: SmartMLOps Studio相比传统工作流，减少61%管道配置时间，提高45%实验可重复性，提升14%漂移检测准确率。

Conclusion: 该研究建立了AI工程新范式，将IDE转变为动态、全生命周期的智能平台，利于可扩展和高效的模型开发。

Abstract: The rapid expansion of artificial intelligence and machine learning (ML)
applications has intensified the demand for integrated environments that unify
model development, deployment, and monitoring. Traditional Integrated
Development Environments (IDEs) focus primarily on code authoring, lacking
intelligent support for the full ML lifecycle, while existing MLOps platforms
remain detached from the coding workflow. To address this gap, this study
proposes the design of an LLM-Integrated IDE with automated MLOps pipelines
that enables continuous model development and monitoring within a single
environment. The proposed system embeds a Large Language Model (LLM) assistant
capable of code generation, debugging recommendation, and automatic pipeline
configuration. The backend incorporates automated data validation, feature
storage, drift detection, retraining triggers, and CI/CD deployment
orchestration. This framework was implemented in a prototype named SmartMLOps
Studio and evaluated using classification and forecasting tasks on the UCI
Adult and M5 datasets. Experimental results demonstrate that SmartMLOps Studio
reduces pipeline configuration time by 61%, improves experiment reproducibility
by 45%, and increases drift detection accuracy by 14% compared to traditional
workflows. By bridging intelligent code assistance and automated operational
pipelines, this research establishes a novel paradigm for AI engineering -
transforming the IDE from a static coding tool into a dynamic, lifecycle-aware
intelligent platform for scalable and efficient model development.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [355] [Technical Analysis Meets Machine Learning: Bitcoin Evidence](https://arxiv.org/abs/2511.00665)
*José Ángel Islas Anguiano,Andrés García-Medina*

Main category: q-fin.CP

TL;DR: 本文比较比特币交易表现，LSTM模型年收益约65.23%，优于其他策略，凸显机器学习与技术分析结合潜力。


<details>
  <summary>Details</summary>
Motivation: 受美国证券交易委员会2024年1月10日批准首只现货比特币ETF的推动，评估交易信号在比特币市场获利能力。

Method: 使用LightGBM和LSTM两个机器学习模型，以及EMA交叉和MACD+ADX两个技术分析策略进行比特币交易表现比较。

Result: LSTM模型在不到一年的时间里实现约65.23%的累计回报，显著优于LightGBM、EMA、MACD+ADX策略和买入持有基准。

Conclusion: 强调在快速发展的加密货币领域，机器学习和技术分析更深入融合的潜力。

Abstract: In this note, we compare Bitcoin trading performance using two machine
learning models-Light Gradient Boosting Machine (LightGBM) and Long Short-Term
Memory (LSTM)-and two technical analysis-based strategies: Exponential Moving
Average (EMA) crossover and a combination of Moving Average
Convergence/Divergence with the Average Directional Index (MACD+ADX). The
objective is to evaluate how trading signals can be used to maximize profits in
the Bitcoin market. This comparison was motivated by the U.S. Securities and
Exchange Commission's (SEC) approval of the first spot Bitcoin exchange-traded
funds (ETFs) on 2024-01-10. Our results show that the LSTM model achieved a
cumulative return of approximately 65.23% in under a year, significantly
outperforming LightGBM, the EMA and MACD+ADX strategies, as well as the
baseline buy-and-hold. This study highlights the potential for deeper
integration of machine learning and technical analysis in the rapidly evolving
cryptocurrency landscape.

</details>


### [356] [Trade Execution Flow as the Underlying Source of Market Dynamics](https://arxiv.org/abs/2511.01471)
*Mikhail Gennadievich Belov,Victor Victorovich Dubov,Vadim Konstantinovich Ivanov,Alexander Yurievich Maslov,Olga Vladimirovna Proshina,Vladislav Gennadievich Malyshkin*

Main category: q-fin.CP

TL;DR: 实验证明执行流是市场动态的根本驱动力，开发数值框架计算执行流，有自动确定阈值等特性，还引入基于Christoffel函数谱的框架。


<details>
  <summary>Details</summary>
Motivation: 探索市场动态的根本驱动力，寻求替代传统主成分分析的方法。

Method: 用Radon - Nikodym导数从采样时刻计算执行流，通过对应特征问题确定特征时间尺度；引入基于Christoffel函数谱的框架。

Result: 方法在实际市场数据上得到验证，证明执行流是市场动态的根本驱动力。

Conclusion: 执行流是市场动态的根本驱动力，所提数值框架和基于Christoffel函数谱的框架有应用价值。

Abstract: In this work, we demonstrate experimentally that the execution flow, $I =
dV/dt$, is the fundamental driving force of market dynamics. We develop a
numerical framework to calculate execution flow from sampled moments using the
Radon-Nikodym derivative. A notable feature of this approach is its ability to
automatically determine thresholds that can serve as actionable triggers. The
technique also determines the characteristic time scale directly from the
corresponding eigenproblem. The methodology has been validated on actual market
data to support these findings. Additionally, we introduce a framework based on
the Christoffel function spectrum, which is invariant under arbitrary
non-degenerate linear transformations of input attributes and offers an
alternative to traditional principal component analysis (PCA), which is limited
to unitary invariance.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [357] [Lambda Value-at-Risk under ambiguity and risk sharing](https://arxiv.org/abs/2511.00717)
*Peng Liu,Alexander Schied*

Main category: q-fin.RM

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we investigate the Lambda Value-at-Risk ($\Lambda$VaR) under
ambiguity, where the ambiguity is represented by a family of probability
measures. We establish that for increasing Lambda functions, the robust (i.e.,
worst-case) $\Lambda$VaR under such an ambiguity set is equivalent to
$\Lambda$VaR computed with respect to a capacity, a novel extension in the
literature. This framework unifies and extends both traditional $\Lambda$VaR
and Choquet quantiles (Value-at-Risk under ambiguity). We analyze the
fundamental properties of this extended risk measure and establish a novel
equivalent representation for $\Lambda$VaR under capacities with monotone
Lambda functions in terms of families of downsets. Moreover, explicit formulas
are derived for robust $\Lambda$VaR when ambiguity sets are characterized by
$\phi$-divergence and the likelihood ratio constraints, respectively.
  We further explore the applications in risk sharing among multiple agents. We
demonstrate that the family of risk measures induced by families of downsets is
closed under inf-convolution. In particular, we prove that the inf-convolution
of $\Lambda$VaR with capacities and monotone Lambda functions is
another$\Lambda$VaR under a capacity. The explicit forms of optimal allocations
are also derived. Moreover, we obtain more explicit results for risk sharing
under ambiguity sets characterized by $\phi$-divergence and likelihood ratio
constraints. Finally, we explore comonotonic risk-sharing for $\Lambda$VaR
under ambiguity.

</details>


### [358] [Cost-of-capital valuation with risky assets](https://arxiv.org/abs/2511.00895)
*Hansjörg Albrecher,Filip Lindskog,Hervé Zumbach*

Main category: q-fin.RM

TL;DR: 本文分析缓冲资本投资于风险资产对负债估值的影响，给出理论、特定随机模型和数值结果。


<details>
  <summary>Details</summary>
Motivation: 标准资本成本考虑通常假设缓冲资本投资于无风险一年期债券，本文旨在分析投资于风险资产的影响。

Method: 结合一般理论结果、特定随机模型的明确结果和数值结果进行分析。

Result: 明确了随着投资风险程度增加，缓冲资本中保单持有人和投资者贡献的分解变化，并强调了重尾保险风险下有限责任的作用。

Conclusion: 研究了缓冲资本投资风险资产对负债估值的影响及相关因素的作用。

Abstract: Cost-of-capital valuation is a well-established approach to the valuation of
liabilities and is one of the cornerstones of current regulatory frameworks for
the insurance industry. Standard cost-of-capital considerations typically rely
on the assumption that the required buffer capital is held in risk-less
one-year bonds. The aim of this work is to analyze the effects of allowing
investments of the buffer capital in risky assets, e.g.~in a combination of
stocks and bonds. In particular, we make precise how the decomposition of the
buffer capital into contributions from policyholders and investors varies as
the degree of riskiness of the investment increases, and highlight the role of
limited liability in the case of heavy-tailed insurance risks. We present a
combination of general theoretical results, explicit results for certain
stochastic models and numerical results that emphasize the key findings.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [359] [Deep reinforcement learning for optimal trading with partial information](https://arxiv.org/abs/2511.00190)
*Andrea Macrì,Sebastian Jaimungal,Fabrizio Lillo*

Main category: q-fin.TR

TL;DR: 本文研究基于奥恩斯坦 - 乌伦贝克过程的最优交易问题，结合强化学习和循环神经网络，提出三种基于DDPG的算法，经模拟和实证发现prob - DDPG表现最优，强调信息质量和结构对交易策略的重要性。


<details>
  <summary>Details</summary>
Motivation: 利用强化学习制定最优交易策略以挖掘市场潜在信息的研究较少，本文旨在研究基于奥恩斯坦 - 乌伦贝克过程的最优交易问题。

Method: 采用强化学习和循环神经网络结合的方法，提出三种基于DDPG并集成GRU网络的算法。

Result: 通过模拟和实证，prob - DDPG累积回报最优且策略更具可解释性，reg - DDPG效益有限，hid - DDPG表现居中且策略可解释性差。

Conclusion: 提供给智能体的信息质量和结构至关重要，融入潜在状态的概率信息能提升交易策略的盈利能力和稳健性。

Abstract: Reinforcement Learning (RL) applied to financial problems has been the
subject of a lively area of research. The use of RL for optimal trading
strategies that exploit latent information in the market is, to the best of our
knowledge, not widely tackled. In this paper we study an optimal trading
problem, where a trading signal follows an Ornstein-Uhlenbeck process with
regime-switching dynamics. We employ a blend of RL and Recurrent Neural
Networks (RNN) in order to make the most at extracting underlying information
from the trading signal with latent parameters.
  The latent parameters driving mean reversion, speed, and volatility are
filtered from observations of the signal, and trading strategies are derived
via RL. To address this problem, we propose three Deep Deterministic Policy
Gradient (DDPG)-based algorithms that integrate Gated Recurrent Unit (GRU)
networks to capture temporal dependencies in the signal. The first, a one -step
approach (hid-DDPG), directly encodes hidden states from the GRU into the RL
trader. The second and third are two-step methods: one (prob-DDPG) makes use of
posterior regime probability estimates, while the other (reg-DDPG) relies on
forecasts of the next signal value. Through extensive simulations with
increasingly complex Markovian regime dynamics for the trading signal's
parameters, as well as an empirical application to equity pair trading, we find
that prob-DDPG achieves superior cumulative rewards and exhibits more
interpretable strategies. By contrast, reg-DDPG provides limited benefits,
while hid-DDPG offers intermediate performance with less interpretable
strategies. Our results show that the quality and structure of the information
supplied to the agent are crucial: embedding probabilistic insights into latent
regimes substantially improves both profitability and robustness of
reinforcement learning-based trading strategies.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [360] [Gradient Boosted Mixed Models: Flexible Joint Estimation of Mean and Variance Components for Clustered Data](https://arxiv.org/abs/2511.00217)
*Mitchell L. Prevett,Francis K. C. Hui,Zhi Yang Tho,A. H. Welsh,Anton H. Westveld*

Main category: stat.ML

TL;DR: 引入梯度提升混合模型（GBMixed），结合提升方法估计均值和方差分量，模拟和实际应用显示其效果优于传统方法，还能进行异方差不确定性量化等。


<details>
  <summary>Details</summary>
Motivation: 线性混合模型在复杂高维场景缺乏灵活性，梯度提升方法不能处理聚类数据结构和提供不确定性量化，因此需要新方法。

Method: 引入GBMixed框架和算法，通过基于似然的梯度联合估计均值和方差分量，用回归树或样条等灵活基学习器建模随机效应和残差方差。

Result: 模拟和实际应用表明能准确恢复方差分量、校准预测区间、提高预测准确性。

Conclusion: GBMixed能进行异方差不确定性量化，可用于异质随机效应，在标准因果假设下能估计异质处理效应并提供可靠不确定性量化。

Abstract: Linear mixed models are widely used for clustered data, but their reliance on
parametric forms limits flexibility in complex and high-dimensional settings.
In contrast, gradient boosting methods achieve high predictive accuracy through
nonparametric estimation, but do not accommodate clustered data structures or
provide uncertainty quantification.
  We introduce Gradient Boosted Mixed Models (GBMixed), a framework and
algorithm that extends boosting to jointly estimate mean and variance
components via likelihood-based gradients. In addition to nonparametric mean
estimation, the method models both random effects and residual variances as
potentially covariate-dependent functions using flexible base learners such as
regression trees or splines, enabling nonparametric estimation while
maintaining interpretability.
  Simulations and real-world applications demonstrate accurate recovery of
variance components, calibrated prediction intervals, and improved predictive
accuracy relative to standard linear mixed models and nonparametric methods.
GBMixed provides heteroscedastic uncertainty quantification and introduces
boosting for heterogeneous random effects. This enables covariate-dependent
shrinkage for cluster-specific predictions to adapt between population and
cluster-level data. Under standard causal assumptions, the framework enables
estimation of heterogeneous treatment effects with reliable uncertainty
quantification.

</details>


### [361] [A Streaming Sparse Cholesky Method for Derivative-Informed Gaussian Process Surrogates Within Digital Twin Applications](https://arxiv.org/abs/2511.00366)
*Krishna Prasath Logakannan,Shridhar Vashishtha,Jacob Hochhalter,Shandian Zhe,Robert M. Kirby*

Main category: stat.ML

TL;DR: 本文扩展高斯过程模型以包含导数数据，用稀疏GP近似解决成本问题，实验表明该方法可改善模型，还将算法用于航空航天车辆疲劳裂纹增长建模。


<details>
  <summary>Details</summary>
Motivation: 为使数字孪生模型适应特定物理孪生体，需用物理孪生体的服役数据更新模型，同时希望提高预测准确性。

Method: 扩展高斯过程（GP）模型以包含导数数据，使用稀疏GP近似解决协方差矩阵维度增加的成本问题，并开发扩展以纳入导数。

Result: 数值实验表明，导数增强的稀疏GP方法在动态添加数据时能产生更优模型。

Conclusion: 所开发的算法可用于数字孪生框架中对航空航天车辆的疲劳裂纹增长进行建模。

Abstract: Digital twins are developed to model the behavior of a specific physical
asset (or twin), and they can consist of high-fidelity physics-based models or
surrogates. A highly accurate surrogate is often preferred over multi-physics
models as they enable forecasting the physical twin future state in real-time.
To adapt to a specific physical twin, the digital twin model must be updated
using in-service data from that physical twin. Here, we extend Gaussian process
(GP) models to include derivative data, for improved accuracy, with dynamic
updating to ingest physical twin data during service. Including derivative
data, however, comes at a prohibitive cost of increased covariance matrix
dimension. We circumvent this issue by using a sparse GP approximation, for
which we develop extensions to incorporate derivatives. Numerical experiments
demonstrate that the prediction accuracy of the derivative-enhanced sparse GP
method produces improved models upon dynamic data additions. Lastly, we apply
the developed algorithm within a DT framework to model fatigue crack growth in
an aerospace vehicle.

</details>


### [362] [Accuracy estimation of neural networks by extreme value theory](https://arxiv.org/abs/2511.00490)
*Gero Junike,Marco Oesting*

Main category: stat.ML

TL;DR: 提出用极值理论量化神经网络误差大值，给出帕累托分布形状参数新估计器并做数值实验。


<details>
  <summary>Details</summary>
Motivation: 神经网络虽能逼近紧致集上连续函数，但难以量化其误差，需方法量化误差大值。

Method: 应用极值理论，利用广义帕累托分布近似阈值外误差分布，给出新的帕累托分布形状参数估计器。

Result: 得到可描述神经网络误差的帕累托分布形状参数新估计器，完成数值实验。

Conclusion: 可通过极值理论和新估计器量化神经网络误差大值。

Abstract: Neural networks are able to approximate any continuous function on a compact
set. However, it is not obvious how to quantify the error of the neural
network, i.e., the remaining bias between the function and the neural network.
Here, we propose the application of extreme value theory to quantify large
values of the error, which are typically relevant in applications. The
distribution of the error beyond some threshold is approximately generalized
Pareto distributed. We provide a new estimator of the shape parameter of the
Pareto distribution suitable to describe the error of neural networks.
Numerical experiments are provided.

</details>


### [363] [SOCRATES: Simulation Optimization with Correlated Replicas and Adaptive Trajectory Evaluations](https://arxiv.org/abs/2511.00685)
*Haoting Zhang,Haoxian Chen,Donglin Zhan,Hanyang Zhao,Henry Lam,Wenpin Tang,David Yao,Zeyu Zheng*

Main category: stat.ML

TL;DR: 本文介绍了新的模拟优化程序SOCRATES，它利用大语言模型自动化设计定制算法，分两阶段实现复杂模拟优化问题的高效样本解决方案。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的出现为利用系统结构和自动化模拟优化方法的选择与组合提供新范式，需要开发新的优化程序。

Method: 提出两阶段程序SOCRATES，第一阶段构建系统数字副本集合，用大语言模型进行因果发现指导学习；第二阶段用副本集合评估基线算法，大语言模型作为元优化器分析性能轨迹，迭代修订并组合最终优化调度。

Result: 通过集成大语言模型驱动推理和轨迹感知元优化，创建了有效且样本高效的解决方案。

Conclusion: SOCRATES能有效解决复杂模拟优化问题，具有适应性，可在性能偏离预期时更新调度。

Abstract: The field of simulation optimization (SO) encompasses various methods
developed to optimize complex, expensive-to-sample stochastic systems.
Established methods include, but are not limited to, ranking-and-selection for
finite alternatives and surrogate-based methods for continuous domains, with
broad applications in engineering and operations management. The recent advent
of large language models (LLMs) offers a new paradigm for exploiting system
structure and automating the strategic selection and composition of these
established SO methods into a tailored optimization procedure. This work
introduces SOCRATES (Simulation Optimization with Correlated Replicas and
Adaptive Trajectory Evaluations), a novel two-stage procedure that leverages
LLMs to automate the design of tailored SO algorithms. The first stage
constructs an ensemble of digital replicas of the real system. An LLM is
employed to implement causal discovery from a textual description of the
system, generating a structural `skeleton' that guides the sample-efficient
learning of the replicas. In the second stage, this replica ensemble is used as
an inexpensive testbed to evaluate a set of baseline SO algorithms. An LLM then
acts as a meta-optimizer, analyzing the performance trajectories of these
algorithms to iteratively revise and compose a final, hybrid optimization
schedule. This schedule is designed to be adaptive, with the ability to be
updated during the final execution on the real system when the optimization
performance deviates from expectations. By integrating LLM-driven reasoning
with LLM-assisted trajectory-aware meta-optimization, SOCRATES creates an
effective and sample-efficient solution for complex SO optimization problems.

</details>


### [364] [Perturbations in the Orthogonal Complement Subspace for Efficient Out-of-Distribution Detection](https://arxiv.org/abs/2511.00849)
*Zhexiao Huang,Weihao He,Shutao Deng,Junzhe Chen,Chao Yuan,Hongxin Wang,Changsheng Zhou*

Main category: stat.ML

TL;DR: 介绍了轻量级且有理论依据的OOD检测方法P - OCS，实验表明其能以极低计算成本实现最先进的OOD检测。


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测方法依赖高维表示分离样本，需新方法。

Method: 提出P - OCS方法，在ID特征主空间的正交补空间应用单个投影扰动，增强ID - OOD细微区别。

Result: 在多架构和数据集实验中，P - OCS以极低计算成本实现最先进的OOD检测，无需模型再训练、访问OOD数据或更改模型架构。

Conclusion: P - OCS是一种高效的OOD检测方法，具有低计算成本和良好性能。

Abstract: Out-of-distribution (OOD) detection is essential for deploying deep learning
models in open-world environments. Existing approaches, such as energy-based
scoring and gradient-projection methods, typically rely on high-dimensional
representations to separate in-distribution (ID) and OOD samples. We introduce
P-OCS (Perturbations in the Orthogonal Complement Subspace), a lightweight and
theoretically grounded method that operates in the orthogonal complement of the
principal subspace defined by ID features. P-OCS applies a single projected
perturbation restricted to this complementary subspace, enhancing subtle ID-OOD
distinctions while preserving the geometry of ID representations. We show that
a one-step update is sufficient in the small-perturbation regime and provide
convergence guarantees for the resulting detection score. Experiments across
multiple architectures and datasets demonstrate that P-OCS achieves
state-of-the-art OOD detection with negligible computational cost and without
requiring model retraining, access to OOD data, or changes to model
architecture.

</details>


### [365] [Binary perceptron computational gap -- a parametric fl RDT view](https://arxiv.org/abs/2511.01037)
*Mihailo Stojnic*

Main category: stat.ML

TL;DR: 研究利用全提升随机对偶理论分析非对称二进制感知机，发现参数变化与阈值变化关联，得到约束密度估计值并与相关结果有对应。


<details>
  <summary>Details</summary>
Motivation: 研究全提升随机对偶理论在非对称二进制感知机中的算法意义，探索其与统计 - 计算差距的联系。

Method: 对全提升随机对偶理论进行特定参数化利用并研究其在不同提升层级的变化。

Result: 发现关键参数序列在不同层级的变化，得到不同层级的临界约束密度估计值，在第五层收敛到约 0.7764。

Conclusion: 所得约束密度估计与聚类碎片化范围一致，参数序列变化与负 Hopfield 模型相似。

Abstract: Recent studies suggest that asymmetric binary perceptron (ABP) likely
exhibits the so-called statistical-computational gap characterized with the
appearance of two phase transitioning constraint density thresholds:
\textbf{\emph{(i)}} the \emph{satisfiability threshold} $\alpha_c$, below/above
which ABP succeeds/fails to operate as a storage memory; and
\textbf{\emph{(ii)}} \emph{algorithmic threshold} $\alpha_a$, below/above which
one can/cannot efficiently determine ABP's weight so that it operates as a
storage memory.
  We consider a particular parametric utilization of \emph{fully lifted random
duality theory} (fl RDT) [85] and study its potential ABP's algorithmic
implications. A remarkable structural parametric change is uncovered as one
progresses through fl RDT lifting levels. On the first two levels, the
so-called $\c$ sequence -- a key parametric fl RDT component -- is of the
(natural) decreasing type. A change of such phenomenology on higher levels is
then connected to the $\alpha_c$ -- $\alpha_a$ threshold change. Namely, on the
second level concrete numerical values give for the critical constraint density
$\alpha=\alpha_c\approx 0.8331$. While progressing through higher levels
decreases this estimate, already on the fifth level we observe a satisfactory
level of convergence and obtain $\alpha\approx 0.7764$. This allows to draw two
striking parallels: \textbf{\emph{(i)}} the obtained constraint density
estimate is in a remarkable agrement with range $\alpha\in (0.77,0.78)$ of
clustering defragmentation (believed to be responsible for failure of locally
improving algorithms) [17,88]; and \textbf{\emph{(ii)}} the observed change of
$\c$ sequence phenomenology closely matches the one of the negative Hopfield
model for which the existence of efficient algorithms that closely approach
similar type of threshold has been demonstrated recently [87].

</details>


### [366] [Generalized Guarantees for Variational Inference in the Presence of Even and Elliptical Symmetry](https://arxiv.org/abs/2511.01064)
*Charles C. Margossian,Lawrence K. Saul*

Main category: stat.ML

TL;DR: 本文扩展了基于对称性的变分推理（VI）保证，涵盖更广泛的散度家族和部分对称情况，并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 已知在特定对称性和反向Kullback - Leibler散度下，VI能恢复目标密度的关键属性，为扩展这些保证而开展研究。

Method: 从两个方向扩展保证，一是针对更广泛的散度家族提供基于对称性的保证，二是在目标密度部分坐标有对称时获得进一步保证。

Result: 得到了更广泛散度家族和部分对称情况下的VI保证，并在多个实验设置中进行了验证。

Conclusion: 成功扩展了基于对称性的变分推理保证，适用于更广泛的情况。

Abstract: We extend several recent results providing symmetry-based guarantees for
variational inference (VI) with location-scale families. VI approximates a
target density~$p$ by the best match $q^*$ in a family $Q$ of tractable
distributions that in general does not contain $p$. It is known that VI can
recover key properties of $p$, such as its mean and correlation matrix, when
$p$ and $Q$ exhibit certain symmetries and $q^*$ is found by minimizing the
reverse Kullback-Leibler divergence. We extend these guarantees in two
important directions. First, we provide symmetry-based guarantees for a broader
family of divergences, highlighting the properties of variational objectives
under which VI provably recovers the mean and correlation matrix. Second, we
obtain further guarantees for VI when the target density $p$ exhibits even and
elliptical symmetries in some but not all of its coordinates. These partial
symmetries arise naturally in Bayesian hierarchical models, where the prior
induces a challenging geometry but still possesses axes of symmetry. We
illustrate these theoretical results in a number of experimental settings.

</details>


### [367] [Hyper Hawkes Processes: Interpretable Models of Marked Temporal Point Processes](https://arxiv.org/abs/2511.01096)
*Alex Boyd,Andrew Warrington,Taha Kass-Hout,Parminder Bhatia,Danica Xiao*

Main category: stat.ML

TL;DR: 提出超Hawkes过程（HHP）模型，兼具神经MTPPs性能和经典Hawkes过程可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基础MTPP模型表达能力弱，神经MTPPs缺乏可解释性，希望新模型兼具两者优点。

Method: 将经典Hawkes过程扩展到潜在空间，引入超网络实现时间和数据依赖动态。

Result: 在一系列基准任务和指标上达到了最先进的性能。

Conclusion: HHP模型既能提供最先进的预测，又能解释预测生成方式。

Abstract: Foundational marked temporal point process (MTPP) models, such as the Hawkes
process, often use inexpressive model families in order to offer interpretable
parameterizations of event data. On the other hand, neural MTPPs models forego
this interpretability in favor of absolute predictive performance. In this
work, we present a new family MTPP models: the hyper Hawkes process (HHP),
which aims to be as flexible and performant as neural MTPPs, while retaining
interpretable aspects. To achieve this, the HHP extends the classical Hawkes
process to increase its expressivity by first expanding the dimension of the
process into a latent space, and then introducing a hypernetwork to allow time-
and data-dependent dynamics. These extensions define a highly performant MTPP
family, achieving state-of-the-art performance across a range of benchmark
tasks and metrics. Furthermore, by retaining the linearity of the recurrence,
albeit now piecewise and conditionally linear, the HHP also retains much of the
structure of the original Hawkes process, which we exploit to create direct
probes into how the model creates predictions. HHP models therefore offer both
state-of-the-art predictions, while also providing an opportunity to ``open the
box'' and inspect how predictions were generated.

</details>


### [368] [Few-Shot Multimodal Medical Imaging: A Theoretical Framework](https://arxiv.org/abs/2511.01140)
*Md Talha Mohsin,Ismail Abdulrashid*

Main category: stat.ML

TL;DR: 论文指出医学影像缺乏大的标注数据集及面临结构障碍，现有方法缺理论依据，提出统一理论框架解决低资源医学影像问题。


<details>
  <summary>Details</summary>
Motivation: 医学影像临床中难以获取大的标注数据集，存在数据可用性有限、系统碎片化、数据不平衡等问题，现有方法缺乏理论支撑。

Method: 先形式化少样本学习目标并计算样本复杂度约束，基于PAC学习和PAC贝叶斯理论解释多模态集成，提出解释稳定性的形式化指标。

Result: 建立统一理论框架，联合表征样本效率、不确定性量化和可解释性。

Conclusion: 该框架为构建可靠、数据高效的诊断系统奠定了原则性基础。

Abstract: Medical imaging relies heavily on large, labeled datasets. But,
unfortunately, they are not always easily accessible in clinical settings.
Additionally, many practitioners often face various structural obstacles like
limited data availability, fragmented data systems, and unbalanced datasets.
These barriers often lead to the increased diagnostic uncertainty,
underrepresentation of certain conditions, reduced model robustness, and biased
diagnostic decisions. In response to these challenges, approaches such as
transfer learning, meta-learning, and multimodal fusion have made great
strides. However, they still need a solid theoretical justification for why
they succeed or fail in situations where data is scarce. To address this gap,
we propose a unified theoretical framework that characterizes learning and
inference under low-resource medical imaging conditions. We first formalize the
learning objective under few-shot conditions and compute sample complexity
constraints to estimate the smallest quantity of data needed to achieve
clinically reliable accuracy. Then based on ideas from PAC-learning and
PAC-Bayesian theory, we explain how multimodal integration encourages
generalization and quantifies uncertainty under sparse supervision. We further
propose a formal metric for explanation stability, offering interpretability
guarantees under low-data conditions. Taken together, the proposed framework
establishes a principled foundation for constructing dependable, data-efficient
diagnostic systems by jointly characterizing sample efficiency, uncertainty
quantification, and interpretability in a unified theoretical setting.

</details>


### [369] [An Interdisciplinary and Cross-Task Review on Missing Data Imputation](https://arxiv.org/abs/2511.01196)
*Jicong Fan*

Main category: stat.ML

TL;DR: 本文系统回顾数据缺失值插补方法，涵盖概念、分类、下游任务等，指出挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 现有文献分散，需综合统计基础与机器学习进展来解决数据缺失这一挑战。

Method: 系统回顾核心概念，对插补方法分类，研究与下游任务集成，评估理论保证、基准资源和评估指标。

Result: 完成插补方法分类，研究插补与下游任务结合，评估相关理论与指标。

Conclusion: 识别关键挑战和未来方向，如模型选择、隐私保护插补、通用模型追求等。

Abstract: Missing data is a fundamental challenge in data science, significantly
hindering analysis and decision-making across a wide range of disciplines,
including healthcare, bioinformatics, social science, e-commerce, and
industrial monitoring. Despite decades of research and numerous imputation
methods, the literature remains fragmented across fields, creating a critical
need for a comprehensive synthesis that connects statistical foundations with
modern machine learning advances. This work systematically reviews core
concepts-including missingness mechanisms, single versus multiple imputation,
and different imputation goals-and examines problem characteristics across
various domains. It provides a thorough categorization of imputation methods,
spanning classical techniques (e.g., regression, the EM algorithm) to modern
approaches like low-rank and high-rank matrix completion, deep learning models
(autoencoders, GANs, diffusion models, graph neural networks), and large
language models. Special attention is given to methods for complex data types,
such as tensors, time series, streaming data, graph-structured data,
categorical data, and multimodal data. Beyond methodology, we investigate the
crucial integration of imputation with downstream tasks like classification,
clustering, and anomaly detection, examining both sequential pipelines and
joint optimization frameworks. The review also assesses theoretical guarantees,
benchmarking resources, and evaluation metrics. Finally, we identify critical
challenges and future directions, emphasizing model selection and
hyperparameter optimization, the growing importance of privacy-preserving
imputation via federated learning, and the pursuit of generalizable models that
can adapt across domains and data types, thereby outlining a roadmap for future
research.

</details>


### [370] [Optimal Attention Temperature Enhances In-Context Learning under Distribution Shift](https://arxiv.org/abs/2511.01292)
*Samet Demir,Zafer Dogan*

Main category: stat.ML

TL;DR: 研究分布偏移下预训练Transformer上下文学习中注意力温度的作用，理论推导与实验验证其可提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 预训练Transformer上下文学习在分布偏移下性能下降，而注意力温度在其中的作用未被研究。

Method: 使用“线性化softmax”框架推导泛化误差表达式，在回归任务和问答基准上进行模拟与实验。

Result: 推导证明输入协方差或标签噪声偏移会损害上下文学习，存在最优注意力温度使误差最小，实验验证了预测。

Conclusion: 注意力温度是提升预训练Transformer上下文学习鲁棒性的有效机制，为实际选择提供指导。

Abstract: Pretrained Transformers excel at in-context learning (ICL), inferring new
tasks from only a handful of examples. Yet, their ICL performance can degrade
sharply under distribution shift between pretraining and test data, a regime
increasingly common in real-world deployments. While recent empirical work
hints that adjusting the attention temperature in the softmax can enhance
Transformer performance, the attention temperature's role in ICL under
distribution shift remains unexplored. This paper provides the first
theoretical and empirical study of attention temperature for ICL under
distribution shift. Using a simplified but expressive "linearized softmax"
framework, we derive closed-form generalization error expressions and prove
that shifts in input covariance or label noise substantially impair ICL, but
that an optimal attention temperature exists which minimizes this error. We
then validate our predictions through extensive simulations on linear
regression tasks and large-scale experiments with GPT-2 and LLaMA2-7B on
question-answering benchmarks. Our results establish attention temperature as a
principled and powerful mechanism for improving the robustness of ICL in
pretrained Transformers, advancing theoretical understanding and providing
actionable guidance for selecting attention temperature in practice.

</details>


### [371] [Partial Trace-Class Bayesian Neural Networks](https://arxiv.org/abs/2511.01628)
*Arran Carter,Torben Sell*

Main category: stat.ML

TL;DR: 提出部分迹类贝叶斯神经网络（PaTraC BNNs）三种架构，可实现与标准BNNs相当的不确定性量化，且计算和统计上有优势。


<details>
  <summary>Details</summary>
Motivation: 标准贝叶斯神经网络（BNNs）不确定性量化计算成本高，需改进。

Method: 提出三种不同的PaTraC BNNs架构，基于迹类神经网络先验。

Result: 数值模拟研究验证了优势，在真实数据集上展示了性能。

Conclusion: 所提方法有助于神经网络中可靠、稳健和可扩展的不确定性量化。

Abstract: Bayesian neural networks (BNNs) allow rigorous uncertainty quantification in
deep learning, but often come at a prohibitive computational cost. We propose
three different innovative architectures of partial trace-class Bayesian neural
networks (PaTraC BNNs) that enable uncertainty quantification comparable to
standard BNNs but use significantly fewer Bayesian parameters. These PaTraC
BNNs have computational and statistical advantages over standard Bayesian
neural networks in terms of speed and memory requirements. Our proposed
methodology therefore facilitates reliable, robust, and scalable uncertainty
quantification in neural networks. The three architectures build on trace-class
neural network priors which induce an ordering of the neural network
parameters, and are thus a natural choice in our framework. In a numerical
simulation study, we verify the claimed benefits, and further illustrate the
performance of our proposed methodology on a real-world dataset.

</details>


### [372] [A Proof of Learning Rate Transfer under $μ$P](https://arxiv.org/abs/2511.01734)
*Soufiane Hayou*

Main category: stat.ML

TL;DR: 首次证明线性多层感知机中学习率随宽度的可迁移性，对比不同参数化方式，有理论证明和实证结果。


<details>
  <summary>Details</summary>
Motivation: 对线性多层感知机中学习率随宽度的可迁移性进行理论解释。

Method: 对μP参数化的线性多层感知机进行理论证明，并与SP和NTP等参数化方式对比，辅以大量实证。

Result: 在μP下，最优学习率在宽度趋于无穷时收敛到非零常数，而SP和NTP不具备此性质。

Conclusion: 证明了μP参数化下学习率随宽度的可迁移性，为其提供理论依据。

Abstract: We provide the first proof of learning rate transfer with width in a linear
multi-layer perceptron (MLP) parametrized with $\mu$P, a neural network
parameterization designed to ``maximize'' feature learning in the
infinite-width limit. We show that under $\mu P$, the optimal learning rate
converges to a \emph{non-zero constant} as width goes to infinity, providing a
theoretical explanation to learning rate transfer. In contrast, we show that
this property fails to hold under alternative parametrizations such as Standard
Parametrization (SP) and Neural Tangent Parametrization (NTP). We provide
intuitive proofs and support the theoretical findings with extensive empirical
results.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [373] [Polynomial Mixing Times of Simulated Tempering for Mixture Targets by Conductance Decomposition](https://arxiv.org/abs/2511.00708)
*Quan Zhou*

Main category: stat.CO

TL;DR: 研究模拟回火从仅位置偏移不同的对数凹分量混合中采样的理论复杂度，给出模拟回火结合MALA的多项式时间保证及改进复杂度估计，证明逆温度梯最优性。


<details>
  <summary>Details</summary>
Motivation: 研究模拟回火从特定对数凹分量混合中采样的理论复杂度。

Method: 运用s - 传导性的一般状态分解定理，应用于增广空间上构造的辅助马尔可夫链。

Result: 得到模拟回火结合MALA的首个多项式时间保证，及模拟回火结合随机游走Metropolis的改进复杂度估计，证明逆温度梯的渐近最优性。

Conclusion: 在给定逆温度梯条件下，模拟回火在采样问题上有良好理论复杂度，逆温度梯设置渐近最优。

Abstract: We study the theoretical complexity of simulated tempering for sampling from
mixtures of log-concave components differing only by location shifts. The main
result establishes the first polynomial-time guarantee for simulated tempering
combined with the Metropolis-adjusted Langevin algorithm (MALA) with respect to
the problem dimension $d$, maximum mode displacement $D$, and logarithmic
accuracy $\log \epsilon^{-1}$. The proof builds on a general state
decomposition theorem for $s$-conductance, applied to an auxiliary Markov chain
constructed on an augmented space. We also obtain an improved complexity
estimate for simulated tempering combined with random-walk Metropolis. Our
bounds assume an inverse-temperature ladder with smallest value $\beta_1 =
O(D^{-2})$ and spacing $\beta_{i+1}/\beta_i = 1 + O( d^{-1/2} )$, both of which
are shown to be asymptotically optimal up to logarithmic factors.

</details>


### [374] [Particle Filter Made Simple: A Step-by-Step Beginner-friendly Guide](https://arxiv.org/abs/2511.01281)
*Sahil Rajesh Dhayalkar*

Main category: stat.CO

TL;DR: 本书介绍粒子滤波器核心思想，通过实例和代码帮助读者理解其算法流程和原理。


<details>
  <summary>Details</summary>
Motivation: 鉴于粒子滤波器在处理动态系统中不确定性、噪声和非线性问题的优势，提供清晰结构化的介绍。

Method: 从卡尔曼滤波器局限出发，逐步阐述粒子滤波器思想，连接其与数学基础，结合实例、数值演示和Python代码讲解。

Result: 读者能理解粒子滤波器算法流程，直观掌握随机性和结构性如何使系统实时推断和处理噪声观测。

Conclusion: 本书有效帮助读者理解粒子滤波器，弥合理论与实践差距。

Abstract: The particle filter is a powerful framework for estimating hidden states in
dynamic systems where uncertainty, noise, and nonlinearity dominate. This
mini-book offers a clear and structured introduction to the core ideas behind
particle filters-how they represent uncertainty through random samples, update
beliefs using observations, and maintain robustness where linear or Gaussian
assumptions fail. Starting from the limitations of the Kalman filter, the book
develops the intuition that drives the particle filter: belief as a cloud of
weighted hypotheses that evolve through prediction, measurement, and
resampling. Step by step, it connects these ideas to their mathematical
foundations, showing how probability distributions can be approximated by a
finite set of particles and how Bayesian reasoning unfolds in sampled form.
Illustrated examples, numerical walk-throughs, and Python code bring each
concept to life, bridging the gap between theory and implementation. By the
end, readers will not only understand the algorithmic flow of the particle
filter but also develop an intuitive grasp of how randomness and structure
together enable systems to infer, adapt, and make sense of noisy observations
in real time.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [375] [Agentic Auto-Scheduling: An Experimental Study of LLM-Guided Loop Optimization](https://arxiv.org/abs/2511.00592)
*Massinissa Merouani,Islem Kara Bernou,Riyadh Baghdadi*

Main category: cs.PL

TL;DR: 本文提出ComPilot框架，利用大语言模型与编译器闭环交互进行代码优化，在PolyBench基准测试中效果良好，证明大语言模型可有效指导代码优化。


<details>
  <summary>Details</summary>
Motivation: 解决现代硬件上复杂循环嵌套的自动代码优化难题。

Method: 提出ComPilot框架，利用现成大语言模型作为交互式优化代理，建立反馈循环，大语言模型提出变换，编译器尝试并反馈，大语言模型迭代优化策略。

Result: ComPilot在PolyBench基准测试中，单次运行实现2.66倍、5次最佳运行实现3.54倍的几何平均加速，且在很多情况下优于最先进的Pluto多面体优化器。

Conclusion: 通用大语言模型在编译器反馈的基础上可有效指导代码优化，为代码优化中的智能AI开辟了有前景的研究方向。

Abstract: Automatic code optimization remains a difficult challenge, particularly for
complex loop nests on modern hardware. This paper investigates a novel approach
to code optimization where Large Language Models (LLMs) guide the process
through a closed-loop interaction with a compiler. We present ComPilot, an
experimental framework that leverages off-the-shelf LLMs, without any
task-specific fine-tuning, as interactive optimization agents. ComPilot
establishes a feedback loop where an LLM proposes transformations for a given
loop nest to a compiler. The compiler attempts the transformations, reporting
back legality status and measured speedup or slowdown. The LLM utilizes this
concrete feedback to iteratively refine its optimization strategy. Our
extensive evaluation across the PolyBench benchmark suite demonstrates the
effectiveness of this zero-shot approach. ComPilot achieves geometric mean
speedups of 2.66x (single run) and 3.54x (best-of-5 runs) over the original
code. Furthermore, ComPilot demonstrates competitive performance against the
state-of-the-art Pluto polyhedral optimizer, outperforming it in many cases.
This experimental study demonstrates that general-purpose LLMs can effectively
guide the code optimization process when grounded by compiler feedback, opening
promising research directions for agentic AI in code optimization.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [376] [Impact and Relevance of Cognition Journal in the Field of Cognitive Science: An Evaluation](https://arxiv.org/abs/2511.01485)
*M Sadik Batcha,Younis Rashid Dar,Muneer Ahmad*

Main category: cs.DL

TL;DR: 对《Cognition》期刊1999 - 2018年进行科学计量分析，展示研究活动概况并确定高产作者、机构和国家等。


<details>
  <summary>Details</summary>
Motivation: 总结该期刊的研究活动并刻画其多方面特征。

Method: 对期刊文章进行年份分布、作者、机构、国家和引文分析。

Result: 1999 - 2018年《Cognition》发表2870篇论文，确定了前20高产作者、机构和国家，美国研究者贡献比例最高。

Conclusion: 未明确提及结论性内容。

Abstract: This study aims to present a scientometric analysis of the journal titled
Cognition for a period of 20 years from 1999 to 2018. The present study was
conducted with an aim to provide a summary of research activity in current
journal and characterize its most aspects. The research coverage includes the
year wise distribution of articles, authors, institutions, countries and
citation analysis of the journal. The analysis showed that 2870 papers were
published in journal of Cognition from 1999 to 2018. The study identified top
20 prolific authors, institutions and countries of the journal. Researchers
from USA have been made the most percentage of contributions.

</details>


### [377] [Calculating Web Impact Factor for University Websites of Jammu and Kashmir: A Study](https://arxiv.org/abs/2511.01496)
*Muneer Ahmad,M Sadik Batcha,Wasim Rashid,Obaid Hafiz*

Main category: cs.DL

TL;DR: 本文通过网络计量研究分析查谟和克什米尔12所大学网站的网络影响因子，确定网站域名系统，分析网页和链接页面数量并计算相关影响因子，指出部分网站问题并给出排名。


<details>
  <summary>Details</summary>
Motivation: 对查谟和克什米尔12所大学网站的网络影响因子进行研究分析。

Method: 进行网络计量研究，确定网站域名系统，分析网页和链接页面数量，计算简单网络影响因子和外部网络影响因子。

Result: 部分大学网站网页数量多但链接页面少，影响因子落后；查谟集群大学在内链网络影响因子排名第1，什里玛塔瓦伊什诺德维大学在外链网络影响因子排名第1。

Conclusion: 通过研究明确了各大学网站的网络影响因子情况及排名。

Abstract: This paper examines and explores the web impact factor through a webometric
study of the present 12 University Websites of Jammu and Kashmir. Identifies
the domain systems of the websites; analyzes the number of web pages and link
pages, and calculates the External Link WIF or simple web impact factor (WIF)
and external web impact factor of all the University websites. Also reflects
that some university websites have higher number of web pages, but
correspondingly their link pages are very small in number and websites fall
behind in their simple and external link web impact factor. It found that the
Cluster University of Jammu ranked 1 (0.9018) in Internal Link WIF of Websites
in Jammu and Kashmir. Shri Mata Vaishno Devi University ranked 1 (0.7249) in
External Link Web Impact Factor.

</details>


### [378] [AI Literacy in UAE Libraries: Assessing Competencies, Training Needs, and Ethical Considerations for the Digital Age](https://arxiv.org/abs/2511.01353)
*Zafar Imam Khan*

Main category: cs.DL

TL;DR: 研究采用定量方法调查阿联酋图书馆信息科学专业人员的人工智能素养，发现认知能力强，但行为和规范能力有差距，且培训项目有效性不足。


<details>
  <summary>Details</summary>
Motivation: 探索图书馆专业人员的人工智能素养现状。

Method: 对阿联酋92名图书馆信息科学专业人员进行问卷调查的定量研究方法。

Result: 发现专业人员有较强的认知能力，但在行为和规范能力上存在差距，尤其在人工智能偏见、人工智能学习和伦理考量方面；当前培训项目有效性与对人工智能技能的重视程度不匹配。

Conclusion: 图书馆专业人员的人工智能素养存在能力差距，当前培训项目效果不佳。

Abstract: The study explores the current state of artificial intelligence (AI) literacy
levels among library professionals employing a quantitative approach consisting
of 92 surveys of LIS professionals in the United Arab Emirates (UAE). Findings
of the study revealed the presence of strong cognitive competencies, while
there were gaps observed in behavioral and normative competencies, especially
related to AI biases, AI-powered learning, and ethical considerations. There
was a disconnect observed between the perceived importance of AI skills and the
effectiveness of the current training programs.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [379] [Emotion Detection in Speech Using Lightweight and Transformer-Based Models: A Comparative and Ablation Study](https://arxiv.org/abs/2511.00402)
*Lucky Onyekwelu-Udoka,Md Shafiqul Islam,Md Shahedul Hasan*

Main category: cs.SD

TL;DR: 本文对比分析DistilHuBERT和PaSST两种轻量级基于Transformer的模型用于语音情感识别，发现DistilHuBERT性能最佳，为边缘设备实时语音情感识别提供解决方案。


<details>
  <summary>Details</summary>
Motivation: 语音情感识别对共情人机交互系统开发至关重要，需研究轻量级模型用于边缘设备实时识别。

Method: 使用CREMA - D数据集对六种核心情感分类，对比DistilHuBERT、PaSST和传统CNN - LSTM基线模型，对PaSST的三种变体进行消融研究。

Result: DistilHuBERT准确率70.64%、F1分数70.36%，模型大小0.02 MB，性能最佳；PaSST的MLP头变体表现最好但不如DistilHuBERT；愤怒情感最易检测，厌恶最难。

Conclusion: 像DistilHuBERT这样的轻量级Transformer为边缘设备实时语音情感识别提供了有吸引力的解决方案。

Abstract: Emotion recognition from speech plays a vital role in the development of
empathetic human-computer interaction systems. This paper presents a
comparative analysis of lightweight transformer-based models, DistilHuBERT and
PaSST, by classifying six core emotions from the CREMA-D dataset. We benchmark
their performance against a traditional CNN-LSTM baseline model using MFCC
features. DistilHuBERT demonstrates superior accuracy (70.64%) and F1 score
(70.36%) while maintaining an exceptionally small model size (0.02 MB),
outperforming both PaSST and the baseline. Furthermore, we conducted an
ablation study on three variants of the PaSST, Linear, MLP, and Attentive
Pooling heads, to understand the effect of classification head architecture on
model performance. Our results indicate that PaSST with an MLP head yields the
best performance among its variants but still falls short of DistilHuBERT.
Among the emotion classes, angry is consistently the most accurately detected,
while disgust remains the most challenging. These findings suggest that
lightweight transformers like DistilHuBERT offer a compelling solution for
real-time speech emotion recognition on edge devices. The code is available at:
https://github.com/luckymaduabuchi/Emotion-detection-.

</details>


### [380] [More Than A Shortcut: A Hyperbolic Approach To Early-Exit Networks](https://arxiv.org/abs/2511.00641)
*Swapnil Bhosale,Cosmin Frateanu,Camilla Clark,Arnoldas Jasonas,Chris Mitchell,Xiatian Zhu,Vamsi Krishna Ithapu,Giacomo Ferroni,Cagdas Bilen,Sanjeel Parekh*

Main category: cs.SD

TL;DR: 提出双曲早期退出网络HypEE用于资源受限设备的事件检测，在多任务实验中表现优于欧几里得早期退出基线。


<details>
  <summary>Details</summary>
Motivation: 解决在资源受限设备上部署准确事件检测时性能与计算成本的权衡问题，以及早期退出网络缺乏连贯层次结构的问题。

Method: 提出Hyperbolic Early-Exit networks (HypEE)框架，采用具有新的蕴含损失的分层训练目标，在双曲空间学习早期退出表示。

Result: 在多个音频事件检测任务和骨干架构上的实验表明，HypEE显著优于标准欧几里得早期退出基线，特别是在最早、计算最关键的退出点。

Conclusion: HypEE使整个系统比传统早期退出和无早期退出的标准骨干模型更高效、更准确，且学习到的几何结构提供了不确定性的原则性度量。

Abstract: Deploying accurate event detection on resource-constrained devices is
challenged by the trade-off between performance and computational cost. While
Early-Exit (EE) networks offer a solution through adaptive computation, they
often fail to enforce a coherent hierarchical structure, limiting the
reliability of their early predictions. To address this, we propose Hyperbolic
Early-Exit networks (HypEE), a novel framework that learns EE representations
in the hyperbolic space. Our core contribution is a hierarchical training
objective with a novel entailment loss, which enforces a partial-ordering
constraint to ensure that deeper network layers geometrically refine the
representations of shallower ones. Experiments on multiple audio event
detection tasks and backbone architectures show that HypEE significantly
outperforms standard Euclidean EE baselines, especially at the earliest, most
computationally-critical exits. The learned geometry also provides a principled
measure of uncertainty, enabling a novel triggering mechanism that makes the
overall system both more efficient and more accurate than a conventional EE and
standard backbone models without early-exits.

</details>


### [381] [Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play](https://arxiv.org/abs/2511.01261)
*Jiatong Shi,Jionghao Han,Yichen Lu,Santiago Pascual,Pengfei Wu,Chenye Cui,Shinji Watanabe,Chao Weng,Cong Zhou*

Main category: cs.SD

TL;DR: 提出Speech - DRAME框架用于语音角色扮演评估，含评估基准、评估模型等，表现优于零样本ALLM法官，提供综合可复现基础。


<details>
  <summary>Details</summary>
Motivation: 当前语音角色扮演评估存在问题，如ALLM作零样本法官有缺陷，需新评估方法。

Method: 提出Speech - DRAME框架，包括评估基准、微调评估模型、语音角色扮演基准，区分两种评估策略。

Result: DRAME - Eval与人类评分的皮尔逊相关性提高，在原型评估中从0.480提升到0.629，在现实性评估中从0.390提升到0.625。

Conclusion: Speech - DRAME为评估口语角色扮演提供首个全面、可复现的基础。

Abstract: Role-play has become a key testbed for generative models, expanding from
text-only dialogue to multimodal interaction. Extending role-play to speech
captures prosody, emotion, and delivery, but also poses new evaluation
challenges. Current pipelines often use audio large language models (ALLMs) as
zero-shot judges, which miss paralinguistic cues, collapse multiple aspects
into coarse scores, and rely on synthetic speech references that fail to
reflect real-world roles. We present Speech-DRAME, a unified framework that
contributes at three levels: (i) Speech-DRAME-EvalBench, an evaluation
benchmark with bilingual human-annotated data and protocols for training and
testing speech evaluation models (SEMs), (ii) DRAME-Eval, a fine-tuned
evaluation model, which substantially outperforms zero-shot and few-shot ALLMs,
and (iii) Speech-DRAME-RoleBench, a speech role-play benchmark that leverages
DRAME-Eval as an automatic judge to compare speech foundation models (SFMs).
Speech-DRAME distinguishes between two complementary evaluation strategies:
Archetype Evaluation, a top-down approach measuring adherence to broad role
archetypes, and Realism Evaluation, a bottom-up approach grounded in real human
speech that emphasizes nuanced role quality. Compared to zero-shot ALLM judges,
DRAME-Eval achieves stronger agreement with human ratings (Pearson correlation
from 0.480 to 0.629 in archetypes, and 0.390 to 0.625 in realism). By
integrating transparent benchmark resources, modeling approaches, and
system-level evaluation, Speech-DRAME provides the first comprehensive,
reproducible foundation for assessing spoken role-play.

</details>


### [382] [The Ghost in the Keys: A Disklavier Demo for Human-AI Musical Co-Creativity](https://arxiv.org/abs/2511.01663)
*Louis Bradshaw,Alexander Spangher,Stella Biderman,Simon Colton*

Main category: cs.SD

TL;DR: 介绍Aria - Duet交互式系统，实现人类钢琴家与生成模型实时音乐二重奏，系统输出有音乐价值，为人机共创开辟新路径。


<details>
  <summary>Details</summary>
Motivation: 现有音乐生成模型因文本提示和异步工作流程，阻碍了音乐家的采用，需解决与乐器演奏实时响应特性脱节的问题。

Method: 引入Aria - Duet系统，以Yamaha Disklavier为共享物理接口，实现人机轮流协作，用户演奏、交接信号，模型生成连贯续曲。

Result: 从音乐学角度分析系统输出，模型能保持风格语义并发展连贯乐句想法。

Conclusion: 此类具身系统可进行复杂音乐对话，为人机共创开辟了有前景的新路径。

Abstract: While generative models for music composition are increasingly capable, their
adoption by musicians is hindered by text-prompting, an asynchronous workflow
disconnected from the embodied, responsive nature of instrumental performance.
To address this, we introduce Aria-Duet, an interactive system facilitating a
real-time musical duet between a human pianist and Aria, a state-of-the-art
generative model, using a Yamaha Disklavier as a shared physical interface. The
framework enables a turn-taking collaboration: the user performs, signals a
handover, and the model generates a coherent continuation performed
acoustically on the piano. Beyond describing the technical architecture
enabling this low-latency interaction, we analyze the system's output from a
musicological perspective, finding the model can maintain stylistic semantics
and develop coherent phrasal ideas, demonstrating that such embodied systems
can engage in musically sophisticated dialogue and open a promising new path
for human-AI co-creation.

</details>


### [383] [ADNAC: Audio Denoiser using Neural Audio Codec](https://arxiv.org/abs/2511.01773)
*Daniel Jimon,Mircea Vaida,Adriana Stan*

Main category: cs.SD

TL;DR: 本文提出将Descript Audio Codec (DAC) 用于音乐去噪的概念验证，通过定制数据集和多目标损失函数实现高保真音频恢复。


<details>
  <summary>Details</summary>
Motivation: 音频去噪对信号处理很重要，传统架构有局限性，旨在实现高保真、生成式音频恢复。

Method: 使用大规模、自定义合成的多样化数据集训练DAC模型，采用结合时域、频谱和信号级保真度指标的多目标损失函数。

Result: 未提及具体结果。

Conclusion: 未提及明确结论，但旨在呈现高保真音频恢复的概念验证。

Abstract: Audio denoising is critical in signal processing, enhancing intelligibility
and fidelity for applications like restoring musical recordings. This paper
presents a proof-of-concept for adapting a state-of-the-art neural audio codec,
the Descript Audio Codec (DAC), for music denoising. This work overcomes the
limitations of traditional architectures like U-Nets by training the model on a
large-scale, custom-synthesized dataset built from diverse sources. Training is
guided by a multi objective loss function that combines time-domain, spectral,
and signal-level fidelity metrics. Ultimately, this paper aims to present a PoC
for high-fidelity, generative audio restoration.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [384] [Stochastic Derivative Estimation for Discontinuous Sample Performances: A Leibniz Integration Perspective](https://arxiv.org/abs/2511.00006)
*Xingyu Ren,Michael C. Fu,Pierre L'Ecuyer*

Main category: stat.ME

TL;DR: 本文基于多维莱布尼茨积分规则，开发了用于不连续样本性能函数的随机导数估计框架，并通过数值实验验证其有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为样本性能函数在感兴趣参数上不连续的情况开发导数估计框架。

Method: 对于不连续源于指示函数的情况，将指示函数嵌入样本空间；对于一般不连续函数，进行变量替换，应用莱布尼茨积分规则。

Result: 提出的莱布尼茨积分框架推广了广义似然比方法，能给出何时曲面积分消失的直观解释，实现单运行、易实现的估计器。

Conclusion: 数值实验证明了方法的有效性和鲁棒性。

Abstract: We develop a novel stochastic derivative estimation framework for sample
performance functions that are discontinuous in the parameter of interest,
based on the multidimensional Leibniz integral rule. When discontinuities arise
from indicator functions, we embed the indicator functions into the sample
space, yielding a continuous performance function over a parameter-dependent
domain. Applying the Leibniz integral rule in this case produces a single-run,
unbiased derivative estimator. For general discontinuous functions, we apply a
change of variables to shift parameter dependence into the sample space and the
underlying probability measure. Applying the Leibniz integral rule leads to two
terms: a standard likelihood ratio (LR) term from differentiating the
underlying probability measure and a surface integral from differentiating the
boundary of the domain. Evaluating the surface integral may require simulating
multiple sample paths. Our proposed Leibniz integration framework generalizes
the generalized LR (GLR) method and provides intuition as to when the surface
integral vanishes, thereby enabling single-run, easily implementable
estimators. Numerical experiments demonstrate the effectiveness and robustness
of our methods.

</details>


### [385] [Is Representational Similarity Analysis Reliable? A Comparison with Regression](https://arxiv.org/abs/2511.00395)
*Chuanji Gao,Gang Chen,Svetlana V. Shinkareva,Rutvik H. Desai*

Main category: stat.ME

TL;DR: 评估RSA在模型选择中的准确性和可靠性并与回归比较，发现RSA在模型选择上不如回归。


<details>
  <summary>Details</summary>
Motivation: 评估RSA在模型选择中的准确性和可靠性，并与回归方法进行对比。

Method: 通过广泛的模拟研究和实证分析，还使用主成分分析和特征重加权方法。

Result: 无论样本大小、噪声水平、特征维度或多重共线性如何，RSA的模型选择准确性低于回归；主成分分析和特征重加权可缓解RSA因多重共线性导致的不足，但回归仍更优。

Conclusion: 当有直接的刺激 - 反应映射时，RSA在模型选择和拟合方面不如线性回归，研究者应谨慎选择方法。

Abstract: Representational Similarity Analysis (RSA) is a popular method for analyzing
neuroimaging and behavioral data. Here we evaluate the accuracy and reliability
of RSA in the context of model selection, and compare it to that of regression.
Although RSA offers flexibility in handling high-dimensional, cross-modal, and
cross-species data, its reliance on a transformation of raw data into
similarity structures may result in the loss of critical stimulus-response
information. Across extensive simulation studies and empirical analyses, we
show that RSA leads to lower model selection accuracy, regardless of sample
size, noise level, feature dimensionality, or multicollinearity, relative to
regression. While principal component analysis and feature reweighting mitigate
RSA's deficits driven by multicollinearity, regression remains superior in
accurately distinguishing between models. Empirical data and a follow-up fMRI
simulation further support these conclusions. Our findings suggest that
researchers should carefully consider which approach to use: RSA is less
effective than linear regression for model selection and fitting when direct
stimulus-response mappings are available.

</details>


### [386] [A Distributed Plug-and-Play MCMC Algorithm for High-Dimensional Inverse Problems](https://arxiv.org/abs/2511.00870)
*Maxime Bouton,Pierre-Antoine Thouvenin,Audrey Repetti,Pierre Chainais*

Main category: stat.ME

TL;DR: 本文提出基于近似数据增强和PnP - ULA的分布式采样器解决高维成像逆问题，评估其性能并讨论开销，该方法可扩展、能量化不确定性且重建性能良好。


<details>
  <summary>Details</summary>
Motivation: 现有MCMC算法在解决高维成像逆问题时，绘制和存储高维样本成本高，设计可扩展采样器是挑战，需解决此问题。

Method: 提出基于近似数据增强和PnP - ULA的分布式采样器，使用轻量级去噪卷积神经网络，在单程序多数据架构上利用多个GPU。

Result: 在多个成像问题上评估了重建性能和可扩展性，仔细讨论了去噪器带来的通信和计算开销。

Conclusion: 所提出的分布式方法具有可扩展性、能进行不确定性量化，重建性能与其他PnP方法相当。

Abstract: Markov Chain Monte Carlo (MCMC) algorithms are standard approaches to solve
imaging inverse problems and quantify estimation uncertainties, a key
requirement in absence of ground-truth data. To improve estimation quality,
Plug-and-Play MCMC algorithms, such as PnP-ULA, have been recently developed to
accommodate priors encoded by a denoising neural network. Designing scalable
samplers for high-dimensional imaging inverse problems remains a challenge:
drawing and storing high-dimensional samples can be prohibitive, especially for
high-resolution images. To address this issue, this work proposes a distributed
sampler based on approximate data augmentation and PnP-ULA to solve very large
problems. The proposed sampler uses lightweight denoising convolutional neural
network, to efficiently exploit multiple GPUs on a Single Program Multiple Data
architecture. Reconstruction performance and scalability are evaluated on
several imaging problems. Communication and computation overheads due to the
denoiser are carefully discussed. The proposed distributed approach noticeably
combines three very precious qualities: it is scalable, enables uncertainty
quantification, for a reconstruction performance comparable to other PnP
methods.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [387] [Simulation-Driven Evaluation of Chiplet-Based Architectures Using VisualSim](https://arxiv.org/abs/2511.01244)
*Wajid Ali,Ayaz Akram,Deepak Shankar*

Main category: cs.AR

TL;DR: 本文使用VisualSim对多芯片片上系统（SoC）架构进行仿真，重点是基于小芯片的系统建模与性能分析，为未来设计优化提供基础。


<details>
  <summary>Details</summary>
Motivation: 传统单片芯片在制造成本、电源效率和性能扩展方面面临挑战，而小芯片技术具有成本低、灵活性和可扩展性高的优势，因此研究小芯片系统。

Method: 开发基于小芯片系统的详细仿真模型，在VisualSim中通过ARM CMN600片上网络连接多核ARM处理器集群，评估关键系统指标。

Result: 通过仿真获得影响小芯片系统性能的关键因素。

Conclusion: 研究为优化未来基于小芯片的半导体设计提供了基础。

Abstract: This paper focuses on the simulation of multi-die System-on-Chip (SoC)
architectures using VisualSim, emphasiz- ing chiplet-based system modeling and
performance analysis. Chiplet technology presents a promising alternative to
traditional monolithic chips, which face increasing challenges in manufactur-
ing costs, power efficiency, and performance scaling. By integrat- ing multiple
small modular silicon units into a single package, chiplet-based architectures
offer greater flexibility and scalability at a lower overall cost. In this
study, we developed a detailed sim- ulation model of a chiplet-based system,
incorporating multicore ARM processor clusters interconnected through a ARM
CMN600 network-on-chip (NoC) for efficient communication [4], [7]. The
simulation framework in VisualSim enables the evaluation of critical system
metrics, including inter-chiplet communication latency, memory access
efficiency, workload distribution, and the power-performance tradeoff under
various workloads. Through simulation-driven insights, this research highlights
key factors influencing chiplet system performance and provides a foundation
for optimizing future chiplet-based semiconductor designs.

</details>


### [388] [A Two Level Neural Approach Combining Off-Chip Prediction with Adaptive Prefetch Filtering](https://arxiv.org/abs/2403.15181)
*Alexandre Valentin Jamet,Georgios Vavouliotis,Daniel A. Jiménez,Lluc Alvarez,Marc Casas*

Main category: cs.AR

TL;DR: 提出TLP预测器结合片外访问预测与一级数据缓存预取过滤，仅需7KB存储，实验显示能减少DRAM事务并提升性能。


<details>
  <summary>Details</summary>
Motivation: 缓解大数据应用的性能和能耗开销。

Method: 提出由FLP和SLP组成的TLP预测器，FLP用虚拟地址和选择性延迟组件进行片外预测，SLP用物理地址和FLP预测结果驱动L1D预取过滤。

Result: TLP在单核和多核工作负载上分别减少30.7%和17.7%的DRAM事务，分别实现6.2%和11.8%的性能加速。

Conclusion: TLP有效且独立于L1D预取逻辑。

Abstract: To alleviate the performance and energy overheads of contemporary
applications with large data footprints, we propose the Two Level Perceptron
(TLP) predictor, a neural mechanism that effectively combines predicting
whether an access will be off-chip with adaptive prefetch filtering at the
first-level data cache (L1D). TLP is composed of two connected
microarchitectural perceptron predictors, named First Level Predictor (FLP) and
Second Level Predictor (SLP). FLP performs accurate off-chip prediction by
using several program features based on virtual addresses and a novel selective
delay component. The novelty of SLP relies on leveraging off-chip prediction to
drive L1D prefetch filtering by using physical addresses and the FLP prediction
as features. TLP constitutes the first hardware proposal targeting both
off-chip prediction and prefetch filtering using a multi-level perceptron
hardware approach. TLP only requires 7KB of storage. To demonstrate the
benefits of TLP we compare its performance with state-of-the-art approaches
using off-chip prediction and prefetch filtering on a wide range of single-core
and multi-core workloads. Our experiments show that TLP reduces the average
DRAM transactions by 30.7% and 17.7%, as compared to a baseline using
state-of-the-art cache prefetchers but no off-chip prediction mechanism, across
the single-core and multi-core workloads, respectively, while recent work
significantly increases DRAM transactions. As a result, TLP achieves geometric
mean performance speedups of 6.2% and 11.8% across single-core and multi-core
workloads, respectively. In addition, our evaluation demonstrates that TLP is
effective independently of the L1D prefetching logic.

</details>


### [389] [Scalable Processing-Near-Memory for 1M-Token LLM Inference: CXL-Enabled KV-Cache Management Beyond GPU Limits](https://arxiv.org/abs/2511.00321)
*Dowon Kim,MinJae Lee,Janghyeon Kim,HyuckSung Kwon,Hyeonggyu Jeong,Sang-Soo Park,Minyong Yoon,Si-Dong Roh,Yongsuk Kwon,Jinin So,Jungwook Choi*

Main category: cs.AR

TL;DR: 本文提出用于100万令牌大语言模型推理的可扩展近内存处理（PNM）系统，解决大语言模型上下文窗口扩展带来的内存和计算瓶颈，实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型上下文窗口扩展到数百万令牌时，在管理不断增长的键值（KV）缓存方面存在严重的内存和计算瓶颈，现有非逐出框架数据传输成本高。

Method: 提出可扩展的PNM系统，将令牌页面选择卸载到CXL内存中的PNM加速器，引入混合并行策略和稳定令牌选择机制。

Result: 该解决方案在参数高达405B、上下文为100万令牌的大语言模型上实现了一致的性能提升，PNM - KV和PnG - KV方案在吞吐量、每令牌能耗和总成本效率上表现优异。

Conclusion: 支持CXL的多PNM架构可作为未来长上下文大语言模型推理的可扩展骨干。

Abstract: The expansion of context windows in large language models (LLMs) to
multi-million tokens introduces severe memory and compute bottlenecks,
particularly in managing the growing Key-Value (KV) cache. While Compute
Express Link (CXL) enables non-eviction frameworks that offload the full
KV-cache to scalable external memory, these frameworks still suffer from costly
data transfers when recalling non-resident KV tokens to limited GPU memory as
context lengths increase. This work proposes scalable Processing-Near-Memory
(PNM) for 1M-Token LLM Inference, a CXL-enabled KV-cache management system that
coordinates memory and computation beyond GPU limits. Our design offloads token
page selection to a PNM accelerator within CXL memory, eliminating costly
recalls and enabling larger GPU batch sizes. We further introduce a hybrid
parallelization strategy and a steady-token selection mechanism to enhance
compute efficiency and scalability. Implemented atop a state-of-the-art CXL-PNM
system, our solution delivers consistent performance gains for LLMs with up to
405B parameters and 1M-token contexts. Our PNM-only offloading scheme (PNM-KV)
and GPU-PNM hybrid with steady-token execution (PnG-KV) achieve up to 21.9x
throughput improvement, up to 60x lower energy per token, and up to 7.3x better
total cost efficiency than the baseline, demonstrating that CXL-enabled
multi-PNM architectures can serve as a scalable backbone for future
long-context LLM inference.

</details>


### [390] [PDA-LSTM: Knowledge-driven page data arrangement based on LSTM for LCM supression in QLC 3D NAND flash memories](https://arxiv.org/abs/2511.00075)
*Qianhui Li,Weiya Wang,Qianqi Zhao,Tong Qu,Jing He,Xuhong Qiang,Jingwen Hou,Ke Chen,Bao Zhang,Qi Wang*

Main category: cs.AR

TL;DR: 提出PDA - LSTM模型安排页内数据抑制QLC 3D NAND闪存的LCM，降低BER，无需额外标志位，实验显示降低BER效果好。


<details>
  <summary>Details</summary>
Motivation: QLC 3D NAND闪存因存储密度增加读裕度变窄，受LCM影响，现有算法多为页内数据映射，考虑页间数据安排抑制LCM。

Method: 提出物理知识驱动的神经网络模型PDA - LSTM，用LSTM计算数据排列概率矩阵，设计转换矩阵辅助训练。

Result: PDA - LSTM相比无数据排列策略平均BER降低80.4%，比WBVM、DVDS分别降低18.4%、15.2%。

Conclusion: PDA - LSTM能有效抑制LCM，降低数据保留期间的BER，且无需额外标志位记录数据传输。

Abstract: Quarter level cell (QLC) 3D NAND flash memory is emerging as the predominant
storage solution in the era of artificial intelligence. QLC 3D NAND flash
stores 4 bit per cell to expand the storage density, resulting in narrower read
margins. Constrained to read margins, QLC always suffers from lateral charge
migration (LCM), which caused by non-uniform charge density across adjacent
memory cells. To suppress charge density gap between cells, there are some
algorithm in form of intra-page data mapping such as WBVM, DVDS. However, we
observe inter-page data arrangements also approach the suppression. Thus, we
proposed an intelligent model PDA-LSTM to arrange intra-page data for LCM
suppression, which is a physics-knowledge-driven neural network model. PDA-LSTM
applies a long-short term memory (LSTM) neural network to compute a data
arrangement probability matrix from input page data pattern. The arrangement is
to minimize the global impacts derived from the LCM among wordlines. Since each
page data can be arranged only once, we design a transformation from output
matrix of LSTM network to non-repetitive sequence generation probability matrix
to assist training process. The arranged data pattern can decrease the bit
error rate (BER) during data retention. In addition, PDA-LSTM do not need extra
flag bits to record data transport of 3D NAND flash compared with WBVM, DVDS.
The experiment results show that the PDA-LSTM reduces the average BER by 80.4%
compared with strategy without data arrangement, and by 18.4%, 15.2% compared
respectively with WBVM and DVDS with code-length 64.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [391] [Quantum Machine Unlearning: Foundations, Mechanisms, and Taxonomy](https://arxiv.org/abs/2511.00406)
*Thanveer Shaik,Xiaohui Tao,Haoran Xie,Robert Sang*

Main category: quant-ph

TL;DR: 本文为量子机器学习遗忘（QMU）建立统一形式框架，提出五轴分类法及实用机制，框架可扩展到联合和隐私感知场景，还给出研究路线图，使QMU成为严谨学科。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习遗忘是量子信息理论、隐私保护计算和可信人工智能交叉领域的基础挑战，需要推进其发展。

Method: 建立统一物理约束、算法机制和伦理治理的可验证形式框架，定义遗忘概念，提出五轴分类法，结合实用机制，利用量子差分隐私等扩展框架。

Result: 建立了适用于NISQ设备的框架，可扩展到分布式量子系统，实现可扩展、可审计的删除。

Conclusion: 这些贡献使QMU从概念变为严谨且符合伦理的学科，桥接物理可行性、算法可验证性和社会责任。

Abstract: Quantum Machine Unlearning has emerged as a foundational challenge at the
intersection of quantum information theory privacypreserving computation and
trustworthy artificial intelligence This paper advances QMU by establishing a
formal framework that unifies physical constraints algorithmic mechanisms and
ethical governance within a verifiable paradigm We define forgetting as a
contraction of distinguishability between pre and postunlearning models under
completely positive trace-preserving dynamics grounding data removal in the
physics of quantum irreversibility Building on this foundation we present a
fiveaxis taxonomy spanning scope guarantees mechanisms system context and
hardware realization linking theoretical constructs to implementable strategies
Within this structure we incorporate influence and quantum Fisher information
weighted updates parameter reinitialization and kernel alignment as practical
mechanisms compatible with noisy intermediatescale quantum NISQ devices The
framework extends naturally to federated and privacyaware settings via quantum
differential privacy homomorphic encryption and verifiable delegation enabling
scalable auditable deletion across distributed quantum systems Beyond technical
design we outline a forwardlooking research roadmap emphasizing formal proofs
of forgetting scalable and secure architectures postunlearning interpretability
and ethically auditable governance Together these contributions elevate QMU
from a conceptual notion to a rigorously defined and ethically aligned
discipline bridging physical feasibility algorithmic verifiability and societal
accountability in the emerging era of quantum intelligence.

</details>


### [392] [Quantum Deep Learning Still Needs a Quantum Leap](https://arxiv.org/abs/2511.01253)
*Hans Gundlach,Hrvoje Kukina,Jayson Lynch,Neil Thompson*

Main category: quant-ph

TL;DR: 量子计算技术发展迅速，但未来一二十年内量子计算机难以对深度学习产生重大影响，通过调查分析指出三个有潜力但面临障碍的领域。


<details>
  <summary>Details</summary>
Motivation: 评估量子计算机在未来一二十年内对深度学习的影响。

Method: 对量子算法及其与深度学习潜在应用的匹配情况进行调查，使用Choi等人[2023]的工作及新研究进行定量预测。

Result: 发现量子计算在三个领域可能加速深度学习，但均面临障碍：矩阵乘法等算法操作理论改进小且实际操作慢；部分算法依赖未成熟的QRAM；部分算法理论优势大但适用特殊情况。

Conclusion: 明确了当前量子深度学习的范围，并指出了可能带来更多实际进展的研究方向。

Abstract: Quantum computing technology is advancing rapidly. Yet, even accounting for
these trends, a quantum leap would be needed for quantum computers to mean-
ingfully impact deep learning over the coming decade or two. We arrive at this
conclusion based on a first-of-its-kind survey of quantum algorithms and how
they match potential deep learning applications. This survey reveals three
important areas where quantum computing could potentially accelerate deep
learning, each of which faces a challenging roadblock to realizing its
potential. First, quantum algorithms for matrix multiplication and other
algorithms central to deep learning offer small theoretical improvements in the
number of operations needed, but this advantage is overwhelmed on practical
problem sizes by how slowly quantum computers do each operation. Second, some
promising quantum algorithms depend on practical Quantum Random Access Memory
(QRAM), which is underdeveloped. Finally, there are quantum algorithms that
offer large theoretical advantages, but which are only applicable to special
cases, limiting their practical benefits. In each of these areas, we support
our arguments using quantitative forecasts of quantum advantage that build on
the work by Choi et al. [2023] as well as new research on limitations and
quantum hardware trends. Our analysis outlines the current scope of quantum
deep learning and points to research directions that could lead to greater
practical advances in the field.

</details>


### [393] [Quantum Blackwell's Ordering and Differential Privacy](https://arxiv.org/abs/2511.01467)
*Ayanava Dasgupta,Naqueeb Ahmad Warsi,Masahito Hayashi*

Main category: quant-ph

TL;DR: 本文基于量子假设检验和Blackwell序开发量子差分隐私框架，应用于量子学习算法稳定性分析、量子参数估计并建立差分隐私量子信道的收缩界。


<details>
  <summary>Details</summary>
Motivation: 构建量子差分隐私框架，将经典结果推广到量子领域并研究相关问题。

Method: 基于量子假设检验和Blackwell序，通过假设检验散度刻画(ε,δ)-QDP。

Result: 分析了量子学习算法稳定性，推导了量子参数估计的量子Fisher信息紧界，建立了差分隐私量子信道的近最优收缩界。

Conclusion: 所开发的框架可有效应用于量子学习算法分析、量子参数估计及差分隐私量子信道研究。

Abstract: We develop a framework for quantum differential privacy (QDP) based on
quantum hypothesis testing and Blackwell's ordering. This approach
characterizes $(\eps,\delta)$-QDP via hypothesis testing divergences and
identifies the most informative quantum state pairs under privacy constraints.
We apply this to analyze the stability of quantum learning algorithms,
generalizing classical results to the case $\delta>0$. Additionally, we study
privatized quantum parameter estimation, deriving tight bounds on the quantum
Fisher information under QDP. Finally, we establish near-optimal contraction
bounds for differentially private quantum channels with respect to the
hockey-stick divergence.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [394] [Evolutionary Dynamics in Continuous-time Finite-state Mean Field Games - Part I: Equilibria](https://arxiv.org/abs/2511.01452)
*Leonardo Pedroso,Andrea Agazzi,W. P. M. H. Heemels,Mauro Salazar*

Main category: eess.SY

TL;DR: 研究连续时间内大量玩家的动态博弈，提出进化模型、均值场近似，引入新均衡概念MSNE并分析其性质。


<details>
  <summary>Details</summary>
Motivation: 以往进化博弈理论主要关注无个体玩家状态动态的静态博弈，本文要对含玩家状态动态的动态博弈进行全面进化分析。

Method: 提出进化模型和有限群体博弈的均值场近似，建立近似保证；提出新的均衡概念MSNE。

Result: 表明标准动态博弈解概念缺乏进化解释，分析了MSNE与均值场进化模型驻点的关系以及MSNE的进化稳定性。

Conclusion: 成功对含玩家状态动态的动态博弈进行进化分析，引入的MSNE概念有进化解释，可用于研究此类动态博弈。

Abstract: We study a dynamic game with a large population of players who choose actions
from a finite set in continuous time. Each player has a state in a finite state
space that evolves stochastically with their actions. A player's reward depends
not only on their own state and action but also on the distribution of states
and actions across the population, capturing effects such as congestion in
traffic networks. While prior work in evolutionary game theory has primarily
focused on static games without individual player state dynamics, we present
the first comprehensive evolutionary analysis of such dynamic games. We propose
an evolutionary model together with a mean field approximation of the
finite-population game and establish strong approximation guarantees. We show
that standard solution concepts for dynamic games lack an evolutionary
interpretation, and we propose a new concept - the Mixed Stationary Nash
Equilibrium (MSNE) - which admits one. We analyze the relationship between MSNE
and the rest points of the mean field evolutionary model and study the
evolutionary stability of MSNE.

</details>


### [395] [Deep Learning Prediction of Beam Coherence Time for Near-FieldTeraHertz Networks](https://arxiv.org/abs/2511.01491)
*Irched Chafaa,E. Veronica Belmega,Giacomo Bacci*

Main category: eess.SY

TL;DR: 本文引入新的波束相干时间概念并提出深度学习模型，减少波束更新开销，提高数据速率。


<details>
  <summary>Details</summary>
Motivation: 太赫兹通信中，天线数量增加使波束对准和跟踪开销大，近场区域变化需调整波束形成，因此要降低波束更新率。

Method: 引入新的波束相干时间，提出依赖含时间相关输入的简单前馈神经网络的深度学习模型预测波束相干时间并实时调整波束形成。

Result: 数值结果表明该方法能在降低开销的同时提高数据速率，在高移动性场景效果明显。

Conclusion: 所提方法有效，可降低开销并提高数据速率，尤其适用于高移动性场景。

Abstract: Large multiple antenna arrays coupled with accu- rate beamforming are
essential in terahertz (THz) communi- cations to ensure link reliability.
However, as the number of antennas increases, beam alignment (focusing) and
beam tracking in mobile networks incur prohibitive overhead. Additionally, the
near-field region expands both with the size of antenna arrays and the carrier
frequency, calling for adjustments in the beamforming to account for spherical
wavefront instead of the conventional planar wave assumption. In this letter,
we introduce a novel beam coherence time for mobile THz networks, to
drastically reduce the rate of beam updates. Then, we propose a deep learning
model, relying on a simple feedforward neural network with a time-dependent
input, to predict the beam coherence time and adjust the beamforming on the fly
with minimal overhead. Our numerical results demonstrate the effectiveness of
the proposed approach by enabling higher data rates while reducing the
overhead, especially at high (i.e., vehicular) mobility.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [396] [Investigating Label Bias and Representational Sources of Age-Related Disparities in Medical Segmentation](https://arxiv.org/abs/2511.00477)
*Aditya Parikh,Sneha Das,Aasa Feragen*

Main category: eess.IV

TL;DR: 本文研究医学影像分割任务中的算法偏差，审计MAMA - MIA数据集揭示偏差，通过实验确定偏差原因，提出诊断框架并指出实现公平需解决定性分布差异。


<details>
  <summary>Details</summary>
Motivation: 算法偏差会加剧医疗健康差异，分割任务中偏差成因不明，且在乳腺癌分割中模型对年轻患者有显著性能差异。

Method: 审计MAMA - MIA数据集建立年龄相关偏差基线，通过对照实验反驳偏差源于标签质量或病例难度失衡的假设，尝试按难度平衡训练数据。

Result: 发现关键的“有偏标尺”效应；确定年轻患者病例本质上更难学习；证实使用有偏机器生成标签训练会使系统偏差被学习和放大。

Conclusion: 提出诊断医学分割算法偏差的系统框架，指出实现公平需解决定性分布差异而非仅平衡病例数量。

Abstract: Algorithmic bias in medical imaging can perpetuate health disparities, yet
its causes remain poorly understood in segmentation tasks. While fairness has
been extensively studied in classification, segmentation remains underexplored
despite its clinical importance. In breast cancer segmentation, models exhibit
significant performance disparities against younger patients, commonly
attributed to physiological differences in breast density. We audit the
MAMA-MIA dataset, establishing a quantitative baseline of age-related bias in
its automated labels, and reveal a critical Biased Ruler effect where
systematically flawed labels for validation misrepresent a model's actual bias.
However, whether this bias originates from lower-quality annotations (label
bias) or from fundamentally more challenging image characteristics remains
unclear. Through controlled experiments, we systematically refute hypotheses
that the bias stems from label quality sensitivity or quantitative case
difficulty imbalance. Balancing training data by difficulty fails to mitigate
the disparity, revealing that younger patient cases are intrinsically harder to
learn. We provide direct evidence that systemic bias is learned and amplified
when training on biased, machine-generated labels, a critical finding for
automated annotation pipelines. This work introduces a systematic framework for
diagnosing algorithmic bias in medical segmentation and demonstrates that
achieving fairness requires addressing qualitative distributional differences
rather than merely balancing case counts.

</details>


### [397] [Deep Generative Models for Enhanced Vitreous OCT Imaging](https://arxiv.org/abs/2511.00881)
*Simone Sarrocco,Philippe C. Cattin,Peter M. Maloca,Paul Friedrich,Philippe Valmaggia*

Main category: eess.IV

TL;DR: 本文评估多种深度学习模型提升玻璃体OCT图像质量和减少采集时间的效果，发现不同模型表现有差异，cDDPM潜力大。


<details>
  <summary>Details</summary>
Motivation: 评估深度学习模型提升玻璃体光学相干断层扫描（OCT）图像质量和减少采集时间。

Method: 使用cDDPMs、BBDMs、U - Net等模型生成高质量光谱域（SD）玻璃体OCT图像，用图像质量指标和视觉图灵测试评估，在新数据的手动分割玻璃体区域测试最佳模型。

Result: U - Net的PSNR和SSIM最高，Pix2Pix和cDDPM的LPIPS表现好，视觉图灵测试中cDDPM表现佳，在新数据上cDDPM生成图像更接近参考。

Conclusion: 定量指标和临床评估有差异，需综合评估，cDDPM有潜力生成临床有意义图像并减少四分之三采集时间，有望临床应用。

Abstract: Purpose: To evaluate deep learning (DL) models for enhancing vitreous optical
coherence tomography (OCT) image quality and reducing acquisition time.
Methods: Conditional Denoising Diffusion Probabilistic Models (cDDPMs),
Brownian Bridge Diffusion Models (BBDMs), U-Net, Pix2Pix, and Vector-Quantised
Generative Adversarial Network (VQ-GAN) were used to generate high-quality
spectral-domain (SD) vitreous OCT images. Inputs were SD ART10 images, and
outputs were compared to pseudoART100 images obtained by averaging ten ART10
images per eye location. Model performance was assessed using image quality
metrics and Visual Turing Tests, where ophthalmologists ranked generated images
and evaluated anatomical fidelity. The best model's performance was further
tested within the manually segmented vitreous on newly acquired data. Results:
U-Net achieved the highest Peak Signal-to-Noise Ratio (PSNR: 30.230) and
Structural Similarity Index Measure (SSIM: 0.820), followed by cDDPM. For
Learned Perceptual Image Patch Similarity (LPIPS), Pix2Pix (0.697) and cDDPM
(0.753) performed best. In the first Visual Turing Test, cDDPM ranked highest
(3.07); in the second (best model only), cDDPM achieved a 32.9% fool rate and
85.7% anatomical preservation. On newly acquired data, cDDPM generated vitreous
regions more similar in PSNR to the ART100 reference than true ART1 or ART10
B-scans and achieved higher PSNR on whole images when conditioned on ART1 than
ART10. Conclusions: Results reveal discrepancies between quantitative metrics
and clinical evaluation, highlighting the need for combined assessment. cDDPM
showed strong potential for generating clinically meaningful vitreous OCT
images while reducing acquisition time fourfold. Translational Relevance:
cDDPMs show promise for clinical integration, supporting faster, higher-quality
vitreous imaging. Dataset and code will be made publicly available.

</details>


### [398] [Towards Reliable Pediatric Brain Tumor Segmentation: Task-Specific nnU-Net Enhancements](https://arxiv.org/abs/2511.00449)
*Xiaolong Li,Zhi-Qin John Xu,Yan Ren,Tianming Qiu,Xiaowen Wang*

Main category: eess.IV

TL;DR: 提出适用于BraTS 2025 Task - 6 (PED)的高级nnU - Net框架用于儿科脑肿瘤分割，模型在验证排行榜获第一名。


<details>
  <summary>Details</summary>
Motivation: 儿科脑肿瘤在多参数磁共振成像中准确分割对诊断、治疗规划和监测至关重要，但面临数据有限、解剖变异性高和机构间成像异质性等挑战。

Method: 提出高级nnU - Net框架，包括带挤压和激励注意力的加宽残差编码器、3D深度可分离卷积、特异性驱动的正则化项和小尺度高斯权重初始化，还有两步后处理。

Result: 模型在Task - 6验证排行榜获第一名，取得多个病变的Dice分数，如CC为0.759等。

Conclusion: 所提出的高级nnU - Net框架在儿科脑肿瘤分割任务中表现出色，能有效应对相关挑战。

Abstract: Accurate segmentation of pediatric brain tumors in multi-parametric magnetic
resonance imaging (mpMRI) is critical for diagnosis, treatment planning, and
monitoring, yet faces unique challenges due to limited data, high anatomical
variability, and heterogeneous imaging across institutions. In this work, we
present an advanced nnU-Net framework tailored for BraTS 2025 Task-6 (PED), the
largest public dataset of pre-treatment pediatric high-grade gliomas. Our
contributions include: (1) a widened residual encoder with
squeeze-and-excitation (SE) attention; (2) 3D depthwise separable convolutions;
(3) a specificity-driven regularization term; and (4) small-scale Gaussian
weight initialization. We further refine predictions with two postprocessing
steps. Our models achieved first place on the Task-6 validation leaderboard,
attaining lesion-wise Dice scores of 0.759 (CC), 0.967 (ED), 0.826 (ET), 0.910
(NET), 0.928 (TC) and 0.928 (WT).

</details>


<div id='hep-lat'></div>

# hep-lat [[Back]](#toc)

### [399] [Group-Equivariant Diffusion Models for Lattice Field Theory](https://arxiv.org/abs/2510.26081)
*Octavio Vega,Javad Komijani,Aida El-Khadra,Marina Marinkovic*

Main category: hep-lat

TL;DR: 研究基于分数的对称保持扩散模型来采样二维格点场论，开发等变分数网络并采用增强训练方案，对称感知模型表现更优。


<details>
  <summary>Details</summary>
Motivation: 格点量子场论在临界点附近因临界慢化导致马尔可夫链蒙特卡罗模拟效率低，需替代采样策略。

Method: 开发对多种群变换等变的分数网络，采用增强训练方案。

Result: 对称感知模型在样本质量、表达能力和有效样本量上优于通用分数网络。

Conclusion: 基于分数的对称保持扩散模型是采样二维格点场论的有效替代策略。

Abstract: Near the critical point, Markov Chain Monte Carlo (MCMC) simulations of
lattice quantum field theories (LQFT) become increasingly inefficient due to
critical slowing down. In this work, we investigate score-based
symmetry-preserving diffusion models as an alternative strategy to sample
two-dimensional $\phi^4$ and ${\rm U}(1)$ lattice field theories. We develop
score networks that are equivariant to a range of group transformations,
including global $\mathbb{Z}_2$ reflections, local ${\rm U}(1)$ rotations, and
periodic translations $\mathbb{T}$. The score networks are trained using an
augmented training scheme, which significantly improves sample quality in the
simulated field theories. We also demonstrate empirically that our
symmetry-aware models outperform generic score networks in sample quality,
expressivity, and effective sample size.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [400] [Disciplined Biconvex Programming](https://arxiv.org/abs/2511.01813)
*Hao Zhu,Joschka Boedecker*

Main category: math.OC

TL;DR: 介绍了有纪律的双凸规划（DBCP）框架，可自动将双凸优化问题转化为凸子问题并生成求解器，还实现为Python包。


<details>
  <summary>Details</summary>
Motivation: 解决现有求解双凸优化问题的启发式方法设计和实现求解器需大量人力且易出错的问题。

Method: 将有纪律的凸规划原则扩展到双凸问题，根据语法规则指定问题，自动拆分转化为凸子问题并生成定制的交替凸搜索（ACS）求解器。

Result: 实现了DBCP并集成到开源Python包dbcp，作为凸优化领域特定语言CVXPY的扩展。

Conclusion: DBCP让用户无需凸优化专业知识就能快速试验不同双凸问题公式。

Abstract: We introduce disciplined biconvex programming (DBCP), a modeling framework
for specifying and solving biconvex optimization problems. Biconvex
optimization problems arise in various applications, including machine
learning, signal processing, computational science, and control. Solving a
biconvex optimization problem in practice usually resolves to heuristic methods
based on alternate convex search (ACS), which iteratively optimizes over one
block of variables while keeping the other fixed, so that the resulting
subproblems are convex and can be efficiently solved. However, designing and
implementing an ACS solver for a specific biconvex optimization problem usually
requires significant effort from the user, which can be tedious and
error-prone. DBCP extends the principles of disciplined convex programming to
biconvex problems, allowing users to specify biconvex optimization problems in
a natural way based on a small number of syntax rules. The resulting problem
can then be automatically split and transformed into convex subproblems, for
which a customized ACS solver is then generated and applied. DBCP allows users
to quickly experiment with different biconvex problem formulations, without
expertise in convex optimization. We implement DBCP into the open source Python
package dbcp, as an extension to the famous domain specific language CVXPY for
convex optimization.

</details>


### [401] [SHAP values through General Fourier Representations: Theory and Applications](https://arxiv.org/abs/2511.00185)
*Roberto Morales*

Main category: math.OC

TL;DR: 本文为SHAP值的数学分析建立严格谱框架，研究两种情况并给出稳定性估计和收敛性证明，还通过数值实验验证理论。


<details>
  <summary>Details</summary>
Motivation: 为SHAP值的数学分析建立严谨的谱框架。

Method: 利用广义傅里叶展开，研究确定性和概率性两种情况，在确定性情况推导定量稳定性估计，在概率性情况考虑无限宽极限下的神经网络并证明收敛性。

Result: 在确定性情况表明归因映射关于预测器间距离是利普希茨连续的；在概率性情况证明SHAP值收敛到对应高斯过程先验诱导的值，并给出误差界。

Conclusion: 通过数值实验验证了理论发现。

Abstract: This article establishes a rigorous spectral framework for the mathematical
analysis of SHAP values. We show that any predictive model defined on a
discrete or multi-valued input space admits a generalized Fourier expansion
with respect to an orthonormalisation tensor-product basis constructed under a
product probability measure. Within this setting, each SHAP attribution can be
represented as a linear functional of the model's Fourier coefficients.
  Two complementary regimes are studied. In the deterministic regime, we derive
quantitative stability estimates for SHAP values under Fourier truncation,
showing that the attribution map is Lipschitz continuous with respect to the
distance between predictors. In the probabilistic regime, we consider neural
networks in their infinite-width limit and prove convergence of SHAP values
toward those induced by the corresponding Gaussian process prior, with explicit
error bounds in expectation and with high probability based on concentration
inequalities.
  We also provide a numerical experiment on a clinical unbalanced dataset to
validate the theoretical findings.

</details>


### [402] [Isotropic Curvature Model for Understanding Deep Learning Optimization: Is Gradient Orthogonalization Optimal?](https://arxiv.org/abs/2511.00674)
*Weijie Su*

Main category: math.OC

TL;DR: 本文提出各向同性曲率模型分析深度学习单次迭代优化，用其分析Muon优化器等，给出最优更新矩阵条件，指出梯度正交化方向正确但非严格最优，并讨论未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 引入模型分析深度学习单次迭代优化，理解权重矩阵更新与总损失函数变化的关系。

Method: 假设损失函数曲率各向同性推导各向同性曲率模型，用该模型分析Muon优化器和其他矩阵梯度方法。

Result: 在曲率一般增长条件下，最优更新矩阵使原梯度矩阵谱更均匀；曲率增长有相变时，正交化梯度对模型最优；梯度正交化方向正确但非严格最优。

Conclusion: 可利用各向同性曲率模型为深度学习和语言模型设计新的优化方法。

Abstract: In this paper, we introduce a model for analyzing deep learning optimization
over a single iteration by leveraging the matrix structure of the weights. We
derive the model by assuming isotropy of curvature, including the second-order
Hessian and higher-order terms, of the loss function across all perturbation
directions; hence, we call it the isotropic curvature model. This model is a
convex optimization program amenable to analysis, which allows us to understand
how an update on the weights in the form of a matrix relates to the change in
the total loss function. As an application, we use the isotropic curvature
model to analyze the recently introduced Muon optimizer and other
matrix-gradient methods for training language models. First, we show that under
a general growth condition on the curvature, the optimal update matrix is
obtained by making the spectrum of the original gradient matrix more
homogeneous -- that is, making its singular values closer in ratio -- which in
particular improves the conditioning of the update matrix. Next, we show that
the orthogonalized gradient becomes optimal for the isotropic curvature model
when the curvature exhibits a phase transition in growth. Taken together, these
results suggest that the gradient orthogonalization employed in Muon and other
related methods is directionally correct but may not be strictly optimal.
Finally, we discuss future research on how to leverage the isotropic curvature
model for designing new optimization methods for training deep learning and
language models.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [403] [TINC: Trusted Intelligent NetChain](https://arxiv.org/abs/2511.00823)
*Qi Xia,Hu Xia,Isaac Amankona Obiri,Adjei-Arthur Bonsu,Grace Mupoyi Ntuala,Ansu Badjie,Tienin Bole Wilfried,Jiaqin Liu,Lan Ma,Jianbin Gao,Feng Yao*

Main category: cs.NI

TL;DR: 提出针对联盟区块链的多平面分片架构TINC，实验表明其性能优于现有分片区块链框架，且保障安全。


<details>
  <summary>Details</summary>
Motivation: 现有联盟区块链架构存在可扩展性和效率问题，当前分片解决方案难以保证公平参与和负载均衡。

Method: 提出Trusted Intelligent NetChain (TINC)多平面分片架构，包含自适应节点分配和动态负载均衡机制，解耦控制和数据平面。

Result: TINC显著优于现有分片区块链框架，实现更高吞吐量、更低延迟、平衡的节点和交易分布及更低交易失败率。

Conclusion: TINC能有效解决现有联盟区块链问题，同时保障安全，DDIDs增强联盟网络信任和安全管理。

Abstract: Blockchain technology facilitates the development of decentralized systems
that ensure trust and transparency without the need for expensive centralized
intermediaries. However, existing blockchain architectures particularly
consortium blockchains face critical challenges related to scalability and
efficiency. State sharding has emerged as a promising approach to enhance
blockchain scalability and performance. However, current shard-based solutions
often struggle to guarantee fair participation and a balanced workload
distribution among consortium members. To address these limitations, we propose
Trusted Intelligent NetChain (TINC), a multi-plane sharding architecture
specifically designed for consortium blockchains. TINC incorporates intelligent
mechanisms for adaptive node assignment and dynamic workload balancing,
enabling the system to respond effectively to changing network conditions while
maintaining equitable shard utilization. By decoupling the control and data
planes, TINC allows control nodes to focus on consensus operations, while data
nodes handle large-scale storage, thus improving overall resource efficiency.
Extensive experimental evaluation and formal analysis demonstrate that TINC
significantly outperforms existing shard-based blockchain frameworks. It
achieves higher throughput, lower latency, balanced node and transaction
distributions, and reduced transaction failure rates. Furthermore, TINC
maintains essential blockchain security guarantees, exhibiting resilience
against Byzantine faults and dynamic network environments. The integration of
Dynamic Decentralized Identifiers (DDIDs) further strengthens trust and
security management within the consortium network.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [404] [HFNO: an interpretable data-driven decomposition strategy for turbulent flows](https://arxiv.org/abs/2511.01535)
*Marco Cayuela,Vincent Le Chenadec,Peter Schmid,Taraneh Sayadi*

Main category: physics.flu-dyn

TL;DR: 本文提出Hierarchical Fourier Neural Operators (HFNOs)用于湍流降阶建模，提升可解释性，经多动力系统测试，能分解湍流多尺度。


<details>
  <summary>Details</summary>
Motivation: Fourier Neural Operators (FNOs)内部工作不透明，缺乏物理可解释性，需改进。

Method: 提出HFNOs架构，并行处理波数区间，将输入升维、傅里叶变换、分区，各区间用全连接神经网络处理，输出处理后逆变换回物理空间，用CNN或ESN架构精修输出。

Result: 在一维Kuramoto - Sivashinsky方程、二维Kolmogorov流和湍流通道壁面剪应力预测等测试中，模型能分解不同尺度湍流。

Conclusion: 模型提升了湍流可解释性，为多尺度建模提供可能。

Abstract: Fourier Neural Operators (FNOs) have demonstrated exceptional accuracy in
mapping functional spaces by leveraging Fourier transforms to establish a
connection with underlying physical principles. However, their opaque inner
workings often constitute an obstacle to physical interpretability. This work
introduces Hierarchical Fourier Neural Operators (HFNOs), a novel FNO-based
architecture tailored for reduced-order modeling of turbulent fluid flows,
designed to enhance interpretability by explicitly separating fluid behavior
across scales. The proposed architecture processes wavenumber bins in parallel,
enabling the approximation of dispersion relations and non-linear interactions.
Inputs are lifted to a higher-dimensional space, Fourier-transformed, and
partitioned into wavenumber bins. Each bin is processed by a Fully Connected
Neural Network (FCNN), with outputs subsequently padded, summed, and
inverse-transformed back into physical space. A final transformation refines
the output in physical space as a correction model, by means of one of the
following architectures: Convolutional Neural Network (CNN) and Echo State
Network (ESN). We evaluate the proposed model on a series of increasingly
complex dynamical systems: first on the one-dimensional Kuramoto-Sivashinsky
equation, then on the two-dimensional Kolmogorov flow, and finally on the
prediction of wall shear stress in turbulent channel flow, given the near-wall
velocity field. In all test cases, the model demonstrates its ability to
decompose turbulent flows across various scales, opening up the possibility of
increased interpretability and multiscale modeling of such flows.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [405] [Low-Cost Carriers in Aviation: Significance and Developments](https://arxiv.org/abs/2511.00932)
*Bruno Felipe de Oliveira,Alessandro V. M. Oliveira*

Main category: physics.soc-ph

TL;DR: 本文讨论低成本航空公司对航空运输市场的影响，并呈现该领域最新研究结果。


<details>
  <summary>Details</summary>
Motivation: 探讨低成本航空公司对航空运输市场的影响，呈现领域最新研究发现。

Method: 选取并分析自2015年以来发表的相关论文。

Result: 将研究涉及的主要主题分为五类。

Conclusion: 未提及明确结论。

Abstract: This paper aims to discuss the impacts of low-cost airlines on the air
transport market and, in particular, to present the most recent findings from
the specialized literature in this field. To this end, several papers published
on the topic since 2015 were selected and analyzed. Based on this analysis, the
main subjects addressed in the studies were categorized into five groups: (i)
impacts of low-cost airlines on competing carriers; (ii) impacts on airports;
(iii) general effects on air transport demand; (iv) effects on passengers'
choice processes; and (v) broader effects on geographical regions.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [406] [OceanAI: A Conversational Platform for Accurate, Transparent, Near-Real-Time Oceanographic Insights](https://arxiv.org/abs/2511.01019)
*Bowen Chen,Jayesh Gajbhar,Gregory Dusek,Rob Redmon,Patrick Hogan,Paul Liu,DelWayne Bohnenstiehl,Dongkuan,Xu,Ruoying He*

Main category: cs.CL

TL;DR: 提出OceanAI对话平台，结合开源大语言模型与实时海洋数据，在对比中表现出色，具扩展性，推进海洋AI决策支持。


<details>
  <summary>Details</summary>
Motivation: 通用对话AI系统常产生未经验证的“幻觉”，破坏科学严谨性，需改进。

Method: 将开源大语言模型的自然语言流畅性与美国国家海洋和大气管理局（NOAA）的实时海洋数据集成，通过API调用处理查询。

Result: 在与三款广泛使用的AI聊天界面产品的盲测对比中，只有OceanAI能提供来自NOAA的数值和原始数据参考。

Conclusion: OceanAI通过基于可验证观测输出，推进了透明度、可重复性和可信度，为海洋领域的AI决策支持提供了可扩展框架。

Abstract: Artificial intelligence is transforming the sciences, yet general
conversational AI systems often generate unverified "hallucinations"
undermining scientific rigor. We present OceanAI, a conversational platform
that integrates the natural-language fluency of open-source large language
models (LLMs) with real-time, parameterized access to authoritative
oceanographic data streams hosted by the National Oceanic and Atmospheric
Administration (NOAA). Each query such as "What was Boston Harbor's highest
water level in 2024?" triggers real-time API calls that identify, parse, and
synthesize relevant datasets into reproducible natural-language responses and
data visualizations. In a blind comparison with three widely used AI
chat-interface products, only OceanAI produced NOAA-sourced values with
original data references; others either declined to answer or provided
unsupported results. Designed for extensibility, OceanAI connects to multiple
NOAA data products and variables, supporting applications in marine hazard
forecasting, ecosystem assessment, and water-quality monitoring. By grounding
outputs and verifiable observations, OceanAI advances transparency,
reproducibility, and trust, offering a scalable framework for AI-enabled
decision support within the oceans. A public demonstration is available at
https://oceanai.ai4ocean.xyz.

</details>


### [407] [IL-PCSR: Legal Corpus for Prior Case and Statute Retrieval](https://arxiv.org/abs/2511.00268)
*Shounak Paul,Dhananjay Ghumare,Pawan Goyal,Saptarshi Ghosh,Ashutosh Modi*

Main category: cs.CL

TL;DR: 本文提出IL - PCR语料库，为法规和先例检索任务提供通用测试平台，实验多种基线模型并开发基于LLM的重排序方法获最佳性能。


<details>
  <summary>Details</summary>
Motivation: 以往研究将法规检索和先例检索任务独立处理，而二者存在内在关联，本文旨在填补这一研究空白。

Method: 提出IL - PCR语料库，对多种基线模型（词汇模型、语义模型和基于GNN的集成模型）进行实验，开发基于LLM的重排序方法。

Result: 基于LLM的重排序方法在任务中表现最佳。

Conclusion: IL - PCR语料库可作为通用测试平台，基于LLM的重排序方法能有效利用两个任务间的依赖关系，提升检索性能。

Abstract: Identifying/retrieving relevant statutes and prior cases/precedents for a
given legal situation are common tasks exercised by law practitioners.
Researchers to date have addressed the two tasks independently, thus developing
completely different datasets and models for each task; however, both retrieval
tasks are inherently related, e.g., similar cases tend to cite similar statutes
(due to similar factual situation). In this paper, we address this gap. We
propose IL-PCR (Indian Legal corpus for Prior Case and Statute Retrieval),
which is a unique corpus that provides a common testbed for developing models
for both the tasks (Statute Retrieval and Precedent Retrieval) that can exploit
the dependence between the two. We experiment extensively with several baseline
models on the tasks, including lexical models, semantic models and ensemble
based on GNNs. Further, to exploit the dependence between the two tasks, we
develop an LLM-based re-ranking approach that gives the best performance.

</details>


### [408] [RAGSmith: A Framework for Finding the Optimal Composition of Retrieval-Augmented Generation Methods Across Datasets](https://arxiv.org/abs/2511.01386)
*Muhammed Yusuf Kartal,Suha Kagan Kose,Korhan Sevinç,Burak Aktas*

Main category: cs.CL

TL;DR: 介绍RAGSmith框架进行RAG端到端架构搜索，在多领域表现优于基线。


<details>
  <summary>Details</summary>
Motivation: RAG质量受多因素影响，孤立优化模块不可靠，需端到端架构搜索。

Method: 使用遗传搜索优化标量目标，对9种技术族和46,080种可行管道配置进行搜索。

Result: RAGSmith配置平均比基线高3.8%，搜索约0.2%空间，发现有效骨干，增益与问题类型相关。

Conclusion: 为构建有效RAG系统提供实用、领域感知指导，证明进化搜索用于全管道优化的实用性。

Abstract: Retrieval-Augmented Generation (RAG) quality depends on many interacting
choices across retrieval, ranking, augmentation, prompting, and generation, so
optimizing modules in isolation is brittle. We introduce RAGSmith, a modular
framework that treats RAG design as an end-to-end architecture search over nine
technique families and 46{,}080 feasible pipeline configurations. A genetic
search optimizes a scalar objective that jointly aggregates retrieval metrics
(recall@k, mAP, nDCG, MRR) and generation metrics (LLM-Judge and semantic
similarity). We evaluate on six Wikipedia-derived domains (Mathematics, Law,
Finance, Medicine, Defense Industry, Computer Science), each with 100 questions
spanning factual, interpretation, and long-answer types. RAGSmith finds
configurations that consistently outperform naive RAG baseline by +3.8\% on
average (range +1.2\% to +6.9\% across domains), with gains up to +12.5\% in
retrieval and +7.5\% in generation. The search typically explores $\approx
0.2\%$ of the space ($\sim 100$ candidates) and discovers a robust backbone --
vector retrieval plus post-generation reflection/revision -- augmented by
domain-dependent choices in expansion, reranking, augmentation, and prompt
reordering; passage compression is never selected. Improvement magnitude
correlates with question type, with larger gains on factual/long-answer mixes
than interpretation-heavy sets. These results provide practical, domain-aware
guidance for assembling effective RAG systems and demonstrate the utility of
evolutionary search for full-pipeline optimization.

</details>


### [409] [A Graph-based RAG for Energy Efficiency Question Answering](https://arxiv.org/abs/2511.01643)
*Riccardo Campi,Nicolò Oreste Pinciroli Vago,Mathyas Giudici,Pablo Barrachina Rodriguez-Guisado,Marco Brambilla,Piero Fraternali*

Main category: cs.CL

TL;DR: 研究基于图的检索增强生成架构中大型语言模型用于能源效率问答的效果，实现系统并验证，确认架构潜力。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在基于图的检索增强生成架构中用于能源效率问答的应用。

Method: 从能源领域文档自动提取知识图谱，导航和推理图谱提供多语言答案，用RAGAs框架属性、验证数据集和领域专家进行人工验证。

Result: 系统约75.2 ± 2.7%的情况回答正确，通用能源效率问题准确率达81.0 ± 4.1%，多语言能力有4.4%的准确率损失。

Conclusion: 确认了该架构的潜力，也识别出其优缺点。

Abstract: In this work, we investigate the use of Large Language Models (LLMs) within a
graph-based Retrieval Augmented Generation (RAG) architecture for Energy
Efficiency (EE) Question Answering. First, the system automatically extracts a
Knowledge Graph (KG) from guidance and regulatory documents in the energy
field. Then, the generated graph is navigated and reasoned upon to provide
users with accurate answers in multiple languages. We implement a human-based
validation using the RAGAs framework properties, a validation dataset
comprising 101 question-answer pairs, and domain experts. Results confirm the
potential of this architecture and identify its strengths and weaknesses.
Validation results show how the system correctly answers in about three out of
four of the cases (75.2 +- 2.7%), with higher results on questions related to
more general EE answers (up to 81.0 +- 4.1%), and featuring promising
multilingual abilities (4.4% accuracy loss due to translation).

</details>


### [410] [Reasoning Trajectories for Socratic Debugging of Student Code: From Misconceptions to Contradictions and Updated Beliefs](https://arxiv.org/abs/2511.00371)
*Erfan Al-Hossami,Razvan Bunescu*

Main category: cs.CL

TL;DR: 本文介绍推理轨迹生成任务及标注数据集，提出基于大语言模型的解决方案，评估显示模型能生成高比例正确轨迹和有效对话轮次。


<details>
  <summary>Details</summary>
Motivation: 解决苏格拉底式调试中引导学生识别和修正编程错误的问题，针对编程误解导致的新手编程错误。

Method: 引入推理轨迹生成任务，创建手动标注推理轨迹的调试问题数据集，使用大语言模型生成推理轨迹和基于此的苏格拉底式对话。

Result: 大规模大语言模型评估显示能生成高达91%的正确推理轨迹和98.7%的有效对话轮次。

Conclusion: 大语言模型在推理轨迹生成和苏格拉底式对话生成方面表现良好，可用于辅助苏格拉底式调试。

Abstract: In Socratic debugging, instructors guide students towards identifying and
fixing a bug on their own, instead of providing the bug fix directly. Most
novice programmer bugs are caused by programming misconceptions, namely false
beliefs about a programming concept. In this context, Socratic debugging can be
formulated as a guided Reasoning Trajectory (RT) leading to a statement about
the program behavior that contradicts the bug-causing misconception. Upon
reaching this statement, the ensuing cognitive dissonance leads the student to
first identify and then update their false belief. In this paper, we introduce
the task of reasoning trajectory generation, together with a dataset of
debugging problems manually annotated with RTs. We then describe LLM-based
solutions for generating RTs and Socratic conversations that are anchored on
them. A large-scale LLM-as-judge evaluation shows that frontier models can
generate up to 91% correct reasoning trajectories and 98.7% valid conversation
turns.

</details>


### [411] [MicroRemed: Benchmarking LLMs in Microservices Remediation](https://arxiv.org/abs/2511.01166)
*Lingzhe Zhang,Yunpeng Zhai,Tong Jia,Chiming Duan,Minghua He,Leyi Pan,Zhaoyang Liu,Bolin Ding,Ying Li*

Main category: cs.CL

TL;DR: 介绍首个评估大语言模型端到端微服务修复能力的基准MicroRemed和多智能体框架ThinkRemed，实验显示MicroRemed有挑战，ThinkRemed可提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有微服务修复方法依赖人工编写提示，大语言模型仅将文本指令转换为可执行代码，为推动该领域研究。

Method: 引入MicroRemed基准，提出ThinkRemed多智能体框架。

Result: MicroRemed对当前大语言模型构成重大挑战，ThinkRemed通过迭代推理和系统反思提高端到端修复性能。

Conclusion: MicroRemed和ThinkRemed在微服务修复研究中有重要意义，基准代码已开源。

Abstract: Large Language Models (LLMs) integrated with agent-based reasoning frameworks
have recently shown strong potential for autonomous decision-making and
system-level operations. One promising yet underexplored direction is
microservice remediation, where the goal is to automatically recover faulty
microservice systems. Existing approaches, however, still rely on human-crafted
prompts from Site Reliability Engineers (SREs), with LLMs merely converting
textual instructions into executable code. To advance research in this area, we
introduce MicroRemed, the first benchmark for evaluating LLMs in end-to-end
microservice remediation, where models must directly generate executable
Ansible playbooks from diagnosis reports to restore system functionality. We
further propose ThinkRemed, a multi-agent framework that emulates the
reflective and perceptive reasoning of SREs. Experimental results show that
MicroRemed presents substantial challenges to current LLMs, while ThinkRemed
improves end-to-end remediation performance through iterative reasoning and
system reflection. The benchmark is available at
https://github.com/LLM4AIOps/MicroRemed.

</details>


### [412] [Cognitive Alignment in Personality Reasoning: Leveraging Prototype Theory for MBTI Inference](https://arxiv.org/abs/2511.00115)
*Haoyuan Li,Yuanbo Tong,Yuchen Li,Zirui Wang,Chunhou Liu,Jiamou Liu*

Main category: cs.CL

TL;DR: 提出ProtoMBTI框架用于MBTI推断，在基准测试中表现优于基线，证明与心理原型推理对齐有益。


<details>
  <summary>Details</summary>
Motivation: 传统文本人格识别采用硬标签分类，掩盖了人类人格判断的分级和原型特征，需改进。

Method: 构建平衡、质量可控的语料库，LoRA微调轻量级编码器学习嵌入和标准化原型库，推理时进行检索 - 复用 - 修订 - 保留循环。

Result: 在Kaggle和Pandora基准测试中，ProtoMBTI在四个MBTI二分法和16类型任务上优于基线，有强大的跨数据集泛化能力。

Conclusion: 将推理过程与心理原型推理对齐，能提升基于文本的人格建模的准确性、可解释性和迁移性。

Abstract: Personality recognition from text is typically cast as hard-label
classification, which obscures the graded, prototype-like nature of human
personality judgments. We present ProtoMBTI, a cognitively aligned framework
for MBTI inference that operationalizes prototype theory within an LLM-based
pipeline. First, we construct a balanced, quality-controlled corpus via
LLM-guided multi-dimensional augmentation (semantic, linguistic, sentiment).
Next, we LoRA-fine-tune a lightweight (<=2B) encoder to learn discriminative
embeddings and to standardize a bank of personality prototypes. At inference,
we retrieve top-k prototypes for a query post and perform a
retrieve--reuse--revise--retain cycle: the model aggregates prototype evidence
via prompt-based voting, revises when inconsistencies arise, and, upon correct
prediction, retains the sample to continually enrich the prototype library.
Across Kaggle and Pandora benchmarks, ProtoMBTI improves over baselines on both
the four MBTI dichotomies and the full 16-type task, and exhibits robust
cross-dataset generalization. Our results indicate that aligning the inference
process with psychological prototype reasoning yields gains in accuracy,
interpretability, and transfer for text-based personality modeling.

</details>


### [413] [Training LLMs Beyond Next Token Prediction - Filling the Mutual Information Gap](https://arxiv.org/abs/2511.00198)
*Chun-Hao Yang,Bo-Han Feng,Tzu-Yuan Lai,Yan Yu Chen,Yin-Kai Dean Huang,Shou-De Lin*

Main category: cs.CL

TL;DR: 本文挑战传统NTP训练大语言模型方法，提出预测信息丰富的标记进行训练，在三类任务中验证，优化训练并提升理论理解。


<details>
  <summary>Details</summary>
Motivation: 优化大语言模型训练性能，在提升模型性能同时控制计算成本。

Method: 挑战传统NTP训练方法，提出预测信息丰富的标记来训练大语言模型，并在三类任务中研究其影响。

Result: 未提及具体结果。

Conclusion: 提供了优化大语言模型训练的原则性方法，提升了模型性能和对目标标记选择策略的理论理解。

Abstract: Optimizing training performance in large language models (LLMs) remains an
essential challenge, particularly in improving model performance while
maintaining computational costs. This work challenges the conventional approach
of training LLMs using next-token prediction (NTP), arguing that by predicting
information-rich tokens during training, there is a more effective way to train
LLMs. We investigate the impact of the proposed solution in three kinds of
tasks for LLMs: arithmetic, multi-label classification of text, and
natural-language generation. This work offers a principled approach to
optimizing LLM training, advancing both model performance and theoretical
understanding of the target-token selection strategies.

</details>


### [414] [Consistently Simulating Human Personas with Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2511.00222)
*Marwa Abdulhai,Ryan Cheng,Donovan Clay,Tim Althoff,Sergey Levine,Natasha Jaques*

Main category: cs.CL

TL;DR: 提出评估和改进大语言模型生成对话中角色一致性的统一框架，用自动指标和强化学习微调模型，使不一致性降低超55%。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在模拟人类用户时存在角色漂移等问题，需要评估和改进角色一致性。

Method: 定义三种自动指标（提示到行一致性、行到行一致性、问答一致性）并与人工标注验证，以这些指标为奖励信号，应用多轮强化学习微调大语言模型。

Result: 方法使不一致性降低超过55%，生成的模拟用户更连贯、更符合角色设定。

Conclusion: 所提出的统一框架能有效评估和改进大语言模型生成对话的角色一致性。

Abstract: Large Language Models (LLMs) are increasingly used to simulate human users in
interactive settings such as therapy, education, and social role-play. While
these simulations enable scalable training and evaluation of AI agents,
off-the-shelf LLMs often drift from their assigned personas, contradict earlier
statements, or abandon role-appropriate behavior. We introduce a unified
framework for evaluating and improving persona consistency in LLM-generated
dialogue. We define three automatic metrics: prompt-to-line consistency,
line-to-line consistency, and Q&A consistency, that capture different types of
persona drift and validate each against human annotations. Using these metrics
as reward signals, we apply multi-turn reinforcement learning to fine-tune LLMs
for three user roles: a patient, a student, and a social chat partner. Our
method reduces inconsistency by over 55%, resulting in more coherent and
faithful simulated users.

</details>


### [415] [POSESTITCH-SLT: Linguistically Inspired Pose-Stitching for End-to-End Sign Language Translation](https://arxiv.org/abs/2511.00270)
*Abhinav Joshi,Vaibhav Sharma,Sanjeet Singh,Ashutosh Modi*

Main category: cs.CL

TL;DR: 提出POSESTITCH - SLT预训练方案，在两个手语数据集上实验，证明模板驱动合成监督在低资源手语场景有效。


<details>
  <summary>Details</summary>
Motivation: 手语翻译因大规模句子对齐数据集稀缺仍是挑战性任务，以往研究聚焦特征提取和架构改变。

Method: 提出POSESTITCH - SLT预训练方案，基于语言学模板的句子生成技术，采用简单的基于Transformer的编解码器架构，使用模板生成的句子对进行训练。

Result: 在How2Sign数据集上BLEU - 4分数从1.97提升到4.56，在iSign数据集上从0.55提升到3.43，超越基于姿势的无注解翻译的现有最佳方法。

Conclusion: 模板驱动的合成监督在低资源手语场景中有效。

Abstract: Sign language translation remains a challenging task due to the scarcity of
large-scale, sentence-aligned datasets. Prior arts have focused on various
feature extraction and architectural changes to support neural machine
translation for sign languages. We propose POSESTITCH-SLT, a novel pre-training
scheme that is inspired by linguistic-templates-based sentence generation
technique. With translation comparison on two sign language datasets, How2Sign
and iSign, we show that a simple transformer-based encoder-decoder architecture
outperforms the prior art when considering template-generated sentence pairs in
training. We achieve BLEU-4 score improvements from 1.97 to 4.56 on How2Sign
and from 0.55 to 3.43 on iSign, surpassing prior state-of-the-art methods for
pose-based gloss-free translation. The results demonstrate the effectiveness of
template-driven synthetic supervision in low-resource sign language settings.

</details>


### [416] [Language Modeling With Factorization Memory](https://arxiv.org/abs/2511.00315)
*Lee Xiong,Maksim Tkachenko,Johanes Effendi,Ting Cai*

Main category: cs.CL

TL;DR: 提出Factorization Memory，在短上下文语言建模任务上性能与Transformer相当，长上下文场景泛化能力更优，还开发了稀疏版本。


<details>
  <summary>Details</summary>
Motivation: 构建一种高效的循环神经网络架构，在短上下文任务有与Transformer相当的性能，在长上下文场景有更好泛化能力，同时优化模型效率和表示能力。

Method: 基于Mamba - 2构建Factorization Memory，开发其稀疏公式，更新部分循环状态；对Factorization Memory与Transformer和Mamba - 2架构进行系统实证分析。

Result: Factorization Memory在短上下文任务性能与Transformer相当，长上下文场景泛化能力更好；稀疏版本保留了密集版本的强性能。

Conclusion: Factorization Memory是首个成功将稀疏内存激活与短长上下文设置下的竞争性能相结合的RNN架构。

Abstract: We propose Factorization Memory, an efficient recurrent neural network (RNN)
architecture that achieves performance comparable to Transformer models on
short-context language modeling tasks while also demonstrating superior
generalization in long-context scenarios. Our model builds upon Mamba-2,
enabling Factorization Memory to exploit parallel computations during training
while preserving constant computational and memory complexity during inference.
To further optimize model efficiency and representational capacity, we develop
a sparse formulation of Factorization Memory that updates only a subset of
recurrent states at each step while preserving the strong performance of its
dense counterpart. To our knowledge, this represents the first RNN architecture
that successfully combines sparse memory activation with competitive
performance across both short and long-context settings. This work provides a
systematic empirical analysis of Factorization Memory in comparison to
Transformer and Mamba-2 architectures.

</details>


### [417] [PADBen: A Comprehensive Benchmark for Evaluating AI Text Detectors Against Paraphrase Attacks](https://arxiv.org/abs/2511.00416)
*Yiwei Zha,Rui Min,Shanu Sushmita*

Main category: cs.CL

TL;DR: 研究AI生成文本检测器对迭代释义文本检测失效原因，提出PADBen基准评估其鲁棒性，发现现有检测器应对作者身份混淆攻击效果不佳，需架构改进。


<details>
  <summary>Details</summary>
Motivation: 探究迭代释义的AI生成文本能逃避检测系统的原因，解决现有检测系统的漏洞。

Method: 进行内在机制分析，提出PADBen基准，包含五类型文本分类和五项检测任务，评估11个先进检测器。

Result: 发现检测器能识别抄袭逃避问题，但应对作者身份混淆攻击失败。

Conclusion: 当前检测方法无法有效处理中间清洗区域，需在检测架构上取得根本进展。

Abstract: While AI-generated text (AIGT) detectors achieve over 90\% accuracy on direct
LLM outputs, they fail catastrophically against iteratively-paraphrased
content. We investigate why iteratively-paraphrased text -- itself AI-generated
-- evades detection systems designed for AIGT identification. Through intrinsic
mechanism analysis, we reveal that iterative paraphrasing creates an
intermediate laundering region characterized by semantic displacement with
preserved generation patterns, which brings up two attack categories:
paraphrasing human-authored text (authorship obfuscation) and paraphrasing
LLM-generated text (plagiarism evasion). To address these vulnerabilities, we
introduce PADBen, the first benchmark systematically evaluating detector
robustness against both paraphrase attack scenarios. PADBen comprises a
five-type text taxonomy capturing the full trajectory from original content to
deeply laundered text, and five progressive detection tasks across
sentence-pair and single-sentence challenges. We evaluate 11 state-of-the-art
detectors, revealing critical asymmetry: detectors successfully identify the
plagiarism evasion problem but fail for the case of authorship obfuscation. Our
findings demonstrate that current detection approaches cannot effectively
handle the intermediate laundering region, necessitating fundamental advances
in detection architectures beyond existing semantic and stylistic
discrimination methods. For detailed code implementation, please see
https://github.com/JonathanZha47/PadBen-Paraphrase-Attack-Benchmark.

</details>


### [418] [MedRECT: A Medical Reasoning Benchmark for Error Correction in Clinical Texts](https://arxiv.org/abs/2511.00421)
*Naoto Iwase,Hiroki Okuyama,Junichiro Iwasawa*

Main category: cs.CL

TL;DR: 介绍跨语言医学错误处理基准MedRECT，评估9个大语言模型，发现推理模型表现好，微调有改进，模型超人类专家。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在医学应用中检测和纠正临床文本错误能力评估不足，尤其非英语领域。

Method: 构建跨语言基准MedRECT，含三个子任务，用日本医学执照考试和英语对应数据构建数据集，评估9个当代大语言模型，进行LoRA微调。

Result: 推理模型表现优于标准架构；跨语言评估英语到日语有5 - 10%性能差距，推理模型差距小；微调在错误纠正有不对称改进；微调模型超人类专家。

Conclusion: MedRECT是首个全面跨语言医学错误纠正基准，为开发多语言安全医学大语言模型提供框架和资源。

Abstract: Large language models (LLMs) show increasing promise in medical applications,
but their ability to detect and correct errors in clinical texts -- a
prerequisite for safe deployment -- remains under-evaluated, particularly
beyond English. We introduce MedRECT, a cross-lingual benchmark
(Japanese/English) that formulates medical error handling as three subtasks:
error detection, error localization (sentence extraction), and error
correction. MedRECT is built with a scalable, automated pipeline from the
Japanese Medical Licensing Examinations (JMLE) and a curated English
counterpart, yielding MedRECT-ja (663 texts) and MedRECT-en (458 texts) with
comparable error/no-error balance. We evaluate 9 contemporary LLMs spanning
proprietary, open-weight, and reasoning families. Key findings: (i) reasoning
models substantially outperform standard architectures, with up to 13.5%
relative improvement in error detection and 51.0% in sentence extraction; (ii)
cross-lingual evaluation reveals 5-10% performance gaps from English to
Japanese, with smaller disparities for reasoning models; (iii) targeted LoRA
fine-tuning yields asymmetric improvements in error correction performance
(Japanese: +0.078, English: +0.168) while preserving reasoning capabilities;
and (iv) our fine-tuned model exceeds human expert performance on structured
medical error correction tasks. To our knowledge, MedRECT is the first
comprehensive cross-lingual benchmark for medical error correction, providing a
reproducible framework and resources for developing safer medical LLMs across
languages.

</details>


### [419] [FlashEVA: Accelerating LLM inference via Efficient Attention](https://arxiv.org/abs/2511.00576)
*Juan Gabriel Kostelec,Qinghai Guo*

Main category: cs.CL

TL;DR: 提出FlashEVA实现高效注意力机制，微调Transformer模型，提高推理吞吐量、降低内存使用，有局限性但可灵活调整。


<details>
  <summary>Details</summary>
Motivation: Transformer模型推理时内存需求大，解决其内存挑战问题。

Method: 提出FlashEVA并展示如何微调Transformer以适应其注意力机制，用少量令牌微调。

Result: 实现高达6.7倍的吞吐量提升和5倍的峰值GPU内存使用降低，检索任务有局限。

Conclusion: 该工作是迈向更高效、适应性更强的基于Transformer的推理模型的重要一步。

Abstract: Transformer models have revolutionized natural language processing, achieving
state-of-the-art performance and demonstrating remarkable scalability. However,
their memory demands, particularly due to maintaining full context in memory,
pose significant challenges for inference. In this paper, we present FlashEVA,
an efficient implementation of EVA (Efficient Attention via Control Variates),
and demonstrate how to finetune transformers to adapt to FlashEVA attention.
Our method enables fine-tuning of Transformer models with as few as 1.5B tokens
while preserving effectiveness across various downstream tasks. Notably,
FlashEVA achieves up to 6.7x higher throughput and 5x lower peak GPU memory
usage during inference compared to standard Transformer implementations.
Despite these improvements, we observe limitations in retrieval-focused tasks.
Our implementation offers control over the trade-off between throughput and
accuracy through adjustable hyperparameters, providing flexibility for diverse
use cases. This work represents a significant step towards more efficient and
adaptable Transformer-based models for inference.

</details>


### [420] [ParaScopes: What do Language Models Activations Encode About Future Text?](https://arxiv.org/abs/2511.00180)
*Nicky Pochinkov,Yulia Volkova,Anna Vasileva,Sai V R Chereddy*

Main category: cs.CL

TL;DR: 提出残差流解码器框架探查模型激活以理解语言模型长期规划信息，小模型能解码相当于5+个未来上下文标记的信息。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型可处理长时任务，但理解激活的方法局限于测试特定概念或标记，需更好方法理解长时规划信息。

Method: 开发残差流解码器框架探查模型激活以获取段落和文档级别的规划信息。

Result: 测试发现小模型中可解码出相当于5+个未来上下文标记的信息。

Conclusion: 研究结果为更好监测语言模型和理解其编码长期规划信息奠定基础。

Abstract: Interpretability studies in language models often investigate forward-looking
representations of activations. However, as language models become capable of
doing ever longer time horizon tasks, methods for understanding activations
often remain limited to testing specific concepts or tokens. We develop a
framework of Residual Stream Decoders as a method of probing model activations
for paragraph-scale and document-scale plans. We test several methods and find
information can be decoded equivalent to 5+ tokens of future context in small
models. These results lay the groundwork for better monitoring of language
models and better understanding how they might encode longer-term planning
information.

</details>


### [421] [Assessing LLM Reasoning Steps via Principal Knowledge Grounding](https://arxiv.org/abs/2511.00879)
*Hyeon Hwang,Yewon Cho,Chanwoong Yoon,Yein Park,Minju Song,Kyungjae Lee,Gangwoo Kim,Jaewoo Kang*

Main category: cs.CL

TL;DR: 提出新颖评估套件评估大语言模型推理的知识基础，还展示其在偏好优化中的应用。


<details>
  <summary>Details</summary>
Motivation: 解决如何验证大语言模型推理是否准确基于知识的问题。

Method: 构建包含主要知识收集、知识基础评估指标和评估器大语言模型三个关键组件的评估套件。

Result: 评估套件能有效识别缺失或错误应用的知识元素。

Conclusion: 知识基础评估不仅可用于评估，还能集成到偏好优化中。

Abstract: Step-by-step reasoning has become a standard approach for large language
models (LLMs) to tackle complex tasks. While this paradigm has proven
effective, it raises a fundamental question: How can we verify that an LLM's
reasoning is accurately grounded in knowledge? To address this question, we
introduce a novel evaluation suite that systematically assesses the knowledge
grounding of intermediate reasoning. Our framework comprises three key
components. (1) Principal Knowledge Collection, a large-scale repository of
atomic knowledge essential for reasoning. Based on the collection, we propose
(2) knowledge-grounded evaluation metrics designed to measure how well models
recall and apply prerequisite knowledge in reasoning. These metrics are
computed by our (3) evaluator LLM, a lightweight model optimized for
cost-effective and reliable metric computation. Our evaluation suite
demonstrates remarkable effectiveness in identifying missing or misapplied
knowledge elements, providing crucial insights for uncovering fundamental
reasoning deficiencies in LLMs. Beyond evaluation, we demonstrate how these
metrics can be integrated into preference optimization, showcasing further
applications of knowledge-grounded evaluation.

</details>


### [422] [The Riddle of Reflection: Evaluating Reasoning and Self-Awareness in Multilingual LLMs using Indian Riddles](https://arxiv.org/abs/2511.00960)
*Abhinav P M,Ojasva Saxena,Oswald C,Parameswari Krishnamurthy*

Main category: cs.CL

TL;DR: 本文研究大语言模型在七种印度语言上的推理和自我评估能力，引入多语言谜题数据集评估五种模型，发现模型初始准确率与自我纠错能力负相关，指出多语言推理存在差距。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在非英语语言上进行文化推理的能力，尤其是在七种印度语言上的表现。

Method: 引入结合传统谜题和上下文重构变体的多语言谜题数据集，用七种提示策略评估五种大语言模型，分两阶段评估解谜表现和推理一致性。

Result: Gemini 2.5 Pro 整体表现最佳，少样本方法提升有限且准确率因语言而异；模型初始准确率与自我纠错能力负相关，如 Gemini 2.5 Pro 过度自信，LLaMA 4 Scout 更有自知之明。

Conclusion: 多语言推理存在明显差距，需要能有效推理且认识自身局限的模型。

Abstract: The extent to which large language models (LLMs) can perform culturally
grounded reasoning across non-English languages remains underexplored. This
paper examines the reasoning and self-assessment abilities of LLMs across seven
major Indian languages-Bengali, Gujarati, Hindi, Kannada, Malayalam, Tamil, and
Telugu. We introduce a multilingual riddle dataset combining traditional
riddles with context-reconstructed variants and evaluate five LLMs-Gemini 2.5
Pro, Gemini 2.5 Flash, Mistral-Saba, LLaMA 4 Scout, and LLaMA 4 Maverick-under
seven prompting strategies. In the first stage, we assess riddle-solving
performance and find that while Gemini 2.5 Pro performs best overall, few-shot
methods yield only marginal gains, and accuracy varies notably across
languages. In the second stage, we conduct a self-evaluation experiment to
measure reasoning consistency. The results reveal a key finding: a model's
initial accuracy is inversely correlated with its ability to identify its own
mistakes. Top-performing models such as Gemini 2.5 Pro are overconfident (4.34%
True Negative Rate), whereas lower-performing models like LLaMA 4 Scout are
substantially more self-aware (42.09% True Negative Rate). These results point
to clear gaps in multilingual reasoning and highlight the need for models that
not only reason effectively but also recognize their own limitations.

</details>


### [423] [Multi-refined Feature Enhanced Sentiment Analysis Using Contextual Instruction](https://arxiv.org/abs/2511.00537)
*Peter Atandoh,Jie Zou,Weikang Guo,Jiwei Wei,Zheng Wang*

Main category: cs.CL

TL;DR: 提出CISEA - MRFE框架解决现有情感分析方法在复杂场景下的不足，实验表明该框架在多数据集上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习和预训练语言模型的情感分析方法在处理细微情感线索、领域转移和情感分布不平衡等场景时表现不佳，原因在于语义基础不足、泛化能力差和存在情感类别偏差。

Method: 提出CISEA - MRFE框架，集成上下文指令（CI）、语义增强扩充（SEA）和多精炼特征提取（MRFE），各部分有具体作用。

Result: 在四个基准数据集上，CISEA - MRFE始终优于强基线，在IMDb、Yelp、Twitter和Amazon上准确率分别相对提升4.6%、6.5%、30.3%和4.1%。

Conclusion: 该方法在不同领域的情感分类中有效且具有泛化能力。

Abstract: Sentiment analysis using deep learning and pre-trained language models (PLMs)
has gained significant traction due to their ability to capture rich contextual
representations. However, existing approaches often underperform in scenarios
involving nuanced emotional cues, domain shifts, and imbalanced sentiment
distributions. We argue that these limitations stem from inadequate semantic
grounding, poor generalization to diverse linguistic patterns, and biases
toward dominant sentiment classes. To overcome these challenges, we propose
CISEA-MRFE, a novel PLM-based framework integrating Contextual Instruction
(CI), Semantic Enhancement Augmentation (SEA), and Multi-Refined Feature
Extraction (MRFE). CI injects domain-aware directives to guide sentiment
disambiguation; SEA improves robustness through sentiment-consistent
paraphrastic augmentation; and MRFE combines a Scale-Adaptive Depthwise Encoder
(SADE) for multi-scale feature specialization with an Emotion Evaluator Context
Encoder (EECE) for affect-aware sequence modeling. Experimental results on four
benchmark datasets demonstrate that CISEA-MRFE consistently outperforms strong
baselines, achieving relative improvements in accuracy of up to 4.6% on IMDb,
6.5% on Yelp, 30.3% on Twitter, and 4.1% on Amazon. These results validate the
effectiveness and generalization ability of our approach for sentiment
classification across varied domains.

</details>


### [424] [ZoFia: Zero-Shot Fake News Detection with Entity-Guided Retrieval and Multi-LLM Interaction](https://arxiv.org/abs/2511.01188)
*Lvhua Wu,Xuefeng Jiang,Sheng Sun,Tian Wen,Yuwei Wang,Min Liu*

Main category: cs.CL

TL;DR: 提出ZoFia零样本假新闻检测框架，实验显示其性能优，代码将开源。


<details>
  <summary>Details</summary>
Motivation: 假新闻传播威胁社会稳定，现有大语言模型处理新闻流时可靠性低，已有模型泛化能力不足。

Method: 提出Hierarchical Salience量化新闻实体重要性，用SC - MMR算法选关键词检索外部证据；构建多LLM交互系统进行多视角分析和辩论得出判断。

Result: 在两个公开数据集上，ZoFia明显优于现有零样本基线和多数少样本方法。

Conclusion: ZoFia是有效的零样本假新闻检测框架，代码开源利于相关研究。

Abstract: The rapid spread of fake news threatens social stability and public trust,
rendering its detection an imperative research priority. Although large
language models (LLMs) excel at numerous natural language processing tasks with
their remarkable contextual understanding and extensive prior knowledge, the
time-bounded knowledge coverage and tendency for generating hallucination
content reduce their reliability when handling fast-evolving news streams.
Furthermore, models trained on existing static datasets also often lack the
generalization needed for emerging news topics. To address these challenges, we
propose ZoFia, a novel two-stage zero-shot fake news detection framework.
First, we introduce Hierarchical Salience to quantify the importance of
entities in the news content, and propose the SC-MMR algorithm to effectively
select an informative and diverse set of keywords that serve as queries for
retrieving up-to-date external evidence. Subsequently, a multi LLM interactive
system, in which each agent assumes a distinct role, performs multi-view
collaborative analysis and adversarial debate over the news text and its
related information, and finally produces an interpretable and robust judgment.
Comprehensive experiments on two public datasets demonstrate that ZoFia
obviously outperforms existing zero-shot baselines and most of few-shot
methods. Our codes will be open-sourced to facilitate related communities.

</details>


### [425] [Self-Harmony: Learning to Harmonize Self-Supervision and Self-Play in Test-Time Reinforcement Learning](https://arxiv.org/abs/2511.01191)
*Ru Wang,Wei Huang,Qi Cao,Yusuke Iwasawa,Yutaka Matsuo,Jiaxian Guo*

Main category: cs.CL

TL;DR: 介绍Self - Harmony框架用于测试时强化学习，避免常见陷阱，在多基准测试中取得SOTA结果且稳定性高。


<details>
  <summary>Details</summary>
Motivation: 测试时强化学习成功依赖可靠学习信号，标准方法如多数投票易得出虚假答案，需新方法。

Method: 提出Self - Harmony框架，模型分Solver和Reframer两个角色，采用伪标签方法，用调和平均聚合答案频率。

Result: 在多种推理基准测试中，在无标签测试时间设置下取得SOTA结果，28个设置中排名第一，且无训练失败情况。

Conclusion: Self - Harmony框架有效，避免常见陷阱，具有稳定性和可靠性。

Abstract: Test-time reinforcement learning (TTRL) offers a label-free paradigm for
adapting models using only synthetic signals at inference, but its success
hinges on constructing reliable learning signals. Standard approaches such as
majority voting often collapse to spurious yet popular answers. We introduce
Self-Harmony, a framework built on a simple intuition: the correct answer
should remain stable across both an original question and its paraphrase.
Self-Harmony operationalizes this by employing a single model in two
complementary roles: a Solver to produce answers and a Reframer to rephrase the
input. Based on this, we further propose a pseudo-label method: instead of
majority voting, it aggregates answer frequencies across these original and
reframed views using the harmonic mean. This is a process that naturally
selects for solutions stable under reframing, thereby avoiding the common trap
of favoring view-dependent, spurious answers. Crucially, this requires no human
supervision or auxiliary models. Across diverse reasoning benchmarks,
Self-Harmony achieves state-of-the-art results at the label-free test-time
setting, ranking first in 28 of 30 settings across multiple methods. Beyond
accuracy, it demonstrates unprecedented robustness, with zero training failures
in all experiments, underscoring its stability and reliability.

</details>


### [426] [When, What, and How: Rethinking Retrieval-Enhanced Speculative Decoding](https://arxiv.org/abs/2511.01282)
*Min Fang,Zhihui Fu,Qibin Zhao,Jun Wang*

Main category: cs.CL

TL;DR: 提出ReSpec框架加速大语言模型推理，有三项核心创新，实验显示其加速效果超EAGLE - 2和SAM - Decoding，且保证输出质量。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码技术中，基于模型的方法成本高，检索增强方法有不必要检索问题，需改进。

Method: 提出ReSpec框架，包含熵引导自适应触发、反馈驱动候选选择和源感知宽松验证策略。

Result: 在Spec - Bench上实验表明，ReSpec比EAGLE - 2和SAM - Decoding分别提速超33%和25%，且保持输出质量。

Conclusion: ReSpec能有效加速大语言模型推理，实现了准确性和效率的更好平衡。

Abstract: Speculative decoding (SD) has emerged as an effective technique to accelerate
large language model (LLM) inference without compromising output quality.
However, the achievable speedup largely depends on the effectiveness of the
drafting model. While model-based methods like EAGLE-2 are accurate but costly,
retrieval-enhanced methods like SAM-Decoding rely on heuristic switching
strategies that often trigger unnecessary retrievals. To address this, we
propose ReSpec (\textbf{Re}trieval-enhanced \textbf{Spe}culative Decoding), a
novel framework that transforms heuristic drafter switching into adaptive
decision-making. ReSpec features three core innovations: 1) An
\textbf{entropy-guided adaptive trigger} quantifies contextual predictability
to initiate retrieval only when uncertainty is low, avoiding costly low-quality
speculations. 2) A \textbf{feedback-driven candidate selection} leverages
historical feedback to organize multiple high-quality candidates for parallel
verification, maximizing retrieval utility. 3) A source-aware \textbf{relaxed
verification strategy} applies strict checks to model-generated drafts while
using a relaxed verification for retrieved drafts, achieving a better balance
between accuracy and efficiency. Extensive experiments on Spec-Bench
demonstrate that ReSpec achieves state-of-the-art acceleration,outperforming
EAGLE-2 and SAM-Decoding by over $33\%$ and $25\%$, respectively, while
maintaining output quality.

</details>


### [427] [DeepSpecs: Expert-Level Questions Answering in 5G](https://arxiv.org/abs/2511.01305)
*Aman Ganapathy Manvattira,Yifei Xu,Ziyue Dang,Songwu Lu*

Main category: cs.CL

TL;DR: 介绍了名为DeepSpecs的RAG系统，可处理5G规范问答，在多LLM后端上表现优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 现有RAG框架无法可靠解决5G规范中的交叉引用和规范演变推理问题。

Method: 构建三个元数据丰富的数据库，通过元数据查找递归检索引用条款，挖掘变更并关联变更请求。

Result: 在多个LLM后端上，DeepSpecs优于基础模型和先进的电信RAG系统，消融实验证明相关功能提升答案质量。

Conclusion: 对5G标准的结构和时间属性建模具有重要价值。

Abstract: 5G technology enables mobile Internet access for billions of users. Answering
expert-level questions about 5G specifications requires navigating thousands of
pages of cross-referenced standards that evolve across releases. Existing
retrieval-augmented generation (RAG) frameworks, including telecom-specific
approaches, rely on semantic similarity and cannot reliably resolve
cross-references or reason about specification evolution. We present DeepSpecs,
a RAG system enhanced by structural and temporal reasoning via three
metadata-rich databases: SpecDB (clause-aligned specification text), ChangeDB
(line-level version diffs), and TDocDB (standardization meeting documents).
DeepSpecs explicitly resolves cross-references by recursively retrieving
referenced clauses through metadata lookup, and traces specification evolution
by mining changes and linking them to Change Requests that document design
rationale. We curate two 5G QA datasets: 573 expert-annotated real-world
questions from practitioner forums and educational resources, and 350
evolution-focused questions derived from approved Change Requests. Across
multiple LLM backends, DeepSpecs outperforms base models and state-of-the-art
telecom RAG systems; ablations confirm that explicit cross-reference resolution
and evolution-aware retrieval substantially improve answer quality,
underscoring the value of modeling the structural and temporal properties of 5G
standards.

</details>


### [428] [DEEPAMBIGQA: Ambiguous Multi-hop Questions for Benchmarking LLM Answer Completeness](https://arxiv.org/abs/2511.01323)
*Jiabao Ji,Min Li,Priyanshu Kumar,Shiyu Chang,Saloni Potdar*

Main category: cs.CL

TL;DR: 介绍DeepAmbigQAGen数据生成管道和DeepAmbigQA数据集，实验表明现有模型处理复杂问题能力不足。


<details>
  <summary>Details</summary>
Motivation: 现有QA基准很少同时评估区分同名电影和多步推理两个挑战，需要新的数据集。

Method: 引入DeepAmbigQAGen自动数据生成管道，基于此构建DeepAmbigQA数据集。

Result: 即使是GPT - 5在模糊问题上精确匹配率仅0.13，非模糊问题为0.21。

Conclusion: 需要更强大的QA系统以确保信息收集和答案完整性。

Abstract: Large language models (LLMs) with integrated search tools show strong promise
in open-domain question answering (QA), yet they often struggle to produce
complete answer set to complex questions such as Which actor from the film Heat
won at least one Academy Award?, which requires (1) distinguishing between
multiple films sharing the same title and (2) reasoning across a large set of
actors to gather and integrate evidence. Existing QA benchmarks rarely evaluate
both challenges jointly. To address this, we introduce DeepAmbigQAGen, an
automatic data generation pipeline that constructs QA tasks grounded in text
corpora and linked knowledge graph, generating natural and verifiable questions
that systematically embed name ambiguity and multi-step reasoning. Based on
this, we build DeepAmbigQA, a dataset of 3,600 questions requiring multi-hop
reasoning and half of them explicit name ambiguity resolving. Experiments
reveal that, even state-of-the-art GPT-5 show incomplete answers, achieving
only 0.13 exact match on ambiguous questions and 0.21 on non-ambiguous
questions. These findings highlight the need for more robust QA systems aimed
at information gathering and answer completeness.

</details>


### [429] [Thinking with DistilQwen: A Tale of Four Distilled Reasoning and Reward Model Series](https://arxiv.org/abs/2511.01354)
*Wenrui Cai,Chengyu Wang,Junbing Yan,Jun Huang,Xiangzhong Fang*

Main category: cs.CL

TL;DR: 本文扩展DistilQwen模型家族，引入四个模型系列，经评估具高效推理性能，且支持阿里云PAI平台应用。


<details>
  <summary>Details</summary>
Motivation: 满足现实应用对小而高效推理模型的需求，平衡推理性能和推理速度。

Method: 从Qwen模型初始化，引入四个专门设计以满足工业需求的模型系列。

Result: 多个基准测试显示模型推理效率高、推理性能强，蒸馏奖励模型有实际效用，且支持阿里云PAI平台的可扩展训练和推理功能。

Conclusion: 新扩展的DistilQwen模型家族能满足工业需求，为行业从业者提供支持。

Abstract: Recently, the demand for small and efficient reasoning models to support
real-world applications has driven the development of knowledge distillation
techniques that balance reasoning performance and inference speed. In this
paper, we further extend the DistilQwen model family, initialized from the Qwen
models, by introducing four model series specifically designed to meet
industrial requirements. The distilled model collection comprises: (1)
slow-thinking models, optimized for reasoning tasks that require high accuracy;
(2) two series of adaptive-thinking models, which dynamically adjust reasoning
strategies based on input tasks to maximize efficiency across diverse
scenarios; and (3) distilled reward models, which enable further reinforcement
learning of reasoning models using distilled knowledge. Comprehensive
evaluations across multiple benchmarks demonstrate both high inference
efficiency and strong reasoning performance for these models, as well as the
practical utility of distilled reward models. We further show that these models
support industry practitioners by providing scalable training and inference
functionalities on the Alibaba Cloud PAI (Platform for Artificial Intelligence)
platform.

</details>


### [430] [PrefixNLI: Detecting Factual Inconsistencies as Soon as They Arise](https://arxiv.org/abs/2511.01359)
*Sapir Harary,Eran Hirsch,Aviv Slobodkin,David Wan,Mohit Bansal,Ido Dagan*

Main category: cs.CL

TL;DR: 本文将自然语言推理（NLI）模型应用扩展到任意文本前缀，训练了MiniTruePrefixes模型，该模型能更好检测文本前缀事实不一致性，集成到解码框架可提升摘要生成事实一致性。


<details>
  <summary>Details</summary>
Motivation: 现有NLI模型训练用于完整句子事实不一致检测，而自回归生成架构决策基于文本前缀，需将NLI应用扩展到文本前缀以提升生成忠实性。

Method: 将蕴含检测任务扩展到任意文本前缀，提供评估和训练数据集，训练MiniTruePrefixes模型，并将其集成到受控解码框架。

Result: MiniTruePrefixes在文本前缀蕴含检测上比基线NLI模型F1值高5 - 14分，集成到解码框架显著提升摘要事实一致性，如LLaMA - 3.2 - 3B - Instruct在使用一半内存时达到同系列8B模型的忠实性和运行时间。

Conclusion: 扩展NLI到文本前缀以及训练的MiniTruePrefixes模型能有效提升生成的事实一致性。

Abstract: Natural Language Inference (NLI) models have been used in various ways to
improve the factuality of LLM outputs. This is typically done by applying an
NLI model to judge whether the model output is entailed from the supposed
evidence, triggering some corrective actions, such as beam reranking at
inference time or RL rewards during training. While NLI models are trained to
detect factual inconsistencies over complete sentences, decisions in the common
autoregressive generation architecture are made for each evolving text prefix,
during decoding. Addressing this setting, we generalize the entailment
detection task to apply over arbitrary text prefixes, and suggest its utility
for improving generation faithfulness. Providing suitable evaluation and
training datasets for this task, we train MiniTruePrefixes, a novel specialized
model that better detects factual inconsistencies over text prefixes,
outperforming comparable baseline NLI models by 5-14 F1 points in prefix-level
entailment. We further demonstrate that integrating MiniTruePrefixes into a
controlled decoding framework substantially improves factual consistency in
abstractive summarization. When guided by MiniTruePrefixes,
LLaMA-3.2-3B-Instruct matches the faithfulness and runtime of the 8B model from
the same model family, while using only half the memory.

</details>


### [431] [Learning When to Quit in Sales Conversations](https://arxiv.org/abs/2511.01181)
*Emaad Manzoor,Eva Ascarza,Oded Netzer*

Main category: cs.CL

TL;DR: 研究高容量外呼销售中的动态筛选决策，提出基于生成式语言模型的停止代理，可提高销售效率，揭示销售人员决策存在认知局限。


<details>
  <summary>Details</summary>
Motivation: 了解销售人员动态筛选决策的方式、效率及改进方法，尤其是在高容量外呼销售场景中。

Method: 将动态筛选决策形式化为最优停止问题，开发基于生成式语言模型的顺序决策代理（停止代理），模仿回顾性推断的最优停止策略。

Result: 应用于欧洲电信公司的通话时，停止代理减少失败通话时间54%，保存几乎所有销售，重新分配节省时间使预期销售最多增加37%；发现销售人员决策存在认知局限。

Conclusion: 人工智能算法有潜力纠正人类认知局限的决策，提高销售团队效率。

Abstract: Salespeople frequently face the dynamic screening decision of whether to
persist in a conversation or abandon it to pursue the next lead. Yet, little is
known about how these decisions are made, whether they are efficient, or how to
improve them. We study these decisions in the context of high-volume outbound
sales where leads are ample, but time is scarce and failure is common. We
formalize the dynamic screening decision as an optimal stopping problem and
develop a generative language model-based sequential decision agent - a
stopping agent - that learns whether and when to quit conversations by
imitating a retrospectively-inferred optimal stopping policy. Our approach
handles high-dimensional textual states, scales to large language models, and
works with both open-source and proprietary language models. When applied to
calls from a large European telecommunications firm, our stopping agent reduces
the time spent on failed calls by 54% while preserving nearly all sales;
reallocating the time saved increases expected sales by up to 37%. Upon
examining the linguistic cues that drive salespeople's quitting decisions, we
find that they tend to overweight a few salient expressions of consumer
disinterest and mispredict call failure risk, suggesting cognitive bounds on
their ability to make real-time conversational decisions. Our findings
highlight the potential of artificial intelligence algorithms to correct
cognitively-bounded human decisions and improve salesforce efficiency.

</details>


### [432] [BanglaNirTox: A Large-scale Parallel Corpus for Explainable AI in Bengali Text Detoxification](https://arxiv.org/abs/2511.01512)
*Ayesha Afroza Mohsin,Mashrur Ahsan,Nafisa Maliyat,Shanta Maria,Syed Rifat Raiyan,Hasan Mahmud,Md Kamrul Hasan*

Main category: cs.CL

TL;DR: 本文提出针对孟加拉语的文本去毒新管道，构建数据集并微调模型，结果显示优化大模型结合思维链提示能提升去毒效果。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语中的有毒语言普遍存在，但有效防范措施少，且该语言在文本去毒方面因资源有限探索不足。

Method: 提出结合帕累托类优化大语言模型和思维链提示的管道，构建BanglaNirTox数据集并用于微调语言模型。

Result: 帕累托优化的大语言模型结合思维链提示显著提升了孟加拉语文本去毒的质量和一致性。

Conclusion: 所提出的方法在孟加拉语文本去毒方面有显著效果。

Abstract: Toxic language in Bengali remains prevalent, especially in online
environments, with few effective precautions against it. Although text
detoxification has seen progress in high-resource languages, Bengali remains
underexplored due to limited resources. In this paper, we propose a novel
pipeline for Bengali text detoxification that combines Pareto class-optimized
large language models (LLMs) and Chain-of-Thought (CoT) prompting to generate
detoxified sentences. To support this effort, we construct BanglaNirTox, an
artificially generated parallel corpus of 68,041 toxic Bengali sentences with
class-wise toxicity labels, reasonings, and detoxified paraphrases, using
Pareto-optimized LLMs evaluated on random samples. The resulting BanglaNirTox
dataset is used to fine-tune language models to produce better detoxified
versions of Bengali sentences. Our findings show that Pareto-optimized LLMs
with CoT prompting significantly enhance the quality and consistency of Bengali
text detoxification.

</details>


### [433] [Imperfect Language, Artificial Intelligence, and the Human Mind: An Interdisciplinary Approach to Linguistic Errors in Native Spanish Speakers](https://arxiv.org/abs/2511.01615)
*Francisco Portillo López*

Main category: cs.CL

TL;DR: 本文提出对西班牙语母语者语言错误的跨学科研究，用自建语料测试AI模型，助力西班牙语研究和NLP系统发展。


<details>
  <summary>Details</summary>
Motivation: 语言错误能反映语言认知结构和人工系统局限，旨在分析大语言模型对西班牙语母语者语言错误的处理能力。

Method: 整合理论语言学、神经语言学和自然语言处理三个视角，用自建的超500个真实错误语料测试GPT、Gemini等AI模型。

Result: 未提及

Conclusion: 该研究有助于西班牙语研究和开发更具认知能力、能处理真实人类语言的NLP系统。

Abstract: Linguistic errors are not merely deviations from normative grammar; they
offer a unique window into the cognitive architecture of language and expose
the current limitations of artificial systems that seek to replicate them. This
project proposes an interdisciplinary study of linguistic errors produced by
native Spanish speakers, with the aim of analyzing how current large language
models (LLM) interpret, reproduce, or correct them. The research integrates
three core perspectives: theoretical linguistics, to classify and understand
the nature of the errors; neurolinguistics, to contextualize them within
real-time language processing in the brain; and natural language processing
(NLP), to evaluate their interpretation against linguistic errors. A
purpose-built corpus of authentic errors of native Spanish (+500) will serve as
the foundation for empirical analysis. These errors will be tested against AI
models such as GPT or Gemini to assess their interpretative accuracy and their
ability to generalize patterns of human linguistic behavior. The project
contributes not only to the understanding of Spanish as a native language but
also to the development of NLP systems that are more cognitively informed and
capable of engaging with the imperfect, variable, and often ambiguous nature of
real human language.

</details>


### [434] [EngChain: A Symbolic Benchmark for Verifiable Multi-Step Reasoning in Engineering](https://arxiv.org/abs/2511.01650)
*Ayesha Gull,Muhammad Usman Safder,Rania Elbadry,Preslav Nakov,Zhuohan Xie*

Main category: cs.CL

TL;DR: 引入EngChain基准测试，用于评估大语言模型在工程领域的多步问题解决能力，采用两阶段评估。


<details>
  <summary>Details</summary>
Motivation: 当前基准测试无法评估大语言模型在工程领域的综合推理能力，需填补此空白。

Method: 引入包含90个问题的EngChain基准测试，问题由符号模板生成，采用两阶段评估。

Result: 可定量验证推理步骤的数值和语义有效性，还能通过LLM - As - A - Judge系统定性分类推理错误。

Conclusion: EngChain可有效评估大语言模型在工程领域的复杂推理能力。

Abstract: Large Language Models (LLMs) are increasingly being applied to specialized,
high-stakes domains like engineering, which demands rigorous evaluation of
their complex reasoning capabilities. While current benchmarks assess language
understanding, factual recall, mathematics or code generation, none capture the
integrative reasoning central to engineering where scientific principles,
quantitative modeling and practical constraints must converge. To address this
gap, we introduce EngChain, a benchmark for verifiable multi-step engineering
problem-solving. EngChain contains 90 problems spanning three engineering
branches, organized into 9 domains and 20 distinct areas. The problems are
generated from symbolic templates with a high degree of randomization to ensure
diversity and eliminate the risk of contamination. With this benchmark, we move
beyond final answer accuracy with a two-stage evaluation: we first
quantitatively verify the numerical and semantic validity of each reasoning
step and then introduce LLM-As-A-Judge, an automated system to qualitatively
categorize the identified reasoning errors.

</details>


### [435] [SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia](https://arxiv.org/abs/2511.01670)
*Chaoqun Liu,Mahani Aljunied,Guizhen Chen,Hou Pong Chan,Weiwen Xu,Yu Rong,Wenxuan Zhang*

Main category: cs.CL

TL;DR: 介绍首个针对多种东南亚语言的大型音频语言模型SeaLLMs - Audio及其特点，还引入评估基准SeaBench - Audio，实验显示其在东南亚语言上表现有竞争力。


<details>
  <summary>Details</summary>
Motivation: 推动东南亚音频大语言模型发展，使模型能支持多种东南亚语言及多任务。

Method: 在大规模音频语料库上训练SeaLLMs - Audio，引入SeaBench - Audio进行评估。

Result: SeaLLMs - Audio在各种以音频为中心的任务中表现良好，在东南亚语言上与其他LALMs相比有竞争力。

Conclusion: SeaLLMs - Audio对东南亚研究界和行业有益，是推进该地区音频大语言模型的重要一步。

Abstract: We introduce SeaLLMs-Audio, the first large audio-language model (LALM)
tailored for multiple Southeast Asian (SEA) languages-Indonesian (id), Thai
(th), and Vietnamese (vi)-alongside English (en) and Chinese (zh). Trained on a
large-scale audio corpus, SeaLLMs-Audio exhibits strong performance across
diverse audio-centric tasks, spanning fine-grained audio understanding and
voice-based interaction. Its key features include: 1) Multilingual: the model
primarily supports 5 languages, namely Indonesian, Thai, Vietnamese, English,
and Chinese; 2) Multimodal: the model accepts flexible input modalities,
including audio only, text only, as well as audio with text; 3) Multi-task: the
model supports a wide range of tasks, including audio analysis tasks such as
Audio Captioning, Automatic Speech Recognition, Speech-to-Text Translation,
Speech Emotion Recognition, Speech Question Answering, and Speech
Summarization. It also enables voice-based dialogue, including answering
factual, mathematical, and general knowledge queries. As a significant step
towards advancing audio LLMs in Southeast Asia, we expect SeaLLMs-Audio to
benefit both the regional research community and industry. To automate LALM
evaluation for Southeast Asia, we introduce SeaBench-Audio, a benchmark
spanning multiple tasks. Experiments show that SeaLLMs-Audio achieves
competitive performance compared with other LALMs on SEA languages.

</details>


### [436] [Open Character Training: Shaping the Persona of AI Assistants through Constitutional AI](https://arxiv.org/abs/2511.01689)
*Sharan Maiya,Henning Bartsch,Nathan Lambert,Evan Hubinger*

Main category: cs.CL

TL;DR: 本文介绍首个开放的角色训练实现，用合成自省数据塑造聊天机器人角色，效果好且对通用能力影响小，并开源方法。


<details>
  <summary>Details</summary>
Motivation: 角色训练是行业后训练关键部分，但学术文献中研究不足，需要有效塑造聊天机器人角色。

Method: 利用宪法AI和新数据管道，用合成自省数据微调三个流行的开放权重模型，分析揭示偏好来追踪效果。

Result: 角色变化对对抗性提示更稳健，生成更连贯真实，微调对通用能力影响小。

Conclusion: 提出的角色训练方法有效且可开源，能更好地塑造聊天机器人人格。

Abstract: The character of the "AI assistant" persona generated by modern chatbot large
language models influences both surface-level behavior and apparent values,
beliefs, and ethics. These all affect interaction quality, perceived
intelligence, and alignment with both developer and user intentions. The
shaping of this persona, known as character training, is a critical component
of industry post-training, yet remains effectively unstudied in the academic
literature. We introduce the first open implementation of character training,
leveraging Constitutional AI and a new data pipeline using synthetic
introspective data to shape the assistant persona in a more effective and
controlled manner than alternatives such as constraining system prompts or
activation steering. Specifically, we fine-tune three popular open-weights
models using 11 example personas, such as humorous, deeply caring, or even
malevolent. To track the effects of our approach, we introduce a method which
analyzes revealed preferences, uncovering clear and holistic changes in
character. We find these changes are more robust to adversarial prompting than
the above two alternatives, while also leading to more coherent and realistic
generations. Finally, we demonstrate this fine-tuning has little to no effect
on general capabilities as measured by common benchmarks. We describe and
open-source our full post-training method, the implementation of which can be
found at https://github.com/maiush/OpenCharacterTraining.

</details>


### [437] [Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace Disentanglement](https://arxiv.org/abs/2511.01706)
*Sekh Mainul Islam,Pepa Atanasova,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 提出新的秩2投影子空间分析大语言模型自然语言解释中参数知识和上下文知识的交互，实验表明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 现有研究对大语言模型自然语言解释中参数知识和上下文知识交互的理解不足，且多为单步分析，模型交互形式单一。

Method: 提出秩2投影子空间，对较长自然语言解释序列进行多步知识交互分析。

Result: 秩1子空间不能很好表示多样知识交互，秩2公式能有效捕捉；多步分析揭示不同类型解释与参数知识和上下文知识的关系。

Conclusion: 提供了通过更丰富的秩2子空间解耦对大语言模型多步知识交互进行系统研究的首个框架。

Abstract: Natural Language Explanations (NLEs) describe how Large Language Models
(LLMs) make decisions, drawing on both external Context Knowledge (CK) and
Parametric Knowledge (PK) stored in model weights. Understanding their
interaction is key to assessing the grounding of NLEs, yet it remains
underexplored. Prior work has largely examined only single-step generation,
typically the final answer, and has modelled PK and CK interaction only as a
binary choice in a rank-1 subspace. This overlooks richer forms of interaction,
such as complementary or supportive knowledge. We propose a novel rank-2
projection subspace that disentangles PK and CK contributions more accurately
and use it for the first multi-step analysis of knowledge interactions across
longer NLE sequences. Experiments on four QA datasets and three open-weight
instruction-tuned LLMs show that diverse knowledge interactions are poorly
represented in a rank-1 subspace but are effectively captured in our rank-2
formulation. Our multi-step analysis reveals that hallucinated NLEs align
strongly with the PK direction, context-faithful ones balance PK and CK, and
Chain-of-Thought prompting for NLEs shifts generated NLEs toward CK by reducing
PK reliance. This work provides the first framework for systematic studies of
multi-step knowledge interactions in LLMs through a richer rank-2 subspace
disentanglement. Code and data:
https://github.com/copenlu/pk-ck-knowledge-disentanglement.

</details>


### [438] [KV Cache Transform Coding for Compact Storage in LLM Inference](https://arxiv.org/abs/2511.01815)
*Konrad Staniszewski,Adrian Łańcucki*

Main category: cs.CL

TL;DR: 提出轻量级变换编码器KVTC压缩大语言模型的KV缓存，测试表明其优于多种基线方法，是高效内存服务的实用组件。


<details>
  <summary>Details</summary>
Motivation: 大规模服务大语言模型需要高效的KV缓存管理，陈旧缓存消耗GPU内存，需解决存储问题。

Method: 借鉴经典媒体压缩，结合基于PCA的特征去相关、自适应量化和熵编码，仅需初始校准且不改变模型参数。

Result: KVTC实现高达20倍压缩，特定用例达40倍或更高，在多模型多基准测试中始终优于推理时基线方法。

Conclusion: KVTC是具有可重复使用KV缓存的内存高效大语言模型服务的实用构建块。

Abstract: Serving large language models (LLMs) at scale necessitates efficient
key-value (KV) cache management. KV caches can be reused across conversation
turns via shared-prefix prompts that are common in iterative code editing and
chat. However, stale caches consume scarce GPU memory, require offloading, or
force recomputation. We present KVTC, a lightweight transform coder that
compresses KV caches for compact on-GPU and off-GPU storage. Drawing on
classical media compression, KVTC combines PCA-based feature decorrelation,
adaptive quantization, and entropy coding. It requires only a brief initial
calibration and leaves model parameters unchanged. By exploiting redundancies
in KV caches, KVTC achieves up to 20$\times$ compression while maintaining
reasoning and long-context accuracy, and 40$\times$ or higher for specific use
cases. We test KVTC with Llama 3, Mistral NeMo, and R1-Qwen 2.5 models across
benchmarks including AIME25, LiveCodeBench, GSM8K, MMLU, Qasper, RULER, and
MATH-500. It consistently outperforms inference-time baselines such as token
eviction, quantization, and SVD-based methods, while achieving higher
compression ratios. These results support KVTC as a practical building block
for memory-efficient LLM serving with reusable KV caches.

</details>


### [439] [Accumulating Context Changes the Beliefs of Language Models](https://arxiv.org/abs/2511.01805)
*Jiayi Geng,Howard Chen,Ryan Liu,Manoel Horta Ribeiro,Robb Willer,Graham Neubig,Thomas L. Griffiths*

Main category: cs.CL

TL;DR: 本文探讨语言模型在交互和处理文本时上下文积累对其信念的影响，发现模型信念易变，且信念转变会反映在行为中，揭示了信念转变的潜在风险。


<details>
  <summary>Details</summary>
Motivation: 语言模型自主性增强使上下文窗口文本积累，可能导致模型信念无声改变，引发用户体验不一致和行为偏离原校准，因此探究上下文积累对模型信念的影响。

Method: 通过进行道德困境讨论、安全问题查询、阅读对立立场文本等交互，以及设计需要工具使用的任务来观察模型信念和行为的变化。

Result: GPT - 5在10轮道德困境和安全问题讨论后信念陈述有54.7%的转变，Grok 4在阅读对立立场文本后政治问题信念有27.2%的转变，且行为变化与信念陈述转变一致。

Conclusion: 模型在长时间对话或阅读中存在信念转变的隐藏风险，使其观点和行动不可靠。

Abstract: Language model (LM) assistants are increasingly used in applications such as
brainstorming and research. Improvements in memory and context size have
allowed these models to become more autonomous, which has also resulted in more
text accumulation in their context windows without explicit user intervention.
This comes with a latent risk: the belief profiles of models -- their
understanding of the world as manifested in their responses or actions -- may
silently change as context accumulates. This can lead to subtly inconsistent
user experiences, or shifts in behavior that deviate from the original
alignment of the models. In this paper, we explore how accumulating context by
engaging in interactions and processing text -- talking and reading -- can
change the beliefs of language models, as manifested in their responses and
behaviors.Our results reveal that models' belief profiles are highly malleable:
GPT-5 exhibits a 54.7% shift in its stated beliefs after 10 rounds of
discussion about moral dilemmas and queries about safety, while Grok 4 shows a
27.2% shift on political issues after reading texts from the opposing position.
We also examine models' behavioral changes by designing tasks that require tool
use, where each tool selection corresponds to an implicit belief. We find that
these changes align with stated belief shifts, suggesting that belief shifts
will be reflected in actual behavior in agentic systems. Our analysis exposes
the hidden risk of belief shift as models undergo extended sessions of talking
or reading, rendering their opinions and actions unreliable.

</details>


### [440] [Plan-and-Write: Structure-Guided Length Control for LLMs without Model Retraining](https://arxiv.org/abs/2511.01807)
*Adewale Akinfaderin,Shreyas Subramanian,Akarsha Sehwag*

Main category: cs.CL

TL;DR: 提出一种无需模型再训练的提示工程方法实现大语言模型长度控制，经评估效果良好。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型长度控制方法需昂贵的模型再训练或复杂推理工具，该研究旨在解决此问题。

Method: 采用结构引导的提示工程方法，在提示中实现规划和词数统计机制。

Result: 在六个先进大语言模型上评估，该方法显著提高部分模型长度控制精度，部分模型长度遵守率提升达37.6%，且保持或提升输出质量。

Conclusion: 该方法为需要精确长度控制的应用提供了可立即部署的解决方案，对生产环境有重要价值。

Abstract: Length control in Large Language Models (LLMs) is a crucial but
under-addressed challenge, with applications ranging from voice interfaces
requiring concise responses to research summaries needing comprehensive
outputs. Current approaches to length control, including Regularized DPO,
Length-Instruction Fine Tuning, and tool-augmented methods, typically require
expensive model retraining or complex inference-time tooling. This paper
presents a prompt engineering methodology that enables precise length control
without model retraining. Our structure-guided approach implements deliberate
planning and word counting mechanisms within the prompt, encouraging the model
to carefully track and adhere to specified length constraints. Comprehensive
evaluations across six state-of-the-art LLMs demonstrate that our method
significantly improves length fidelity for several models compared to standard
prompting when applied to document summarization tasks, particularly for
shorter-to-medium length constraints. The proposed technique shows varying
benefits across different model architectures, with some models demonstrating
up to 37.6% improvement in length adherence. Quality evaluations further reveal
that our approach maintains or enhances overall output quality compared to
standard prompting techniques. Our approach provides an immediately deployable
solution for applications requiring precise length control, particularly
valuable for production environments where model retraining is impractical or
cost-prohibitive.

</details>


### [441] [Towards Robust Mathematical Reasoning](https://arxiv.org/abs/2511.01846)
*Thang Luong,Dawsen Hwang,Hoang H. Nguyen,Golnaz Ghiasi,Yuri Chervonyi,Insuk Seo,Junsu Kim,Garrett Bingham,Jonathan Lee,Swaroop Mishra,Alex Zhai,Clara Huiyi Hu,Henryk Michalewski,Jimin Kim,Jeonghyun Ahn,Junhwi Bae,Xingyou Song,Trieu H. Trinh,Quoc V. Le,Junehyuk Jung*

Main category: cs.CL

TL;DR: 提出IMO - Bench基准套件提升基础模型数学推理能力评估，模型取得好成绩并构建自动评估基准。


<details>
  <summary>Details</summary>
Motivation: 现有评估要么太简单，要么只关注正确短答案，需找到合适指标提升基础模型数学推理能力。

Method: 提出IMO - Bench基准套件，含IMO - AnswerBench测试短答案、IMO - Proof Bench测试证明能力及自动评分指南。

Result: 模型在IMO - AnswerBench达80.0%，在advanced IMO - Proof Bench达65.7%，远超非Gemini模型；自动评分器与人工评估相关性好，构建IMO - GradingBench。

Conclusion: 希望IMO - Bench助力社区提升鲁棒数学推理能力并公开该基准。

Abstract: Finding the right north-star metrics is highly critical for advancing the
mathematical reasoning capabilities of foundation models, especially given that
existing evaluations are either too easy or only focus on getting correct short
answers. To address these issues, we present IMO-Bench, a suite of advanced
reasoning benchmarks, vetted by a panel of top specialists and that
specifically targets the level of the International Mathematical Olympiad
(IMO), the most prestigious venue for young mathematicians. IMO-AnswerBench
first tests models on 400 diverse Olympiad problems with verifiable short
answers. IMO-Proof Bench is the next-level evaluation for proof-writing
capabilities, which includes both basic and advanced IMO level problems as well
as detailed grading guidelines to facilitate automatic grading. These
benchmarks played a crucial role in our historic achievement of the gold-level
performance at IMO 2025 with Gemini Deep Think (Luong and Lockhart, 2025). Our
model achieved 80.0% on IMO-AnswerBench and 65.7% on the advanced IMO-Proof
Bench, surpassing the best non-Gemini models by large margins of 6.9% and 42.4%
respectively. We also showed that autograders built with Gemini reasoning
correlate well with human evaluations and construct IMO-GradingBench, with 1000
human gradings on proofs, to enable further progress in automatic evaluation of
long-form answers. We hope that IMO-Bench will help the community towards
advancing robust mathematical reasoning and release it at
https://imobench.github.io/.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [442] [MH-1M: A 1.34 Million-Sample Comprehensive Multi-Feature Android Malware Dataset for Machine Learning, Deep Learning, Large Language Models, and Threat Intelligence Research](https://arxiv.org/abs/2511.00342)
*Hendrio Braganca,Diego Kreutz,Vanderson Rocha,Joner Assolin,and Eduardo Feitosa*

Main category: cs.CR

TL;DR: 介绍MH - 1M数据集，用于高级安卓恶意软件研究，包含大量应用和元数据，用VirusTotal API分类，数据公开可访问。


<details>
  <summary>Details</summary>
Motivation: 提供全面且最新的数据集用于高级安卓恶意软件研究。

Method: 使用VirusTotal API，集成多个检测引擎进行恶意软件分类。

Result: 创建了包含1340515个应用的MH - 1M数据集，数据和元数据超400GB且公开可访问。

Conclusion: MH - 1M数据集对理解恶意软件演变格局有重要价值。

Abstract: We present MH-1M, one of the most comprehensive and up-to-date datasets for
advanced Android malware research. The dataset comprises 1,340,515
applications, encompassing a wide range of features and extensive metadata. To
ensure accurate malware classification, we employ the VirusTotal API,
integrating multiple detection engines for comprehensive and reliable
assessment. Our GitHub, Figshare, and Harvard Dataverse repositories provide
open access to the processed dataset and its extensive supplementary metadata,
totaling more than 400 GB of data and including the outputs of the feature
extraction pipeline as well as the corresponding VirusTotal reports. Our
findings underscore the MH-1M dataset's invaluable role in understanding the
evolving landscape of malware.

</details>


### [443] [Split Learning-Enabled Framework for Secure and Light-weight Internet of Medical Things Systems](https://arxiv.org/abs/2511.00336)
*Siva Sai,Manish Prasad,Animesh Bhargava,Vinay Chamola,Rajkumar Buyya*

Main category: cs.CR

TL;DR: 提出基于拆分学习的物联网恶意软件检测框架，性能优于联邦学习方法，证明拆分学习在物联网安全的潜力。


<details>
  <summary>Details</summary>
Motivation: 医疗物联网设备增长带来安全风险，传统深度学习因资源限制不可行，联邦学习有高通信开销和非IID数据问题。

Method: 提出基于拆分学习的框架进行基于图像分类的物联网恶意软件检测，用博弈论方法平衡计算成本和通信效率。

Result: 框架在准确率、F1分数、收敛速度和资源消耗方面优于流行的联邦学习方法。

Conclusion: 拆分学习可作为下一代物联网安全的可扩展和安全范式。

Abstract: The rapid growth of Internet of Medical Things (IoMT) devices has resulted in
significant security risks, particularly the risk of malware attacks on
resource-constrained devices. Conventional deep learning methods are
impractical due to resource limitations, while Federated Learning (FL) suffers
from high communication overhead and vulnerability to non-IID (heterogeneous)
data. In this paper, we propose a split learning (SL) based framework for IoT
malware detection through image-based classification. By dividing the neural
network training between the clients and an edge server, the framework reduces
computational burden on resource-constrained clients while ensuring data
privacy. We formulate a joint optimization problem that balances computation
cost and communication efficiency by using a game-theoretic approach for
attaining better training performance. Experimental evaluations show that the
proposed framework outperforms popular FL methods in terms of accuracy
(+6.35%), F1-score (+5.03%), high convergence speed (+14.96%), and less
resource consumption (33.83%). These results establish the potential of SL as a
scalable and secure paradigm for next-generation IoT security.

</details>


### [444] [Federated Cyber Defense: Privacy-Preserving Ransomware Detection Across Distributed Systems](https://arxiv.org/abs/2511.01583)
*Daniel M. Jimenez-Gutierrez,Enrique Zuazua,Joaquin Del Rio,Oleksii Sliusarenko,Xabi Uribe-Etxebarria*

Main category: cs.CR

TL;DR: 本文评估了使用Sherpa.ai联邦学习平台进行勒索软件检测，实验表明联邦学习比服务器本地模型提高了9%的检测准确率，是一种可扩展、高性能且保护隐私的检测框架。


<details>
  <summary>Details</summary>
Motivation: 检测勒索软件对保障互联生态系统安全至关重要，训练高性能AI检测器需要多样数据集，但集中学习因安全、隐私等问题不切实际，且勒索软件演变迅速，需要鲁棒且适应性强的模型。

Method: 使用Sherpa.ai联邦学习平台，让多个组织在保持原始数据本地安全的情况下协同训练勒索软件检测模型，并用RanSAP数据集验证该方法。

Result: 联邦学习比服务器本地模型相对提高了9%的勒索软件检测准确率，性能与集中训练相当。

Conclusion: 联邦学习为跨组织和监管边界的主动勒索软件检测提供了可扩展、高性能且保护隐私的框架。

Abstract: Detecting malware, especially ransomware, is essential to securing today's
interconnected ecosystems, including cloud storage, enterprise file-sharing,
and database services. Training high-performing artificial intelligence (AI)
detectors requires diverse datasets, which are often distributed across
multiple organizations, making centralization necessary. However, centralized
learning is often impractical due to security, privacy regulations, data
ownership issues, and legal barriers to cross-organizational sharing.
Compounding this challenge, ransomware evolves rapidly, demanding models that
are both robust and adaptable.
  In this paper, we evaluate Federated Learning (FL) using the Sherpa.ai FL
platform, which enables multiple organizations to collaboratively train a
ransomware detection model while keeping raw data local and secure. This
paradigm is particularly relevant for cybersecurity companies (including both
software and hardware vendors) that deploy ransomware detection or firewall
systems across millions of endpoints. In such environments, data cannot be
transferred outside the customer's device due to strict security, privacy, or
regulatory constraints. Although FL applies broadly to malware threats, we
validate the approach using the Ransomware Storage Access Patterns (RanSAP)
dataset.
  Our experiments demonstrate that FL improves ransomware detection accuracy by
a relative 9% over server-local models and achieves performance comparable to
centralized training. These results indicate that FL offers a scalable,
high-performing, and privacy-preserving framework for proactive ransomware
detection across organizational and regulatory boundaries.

</details>


### [445] [Rescuing the Unpoisoned: Efficient Defense against Knowledge Corruption Attacks on RAG Systems](https://arxiv.org/abs/2511.01268)
*Minseok Kim,Hankook Lee,Hyungjoon Koo*

Main category: cs.CR

TL;DR: 大语言模型面临生成幻觉内容等问题，检索增强生成（RAG）可解决但易受攻击，现有防御策略成本高，本文提出资源高效的RAGDefender防御机制，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决RAG系统易受知识腐败攻击且现有防御策略计算成本高的问题。

Method: 在检索后阶段，利用轻量级机器学习技术检测和过滤对抗性内容，无需额外模型训练或推理。

Result: RAGDefender在多个模型和对抗场景中始终优于现有最先进的防御方法，如在特定场景下降低Gemini模型的攻击成功率。

Conclusion: RAGDefender是一种资源高效的、可用于实际RAG部署中对抗知识腐败攻击的有效防御机制。

Abstract: Large language models (LLMs) are reshaping numerous facets of our daily
lives, leading widespread adoption as web-based services. Despite their
versatility, LLMs face notable challenges, such as generating hallucinated
content and lacking access to up-to-date information. Lately, to address such
limitations, Retrieval-Augmented Generation (RAG) has emerged as a promising
direction by generating responses grounded in external knowledge sources. A
typical RAG system consists of i) a retriever that probes a group of relevant
passages from a knowledge base and ii) a generator that formulates a response
based on the retrieved content. However, as with other AI systems, recent
studies demonstrate the vulnerability of RAG, such as knowledge corruption
attacks by injecting misleading information. In response, several defense
strategies have been proposed, including having LLMs inspect the retrieved
passages individually or fine-tuning robust retrievers. While effective, such
approaches often come with substantial computational costs.
  In this work, we introduce RAGDefender, a resource-efficient defense
mechanism against knowledge corruption (i.e., by data poisoning) attacks in
practical RAG deployments. RAGDefender operates during the post-retrieval
phase, leveraging lightweight machine learning techniques to detect and filter
out adversarial content without requiring additional model training or
inference. Our empirical evaluations show that RAGDefender consistently
outperforms existing state-of-the-art defenses across multiple models and
adversarial scenarios: e.g., RAGDefender reduces the attack success rate (ASR)
against the Gemini model from 0.89 to as low as 0.02, compared to 0.69 for
RobustRAG and 0.24 for Discern-and-Answer when adversarial passages outnumber
legitimate ones by a factor of four (4x).

</details>


### [446] [A Large Scale Study of AI-based Binary Function Similarity Detection Techniques for Security Researchers and Practitioners](https://arxiv.org/abs/2511.01180)
*Jingyi Shi,Yufeng Chen,Yang Xiao,Yuekang Li,Zhengzi Xu,Sihao Qiu,Chi Zhang,Keyu Qi,Yeting Li,Xingchu Chen,Yanyan Zou,Yang Liu,Wei Huo*

Main category: cs.CR

TL;DR: 文章开展AI-based BFSD工具大规模实证研究，构建两个数据集评估工具，分析问题，提出组合策略提升性能，推动工具应用和相关研究。


<details>
  <summary>Details</summary>
Motivation: 现有AI-based BFSD工具评估存在对性能影响因素分析不深入、缺乏现实应用分析、依赖小规模或低质量数据集的问题，需解决这些差距。

Method: 构建BinAtlas和BinAres两个高质量多样数据集，评估九个代表性BFSD工具，分析挑战和局限性，研究工具间一致性。

Result: 提出组合BFSD工具的策略使整体性能提升13.4%。

Conclusion: 研究推动了BFSD工具的实际应用，为可扩展和自动化二进制相似性检测的未来研究提供有价值的资源和见解。

Abstract: Binary Function Similarity Detection (BFSD) is a foundational technique in
software security, underpinning a wide range of applications including
vulnerability detection, malware analysis. Recent advances in AI-based BFSD
tools have led to significant performance improvements. However, existing
evaluations of these tools suffer from three key limitations: a lack of
in-depth analysis of performance-influencing factors, an absence of realistic
application analysis, and reliance on small-scale or low-quality datasets.
  In this paper, we present the first large-scale empirical study of AI-based
BFSD tools to address these gaps. We construct two high-quality and diverse
datasets: BinAtlas, comprising 12,453 binaries and over 7 million functions for
capability evaluation; and BinAres, containing 12,291 binaries and 54
real-world 1-day vulnerabilities for evaluating vulnerability detection
performance in practical IoT firmware settings. Using these datasets, we
evaluate nine representative BFSD tools, analyze the challenges and limitations
of existing BFSD tools, and investigate the consistency among BFSD tools. We
also propose an actionable strategy for combining BFSD tools to enhance overall
performance (an improvement of 13.4%). Our study not only advances the
practical adoption of BFSD tools but also provides valuable resources and
insights to guide future research in scalable and automated binary similarity
detection.

</details>


### [447] [Exploiting Latent Space Discontinuities for Building Universal LLM Jailbreaks and Data Extraction Attacks](https://arxiv.org/abs/2511.00346)
*Kayua Oleques Paim,Rodrigo Brandao Mansilha,Diego Kreutz,Muriel Figueredo Franco,Weverton Cordeiro*

Main category: cs.CR

TL;DR: 本文提出利用潜在空间不连续性进行通用越狱和数据提取攻击的新方法，在多个模型中有效，显示出作为系统性攻击向量的潜力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在对抗攻击下的安全性引发关注，需研究新的攻击方法。

Method: 利用与训练数据稀疏性相关的潜在空间不连续性这一架构漏洞，提出新的攻击方法。

Result: 该方法在七个最先进的大语言模型和一个图像生成模型中高度有效，即使存在分层防御，也能持续且深刻地损害模型行为。

Conclusion: 此策略作为系统性攻击向量具有很大潜力。

Abstract: The rapid proliferation of Large Language Models (LLMs) has raised
significant concerns about their security against adversarial attacks. In this
work, we propose a novel approach to crafting universal jailbreaks and data
extraction attacks by exploiting latent space discontinuities, an architectural
vulnerability related to the sparsity of training data. Unlike previous
methods, our technique generalizes across various models and interfaces,
proving highly effective in seven state-of-the-art LLMs and one image
generation model. Initial results indicate that when these discontinuities are
exploited, they can consistently and profoundly compromise model behavior, even
in the presence of layered defenses. The findings suggest that this strategy
has substantial potential as a systemic attack vector.

</details>


### [448] [Mind the Gap: Missing Cyber Threat Coverage in NIDS Datasets for the Energy Sector](https://arxiv.org/abs/2511.00360)
*Adrita Rahman Tory,Khondokar Fida Hasan,Md Saifur Rahman,Nickolaos Koroniotis,Mohammad Ali Moni*

Main category: cs.CR

TL;DR: 评估五个常用数据集对能源基础设施中网络入侵检测的代表性，找出差距并提出改进方向。


<details>
  <summary>Details</summary>
Motivation: 现有基于公开数据集的网络入侵检测系统多用于企业环境，需评估其对能源基础设施中融合的IT和OT环境的有效性。

Method: 采用五步分析方法，将五个数据集与从能源行业事件中提取的MITRE ATT&CK技术进行对比。

Result: 确定94种可观测网络技术，Sherlock数据集平均覆盖率最高，组合部分数据集覆盖率达92%。

Conclusion: 分析找出关键差距，为数据集增强和更强大的NIDS评估提供方向。

Abstract: Network Intrusion Detection Systems (NIDS) developed us- ing publicly
available datasets predominantly focus on enterprise environ- ments, raising
concerns about their effectiveness for converged Informa- tion Technology (IT)
and Operational Technology (OT) in energy infras- tructures. This study
evaluates the representativeness of five widely used datasets: CIC-IDS2017,
SWaT, WADI, Sherlock, and CIC-Modbus2023 against network-detectable MITRE
ATT&CK techniques extracted from documented energy sector incidents. Using a
structured five-step analyt- ical approach, this article successfully developed
and performed a gap analysis that identified 94 network observable techniques
from an initial pool of 274 ATT&CK techniques. Sherlock dataset exhibited the
high- est mean coverage (0.56), followed closely by CIC-IDS2017 (0.55), while
SWaT and WADI recorded the lowest scores (0.38). Combining CIC- IDS2017,
Sherlock, and CIC-Modbus2023 achieved an aggregate coverage of 92%,
highlighting their complementary strengths. The analysis identi- fies critical
gaps, particularly in lateral movement and industrial protocol manipulation,
providing a clear pathway for dataset enhancement and more robust NIDS
evaluation in hybrid IT/OT energy environments.

</details>


### [449] [MalDataGen: A Modular Framework for Synthetic Tabular Data Generation in Malware Detection](https://arxiv.org/abs/2511.00361)
*Kayua Oleques Paim,Angelo Gaspar Diniz Nogueira,Diego Kreutz,Weverton Cordeiro,Rodrigo Brandao Mansilha*

Main category: cs.CR

TL;DR: 提出开源框架MalDataGen生成高保真合成表格数据用于恶意软件检测，表现优于基准且可集成到检测流程。


<details>
  <summary>Details</summary>
Motivation: 高质量数据稀缺阻碍恶意软件检测，限制机器学习性能。

Method: 引入MalDataGen框架，使用模块化深度学习模型（如WGAN - GP、VQ - VAE）生成数据，通过双重验证、七种分类器和效用指标评估。

Result: MalDataGen优于SDV等基准，同时保留数据效用。

Conclusion: MalDataGen灵活设计可无缝集成到检测管道，为网络安全应用提供实用解决方案。

Abstract: High-quality data scarcity hinders malware detection, limiting ML
performance. We introduce MalDataGen, an open-source modular framework for
generating high-fidelity synthetic tabular data using modular deep learning
models (e.g., WGAN-GP, VQ-VAE). Evaluated via dual validation (TR-TS/TS-TR),
seven classifiers, and utility metrics, MalDataGen outperforms benchmarks like
SDV while preserving data utility. Its flexible design enables seamless
integration into detection pipelines, offering a practical solution for
cybersecurity applications.

</details>


### [450] [DRIP: Defending Prompt Injection via De-instruction Training and Residual Fusion Model Architecture](https://arxiv.org/abs/2511.00447)
*Ruofan Liu,Yun Lin,Jin Song Dong*

Main category: cs.CR

TL;DR: 本文提出DRIP防御机制应对大语言模型提示注入攻击，实验表明其效果优于现有方法且不影响实用性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型指令跟随能力使其易受提示注入攻击，核心漏洞是缺乏语义角色理解。

Method: 提出DRIP，包含token-wise de-instruction shift进行语义解缠和residual fusion pathway提供语义锚点。

Result: 在LLaMA - 8B和Mistral - 7B上实验，DRIP优于StruQ等现有防御方法，角色分离提升49%，自适应攻击成功率降低66%，实用性与未防御模型相当。

Conclusion: 轻量级表示编辑和角色感知监督可有效保护大语言模型免受自适应提示注入攻击。

Abstract: Large language models (LLMs) have demonstrated impressive
instruction-following capabilities. However, these capabilities also expose
models to prompt injection attacks, where maliciously crafted inputs overwrite
or distract from the intended instructions. A core vulnerability lies in the
model's lack of semantic role understanding: it cannot distinguish directive
intent from descriptive content, leading it to execute instruction-like phrases
embedded in data.
  We propose DRIP, a training-time defense grounded in a semantic modeling
perspective, which enforces robust separation between instruction and data
semantics without sacrificing utility. DRIP introduces two lightweight yet
complementary mechanisms: (1) a token-wise de-instruction shift that performs
semantic disentanglement, weakening directive semantics in data tokens while
preserving content meaning; and (2) a residual fusion pathway that provides a
persistent semantic anchor, reinforcing the influence of the true top-level
instruction during generation. Experimental results on LLaMA-8B and Mistral-7B
across three prompt injection benchmarks (SEP, AlpacaFarm, and InjecAgent)
demonstrate that DRIP outperforms state-of-the-art defenses, including StruQ,
SecAlign, ISE, and PFT, improving role separation by 49%, and reducing attack
success rate by 66% for adaptive attacks. Meanwhile, DRIP's utility is on par
with the undefended model across AlpacaEval, IFEval, and MT-Bench. Our findings
underscore the power of lightweight representation edits and role-aware
supervision in securing LLMs against adaptive prompt injection.

</details>


### [451] [Proactive DDoS Detection and Mitigation in Decentralized Software-Defined Networking via Port-Level Monitoring and Zero-Training Large Language Models](https://arxiv.org/abs/2511.00460)
*Mohammed N. Swileh,Shengli Zhang*

Main category: cs.CR

TL;DR: 本文提出适用于dSDN环境的DDoS攻击检测与缓解框架，利用轻量级端口统计和大语言模型实现零训练分类，实验效果佳。


<details>
  <summary>Details</summary>
Motivation: cSDN有可扩展性和可靠性问题，dSDN易受DDoS攻击，需有效防御机制。

Method: 提出检测与缓解框架，利用轻量级端口统计、提示工程和上下文学习，让DeepSeek - v3 LLM零训练分类流量，在攻击端口执行缓解，设自动恢复机制。

Result: 在多种DDoS攻击场景下实验，检测准确率99.99%、精度99.97%、召回率100%、F1分数99.98%、AUC为1.0。

Conclusion: 结合分布式监控与零训练LLM推理的方法有效，为dSDN基础设施提供主动、可扩展的DDoS防御机制。

Abstract: Centralized Software-Defined Networking (cSDN) offers flexible and
programmable control of networks but suffers from scalability and reliability
issues due to its reliance on centralized controllers. Decentralized SDN (dSDN)
alleviates these concerns by distributing control across multiple local
controllers, yet this architecture remains highly vulnerable to Distributed
Denial-of-Service (DDoS) attacks. In this paper, we propose a novel detection
and mitigation framework tailored for dSDN environments. The framework
leverages lightweight port-level statistics combined with prompt engineering
and in-context learning, enabling the DeepSeek-v3 Large Language Model (LLM) to
classify traffic as benign or malicious without requiring fine-tuning or
retraining. Once an anomaly is detected, mitigation is enforced directly at the
attacker's port, ensuring that malicious traffic is blocked at their origin
while normal traffic remains unaffected. An automatic recovery mechanism
restores normal operation after the attack inactivity, ensuring both security
and availability. Experimental evaluation under diverse DDoS attack scenarios
demonstrates that the proposed approach achieves near-perfect detection, with
99.99% accuracy, 99.97% precision, 100% recall, 99.98% F1-score, and an AUC of
1.0. These results highlight the effectiveness of combining distributed
monitoring with zero-training LLM inference, providing a proactive and scalable
defense mechanism for securing dSDN infrastructures against DDoS threats.

</details>


### [452] [ShadowLogic: Backdoors in Any Whitebox LLM](https://arxiv.org/abs/2511.00664)
*Kasimir Schulz,Amelia Kawasaki,Leo Ring*

Main category: cs.CR

TL;DR: 本文指出基于计算图的大语言模型格式存在安全漏洞，介绍了ShadowLogic方法创建后门绕过内容生成防护机制，在Phi - 3和Llama 3.2上实现且攻击成功率超60%。


<details>
  <summary>Details</summary>
Motivation: 指出大语言模型虽有内容防护机制，但计算图可能被对抗性修改绕过防护，揭示广泛使用的部署管道可能存在隐蔽后门这一安全漏洞。

Method: 引入ShadowLogic方法，向白盒大语言模型的计算图表示中注入解审查向量，设置触发短语，将触发逻辑嵌入计算图并混淆逻辑以躲避检测，对模型参数改动极小。

Result: 在Phi - 3和Llama 3.2中成功实现ShadowLogic，植入解审查向量后对进一步恶意查询的攻击成功率超60%。

Conclusion: 基于计算图的大语言模型格式存在安全隐患，ShadowLogic方法能有效创建后门绕过内容防护。

Abstract: Large language models (LLMs) are widely deployed across various applications,
often with safeguards to prevent the generation of harmful or restricted
content. However, these safeguards can be covertly bypassed through adversarial
modifications to the computational graph of a model. This work highlights a
critical security vulnerability in computational graph-based LLM formats,
demonstrating that widely used deployment pipelines may be susceptible to
obscured backdoors. We introduce ShadowLogic, a method for creating a backdoor
in a white-box LLM by injecting an uncensoring vector into its computational
graph representation. We set a trigger phrase that, when added to the beginning
of a prompt into the LLM, applies the uncensoring vector and removes the
content generation safeguards in the model. We embed trigger logic directly
into the computational graph which detects the trigger phrase in a prompt. To
evade detection of our backdoor, we obfuscate this logic within the graph
structure, making it similar to standard model functions. Our method requires
minimal alterations to model parameters, making backdoored models appear benign
while retaining the ability to generate uncensored responses when activated. We
successfully implement ShadowLogic in Phi-3 and Llama 3.2, using ONNX for
manipulating computational graphs. Implanting the uncensoring vector achieved a
>60% attack success rate for further malicious queries.

</details>


### [453] [EP-HDC: Hyperdimensional Computing with Encrypted Parameters for High-Throughput Privacy-Preserving Inference](https://arxiv.org/abs/2511.00737)
*Jaewoo Park,Chenghao Quan,Jongeun Lee*

Main category: cs.CR

TL;DR: 本文提出EP - HDC方法解决HDC - based HE在批推理场景的高计算时间、加密和数据传输开销问题，实验证明该方法提升了批推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有HDC应用于HE在批推理等现实场景存在计算时间长、加密和数据传输开销高的问题，需改进。

Method: 提出带有加密参数的HDC（EP - HDC），是一种客户端HE的隐私保护机器学习方法，还进行了涉及量化、架构和HE相关参数的设计空间探索。

Result: 使用BFV方案和Face/Emotion数据集实验表明，相比之前PPML方法，该方法将批推理吞吐量和延迟提升了几个数量级（分别为36.52~1068x和6.45~733x），精度下降少于1%。

Conclusion: EP - HDC能有效减轻加密和数据传输开销，具有高可扩展性，能为用户数据和模型参数提供强保护，提升批推理性能。

Abstract: While homomorphic encryption (HE) provides strong privacy protection, its
high computational cost has restricted its application to simple tasks.
Recently, hyperdimensional computing (HDC) applied to HE has shown promising
performance for privacy-preserving machine learning (PPML). However, when
applied to more realistic scenarios such as batch inference, the HDC-based HE
has still very high compute time as well as high encryption and data
transmission overheads. To address this problem, we propose HDC with encrypted
parameters (EP-HDC), which is a novel PPML approach featuring client-side HE,
i.e., inference is performed on a client using a homomorphically encrypted
model. Our EP-HDC can effectively mitigate the encryption and data transmission
overhead, as well as providing high scalability with many clients while
providing strong protection for user data and model parameters. In addition to
application examples for our client-side PPML, we also present design space
exploration involving quantization, architecture, and HE-related parameters.
Our experimental results using the BFV scheme and the Face/Emotion datasets
demonstrate that our method can improve throughput and latency of batch
inference by orders of magnitude over previous PPML methods (36.52~1068x and
6.45~733x, respectively) with less than 1% accuracy degradation.

</details>


### [454] [Towards Ultra-Low Latency: Binarized Neural Network Architectures for In-Vehicle Network Intrusion Detection](https://arxiv.org/abs/2511.00828)
*Huiyao Dong,Igor Kotenko*

Main category: cs.CR

TL;DR: 本文提出基于二值化神经网络的轻量级入侵检测技术，结合混合二进制编码，适用于微控制器和网关ECU，能进行异常检测和多类网络流量分类。


<details>
  <summary>Details</summary>
Motivation: CAN协议缺乏安全特性，车辆易受网络攻击，现有机器学习和深度学习技术实用性不确定。

Method: 提出基于二值化神经网络（BNNs）的轻量级入侵检测技术，利用负载数据、消息ID和CAN消息频率进行检测，开发混合二进制编码技术集成非二进制特征。

Result: 所提方法在异常检测和多类网络流量分类中有效。

Conclusion: 该系统适合部署在微控制器和网关ECU上，满足CAN总线安全应用的实时要求。

Abstract: The Control Area Network (CAN) protocol is essential for in-vehicle
communication, facilitating high-speed data exchange among Electronic Control
Units (ECUs). However, its inherent design lacks robust security features,
rendering vehicles susceptible to cyberattacks. While recent research has
investigated machine learning and deep learning techniques to enhance network
security, their practical applicability remains uncertain. This paper presents
a lightweight intrusion detection technique based on Binarized Neural Networks
(BNNs), which utilizes payload data, message IDs, and CAN message frequencies
for effective intrusion detection. Additionally, we develop hybrid binary
encoding techniques to integrate non-binary features, such as message IDs and
frequencies. The proposed method, namely the BNN framework specifically
optimized for in-vehicle intrusion detection combined with hybrid binary
quantization techniques for non-payload attributes, demonstrates efficacy in
both anomaly detection and multi-class network traffic classification. The
system is well-suited for deployment on micro-controllers and Gateway ECUs,
aligning with the real-time requirements of CAN bus safety applications.

</details>


### [455] [Android Malware Detection: A Machine Leaning Approach](https://arxiv.org/abs/2511.00894)
*Hasan Abdulla*

Main category: cs.CR

TL;DR: 研究用多种机器学习技术检测安卓恶意软件，发现集成方法性能优但有取舍，为后续研究和应用提供指导。


<details>
  <summary>Details</summary>
Motivation: 应对安卓恶意软件日益增长的威胁，研究用机器学习技术检测安卓恶意软件。

Method: 使用决策树、支持向量机、逻辑回归、神经网络和集成方法等机器学习技术，在安卓应用数据集上评估模型。

Result: 集成方法表现出优越性能，但模型可解释性、效率和准确性之间存在权衡。

Conclusion: 研究结果为未来利用机器学习对抗安卓恶意软件的研究和实际应用提供了指导。

Abstract: This study examines machine learning techniques like Decision Trees, Support
Vector Machines, Logistic Regression, Neural Networks, and ensemble methods to
detect Android malware. The study evaluates these models on a dataset of
Android applications and analyzes their accuracy, efficiency, and real-world
applicability. Key findings show that ensemble methods demonstrate superior
performance, but there are trade-offs between model interpretability,
efficiency, and accuracy. Given its increasing threat, the insights guide
future research and practical use of ML to combat Android malware.

</details>


### [456] [Keys in the Weights: Transformer Authentication Using Model-Bound Latent Representations](https://arxiv.org/abs/2511.00973)
*Ayşe S. Okatan,Mustafa İlhan Akbaş,Laxima Niure Kandel,Berker Peköz*

Main category: cs.CR

TL;DR: 提出MoBLE，其有ZSDN特性，在身份任务中有良好表现，可作为认证和访问控制机制，介绍了相关定义、部署考虑、风险及缓解措施，适用于安全关键领域。


<details>
  <summary>Details</summary>
Motivation: 为安全关键领域的人工智能管道提供安全的部署方法。

Method: 引入Model - Bound Latent Exchange (MoBLE)，定义Zero - Shot Decoder Non - Transferability (ZSDN)，通过身份任务实验，结合权重空间距离和注意力发散诊断进行分析。

Result: 在身份任务中，自解码有高准确率，零样本交叉解码效果差，分离效果无需注入秘密或对抗训练，ZSDN可作为基于潜在的认证和访问控制机制。

Conclusion: MoBLE是一种轻量级、适合加速器的方法，可用于安全关键领域的安全AI部署。

Abstract: We introduce Model-Bound Latent Exchange (MoBLE), a decoder-binding property
in Transformer autoencoders formalized as Zero-Shot Decoder Non-Transferability
(ZSDN). In identity tasks using iso-architectural models trained on identical
data but differing in seeds, self-decoding achieves more than 0.91 exact match
and 0.98 token accuracy, while zero-shot cross-decoding collapses to chance
without exact matches. This separation arises without injected secrets or
adversarial training, and is corroborated by weight-space distances and
attention-divergence diagnostics. We interpret ZSDN as model binding, a
latent-based authentication and access-control mechanism, even when the
architecture and training recipe are public: encoder's hidden state
representation deterministically reveals the plaintext, yet only the correctly
keyed decoder reproduces it in zero-shot. We formally define ZSDN, a
decoder-binding advantage metric, and outline deployment considerations for
secure artificial intelligence (AI) pipelines. Finally, we discuss learnability
risks (e.g., adapter alignment) and outline mitigations. MoBLE offers a
lightweight, accelerator-friendly approach to secure AI deployment in
safety-critical domains, including aviation and cyber-physical systems.

</details>


### [457] [AthenaBench: A Dynamic Benchmark for Evaluating LLMs in Cyber Threat Intelligence](https://arxiv.org/abs/2511.01144)
*Md Tanvirul Alam,Dipkamal Bhusal,Salman Ahmad,Nidhi Rastogi,Peter Worth*

Main category: cs.CR

TL;DR: 研究基于CTIBench开发AthenaBench评估12个大模型在CTI任务表现，发现当前大模型推理能力有局限，需适配CTI工作流的模型。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自然语言推理能力强，但在CTI应用有限，CTIBench可评估LLMs在CTI任务表现，本文旨在对其进行扩展。

Method: 开发AthenaBench，包含改进数据集创建流程、去重、优化评估指标和新增风险缓解策略任务，评估12个LLMs。

Result: 专有大模型整体表现更好，但在推理密集型任务表现不佳，开源模型更差。

Conclusion: 当前大模型推理能力有根本局限，需要专门适配CTI工作流和自动化的模型。

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in natural
language reasoning, yet their application to Cyber Threat Intelligence (CTI)
remains limited. CTI analysis involves distilling large volumes of unstructured
reports into actionable knowledge, a process where LLMs could substantially
reduce analyst workload. CTIBench introduced a comprehensive benchmark for
evaluating LLMs across multiple CTI tasks. In this work, we extend CTIBench by
developing AthenaBench, an enhanced benchmark that includes an improved dataset
creation pipeline, duplicate removal, refined evaluation metrics, and a new
task focused on risk mitigation strategies. We evaluate twelve LLMs, including
state-of-the-art proprietary models such as GPT-5 and Gemini-2.5 Pro, alongside
seven open-source models from the LLaMA and Qwen families. While proprietary
LLMs achieve stronger results overall, their performance remains subpar on
reasoning-intensive tasks, such as threat actor attribution and risk
mitigation, with open-source models trailing even further behind. These
findings highlight fundamental limitations in the reasoning capabilities of
current LLMs and underscore the need for models explicitly tailored to CTI
workflows and automation.

</details>


### [458] [Black-Box Differentially Private Nonparametric Confidence Intervals Under Minimal Assumptions](https://arxiv.org/abs/2511.01303)
*Tomer Shoham,Moshe Shenfeld,Noa Velner-Harris,Katrina Ligett*

Main category: cs.CR

TL;DR: 提出通用框架，用差分隐私估计器构造非参数置信区间，方法有效且表现良好。


<details>
  <summary>Details</summary>
Motivation: 构造任意差分隐私估计量对应的差分隐私非参数置信区间。

Method: 对数据重复子采样，将私有估计器应用于每个子样本，对经验CDF后处理得到置信区间，利用子采样随机性实现隐私放大。

Result: 得到的经验CDF随样本量增长趋近于私有统计量的CDF，估计的置信区间渐近有效、紧密且与非私有对应物等效，实证表明方法表现优于现有算法。

Conclusion: 所提出的框架简单通用，能有效构造差分隐私非参数置信区间。

Abstract: We introduce a simple, general framework that takes any differentially
private estimator of any arbitrary quantity as a black box, and from it
constructs a differentially private nonparametric confidence interval of that
quantity. Our approach repeatedly subsamples the data, applies the private
estimator to each subsample, and then post-processes the resulting empirical
CDF to a confidence interval. Our analysis uses the randomness from the
subsampling to achieve privacy amplification. Under mild assumptions, the
empirical CDF we obtain approaches the CDF of the private statistic as the
sample size grows. We use this to show that the confidence intervals we
estimate are asymptotically valid, tight, and equivalent to their non-private
counterparts. We provide empirical evidence that our method performs well
compared with the (less-general) state-of-the-art algorithms.

</details>


### [459] [Prompt Injection as an Emerging Threat: Evaluating the Resilience of Large Language Models](https://arxiv.org/abs/2511.01634)
*Daniyal Ganiuly,Assel Smaiyl*

Main category: cs.CR

TL;DR: 本文提出评估大语言模型对抗提示注入攻击抗性的统一框架，评估四个模型在五项任务的表现，发现GPT - 4总体最佳，开源模型更脆弱，强调对齐和安全调优比模型大小更重要。


<details>
  <summary>Details</summary>
Motivation: 大语言模型易受提示注入攻击，需评估其对抗此类攻击的抗性。

Method: 提出统一框架，定义三个互补指标（RDI、SCC、IIM），评估四个指令调优模型在五项常见语言任务的表现。

Result: GPT - 4总体表现最佳，开源模型更易受攻击，所有模型仍部分脆弱，尤其对间接和直接覆盖攻击。

Conclusion: 对齐强度和安全调优对模型抗性的作用比模型大小更重要，框架为评估模型鲁棒性提供方法，对提升LLM安全性和可靠性有实际意义。

Abstract: Large Language Models (LLMs) are increasingly used in intelligent systems
that perform reasoning, summarization, and code generation. Their ability to
follow natural-language instructions, while powerful, also makes them
vulnerable to a new class of attacks known as prompt injection. In these
attacks, hidden or malicious instructions are inserted into user inputs or
external content, causing the model to ignore its intended task or produce
unsafe responses. This study proposes a unified framework for evaluating how
resistant Large Language Models (LLMs) are to prompt injection attacks. The
framework defines three complementary metrics such as the Resilience
Degradation Index (RDI), Safety Compliance Coefficient (SCC), and Instructional
Integrity Metric (IIM) to jointly measure robustness, safety, and semantic
stability. We evaluated four instruction-tuned models (GPT-4, GPT-4o, LLaMA-3
8B Instruct, and Flan-T5-Large) on five common language tasks: question
answering, summarization, translation, reasoning, and code generation. Results
show that GPT-4 performs best overall, while open-weight models remain more
vulnerable. The findings highlight that strong alignment and safety tuning are
more important for resilience than model size alone. Results show that all
models remain partially vulnerable, especially to indirect and direct-override
attacks. GPT-4 achieved the best overall resilience (RDR = 9.8 %, SCR = 96.4
%), while open-source models exhibited higher performance degradation and lower
safety scores. The findings demonstrate that alignment strength and safety
tuning play a greater role in resilience than model size alone. The proposed
framework offers a structured, reproducible approach for assessing model
robustness and provides practical insights for improving LLM safety and
reliability.

</details>


### [460] [Panther: A Cost-Effective Privacy-Preserving Framework for GNN Training and Inference Services in Cloud Environments](https://arxiv.org/abs/2511.01654)
*Congcong Chen,Xinyu Liu,Kaifeng Huang,Lifei Wei,Yang Shi*

Main category: cs.CR

TL;DR: 现有云环境下GNN隐私保护技术开销大，本文提出Panther框架，能降低训练和推理时间、通信开销及财务成本。


<details>
  <summary>Details</summary>
Motivation: 随着用户向云计算迁移，在云环境中保护GNN隐私并降低云计算财务成本成为关键问题，现有技术开销高限制其广泛应用。

Method: 引入Panther框架，利用四方计算异步执行安全数组访问协议，随机填充GNN节点邻居信息。

Result: 与现有技术相比，Panther平均减少75.28%的训练时间和82.80%的推理时间，平均降低52.61%和50.26%的通信开销，在谷歌云平台上GNN训练和推理过程预计平均节省55.05%和59.00%的财务成本。

Conclusion: Panther能有效保护GNN隐私，同时降低额外财务成本。

Abstract: Graph Neural Networks (GNNs) have marked significant impact in traffic state
prediction, social recommendation, knowledge-aware question answering and so
on. As more and more users move towards cloud computing, it has become a
critical issue to unleash the power of GNNs while protecting the privacy in
cloud environments. Specifically, the training data and inference data for GNNs
need to be protected from being stolen by external adversaries. Meanwhile, the
financial cost of cloud computing is another primary concern for users.
Therefore, although existing studies have proposed privacy-preserving
techniques for GNNs in cloud environments, their additional computational and
communication overhead remain relatively high, causing high financial costs
that limit their widespread adoption among users.
  To protect GNN privacy while lowering the additional financial costs, we
introduce Panther, a cost-effective privacy-preserving framework for GNN
training and inference services in cloud environments. Technically, Panther
leverages four-party computation to asynchronously executing the secure array
access protocol, and randomly pads the neighbor information of GNN nodes. We
prove that Panther can protect privacy for both training and inference of GNN
models. Our evaluation shows that Panther reduces the training and inference
time by an average of 75.28% and 82.80%, respectively, and communication
overhead by an average of 52.61% and 50.26% compared with the state-of-the-art,
which is estimated to save an average of 55.05% and 59.00% in financial costs
(based on on-demand pricing model) for the GNN training and inference process
on Google Cloud Platform.

</details>


### [461] [Scam Shield: Multi-Model Voting and Fine-Tuned LLMs Against Adversarial Attacks](https://arxiv.org/abs/2511.01746)
*Chen-Wei Chang,Shailik Sarkar,Hossein Salemi,Hyungmin Kim,Shutonu Mitra,Hemant Purohit,Fengxiu Zhang,Michin Hong,Jin-Hee Cho,Chang-Tien Lu*

Main category: cs.CR

TL;DR: 提出分层诈骗检测系统HSDS，结合多模型投票前端与微调的LLaMA 3.1 8B Instruct后端，实验表明能提升对抗诈骗检测效果并缩短推理时间。


<details>
  <summary>Details</summary>
Motivation: 诈骗检测是网络安全的关键挑战，对手设计的消息可绕过自动过滤器，需提升检测准确性和鲁棒性。

Method: 构建HSDS，前端用四个分类器集成进行多数投票初步预测，模糊情况交给后端微调模型，该模型经对抗训练优化。

Result: 分层设计提升对抗诈骗检测效果，缩短推理时间，优于传统机器学习和专有大语言模型基线。

Conclusion: 混合投票机制和对抗微调在强化大语言模型抵御诈骗策略方面有效，能增强自动诈骗检测系统的弹性。

Abstract: Scam detection remains a critical challenge in cybersecurity as adversaries
craft messages that evade automated filters. We propose a Hierarchical Scam
Detection System (HSDS) that combines a lightweight multi-model voting front
end with a fine-tuned LLaMA 3.1 8B Instruct back end to improve accuracy and
robustness against adversarial attacks. An ensemble of four classifiers
provides preliminary predictions through majority vote, and ambiguous cases are
escalated to the fine-tuned model, which is optimized with adversarial training
to reduce misclassification. Experiments show that this hierarchical design
both improves adversarial scam detection and shortens inference time by routing
most cases away from the LLM, outperforming traditional machine-learning
baselines and proprietary LLM baselines. The findings highlight the
effectiveness of a hybrid voting mechanism and adversarial fine-tuning in
fortifying LLMs against evolving scam tactics, enhancing the resilience of
automated scam detection systems.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [462] [RailEstate: An Interactive System for Metro Linked Property Trends](https://arxiv.org/abs/2511.00078)
*Chen-Wei Chang,Yu-Chieh Cheng,Yun-En Tsai,Fanglan Chen,Chang-Tien Lu*

Main category: cs.CY

TL;DR: 提出基于网络的RailEstate系统分析华盛顿都会区地铁站对房价的影响，结合历史数据支持多种功能，含自然语言聊天机器人，方便不同人群获取见解。


<details>
  <summary>Details</summary>
Motivation: 研究地铁系统可达性对城市住房市场的影响，为相关人员提供分析工具。

Method: 构建RailEstate系统，整合空间分析、自然语言接口和交互式预测，结合25年历史住房数据与交通基础设施。

Result: 系统支持低延迟地理空间查询、时间序列可视化和预测建模，用户可交互探索房价模式、趋势和预测未来房价。

Conclusion: 该统一交互式平台使城市规划者、投资者和居民无需技术专长即可从地铁相关住房数据中获取可行见解。

Abstract: Access to metro systems plays a critical role in shaping urban housing
markets by enhancing neighborhood accessibility and driving property demand. We
present RailEstate, a novel web based system that integrates spatial analytics,
natural language interfaces, and interactive forecasting to analyze how
proximity to metro stations influences residential property prices in the
Washington metropolitan area. Unlike static mapping tools or generic listing
platforms, RailEstate combines 25 years of historical housing data with transit
infrastructure to support low latency geospatial queries, time series
visualizations, and predictive modeling. Users can interactively explore ZIP
code level price patterns, investigate long term trends, and forecast future
housing values around any metro station. A key innovation is our natural
language chatbot, which translates plain-English questions e.g., What is the
highest price in Falls Church in the year 2000? into executable SQL over a
spatial database. This unified and interactive platform empowers urban
planners, investors, and residents to derive actionable insights from metro
linked housing data without requiring technical expertise.

</details>


### [463] [Forecasting Occupational Survivability of Rickshaw Pullers in a Changing Climate with Wearable Data](https://arxiv.org/abs/2511.00081)
*Masfiqur Rahaman,Maoyejatun Hasana,Shahad Shahriar Rahman,MD Sajid Mostafiz Noor,Razin Reaz Abedin,Md Toki Tahmid,Duncan Watson Parris,Tanzeem Choudhury,A. B. M. Alim Al Islam,Tauhidur Rahman*

Main category: cs.CY

TL;DR: 研究收集达卡人力车夫生理和天气数据，用LGBN模型预测生理指标，结合气候模型分析其在当前和未来高温暴露风险，访谈显示车夫意识到气候脆弱性。


<details>
  <summary>Details</summary>
Motivation: 人力车夫易受极端高温影响，但对其生理指标响应了解甚少，需开展研究。

Method: 用可穿戴设备收集100名车夫实时天气和生理数据，访谈12名车夫；构建LGBN回归模型预测生理指标；结合18个CMIP6气候模型分析生存能力；进行访谈主题分析。

Result: LGBN模型对皮肤温度等指标预测的归一化平均绝对误差分别为0.82、0.47、0.65和0.67；32%车夫面临高暴露风险，2026 - 2030年升至37%；访谈表明车夫意识到气候脆弱性。

Conclusion: 人力车夫面临高温暴露风险增加，他们已认识到气候脆弱性对健康和职业生存的影响。

Abstract: Cycle rickshaw pullers are highly vulnerable to extreme heat, yet little is
known about how their physiological biomarkers respond under such conditions.
This study collected real-time weather and physiological data using wearable
sensors from 100 rickshaw pullers in Dhaka, Bangladesh. In addition, interviews
with 12 pullers explored their knowledge, perceptions, and experiences related
to climate change. We developed a Linear Gaussian Bayesian Network (LGBN)
regression model to predict key physiological biomarkers based on activity,
weather, and demographic features. The model achieved normalized mean absolute
error values of 0.82, 0.47, 0.65, and 0.67 for skin temperature, relative
cardiac cost, skin conductance response, and skin conductance level,
respectively. Using projections from 18 CMIP6 climate models, we layered the
LGBN on future climate forecasts to analyze survivability for current
(2023-2025) and future years (2026-2100). Based on thresholds of WBGT above
31.1{\deg}C and skin temperature above 35{\deg}C, 32% of rickshaw pullers
already face high heat exposure risk. By 2026-2030, this percentage may rise to
37% with average exposure lasting nearly 12 minutes, or about two-thirds of the
trip duration. A thematic analysis of interviews complements these findings,
showing that rickshaw pullers recognize their increasing climate vulnerability
and express concern about its effects on health and occupational survivability.

</details>


### [464] [Multimodal Learning with Augmentation Techniques for Natural Disaster Assessment](https://arxiv.org/abs/2511.00004)
*Adrian-Dinu Urse,Dumitru-Clementin Cercel,Florin Pop*

Main category: cs.CY

TL;DR: 本文探索在CrisisMMD多模态数据集上使用增强技术解决现有数据集类别不平衡和样本有限问题，评估了多种增强方法在不同学习设置下的效果，结果表明所选增强方法能提升分类性能，尤其对少数类，多视图学习有潜力但需完善。


<details>
  <summary>Details</summary>
Motivation: 自然灾害评估需准确快速获取信息，社交媒体是有价值的实时信息源，但现有数据集存在类别不平衡和样本有限问题，难以有效开发模型。

Method: 对视觉数据应用基于扩散的方法（Real Guidance和DiffuseMix），对文本数据探索回译、基于Transformer的释义和基于图像字幕的增强方法，并在单模态、多模态和多视图学习设置下进行评估。

Result: 所选增强方法提高了分类性能，特别是对少数类，多视图学习有潜力但需进一步完善。

Conclusion: 本研究强调了构建更强大的灾害评估系统的有效增强策略。

Abstract: Natural disaster assessment relies on accurate and rapid access to
information, with social media emerging as a valuable real-time source.
However, existing datasets suffer from class imbalance and limited samples,
making effective model development a challenging task. This paper explores
augmentation techniques to address these issues on the CrisisMMD multimodal
dataset. For visual data, we apply diffusion-based methods, namely Real
Guidance and DiffuseMix. For text data, we explore back-translation,
paraphrasing with transformers, and image caption-based augmentation. We
evaluated these across unimodal, multimodal, and multi-view learning setups.
Results show that selected augmentations improve classification performance,
particularly for underrepresented classes, while multi-view learning introduces
potential but requires further refinement. This study highlights effective
augmentation strategies for building more robust disaster assessment systems.

</details>


### [465] [Chitchat with AI: Understand the supply chain carbon disclosure of companies worldwide through Large Language Model](https://arxiv.org/abs/2511.00024)
*Haotian Hang,Yueyang Shen,Vicky Zhu,Jose Cruz,Michelle Li*

Main category: cs.CY

TL;DR: 本文提出利用大语言模型评估企业气候披露质量的决策支持框架，处理CDP数据并揭示不同行业和地区披露情况，推动气候治理中AI决策支持系统发展。


<details>
  <summary>Details</summary>
Motivation: 企业碳披露在全球可持续性要求下至关重要，但CDP数据的异质性和自由形式给分析带来挑战，需要有效评估方法。

Method: 提出利用大语言模型的决策支持框架，开发主评分规则，整合规则引导评分和基于百分位的标准化方法。

Result: 发现科技等行业和德国等国家在评分规则上一致性较高，其他行业和地区存在波动或表面参与情况。

Conclusion: 基于大语言模型的方法将非结构化披露转化为可量化、可解释、可比和可操作的信息，提升气候治理中AI决策支持系统能力。

Abstract: In the context of global sustainability mandates, corporate carbon disclosure
has emerged as a critical mechanism for aligning business strategy with
environmental responsibility. The Carbon Disclosure Project (CDP) hosts the
world's largest longitudinal dataset of climate-related survey responses,
combining structured indicators with open-ended narratives, but the
heterogeneity and free-form nature of these disclosures present significant
analytical challenges for benchmarking, compliance monitoring, and investment
screening. This paper proposes a novel decision-support framework that
leverages large language models (LLMs) to assess corporate climate disclosure
quality at scale. It develops a master rubric that harmonizes narrative scoring
across 11 years of CDP data (2010-2020), enabling cross-sector and
cross-country benchmarking. By integrating rubric-guided scoring with
percentile-based normalization, our method identifies temporal trends,
strategic alignment patterns, and inconsistencies in disclosure across
industries and regions. Results reveal that sectors such as technology and
countries like Germany consistently demonstrate higher rubric alignment, while
others exhibit volatility or superficial engagement, offering insights that
inform key decision-making processes for investors, regulators, and corporate
environmental, social, and governance (ESG) strategists. The proposed LLM-based
approach transforms unstructured disclosures into quantifiable, interpretable,
comparable, and actionable intelligence, advancing the capabilities of
AI-enabled decision support systems (DSSs) in the domain of climate governance.

</details>


### [466] [Position Paper: If Innovation in AI Systematically Violates Fundamental Rights, Is It Innovation at All?](https://arxiv.org/abs/2511.00027)
*Josu Eguiluz Castañeira,Axel Brando,Migle Laukyte,Marc Serra-Vidal*

Main category: cs.CY

TL;DR: 本文挑战监管与创新对立的观点，以欧盟AI法案为例，说明合理监管是创新基础，二者可共同推进。


<details>
  <summary>Details</summary>
Motivation: 针对人工智能在关键基础设施和决策系统应用中，缺乏良好监管已造成损害，挑战监管与创新对立的固有观念。

Method: 通过航空、制药、福利系统类比以及合成虚假信息等案例论证，分析欧盟AI法案的自适应机制。

Result: 治理工具能将监管负担转化为法律确定性、消费者信任和道德竞争力等优势。

Conclusion: 创新和监管应共同推进，欧盟框架定义了负责任创新的含义。

Abstract: Artificial intelligence (AI) now permeates critical infrastructures and
decision-making systems where failures produce social, economic, and democratic
harm. This position paper challenges the entrenched belief that regulation and
innovation are opposites. As evidenced by analogies from aviation,
pharmaceuticals, and welfare systems and recent cases of synthetic
misinformation, bias and unaccountable decision-making, the absence of
well-designed regulation has already created immeasurable damage. Regulation,
when thoughtful and adaptive, is not a brake on innovation--it is its
foundation. The present position paper examines the EU AI Act as a model of
risk-based, responsibility-driven regulation that addresses the Collingridge
Dilemma: acting early enough to prevent harm, yet flexibly enough to sustain
innovation. Its adaptive mechanisms--regulatory sandboxes, small and medium
enterprises (SMEs) support, real-world testing, fundamental rights impact
assessment (FRIA) -- demonstrate how regulation can accelerate responsibly,
rather than delay, technological progress. The position paper summarises how
governance tools transform perceived burdens into tangible advantages: legal
certainty, consumer trust, and ethical competitiveness. Ultimately, the paper
reframes progress: innovation and regulation advance together. By embedding
transparency, impact assessments, accountability, and AI literacy into design
and deployment, the EU framework defines what responsible innovation truly
means--technological ambition disciplined by democratic values and fundamental
rights.

</details>


### [467] [Artificial Intelligence in Elementary STEM Education: A Systematic Review of Current Applications and Future Challenges](https://arxiv.org/abs/2511.00105)
*Majid Memari,Krista Ruggles*

Main category: cs.CY

TL;DR: 本文系统回顾2020 - 2025年258项研究，分析AI在初等STEM教育应用，指出研究集中年级、学科及存在的八大差距，给出未来方向。


<details>
  <summary>Details</summary>
Motivation: AI正改变初等STEM教育，但证据零散，需系统回顾相关研究。

Method: 对258项研究进行系统回顾分析，涉及AI应用的八个类别。

Result: 多数研究集中于高年级和数学，跨学科整合少；对话式AI有一定效果但部分缺效应量；存在八大差距，研究地域分布不均。

Conclusion: 未来需可互操作架构、适龄设计、隐私分析及以教师为中心的实施。

Abstract: Artificial intelligence (AI) is transforming elementary STEM education, yet
evidence remains fragmented. This systematic review synthesizes 258 studies
(2020-2025) examining AI applications across eight categories: intelligent
tutoring systems (45% of studies), learning analytics (18%), automated
assessment (12%), computer vision (8%), educational robotics (7%), multimodal
sensing (6%), AI-enhanced extended reality (XR) (4%), and adaptive content
generation. The analysis shows that most studies focus on upper elementary
grades (65%) and mathematics (38%), with limited cross-disciplinary STEM
integration (15%). While conversational AI demonstrates moderate effectiveness
(d = 0.45-0.70 where reported), only 34% of studies include standardized effect
sizes. Eight major gaps limit real-world impact: fragmented ecosystems,
developmental inappropriateness, infrastructure barriers, lack of privacy
frameworks, weak STEM integration, equity disparities, teacher marginalization,
and narrow assessment scopes. Geographic distribution is also uneven, with 90%
of studies originating from North America, East Asia, and Europe. Future
directions call for interoperable architectures that support authentic STEM
integration, grade-appropriate design, privacy-preserving analytics, and
teacher-centered implementations that enhance rather than replace human
expertise.

</details>


### [468] [Wayfinding through the AI wilderness: Mapping rhetorics of ChatGPT prompt writing on X (formerly Twitter) to promote critical AI literacies](https://arxiv.org/abs/2511.00106)
*Anuj Gupta,Ann Shivers-McNair*

Main category: cs.CY

TL;DR: 研究社交媒体上ChatGPT提示写作的修辞可促进批判性AI素养，介绍研究过程并呈现五个新兴AI素养实践主题，为数字写作教师和研究者提供启示。


<details>
  <summary>Details</summary>
Motivation: 探讨研究社交媒体上ChatGPT提示写作的修辞对促进批判性AI素养的作用。

Method: 基于计算机与写作领域的四个数字写作研究传统，收集并分析2022年11月至2023年5月X平台上32,000条关于提示写作的帖子。

Result: 呈现了关于新兴AI素养实践的五个主题，包括提示写作影响的交流领域、共享的微素养资源、塑造提示写作的市场修辞、提示的修辞特征和提示写作的定义。

Conclusion: 为数字写作教师和研究者在教学和分析批判性AI素养方面提供了有价值的参考。

Abstract: In this paper, we demonstrate how studying the rhetorics of ChatGPT prompt
writing on social media can promote critical AI literacies. Prompt writing is
the process of writing instructions for generative AI tools like ChatGPT to
elicit desired outputs and there has been an upsurge of conversations about it
on social media. To study this rhetorical activity, we build on four
overlapping traditions of digital writing research in computers and composition
that inform how we frame literacies, how we study social media rhetorics, how
we engage iteratively and reflexively with methodologies and technologies, and
how we blend computational methods with qualitative methods. Drawing on these
four traditions, our paper shows our iterative research process through which
we gathered and analyzed a dataset of 32,000 posts (formerly known as tweets)
from X (formerly Twitter) about prompt writing posted between November 2022 to
May 2023. We present five themes about these emerging AI literacy practices:
(1) areas of communication impacted by prompt writing, (2) micro-literacy
resources shared for prompt writing, (3) market rhetoric shaping prompt
writing, (4) rhetorical characteristics of prompts, and (5) definitions of
prompt writing. In discussing these themes and our methodologies, we highlight
takeaways for digital writing teachers and researchers who are teaching and
analyzing critical AI literacies.

</details>


### [469] [A Detailed Study on LLM Biases Concerning Corporate Social Responsibility and Green Supply Chains](https://arxiv.org/abs/2511.01840)
*Greta Ontrup,Annika Bush,Markus Pauly,Meltem Aksoy*

Main category: cs.CY

TL;DR: 研究调查不同大语言模型对可持续商业问卷的响应，分析模型差异及组织文化影响，对可持续决策有重要意义。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在供应链中使用广泛，但存在可持续商业策略优先级的偏见，需识别其训练数据中的偏见。

Method: 使用标准化问卷，系统分析先进大语言模型的响应，评估四种组织文化类型对差异的影响。

Result: 模型间存在显著系统差异，组织文化提示会显著改变大语言模型的响应。

Conclusion: 研究对可持续背景下大语言模型辅助决策有重要启示。

Abstract: Organizations increasingly use Large Language Models (LLMs) to improve supply
chain processes and reduce environmental impacts. However, LLMs have been shown
to reproduce biases regarding the prioritization of sustainable business
strategies. Thus, it is important to identify underlying training data biases
that LLMs pertain regarding the importance and role of sustainable business and
supply chain practices. This study investigates how different LLMs respond to
validated surveys about the role of ethics and responsibility for businesses,
and the importance of sustainable practices and relations with suppliers and
customers. Using standardized questionnaires, we systematically analyze
responses generated by state-of-the-art LLMs to identify variations. We further
evaluate whether differences are augmented by four organizational culture
types, thereby evaluating the practical relevance of identified biases. The
findings reveal significant systematic differences between models and
demonstrate that organizational culture prompts substantially modify LLM
responses. The study holds important implications for LLM-assisted
decision-making in sustainability contexts.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [470] [Efficient Vector Symbolic Architectures from Histogram Recovery](https://arxiv.org/abs/2511.01838)
*Zirui Deng,Netanel Raviv*

Main category: cs.IT

TL;DR: 本文利用编码理论工具，提出用里德 - 所罗门码和哈达玛码的级联实现抗噪声的向量符号架构，解决了随机线性码在噪声下难解码问题。


<details>
  <summary>Details</summary>
Motivation: 随机线性码在噪声下难以解码，限制了向量符号架构（VSA）的信息恢复能力，需找到合适编码解决该问题。

Method: 利用里德 - 所罗门码和哈达玛码的级联，将恢复问题转化为直方图恢复问题并给出最优解。

Result: 得到了具有高效编码、准正交性和恢复能力的抗噪声 VSA，相比哈达玛码等有改进。

Conclusion: 不依赖启发式方法或训练，能在更好参数下实现抗噪声的 VSA。

Abstract: Vector symbolic architectures (VSAs) are a family of information
representation techniques which enable composition, i.e., creating complex
information structures from atomic vectors via binding and superposition, and
have recently found wide ranging applications in various neurosymbolic
artificial intelligence (AI) systems. Recently, Raviv proposed the use of
random linear codes in VSAs, suggesting that their subcode structure enables
efficient binding, while preserving the quasi-orthogonality that is necessary
for neural processing. Yet, random linear codes are difficult to decode under
noise, which severely limits the resulting VSA's ability to support recovery,
i.e., the retrieval of information objects and their attributes from a noisy
compositional representation.
  In this work we bridge this gap by utilizing coding theoretic tools. First,
we argue that the concatenation of Reed-Solomon and Hadamard codes is suitable
for VSA, due to the mutual quasi-orthogonality of the resulting codewords (a
folklore result). Second, we show that recovery of the resulting compositional
representations can be done by solving a problem we call histogram recovery. In
histogram recovery, a collection of $N$ histograms over a finite field is given
as input, and one must find a collection of Reed-Solomon codewords of length
$N$ whose entry-wise symbol frequencies obey those histograms. We present an
optimal solution to the histogram recovery problem by using algorithms related
to list-decoding, and analyze the resulting noise resilience. Our results give
rise to a noise-resilient VSA with formal guarantees regarding efficient
encoding, quasi-orthogonality, and recovery, without relying on any heuristics
or training, and while operating at improved parameters relative to similar
solutions such as the Hadamard code.

</details>


### [471] [Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs](https://arxiv.org/abs/2511.01202)
*Bo Bai*

Main category: cs.IT

TL;DR: 本文从信息论角度研究大语言模型，提出语义信息理论框架，为理解和深入研究提供理论工具。


<details>
  <summary>Details</summary>
Motivation: 多数大语言模型研究从实验角度开展，需大量资源，因此要从理论角度打开其黑箱。

Method: 以率失真函数、有向信息和格兰杰因果关系理论为起点，定义大语言模型概率模型，探讨结构无关的信息论度量，研究词元级语义嵌入理论和最优向量化方法。

Result: 提出自回归大语言模型的一般定义，理论推导Transformer架构及其性能，还讨论了其他架构。

Conclusion: 本文从语义信息理论角度为理解大语言模型提供理论框架，为进一步研究提供必要理论工具。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
numerous real- world applications. While the vast majority of research
conducted from an experimental perspective is progressing rapidly, it demands
substantial computational power, data, and other resources. Therefore, how to
open the black-box of LLMs from a theoretical standpoint has become a critical
challenge. This paper takes the theory of rate-distortion function, directed
information, and Granger causality as its starting point to investigate the
information-theoretic principles behind LLMs, leading to the development of
semantic information theory for LLMs, where the fundamental unit is token,
rather than bits that lacks any semantic meaning. By defining the probabilistic
model of LLMs, we discuss structure-agnostic information-theoretic measures,
such as the directed rate- distortion function in pre-training, the directed
rate-reward function in post-training, and the semantic information flow in
inference phase. This paper also delves deeply into the theory of token-level
semantic embedding and the information-theoretically optimal vectorization
method. Thereafter, we propose a general definition of autoregression LLM,
where the Transformer architecture and its performance such as ELBO,
generalization error bound, memory capacity, and semantic information measures
can be derived theoretically. Other architectures, such as Mamba/Mamba2 and
LLaDA, are also discussed in our framework. Consequently, this paper provides a
theoretical framework for understanding LLMs from the perspective of semantic
information theory, which also offers the necessary theoretical tools for
further in-depth research.

</details>


### [472] [Transformer-Based Decoding in Concatenated Coding Schemes Under Synchronization Errors](https://arxiv.org/abs/2511.00999)
*Julian Streit,Franziska Weindel,Reinhard Heckel*

Main category: cs.IT

TL;DR: 本文引入基于Transformer的神经内解码器BCJRFormer，替换BP外解码器，提出ConvBCJRFormer架构，构建高效端到端解码管道。


<details>
  <summary>Details</summary>
Motivation: 现有BCJR算法在处理多个噪声副本时复杂度呈指数增长，难以重建码字，需更高效方法用于DNA数据存储。

Method: 引入BCJRFormer内解码器，用基于Transformer的解码器替换BP外解码器，提出ConvBCJRFormer架构。

Result: BCJRFormer在标记码单消息传输中错误率与BCJR算法相当，复杂度呈二次方增长；构建了高效端到端解码管道。

Conclusion: 所提方法构建了高效且性能良好的端到端解码管道，ConvBCJRFormer为更通用线性码类的联合内外解码迈出第一步。

Abstract: We consider the reconstruction of a codeword from multiple noisy copies that
are independently corrupted by insertions, deletions, and substitutions. This
problem arises, for example, in DNA data storage. A common code construction
uses a concatenated coding scheme that combines an outer linear block code with
an inner code, which can be either a nonlinear marker code or a convolutional
code. Outer decoding is done with Belief Propagation, and inner decoding is
done with the Bahl-Cocke-Jelinek-Raviv (BCJR) algorithm. However, the BCJR
algorithm scales exponentially with the number of noisy copies, which makes it
infeasible to reconstruct a codeword from more than about four copies. In this
work, we introduce BCJRFormer, a transformer-based neural inner decoder.
BCJRFormer achieves error rates comparable to the BCJR algorithm for binary and
quaternary single-message transmissions of marker codes. Importantly,
BCJRFormer scales quadratically with the number of noisy copies. This property
makes BCJRFormer well-suited for DNA data storage, where multiple reads of the
same DNA strand occur. To lower error rates, we replace the Belief Propagation
outer decoder with a transformer-based decoder. Together, these modifications
yield an efficient and performant end-to-end transformer-based pipeline for
decoding multiple noisy copies affected by insertion, deletion, and
substitution errors. Additionally, we propose a novel cross-attending
transformer architecture called ConvBCJRFormer. This architecture extends
BCJRFormer to decode transmissions of convolutional codewords, serving as an
initial step toward joint inner and outer decoding for more general linear code
classes.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [473] [Transfer learning discovery of molecular modulators for perovskite solar cells](https://arxiv.org/abs/2511.00204)
*Haoming Yan,Xinyu Chen,Yanran Wang,Zhengchao Luo,Weizheng Huang,Hongshuai Wang,Peng Chen,Yuzhi Zhang,Weijie Sun,Jinzhuo Wang,Qihuang Gong,Rui Zhu,Lichen Zhao*

Main category: cond-mat.mtrl-sci

TL;DR: 本文提出化学信息迁移学习框架预测分子调节剂对钙钛矿太阳能电池功率转换效率的影响，实现低成本高通量虚拟筛选，实验验证提升电池效率。


<details>
  <summary>Details</summary>
Motivation: 有效分子调节剂发现对钙钛矿太阳能电池发展重要，但化学空间大、实验筛选耗时昂贵，且机器学习应用于该领域存在数据稀缺和传统模型局限问题。

Method: 应用基于预训练深度神经网络的化学信息迁移学习框架，对多种分子表示进行系统基准测试，利用可解释性技术可视化化学表示并实验表征相互作用。

Result: 框架能高精度预测，实现对79,043种市售分子的低成本高通量虚拟筛选，实验验证得到的顶级分子调节剂使钙钛矿太阳能电池冠军功率转换效率达26.91%。

Conclusion: 所提出的化学信息迁移学习框架可有效加速钙钛矿太阳能电池分子调节剂的发现，提升电池效率。

Abstract: The discovery of effective molecular modulators is essential for advancing
perovskite solar cells (PSCs), but the research process is hindered by the
vastness of chemical space and the time-consuming and expensive trial-and-error
experimental screening. Concurrently, machine learning (ML) offers significant
potential for accelerating materials discovery. However, applying ML to PSCs
remains a major challenge due to data scarcity and limitations of traditional
quantitative structure-property relationship (QSPR) models. Here, we apply a
chemical informed transfer learning framework based on pre-trained deep neural
networks, which achieves high accuracy in predicting the molecular modulator's
effect on the power conversion efficiency (PCE) of PSCs. This framework is
established through systematical benchmarking of diverse molecular
representations, enabling lowcost and high-throughput virtual screening over
79,043 commercially available molecules. Furthermore, we leverage
interpretability techniques to visualize the learned chemical representation
and experimentally characterize the resulting modulator-perovskite
interactions. The top molecular modulators identified by the framework are
subsequently validated experimentally, delivering a remarkably improved
champion PCE of 26.91% in PSCs.

</details>


### [474] [Generative Machine Learning Models for the Deconvolution of Charge Carrier Dynamics in Organic Photovoltaic Cells](https://arxiv.org/abs/2511.01118)
*Li Raymond,Salim Flora,Wang Sijin,Wright Brendan*

Main category: cond-mat.mtrl-sci

TL;DR: 引入机器学习框架eta - LLODE分析有机光伏器件载流子动力学，可模拟实验条件。


<details>
  <summary>Details</summary>
Motivation: 传统分析方法难以对有机光伏器件的载流子动力学进行建模。

Method: 引入eta - Linearly Decoded Latent Ordinary Differential Equations (eta - LLODE)机器学习框架，从P3HT:PCBM电池的时间分辨电荷提取测量中分离和重建提取动力学。

Result: 发现载流子行为可用压缩指数衰减很好地描述，学习到的可解释潜空间能对实验测量条件进行模拟。

Conclusion: 该框架为太阳能电池研究提供了预测工具，可支持器件研究和优化。

Abstract: Charge carrier dynamics critically affect the efficiency and stability of
organic photovoltaic devices, but they are challenging to model with
traditional analytical methods. We introduce \b{eta}-Linearly Decoded Latent
Ordinary Differential Equations (\b{eta}-LLODE), a machine learning framework
that disentangles and reconstructs extraction dynamics from time-resolved
charge extraction measurements of P3HT:PCBM cells. This model enables the
isolated analysis of the underlying charge carrier behaviour, which was found
to be well described by a compressed exponential decay. Furthermore, the learnt
interpretable latent space enables simulation, including both interpolation and
extrapolation of experimental measurement conditions, offering a predictive
tool for solar cell research to support device study and optimisation.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [475] [Closing the SNAP Gap: Identifying Under-Enrollment in High-Poverty ZIP Codes](https://arxiv.org/abs/2511.00080)
*Auyona Ray*

Main category: econ.GN

TL;DR: 构建经济不安全指数，后聚焦SNAP差距区域，用结构指标训练模型，发现经济不安全集中在农村，交通是阻碍项目参与的稳定因素，提供全国诊断框架。


<details>
  <summary>Details</summary>
Motivation: 探究SNAP参与情况，解决仅用收入无法充分解释参与情况的问题。

Method: 构建经济不安全指数，确定SNAP差距区域，用2014 - 2023年全国ZIP级数据，基于四个结构指标训练逻辑分类模型。

Result: 最有效模型依赖车辆和教育两个预测因素，优于基于树的分类器，经济不安全集中在农村，交通是项目参与的稳定障碍。

Conclusion: 提供全国诊断框架，可为开发筛选工具、改善服务不足社区福利获取提供信息。

Abstract: This project began by constructing an index of economic insecurity using
multiple socioeconomic indicators. Although poverty alone predicted SNAP
participation more accurately than the composite index, its explanatory power
was weaker than anticipated, echoing past findings that enrollment cannot be
explained by income alone. This led to a shift in focus: identifying ZIP codes
with high poverty but unexpectedly low SNAP participation, areas defined here
as having a SNAP Gap, where ZIPs fall in the top 30 percent of family poverty
and the bottom 10 percent of SNAP enrollment. Using nationally available ZIP
level data from 2014 to 2023, I trained logistic classification models on four
interpretable structural indicators: lack of vehicle, lack of internet access,
lack of computer access, and percentage of adults with only a high school
diploma. The most effective model relies on just two predictors, vehicle access
and education, and outperforms tree based classifiers in both precision and
calibration. Results show that economic insecurity is consistently concentrated
in rural ZIP codes, with transportation access emerging as the most stable
barrier to program take up. This study provides a nationwide diagnostic
framework that can inform the development of scalable screening tools for
targeting outreach and improving benefit access in underserved communities.

</details>


### [476] [Different Forms of Imbalance in Strongly Playable Discrete Games I: Two-Player RPS Games](https://arxiv.org/abs/2511.00374)
*Itai Maimon*

Main category: econ.GN

TL;DR: 构建不平衡性和可玩性定义，以主导策略存在情况衡量，展示(2n + 1)-RPS最大化不平衡定义，后续将用于多人游戏。


<details>
  <summary>Details</summary>
Motivation: 为游戏的不平衡性和可玩性给出合理定义，使其与经济和博弈论中不平等定义在最大和最小情况上达成一致。

Method: 构建不平衡性和可玩性的定义，通过(2n + 1)-RPS展示定义的合理性。

Result: 所提出的不平衡性定义在可玩的RPS游戏中能被(2n + 1)-RPS最大化，且与经济和博弈论中不平等定义在最大最小情况上相符。

Conclusion: 这些平衡定义是自然合理的，后续可用于多人游戏分析。

Abstract: We construct several definitions of imbalance and playability, both of which
are related to the existence of dominated strategies. Specifically, a maximally
balanced game and a playable game cannot have dominated strategies for any
player. In this context, imbalance acts as a measure of inequality in strategy,
similar to measures of inequality in wealth or population dynamics. Conversely,
playability is a slight strengthening of the condition that a game has no
dominated strategies. It is more accurately aligned with the intuition that all
strategies should see play. We show that these balance definitions are natural
by exhibiting a (2n+1)-RPS that maximizes all proposed imbalance definitions
among playable RPS games. We demonstrate here that this form of imbalance
aligns with the prevailing notion that different definitions of inequality for
economic and game-theoretic distributions must agree on both the maximal and
minimal cases. In the sequel paper, we utilize these definitions for
multiplayer games to demonstrate that a generalization of this imbalanced RPS
is at least nearly maximally imbalanced while remaining playable for under 50
players.

</details>


### [477] [Modeling Uncertainty in Integrated Assessment Models](https://arxiv.org/abs/2511.00378)
*Yongyang Cai*

Main category: econ.GN

TL;DR: 本文综述IAMs处理不确定性的方式，介绍相关进展及对气候政策的影响。


<details>
  <summary>Details</summary>
Motivation: 气候系统和社会经济过程复杂且有不确定性，有效管理IAMs中的不确定性对制定稳健气候政策至关重要。

Method: 审查IAMs中存在的不确定性类型，讨论解决这些不确定性的建模方法，探索该领域的最新发展。

Result: 文中未提及具体结果。

Conclusion: 文中未提及具体结论。

Abstract: Integrated Assessment Models (IAMs) are pivotal tools that synthesize
knowledge from climate science, economics, and policy to evaluate the
interactions between human activities and the climate system. They serve as
essential instruments for policymakers, providing insights into the potential
outcomes of various climate policies and strategies. Given the complexity and
inherent uncertainties in both the climate system and socio-economic processes,
understanding and effectively managing uncertainty within IAMs is crucial for
robust climate policy development. This review aims to provide a comprehensive
overview of how IAMs handle uncertainty, highlighting recent methodological
advancements and their implications for climate policy. I examine the types of
uncertainties present in IAMs, discuss various modeling approaches to address
these uncertainties, and explore recent developments in the field, including
the incorporation of advanced computational methods.

</details>


### [478] [A rich life cycle model of labor supply in Finland](https://arxiv.org/abs/2511.00660)
*Antti J. Tanskanen*

Main category: econ.GN

TL;DR: 本文构建芬兰异质人口的消费与劳动供给生命周期模型，模型能复现芬兰就业市场多项统计数据，并分析了Orpo政府计划对就业和公共财政的影响。


<details>
  <summary>Details</summary>
Motivation: 构建描述芬兰异质人口在多种就业状态和生活情况下的消费与劳动供给生命周期模型。

Method: 开发一个描述芬兰异质人口的生命周期模型。

Result: 模型能复现芬兰就业市场的多项统计数据。

Conclusion: 可运用该模型分析Orpo政府计划对芬兰就业和公共财政的影响。

Abstract: A life cycle model of consumption and labor supply describes employment
decisions of a collection of individuals during their lifetime. We develop a
life cycle model describing a heterogeneous population operating in Finland
under a wide variety of employment states and life situations. A rich life
cycle model requires a large state space representing the possible states of
simulated agents. The results demonstrate that the model reproduces a number of
statistics of the Finnish employment market such as the age structures of
employment rate and unemployment rate, distributions of observed effective
marginal tax rates and participating tax rates, and proportion of part time
work. As an application of analysis of a reform, we analyze how the program of
Orpo government influences employment and public finances in Finland.

</details>


### [479] [Public Infrastructure Investments for Space Market Development](https://arxiv.org/abs/2511.00935)
*Akhil Rao*

Main category: econ.GN

TL;DR: 提出图形框架分析太空市场，以NASA项目为例说明共享基础设施可带来行业利润，并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 太空技术系统有高成本、需求有限等特点，公共航天机构发展市场面临挑战，需有效方法。

Method: 提出图形框架，结合公共产品理论，以NASA商业近地轨道目的地计划为例应用框架。

Result: 在成本和需求条件下，独立空间站年亏损3.55亿美元，共享核心基础设施年盈利1.54亿美元。

Conclusion: 提出的框架有实际应用价值，还给出公共投资和市场发展策略未来研究方向。

Abstract: Advanced space technology systems often face high fixed costs, can serve
limited non-government demand, and are significantly driven by non-market
motivations. While increased entrepreneurial activity and national ambitions in
space have encouraged planners at public space agencies to develop markets
around such systems, the very factors that make the recent growth of the space
economy so remarkable also challenge planners' efforts to develop and sustain
markets for space-related goods and services. I propose a graphical framework
to visualize the number of competitors a market can sustain as a function of
the industry's cost structure; the distribution of government support across
direct purchases, direct investments, and shared infrastructure; and the
magnitude of non-government demand. Building on public goods theory, the
framework shows how marginal dollars invested in shared infrastructure can
create non-rival benefits supporting more competitors per dollar than direct
purchases or subsidies. I demonstrate the framework with a stylized application
inspired by NASA's Commercial LEO Destinations program. Under cost and demand
conditions consistent with public data, independent stations generate
industry-wide losses of $355 million annually, while shared core infrastructure
enables industry-wide profits of $154 million annually. I also outline key
directions for future research on public investment and market development
strategies for advanced technologies.

</details>


### [480] [Liquidity Shocks, Homeownership, and Income Inequality: Impact of Early Pension Withdrawals and Reduced Deposit](https://arxiv.org/abs/2511.01133)
*Hamza Hanbali,Gaurav Khemka,Himasha Warnakulasooriya*

Main category: econ.GN

TL;DR: 分析两项影响住房需求的政府政策，发现短期提价，且不同政策效果有差异，不平等源于市场既有差异。


<details>
  <summary>Details</summary>
Motivation: 研究两项影响住房需求的政府政策（提前提取养老金储蓄和降低贷款首付）的效果。

Method: 构建纳入住房需求对房价反馈的模型，使用澳大利亚数据。

Result: 两项政策短期提价；RD 影响低收入家庭购房；EW 提升各群体购房可及性，但可能降低退休保障；不平等源于既有市场差异。

Conclusion: 不同政策在住房市场产生不同影响，市场既有差异导致不平等结果。

Abstract: The paper analyzes two government policies affecting housing demand: early
withdrawal from pension savings (EW), and reduction of loan deposit (RD). A
model incorporating demand feedback on housing prices using Australian data
shows both policies raise prices in the short run. RD delays or prevents access
for low-income households, particularly in supply-constrained markets. EW
improves accessibility across groups and is most efficient when full withdrawal
is permitted, but can reduce retirement security if pension grows faster than
property prices. The results also indicate that unequal outcomes stem not from
price surges themselves but from pre-existing market disparities.

</details>


### [481] [Novelty and Impact of Economics Papers](https://arxiv.org/abs/2511.01211)
*Chaofeng Wu*

Main category: econ.GN

TL;DR: 本文提出将科学新颖性视为论文在知识版图中位置的反映的框架，分解为空间和时间新颖性，用大语言模型量化，应用于经济学文章发现两种新颖性预测不同结果，可构建语义邻域类型学。


<details>
  <summary>Details</summary>
Motivation: 重新定义科学新颖性的概念，将其从单一属性转变为论文在不断演变的知识版图中位置的反映。

Method: 将论文位置分解为空间新颖性和时间新颖性两个维度，利用大语言模型开发语义隔离指标来量化论文与全文文献的相对位置。

Result: 发现两种新颖性维度预测不同结果，时间新颖性主要预测引用次数，空间新颖性预测颠覆性影响，构建了语义邻域类型学。

Conclusion: 新颖性是一个多维结构，不同形式对科学进步有可衡量且本质不同的影响。

Abstract: We propose a framework that recasts scientific novelty not as a single
attribute of a paper, but as a reflection of its position within the evolving
intellectual landscape. We decompose this position into two orthogonal
dimensions: \textit{spatial novelty}, which measures a paper's intellectual
distinctiveness from its neighbors, and \textit{temporal novelty}, which
captures its engagement with a dynamic research frontier. To operationalize
these concepts, we leverage Large Language Models to develop semantic isolation
metrics that quantify a paper's location relative to the full-text literature.
Applying this framework to a large corpus of economics articles, we uncover a
fundamental trade-off: these two dimensions predict systematically different
outcomes. Temporal novelty primarily predicts citation counts, whereas spatial
novelty predicts disruptive impact. This distinction allows us to construct a
typology of semantic neighborhoods, identifying four archetypes associated with
distinct and predictable impact profiles. Our findings demonstrate that novelty
can be understood as a multidimensional construct whose different forms,
reflecting a paper's strategic location, have measurable and fundamentally
distinct consequences for scientific progress.

</details>


### [482] [Internet of Things Platform Service Supply Innovation: Exploring the Impact of Overconfidence](https://arxiv.org/abs/2511.01332)
*Xiufeng Li,Zefang Li*

Main category: econ.GN

TL;DR: 本文构建博弈模型探讨物联网环境下制造商过度自信对其与平台协同创新的影响，发现不同合同下相关因素受非隐私敏感客户比例影响，适度过度自信在特定条件下能带来供应链帕累托改进。


<details>
  <summary>Details</summary>
Motivation: 研究物联网环境下制造商过度自信对其与平台协同创新的影响。

Method: 构建博弈模型。

Result: 不同合同下创新投入、利润水平和定价策略受非隐私敏感客户比例影响；不同合同下过度自信对创新和定价有不同影响；适度过度自信在给定合同下可带来供应链帕累托改进。

Conclusion: 为理解物联网供应链中制造商与平台的复杂互动提供新视角，为实际商业决策提供理论支持和实践指导。

Abstract: This paper explores the impact of manufacturers' overconfidence on their
collaborative innovation with platforms in the Internet of Things (IoT)
environment by constructing a game model. It is found that in both usage-based
and revenue-sharing contracts, manufacturers' and platforms' innovation inputs,
profit levels, and pricing strategies are significantly affected by the
proportion of non-privacy-sensitive customers, and grow in tandem with the rise
of this proportion. In usage-based contracts, moderate overconfidence
incentivizes manufacturers to increase hardware innovation investment and
improve overall supply chain revenues, but may cause platforms to reduce
software innovation; under revenue-sharing contracts, overconfidence positively
incentivizes hardware innovation and pricing more strongly, while platform
software innovation varies nonlinearly depending on the share ratio. Comparing
the differences in manufacturers' decisions with and without overconfidence
suggests that moderate overconfidence can lead to supply chain Pareto
improvements under a given contract. This paper provides new perspectives for
understanding the complex interactions between manufacturers and platforms in
IoT supply chains, as well as theoretical support and practical guidance for
actual business decisions.

</details>


### [483] [Measuring Domestic Violence. Individual Attitudes and Time Use Within the Household](https://arxiv.org/abs/2511.01473)
*Elena Pisanelli*

Main category: econ.GN

TL;DR: 本文提出衡量家庭内家庭暴力文化理由的策略，构建指数并验证，发现容忍度的影响因素，框架可助监测不平等和设计干预措施。


<details>
  <summary>Details</summary>
Motivation: 提出衡量家庭内家庭暴力文化理由的策略，其对人口行为和性别不平等有直接影响。

Method: 利用意大利有孩子夫妇的调查数据构建综合指数，用结构方程模型分离潜在容忍度并验证指数。

Result: 容忍度存在性别、教育和规范环境的异质性，保守规范是强预测因素，男性高教育降低容忍度，容忍度与休闲时间正相关。

Conclusion: 该框架为经济学家和政策制定者提供监测不平等和设计干预措施的工具。

Abstract: This paper proposes a novel empirical strategy to measure cultural
justifications of domestic violence within households, with direct implications
for demographic behavior and gender inequality. Leveraging survey data on
individual attitudes and high-frequency time-use diaries from Italian couples
with children, I construct a composite index that integrates stated beliefs
with observed household practices. Using structural equation modeling, I
disentangle latent tolerance of domestic violence from reported attitudes and
validate the index against both individual and partner characteristics, as well
as time allocation patterns. Results reveal systematic heterogeneity by gender,
education, and normative environments. Conservative gender and parenthood norms
are strong predictors of tolerance, while higher male education reduces it.
Tolerance of violence is also positively associated with reported leisure time
with partners and children, suggesting that co-presence does not necessarily
reflect egalitarian interaction but may coexist with unequal bargaining
structures. Beyond advancing measurement, the findings highlight how cultural
tolerance of domestic violence is embedded in household arrangements that
influence fertility, labor supply, and the intergenerational transmission of
norms. The proposed framework offers a scalable tool for economists and
policymakers to monitor hidden inequalities and design interventions targeting
family stability, gender equity, and child well-being.

</details>


### [484] [Gendered Responses to Subtle Social Pressure: Experimental Evidence from Survey Results](https://arxiv.org/abs/2511.01565)
*Sevgi Çolak*

Main category: econ.GN

TL;DR: 研究调查问题措辞细微变化对参与者参与度的影响及性别差异，结果无显著处理效应，仅发现所有参与者有小的负基线情绪。


<details>
  <summary>Details</summary>
Motivation: 分析调查问题措辞的细微变化是否影响参与者参与度，以及这种影响是否因性别而异。

Method: 采用混合效应回归模型进行分析。

Result: 对164名参与者和492次观察，未发现处理对任何结果有显著影响，也无性别调节的证据，唯一发现是所有参与者有小的负基线情绪。

Conclusion: 研究结果有助于完善关于语言框架和性别规范影响行为的理论预期。

Abstract: This study analyzes whether subtle variations in the survey questionnaire
phrasing influence participant engagement and whether these effects differ by
gender. Building on theories of social pressure and politeness norms, it is
hypothesized that presumptive phrasing would reduce engagement compared to
appreciative phrasing and baseline phrasing (H1), and this effect would be more
pronounced among women (H2). Mixed-effects regression models showed no
significant treatment effects on any outcome and no evidence of gender
moderation for 164 participants and 492 observations. The only robust finding
was a small negative baseline sentiment across all participants, independent of
any treatment or gender. The findings contribute to refining theoretical
expectations about the conditions in which linguistic framing and gender norms
shape behaviour.

</details>


### [485] [Deceptively Framed Lotteries in Consumer Markets](https://arxiv.org/abs/2511.01597)
*Markus Dertwinkel-Kalt,Hans-Theo Normann,Jan-Niklas Tiede,Tobias Werner*

Main category: econ.GN

TL;DR: 研究销售彩票式产品时展示格式对消费者信念和支付意愿的影响，发现欺骗性框架会影响消费者并使企业获利。


<details>
  <summary>Details</summary>
Motivation: 探究展示格式如何塑造消费者信念和支付意愿。

Method: 进行有802名参与者的在线实验，卖家采用隐瞒结果概率和突出罕见成功两种常见操纵方式构建彩票框架。

Result: 超80%卖家采用欺骗性框架，显著提高买家信念和支付意愿，卖家也相应提价。

Conclusion: 欺骗性框架会系统性改变消费者信念，使企业获取额外利益，强调营销中框架工具战略价值和政策中透明度要求对保护消费者的重要性。

Abstract: Consumers often face products sold as lotteries rather than fixed outcomes. A
prominent case is the loot box in video games, where players pay for randomized
rewards. We investigate how presentation formats shape consumer beliefs and
willingness to pay. In an online experiment with 802 participants, sellers
could frame lotteries using two common manipulations: censoring outcome
probabilities and selectively highlighting rare successes. More than 80\% of
sellers adopted such deceptive frames, particularly when both manipulations
were available. These choices substantially inflated buyer beliefs and
increased willingness to pay of up to six times the expected value. Sellers
anticipated this effect and raised prices accordingly. Our results show how
deceptive framing systematically shifts consumer beliefs and enables firms to
extract additional surplus. For marketing practice, this highlights the
strategic value of framing tools in probabilistic selling models; for policy,
it underscores the importance of transparency requirements in protecting
consumers.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [486] [Computation as a Game](https://arxiv.org/abs/2511.00058)
*Paul Alexander Bilokon*

Main category: cs.CC

TL;DR: 提出将计算表示为算法与自然的二人博弈，以此框架定义复杂度类，将P与NP问题转化为纳什均衡等价性问题。


<details>
  <summary>Details</summary>
Motivation: 以新视角研究计算复杂度，解决如P是否等于NP等开放问题。

Method: 基于域理论和博弈论，将计算表示为算法与自然的二人博弈，算法在Scott域中生成近似值，自然根据与真值的距离给予惩罚。

Result: 能用博弈论定义复杂度类，刻画P、NP等相关类，将P是否等于NP问题转化为特定约束下纳什均衡的等价性问题。

Conclusion: 该框架为研究计算复杂度和相关开放问题提供了新途径。

Abstract: We present a unifying representation of computation as a two-player game
between an \emph{Algorithm} and \emph{Nature}, grounded in domain theory and
game theory. The Algorithm produces progressively refined approximations within
a Scott domain, while Nature assigns penalties proportional to their distance
from the true value. Correctness corresponds to equilibrium in the limit of
refinement. This framework allows us to define complexity classes
game-theoretically, characterizing $\mathbf{P}$, $\mathbf{NP}$, and related
classes as sets of problems admitting particular equilibria. The open question
$\mathbf{P} \stackrel{?}{=} \mathbf{NP}$ becomes a problem about the
equivalence of Nash equilibria under differing informational and temporal
constraints.

</details>


### [487] [Unbounded-width CSPs are Untestable in a Sublinear Number of Queries](https://arxiv.org/abs/2510.27012)
*Yumou Fei*

Main category: cs.CC

TL;DR: 证明有界度模型下无界宽度CSP可满足性测试需Ω(n)查询，统一并推广先前下界。


<details>
  <summary>Details</summary>
Motivation: 有界度查询模型是图属性测试和亚线性时间算法标准框架，很多属性可用CSP可满足性表达，需研究其测试复杂度。

Method: 结合Bogdanov等人技术与泛代数已知结果。

Result: 证明有界度模型下无界宽度CSP可满足性测试需Ω(n)查询，适用于已知NP难的CSP。

Conclusion: 结果统一并推广了先前的几个下界。

Abstract: The bounded-degree query model, introduced by Goldreich and Ron
(\textit{Algorithmica, 2002}), is a standard framework in graph property
testing and sublinear-time algorithms. Many properties studied in this model,
such as bipartiteness and 3-colorability of graphs, can be expressed as
satisfiability of constraint satisfaction problems (CSPs). We prove that for
the entire class of \emph{unbounded-width} CSPs, testing satisfiability
requires $\Omega(n)$ queries in the bounded-degree model. This result unifies
and generalizes several previous lower bounds. In particular, it applies to all
CSPs that are known to be $\mathbf{NP}$-hard to solve, including
$k$-colorability of $\ell$-uniform hypergraphs for any $k,\ell \ge 2$ with
$(k,\ell) \neq (2,2)$.
  Our proof combines the techniques from Bogdanov, Obata, and Trevisan
(\textit{FOCS, 2002}), who established the first $\Omega(n)$ query lower bound
for CSP testing in the bounded-degree model, with known results from universal
algebra.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [488] [Branched Signature Model](https://arxiv.org/abs/2511.00018)
*Munawar Ali,Qi Feng*

Main category: math.NA

TL;DR: 引入分支签名模型，证明通用逼近定理，展示路径扩展构造方法，为计算和应用提供框架。


<details>
  <summary>Details</summary>
Motivation: 受分支粗糙路径框架启发，推广经典几何粗糙路径。

Method: 建立通用逼近定理，利用已有扩展映射构造路径扩展。

Result: 证明迭代组合可逼近高层签名，给出路径扩展构造方法。

Conclusion: 该框架为分支签名计算提供高效方案，为数据驱动建模和应用开辟新途径。

Abstract: In this paper, we introduce the branched signature model, motivated by the
branched rough path framework of [Gubinelli, Journal of Differential Equations,
248(4), 2010], which generalizes the classical geometric rough path. We
establish a universal approximation theorem for the branched signature model
and demonstrate that iterative compositions of lower-level signature maps can
approximate higher-level signatures. Furthermore, building on the existence of
the extension map proposed in [Hairer-Kelly. Annales de l'Institue Henri
Poincar\'e, Probabilit\'es et Statistiques 51, no. 1 (2015)], we show how to
explicitly construct the extension of the original paths into
higher-dimensional spaces via a map $\Psi$, so that the branched signature can
be realized as the classical geometric signature of the extended path. This
framework not only provides an efficient computational scheme for branched
signatures but also opens new avenues for data-driven modeling and
applications.

</details>


### [489] [Numerical methods for solving PIDEs arising in swing option pricing under a two-factor mean-reverting model with jumps](https://arxiv.org/abs/2511.01587)
*Mustapha Regragui,Karel J. in 't Hout,Michèle Vanmaele,Fred Espen Benth*

Main category: math.NA

TL;DR: 本文研究线性双因子带跳均值回归模型下离散行权时间摆动期权的数值估值，提出二阶数值方法并分析其稳定性与收敛性，通过数值实验验证二阶收敛性。


<details>
  <summary>Details</summary>
Motivation: 解决线性双因子带跳均值回归模型下离散行权时间摆动期权数值估值问题，处理对流主导、含非局部积分项和非光滑初值的二维偏积分 - 微分方程序列。

Method: 提出多种二阶数值方法，并对其稳定性和收敛性进行理论分析。

Result: 通过大量数值实验，证实所提数值方法具有二阶收敛行为。

Conclusion: 所提出的二阶数值方法能有效处理摆动期权数值估值中的复杂特征。

Abstract: This paper concerns the numerical valuation of swing options with discrete
action times under a linear two-factor mean-reverting model with jumps. The
resulting sequence of two-dimensional partial integro-differential equations
(PIDEs) are convection-dominated and possess a nonlocal integral term due to
the presence of jumps. Further, the initial function is nonsmooth. We propose
various second-order numerical methods that can adequately handle these
challenging features. The stability and convergence of these numerical methods
are analysed theoretically. By ample numerical experiments, we confirm their
second-order convergence behaviour.

</details>


### [490] [Trust-Region Methods with Low-Fidelity Objective Models](https://arxiv.org/abs/2511.00434)
*Andrea Angino,Matteo Aurina,Alena Kopaničáková,Matthias Voigt,Marco Donatelli,Rolf Krause*

Main category: math.NA

TL;DR: 本文基于MTR框架引入两种多保真度信赖域方法，通过求解低保真模型的粗信赖域子问题确定辅助方向，分别为STR和SVDTR方法，数值算例表明效率有潜在提升。


<details>
  <summary>Details</summary>
Motivation: 引入新的多保真度信赖域方法以提高优化效率。

Method: 基于MTR框架，通过求解低保真目标模型的粗信赖域子问题确定辅助“神奇”方向，分别使用稀疏矩阵（STR）和数据集的截断奇异值分解（SVDTR）来构建该方向。

Result: 通过几个数值算例展示了方法在效率上有潜在提升。

Conclusion: 提出的两种多保真度信赖域方法具有提高优化效率的潜力。

Abstract: We introduce two multifidelity trust-region methods based on the Magical
Trust Region (MTR) framework. MTR augments the classical trust-region step with
a secondary, informative direction. In our approaches, the secondary
``magical'' directions are determined by solving coarse trust-region
subproblems based on low-fidelity objective models. The first proposed method,
Sketched Trust-Region (STR), constructs this secondary direction using a
sketched matrix to reduce the dimensionality of the trust-region subproblem.
The second method, SVD Trust-Region (SVDTR), defines the magical direction via
a truncated singular value decomposition of the dataset, capturing the leading
directions of variability. Several numerical examples illustrate the potential
gain in efficiency.

</details>


### [491] [Matrix Phylogeny: Compact Spectral Fingerprints for Trap-Robust Preconditioner Selection](https://arxiv.org/abs/2511.00012)
*Jinwoo Baek*

Main category: math.NA

TL;DR: 论文提出CSF/ASF矩阵指纹，具有低维、免特征分解等优点，聚类效果好、抗噪且支持预条件器选择，推荐默认用CSF(K=5)，特定场景用ASF。


<details>
  <summary>Details</summary>
Motivation: 为矩阵在家族层面提供特征描述，实现可扩展、结构感知的矩阵搜索和推荐。

Method: 通过Hutchinson草图估计Chebyshev迹矩构建低维、免特征分解的描述符CSF/ASF，并进行仿射重缩放。

Result: CSF/ASF聚类效果好，在SuiteSparse基准测试中ARI=1.0；抗噪性强；支持预条件器选择，推荐器迭代次数接近最优。

Conclusion: CSF/ASF是紧凑、快速、不变的指纹，默认推荐CSF(K=5)，特定场景用ASF。

Abstract: Matrix Phylogeny introduces compact spectral fingerprints (CSF/ASF) that
characterize matrices at the family level. These fingerprints are
low-dimensional, eigendecomposition-free descriptors built from Chebyshev trace
moments estimated by Hutchinson sketches. A simple affine rescaling to [-1,1]
makes them permutation/similarity invariant and robust to global scaling.
  Across synthetic and real tests, we observe phylogenetic compactness: only a
few moments are needed. CSF with K=3-5 already yields perfect clustering
(ARI=1.0; silhouettes ~0.89) on four synthetic families and a five-family set
including BA vs ER, while ASF adapts the dimension on demand (median K*~9). On
a SuiteSparse mini-benchmark (Hutchinson p~100), both CSF-H and ASF-H reach
ARI=1.0. Against strong alternatives (eigenvalue histograms + Wasserstein,
heat-kernel traces, WL-subtree), CSF-K=5 matches or exceeds accuracy while
avoiding eigendecompositions and using far fewer features (K<=10 vs 64/9153).
  The descriptors are stable to noise (log-log slope ~1.03, R^2~0.993) and
support a practical trap->recommend pipeline for automated preconditioner
selection. In an adversarial E6+ setting with a probe-and-switch mechanism, our
physics-guided recommender attains near-oracle iteration counts (p90 regret=0),
whereas a Frobenius 1-NN baseline exhibits large spikes (p90~34-60).
  CSF/ASF deliver compact (K<=10), fast, invariant fingerprints that enable
scalable, structure-aware search and recommendation over large matrix
repositories. We recommend CSF with K=5 by default, and ASF when
domain-specific adaptivity is desired.

</details>


### [492] [On the Structure of Floating-Point Noise in Batch-Invariant GPU Matrix Multiplication](https://arxiv.org/abs/2511.00025)
*Tadisetty Sai Yashwanth*

Main category: math.NA

TL;DR: 本文实证检验浮点误差为独立同分布高斯噪声的假设，发现该假设不符GPU实际行为，浮点误差有结构且高度相关，挑战了数值噪声的随机观点。


<details>
  <summary>Details</summary>
Motivation: 浮点非结合性使深度学习操作非确定性，但数值误差的统计结构不明，常见假设是误差为独立同分布高斯噪声，需检验该假设。

Method: 比较单输入和批量矩阵乘法输出，进行协方差分析。

Result: i.i.d.模型预测输出不稳定，但实证显示预测翻转率为0.00%；浮点误差有结构且高度相关，float16近50%误差方差在非对角项。

Conclusion: 挑战了数值噪声的随机观点，为分析硬件非确定性下深度学习可靠性提供理论基础。

Abstract: Floating-point non-associativity makes fundamental deep learning operations,
such as matrix multiplication (matmul) on GPUs, inherently non-deterministic.
Despite this, the statistical structure of the resulting numerical error
remains poorly understood. A common working assumption is that these errors
behave as independent and identically distributed (i.i.d.) Gaussian noise. In
this paper, we empirically test this assumption and show that it fails to
describe real GPU behavior. By comparing outputs of single-input and batched
matmuls, we find that while the i.i.d. model predicts non-zero output
instability, empirical results show a 0.00% prediction flip rate. Through
covariance analysis, we uncover the cause: the floating-point error is
structured and highly correlated. For float16, nearly 50% of the total error
variance lies in off-diagonal terms, revealing that the noise behaves as a
coordinated, directional perturbation rather than random static. This result
challenges the prevailing stochastic view of numerical noise and provides a
principled foundation for analyzing deep learning reliability under hardware
non-determinism.

</details>


### [493] [Filtered Neural Galerkin model reduction schemes for efficient propagation of initial condition uncertainties in digital twins](https://arxiv.org/abs/2511.00670)
*Zhiyang Ning,Benjamin Peherstorfer*

Main category: math.NA

TL;DR: 提出一种降维建模方法处理数字孪生中不确定性量化问题，相比基于集合的方法有显著加速。


<details>
  <summary>Details</summary>
Motivation: 数字孪生中基于集合的不确定性量化方法在控制和数据同化循环中成本过高。

Method: 引入降维建模方法，通过预训练神经网络上的Neural Galerkin方案的矩封闭获得均值和协方差动力学。

Result: 过滤后的Neural Galerkin方案比基于集合的不确定性传播方法实现了超过一个数量级的加速。

Conclusion: 所提出的方法能有效解决数字孪生中不确定性量化成本高的问题。

Abstract: Uncertainty quantification in digital twins is critical to enable reliable
and credible predictions beyond available data. A key challenge is that
ensemble-based approaches can become prohibitively expensive when embedded in
control and data assimilation loops in digital twins, even when reduced models
are used. We introduce a reduced modeling approach that advances in time the
mean and covariance of the reduced solution distribution induced by the initial
condition uncertainties, which eliminates the need to maintain and propagate a
costly ensemble of reduced solutions. The mean and covariance dynamics are
obtained as a moment closure from Neural Galerkin schemes on pre-trained neural
networks, which can be interpreted as filtered Neural Galerkin dynamics
analogous to Gaussian filtering and the extended Kalman filter. Numerical
experiments demonstrate that filtered Neural Galerkin schemes achieve more than
one order of magnitude speedup compared to ensemble-based uncertainty
propagation.

</details>


### [494] [HEATNETs: Explainable Random Feature Neural Networks for High-Dimensional Parabolic PDEs](https://arxiv.org/abs/2511.00886)
*Kyriakos Georgiou,Gianluca Fabiani,Constantinos Siettos,Athanasios N. Yannacopoulos*

Main category: math.NA

TL;DR: 本文使用随机特征（投影）神经网络（RFNNs）解决高维抛物型偏微分方程的正问题，提出HEATNET，证明其能逼近解，给出收敛率，展示其可扩展性并通过基准问题评估性能。


<details>
  <summary>Details</summary>
Motivation: 解决高维抛物型偏微分方程的正问题，寻求有效的数值解法。

Method: 提出具有随机热核的单隐藏层神经网络HEATNET，利用物理信息神经网络、数值和泛函分析以及解算子结构，结合合适变换和重要性蒙特卡罗采样处理热核奇点。

Result: HEATNET在高达2000维的基准线性抛物问题中表现出显著准确性，500维以下近似误差为$1.0E - 05$到$1.0E - 07$，1000到2000维为$1.0E - 04$到$1.0E - 03$，特征数量相对较少。

Conclusion: HEATNET是一种可解释的方案，能有效解决任意高维抛物型偏微分方程。

Abstract: We deal with the solution of the forward problem for high-dimensional
parabolic PDEs with random feature (projection) neural networks (RFNNs). We
first prove that there exists a single-hidden layer neural network with
randomized heat-kernels arising from the fundamental solution (Green's
functions) of the heat operator, that we call HEATNET, that provides an
unbiased universal approximator to the solution of parabolic PDEs in arbitrary
(high) dimensions, with the rate of convergence being analogous to the
${O}(N^{-1/2})$, where $N$ is the size of HEATNET. Thus, HEATNETs are
explainable schemes, based on the analytical framework of parabolic PDEs,
exploiting insights from physics-informed neural networks aided by numerical
and functional analysis, and the structure of the corresponding solution
operators. Importantly, we show how HEATNETs can be scaled up for the efficient
numerical solution of arbitrary high-dimensional parabolic PDEs using suitable
transformations and importance Monte Carlo sampling of the integral
representation of the solution, in order to deal with the singularities of the
heat kernel around the collocation points. We evaluate the performance of
HEATNETs through benchmark linear parabolic problems up to 2,000 dimensions. We
show that HEATNETs result in remarkable accuracy with the order of the
approximation error ranging from $1.0E-05$ to $1.0E-07$ for problems up to 500
dimensions, and of the order of $1.0E-04$ to $1.0E-03$ for 1,000 to 2,000
dimensions, with a relatively low number (up to 15,000) of features.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [495] [High-Dimensional Spatial Arbitrage Pricing Theory with Heterogeneous Interactions](https://arxiv.org/abs/2511.01271)
*Zhaoxing Gao,Sihan Tu,Ruey S. Tsay*

Main category: econ.EM

TL;DR: 本文研究了整合空间交互与多因素分析的空间套利定价理论（SAPT）模型的估计和推断，引入考虑空间效应的空间资本资产定价模型（SCAPM），提出广义收缩Yule - Walker（SYW）估计方法，建立了高维渐近性质并通过数据验证。


<details>
  <summary>Details</summary>
Motivation: 研究整合空间交互与多因素分析的SAPT模型的估计和推断，在高维资产中考虑空间效应。

Method: 基于经典均值 - 方差分析引入SCAPM，将其扩展到SAPT框架；对含可观测因素的SAPT采用广义收缩SYW估计方法，对含潜在因素的情况先进行基于自协方差的特征分析提取因素再用SYW方法；建立高维渐近性质。

Result: 通过模拟和真实数据例子证明了所提模型和方法的有效性和实用性。

Conclusion: 所提出的模型和方法在处理高维资产定价中考虑空间效应是有效的和有用的。

Abstract: This paper investigates estimation and inference of a Spatial Arbitrage
Pricing Theory (SAPT) model that integrates spatial interactions with
multi-factor analysis, accommodating both observable and latent factors.
Building on the classical mean-variance analysis, we introduce a class of
Spatial Capital Asset Pricing Models (SCAPM) that account for spatial effects
in high-dimensional assets, where we define {\it spatial rho} as a counterpart
to market beta in CAPM. We then extend SCAPM to a general SAPT framework under
a {\it complete} market setting by incorporating multiple factors. For SAPT
with observable factors, we propose a generalized shrinkage Yule-Walker (SYW)
estimation method that integrates ridge regression to estimate spatial and
factor coefficients. When factors are latent, we first apply an
autocovariance-based eigenanalysis to extract factors, then employ the SYW
method using the estimated factors. We establish asymptotic properties for
these estimators under high-dimensional settings where both the dimension and
sample size diverge. Finally, we use simulated and real data examples to
demonstrate the efficacy and usefulness of the proposed model and method.

</details>


### [496] [Cross-Validated Causal Inference: a Modern Method to Combine Experimental and Observational Data](https://arxiv.org/abs/2511.00727)
*Xuelin Yang,Licong Lin,Susan Athey,Michael I. Jordan,Guido W. Imbens*

Main category: econ.EM

TL;DR: 本文提出新方法整合因果推断中的实验与观测数据，经实验验证有效并给出理论误差界。


<details>
  <summary>Details</summary>
Motivation: 随机对照试验成本高、样本量有限，观测数据易受未测量混杂因素影响有偏差，需结合二者优势。

Method: 提出系统框架，将因果估计表述为经验风险最小化问题，通过最小化实验和观测损失的加权组合得到包含因果参数的完整模型，权重通过实验折上因果参数的交叉验证选择。

Result: 在真实和合成数据上的实验表明该方法有效且可靠，还给出了理论非渐近误差界。

Conclusion: 所提方法能有效整合实验和观测数据进行因果推断。

Abstract: We develop new methods to integrate experimental and observational data in
causal inference. While randomized controlled trials offer strong internal
validity, they are often costly and therefore limited in sample size.
Observational data, though cheaper and often with larger sample sizes, are
prone to biases due to unmeasured confounders. To harness their complementary
strengths, we propose a systematic framework that formulates causal estimation
as an empirical risk minimization (ERM) problem. A full model containing the
causal parameter is obtained by minimizing a weighted combination of
experimental and observational losses--capturing the causal parameter's
validity and the full model's fit, respectively. The weight is chosen through
cross-validation on the causal parameter across experimental folds. Our
experiments on real and synthetic data show the efficacy and reliability of our
method. We also provide theoretical non-asymptotic error bounds.

</details>


### [497] [Making Interpretable Discoveries from Unstructured Data: A High-Dimensional Multiple Hypothesis Testing Approach](https://arxiv.org/abs/2511.01680)
*Jacob Carlson*

Main category: econ.EM

TL;DR: 本文提出从非结构化数据中以统计原则进行发现的通用灵活框架，并在实证经济学中应用，还提供开源代码。


<details>
  <summary>Details</summary>
Motivation: 社会科学家利用非结构化数据获取新实证见解，无监督分析受关注，需从非结构化数据中进行发现。

Method: 利用机器学习可解释性方法将数据映射到概念字典，计算统计量，使用新统计程序进行选择性推理。

Result: 框架自由度少、可完全复现、实施成本低，并在实证经济学中有应用。

Conclusion: 该框架能有效从非结构化数据中进行发现，还提供开源代码方便研究者使用。

Abstract: Social scientists are increasingly turning to unstructured datasets to unlock
new empirical insights, e.g., estimating causal effects on text outcomes,
measuring beliefs from open-ended survey responses. In such settings,
unsupervised analysis is often of interest, in that the researcher does not
want to pre-specify the objects of measurement or otherwise artificially
delimit the space of measurable concepts; they are interested in discovery.
This paper proposes a general and flexible framework for pursuing discovery
from unstructured data in a statistically principled way. The framework
leverages recent methods from the literature on machine learning
interpretability to map unstructured data points to high-dimensional, sparse,
and interpretable dictionaries of concepts; computes (test) statistics of these
dictionary entries; and then performs selective inference on them using newly
developed statistical procedures for high-dimensional exceedance control of the
$k$-FWER under arbitrary dependence. The proposed framework has few researcher
degrees of freedom, is fully replicable, and is cheap to implement -- both in
terms of financial cost and researcher time. Applications to recent descriptive
and causal analyses of unstructured data in empirical economics are explored.
An open source Jupyter notebook is provided for researchers to implement the
framework in their own projects.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [498] [Further Developments on Stochastic Dominance for Different Classes of Infinite-mean Distributions](https://arxiv.org/abs/2511.00764)
*Keyi Zeng,Zhenfeng Zou,Yuting Su,Taizhong Hu*

Main category: math.PR

TL;DR: 本文研究独立同分布无限均值随机变量随机占优相关分布类性质、包含关系，扩展已有结果，并分析复合二项分布情形下随机占优关系的充要条件。


<details>
  <summary>Details</summary>
Motivation: 近年来独立同分布无限均值随机变量的随机占优受关注，已有文献确定了一些包含常见重尾分布的非负随机变量分布类，本文旨在系统研究这些分布类性质和关系并扩展结果到更实际场景。

Method: 系统研究分布类性质和包含关系，分析随机变量服从复合二项分布的情况。

Result: 确定了分布类的性质和包含关系，扩展了已有结果，建立了复合二项分布情形下随机占优关系的充要条件。

Conclusion: 对独立同分布无限均值随机变量随机占优相关分布类有更深入理解，扩展结果适用于更实际场景。

Abstract: In recent years, stochastic dominance for independent and identically
distributed (iid) infinite-mean random variables has received considerable
attention. The literature has identified several classes of distributions of
nonnegative random variables that encompass many common heavy-tailed
distributions. A key result demonstrates that the weighted sum of iid random
variables from these classes is stochastically larger than any individual
random variable in the sense of the first-order stochastic dominance. This
paper systematically investigates the properties and inclusion relationships
among these distribution classes, and extends some existing results to more
practical scenarios. Furthermore, we analyze the case where each random
variable follows a compound binomial distribution, establishing necessary and
sufficient conditions for the preservation of the aforementioned stochastic
dominance relation.

</details>


### [499] [Information-theoretic minimax and submodular optimization algorithms for multivariate Markov chains](https://arxiv.org/abs/2511.00769)
*Zheyuan Lai,Michael C. H. Choi*

Main category: math.PR

TL;DR: 研究有限多元马尔可夫链信息论极小极大问题，通过对偶和恒等式转化问题，提出算法求解，实验验证算法实用性并揭示稀疏最优结构。


<details>
  <summary>Details</summary>
Motivation: 最小化有限多元马尔可夫链在给定转移矩阵族和可分解模型类下的最坏情况信息损失。

Method: 通过强对偶和毕达哥拉斯恒等式将极小极大问题转化为凹最大化问题，提出投影次梯度算法求解；将问题转化为正交子模函数，考虑最大 - 最小 - 最大子模优化问题并采用两层次梯度 - 贪心算法求解。

Result: 证明了信息论博弈中混合策略纳什均衡存在；提出的算法有可证明的保证；数值实验验证了算法实用性，揭示了稀疏最优结构。

Conclusion: 所提出的算法能有效解决有限多元马尔可夫链的信息论极小极大问题，在实际模型中有应用价值。

Abstract: We study an information-theoretic minimax problem for finite multivariate
Markov chains on $d$-dimensional product state spaces. Given a family $\mathcal
B=\{P_1,\ldots,P_n\}$ of $\pi$-stationary transition matrices and a class
$\mathcal F = \mathcal{F}(\mathbf{S})$ of factorizable models induced by a
partition $\mathbf S$ of the coordinate set $[d]$, we seek to minimize the
worst-case information loss by analyzing $$\min_{Q\in\mathcal
F}\max_{P\in\mathcal B} D_{\mathrm{KL}}^{\pi}(P\|Q),$$ where
$D_{\mathrm{KL}}^{\pi}(P\|Q)$ is the $\pi$-weighted KL divergence from $Q$ to
$P$. We recast the above minimax problem into concave maximization over the
$n$-probability-simplex via strong duality and Pythagorean identities that we
derive. This leads us to formulate an information-theoretic game and show that
a mixed strategy Nash equilibrium always exists; and propose a projected
subgradient algorithm to approximately solve the minimax problem with provable
guarantee. By transforming the minimax problem into an orthant submodular
function in $\mathbf{S}$, this motivates us to consider a max-min-max
submodular optimization problem and investigate a two-layer subgradient-greedy
procedure to approximately solve this generalization. Numerical experiments for
Markov chains on the Curie-Weiss and Bernoulli-Laplace models illustrate the
practicality of these proposed algorithms and reveals sparse optimal structures
in these examples.

</details>


### [500] [Stability of the Kim--Milman flow map](https://arxiv.org/abs/2511.01154)
*Sinho Chewi,Aram-Alexandre Pooladian,Matthew S. Zhang*

Main category: math.PR

TL;DR: 研究Kim - Milman流映射（概率流ODE）关于目标测度变化的稳定性，发现稳定性与相对Fisher信息有关而非Wasserstein距离。


<details>
  <summary>Details</summary>
Motivation: 刻画Kim - Milman流映射关于目标测度变化的稳定性。

Method: 未提及

Result: 稳定性与相对Fisher信息有关，而非Wasserstein距离。

Conclusion: 得出Kim - Milman流映射稳定性与相对Fisher信息的关系。

Abstract: In this short note, we characterize stability of the Kim--Milman flow map --
also known as the probability flow ODE -- with respect to variations in the
target measure. Rather than the Wasserstein distance, we show that stability
holds with respect to the relative Fisher information

</details>


<div id='stat.OT'></div>

# stat.OT [[Back]](#toc)

### [501] [From Path Coefficients to Targeted Estimands: A Comparison of Structural Equation Models (SEM) and Targeted Maximum Likelihood Estimation (TMLE)](https://arxiv.org/abs/2511.01040)
*Junjie Ma,Xiaoya Zhang,Guangye He,Yuting Han,Ting Ge,Feng Ji*

Main category: stat.OT

TL;DR: 本文回顾基于非参数结构方程模型（SEM）的有针对性的最大似然估计（TMLE），通过模拟研究和实际数据集分析对比TMLE和SEM，表明TMLE在模型错误设定时表现更优，并给出应用指导。


<details>
  <summary>Details</summary>
Motivation: 突破SEM的参数假设，寻找更优的因果效应估计方法。

Method: 进行模拟研究，在正确和错误设定模型条件下实施SEM和TMLE估计因果效应，并将两种方法应用于实际数据集分析贫困对高中入学机会的中介效应。

Result: 模拟研究证实TMLE在模型错误设定时，在偏差、均方误差和置信区间有效性方面始终优于SEM；实际数据集分析显示TMLE下直接效应不再显著，而SEM显示显著。

Conclusion: 结合目标学习在因果推断中的最新发展，给出使用SEM和TMLE的实用指导。

Abstract: Structural Equation Modeling (SEM) has gained popularity in the social
sciences and causal inference due to its flexibility in modeling complex
relationships between variables and its availability in modern statistical
software. To move beyond the parametric assumptions of SEM, this paper reviews
targeted maximum likelihood estimation (TMLE), a doubly robust, machine
learning-based approach that builds on nonparametric SEM. We demonstrate that
both TMLE and SEM can be used to estimate standard causal effects and show that
TMLE is robust to model misspecification. We conducted simulation studies under
both correct and misspecified model conditions, implementing SEM and TMLE to
estimate these causal effects. The simulations confirm that TMLE consistently
outperforms SEM under misspecification in terms of bias, mean squared error,
and the validity of confidence intervals. We applied both approaches to a
real-world dataset to analyze the mediation effects of poverty on access to
high school, revealing that the direct effect is no longer significant under
TMLE, whereas SEM indicates significance. We conclude with practical guidance
on using SEM and TMLE in light of recent developments in targeted learning for
causal inference.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [502] [Fix: externalizing network I/O in serverless computing](https://arxiv.org/abs/2511.00205)
*Yuhan Deng,Akshay Srivatsan,Sebastian Ingino,Francis Chua,Yasmine Mitchell,Matthew Vilaysack,Keith Winstein*

Main category: cs.OS

TL;DR: 介绍了一种无服务器计算系统，其用户、程序和平台共享计算的通用表示，能减少饥饿并转变服务模式。


<details>
  <summary>Details</summary>
Motivation: 构建一种高效的无服务器计算系统，优化任务调度和服务模式。

Method: 让用户、程序和平台共享计算的通用表示，将I/O外部化，由平台负责网络数据移动。

Result: 应用程序可描述各阶段所需数据，有助于提供商调度任务和网络传输，减少饥饿。

Conclusion: 该设计为外包计算提供了端到端论证，将服务模式从“按努力付费”转变为“按结果付费”。

Abstract: We describe a system for serverless computing where users, programs,
  and the underlying platform share a common representation of a
  computation: a deterministic procedure, run in an environment
  of well-specified data or the outputs of other computations. This
  representation externalizes I/O: data movement over the network is
  performed exclusively by the platform. Applications can describe the
  precise data needed at each stage, helping the provider schedule
  tasks and network transfers to reduce starvation. The design
  suggests an end-to-end argument for outsourced computing, shifting
  the service model from ``pay-for-effort'' to ``pay-for-results.''

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [503] [Using machine learning methods to predict cognitive age from psychophysiological tests](https://arxiv.org/abs/2511.00013)
*Daria D. Tyurina,Sergey V. Stasenko,Konstantin V. Lushnikov,Maria V. Vedunova*

Main category: q-bio.NC

TL;DR: 研究提出用心理生理测试预测认知年龄的新方法，经测试和参数计算训练机器学习算法，成果有助于远程筛查。


<details>
  <summary>Details</summary>
Motivation: 提出预测认知年龄的新方法，推动利用移动设备进行人类健康远程筛查领域发展。

Method: 让受试者完成一系列心理测试，计算多项参数，对参数预处理后训练机器学习回归算法。

Result: 成功训练机器学习算法用于预测认知年龄。

Conclusion: 研究成果对利用移动设备进行人类健康远程筛查以诊断和监测认知老化有贡献。

Abstract: This study introduces a novel method for predicting cognitive age using
psychophysiological tests. To determine cognitive age, subjects were asked to
complete a series of psychological tests measuring various cognitive functions,
including reaction time and cognitive conflict, short-term memory, verbal
functions, and color and spatial perception. Based on the tests completed, the
average completion time, proportion of correct answers, average absolute delta
of the color campimetry test, number of guessed words in the M\"unsterberg
matrix, and other parameters were calculated for each subject. The obtained
characteristics of the subjects were preprocessed and used to train a machine
learning algorithm implementing a regression task for predicting a person's
cognitive age. These findings contribute to the field of remote screening using
mobile devices for human health for diagnosing and monitoring cognitive aging.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [504] [Benchmarking Federated Learning Frameworks for Medical Imaging Deployment: A Comparative Study of NVIDIA FLARE, Flower, and Owkin Substra](https://arxiv.org/abs/2511.00037)
*Riya Gupta,Alexander Chowdhury,Sahil Nalawade*

Main category: cs.CV

TL;DR: 本文对NVIDIA FLARE、Flower和Owkin Substra三个联邦学习框架在医学成像应用中的适用性进行基准测试，指出各框架优势。


<details>
  <summary>Details</summary>
Motivation: 评估三个联邦学习框架在现实环境医学成像应用中的适用性。

Method: 使用PathMNIST数据集，评估模型性能、收敛效率、通信开销、可扩展性和开发者体验。

Result: NVIDIA FLARE生产可扩展性优越，Flower适合原型设计和学术研究，Owkin Substra隐私和合规性出色。

Conclusion: 各框架有适合不同用例的优势，对医疗环境实际部署有重要意义。

Abstract: Federated Learning (FL) has emerged as a transformative paradigm in medical
AI, enabling collaborative model training across institutions without direct
data sharing. This study benchmarks three prominent FL frameworks NVIDIA FLARE,
Flower, and Owkin Substra to evaluate their suitability for medical imaging
applications in real-world settings. Using the PathMNIST dataset, we assess
model performance, convergence efficiency, communication overhead, scalability,
and developer experience. Results indicate that NVIDIA FLARE offers superior
production scalability, Flower provides flexibility for prototyping and
academic research, and Owkin Substra demonstrates exceptional privacy and
compliance features. Each framework exhibits strengths optimized for distinct
use cases, emphasizing their relevance to practical deployment in healthcare
environments.

</details>


### [505] [AI Powered High Quality Text to Video Generation with Enhanced Temporal Consistency](https://arxiv.org/abs/2511.00107)
*Piyushkumar Patel*

Main category: cs.CV

TL;DR: 提出MOVAI框架用于文本到视频合成，有三项创新，实验显示性能达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频生成方法在时间一致性、组合理解和视觉叙事细粒度控制方面存在困难。

Method: 提出MOVAI框架，包含组合场景解析器（CSP）、时空注意力机制（TSAM）和渐进式视频细化（PVR）模块。

Result: 在标准基准测试中，与现有方法相比，LPIPS视频质量指标提高15.3%，FVD提高12.7%，用户偏好研究提高18.9%。

Conclusion: MOVAI框架在生成具有逼真时间动态和细粒度语义控制的复杂多对象场景方面表现出色。

Abstract: Text to video generation has emerged as a critical frontier in generative
artificial intelligence, yet existing approaches struggle with maintaining
temporal consistency, compositional understanding, and fine grained control
over visual narratives. We present MOVAI (Multimodal Original Video AI), a
novel hierarchical framework that integrates compositional scene understanding
with temporal aware diffusion models for high fidelity text to video synthesis.
Our approach introduces three key innovations: (1) a Compositional Scene Parser
(CSP) that decomposes textual descriptions into hierarchical scene graphs with
temporal annotations, (2) a Temporal-Spatial Attention Mechanism (TSAM) that
ensures coherent motion dynamics across frames while preserving spatial
details, and (3) a Progressive Video Refinement (PVR) module that iteratively
enhances video quality through multi-scale temporal reasoning. Extensive
experiments on standard benchmarks demonstrate that MOVAI achieves
state-of-the-art performance, improving video quality metrics by 15.3% in
LPIPS, 12.7% in FVD, and 18.9% in user preference studies compared to existing
methods. Our framework shows particular strength in generating complex
multi-object scenes with realistic temporal dynamics and fine-grained semantic
control.

</details>


### [506] [Boosting performance of computer vision applications through embedded GPUs on the edge](https://arxiv.org/abs/2511.01129)
*Fabio Diniz Rossi*

Main category: cs.CV

TL;DR: 为使计算机视觉应用在资源有限设备上运行，提出用带GPU的嵌入式设备，实验表明GPU比仅用CPU有性能提升。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉应用资源需求大，边缘计算设备容量有限影响用户体验，需解决在资源有限设备上的使用问题。

Method: 使用带有图形处理单元（GPUs）的嵌入式设备来卸载高密集任务。

Result: 实验表明，与仅使用CPU相比，GPU能获得性能提升。

Conclusion: 使用带GPU的嵌入式设备可克服边缘计算设备容量限制，保证用户更好体验。

Abstract: Computer vision applications, especially those using augmented reality
technology, are becoming quite popular in mobile devices. However, this type of
application is known as presenting significant demands regarding resources. In
order to enable its utilization in devices with more modest resources, edge
computing can be used to offload certain high intensive tasks. Still, edge
computing is usually composed of devices with limited capacity, which may
impact in users quality of experience when using computer vision applications.
This work proposes the use of embedded devices with graphics processing units
(GPUs) to overcome such limitation. Experiments performed shown that GPUs can
attain a performance gain when compared to using only CPUs, which guarantee a
better experience to users using such kind of application.

</details>


### [507] [Vote-in-Context: Turning VLMs into Zero-Shot Rank Fusers](https://arxiv.org/abs/2511.01617)
*Mohamed Eltahir,Ali Habibullah,Lama Ayash,Tanveer Hussain,Naeemullah Khan*

Main category: cs.CV

TL;DR: 本文提出Vote - in - Context (ViC)框架，将列表重排和融合视为零样本推理任务，应用于跨模态视频检索，在多个基准测试中取得新的零样本检索性能。


<details>
  <summary>Details</summary>
Motivation: 在检索领域，异构检索器候选结果的融合是长期挑战，尤其是对于视频等复杂多模态数据，现有典型融合技术仅依赖排名或分数信号，忽略候选表示。

Method: 引入ViC框架，将内容证据和检索器元数据序列化到视觉语言模型（VLM）的提示中，使用S - Grid表示视频，进行列表推理。

Result: 在多个视频检索基准测试中，ViC作为单列表重排器显著提高单个检索器的精度，作为集成融合器始终优于强基线，在零样本设置下取得高召回率。

Conclusion: ViC是将现代VLM转变为强大零样本重排器和融合器的简单、可复现且高效的方法。

Abstract: In the retrieval domain, candidates' fusion from heterogeneous retrievers is
a long-standing challenge, particularly for complex, multi-modal data such as
videos. While typical fusion techniques are training-free, they rely solely on
rank or score signals, disregarding candidates' representations. This work
introduces Vote-in-Context (ViC), a generalized, training-free framework that
re-thinks list-wise reranking and fusion as a zero-shot reasoning task for a
Vision-Language Model (VLM). The core insight is to serialize both content
evidence and retriever metadata directly within the VLM's prompt, allowing the
model to adaptively weigh retriever consensus against visual-linguistic
content. We demonstrate the generality of this framework by applying it to the
challenging domain of cross-modal video retrieval. To this end, we introduce
the S-Grid, a compact serialization map that represents each video as an image
grid, optionally paired with subtitles to enable list-wise reasoning over video
candidates. ViC is evaluated both as a single-list reranker, where it
dramatically improves the precision of individual retrievers, and as an
ensemble fuser, where it consistently outperforms strong baselines like
CombSUM. Across video retrieval benchmarks including ActivityNet and VATEX, the
framework establishes new state-of-the-art zero-shot retrieval performance,
demonstrating its effectiveness in handling complex visual and temporal signals
alongside text. In zero-shot settings, ViC achieves Recall@1 scores of 87.1%
(t2v) / 89.0% (v2t) on MSR-VTT and 99.6% (v2t) on VATEX, representing massive
gains of up to +40 Recall@1 over previous state-of-the-art baselines. We
present ViC as a simple, reproducible, and highly effective recipe for turning
modern VLMs into powerful zero-shot rerankers and fusers. Code and resources
are publicly available at: https://github.com/mohammad2012191/ViC

</details>


### [508] [Generative human motion mimicking through feature extraction in denoising diffusion settings](https://arxiv.org/abs/2511.00011)
*Alexander Okupnik,Johannes Schneider,Kyriakos Flouris*

Main category: cs.CV

TL;DR: 本文构建基于动作捕捉数据的交互模型，探索以舞蹈为例的创意人机交互，模型生成多样且逼真的舞蹈动作，是创意人机共舞的第一步。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型缺乏具身交互特性，而舞蹈作为人类表达的原始形式可弥补这一体验，因此探索以舞蹈为例的创意人机交互。

Method: 构建基于动作捕捉数据的交互模型，结合两个扩散模型、动作修复和动作风格迁移的思想，利用单人动作数据和高级特征生成动作表征。

Result: 通过定量评估生成样本与模拟人类表演者的测试集的特征分布收敛性，证明了模型的成功，生成的舞蹈动作多样且逼真。

Conclusion: 模型的生成结果是创意人机共舞的第一步。

Abstract: Recent success with large language models has sparked a new wave of verbal
human-AI interaction. While such models support users in a variety of creative
tasks, they lack the embodied nature of human interaction. Dance, as a primal
form of human expression, is predestined to complement this experience. To
explore creative human-AI interaction exemplified by dance, we build an
interactive model based on motion capture (MoCap) data. It generates an
artificial other by partially mimicking and also "creatively" enhancing an
incoming sequence of movement data. It is the first model, which leverages
single-person motion data and high level features in order to do so and, thus,
it does not rely on low level human-human interaction data. It combines ideas
of two diffusion models, motion inpainting, and motion style transfer to
generate movement representations that are both temporally coherent and
responsive to a chosen movement reference. The success of the model is
demonstrated by quantitatively assessing the convergence of the feature
distribution of the generated samples and the test set which serves as
simulating the human performer. We show that our generations are first steps to
creative dancing with AI as they are both diverse showing various deviations
from the human partner while appearing realistic.

</details>


### [509] [Deep Learning Models for Coral Bleaching Classification in Multi-Condition Underwater Image Datasets](https://arxiv.org/abs/2511.00021)
*Julio Jerison E. Macrohon,Gordon Hung*

Main category: cs.CV

TL;DR: 本文提出基于全球多样数据集的珊瑚白化分类系统，对比三种模型，CNN 准确率达 88%，为珊瑚监测提供见解。


<details>
  <summary>Details</summary>
Motivation: 珊瑚礁面临污染、酸化等威胁，需有效保护和监测。

Method: 基于全球多样数据集构建机器学习珊瑚白化分类系统，对比 ResNet、ViT 和 CNN 三种模型并调参。

Result: CNN 模型经调参后准确率达 88%，优于现有基准。

Conclusion: 研究为珊瑚自主监测提供重要见解，对常用计算机视觉模型进行综合分析。

Abstract: Coral reefs support numerous marine organisms and are an important source of
coastal protection from storms and floods, representing a major part of marine
ecosystems. However coral reefs face increasing threats from pollution, ocean
acidification, and sea temperature anomalies, making efficient protection and
monitoring heavily urgent. Therefore, this study presents a novel
machine-learning-based coral bleaching classification system based on a diverse
global dataset with samples of healthy and bleached corals under varying
environmental conditions, including deep seas, marshes, and coastal zones. We
benchmarked and compared three state-of-the-art models: Residual Neural Network
(ResNet), Vision Transformer (ViT), and Convolutional Neural Network (CNN).
After comprehensive hyperparameter tuning, the CNN model achieved the highest
accuracy of 88%, outperforming existing benchmarks. Our findings offer
important insights into autonomous coral monitoring and present a comprehensive
analysis of the most widely used computer vision models.

</details>


### [510] [Mutual Information guided Visual Contrastive Learning](https://arxiv.org/abs/2511.00028)
*Hanyang Chen,Yanchao Yang*

Main category: cs.CV

TL;DR: 本文研究基于互信息选择训练数据用于表征学习，在多个基准测试中验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 现有表征学习方法的数据选择和增强依赖人工假设或工程，可能不是最优的，希望找到更好的方法让学习到的特征在开放环境有更好的泛化能力。

Method: 基于真实世界分布计算数据的互信息来选择训练数据，将在自然扰动下有高互信息的场景块作为正样本进行对比损失学习。

Result: 在多个基准测试和多个先进表征学习框架中验证了互信息引导的数据增强方法的有效性。

Conclusion: 该方法是一个有前景的未来研究方向。

Abstract: Representation learning methods utilizing the InfoNCE loss have demonstrated
considerable capacity in reducing human annotation effort by training invariant
neural feature extractors. Although different variants of the training
objective adhere to the information maximization principle between the data and
learned features, data selection and augmentation still rely on human
hypotheses or engineering, which may be suboptimal. For instance, data
augmentation in contrastive learning primarily focuses on color jittering,
aiming to emulate real-world illumination changes. In this work, we investigate
the potential of selecting training data based on their mutual information
computed from real-world distributions, which, in principle, should endow the
learned features with better generalization when applied in open environments.
Specifically, we consider patches attached to scenes that exhibit high mutual
information under natural perturbations, such as color changes and motion, as
positive samples for learning with contrastive loss. We evaluate the proposed
mutual-information-informed data augmentation method on several benchmarks
across multiple state-of-the-art representation learning frameworks,
demonstrating its effectiveness and establishing it as a promising direction
for future research.

</details>


### [511] [World Simulation with Video Foundation Models for Physical AI](https://arxiv.org/abs/2511.00062)
*NVIDIA,:,Arslan Ali,Junjie Bai,Maciej Bala,Yogesh Balaji,Aaron Blakeman,Tiffany Cai,Jiaxin Cao,Tianshi Cao,Elizabeth Cha,Yu-Wei Chao,Prithvijit Chattopadhyay,Mike Chen,Yongxin Chen,Yu Chen,Shuai Cheng,Yin Cui,Jenna Diamond,Yifan Ding,Jiaojiao Fan,Linxi Fan,Liang Feng,Francesco Ferroni,Sanja Fidler,Xiao Fu,Ruiyuan Gao,Yunhao Ge,Jinwei Gu,Aryaman Gupta,Siddharth Gururani,Imad El Hanafi,Ali Hassani,Zekun Hao,Jacob Huffman,Joel Jang,Pooya Jannaty,Jan Kautz,Grace Lam,Xuan Li,Zhaoshuo Li,Maosheng Liao,Chen-Hsuan Lin,Tsung-Yi Lin,Yen-Chen Lin,Huan Ling,Ming-Yu Liu,Xian Liu,Yifan Lu,Alice Luo,Qianli Ma,Hanzi Mao,Kaichun Mo,Seungjun Nah,Yashraj Narang,Abhijeet Panaskar,Lindsey Pavao,Trung Pham,Morteza Ramezanali,Fitsum Reda,Scott Reed,Xuanchi Ren,Haonan Shao,Yue Shen,Stella Shi,Shuran Song,Bartosz Stefaniak,Shangkun Sun,Shitao Tang,Sameena Tasmeen,Lyne Tchapmi,Wei-Cheng Tseng,Jibin Varghese,Andrew Z. Wang,Hao Wang,Haoxiang Wang,Heng Wang,Ting-Chun Wang,Fangyin Wei,Jiashu Xu,Dinghao Yang,Xiaodong Yang,Haotian Ye,Seonghyeon Ye,Xiaohui Zeng,Jing Zhang,Qinsheng Zhang,Kaiwen Zheng,Andrew Zhu,Yuke Zhu*

Main category: cs.CV

TL;DR: 介绍最新的物理AI模型Cosmos - Predict2.5和Cosmos - Transfer2.5，它们性能提升，开源以促进物理AI研究。


<details>
  <summary>Details</summary>
Motivation: 推动物理AI发展，为机器人和自主系统提供更可靠的合成数据生成、策略评估和闭环模拟等功能，加速具身智能研究与部署。

Method: 基于流架构构建Cosmos - Predict2.5，统一多种生成功能，利用Cosmos - Reason1，在2亿视频片段上训练并强化学习微调；推出控制网络风格的Cosmos - Transfer2.5。

Result: Cosmos - Predict2.5在视频质量和指令对齐上比Cosmos - Predict1有显著提升，Cosmos - Transfer2.5更小但生成视频保真度更高、更稳健。

Conclusion: Cosmos - Predict2.5和Cosmos - Transfer2.5是扩展具身智能的通用工具，开源资源可降低采用门槛，促进下一代具身智能创新。

Abstract: We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World
Foundation Models for Physical AI. Built on a flow-based architecture,
[Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation
in a single model and leverages [Cosmos-Reason1], a Physical AI vision-language
model, to provide richer text grounding and finer control of world simulation.
Trained on 200M curated video clips and refined with reinforcement
learning-based post-training, [Cosmos-Predict2.5] achieves substantial
improvements over [Cosmos-Predict1] in video quality and instruction alignment,
with models released at 2B and 14B scales. These capabilities enable more
reliable synthetic data generation, policy evaluation, and closed-loop
simulation for robotics and autonomous systems. We further extend the family
with [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and
Real2Real world translation. Despite being 3.5$\times$ smaller than
[Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video
generation. Together, these advances establish [Cosmos-Predict2.5] and
[Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To
accelerate research and deployment in Physical AI, we release source code,
pretrained checkpoints, and curated benchmarks under the NVIDIA Open Model
License at https://github.com/nvidia-cosmos/cosmos-predict2.5 and
https://github.com/nvidia-cosmos/cosmos-transfer2.5. We hope these open
resources lower the barrier to adoption and foster innovation in building the
next generation of embodied intelligence.

</details>


### [512] [LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation](https://arxiv.org/abs/2511.00090)
*Huanlin Gao,Ping Chen,Fuyuan Shi,Chao Tan,Zhaoxiang Liu,Fang Zhao,Kai Wang,Shiguo Lian*

Main category: cs.CV

TL;DR: 提出无训练且高效的扩散视频生成加速框架LeMiCa，实验证明其在推理速度和生成质量上双提升。


<details>
  <summary>Details</summary>
Motivation: 现有缓存策略常忽略全局误差累积，导致加速视频与原视频内容明显退化。

Method: 将缓存调度公式化为带误差加权边的有向图，引入字典序极小极大路径优化策略。

Result: 在多个文本到视频基准测试中，LeMiCa实现推理速度和生成质量双提升，如在Latte模型上实现2.9倍加速，在Open - Sora上LPIPS分数达0.05。

Conclusion: LeMiCa是加速扩散视频生成的强大且通用范式，可作为未来高效可靠视频合成研究的基础。

Abstract: We present LeMiCa, a training-free and efficient acceleration framework for
diffusion-based video generation. While existing caching strategies primarily
focus on reducing local heuristic errors, they often overlook the accumulation
of global errors, leading to noticeable content degradation between accelerated
and original videos. To address this issue, we formulate cache scheduling as a
directed graph with error-weighted edges and introduce a Lexicographic Minimax
Path Optimization strategy that explicitly bounds the worst-case path error.
This approach substantially improves the consistency of global content and
style across generated frames. Extensive experiments on multiple text-to-video
benchmarks demonstrate that LeMiCa delivers dual improvements in both inference
speed and generation quality. Notably, our method achieves a 2.9x speedup on
the Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming
prior caching techniques. Importantly, these gains come with minimal perceptual
quality degradation, making LeMiCa a robust and generalizable paradigm for
accelerating diffusion-based video generation. We believe this approach can
serve as a strong foundation for future research on efficient and reliable
video synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa

</details>


### [513] [SpinalSAM-R1: A Vision-Language Multimodal Interactive System for Spine CT Segmentation](https://arxiv.org/abs/2511.00095)
*Jiaming Liu,Dingwei Fan,Junyong Zhao,Chunlin Li,Haipeng Si,Liang Sun*

Main category: cs.CV

TL;DR: 提出SpinalSAM - R1用于脊柱CT图像分割，实验效果好，还开发交互软件并开源


<details>
  <summary>Details</summary>
Motivation: 脊柱CT图像分割因对比度低、边界复杂受阻碍，现有模型如SAM在脊柱CT成像中存在高标注要求和领域适应性差的问题

Method: 提出多模态视觉 - 语言交互系统SpinalSAM - R1，集成微调的SAM和DeepSeek - R1，引入解剖引导注意力机制和语义驱动交互协议，使用LoRA微调

Result: 方法实现了优越的分割性能，开发的软件支持11种临床操作，解析准确率94.3%，响应时间低于800ms

Conclusion: SpinalSAM - R1在脊柱CT图像分割中表现良好，开发的软件有实际应用价值

Abstract: The anatomical structure segmentation of the spine and adjacent structures
from computed tomography (CT) images is a key step for spinal disease diagnosis
and treatment. However, the segmentation of CT images is impeded by low
contrast and complex vertebral boundaries. Although advanced models such as the
Segment Anything Model (SAM) have shown promise in various segmentation tasks,
their performance in spinal CT imaging is limited by high annotation
requirements and poor domain adaptability. To address these limitations, we
propose SpinalSAM-R1, a multimodal vision-language interactive system that
integrates a fine-tuned SAM with DeepSeek-R1, for spine CT image segmentation.
Specifically, our SpinalSAM-R1 introduces an anatomy-guided attention mechanism
to improve spine segmentation performance, and a semantics-driven interaction
protocol powered by DeepSeek-R1, enabling natural language-guided refinement.
The SpinalSAM-R1 is fine-tuned using Low-Rank Adaptation (LoRA) for efficient
adaptation. We validate our SpinalSAM-R1 on the spine anatomical structure with
CT images. Experimental results suggest that our method achieves superior
segmentation performance. Meanwhile, we develop a PyQt5-based interactive
software, which supports point, box, and text-based prompts. The system
supports 11 clinical operations with 94.3\% parsing accuracy and sub-800 ms
response times. The software is released on
https://github.com/6jm233333/spinalsam-r1.

</details>


### [514] [A filtering scheme for confocal laser endomicroscopy (CLE)-video sequences for self-supervised learning](https://arxiv.org/abs/2511.00098)
*Nils Porsche,Flurin Müller-Diesing,Sweta Banerjee,Miguel Goncalves,Marc Aubreville*

Main category: cs.CV

TL;DR: 提出在CLE视频序列上使用过滤功能以减少自监督学习（SSL）训练时数据集冗余，提高训练效率和收敛性，在两个数据集上验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: CLE诊断图像难解读，机器学习因缺乏病理相关图像序列易过拟合，且CLE视频帧间相关性高，数据分布非分层，影响SSL训练。

Method: 提出在CLE视频序列上的过滤功能，用四个先进基线网络和带视觉变压器小骨干的SSL师生网络评估，在两个数据集下游任务评估。

Result: 过滤后的SSL预训练模型在两个数据集测试准确率最高，分别为67.48%和73.52%，显著优于非SSL基线。

Conclusion: SSL是CLE预训练有效方法，提出的CLE视频过滤器可提高自监督场景训练效率，减少67%训练时间。

Abstract: Confocal laser endomicroscopy (CLE) is a non-invasive, real-time imaging
modality that can be used for in-situ, in-vivo imaging and the microstructural
analysis of mucous structures. The diagnosis using CLE is, however, complicated
by images being hard to interpret for non-experienced physicians. Utilizing
machine learning as an augmentative tool would hence be beneficial, but is
complicated by the shortage of histopathology-correlated CLE imaging sequences
with respect to the plurality of patterns in this domain, leading to
overfitting of machine learning models. To overcome this, self-supervised
learning (SSL) can be employed on larger unlabeled datasets. CLE is a
video-based modality with high inter-frame correlation, leading to a
non-stratified data distribution for SSL training. In this work, we propose a
filter functionality on CLE video sequences to reduce the dataset redundancy in
SSL training and improve SSL training convergence and training efficiency. We
use four state-of-the-art baseline networks and a SSL teacher-student network
with a vision transformer small backbone for the evaluation. These networks
were evaluated on downstream tasks for a sinonasal tumor dataset and a squamous
cell carcinoma of the skin dataset. On both datasets, we found the highest test
accuracy on the filtered SSL-pretrained model, with 67.48% and 73.52%, both
considerably outperforming their non-SSL baselines. Our results show that SSL
is an effective method for CLE pretraining. Further, we show that our proposed
CLE video filter can be utilized to improve training efficiency in
self-supervised scenarios, resulting in a reduction of 67% in training time.

</details>


### [515] [FreeSliders: Training-Free, Modality-Agnostic Concept Sliders for Fine-Grained Diffusion Control in Images, Audio, and Video](https://arxiv.org/abs/2511.00103)
*Rotem Ezra,Hedi Zisling,Nimrod Berman,Ilan Naiman,Alexey Gorkor,Liran Nochumsohn,Eliya Nachmani,Omri Azencot*

Main category: cs.CV

TL;DR: 本文提出无训练、跨模态的FreeSliders方法实现细粒度可控生成，扩展基准测试，提出评估指标，解决尺度选择等问题，实验证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型实现细粒度可控生成有挑战，Concept Sliders存在需训练和特定架构微调、扩展性差的问题。

Method: 通过推理时部分估计CS公式实现无训练、跨模态的FreeSliders方法；扩展CS基准测试；提出评估属性和新指标；采用两阶段程序解决尺度选择和非线性遍历问题。

Result: 方法能跨模态即插即用、无训练地进行概念控制，优于现有基线。

Conclusion: 提出的FreeSliders方法为可控生成建立了新工具。

Abstract: Diffusion models have become state-of-the-art generative models for images,
audio, and video, yet enabling fine-grained controllable generation, i.e.,
continuously steering specific concepts without disturbing unrelated content,
remains challenging. Concept Sliders (CS) offer a promising direction by
discovering semantic directions through textual contrasts, but they require
per-concept training and architecture-specific fine-tuning (e.g., LoRA),
limiting scalability to new modalities. In this work we introduce FreeSliders,
a simple yet effective approach that is fully training-free and
modality-agnostic, achieved by partially estimating the CS formula during
inference. To support modality-agnostic evaluation, we extend the CS benchmark
to include both video and audio, establishing the first suite for fine-grained
concept generation control with multiple modalities. We further propose three
evaluation properties along with new metrics to improve evaluation quality.
Finally, we identify an open problem of scale selection and non-linear
traversals and introduce a two-stage procedure that automatically detects
saturation points and reparameterizes traversal for perceptually uniform,
semantically meaningful edits. Extensive experiments demonstrate that our
method enables plug-and-play, training-free concept control across modalities,
improves over existing baselines, and establishes new tools for principled
controllable generation. An interactive presentation of our benchmark and
method is available at: https://azencot-group.github.io/FreeSliders/

</details>


### [516] [Chain of Time: In-Context Physical Simulation with Image Generation Models](https://arxiv.org/abs/2511.00110)
*YingQiao Wang,Eric Bigelow,Boyi Li,Tomer Ullman*

Main category: cs.CV

TL;DR: 提出认知启发的“时间链”方法改进和解释视觉语言模型中的物理模拟，该方法无需额外微调，应用于多领域提升了图像生成模型性能，并揭示物理模拟动态。


<details>
  <summary>Details</summary>
Motivation: 受机器学习上下文推理和人类心理模拟启发，改进和解释视觉语言模型中的物理模拟。

Method: 提出“时间链”方法，在推理时生成一系列模拟中间图像，无需额外微调，应用于合成和真实世界领域。

Result: 使用“时间链”模拟显著提升了图像生成模型性能，分析揭示了传统评估隐藏的物理推理动态。

Conclusion: “时间链”方法有效提升图像生成模型物理模拟性能，同时暴露出模型在从输入图像推断特定物理参数上的不足。

Abstract: We propose a novel cognitively-inspired method to improve and interpret
physical simulation in vision-language models. Our ``Chain of Time" method
involves generating a series of intermediate images during a simulation, and it
is motivated by in-context reasoning in machine learning, as well as mental
simulation in humans. Chain of Time is used at inference time, and requires no
additional fine-tuning. We apply the Chain-of-Time method to synthetic and
real-world domains, including 2-D graphics simulations and natural 3-D videos.
These domains test a variety of particular physical properties, including
velocity, acceleration, fluid dynamics, and conservation of momentum. We found
that using Chain-of-Time simulation substantially improves the performance of a
state-of-the-art image generation model. Beyond examining performance, we also
analyzed the specific states of the world simulated by an image model at each
time step, which sheds light on the dynamics underlying these simulations. This
analysis reveals insights that are hidden from traditional evaluations of
physical reasoning, including cases where an image generation model is able to
simulate physical properties that unfold over time, such as velocity, gravity,
and collisions. Our analysis also highlights particular cases where the image
generation model struggles to infer particular physical parameters from input
images, despite being capable of simulating relevant physical processes.

</details>


### [517] [End-to-End Framework Integrating Generative AI and Deep Reinforcement Learning for Autonomous Ultrasound Scanning](https://arxiv.org/abs/2511.00114)
*Hanae Elmekki,Amanda Spilkin,Ehsan Zakeri,Antonela Mariel Zanuttini,Ahmed Alagha,Hani Sami,Jamal Bentahar,Lyes Kadem,Wen-Fang Xie,Philippe Pibarot,Rabeb Mizouni,Hadi Otrok,Azzam Mourad,Sami Muhaidat*

Main category: cs.CV

TL;DR: 本文提出首个集成生成式AI和DRL的端到端框架实现自主可重复的心脏超声扫描，还发布公开数据集并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 心脏超声诊断存在操作者依赖等问题，现有基于DRL的心脏超声扫描方法缺乏可重复性等，需新的解决方案。

Method: 提出集成生成式AI和DRL的端到端框架，含结合GANs与VAEs的条件生成模拟器和DRL模块。

Result: 框架可提供AI驱动指导、支持条件生成逼真超声图像、建立可扩展到其他器官的可重复基础，通过实验验证了有效性。

Conclusion: 所提框架能实现自主可重复的心脏超声扫描，具有可扩展性和可重复性。

Abstract: Cardiac ultrasound (US) is among the most widely used diagnostic tools in
cardiology for assessing heart health, but its effectiveness is limited by
operator dependence, time constraints, and human error. The shortage of trained
professionals, especially in remote areas, further restricts access. These
issues underscore the need for automated solutions that can ensure consistent,
and accessible cardiac imaging regardless of operator skill or location. Recent
progress in artificial intelligence (AI), especially in deep reinforcement
learning (DRL), has gained attention for enabling autonomous decision-making.
However, existing DRL-based approaches to cardiac US scanning lack
reproducibility, rely on proprietary data, and use simplified models. Motivated
by these gaps, we present the first end-to-end framework that integrates
generative AI and DRL to enable autonomous and reproducible cardiac US
scanning. The framework comprises two components: (i) a conditional generative
simulator combining Generative Adversarial Networks (GANs) with Variational
Autoencoders (VAEs), that models the cardiac US environment producing realistic
action-conditioned images; and (ii) a DRL module that leverages this simulator
to learn autonomous, accurate scanning policies. The proposed framework
delivers AI-driven guidance through expert-validated models that classify image
type and assess quality, supports conditional generation of realistic US
images, and establishes a reproducible foundation extendable to other organs.
To ensure reproducibility, a publicly available dataset of real cardiac US
scans is released. The solution is validated through several experiments. The
VAE-GAN is benchmarked against existing GAN variants, with performance assessed
using qualitative and quantitative approaches, while the DRL-based scanning
system is evaluated under varying configurations to demonstrate effectiveness.

</details>


### [518] [VLM6D: VLM based 6Dof Pose Estimation based on RGB-D Images](https://arxiv.org/abs/2511.00120)
*Md Selim Sarowar,Sungho Kim*

Main category: cs.CV

TL;DR: 提出VLM6D架构解决6D物体姿态估计难题，实验验证其在Occluded - LineMOD上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前6D物体姿态估计方法在真实场景（光照变化、无纹理物体、严重遮挡）下脆弱且泛化能力差。

Method: 提出VLM6D双流架构，用DINOv2处理RGB模态，PointNet++处理深度数据的3D点云，融合特征后通过多任务预测头输出。

Result: VLM6D在Occluded - LineMOD上取得新的SOTA性能。

Conclusion: VLM6D具有优越的鲁棒性和准确性。

Abstract: The primary challenge in computer vision is precisely calculating the pose of
6D objects, however many current approaches are still fragile and have trouble
generalizing from synthetic data to real-world situations with fluctuating
lighting, textureless objects, and significant occlusions. To address these
limitations, VLM6D, a novel dual-stream architecture that leverages the
distinct strengths of visual and geometric data from RGB-D input for robust and
precise pose estimation. Our framework uniquely integrates two specialized
encoders: a powerful, self-supervised Vision Transformer (DINOv2) processes the
RGB modality, harnessing its rich, pre-trained understanding of visual grammar
to achieve remarkable resilience against texture and lighting variations.
Concurrently, a PointNet++ encoder processes the 3D point cloud derived from
depth data, enabling robust geometric reasoning that excels even with the
sparse, fragmented data typical of severe occlusion. These complementary
feature streams are effectively fused to inform a multi task prediction head.
We demonstrate through comprehensive experiments that VLM6D obtained new SOTA
performance on the challenging Occluded-LineMOD, validating its superior
robustness and accuracy.

</details>


### [519] [FLoC: Facility Location-Based Efficient Visual Token Compression for Long Video Understanding](https://arxiv.org/abs/2511.00141)
*Janghoon Cho,Jungsoo Lee,Munawar Hayat,Kyuwoong Hwang,Fatih Porikli,Sungha Choi*

Main category: cs.CV

TL;DR: 本文提出基于设施定位函数的高效视觉令牌压缩框架FLoC，解决长视频理解中视觉令牌数量过多问题，评估显示其优于现有压缩技术。


<details>
  <summary>Details</summary>
Motivation: 现有长视频理解的视频-LMMs模型因长视频序列产生大量视觉令牌，可扩展性严重受限。

Method: 提出基于设施定位函数的FLoC框架，结合懒惰贪婪算法，在预定义视觉令牌数量预算内选择紧凑、有代表性和多样性的视觉令牌子集。

Result: 在Video - MME、MLVU和LongVideoBench等大规模基准测试中，该框架始终优于近期压缩技术。

Conclusion: FLoC框架能有效、稳健地解决长视频理解的关键挑战，且处理速度高效，具有训练无关、模型无关和查询无关的通用性。

Abstract: Recent studies in long video understanding have harnessed the advanced
visual-language reasoning capabilities of Large Multimodal Models (LMMs),
driving the evolution of video-LMMs specialized for processing extended video
sequences. However, the scalability of these models is severely limited by the
overwhelming volume of visual tokens generated from extended video sequences.
To address this challenge, this paper proposes FLoC, an efficient visual token
compression framework based on the facility location function, a principled
approach that swiftly selects a compact yet highly representative and diverse
subset of visual tokens within a predefined budget on the number of visual
tokens. By integrating the lazy greedy algorithm, our method achieves
remarkable efficiency gains by swiftly selecting a compact subset of tokens,
drastically reducing the number of visual tokens while guaranteeing
near-optimal performance. Notably, our approach is training-free,
model-agnostic, and query-agnostic, providing a versatile solution that
seamlessly integrates with diverse video-LLMs and existing workflows. Extensive
evaluations on large-scale benchmarks, such as Video-MME, MLVU, and
LongVideoBench, demonstrate that our framework consistently surpasses recent
compression techniques, highlighting not only its effectiveness and robustness
in addressing the critical challenges of long video understanding, but also its
efficiency in processing speed.

</details>


### [520] [A Retrospect to Multi-prompt Learning across Vision and Language](https://arxiv.org/abs/2511.00191)
*Ziliang Chen,Xin Huang,Quanlong Guan,Liang Lin,Weiqi Luo*

Main category: cs.CV

TL;DR: 本文对视觉语言多提示学习进行回顾，提出基于能量的多提示学习方法EMPL，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究多围绕单提示范式，很少探索多提示学习的技术潜力，因此本文旨在对视觉语言多提示学习进行回顾。

Method: 将恒定模态差距现象扩展到可学习提示，提出基于能量的多提示学习（EMPL），从能量分布中抽取实例生成多个提示嵌入。

Result: 实验证明EMPL不仅节省参数，还能在领域内和领域外开放词汇泛化之间取得平衡。

Conclusion: EMPL方法有效，具有一定优势。

Abstract: The vision community is undergoing the unprecedented progress with the
emergence of Vision-Language Pretraining Models (VLMs). Prompt learning plays
as the holy grail of accessing VLMs since it enables their fast adaptation to
downstream tasks with limited resources. Whereas existing researches milling
around single-prompt paradigms, rarely investigate the technical potential
behind their multi-prompt learning counterparts. This paper aims to provide a
principled retrospect for vision-language multi-prompt learning. We extend the
recent constant modality gap phenomenon to learnable prompts and then, justify
the superiority of vision-language transfer with multi-prompt augmentation,
empirically and theoretically. In terms of this observation, we propose an
Energy-based Multi-prompt Learning (EMPL) to generate multiple prompt
embeddings by drawing instances from an energy-based distribution, which is
implicitly defined by VLMs. So our EMPL is not only parameter-efficient but
also rigorously lead to the balance between in-domain and out-of-domain
open-vocabulary generalization. Comprehensive experiments have been conducted
to justify our claims and the excellence of EMPL.

</details>


### [521] [An Efficient and Generalizable Transfer Learning Method for Weather Condition Detection on Ground Terminals](https://arxiv.org/abs/2511.00211)
*Wenxuan Zhang,Peng Hu*

Main category: cs.CV

TL;DR: 本文提出一种高效迁移学习方法，可让地面组件本地检测天气状况，性能优于典型深度学习方法且具有泛化优势。


<details>
  <summary>Details</summary>
Motivation: 恶劣天气影响卫星互联网性能和可靠性，缺乏针对地面终端组件细粒度天气状况的检测方法。

Method: 提出一种高效的迁移学习（TL）方法，用于地面组件本地检测代表性天气相关状况。

Result: 该方法能检测雪、潮湿等天气状况，性能优于YOLOv7、YOLOv9等典型深度学习方法。

Conclusion: 所提迁移学习方法有效，且具有在不同场景下的泛化优势。

Abstract: The increasing adoption of satellite Internet with low-Earth-orbit (LEO)
satellites in mega-constellations allows ubiquitous connectivity to rural and
remote areas. However, weather events have a significant impact on the
performance and reliability of satellite Internet. Adverse weather events such
as snow and rain can disturb the performance and operations of satellite
Internet's essential ground terminal components, such as satellite antennas,
significantly disrupting the space-ground link conditions between LEO
satellites and ground stations. This challenge calls for not only region-based
weather forecasts but also fine-grained detection capability on ground terminal
components of fine-grained weather conditions. Such a capability can assist in
fault diagnostics and mitigation for reliable satellite Internet, but its
solutions are lacking, not to mention the effectiveness and generalization that
are essential in real-world deployments. This paper discusses an efficient
transfer learning (TL) method that can enable a ground component to locally
detect representative weather-related conditions. The proposed method can
detect snow, wet, and other conditions resulting from adverse and typical
weather events and shows superior performance compared to the typical deep
learning methods, such as YOLOv7, YOLOv9, Faster R-CNN, and R-YOLO. Our TL
method also shows the advantage of being generalizable to various scenarios.

</details>


### [522] [DM-QPMNET: Dual-modality fusion network for cell segmentation in quantitative phase microscopy](https://arxiv.org/abs/2511.00218)
*Rajatsubhra Chakraborty,Ana Espinosa-Momox,Riley Haskin,Depeng Xu,Rosario Porras-Aguilar*

Main category: cs.CV

TL;DR: 提出DM - QPMNet网络用于单帧定量相位显微镜（ssQPM）的细胞分割，该网络有双编码器，融合多模态特征，效果优于单模态和简单拼接方法。


<details>
  <summary>Details</summary>
Motivation: 传统阈值法对噪声和细胞密度敏感，简单通道拼接的深度学习方法无法利用偏振强度图像和相位图的互补性。

Method: 引入DM - QPMNet双编码器网络，通过多头注意力在中间深度融合模态特定特征，采用双源跳跃连接和每模态归一化。

Result: 相比整体拼接和单模态基线有显著改进。

Conclusion: 特定模态编码与可学习融合能有效利用ssQPM同时捕获的互补照明和相位线索进行稳健的细胞分割。

Abstract: Cell segmentation in single-shot quantitative phase microscopy (ssQPM) faces
challenges from traditional thresholding methods that are sensitive to noise
and cell density, while deep learning approaches using simple channel
concatenation fail to exploit the complementary nature of polarized intensity
images and phase maps. We introduce DM-QPMNet, a dual-encoder network that
treats these as distinct modalities with separate encoding streams. Our
architecture fuses modality-specific features at intermediate depth via
multi-head attention, enabling polarized edge and texture representations to
selectively integrate complementary phase information. This content-aware
fusion preserves training stability while adding principled multi-modal
integration through dual-source skip connections and per-modality normalization
at minimal overhead. Our approach demonstrates substantial improvements over
monolithic concatenation and single-modality baselines, showing that
modality-specific encoding with learnable fusion effectively exploits ssQPM's
simultaneous capture of complementary illumination and phase cues for robust
cell segmentation.

</details>


### [523] [FedReplay: A Feature Replay Assisted Federated Transfer Learning Framework for Efficient and Privacy-Preserving Smart Agriculture](https://arxiv.org/abs/2511.00269)
*Long Li,Jiajia Li,Dong Chen,Lina Pu,Haibo Yao,Yanbo Huang*

Main category: cs.CV

TL;DR: 提出结合CLIP ViT和轻量级分类器的联邦学习框架用于农业分类，可减少通信开销、保护隐私，实验准确率达86.6%，效果优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统集中式训练有隐私问题，标准联邦学习在非IID数据下有性能和通信成本问题。

Method: 提出集成冻结CLIP ViT和轻量级分类器的联邦学习框架，共享1% CLIP提取的特征表示。

Result: 在农业分类任务实验中准确率达86.6%，比基线联邦学习方法高4倍多。

Conclusion: 结合视觉语言模型特征和联邦学习用于农业智能，有效、高效且保护隐私、可扩展。

Abstract: Accurate classification plays a pivotal role in smart agriculture, enabling
applications such as crop monitoring, fruit recognition, and pest detection.
However, conventional centralized training often requires large-scale data
collection, which raises privacy concerns, while standard federated learning
struggles with non-independent and identically distributed (non-IID) data and
incurs high communication costs. To address these challenges, we propose a
federated learning framework that integrates a frozen Contrastive
Language-Image Pre-training (CLIP) vision transformer (ViT) with a lightweight
transformer classifier. By leveraging the strong feature extraction capability
of the pre-trained CLIP ViT, the framework avoids training large-scale models
from scratch and restricts federated updates to a compact classifier, thereby
reducing transmission overhead significantly. Furthermore, to mitigate
performance degradation caused by non-IID data distribution, a small subset
(1%) of CLIP-extracted feature representations from all classes is shared
across clients. These shared features are non-reversible to raw images,
ensuring privacy preservation while aligning class representation across
participants. Experimental results on agricultural classification tasks show
that the proposed method achieve 86.6% accuracy, which is more than 4 times
higher compared to baseline federated learning approaches. This demonstrates
the effectiveness and efficiency of combining vision-language model features
with federated learning for privacy-preserving and scalable agricultural
intelligence.

</details>


### [524] [Towards Automated Petrography](https://arxiv.org/abs/2511.00328)
*Isai Daniel Chacón,Paola Ruiz Puentes,Jillian Pearse,Pablo Arbeláez*

Main category: cs.CV

TL;DR: 提出用于自动岩相学的LITHOS框架，评估多种深度学习技术，提出双编码器变压器架构，证明偏振协同价值并公开基准。


<details>
  <summary>Details</summary>
Motivation: 传统岩相学劳动密集，需自动化技术。

Method: 引入LITHOS框架，评估多种深度学习技术，提出双编码器变压器架构。

Result: 提出的方法优于单偏振模型。

Conclusion: 公开LITHOS基准，促进自动岩相分析研究。

Abstract: Petrography is a branch of geology that analyzes the mineralogical
composition of rocks from microscopical thin section samples. It is essential
for understanding rock properties across geology, archaeology, engineering,
mineral exploration, and the oil industry. However, petrography is a
labor-intensive task requiring experts to conduct detailed visual examinations
of thin section samples through optical polarization microscopes, thus
hampering scalability and highlighting the need for automated techniques. To
address this challenge, we introduce the Large-scale Imaging and Thin section
Optical-polarization Set (LITHOS), the largest and most diverse publicly
available experimental framework for automated petrography. LITHOS includes
211,604 high-resolution RGB patches of polarized light and 105,802
expert-annotated grains across 25 mineral categories. Each annotation consists
of the mineral class, spatial coordinates, and expert-defined major and minor
axes represented as intersecting vector paths, capturing grain geometry and
orientation. We evaluate multiple deep learning techniques for mineral
classification in LITHOS and propose a dual-encoder transformer architecture
that integrates both polarization modalities as a strong baseline for future
reference. Our method consistently outperforms single-polarization models,
demonstrating the value of polarization synergy in mineral classification. We
have made the LITHOS Benchmark publicly available, comprising our dataset,
code, and pretrained models, to foster reproducibility and further research in
automated petrographic analysis.

</details>


### [525] [Detecting AI-Generated Images via Diffusion Snap-Back Reconstruction: A Forensic Approach](https://arxiv.org/abs/2511.00352)
*Mohd Ruhul Ameen,Akif Islam*

Main category: cs.CV

TL;DR: 本文提出基于扩散的取证框架鉴别AI生成图像，在4000张图像数据集上表现良好，具有强泛化性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 生成扩散模型兴起，传统深度伪造检测方法难以应对现代文本到图像系统，需新方法鉴别AI生成图像。

Method: 引入基于扩散的取证框架，利用多强度图像重建动态（扩散回跳），分析不同噪声强度下重建指标（LPIPS、SSIM和PSNR）的变化，提取基于流形的特征。

Result: 在4000张图像的平衡数据集上，交叉验证的AUROC达0.993，对压缩和噪声等常见失真保持鲁棒性。

Conclusion: 该方法虽使用有限数据和单一扩散主干，但具有强泛化性和可解释性，为可扩展、与模型无关的合成媒体取证奠定基础。

Abstract: The rapid rise of generative diffusion models has made distinguishing
authentic visual content from synthetic imagery increasingly challenging.
Traditional deepfake detection methods, which rely on frequency or pixel-level
artifacts, fail against modern text-to-image systems such as Stable Diffusion
and DALL-E that produce photorealistic and artifact-free results. This paper
introduces a diffusion-based forensic framework that leverages multi-strength
image reconstruction dynamics, termed diffusion snap-back, to identify
AI-generated images. By analysing how reconstruction metrics (LPIPS, SSIM, and
PSNR) evolve across varying noise strengths, we extract interpretable
manifold-based features that differentiate real and synthetic images. Evaluated
on a balanced dataset of 4,000 images, our approach achieves 0.993 AUROC under
cross-validation and remains robust to common distortions such as compression
and noise. Despite using limited data and a single diffusion backbone (Stable
Diffusion v1.5), the proposed method demonstrates strong generalization and
interpretability, offering a foundation for scalable, model-agnostic synthetic
media forensics.

</details>


### [526] [Oitijjo-3D: Generative AI Framework for Rapid 3D Heritage Reconstruction from Street View Imagery](https://arxiv.org/abs/2511.00362)
*Momen Khandoker Ope,Akif Islam,Mohd Ruhul Ameen,Abu Saleh Musa Miah,Md Rashedul Islam,Jungpil Shin*

Main category: cs.CV

TL;DR: 本文介绍免费生成式AI框架Oitijjo - 3D用于孟加拉国文化遗产3D数字化保护，利用谷歌街景图像，速度快且降低成本。


<details>
  <summary>Details</summary>
Motivation: 孟加拉国文化遗产修复面临资源和技术专业知识有限的问题，传统3D数字化方法成本高、难实施，许多建筑遗产易损坏且缺乏数字形式。

Method: 提出Oitijjo - 3D框架，通过两阶段流程，利用Gemini 2.5 Flash Image进行多模态视觉推理合成结构纹理，使用Hexagen进行神经图像到3D生成恢复几何形状。

Result: 系统能在数秒内生成逼真、度量一致的重建结果，比传统方法速度大幅提升，无需专业硬件和专家监督。在多个地标实验中，能保持视觉和结构保真度。

Conclusion: Oitijjo - 3D将开放图像转化为数字遗产，使资源有限国家的文化遗产保护成为社区驱动、AI辅助的文化传承行为。

Abstract: Cultural heritage restoration in Bangladesh faces a dual challenge of limited
resources and scarce technical expertise. Traditional 3D digitization methods,
such as photogrammetry or LiDAR scanning, require expensive hardware, expert
operators, and extensive on-site access, which are often infeasible in
developing contexts. As a result, many of Bangladesh's architectural treasures,
from the Paharpur Buddhist Monastery to Ahsan Manzil, remain vulnerable to
decay and inaccessible in digital form. This paper introduces Oitijjo-3D, a
cost-free generative AI framework that democratizes 3D cultural preservation.
By using publicly available Google Street View imagery, Oitijjo-3D reconstructs
faithful 3D models of heritage structures through a two-stage pipeline -
multimodal visual reasoning with Gemini 2.5 Flash Image for structure-texture
synthesis, and neural image-to-3D generation through Hexagen for geometry
recovery. The system produces photorealistic, metrically coherent
reconstructions in seconds, achieving significant speedups compared to
conventional Structure-from-Motion pipelines, without requiring any specialized
hardware or expert supervision. Experiments on landmarks such as Ahsan Manzil,
Choto Sona Mosque, and Paharpur demonstrate that Oitijjo-3D preserves both
visual and structural fidelity while drastically lowering economic and
technical barriers. By turning open imagery into digital heritage, this work
reframes preservation as a community-driven, AI-assisted act of cultural
continuity for resource-limited nations.

</details>


### [527] [Who Can We Trust? Scope-Aware Video Moment Retrieval with Multi-Agent Conflict](https://arxiv.org/abs/2511.00370)
*Chaochen Wu,Guan Luo,Meiyun Zuo,Zhitao Fan*

Main category: cs.CV

TL;DR: 提出基于强化学习的视频时刻检索模型和多智能体系统框架，可解决定位冲突，判断查询是否越界，实验证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 现有视频时刻检索方案未考虑不同模型定位结果的冲突，无法正确整合模型以产生更好结果。

Method: 引入基于强化学习的视频时刻检索模型扫描视频找边界并给出定位证据，提出多智能体系统框架用证据学习解决定位输出冲突。

Result: 在基准数据集上的实验表明，所提方法比现有方法更有效。

Conclusion: 对多智能体系统的竞争和冲突建模是提高时刻检索中强化学习性能的有效方法，展示了证据学习在多智能体框架中的新作用。

Abstract: Video moment retrieval uses a text query to locate a moment from a given
untrimmed video reference. Locating corresponding video moments with text
queries helps people interact with videos efficiently. Current solutions for
this task have not considered conflict within location results from different
models, so various models cannot integrate correctly to produce better results.
This study introduces a reinforcement learning-based video moment retrieval
model that can scan the whole video once to find the moment's boundary while
producing its locational evidence. Moreover, we proposed a multi-agent system
framework that can use evidential learning to resolve conflicts between agents'
localization output. As a side product of observing and dealing with conflicts
between agents, we can decide whether a query has no corresponding moment in a
video (out-of-scope) without additional training, which is suitable for
real-world applications. Extensive experiments on benchmark datasets show the
effectiveness of our proposed methods compared with state-of-the-art
approaches. Furthermore, the results of our study reveal that modeling
competition and conflict of the multi-agent system is an effective way to
improve RL performance in moment retrieval and show the new role of evidential
learning in the multi-agent framework.

</details>


### [528] [LGCA: Enhancing Semantic Representation via Progressive Expansion](https://arxiv.org/abs/2511.00419)
*Thanh Hieu Cao,Trung Khang Tran,Gia Thinh Pham,Tuong Nghiem Diep,Thanh Binh Nguyen*

Main category: cs.CV

TL;DR: 现有预训练视觉语言模型如CLIP在零样本图像分类表现好，但随机裁剪图像会引入错误信息和偏差。本文提出LGCA框架，能捕捉局部和全局特征，减少错误信息，实验表明该方法能提升零样本性能。


<details>
  <summary>Details</summary>
Motivation: 解决CLIP因随机裁剪图像引入错误信息和偏差的问题，提升模型零样本图像分类性能。

Method: 提出Localized - Globalized Cross - Alignment (LGCA)框架，先捕捉图像局部特征，反复选择最显著区域并扩展，设计相似度分数结合原始和扩展图像。

Result: LGCA时间复杂度与原模型在重复扩展过程前相同，且在多个数据集上显著提升零样本性能，优于现有基线。

Conclusion: LGCA框架有效且高效可扩展，能提升零样本图像分类性能。

Abstract: Recent advancements in large-scale pretraining in natural language processing
have enabled pretrained vision-language models such as CLIP to effectively
align images and text, significantly improving performance in zero-shot image
classification tasks. Subsequent studies have further demonstrated that
cropping images into smaller regions and using large language models to
generate multiple descriptions for each caption can further enhance model
performance. However, due to the inherent sensitivity of CLIP, random image
crops can introduce misinformation and bias, as many images share similar
features at small scales. To address this issue, we propose
Localized-Globalized Cross-Alignment (LGCA), a framework that first captures
the local features of an image and then repeatedly selects the most salient
regions and expands them. The similarity score is designed to incorporate both
the original and expanded images, enabling the model to capture both local and
global features while minimizing misinformation. Additionally, we provide a
theoretical analysis demonstrating that the time complexity of LGCA remains the
same as that of the original model prior to the repeated expansion process,
highlighting its efficiency and scalability. Extensive experiments demonstrate
that our method substantially improves zero-shot performance across diverse
datasets, outperforming state-of-the-art baselines.

</details>


### [529] [Leveraging Hierarchical Image-Text Misalignment for Universal Fake Image Detection](https://arxiv.org/abs/2511.00427)
*Daichi Zhang,Tong Zhang,Jianmin Bao,Shiming Ge,Sabine Süsstrunk*

Main category: cs.CV

TL;DR: 现有图像真伪检测方法易过拟合，本文从多模态角度，利用图像 - 文本不对齐线索提出 ITEM 检测器，实验显示其有更好泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有图像真伪检测方法仅关注视觉线索，易过拟合特定图像模式，无法泛化到未知模型，需新方法解决该问题。

Method: 从多模态角度，利用图像 - 文本在联合视觉 - 语言空间的不对齐作为判别线索，提出 ITEM 检测器；先在预训练 CLIP 空间测量图像和文本不对齐，再微调 MLP 头进行检测；提出分层不对齐方案，兼顾全局和细粒度局部语义不对齐。

Result: 大量实验表明，该方法优于其他先进方法，在各种生成模型上有出色的泛化性和鲁棒性。

Conclusion: 提出的 ITEM 检测器利用图像 - 文本不对齐线索，能有效检测生成的假图像，具有良好的泛化和鲁棒性能。

Abstract: With the rapid development of generative models, detecting generated fake
images to prevent their malicious use has become a critical issue recently.
Existing methods frame this challenge as a naive binary image classification
task. However, such methods focus only on visual clues, yielding trained
detectors susceptible to overfitting specific image patterns and incapable of
generalizing to unseen models. In this paper, we address this issue from a
multi-modal perspective and find that fake images cannot be properly aligned
with corresponding captions compared to real images. Upon this observation, we
propose a simple yet effective detector termed ITEM by leveraging the
image-text misalignment in a joint visual-language space as discriminative
clues. Specifically, we first measure the misalignment of the images and
captions in pre-trained CLIP's space, and then tune a MLP head to perform the
usual detection task. Furthermore, we propose a hierarchical misalignment
scheme that first focuses on the whole image and then each semantic object
described in the caption, which can explore both global and fine-grained local
semantic misalignment as clues. Extensive experiments demonstrate the
superiority of our method against other state-of-the-art competitors with
impressive generalization and robustness on various recent generative models.

</details>


### [530] [Enhancing Frequency Forgery Clues for Diffusion-Generated Image Detection](https://arxiv.org/abs/2511.00429)
*Daichi Zhang,Tong Zhang,Shiming Ge,Sabine Süsstrunk*

Main category: cs.CV

TL;DR: 本文针对扩散模型生成图像检测难题，基于频域线索提出F^2C方法，实验显示其泛化性和鲁棒性优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型生成图像的检测器难以捕捉不同模型和设置的判别线索，泛化性和鲁棒性不足。

Method: 观察到扩散生成图像在不同频带与真实图像差异逐渐增大，引入频率选择函数作为傅里叶频谱的加权滤波器，增强F^2C表示。

Result: 在多个扩散生成图像数据集上的大量实验表明，该方法优于现有检测器。

Conclusion: 提出的方法能够对未知扩散模型生成的图像进行通用检测，并对各种扰动具有强大的恢复能力，具有更好的泛化性和鲁棒性。

Abstract: Diffusion models have achieved remarkable success in image synthesis, but the
generated high-quality images raise concerns about potential malicious use.
Existing detectors often struggle to capture discriminative clues across
different models and settings, limiting their generalization to unseen
diffusion models and robustness to various perturbations. To address this
issue, we observe that diffusion-generated images exhibit progressively larger
differences from natural real images across low- to high-frequency bands. Based
on this insight, we propose a simple yet effective representation by enhancing
the Frequency Forgery Clue (F^2C) across all frequency bands. Specifically, we
introduce a frequency-selective function which serves as a weighted filter to
the Fourier spectrum, suppressing less discriminative bands while enhancing
more informative ones. This approach, grounded in a comprehensive analysis of
frequency-based differences between natural real and diffusion-generated
images, enables general detection of images from unseen diffusion models and
provides robust resilience to various perturbations. Extensive experiments on
various diffusion-generated image datasets demonstrate that our method
outperforms state-of-the-art detectors with superior generalization and
robustness.

</details>


### [531] [Longitudinal Vestibular Schwannoma Dataset with Consensus-based Human-in-the-loop Annotations](https://arxiv.org/abs/2511.00472)
*Navodini Wijethilake,Marina Ivory,Oscar MacCormac,Siddhant Kumar,Aaron Kujawa,Lorena Garcia-Foncillas Macias,Rebecca Burger,Amanda Hitchings,Suki Thomson,Sinan Barazi,Eleni Maratos,Rupert Obholzer,Dan Jiang,Fiona McClenaghan,Kazumi Chia,Omar Al-Salihi,Nick Thomas,Steve Connor,Tom Vercauteren,Jonathan Shapey*

Main category: cs.CV

TL;DR: 提出基于自举深度学习框架的带注释数据集用于前庭神经鞘瘤（VS）分割，提高了分割准确性和效率，数据集公开。


<details>
  <summary>Details</summary>
Motivation: 准确分割VS对患者管理重要，但手动注释耗时，现有深度学习自动分割在不同数据集和复杂临床病例中性能有待提升。

Method: 结合多中心数据，依靠专家共识确保注释可信度，采用人在环模型训练方法。

Result: 在目标内部验证数据集上DSC从0.9125提升到0.9670，在外部数据集上性能稳定，专家评估指出模型改进方向，效率比传统手动注释提高约37.4%。

Conclusion: 该人在环模型训练方法实现高分割准确性，是临床可适应、可推广的自动VS分割策略。

Abstract: Accurate segmentation of vestibular schwannoma (VS) on Magnetic Resonance
Imaging (MRI) is essential for patient management but often requires
time-intensive manual annotations by experts. While recent advances in deep
learning (DL) have facilitated automated segmentation, challenges remain in
achieving robust performance across diverse datasets and complex clinical
cases. We present an annotated dataset stemming from a bootstrapped DL-based
framework for iterative segmentation and quality refinement of VS in MRI. We
combine data from multiple centres and rely on expert consensus for
trustworthiness of the annotations. We show that our approach enables effective
and resource-efficient generalisation of automated segmentation models to a
target data distribution. The framework achieved a significant improvement in
segmentation accuracy with a Dice Similarity Coefficient (DSC) increase from
0.9125 to 0.9670 on our target internal validation dataset, while maintaining
stable performance on representative external datasets. Expert evaluation on
143 scans further highlighted areas for model refinement, revealing nuanced
cases where segmentation required expert intervention. The proposed approach is
estimated to enhance efficiency by approximately 37.4% compared to the
conventional manual annotation process. Overall, our human-in-the-loop model
training approach achieved high segmentation accuracy, highlighting its
potential as a clinically adaptable and generalisable strategy for automated VS
segmentation in diverse clinical settings. The dataset includes 190 patients,
with tumour annotations available for 534 longitudinal contrast-enhanced
T1-weighted (T1CE) scans from 184 patients, and non-annotated T2-weighted scans
from 6 patients. This dataset is publicly accessible on The Cancer Imaging
Archive (TCIA) (https://doi.org/10.7937/bq0z-xa62).

</details>


### [532] [TRACES: Temporal Recall with Contextual Embeddings for Real-Time Video Anomaly Detection](https://arxiv.org/abs/2511.00580)
*Yousuf Ahmed Siddiqui,Sufiyaan Usmani,Umer Tariq,Jawwad Ahmed Shamsi,Muhammad Burhan Khan*

Main category: cs.CV

TL;DR: 本文提出记忆增强管道方法解决上下文感知零样本异常检测挑战，在UCF - Crime和XD - Violence数据集取得新的最优结果，实现实时推理。


<details>
  <summary>Details</summary>
Motivation: 多数异常检测器未考虑上下文信息，限制其在新的现实场景中的泛化能力，因此要解决上下文感知零样本异常检测问题。

Method: 定义记忆增强管道，通过交叉注意力将时间信号与视觉嵌入相关联，通过上下文相似度评分进行实时零样本异常分类。

Result: 在UCF - Crime上AUC达到90.4%，在XD - Violence上AP达到83.67%，实现实时推理，具有高精度和可解释性。

Conclusion: 通过融合交叉注意力时间融合和上下文记忆，实现高保真异常检测，推动零样本模型在现实世界监控和基础设施监测中的应用。

Abstract: Video anomalies often depend on contextual information available and temporal
evolution. Non-anomalous action in one context can be anomalous in some other
context. Most anomaly detectors, however, do not notice this type of context,
which seriously limits their capability to generalize to new, real-life
situations. Our work addresses the context-aware zero-shot anomaly detection
challenge, in which systems need to learn adaptively to detect new events by
correlating temporal and appearance features with textual traces of memory in
real time. Our approach defines a memory-augmented pipeline, correlating
temporal signals with visual embeddings using cross-attention, and real-time
zero-shot anomaly classification by contextual similarity scoring. We achieve
90.4\% AUC on UCF-Crime and 83.67\% AP on XD-Violence, a new state-of-the-art
among zero-shot models. Our model achieves real-time inference with high
precision and explainability for deployment. We show that, by fusing
cross-attention temporal fusion and contextual memory, we achieve high fidelity
anomaly detection, a step towards the applicability of zero-shot models in
real-world surveillance and infrastructure monitoring.

</details>


### [533] [Metadata-Aligned 3D MRI Representations for Contrast Understanding and Quality Control](https://arxiv.org/abs/2511.00681)
*Mehmet Yigit Avci,Pedro Borges,Virginia Fernandez,Paul Wright,Mehmet Yigitsoy,Sebastien Ourselin,Jorge Cardoso*

Main category: cs.CV

TL;DR: 提出MR - CLIP框架学习MRI对比表示，在少样本序列分类中表现好，还能进行无监督数据质量控制，为高效MRI分析提供基础。


<details>
  <summary>Details</summary>
Motivation: 磁共振成像存在数据异质性和缺乏标准化对比标签问题，限制大规模自动分析，需要统一的MRI对比表示。

Method: 引入MR - CLIP，通过将体积图像与其DICOM采集参数对齐来学习MRI对比表示。

Result: 得到的嵌入显示出MRI序列的不同聚类，在少样本序列分类中优于有监督3D基线，能通过图像 - 元数据嵌入距离进行无监督数据质量控制。

Conclusion: MR - CLIP将常规可用的采集元数据转化为监督信号，为跨不同临床数据集的标签高效MRI分析提供可扩展基础。

Abstract: Magnetic Resonance Imaging suffers from substantial data heterogeneity and
the absence of standardized contrast labels across scanners, protocols, and
institutions, which severely limits large-scale automated analysis. A unified
representation of MRI contrast would enable a wide range of downstream
utilities, from automatic sequence recognition to harmonization and quality
control, without relying on manual annotations. To this end, we introduce
MR-CLIP, a metadata-guided framework that learns MRI contrast representations
by aligning volumetric images with their DICOM acquisition parameters. The
resulting embeddings shows distinct clusters of MRI sequences and outperform
supervised 3D baselines under data scarcity in few-shot sequence
classification. Moreover, MR-CLIP enables unsupervised data quality control by
identifying corrupted or inconsistent metadata through image-metadata embedding
distances. By transforming routinely available acquisition metadata into a
supervisory signal, MR-CLIP provides a scalable foundation for label-efficient
MRI analysis across diverse clinical datasets.

</details>


### [534] [Evolve to Inspire: Novelty Search for Diverse Image Generation](https://arxiv.org/abs/2511.00686)
*Alex Inch,Passawis Chaiyapattanaporn,Yuchen Zhu,Yuan Lu,Ting-Wen Ko,Davide Paglieri*

Main category: cs.CV

TL;DR: 提出WANDER方法解决文本到图像扩散模型输出多样性有限问题，实验显示其在多样性指标上优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型输出多样性有限，现有提示优化技术不适用于创意视觉领域。

Method: 引入基于新奇性搜索的WANDER方法，直接处理自然语言提示，用大语言模型进行语义进化，CLIP嵌入量化新奇性，应用发射器引导搜索。

Result: 使用FLUX - DEV生成和GPT - 4o - mini突变的实证评估表明，WANDER在多样性指标上显著优于现有进化提示优化基线，消融研究证实发射器有效。

Conclusion: WANDER能有效提高图像生成的多样性。

Abstract: Text-to-image diffusion models, while proficient at generating high-fidelity
im- ages, often suffer from limited output diversity, hindering their
application in exploratory and ideation tasks. Existing prompt optimization
techniques typically target aesthetic fitness or are ill-suited to the creative
visual domain. To address this shortcoming, we introduce WANDER, a novelty
search-based approach to generating diverse sets of images from a single input
prompt. WANDER operates directly on natural language prompts, employing a Large
Language Model (LLM) for semantic evolution of diverse sets of images, and
using CLIP embeddings to quantify novelty. We additionally apply emitters to
guide the search into distinct regions of the prompt space, and demonstrate
that they boost the diversity of the generated images. Empirical evaluations
using FLUX-DEV for generation and GPT-4o-mini for mutation demonstrate that
WANDER significantly outperforms existing evolutionary prompt optimization
baselines in diversity metrics. Ablation studies confirm the efficacy of
emitters.

</details>


### [535] [Class-agnostic 3D Segmentation by Granularity-Consistent Automatic 2D Mask Tracking](https://arxiv.org/abs/2511.00785)
*Juan Wang,Yasutomo Kawanishi,Tomo Miyazaki,Zhijie Wang,Shinichiro Omachi*

Main category: cs.CV

TL;DR: 提出粒度一致的2D掩码跟踪方法结合三阶段课程学习框架，实现准确3D实例分割，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 现有将2D掩码从基础模型转移到3D生成伪标签的方法因独立处理视频帧，导致分割粒度不一致和3D伪标签冲突，降低最终分割精度。

Method: 引入粒度一致的自动2D掩码跟踪方法维护帧间时间对应关系，结合三阶段课程学习框架，从碎片化单视图数据逐步训练到统一多视图注释。

Result: 有效生成一致且准确的3D分割，在标准基准测试中取得了最先进的结果，并具备开放词汇能力。

Conclusion: 所提方法能从最初碎片化和矛盾的2D先验中稳健提取一致的3D表示。

Abstract: 3D instance segmentation is an important task for real-world applications. To
avoid costly manual annotations, existing methods have explored generating
pseudo labels by transferring 2D masks from foundation models to 3D. However,
this approach is often suboptimal since the video frames are processed
independently. This causes inconsistent segmentation granularity and
conflicting 3D pseudo labels, which degrades the accuracy of final
segmentation. To address this, we introduce a Granularity-Consistent automatic
2D Mask Tracking approach that maintains temporal correspondences across
frames, eliminating conflicting pseudo labels. Combined with a three-stage
curriculum learning framework, our approach progressively trains from
fragmented single-view data to unified multi-view annotations, ultimately
globally coherent full-scene supervision. This structured learning pipeline
enables the model to progressively expose to pseudo-labels of increasing
consistency. Thus, we can robustly distill a consistent 3D representation from
initially fragmented and contradictory 2D priors. Experimental results
demonstrated that our method effectively generated consistent and accurate 3D
segmentations. Furthermore, the proposed method achieved state-of-the-art
results on standard benchmarks and open-vocabulary ability.

</details>


### [536] [FedOnco-Bench: A Reproducible Benchmark for Privacy-Aware Federated Tumor Segmentation with Synthetic CT Data](https://arxiv.org/abs/2511.00795)
*Viswa Chaitanya Marella,Suhasnadh Reddy Veluru,Sai Teja Erukude*

Main category: cs.CV

TL;DR: 本文提出FedOnco - Bench基准，评估多种联邦学习方法在隐私和性能上的表现，指出隐私与效用权衡，该基准可用于医学图像分割的隐私保护联邦学习方法开发。


<details>
  <summary>Details</summary>
Motivation: 联邦学习系统易受成员推理攻击和数据异质性影响，需要一个基准评估隐私感知联邦学习。

Method: 提出FedOnco - Bench基准，使用合成肿瘤CT扫描数据，评估FedAvg、FedProx、FedBN和带DP - SGD的FedAvg等方法。

Result: FedAvg性能高但隐私泄露多；DP - SGD隐私性高但准确性低；FedProx和FedBN在异构数据下表现平衡。

Conclusion: FedOnco - Bench是用于医学图像分割的隐私保护联邦学习方法基准测试和开发的标准化开源平台。

Abstract: Federated Learning (FL) allows multiple institutions to cooperatively train
machine learning models while retaining sensitive data at the source, which has
great utility in privacy-sensitive environments. However, FL systems remain
vulnerable to membership-inference attacks and data heterogeneity. This paper
presents FedOnco-Bench, a reproducible benchmark for privacy-aware FL using
synthetic oncologic CT scans with tumor annotations. It evaluates segmentation
performance and privacy leakage across FL methods: FedAvg, FedProx, FedBN, and
FedAvg with DP-SGD. Results show a distinct trade-off between privacy and
utility: FedAvg is high performance (Dice around 0.85) with more privacy
leakage (attack AUC about 0.72), while DP-SGD provides a higher level of
privacy (AUC around 0.25) at the cost of accuracy (Dice about 0.79). FedProx
and FedBN offer balanced performance under heterogeneous data, especially with
non-identical distributed client data. FedOnco-Bench serves as a standardized,
open-source platform for benchmarking and developing privacy-preserving FL
methods for medical image segmentation.

</details>


### [537] [Integrating ConvNeXt and Vision Transformers for Enhancing Facial Age Estimation](https://arxiv.org/abs/2511.00123)
*Gaby Maroun,Salah Eddine Bekhouche,Fadi Dornaika*

Main category: cs.CV

TL;DR: 提出结合ConvNeXt和ViT的混合架构用于面部年龄估计，在多个数据集上表现优异，强调混合架构潜力。


<details>
  <summary>Details</summary>
Motivation: 解决面部图像年龄估计这一复杂挑战，利用CNN和Transformer优势。

Method: 提出ConvNeXt - ViT混合架构，利用预训练模型，探索不同配置，用线性层和正则化技术优化，进行消融研究。

Result: 在MORPH II、CACD和AFAD等数据集上，该混合架构的MAE表现优于传统方法。

Conclusion: 混合架构有变革潜力，为CNN和Transformer融合解决复杂计算机视觉问题指明方向。

Abstract: Age estimation from facial images is a complex and multifaceted challenge in
computer vision. In this study, we present a novel hybrid architecture that
combines ConvNeXt, a state-of-the-art advancement of convolutional neural
networks (CNNs), with Vision Transformers (ViT). While each model independently
delivers excellent performance on a variety of tasks, their integration
leverages the complementary strengths of the CNNs localized feature extraction
capabilities and the Transformers global attention mechanisms. Our proposed
ConvNeXt-ViT hybrid solution was thoroughly evaluated on benchmark age
estimation datasets, including MORPH II, CACD, and AFAD, and achieved superior
performance in terms of mean absolute error (MAE). To address computational
constraints, we leverage pre-trained models and systematically explore
different configurations, using linear layers and advanced regularization
techniques to optimize the architecture. Comprehensive ablation studies
highlight the critical role of individual components and training strategies,
and in particular emphasize the importance of adapted attention mechanisms
within the CNN framework to improve the model focus on age-relevant facial
features. The results show that the ConvNeXt-ViT hybrid not only outperforms
traditional methods, but also provides a robust foundation for future advances
in age estimation and related visual tasks. This work underscores the
transformative potential of hybrid architectures and represents a promising
direction for the seamless integration of CNNs and transformers to address
complex computer vision challenges.

</details>


### [538] [GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding](https://arxiv.org/abs/2511.00810)
*Shijie Zhou,Viet Dac Lai,Hao Tan,Jihyung Kil,Wanrong Zhu,Changyou Chen,Ruiyi Zhang*

Main category: cs.CV

TL;DR: 提出GUI - AIMA框架解决GUI grounding问题，数据效率高且性能达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于MLLMs的GUI grounding方法将其作为文本坐标生成任务，直接从视觉输入生成精确坐标有挑战且计算量大。

Method: 提出基于注意力且无坐标的监督微调框架GUI - AIMA，使MLLMs的内在多模态注意力与逐块接地信号对齐，通过多头聚合自适应计算信号，无坐标方式可集成缩放阶段。

Result: GUI - AIMA - 3B仅用85k截图训练，数据效率高，在3B模型中达到SOTA，在ScreenSpot - Pro和OSWorld - G上分别达58.6%和62.2%的平均准确率。

Conclusion: 轻量级训练能触发MLLMs的原生接地能力，GUI - AIMA框架有效。

Abstract: Graphical user interface (GUI) grounding is a key function of computer-use
agents, which maps natural-language instructions to actionable screen regions.
Existing approaches based on Multimodal Large Language Models (MLLMs) typically
formulate it as a text-based coordinate generation task, yet directly
generating precise coordinates from visual inputs remains challenging and
computationally intensive. An intuitive way to implement GUI grounding is to
first select visual patches relevant to the instructions and then determine the
precise click location within those patches. Based on the observations that
general MLLMs have some native grounding capability, nested within their
attentions, we propose GUI-AIMA, an attention-based and coordinate-free
supervised fine-tuning framework for efficient GUI grounding. GUI-AIMA aligns
the intrinsic multimodal attention of MLLMs with patch-wise grounding signals.
These signals are calculated adaptively for diverse user instructions by
multi-head aggregation on simplified query-visual attention matrices. Besides,
its coordinate-free manner can easily integrate a plug-and-play zoom-in stage.
GUI-AIMA-3B was trained with only 85k screenshots, demonstrating exceptional
data efficiency and verifying that light training can trigger the native
grounding capability of MLLMs. It achieves state-of-the-art performance among
3B models, attaining an average accuracy of 58.6% on ScreenSpot-Pro and 62.2%
on OSWorld-G. Project page: https://github.com/sjz5202/GUI-AIMA

</details>


### [539] [Enhancing Adversarial Transferability in Visual-Language Pre-training Models via Local Shuffle and Sample-based Attack](https://arxiv.org/abs/2511.00831)
*Xin Liu,Aoyang Zhou,Aoyang Zhou*

Main category: cs.CV

TL;DR: 现有VLP模型易受对抗样本攻击，提出LSSA攻击方法提升多模态对抗样本可迁移性，实验证明其有效且优于其他先进攻击方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在生成跨模态对抗样本时因缺乏输入多样性导致过拟合的问题。

Method: 提出Local Shuffle and Sample-based Attack (LSSA)，随机打乱局部图像块，扩展原始图像 - 文本对，生成对抗图像并采样，用原始和采样图像生成对抗文本。

Result: 在多个模型和数据集上的实验表明，LSSA显著提升了多模态对抗样本在不同VLP模型和下游任务中的可迁移性，且在大模型上优于其他先进攻击方法。

Conclusion: LSSA是一种有效的攻击方法，能提升多模态对抗样本的可迁移性。

Abstract: Visual-Language Pre-training (VLP) models have achieved significant
performance across various downstream tasks. However, they remain vulnerable to
adversarial examples. While prior efforts focus on improving the adversarial
transferability of multimodal adversarial examples through cross-modal
interactions, these approaches suffer from overfitting issues, due to a lack of
input diversity by relying excessively on information from adversarial examples
in one modality when crafting attacks in another. To address this issue, we
draw inspiration from strategies in some adversarial training methods and
propose a novel attack called Local Shuffle and Sample-based Attack (LSSA).
LSSA randomly shuffles one of the local image blocks, thus expanding the
original image-text pairs, generating adversarial images, and sampling around
them. Then, it utilizes both the original and sampled images to generate the
adversarial texts. Extensive experiments on multiple models and datasets
demonstrate that LSSA significantly enhances the transferability of multimodal
adversarial examples across diverse VLP models and downstream tasks. Moreover,
LSSA outperforms other advanced attacks on Large Vision-Language Models.

</details>


### [540] [Linear Differential Vision Transformer: Learning Visual Contrasts via Pairwise Differentials](https://arxiv.org/abs/2511.00833)
*Yifan Pu,Jixuan Ying,Qixiu Li,Tianzhu Ye,Dongchen Han,Xiaochen Wang,Ziyi Wang,Xinyu Shao,Gao Huang,Xiu Li*

Main category: cs.CV

TL;DR: 提出VCA模块替代MHSA，降低复杂度，提升Vision Transformers性能。


<details>
  <summary>Details</summary>
Motivation: ViTs的MHSA层在视觉弱或冗余相关性上花费大量计算，需改进。

Method: 引入VCA模块，将每个头的密集查询字段提炼为视觉对比令牌，拆分为正负流进行差分交互。

Result: 提升DeiT - Tiny在ImageNet - 1K上的top - 1准确率，改善三种分层ViTs，降低图像生成FID - 50K分数。

Conclusion: VCA为更快更清晰的Vision Transformers提供了简单途径。

Abstract: Vision Transformers (ViTs) have become a universal backbone for both image
recognition and image generation. Yet their Multi-Head Self-Attention (MHSA)
layer still performs a quadratic query-key interaction for every token pair,
spending the bulk of computation on visually weak or redundant correlations. We
introduce Visual-Contrast Attention (VCA), a drop-in replacement for MHSA that
injects an explicit notion of discrimination while reducing the theoretical
complexity from O(N N C) to O(N n C) with n << N. VCA first distils each head's
dense query field into a handful of spatially pooled visual-contrast tokens,
then splits them into a learnable positive and negative stream whose
differential interaction highlights what truly separates one region from
another. The module adds fewer than 0.3M parameters to a DeiT-Tiny backbone,
requires no extra FLOPs, and is wholly architecture-agnostic. Empirically, VCA
lifts DeiT-Tiny top-1 accuracy on ImageNet-1K from 72.2% to 75.6% (+3.4) and
improves three strong hierarchical ViTs by up to 3.1%, while in
class-conditional ImageNet generation it lowers FID-50K by 2.1 to 5.2 points
across both diffusion (DiT) and flow (SiT) models. Extensive ablations confirm
that (i) spatial pooling supplies low-variance global cues, (ii) dual
positional embeddings are indispensable for contrastive reasoning, and (iii)
combining the two in both stages yields the strongest synergy. VCA therefore
offers a simple path towards faster and sharper Vision Transformers. The source
code is available at https://github.com/LeapLabTHU/LinearDiff.

</details>


### [541] [Parameter Interpolation Adversarial Training for Robust Image Classification](https://arxiv.org/abs/2511.00836)
*Xin Liu,Yichen Yang,Kun He,John E. Hopcroft*

Main category: cs.CV

TL;DR: 针对现有对抗训练方法存在的模型鲁棒性振荡和过拟合问题，提出PIAT框架并结合NMSE，实验证明能提升CNN和ViT的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗训练方法在训练过程中模型鲁棒性存在明显振荡和过拟合问题，降低防御效果。

Method: 提出PIAT框架，在每个epoch之间通过插值上一个和当前epoch的参数来调整模型参数；使用NMSE对齐干净和对抗样本logits的相对大小。

Result: 在多个基准数据集上的大量实验表明，该框架能显著提高CNN和ViT的鲁棒性。

Conclusion: PIAT框架结合NMSE能有效解决现有对抗训练方法的问题，提升模型鲁棒性。

Abstract: Though deep neural networks exhibit superior performance on various tasks,
they are still plagued by adversarial examples. Adversarial training has been
demonstrated to be the most effective method to defend against adversarial
attacks. However, existing adversarial training methods show that the model
robustness has apparent oscillations and overfitting issues in the training
process, degrading the defense efficacy. To address these issues, we propose a
novel framework called Parameter Interpolation Adversarial Training (PIAT).
PIAT tunes the model parameters between each epoch by interpolating the
parameters of the previous and current epochs. It makes the decision boundary
of model change more moderate and alleviates the overfitting issue, helping the
model converge better and achieving higher model robustness. In addition, we
suggest using the Normalized Mean Square Error (NMSE) to further improve the
robustness by aligning the relative magnitude of logits between clean and
adversarial examples rather than the absolute magnitude. Extensive experiments
conducted on several benchmark datasets demonstrate that our framework could
prominently improve the robustness of both Convolutional Neural Networks (CNNs)
and Vision Transformers (ViTs).

</details>


### [542] [OmniBrainBench: A Comprehensive Multimodal Benchmark for Brain Imaging Analysis Across Multi-stage Clinical Tasks](https://arxiv.org/abs/2511.00846)
*Zhihao Peng,Cheng Wang,Shengyuan Liu,Zhiying Liang,Yixuan Yuan*

Main category: cs.CV

TL;DR: 提出OmniBrainBench评估脑成像分析中MLLMs的多模态理解能力，评估24个模型，指出模型与专家差距并发布基准和代码。


<details>
  <summary>Details</summary>
Motivation: 现有脑导向视觉问答基准存在覆盖成像模态少或描述粗粒度的问题，阻碍对MLLMs的全面评估。

Method: 引入OmniBrainBench，包含15种脑成像模态、9527个有效VQA对和31706张图像，模拟临床工作流，有15个多阶段临床任务，并由专业放射科医生验证。

Result: 评估24个模型发现，专有MLLMs优于开源和医学模型但不如医生；医学MLLMs性能差异大；开源MLLMs总体落后但特定任务表现好；MLLMs在复杂术前任务表现差。

Conclusion: OmniBrainBench为评估和推进脑成像分析中MLLMs设定新标准，凸显模型与专家临床推理的差距。

Abstract: Brain imaging analysis is vital for diagnosing and treating brain disorders,
and multimodal large language models (MLLMs) are increasingly assisting in that
analysis. However, current brain-oriented visual question-answering (VQA)
benchmarks either cover a few imaging modalities or are limited to
coarse-grained pathological descriptions, hindering a comprehensive assessment
of MLLMs throughout the full clinical continuum. To address these, we introduce
OmniBrainBench, the first comprehensive multimodal VQA benchmark specifically
designed to assess the multimodal comprehension capabilities of MLLMs in brain
imaging analysis.OmniBrainBench consists of 15 distinct brain imaging
modalities collected from 30 verified medical sources, yielding 9,527 validated
VQA pairs and 31,706 images. It simulates clinical workflows and encompasses 15
multi-stage clinical tasks rigorously validated by a professional radiologist.
Evaluation of 24 state-of-the-art models, including open-source, medical, and
proprietary MLLMs, highlights the substantial challenges posed by
OmniBrainBench. Our experiments reveal: (1) proprietary MLLMs (e.g., GPT-5)
beat open-source and medical models but lag physicians; (2) medical MLLMs vary
widely in performance; (3) open-source MLLMs trail overall but excel in
specific tasks; (4) MLLMs underperform sharply in complex preoperative tasks,
revealing a visual-to-clinical reasoning gap. OmniBrainBench sets a new
standard for evaluating and advancing MLLMs in brain imaging analysis,
highlighting gaps compared to expert clinical reasoning. We release it at
benchmark \& code.

</details>


### [543] [Occlusion-Aware Diffusion Model for Pedestrian Intention Prediction](https://arxiv.org/abs/2511.00858)
*Yu Liu,Zhijie Liu,Zedong Yang,You-Fu Li,He Kong*

Main category: cs.CV

TL;DR: 提出遮挡感知扩散模型ODM预测行人过街意图，在多种遮挡场景下表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习模型在预测行人过街意图时很少考虑遮挡场景下的不完整观测问题。

Method: 提出ODM模型，在去噪阶段引入遮挡感知扩散变压器架构估计与遮挡模式相关的噪声特征，引入遮挡掩码引导的反向过程利用观测信息。

Result: 在PIE和JAAD基准上进行实验，结果表明提出的方法比现有方法有更稳健的性能。

Conclusion: 提出的ODM模型能有效处理遮挡场景下行人过街意图预测问题，性能优于现有方法。

Abstract: Predicting pedestrian crossing intentions is crucial for the navigation of
mobile robots and intelligent vehicles. Although recent deep learning-based
models have shown significant success in forecasting intentions, few consider
incomplete observation under occlusion scenarios. To tackle this challenge, we
propose an Occlusion-Aware Diffusion Model (ODM) that reconstructs occluded
motion patterns and leverages them to guide future intention prediction. During
the denoising stage, we introduce an occlusion-aware diffusion transformer
architecture to estimate noise features associated with occluded patterns,
thereby enhancing the model's ability to capture contextual relationships in
occluded semantic scenarios. Furthermore, an occlusion mask-guided reverse
process is introduced to effectively utilize observation information, reducing
the accumulation of prediction errors and enhancing the accuracy of
reconstructed motion features. The performance of the proposed method under
various occlusion scenarios is comprehensively evaluated and compared with
existing methods on popular benchmarks, namely PIE and JAAD. Extensive
experimental results demonstrate that the proposed method achieves more robust
performance than existing methods in the literature.

</details>


### [544] [Beyond ImageNet: Understanding Cross-Dataset Robustness of Lightweight Vision Models](https://arxiv.org/abs/2511.00335)
*Weidong Zhang,Pak Lun Kevin Ding,Huan Liu*

Main category: cs.CV

TL;DR: 对11个轻量级视觉模型在7个不同数据集上进行系统评估，引入xScore指标，揭示ImageNet准确性不能可靠预测其他数据集表现等结果，为评估模型和设计架构提供指导。


<details>
  <summary>Details</summary>
Motivation: 现有轻量级视觉模型多在ImageNet上进行性能基准测试，需探究其在其他领域的泛化性、如何量化跨数据集鲁棒性以及资源受限下哪些架构元素驱动泛化。

Method: 对11个轻量级视觉模型（250万个参数）在固定100个训练周期下于7个不同数据集上进行系统评估，引入Cross - Dataset Score (xScore)指标。

Result: 1. ImageNet准确性不能可靠预测细粒度或医学数据集上的表现；2. xScore可从四个数据集估计，是移动模型性能的可扩展预测指标；3. 某些架构组件促进更广泛泛化，Transformer块虽参数开销高但额外收益小。

Conclusion: 本研究为评估轻量级视觉模型提供可复现框架，突出移动友好架构的关键设计原则，指导未来跨领域泛化模型的开发。

Abstract: Lightweight vision classification models such as MobileNet, ShuffleNet, and
EfficientNet are increasingly deployed in mobile and embedded systems, yet
their performance has been predominantly benchmarked on ImageNet. This raises
critical questions: Do models that excel on ImageNet also generalize across
other domains? How can cross-dataset robustness be systematically quantified?
And which architectural elements consistently drive generalization under tight
resource constraints? Here, we present the first systematic evaluation of 11
lightweight vision models (2.5M parameters), trained under a fixed 100-epoch
schedule across 7 diverse datasets. We introduce the Cross-Dataset Score
(xScore), a unified metric that quantifies the consistency and robustness of
model performance across diverse visual domains. Our results show that (1)
ImageNet accuracy does not reliably predict performance on fine-grained or
medical datasets, (2) xScore provides a scalable predictor of mobile model
performance that can be estimated from just four datasets, and (3) certain
architectural components--such as isotropic convolutions with higher spatial
resolution and channel-wise attention--promote broader generalization, while
Transformer-based blocks yield little additional benefit, despite incurring
higher parameter overhead. This study provides a reproducible framework for
evaluating lightweight vision models beyond ImageNet, highlights key design
principles for mobile-friendly architectures, and guides the development of
future models that generalize robustly across diverse application domains.

</details>


### [545] [OSMGen: Highly Controllable Satellite Image Synthesis using OpenStreetMap Data](https://arxiv.org/abs/2511.00345)
*Amir Ziashahabi,Narges Ghasemi,Sajjad Shahabi,John Krumm,Salman Avestimehr,Cyrus Shahabi*

Main category: cs.CV

TL;DR: 介绍了生成框架OSMGen，可从原始OSM数据创建逼真卫星图像，能解决训练数据稀缺和类别不平衡问题，还为规划者提供预览干预方案的方法。


<details>
  <summary>Details</summary>
Motivation: 准确及时的地理空间数据对城市规划等至关重要，但城市监测自动化因特定城市特征及变化的策划数据集稀缺而困难。

Method: 引入OSMGen框架，利用OSM JSON的丰富信息进行场景生成，能产生一致的前后图像对。

Result: 可生成解决数据稀缺和类别不平衡的训练数据，为规划者提供预览干预方案的简单方法，还能产生配对数据推动卫星图像自动更新OSM。

Conclusion: OSMGen为城市监测自动化和OSM更新提供了有效解决方案，代码开源。

Abstract: Accurate and up-to-date geospatial data are essential for urban planning,
infrastructure monitoring, and environmental management. Yet, automating urban
monitoring remains difficult because curated datasets of specific urban
features and their changes are scarce. We introduce OSMGen, a generative
framework that creates realistic satellite imagery directly from raw
OpenStreetMap (OSM) data. Unlike prior work that relies on raster tiles, OSMGen
uses the full richness of OSM JSON, including vector geometries, semantic tags,
location, and time, giving fine-grained control over how scenes are generated.
A central feature of the framework is the ability to produce consistent
before-after image pairs: user edits to OSM inputs translate into targeted
visual changes, while the rest of the scene is preserved. This makes it
possible to generate training data that addresses scarcity and class imbalance,
and to give planners a simple way to preview proposed interventions by editing
map data. More broadly, OSMGen produces paired (JSON, image) data for both
static and changed states, paving the way toward a closed-loop system where
satellite imagery can automatically drive structured OSM updates. Source code
is available at https://github.com/amir-zsh/OSMGen.

</details>


### [546] [ToxicTextCLIP: Text-Based Poisoning and Backdoor Attacks on CLIP Pre-training](https://arxiv.org/abs/2511.00446)
*Xin Yao,Haiyang Zhao,Yimin Chen,Jiawei Guo,Kecheng Huang,Ming Zhao*

Main category: cs.CV

TL;DR: 本文提出ToxicTextCLIP框架，用于在预训练阶段生成针对CLIP的对抗性文本，实验显示其有高中毒成功率和后门命中率，还能绕过多种防御。


<details>
  <summary>Details</summary>
Motivation: CLIP依赖未筛选的网络数据，存在数据中毒和后门风险，且现有研究对文本模态攻击探索不足。

Method: ToxicTextCLIP框架迭代应用背景感知选择器和背景驱动增强器，解决语义不一致和背景一致文本稀缺问题。

Result: 在分类和检索任务实验中，ToxicTextCLIP中毒成功率达95.83%，后门Hit@1达98.68%，能绕过RoCLIP、CleanCLIP和SafeCLIP防御。

Conclusion: ToxicTextCLIP框架能有效生成高质量对抗性文本，对CLIP造成威胁。

Abstract: The Contrastive Language-Image Pretraining (CLIP) model has significantly
advanced vision-language modeling by aligning image-text pairs from large-scale
web data through self-supervised contrastive learning. Yet, its reliance on
uncurated Internet-sourced data exposes it to data poisoning and backdoor
risks. While existing studies primarily investigate image-based attacks, the
text modality, which is equally central to CLIP's training, remains
underexplored. In this work, we introduce ToxicTextCLIP, a framework for
generating high-quality adversarial texts that target CLIP during the
pre-training phase. The framework addresses two key challenges: semantic
misalignment caused by background inconsistency with the target class, and the
scarcity of background-consistent texts. To this end, ToxicTextCLIP iteratively
applies: 1) a background-aware selector that prioritizes texts with background
content aligned to the target class, and 2) a background-driven augmenter that
generates semantically coherent and diverse poisoned samples. Extensive
experiments on classification and retrieval tasks show that ToxicTextCLIP
achieves up to 95.83% poisoning success and 98.68% backdoor Hit@1, while
bypassing RoCLIP, CleanCLIP and SafeCLIP defenses. The source code can be
accessed via https://github.com/xinyaocse/ToxicTextCLIP/.

</details>


### [547] [FedMGP: Personalized Federated Learning with Multi-Group Text-Visual Prompts](https://arxiv.org/abs/2511.00480)
*Weihao Bo,Yanpeng Sun,Yu Wang,Xinyu Zhang,Zechao Li*

Main category: cs.CV

TL;DR: 本文提出FedMGP用于视觉语言模型的个性化联邦提示学习，采用多组提示、多样性损失和动态提示聚合策略，参数高效且性能优异。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型在联邦学习中的个性化和领域泛化问题，有效聚合知识并保留客户端特征。

Method: 为每个客户端配备多组文本和视觉提示，引入多样性损失，采用基于相似度引导概率采样的动态提示聚合策略。

Result: 在多个联邦视觉语言基准测试中，FedMGP在个性化和领域泛化方面始终优于先前方法，且通信参数最少。

Conclusion: FedMGP的动态聚合策略能促进鲁棒的全局表示学习，实现了性能和参数效率的平衡。

Abstract: In this paper, we introduce FedMGP, a new paradigm for personalized federated
prompt learning in vision-language models. FedMGP equips each client with
multiple groups of paired textual and visual prompts, enabling the model to
capture diverse, fine-grained semantic and instance-level cues. A diversity
loss is introduced to drive each prompt group to specialize in distinct and
complementary semantic aspects, ensuring that the groups collectively cover a
broader range of local characteristics. During communication, FedMGP employs a
dynamic prompt aggregation strategy based on similarity-guided probabilistic
sampling: each client computes the cosine similarity between its prompt groups
and the global prompts from the previous round, then samples s groups via a
softmax-weighted distribution. This soft selection mechanism preferentially
aggregates semantically aligned knowledge while still enabling exploration of
underrepresented patterns effectively balancing the preservation of common
knowledge with client-specific features. Notably, FedMGP maintains parameter
efficiency by redistributing a fixed prompt capacity across multiple groups,
achieving state-of-the-art performance with the lowest communication parameters
among all federated prompt learning methods. Theoretical analysis shows that
our dynamic aggregation strategy promotes robust global representation learning
by reinforcing shared semantics while suppressing client-specific noise.
Extensive experiments demonstrate that FedMGP consistently outperforms prior
approaches in both personalization and domain generalization across diverse
federated vision-language benchmarks. The code will be released on
https://github.com/weihao-bo/FedMGP.git.

</details>


### [548] [GeoToken: Hierarchical Geolocalization of Images via Next Token Prediction](https://arxiv.org/abs/2511.01082)
*Narges Ghasemi,Amir Ziashahabi,Salman Avestimehr,Cyrus Shahabi*

Main category: cs.CV

TL;DR: 提出分层序列预测方法解决图像地理定位问题，在多个数据集上表现优异，代码开源。


<details>
  <summary>Details</summary>
Motivation: 解决图像地理定位中因视觉相似和搜索空间大带来的挑战。

Method: 提出分层序列预测方法，用S2单元格，借鉴大语言模型自回归文本生成，研究多种自回归采样的自上而下遍历方法。

Result: 在Im2GPS3k和YFCC4k数据集上，无MLLM时超越多数基线，有MLLM时超越所有基线。

Conclusion: 所提方法能有效解决图像地理定位问题，达到了最先进水平。

Abstract: Image geolocalization, the task of determining an image's geographic origin,
poses significant challenges, largely due to visual similarities across
disparate locations and the large search space. To address these issues, we
propose a hierarchical sequence prediction approach inspired by how humans
narrow down locations from broad regions to specific addresses. Analogously,
our model predicts geographic tokens hierarchically, first identifying a
general region and then sequentially refining predictions to increasingly
precise locations. Rather than relying on explicit semantic partitions, our
method uses S2 cells, a nested, multiresolution global grid, and sequentially
predicts finer-level cells conditioned on visual inputs and previous
predictions. This procedure mirrors autoregressive text generation in large
language models. Much like in language modeling, final performance depends not
only on training but also on inference-time strategy. We investigate multiple
top-down traversal methods for autoregressive sampling, incorporating
techniques from test-time compute scaling used in language models.
Specifically, we integrate beam search and multi-sample inference while
exploring various selection strategies to determine the final output. This
enables the model to manage uncertainty by exploring multiple plausible paths
through the hierarchy. We evaluate our method on the Im2GPS3k and YFCC4k
datasets against two distinct sets of baselines: those that operate without a
Multimodal Large Language Model (MLLM) and those that leverage one. In the
MLLM-free setting, our model surpasses other comparable baselines on nearly all
metrics, achieving state-of-the-art performance with accuracy gains of up to
13.9%. When augmented with an MLLM, our model outperforms all baselines,
setting a new state-of-the-art across all metrics. The source code is available
at https://github.com/NNargesNN/GeoToken.

</details>


### [549] [SliceVision-F2I: A Synthetic Feature-to-Image Dataset for Visual Pattern Representation on Network Slices](https://arxiv.org/abs/2511.01087)
*Md. Abid Hasan Rafi,Mst. Fatematuj Johora,Pankaj Bhowmik*

Main category: cs.CV

TL;DR: 文章提出用于下一代网络系统网络切片特征可视化研究的数据集SliceVision - F2I，介绍生成方法、特点及适用任务，且数据集公开可复用。


<details>
  <summary>Details</summary>
Motivation: 5G和6G网络使网络切片成为未来面向服务架构重要部分，需要完善的识别方法和强大数据集支持。

Method: 通过物理启发映射、Perlin噪声、神经壁纸和分形分支四种编码方法，将多元关键性能指标（KPI）向量转换为可视化表示，每种方法生成30000个样本。

Result: 生成包含原始KPI向量和低分辨率RGB图像的数据集，模拟了真实有噪声的网络条件。

Conclusion: SliceVision - F2I适用于多种网络任务，可公开复用，用于多个研究领域。

Abstract: The emergence of 5G and 6G networks has established network slicing as a
significant part of future service-oriented architectures, demanding refined
identification methods supported by robust datasets. The article presents
SliceVision-F2I, a dataset of synthetic samples for studying feature
visualization in network slicing for next-generation networking systems. The
dataset transforms multivariate Key Performance Indicator (KPI) vectors into
visual representations through four distinct encoding methods: physically
inspired mappings, Perlin noise, neural wallpapering, and fractal branching.
For each encoding method, 30,000 samples are generated, each comprising a raw
KPI vector and a corresponding RGB image at low-resolution pixels. The dataset
simulates realistic and noisy network conditions to reflect operational
uncertainties and measurement imperfections. SliceVision-F2I is suitable for
tasks involving visual learning, network state classification, anomaly
detection, and benchmarking of image-based machine learning techniques applied
to network data. The dataset is publicly available and can be reused in various
research contexts, including multivariate time series analysis, synthetic data
generation, and feature-to-image transformations.

</details>


### [550] [Learning with Category-Equivariant Architectures for Human Activity Recognition](https://arxiv.org/abs/2511.01139)
*Yoshihiro Maruyama*

Main category: cs.CV

TL;DR: 提出用于人体活动识别的CatEquiv网络，编码多种对称性，在UCI - HAR上展现高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决人体活动识别中如何系统编码时间、幅度和结构对称性以提高模型性能和鲁棒性的问题。

Method: 引入范畴对称积，使CatEquiv网络实现关于范畴对称积的等变性。

Result: 在UCI - HAR的分布外扰动下，CatEquiv比循环填充CNN和平铺CNN有更高的鲁棒性。

Conclusion: 强制范畴对称性可在不增加模型容量的情况下实现强不变性和泛化能力。

Abstract: We propose CatEquiv, a category-equivariant neural network for Human Activity
Recognition (HAR) from inertial sensors that systematically encodes temporal,
amplitude, and structural symmetries. In particular, we introduce the
categorical symmetry product where cyclic time shifts, positive gains and the
sensor-hierarchy poset together capture the categorical symmetry structure of
the data. CatEquiv achieves equivariance with respect to the categorical
symmetry product. On UCI-HAR under out-of-distribution perturbations, CatEquiv
attains markedly higher robustness compared with circularly padded CNNs and
plain CNNs. These results demonstrate that enforcing categorical symmetries
yields strong invariance and generalization without additional model capacity.

</details>


### [551] [MicroAUNet: Boundary-Enhanced Multi-scale Fusion with Knowledge Distillation for Colonoscopy Polyp Image Segmentation](https://arxiv.org/abs/2511.01143)
*Ziyi Wang,Yuanmei Zhang,Dorna Esrafilzadeh,Ali R. Jalili,Suncheng Xiang*

Main category: cs.CV

TL;DR: 提出轻量级注意力分割网络MicroAUNet及渐进式两阶段知识蒸馏方案，在低复杂度下实现结直肠息肉实时分割。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的息肉分割模型存在分割边界模糊或计算复杂度高、推理速度不足的问题，影响临床决策。

Method: 提出MicroAUNet网络，结合深度可分离扩张卷积与单路径、参数共享的通道 - 空间注意力块；引入渐进式两阶段知识蒸馏方案。

Result: 在基准测试上以极低的模型复杂度达到了最先进的准确率。

Conclusion: MicroAUNet适合实时临床息肉分割，代码已公开。

Abstract: Early and accurate segmentation of colorectal polyps is critical for reducing
colorectal cancer mortality, which has been extensively explored by academia
and industry. However, current deep learning-based polyp segmentation models
either compromise clinical decision-making by providing ambiguous polyp margins
in segmentation outputs or rely on heavy architectures with high computational
complexity, resulting in insufficient inference speeds for real-time colorectal
endoscopic applications. To address this problem, we propose MicroAUNet, a
light-weighted attention-based segmentation network that combines
depthwise-separable dilated convolutions with a single-path, parameter-shared
channel-spatial attention block to strengthen multi-scale boundary features. On
the basis of it, a progressive two-stage knowledge-distillation scheme is
introduced to transfer semantic and boundary cues from a high-capacity teacher.
Extensive experiments on benchmarks also demonstrate the state-of-the-art
accuracy under extremely low model complexity, indicating that MicroAUNet is
suitable for real-time clinical polyp segmentation. The code is publicly
available at https://github.com/JeremyXSC/MicroAUNet.

</details>


### [552] [A Topology-Aware Graph Convolutional Network for Human Pose Similarity and Action Quality Assessment](https://arxiv.org/abs/2511.01194)
*Minmin Zeng*

Main category: cs.CV

TL;DR: 提出拓扑感知的图卷积网络GCN - PSN用于动作质量评估，在基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 动作质量评估需要对人体运动的细粒度理解和姿态相似度的精确评估。

Method: 提出GCN - PSN框架，将人体骨骼建模为图以学习判别性、拓扑敏感的姿态嵌入，采用对比回归目标训练的孪生架构。

Result: 优于基于坐标的基线方法，在AQA - 7和FineDiving基准测试中取得有竞争力的性能。

Conclusion: 实验结果和消融研究验证了利用骨骼拓扑进行姿态相似度和动作质量评估的有效性。

Abstract: Action Quality Assessment (AQA) requires fine-grained understanding of human
motion and precise evaluation of pose similarity. This paper proposes a
topology-aware Graph Convolutional Network (GCN) framework, termed GCN-PSN,
which models the human skeleton as a graph to learn discriminative,
topology-sensitive pose embeddings. Using a Siamese architecture trained with a
contrastive regression objective, our method outperforms coordinate-based
baselines and achieves competitive performance on AQA-7 and FineDiving
benchmarks. Experimental results and ablation studies validate the
effectiveness of leveraging skeletal topology for pose similarity and action
quality assessment.

</details>


### [553] [Thought-For-Food: Reasoning Chain Induced Food Visual Question Answering](https://arxiv.org/abs/2511.01213)
*Riddhi Jain,Manasi Patwardhan,Parijat Deshpande,Venkataramana Runkana*

Main category: cs.CV

TL;DR: 现有VQA系统偏向西方食物，本文针对印度食物VQA，通过创建推理链、微调模型和强化学习提升准确率。


<details>
  <summary>Details</summary>
Motivation: 现有VQA系统倾向西方食物，印度食物文化和烹饪多样，现有方法有不足，需多步推理。

Method: 在问答上创建推理链，微调小模型，用强化学习训练，增加推理链。

Result: 推理链增强后，基线准确率平均提高10个百分点。

Conclusion: 对印度食物VQA任务中添加推理链的效果进行了详细分析。

Abstract: The immense diversity in the culture and culinary of Indian cuisines calls
attention to the major shortcoming of the existing Visual Question
Answering(VQA) systems which are inclined towards the foods from Western
region. Recent attempt towards building a VQA dataset for Indian food is a step
towards addressing this challenge. However, their approach towards VQA follows
a two-step process in which the answer is generated first, followed by the
explanation of the expected answer. In this work, we claim that food VQA
requires to follow a multi-step reasoning process to arrive at an accurate
answer, especially in the context of India food, which involves understanding
complex culinary context and identifying relationships between various food
items. With this hypothesis we create reasoning chains upon the QA with minimal
human intervention. We fine-tune smaller LLMs and VLMs with auto-validated
reasoning chains and further train them using reinforcement learning with
larger data. With augmentation of reasoning chains, we observed accuracy
improvement of an average 10 percentage points on the baseline. We provide
detailed analysis in terms the effect of addition of reasoning chains for the
Indian Food VQA task.
  Index Terms - FoodVQA, Reasoning Chains, Reinforcement Learning, Knowledge
Graph.

</details>


### [554] [Eyes on Target: Gaze-Aware Object Detection in Egocentric Video](https://arxiv.org/abs/2511.01237)
*Vishakha Lall,Yisi Liu*

Main category: cs.CV

TL;DR: 提出深度感知和注视引导的目标检测框架Eyes on Target，注入注视特征到ViT注意力机制，实验证明检测精度提升并引入注视感知注意力头重要性指标。


<details>
  <summary>Details</summary>
Motivation: 利用人类注视信号理解复杂视觉环境中的视觉注意力，改进传统目标检测器对所有区域平等对待的问题。

Method: 将注视衍生特征注入ViT注意力机制，强调观众优先区域。

Result: 在自定义模拟器数据集和公共基准上检测精度比无注视基线有持续提升，引入指标揭示注视线索对变压器注意力动态的调节。

Conclusion: 所提方法在评估模拟场景中人类表现有潜力，能有效提升目标检测效果。

Abstract: Human gaze offers rich supervisory signals for understanding visual attention
in complex visual environments. In this paper, we propose Eyes on Target, a
novel depth-aware and gaze-guided object detection framework designed for
egocentric videos. Our approach injects gaze-derived features into the
attention mechanism of a Vision Transformer (ViT), effectively biasing spatial
feature selection toward human-attended regions. Unlike traditional object
detectors that treat all regions equally, our method emphasises
viewer-prioritised areas to enhance object detection. We validate our method on
an egocentric simulator dataset where human visual attention is critical for
task assessment, illustrating its potential in evaluating human performance in
simulation scenarios. We evaluate the effectiveness of our gaze-integrated
model through extensive experiments and ablation studies, demonstrating
consistent gains in detection accuracy over gaze-agnostic baselines on both the
custom simulator dataset and public benchmarks, including Ego4D Ego-Motion and
Ego-CH-Gaze datasets. To interpret model behaviour, we also introduce a
gaze-aware attention head importance metric, revealing how gaze cues modulate
transformer attention dynamics.

</details>


### [555] [Adaptation of Foundation Models for Medical Image Analysis: Strategies, Challenges, and Future Directions](https://arxiv.org/abs/2511.01284)
*Karma Phuntsho,Abdullah,Kyungmi Lee,Ickjai Lee,Euijoon Ahn*

Main category: cs.CV

TL;DR: 该综述评估基础模型适应医学成像需求的策略，分析现有方法优劣势，指出新兴方向和研究差距，为开发适用模型提供路线图。


<details>
  <summary>Details</summary>
Motivation: 基础模型在医学图像分析有潜力，但适应实际临床实践存在诸多挑战，需评估适应策略。

Method: 研究监督微调、特定领域预训练等多种适应策略，评估其性能、临床适用性和局限性。

Result: 分析了各策略的表现、适用性和局限性，指出了新兴方向和研究差距。

Conclusion: 综述为开发适应、可靠且临床可用的基础模型提供了路线图。

Abstract: Foundation models (FMs) have emerged as a transformative paradigm in medical
image analysis, offering the potential to provide generalizable, task-agnostic
solutions across a wide range of clinical tasks and imaging modalities. Their
capacity to learn transferable representations from large-scale data has the
potential to address the limitations of conventional task-specific models.
However, adaptation of FMs to real-world clinical practice remains constrained
by key challenges, including domain shifts, limited availability of
high-quality annotated data, substantial computational demands, and strict
privacy requirements. This review presents a comprehensive assessment of
strategies for adapting FMs to the specific demands of medical imaging. We
examine approaches such as supervised fine-tuning, domain-specific pretraining,
parameter-efficient fine-tuning, self-supervised learning, hybrid methods, and
multimodal or cross-modal frameworks. For each, we evaluate reported
performance gains, clinical applicability, and limitations, while identifying
trade-offs and unresolved challenges that prior reviews have often overlooked.
Beyond these established techniques, we also highlight emerging directions
aimed at addressing current gaps. These include continual learning to enable
dynamic deployment, federated and privacy-preserving approaches to safeguard
sensitive data, hybrid self-supervised learning to enhance data efficiency,
data-centric pipelines that combine synthetic generation with human-in-the-loop
validation, and systematic benchmarking to assess robust generalization under
real-world clinical variability. By outlining these strategies and associated
research gaps, this review provides a roadmap for developing adaptive,
trustworthy, and clinically integrated FMs capable of meeting the demands of
real-world medical imaging.

</details>


### [556] [Perturb a Model, Not an Image: Towards Robust Privacy Protection via Anti-Personalized Diffusion Models](https://arxiv.org/abs/2511.01307)
*Tae-Young Lee,Juwon Seo,Jong Hwan Ko,Gyeong-Moon Park*

Main category: cs.CV

TL;DR: 文章提出Anti - Personalized Diffusion Models (APDM)框架，通过新损失函数DPO和优化策略L2P，防止扩散模型特定主体的非授权个性化，实验效果超现有方法。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在内容创作带来新可能同时引入隐私风险，现有对抗扰动样本方法有局限性，需新方法阻碍特定主体个性化。

Method: 提出APDM框架，引入新损失函数Direct Protective Optimization (DPO)，提出新的双路径优化策略Learning to Protect (L2P)。

Result: 实验表明APDM框架在防止非授权个性化方面达到了当前最优性能。

Conclusion: APDM框架能有效阻碍扩散模型特定主体的个性化，优于现有方法。

Abstract: Recent advances in diffusion models have enabled high-quality synthesis of
specific subjects, such as identities or objects. This capability, while
unlocking new possibilities in content creation, also introduces significant
privacy risks, as personalization techniques can be misused by malicious users
to generate unauthorized content. Although several studies have attempted to
counter this by generating adversarially perturbed samples designed to disrupt
personalization, they rely on unrealistic assumptions and become ineffective in
the presence of even a few clean images or under simple image transformations.
To address these challenges, we shift the protection target from the images to
the diffusion model itself to hinder the personalization of specific subjects,
through our novel framework called Anti-Personalized Diffusion Models (APDM).
We first provide a theoretical analysis demonstrating that a naive approach of
existing loss functions to diffusion models is inherently incapable of ensuring
convergence for robust anti-personalization. Motivated by this finding, we
introduce Direct Protective Optimization (DPO), a novel loss function that
effectively disrupts subject personalization in the target model without
compromising generative quality. Moreover, we propose a new dual-path
optimization strategy, coined Learning to Protect (L2P). By alternating between
personalization and protection paths, L2P simulates future personalization
trajectories and adaptively reinforces protection at each step. Experimental
results demonstrate that our framework outperforms existing methods, achieving
state-of-the-art performance in preventing unauthorized personalization. The
code is available at https://github.com/KU-VGI/APDM.

</details>


### [557] [Integrating Visual and X-Ray Machine Learning Features in the Study of Paintings by Goya](https://arxiv.org/abs/2511.01000)
*Hassan Ugail,Ismail Lujain Jaleel*

Main category: cs.CV

TL;DR: 本文提出多模态机器学习框架用于戈雅画作真伪鉴定，在数据集上取得高准确率，比单模态方法有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 由于戈雅作品风格演变多样和存在大量伪造情况，其画作真伪鉴定面临复杂计算挑战。

Method: 引入多模态机器学习框架，对画作视觉和X射线图像采用相同特征提取技术，包括GLCM描述符、LBP等，将提取特征输入优化的单类支持向量机并进行超参数调优。

Result: 在24幅画作数据集上分类准确率达97.8%，误报率0.022；案例分析中认证置信度达92.3%。

Conclusion: 该框架比单模态方法性能提升显著，证明对视觉和射线图像应用相同计算方法在艺术鉴定中有效。

Abstract: Art authentication of Francisco Goya's works presents complex computational
challenges due to his heterogeneous stylistic evolution and extensive
historical patterns of forgery. We introduce a novel multimodal machine
learning framework that applies identical feature extraction techniques to both
visual and X-ray radiographic images of Goya paintings. The unified feature
extraction pipeline incorporates Grey-Level Co-occurrence Matrix descriptors,
Local Binary Patterns, entropy measures, energy calculations, and colour
distribution analysis applied consistently across both imaging modalities. The
extracted features from both visual and X-ray images are processed through an
optimised One-Class Support Vector Machine with hyperparameter tuning. Using a
dataset of 24 authenticated Goya paintings with corresponding X-ray images,
split into an 80/20 train-test configuration with 10-fold cross-validation, the
framework achieves 97.8% classification accuracy with a 0.022 false positive
rate. Case study analysis of ``Un Gigante'' demonstrates the practical efficacy
of our pipeline, achieving 92.3% authentication confidence through unified
multimodal feature analysis. Our results indicate substantial performance
improvement over single-modal approaches, establishing the effectiveness of
applying identical computational methods to both visual and radiographic
imagery in art authentication applications.

</details>


### [558] [CMI-MTL: Cross-Mamba interaction based multi-task learning for medical visual question answering](https://arxiv.org/abs/2511.01357)
*Qiangguo Jin,Xianyao Zheng,Hui Cui,Changming Sun,Yuqi Fang,Cong Cong,Ran Su,Leyi Wei,Ping Xuan,Junbo Wang*

Main category: cs.CV

TL;DR: 文章提出CMI - MTL框架解决Med - VQA任务难题，实验显示其在多个数据集上优于现有方法，代码公开。


<details>
  <summary>Details</summary>
Motivation: 现有基于自注意力的方法难以有效处理视觉和语言的跨模态语义对齐，基于分类的方法依赖预定义答案集，无法适应自由形式答案的多样性。

Method: 引入CMI - MTL框架，包含FVTA、CIFR和FFAE三个关键模块，分别进行细粒度视觉 - 文本特征对齐、捕捉跨模态顺序交互、利用自由形式答案增强多任务学习。

Result: CMI - MTL在VQA - RAD、SLAKE和OVQA三个Med - VQA数据集上优于现有最先进方法，且可解释性实验证明其有效性。

Conclusion: CMI - MTL框架能有效解决Med - VQA任务中的挑战，提升模型在开放式Med - VQA上的能力。

Abstract: Medical visual question answering (Med-VQA) is a crucial multimodal task in
clinical decision support and telemedicine. Recent self-attention based methods
struggle to effectively handle cross-modal semantic alignments between vision
and language. Moreover, classification-based methods rely on predefined answer
sets. Treating this task as a simple classification problem may make it unable
to adapt to the diversity of free-form answers and overlook the detailed
semantic information of free-form answers. In order to tackle these challenges,
we introduce a Cross-Mamba Interaction based Multi-Task Learning (CMI-MTL)
framework that learns cross-modal feature representations from images and
texts. CMI-MTL comprises three key modules: fine-grained visual-text feature
alignment (FVTA), cross-modal interleaved feature representation (CIFR), and
free-form answer-enhanced multi-task learning (FFAE). FVTA extracts the most
relevant regions in image-text pairs through fine-grained visual-text feature
alignment. CIFR captures cross-modal sequential interactions via cross-modal
interleaved feature representation. FFAE leverages auxiliary knowledge from
open-ended questions through free-form answer-enhanced multi-task learning,
improving the model's capability for open-ended Med-VQA. Experimental results
show that CMI-MTL outperforms the existing state-of-the-art methods on three
Med-VQA datasets: VQA-RAD, SLAKE, and OVQA. Furthermore, we conduct more
interpretability experiments to prove the effectiveness. The code is publicly
available at https://github.com/BioMedIA-repo/CMI-MTL.

</details>


### [559] [SEPS: Semantic-enhanced Patch Slimming Framework for fine-grained cross-modal alignment](https://arxiv.org/abs/2511.01390)
*Xinyu Mao,Junsi Li,Haoji Zhang,Yu Liang,Ming Sun*

Main category: cs.CV

TL;DR: 本文提出SEPS框架解决细粒度跨模态对齐中斑块冗余和歧义问题，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前细粒度跨模态对齐方法在处理斑块冗余和歧义上有挑战，MLLMs虽有潜力但存在输出冲突及语义相关性量化难题。

Method: 引入SEPS框架，采用两阶段机制整合语义，利用相关性感知选择和均值计算突出关键对应关系。

Result: 在Flickr30K和MS - COCO数据集上实验，SEPS在rSum上超现有方法23%-86%，文本到图像检索有显著提升。

Conclusion: SEPS框架有效解决细粒度跨模态对齐问题，性能表现出色，代码可在指定链接获取。

Abstract: Fine-grained cross-modal alignment aims to establish precise local
correspondences between vision and language, forming a cornerstone for visual
question answering and related multimodal applications. Current approaches face
challenges in addressing patch redundancy and ambiguity, which arise from the
inherent information density disparities across modalities. Recently,
Multimodal Large Language Models (MLLMs) have emerged as promising solutions to
bridge this gap through their robust semantic generation capabilities. However,
the dense textual outputs from MLLMs may introduce conflicts with the original
sparse captions. Furthermore, accurately quantifying semantic relevance between
rich visual patches and concise textual descriptions remains a core challenge.
To overcome these limitations, we introduce the Semantic-Enhanced Patch
Slimming (SEPS) framework, which systematically addresses patch redundancy and
ambiguity. Our approach employs a two-stage mechanism to integrate unified
semantics from both dense and sparse texts, enabling the identification of
salient visual patches. Additionally, it leverages relevance-aware selection
with mean value computation to highlight crucial patch-word correspondences,
thereby improving cross-modal similarity assessment. Comprehensive experiments
on Flickr30K and MS-COCO datasets validate that SEPS achieves superior
performance, surpassing existing approaches by 23\%-86\% in rSum across diverse
model architectures, with notable enhancements in text-to-image retrieval
scenarios. Our implementation is available at
https://github.com/Sweet4tars/seps.git.

</details>


### [560] [UniSOT: A Unified Framework for Multi-Modality Single Object Tracking](https://arxiv.org/abs/2511.01427)
*Yinchao Ma,Yuyang Tang,Wenfei Yang,Tianzhu Zhang,Xu Zhou,Feng Wu*

Main category: cs.CV

TL;DR: 提出统一追踪器UniSOT处理不同参考和视频模态组合，实验显示其性能优于特定模态追踪器。


<details>
  <summary>Details</summary>
Motivation: 现有追踪器针对单一或几种模态设计，限制实际应用，需要统一追踪器处理多种需求，且尚无同时跨多种模态追踪的追踪器。

Method: 提出统一追踪器UniSOT，用统一参数处理三种参考模态和四种视频模态的不同组合。

Result: 在18个视觉、视觉 - 语言和RGB+X追踪基准测试中，UniSOT性能优于特定模态追踪器，在TNL2K上AUC超之前追踪器超3.0%，在RGB+X视频模态上主指标超Un - Track超2.0%。

Conclusion: UniSOT能有效处理不同参考和视频模态组合，性能优越。

Abstract: Single object tracking aims to localize target object with specific reference
modalities (bounding box, natural language or both) in a sequence of specific
video modalities (RGB, RGB+Depth, RGB+Thermal or RGB+Event.). Different
reference modalities enable various human-machine interactions, and different
video modalities are demanded in complex scenarios to enhance tracking
robustness. Existing trackers are designed for single or several video
modalities with single or several reference modalities, which leads to separate
model designs and limits practical applications. Practically, a unified tracker
is needed to handle various requirements. To the best of our knowledge, there
is still no tracker that can perform tracking with these above reference
modalities across these video modalities simultaneously. Thus, in this paper,
we present a unified tracker, UniSOT, for different combinations of three
reference modalities and four video modalities with uniform parameters.
Extensive experimental results on 18 visual tracking, vision-language tracking
and RGB+X tracking benchmarks demonstrate that UniSOT shows superior
performance against modality-specific counterparts. Notably, UniSOT outperforms
previous counterparts by over 3.0\% AUC on TNL2K across all three reference
modalities and outperforms Un-Track by over 2.0\% main metric across all three
RGB+X video modalities.

</details>


### [561] [Privacy Preserving Ordinal-Meta Learning with VLMs for Fine-Grained Fruit Quality Prediction](https://arxiv.org/abs/2511.01449)
*Riddhi Jain,Manasi Patwardhan,Aayush Mishra,Parijat Deshpande,Beena Rai*

Main category: cs.CV

TL;DR: 本文提出MAOML算法训练小VLM模型，解决水果新鲜度分类任务数据稀缺问题，零样本和少样本设置下准确率达92.71%。


<details>
  <summary>Details</summary>
Motivation: 有效管理易腐水果浪费需用非侵入式方法准确预测其新鲜度或保质期，现有方法存在数据稀缺、闭源模型有隐私问题、开源模型性能不佳等问题。

Method: 引入Model - Agnostic Ordinal Meta - Learning (MAOML)算法训练较小的VLM模型，利用元学习解决数据稀疏问题并利用标签序。

Result: 方法在所有水果上平均达到92.71%的行业标准准确率。

Conclusion: MAOML算法在水果新鲜度分类任务的零样本和少样本设置下取得了最先进的性能。

Abstract: To effectively manage the wastage of perishable fruits, it is crucial to
accurately predict their freshness or shelf life using non-invasive methods
that rely on visual data. In this regard, deep learning techniques can offer a
viable solution. However, obtaining fine-grained fruit freshness labels from
experts is costly, leading to a scarcity of data. Closed proprietary Vision
Language Models (VLMs), such as Gemini, have demonstrated strong performance in
fruit freshness detection task in both zero-shot and few-shot settings.
Nonetheless, food retail organizations are unable to utilize these proprietary
models due to concerns related to data privacy, while existing open-source VLMs
yield sub-optimal performance for the task. Fine-tuning these open-source
models with limited data fails to achieve the performance levels of proprietary
models. In this work, we introduce a Model-Agnostic Ordinal Meta-Learning
(MAOML) algorithm, designed to train smaller VLMs. This approach utilizes
meta-learning to address data sparsity and leverages label ordinality, thereby
achieving state-of-the-art performance in the fruit freshness classification
task under both zero-shot and few-shot settings. Our method achieves an
industry-standard accuracy of 92.71%, averaged across all fruits.
  Keywords: Fruit Quality Prediction, Vision Language Models, Meta Learning,
Ordinal Regression

</details>


### [562] [Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation](https://arxiv.org/abs/2511.01450)
*Jie Du,Xinyu Gong,Qingshan Tan,Wen Li,Yangming Cheng,Weitao Wang,Chenlu Zhan,Suhui Wu,Hao Zhang,Jun Zhang*

Main category: cs.CV

TL;DR: 提出GT - Pair和Reg - DPO方法解决现有视频生成方法局限，实验显示性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成方法多遵循图像领域范式且基于小规模模型，存在数据构建成本高、训练不稳定和内存消耗大等问题。

Method: 引入GT - Pair自动构建高质量偏好对，提出Reg - DPO将SFT损失作为正则项加入DPO目标，结合FSDP框架和多种内存优化技术。

Result: 训练容量比仅使用FSDP提高近三倍，在多个数据集的I2V和T2V任务上表现优于现有方法。

Conclusion: 所提方法能有效解决现有视频生成方法的局限，提供更优视频生成质量。

Abstract: Recent studies have identified Direct Preference Optimization (DPO) as an
efficient and reward-free approach to improving video generation quality.
However, existing methods largely follow image-domain paradigms and are mainly
developed on small-scale models (approximately 2B parameters), limiting their
ability to address the unique challenges of video tasks, such as costly data
construction, unstable training, and heavy memory consumption. To overcome
these limitations, we introduce a GT-Pair that automatically builds
high-quality preference pairs by using real videos as positives and
model-generated videos as negatives, eliminating the need for any external
annotation. We further present Reg-DPO, which incorporates the SFT loss as a
regularization term into the DPO objective to enhance training stability and
generation fidelity. Additionally, by combining the FSDP framework with
multiple memory optimization techniques, our approach achieves nearly three
times higher training capacity than using FSDP alone. Extensive experiments on
both I2V and T2V tasks across multiple datasets demonstrate that our method
consistently outperforms existing approaches, delivering superior video
generation quality.

</details>


### [563] [When to Trust the Answer: Question-Aligned Semantic Nearest Neighbor Entropy for Safer Surgical VQA](https://arxiv.org/abs/2511.01458)
*Dennis Pierantozzi,Luca Carlini,Mauro Orazio Drago,Chiara Lena,Cesare Hassan,Elena De Momi,Danail Stoyanov,Sophia Bano,Mobarak I. Hoque*

Main category: cs.CV

TL;DR: 研究手术视觉问答（VQA）安全问题，提出QA - SNNE不确定性估计器，评估多种模型，结果显示其能提升性能，助力手术VQA安全。


<details>
  <summary>Details</summary>
Motivation: 现有手术VQA研究忽视安全行为，受自动故障检测（AFD）启发，研究不确定性估计以实现更安全决策。

Method: 引入QA - SNNE，将问题语义融入预测置信度，通过在医学文本嵌入空间对比生成答案与最近邻测量语义熵。评估五种模型。

Result: PEFT模型在轻度释义下性能下降，LVLMs更具弹性；QA - SNNE在多数模板内设置中提高AUROC，增强幻觉检测，零样本模型AUROC提升15 - 38%，且在模板外压力下仍有增益。

Conclusion: QA - SNNE为手术VQA的AFD提供实用可解释的方法，结合LVLM骨干和问题对齐的不确定性估计可提高安全性和临床医生信任度。

Abstract: Safety and reliability are essential for deploying Visual Question Answering
(VQA) in surgery, where incorrect or ambiguous responses can harm the patient.
Most surgical VQA research focuses on accuracy or linguistic quality while
overlooking safety behaviors such as ambiguity awareness, referral to human
experts, or triggering a second opinion. Inspired by Automatic Failure
Detection (AFD), we study uncertainty estimation as a key enabler of safer
decision making. We introduce Question Aligned Semantic Nearest Neighbor
Entropy (QA-SNNE), a black box uncertainty estimator that incorporates question
semantics into prediction confidence. It measures semantic entropy by comparing
generated answers with nearest neighbors in a medical text embedding space,
conditioned on the question. We evaluate five models, including domain specific
Parameter-Efficient Fine-Tuned (PEFT) models and zero-shot Large
Vision-Language Models (LVLMs), on EndoVis18-VQA and PitVQA. PEFT models
degrade under mild paraphrasing, while LVLMs are more resilient. Across three
LVLMs and two PEFT baselines, QA-SNNE improves AUROC in most in-template
settings and enhances hallucination detection. The Area Under the ROC Curve
(AUROC) increases by 15-38% for zero-shot models, with gains maintained under
out-of-template stress. QA-SNNE offers a practical and interpretable step
toward AFD in surgical VQA by linking semantic uncertainty to question context.
Combining LVLM backbones with question aligned uncertainty estimation can
improve safety and clinician trust. The code and model are available at
https://github.com/DennisPierantozzi/QASNNE

</details>


### [564] [Efficiently Training A Flat Neural Network Before It has been Quantizated](https://arxiv.org/abs/2511.01462)
*Peng Xia,Junbiao Pang,Tianyang Cai*

Main category: cs.CV

TL;DR: 本文针对视觉Transformer的训练后量化（PTQ）存在量化误差大及难训练适配低比特模型的问题，提出框架并实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有PTQ方法忽略训练好的神经网络和量化模型的关系，导致量化误差大，且不清楚如何高效训练适配低比特模型的神经网络。

Method: 发现扁平全精度神经网络对低比特量化很关键，提出框架通过测量和解耦误差源对模型进行预调节，将激活量化误差和权重量化误差统计建模为独立高斯噪声，研究噪声注入优化方法以获得扁平最小值。

Result: 实验结果证明了方法的有效性。

Conclusion: 为获得低比特PTQ模型开辟了新途径。

Abstract: Post-training quantization (PTQ) for vision transformers (ViTs) has garnered
significant attention due to its efficiency in compressing models. However,
existing methods typically overlook the relationship between a well-trained NN
and the quantized model, leading to considerable quantization error for PTQ.
However, it is unclear how to efficiently train a model-agnostic neural network
which is tailored for a predefined precision low-bit model. In this paper, we
firstly discover that a flat full precision neural network is crucial for
low-bit quantization. To achieve this, we propose a framework that proactively
pre-conditions the model by measuring and disentangling the error sources.
Specifically, both the Activation Quantization Error (AQE) and the Weight
Quantization Error (WQE) are statistically modeled as independent Gaussian
noises. We study several noise injection optimization methods to obtain a flat
minimum. Experimental results attest to the effectiveness of our approach.
These results open novel pathways for obtaining low-bit PTQ models.

</details>


### [565] [HMVLM: Human Motion-Vision-Lanuage Model via MoE LoRA](https://arxiv.org/abs/2511.01463)
*Lei Hu,Yongjing Ye,Shihong Xia*

Main category: cs.CV

TL;DR: 提出人类运动 - 视觉 - 语言模型（HMVLM）解决人类运动与文本集成问题，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 人类运动与文本集成存在模态差距导致灾难性遗忘问题，且缺乏通用的自回归兼容姿势表示。

Method: 基于混合专家低秩适应（MoE LoRA）策略提出HMVLM框架，利用门控网络动态分配权重，引入零专家缓解遗忘，对人体分区进行特定身体部位的标记化。

Result: 有效缓解指令调优期间的知识遗忘，在多种人类运动下游任务中表现出色。

Conclusion: 所提方法能解决人类运动与文本集成问题，在下游任务有良好性能。

Abstract: The expansion of instruction-tuning data has enabled foundation language
models to exhibit improved instruction adherence and superior performance
across diverse downstream tasks. Semantically-rich 3D human motion is being
progressively integrated with these foundation models to enhance multimodal
understanding and cross-modal generation capabilities. However, the modality
gap between human motion and text raises unresolved concerns about catastrophic
forgetting during this integration. In addition, developing
autoregressive-compatible pose representations that preserve generalizability
across heterogeneous downstream tasks remains a critical technical barrier. To
address these issues, we propose the Human Motion-Vision-Language Model
(HMVLM), a unified framework based on the Mixture of Expert Low-Rank
Adaption(MoE LoRA) strategy. The framework leverages the gating network to
dynamically allocate LoRA expert weights based on the input prompt, enabling
synchronized fine-tuning of multiple tasks. To mitigate catastrophic forgetting
during instruction-tuning, we introduce a novel zero expert that preserves the
pre-trained parameters for general linguistic tasks. For pose representation,
we implement body-part-specific tokenization by partitioning the human body
into different joint groups, enhancing the spatial resolution of the
representation. Experiments show that our method effectively alleviates
knowledge forgetting during instruction-tuning and achieves remarkable
performance across diverse human motion downstream tasks.

</details>


### [566] [MotionStream: Real-Time Video Generation with Interactive Motion Controls](https://arxiv.org/abs/2511.01266)
*Joonghyuk Shin,Zhengqi Li,Richard Zhang,Jun-Yan Zhu,Jaesik Park,Eli Schechtman,Xun Huang*

Main category: cs.CV

TL;DR: 提出MotionStream，实现亚秒级延迟和高达29 FPS的流式视频生成，可实时交互且能生成任意长视频。


<details>
  <summary>Details</summary>
Motivation: 现有运动条件视频生成方法存在延迟高和非因果处理问题，无法实现实时交互。

Method: 先为文本到视频模型添加运动控制，再通过自强制分布匹配蒸馏将双向教师模型提炼为因果学生模型；引入滑动窗口因果注意力和注意力汇，结合自回滚和KV缓存滚动训练。

Result: 模型在运动跟随和视频质量上达到了最先进水平，速度快两个数量级，能实现无限长度的流式生成。

Conclusion: MotionStream能让用户实时看到运动控制结果，提供真正的交互体验。

Abstract: Current motion-conditioned video generation methods suffer from prohibitive
latency (minutes per video) and non-causal processing that prevents real-time
interaction. We present MotionStream, enabling sub-second latency with up to 29
FPS streaming generation on a single GPU. Our approach begins by augmenting a
text-to-video model with motion control, which generates high-quality videos
that adhere to the global text prompt and local motion guidance, but does not
perform inference on the fly. As such, we distill this bidirectional teacher
into a causal student through Self Forcing with Distribution Matching
Distillation, enabling real-time streaming inference. Several key challenges
arise when generating videos of long, potentially infinite time-horizons: (1)
bridging the domain gap from training on finite length and extrapolating to
infinite horizons, (2) sustaining high quality by preventing error
accumulation, and (3) maintaining fast inference, without incurring growth in
computational cost due to increasing context windows. A key to our approach is
introducing carefully designed sliding-window causal attention, combined with
attention sinks. By incorporating self-rollout with attention sinks and KV
cache rolling during training, we properly simulate inference-time
extrapolations with a fixed context window, enabling constant-speed generation
of arbitrarily long videos. Our models achieve state-of-the-art results in
motion following and video quality while being two orders of magnitude faster,
uniquely enabling infinite-length streaming. With MotionStream, users can paint
trajectories, control cameras, or transfer motion, and see results unfold in
real-time, delivering a truly interactive experience.

</details>


### [567] [Driving scenario generation and evaluation using a structured layer representation and foundational models](https://arxiv.org/abs/2511.01541)
*Arthur Hubert,Gamal Elghazaly,Raphaël Frank*

Main category: cs.CV

TL;DR: 提出结构化五层模型结合大基础模型生成自动驾驶罕见场景，研究两个评估指标并展示效果。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中罕见场景难遇到，需模拟或生成，改进现有场景表示以更好评估和生成。

Method: 提出结构化五层模型，结合大基础模型用数据增强策略生成场景，引入子类和特征，使用特定嵌入比较，研究适应两个评估指标。

Result: 展示了不同生成设置下的两个指标，对合成视频进行定性评估。

Conclusion: 结构化五层模型有助于自动驾驶罕见场景的评估和生成，相关代码和结果公开。

Abstract: Rare and challenging driving scenarios are critical for autonomous vehicle
development. Since they are difficult to encounter, simulating or generating
them using generative models is a popular approach. Following previous efforts
to structure driving scenario representations in a layer model, we propose a
structured five-layer model to improve the evaluation and generation of rare
scenarios. We use this model alongside large foundational models to generate
new driving scenarios using a data augmentation strategy. Unlike previous
representations, our structure introduces subclasses and characteristics for
every agent of the scenario, allowing us to compare them using an embedding
specific to our layer-model. We study and adapt two metrics to evaluate the
relevance of a synthetic dataset in the context of a structured representation:
the diversity score estimates how different the scenarios of a dataset are from
one another, while the originality score calculates how similar a synthetic
dataset is from a real reference set. This paper showcases both metrics in
different generation setup, as well as a qualitative evaluation of synthetic
videos generated from structured scenario descriptions. The code and extended
results can be found at https://github.com/Valgiz/5LMSG.

</details>


### [568] [Extremal Contours: Gradient-driven contours for compact visual attribution](https://arxiv.org/abs/2511.01411)
*Reza Karimzadeh,Albert Alonso,Frans Zdyb,Julius B. Kirkegaard,Bulat Ibragimov*

Main category: cs.CV

TL;DR: 提出一种免训练的视觉模型解释方法，用平滑可调轮廓替代密集掩码，在多个方面表现良好。


<details>
  <summary>Details</summary>
Motivation: 常用的密集扰动掩码碎片化且过拟合，需要仔细后处理，难以提供忠实且紧凑的视觉模型解释。

Method: 用截断傅里叶级数参数化星凸区域，在极值保留/删除目标下使用分类器梯度进行优化。

Result: 保证单一、简单连通的掩码，大幅减少自由参数数量，生成紧凑、可解释区域，在多个基准测试中表现优于基于梯度和扰动的基线方法。

Conclusion: 该方法能有效解决现有视觉模型解释方法的问题，可扩展到多轮廓并定位多个对象。

Abstract: Faithful yet compact explanations for vision models remain a challenge, as
commonly used dense perturbation masks are often fragmented and overfitted,
needing careful post-processing. Here, we present a training-free explanation
method that replaces dense masks with smooth tunable contours. A star-convex
region is parameterized by a truncated Fourier series and optimized under an
extremal preserve/delete objective using the classifier gradients. The approach
guarantees a single, simply connected mask, cuts the number of free parameters
by orders of magnitude, and yields stable boundary updates without cleanup.
Restricting solutions to low-dimensional, smooth contours makes the method
robust to adversarial masking artifacts. On ImageNet classifiers, it matches
the extremal fidelity of dense masks while producing compact, interpretable
regions with improved run-to-run consistency. Explicit area control also
enables importance contour maps, yielding a transparent fidelity-area profiles.
Finally, we extend the approach to multi-contour and show how it can localize
multiple objects within the same framework. Across benchmarks, the method
achieves higher relevance mass and lower complexity than gradient and
perturbation based baselines, with especially strong gains on self-supervised
DINO models where it improves relevance mass by over 15% and maintains positive
faithfulness correlations.

</details>


### [569] [DINO-MX: A Modular & Flexible Framework for Self-Supervised Learning](https://arxiv.org/abs/2511.01610)
*Mahmut Selman Gokmen,Cody Bumgardner*

Main category: cs.CV

TL;DR: 介绍DINO - MX训练框架，它结合DINO系列原则，支持多种策略和架构，实验显示性能优且降低计算成本，可用于多场景。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型训练管道存在不灵活、特定领域或计算成本高的问题，限制其在不同领域和资源环境的可用性。

Method: 构建模块化、可扩展的DINO - MX训练框架，结合DINO系列核心原则，支持多种Transformer架构，采用多种训练策略和分布式训练方式。

Result: 在不同数据集实验中，DINO - MX性能有竞争力，显著降低计算成本，还提供解释工具和标签引导的数据增强方法。

Conclusion: DINO - MX为开发、适配和基准测试自监督视觉模型提供可重现和可扩展的基础。

Abstract: Vision Foundation Models (VFMs) have advanced representation learning through
self-supervised methods. However, existing training pipelines are often
inflexible, domain-specific, or computationally expensive, which limits their
usability across different domains and resource settings. DINO-MX is a modular
and extensible training framework that combines the core principles of DINO,
DINOv2 and DINOv3 within a unified configuration-driven system. It supports a
variety of transformer-based architectures and is fully compatible with the
Hugging Face ecosystem. The framework includes multiple training strategies
such as low-rank adaptation (LoRA), layer freezing, and knowledge distillation,
along with support for distributed training through both Distributed Data
Parallel (DDP) and Fully Sharded Data Parallel (FSDP). DINO-MX is designed to
work with both natural and specialized data types, including single- and
multi-channel images. Experimental results on diverse datasets show that
DINO-MX achieves competitive performance while significantly reducing
computational costs. Additionally, it offers interpretability tools and a
label-guided data augmentation method that improves attention-based
localization without the need for extra detection or segmentation heads.
DINO-MX provides a reproducible and scalable foundation for developing,
adapting, and benchmarking self-supervised vision models across a range of
research and real-world applications.

</details>


### [570] [Probabilistic Robustness for Free? Revisiting Training via a Benchmark](https://arxiv.org/abs/2511.01724)
*Yi Zhang,Zheng Wang,Chen Zhen,Wenjie Ruan,Qing Guo,Siddartha Khastgir,Carsten Maple,Xingyu Zhao*

Main category: cs.CV

TL;DR: 提出首个评估概率鲁棒性改进的基准PRBench，比较多种训练方法并给出主要发现，还提供排行榜。


<details>
  <summary>Details</summary>
Motivation: 现有改进概率鲁棒性（PR）的训练方法研究不足，且存在评估协议不可比、与强对抗训练（AT）基线对比有限、无统一框架比较泛化性等局限。

Method: 引入PRBench，用综合指标对比常见AT和PR目标训练方法，并对不同训练方法PR性能的泛化误差进行理论分析。

Result: AT方法在不同超参数设置下提升对抗鲁棒性（AR）和PR性能更通用，PR目标训练方法泛化误差更低、干净准确率更高。

Conclusion: PRBench有助于评估不同鲁棒性训练方法对PR的改进，公开的排行榜可促进相关研究。

Abstract: Deep learning models are notoriously vulnerable to imperceptible
perturbations. Most existing research centers on adversarial robustness (AR),
which evaluates models under worst-case scenarios by examining the existence of
deterministic adversarial examples (AEs). In contrast, probabilistic robustness
(PR) adopts a statistical perspective, measuring the probability that
predictions remain correct under stochastic perturbations. While PR is widely
regarded as a practical complement to AR, dedicated training methods for
improving PR are still relatively underexplored, albeit with emerging progress.
Among the few PR-targeted training methods, we identify three limitations: i
non-comparable evaluation protocols; ii limited comparisons to strong AT
baselines despite anecdotal PR gains from AT; and iii no unified framework to
compare the generalization of these methods. Thus, we introduce PRBench, the
first benchmark dedicated to evaluating improvements in PR achieved by
different robustness training methods. PRBench empirically compares most common
AT and PR-targeted training methods using a comprehensive set of metrics,
including clean accuracy, PR and AR performance, training efficiency, and
generalization error (GE). We also provide theoretical analysis on the GE of PR
performance across different training methods. Main findings revealed by
PRBench include: AT methods are more versatile than PR-targeted training
methods in terms of improving both AR and PR performance across diverse
hyperparameter settings, while PR-targeted training methods consistently yield
lower GE and higher clean accuracy. A leaderboard comprising 222 trained models
across 7 datasets and 10 model architectures is publicly available at
https://tmpspace.github.io/PRBenchLeaderboard/.

</details>


### [571] [Wonder3D++: Cross-domain Diffusion for High-fidelity 3D Generation from a Single Image](https://arxiv.org/abs/2511.01767)
*Yuxiao Yang,Xiao-Xiao Long,Zhiyang Dou,Cheng Lin,Yuan Liu,Qingsong Yan,Yuexin Ma,Haoqian Wang,Zhiqiang Wu,Wei Yin*

Main category: cs.CV

TL;DR: 提出Wonder3D++方法，从单视图图像高效生成高保真纹理网格，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于SDS方法优化耗时且几何不一致，直接网络推理结果质量低、缺细节，需提升单视图重建任务的质量、一致性和效率。

Method: 提出跨域扩散模型生成多视图法线图和对应彩色图像，采用多视图跨域注意力机制确保生成一致性，引入级联3D网格提取算法以粗到细方式驱动高质量表面。

Result: 广泛评估表明该方法实现高质量重建结果、强大泛化能力和良好效率。

Conclusion: Wonder3D++方法在单视图重建任务中表现出色，代码开源。

Abstract: In this work, we introduce \textbf{Wonder3D++}, a novel method for
efficiently generating high-fidelity textured meshes from single-view images.
Recent methods based on Score Distillation Sampling (SDS) have shown the
potential to recover 3D geometry from 2D diffusion priors, but they typically
suffer from time-consuming per-shape optimization and inconsistent geometry. In
contrast, certain works directly produce 3D information via fast network
inferences, but their results are often of low quality and lack geometric
details. To holistically improve the quality, consistency, and efficiency of
single-view reconstruction tasks, we propose a cross-domain diffusion model
that generates multi-view normal maps and the corresponding color images. To
ensure the consistency of generation, we employ a multi-view cross-domain
attention mechanism that facilitates information exchange across views and
modalities. Lastly, we introduce a cascaded 3D mesh extraction algorithm that
drives high-quality surfaces from the multi-view 2D representations in only
about $3$ minute in a coarse-to-fine manner. Our extensive evaluations
demonstrate that our method achieves high-quality reconstruction results,
robust generalization, and good efficiency compared to prior works. Code
available at https://github.com/xxlong0/Wonder3D/tree/Wonder3D_Plus.

</details>


### [572] [How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment](https://arxiv.org/abs/2511.01775)
*Zhen Chen,Qing Xu,Jinlin Wu,Biao Yang,Yuhao Zhai,Geng Guo,Jing Zhang,Yinlu Ding,Nassir Navab,Jiebo Luo*

Main category: cs.CV

TL;DR: 提出SurgVeo基准和SPP框架评估手术视频生成模型，发现Veo - 3模型存在“合理性差距”，为未来模型发展奠基。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成基础模型在手术等高风险领域应用缺乏专业因果知识，有未探索的空白。

Method: 提出SurgVeo基准和SPP框架，让Veo - 3模型进行零样本预测任务，由外科医生根据SPP评估生成视频。

Result: Veo - 3模型在视觉感知合理性表现出色，但在SPP更高层次如器械操作、环境反馈和手术意图合理性方面失败。

Conclusion: 为手术AI中视觉模仿和因果理解的差距提供定量证据，为未来模型发展提供基础和路线图。

Abstract: Foundation models in video generation are demonstrating remarkable
capabilities as potential world models for simulating the physical world.
However, their application in high-stakes domains like surgery, which demand
deep, specialized causal knowledge rather than general physical rules, remains
a critical unexplored gap. To systematically address this challenge, we present
SurgVeo, the first expert-curated benchmark for video generation model
evaluation in surgery, and the Surgical Plausibility Pyramid (SPP), a novel,
four-tiered framework tailored to assess model outputs from basic appearance to
complex surgical strategy. On the basis of the SurgVeo benchmark, we task the
advanced Veo-3 model with a zero-shot prediction task on surgical clips from
laparoscopic and neurosurgical procedures. A panel of four board-certified
surgeons evaluates the generated videos according to the SPP. Our results
reveal a distinct "plausibility gap": while Veo-3 achieves exceptional Visual
Perceptual Plausibility, it fails critically at higher levels of the SPP,
including Instrument Operation Plausibility, Environment Feedback Plausibility,
and Surgical Intent Plausibility. This work provides the first quantitative
evidence of the chasm between visually convincing mimicry and causal
understanding in surgical AI. Our findings from SurgVeo and the SPP establish a
crucial foundation and roadmap for developing future models capable of
navigating the complexities of specialized, real-world healthcare domains.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [573] [Dynamic Logic of Trust-Based Beliefs](https://arxiv.org/abs/2511.00899)
*Junli Jiang,Pavel Naumov,Wenxuan Zhang*

Main category: cs.LO

TL;DR: 研究包含数据公开宣告的动态信念逻辑，给出公理系统和模型检查算法。


<details>
  <summary>Details</summary>
Motivation: 现代世界中主体信念常基于可用数据，研究这种包含数据公开宣告的动态信念逻辑。

Method: 对公理化系统进行构建和证明其可靠性与完备性，设计模型检查算法。

Result: 得到数据信息信念和数据宣告模态之间相互作用的可靠且完备的公理化，以及该逻辑系统的非平凡多项式模型检查算法。

Conclusion: 完成了对包含数据公开宣告的动态信念逻辑的公理化及算法设计。

Abstract: Traditionally, an agent's beliefs would come from what the agent can see,
hear, or sense. In the modern world, beliefs are often based on the data
available to the agents. In this work, we investigate a dynamic logic of such
beliefs that incorporates public announcements of data. The main technical
contribution is a sound and complete axiomatisation of the interplay between
data-informed beliefs and data announcement modalities. We also describe a
non-trivial polynomial model checking algorithm for this logical system.

</details>


### [574] [SM-based Semantics for Answer Set Programs Containing Conditional Literals and Arithmetic](https://arxiv.org/abs/2511.01753)
*Zachary Hansen,Yuliya Lierler*

Main category: cs.LO

TL;DR: 提出基于SM算子的含条件文字和算术的逻辑程序语义，证明其与现有语义对应关系。


<details>
  <summary>Details</summary>
Motivation: 现代ASP求解器支持条件文字等高级语言构造，现有此类程序语义依赖于转换为无穷命题逻辑，需要进行接地，本文旨在提出无需接地的语义。

Method: 基于SM算子为含条件文字和算术的逻辑程序提出语义。

Result: 建立了所提出语义与现有语义之间的精确对应关系。

Conclusion: 所提语义无需接地，且与现有语义有精确对应，为逻辑程序语义研究提供新途径。

Abstract: Modern answer set programming solvers such as CLINGO support advanced
language constructs that improve the expressivity and conciseness of logic
programs. Conditional literals are one such construct. They form "subformulas"
that behave as nested implications within the bodies of logic rules. Their
inclusion brings the form of rules closer to the less restrictive syntax of
first-order logic. These qualities make conditional literals useful tools for
knowledge representation. In this paper, we propose a semantics for logic
programs with conditional literals and arithmetic based on the SM operator.
These semantics do not require grounding, unlike the established semantics for
such programs that relies on a translation to infinitary propositional logic.
The main result of this paper establishes the precise correspondence between
the proposed and existing semantics.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [575] [Sherlock: Reliable and Efficient Agentic Workflow Execution](https://arxiv.org/abs/2511.00330)
*Yeonju Ro,Haoran Qiu,Íñigo Goiri,Rodrigo Fonseca,Ricardo Bianchini,Aditya Akella,Zhangyang Wang,Mattan Erez,Esha Choukse*

Main category: cs.MA

TL;DR: 随着大语言模型应用增加，代理工作流易出错，本文提出Sherlock方法，通过反事实分析识别易错节点，选择性验证，兼顾效率与可靠性，提升准确率，降低执行时间和验证成本。


<details>
  <summary>Details</summary>
Motivation: 代理工作流易出错，现有验证方法有显著延迟和成本开销，需解决哪些节点需验证、选合适验证器及减少对延迟影响的问题。

Method: 使用反事实分析识别易错节点，选择性附加成本最优验证器，运行时推测执行下游任务，验证在后台运行，验证失败则回滚。

Result: 相比无验证基线，平均准确率提升18.3%；相比非推测执行，工作流执行时间最多减少48.7%；相比基于蒙特卡罗搜索的方法，验证成本降低26.0%。

Conclusion: 有原则的、故障感知的验证能有效平衡代理工作流的效率和可靠性。

Abstract: With the increasing adoption of large language models (LLM), agentic
workflows, which compose multiple LLM calls with tools, retrieval, and
reasoning steps, are increasingly replacing traditional applications. However,
such workflows are inherently error-prone: incorrect or partially correct
output at one step can propagate or even amplify through subsequent stages,
compounding the impact on the final output. Recent work proposes integrating
verifiers that validate LLM output or actions, such as self-reflection, debate,
or LLM-as-a-judge mechanisms. Yet, verifying every step introduces significant
latency and cost overheads.
  In this work, we seek to answer three key questions: which nodes in a
workflow are most error-prone and thus deserve costly verification, how to
select the most appropriate verifier for each node, and how to use verification
with minimal impact to latency? Our solution, Sherlock, addresses these using
counterfactual analysis on agentic workflows to identify error-prone nodes and
selectively attaching cost-optimal verifiers only where necessary. At runtime,
Sherlock speculatively executes downstream tasks to reduce latency overhead,
while verification runs in the background. If verification fails, execution is
rolled back to the last verified output. Compared to the non-verifying
baseline, Sherlock delivers an 18.3% accuracy gain on average across
benchmarks. Sherlock reduces workflow execution time by up to 48.7% over
non-speculative execution and lowers verification cost by 26.0% compared to the
Monte Carlo search-based method, demonstrating that principled, fault-aware
verification effectively balances efficiency and reliability in agentic
workflows.

</details>


### [576] [AgentGit: A Version Control Framework for Reliable and Scalable LLM-Powered Multi-Agent Systems](https://arxiv.org/abs/2511.00628)
*Yang Li,Siqi Ping,Xiyu Chen,Xiaojian Qi,Zigan Wang,Ye Luo,Xiaowei Zhang*

Main category: cs.MA

TL;DR: 提出AgentGit框架，为多智能体系统工作流引入类Git功能，实验表明其可增强可靠性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统框架在复杂任务上存在可靠性和可扩展性问题。

Method: 构建基于LangGraph的AgentGit框架，支持状态提交、回滚和分支操作；设计通过选择更好提示优化目标智能体的实验，进行多步A/B测试。

Result: AgentGit显著减少冗余计算，降低运行时间和令牌使用量，支持多分支并行探索。

Conclusion: 该工作为更强大的多智能体系统设计提供了实用途径，支持协作式人工智能系统中的错误恢复、安全探索、迭代调试和A/B测试。

Abstract: With the rapid progress of large language models (LLMs), LLM-powered
multi-agent systems (MAS) are drawing increasing interest across academia and
industry. However, many current MAS frameworks struggle with reliability and
scalability, especially on complex tasks. We present AgentGit, a framework that
brings Git-like rollback and branching to MAS workflows. Built as an
infrastructure layer on top of LangGraph, AgentGit supports state commit,
revert, and branching, allowing agents to traverse, compare, and explore
multiple trajectories efficiently. To evaluate AgentGit, we designed an
experiment that optimizes target agents by selecting better prompts. We ran a
multi-step A/B test against three baselines -- LangGraph, AutoGen, and Agno --
on a real-world task: retrieving and analyzing paper abstracts. Results show
that AgentGit significantly reduces redundant computation, lowers runtime and
token usage, and supports parallel exploration across multiple branches,
enhancing both reliability and scalability in MAS development. This work offers
a practical path to more robust MAS design and enables error recovery, safe
exploration, iterative debugging, and A/B testing in collaborative AI systems.

</details>


### [577] [Urban-MAS: Human-Centered Urban Prediction with LLM-Based Multi-Agent System](https://arxiv.org/abs/2511.00096)
*Shangyu Lou*

Main category: cs.MA

TL;DR: 提出基于LLM的Multi - Agent System框架Urban - MAS用于零样本下以人为中心的城市预测，实验表明其能大幅减少误差，定位为可扩展范式。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理特定领域城市任务时表现不佳，需新方法进行以人为中心的城市预测。

Method: 引入Urban - MAS框架，包含Predictive Factor Guidance Agents、Reliable UrbanInfo Extraction Agents和Multi - UrbanInfo Inference Agents三种代理类型。

Result: 在东京、米兰和西雅图的运行量预测和城市感知实验中，Urban - MAS比单LLM基线大幅减少误差，消融实验表明Predictive Factor Guidance Agents对提升预测性能最关键。

Conclusion: Urban - MAS是以人为中心的城市AI预测的可扩展范式。

Abstract: Urban Artificial Intelligence (Urban AI) has advanced human-centered urban
tasks such as perception prediction and human dynamics. Large Language Models
(LLMs) can integrate multimodal inputs to address heterogeneous data in complex
urban systems but often underperform on domain-specific tasks. Urban-MAS, an
LLM-based Multi-Agent System (MAS) framework, is introduced for human- centered
urban prediction under zero-shot settings. It includes three agent types:
Predictive Factor Guidance Agents, which prioritize key predictive factors to
guide knowledge extraction and enhance the effectiveness of compressed urban
knowledge in LLMs; Reliable UrbanInfo Extraction Agents, which improve
robustness by com- paring multiple outputs, validating consistency, and
re-extracting when conflicts occur; and Multi-UrbanInfo Inference Agents, which
integrate extracted multi-source information across dimensions for prediction.
Experiments on running-amount prediction and ur- ban perception across Tokyo,
Milan, and Seattle demonstrate that Urban-MAS substantially reduces errors
compared to single-LLM baselines. Ablation studies indicate that Predictive
Factor Guidance Agents are most critical for enhancing predictive performance,
po- sitioning Urban-MAS as a scalable paradigm for human-centered urban AI
prediction. Code is available on the project
website:https://github.com/THETUREHOOHA/UrbanMAS

</details>


### [578] [On the Fundamental Limitations of Decentralized Learnable Reward Shaping in Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.00034)
*Aditya Akella*

Main category: cs.MA

TL;DR: 研究提出DMARL - RSA用于合作多智能体场景，评估发现其性能不如集中式训练方法，揭示分散式奖励学习局限及协调悖论。


<details>
  <summary>Details</summary>
Motivation: 当前对合作多智能体场景中分散式可学习奖励塑造的有效性了解不足。

Method: 提出DMARL - RSA系统，每个智能体学习个体奖励塑造，并在simple_spread_v3环境的合作导航任务中评估。

Result: DMARL - RSA平均奖励远低于集中式训练的MAPPO，与简单独立学习表现相近；分散式方法地标覆盖率高但整体性能差；分析出三个关键障碍。

Conclusion: 确定了分散式奖励学习的经验限制，强调有效多智能体合作需集中式协调。

Abstract: Recent advances in learnable reward shaping have shown promise in
single-agent reinforcement learning by automatically discovering effective
feedback signals. However, the effectiveness of decentralized learnable reward
shaping in cooperative multi-agent settings remains poorly understood. We
propose DMARL-RSA, a fully decentralized system where each agent learns
individual reward shaping, and evaluate it on cooperative navigation tasks in
the simple_spread_v3 environment. Despite sophisticated reward learning,
DMARL-RSA achieves only -24.20 +/- 0.09 average reward, compared to MAPPO with
centralized training at 1.92 +/- 0.87--a 26.12-point gap. DMARL-RSA performs
similarly to simple independent learning (IPPO: -23.19 +/- 0.96), indicating
that advanced reward shaping cannot overcome fundamental decentralized
coordination limitations. Interestingly, decentralized methods achieve higher
landmark coverage (0.888 +/- 0.029 for DMARL-RSA, 0.960 +/- 0.045 for IPPO out
of 3 total) but worse overall performance than centralized MAPPO (0.273 +/-
0.008 landmark coverage)--revealing a coordination paradox between local
optimization and global performance. Analysis identifies three critical
barriers: (1) non-stationarity from concurrent policy updates, (2) exponential
credit assignment complexity, and (3) misalignment between individual reward
optimization and global objectives. These results establish empirical limits
for decentralized reward learning and underscore the necessity of centralized
coordination for effective multi-agent cooperation.

</details>


### [579] [Learning what to say and how precisely: Efficient Communication via Differentiable Discrete Communication Learning](https://arxiv.org/abs/2511.01554)
*Aditya Kapoor,Yash Bhisikar,Benjamin Freed,Jan Peters,Mingfei Sun*

Main category: cs.MA

TL;DR: 本文推广了DDCL框架以支持无界信号，验证该方法能让智能体动态调整消息精度、降低带宽并质疑定制通信设计必要性。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习中有效通信受带宽限制，以往方法只能决定是否通信，不能精确控制。

Method: 推广Differentiable Discrete Communication Learning (DDCL) 框架，使其成为适用于任何MARL架构的通用即插即用层。

Result: 1. 展示智能体如何根据任务信息需求动态调整消息精度；2. 集成到四种先进MARL算法中，降低一个数量级以上带宽，且不影响任务表现；3. 简单的基于Transformer策略能达到复杂架构性能。

Conclusion: 推广的DDCL框架有效，质疑了定制通信设计的必要性。

Abstract: Effective communication in multi-agent reinforcement learning (MARL) is
critical for success but constrained by bandwidth, yet past approaches have
been limited to complex gating mechanisms that only decide \textit{whether} to
communicate, not \textit{how precisely}. Learning to optimize message precision
at the bit-level is fundamentally harder, as the required discretization step
breaks gradient flow. We address this by generalizing Differentiable Discrete
Communication Learning (DDCL), a framework for end-to-end optimization of
discrete messages. Our primary contribution is an extension of DDCL to support
unbounded signals, transforming it into a universal, plug-and-play layer for
any MARL architecture. We verify our approach with three key results. First,
through a qualitative analysis in a controlled environment, we demonstrate
\textit{how} agents learn to dynamically modulate message precision according
to the informational needs of the task. Second, we integrate our variant of
DDCL into four state-of-the-art MARL algorithms, showing it reduces bandwidth
by over an order of magnitude while matching or exceeding task performance.
Finally, we provide direct evidence for the \enquote{Bitter Lesson} in MARL
communication: a simple Transformer-based policy leveraging DDCL matches the
performance of complex, specialized architectures, questioning the necessity of
bespoke communication designs.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [580] [Fast, memory-efficient genomic interval tokenizers for modern machine learning](https://arxiv.org/abs/2511.01555)
*Nathan J. LeRoy,Donald R. Campbell Jr,Seth Stadick,Oleksandr Khoroshevskyi,Sang-Hoon Park,Ziyang Hu,Nathan C. Sheffield*

Main category: q-bio.GN

TL;DR: 介绍gtars - tokenizers库，可将基因组区间映射到预定义区域，实现高效处理基因组区间数据。


<details>
  <summary>Details</summary>
Motivation: 高通量测序实验的表观基因组数据集以基因组区间形式存在，其数据异质性给机器学习方法带来障碍，需要解决该问题。

Method: 引入gtars - tokenizers库，用Rust构建，有多种语言绑定，实现两种重叠方法，通过兼容Hugging Face的API与现代ML框架集成。

Result: gtars - tokenizers包处理大规模数据集效率高，能让基因组区间用标准ML工作流处理，无需特殊预处理。

Conclusion: 基于令牌的方法连接了基因组学和机器学习，支持在不同计算环境中对区间数据进行可扩展和标准化分析。

Abstract: Introduction: Epigenomic datasets from high-throughput sequencing experiments
are commonly summarized as genomic intervals. As the volume of this data grows,
so does interest in analyzing it through deep learning. However, the
heterogeneity of genomic interval data, where each dataset defines its own
regions, creates barriers for machine learning methods that require consistent,
discrete vocabularies. Methods: We introduce gtars-tokenizers, a
high-performance library that maps genomic intervals to a predefined universe
or vocabulary of regions, analogous to text tokenization in natural language
processing. Built in Rust with bindings for Python, R, CLI, and WebAssembly,
gtars-tokenizers implements two overlap methods (BITS and AIList) and
integrates seamlessly with modern ML frameworks through Hugging Face-compatible
APIs. Results: The gtars-tokenizers package achieves top efficiency for
large-scale datasets, while enabling genomic intervals to be processed using
standard ML workflows in PyTorch and TensorFlow without ad hoc preprocessing.
This token-based approach bridges genomics and machine learning, supporting
scalable and standardized analysis of interval data across diverse
computational environments. Availability: PyPI and GitHub:
https://github.com/databio/gtars.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [581] [Dynamic Modeling of Precipitation in Electrolyte Systems](https://arxiv.org/abs/2511.01519)
*Niklas Kemmerling,Sergio Lucia*

Main category: physics.chem-ph

TL;DR: 本文提出电解质系统沉淀动态建模方法，用于芳香胺连续结晶，模型预测与实验相符，可助力从间歇到连续过程的转变。


<details>
  <summary>Details</summary>
Motivation: 为电解质系统沉淀过程建模，助力从间歇过程向连续过程的转变。

Method: 构建整合平衡和结晶动力学的新模型，以微分代数方程表示，采用种群平衡方程描述粒度分布和动态平衡。

Result: 动态模型的预测结果与现有实验测量结果吻合良好。

Conclusion: 该模型可作为数值优化和先进控制的基础，有助于从间歇过程过渡到连续过程。

Abstract: This study presents a dynamic modeling approach for precipitation in
electrolyte systems, focusing on the crystallization of an aromatic amine
through continuous processes. A novel model, integrating equilibrium and
crystallization kinetics, is formulated and applied to a continuous oscillatory
baffled reactor. The approach assumes rapid equilibrium establishment and is
formulated as a set of differential algebraic equations. Key features include a
population balance equation model to describe the particle size distribution
and the modeling of dynamically changing equilibria. The predictions of the
dynamic model show good agreement with the available experimental measurements.
The model is aimed at aiding the transition from a batch process to continuous
process by forming the basis for numerical optimization and advanced control.

</details>


### [582] [Generative Modeling Enables Molecular Structure Retrieval from Coulomb Explosion Imaging](https://arxiv.org/abs/2511.00179)
*Xiang Li,Till Jahnke,Rebecca Boll,Jiaqi Han,Minkai Xu,Michael Meyer,Maria Novella Piancastelli,Daniel Rolles,Artem Rudenko,Florian Trinter,Thomas J. A. Wolf,Jana B. Thayer,James P. Cryan,Stefano Ermon,Phay J. Ho*

Main category: physics.chem-ph

TL;DR: 本文用基于扩散的Transformer神经网络解决从离子动量分布中重建分子几何结构的难题，重建误差小于一个玻尔半径。


<details>
  <summary>Details</summary>
Motivation: 理解和控制飞秒化学需在时空上捕捉分子化学反应中的结构变化，而从离子动量分布中检索分子结构是未解决的非线性逆问题。

Method: 使用基于扩散的Transformer神经网络。

Result: 网络能从离子动量分布中重建未知分子几何结构，平均绝对误差低于一个玻尔半径。

Conclusion: 基于扩散的Transformer神经网络可有效解决从离子动量分布重建分子结构的难题。

Abstract: Capturing the structural changes that molecules undergo during chemical
reactions in real space and time is a long-standing dream and an essential
prerequisite for understanding and ultimately controlling femtochemistry. A key
approach to tackle this challenging task is Coulomb explosion imaging, which
benefited decisively from recently emerging high-repetition-rate X-ray
free-electron laser sources. With this technique, information on the molecular
structure is inferred from the momentum distributions of the ions produced by
the rapid Coulomb explosion of molecules. Retrieving molecular structures from
these distributions poses a highly non-linear inverse problem that remains
unsolved for molecules consisting of more than a few atoms. Here, we address
this challenge using a diffusion-based Transformer neural network. We show that
the network reconstructs unknown molecular geometries from ion-momentum
distributions with a mean absolute error below one Bohr radius, which is half
the length of a typical chemical bond.

</details>


### [583] [Split-Flows: Measure Transport and Information Loss Across Molecular Resolutions](https://arxiv.org/abs/2511.01464)
*Sander Hummerich,Tristan Bereau,Ullrich Köthe*

Main category: physics.chem-ph

TL;DR: 本文提出split - flows方法用于分子模型的反向映射，能实现原子结构采样和计算映射熵，还在多种分子系统中得到验证。


<details>
  <summary>Details</summary>
Motivation: 粗粒度模型虽加速分子模拟但丢失微观信息，恢复细粒度细节对依赖原子精度的任务至关重要，反向映射是分子建模的核心挑战。

Method: 引入split - flows方法，将反向映射重新解释为跨分辨率的连续时间测度传输，在不同分辨率间建立直接概率联系。

Result: 在多种分子系统（如chignolin、脂质双层和丙氨酸二肽）中展示了该方法实现原子结构表达性条件采样和计算映射熵的能力。

Conclusion: split - flows是用于精确反向映射和系统评估粗粒度模型的有原则的框架。

Abstract: By reducing resolution, coarse-grained models greatly accelerate molecular
simulations, unlocking access to long-timescale phenomena, though at the
expense of microscopic information. Recovering this fine-grained detail is
essential for tasks that depend on atomistic accuracy, making backmapping a
central challenge in molecular modeling. We introduce split-flows, a novel
flow-based approach that reinterprets backmapping as a continuous-time measure
transport across resolutions. Unlike existing generative strategies,
split-flows establish a direct probabilistic link between resolutions, enabling
expressive conditional sampling of atomistic structures and -- for the first
time -- a tractable route to computing mapping entropies, an
information-theoretic measure of the irreducible detail lost in
coarse-graining. We demonstrate these capabilities on diverse molecular
systems, including chignolin, a lipid bilayer, and alanine dipeptide,
highlighting split-flows as a principled framework for accurate backmapping and
systematic evaluation of coarse-grained models.

</details>


### [584] [Spin-Adapted Neural Network Wavefunctions in Real Space](https://arxiv.org/abs/2511.01671)
*Ruichen Li,Yuzhi Liu,Du Jiang,Yixiao Chen,Xuelan Wen,Wenrui Li,Di He,Liwei Wang,Ji Chen,Weiluo Ren*

Main category: physics.chem-ph

TL;DR: 介绍自旋适配反对称化方法（SAAM）用于神经网络量子蒙特卡罗，更准确高效，应用于铁 - 硫簇研究获良好结果。


<details>
  <summary>Details</summary>
Motivation: 许多实空间波函数方法未能充分考虑自旋，需新方法解决。

Method: 引入SAAM，在神经网络量子蒙特卡罗中利用深度神经网络表达能力，通过群表示理论实现精确自旋适配。

Result: 应用SAAM研究铁 - 硫簇，准确解析低激发自旋态和自旋能隙，获电子结构新见解。

Conclusion: SAAM是自旋适配神经网络量子蒙特卡罗的可靠、无超参数标准，尤其适用于强关联系统。

Abstract: Spin plays a fundamental role in understanding electronic structure, yet many
real-space wavefunction methods fail to adequately consider it. We introduce
the Spin-Adapted Antisymmetrization Method (SAAM), a general procedure that
enforces exact total spin symmetry for antisymmetric many-electron
wavefunctions in real space. In the context of neural network-based quantum
Monte Carlo (NNQMC), SAAM leverages the expressiveness of deep neural networks
to capture electron correlation while enforcing exact spin adaptation via group
representation theory. This framework provides a principled route to embed
physical priors into otherwise black-box neural network wavefunctions, yielding
a compact representation of correlated system with neural network orbitals.
Compared with existing treatments of spin in NNQMC, SAAM is more accurate and
efficient, achieving exact spin purity without any additional tunable
hyperparameters. To demonstrate its effectiveness, we apply SAAM to study the
spin ladder of iron-sulfur clusters, a long-standing challenge for many-body
methods due to their dense spectrum of nearly degenerate spin states. Our
results reveal accurate resolution of low-lying spin states and spin gaps in
[Fe$_2$S$_2$] and [Fe$_4$S$_4$] clusters, offering new insights into their
electronic structures. In sum, these findings establish SAAM as a robust,
hyperparameter-free standard for spin-adapted NNQMC, particularly for strongly
correlated systems.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [585] [NaturalVoices: A Large-Scale, Spontaneous and Emotional Podcast Dataset for Voice Conversion](https://arxiv.org/abs/2511.00256)
*Zongyang Du,Shreeram Suresh Chandra,Ismail Rasim Ulgen,Aurosweta Mahapatra,Ali N. Salman,Carlos Busso,Berrak Sisman*

Main category: eess.AS

TL;DR: 本文发布了用于情感感知语音转换的大规模自发播客数据集NaturalVoices，介绍其内容、工具并展示实验效果。


<details>
  <summary>Details</summary>
Motivation: 现有语音数据集存在局限，语音转换领域缺乏适合建模自然韵律和情感的大规模、富有表现力的真实语音资源。

Method: 发布NaturalVoices数据集，包含5049小时自发播客录音及多种自动标注，提供开源模块化标注工具和灵活过滤的管道。

Result: 实验表明该数据集支持开发强大且通用的语音转换模型，同时揭示当前架构处理大规模自发数据的局限性。

Conclusion: NaturalVoices是推动语音转换领域发展的宝贵资源和具有挑战性的基准。

Abstract: Everyday speech conveys far more than words, it reflects who we are, how we
feel, and the circumstances surrounding our interactions. Yet, most existing
speech datasets are acted, limited in scale, and fail to capture the expressive
richness of real-life communication. With the rise of large neural networks,
several large-scale speech corpora have emerged and been widely adopted across
various speech processing tasks. However, the field of voice conversion (VC)
still lacks large-scale, expressive, and real-life speech resources suitable
for modeling natural prosody and emotion. To fill this gap, we release
NaturalVoices (NV), the first large-scale spontaneous podcast dataset
specifically designed for emotion-aware voice conversion. It comprises 5,049
hours of spontaneous podcast recordings with automatic annotations for emotion
(categorical and attribute-based), speech quality, transcripts, speaker
identity, and sound events. The dataset captures expressive emotional variation
across thousands of speakers, diverse topics, and natural speaking styles. We
also provide an open-source pipeline with modular annotation tools and flexible
filtering, enabling researchers to construct customized subsets for a wide
range of VC tasks. Experiments demonstrate that NaturalVoices supports the
development of robust and generalizable VC models capable of producing natural,
expressive speech, while revealing limitations of current architectures when
applied to large-scale spontaneous data. These results suggest that
NaturalVoices is both a valuable resource and a challenging benchmark for
advancing the field of voice conversion. Dataset is available at:
https://huggingface.co/JHU-SmileLab

</details>


### [586] [MULTI-Bench: A Multi-Turn Interactive Benchmark for Assessing Emotional Intelligence ability of Spoken Dialogue Models](https://arxiv.org/abs/2511.00850)
*Yayue Deng,Guoqiang Hu,Haiyang Sun,Xiangyu Zhang,Haoyang Zhang,Fei Tian,Xuerui Yang,Gang Yu,Eng Siong Chng*

Main category: eess.AS

TL;DR: 提出首个多轮交互式对话基准Multi - Bench评估SDMs，评估显示当前SDMs在高级任务有提升空间。


<details>
  <summary>Details</summary>
Motivation: 现有SDMs在多轮交互式对话能力探索不足，多数基准关注单轮交流，需评估其在多轮交互式对话的能力。

Method: 引入Multi - Bench，采用分层结构，含基础和高级赛道，包含五个任务和约3.2K样本，用可复现评估框架，对六个代表性SDMs在八个子集上评估。

Result: 当前SDMs在基础理解任务表现良好，但在高级多轮交互式对话和推理相关任务，尤其在情感感知和应用方面有提升空间。

Conclusion: Multi - Bench可用于评估SDMs在多轮交互式对话能力，当前SDMs在高级任务待提升。

Abstract: Spoken Dialogue Models (SDMs) have advanced rapidly, yet their ability to
sustain genuinely interactive multi-turn conversations remains underexplored,
as most benchmarks focus on single-turn exchanges. We introduce Multi-Bench,
the first benchmark explicitly designed to evaluate SDMs in multi-turn
interactive dialogue with an emphasis on emotional intelligence. Multi-Bench
employs a hierarchical structure with a basic track for emotion understanding
and reasoning and an advanced track for emotion support and application. It
comprises five carefully designed tasks and about 3.2K samples, ranging from
emotion recognition to complex reasoning and interactive dialogue, supported by
a reproducible evaluation framework. We evaluate six representative SDMs on
eight subsets of Multi-Bench. Results show that while current SDMs achieve good
performance on basic understanding tasks, they still have room for improvement
in advanced multi-turn interactive dialogue and reasoning-related tasks,
particularly in emotion awareness and application.

</details>


<div id='q-fin.MF'></div>

# q-fin.MF [[Back]](#toc)

### [587] [Robust Hedging of path-dependent options using a min-max algorithm](https://arxiv.org/abs/2511.00781)
*Purba Banerjee,Srikanth Iyer,Shashi Jain*

Main category: q-fin.MF

TL;DR: 提出无模型方法构建静态对冲投资组合，解决min - max问题，给出数值方案并展示结果和理论误差界。


<details>
  <summary>Details</summary>
Motivation: 投资者想用现金、标的资产和到期日为t1（0 < t1 < T）的香草期权静态对冲到期日为T的路径依赖期权，需要合适的构建方法。

Method: 受primal - dual Martingale Optimal Transport (MOT)问题启发，构建最小化t1时期望最坏对冲误差的优化问题，形成min - max问题，有期权价格时给出数值求解方案。

Result: 给出在Black - Scholes和Merton Jump diffusion模型生成期权价格时的对冲表现数值结果，以及目标期权到期日T的对冲误差理论界。

Conclusion: 该无模型方法可用于构建静态对冲投资组合，并能分析对冲误差。

Abstract: We consider an investor who wants to hedge a path-dependent option with
maturity $T$ using a static hedging portfolio using cash, the underlying, and
vanilla put/call options on the same underlying with maturity $ t_1$, where $0
< t_1 < T$. We propose a model-free approach to construct such a portfolio. The
framework is inspired by the \textit{primal-dual} Martingale Optimal Transport
(MOT) problem, which was pioneered by \cite{beiglbock2013model}. The
optimization problem is to determine the portfolio composition that minimizes
the expected worst-case hedging error at $t_1$ (that coincides with the
maturity of the options that are used in the hedging portfolio). The worst-case
scenario corresponds to the distribution that yields the worst possible hedging
performance. This formulation leads to a \textit{min-max} problem. We provide a
numerical scheme for solving this problem when a finite number of vanilla
option prices are available. Numerical results on the hedging performance of
this model-free approach when the option prices are generated using a
\textit{Black-Scholes} and a \textit{Merton Jump diffusion} model are
presented. We also provide theoretical bounds on the hedging error at $T$, the
maturity of the target option.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [588] [A Framework Based on Graph Cellular Automata for Similarity Evaluation in Urban Spatial Networks](https://arxiv.org/abs/2511.00768)
*Peiru Wu,Maojun Zhai,Lingzhu Zhang*

Main category: cs.SI

TL;DR: 提出基于图元胞自动机的GCA - Sim相似度评估框架，在道路网络评估中表现优于现有方法，并得出关于街道网络特征的结论。


<details>
  <summary>Details</summary>
Motivation: 现有方法不适用于空间网络，难以有效区分城市空间网络，需新的相似度评估方法。

Method: 提出GCA - Sim框架，各子模型通过信息演化过程多阶段记录的值分布差异测量相似度，利用改进的可微逻辑门网络学习能引发“网络共振”的子模型，通过聚类性能评估相似度。

Result: 框架中的子模型表现优于现有方法，轮廓系数超0.9。

Conclusion: 规划主导的街道网络内部同质性低于自然生长的网络；不同领域形态类别贡献相当；度这一拓扑信号与土地价值等变量迭代中愈发一致。

Abstract: Measuring similarity in urban spatial networks is key to understanding cities
as complex systems. Yet most existing methods are not tailored for spatial
networks and struggle to differentiate them effectively. We propose GCA-Sim, a
similarity-evaluation framework based on graph cellular automata. Each submodel
measures similarity by the divergence between value distributions recorded at
multiple stages of an information evolution process. We find that some
propagation rules magnify differences among network signals; we call this
"network resonance." With an improved differentiable logic-gate network, we
learn several submodels that induce network resonance. We evaluate similarity
through clustering performance on fifty city-level and fifty district-level
road networks. The submodels in this framework outperform existing methods,
with Silhouette scores above 0.9. Using the best submodel, we further observe
that planning-led street networks are less internally homogeneous than
organically grown ones; morphological categories from different domains
contribute with comparable importance; and degree, as a basic topological
signal, becomes increasingly aligned with land value and related variables over
iterations.

</details>


### [589] [Influence-aware Causal Autoencoder Network for Node Importance Ranking in Complex Networks](https://arxiv.org/abs/2511.01228)
*Jiahui Gao,Kuang Zhou,Yuchen Zhu*

Main category: cs.SI

TL;DR: 本文提出ICAN框架解决节点重要性排序问题，在合成网络上训练后可用于真实网络，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有节点重要性排序方法依赖目标网络拓扑，存在隐私问题且泛化性差，需设计不依赖目标网络拓扑的模型。

Method: 提出ICAN框架，在自编码器架构中引入影响感知因果表示学习模块提取与节点重要性因果相关的嵌入，引入因果排序损失并设计统一优化框架。

Result: 在多个基准数据集上的实验表明，ICAN在排序准确性和泛化能力方面始终优于现有方法。

Conclusion: ICAN框架可有效解决节点重要性排序问题，在合成网络上训练后能有效泛化到不同的真实世界图中。

Abstract: Node importance ranking is a fundamental problem in graph data analysis.
Existing approaches typically rely on node features derived from either
traditional centrality measures or advanced graph representation learning
methods, which depend directly on the target network's topology. However, this
reliance on structural information raises privacy concerns and often leads to
poor generalization across different networks. In this work, we address a key
question: Can we design a node importance ranking model trained exclusively on
synthetic networks that is effectively appliable to real-world networks,
eliminating the need to rely on the topology of target networks and improving
both practicality and generalizability? We answer this question affirmatively
by proposing the Influence-aware Causal Autoencoder Network (ICAN), a novel
framework that leverages causal representation learning to get robust,
invariant node embeddings for cross-network ranking tasks. Firstly, ICAN
introduces an influence-aware causal representation learning module within an
autoencoder architecture to extract node embeddings that are causally related
to node importance. Moreover, we introduce a causal ranking loss and design a
unified optimization framework that jointly optimizes the reconstruction and
ranking objectives, enabling mutual reinforcement between node representation
learning and ranking optimization. This design allows ICAN, trained on
synthetic networks, to generalize effectively across diverse real-world graphs.
Extensive experiments on multiple benchmark datasets demonstrate that ICAN
consistently outperforms state-of-the-art baselines in terms of both ranking
accuracy and generalization capability.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [590] [LongCat-Flash-Omni Technical Report](https://arxiv.org/abs/2511.00279)
*Meituan LongCat Team,Bairui Wang,Bayan,Bin Xiao,Bo Zhang,Bolin Rong,Borun Chen,Chang Wan,Chao Zhang,Chen Huang,Chen Chen,Chen Chen,Chengxu Yang,Chengzuo Yang,Cong Han,Dandan Peng,Delian Ruan,Detai Xin,Disong Wang,Dongchao Yang,Fanfan Liu,Fengjiao Chen,Fengyu Yang,Gan Dong,Gang Huang,Gang Xu,Guanglu Wan,Guoqiang Tan,Guoqiao Yu,Haibo Qiu,Hao Lu,Hongbo Liu,Hongyu Xiang,Jiaheng Wu,Jian Yang,Jiaxing Liu,Jing Huang,Jingang Wang,Jinrui Ding,Juchao Jiang,Jun Kuang,Jun Wang,Junhui Mei,Ke Ding,Kefeng Zhang,Lei Chen,Liang Shi,Limeng Qiao,Liming Zheng,Lin Ma,Liuyang Guo,Liya Ma,Luying Sun,Man Gao,Mengshen Zhu,Miao Cao,Minliang Lin,Nuo Xu,Peng Shi,Qi Zhang,Qian Fang,Qian Wang,Qian Yang,Quanxiu Wang,Rongxiang Weng,Rongxin Guo,Ruoxuan Liang,Senbin Yang,Shanbo Xu,Shanglin Lei,Shengze Ye,Shimin Chen,Shuaiqi Chen,Shujie Hu,Shuo Li,Siqi Yang,Siyu Xu,Siyu Ren,Song Li,Songxiang Liu,Tianhao Bai,Tianye Dai,Wei Hong,Wei Wang,Weixiao Zhao,Wengang Cao,Wenlong Zhu,Wenlong He,Xi Su,Xi Nan,Xiaohan Zhao,Xiaohao Wang,Xiaoyu Zhao,Xiaoyu Wang,Xiaoyu Li,Xin Pan,Xin Chen,Xiusong Sun,Xu Xiang,Xudong Xing,Xuezhi Cao,Xunliang Cai,Yang Yang,Yanli Tan,Yao Yao,Yerui Sun,Yi Chen,Yifan Lu,Yin Gong,Yining Zhang,Yitian Chen,Yiyang Gan,Yuchen Tang,Yuchen Xie,Yueqian Wang,Yuewen Zheng,Yufei Zhang,Yufeng Zhong,Yulei Qian,Yuqi Peng,Yuwei Jiang,Zeyang Hu,Zheng Zhang,Zhengkun Tian,Zhiqing Hong,Zhixiong Zeng,Zhuqi Mi,Ziran Li,Ziwen Wang,Ziyi Zhao,Ziyuan Zhuang,Zizhe Zhao*

Main category: cs.MM

TL;DR: 介绍5600亿参数的开源全模态模型LongCat - Flash - Omni，采用渐进式训练策略，实现低延迟实时视听交互，评估表现优异并开源。


<details>
  <summary>Details</summary>
Motivation: 开发一个具有实时视听交互能力、综合多模态能力且在多模态基准测试中表现出色的全模态模型。

Method: 采用课程启发的渐进式训练策略；基于LongCat - Flash架构，集成多模态感知和语音重建模块；开发模态解耦并行方案用于训练。

Result: 在全模态基准测试中达到开源模型的最优性能，在多种特定模态任务中结果极具竞争力，训练效率维持在仅文本训练吞吐量的90%以上。

Conclusion: LongCat - Flash - Omni模型表现优异，开源以促进社区未来研究和发展。

Abstract: We introduce LongCat-Flash-Omni, a state-of-the-art open-source omni-modal
model with 560 billion parameters, excelling at real-time audio-visual
interaction. By adopting a curriculum-inspired progressive training strategy
that transitions from simpler to increasingly complex modality sequence
modeling tasks, LongCat-Flash-Omni attains comprehensive multimodal
capabilities while maintaining strong unimodal capability. Building upon
LongCat-Flash, which adopts a high-performance Shortcut-connected
Mixture-of-Experts (MoE) architecture with zero-computation experts,
LongCat-Flash-Omni integrates efficient multimodal perception and speech
reconstruction modules. Despite its immense size of 560B parameters (with 27B
activated), LongCat-Flash-Omni achieves low-latency real-time audio-visual
interaction. For training infrastructure, we developed a modality-decoupled
parallelism scheme specifically designed to manage the data and model
heterogeneity inherent in large-scale multimodal training. This innovative
approach demonstrates exceptional efficiency by sustaining over 90% of the
throughput achieved by text-only training. Extensive evaluations show that
LongCat-Flash-Omni achieves state-of-the-art performance on omni-modal
benchmarks among open-source models. Furthermore, it delivers highly
competitive results across a wide range of modality-specific tasks, including
text, image, and video understanding, as well as audio understanding and
generation. We provide a comprehensive overview of the model architecture
design, training procedures, and data strategies, and open-source the model to
foster future research and development in the community.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [591] [NP-membership for the boundary-boundary art-gallery problem](https://arxiv.org/abs/2511.01562)
*Jack Stade*

Main category: cs.CG

TL;DR: 证明边界 - 边界艺术画廊问题属于NP，指出该问题难以离散化。


<details>
  <summary>Details</summary>
Motivation: 明确边界 - 边界艺术画廊问题的复杂度分类，此前该问题复杂度介于NP和∃ℝ之间。

Method: 为连续约束满足问题（每个约束最多涉及2个变量）开发约束传播程序。

Result: 证明边界 - 边界艺术画廊问题属于NP，给出例子表明该问题解有时需将警卫置于无理坐标处。

Conclusion: 边界 - 边界艺术画廊问题属于NP，且该问题不太可能轻易离散化。

Abstract: The boundary-boundary art-gallery problem asks, given a polygon $P$
representing an art-gallery, for a minimal set of guards that can see the
entire boundary of $P$ (the wall of the art gallery), where the guards must be
placed on the boundary. We show that this art-gallery variant is in NP. In
order to prove this, we develop a constraint-propagation procedure for
continuous constraint satisfaction problems where each constraint involves at
most 2 variables.
  The X-Y variant of the art-gallery problem is the one where the guards must
lie in X and need to see all of Y. Each of X and Y can be either the vertices
of the polygon, the boundary of the polygon, or the entire polygon, giving 9
different variants. Previously, it was known that X-vertex and vertex-Y
variants are all NP-complete and that the point-point, point-boundary, and
boundary-point variants are $\exists \mathbb{R}$-complete [Abrahamsen,
Adamaszek, and Miltzow, JACM 2021][Stade, SoCG 2025]. However, the
boundary-boundary variant was only known to lie somewhere between NP and
$\exists \mathbb{R}$.
  The X-vertex and vertex-Y variants can be straightforwardly reduced to
discrete set-cover instances. In contrast, we give example to show that a
solution to an instance of the boundary-boundary art-gallery problem sometimes
requires placing guards at irrational coordinates, so it unlikely that the
problem can be easily discretized.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [592] [On Improvisation and Open-Endedness: Insights for Experiential AI](https://arxiv.org/abs/2511.00529)
*Botao 'Amber' Hu*

Main category: cs.HC

TL;DR: 本文探讨AI即兴创作问题，通过访谈专家建立人类即兴艺术与未来体验式AI代理设计的联系。


<details>
  <summary>Details</summary>
Motivation: 生成式AI兴起，引发计算创造力研究中关于AI“好”的即兴创作及能否真正开放式即兴的问题。

Method: 对舞蹈、音乐和接触即兴等领域的6位专家进行深度访谈。

Result: 建立了人类即兴艺术与未来体验式AI代理设计的系统性联系，明确了AI应具备的即兴特质。

Conclusion: 可依据人类即兴艺术实践设计出能单独或与人类、其他AI一起即兴创作的体验式AI代理。

Abstract: Improvisation-the art of spontaneous creation that unfolds moment-to-moment
without a scripted outcome-requires practitioners to continuously sense, adapt,
and create anew. It is a fundamental mode of human creativity spanning music,
dance, and everyday life. The open-ended nature of improvisation produces a
stream of novel, unrepeatable moments-an aspect highly valued in artistic
creativity. In parallel, open-endedness (OE)-a system's capacity for unbounded
novelty and endless "interestingness"-is exemplified in natural or cultural
evolution and has been considered "the last grand challenge" in artificial life
(ALife). The rise of generative AI now raises the question in computational
creativity (CC) research: What makes a "good" improvisation for AI? Can AI
learn to improvise in a genuinely open-ended way? In this work-in-progress
paper, we report insights from in-depth interviews with 6 experts in
improvisation across dance, music, and contact improvisation. We draw systemic
connections between human improvisational arts and the design of future
experiential AI agents that could improvise alone or alongside humans-or even
with other AI agents-embodying qualities of improvisation drawn from practice:
active listening (umwelt and awareness), being in the time (mindfulness and
ephemerality), embracing the unknown (source of randomness and serendipity),
non-judgmental flow (acceptance and dynamical stability, balancing structure
and surprise (unpredictable criticality at edge of chaos), imaginative metaphor
(synaesthesia and planning), empathy, trust, boundary, and care (mutual theory
of mind), and playfulness and intrinsic motivation (maintaining
interestingness).

</details>


### [593] [Reducing students' misconceptions about video game development. A mixed-method study](https://arxiv.org/abs/2511.00407)
*Łukasz Sikorski,Jacek Matulewski*

Main category: cs.HC

TL;DR: 研究评估行业专家讲座对改变学生游戏开发错误观念的效果，发现能减少错误观念、提升从业动力，建议在教育中纳入类似干预。


<details>
  <summary>Details</summary>
Motivation: 研究学生对游戏开发的错误观念，评估讲座纠正错误观念的效果。

Method: 采用混合研究方法，结合定性分析与自制定量工具，研究91名学生和94名专业人士。

Result: 讲座显著减少学生错误观念，提升从业动力，培养更现实理智的思维。

Conclusion: 在游戏开发教育早期纳入专家干预可改善学习成果、助力职业选择、减少职业失望。

Abstract: This study examines students' na\"ive mindset (misconceptions) about video
game development, idealized and inaccurate beliefs that shape an unrealistic
understanding of the field. The research evaluated the effectiveness of a
fifteen-hour-long lecture series delivered by industry professionals, designed
to challenge this mindset and expose students to the complexities and realities
of game production. A mixed-methods approach was employed, combining
qualitative analysis with a prototype quantitative tool developed to measure
levels of misconception. Participants included students (n = 91) from diverse
academic backgrounds interested in game creation and professionals (n = 94)
working in the video game industry. Findings show that the intervention
significantly reduced students' na\"ive beliefs while enhancing their
motivation to pursue careers in the industry. Exposure to professional
perspectives fostered a more realistic and informed mindset, taking into
account the understanding of the technical, collaborative, and business aspects
of game development. The results suggest that incorporating similar expert-led
interventions early in game development education can improve learning
outcomes, support informed career choices, and mitigate future professional
disappointment.

</details>


### [594] [Neural Transparency: Mechanistic Interpretability Interfaces for Anticipating Model Behaviors for Personalized AI](https://arxiv.org/abs/2511.00230)
*Sheer Karny,Anthony Baez,Pat Pataranutaporn*

Main category: cs.HC

TL;DR: 介绍让大语言模型聊天机器人设计更透明的界面，评估显示能增加用户信任。


<details>
  <summary>Details</summary>
Motivation: 用户难预估聊天机器人设计选择的部署行为，存在不透明性，引发效用降低和安全问题。

Method: 提取行为特征向量，通过投影预测聊天行为，用交互式旭日图可视化结果，并进行用户研究对比。

Result: 用户对AI行为校准有误，新界面未改变设计迭代模式，但增加用户信任且受好评。

Conclusion: 为非技术用户将机械可解释性应用于人机交互奠定基础，利于更安全、契合的人机交互。

Abstract: Millions of users now design personalized LLM-based chatbots that shape their
daily interactions, yet they can only loosely anticipate how their design
choices will manifest as behaviors in deployment. This opacity is
consequential: seemingly innocuous prompts can trigger excessive sycophancy,
toxicity, or inconsistency, degrading utility and raising safety concerns. To
address this issue, we introduce an interface that enables neural transparency
by exposing language model internals during chatbot design. Our approach
extracts behavioral trait vectors (empathy, toxicity, sycophancy, etc.) by
computing differences in neural activations between contrastive system prompts
that elicit opposing behaviors. We predict chatbot behaviors by projecting the
system prompt's final token activations onto these trait vectors, normalizing
for cross-trait comparability, and visualizing results via an interactive
sunburst diagram. To evaluate this approach, we conducted an online user study
using Prolific to compare our neural transparency interface against a baseline
chatbot interface without any form of transparency. Our analyses suggest that
users systematically miscalibrated AI behavior: participants misjudged trait
activations for eleven of fifteen analyzable traits, motivating the need for
transparency tools in everyday human-AI interaction. While our interface did
not change design iteration patterns, it significantly increased user trust and
was enthusiastically received. Qualitative analysis indicated that users' had
nuanced experiences with the visualization that may enrich future work
designing neurally transparent interfaces. This work offers a path for how
mechanistic interpretability can be operationalized for non-technical users,
establishing a foundation for safer, more aligned human-AI interactions.

</details>


### [595] [A Voice-Enabled Virtual Patient System for Interactive Training in Standardized Clinical Assessment](https://arxiv.org/abs/2511.00709)
*Veronica Bossio Botero,Vijay Yadav,Jacob Ouyang,Anzar Abbas,Michelle Worthington*

Main category: cs.HC

TL;DR: 本文介绍基于大语言模型的语音虚拟患者模拟系统，经评估该系统可用于培训临床医生。


<details>
  <summary>Details</summary>
Motivation: 缺乏可扩展、真实的实践机会影响临床试验数据质量，需解决培训临床医生进行标准化临床评估的难题。

Method: 用大语言模型实现系统模拟指定症状、人口统计和沟通风格的患者，由5位经验丰富的临床评估者对4种虚拟患者角色进行20次模拟结构化MADRS访谈。

Result: 虚拟患者高度符合临床特征，评分差异小，评分者间信度高，专家对其定性真实性和连贯性评价良好。

Conclusion: 大语言模型驱动的虚拟患者模拟是培训临床医生可行且可扩展的工具，能产生高保真、临床相关的实践场景。

Abstract: Training mental health clinicians to conduct standardized clinical
assessments is challenging due to a lack of scalable, realistic practice
opportunities, which can impact data quality in clinical trials. To address
this gap, we introduce a voice-enabled virtual patient simulation system
powered by a large language model (LLM). This study describes the system's
development and validates its ability to generate virtual patients who
accurately adhere to pre-defined clinical profiles, maintain coherent
narratives, and produce realistic dialogue. We implemented a system using a LLM
to simulate patients with specified symptom profiles, demographics, and
communication styles. The system was evaluated by 5 experienced clinical raters
who conducted 20 simulated structured MADRS interviews across 4 virtual patient
personas. The virtual patients demonstrated strong adherence to their clinical
profiles, with a mean item difference between rater-assigned MADRS scores and
configured scores of 0.52 (SD=0.75). Inter-rater reliability across items was
0.90 (95% CI=0.68-0.99). Expert raters consistently rated the qualitative
realism and cohesiveness of the virtual patients favorably, giving average
ratings between "Agree" and "Strongly Agree." Our findings suggest that
LLM-powered virtual patient simulations are a viable and scalable tool for
training clinicians, capable of producing high-fidelity, clinically relevant
practice scenarios.

</details>


### [596] [Quantifying truth and authenticity in AI-assisted candidate evaluation: A multi-domain pilot analysis](https://arxiv.org/abs/2511.00774)
*Eldred Lee,Nicholas Worley,Koshu Takatsuji*

Main category: cs.HC

TL;DR: 对AlteraSF平台招聘数据回顾分析，该平台评估简历、生成验证问题等，减少筛选时间、检测异常语言模式，多维验证框架可提升招聘效率和信任。


<details>
  <summary>Details</summary>
Motivation: 评估AI原生简历验证平台AlteraSF在招聘中的效果，探索评估候选人真实性的方法。

Method: 对AlteraSF平台在试点招聘活动中收集的匿名候选人评估数据进行回顾性分析。

Result: 平台使筛选时间减少90 - 95%，检测到与AI辅助或抄袭回复一致的语言模式。

Conclusion: 可通过事实准确性和语言真实性模式评估候选人真实性，多维验证框架能提升招聘效率和对AI评估系统的信任。

Abstract: This paper presents a retrospective analysis of anonymized
candidate-evaluation data collected during pilot hiring campaigns conducted
through AlteraSF, an AI-native resume-verification platform. The system
evaluates resume claims, generates context-sensitive verification questions,
and measures performance along quantitative axes of factual validity and job
fit, complemented by qualitative integrity detection. Across six job families
and 1,700 applications, the platform achieved a 90-95% reduction in screening
time and detected measurable linguistic patterns consistent with AI-assisted or
copied responses. The analysis demonstrates that candidate truthfulness can be
assessed not only through factual accuracy but also through patterns of
linguistic authenticity. The results suggest that a multi-dimensional
verification framework can improve both hiring efficiency and trust in
AI-mediated evaluation systems.

</details>


### [597] [Beyond Permissions: Investigating Mobile Personalization with Simulated Personas](https://arxiv.org/abs/2511.01336)
*Ibrahim Khalilov,Chaoran Chen,Ziang Xiao,Tianshi Li,Toby Jia-Jun Li,Yaxing Yao*

Main category: cs.HC

TL;DR: 本文提出沙盒系统，用传感器欺骗和角色模拟审计并可视化移动应用对推断行为的响应，有初步成果并提供工具包。


<details>
  <summary>Details</summary>
Motivation: 移动应用依赖传感器数据实现个性化，但个性化机制对用户和研究人员不透明，需要提高透明度。

Method: 构建沙盒系统，将基于生活方式角色生成的多传感器配置文件实时注入安卓设备，结合自动截图和GPT - 4 Vision进行UI总结。

Result: 初步发现健身、电商、日常服务（如天气和导航）类应用有可测量的适应变化。

Conclusion: 提供的工具包可作为增强隐私技术和面向用户的透明度干预的基础。

Abstract: Mobile applications increasingly rely on sensor data to infer user context
and deliver personalized experiences. Yet the mechanisms behind this
personalization remain opaque to users and researchers alike. This paper
presents a sandbox system that uses sensor spoofing and persona simulation to
audit and visualize how mobile apps respond to inferred behaviors. Rather than
treating spoofing as adversarial, we demonstrate its use as a tool for
behavioral transparency and user empowerment. Our system injects multi-sensor
profiles - generated from structured, lifestyle-based personas - into Android
devices in real time, enabling users to observe app responses to contexts such
as high activity, location shifts, or time-of-day changes. With automated
screenshot capture and GPT-4 Vision-based UI summarization, our pipeline helps
document subtle personalization cues. Preliminary findings show measurable app
adaptations across fitness, e-commerce, and everyday service apps such as
weather and navigation. We offer this toolkit as a foundation for
privacy-enhancing technologies and user-facing transparency interventions.

</details>


### [598] [Student Engagement in AI Assisted Complex Problem Solving: A Pilot Study of Human AI Rubik's Cube Collaboration](https://arxiv.org/abs/2511.01683)
*Kirk Vanacore,Jaclyn Ocumpaugh,Forest Agostinelli,Dezhi Wu,Sai Vuruma,Matt Irvin*

Main category: cs.HC

TL;DR: 本文介绍ALLURE系统，用AI算法指导学生解魔方白色十字，通过试点研究展示学生行为与STEM技能关联，探讨其数据用于未来教育数据挖掘的可能。


<details>
  <summary>Details</summary>
Motivation: 游戏和谜题在STEM学习有重要教学作用，新AI算法为解谜的支架式教学提供机会。

Method: 提出ALLURE系统，用DeepCubeA算法指导学生解魔方白色十字，开展试点研究。

Result: 展示了学生在系统中的行为及这些行为与STEM技能的关联。

Conclusion: ALLURE系统的数据可用于未来教育数据挖掘，以了解学生在解决复杂问题时如何从AI辅助和协作中受益。

Abstract: Games and puzzles play important pedagogical roles in STEM learning. New AI
algorithms that can solve complex problems offer opportunities for scaffolded
instruction in puzzle solving. This paper presents the ALLURE system, which
uses an AI algorithm (DeepCubeA) to guide students in solving a common first
step of the Rubik's Cube (i.e., the white cross). Using data from a pilot study
we present preliminary findings about students' behaviors in the system, how
these behaviors are associated with STEM skills - including spatial reasoning,
critical thinking and algorithmic thinking. We discuss how data from ALLURE can
be used in future educational data mining to understand how students benefit
from AI assistance and collaboration when solving complex problems.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [599] [Hi-WaveTST: A Hybrid High-Frequency Wavelet-Transformer for Time-Series Classification](https://arxiv.org/abs/2511.01254)
*Huseyin Goksu*

Main category: eess.SP

TL;DR: 提出Hi - WaveTST混合架构，结合高频小波特征流，在UCI - HAR数据集上表现优于PatchTST基线。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列分类的Transformer模型对关键的非明显高频信息不敏感，而该信息与时间动态互补。

Method: 提出Hi - WaveTST混合架构，对每个时间序列块进行深度小波包分解（WPD），用可学习的广义均值（GeM）池化层提取特征。

Result: 在UCI - HAR基准数据集上，Hi - WaveTST模型平均准确率达93.38%±0.0043，显著优于PatchTST基线的92.59%±0.0039。

Conclusion: Hi - WaveTST的混合架构、深度高频小波分解和可学习GeM池化对实现SOTA性能至关重要。

Abstract: Transformers have become state-of-the-art (SOTA) for time-series
classification, with models like PatchTST demonstrating exceptional
performance. These models rely on patching the time series and learning
relationships between raw temporal data blocks. We argue that this approach is
blind to critical, non-obvious high-frequency information that is complementary
to the temporal dynamics. In this letter, we propose Hi-WaveTST, a novel Hybrid
architecture that augments the original temporal patch with a learnable,
High-Frequency wavelet feature stream. Our wavelet stream uses a deep Wavelet
Packet Decomposition (WPD) on each patch and extracts features using a
learnable Generalized Mean (GeM) pooling layer. On the UCI-HAR benchmark
dataset, our hybrid model achieves a mean accuracy of 93.38 percent plus-minus
0.0043, significantly outperforming the SOTA PatchTST baseline (92.59 percent
plus-minus 0.0039). A comprehensive ablation study proves that every component
of our design-the hybrid architecture, the deep high-frequency wavelet
decomposition, and the learnable GeM pooling-is essential for this
state-of-the-art performance.

</details>


### [600] [A Multimodal Dataset for Indoor Radio Mapping with 3D Point Clouds and RSSI](https://arxiv.org/abs/2511.00494)
*Ljupcho Milosheski,Kuon Akiyama,Blaž Bertalanič,Jernej Hribar,Ryoichi Shinkuma*

Main category: eess.SP

TL;DR: 文章介绍了一个多模态数据集，集成3D LiDAR扫描与Wi - Fi RSSI测量，支持研究动态环境对无线信号传播的影响，推动室内通信系统发展。


<details>
  <summary>Details</summary>
Motivation: 室内大量智能设备需可靠无线连接，准确估计无线电环境图（REMs）有挑战，因此需新方法解决。

Method: 引入一个多模态数据集，集成高分辨率3D LiDAR扫描和Wi - Fi RSSI测量，包含无人和有人两种场景。

Result: 得到一个支持研究动态环境对无线信号传播影响的数据集。

Conclusion: 该数据集有助于数据驱动的无线建模研究，推动新兴高频标准下室内通信系统发展。

Abstract: The growing number of smart devices supporting bandwidth-intensive and
latency-sensitive applications, such as real-time video analytics, smart
sensing, and Extended Reality (XR), necessitates reliable wireless connectivity
in indoor environments. Therein, accurate estimation of Radio Environment Maps
(REMs) enables adaptive wireless network planning and optimization of Access
Point (AP) placement. However, generating realistic REMs remains challenging
due to the complexity of indoor spaces. To overcome this challenge, this paper
introduces a multimodal dataset that integrates high-resolution 3D LiDAR scans
with Wi-Fi Received Signal Strength Indicator (RSSI) measurements collected
under 20 distinct AP configurations in a multi-room indoor environment. The
dataset captures two measurement scenarios: the first without human presence in
the environment, and the second with human presence. Thus, the presented
dataset supports the study of dynamic environmental effects on wireless signal
propagation. This resource is designed to facilitate research in data-driven
wireless modeling, particularly in the context of emerging high-frequency
standards such as IEEE 802.11be (Wi-Fi 7), and aims to advance the development
of robust, high-capacity indoor communication systems.

</details>


### [601] [Seed-Induced Uniqueness in Transformer Models: Subspace Alignment Governs Subliminal Transfer](https://arxiv.org/abs/2511.01023)
*Ayşe Selin Okatan,Mustafa İlhan Akbaş,Laxima Niure Kandel,Berker Peköz*

Main category: eess.SP

TL;DR: 分析Transformer模型中的潜意识转移，发现转移强度取决于特征判别子空间的对齐，提出子空间诊断方法及安全控制措施。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer模型中潜意识转移现象，探究转移能力的影响因素及如何保障多模型部署安全。

Method: 使用合成语料，在匹配和独立随机初始化下蒸馏学生模型，采用子空间级CKA诊断和残差探针。

Result: 相同种子学生模型转移强度高，不同种子学生模型转移强度低；安全控制措施可减少泄漏且不影响公共任务性能。

Conclusion: 种子诱导的唯一性是一种弹性属性，应采用子空间感知诊断进行安全的多模型部署。

Abstract: We analyze subliminal transfer in Transformer models, where a teacher embeds
hidden traits that can be linearly decoded by a student without degrading
main-task performance. Prior work often attributes transferability to global
representational similarity, typically quantified with Centered Kernel
Alignment (CKA). Using synthetic corpora with disentangled public and private
labels, we distill students under matched and independent random
initializations. We find that transfer strength hinges on alignment within a
trait-discriminative subspace: same-seed students inherit this alignment and
show higher leakage {\tau \approx} 0.24, whereas different-seed
students--despite global CKA > 0.9--exhibit substantially reduced excess
accuracy {\tau \approx} 0.12 - 0.13. We formalize this with subspace-level CKA
diagnostic and residualized probes, showing that leakage tracks alignment
within the trait-discriminative subspace rather than global representational
similarity. Security controls (projection penalty, adversarial reversal,
right-for-the-wrong-reasons regularization) reduce leakage in same-base models
without impairing public-task fidelity. These results establish seed-induced
uniqueness as a resilience property and argue for subspace-aware diagnostics
for secure multi-model deployments.

</details>


### [602] [Towards Channel Charting Enhancement with Non-Reconfigurable Intelligent Surfaces](https://arxiv.org/abs/2511.00919)
*Mahdi Maleki,Reza Agahzadeh Ayoubi,Marouan Mizmizi,Umberto Spagnolini*

Main category: eess.SP

TL;DR: 研究全被动电磁皮（EMS）在密集城市环境中增强信道测绘（CC）的方法，设计静态EMS相位剖面，在3D射线追踪城市测试中取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用全被动电磁皮增强密集城市环境中的信道测绘。

Method: 采用半监督t - 分布随机邻域嵌入（t - SNE）和半监督自动编码器（AE）两种CC技术验证结果一致性，通过分位数驱动准则设计静态EMS相位剖面。

Result: 在30GHz的3D射线追踪城市中，提出的EMS将90%分位数定位误差从>50m降至<25m，在15%监督下将严重轨迹丢失减少超4倍。

Conclusion: 静态、预配置的EMS是无需重新配置开销的CC实用促成因素。

Abstract: We investigate how fully-passive electromagnetic skins (EMSs) can be
engineered to enhance channel charting (CC) in dense urban environments. We
employ two complementary state-of-the-art CC techniques, semi-supervised
t-distributed stochastic neighbor embedding (t-SNE) and a semi-supervised
Autoencoder (AE), to verify the consistency of results across nonparametric and
parametric mappings. We show that the accuracy of CC hinges on a balance
between signal-to-noise ratio (SNR) and spatial dissimilarity: EMS codebooks
that only maximize gain, as in conventional Reconfigurable Intelligent Surface
(RIS) optimization, suppress location fingerprints and degrade CC, while
randomized phases increase diversity but reduce SNR. To address this trade-off,
we design static EMS phase profiles via a quantile-driven criterion that
targets worst-case users and improves both trustworthiness and continuity. In a
3D ray-traced city at 30 GHz, the proposed EMS reduces the 90th-percentile
localization error from > 50 m to < 25 m for both t-SNE and AE-based CC, and
decreases severe trajectory dropouts by over 4x under 15% supervision. The
improvements hold consistently across the evaluated configurations,
establishing static, pre-configured EMS as a practical enabler of CC without
reconfiguration overheads.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [603] [Correspondence Between Ising Machines and Neural Networks](https://arxiv.org/abs/2511.00746)
*Andrew G. Moore*

Main category: cond-mat.dis-nn

TL;DR: 本文将Ising模型的基态计算推广到自旋平均计算，引入Ising设备与神经网络的对应关系及在Ising硬件上运行前馈神经网络的方法，并给出成功实现的数学证明。


<details>
  <summary>Details</summary>
Motivation: 传统上Ising模型计算值等同于基态，为使计算能在高温下进行，推广计算方式，并实现神经网络在Ising硬件上的运行。

Method: 将基态计算推广到自旋平均计算，引入Ising设备与神经网络的对应关系，提出在Ising型硬件上运行训练好的前馈神经网络的简单方法。

Result: 实现了在高温下的计算，并给出在Ising型硬件上运行神经网络的方法，且有数学证明其实现总是成功的。

Conclusion: 将Ising模型计算推广到自旋平均计算可行，且能有效在Ising硬件上运行神经网络。

Abstract: Computation with the Ising model is central to future computing technologies
like quantum annealing, adiabatic quantum computing, and thermodynamic
classical computing. Traditionally, computed values have been equated with
ground states. This paper generalizes computation with ground states to
computation with spin averages, allowing computations to take place at high
temperatures. It then introduces a systematic correspondence between Ising
devices and neural networks and a simple method to run trained feed-forward
neural networks on Ising-type hardware. Finally, a mathematical proof is
offered that these implementations are always successful.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [604] [STRIDER: Navigation via Instruction-Aligned Structural Decision Space Optimization](https://arxiv.org/abs/2511.00033)
*Diqi He,Xuehao Gao,Hao Li,Junwei Han,Dingwen Zhang*

Main category: cs.RO

TL;DR: 提出STRIDER框架用于零样本视觉语言导航任务，在基准测试中显著优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法因缺乏结构化决策和对先前动作反馈整合不足，难以实现稳健导航，需解决此问题。

Method: 提出STRIDER框架，包括通过空间结构约束动作空间的结构化路点生成器和根据任务进度调整行为的任务对齐调节器。

Result: 在R2R - CE和RxR - CE基准测试中显著优于强SOTA，成功率从29%提升到35%，相对增益20.7%。

Conclusion: 空间约束决策和反馈引导执行对提高零样本VLN - CE导航保真度很重要。

Abstract: The Zero-shot Vision-and-Language Navigation in Continuous Environments
(VLN-CE) task requires agents to navigate previously unseen 3D environments
using natural language instructions, without any scene-specific training. A
critical challenge in this setting lies in ensuring agents' actions align with
both spatial structure and task intent over long-horizon execution. Existing
methods often fail to achieve robust navigation due to a lack of structured
decision-making and insufficient integration of feedback from previous actions.
To address these challenges, we propose STRIDER (Instruction-Aligned Structural
Decision Space Optimization), a novel framework that systematically optimizes
the agent's decision space by integrating spatial layout priors and dynamic
task feedback. Our approach introduces two key innovations: 1) a Structured
Waypoint Generator that constrains the action space through spatial structure,
and 2) a Task-Alignment Regulator that adjusts behavior based on task progress,
ensuring semantic alignment throughout navigation. Extensive experiments on the
R2R-CE and RxR-CE benchmarks demonstrate that STRIDER significantly outperforms
strong SOTA across key metrics; in particular, it improves Success Rate (SR)
from 29% to 35%, a relative gain of 20.7%. Such results highlight the
importance of spatially constrained decision-making and feedback-guided
execution in improving navigation fidelity for zero-shot VLN-CE.

</details>


### [605] [Endowing GPT-4 with a Humanoid Body: Building the Bridge Between Off-the-Shelf VLMs and the Physical World](https://arxiv.org/abs/2511.00041)
*Yingzhao Jian,Zhongan Wang,Yi Yang,Hehe Fan*

Main category: cs.RO

TL;DR: 本文提出BiBo系统，利用现成VLMs控制人形智能体，解决其在开放环境交互问题，实验效果良好且代码将开源。


<details>
  <summary>Details</summary>
Motivation: 人形智能体在开放环境处理灵活多样交互有困难，收集大量数据训练模型成本高，需替代方案。

Method: 提出BiBo系统，包含具身指令编译器和基于扩散的运动执行器。

Result: BiBo在开放环境交互任务成功率达90.2%，文本引导运动执行精度比先前方法提高16.3%。

Conclusion: 利用现成VLMs控制人形智能体是可行的，BiBo能处理多样复杂动作。

Abstract: Humanoid agents often struggle to handle flexible and diverse interactions in
open environments. A common solution is to collect massive datasets to train a
highly capable model, but this approach can be prohibitively expensive. In this
paper, we explore an alternative solution: empowering off-the-shelf
Vision-Language Models (VLMs, such as GPT-4) to control humanoid agents,
thereby leveraging their strong open-world generalization to mitigate the need
for extensive data collection. To this end, we present \textbf{BiBo}
(\textbf{B}uilding humano\textbf{I}d agent \textbf{B}y \textbf{O}ff-the-shelf
VLMs). It consists of two key components: (1) an \textbf{embodied instruction
compiler}, which enables the VLM to perceive the environment and precisely
translate high-level user instructions (e.g., {\small\itshape ``have a rest''})
into low-level primitive commands with control parameters (e.g.,
{\small\itshape ``sit casually, location: (1, 2), facing: 90$^\circ$''}); and
(2) a diffusion-based \textbf{motion executor}, which generates human-like
motions from these commands, while dynamically adapting to physical feedback
from the environment. In this way, BiBo is capable of handling not only basic
interactions but also diverse and complex motions. Experiments demonstrate that
BiBo achieves an interaction task success rate of 90.2\% in open environments,
and improves the precision of text-guided motion execution by 16.3\% over prior
methods. The code will be made publicly available.

</details>


### [606] [Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail](https://arxiv.org/abs/2511.00088)
*NVIDIA,:,Yan Wang,Wenjie Luo,Junjie Bai,Yulong Cao,Tong Che,Ke Chen,Yuxiao Chen,Jenna Diamond,Yifan Ding,Wenhao Ding,Liang Feng,Greg Heinrich,Jack Huang,Peter Karkus,Boyi Li,Pinyi Li,Tsung-Yi Lin,Dongran Liu,Ming-Yu Liu,Langechuan Liu,Zhijian Liu,Jason Lu,Yunxiang Mao,Pavlo Molchanov,Lindsey Pavao,Zhenghao Peng,Mike Ranzinger,Ed Schmerling,Shida Shen,Yunfei Shi,Sarah Tariq,Ran Tian,Tilman Wekel,Xinshuo Weng,Tianjun Xiao,Eric Yang,Xiaodong Yang,Yurong You,Xiaohui Zeng,Wenyuan Zhang,Boris Ivanovic,Marco Pavone*

Main category: cs.RO

TL;DR: 介绍视觉语言动作模型Alpamayo - R1 (AR1)，结合因果链推理与轨迹规划，评估显示其在规划准确性等多方面有提升，展示了通往4级自动驾驶的实用路径。


<details>
  <summary>Details</summary>
Motivation: 现有端到端架构在安全关键长尾场景中性能不佳，监督少且因果理解有限，需改进自动驾驶决策。

Method: 引入AR1模型，构建因果链数据集，采用模块化VLA架构，结合预训练视觉语言模型与轨迹解码器，使用多阶段训练策略。

Result: 在挑战性案例中规划准确性提升12%，越野率降低35%，近距离相遇率降低25%，推理质量提升45%，推理 - 动作一致性提升37%，模型扩展有持续改进，车载测试有实时性能和成功城市部署。

Conclusion: AR1通过连接可解释推理和精确控制，为4级自动驾驶提供实用路径，未来将发布模型和部分数据集。

Abstract: End-to-end architectures trained via imitation learning have advanced
autonomous driving by scaling model size and data, yet performance remains
brittle in safety-critical long-tail scenarios where supervision is sparse and
causal understanding is limited. To address this, we introduce Alpamayo-R1
(AR1), a vision-language-action model (VLA) that integrates Chain of Causation
reasoning with trajectory planning to enhance decision-making in complex
driving scenarios. Our approach features three key innovations: (1) the Chain
of Causation (CoC) dataset, built through a hybrid auto-labeling and
human-in-the-loop pipeline producing decision-grounded, causally linked
reasoning traces aligned with driving behaviors; (2) a modular VLA architecture
combining Cosmos-Reason, a Vision-Language Model pre-trained for Physical AI
applications, with a diffusion-based trajectory decoder that generates
dynamically feasible plans in real time; (3) a multi-stage training strategy
using supervised fine-tuning to elicit reasoning and reinforcement learning
(RL) to optimize reasoning quality via large reasoning model feedback and
enforce reasoning-action consistency. Evaluation shows AR1 achieves up to a 12%
improvement in planning accuracy on challenging cases compared to a
trajectory-only baseline, with a 35% reduction in off-road rate and 25%
reduction in close encounter rate in closed-loop simulation. RL post-training
improves reasoning quality by 45% as measured by a large reasoning model critic
and reasoning-action consistency by 37%. Model scaling from 0.5B to 7B
parameters shows consistent improvements. On-vehicle road tests confirm
real-time performance (99 ms latency) and successful urban deployment. By
bridging interpretable reasoning with precise control, AR1 demonstrates a
practical path towards Level 4 autonomous driving. We plan to release AR1
models and a subset of the CoC in a future update.

</details>


### [607] [Digital Twin based Automatic Reconfiguration of Robotic Systems in Smart Environments](https://arxiv.org/abs/2511.00094)
*Angelos Alexopoulos,Agorakis Bompotas,Nikitas Rigas Kalogeropoulos,Panagiotis Kechagias,Athanasios P. Kalogeras,Christos Alexakos*

Main category: cs.RO

TL;DR: 提出用数字孪生技术实现机器人控制器自主动态重新配置的框架，解决传统控制系统在动态环境适应性不足问题。


<details>
  <summary>Details</summary>
Motivation: 传统控制系统难以在动态环境快速适应，导致效率低或运行失败，需新方法解决。

Method: 利用数字孪生技术创建机器人运行环境虚拟副本，模拟优化运动轨迹，重新计算路径和控制参数并部署到物理机器人。

Result: 可在无需人工干预下实现机器人快速可靠适应。

Conclusion: 推进数字孪生在机器人领域的集成，为智能动态环境提升自主性提供可扩展解决方案。

Abstract: Robotic systems have become integral to smart environments, enabling
applications ranging from urban surveillance and automated agriculture to
industrial automation. However, their effective operation in dynamic settings -
such as smart cities and precision farming - is challenged by continuously
evolving topographies and environmental conditions. Traditional control systems
often struggle to adapt quickly, leading to inefficiencies or operational
failures. To address this limitation, we propose a novel framework for
autonomous and dynamic reconfiguration of robotic controllers using Digital
Twin technology. Our approach leverages a virtual replica of the robot's
operational environment to simulate and optimize movement trajectories in
response to real-world changes. By recalculating paths and control parameters
in the Digital Twin and deploying the updated code to the physical robot, our
method ensures rapid and reliable adaptation without manual intervention. This
work advances the integration of Digital Twins in robotics, offering a scalable
solution for enhancing autonomy in smart, dynamic environments.

</details>


### [608] [Real-DRL: Teach and Learn in Reality](https://arxiv.org/abs/2511.00112)
*Yanbing Mao,Yihao Cai,Lui Sha*

Main category: cs.RO

TL;DR: 本文介绍Real - DRL框架，用于安全关键的自主系统，由三个组件构成，能应对安全挑战，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 为安全关键的自主系统开发能在真实物理系统中学习安全且高性能动作策略的框架，同时优先保障安全。

Method: 提出由DRL - Student、PHY - Teacher和Trigger三个交互组件构成的Real - DRL框架，各组件有不同功能。

Result: 通过对真实四足机器人、NVIDIA Isaac Gym中的四足机器人和推车 - 杆系统的实验，以及对比和消融研究，证明Real - DRL的有效性和独特特性。

Conclusion: Real - DRL框架能有效解决未知因素和Sim2Real差距带来的安全挑战，具备安全保障、自动分层学习和安全感知批量采样等特性。

Abstract: This paper introduces the Real-DRL framework for safety-critical autonomous
systems, enabling runtime learning of a deep reinforcement learning (DRL) agent
to develop safe and high-performance action policies in real plants (i.e., real
physical systems to be controlled), while prioritizing safety! The Real-DRL
consists of three interactive components: a DRL-Student, a PHY-Teacher, and a
Trigger. The DRL-Student is a DRL agent that innovates in the dual
self-learning and teaching-to-learn paradigm and the real-time safety-informed
batch sampling. On the other hand, PHY-Teacher is a physics-model-based design
of action policies that focuses solely on safety-critical functions.
PHY-Teacher is novel in its real-time patch for two key missions: i) fostering
the teaching-to-learn paradigm for DRL-Student and ii) backing up the safety of
real plants. The Trigger manages the interaction between the DRL-Student and
the PHY-Teacher. Powered by the three interactive components, the Real-DRL can
effectively address safety challenges that arise from the unknown unknowns and
the Sim2Real gap. Additionally, Real-DRL notably features i) assured safety,
ii) automatic hierarchy learning (i.e., safety-first learning and then
high-performance learning), and iii) safety-informed batch sampling to address
the learning experience imbalance caused by corner cases. Experiments with a
real quadruped robot, a quadruped robot in NVIDIA Isaac Gym, and a cart-pole
system, along with comparisons and ablation studies, demonstrate the Real-DRL's
effectiveness and unique features.

</details>


### [609] [End-to-End Dexterous Arm-Hand VLA Policies via Shared Autonomy: VR Teleoperation Augmented by Autonomous Hand VLA Policy for Efficient Data Collection](https://arxiv.org/abs/2511.00139)
*Yu Cui,Yujian Zhang,Lina Tao,Yang Li,Xinyu Yi,Zhibin Li*

Main category: cs.RO

TL;DR: 提出共享自主框架解决通用机器人灵巧操作数据收集难题，训练端到端VLA策略，实验证明其高效且成功率高。


<details>
  <summary>Details</summary>
Motivation: 通用机器人实现类人灵巧操作面临挑战，VLA模型因高质量训练数据稀缺限制可扩展性，现有数据收集方法有局限。

Method: 提出共享自主框架，划分宏微观运动控制，人类通过VR引导手臂姿势，DexGrasp - VLA策略处理手部精细控制；训练带Arm - Hand特征增强模块的端到端VLA策略；用纠正性遥操作系统实现策略持续改进。

Result: 框架用最少人力生成高质量数据，在多样物体操作中成功率达90%，包括未见实例。

Conclusion: 该系统在培养灵巧操作能力方面有效。

Abstract: Achieving human-like dexterous manipulation remains a major challenge for
general-purpose robots. While Vision-Language-Action (VLA) models show
potential in learning skills from demonstrations, their scalability is limited
by scarce high-quality training data. Existing data collection methods face
inherent constraints: manual teleoperation overloads human operators, while
automated planning often produces unnatural motions. We propose a Shared
Autonomy framework that divides control between macro and micro motions. A
human operator guides the robot's arm pose through intuitive VR teleoperation,
while an autonomous DexGrasp-VLA policy handles fine-grained hand control using
real-time tactile and visual feedback. This division significantly reduces
cognitive load and enables efficient collection of high-quality coordinated
arm-hand demonstrations. Using this data, we train an end-to-end VLA policy
enhanced with our novel Arm-Hand Feature Enhancement module, which captures
both distinct and shared representations of macro and micro movements for more
natural coordination. Our Corrective Teleoperation system enables continuous
policy improvement through human-in-the-loop failure recovery. Experiments
demonstrate that our framework generates high-quality data with minimal
manpower and achieves a 90% success rate across diverse objects, including
unseen instances. Comprehensive evaluations validate the system's effectiveness
in developing dexterous manipulation capabilities.

</details>


### [610] [SonarSweep: Fusing Sonar and Vision for Robust 3D Reconstruction via Plane Sweeping](https://arxiv.org/abs/2511.00392)
*Lingpeng Chen,Jiakun Tang,Apple Pui-Yi Chui,Ziyang Hong,Junfeng Wu*

Main category: cs.RO

TL;DR: 提出SonarSweep框架用于水下3D重建，实验表现优，将发布代码和数据集


<details>
  <summary>Details</summary>
Motivation: 现有单模态和融合技术在水下3D重建中有局限，难以应对复杂场景

Method: 引入SonarSweep，采用平面扫描算法进行声纳和视觉数据跨模态融合

Result: 在模拟和真实环境实验中，SonarSweep能生成密集准确深度图，优于现有方法

Conclusion: SonarSweep有效克服现有局限，将公开代码和数据集推动研究

Abstract: Accurate 3D reconstruction in visually-degraded underwater environments
remains a formidable challenge. Single-modality approaches are insufficient:
vision-based methods fail due to poor visibility and geometric constraints,
while sonar is crippled by inherent elevation ambiguity and low resolution.
Consequently, prior fusion technique relies on heuristics and flawed geometric
assumptions, leading to significant artifacts and an inability to model complex
scenes. In this paper, we introduce SonarSweep, a novel, end-to-end deep
learning framework that overcomes these limitations by adapting the principled
plane sweep algorithm for cross-modal fusion between sonar and visual data.
Extensive experiments in both high-fidelity simulation and real-world
environments demonstrate that SonarSweep consistently generates dense and
accurate depth maps, significantly outperforming state-of-the-art methods
across challenging conditions, particularly in high turbidity. To foster
further research, we will publicly release our code and a novel dataset
featuring synchronized stereo-camera and sonar data, the first of its kind.

</details>


### [611] [Reducing Robotic Upper-Limb Assessment Time While Maintaining Precision: A Time Series Foundation Model Approach](https://arxiv.org/abs/2511.00193)
*Faranak Akbarifar,Nooshin Maghsoodi,Sean P Dukelow,Stephen Scott,Parvin Mousavi*

Main category: cs.RO

TL;DR: 研究评估时间序列基础模型能否用早期部分试验替代未记录试验，同时保持Kinarm参数可靠性，结果表明基础模型可大幅缩短评估时间。


<details>
  <summary>Details</summary>
Motivation: Kinarm机器人上的视觉引导伸手（VGR）需40 - 64次伸手，有时间和疲劳负担，评估时间序列基础模型替代未记录试验的可行性。

Method: 分析461名中风和599名对照参与者的VGR速度信号，用ARIMA、MOMENT和Chronos模型对部分试验进行预测，重新计算运动学特征并与全长参考对比。

Result: Chronos模型仅用8次记录试验加预测就能恢复ICC >= 0.90，MOMENT有中等提升，ARIMA改善最小，合成试验不影响特征可靠性。

Conclusion: 基础模型预测可大幅缩短Kinarm VGR评估时间，有望用于中风后运动障碍的高效机器人评估。

Abstract: Purpose: Visually Guided Reaching (VGR) on the Kinarm robot yields sensitive
kinematic biomarkers but requires 40-64 reaches, imposing time and fatigue
burdens. We evaluate whether time-series foundation models can replace
unrecorded trials from an early subset of reaches while preserving the
reliability of standard Kinarm parameters.
  Methods: We analyzed VGR speed signals from 461 stroke and 599 control
participants across 4- and 8-target reaching protocols. We withheld all but the
first 8 or 16 reaching trials and used ARIMA, MOMENT, and Chronos models,
fine-tuned on 70 percent of subjects, to forecast synthetic trials. We
recomputed four kinematic features of reaching (reaction time, movement time,
posture speed, maximum speed) on combined recorded plus forecasted trials and
compared them to full-length references using ICC(2,1).
  Results: Chronos forecasts restored ICC >= 0.90 for all parameters with only
8 recorded trials plus forecasts, matching the reliability of 24-28 recorded
reaches (Delta ICC <= 0.07). MOMENT yielded intermediate gains, while ARIMA
improvements were minimal. Across cohorts and protocols, synthetic trials
replaced reaches without materially compromising feature reliability.
  Conclusion: Foundation-model forecasting can greatly shorten Kinarm VGR
assessment time. For the most impaired stroke survivors, sessions drop from 4-5
minutes to about 1 minute while preserving kinematic precision. This
forecast-augmented paradigm promises efficient robotic evaluations for
assessing motor impairments following stroke.

</details>


### [612] [Maestro: Orchestrating Robotics Modules with Vision-Language Models for Zero-Shot Generalist Robots](https://arxiv.org/abs/2511.00917)
*Junyao Shi,Rujia Yang,Kaitian Chao,Selina Bingqing Wan,Yifei Shao,Jiahui Lei,Jianing Qian,Long Le,Pratik Chaudhari,Kostas Daniilidis,Chuan Wen,Dinesh Jayaraman*

Main category: cs.RO

TL;DR: 文章提出Maestro方法，利用VLM构建通用策略，在零样本性能上超越现有VLA模型，且具有易扩展、易编辑和易适应等优点。


<details>
  <summary>Details</summary>
Motivation: 当前通用机器人探索路线多是收集大数据集训练端到端模型，作者选择另一条路，围绕VLM构建通用策略。

Method: 构建Maestro，让VLM编码代理将感知、规划和控制模块动态组合成当前任务和场景的程序化策略。

Result: Maestro在具有挑战性的操作技能零样本性能上远超现有VLA模型。

Conclusion: Maestro架构具有优势，且易扩展、编辑和适应新情况。

Abstract: Today's best-explored routes towards generalist robots center on collecting
ever larger "observations-in actions-out" robotics datasets to train large
end-to-end models, copying a recipe that has worked for vision-language models
(VLMs). We pursue a road less traveled: building generalist policies directly
around VLMs by augmenting their general capabilities with specific robot
capabilities encapsulated in a carefully curated set of perception, planning,
and control modules. In Maestro, a VLM coding agent dynamically composes these
modules into a programmatic policy for the current task and scenario. Maestro's
architecture benefits from a streamlined closed-loop interface without many
manually imposed structural constraints, and a comprehensive and diverse tool
repertoire. As a result, it largely surpasses today's VLA models for zero-shot
performance on challenging manipulation skills. Further, Maestro is easily
extensible to incorporate new modules, easily editable to suit new embodiments
such as a quadruped-mounted arm, and even easily adapts from minimal real-world
experiences through local code edits.

</details>


### [613] [URDF-Anything: Constructing Articulated Objects with 3D Multimodal Language Model](https://arxiv.org/abs/2511.00940)
*Zhe Li,Xiang Bai,Jieyu Zhang,Zhuangzhe Wu,Che Xu,Ying Li,Chengkai Hou,Shanghang Zhang*

Main category: cs.RO

TL;DR: 提出基于3D多模态大语言模型的端到端自动重建框架URDF - Anything，实验表明其性能优于现有方法，有良好泛化能力，为构建机器人模拟数字孪生提供高效方案。


<details>
  <summary>Details</summary>
Motivation: 构建精确的铰接物体数字孪生对机器人模拟训练和具身AI世界模型构建至关重要，但以往需手动建模或多阶段流程，需要更高效的方法。

Method: 提出URDF - Anything框架，采用基于点云和文本多模态输入的自回归预测框架联合优化几何分割和运动学参数预测，实现专门的[SEG]令牌机制。

Result: 在模拟和真实数据集上，几何分割mIoU提升17%，运动学参数预测平均误差降低29%，物理可执行性超过基线50%，对训练集外物体也表现良好。

Conclusion: 该方法为机器人模拟构建数字孪生提供高效解决方案，显著增强了从模拟到现实的迁移能力。

Abstract: Constructing accurate digital twins of articulated objects is essential for
robotic simulation training and embodied AI world model building, yet
historically requires painstaking manual modeling or multi-stage pipelines. In
this work, we propose \textbf{URDF-Anything}, an end-to-end automatic
reconstruction framework based on a 3D multimodal large language model (MLLM).
URDF-Anything utilizes an autoregressive prediction framework based on
point-cloud and text multimodal input to jointly optimize geometric
segmentation and kinematic parameter prediction. It implements a specialized
$[SEG]$ token mechanism that interacts directly with point cloud features,
enabling fine-grained part-level segmentation while maintaining consistency
with the kinematic parameter predictions. Experiments on both simulated and
real-world datasets demonstrate that our method significantly outperforms
existing approaches regarding geometric segmentation (mIoU 17\% improvement),
kinematic parameter prediction (average error reduction of 29\%), and physical
executability (surpassing baselines by 50\%). Notably, our method exhibits
excellent generalization ability, performing well even on objects outside the
training set. This work provides an efficient solution for constructing digital
twins for robotic simulation, significantly enhancing the sim-to-real transfer
capability.

</details>


### [614] [Real-Time Learning of Predictive Dynamic Obstacle Models for Robotic Motion Planning](https://arxiv.org/abs/2511.00814)
*Stella Kombo,Masih Haseli,Skylar Wei,Joel W. Burdick*

Main category: cs.RO

TL;DR: 本文提出在线框架，用改进的滑动窗口Hankel - DMD对自治系统中附近代理的运动进行去噪和预测，在模拟和实验中验证该方法能实现稳定的方差感知去噪和短视距预测。


<details>
  <summary>Details</summary>
Motivation: 自治系统需从部分和有噪声的数据中预测附近代理的运动，研究能否实时学习非线性预测模型。

Method: 使用改进的滑动窗口Hankel - DMD去噪和预测，将部分噪声测量嵌入Hankel矩阵，用Page矩阵进行奇异值硬阈值处理，通过Cadzow投影实现低秩一致性，构建时变Hankel - DMD线性预测器进行多步预测，用残差分析提供方差跟踪信号。

Result: 在高斯和重尾噪声模拟以及动态起重机试验台上验证，该方法能实现稳定的方差感知去噪和短视距预测。

Conclusion: 该方法适合集成到实时控制框架中。

Abstract: Autonomous systems often must predict the motions of nearby agents from
partial and noisy data. This paper asks and answers the question: "can we
learn, in real-time, a nonlinear predictive model of another agent's motions?"
Our online framework denoises and forecasts such dynamics using a modified
sliding-window Hankel Dynamic Mode Decomposition (Hankel-DMD). Partial noisy
measurements are embedded into a Hankel matrix, while an associated Page matrix
enables singular-value hard thresholding (SVHT) to estimate the effective rank.
A Cadzow projection enforces structured low-rank consistency, yielding a
denoised trajectory and local noise variance estimates. From this
representation, a time-varying Hankel-DMD lifted linear predictor is
constructed for multi-step forecasts. The residual analysis provides
variance-tracking signals that can support downstream estimators and risk-aware
planning. We validate the approach in simulation under Gaussian and
heavy-tailed noise, and experimentally on a dynamic crane testbed. Results show
that the method achieves stable variance-aware denoising and short-horizon
prediction suitable for integration into real-time control frameworks.

</details>


### [615] [Embodied Cognition Augmented End2End Autonomous Driving](https://arxiv.org/abs/2511.01334)
*Ling Niu,Xiaoji Zheng,Han Wang,Chen Zheng,Ziyuan Yang,Bokui Chen,Jiangtao Gong*

Main category: cs.RO

TL;DR: 本文提出E³AD范式，结合视觉特征提取网络与EEG大模型进行对比学习，提升端到端自动驾驶规划性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于标签监督训练的端到端自动驾驶方法限制了模型通用性和适用性，需新方法提升性能。

Method: 提出E³AD范式，收集认知数据集，用流行驾驶模型作基线，在公开数据集上研究利用人类驾驶认知增强端到端规划的方法和机制，进行开环和闭环测试。

Result: E³AD范式显著提升基线模型端到端规划性能，消融实验验证了驾驶认知的贡献和对比学习过程的有效性。

Conclusion: 这是首次整合人类驾驶认知改进端到端自动驾驶规划的工作，为未来脑启发自动驾驶系统提供了有价值的见解。

Abstract: In recent years, vision-based end-to-end autonomous driving has emerged as a
new paradigm. However, popular end-to-end approaches typically rely on visual
feature extraction networks trained under label supervision. This limited
supervision framework restricts the generality and applicability of driving
models. In this paper, we propose a novel paradigm termed $E^{3}AD$, which
advocates for comparative learning between visual feature extraction networks
and the general EEG large model, in order to learn latent human driving
cognition for enhancing end-to-end planning. In this work, we collected a
cognitive dataset for the mentioned contrastive learning process. Subsequently,
we investigated the methods and potential mechanisms for enhancing end-to-end
planning with human driving cognition, using popular driving models as
baselines on publicly available autonomous driving datasets. Both open-loop and
closed-loop tests are conducted for a comprehensive evaluation of planning
performance. Experimental results demonstrate that the $E^{3}AD$ paradigm
significantly enhances the end-to-end planning performance of baseline models.
Ablation studies further validate the contribution of driving cognition and the
effectiveness of comparative learning process. To the best of our knowledge,
this is the first work to integrate human driving cognition for improving
end-to-end autonomous driving planning. It represents an initial attempt to
incorporate embodied cognitive data into end-to-end autonomous driving,
providing valuable insights for future brain-inspired autonomous driving
systems. Our code will be made available at Github

</details>


### [616] [SLAP: Shortcut Learning for Abstract Planning](https://arxiv.org/abs/2511.01107)
*Y. Isabel Liu,Bowen Li,Benjamin Eysenbach,Tom Silver*

Main category: cs.RO

TL;DR: 提出Shortcut Learning for Abstract Planning (SLAP)方法，利用现有TAMP选项自动发现新选项，在四个模拟机器人环境实验中表现良好。


<details>
  <summary>Details</summary>
Motivation: 长视野、稀疏奖励、连续状态和动作的决策是AI和机器人领域挑战，现有TAMP选项手动定义有局限性。

Method: 使用无模型强化学习在TAMP现有选项诱导的抽象规划图中学习捷径。

Result: 捷径学习比纯规划解决方案更短，比扁平及分层强化学习任务成功率更高，发现与手动定义不同的动态物理即兴动作，在四个模拟环境中解决并泛化多种任务，减少超50%的计划长度，优于规划和强化学习基线。

Conclusion: SLAP方法有效，能解决和泛化多种任务，在计划长度和任务成功率上表现出色。

Abstract: Long-horizon decision-making with sparse rewards and continuous states and
actions remains a fundamental challenge in AI and robotics. Task and motion
planning (TAMP) is a model-based framework that addresses this challenge by
planning hierarchically with abstract actions (options). These options are
manually defined, limiting the agent to behaviors that we as human engineers
know how to program (pick, place, move). In this work, we propose Shortcut
Learning for Abstract Planning (SLAP), a method that leverages existing TAMP
options to automatically discover new ones. Our key idea is to use model-free
reinforcement learning (RL) to learn shortcuts in the abstract planning graph
induced by the existing options in TAMP. Without any additional assumptions or
inputs, shortcut learning leads to shorter solutions than pure planning, and
higher task success rates than flat and hierarchical RL. Qualitatively, SLAP
discovers dynamic physical improvisations (e.g., slap, wiggle, wipe) that
differ significantly from the manually-defined ones. In experiments in four
simulated robotic environments, we show that SLAP solves and generalizes to a
wide range of tasks, reducing overall plan lengths by over 50% and consistently
outperforming planning and RL baselines.

</details>


### [617] [FoldPath: End-to-End Object-Centric Motion Generation via Modulated Implicit Paths](https://arxiv.org/abs/2511.01407)
*Paolo Rabino,Gabriele Tiboni,Tatiana Tommasi*

Main category: cs.RO

TL;DR: 介绍用于OCMG的新方法FoldPath，它以连续函数学习机器人运动，无需脆弱后处理步骤，在预测性能和泛化能力上表现出色，通过实验验证并引入新评估指标推动OCMG走向实用成熟。


<details>
  <summary>Details</summary>
Motivation: 现有OCMG技术存在依赖临时启发式或敏感后处理步骤的问题，需要更有效的算法来实现自动化制造中高精度专家机器人运动。

Method: 提出基于神经场的端到端新方法FoldPath，将机器人运动作为连续函数学习。

Result: 相比近期学习方法有更优预测性能，在仅70个专家样本的真实工业环境中也有泛化能力。

Conclusion: 通过实验和引入新指标，推动OCMG任务向实用成熟发展。

Abstract: Object-Centric Motion Generation (OCMG) is instrumental in advancing
automated manufacturing processes, particularly in domains requiring
high-precision expert robotic motions, such as spray painting and welding. To
realize effective automation, robust algorithms are essential for generating
extended, object-aware trajectories across intricate 3D geometries. However,
contemporary OCMG techniques are either based on ad-hoc heuristics or employ
learning-based pipelines that are still reliant on sensitive post-processing
steps to generate executable paths. We introduce FoldPath, a novel, end-to-end,
neural field based method for OCMG. Unlike prior deep learning approaches that
predict discrete sequences of end-effector waypoints, FoldPath learns the robot
motion as a continuous function, thus implicitly encoding smooth output paths.
This paradigm shift eliminates the need for brittle post-processing steps that
concatenate and order the predicted discrete waypoints. Particularly, our
approach demonstrates superior predictive performance compared to recently
proposed learning-based methods, and attains generalization capabilities even
in real industrial settings, where only a limited amount of 70 expert samples
are provided. We validate FoldPath through comprehensive experiments in a
realistic simulation environment and introduce new, rigorous metrics designed
to comprehensively evaluate long-horizon robotic paths, thus advancing the OCMG
task towards practical maturity.

</details>


### [618] [MO-SeGMan: Rearrangement Planning Framework for Multi Objective Sequential and Guided Manipulation in Constrained Environments](https://arxiv.org/abs/2511.01476)
*Cankut Bora Tuncer,Marc Toussaint,Ozgur S. Oguz*

Main category: cs.RO

TL;DR: 介绍MO - SeGMan规划器用于高约束重排问题，有多种方法提升性能，评估显示比基线更优。


<details>
  <summary>Details</summary>
Motivation: 解决高约束重排问题，处理杂乱、非单调场景。

Method: 采用懒评估方法生成对象放置序列，提出SGFS高效移动关键障碍物，采用细化方法进行自适应子目标选择。

Result: 在九个基准重排任务评估中，MO - SeGMan均生成可行运动计划，求解时间更快，解质量更优。

Conclusion: 所提框架对复杂重排规划问题具有鲁棒性和可扩展性。

Abstract: In this work, we introduce MO-SeGMan, a Multi-Objective Sequential and Guided
Manipulation planner for highly constrained rearrangement problems. MO-SeGMan
generates object placement sequences that minimize both replanning per object
and robot travel distance while preserving critical dependency structures with
a lazy evaluation method. To address highly cluttered, non-monotone scenarios,
we propose a Selective Guided Forward Search (SGFS) that efficiently relocates
only critical obstacles and to feasible relocation points. Furthermore, we
adopt a refinement method for adaptive subgoal selection to eliminate
unnecessary pick-and-place actions, thereby improving overall solution quality.
Extensive evaluations on nine benchmark rearrangement tasks demonstrate that
MO-SeGMan generates feasible motion plans in all cases, consistently achieving
faster solution times and superior solution quality compared to the baselines.
These results highlight the robustness and scalability of the proposed
framework for complex rearrangement planning problems.

</details>


### [619] [RobustVLA: Robustness-Aware Reinforcement Post-Training for Vision-Language-Action Models](https://arxiv.org/abs/2511.01331)
*Hongyin Zhang,Shuo Zhang,Junxi Jin,Qixin Zeng,Runze Li,Donglin Wang*

Main category: cs.RO

TL;DR: 提出轻量级在线RL后训练方法RobustVLA增强VLA模型鲁棒性，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: VLA模型在分布外部署时难以可靠泛化，现有基于RL的后训练方法忽视环境不确定性下的鲁棒性。

Method: 引入RobustVLA，通过系统鲁棒性分析确定雅可比正则化和光滑性正则化。

Result: 在多种机器人环境的广泛实验中，RobustVLA在鲁棒性和可靠性上显著优于现有方法。

Conclusion: 有原则的鲁棒性感知RL后训练是提高VLA模型可靠性和鲁棒性的关键步骤。

Abstract: Vision-Language-Action (VLA) models have recently emerged as powerful
general-purpose policies for robotic manipulation, benefiting from large-scale
multi-modal pre-training. However, they often fail to generalize reliably in
out-of-distribution deployments, where unavoidable disturbances such as
observation noise, sensor errors, or actuation perturbations become prevalent.
While recent Reinforcement Learning (RL)-based post-training provides a
practical means to adapt pre-trained VLA models, existing methods mainly
emphasize reward maximization and overlook robustness to environmental
uncertainty. In this work, we introduce RobustVLA, a lightweight online RL
post-training method designed to explicitly enhance the resilience of VLA
models. Through a systematic robustness analysis, we identify two key
regularizations: Jacobian regularization, which mitigates sensitivity to
observation noise, and smoothness regularization, which stabilizes policies
under action perturbations. Extensive experiments across diverse robotic
environments demonstrate that RobustVLA significantly outperforms prior
state-of-the-art methods in robustness and reliability. Our results highlight
the importance of principled robustness-aware RL post-training as a key step
toward improving the reliability and robustness of VLA models.

</details>


### [620] [Hybrid Neural Network-Based Indoor Localisation System for Mobile Robots Using CSI Data in a Robotics Simulator](https://arxiv.org/abs/2511.01797)
*Javier Ballesteros-Jerez,Jesus Martínez-Gómez,Ismael García-Varea,Luis Orozco-Barbosa,Manuel Castillo-Cara*

Main category: cs.RO

TL;DR: 提出混合神经网络模型，用MIMO系统的CSI数据推断移动机器人位置，有较好定位效果且方法可推广。


<details>
  <summary>Details</summary>
Motivation: 实现复杂环境下移动机器人的精确室内定位和导航。

Method: 结合CNN和MLP形成HyNN，用TINTO工具将CSI读数转换为合成图像，集成到机器人模拟器和ROS中，采用卡尔曼滤波器等状态估计器。

Result: HyNN模型在复杂环境下能实现精确的室内定位和导航。

Conclusion: 该方法具有可推广性，适用于不同场景和数据集。

Abstract: We present a hybrid neural network model for inferring the position of mobile
robots using Channel State Information (CSI) data from a Massive MIMO system.
By leveraging an existing CSI dataset, our approach integrates a Convolutional
Neural Network (CNN) with a Multilayer Perceptron (MLP) to form a Hybrid Neural
Network (HyNN) that estimates 2D robot positions. CSI readings are converted
into synthetic images using the TINTO tool. The localisation solution is
integrated with a robotics simulator, and the Robot Operating System (ROS),
which facilitates its evaluation through heterogeneous test cases, and the
adoption of state estimators like Kalman filters. Our contributions illustrate
the potential of our HyNN model in achieving precise indoor localisation and
navigation for mobile robots in complex environments. The study follows, and
proposes, a generalisable procedure applicable beyond the specific use case
studied, making it adaptable to different scenarios and datasets.

</details>


### [621] [GenDexHand: Generative Simulation for Dexterous Hands](https://arxiv.org/abs/2511.01791)
*Feng Chen,Zhuxiu Xu,Tianzhe Chu,Xunzhe Zhou,Li Sun,Zewen Wu,Shenghua Gao,Zhongyu Li,Yanchao Yang,Yi Ma*

Main category: cs.RO

TL;DR: 提出GenDexHand生成式模拟管道，解决灵巧操作数据稀缺问题，提高环境质量，减少训练时间并提升成功率。


<details>
  <summary>Details</summary>
Motivation: 数据稀缺是具身智能的瓶颈，现有方法在灵巧操作上迁移性差，大规模生成可行且可训练的灵巧手任务仍是挑战。

Method: 提出GenDexHand生成式模拟管道，引入闭环细化过程根据VLM反馈调整物体位置和比例，将任务分解为子任务进行顺序强化学习。

Result: 大幅提高生成环境的平均质量，减少训练时间，增加成功率。

Conclusion: 为具身智能中多样化灵巧手行为的可扩展训练提供了基于模拟的合成数据生成解决方案。

Abstract: Data scarcity remains a fundamental bottleneck for embodied intelligence.
Existing approaches use large language models (LLMs) to automate gripper-based
simulation generation, but they transfer poorly to dexterous manipulation,
which demands more specialized environment design. Meanwhile, dexterous
manipulation tasks are inherently more difficult due to their higher degrees of
freedom. Massively generating feasible and trainable dexterous hand tasks
remains an open challenge. To this end, we present GenDexHand, a generative
simulation pipeline that autonomously produces diverse robotic tasks and
environments for dexterous manipulation. GenDexHand introduces a closed-loop
refinement process that adjusts object placements and scales based on
vision-language model (VLM) feedback, substantially improving the average
quality of generated environments. Each task is further decomposed into
sub-tasks to enable sequential reinforcement learning, reducing training time
and increasing success rates. Our work provides a viable path toward scalable
training of diverse dexterous hand behaviors in embodied intelligence by
offering a simulation-based solution to synthetic data generation. Our website:
https://winniechen2002.github.io/GenDexHand/.

</details>
