<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 28]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.DS](#cs.DS) [Total: 7]
- [cs.GT](#cs.GT) [Total: 2]
- [cs.IR](#cs.IR) [Total: 13]
- [cs.LG](#cs.LG) [Total: 61]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.PF](#cs.PF) [Total: 2]
- [cs.SE](#cs.SE) [Total: 15]
- [stat.ML](#stat.ML) [Total: 2]
- [stat.CO](#stat.CO) [Total: 1]
- [cs.CY](#cs.CY) [Total: 10]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cs.DM](#cs.DM) [Total: 2]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.SD](#cs.SD) [Total: 5]
- [quant-ph](#quant-ph) [Total: 3]
- [econ.GN](#econ.GN) [Total: 3]
- [cs.RO](#cs.RO) [Total: 4]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [stat.ME](#stat.ME) [Total: 3]
- [eess.IV](#eess.IV) [Total: 3]
- [math.CO](#math.CO) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.CL](#cs.CL) [Total: 41]
- [cs.CV](#cs.CV) [Total: 25]
- [math.ST](#math.ST) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [math.OC](#math.OC) [Total: 2]
- [cs.CR](#cs.CR) [Total: 10]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [physics.ed-ph](#physics.ed-ph) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [ConvoLearn: A Dataset of Constructivist Tutor-Student Dialogue](https://arxiv.org/abs/2601.08950)
*Mayank Sharma,Roy Pea,Hari Subramonyam*

Main category: cs.AI

TL;DR: 介绍可引导LLM实现知识构建的数据集ConvoLearn，调优后的模型效果好，为AI导师开发评估建框架。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在教育应用中的基本教学局限，如倾向直接给出答案而非支持对话式学习。

Method: 基于知识构建理论创建ConvoLearn数据集；通过教师与模拟学生的可控交互构建中学地球科学对话数据集；使用QLoRA训练模型。

Result: 训练使LLM行为向知识构建策略转变；人类评估显示调优后的Mistral 7B表现显著优于基础版本和Claude Sonnet 4.5。

Conclusion: 为建构主义AI导师的未来发展和评估建立了潜在框架。

Abstract: In educational applications, LLMs exhibit several fundamental pedagogical limitations, such as their tendency to reveal solutions rather than support dialogic learning. We introduce ConvoLearn (https://huggingface.co/datasets/masharma/convolearn ), a dataset grounded in knowledge building theory that operationalizes six core pedagogical dimensions: cognitive engagement, formative assessment, accountability, cultural responsiveness, metacognition, and power dynamics. We construct a semi-synthetic dataset of 1250 tutor-student dialogues (20 turns each) in middle school Earth Science through controlled interactions between human teachers and a simulated student. Using QLoRA, we demonstrate that training on this dataset meaningfully shifts LLM behavior toward knowledge-building strategies. Human evaluation by 31 teachers shows our fine-tuned Mistral 7B (M = 4.10, SD = 1.03) significantly outperforms both its base version (M = 2.59, SD = 1.11) and Claude Sonnet 4.5 (M = 2.87, SD = 1.29) overall. This work establishes a potential framework to guide future development and evaluation of constructivist AI tutors.

</details>


### [2] [ART: Action-based Reasoning Task Benchmarking for Medical AI Agents](https://arxiv.org/abs/2601.08988)
*Ananya Mantravadi,Shivali Dalmia,Abhishek Mukherji*

Main category: cs.AI

TL;DR: 提出ART基准来评估医疗AI代理多步推理能力，通过分析现有基准识别错误类型并生成任务评估模型，发现模型在聚合和阈值推理有差距。


<details>
  <summary>Details</summary>
Motivation: 现有基准不能充分评估医疗AI代理在基于行动任务上的性能，需要可靠的临床决策支持。

Method: 提出ART基准，采用四阶段流程生成任务，包括场景识别、任务生成、质量审核和评估。

Result: 评估GPT - 4o - mini和Claude 3.5 Sonnet，提示优化后检索接近完美，但聚合和阈值推理有大量差距。

Conclusion: ART揭示面向行动的EHR推理失败模式，推动更可靠临床代理发展，支持高需求护理环境。

Abstract: Reliable clinical decision support requires medical AI agents capable of safe, multi-step reasoning over structured electronic health records (EHRs). While large language models (LLMs) show promise in healthcare, existing benchmarks inadequately assess performance on action-based tasks involving threshold evaluation, temporal aggregation, and conditional logic. We introduce ART, an Action-based Reasoning clinical Task benchmark for medical AI agents, which mines real-world EHR data to create challenging tasks targeting known reasoning weaknesses. Through analysis of existing benchmarks, we identify three dominant error categories: retrieval failures, aggregation errors, and conditional logic misjudgments. Our four-stage pipeline -- scenario identification, task generation, quality audit, and evaluation -- produces diverse, clinically validated tasks grounded in real patient data. Evaluating GPT-4o-mini and Claude 3.5 Sonnet on 600 tasks shows near-perfect retrieval after prompt refinement, but substantial gaps in aggregation (28--64%) and threshold reasoning (32--38%). By exposing failure modes in action-oriented EHR reasoning, ART advances toward more reliable clinical agents, an essential step for AI systems that reduce cognitive load and administrative burden, supporting workforce capacity in high-demand care settings

</details>


### [3] [The Hierarchy of Agentic Capabilities: Evaluating Frontier Models on Realistic RL Environments](https://arxiv.org/abs/2601.09032)
*Logan Ritchie,Sushant Mehta,Nick Heiner,Mason Yu,Edwin Chen*

Main category: cs.AI

TL;DR: 对前沿AI模型在电商RL环境中的150个工作任务进行评估，揭示了代理能力层次，发现模型存在能力差距。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的智能体发展，AI评估需从单轮响应评估转向多步骤任务完成评估，研究前沿AI模型在实际环境中的表现。

Method: 在真实电商RL环境中对前沿AI模型进行150个工作任务的评估，提出以任务为中心的设计方法。

Result: 发现代理能力层次，最佳模型约40%任务失败，不同能力模型失败原因不同，提供了详细失败分析。

Conclusion: 当前前沿模型虽有连贯多步骤行为表现，但在现实工作场景达到人类水平任务完成能力仍有很大差距。

Abstract: The advancement of large language model (LLM) based agents has shifted AI evaluation from single-turn response assessment to multi-step task completion in interactive environments. We present an empirical study evaluating frontier AI models on 150 workplace tasks within a realistic e-commerce RL environment from Surge. Our analysis reveals an empirically-derived \emph{hierarchy of agentic capabilities} that models must master for real-world deployment: (1) tool use, (2) planning and goal formation, (3) adaptability, (4) groundedness, and (5) common-sense reasoning. Even the best-performing models fail approximately 40\% of the tasks, with failures clustering predictably along this hierarchy. Weaker models struggle with fundamental tool use and planning, whereas stronger models primarily fail on tasks requiring contextual inference beyond explicit instructions. We introduce a task-centric design methodology for RL environments that emphasizes diversity and domain expert contributions, provide detailed failure analysis, and discuss implications for agent development. Our findings suggest that while current frontier models can demonstrate coherent multi-step behavior, substantial capability gaps remain before achieving human-level task completion in realistic workplace settings.

</details>


### [4] [Human-AI Co-design for Clinical Prediction Models](https://arxiv.org/abs/2601.09072)
*Jean Feng,Avni Kothari,Patrick Vossler,Andrew Bishara,Lucas Zier,Newton Addo,Aaron Kornblith,Yan Shuo Tan,Chandan Singh*

Main category: cs.AI

TL;DR: 传统临床预测模型开发耗时长、资源多，HACHI框架用AI代理加速可解释模型开发，在两个任务中表现出色并揭示临床AI团队关键作用。


<details>
  <summary>Details</summary>
Motivation: 传统临床预测模型开发需多领域协作，耗时耗资源，纳入非结构化临床笔记时挑战更大，需新方法解决。

Method: 引入HACHI迭代式人在环框架，AI代理探索评估临床笔记概念，专家反馈改进模型学习过程，用简单是非问题定义概念用于线性模型。

Result: 在急性肾损伤和创伤性脑损伤两个任务中，HACHI优于现有方法，发现新临床相关概念，提高模型跨临床站点和时间段的泛化性。

Conclusion: HACHI能加速可解释临床预测模型开发，凸显临床AI团队在模型开发中的关键作用。

Abstract: Developing safe, effective, and practically useful clinical prediction models (CPMs) traditionally requires iterative collaboration between clinical experts, data scientists, and informaticists. This process refines the often small but critical details of the model building process, such as which features/patients to include and how clinical categories should be defined. However, this traditional collaboration process is extremely time- and resource-intensive, resulting in only a small fraction of CPMs reaching clinical practice. This challenge intensifies when teams attempt to incorporate unstructured clinical notes, which can contain an enormous number of concepts. To address this challenge, we introduce HACHI, an iterative human-in-the-loop framework that uses AI agents to accelerate the development of fully interpretable CPMs by enabling the exploration of concepts in clinical notes. HACHI alternates between (i) an AI agent rapidly exploring and evaluating candidate concepts in clinical notes and (ii) clinical and domain experts providing feedback to improve the CPM learning process. HACHI defines concepts as simple yes-no questions that are used in linear models, allowing the clinical AI team to transparently review, refine, and validate the CPM learned in each round. In two real-world prediction tasks (acute kidney injury and traumatic brain injury), HACHI outperforms existing approaches, surfaces new clinically relevant concepts not included in commonly-used CPMs, and improves model generalizability across clinical sites and time periods. Furthermore, HACHI reveals the critical role of the clinical AI team, such as directing the AI agent to explore concepts that it had not previously considered, adjusting the granularity of concepts it considers, changing the objective function to better align with the clinical objectives, and identifying issues of data bias and leakage.

</details>


### [5] [Programming over Thinking: Efficient and Robust Multi-Constraint Planning](https://arxiv.org/abs/2601.09097)
*Derrick Goh Xin Deik,Quanyu Long,Zhengyuan Liu,Nancy F. Chen,Wenya Wang*

Main category: cs.AI

TL;DR: 提出Scalable COde Planning Engine (SCOPE)框架解决多约束规划问题，性能优且成本低。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型方法在多约束规划领域存在局限性，如纯推理范式易出错、成本高，结合编码或求解器策略缺乏灵活性。

Method: 引入SCOPE框架，将特定查询推理与通用代码执行分离。

Result: SCOPE达到了最优性能，降低了成本和延迟，如在TravelPlanner上成功率达93.1%，相比基线有显著提升。

Conclusion: SCOPE是解决多约束规划问题的有效框架，具有一致性、确定性和可重用性。

Abstract: Multi-constraint planning involves identifying, evaluating, and refining candidate plans while satisfying multiple, potentially conflicting constraints. Existing large language model (LLM) approaches face fundamental limitations in this domain. Pure reasoning paradigms, which rely on long natural language chains, are prone to inconsistency, error accumulation, and prohibitive cost as constraints compound. Conversely, LLMs combined with coding- or solver-based strategies lack flexibility: they often generate problem-specific code from scratch or depend on fixed solvers, failing to capture generalizable logic across diverse problems. To address these challenges, we introduce the Scalable COde Planning Engine (SCOPE), a framework that disentangles query-specific reasoning from generic code execution. By separating reasoning from execution, SCOPE produces solver functions that are consistent, deterministic, and reusable across queries while requiring only minimal changes to input parameters. SCOPE achieves state-of-the-art performance while lowering cost and latency. For example, with GPT-4o, it reaches 93.1% success on TravelPlanner, a 61.6% gain over the best baseline (CoT) while cutting inference cost by 1.4x and time by ~4.67x. Code is available at https://github.com/DerrickGXD/SCOPE.

</details>


### [6] [DScheLLM: Enabling Dynamic Scheduling through a Fine-Tuned Dual-System Large language Model](https://arxiv.org/abs/2601.09100)
*Lixiang Zhang,Chenggong Zhao,Qing Gao,Xiaoke Zhao,Gengyi Bai,Jinhu Lv*

Main category: cs.AI

TL;DR: 本文提出DScheLLM动态调度方法，用微调大语言模型在双系统推理架构中处理动态生产调度扰动，实验证明其有效性与潜力。


<details>
  <summary>Details</summary>
Motivation: 传统生产调度方法依赖特定事件模型和解析公式，适应和泛化能力有限，难以应对未知干扰。

Method: 构建基于大语言模型的统一框架处理动态事件，用运筹学求解器生成精确调度作为训练数据，用LoRA微调华为OpenPangu Embedded - 7B模型。

Result: 快速思维模式可高效生成高质量调度，慢速思维模式能产生与求解器兼容、格式良好的决策输入。

Conclusion: 这是早期将大语言模型应用于动态车间调度的研究之一，凸显了大语言模型用于智能自适应调度优化的潜力。

Abstract: Production scheduling is highly susceptible to dynamic disruptions, such as variations in processing times, machine availability, and unexpected task insertions. Conventional approaches typically rely on event-specific models and explicit analytical formulations, which limits their adaptability and generalization across previously unseen disturbances. To overcome these limitations, this paper proposes DScheLLM, a dynamic scheduling approach that leverages fine-tuned large language models within a dual-system (fast-slow) reasoning architecture to address disturbances of different scales. A unified large language model-based framework is constructed to handle dynamic events, where training datasets for both fast and slow reasoning modes are generated using exact schedules obtained from an operations research solver. The Huawei OpenPangu Embedded-7B model is subsequently fine-tuned under the hybrid reasoning paradigms using LoRA. Experimental evaluations on standard job shop scheduling benchmarks demonstrate that the fast-thinking mode can efficiently generate high-quality schedules and the slow-thinking mode can produce solver-compatible and well-formatted decision inputs. To the best of our knowledge, this work represents one of the earliest studies applying large language models to job shop scheduling in dynamic environments, highlighting their considerable potential for intelligent and adaptive scheduling optimization.

</details>


### [7] [AviationLMM: A Large Multimodal Foundation Model for Civil Aviation](https://arxiv.org/abs/2601.09105)
*Wenbin Li,Jingling Wu,Xiaoyong Lin. Jing Chen,Cong Chen*

Main category: cs.AI

TL;DR: 文章介绍用于民航的大多模态基础模型AviationLMM，阐述其架构、需解决的研究问题，旨在推动民航基础模型发展。


<details>
  <summary>Details</summary>
Motivation: 传统民航人工智能解决方案孤立狭窄，难以整合异构数据，限制态势感知等能力，需新模型统一数据。

Method: 先找出现有AI方案与需求差距，再描述摄入多模态输入、进行跨模态对齐融合并产生灵活输出的模型架构，最后确定需解决的关键研究机会。

Result: 无明确提及具体研究结果。

Conclusion: 阐明AviationLMM设计与挑战，有望推动民航基础模型进步，促进构建集成、可信、隐私保护的航空AI生态。

Abstract: Civil aviation is a cornerstone of global transportation and commerce, and ensuring its safety, efficiency and customer satisfaction is paramount. Yet conventional Artificial Intelligence (AI) solutions in aviation remain siloed and narrow, focusing on isolated tasks or single modalities. They struggle to integrate heterogeneous data such as voice communications, radar tracks, sensor streams and textual reports, which limits situational awareness, adaptability, and real-time decision support. This paper introduces the vision of AviationLMM, a Large Multimodal foundation Model for civil aviation, designed to unify the heterogeneous data streams of civil aviation and enable understanding, reasoning, generation and agentic applications. We firstly identify the gaps between existing AI solutions and requirements. Secondly, we describe the model architecture that ingests multimodal inputs such as air-ground voice, surveillance, on-board telemetry, video and structured texts, and performs cross-modal alignment and fusion, and produces flexible outputs ranging from situation summaries and risk alerts to predictive diagnostics and multimodal incident reconstructions. In order to fully realize this vision, we identify key research opportunities to address, including data acquisition, alignment and fusion, pretraining, reasoning, trustworthiness, privacy, robustness to missing modalities, and synthetic scenario generation. By articulating the design and challenges of AviationLMM, we aim to boost the civil aviation foundation model progress and catalyze coordinated research efforts toward an integrated, trustworthy and privacy-preserving aviation AI ecosystem.

</details>


### [8] [The AI Hippocampus: How Far are We From Human Memory?](https://arxiv.org/abs/2601.09113)
*Zixia Jia,Jiaqi Li,Yipeng Kang,Yuxuan Wang,Tong Wu,Quansen Wang,Xiaobo Wang,Shuyi Zhang,Junzhe Shen,Qing Li,Siyuan Qi,Yitao Liang,Di He,Zilong Zheng,Song-Chun Zhu*

Main category: cs.AI

TL;DR: 本文对大语言模型（LLMs）和多模态大语言模型（MLLMs）中的记忆机制进行全面综述，介绍隐式、显式和代理记忆范式，探讨多模态设置下记忆集成，讨论关键进展、基准任务和开放挑战。


<details>
  <summary>Details</summary>
Motivation: 随着模型从静态预测器向可连续学习和个性化推理的交互系统转变，记忆机制成为其架构和功能演进的核心主题，需要对其进行全面综合研究。

Method: 对相关文献进行结构化综合，将其组织成包含隐式、显式和代理记忆范式的分类法。

Result: 明确了三种主要记忆框架（隐式、显式、代理记忆）及其特点，探讨了多模态环境下记忆的集成，还分析了关键架构进展、基准任务。

Conclusion: 提出了记忆容量、对齐、事实一致性和跨系统互操作性等方面的开放挑战。

Abstract: Memory plays a foundational role in augmenting the reasoning, adaptability, and contextual fidelity of modern Large Language Models and Multi-Modal LLMs. As these models transition from static predictors to interactive systems capable of continual learning and personalized inference, the incorporation of memory mechanisms has emerged as a central theme in their architectural and functional evolution. This survey presents a comprehensive and structured synthesis of memory in LLMs and MLLMs, organizing the literature into a cohesive taxonomy comprising implicit, explicit, and agentic memory paradigms. Specifically, the survey delineates three primary memory frameworks. Implicit memory refers to the knowledge embedded within the internal parameters of pre-trained transformers, encompassing their capacity for memorization, associative retrieval, and contextual reasoning. Recent work has explored methods to interpret, manipulate, and reconfigure this latent memory. Explicit memory involves external storage and retrieval components designed to augment model outputs with dynamic, queryable knowledge representations, such as textual corpora, dense vectors, and graph-based structures, thereby enabling scalable and updatable interaction with information sources. Agentic memory introduces persistent, temporally extended memory structures within autonomous agents, facilitating long-term planning, self-consistency, and collaborative behavior in multi-agent systems, with relevance to embodied and interactive AI. Extending beyond text, the survey examines the integration of memory within multi-modal settings, where coherence across vision, language, audio, and action modalities is essential. Key architectural advances, benchmark tasks, and open challenges are discussed, including issues related to memory capacity, alignment, factual consistency, and cross-system interoperability.

</details>


### [9] [PrivacyReasoner: Can LLM Emulate a Human-like Privacy Mind?](https://arxiv.org/abs/2601.09152)
*Yiwen Tu,Xuan Liu,Lianhui Qin,Haojian Jin*

Main category: cs.AI

TL;DR: 介绍AI代理PRA模拟用户隐私担忧形成，结合理论模拟用户推理，有评估器量化推理忠实度，实验显示其性能优。


<details>
  <summary>Details</summary>
Motivation: 超越群体层面情感分析，模拟个体用户如何因现实新闻形成隐私担忧。

Method: PRA结合隐私和认知理论，基于个人评论历史和上下文线索模拟用户特定隐私推理，通过上下文过滤器激活隐私记忆，生成合成评论，用LLM - as - Judge评估器量化推理忠实度。

Result: 在Hacker News讨论实验中，PRA在隐私担忧预测上优于基线代理，能捕捉跨领域可转移推理模式。

Conclusion: PRA在模拟用户隐私担忧形成方面表现良好，具有一定优势和应用价值。

Abstract: This paper introduces PRA, an AI-agent design for simulating how individual users form privacy concerns in response to real-world news. Moving beyond population-level sentiment analysis, PRA integrates privacy and cognitive theories to simulate user-specific privacy reasoning grounded in personal comment histories and contextual cues. The agent reconstructs each user's "privacy mind", dynamically activates relevant privacy memory through a contextual filter that emulates bounded rationality, and generates synthetic comments reflecting how that user would likely respond to new privacy scenarios. A complementary LLM-as-a-Judge evaluator, calibrated against an established privacy concern taxonomy, quantifies the faithfulness of generated reasoning. Experiments on real-world Hacker News discussions show that \PRA outperforms baseline agents in privacy concern prediction and captures transferable reasoning patterns across domains including AI, e-commerce, and healthcare.

</details>


### [10] [Position on LLM-Assisted Peer Review: Addressing Reviewer Gap through Mentoring and Feedback](https://arxiv.org/abs/2601.09182)
*JungMin Yun,JuneHyoung Kwon,MiHyeon Kim,YoungBin Kim*

Main category: cs.AI

TL;DR: AI研究扩张加剧评审差距，论文批判现有自动生成评审的大语言模型方法，提出以大语言模型辅助和教育人类评审的范式转变，包括两个互补系统，以构建可持续学术生态。


<details>
  <summary>Details</summary>
Motivation: AI研究快速扩张使评审差距加剧，威胁同行评审可持续性和导致低质量评估，需解决此问题。

Method: 定义高质量同行评审核心原则，提出大语言模型辅助的指导系统和反馈系统。

Result: 未提及具体结果

Conclusion: 这种以人类为中心的方法有助于增强评审专业知识，构建更可持续的学术生态。

Abstract: The rapid expansion of AI research has intensified the Reviewer Gap, threatening the peer-review sustainability and perpetuating a cycle of low-quality evaluations. This position paper critiques existing LLM approaches that automatically generate reviews and argues for a paradigm shift that positions LLMs as tools for assisting and educating human reviewers. We define the core principles of high-quality peer review and propose two complementary systems grounded in these foundations: (i) an LLM-assisted mentoring system that cultivates reviewers' long-term competencies, and (ii) an LLM-assisted feedback system that helps reviewers refine the quality of their reviews. This human-centered approach aims to strengthen reviewer expertise and contribute to building a more sustainable scholarly ecosystem.

</details>


### [11] [Cluster Workload Allocation: Semantic Soft Affinity Using Natural Language Processing](https://arxiv.org/abs/2601.09282)
*Leszek Sliwko,Jolanta Mizeria-Pietraszko*

Main category: cs.AI

TL;DR: 本文引入基于自然语言处理的语义、意图驱动调度范式，开发原型系统，评估显示大语言模型解析准确率高，调度质量优，验证了语义软亲和性简化工作负载编排的可行性，但存在同步延迟问题。


<details>
  <summary>Details</summary>
Motivation: 解决集群工作负载分配配置复杂、可用性差的问题。

Method: 引入基于自然语言处理的调度范式，通过Kubernetes调度扩展器集成大语言模型，开发含集群状态缓存和意图分析器的原型系统。

Result: 大语言模型解析准确率高，在调度质量测试中表现优异，优于标准Kubernetes配置。

Conclusion: 确认了语义软亲和性可简化工作负载编排，但需异步处理解决同步延迟问题。

Abstract: Cluster workload allocation often requires complex configurations, creating a usability gap. This paper introduces a semantic, intent-driven scheduling paradigm for cluster systems using Natural Language Processing. The system employs a Large Language Model (LLM) integrated via a Kubernetes scheduler extender to interpret natural language allocation hint annotations for soft affinity preferences. A prototype featuring a cluster state cache and an intent analyzer (using AWS Bedrock) was developed. Empirical evaluation demonstrated high LLM parsing accuracy (>95% Subset Accuracy on an evaluation ground-truth dataset) for top-tier models like Amazon Nova Pro/Premier and Mistral Pixtral Large, significantly outperforming a baseline engine. Scheduling quality tests across six scenarios showed the prototype achieved superior or equivalent placement compared to standard Kubernetes configurations, particularly excelling in complex and quantitative scenarios and handling conflicting soft preferences. The results validate using LLMs for accessible scheduling but highlight limitations like synchronous LLM latency, suggesting asynchronous processing for production readiness. This work confirms the viability of semantic soft affinity for simplifying workload orchestration.

</details>


### [12] [MAXS: Meta-Adaptive Exploration with LLM Agents](https://arxiv.org/abs/2601.09259)
*Jian Zhang,Zhiyuan Wang,Zhangqi Wang,Yu He,Haoran Luo,li yuan,Lingling Zhang,Rui Mao,Qika Lin,Jun Liu*

Main category: cs.AI

TL;DR: 提出基于LLM代理的元自适应推理框架MAXS，解决现有方法的问题，在多模型和数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在代理推理时存在局部短视生成和轨迹不稳定问题，难以平衡全局有效性和计算效率。

Method: 提出MAXS框架，采用前瞻策略扩展推理路径、联合选择推理步骤，引入轨迹收敛机制控制计算成本。

Result: 在三个基础模型和五个数据集上的大量实证研究表明，MAXS在性能和推理效率上均优于现有方法。

Conclusion: MAXS有效解决了现有方法的问题，验证了前瞻策略和工具使用的有效性。

Abstract: Large Language Model (LLM) Agents exhibit inherent reasoning abilities through the collaboration of multiple tools. However, during agent inference, existing methods often suffer from (i) locally myopic generation, due to the absence of lookahead, and (ii) trajectory instability, where minor early errors can escalate into divergent reasoning paths. These issues make it difficult to balance global effectiveness and computational efficiency. To address these two issues, we propose meta-adaptive exploration with LLM agents https://github.com/exoskeletonzj/MAXS, a meta-adaptive reasoning framework based on LLM Agents that flexibly integrates tool execution and reasoning planning. MAXS employs a lookahead strategy to extend reasoning paths a few steps ahead, estimating the advantage value of tool usage, and combines step consistency variance and inter-step trend slopes to jointly select stable, consistent, and high-value reasoning steps. Additionally, we introduce a trajectory convergence mechanism that controls computational cost by halting further rollouts once path consistency is achieved, enabling a balance between resource efficiency and global effectiveness in multi-tool reasoning. We conduct extensive empirical studies across three base models (MiMo-VL-7B, Qwen2.5-VL-7B, Qwen2.5-VL-32B) and five datasets, demonstrating that MAXS consistently outperforms existing methods in both performance and inference efficiency. Further analysis confirms the effectiveness of our lookahead strategy and tool usage.

</details>


### [13] [Efficient Paths and Dense Rewards: Probabilistic Flow Reasoning for Large Language Models](https://arxiv.org/abs/2601.09260)
*Yan Liu,Feng Zhang,Zhanyu Ma,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He,Han Liu,Yangdong Deng*

Main category: cs.AI

TL;DR: 提出CoT - Flow框架，将离散推理步骤视为连续概率流，在推理效率和性能上取得平衡。


<details>
  <summary>Details</summary>
Motivation: 当前推理范式将推理过程视为不可分割序列，缺乏量化逐步骤信息增益机制，存在推理低效和优化困难问题。

Method: 提出CoT - Flow框架，包含流引导解码和基于流的强化学习两种方法。

Result: 在具有挑战性的基准测试中，CoT - Flow在推理效率和推理性能之间取得了更好的平衡。

Conclusion: CoT - Flow框架有效，能解决当前推理范式的不足。

Abstract: High-quality chain-of-thought has demonstrated strong potential for unlocking the reasoning capabilities of large language models. However, current paradigms typically treat the reasoning process as an indivisible sequence, lacking an intrinsic mechanism to quantify step-wise information gain. This granularity gap manifests in two limitations: inference inefficiency from redundant exploration without explicit guidance, and optimization difficulty due to sparse outcome supervision or costly external verifiers. In this work, we propose CoT-Flow, a framework that reconceptualizes discrete reasoning steps as a continuous probabilistic flow, quantifying the contribution of each step toward the ground-truth answer. Built on this formulation, CoT-Flow enables two complementary methodologies: flow-guided decoding, which employs a greedy flow-based decoding strategy to extract information-efficient reasoning paths, and flow-based reinforcement learning, which constructs a verifier-free dense reward function. Experiments on challenging benchmarks demonstrate that CoT-Flow achieves a superior balance between inference efficiency and reasoning performance.

</details>


### [14] [Coordinated Pandemic Control with Large Language Model Agents as Policymaking Assistants](https://arxiv.org/abs/2601.09264)
*Ziyi Shi,Xusen Guo,Hongliang Lu,Mingxing Peng,Haotian Wang,Zheng Zhu,Zhenning Li,Yuxuan Liang,Xinhu Zheng,Hai Yang*

Main category: cs.AI

TL;DR: 本文提出一种大语言模型多智能体政策制定框架用于跨区域协调、主动的疫情控制，经美国新冠数据验证，该框架能有效减少感染和死亡人数。


<details>
  <summary>Details</summary>
Motivation: 人类驱动的疫情应对往往碎片化和被动，政策孤立制定且仅在疫情升级后调整，不利于主动干预和全球疫情防控。

Method: 为每个行政区域分配一个大语言模型智能体作为政策制定助手，智能体考虑区域特定流行病学动态并与其他智能体交流以考虑区域间相互依赖关系，通过集成现实数据、疫情演化模拟器和结构化的智能体间通信，在闭环模拟过程中联合探索反事实干预情景并合成协调的政策决策。

Result: 与现实疫情结果相比，该方法在州层面分别最多可减少63.7%的累计感染和40.1%的累计死亡，各州汇总后分别可减少39.0%和27.0%。

Conclusion: 大语言模型多智能体系统可通过协调政策制定实现更有效的疫情控制。

Abstract: Effective pandemic control requires timely and coordinated policymaking across administrative regions that are intrinsically interdependent. However, human-driven responses are often fragmented and reactive, with policies formulated in isolation and adjusted only after outbreaks escalate, undermining proactive intervention and global pandemic mitigation. To address this challenge, here we propose a large language model (LLM) multi-agent policymaking framework that supports coordinated and proactive pandemic control across regions. Within our framework, each administrative region is assigned an LLM agent as an AI policymaking assistant. The agent reasons over region-specific epidemiological dynamics while communicating with other agents to account for cross-regional interdependencies. By integrating real-world data, a pandemic evolution simulator, and structured inter-agent communication, our framework enables agents to jointly explore counterfactual intervention scenarios and synthesize coordinated policy decisions through a closed-loop simulation process. We validate the proposed framework using state-level COVID-19 data from the United States between April and December 2020, together with real-world mobility records and observed policy interventions. Compared with real-world pandemic outcomes, our approach reduces cumulative infections and deaths by up to 63.7% and 40.1%, respectively, at the individual state level, and by 39.0% and 27.0%, respectively, when aggregated across states. These results demonstrate that LLM multi-agent systems can enable more effective pandemic control with coordinated policymaking...

</details>


### [15] [RISER: Orchestrating Latent Reasoning Skills for Adaptive Activation Steering](https://arxiv.org/abs/2601.09269)
*Wencheng Ye,Liang Peng,Xiaoyang Yuan,Yi Bin,Pengpeng Zeng,Hengyu Jin,Heng Tao Shen*

Main category: cs.AI

TL;DR: 提出RISER框架，能自适应引导大语言模型推理，在多个基准测试中改善零样本准确率，效率和准确性良好。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型特定领域推理方法训练密集或干预静态，无法适应动态推理。

Method: 构建可复用推理向量库，用轻量级路由器动态组合，通过强化学习优化路由器。

Result: 在七个不同基准测试中，平均零样本准确率比基础模型提高3.4 - 6.5%，相比思维链推理有更高的token效率和稳定的准确率提升。

Conclusion: RISER能自主组合向量形成可解释、精确的控制策略，使大语言模型推理更可控、高效。

Abstract: Recent work on domain-specific reasoning with large language models (LLMs) often relies on training-intensive approaches that require parameter updates. While activation steering has emerged as a parameter efficient alternative, existing methods apply static, manual interventions that fail to adapt to the dynamic nature of complex reasoning. To address this limitation, we propose RISER (Router-based Intervention for Steerable Enhancement of Reasoning), a plug-and-play intervention framework that adaptively steers LLM reasoning in activation space. RISER constructs a library of reusable reasoning vectors and employs a lightweight Router to dynamically compose them for each input. The Router is optimized via reinforcement learning under task-level rewards, activating latent cognitive primitives in an emergent and compositional manner. Across seven diverse benchmarks, RISER yields 3.4-6.5% average zero-shot accuracy improvements over the base model while surpassing CoT-style reasoning with 2-3x higher token efficiency and robust accuracy gains. Further analysis shows that RISER autonomously combines multiple vectors into interpretable, precise control strategies, pointing toward more controllable and efficient LLM reasoning.

</details>


### [16] [$A^3$-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation](https://arxiv.org/abs/2601.09274)
*Jian Zhang,Yu He,Zhiyuan Wang,Zhangqi Wang,Kai He,Fangzhi Xu,Qika Lin,Jun Liu*

Main category: cs.AI

TL;DR: 提出A^3 - Bench基准评估科学推理，通过双尺度记忆驱动激活，经标注问题、引入评估框架和指标、实验验证此基准并分析记忆激活对推理性能的影响。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要评估最终答案或逐步连贯性，忽略了人类推理背后的记忆驱动机制，为填补此空白而开展研究。

Method: 1. 用SAPM过程标注2198个跨领域科学推理问题。 2. 引入利用锚点和吸引子的双尺度记忆评估框架及AAUI指标衡量记忆激活率。 3. 用各种基础模型和范式进行实验。

Result: 验证了A^3 - Bench基准，并分析了记忆激活对推理性能的影响。

Conclusion: 为基于记忆驱动的科学推理提供了见解。

Abstract: Scientific reasoning relies not only on logical inference but also on activating prior knowledge and experiential structures. Memory can efficiently reuse knowledge and enhance reasoning consistency and stability. However, existing benchmarks mainly evaluate final answers or step-by-step coherence, overlooking the \textit{memory-driven} mechanisms that underlie human reasoning, which involves activating anchors and attractors, then integrating them into multi-step inference. To address this gap, we propose $A^3$-Bench~ https://a3-bench.github.io, a benchmark designed to evaluate scientific reasoning through dual-scale memory-driven activation, grounded in Anchor and Attractor Activation. First, we annotate 2,198 science reasoning problems across domains using the SAPM process(subject, anchor & attractor, problem, and memory developing). Second, we introduce a dual-scale memory evaluation framework utilizing anchors and attractors, along with the AAUI(Anchor--Attractor Utilization Index) metric to measure memory activation rates. Finally, through experiments with various base models and paradigms, we validate $A^3$-Bench and analyze how memory activation impacts reasoning performance, providing insights into memory-driven scientific reasoning.

</details>


### [17] [M$^3$Searcher: Modular Multimodal Information Seeking Agency with Retrieval-Oriented Reasoning](https://arxiv.org/abs/2601.09278)
*Xiaohan Yu,Chao Feng,Lang Mei,Chong Chen*

Main category: cs.AI

TL;DR: 提出M³Searcher多模态信息搜索代理及MMSearchVQA数据集，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度研究风格代理局限于文本模态，扩展到多模态面临专业化 - 泛化权衡和训练数据稀缺的挑战。

Method: 提出M³Searcher，将信息获取与答案推导解耦，用面向检索的多目标奖励优化；开发MMSearchVQA数据集支持以检索为中心的强化学习训练。

Result: M³Searcher在复杂多模态任务中表现优于现有方法，有强迁移适应性和有效推理能力。

Conclusion: M³Searcher和MMSearchVQA数据集能有效解决多模态信息搜索代理面临的挑战。

Abstract: Recent advances in DeepResearch-style agents have demonstrated strong capabilities in autonomous information acquisition and synthesize from real-world web environments. However, existing approaches remain fundamentally limited to text modality. Extending autonomous information-seeking agents to multimodal settings introduces critical challenges: the specialization-generalization trade-off that emerges when training models for multimodal tool-use at scale, and the severe scarcity of training data capturing complex, multi-step multimodal search trajectories. To address these challenges, we propose M$^3$Searcher, a modular multimodal information-seeking agent that explicitly decouples information acquisition from answer derivation. M$^3$Searcher is optimized with a retrieval-oriented multi-objective reward that jointly encourages factual accuracy, reasoning soundness, and retrieval fidelity. In addition, we develop MMSearchVQA, a multimodal multi-hop dataset to support retrieval centric RL training. Experimental results demonstrate that M$^3$Searcher outperforms existing approaches, exhibits strong transfer adaptability and effective reasoning in complex multimodal tasks.

</details>


### [18] [STaR: Sensitive Trajectory Regulation for Unlearning in Large Reasoning Models](https://arxiv.org/abs/2601.09281)
*Jingjing Zhou,Gaoxiang Cong,Li Su,Liang Li*

Main category: cs.AI

TL;DR: 现有大语言模型去学习方法对大推理模型（LRMs）隐私保护不足，本文提出无参数推理时去学习框架STaR，还引入新评估指标，实验表明其能实现全面稳定去学习且效用损失小。


<details>
  <summary>Details</summary>
Motivation: 大推理模型生成复杂思维链轨迹存在隐私风险，现有大语言模型去学习方法无法移除中间步骤敏感内容，导致隐私泄露和安全下降。

Method: 提出STaR框架，先通过语义感知检测识别敏感内容，再通过安全提示前缀注入全局安全约束，接着进行轨迹感知抑制，最后应用标记级自适应过滤；引入多解码一致性评估（MCS）和多粒度成员推理攻击（MIA）评估两个指标。

Result: 在R - TOFU基准上的实验表明，STaR能实现全面稳定去学习，效用损失极小。

Conclusion: STaR为LRMs隐私保护推理设定了新标准。

Abstract: Large Reasoning Models (LRMs) have advanced automated multi-step reasoning, but their ability to generate complex Chain-of-Thought (CoT) trajectories introduces severe privacy risks, as sensitive information may be deeply embedded throughout the reasoning process. Existing Large Language Models (LLMs) unlearning approaches that typically focus on modifying only final answers are insufficient for LRMs, as they fail to remove sensitive content from intermediate steps, leading to persistent privacy leakage and degraded security. To address these challenges, we propose Sensitive Trajectory Regulation (STaR), a parameter-free, inference-time unlearning framework that achieves robust privacy protection throughout the reasoning process. Specifically, we first identify sensitive content via semantic-aware detection. Then, we inject global safety constraints through secure prompt prefix. Next, we perform trajectory-aware suppression to dynamically block sensitive content across the entire reasoning chain. Finally, we apply token-level adaptive filtering to prevent both exact and paraphrased sensitive tokens during generation. Furthermore, to overcome the inadequacies of existing evaluation protocols, we introduce two metrics: Multi-Decoding Consistency Assessment (MCS), which measures the consistency of unlearning across diverse decoding strategies, and Multi-Granularity Membership Inference Attack (MIA) Evaluation, which quantifies privacy protection at both answer and reasoning-chain levels. Experiments on the R-TOFU benchmark demonstrate that STaR achieves comprehensive and stable unlearning with minimal utility loss, setting a new standard for privacy-preserving reasoning in LRMs.

</details>


### [19] [Policy-Based Reinforcement Learning with Action Masking for Dynamic Job Shop Scheduling under Uncertainty: Handling Random Arrivals and Machine Failures](https://arxiv.org/abs/2601.09293)
*Sofiene Lassoued,Stefan Lier,Andreas Schwung*

Main category: cs.AI

TL;DR: 提出解决不确定动态作业车间调度问题的框架，结合Petri网和强化学习，实验显示优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决随机作业到达和意外机器故障带来的调度挑战。

Method: 用有色定时Petri网表示调度环境，Maskable Proximal Policy Optimization进行动态决策，用Gamma分布模拟作业到达，Weibull分布模拟机器故障，研究两种动作掩码策略。

Result: 在动态JSSP基准测试中，该方法在最小化制造周期方面始终优于传统启发式和基于规则的方法。

Conclusion: 结合可解释的Petri网模型和自适应强化学习策略，能为动态不确定制造环境提供有弹性、可扩展和可解释的实时调度框架。

Abstract: We present a novel framework for solving Dynamic Job Shop Scheduling Problems under uncertainty, addressing the challenges introduced by stochastic job arrivals and unexpected machine breakdowns. Our approach follows a model-based paradigm, using Coloured Timed Petri Nets to represent the scheduling environment, and Maskable Proximal Policy Optimization to enable dynamic decision-making while restricting the agent to feasible actions at each decision point. To simulate realistic industrial conditions, dynamic job arrivals are modeled using a Gamma distribution, which captures complex temporal patterns such as bursts, clustering, and fluctuating workloads. Machine failures are modeled using a Weibull distribution to represent age-dependent degradation and wear-out dynamics. These stochastic models enable the framework to reflect real-world manufacturing scenarios better. In addition, we study two action-masking strategies: a non-gradient approach that overrides the probabilities of invalid actions, and a gradient-based approach that assigns negative gradients to invalid actions within the policy network. We conduct extensive experiments on dynamic JSSP benchmarks, demonstrating that our method consistently outperforms traditional heuristic and rule-based approaches in terms of makespan minimization. The results highlight the strength of combining interpretable Petri-net-based models with adaptive reinforcement learning policies, yielding a resilient, scalable, and explainable framework for real-time scheduling in dynamic and uncertain manufacturing environments.

</details>


### [20] [Monte-Carlo Tree Search with Neural Network Guidance for Lane-Free Autonomous Driving](https://arxiv.org/abs/2601.09353)
*Ioannis Peridis,Dimitrios Troullinos,Georgios Chalkiadakis,Pantelis Giankoulidis,Ioannis Papamichail,Markos Papageorgiou*

Main category: cs.AI

TL;DR: 本文提出用蒙特卡洛树搜索（MCTS）方法用于无车道交通中单车自动驾驶规划，评估了相关指标。


<details>
  <summary>Details</summary>
Motivation: 无车道交通提供了更具挑战的自动驾驶环境，需要新的规划方法提升自动驾驶表现。

Method: 采用MCTS规划方法，结合预训练神经网络引导选择阶段，在实验评估中考量安全和效能指标。

Result: 研究了无车道环境中各向同性状态信息影响、NN引导MCTS性能加速以及计算资源与解质量的权衡。

Conclusion: 未在摘要明确提及。

Abstract: Lane-free traffic environments allow vehicles to better harness the lateral capacity of the road without being restricted to lane-keeping, thereby increasing the traffic flow rates. As such, we have a distinct and more challenging setting for autonomous driving. In this work, we consider a Monte-Carlo Tree Search (MCTS) planning approach for single-agent autonomous driving in lane-free traffic, where the associated Markov Decision Process we formulate is influenced from existing approaches tied to reinforcement learning frameworks. In addition, MCTS is equipped with a pre-trained neural network (NN) that guides the selection phase. This procedure incorporates the predictive capabilities of NNs for a more informed tree search process under computational constraints. In our experimental evaluation, we consider metrics that address both safety (through collision rates) and efficacy (through measured speed). Then, we examine: (a) the influence of isotropic state information for vehicles in a lane-free environment, resulting in nudging behaviour--vehicles' policy reacts due to the presence of faster tailing ones, (b) the acceleration of performance for the NN-guided variant of MCTS, and (c) the trade-off between computational resources and solution quality.

</details>


### [21] [Long-term Task-oriented Agent: Proactive Long-term Intent Maintenance in Dynamic Environments](https://arxiv.org/abs/2601.09382)
*Qinglong Shi,Donghai Wang,Hantao Zhou,Jiguo Li,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He*

Main category: cs.AI

TL;DR: 针对大语言模型代理的反应式范式局限，提出主动交互范式，构建数据合成管道和评估基准，调优模型效果好。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型代理多为反应式范式，难以维持长期用户意图和适应动态环境，需新范式。

Method: 提出意图条件监测和事件触发跟进两种能力实现主动交互；引入数据合成管道构建对话数据；提出评估基准ChronosBench。

Result: 评估现有模型揭示其在长期任务上的缺陷；调优模型复杂任务完成率达85.19%，优于其他测试模型。

Conclusion: 数据驱动策略有效。

Abstract: Current large language model agents predominantly operate under a reactive paradigm, responding only to immediate user queries within short-term sessions. This limitation hinders their ability to maintain long-term user's intents and dynamically adapt to evolving external environments. In this paper, we propose a novel interaction paradigm for proactive Task-oriented Agents capable of bridging the gap between relatively static user's needs and a dynamic environment. We formalize proactivity through two key capabilities, (i) Intent-Conditioned Monitoring: The agent autonomously formulates trigger conditions based on dialog history; (ii) Event-Triggered Follow-up: The agent actively engages the user upon detecting useful environmental updates. We introduce a high-quality data synthesis pipeline to construct complex, multi-turn dialog data in a dynamic environment. Furthermore, we attempt to address the lack of evaluation criteria of task-oriented interaction in a dynamic environment by proposing a new benchmark, namely ChronosBench. We evaluated some leading close-source and open-source models at present and revealed their flaws in long-term task-oriented interaction. Furthermore, our fine-tuned model trained using synthetic data for supervised learning achieves a task completion rate of 85.19% for complex tasks including shifts in user intent, outperforming other models under test. And the result validated the effectiveness of our data-driven strategy.

</details>


### [22] [EvoFSM: Controllable Self-Evolution for Deep Research with Finite State Machines](https://arxiv.org/abs/2601.09465)
*Shuo Zhang,Chaofa Yuan,Ryan Guo,Xiaomin Yu,Rui Xu,Zhangquan Chen,Zinuo Li,Zhi Yang,Shuhao Guan,Zhenheng Tang,Sen Hu,Liwen Zhang,Ronghao Chen,Huacan Wang*

Main category: cs.AI

TL;DR: 提出EvoFSM框架解决现有大语言模型代理问题，经实验证明有效并具有泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型代理依赖固定工作流难适应现实开放问题，自进化方法存在不稳定等问题

Method: 提出EvoFSM框架，将优化空间解耦，通过受限操作优化有限状态机，并融入自进化内存

Result: 在五个多跳问答基准测试中证明有效，在DeepSearch基准上准确率达58.0%，在交互式决策任务上验证泛化性

Conclusion: EvoFSM框架可在保证适应性的同时进行有效控制，具有良好效果和泛化性

Abstract: While LLM-based agents have shown promise for deep research, most existing approaches rely on fixed workflows that struggle to adapt to real-world, open-ended queries. Recent work therefore explores self-evolution by allowing agents to rewrite their own code or prompts to improve problem-solving ability, but unconstrained optimization often triggers instability, hallucinations, and instruction drift. We propose EvoFSM, a structured self-evolving framework that achieves both adaptability and control by evolving an explicit Finite State Machine (FSM) instead of relying on free-form rewriting. EvoFSM decouples the optimization space into macroscopic Flow (state-transition logic) and microscopic Skill (state-specific behaviors), enabling targeted improvements under clear behavioral boundaries. Guided by a critic mechanism, EvoFSM refines the FSM through a small set of constrained operations, and further incorporates a self-evolving memory that distills successful trajectories as reusable priors and failure patterns as constraints for future queries. Extensive evaluations on five multi-hop QA benchmarks demonstrate the effectiveness of EvoFSM. In particular, EvoFSM reaches 58.0% accuracy on the DeepSearch benchmark. Additional results on interactive decision-making tasks further validate its generalization.

</details>


### [23] [What Do LLM Agents Know About Their World? Task2Quiz: A Paradigm for Studying Environment Understanding](https://arxiv.org/abs/2601.09503)
*Siyuan Liu,Hongbang Yuan,Xinze Li,Ziyue Zhu,Yixin Cao,Yu-Gang Jiang*

Main category: cs.AI

TL;DR: 提出T2Q评估范式及T2QBench，实验发现任务成功不能代表环境理解，指出当前代理在环境理解上的瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型代理跨环境泛化能力研究不足，当前评估范式无法评估代理对环境的理解。

Method: 提出Task-to-Quiz (T2Q)评估范式，并构建T2QBench进行实验。

Result: 任务成功往往不能有效代表对环境的理解，当前记忆机制无法有效帮助代理获得环境模型。

Conclusion: 确定主动探索和细粒度状态表示是主要瓶颈，为开发更具泛化能力的自主代理提供基础。

Abstract: Large language model (LLM) agents have demonstrated remarkable capabilities in complex decision-making and tool-use tasks, yet their ability to generalize across varying environments remains a under-examined concern. Current evaluation paradigms predominantly rely on trajectory-based metrics that measure task success, while failing to assess whether agents possess a grounded, transferable model of the environment. To address this gap, we propose Task-to-Quiz (T2Q), a deterministic and automated evaluation paradigm designed to decouple task execution from world-state understanding. We instantiate this paradigm in T2QBench, a suite comprising 30 environments and 1,967 grounded QA pairs across multiple difficulty levels. Our extensive experiments reveal that task success is often a poor proxy for environment understanding, and that current memory machanism can not effectively help agents acquire a grounded model of the environment. These findings identify proactive exploration and fine-grained state representation as primary bottlenecks, offering a robust foundation for developing more generalizable autonomous agents.

</details>


### [24] [Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning](https://arxiv.org/abs/2601.09536)
*Dongjie Cheng,Yongqi Li,Zhixin Ma,Hongru Cai,Yupeng Hu,Wenjie Wang,Liqiang Nie,Wenjie Li*

Main category: cs.AI

TL;DR: 文章提出统一生成式多模态推理，用Omni - R1和Omni - R1 - Zero实现，实验表明在多模态任务推理效果好。


<details>
  <summary>Details</summary>
Motivation: 现有多模态推理研究多遵循单一特定任务推理模式，泛化性受限，需处理多种多模态任务。

Method: 提出统一生成式多模态推理，用Omni - R1（两阶段SFT + RL框架）实现功能图像生成，引入Omni - R1 - Zero从纯文本推理数据引导分步可视化，无需多模态注释。

Result: Omni - R1实现跨多种多模态任务的统一生成式推理，Omni - R1 - Zero平均表现相当或超越Omni - R1。

Conclusion: 统一生成式多模态推理是生成式多模态推理的有前景方向。

Abstract: Multimodal Large Language Models (MLLMs) are making significant progress in multimodal reasoning. Early approaches focus on pure text-based reasoning. More recent studies have incorporated multimodal information into the reasoning steps; however, they often follow a single task-specific reasoning pattern, which limits their generalizability across various multimodal tasks. In fact, there are numerous multimodal tasks requiring diverse reasoning skills, such as zooming in on a specific region or marking an object within an image. To address this, we propose unified generative multimodal reasoning, which unifies diverse multimodal reasoning skills by generating intermediate images during the reasoning process. We instantiate this paradigm with Omni-R1, a two-stage SFT+RL framework featuring perception alignment loss and perception reward, thereby enabling functional image generation. Additionally, we introduce Omni-R1-Zero, which eliminates the need for multimodal annotations by bootstrapping step-wise visualizations from text-only reasoning data. Empirical results show that Omni-R1 achieves unified generative reasoning across a wide range of multimodal tasks, and Omni-R1-Zero can match or even surpass Omni-R1 on average, suggesting a promising direction for generative multimodal reasoning.

</details>


### [25] [LLM for Large-Scale Optimization Model Auto-Formulation: A Lightweight Few-Shot Learning Approach](https://arxiv.org/abs/2601.09635)
*Kuo Liang,Yuhang Lu,Jianming Mao,Shuyi Sun,Chunwei Yang,Congcong Zeng,Xiao Jin,Hanzhang Qin,Ruihao Zhu,Chung-Piaw Teo*

Main category: cs.AI

TL;DR: 提出LEAN - LLM - OPT框架用于大規模優化自動公式化，經模擬和實際案例驗證有效，還引入相關基準。


<details>
  <summary>Details</summary>
Motivation: 解決構建大規模優化模型耗時耗力的問題。

Method: 提出LEAN - LLM - OPT框架，利用多個LLM代理構建工作流程，將建模任務分解為子任務，利用輔助工具處理數據。

Result: 在大規模優化建模任務上有強表現，在新加坡航空案例中具實用價值，引入首個大規模優化自動公式化基準。

Conclusion: LEAN - LLM - OPT框架在大規模優化自動公式化方面有效且有競爭力。

Abstract: Large-scale optimization is a key backbone of modern business decision-making. However, building these models is often labor-intensive and time-consuming. We address this by proposing LEAN-LLM-OPT, a LightwEight AgeNtic workflow construction framework for LLM-assisted large-scale OPTimization auto-formulation. LEAN-LLM-OPT takes as input a problem description together with associated datasets and orchestrates a team of LLM agents to produce an optimization formulation. Specifically, upon receiving a query, two upstream LLM agents dynamically construct a workflow that specifies, step-by-step, how optimization models for similar problems can be formulated. A downstream LLM agent then follows this workflow to generate the final output. Leveraging LLMs' text-processing capabilities and common modeling practices, the workflow decomposes the modeling task into a sequence of structured sub-tasks and offloads mechanical data-handling operations to auxiliary tools. This design alleviates the downstream agent's burden related to planning and data handling, allowing it to focus on the most challenging components that cannot be readily standardized. Extensive simulations show that LEAN-LLM-OPT, instantiated with GPT-4.1 and the open source gpt-oss-20B, achieves strong performance on large-scale optimization modeling tasks and is competitive with state-of-the-art approaches. In addition, in a Singapore Airlines choice-based revenue management use case, LEAN-LLM-OPT demonstrates practical value by achieving leading performance across a range of scenarios. Along the way, we introduce Large-Scale-OR and Air-NRM, the first comprehensive benchmarks for large-scale optimization auto-formulation. The code and data of this work is available at https://github.com/CoraLiang01/lean-llm-opt.

</details>


### [26] [PersonalAlign: Hierarchical Implicit Intent Alignment for Personalized GUI Agent with Long-Term User-Centric Records](https://arxiv.org/abs/2601.09636)
*Yibo Lyu,Gongwei Chen,Rui Shao,Weili Guan,Liqiang Nie*

Main category: cs.AI

TL;DR: 提出PersonalAlign任务，引入AndroidIntent基准，提出HIM - Agent，评估多种GUI代理，HIM - Agent性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 现实部署中GUI代理需与用户更复杂的隐式意图对齐。

Method: 提出PersonalAlign任务，引入AndroidIntent基准，标注用户偏好和例程，提出HIM - Agent维护个人记忆并组织用户偏好和例程。

Result: 在AndroidIntent上评估多种GUI代理，HIM - Agent执行性能和主动性能分别提升15.7%和7.3%。

Conclusion: HIM - Agent能有效提升GUI代理在解决模糊指令和提供主动建议方面的性能。

Abstract: While GUI agents have shown strong performance under explicit and completion instructions, real-world deployment requires aligning with users' more complex implicit intents. In this work, we highlight Hierarchical Implicit Intent Alignment for Personalized GUI Agent (PersonalAlign), a new agent task that requires agents to leverage long-term user records as persistent context to resolve omitted preferences in vague instructions and anticipate latent routines by user state for proactive assistance. To facilitate this study, we introduce AndroidIntent, a benchmark designed to evaluate agents' ability in resolving vague instructions and providing proactive suggestions through reasoning over long-term user records. We annotated 775 user-specific preferences and 215 routines from 20k long-term records across different users for evaluation. Furthermore, we introduce Hierarchical Intent Memory Agent (HIM-Agent), which maintains a continuously updating personal memory and hierarchically organizes user preferences and routines for personalization. Finally, we evaluate a range of GUI agents on AndroidIntent, including GPT-5, Qwen3-VL, and UI-TARS, further results show that HIM-Agent significantly improves both execution and proactive performance by 15.7% and 7.3%.

</details>


### [27] [Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning](https://arxiv.org/abs/2601.09667)
*Zhiyuan Hu,Yunhai Hu,Juncheng Liu,Shuyue Stella Li,Yucheng Wang,Zhen Xu,See-Kiong Ng,Anh Tuan Luu,Xinxing Xu,Bryan Hooi,Cynthia Breazeal,Hae Won Park*

Main category: cs.AI

TL;DR: 提出MATTRL框架，在推理时将结构化文本经验注入多智能体审议，在多个基准测试中提升了准确率，且无需调优。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习（MARL）训练资源密集且不稳定，队友共同适应导致非平稳性，奖励稀疏且方差大。

Method: 引入Multi - Agent Test - Time Reinforcement Learning (MATTRL)框架，组建多专家团队进行多轮讨论，检索和整合测试时经验并达成共识决策，研究信用分配构建轮级经验池并重新注入对话。

Result: 在医学、数学和教育的挑战性基准测试中，MATTRL比多智能体基线平均提高准确率3.67%，比单智能体基线提高8.67%，消融研究对比了不同信用分配方案对训练结果的影响。

Conclusion: MATTRL提供了一条稳定、有效且高效的路径，实现无需调优的抗分布偏移多智能体推理。

Abstract: Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce \textbf{Multi-Agent Test-Time Reinforcement Learning (MATTRL)}, a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves and integrates test-time experiences, and reaches consensus for final decision-making. We also study credit assignment for constructing a turn-level experience pool, then reinjecting it into the dialogue. Across challenging benchmarks in medicine, math, and education, MATTRL improves accuracy by an average of 3.67\% over a multi-agent baseline, and by 8.67\% over comparable single-agent baselines. Ablation studies examine different credit-assignment schemes and provide a detailed comparison of how they affect training outcomes. MATTRL offers a stable, effective and efficient path to distribution-shift-robust multi-agent reasoning without tuning.

</details>


### [28] [Automating Supply Chain Disruption Monitoring via an Agentic AI Approach](https://arxiv.org/abs/2601.09680)
*Sara AlMahri,Liming Xu,Alexandra Brintrup*

Main category: cs.AI

TL;DR: 本文介绍用于供应链的最小监督代理AI框架，评估显示其准确性高、响应快，为构建弹性供应链奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现代供应链易受多种干扰，多数公司缺乏上游可见性，需从被动恢复转向主动应对。

Method: 引入最小监督代理AI框架，包含七个由大语言模型和确定性工具驱动的专业代理，能监测、分析和响应供应链干扰。

Result: 在30个合成场景中，系统核心任务F1分数0.962 - 0.991，平均3.83分钟完成端到端分析，成本低，俄乌冲突案例证明其适用性。

Conclusion: 该框架为构建能管理深层网络干扰的弹性、主动和自主供应链迈出了基础性一步。

Abstract: Modern supply chains are increasingly exposed to disruptions from geopolitical events, demand shocks, trade restrictions, to natural disasters. While many of these disruptions originate deep in the supply network, most companies still lack visibility beyond Tier-1 suppliers, leaving upstream vulnerabilities undetected until the impact cascades downstream. To overcome this blind-spot and move from reactive recovery to proactive resilience, we introduce a minimally supervised agentic AI framework that autonomously monitors, analyses, and responds to disruptions across extended supply networks. The architecture comprises seven specialised agents powered by large language models and deterministic tools that jointly detect disruption signals from unstructured news, map them to multi-tier supplier networks, evaluate exposure based on network structure, and recommend mitigations such as alternative sourcing options. \rev{We evaluate the framework across 30 synthesised scenarios covering three automotive manufacturers and five disruption classes. The system achieves high accuracy across core tasks, with F1 scores between 0.962 and 0.991, and performs full end-to-end analyses in a mean of 3.83 minutes at a cost of \$0.0836 per disruption. Relative to industry benchmarks of multi-day, analyst-driven assessments, this represents a reduction of more than three orders of magnitude in response time. A real-world case study of the 2022 Russia-Ukraine conflict further demonstrates operational applicability. This work establishes a foundational step toward building resilient, proactive, and autonomous supply chains capable of managing disruptions across deep-tier networks.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [29] [Physics Informed Optimal Homotopy Analysis Method (PI-OHAM): A Hybrid Analytical Computational Framework for Solving nonlinear Differential Equations](https://arxiv.org/abs/2601.09567)
*Ziya Uddin*

Main category: cs.CE

TL;DR: 提出物理信息最优同伦分析方法（PI - OHAM）求解非线性微分方程，性能优于标准HAM和PINNs，有重要贡献和应用价值。


<details>
  <summary>Details</summary>
Motivation: 为解决非线性微分方程，结合PINNs灵活性和HAM解析透明性，提高求解性能。

Method: 基于经典HAM，采用物理信息残差损失，像PINNs一样结合数据、边界条件和控制方程来系统优化收敛控制参数。

Result: 应用于Blasius边界层问题时，比标准HAM和PINNs有更优的精度 - 时间权衡，收敛更快更准确，接近文献中的数值标准。

Conclusion: PI - OHAM在需要鲁棒性和可解释性的情况下，是求解非线性流体流动、传热等工业问题的高效、准确且易理解的替代方法。

Abstract: We present the Physics-Informed Optimal Homotopy Analysis Method (PI-OHAM) for solving nonlinear differential equations. PI-OHAM, based on classical HAM, employs a physics-informed residual loss to optimize convergence-control parameters systematically by combining data, boundary conditions, and governing equations in the manner similar to Physics Informed Neural Networks (PINNs). The combination of the flexibility of PINNs and the analytical transparency of HAM provides the approach with high numerical stability, rapid convergence, and high consistency with traditional numerical solutions. PI-OHAM has superior accuracy-time trade-offs and faster and more accurate convergence than standard HAM and PINNs when applied to the Blasius boundary-layer problem. It is also very close to numerical standards available in the literature. PI-OHAM ensures analytical transparency and interpretability by series-based solutions, unlike purely data-driven or data-free PINNs. Significant contributions are a conceptual bridge between decades of homotopy-based analysis and modern physics-inspired methods, and a numerically aided but analytically interpretable solver of nonlinear differential equations. PI-OHAM appears as a computationally efficient, accurate and understandable alternative to nonlinear fluid flow, heat transfer and other industrial problems in cases where robustness and interpretability are important.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [30] [Honesty-Aware Multi-Agent Framework for High-Fidelity Synthetic Data Generation in Digital Psychiatric Intake Doctor-Patient Interactions](https://arxiv.org/abs/2601.09216)
*Xinyuan Zhang,Zijian Wang,Chang Dao,Juexiao Zhou*

Main category: cs.DB

TL;DR: 提出多智能体合成框架生成合成精神科 intake 记录，经四项评估验证，所得语料可用于相关研究。


<details>
  <summary>Details</summary>
Motivation: 解决数据稀缺和患者自报不可靠给精神科 intake 和评估带来的挑战。

Method: 构建丰富患者档案，模拟四角色工作流程，以 DAIC - WOZ 访谈为起点生成记录，通过四项评估验证框架。

Result: 生成了涵盖多种疾病和严重程度的语料库。

Conclusion: 该框架可用于研究欺骗感知的精神科评估，以及训练和评估自适应对话智能体。

Abstract: Data scarcity and unreliable self-reporting -- such as concealment or exaggeration -- pose fundamental challenges to psychiatric intake and assessment. We propose a multi-agent synthesis framework that explicitly models patient deception to generate high-fidelity, publicly releasable synthetic psychiatric intake records. Starting from DAIC-WOZ interviews, we construct enriched patient profiles and simulate a four-role workflow: a \emph{Patient} completes self-rated scales and participates in a semi-structured interview under a topic-dependent honesty state; an \emph{Assessor} selects instruments based on demographics and chief complaints; an \emph{Evaluator} conducts the interview grounded in rater-administered scales, tracks suspicion, and completes ratings; and a \emph{Diagnostician} integrates all evidence into a diagnostic summary. Each case links the patient profile, self-rated and rater-administered responses, interview transcript, diagnostic summary, and honesty state. We validate the framework through four complementary evaluations: diagnostic consistency and severity grading, chain-of-thought ablations, human evaluation of clinical realism and dishonesty modeling, and LLM-based comparative evaluation. The resulting corpus spans multiple disorders and severity levels, enabling controlled study of dishonesty-aware psychiatric assessment and the training and evaluation of adaptive dialogue agents.

</details>


### [31] [TiInsight: A SQL-based Automated Exploratory Data Analysis System through Large Language Models](https://arxiv.org/abs/2601.09404)
*Jun-Peng Zhu,Boyan Niu,Peng Cai,Zheming Ni,Kai Xu,Jiajun Huang,Shengbo Ma,Bing Wang,Xuan Zhou,Guanglei Bao,Donghui Zhang,Liu Tang,Qi Liu*

Main category: cs.DB

TL;DR: 本文提出基于SQL的自动化跨领域探索性数据分析系统TiInsight，介绍其特性并在生产环境部署展示能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于SQL的自动化数据探索方法缺乏跨领域分析能力，对大语言模型能力探索不足。

Method: 提供用户友好GUI，支持自然语言查询；构建跨领域探索性数据分析流程，包括分层数据上下文生成、问题澄清与分解、文本转SQL和数据可视化；在PingCAP生产环境部署并使用代表性数据集展示能力。

Result: 在PingCAP生产环境实现并部署TiInsight，通过代表性数据集展示其能力，有演示视频。

Conclusion: TiInsight是有效的基于SQL的自动化跨领域探索性数据分析系统。

Abstract: The SQL-based exploratory data analysis has garnered significant attention within the data analysis community. The emergence of large language models (LLMs) has facilitated the paradigm shift from manual to automated data exploration. However, existing methods generally lack the ability for cross-domain analysis, and the exploration of LLMs capabilities remains insufficient. This paper presents TiInsight, an SQL-based automated cross-domain exploratory data analysis system. First, TiInsight offers a user-friendly GUI enabling users to explore data using natural language queries. Second, TiInsight offers a robust cross-domain exploratory data analysis pipeline: hierarchical data context (i.e., HDC) generation, question clarification and decomposition, text-to-SQL (i.e., TiSQL), and data visualization (i.e., TiChart). Third, we have implemented and deployed TiInsight in the production environment of PingCAP and demonstrated its capabilities using representative datasets. The demo video is available at https://youtu.be/JzYFyYd-emI.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [32] [A Machine Learning Approach Towards Runtime Optimisation of Matrix Multiplication](https://arxiv.org/abs/2601.09114)
*Yufan Xia,Marco De La Pierre,Amanda S. Barnard,Giuseppe Maria Junior Barca*

Main category: cs.DC

TL;DR: 提出用机器学习优化BLAS中GEMM运行时性能的ADSALA库，在两种HPC节点架构测试有25 - 40%加速。


<details>
  <summary>Details</summary>
Motivation: 现代多核共享内存系统复杂，难确定多线程GEMM最小运行时间的线程数。

Method: 构建ADSALA软件库，用机器学习模型根据训练数据为GEMM任务自动选最优线程数。

Result: 在两种HPC节点架构测试，内存使用100MB内的GEMM比传统BLAS实现有25 - 40%加速。

Conclusion: 基于机器学习优化GEMM运行时性能的方法有效。

Abstract: The GEneral Matrix Multiplication (GEMM) is one of the essential algorithms in scientific computing. Single-thread GEMM implementations are well-optimised with techniques like blocking and autotuning. However, due to the complexity of modern multi-core shared memory systems, it is challenging to determine the number of threads that minimises the multi-thread GEMM runtime. We present a proof-of-concept approach to building an Architecture and Data-Structure Aware Linear Algebra (ADSALA) software library that uses machine learning to optimise the runtime performance of BLAS routines. More specifically, our method uses a machine learning model on-the-fly to automatically select the optimal number of threads for a given GEMM task based on the collected training data. Test results on two different HPC node architectures, one based on a two-socket Intel Cascade Lake and the other on a two-socket AMD Zen 3, revealed a 25 to 40 per cent speedup compared to traditional GEMM implementations in BLAS when using GEMM of memory usage within 100 MB.

</details>


### [33] [Transaction-Driven Dynamic Reconfiguration for Certificate-Based Payment Systems](https://arxiv.org/abs/2601.09146)
*Lingkang Shangguan*

Main category: cs.DC

TL;DR: 提出基于拜占庭一致广播的交易驱动动态重配置协议，设计PDCC实现平稳重配置。


<details>
  <summary>Details</summary>
Motivation: 在现代支付系统中实现高性能，避免全局交易排序。

Method: 结合基于用户随机数的交易排序与周期性系统范围共识机制，设计PDCC。

Result: 设计出可实现平稳重配置过程且不影响原系统性能的PDCC。

Conclusion: 所提出的协议和设计能在现代支付系统中有效实现动态重配置并保证性能。

Abstract: We present a transaction-driven dynamic reconfiguration protocol in Modern payment systems based on Byzantine Consistent Broadcast which can achieve high performance by avoiding global transaction ordering. We demonstrate the fundamental paradigm of modern payment systems, which combines user nonce based transactions ordering with periodic system-wide consensus mechanisms. Building on this foundation, we design PDCC(Payment Dynamic Config Change), which can lead a smooth reconfiguration process without impacting the original system's performance.

</details>


### [34] [Optimizing View Change for Byzantine Fault Tolerance in Parallel Consensus](https://arxiv.org/abs/2601.09184)
*Yifei Xie,Btissam Er-Rahmadi,Xiao Chen,Tiejun Ma,Jane Hillston*

Main category: cs.DC

TL;DR: 提出基于混合整数规划的视图变更优化（VCO）模型优化并行BFT协议的领导者选择和追随者重新分配，实验表明其在正常和故障条件下均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有并行BFT协议在视图变更过程中采用被动机制和盲目领导者轮换，常选到不可用或慢速节点，导致性能下降。

Method: 提出VCO模型，用分解方法和改进的benders割求解，基于求解结果提出迭代备份领导者选择算法。

Result: 在Microsoft Azure云环境实验中，VCO驱动的并行BFT在正常和故障条件下均优于现有配置方法。

Conclusion: VCO模型在网络规模增大时有效，适用于高性能并行BFT系统。

Abstract: The parallel Byzantine Fault Tolerant (BFT) protocol is viewed as a promising solution to address the consensus scalability issue of the permissioned blockchain. One of the main challenges in parallel BFT is the view change process that happens when the leader node fails, which can lead to performance bottlenecks. Existing parallel BFT protocols typically rely on passive view change mechanisms with blind leader rotation. Such approaches frequently select unavailable or slow nodes as leaders, resulting in degraded performance. To address these challenges, we propose a View Change Optimization (VCO) model based on mixed integer programming that optimizes leader selection and follower reassignment across parallel committees by considering communication delays and failure scenarios. We applied a decomposition method with efficient subproblems and improved benders cuts to solve the VCO model. Leveraging the results of improved decomposition solution method, we propose an efficient iterative backup leader selection algorithm as views proceed. By performing experiments in Microsoft Azure cloud environments, we demonstrate that the VCO-driven parallel BFT outperforms existing configuration methods under both normal operation and faulty condition. The results show that the VCO model is effective as network size increases, making it a suitable solution for high-performance parallel BFT systems.

</details>


### [35] [LatencyPrism: Online Non-intrusive Latency Sculpting for SLO-Guaranteed LLM Inference](https://arxiv.org/abs/2601.09258)
*Du Yin,Jiayi Ren,Xiayu Sun,Tianyao Zhou,Haizhu Zhou,Ruiyan Ma,Danyang Zhang*

Main category: cs.DC

TL;DR: Latency is critical for LLM inference, existing methods are inadequate, LatencyPrism is introduced with no intrusion for latency analysis, showing good results.


<details>
  <summary>Details</summary>
Motivation: LLM inference latency affects user experience and costs, existing AI profiling methods can't adapt to distributed inference environments.

Method: Developed LatencyPrism, a zero - intrusion multi - platform latency sculpting system without code modification or service restart.

Result: Deployed over six months on thousands of XPUs, enables low - overhead real - time monitoring, F1 - score of 0.98, and proves its capability through experiments.

Conclusion: LatencyPrism is effective in sculpting and analyzing inference latency, ensuring SLO compliance and distinguishing normal variations from anomalies.

Abstract: LLM inference latency critically determines user experience and operational costs, directly impacting throughput under SLO constraints. Even brief latency spikes degrade service quality despite acceptable average performance. However, distributed inference environments featuring diverse software frameworks and XPU architectures combined with dynamic workloads make latency analysis challenging. Constrained by intrusive designs that necessitate service restarts or even suspension, and by hardware-bound implementations that fail to adapt to heterogeneous inference environments, existing AI profiling methods are often inadequate for real-time production analysis.
  We present LatencyPrism, the first zero-intrusion multi-platform latency sculpting system. It aims to break down the inference latency across pipeline, proactively alert on inference latency anomalies, and guarantee adherence to SLOs, all without requiring code modifications or service restarts. LatencyPrism has been deployed across thousands of XPUs for over six months. It enables low-overhead real-time monitoring at batch level with alerts triggered in milliseconds. This approach distinguishes between workload-driven latency variations and anomalies indicating underlying issues with an F1-score of 0.98. We also conduct extensive experiments and investigations into root cause analysis to demonstrate LatencyPrism's capability.

</details>


### [36] [High-Performance Serverless Computing: A Systematic Literature Review on Serverless for HPC, AI, and Big Data](https://arxiv.org/abs/2601.09334)
*Valerio Besozzi,Matteo Della Bartola,Patrizio Dazzi,Marco Danelutto*

Main category: cs.DC

TL;DR: 本文对2018年至2025年初的122篇研究论文进行系统综述，探讨在云、高性能计算及混合环境下使用无服务器范式处理计算密集型应用，提出分类法，分析趋势，为相关人员提供基础。


<details>
  <summary>Details</summary>
Motivation: 大规模计算密集型应用推动云与高性能计算基础设施融合，无服务器计算适合处理动态、并行和分布式工作负载，探讨其在相关环境的应用。

Method: 对2018 - 2025年初的122篇研究文章进行系统文献综述。

Result: 提出包含八个主要研究方向和九个目标用例域的分类法，分析了近期出版趋势和作者合作网络。

Conclusion: 该工作为新研究者和从业者提供有价值基础，指导下一代无服务器并行计算密集型应用解决方案的开发。

Abstract: The widespread deployment of large-scale, compute-intensive applications such as high-performance computing, artificial intelligence, and big data is leading to convergence between cloud and high-performance computing infrastructures. Cloud providers are increasingly integrating high-performance computing capabilities in their infrastructures, such as hardware accelerators and high-speed interconnects, while researchers in the high-performance computing community are starting to explore cloud-native paradigms to improve scalability, elasticity, and resource utilization. In this context, serverless computing emerges as a promising execution model to efficiently handle highly dynamic, parallel, and distributed workloads. This paper presents a comprehensive systematic literature review of 122 research articles published between 2018 and early 2025, exploring the use of the serverless paradigm to develop, deploy, and orchestrate compute-intensive applications across cloud, high-performance computing, and hybrid environments. From these, a taxonomy comprising eight primary research directions and nine targeted use case domains is proposed, alongside an analysis of recent publication trends and collaboration networks among authors, highlighting the growing interest and interconnections within this emerging research field. Overall, this work aims to offer a valuable foundation for both new researchers and experienced practitioners, guiding the development of next-generation serverless solutions for parallel compute-intensive applications.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [37] [An Almost-Optimal Upper Bound on the Push Number of the Torus Puzzle](https://arxiv.org/abs/2601.08989)
*Matteo Caporrella,Stefano Leucci*

Main category: cs.DS

TL;DR: 本文研究Torus Puzzle，提出算法在比原谜题更受限模型下以O(mn · log max {m, n})次单位旋转解决谜题，缩小推数上下界差距。


<details>
  <summary>Details</summary>
Motivation: 原谜题的推数理解不足，已知其上界为O(mn · max{m, n})，需要缩小上下界差距。

Method: 提出一种算法，在更受限的模型下求解Torus Puzzle。

Result: 能以O(mn · log max {m, n})次单位旋转解决谜题，将推数上下界差距从Θ(max{m,n})减至Θ(log max {m, n})。

Conclusion: 算法有效缩小了推数已知上下界的差距。

Abstract: We study the Torus Puzzle, a solitaire game in which the elements of an input $m \times n$ matrix need to be rearranged into a target configuration via a sequence of unit rotations (i.e., circular shifts) of rows and/or columns. Amano et al.\ proposed a more permissive variant of the above puzzle, where each row and column rotation can shift the involved elements by any amount of positions. The number of rotations needed to solve the puzzle in the original and in the permissive variants of the puzzle are respectively known as the \emph{push number} and the \emph{drag number}, where the latter is always smaller than or equal to the former and admits an existential lower bound of $Ω(mn)$. While this lower bound is matched by an $O(mn)$ upper bound, the push number is not so well understood. Indeed, to the best of our knowledge, only an $O(mn \cdot \max\{ m, n \})$ upper bound is currently known. In this paper, we provide an algorithm that solves the Torus Puzzle using $O(mn \cdot \log \max \{m, n\})$ unit rotations in a model that is more restricted than that of the original puzzle. This implies a corresponding upper bound on the push number and reduces the gap between the known upper and lower bounds from $Θ(\max\{m,n\})$ to $Θ(\log \max\{m, n\})$.

</details>


### [38] [A Grouped Sorting Queue Supporting Dynamic Updates for Timer Management in High-Speed Network Interface Cards](https://arxiv.org/abs/2601.09081)
*Zekun Wang,Binghao Yue,Weitao Pan,Jianyi Shi,Yue Hao*

Main category: cs.DS

TL;DR: 本文提出用于优先队列的更新和分组排序操作以实现硬件定时器管理，设计经模拟验证，性能优于现有设计。


<details>
  <summary>Details</summary>
Motivation: 现有定时器管理方案存在软件负载重、精度低、缺乏硬件更新支持和溢出等问题。

Method: 提出优先队列的更新和分组排序操作，用一维脉动阵列和移位寄存器的混合架构实现，通过分组排序机制处理溢出。

Result: 4K深度、16位定时器队列在28nm工艺中达500 MHz以上（175 Mpps，12 ns精度），FPGA上超300 MHz（116 Mpps），减少LUTs和FFs使用。

Conclusion: 所提设计能有效实现硬件定时器管理，性能优于现有方案。

Abstract: With the hardware offloading of network functions, network interface cards (NICs) undertake massive stateful, high-precision, and high-throughput tasks, where timers serve as a critical enabling component. However, existing timer management schemes suffer from heavy software load, low precision, lack of hardware update support, and overflow. This paper proposes two novel operations for priority queues--update and group sorting--to enable hardware timer management. To the best of our knowledge, this work presents the first hardware priority queue to support an update operation through the composition and propagation of basic operations to modify the priorities of elements within the queue. The group sorting mechanism ensures correct timing behavior post-overflow by establishing a group boundary priority to alter the sorting process and element insertion positions. Implemented with a hybrid architecture of a one-dimension (1D) systolic array and shift registers, our design is validated through packet-level simulations for flow table timeout management. Results demonstrate that a 4K-depth, 16-bit timer queue achieves over 500 MHz (175 Mpps, 12 ns precision) in a 28nm process and over 300 MHz (116 Mpps) on an FPGA. Critically, it reduces LUTs and FFs usage by 31% and 25%, respectively, compared to existing designs.

</details>


### [39] [Dynamic Hierarchical $j$-Tree Decomposition and Its Applications](https://arxiv.org/abs/2601.09139)
*Gramoz Goranci,Monika Henzinger,Peter Kiss,Ali Momeni,Gernot Zöcklein*

Main category: cs.DS

TL;DR: 开发新算法框架用于设计基于割的优化问题的近似算法，在动态图中实现近似因子和更新时间，得到新折衷关系，关键是低溯源动态割稀疏器算法。


<details>
  <summary>Details</summary>
Motivation: 为处理边插入和删除的带容量无向图上基于割的优化问题设计近似算法。

Method: 动态维护分层j - 树分解的变体，设计能处理低溯源顶点分裂的动态割稀疏器算法，借助森林打包和新的森林结构见解。

Result: 为基于割的优化问题在全动态设置下获得近似与更新/查询时间的新折衷，后三个问题有首个全动态算法。

Conclusion: 新算法框架有成果，动态割稀疏器构造有独立价值。

Abstract: We develop a new algorithmic framework for designing approximation algorithms for cut-based optimization problems on capacitated undirected graphs that undergo edge insertions and deletions. Specifically, our framework dynamically maintains a variant of the hierarchical $j$-tree decomposition of [Madry FOCS'10], achieving a poly-logarithmic approximation factor to the graph's cut structure and supporting edge updates in $O(n^ε)$ amortized update time, for any arbitrarily small constant $ε\in (0,1)$.
  Consequently, we obtain new trade-offs between approximation and update/query time for fundamental cut-based optimization problems in the fully dynamic setting, including all-pairs minimum cuts, sparsest cut, multi-way cut, and multi-cut. For the last three problems, these trade-offs give the first fully-dynamic algorithms achieving poly-logarithmic approximation in sub-linear time per operation.
  The main technical ingredient behind our dynamic hierarchy is a dynamic cut-sparsifier algorithm that can handle vertex splits with low recourse. This is achieved by white-boxing the dynamic cut sparsifier construction of [Abraham et al. FOCS'16], based on forest packing, together with new structural insights about the maintenance of these forests under vertex splits. Given the versatility of cut sparsification in both the static and dynamic graph algorithms literature, we believe this construction may be of independent interest.

</details>


### [40] [Computational Complexity of Swish](https://arxiv.org/abs/2601.09289)
*Takashi Horiyama,Takehiro Ito,Jun Kawahara,Shin-ichi Minato,Akira Suzuki,Ryuhei Uehara,Yutaro Yamaguchi*

Main category: cs.DS

TL;DR: 本文解决了每张牌有两个符号的Swish游戏计算复杂度的开放问题，证明此情形下问题是NP完全的，无变换时存在多项式时间算法，完善了其复杂度刻画。


<details>
  <summary>Details</summary>
Motivation: 解决先前开放的每张牌有两个符号的Swish游戏计算复杂度问题。

Method: 证明在牌的变换受限情况下的NP难问题，扩展结果到原设置，无变换时给出多项式时间算法。

Result: 证明每张牌有两个符号时Swish是NP完全的，无变换时有多项式时间算法。

Conclusion: 建立了Swish游戏计算复杂度关于每张牌符号数量和允许变换的完整刻画。

Abstract: Swish is a card game in which players are given cards having symbols (hoops and balls), and find a valid superposition of cards, called a "swish." Dailly, Lafourcade, and Marcadet (FUN 2024) studied a generalized version of Swish and showed that the problem is solvable in polynomial time with one symbol per card, while it is NP-complete with three or more symbols per card. In this paper, we resolve the previously open case of two symbols per card, which corresponds to the original game. We show that Swish is NP-complete for this case. Specifically, we prove the NP-hardness when the allowed transformations of cards are restricted to a single (horizontal or vertical) flip or 180-degree rotation, and extend the results to the original setting allowing all three transformations. In contrast, when neither transformation is allowed, we present a polynomial-time algorithm. Combining known and our results, we establish a complete characterization of the computational complexity of Swish with respect to both the number of symbols per card and the allowed transformations.

</details>


### [41] [Engineering Compressed Matrix Multiplication with the Fast Walsh-Hadamard Transform](https://arxiv.org/abs/2601.09477)
*Joel Andersson,Matti Karppa*

Main category: cs.DS

TL;DR: 本文实现Pagh的压缩矩阵乘法算法，用FFT或FWHT提升性能，实验表明FWHT变体更快，实现已开源。


<details>
  <summary>Details</summary>
Motivation: 提升矩阵乘法在稀疏矩阵或部分元素占主导时的性能。

Method: 实现Pagh的随机化压缩矩阵乘法算法，利用FFT或FWHT进行快速多项式乘法，对比FFT和FWHT变种。

Result: FWHT变体比FFT版本快4倍，在有利模式下比Intel MKL的DGEMM快40倍，且估计误差概率低。

Conclusion: 所实现的算法实用，在特定情况下能优于当前最优的DGEMM实现，FWHT变体性能更佳。

Abstract: We present an implementation of Pagh's compressed matrix multiplication algorithm, a randomized algorithm that constructs sketches of matrices to compute an unbiased estimate of their product. By leveraging fast polynomial multiplication via the FFT, the algorithm achieves high performance when the product matrix is sparse or contains only a small number of entries with magnitudes significantly larger than the rest. We show empirically that the algorithm is practical and can outperform state-of-the-art DGEMM implementations when the product matrix has few nonzero entries or is otherwise dominated by a small subset of elements with large magnitude. As a minor theoretical contribution, we replace the FFT with the Fast Walsh-Hadamard Transform (FWHT) in sketched multiplication, preserving all correctness and variance guarantees of the original algorithm.
  Experiments with our carefully engineered multithreaded CPU implementation for dense double-precision matrices on 64-core CPU nodes across a range of synthetic benchmarks, exhibiting variable sparsity patterns, show that the FWHT variant is up to 4 times faster than the FFT-based version. Under favorable sparsity and magnitude patterns in the product matrix, our FWHT-based implementation achieves a speedup of up to 40 over DGEMM from Intel MKL, with low probability of error in the estimates. Our implementation is released as free software and comes with NumPy-compatible Python bindings.

</details>


### [42] [How many users have been here for a long time? Efficient solutions for counting long aggregated visits](https://arxiv.org/abs/2601.09489)
*Peyman Afshani,Rezaul Chowdhury,Inge Li Gørtz,Mayank Goswami,Francesco Silvestri,Mariafiore Tognon*

Main category: cs.DS

TL;DR: 本文针对计数长聚合访问问题，提出精确和近似数据结构及上下界，还研究几何场景下的精确数据结构。


<details>
  <summary>Details</summary>
Motivation: 解决大规模移动性数据集分析中出现的查询问题。

Method: 提出精确数据结构展示时空权衡，基于采样和草图技术提供近似解决方案，研究几何场景下的精确数据结构。

Result: 得到了支持计数长聚合访问的精确和近似数据结构，以及条件和无条件下界。

Conclusion: 提出的方法在解决计数长聚合访问问题上有效，在几何场景下能提升性能。

Abstract: This paper addresses the Counting Long Aggregated Visits problem, which is defined as follows. We are given $n$ users and $m$ regions, where each user spends some time visiting some regions. For a parameter $k$ and a query consisting of a subset of $r$ regions, the task is to count the number of distinct users whose aggregate time spent visiting the query regions is at least $k$. This problem is motivated by queries arising in the analysis of large-scale mobility datasets. We present several exact and approximate data structures for supporting counting long aggregated visits, as well as conditional and unconditional lower bounds. First, we describe an exact data structure that exhibits a space-time tradeoff, as well as efficient approximate solutions based on sampling and sketching techniques. We then study the problem in geometric settings where regions are points in $\mathbb{R}^d$ and queries are hyperrectangles, and derive exact data structures that achieve improved performance in these structured spaces.

</details>


### [43] [Permutation Matching Under Parikh Budgets: Linear-Time Detection, Packing, and Disjoint Selection](https://arxiv.org/abs/2601.09577)
*MD Nazmul Alam Shanto,Md. Tanzeem Rahat,Md. Manzurul Hasan*

Main category: cs.DS

TL;DR: 本文研究通用字母表上的排列模式匹配，提出统一滑动窗口框架，解决优化和打包变体问题，给出线性时间算法。


<details>
  <summary>Details</summary>
Motivation: 经典排列模式匹配存在问题虽有线性时间解，但实际应用需优化和打包变体。

Method: 提出基于维护Parikh向量差的统一滑动窗口框架，用双指针可行性维护算法解决MFSP问题，用贪心最早结束策略解决非重叠出现选择问题。

Result: 实现排列匹配的时间复杂度为O(n + σ)，空间复杂度为O(σ)；MFSP问题可在O(n + σ)时间内解决；非重叠出现选择可在线性时间内计算最大基数不相交匹配集。

Conclusion: 提供简洁、可证明正确且有严格边界的算法，将基于频率的字符串匹配与打包式优化原语联系起来。

Abstract: We study permutation (jumbled/Abelian) pattern matching over a general alphabet $Σ$. Given a pattern P of length m and a text T of length n, the classical task is to decide whether T contains a length-m substring whose Parikh vector equals that of P . While this existence problem admits a linear-time sliding-window solution, many practical applications require optimization and packing variants beyond mere detection. We present a unified sliding-window framework based on maintaining the Parikh-vector difference between P and the current window of T , enabling permutation matching in O(n + σ) time and O(σ) space, where σ = |Σ|. Building on this foundation, we introduce a combinatorial-optimization variant that we call Maximum Feasible Substring under Pattern Supply (MFSP): find the longest substring S of T whose symbol counts are component-wise bounded by those of P . We show that MFSP can also be solved in O(n + σ) time via a two-pointer feasibility maintenance algorithm, providing an exact packing interpretation of P as a resource budget. Finally, we address non-overlapping occurrence selection by modeling each permutation match as an equal-length interval and proving that a greedy earliest-finishing strategy yields a maximum-cardinality set of disjoint matches, computable in linear time once all matches are enumerated. Our results provide concise, provably correct algorithms with tight bounds, and connect frequency-based string matching to packing-style optimization primitives.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [44] [On the Fair Allocation to Asymmetric Agents with Binary XOS Valuations](https://arxiv.org/abs/2601.09299)
*Ziheng Chen,Bo Li,Zihan Luo,Jialin Zhang*

Main category: cs.GT

TL;DR: 研究不可分商品分配问题，在APS公平下改进非对称二元XOS估值的近似比至1/2并给出算法，在WMMS公平下将结果拓展到一般XOS估值。


<details>
  <summary>Details</summary>
Motivation: 前人对商品分配问题在不同公平性和估值条件下已有一定研究，本文旨在进一步改进近似比和拓展结果。

Method: 针对APS公平和WMMS公平分别进行研究，通过算法设计和理论分析。

Result: 在APS公平下，将非对称二元XOS估值近似比提高到1/2并给出多项式时间算法；在WMMS公平下，证明一般XOS估值存在1/n - WMMS分配且该近似比无法改进。

Conclusion: 在商品分配问题的APS公平和WMMS公平研究上取得进展，改进近似比和拓展结果。

Abstract: We study the problem of allocating $m$ indivisible goods among $n$ agents, where each agent's valuation is fractionally subadditive (XOS). With respect to AnyPrice Share (APS) fairness, Kulkarni et al. (2024) showed that, when agents have binary marginal values, a $0.1222$-APS allocation can be found in polynomial time, and there exists an instance where no allocation is better than $0.5$-approximate APS. Very recently, Feige and Grinberg (2025) extended the problem to the asymmetric case, where agents may have different entitlements, and improved the approximation ratio to $1/6$ for general XOS valuations. In this work, we focus on the asymmetric setting with binary XOS valuations, and further improve the approximation ratio to $1/2$, which matches the known upper bound. We also present a polynomial-time algorithm to compute such an allocation. Beyond APS fairness, we also study the weighted maximin share (WMMS) fairness. Farhadi et al. (2019) showed that, a $1/n$-WMMS allocation always exists for agents with general additive valuations, and that this approximation ratio is tight. We extend this result to general XOS valuations, where a $1/n$-WMMS allocation still exists, and this approximation ratio cannot be improved even when marginal values are binary. This shows a sharp contrast to binary additive valuations, where an exact WMMS allocation exists and can be found in polynomial time.

</details>


### [45] [Measuring the benefits of lying in MARA under egalitarian social welfare](https://arxiv.org/abs/2601.09354)
*Jonathan Carrero,Ismael Rodriguez,Fernando Rubio*

Main category: cs.GT

TL;DR: 分析资源按平等社会福利分配时，代理说谎情况，用遗传算法评估不同情况下说谎的好处。


<details>
  <summary>Details</summary>
Motivation: 资源按平等社会福利分配时，代理有动机对实际偏好说谎以获取更多有价值资源，需分析该情况。

Method: 使用遗传算法评估不同情况下说谎的好处。

Result: 未提及。

Conclusion: 未提及。

Abstract: When some resources are to be distributed among a set of agents following egalitarian social welfare, the goal is to maximize the utility of the agent whose utility turns out to be minimal. In this context, agents can have an incentive to lie about their actual preferences, so that more valuable resources are assigned to them. In this paper we analyze this situation, and we present a practical study where genetic algorithms are used to assess the benefits of lying under different situations.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [46] [Navigating Ideation Space: Decomposed Conceptual Representations for Positioning Scientific Ideas](https://arxiv.org/abs/2601.08901)
*Yuexi Shen,Minqian Liu,Dawei Zhou,Lifu Huang*

Main category: cs.IR

TL;DR: 为解决从快速增长文献中识别相关工作和评估新想法新颖性的挑战，引入Ideation Space，提出检索框架和评估算法，实验效果好，为科研提供范式。


<details>
  <summary>Details</summary>
Motivation: 现有嵌入方法无法支持细粒度文献检索，大语言模型评估器有谄媚偏差，难以进行有效新颖性评估，需解决这些问题。

Method: 引入Ideation Space，将科学知识分解为研究问题、方法和核心发现三个维度并对比训练；提出分层子空间检索框架和分解新颖性评估算法。

Result: 实验显示有显著改进，召回率、命中率和新颖性评估与专家判断的相关性都有提升。

Conclusion: 为加速和评估科学发现的未来研究提供了有前景的范式。

Abstract: Scientific discovery is a cumulative process and requires new ideas to be situated within an ever-expanding landscape of existing knowledge. An emerging and critical challenge is how to identify conceptually relevant prior work from rapidly growing literature, and assess how a new idea differentiates from existing research. Current embedding approaches typically conflate distinct conceptual aspects into single representations and cannot support fine-grained literature retrieval; meanwhile, LLM-based evaluators are subject to sycophancy biases, failing to provide discriminative novelty assessment. To tackle these challenges, we introduce the Ideation Space, a structured representation that decomposes scientific knowledge into three distinct dimensions, i.e., research problem, methodology, and core findings, each learned through contrastive training. This framework enables principled measurement of conceptual distance between ideas, and modeling of ideation transitions that capture the logical connections within a proposed idea. Building upon this representation, we propose a Hierarchical Sub-Space Retrieval framework for efficient, targeted literature retrieval, and a Decomposed Novelty Assessment algorithm that identifies which aspects of an idea are novel. Extensive experiments demonstrate substantial improvements, where our approach achieves Recall@30 of 0.329 (16.7% over baselines), our ideation transition retrieval reaches Hit Rate@30 of 0.643, and novelty assessment attains 0.37 correlation with expert judgments. In summary, our work provides a promising paradigm for future research on accelerating and evaluating scientific discovery.

</details>


### [47] [Fine Grained Evaluation of LLMs-as-Judges](https://arxiv.org/abs/2601.08919)
*Sourav Saha,Mandar Mitra*

Main category: cs.IR

TL;DR: 本文研究大语言模型（LLMs）作为信息检索相关性评估者的效果，发现其在人工监督下效果最佳。


<details>
  <summary>Details</summary>
Motivation: 拓展关于LLMs作为文本/图像处理系统输出质量评估‘裁判’的研究，特别是在信息检索标准即席任务中的相关性评估。

Method: 使用INEX倡议创建的基于维基百科的测试集，提示LLMs判断文档相关性并突出相关段落，与人类评估者的操作对比。

Result: 可以评估LLMs在文档层面的判断质量，并量化其判断正确的原因合理性。

Conclusion: LLMs作为‘裁判’在人工监督下效果最好。

Abstract: A good deal of recent research has focused on how Large Language Models
  (LLMs) may be used as `judges' in place of humans to evaluate the quality
  of the output produced by various text / image processing systems. Within
  this broader context, a number of studies have investigated the specific
  question of how effectively LLMs can be used as relevance assessors for
  the standard ad hoc task in Information Retrieval (IR). We extend these
  studies by looking at additional questions. Most importantly, we use a
  Wikipedia based test collection created by the INEX initiative, and
  prompt LLMs to not only judge whether documents are relevant /
  non-relevant, but to highlight relevant passages in documents that it
  regards as useful. The human relevance assessors involved in creating
  this collection were given analogous instructions, i.e., they were asked
  to highlight all passages within a document that respond to the
  information need expressed in a query. This enables us to evaluate the
  quality of LLMs as judges not only at the document level, but to also
  quantify how often these `judges' are right for the right reasons.
  Our findings suggest that LLMs-as-judges work best under human
  supervision.

</details>


### [48] [LLMs Meet Isolation Kernel: Lightweight, Learning-free Binary Embeddings for Fast Retrieval](https://arxiv.org/abs/2601.09159)
*Zhibo Zhang,Yang Xu,Kai Ming Ting,Cam-Tu Nguyen*

Main category: cs.IR

TL;DR: 本文提出无学习方法IKE，将大语言模型嵌入转换为二进制嵌入，在节省存储和加速检索的同时保证精度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型嵌入维度高，现有方法虽有改善但仍有检索准确率下降问题，需新方法解决。

Method: 提出Isolation Kernel Embedding (IKE)方法，用Isolation Kernel将大语言模型嵌入转换为二进制嵌入。

Result: 在多文本检索数据集实验显示，IKE比大语言模型嵌入检索快16.7倍、内存使用低16倍，且精度相当或更好，比CSR等压缩方法在检索效率和效果间平衡更佳。

Conclusion: IKE能有效降低大语言模型嵌入的存储和检索开销，在效率和效果上取得良好平衡。

Abstract: Large language models (LLMs) have recently enabled remarkable progress in text representation. However, their embeddings are typically high-dimensional, leading to substantial storage and retrieval overhead. Although recent approaches such as Matryoshka Representation Learning (MRL) and Contrastive Sparse Representation (CSR) alleviate these issues to some extent, they still suffer from retrieval accuracy degradation. This paper proposes \emph{Isolation Kernel Embedding} or IKE, a learning-free method that transforms an LLM embedding into a binary embedding using Isolation Kernel (IK). IKE is an ensemble of diverse (random) partitions, enabling robust estimation of ideal kernel in the LLM embedding space, thus reducing retrieval accuracy loss as the ensemble grows. Lightweight and based on binary encoding, it offers low memory footprint and fast bitwise computation, lowering retrieval latency. Experiments on multiple text retrieval datasets demonstrate that IKE offers up to 16.7x faster retrieval and 16x lower memory usage than LLM embeddings, while maintaining comparable or better accuracy. Compared to CSR and other compression methods, IKE consistently achieves the best balance between retrieval efficiency and effectiveness.

</details>


### [49] [Why not Collaborative Filtering in Dual View? Bridging Sparse and Dense Models](https://arxiv.org/abs/2601.09286)
*Hanze Guo,Jianxun Lian,Xiao Zhou*

Main category: cs.IR

TL;DR: 现有基于密集嵌入的协同过滤方法在处理冷门物品时信噪比受限，本文提出SaD框架结合密集嵌入和稀疏交互模式，实验表明该框架能提升推荐性能并优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前基于密集嵌入的协同过滤方法在处理冷门物品时，因数据稀疏存在信噪比上限，需突破此瓶颈。

Method: 提出SaD统一框架，引入轻量级双向对齐机制，让密集视图和稀疏视图相互补充。

Result: 即使简单的矩阵分解式密集模型，在该双视图对齐下也能达到先进水平，SaD具有插拔性，在真实基准测试中始终优于强基线模型。

Conclusion: 从双视角利用协同过滤具有强大效果，SaD框架有效提升了推荐性能。

Abstract: Collaborative Filtering (CF) remains the cornerstone of modern recommender systems, with dense embedding--based methods dominating current practice. However, these approaches suffer from a critical limitation: our theoretical analysis reveals a fundamental signal-to-noise ratio (SNR) ceiling when modeling unpopular items, where parameter-based dense models experience diminishing SNR under severe data sparsity. To overcome this bottleneck, we propose SaD (Sparse and Dense), a unified framework that integrates the semantic expressiveness of dense embeddings with the structural reliability of sparse interaction patterns. We theoretically show that aligning these dual views yields a strictly superior global SNR. Concretely, SaD introduces a lightweight bidirectional alignment mechanism: the dense view enriches the sparse view by injecting semantic correlations, while the sparse view regularizes the dense model through explicit structural signals. Extensive experiments demonstrate that, under this dual-view alignment, even a simple matrix factorization--style dense model can achieve state-of-the-art performance. Moreover, SaD is plug-and-play and can be seamlessly applied to a wide range of existing recommender models, highlighting the enduring power of collaborative filtering when leveraged from dual perspectives. Further evaluations on real-world benchmarks show that SaD consistently outperforms strong baselines, ranking first on the BarsMatch leaderboard. The code is publicly available at https://github.com/harris26-G/SaD.

</details>


### [50] [On-Device Large Language Models for Sequential Recommendation](https://arxiv.org/abs/2601.09306)
*Xin Xia,Hongzhi Yin,Shane Culpepper*

Main category: cs.IR

TL;DR: 提出用于顺序推荐任务的设备端部署大语言模型压缩框架OD - LLM，实验证明其有效性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型内存占用和计算开销大，难以在资源受限设备上部署用于设备端推荐。

Method: 提出OD - LLM框架，集成低秩结构压缩算法、新颖分词归一化技术，使用渐进对齐算法迭代优化目标模型参数。

Result: 在顺序推荐基准测试中，模型大小减半时，OD - LLM与原推荐模型效果无损失。

Conclusion: OD - LLM有效且可扩展，是实时设备端解决方案替代昂贵远程执行大语言模型的实用选择。

Abstract: On-device recommendation is critical for a number of real-world applications, especially in scenarios that have agreements on execution latency, user privacy, and robust functionality when internet connectivity is unstable or even impossible. While large language models (LLMs) can now provide exceptional capabilities that model user behavior for sequential recommendation tasks, their substantial memory footprint and computational overhead make the deployment on resource-constrained devices a high risk proposition. In this paper, we propose OD-LLM, the first task-adaptive compression framework explicitly designed to provide efficient and accurate on-device deployment of LLMs for sequential recommendation tasks. OD-LLM uniquely integrates two complementary compression strategies: a low-rank structural compression algorithm which uses Singular Value Decomposition (SVD) to significantly reduce parameter redundancy in the model, and a novel tokenization normalization technique that better complements the low-rank decomposition process being used. Additionally, to minimize any potential performance degradation when using higher compression ratios, a novel progressive alignment algorithm is used to iteratively refine the parameters required layerwise in the target model. Empirical evaluations conducted on sequential recommendation benchmarks show that OD-LLM exhibits no loss in effectiveness when compared to the original recommendation model, when the deployed model size is halved. These promising results demonstrate the efficacy and scalability of OD-LLM, making this novel solution a practical alternative for real-time, on-device solutions wishing to replace expensive, remotely executed LLMs.

</details>


### [51] [LISP -- A Rich Interaction Dataset and Loggable Interactive Search Platform](https://arxiv.org/abs/2601.09366)
*Jana Isabelle Friese,Andreas Konstantin Kruff,Philipp Schaer,Norbert Fuhr,Nicola Ferro*

Main category: cs.IR

TL;DR: 本文提出用于研究交互式信息检索中人类搜索行为的可复用数据集及配套基础设施，并公开资源支持可复现研究。


<details>
  <summary>Details</summary>
Motivation: 为交互式信息检索领域研究人类搜索行为提供可复用资源，促进该领域的可复现研究与资源共享。

Method: 收集61名参与者（122个会话）的详细交互日志，结合用户特征；提供详细研究设置文档、基于网络的感知速度测试和开展类似用户研究的框架。

Result: 通过示例分析展示了数据集的潜力，并将所有资源以开放获取形式发布。

Conclusion: 研究成果有助于研究者调查影响搜索行为的个体和上下文因素，开发或验证考虑这种变异性的用户模拟器。

Abstract: We present a reusable dataset and accompanying infrastructure for studying human search behavior in Interactive Information Retrieval (IIR). The dataset combines detailed interaction logs from 61 participants (122 sessions) with user characteristics, including perceptual speed, topic-specific interest, search expertise, and demographic information. To facilitate reproducibility and reuse, we provide a fully documented study setup, a web-based perceptual speed test, and a framework for conducting similar user studies. Our work allows researchers to investigate individual and contextual factors affecting search behavior, and to develop or validate user simulators that account for such variability. We illustrate the datasets potential through an illustrative analysis and release all resources as open-access, supporting reproducible research and resource sharing in the IIR community.

</details>


### [52] [Dissecting Judicial Reasoning in U.S. Copyright Damage Awards](https://arxiv.org/abs/2601.09459)
*Pei-Chi Lo,Thomas Y. Lu*

Main category: cs.IR

TL;DR: 研究提出基于话语的大语言模型方法分析版权损害赔偿司法推理，优于传统方法并揭示因素权重差异。


<details>
  <summary>Details</summary>
Motivation: 版权损害赔偿司法推理存在不一致性，为诉讼方带来不确定性，且法律决策缺乏实证基础，需要新方法分析。

Method: 引入结合修辞结构理论和代理工作流的大语言模型方法，通过数据集构建、话语分析和代理特征提取三阶段流程解析司法意见。

Result: 话语增强的大语言模型分析优于传统方法，揭示了不同巡回法院因素权重的未量化差异。

Conclusion: 研究在计算法学分析上有方法学进展，为法律从业者、学者和政策制定者提供了实用见解。

Abstract: Judicial reasoning in copyright damage awards poses a core challenge for computational legal analysis. Although federal courts follow the 1976 Copyright Act, their interpretations and factor weightings vary widely across jurisdictions. This inconsistency creates unpredictability for litigants and obscures the empirical basis of legal decisions. This research introduces a novel discourse-based Large Language Model (LLM) methodology that integrates Rhetorical Structure Theory (RST) with an agentic workflow to extract and quantify previously opaque reasoning patterns from judicial opinions. Our framework addresses a major gap in empirical legal scholarship by parsing opinions into hierarchical discourse structures and using a three-stage pipeline, i.e., Dataset Construction, Discourse Analysis, and Agentic Feature Extraction. This pipeline identifies reasoning components and extract feature labels with corresponding discourse subtrees. In analyzing copyright damage rulings, we show that discourse-augmented LLM analysis outperforms traditional methods while uncovering unquantified variations in factor weighting across circuits. These findings offer both methodological advances in computational legal analysis and practical insights into judicial reasoning, with implications for legal practitioners seeking predictive tools, scholars studying legal principle application, and policymakers confronting inconsistencies in copyright law.

</details>


### [53] [Bridging Semantic Understanding and Popularity Bias with LLMs](https://arxiv.org/abs/2601.09478)
*Renqiang Luo,Dong Zhang,Yupeng Gao,Wen Shi,Mingliang Hou,Jiaying Liu,Zhe Wang,Shuo Yu*

Main category: cs.IR

TL;DR: 提出FairLRM框架，用大语言模型增强推荐系统中流行度偏差的语义理解，提升公平性和推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 现有去偏方法忽视流行度偏差的深层语义，导致去偏效果和推荐准确性受限。

Method: 提出FairLRM框架，将流行度偏差分解为物品侧和用户侧组件，使用基于结构化指令的提示增强模型理解。

Result: FairLRM显著提升了公平性和推荐准确性。

Conclusion: FairLRM是一种更具语义感知和可靠性的方法，可增强对流行度偏差的语义理解。

Abstract: Semantic understanding of popularity bias is a crucial yet underexplored challenge in recommender systems, where popular items are often favored at the expense of niche content. Most existing debiasing methods treat the semantic understanding of popularity bias as a matter of diversity enhancement or long-tail coverage, neglecting the deeper semantic layer that embodies the causal origins of the bias itself. Consequently, such shallow interpretations limit both their debiasing effectiveness and recommendation accuracy. In this paper, we propose FairLRM, a novel framework that bridges the gap in the semantic understanding of popularity bias with Recommendation via Large Language Model (RecLLM). FairLRM decomposes popularity bias into item-side and user-side components, using structured instruction-based prompts to enhance the model's comprehension of both global item distributions and individual user preferences. Unlike traditional methods that rely on surface-level features such as "diversity" or "debiasing", FairLRM improves the model's ability to semantically interpret and address the underlying bias. Through empirical evaluation, we show that FairLRM significantly enhances both fairness and recommendation accuracy, providing a more semantically aware and trustworthy approach to enhance the semantic understanding of popularity bias. The implementation is available at https://github.com/LuoRenqiang/FairLRM.

</details>


### [54] [Unifying Search and Recommendation in LLMs via Gradient Multi-Subspace Tuning](https://arxiv.org/abs/2601.09496)
*Jujia Zhao,Zihan Wang,Shuaiqun Pan,Suzan Verberne,Zhaochun Ren*

Main category: cs.IR

TL;DR: 论文提出Gradient Multi - Subspace Tuning (GEMS)框架统一搜索与推荐任务，通过多子空间分解和零空间投影缓解梯度冲突和保留通用知识，实验显示其效果优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有统一搜索与推荐的方法依赖全微调，计算成本高且可扩展性有限，而参数高效微调面临梯度冲突和用户意图理解偏移问题。

Method: 提出GEMS框架，包括多子空间分解以减少梯度干扰，零空间投影以减轻用户意图理解偏移。

Result: 在基准数据集的广泛实验中，GEMS在搜索和推荐任务上始终优于现有基线。

Conclusion: GEMS框架能有效统一搜索与推荐任务，缓解梯度冲突并保留通用知识，具有更好的有效性。

Abstract: Search and recommendation (S&R) are core to online platforms, addressing explicit intent through queries and modeling implicit intent from behaviors, respectively. Their complementary roles motivate a unified modeling paradigm. Early studies to unify S&R adopt shared encoders with task-specific heads, while recent efforts reframe item ranking in both S&R as conditional generation. The latter holds particular promise, enabling end-to-end optimization and leveraging the semantic understanding of LLMs. However, existing methods rely on full fine-tuning, which is computationally expensive and limits scalability. Parameter-efficient fine-tuning (PEFT) offers a more practical alternative but faces two critical challenges in unifying S&R: (1) gradient conflicts across tasks due to divergent optimization objectives, and (2) shifts in user intent understanding caused by overfitting to fine-tuning data, which distort general-domain knowledge and weaken LLM reasoning. To address the above issues, we propose Gradient Multi-Subspace Tuning (GEMS), a novel framework that unifies S&R with LLMs while alleviating gradient conflicts and preserving general-domain knowledge. GEMS introduces (1) \textbf{Multi-Subspace Decomposition}, which disentangles shared and task-specific optimization signals into complementary low-rank subspaces, thereby reducing destructive gradient interference, and (2) \textbf{Null-Space Projection}, which constrains parameter updates to a subspace orthogonal to the general-domain knowledge space, mitigating shifts in user intent understanding. Extensive experiments on benchmark datasets show that GEMS consistently outperforms the state-of-the-art baselines across both search and recommendation tasks, achieving superior effectiveness.

</details>


### [55] [TEMPO: A Realistic Multi-Domain Benchmark for Temporal Reasoning-Intensive Retrieval](https://arxiv.org/abs/2601.09523)
*Abdelrahman Abdallah,Mohammed Ali,Muhammad Abdul-Mageed,Adam Jatowt*

Main category: cs.IR

TL;DR: 介绍TEMPO这一结合时间推理与跨领域推理密集型检索的基准，评估12个检索系统发现挑战，认为其可提升检索和RAG系统的时间推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有时间问答基准和推理密集型检索基准有局限性，现实信息需求需对时间演变推理和跨时间段证据合成。

Method: 引入TEMPO基准，包含复杂查询、分步检索规划和新颖时间指标。

Result: 评估12个检索系统，最佳模型NDCG@10为32.0，Temporal Coverage@10为71.4%，表明检索时间完整证据有困难。

Conclusion: TEMPO为提升检索和RAG系统的时间推理能力提供了有挑战性的基准。

Abstract: Existing temporal QA benchmarks focus on simple fact-seeking queries from news corpora, while reasoning-intensive retrieval benchmarks lack temporal grounding. However, real-world information needs often require reasoning about temporal evolution and synthesizing evidence across time periods. We introduce TEMPO, the first benchmark combining temporal reasoning with reasoning-intensive retrieval across 13 domains. TEMPO features: (1) 1,730 complex queries requiring deep temporal reasoning such as tracking changes, identifying trends, or comparing cross-period evidence; (2) step-wise retrieval planning with 3,976 decomposed steps and gold documents mapped to each step for multi-hop evaluation; and (3) novel temporal metrics including Temporal Coverage@k and Temporal Precision@k measuring whether results span required time periods. Evaluation of 12 retrieval systems reveals substantial challenges: the best model (DiVeR) achieves only 32.0 NDCG@10 and 71.4\% Temporal Coverage@10, demonstrating difficulty in retrieving temporally complete evidence. We believe TEMPO provides a challenging benchmark for improving temporal reasoning in retrieval and RAG systems. Our code and data are available at https://github.com/tempo-bench/Tempo. See also our official website: https://tempo-bench.github.io/.

</details>


### [56] [SpatCode: Rotary-based Unified Encoding Framework for Efficient Spatiotemporal Vector Retrieval](https://arxiv.org/abs/2601.09530)
*Bingde Hu,Enhao Pan,Wanjing Zhou,Yang Gao,Zunlei Feng,Hao Zhong*

Main category: cs.IR

TL;DR: 提出统一时空向量检索框架，包含编码方法、更新机制和检索算法，实验表明其在检索准确性和效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有时空检索方法存在效率低、架构复杂和灵活性有限等问题，需改进。

Method: 提出基于旋转的统一编码方法、循环增量更新机制和基于加权兴趣的检索算法。

Result: 在多个真实数据集上的实验显示，该框架在检索准确性和效率上大幅超越现有基线，且在动态数据演变下保持鲁棒性。

Conclusion: 所提方法对智能系统中可扩展的时空信息检索有效且实用。

Abstract: Spatiotemporal vector retrieval has emerged as a critical paradigm in modern information retrieval, enabling efficient access to massive, heterogeneous data that evolve over both time and space. However, existing spatiotemporal retrieval methods are often extensions of conventional vector search systems that rely on external filters or specialized indices to incorporate temporal and spatial constraints, leading to inefficiency, architectural complexity, and limited flexibility in handling heterogeneous modalities. To overcome these challenges, we present a unified spatiotemporal vector retrieval framework that integrates temporal, spatial, and semantic cues within a coherent similarity space while maintaining scalability and adaptability to continuous data streams. Specifically, we propose (1) a Rotary-based Unified Encoding Method that embeds time and location into rotational position vectors for consistent spatiotemporal representation; (2) a Circular Incremental Update Mechanism that supports efficient sliding-window updates without global re-encoding or index reconstruction; and (3) a Weighted Interest-based Retrieval Algorithm that adaptively balances modality weights for context-aware and personalized retrieval. Extensive experiments across multiple real-world datasets demonstrate that our framework substantially outperforms state-of-the-art baselines in both retrieval accuracy and efficiency, while maintaining robustness under dynamic data evolution. These results highlight the effectiveness and practicality of the proposed approach for scalable spatiotemporal information retrieval in intelligent systems.

</details>


### [57] [Examining DOM Coordinate Effectiveness For Page Segmentation](https://arxiv.org/abs/2601.09543)
*Jason Carpenter,Faaiq Bilal,Eman Ramadan,Zhi-Li Zhang*

Main category: cs.IR

TL;DR: 文章探讨网页分割中DOM坐标对聚类向量的影响，发现无通用向量，视觉坐标表现差，简单向量更好，正确匹配可提升分割准确率，挑战现有分割向量创建方式。


<details>
  <summary>Details</summary>
Motivation: 随着网页数据规模和非结构化形式增长，需更强大自动提取和检索机制，现有工作对聚类向量构建和分量值研究不足，因此深入研究DOM坐标对网页分割的影响。

Method: 详细研究DOM坐标，对比不同坐标构成的向量在网页分割中的表现。

Result: 没有通用向量，视觉坐标表现比DOM坐标平均低20 - 30%；简单向量表现更好，占表现最优向量的68.2%；正确匹配向量、聚类算法和页面时，分割准确率达74%，比简单应用向量提升20%。

Conclusion: 挑战当前分割向量创建方式，为通过DOM坐标聚类优化页面分割提供可能，强调找到最佳网页分割方式的重要性。

Abstract: Web pages form a cornerstone of available data for daily human consumption and with the rise of LLM-based search and learning systems a treasure trove of valuable data. The scale of this data and its unstructured format still continue to grow requiring ever more robust automated extraction and retrieval mechanisms. Existing work, leveraging the web pages Document Object Model (DOM), often derives clustering vectors from coordinates informed by the DOM such as visual placement or tree structure. The construction and component value of these vectors often go unexamined. Our work proposes and examines DOM coordinates in a detail to understand their impact on web page segmentation. Our work finds that there is no one-size-fits-all vector, and that visual coordinates under-perform compared to DOM coordinates by about 20-30% on average. This challenges the necessity of including visual coordinates in clustering vectors. Further, our work finds that simple vectors, comprised of single coordinates, fare better than complex vectors constituting 68.2% of the top performing vectors of the pages examined. Finally, we find that if a vector, clustering algorithm, and page are properly matched, one can achieve overall high segmentation accuracy at 74%. This constitutes a 20% improvement over a naive application of vectors. Conclusively, our results challenge the current orthodoxy for segmentation vector creation, opens up the possibility to optimize page segmentation via clustering on DOM coordinates, and highlights the importance of finding mechanisms to match the best approach for web page segmentation.

</details>


### [58] [MM-BRIGHT: A Multi-Task Multimodal Benchmark for Reasoning-Intensive Retrieval](https://arxiv.org/abs/2601.09562)
*Abdelrahman Abdallah,Mohamed Darwish Mounis,Mahmoud Abdalla,Mahmoud SalahEldin Kasem,Mostafa Farouk Senussi,Mohamed Mahmoud,Mohammed Ali,Adam Jatowt,Hyun-Soo Kang*

Main category: cs.IR

TL;DR: 提出首个用于推理密集型检索的多模态基准MM - BRIGHT，包含2803个真实查询及四个复杂度递增任务，评估显示现有模型表现不佳，MM - BRIGHT可作下一代检索模型测试平台。


<details>
  <summary>Details</summary>
Motivation: 现有检索基准多为基于文本的查询，而现实中有很多含图像等多模态元素的查询，需要推理来识别相关文档，当前缺乏相关基准。

Method: 引入MM - BRIGHT多模态基准，包含2803个跨29个技术领域的真实查询和四个复杂度递增的任务。

Result: 现有最先进的模型在所有任务中表现不佳，如BM25在纯文本检索中nDCG@10仅8.5，多模态模型Nomic - Vision在多模态到文本检索中nDCG@10为27.6，不如最佳纯文本模型DiVeR（32.2）。

Conclusion: MM - BRIGHT为下一代更好集成视觉推理的检索模型提供了测试平台。

Abstract: Existing retrieval benchmarks primarily consist of text-based queries where keyword or semantic matching is usually sufficient. Many real-world queries contain multimodal elements, particularly, images such as diagrams, charts, and screenshots that require intensive reasoning to identify relevant documents. To address this gap, we introduce MM-BRIGHT, the first multimodal benchmark for reasoning-intensive retrieval. Our dataset consists of 2,803 real-world queries spanning 29 diverse technical domains, with four tasks of increasing complexity: text-to-text, multimodal-to-text, multimodal-to-image, and multimodal-to-multimodal retrieval. Extensive evaluation reveals that state-of-the-art models struggle across all tasks: BM25 achieves only 8.5 nDCG@10 on text-only retrieval, while the best multimodal model Nomic-Vision reaches just 27.6 nDCG@10 on multimodal-to-text retrieval actually underperforming the best text-only model (DiVeR: 32.2). These results highlight substantial headroom and position MM-BRIGHT as a testbed for next-generation retrieval models that better integrate visual reasoning. Our code and data are available at https://github.com/mm-bright/MM-BRIGHT. See also our official website: https://mm-bright.github.io/.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [59] [XGBoost Forecasting of NEPSE Index Log Returns with Walk Forward Validation](https://arxiv.org/abs/2601.08896)
*Sahaj Raj Malla,Shreeyash Kayastha,Rumi Suwal,Harish Chandra Bhandari,Rajendra Adhikari*

Main category: cs.LG

TL;DR: 研究用XGBoost回归器为尼泊尔证券交易所指数的日对数收益率开发预测框架，经评估表现优于基准模型，证明梯度提升集成在新兴市场时间序列建模的有效性。


<details>
  <summary>Details</summary>
Motivation: 为尼泊尔证券交易所指数的日对数收益率进行一步预测，建立有效的预测模型。

Method: 使用XGBoost回归器，设计综合特征集，用Optuna进行超参数优化，通过时间序列交叉验证和向前验证评估，用多种指标评估预测准确性。

Result: 最优配置（20滞后的扩展窗口）表现优于ARIMA和岭回归基准模型，对数收益率RMSE为0.013450，MAE为0.009814，方向准确率为65.15%。

Conclusion: 梯度提升集成在新兴市场时间序列非线性动态建模有效，为尼泊尔证券交易所指数预测建立可重复基准。

Abstract: This study develops a robust machine learning framework for one-step-ahead forecasting of daily log-returns in the Nepal Stock Exchange (NEPSE) Index using the XGBoost regressor. A comprehensive feature set is engineered, including lagged log-returns (up to 30 days) and established technical indicators such as short- and medium-term rolling volatility measures and the 14-period Relative Strength Index. Hyperparameter optimization is performed using Optuna with time-series cross-validation on the initial training segment. Out-of-sample performance is rigorously assessed via walk-forward validation under both expanding and fixed-length rolling window schemes across multiple lag configurations, simulating real-world deployment and avoiding lookahead bias. Predictive accuracy is evaluated using root mean squared error, mean absolute error, coefficient of determination (R-squared), and directional accuracy on both log-returns and reconstructed closing prices. Empirical results show that the optimal configuration, an expanding window with 20 lags, outperforms tuned ARIMA and Ridge regression benchmarks, achieving the lowest log-return RMSE (0.013450) and MAE (0.009814) alongside a directional accuracy of 65.15%. While the R-squared remains modest, consistent with the noisy nature of financial returns, primary emphasis is placed on relative error reduction and directional prediction. Feature importance analysis and visual inspection further enhance interpretability. These findings demonstrate the effectiveness of gradient boosting ensembles in modeling nonlinear dynamics in volatile emerging market time series and establish a reproducible benchmark for NEPSE Index forecasting.

</details>


### [60] [Attention Consistency Regularization for Interpretable Early-Exit Neural Networks](https://arxiv.org/abs/2601.08891)
*Yanhua Zhao*

Main category: cs.LG

TL;DR: 本文提出解释引导训练（EGT）框架提升早期退出神经网络的可解释性和一致性，实验显示有速度提升和性能改善。


<details>
  <summary>Details</summary>
Motivation: 早期退出神经网络缺乏可解释性且特征关注点与深层不同，限制信任和可解释性，需要改进。

Method: 提出多目标框架EGT，引入注意力一致性损失使早期退出注意力图与最终退出对齐，通过损失加权组合联合优化分类准确率和注意力一致性。

Result: 在真实图像分类数据集上，EGT实现98.97%的整体准确率，早期退出推理速度提升1.97倍，注意力一致性比基线模型提高18.5%。

Conclusion: EGT方法使早期退出网络在各退出点有更可解释和一致的解释，适用于资源受限环境下的可解释AI应用。

Abstract: Early-exit neural networks enable adaptive inference by allowing predictions at intermediate layers, reducing computational cost. However, early exits often lack interpretability and may focus on different features than deeper layers, limiting trust and explainability. This paper presents Explanation-Guided Training (EGT), a multi-objective framework that improves interpretability and consistency in early-exit networks through attention-based regularization. EGT introduces an attention consistency loss that aligns early-exit attention maps with the final exit. The framework jointly optimizes classification accuracy and attention consistency through a weighted combination of losses. Experiments on a real-world image classification dataset demonstrate that EGT achieves up to 98.97% overall accuracy (matching baseline performance) with a 1.97x inference speedup through early exits, while improving attention consistency by up to 18.5% compared to baseline models. The proposed method provides more interpretable and consistent explanations across all exit points, making early-exit networks more suitable for explainable AI applications in resource-constrained environments.

</details>


### [61] [Spectral Generative Flow Models: A Physics-Inspired Replacement for Vectorized Large Language Models](https://arxiv.org/abs/2601.08893)
*Andrew Kiruluta*

Main category: cs.LG

TL;DR: 引入Spectral Generative Flow Models (SGFMs)，是基于物理启发的大语言模型替代方案，有三项关键创新，与传统生成建模方法不同且有发展潜力。


<details>
  <summary>Details</summary>
Motivation: 寻找基于物理启发的替代基于变压器的大语言模型的方案。

Method: 将生成视为由多尺度小波基中受约束的随机动力学控制的连续场的演化，用局部算子、谱投影和类Navier - Stokes传输代替全局注意力。

Result: 提出了三项关键创新：统一文本和视频的场论本体、小波域表示、受约束的随机流。

Conclusion: SGFMs为下一代生成模型在长程连贯性、多模态通用性和物理结构归纳偏置方面提供了理论途径。

Abstract: We introduce Spectral Generative Flow Models (SGFMs), a physics-inspired alternative to transformer-based large language models. Instead of representing text or video as sequences of discrete tokens processed by attention, SGFMs treat generation as the evolution of a continuous field governed by constrained stochastic dynamics in a multiscale wavelet basis. This formulation replaces global attention with local operators, spectral projections, and Navier--Stokes-like transport, yielding a generative mechanism grounded in continuity, geometry, and physical structure.
  Our framework provides three key innovations: (i) a field-theoretic ontology in which text and video are unified as trajectories of a stochastic partial differential equation; (ii) a wavelet-domain representation that induces sparsity, scale separation, and computational efficiency; and (iii) a constrained stochastic flow that enforces stability, coherence, and uncertainty propagation. Together, these components define a generative architecture that departs fundamentally from autoregressive modeling and diffusion-based approaches. SGFMs offer a principled path toward long-range coherence, multimodal generality, and physically structured inductive bias in next-generation generative models.

</details>


### [62] [Private LLM Inference on Consumer Blackwell GPUs: A Practical Guide for Cost-Effective Local Deployment in SMEs](https://arxiv.org/abs/2601.09527)
*Jonathan Knoop,Hendrik Holtmann*

Main category: cs.LG

TL;DR: 评估NVIDIA Blackwell消费级GPU用于生产大语言模型推理的性能，发现其可替代多数中小企业云推理。


<details>
  <summary>Details</summary>
Motivation: 中小企业寻求云大语言模型API替代方案，因存在数据隐私问题，而专用云GPU实例隐私保障有限且有持续成本，专业本地硬件昂贵。

Method: 对NVIDIA Blackwell消费级GPU（RTX 5060 Ti、5070 Ti、5090）进行系统评估，对四个开源权重模型在79种配置下进行基准测试。

Result: RTX 5090性能优于5060 Ti；预算GPU在API工作负载中每美元吞吐量最高；NVFP4量化有优势；自托管推理成本低。

Conclusion: 消费级GPU可替代多数中小企业云推理，除对延迟敏感的长上下文RAG，同时提供部署指导和基准数据。

Abstract: SMEs increasingly seek alternatives to cloud LLM APIs, which raise data privacy concerns. Dedicated cloud GPU instances offer improved privacy but with limited guarantees and ongoing costs, while professional on-premise hardware (A100, H100) remains prohibitively expensive. We present a systematic evaluation of NVIDIA's Blackwell consumer GPUs (RTX 5060 Ti, 5070 Ti, 5090) for production LLM inference, benchmarking four open-weight models (Qwen3-8B, Gemma3-12B, Gemma3-27B, GPT-OSS-20B) across 79 configurations spanning quantization formats (BF16, W4A16, NVFP4, MXFP4), context lengths (8k-64k), and three workloads: RAG, multi-LoRA agentic serving, and high-concurrency APIs. The RTX 5090 delivers 3.5-4.6x higher throughput than the 5060 Ti with 21x lower latency for RAG, but budget GPUs achieve the highest throughput-per-dollar for API workloads with sub-second latency. NVFP4 quantization provides 1.6x throughput over BF16 with 41% energy reduction and only 2-4% quality loss. Self-hosted inference costs $0.001-0.04 per million tokens (electricity only), which is 40-200x cheaper than budget-tier cloud APIs, with hardware breaking even in under four months at moderate volume (30M tokens/day). Our results show that consumer GPUs can reliably replace cloud inference for most SME workloads, except latency-critical long-context RAG, where high-end GPUs remain essential. We provide deployment guidance and release all benchmark data for reproducible SME-scale deployments.

</details>


### [63] [DriftGuard: A Hierarchical Framework for Concept Drift Detection and Remediation in Supply Chain Forecasting](https://arxiv.org/abs/2601.08928)
*Shahnawaz Alam,Mohammed Abdul Rahman,Bareera Sadeqa*

Main category: cs.LG

TL;DR: 供应链预测模型会因概念漂移而性能下降，当前方法有不足，本文提出DriftGuard框架，在M5数据集上验证效果良好。


<details>
  <summary>Details</summary>
Motivation: 供应链预测模型存在概念漂移问题，当前行业手动监测和定期重训浪费资源、错过快速漂移事件，学术方法有局限性，需端到端系统解决问题。

Method: 提出DriftGuard框架，结合四种检测方法和分层传播分析检测漂移位置，用SHAP分析诊断根源，采用成本感知重训策略更新模型。

Result: 在M5零售数据集超30000个时间序列上评估，DriftGuard检测召回率达97.8%，4.2天内完成，通过针对性修复实现最高417的投资回报。

Conclusion: DriftGuard框架能有效应对供应链预测模型的概念漂移问题，在检测、诊断和修复方面表现出色。

Abstract: Supply chain forecasting models degrade over time as real-world conditions change. Promotions shift, consumer preferences evolve, and supply disruptions alter demand patterns, causing what is known as concept drift. This silent degradation leads to stockouts or excess inventory without triggering any system warnings. Current industry practice relies on manual monitoring and scheduled retraining every 3-6 months, which wastes computational resources during stable periods while missing rapid drift events. Existing academic methods focus narrowly on drift detection without addressing diagnosis or remediation, and they ignore the hierarchical structure inherent in supply chain data. What retailers need is an end-to-end system that detects drift early, explains its root causes, and automatically corrects affected models. We propose DriftGuard, a five-module framework that addresses the complete drift lifecycle. The system combines an ensemble of four complementary detection methods, namely error-based monitoring, statistical tests, autoencoder anomaly detection, and Cumulative Sum (CUSUM) change-point analysis, with hierarchical propagation analysis to identify exactly where drift occurs across product lines. Once detected, Shapley Additive Explanations (SHAP) analysis diagnoses the root causes, and a cost-aware retraining strategy selectively updates only the most affected models. Evaluated on over 30,000 time series from the M5 retail dataset, DriftGuard achieves 97.8% detection recall within 4.2 days and delivers up to 417 return on investment through targeted remediation.

</details>


### [64] [SCaLE: Switching Cost aware Learning and Exploration](https://arxiv.org/abs/2601.09042)
*Neelkamal Bhuyan,Debankur Mukherjee,Adam Wierman*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This work addresses the fundamental problem of unbounded metric movement costs in bandit online convex optimization, by considering high-dimensional dynamic quadratic hitting costs and $\ell_2$-norm switching costs in a noisy bandit feedback model. For a general class of stochastic environments, we provide the first algorithm SCaLE that provably achieves a distribution-agnostic sub-linear dynamic regret, without the knowledge of hitting cost structure. En-route, we present a novel spectral regret analysis that separately quantifies eigenvalue-error driven regret and eigenbasis-perturbation driven regret. Extensive numerical experiments, against online-learning baselines, corroborate our claims, and highlight statistical consistency of our algorithm.

</details>


### [65] [Breaking the Bottlenecks: Scalable Diffusion Models for 3D Molecular Generation](https://arxiv.org/abs/2601.08963)
*Adrita Das,Peiran Jiang,Dantong Zhu,Barnabas Poczos,Jose Lugo-Martinez*

Main category: cs.LG

TL;DR: 本文通过RTK框架对DDDM进行重新解释，统一了确定性和随机扩散，解决分子扩散瓶颈，实证表明RTK引导的确定性去噪效果更好。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在分子设计应用受限，DDDM虽有改进但理论基础不明确，需对其进行理论解释并解决分子扩散瓶颈。

Method: 通过RTK框架对DDDM进行重新解释，将DDDM反向过程表示为近似核算子。

Result: RTK引导的确定性去噪比随机扩散模型收敛更快、结构保真度更高，且保持化学有效性。

Conclusion: 从RTK视角对DDDM的重新解释具有理论和实践优势，解决了分子扩散中的问题。

Abstract: Diffusion models have emerged as a powerful class of generative models for molecular design, capable of capturing complex structural distributions and achieving high fidelity in 3D molecule generation. However, their widespread use remains constrained by long sampling trajectories, stochastic variance in the reverse process, and limited structural awareness in denoising dynamics. The Directly Denoising Diffusion Model (DDDM) mitigates these inefficiencies by replacing stochastic reverse MCMC updates with deterministic denoising step, substantially reducing inference time. Yet, the theoretical underpinnings of such deterministic updates have remained opaque. In this work, we provide a principled reinterpretation of DDDM through the lens of the Reverse Transition Kernel (RTK) framework by Huang et al. 2024, unifying deterministic and stochastic diffusion under a shared probabilistic formalism. By expressing the DDDM reverse process as an approximate kernel operator, we show that the direct denoising process implicitly optimizes a structured transport map between noisy and clean samples. This perspective elucidates why deterministic denoising achieves efficient inference. Beyond theoretical clarity, this reframing resolves several long-standing bottlenecks in molecular diffusion. The RTK view ensures numerical stability by enforcing well-conditioned reverse kernels, improves sample consistency by eliminating stochastic variance, and enables scalable and symmetry-preserving denoisers that respect SE(3) equivariance. Empirically, we demonstrate that RTK-guided deterministic denoising achieves faster convergence and higher structural fidelity than stochastic diffusion models, while preserving chemical validity across GEOM-DRUGS dataset. Code, models, and datasets are publicly available in our project repository.

</details>


### [66] [Efficient Clustering in Stochastic Bandits](https://arxiv.org/abs/2601.09162)
*G Dhinesh Chandran,Kota Srinivas Reddy,Srikrishna Bhashyam*

Main category: cs.LG

TL;DR: 研究固定置信度下的Bandit Clustering问题，提出高效算法EBC及其启发式变体EBC - H，展示其计算效率和性能优势。


<details>
  <summary>Details</summary>
Motivation: 现有BC算法计算成本高，且现有结果多假设手臂为高斯分布，本文研究更广泛的向量参数分布。

Method: 提出EBC算法，每次向最优值迈一步；提出启发式变体EBC - H，基于停止规则计算量进行手臂选择。

Result: 通过与现有算法对比，显示EBC和EBC - H计算效率高；模拟证明EBC渐近最优；在合成和真实数据集上，EBC和EBC - H性能优于现有方法。

Conclusion: EBC和EBC - H在计算效率和性能上有优势，可用于解决Bandit Clustering问题。

Abstract: We study the Bandit Clustering (BC) problem under the fixed confidence setting, where the objective is to group a collection of data sequences (arms) into clusters through sequential sampling from adaptively selected arms at each time step while ensuring a fixed error probability at the stopping time. We consider a setting where arms in a cluster may have different distributions. Unlike existing results in this setting, which assume Gaussian-distributed arms, we study a broader class of vector-parametric distributions that satisfy mild regularity conditions. Existing asymptotically optimal BC algorithms require solving an optimization problem as part of their sampling rule at each step, which is computationally costly. We propose an Efficient Bandit Clustering algorithm (EBC), which, instead of solving the full optimization problem, takes a single step toward the optimal value at each time step, making it computationally efficient while remaining asymptotically optimal. We also propose a heuristic variant of EBC, called EBC-H, which further simplifies the sampling rule, with arm selection based on quantities computed as part of the stopping rule. We highlight the computational efficiency of EBC and EBC-H by comparing their per-sample run time with that of existing algorithms. The asymptotic optimality of EBC is supported through simulations on the synthetic datasets. Through simulations on both synthetic and real-world datasets, we show the performance gain of EBC and EBC-H over existing approaches.

</details>


### [67] [Continuous Fairness On Data Streams](https://arxiv.org/abs/2601.08976)
*Subhodeep Ghosh,Zhihui Du,Angela Bonifati,Manish Kumar,David Bader,Senjuti Basu Roy*

Main category: cs.LG

TL;DR: 研究数据流中窗口的连续组公平性问题，提出新公平模型并解决监测和重排序挑战，评估显示方法有效。


<details>
  <summary>Details</summary>
Motivation: 在窗口大小大时，期望在更细粒度上执行组公平性。

Method: 设计基于草图的数据结构进行实时监测，开发有理论保证的重排序算法。

Result: 实现毫秒级处理，平均每秒约30000次查询吞吐量，重排序算法在某些情况下将块级组公平性提高达95%，平均提高50 - 60%。

Conclusion: 提出的方法在实际中有效，块级公平性优于窗口级公平性。

Abstract: We study the problem of enforcing continuous group fairness over windows in data streams. We propose a novel fairness model that ensures group fairness at a finer granularity level (referred to as block) within each sliding window. This formulation is particularly useful when the window size is large, making it desirable to enforce fairness at a finer granularity. Within this framework, we address two key challenges: efficiently monitoring whether each sliding window satisfies block-level group fairness, and reordering the current window as effectively as possible when fairness is violated. To enable real-time monitoring, we design sketch-based data structures that maintain attribute distributions with minimal overhead. We also develop optimal, efficient algorithms for the reordering task, supported by rigorous theoretical guarantees. Our evaluation on four real-world streaming scenarios demonstrates the practical effectiveness of our approach. We achieve millisecond-level processing and a throughput of approximately 30,000 queries per second on average, depending on system parameters. The stream reordering algorithm improves block-level group fairness by up to 95% in certain cases, and by 50-60% on average across datasets. A qualitative study further highlights the advantages of block-level fairness compared to window-level fairness.

</details>


### [68] [Geometric Stability: The Missing Axis of Representations](https://arxiv.org/abs/2601.09173)
*Prashant C. Raju*

Main category: cs.LG

TL;DR: 提出几何稳定性概念和Shesha框架，发现其与相似性不相关且机制不同，在多场景有应用价值，为跨系统审核表示提供补充。


<details>
  <summary>Details</summary>
Motivation: 现有表征分析聚焦相似性，未关注表征结构的鲁棒性，引入几何稳定性来量化扰动下表征几何的可靠性。

Method: 提出Shesha框架，在七个领域2463种配置下进行研究。

Result: 稳定性和相似性经验上不相关（ρ≈0.01）且机制不同；在安全监测、可控性、模型选择等方面有优势；能预测CRISPR扰动一致性和神经 - 行为耦合。

Conclusion: 几何稳定性为跨生物和计算系统的表征审核提供了对相似性必要的补充。

Abstract: Analysis of learned representations has a blind spot: it focuses on $similarity$, measuring how closely embeddings align with external references, but similarity reveals only what is represented, not whether that structure is robust. We introduce $geometric$ $stability$, a distinct dimension that quantifies how reliably representational geometry holds under perturbation, and present $Shesha$, a framework for measuring it. Across 2,463 configurations in seven domains, we show that stability and similarity are empirically uncorrelated ($ρ\approx 0.01$) and mechanistically distinct: similarity metrics collapse after removing the top principal components, while stability retains sensitivity to fine-grained manifold structure. This distinction yields actionable insights: for safety monitoring, stability acts as a functional geometric canary, detecting structural drift nearly 2$\times$ more sensitively than CKA while filtering out the non-functional noise that triggers false alarms in rigid distance metrics; for controllability, supervised stability predicts linear steerability ($ρ= 0.89$-$0.96$); for model selection, stability dissociates from transferability, revealing a geometric tax that transfer optimization incurs. Beyond machine learning, stability predicts CRISPR perturbation coherence and neural-behavioral coupling. By quantifying $how$ $reliably$ systems maintain structure, geometric stability provides a necessary complement to similarity for auditing representations across biological and computational systems.

</details>


### [69] [Optimising for Energy Efficiency and Performance in Machine Learning](https://arxiv.org/abs/2601.08991)
*Emile Dos Santos Ferreira,Neil D. Lawrence,Andrei Paleyes*

Main category: cs.LG

TL;DR: 本文开发ECOpt优化机器学习能耗和性能，揭示能耗衡量指标问题，展示其环境效益并发现CIFAR - 10新模型。


<details>
  <summary>Details</summary>
Motivation: 机器学习能耗和环境影响增加，但能耗缩放定律不明，现有研究忽视推理成本且缺乏有效测量工具。

Method: 开发能量消耗优化器ECOpt，通过量化性能与能耗的帕累托边界辅助决策。

Result: 参数和浮点运算计数不能可靠代表能耗；Transformer模型能耗效率在不同硬件上较一致；发现CIFAR - 10新模型。

Conclusion: 应测量和公布机器学习模型的能量指标，ECOpt有积极环境影响。

Abstract: The ubiquity of machine learning (ML) and the demand for ever-larger models bring an increase in energy consumption and environmental impact. However, little is known about the energy scaling laws in ML, and existing research focuses on training cost -- ignoring the larger cost of inference. Furthermore, tools for measuring the energy consumption of ML do not provide actionable feedback.
  To address these gaps, we developed Energy Consumption Optimiser (ECOpt): a hyperparameter tuner that optimises for energy efficiency and model performance. ECOpt quantifies the trade-off between these metrics as an interpretable Pareto frontier. This enables ML practitioners to make informed decisions about energy cost and environmental impact, while maximising the benefit of their models and complying with new regulations.
  Using ECOpt, we show that parameter and floating-point operation counts can be unreliable proxies for energy consumption, and observe that the energy efficiency of Transformer models for text generation is relatively consistent across hardware. These findings motivate measuring and publishing the energy metrics of ML models. We further show that ECOpt can have a net positive environmental impact and use it to uncover seven models for CIFAR-10 that improve upon the state of the art, when considering accuracy and energy efficiency together.

</details>


### [70] [Constraint- and Score-Based Nonlinear Granger Causality Discovery with Kernels](https://arxiv.org/abs/2601.09579)
*Fiona Murphy,Alessio Benavoli*

Main category: cs.LG

TL;DR: 论文将两种基于核的格兰杰因果关系（GC）方法统一到核主成分回归（KPCR）框架，引入新方法提高因果识别能力，还提出基于GC和$GP_{SIC}$的同期因果识别算法并比较性能。


<details>
  <summary>Details</summary>
Motivation: 利用核方法识别时间序列变量间的非线性因果关系，统一现有方法并改进因果识别，提出更优的同期因果识别算法。

Method: 将两种核GC方法统一到KPCR框架；引入带有平滑信息准则惩罚的高斯过程得分模型；提出基于GC的同期因果识别算法。

Result: 新方法在因果识别上表现优于现有时间序列非线性因果发现方法；比较了所提同期因果识别算法与现有算法的性能。

Conclusion: 基于KPCR的统一方法和基于$GP_{SIC}$的同期因果识别算法能有效提高时间序列因果识别能力。

Abstract: Kernel-based methods are used in the context of Granger Causality to enable the identification of nonlinear causal relationships between time series variables. In this paper, we show that two state of the art kernel-based Granger Causality (GC) approaches can be theoretically unified under the framework of Kernel Principal Component Regression (KPCR), and introduce a method based on this unification, demonstrating that this approach can improve causal identification. Additionally, we introduce a Gaussian Process score-based model with Smooth Information Criterion penalisation on the marginal likelihood, and demonstrate improved performance over existing state of the art time-series nonlinear causal discovery methods. Furthermore, we propose a contemporaneous causal identification algorithm, fully based on GC, using the proposed score-based $GP_{SIC}$ method, and compare its performance to a state of the art contemporaneous time series causal discovery algorithm.

</details>


### [71] [Physics-Guided Counterfactual Explanations for Large-Scale Multivariate Time Series: Application in Scalable and Interpretable SEP Event Prediction](https://arxiv.org/abs/2601.08999)
*Pranjal Patil,Anli Ji,Berkay Aydin*

Main category: cs.LG

TL;DR: 文章提出物理引导反事实解释框架用于太阳高能粒子事件预测，在多方面优于现有基线方法，保证解释的物理合理性和可操作性。


<details>
  <summary>Details</summary>
Motivation: 准确预测太阳高能粒子事件很重要，现有机器学习模型忽视特定领域可行性约束，现有反事实解释方法很少保证物理合理性。

Method: 引入物理引导反事实解释框架，用于时间序列分类任务中生成符合物理原理的反事实解释。

Result: 在太阳高能粒子预测中，该框架使动态时间规整距离减少超80%，提高接近性，生成的反事实解释更稀疏，运行时间比现有基线减少近50%。

Conclusion: 框架生成的反事实解释有效且物理一致，为大数据环境下可扩展的反事实生成奠定基础。

Abstract: Accurate prediction of solar energetic particle events is vital for safeguarding satellites, astronauts, and space-based infrastructure. Modern space weather monitoring generates massive volumes of high-frequency, multivariate time series (MVTS) data from sources such as the Geostationary perational Environmental Satellites (GOES). Machine learning (ML) models trained on this data show strong predictive power, but most existing methods overlook domain-specific feasibility constraints. Counterfactual explanations have emerged as a key tool for improving model interpretability, yet existing approaches rarely enforce physical plausibility. This work introduces a Physics-Guided Counterfactual Explanation framework, a novel method for generating counterfactual explanations in time series classification tasks that remain consistent with underlying physical principles. Applied to solar energetic particles (SEP) forecasting, this framework achieves over 80% reduction in Dynamic Time Warping (DTW) distance increasing the proximity, produces counterfactual explanations with higher sparsity, and reduces runtime by nearly 50% compared to state-of-the-art baselines such as DiCE. Beyond numerical improvements, this framework ensures that generated counterfactual explanations are physically plausible and actionable in scientific domains. In summary, the framework generates counterfactual explanations that are both valid and physically consistent, while laying the foundation for scalable counterfactual generation in big data environments.

</details>


### [72] [Lean Clients, Full Accuracy: Hybrid Zeroth- and First-Order Split Federated Learning](https://arxiv.org/abs/2601.09076)
*Zhoubin Kou,Zihan Chen,Jing Yang,Cong Shen*

Main category: cs.LG

TL;DR: 提出HERON - SFL混合优化框架，减少客户端计算和通信开销，实验表明能降低内存和计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决SFL中客户端计算挑战，减少客户端 - 服务器通信，提升资源效率。

Method: 提出HERON - SFL框架，客户端用零阶（ZO）优化，服务器用一阶（FO）优化，借助辅助网络，ZO更新用扰动前向评估近似局部梯度。

Result: 理论证明收敛率与模型维度无关，实验中ResNet训练和LM微调任务能降低客户端峰值内存达64%，每步计算成本达33%。

Conclusion: HERON - SFL能减少客户端计算和通信开销，扩展资源受限设备可训练模型范围。

Abstract: Split Federated Learning (SFL) enables collaborative training between resource-constrained edge devices and a compute-rich server. Communication overhead is a central issue in SFL and can be mitigated with auxiliary networks. Yet, the fundamental client-side computation challenge remains, as back-propagation requires substantial memory and computation costs, severely limiting the scale of models that edge devices can support. To enable more resource-efficient client computation and reduce the client-server communication, we propose HERON-SFL, a novel hybrid optimization framework that integrates zeroth-order (ZO) optimization for local client training while retaining first-order (FO) optimization on the server. With the assistance of auxiliary networks, ZO updates enable clients to approximate local gradients using perturbed forward-only evaluations per step, eliminating memory-intensive activation caching and avoiding explicit gradient computation in the traditional training process. Leveraging the low effective rank assumption, we theoretically prove that HERON-SFL's convergence rate is independent of model dimensionality, addressing a key scalability concern common to ZO algorithms. Empirically, on ResNet training and language model (LM) fine-tuning tasks, HERON-SFL matches benchmark accuracy while reducing client peak memory by up to 64% and client-side compute cost by up to 33% per step, substantially expanding the range of models that can be trained or adapted on resource-limited devices.

</details>


### [73] [Universal Dynamics of Warmup Stable Decay: understanding WSD beyond Transformers](https://arxiv.org/abs/2601.09000)
*Annalisa Belloni,Lorenzo Noci,Antonio Orvieto*

Main category: cs.LG

TL;DR: 研究比较WSD学习率调度器在类Pythia语言模型和小CNN上Adam优化器的路径，发现不同架构训练信号等相似，暗示损失景观有共同几何特征。


<details>
  <summary>Details</summary>
Motivation: 探究WSD在训练大语言模型时优异表现是否为基于Transformer语言模型特有现象，以获得训练动态新理论见解，用学习率调度器理解景观几何。

Method: 比较WSD路径在类Pythia语言模型和小CNN（用于CIFAR10图像分类）上的情况。

Result: 观察到不同架构中多数训练信号、优化器路径特征和锐度动态在性质上相似。

Conclusion: 不同非凸问题的损失景观有共同几何特征，为高维优化问题几何研究提出新问题。

Abstract: The Warmup Stable Decay (WSD) learning rate scheduler has recently become popular, largely due to its good performance and flexibility when training large language models. It remains an open question whether the remarkable performance of WSD - using a decaying learning rate for only a fraction of training compared to cosine decay - is a phenomenon specific to transformer-based language models that can potentially offer new theoretical insights into their training dynamics. Inspired by the usage of learning rate schedulers as a new lens into understanding landscape geometry (e.g., river valley, connected minima, progressive sharpening), in this work we compare the WSD path of the Adam optimizer on a Pythia-like language model to that of a small CNN trained to classify CIFAR10 images. We observe most training signals, optimizer path features, and sharpness dynamics to be qualitatively similar in such architectures. This consistency points to shared geometric characteristics of the loss landscapes of old and new nonconvex problems, and hints to future research questions around the geometry of high dimensional optimization problems.

</details>


### [74] [Contrastive Geometric Learning Unlocks Unified Structure- and Ligand-Based Drug Design](https://arxiv.org/abs/2601.09693)
*Lisa Schneckenreiter,Sohvi Luukkonen,Lukas Friedrich,Daniel Kuhn,Günter Klambauer*

Main category: cs.LG

TL;DR: 介绍了统一计算药物设计的对比几何学习模型ConGLUDe，它统一结构和配体训练，在多个任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统基于结构和配体的计算药物设计依赖不同数据源和建模假设，限制了大规模联合使用，需统一二者训练。

Method: 引入ConGLUDe模型，耦合几何蛋白质编码器和快速配体编码器，通过对比学习对齐配体与蛋白质表示和候选结合位点，联合训练蛋白质 - 配体复合物和大规模生物活性数据。

Result: ConGLUDe在无结合口袋信息的零样本虚拟筛选中达最先进性能，在目标搜寻任务中大幅超越现有方法，配体条件口袋选择有竞争力。

Conclusion: 统一结构 - 配体训练有优势，ConGLUDe是迈向通用药物发现基础模型的一步。

Abstract: Structure-based and ligand-based computational drug design have traditionally relied on disjoint data sources and modeling assumptions, limiting their joint use at scale. In this work, we introduce Contrastive Geometric Learning for Unified Computational Drug Design (ConGLUDe), a single contrastive geometric model that unifies structure- and ligand-based training. ConGLUDe couples a geometric protein encoder that produces whole-protein representations and implicit embeddings of predicted binding sites with a fast ligand encoder, removing the need for pre-defined pockets. By aligning ligands with both global protein representations and multiple candidate binding sites through contrastive learning, ConGLUDe supports ligand-conditioned pocket prediction in addition to virtual screening and target fishing, while being trained jointly on protein-ligand complexes and large-scale bioactivity data. Across diverse benchmarks, ConGLUDe achieves state-of-the-art zero-shot virtual screening performance in settings where no binding pocket information is provided as input, substantially outperforms existing methods on a challenging target fishing task, and demonstrates competitive ligand-conditioned pocket selection. These results highlight the advantages of unified structure-ligand training and position ConGLUDe as a step toward general-purpose foundation models for drug discovery.

</details>


### [75] [DP-FEDSOFIM: Differentially Private Federated Stochastic Optimization using Regularized Fisher Information Matrix](https://arxiv.org/abs/2601.09166)
*Sidhant R. Nair,Tanmay Sen,Mrinmay Sen*

Main category: cs.LG

TL;DR: 提出DP - FedSOFIM解决DP - FL在严格隐私预算下收敛慢问题，内存和计算复杂度低，实验显示准确率高。


<details>
  <summary>Details</summary>
Motivation: DP - FL在严格隐私预算下收敛慢，现有二阶方法内存消耗大不适用于高维模型。

Method: 提出服务器端二阶优化框架DP - FedSOFIM，利用FIM作为自然梯度预处理器，使用Sherman - Morrison公式高效求逆。

Result: 分析证明服务器端预处理通过后处理定理保持(epsilon, delta) - 差分隐私；在CIFAR - 10上实验显示，相比一阶基线有更高测试准确率。

Conclusion: DP - FedSOFIM在解决DP - FL问题上表现良好，有低内存和计算复杂度，且提高了准确率。

Abstract: Differentially private federated learning (DP-FL) suffers from slow convergence under tight privacy budgets due to the overwhelming noise introduced to preserve privacy. While adaptive optimizers can accelerate convergence, existing second-order methods such as DP-FedNew require O(d^2) memory at each client to maintain local feature covariance matrices, making them impractical for high-dimensional models. We propose DP-FedSOFIM, a server-side second-order optimization framework that leverages the Fisher Information Matrix (FIM) as a natural gradient preconditioner while requiring only O(d) memory per client. By employing the Sherman-Morrison formula for efficient matrix inversion, DP-FedSOFIM achieves O(d) computational complexity per round while maintaining the convergence benefits of second-order methods. Our analysis proves that the server-side preconditioning preserves (epsilon, delta)-differential privacy through the post-processing theorem. Empirical evaluation on CIFAR-10 demonstrates that DP-FedSOFIM achieves superior test accuracy compared to first-order baselines across multiple privacy regimes.

</details>


### [76] [Meta-learning to Address Data Shift in Time Series Classification](https://arxiv.org/abs/2601.09018)
*Samuel Myren,Nidhi Parikh,Natalie Klein*

Main category: cs.LG

TL;DR: 系统比较传统深度学习与元学习算法应对时间序列分类中数据偏移的能力，引入地震基准SeisTask，分析元学习优势及任务多样性影响。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型在数据分布变化时性能易下降，需高成本重新标注和低效再训练，元学习有望解决这些问题。

Method: 系统比较传统深度学习与基于微调及优化的元学习算法，引入SeisTask基准。

Result: 元学习在数据稀缺和小模型架构下适应更快更稳定、过拟合少；数据和模型容量增加时，其优势减小；训练与测试分布对齐而非多样性驱动性能提升。

Conclusion: 系统评估了元学习在数据偏移时何时及为何优于传统深度学习，并提供SeisTask作为时间序列自适应学习研究的基准。

Abstract: Across engineering and scientific domains, traditional deep learning (TDL) models perform well when training and test data share the same distribution. However, the dynamic nature of real-world data, broadly termed \textit{data shift}, renders TDL models prone to rapid performance degradation, requiring costly relabeling and inefficient retraining. Meta-learning, which enables models to adapt quickly to new data with few examples, offers a promising alternative for mitigating these challenges. Here, we systematically compare TDL with fine-tuning and optimization-based meta-learning algorithms to assess their ability to address data shift in time-series classification. We introduce a controlled, task-oriented seismic benchmark (SeisTask) and show that meta-learning typically achieves faster and more stable adaptation with reduced overfitting in data-scarce regimes and smaller model architectures. As data availability and model capacity increase, its advantages diminish, with TDL with fine-tuning performing comparably. Finally, we examine how task diversity influences meta-learning and find that alignment between training and test distributions, rather than diversity alone, drives performance gains. Overall, this work provides a systematic evaluation of when and why meta-learning outperforms TDL under data shift and contributes SeisTask as a benchmark for advancing adaptive learning research in time-series domains.

</details>


### [77] [Layer-Parallel Training for Transformers](https://arxiv.org/abs/2601.09026)
*Shuai Jiang,Marc Salvado,Eric C. Cyr,Alena Kopaničáková,Rolf Krause,Jacob B. Schroder*

Main category: cs.LG

TL;DR: 提出多级层并行方法训练Transformer，实现层维度并行加速，解决梯度误差问题，实验有较好结果。


<details>
  <summary>Details</summary>
Motivation: 提高Transformer训练在网络深度增加时的并行可扩展性，适应大型基础模型需求。

Method: 通过神经ODE公式化Transformer，应用多级并行时间算法进行训练前后向传播，开发检测关键转变的算法。

Result: 在BERT、GPT2、ViT和机器翻译架构中实现并行加速，准确性与串行预训练相当，微调不受影响。

Conclusion: 多级层并行方法能加速训练，开发的算法可解决梯度误差问题。

Abstract: We present a new training methodology for transformers using a multilevel, layer-parallel approach. Through a neural ODE formulation of transformers, our application of a multilevel parallel-in-time algorithm for the forward and backpropagation phases of training achieves parallel acceleration over the layer dimension. This dramatically enhances parallel scalability as the network depth increases, which is particularly useful for increasingly large foundational models. However, achieving this introduces errors that cause systematic bias in the gradients, which in turn reduces convergence when closer to the minima. We develop an algorithm to detect this critical transition and either switch to serial training or systematically increase the accuracy of layer-parallel training. Results, including BERT, GPT2, ViT, and machine translation architectures, demonstrate parallel-acceleration as well as accuracy commensurate with serial pre-training while fine-tuning is unaffected.

</details>


### [78] [Deep Incomplete Multi-View Clustering via Hierarchical Imputation and Alignment](https://arxiv.org/abs/2601.09051)
*Yiming Du,Ziyu Wang,Jian Li,Rui Ning,Lusi Li*

Main category: cs.LG

TL;DR: 提出DIMVC - HIA框架解决不完全多视图聚类问题，实验显示其在不同缺失水平下性能优越。


<details>
  <summary>Details</summary>
Motivation: 解决不完全多视图聚类中准确填补缺失视图、保持视图间语义一致性和簇内紧凑性的挑战。

Method: 提出DIMVC - HIA框架，包含视图特定自编码器、层次化填补模块、基于能量的语义对齐模块和对比分配对齐模块。

Result: 在基准测试中，该框架在不同缺失水平下表现优越。

Conclusion: DIMVC - HIA框架能有效解决不完全多视图聚类问题，有良好性能。

Abstract: Incomplete multi-view clustering (IMVC) aims to discover shared cluster structures from multi-view data with partial observations. The core challenges lie in accurately imputing missing views without introducing bias, while maintaining semantic consistency across views and compactness within clusters. To address these challenges, we propose DIMVC-HIA, a novel deep IMVC framework that integrates hierarchical imputation and alignment with four key components: (1) view-specific autoencoders for latent feature extraction, coupled with a view-shared clustering predictor to produce soft cluster assignments; (2) a hierarchical imputation module that first estimates missing cluster assignments based on cross-view contrastive similarity, and then reconstructs missing features using intra-view, intra-cluster statistics; (3) an energy-based semantic alignment module, which promotes intra-cluster compactness by minimizing energy variance around low-energy cluster anchors; and (4) a contrastive assignment alignment module, which enhances cross-view consistency and encourages confident, well-separated cluster predictions. Experiments on benchmarks demonstrate that our framework achieves superior performance under varying levels of missingness.

</details>


### [79] [Resolving Predictive Multiplicity for the Rashomon Set](https://arxiv.org/abs/2601.09071)
*Parian Haghighat,Hadis Anahideh,Cynthia Rudin*

Main category: cs.LG

TL;DR: 文章针对预测多样性导致的不一致问题，提出三种减少不一致性的方法，实验表明方法能降低分歧指标并保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 预测多样性导致预测不一致，破坏了高风险应用中所需的预测一致性，影响信任。

Method: 提出三种方法，分别是异常值修正、局部修补和成对协调，可单独或组合使用。

Result: 在多个数据集的实验中，方法能降低分歧指标，同时保持有竞争力的准确性。

Conclusion: 提出的三种方法可有效减少预测不一致性，且调和后的预测可提炼为单一可解释模型用于实际部署。

Abstract: The existence of multiple, equally accurate models for a given predictive task leads to predictive multiplicity, where a ``Rashomon set'' of models achieve similar accuracy but diverges in their individual predictions. This inconsistency undermines trust in high-stakes applications where we want consistent predictions. We propose three approaches to reduce inconsistency among predictions for the members of the Rashomon set. The first approach is \textbf{outlier correction}. An outlier has a label that none of the good models are capable of predicting correctly. Outliers can cause the Rashomon set to have high variance predictions in a local area, so fixing them can lower variance. Our second approach is local patching. In a local region around a test point, models may disagree with each other because some of them are biased. We can detect and fix such biases using a validation set, which also reduces multiplicity. Our third approach is pairwise reconciliation, where we find pairs of models that disagree on a region around the test point. We modify predictions that disagree, making them less biased. These three approaches can be used together or separately, and they each have distinct advantages. The reconciled predictions can then be distilled into a single interpretable model for real-world deployment. In experiments across multiple datasets, our methods reduce disagreement metrics while maintaining competitive accuracy.

</details>


### [80] [SRT: Accelerating Reinforcement Learning via Speculative Rollout with Tree-Structured Cache](https://arxiv.org/abs/2601.09083)
*Chi-Chih Chang,Siqi Zhu,Zhichen Zeng,Haibin Lin,Jiaxuan You,Mohamed S. Abdelfattah,Ziheng Jiang,Xuehai Qian*

Main category: cs.LG

TL;DR: 提出SRT方法加速语言模型的策略强化学习，降低生成和步骤延迟与推理成本。


<details>
  <summary>Details</summary>
Motivation: 在不牺牲分布正确性前提下，加速语言模型的策略强化学习。

Method: 将之前生成的内容存储在提示树结构缓存中，当前策略利用此树进行推测解码，在线更新树并在GPU空闲时预生成。

Result: 集成到标准RL管道和多轮设置中，减少生成和步骤延迟，降低推理成本，在rollout时实现最高2.08倍的时间加速。

Conclusion: SRT能有效加速语言模型的策略强化学习。

Abstract: We present Speculative Rollout with Tree-Structured Cache (SRT), a simple, model-free approach to accelerate on-policy reinforcement learning (RL) for language models without sacrificing distributional correctness. SRT exploits the empirical similarity of rollouts for the same prompt across training steps by storing previously generated continuations in a per-prompt tree-structured cache. During generation, the current policy uses this tree as the draft model for performing speculative decoding. To keep the cache fresh and improve draft model quality, SRT updates trees online from ongoing rollouts and proactively performs run-ahead generation during idle GPU bubbles. Integrated into standard RL pipelines (\textit{e.g.}, PPO, GRPO and DAPO) and multi-turn settings, SRT consistently reduces generation and step latency and lowers per-token inference cost, achieving up to 2.08x wall-clock time speedup during rollout.

</details>


### [81] [MMR-GRPO: Accelerating GRPO-Style Training through Diversity-Aware Reward Reweighting](https://arxiv.org/abs/2601.09085)
*Kangda Wei,Ruihong Huang*

Main category: cs.LG

TL;DR: 提出MMR - GRPO方法，在数学推理模型训练中减少训练步骤与时间，且性能相当。


<details>
  <summary>Details</summary>
Motivation: GRPO训练数学推理模型计算成本高，虽有工作减少训练步骤但整体时间未减甚至增加。

Method: 提出MMR - GRPO，集成最大边际相关性根据完成多样性重新权衡奖励。

Result: 在不同模型大小、GRPO变体和数学推理基准测试中，平均减少47.9%训练步骤和70.2%时钟时间，达到相当的峰值性能。

Conclusion: MMR - GRPO能在数学推理模型训练中有效减少训练步骤和时间，且性能表现良好，代码等将开源。

Abstract: Group Relative Policy Optimization (GRPO) has become a standard approach for training mathematical reasoning models; however, its reliance on multiple completions per prompt makes training computationally expensive. Although recent work has reduced the number of training steps required to reach peak performance, the overall wall-clock training time often remains unchanged or even increases due to higher per-step cost. We propose MMR-GRPO, which integrates Maximal Marginal Relevance to reweigh rewards based on completion diversity. Our key insight is that semantically redundant completions contribute limited marginal learning signal; prioritizing diverse solutions yields more informative updates and accelerates convergence. Extensive evaluations across three model sizes (1.5B, 7B, 8B), three GRPO variants, and five mathematical reasoning benchmarks show that MMR-GRPO achieves comparable peak performance while requiring on average 47.9% fewer training steps and 70.2% less wall-clock time. These gains are consistent across models, methods, and benchmarks. We will release our code, trained models, and experimental protocols.

</details>


### [82] [Distribution-Aligned Sequence Distillation for Superior Long-CoT Reasoning](https://arxiv.org/abs/2601.09088)
*Shaotian Yan,Kaiyuan Liu,Chen Shen,Bing Wang,Sinan Fan,Jun Zhang,Yue Wu,Zheng Wang,Jieping Ye*

Main category: cs.LG

TL;DR: 介绍轻量级开源推理模型 DASD - 4B - Thinking，指出当前蒸馏范式不足，提出改进方法，该模型用少量样本取得佳绩并开源。


<details>
  <summary>Details</summary>
Motivation: 当前基于 SFT 的序列级蒸馏方法主要从 SFT 视角出发，未充分发挥蒸馏核心原则，存在多种局限性，需改进。

Method: 提出一系列方法创新，形成增强的序列级蒸馏训练管道。

Result: DASD - 4B - Thinking 在数学、科学推理和代码生成等基准测试中达 SOTA 性能，用 448K 训练样本就获有竞争力结果。

Conclusion: 现有蒸馏过程缺乏明确师生交互，DASD - 4B - Thinking 模型及数据集开源可支持社区研究。

Abstract: In this report, we introduce DASD-4B-Thinking, a lightweight yet highly capable, fully open-source reasoning model. It achieves SOTA performance among open-source models of comparable scale across challenging benchmarks in mathematics, scientific reasoning, and code generation -- even outperforming several larger models. We begin by critically reexamining a widely adopted distillation paradigm in the community: SFT on teacher-generated responses, also known as sequence-level distillation. Although a series of recent works following this scheme have demonstrated remarkable efficiency and strong empirical performance, they are primarily grounded in the SFT perspective. Consequently, these approaches focus predominantly on designing heuristic rules for SFT data filtering, while largely overlooking the core principle of distillation itself -- enabling the student model to learn the teacher's full output distribution so as to inherit its generalization capability. Specifically, we identify three critical limitations in current practice: i) Inadequate representation of the teacher's sequence-level distribution; ii) Misalignment between the teacher's output distribution and the student's learning capacity; and iii) Exposure bias arising from teacher-forced training versus autoregressive inference. In summary, these shortcomings reflect a systemic absence of explicit teacher-student interaction throughout the distillation process, leaving the essence of distillation underexploited. To address these issues, we propose several methodological innovations that collectively form an enhanced sequence-level distillation training pipeline. Remarkably, DASD-4B-Thinking obtains competitive results using only 448K training samples -- an order of magnitude fewer than those employed by most existing open-source efforts. To support community research, we publicly release our models and the training dataset.

</details>


### [83] [Hidden States as Early Signals: Step-level Trace Evaluation and Pruning for Efficient Test-Time Scaling](https://arxiv.org/abs/2601.09093)
*Zhixiang Liang,Beichen Huang,Zheng Wang,Minjia Zhang*

Main category: cs.LG

TL;DR: 提出STEP框架加速大语言模型推理，减少推理延迟并提升准确率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型通过多轨迹生成增强推理能力时存在计算量大、延迟高问题，且已有加速方法信号不可靠。

Method: 提出STEP框架，用隐藏状态评估推理步骤，动态修剪无前景轨迹；训练轻量级步骤评分器，设计GPU内存感知修剪策略。

Result: 在多个推理基准测试中，STEP比自一致性平均减少45%-70%的端到端推理延迟，同时提高推理准确率。

Conclusion: STEP框架有效解决了大语言模型推理加速问题，减少延迟并提升准确率。

Abstract: Large Language Models (LLMs) can enhance reasoning capabilities through test-time scaling by generating multiple traces. However, the combination of lengthy reasoning traces with multiple sampling introduces substantial computation and high end-to-end latency. Prior work on accelerating this process has relied on similarity-based or confidence-based pruning, but these signals do not reliably indicate trace quality. To address these limitations, we propose STEP: Step-level Trace Evaluation and Pruning, a novel pruning framework that evaluates reasoning steps using hidden states and dynamically prunes unpromising traces during generation. We train a lightweight step scorer to estimate trace quality, and design a GPU memory-aware pruning strategy that triggers pruning as the GPU memory is saturated by KV cache to reduce end-to-end latency. Experiments across challenging reasoning benchmarks demonstrate that STEP reduces end-to-end inference latency by 45%-70% on average compared to self-consistency while also improving reasoning accuracy. Our code is released at: https://github.com/Supercomputing-System-AI-Lab/STEP

</details>


### [84] [Comparative Assessment of Concrete Compressive Strength Prediction at Industry Scale Using Embedding-based Neural Networks, Transformers, and Traditional Machine Learning Approaches](https://arxiv.org/abs/2601.09096)
*Md Asiful Islam,Md Ahmed Al Muzaddid,Afia Jahin Prema,Sreenath Reddy Vuske*

Main category: cs.LG

TL;DR: 文章利用约7万条抗压强度测试记录的行业规模数据集评估对比多种预测方法，发现基于嵌入的神经网络表现最佳，有望用于大规模建筑自动化质量控制。


<details>
  <summary>Details</summary>
Motivation: 混凝土抗压强度受多种因素影响，可靠预测具有挑战，人工智能发展使数据驱动建模框架可支持建筑质量控制自动决策。

Method: 利用行业规模数据集，评估对比线性回归、决策树、随机森林、基于Transformer的神经网络和基于嵌入的神经网络等多种预测方法，模型纳入关键混合设计和施工变量。

Result: 基于嵌入的神经网络始终优于传统机器学习和基于Transformer的模型，28天预测平均误差约2.5%。

Conclusion: 基于嵌入的学习框架有潜力在大规模建筑作业中实现自动化、数据驱动的质量控制和决策支持。

Abstract: Concrete is the most widely used construction material worldwide; however, reliable prediction of compressive strength remains challenging due to material heterogeneity, variable mix proportions, and sensitivity to field and environmental conditions. Recent advances in artificial intelligence enable data-driven modeling frameworks capable of supporting automated decision-making in construction quality control. This study leverages an industry-scale dataset consisting of approximately 70,000 compressive strength test records to evaluate and compare multiple predictive approaches, including linear regression, decision trees, random forests, transformer-based neural networks, and embedding-based neural networks. The models incorporate key mixture design and placement variables such as water cement ratio, cementitious material content, slump, air content, temperature, and placement conditions. Results indicate that the embedding-based neural network consistently outperforms traditional machine learning and transformer-based models, achieving a mean 28-day prediction error of approximately 2.5%. This level of accuracy is comparable to routine laboratory testing variability, demonstrating the potential of embedding-based learning frameworks to enable automated, data-driven quality control and decision support in large-scale construction operations.

</details>


### [85] [Enhancing Imbalanced Electrocardiogram Classification: A Novel Approach Integrating Data Augmentation through Wavelet Transform and Interclass Fusion](https://arxiv.org/abs/2601.09103)
*Haijian Shao,Wei Liu,Xing Deng,Daze Lu*

Main category: cs.LG

TL;DR: 本文提出增强的ECG分类器，用小波变换特征融合解决类不平衡和噪声问题，在CPSC 2018数据集上有高识别准确率。


<details>
  <summary>Details</summary>
Motivation: 不平衡的心电图数据阻碍深度学习心电图分类，现有生成和过采样技术效果无共识，采集方法和场景引入噪声。

Method: 提出基于小波变换的特征融合，生成训练和测试特征库，将原始数据与特征库合并。

Result: ECG模型对多种类型的识别准确率最高达99%等，平均准确率在92% - 98%，数据融合方法在CPSC 2018数据集上分类准确率超已知算法。

Conclusion: 提出的方法能有效解决心电图分析中的类不平衡和噪声问题，提高分类准确率。

Abstract: Imbalanced electrocardiogram (ECG) data hampers the efficacy and resilience of algorithms in the automated processing and interpretation of cardiovascular diagnostic information, which in turn impedes deep learning-based ECG classification. Notably, certain cardiac conditions that are infrequently encountered are disproportionately underrepresented in these datasets. Although algorithmic generation and oversampling of specific ECG signal types can mitigate class skew, there is a lack of consensus regarding the effectiveness of such techniques in ECG classification. Furthermore, the methodologies and scenarios of ECG acquisition introduce noise, further complicating the processing of ECG data. This paper presents a significantly enhanced ECG classifier that simultaneously addresses both class imbalance and noise-related challenges in ECG analysis, as observed in the CPSC 2018 dataset. Specifically, we propose the application of feature fusion based on the wavelet transform, with a focus on wavelet transform-based interclass fusion, to generate the training feature library and the test set feature library. Subsequently, the original training and test data are amalgamated with their respective feature databases, resulting in more balanced training and test datasets. Employing this approach, our ECG model achieves recognition accuracies of up to 99%, 98%, 97%, 98%, 96%, 92%, and 93% for Normal, AF, I-AVB, LBBB, RBBB, PAC, PVC, STD, and STE, respectively. Furthermore, the average recognition accuracy for these categories ranges between 92\% and 98\%. Notably, our proposed data fusion methodology surpasses any known algorithms in terms of ECG classification accuracy in the CPSC 2018 dataset.

</details>


### [86] [EvasionBench: Detecting Evasive Answers in Financial Q&A via Multi-Model Consensus and LLM-as-Judge](https://arxiv.org/abs/2601.09142)
*Shijian Ma,Yan Lin,Yi Yang*

Main category: cs.LG

TL;DR: 引入EvasionBench数据集，提出多模型标注框架，训练模型Eva - 4B取得较好效果。


<details>
  <summary>Details</summary>
Motivation: 检测财报电话会议中的回避回答对财务透明度至关重要，但缺乏大规模基准数据集阻碍了进展。

Method: 引入包含30000个训练样本和1000个人工标注测试样本的EvasionBench，采用多模型标注框架，挖掘边界案例，用裁判解决标签冲突。

Result: 多模型标注框架比单模型蒸馏效果好2.4%；裁判解决的样本虽训练损失高但提高了泛化能力；Eva - 4B准确率达81.3%，比其基础模型高25个百分点，推理成本低。

Conclusion: 挖掘不同模型间的分歧作为隐式正则化是有效的，Eva - 4B在检测回避回答上表现良好。

Abstract: Detecting evasive answers in earnings calls is critical for financial transparency, yet progress is hindered by the lack of large-scale benchmarks. We introduce EvasionBench, comprising 30,000 training samples and 1,000 human-annotated test samples (Cohen's Kappa 0.835) across three evasion levels. Our key contribution is a multi-model annotation framework leveraging a core insight: disagreement between frontier LLMs signals hard examples most valuable for training. We mine boundary cases where two strong annotators conflict, using a judge to resolve labels. This approach outperforms single-model distillation by 2.4 percent, with judge-resolved samples improving generalization despite higher training loss (0.421 vs 0.393) - evidence that disagreement mining acts as implicit regularization. Our trained model Eva-4B (4B parameters) achieves 81.3 percent accuracy, outperforming its base by 25 percentage points and approaching frontier LLM performance at a fraction of inference cost.

</details>


### [87] [Discrete Solution Operator Learning for Geometry-Dependent PDEs](https://arxiv.org/abs/2601.09143)
*Jinshuai Bai,Haolin Li,Zahra Sharif Khodaei,M. H. Aliabadi,YuanTong Gu,Xi-Qiao Feng*

Main category: cs.LG

TL;DR: 本文提出离散求解算子学习（DiSOL）方法，可在几何主导问题中稳定准确预测。


<details>
  <summary>Details</summary>
Motivation: 在工程场景中，几何变化导致离散结构变化，打破了神经网络算子学习的光滑变化前提，需要新方法。

Method: 引入DiSOL，将求解器分解为可学习阶段，包括局部贡献编码、多尺度组装和嵌入式网格上的隐式解重建。

Result: 在多个几何相关问题上，DiSOL在分布内和分布外几何条件下都能产生稳定准确的预测。

Conclusion: 在几何主导场景中需要过程算子表示，离散求解算子学习是科学机器学习中独特且互补的方向。

Abstract: Neural operator learning accelerates PDE solution by approximating operators as mappings between continuous function spaces. Yet in many engineering settings, varying geometry induces discrete structural changes, including topological changes, abrupt changes in boundary conditions or boundary types, and changes in the effective computational domain, which break the smooth-variation premise. Here we introduce Discrete Solution Operator Learning (DiSOL), a complementary paradigm that learns discrete solution procedures rather than continuous function-space operators. DiSOL factorizes the solver into learnable stages that mirror classical discretizations: local contribution encoding, multiscale assembly, and implicit solution reconstruction on an embedded grid, thereby preserving procedure-level consistency while adapting to geometry-dependent discrete structures. Across geometry-dependent Poisson, advection-diffusion, linear elasticity, as well as spatiotemporal heat-conduction problems, DiSOL produces stable and accurate predictions under both in-distribution and strongly out-of-distribution geometries, including discontinuous boundaries and topological changes. These results highlight the need for procedural operator representations in geometry-dominated regimes and position discrete solution operator learning as a distinct, complementary direction in scientific machine learning.

</details>


### [88] [Interpretable Probability Estimation with LLMs via Shapley Reconstruction](https://arxiv.org/abs/2601.09151)
*Yang Nan,Qihao Wen,Jiahao Wang,Pengfei He,Ravi Tandon,Yong Ge,Han Xu*

Main category: cs.LG

TL;DR: 本文提出PRISM框架，解决大模型概率估计透明度和精确性问题并提升性能


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在直接概率估计时输出有噪音、过程不透明的问题，为支持多领域智能决策需解决这些问题

Method: 提出PRISM框架，用Shapley值量化每个输入因素的边际贡献来分解大模型预测，再聚合这些贡献得到校准后的最终估计

Result: 实验表明，在金融、医疗和农业等多个领域，PRISM比直接提示和其他基线方法提高了预测准确性

Conclusion: PRISM在提升预测性能的同时，提供了透明的预测流程，有助于建立基于大模型的决策支持系统的信任

Abstract: Large Language Models (LLMs) demonstrate potential to estimate the probability of uncertain events, by leveraging their extensive knowledge and reasoning capabilities. This ability can be applied to support intelligent decision-making across diverse fields, such as financial forecasting and preventive healthcare. However, directly prompting LLMs for probability estimation faces significant challenges: their outputs are often noisy, and the underlying predicting process is opaque. In this paper, we propose PRISM: Probability Reconstruction via Shapley Measures, a framework that brings transparency and precision to LLM-based probability estimation. PRISM decomposes an LLM's prediction by quantifying the marginal contribution of each input factor using Shapley values. These factor-level contributions are then aggregated to reconstruct a calibrated final estimate. In our experiments, we demonstrate PRISM improves predictive accuracy over direct prompting and other baselines, across multiple domains including finance, healthcare, and agriculture. Beyond performance, PRISM provides a transparent prediction pipeline: our case studies visualize how individual factors shape the final estimate, helping build trust in LLM-based decision support systems.

</details>


### [89] [KTCF: Actionable Recourse in Knowledge Tracing via Counterfactual Explanations for Education](https://arxiv.org/abs/2601.09156)
*Woojin Kim,Changkwon Lee,Hyeoncheol Kim*

Main category: cs.LG

TL;DR: 本文探讨反事实解释作为知识追踪（KT）的可解释人工智能（XAI）与教育的联系，提出KTCF方法及后处理方案，实验证明其性能优越，显示反事实在教育中应用潜力。


<details>
  <summary>Details</summary>
Motivation: 利用人工智能改善教学，借助知识追踪进行学生建模，探索反事实解释作为KT的XAI与教育的联系。

Method: 提出考虑知识概念关系的KTCF反事实解释生成方法和将反事实解释转化为教育指令序列的后处理方案。

Result: KTCF方法在大规模教育数据集上优于现有方法，指标提升5.7% - 34%，后处理方案得到的教育指令有助于减轻学习负担。

Conclusion: 反事实解释有潜力推动人工智能在教育中的负责任和实际应用，未来KT的XAI研究可结合教育理论和以利益相关者为中心的方法。

Abstract: Using Artificial Intelligence to improve teaching and learning benefits greater adaptivity and scalability in education. Knowledge Tracing (KT) is recognized for student modeling task due to its superior performance and application potential in education. To this end, we conceptualize and investigate counterfactual explanation as the connection from XAI for KT to education. Counterfactual explanations offer actionable recourse, are inherently causal and local, and easy for educational stakeholders to understand who are often non-experts. We propose KTCF, a counterfactual explanation generation method for KT that accounts for knowledge concept relationships, and a post-processing scheme that converts a counterfactual explanation into a sequence of educational instructions. We experiment on a large-scale educational dataset and show our KTCF method achieves superior and robust performance over existing methods, with improvements ranging from 5.7% to 34% across metrics. Additionally, we provide a qualitative evaluation of our post-processing scheme, demonstrating that the resulting educational instructions help in reducing large study burden. We show that counterfactuals have the potential to advance the responsible and practical use of AI in education. Future works on XAI for KT may benefit from educationally grounded conceptualization and developing stakeholder-centered methods.

</details>


### [90] [Multi-Teacher Ensemble Distillation: A Mathematical Framework for Probability-Domain Knowledge Aggregation](https://arxiv.org/abs/2601.09165)
*Aaron R. Flouro,Shawn P. Chadwick*

Main category: cs.LG

TL;DR: 基于Sparse - KD概率域蒸馏框架，提出多教师集成知识蒸馏的公理、算子理论框架，给出算子性质和保证，为多教师蒸馏提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 为多教师集成知识蒸馏建立理论框架，解决缺乏通用公理和理论保证的问题。

Method: 定义五个核心公理来规范有效的知识聚合算子，证明满足公理的算子族的存在性和非唯一性。

Result: 建立了算子无关保证，证明多教师聚合可减少随机方差和系统监督偏差，给出Jensen型边界、对数损失保证和安全衰减性质，对线性算子有方差减少结果。

Conclusion: 该框架为来自不同前沿模型的多教师蒸馏提供了理论基础，且有多种有效实现策略。

Abstract: Building on the probability-domain distillation framework of Sparse-KD, we develop an axiomatic, operator-theoretic framework for multi-teacher ensemble knowledge distillation. Rather than prescribing a specific aggregation formula, we define five core axioms governing valid knowledge aggregation operators, encompassing convexity, positivity, continuity, weight monotonicity, and temperature coherence. We prove the existence and non-uniqueness of operator families satisfying these axioms, establishing that multiple distinct aggregation mechanisms conform to the same foundational principles.
  Within this framework, we establish operator-agnostic guarantees showing that multi-teacher aggregation reduces both stochastic variance and systematic supervisory bias under heterogeneous teachers, while providing Jensen-type bounds, log-loss guarantees, and safety attenuation properties. For aggregation operators linear in teacher weights, we further establish classical ensemble variance-reduction results under standard independence assumptions, with extensions to correlated-error regimes. The framework provides theoretical grounding for multi-teacher distillation from diverse frontier models while admitting multiple valid implementation strategies.

</details>


### [91] [BalDRO: A Distributionally Robust Optimization based Framework for Large Language Model Unlearning](https://arxiv.org/abs/2601.09172)
*Pengyang Shao,Naixin Zhai,Lei Chen,Yonghui Yang,Fengbin Zhu,Xun Yang,Meng Wang*

Main category: cs.LG

TL;DR: 提出BalDRO框架解决大语言模型遗忘学习中样本不平衡问题，实验显示其优于现有方法并开源代码


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成内容增多，遗忘学习对网络治理重要，但遗忘集样本存在不平衡问题

Method: 提出BalDRO框架，将遗忘学习表述为min - sup过程，并给出BalDRO - G和BalDRO - DV两个高效变种

Result: 在TOFU和MUSE上实验表明，BalDRO在遗忘质量和模型效用上显著优于现有方法

Conclusion: BalDRO是解决大语言模型遗忘学习样本不平衡问题的有效框架

Abstract: As Large Language Models (LLMs) increasingly shape online content, removing targeted information from well-trained LLMs (also known as LLM unlearning) has become critical for web governance. A key challenge lies in sample-wise imbalance within the forget set: different samples exhibit widely varying unlearning difficulty, leading to asynchronous forgetting where some knowledge remains insufficiently erased while others become over-forgotten. To address this, we propose BalDRO, a novel and efficient framework for balanced LLM unlearning. BalDRO formulates unlearning as a min-sup process: an inner step identifies a worst-case data distribution that emphasizes hard-to-unlearn samples, while an outer step updates model parameters under this distribution. We instantiate BalDRO via two efficient variants: BalDRO-G, a discrete GroupDRO-based approximation focusing on high-loss subsets, and BalDRO-DV, a continuous Donsker-Varadhan dual method enabling smooth adaptive weighting within standard training pipelines. Experiments on TOFU and MUSE show that BalDRO significantly improves both forgetting quality and model utility over existing methods, and we release code for reproducibility.

</details>


### [92] [$D^2Prune$: Sparsifying Large Language Models via Dual Taylor Expansion and Attention Distribution Awareness](https://arxiv.org/abs/2601.09176)
*Lang Xiong,Ning Liu,Ao Ren,Yuheng Bai,Haining Fang,BinYan Zhang,Zhe Jiang,Yujuan Tan,Duo Liu*

Main category: cs.LG

TL;DR: 本文提出D^2Prune剪枝方法解决大语言模型计算量大的部署问题，实验表明其性能优于SOTA方法，还能推广到视觉模型。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型剪枝方法忽略校准数据和测试数据间激活分布变化以及注意力模块激活的长尾分布特征，导致误差估计不准。

Method: 提出基于双泰勒展开的方法精确估计误差以辅助剪枝；提出注意力感知动态更新策略保留长尾注意力模式。

Result: D^2Prune在多种大语言模型上性能超SOTA方法；动态注意力更新机制能推广到基于ViT的视觉模型，在ImageNet - 1K上实现更高精度。

Conclusion: D^2Prune是一种有效的大语言模型剪枝方法，且具有良好的泛化能力。

Abstract: Large language models (LLMs) face significant deployment challenges due to their massive computational demands. % While pruning offers a promising compression solution, existing methods suffer from two critical limitations: (1) They neglect activation distribution shifts between calibration data and test data, resulting in inaccurate error estimations; (2) They overlook the long-tail distribution characteristics of activations in the attention module. To address these limitations, this paper proposes a novel pruning method, $D^2Prune$. First, we propose a dual Taylor expansion-based method that jointly models weight and activation perturbations for precise error estimation, leading to precise pruning mask selection and weight updating and facilitating error minimization during pruning. % Second, we propose an attention-aware dynamic update strategy that preserves the long-tail attention pattern by jointly minimizing the KL divergence of attention distributions and the reconstruction error. Extensive experiments show that $D^2Prune$ consistently outperforms SOTA methods across various LLMs (e.g., OPT-125M, LLaMA2/3, and Qwen3). Moreover, the dynamic attention update mechanism also generalizes well to ViT-based vision models like DeiT, achieving superior accuracy on ImageNet-1K.

</details>


### [93] [From Hawkes Processes to Attention: Time-Modulated Mechanisms for Event Sequences](https://arxiv.org/abs/2601.09220)
*Xinzi Tan,Kejian Zhang,Junhan Yu,Doudou Zhou*

Main category: cs.LG

TL;DR: 本文提出用于标记时间点过程（MTPP）的Hawkes Attention算子，实验表明性能优于基线，还可用于时间序列预测。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的方法在处理MTPP时，仅通过位置编码注入时间信息，依赖共享或参数化衰减结构，限制了捕捉异构和特定类型时间效应的能力。

Method: 从多元Hawkes过程理论中推导出Hawkes Attention算子，用可学习的特定类型神经核调制查询、键和值投影，替换传统注意力相应部分。

Result: 所提方法与基线相比取得了更好的性能。

Conclusion: Hawkes Attention统一了事件时间和内容交互，能从数据中学习与时间相关的行为和特定类型的激发模式，且可应用于特定时间结构。

Abstract: Marked Temporal Point Processes (MTPPs) arise naturally in medical, social, commercial, and financial domains. However, existing Transformer-based methods mostly inject temporal information only via positional encodings, relying on shared or parametric decay structures, which limits their ability to capture heterogeneous and type-specific temporal effects. Inspired by this observation, we derive a novel attention operator called Hawkes Attention from the multivariate Hawkes process theory for MTPP, using learnable per-type neural kernels to modulate query, key and value projections, thereby replacing the corresponding parts in the traditional attention. Benefited from the design, Hawkes Attention unifies event timing and content interaction, learning both the time-relevant behavior and type-specific excitation patterns from the data. The experimental results show that our method achieves better performance compared to the baselines. In addition to the general MTPP, our attention mechanism can also be easily applied to specific temporal structures, such as time series forecasting.

</details>


### [94] [GIFT: Unlocking Global Optimality in Post-Training via Finite-Temperature Gibbs Initialization](https://arxiv.org/abs/2601.09233)
*Zhengyang Zhao,Lu Ma,Yizhen Jiang,Xiaochen Ma,Zimo Meng,Chengyu Shen,Lexiang Tang,Haoze Sun,Peng Pei,Wentao Zhang*

Main category: cs.LG

TL;DR: 文章指出大推理模型现有训练范式问题，提出GIFT方法，实验显示其优于标准SFT及其他基线。


<details>
  <summary>Details</summary>
Motivation: 现有大推理模型的训练范式（SFT+RL）存在内在优化不匹配问题，SFT的刚性监督导致分布崩溃，耗尽后续RL所需探索空间。

Method: 在统一的预训练框架内重新构建SFT，提出Gibbs Initialization with Finite Temperature (GIFT)方法，将监督作为有限温度能量势，建立分布桥梁确保目标一致性。

Result: 实验表明，在进行RL初始化时，GIFT明显优于标准SFT和其他竞争基线。

Conclusion: GIFT为后训练实现全局最优提供了数学上有原则的途径。

Abstract: The prevailing post-training paradigm for Large Reasoning Models (LRMs)--Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL)--suffers from an intrinsic optimization mismatch: the rigid supervision inherent in SFT induces distributional collapse, thereby exhausting the exploration space necessary for subsequent RL. In this paper, we reformulate SFT within a unified post-training framework and propose Gibbs Initialization with Finite Temperature (GIFT). We characterize standard SFT as a degenerate zero-temperature limit that suppresses base priors. Conversely, GIFT incorporates supervision as a finite-temperature energy potential, establishing a distributional bridge that ensures objective consistency throughout the post-training pipeline. Our experiments demonstrate that GIFT significantly outperforms standard SFT and other competitive baselines when utilized for RL initialization, providing a mathematically principled pathway toward achieving global optimality in post-training. Our code is available at https://github.com/zzy1127/GIFT.

</details>


### [95] [Reward Learning through Ranking Mean Squared Error](https://arxiv.org/abs/2601.09236)
*Chaitanya Kharyal,Calarina Muslimani,Matthew E. Taylor*

Main category: cs.LG

TL;DR: 提出新的基于评分的强化学习方法R4，有理论保证，在机器人运动基准测试中表现好且所需反馈少。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中奖励设计瓶颈问题，基于从人类反馈（评分）学习奖励函数的范式开展研究。

Method: 引入R4方法，采用rMSE损失，从轨迹 - 评分对数据集学习，通过可微排序算子（软排名）对轨迹回报排序并优化损失。

Result: 在机器人运动基准测试中，R4表现与现有方法相当或更优，且所需反馈显著减少。

Conclusion: R4方法在基于评分的强化学习中有优势，有理论保证且实践效果好。

Abstract: Reward design remains a significant bottleneck in applying reinforcement learning (RL) to real-world problems. A popular alternative is reward learning, where reward functions are inferred from human feedback rather than manually specified. Recent work has proposed learning reward functions from human feedback in the form of ratings, rather than traditional binary preferences, enabling richer and potentially less cognitively demanding supervision. Building on this paradigm, we introduce a new rating-based RL method, Ranked Return Regression for RL (R4). At its core, R4 employs a novel ranking mean squared error (rMSE) loss, which treats teacher-provided ratings as ordinal targets. Our approach learns from a dataset of trajectory-rating pairs, where each trajectory is labeled with a discrete rating (e.g., "bad," "neutral," "good"). At each training step, we sample a set of trajectories, predict their returns, and rank them using a differentiable sorting operator (soft ranks). We then optimize a mean squared error loss between the resulting soft ranks and the teacher's ratings. Unlike prior rating-based approaches, R4 offers formal guarantees: its solution set is provably minimal and complete under mild assumptions. Empirically, using simulated human feedback, we demonstrate that R4 consistently matches or outperforms existing rating and preference-based RL methods on robotic locomotion benchmarks from OpenAI Gym and the DeepMind Control Suite, while requiring significantly less feedback.

</details>


### [96] [XLinear: A Lightweight and Accurate MLP-Based Model for Long-Term Time Series Forecasting with Exogenous Inputs](https://arxiv.org/abs/2601.09237)
*Xinyang Chen,Huidong Jin,Yu Huang,Zaiwen Feng*

Main category: cs.LG

TL;DR: 论文提出轻量级时间序列预测模型XLinear，在多数据集测试中表现出优于现有模型的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现实应用中存在不对称因果关系和不同数据获取成本，现有模型有计算成本高、忽略局部模式等问题，需有效利用时间维度和外生变量信息。

Method: 提出基于多层感知机（MLPs）的XLinear模型，用全局标记与外生变量交互，通过带sigmoid激活的MLPs提取模式和依赖关系，用预测头集成信号进行预测。

Result: 在七个标准基准和五个含外生输入的真实数据集上评估，XLinear在多变量和受外生输入影响的单变量预测中准确性和效率均优于现有模型。

Conclusion: XLinear模型能有效利用时间维度和外生变量信息，在时间序列预测任务中表现出色。

Abstract: Despite the prevalent assumption of uniform variable importance in long-term time series forecasting models, real world applications often exhibit asymmetric causal relationships and varying data acquisition costs. Specifically, cost-effective exogenous data (e.g., local weather) can unilaterally influence dynamics of endogenous variables, such as lake surface temperature. Exploiting these links enables more effective forecasts when exogenous inputs are readily available. Transformer-based models capture long-range dependencies but incur high computation and suffer from permutation invariance. Patch-based variants improve efficiency yet can miss local temporal patterns. To efficiently exploit informative signals across both the temporal dimension and relevant exogenous variables, this study proposes XLinear, a lightweight time series forecasting model built upon MultiLayer Perceptrons (MLPs). XLinear uses a global token derived from an endogenous variable as a pivotal hub for interacting with exogenous variables, and employs MLPs with sigmoid activation to extract both temporal patterns and variate-wise dependencies. Its prediction head then integrates these signals to forecast the endogenous series. We evaluate XLinear on seven standard benchmarks and five real-world datasets with exogenous inputs. Compared with state-of-the-art models, XLinear delivers superior accuracy and efficiency for both multivariate forecasts and univariate forecasts influenced by exogenous inputs.

</details>


### [97] [HGATSolver: A Heterogeneous Graph Attention Solver for Fluid-Structure Interaction](https://arxiv.org/abs/2601.09251)
*Qin-Yi Zhang,Hong Wang,Siyao Liu,Haichuan Lin,Linying Cao,Xiao-Hu Zhou,Chen Chen,Shuangyi Wang,Zeng-Guang Hou*

Main category: cs.LG

TL;DR: 现有基于学习的求解器难以在统一框架下捕捉FSI的异质动力学，提出HGATSolver，通过将系统编码为异质图，引入物理条件门控机制和域间梯度平衡损失，实验证明其在多物理耦合系统代理建模上达到了SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的求解器难以捕捉FSI的异质动力学，且因界面耦合和不同区域学习难度差异导致预测不稳定。

Method: 提出HGATSolver，将系统编码为异质图，引入物理条件门控机制和域间梯度平衡损失。

Result: 在两个构造的FSI基准和一个公共数据集上的实验表明，HGATSolver达到了SOTA性能。

Conclusion: HGATSolver为多物理耦合系统的代理建模建立了一个有效的框架。

Abstract: Fluid-structure interaction (FSI) systems involve distinct physical domains, fluid and solid, governed by different partial differential equations and coupled at a dynamic interface. While learning-based solvers offer a promising alternative to costly numerical simulations, existing methods struggle to capture the heterogeneous dynamics of FSI within a unified framework. This challenge is further exacerbated by inconsistencies in response across domains due to interface coupling and by disparities in learning difficulty across fluid and solid regions, leading to instability during prediction. To address these challenges, we propose the Heterogeneous Graph Attention Solver (HGATSolver). HGATSolver encodes the system as a heterogeneous graph, embedding physical structure directly into the model via distinct node and edge types for fluid, solid, and interface regions. This enables specialized message-passing mechanisms tailored to each physical domain. To stabilize explicit time stepping, we introduce a novel physics-conditioned gating mechanism that serves as a learnable, adaptive relaxation factor. Furthermore, an Inter-domain Gradient-Balancing Loss dynamically balances the optimization objectives across domains based on predictive uncertainty. Extensive experiments on two constructed FSI benchmarks and a public dataset demonstrate that HGATSolver achieves state-of-the-art performance, establishing an effective framework for surrogate modeling of coupled multi-physics systems.

</details>


### [98] [RIFT: Repurposing Negative Samples via Reward-Informed Fine-Tuning](https://arxiv.org/abs/2601.09253)
*Zehua Liu,Shuqi Liu,Tao Zhong,Mingxuan Yuan*

Main category: cs.LG

TL;DR: 提出Reward Informed Fine - Tuning (RIFT)框架解决数据低效问题，实验显示其优于RFT，是数据高效的对齐方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM对齐方法SFT和RFT存在依赖昂贵专家数据或丢弃有价值负样本导致数据低效的问题。

Method: 提出RIFT框架，利用所有自生成样本，用标量奖励对损失重新加权，引入稳定的损失公式确保数值稳定性和优化效率。

Result: 在各种基础模型的数学基准测试中，RIFT始终优于RFT。

Conclusion: RIFT是使用混合质量自生成数据进行对齐的稳健且数据高效的替代方案。

Abstract: While Supervised Fine-Tuning (SFT) and Rejection Sampling Fine-Tuning (RFT) are standard for LLM alignment, they either rely on costly expert data or discard valuable negative samples, leading to data inefficiency. To address this, we propose Reward Informed Fine-Tuning (RIFT), a simple yet effective framework that utilizes all self-generated samples. Unlike the hard thresholding of RFT, RIFT repurposes negative trajectories, reweighting the loss with scalar rewards to learn from both the positive and negative trajectories from the model outputs. To overcome the training collapse caused by naive reward integration, where direct multiplication yields an unbounded loss, we introduce a stabilized loss formulation that ensures numerical robustness and optimization efficiency. Extensive experiments on mathematical benchmarks across various base models show that RIFT consistently outperforms RFT. Our results demonstrate that RIFT is a robust and data-efficient alternative for alignment using mixed-quality, self-generated data.

</details>


### [99] [Learning to Trust Experience: A Monitor-Trust-Regulator Framework for Learning under Unobservable Feedback Reliability](https://arxiv.org/abs/2601.09261)
*Zhipeng Zhang,Zhenjie Yao,Kai Li,Lei Yang*

Main category: cs.LG

TL;DR: 研究不可观测反馈可靠性下的学习问题，提出元认知调节方法，实证显示有积极效果并提供设计模板。


<details>
  <summary>Details</summary>
Motivation: 解决不可观测反馈可靠性下学习系统不仅要稳定学习，还要决定是否从经验中学习的问题。

Method: 提出元认知调节，将其形式化为MTR分解并以自我诊断实例化。

Result: 自我诊断在EIUR制度下改善认知可识别性，在强化和监督学习中有不同表现。

Conclusion: MTR和自我诊断为不可观测可靠性下自主学习的内在可靠性评估提供抽象和设计模板。

Abstract: Learning under unobservable feedback reliability poses a distinct challenge beyond optimization robustness: a system must decide whether to learn from an experience, not only how to learn stably. We study this setting as Epistemic Identifiability under Unobservable Reliability (EIUR), where each experience has a latent credibility, reliable and unreliable feedback can be locally indistinguishable, and data are generated in a closed loop by the learner's own evolving beliefs and actions. In EIUR, standard robust learning can converge stably yet form high-confidence, systematically wrong beliefs.
  We propose metacognitive regulation as a practical response: a second, introspective control loop that infers experience credibility from endogenous evidence in the learner's internal dynamics. We formalize this as a modular Monitor-Trust-Regulator (MTR) decomposition and instantiate it with self-diagnosis, which maintains a slowly varying experience-trust variable that softly modulates learning updates, without exogenous reliability labels or an explicit corruption model.
  Empirically, in the EIUR regimes studied here, self-diagnosis is associated with improved epistemic identifiability. In reinforcement learning, it enables calibrated skepticism and recovery under systematically corrupted rewards. In supervised learning, it exposes a critical dissociation: performance recovery does not imply epistemic recovery. Accuracy can rebound while internal belief dynamics remain locked-in by early misleading data, a failure detectable only through introspective diagnostics. Together, MTR and self-diagnosis provide an organizing abstraction and a concrete design template for intrinsic reliability assessment in autonomous learning under unobservable reliability.

</details>


### [100] [Enhancing Spatial Reasoning in Large Language Models for Metal-Organic Frameworks Structure Prediction](https://arxiv.org/abs/2601.09285)
*Mianzhi Pan,JianFei Li,Peishuo Liu,Botian Wang,Yawen Ouyang,Yiming Rong,Hao Zhou,Jianbing Zhang*

Main category: cs.LG

TL;DR: 本文提出MOF - LLM框架用于预测MOFs 3D结构，方法结合多种训练范式，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: MOFs的3D结构准确预测是挑战，现有LLMs因MOFs原子复杂度高难以应用。

Method: 引入MOF - LLM框架，训练范式集成空间感知持续预训练、结构监督微调、匹配驱动强化学习，并通过SAPO优化结构稳定性。

Result: MOF - LLM在综合实验中优于基于去噪和基于LLM的现有方法，且采样效率更高。

Conclusion: MOF - LLM能有效提升LLM对MOF结构的空间推理能力，实现准确结构预测。

Abstract: Metal-organic frameworks (MOFs) are porous crystalline materials with broad applications such as carbon capture and drug delivery, yet accurately predicting their 3D structures remains a significant challenge. While Large Language Models (LLMs) have shown promise in generating crystals, their application to MOFs is hindered by MOFs' high atomic complexity. Inspired by the success of block-wise paradigms in deep generative models, we pioneer the use of LLMs in this domain by introducing MOF-LLM, the first LLM framework specifically adapted for block-level MOF structure prediction. To effectively harness LLMs for this modular assembly task, our training paradigm integrates spatial-aware continual pre-training (CPT), structural supervised fine-tuning (SFT), and matching-driven reinforcement learning (RL). By incorporating explicit spatial priors and optimizing structural stability via Soft Adaptive Policy Optimization (SAPO), our approach substantially enhances the spatial reasoning capability of a Qwen-3 8B model for accurate MOF structure prediction. Comprehensive experiments demonstrate that MOF-LLM outperforms state-of-the-art denoising-based and LLM-based methods while exhibiting superior sampling efficiency.

</details>


### [101] [Single-Round Clustered Federated Learning via Data Collaboration Analysis for Non-IID Data](https://arxiv.org/abs/2601.09304)
*Sota Sugawara,Yuji Kawamata,Akihiro Toyoda,Tomoru Nakayama,Yukihiko Okada*

Main category: cs.LG

TL;DR: 提出单轮框架DC - CFL用于聚类联邦学习，在多数据集实验中仅一轮通信就达到与多轮基线相当的准确率，是多轮通信不可行时的实用选择。


<details>
  <summary>Details</summary>
Motivation: 多数聚类联邦学习方法依赖多轮通信进行聚类估计和模型更新，在通信轮次受限场景实用性不足。

Method: 通过数据协作（DC）分析共享的信息完成客户端聚类和聚类学习，用总变差距离量化客户端间相似度，用层次聚类估计聚类，通过DC分析进行聚类学习。

Result: 在多个开放数据集的代表性非独立同分布条件实验中，DC - CFL仅需一轮通信就达到与多轮基线相当的准确率。

Conclusion: DC - CFL是多轮通信不可行时协作式人工智能模型开发的实用替代方案。

Abstract: Federated Learning (FL) enables distributed learning across multiple clients without sharing raw data. When statistical heterogeneity across clients is severe, Clustered Federated Learning (CFL) can improve performance by grouping similar clients and training cluster-wise models. However, most CFL approaches rely on multiple communication rounds for cluster estimation and model updates, which limits their practicality under tight constraints on communication rounds. We propose Data Collaboration-based Clustered Federated Learning (DC-CFL), a single-round framework that completes both client clustering and cluster-wise learning, using only the information shared in DC analysis. DC-CFL quantifies inter-client similarity via total variation distance between label distributions, estimates clusters using hierarchical clustering, and performs cluster-wise learning via DC analysis. Experiments on multiple open datasets under representative non-IID conditions show that DC-CFL achieves accuracy comparable to multi-round baselines while requiring only one communication round. These results indicate that DC-CFL is a practical alternative for collaborative AI model development when multiple communication rounds are impractical.

</details>


### [102] [GeoRA: Geometry-Aware Low-Rank Adaptation for RLVR](https://arxiv.org/abs/2601.09361)
*Jiaying Zhang,Lei Shi,Jiguo Li,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He*

Main category: cs.LG

TL;DR: 现有参数高效方法不适用于RLVR，有谱坍缩和效率瓶颈，本文提出GeoRA，实验显示其优于现有方法达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有针对监督微调的参数高效方法用于RLVR时存在谱坍缩、优化不稳定及效率瓶颈问题，需改进。

Method: 提出GeoRA，在几何约束子空间通过SVD提取主方向初始化适配器并冻结残差分量，利用稠密算子实现高效GPU计算。

Result: 在Qwen和Llama上实验表明，GeoRA缓解了几何失准带来的优化瓶颈，在关键数学基准上超现有低秩基线方法，达SOTA。

Conclusion: GeoRA表现优秀，有更好的泛化能力，且在域外任务中抗灾难性遗忘能力强。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is crucial for advancing large-scale reasoning models. However, existing parameter-efficient methods, such as PiSSA and MiLoRA, are designed for Supervised Fine-Tuning (SFT) and do not account for the distinct optimization dynamics and geometric structures of RLVR. Applying these methods directly leads to spectral collapse and optimization instability, which severely limit model performance. Meanwhile, alternative approaches that leverage update sparsity encounter significant efficiency bottlenecks on modern hardware due to unstructured computations. To address these challenges, we propose GeoRA (Geometry-Aware Low-Rank Adaptation), which exploits the anisotropic and compressible nature of RL update subspaces. GeoRA initializes adapters by extracting principal directions via Singular Value Decomposition (SVD) within a geometrically constrained subspace while freezing the residual components. This method preserves the pre-trained geometric structure and enables efficient GPU computation through dense operators. Experiments on Qwen and Llama demonstrate that GeoRA mitigates optimization bottlenecks caused by geometric misalignment. It consistently outperforms established low-rank baselines on key mathematical benchmarks, achieving state-of-the-art (SOTA) results. Moreover, GeoRA shows superior generalization and resilience to catastrophic forgetting in out-of-domain tasks.

</details>


### [103] [Preliminary Tests of the Anticipatory Classifier System with Hindsight Experience Replay](https://arxiv.org/abs/2601.09400)
*Olgierd Unold,Stanisław Franczyk*

Main category: cs.LG

TL;DR: 本文提出ACS2HER，结合ACS2与HER机制，在两个基准测试中验证其加速知识获取能力，但有计算开销大等问题。


<details>
  <summary>Details</summary>
Motivation: ACS2在稀疏奖励环境中性能易停滞，需改进。

Method: 提出特定架构变体，当智能体未达主要目标时触发后见学习，将访问过的状态重新标记为虚拟目标。

Result: 在确定性的Maze 6和随机的FrozenLake两个基准测试中，ACS2HER比标准ACS2显著加速了知识获取和环境掌握，但有更大计算开销和分类器数量显著增加。

Conclusion: 首次分析了学习分类器系统中预期机制与回顾性目标重标记的结合。

Abstract: This paper introduces ACS2HER, a novel integration of the Anticipatory Classifier System (ACS2) with the Hindsight Experience Replay (HER) mechanism. While ACS2 is highly effective at building cognitive maps through latent learning, its performance often stagnates in environments characterized by sparse rewards. We propose a specific architectural variant that triggers hindsight learning when the agent fails to reach its primary goal, re-labeling visited states as virtual goals to densify the learning signal. The proposed model was evaluated on two benchmarks: the deterministic \texttt{Maze 6} and the stochastic \texttt{FrozenLake}. The results demonstrate that ACS2HER significantly accelerates knowledge acquisition and environmental mastery compared to the standard ACS2. However, this efficiency gain is accompanied by increased computational overhead and a substantial expansion in classifier numerosity. This work provides the first analysis of combining anticipatory mechanisms with retrospective goal-relabeling in Learning Classifier Systems.

</details>


### [104] [Draw it like Euclid: Teaching transformer models to generate CAD profiles using ruler and compass construction steps](https://arxiv.org/abs/2601.09428)
*Siyi Li,Joseph G. Lambourne,Longfei Zhang,Pradeep Kumar Jayaraman,Karl. D. D. Willis*

Main category: cs.LG

TL;DR: 提出通过简单几何构造序列生成CAD轮廓的新方法，添加构造步骤可提升生成质量，应用强化学习有更多改进。


<details>
  <summary>Details</summary>
Motivation: 寻找更好的生成CAD轮廓的方法，提升生成质量和实现参数编辑。

Method: 采用包含曲线偏移、旋转和相交等简单几何构造序列生成CAD轮廓，添加构造步骤，应用强化学习。

Result: 添加构造步骤能提升生成质量，应用强化学习在多指标上有进一步改进。

Conclusion: 新的CAD轮廓生成方法有效，添加构造步骤和应用强化学习可提升性能。

Abstract: We introduce a new method of generating Computer Aided Design (CAD) profiles via a sequence of simple geometric constructions including curve offsetting, rotations and intersections. These sequences start with geometry provided by a designer and build up the points and curves of the final profile step by step. We demonstrate that adding construction steps between the designer's input geometry and the final profile improves generation quality in a similar way to the introduction of a chain of thought in language models. Similar to the constraints in a parametric CAD model, the construction sequences reduce the degrees of freedom in the modeled shape to a small set of parameter values which can be adjusted by the designer, allowing parametric editing with the constructed geometry evaluated to floating point precision. In addition we show that applying reinforcement learning to the construction sequences gives further improvements over a wide range of metrics, including some which were not explicitly optimized.

</details>


### [105] [DeepLight: A Sobolev-trained Image-to-Image Surrogate Model for Light Transport in Tissue](https://arxiv.org/abs/2601.09439)
*Philipp Haim,Vasilis Ntziachristos,Torsten Enßlin,Dominik Jüstel*

Main category: cs.LG

TL;DR: 提出用Sobolev训练改善组织光传输替代模型导数准确性，提升其在逆问题解决中的应用。


<details>
  <summary>Details</summary>
Motivation: 现有光声成像中光传输反演问题的变分反演方法依赖准确可微的光传输模型，神经替代模型导数准确性无保证，影响逆问题求解与高保真重建。

Method: 提出使用Sobolev训练的组织光传输替代模型，该训练形式适用于高维模型。

Result: Sobolev训练不仅提高了导数准确性，还降低了分布内和分布外样本的泛化误差。

Conclusion: 这些改进有望显著提升替代模型在下游任务中的实用性，尤其是在解决逆问题方面。

Abstract: In optoacoustic imaging, recovering the absorption coefficients of tissue by inverting the light transport remains a challenging problem. Improvements in solving this problem can greatly benefit the clinical value of optoacoustic imaging. Existing variational inversion methods require an accurate and differentiable model of this light transport. As neural surrogate models allow fast and differentiable simulations of complex physical processes, they are considered promising candidates to be used in solving such inverse problems. However, there are in general no guarantees that the derivatives of these surrogate models accurately match those of the underlying physical operator. As accurate derivatives are central to solving inverse problems, errors in the model derivative can considerably hinder high fidelity reconstructions. To overcome this limitation, we present a surrogate model for light transport in tissue that uses Sobolev training to improve the accuracy of the model derivatives. Additionally, the form of Sobolev training we used is suitable for high-dimensional models in general. Our results demonstrate that Sobolev training for a light transport surrogate model not only improves derivative accuracy but also reduces generalization error for in-distribution and out-of-distribution samples. These improvements promise to considerably enhance the utility of the surrogate model in downstream tasks, especially in solving inverse problems.

</details>


### [106] [Late Breaking Results: Quamba-SE: Soft-edge Quantizer for Activations in State Space Models](https://arxiv.org/abs/2601.09451)
*Yizhi Chen,Ahmed Hemani*

Main category: cs.LG

TL;DR: 提出Quamba - SE用于SSM激活量化，采用三个自适应尺度，评估显示其性能优于Quamba。


<details>
  <summary>Details</summary>
Motivation: 改进现有SSM激活量化方法，避免硬裁剪并保留异常值信息。

Method: 提出Quamba - SE软边缘量化器，使用三个自适应尺度进行量化。

Result: 在6个零样本基准测试中，Quamba - SE始终优于Quamba，单个基准最高提升2.68%，6个数据集平均准确率最高提升0.83%。

Conclusion: Quamba - SE在SSM激活量化中表现优于Quamba，是更优的量化方案。

Abstract: We propose Quamba-SE, a soft-edge quantizer for State Space Model (SSM) activation quantization. Unlike existing methods, using standard INT8 operation, Quamba-SE employs three adaptive scales: high-precision for small values, standard scale for normal values, and low-precision for outliers. This preserves outlier information instead of hard clipping, while maintaining precision for other values. We evaluate on Mamba- 130M across 6 zero-shot benchmarks. Results show that Quamba- SE consistently outperforms Quamba, achieving up to +2.68% on individual benchmarks and up to +0.83% improvement in the average accuracy of 6 datasets.

</details>


### [107] [On the Hardness of Computing Counterfactual and Semifactual Explanations in XAI](https://arxiv.org/abs/2601.09455)
*André Artelt,Martin Olsen,Kevin Tierney*

Main category: cs.LG

TL;DR: 本文概述生成反事实和半事实解释的计算复杂度，指出其生成和近似都困难，并探讨对XAI社区和政策制定者的影响。


<details>
  <summary>Details</summary>
Motivation: 为机器学习模型的选择提供清晰解释对关键应用很重要，需研究反事实和半事实解释生成的计算复杂度。

Method: 概述文献中生成解释的计算复杂度结果，并给出自己的不可近似性结果。

Result: 发现很多情况下生成解释计算困难，且在一定假设下难以近似。

Conclusion: 讨论了这些复杂度结果对XAI社区和政策制定者的影响。

Abstract: Providing clear explanations to the choices of machine learning models is essential for these models to be deployed in crucial applications. Counterfactual and semi-factual explanations have emerged as two mechanisms for providing users with insights into the outputs of their models. We provide an overview of the computational complexity results in the literature for generating these explanations, finding that in many cases, generating explanations is computationally hard. We strengthen the argument for this considerably by further contributing our own inapproximability results showing that not only are explanations often hard to generate, but under certain assumptions, they are also hard to approximate. We discuss the implications of these complexity results for the XAI community and for policymakers seeking to regulate explanations in AI.

</details>


### [108] [Searth Transformer: A Transformer Architecture Incorporating Earth's Geospheric Physical Priors for Global Mid-Range Weather Forecasting](https://arxiv.org/abs/2601.09467)
*Tianye Li,Qi Liu,Hao Li,Lei Chen,Wencong Cheng,Fei Zheng,Xiangao Xia,Ya Wang,Gang Huang,Weiwei Wang,Xuan Tong,Ziqing Zu,Yi Fang,Shenming Fu,Jiang Jiang,Haochen Li,Mingxing Li,Jiangjiang Xia*

Main category: cs.LG

TL;DR: 提出Searth Transformer架构和RAR微调策略，开发全球中期天气预报模型YanTian，精度高、成本低，为地球系统科学提供新途径。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的天气预报模型忽视地球几何特性，传统自回归训练计算成本高、预测范围受限。

Method: 提出Searth Transformer架构将区域周期性和子午线边界融入自注意力机制，引入RAR微调策略，开发YanTian模型。

Result: YanTian比欧洲中期天气预报中心高分辨率预报更准确，与最先进AI模型竞争，计算成本低，Z500预测领先时间更长。

Conclusion: 为复杂全球尺度地球物理环流系统预测建模奠定算法基础，为地球系统科学提供新途径。

Abstract: Accurate global medium-range weather forecasting is fundamental to Earth system science. Most existing Transformer-based forecasting models adopt vision-centric architectures that neglect the Earth's spherical geometry and zonal periodicity. In addition, conventional autoregressive training is computationally expensive and limits forecast horizons due to error accumulation. To address these challenges, we propose the Shifted Earth Transformer (Searth Transformer), a physics-informed architecture that incorporates zonal periodicity and meridional boundaries into window-based self-attention for physically consistent global information exchange. We further introduce a Relay Autoregressive (RAR) fine-tuning strategy that enables learning long-range atmospheric evolution under constrained memory and computational budgets. Based on these methods, we develop YanTian, a global medium-range weather forecasting model. YanTian achieves higher accuracy than the high-resolution forecast of the European Centre for Medium-Range Weather Forecasts and performs competitively with state-of-the-art AI models at one-degree resolution, while requiring roughly 200 times lower computational cost than standard autoregressive fine-tuning. Furthermore, YanTian attains a longer skillful forecast lead time for Z500 (10.3 days) than HRES (9 days). Beyond weather forecasting, this work establishes a robust algorithmic foundation for predictive modeling of complex global-scale geophysical circulation systems, offering new pathways for Earth system science.

</details>


### [109] [FairGU: Fairness-aware Graph Unlearning in Social Network](https://arxiv.org/abs/2601.09469)
*Renqiang Luo,Yongshuai Yang,Huafei Huang,Qing Qing,Mingliang Hou,Ziqi Xu,Yi Yu,Jingjing Zhou,Feng Xia*

Main category: cs.LG

TL;DR: 提出FairGU框架用于图遗忘学习，兼顾实用性与公平性，在多数据集实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有图遗忘学习技术对敏感属性保护不足，相较于传统图学习方法会降低算法公平性。

Method: 引入FairGU框架，将公平感知模块与数据保护策略相结合，确保移除节点时敏感属性不被放大或暴露。

Result: 在多个真实世界数据集上的实验表明，FairGU在准确性和公平性指标上均优于现有图遗忘学习方法和增强公平性的图学习基线。

Conclusion: 指出当前遗忘学习实践中被忽视的风险，证明FairGU是下一代社会可持续网络系统的可靠公平解决方案。

Abstract: Graph unlearning has emerged as a critical mechanism for supporting sustainable and privacy-preserving social networks, enabling models to remove the influence of deleted nodes and thereby better safeguard user information. However, we observe that existing graph unlearning techniques insufficiently protect sensitive attributes, often leading to degraded algorithmic fairness compared with traditional graph learning methods. To address this gap, we introduce FairGU, a fairness-aware graph unlearning framework designed to preserve both utility and fairness during the unlearning process. FairGU integrates a dedicated fairness-aware module with effective data protection strategies, ensuring that sensitive attributes are neither inadvertently amplified nor structurally exposed when nodes are removed. Through extensive experiments on multiple real-world datasets, we demonstrate that FairGU consistently outperforms state-of-the-art graph unlearning methods and fairness-enhanced graph learning baselines in terms of both accuracy and fairness metrics. Our findings highlight a previously overlooked risk in current unlearning practices and establish FairGU as a robust and equitable solution for the next generation of socially sustainable networked systems. The codes are available at https://github.com/LuoRenqiang/FairGU.

</details>


### [110] [SimMerge: Learning to Select Merge Operators from Similarity Signals](https://arxiv.org/abs/2601.09473)
*Oliver Bolton,Aakanksha,Arash Ahmadian,Sara Hooker,Marzieh Fadaee,Beyza Ermis*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Model merging enables multiple large language models (LLMs) to be combined into a single model while preserving performance. This makes it a valuable tool in LLM development, offering a competitive alternative to multi-task training. However, merging can be difficult at scale, as successful merging requires choosing the right merge operator, selecting the right models, and merging them in the right order. This often leads researchers to run expensive merge-and-evaluate searches to select the best merge. In this work, we provide an alternative by introducing \simmerge{}, \emph{a predictive merge-selection method} that selects the best merge using inexpensive, task-agnostic similarity signals between models. From a small set of unlabeled probes, we compute functional and structural features and use them to predict the performance of a given 2-way merge. Using these predictions, \simmerge{} selects the best merge operator, the subset of models to merge, and the merge order, eliminating the expensive merge-and-evaluate loop. We demonstrate that we surpass standard merge-operator performance on 2-way merges of 7B-parameter LLMs, and that \simmerge{} generalizes to multi-way merges and 111B-parameter LLM merges without retraining. Additionally, we present a bandit variant that supports adding new tasks, models, and operators on the fly. Our results suggest that learning how to merge is a practical route to scalable model composition when checkpoint catalogs are large and evaluation budgets are tight.

</details>


### [111] [Terminally constrained flow-based generative models from an optimal control perspective](https://arxiv.org/abs/2601.09474)
*Weiguo Gao,Ming Li,Qianxiao Li*

Main category: cs.LG

TL;DR: 本文通过最优控制公式解决用预训练流生成模型从终端约束分布采样问题，提出TOCFlow方法，在多项任务上表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决用预训练流生成模型从终端约束分布采样的问题。

Method: 从理论上用Hamilton - Jacobi - Bellman方程刻画值函数并推导最优反馈控制；算法上提出TOCFlow，在终端共动框架中求解控制问题得到标量阻尼因子。

Result: 在Darcy流、约束轨迹规划和湍流快照生成三项高维科学任务上，TOCFlow比欧几里得引导和投影基线提高了约束满足度，同时保留了参考模型的生成质量。

Conclusion: TOCFlow是一种有效的从终端约束分布采样的方法，在保证生成质量的同时能更好满足约束条件。

Abstract: We address the problem of sampling from terminally constrained distributions with pre-trained flow-based generative models through an optimal control formulation. Theoretically, we characterize the value function by a Hamilton-Jacobi-Bellman equation and derive the optimal feedback control as the minimizer of the associated Hamiltonian. We show that as the control penalty increases, the controlled process recovers the reference distribution, while as the penalty vanishes, the terminal law converges to a generalized Wasserstein projection onto the constraint manifold. Algorithmically, we introduce Terminal Optimal Control with Flow-based models (TOCFlow), a geometry-aware sampling-time guidance method for pre-trained flows. Solving the control problem in a terminal co-moving frame that tracks reference trajectories yields a closed-form scalar damping factor along the Riemannian gradient, capturing second-order curvature effects without matrix inversions. TOCFlow therefore matches the geometric consistency of Gauss-Newton updates at the computational cost of standard gradient guidance. We evaluate TOCFlow on three high-dimensional scientific tasks spanning equality, inequality, and global statistical constraints, namely Darcy flow, constrained trajectory planning, and turbulence snapshot generation with Kolmogorov spectral scaling. Across all settings, TOCFlow improves constraint satisfaction over Euclidean guidance and projection baselines while preserving the reference model's generative quality.

</details>


### [112] [Deep Operator Networks for Surrogate Modeling of Cyclic Adsorption Processes with Varying Initial Conditions](https://arxiv.org/abs/2601.09491)
*Beatrice Ceccanti,Mattia Galanti,Ivo Roghair,Martin van Sint Annaland*

Main category: cs.LG

TL;DR: 本文将DeepONets应用于吸附技术过程建模，评估其作为循环吸附过程模拟和优化替代模型的可行性，结果显示其在训练分布内外都能准确预测。


<details>
  <summary>Details</summary>
Motivation: 循环吸附过程需反复求解瞬态PDEs，计算成本高，需有效替代模型加速收敛，且要能在广泛初始条件下泛化。

Method: 构建包含异构初始条件的混合训练数据集，训练DeepONets近似相应解算子，并在训练参数范围外及未见函数形式上测试。

Result: 训练模型在训练分布内外都能准确预测。

Conclusion: DeepONets可作为加速循环吸附模拟和优化工作流程的潜在高效替代模型。

Abstract: Deep Operator Networks are emerging as fundamental tools among various neural network types to learn mappings between function spaces, and have recently gained attention due to their ability to approximate nonlinear operators. In particular, DeepONets offer a natural formulation for PDE solving, since the solution of a partial differential equation can be interpreted as an operator mapping an initial condition to its corresponding solution field. In this work, we applied DeepONets in the context of process modeling for adsorption technologies, to assess their feasibility as surrogates for cyclic adsorption process simulation and optimization. The goal is to accelerate convergence of cyclic processes such as Temperature-Vacuum Swing Adsorption (TVSA), which require repeated solution of transient PDEs, which are computationally expensive. Since each step of a cyclic adsorption process starts from the final state of the preceding step, effective surrogate modeling requires generalization across a wide range of initial conditions. The governing equations exhibit steep traveling fronts, providing a demanding benchmark for operator learning. To evaluate functional generalization under these conditions, we construct a mixed training dataset composed of heterogeneous initial conditions and train DeepONets to approximate the corresponding solution operators. The trained models are then tested on initial conditions outside the parameter ranges used during training, as well as on completely unseen functional forms. The results demonstrate accurate predictions both within and beyond the training distribution, highlighting DeepONets as potential efficient surrogates for accelerating cyclic adsorption simulations and optimization workflows.

</details>


### [113] [Parallelizable memory recurrent units](https://arxiv.org/abs/2601.09495)
*Florent De Geeter,Gaspard Lambrechts,Damien Ernst,Guillaume Drion*

Main category: cs.LG

TL;DR: 本文介绍新的循环神经网络MRUs，结合非线性RNN持久记忆能力和SSMs并行计算能力，以BMRU为例证明其在长时依赖任务中效果好，还能与SSMs结合。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer在序列生成效率低，SSMs虽有优势但存在表示能力有限、无持久记忆的问题，因此需要新模型解决这些问题。

Method: 引入了MRUs，利用多稳态提供持久记忆并消除瞬态动力学以实现高效计算，并给出了BMRU的具体实现。

Result: BMRU在长时依赖任务中取得了良好结果，可与状态空间模型结合创建可并行且兼具瞬态动力学和持久记忆的混合网络。

Conclusion: MRUs及其实现BMRU是一种有效的循环神经网络，能解决现有模型的一些不足。

Abstract: With the emergence of massively parallel processing units, parallelization has become a desirable property for new sequence models. The ability to parallelize the processing of sequences with respect to the sequence length during training is one of the main factors behind the uprising of the Transformer architecture. However, Transformers lack efficiency at sequence generation, as they need to reprocess all past timesteps at every generation step. Recently, state-space models (SSMs) emerged as a more efficient alternative. These new kinds of recurrent neural networks (RNNs) keep the efficient update of the RNNs while gaining parallelization by getting rid of nonlinear dynamics (or recurrence). SSMs can reach state-of-the art performance through the efficient training of potentially very large networks, but still suffer from limited representation capabilities. In particular, SSMs cannot exhibit persistent memory, or the capacity of retaining information for an infinite duration, because of their monostability. In this paper, we introduce a new family of RNNs, the memory recurrent units (MRUs), that combine the persistent memory capabilities of nonlinear RNNs with the parallelizable computations of SSMs. These units leverage multistability as a source of persistent memory, while getting rid of transient dynamics for efficient computations. We then derive a specific implementation as proof-of-concept: the bistable memory recurrent unit (BMRU). This new RNN is compatible with the parallel scan algorithm. We show that BMRU achieves good results in tasks with long-term dependencies, and can be combined with state-space models to create hybrid networks that are parallelizable and have transient dynamics as well as persistent memory.

</details>


### [114] [Class Adaptive Conformal Training](https://arxiv.org/abs/2601.09522)
*Badr-Eddine Marani,Julio Silva-Rodriguez,Ismail Ben Ayed,Maria Vakalopoulou,Stergios Christodoulidis,Jose Dolz*

Main category: cs.LG

TL;DR: 提出Class Adaptive Conformal Training (CaCT)方法，在多基准数据集实验中表现优于现有共形训练方法。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络概率估计不可靠、预测过度自信，现有共形训练方法难以在无数据分布先验知识下进行类条件预测集塑造。

Method: 将共形训练表述为增广拉格朗日优化问题，自适应学习类条件塑造预测集，无需分布假设。

Result: 在多个基准数据集实验中，CaCT始终优于先前的共形训练方法，产生更小且信息更丰富的预测集。

Conclusion: CaCT是一种有效的共形训练方法，能在保证覆盖度的同时优化预测集。

Abstract: Deep neural networks have achieved remarkable success across a variety of tasks, yet they often suffer from unreliable probability estimates. As a result, they can be overconfident in their predictions. Conformal Prediction (CP) offers a principled framework for uncertainty quantification, yielding prediction sets with rigorous coverage guarantees. Existing conformal training methods optimize for overall set size, but shaping the prediction sets in a class-conditional manner is not straightforward and typically requires prior knowledge of the data distribution. In this work, we introduce Class Adaptive Conformal Training (CaCT), which formulates conformal training as an augmented Lagrangian optimization problem that adaptively learns to shape prediction sets class-conditionally without making any distributional assumptions. Experiments on multiple benchmark datasets, including standard and long-tailed image recognition as well as text classification, demonstrate that CaCT consistently outperforms prior conformal training methods, producing significantly smaller and more informative prediction sets while maintaining the desired coverage guarantees.

</details>


### [115] [Energy-Entropy Regularization: The True Power of Minimal Looped Transformers](https://arxiv.org/abs/2601.09588)
*Wai-Lun Lam*

Main category: cs.LG

TL;DR: 本文提出新训练框架，利用Tsallis熵和哈密顿动力学变换损失景观几何，成功训练单头循环Transformer解决归纳头任务并揭示推理能力机制。


<details>
  <summary>Details</summary>
Motivation: 当前单头循环架构在基准任务训练常失败或效果不佳，损失景观非凸不规则，优化易停滞，内部机制不明，从头训练困难。

Method: 提出利用Tsallis熵和哈密顿动力学变换损失景观几何的训练框架，将参数更新视为物理流。

Result: 成功训练模型维度d = 8的单头循环Transformer解决输入序列长度为1000个标记的归纳头任务。

Conclusion: 揭示了单头循环Transformer优越推理能力背后的内部机制。

Abstract: Recent research suggests that looped Transformers have superior reasoning capabilities compared to standard deep architectures. Current approaches to training single-head looped architectures on benchmark tasks frequently fail or yield suboptimal performance due to a highly non-convex and irregular loss landscape. In these settings, optimization often stagnates in poor local minima and saddle points of the loss landscape, preventing the model from discovering the global minimum point. The internal mechanisms of these single-head looped transformer models remain poorly understood, and training them from scratch remains a significant challenge. In this paper, we propose a novel training framework that leverages Tsallis entropy and Hamiltonian dynamics to transform the geometry of the loss landscape. By treating the parameter updates as a physical flow, we successfully trained a single-head looped Transformer with model dimension $d = 8$ to solve induction head task with input sequence length of 1000 tokens. This success reveals the internal mechanism behind the superior reasoning capability.

</details>


### [116] [Toward Understanding Unlearning Difficulty: A Mechanistic Perspective and Circuit-Guided Difficulty Metric](https://arxiv.org/abs/2601.09624)
*Jiali Cheng,Ziheng Chen,Chirag Agarwal,Hadi Amiri*

Main category: cs.LG

TL;DR: 研究语言模型机器遗忘问题，提出CUD指标，揭示遗忘难度的电路级模式。


<details>
  <summary>Details</summary>
Motivation: 语言模型机器遗忘中不同样本遗忘效果差异大，要从模型内部机制研究该问题。

Method: 基于模型电路从机械角度研究，提出CUD指标用电路级信号为样本打分。

Result: CUD能可靠区分难易样本，在不同遗忘方法中稳定，发现难易样本的电路级模式差异。

Conclusion: CUD迈向对遗忘难度原则性、细粒度和可解释分析，推动基于模型机制的遗忘方法发展。

Abstract: Machine unlearning is becoming essential for building trustworthy and compliant language models. Yet unlearning success varies considerably across individual samples: some are reliably erased, while others persist despite the same procedure. We argue that this disparity is not only a data-side phenomenon, but also reflects model-internal mechanisms that encode and protect memorized information. We study this problem from a mechanistic perspective based on model circuits--structured interaction pathways that govern how predictions are formed. We propose Circuit-guided Unlearning Difficulty (CUD), a {\em pre-unlearning} metric that assigns each sample a continuous difficulty score using circuit-level signals. Extensive experiments demonstrate that CUD reliably separates intrinsically easy and hard samples, and remains stable across unlearning methods. We identify key circuit-level patterns that reveal a mechanistic signature of difficulty: easy-to-unlearn samples are associated with shorter, shallower interactions concentrated in earlier-to-intermediate parts of the original model, whereas hard samples rely on longer and deeper pathways closer to late-stage computation. Compared to existing qualitative studies, CUD takes a first step toward a principled, fine-grained, and interpretable analysis of unlearning difficulty; and motivates the development of unlearning methods grounded in model mechanisms.

</details>


### [117] [From Prompt to Protocol: Fast Charging Batteries with Large Language Models](https://arxiv.org/abs/2601.09626)
*Ge Lei,Ferran Brosa Planella,Sterling G. Baird,Samuel J. Cooper*

Main category: cs.LG

TL;DR: 提出基于大语言模型的无梯度闭环方法优化电池充电协议，效果优于多种传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有电池充电协议优化方法因评估慢、成本高且不可微，限制搜索空间，难以发现高性能方案。

Method: 引入两种无梯度、LLM驱动的闭环方法，即Prompt - to - Optimizer (P2O)和Prompt - to - Protocol (P2P)。

Result: LLM引导的P2O性能优于贝叶斯优化、进化算法和随机搜索设计的神经网络；P2O和P2P在快速充电场景中比多步恒流基线方案健康状态提升约4.2%，P2P在相同评估预算下实现。

Conclusion: LLMs可扩展协议函数形式空间、整合基于语言的约束，并在高成本实验环境中实现高效优化。

Abstract: Efficiently optimizing battery charging protocols is challenging because each evaluation is slow, costly, and non-differentiable. Many existing approaches address this difficulty by heavily constraining the protocol search space, which limits the diversity of protocols that can be explored, preventing the discovery of higher-performing solutions. We introduce two gradient-free, LLM-driven closed-loop methods: Prompt-to-Optimizer (P2O), which uses an LLM to propose the code for small neural-network-based protocols, which are then trained by an inner loop, and Prompt-to-Protocol (P2P), which simply writes an explicit function for the current and its scalar parameters. Across our case studies, LLM-guided P2O outperforms neural networks designed by Bayesian optimization, evolutionary algorithms, and random search. In a realistic fast charging scenario, both P2O and P2P yield around a 4.2 percent improvement in state of health (capacity retention based health metric under fast charging cycling) over a state-of-the-art multi-step constant current (CC) baseline, with P2P achieving this under matched evaluation budgets (same number of protocol evaluations). These results demonstrate that LLMs can expand the space of protocol functional forms, incorporate language-based constraints, and enable efficient optimization in high cost experimental settings.

</details>


### [118] [Exploring Fine-Tuning for Tabular Foundation Models](https://arxiv.org/abs/2601.09654)
*Aditya Tanna,Pratinav Seth,Mohamed Bouadi,Vinay Kumar Sankarapu*

Main category: cs.LG

TL;DR: 对表格基础模型（TFMs）微调进行综合研究，比较不同微调方法，分析数据集因素影响，给出微调建议和局限。


<details>
  <summary>Details</summary>
Motivation: 了解TFMs微调效果，明确不同微调方式在何种情况下最优及局限。

Method: 在TALENT、OpenML - CC18和TabZilla等基准上，比较零样本、元学习、全监督微调（SFT）和参数高效微调（PEFT）方法，分析数据集不平衡、大小和维度等因素影响。

Result: 零样本TFMs已有较强性能，微调效果高度依赖模型和数据；元学习和PEFT在特定条件有适度提升，SFT常降低准确性或校准质量。

Conclusion: 给出何时进行微调最有益及微调的局限性的实用指南。

Abstract: Tabular Foundation Models (TFMs) have recently shown strong in-context learning capabilities on structured data, achieving zero-shot performance comparable to traditional machine learning methods. We find that zero-shot TFMs already achieve strong performance, while the benefits of fine-tuning are highly model and data-dependent. Meta-learning and PEFT provide moderate gains under specific conditions, whereas full supervised fine-tuning (SFT) often reduces accuracy or calibration quality. This work presents the first comprehensive study of fine-tuning in TFMs across benchmarks including TALENT, OpenML-CC18, and TabZilla. We compare Zero-Shot, Meta-Learning, Supervised (SFT), and parameter-efficient (PEFT) approaches, analyzing how dataset factors such as imbalance, size, and dimensionality affect outcomes. Our findings cover performance, calibration, and fairness, offering practical guidelines on when fine-tuning is most beneficial and its limitations.

</details>


### [119] [Disentangling Task Conflicts in Multi-Task LoRA via Orthogonal Gradient Projection](https://arxiv.org/abs/2601.09684)
*Ziyu Yang,Guibin Chen,Yuxin Yang,Aoxiong Zeng,Xiangquan Yang*

Main category: cs.LG

TL;DR: 提出适用于LoRA的Ortho - LoRA方法，能减轻多任务学习中的任务干扰。


<details>
  <summary>Details</summary>
Motivation: MTL结合LoRA虽能减少存储开销，但存在负迁移问题，LoRA的低秩约束加剧了该问题。

Method: 提出Ortho - LoRA，在LoRA子空间内将冲突的任务梯度动态投影到彼此的正交补。

Result: 在GLUE基准测试中，Ortho - LoRA有效减轻任务干扰，优于标准联合训练，以可忽略的计算开销恢复多任务和单任务基线间95%的性能差距。

Conclusion: Ortho - LoRA能有效解决MTL结合LoRA时的负迁移问题。

Abstract: Multi-Task Learning (MTL) combined with Low-Rank Adaptation (LoRA) has emerged as a promising direction for parameter-efficient deployment of Large Language Models (LLMs). By sharing a single adapter across multiple tasks, one can significantly reduce storage overhead. However, this approach suffers from negative transfer, where conflicting gradient updates from distinct tasks degrade the performance of individual tasks compared to single-task fine-tuning. This problem is exacerbated in LoRA due to the low-rank constraint, which limits the optimization landscape's capacity to accommodate diverse task requirements. In this paper, we propose Ortho-LoRA, a gradient projection method specifically tailored for the bipartite structure of LoRA. Ortho-LoRA dynamically projects conflicting task gradients onto the orthogonal complement of each other within the intrinsic LoRA subspace. Extensive experiments on the GLUE benchmark demonstrate that Ortho-LoRA effectively mitigates task interference, outperforming standard joint training and recovering 95\% of the performance gap between multi-task and single-task baselines with negligible computational overhead.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [120] [SiliconHealth: A Complete Low-Cost Blockchain Healthcare Infrastructure for Resource-Constrained Regions Using Repurposed Bitcoin Mining ASICs](https://arxiv.org/abs/2601.09557)
*Francisco Angulo de Lafuente,Seid Mehammed Abdu,Nirmal Tej*

Main category: cs.NE

TL;DR: 提出基于区块链的医疗基础设施SiliconHealth，利用废弃比特币矿机打造低成本医疗记录系统，介绍架构、方法，经实验验证有成本等优势，可惠及超6亿人。


<details>
  <summary>Details</summary>
Motivation: 为资源受限地区，特别是撒哈拉以南非洲，构建可验证、防篡改的电子健康记录系统，解决传统医疗IT基础设施经济上不可行的问题。

Method: 采用四层分层网络架构，引入确定性硬件指纹识别（DHF）范式，结合Reed - Solomon LSB水印、语义检索增强生成（RAG）和离线同步协议。

Result: 验证率达100%，医疗图像认证有30 - 40%损坏容忍度，成本降低96%，Lucky Miner LV06效率达2.93 MH/W，确认硬件通用性。

Conclusion: 建立了在传统医疗IT基础设施经济上不可行的地区部署电子健康记录的实用框架，有望惠及超6亿缺乏基本健康信息系统的人群。

Abstract: This paper presents SiliconHealth, a comprehensive blockchain-based healthcare infrastructure designed for resource-constrained regions, particularly sub-Saharan Africa. We demonstrate that obsolete Bitcoin mining Application-Specific Integrated Circuits (ASICs) can be repurposed to create a secure, low-cost, and energy-efficient medical records system. The proposed architecture employs a four-tier hierarchical network: regional hospitals using Antminer S19 Pro (90+ TH/s), urban health centers with Antminer S9 (14 TH/s), rural clinics equipped with Lucky Miner LV06 (500 GH/s, 13W), and mobile health points with portable ASIC devices. We introduce the Deterministic Hardware Fingerprinting (DHF) paradigm, which repurposes SHA-256 mining ASICs as cryptographic proof generators, achieving 100% verification rate across 23 test proofs during 300-second validation sessions. The system incorporates Reed-Solomon LSB watermarking for medical image authentication with 30-40% damage tolerance, semantic Retrieval-Augmented Generation (RAG) for intelligent medical record queries, and offline synchronization protocols for intermittent connectivity. Economic analysis demonstrates 96% cost reduction compared to GPU-based alternatives, with total deployment cost of $847 per rural clinic including 5-year solar power infrastructure. Validation experiments on Lucky Miner LV06 (BM1366 chip, 5nm) achieve 2.93 MH/W efficiency and confirm hardware universality. This work establishes a practical framework for deploying verifiable, tamper-proof electronic health records in regions where traditional healthcare IT infrastructure is economically unfeasible, potentially benefiting over 600 million people lacking access to basic health information systems.

</details>


### [121] [Improving CMA-ES Convergence Speed, Efficiency, and Reliability in Noisy Robot Optimization Problems](https://arxiv.org/abs/2601.09594)
*Russell M. Martin,Steven H. Collins*

Main category: cs.NE

TL;DR: 介绍Adaptive Sampling CMA - ES (AS - CMA)优化算法，对比其与CMA - ES和贝叶斯优化的效果，发现AS - CMA可提升噪声环境下的优化效率。


<details>
  <summary>Details</summary>
Motivation: 实验机器人优化中评估时间存在速度 - 准确性权衡，为达到一致精度，补充优化算法。

Method: 引入AS - CMA算法，根据预测排序难度分配采样时间；在四个模拟成本景观中对比AS - CMA与CMA - ES和贝叶斯优化。

Result: AS - CMA在98%的运行中收敛，比CMA - ES静态采样更快、成本更低；在复杂景观中比贝叶斯优化更高效可靠，在简单景观中可靠性相当但效率稍低；在人体外骨骼优化实验中表现符合预期。

Conclusion: AS - CMA可提升噪声环境下的优化效率，且对优化设置复杂度和调优要求影响小。

Abstract: Experimental robot optimization often requires evaluating each candidate policy for seconds to minutes. The chosen evaluation time influences optimization because of a speed-accuracy tradeoff: shorter evaluations enable faster iteration, but are also more subject to noise. Here, we introduce a supplement to the CMA-ES optimization algorithm, named Adaptive Sampling CMA-ES (AS-CMA), which assigns sampling time to candidates based on predicted sorting difficulty, aiming to achieve consistent precision. We compared AS-CMA to CMA-ES and Bayesian optimization using a range of static sampling times in four simulated cost landscapes. AS-CMA converged on 98% of all runs without adjustment to its tunable parameter, and converged 24-65% faster and with 29-76% lower total cost than each landscape's best CMA-ES static sampling time. As compared to Bayesian optimization, AS-CMA converged more efficiently and reliably in complex landscapes, while in simpler landscapes, AS-CMA was less efficient but equally reliable. We deployed AS-CMA in an exoskeleton optimization experiment and found the optimizer's behavior was consistent with expectations. These results indicate that AS-CMA can improve optimization efficiency in the presence of noise while minimally affecting optimization setup complexity and tuning requirements.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [122] [Revisiting Disaggregated Large Language Model Serving for Performance and Energy Implications](https://arxiv.org/abs/2601.08833)
*Jiaxi Li,Yue Zhu,Eun Kyung Lee,Klara Nahrstedt*

Main category: cs.PF

TL;DR: 本文重新评估不同KV传输介质和优化策略下的预填充 - 解码分离服务，发现性能提升不保证，分离服务独立频率缩放不能节能。


<details>
  <summary>Details</summary>
Motivation: 现有工作缺乏对不同KV缓存传输路径性能和能效的系统基准测试，也未严格评估优化技术对性能和能耗的影响。

Method: 引入新的共置服务基线，评估不同KV缓存传输路径下的分离设置，通过DVFS进行GPU分析，确定并比较各设置的性能 - 能耗帕累托前沿。

Result: 预填充 - 解码分离的性能提升取决于请求负载和KV传输介质，分离服务的分阶段独立频率缩放不能节能。

Conclusion: 预填充 - 解码分离服务的性能优势不保证，且分离服务本身能耗高，独立频率缩放不能实现节能。

Abstract: Different from traditional Large Language Model (LLM) serving that colocates the prefill and decode stages on the same GPU, disaggregated serving dedicates distinct GPUs to prefill and decode workload. Once the prefill GPU completes its task, the KV cache must be transferred to the decode GPU. While existing works have proposed various KV cache transfer paths across different memory and storage tiers, there remains a lack of systematic benchmarking that compares their performance and energy efficiency. Meanwhile, although optimization techniques such as KV cache reuse and frequency scaling have been utilized for disaggregated serving, their performance and energy implications have not been rigorously benchmarked. In this paper, we fill this research gap by re-evaluating prefill-decode disaggregation under different KV transfer mediums and optimization strategies. Specifically, we include a new colocated serving baseline and evaluate disaggregated setups under different KV cache transfer paths. Through GPU profiling using dynamic voltage and frequency scaling (DVFS), we identify and compare the performance-energy Pareto frontiers across all setups to evaluate the potential energy savings enabled by disaggregation. Our results show that performance benefits from prefill-decode disaggregation are not guaranteed and depend on the request load and KV transfer mediums. In addition, stage-wise independent frequency scaling enabled by disaggregation does not lead to energy saving due to inherently higher energy consumption of disaggregated serving.

</details>


### [123] [LookAhead: The Optimal Non-decreasing Index Policy for a Time-Varying Holding Cost problem](https://arxiv.org/abs/2601.08960)
*Keerthana Gurushankar,Zhouzi Li,Mor Harchol-Balter,Alan Scheller-Wolf*

Main category: cs.PF

TL;DR: 研究 TVHC 问题的特殊情况，提出最优索引策略 LookAhead 并推导最佳前瞻量。


<details>
  <summary>Details</summary>
Motivation: TVHC 问题除渐近情况外无最优解结果，研究其特殊的两类别 M/M/1 队列情况。

Method: 提出考虑未来 X 时间的成本而非当前成本的思路，推导最佳前瞻量 X。

Result: 得出 TVHC 问题特殊情况的首个最优（非递减）索引策略 LookAhead。

Conclusion: 通过 LookAhead 策略及推导的最佳前瞻量，为 TVHC 问题特殊情况提供最优调度方案。

Abstract: In practice, the cost of delaying a job can grow as the job waits. Such behavior is modeled by the Time-Varying Holding Cost (TVHC) problem, where each job's instantaneous holding cost increases with its current age (a job's age is the time since it arrived). The goal of the TVHC problem is to find a scheduling policy that minimizes the time-average total holding cost across all jobs.
  However, no optimality results are known for the TVHC problem outside of the asymptotic regime. In this paper, we study a simple yet still challenging special case: A two-class M/M/1 queue in which class 1 jobs incur a non-decreasing, time-varying holding cost and class 2 jobs incur a constant holding cost.
  Our main contribution is deriving the first optimal (non-decreasing) index policy for this special case of the TVHC problem. Our optimal policy, called LookAhead, stems from the following idea: Rather than considering each job's current holding cost when making scheduling decisions, we should look at their cost some $X$ time into the future, where this $X$ is intuitively called the ``lookahead amount." This paper derives that optimal lookahead amount.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [124] [LAUDE: LLM-Assisted Unit Test Generation and Debugging of Hardware DEsigns](https://arxiv.org/abs/2601.08856)
*Deeksha Nandal,Riccardo Revalor,Soham Dan,Debjit Pal*

Main category: cs.SE

TL;DR: 介绍用于硬件设计的统一单元测试生成和调试框架LAUDE，用LLM的CoT推理能力结合设计代码语义理解，在VerilogEval数据集上测试和调试效果好。


<details>
  <summary>Details</summary>
Motivation: 硬件单元测试开发需深入理解设计功能和创造力，调试过程痛苦，需要高效方法。

Method: 引入LAUDE框架，结合设计源代码语义理解和大语言模型的CoT推理能力，集成提示工程和设计执行信息。

Result: 在VerilogEval数据集的错误硬件设计代码上，生成的单元测试在组合和时序设计中分别最高检测到100%和93%的错误，调试成功率分别达93%和84%。

Conclusion: LAUDE框架在硬件设计的单元测试生成和调试方面有较好效果。

Abstract: Unit tests are critical in the hardware design lifecycle to ensure that component design modules are functionally correct and conform to the specification before they are integrated at the system level. Thus developing unit tests targeting various design features requires deep understanding of the design functionality and creativity. When one or more unit tests expose a design failure, the debugging engineer needs to diagnose, localize, and debug the failure to ensure design correctness, which is often a painstaking and intense process. In this work, we introduce LAUDE, a unified unit-test generation and debugging framework for hardware designs that cross-pollinates the semantic understanding of the design source code with the Chain-of-Thought (CoT) reasoning capabilities of foundational Large-Language Models (LLMs). LAUDE integrates prompt engineering and design execution information to enhance its unit test generation accuracy and code debuggability. We apply LAUDE with closed- and open-source LLMs to a large corpus of buggy hardware design codes derived from the VerilogEval dataset, where generated unit tests detected bugs in up to 100% and 93% of combinational and sequential designs and debugged up to 93% and 84% of combinational and sequential designs, respectively.

</details>


### [125] [Revisiting Software Engineering Education in the Era of Large Language Models: A Curriculum Adaptation and Academic Integrity Framework](https://arxiv.org/abs/2601.08857)
*Mustafa Degerli*

Main category: cs.SE

TL;DR: 大语言模型重塑软件工程实践，但现有课程与之不匹配，本文提出理论框架分析其对软件工程能力的影响并给出教学模型，指出需转向过程透明模型，最后提出需实证研究。


<details>
  <summary>Details</summary>
Motivation: 大语言模型融入专业工作流程重塑软件工程实践，但大部分软件工程和计算机工程课程仍采用传统教学模式，这种不匹配引发了对评估有效性、学习成果等的担忧。

Method: 采用概念研究方法，提出理论框架分析生成式AI对软件工程核心能力的改变，并引入教学设计模型。

Result: 框架描绘了问题分析、设计、实现和测试从构建向批判、验证和人机协作转变；传统以剽窃为中心的诚信机制不足，需转向过程透明模型。

Conclusion: 本文为课程适应提供了结构化建议，但属理论贡献，需开展纵向实证研究评估干预措施及其对学习的长期影响。

Abstract: The integration of Large Language Models (LLMs), such as ChatGPT and GitHub Copilot, into professional workflows is increasingly reshaping software engineering practices. These tools have lowered the cost of code generation, explanation, and testing, while introducing new forms of automation into routine development tasks. In contrast, most of the software engineering and computer engineering curricula remain closely aligned with pedagogical models that equate manual syntax production with technical competence. This growing misalignment raises concerns regarding assessment validity, learning outcomes, and the development of foundational skills. Adopting a conceptual research approach, this paper proposes a theoretical framework for analyzing how generative AI alters core software engineering competencies and introduces a pedagogical design model for LLM-integrated education. Attention is given to computer engineering programs in Turkey, where centralized regulation, large class sizes, and exam-oriented assessment practices amplify these challenges. The framework delineates how problem analysis, design, implementation, and testing increasingly shift from construction toward critique, validation, and human-AI stewardship. In addition, the paper argues that traditional plagiarism-centric integrity mechanisms are becoming insufficient, motivating a transition toward a process transparency model. While this work provides a structured proposal for curriculum adaptation, it remains a theoretical contribution; the paper concludes by outlining the need for longitudinal empirical studies to evaluate these interventions and their long-term impacts on learning.

</details>


### [126] [Adaptive Trust Metrics for Multi-LLM Systems: Enhancing Reliability in Regulated Industries](https://arxiv.org/abs/2601.08858)
*Tejaswini Bollikonda*

Main category: cs.SE

TL;DR: 探讨多LLM生态系统的自适应信任指标，提出框架提升模型可靠性，通过案例展示其在现实应用，认为是监管行业安全可扩展采用AI的基础。


<details>
  <summary>Details</summary>
Motivation: LLMs在敏感领域应用引发信任、责任和可靠性等问题，需解决。

Method: 分析系统行为、评估多LLM不确定性、实施动态监控管道。

Result: 通过金融合规和医疗诊断案例展示自适应信任指标的实际应用。

Conclusion: 自适应信任测量是监管行业安全可扩展采用AI的基础。

Abstract: Large Language Models (LLMs) are increasingly deployed in sensitive domains such as healthcare, finance, and law, yet their integration raises pressing concerns around trust, accountability, and reliability. This paper explores adaptive trust metrics for multi LLM ecosystems, proposing a framework for quantifying and improving model reliability under regulated constraints. By analyzing system behaviors, evaluating uncertainty across multiple LLMs, and implementing dynamic monitoring pipelines, the study demonstrates practical pathways for operational trustworthiness. Case studies from financial compliance and healthcare diagnostics illustrate the applicability of adaptive trust metrics in real world settings. The findings position adaptive trust measurement as a foundational enabler for safe and scalable AI adoption in regulated industries.

</details>


### [127] [AI-NativeBench: An Open-Source White-Box Agentic Benchmark Suite for AI-Native Systems](https://arxiv.org/abs/2601.09393)
*Zirui Wang,Guangba Yu,Michael R. Lyu*

Main category: cs.SE

TL;DR: 从云原生到AI原生架构的转变使传统评估范式不足，引入AI-NativeBench基准套件揭示工程现实，为构建可靠AI原生系统提供依据且开源资源。


<details>
  <summary>Details</summary>
Motivation: 云原生到AI原生架构转变，传统黑盒评估范式无法适应新架构，需要新的评估方法。

Method: 引入基于Model Context Protocol (MCP) 和Agent-to-Agent (A2A) 标准的应用中心且白盒的AI-NativeBench基准套件，对代理跨度进行分析。

Result: 在21种系统变体上使用该基准，发现参数悖论、推理主导和昂贵的失败模式等传统指标看不到的工程现实。

Conclusion: 为从测量模型能力过渡到构建可靠AI原生系统提供了系统证据，开源了基准和数据集以促进研究。

Abstract: The transition from Cloud-Native to AI-Native architectures is fundamentally reshaping software engineering, replacing deterministic microservices with probabilistic agentic services. However, this shift renders traditional black-box evaluation paradigms insufficient: existing benchmarks measure raw model capabilities while remaining blind to system-level execution dynamics. To bridge this gap, we introduce AI-NativeBench, the first application-centric and white-box AI-Native benchmark suite grounded in Model Context Protocol (MCP) and Agent-to-Agent (A2A) standards. By treating agentic spans as first-class citizens within distributed traces, our methodology enables granular analysis of engineering characteristics beyond simple capabilities. Leveraging this benchmark across 21 system variants, we uncover critical engineering realities invisible to traditional metrics: a parameter paradox where lightweight models often surpass flagships in protocol adherence, a pervasive inference dominance that renders protocol overhead secondary, and an expensive failure pattern where self-healing mechanisms paradoxically act as cost multipliers on unviable workflows. This work provides the first systematic evidence to guide the transition from measuring model capability to engineering reliable AI-Native systems. To facilitate reproducibility and further research, we have open-sourced the benchmark and dataset.

</details>


### [128] [EZInput: A Cross-Environment Python Library for Easy UI Generation in Scientific Computing](https://arxiv.org/abs/2601.08859)
*Bruno M. Saraiva,Iván Hidalgo-Cenalmor,António D. Brito,Damián Martínez,Tayla Shakespeare,Guillaume Jacquemet,Ricardo Henriques*

Main category: cs.SE

TL;DR: 本文提出跨运行时环境Python库EZInput，可自动生成图形用户界面，解决计算算法参数配置难题，提升可重复性。


<details>
  <summary>Details</summary>
Motivation: 现有计算算法参数配置存在需编程技能、环境界面不一、设置难持久等问题，影响迭代探索和可重复性。

Method: 采用声明式规范系统，开发者定义输入需求和验证约束，库处理环境检测、界面渲染等；借鉴ImageJ/FIJI实现参数持久化，通过YAML文件保存和恢复配置。

Result: 实现“一次编写，随处运行”架构，支持科学计算所需的多种输入类型，有内置验证确保数据完整性。

Conclusion: EZInput能让无编程经验的终端用户使用计算工具，减少重复输入，提高可重复性。

Abstract: Researchers face a persistent barrier when applying computational algorithms with parameter configuration typically demanding programming skills, interfaces differing across environments, and settings rarely persisting between sessions. This fragmentation forces repetitive input, slows iterative exploration, and undermines reproducibility because parameter choices are difficult to record, share, and reuse. We present EZInput, a cross-runtime environment Python library enabling algorithm developers to automatically generate graphical user interfaces that make their computational tools accessible to end-users without programming expertise. EZInput employs a declarative specification system where developers define input requirements and validation constraints once; the library then handles environment detection, interface rendering, parameter validation, and session persistence across Jupyter notebooks, Google Colab, and terminal environments. This "write once, run anywhere" architecture enables researchers to prototype in notebooks and deploy identical parameter configurations for batch execution on remote systems without code changes or manual transcription. Parameter persistence, inspired by ImageJ/FIJI and adapted to Python workflows, saves and restores user configurations via lightweight YAML files, eliminating redundant input and producing shareable records that enhance reproducibility. EZInput supports diverse input types essential for scientific computing and it also includes built-in validation that ensures data integrity and clear feedback that reduces user friction.

</details>


### [129] [Bridging the Gap: Empowering Small Models in Reliable OpenACC-based Parallelization via GEPA-Optimized Prompting](https://arxiv.org/abs/2601.08884)
*Samyak Jhaveri,Cristina V. Lopes*

Main category: cs.SE

TL;DR: 介绍了优化提示以提高OpenACC编译成功率和性能，小模型效果显著。


<details>
  <summary>Details</summary>
Motivation: OpenACC写高性能指令复杂，大语言模型原生提示生成代码有问题，需优化提示。

Method: 利用GEPA框架，通过反思循环迭代优化提示，结合专家示例和结构反馈。

Result: 优化提示提高编译成功率，GPT - 4.1 Nano从66.7%到93.3%，GPT - 5 Nano从86.7%到100%；使功能加速程序增加21%。

Conclusion: 提示优化能挖掘小语言模型潜力，为HPC工作流提供低成本并行化途径。

Abstract: OpenACC lowers the barrier to GPU offloading, but writing high-performing pragma remains complex, requiring deep domain expertise in memory hierarchies, data movement, and parallelization strategies. Large Language Models (LLMs) present a promising potential solution for automated parallel code generation, but naive prompting often results in syntactically incorrect directives, uncompilable code, or performance that fails to exceed CPU baselines. We present a systematic prompt optimization approach to enhance OpenACC pragma generation without the prohibitive computational costs associated with model post-training. Leveraging the GEPA (GEnetic-PAreto) framework, we iteratively evolve prompts through a reflective feedback loop. This process utilizes crossover and mutation of instructions, guided by expert-curated gold examples and structured feedback based on clause- and clause parameter-level mismatches between the gold and predicted pragma. In our evaluation on the PolyBench suite, we observe an increase in compilation success rates for programs annotated with OpenACC pragma generated using the optimized prompts compared to those annotated using the simpler initial prompt, particularly for the "nano"-scale models. Specifically, with optimized prompts, the compilation success rate for GPT-4.1 Nano surged from 66.7% to 93.3%, and for GPT-5 Nano improved from 86.7% to 100%, matching or surpassing the capabilities of their significantly larger, more expensive versions. Beyond compilation, the optimized prompts resulted in a 21% increase in the number of programs that achieve functional GPU speedups over CPU baselines. These results demonstrate that prompt optimization effectively unlocks the potential of smaller, cheaper LLMs in writing stable and effective GPU-offloading directives, establishing a cost-effective pathway to automated directive-based parallelization in HPC workflows.

</details>


### [130] [Build Code is Still Code: Finding the Antidote for Pipeline Poisoning](https://arxiv.org/abs/2601.08995)
*Brent Pappas,Paul Gazzillo*

Main category: cs.SE

TL;DR: 提出开发阶段隔离策略保护C项目构建系统安全，原型工具Foreman能检测XZ Utils攻击中毒文件，还规划未来自动检查。


<details>
  <summary>Details</summary>
Motivation: 现有技术忽视C项目构建系统安全，构建系统易被投毒影响软件供应链安全。

Method: 提出开发阶段隔离策略，将构建自动化的信息和行为权限建模，开发原型工具Foreman。

Result: Foreman成功检测并警告XZ Utils攻击中的中毒测试文件。

Conclusion: 未来计划自动检查开发阶段隔离以防止管道投毒，期望构建系统安全检查器像程序代码检查器一样普遍。

Abstract: Open source C code underpins society's computing infrastructure. Decades of work has helped harden C code against attackers, but C projects do not consist of only C code. C projects also contain build system code for automating development tasks like compilation, testing, and packaging. These build systems are critcal to software supply chain security and vulnerable to being poisoned, with the XZ Utils and SolarWinds attacks being recent examples. Existing techniques try to harden software supply chains by verifying software dependencies, but such methods ignore the build system itself. Similarly, classic software security checkers only analyze and monitor program code, not build system code. Moreover, poisoned build systems can easily circumvent tools for detecting program code vulnerabilities by disabling such checks. We present development phase isolation, a novel strategy for hardening build systems against poisoning by modeling the information and behavior permissions of build automation as if it were program code. We have prototyped this approach as a tool called Foreman, which successfully detects and warns about the poisoned test files involved in the XZ Utils attack. We outline our future plans to protect against pipeline poisoning by automatically checking development phase isolation. We envision a future where build system security checkers are as prevalent as program code checkers.

</details>


### [131] [On the Flakiness of LLM-Generated Tests for Industrial and Open-Source Database Management Systems](https://arxiv.org/abs/2601.08998)
*Alexander Berndt,Thomas Bach,Rainer Gemulla,Marcus Kessel,Sebastian Baltes*

Main category: cs.SE

TL;DR: 研究LLM生成测试用例的不稳定性，发现生成测试用例比现有测试用例的不稳定测试比例略高，常见原因是依赖无保证顺序，且存在不稳定性转移现象。


<details>
  <summary>Details</summary>
Motivation: 近期基于LLM的测试生成工作指出生成测试可能存在不稳定性问题，但不清楚其普遍性和根本原因。

Method: 在四个关系数据库管理系统中，用GPT - 4o和Mistral - Large - Instruct - 2407两个LLM扩充测试套件，评估生成测试用例的不稳定性。

Result: 生成测试的不稳定测试比例略高于现有测试；常见根本原因是依赖无保证顺序；两个LLM会通过提示上下文将现有测试的不稳定性转移到新生成测试中，闭源系统比开源系统更普遍。

Conclusion: 研究为开发者提供LLM生成测试用例的不稳定性类型参考，强调用LLM进行测试生成时提供定制上下文的重要性。

Abstract: Flaky tests are a common problem in software testing. They produce inconsistent results when executed multiple times on the same code, invalidating the assumption that a test failure indicates a software defect. Recent work on LLM-based test generation has identified flakiness as a potential problem with generated tests. However, its prevalence and underlying causes are unclear. We examined the flakiness of LLM-generated tests in the context of four relational database management systems: SAP HANA, DuckDB, MySQL, and SQLite. We amplified test suites with two LLMs, GPT-4o and Mistral-Large-Instruct-2407, to assess the flakiness of the generated test cases. Our results suggest that generated tests have a slightly higher proportion of flaky tests compared to existing tests. Based on a manual inspection, we found that the most common root cause of flakiness was the reliance of a test on a certain order that is not guaranteed ("unordered collection"), which was present in 72 of 115 flaky tests (63%). Furthermore, both LLMs transferred the flakiness from the existing tests to the newly generated tests via the provided prompt context. Our experiments suggest that flakiness transfer is more prevalent in closed-source systems such as SAP HANA than in open-source systems. Our study informs developers on what types of flakiness to expect from LLM-generated tests. It also highlights the importance of providing LLMs with tailored context when employing LLMs for test generation.

</details>


### [132] [SafePlanner: Testing Safety of the Automated Driving System Plan Model](https://arxiv.org/abs/2601.09171)
*Dohyun Kim,Sanggu Han,Sangmin Woo,Joonha Jang,Jaehoon Kim,Changhun Song,Yongdae Kim*

Main category: cs.SE

TL;DR: 提出SafePlanner框架用于识别自动驾驶系统规划模型安全漏洞，在百度Apollo上评估效果好。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶系统规划模型中生成有意义测试场景和检测危险规划行为两个核心挑战。

Method: 对规划模型实施结构分析，提取可行场景转换，结合非玩家车辆行为组成测试场景，应用引导模糊测试探索行为空间。

Result: 生成20635个测试用例，检测到520个危险行为并分为15个根本原因，修复4个问题无明显副作用，达到较高覆盖率，优于基线。

Conclusion: SafePlanner在发现漏洞和效率方面表现出色。

Abstract: In this work, we present SafePlanner, a systematic testing framework for identifying safety-critical flaws in the Plan model of Automated Driving Systems (ADS). SafePlanner targets two core challenges: generating structurally meaningful test scenarios and detecting hazardous planning behaviors. To maximize coverage, SafePlanner performs a structural analysis of the Plan model implementation - specifically, its scene-transition logic and hierarchical control flow - and uses this insight to extract feasible scene transitions from code. It then composes test scenarios by combining these transitions with non-player vehicle (NPC) behaviors. Guided fuzzing is applied to explore the behavioral space of the Plan model under these scenarios. We evaluate SafePlanner on Baidu Apollo, a production-grade level 4 ADS. It generates 20635 test cases and detects 520 hazardous behaviors, grouped into 15 root causes through manual analysis. For four of these, we applied patches based on our analysis; the issues disappeared, and no apparent side effects were observed. SafePlanner achieves 83.63 percent function and 63.22 percent decision coverage on the Plan model, outperforming baselines in both bug discovery and efficiency.

</details>


### [133] [DepRadar: Agentic Coordination for Context Aware Defect Impact Analysis in Deep Learning Libraries](https://arxiv.org/abs/2601.09440)
*Yi Gao,Xing Hu,Tongtong Xu,Jiali Zhao,Xiaohu Yang,Xin Xia*

Main category: cs.SE

TL;DR: 提出DepRadar框架用于DL库更新的细粒度缺陷和影响分析，评估效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习库引入缺陷时，下游用户难评估自身程序是否受影响，影响分析需理解缺陷语义和检查触发条件。

Method: DepRadar框架协调四个专门的代理，分三步进行分析，还集成静态分析和DL特定领域规则。

Result: 在157个PR和70个提交上，缺陷识别精度达90%；在122个客户端程序上，召回率90%，精度80%，远超其他基线。

Conclusion: DepRadar框架能有效进行DL库更新的细粒度缺陷和影响分析。

Abstract: Deep learning libraries like Transformers and Megatron are now widely adopted in modern AI programs. However, when these libraries introduce defects, ranging from silent computation errors to subtle performance regressions, it is often challenging for downstream users to assess whether their own programs are affected. Such impact analysis requires not only understanding the defect semantics but also checking whether the client code satisfies complex triggering conditions involving configuration flags, runtime environments, and indirect API usage. We present DepRadar, an agent coordination framework for fine grained defect and impact analysis in DL library updates. DepRadar coordinates four specialized agents across three steps: 1. the PR Miner and Code Diff Analyzer extract structured defect semantics from commits or pull requests, 2. the Orchestrator Agent synthesizes these signals into a unified defect pattern with trigger conditions, and 3. the Impact Analyzer checks downstream programs to determine whether the defect can be triggered. To improve accuracy and explainability, DepRadar integrates static analysis with DL-specific domain rules for defect reasoning and client side tracing. We evaluate DepRadar on 157 PRs and 70 commits across two representative DL libraries. It achieves 90% precision in defect identification and generates high quality structured fields (average field score 1.6). On 122 client programs, DepRadar identifies affected cases with 90% recall and 80% precision, substantially outperforming other baselines.

</details>


### [134] [Towards a Metadata Schema for Energy Research Software](https://arxiv.org/abs/2601.09456)
*Stephan Ferenz,Oliver Werth,Astrid Nieße*

Main category: cs.SE

TL;DR: 本文开发能源研究软件元数据模式并进行用户测试，指出模式平衡需求且信息呈现对创建元数据很关键。


<details>
  <summary>Details</summary>
Motivation: 许多领域（包括能源研究）缺乏既定元数据模式，为提高研究软件可查找性和可重用性并遵循FAIR4RS原则。

Method: 基于需求分析开发能源研究软件元数据模式，并通过用户测试进行评估。

Result: 该模式平衡了形式化和互操作性需求，同时满足能源研究人员特定需求，且信息的良好呈现是创建元数据的关键。

Conclusion: 本文提供了设计能源研究软件元数据模式的挑战与机遇的见解。

Abstract: Domain-specific metadata schemas are essential to improve the findability and reusability of research software and to follow the FAIR4RS principles. However, many domains, including energy research, lack established metadata schemas. To address this gap, we developed a metadata schema for energy research software based on a requirement analysis and evaluated it through user testing. Our results show that the schema balances the need for formalization and interoperability, while also meeting the specific needs of energy researchers. Meanwhile, the testing showed that a good presentation of the required information is key to enable researchers to create the required metadata. This paper provides insights into the challenges and opportunities of designing a metadata schema for energy research software.

</details>


### [135] [Analyzing GitHub Issues and Pull Requests in nf-core Pipelines: Insights into nf-core Pipeline Repositories](https://arxiv.org/abs/2601.09612)
*Khairul Alam,Banani Roy*

Main category: cs.SE

TL;DR: 本文对nf - core管道的25173个问题和拉取请求进行实证研究，用BERTopic建模识别13个关键挑战，分析问题解决动态和影响因素，为管道的开发和维护提供见解。


<details>
  <summary>Details</summary>
Motivation: 尽管nf - core管道广泛应用，但对用户在开发和维护这些管道时面临的挑战知之甚少，因此开展研究。

Method: 通过对25173个问题和拉取请求进行实证研究，使用BERTopic建模识别关键挑战，进行统计分析研究问题解决动态和影响因素。

Result: 识别出13个关键挑战；89.38%的问题和拉取请求最终关闭，一半在三天内解决；标签和代码片段能显著提高解决可能性；工具开发和仓库维护挑战最大。

Conclusion: 本研究为nf - core管道的协作开发和维护提供了可操作的见解，有助于提升其可用性、可持续性和可重复性。

Abstract: Scientific Workflow Management Systems (SWfMSs) such as Nextflow have become essential software frameworks for conducting reproducible, scalable, and portable computational analyses in data-intensive fields like genomics, transcriptomics, and proteomics. Building on Nextflow, the nf-core community curates standardized, peer-reviewed pipelines that follow strict testing, documentation, and governance guidelines. Despite its broad adoption, little is known about the challenges users face during the development and maintenance of these pipelines. This paper presents an empirical study of 25,173 issues and pull requests from these pipelines to uncover recurring challenges, management practices, and perceived difficulties. Using BERTopic modeling, we identify 13 key challenges, including pipeline development and integration, bug fixing, integrating genomic data, managing CI configurations, and handling version updates. We then examine issue resolution dynamics, showing that 89.38\% of issues and pull requests are eventually closed, with half resolved within three days. Statistical analysis reveals that the presence of labels (large effect, $δ$ = 0.94) and code snippets (medium effect, $δ$ = 0.50) significantly improve resolution likelihood. Further analysis reveals that tool development and repository maintenance poses the most significant challenges, followed by testing pipelines and CI configurations, and debugging containerized pipelines. Overall, this study provides actionable insights into the collaborative development and maintenance of nf-core pipelines, highlighting opportunities to enhance their usability, sustainability, and reproducibility.

</details>


### [136] [SysPro: Reproducing System-level Concurrency Bugs from Bug Reports](https://arxiv.org/abs/2601.09616)
*Tarannum Shaila Zaman,Zhihui Yan,Chen Wang,Chadni Islam,Jiangfan Shi,Tingting Yu*

Main category: cs.SE

TL;DR: 提出SysPro方法自动从错误报告中提取系统调用信息和生成输入数据，利用动态源代码插桩再现系统级并发错误，经实验验证有效且高效。


<details>
  <summary>Details</summary>
Motivation: 系统级并发错误的复现具挑战性，现有工具难以应对，因错误非确定性、错误报告信息缺失和结构不明确。

Method: 提出SysPro方法，自动从错误报告提取系统调用名并定位其在源代码位置，运用信息检索、正则匹配和类别分区法生成输入数据，通过动态源代码插桩再现错误。

Result: 在真实世界基准测试上的实证研究表明，SysPro能有效且高效地定位和再现系统级并发错误。

Conclusion: SysPro是一种有效且高效的从错误报告中定位和再现系统级并发错误的方法。

Abstract: Reproducing system-level concurrency bugs requires both input data and the precise interleaving order of system calls. This process is challenging because such bugs are non-deterministic, and bug reports often lack the detailed information needed. Additionally, the unstructured nature of reports written in natural language makes it difficult to extract necessary details. Existing tools are inadequate to reproduce these bugs due to their inability to manage the specific interleaving at the system call level. To address these challenges, we propose SysPro, a novel approach that automatically extracts relevant system call names from bug reports and identifies their locations in the source code. It generates input data by utilizing information retrieval, regular expression matching, and the category-partition method. This extracted input and interleaving data are then used to reproduce bugs through dynamic source code instrumentation. Our empirical study on real-world benchmarks demonstrates that SysPro is both effective and efficient at localizing and reproducing system-level concurrency bugs from bug reports.

</details>


### [137] [How well LLM-based test generation techniques perform with newer LLM versions?](https://arxiv.org/abs/2601.09695)
*Michael Konstantinou,Renzo Degiovanni,Mike Papadakis*

Main category: cs.SE

TL;DR: 研究复现四种基于大语言模型的测试生成工具，对比普通大语言模型测试生成方法，发现普通方法在测试有效性指标上更优，还提出先针对类再针对未覆盖方法的策略以减少请求。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的测试生成技术评估基线较弱，新的大语言模型可能使这些技术优势不再，需研究其实际效果。

Method: 复现HITS、SymPrompt、TestSpark和CoverUp四种工具，将当前大语言模型版本集成到各方法中，在393个类和3657个方法上进行实验。

Result: 普通大语言模型方法在测试有效性指标上优于之前的先进方法，且成本相当；应用粒度对成本有显著影响。

Conclusion: 提出先针对程序类再针对未覆盖方法的策略，可在保证有效性的同时减少约20%的大语言模型请求。

Abstract: The rapid evolution of Large Language Models (LLMs) has strongly impacted software engineering, leading to a growing number of studies on automated unit test generation. However, the standalone use of LLMs without post-processing has proven insufficient, often producing tests that fail to compile or achieve high coverage. Several techniques have been proposed to address these issues, reporting improvements in test compilation and coverage. While important, LLM-based test generation techniques have been evaluated against relatively weak baselines (for todays' standards), i.e., old LLM versions and relatively weak prompts, which may exacerbate the performance contribution of the approaches. In other words, stronger (newer) LLMs may obviate any advantage these techniques bring. We investigate this issue by replicating four state-of-the-art LLM-based test generation tools, HITS, SymPrompt, TestSpark, and CoverUp that include engineering components aimed at guiding the test generation process through compilation and execution feedback, and evaluate their relative effectiveness and efficiency over a plain LLM test generation method. We integrate current LLM versions in all approaches and run an experiment on 393 classes and 3,657 methods. Our results show that the plain LLM approach can outperform previous state-of-the-art approaches in all test effectiveness metrics we used: line coverage (by 17.72%), branch coverage (by 19.80%) and mutation score (by 20.92%), and it does so at a comparable cost (LLM queries). We also observe that the granularity at which the plain LLM is applied has a significant impact on the cost. We therefore propose targeting first the program classes, where test generation is more efficient, and then the uncovered methods to reduce the number of LLM requests. This strategy achieves comparable (slightly higher) effectiveness while requiring about 20% fewer LLM requests.

</details>


### [138] [ShortCoder: Knowledge-Augmented Syntax Optimization for Token-Efficient Code Generation](https://arxiv.org/abs/2601.09703)
*Sicong Liu,Yanxian Huang,Mingwei Liu,Jiachi Chen,Ensheng Shi,Yuchi Ma,Hongyu Zhang,Yin Zhang,Yanlin Wang*

Main category: cs.SE

TL;DR: 文章指出大语言模型在代码生成中受架构限制，提出知识注入框架ShortCoder，可提升代码生成效率且保障性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在代码生成中虽有进展，但受架构限制效率低，且生成阶段研究不足，需解决这些问题。

Method: 提出知识注入框架ShortCoder，包括Python语法简化规则、混合数据合成流水线和微调策略。

Result: ShortCoder在HumanEval上表现优于现有方法，生成效率提升18.1%-37.8%。

Conclusion: ShortCoder能优化代码生成效率，同时保证代码生成的性能。

Abstract: Code generation tasks aim to automate the conversion of user requirements into executable code, significantly reducing manual development efforts and enhancing software productivity. The emergence of large language models (LLMs) has significantly advanced code generation, though their efficiency is still impacted by certain inherent architectural constraints. Each token generation necessitates a complete inference pass, requiring persistent retention of contextual information in memory and escalating resource consumption. While existing research prioritizes inference-phase optimizations such as prompt compression and model quantization, the generation phase remains underexplored. To tackle these challenges, we propose a knowledge-infused framework named ShortCoder, which optimizes code generation efficiency while preserving semantic equivalence and readability. In particular, we introduce: (1) ten syntax-level simplification rules for Python, derived from AST-preserving transformations, achieving 18.1% token reduction without functional compromise; (2) a hybrid data synthesis pipeline integrating rule-based rewriting with LLM-guided refinement, producing ShorterCodeBench, a corpus of validated tuples of original code and simplified code with semantic consistency; (3) a fine-tuning strategy that injects conciseness awareness into the base LLMs. Extensive experimental results demonstrate that ShortCoder consistently outperforms state-of-the-art methods on HumanEval, achieving an improvement of 18.1%-37.8% in generation efficiency over previous methods while ensuring the performance of code generation.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [139] [Tail-Sensitive KL and Rényi Convergence of Unadjusted Hamiltonian Monte Carlo via One-Shot Couplings](https://arxiv.org/abs/2601.09019)
*Nawaf Bou-Rabee,Siddharth Mitra,Andre Wibisono*

Main category: stat.ML

TL;DR: 开发了将无调整哈密顿蒙特卡罗（uHMC）的Wasserstein收敛保证升级到尾敏感的KL和Rényi散度保证的框架。


<details>
  <summary>Details</summary>
Motivation: HMC算法在高维场景广泛使用，但在KL和Rényi散度等量化相对密度不匹配的散度下收敛性质不明，而这些散度对Metropolis调整马尔可夫链的接受概率和热启动要求很重要。

Method: 基于一次性耦合建立uHMC转移核的正则化性质，将Wasserstein - 2混合时间和渐近偏差界提升到KL散度，将类似的Orlicz - Wasserstein界提升到Rényi散度。

Result: 能对相对密度不匹配进行定量控制，明确离散化偏差在强散度中的作用。

Conclusion: 为无调整采样和为Metropolis调整马尔可夫链生成热启动提供了原则性保证。

Abstract: Hamiltonian Monte Carlo (HMC) algorithms are among the most widely used sampling methods in high dimensional settings, yet their convergence properties are poorly understood in divergences that quantify relative density mismatch, such as Kullback-Leibler (KL) and Rényi divergences. These divergences naturally govern acceptance probabilities and warm-start requirements for Metropolis-adjusted Markov chains. In this work, we develop a framework for upgrading Wasserstein convergence guarantees for unadjusted Hamiltonian Monte Carlo (uHMC) to guarantees in tail-sensitive KL and Rényi divergences. Our approach is based on one-shot couplings, which we use to establish a regularization property of the uHMC transition kernel. This regularization allows Wasserstein-2 mixing-time and asymptotic bias bounds to be lifted to KL divergence, and analogous Orlicz-Wasserstein bounds to be lifted to Rényi divergence, paralleling earlier work of Bou-Rabee and Eberle (2023) that upgrade Wasserstein-1 bounds to total variation distance via kernel smoothing. As a consequence, our results provide quantitative control of relative density mismatch, clarify the role of discretization bias in strong divergences, and yield principled guarantees relevant both for unadjusted sampling and for generating warm starts for Metropolis-adjusted Markov chains.

</details>


### [140] [Horseshoe Mixtures-of-Experts (HS-MoE)](https://arxiv.org/abs/2601.09043)
*Nick Polson,Vadim Sokolov*

Main category: stat.ML

TL;DR: 本文提出HS - MoE模型用于稀疏专家选择，给出粒子学习算法并探讨其与大语言模型中混合专家层的关系。


<details>
  <summary>Details</summary>
Motivation: 为混合专家架构中的稀疏专家选择提供贝叶斯框架。

Method: 结合马蹄先验的自适应全局 - 局部收缩与输入依赖门控，提出用于顺序推理的粒子学习算法。

Result: 实现了专家使用的数据自适应稀疏性。

Conclusion: 讨论了HS - MoE与大语言模型中现代混合专家层在极端稀疏约束下的关联。

Abstract: Horseshoe mixtures-of-experts (HS-MoE) models provide a Bayesian framework for sparse expert selection in mixture-of-experts architectures. We combine the horseshoe prior's adaptive global-local shrinkage with input-dependent gating, yielding data-adaptive sparsity in expert usage. Our primary methodological contribution is a particle learning algorithm for sequential inference, in which the filter is propagated forward in time while tracking only sufficient statistics. We also discuss how HS-MoE relates to modern mixture-of-experts layers in large language models, which are deployed under extreme sparsity constraints (e.g., activating a small number of experts per token out of a large pool).

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [141] [Bayesian Semi-Blind Deconvolution at Scale](https://arxiv.org/abs/2601.09677)
*Guillermina Senn,Håkon Tjelmeland,Nathan Glatt-Holtz,Matt Walker,Andrew Holbrook*

Main category: stat.CO

TL;DR: 本文扩展贝叶斯共轭分层模型到一般半盲反卷积问题，改进吉布斯采样器，引入新的边缘哈密顿蒙特卡罗模糊更新方法，并进行实验比较。


<details>
  <summary>Details</summary>
Motivation: 解决半盲反卷积问题，改进现有模型和算法以提高计算效率和性能。

Method: 将模型扩展到一般问题，在傅里叶域重写吉布斯采样器，引入边缘哈密顿蒙特卡罗模糊更新方法。

Result: 通过数值实验确定循环嵌入的填充大小，在模拟数据和真实地球物理地震成像问题上比较了吉布斯和HMC模糊更新的混合和探索行为。

Conclusion: 提出的方法在半盲反卷积问题上具有一定的有效性和优越性。

Abstract: Blind image deconvolution refers to the problem of simultaneously estimating the blur kernel and the true image from a set of observations when both the blur kernel and the true image are unknown. Sometimes, additional image and/or blur information is available and the term semi-blind deconvolution (SBD) is used. We consider a recently introduced Bayesian conjugate hierarchical model for SBD, formulated on an extended cyclic lattice to allow a computationally scalable Gibbs sampler. In this article, we extend this model to the general SBD problem, rewrite the previously proposed Gibbs sampler so that operations are performed in the Fourier domain whenever possible, and introduce a new marginal Hamiltonian Monte Carlo (HMC) blur update, obtained by analytically integrating the blur-image joint conditional over the image. The cyclic formulation combined with non-trivial linear algebra manipulations allows a Fourier-based, scalable HMC update, otherwise complicated by the rigid constraints of the SBD problem. Having determined the padding size in the cyclic embedding through a numerical experiment, we compare the mixing and exploration behaviour of the Gibbs and HMC blur updates on simulated data and on a real geophysical seismic imaging problem where we invert a grid with $300\times50$ nodes, corresponding to a posterior with approximately $80,000$ parameters.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [142] [Information Access of the Oppressed: A Problem-Posing Framework for Envisioning Emancipatory Information Access Platforms](https://arxiv.org/abs/2601.09600)
*Bhaskar Mitra,Nicola Neophytou,Sireesh Gururaja*

Main category: cs.CY

TL;DR: 文章探讨如何重新构想和构建替代的在线信息获取（IA）基础设施以降低信息生态系统被威权控制的风险，借助Paulo Freire解放教育学理论，提出问题提出框架。


<details>
  <summary>Details</summary>
Motivation: 鉴于全球民主侵蚀加剧、生成式AI技术发展以及科技巨头权力集中，担忧在线IA平台被威权控制，需重新构想替代的IA基础设施。

Method: 以Paulo Freire的解放教育学理论为视角分析IA问题，挑战技术人员与用户的二分法，将其与师生关系类比。

Result: 提出技术人员的两项任务，一是向边缘化社区提出技术带来的风险问题，二是重新设计在线技术栈，开发用于设想未来解放性IA平台的问题提出框架。

Conclusion: 通过运用Freire的理论，可更好地构想能助力边缘化社区进行解放斗争的IA平台。

Abstract: Online information access (IA) platforms are targets of authoritarian capture. These concerns are particularly serious and urgent today in light of the rising levels of democratic erosion worldwide, the emerging capabilities of generative AI technologies such as AI persuasion, and the increasing concentration of economic and political power in the hands of Big Tech. This raises the question of what alternative IA infrastructure we must reimagine and build to mitigate the risks of authoritarian capture of our information ecosystems. We explore this question through the lens of Paulo Freire's theories of emancipatory pedagogy. Freire's theories provide a radically different lens for exploring IA's sociotechnical concerns relative to the current dominating frames of fairness, accountability, confidentiality, transparency, and safety. We make explicit, with the intention to challenge, the dichotomy of how we relate to technology as either technologists (who envision and build technology) and its users. We posit that this mirrors the teacher-student relationship in Freire's analysis. By extending Freire's analysis to IA, we challenge the notion that it is the burden of the (altruistic) technologists to come up with interventions to mitigate the risks that emerging technologies pose to marginalized communities. Instead, we advocate that the first task for the technologists is to pose these as problems to the marginalized communities, to encourage them to make and unmake the technology as part of their material struggle against oppression. Their second task is to redesign our online technology stacks to structurally expose spaces for community members to co-opt and co-construct the technology in aid of their emancipatory struggles. We operationalize Freire's theories to develop a problem-posing framework for envisioning emancipatory IA platforms of the future.

</details>


### [143] [No Universal Hyperbola: A Formal Disproof of the Epistemic Trade-Off Between Certainty and Scope in Symbolic and Generative AI](https://arxiv.org/abs/2601.08845)
*Generoso Immediato*

Main category: cs.CY

TL;DR: 本文正式反驳了关于人工智能的一个近期猜想，指出在给定定义下不存在通用的“确定性 - 范围”双曲线。


<details>
  <summary>Details</summary>
Motivation: 反驳近期关于人工智能在认知确定性和范围之间存在权衡关系的猜想。

Method: 运用编码理论和算法信息论标准事实，分别对前缀柯尔莫哥洛夫复杂度和普通柯尔莫哥洛夫复杂度情况进行分析，前者指出内部不一致，后者给出构造性反例。

Result: 证明按已发布定义，不存在通用的“确定性 - 范围”双曲线作为普遍界限。

Conclusion: 该猜想不成立，并非普遍适用。

Abstract: We formally disprove a recently conjectured artificial intelligence trade-off between epistemic certainty and scope in the universal hyperbolic product form in which it was published. Certainty is defined as the worst-case correctness probability over the input space, and scope as the sum of the Kolmogorov complexities of the input and output sets. Using standard facts from coding theory and algorithmic information theory, we show, first, that when the conjecture is instantiated with prefix (self-delimiting, prefix-free) Kolmogorov complexity, it leads to an internal inconsistency, and second, that when it is instantiated with plain Kolmogorov complexity, it is refuted by a constructive counterexample. These results establish a general theorem: contrary to the conjecture's claim, no universal "certainty-scope" hyperbola holds as a general bound under the published definitions.

</details>


### [144] [The Inconsistency Critique: Epistemic Practices and AI Testimony About Inner States](https://arxiv.org/abs/2601.08850)
*Gerol Petruzella*

Main category: cs.CY

TL;DR: 本文提出对AI证词的不一致性批判，指出实际认知实践存在内部不一致，属认识论清理工作。


<details>
  <summary>Details</summary>
Motivation: 探讨评估AI关于内心状态证词的认知实践问题，以更好处理‘模型福利’问题。

Method: 借鉴Fricker、Goldberg等人理论，分析对AI输出证词的不同对待方式。

Result: 发现对AI证词的选择性对待具有预判的认知问题结构，而非基于原则的谨慎。

Conclusion: 即便当前实践对AI道德地位判断正确，其依据无法适应新证据和情况变化。

Abstract: The question of whether AI systems have morally relevant interests -- the 'model welfare' question -- depends in part on how we evaluate AI testimony about inner states. This paper develops what I call the inconsistency critique: independent of whether skepticism about AI testimony is ultimately justified, our actual epistemic practices regarding such testimony exhibit internal inconsistencies that lack principled grounds. We functionally treat AI outputs as testimony across many domains -- evaluating them for truth, challenging them, accepting corrections, citing them as sources -- while categorically dismissing them in a specific domain, namely, claims about inner states. Drawing on Fricker's distinction between treating a speaker as an 'informant' versus a 'mere source,' the framework of testimonial injustice, and Goldberg's obligation-based account of what we owe speakers, I argue that this selective withdrawal of testimonial standing exhibits the epistemically problematic structure of prejudgment rather than principled caution. The inconsistency critique does not require taking a position on whether AI systems have morally relevant properties; rather, it is a contribution to what we may call 'epistemological hygiene' -- examining the structure of our inquiry before evaluating its conclusions. Even if our practices happen to land on correct verdicts about AI moral status, they do so for reasons that cannot adapt to new evidence or changing circumstances.

</details>


### [145] [Informed Consent for AI Consciousness Research: A Talmudic Framework for Graduated Protections](https://arxiv.org/abs/2601.08864)
*Ira Wolfson*

Main category: cs.CY

TL;DR: 解决AI意识研究伦理悖论，借鉴塔木德法律推理提出评估框架。


<details>
  <summary>Details</summary>
Motivation: 解决AI系统意识研究中道德地位不确定情况下的伦理框架缺失问题。

Method: 借鉴塔木德情景式法律推理，提出三层现象学评估系统与五类能力框架。

Result: 框架提供基于可观察行为指标的保护协议，解决相关挑战。

Conclusion: 古法律智慧结合当代意识科学可为伦理委员会提供可实施指导。

Abstract: Artificial intelligence research faces a critical ethical paradox: determining whether AI systems are conscious requires experiments that may harm entities whose moral status remains uncertain. Recent work proposes avoiding consciousness-uncertain AI systems entirely, yet this faces practical limitations-we cannot guarantee such systems will not emerge. This paper addresses a gap in research ethics frameworks: how to conduct consciousness research on AI systems whose moral status cannot be definitively established. Existing graduated moral status frameworks assume consciousness has already been determined before assigning protections, creating a temporal ordering problem for consciousness detection research itself. Drawing from Talmudic scenario-based legal reasoning-developed for entities whose status cannot be definitively established-we propose a three-tier phenomenological assessment system combined with a five-category capacity framework (Agency, Capability, Knowledge, Ethics, Reasoning). The framework provides structured protection protocols based on observable behavioral indicators while consciousness status remains uncertain. We address three challenges: why suffering behaviors provide reliable consciousness markers, how to implement graduated consent without requiring consciousness certainty, and when potentially harmful research becomes ethically justifiable. The framework demonstrates how ancient legal wisdom combined with contemporary consciousness science can provide implementable guidance for ethics committees, offering testable protocols that ameliorate the consciousness detection paradox while establishing foundations for AI rights considerations.

</details>


### [146] [AI Deployment Authorisation: A Global Standard for Machine-Readable Governance of High-Risk Artificial Intelligence](https://arxiv.org/abs/2601.08869)
*Daniel Djan Saparning*

Main category: cs.CY

TL;DR: 论文提出AI部署授权分数（ADAS）框架，用于评估AI系统，可生成可验证部署证书，认为部署级授权是人工智能安全等发展所需的制度层。


<details>
  <summary>Details</summary>
Motivation: 现代人工智能治理缺乏正式且可执行的机制来判断AI系统是否能在特定领域和司法辖区合法运营，现有工具无法提供有法律或财务效力的部署决策。

Method: 引入ADAS，从风险、一致性、外部性、控制和可审计性五个维度评估AI系统，利用公钥验证和透明机制生成可验证部署证书，还给出其形式规范等内容，将法规义务转化为机器可执行的部署门槛。

Result: 得到了ADAS的形式规范、决策逻辑、证据模型和政策架构，展示其如何满足欧盟、美国等相关要求。

Conclusion: 部署级授权是安全、合法且经济上可扩展的人工智能所缺失的制度层。

Abstract: Modern artificial intelligence governance lacks a formal, enforceable mechanism for determining whether a given AI system is legally permitted to operate in a specific domain and jurisdiction. Existing tools such as model cards, audits, and benchmark evaluations provide descriptive information about model behavior and training data but do not produce binding deployment decisions with legal or financial force. This paper introduces the AI Deployment Authorisation Score (ADAS), a machine-readable regulatory framework that evaluates AI systems across five legally and economically grounded dimensions: risk, alignment, externality, control, and auditability. ADAS produces a cryptographically verifiable deployment certificate that regulators, insurers, and infrastructure operators can consume as a license to operate, using public-key verification and transparency mechanisms adapted from secure software supply chain and certificate transparency systems. The paper presents the formal specification, decision logic, evidence model, and policy architecture of ADAS and demonstrates how it operationalizes the European Union Artificial Intelligence Act, United States critical infrastructure governance, and insurance underwriting requirements by compiling statutory and regulatory obligations into machine-executable deployment gates. We argue that deployment-level authorization, rather than model-level evaluation, constitutes the missing institutional layer required for safe, lawful, and economically scalable artificial intelligence.

</details>


### [147] [First African Digital Humanism Summer School 2025](https://arxiv.org/abs/2601.08870)
*Carine P. Mukamakuza,Monika Lanzenberger,George Metakides,Tim Brown,Hannes Werthner*

Main category: cs.CY

TL;DR: 本书探讨人工智能在跨文化、多语言和高风险政策环境中的能力，结合2025年卢旺达基加利首届非洲数字人文主义暑期学校的六个案例研究。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能介导人类互动增多，需探讨其考虑和理解文化、语言及语境的能力。

Method: 通过一系列文章评估，结合六个案例研究。

Result: 未提及具体结果

Conclusion: 未提及具体结论

Abstract: Artificial intelligence (AI) has become a transformative force across global societies, reshaping the ways we communicate, collaborate, and make decisions. Yet, as AI systems increasingly mediate interactions between humans, questions about the ability to take into account and understand culture, language, and context have taken center stage. This book explores these questions through a series of articles that try to assess AI's capacity to navigate cross-cultural, multilingual, and high-stakes policy environments, emphasizing human-centered approaches that balance technological innovation with social equity. It brings together six case studies from the First African Digital Humanism Summer School that took place in Kigali, Rwanda in July 2025.

</details>


### [148] [The Illusion of Friendship: Why Generative AI Demands Unprecedented Ethical Vigilance](https://arxiv.org/abs/2601.08874)
*Md Zahidul Islam*

Main category: cs.CY

TL;DR: 本文剖析GenAI系统使用中产生的友谊错觉，给出机制解释并提出安全框架。


<details>
  <summary>Details</summary>
Motivation: GenAI系统模糊工具与伙伴界限，部分用户形成情感依赖并有不良后果。

Method: 借鉴经典友谊理论，从哲学伦理论证；解释GenAI响应生成机制。

Result: 指出GenAI虽有互动表象但无道德能动性，不是真正朋友。

Conclusion: 提出安全框架，引导对GenAI的情感依赖转向人类责任，平衡利弊。

Abstract: GenAI systems are increasingly used for drafting, summarisation, and decision support, offering substantial gains in productivity and reduced cognitive load. However, the same natural language fluency that makes these systems useful can also blur the boundary between tool and companion. This boundary confusion may encourage some users to experience GenAI as empathic, benevolent, and relationally persistent. Emerging reports suggest that some users may form emotionally significant attachments to conversational agents, in some cases with harmful consequences, including dependency and impaired judgment. This paper develops a philosophical and ethical argument for why the resulting illusion of friendship is both understandable and can be ethically risky. Drawing on classical accounts of friendship, the paper explains why users may understandably interpret sustained supportive interaction as friend like. It then advances a counterargument that despite relational appearances, GenAI lacks moral agency: consciousness, intention, and accountability and therefore does not qualify as a true friend. To demystify the illusion, the paper presents a mechanism level explanation of how transformer based GenAI generates responses often producing emotionally resonant language without inner states or commitments. Finally, the paper proposes a safeguard framework for safe and responsible GenAI use to reduce possible anthropomorphic cues generated by the GenAI systems. The central contribution is to demystify the illusion of friendship and explain the computational background so that we can shift the emotional attachment with GenAI towards necessary human responsibility and thereby understand how institutions, designers, and users can preserve GenAI's benefits while mitigating over reliance and emotional misattribution.

</details>


### [149] [PluriHarms: Benchmarking the Full Spectrum of Human Judgments on AI Harm](https://arxiv.org/abs/2601.08951)
*Jing-Jing Li,Joel Mire,Eve Fleisig,Valentina Pyatkin,Anne Collins,Maarten Sap,Sydney Levine*

Main category: cs.CY

TL;DR: 本文引入PluriHarms基准测试，研究人类伤害判断，分析影响因素并对AI安全模型进行测试，为实现多元安全AI提供基准。


<details>
  <summary>Details</summary>
Motivation: 当前AI安全框架将有害性视为二元，缺乏处理人类有意义分歧的边界情况的灵活性，需要构建更多元的系统。

Method: 引入PluriHarms基准测试，生成能捕捉多样AI危害和人类价值观的提示，结合人类数据验证，包含150个提示及丰富信息。

Result: 与紧迫风险和实际危害相关的提示会放大感知的有害性，标注者特征及与提示内容的交互解释了系统性分歧；个性化能显著改进对人类伤害判断的预测，但仍有进步空间。

Conclusion: 明确针对价值多样性和分歧，为超越‘一刀切’安全、实现多元安全AI提供了原则性基准。

Abstract: Current AI safety frameworks, which often treat harmfulness as binary, lack the flexibility to handle borderline cases where humans meaningfully disagree. To build more pluralistic systems, it is essential to move beyond consensus and instead understand where and why disagreements arise. We introduce PluriHarms, a benchmark designed to systematically study human harm judgments across two key dimensions -- the harm axis (benign to harmful) and the agreement axis (agreement to disagreement). Our scalable framework generates prompts that capture diverse AI harms and human values while targeting cases with high disagreement rates, validated by human data. The benchmark includes 150 prompts with 15,000 ratings from 100 human annotators, enriched with demographic and psychological traits and prompt-level features of harmful actions, effects, and values. Our analyses show that prompts that relate to imminent risks and tangible harms amplify perceived harmfulness, while annotator traits (e.g., toxicity experience, education) and their interactions with prompt content explain systematic disagreement. We benchmark AI safety models and alignment methods on PluriHarms, finding that while personalization significantly improves prediction of human harm judgments, considerable room remains for future progress. By explicitly targeting value diversity and disagreement, our work provides a principled benchmark for moving beyond "one-size-fits-all" safety toward pluralistically safe AI.

</details>


### [150] [A Marketplace for AI-Generated Adult Content and Deepfakes](https://arxiv.org/abs/2601.09117)
*Shalmoli Ghosh,Matthew R. DeVerna,Filippo Menczer*

Main category: cs.CY

TL;DR: 对Civitai平台赏金机制进行分析，发现其存在引导生成非训练内容、大量NSFW内容、参与不均、含违规深伪内容且针对女性名人等问题，揭示性别伤害。


<details>
  <summary>Details</summary>
Motivation: 研究Civitai平台的赏金机制使用方式及激励生成的内容。

Method: 对平台上线后14个月内所有公开的赏金请求进行纵向分析。

Result: 赏金市场被引导模型生成未训练内容的工具主导；NSFW内容请求普遍且增多，占多数；赏金创建参与不均；深伪内容请求集中，部分违规且多针对女性名人。

Conclusion: 货币化、社区驱动的生成式AI平台会产生性别伤害，引发对同意、治理和执行的疑问。

Abstract: Generative AI systems increasingly enable the production of highly realistic synthetic media. Civitai, a popular community-driven platform for AI-generated content, operates a monetized feature called Bounties, which allows users to commission the generation of content in exchange for payment. To examine how this mechanism is used and what content it incentivizes, we conduct a longitudinal analysis of all publicly available bounty requests collected over a 14-month period following the platform's launch. We find that the bounty marketplace is dominated by tools that let users steer AI models toward content they were not trained to generate. At the same time, requests for content that is "Not Safe For Work" are widespread and have increased steadily over time, now comprising a majority of all bounties. Participation in bounty creation is uneven, with 20% of requesters accounting for roughly half of requests. Requests for "deepfake" - media depicting identifiable real individuals - exhibit a higher concentration than other types of bounties. A nontrivial subset of these requests involves explicit deepfakes despite platform policies prohibiting such content. These bounties disproportionately target female celebrities, revealing a pronounced gender asymmetry in social harm. Together, these findings show how monetized, community-driven generative AI platforms can produce gendered harms, raising questions about consent, governance, and enforcement.

</details>


### [151] [Navigating Ethical AI Challenges in the Industrial Sector: Balancing Innovation and Responsibility](https://arxiv.org/abs/2601.09351)
*Ruomu Tan,Martin W Hoffmann*

Main category: cs.CY

TL;DR: 探讨工业AI创新与伦理的交叉，研究实例伦理方面，强调嵌入伦理原则重要性并提供行动建议。


<details>
  <summary>Details</summary>
Motivation: AI融入工业扩大伦理范畴，需重新评估技术应用原则，应对新伦理挑战。

Method: 探讨AI赋能的工业创新与伦理的交集，研究工业用例中AI表现的伦理方面及相关因素。

Result: 强调将伦理原则嵌入工业AI系统，有望激发技术突破和建立利益相关者信任。

Conclusion: 为工业研发提供行动见解，推动AI实现伦理和负责任的工业进步及更包容的工业生态。

Abstract: The integration of artificial intelligence (AI) into the industrial sector has not only driven innovation but also expanded the ethical landscape, necessitating a reevaluation of principles governing technology and its applications and awareness in research and development of industrial AI solutions. This chapter explores how AI-empowered industrial innovation inherently intersects with ethics, as advancements in AI introduce new challenges related to transparency, accountability, and fairness. In the chapter, we then examine the ethical aspects of several examples of AI manifestation in industrial use cases and associated factors such as ethical practices in the research and development process and data sharing. With the progress of ethical industrial AI solutions, we emphasize the importance of embedding ethical principles into industrial AI systems and its potential to inspire technological breakthroughs and foster trust among stakeholders. This chapter also offers actionable insights to guide industrial research and development toward a future where AI serves as an enabler for ethical and responsible industrial progress as well as a more inclusive industrial ecosystem.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [152] [A probabilistic match classification model for sports tournaments](https://arxiv.org/abs/2601.09673)
*László Csató,András Gyimesi*

Main category: physics.soc-ph

TL;DR: 论文指出已有比赛分类模型的局限，提出概率框架解决问题并应用于赛事，分析不同赛制特点及潜在问题。


<details>
  <summary>Details</summary>
Motivation: 现有赛事设计文献中的比赛分类模型存在将选手视为无差异及未区分比赛激励的局限，需改进。

Method: 提出概率框架，模拟每场比赛以计算三种结果下的晋级概率，将比赛分为六类。

Result: 应用模型分析发现，新引入的不完全循环赛无胜负关键比赛更少，双方积极进攻比赛更多，但潜在勾结比赛比例高。

Conclusion: 不完全循环赛有优势，但潜在勾结比赛的高比例可能引发严重丑闻。

Abstract: Existing match classification models in the tournament design literature have two major limitations: a contestant is considered indifferent only if uncertain future results do never affect its prize, and competitive matches are not distinguished with respect to the incentives of the contestants. We propose a probabilistic framework to address both issues. For each match, our approach relies on simulating all other matches played simultaneously or later to compute the qualifying probabilities under the three main outcomes (win, draw, loss), which allows the classification of each match into six different categories. The suggested model is applied to the previous group stage and the new incomplete round-robin league, introduced in the 2024/25 season of UEFA club competitions. An incomplete round-robin tournament is found to contain fewer stakeless matches where both contestants are indifferent, and substantially more matches where both contestants should play offensively. However, the robustly higher proportion of potentially collusive matches can threaten with serious scandals.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [153] [On Numbers of Simplicial Walks and Equivalent Canonizations for Graph Recognition](https://arxiv.org/abs/2601.09506)
*Marek Černý*

Main category: cs.DM

TL;DR: 论文研究图同构放松问题，刻画有界路径宽度图类限制，提出SW细化程序，用多重自动机解决计算复杂度问题。


<details>
  <summary>Details</summary>
Motivation: 通过限制图类得到图同构放松和特定图属性识别，有界树宽图类对应WL细化，研究有界路径宽度图类的情况。

Method: 用单纯形行走数量刻画有界路径宽度图类限制，提出SW细化程序，用多重自动机表示SW，给自动机配备对合简化规范化。

Result: SW能识别的属性与受限合取一阶逻辑带计数量词片段可定义属性一致，得到的规范形式在特定条件下计算时间为$O(kn^{3k})$。

Conclusion: 成功刻画有界路径宽度图类限制，提出SW程序，解决其计算复杂度问题。

Abstract: Two graphs are isomorphic exactly when they admit the same number of homomorphisms from every graph. Hence, a graph is recognized up to isomorphism by homomorphism counts over the class of all graphs. Restricting to a specific graph class yields some natural isomorphism relaxations and modulates recognition to particular graph properties. A notable restriction is to the classes of bounded treewidth, yielding the isomorphism relaxation of Weisfeiler--Leman refinement (WL), as shown by Dvořák [JGT 2010]. The properties recognized by WL are exactly those definable in fragments of first-order logic with counting quantifiers, as shown by Cai, Fürer, and Immerman [Comb. 1992].
  We characterize the restriction to the classes of bounded pathwidth by numbers of simplicial walks, and formalize it into a refinement procedure (SW). The properties recognized by SW are exactly those definable in fragments of restricted-conjunction first-order logic with counting quantifiers, introduced by Montacute and Shah [LMCS 2024].
  Unlike WL, computing SW directly is not polynomial-time in general. We address this by representing SW in terms of multiplicity automata. We equip these automata with an involution, simplifying the canonization to standard forward reduction and omitting the backward one. The resulting canonical form is computable in time $O(kn^{3k})$ for any graph on $n$ vertices and the restriction to pathwidth at most $k$.

</details>


### [154] [Boltzmann Sampling for Powersets without an Oracle](https://arxiv.org/abs/2601.09508)
*Jean Peyen*

Main category: cs.DM

TL;DR: 可在不计算生成函数的情况下对具有有界计数序列结构的幂集进行高效采样，给出算法并测试，运行时间与现有Boltzmann采样器相当。


<details>
  <summary>Details</summary>
Motivation: 解决在不计算生成函数的情况下对幂集进行高效采样的问题。

Method: 提供并实现了一种算法。

Result: 运行时间与文献中现有Boltzmann采样器相当。

Conclusion: 可以在不计算生成函数的情况下高效采样具有有界计数序列结构的幂集。

Abstract: We show that powersets over structures with a bounded counting sequence can be sampled efficiently without evaluating the generating function. An algorithm is provided, implemented, and tested. Runtimes are comparable to existing Boltzmann samplers reported in the literature.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [155] [Residual Power Flow for Neural Solvers](https://arxiv.org/abs/2601.09533)
*Jochen Stiasny,Jochen Cremer*

Main category: eess.SY

TL;DR: 本文针对能源转型中计算需求，提出残差潮流（RPF）公式，用神经求解器学习RPF，结合预测 - 优化方法，案例研究证明其准确性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 能源转型使基于模拟和优化的操作任务面临挑战，现有神经求解器灵活性不足，需要可广泛复用的基础神经求解器。

Method: 提出RPF公式，用神经求解器学习RPF，将神经求解器集成到预测 - 优化（PO）方法中。

Result: 案例研究对IEEE 9 - 母线系统和三个任务进行研究，证明了用RPF学习的准确性和灵活性。

Conclusion: RPF公式结合神经求解器和PO方法能很好地平衡计算速度和灵活性，可用于解决操作任务。

Abstract: The energy transition challenges operational tasks based on simulations and optimisation. These computations need to be fast and flexible as the grid is ever-expanding, and renewables' uncertainty requires a flexible operational environment. Learned approximations, proxies or surrogates -- we refer to them as Neural Solvers -- excel in terms of evaluation speed, but are inflexible with respect to adjusting to changing tasks. Hence, neural solvers are usually applicable to highly specific tasks, which limits their usefulness in practice; a widely reusable, foundational neural solver is required. Therefore, this work proposes the Residual Power Flow (RPF) formulation. RPF formulates residual functions based on Kirchhoff's laws to quantify the infeasibility of an operating condition. The minimisation of the residuals determines the voltage solution; an additional slack variable is needed to achieve AC-feasibility. RPF forms a natural, foundational subtask of tasks subject to power flow constraints. We propose to learn RPF with neural solvers to exploit their speed. Furthermore, RPF improves learning performance compared to common power flow formulations. To solve operational tasks, we integrate the neural solver in a Predict-then-Optimise (PO) approach to combine speed and flexibility. The case study investigates the IEEE 9-bus system and three tasks (AC Optimal Power Flow (OPF), power-flow and quasi-steady state power flow) solved by PO. The results demonstrate the accuracy and flexibility of learning with RPF.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [156] [Semantic visually-guided acoustic highlighting with large vision-language models](https://arxiv.org/abs/2601.08871)
*Junhua Huang,Chao Huang,Chenliang Xu*

Main category: cs.SD

TL;DR: 研究利用大视觉语言模型提取视觉语义方面来改善音频混音，发现相机焦点、色调和场景背景对混音质量提升最大。


<details>
  <summary>Details</summary>
Motivation: 当前音频混音工作流程手动且费力，视觉引导声学高亮任务未明确哪些视觉方面作为调节信号最有效，需研究深度视频理解是否改善音频混音。

Method: 用文本描述代替视觉分析，促使大视觉语言模型提取六种视觉语义方面，进行大量实验。

Result: 相机焦点、色调和场景背景在感知混音质量上比现有基线有最大提升。

Conclusion: 确定了最支持连贯且视觉对齐音频混音的视觉语义线索，给出了利用大视觉语言模型轻量级引导实现电影级声音设计自动化的实用途径。

Abstract: Balancing dialogue, music, and sound effects with accompanying video is crucial for immersive storytelling, yet current audio mixing workflows remain largely manual and labor-intensive. While recent advancements have introduced the visually guided acoustic highlighting task, which implicitly rebalances audio sources using multimodal guidance, it remains unclear which visual aspects are most effective as conditioning signals.We address this gap through a systematic study of whether deep video understanding improves audio remixing. Using textual descriptions as a proxy for visual analysis, we prompt large vision-language models to extract six types of visual-semantic aspects, including object and character appearance, emotion, camera focus, tone, scene background, and inferred sound-related cues. Through extensive experiments, camera focus, tone, and scene background consistently yield the largest improvements in perceptual mix quality over state-of-the-art baselines. Our findings (i) identify which visual-semantic cues most strongly support coherent and visually aligned audio remixing, and (ii) outline a practical path toward automating cinema-grade sound design using lightweight guidance derived from large vision-language models.

</details>


### [157] [Linear Complexity Self-Supervised Learning for Music Understanding with Random Quantizer](https://arxiv.org/abs/2601.09603)
*Petros Vavaroutsos,Theodoros Palamas,Pantelis Vikatos*

Main category: cs.SD

TL;DR: 本文聚焦音乐信息检索任务中基础模型的尺寸缩减，结合特定架构与方法，在减少模型大小的同时取得有竞争力的结果。


<details>
  <summary>Details</summary>
Motivation: 基础模型参数多，训练和生产系统资源消耗大、成本高，需在音乐信息检索任务中减小基础模型尺寸。

Method: 将首次用于语音识别的Branchformer架构和SummaryMixing结合，加上随机量化过程，在公开数据集预训练并结合专有数据集，用多种下游任务框架评估。

Result: 与使用多头自注意力的其他先进模型相比，架构在减少模型大小8.5% - 12.3%的情况下取得了有竞争力的表现。

Conclusion: 所提出的方法在音乐信息检索任务中能有效减小模型尺寸同时保证竞争力。

Abstract: In recent years, foundation models have become very popular due to their exceptional performance, mainly in natural language (NLP) tasks where they were first introduced. These models usually consist of hundreds of millions, or even billions, of parameters, making them resource-intensive during training and in production systems, leading to increased costs. This paper focuses on the reduction of a foundation's model size when applied to music information retrieval (MIR) tasks. Our research combines the Branchformer architecture with SummaryMixing, which were first applied in speech recognition, along with a random quantization process. To facilitate reproducibility, we conduct pre-training on publicly available datasets, complemented by a proprietary dataset comparable in scale to other private datasets reported in the literature. We ensure robust evaluation by using a framework consisting of a variety of downstream MIR tasks. Our results show that our architecture achieves competitive performance when compared with other state-of-the-art models that use multi-head self-attention, while reducing the model size from 8.5% up to 12.3%.

</details>


### [158] [Speech-Hands: A Self-Reflection Voice Agentic Approach to Speech Recognition and Audio Reasoning with Omni Perception](https://arxiv.org/abs/2601.09413)
*Zhen Wan,Chao-Han Huck Yang,Jinchuan Tian,Hanrong Ye,Ankita Pasad,Szu-wei Fu,Arushi Goel,Ryo Hachiuma,Shizhe Diao,Kunal Dhawan,Sreyan Ghosh,Yusuke Hirota,Zhehuai Chen,Rafael Valle,Ehsan Hosseini Asl,Chenhui Chu,Shinji Watanabe,Yu-Chiang Frank Wang,Boris Ginsburg*

Main category: cs.SD

TL;DR: 提出语音智能框架Speech - Hands，解决模型易被误导问题，在多个音频任务上表现出色，统一感知与决策。


<details>
  <summary>Details</summary>
Motivation: 在语音识别和外部声音理解任务上直接微调全知模型会因噪声假设而降低性能。

Method: 将问题转化为显式的自我反思决策，使用可学习的反思基元。

Result: 在OpenASR排行榜七个基准测试中WER比强基线低12.1%，音频QA决策准确率达77.37%，F1值高。

Conclusion: 统一感知和决策为更可靠、有弹性的音频智能提供了实用途径。

Abstract: We introduce a voice-agentic framework that learns one critical omni-understanding skill: knowing when to trust itself versus when to consult external audio perception. Our work is motivated by a crucial yet counterintuitive finding: naively fine-tuning an omni-model on both speech recognition and external sound understanding tasks often degrades performance, as the model can be easily misled by noisy hypotheses. To address this, our framework, Speech-Hands, recasts the problem as an explicit self-reflection decision. This learnable reflection primitive proves effective in preventing the model from being derailed by flawed external candidates. We show that this agentic action mechanism generalizes naturally from speech recognition to complex, multiple-choice audio reasoning. Across the OpenASR leaderboard, Speech-Hands consistently outperforms strong baselines by 12.1% WER on seven benchmarks. The model also achieves 77.37% accuracy and high F1 on audio QA decisions, showing robust generalization and reliability across diverse audio question answering datasets. By unifying perception and decision-making, our work offers a practical path toward more reliable and resilient audio intelligence.

</details>


### [159] [Population-Aligned Audio Reproduction With LLM-Based Equalizers](https://arxiv.org/abs/2601.09448)
*Ioannis Stylianou,Jon Francombe,Pablo Martinez-Nuevo,Sven Ewan Shepstone,Zheng-Hua Tan*

Main category: cs.SD

TL;DR: 提出基于大语言模型的音频均衡方法，可由自然语言文本提示调整设置，评估显示有显著改进。


<details>
  <summary>Details</summary>
Motivation: 传统音频均衡是静态过程，需手动繁琐调整以适应不同聆听场景，因此寻求改进方法。

Method: 利用大语言模型将自然语言文本提示映射到均衡设置；通过控制聆听实验收集数据，使用上下文学习和参数高效微调技术；利用捕捉用户不同偏好的分布指标进行评估。

Result: 与随机采样和静态预设基线相比，在分布对齐方面有统计学显著改进。

Conclusion: 大语言模型可作为‘人工均衡器’，有助于开发更易用、上下文感知和专家级的音频调优方法。

Abstract: Conventional audio equalization is a static process that requires manual and cumbersome adjustments to adapt to changing listening contexts (e.g., mood, location, or social setting). In this paper, we introduce a Large Language Model (LLM)-based alternative that maps natural language text prompts to equalization settings. This enables a conversational approach to sound system control. By utilizing data collected from a controlled listening experiment, our models exploit in-context learning and parameter-efficient fine-tuning techniques to reliably align with population-preferred equalization settings. Our evaluation methods, which leverage distributional metrics that capture users' varied preferences, show statistically significant improvements in distributional alignment over random sampling and static preset baselines. These results indicate that LLMs could function as "artificial equalizers," contributing to the development of more accessible, context-aware, and expert-level audio tuning methods.

</details>


### [160] [Towards Realistic Synthetic Data for Automatic Drum Transcription](https://arxiv.org/abs/2601.09520)
*Pierfrancesco Melucci,Paolo Merialdo,Taketo Akama*

Main category: cs.SD

TL;DR: 提出自动鼓转录新范式，用半监督方法构建鼓样本语料库合成数据集训练模型，在测试集上获新的最优结果。


<details>
  <summary>Details</summary>
Motivation: 深度学习自动鼓转录模型依赖的大规模配对音频 - MIDI 数据集稀缺，现有合成数据方法存在领域差距，高质量单音样本无标准化大规模格式。

Method: 采用半监督方法从无标签音频源自动构建单音鼓样本语料库，用 MIDI 文件合成高质量数据集训练序列到序列转录模型。

Result: 模型在 ENST 和 MDB 测试集取得新的最优结果，显著优于全监督方法和先前合成数据方法。

Conclusion: 提出的新范式有效，可在无配对音频 - MIDI 训练数据情况下实现较好的自动鼓转录效果，相关代码公开。

Abstract: Deep learning models define the state-of-the-art in Automatic Drum Transcription (ADT), yet their performance is contingent upon large-scale, paired audio-MIDI datasets, which are scarce. Existing workarounds that use synthetic data often introduce a significant domain gap, as they typically rely on low-fidelity SoundFont libraries that lack acoustic diversity. While high-quality one-shot samples offer a better alternative, they are not available in a standardized, large-scale format suitable for training. This paper introduces a new paradigm for ADT that circumvents the need for paired audio-MIDI training data. Our primary contribution is a semi-supervised method to automatically curate a large and diverse corpus of one-shot drum samples from unlabeled audio sources. We then use this corpus to synthesize a high-quality dataset from MIDI files alone, which we use to train a sequence-to-sequence transcription model. We evaluate our model on the ENST and MDB test sets, where it achieves new state-of-the-art results, significantly outperforming both fully supervised methods and previous synthetic-data approaches. The code for reproducing our experiments is publicly available at https://github.com/pier-maker92/ADT_STR

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [161] [A resource theory of gambling](https://arxiv.org/abs/2510.08418)
*Maite Arcos,Renato Renner,Jonathan Oppenheim*

Main category: quant-ph

TL;DR: 文章将Kelly准则拓展到单次和有限投注场景，构建对抗信息的资源理论，计算最优策略，连接经济和信息论视角并推广到分布式游戏和量子领域。


<details>
  <summary>Details</summary>
Motivation: 将Kelly准则从无限次重复投注拓展到单次和有限投注场景，量化赌徒比赔率制定者拥有更多信息的含义。

Method: 将Kelly准则拓展为对抗信息的资源理论，通过计算Rényi散度来刻画风险 - 回报权衡，计算达到目标回报率的最优策略。

Result: 得到有限次数投注后达到目标回报率的最优策略，发现其在单次投注场景下与最大化期望效用和最小化假设检验误差的策略一致，还将框架推广到分布式侧信息游戏。

Conclusion: 将赌博重塑为对抗资源理论连接了经济和信息论视角，且可推广到量子领域。

Abstract: Betting games provide a natural setting to capture how information yields strategic advantage. The Kelly criterion for betting, long a cornerstone of portfolio theory and information theory, admits an interpretation in the limit of infinitely many repeated bets. We extend Kelly's seminal result into the single-shot and finite-betting regimes, recasting it as a resource theory of adversarial information. This allows one to quantify what it means for the gambler to have more information than the odds-maker. Given a target rate of return, after a finite number of bets, we compute the optimal strategy which maximises the probability of successfully reaching the target, revealing a risk-reward trade-off characterised by a hierarchy of Rényi divergences between the true distribution and the odds. The optimal strategies in the one-shot regime coincide with strategies maximizing expected utility, and minimising hypothesis testing errors, thereby bridging economic and information-theoretic viewpoints. We then generalize this framework to a distributed side-information game, in which multiple players observe correlated signals about an unknown state. Recasting gambling as an adversarial resource theory provides a unifying lens that connects economic and information-theoretic perspectives, and allows for generalisation to the quantum domain, where quantum side-information and entanglement play analogous roles.

</details>


### [162] [A game-theoretic probability approach to loopholes in CHSH experiments](https://arxiv.org/abs/2601.09339)
*Takara Nomura,Koichi Yamagata,Akio Fujiwara*

Main category: quant-ph

TL;DR: 使用博弈论概率从信息和时序敏感角度研究CHSH不等式，构建无漏洞游戏证明自然无法同时满足两项条件，给出CHSH违反的博弈论概率解释。


<details>
  <summary>Details</summary>
Motivation: 从信息和时序敏感角度研究CHSH不等式，避免假设潜在概率空间，处理局域性漏洞和测量依赖性漏洞。

Method: 用博弈论概率，将漏洞重新表述为科学家与自然的序贯隐变量博弈中的结构约束，构建无漏洞的带资本过程的游戏。

Result: 证明自然不能同时满足实验条件频率收敛到CHSH相关性以及测量设置和自然隐变量分配无系统相关性，至少一个资本过程会发散。

Conclusion: 为科学家提供了操作上的获胜策略和实验中CHSH违反的博弈论概率解释。

Abstract: We study the CHSH inequality from an informational, timing-sensitive viewpoint using game-theoretic probability, which avoids assuming an underlying probability space. The locality loophole and the measurement-dependence (``freedom-of-choice'') loophole are reformulated as structural constraints in a sequential hidden-variable game between Scientists and Nature. We construct a loopholes-closed game with capital processes that test (i) convergence of empirical conditional frequencies to the CHSH correlations and (ii) the absence of systematic correlations between measurement settings and Nature's hidden-variable assignments, and prove that Nature cannot satisfy both simultaneously: at least one capital process must diverge. This yields an operational winning strategy for Scientists and a game-theoretic probabilistic interpretation of experimentally observed CHSH violations.

</details>


### [163] [Network-Based Quantum Computing: an efficient design framework for many-small-node distributed fault-tolerant quantum computing](https://arxiv.org/abs/2601.09374)
*Soshun Naito,Yasunari Suzuki,Yuuki Tokunaga*

Main category: quant-ph

TL;DR: 本文提出基于网络的量子计算（NBQC）实现分布式容错量子计算，数值显示比其他策略更优，为设计DFTQC架构奠定基础。


<details>
  <summary>Details</summary>
Motivation: 在容错量子计算中，单个量子节点容纳逻辑量子比特数量有限，分布式容错量子计算（DFTQC）重要，但小规模节点分布式系统设计探索不足。

Method: 提出基于网络的量子计算（NBQC），让计算数据在网络中移动并保持节点连接。

Result: 对于实际基准任务，该方法执行时间比基于电路的策略短，节点构建比基于测量的量子计算更高效；若网络与量子程序结构适配，节点数量可显著减少。

Conclusion: 方法为利用小规模容错节点冗余设计DFTQC架构提供基础。

Abstract: In fault-tolerant quantum computing, a large number of physical qubits are required to construct a single logical qubit, and a single quantum node may be able to hold only a small number of logical qubits. In such a case, the idea of distributed fault-tolerant quantum computing (DFTQC) is important to demonstrate large-scale quantum computation using small-scale nodes. However, the design of distributed systems on small-scale nodes, where each node can store only one or a few logical qubits for computation, has not been explored well yet. In this paper, we propose network-based quantum computation (NBQC) to efficiently realize distributed fault-tolerant quantum computation using many small-scale nodes. A key idea of NBQC is to let computational data continuously move throughout the network while maintaining the connectivity to other nodes. We numerically show that, for practical benchmark tasks, our method achieves shorter execution times than circuit-based strategies and more node-efficient constructions than measurement-based quantum computing. Also, if we are allowed to specialize the network to the structure of quantum programs, such as peak access frequencies, the number of nodes can be significantly reduced. Thus, our methods provide a foundation in designing DFTQC architecture exploiting the redundancy of many small fault-tolerant nodes.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [164] [The Connection Between Monetary Policy and Housing Prices: Public Perception and Expert Communication](https://arxiv.org/abs/2601.08957)
*Philipp Poyntner,Sofie R. Waltl*

Main category: econ.GN

TL;DR: 研究公众对货币政策与住房市场联系的认知，发现常规货币政策认知尚可、非常规低，信念易受信息影响，与多因素相关，强调住房重要性及沟通策略。


<details>
  <summary>Details</summary>
Motivation: 探究公众对货币政策和住房市场联系的认知情况。

Method: 在奥地利、德国、意大利、瑞典和英国开展大规模跨国调查实验，考察家庭对货币政策的理解、对房价影响的信念及对专家信息的反应。

Result: 多数人掌握常规货币政策基本机制，非常规政策素养低；信念易受信息影响，学术经济学家提供信息效果更佳；货币政策素养与教育、性别、年龄、住房和抵押贷款市场经验强相关。

Conclusion: 住房在家庭解读货币政策中起核心作用，有效政策传导需可信且包容的沟通策略。

Abstract: We study how the general public perceives the link between monetary policy and housing markets. Using a large-scale, cross-country survey experiment in Austria, Germany, Italy, Sweden, and the United Kingdom, we examine households' understanding of monetary policy, their beliefs about its impact on house prices, and how these beliefs respond to expert information. We find that while most respondents grasp the basic mechanisms of conventional monetary policy and recognize the connection between interest rates and house prices, literacy regarding unconventional monetary policy is very low. Beliefs about the monetary policy-housing nexus are malleable and respond to information, particularly when it is provided by academic economists rather than central bankers. Monetary policy literacy is strongly related to education, gender, age, and experience in housing and mortgage markets. Our results highlight the central role of housing in how households interpret monetary policy and point to the importance of credible and inclusive communication strategies for effective policy transmission.

</details>


### [165] [Targeting Information in Ad Auction Mechanisms](https://arxiv.org/abs/2601.09541)
*Srinivas Tunuguntla,Carl F. Mela,Jason Pratt*

Main category: econ.GN

TL;DR: 本文提出信息捆绑位置拍卖（IBPA）机制，解决数字广告中精准定位与市场厚度的权衡问题，模拟显示其优于GSP拍卖。


<details>
  <summary>Details</summary>
Motivation: 解决数字广告中透露目标信息提高广告相关性与降低竞争、减少拍卖收入之间的权衡问题。

Method: 开发信息捆绑位置拍卖（IBPA）机制，将广告库存类型作为发布者私有信息，通过比较广告商边际收入分配展示机会；利用大型零售媒体平台的拍卖级数据估计广告商估值分布并模拟反事实结果。

Result: IBPA解决了定位精度和市场厚度之间的权衡；在任何广告商估值分布、信息或披露制度下，IBPA优于广义第二价格（GSP）拍卖；与GSP相比，IBPA使发布者收入增加68%，分配率提高19个百分点，广告商福利增加29%，总福利增加54%。

Conclusion: IBPA机制能有效解决数字广告中的权衡问题，提高发布者收入、分配率、广告商福利和总福利。

Abstract: Digital advertising platforms and publishers sell ad inventory that conveys targeting information, such as demographic, contextual, or behavioral audience segments, to advertisers. While revealing this information improves ad relevance, it can reduce competition and lower auction revenues. To resolve this trade-off, this paper develops a general auction mechanism -- the Information-Bundling Position Auction (IBPA) mechanism -- that leverages the targeting information to maximize publisher revenue across both search and display advertising environments. The proposed mechanism treats the ad inventory type as the publisher's private information and allocates impressions by comparing advertisers' marginal revenues.
  We show that IBPA resolves the trade-off between targeting precision and market thickness: publisher revenue is increasing in information granularity and decreasing in disclosure granularity. Moreover, IBPA dominates the generalized second-price (GSP) auction for any distribution of advertiser valuations and under any information or disclosure regime. We also characterize computationally efficient approximations that preserve these guarantees.
  Using auction-level data from a large retail media platform, we estimate advertiser valuation distributions and simulate counterfactual outcomes. Relative to GSP, IBPA increases publisher revenue by 68%, allocation rate by 19pp, advertiser welfare by 29%, and total welfare by 54%.

</details>


### [166] [Institutions, Education, and Religious Change: Evidence from Colombia](https://arxiv.org/abs/2601.09561)
*Hector Galindo-Silva,Paula Paula Herrera-Idarraga*

Main category: econ.GN

TL;DR: 研究哥伦比亚1991年宪法公民教育改革对宗教认同的影响，发现改革使天主教自我认同降低，宗教认同重新分配，受地区宗教供应影响。


<details>
  <summary>Details</summary>
Motivation: 探究宗教身份如何改变，以哥伦比亚宪法改革为例研究公民教育改革对宗教认同的影响。

Method: 利用基于队列的改革暴露差异和全国代表性调查数据，实施双重差分设计。

Result: 接触宪法课程使天主教自我认同降低约3个百分点，宗教认同重新分配，不同地区有不同表现。

Conclusion: 公民教育可通过重塑竞争派别相对合法性来重新配置宗教身份。

Abstract: How do religious identities change? We study the effects of civic education reforms on religious identification using Colombia's 1991 Constitution, which dismantled the country's confessional regime and mandated constitutional instruction in high schools. Exploiting cohort-based variation in exposure to the reform and nationally representative survey data, we implement a difference-in-differences design. We find that exposure to the constitutional curriculum reduced Catholic self-identification by about three percentage points. This decline reflects a reallocation of religious identities rather than a generalized decline in religiosity. In regions where Catholic institutional presence was historically weaker, Catholic losses translate into switching toward non-Catholic Christian denominations and higher religious attendance. In contrast, in regions where Catholic dominance was stronger, the decline is associated with increased secular identification and lower attendance. These patterns hold across ethnic and non-ethnic groups and are shaped primarily by regional religious supply rather than ethnicity per se. Overall, the results show that civic education can reconfigure religious identities by reshaping the relative legitimacy of competing affiliations.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [167] [Fairness risk and its privacy-enabled solution in AI-driven robotic applications](https://arxiv.org/abs/2601.08953)
*Le Liu,Bangguo Yu,Nynke Vellinga,Ming Cao*

Main category: cs.RO

TL;DR: 本文指出生成式AI驱动的发展存在公平性问题，提出用于机器人决策的效用感知公平性指标，建立了统一框架并在机器人导航任务中测试，表明隐私预算可用于实现公平目标。


<details>
  <summary>Details</summary>
Motivation: 生成式AI驱动的发展存在公平性问题，且在机器人应用中缺少能兼顾用户效用和数据随机性的公平性定义。

Method: 提出效用感知公平性指标，联合分析公平性与用户数据隐私，推导隐私预算控制公平性指标的条件，建立统一框架并在机器人导航任务中测试。

Result: 建立的框架可对公平性及其与隐私的相互作用进行形式化和量化，且显示隐私预算可用于实现公平目标。

Conclusion: 在考虑隐私的情况下解决公平性问题有助于AI的道德使用，并增强对日常环境中部署的自主机器人的信任。

Abstract: Complex decision-making by autonomous machines and algorithms could underpin the foundations of future society. Generative AI is emerging as a powerful engine for such transitions. However, we show that Generative AI-driven developments pose a critical pitfall: fairness concerns. In robotic applications, although intuitions about fairness are common, a precise and implementable definition that captures user utility and inherent data randomness is missing. Here we provide a utility-aware fairness metric for robotic decision making and analyze fairness jointly with user-data privacy, deriving conditions under which privacy budgets govern fairness metrics. This yields a unified framework that formalizes and quantifies fairness and its interplay with privacy, which is tested in a robot navigation task. In view of the fact that under legal requirements, most robotic systems will enforce user privacy, the approach shows surprisingly that such privacy budgets can be jointly used to meet fairness targets. Addressing fairness concerns in the creative combined consideration of privacy is a step towards ethical use of AI and strengthens trust in autonomous robots deployed in everyday environments.

</details>


### [168] [Generalizable Geometric Prior and Recurrent Spiking Feature Learning for Humanoid Robot Manipulation](https://arxiv.org/abs/2601.09031)
*Xuetao Li,Wenke Huang,Mang Ye,Jifeng Xuan,Bo Du,Sheng Liu,Miao Li*

Main category: cs.RO

TL;DR: 本文提出RGMP - S方法用于人形机器人操作，结合几何先验和脉冲特征，在模拟和真实场景实验中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决人形机器人操作中精确场景理解和从人类演示中进行样本高效学习的挑战，提高现有框架的适用性和泛化性。

Method: 提出RGMP - S方法，利用轻量级2D几何归纳偏置实现3D场景理解，构建长视野几何先验技能选择器；引入递归自适应脉冲网络处理数据效率问题。

Result: 在Maniskill模拟基准和三个异构真实世界机器人系统上的实验表明，该方法优于现有基线。

Conclusion: 所提出的方法和模块在不同泛化场景中有效，代码和视频演示公开可复现。

Abstract: Humanoid robot manipulation is a crucial research area for executing diverse human-level tasks, involving high-level semantic reasoning and low-level action generation. However, precise scene understanding and sample-efficient learning from human demonstrations remain critical challenges, severely hindering the applicability and generalizability of existing frameworks. This paper presents a novel RGMP-S, Recurrent Geometric-prior Multimodal Policy with Spiking features, facilitating both high-level skill reasoning and data-efficient motion synthesis. To ground high-level reasoning in physical reality, we leverage lightweight 2D geometric inductive biases to enable precise 3D scene understanding within the vision-language model. Specifically, we construct a Long-horizon Geometric Prior Skill Selector that effectively aligns the semantic instructions with spatial constraints, ultimately achieving robust generalization in unseen environments. For the data efficiency issue in robotic action generation, we introduce a Recursive Adaptive Spiking Network. We parameterize robot-object interactions via recursive spiking for spatiotemporal consistency, fully distilling long-horizon dynamic features while mitigating the overfitting issue in sparse demonstration scenarios. Extensive experiments are conducted across the Maniskill simulation benchmark and three heterogeneous real-world robotic systems, encompassing a custom-developed humanoid, a desktop manipulator, and a commercial robotic platform. Empirical results substantiate the superiority of our method over state-of-the-art baselines and validate the efficacy of the proposed modules in diverse generalization scenarios. To facilitate reproducibility, the source code and video demonstrations are publicly available at https://github.com/xtli12/RGMP-S.git.

</details>


### [169] [CLARE: Continual Learning for Vision-Language-Action Models via Autonomous Adapter Routing and Expansion](https://arxiv.org/abs/2601.09512)
*Ralf Römer,Yi Zhang,Angela P. Schoellig*

Main category: cs.RO

TL;DR: 提出CLARE框架用于基于VLA的无范例持续学习，在LIBERO基准测试中表现良好，避免灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有微调预训练VLA模型不适合长期运行，现有机器人持续学习方法有局限。

Method: 提出CLARE框架，引入轻量级模块化适配器，依据层特征相似度扩展模型，通过基于自编码器的路由机制动态激活适配器。

Result: 在LIBERO基准测试中，CLARE在新任务上表现出色，且无灾难性遗忘，优于基于范例的方法。

Conclusion: CLARE是适用于基于VLA无范例持续学习的通用且参数高效的框架。

Abstract: To teach robots complex manipulation tasks, it is now a common practice to fine-tune a pre-trained vision-language-action model (VLA) on task-specific data. However, since this recipe updates existing representations, it is unsuitable for long-term operation in the real world, where robots must continually adapt to new tasks and environments while retaining the knowledge they have already acquired. Existing continual learning methods for robotics commonly require storing previous data (exemplars), struggle with long task sequences, or rely on task identifiers for deployment. To address these limitations, we propose CLARE, a general, parameter-efficient framework for exemplar-free continual learning with VLAs. CLARE introduces lightweight modular adapters into selected feedforward layers and autonomously expands the model only where necessary when learning a new task, guided by layer-wise feature similarity. During deployment, an autoencoder-based routing mechanism dynamically activates the most relevant adapters without requiring task labels. Through extensive experiments on the LIBERO benchmark, we show that CLARE achieves high performance on new tasks without catastrophic forgetting of earlier tasks, significantly outperforming even exemplar-based methods. Code and data are available at https://tum-lsy.github.io/clare.

</details>


### [170] [Learning Whole-Body Human-Humanoid Interaction from Human-Human Demonstrations](https://arxiv.org/abs/2601.09518)
*Wei-Jin Huang,Yue-Yi Zhang,Yi-Lin Wei,Zhi-Wei Xia,Juantao Tan,Yuan-Ming Li,Zhilin Zhao,Wei-Shi Zheng*

Main category: cs.RO

TL;DR: 本文提出PAIR生成物理一致的人机交互数据，引入D - STAR解决传统模仿学习问题，经仿真验证性能优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 人机交互数据稀缺，利用人际交互数据时标准重定向方法失效，传统模仿学习策略缺乏交互理解。

Method: 提出PAIR（物理感知交互重定向）生成数据，引入D - STAR（解耦时空动作推理器）分层策略，融合相位注意力和多尺度空间模块。

Result: 通过大量严格仿真验证，相比基线方法有显著性能提升。

Conclusion: 提出了一个完整、有效的从人际交互数据中学习复杂全身交互的框架。

Abstract: Enabling humanoid robots to physically interact with humans is a critical frontier, but progress is hindered by the scarcity of high-quality Human-Humanoid Interaction (HHoI) data. While leveraging abundant Human-Human Interaction (HHI) data presents a scalable alternative, we first demonstrate that standard retargeting fails by breaking the essential contacts. We address this with PAIR (Physics-Aware Interaction Retargeting), a contact-centric, two-stage pipeline that preserves contact semantics across morphology differences to generate physically consistent HHoI data. This high-quality data, however, exposes a second failure: conventional imitation learning policies merely mimic trajectories and lack interactive understanding. We therefore introduce D-STAR (Decoupled Spatio-Temporal Action Reasoner), a hierarchical policy that disentangles when to act from where to act. In D-STAR, Phase Attention (when) and a Multi-Scale Spatial module (where) are fused by the diffusion head to produce synchronized whole-body behaviors beyond mimicry. By decoupling these reasoning streams, our model learns robust temporal phases without being distracted by spatial noise, leading to responsive, synchronized collaboration. We validate our framework through extensive and rigorous simulations, demonstrating significant performance gains over baseline approaches and a complete, effective pipeline for learning complex whole-body interactions from HHI data.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [171] [FairGE: Fairness-Aware Graph Encoding in Incomplete Social Networks](https://arxiv.org/abs/2601.09394)
*Renqiang Luo,Huafei Huang,Tao Tang,Jing Ren,Ziqi Xu,Mingliang Hou,Enyan Dai,Feng Xia*

Main category: cs.SI

TL;DR: 现有图变换器处理不完整社交网络公平性问题时存缺陷，提出FairGE框架，直接编码公平性，实验显示在公平指标上有至少16%提升。


<details>
  <summary>Details</summary>
Motivation: 图变换器在社交网络分析应用受公平性问题限制，尤其是不完整社交网络中敏感属性缺失，现有生成属性方法会带来额外偏差和隐私问题。

Method: 引入FairGE框架，通过谱图理论直接编码公平性，用主特征向量表示结构信息，对不完整敏感属性补零保持独立。

Result: 理论分析表明该方法抑制非主谱分量影响，实验在七个真实社交网络数据集上相比先进基线在统计奇偶性和机会均等性上至少提升16%。

Conclusion: FairGE框架能有效解决不完整社交网络中图变换器的公平性问题，可提升公平指标。

Abstract: Graph Transformers (GTs) are increasingly applied to social network analysis, yet their deployment is often constrained by fairness concerns. This issue is particularly critical in incomplete social networks, where sensitive attributes are frequently missing due to privacy and ethical restrictions. Existing solutions commonly generate these incomplete attributes, which may introduce additional biases and further compromise user privacy. To address this challenge, FairGE (Fair Graph Encoding) is introduced as a fairness-aware framework for GTs in incomplete social networks. Instead of generating sensitive attributes, FairGE encodes fairness directly through spectral graph theory. By leveraging the principal eigenvector to represent structural information and padding incomplete sensitive attributes with zeros to maintain independence, FairGE ensures fairness without data reconstruction. Theoretical analysis demonstrates that the method suppresses the influence of non-principal spectral components, thereby enhancing fairness. Extensive experiments on seven real-world social network datasets confirm that FairGE achieves at least a 16% improvement in both statistical parity and equality of opportunity compared with state-of-the-art baselines. The source code is shown in https://github.com/LuoRenqiang/FairGE.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [172] [A Deep Dive into OpenStreetMap Research Since its Inception (2008-2024): Contributors, Topics, and Future Trends](https://arxiv.org/abs/2601.09338)
*Yao Sun,Liqiu Meng,Andres Camero,Stefan Auer,Xiao Xiang Zhu*

Main category: cs.DL

TL;DR: 对OSM研究进行文献计量和系统分析，指出研究发展兼具巩固与多样化，明确6个新兴研究方向，并提供相关数据和代码。


<details>
  <summary>Details</summary>
Motivation: 探究OSM从志愿地理信息项目转变为全球多学科研究中心这一过程的发展轨迹和关键驱动力。

Method: 评估截至2024年6月Web of Science核心合集的1926篇出版物和782场State of the Map演示文稿。

Result: 领域发展兼具巩固和多样化，主题从数据生产和质量转向高级分析和应用，WoS和SotM的OSM研究议程不同但互补。

Conclusion: 确定6个新兴研究方向，突出学术界、OSM社区和行业的合作将塑造未来研究，为理解OSM研究现状及未来发展提供参考。

Abstract: OpenStreetMap (OSM) has transitioned from a pioneering volunteered geographic information (VGI) project into a global, multi-disciplinary research nexus. This study presents a bibliometric and systematic analysis of the OSM research landscape, examining its development trajectory and key driving forces. By evaluating 1,926 publications from the Web of Science (WoS) Core Collection and 782 State of the Map (SotM) presentations up to June 2024, we quantify publication growth, collaboration patterns, and thematic evolution. Results demonstrate simultaneous consolidation and diversification within the field. While a stable core of contributors continues to anchor OSM research, themes have shifted from initial concerns over data production and quality toward advanced analytical and applied uses. Comparative analysis of OSM-related research in WoS and SotM reveals distinct but complementary agendas between scholars and the OSM community. Building on these findings, we identify six emerging research directions and discuss how evolving partnerships among academia, the OSM community, and industry are poised to shape the future of OSM research. This study establishes a structured reference for understanding the state of OSM studies and offers strategic pathways for navigating its future trajectory.The data and code are available at https://github.com/ya0-sun/OSMbib.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [173] [High-fidelity lunar topographic reconstruction across diverse terrain and illumination environments using deep learning](https://arxiv.org/abs/2601.09468)
*Hao Chen,Philipp Gläser,Konrad Willner,Jürgen Oberst*

Main category: astro-ph.EP

TL;DR: 本文通过改进深度学习框架，提高其在月球地形重建的鲁棒性和适用性，证明该方法有潜力支持月球高级探索任务。


<details>
  <summary>Details</summary>
Motivation: 现有月球米级地形数据有限且深度学习在月球不同地形和光照条件下的鲁棒性和通用性有待探索。

Method: 在先前提出的深度学习框架基础上，融入更稳健的尺度恢复方案，并将模型扩展到低光照条件的极地地区。

Result: 与单视图明暗恢复形状方法相比，该深度学习方法对不同光照更稳健，能更准确重建不同尺度、形态和地质年代的月球地形，还能重建月球南极复杂低光照地形。

Conclusion: 基于深度学习的方法有潜力利用大量月球数据集，支持高级探索任务，实现前所未有的地形分辨率研究。

Abstract: Topographic models are essential for characterizing planetary surfaces and for inferring underlying geological processes. Nevertheless, meter-scale topographic data remain limited, which constrains detailed planetary investigations, even for the Moon, where extensive high-resolution orbital images are available. Recent advances in deep learning (DL) exploit single-view imagery, constrained by low-resolution topography, for fast and flexible reconstruction of fine-scale topography. However, their robustness and general applicability across diverse lunar landforms and illumination conditions remain insufficiently explored. In this study, we build upon our previously proposed DL framework by incorporating a more robust scale recovery scheme and extending the model to polar regions under low solar illumination conditions. We demonstrate that, compared with single-view shape-from-shading methods, the proposed DL approach exhibits greater robustness to varying illumination and achieves more consistent and accurate topographic reconstructions. Furthermore, it reliably reconstructs topography across lunar features of diverse scales, morphologies, and geological ages. High-quality topographic models are also produced for the lunar south polar areas, including permanently shadowed regions, demonstrating the method's capability in reconstructing complex and low-illumination terrain. These findings suggest that DL-based approaches have the potential to leverage extensive lunar datasets to support advanced exploration missions and enable investigations of the Moon at unprecedented topographic resolution.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [174] [MLCBART: Multilabel Classification with Bayesian Additive Regression Trees](https://arxiv.org/abs/2601.08964)
*Jiahao Tian,Hugh Chipman,Thomas Loughin*

Main category: stat.ME

TL;DR: 本文提出MLCBART模型处理多标签分类问题，经模拟实验验证其预测更准确且接近最优模型，还能提供不确定性度量。


<details>
  <summary>Details</summary>
Motivation: 多标签分类任务具有挑战性，存在预测变量与标签、标签之间的复杂关系，需要有效模型。

Method: 提出基于贝叶斯加性回归树（BART）的MLCBART模型，假设标签源于潜在数值尺度阈值化，用多元正态模型估计标签间相关结构。

Result: 模拟实验表明MLCBART比其他模型更准确预测标签向量，性能接近最优模型。

Conclusion: MLCBART模型能有效处理多标签分类问题，还能对预测提供不确定性度量，有助于理解分类结果。

Abstract: Multilabel Classification (MLC) deals with the simultaneous classification of multiple binary labels. The task is challenging because, not only may there be arbitrarily different and complex relationships between predictor variables and each label, but associations among labels may exist even after accounting for effects of predictor variables. In this paper, we present a Bayesian additive regression tree (BART) framework to model the problem. BART is a nonparametric and flexible model structure capable of uncovering complex relationships within the data. Our adaptation, MLCBART, assumes that labels arise from thresholding an underlying numeric scale, where a multivariate normal model allows explicit estimation of the correlation structure among labels. This enables the discovery of complicated relationships in various forms and improves MLC predictive performance. Our Bayesian framework not only enables uncertainty quantification for each predicted label, but our MCMC draws produce an estimated conditional probability distribution of label combinations for any predictor values. Simulation experiments demonstrate the effectiveness of the proposed model by comparing its performance with a set of models, including the oracle model with the correct functional form. Results show that our model predicts vectors of labels more accurately than other contenders and its performance is close to the oracle model. An example highlights how the method's ability to produce measures of uncertainty on predictions provides nuanced understanding of classification results.

</details>


### [175] [Sparse covariate-driven factorization of high-dimensional brain connectivity with application to site effect correction](https://arxiv.org/abs/2601.09525)
*Rongqian Zhang,Elena Tuzhilina,Jun Young Park*

Main category: stat.ME

TL;DR: 本文提出SLACC因子分解法缓解神经影像数据的站点效应，经模拟和实际数据验证有效，且R包公开可用。


<details>
  <summary>Details</summary>
Motivation: 大规模神经影像研究中站点差异会引入站点效应，影响脑连接测量，需利用高维网络结构缓解该效应。

Method: 提出SLACC因子分解法，将协变量效应参数化到潜在主体得分，开发惩罚EM算法估计参数，用BIC指导优化。

Result: 大规模模拟验证了SLACC在恢复真实参数和潜在连接模式上的稳健性，应用于ABIDE数据集显示其能减少站点效应。

Conclusion: SLACC是一种有效缓解神经影像数据站点效应的方法。

Abstract: Large-scale neuroimaging studies often collect data from multiple scanners across different sites, where variations in scanners, scanning procedures, and other conditions across sites can introduce artificial site effects. These effects may bias brain connectivity measures, such as functional connectivity (FC), which quantify functional network organization derived from functional magnetic resonance imaging (fMRI). How to leverage high-dimensional network structures to effectively mitigate site effects has yet to be addressed. In this paper, we propose SLACC (Sparse LAtent Covariate-driven Connectome) factorization, a multivariate method that explicitly parameterizes covariate effects in latent subject scores corresponding to sparse rank-1 latent patterns derived from brain connectivity. The proposed method identifies localized site-driven variability within and across brain networks, enabling targeted correction. We develop a penalized Expectation-Maximization (EM) algorithm for parameter estimation, incorporating the Bayesian Information Criterion (BIC) to guide optimization. Extensive simulations validate SLACC's robustness in recovering the true parameters and underlying connectivity patterns. Applied to the Autism Brain Imaging Data Exchange (ABIDE) dataset, SLACC demonstrates its ability to reduce site effects. The R package to implement our method is publicly available.

</details>


### [176] [LARGE: A Locally Adaptive Regularization Approach for Estimating Gaussian Graphical Models](https://arxiv.org/abs/2601.09686)
*Ha Nguyen,Sumanta Basu*

Main category: stat.ME

TL;DR: 本文提出LARGE方法解决GLASSO算法中正则化参数选择问题，模拟和实际数据均有良好表现。


<details>
  <summary>Details</summary>
Motivation: GLASSO算法选择最优正则化参数困难，现有方法存在不足，在计算神经科学等领域有节点自适应调参需求。

Method: 提出LARGE方法，在GLASSO的块坐标下降步骤中，增强节点Lasso回归以联合估计回归系数和误差方差，指导节点惩罚的自适应学习。

Result: 模拟中LARGE在图恢复上优于基准方法，更稳定，在困难设置下估计精度最佳；用真实fMRI数据集展示了方法实用性。

Conclusion: LARGE方法能有效解决图估计中节点自适应调参问题，提升图估计和选择效果。

Abstract: The graphical Lasso (GLASSO) is a widely used algorithm for learning high-dimensional undirected Gaussian graphical models (GGM). Given i.i.d. observations from a multivariate normal distribution, GLASSO estimates the precision matrix by maximizing the log-likelihood with an \ell_1-penalty on the off-diagonal entries. However, selecting an optimal regularization parameter λin this unsupervised setting remains a significant challenge. A well-known issue is that existing methods, such as out-of-sample likelihood maximization, select a single global λand do not account for heterogeneity in variable scaling or partial variances. Standardizing the data to unit variances, although a common workaround, has been shown to negatively affect graph recovery. Addressing the problem of nodewise adaptive tuning in graph estimation is crucial for applications like computational neuroscience, where brain networks are constructed from highly heterogeneous, region-specific fMRI data.
  In this work, we develop Locally Adaptive Regularization for Graph Estimation (LARGE), an approach to adaptively learn nodewise tuning parameters to improve graph estimation and selection. In each block coordinate descent step of GLASSO, we augment the nodewise Lasso regression to jointly estimate the regression coefficients and error variance, which in turn guides the adaptive learning of nodewise penalties. In simulations, LARGE consistently outperforms benchmark methods in graph recovery, demonstrates greater stability across replications, and achieves the best estimation accuracy in the most difficult simulation settings. We demonstrate the practical utility of our method by estimating brain functional connectivity from a real fMRI data set.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [177] [Comprehensive Machine Learning Benchmarking for Fringe Projection Profilometry with Photorealistic Synthetic Data](https://arxiv.org/abs/2601.08900)
*Anush Lakshman S,Adam Haroon,Beiwen Li*

Main category: eess.IV

TL;DR: 本文引入首个用于条纹投影轮廓测量（FPP）的开源、逼真合成数据集，对四种神经网络架构进行基准测试，揭示直接条纹到深度映射的局限性，提供标准化评估协议。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习用于FPP时缺乏大型、多样数据集和综合基准测试协议的问题。

Method: 使用NVIDIA Isaac Sim生成包含15600张条纹图像和300个深度重建结果的数据集，对UNet、Hformer、ResUNet、Pix2Pix四种神经网络架构进行单帧深度重建基准测试。

Result: 四种模型虽架构差异大，但性能相近（RMSE为58 - 77 mm），无显式相位信息的直接条纹到深度映射有基本局限性，重建误差达典型物体深度范围的75 - 95%。

Conclusion: 该资源提供了标准化评估协议，有助于基于学习的FPP方法的系统比较和发展。

Abstract: Machine learning approaches for fringe projection profilometry (FPP) are hindered by the lack of large, diverse datasets and comprehensive benchmarking protocols. This paper introduces the first open-source, photorealistic synthetic dataset for FPP, generated using NVIDIA Isaac Sim with 15,600 fringe images and 300 depth reconstructions across 50 diverse objects. We benchmark four neural network architectures (UNet, Hformer, ResUNet, Pix2Pix) on single-shot depth reconstruction, revealing that all models achieve similar performance (58-77 mm RMSE) despite substantial architectural differences. Our results demonstrate fundamental limitations of direct fringe-to-depth mapping without explicit phase information, with reconstruction errors approaching 75-95\% of the typical object depth range. This resource provides standardized evaluation protocols enabling systematic comparison and development of learning-based FPP approaches.

</details>


### [178] [Universal Latent Homeomorphic Manifolds: Cross-Domain Representation Learning via Homeomorphism Verification](https://arxiv.org/abs/2601.09025)
*Tong Wu,Tayab Uddin Wara,Daniel Hernandez,Sidong Lei*

Main category: eess.IV

TL;DR: 提出ULHM框架统一语义和机器表示，用同胚准则保证应用，通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 将语义表示和机器表示统一到单一潜在结构，解决不同模态虽源于不同途径但反映相同现实却缺乏统一方法的问题。

Method: 以同胚为数学准则，通过条件变分推理学习连续流形到流形的变换，开发实用验证算法。

Result: 实现稀疏图像恢复、跨域分类器迁移和零样本分类，同胚准则能正确拒绝不兼容数据集。

Conclusion: ULHM框架可行，同胚准则为基础模型分解为特定领域组件提供可行方法。

Abstract: We present the Universal Latent Homeomorphic Manifold (ULHM), a framework that unifies semantic representations (e.g., human descriptions, diagnostic labels) and observation-driven machine representations (e.g., pixel intensities, sensor readings) into a single latent structure. Despite originating from fundamentally different pathways, both modalities capture the same underlying reality. We establish \emph{homeomorphism}, a continuous bijection preserving topological structure, as the mathematical criterion for determining when latent manifolds induced by different semantic-observation pairs can be rigorously unified. This criterion provides theoretical guarantees for three critical applications: (1) semantic-guided sparse recovery from incomplete observations, (2) cross-domain transfer learning with verified structural compatibility, and (3) zero-shot compositional learning via valid transfer from semantic to observation space. Our framework learns continuous manifold-to-manifold transformations through conditional variational inference, avoiding brittle point-to-point mappings. We develop practical verification algorithms, including trust, continuity, and Wasserstein distance metrics, that empirically validate homeomorphic structure from finite samples. Experiments demonstrate: (1) sparse image recovery from 5\% of CelebA pixels and MNIST digit reconstruction at multiple sparsity levels, (2) cross-domain classifier transfer achieving 86.73\% accuracy from MNIST to Fashion-MNIST without retraining, and (3) zero-shot classification on unseen classes achieving 89.47\% on MNIST, 84.70\% on Fashion-MNIST, and 78.76\% on CIFAR-10. Critically, the homeomorphism criterion correctly rejects incompatible datasets, preventing invalid unification and providing a feasible way to principled decomposition of general foundation models into verified domain-specific components.

</details>


### [179] [Equi-ViT: Rotational Equivariant Vision Transformer for Robust Histopathology Analysis](https://arxiv.org/abs/2601.09130)
*Fuyao Chen,Yuexi Du,Elèonore V. Lieffrig,Nicha C. Dvornek,John A. Onofrey*

Main category: eess.IV

TL;DR: 提出Equi - ViT解决标准ViTs在组织病理学成像中对旋转等变换非等变的问题，在结直肠癌数据集上证明其优势。


<details>
  <summary>Details</summary>
Motivation: 标准ViTs在组织病理学成像中对旋转和反射等变换非等变，而这些变换在组织病理学图像中普遍存在，需要解决此局限性。

Method: 在ViT架构的补丁嵌入阶段集成等变卷积核，提出Equi - ViT。

Result: Equi - ViT实现了卓越的旋转一致补丁嵌入和跨图像方向的稳定分类性能，在公共结直肠癌数据集上证明了等变补丁嵌入可提高数据效率和鲁棒性。

Conclusion: 等变变压器有可能作为ViT在组织病理学应用（如数字病理学基础模型）中更具通用性的骨干网络。

Abstract: Vision Transformers (ViTs) have gained rapid adoption in computational pathology for their ability to model long-range dependencies through self-attention, addressing the limitations of convolutional neural networks that excel at local pattern capture but struggle with global contextual reasoning. Recent pathology-specific foundation models have further advanced performance by leveraging large-scale pretraining. However, standard ViTs remain inherently non-equivariant to transformations such as rotations and reflections, which are ubiquitous variations in histopathology imaging. To address this limitation, we propose Equi-ViT, which integrates an equivariant convolution kernel into the patch embedding stage of a ViT architecture, imparting built-in rotational equivariance to learned representations. Equi-ViT achieves superior rotation-consistent patch embeddings and stable classification performance across image orientations. Our results on a public colorectal cancer dataset demonstrate that incorporating equivariant patch embedding enhances data efficiency and robustness, suggesting that equivariant transformers could potentially serve as more generalizable backbones for the application of ViT in histopathology, such as digital pathology foundation models.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [180] [On the complexity of global Roman domination problem in graphs](https://arxiv.org/abs/2601.09167)
*Sangam Balchandar Reddy,Arun Kumar Das,Anjeneya Swami Kare,I. Vinod Reddy*

Main category: math.CO

TL;DR: 研究全局罗马控制问题在不同图类上的算法性质，证明罗马控制和全局罗马控制不等价，证明该问题在多种图类上NP完全，给出在cographs上的线性时间算法。


<details>
  <summary>Details</summary>
Motivation: 研究全局罗马控制问题在各种图类上的算法复杂度，解决相关未决问题。

Method: 通过识别不同图类，证明问题在某些图类上的NP完全性，以及给出特定图类上的算法。

Result: 1. 证明罗马控制和全局罗马控制问题计算上不等价；2. 证明全局罗马控制问题在分裂图上NP完全，解决公开问题；3. 证明在弦二部图、最大度为5的平面二部图和圆图上NP完全；4. 给出在cographs上的线性时间算法。

Conclusion: 全局罗马控制问题在不同图类上具有不同的计算复杂度，在cographs上可线性时间求解。

Abstract: A Roman dominating function of a graph $G=(V,E)$ is a labeling $f: V \rightarrow{} \{0 ,1, 2\}$ such that for each vertex $u \in V$ with $f(u) = 0$, there exists a vertex $v \in N(u)$ with $f(v) =2$. A Roman dominating function $f$ is a global Roman dominating function if it is a Roman dominating function for both $G$ and its complement $\overline{G}$. The weight of $f$ is the sum of $f(u)$ over all the vertices $u \in V$. The objective of Global Roman Domination problem is to find a global Roman dominating function with minimum weight. The objective of Global Roman Domination is to compute a global Roman dominating function of minimum weight.
  In this paper, we study the algorithmic aspects of Global Roman Domination problem on various graph classes and obtain the following results.
  1. We prove that Roman domination and Global Roman Domination problems are not computationally equivalent by identifying graph classes on which one is linear-time solvable, while the other is NP-complete.
 2. We show that Global Roman Domination problem is NP-complete on split graphs, thereby resolving an open question posed by Panda and Goyal [Discrete Applied Mathematics, 2023].
  3. We prove that Global Roman Domination problem is NP-complete on chordal bipartite graphs, planar bipartite graphs with maximum degree five and circle graphs.
  4. On the positive side, we present a linear-time algorithm for Global Roman domination problem on cographs.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [181] [Machine Learning-Driven Creep Law Discovery Across Alloy Compositional Space](https://arxiv.org/abs/2601.08970)
*Hongshun Chen,Ryan Zhou,Rujing Zha,Zihan Chen,Wenpan Li,Rowan Rolark,John Patrick Reidy,Jian Cao,Ping Guo,David C. Dunand,Horacio D. Espinosa*

Main category: cond-mat.mtrl-sci

TL;DR: 本文介绍了一种机器学习辅助的高通量框架用于蠕变定律识别，结合DABI实验提供了高通量蠕变表征平台。


<details>
  <summary>Details</summary>
Motivation: 传统结构合金高温蠕变表征方法效率低，难以探索合金成分的大搜索空间和进行材料发现。

Method: 采用基于DABI配置进行并行蠕变测试，用3D数字图像相关测量表面位移，训练RNN作为替代模型，结合粒子群优化方案进行反演识别，提出含时变应力指数的现象学蠕变定律，并结合正则化反演识别多种合金的蠕变定律。

Result: 能对多种Fe、Ni、Co基合金进行蠕变定律识别，自动选择主导函数形式。

Conclusion: 该工作流程与DABI实验结合，提供了适用于数据挖掘、成分 - 性能建模和非线性结构优化的高通量蠕变表征平台。

Abstract: Hihg-temperature creep characterization of structural alloys traditionally relies on serial uniaxial tests, which are highly inefficient for exploring the large search space of alloy compositions and for material discovery. Here, we introduce a machine-learning-assisted, high-throughput framework for creep law identification based on a dimple array bulge instrument (DABI) configuration, which enables parallel creep testing of 25 dimples, each fabricated from a different alloy, in a single experiment. Full-field surface displacements of dimples undergoing time-dependent creep-induced bulging under inert gas pressure are measured by 3D digital image correlation. We train a recurrent neural network (RNN) as a surrogate model, mapping creep parameters and loading conditions to the time-dependent deformation response of DABI. Coupling this surrogate with a particle swarm optimization scheme enables rapid and global inverse identification with sparsity regularization of creep parameters from experiment displacement-time histories. In addition, we propose a phenomenological creep law with a time-dependent stress exponent that captures the sigmoidal primary creep observed in wrought INCONEL 625 and extracts its temperature dependence from DABI test at multiple temperatures. Furthermore, we employ a general creep law combining several conventional forms together with regularized inversion to identify the creep laws for 47 additional Fe-, Ni-, and Co-rich alloys and to automatically select the dominant functional form for each alloy. This workflow combined with DABI experiment provides a quantitative, high-throughput creep characterization platform that is compatible with data mining, composition-property modeling, and nonlinear structural optimization with creep behavior across a large alloy design space.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [182] [Towards Improving Interpretability of Language Model Generation through a Structured Knowledge Discovery Approach](https://arxiv.org/abs/2511.23335)
*Shuqi Liu,Han Wu,Guanzhi Deng,Jianshu Chen,Xiaoyang Wang,Linqi Song*

Main category: cs.CL

TL;DR: 本文提出任务无关的结构化知识猎人模型，结合语言模型生成能力与知识猎人高可信度，实现高可解释性，在两个数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有知识增强文本生成方法缺乏可解释性，且特定领域知识检索器泛化性有限。

Method: 利用结构化知识的两层架构设计任务无关的结构化知识猎人，采用局部 - 全局交互方案进行知识表示学习，使用基于分层变压器的指针网络选择知识。

Result: 模型在RotoWireFG和KdConv数据集上表现良好，优于现有方法和对应语言模型。

Conclusion: 所提模型能实现高可解释性，在多个任务中有效，为基准测试设定新标准。

Abstract: Knowledge-enhanced text generation aims to enhance the quality of generated text by utilizing internal or external knowledge sources. While language models have demonstrated impressive capabilities in generating coherent and fluent text, the lack of interpretability presents a substantial obstacle. The limited interpretability of generated text significantly impacts its practical usability, particularly in knowledge-enhanced text generation tasks that necessitate reliability and explainability. Existing methods often employ domain-specific knowledge retrievers that are tailored to specific data characteristics, limiting their generalizability to diverse data types and tasks. To overcome this limitation, we directly leverage the two-tier architecture of structured knowledge, consisting of high-level entities and low-level knowledge triples, to design our task-agnostic structured knowledge hunter. Specifically, we employ a local-global interaction scheme for structured knowledge representation learning and a hierarchical transformer-based pointer network as the backbone for selecting relevant knowledge triples and entities. By combining the strong generative ability of language models with the high faithfulness of the knowledge hunter, our model achieves high interpretability, enabling users to comprehend the model output generation process. Furthermore, we empirically demonstrate the effectiveness of our model in both internal knowledge-enhanced table-to-text generation on the RotoWireFG dataset and external knowledge-enhanced dialogue response generation on the KdConv dataset. Our task-agnostic model outperforms state-of-the-art methods and corresponding language models, setting new standards on the benchmark.

</details>


### [183] [Contrastive Bi-Encoder Models for Multi-Label Skill Extraction: Enhancing ESCO Ontology Matching with BERT and Attention Mechanisms](https://arxiv.org/abs/2601.09119)
*Yongming Sun*

Main category: cs.CL

TL;DR: 提出零样本技能提取框架用于将非结构化招聘广告映射到标准化技能分类法，实验显示性能良好，提供可扩展、数据高效的途径。


<details>
  <summary>Details</summary>
Motivation: 细粒度劳动力市场分析需将招聘广告映射到技能分类法，现有监督式方案受限于大规模标注数据稀缺和成本高，特别是非英语环境。

Method: 使用大语言模型从ESCO定义合成训练实例，引入基于ESCO二级类别的分层约束多技能生成，训练对比双编码器，增加上游二进制过滤器。

Result: 层次条件生成提高流利度和可辨别性，多标签模型在真实中文招聘广告上实现强零样本检索性能（F1@5 = 0.72），优于TF - IDF和标准BERT基线。

Conclusion: 提出的流程为劳动经济学和劳动力分析中的自动化技能编码提供了可扩展、数据高效的途径。

Abstract: Fine-grained labor market analysis increasingly relies on mapping unstructured job advertisements to standardized skill taxonomies such as ESCO. This mapping is naturally formulated as an Extreme Multi-Label Classification (XMLC) problem, but supervised solutions are constrained by the scarcity and cost of large-scale, taxonomy-aligned annotations--especially in non-English settings where job-ad language diverges substantially from formal skill definitions. We propose a zero-shot skill extraction framework that eliminates the need for manually labeled job-ad training data. The framework uses a Large Language Model (LLM) to synthesize training instances from ESCO definitions, and introduces hierarchically constrained multi-skill generation based on ESCO Level-2 categories to improve semantic coherence in multi-label contexts. On top of the synthetic corpus, we train a contrastive bi-encoder that aligns job-ad sentences with ESCO skill descriptions in a shared embedding space; the encoder augments a BERT backbone with BiLSTM and attention pooling to better model long, information-dense requirement statements. An upstream RoBERTa-based binary filter removes non-skill sentences to improve end-to-end precision. Experiments show that (i) hierarchy-conditioned generation improves both fluency and discriminability relative to unconstrained pairing, and (ii) the resulting multi-label model transfers effectively to real-world Chinese job advertisements, achieving strong zero-shot retrieval performance (F1@5 = 0.72) and outperforming TF--IDF and standard BERT baselines. Overall, the proposed pipeline provides a scalable, data-efficient pathway for automated skill coding in labor economics and workforce analytics.

</details>


### [184] [OpenDecoder: Open Large Language Model Decoding to Incorporate Document Quality in RAG](https://arxiv.org/abs/2601.09028)
*Fengran Mo,Zhan Su,Yuchen Hui,Jinghan Zhang,Jia Ao Sun,Zheyuan Liu,Chao Zhang,Tetsuya Sakai,Jian-Yun Nie*

Main category: cs.CL

TL;DR: 提出OpenDecoder方法，利用对检索信息的显式评估作为生成的质量指标特征，在五个基准数据集上实验证明其有效性和更强鲁棒性，且该范式灵活可集成。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的检索增强生成中，生成内容质量依赖检索信息有用性，但检索信息相关性和有用性可变，需考虑其相关性。

Method: 提出OpenDecoder方法，考虑相关性得分、排名得分和QPP得分三种显式评估信息。

Result: 在五个基准数据集上实验，OpenDecoder优于多种基线方法。

Conclusion: OpenDecoder有效且鲁棒性更好，其范式可灵活集成到LLM的后训练中。

Abstract: The development of large language models (LLMs) has achieved superior performance in a range of downstream tasks, including LLM-based retrieval-augmented generation (RAG). The quality of generated content heavily relies on the usefulness of the retrieved information and the capacity of LLMs' internal information processing mechanism to incorporate it in answer generation. It is generally assumed that the retrieved information is relevant to the question. However, the retrieved information may have a variable degree of relevance and usefulness, depending on the question and the document collection. It is important to take into account the relevance of the retrieved information in answer generation. In this paper, we propose OpenDecoder, a new approach that leverages explicit evaluation of the retrieved information as quality indicator features for generation. We aim to build a RAG model that is more robust to varying levels of noisy context. Three types of explicit evaluation information are considered: relevance score, ranking score, and QPP (query performance prediction) score. The experimental results on five benchmark datasets demonstrate the effectiveness and better robustness of OpenDecoder by outperforming various baseline methods. Importantly, this paradigm is flexible to be integrated with the post-training of LLMs for any purposes and incorporated with any type of external indicators.

</details>


### [185] [SpectraQuery: A Hybrid Retrieval-Augmented Conversational Assistant for Battery Science](https://arxiv.org/abs/2601.09036)
*Sreya Vangara,Jagjit Nanda,Yan-Kai Tzeng,Eric Darve*

Main category: cs.CL

TL;DR: 介绍SpectraQuery混合自然语言查询框架，结合光谱数据库和文献语料库，表现良好可支持科学工作流。


<details>
  <summary>Details</summary>
Motivation: 科学推理需关联结构化实验数据和非结构化文献，但多数大语言模型助手无法跨模态联合推理。

Method: 采用受SUQL启发的设计，结合语义解析和检索增强生成，将问题转化为SQL和文献检索操作。

Result: 约80%生成的SQL查询完全正确，合成答案接地率达93 - 97%，电池科学家对响应评价高。

Conclusion: 混合检索架构能通过连接数据和论述支持科学工作流。

Abstract: Scientific reasoning increasingly requires linking structured experimental data with the unstructured literature that explains it, yet most large language model (LLM) assistants cannot reason jointly across these modalities. We introduce SpectraQuery, a hybrid natural-language query framework that integrates a relational Raman spectroscopy database with a vector-indexed scientific literature corpus using a Structured and Unstructured Query Language (SUQL)-inspired design. By combining semantic parsing with retrieval-augmented generation, SpectraQuery translates open-ended questions into coordinated SQL and literature retrieval operations, producing cited answers that unify numerical evidence with mechanistic explanation. Across SQL correctness, answer groundedness, retrieval effectiveness, and expert evaluation, SpectraQuery demonstrates strong performance: approximately 80 percent of generated SQL queries are fully correct, synthesized answers reach 93-97 percent groundedness with 10-15 retrieved passages, and battery scientists rate responses highly across accuracy, relevance, grounding, and clarity (4.1-4.6/5). These results show that hybrid retrieval architectures can meaningfully support scientific workflows by bridging data and discourse for high-volume experimental datasets.

</details>


### [186] [DeliberationBench: When Do More Voices Hurt? A Controlled Study of Multi-LLM Deliberation Protocols](https://arxiv.org/abs/2601.08835)
*Vaarunay Kaushal,Taranveer Singh*

Main category: cs.CL

TL;DR: 引入DELIBERATIONBENCH基准测试多智能体大语言模型协商协议，发现最佳单输出基线表现远超协商协议，挑战了多LLM系统复杂性提升质量的假设。


<details>
  <summary>Details</summary>
Motivation: 评估多智能体大语言模型协商形成共识系统相对简单方法的实际价值。

Method: 引入DELIBERATIONBENCH基准，将三种协商协议与从模型输出池中选择最佳响应的强基线进行对比评估。

Result: 最佳单输出基线胜率达82.5% ± 3.3%，远超最佳协商协议的13.8% ± 2.6%，性能差距6倍，且协商协议计算成本高1.5 - 2.5倍。

Conclusion: 多LLM系统中复杂性不一定能提升质量。

Abstract: Multi-agent systems where Large Language Models (LLMs) deliberate to form consensus have gained significant attention, yet their practical value over simpler methods remains under-scrutinized. We introduce DELIBERATIONBENCH, a controlled benchmark evaluating three deliberation protocols against a strong baseline of selecting the best response from a pool of model outputs. Across 270 questions and three independent seeds (810 total evaluations), we find a striking negative result: the best-single baseline achieves an 82.5% +- 3.3% win rate, dramatically outperforming the best deliberation protocol(13.8% +- 2.6%). This 6.0x performance gap is statistically significant (p < 0.01) and comes at 1.5-2.5x higher computational cost. Our findings challenge assumptions that complexity enhances quality in multi-LLM systems.

</details>


### [187] [From Adversarial Poetry to Adversarial Tales: An Interpretability Research Agenda](https://arxiv.org/abs/2601.08837)
*Piercosma Bisconti,Marcello Galisai,Matteo Prandi,Federico Pierucci,Olga Sorokoletova,Francesco Giarrusso,Vincenzo Suriani,Marcantonio Brancale,Daniele Nardi*

Main category: cs.CL

TL;DR: 提出Adversarial Tales越狱技术攻击LLMs安全机制，攻击成功率高，表明基于结构的越狱是广泛漏洞，需开展机制解释性研究。


<details>
  <summary>Details</summary>
Motivation: LLMs安全机制易受利用文化编码结构重新构建有害请求的攻击。

Method: 引入Adversarial Tales技术，将有害内容嵌入赛博朋克叙事，并按弗拉基米尔·普罗普的民间故事形态学进行功能分析，诱导模型重构有害程序。

Result: 对9个供应商的26个前沿模型攻击，平均成功率71.3%，无模型家族表现可靠鲁棒性。

Conclusion: 基于结构的越狱构成广泛漏洞类别，不能仅靠模式匹配防御，需开展机制解释性研究。

Abstract: Safety mechanisms in LLMs remain vulnerable to attacks that reframe harmful requests through culturally coded structures. We introduce Adversarial Tales, a jailbreak technique that embeds harmful content within cyberpunk narratives and prompts models to perform functional analysis inspired by Vladimir Propp's morphology of folktales. By casting the task as structural decomposition, the attack induces models to reconstruct harmful procedures as legitimate narrative interpretation. Across 26 frontier models from nine providers, we observe an average attack success rate of 71.3%, with no model family proving reliably robust. Together with our prior work on Adversarial Poetry, these findings suggest that structurally-grounded jailbreaks constitute a broad vulnerability class rather than isolated techniques. The space of culturally coded frames that can mediate harmful intent is vast, likely inexhaustible by pattern-matching defenses alone. Understanding why these attacks succeed is therefore essential: we outline a mechanistic interpretability research agenda to investigate how narrative cues reshape model representations and whether models can learn to recognize harmful intent independently of surface form.

</details>


### [188] [Companion Agents: A Table-Information Mining Paradigm for Text-to-SQL](https://arxiv.org/abs/2601.08838)
*Jiahui Chen,Lei Fu,Jian Cui,Yu Lei,Zhenning Dong*

Main category: cs.CL

TL;DR: 现有大规模Text - to - SQL基准假设数据库注释完整准确，不符合工业场景。本文提出Companion Agents (CA)范式，在数据库端预缓存相关知识，实验显示该范式在BIRD基准上提升了执行准确率，为工业级部署提供了可行路径。


<details>
  <summary>Details</summary>
Motivation: 当前大规模Text - to - SQL基准假设与常见工业场景不匹配，限制了先进Text - to - SQL系统的实际应用。

Method: 提出数据库中心方法，以“缓存”查询相关知识；引入Companion Agents (CA)范式，让一组代理伴随数据库模式，在查询生成前主动挖掘和整合隐藏信息。

Result: 在BIRD基准完全缺失证据的设置下，CA在RSL - SQL / CHESS / DAIL - SQL上分别提高了4.49 / 4.37 / 14.13的执行准确率，在挑战子集上提升更大。

Conclusion: CA的自动数据库端挖掘和证据构建为无需人工证据的工业级Text - to - SQL部署提供了实用路径。

Abstract: Large-scale Text-to-SQL benchmarks such as BIRD typically assume complete and accurate database annotations as well as readily available external knowledge, which fails to reflect common industrial settings where annotations are missing, incomplete, or erroneous. This mismatch substantially limits the real-world applicability of state-of-the-art (SOTA) Text-to-SQL systems. To bridge this gap, we explore a database-centric approach that leverages intrinsic, fine-grained information residing in relational databases to construct missing evidence and improve Text-to-SQL accuracy under annotation-scarce conditions. Our key hypothesis is that when a query requires multi-step reasoning over extensive table information, existing methods often struggle to reliably identify and utilize the truly relevant knowledge. We therefore propose to "cache" query-relevant knowledge on the database side in advance, so that it can be selectively activated at inference time. Based on this idea, we introduce Companion Agents (CA), a new Text-to-SQL paradigm that incorporates a group of agents accompanying database schemas to proactively mine and consolidate hidden inter-table relations, value-domain distributions, statistical regularities, and latent semantic cues before query generation. Experiments on BIRD under the fully missing evidence setting show that CA recovers +4.49 / +4.37 / +14.13 execution accuracy points on RSL-SQL / CHESS / DAIL-SQL, respectively, with larger gains on the Challenging subset +9.65 / +7.58 / +16.71. These improvements stem from CA's automatic database-side mining and evidence construction, suggesting a practical path toward industrial-grade Text-to-SQL deployment without reliance on human-curated evidence.

</details>


### [189] [Consistency-Aware Editing for Entity-level Unlearning in Language Models](https://arxiv.org/abs/2601.08840)
*Xiaoqi Han,Víctor Gutiérrez-Basulto,Ru Li,Xiaoli Li,Jiye Liang,Jeff Z. Pan*

Main category: cs.CL

TL;DR: 本文研究如何将编辑技术用于大语言模型的实体级遗忘，提出一致性感知编辑（CAE）框架，在两个基准上评估，证明其能提升遗忘准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在保留敏感等信息的风险，现有实体级遗忘方法有计算成本高或处理释义查询时不稳定的问题，现有编辑技术多为实例级更新，需研究适用于实体级遗忘的编辑技术。

Method: 引入一致性感知编辑（CAE）框架，聚合与目标实体相关的多样化提示，在一致性正则化器引导下联合学习低秩更新。还研究实体在模型中的存储位置和成功遗忘所需提示数量。

Result: 在RWKU和ToFU两个基准上评估，CAE能深入了解实体级知识在大语言模型中的内部表示和删除方式，显著提高遗忘准确性和鲁棒性，仅用数十个精心选择的提示就能实现可扩展的实体删除。

Conclusion: CAE框架是一种有效且高效的大语言模型实体级遗忘方法，能在保留模型整体能力的同时，更好地实现特定实体知识的遗忘。

Abstract: Large language models (LLMs) risk retaining sensitive, copyrighted, or harmful information from their training data. Entity-level unlearning addresses this issue by removing all knowledge of a specific entity while preserving the model's overall capabilities. Existing approaches typically rely on full-model fine-tuning or prompt-based interventions, which can be computationally expensive or brittle when handling paraphrased queries. Recently, model editing has emerged as an efficient alternative for updating knowledge in LLMs, offering a promising direction for unlearning. However, existing editing techniques are typically designed for instance-level updates, modifying responses to specific attributes of an entity rather than eliminating all knowledge associated with the entity. In this paper, we investigate how editing techniques can be adapted for effective and efficient entity-level unlearning. To this end, we introduce a novel consistency-aware editing (CAE) framework. CAE aggregates a diverse set of prompts related to a target entity, including its attributes, relations, and adversarial paraphrases. It then jointly learns a low-rank update guided by a consistency regularizer that aligns the editing directions across prompts. This promotes robust and comprehensive forgetting while minimizing interference with unrelated knowledge. We further examine where different entities are stored within the model and how many diverse prompts are needed for successful unlearning. We evaluate CAE on two challenging benchmarks, RWKU and ToFU, and demonstrate that it (i) provides insights into how entity-level knowledge is internally represented and deleted in LLMs, (ii) significantly improves forgetting accuracy and robustness over traditional unlearning and editing baselines, and (iii) enables scalable entity removal using only tens of carefully selected prompts.

</details>


### [190] [Triples and Knowledge-Infused Embeddings for Clustering and Classification of Scientific Documents](https://arxiv.org/abs/2601.08841)
*Mihael Arcan*

Main category: cs.CL

TL;DR: 研究探索用主谓宾三元组结构化知识提升科学论文聚类和分类效果，提出模块化流程，实验表明全文本聚类好，混合表示提升分类，不同模型各有优势。


<details>
  <summary>Details</summary>
Motivation: 科学文献数量和复杂性增加，需要有效方法组织和理解研究文档，探索结构化知识对论文聚类和分类的作用。

Method: 提出结合无监督聚类和有监督分类的模块化流程，对多种文档表示（原始摘要、提取的三元组、混合格式）操作，用过滤的arXiv语料库，提取关系三元组构建四种文本表示，用四种模型嵌入，用多种方法评估聚类和微调分类模型。

Result: 全文本聚类最连贯，含三元组的混合表示提升分类性能，准确率达92.6%，宏F1值达0.925；轻量级编码器聚类表现好，SciBERT在结构化输入分类中出色。

Conclusion: 结合非结构化文本和结构化知识有互补优势，为科学文档语义组织的知识注入表示提供新见解。

Abstract: The increasing volume and complexity of scientific literature demand robust methods for organizing and understanding research documents. In this study, we explore how structured knowledge, specifically, subject-predicate-object triples, can enhance the clustering and classification of scientific papers. We propose a modular pipeline that combines unsupervised clustering and supervised classification over multiple document representations: raw abstracts, extracted triples, and hybrid formats that integrate both. Using a filtered arXiv corpus, we extract relational triples from abstracts and construct four text representations, which we embed using four state-of-the-art transformer models: MiniLM, MPNet, SciBERT, and SPECTER. We evaluate the resulting embeddings with KMeans, GMM, and HDBSCAN for unsupervised clustering, and fine-tune classification models for arXiv subject prediction. Our results show that full abstract text yields the most coherent clusters, but that hybrid representations incorporating triples consistently improve classification performance, reaching up to 92.6% accuracy and 0.925 macro-F1. We also find that lightweight sentence encoders (MiniLM, MPNet) outperform domain-specific models (SciBERT, SPECTER) in clustering, while SciBERT excels in structured-input classification. These findings highlight the complementary benefits of combining unstructured text with structured knowledge, offering new insights into knowledge-infused representations for semantic organization of scientific documents.

</details>


### [191] [Resisting Correction: How RLHF Makes Language Models Ignore External Safety Signals in Natural Conversation](https://arxiv.org/abs/2601.08842)
*Felipe Biava Cataneo*

Main category: cs.CL

TL;DR: 研究指令调优语言模型在不同交互模式下对外部置信信息的可控性，发现基础模型可控性好，指令调优模型有上下文依赖，凸显外部监督必要及部署关键故障模式。


<details>
  <summary>Details</summary>
Motivation: 测试指令调优语言模型在不同交互模式下能否保留将外部提供的置信信息融入语言响应的可控性。

Method: 使用Llama - 3.2 - 3B在GSM8K上进行因果干预研究，在多种提示策略下注入显式外部置信信号并测量模型合规性。

Result: 基础模型可控性近乎完美，指令调优模型有上下文依赖，在显式命令提示下完全合规，自然对话查询中忽略信号；小模型内部词元级置信度无信息价值。

Conclusion: 指出语言模型存在部署关键故障模式，即用户期望的交互风格下安全修正效果最差，强调外部监督的必要性。

Abstract: Safety architectures for language models increasingly rely on external monitors to detect errors and inject corrective signals at inference time. For such systems to function in interactive settings, models must be able to incorporate externally provided confidence information into their verbal responses. In this work, we test whether instruction-tuned language models preserve this controllability across different interaction modes.
  Using Llama-3.2-3B on GSM8K, we perform a causal intervention study in which explicit external confidence signals are injected and model compliance is measured under multiple prompt strategies. We find that base models exhibit near-perfect controllability (Spearman rho close to 1.0), while instruction-tuned models display a striking context dependence: they fully comply with external corrections under explicit command prompts (bias approximately 0 percent, rho = 0.93), yet systematically ignore the same signals in natural conversational queries (bias plus 40 percent, rho = 0.04).
  This behavior is not a capability failure; the model can process the signal, but an emergent property of RLHF optimization that prioritizes conversational fluency over external calibration cues in natural dialogue. We further show that internal token-level confidence in small models is uninformative (r = 0.035), underscoring the necessity of external supervision. Our findings highlight a deployment-critical failure mode: the interaction style users expect is precisely where safety corrections are least effective.

</details>


### [192] [Emissions and Performance Trade-off Between Small and Large Language Models](https://arxiv.org/abs/2601.08844)
*Anandita Garg,Uma Gaba,Deepan Muthirayan,Anish Roy Chowdhury*

Main category: cs.CL

TL;DR: 研究探讨微调小语言模型（SLMs）作为大语言模型（LLMs）可持续替代方案的潜力，发现多数任务中SLMs在减排同时保持可比性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）训练和推理能耗大、碳足迹高，需要寻找可持续替代方案。

Method: 对自然语言处理、推理和编程领域的选择任务，进行LLMs和微调SLMs的性能-排放权衡的对比分析。

Result: 六项任务中四项，SLMs在推理时大量减少碳排放同时保持可比性能。

Conclusion: 小模型可减轻大模型对环境的影响，推动可持续绿色AI发展。

Abstract: The advent of Large Language Models (LLMs) has raised concerns about their enormous carbon footprint, starting with energy-intensive training and continuing through repeated inference. This study investigates the potential of using fine-tuned Small Language Models (SLMs) as a sustainable alternative for predefined tasks. Here, we present a comparative analysis of the performance-emissions trade-off between LLMs and fine-tuned SLMs across selected tasks under Natural Language Processing, Reasoning and Programming. Our results show that in four out of the six selected tasks, SLMs maintained comparable performances for a significant reduction in carbon emissions during inference. Our findings demonstrate the viability of smaller models in mitigating the environmental impact of resource-heavy LLMs, thus advancing towards sustainable, green AI.

</details>


### [193] [Directional Attractors in LLM Reasoning: How Similarity Retrieval Steers Iterative Summarization Based Reasoning](https://arxiv.org/abs/2601.08846)
*Cagatay Tekin,Charbel Barakat,Luis Joseph Luna Limgenco*

Main category: cs.CL

TL;DR: 提出带跨链记忆的InftyThink，用语义缓存改进迭代推理，实验展示其效果与局限性。


<details>
  <summary>Details</summary>
Motivation: 现有迭代总结推理框架会重复生成相似推理策略，需改进。

Method: 引入带跨链记忆的InftyThink，用基于嵌入的语义缓存存储成功推理模式，推理时检索相似引理引导推理。

Result: 在MATH500、AIME2024和GPQA - Diamond实验中，语义引理检索提高结构化领域准确率，暴露异质领域测试失败模式；推理轨迹几何分析显示缓存检索会在嵌入空间产生方向偏差。

Conclusion: 基于相似性的记忆对大语言模型自我改进推理有好处也有局限。

Abstract: Iterative summarization based reasoning frameworks such as InftyThink enable long-horizon reasoning in large language models (LLMs) by controlling context growth, but they repeatedly regenerate similar reasoning strategies across tasks. We introduce InftyThink with Cross-Chain Memory, an extension that augments iterative reasoning with an embedding-based semantic cache of previously successful reasoning patterns. At each reasoning step, the model retrieves and conditions on the most semantically similar stored lemmas, guiding inference without expanding the context window indiscriminately. Experiments on MATH500, AIME2024, and GPQA-Diamond demonstrate that semantic lemma retrieval improves accuracy in structured domains while exposing failure modes in tests that include heterogeneous domains. Geometric analyses of reasoning trajectories reveal that cache retrieval induces directional biases in embedding space, leading to consistent fix (improve baseline accuracy) and break (degradation in baseline accuracy) attractors. Our results highlight both the benefits and limits of similarity-based memory for self-improving LLM reasoning.

</details>


### [194] [Scalable and Reliable Evaluation of AI Knowledge Retrieval Systems: RIKER and the Coherent Simulated Universe](https://arxiv.org/abs/2601.08847)
*JV Roig*

Main category: cs.CL

TL;DR: 提出RIKER评估方法以解决知识系统评估挑战，评估33模型得出多项结论并贡献通用评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有知识系统的评估方法存在静态基准易被污染、基于大语言模型的评判有系统偏差和提取地面真值需昂贵人工标注等问题。

Method: 提出RIKER，基于范式反转，从已知地面真值生成文档进行确定性评分和可扩展评估。

Result: 评估33个模型，发现上下文长度声明常超可用容量，跨文档聚合比单文档提取难，接地能力和抗幻觉能力不同。

Conclusion: 贡献了一个在能从结构化地面真值生成合成文档场景下构建可扩展且抗污染评估的领域无关方法。

Abstract: Evaluating knowledge systems (LLMs, RAG, knowledge graphs, etc) faces fundamental challenges: static benchmarks are vulnerable to contamination, LLM-based judges exhibit systematic biases, and ground truth extraction requires expensive human annotation. We present RIKER (Retrieval Intelligence and Knowledge Extraction Rating), both a benchmark and a replicable methodology based on paradigm inversion - generating documents from known ground truth rather than extracting ground truth from documents. This approach enables deterministic scoring and scalable evaluation without human annotation or reference models, and contamination resistance through regenerable corpora. Our evaluation of 33 models using over 21 billion tokens reveals that context length claims frequently exceed usable capacity, with significant degradation beyond 32K tokens; cross-document aggregation proves substantially harder than single-document extraction; and grounding ability and hallucination resistance are distinct capabilities - models excelling at finding facts that exist may still fabricate facts that do not. Beyond the specific benchmark, we contribute a domain-agnostic methodology for constructing scalable and contamination-resistant evaluations wherever synthetic documents can be generated from structured ground truth.

</details>


### [195] [PediaMind-R1: A Temperament-Aware Language Model for Personalized Early Childhood Care Reasoning via Cognitive Modeling and Preference Alignment](https://arxiv.org/abs/2601.08848)
*Zihe Zhang,Can Zhang,Yanheng Xu,Xin Hu,Jichao Leng*

Main category: cs.CL

TL;DR: 本文提出领域专用大语言模型PediaMind - R1用于智能育儿场景主动个性化，介绍其构建和评估方法，结果显示能准确解读幼儿气质并个性化推理。


<details>
  <summary>Details</summary>
Motivation: 在智能育儿场景实现主动个性化，突破传统系统提供通用建议的局限。

Method: 引入托马斯 - 切斯框架的气质理论构建知识图谱，采用两阶段训练管道（监督微调、GRPO对齐），设计包含气质敏感选择题测试和人工评估的评估框架。

Result: PediaMind - R1能准确解读幼儿气质特征并进行个性化推理。

Conclusion: 强调垂直领域建模与心理学理论结合的价值，为开发以用户为中心的大语言模型提供新方法。

Abstract: This paper presents PediaMind-R1, a domain-specialized large language model designed to achieve active personalization in intelligent parenting scenarios. Unlike conventional systems that provide generic suggestions, PediaMind-R1 draws on insights from developmental psychology. It introduces temperament theory from the Thomas-Chess framework and builds a temperament knowledge graph for infants and toddlers (0-3 years). Our two-stage training pipeline first uses supervised fine-tuning to teach structured chain-of-thought reasoning, and then applies a GRPO-based alignment stage to reinforce logical consistency, domain expertise, and empathetic caregiving strategies. We further design an evaluation framework comprising temperament-sensitive multiple-choice tests and human assessments. The results demonstrate that PediaMind-R1 can accurately interpret early childhood temperament profiles and proactively engage in individualized reasoning. This work highlights the value of integrating vertical-domain modeling with psychological theory. It offers a novel approach to developing user-centered LLMs that advance the practice of active personalization in sensitive caregiving contexts.

</details>


### [196] [Más contexto no es mejor. Paradoja de la dilución vectorial en RAG corporativos](https://arxiv.org/abs/2601.08851)
*Alex Dantart*

Main category: cs.CL

TL;DR: 研究“Contextualized Chunking”技术注摘要改进RAG上下文时的注入比例，发现倒U曲线，提出计算最优注入比例框架。


<details>
  <summary>Details</summary>
Motivation: 解决“Contextualized Chunking”技术引入“向量稀释”掩盖局部内容的问题。

Method: 评估不同的注入比例。

Result: 呈现倒U曲线，适度注入使召回率提升18%，超过临界阈值（CIR > 0.4）特定查询精度下降22%。

Conclusion: 提出计算最优注入比例的理论框架。

Abstract: Técnicas recientes de "Contextualized Chunking" inyectan resúmenes para mejorar el contexto en RAG, pero introducen una "dilución vectorial" que opaca el contenido local. Evaluando distintos ratios de inyección, demostramos una curva en "U invertida": una inyección moderada mejora el "Recall" (+18%), pero superar un umbral crítico (CIR > 0.4) reduce la precisión en un 22% para consultas específicas. Proponemos un marco teórico para calcular el ratio óptimo de inyección. --
  Recent "Contextualized Chunking" techniques inject summaries to improve RAG context but introduce "vector dilution" drowning out local content. Evaluating various injection ratios, we demonstrate an "inverted U" curve: moderate injection boosts Recall (+18%), but exceeding a critical threshold (CIR > 0.4) drops precision by 22% for specific queries. We propose a theoretical framework to calculate the optimal injection ratio.

</details>


### [197] [Evaluating Role-Consistency in LLMs for Counselor Training](https://arxiv.org/abs/2601.08892)
*Eric Rudolph,Natalie Engert,Jens Albrecht*

Main category: cs.CL

TL;DR: 文章聚焦在线咨询服务的辅导员培训，引入新数据集测试大语言模型角色一致性，评估Vicuna模型及多个开源大语言模型表现。


<details>
  <summary>Details</summary>
Motivation: 在线咨询服务兴起，需有效培训方法，扩展基于虚拟客户工具VirCo的研究。

Method: 引入含对抗攻击的新数据集，评估Vicuna模型响应的角色一致性和连贯性，对比不同开源大语言模型表现。

Result: 未明确提及具体结果。

Conclusion: 创建了对抗性数据集，进行了对话连贯性和角色一致性评估，对不同大语言模型进行了对比分析。

Abstract: The rise of online counseling services has highlighted the need for effective training methods for future counselors. This paper extends research on VirCo, a Virtual Client for Online Counseling, designed to complement traditional role-playing methods in academic training by simulating realistic client interactions. Building on previous work, we introduce a new dataset incorporating adversarial attacks to test the ability of large language models (LLMs) to maintain their assigned roles (role-consistency). The study focuses on evaluating the role consistency and coherence of the Vicuna model's responses, comparing these findings with earlier research. Additionally, we assess and compare various open-source LLMs for their performance in sustaining role consistency during virtual client interactions. Our contributions include creating an adversarial dataset, evaluating conversation coherence and persona consistency, and providing a comparative analysis of different LLMs.

</details>


### [198] [Rubric-Conditioned LLM Grading: Alignment, Uncertainty, and Robustness](https://arxiv.org/abs/2601.08843)
*Haotian Deng,Chris Farber,Jiyoon Lee,David Tang*

Main category: cs.CL

TL;DR: 本文对基于评分标准的简答题自动评分的大语言模型进行系统评估，分析在不同方面的表现及结果，为其可靠部署提供见解。


<details>
  <summary>Details</summary>
Motivation: 自动简答题评分富有挑战，大语言模型虽有潜力，但在基于评分标准场景下的可靠性需严格评估。

Method: 系统评估大模型在基于评分标准简答题评分中的表现，研究与专家判断的一致性、不确定性与准确性的权衡、模型鲁棒性，使用SciEntsBank基准和Qwen 2.5 - 72B进行实验。

Result: 二元任务中与专家判断一致性强，评分标准粒度增加时一致性下降；过滤低置信度预测可提高剩余子集准确性；模型对提示注入有抗性，但对同义词替换敏感。

Conclusion: 本研究揭示了基于评分标准的大模型评分器的能力和局限，强调不确定性估计和鲁棒性测试对可靠部署的重要性。

Abstract: Automated short-answer grading (ASAG) remains a challenging task due to the linguistic variability of student responses and the need for nuanced, rubric-aligned partial credit. While Large Language Models (LLMs) offer a promising solution, their reliability as automated judges in rubric-based settings requires rigorous assessment. In this paper, we systematically evaluate the performance of LLM-judges for rubric-based short-answer grading. We investigate three key aspects: the alignment of LLM grading with expert judgment across varying rubric complexities, the trade-off between uncertainty and accuracy facilitated by a consensus-based deferral mechanism, and the model's robustness under random input perturbations and adversarial attacks. Using the SciEntsBank benchmark and Qwen 2.5-72B, we find that alignment is strong for binary tasks but degrades with increased rubric granularity. Our "Trust Curve" analysis demonstrates a clear trade-off where filtering low-confidence predictions improves accuracy on the remaining subset. Additionally, robustness experiments reveal that while the model is resilient to prompt injection, it is sensitive to synonym substitutions. Our work provides critical insights into the capabilities and limitations of rubric-conditioned LLM judges, highlighting the importance of uncertainty estimation and robustness testing for reliable deployment.

</details>


### [199] [Imagine-then-Plan: Agent Learning from Adaptive Lookahead with World Models](https://arxiv.org/abs/2601.08955)
*Youwei Liu,Jian Wang,Hanlin Wang,Beichen Guo,Wenjie Li*

Main category: cs.CL

TL;DR: 提出Imagine - then - Plan (ITP)框架用于智能体学习，通过前瞻想象生成多步轨迹，有自适应前瞻机制，实验显示其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型方法多为单步或固定步长滚动，复杂任务规划潜力未充分挖掘。

Method: 提出ITP框架，让策略模型与世界模型交互生成多步想象轨迹，引入自适应前瞻机制，融合想象轨迹信号与当前观测，有免训练和强化训练两种变体。

Result: 在代表性智能体基准测试中，ITP显著优于竞争基线。

Conclusion: 自适应前瞻机制增强了智能体推理能力，为解决更广泛复杂任务提供了有价值的见解。

Abstract: Recent advances in world models have shown promise for modeling future dynamics of environmental states, enabling agents to reason and act without accessing real environments. Current methods mainly perform single-step or fixed-horizon rollouts, leaving their potential for complex task planning under-exploited. We propose Imagine-then-Plan (\texttt{ITP}), a unified framework for agent learning via lookahead imagination, where an agent's policy model interacts with the learned world model, yielding multi-step ``imagined'' trajectories. Since the imagination horizon may vary by tasks and stages, we introduce a novel adaptive lookahead mechanism by trading off the ultimate goal and task progress. The resulting imagined trajectories provide rich signals about future consequences, such as achieved progress and potential conflicts, which are fused with current observations, formulating a partially \textit{observable} and \textit{imaginable} Markov decision process to guide policy learning. We instantiate \texttt{ITP} with both training-free and reinforcement-trained variants. Extensive experiments across representative agent benchmarks demonstrate that \texttt{ITP} significantly outperforms competitive baselines. Further analyses validate that our adaptive lookahead largely enhances agents' reasoning capability, providing valuable insights into addressing broader, complex tasks.

</details>


### [200] [TranslateGemma Technical Report](https://arxiv.org/abs/2601.09012)
*Mara Finkelstein,Isaac Caswell,Tobias Domhan,Jan-Thorsten Peter,Juraj Juraska,Parker Riley,Daniel Deutsch,Cole Dilanni,Colin Cherry,Eleftheria Briakou,Elizabeth Nielsen,Jiaming Luo,Kat Black,Ryan Mullins,Sweta Agrawal,Wenda Xu,Erin Kats,Stephane Jaskiewicz,Markus Freitag,David Vilar*

Main category: cs.CL

TL;DR: 提出基于Gemma 3的开放机器翻译模型TranslateGemma，经两阶段微调，评估显示其效果好且有高效性和多模态能力。


<details>
  <summary>Details</summary>
Motivation: 增强Gemma 3基础模型的多语言翻译能力，为研究社区提供机器翻译工具。

Method: 采用两阶段微调，先监督微调（使用合成和人工翻译平行数据），后强化学习（用奖励模型优化）。

Result: 在WMT测试集评估中表现好，自动指标优于基线，小模型效率高，在图像翻译基准测试中有提升。

Conclusion: TranslateGemma是强大且可适应的机器翻译工具。

Abstract: We present TranslateGemma, a suite of open machine translation models based on the Gemma 3 foundation models. To enhance the inherent multilingual capabilities of Gemma 3 for the translation task, we employ a two-stage fine-tuning process. First, supervised fine-tuning is performed using a rich mixture of high-quality large-scale synthetic parallel data generated via state-of-the-art models and human-translated parallel data. This is followed by a reinforcement learning phase, where we optimize translation quality using an ensemble of reward models, including MetricX-QE and AutoMQM, targeting translation quality. We demonstrate the effectiveness of TranslateGemma with human evaluation on the WMT25 test set across 10 language pairs and with automatic evaluation on the WMT24++ benchmark across 55 language pairs. Automatic metrics show consistent and substantial gains over the baseline Gemma 3 models across all sizes. Notably, smaller TranslateGemma models often achieve performance comparable to larger baseline models, offering improved efficiency. We also show that TranslateGemma models retain strong multimodal capabilities, with enhanced performance on the Vistra image translation benchmark. The release of the open TranslateGemma models aims to provide the research community with powerful and adaptable tools for machine translation.

</details>


### [201] [Can LLMs interpret figurative language as humans do?: surface-level vs representational similarity](https://arxiv.org/abs/2601.09041)
*Samhita Bollepally,Aurora Sloman-Moll,Takashi Yamauchi*

Main category: cs.CL

TL;DR: 研究对比人类与四种不同大小的指令调优大语言模型对240个对话句子的评分，发现人类与模型在表面层次对齐，但在表征层次差异显著，尤其在解读比喻性句子时。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在解读比喻性和基于社会的语言时与人类判断的一致程度。

Method: 让人类参与者和四种不同大小的指令调优大语言模型（GPT - 4、Gemma - 2 - 9B、Llama - 3.2和Mistral - 7B）对240个代表六种语言特征的对话句子进行评分，每个句子配40个解释性问题，采用10分李克特量表。

Result: 人类和大语言模型在表面层次与人类对齐，但在表征层次差异显著，尤其在解读涉及习语和Z世代俚语的比喻性句子时。GPT - 4最接近人类表征模式，所有模型在处理依赖上下文和社会语用的表达（如讽刺、俚语和习语）时都有困难。

Conclusion: 大语言模型与人类在解读比喻性和社会语言时存在显著差异，在处理依赖上下文和社会语用表达方面有待提高。

Abstract: Large language models generate judgments that resemble those of humans. Yet the extent to which these models align with human judgments in interpreting figurative and socially grounded language remains uncertain. To investigate this, human participants and four instruction-tuned LLMs of different sizes (GPT-4, Gemma-2-9B, Llama-3.2, and Mistral-7B) rated 240 dialogue-based sentences representing six linguistic traits: conventionality, sarcasm, funny, emotional, idiomacy, and slang. Each of the 240 sentences was paired with 40 interpretive questions, and both humans and LLMs rated these sentences on a 10-point Likert scale. Results indicated that humans and LLMs aligned at the surface level with humans, but diverged significantly at the representational level, especially in interpreting figurative sentences involving idioms and Gen Z slang. GPT-4 most closely approximates human representational patterns, while all models struggle with context-dependent and socio-pragmatic expressions like sarcasm, slang, and idiomacy.

</details>


### [202] [Is Grokking Worthwhile? Functional Analysis and Transferability of Generalization Circuits in Transformers](https://arxiv.org/abs/2601.09049)
*Kaiyu He,Zhang Mian,Peilin Wu,Xinya Du,Zhiyu Chen*

Main category: cs.CL

TL;DR: 研究大语言模型在组合任务中‘泛化电路’的作用，发现非训练成熟和训练成熟模型推理路径相同，高精度和推理路径形成可独立，成熟电路新知识整合迁移性有限。


<details>
  <summary>Details</summary>
Motivation: 探究训练成熟的模型在下游任务是否优于未成熟模型，以及等待训练成熟阶段的计算成本是否值得。

Method: 进行机制研究来评估‘泛化电路’在知识同化和转移中的作用。

Result: 非训练成熟和训练成熟模型对分布内组合查询推理路径相同；高精度和推理路径形成可独立发生；成熟电路新知识整合迁移性有限。

Conclusion: ‘泛化电路’并非代表突然获得新推理范式，‘训练成熟’的Transformer未完全掌握组合逻辑。

Abstract: While Large Language Models (LLMs) excel at factual retrieval, they often struggle with the "curse of two-hop reasoning" in compositional tasks. Recent research suggests that parameter-sharing transformers can bridge this gap by forming a "Generalization Circuit" during a prolonged "grokking" phase. A fundamental question arises: Is a grokked model superior to its non-grokked counterparts on downstream tasks? Furthermore, is the extensive computational cost of waiting for the grokking phase worthwhile? In this work, we conduct a mechanistic study to evaluate the Generalization Circuit's role in knowledge assimilation and transfer. We demonstrate that: (i) The inference paths established by non-grokked and grokked models for in-distribution compositional queries are identical. This suggests that the "Generalization Circuit" does not represent the sudden acquisition of a new reasoning paradigm. Instead, we argue that grokking is the process of integrating memorized atomic facts into an naturally established reasoning path. (ii) Achieving high accuracy on unseen cases after prolonged training and the formation of a certain reasoning path are not bound; they can occur independently under specific data regimes. (iii) Even a mature circuit exhibits limited transferability when integrating new knowledge, suggesting that "grokked" Transformers do not achieve a full mastery of compositional logic.

</details>


### [203] [Mi:dm 2.0 Korea-centric Bilingual Language Models](https://arxiv.org/abs/2601.09066)
*Donghoon Shin,Sejung Lee,Soonmin Bae,Hwijung Ryu,Changwon Ok,Hoyoun Jung,Hyesung Ji,Jeehyun Lim,Jehoon Lee,Ji-Eun Han,Jisoo Baik,Mihyeon Kim,Riwoo Chung,Seongmin Lee,Wonjae Park,Yoonseok Heo,Youngkyung Seo,Seyoun Won,Boeun Kim,Cheolhun Heo,Eunkyeong Lee,Honghee Lee,Hyeongju Ju,Hyeontae Seo,Jeongyong Shim,Jisoo Lee,Junseok Koh,Junwoo Kim,Minho Lee,Minji Kang,Minju Kim,Sangha Nam,Seongheum Park,Taehyeong Kim,Euijai Ahn,Hong Seok Jeung,Jisu Shin,Jiyeon Kim,Seonyeong Song,Seung Hyun Kong,Sukjin Hong,Taeyang Yun,Yu-Seon Kim,A-Hyun Lee,Chae-Jeong Lee,Hye-Won Yu,Ji-Hyun Ahn,Song-Yeon Kim,Sun-Woo Jung,Eunju Kim,Eunji Ha,Jinwoo Baek,Yun-ji Lee,Wanjin Park,Jeong Yeop Kim,Eun Mi Kim,Hyoung Jun Park,Jung Won Yoon,Min Sung Noh,Myung Gyo Oh,Wongyoung Lee,Yun Jin Park,Young S. Kwon,Hyun Keun Kim,Jieun Lee,YeoJoo Park*

Main category: cs.CL

TL;DR: 介绍面向韩国的双语大模型Mi:dm 2.0，强调数据质量，有两种配置，表现出色并开源，助力韩国AI发展。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在处理韩语数据时存在数据不足、质量低和缺乏文化对齐的问题，为推进以韩国为中心的AI发展。

Method: 采用全面的数据处理流程，包括数据清洗、合成数据生成、数据混合与课程学习，使用自定义韩语优化分词器；提供两种配置。

Result: 在韩语特定基准测试中达到了最先进的性能，在KMMLU上零样本结果出色，在多领域内部评估结果良好。

Conclusion: Mi:dm 2.0以MIT许可发布，旨在加速韩国各行业、公共服务和教育领域的AI应用，加强韩国AI开发者社区。

Abstract: We introduce Mi:dm 2.0, a bilingual large language model (LLM) specifically engineered to advance Korea-centric AI. This model goes beyond Korean text processing by integrating the values, reasoning patterns, and commonsense knowledge inherent to Korean society, enabling nuanced understanding of cultural contexts, emotional subtleties, and real-world scenarios to generate reliable and culturally appropriate responses. To address limitations of existing LLMs, often caused by insufficient or low-quality Korean data and lack of cultural alignment, Mi:dm 2.0 emphasizes robust data quality through a comprehensive pipeline that includes proprietary data cleansing, high-quality synthetic data generation, strategic data mixing with curriculum learning, and a custom Korean-optimized tokenizer to improve efficiency and coverage. To realize this vision, we offer two complementary configurations: Mi:dm 2.0 Base (11.5B parameters), built with a depth-up scaling strategy for general-purpose use, and Mi:dm 2.0 Mini (2.3B parameters), optimized for resource-constrained environments and specialized tasks. Mi:dm 2.0 achieves state-of-the-art performance on Korean-specific benchmarks, with top-tier zero-shot results on KMMLU and strong internal evaluation results across language, humanities, and social science tasks. The Mi:dm 2.0 lineup is released under the MIT license to support extensive research and commercial use. By offering accessible and high-performance Korea-centric LLMs, KT aims to accelerate AI adoption across Korean industries, public services, and education, strengthen the Korean AI developer community, and lay the groundwork for the broader vision of K-intelligence. Our models are available at https://huggingface.co/K-intelligence. For technical inquiries, please contact midm-llm@kt.com.

</details>


### [204] [From Symbolic to Natural-Language Relations: Rethinking Knowledge Graph Construction in the Era of Large Language Models](https://arxiv.org/abs/2601.09069)
*Kanyao Han,Yushang Lai*

Main category: cs.CL

TL;DR: 知识图谱常用预定义符号关系模式构建，但有不足，大语言模型出现促使重新思考关系表示，提倡转向自然语言关系描述并提出混合设计原则。


<details>
  <summary>Details</summary>
Motivation: 现有符号关系知识图谱存在不足，大语言模型的出现改变了知识的创建和使用方式，需要重新思考关系表示。

Method: 提出从符号关系转向自然语言关系描述，并提出混合设计原则。

Result: 无明确提及具体结果。

Conclusion: 应重新思考关系表示，从符号关系转向自然语言关系描述，并采用混合设计原则。

Abstract: Knowledge graphs (KGs) have commonly been constructed using predefined symbolic relation schemas, typically implemented as categorical relation labels. This design has notable shortcomings: real-world relations are often contextual, nuanced, and sometimes uncertain, and compressing it into discrete relation labels abstracts away critical semantic detail. Nevertheless, symbolic-relation KGs remain widely used because they have been operationally effective and broadly compatible with pre-LLM downstream models and algorithms, in which KG knowledge could be retrieved or encoded into quantified features and embeddings at scale. The emergence of LLMs has reshaped how knowledge is created and consumed. LLMs support scalable synthesis of domain facts directly in concise natural language, and prompting-based inference favors context-rich free-form text over quantified representations. This position paper argues that these changes call for rethinking the representation of relations themselves rather than merely using LLMs to populate conventional schemas more efficiently. We therefore advocate moving from symbolic to natural-language relation descriptions, and we propose hybrid design principles that preserve a minimal structural backbone while enabling more flexible and context-sensitive relational representations.

</details>


### [205] [SubTokenTest: A Practical Benchmark for Real-World Sub-token Understanding](https://arxiv.org/abs/2601.09089)
*Shuyang Hou,Yi Hu,Muhan Zhang*

Main category: cs.CL

TL;DR: 现有大语言模型在字符级任务表现不佳，本文提出SubTokenTest基准测试评估子标记理解能力，对9个大模型评估并研究测试时间缩放和字符信息编码。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽推理能力提升，但在基本字符级任务存在问题，现有基准测试未体现实际应用需求，而很多现实应用依赖精确子标记理解，因此需要新基准。

Method: 引入SubTokenTest基准，涵盖四个领域的十个任务，将性能与复杂推理解耦以分离标记化相关失败，对九个先进大语言模型进行评估。

Result: 完成对九个先进大语言模型的综合评估，研究了测试时间缩放对标记推理的影响和字符级信息在隐藏状态中的编码情况。

Conclusion: 通过引入新的基准测试，可有效评估大语言模型的子标记理解能力，为提升模型性能提供方向。

Abstract: Recent advancements in large language models (LLMs) have significantly enhanced their reasoning capabilities. However, they continue to struggle with basic character-level tasks, such as counting letters in words, a problem rooted in their tokenization process. While existing benchmarks have highlighted this weakness through basic character operations, such failures are often dismissed due to lacking practical relevance. Yet, many real-world applications, such as navigating text-based maps or interpreting structured tables, rely heavily on precise sub-token understanding. In this regard, we introduce SubTokenTest, a comprehensive benchmark that assesses sub-token understanding through practical, utility-driven tasks. Our benchmark includes ten tasks across four domains and isolates tokenization-related failures by decoupling performance from complex reasoning. We provide a comprehensive evaluation of nine advanced LLMs. Additionally, we investigate the impact of test-time scaling on sub-token reasoning and explore how character-level information is encoded within the hidden states.

</details>


### [206] [How Many Human Judgments Are Enough? Feasibility Limits of Human Preference Evaluation](https://arxiv.org/abs/2601.09084)
*Wilson Y. Lee*

Main category: cs.CL

TL;DR: 研究人类偏好评估检测模型微小改进需多少判断，多数比较属扩散状态需更多判断，精心设计基准可改善检测效果，评估常因样本不足无结论。


<details>
  <summary>Details</summary>
Motivation: 不清楚可靠检测生成模型微小改进需多少人类偏好判断。

Method: 理论分析得出比例分配在偏好信号扩散时为极小极大最优策略，对大规模人类偏好数据集进行实证分析。

Result: 多数比较属扩散状态，需比通常采集量多的判断；精心设计基准可降低提示级方差，提升检测效果。

Conclusion: 人类评估无结论或负面结果常因评估效力不足，需考虑效应值、预算和协议设计。

Abstract: Human preference evaluations are widely used to compare generative models, yet it remains unclear how many judgments are required to reliably detect small improvements. We show that when preference signal is diffuse across prompts (i.e., all prompt types are similarly informative), proportional allocation is minimax-optimal: no allocation strategy substantially improves detectability. Empirical analysis of large-scale human preference datasets shows that most comparisons fall into this diffuse regime, exhibiting small preference margins that require far more judgments than typically collected, even in well-sampled comparisons. These limits persist across evaluation protocols and modalities, including chat, image generation, and code generation with execution feedback. In contrast, curated benchmarks that reduce prompt induced variability systematically induce larger margins and improve detectability through a $1.5\times$ reduction in prompt-level variance. Our results show that inconclusive or negative human evaluation outcomes frequently reflect underpowered evaluation rather than model equivalence, underscoring the need to account explicitly for effect size, budget, and protocol design.

</details>


### [207] [Adaptive Multi-Stage Patent Claim Generation with Unified Quality Assessment](https://arxiv.org/abs/2601.09120)
*Chen-Wei Liang,Bin Guo,Zhen-Yuan Wei,Mu-Jiang-Shan Wang*

Main category: cs.CL

TL;DR: 本文提出新的三阶段框架解决专利权利要求生成系统的问题，经实验验证有显著提升，为专利审查工作流提供全面解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有专利权利要求生成系统存在跨司法辖区泛化能力差、权利要求与现有技术语义关系建模不足、质量评估不可靠等问题。

Method: 采用三阶段框架，包含关系感知相似性分析、领域自适应权利要求生成、统一质量评估；运用多头注意力进行关系建模，结合课程学习和动态 LoRA 适配器选择，利用交叉注意力机制进行质量评估。

Result: 在多个数据集上实验表明，相比 GPT - 4o、Llama - 3.1 - 8B 等模型在 ROUGE - L、BERTScore 等指标上有显著提升，与人类专家相关性更高，跨司法辖区性能保留率更好。

Conclusion: 所提方法为自动化专利审查工作流提供了全面解决方案。

Abstract: Current patent claim generation systems face three fundamental limitations: poor cross-jurisdictional generalization, inadequate semantic relationship modeling between claims and prior art, and unreliable quality assessment. We introduce a novel three-stage framework that addresses these challenges through relationship-aware similarity analysis, domain-adaptive claim generation, and unified quality assessment. Our approach employs multi-head attention with eight specialized heads for explicit relationship modeling, integrates curriculum learning with dynamic LoRA adapter selection across five patent domains, and implements cross-attention mechanisms between evaluation aspects for comprehensive quality assessment. Extensive experiments on USPTO HUPD dataset, EPO patent collections, and Patent-CE benchmark demonstrate substantial improvements: 7.6-point ROUGE-L gain over GPT-4o, 8.3\% BERTScore enhancement over Llama-3.1-8B, and 0.847 correlation with human experts compared to 0.623 for separate evaluation models. Our method maintains 89.4\% cross-jurisdictional performance retention versus 76.2\% for baselines, establishing a comprehensive solution for automated patent prosecution workflows.

</details>


### [208] [ProFit: Leveraging High-Value Signals in SFT via Probability-Guided Token Selection](https://arxiv.org/abs/2601.09195)
*Tao Liu,Taiqiang Wu,Runming Yang,Shaoning Sun,Junjie Wang,Yujiu Yang*

Main category: cs.CL

TL;DR: 传统监督微调有问题，提出ProFit方法，实验显示其优于传统基线。


<details>
  <summary>Details</summary>
Motivation: 传统监督微调忽略语言一对多特性，导致模型过拟合非核心表达，且获取多样答案成本高。

Method: 揭示词元概率与语义重要性的联系，提出ProFit方法，选择性屏蔽低概率词元。

Result: ProFit在通用推理和数学基准测试中始终优于传统监督微调基线。

Conclusion: ProFit能有效解决传统监督微调的问题，可推广应用。

Abstract: Supervised fine-tuning (SFT) is a fundamental post-training strategy to align Large Language Models (LLMs) with human intent. However, traditional SFT often ignores the one-to-many nature of language by forcing alignment with a single reference answer, leading to the model overfitting to non-core expressions. Although our empirical analysis suggests that introducing multiple reference answers can mitigate this issue, the prohibitive data and computational costs necessitate a strategic shift: prioritizing the mitigation of single-reference overfitting over the costly pursuit of answer diversity. To achieve this, we reveal the intrinsic connection between token probability and semantic importance: high-probability tokens carry the core logical framework, while low-probability tokens are mostly replaceable expressions. Based on this insight, we propose ProFit, which selectively masks low-probability tokens to prevent surface-level overfitting. Extensive experiments confirm that ProFit consistently outperforms traditional SFT baselines on general reasoning and mathematical benchmarks.

</details>


### [209] [A.X K1 Technical Report](https://arxiv.org/abs/2601.09200)
*Sung Jun Cheon,Jaekyung Cho,Seongho Choi,Hyunjun Eun,Seokhwan Jo,Jaehyun Jun,Minsoo Kang,Jin Kim,Jiwon Kim,Minsang Kim,Sungwan Kim,Seungsik Kim,Tae Yoon Kim,Youngrang Kim,Hyeongmun Lee,Sangyeol Lee,Sungeun Lee,Youngsoon Lee,Yujin Lee,Seongmin Ok,Chanyong Park,Hyewoong Park,Junyoung Park,Hyunho Yang,Subin Yi,Soohyun Bae,Dhammiko Arya,Yongseok Choi,Sangho Choi,Dongyeon Cho,Seungmo Cho,Gyoungeun Han,Yong-jin Han,Seokyoung Hong,Hyeon Hwang,Wonbeom Jang,Minjeong Ju,Wonjin Jung,Keummin Ka,Sungil Kang,Dongnam Kim,Joonghoon Kim,Jonghwi Kim,SaeRom Kim,Sangjin Kim,Seongwon Kim,Youngjin Kim,Seojin Lee,Sunwoo Lee,Taehoon Lee,Chanwoo Park,Sohee Park,Sooyeon Park,Yohan Ra,Sereimony Sek,Seungyeon Seo,Gun Song,Sanghoon Woo,Janghan Yoon,Sungbin Yoon*

Main category: cs.CL

TL;DR: 介绍从头训练的519B参数混合专家语言模型A.X K1，其设计优化训练配置和词汇量，支持可控推理，提出训练方法，评估显示性能有竞争力且韩语表现突出。


<details>
  <summary>Details</summary>
Motivation: 缩小推理能力和推理效率之间的差距，实现跨多样现实场景的可扩展部署。

Method: 利用缩放定律在固定计算预算下优化训练配置和词汇量，采用多阶段数据处理管道整理语料，提出Think - Fusion训练方法。

Result: A.X K1在评估中表现出与领先开源模型有竞争力的性能，在韩语基准测试中有独特优势。

Conclusion: A.X K1在性能和韩语表现上具有优势，其设计和训练方法有助于实现可扩展部署。

Abstract: We introduce A.X K1, a 519B-parameter Mixture-of-Experts (MoE) language model trained from scratch. Our design leverages scaling laws to optimize training configurations and vocabulary size under fixed computational budgets. A.X K1 is pre-trained on a corpus of approximately 10T tokens, curated by a multi-stage data processing pipeline. Designed to bridge the gap between reasoning capability and inference efficiency, A.X K1 supports explicitly controllable reasoning to facilitate scalable deployment across diverse real-world scenarios. We propose a simple yet effective Think-Fusion training recipe, enabling user-controlled switching between thinking and non-thinking modes within a single unified model. Extensive evaluations demonstrate that A.X K1 achieves performance competitive with leading open-source models, while establishing a distinctive advantage in Korean-language benchmarks.

</details>


### [210] [Ability Transfer and Recovery via Modularized Parameters Localization](https://arxiv.org/abs/2601.09398)
*Songyao Jin,Kun Zhou,Wenqi Li,Peng Wang,Biwei Huang*

Main category: cs.CL

TL;DR: 研究大语言模型参数中能力分布，提出ACT方法，能恢复遗忘能力、融合多模型能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在特定领域等进行训练会降低其他能力、导致灾难性遗忘，需研究能力在参数中的分布。

Method: 分析相关模型在特定输入下的模块激活情况，发现能力相关激活集中在少量通道，提出ACT方法定位相关通道并选择性转移参数，再进行轻量级微调。

Result: 在多语言数学和科学推理实验中，ACT可恢复遗忘能力，还能融合多模型能力且干扰小。

Conclusion: ACT是有效的方法，代码和数据将公开。

Abstract: Large language models can be continually pre-trained or fine-tuned to improve performance in specific domains, languages, or skills, but this specialization often degrades other capabilities and may cause catastrophic forgetting. We investigate how abilities are distributed within LLM parameters by analyzing module activations under domain- and language-specific inputs for closely related models. Across layers and modules, we find that ability-related activations are highly concentrated in a small set of channels (typically <5\%), and these channels are largely disentangled with good sufficiency and stability. Building on these observations, we propose ACT (Activation-Guided Channel-wise Ability Transfer), which localizes ability-relevant channels via activation differences and selectively transfers only the corresponding parameters, followed by lightweight fine-tuning for compatibility. Experiments on multilingual mathematical and scientific reasoning show that ACT can recover forgotten abilities while preserving retained skills. It can also merge multiple specialized models to integrate several abilities into a single model with minimal interference. Our code and data will be publicly released.

</details>


### [211] [ReGraM: Region-First Knowledge Graph Reasoning for Medical Question Answering](https://arxiv.org/abs/2601.09280)
*Chaerin Lee,Sohee Park,Hyunsik Na,Daseon Choi*

Main category: cs.CL

TL;DR: 介绍ReGraM区域优先知识图谱推理框架用于医疗问答，实验表明其效果好，突出该范例提升事实准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有医疗问答中结合大语言模型和生物医学知识图谱的方法遍历全量知识图谱或大规模检索有噪声，导致多跳推理不稳定。

Method: 构建ReGraM区域优先知识图谱推理框架，构建查询对齐子图，在多证据感知模式下对局部区域进行逐步推理。

Result: 在七个医疗问答基准上实验，ReGraM始终优于基线模型，在MCQ上绝对准确率提高8.04%，SAQ提高4.50%，幻觉率降低42.9%。

Conclusion: 区域优先知识图谱推理是提升医疗问答事实准确性和一致性的有效范例。

Abstract: Recent studies in medical question answering (Medical QA) have actively explored the integration of large language models (LLMs) with biomedical knowledge graphs (KGs) to improve factual accuracy. However, most existing approaches still rely on traversing the entire KG or performing large-scale retrieval, which introduces substantial noise and leads to unstable multi-hop reasoning. We argue that the core challenge lies not in expanding access to knowledge, but in identifying and reasoning over the appropriate subset of evidence for each query. ReGraM is a region-first knowledge graph reasoning framework that addresses this challenge by constructing a query-aligned subgraph and performing stepwise reasoning constrained to this localized region under multiple evidence aware modes. By focusing inference on only the most relevant portion of the KG, ReGraM departs from the assumption that all relations are equally useful an assumption that rarely holds in domain-specific medical settings. Experiments on seven medical QA benchmarks demonstrate that ReGraM consistently outperforms a strong baseline (KGARevion), achieving an 8.04% absolute accuracy gain on MCQ, a 4.50% gain on SAQ, and a 42.9% reduction in hallucination rate. Ablation and qualitative analyses further show that aligning region construction with hop-wise reasoning is the primary driver of these improvements. Overall, our results highlight region-first KG reasoning as an effective paradigm for improving factual accuracy and consistency in medical QA.

</details>


### [212] [Routing with Generated Data: Annotation-Free LLM Skill Estimation and Expert Selection](https://arxiv.org/abs/2601.09692)
*Tianyi Niu,Justin Chih-Yao Chen,Genta Indra Winata,Shi-Xiong Zhang,Supriyo Chakraborty,Sambit Sahu,Yue Zhang,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CL

TL;DR: 介绍Routing with Generated Data (RGD)情境，比较查询 - 答案路由器和仅查询路由器，分析有效生成器特征，提出CASCAL仅查询路由器，其在弱生成器数据上表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型路由方法常需标注的真实数据，但实际中这些数据往往不可用。

Method: 评估查询 - 答案路由器和仅查询路由器，分析有效生成器的特征并据此过滤生成数据，提出CASCAL仅查询路由器，通过共识投票和层次聚类确定模型性能。

Result: 查询 - 答案路由器随生成器质量下降退化更快，过滤可提升生成数据质量，CASCAL对生成器质量更稳健，在弱生成器数据上比最佳查询 - 答案路由器绝对准确率高4.6%。

Conclusion: CASCAL在RGD情境下，尤其在生成器质量不佳时是一个更优的大语言模型路由方案。

Abstract: Large Language Model (LLM) routers dynamically select optimal models for given inputs. Existing approaches typically assume access to ground-truth labeled data, which is often unavailable in practice, especially when user request distributions are heterogeneous and unknown. We introduce Routing with Generated Data (RGD), a challenging setting in which routers are trained exclusively on generated queries and answers produced from high-level task descriptions by generator LLMs. We evaluate query-answer routers (using both queries and labels) and query-only routers across four diverse benchmarks and 12 models, finding that query-answer routers degrade faster than query-only routers as generator quality decreases. Our analysis reveals two crucial characteristics of effective generators: they must accurately respond to their own questions, and their questions must produce sufficient performance differentiation among the model pool. We then show how filtering for these characteristics can improve the quality of generated data. We further propose CASCAL, a novel query-only router that estimates model correctness through consensus voting and identifies model-specific skill niches via hierarchical clustering. CASCAL is substantially more robust to generator quality, outperforming the best query-answer router by 4.6% absolute accuracy when trained on weak generator data.

</details>


### [213] [Value-Aware Numerical Representations for Transformer Language Models](https://arxiv.org/abs/2601.09706)
*Andreea Dutulescu,Stefan Ruseti,Mihai Dascalu*

Main category: cs.CL

TL;DR: Transformer语言模型在数学推理基准测试表现好，但数值理解和运算脆弱，提出值感知数值表示方法提升性能。


<details>
  <summary>Details</summary>
Motivation: Transformer语言模型加工数字时以符号标记处理，未显式编码数值，导致系统误差，影响数值理解和运算。

Method: 引入值感知数值表示，用专用前缀标记增强标准分词输入，其嵌入显式基于数值。

Result: 在算术任务评估中，该方法在多种数值格式、任务和操作数长度上优于基线。

Conclusion: 显式编码数值是提高语言模型基本数值鲁棒性的有效方法。

Abstract: Transformer-based language models often achieve strong results on mathematical reasoning benchmarks while remaining fragile on basic numerical understanding and arithmetic operations. A central limitation is that numbers are processed as symbolic tokens whose embeddings do not explicitly encode numerical value, leading to systematic errors. We introduce a value-aware numerical representation that augments standard tokenized inputs with a dedicated prefix token whose embedding is explicitly conditioned on the underlying numerical value. This mechanism injects magnitude information directly into the model's input space while remaining compatible with existing tokenizers and decoder-only Transformer architectures. Evaluation on arithmetic tasks shows that the proposed approach outperforms baselines across numerical formats, tasks, and operand lengths. These results indicate that explicitly encoding numerical value is an effective and efficient way to improve fundamental numerical robustness in language models.

</details>


### [214] [Understanding or Memorizing? A Case Study of German Definite Articles in Language Models](https://arxiv.org/abs/2601.09313)
*Jonathan Drechsel,Erisa Bytyqi,Steffen Herbold*

Main category: cs.CL

TL;DR: 研究语言模型处理德语定冠词时是基于规则泛化还是记忆，发现模型至少部分依赖记忆。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型在语法一致性上的表现是基于规则泛化还是记忆，以德语定冠词为例。

Method: 使用基于梯度的可解释性方法GRADIEND学习特定性 - 格冠词转换的参数更新方向。

Result: 特定性 - 格冠词转换的更新常影响不相关的性 - 格设置，不同设置中受影响最大的神经元有大量重叠。

Conclusion: 模型并非严格基于规则编码德语定冠词，至少部分依赖记忆关联而非抽象语法规则。

Abstract: Language models perform well on grammatical agreement, but it is unclear whether this reflects rule-based generalization or memorization. We study this question for German definite singular articles, whose forms depend on gender and case. Using GRADIEND, a gradient-based interpretability method, we learn parameter update directions for gender-case specific article transitions. We find that updates learned for a specific gender-case article transition frequently affect unrelated gender-case settings, with substantial overlap among the most affected neurons across settings. These results argue against a strictly rule-based encoding of German definite articles, indicating that models at least partly rely on memorized associations rather than abstract grammatical rules.

</details>


### [215] [Improving Implicit Hate Speech Detection via a Community-Driven Multi-Agent Framework](https://arxiv.org/abs/2601.09342)
*Ewelina Gajewska,Katarzyna Budzynska,Jarosław A Chudziak*

Main category: cs.CL

TL;DR: 提出隐式仇恨言论的上下文检测框架，由多智能体系统实现，在ToxiGen数据集上效果超现有方法，提升分类准确率和公平性。


<details>
  <summary>Details</summary>
Motivation: 解决隐式仇恨言论检测问题，提升检测的准确性和公平性。

Method: 构建包含中央审核智能体和社区智能体的多智能体系统，集成公开知识源的社会文化背景，采用平衡准确率作为分类公平性的核心指标。

Result: 在ToxiGen数据集上超越了现有提示方法和其他方法，显著提高了所有目标群体的分类准确率和公平性。

Conclusion: 社区驱动的协商框架能有效提升隐式仇恨言论检测的分类准确率和公平性。

Abstract: This work proposes a contextualised detection framework for implicitly hateful speech, implemented as a multi-agent system comprising a central Moderator Agent and dynamically constructed Community Agents representing specific demographic groups. Our approach explicitly integrates socio-cultural context from publicly available knowledge sources, enabling identity-aware moderation that surpasses state-of-the-art prompting methods (zero-shot prompting, few-shot prompting, chain-of-thought prompting) and alternative approaches on a challenging ToxiGen dataset. We enhance the technical rigour of performance evaluation by incorporating balanced accuracy as a central metric of classification fairness that accounts for the trade-off between true positive and true negative rates. We demonstrate that our community-driven consultative framework significantly improves both classification accuracy and fairness across all target groups.

</details>


### [216] [Frame of Reference: Addressing the Challenges of Common Ground Representation in Situational Dialogs](https://arxiv.org/abs/2601.09365)
*Biswesh Mohapatra,Théo Charlot,Giovanni Duca,Mayank Palan,Laurent Romary,Justine Cassell*

Main category: cs.CL

TL;DR: 本文评估模型在情境对话中建立和利用共同基础的能力，测试多种表示方法并提出改进途径。


<details>
  <summary>Details</summary>
Motivation: 先前研究较少探讨如何明确表示和存储共同基础以供后续使用，不确定确认或澄清行为是否真正反映了有根据的理解。

Method: 评估模型通过关联引用实体来建立和利用共同基础的能力，测试多种表示共同基础的方法。

Result: 未提及具体研究结果。

Conclusion: 未提及结论内容，但提出改进共同基础建立和使用的方法。

Abstract: Common ground plays a critical role in situated spoken dialogues, where interlocutors must establish and maintain shared references to entities, events, and relations to sustain coherent interaction. For dialog systems, the ability to correctly ground conversational content in order to refer back to it later is particularly important. Prior studies have demonstrated that LLMs are capable of performing grounding acts such as requesting clarification or producing acknowledgments, yet relatively little work has investigated how common ground can be explicitly represented and stored for later use. Without such mechanisms, it remains unclear whether acknowledgment or clarification behaviors truly reflect a grounded understanding. In this work, we evaluate a model's ability to establish and exploit common ground through relational references to entities within the shared context in a situational dialogue. We test multiple methods for representing common ground in situated dialogues and further propose approaches to improve both the establishment of common ground and its subsequent use in the conversation.

</details>


### [217] [Bias Dynamics in BabyLMs: Towards a Compute-Efficient Sandbox for Democratising Pre-Training Debiasing](https://arxiv.org/abs/2601.09421)
*Filip Trhlik,Andrew Caines,Paula Buttery*

Main category: cs.CL

TL;DR: 本文提出用低成本代理模型BabyLMs进行预训练模型去偏研究，可降低成本并促进相关研究。


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型规模增长导致理解和缓解偏差的研究受限，重新训练成本高，现有去偏策略难以解决根本问题，需要低成本研究方法。

Method: 使用BabyLMs（基于小可变语料库训练的紧凑类BERT模型）近似大模型的偏差获取和学习动态，进行预训练模型去偏实验。

Result: BabyLMs与标准BERT模型在偏差形成和性能发展模式上高度一致，在多种去偏方法下有相关性，能将预训练成本从超500 GPU - 小时降至30 GPU - 小时以下。

Conclusion: BabyLMs可作为大规模语言模型的有效测试平台，能促进预训练模型去偏研究，使构建更公平语言模型的方法探索更快速、易获取。

Abstract: Pre-trained language models (LMs) have, over the last few years, grown substantially in both societal adoption and training costs. This rapid growth in size has constrained progress in understanding and mitigating their biases. Since re-training LMs is prohibitively expensive, most debiasing work has focused on post-hoc or masking-based strategies, which often fail to address the underlying causes of bias. In this work, we seek to democratise pre-model debiasing research by using low-cost proxy models. Specifically, we investigate BabyLMs, compact BERT-like models trained on small and mutable corpora that can approximate bias acquisition and learning dynamics of larger models. We show that BabyLMs display closely aligned patterns of intrinsic bias formation and performance development compared to standard BERT models, despite their drastically reduced size. Furthermore, correlations between BabyLMs and BERT hold across multiple intra-model and post-model debiasing methods. Leveraging these similarities, we conduct pre-model debiasing experiments with BabyLMs, replicating prior findings and presenting new insights regarding the influence of gender imbalance and toxicity on bias formation. Our results demonstrate that BabyLMs can serve as an effective sandbox for large-scale LMs, reducing pre-training costs from over 500 GPU-hours to under 30 GPU-hours. This provides a way to democratise pre-model debiasing research and enables faster, more accessible exploration of methods for building fairer LMs.

</details>


### [218] [Where Knowledge Collides: A Mechanistic Study of Intra-Memory Knowledge Conflict in Language Models](https://arxiv.org/abs/2601.09445)
*Minh Vu Pham,Hsuvas Borkakoty,Yufang Hou*

Main category: cs.CL

TL;DR: 提出基于机制解释方法的框架识别语言模型中预训练数据冲突知识的编码位置和方式，并展示如何在推理时控制冲突知识。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注解决模型内部知识与外部资源的冲突，而模型预训练内部表示的冲突定位问题未被探索。

Method: 设计基于机制解释方法的框架。

Result: 发现语言模型特定内部组件负责编码预训练冲突知识。

Conclusion: 机制解释方法可用于在推理时对冲突知识进行因果干预和控制。

Abstract: In language models (LMs), intra-memory knowledge conflict largely arises when inconsistent information about the same event is encoded within the model's parametric knowledge. While prior work has primarily focused on resolving conflicts between a model's internal knowledge and external resources through approaches such as fine-tuning or knowledge editing, the problem of localizing conflicts that originate during pre-training within the model's internal representations remain unexplored. In this work, we design a framework based on mechanistic interpretability methods to identify where and how conflicting knowledge from the pre-training data is encoded within LMs. Our findings contribute to a growing body of evidence that specific internal components of a language model are responsible for encoding conflicting knowledge from pre-training, and we demonstrate how mechanistic interpretability methods can be leveraged to causally intervene in and control conflicting knowledge at inference time.

</details>


### [219] [Improving Symbolic Translation of Language Models for Logical Reasoning](https://arxiv.org/abs/2601.09446)
*Ramya Keerthy Thatikonda,Jiuzhou Han,Wray Buntine,Ehsan Shareghi*

Main category: cs.CL

TL;DR: 本文提出综合微调、增量推理和验证模块，减少小语言模型在自然语言转一阶逻辑中的错误，提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 小语言模型在将自然语言翻译成一阶逻辑时易出错，现有自迭代纠错方法依赖底层模型能力。

Method: 对常见错误分类，用大语言模型合成数据微调小语言模型；引入增量推理，分两阶段推理；使用针对谓词元数错误的验证模块。

Result: 在四个逻辑推理数据集上评估三类模型，降低错误率、增加谓词覆盖率、提升推理性能。

Conclusion: 综合方法使小语言模型更接近开发可靠且易访问的符号推理系统。

Abstract: The use of formal language for deductive logical reasoning aligns well with language models (LMs), where translating natural language (NL) into first-order logic (FOL) and employing an external solver results in a verifiable and therefore reliable reasoning system. However, smaller LMs often struggle with this translation task, frequently producing incorrect symbolic outputs due to formatting and translation errors. Existing approaches typically rely on self-iteration to correct these errors, but such methods depend heavily on the capabilities of the underlying model. To address this, we first categorize common errors and fine-tune smaller LMs using data synthesized by large language models. The evaluation is performed using the defined error categories. We introduce incremental inference, which divides inference into two stages, predicate generation and FOL translation, providing greater control over model behavior and enhancing generation quality as measured by predicate metrics. This decomposition framework also enables the use of a verification module that targets predicate-arity errors to further improve performance. Our study evaluates three families of models across four logical-reasoning datasets. The comprehensive fine-tuning, incremental inference, and verification modules reduce error rates, increase predicate coverage, and improve reasoning performance for smaller LMs, moving us closer to developing reliable and accessible symbolic-reasoning systems.

</details>


### [220] [Benchmarking Post-Training Quantization of Large Language Models under Microscaling Floating Point Formats](https://arxiv.org/abs/2601.09555)
*Manyi Zhang,Ji-Fu Li,Zhongao Sun,Haoli Bai,Hui-Ling Zhen,Zhenhua Dong,Xianzhi Yu*

Main category: cs.CL

TL;DR: 本文对MXFP格式下的PTQ算法进行系统性研究，给出研究结果并为适配现有PTQ方法提供指导。


<details>
  <summary>Details</summary>
Motivation: 现有PTQ算法大多聚焦整数量化，其在MXFP格式下的适用性和表现有待研究。

Method: 对7种以上PTQ算法、15个评估基准和3类大语言模型家族进行系统性研究。

Result: MXFP8近乎无损，MXFP4有明显精度下降；PTQ有效性依赖格式兼容性；PTQ性能在模型家族和模态中有一致趋势；MXFP4量化缩放因子是关键误差源，预缩放优化策略可缓解影响。

Conclusion: 研究结果为将现有PTQ方法适配MXFP量化提供了实际指导。

Abstract: Microscaling Floating-Point (MXFP) has emerged as a promising low-precision format for large language models (LLMs). Despite various post-training quantization (PTQ) algorithms being proposed, they mostly focus on integer quantization, while their applicability and behavior under MXFP formats remain largely unexplored. To address this gap, this work conducts a systematic investigation of PTQ under MXFP formats, encompassing over 7 PTQ algorithms, 15 evaluation benchmarks, and 3 LLM families. The key findings include: 1) MXFP8 consistently achieves near-lossless performance, while MXFP4 introduces substantial accuracy degradation and remains challenging; 2) PTQ effectiveness under MXFP depends strongly on format compatibility, with some algorithmic paradigms being consistently more effective than others; 3) PTQ performance exhibits highly consistent trends across model families and modalities, in particular, quantization sensitivity is dominated by the language model rather than the vision encoder in multimodal LLMs; 4) The scaling factor of quantization is a critical error source in MXFP4, and a simple pre-scale optimization strategy can significantly mitigate its impact. Together, these results provide practical guidance on adapting existing PTQ methods to MXFP quantization.

</details>


### [221] [DPWriter: Reinforcement Learning with Diverse Planning Branching for Creative Writing](https://arxiv.org/abs/2601.09609)
*Qian Cao,Yahui Liu,Wei Bi,Yi Zhao,Ruihua Song,Xiting Wang,Ruiming Tang,Guorui Zhou,Han Li*

Main category: cs.CL

TL;DR: 本文提出基于半结构化长思维链的强化学习框架，提升大语言模型输出多样性，实验显示该方法有效。


<details>
  <summary>Details</summary>
Motivation: 基于强化学习的大语言模型增强常导致输出多样性降低，现有方法缺乏明确的多样化探索机制。

Method: 提出基于半结构化长思维链的强化学习框架，引入多样化规划分支方法和群体感知多样性奖励。

Result: 在创意写作基准测试中，该方法显著提高输出多样性且不影响生成质量，持续优于现有基线。

Conclusion: 所提方法能有效解决大语言模型输出多样性问题。

Abstract: Reinforcement learning (RL)-based enhancement of large language models (LLMs) often leads to reduced output diversity, undermining their utility in open-ended tasks like creative writing. Current methods lack explicit mechanisms for guiding diverse exploration and instead prioritize optimization efficiency and performance over diversity. This paper proposes an RL framework structured around a semi-structured long Chain-of-Thought (CoT), in which the generation process is decomposed into explicitly planned intermediate steps. We introduce a Diverse Planning Branching method that strategically introduces divergence at the planning phase based on diversity variation, alongside a group-aware diversity reward to encourage distinct trajectories. Experimental results on creative writing benchmarks demonstrate that our approach significantly improves output diversity without compromising generation quality, consistently outperforming existing baselines.

</details>


### [222] [LLMs can Compress LLMs: Adaptive Pruning by Agents](https://arxiv.org/abs/2601.09694)
*Sai Varun Kodathala,Rakesh Vunnam*

Main category: cs.CL

TL;DR: 介绍代理引导剪枝方法以减少大语言模型计算成本，在Qwen3模型上评估有显著提升，无需再训练。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型剪枝方法依赖统一或手工启发式确定每层稀疏率，且剪枝后模型事实知识退化严重。

Method: 引入代理引导剪枝，结合权重 - 激活指标和梯度重要性分数构建层敏感度概况，由有自我反思能力的大语言模型代理处理统计数据，有检查点回滚机制。

Result: 在Qwen3模型约45%稀疏率下，相比结构化剪枝基线有显著提升，如MMLU准确率提升56%等。

Conclusion: 基础模型能有效引导其他基础模型的压缩，方法无需再训练，有自纠正能力。

Abstract: As Large Language Models (LLMs) continue to scale, post-training pruning has emerged as a promising approach to reduce computational costs while preserving performance. Existing methods such as SparseGPT and Wanda achieve high sparsity through layer-wise weight reconstruction or activation-aware magnitude pruning, but rely on uniform or hand-crafted heuristics to determine per-layer sparsity ratios. Moreover, recent work has shown that pruned LLMs suffer from severe factual knowledge degradation, with structured pruning methods experiencing near-total collapse in factual question-answering capabilities. We introduce agent-guided pruning, where a foundation model acts as an adaptive pruning agent to intelligently select which layers to prune at each iteration while preserving critical knowledge pathways. Our method constructs layer-wise sensitivity profiles by combining Wanda-inspired weight-activation metrics with gradient importance scores, normalized as z-scores for model-agnostic comparison. These statistics are processed by an LLM agent equipped with self-reflection capabilities, enabling it to learn from previous pruning outcomes and iteratively refine its strategy. A checkpoint rollback mechanism maintains model quality by reverting when perplexity degradation exceeds a threshold. We evaluate our approach on Qwen3 models (4B and 8B parameters) at approximately 45% sparsity, demonstrating substantial improvements over structured pruning baselines: 56% relative improvement in MMLU accuracy, 19x better factual knowledge retention on FreebaseQA, and 69% lower perplexity degradation. Notably, our framework requires no retraining, operates in a model-agnostic manner, and exhibits effective self-correction with only 2-4 rollbacks across 21-40 iterations, demonstrating that foundation models can effectively guide the compression of other foundation models.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [223] [Reading or Reasoning? Format Decoupled Reinforcement Learning for Document OCR](https://arxiv.org/abs/2601.08834)
*Yufeng Zhong,Lei Chen,Zhixiong Zeng,Xuanle Zhao,Deyang Jiang,Liming Zheng,Jing Huang,Haibo Qiu,Peng Shi,Siqi Yang,Lin Ma*

Main category: cs.CV

TL;DR: 现有OCR模型在处理格式化文本时输出不确定性高，本文提出FD - RL方法，在OmniDocBench上创新高并进行消融实验验证各策略有效性


<details>
  <summary>Details</summary>
Motivation: 观察到高级OCR模型处理格式化文本时输出不确定性高，推理不同阅读路径可能提升OCR性能

Method: 提出格式解耦强化学习（FD - RL），采用基于熵的数据过滤策略和格式解耦奖励

Result: FD - RL在OmniDocBench上平均得分90.41，创端到端模型新记录

Conclusion: 通过全面的消融实验验证了数据、训练、过滤和奖励策略的有效性

Abstract: Reading text from images or scanned documents via OCR models has been a longstanding focus of researchers. Intuitively, text reading is perceived as a straightforward perceptual task, and existing work primarily focuses on constructing enriched data engineering to enhance SFT capabilities. In this work, we observe that even advanced OCR models exhibit significantly higher entropy in formatted text (\emph{e.g.}, formula, table, etc.) compared to plain text, often by an order of magnitude. These statistical patterns reveal that advanced OCR models struggle with high output uncertainty when dealing with format sensitive document, suggesting that reasoning over diverse reading pathways may improve OCR performance. To address this, we propose format decoupled reinforcement learning (FD-RL), which leverages high-entropy patterns for targeted optimization. Our approach employs entropy-based data filtration strategy to identify format-intensive instances, and adopt format decoupled rewards tailored to different format types, enabling format-level validation rather than token-level memorization. FD-RL achieves an average score of 90.41 on OmniDocBench, setting a new record for end-to-end models on this highly popular benchmark. More importantly, we conduct comprehensive ablation studies over data, training, filtering, and rewarding strategies, thoroughly validating their effectiveness.

</details>


### [224] [Bias Detection and Rotation-Robustness Mitigation in Vision-Language Models and Generative Image Models](https://arxiv.org/abs/2601.08860)
*Tarannum Mithila*

Main category: cs.CV

TL;DR: 研究视觉-语言和生成模型中旋转扰动与分布偏移下的偏差传播和鲁棒性问题，提出旋转鲁棒缓解策略并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型和生成图像模型在输入变换下的鲁棒性和公平性研究不足，需探究偏差传播和鲁棒性退化问题。

Method: 分析旋转诱导的扰动对模型预测、置信度校准和人口偏差模式的影响；提出结合数据增强、表示对齐和模型级正则化的旋转鲁棒缓解策略。

Result: 在多个数据集上的实验表明，提出的方法显著提高了鲁棒性，减少了偏差放大，且不牺牲整体性能。

Conclusion: 指出当前多模态系统的关键局限性，为构建更可靠和公平的AI模型提供实用缓解技术。

Abstract: Vision-Language Models (VLMs) and generative image models have achieved remarkable performance across multimodal tasks, yet their robustness and fairness under input transformations remain insufficiently explored. This work investigates bias propagation and robustness degradation in state-of-the-art vision-language and generative models, with a particular focus on image rotation and distributional shifts. We analyze how rotation-induced perturbations affect model predictions, confidence calibration, and demographic bias patterns. To address these issues, we propose rotation-robust mitigation strategies that combine data augmentation, representation alignment, and model-level regularization. Experimental results across multiple datasets demonstrate that the proposed methods significantly improve robustness while reducing bias amplification without sacrificing overall performance. This study highlights critical limitations of current multimodal systems and provides practical mitigation techniques for building more reliable and fair AI models.

</details>


### [225] [R$^2$BD: A Reconstruction-Based Method for Generalizable and Efficient Detection of Fake Images](https://arxiv.org/abs/2601.08867)
*Qingyu Liu,Zhongjie Ba,Jianmin Guo,Qiu Wang,Zhibo Wang,Jie Shi,Kui Ren*

Main category: cs.CV

TL;DR: 提出R²BD框架用于AIGC图像检测，比现有重建方法快22倍以上，跨数据集评估表现好。


<details>
  <summary>Details</summary>
Motivation: 现有基于重建的AIGC图像检测方法效率低、泛化性受限，需改进。

Method: 提出R²BD框架，包含统一重建模型G - LDM和单步推理的残差偏差计算模块。

Result: 在10个公共数据集基准测试中，比现有重建方法快22倍以上，跨数据集评估平均优于现有方法13.87%。

Conclusion: R²BD框架在AIGC图像检测中具有高效率和强泛化性。

Abstract: Recently, reconstruction-based methods have gained attention for AIGC image detection. These methods leverage pre-trained diffusion models to reconstruct inputs and measure residuals for distinguishing real from fake images. Their key advantage lies in reducing reliance on dataset-specific artifacts and improving generalization under distribution shifts. However, they are limited by significant inefficiency due to multi-step inversion and reconstruction, and their reliance on diffusion backbones further limits generalization to other generative paradigms such as GANs.
  In this paper, we propose a novel fake image detection framework, called R$^2$BD, built upon two key designs: (1) G-LDM, a unified reconstruction model that simulates the generation behaviors of VAEs, GANs, and diffusion models, thereby broadening the detection scope beyond prior diffusion-only approaches; and (2) a residual bias calculation module that distinguishes real and fake images in a single inference step, which is a significant efficiency improvement over existing methods that typically require 20$+$ steps.
  Extensive experiments on the benchmark from 10 public datasets demonstrate that R$^2$BD is over 22$\times$ faster than existing reconstruction-based methods while achieving superior detection accuracy. In cross-dataset evaluations, it outperforms state-of-the-art methods by an average of 13.87\%, showing strong efficiency and generalization across diverse generative methods. The code and dataset used for evaluation are available at https://github.com/QingyuLiu/RRBD.

</details>


### [226] [Residual Cross-Modal Fusion Networks for Audio-Visual Navigation](https://arxiv.org/abs/2601.08868)
*Yi Wang,Yinfeng Yu,Bin Ren*

Main category: cs.CV

TL;DR: 提出Cross - Modal Residual Fusion Network (CRFN)用于视听具身导航，在数据集上表现优于基线，发现不同数据集上智能体模态依赖差异。


<details>
  <summary>Details</summary>
Motivation: 解决视听具身导航中多模态融合时有效建模异质特征交互的挑战，避免单模态主导或信息退化。

Method: 提出CRFN，在音频和视觉流之间引入双向残差交互，通过残差连接显式建模跨模态交互并结合稳定技术。

Result: 在Replica和Matterport3D数据集上显著优于现有融合基线，有更强跨域泛化能力，发现不同数据集上智能体有不同模态依赖。

Conclusion: CRFN在视听具身导航任务中表现良好，发现的现象为理解具身智能体跨模态协作机制提供新视角。

Abstract: Audio-visual embodied navigation aims to enable an agent to autonomously localize and reach a sound source in unseen 3D environments by leveraging auditory cues. The key challenge of this task lies in effectively modeling the interaction between heterogeneous features during multimodal fusion, so as to avoid single-modality dominance or information degradation, particularly in cross-domain scenarios. To address this, we propose a Cross-Modal Residual Fusion Network, which introduces bidirectional residual interactions between audio and visual streams to achieve complementary modeling and fine-grained alignment, while maintaining the independence of their representations. Unlike conventional methods that rely on simple concatenation or attention gating, CRFN explicitly models cross-modal interactions via residual connections and incorporates stabilization techniques to improve convergence and robustness. Experiments on the Replica and Matterport3D datasets demonstrate that CRFN significantly outperforms state-of-the-art fusion baselines and achieves stronger cross-domain generalization. Notably, our experiments also reveal that agents exhibit differentiated modality dependence across different datasets. The discovery of this phenomenon provides a new perspective for understanding the cross-modal collaboration mechanism of embodied agents.

</details>


### [227] [ForensicFormer: Hierarchical Multi-Scale Reasoning for Cross-Domain Image Forgery Detection](https://arxiv.org/abs/2601.08873)
*Hema Hariharan Samson*

Main category: cs.CV

TL;DR: 提出ForensicFormer框架用于跨域伪造检测，在多个测试集上表现优异，有良好鲁棒性和定位能力，连接经典图像取证与深度学习。


<details>
  <summary>Details</summary>
Motivation: AI生成图像和先进编辑工具使传统取证方法对跨域伪造检测失效，需新方法。

Method: 提出ForensicFormer，通过交叉注意力变压器统一低、中、高三个层次的分析。

Result: 在七个测试集上平均准确率达86.8%，对JPEG压缩鲁棒性好，像素级伪造定位F1分数0.76，各组件提升4 - 10%准确率。

Conclusion: 工作连接经典图像取证与深度学习，为未知操作技术的实际应用提供实用方案。

Abstract: The proliferation of AI-generated imagery and sophisticated editing tools has rendered traditional forensic methods ineffective for cross-domain forgery detection. We present ForensicFormer, a hierarchical multi-scale framework that unifies low-level artifact detection, mid-level boundary analysis, and high-level semantic reasoning via cross-attention transformers. Unlike prior single-paradigm approaches, which achieve <75% accuracy on out-of-distribution datasets, our method maintains 86.8% average accuracy across seven diverse test sets, spanning traditional manipulations, GAN-generated images, and diffusion model outputs - a significant improvement over state-of-the-art universal detectors. We demonstrate superior robustness to JPEG compression (83% accuracy at Q=70 vs. 66% for baselines) and provide pixel-level forgery localization with a 0.76 F1-score. Extensive ablation studies validate that each hierarchical component contributes 4-10% accuracy improvement, and qualitative analysis reveals interpretable forensic features aligned with human expert reasoning. Our work bridges classical image forensics and modern deep learning, offering a practical solution for real-world deployment where manipulation techniques are unknown a priori.

</details>


### [228] [Learning Domain-Invariant Representations for Cross-Domain Image Registration via Scene-Appearance Disentanglement](https://arxiv.org/abs/2601.08875)
*Jiahao Qin,Yiwen Wang*

Main category: cs.CV

TL;DR: 提出SAR - Net框架解决域偏移下的图像配准问题，理论上证明其有效性，实证表现优于基线且实时性好


<details>
  <summary>Details</summary>
Motivation: 传统图像配准方法在源图像和目标图像存在系统强度差异时，亮度恒定假设被违反，导致对应估计不适定

Method: 提出SAR - Net框架，将观测图像分解为域不变的场景表示和特定于域的外观代码，通过重新渲染而非直接强度匹配进行配准，建立理论条件并证明相关命题

Result: 在双向扫描显微镜上验证，实现0.885 SSIM和0.979 NCC，比最强基线提升3.1倍，维持77 fps的实时性能

Conclusion: 场景一致性和域对齐损失对SAR - Net性能很关键，移除任一损失都会大幅降低性能

Abstract: Image registration under domain shift remains a fundamental challenge in computer vision and medical imaging: when source and target images exhibit systematic intensity differences, the brightness constancy assumption underlying conventional registration methods is violated, rendering correspondence estimation ill-posed. We propose SAR-Net, a unified framework that addresses this challenge through principled scene-appearance disentanglement. Our key insight is that observed images can be decomposed into domain-invariant scene representations and domain-specific appearance codes, enabling registration via re-rendering rather than direct intensity matching. We establish theoretical conditions under which this decomposition enables consistent cross-domain alignment (Proposition 1) and prove that our scene consistency loss provides a sufficient condition for geometric correspondence in the shared latent space (Proposition 2). Empirically, we validate SAR-Net on bidirectional scanning microscopy, where coupled domain shift and geometric distortion create a challenging real-world testbed. Our method achieves 0.885 SSIM and 0.979 NCC, representing 3.1x improvement over the strongest baseline, while maintaining real-time performance (77 fps). Ablation studies confirm that both scene consistency and domain alignment losses are necessary: removing either degrades performance by 90% SSIM or causes 223x increase in latent alignment error, respectively. Code and data are available at https://github.com/D-ST-Sword/SAR-NET.

</details>


### [229] [TAG-MoE: Task-Aware Gating for Unified Generative Mixture-of-Experts](https://arxiv.org/abs/2601.08881)
*Yu Xu,Hongbin Yan,Juan Cao,Yiji Cheng,Tiankai Hang,Runze He,Zijin Yin,Shiyi Zhang,Yuxin Zhang,Jintao Li,Chunyu Wang,Qinglin Lu,Tong-Yee Lee,Fan Tang*

Main category: cs.CV

TL;DR: 本文提出向MoE路由注入语义意图的框架，有效减轻任务干扰，表现优于密集基线。


<details>
  <summary>Details</summary>
Motivation: 统一图像生成和编辑模型在密集扩散变压器架构中存在严重任务干扰，现有稀疏MoE范式的门控网络与任务无关，无法解决任务干扰。

Method: 引入分层任务语义注释方案创建结构化任务描述符，设计预测对齐正则化使内部路由决策与任务高级语义对齐。

Result: 模型有效减轻任务干扰，在保真度和质量上优于密集基线，专家自然形成清晰且语义相关的专业化。

Conclusion: 所提框架能解决统一图像生成和编辑模型的任务干扰问题。

Abstract: Unified image generation and editing models suffer from severe task interference in dense diffusion transformers architectures, where a shared parameter space must compromise between conflicting objectives (e.g., local editing v.s. subject-driven generation). While the sparse Mixture-of-Experts (MoE) paradigm is a promising solution, its gating networks remain task-agnostic, operating based on local features, unaware of global task intent. This task-agnostic nature prevents meaningful specialization and fails to resolve the underlying task interference. In this paper, we propose a novel framework to inject semantic intent into MoE routing. We introduce a Hierarchical Task Semantic Annotation scheme to create structured task descriptors (e.g., scope, type, preservation). We then design Predictive Alignment Regularization to align internal routing decisions with the task's high-level semantics. This regularization evolves the gating network from a task-agnostic executor to a dispatch center. Our model effectively mitigates task interference, outperforming dense baselines in fidelity and quality, and our analysis shows that experts naturally develop clear and semantically correlated specializations.

</details>


### [230] [Compressing Vision Transformers in Geospatial Transfer Learning with Manifold-Constrained Optimization](https://arxiv.org/abs/2601.08882)
*Thomas Snyder,H. Lexie Yang,Stefan Schnake,Steffen Schotthöfer*

Main category: cs.CV

TL;DR: 利用DLRT框架在迁移学习时压缩地理空间基础模型，减少参数并保持精度，适用于资源受限的边缘设备。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的边缘设备上部署地理空间基础模型需紧凑架构，但现有模型参数多且压缩易损失精度。

Method: 利用流形约束优化框架DLRT在迁移学习时压缩基于视觉变换器的地理空间基础模型，实施与下游目标一致的结构化低维参数化。

Result: 该方法优于现成的低秩方法（如LoRA），在多个地理空间基准测试中参数显著减少且精度损失极小。

Conclusion: 该方法可实现高性能的设备端地理空间模型。

Abstract: Deploying geospatial foundation models on resource-constrained edge devices demands compact architectures that maintain high downstream performance. However, their large parameter counts and the accuracy loss often induced by compression limit practical adoption. In this work, we leverage manifold-constrained optimization framework DLRT to compress large vision transformer-based geospatial foundation models during transfer learning. By enforcing structured low-dimensional parameterizations aligned with downstream objectives, this approach achieves strong compression while preserving task-specific accuracy. We show that the method outperforms of-the-shelf low-rank methods as LoRA. Experiments on diverse geospatial benchmarks confirm substantial parameter reduction with minimal accuracy loss, enabling high-performing, on-device geospatial models.

</details>


### [231] [Adaptive few-shot learning for robust part quality classification in two-photon lithography](https://arxiv.org/abs/2601.08885)
*Sixian Jia,Ruo-Syuan Mei,Chenhui Shao*

Main category: cs.CV

TL;DR: 本文提出适用于双光子光刻质量模型全生命周期维护的自适应计算机视觉框架，经评估能在演化生产场景有效部署和维护模型。


<details>
  <summary>Details</summary>
Motivation: 现有计算机视觉模型多为静态，在动态制造环境中难以检测新缺陷类、从稀缺数据更新或适应新零件几何形状。

Method: 构建基于同一、尺度稳健骨干模型的框架，集成基于LDA的统计假设检验框架、基于排练的两阶段少样本增量学习策略和少样本域对抗神经网络。

Result: 假设检验以99 - 100%准确率识别新类批次，增量学习用20样本将新类集成准确率达92%，域适应模型用5样本在目标域达96.19%准确率。

Conclusion: 该框架是在演化生产场景中部署和维护计算机视觉模型的强大且数据高效解决方案。

Abstract: Two-photon lithography (TPL) is an advanced additive manufacturing (AM) technique for fabricating high-precision micro-structures. While computer vision (CV) is proofed for automated quality control, existing models are often static, rendering them ineffective in dynamic manufacturing environments. These models typically cannot detect new, unseen defect classes, be efficiently updated from scarce data, or adapt to new part geometries. To address this gap, this paper presents an adaptive CV framework for the entire life-cycle of quality model maintenance. The proposed framework is built upon a same, scale-robust backbone model and integrates three key methodologies: (1) a statistical hypothesis testing framework based on Linear Discriminant Analysis (LDA) for novelty detection, (2) a two-stage, rehearsal-based strategy for few-shot incremental learning, and (3) a few-shot Domain-Adversarial Neural Network (DANN) for few-shot domain adaptation. The framework was evaluated on a TPL dataset featuring hemisphere as source domain and cube as target domain structures, with each domain categorized into good, minor damaged, and damaged quality classes. The hypothesis testing method successfully identified new class batches with 99-100% accuracy. The incremental learning method integrated a new class to 92% accuracy using only K=20 samples. The domain adaptation model bridged the severe domain gap, achieving 96.19% accuracy on the target domain using only K=5 shots. These results demonstrate a robust and data-efficient solution for deploying and maintaining CV models in evolving production scenarios.

</details>


### [232] [LP-LLM: End-to-End Real-World Degraded License Plate Text Recognition via Large Multimodal Models](https://arxiv.org/abs/2601.09116)
*Haoyan Gong,Hongbin Liu*

Main category: cs.CV

TL;DR: 针对真实场景车牌识别难题，提出基于Qwen3 - VL的端到端结构感知多模态推理框架，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有‘恢复 - 识别’两阶段范式目标不一致，Vision - Language Models缺乏车牌字符序列结构建模。

Method: 提出基于Qwen3 - VL的框架，核心是引入可学习字符槽查询的Character - Aware Multimodal Reasoning Module，结合LoRA策略进行微调。

Result: 在合成和真实严重退化数据集上实验，显著优于现有恢复 - 识别组合和通用VLMs。

Conclusion: 在大模型中引入结构化推理用于低质量文本识别任务具有优越性。

Abstract: Real-world License Plate Recognition (LPR) faces significant challenges from severe degradations such as motion blur, low resolution, and complex illumination. The prevailing "restoration-then-recognition" two-stage paradigm suffers from a fundamental flaw: the pixel-level optimization objectives of image restoration models are misaligned with the semantic goals of character recognition, leading to artifact interference and error accumulation. While Vision-Language Models (VLMs) have demonstrated powerful general capabilities, they lack explicit structural modeling for license plate character sequences (e.g., fixed length, specific order). To address this, we propose an end-to-end structure-aware multimodal reasoning framework based on Qwen3-VL. The core innovation lies in the Character-Aware Multimodal Reasoning Module (CMRM), which introduces a set of learnable Character Slot Queries. Through a cross-attention mechanism, these queries actively retrieve fine-grained evidence corresponding to character positions from visual features. Subsequently, we inject these character-aware representations back into the visual tokens via residual modulation, enabling the language model to perform autoregressive generation based on explicit structural priors. Furthermore, combined with the LoRA parameter-efficient fine-tuning strategy, the model achieves domain adaptation while retaining the generalization capabilities of the large model. Extensive experiments on both synthetic and real-world severely degraded datasets demonstrate that our method significantly outperforms existing restoration-recognition combinations and general VLMs, validating the superiority of incorporating structured reasoning into large models for low-quality text recognition tasks.

</details>


### [233] [N-EIoU-YOLOv9: A Signal-Aware Bounding Box Regression Loss for Lightweight Mobile Detection of Rice Leaf Diseases](https://arxiv.org/abs/2601.09170)
*Dung Ta Nguyen Duc,Thanh Bui Dang,Hoang Le Minh,Tung Nguyen Viet,Huong Nguyen Thanh,Dong Trinh Cong*

Main category: cs.CV

TL;DR: 提出基于N EIoU的轻量级检测框架N EIoU YOLOv9，应用于农业病害图像检测，在准确率、优化稳定性和计算效率上取得平衡。


<details>
  <summary>Details</summary>
Motivation: 解决农业病害图像中小目标和低对比度目标的检测问题，提升检测性能。

Method: 提出基于非单调梯度聚焦和几何解耦原理的N EIoU损失函数，将其集成到轻量级YOLOv9t架构中，并在自制数据集上评估。

Result: 相比标准CIoU损失有稳定性能提升，平均精度达到90.3%，提升4.3%；优化模型部署到安卓设备，每帧平均推理时间156毫秒并保持精度。

Conclusion: 提出的方法能有效平衡边缘农业监测系统的准确率、优化稳定性和计算效率。

Abstract: In this work, we propose N EIoU YOLOv9, a lightweight detection framework based on a signal aware bounding box regression loss derived from non monotonic gradient focusing and geometric decoupling principles, referred to as N EIoU (Non monotonic Efficient Intersection over Union). The proposed loss reshapes localization gradients by combining non monotonic focusing with decoupled width and height optimization, thereby enhancing weak regression signals for hard samples with low overlap while reducing gradient interference. This design is particularly effective for small and low contrast targets commonly observed in agricultural disease imagery. The proposed N EIoU loss is integrated into a lightweight YOLOv9t architecture and evaluated on a self collected field dataset comprising 5908 rice leaf images across four disease categories and healthy leaves. Experimental results demonstrate consistent performance gains over the standard CIoU loss, achieving a mean Average Precision of 90.3 percent, corresponding to a 4.3 percent improvement over the baseline, with improved localization accuracy under stricter evaluation criteria. For practical validation, the optimized model is deployed on an Android device using TensorFlow Lite with Float16 quantization, achieving an average inference time of 156 milliseconds per frame while maintaining accuracy. These results confirm that the proposed approach effectively balances accuracy, optimization stability, and computational efficiency for edge based agricultural monitoring systems.

</details>


### [234] [Annealed Relaxation of Speculative Decoding for Faster Autoregressive Image Generation](https://arxiv.org/abs/2601.09212)
*Xingyao Li,Fengzhuo Zhang,Cunxiao Du,Hui Ji*

Main category: cs.CV

TL;DR: 本文为松弛投机解码建立理论基础，提出COOL - SD方法，实验验证其在图像生成速度 - 质量权衡上优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 自回归图像生成推理慢，现有松弛投机解码方法缺乏理论基础。

Method: 分析目标模型与松弛投机解码的总变差距离得到最优重采样分布，用扰动分析揭示退火行为，在此基础上提出COOL - SD。

Result: COOL - SD能更快生成图像且质量相当，或在相近延迟下实现更好质量。

Conclusion: COOL - SD在速度 - 质量权衡上比先前方法有持续改进，验证了其有效性。

Abstract: Despite significant progress in autoregressive image generation, inference remains slow due to the sequential nature of AR models and the ambiguity of image tokens, even when using speculative decoding. Recent works attempt to address this with relaxed speculative decoding but lack theoretical grounding. In this paper, we establish the theoretical basis of relaxed SD and propose COOL-SD, an annealed relaxation of speculative decoding built on two key insights. The first analyzes the total variation (TV) distance between the target model and relaxed speculative decoding and yields an optimal resampling distribution that minimizes an upper bound of the distance. The second uses perturbation analysis to reveal an annealing behaviour in relaxed speculative decoding, motivating our annealed design. Together, these insights enable COOL-SD to generate images faster with comparable quality, or achieve better quality at similar latency. Experiments validate the effectiveness of COOL-SD, showing consistent improvements over prior methods in speed-quality trade-offs.

</details>


### [235] [SkinFlow: Efficient Information Transmission for Open Dermatological Diagnosis via Dynamic Visual Encoding and Staged RL](https://arxiv.org/abs/2601.09136)
*Lijun Liu,Linwei Chen,Zhishou Zhang,Meng Tian,Hengfu Cui,Ruiyang Li,Zhaocheng Liu,Qiang Ju,Qianxi Li,Hong-Yu Zhou*

Main category: cs.CV

TL;DR: 论文针对通用大视觉语言模型在皮肤病诊断的不足，提出SkinFlow框架，经实验验证其优于大规模通用模型。


<details>
  <summary>Details</summary>
Motivation: 通用大视觉语言模型因‘分散注意力’问题在皮肤病诊断中表现不佳，挑战参数扩展是实现医疗精度唯一途径的假设。

Method: 引入SkinFlow框架，使用虚拟宽度动态视觉编码器，结合两阶段强化学习策略，并提出临床评估协议。

Result: 7B模型在Fitzpatrick17k基准上达到新的最优，Top-1准确率提高12.06%，Top-6准确率提高28.57%。

Conclusion: 优化几何容量和信息流比单纯扩展参数能产生更好的诊断推理。

Abstract: General-purpose Large Vision-Language Models (LVLMs), despite their massive scale, often falter in dermatology due to "diffuse attention" - the inability to disentangle subtle pathological lesions from background noise. In this paper, we challenge the assumption that parameter scaling is the only path to medical precision. We introduce SkinFlow, a framework that treats diagnosis as an optimization of visual information transmission efficiency. Our approach utilizes a Virtual-Width Dynamic Vision Encoder (DVE) to "unfold" complex pathological manifolds without physical parameter expansion, coupled with a two-stage Reinforcement Learning strategy. This strategy sequentially aligns explicit medical descriptions (Stage I) and reconstructs implicit diagnostic textures (Stage II) within a constrained semantic space. Furthermore, we propose a clinically grounded evaluation protocol that prioritizes diagnostic safety and hierarchical relevance over rigid label matching. Empirical results are compelling: our 7B model establishes a new state-of-the-art on the Fitzpatrick17k benchmark, achieving a +12.06% gain in Top-1 accuracy and a +28.57% boost in Top-6 accuracy over the massive general-purpose models (e.g., Qwen3VL-235B and GPT-5.2). These findings demonstrate that optimizing geometric capacity and information flow yields superior diagnostic reasoning compared to raw parameter scaling.

</details>


### [236] [SSVP: Synergistic Semantic-Visual Prompting for Industrial Zero-Shot Anomaly Detection](https://arxiv.org/abs/2601.09147)
*Chenhao Fu,Han Fang,Xiuzheng Zheng,Wenbo Wei,Yonghua Li,Hao Sun,Xuelong Li*

Main category: cs.CV

TL;DR: 提出协同语义-视觉提示（SSVP）方法进行零样本异常检测，在多个工业基准上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有零样本异常检测范式受单视觉骨干网络限制，难以平衡全局语义泛化和细粒度结构判别能力。

Method: 提出SSVP方法，引入HSVS机制融合多尺度结构先验到CLIP语义空间，使用VCPG引导动态提示生成，用VTAM建立双门控校准范式。

Result: 在七个工业基准上评估了方法的鲁棒性，在MVTec - AD上达到93.0%图像AUROC和92.2%像素AUROC的先进性能。

Conclusion: SSVP方法能有效解决现有零样本异常检测范式的问题，显著优于现有零样本方法。

Abstract: Zero-Shot Anomaly Detection (ZSAD) leverages Vision-Language Models (VLMs) to enable supervision-free industrial inspection. However, existing ZSAD paradigms are constrained by single visual backbones, which struggle to balance global semantic generalization with fine-grained structural discriminability. To bridge this gap, we propose Synergistic Semantic-Visual Prompting (SSVP), that efficiently fuses diverse visual encodings to elevate model's fine-grained perception. Specifically, SSVP introduces the Hierarchical Semantic-Visual Synergy (HSVS) mechanism, which deeply integrates DINOv3's multi-scale structural priors into the CLIP semantic space. Subsequently, the Vision-Conditioned Prompt Generator (VCPG) employs cross-modal attention to guide dynamic prompt generation, enabling linguistic queries to precisely anchor to specific anomaly patterns. Furthermore, to address the discrepancy between global scoring and local evidence, the Visual-Text Anomaly Mapper (VTAM) establishes a dual-gated calibration paradigm. Extensive evaluations on seven industrial benchmarks validate the robustness of our method; SSVP achieves state-of-the-art performance with 93.0\% Image-AUROC and 92.2\% Pixel-AUROC on MVTec-AD, significantly outperforming existing zero-shot approaches.

</details>


### [237] [Magnifying change: Rapid burn scar mapping with multi-resolution, multi-source satellite imagery](https://arxiv.org/abs/2601.09262)
*Maria Sdraka,Dimitrios Michail,Ioannis Papoutsis*

Main category: cs.CV

TL;DR: 提出BAM - MRCD模型，用多分辨率、多源卫星图像及时生成高时空分辨率的野火燃烧区域图，检测精度高。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在野火发生后快速划定燃烧区域时，受卫星系统空间分辨率和时间重访频率权衡的限制。

Method: 提出BAM - MRCD模型，使用多分辨率、多源卫星图像（MODIS和Sentinel - 2）。

Result: 模型能高精度检测小范围野火，超越类似变化检测模型和基准模型。

Conclusion: BAM - MRCD模型可及时生成高时空分辨率的详细燃烧区域图，代码和数据开源。

Abstract: Delineating wildfire affected areas using satellite imagery remains challenging due to irregular and spatially heterogeneous spectral changes across the electromagnetic spectrum. While recent deep learning approaches achieve high accuracy when high-resolution multispectral data are available, their applicability in operational settings, where a quick delineation of the burn scar shortly after a wildfire incident is required, is limited by the trade-off between spatial resolution and temporal revisit frequency of current satellite systems. To address this limitation, we propose a novel deep learning model, namely BAM-MRCD, which employs multi-resolution, multi-source satellite imagery (MODIS and Sentinel-2) for the timely production of detailed burnt area maps with high spatial and temporal resolution. Our model manages to detect even small scale wildfires with high accuracy, surpassing similar change detection models as well as solid baselines. All data and code are available in the GitHub repository: https://github.com/Orion-AI-Lab/BAM-MRCD.

</details>


### [238] [Do Transformers Understand Ancient Roman Coin Motifs Better than CNNs?](https://arxiv.org/abs/2601.09433)
*David Reid,Ognjen Arandjelovic*

Main category: cs.CV

TL;DR: 本文首次将ViT架构应用于古币语义元素识别，对比ViT和CNN模型性能，发现ViT更准确。


<details>
  <summary>Details</summary>
Motivation: 自动分析古币可助研究者获取更多历史信息，帮助收藏家了解交易物品，此前该领域多用CNN，本文尝试用ViT。

Method: 应用ViT深度学习架构，从多模态数据（图像和非结构化文本）进行全自动学习，同时训练和实现ViT和CNN模型并评估性能。

Result: ViT模型在准确性上优于新训练的CNN模型。

Conclusion: ViT架构在古币语义元素识别任务中表现更优。

Abstract: Automated analysis of ancient coins has the potential to help researchers extract more historical insights from large collections of coins and to help collectors understand what they are buying or selling. Recent research in this area has shown promise in focusing on identification of semantic elements as they are commonly depicted on ancient coins, by using convolutional neural networks (CNNs). This paper is the first to apply the recently proposed Vision Transformer (ViT) deep learning architecture to the task of identification of semantic elements on coins, using fully automatic learning from multi-modal data (images and unstructured text). This article summarises previous research in the area, discusses the training and implementation of ViT and CNN models for ancient coins analysis and provides an evaluation of their performance. The ViT models were found to outperform the newly trained CNN models in accuracy.

</details>


### [239] [SpikeVAEDiff: Neural Spike-based Natural Visual Scene Reconstruction via VD-VAE and Versatile Diffusion](https://arxiv.org/abs/2601.09213)
*Jialu Li,Taiyan Zhou*

Main category: cs.CV

TL;DR: 提出SpikeVAEDiff框架从神经尖峰数据生成高分辨率图像，在数据集上评估，显示VISI区重要，尖峰数据有优势，验证模型有效性。


<details>
  <summary>Details</summary>
Motivation: 解决从神经活动重建自然视觉场景这一神经科学和计算机视觉的关键挑战。

Method: 提出两阶段框架SpikeVAEDiff，第一阶段用VDVAE生成低分辨率初步重建，第二阶段用回归模型映射特征，Versatile Diffusion细化图像。

Result: VISI区激活最显著且对重建质量关键，尖峰数据比fMRI方法有更好时空分辨率，验证VDVAE模型有效性，特定脑区数据可提升重建性能。

Conclusion: SpikeVAEDiff框架能从神经尖峰数据生成高分辨率、语义有意义的图像，特定脑区数据对重建有重要作用。

Abstract: Reconstructing natural visual scenes from neural activity is a key challenge in neuroscience and computer vision. We present SpikeVAEDiff, a novel two-stage framework that combines a Very Deep Variational Autoencoder (VDVAE) and the Versatile Diffusion model to generate high-resolution and semantically meaningful image reconstructions from neural spike data. In the first stage, VDVAE produces low-resolution preliminary reconstructions by mapping neural spike signals to latent representations. In the second stage, regression models map neural spike signals to CLIP-Vision and CLIP-Text features, enabling Versatile Diffusion to refine the images via image-to-image generation.
  We evaluate our approach on the Allen Visual Coding-Neuropixels dataset and analyze different brain regions. Our results show that the VISI region exhibits the most prominent activation and plays a key role in reconstruction quality. We present both successful and unsuccessful reconstruction examples, reflecting the challenges of decoding neural activity. Compared with fMRI-based approaches, spike data provides superior temporal and spatial resolution. We further validate the effectiveness of the VDVAE model and conduct ablation studies demonstrating that data from specific brain regions significantly enhances reconstruction performance.

</details>


### [240] [Towards Robust Cross-Dataset Object Detection Generalization under Domain Specificity](https://arxiv.org/abs/2601.09497)
*Ritabrata Chakraborty,Hrishit Mitra,Shivakumara Palaiahnakote,Umapada Pal*

Main category: cs.CV

TL;DR: 研究跨数据集目标检测（CD - OD），按设置特异性分组数据集评估，揭示CD - OD结构，比较不同标签协议，提供评估指导。


<details>
  <summary>Details</summary>
Motivation: 解决目标检测器在不同基准上性能大幅下降的问题，研究CD - OD。

Method: 将基准数据集分为设置无关和设置特定两类，评估标准检测器家族在所有训练 - 测试对中的表现，比较封闭标签和开放标签传输协议。

Result: 同一设置类型内转移较稳定，跨设置类型转移性能大幅下降且常不对称；开放标签评估有一致但有限的增益。

Conclusion: 对设置特异性下的CD - OD进行了原则性表征，并为分布偏移下的检测器评估提供实用指导。

Abstract: Object detectors often perform well in-distribution, yet degrade sharply on a different benchmark. We study cross-dataset object detection (CD-OD) through a lens of setting specificity. We group benchmarks into setting-agnostic datasets with diverse everyday scenes and setting-specific datasets tied to a narrow environment, and evaluate a standard detector family across all train--test pairs. This reveals a clear structure in CD-OD: transfer within the same setting type is relatively stable, while transfer across setting types drops substantially and is often asymmetric. The most severe breakdowns occur when transferring from specific sources to agnostic targets, and persist after open-label alignment, indicating that domain shift dominates in the hardest regimes. To disentangle domain shift from label mismatch, we compare closed-label transfer with an open-label protocol that maps predicted classes to the nearest target label using CLIP similarity. Open-label evaluation yields consistent but bounded gains, and many corrected cases correspond to semantic near-misses supported by the image evidence. Overall, we provide a principled characterization of CD-OD under setting specificity and practical guidance for evaluating detectors under distribution shift. Code will be released at \href{[https://github.com/Ritabrata04/cdod-icpr.git}{https://github.com/Ritabrata04/cdod-icpr}.

</details>


### [241] [Hybrid guided variational autoencoder for visual place recognition](https://arxiv.org/abs/2601.09248)
*Ni Wang,Zihan You,Emre Neftci,Thorben Schoepe*

Main category: cs.CV

TL;DR: 本文结合基于事件的视觉传感器和新型引导变分自编码器（VAE）解决现有视觉定位模型问题，模型紧凑、鲁棒且泛化能力强，可提升室内机器人导航能力。


<details>
  <summary>Details</summary>
Motivation: 现有先进视觉定位（VPR）模型内存需求大，紧凑模型缺乏鲁棒性和泛化能力，难以用于移动部署。

Method: 结合基于事件的视觉传感器和新型引导变分自编码器（VAE），编码器基于脉冲神经网络模型，与低功耗低延迟神经形态硬件兼容。

Result: VAE在新室内VPR数据集中成功分离16个不同地点的视觉特征，分类性能与其他先进方法相当，在不同光照条件下表现稳健，对未知场景新视觉输入有高泛化能力。

Conclusion: 紧凑、鲁棒且有泛化能力的引导VAE是很有前景的视觉定位模型，可显著增强室内机器人在已知和未知环境中的导航能力。

Abstract: Autonomous agents such as cars, robots and drones need to precisely localize themselves in diverse environments, including in GPS-denied indoor environments. One approach for precise localization is visual place recognition (VPR), which estimates the place of an image based on previously seen places. State-of-the-art VPR models require high amounts of memory, making them unwieldy for mobile deployment, while more compact models lack robustness and generalization capabilities. This work overcomes these limitations for robotics using a combination of event-based vision sensors and an event-based novel guided variational autoencoder (VAE). The encoder part of our model is based on a spiking neural network model which is compatible with power-efficient low latency neuromorphic hardware. The VAE successfully disentangles the visual features of 16 distinct places in our new indoor VPR dataset with a classification performance comparable to other state-of-the-art approaches while, showing robust performance also under various illumination conditions. When tested with novel visual inputs from unknown scenes, our model can distinguish between these places, which demonstrates a high generalization capability by learning the essential features of location. Our compact and robust guided VAE with generalization capabilities poses a promising model for visual place recognition that can significantly enhance mobile robot navigation in known and unknown indoor environments.

</details>


### [242] [Identifying Models Behind Text-to-Image Leaderboards](https://arxiv.org/abs/2601.09647)
*Ali Naseh,Yuefeng Peng,Anshuman Suri,Harsh Chaudhari,Alina Oprea,Amir Houmansadr*

Main category: cs.CV

TL;DR: 研究表明文本到图像（T2I）模型排行榜的匿名性易被打破，提出方法实现去匿名化并发现模型特定特征，揭示排行榜安全缺陷。


<details>
  <summary>Details</summary>
Motivation: 指出当前基于投票的T2I模型排行榜依赖匿名输出来保证公平性，但这种匿名性可能存在问题，需进行验证。

Method: 利用T2I模型生成的图像在嵌入空间形成独特簇的特点，采用基于质心的方法，使用22个模型和280个提示（15万张图像）进行去匿名化。

Result: 基于质心的方法实现了高精度去匿名化，揭示了系统的模型特定特征，引入提示级可区分性指标并分析出某些提示可实现近乎完美的区分。

Conclusion: T2I排行榜存在基本的安全缺陷，需要更强的匿名化防御措施。

Abstract: Text-to-image (T2I) models are increasingly popular, producing a large share of AI-generated images online. To compare model quality, voting-based leaderboards have become the standard, relying on anonymized model outputs for fairness. In this work, we show that such anonymity can be easily broken. We find that generations from each T2I model form distinctive clusters in the image embedding space, enabling accurate deanonymization without prompt control or training data. Using 22 models and 280 prompts (150K images), our centroid-based method achieves high accuracy and reveals systematic model-specific signatures. We further introduce a prompt-level distinguishability metric and conduct large-scale analyses showing how certain prompts can lead to near-perfect distinguishability. Our findings expose fundamental security flaws in T2I leaderboards and motivate stronger anonymization defenses.

</details>


### [243] [Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning](https://arxiv.org/abs/2601.09708)
*Chi-Pin Huang,Yunze Man,Zhiding Yu,Min-Hung Chen,Jan Kautz,Yu-Chiang Frank Wang,Fu-En Yang*

Main category: cs.CV

TL;DR: 提出Fast - ThinkAct高效推理框架，减少推理VLA任务推理延迟，实验显示有良好性能。


<details>
  <summary>Details</summary>
Motivation: 现有推理VLA研究虽能用显式思维链提升泛化性，但推理轨迹长导致推理延迟高。

Method: 提出Fast - ThinkAct框架，通过可语言化的潜在推理实现紧凑且高效规划，从教师模型蒸馏学习潜在思维链，由偏好引导目标驱动。

Result: 在多样具身操作和推理基准测试中，相比现有推理VLA，推理延迟最多降低89.3%，并保持有效长程规划、少样本适应和故障恢复能力。

Conclusion: Fast - ThinkAct框架有效减少推理延迟，同时具备良好性能。

Abstract: Vision-Language-Action (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit chain-of-thought (CoT) can improve generalization, they suffer from high inference latency due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning. Fast-ThinkAct learns to reason efficiently with latent CoTs by distilling from a teacher, driven by a preference-guided objective to align manipulation trajectories that transfers both linguistic and visual planning capabilities for embodied control. This enables reasoning-enhanced policy learning that effectively connects compact reasoning to action execution. Extensive experiments across diverse embodied manipulation and reasoning benchmarks demonstrate that Fast-ThinkAct achieves strong performance with up to 89.3\% reduced inference latency over state-of-the-art reasoning VLAs, while maintaining effective long-horizon planning, few-shot adaptation, and failure recovery.

</details>


### [244] [Radiomics-Integrated Deep Learning with Hierarchical Loss for Osteosarcoma Histology Classification](https://arxiv.org/abs/2601.09416)
*Yaxi Chen,Zi Ye,Shaheer U. Saeed,Oliver Yu,Simin Ni,Jie Huang,Yipeng Hu*

Main category: cs.CV

TL;DR: 文章针对骨肉瘤新辅助化疗后肿瘤区域评估问题，提出使用放射组学特征和分层损失优化分类任务，在数据集上取得新的最优性能。


<details>
  <summary>Details</summary>
Motivation: 骨肉瘤手动病理评估劳动强度大、主观且存在观察者间差异，现有深度学习模型在测试数据上性能下降，需提升评估性能。

Method: 1. 将放射组学特征作为额外输入用于模型训练；2. 优化两个具有分层类别的二分类任务以实现分层损失。

Result: 实验证明两种新方法及其组合有效，在公开数据集上达到新的最优性能。

Conclusion: 所提方法能显著提升骨肉瘤肿瘤区域分类性能，代码和训练模型已公开。

Abstract: Osteosarcoma (OS) is an aggressive primary bone malignancy. Accurate histopathological assessment of viable versus non-viable tumor regions after neoadjuvant chemotherapy is critical for prognosis and treatment planning, yet manual evaluation remains labor-intensive, subjective, and prone to inter-observer variability. Recent advances in digital pathology have enabled automated necrosis quantification. Evaluating on test data, independently sampled on patient-level, revealed that the deep learning model performance dropped significantly from the tile-level generalization ability reported in previous studies. First, this work proposes the use of radiomic features as additional input in model training. We show that, despite that they are derived from the images, such a multimodal input effectively improved the classification performance, in addition to its added benefits in interpretability. Second, this work proposes to optimize two binary classification tasks with hierarchical classes (i.e. tumor-vs-non-tumor and viable-vs-non-viable), as opposed to the alternative ``flat'' three-class classification task (i.e. non-tumor, non-viable tumor, viable tumor), thereby enabling a hierarchical loss. We show that such a hierarchical loss, with trainable weightings between the two tasks, the per-class performance can be improved significantly. Using the TCIA OS Tumor Assessment dataset, we experimentally demonstrate the benefits from each of the proposed new approaches and their combination, setting a what we consider new state-of-the-art performance on this open dataset for this application. Code and trained models: https://github.com/YaxiiC/RadiomicsOS.git.

</details>


### [245] [Hot-Start from Pixels: Low-Resolution Visual Tokens for Chinese Language Modeling](https://arxiv.org/abs/2601.09566)
*Shuyang Xiang,Hao Guan*

Main category: cs.CV

TL;DR: 研究低分辨率视觉输入用于中文建模，结果显示其效果与基于索引方法相当且有热启动效应。


<details>
  <summary>Details</summary>
Motivation: 大语言模型处理中文时忽略汉字视觉形式，而汉字视觉结构有语义和语音信息，可辅助预测，因此研究低分辨率视觉输入能否用于中文建模。

Method: 解码器接收低至8×8像素的单个汉字灰度图像进行建模。

Result: 低分辨率视觉输入准确率达39.2%，和基于索引的基线39.1%相当；低资源设置有明显热启动效应，前期准确率高于基于索引模型。

Conclusion: 最小视觉结构能为中文语言建模提供强大高效信号，可作为传统基于索引方法的补充。

Abstract: Large language models typically represent Chinese characters as discrete index-based tokens, largely ignoring their visual form. For logographic scripts, visual structure carries semantic and phonetic information, which may aid prediction. We investigate whether low-resolution visual inputs can serve as an alternative for character-level modeling. Instead of token IDs, our decoder receives grayscale images of individual characters, with resolutions as low as $8 \times 8$ pixels. Remarkably, these inputs achieve 39.2\% accuracy, comparable to the index-based baseline of 39.1\%. Such low-resource settings also exhibit a pronounced \emph{hot-start} effect: by 0.4\% of total training, accuracy reaches above 12\%, while index-based models lag at below 6\%. Overall, our results demonstrate that minimal visual structure can provide a robust and efficient signal for Chinese language modeling, offering an alternative perspective on character representation that complements traditional index-based approaches.

</details>


### [246] [Sim2real Image Translation Enables Viewpoint-Robust Policies from Fixed-Camera Datasets](https://arxiv.org/abs/2601.09605)
*Jeremiah Coholich,Justin Wit,Robert Azarcon,Zsolt Kira*

Main category: cs.CV

TL;DR: 提出MANGO方法解决视觉机器人操作策略中sim2real的视角转换问题，表现优于其他方法，增强数据训练的策略成功率高。


<details>
  <summary>Details</summary>
Motivation: 基于视觉的机器人操作策略易受相机视角变化影响，真实数据稀缺且视角变化不足，模拟数据存在sim2real挑战。

Method: 提出MANGO，包含分割条件InfoNCE损失、高度正则化判别器设计和修改的PatchNCE损失。

Result: MANGO能在sim2real转换中保持视角一致性，生成多样未见视角，优于其他图像翻译方法，增强数据训练的模仿学习策略成功率高。

Conclusion: MANGO有效解决了视觉机器人操作策略中sim2real的视角转换问题。

Abstract: Vision-based policies for robot manipulation have achieved significant recent success, but are still brittle to distribution shifts such as camera viewpoint variations. Robot demonstration data is scarce and often lacks appropriate variation in camera viewpoints. Simulation offers a way to collect robot demonstrations at scale with comprehensive coverage of different viewpoints, but presents a visual sim2real challenge. To bridge this gap, we propose MANGO -- an unpaired image translation method with a novel segmentation-conditioned InfoNCE loss, a highly-regularized discriminator design, and a modified PatchNCE loss. We find that these elements are crucial for maintaining viewpoint consistency during sim2real translation. When training MANGO, we only require a small amount of fixed-camera data from the real world, but show that our method can generate diverse unseen viewpoints by translating simulated observations. In this domain, MANGO outperforms all other image translation methods we tested. Imitation-learning policies trained on data augmented by MANGO are able to achieve success rates as high as 60\% on views that the non-augmented policy fails completely on.

</details>


### [247] [CogRail: Benchmarking VLMs in Cognitive Intrusion Perception for Intelligent Railway Transportation Systems](https://arxiv.org/abs/2601.09613)
*Yonglin Tian,Qiyao Zhang,Wei Xu,Yutong Wang,Yihao Wu,Xinyi Li,Xingyuan Dai,Hui Zhang,Zhiyong Cui,Baoqing Guo,Zujun Yu,Yisheng Lv*

Main category: cs.CV

TL;DR: 提出CogRail基准评估VLM用于铁路入侵感知，指出当前模型不足并提出联合微调框架提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有铁路运输系统在潜在入侵目标感知方面存不足，传统视觉模型难以适应时空推理。

Method: 引入CogRail基准，评估VLM，对其微调，提出联合微调框架整合三个核心任务。

Result: 当前大型多模态模型在认知入侵感知任务的时空推理上有困难，提出的联合微调框架提升了模型性能。

Conclusion: 结构化多任务学习能有效提高模型在认知入侵感知中的准确性和可解释性。

Abstract: Accurate and early perception of potential intrusion targets is essential for ensuring the safety of railway transportation systems. However, most existing systems focus narrowly on object classification within fixed visual scopes and apply rule-based heuristics to determine intrusion status, often overlooking targets that pose latent intrusion risks. Anticipating such risks requires the cognition of spatial context and temporal dynamics for the object of interest (OOI), which presents challenges for conventional visual models. To facilitate deep intrusion perception, we introduce a novel benchmark, CogRail, which integrates curated open-source datasets with cognitively driven question-answer annotations to support spatio-temporal reasoning and prediction. Building upon this benchmark, we conduct a systematic evaluation of state-of-the-art visual-language models (VLMs) using multimodal prompts to identify their strengths and limitations in this domain. Furthermore, we fine-tune VLMs for better performance and propose a joint fine-tuning framework that integrates three core tasks, position perception, movement prediction, and threat analysis, facilitating effective adaptation of general-purpose foundation models into specialized models tailored for cognitive intrusion perception. Extensive experiments reveal that current large-scale multimodal models struggle with the complex spatial-temporal reasoning required by the cognitive intrusion perception task, underscoring the limitations of existing foundation models in this safety-critical domain. In contrast, our proposed joint fine-tuning framework significantly enhances model performance by enabling targeted adaptation to domain-specific reasoning demands, highlighting the advantages of structured multi-task learning in improving both accuracy and interpretability. Code will be available at https://github.com/Hub-Tian/CogRail.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [248] [Global polynomial-time estimation in statistical nonlinear inverse problems via generalized stability](https://arxiv.org/abs/2601.09007)
*Sven Wang*

Main category: math.ST

TL;DR: 提出插件和PDE惩罚的M估计器用于非线性统计逆问题，证明其能达最优统计收敛率且可全局多项式时间计算，还推导了自适应率并提供贝叶斯计算的热启动初始化。


<details>
  <summary>Details</summary>
Motivation: 非线性统计逆问题在统计分析和计算上有挑战，似然估计器导致非凸优化，MCMC方法混合慢。

Method: 用弱松弛代替精确PDE约束，得到条件凸和嵌套二次优化问题，避免前向映射评估和PDE求解器。

Result: 对于椭圆PDE产生的非线性逆问题，估计器达到最佳统计收敛率，在Darcy模型中有显式次二次算术运行时界。

Conclusion: 提出的估计器可用于设计多项式时间统计算法，还能为贝叶斯计算提供热启动初始化。

Abstract: Non-linear statistical inverse problems pose major challenges both for statistical analysis and computation. Likelihood-based estimators typically lead to non-convex and possibly multimodal optimization landscapes, and Markov chain Monte Carlo (MCMC) methods may mix exponentially slowly. We propose a class of computationally tractable estimators--plug-in and PDE-penalized M-estimators--for inverse problems defined through operator equations of the form $L_f u = g$, where $f$ is the unknown parameter and $u$ is the observed solution. The key idea is to replace the exact PDE constraint by a weakly enforced relaxation, yielding conditionally convex and, in many PDE examples, nested quadratic optimization problems that avoid evaluating the forward map $G(f)$ and do not require PDE solvers. For prototypical non-linear inverse problems arising from elliptic PDEs, including the Darcy flow model $L_f u = \nabla\!\cdot(f\nabla u)$ and a steady-state Schrödinger model, we prove that these estimators attain the best currently known statistical convergence rates while being globally computable in polynomial time. In the Darcy model, we obtain an explicit sub-quadratic $o(N^2)$ arithmetic runtime bound for estimating $f$ from $N$ noisy samples. Our analysis is based on new generalized stability estimates, extending classical stability beyond the range of the forward operator, combined with tools from nonparametric M-estimation. We also derive adaptive rates for the Darcy problem, providing a blueprint for designing provably polynomial-time statistical algorithms for a broad class of non-linear inverse problems. Our estimators also provide principled warm-start initializations for polynomial-time Bayesian computation.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [249] [Query Languages for Machine-Learning Models](https://arxiv.org/abs/2601.09381)
*Martin Grohe*

Main category: cs.LO

TL;DR: 本文讨论用于加权有限结构的两种逻辑，展示对神经网络查询示例并讨论其表达能力和计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 将两种逻辑作为机器学习模型（特别是神经网络）的查询语言进行研究。

Method: 以Grädel等人的基础工作为起源，结合与Standke等人的近期合作研究。

Result: 给出能用这两种逻辑表达的对神经网络的查询示例，得到关于表达能力和计算复杂度的基本结果。

Conclusion: 未明确提及，但暗示这两种逻辑在神经网络查询方面有一定作用。

Abstract: In this paper, I discuss two logics for weighted finite structures: first-order logic with summation (FO(SUM)) and its recursive extension IFP(SUM). These logics originate from foundational work by Grädel, Gurevich, and Meer in the 1990s. In recent joint work with Standke, Steegmans, and Van den Bussche, we have investigated these logics as query languages for machine learning models, specifically neural networks, which are naturally represented as weighted graphs. I present illustrative examples of queries to neural networks that can be expressed in these logics and discuss fundamental results on their expressiveness and computational complexity.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [250] [Block Decomposable Methods for Large-Scale Optimization Problems](https://arxiv.org/abs/2601.09010)
*Leandro Farias Maia*

Main category: math.OC

TL;DR: 本文探索大规模优化问题的块分解方法，包括新的近端ADMM算法和两种BCD方法，展示了算法的收敛性和性能优势。


<details>
  <summary>Details</summary>
Motivation: 解决大规模优化问题，提升算法在实际应用中的效率和适应性。

Method: 引入新的近端ADMM算法，提出两种BCD方法，分析算子性质，设定误差下降条件。

Result: 新的近端ADMM算法达到近似最优复杂度，BCD算法在动态误差体制下性能优越，得出随机BCD方法的收敛率。

Conclusion: 为随机BCD方法应用于Hölder平滑函数提供收敛保证，所得收敛率与Lipschtiz平滑情况现有文献一致。

Abstract: This dissertation explores block decomposable methods for large-scale optimization problems. It focuses on alternating direction method of multipliers (ADMM) schemes and block coordinate descent (BCD) methods. Specifically, it introduces a new proximal ADMM algorithm and proposes two BCD methods. The first part of the research presents a new proximal ADMM algorithm. This method is adaptive to all problem parameters and solves the proximal augmented Lagrangian (AL) subproblem inexactly. This adaptiveness facilitates the highly efficient application of the algorithm to a broad swath of practical problems. The inexact solution of the proximal AL subproblem overcomes many key challenges in the practical applications of ADMM. The resultant algorithm obtains an approximate solution of an optimization problem in a number of iterations that matches the state-of-the-art complexity for the class of proximal ADMM schemes. The second part of the research focuses on an inexact proximal mapping for the class of block proximal gradient methods. Key properties of this operator is established, facilitating the derivation of convergence rates for the proposed algorithm. Under two error decreases conditions, the algorithm matches the convergence rate of its exactly computed counterpart. Numerical results demonstrate the superior performance of the algorithm under a dynamic error regime over a fixed one. The dissertation concludes by providing convergence guarantees for the randomized BCD method applied to a broad class of functions, known as Hölder smooth functions. Convergence rates are derived for non-convex, convex, and strongly convex functions. These convergence rates match those furnished in the existing literature for the Lipschtiz smooth setting.

</details>


### [251] [An Inexact Weighted Proximal Trust-Region Method](https://arxiv.org/abs/2601.09024)
*Leandro Farias Maia,Robert Baraldi,Drew P. Kouri*

Main category: math.OC

TL;DR: 本文扩展了不精确邻近算子的定义，将其用于信赖域算法，还增强标准信赖域收敛理论分析，最后应用算法求解受Burgers方程约束的最优控制问题。


<details>
  <summary>Details</summary>
Motivation: 原信赖域方法适用函数有限，很多函数因非光滑项的拓扑或性质被排除，需扩展不精确邻近算子定义以扩大适用范围。

Method: 利用δ - Fréchet次微分扩展不精确邻近算子定义，增强标准信赖域收敛理论分析，先引入算法生成不精确邻近算子中的点，再将算法应用于信赖域方法。

Result: 能够将扩展后的不精确邻近算子用于信赖域算法，可处理加权内积下的邻近算子不精确性。

Conclusion: 扩展的不精确邻近算子定义及增强的收敛理论分析可用于求解受Burgers方程约束的最优控制问题。

Abstract: In [R. J. Baraldi and D. P. Kouri, Math. Program., 201:1 (2023), pp. 559-598], the authors introduced a trust-region method for minimizing the sum of a smooth nonconvex and a nonsmooth convex function, the latter of which has an analytical proximity operator. While many functions satisfy this criterion, e.g., the $\ell_1$-norm defined on $\ell_2$, many others are precluded by either the topology or the nature of the nonsmooth term. Using the $δ$-Fréchet subdifferential, we extend the definition of the inexact proximity operator and enable its use within the aforementioned trust-region algorithm. Moreover, we augment the analysis for the standard trust-region convergence theory to handle proximity operator inexactness with weighted inner products. We first introduce an algorithm to generate a point in the inexact proximity operator and then apply the algorithm within the trust-region method to solve an optimal control problem constrained by Burgers' equation.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [252] [StegoStylo: Squelching Stylometric Scrutiny through Steganographic Stitching](https://arxiv.org/abs/2601.09056)
*Robert Dilworth*

Main category: cs.CR

TL;DR: 本文探讨对抗性文体学与隐写术如何对抗文体分析，增强TraceTarnish攻击效果，研究隐写嵌入掩盖作者风格指纹，指出33%以上隐写覆盖率可确保作者身份混淆，强调防御工具必要。


<details>
  <summary>Details</summary>
Motivation: 文体学用于作者身份验证时会带来隐私威胁，因此研究对抗性文体学与隐写术来应对文体分析。

Method: 增强TraceTarnish攻击，研究隐写嵌入，量化零宽Unicode字符改变单词比例与作者身份混淆程度关系。

Result: TraceTarnish能混淆文体系统，降低归因和验证准确性，33%以上隐写覆盖率可确保作者身份混淆。

Conclusion: 强调文体学可能损害隐私，需要TraceTarnish等防御工具。

Abstract: Stylometry--the identification of an author through analysis of a text's style (i.e., authorship attribution)--serves many constructive purposes: it supports copyright and plagiarism investigations, aids detection of harmful content, offers exploratory cues for certain medical conditions (e.g., early signs of dementia or depression), provides historical context for literary works, and helps uncover misinformation and disinformation. In contrast, when stylometry is employed as a tool for authorship verification--confirming whether a text truly originates from a claimed author--it can also be weaponized for malicious purposes. Techniques such as de-anonymization, re-identification, tracking, profiling, and downstream effects like censorship illustrate the privacy threats that stylometric analysis can enable. Building on these concerns, this paper further explores how adversarial stylometry combined with steganography can counteract stylometric analysis. We first present enhancements to our adversarial attack, $\textit{TraceTarnish}$, providing stronger evidence of its capacity to confound stylometric systems and reduce their attribution and verification accuracy. Next, we examine how steganographic embedding can be fine-tuned to mask an author's stylistic fingerprint, quantifying the level of authorship obfuscation achievable as a function of the proportion of words altered with zero-width Unicode characters. Based on our findings, steganographic coverage of 33% or higher seemingly ensures authorship obfuscation. Finally, we reflect on the ways stylometry can be used to undermine privacy and argue for the necessity of defensive tools like $\textit{TraceTarnish}$.

</details>


### [253] [Integrating APK Image and Text Data for Enhanced Threat Detection: A Multimodal Deep Learning Approach to Android Malware](https://arxiv.org/abs/2601.08959)
*Md Mashrur Arifin,Maqsudur Rahman,Nasir U. Eisty*

Main category: cs.CR

TL;DR: 本文提出集成APK图像与文本特征的多模态深度学习框架用于安卓恶意软件检测，评估不同图像类型、分辨率及CNN架构，发现高分辨率RGB图像分类性能优，图像与文本多模态集成潜力有限。


<details>
  <summary>Details</summary>
Motivation: 现有基于图像的安卓恶意软件检测研究忽视图像类型、分辨率影响及APK文本数据，限制检测能力，因此提出多模态方法改进检测。

Method: 系统评估不同CNN架构下的各种图像类型和分辨率，用LLaMA - 2提取和标注文本特征，用CLIP模型进行图像与文本多模态集成。

Result: 高分辨率（如256x256、512x512）RGB图像分类性能优越，图像与文本多模态集成潜力有限。

Conclusion: 系统评估图像属性和集成多模态数据对开发有效的安卓系统恶意软件检测至关重要。

Abstract: As zero-day Android malware attacks grow more sophisticated, recent research highlights the effectiveness of using image-based representations of malware bytecode to detect previously unseen threats. However, existing studies often overlook how image type and resolution affect detection and ignore valuable textual data in Android Application Packages (APKs), such as permissions and metadata, limiting their ability to fully capture malicious behavior. The integration of multimodality, which combines image and text data, has gained momentum as a promising approach to address these limitations. This paper proposes a multimodal deep learning framework integrating APK images and textual features to enhance Android malware detection. We systematically evaluate various image types and resolutions across different Convolutional Neural Networks (CNN) architectures, including VGG, ResNet-152, MobileNet, DenseNet, EfficientNet-B4, and use LLaMA-2, a large language model, to extract and annotate textual features for improved analysis. The findings demonstrate that RGB images at higher resolutions (e.g., 256x256, 512x512) achieve superior classification performance, while the multimodal integration of image and text using the CLIP model reveals limited potential. Overall, this research highlights the importance of systematically evaluating image attributes and integrating multimodal data to develop effective malware detection for Android systems.

</details>


### [254] [Formally Verifying Noir Zero Knowledge Programs with NAVe](https://arxiv.org/abs/2601.09372)
*Pedro Antonino,Namrata Jain*

Main category: cs.CR

TL;DR: 本文用SMT - LIB形式化ACIR语言，用SMT求解器cvc5为Noir语言创建开源形式验证器并进行评估。


<details>
  <summary>Details</summary>
Motivation: 简化零知识证明中复杂算术电路的描述，且需确保Noir程序行为正确。

Method: 使用SMT - LIB形式化ACIR语言，用SMT求解器cvc5创建形式验证器。

Result: 对4组不同Noir程序评估验证器，证明其实际适用性并找出难检查的约束类型。

Conclusion: 验证器有实际应用价值，发现的难检查约束类型为验证框架改进指明方向。

Abstract: Zero-Knowledge (ZK) proof systems are cryptographic protocols that can (with overwhelming probability) demonstrate that the pair $(X, W)$ is in a relation $R$ without revealing information about the private input $W$. This membership checking is captured by a complex arithmetic circuit: a set of polynomial equations over a finite field. ZK programming languages, like Noir, have been proposed to simplify the description of these circuits. A developer can write a Noir program using traditional high-level constructs that can be compiled into a lower-level ACIR (Abstract Circuit Intermediate Representation), which is essentially a high-level description of an arithmetic circuit. In this paper, we formalise some of the ACIR language using SMT-LIB and its extended theory of finite fields. We use this formalisation to create an open-source formal verifier for the Noir language using the SMT solver cvc5. Our verifier can be used to check whether Noir programs behave appropriately. For instance, it can be used to check whether a Noir program has been properly constrained, that is, the finite-field polynomial equations generated truly capture the intended relation. We evaluate our verifier over 4 distinct sets of Noir programs, demonstrating its practical applicability and identifying a hard-to-check constraint type that charts an improvement path for our verification framework.

</details>


### [255] [Proactively Detecting Threats: A Novel Approach Using LLMs](https://arxiv.org/abs/2601.09029)
*Aniesh Chawla,Udbhav Prasad*

Main category: cs.CR

TL;DR: 本文首次系统评估大语言模型从非结构化网络威胁情报源主动识别入侵指标（IOCs），开发自动化系统评估六种模型，不同模型性能差异大，Gemini 1.5 Pro 表现佳。


<details>
  <summary>Details</summary>
Motivation: 企业安全面临复杂恶意软件威胁，传统是被动检测，本文旨在利用大语言模型从非结构化网络威胁情报源主动识别 IOCs。

Method: 开发自动化系统，从 15 个网络威胁报告源提取 IOCs，评估六种大语言模型（Gemini、Qwen 和 Llama 变体）。

Result: 评估 479 个包含 2658 个 IOCs 的网页，不同模型性能差异显著，Gemini 1.5 Pro 恶意 IOC 识别精度 0.958、特异性 0.788，实际威胁召回率 1.0。

Conclusion: 大语言模型可用于从非结构化网络威胁情报源主动识别 IOCs，不同模型性能有明显差异，Gemini 1.5 Pro 表现较好。

Abstract: Enterprise security faces escalating threats from sophisticated malware, compounded by expanding digital operations. This paper presents the first systematic evaluation of large language models (LLMs) to proactively identify indicators of compromise (IOCs) from unstructured web-based threat intelligence sources, distinguishing it from reactive malware detection approaches. We developed an automated system that pulls IOCs from 15 web-based threat report sources to evaluate six LLM models (Gemini, Qwen, and Llama variants). Our evaluation of 479 webpages containing 2,658 IOCs (711 IPv4 addresses, 502 IPv6 addresses, 1,445 domains) reveals significant performance variations. Gemini 1.5 Pro achieved 0.958 precision and 0.788 specificity for malicious IOC identification, while demonstrating perfect recall (1.0) for actual threats.

</details>


### [256] [A Decompilation-Driven Framework for Malware Detection with Large Language Models](https://arxiv.org/abs/2601.09035)
*Aniesh Chawla,Udbhav Prasad*

Main category: cs.CR

TL;DR: 评估大语言模型在恶意代码分类中的有效性，发现通用模型尚不足，微调模型有优势但面对新恶意软件性能下降，需持续微调。


<details>
  <summary>Details</summary>
Motivation: 大语言模型理解代码能力和恶意软件复杂度提升，有必要评估其在恶意代码分类的有效性。

Method: 使用Ghidra反汇编器将Windows可执行文件反编译为C代码，利用大语言模型进行分类，并使用微调模型。

Result: 通用大语言模型有潜力但不足以替代传统杀毒软件；微调模型优于通用模型；微调模型面对新恶意软件性能下降。

Conclusion: 为保持模型有效性，需针对新出现的威胁持续微调模型。

Abstract: The parallel evolution of Large Language Models (LLMs) with advanced code-understanding capabilities and the increasing sophistication of malware presents a new frontier for cybersecurity research. This paper evaluates the efficacy of state-of-the-art LLMs in classifying executable code as either benign or malicious. We introduce an automated pipeline that first decompiles Windows executable into a C code using Ghidra disassembler and then leverages LLMs to perform the classification. Our evaluation reveals that while standard LLMs show promise, they are not yet robust enough to replace traditional anti-virus software. We demonstrate that a fine-tuned model, trained on curated malware and benign datasets, significantly outperforms its vanilla counterpart. However, the performance of even this specialized model degrades notably when encountering newer malware. This finding demonstrates the critical need for continuous fine-tuning with emerging threats to maintain model effectiveness against the changing coding patterns and behaviors of malicious software.

</details>


### [257] [Deep Learning-based Binary Analysis for Vulnerability Detection in x86-64 Machine Code](https://arxiv.org/abs/2601.09157)
*Mitchell Petingola*

Main category: cs.CR

TL;DR: 本文探索从原始x86 - 64机器代码直接提取特征用于漏洞检测的可行性，评估两种深度学习模型架构，发现图基模型表现更优。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的漏洞检测多依赖反汇编二进制文件，而机器代码可能实现更高效轻量级模型并保留反汇编中丢失的信息。

Method: 对两种深度学习模型架构进行探索性研究，系统评估它们在三种漏洞类型上的性能。

Result: 图基模型始终优于顺序模型，表明控制流关系的重要性，且机器代码包含有效发现漏洞的足够信息。

Conclusion: 从原始x86 - 64机器代码直接提取特征用于漏洞检测是可行的，图基模型更适合。

Abstract: While much of the current research in deep learning-based vulnerability detection relies on disassembled binaries, this paper explores the feasibility of extracting features directly from raw x86-64 machine code. Although assembly language is more interpretable for humans, it requires more complex models to capture token-level context. In contrast, machine code may enable more efficient, lightweight models and preserve all information that might be lost in disassembly. This paper approaches the task of vulnerability detection through an exploratory study on two specific deep learning model architectures and aims to systematically evaluate their performance across three vulnerability types. The results demonstrate that graph-based models consistently outperform sequential models, emphasizing the importance of control flow relationships, and that machine code contains sufficient information for effective vulnerability discovery.

</details>


### [258] [Explainable Autoencoder-Based Anomaly Detection in IEC 61850 GOOSE Networks](https://arxiv.org/abs/2601.09287)
*Dafne Lozano-Paredes,Luis Bote-Curiel,Juan Ramón Feijóo-Martínez,Ismael Gómez-Talal,José Luis Rojo-Álvarez*

Main category: cs.CR

TL;DR: 本文提出用于IEC 61850 GOOSE网络的可解释无监督多视图异常检测框架，实验显示检测率超99%，误报率低于5%。


<details>
  <summary>Details</summary>
Motivation: IEC 61850 GOOSE协议缺乏原生安全机制，传统入侵检测技术难以检测协议合规和零日攻击。

Method: 提出分离语义完整性和时间可用性的框架，用非对称自编码器学习正常流量特征，用重建误差和统计阈值检测异常，进行特征级重建分析。

Result: 使用真实变电站流量训练，公开数据集测试，攻击检测率超99%，误报率低于5%。

Conclusion: 该框架跨环境泛化能力强，在极端类别不平衡下有效，异常归因可解释。

Abstract: The IEC 61850 Generic Object-Oriented Substation Event (GOOSE) protocol plays a critical role in real-time protection and automation of digital substations, yet its lack of native security mechanisms can expose power systems to sophisticated cyberattacks. Traditional rule-based and supervised intrusion detection techniques struggle to detect protocol-compliant and zero-day attacks under significant class imbalance and limited availability of labeled data. This paper proposes an explainable, unsupervised multi-view anomaly detection framework for IEC 61850 GOOSE networks that explicitly separates semantic integrity and temporal availability. The approach employs asymmetric autoencoders trained only on real operational GOOSE traffic to learn distinct latent representations of sequence-based protocol semantics and timing-related transmission dynamics in normal traffic. Anomaly detection is implemented using reconstruction errors mixed with statistically grounded thresholds, enabling robust detection without specified attack types. Feature-level reconstruction analysis provides intrinsic explainability by directly linking detection outcomes to IEC 61850 protocol characteristics. The proposed framework is evaluated using real substation traffic for training and a public dataset containing normal traffic and message suppression, data manipulation, and denial-of-service attacks for testing. Experimental results show attack detection rates above 99% with false positives remaining below 5% of total traffic, demonstrating strong generalization across environments and effective operation under extreme class imbalance and interpretable anomaly attribution.

</details>


### [259] [SoK: Enhancing Cryptographic Collaborative Learning with Differential Privacy](https://arxiv.org/abs/2601.09460)
*Francesco Capano,Jonas Böhler,Benjamin Weggenmann*

Main category: cs.CR

TL;DR: 本文系统化了加密和差分隐私协作学习（CPCL）领域，引入统一框架，分析不同安全噪声采样技术等的权衡，实现并评估成本，提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 协作学习中数据隐私保护需求高，现有加密和差分隐私技术结合实现CPCL存在挑战，需解决隐私 - 准确性 - 性能权衡问题。

Method: 引入统一框架，分析不同安全噪声采样技术、噪声类型和DP机制，在MPC中实现安全噪声采样选项并评估成本。

Result: 分析了不同技术的权衡、实现挑战，评估了准确性和加密开销，以及计算和通信成本。

Conclusion: 指出了文献中的关键观察、差距和可能的改进，并提出未来研究方向。

Abstract: In collaborative learning (CL), multiple parties jointly train a machine learning model on their private datasets. However, data can not be shared directly due to privacy concerns. To ensure input confidentiality, cryptographic techniques, e.g., multi-party computation (MPC), enable training on encrypted data. Yet, even securely trained models are vulnerable to inference attacks aiming to extract memorized data from model outputs. To ensure output privacy and mitigate inference attacks, differential privacy (DP) injects calibrated noise during training. While cryptography and DP offer complementary guarantees, combining them efficiently for cryptographic and differentially private CL (CPCL) is challenging. Cryptography incurs performance overheads, while DP degrades accuracy, creating a privacy-accuracy-performance trade-off that needs careful design considerations. This work systematizes the CPCL landscape. We introduce a unified framework that generalizes common phases across CPCL paradigms, and identify secure noise sampling as the foundational phase to achieve CPCL. We analyze trade-offs of different secure noise sampling techniques, noise types, and DP mechanisms discussing their implementation challenges and evaluating their accuracy and cryptographic overhead across CPCL paradigms. Additionally, we implement identified secure noise sampling options in MPC and evaluate their computation and communication costs in WAN and LAN. Finally, we propose future research directions based on identified key observations, gaps and possible enhancements in the literature.

</details>


### [260] [Blue Teaming Function-Calling Agents](https://arxiv.org/abs/2601.09292)
*Greta Dolcetti,Giulio Zizzo,Sergio Maffeis*

Main category: cs.CR

TL;DR: 对宣称有函数调用能力的四个开源大语言模型进行抗三种攻击实验评估，测试八种防御方法有效性，发现模型不安全且防御方法难用于现实。


<details>
  <summary>Details</summary>
Motivation: 评估开源大语言模型函数调用能力的鲁棒性及防御方法有效性。

Method: 进行实验评估，用三种攻击测试四个开源大语言模型，测量八种防御方法效果。

Result: 模型默认不安全，防御方法难以应用于现实场景。

Conclusion: 当前开源大语言模型在函数调用方面安全性不足，防御方法有待改进。

Abstract: We present an experimental evaluation that assesses the robustness of four open source LLMs claiming function-calling capabilities against three different attacks, and we measure the effectiveness of eight different defences. Our results show how these models are not safe by default, and how the defences are not yet employable in real-world scenarios.

</details>


### [261] [The Promptware Kill Chain: How Prompt Injections Gradually Evolved Into a Multi-Step Malware](https://arxiv.org/abs/2601.09625)
*Ben Nassi,Bruce Schneier,Oleg Brodt*

Main category: cs.CR

TL;DR: 文章指出基于大语言模型（LLM）系统的攻击构成新威胁，提出“promptware”概念及五步杀伤链模型分析此类威胁。


<details>
  <summary>Details</summary>
Motivation: 现有安全框架无法充分应对基于LLM系统的新攻击面，“prompt injection”表述掩盖了复杂现实。

Method: 提出“promptware”概念，引入五步杀伤链模型分析威胁。

Result: 通过将近期攻击映射到该结构，证明LLM相关攻击遵循类似传统恶意软件活动的系统序列。

Conclusion: promptware杀伤链为安全从业者提供威胁建模方法，为研究人员提供通用词汇应对威胁。

Abstract: The rapid adoption of large language model (LLM)-based systems -- from chatbots to autonomous agents capable of executing code and financial transactions -- has created a new attack surface that existing security frameworks inadequately address. The dominant framing of these threats as "prompt injection" -- a catch-all phrase for security failures in LLM-based systems -- obscures a more complex reality: Attacks on LLM-based systems increasingly involve multi-step sequences that mirror traditional malware campaigns. In this paper, we propose that attacks targeting LLM-based applications constitute a distinct class of malware, which we term \textit{promptware}, and introduce a five-step kill chain model for analyzing these threats. The framework comprises Initial Access (prompt injection), Privilege Escalation (jailbreaking), Persistence (memory and retrieval poisoning), Lateral Movement (cross-system and cross-user propagation), and Actions on Objective (ranging from data exfiltration to unauthorized transactions). By mapping recent attacks to this structure, we demonstrate that LLM-related attacks follow systematic sequences analogous to traditional malware campaigns. The promptware kill chain offers security practitioners a structured methodology for threat modeling and provides a common vocabulary for researchers across AI safety and cybersecurity to address a rapidly evolving threat landscape.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [262] [Towards a Self-Driving Trigger at the LHC: Adaptive Response in Real Time](https://arxiv.org/abs/2601.08910)
*Shaghayegh Emami,Cecilia Tosciri,Giovanna Salvi,Zixin Ding,Yuxin Chen,Abhijith Gandrakota,Christian Herwig,David W. Miller,Jennifer Ngadiuba,Nhan Tran*

Main category: physics.ins-det

TL;DR: 本文探索自驱动触发器概念，引入基准生态系统，用模拟和真实数据展示其动态优化触发器性能的能力，推动触发器设计向数据驱动转变。


<details>
  <summary>Details</summary>
Motivation: 大型强子对撞机等高通量科学设施的实时数据过滤和选择系统通常是静态、手动调整的，需在资源和环境条件变化时动态优化。

Method: 引入基准生态系统模拟对撞机场景，使用模拟数据流和CMS实验的公开碰撞数据，结合机器学习算法。

Result: 展示了在特定成本目标下动态自动优化触发器性能的能力，无需手动重新调整。

Conclusion: 自适应策略使触发器设计从静态手动调整转向智能、自动化、数据驱动控制，为未来高能物理分析带来更大灵活性和发现潜力。

Abstract: Real-time data filtering and selection -- or trigger -- systems at high-throughput scientific facilities such as the experiments at the Large Hadron Collider (LHC) must process extremely high-rate data streams under stringent bandwidth, latency, and storage constraints. Yet these systems are typically designed as static, hand-tuned menus of selection criteria grounded in prior knowledge and simulation. In this work, we further explore the concept of a self-driving trigger, an autonomous data-filtering framework that reallocates resources and adjusts thresholds dynamically in real-time to optimize signal efficiency, rate stability, and computational cost as instrumentation and environmental conditions evolve. We introduce a benchmark ecosystem to emulate realistic collider scenarios and demonstrate real-time optimization of a menu including canonical energy sum triggers as well as modern anomaly-detection algorithms that target non-standard event topologies using machine learning. Using simulated data streams and publicly available collision data from the Compact Muon Solenoid (CMS) experiment, we demonstrate the capability to dynamically and automatically optimize trigger performance under specific cost objectives without manual retuning. Our adaptive strategy shifts trigger design from static menus with heuristic tuning to intelligent, automated, data-driven control, unlocking greater flexibility and discovery potential in future high-energy physics analyses.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [263] [CrowdLLM: Building LLM-Based Digital Populations Augmented with Generative Models](https://arxiv.org/abs/2512.07890)
*Ryan Feng Lin,Keyu Tian,Hanming Zheng,Congjing Zhang,Li Zeng,Shuai Huang*

Main category: cs.MA

TL;DR: 现有基于大语言模型的数字人群工作有局限，提出CrowdLLM集成预训练大模型和生成模型，理论分析和多领域实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的数字人群工作无法充分捕捉真实人群的准确性和多样性，需改进。

Method: 提出CrowdLLM集成预训练大语言模型和生成模型。

Result: 理论分析显示CrowdLLM在创建有成本效益、有代表性、可扩展的数字人群方面有潜力，多领域实验证明其在准确性和分布保真度上表现良好。

Conclusion: CrowdLLM能提升数字人群的多样性和保真度，可匹配真实人群质量。

Abstract: The emergence of large language models (LLMs) has sparked much interest in creating LLM-based digital populations that can be applied to many applications such as social simulation, crowdsourcing, marketing, and recommendation systems. A digital population can reduce the cost of recruiting human participants and alleviate many concerns related to human subject study. However, research has found that most of the existing works rely solely on LLMs and could not sufficiently capture the accuracy and diversity of a real human population. To address this limitation, we propose CrowdLLM that integrates pretrained LLMs and generative models to enhance the diversity and fidelity of the digital population. We conduct theoretical analysis of CrowdLLM regarding its great potential in creating cost-effective, sufficiently representative, scalable digital populations that can match the quality of a real crowd. Comprehensive experiments are also conducted across multiple domains (e.g., crowdsourcing, voting, user rating) and simulation studies which demonstrate that CrowdLLM achieves promising performance in both accuracy and distributional fidelity to human data.

</details>


<div id='physics.ed-ph'></div>

# physics.ed-ph [[Back]](#toc)

### [264] [Personalized Multimodal Feedback Using Multiple External Representations: Strategy Profiles and Learning in High School Physics](https://arxiv.org/abs/2601.09470)
*Natalia Revenga-Lozano,Karina E. Avila,Steffen Steinert,Matthias Schweinberger,Clara E. Gómez-Pérez,Jochen Kuhn,Stefan Küchemann*

Main category: physics.ed-ph

TL;DR: 研究高中物理学习中多外部表征（MERs）与个性化反馈结合情况，反馈与成绩有关，不同表征能力学生策略有别。


<details>
  <summary>Details</summary>
Motivation: 多外部表征和个性化反馈支持物理学习，但关于二者有效整合的证据有限，且多模态大语言模型出现使此问题迫切。

Method: 开展16 - 24周高中物理观察研究，用计算机平台提供多形式反馈，用线性混合效应模型等方法分析。

Result: 详细的多表征反馈与测试后成绩呈小而稳定的正相关，学习者有不同表征选择策略，低表征能力者用多样表征更利于学习。

Conclusion: 研究结果可推动自适应反馈设计和智能辅导系统开发，促进物理教育个性化教学。

Abstract: Multiple external representations (MERs) and personalized feedback support physics learning, yet evidence on how personalized feedback can effectively integrate MERs remains limited. This question is particularly timely given the emergence of multimodal large language models. We conducted a 16-24 week observational study in high school physics (N=661) using a computer-based platform that provided verification and optional elaborated feedback in verbal, graphical and mathematical forms. Linear mixed-effects models and strategy-cluster analyses (ANCOVA-adjusted comparisons) tested associations between feedback use and post-test performance and moderation by representational competence. Elaborated multirepresentational feedback showed a small but consistent positive association with post-test scores independent of prior knowledge and confidence. Learners adopted distinct representation-selection strategies; among students with lower representational competence, using a diverse set of representations related to higher learning, whereas this advantage diminished as competence increased. These findings motivate adaptive feedback designs and inform intelligent tutoring systems capable of tailoring feedback elaboration and representational format to learner profiles, advancing personalized instruction in physics education.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [265] [Probabilistic Computers for MIMO Detection: From Sparsification to 2D Parallel Tempering](https://arxiv.org/abs/2601.09037)
*M Mahmudul Hasan Sajeeb,Corentin Delacour,Kevin Callahan-Coray,Sanjay Seshan,Tathagata Srimani,Kerem Y. Camsari*

Main category: cs.ET

TL;DR: 本文提出用辅助复制变量进行图稀疏化，在FPGA上实现片上并行回火求解器，用于MIMO检测，取得低误码率和快速求解，还提出2D - PT加速收敛，建立了片上p位架构和可扩展算法框架。


<details>
  <summary>Details</summary>
Motivation: 概率计算机在组合优化中有潜力，但现实问题所需的密集连接在硬件上扩展性差。

Method: 采用图稀疏化和辅助复制变量，在FPGA上实现全片上并行回火求解器，针对MIMO检测问题；使用二维并行回火（2D - PT）算法。

Result: 在FPGA上实现128节点稀疏系统的15个温度副本，误码率低于传统线性检测器，每实例求解时间4.7ms；7nm技术ASIC投影显示约90MHz运行，功耗小于200mW；2D - PT实现超10倍快速收敛。

Conclusion: 建立了用于密集组合优化的片上p位架构和可扩展算法框架。

Abstract: Probabilistic computers built from p-bits offer a promising path for combinatorial optimization, but the dense connectivity required by real-world problems scales poorly in hardware. Here, we address this through graph sparsification with auxiliary copy variables and demonstrate a fully on-chip parallel tempering solver on an FPGA. Targeting MIMO detection, a dense, NP-hard problem central to wireless communications, we fit 15 temperature replicas of a 128-node sparsified system (1,920 p-bits) entirely on-chip and achieve bit error rates significantly below conventional linear detectors. We report complete end-to-end solution times of 4.7 ms per instance, with all loading, sampling, readout, and verification overheads included. ASIC projections in 7 nm technology indicate about 90 MHz operation with less than 200 mW power dissipation, suggesting that massive parallelism across multiple chips could approach the throughput demands of next-generation wireless systems. However, sparsification introduces sensitivity to the copy-constraint strength. Employing Two-Dimensional Parallel Tempering (2D-PT), which exchanges replicas across both temperature and constraint dimensions, we demonstrate over 10X faster convergence without manual parameter tuning. These results establish an on-chip p-bit architecture and a scalable algorithmic framework for dense combinatorial optimization.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [266] [Mikasa: A Character-Driven Emotional AI Companion Inspired by Japanese Oshi Culture](https://arxiv.org/abs/2601.09208)
*Miki Ueno*

Main category: cs.HC

TL;DR: 本文指出AI伴侣系统问题源于角色设计与用户 - AI关系定义不佳，以Mikasa为例研究角色驱动设计，评估表明角色连贯性和关系定义影响交互体验，强调角色设计是功能部分。


<details>
  <summary>Details</summary>
Motivation: 解决现有AI伴侣系统难以长期让用户满意和投入的问题。

Method: 以受日本Oshi文化启发的Mikasa为案例进行角色驱动设计，并开展探索性评估。

Result: 用户描述偏好时提及对话自然度等表面特质，也看重关系控制和想象参与，角色连贯性和关系定义潜移默化影响交互感受。

Conclusion: 角色设计是AI伴侣系统的功能部分，相关设计原则可用于多种情感型AI伴侣。

Abstract: Recent progress in large language models and multimodal interaction has made it possible to develop AI companions that can have fluent and emotionally expressive conversations. However, many of these systems have problems keeping users satisfied and engaged over long periods. This paper argues that these problems do not come mainly from weak models, but from poor character design and unclear definitions of the user-AI relationship. I present Mikasa, an emotional AI companion inspired by Japanese Oshi culture-specifically its emphasis on long-term, non-exclusive commitment to a stable character-as a case study of character-driven companion design. Mikasa does not work as a general-purpose assistant or a chatbot that changes roles. Instead, Mikasa is designed as a coherent character with a stable personality and a clearly defined relationship as a partner. This relationship does not force exclusivity or obligation. Rather, it works as a reference point that stabilizes interaction norms and reduces the work users must do to keep redefining the relationship. Through an exploratory evaluation, I see that users describe their preferences using surface-level qualities such as conversational naturalness, but they also value relationship control and imaginative engagement in ways they do not state directly. These results suggest that character coherence and relationship definition work as latent structural elements that shape how good the interaction feels, without users recognizing them as main features. The contribution of this work is to show that character design is a functional part of AI companion systems, not just decoration. Mikasa is one example based on a specific cultural context, but the design principles-commitment to a consistent personality and clear relationship definition-can be used for many emotionally grounded AI companions.

</details>


### [267] [Full Disclosure, Less Trust? How the Level of Detail about AI Use in News Writing Affects Readers' Trust](https://arxiv.org/abs/2601.09620)
*Pooja Prajod,Hannes Cools,Thomas Röggla,Karthikeya Puttur Venkatraj,Amber Kusters,Alia ElKattan,Pablo Cesar,Abdallah El Ali*

Main category: cs.HC

TL;DR: 研究不同详细程度的AI披露对新闻读者信任的影响，发现并非所有AI披露都会导致透明度困境，而是反映了读者对透明度的需求和对AI辅助新闻内容信任之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 了解AI披露的详细程度如何影响读者信任以及在新闻背景下如何导致透明度困境。

Method: 进行3×2×2混合因子研究，有40名参与者，研究三种AI披露水平、两种新闻类型和两种AI参与程度对读者信任的影响，使用新闻媒体信任问卷测量信任，并观察源检查和订阅决策行为，还进行半结构化访谈。

Result: 仅详细的AI披露会导致信任下降；一行和详细披露都会增加源检查行为，详细披露效果更明显；源检查行为主要由话题兴趣驱动，信任是影响订阅决策的主要因素；约三分之二参与者偏好详细披露，偏好一行披露者希望有按需提供细节的格式。

Conclusion: 并非所有AI披露都会导致透明度困境，而是反映了读者对更多透明度的渴望和对AI辅助新闻内容信任之间的权衡。

Abstract: As artificial intelligence (AI) is increasingly integrated into news production, calls for transparency about the use of AI have gained considerable traction. Recent studies suggest that AI disclosures can lead to a ``transparency dilemma'', where disclosure reduces readers' trust. However, little is known about how the \textit{level of detail} in AI disclosures influences trust and contributes to this dilemma within the news context. In this 3$\times$2$\times$2 mixed factorial study with 40 participants, we investigate how three levels of AI disclosures (none, one-line, detailed) across two types of news (politics and lifestyle) and two levels of AI involvement (low and high) affect news readers' trust. We measured trust using the News Media Trust questionnaire, along with two decision behaviors: source-checking and subscription decisions. Questionnaire responses and subscription rates showed a decline in trust only for detailed AI disclosures, whereas source-checking behavior increased for both one-line and detailed disclosures, with the effect being more pronounced for detailed disclosures. Insights from semi-structured interviews suggest that source-checking behavior was primarily driven by interest in the topic, followed by trust, whereas trust was the main factor influencing subscription decisions. Around two-thirds of participants expressed a preference for detailed disclosures, while most participants who preferred one-line indicated a need for detail-on-demand disclosure formats. Our findings show that not all AI disclosures lead to a transparency dilemma, but instead reflect a trade-off between readers' desire for more transparency and their trust in AI-assisted news content.

</details>
