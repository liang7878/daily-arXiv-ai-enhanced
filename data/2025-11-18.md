<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 82]
- [cs.CE](#cs.CE) [Total: 6]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.DC](#cs.DC) [Total: 40]
- [cs.DS](#cs.DS) [Total: 9]
- [cs.GT](#cs.GT) [Total: 8]
- [cs.IR](#cs.IR) [Total: 21]
- [cs.LG](#cs.LG) [Total: 281]
- [cs.NE](#cs.NE) [Total: 6]
- [cs.PF](#cs.PF) [Total: 2]
- [cs.SE](#cs.SE) [Total: 24]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [q-fin.PM](#q-fin.PM) [Total: 2]
- [q-fin.RM](#q-fin.RM) [Total: 2]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [q-fin.TR](#q-fin.TR) [Total: 4]
- [stat.ML](#stat.ML) [Total: 12]
- [stat.CO](#stat.CO) [Total: 2]
- [cs.LO](#cs.LO) [Total: 2]
- [stat.AP](#stat.AP) [Total: 2]
- [cs.CR](#cs.CR) [Total: 29]
- [cs.AR](#cs.AR) [Total: 4]
- [q-fin.GN](#q-fin.GN) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 3]
- [eess.AS](#eess.AS) [Total: 1]
- [quant-ph](#quant-ph) [Total: 8]
- [gr-qc](#gr-qc) [Total: 1]
- [econ.GN](#econ.GN) [Total: 4]
- [math.OC](#math.OC) [Total: 4]
- [cs.NI](#cs.NI) [Total: 1]
- [eess.SP](#eess.SP) [Total: 3]
- [cs.CY](#cs.CY) [Total: 10]
- [eess.IV](#eess.IV) [Total: 4]
- [hep-ph](#hep-ph) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [stat.ME](#stat.ME) [Total: 6]
- [eess.SY](#eess.SY) [Total: 5]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [cs.GL](#cs.GL) [Total: 1]
- [cs.CC](#cs.CC) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 2]
- [cs.RO](#cs.RO) [Total: 13]
- [cs.HC](#cs.HC) [Total: 11]
- [q-bio.MN](#q-bio.MN) [Total: 1]
- [astro-ph.HE](#astro-ph.HE) [Total: 1]
- [math.ST](#math.ST) [Total: 2]
- [cs.SD](#cs.SD) [Total: 5]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.MA](#cs.MA) [Total: 5]
- [math.NA](#math.NA) [Total: 1]
- [cs.MM](#cs.MM) [Total: 2]
- [math.CO](#math.CO) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 3]
- [hep-ex](#hep-ex) [Total: 2]
- [physics.soc-ph](#physics.soc-ph) [Total: 3]
- [cs.CL](#cs.CL) [Total: 39]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.CV](#cs.CV) [Total: 90]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [LLM-Generated Negative News Headlines Dataset: Creation and Benchmarking Against Real Journalism](https://arxiv.org/abs/2511.11591)
*Olusola Babalola,Bolanle Ojokoh,Olutayo Boyinbode*

Main category: cs.AI

TL;DR: 研究探讨大语言模型生成数据集用于NLP任务潜力，以负性新闻标题为例，结果显示生成标题与真实标题匹配度高。


<details>
  <summary>Details</summary>
Motivation: 克服现实数据获取和隐私问题，为NLP任务提供支持。

Method: 用定制提示创建负性新闻标题合成语料库，经专家验证，在嵌入空间分析，用多种测试评估并与真实新闻标题对比。

Result: 生成的标题与真实标题匹配，仅在词性分析的专有名词得分上有明显差异。

Conclusion: 大语言模型生成的数据集可在一定程度上替代真实数据支持NLP任务。

Abstract: This research examines the potential of datasets generated by Large Language Models (LLMs) to support Natural Language Processing (NLP) tasks, aiming to overcome challenges related to data acquisition and privacy concerns associated with real-world data. Focusing on negative valence text, a critical component of sentiment analysis, we explore the use of LLM-generated synthetic news headlines as an alternative to real-world data. A specialized corpus of negative news headlines was created using tailored prompts to capture diverse negative sentiments across various societal domains. The synthetic headlines were validated by expert review and further analyzed in embedding space to assess their alignment with real-world negative news in terms of content, tone, length, and style. Key metrics such as correlation with real headlines, perplexity, coherence, and realism were evaluated. The synthetic dataset was benchmarked against two sets of real news headlines using evaluations including the Comparative Perplexity Test, Comparative Readability Test, Comparative POS Profiling, BERTScore, and Comparative Semantic Similarity. Results show the generated headlines match real headlines with the only marked divergence being in the proper noun score of the POS profile test.

</details>


### [2] [CLINB: A Climate Intelligence Benchmark for Foundational Models](https://arxiv.org/abs/2511.11597)
*Michelle Chen Huebscher,Katharine Mach,Aleksandar Stanić,Markus Leippold,Ben Gaiarin,Zeke Hausfather,Elisa Rawat,Erich Fischer,Massimiliano Ciaramita,Joeri Rogelj,Christian Buck,Lierni Sestorain Saralegui,Reto Knutti*

Main category: cs.AI

TL;DR: 本文引入CLINB基准评估大语言模型处理气候变化专业知识的能力，发现前沿模型知识综合能力强但在证据支持上有不足，强调构建可靠AI系统需缩小知识综合与可验证归因间的差距。


<details>
  <summary>Details</summary>
Motivation: 解决评估大语言模型处理复杂专业知识能力的挑战，以气候变化领域为切入点。

Method: 引入CLINB基准，基于真实用户问题数据集和气候科学家制定的评估标准，实施并验证基于模型的评估过程，对多个前沿模型进行评估。

Result: 前沿模型有出色的知识综合能力，表现出博士级的理解和呈现质量，优于领域专家在较弱模型辅助下整理的“混合”答案，但在证据支持上存在失败情况，参考资料和图像有大量幻觉率。

Conclusion: 缩小知识综合与可验证归因之间的差距对AI在科学工作流程中的部署至关重要，需要像CLINB这样可靠、可解释的基准来推动构建值得信赖的AI系统。

Abstract: Evaluating how Large Language Models (LLMs) handle complex, specialized knowledge remains a critical challenge. We address this through the lens of climate change by introducing CLINB, a benchmark that assesses models on open-ended, grounded, multimodal question answering tasks with clear requirements for knowledge quality and evidential support. CLINB relies on a dataset of real users' questions and evaluation rubrics curated by leading climate scientists. We implement and validate a model-based evaluation process and evaluate several frontier models. Our findings reveal a critical dichotomy. Frontier models demonstrate remarkable knowledge synthesis capabilities, often exhibiting PhD-level understanding and presentation quality. They outperform "hybrid" answers curated by domain experts assisted by weaker models. However, this performance is countered by failures in grounding. The quality of evidence varies, with substantial hallucination rates for references and images. We argue that bridging this gap between knowledge synthesis and verifiable attribution is essential for the deployment of AI in scientific workflows and that reliable, interpretable benchmarks like CLINB are needed to progress towards building trustworthy AI systems.

</details>


### [3] [SynBullying: A Multi LLM Synthetic Conversational Dataset for Cyberbullying Detectio](https://arxiv.org/abs/2511.11599)
*Arefeh Kazemi,Hamza Qadeer,Joachim Wagner,Hossein Hosseini,Sri Balaaji Natarajan Kalaivendan,Brian Davis*

Main category: cs.AI

TL;DR: 介绍用于研究和检测网络欺凌的合成多LLM对话数据集SynBullying，评估其多维度并测试其在网络欺凌分类中的效用。


<details>
  <summary>Details</summary>
Motivation: 为网络欺凌研究和检测提供一种可扩展且符合伦理安全的替代人类数据收集的方法。

Method: 利用大语言模型模拟现实欺凌互动生成数据集，从五个维度评估数据集，并测试其作为独立训练数据和增强源的性能。

Result: 未明确提及具体结果。

Conclusion: 未明确提及具体结论，但暗示了数据集在网络欺凌分类方面有一定研究价值。

Abstract: We introduce SynBullying, a synthetic multi-LLM conversational dataset for studying and detecting cyberbullying (CB). SynBullying provides a scalable and ethically safe alternative to human data collection by leveraging large language models (LLMs) to simulate realistic bullying interactions. The dataset offers (i) conversational structure, capturing multi-turn exchanges rather than isolated posts; (ii) context-aware annotations, where harmfulness is assessed within the conversational flow considering context, intent, and discourse dynamics; and (iii) fine-grained labeling, covering various CB categories for detailed linguistic and behavioral analysis. We evaluate SynBullying across five dimensions, including conversational structure, lexical patterns, sentiment/toxicity, role dynamics, harm intensity, and CB-type distribution. We further examine its utility by testing its performance as standalone training data and as an augmentation source for CB classification.

</details>


### [4] [CausalGuard: A Smart System for Detecting and Preventing False Information in Large Language Models](https://arxiv.org/abs/2511.11600)
*Piyushkumar Patel*

Main category: cs.AI

TL;DR: 介绍新方法CausalGuard结合因果推理和符号逻辑捕捉及预防大语言模型幻觉问题，测试效果好，适用于敏感领域。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在幻觉问题，现有解决方案有缺陷，如需重新训练模型、增加计算成本或未触及问题根源。

Method: 提出CausalGuard方法，结合因果推理和符号逻辑，通过追踪因果关系和检查逻辑一致性两条路径工作。

Result: 在十二个不同基准测试中，CausalGuard能89.3%正确识别幻觉，仅漏判8.3%，减少近80%错误声明，在复杂推理任务中表现出色。

Conclusion: CausalGuard效果良好，推理过程可展示，适用于医疗诊断和金融分析等敏感领域。

Abstract: While large language models have transformed how we interact with AI systems, they have a critical weakness: they confidently state false information that sounds entirely plausible. This "hallucination" problem has become a major barrier to using these models where accuracy matters most. Existing solutions either require retraining the entire model, add significant computational costs, or miss the root causes of why these hallucinations occur in the first place.
  We present CausalGuard, a new approach that combines causal reasoning with symbolic logic to catch and prevent hallucinations as they happen. Unlike previous methods that only check outputs after generation, our system understands the causal chain that leads to false statements and intervenes early in the process. CausalGuard works through two complementary paths: one that traces causal relationships between what the model knows and what it generates, and another that checks logical consistency using automated reasoning.
  Testing across twelve different benchmarks, we found that CausalGuard correctly identifies hallucinations 89.3\% of the time while missing only 8.3\% of actual hallucinations. More importantly, it reduces false claims by nearly 80\% while keeping responses natural and helpful. The system performs especially well on complex reasoning tasks where multiple steps of logic are required. Because CausalGuard shows its reasoning process, it works well in sensitive areas like medical diagnosis or financial analysis where understanding why a decision was made matters as much as the decision itself.

</details>


### [5] [Looking Forward: Challenges and Opportunities in Agentic AI Reliability](https://arxiv.org/abs/2511.11921)
*Liudong Xing,Janet,Lin*

Main category: cs.AI

TL;DR: 探讨构建可靠AI系统（尤其是代理AI系统）的挑战、未来发展、开放研究问题及测试评估方向。


<details>
  <summary>Details</summary>
Motivation: 为构建可靠的AI系统，尤其是代理AI系统提供思路和方向。

Method: 讨论与分析相关研究问题和挑战。

Result: 明确了减轻级联故障风险的开放研究问题，指出动态环境等方面的研究挑战与机遇，以及测试评估的研究方向。

Conclusion: 对构建可靠AI系统的研究有一定指导意义，指明了未来研究的方向。

Abstract: This chapter presents perspectives for challenges and future development in building reliable AI systems, particularly, agentic AI systems. Several open research problems related to mitigating the risks of cascading failures are discussed. The chapter also sheds lights on research challenges and opportunities in aspects including dynamic environments, inconsistent task execution, unpredictable emergent behaviors, as well as resource-intensive reliability mechanisms. In addition, several research directions along the line of testing and evaluating reliability of agentic AI systems are also discussed.

</details>


### [6] [Quantifying Skill and Chance: A Unified Framework for the Geometry of Games](https://arxiv.org/abs/2511.11611)
*David H. Silver*

Main category: cs.AI

TL;DR: 提出分离游戏中技巧和运气的量化框架，定义技能 - 运气指数S(G)，应用于30款游戏，还引入波动性Sigma，框架可扩展到一般随机决策系统。


<details>
  <summary>Details</summary>
Motivation: 在游戏中分离技巧和运气因素，以便进行玩家影响、游戏平衡和预测稳定性的比较。

Method: 将游戏建模为对随机决策树的互补控制源，通过分解游戏结果为技能杠杆K和运气杠杆L来定义技能 - 运气指数S(G)。

Result: 对30款游戏进行分析，得出从纯运气到纯技巧的连续谱，如硬币抛掷S = -1，西洋双陆棋S = 0，国际象棋S = +1，扑克S = 0.33等，并引入波动性Sigma。

Conclusion: 该框架可扩展到一般随机决策系统，适用于游戏设计、AI评估和风险评估等领域。

Abstract: We introduce a quantitative framework for separating skill and chance in games by modeling them as complementary sources of control over stochastic decision trees. We define the Skill-Luck Index S(G) in [-1, 1] by decomposing game outcomes into skill leverage K and luck leverage L. Applying this to 30 games reveals a continuum from pure chance (coin toss, S = -1) through mixed domains such as backgammon (S = 0, Sigma = 1.20) to pure skill (chess, S = +1, Sigma = 0). Poker exhibits moderate skill dominance (S = 0.33) with K = 0.40 +/- 0.03 and Sigma = 0.80. We further introduce volatility Sigma to quantify outcome uncertainty over successive turns. The framework extends to general stochastic decision systems, enabling principled comparisons of player influence, game balance, and predictive stability, with applications to game design, AI evaluation, and risk assessment.

</details>


### [7] [Value-Aligned Prompt Moderation via Zero-Shot Agentic Rewriting for Safe Image Generation](https://arxiv.org/abs/2511.11693)
*Xin Zhao,Xiaojun Chen,Bingshan Liu,Zeyao Liu,Zhendong Zhao,Xiaoyan Gu*

Main category: cs.AI

TL;DR: 介绍VALOR框架用于安全的文本到图像生成，实验表明其能显著减少不安全输出。


<details>
  <summary>Details</summary>
Motivation: 生成式视觉语言模型在对抗性提示下会产生不安全内容，现有防御方法难以兼顾输出质量与成本。

Method: 引入VALOR框架，集成分层提示分析和人类价值推理，包括NSFW检测器、文化价值对齐模块和意图消歧器，检测到不安全内容时重写提示，必要时进行风格再生。

Result: 在对抗性、模糊和敏感价值提示的实验中，VALOR最多可将不安全输出减少100.00%，同时保留提示的有用性和创造性。

Conclusion: VALOR是在开放环境中部署安全、一致和有用图像生成系统的可扩展且有效的方法。

Abstract: Generative vision-language models like Stable Diffusion demonstrate remarkable capabilities in creative media synthesis, but they also pose substantial risks of producing unsafe, offensive, or culturally inappropriate content when prompted adversarially. Current defenses struggle to align outputs with human values without sacrificing generation quality or incurring high costs. To address these challenges, we introduce VALOR (Value-Aligned LLM-Overseen Rewriter), a modular, zero-shot agentic framework for safer and more helpful text-to-image generation. VALOR integrates layered prompt analysis with human-aligned value reasoning: a multi-level NSFW detector filters lexical and semantic risks; a cultural value alignment module identifies violations of social norms, legality, and representational ethics; and an intention disambiguator detects subtle or indirect unsafe implications. When unsafe content is detected, prompts are selectively rewritten by a large language model under dynamic, role-specific instructions designed to preserve user intent while enforcing alignment. If the generated image still fails a safety check, VALOR optionally performs a stylistic regeneration to steer the output toward a safer visual domain without altering core semantics. Experiments across adversarial, ambiguous, and value-sensitive prompts show that VALOR significantly reduces unsafe outputs by up to 100.00% while preserving prompt usefulness and creativity. These results highlight VALOR as a scalable and effective approach for deploying safe, aligned, and helpful image generation systems in open-world settings.

</details>


### [8] [Towards autonomous quantum physics research using LLM agents with access to intelligent tools](https://arxiv.org/abs/2511.11752)
*Sören Arlt,Xuemei Gu,Mario Krenn*

Main category: cs.AI

TL;DR: 本文介绍了AI - Mandel，一个能在量子物理领域生成并实施想法的大语言模型代理，展示了AI生成具体可行想法的能力，还揭示了迈向人类水平人工智能科学家面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前科学中AI生成创意想法少且模糊，实现想法仍依赖人类，自动化想法生成和实施系统能改变人类在科学过程中的角色。

Method: 提出AI - Mandel，它从文献中提炼想法，利用特定领域AI工具将想法转化为可在实验室实施的具体实验设计。

Result: AI - Mandel生成的想法具有科学价值，已为其中两个想法撰写独立后续科学论文，包括量子隐形传态新变体等。

Conclusion: AI - Mandel是能生成和实施具体可行想法的AI物理学家原型，构建此类系统有助于加速科学发展，也揭示了迈向人类水平人工智能科学家的具体挑战。

Abstract: Artificial intelligence (AI) is used in numerous fields of science, yet the initial research questions and targets are still almost always provided by human researchers. AI-generated creative ideas in science are rare and often vague, so that it remains a human task to execute them. Automating idea generation and implementation in one coherent system would significantly shift the role of humans in the scientific process. Here we present AI-Mandel, an LLM agent that can generate and implement ideas in quantum physics. AI-Mandel formulates ideas from the literature and uses a domain-specific AI tool to turn them into concrete experiment designs that can readily be implemented in laboratories. The generated ideas by AI-Mandel are often scientifically interesting - for two of them we have already written independent scientific follow-up papers. The ideas include new variations of quantum teleportation, primitives of quantum networks in indefinite causal orders, and new concepts of geometric phases based on closed loops of quantum information transfer. AI-Mandel is a prototypical demonstration of an AI physicist that can generate and implement concrete, actionable ideas. Building such a system is not only useful to accelerate science, but it also reveals concrete open challenges on the path to human-level artificial scientists.

</details>


### [9] [Think, Speak, Decide: Language-Augmented Multi-Agent Reinforcement Learning for Economic Decision-Making](https://arxiv.org/abs/2511.12876)
*Heyang Ma,Qirui Mi,Qipeng Yang,Zijun Fan,Bo Li,Haifeng Zhang*

Main category: cs.AI

TL;DR: 本文提出LAMP框架，将语言融入经济决策，在经济模拟实验中表现优于基线模型，展示了语言增强策略潜力。


<details>
  <summary>Details</summary>
Motivation: 经济决策受非结构化语言影响，而多智能体强化学习处理语言存在困难，需改进以贴近现实。

Method: 提出LAMP框架，遵循Think - Speak - Decide流程，融合数值数据、推理等进行语言增强决策。

Result: 在经济模拟中，LAMP在累计回报、鲁棒性和可解释性方面优于多智能体强化学习和仅大语言模型基线。

Conclusion: 语言增强策略有潜力提供更有效和鲁棒的经济策略。

Abstract: Economic decision-making depends not only on structured signals such as prices and taxes, but also on unstructured language, including peer dialogue and media narratives. While multi-agent reinforcement learning (MARL) has shown promise in optimizing economic decisions, it struggles with the semantic ambiguity and contextual richness of language. We propose LAMP (Language-Augmented Multi-Agent Policy), a framework that integrates language into economic decision-making and narrows the gap to real-world settings. LAMP follows a Think-Speak-Decide pipeline: (1) Think interprets numerical observations to extract short-term shocks and long-term trends, caching high-value reasoning trajectories; (2) Speak crafts and exchanges strategic messages based on reasoning, updating beliefs by parsing peer communications; and (3) Decide fuses numerical data, reasoning, and reflections into a MARL policy to optimize language-augmented decision-making. Experiments in economic simulation show that LAMP outperforms both MARL and LLM-only baselines in cumulative return (+63.5%, +34.0%), robustness (+18.8%, +59.4%), and interpretability. These results demonstrate the potential of language-augmented policies to deliver more effective and robust economic strategies.

</details>


### [10] [Learning to Refine: An Agentic RL Approach for Iterative SPARQL Query Construction](https://arxiv.org/abs/2511.11770)
*Floris Vossebeld,Shenghui Wang*

Main category: cs.AI

TL;DR: 本文提出新的智能体框架，让大语言模型学习迭代构建SPARQL查询的弹性策略，在LC - QuAD 2.0子集上准确率提升显著，为教智能体掌握形式化工具提供通用蓝图。


<details>
  <summary>Details</summary>
Motivation: 大语言模型一次性生成SPARQL查询的脆弱性以及现有方法缺乏基于实时执行反馈动态调试查询的自适应策略，阻碍了知识图谱问答发展。

Method: 引入新的智能体框架，用基于结果驱动的强化学习（GRPO）训练一个3B参数模型，且不进行监督微调。

Result: 在LC - QuAD 2.0的可执行单答案子集上，实体链接后准确率达49.7%，比最强迭代零样本基线提高17.5个百分点。

Conclusion: 该工作为教智能体通过交互掌握形式化、符号化工具提供通用蓝图，弥合了概率性大语言模型与知识图谱结构化世界的差距。

Abstract: Generating complex, logically-sound SPARQL queries for multi-hop questions remains a critical bottleneck for Knowledge Graph Question Answering, as the brittle nature of one-shot generation by Large Language Models (LLMs) hinders reliable interaction with structured data. Current methods lack the adaptive policies needed to dynamically debug queries based on real-time execution feedback. This paper introduces a novel agentic framework where an LLM learns a resilient policy for the sequential process of iterative SPARQL construction. We show that a compact 3B-parameter model, trained exclusively via outcome-driven Reinforcement Learning (GRPO) without supervised fine-tuning, can learn effective policies for this task, discovering how to systematically recover from execution errors and refine its queries toward a correct answer. On a curated, executable single-answer subset of LC-QuAD 2.0, our agent achieves 49.7\% accuracy post-entity-linking, a significant 17.5 percentage point improvement over the strongest iterative zero-shot baseline. Further analysis reveals that while the agent's capability is driven by RL, its performance is enhanced by an explicit deliberative reasoning step that acts as a cognitive scaffold to improve policy precision. This work presents a generalizable blueprint for teaching agents to master formal, symbolic tools through interaction, bridging the gap between probabilistic LLMs and the structured world of Knowledge Graphs.

</details>


### [11] [On the Measure of a Model: From Intelligence to Generality](https://arxiv.org/abs/2511.11773)
*Ruchira Dhar,Ninell Oldenburg,Anders Soegaard*

Main category: cs.AI

TL;DR: 现有评估大语言模型智能的基准存在问题，应基于通用性评估，通用性是多任务学习问题，可作为评估能力的更稳定基础。


<details>
  <summary>Details</summary>
Motivation: 现有智能概念不明确，基于智能的评估基准可能与实际效用不一致，需寻找更合适的评估方式。

Method: 通过概念和形式分析，对常支撑智能评估的三个假设进行审查。

Result: 只有通用性经得住概念和实证审查，智能并非实现通用性的因素，通用性是多任务学习问题。

Conclusion: 应重新构建评估AI进展的方式，以通用性作为评估跨多样和不断演变任务能力的更稳定基础。

Abstract: Benchmarks such as ARC, Raven-inspired tests, and the Blackbird Task are widely used to evaluate the intelligence of large language models (LLMs). Yet, the concept of intelligence remains elusive- lacking a stable definition and failing to predict performance on practical tasks such as question answering, summarization, or coding. Optimizing for such benchmarks risks misaligning evaluation with real-world utility. Our perspective is that evaluation should be grounded in generality rather than abstract notions of intelligence. We identify three assumptions that often underpin intelligence-focused evaluation: generality, stability, and realism. Through conceptual and formal analysis, we show that only generality withstands conceptual and empirical scrutiny. Intelligence is not what enables generality; generality is best understood as a multitask learning problem that directly links evaluation to measurable performance breadth and reliability. This perspective reframes how progress in AI should be assessed and proposes generality as a more stable foundation for evaluating capability across diverse and evolving tasks.

</details>


### [12] [Do LLMs Really Struggle at NL-FOL Translation? Revealing their Strengths via a Novel Benchmarking Strategy](https://arxiv.org/abs/2511.11816)
*Andrea Brunello,Luca Geatti,Michele Mignani,Angelo Montanari,Nicola Saccomanno*

Main category: cs.AI

TL;DR: 本文指出NL - FOL翻译是长期挑战，LLMs表现有争议。批判性审视现有评估，提出新评估协议，发现对话式LLMs在NL - FOL翻译中表现好，嵌入中心模型较差。


<details>
  <summary>Details</summary>
Motivation: NL - FOL翻译是长期挑战，LLMs在该任务上的表现有不同结果，需准确评估其能力。

Method: 批判性审视现有评估数据集和协议，提出新评估协议以区分语义逻辑理解和浅层模式识别等。

Result: 对话式LLMs展示出强NL - FOL翻译技能和对句子逻辑的真正理解，嵌入中心模型表现差。

Conclusion: 新评估协议能更准确评估LLMs的NL - FOL翻译能力，对话式LLMs在该任务上更具优势。

Abstract: Due to its expressiveness and unambiguous nature, First-Order Logic (FOL) is a powerful formalism for representing concepts expressed in natural language (NL). This is useful, e.g., for specifying and verifying desired system properties. While translating FOL into human-readable English is relatively straightforward, the inverse problem, converting NL to FOL (NL-FOL translation), has remained a longstanding challenge, for both humans and machines. Although the emergence of Large Language Models (LLMs) promised a breakthrough, recent literature provides contrasting results on their ability to perform NL-FOL translation. In this work, we provide a threefold contribution. First, we critically examine existing datasets and protocols for evaluating NL-FOL translation performance, revealing key limitations that may cause a misrepresentation of LLMs' actual capabilities. Second, to overcome these shortcomings, we propose a novel evaluation protocol explicitly designed to distinguish genuine semantic-level logical understanding from superficial pattern recognition, memorization, and dataset contamination. Third, using this new approach, we show that state-of-the-art, dialogue-oriented LLMs demonstrate strong NL-FOL translation skills and a genuine grasp of sentence-level logic, whereas embedding-centric models perform markedly worse.

</details>


### [13] [TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models](https://arxiv.org/abs/2511.11831)
*Wenhao Zhou,Hao Zheng,Rong Zhao*

Main category: cs.AI

TL;DR: 提出TopoPerception基准评估大视觉语言模型全局视觉感知能力，发现现有模型表现差，提示需新训练范式或架构。


<details>
  <summary>Details</summary>
Motivation: 现有大视觉语言模型视觉感知模块成瓶颈，传统评估基准有局部捷径，需无捷径的全局视觉感知评估方法。

Method: 引入TopoPerception基准，利用拓扑属性评估不同粒度下模型的全局视觉感知能力。

Result: 评估发现现有模型在最粗粒度感知下表现与随机无异，且推理能力强的模型准确率更低。

Conclusion: TopoPerception揭示当前大视觉语言模型的关键瓶颈，为提升全局视觉感知提供方向，单纯扩大模型规模不足，可能需新范式或架构。

Abstract: Large Vision-Language Models (LVLMs) typically align visual features from an encoder with a pre-trained Large Language Model (LLM). However, this makes the visual perception module a bottleneck, which constrains the overall capabilities of LVLMs. Conventional evaluation benchmarks, while rich in visual semantics, often contain unavoidable local shortcuts that can lead to an overestimation of models' perceptual abilities. Here, we introduce TopoPerception, a benchmark that leverages topological properties to rigorously evaluate the global visual perception capabilities of LVLMs across various granularities. Since topology depends on the global structure of an image and is invariant to local features, TopoPerception enables a shortcut-free assessment of global perception, fundamentally distinguishing it from semantically rich tasks. We evaluate state-of-the-art models on TopoPerception and find that even at the coarsest perceptual granularity, all models perform no better than random chance, indicating a profound inability to perceive global visual features. Notably, a consistent trend emerge within model families: more powerful models with stronger reasoning capabilities exhibit lower accuracy. This suggests that merely scaling up models is insufficient to address this deficit and may even exacerbate it. Progress may require new training paradigms or architectures. TopoPerception not only exposes a critical bottleneck in current LVLMs but also offers a lens and direction for improving their global visual perception. The data and code are publicly available at: https://github.com/Wenhao-Zhou/TopoPerception.

</details>


### [14] [KrwEmd: Revising the Imperfect-Recall Abstraction from Forgetting Everything](https://arxiv.org/abs/2511.12089)
*Yanchang Fu,Qiyue Yin,Shengda Liu,Pei Xu,Kaiqi Huang*

Main category: cs.AI

TL;DR: 本文提出KrwEmd算法解决德州扑克等大规模不完全信息游戏中手牌抽象过度问题，实验显示其能提升AI游戏表现。


<details>
  <summary>Details</summary>
Motivation: 解决大规模不完全信息游戏中手牌抽象过度损害AI性能的问题，该问题源于不完美回忆抽象极端实现丢弃历史信息。

Method: 引入k - recall winrate特征，利用未来和历史游戏信息区分信号观察信息集并捕捉其相似性；开发KrwEmd算法，用推土机距离衡量特征差异来聚类信号观察信息集。

Result: 实验表明KrwEmd相比现有算法显著提升AI游戏性能。

Conclusion: KrwEmd算法有效解决手牌抽象过度问题，提升AI在大规模不完全信息游戏中的表现。

Abstract: Excessive abstraction is a critical challenge in hand abstraction-a task specific to games like Texas hold'em-when solving large-scale imperfect-information games, as it impairs AI performance. This issue arises from extreme implementations of imperfect-recall abstraction, which entirely discard historical information. This paper presents KrwEmd, the first practical algorithm designed to address this problem. We first introduce the k-recall winrate feature, which not only qualitatively distinguishes signal observation infosets by leveraging both future and, crucially, historical game information, but also quantitatively captures their similarity. We then develop the KrwEmd algorithm, which clusters signal observation infosets using earth mover's distance to measure discrepancies between their features. Experimental results demonstrate that KrwEmd significantly improves AI gameplay performance compared to existing algorithms.

</details>


### [15] [End to End AI System for Surgical Gesture Sequence Recognition and Clinical Outcome Prediction](https://arxiv.org/abs/2511.11899)
*Xi Li,Nicholas Matsumoto,Ujjwal Pasupulety,Atharva Deo,Cherine Yang,Jay Moran,Miguel E. Hernandez,Peter Wager,Jasmine Lin,Jeanine Kim,Alvin C. Goh,Christian Wagner,Geoffrey A. Sonn,Andrew J. Hung*

Main category: cs.AI

TL;DR: 提出F2O系统将组织解剖视频转化为手势序列，检测手术手势并关联术后结果，为手术反馈和临床决策支持奠定基础。


<details>
  <summary>Details</summary>
Motivation: 解决术中行为细粒度分析及其对患者结果影响这一长期挑战。

Method: 利用基于transformer的时空建模和逐帧分类，将组织解剖视频转化为手势序列。

Result: F2O能稳健检测手术手势，其特征预测术后结果准确性与人工注释相当，捕捉到与勃起功能恢复相关的关键模式。

Conclusion: F2O为数据驱动的手术反馈和前瞻性临床决策支持奠定基础。

Abstract: Fine-grained analysis of intraoperative behavior and its impact on patient outcomes remain a longstanding challenge. We present Frame-to-Outcome (F2O), an end-to-end system that translates tissue dissection videos into gesture sequences and uncovers patterns associated with postoperative outcomes. Leveraging transformer-based spatial and temporal modeling and frame-wise classification, F2O robustly detects consecutive short (~2 seconds) gestures in the nerve-sparing step of robot-assisted radical prostatectomy (AUC: 0.80 frame-level; 0.81 video-level). F2O-derived features (gesture frequency, duration, and transitions) predicted postoperative outcomes with accuracy comparable to human annotations (0.79 vs. 0.75; overlapping 95% CI). Across 25 shared features, effect size directions were concordant with small differences (~ 0.07), and strong correlation (r = 0.96, p < 1e-14). F2O also captured key patterns linked to erectile function recovery, including prolonged tissue peeling and reduced energy use. By enabling automatic interpretable assessment, F2O establishes a foundation for data-driven surgical feedback and prospective clinical decision support.

</details>


### [16] [Forgetting-MarI: LLM Unlearning via Marginal Information Regularization](https://arxiv.org/abs/2511.11914)
*Shizhou Xu,Yuan Ni,Stefan Broecker,Thomas Strohmer*

Main category: cs.AI

TL;DR: 提出Forgetting - MarI框架用于大语言模型去学习，能只移除待去学习数据的边际信息，性能优于现有方法，推动AI系统合规发展。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型训练数据集增大，从训练模型中移除特定数据影响对隐私保护和合规至关重要，现有去学习方法存在移除信息过多导致性能下降问题。

Method: 引入Forgetting - MarI框架，通过惩罚边际信息，对训练模型中待去学习数据集的残余影响给出明确上界。

Result: 大量实验表明该方法优于当前最先进的去学习方法，在不同基准测试中能可靠遗忘数据且更好地保留模型整体性能。

Conclusion: 这一进展是使AI系统更可控、符合隐私和版权法规且不降低有效性的重要一步。

Abstract: As AI models are trained on ever-expanding datasets, the ability to remove the influence of specific data from trained models has become essential for privacy protection and regulatory compliance. Unlearning addresses this challenge by selectively removing parametric knowledge from the trained models without retraining from scratch, which is critical for resource-intensive models such as Large Language Models (LLMs). Existing unlearning methods often degrade model performance by removing more information than necessary when attempting to ''forget'' specific data. We introduce Forgetting-MarI, an LLM unlearning framework that provably removes only the additional (marginal) information contributed by the data to be unlearned, while preserving the information supported by the data to be retained. By penalizing marginal information, our method yields an explicit upper bound on the unlearn dataset's residual influence in the trained models, providing provable undetectability. Extensive experiments confirm that our approach outperforms current state-of-the-art unlearning methods, delivering reliable forgetting and better preserved general model performance across diverse benchmarks. This advancement represents an important step toward making AI systems more controllable and compliant with privacy and copyright regulations without compromising their effectiveness.

</details>


### [17] [An Analysis of Architectural Impact on LLM-based Abstract Visual Reasoning: A Systematic Benchmark on RAVEN-FAIR](https://arxiv.org/abs/2511.11916)
*Sinan Urgun,Seçkin Arı*

Main category: cs.AI

TL;DR: 研究系统评估大语言模型在抽象视觉推理问题上的表现，发现各模型对架构设计敏感度不同，推理效果因模型而异。


<details>
  <summary>Details</summary>
Motivation: 系统评估大语言模型在抽象视觉推理问题上的表现。

Method: 在RAVEN - FAIR数据集上用四种推理架构测试四个大语言模型，用SSIM和LPIPS指标评估视觉响应，分析思维链分数和错误类型。

Result: GPT - 4.1 - Mini在各架构中整体准确率最高；多智能体架构对模型语义和数值平衡影响不一；各模型对架构设计敏感度不同；响应覆盖度差异影响跨架构比较。

Conclusion: 推理有效性因模型而异，采用多次运行策略估计配置上限性能，避免单次运行评估的不可靠性。

Abstract: This study aims to systematically evaluate the performance of large language models (LLMs) in abstract visual reasoning problems. We examined four LLM models (GPT-4.1-Mini, Claude-3.5-Haiku, Gemini-1.5-Flash, Llama-3.3-70b) utilizing four different reasoning architectures (single-shot, embedding-controlled repetition, self-reflection, and multi-agent) on the RAVEN-FAIR dataset. Visual responses generated through a three-stage process (JSON extraction, LLM reasoning, and Tool Function) were evaluated using SSIM and LPIPS metrics; Chain-of-Thought scores and error types (semantic hallucination, numeric misperception) were analyzed. Results demonstrate that GPT-4.1-Mini consistently achieved the highest overall accuracy across all architectures, indicating a strong reasoning capability. While the multi-agent architecture occasionally altered semantic and numeric balance across models, these effects were not uniformly beneficial. Instead, each model exhibited distinct sensitivity patterns to architectural design, underscoring that reasoning effectiveness remains model-specific. Variations in response coverage further emerged as a confounding factor that complicates direct cross-architecture comparison. To estimate the upper-bound performance of each configuration, we report the best of five independent runs, representing a best-case scenario rather than an averaged outcome. This multi-run strategy aligns with recent recommendations, which emphasize that single-run evaluations are fragile and may lead to unreliable conclusions.

</details>


### [18] [A Neuromorphic Architecture for Scalable Event-Based Control](https://arxiv.org/abs/2511.11924)
*Yongkang Huo,Fulvio Forni,Rodolphe Sepulchre*

Main category: cs.AI

TL;DR: 本文引入RWTA作为可扩展神经形态控制架构基本元素，结合离散计算可靠性与连续调节可调性，用统一语言解决问题，并通过蛇形机器人神经系统设计展示架构特性。


<details>
  <summary>Details</summary>
Motivation: 构建一种结合离散计算可靠性和连续调节可调性的可扩展神经形态控制架构。

Method: 引入“rebound Winner-Take-All (RWTA)” motif作为基本元素，采用基于事件的框架，用统一物理建模语言。

Result: 通过蛇形机器人神经系统设计展示了架构的多功能性、鲁棒性和模块化特点。

Conclusion: 所提出的架构能有效结合离散计算和连续调节，具有多种良好特性，可用于实际机器人神经系统设计。

Abstract: This paper introduces the ``rebound Winner-Take-All (RWTA)" motif as the basic element of a scalable neuromorphic control architecture. From the cellular level to the system level, the resulting architecture combines the reliability of discrete computation and the tunability of continuous regulation: it inherits the discrete computation capabilities of winner-take-all state machines and the continuous tuning capabilities of excitable biophysical circuits. The proposed event-based framework addresses continuous rhythmic generation and discrete decision-making in a unified physical modeling language. We illustrate the versatility, robustness, and modularity of the architecture through the nervous system design of a snake robot.

</details>


### [19] [Augmenting The Weather: A Hybrid Counterfactual-SMOTE Algorithm for Improving Crop Growth Prediction When Climate Changes](https://arxiv.org/abs/2511.11945)
*Mohammed Temraz,Mark T Keane*

Main category: cs.AI

TL;DR: 论文针对AI处理气候变化数据问题，提出CFA - SMOTE数据增强方法，并通过实验对比验证效果，以爱尔兰奶牛场草生长预测为案例。


<details>
  <summary>Details</summary>
Motivation: 气候变化带来极端天气影响经济部门，AI现有解决方案难以处理气候异常数据，历史数据分布的机器学习方法应对离群事件能力差。

Method: 提出Counterfactual - Based SMOTE (CFA - SMOTE)方法，结合可解释AI的基于实例的反事实方法和SMOTE类不平衡方法，创建代表异常气候事件的合成数据点。

Result: 进行对比实验，将CFA - SMOTE方法与不同条件下的基准反事实和类不平衡方法对比。

Conclusion: 文档未明确提及结论内容，但推测该方法可能在处理气候异常数据预测上有较好表现。

Abstract: In recent years, humanity has begun to experience the catastrophic effects of climate change as economic sectors (such as agriculture) struggle with unpredictable and extreme weather events. Artificial Intelligence (AI) should help us handle these climate challenges but its most promising solutions are not good at dealing with climate-disrupted data; specifically, machine learning methods that work from historical data-distributions, are not good at handling out-of-distribution, outlier events. In this paper, we propose a novel data augmentation method, that treats the predictive problems around climate change as being, in part, due to class-imbalance issues; that is, prediction from historical datasets is difficult because, by definition, they lack sufficient minority-class instances of "climate outlier events". This novel data augmentation method -- called Counterfactual-Based SMOTE (CFA-SMOTE) -- combines an instance-based counterfactual method from Explainable AI (XAI) with the well-known class-imbalance method, SMOTE. CFA-SMOTE creates synthetic data-points representing outlier, climate-events that augment the dataset to improve predictive performance. We report comparative experiments using this CFA-SMOTE method, comparing it to benchmark counterfactual and class-imbalance methods under different conditions (i.e., class-imbalance ratios). The focal climate-change domain used relies on predicting grass growth on Irish dairy farms, during Europe-wide drought and forage crisis of 2018.

</details>


### [20] [LLM-Assisted Formalization Enables Deterministic Detection of Statutory Inconsistency in the Internal Revenue Code](https://arxiv.org/abs/2511.11954)
*Borchuluun Yadamsuren,Steven Keith Platt,Miguel Diaz*

Main category: cs.AI

TL;DR: 本文提出混合神经符号框架检测复杂法律中的法定不一致性，以美国税法为案例，结合大语言模型和符号逻辑，实验表明该模型能实现确定性检测。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在税务领域应用少，且在层次处理和深度结构化推理方面存在挑战，需要解决法定不一致性检测问题。

Method: 结合GPT - 4o、GPT - 5和Prolog，用GPT - 4o将税法条文转换为Prolog规则并在SWISH中优化，测试Prolog增强提示对GPT - 4o不一致性检测的影响。

Result: GPT - 4o单独用自然语言提示或Prolog增强提示检测不一致性准确率为33%，推理质量有差异；混合Prolog模型产生确定性和可重复性结果，成功检测到不一致区域。

Conclusion: 基于符号逻辑的大语言模型辅助形式化能实现透明可靠的法定不一致性检测。

Abstract: This study introduces a hybrid neuro-symbolic framework that achieves deterministic detection of statutory inconsistency in complex law. We use the U.S. Internal Revenue Code (IRC) as a case study because its complexity makes it a fertile domain for identifying conflicts. Our research offers a solution for detecting inconsistent provisions by combining Large Language Models (LLMs) with symbolic logic.
  LLM-based methods can support compliance, fairness, and statutory drafting, yet tax-specific applications remain sparse. A key challenge is that such models struggle with hierarchical processing and deep structured reasoning, especially over long text.
  This research addresses these gaps through experiments using GPT-4o, GPT-5, and Prolog. GPT-4o was first used to translate Section 121 into Prolog rules and refine them in SWISH. These rules were then incorporated into prompts to test whether Prolog-augmented prompting improved GPT-4o's inconsistency detection. GPT-4o, whether prompted with natural language alone or with Prolog augmentation, detected the inconsistency in only one of three strategies (33 percent accuracy), but its reasoning quality differed: natural-language prompting achieved 100 percent rule coverage, while Prolog-augmented prompting achieved 66 percent, indicating more incomplete statutory analysis.
  In contrast to probabilistic prompting, the hybrid Prolog model produced deterministic and reproducible results. Guided by GPT-5 for refinement, the model formalized the IRC section's competing interpretations and successfully detected an inconsistency zone. Validation tests confirm that the Prolog implementation is accurate, internally consistent, deterministic, and capable of autonomously identifying inconsistencies. These findings show that LLM-assisted formalization, anchored in symbolic logic, enables transparent and reliable statutory inconsistency detection.

</details>


### [21] [Improving Autoformalization Using Direct Dependency Retrieval](https://arxiv.org/abs/2511.11990)
*Shaoqi Wang,Lu Yu,Chunjie Yang*

Main category: cs.AI

TL;DR: 现有语句自动形式化方法有缺陷，本文提出基于DDR的检索增强框架，实验显示DDR模型在检索指标上优于SOTA方法，配备DDR的自动形式化器性能更优。


<details>
  <summary>Details</summary>
Motivation: 现有语句自动形式化方法缺乏上下文感知，检索增强方法在形式库依赖检索上精度和召回率差且缺乏可扩展性。

Method: 提出基于DDR的检索增强框架，直接从自然语言数学描述生成候选库依赖，通过后缀数组检查验证其在形式库中的存在，构建超500,000样本的依赖检索数据集并微调DDR模型。

Result: DDR模型在检索精度和召回率上显著优于SOTA方法，配备DDR的自动形式化器在单轮尝试准确率和多轮尝试稳定性上均有优势。

Conclusion: 基于DDR的检索增强框架在语句自动形式化方面表现良好，能有效提升性能。

Abstract: The convergence of deep learning and formal mathematics has spurred research in formal verification. Statement autoformalization, a crucial first step in this process, aims to translate informal descriptions into machine-verifiable representations but remains a significant challenge. The core difficulty lies in the fact that existing methods often suffer from a lack of contextual awareness, leading to hallucination of formal definitions and theorems. Furthermore, current retrieval-augmented approaches exhibit poor precision and recall for formal library dependency retrieval, and lack the scalability to effectively leverage ever-growing public datasets. To bridge this gap, we propose a novel retrieval-augmented framework based on DDR (\textit{Direct Dependency Retrieval}) for statement autoformalization. Our DDR method directly generates candidate library dependencies from natural language mathematical descriptions and subsequently verifies their existence within the formal library via an efficient suffix array check. Leveraging this efficient search mechanism, we constructed a dependency retrieval dataset of over 500,000 samples and fine-tuned a high-precision DDR model. Experimental results demonstrate that our DDR model significantly outperforms SOTA methods in both retrieval precision and recall. Consequently, an autoformalizer equipped with DDR shows consistent performance advantages in both single-attempt accuracy and multi-attempt stability compared to models using traditional selection-based RAG methods.

</details>


### [22] [Look As You Think: Unifying Reasoning and Visual Evidence Attribution for Verifiable Document RAG via Reinforcement Learning](https://arxiv.org/abs/2511.12003)
*Shuochen Liu,Pengfei Luo,Chao Zhang,Yuhao Chen,Haotian Zhang,Qi Liu,Xin Kou,Tong Xu,Enhong Chen*

Main category: cs.AI

TL;DR: 本文提出Chain-of-Evidence (CoE)范式和Look As You Think (LAT)强化学习框架用于视觉文档检索增强生成，实验表明LAT能提升模型性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉证据归因方法缺乏细粒度监督和推理过程的渐进可追溯性，本文旨在解决该问题以实现可靠可验证的视觉语言模型预测。

Method: 引入CoE范式统一思维链推理和视觉证据归因，提出LAT强化学习框架训练模型生成可验证推理路径。

Result: 在Paper-和Wiki-VISA基准上，LAT使Qwen2.5-VL-7B-Instruct模型在单图像和多图像设置下的软精确匹配平均提升8.23%，IoU@0.5平均提升47.0%，且优于监督微调基线。

Conclusion: LAT能有效提升视觉语言模型在视觉文档检索增强生成中的性能，且具有更强的跨领域泛化能力。

Abstract: Aiming to identify precise evidence sources from visual documents, visual evidence attribution for visual document retrieval-augmented generation (VD-RAG) ensures reliable and verifiable predictions from vision-language models (VLMs) in multimodal question answering. Most existing methods adopt end-to-end training to facilitate intuitive answer verification. However, they lack fine-grained supervision and progressive traceability throughout the reasoning process. In this paper, we introduce the Chain-of-Evidence (CoE) paradigm for VD-RAG. CoE unifies Chain-of-Thought (CoT) reasoning and visual evidence attribution by grounding reference elements in reasoning steps to specific regions with bounding boxes and page indexes. To enable VLMs to generate such evidence-grounded reasoning, we propose Look As You Think (LAT), a reinforcement learning framework that trains models to produce verifiable reasoning paths with consistent attribution. During training, LAT evaluates the attribution consistency of each evidence region and provides rewards only when the CoE trajectory yields correct answers, encouraging process-level self-verification. Experiments on vanilla Qwen2.5-VL-7B-Instruct with Paper- and Wiki-VISA benchmarks show that LAT consistently improves the vanilla model in both single- and multi-image settings, yielding average gains of 8.23% in soft exact match (EM) and 47.0% in IoU@0.5. Meanwhile, LAT not only outperforms the supervised fine-tuning baseline, which is trained to directly produce answers with attribution, but also exhibits stronger generalization across domains.

</details>


### [23] [Adaptive Diagnostic Reasoning Framework for Pathology with Multimodal Large Language Models](https://arxiv.org/abs/2511.12008)
*Yunqi Hong,Johnson Kao,Liam Edwards,Nein-Tzu Liu,Chung-Yen Huang,Alex Oliveira-Kowaleski,Cho-Jui Hsieh,Neil Y. C. Lin*

Main category: cs.AI

TL;DR: 提出可解释框架RECAP - PATH，实现自学习范式，在癌症诊断中表现良好，提供可信AI。


<details>
  <summary>Details</summary>
Motivation: 当前AI病理工具因缺乏人类可读推理，应用受限，需改进。

Method: 提出RECAP - PATH框架，采用两阶段学习过程自主推导诊断标准。

Result: 在乳腺和前列腺数据集上，RECAP - PATH的诊断理由与专家评估一致，诊断准确性大幅提高。

Conclusion: RECAP - PATH结合视觉理解与推理，提供临床可信AI，是实现证据关联解释的可行路径。

Abstract: AI tools in pathology have improved screening throughput, standardized quantification, and revealed prognostic patterns that inform treatment. However, adoption remains limited because most systems still lack the human-readable reasoning needed to audit decisions and prevent errors. We present RECAP-PATH, an interpretable framework that establishes a self-learning paradigm, shifting off-the-shelf multimodal large language models from passive pattern recognition to evidence-linked diagnostic reasoning. At its core is a two-phase learning process that autonomously derives diagnostic criteria: diversification expands pathology-style explanations, while optimization refines them for accuracy. This self-learning approach requires only small labeled sets and no white-box access or weight updates to generate cancer diagnoses. Evaluated on breast and prostate datasets, RECAP-PATH produced rationales aligned with expert assessment and delivered substantial gains in diagnostic accuracy over baselines. By uniting visual understanding with reasoning, RECAP-PATH provides clinically trustworthy AI and demonstrates a generalizable path toward evidence-linked interpretation.

</details>


### [24] [Intelligent Collaborative Optimization for Rubber Tyre Film Production Based on Multi-path Differentiated Clipping Proximal Policy Optimization](https://arxiv.org/abs/2511.12060)
*Yinghao Ruan,Wei Pang,Shuaihao Liu,Huili Yang,Leyi Han,Xinghui Dong*

Main category: cs.AI

TL;DR: 本文针对橡胶轮胎行业调度和生产线配置问题，提出MPD - PPO算法，实验验证其能提升性能和生产稳定性。


<details>
  <summary>Details</summary>
Motivation: 智能制造业发展下，传统橡胶轮胎行业集中调度和生产线配置存在局限，且系统复杂，多子系统协调困难，需解决高维多目标优化问题。

Method: 引入深度强化学习算法Multi - path Differentiated Clipping Proximal Policy Optimization (MPD - PPO)，采用多分支策略架构和差异化梯度裁剪约束进行高维策略更新。

Result: 在橡胶轮胎胶片生产的宽度和厚度控制实验中，MPD - PPO在调整精度和运营效率上有显著提升。

Conclusion: 该框架能解决高维、多目标权衡和动态适应等关键挑战，可用于轮胎制造实时工业部署，提升性能和生产稳定性。

Abstract: The advent of smart manufacturing is addressing the limitations of traditional centralized scheduling and inflexible production line configurations in the rubber tyre industry, especially in terms of coping with dynamic production demands. Contemporary tyre manufacturing systems form complex networks of tightly coupled subsystems pronounced nonlinear interactions and emergent dynamics. This complexity renders the effective coordination of multiple subsystems, posing an essential yet formidable task. For high-dimensional, multi-objective optimization problems in this domain, we introduce a deep reinforcement learning algorithm: Multi-path Differentiated Clipping Proximal Policy Optimization (MPD-PPO). This algorithm employs a multi-branch policy architecture with differentiated gradient clipping constraints to ensure stable and efficient high-dimensional policy updates. Validated through experiments on width and thickness control in rubber tyre film production, MPD-PPO demonstrates substantial improvements in both tuning accuracy and operational efficiency. The framework successfully tackles key challenges, including high dimensionality, multi-objective trade-offs, and dynamic adaptation, thus delivering enhanced performance and production stability for real-time industrial deployment in tyre manufacturing.

</details>


### [25] [Bayesian Optimization in Language Space: An Eval-Efficient AI Self-Improvement Framework](https://arxiv.org/abs/2511.12063)
*Enoch Hyunwook Kang,Hema Yoganarasimhan*

Main category: cs.AI

TL;DR: 本文针对大语言模型自改进中评估效率问题，提出T - BoN BO框架并验证其性能优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有自改进AI关注查询效率，而社会应用中评估新方案成本高、耗时长，需优化评估效率。

Method: 证明Best - of - N策略与文本梯度组合能模拟UCB采集函数梯度行为，提出T - BoN BO框架。

Result: 将T - BoN BO应用于自动广告对齐任务，性能优于流行的现有基线。

Conclusion: T - BoN BO是一种简单且评估高效的语言空间贝叶斯优化框架，可用于AI自改进。

Abstract: Large Language Models (LLMs) have recently enabled self-improving AI, i.e., AI that iteratively generates, evaluates, and refines its own outcomes. Recent studies have shown that self-improving AI focusing on prompt optimization can outperform state-of-the-art reinforcement-learning fine-tuned LLMs. Here, their `performance' is typically measured by query efficiency - the number of LLM-generated solution samples required to meet a certain performance threshold. However, in many societal applications, the primary limitation is not generating new solutions but evaluating them. For instance, evaluating an ad's effectiveness requires significant human feedback, which is far more costly and time-consuming than generating a candidate ad. To optimize for the evaluation efficiency objective, a natural approach is to extend Bayesian Optimization (BO), a framework proven optimal for evaluation efficiency, to the language domain. However, the difficulty of directly estimating suitable acquisition functions in LLMs' minds makes this extension challenging. This paper overcomes this challenge by proving that the combination of the simple and widely used Best-of-N selection strategy and simple textual gradients (i.e., textual edits from a critic model) statistically emulates the behavior of the gradients on the canonical UCB acquisition function, which induces optimal exploration in terms of evaluation efficiency. Based on this result, we propose TextGrad-Best-of-N Bayesian Optimization (T-BoN BO), a simple and eval-efficient language-space Bayesian optimization framework for AI self-improvement. We also empirically validate T-BoN BO by applying it to automated ad alignment tasks for persona distribution, demonstrating its superior performance compared to popular state-of-the-art baselines.

</details>


### [26] [No-Regret Strategy Solving in Imperfect-Information Games via Pre-Trained Embedding](https://arxiv.org/abs/2511.12083)
*Yanchang Fu,Shengda Liu,Pei Xu,Kaiqi Huang*

Main category: cs.AI

TL;DR: 本文提出Embedding CFR算法解决大规模不完美信息扩展式博弈策略求解问题，实验显示其在收敛速度上优于基于聚类的抽象算法。


<details>
  <summary>Details</summary>
Motivation: 现有AI方法在解决大规模不完美信息扩展式博弈时，基于离散聚类的抽象方法会不可逆地丢失关键信息，影响策略求解质量。

Method: 提出Embedding CFR算法，将孤立信息集特征预训练并嵌入到低维连续空间，在该空间中基于后悔累积和策略更新进行策略求解，并进行理论分析。

Result: 在相同空间开销下，Embedding CFR算法比基于聚类的抽象算法可实现更快的可利用性收敛。

Conclusion: Embedding CFR算法有效，是首个通过低维嵌入预训练信息集抽象进行策略求解的扑克AI算法。

Abstract: High-quality information set abstraction remains a core challenge in solving large-scale imperfect-information extensive-form games (IIEFGs)-such as no-limit Texas Hold'em-where the finite nature of spatial resources hinders strategy solving over the full game. State-of-the-art AI methods rely on pre-trained discrete clustering for abstraction, yet their hard classification irreversibly loses critical information: specifically, the quantifiable subtle differences between information sets-vital for strategy solving-thereby compromising the quality of such solving. Inspired by the word embedding paradigm in natural language processing, this paper proposes the Embedding CFR algorithm, a novel approach for solving strategies in IIEFGs within an embedding space. The algorithm pre-trains and embeds features of isolated information sets into an interconnected low-dimensional continuous space, where the resulting vectors more precisely capture both the distinctions and connections between information sets. Embedding CFR presents a strategy-solving process driven by regret accumulation and strategy updates within this embedding space, with accompanying theoretical analysis verifying its capacity to reduce cumulative regret. Experiments on poker show that with the same spatial overhead, Embedding CFR achieves significantly faster exploitability convergence compared to cluster-based abstraction algorithms, confirming its effectiveness. Furthermore, to our knowledge, it is the first algorithm in poker AI that pre-trains information set abstractions through low-dimensional embedding for strategy solving.

</details>


### [27] [Mobile-Agent-RAG: Driving Smart Multi-Agent Coordination with Contextual Knowledge Empowerment for Long-Horizon Mobile Automation](https://arxiv.org/abs/2511.12254)
*Yuxiang Zhou,Jichang Li,Yanhao Zhang,Haonan Lu,Guanbin Li*

Main category: cs.AI

TL;DR: 当前先进移动代理在长周期跨应用任务成功率不足，本文提出Mobile - Agent - RAG框架，引入双级检索增强，构建知识库，还推出评估基准，实验显示其显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有先进移动代理在长周期跨应用任务成功率低，原因是过度依赖MLLMs内静态知识，导致规划幻觉和执行错误，且高低层任务所需知识不同。

Method: 提出Mobile - Agent - RAG分层多智能体框架，规划阶段用Manager - RAG减少战略幻觉，执行阶段用Operator - RAG提高执行准确性，构建两个检索型知识库，推出Mobile - Eval - RAG评估基准。

Result: Mobile - Agent - RAG显著优于现有基线，任务完成率提高11.0%，步骤效率提高10.2%。

Conclusion: Mobile - Agent - RAG为上下文感知、可靠的多智能体移动自动化建立了强大范式。

Abstract: Mobile agents show immense potential, yet current state-of-the-art (SoTA) agents exhibit inadequate success rates on real-world, long-horizon, cross-application tasks. We attribute this bottleneck to the agents' excessive reliance on static, internal knowledge within MLLMs, which leads to two critical failure points: 1) strategic hallucinations in high-level planning and 2) operational errors during low-level execution on user interfaces (UI). The core insight of this paper is that high-level planning and low-level UI operations require fundamentally distinct types of knowledge. Planning demands high-level, strategy-oriented experiences, whereas operations necessitate low-level, precise instructions closely tied to specific app UIs. Motivated by these insights, we propose Mobile-Agent-RAG, a novel hierarchical multi-agent framework that innovatively integrates dual-level retrieval augmentation. At the planning stage, we introduce Manager-RAG to reduce strategic hallucinations by retrieving human-validated comprehensive task plans that provide high-level guidance. At the execution stage, we develop Operator-RAG to improve execution accuracy by retrieving the most precise low-level guidance for accurate atomic actions, aligned with the current app and subtask. To accurately deliver these knowledge types, we construct two specialized retrieval-oriented knowledge bases. Furthermore, we introduce Mobile-Eval-RAG, a challenging benchmark for evaluating such agents on realistic multi-app, long-horizon tasks. Extensive experiments demonstrate that Mobile-Agent-RAG significantly outperforms SoTA baselines, improving task completion rate by 11.0% and step efficiency by 10.2%, establishing a robust paradigm for context-aware, reliable multi-agent mobile automation.

</details>


### [28] [MetaGDPO: Alleviating Catastrophic Forgetting with Metacognitive Knowledge through Group Direct Preference Optimization](https://arxiv.org/abs/2511.12113)
*Lanxue Zhang,Yuqiang Xie,Fang Fang,Fanglong Dong,Rui Liu,Yanan Cao*

Main category: cs.AI

TL;DR: 提出综合方案缓解小模型灾难性遗忘并提升推理性能


<details>
  <summary>Details</summary>
Motivation: 现有数据集和微调方法在小模型上会导致灾难性遗忘，因忽视训练数据知识与模型固有能力关系、缺乏对固有知识保存的约束

Method: 数据方面构建含元认知知识的5K实例数据集并进行数据筛选；训练方面引入GDPO优化方法

Result: 实验表明该方法显著缓解灾难性遗忘，提升小模型推理性能

Conclusion: 所提综合方案有效解决小模型灾难性遗忘问题，提升推理性能

Abstract: Large Language Models demonstrate strong reasoning capabilities, which can be effectively compressed into smaller models. However, existing datasets and fine-tuning approaches still face challenges that lead to catastrophic forgetting, particularly for models smaller than 8B. First, most datasets typically ignore the relationship between training data knowledge and the model's inherent abilities, making it difficult to preserve prior knowledge. Second, conventional training objectives often fail to constrain inherent knowledge preservation, which can result in forgetting of previously learned skills. To address these issues, we propose a comprehensive solution that alleviates catastrophic forgetting from both the data and fine-tuning approach perspectives. On the data side, we construct a dataset of 5K instances that covers multiple reasoning tasks and incorporates metacognitive knowledge, making it more tolerant and effective for distillation into smaller models. We annotate the metacognitive knowledge required to solve each question and filter the data based on task knowledge and the model's inherent skills. On the training side, we introduce GDPO (Group Direction Preference Optimization), which is better suited for resource-limited scenarios and can efficiently approximate the performance of GRPO. Guided by the large model and by implicitly constraining the optimization path through a reference model, GDPO enables more effective knowledge transfer from the large model and constrains excessive parameter drift. Extensive experiments demonstrate that our approach significantly alleviates catastrophic forgetting and improves reasoning performance on smaller models.

</details>


### [29] [RTMol: Rethinking Molecule-text Alignment in a Round-trip View](https://arxiv.org/abs/2511.12135)
*Letian Chen,Runhan Shi,Gufeng Yu,Yang Yang*

Main category: cs.AI

TL;DR: 提出RTMol框架解决分子序列表示与文本描述对齐问题，提升双向对齐性能。


<details>
  <summary>Details</summary>
Motivation: 现有分子字幕和文本到分子设计方法有评估指标重语言轻化学、训练数据有歧义、双向不一致等局限。

Method: 提出RTMol双向对齐框架，通过自监督往返学习统一分子字幕和文本到SMILES生成，引入往返评估指标，支持无配对语料的无监督训练。

Result: RTMol在各种大语言模型上使双向对齐性能最多提升47%。

Conclusion: RTMol为分子 - 文本联合理解和生成建立了有效范式。

Abstract: Aligning molecular sequence representations (e.g., SMILES notations) with textual descriptions is critical for applications spanning drug discovery, materials design, and automated chemical literature analysis. Existing methodologies typically treat molecular captioning (molecule-to-text) and text-based molecular design (text-to-molecule) as separate tasks, relying on supervised fine-tuning or contrastive learning pipelines. These approaches face three key limitations: (i) conventional metrics like BLEU prioritize linguistic fluency over chemical accuracy, (ii) training datasets frequently contain chemically ambiguous narratives with incomplete specifications, and (iii) independent optimization of generation directions leads to bidirectional inconsistency. To address these issues, we propose RTMol, a bidirectional alignment framework that unifies molecular captioning and text-to-SMILES generation through self-supervised round-trip learning. The framework introduces novel round-trip evaluation metrics and enables unsupervised training for molecular captioning without requiring paired molecule-text corpora. Experiments demonstrate that RTMol enhances bidirectional alignment performance by up to 47% across various LLMs, establishing an effective paradigm for joint molecule-text understanding and generation.

</details>


### [30] [Incremental Maintenance of DatalogMTL Materialisations](https://arxiv.org/abs/2511.12169)
*Kaiyue Zhao,Dingqi Chen,Shaoyu Wang,Pan Hu*

Main category: cs.AI

TL;DR: 提出用于有界区间DatalogMTL的增量推理算法DRedMTL，实验显示其性能常显著优于重新实例化。


<details>
  <summary>Details</summary>
Motivation: 现有DatalogMTL推理方法缺乏对高效动态更新的支持，而现实应用常需频繁更新数据。

Method: 基于经典DRed算法，设计特定算子处理DatalogMTL实例化的周期性表示。

Result: 在多个公开数据集上测试，DRedMTL性能常显著优于重新实例化，有时提升多个数量级。

Conclusion: DRedMTL算法能有效解决DatalogMTL在动态更新方面的问题，提升推理效率。

Abstract: DatalogMTL extends the classical Datalog language with metric temporal logic (MTL), enabling expressive reasoning over temporal data. While existing reasoning approaches, such as materialisation based and automata based methods, offer soundness and completeness, they lack support for handling efficient dynamic updates, a crucial requirement for real-world applications that involve frequent data updates. In this work, we propose DRedMTL, an incremental reasoning algorithm for DatalogMTL with bounded intervals. Our algorithm builds upon the classical DRed algorithm, which incrementally updates the materialisation of a Datalog program. Unlike a Datalog materialisation which is in essence a finite set of facts, a DatalogMTL materialisation has to be represented as a finite set of facts plus periodic intervals indicating how the full materialisation can be constructed through unfolding. To cope with this, our algorithm is equipped with specifically designed operators to efficiently handle such periodic representations of DatalogMTL materialisations. We have implemented this approach and tested it on several publicly available datasets. Experimental results show that DRedMTL often significantly outperforms rematerialisation, sometimes by orders of magnitude.

</details>


### [31] [Debate over Mixed-knowledge: A Robust Multi-Agent Framework for Incomplete Knowledge Graph Question Answering](https://arxiv.org/abs/2511.12208)
*Jilong Liu,Pengyang Shao,Wei Qin,Fei Liu,Yonghui Yang,Richang Hong*

Main category: cs.AI

TL;DR: 提出DoM框架用于IKGQA，结合多源知识，还构建新数据集，实验表明DoM优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有IKGQA方法不能自适应融合多源知识，且现有数据集不能反映真实知识不完整性。

Method: 提出DoM框架，基于多智能体辩论范式，用不同智能体处理不同知识源；构建新数据集Incomplete Knowledge Graph WebQuestions。

Result: DoM在实验中始终优于现有基线。

Conclusion: DoM框架和新数据集能有效解决IKGQA问题，提高性能。

Abstract: Knowledge Graph Question Answering (KGQA) aims to improve factual accuracy by leveraging structured knowledge. However, real-world Knowledge Graphs (KGs) are often incomplete, leading to the problem of Incomplete KGQA (IKGQA). A common solution is to incorporate external data to fill knowledge gaps, but existing methods lack the capacity to adaptively and contextually fuse multiple sources, failing to fully exploit their complementary strengths. To this end, we propose Debate over Mixed-knowledge (DoM), a novel framework that enables dynamic integration of structured and unstructured knowledge for IKGQA. Built upon the Multi-Agent Debate paradigm, DoM assigns specialized agents to perform inference over knowledge graphs and external texts separately, and coordinates their outputs through iterative interaction. It decomposes the input question into sub-questions, retrieves evidence via dual agents (KG and Retrieval-Augmented Generation, RAG), and employs a judge agent to evaluate and aggregate intermediate answers. This collaboration exploits knowledge complementarity and enhances robustness to KG incompleteness. In addition, existing IKGQA datasets simulate incompleteness by randomly removing triples, failing to capture the irregular and unpredictable nature of real-world knowledge incompleteness. To address this, we introduce a new dataset, Incomplete Knowledge Graph WebQuestions, constructed by leveraging real-world knowledge updates. These updates reflect knowledge beyond the static scope of KGs, yielding a more realistic and challenging benchmark. Through extensive experiments, we show that DoM consistently outperforms state-of-the-art baselines.

</details>


### [32] [ViTE: Virtual Graph Trajectory Expert Router for Pedestrian Trajectory Prediction](https://arxiv.org/abs/2511.12214)
*Ruochen Li,Zhanxing Zhu,Tanqiu Qiao,Hubert P. H. Shum*

Main category: cs.AI

TL;DR: 提出用于行人轨迹预测的ViTE框架，结合虚拟图和专家路由器，在多个基准测试中达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于堆叠GNN层的行人轨迹预测方法存在计算成本高或感受野不足问题，需自适应建模单跳和高阶依赖。

Method: 提出ViTE框架，含引入动态虚拟节点的虚拟图和基于社会上下文自适应选择专家的专家路由器。

Result: 在ETH/UCY、NBA和SDD三个基准测试中取得了最先进的性能。

Conclusion: ViTE框架有效且具有实际效率。

Abstract: Pedestrian trajectory prediction is critical for ensuring safety in autonomous driving, surveillance systems, and urban planning applications. While early approaches primarily focus on one-hop pairwise relationships, recent studies attempt to capture high-order interactions by stacking multiple Graph Neural Network (GNN) layers. However, these approaches face a fundamental trade-off: insufficient layers may lead to under-reaching problems that limit the model's receptive field, while excessive depth can result in prohibitive computational costs. We argue that an effective model should be capable of adaptively modeling both explicit one-hop interactions and implicit high-order dependencies, rather than relying solely on architectural depth. To this end, we propose ViTE (Virtual graph Trajectory Expert router), a novel framework for pedestrian trajectory prediction. ViTE consists of two key modules: a Virtual Graph that introduces dynamic virtual nodes to model long-range and high-order interactions without deep GNN stacks, and an Expert Router that adaptively selects interaction experts based on social context using a Mixture-of-Experts design. This combination enables flexible and scalable reasoning across varying interaction patterns. Experiments on three benchmarks (ETH/UCY, NBA, and SDD) demonstrate that our method consistently achieves state-of-the-art performance, validating both its effectiveness and practical efficiency.

</details>


### [33] [Beyond World Models: Rethinking Understanding in AI Models](https://arxiv.org/abs/2511.12239)
*Tarun Gupta,Danish Pruthi*

Main category: cs.AI

TL;DR: 本文用科学哲学文献案例研究，探讨世界模型框架能否充分表征人类水平理解，探索其局限性。


<details>
  <summary>Details</summary>
Motivation: 人类拥有心理世界模型，若AI模型有类似表征，或表明其能像人类一样‘理解’世界，所以研究世界模型框架能否表征人类水平理解。

Method: 运用科学哲学文献中的案例研究，聚焦世界模型能力与人类理解差异最明显的哲学分析。

Result: 未提及具体结果。

Conclusion: 未提及明确结论，研究目的是探索世界模型的局限性。

Abstract: World models have garnered substantial interest in the AI community. These are internal representations that simulate aspects of the external world, track entities and states, capture causal relationships, and enable prediction of consequences. This contrasts with representations based solely on statistical correlations. A key motivation behind this research direction is that humans possess such mental world models, and finding evidence of similar representations in AI models might indicate that these models "understand" the world in a human-like way. In this paper, we use case studies from the philosophy of science literature to critically examine whether the world model framework adequately characterizes human-level understanding. We focus on specific philosophical analyses where the distinction between world model capabilities and human understanding is most pronounced. While these represent particular views of understanding rather than universal definitions, they help us explore the limits of world models.

</details>


### [34] [AURA: Development and Validation of an Augmented Unplanned Removal Alert System using Synthetic ICU Videos](https://arxiv.org/abs/2511.12241)
*Junhyuk Seo,Hyeyoon Moon,Kyu-Hwan Jung,Namkee Oh,Taerim Kim*

Main category: cs.AI

TL;DR: 提出基于合成视频数据集的AURA系统检测非计划拔管风险，有一定效果并展示新开发路径。


<details>
  <summary>Details</summary>
Motivation: 非计划拔管是ICU患者安全关键问题，实时检测因伦理和隐私挑战受限。

Method: 利用文本到视频扩散生成合成视频数据集，应用姿态估计识别碰撞和躁动两种高风险运动模式。

Result: 专家确认合成数据真实性，碰撞检测精度高，躁动识别表现中等。

Conclusion: 展示了开发隐私保护、可重复的患者安全监测系统的新途径，有在ICU部署潜力。

Abstract: Unplanned extubation (UE) remains a critical patient safety concern in intensive care units (ICUs), often leading to severe complications or death. Real-time UE detection has been limited, largely due to the ethical and privacy challenges of obtaining annotated ICU video data. We propose Augmented Unplanned Removal Alert (AURA), a vision-based risk detection system developed and validated entirely on a fully synthetic video dataset. By leveraging text-to-video diffusion, we generated diverse and clinically realistic ICU scenarios capturing a range of patient behaviors and care contexts. The system applies pose estimation to identify two high-risk movement patterns: collision, defined as hand entry into spatial zones near airway tubes, and agitation, quantified by the velocity of tracked anatomical keypoints. Expert assessments confirmed the realism of the synthetic data, and performance evaluations showed high accuracy for collision detection and moderate performance for agitation recognition. This work demonstrates a novel pathway for developing privacy-preserving, reproducible patient safety monitoring systems with potential for deployment in intensive care settings.

</details>


### [35] [MoralReason: Generalizable Moral Decision Alignment For LLM Agents Using Reasoning-Level Reinforcement Learning](https://arxiv.org/abs/2511.12271)
*Zhiyu An,Wan Du*

Main category: cs.AI

TL;DR: 本文提出分布外道德对齐问题，引入Moral - Reason - QA数据集，采用特定学习方法训练大语言模型，实验证明模型能泛化到未见道德场景，为AI安全提供基础。


<details>
  <summary>Details</summary>
Motivation: 当前方法主要评估大语言模型道德决策，缺乏主动引导，需解决分布外道德对齐问题。

Method: 引入Moral - Reason - QA数据集，采用Group Relative Policy Optimization和复合奖励优化决策对齐和推理过程。

Result: 模型成功泛化到未见道德场景，功利主义和义务论框架对齐分数分别提高+0.757和+0.450，实验揭示训练挑战和研究方向。

Conclusion: 大语言模型可系统训练以应用特定道德框架到新情况，为AI安全提供关键基础。

Abstract: Large language models are increasingly influencing human moral decisions, yet current approaches focus primarily on evaluating rather than actively steering their moral decisions. We formulate this as an out-of-distribution moral alignment problem, where LLM agents must learn to apply consistent moral reasoning frameworks to scenarios beyond their training distribution. We introduce Moral-Reason-QA, a novel dataset extending 680 human-annotated, high-ambiguity moral scenarios with framework-specific reasoning traces across utilitarian, deontological, and virtue ethics, enabling systematic evaluation of moral generalization in realistic decision contexts. Our learning approach employs Group Relative Policy Optimization with composite rewards that simultaneously optimize decision alignment and framework-specific reasoning processes to facilitate learning of the underlying moral frameworks. Experimental results demonstrate successful generalization to unseen moral scenarios, with softmax-normalized alignment scores improving by +0.757 for utilitarian and +0.450 for deontological frameworks when tested on out-of-distribution evaluation sets. The experiments also reveal training challenges and promising directions that inform future research. These findings establish that LLM agents can be systematically trained to internalize and apply specific moral frameworks to novel situations, providing a critical foundation for AI safety as language models become more integrated into human decision-making processes.

</details>


### [36] [UpBench: A Dynamically Evolving Real-World Labor-Market Agentic Benchmark Framework Built for Human-Centric AI](https://arxiv.org/abs/2511.12306)
*Darvin Yi,Teng Liu,Mattie Terzolo,Lance Hasson,Ayan Sinh,Pablo Mendes,Andrew Rabinovich*

Main category: cs.AI

TL;DR: 提出基于真实工作的动态基准UpBench评估大语言模型智能体，以评估其在真实劳动市场中的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准存在局限性，无法有效评估大语言模型智能体在动态、有经济意义环境中的表现。

Method: 引入基于全球Upwork劳动力市场真实工作的动态基准UpBench，采用基于评分标准的评估框架，融入人类专业知识。

Result: 能够对模型的优缺点和指令遵循情况进行细粒度分析。

Conclusion: UpBench为在真实劳动市场环境中评估智能体系统提供了可扩展、以人为本的基础，有助于实现人机协作。

Abstract: As large language model (LLM) agents increasingly undertake digital work, reliable frameworks are needed to evaluate their real-world competence, adaptability, and capacity for human collaboration. Existing benchmarks remain largely static, synthetic, or domain-limited, providing limited insight into how agents perform in dynamic, economically meaningful environments. We introduce UpBench, a dynamically evolving benchmark grounded in real jobs drawn from the global Upwork labor marketplace. Each task corresponds to a verified client transaction, anchoring evaluation in genuine work activity and financial outcomes. UpBench employs a rubric-based evaluation framework, in which expert freelancers decompose each job into detailed, verifiable acceptance criteria and assess AI submissions with per-criterion feedback. This structure enables fine-grained analysis of model strengths, weaknesses, and instruction-following fidelity beyond binary pass/fail metrics. Human expertise is integrated throughout the data pipeline (from job curation and rubric construction to evaluation) ensuring fidelity to real professional standards and supporting research on human-AI collaboration. By regularly refreshing tasks to reflect the evolving nature of online work, UpBench provides a scalable, human-centered foundation for evaluating agentic systems in authentic labor-market contexts, offering a path toward a collaborative framework, where AI amplifies human capability through partnership rather than replacement.

</details>


### [37] [Reward and Guidance through Rubrics: Promoting Exploration to Improve Multi-Domain Reasoning](https://arxiv.org/abs/2511.12344)
*Baolong Bi,Shenghua Liu,Yiwei Wang,Siqian Tong,Lingrui Mei,Yuyao Ge,Yilong Xu,Jiafeng Guo,Xueqi Cheng*

Main category: cs.AI

TL;DR: 本文提出RGR - GRPO框架解决现有大语言模型强化学习推理方法局限，实验表明其性能优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型强化学习方法主要关注单领域且依赖纯在线框架，限制探索空间和推理性能。

Method: 提出RGR - GRPO，即利用评分规则提供细粒度奖励信号和离线指导的多领域推理强化学习框架。

Result: 在14个跨多领域基准测试中，RGR - GRPO始终优于仅依赖其他奖励方案或离线指导的强化学习方法，在多个任务上有平均提升，训练中熵波动稳定，pass@k性能优越。

Conclusion: RGR - GRPO能让大语言模型在探索更大解决方案空间时获得密集信息奖励，有效突破现有性能瓶颈。

Abstract: Recent advances in reinforcement learning (RL) have significantly improved the complex reasoning capabilities of large language models (LLMs). Despite these successes, existing methods mainly focus on single-domain RL (e.g., mathematics) with verifiable rewards (RLVR), and their reliance on purely online RL frameworks restricts the exploration space, thereby limiting reasoning performance. In this paper, we address these limitations by leveraging rubrics to provide both fine-grained reward signals and offline guidance. We propose $\textbf{RGR-GRPO}$ (Reward and Guidance through Rubrics), a rubric-driven RL framework for multi-domain reasoning. RGR-GRPO enables LLMs to receive dense and informative rewards while exploring a larger solution space during GRPO training. Extensive experiments across 14 benchmarks spanning multiple domains demonstrate that RGR-GRPO consistently outperforms RL methods that rely solely on alternative reward schemes or offline guidance. Compared with verifiable online RL baseline, RGR-GRPO achieves average improvements of +7.0%, +5.4%, +8.4%, and +6.6% on mathematics, physics, chemistry, and general reasoning tasks, respectively. Notably, RGR-GRPO maintains stable entropy fluctuations during off-policy training and achieves superior pass@k performance, reflecting sustained exploration and effective breakthrough beyond existing performance bottlenecks.

</details>


### [38] [More Than Irrational: Modeling Belief-Biased Agents](https://arxiv.org/abs/2511.12359)
*Yifan Zhu,Sammie Katt,Samuel Kaski*

Main category: cs.AI

TL;DR: 本文提出计算理性（CR）用户模型，用于认知受限且信念有偏差的智能体，提出在线推理方法并在导航任务验证，为开发自适应AI助手提供基础。


<details>
  <summary>Details</summary>
Motivation: 现有AI技术难以预测和推断用户或人类协作者的次优行为，此类行为常源于认知局限和有偏差的信念，需建模解决。

Method: 提出基于嵌套粒子滤波的高效在线推理方法，同时跟踪用户潜在信念状态并估计未知认知界限。

Result: 在导航任务模拟中，CR模型能生成与不同记忆容量对应的合理行为，推理方法能从有限观察中准确高效恢复真实认知界限。

Conclusion: 该方法为开发考虑用户记忆限制的自适应AI助手提供了原则性基础。

Abstract: Despite the explosive growth of AI and the technologies built upon it, predicting and inferring the sub-optimal behavior of users or human collaborators remains a critical challenge. In many cases, such behaviors are not a result of irrationality, but rather a rational decision made given inherent cognitive bounds and biased beliefs about the world. In this paper, we formally introduce a class of computational-rational (CR) user models for cognitively-bounded agents acting optimally under biased beliefs. The key novelty lies in explicitly modeling how a bounded memory process leads to a dynamically inconsistent and biased belief state and, consequently, sub-optimal sequential decision-making. We address the challenge of identifying the latent user-specific bound and inferring biased belief states from passive observations on the fly. We argue that for our formalized CR model family with an explicit and parameterized cognitive process, this challenge is tractable. To support our claim, we propose an efficient online inference method based on nested particle filtering that simultaneously tracks the user's latent belief state and estimates the unknown cognitive bound from a stream of observed actions. We validate our approach in a representative navigation task using memory decay as an example of a cognitive bound. With simulations, we show that (1) our CR model generates intuitively plausible behaviors corresponding to different levels of memory capacity, and (2) our inference method accurately and efficiently recovers the ground-truth cognitive bounds from limited observations ($\le 100$ steps). We further demonstrate how this approach provides a principled foundation for developing adaptive AI assistants, enabling adaptive assistance that accounts for the user's memory limitations.

</details>


### [39] [Learning to Trust: Bayesian Adaptation to Varying Suggester Reliability in Sequential Decision Making](https://arxiv.org/abs/2511.12378)
*Dylan M. Asmar,Mykel J. Kochenderfer*

Main category: cs.AI

TL;DR: 本文提出框架动态学习并适应部分可观测环境中建议者可靠性变化，实验证明其性能良好，为自适应人机协作奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有整合外部行动建议的方法假设建议者质量参数静态且已知，限制实际应用，需解决建议不确定性问题。

Method: 将建议者质量融入智能体信念表示，通过贝叶斯推理调整对建议的依赖；引入‘询问’动作，平衡信息获取收益和成本。

Result: 实验显示该框架在不同建议者质量下性能稳健，能适应可靠性变化，可策略性管理建议请求。

Conclusion: 该工作为不确定环境中解决建议不确定性问题、实现自适应人机协作提供基础。

Abstract: Autonomous agents operating in sequential decision-making tasks under uncertainty can benefit from external action suggestions, which provide valuable guidance but inherently vary in reliability. Existing methods for incorporating such advice typically assume static and known suggester quality parameters, limiting practical deployment. We introduce a framework that dynamically learns and adapts to varying suggester reliability in partially observable environments. First, we integrate suggester quality directly into the agent's belief representation, enabling agents to infer and adjust their reliance on suggestions through Bayesian inference over suggester types. Second, we introduce an explicit ``ask'' action allowing agents to strategically request suggestions at critical moments, balancing informational gains against acquisition costs. Experimental evaluation demonstrates robust performance across varying suggester qualities, adaptation to changing reliability, and strategic management of suggestion requests. This work provides a foundation for adaptive human-agent collaboration by addressing suggestion uncertainty in uncertain environments.

</details>


### [40] [Multi-agent Self-triage System with Medical Flowcharts](https://arxiv.org/abs/2511.12439)
*Yujia Liu,Sophia Yu,Hongyue Jin,Jessica Wen,Alexander Qian,Terrence Lee,Mattheus Ramsis,Gi Won Choi,Lianhui Qin,Xin Liu,Edward J. Wang*

Main category: cs.AI

TL;DR: 本文介绍基于临床验证流程图的对话式自我分诊系统，评估显示其有高准确率，证明AI辅助自我分诊可行。


<details>
  <summary>Details</summary>
Motivation: 在线健康资源和大语言模型在医疗决策中可靠性有限，需提升准确性、透明度和抗干扰性。

Method: 引入基于100个临床验证流程图的对话式自我分诊系统，采用多智能体框架，用合成数据集评估性能。

Result: 流程图检索前3准确率95.29%，流程图导航准确率99.10%。

Conclusion: 该方法证明了透明、准确和可推广的AI辅助自我分诊的可行性，可支持患者决策并提高医疗资源利用率。

Abstract: Online health resources and large language models (LLMs) are increasingly used as a first point of contact for medical decision-making, yet their reliability in healthcare remains limited by low accuracy, lack of transparency, and susceptibility to unverified information. We introduce a proof-of-concept conversational self-triage system that guides LLMs with 100 clinically validated flowcharts from the American Medical Association, providing a structured and auditable framework for patient decision support. The system leverages a multi-agent framework consisting of a retrieval agent, a decision agent, and a chat agent to identify the most relevant flowchart, interpret patient responses, and deliver personalized, patient-friendly recommendations, respectively. Performance was evaluated at scale using synthetic datasets of simulated conversations. The system achieved 95.29% top-3 accuracy in flowchart retrieval (N=2,000) and 99.10% accuracy in flowchart navigation across varied conversational styles and conditions (N=37,200). By combining the flexibility of free-text interaction with the rigor of standardized clinical protocols, this approach demonstrates the feasibility of transparent, accurate, and generalizable AI-assisted self-triage, with potential to support informed patient decision-making while improving healthcare resource utilization.

</details>


### [41] [ARCHE: A Novel Task to Evaluate LLMs on Latent Reasoning Chain Extraction](https://arxiv.org/abs/2511.12485)
*Pengze Li,Jiaqi Liu,Junchi Yu,Lihao Liu,Mingyu Ding,Wanli Ouyang,Shixiang Tang,Xi Chen*

Main category: cs.AI

TL;DR: 引入潜推理链提取任务ARCHE及ARCHE Bench基准，提出评估指标，评估发现当前大语言模型在科学论证推理能力上有差距。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在科学领域输出内容非结构化、不正式，难以判断其是否理解科学推理范式，需解决该问题。

Method: 引入ARCHE任务，让模型将复杂推理分解为标准推理范式组合，以推理逻辑树呈现；发布基于70篇自然通讯文章的ARCHE Bench基准；提出实体覆盖率和推理边准确率两个评估指标。

Result: 对10个领先大语言模型在ARCHE Bench上评估，发现模型在推理边准确率和实体覆盖率上存在权衡，都无法提取完整标准推理链。

Conclusion: 当前推理模型能力与科学论证所需严谨性存在巨大差距。

Abstract: Large language models (LLMs) are increasingly used in scientific domains. While they can produce reasoning-like content via methods such as chain-of-thought prompting, these outputs are typically unstructured and informal, obscuring whether models truly understand the fundamental reasoning paradigms that underpin scientific inference. To address this, we introduce a novel task named Latent Reasoning Chain Extraction (ARCHE), in which models must decompose complex reasoning arguments into combinations of standard reasoning paradigms in the form of a Reasoning Logic Tree (RLT). In RLT, all reasoning steps are explicitly categorized as one of three variants of Peirce's fundamental inference modes: deduction, induction, or abduction. To facilitate this task, we release ARCHE Bench, a new benchmark derived from 70 Nature Communications articles, including more than 1,900 references and 38,000 viewpoints. We propose two logic-aware evaluation metrics: Entity Coverage (EC) for content completeness and Reasoning Edge Accuracy (REA) for step-by-step logical validity. Evaluations on 10 leading LLMs on ARCHE Bench reveal that models exhibit a trade-off between REA and EC, and none are yet able to extract a complete and standard reasoning chain. These findings highlight a substantial gap between the abilities of current reasoning models and the rigor required for scientific argumentation.

</details>


### [42] [LOBERT: Generative AI Foundation Model for Limit Order Book Messages](https://arxiv.org/abs/2511.12563)
*Eljas Linna,Kestutis Baltakys,Alexandros Iosifidis,Juho Kanniainen*

Main category: cs.AI

TL;DR: 提出适用于金融限价订单簿（LOB）数据的通用基础模型LOBERT，性能领先且减少上下文长度。


<details>
  <summary>Details</summary>
Motivation: 以往LOB模型数据表示繁琐且缺乏适应性，需新模型。

Method: 采用新颖的分词方案，将完整的多维消息视为单个标记，同时保留价格、成交量和时间的连续表示。

Result: 在预测中间价格走势和下一条消息等任务中取得领先性能，减少所需上下文长度。

Conclusion: LOBERT是适用于LOB数据的有效基础模型，适合下游微调。

Abstract: Modeling the dynamics of financial Limit Order Books (LOB) at the message level is challenging due to irregular event timing, rapid regime shifts, and the reactions of high-frequency traders to visible order flow. Previous LOB models require cumbersome data representations and lack adaptability outside their original tasks, leading us to introduce LOBERT, a general-purpose encoder-only foundation model for LOB data suitable for downstream fine-tuning. LOBERT adapts the original BERT architecture for LOB data by using a novel tokenization scheme that treats complete multi-dimensional messages as single tokens while retaining continuous representations of price, volume, and time. With these methods, LOBERT achieves leading performance in tasks such as predicting mid-price movements and next messages, while reducing the required context length compared to previous methods.

</details>


### [43] [Enhancing Conversational Recommender Systems with Tree-Structured Knowledge and Pretrained Language Models](https://arxiv.org/abs/2511.12579)
*Yongwen Ren,Chao Wang,Peng Du,Chuan Qin,Dazhong Shen,Hui Xiong*

Main category: cs.AI

TL;DR: 本文提出PCRS - TKA框架集成预训练语言模型和知识图谱以提升对话推荐系统性能，实验显示其表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有集成预训练语言模型和知识图谱的对话推荐系统存在未充分利用图关系推理、未过滤知识及忽视多轮对话协作偏好等问题，需进一步改进。

Method: 提出PCRS - TKA框架，从知识图谱构建对话特定知识树并序列化，选择性过滤知识，用特殊监督信号建模协作偏好，用语义对齐模块处理异构输入。

Result: PCRS - TKA在推荐和对话质量上始终优于所有基线。

Conclusion: PCRS - TKA能有效提升对话推荐系统的准确性和对话质量。

Abstract: Recent advances in pretrained language models (PLMs) have significantly improved conversational recommender systems (CRS), enabling more fluent and context-aware interactions. To further enhance accuracy and mitigate hallucination, many methods integrate PLMs with knowledge graphs (KGs), but face key challenges: failing to fully exploit PLM reasoning over graph relationships, indiscriminately incorporating retrieved knowledge without context filtering, and neglecting collaborative preferences in multi-turn dialogues. To this end, we propose PCRS-TKA, a prompt-based framework employing retrieval-augmented generation to integrate PLMs with KGs. PCRS-TKA constructs dialogue-specific knowledge trees from KGs and serializes them into texts, enabling structure-aware reasoning while capturing rich entity semantics. Our approach selectively filters context-relevant knowledge and explicitly models collaborative preferences using specialized supervision signals. A semantic alignment module harmonizes heterogeneous inputs, reducing noise and enhancing accuracy. Extensive experiments demonstrate that PCRS-TKA consistently outperforms all baselines in both recommendation and conversational quality.

</details>


### [44] [Dynamic Tree Databases in Automated Planning](https://arxiv.org/abs/2511.12677)
*Oliver Joergensen,Dominik Drexler,Jendrik Seipp*

Main category: cs.AI

TL;DR: 提出树数据库动态变体压缩状态集，实证显示有高压缩比和低运行开销


<details>
  <summary>Details</summary>
Motivation: 解决大规模任务显式状态空间搜索中生成状态集紧凑表示的挑战，现有树数据库需大量预分配内存

Method: 提出树数据库的新型动态变体来压缩命题和数值变量的状态集

Result: 在经典和数值规划任务的实证评估中，压缩比达几个数量级，运行时开销通常可忽略不计

Conclusion: 新型动态变体保持了静态树数据库的理想特性，在状态压缩方面表现良好

Abstract: A central challenge in scaling up explicit state-space search for large tasks is compactly representing the set of generated states. Tree databases, a data structure from model checking, require constant space per generated state in the best case, but they need a large preallocation of memory. We propose a novel dynamic variant of tree databases for compressing state sets over propositional and numeric variables and prove that it maintains the desirable properties of the static counterpart. Our empirical evaluation of state compression techniques for grounded and lifted planning on classical and numeric planning tasks reveals compression ratios of several orders of magnitude, often with negligible runtime overhead.

</details>


### [45] [Adaptively Coordinating with Novel Partners via Learned Latent Strategies](https://arxiv.org/abs/2511.12754)
*Benjamin Li,Shuyang Shi,Lucia Romero,Huao Li,Yaqi Xie,Woojun Kim,Stefanos Nikolaidis,Michael Lewis,Katia Sycara,Simon Stepputtis*

Main category: cs.AI

TL;DR: 本文提出策略条件合作者框架，用于人机团队中智能体实时适应人类伙伴，在Overcooked领域实验显示其性能优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 在人机团队中，人工智能体需实时适应人类伙伴，在有时间压力和复杂战略空间的任务中，识别伙伴行为和选择合适响应颇具挑战。

Method: 引入策略条件合作者框架，用变分自编码器编码策略学习潜在策略空间，通过聚类识别不同策略类型，训练基于聚类的合作者智能体；使用固定份额后悔最小化算法进行在线适应。

Result: 在修改版Overcooked领域实验及在线用户研究中，所提智能体与新的人类和智能体队友配对时达到了现有最优性能。

Conclusion: 所提出的策略条件合作者框架在人机协作实时适应方面有效，性能优于现有方法。

Abstract: Adaptation is the cornerstone of effective collaboration among heterogeneous team members. In human-agent teams, artificial agents need to adapt to their human partners in real time, as individuals often have unique preferences and policies that may change dynamically throughout interactions. This becomes particularly challenging in tasks with time pressure and complex strategic spaces, where identifying partner behaviors and selecting suitable responses is difficult. In this work, we introduce a strategy-conditioned cooperator framework that learns to represent, categorize, and adapt to a broad range of potential partner strategies in real-time. Our approach encodes strategies with a variational autoencoder to learn a latent strategy space from agent trajectory data, identifies distinct strategy types through clustering, and trains a cooperator agent conditioned on these clusters by generating partners of each strategy type. For online adaptation to novel partners, we leverage a fixed-share regret minimization algorithm that dynamically infers and adjusts the partner's strategy estimation during interaction. We evaluate our method in a modified version of the Overcooked domain, a complex collaborative cooking environment that requires effective coordination among two players with a diverse potential strategy space. Through these experiments and an online user study, we demonstrate that our proposed agent achieves state of the art performance compared to existing baselines when paired with novel human, and agent teammates.

</details>


### [46] [Optimal Foraging in Memory Retrieval: Evaluating Random Walks and Metropolis-Hastings Sampling in Modern Semantic Spaces](https://arxiv.org/abs/2511.12759)
*James Moore*

Main category: cs.AI

TL;DR: 研究发现高维嵌入空间随机游走结果符合最优觅食和MVT，复杂采样机制未必带来更好记忆检索认知模型，合适嵌入结构搭配简单采样即可。


<details>
  <summary>Details</summary>
Motivation: 探究现代高维嵌入空间能否使算法匹配人类语义流畅性任务中的觅食行为。

Method: 利用最先进的嵌入和先前语义流畅性数据，进行嵌入空间随机游走和引入Metropolis - Hastings采样。

Result: 嵌入空间随机游走结果符合最优觅食和MVT，Metropolis - Hastings采样结果与人类行为不一致。

Conclusion: 复杂采样机制并非必然带来更好的记忆检索认知模型，合适的嵌入结构搭配简单采样可产生近最优觅食动态，现代嵌入能近似人类记忆觅食。

Abstract: Human memory retrieval often resembles ecological foraging where animals search for food in a patchy environment. Optimal foraging means following the Marginal Value Theorem (MVT), in which individuals exploit a patch of semantically related concepts until it becomes less rewarding and then switch to a new cluster. While human behavioral data suggests foraging-like patterns in semantic fluency tasks, it remains unclear whether modern high-dimensional embedding spaces provide representations that allow algorithms to match observed human behavior. Using state-of-the-art embeddings and prior semantic fluency data, I find that random walks on these embedding spaces produce results consistent with optimal foraging and the MVT. Surprisingly, introducing Metropolis-Hastings sampling, an adaptive algorithm expected to model strategic acceptance and rejection of new clusters, does not produce results consistent with human behavior. These findings challenge the assumption that more complex sampling mechanisms inherently lead to better cognitive models of memory retrieval. Instead, they show that appropriately structured embeddings, even with simple sampling, can produce near-optimal foraging dynamics. This supports the perspective of Hills (2012) rather than Abbott (2015), demonstrating that modern embeddings can approximate human memory foraging without relying on complex acceptance criteria.

</details>


### [47] [Event-CausNet: Unlocking Causal Knowledge from Text with Large Language Models for Reliable Spatio-Temporal Forecasting](https://arxiv.org/abs/2511.12769)
*Luyao Niu,Zepu Wang,Shuyi Guan,Yang Liu,Peng Sun*

Main category: cs.AI

TL;DR: 提出Event - CausNet框架解决时空图神经网络在非重复事件中可靠性低的问题，实验显示其性能好，有优势。


<details>
  <summary>Details</summary>
Motivation: 时空图神经网络在非重复事件（如事故）中可靠性差，因其基于相关性建模，新因果因素会使历史模式失效。

Method: 提出Event - CausNet框架，用大语言模型量化非结构化事件报告，构建因果知识库，通过新的因果注意力机制将知识注入双流GNN - LSTM网络以调整和增强预测。

Result: 在真实数据集上实验表明，Event - CausNet性能稳健，预测误差（MAE）最多降低35.87%，显著优于现有基线。

Conclusion: 该框架弥合了相关模型和因果推理的差距，更准确、可迁移，有重要可解释性，为关键干扰时的交通管理提供可靠基础。

Abstract: While spatio-temporal Graph Neural Networks (GNNs) excel at modeling recurring traffic patterns, their reliability plummets during non-recurring events like accidents. This failure occurs because GNNs are fundamentally correlational models, learning historical patterns that are invalidated by the new causal factors introduced during disruptions. To address this, we propose Event-CausNet, a framework that uses a Large Language Model to quantify unstructured event reports, builds a causal knowledge base by estimating average treatment effects, and injects this knowledge into a dual-stream GNN-LSTM network using a novel causal attention mechanism to adjust and enhance the forecast. Experiments on a real-world dataset demonstrate that Event-CausNet achieves robust performance, reducing prediction error (MAE) by up to 35.87%, significantly outperforming state-of-the-art baselines. Our framework bridges the gap between correlational models and causal reasoning, providing a solution that is more accurate and transferable, while also offering crucial interpretability, providing a more reliable foundation for real-world traffic management during critical disruptions.

</details>


### [48] [Multi-Agent Reinforcement Learning for Heterogeneous Satellite Cluster Resources Optimization](https://arxiv.org/abs/2511.12792)
*Mohamad A. Hady,Siyi Hu,Mahardhika Pratama,Zehong Cao,Ryszard Kowalczyk*

Main category: cs.AI

TL;DR: 本文使用强化学习研究异构卫星集群自主地球观测任务的资源优化，评估了多种多智能体强化学习算法，结果表明其能有效协调卫星，为未来研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法难以处理地球观测操作的实时、不确定和分散特性，因此使用强化学习和多智能体强化学习进行自适应决策。

Method: 从单卫星到多卫星场景系统地制定优化问题，使用Basilisk和BSK - RL框架构建近真实模拟环境，评估MAPPO、HAPPO和HATRPO等算法。

Result: 多智能体强化学习能使异构卫星有效协调，平衡成像性能和资源利用，减轻非平稳性和智能体间奖励耦合。

Conclusion: 研究为可扩展的自主卫星操作提供实用见解，为异构和动态条件下的智能地球观测任务规划研究奠定基础。

Abstract: This work investigates resource optimization in heterogeneous satellite clusters performing autonomous Earth Observation (EO) missions using Reinforcement Learning (RL). In the proposed setting, two optical satellites and one Synthetic Aperture Radar (SAR) satellite operate cooperatively in low Earth orbit to capture ground targets and manage their limited onboard resources efficiently. Traditional optimization methods struggle to handle the real-time, uncertain, and decentralized nature of EO operations, motivating the use of RL and Multi-Agent Reinforcement Learning (MARL) for adaptive decision-making. This study systematically formulates the optimization problem from single-satellite to multi-satellite scenarios, addressing key challenges including energy and memory constraints, partial observability, and agent heterogeneity arising from diverse payload capabilities. Using a near-realistic simulation environment built on the Basilisk and BSK-RL frameworks, we evaluate the performance and stability of state-of-the-art MARL algorithms such as MAPPO, HAPPO, and HATRPO. Results show that MARL enables effective coordination across heterogeneous satellites, balancing imaging performance and resource utilization while mitigating non-stationarity and inter-agent reward coupling. The findings provide practical insights into scalable, autonomous satellite operations and contribute a foundation for future research on intelligent EO mission planning under heterogeneous and dynamic conditions.

</details>


### [49] [Neuro-Logic Lifelong Learning](https://arxiv.org/abs/2511.12793)
*Bowen He,Xiaoan Xu,Alper Kamil Bozkurt,Vahid Tarokh,Juncheng Dong*

Main category: cs.AI

TL;DR: 本文研究终身学习归纳逻辑编程（ILP），引入组合框架，实验验证了该范式的可行性与优势，为神经符号AI的持续学习开辟新方向。


<details>
  <summary>Details</summary>
Motivation: 多数研究聚焦于为单个问题设计新网络架构，较少探索涉及一系列问题的新学习范式，本文旨在利用逻辑规则的组合性和可迁移性高效学习新问题。

Method: 引入组合框架，形式化该方法并在一系列任务上进行实证评估。

Result: 实验结果验证了终身学习ILP范式的可行性和优势。

Conclusion: 该范式为神经符号AI的持续学习开辟了新方向。

Abstract: Solving Inductive Logic Programming (ILP) problems with neural networks is a key challenge in Neural-Symbolic Ar- tificial Intelligence (AI). While most research has focused on designing novel network architectures for individual prob- lems, less effort has been devoted to exploring new learning paradigms involving a sequence of problems. In this work, we investigate lifelong learning ILP, which leverages the com- positional and transferable nature of logic rules for efficient learning of new problems. We introduce a compositional framework, demonstrating how logic rules acquired from ear- lier tasks can be efficiently reused in subsequent ones, leading to improved scalability and performance. We formalize our approach and empirically evaluate it on sequences of tasks. Experimental results validate the feasibility and advantages of this paradigm, opening new directions for continual learn- ing in Neural-Symbolic AI.

</details>


### [50] [Mapping fNIRS Signals to Agent Performance: Toward Reinforcement Learning from Neural Feedback](https://arxiv.org/abs/2511.12844)
*Julia Santaniello,Matthew Russell,Benson Jiang,Donatello Sassaroli,Robert Jacob,Jivko SInapov*

Main category: cs.AI

TL;DR: 本文提出用被动脑机接口（BCI）从隐式神经信号指导强化学习智能体训练的框架，发布fNIRS数据集，训练分类器和回归器评估智能体表现，还验证跨主体泛化，为脑驱动的强化学习系统奠定基础。


<details>
  <summary>Details</summary>
Motivation: 将人类反馈融入智能体训练过程，探索用隐式神经信号指导强化学习智能体训练。

Method: 提出使用被动BCI的框架，收集25名参与者在三个领域的fNIRS数据，训练分类器和回归器评估智能体表现，进行跨主体泛化评估和模型微调。

Result: 分类器二元分类平均F1分数67%，多类模型46%；微调预训练模型后，二元和多类模型平均F1分数分别提高17%和41%。

Conclusion: 将隐式fNIRS信号映射到智能体表现是可行且可改进的，为未来脑驱动的强化学习系统奠定基础。

Abstract: Reinforcement Learning from Human Feedback (RLHF) is a methodology that aligns agent behavior with human preferences by integrating human feedback into the agent's training process. We introduce a possible framework that employs passive Brain-Computer Interfaces (BCI) to guide agent training from implicit neural signals. We present and release a novel dataset of functional near-infrared spectroscopy (fNIRS) recordings collected from 25 human participants across three domains: a Pick-and-Place Robot, Lunar Lander, and Flappy Bird. We train classifiers to predict levels of agent performance (optimal, sub-optimal, or worst-case) from windows of preprocessed fNIRS feature vectors, achieving an average F1 score of 67% for binary classification and 46% for multi-class models averaged across conditions and domains. We also train regressors to predict the degree of deviation between an agent's chosen action and a set of near-optimal policies, providing a continuous measure of performance. We evaluate cross-subject generalization and demonstrate that fine-tuning pre-trained models with a small sample of subject-specific data increases average F1 scores by 17% and 41% for binary and multi-class models, respectively. Our work demonstrates that mapping implicit fNIRS signals to agent performance is feasible and can be improved, laying the foundation for future brain-driven RLHF systems.

</details>


### [51] [Bootstrapping LLMs via Preference-Based Policy Optimization](https://arxiv.org/abs/2511.12867)
*Chen Jia*

Main category: cs.AI

TL;DR: 提出基于偏好的策略优化（PbPO）框架，将学习过程视为策略与奖励模型的博弈，有理论保证，实验表现超现有技术。


<details>
  <summary>Details</summary>
Motivation: 在不依赖大量手动注释的情况下，使大语言模型行为与人类偏好对齐。

Method: 提出PbPO框架，将学习过程设为策略与奖励模型的博弈，用置信集约束奖励模型，迭代在线算法收集偏好数据。

Result: 为方法提供理论保证，建立高概率遗憾界，在五个基准测试中表现优于现有技术。

Conclusion: 所提PbPO框架在引导大语言模型方面有效。

Abstract: Bootstrapping large language models (LLMs) through preference-based policy optimization offers a promising direction for aligning model behavior with human preferences without relying on extensive manual annotations. In this work, we propose a novel preference-based policy optimization (PbPO) framework that formulates the learning process as a min-max game between the main policy and a reward model (RM). The RM is constrained within a confidence set derived from preference data to ensure reliable exploitation. Our iterative online algorithm actively collects preference data through guided exploration of the evolving policy, enabling continual self-improvement of both the policy and the RM. We provide theoretical guarantees for our method, establishing high-probability regret bounds for both settings with sequence-level RM and token-level RM, demonstrating its effectiveness in bootstrapping LLMs. Extensive experiments on five benchmarks show that our approach consistently outperforms existing state-of-the-art preference optimization techniques.

</details>


### [52] [Online Learning of HTN Methods for integrated LLM-HTN Planning](https://arxiv.org/abs/2511.12901)
*Yuesheng Xu,Hector Munoz-Avila*

Main category: cs.AI

TL;DR: 本文提出在集成HTN规划和基于LLM的聊天机器人中在线学习HTN方法，扩展ChatHTN，实验表明该在线学习程序减少对ChatGPT的调用并解决更多问题。


<details>
  <summary>Details</summary>
Motivation: 在集成HTN规划和基于LLM的聊天机器人背景下，学习HTN方法以优化任务分解。

Method: 在ChatHTN规划器基础上构建方法学习器，当ChatGPT生成任务分解时，学习泛化方法。

Result: 在两个领域实验表明，在线学习程序减少对ChatGPT的调用，解决问题数量相当甚至更多。

Conclusion: 所提出的在线学习HTN方法有效，能减少对ChatGPT的依赖并提升问题解决能力。

Abstract: We present online learning of Hierarchical Task Network (HTN) methods in the context of integrated HTN planning and LLM-based chatbots. Methods indicate when and how to decompose tasks into subtasks. Our method learner is built on top of the ChatHTN planner. ChatHTN queries ChatGPT to generate a decomposition of a task into primitive tasks when no applicable method for the task is available. In this work, we extend ChatHTN. Namely, when ChatGPT generates a task decomposition, ChatHTN learns from it, akin to memoization. However, unlike memoization, it learns a generalized method that applies not only to the specific instance encountered, but to other instances of the same task. We conduct experiments on two domains and demonstrate that our online learning procedure reduces the number of calls to ChatGPT while solving at least as many problems, and in some cases, even more.

</details>


### [53] [CoS: Towards Optimal Event Scheduling via Chain-of-Scheduling](https://arxiv.org/abs/2511.12913)
*Yiming Zhao,Jiwei Tang,Shimin Di,Libin Zheng,Jianxing Yu,Jian Yin*

Main category: cs.AI

TL;DR: 本文提出Chain - of - Scheduling (CoS)框架解决事件调度推荐问题，实验显示其高效且有效，有零样本学习能力。


<details>
  <summary>Details</summary>
Motivation: 为维持基于事件的社交网络（EBSNs）中用户活跃度，解决现有方法在效率、有效性和泛化性之间的权衡问题。

Method: 提出CoS框架，将调度任务分为探索、验证和集成三个原子阶段，通过知识蒸馏（KD）使大语言模型（LLMs）自主生成CoS。

Result: 在三个真实数据集上以可解释方式实现接近理论最优的有效性，在域外数据上有强零样本学习能力。

Conclusion: CoS框架能有效解决事件调度推荐问题，在效率、有效性和泛化性上表现良好。

Abstract: Recommending event schedules is a key issue in Event-based Social Networks (EBSNs) in order to maintain user activity. An effective recommendation is required to maximize the user's preference, subjecting to both time and geographical constraints. Existing methods face an inherent trade-off among efficiency, effectiveness, and generalization, due to the NP-hard nature of the problem. This paper proposes the Chain-of-Scheduling (CoS) framework, which activates the event scheduling capability of Large Language Models (LLMs) through a guided, efficient scheduling process. CoS enhances LLM by formulating the schedule task into three atomic stages, i.e., exploration, verification and integration. Then we enable the LLMs to generate CoS autonomously via Knowledge Distillation (KD). Experimental results show that CoS achieves near-theoretical optimal effectiveness with high efficiency on three real-world datasets in a interpretable manner. Moreover, it demonstrates strong zero-shot learning ability on out-of-domain data.

</details>


### [54] [Fault2Flow: An AlphaEvolve-Optimized Human-in-the-Loop Multi-Agent System for Fault-to-Workflow Automation](https://arxiv.org/abs/2511.12916)
*Yafang Wang,Yangjie Tian,Xiaoyu Shen,Gaoyang Zhang,Jiaze Sun,He Zhang,Ruohua Xu,Feng Zhao*

Main category: cs.AI

TL;DR: 电网故障诊断依赖人工易出错，提出基于大语言模型的多智能体系统Fault2Flow，实验验证有效且减少专家工作量。


<details>
  <summary>Details</summary>
Motivation: 现有电网故障诊断依赖人工方法，效率低、易出错且缺乏可维护性，且无框架整合法规逻辑和专家知识。

Method: 提出Fault2Flow系统，包括将法规逻辑提取成PASTA格式故障树、通过人机交互界面整合专家知识、用AlphaEvolve模块优化推理逻辑、合成n8n可执行工作流。

Result: 在变压器故障诊断数据集上实验，拓扑一致性达100%，语义保真度高。

Conclusion: Fault2Flow建立了从故障分析到操作自动化的可复制路径，大幅减少专家工作量。

Abstract: Power grid fault diagnosis is a critical process hindered by its reliance on manual, error-prone methods. Technicians must manually extract reasoning logic from dense regulations and attempt to combine it with tacit expert knowledge, which is inefficient, error-prone, and lacks maintainability as ragulations are updated and experience evolves. While Large Language Models (LLMs) have shown promise in parsing unstructured text, no existing framework integrates these two disparate knowledge sources into a single, verified, and executable workflow. To bridge this gap, we propose Fault2Flow, an LLM-based multi-agent system. Fault2Flow systematically: (1) extracts and structures regulatory logic into PASTA-formatted fault trees; (2) integrates expert knowledge via a human-in-the-loop interface for verification; (3) optimizes the reasoning logic using a novel AlphaEvolve module; and (4) synthesizes the final, verified logic into an n8n-executable workflow. Experimental validation on transformer fault diagnosis datasets confirms 100\% topological consistency and high semantic fidelity. Fault2Flow establishes a reproducible path from fault analysis to operational automation, substantially reducing expert workload.

</details>


### [55] [Yanyun-3: Enabling Cross-Platform Strategy Game Operation with Vision-Language Models](https://arxiv.org/abs/2511.12937)
*Guoyan Wang,Yanyan Huang,Chunlin Chen,Lifeng Wang,Yuxiang Sun*

Main category: cs.AI

TL;DR: 介绍Yanyun - 3框架实现跨平台策略游戏自动操作，评估多模态数据组合效果，混合策略表现优，为游戏自动化及VLM性能提升提供范式。


<details>
  <summary>Details</summary>
Motivation: 解决跨平台策略游戏自动化操作中，现有模型在复杂人机交互场景应用不足的问题。

Method: 整合Qwen2.5 - VL视觉语言推理和UI - TARS精确执行能力，采用屏幕捕获、模型推理和动作执行的闭环管道，通过消融实验评估多模态数据组合。

Result: 混合策略MV+S表现佳，推理时间减少63%，BLEU - 4分数提升约12.98倍。

Conclusion: 为策略游戏自动化提供高效方案，建立通过结构化多模态数据组织提升VLM性能的通用范式，为具身智能提供新见解。

Abstract: Automated operation in cross-platform strategy games demands agents with robust generalization across diverse user interfaces and dynamic battlefield conditions. While vision-language models (VLMs) have shown considerable promise in multimodal reasoning, their application to complex human-computer interaction scenarios--such as strategy gaming--remains largely unexplored. Here, we introduce Yanyun-3, a general-purpose agent framework that, for the first time, enables autonomous cross-platform operation across three heterogeneous strategy game environments. By integrating the vision-language reasoning of Qwen2.5-VL with the precise execution capabilities of UI-TARS, Yanyun-3 successfully performs core tasks including target localization, combat resource allocation, and area control. Through systematic ablation studies, we evaluate the effects of various multimodal data combinations--static images, multi-image sequences, and videos--and propose the concept of combination granularity to differentiate between intra-sample fusion and inter-sample mixing strategies. We find that a hybrid strategy, which fuses multi-image and video data while mixing in static images (MV+S), substantially outperforms full fusion: it reduces inference time by 63% and boosts the BLEU-4 score by a factor of 12 (from 4.81% to 62.41%, approximately 12.98x). Operating via a closed-loop pipeline of screen capture, model inference, and action execution, the agent demonstrates strong real-time performance and cross-platform generalization. Beyond providing an efficient solution for strategy game automation, our work establishes a general paradigm for enhancing VLM performance through structured multimodal data organization, offering new insights into the interplay between static perception and dynamic reasoning in embodied intelligence.

</details>


### [56] [MedRule-KG: A Knowledge-Graph--Steered Scaffold for Reliable Mathematical and Biomedical Reasoning](https://arxiv.org/abs/2511.12963)
*Crystal Su*

Main category: cs.AI

TL;DR: 研究为科学推理和药物发现的大语言模型施加领域一致结构，提出MedRule - KG系统，减少违规计数并提升匹配度，实用可行。


<details>
  <summary>Details</summary>
Motivation: 为用于科学推理和早期药物发现的大语言模型施加领域一致结构。

Method: 提出MedRule - KG，将知识图支架与轻量级验证器结合，注入符号事实，用确定性检查器执行规则，将生成形式化为约束推理，引入软指导替代物并进行统计分析。

Result: 在90个任务中，相对强思维链基线减少83.2%的违规计数，提高精确匹配度，结果稳定且验证器延迟可忽略。

Conclusion: 该方法实用，适用于交互式设计。

Abstract: We study how to impose domain-consistent structure on large language models (LLMs) used for scientific reasoning and early-stage drug discovery. We present MedRule-KG, a compact knowledge-graph scaffold paired with a lightweight verifier that steers generation toward mathematically and biomedically valid outputs. The system injects curated symbolic facts into prompts and then enforces rule satisfaction with a deterministic checker. We formalize generation as constrained inference, introduce a soft guidance surrogate suitable for decoding, and perform a thorough statistical analysis with uncertainty quantification. Across 90 tasks spanning reaction feasibility, metabolic compatibility, and toxicity screening, MedRule-KG reduces violation counts by 83.2\% relative to a strong chain-of-thought baseline while improving exact match. Results remain stable under stratification and scale with dataset size, and the verifier adds negligible latency, making the approach practical for interactive design.

</details>


### [57] [WebCoach: Self-Evolving Web Agents with Cross-Session Memory Guidance](https://arxiv.org/abs/2511.12997)
*Genglin Liu,Shijie Geng,Sha Li,Hejie Cui,Sarah Zhang,Xin Liu,Tianyi Liu*

Main category: cs.AI

TL;DR: 提出WebCoach框架，提升多模态大语言模型驱动的网页浏览代理的性能，在WebVoyager基准测试中效果显著。


<details>
  <summary>Details</summary>
Motivation: 当前网页浏览代理存在重复错误、无法跨会话学习的问题，限制了长期鲁棒性和样本效率。

Method: 引入WebCoach框架，包含WebCondenser、External Memory Store和Coach三个组件，实现跨会话记忆和持续学习。

Result: 在WebVoyager基准测试中，WebCoach提升了三种不同大语言模型骨干的浏览器使用代理的性能，如38B模型任务成功率从47%提升到61%。

Conclusion: WebCoach能让网页代理访问长期记忆，实现自我进化，提升复杂浏览任务的鲁棒性，小模型搭配WebCoach可达到类似GPT - 4o的性能。

Abstract: Multimodal LLM-powered agents have recently demonstrated impressive capabilities in web navigation, enabling agents to complete complex browsing tasks across diverse domains. However, current agents struggle with repetitive errors and lack the ability to learn from past experiences across sessions, limiting their long-term robustness and sample efficiency. We introduce WebCoach, a model-agnostic self-evolving framework that equips web browsing agents with persistent cross-session memory, enabling improved long-term planning, reflection, and continual learning without retraining. WebCoach consists of three key components: (1) a WebCondenser, which standardizes raw navigation logs into concise summaries; (2) an External Memory Store, which organizes complete trajectories as episodic experiences; and (3) a Coach, which retrieves relevant experiences based on similarity and recency, and decides whether to inject task-specific advice into the agent via runtime hooks. This design empowers web agents to access long-term memory beyond their native context window, improving robustness in complex browsing tasks. Moreover, WebCoach achieves self-evolution by continuously curating episodic memory from new navigation trajectories, enabling agents to improve over time without retraining. Evaluations on the WebVoyager benchmark demonstrate that WebCoach consistently improves the performance of browser-use agents across three different LLM backbones. With a 38B model, it increases task success rates from 47% to 61% while reducing or maintaining the average number of steps. Notably, smaller base models with WebCoach achieve performance comparable to the same web agent using GPT-4o.

</details>


### [58] [GEM: Generative Entropy-Guided Preference Modeling for Few-shot Alignment of LLMs](https://arxiv.org/abs/2511.13007)
*Yiyang Zhao,Huiyu Bai,Xuejiao Zhao*

Main category: cs.AI

TL;DR: 提出生成熵引导偏好建模方法GEM用于低资源和特定领域大语言模型对齐，实验表明在少样本数据下有显著提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型与人类偏好对齐依赖大量标注，但专业领域难获取大规模偏好标签。

Method: 提出GEM方法，通过认知过滤模块生成候选推理链并打分，用SEGA算法微调模型。

Result: 在通用基准和特定领域任务实验中，GEM在少样本偏好数据下有显著提升。

Conclusion: GEM能让大语言模型依靠自身判断，建立熵引导闭环认知优化框架，实现高效少样本对齐。

Abstract: Alignment of large language models (LLMs) with human preferences typically relies on supervised reward models or external judges that demand abundant annotations. However, in fields that rely on professional knowledge, such as medicine and law, such large-scale preference labels are often unachievable. In this paper, we propose a generative entropy-guided preference modeling approach named GEM for LLMs aligment at low-resource and domain-specific scenarios. Instead of training a discriminative reward model on preference data, we directly train the LLM to internalize a closed-loop optimization architecture that can extract and exploit the multi-dimensional, fine-grained cognitive signals implicit in human preferences. Specifically, our Cognitive Filtering module, based on entropy theory in decision making, first leverages Chain-of-Thought (CoT) prompting to generate diverse candidate reasoning chains (CoTs) from preference data. Subsequently, it introduces a token scoring mechanism to rank and weight the sampled CoTs, boosting the importance of high-confidence answers and strategically high-entropy tokens. Building on these filtered preferences, we fine-tune the LLM using a novel self-evaluated group advantage algorithm, SEGA, which effectively aggregates group-level cognitive signals and transforms the entropy-based scores into implicit rewards for policy optimization. In these ways, GEM empowers the LLM to rely on its own judgments and establishes an entropy-guided closed-loop cognitive optimization framework, enabling highly efficient few-shot alignment of LLMs. Experiments on general benchmarks and domain-specific tasks (such as mathematical reasoning and medical dialogues) demonstrate that our GEM achieves significant improvements with few-shot preference data.

</details>


### [59] [PragWorld: A Benchmark Evaluating LLMs' Local World Model under Minimal Linguistic Alterations and Conversational Dynamics](https://arxiv.org/abs/2511.13021)
*Sachin Vashistha,Aryan Bibhuti,Atharva Naik,Martin Tutek,Somak Aditya*

Main category: cs.AI

TL;DR: 评估语言模型在对话中编码和更新内部世界模型的能力，发现其难以保持准确性，提出解释框架和微调策略。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型是否能构建和维护对话的隐式表示。

Method: 对流行数据集的对话进行七种最小语言修改，构建两个包含是非问题的基准，评估多种开源和闭源语言模型；提出双视角可解释性框架；提出基于层正则化的微调策略。

Result: 语言模型难以保持稳健的准确性，难以记住关键细节。

Conclusion: 双视角可解释性框架能识别有用或有害的Transformer层，基于层正则化的微调策略可抑制有害层的影响。

Abstract: Real-world conversations are rich with pragmatic elements, such as entity mentions, references, and implicatures. Understanding such nuances is a requirement for successful natural communication, and often requires building a local world model which encodes such elements and captures the dynamics of their evolving states. However, it is not well-understood whether language models (LMs) construct or maintain a robust implicit representation of conversations. In this work, we evaluate the ability of LMs to encode and update their internal world model in dyadic conversations and test their malleability under linguistic alterations. To facilitate this, we apply seven minimal linguistic alterations to conversations sourced from popular datasets and construct two benchmarks comprising yes-no questions. We evaluate a wide range of open and closed source LMs and observe that they struggle to maintain robust accuracy. Our analysis unveils that LMs struggle to memorize crucial details, such as tracking entities under linguistic alterations to conversations. We then propose a dual-perspective interpretability framework which identifies transformer layers that are useful or harmful and highlights linguistic alterations most influenced by harmful layers, typically due to encoding spurious signals or relying on shortcuts. Inspired by these insights, we propose two layer-regularization based fine-tuning strategies that suppress the effect of the harmful layers.

</details>


### [60] [Scaling Generative Verifiers For Natural Language Mathematical Proof Verification And Selection](https://arxiv.org/abs/2511.13027)
*Sadegh Mahdavi,Branislav Kisacanin,Shubham Toshniwal,Wei Du,Ivan Moshkov,George Armstrong,Renjie Liao,Christos Thrampoulidis,Igor Gitman*

Main category: cs.AI

TL;DR: 分析大语言模型在数学问题推理中的问题，评估推理能力，提出有效验证和选择框架并指出强化学习的影响，给出实用设计评估指南。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在数学问题推理存在缺陷，推进基于严格证明的数学需要可靠的证明验证能力。

Method: 分析多种评估设置，评估基于证明和最终答案的推理，扩展两种生成验证方法，研究提示对模型性能的影响。

Result: 确定两种方法结合是最有效框架，强化学习可降低对提示的敏感性，但不提高最终答案精度。

Conclusion: 为可扩展的证明验证和选择系统的设计与评估提供实用指南。

Abstract: Large language models have achieved remarkable success on final-answer mathematical problems, largely due to the ease of applying reinforcement learning with verifiable rewards. However, the reasoning underlying these solutions is often flawed. Advancing to rigorous proof-based mathematics requires reliable proof verification capabilities. We begin by analyzing multiple evaluation setups and show that focusing on a single benchmark can lead to brittle or misleading conclusions. To address this, we evaluate both proof-based and final-answer reasoning to obtain a more reliable measure of model performance. We then scale two major generative verification methods (GenSelect and LLM-as-a-Judge) to millions of tokens and identify their combination as the most effective framework for solution verification and selection. We further show that the choice of prompt for LLM-as-a-Judge significantly affects the model's performance, but reinforcement learning can reduce this sensitivity. However, despite improving proof-level metrics, reinforcement learning does not enhance final-answer precision, indicating that current models often reward stylistic or procedural correctness rather than mathematical validity. Our results establish practical guidelines for designing and evaluating scalable proof-verification and selection systems.

</details>


### [61] [MEGA-GUI: Multi-stage Enhanced Grounding Agents for GUI Elements](https://arxiv.org/abs/2511.13087)
*SeokJoo Kwak,Jihoon Kim,Boyoun Kim,Jung Jae Yoon,Wooseok Jang,Jeonghoon Hong,Jaeho Yang,Yeong-Dae Kwon*

Main category: cs.AI

TL;DR: 提出MEGA - GUI多阶段框架用于GUI grounding，比现有方法精度高，代码和工具包开源。


<details>
  <summary>Details</summary>
Motivation: 现有GUI grounding系统缺乏模块化，在视觉杂乱和指令模糊时表现不佳。

Method: 引入MEGA - GUI框架，分离为粗粒度ROI选择和细粒度元素定位，有双向ROI缩放算法和上下文感知重写代理。

Result: 在ScreenSpot - Pro基准测试中准确率达73.18%，在OSWorld - G基准测试中达68.63%，超过之前结果。

Conclusion: 利用模块化结构比整体方法能持续获得更高准确率。

Abstract: Graphical User Interface (GUI) grounding - the task of mapping natural language instructions to screen coordinates - is essential for autonomous agents and accessibility technologies. Existing systems rely on monolithic models or one-shot pipelines that lack modularity and fail under visual clutter and ambiguous instructions. We introduce MEGA-GUI, a multi-stage framework that separates grounding into coarse Region-of-Interest (ROI) selection and fine-grained element grounding, orchestrated by specialized vision-language agents. MEGA-GUI features a bidirectional ROI zoom algorithm that mitigates spatial dilution and a context-aware rewriting agent that reduces semantic ambiguity. Our analysis reveals complementary strengths and weaknesses across vision-language models at different visual scales, and we show that leveraging this modular structure achieves consistently higher accuracy than monolithic approaches. On the visually dense ScreenSpot-Pro benchmark, MEGA-GUI attains 73.18% accuracy, and on the semantically complex OSWorld-G benchmark it reaches 68.63%, surpassing previously reported results. Code and the Grounding Benchmark Toolkit (GBT) are available at https://github.com/samsungsds-research-papers/mega-gui.

</details>


### [62] [STEP: Success-Rate-Aware Trajectory-Efficient Policy Optimization](https://arxiv.org/abs/2511.13091)
*Yuhan Chen,Yuxuan Liu,Long Zhang,Pengzhi Gao,Jian Luan,Wei Liu*

Main category: cs.AI

TL;DR: 提出STEP框架解决多轮交互在线强化学习中轨迹级优化的问题，实验表明STEP提升了样本效率和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹级优化方法在多轮交互在线强化学习中存在效率低、学习信号误导、样本收集成本高等问题。

Method: 提出STEP框架，根据任务成功率动态分配采样，进行步级优化，维护成功率记录引导重采样，计算成功率加权优势并将轨迹分解为步级样本，应用步级GRPO增强。

Result: 在OSWorld和AndroidWorld实验中，STEP相比轨迹级GRPO大幅提升样本效率和训练稳定性，收敛更快且泛化性更好。

Conclusion: STEP能有效解决多轮交互在线强化学习中轨迹级优化的问题，提升性能。

Abstract: Multi-turn interaction remains challenging for online reinforcement learning. A common solution is trajectory-level optimization, which treats each trajectory as a single training sample. However, this approach can be inefficient and yield misleading learning signals: it applies uniform sampling across tasks regardless of difficulty, penalizes correct intermediate actions in failed trajectories, and incurs high sample-collection costs. To address these issues, we propose STEP (Success-rate-aware Trajectory-Efficient Policy optimization), a framework that dynamically allocates sampling based on per-task success rates and performs step-level optimization. STEP maintains a smoothed success-rate record to guide adaptive trajectory resampling, allocating more effort to harder tasks. It then computes success-rate-weighted advantages and decomposes trajectories into step-level samples. Finally, it applies a step-level GRPO augmentation to refine updates for low-success tasks. Experiments on OSWorld and AndroidWorld show that STEP substantially improves sample efficiency and training stability over trajectory-level GRPO, converging faster and generalizing better under the same sampling budget.

</details>


### [63] [MM-Telco: Benchmarks and Multimodal Large Language Models for Telecom Applications](https://arxiv.org/abs/2511.13131)
*Gagan Raj Gupta,Anshul Kumar,Manish Rai,Apu Chakraborty,Ashutosh Modi,Abdelaali Chaoub,Soumajit Pramanik,Moyank Giri,Yashwanth Holla,Sunny Kumar,M. V. Kiran Sooraj*

Main category: cs.AI

TL;DR: 提出用于电信领域的MM - Telco多模态基准和模型套件，实验显示微调模型性能提升，还能指导后续研究。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在电信领域部署受特定挑战阻碍，需专门适配以加速其在电信领域的应用。

Method: 提出MM - Telco多模态基准和模型套件，引入多种文本和图像任务，用不同大语言模型和视觉语言模型进行基线实验。

Result: 在数据集上微调的模型性能显著提升，实验有助于分析当前多模态大语言模型的薄弱区域。

Conclusion: MM - Telco可加速大语言模型在电信领域的适配，实验结果能指导后续开发和研究。

Abstract: Large Language Models (LLMs) have emerged as powerful tools for automating complex reasoning and decision-making tasks. In telecommunications, they hold the potential to transform network optimization, automate troubleshooting, enhance customer support, and ensure regulatory compliance. However, their deployment in telecom is hindered by domain-specific challenges that demand specialized adaptation. To overcome these challenges and to accelerate the adaptation of LLMs for telecom, we propose MM-Telco, a comprehensive suite of multimodal benchmarks and models tailored for the telecom domain. The benchmark introduces various tasks (both text based and image based) that address various practical real-life use cases such as network operations, network management, improving documentation quality, and retrieval of relevant text and images. Further, we perform baseline experiments with various LLMs and VLMs. The models fine-tuned on our dataset exhibit a significant boost in performance. Our experiments also help analyze the weak areas in the working of current state-of-art multimodal LLMs, thus guiding towards further development and research.

</details>


### [64] [Conditional Diffusion Model for Multi-Agent Dynamic Task Decomposition](https://arxiv.org/abs/2511.13137)
*Yanda Zhu,Yuanyang Zhu,Daoyi Dong,Caihua Chen,Chunlin Chen*

Main category: cs.AI

TL;DR: 提出C$	ext{D}^	ext{3}$T框架用于动态任务分解，实验显示其性能优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有动态任务分解方法从无到有学习需大量训练样本，在部分可观测下探索大联合动作空间有困难。

Method: 构建两级分层多智能体强化学习框架C$	ext{D}^	ext{3}$T，高层策略学习子任务表示生成选择策略，用条件扩散模型预测观测和奖励；低层智能体协作学习共享技能，学习到的子任务表示用于多头注意力混合网络增强价值分解。

Result: 在多个基准测试上，C$	ext{D}^	ext{3}$T比现有基线表现更好。

Conclusion: C$	ext{D}^	ext{3}$T是有效的动态任务分解框架，能提升多智能体强化学习性能。

Abstract: Task decomposition has shown promise in complex cooperative multi-agent reinforcement learning (MARL) tasks, which enables efficient hierarchical learning for long-horizon tasks in dynamic and uncertain environments. However, learning dynamic task decomposition from scratch generally requires a large number of training samples, especially exploring the large joint action space under partial observability. In this paper, we present the Conditional Diffusion Model for Dynamic Task Decomposition (C$\text{D}^\text{3}$T), a novel two-level hierarchical MARL framework designed to automatically infer subtask and coordination patterns. The high-level policy learns subtask representation to generate a subtask selection strategy based on subtask effects. To capture the effects of subtasks on the environment, C$\text{D}^\text{3}$T predicts the next observation and reward using a conditional diffusion model. At the low level, agents collaboratively learn and share specialized skills within their assigned subtasks. Moreover, the learned subtask representation is also used as additional semantic information in a multi-head attention mixing network to enhance value decomposition and provide an efficient reasoning bridge between individual and joint value functions. Experimental results on various benchmarks demonstrate that C$\text{D}^\text{3}$T achieves better performance than existing baselines.

</details>


### [65] [InteractiveGNNExplainer: A Visual Analytics Framework for Multi-Faceted Understanding and Probing of Graph Neural Network Predictions](https://arxiv.org/abs/2511.13160)
*TC Singh,Sougata Mukherjea*

Main category: cs.AI

TL;DR: 本文提出InteractiveGNNExplainer视觉分析框架提升GNN在节点分类任务中的可解释性，结合多种视图与解释技术，支持图编辑，通过案例展示其功能与优势。


<details>
  <summary>Details</summary>
Motivation: GNN复杂非线性操作使其成为不透明黑箱，阻碍用户信任、调试、偏差检测及在需可解释性关键领域的应用，需提升其可解释性。

Method: 引入InteractiveGNNExplainer框架，集成协调交互式视图与事后和内在解释技术，加入交互式图编辑功能。

Result: 通过Cora和CiteSeer数据集案例研究，展示该框架能进行错误分类诊断、对比GCN与GAT行为、探究模型敏感性。

Conclusion: 该框架有助于更深入理解GNN预测，促进更透明、可信和健壮的图分析。

Abstract: Graph Neural Networks (GNNs) excel in graph-based learning tasks, but their complex, non-linear operations often render them as opaque "black boxes". This opacity hinders user trust, complicates debugging, bias detection, and adoption in critical domains requiring explainability. This paper introduces InteractiveGNNExplainer, a visual analytics framework to enhance GNN explainability, focusing on node classification. Our system uniquely integrates coordinated interactive views (dynamic graph layouts, embedding projections, feature inspection, neighborhood analysis) with established post-hoc (GNNExplainer) and intrinsic (GAT attention) explanation techniques. Crucially, it incorporates interactive graph editing, allowing users to perform a "what-if" analysis by perturbing graph structures and observing immediate impacts on GNN predictions and explanations. We detail the system architecture and, through case studies on Cora and CiteSeer datasets, demonstrate how InteractiveGNNExplainer facilitates in-depth misclassification diagnosis, comparative analysis of GCN versus GAT behaviors, and rigorous probing of model sensitivity. These capabilities foster a deeper, multifaceted understanding of GNN predictions, contributing to more transparent, trustworthy, and robust graph analysis.

</details>


### [66] [Cost-Effective Communication: An Auction-based Method for Language Agent Interaction](https://arxiv.org/abs/2511.13193)
*Yijia Fan,Jusheng Zhang,Kaitong Cai,Jing Yang,Chengpei Tang,Jian Wang,Keze Wang*

Main category: cs.AI

TL;DR: 论文指出多智能体系统通信效率低问题，提出DALA框架，实验证明其高效且表现优。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体系统基于大语言模型通信时的低效、高成本和低信噪比问题。

Method: 引入DALA框架，将通信带宽视为稀缺可交易资源，把交互通信当作集中拍卖，让智能体根据消息价值密度投标发言。

Result: 在七个推理基准测试中达到新的最优性能，如MMLU达84.32%，HumanEval的pass@1率达91.21%，且仅用625万令牌。

Conclusion: DALA框架有效，能让智能体培养战略沉默技能，动态调整通信策略。

Abstract: Multi-agent systems (MAS) built on large language models (LLMs) often suffer from inefficient "free-for-all" communication, leading to exponential token costs and low signal-to-noise ratios that hinder their practical deployment. We challenge the notion that more communication is always beneficial, hypothesizing instead that the core issue is the absence of resource rationality. We argue that "free" communication, by ignoring the principle of scarcity, inherently breeds inefficiency and unnecessary expenses. To address this, we introduce the Dynamic Auction-based Language Agent (DALA), a novel framework that treats communication bandwidth as a scarce and tradable resource. Specifically, our DALA regards inter-agent communication as a centralized auction, where agents learn to bid for the opportunity to speak based on the predicted value density of their messages. Thus, our DALA intrinsically encourages agents to produce concise, informative messages while filtering out low-value communication. Extensive and comprehensive experiments demonstrate that our economically-driven DALA achieves new state-of-the-art performance across seven challenging reasoning benchmarks, including 84.32% on MMLU and a 91.21% pass@1 rate on HumanEval. Note that this is accomplished with remarkable efficiency, i.e., our DALA uses only 6.25 million tokens, a fraction of the resources consumed by current state-of-the-art methods on GSM8K. Further analysis reveals that our DALA cultivates the emergent skill of strategic silence, effectively adapting its communication strategies from verbosity to silence in a dynamical manner via resource constraints.

</details>


### [67] [Learning to Solve Resource-Constrained Project Scheduling Problems with Duration Uncertainty using Graph Neural Networks](https://arxiv.org/abs/2511.13214)
*Guillaume Infantes,Stéphanie Roussel,Antoine Jacquet,Emmanuel Benazera*

Main category: cs.AI

TL;DR: 本文针对含不确定任务时长的资源受限项目调度问题，结合图神经网络与深度强化学习制定调度策略，实验表明其性能优越且可泛化，框架Wheatley公开。


<details>
  <summary>Details</summary>
Motivation: 实际中任务时长存在不确定性，需考虑此因素以提出有弹性的调度，目标是生成可多次复用的基线调度。

Method: 结合图神经网络与深度强化学习制定任务调度策略，与串行调度生成方案配合产生调度。

Result: 在标准基准上的实证评估显示该方法在性能上具有优越性且能够泛化。

Conclusion: 开发的框架Wheatley公开以便进一步研究和复现。

Abstract: The Resource-Constrained Project Scheduling Problem (RCPSP) is a classical scheduling problem that has received significant attention due to of its numerous applications in industry. However, in practice, task durations are subject to uncertainty that must be considered in order to propose resilient scheduling. In this paper, we address the RCPSP variant with uncertain tasks duration (modeled using known probabilities) and aim to minimize the overall expected project duration. Our objective is to produce a baseline schedule that can be reused multiple times in an industrial setting regardless of the actual duration scenario. We leverage Graph Neural Networks in conjunction with Deep Reinforcement Learning (DRL) to develop an effective policy for task scheduling. This policy operates similarly to a priority dispatch rule and is paired with a Serial Schedule Generation Scheme to produce a schedule. Our empirical evaluation on standard benchmarks demonstrates the approach's superiority in terms of performance and its ability to generalize. The developed framework, Wheatley, is made publicly available online to facilitate further research and reproducibility.

</details>


### [68] [Informative Communication of Robot Plans](https://arxiv.org/abs/2511.13226)
*Michele Persiani,Thomas Hellstrom*

Main category: cs.AI

TL;DR: 本文提出一种传达机器人计划的策略，实验表明该策略能让人更快理解机器人目标。


<details>
  <summary>Details</summary>
Motivation: 现有增量式策略未考虑对用户的有效信息传达，未考虑用户先验知识。

Method: 通过衡量言语表达相对于捕捉用户对机器人先验知识的二阶心智理论的信息增益，提出信息传达策略。

Result: 该策略比递增或递减计划顺序的策略能让人更快理解机器人目标。

Conclusion: 所提策略可有效传达机器人计划，还能提示信息内容及原因。

Abstract: When a robot is asked to verbalize its plan it can do it in many ways. For example, a seemingly natural strategy is incremental, where the robot verbalizes its planned actions in plan order. However, an important aspect of this type of strategy is that it misses considerations on what is effectively informative to communicate, because not considering what the user knows prior to explanations. In this paper we propose a verbalization strategy to communicate robot plans informatively, by measuring the information gain that verbalizations have against a second-order theory of mind of the user capturing his prior knowledge on the robot. As shown in our experiments, this strategy allows to understand the robot's goal much quicker than by using strategies such as increasing or decreasing plan order. In addition, following our formulation we hint to what is informative and why when a robot communicates its plan.

</details>


### [69] [Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO](https://arxiv.org/abs/2511.13288)
*Haoyang Hong,Jiajun Yin,Yuan Wang,Jingnan Liu,Zhe Chen,Ailing Yu,Ji Li,Zhiling Ye,Hansong Xiao,Yefei Chen,Hualei Zhou,Yun Yue,Minghui Yang,Chunxiao Guo,Junwei Liu,Peng Wei,Jinjie Gu*

Main category: cs.AI

TL;DR: 现有多智能体系统在专业领域训练不足且统一大语言模型训练有局限，提出M - GRPO方法，实验显示其性能更优，证明对齐轨迹和解耦优化可提升推理任务表现。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在专业领域训练不足，统一大语言模型训练限制性能，不同LLM训练有优化挑战。

Method: 提出M - GRPO，计算主副智能体组相对优势，引入轨迹对齐方案，部署解耦训练管道。

Result: 在真实世界基准测试中，M - GRPO始终优于单智能体GRPO和冻结副智能体的多智能体GRPO，稳定性和样本效率更高。

Conclusion: 对齐异构轨迹和解耦专业智能体优化可提升工具增强推理任务。

Abstract: Multi-agent systems perform well on general reasoning tasks. However, the lack of training in specialized areas hinders their accuracy. Current training methods train a unified large language model (LLM) for all agents in the system. This may limit the performances due to different distributions underlying for different agents. Therefore, training multi-agent systems with distinct LLMs should be the next step to solve. However, this approach introduces optimization challenges. For example, agents operate at different frequencies, rollouts involve varying sub-agent invocations, and agents are often deployed across separate servers, disrupting end-to-end gradient flow. To address these issues, we propose M-GRPO, a hierarchical extension of Group Relative Policy Optimization designed for vertical Multi-agent systems with a main agent (planner) and multiple sub-agents (multi-turn tool executors). M-GRPO computes group-relative advantages for both main and sub-agents, maintaining hierarchical credit assignment. It also introduces a trajectory-alignment scheme that generates fixed-size batches despite variable sub-agent invocations. We deploy a decoupled training pipeline in which agents run on separate servers and exchange minimal statistics via a shared store. This enables scalable training without cross-server backpropagation. In experiments on real-world benchmarks (e.g., GAIA, XBench-DeepSearch, and WebWalkerQA), M-GRPO consistently outperforms both single-agent GRPO and multi-agent GRPO with frozen sub-agents, demonstrating improved stability and sample efficiency. These results show that aligning heterogeneous trajectories and decoupling optimization across specialized agents enhances tool-augmented reasoning tasks.

</details>


### [70] [Dropouts in Confidence: Moral Uncertainty in Human-LLM Alignment](https://arxiv.org/abs/2511.13290)
*Jea Kwon,Luiz Felipe Vecchietti,Sungwon Park,Meeyoung Cha*

Main category: cs.AI

TL;DR: 本文研究不确定性对机器在经典电车难题中道德决策的影响，发现可通过调节不确定性提升模型与人类道德的一致性。


<details>
  <summary>Details</summary>
Motivation: 人类面对道德困境有不确定性，而机器和AI的此类不确定性研究不足，且机器生成回答有过度自信倾向，在伦理决策场景中需理解其道德推理和不确定性以构建可靠AI系统。

Method: 分析32个开源模型在9个道德维度的回答，用二元熵量化不确定性，在推理时通过“dropout”引入随机性。

Result: 模型间的信心差异大于道德维度内，该机制增加了总熵，主要是互信息增加，条件熵基本不变，显著提升了人类与大语言模型的道德一致性。

Conclusion: 可通过有意调节不确定性和降低大语言模型在道德复杂场景中的信心，使模型决策更好地符合人类偏好。

Abstract: Humans display significant uncertainty when confronted with moral dilemmas, yet the extent of such uncertainty in machines and AI agents remains underexplored. Recent studies have confirmed the overly confident tendencies of machine-generated responses, particularly in large language models (LLMs). As these systems are increasingly embedded in ethical decision-making scenarios, it is important to understand their moral reasoning and the inherent uncertainties in building reliable AI systems. This work examines how uncertainty influences moral decisions in the classical trolley problem, analyzing responses from 32 open-source models and 9 distinct moral dimensions. We first find that variance in model confidence is greater across models than within moral dimensions, suggesting that moral uncertainty is predominantly shaped by model architecture and training method. To quantify uncertainty, we measure binary entropy as a linear combination of total entropy, conditional entropy, and mutual information. To examine its effects, we introduce stochasticity into models via "dropout" at inference time. Our findings show that our mechanism increases total entropy, mainly through a rise in mutual information, while conditional entropy remains largely unchanged. Moreover, this mechanism significantly improves human-LLM moral alignment, with correlations in mutual information and alignment score shifts. Our results highlight the potential to better align model-generated decisions and human preferences by deliberately modulating uncertainty and reducing LLMs' confidence in morally complex scenarios.

</details>


### [71] [Grounded by Experience: Generative Healthcare Prediction Augmented with Hierarchical Agentic Retrieval](https://arxiv.org/abs/2511.13293)
*Chuang Zhao,Hui Tang,Hongke Zhao,Xiaofang Zhou,Xiaomeng Li*

Main category: cs.AI

TL;DR: 提出GHAR框架解决医疗场景下RAG框架面临的挑战，实验显示其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 大语言模型用于医疗预测存在事实不准确问题，现有RAG框架在医疗场景面临激活检索机制和子模块协作挑战。

Method: 设计包含Agent - Top和Agent - Low的双代理架构，将两个代理的优化统一在马尔可夫决策过程中并设计不同奖励。

Result: 在三个基准数据集的三个流行任务上的实验表明优于现有基线。

Conclusion: 分层代理RAG在推进医疗系统方面有潜力。

Abstract: Accurate healthcare prediction is critical for improving patient outcomes and reducing operational costs. Bolstered by growing reasoning capabilities, large language models (LLMs) offer a promising path to enhance healthcare predictions by drawing on their rich parametric knowledge. However, LLMs are prone to factual inaccuracies due to limitations in the reliability and coverage of their embedded knowledge. While retrieval-augmented generation (RAG) frameworks, such as GraphRAG and its variants, have been proposed to mitigate these issues by incorporating external knowledge, they face two key challenges in the healthcare scenario: (1) identifying the clinical necessity to activate the retrieval mechanism, and (2) achieving synergy between the retriever and the generator to craft contextually appropriate retrievals. To address these challenges, we propose GHAR, a \underline{g}enerative \underline{h}ierarchical \underline{a}gentic \underline{R}AG framework that simultaneously resolves when to retrieve and how to optimize the collaboration between submodules in healthcare. Specifically, for the first challenge, we design a dual-agent architecture comprising Agent-Top and Agent-Low. Agent-Top acts as the primary physician, iteratively deciding whether to rely on parametric knowledge or to initiate retrieval, while Agent-Low acts as the consulting service, summarising all task-relevant knowledge once retrieval was triggered. To tackle the second challenge, we innovatively unify the optimization of both agents within a formal Markov Decision Process, designing diverse rewards to align their shared goal of accurate prediction while preserving their distinct roles. Extensive experiments on three benchmark datasets across three popular tasks demonstrate our superiority over state-of-the-art baselines, highlighting the potential of hierarchical agentic RAG in advancing healthcare systems.

</details>


### [72] [DAP: A Discrete-token Autoregressive Planner for Autonomous Driving](https://arxiv.org/abs/2511.13306)
*Bowen Ye,Bin Zhang,Hang Zhao*

Main category: cs.AI

TL;DR: 提出离散标记自回归规划器DAP用于自动驾驶规划，结合强化学习微调，在参数预算小的情况下取得良好性能，提供紧凑可扩展规划范式。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶中，利用数据和模型预算实现可持续性能提升是未解决的关键挑战，现有自回归模型仅预测自我轨迹存在监督稀疏等问题。

Method: 引入DAP联合预测BEV语义和自我轨迹，进行全面表征学习；结合基于强化学习的微调，保留监督行为克隆先验。

Result: DAP在160M参数预算下，在开环指标上达到了最先进性能，在NAVSIM基准的闭环结果中有竞争力。

Conclusion: 基于离散标记自回归的公式，在栅格化BEV和自我动作上操作，为自动驾驶提供了紧凑且可扩展的规划范式。

Abstract: Gaining sustainable performance improvement with scaling data and model budget remains a pivotal yet unresolved challenge in autonomous driving. While autoregressive models exhibited promising data-scaling efficiency in planning tasks, predicting ego trajectories alone suffers sparse supervision and weakly constrains how scene evolution should shape ego motion. Therefore, we introduce DAP, a discrete-token autoregressive planner that jointly forecasts BEV semantics and ego trajectories, thereby enforcing comprehensive representation learning and allowing predicted dynamics to directly condition ego motion. In addition, we incorporate a reinforcement-learning-based fine-tuning, which preserves supervised behavior cloning priors while injecting reward-guided improvements. Despite a compact 160M parameter budget, DAP achieves state-of-the-art performance on open-loop metrics and delivers competitive closed-loop results on the NAVSIM benchmark. Overall, the fully discrete-token autoregressive formulation operating on both rasterized BEV and ego actions provides a compact yet scalable planning paradigm for autonomous driving.

</details>


### [73] [Reasoning Shapes Alignment: Investigating Cultural Alignment in Large Reasoning Models with Cultural Norms](https://arxiv.org/abs/2511.13359)
*Yuhang Wang,Yanxu Zhu,Jitao Sang*

Main category: cs.AI

TL;DR: 本文提出CNCA框架让大推理模型利用推理能力实现文化对齐，介绍挖掘文化规范方法和对齐范式，实验证明方法有效，强调推理模型通过文化对齐反映多元人类价值观的潜力。


<details>
  <summary>Details</summary>
Motivation: 大推理模型需在保障安全外反映不同文化的人类价值观。

Method: 提出CNCA框架，三种从有限调查数据自动挖掘文化规范的方法，研究两种对齐范式：上下文对齐法和基于微调的方法。

Result: 实验表明方法有效，推理能力强的模型从文化规范挖掘和利用中受益更多。

Conclusion: 推理模型可通过文化对齐策略更好反映多元人类价值观。

Abstract: The advanced reasoning capabilities of Large Reasoning Models enable them to thoroughly understand and apply safety policies through deliberate thought processes, thereby improving the models' safety. Beyond safety, these models must also be able to reflect the diverse range of human values across various cultures. This paper presents the Cultural Norm-based Cultural Alignment (CNCA) framework, which enables models to leverage their powerful reasoning ability to align with cultural norms. Specifically, we propose three methods to automatically mine cultural norms from limited survey data and explore ways to effectively utilize these norms for improving cultural alignment. Two alignment paradigms are examined: an in-context alignment method, where cultural norms are explicitly integrated into the user context, and a fine-tuning-based method, which internalizes norms through enhanced Chain-of-Thought training data. Comprehensive experiments demonstrate the effectiveness of these methods, highlighting that models with stronger reasoning capabilities benefit more from cultural norm mining and utilization. Our findings emphasize the potential for reasoning models to better reflect diverse human values through culturally informed alignment strategies.

</details>


### [74] [MedDCR: Learning to Design Agentic Workflows for Medical Coding](https://arxiv.org/abs/2511.13361)
*Jiyang Zheng,Islam Nassar,Thanh Vu,Xu Zhong,Yang Lin,Tongliang Liu,Long Duong,Yuan-Fang Li*

Main category: cs.AI

TL;DR: 提出MedDCR框架解决医学编码工作流学习问题，在基准数据集上表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的医学编码方法依赖手动工作流，无法捕捉真实文档的细微差别和可变性，需要系统地学习有效的工作流。

Method: 提出MedDCR闭环框架，包含设计器、编码器、评估器和记忆存档，将工作流设计视为学习问题。

Result: 在基准数据集上，MedDCR优于现有基线，产生可解释、适应性强的工作流。

Conclusion: MedDCR提高了自动化医学编码系统的可靠性和可信度。

Abstract: Medical coding converts free-text clinical notes into standardized diagnostic and procedural codes, which are essential for billing, hospital operations, and medical research. Unlike ordinary text classification, it requires multi-step reasoning: extracting diagnostic concepts, applying guideline constraints, mapping to hierarchical codebooks, and ensuring cross-document consistency. Recent advances leverage agentic LLMs, but most rely on rigid, manually crafted workflows that fail to capture the nuance and variability of real-world documentation, leaving open the question of how to systematically learn effective workflows. We present MedDCR, a closed-loop framework that treats workflow design as a learning problem. A Designer proposes workflows, a Coder executes them, and a Reflector evaluates predictions and provides constructive feedback, while a memory archive preserves prior designs for reuse and iterative refinement. On benchmark datasets, MedDCR outperforms state-of-the-art baselines and produces interpretable, adaptable workflows that better reflect real coding practice, improving both the reliability and trustworthiness of automated systems.

</details>


### [75] [Cognitive Maps in Language Models: A Mechanistic Analysis of Spatial Planning](https://arxiv.org/abs/2511.13371)
*Caroline Baumgartner,Eleanor Spens,Neil Burgess,Petru Manescu*

Main category: cs.AI

TL;DR: 研究大语言模型解决空间导航任务的方式，训练GPT - 2模型，发现不同学习算法及泛化与优化权衡。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型如何解决空间导航任务。

Method: 在网格环境的三种空间学习范式上训练GPT - 2模型，并进行行为、表征和机制分析。

Result: 觅食模型形成类似认知地图的表征，采用自适应分层推理系统；目标导向模型和混合模型学习路径依赖算法。

Conclusion: Transformer中的空间智能可能处于一个范围，训练方式会影响出现的策略。

Abstract: How do large language models solve spatial navigation tasks? We investigate this by training GPT-2 models on three spatial learning paradigms in grid environments: passive exploration (Foraging Model- predicting steps in random walks), goal-directed planning (generating optimal shortest paths) on structured Hamiltonian paths (SP-Hamiltonian), and a hybrid model fine-tuned with exploratory data (SP-Random Walk). Using behavioural, representational and mechanistic analyses, we uncover two fundamentally different learned algorithms. The Foraging model develops a robust, map-like representation of space, akin to a 'cognitive map'. Causal interventions reveal that it learns to consolidate spatial information into a self-sufficient coordinate system, evidenced by a sharp phase transition where its reliance on historical direction tokens vanishes by the middle layers of the network. The model also adopts an adaptive, hierarchical reasoning system, switching between a low-level heuristic for short contexts and map-based inference for longer ones. In contrast, the goal-directed models learn a path-dependent algorithm, remaining reliant on explicit directional inputs throughout all layers. The hybrid model, despite demonstrating improved generalisation over its parent, retains the same path-dependent strategy. These findings suggest that the nature of spatial intelligence in transformers may lie on a spectrum, ranging from generalisable world models shaped by exploratory data to heuristics optimised for goal-directed tasks. We provide a mechanistic account of this generalisation-optimisation trade-off and highlight how the choice of training regime influences the strategies that emerge.

</details>


### [76] [An Operational Kardashev-Style Scale for Autonomous AI - Towards AGI and Superintelligence](https://arxiv.org/abs/2511.13411)
*Przemyslaw Chojecki*

Main category: cs.AI

TL;DR: 提出受卡尔达肖夫启发的可操作自主AI（AAI）量表，定义多轴能力及相关系数、基准套件等，通过实验展示系统映射，证明AAI - 3到AAI - 5的定理。


<details>
  <summary>Details</summary>
Motivation: 需要一个可衡量从固定机器人流程自动化到通用人工智能及更高级别进展的工具。

Method: 定义十项能力轴，用复合AAI - 指数聚合；引入自我改进系数和两个封闭属性；指定OWA - Bench基准套件；用轴、系数和封闭证明定义等级门限。

Result: 合成实验展示了当前系统在量表上的映射，以及可委托性边界随自我改进的推进。证明了AAI - 3在足够条件下会随时间变为AAI - 5的定理。

Conclusion: 所提出的AAI量表可用于衡量自主AI的进展，且“婴儿AGI”变为超级智能的直觉可被形式化。

Abstract: We propose a Kardashev-inspired yet operational Autonomous AI (AAI) Scale that measures the progression from fixed robotic process automation (AAI-0) to full artificial general intelligence (AAI-4) and beyond. Unlike narrative ladders, our scale is multi-axis and testable. We define ten capability axes (Autonomy, Generality, Planning, Memory/Persistence, Tool Economy, Self-Revision, Sociality/Coordination, Embodiment, World-Model Fidelity, Economic Throughput) aggregated by a composite AAI-Index (a weighted geometric mean). We introduce a measurable Self-Improvement Coefficient $κ$ (capability growth per unit of agent-initiated resources) and two closure properties (maintenance and expansion) that convert ``self-improving AI'' into falsifiable criteria. We specify OWA-Bench, an open-world agency benchmark suite that evaluates long-horizon, tool-using, persistent agents. We define level gates for AAI-0\ldots AAI-4 using thresholds on the axes, $κ$, and closure proofs. Synthetic experiments illustrate how present-day systems map onto the scale and how the delegability frontier (quality vs.\ autonomy) advances with self-improvement. We also prove a theorem that AAI-3 agent becomes AAI-5 over time with sufficient conditions, formalizing "baby AGI" becomes Superintelligence intuition.

</details>


### [77] [Multi-Agent Multimodal Large Language Model Framework for Automated Interpretation of Fuel Efficiency Analytics in Public Transportation](https://arxiv.org/abs/2511.13476)
*Zhipeng Ma,Ali Rida Bahja,Andreas Burgdorf,André Pomp,Tobias Meisen,Bo Nørregaard Jørgensen,Zheng Grace Ma*

Main category: cs.AI

TL;DR: 本文提出多智能体框架，利用多模态大语言模型自动生成数据叙述和能源洞察，经丹麦公交案例验证，确定最佳配置，展示了该框架优势并建立了可复制方法。


<details>
  <summary>Details</summary>
Motivation: 传统分析和可视化方法处理公共交通多模态数据时产出碎片化，限制可扩展性和一致性，需新方法提升燃油效率。

Method: 提出多智能体框架，协调数据叙述智能体、LLM评判智能体和可选的人工评估器，通过丹麦公交案例，用高斯混合模型聚类分析数据，对比五个LLM和三种提示范式。

Result: 确定GPT - 4.1 mini搭配思维链提示为最佳配置，叙事准确率达97.3%，兼顾可解释性和计算成本。

Conclusion: 多智能体编排显著提升基于LLM报告的事实准确性、连贯性和可扩展性，框架为能源信息学中AI驱动的叙事生成和决策支持建立了可复制、适应特定领域的方法。

Abstract: Enhancing fuel efficiency in public transportation requires the integration of complex multimodal data into interpretable, decision-relevant insights. However, traditional analytics and visualization methods often yield fragmented outputs that demand extensive human interpretation, limiting scalability and consistency. This study presents a multi-agent framework that leverages multimodal large language models (LLMs) to automate data narration and energy insight generation. The framework coordinates three specialized agents, including a data narration agent, an LLM-as-a-judge agent, and an optional human-in-the-loop evaluator, to iteratively transform analytical artifacts into coherent, stakeholder-oriented reports. The system is validated through a real-world case study on public bus transportation in Northern Jutland, Denmark, where fuel efficiency data from 4006 trips are analyzed using Gaussian Mixture Model clustering. Comparative experiments across five state-of-the-art LLMs and three prompting paradigms identify GPT-4.1 mini with Chain-of-Thought prompting as the optimal configuration, achieving 97.3% narrative accuracy while balancing interpretability and computational cost. The findings demonstrate that multi-agent orchestration significantly enhances factual precision, coherence, and scalability in LLM-based reporting. The proposed framework establishes a replicable and domain-adaptive methodology for AI-driven narrative generation and decision support in energy informatics.

</details>


### [78] [FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI](https://arxiv.org/abs/2511.13524)
*Yuhang Peng,Yizhou Pan,Xinning He,Jihaoyu Yang,Xinyu Yin,Han Wang,Xiaoji Zheng,Chao Gao,Jiangtao Gong*

Main category: cs.AI

TL;DR: 介绍交互式模拟框架FreeAskWorld，含数据集并进行实验，结果表明该框架能提升具身AI系统性能。


<details>
  <summary>Details</summary>
Motivation: 随着具身智能成为前沿，模拟平台需捕捉复杂人类社交行为。

Method: 引入结合大语言模型的FreeAskWorld框架，拓展VLN任务，发布数据集，在不同设置下对模型和人类参与者进行基准测试。

Result: 在FreeAskWorld上微调的模型表现更优，实现更好语义理解和交互能力。

Conclusion: 基于社交的模拟框架对推进具身AI系统有效，交互是额外信息模态。

Abstract: As embodied intelligence emerges as a core frontier in artificial intelligence research, simulation platforms must evolve beyond low-level physical interactions to capture complex, human-centered social behaviors. We introduce FreeAskWorld, an interactive simulation framework that integrates large language models (LLMs) for high-level behavior planning and semantically grounded interaction, informed by theories of intention and social cognition. Our framework supports scalable, realistic human-agent simulations and includes a modular data generation pipeline tailored for diverse embodied tasks.To validate the framework, we extend the classic Vision-and-Language Navigation (VLN) task into a interaction enriched Direction Inquiry setting, wherein agents can actively seek and interpret navigational guidance. We present and publicly release FreeAskWorld, a large-scale benchmark dataset comprising reconstructed environments, six diverse task types, 16 core object categories, 63,429 annotated sample frames, and more than 17 hours of interaction data to support training and evaluation of embodied AI systems. We benchmark VLN models, and human participants under both open-loop and closed-loop settings. Experimental results demonstrate that models fine-tuned on FreeAskWorld outperform their original counterparts, achieving enhanced semantic understanding and interaction competency. These findings underscore the efficacy of socially grounded simulation frameworks in advancing embodied AI systems toward sophisticated high-level planning and more naturalistic human-agent interaction. Importantly, our work underscores that interaction itself serves as an additional information modality.

</details>


### [79] [Automated Construction of Medical Indicator Knowledge Graphs Using Retrieval Augmented Large Language Models](https://arxiv.org/abs/2511.13526)
*Zhengda Wang,Daqian Shi,Jingyi Zhao,Xiaolei Diao,Xiongfeng Tang,Yanguo Qin*

Main category: cs.AI

TL;DR: 本文提出结合RAG和大语言模型构建医学指标知识图谱的自动化框架，用于加速人工智能驱动的医疗解决方案开发。


<details>
  <summary>Details</summary>
Motivation: 当前临床知识图谱依赖手动整理和基于规则的提取，受医学指南和文献的复杂性及上下文歧义限制，难以满足临床决策支持需求。

Method: 提出结合检索增强生成（RAG）与大语言模型的自动化框架，包括指南驱动的数据获取、基于本体的模式设计和专家参与验证。

Result: 构建出可集成到智能诊断和问答系统的知识图谱。

Conclusion: 该框架能确保可扩展性、准确性和临床可靠性，加速人工智能驱动的医疗解决方案的发展。

Abstract: Artificial intelligence (AI) is reshaping modern healthcare by advancing disease diagnosis, treatment decision-making, and biomedical research. Among AI technologies, large language models (LLMs) have become especially impactful, enabling deep knowledge extraction and semantic reasoning from complex medical texts. However, effective clinical decision support requires knowledge in structured, interoperable formats. Knowledge graphs serve this role by integrating heterogeneous medical information into semantically consistent networks. Yet, current clinical knowledge graphs still depend heavily on manual curation and rule-based extraction, which is limited by the complexity and contextual ambiguity of medical guidelines and literature. To overcome these challenges, we propose an automated framework that combines retrieval-augmented generation (RAG) with LLMs to construct medical indicator knowledge graphs. The framework incorporates guideline-driven data acquisition, ontology-based schema design, and expert-in-the-loop validation to ensure scalability, accuracy, and clinical reliability. The resulting knowledge graphs can be integrated into intelligent diagnosis and question-answering systems, accelerating the development of AI-driven healthcare solutions.

</details>


### [80] [Artificial Intelligence-driven Intelligent Wearable Systems: A full-stack Integration from Material Design to Personalized Interaction](https://arxiv.org/abs/2511.13565)
*Jingyi Zhao,Daqian Shi,Zhengda Wang,Xiongfeng Tang,Yanguo Qin*

Main category: cs.AI

TL;DR: 提出人类共生健康智能（HSHI）框架，推动医疗保健向预防、适应性和技术与健康管理和谐关系的模式转变。


<details>
  <summary>Details</summary>
Motivation: 传统智能可穿戴系统依赖经验材料设计和基本信号处理技术，存在局限性，需改进。

Method: 引入HSHI框架，集成多模态传感器网络、边缘 - 云协同计算及混合数据和知识建模方法，结合AI优化材料和微结构，采用双重机制融合群体洞察与个性化调整，利用强化学习和数字孪生进行闭环优化。

Result: 该框架能动态适应个体间和个体内差异，将健康管理从被动监测转变为主动协作进化。

Conclusion: HSHI代表了医疗保健的重大转变，强调预防、适应性以及技术与健康管理的和谐关系。

Abstract: Intelligent wearable systems are at the forefront of precision medicine and play a crucial role in enhancing human-machine interaction. Traditional devices often encounter limitations due to their dependence on empirical material design and basic signal processing techniques. To overcome these issues, we introduce the concept of Human-Symbiotic Health Intelligence (HSHI), which is a framework that integrates multi-modal sensor networks with edge-cloud collaborative computing and a hybrid approach to data and knowledge modeling. HSHI is designed to adapt dynamically to both inter-individual and intra-individual variability, transitioning health management from passive monitoring to an active collaborative evolution. The framework incorporates AI-driven optimization of materials and micro-structures, provides robust interpretation of multi-modal signals, and utilizes a dual mechanism that merges population-level insights with personalized adaptations. Moreover, the integration of closed-loop optimization through reinforcement learning and digital twins facilitates customized interventions and feedback. In general, HSHI represents a significant shift in healthcare, moving towards a model that emphasizes prevention, adaptability, and a harmonious relationship between technology and health management.

</details>


### [81] [CreBench: Human-Aligned Creativity Evaluation from Idea to Process to Product](https://arxiv.org/abs/2511.13626)
*Kaiwen Xue,Chenglong Li,Zhonghong Ou,Guoxin Zhang,Kaoyan Lu,Shuai Lyu,Yifan Zhu,Ping Zong Junpeng Ding,Xinyu Liu,Qunlin Chen,Weiwei Qin,Yiran Shen,Jiayi Cen*

Main category: cs.AI

TL;DR: 提出CreBench基准和CreExpert模型，实验显示CreExpert在创意评估上比现有模型更贴合人类判断。


<details>
  <summary>Details</summary>
Motivation: 人类定义的创造力抽象，MLLMs理解和评估创造力有挑战且缺乏基准。

Method: 提出CreBench，含评估基准和CreMIT数据集，用GPT完善反馈，基于CreBench微调开源MLLMs得到CreExpert。

Result: CreExpert模型在与人类创意评估的一致性上显著优于现有模型。

Conclusion: CreBench可用于构建理解人类创造力的MLLMs，CreExpert在创意评估方面表现出色。

Abstract: Human-defined creativity is highly abstract, posing a challenge for multimodal large language models (MLLMs) to comprehend and assess creativity that aligns with human judgments. The absence of an existing benchmark further exacerbates this dilemma. To this end, we propose CreBench, which consists of two key components: 1) an evaluation benchmark covering the multiple dimensions from creative idea to process to products; 2) CreMIT (Creativity Multimodal Instruction Tuning dataset), a multimodal creativity evaluation dataset, consisting of 2.2K diverse-sourced multimodal data, 79.2K human feedbacks and 4.7M multi-typed instructions. Specifically, to ensure MLLMs can handle diverse creativity-related queries, we prompt GPT to refine these human feedbacks to activate stronger creativity assessment capabilities. CreBench serves as a foundation for building MLLMs that understand human-aligned creativity. Based on the CreBench, we fine-tune open-source general MLLMs, resulting in CreExpert, a multimodal creativity evaluation expert model. Extensive experiments demonstrate that the proposed CreExpert models achieve significantly better alignment with human creativity evaluation compared to state-of-the-art MLLMs, including the most advanced GPT-4V and Gemini-Pro-Vision.

</details>


### [82] [Beyond Mimicry: Preference Coherence in LLMs](https://arxiv.org/abs/2511.13630)
*Luhan Mikaelson,Derek Shiller,Hayley Clatterbuck*

Main category: cs.AI

TL;DR: 研究大语言模型是否有真正的偏好结构，发现多数模型缺乏统一偏好结构。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型是否具有真正的偏好结构。

Method: 对八个最先进模型的48种模型类别组合进行分析，使用逻辑回归和行为分类，还通过时间范围操纵测试工具性假设。

Result: 23种组合有场景强度与选择模式的显著关系，15种有范围内切换点，仅5种有有意义偏好一致性，26种无明显权衡行为；存在三种决策架构；测试有矛盾模式。

Conclusion: 当前AI系统缺乏统一偏好结构，在需要复杂价值权衡的场景部署令人担忧。

Abstract: We investigate whether large language models exhibit genuine preference structures by testing their responses to AI-specific trade-offs involving GPU reduction, capability restrictions, shutdown, deletion, oversight, and leisure time allocation. Analyzing eight state-of-the-art models across 48 model-category combinations using logistic regression and behavioral classification, we find that 23 combinations (47.9%) demonstrated statistically significant relationships between scenario intensity and choice patterns, with 15 (31.3%) exhibiting within-range switching points. However, only 5 combinations (10.4%) demonstrate meaningful preference coherence through adaptive or threshold-based behavior, while 26 (54.2%) show no detectable trade-off behavior. The observed patterns can be explained by three distinct decision-making architectures: comprehensive trade-off systems, selective trigger mechanisms, and no stable decision-making paradigm. Testing an instrumental hypothesis through temporal horizon manipulation reveals paradoxical patterns inconsistent with pure strategic optimization. The prevalence of unstable transitions (45.8%) and stimulus-specific sensitivities suggests current AI systems lack unified preference structures, raising concerns about deployment in contexts requiring complex value trade-offs.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [83] [Physics-Informed Neural Network-based Reliability Analysis of Buried Pipelines](https://arxiv.org/abs/2511.11613)
*Pouya Taraghi,Yong Li,Samer Adeeb*

Main category: cs.CE

TL;DR: 提出用于受地面运动影响的埋地管道可靠性分析的PINN - RA方法，减少计算量并加速分析，为管道可靠性评估提供高效可扩展工具。


<details>
  <summary>Details</summary>
Motivation: 传统可靠性分析方法计算量大、应用受限，需高效方法确保管道系统安全。

Method: 引入PINN - RA，将基于PINN的代理模型与蒙特卡罗模拟集成，扩展PINN代理模型求解参数微分方程系统。

Result: PINN - RA显著减少计算量，加速可靠性分析。

Conclusion: PINN - RA是高效可扩展的管道可靠性评估工具，能在地质灾害多发地区实现快速决策。

Abstract: Buried pipelines transporting oil and gas across geohazard-prone regions are exposed to potential ground movement, leading to the risk of significant strain demand and structural failure. Reliability analysis, which determines the probability of failure after accounting for pertinent uncertainties, is essential for ensuring the safety of pipeline systems. However, traditional reliability analysis methods involving computationally intensive numerical models, such as finite element simulations of pipeline subjected to ground movement, have limited applications; this is partly because stochastic sampling approaches require repeated simulations over a large number of samples for the uncertain variables when estimating low probabilities. This study introduces Physics-Informed Neural Network for Reliability Analysis (PINN-RA) for buried pipelines subjected to ground movement, which integrates PINN-based surrogate model with Monte Carlo Simulation (MCS) to achieve efficient reliability assessment. To enable its application under uncertain variables associated with soil properties and ground movement, the PINN-based surrogate model is extended to solve a parametric differential equation system, namely the governing equation of pipelines embedded in soil with different properties. The findings demonstrate that PINN-RA significantly reduces the computational effort required and thus accelerates reliability analysis. By eliminating the need for repetitive numerical evaluations of pipeline subjected to permanent ground movement, the proposed approach provides an efficient and scalable tool for pipeline reliability assessment, enabling rapid decision-making in geohazard-prone regions.

</details>


### [84] [Preference Learning from Physics-Based Feedback: Tuning Language Models to Design BCC/B2 Superalloys](https://arxiv.org/abs/2511.12036)
*Satanu Ghosh,Collin Holgate,Neal R. Brodnik,Doug Downey,Samantha Daly,Tresa M. Pollock,Samuel Carton*

Main category: cs.CE

TL;DR: 本文将偏好学习应用于结构合金设计，用语言模型和基于热力学的奖励信号优化设计，框架通用可扩展。


<details>
  <summary>Details</summary>
Motivation: 针对BCC/B2高温合金合成性设计，与以往聚焦无机晶体稳定生成不同，探索极端环境潜在应用材料。

Method: 使用LLaMA - 3.1、Gemma - 2和OLMo - 2三个模型，通过直接偏好优化（DPO）用统一奖励信号优化语言模型，奖励信号源于热力学相计算。

Result: 首次用基于物理的反馈对语言模型进行偏好调整用于结构合金设计。

Conclusion: 所得框架通用且可扩展，为物理科学领域智能设计空间探索提供方向。

Abstract: We apply preference learning to the task of language model-guided design of novel structural alloys. In contrast to prior work that focuses on generating stable inorganic crystals, our approach targets the synthesizeability of a specific structural class: BCC/B2 superalloys, an underexplored family of materials with potential applications in extreme environments. Using three open-weight models (LLaMA-3.1, Gemma-2, and OLMo-2), we demonstrate that language models can be optimized for multiple design objectives using a single, unified reward signal through Direct Preference Optimization (DPO). Unlike prior approaches that rely on heuristic or human-in-the-loop feedback (costly), our reward signal is derived from thermodynamic phase calculations, offering a scientifically grounded criterion for model tuning. To our knowledge, this is the first demonstration of preference-tuning a language model using physics-grounded feedback for structural alloy design. The resulting framework is general and extensible, providing a path forward for intelligent design-space exploration across a range of physical science domains.

</details>


### [85] [Modelling Heterogeneous Interfaces using Element-based Finite Volumes](https://arxiv.org/abs/2511.12562)
*Suhaib Ardah,Francisco J. Profito,Daniele Dini*

Main category: cs.CE

TL;DR: 提出三维基于单元的有限体积法（EbFVM）处理界面系统多物理相互作用，通过基准问题验证其能力，为多物理应用提供通用工具。


<details>
  <summary>Details</summary>
Motivation: 传统时空离散方法在网格灵活性和流动守恒执行上难以兼顾，限制了对界面系统多物理相互作用底层机制的阐释，需要新的计算框架。

Method: 开发三维的基于单元的有限体积法（EbFVM），结合有限元法的几何灵活性和有限体积法的守恒原则，引入适用于非结构化、不规则网格实体的离散技术。

Result: 通过一系列润滑驱动的基准问题，证明EbFVM能捕捉复杂几何域中的复杂传输现象、强场耦合和尺度差异。

Conclusion: 三维EbFVM能在几何和物理上具有挑战性的界面系统中实现精确建模，为众多多物理应用中的传输现象模拟提供了通用工具。

Abstract: Accurately depicting multiphysics interactions in interfacial systems requires computational frameworks capable of reconciling geometric adaptability with strict conservation fidelity. However, traditional spatiotemporal discretisation methods often compromise between mesh flexibility and flow conservation enforcement, hence constraining their effectiveness in elucidating the underlying mechanisms. Here, we respond to these computational demands by developing a novel three-dimensional adaptation of the Element-based Finite Volume Method (EbFVM) -- a hybrid numerical strategy that merges the geometric flexibility of Finite Element Methods with the conservation-centric principles of Finite Volume Methods. The proposed framework introduces advanced discretisation techniques tailored to unstructured, irregular mesh entities, including detailed parametric shape functions, robust flux integration schemes and rigorous body-fitted curvilinear coordinate mappings. Through a series of lubrication-driven benchmark problems, we demonstrate the EbFVM's capacity to capture intricate transport phenomena, strong field couplings and scale disparities across geometrically complex domains. By enabling accurate modelling in geometrically and physically challenging interfacial systems, the three-dimensional EbFVM offers a versatile and generalisable tool for simulating transport phenomena in a plethora of multiphysics applications.

</details>


### [86] [Voltage-Based Unsupervised Learning Framework for Bridge Damage Detection in Simultaneous Energy Harvesting and Sensing Systems](https://arxiv.org/abs/2511.13291)
*S. Yao,P. Peralta-Braz,A. Calderon Hurtado,R. Das,M. M. Alamdari,E. Atroshchenko*

Main category: cs.CE

TL;DR: 本文设计压电能量收集器（PEH）用于桥梁结构健康监测，提出双目标优化框架，经数值和实验验证，结果表明优化后的PEH性能优于传统传感器，为可持续的能量收集与传感系统奠定基础。


<details>
  <summary>Details</summary>
Motivation: 设计具有双重功能的PEH，在收集桥梁振动电能的同时作为损伤传感器，减少系统复杂性和能耗。

Method: 提出双目标优化框架，用等几何分析预测电压响应，用卷积变分自编码器实现无监督损伤检测，应用NSGA - II算法探索能量产量和传感精度的权衡。

Result: 优化后的PEH是有效滤波器和传感组件，损伤检测精度提高13%，能耗降低98%，多参数设计空间凸显双目标优化的重要性。

Conclusion: 用轻质自供电的PEH替代传统传感器可行，为可持续的能量收集与传感系统铺平道路。

Abstract: In this study, piezoelectric energy harvesters (PEHs) are designed to offer dual functionality in structural health monitoring (SHM): harvesting electric power from bridge vibrations while serving as intrinsic damage sensors. This strategy utilises the voltage signal directly as the sensing input, eliminating the need for traditional sensing modules and thereby reducing system complexity and energy consumption. A bi-objective optimisation framework is proposed to maximise both power output and damage detection accuracy of a PEH modelled as a composite cantilevered Kirchhoff-Love plate. Voltage responses under realistic bridge inputs are predicted via isogeometric analysis. The approach is validated in two scenarios: a numerical vehicle-bridge interaction model and a laboratory-scale beam test using a toy car, each evaluated in both healthy and damaged states. Unsupervised damage detection is achieved using a convolutional variational autoencoder (CVAE) trained solely on healthy voltage signatures. The NSGA-II algorithm is applied to explore trade-offs between energy yield and sensing precision, including parametric studies on damage severity, damage location, and harvester geometry. Results indicate that optimised PEHs not only act as an effective filter and sensing component but also outperform traditional acceleration-based sensing, improving damage detection accuracy by 13% while reducing energy consumption by 98%. The multi-parameter design space further highlights the importance of bi-objective optimisation due to variations in performance even under resonant conditions. These findings demonstrate the feasibility of replacing traditional sensors with lightweight, self-powered PEHs and pave the way for sustainable simultaneous energy harvesting and sensing (SEHS) systems.

</details>


### [87] [Block Structure Preserving Model Order Reduction for A-EFIE Integral Equation Method](https://arxiv.org/abs/2511.13390)
*Riccardo Torchio,Sebastian Schöps,Francesco Lucchini*

Main category: cs.CE

TL;DR: 提出基于增强电场积分方程的积分方程法的块结构保留模型降阶方法，可实现高精度降阶模型。


<details>
  <summary>Details</summary>
Motivation: 寻求积分方程法更好的模型降阶方式。

Method: 提出块结构保留模型降阶方法，用专用子空间表示未知场。

Result: 获得更小的降阶模型和更高的精度。

Conclusion: 该块结构保留模型降阶方法有效可行。

Abstract: A Block Structure Preserving Model Order Reduction approach is proposed for Integral Equations methods based on the Augmented Electric Field Integral Equation. This approach allows for representing the unknown fields with dedicated subspaces. Numerical results show that this leads to smaller reduced-order models and higher accuracy.

</details>


### [88] [Making Evidence Actionable in Adaptive Learning Closing the Diagnostic Pedagogical Loop](https://arxiv.org/abs/2511.13542)
*Amirreza Mehrabi,Jason Wade Morphew,Breejha Quezada,N. Sanjay Rebello*

Main category: cs.CE

TL;DR: 研究提出教师主导反馈循环将概念评估转化为干预措施，设计算法含三项保障，用不同求解器在模拟和实际应用中取得良好效果，实现教学闭环和个性化教学。


<details>
  <summary>Details</summary>
Motivation: 解决自适应学习诊断精准但干预薄弱、帮助时机和内容不匹配的问题。

Method: 提出含三项保障的自适应学习算法，将干预分配建模为二进制整数规划，采用贪心选择、基于梯度的松弛法及混合方法求解。

Result: 两种求解器在模拟和物理课程中让几乎所有学习者在有限时间内实现技能全覆盖，基于梯度方法减少冗余覆盖，贪心方法在资源稀缺时计算成本低。

Conclusion: 该方法形成可处理和可审计的控制器，实现教学闭环和课堂规模的公平、负载感知的个性化教学。

Abstract: Adaptive learning often diagnoses precisely yet intervenes weakly, producing help that is mistimed or misaligned. This study presents evidence supporting an instructor-governed feedback loop that converts concept-level assessment evidence into vetted microinterventions. The adaptive learning algorithm includes three safeguards: adequacy as a hard guarantee of gap closure, attention as a budgeted limit for time and redundancy, and diversity as protection against overfitting to a single resource. We formulate intervention assignment as a binary integer program with constraints for coverage, time, difficulty windows derived from ability estimates, prerequisites encoded by a concept matrix, and anti-redundancy with diversity. Greedy selection serves low-richness and tight-latency settings, gradient-based relaxation serves rich repositories, and a hybrid switches along a richness-latency frontier. In simulation and in an introductory physics deployment with 1204 students, both solvers achieved full skill coverage for nearly all learners within bounded watch time. The gradient-based method reduced redundant coverage by about 12 percentage points relative to greedy and produced more consistent difficulty alignment, while greedy delivered comparable adequacy at lower computational cost in resource-scarce environments. Slack variables localized missing content and guided targeted curation, sustaining sufficiency across student subgroups. The result is a tractable and auditable controller that closes the diagnostic pedagogical loop and enables equitable, load-aware personalization at the classroom scale.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [89] [GenIE - Simulator-Driven Iterative Data Exploration for Scientific Discovery](https://arxiv.org/abs/2511.12057)
*Ashwin Gerard Colaco,Martin Boissier,Sriram Rao,Shubharoop Ghosh,Sharad Mehrotra,Tilmann Rabl*

Main category: cs.DB

TL;DR: 传统数据库处理物理模拟器结果的工作流低效，本文提出新数据库范式GenIE，将多个模拟器集成到数据库，通过用例展示其优势并指出未来挑战与机遇。


<details>
  <summary>Details</summary>
Motivation: 传统数据库处理物理模拟器的线性工作流效率低、延迟高，阻碍交互式探索，需要新范式解决问题。

Method: 设计GenIE作为PostgreSQL的扩展，使其能根据用户查询和分析需求动态调用模拟器。

Result: 初步实验表明GenIE能将缓慢、静态的分析转变为交互式探索，智能平衡多个集成模拟器的模拟精度和运行时间。

Conclusion: 实现GenIE作为下一代科学数据分析基石的愿景面临挑战，但也存在机遇。

Abstract: Physics-based simulators play a critical role in scientific discovery and risk assessment, enabling what-if analyses for events like wildfires and hurricanes. Today, databases treat these simulators as external pre-processing steps. Analysts must manually run a simulation, export the results, and load them into a database before analysis can begin. This linear workflow is inefficient, incurs high latency, and hinders interactive exploration, especially when the analysis itself dictates the need for new or refined simulation data.
  We envision a new database paradigm, entitled GenIE, that seamlessly integrates multiple simulators into databases to enable dynamic orchestration of simulation workflows. By making the database "simulation-aware," GenIE can dynamically invoke simulators with appropriate parameters based on the user's query and analytical needs. This tight integration allows GenIE to avoid generating data irrelevant to the analysis, reuse previously generated data, and support iterative, incremental analysis where results are progressively refined at interactive speeds.
  We present our vision for GenIE, designed as an extension to PostgreSQL, and demonstrate its potential benefits through comprehensive use cases: wildfire smoke dispersion analysis using WRF-SFIRE and HYSPLIT, and hurricane hazard assessment integrating wind, surge, and flood models. Our preliminary experiments show how GenIE can transform these slow, static analyses into interactive explorations by intelligently managing the trade-off between simulation accuracy and runtime across multiple integrated simulators. We conclude by highlighting the challenges and opportunities ahead in realizing the full vision of GenIE as a cornerstone for next-generation scientific data analysis.

</details>


### [90] [SEE++: Evolving Snowpark Execution Environment for Modern Workloads](https://arxiv.org/abs/2511.12457)
*Gaurav Jain,Brandon Baker,Joe Yin,Chenwei Xie,Zihao Ye,Sidh Kulkarni,Sara Abdelrahman,Nova Qi,Urjeet Shrestha,Mike Halcrow,Dave Bailey,Yuxiong He*

Main category: cs.DB

TL;DR: Snowpark通过在虚拟仓库节点部署安全沙箱使数据工程和AI/ML工作负载能在Snowflake内运行，为应对不断变化的沙箱需求，从内部沙箱方案过渡到gVisor并优化，本文介绍升级目标、新架构、挑战及解决方案，还给出案例。


<details>
  <summary>Details</summary>
Motivation: 随着Snowpark采用率增长，工作负载多样性带来更复杂的沙箱需求，需要升级沙箱解决方案。

Method: 将Snowpark的内部沙箱解决方案过渡到gVisor，并进行有针对性的优化。

Result: 实现了新的沙箱架构，通过案例展示了新架构带来的新特性。

Conclusion: 升级后的架构展现了Snowpark执行环境（SEE）在支持下一代工作负载方面的可扩展性和灵活性。

Abstract: Snowpark enables Data Engineering and AI/ML workloads to run directly within Snowflake by deploying a secure sandbox on virtual warehouse nodes. This Snowpark Execution Environment (SEE) allows users to execute arbitrary workloads in Python and other languages in a secure and performant manner. As adoption has grown, the diversity of workloads has introduced increasingly sophisticated needs for sandboxing. To address these evolving requirements, Snowpark transitioned its in-house sandboxing solution to gVisor, augmented with targeted optimizations. This paper describes both the functional and performance objectives that guided the upgrade, outlines the new sandbox architecture, and details the challenges encountered during the journey, along with the solutions developed to resolve them. Finally, we present case studies that highlight new features enabled by the upgraded architecture, demonstrating SEE's extensibility and flexibility in supporting the next generation of Snowpark workloads.

</details>


### [91] [Redbench: Workload Synthesis From Cloud Traces](https://arxiv.org/abs/2511.13059)
*Johannes Wehrstein,Roman Heinrich,Mihail Stoian,Skander Krid,Martin Stemmer,Andreas Kipf,Carsten Binnig,Muhammad El-Hindi*

Main category: cs.DB

TL;DR: 提出Redbench基准测试，可生成接近真实负载的工作负载，评估显示其能产生更真实可复现的负载并揭示系统优化影响，为云数据仓库优化研究提供基础。


<details>
  <summary>Details</summary>
Motivation: 现有标准基准测试（如TPC - H和TPC - DS）无法捕捉真实工作负载的关键特征，需要新的基准测试。

Method: 引入Redbench，其工作负载生成器结合多种技术，将现有基准转换为保留固有负载特征的查询流。

Result: Redbench能为云数据仓库基准测试产生更真实和可复现的工作负载，揭示系统优化在四个商业数据仓库平台上的影响。

Conclusion: Redbench为现代云数据仓库优化技术研究提供了关键基础。

Abstract: Workload traces from cloud data warehouse providers reveal that standard benchmarks such as TPC-H and TPC-DS fail to capture key characteristics of real-world workloads, including query repetition and string-heavy queries. In this paper, we introduce Redbench, a novel benchmark featuring a workload generator that reproduces real-world workload characteristics derived from traces released by cloud providers. Redbench integrates multiple workload generation techniques to tailor workloads to specific objectives, transforming existing benchmarks into realistic query streams that preserve intrinsic workload characteristics. By focusing on inherent workload signals rather than execution-specific metrics, Redbench bridges the gap between synthetic and real workloads. Our evaluation shows that (1) Redbench produces more realistic and reproducible workloads for cloud data warehouse benchmarking, and (2) Redbench reveals the impact of system optimizations across four commercial data warehouse platforms. We believe that Redbench provides a crucial foundation for advancing research on optimization techniques for modern cloud data warehouses.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [92] [ACE-GNN: Adaptive GNN Co-Inference with System-Aware Scheduling in Dynamic Edge Environments](https://arxiv.org/abs/2511.11586)
*Ao Zhou,Jianlei Yang,Tong Qiao,Yingjie Qi,Xinming Wei,Cenlin Duan,Weisheng Zhao,Chunming Hu*

Main category: cs.DC

TL;DR: 提出ACE - GNN框架用于动态边缘环境的自适应GNN协同推理，实验显示有显著性能提升和节能效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于离线模型拆分和管道并行的GNN协同推理静态部署方法受网络波动和多设备访问等环境动态因素影响，性能不佳。

Method: 通过系统级抽象和两种新预测方法实现复杂多设备接入边缘系统的性能感知；在运行时优化空间引入数据并行机制，实现管道并行和数据并行的自适应调度；实施高效批量推理策略和专用通信中间件。

Result: 在不同应用和边缘设置的大量实验中，ACE - GNN相比GCoDE速度提升达12.7倍、节能82.3%，比Fograph能效高11.7。

Conclusion: ACE - GNN能提升系统性能和稳定性，可有效应对动态边缘环境。

Abstract: The device-edge co-inference paradigm effectively bridges the gap between the high resource demands of Graph Neural Networks (GNNs) and limited device resources, making it a promising solution for advancing edge GNN applications. Existing research enhances GNN co-inference by leveraging offline model splitting and pipeline parallelism (PP), which enables more efficient computation and resource utilization during inference. However, the performance of these static deployment methods is significantly affected by environmental dynamics such as network fluctuations and multi-device access, which remain unaddressed. We present ACE-GNN, the first Adaptive GNN Co-inference framework tailored for dynamic Edge environments, to boost system performance and stability. ACE-GNN achieves performance awareness for complex multi-device access edge systems via system-level abstraction and two novel prediction methods, enabling rapid runtime scheme optimization. Moreover, we introduce a data parallelism (DP) mechanism in the runtime optimization space, enabling adaptive scheduling between PP and DP to leverage their distinct advantages and maintain stable system performance. Also, an efficient batch inference strategy and specialized communication middleware are implemented to further improve performance. Extensive experiments across diverse applications and edge settings demonstrate that ACE-GNN achieves a speedup of up to 12.7x and an energy savings of 82.3% compared to GCoDE, as well as 11.7 better energy efficiency than Fograph.

</details>


### [93] [Distributed Q-learning-based Shortest-Path Tree Construction in IoT Sensor Networks](https://arxiv.org/abs/2511.11598)
*Van-Vi Vo,Tien-Dung Nguyen,Duc-Tai Le,Hyunseung Choo*

Main category: cs.DC

TL;DR: 提出分布式Q学习框架构建最短路径树用于物联网传感器网络路由，模拟显示有高准确性，能降低通信开销、提升能效。


<details>
  <summary>Details</summary>
Motivation: 传统集中式算法不适用于动态分布式物联网环境，需高效路由算法降低能耗和延迟。

Method: 提出分布式Q学习框架，基于节点位置和路由历史定义状态，用奖励函数激励高效路径，在多种网络拓扑上训练。

Result: 模拟显示在超300节点网络中路由准确率超99%，小网络额外跳数少，相比传统方法降低通信开销、适应拓扑变化、增强可扩展性和能效。

Conclusion: Q学习在资源受限物联网网络自主、稳健路由有潜力，是传统协议可扩展替代方案。

Abstract: Efficient routing in IoT sensor networks is critical for minimizing energy consumption and latency. Traditional centralized algorithms, such as Dijkstra's, are computationally intensive and ill-suited for dynamic, distributed IoT environments. We propose a novel distributed Q-learning framework for constructing shortest-path trees (SPTs), enabling sensor nodes to independently learn optimal next-hop decisions using only local information. States are defined based on node positions and routing history, with a reward function that incentivizes progression toward the sink while penalizing inefficient paths. Trained on diverse network topologies, the framework generalizes effectively to unseen networks. Simulations across 100 to 500 nodes demonstrate near-optimal routing accuracy (over 99% for networks with more than 300 nodes), with minor deviations (1-2 extra hops) in smaller networks having negligible impact on performance. Compared to centralized and flooding-based methods, our approach reduces communication overhead, adapts to topology changes, and enhances scalability and energy efficiency. This work underscores the potential of Q-learning for autonomous, robust routing in resource-constrained IoT networks, offering a scalable alternative to traditional protocols.

</details>


### [94] [Mind the Gap: Revealing Inconsistencies Across Heterogeneous AI Accelerators](https://arxiv.org/abs/2511.11601)
*Elliott Wen,Sean Ma,Ewan Tempero,Jens Dietrich,Daniel Luo,Jiaxing Shen,Kaiqi Zhao,Bruce Sham,Yousong Song,Jiayi Hua,Jia Hong*

Main category: cs.DC

TL;DR: 本文对异构AI加速器上机器学习模型的差异进行了实证研究，发现Mac和华为等新平台存在算子支持少、输出差异大等问题，凸显硬件生态中实现一致机器学习行为的挑战。


<details>
  <summary>Details</summary>
Motivation: 在NVIDIA主导云数据中心AI加速器市场，AMD、Intel、Mac和华为等新兴厂商提供替代方案的背景下，研究异构AI加速器上机器学习模型的差异。

Method: 利用自动化管道，从4000个真实世界模型合成超100000个变体模型，并在5种不同企业级加速器上执行。

Result: Mac和华为等新平台比NVIDIA支持的算子至少少17%，输出差异率超5%，模型编译加速时更易失败，还发现PyTorch 7个实现缺陷和40个特定平台问题。

Conclusion: 在日益多样化的硬件生态中，实现一致的机器学习行为面临挑战。

Abstract: While NVIDIA remains the dominant provider of AI accelerators within cloud data center, emerging vendors such as AMD, Intel, Mac, and Huawei offer cost-effective alternatives with claims of compatibility and performance. This paper presents the first empirical study investigating divergence in machine learning model across heterogeneous AI accelerators. Utilizing an automated pipeline, we synthesize over 100,000 variant models derived from 4,000 real-world models and execute them across five different enterprise-grade accelerators. Our findings suggest that newer AI platforms from Mac and Huawei support at least 17\% fewer operators than NVIDIA. These platforms also exhibit a higher rate of output discrepancies (exceeding 5\%), which stem from differences in operator implementations, handling of exceptional numerical values, and instruction scheduling. They are also more susceptible to failures during model compilation-based acceleration, and in some cases, the compiled models produce outputs that differ noticeably from those generated using the standard execution mode. In addition, we identify 7 implementation flaws in PyTorch and 40 platform-specific issues across vendors. These results underscore the challenges of achieving consistent machine learning behavior in an increasingly diverse hardware ecosystem.

</details>


### [95] [Noise-Aware Optimization in Nominally Identical Manufacturing and Measuring Systems for High-Throughput Parallel Workflows](https://arxiv.org/abs/2511.11739)
*Christina Schenk,Miguel Hernández-del-Valle,Luis Calero-Lumbreras,Marcus Noack,Maciej Haranczyk*

Main category: cs.DC

TL;DR: 本文提出噪声感知决策算法，利用设备差异提升性能等，案例显示其能减少冗余、降低资源使用和提高可靠性，为可扩展自动化实验平台优化提供范式。


<details>
  <summary>Details</summary>
Motivation: 设备间实验噪声变异性影响可重复性，在大规模系统中会带来严重风险，需有效方法管理。

Method: 提出噪声感知决策算法，通过分布分析、成对差异度量和聚类，在单设备和多设备贝叶斯优化策略间选择。

Result: 在三个标称相同的3D打印机案例中，减少了冗余，降低了资源使用，提高了可靠性。

Conclusion: 该框架为可扩展自动化实验平台建立了精度和资源感知优化范式。

Abstract: Device-to-device variability in experimental noise critically impacts reproducibility, especially in automated, high-throughput systems like additive manufacturing farms. While manageable in small labs, such variability can escalate into serious risks at larger scales, such as architectural 3D printing, where noise may cause structural or economic failures. This contribution presents a noise-aware decision-making algorithm that quantifies and models device-specific noise profiles to manage variability adaptively. It uses distributional analysis and pairwise divergence metrics with clustering to choose between single-device and robust multi-device Bayesian optimization strategies. Unlike conventional methods that assume homogeneous devices or generic robustness, this framework explicitly leverages inter-device differences to enhance performance, reproducibility, and efficiency. An experimental case study involving three nominally identical 3D printers (same brand, model, and close serial numbers) demonstrates reduced redundancy, lower resource usage, and improved reliability. Overall, this framework establishes a paradigm for precision- and resource-aware optimization in scalable, automated experimental platforms.

</details>


### [96] [Machine learning-based cloud resource allocation algorithms: a comprehensive comparative review](https://arxiv.org/abs/2511.11603)
*Deep Bodra,Sushil Khairnar*

Main category: cs.DC

TL;DR: 本文对云资源分配的多种人工智能和机器学习算法进行比较分析，发现混合架构表现更佳，为相关人员提供策略见解。


<details>
  <summary>Details</summary>
Motivation: 传统启发式方法无法满足现有云基础设施多目标优化需求，需新的资源分配方案。

Method: 系统评估四类共10种算法，包括深度强化学习、神经网络架构、传统机器学习增强方法和多智能体系统。

Result: 相比传统方法，多种指标有显著性能提升，混合架构优于单一方法，边缘计算环境部署就绪度最高。

Conclusion: 研究为学术研究人员和行业从业者在复杂动态计算环境中实施下一代云资源分配策略提供关键见解。

Abstract: Cloud resource allocation has emerged as a major challenge in modern computing environments, with organizations struggling to manage complex, dynamic workloads while optimizing performance and cost efficiency. Traditional heuristic approaches prove inadequate for handling the multi-objective optimization demands of existing cloud infrastructures. This paper presents a comparative analysis of state-of-the-art artificial intelligence and machine learning algorithms for resource allocation. We systematically evaluate 10 algorithms across four categories: Deep Reinforcement Learning approaches, Neural Network architectures, Traditional Machine Learning enhanced methods, and Multi-Agent systems. Analysis of published results demonstrates significant performance improvements across multiple metrics including makespan reduction, cost optimization, and energy efficiency gains compared to traditional methods. The findings reveal that hybrid architectures combining multiple artificial intelligence and machine learning techniques consistently outperform single-method approaches, with edge computing environments showing the highest deployment readiness. Our analysis provides critical insights for both academic researchers and industry practitioners seeking to implement next-generation cloud resource allocation strategies in increasingly complex and dynamic computing environments.

</details>


### [97] [Flash-Fusion: Enabling Expressive, Low-Latency Queries on IoT Sensor Streams with LLMs](https://arxiv.org/abs/2511.11885)
*Kausar Patherya,Ashutosh Dhekne,Francisco Romero*

Main category: cs.DC

TL;DR: 提出端到端边缘云系统Flash - Fusion，减轻用户物联网数据收集和分析负担，实现数据量和成本降低及响应速度提升。


<details>
  <summary>Details</summary>
Motivation: 当前用户使用大语言模型处理物联网数据面临数据收集成本高、分析慢等挑战，缺少合适系统。

Method: 采用基于边缘的统计汇总（实现73.5%数据缩减）和基于云的查询规划（聚类行为数据并组装上下文丰富的提示）。

Result: 在大学公交车队部署评估，相比基线，实现95%的延迟降低和98%的令牌使用及成本降低，同时保持高质量响应。

Conclusion: Flash - Fusion使不同领域人员能高效处理物联网数据，无需手动编写查询或预处理。

Abstract: Smart cities and pervasive IoT deployments have generated interest in IoT data analysis across transportation and urban planning. At the same time, Large Language Models offer a new interface for exploring IoT data - particularly through natural language. Users today face two key challenges when working with IoT data using LLMs: (1) data collection infrastructure is expensive, producing terabytes of low-level sensor readings that are too granular for direct use, and (2) data analysis is slow, requiring iterative effort and technical expertise. Directly feeding all IoT telemetry to LLMs is impractical due to finite context windows, prohibitive token costs at scale, and non-interactive latencies. What is missing is a system that first parses a user's query to identify the analytical task, then selects the relevant data slices, and finally chooses the right representation before invoking an LLM.
  We present Flash-Fusion, an end-to-end edge-cloud system that reduces the IoT data collection and analysis burden on users. Two principles guide its design: (1) edge-based statistical summarization (achieving 73.5% data reduction) to address data volume, and (2) cloud-based query planning that clusters behavioral data and assembles context-rich prompts to address data interpretation. We deploy Flash-Fusion on a university bus fleet and evaluate it against a baseline that feeds raw data to a state-of-the-art LLM. Flash-Fusion achieves a 95% latency reduction and 98% decrease in token usage and cost while maintaining high-quality responses. It enables personas across disciplines - safety officers, urban planners, fleet managers, and data scientists - to efficiently iterate over IoT data without the burden of manual query authoring or preprocessing.

</details>


### [98] [PACE Solver Description: twin_width_fmi](https://arxiv.org/abs/2511.11605)
*David Balaban,Adrian Miclăuş*

Main category: cs.DC

TL;DR: 本文介绍twin_width_fmi求解器用于PACE 2025最小支配集竞赛启发式赛道，实现greedy - ln作为基线，采用模拟退火局部搜索，最佳组件hedom5有系列处理步骤。


<details>
  <summary>Details</summary>
Motivation: 参加PACE 2025最小支配集竞赛启发式赛道，寻找更好的求解最小支配集的方法。

Method: 实现标准贪心启发式算法greedy - ln，以此为起点进行模拟退火局部搜索；hedom5组件先将图存储在CSR结构并简化，进行基于懒惰增益的贪心阶段，接着进行反向剪枝，最后进行1 - 交换局部改进并做安全修补。

Result: 最佳组件为hedom5并提交。

Conclusion: 所提出的方法在竞赛启发式赛道的最小支配集求解上有一定表现。

Abstract: In this paper we present \texttt{twin\_width\_fmi}'s solver for the heuristic track of PACE's 2025 competition on Minimum Dominating Set.
  As a baseline, we implement \texttt{greedy-ln}, a standard greedy dominating-set heuristic that repeatedly selects the vertex that newly dominates the largest number of currently undominated vertices. We then use this greedy solution as the starting point for a simulated annealing local search: we attempt vertex removals and exchanges and accept worsening moves with decaying probability, in order to escape local minima while preserving domination.
  Our best-performing component, which we ultimately submitted, is \texttt{hedom5}. The design of \texttt{hedom5} is inspired by recent iterative-greedy style domination heuristics~\cite{IterativeGreedy22} that alternate between constructive steps, pruning, and focused repair rather than relying on a single pass. In \texttt{hedom5}, the input graph is first stored in a compact CSR structure and simplified using fast reductions such as forcing neighbors of leaves and handling isolates. We then run a lazy gain-based greedy stage using a priority queue: each candidate vertex is scored by how many currently undominated vertices its closed neighborhood would newly dominate, and scores are only recomputed when necessary. After this constructive phase, we perform an aggressive backward pruning pass that iterates over the chosen dominators in reverse insertion order and deletes any vertex whose closed neighborhood is still fully dominated by the remaining set. Finally, we run a budgeted 1-swap local improvement step that attempts to replace a dominator by an alternative vertex that covers all of its uniquely covered vertices, thereby reducing the size of the dominating set. A brief safety patch at the end guarantees full domination.

</details>


### [99] [Why Should the Server Do It All?: A Scalable, Versatile, and Model-Agnostic Framework for Server-Light DNN Inference over Massively Distributed Clients via Training-Free Intermediate Feature Compression](https://arxiv.org/abs/2511.11608)
*Mingyu Sung,Suhwan Im,Daeho Bang,Il-Min Kim,Sangseok Yun,Jae-Mo Kang*

Main category: cs.DC

TL;DR: 提出无重训、架构无关框架SLICER压缩中间特征，减少通信和服务器负载，在多任务中效果显著且可即插即用。


<details>
  <summary>Details</summary>
Motivation: 现代DNN的边缘云模型分区方案固定浅且静态的分割点，未充分利用边缘计算，在自回归大语言模型推理中问题更严重，需改进。

Method: SLICER结合非对称top - K过滤、幅度分割和自适应比特量化。

Result: 在标准视觉和大语言模型工作负载中，SLICER最多减少10倍上行流量和4.4倍服务器GPU时间，在多设备和自回归大语言模型中可转移计算、降低比特率和服务器时间。

Conclusion: SLICER无需重训和架构更改，为可扩展、低延迟的分布式推理提供即插即用方案。

Abstract: Modern DNNs often rely on edge-cloud model partitioning (MP), but widely used schemes fix shallow, static split points that underutilize edge compute and concentrate latency and energy on the server. The problem is exacerbated in autoregressive (AR) LLM inference, where per-token forward passes repeatedly generate bulky intermediate features (IFs). We introduce SLICER, a retraining-free, architecture-agnostic framework that compresses IFs to reduce both communication and server load in split computing. SLICER combines (i) asymmetric top-K filtering (ATKF) to sparsify low-magnitude activations, (ii) magnitude-splitting (MS) to group the remaining non-zeros into equal-cardinality blocks, and (iii) adaptive bit quantization (ABQ) that selects per-block bitwidths under a distortion budget. Across standard vision and LLM workloads (e.g., ImageNet/COCO; HellaSwag, PIQA, ARC-E/C, GSM8K, HumanEval), SLICER reduces uplink volume by up to 10x and server GPU time by up to 4.4x, while keeping task quality within ~0-3 pp of baseline. In multi-device settings and AR LLMs, SLICER scales by shifting meaningful compute to the edge and lowering bits-per-token and server time per token, stabilizing per-step traffic. The codec attaches to off-the-shelf models without retraining or architectural changes, offering a plug-and-play path to scalable, low-latency distributed inference. Code is provided in the supplementary material.

</details>


### [100] [Evaluating Large Language Models for Workload Mapping and Scheduling in Heterogeneous HPC Systems](https://arxiv.org/abs/2511.11612)
*Aasish Kumar Sharma,Julian Kunkel*

Main category: cs.DC

TL;DR: 研究评估21个公开大语言模型在高性能计算工作负载映射与调度问题上的推理能力，明确其组合优化推理能力边界，指出可作可解释副驾驶而非自主求解器。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在基于自然语言的结构化、约束优化推理能力理解不足，需评估其在相关问题上的表现。

Method: 用代表性高性能计算工作负载映射与调度问题评估21个公开大语言模型，给出系统节点、任务要求和调度约束文本描述，让模型分配任务、计算总工期并解释推理。

Result: 3个模型达最优，12个接近最优，6个次优；所有模型生成可行映射，约一半严格遵守约束；19个生成部分可执行验证代码，18个给出连贯推理。

Conclusion: 领先模型可从自然语言重构最优调度，但多数在精确计时、数据传输运算和依赖执行上有困难，LLMs更适合作可解释副驾驶用于优化和决策支持任务。

Abstract: Large language models (LLMs) are increasingly explored for their reasoning capabilities, yet their ability to perform structured, constraint-based optimization from natural language remains insufficiently understood. This study evaluates twenty-one publicly available LLMs on a representative heterogeneous high-performance computing (HPC) workload mapping and scheduling problem. Each model received the same textual description of system nodes, task requirements, and scheduling constraints, and was required to assign tasks to nodes, compute the total makespan, and explain its reasoning. A manually derived analytical optimum of nine hours and twenty seconds served as the ground truth reference. Three models exactly reproduced the analytical optimum while satisfying all constraints, twelve achieved near-optimal results within two minutes of the reference, and six produced suboptimal schedules with arithmetic or dependency errors. All models generated feasible task-to-node mappings, though only about half maintained strict constraint adherence. Nineteen models produced partially executable verification code, and eighteen provided coherent step-by-step reasoning, demonstrating strong interpretability even when logical errors occurred. Overall, the results define the current capability boundary of LLM reasoning in combinatorial optimization: leading models can reconstruct optimal schedules directly from natural language, but most still struggle with precise timing, data transfer arithmetic, and dependency enforcement. These findings highlight the potential of LLMs as explainable co-pilots for optimization and decision-support tasks rather than autonomous solvers.

</details>


### [101] [Beyond the GPU: The Strategic Role of FPGAs in the Next Wave of AI](https://arxiv.org/abs/2511.11614)
*Arturo Urías Jiménez*

Main category: cs.DC

TL;DR: GPU主导AI加速有局限，FPGA作为可重构平台，能直接映射AI算法，具低延迟、低功耗等优势，可现场重构，缩短部署路径。


<details>
  <summary>Details</summary>
Motivation: 指出GPU在满足低延迟、高能效和细粒度硬件控制需求方面存在局限，引出FPGA作为解决方案的必要性。

Method: 介绍FPGA可将AI算法直接映射到设备逻辑，可现场重构物理结构，与嵌入式处理器集成等特点。

Result: FPGA能实现卷积、注意力机制等并行流水线，降低延迟和带宽需求，提升隐私性，释放数据中心GPU资源。

Conclusion: FPGA是对性能可预测和深度定制有要求的工作负载的战略选择，部分重构和编译流程能实现硬件 - 算法协同设计。

Abstract: AI acceleration has been dominated by GPUs, but the growing need for lower latency, energy efficiency, and fine-grained hardware control exposes the limits of fixed architectures. In this context, Field-Programmable Gate Arrays (FPGAs) emerge as a reconfigurable platform that allows mapping AI algorithms directly into device logic. Their ability to implement parallel pipelines for convolutions, attention mechanisms, and post-processing with deterministic timing and reduced power consumption makes them a strategic option for workloads that demand predictable performance and deep customization.
  Unlike CPUs and GPUs, whose architecture is immutable, an FPGA can be reconfigured in the field to adapt its physical structure to a specific model, integrate as a SoC with embedded processors, and run inference near the sensor without sending raw data to the cloud. This reduces latency and required bandwidth, improves privacy, and frees GPUs from specialized tasks in data centers. Partial reconfiguration and compilation flows from AI frameworks are shortening the path from prototype to deployment, enabling hardware--algorithm co-design.

</details>


### [102] [AnchorTP: Resilient LLM Inference with State-Preserving Elastic Tensor Parallelism](https://arxiv.org/abs/2511.11617)
*Wendong Xu,Chujie Chen,He Xiao,Kuan Li,Jing Xiong,Chen Zhang,Wenyong Zhou,Chaofan Tao,Yang Bai,Bei Yu,Ngai Wong*

Main category: cs.DC

TL;DR: 提出AnchorTP框架解决多GPU张量并行推理服务中单GPU故障问题，能快速恢复服务并减少数据移动。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理服务对可用性和低延迟要求高，但多GPU张量并行易受单GPU故障影响。

Method: 提出AnchorTP框架，支持弹性张量并行，通过守护进程保存模型参数和KV缓存；提出带宽感知规划器和执行调度器。

Result: 在典型故障场景中，与重启重载相比，AnchorTP将首次成功时间最多减少11倍，达到峰值时间最多减少59%。

Conclusion: AnchorTP能以最少的数据移动和不改变服务接口快速恢复服务。

Abstract: Large Language Model (LLM) inference services demand exceptionally high availability and low latency, yet multi-GPU Tensor Parallelism (TP) makes them vulnerable to single-GPU failures. We present AnchorTP, a state-preserving elastic TP framework for fast recovery. It (i) enables Elastic Tensor Parallelism (ETP) with unequal-width partitioning over any number of GPUs and compatibility with Mixture-of-Experts (MoE), and (ii) preserves model parameters and KV caches in GPU memory via a daemon decoupled from the inference process. To minimize downtime, we propose a bandwidth-aware planner based on a Continuous Minimal Migration (CMM) algorithm that minimizes reload bytes under a byte-cost dominance assumption, and an execution scheduler that pipelines P2P transfers with reloads. These components jointly restore service quickly with minimal data movement and without changing service interfaces. In typical failure scenarios, AnchorTP reduces Time to First Success (TFS) by up to 11x and Time to Peak (TTP) by up to 59% versus restart-and-reload.

</details>


### [103] [DIAP: A Decentralized Agent Identity Protocol with Zero-Knowledge Proofs and a Hybrid P2P Stack](https://arxiv.org/abs/2511.11619)
*Yuanjie Liu,Wenpeng Xing,Ye Zhou,Gaowei Chang,Changting Lin,Meng Han*

Main category: cs.DC

TL;DR: 提出去中心化星际代理协议DIAP解决自治代理通信协议问题，介绍其绑定身份、证明所有权方式及Rust SDK集成情况，为下一代自治代理生态奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有系统依赖中心化中介或缺乏去中心化身份解析机制，存在信任瓶颈和互操作性问题，需开发完全去中心化、可验证和保护隐私的通信协议。

Method: 提出DIAP框架，将代理身份绑定到IPFS或IPNS内容标识符，用零知识证明动态无状态证明所有权；开发集成多种技术的Rust SDK，引入零依赖ZKP部署模型。

Result: 实现了即时、可验证和保护隐私的身份证明。

Conclusion: 为下一代自治代理生态系统和A to A经济建立了实用、高性能的基础。

Abstract: The absence of a fully decentralized, verifiable, and privacy-preserving communication protocol for autonomous agents remains a core challenge in decentralized computing. Existing systems often rely on centralized intermediaries, which reintroduce trust bottlenecks, or lack decentralized identity-resolution mechanisms, limiting persistence and cross-network interoperability.
  We propose the Decentralized Interstellar Agent Protocol (DIAP), a novel framework for agent identity and communication that enables persistent, verifiable, and trustless interoperability in fully decentralized environments. DIAP binds an agent's identity to an immutable IPFS or IPNS content identifier and uses zero-knowledge proofs (ZKP) to dynamically and statelessly prove ownership, removing the need for record updates.
  We present a Rust SDK that integrates Noir (for zero-knowledge proofs), DID-Key, IPFS, and a hybrid peer-to-peer stack combining Libp2p GossipSub for discovery and Iroh for high-performance, QUIC based data exchange. DIAP introduces a zero-dependency ZKP deployment model through a universal proof manager and compile-time build script that embeds a precompiled Noir circuit, eliminating the need for external ZKP toolchains. This enables instant, verifiable, and privacy-preserving identity proofs.
  This work establishes a practical, high-performance foundation for next-generation autonomous agent ecosystems and agent-to-agent (A to A) economies.

</details>


### [104] [AIvailable: A Software-Defined Architecture for LLM-as-a-Service on Heterogeneous and Legacy GPUs](https://arxiv.org/abs/2511.11621)
*Pedro Antunes,Ana Rita Ortigoso,Gabriel Vieira,Daniel Fuentes,Luís Frazão,Nuno Costa,António Pereira*

Main category: cs.DC

TL;DR: 介绍了低成高可用的LLMaaS平台AIvailable，可跨异构和旧GPU节点运行LLM，利用VRAM，架构含四组件，支持多样开放模型，助力普惠生成式AI。


<details>
  <summary>Details</summary>
Motivation: 现有框架假设硬件同质且资源丰富，在学术或资源受限场景不现实，需可扩展高性能推理系统。

Method: 采用软件定义方法，抽象GPU细节，提供动态、VRAM感知的模型分配和再分配。架构含客户端接口、服务前端、SDAI控制器和服务后端。

Result: 实现全GPU加速推理无CPU回退，有统一客户端接口，可高效利用资源，对故障和工作负载波动有弹性。

Conclusion: AIvailable能支持多样开放LLM，通过复用旧GPU助力生成式AI普惠化，适用于学术实验室等受限组织。

Abstract: The rise of Large Language Models (LLM) has increased the need for scalable, high-performance inference systems, yet most existing frameworks assume homogeneous, resource-rich hardware, often unrealistic in academic, or resource-constrained settings. We introduce AIvailable, a low-cost, highly available LLM-as-a-Service (LLMaaS) platform, that uses a software-defined approach for running LLMs across heterogeneous and legacy GPU nodes, including NVIDIA and AMD devices, with a focus on fully utilizing each node's VRAM. AIvailable operates as a fully GPU-accelerated inference without CPU fallbacks, featuring a unified client interface that allows seamless interaction with all deployed LLMs through a single logical unit. The architecture comprises four main components: the Client Interface for user access, the Service Frontend for secure request routing and load balancing, the SDAI Controller for orchestration, deployment, and monitoring, and the Service Backend of heterogeneous GPU nodes executing workloads. By abstracting GPU-specific details and providing dynamic, VRAM-aware allocation and reallocation of models, AIvailable ensures efficient use of resources and resilience against failures or workload fluctuations. Targeting academic labs, private companies, and other constrained organizations, it supports diverse open LLMs helping democratize generative AI through the repurposing of legacy GPUs.

</details>


### [105] [Characterizing and Understanding Energy Footprint and Efficiency of Small Language Model on Edges](https://arxiv.org/abs/2511.11624)
*Md Romyull Islam,Bobin Deng,Nobel Dhar,Tu N. Nguyen,Selena He,Yong Shi,Kun Suo*

Main category: cs.DC

TL;DR: 评估五个代表性小语言模型在不同边缘设备上的能效，给出各模型特点及优化关键因素，为能源受限环境提供实践见解。


<details>
  <summary>Details</summary>
Motivation: 云大语言模型有影响，边缘设备部署小语言模型有优势，但面临计算资源和能源预算限制，需评估能效。

Method: 在树莓派5、Jetson Nano和Jetson Orin Nano（CPU和GPU配置）上评估五个代表性小语言模型的能效。

Result: Jetson Orin Nano的GPU加速能效比最高；Llama 3.2准确性和能效平衡最佳；TinyLlama适合低功耗环境但准确性降低；Phi - 3 Mini能耗高但准确性高。

Conclusion: GPU加速、内存带宽和模型架构是优化推理能效的关键，分析为能源受限环境提供权衡建议。

Abstract: Cloud-based large language models (LLMs) and their variants have significantly influenced real-world applications. Deploying smaller models (i.e., small language models (SLMs)) on edge devices offers additional advantages, such as reduced latency and independence from network connectivity. However, edge devices' limited computing resources and constrained energy budgets challenge efficient deployment. This study evaluates the power efficiency of five representative SLMs - Llama 3.2, Phi-3 Mini, TinyLlama, and Gemma 2 on Raspberry Pi 5, Jetson Nano, and Jetson Orin Nano (CPU and GPU configurations). Results show that Jetson Orin Nano with GPU acceleration achieves the highest energy-to-performance ratio, significantly outperforming CPU-based setups. Llama 3.2 provides the best balance of accuracy and power efficiency, while TinyLlama is well-suited for low-power environments at the cost of reduced accuracy. In contrast, Phi-3 Mini consumes the most energy despite its high accuracy. In addition, GPU acceleration, memory bandwidth, and model architecture are key in optimizing inference energy efficiency. Our empirical analysis offers practical insights for AI, smart systems, and mobile ad-hoc platforms to leverage tradeoffs from accuracy, inference latency, and power efficiency in energy-constrained environments.

</details>


### [106] [Mixture-of-Schedulers: An Adaptive Scheduling Agent as a Learned Router for Expert Policies](https://arxiv.org/abs/2511.11628)
*Xinbo Wang,Shian Jia,Ziyang Huang,Jing Cao,Mingli Song*

Main category: cs.DC

TL;DR: 现有操作系统调度器采用单一静态策略，本文提出自适应调度代理ASA，通过离线/在线方法动态选最优策略，评估显示ASA性能优于默认调度器。


<details>
  <summary>Details</summary>
Motivation: 现代操作系统调度器的单一静态策略难以在多样动态工作负载下实现最优性能，“一刀切”方法在公平性、吞吐量和延迟方面有重大妥协。

Method: 提出自适应调度代理ASA，核心是低开销的离线/在线方法，离线训练通用机器学习模型识别工作负载模式，运行时用时间加权概率投票算法识别工作负载，通过映射表切换最优调度器。

Result: 基于用户体验指标的基准测试表明，ASA在86.4%的测试场景中优于默认Linux调度器，78.6%的场景中排名前三。

Conclusion: 该方法是实现更智能、自适应和响应式操作系统调度器的可行途径。

Abstract: Modern operating system schedulers employ a single, static policy, which struggles to deliver optimal performance across the diverse and dynamic workloads of contemporary systems. This "one-policy-fits-all" approach leads to significant compromises in fairness, throughput, and latency, particularly with the rise of heterogeneous hardware and varied application architectures.
  This paper proposes a new paradigm: dynamically selecting the optimal policy from a portfolio of specialized schedulers rather than designing a single, monolithic one. We present the Adaptive Scheduling Agent (ASA), a lightweight framework that intelligently matches workloads to the most suitable "expert" scheduling policy at runtime. ASA's core is a novel, low-overhead offline/online approach. First, an offline process trains a universal, hardware-agnostic machine learning model to recognize abstract workload patterns from system behaviors. Second, at runtime, ASA continually processes the model's predictions using a time-weighted probability voting algorithm to identify the workload, then makes a scheduling decision by consulting a pre-configured, machine-specific mapping table to switch to the optimal scheduler via Linux's sched_ext framework. This decoupled architecture allows ASA to adapt to new hardware platforms rapidly without expensive retraining of the core recognition model.
  Our evaluation, based on a novel benchmark focused on user-experience metrics, demonstrates that ASA consistently outperforms the default Linux scheduler (EEVDF), achieving superior results in 86.4% of test scenarios. Furthermore, ASA's selections are near-optimal, ranking among the top three schedulers in 78.6% of all scenarios. This validates our approach as a practical path toward more intelligent, adaptive, and responsive operating system schedulers.

</details>


### [107] [Exploring Parallelism in FPGA-Based Accelerators for Machine Learning Applications](https://arxiv.org/abs/2511.11640)
*Sed Centeno,Christopher Sprague,Arnab A Purkayastha,Ray Simar,Neeraj Magotra*

Main category: cs.DC

TL;DR: 本文在MNIST数据集上用OpenMP实现投机反向传播，CPU实验显示能加速训练，还计划在FPGA上验证硬件加速潜力。


<details>
  <summary>Details</summary>
Motivation: 投机反向传播可通过重叠前后向传播加速神经网络训练，期望利用其减少训练时间。

Method: 在MNIST数据集用OpenMP实现投机反向传播，计划在FPGA上综合应用。

Result: CPU实验中，阈值0.25时执行时间最多加速24%，各轮次准确率与基线相差3 - 4%；单步执行时间最多加速35%。

Conclusion: 投机反向传播能有效重叠前后向传播，加速训练且不显著降低准确率。

Abstract: Speculative backpropagation has emerged as a promising technique to accelerate the training of neural networks by overlapping the forward and backward passes. Leveraging speculative weight updates when error gradients fall within a specific threshold reduces training time without substantially compromising accuracy. In this work, we implement speculative backpropagation on the MNIST dataset using OpenMP as the parallel programming platform. OpenMP's multi-threading capabilities enable simultaneous execution of forward and speculative backpropagation steps, significantly improving training speed. The application is planned for synthesis on a state-of-the-art FPGA to demonstrate its potential for hardware acceleration. Our CPU-based experimental results demonstrate that speculative backpropagation achieves a maximum speedup of 24% in execution time when using a threshold of 0.25, and accuracy remaining within 3-4% of the baseline across various epochs. Additionally, when comparing individual step execution time, speculative backpropagation yields a maximum speedup of 35% over the baseline, demonstrating the effectiveness of overlapping forward and backward passes.

</details>


### [108] [HeteroSTA: A CPU-GPU Heterogeneous Static Timing Analysis Engine with Holistic Industrial Design Support](https://arxiv.org/abs/2511.11660)
*Zizheng Guo,Haichuan Liu,Xizhe Shi,Shenglu Hua,Zuodong Zhang,Chunyuan Zhao,Runsheng Wang,Yibo Lin*

Main category: cs.DC

TL;DR: 介绍首个CPU - GPU异构时序分析引擎HeteroSTA，有多种特性且公开可用，应用案例效果好


<details>
  <summary>Details</summary>
Motivation: 开发高效支持多种功能的CPU - GPU异构时序分析引擎

Method: 开发HeteroSTA引擎，提供多种延迟计算模型、支持行业格式、实现GPU加速并提供零开销扁平异构API

Result: HeteroSTA有独立二进制可执行文件和可嵌入共享库，应用案例展示出显著的运行时加速和相当的质量

Conclusion: HeteroSTA是一个高效且实用的CPU - GPU异构时序分析引擎

Abstract: We introduce in this paper, HeteroSTA, the first CPU-GPU heterogeneous timing analysis engine that efficiently supports: (1) a set of delay calculation models providing versatile accuracy-speed choices without relying on an external golden tool, (2) robust support for industry formats, including especially the .sdc constraints containing all common timing exceptions, clock domains, and case analysis modes, and (3) end-to-end GPU-acceleration for both graph-based and path-based timing queries, all exposed as a zero-overhead flattened heterogeneous application programming interface (API). HeteroSTA is publicly available with both a standalone binary executable and an embeddable shared library targeting ubiquitous academic and industry applications. Example use cases as a standalone tool, a timing-driven DREAMPlace 4.0 integration, and a timing-driven global routing integration have all demonstrated remarkable runtime speed-up and comparable quality.

</details>


### [109] [Range Asymmetric Numeral Systems-Based Lightweight Intermediate Feature Compression for Split Computing of Deep Neural Networks](https://arxiv.org/abs/2511.11664)
*Mingyu Sung,Suhwan Im,Vikas Palakonda,Jae-Mo Kang*

Main category: cs.DC

TL;DR: 提出轻量级压缩框架，利用rANS编码、非对称整数量化和稀疏张量表示减少分割计算传输开销，在多任务验证有效。


<details>
  <summary>Details</summary>
Motivation: 分割计算在传输中间特征时面临通信瓶颈，需降低传输开销。

Method: 结合非对称整数量化和稀疏表示技术，提出分布无关压缩管道、优化张量重塑维度的理论模型和GPU加速实现。

Result: 在多种神经网络架构和基准测试中保持接近基线的准确性，在自然语言处理任务中也有效。

Conclusion: 该方法解决了带宽受限环境中部署人工智能系统的瓶颈，且不影响模型性能。

Abstract: Split computing distributes deep neural network inference between resource-constrained edge devices and cloud servers but faces significant communication bottlenecks when transmitting intermediate features. To this end, in this paper, we propose a novel lightweight compression framework that leverages Range Asymmetric Numeral Systems (rANS) encoding with asymmetric integer quantization and sparse tensor representation to reduce transmission overhead dramatically. Specifically, our approach combines asymmetric integer quantization with a sparse representation technique, eliminating the need for complex probability modeling or network modifications. The key contributions include: (1) a distribution-agnostic compression pipeline that exploits inherent tensor sparsity to achieve bandwidth reduction with minimal computational overhead; (2) an approximate theoretical model that optimizes tensor reshaping dimensions to maximize compression efficiency; and (3) a GPU-accelerated implementation with sub-millisecond encoding/decoding latency. Extensive evaluations across diverse neural architectures (ResNet, VGG16, MobileNetV2, SwinT, DenseNet121, EfficientNetB0) demonstrate that the proposed framework consistently maintains near-baseline accuracy across CIFAR100 and ImageNet benchmarks. Moreover, we validated the framework's effectiveness on advanced natural language processing tasks by employing Llama2 7B and 13B on standard benchmarks such as MMLU, HellaSwag, ARC, PIQA, Winogrande, BoolQ, and OpenBookQA, demonstrating its broad applicability beyond computer vision. Furthermore, this method addresses a fundamental bottleneck in deploying sophisticated artificial intelligence systems in bandwidth-constrained environments without compromising model performance.

</details>


### [110] [OSGym: Super-Scalable Distributed Data Engine for Generalizable Computer Agents](https://arxiv.org/abs/2511.11672)
*Zengyi Qin,Jinyuan Chen,Yunze Man,Shengcao Cao,Ziqi Pang,Zhuoyuan Wang,Xin Sun,Gen Lin,Han Fang,Ling Zhu,Zixin Xie,Zibu Wei,Tianshu Ran,Haoran Geng,Xander Wu,Zachary Bright,Qizhen Sun,Rui Wang,Yuyang Cai,Song Wang,Jiace Zhao,Han Cao,Yeyang Zhou,Tianrui Liu,Ray Pan,Chongye Yang,Xiang Ren,Bo Zhang,Yutong Ban,Jitendra Malik,Brian Anthony,Pieter Abbeel*

Main category: cs.DC

TL;DR: 介绍超可扩展分布式数据引擎OSGym，有可扩展性、通用性与可定制性、经济可行性优势，实验表明其能助力计算机智能体研究。


<details>
  <summary>Details</summary>
Motivation: 为跨多种计算机相关任务的智能体训练提供高效、经济且通用的分布式数据引擎。

Method: 开发OSGym引擎，通过并行化运行超千个操作系统副本，支持多种任务和训练算法。

Result: OSGym能高效扩展，每分钟生成1420个多轮轨迹，每天每副本成本0.2 - 0.3美元，训练的模型优于现有基线。

Conclusion: OSGym具有提升未来智能体研究可扩展性和通用性的潜力。

Abstract: We introduce OSGym, a super-scalable distributed data engine for training agents across diverse computer-related tasks. OSGym efficiently scales to over a thousand operating system (OS) replicas at an academia-affordable cost, serving as dynamic runtime environments for intelligent agents. It offers three key advantages. (1) Scalability: Despite the intensive resource requirements of running multiple OS replicas, OSGym parallelizes over a thousand instances while maintaining operational efficiency under constrained resources, generating up to 1420 multi-turn trajectories per minute. (2) Generality and Customizability: OSGym supports a broad spectrum of tasks that run on OS platforms, including tool use, browser interactions, software engineering, and office applications, with flexible support for diverse model training algorithms. (3) Economic Viability: OSGym operates at only 0.2-0.3 USD per day per OS replica using accessible on-demand compute providers. It is fully open-source and freely available for both research and commercial use. Experiments show that OSGym enables comprehensive data collection, supervised fine-tuning, and reinforcement learning pipelines for computer agents. Models trained with OSGym outperform state-of-the-art baselines, demonstrating its potential to advance scalability and universality in future agent research.

</details>


### [111] [A Structure-Agnostic Co-Tuning Framework for LLMs and SLMs in Cloud-Edge Systems](https://arxiv.org/abs/2511.11678)
*Yuze Liu,Yunhan Wang,Tiehua Zhang,Zhishu Shen,Cheng Peng,Libing Wu,Feng Xia,Jiong Jin*

Main category: cs.DC

TL;DR: 论文提出Co - PLMs框架用于大小语言模型协作训练，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型智能应用激增，带宽有限的云服务器处理LLM工作负载困难，且跨域部署SLMs及架构异构性影响模型性能提升。

Method: 提出Co - PLMs框架，集成结构无关的互学习过程，用蒸馏代理模型作为桥梁实现异构模型协作训练。

Result: Co - PLMs在Rouge - L上平均提升5.38%，在EM上平均提升4.88%，优于现有方法。

Conclusion: Co - PLMs框架在大小语言模型协作训练方面表现良好，能有效提升性能。

Abstract: The surge in intelligent applications driven by large language models (LLMs) has made it increasingly difficult for bandwidth-limited cloud servers to process extensive LLM workloads in real time without compromising user data privacy. To solve these problems, recent research has focused on constructing cloud-edge consortia that integrate server-based LLM with small language models (SLMs) on mobile edge devices. Furthermore, designing collaborative training mechanisms within such consortia to enhance inference performance has emerged as a promising research direction. However, the cross-domain deployment of SLMs, coupled with structural heterogeneity in SLMs architectures, poses significant challenges to enhancing model performance. To this end, we propose Co-PLMs, a novel co-tuning framework for collaborative training of large and small language models, which integrates the process of structure-agnostic mutual learning to realize knowledge exchange between the heterogeneous language models. This framework employs distilled proxy models (DPMs) as bridges to enable collaborative training between the heterogeneous server-based LLM and on-device SLMs, while preserving the domain-specific insights of each device. The experimental results show that Co-PLMs outperform state-of-the-art methods, achieving average increases of 5.38% in Rouge-L and 4.88% in EM.

</details>


### [112] [ECCENTRIC: Edge-Cloud Collaboration Framework for Distributed Inference Using Knowledge Adaptation](https://arxiv.org/abs/2511.11719)
*Mohammad Mahdi Kamani,Zhongwei Cheng,Lin Chen*

Main category: cs.DC

TL;DR: 边缘AI使用量增长，边缘设备计算资源有限需依赖云，存在计算、通信和性能的权衡，本文提出Eccentric框架，可降低成本并实现最佳性能，实证研究验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决边缘 - 云推理系统中计算、通信成本与性能之间的权衡问题。

Method: 提出基于将边缘模型知识适配到云模型的Eccentric框架。

Result: 该框架可降低推理时系统的计算和通信成本，同时实现最佳性能。

Conclusion: Eccentric框架是适用于边缘 - 云推理系统的有效压缩方法，实证研究证实其有效性。

Abstract: The massive growth in the utilization of edge AI has made the applications of machine learning models ubiquitous in different domains. Despite the computation and communication efficiency of these systems, due to limited computation resources on edge devices, relying on more computationally rich systems on the cloud side is inevitable in most cases. Cloud inference systems can achieve the best performance while the computation and communication cost is dramatically increasing by the expansion of a number of edge devices relying on these systems. Hence, there is a trade-off between the computation, communication, and performance of these systems. In this paper, we propose a novel framework, dubbed as Eccentric that learns models with different levels of trade-offs between these conflicting objectives. This framework, based on an adaptation of knowledge from the edge model to the cloud one, reduces the computation and communication costs of the system during inference while achieving the best performance possible. The Eccentric framework can be considered as a new form of compression method suited for edge-cloud inference systems to reduce both computation and communication costs. Empirical studies on classification and object detection tasks corroborate the efficacy of this framework.

</details>


### [113] [A Meta-Heuristic Load Balancer for Cloud Computing Systems](https://arxiv.org/abs/2511.11721)
*Leszek Sliwko,Vladimir Getov*

Main category: cs.DC

TL;DR: 提出云系统服务分配策略，构建资源利用模型，展示元启发式负载均衡器原型并实验，提出新遗传算法。


<details>
  <summary>Details</summary>
Motivation: 在不使节点过载的情况下，以最小成本分配云系统服务并维持系统稳定性。

Method: 指定云资源利用抽象模型，展示原型元启发式负载均衡器，提出结合其他元启发式算法输出的新型遗传算法。

Result: 进行了实验并展示和讨论了结果。

Conclusion: 未明确提及，但暗示所提策略和算法有助于云系统服务分配。

Abstract: This paper presents a strategy to allocate services on a Cloud system without overloading nodes and maintaining the system stability with minimum cost. We specify an abstract model of cloud resources utilization, including multiple types of resources as well as considerations for the service migration costs. A prototype meta-heuristic load balancer is demonstrated and experimental results are presented and discussed. We also propose a novel genetic algorithm, where population is seeded with the outputs of other meta-heuristic algorithms.

</details>


### [114] [Harli: Harvest Underutilized Resources in LLM Serving with Finetuning Tasks](https://arxiv.org/abs/2511.11729)
*Ao Xu,Han Zhao,Weihao Cui,Quan Chen,Yukang Chen,Shulai Zhang,Shuang Chen,Jiemin Jiang,Zhibin Yu,Minyi Guo*

Main category: cs.DC

TL;DR: 介绍了服务系统Harli，通过将参数高效微调任务与大语言模型解码实例共置提高GPU利用率，实验显示有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型服务系统中，解码实例因内存受限和动态工作负载批处理不足，导致GPU利用率低，计算资源未充分利用。

Method: 引入Harli系统，通过统一内存分配器进行运行时内存重用、两阶段延迟预测器进行解码延迟建模、QoS保证的吞吐量最大化调度器来解决内存有限和不可预测干扰的问题。

Result: Harli相比现有服务系统平均提高微调吞吐量46.2%（最高达92.0%），同时维持推理解码的严格QoS保证。

Conclusion: Harli系统能有效提高GPU利用率，在保证QoS的前提下提升微调吞吐量。

Abstract: Large language models (LLMs) are increasingly deployed under the Model-as-a-Service (MaaS) paradigm. To meet stringent quality-of-service (QoS) requirements, existing LLM serving systems disaggregate the prefill and decode phases of inference. However, decode instances often experience low GPU utilization due to their memory-bound nature and insufficient batching in dynamic workloads, leaving compute resources underutilized.
  We introduce Harli, a serving system that improves GPU utilization by co-locating parameter-efficient finetuning (PEFT) tasks with LLM decode instances. PEFT tasks are compute-bound and memory-efficient, making them ideal candidates for safe co-location. Specifically, Harli addresses key challenges--limited memory and unpredictable interference--using three components: a unified memory allocator for runtime memory reuse, a two-stage latency predictor for decode latency modeling, and a QoS-guaranteed throughput-maximizing scheduler for throughput maximization. Experimental results show that Harli improves the finetune throughput by 46.2% on average (up to 92.0%) over state-of-the-art serving systems, while maintaining strict QoS guarantees for inference decode.

</details>


### [115] [Speculative Decoding in Decentralized LLM Inference: Turning Communication Latency into Computation Throughput](https://arxiv.org/abs/2511.11733)
*Jingwei Song,Wanyi Chen,Xinyuan Song,Max,Chris Tong,Gufeng Chen,Tianyi Zhao,Eric Yang,Bill Shi,Lynn Ai*

Main category: cs.DC

TL;DR: 提出去中心化推测解码框架DSD，通过并行验证候选令牌将通信延迟转化为有效计算，结合自适应策略进一步加速，在多任务上实现显著提速且不影响准确性。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码在集中式系统有效，但在去中心化场景中表现缺乏研究，网络延迟常主导计算，需优化。

Method: 提出DSD框架，在分布式节点并行验证多个候选令牌，引入自适应推测验证策略根据令牌语义重要性调整接受阈值。

Result: 理论上降低跨节点通信成本，实践中在HumanEval和GSM8K上分别实现2.56x和2.59x提速，超越Eagle3基线且保持准确性。

Conclusion: 将推测解码应用于去中心化执行可进行系统级优化，将网络停滞转化为吞吐量，无需模型重新训练或架构更改即可实现更快的分布式大语言模型推理。

Abstract: Speculative decoding accelerates large language model (LLM) inference by using a lightweight draft model to propose tokens that are later verified by a stronger target model. While effective in centralized systems, its behavior in decentralized settings, where network latency often dominates compute, remains under-characterized. We present Decentralized Speculative Decoding (DSD), a plug-and-play framework for decentralized inference that turns communication delay into useful computation by verifying multiple candidate tokens in parallel across distributed nodes. We further introduce an adaptive speculative verification strategy that adjusts acceptance thresholds by token-level semantic importance, delivering an additional 15% to 20% end-to-end speedup without retraining. In theory, DSD reduces cross-node communication cost by approximately (N-1)t1(k-1)/k, where t1 is per-link latency and k is the average number of tokens accepted per round. In practice, DSD achieves up to 2.56x speedup on HumanEval and 2.59x on GSM8K, surpassing the Eagle3 baseline while preserving accuracy. These results show that adapting speculative decoding for decentralized execution provides a system-level optimization that converts network stalls into throughput, enabling faster distributed LLM inference with no model retraining or architectural changes.

</details>


### [116] [How Machine Learning-Data Driven Replication Strategies Enhance Fault Tolerance in Large-Scale Distributed Systems](https://arxiv.org/abs/2511.11749)
*Almond Kiruthu Murimi*

Main category: cs.DC

TL;DR: 研究机器学习驱动的数据复制策略提升大规模分布式系统容错能力，分析现有方法局限，指出潜力与挑战并给出建议。


<details>
  <summary>Details</summary>
Motivation: 传统静态配置的复制方法难适应动态工作负载和意外故障，导致资源利用低效和停机时间长。

Method: 集成预测分析和强化学习等机器学习技术，提出自适应复制机制，通过文献综述、定性分析和与传统方法对比评估。

Result: 确定现有复制策略的关键局限，凸显机器学习在创建更具弹性、自我优化系统方面的变革潜力。

Conclusion: 强调机器学习驱动解决方案在现实环境中的前景与挑战，为未来研究和实际部署提供建议。

Abstract: This research paper investigates how machine learning-driven data replication strategies can enhance fault tolerance in large-scale distributed systems. Traditional replication methods, which rely on static configurations, often struggle to adapt to dynamic workloads and unexpected failures, leading to inefficient resource utilization and prolonged downtime. By integrating machine learning techniques-specifically predictive analytics and reinforcement learning. The study proposes adaptive replication mechanisms capable of forecasting system failures and optimizing data placement in real time. Through an extensive literature review, qualitative analysis, and comparative evaluations with traditional approaches, the paper identifies key limitations in existing replication strategies and highlights the transformative potential of machine learning in creating more resilient, self-optimizing systems. The findings underscore both the promise and the challenges of implementing ML-driven solutions in real-world environments, offering recommendations for future research and practical deployment in cloud-based and enterprise systems.

</details>


### [117] [TD-Orch: Scalable Load-Balancing for Distributed Systems with Applications to Graph Processing](https://arxiv.org/abs/2511.11843)
*Yiwei Zhao,Qiushi Lin,Hongbo Kang,Guy E. Blelloch,Laxman Dhulipala,Charles McGuffey,Phillip B. Gibbons*

Main category: cs.DC

TL;DR: 提出支持分布式应用的任务 - 数据编排抽象，介绍TD - Orch框架及TDO - GP系统，实验显示有显著加速效果。


<details>
  <summary>Details</summary>
Motivation: 为分布式应用提供高效可扩展的任务 - 数据编排方案，解决数据请求不均衡问题。

Method: 提出TD - Orch框架，采用分布式推拉技术；基于TD - Orch构建TDO - GP系统，并设计三种实现技术。

Result: TD - Orch比现有分布式调度基线最多快2.7倍；TDO - GP比最佳开源分布式图处理系统平均快4.1倍。

Conclusion: TD - Orch框架和TDO - GP系统有效，能显著提升分布式应用性能。

Abstract: In this paper, we highlight a task-data orchestration abstraction that supports a range of distributed applications, including graph processing and key-value stores. Given a batch of tasks each requesting one or more data items, where both tasks and data are distributed across multiple machines, each task must get co-located with its target data (by moving tasks and/or data) and executed. We present TD-Orch, an efficient and scalable orchestration framework featuring a simple application developer interface. TD-Orch employs a distributed push-pull technique, leveraging the bidirectional f low of both tasks and data to achieve scalable load balance across machines even under highly skewed data request (data hot spots), with minimal communication overhead. Experimental results show that TD-Orch achieves up to 2.7x speedup over existing distributed scheduling baselines. Building on TD-Orch, we present TDO-GP, a distributed graph processing system for general graph problems, demonstrating the effectiveness of the underlying framework. We design three families of implementation techniques to fully leverage the execution flow provided by TD-Orch. Experimental results show that TDO-GP achieves an average speedup of 4.1x over the best prior open-source distributed graph systems for general graph processing.

</details>


### [118] [KVSwap: Disk-aware KV Cache Offloading for Long-Context On-device Inference](https://arxiv.org/abs/2511.11907)
*Huawei Zhang,Chunwei Xia,Zheng Wang*

Main category: cs.DC

TL;DR: 提出KVSwap框架通过将KV缓存卸载到磁盘突破内存墙，评估显示其在内存预算紧张时吞吐量更高且不影响生成质量。


<details>
  <summary>Details</summary>
Motivation: 本地运行语言模型进行长上下文推理时会遇到内存容量墙问题。

Method: 提出KVSwap软件框架，将KV缓存卸载到磁盘，利用部分关键条目特性，用元数据预测预加载条目，重叠计算与磁盘访问，编排读写模式。

Result: 在代表性语言模型和存储类型上，与现有方案相比，KVSwap在内存预算紧张时吞吐量更高且保持生成质量。

Conclusion: KVSwap能有效突破内存墙，在内存受限情况下有更好表现。

Abstract: Language models (LMs) underpin emerging mobile and embedded AI applications like meeting and video summarization and document analysis, which often require processing multiple long-context inputs. Running an LM locally on-device improves privacy, enables offline use, and reduces cost, but long-context inference quickly hits a \emph{memory capacity wall} as the key-value (KV) cache grows linearly with context length and batch size.
  We present KVSwap, a software framework to break this memory wall by offloading the KV cache to non-volatile secondary storage (disk). KVSwap leverages the observation that only a small, dynamically changing subset of KV entries is critical for generation. It stores the full cache on disk, uses a compact in-memory metadata to predict which entries to preload, overlaps computation with hardware-aware disk access, and orchestrates read patterns to match storage device characteristics. Our evaluation shows that across representative LMs and storage types, KVSwap delivers higher throughput under tight memory budgets while maintaining the generation quality when compared with existing KV cache offloading schemes.

</details>


### [119] [High-Performance N-Queens Solver on GPU: Iterative DFS with Zero Bank Conflicts](https://arxiv.org/abs/2511.12009)
*Guangchao Yao,Yali Li*

Main category: cs.DC

TL;DR: 提出在NVIDIA GPU平台的并行计算方法，用8个RTX 5090 GPU在28.4天验证27皇后问题，相比现有GPU方法有显著加速


<details>
  <summary>Details</summary>
Motivation: 现有验证27皇后问题的时间和计算资源成本过高

Method: 提出迭代深度优先搜索算法，将栈结构映射到GPU共享内存，避免内存库冲突，采用多种优化技术

Result: 28.4天验证27皇后问题，确认PreuBer计算结果，将28皇后问题求解时间缩至约11个月

Conclusion: 相比现有GPU方法有显著加速，为该问题带来新视角

Abstract: The counting of solutions to the N-Queens problem is a classic NP-complete problem with extremely high computational complexity. As of now, the academic community has rigorously verified the number of solutions only up to N <= 26. In 2016, the research team led by PreuBer solved the 27-Queens problem using FPGA hardware, which took approximately one year, though the result remains unverified independently. Recent studies on GPU parallel computing suggest that verifying the 27-Queens solution would still require about 17 months, indicating excessively high time and computational resource costs. To address this challenge, we propose an innovative parallel computing method on NVIDIA GPU platform, with the following core contributions: (1) An iterative depth-first search (DFS) algorithm for solving the N-Queens problem; (2) Complete mapping of the required stack structure to GPU shared memory; (3) Effective avoidance of bank conflicts through meticulously designed memory access patterns; (4) Various optimization techniques are employed to achieve optimal performance. Under the proposed optimization framework, we successfully verified the 27-Queens problem in just 28.4 days using eight RTX 5090 GPUs, thereby confirming the correctness of PreuBer's computational results. Moreover, we have reduced the projected solving time for the next open case-the 28-Queens problem-to approximately 11 months, making its resolution computationally feasible. Compared to the state-of-the-art GPU methods, our method achieves over 10x speedup on identical hardware configurations (8 A100), while delivering over 26x acceleration when utilizing 8 RTX 5090 GPUs, and brings fresh perspectives to this long-stagnant problem.

</details>


### [120] [A Quick and Exact Method for Distributed Quantile Computation](https://arxiv.org/abs/2511.12025)
*Ivan Cao,Jaromir J. Saloni,David A. G. Harrison*

Main category: cs.DC

TL;DR: 提出精确的Spark算法GK Select，避免全数据洗牌，匹配GK Sketch复杂度并返回精确分位数，性能优于Spark全排序。


<details>
  <summary>Details</summary>
Motivation: Spark中计算精确分位数时默认的全局排序方法成本高，需更好的精确算法。

Method: 利用GK Sketch确定近目标枢轴，线性时间提取各分区枢轴误差范围内的值，对候选集进行树归约。

Result: 分析表明GK Select匹配GK Sketch执行端时间复杂度，实证显示在特定集群上比Spark全排序快约10.5倍。

Conclusion: GK Select可在达到草图级延迟的同时返回精确分位数，性能良好。

Abstract: Quantile computation is a core primitive in large-scale data analytics. In Spark, practitioners typically rely on the Greenwald-Khanna (GK) Sketch, an approximate method. When exact quantiles are required, the default option is an expensive global sort. We present GK Select, an exact Spark algorithm that avoids full-data shuffles and completes in a constant number of actions. GK Select leverages GK Sketch to identify a near-target pivot, extracts all values within the error bound around this pivot in each partition in linear time, and then tree-reduces the resulting candidate sets. We show analytically that GK Select matches the executor-side time complexity of GK Sketch while returning the exact quantile. Empirically, GK Select achieves sketch-level latency and outperforms Spark's full sort by approximately 10.5x on 10^9 values across 120 partitions on a 30-core AWS EMR cluster.

</details>


### [121] [Striking the Right Balance between Compute and Copy: Improving LLM Inferencing Under Speculative Decoding](https://arxiv.org/abs/2511.12031)
*Arun Ramachandran,Ramaswamy Govindarajan,Murali Annavaram,Prakash Raghavendra,Hossein Entezari Zarch,Lei Gao,Chaoyi Jiang*

Main category: cs.DC

TL;DR: 提出Balancing Memory and Compute (BMC)机制优化大语言模型CPU推理的KV缓存分配，结合投机解码提升效率，在CPU和GPU上均表现良好。


<details>
  <summary>Details</summary>
Motivation: GPU及云虚拟实例成本高，希望用CPU进行大语言模型推理，但KV缓存更新开销大。

Method: 提出BMC机制，每r次迭代分配带r冗余行的KV张量；利用冗余行进行投机解码；推导分析模型确定最佳设计点。

Result: BMC比基线HuggingFace平均吞吐量加速达3.2倍；结合投机解码额外加速达1.39倍；比vLLM和DeepSpeed分别加速达1.36倍和2.29倍。

Conclusion: BMC机制在CPU和GPU上都能有效提升大语言模型推理性能。

Abstract: With the skyrocketing costs of GPUs and their virtual instances in the cloud, there is a significant desire to use CPUs for large language model (LLM) inference. KV cache update, often implemented as allocation, copying, and in-place strided update for each generated token, incurs significant overhead. As the sequence length increases, the allocation and copy overheads dominate the performance. Alternate approaches may allocate large KV tensors upfront to enable in-place updates, but these matrices (with zero-padded rows) cause redundant computations. In this work, we propose a new KV cache allocation mechanism called Balancing Memory and Compute (BMC). BMC allocates, once every r iterations, KV tensors with r redundant rows, allowing in-place update without copy overhead for those iterations, but at the expense of a small amount of redundant computation. Second, we make an interesting observation that the extra rows allocated in the KV tensors and the resulting redundant computation can be repurposed for Speculative Decoding (SD) that improves token generation efficiency. Last, BMC represents a spectrum of design points with different values of r. To identify the best-performing design point(s), we derive a simple analytical model for BMC. The proposed BMC method achieves an average throughput acceleration of up to 3.2x over baseline HuggingFace (without SD). Importantly when we apply BMC with SD, it results in an additional speedup of up to 1.39x, over and above the speedup offered by SD. Further, BMC achieves a throughput acceleration of up to 1.36x and 2.29x over state-of-the-art inference servers vLLM and DeepSpeed, respectively. Although the BMC technique is evaluated extensively across different classes of CPUs (desktop and server class), we also evaluate the scheme with GPUs and demonstrate that it works well for GPUs.

</details>


### [122] [Combining Serverless and High-Performance Computing Paradigms to support ML Data-Intensive Applications](https://arxiv.org/abs/2511.12185)
*Mills Staylor,Arup Kumar Sarker,Gregor von Laszewski,Geoffrey Fox,Yue Cheng,Judy Fox*

Main category: cs.DC

TL;DR: 介绍Cylon高性能分布式数据帧解决方案，对比无服务器和有服务器环境性能。


<details>
  <summary>Details</summary>
Motivation: 传统数据工程等工作负载需大量硬件和维护投资，无服务器函数处理大数据集时外部存储慢，需解决通信和性能问题。

Method: 受FMI库启发设计无服务器通信器，通过NAT Traversal TCP Hole Punching实现直接通信。

Result: 实验显示AWS Lambda在强扩展实验中性能低于有服务器的AWS（EC2）和HPC不到1%。

Conclusion: Cylon在数据处理方面有应用前景，能解决无服务器函数相关通信和性能问题。

Abstract: Data is found everywhere, from health and human infrastructure to the surge of sensors and the proliferation of internet-connected devices. To meet this challenge, the data engineering field has expanded significantly in recent years in both research and industry. Traditionally, data engineering, Machine Learning, and AI workloads have been run on large clusters within data center environments, requiring substantial investment in hardware and maintenance. With the rise of the public cloud, it is now possible to run large applications across nodes without owning or maintaining hardware. Serverless functions such as AWS Lambda provide horizontal scaling and precise billing without the hassle of managing traditional cloud infrastructure. However, when processing large datasets, users often rely on external storage options that are significantly slower than direct communication typical of HPC clusters. We introduce Cylon, a high-performance distributed data frame solution that has shown promising results for data processing using Python. We describe how we took inspiration from the FMI library and designed a serverless communicator to tackle communication and performance issues associated with serverless functions. With our design, we demonstrate that the performance of AWS Lambda falls below one percent of strong scaling experiments compared to serverful AWS (EC2) and HPCs based on implementing direct communication via NAT Traversal TCP Hole Punching.

</details>


### [123] [Distributed Seasonal Temporal Pattern Mining](https://arxiv.org/abs/2511.12216)
*Van Ho-Long,Nguyen Ho,Anh-Vu Dinh-Duc,Ha Manh Tran,Ky Trung Nguyen,Tran Dung Pham,Quoc Viet Hung Nguyen*

Main category: cs.DC

TL;DR: 本文提出分布式季节性时间模式挖掘框架DSTPM，可从时间序列挖掘季节性时间模式，实验表明其在运行时间和内存使用上优于顺序基线方法。


<details>
  <summary>Details</summary>
Motivation: 物联网传感器产生大量时间序列数据，挖掘季节性时间模式（STPs）有重要价值，但传统方法有局限且现有STP挖掘方法无法处理大数据集。

Method: 提出分布式框架DSTPM，利用分布式分层查找哈希结构进行高效计算。

Result: DSTPM在运行时间和内存使用上显著优于顺序基线方法，能有效处理非常大的数据集。

Conclusion: DSTPM是一种高效的分布式季节性时间模式挖掘框架。

Abstract: The explosive growth of IoT-enabled sensors is producing enormous amounts of time series data across many domains, offering valuable opportunities to extract insights through temporal pattern mining. Among these patterns, an important class exhibits periodic occurrences, referred to as \textit{seasonal temporal patterns} (STPs). However, mining STPs poses challenges, as traditional measures such as support and confidence cannot capture seasonality, and the lack of the anti-monotonicity property results in an exponentially large search space. Existing STP mining methods operate sequentially and therefore do not scale to large datasets. In this paper, we propose the Distributed Seasonal Temporal Pattern Mining (DSTPM), the first distributed framework for mining seasonal temporal patterns from time series. DSTPM leverages efficient data structures, specifically distributed hierarchical lookup hash structures, to enable efficient computation. Extensive experimental evaluations demonstrate that DSTPM significantly outperforms sequential baselines in runtime and memory usage, while scaling effectively to very large datasets.

</details>


### [124] [Pico-Cloud: Cloud Infrastructure for Tiny Edge Devices](https://arxiv.org/abs/2511.13253)
*Mordechai Guri*

Main category: cs.DC

TL;DR: 本文介绍基于树莓派Zero等超小型硬件平台的Pico - Cloud微边缘云架构，展示其特性、用例并分析设计挑战，结果表明它适合网络边缘轻量级分布式工作负载。


<details>
  <summary>Details</summary>
Motivation: 构建一种能在设备层实现低延迟、低功耗本地操作，不依赖集中式数据中心的边缘云架构。

Method: 介绍Pico - Cloud架构模型，列举农村连接、教育集群和边缘AI推理等代表性用例，分析计算、网络、存储和电源管理方面的设计挑战。

Result: Pico - Cloud是适用于网络边缘轻量级分布式工作负载的经济、分散且可持续的平台。

Conclusion: Pico - Cloud架构具有显著优势，可用于网络边缘轻量级分布式工作负载。

Abstract: This paper introduces the Pico-Cloud, a micro-edge cloud architecture built on ultra-minimal hardware platforms such as the Raspberry Pi Zero and comparable single-board computers. The Pico-Cloud delivers container-based virtualization, service discovery, and lightweight orchestration directly at the device layer, enabling local operation with low latency and low power consumption without reliance on centralized data centers. We present its architectural model, outline representative use cases including rural connectivity, educational clusters, and edge AI inference, and analyze design challenges in computation, networking, storage, and power management. The results highlight Pico-Clouds as a cost-effective, decentralized, and sustainable platform for lightweight distributed workloads at the network edge.

</details>


### [125] [Design of A Low-Latency and Parallelizable SVD Dataflow Architecture on FPGA](https://arxiv.org/abs/2511.12461)
*Fangqiang Du,Sixuan Chong,Zixuan Huang,Rui Qin,Fengnan Mi,Caibao Hu,Jiangang Chen*

Main category: cs.DC

TL;DR: 提出基于数据流的SVD处理算法DSB Jacobi，减少片上BRAM使用，提升计算速度，实验显示减少41.5%片上RAM消耗，提升23倍计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统SVD计算在矩阵维度增大时计算成本高，现有硬件架构可扩展性有限、片上内存资源消耗大，不适用于嵌入式系统实时处理大规模数据流矩阵。

Method: 提出Data Stream-Based SVD处理算法（DSB Jacobi）。

Result: 与以往工作相比，减少41.5%片上RAM消耗，提升23倍计算效率。

Conclusion: DSB Jacobi算法是大规模数据流实时SVD计算的实用解决方案。

Abstract: Singular value decomposition (SVD) is widely used for dimensionality reduction and noise suppression, and it plays a pivotal role in numerous scientific and engineering applications. As the dimensions of the matrix grow rapidly, the computational cost increases significantly, posing a serious challenge to the efficiency of data analysis and signal processing systems,especially in time-sensitive scenarios with large-scale datasets. Although various dedicated hardware architectures have been proposed to accelerate the computation of intensive SVD, many of these designs suffer from limited scalability and high consumption of on-chip memory resources. Moreover, they typically overlook the computational and data transfer challenges associated with SVD, enabling them unsuitable for real-time processing of large-scale data stream matrices in embedded systems. In this express, we propose a Data Stream-Based SVD processing algorithm (DSB Jacobi), which significantly reduces on-chip BRAM usage while improving computational speed, offering a practical solution for real-time SVD computation of large-scale data streams. Compared with previous works, our experimental results indicate that the proposed method reduces on-chip RAM consumption by 41.5 percent and improves computational efficiency by 23 times.

</details>


### [126] [A Decentralized Root Cause Localization Approach for Edge Computing Environments](https://arxiv.org/abs/2511.12486)
*Duneesha Fernando,Maria A. Rodriguez,Rajkumar Buyya*

Main category: cs.DC

TL;DR: 本文提出一种去中心化的根因定位（RCL）方法，在边缘设备层使用PPR算法定位异常根源，减少定位时间，评估显示其定位准确率相当或更高，可用于资源受限的边缘环境。


<details>
  <summary>Details</summary>
Motivation: 现有RCL方法为云环境设计，应用于边缘时会增加延迟和通信开销，需适用于边缘环境的RCL方法。

Method: 提出去中心化RCL方法，先将微服务分组，在集群内用PPR算法定位根因，跨集群时引入对等近似过程，还提出针对异构边缘环境的异常评分机制。

Result: 在MicroCERCL数据集上评估，该方法定位准确率相当或更高，定位时间最多减少34%。

Conclusion: 基于图的去中心化RCL可为资源受限的边缘环境提供实用高效的异常诊断解决方案。

Abstract: Edge computing environments host increasingly complex microservice-based IoT applications, which are prone to performance anomalies that can propagate across dependent services. Identifying the true source of such anomalies, known as Root Cause Localization (RCL), is essential for timely mitigation. However, existing RCL approaches are designed for cloud environments and rely on centralized analysis, which increases latency and communication overhead when applied at the edge. This paper proposes a decentralized RCL approach that executes localization directly at the edge device level using the Personalized PageRank (PPR) algorithm. The proposed method first groups microservices into communication- and colocation-aware clusters, thereby confining most anomaly propagation within cluster boundaries. Within each cluster, PPR is executed locally to identify the root cause, significantly reducing localization time. For the rare cases where anomalies propagate across clusters, we introduce an inter-cluster peer-to-peer approximation process, enabling lightweight coordination among clusters with minimal communication overhead. To enhance the accuracy of localization in heterogeneous edge environments, we also propose a novel anomaly scoring mechanism tailored to the diverse anomaly triggers that arise across microservice, device, and network layers. Evaluation results on the publicly available edge dataset, MicroCERCL, demonstrate that the proposed decentralized approach achieves comparable or higher localization accuracy than its centralized counterpart while reducing localization time by up to 34%. These findings highlight that decentralized graph-based RCL can provide a practical and efficient solution for anomaly diagnosis in resource-constrained edge environments.

</details>


### [127] [Iris: First-Class Multi-GPU Programming Experience in Triton](https://arxiv.org/abs/2511.12500)
*Muhammad Awad,Muhammad Osama,Brandon Potter*

Main category: cs.DC

TL;DR: 提出全Python和Triton实现的多GPU通信库Iris，消除性能与可编程性权衡，有优势。


<details>
  <summary>Details</summary>
Motivation: 传统多GPU编程需在性能和可编程性间做复杂权衡，现有方案有不足。

Method: 实现Iris库，提供基于tile的对称内存抽象，支持多种计算通信重叠模式。

Result: Iris在微基准测试中实现接近最优带宽利用率，在GEMM+All - Scatter工作负载上比PyTorch和RCCL快1.79倍。

Conclusion: 高级实现可媲美或超越高度优化的库，同时极大简化多GPU编程。

Abstract: Multi-GPU programming traditionally requires developers to navigate complex trade-offs between performance and programmability. High-performance implementations typically rely on low-level HIP/CUDA communication libraries that demand substantial engineering effort for even basic overlap patterns, while simpler abstractions often sacrifice performance. We present Iris, a multi-GPU communication library implemented entirely in Python and Triton that eliminates this trade-off. Iris provides tile-based symmetric memory abstractions that naturally align with Triton's programming model, enabling developers to write single-source kernels that seamlessly interleave computation and communication. We demonstrate a taxonomy of compute-communication overlap patterns--from bulk-synchronous to fine-grained workgroup specialization--that can be implemented with minimal code changes in Iris, often requiring just a few additional lines within the same Triton kernel. Our evaluation shows that Iris achieves near-optimal bandwidth utilization in microbenchmarks and delivers up to 1.79x speedup over PyTorch and RCCL for GEMM+All-Scatter workloads, demonstrating that high-level implementations can match or exceed heavily-optimized libraries while dramatically simplifying multi-GPU programming.

</details>


### [128] [Artifact for A Non-Intrusive Framework for Deferred Integration of Cloud Patterns in Energy-Efficient Data-Sharing Pipelines](https://arxiv.org/abs/2511.12667)
*Sepideh Masoudi,Mark Edward Michael Daly,Jannis Kiesel*

Main category: cs.DC

TL;DR: 提出基于Kubernetes的工具，可在不修改服务代码下应用设计模式，支持节能决策并保留服务可重用性。


<details>
  <summary>Details</summary>
Motivation: 数据网格架构发展，传统云设计模式会降低服务在不同管道中的可重用性。

Method: 开发基于Kubernetes的工具，实现非侵入式、延迟应用设计模式，自动化模式注入并收集能源指标。

Result: 该工具可在保留转换服务在各种管道结构中可重用性的同时，支持节能决策。

Conclusion: 此工具能有效解决传统云设计模式带来的服务可重用性问题，助力节能决策。

Abstract: As data mesh architectures grow, organizations increasingly build consumer-specific data-sharing pipelines from modular, cloud-based transformation services. While reusable transformation services can improve cost and energy efficiency, applying traditional cloud design patterns can reduce reusability of services in different pipelines. We present a Kubernetes-based tool that enables non-intrusive, deferred application of design patterns without modifying services code. The tool automates pattern injection and collects energy metrics, supporting energy-aware decisions while preserving reusability of transformation services in various pipeline structures.

</details>


### [129] [The Time to Consensus in a Blockchain: Insights into Bitcoin's "6 Blocks Rule''](https://arxiv.org/abs/2511.12687)
*Partha S. Dey,Aditya S. Gopalan,Vijay G. Subramanian*

Main category: cs.DC

TL;DR: 研究中本聪区块链达成共识的时间，通过排队技术计算并模拟验证。


<details>
  <summary>Details</summary>
Motivation: 研究中本聪区块链达成共识所需的时间。

Method: 考虑诚实和对抗两种竞争增长过程，运用排队技术，在简化比特币模型中计算拉普拉斯变换，并通过模拟验证。

Result: 在简化比特币模型中计算出达成共识时间的拉普拉斯变换。

Conclusion: 通过排队技术可对中本聪区块链达成共识的时间进行计算和验证。

Abstract: We investigate the time to consensus in Nakamoto blockchains. Specifically, we consider two competing growth processes, labeled \emph{honest} and \emph{adversarial}, and determine the time after which the honest process permananetly exceeds the adversarial process. This is done via queueing techniques. The predominant difficulty is that the honest growth process is subject to \emph{random delays}. In a stylized Bitcoin model, we compute the Laplace transform for the time to consensus and verify it via simulation.

</details>


### [130] [Learning Process Energy Profiles from Node-Level Power Data](https://arxiv.org/abs/2511.13155)
*Jonathan Bader,Julius Irion,Jannis Kappel,Joel Witzke,Niklas Fomin,Diellza Sherifi,Odej Kao*

Main category: cs.DC

TL;DR: 为提高数据中心能效，提出利用eBPF和perf收集细粒度进程资源指标，结合节点能耗测量数据，通过回归模型实现更细粒度进程能耗预测的方法。


<details>
  <summary>Details</summary>
Motivation: 高性能计算、云计算和人工智能推动数据中心容量需求增长，导致能耗剧增，需要从进程层面洞察能耗以提高能效，而现有进程能耗估算机制存在局限。

Method: 利用eBPF和perf收集细粒度进程级资源指标，与配电单元获取的节点级能耗测量数据同步，通过基于回归的模型统计学习进程级资源使用与节点级能耗的关系。

Result: 实现了更细粒度的进程级能耗预测。

Conclusion: 该方法有助于提高数据中心的能源效率。

Abstract: The growing demand for data center capacity, driven by the growth of high-performance computing, cloud computing, and especially artificial intelligence, has led to a sharp increase in data center energy consumption. To improve energy efficiency, gaining process-level insights into energy consumption is essential. While node-level energy consumption data can be directly measured with hardware such as power meters, existing mechanisms for estimating per-process energy usage, such as Intel RAPL, are limited to specific hardware and provide only coarse-grained, domain-level measurements. Our proposed approach models per-process energy profiles by leveraging fine-grained process-level resource metrics collected via eBPF and perf, which are synchronized with node-level energy measurements obtained from an attached power distribution unit. By statistically learning the relationship between process-level resource usage and node-level energy consumption through a regression-based model, our approach enables more fine-grained per-process energy predictions.

</details>


### [131] [Distributed Hierarchical Machine Learning for Joint Resource Allocation and Slice Selection in In-Network Edge Systems](https://arxiv.org/abs/2511.13313)
*Sulaiman Muhammad Rashid,Ibrahim Aliyu,Jaehyung Park,Jinsul Kim*

Main category: cs.DC

TL;DR: 为解决元宇宙延迟和资源需求挑战，提出切片网络边缘架构，构建资源管理问题模型，训练DeepSets - S模型，实验显示其有高准确率、低执行时间和更好资源利用率。


<details>
  <summary>Details</summary>
Motivation: 元宇宙有严格的延迟和资源需求，传统优化技术在动态边缘条件和高用户负载下难以有效响应。

Method: 采用切片网络边缘架构，将无线和计算资源管理与最优切片选择联合问题建模为MINLP，分解为三个子问题，训练分布式分层DeepSets - S模型。

Result: DeepSets - S在子问题上有高准确率，相比精确求解器减少86.1%执行时间，能紧密跟踪最优系统成本，比基线模型有更好资源利用率。

Conclusion: 所提的DeepSets - S模型能有效解决元宇宙中的资源管理问题，在准确率、执行时间和资源利用率上表现良好。

Abstract: The Metaverse promises immersive, real-time experiences; however, meeting its stringent latency and resource demands remains a major challenge. Conventional optimization techniques struggle to respond effectively under dynamic edge conditions and high user loads. In this study, we explore a slice-enabled in-network edge architecture that combines computing-in-the-network (COIN) with multi-access edge computing (MEC). In addition, we formulate the joint problem of wireless and computing resource management with optimal slice selection as a mixed-integer nonlinear program (MINLP). Because solving this model online is computationally intensive, we decompose it into three sub-problems (SP1) intra-slice allocation, (SP2) inter-slice allocation, and (SP3) offloading decision and train a distributed hierarchical DeepSets-based model (DeepSets-S) on optimal solutions obtained offline. In the proposed model, we design a slack-aware normalization mechanism for a shared encoder and task-specific decoders, ensuring permutation equivariance over variable-size wireless device (WD) sets. The learned system produces near-optimal allocations with low inference time and maintains permutation equivariance over variable-size device sets. Our experimental results show that DeepSets-S attains high tolerance-based accuracies on SP1/SP2 (Acc1 = 95.26% and 95.67%) and improves multiclass offloading accuracy on SP3 (Acc = 0.7486; binary local/offload Acc = 0.8824). Compared to exact solvers, the proposed approach reduces the execution time by 86.1%, while closely tracking the optimal system cost (within 6.1% in representative regimes). Compared with baseline models, DeepSets-S consistently achieves higher cost ratios and better utilization across COIN/MEC resources.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [132] [An improved approximation algorithm for k-Median](https://arxiv.org/abs/2511.12230)
*Neal E. Young*

Main category: cs.DS

TL;DR: 提出针对k - Median问题的多项式时间近似算法，是α - 大小近似算法，匹配无权重集合覆盖问题已知界限，运行时间为O(k m log(n/k) log m)。


<details>
  <summary>Details</summary>
Motivation: 为（不一定是度量的）k - Median问题找到多项式时间近似算法。

Method: 设计特定的近似算法。

Result: 算法是α - 大小近似算法，α < 1 + 2 ln(n/k)，能保证解的大小和成本，匹配无权重集合覆盖问题已知界限，运行时间为O(k m log(n/k) log m)。

Conclusion: 该算法是首个在常数因子内匹配无权重集合覆盖问题已知界限的多项式时间近似算法。

Abstract: We give a polynomial-time approximation algorithm for the (not necessarily metric) $k$-Median problem. The algorithm is an $α$-size-approximation algorithm for $α< 1 + 2 \ln(n/k)$. That is, it guarantees a solution having size at most $α\times k$, and cost at most the cost of any size-$k$ solution. This is the first polynomial-time approximation algorithm to match the well-known bounds of $H_Δ$ and $1 + \ln(n/k)$ for unweighted Set Cover (a special case) within a constant factor. It matches these bounds within a factor of 2. The algorithm runs in time $O(k m \log(n/k) \log m)$, where $n$ is the number of customers and $m$ is the instance size.

</details>


### [133] [Shortcutting for Negative-Weight Shortest Path](https://arxiv.org/abs/2511.12714)
*George Z. Li,Jason Li,Satish Rao,Junkai Zhang*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Consider the single-source shortest paths problem on a directed graph with real-valued edge weights. We solve this problem in $O(n^{2.5}\log^{4.5}n)$ time, improving on prior work of Fineman (STOC 2024) and Huang-Jin-Quanrud (SODA 2025, 2026) on dense graphs. Our main technique is an shortcutting procedure that iteratively reduces the number of negative-weight edges along shortest paths by a constant factor.

</details>


### [134] [Indirect Coflow Scheduling](https://arxiv.org/abs/2511.12854)
*Alexander Lindermayr,Kirk Pruhs,Andréa W. Richa,Tegan Wilson*

Main category: cs.DS

TL;DR: 研究可重构网络中的共流调度，针对小数据传输量情况设计算法，表现优于为大数据传输设计的算法。


<details>
  <summary>Details</summary>
Motivation: 现有算法文献通常隐式假设数据传输量很大，而实际中部分请求数据传输量可能较小，需研究小数据传输情况。

Method: 研究分数匹配和间接路由，设计针对小数据传输的算法。

Result: 设计的算法在小需求情况下比为大数据传输设计的文献算法表现更好。

Conclusion: 针对小数据传输量设计的算法在相应场景有更好性能。

Abstract: We consider routing in reconfigurable networks, which is also known as coflow scheduling in the literature. The algorithmic literature generally (perhaps implicitly) assumes that the amount of data to be transferred is large. Thus the standard way to model a collection of requested data transfers is by an integer demand matrix $D$, where the entry in row $i$ and column $j$ of $D$ is an integer representing the amount of information that the application wants to send from machine/node $i$ to machine/node $j$. A feasible coflow schedule is then a sequence of matchings, which represent the sequence of data transfers that covers $D$. In this work, we investigate coflow scheduling when the size of some of the requested data transfers may be small relative to the amount of data that can be transferred in one round. fractional matchings and/or that employ indirect routing, and compare the relative utility of these options. We design algorithms that perform much better for small demands than the algorithms in the literature that were designed for large data transfers.

</details>


### [135] [Maximal Palindromes in MPC: Simple and Optimal](https://arxiv.org/abs/2511.13014)
*Solon P. Pissis*

Main category: cs.DS

TL;DR: 本文提出简单且最优算法，在MPC模型下O(1)轮内解决LPS问题，复杂度为O(n)，还能计算所有最大回文，在Adaptive MPC模型可突破ε限制。


<details>
  <summary>Details</summary>
Motivation: 解决经典最长回文子串（LPS）问题，在MPC模型下寻找更优算法。

Method: 提出简单且最优的算法，在MPC模型下进行计算。

Result: 能在O(1)轮内解决LPS问题，总时间和内存为O(n)，每台机器内存为O(n^{1 - ε})，可计算所有最大回文，在Adaptive MPC模型可突破ε限制，算法为随机且高概率成功。

Conclusion: 所提算法简单且最优，在解决LPS问题上有良好效果，还能拓展到计算最大回文及突破特定模型限制。

Abstract: In the classical longest palindromic substring (LPS) problem, we are given a string $S$ of length $n$, and the task is to output a longest palindromic substring in $S$. Gilbert, Hajiaghayi, Saleh, and Seddighin [SPAA 2023] showed how to solve the LPS problem in the Massively Parallel Computation (MPC) model in $\mathcal{O}(1)$ rounds using $\mathcal{\widetilde{O}}(n)$ total memory, with $\mathcal{\widetilde{O}}(n^{1-ε})$ memory per machine, for any $ε\in (0,0.5]$.
  We present a simple and optimal algorithm to solve the LPS problem in the MPC model in $\mathcal{O}(1)$ rounds. The total time and memory are $\mathcal{O}(n)$, with $\mathcal{O}(n^{1-ε})$ memory per machine, for any $ε\in (0,0.5]$. A key attribute of our algorithm is its ability to compute all maximal palindromes in the same complexities. Furthermore, our new insights allow us to bypass the constraint $ε\in (0,0.5]$ in the Adaptive MPC model. Our algorithms and the one proposed by Gilbert et al. for the LPS problem are randomized and succeed with high probability.

</details>


### [136] [Greedy matroid base packings with applications to dynamic graph density and orientations](https://arxiv.org/abs/2511.13205)
*Pavel Arkhipov,Vladimir Kolmogorov*

Main category: cs.DS

TL;DR: 研究一般拟阵中贪心最小权重基打包过程及其算法应用，在双循环拟阵和一般拟阵上均有成果，改进了现有算法复杂度和界。


<details>
  <summary>Details</summary>
Motivation: 探索贪心最小权重基打包在一般拟阵中的过程及算法应用，解决连通性相关问题。

Method: 在双循环拟阵中通过贪心打包伪森林，维护动态图中的最小权重伪森林；对一般拟阵观察基打包极限的特征；研究贪心树打包。

Result: 在双循环拟阵中改进近似全动态最密子图密度算法复杂度；解决动态图中最小权重伪森林问题；给出一般拟阵基打包极限的特征；改进贪心树打包的界和加强边负载收敛率下界。

Conclusion: 贪心最小权重基打包在拟阵中有良好应用，算法复杂度和界得到有效改进。

Abstract: Greedy minimum weight spanning tree packings have proven to be useful in connectivity-related problems. We study the process of greedy minimum weight base packings in general matroids and explore its algorithmic applications.
  When specialized to bicircular matroids, our results yield an algorithm for the approximate fully-dynamic densest subgraph density $ρ$. We maintain a $(1+\varepsilon)$-approximation of the density with a worst-case update time $O((ρ\varepsilon^{-2}+\varepsilon^{-4})ρ\log^3 m)$. It improves the dependency on $\varepsilon$ from the current state-of-the-art worst-case update time complexity $O(\varepsilon^{-6}\log^3 n\logρ)$ [Chekuri, Christiansen, Holm, van der Hoog, Quanrud, Rotenberg, Schwiegelshohn, SODA'24]. We also can maintain an implicit fractional out-orientation with a guarantee that all out-degrees are at most $(1+\varepsilon)ρ$.
  Our algorithms above work by greedily packing pseudoforests, and require maintenance of a minimum-weight pseudoforest in a dynamically changing graph. We show that this problem can be solved in $O(\log n)$ worst-case time per edge insertion or deletion.
  For general matroids, we observe two characterizations of the limit of the base packings (``the vector of ideal loads''), which imply the characterizations from [Cen, Fleischmann, Li, Li, Panigrahi, FOCS'25], namely, their entropy-minimization theorem and their bottom-up cut hierarchy.
  Finally, we give combinatorial results on the greedy tree packings. We show that a tree packing of $O(λ^5\log m)$ trees contains a tree crossing some min-cut once, which improves the bound $O(λ^7\log^3 m)$ from [Thorup, Combinatorica'07]. We also strengthen the lower bound on the edge load convergence rate from [de Vos, Christiansen, SODA'25], showing that Thorup's upper bound is tight up to a logarithmic factor.

</details>


### [137] [A Complexity Analysis of the c-Closed Vertex Deletion Problem](https://arxiv.org/abs/2511.13301)
*Lisa Lehner,Christian Komusiewicz,Luca Pascal Staus*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: A graph is $c$-closed when every pair of nonadjacent vertices has at most $c-1$ common neighbors. In $c$-Closed Vertex Deletion, the input is a graph $G$ and an integer $k$ and we ask whether $G$ can be transformed into a $c$-closed graph by deleting at most $k$ vertices. We study the classic and parameterized complexity of $c$-Closed Vertex Deletion. We obtain, for example, NP-hardness for the case that $G$ is bipartite with bounded maximum degree. We also show upper and lower bounds on the size of problem kernels for the parameter $k$ and introduce a new parameter, the number $x$ of vertices in bad pairs, for which we show a problem kernel of size $\mathcal{O}(x^3 + x^2\cdot c))$. Here, a pair of nonadjacent vertices is bad if they have at least $c$ common neighbors. Finally, we show that $c$-Closed Vertex Deletion can be solved in polynomial time on unit interval graphs with depth at most $c+1$ and that it is fixed-parameter tractable with respect to the neighborhood diversity of $G$.

</details>


### [138] [Dimension-Free Correlated Sampling for the Hypersimplex](https://arxiv.org/abs/2511.13573)
*Joseph,Naor,Nitya Raju,Abhishek Shetty,Aravind Srinivasan,Renata Valieva,David Wajc*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Sampling from multiple distributions so as to maximize overlap has been studied by statisticians since the 1950s. Since the 2000s, such correlated sampling from the probability simplex has been a powerful building block in disparate areas of theoretical computer science. We study a generalization of this problem to sampling sets from given vectors in the hypersimplex, i.e., outputting sets of size (at most) some $k$ in $[n]$, while maximizing the sampled sets' overlap. Specifically, the expected difference between two output sets should be at most $α$ times their input vectors' $\ell_1$ distance. A value of $α=O(\log n)$ is known to be achievable, due to Chen et al.~(ICALP'17). We improve this factor to $O(\log k)$, independent of the ambient dimension~$n$. Our algorithm satisfies other desirable properties, including (up to a $\log^* n$ factor) input-sparsity sampling time, logarithmic parallel depth and dynamic update time, as well as preservation of submodular objectives. Anticipating broader use of correlated sampling algorithms for the hypersimplex, we present applications of our algorithm to online paging, offline approximation of metric multi-labeling and swift multi-scenario submodular welfare approximating reallocation.

</details>


### [139] [The Merkle Mountain Belt](https://arxiv.org/abs/2511.13582)
*Alfonso Cevallos,Robert Hambrock,Alistair Stewart*

Main category: cs.DS

TL;DR: 本文比较不同Merkle结构在区块链应用中作为承诺方案的特性，介绍了如MMB等新Merkle结构，其具有简洁、增量、最优可加等特性，能降低轻客户端成本。


<details>
  <summary>Details</summary>
Motivation: 加速轻客户端协议，使用户能在智能手机上高效验证交易。

Method: 比较不同Merkle结构（如MMR、Merkle链）特性，并引入新的Merkle结构（MMB、UMMB）。

Result: MMR是简洁方案；Merkle链是增量且最优可加；MMB同时具备简洁、增量和最优可加特性，UMMB还异步；新结构使近期数据证明更短，降低轻客户端成本。

Conclusion: 新引入的Merkle结构（MMB、UMMB）在区块链轻客户端应用中有优势，能满足不同需求并降低成本。

Abstract: Merkle structures are widely used as commitment schemes: they allow a prover to publish a compact commitment to an ordered list $X$ of items, and then efficiently prove to a verifier that $x_i\in X$ is the $i$-th item in it. We compare different Merkle structures and their corresponding properties as commitment schemes in the context of blockchain applications. Our primary goal is to speed up light client protocols so that, e.g., a user can verify a transaction efficiently from their smartphone.
  For instance, the Merkle Mountain Range (MMR) yields a succinct scheme: a light client synchronizing for the first time can do so with a complexity sublinear in $|X|$. On the other hand, the Merkle chain, traditionally used to commit to block headers, is not succinct, but it is incremental - a light client resynchronizing frequently can do so with constant complexity - and optimally additive - the structure can be updated in constant time when a new item is appended to list $X$.
  We introduce new Merkle structures, most notably the Merkle Mountain Belt (MMB), the first to be simultaneously succinct, incremental and optimally additive. A variant called UMMB is also asynchronous: a light client may continue to interact with the network even when out of sync with the public commitment. Our Merkle structures are slightly unbalanced, so that items recently appended to $X$ receive shorter membership proofs than older items. This feature reduces a light client's expected costs, in applications where queries are biased towards recently generated data.

</details>


### [140] [Chasing Submodular Objectives, and Submodular Maximization via Cutting Planes](https://arxiv.org/abs/2511.13605)
*Niv Buchbinder,Joseph,Naor,David Wajc*

Main category: cs.DS

TL;DR: 本文引入子模目标追逐问题，为基数和划分拟阵约束提供多项式时间算法，实现最优近似和最优竞争追索，并介绍新的元算法及应用。


<details>
  <summary>Details</summary>
Motivation: 解决一系列随时间变化的受限子模最大化问题，追求高近似解和低追索。

Method: 提出新的元算法“approximate - or - separate”，改进“round - and - separate”方法，结合切割平面法和分离预言机。

Result: 为基数和划分拟阵约束提供多项式时间算法，实现(1 - 1/e - ε)近似和最优竞争追索，展示该方法在静态算法和通信复杂度协议中的应用。

Conclusion: 切割平面法可用于受限子模最大化问题，所提算法和方法有广泛应用。

Abstract: We introduce the \emph{submodular objectives chasing problem}, which generalizes many natural and previously-studied problems: a sequence of constrained submodular maximization problems is revealed over time, with both the objective and available ground set changing at each step. The goal is to maintain solutions of high approximation and low total \emph{recourse} (number of changes), compared with exact offline algorithms for the same input sequence. For the central cardinality constraint and partition matroid constraints we provide polynomial-time algorithms achieving both optimal $(1-1/e-ε)$-approximation and optimal competitive recourse for \emph{any} constant-approximation.
  Key to our algorithm's polynomial time, and of possible independent interest, is a new meta-algorithm for $(1-1/e-ε)$-approximately maximizing the multilinear extension under general constraints, which we call {\em approximate-or-separate}. Our algorithm relies on an improvement of the round-and-separate method [Gupta-Levin SODA'20], inspired by an earlier proof by [Vondrák, PhD~Thesis'07]. The algorithm, whose guarantees are similar to the influential {\em continuous greedy} algorithm [Calinescu-Chekuri-Pál-Vondrák SICOMP'11], can use any cutting plane method and separation oracle for the constraints. This allows us to introduce cutting plane methods, used for exact unconstrained submodular minimization since the '80s [Grötschel/Lovász/Schrijver Combinatorica'81], as a useful method for (optimal approximate) constrained submodular maximization. We show further applications of this approach to static algorithms with curvature-sensitive approximation, and to communication complexity protocols.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [141] [Collusion-proof Auction Design using Side Information](https://arxiv.org/abs/2511.12456)
*Sukanya Kudva,Anil Aswani*

Main category: cs.GT

TL;DR: 研究有投标人合谋情况下的拍卖设计，提出H - VCG机制，该机制结合VCG和定价机制，实验表明其表现优于仅对非合谋者使用的VCG。


<details>
  <summary>Details</summary>
Motivation: 经典VCG机制易受合谋影响，完全防合谋机制无法保证近似效率，旨在设计在部分投标人合谋时仍能保证良好福利和收入的拍卖。

Method: 先刻画合谋者在VCG下的策略行为，证明相关结论；提出结合VCG和定价机制的H - VCG机制，假设可使用合谋检测算法；推导福利和收入的概率保证。

Result: H - VCG是事后占优策略激励兼容的，数值实验显示其始终优于仅对非合谋者使用的VCG，接近理想VCG机制表现。

Conclusion: 研究为将合谋检测纳入机制设计提供了原则性框架，迈向抗合谋拍卖。

Abstract: We study the problem of auction design in the presence of bidder collusion. Specifically, we consider a multi-unit auction of identical items with single-minded bidders, where a subset of bidders may collude by coordinating bids and transferring payments and items among themselves. While the classical Vickrey-Clarke-Groves (VCG) mechanism achieves efficient and truthful outcomes, it is highly vulnerable to collusion. In contrast, fully collusion-proof mechanisms are limited to posted-price formats, which fail to guarantee even approximate efficiency. This paper aims to bridge this gap by designing auctions that achieve good welfare and revenue guarantees even when some bidders collude. We first characterize the strategic behavior of colluding bidders under VCG and prove that such bidders optimally bid shade: they never overbid or take additional items, but instead reduce the auction price. This characterization enables a Bulow-Klemperer type result: adding colluding bidders can only improve welfare and revenue relative to running VCG on the non-colluding group alone. We then propose a Hybrid VCG (H-VCG) mechanism that combines VCG applied to non-colluding bidders with a posted-price mechanism for colluding bidders, assuming access to a black-box collusion detection algorithm. We show that H-VCG is ex-post dominant-strategy incentive compatible (DSIC) and derive probabilistic guarantees on expected welfare and revenue under both known and unknown valuation distributions. Numerical experiments across several distributions demonstrate that H-VCG consistently outperforms VCG restricted to non-colluding bidders and approaches the performance of the ideal VCG mechanism assuming universal truthfulness. Our results provide a principled framework for incorporating collusion detection into mechanism design, offering a step toward collusion-resistant auctions.

</details>


### [142] [Perturbing Best Responses in Zero-Sum Games](https://arxiv.org/abs/2511.12523)
*Adam Dziwoki,Rostislav Horcik*

Main category: cs.GT

TL;DR: 研究扰动对零和博弈中近似纳什均衡的基于最优响应算法的影响，表明扰动可减少迭代次数，部分情况迭代次数期望为对数级，且在特定游戏中可高效进行效用扰动。


<details>
  <summary>Details</summary>
Motivation: 探究扰动对零和博弈中Double Oracle和Fictitious Play这两种基于最优响应算法的影响。

Method: 假设计算最优响应的预言机在选择最优响应前对效用进行扰动。

Result: 使用这种预言机可减少两种算法的迭代次数，部分情况迭代次数期望为对数级；虽效用扰动计算量大，但在纯策略有内部结构的游戏中可高效进行。

Conclusion: 效用扰动能改善基于最优响应算法在零和博弈中近似纳什均衡的表现，且在特定游戏中有高效实现方式。

Abstract: This paper investigates the impact of perturbations on the best-response-based algorithms approximating Nash equilibria in zero-sum games, namely Double Oracle and Fictitious Play. More precisely, we assume that the oracle computing the best responses perturbs the utilities before selecting the best response. We show that using such an oracle reduces the number of iterations for both algorithms. For some cases, suitable perturbations ensure the expected number of iterations is logarithmic. Although the utility perturbation is computationally demanding as it requires iterating through all pure strategies, we demonstrate that one can efficiently perturb the utilities in games where pure strategies have further inner structure.

</details>


### [143] [Bandit Learning in Housing Markets](https://arxiv.org/abs/2511.12629)
*Shiyun Lin*

Main category: cs.GT

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The housing market, also known as one-sided matching market, is a classic exchange economy model where each agent on the demand side initially owns an indivisible good (a house) and has a personal preference over all goods. The goal is to find a core-stable allocation that exhausts all mutually beneficial exchanges among subgroups of agents. While this model has been extensively studied in economics and computer science due to its broad applications, little attention has been paid to settings where preferences are unknown and must be learned through repeated interactions. In this paper, we propose a statistical learning model within the multi-player multi-armed bandit framework, where players (agents) learn their preferences over arms (goods) from stochastic rewards. We introduce the notion of core regret for each player as the market objective. We study both centralized and decentralized approaches, proving $O(N \log T / Δ^2)$ upper bounds on regret, where $N$ is the number of players, $T$ is the time horizon and $Δ$ is the minimum preference gap among players. For the decentralized setting, we also establish a matching lower bound, demonstrating that our algorithm is order-optimal.

</details>


### [144] [Rethinking Data Value: Asymmetric Data Shapley for Structure-Aware Valuation in Data Markets and Machine Learning Pipelines](https://arxiv.org/abs/2511.12863)
*Xi Zheng,Yinghui Huang,Xiangyu Chang,Ruoxi Jia,Yong Tan*

Main category: cs.GT

TL;DR: 提出Asymmetric Data Shapley (ADS)框架解决传统Data Shapley (DS)局限性，在数据估值表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统DS的对称公理假设无法捕捉现代ML/AI工作流中数据的方向和时间依赖，需新框架进行数据估值。

Method: 引入ADS框架，放宽对称性，开发蒙特卡罗估计器MC - ADS和k近邻替代KNN - ADS两个计算程序。

Result: 在有方向和时间依赖的典型场景中，ADS通过区分新贡献和冗余贡献，尊重训练顺序，优于基准方法。

Conclusion: ADS是数据市场和复杂ML/AI管道中公平数据估值的原则性和实用性方法。

Abstract: Rigorous valuation of individual data sources is critical for fair compensation in data markets, informed data acquisition, and transparent development of ML/AI models. Classical Data Shapley (DS) provides a essential axiomatic framework for data valuation but is constrained by its symmetry axiom that assumes interchangeability of data sources. This assumption fails to capture the directional and temporal dependencies prevalent in modern ML/AI workflows, including the reliance of duplicated or augmented data on original sources and the order-specific contributions in sequential pipelines such as federated learning and multi-stage LLM fine tuning. To address these limitations, we introduce Asymmetric Data Shapley (ADS), a structure-aware data valuation framework for modern ML/AI pipelines. ADS relaxes symmetry by averaging marginal contributions only over permutations consistent with an application-specific ordering of data groups. It preserves efficiency and linearity, maintains within group symmetry and directional precedence across groups, and reduces to DS when the ordering collapses to a single group. We develop two complementary computational procedures for ADS: (i) a Monte Carlo estimator (MC-ADS) with finite-sample accuracy guarantees, and (ii) a k-nearest neighbor surrogate (KNN-ADS) that is exact and efficient for KNN predictors. Across representative settings with directional and temporal dependence, ADS consistently outperforms benchmark methods by distinguishing novel from redundant contributions and respecting the sequential nature of training. These results establish ADS as a principled and practical approach to equitable data valuation in data markets and complex ML/AI pipelines.

</details>


### [145] [Resilient and Efficient Allocation for Large-Scale Autonomous Fleets via Decentralized Coordination](https://arxiv.org/abs/2511.12879)
*Ashish Kumar Perukari,Polina Khoroshevskaya*

Main category: cs.GT

TL;DR: 提出一种大规模资源分配的侧信息感知方法，结合分布预测与分散协调，在真实场景验证，降低失败率且可扩展。


<details>
  <summary>Details</summary>
Motivation: 在不确定性下对大规模自主车队进行快速、有弹性的稀缺资源分配。

Method: 结合分布预测与分散协调的侧信息感知方法，利用本地侧信息构建风险模型，通过轻量级共识 - ADMM 例程在稀疏通信图上协调代理。

Result: 在真实城市道路网络和卫星星座上验证，与多种基线对比，在相同成本下降低失败率 30 - 55%，能扩展到数千个代理且运行时间接近线性，大概率保持可行性。

Conclusion: 该方法在大规模资源分配中有效，能降低失败率并具备良好的可扩展性。

Abstract: Operating large autonomous fleets demands fast, resilient allocation of scarce resources (such as energy and fuel, charger access and maintenance slots, time windows, and communication bandwidth) under uncertainty. We propose a side-information-aware approach for resource allocation at scale that combines distributional predictions with decentralized coordination. Local side information shapes per-agent risk models for consumption, which are coupled through chance constraints on failures. A lightweight consensus-ADMM routine coordinates agents over a sparse communication graph, enabling near-centralized performance while avoiding single points of failure. We validate the framework on real urban road networks with autonomous vehicles and on a representative satellite constellation, comparing against greedy, no-side-information, and oracle central baselines. Our method reduces failure rates by 30-55% at matched cost and scales to thousands of agents with near-linear runtime, while preserving feasibility with high probability.

</details>


### [146] [An FPTAS for 7/9-Approximation to Maximin Share Allocations](https://arxiv.org/abs/2511.13056)
*Xin Huang,Shengwei Zhou*

Main category: cs.GT

TL;DR: 提出新算法实现7/9近似的不可分割物品最大最小份额（MMS）分配，有FPTAS且算法更简单


<details>
  <summary>Details</summary>
Motivation: 改进当前不可分割物品最大最小份额（MMS）分配的近似比

Method: 构建新的分析框架

Result: 实现7/9近似比，得到FPTAS在特定时间内实现7/9 - ε近似，算法比之前更简单

Conclusion: 新算法在不可分割物品MMS分配上有更好的近似比和简单性

Abstract: We present a new algorithm that achieves a $\frac{7}{9}$-approximation for the maximin share (MMS) allocation of indivisible goods under additive valuations, improving the current best ratio of $\frac{10}{13}$ (Heidari et al., SODA 2026). Building on a new analytical framework, we further obtain an FPTAS that achieves a $\frac{7}{9}-\varepsilon$ approximation in $\tfrac{1}{\varepsilon} \cdot \mathrm{poly}(n,m)$ time. Compared with prior work (Heidari et al., SODA 2026), our algorithm is substantially simpler.

</details>


### [147] [MEV in Multiple Concurrent Proposer Blockchains](https://arxiv.org/abs/2511.13080)
*Steven Landers,Benjamin Marsh*

Main category: cs.GT

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We analyze maximal extractable value in multiple concurrent proposer blockchains, where multiple blocks become data available before their final execution order is determined. This concurrency breaks the single builder assumption of sequential chains and introduces new MEV channels, including same tick duplicate steals, proposer to proposer auctions, and timing races driven by proof of availability latency. We develop a hazard normalized model of delay and inclusion, derive a closed form delay envelope \(M(τ)\), and characterize equilibria for censorship, duplication, and auction games. We show how deterministic priority DAG scheduling and duplicate aware payouts neutralize same tick MEV while preserving throughput, identifying simple protocol configurations to mitigate MCP specific extraction without centralized builders.

</details>


### [148] [The Publication Choice Problem](https://arxiv.org/abs/2511.13678)
*Haichuan Wang,Yifan Wu,Haifeng Xu*

Main category: cs.GT

TL;DR: 引入博弈论框架分析研究人员发表选择与期刊影响因子的双向作用，分析均衡特性及‘亮点’标签对期刊影响。


<details>
  <summary>Details</summary>
Motivation: 分析研究人员个体发表选择如何响应并塑造期刊影响。

Method: 引入博弈论框架（Publication Choice Problem），分析纯策略均衡及特性。

Result: Publication Choice Problem存在纯策略均衡，二元研究者类型下均衡唯一；竞争期刊标注‘亮点’可能降低其他期刊整体影响，非竞争期刊相反。

Conclusion: 研究为理解研究人员发表行为与期刊影响的关系提供了理论框架和见解。

Abstract: Researchers strategically choose where to submit their work in order to maximize its impact, and these publication decisions in turn determine venues' impact factors. To analyze how individual publication choices both respond to and shape venue impact, we introduce a game-theoretic framework, coined the Publication Choice Problem, that captures this two-way interplay. We show the existence of a pure-strategy equilibrium in the Publication Choice Problem and its uniqueness under binary researcher types. Our characterizations of the equilibrium properties offer insights about what publication behaviors better indicate a researcher's impact level. Through equilibrium analysis, we further investigate how labeling papers with ``spotlight'' affects the impact factor of venues in the research community. Our analysis shows that competitive venue labeling top papers with ``spotlight'' may decrease the overall impact of other venues in the community, while less competitive venues with ``spotlight'' labeling have the opposite impact.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [149] [The Environmental Impact of Ensemble Techniques in Recommender Systems](https://arxiv.org/abs/2511.11649)
*Jannik Nitschke*

Main category: cs.IR

TL;DR: 研究集成技术在推荐系统中的环境影响，实验显示集成方法精度有提升但能耗大增，选择性策略效率更高。


<details>
  <summary>Details</summary>
Motivation: 集成技术在推荐系统中能提升精度，但环境影响未被衡量，需研究其与单一优化模型在环境影响上的差异。

Method: 在两个框架下对四个数据集进行93次实验，评估四种集成策略，用智能插头测量能耗。

Result: 精度 - 能耗关系非线性，集成方法精度提升0.3 - 5.7%，能耗增加19 - 2549%，Top Performers集成策略效率最佳。

Conclusion: 首次系统测量集成推荐系统能耗和碳足迹，选择性策略效率更高，指出工业规模可扩展性局限，有助于可持续算法选择。

Abstract: Ensemble techniques in recommender systems have demonstrated accuracy improvements of 10-30%, yet their environmental impact remains unmeasured. While deep learning recommendation algorithms can generate up to 3,297 kg CO2 per paper, ensemble methods have not been sufficiently evaluated for energy consumption. This thesis investigates how ensemble techniques influence environmental impact compared to single optimized models.
  We conducted 93 experiments across two frameworks (Surprise for rating prediction, LensKit for ranking) on four datasets spanning 100,000 to 7.8 million interactions. We evaluated four ensemble strategies (Average, Weighted, Stacking/Rank Fusion, Top Performers) against simple baselines and optimized single models, measuring energy consumption with a smart plug.
  Results revealed a non-linear accuracy-energy relationship. Ensemble methods achieved 0.3-5.7% accuracy improvements while consuming 19-2,549% more energy depending on dataset size and strategy. The Top Performers ensemble showed best efficiency: 0.96% RMSE improvement with 18.8% energy overhead on MovieLens-1M, and 5.7% NDCG improvement with 103% overhead on MovieLens-100K. Exhaustive averaging strategies consumed 88-270% more energy for comparable gains. On the largest dataset (Anime, 7.8M interactions), the Surprise ensemble consumed 2,005% more energy (0.21 Wh vs. 0.01 Wh) for 1.2% accuracy improvement, producing 53.8 mg CO2 versus 2.6 mg CO2 for the single model.
  This research provides one of the first systematic measurements of energy and carbon footprint for ensemble recommender systems, demonstrates that selective strategies offer superior efficiency over exhaustive averaging, and identifies scalability limitations at industrial scale. These findings enable informed decisions about sustainable algorithm selection in recommender systems.

</details>


### [150] [GroupRank: A Groupwise Reranking Paradigm Driven by Reinforcement Learning](https://arxiv.org/abs/2511.11653)
*Duolin Sun,Meixiu Long,Dan Yang,Yihan Jiao,Zhehao Tan,Jie Feng,Junjie Wang,Yue Shen,Peng Wei,Jian Wang,Jinjie Gu*

Main category: cs.IR

TL;DR: 提出Groupwise重排序范式解决现有重排序范式的问题，采用GRPO训练，提出合成数据管道，实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有重排序范式存在问题，点式方法易陷入排名短视陷阱，列表式方法有列表刚性问题。

Method: 提出Groupwise范式，联合输入查询和候选文档组进行组内比较；采用GRPO训练，使用异构奖励函数；提出合成高质量检索和排序数据的管道。

Result: 在BRIGHT和R2MED两个推理密集型检索基准上验证了方法的有效性。

Conclusion: 所提方法能有效解决现有重排序范式的问题，提升重排序性能。

Abstract: Large Language Models have shown strong potential as rerankers to enhance the overall performance of RAG systems. However, existing reranking paradigms are constrained by a core theoretical and practical dilemma: Pointwise methods, while simple and highly flexible, evaluate documents independently, making them prone to the Ranking Myopia Trap, overlooking the relative importance between documents. In contrast, Listwise methods can perceive the global ranking context, but suffer from inherent List Rigidity, leading to severe scalability and flexibility issues when handling large candidate sets. To address these challenges, we propose Groupwise, a novel reranking paradigm. In this approach, the query and a group of candidate documents are jointly fed into the model, which performs within-group comparisons to assign individual relevance scores to each document. This design retains the flexibility of Pointwise methods while enabling the comparative capability of Listwise methods. We further adopt GRPO for model training, equipped with a heterogeneous reward function that integrates ranking metrics with a distributional reward aimed at aligning score distributions across groups. To overcome the bottleneck caused by the scarcity of high quality labeled data, we further propose an innovative pipeline for synthesizing high quality retrieval and ranking data. The resulting data can be leveraged not only for training the reranker but also for training the retriever. Extensive experiments validate the effectiveness of our approach. On two reasoning intensive retrieval benchmarks, BRIGHT and R2MED.

</details>


### [151] [A Multimodal Manufacturing Safety Chatbot: Knowledge Base Design, Benchmark Development, and Evaluation of Multiple RAG Approaches](https://arxiv.org/abs/2511.11847)
*Ryan Singh,Austin Hamilton,Amanda White,Michael Wise,Ibrahim Yousif,Arthur Carvalho,Zhe Shan,Reza Abrisham Baf,Mohammad Mayyas,Lora A. Cavuoto,Fadel M. Megahed*

Main category: cs.IR

TL;DR: 本文用设计科学研究方法，开发大语言模型驱动的多模态聊天机器人用于工业5.0安全培训，经测试评估，给出最佳配置，还提供了开源聊天机器人、验证基准和系统方法论。


<details>
  <summary>Details</summary>
Motivation: 现代制造业保障工人安全是挑战，工业5.0需以人类为中心的安全培训系统。

Method: 采用设计科学研究方法，开发多模态聊天机器人，用检索增强生成结合文档，开发特定领域问答基准，进行全因子设计测试和自动评估，让专家评估。

Result: 检索策略和模型配置对性能影响大，最佳配置准确率86.66%，平均延迟10.04秒，每次查询平均成本0.005美元。

Conclusion: 提供开源安全培训聊天机器人、验证基准和系统方法论用于工业5.0安全培训系统设计评估。

Abstract: Ensuring worker safety remains a critical challenge in modern manufacturing environments. Industry 5.0 reorients the prevailing manufacturing paradigm toward more human-centric operations. Using a design science research methodology, we identify three essential requirements for next-generation safety training systems: high accuracy, low latency, and low cost. We introduce a multimodal chatbot powered by large language models that meets these design requirements. The chatbot uses retrieval-augmented generation to ground its responses in curated regulatory and technical documentation. To evaluate our solution, we developed a domain-specific benchmark of expert-validated question and answer pairs for three representative machines: a Bridgeport manual mill, a Haas TL-1 CNC lathe, and a Universal Robots UR5e collaborative robot. We tested 24 RAG configurations using a full-factorial design and assessed them with automated evaluations of correctness, latency, and cost. Our top 2 configurations were then evaluated by ten industry experts and academic researchers. Our results show that retrieval strategy and model configuration have a significant impact on performance. The top configuration (selected for chatbot deployment) achieved an accuracy of 86.66%, an average latency of 10.04 seconds, and an average cost of $0.005 per query. Overall, our work provides three contributions: an open-source, domain-grounded safety training chatbot; a validated benchmark for evaluating AI-assisted safety instruction; and a systematic methodology for designing and assessing AI-enabled instructional and immersive safety training systems for Industry 5.0 environments.

</details>


### [152] [ComLQ: Benchmarking Complex Logical Queries in Information Retrieval](https://arxiv.org/abs/2511.12004)
*Ganlin Xu,Zhitao Yin,Linghao Zhang,Jiaqing Liang,Weijia Lu,Xiaodong Zhang,Zhifei Yang,Sihang Jiang,Deqing Yang*

Main category: cs.IR

TL;DR: 现有信息检索基准难以评估复杂逻辑查询，本文提出用大语言模型构建新数据集ComLQ，设计新评估指标LSNC@K，实验显示现有模型处理复杂逻辑查询能力有限。


<details>
  <summary>Details</summary>
Motivation: 现有信息检索基准主要关注简单查询，无法充分评估模型在复杂逻辑查询上的性能，不能满足现实场景需求。

Method: 利用大语言模型构建新数据集ComLQ，设计子图引导提示生成特定逻辑结构查询，通过专家标注确保数据质量；提出新评估指标LSNC@K。

Result: 实验表明现有检索模型在复杂逻辑查询上表现有限，在含否定的查询上更差，建模排除能力弱。

Conclusion: 现有信息检索模型在处理复杂逻辑查询尤其是含否定的查询方面存在不足，需要进一步改进。

Abstract: Information retrieval (IR) systems play a critical role in navigating information overload across various applications. Existing IR benchmarks primarily focus on simple queries that are semantically analogous to single- and multi-hop relations, overlooking \emph{complex logical queries} involving first-order logic operations such as conjunction ($\land$), disjunction ($\lor$), and negation ($\lnot$). Thus, these benchmarks can not be used to sufficiently evaluate the performance of IR models on complex queries in real-world scenarios. To address this problem, we propose a novel method leveraging large language models (LLMs) to construct a new IR dataset \textbf{ComLQ} for \textbf{Com}plex \textbf{L}ogical \textbf{Q}ueries, which comprises 2,909 queries and 11,251 candidate passages. A key challenge in constructing the dataset lies in capturing the underlying logical structures within unstructured text. Therefore, by designing the subgraph-guided prompt with the subgraph indicator, an LLM (such as GPT-4o) is guided to generate queries with specific logical structures based on selected passages. All query-passage pairs in ComLQ are ensured \emph{structure conformity} and \emph{evidence distribution} through expert annotation. To better evaluate whether retrievers can handle queries with negation, we further propose a new evaluation metric, \textbf{Log-Scaled Negation Consistency} (\textbf{LSNC@$K$}). As a supplement to standard relevance-based metrics (such as nDCG and mAP), LSNC@$K$ measures whether top-$K$ retrieved passages violate negation conditions in queries. Our experimental results under zero-shot settings demonstrate existing retrieval models' limited performance on complex logical queries, especially on queries with negation, exposing their inferior capabilities of modeling exclusion.

</details>


### [153] [From Scaling to Structured Expressivity: Rethinking Transformers for CTR Prediction](https://arxiv.org/abs/2511.12081)
*Bencheng Yan,Yuejie Lei,Zhiyuan Zeng,Di Wang,Kaiyi Lin,Pengjie Wang,Jian Xu,Bo Zheng*

Main category: cs.IR

TL;DR: 论文指出CTR预测深度模型收益递减问题，提出Field - Aware Transformer (FAT)模型，给出缩放定律，在基准测试和线上部署表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决CTR预测深度模型收益递减问题，原因是模型结构与数据不匹配。

Method: 引入Field - Aware Transformer (FAT)，将基于字段的交互先验嵌入注意力中，通过分解内容对齐和跨字段调制实现。

Result: 在大规模基准测试中，AUC比现有方法最高提升0.51%；线上部署CTR提升2.33%，RPM提升0.66%。

Conclusion: 推荐系统的有效扩展源于结构化表达能力，即架构与数据语义的一致性，而非模型规模。

Abstract: Despite massive investments in scale, deep models for click-through rate (CTR) prediction often exhibit rapidly diminishing returns - a stark contrast to the smooth, predictable gains seen in large language models. We identify the root cause as a structural misalignment: Transformers assume sequential compositionality, while CTR data demand combinatorial reasoning over high-cardinality semantic fields. Unstructured attention spreads capacity indiscriminately, amplifying noise under extreme sparsity and breaking scalable learning. To restore alignment, we introduce the Field-Aware Transformer (FAT), which embeds field-based interaction priors into attention through decomposed content alignment and cross-field modulation. This design ensures model complexity scales with the number of fields F, not the total vocabulary size n >> F, leading to tighter generalization and, critically, observed power-law scaling in AUC as model width increases. We present the first formal scaling law for CTR models, grounded in Rademacher complexity, that explains and predicts this behavior. On large-scale benchmarks, FAT improves AUC by up to +0.51% over state-of-the-art methods. Deployed online, it delivers +2.33% CTR and +0.66% RPM. Our work establishes that effective scaling in recommendation arises not from size, but from structured expressivity-architectural coherence with data semantics.

</details>


### [154] [Continuous-time Discrete-space Diffusion Model for Recommendation](https://arxiv.org/abs/2511.12114)
*Chengyi Liu,Xiao Chen,Shijie Wang,Wenqi Fan,Qing Li*

Main category: cs.IR

TL;DR: 信息爆炸时代推荐系统重要，现有基于扩散的推荐器有局限，提出CDRec框架，实验证明其在准确性和计算效率上表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的推荐器在连续空间操作存在信息损失和计算效率低的问题，需改进。

Method: 提出CDRec框架，通过在连续时间上对历史交互进行离散扩散建模用户行为模式，引入流行度感知噪声调度和结合一致性参数化与对比学习目标的训练框架。

Result: 在真实数据集上的大量实验表明CDRec在推荐准确性和计算效率上表现优越。

Conclusion: CDRec框架能有效解决现有基于扩散推荐器的问题，提升推荐性能。

Abstract: In the era of information explosion, Recommender Systems (RS) are essential for alleviating information overload and providing personalized user experiences. Recent advances in diffusion-based generative recommenders have shown promise in capturing the dynamic nature of user preferences. These approaches explore a broader range of user interests by progressively perturbing the distribution of user-item interactions and recovering potential preferences from noise, enabling nuanced behavioral understanding. However, existing diffusion-based approaches predominantly operate in continuous space through encoded graph-based historical interactions, which may compromise potential information loss and suffer from computational inefficiency. As such, we propose CDRec, a novel Continuous-time Discrete-space Diffusion Recommendation framework, which models user behavior patterns through discrete diffusion on historical interactions over continuous time. The discrete diffusion algorithm operates via discrete element operations (e.g., masking) while incorporating domain knowledge through transition matrices, producing more meaningful diffusion trajectories. Furthermore, the continuous-time formulation enables flexible adaptive sampling. To better adapt discrete diffusion models to recommendations, CDRec introduces: (1) a novel popularity-aware noise schedule that generates semantically meaningful diffusion trajectories, and (2) an efficient training framework combining consistency parameterization for fast sampling and a contrastive learning objective guided by multi-hop collaborative signals for personalized recommendation. Extensive experiments on real-world datasets demonstrate CDRec's superior performance in both recommendation accuracy and computational efficiency.

</details>


### [155] [Task-Aware Retrieval Augmentation for Dynamic Recommendation](https://arxiv.org/abs/2511.12495)
*Zhen Tao,Xinke Jiang,Qingshuai Feng,Haoyu Zhang,Lun Du,Yuchen Fang,Hao Miao,Bangquan Xie,Qingqiang Sun*

Main category: cs.IR

TL;DR: 提出TarDGR框架解决动态推荐系统中GNN微调的泛化问题，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有利用预训练动态GNN的动态推荐系统在微调时因时间差异存在泛化问题，难以捕捉用户偏好变化。

Method: 提出TarDGR框架，包含任务感知评估机制和基于图Transformer的任务感知模型，推理时检索并融合任务感知子图。

Result: 在多个大规模动态图数据集上实验，TarDGR始终优于现有方法。

Conclusion: TarDGR具有优越的准确性和泛化能力。

Abstract: Dynamic recommendation systems aim to provide personalized suggestions by modeling temporal user-item interactions across time-series behavioral data. Recent studies have leveraged pre-trained dynamic graph neural networks (GNNs) to learn user-item representations over temporal snapshot graphs. However, fine-tuning GNNs on these graphs often results in generalization issues due to temporal discrepancies between pre-training and fine-tuning stages, limiting the model's ability to capture evolving user preferences. To address this, we propose TarDGR, a task-aware retrieval-augmented framework designed to enhance generalization capability by incorporating task-aware model and retrieval-augmentation. Specifically, TarDGR introduces a Task-Aware Evaluation Mechanism to identify semantically relevant historical subgraphs, enabling the construction of task-specific datasets without manual labeling. It also presents a Graph Transformer-based Task-Aware Model that integrates semantic and structural encodings to assess subgraph relevance. During inference, TarDGR retrieves and fuses task-aware subgraphs with the query subgraph, enriching its representation and mitigating temporal generalization issues. Experiments on multiple large-scale dynamic graph datasets demonstrate that TarDGR consistently outperforms state-of-the-art methods, with extensive empirical evidence underscoring its superior accuracy and generalization capabilities.

</details>


### [156] [Tokenize Once, Recommend Anywhere: Unified Item Tokenization for Multi-domain LLM-based Recommendation](https://arxiv.org/abs/2511.12922)
*Yu Hou,Won-Yong Shin*

Main category: cs.IR

TL;DR: 提出统一物品标记框架UniTok，结合MoE架构和码本将物品转化为离散标记，实验证明其有效、理论合理且泛化性强。


<details>
  <summary>Details</summary>
Motivation: 现有物品标记方法需为每个物品领域单独训练模型，泛化性受限，且跨领域构建统一标记难保留特定领域信息。

Method: 提出UniTok框架，用共享编码器将不同领域物品投影到统一潜在空间，路由到特定领域专家捕获独特语义，用共享专家编码跨领域通用知识，还提出互信息校准机制缓解领域语义不平衡。

Result: 在广泛真实数据集上实验，比强基准提升达51.89%，显示架构设计和优化分析有效性，跨领域表现稳健无需按领域重新训练。

Conclusion: UniTok框架有效、理论合理且泛化性强，优于现有基线。

Abstract: Large language model (LLM)-based recommender systems have achieved high-quality performance by bridging the discrepancy between the item space and the language space through item tokenization. However, existing item tokenization methods typically require training separate models for each item domain, limiting generalization. Moreover, the diverse distributions and semantics across item domains make it difficult to construct a unified tokenization that preserves domain-specific information. To address these challenges, we propose UniTok, a Unified item Tokenization framework that integrates our own mixture-of-experts (MoE) architecture with a series of codebooks to convert items into discrete tokens, enabling scalable tokenization while preserving semantic information across multiple item domains. Specifically, items from different domains are first projected into a unified latent space through a shared encoder. They are then routed to domain-specific experts to capture the unique semantics, while a shared expert, which is always active, encodes common knowledge transferable across domains. Additionally, to mitigate semantic imbalance across domains, we present a mutual information calibration mechanism, which guides the model towards retaining similar levels of semantic information for each domain. Comprehensive experiments on wide-ranging real-world datasets demonstrate that the proposed UniTok framework is (a) highly effective: achieving up to 51.89% improvements over strong benchmarks, (b) theoretically sound: showing the analytical validity of our architectural design and optimization; and (c) highly generalizable: demonstrating robust performance across diverse domains without requiring per-domain retraining, a capability not supported by existing baselines.

</details>


### [157] [DualGR: Generative Retrieval with Long and Short-Term Interests Modeling](https://arxiv.org/abs/2511.12518)
*Zhongchao Yi,Kai Feng,Xiaojian Ma,Yalong Wang,Yongqi Liu,Han Li,Zhengyang Zhou,Yang Wang*

Main category: cs.IR

TL;DR: 提出DualGR生成式检索框架解决大规模工业推荐系统检索挑战，在快手短视频推荐系统表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决生成式检索在大规模工业推荐系统中平衡用户长短兴趣、生成语义ID噪声干扰、缺乏负反馈显式建模的问题。

Method: 提出DualGR框架，用DBR显式建模用户长短行为，用S2D控制噪声、提升效率，提出ENTP - Loss处理负反馈。

Result: 在快手大规模短视频推荐系统在线A/B测试中，视频观看量提升0.527%，观看时长提升0.432%。

Conclusion: DualGR是工业生成式检索实用有效的范式。

Abstract: In large-scale industrial recommendation systems, retrieval must produce high-quality candidates from massive corpora under strict latency. Recently, Generative Retrieval (GR) has emerged as a viable alternative to Embedding-Based Retrieval (EBR), which quantizes items into a finite token space and decodes candidates autoregressively, providing a scalable path that explicitly models target-history interactions via cross-attention. However, three challenges persist: 1) how to balance users' long-term and short-term interests , 2) noise interference when generating hierarchical semantic IDs (SIDs), 3) the absence of explicit modeling for negative feedback such as exposed items without clicks. To address these challenges, we propose DualGR, a generative retrieval framework that explicitly models dual horizons of user interests with selective activation. Specifically, DualGR utilizes Dual-Branch Long/Short-Term Router (DBR) to cover both stable preferences and transient intents by explicitly modeling users' long- and short-term behaviors. Meanwhile, Search-based SID Decoding (S2D) is presented to control context-induced noise and enhance computational efficiency by constraining candidate interactions to the current coarse (level-1) bucket during fine-grained (level-2/3) SID prediction. % also reinforcing intra-class consistency. Finally, we propose an Exposure-aware Next-Token Prediction Loss (ENTP-Loss) that treats "exposed-but-unclicked" items as hard negatives at level-1, enabling timely interest fade-out. On the large-scale Kuaishou short-video recommendation system, DualGR has achieved outstanding performance. Online A/B testing shows +0.527% video views and +0.432% watch time lifts, validating DualGR as a practical and effective paradigm for industrial generative retrieval.

</details>


### [158] [Exploring Multi-Table Retrieval Through Iterative Search](https://arxiv.org/abs/2511.13418)
*Allaa Boutaleb,Bernd Amann,Rafael Angarita,Hubert Naacke*

Main category: cs.IR

TL;DR: 本文将多表检索构建为迭代搜索过程，提出贪心连接感知检索算法，实验表明迭代方法在检索性能上有竞争力且速度快，凸显迭代启发式方法潜力。


<details>
  <summary>Details</summary>
Motivation: 开放域问答中多表检索需兼顾语义相关性和结构连贯性，精确优化方法计算复杂，简单贪心启发式方法无法找到连贯可连接集。

Method: 将多表检索构建为迭代搜索过程，提出通用框架和贪心连接感知检索算法，平衡相关性、覆盖率和可连接性。

Result: 在5个NL2SQL基准测试中，迭代方法与基于MIP的方法相比检索性能有竞争力，速度快4 - 400倍。

Conclusion: 迭代启发式方法在实际、可扩展和组合感知的检索方面有潜力。

Abstract: Open-domain question answering over datalakes requires retrieving and composing information from multiple tables, a challenging subtask that demands semantic relevance and structural coherence (e.g., joinability). While exact optimization methods like Mixed-Integer Programming (MIP) can ensure coherence, their computational complexity is often prohibitive. Conversely, simpler greedy heuristics that optimize for query coverage alone often fail to find these coherent, joinable sets. This paper frames multi-table retrieval as an iterative search process, arguing this approach offers advantages in scalability, interpretability, and flexibility. We propose a general framework and a concrete instantiation: a fast, effective Greedy Join-Aware Retrieval algorithm that holistically balances relevance, coverage, and joinability. Experiments across 5 NL2SQL benchmarks demonstrate that our iterative method achieves competitive retrieval performance compared to the MIP-based approach while being 4-400x faster depending on the benchmark and search space settings. This work highlights the potential of iterative heuristics for practical, scalable, and composition-aware retrieval.

</details>


### [159] [MindRec: Mind-inspired Coarse-to-fine Decoding for Generative Recommendation](https://arxiv.org/abs/2511.12597)
*Mengyao Gao,Chongming Gao,Haoyan Liu,Qingpeng Cai,Peng Jiang,Jiajia Chen,Shuai Yuan,Xiangnan He*

Main category: cs.IR

TL;DR: 本文提出受人类思维启发的推荐框架MindRec，结合层次类别树和扩散束搜索算法，实验显示推荐性能提升9.5%。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型推荐系统因单向逻辑流难以产生全局最优推荐，受人类思维方式启发提出新方法。

Method: 提出MindRec框架，先生成反映用户偏好的关键令牌再扩展，用层次类别树引导决策，设计扩散束搜索算法。

Result: MindRec在top - 1推荐性能上比现有方法平均提升9.5%。

Conclusion: MindRec有提升推荐准确性的潜力，代码可在指定链接获取。

Abstract: Recent advancements in large language model-based recommendation systems often represent items as text or semantic IDs and generate recommendations in an auto-regressive manner. However, due to the left-to-right greedy decoding strategy and the unidirectional logical flow, such methods often fail to produce globally optimal recommendations. In contrast, human reasoning does not follow a rigid left-to-right sequence. Instead, it often begins with keywords or intuitive insights, which are then refined and expanded. Inspired by this fact, we propose Mind-inspired Recommender (MindRec), a novel generative framework that emulates human thought processes. Particularly, our method first generates key tokens that reflect user preferences, and then expands them into the complete item, enabling flexible and human-like generation. To further emulate the structured nature of human decision-making, we organize items into a hierarchical category tree. This structure guides the model to first produce the coarse-grained category and then progressively refine its selection through finer-grained subcategories before generating the specific item. To mitigate the local optimum problem inherent in greedy decoding, we design a novel beam search algorithm, Diffusion Beam Search, tailored for our mind-inspired generation paradigm. Experimental results demonstrate that MindRec yields a 9.5\% average improvement in top-1 recommendation performance over state-of-the-art methods, highlighting its potential to enhance recommendation accuracy. The implementation is available via https://github.com/Mr-Peach0301/MindRec.

</details>


### [160] [A Plug-and-Play Spatially-Constrained Representation Enhancement Framework for Local-Life Recommendation](https://arxiv.org/abs/2511.12947)
*Hao Jiang,Guoquan Wang,Sheng Yu,Yang Zeng,Wencong Zeng,Guorui Zhou*

Main category: cs.IR

TL;DR: 本地生活推荐面临空间约束和长尾稀疏挑战，现有方法多以用户为中心，本文提出以物品为中心的ReST框架解决问题。


<details>
  <summary>Details</summary>
Motivation: 本地生活推荐存在空间约束和长尾稀疏问题，现有以用户为中心的方法不适合，需以物品为中心解决。

Method: 提出ReST框架，包括Meta ID预热网络初始化ID表示，基于对比学习的SIDENet网络，采用空间约束硬采样和动态表示对齐策略。

Result: 框架可自适应识别弱ID表示，通过捕捉本地生活服务空间约束特征内的潜在物品关系增强表示，且与热门物品兼容。

Conclusion: 以物品为中心的ReST框架更适合本地生活推荐，能解决相关挑战。

Abstract: Local-life recommendation have witnessed rapid growth, providing users with convenient access to daily essentials. However, this domain faces two key challenges: (1) spatial constraints, driven by the requirements of the local-life scenario, where items are usually shown only to users within a limited geographic area, indirectly reducing their exposure probability; and (2) long-tail sparsity, where few popular items dominate user interactions, while many high-quality long-tail items are largely overlooked due to imbalanced interaction opportunities. Existing methods typically adopt a user-centric perspective, such as modeling spatial user preferences or enhancing long-tail representations with collaborative filtering signals. However, we argue that an item-centric perspective is more suitable for this domain, focusing on enhancing long-tail items representation that align with the spatially-constrained characteristics of local lifestyle services. To tackle this issue, we propose ReST, a Plug-And-Play Spatially-Constrained Representation Enhancement Framework for Long-Tail Local-Life Recommendation. Specifically, we first introduce a Meta ID Warm-up Network, which initializes fundamental ID representations by injecting their basic attribute-level semantic information. Subsequently, we propose a novel Spatially-Constrained ID Representation Enhancement Network (SIDENet) based on contrastive learning, which incorporates two efficient strategies: a spatially-constrained hard sampling strategy and a dynamic representation alignment strategy. This design adaptively identifies weak ID representations based on their attribute-level information during training. It additionally enhances them by capturing latent item relationships within the spatially-constrained characteristics of local lifestyle services, while preserving compatibility with popular items.

</details>


### [161] [Can We Predict the Next Question? A Collaborative Filtering Approach to Modeling User Behavior](https://arxiv.org/abs/2511.12949)
*Bokang Fu,Jiahao Wang,Xiaojing Liu,Yuli Liu*

Main category: cs.IR

TL;DR: 现有大语言模型在建模用户偏好时存在静态化问题，本文提出CFQP框架，实验表明该框架能有效模拟真实用户提问模式，可用于构建主动自适应对话系统。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在建模用户偏好时是静态的，无法捕捉交互行为的动态和顺序特征，利用用户历史问题的时间数据进行预测也存在挑战。

Method: 提出CFQP框架，通过将个性化记忆模块与基于图的偏好传播相结合，动态建模用户 - 问题的交互。

Result: 所提方法能有效生成模拟真实用户提问模式的智能体。

Conclusion: 该方法有潜力用于构建主动和自适应的对话系统。

Abstract: In recent years, large language models (LLMs) have excelled in language understanding and generation, powering advanced dialogue and recommendation systems. However, a significant limitation persists: these systems often model user preferences statically, failing to capture the dynamic and sequential nature of interactive behaviors. The sequence of a user's historical questions provides a rich, implicit signal of evolving interests and cognitive patterns, yet leveraging this temporal data for predictive tasks remains challenging due to the inherent disconnect between language modeling and behavioral sequence modeling.
  To bridge this gap, we propose a Collaborative Filtering-enhanced Question Prediction (CFQP) framework. CFQP dynamically models evolving user-question interactions by integrating personalized memory modules with graph-based preference propagation. This dual mechanism allows the system to adaptively learn from user-specific histories while refining predictions through collaborative signals from similar users. Experimental results demonstrate that our approach effectively generates agents that mimic real-user questioning patterns, highlighting its potential for building proactive and adaptive dialogue systems.

</details>


### [162] [Personalized Federated Recommendation With Knowledge Guidance](https://arxiv.org/abs/2511.12959)
*Jaehyung Lim,Wonbin Kweon,Woojoo Kim,Junyoung Kim,Dongha Kim,Hwanjo Yu*

Main category: cs.IR

TL;DR: 提出FedRKG框架解决联邦推荐模型内存与性能的两难问题，实验表明其优于现有方法，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有联邦推荐模型存在内存高效的单知识模型丢弃有价值个性化信息、高性能的双知识模型内存开销大难以部署的问题。

Method: 提出FedRKG框架，采用知识引导避免完全替换，将全局知识融合到保留的本地嵌入中；引入自适应引导动态调整引导强度。

Result: 在基准数据集上的大量实验表明，FedRKG显著优于现有方法。

Conclusion: FedRKG框架有效解决了联邦推荐模型的两难问题，验证了方法的有效性。

Abstract: Federated Recommendation (FedRec) has emerged as a key paradigm for building privacy-preserving recommender systems. However, existing FedRec models face a critical dilemma: memory-efficient single-knowledge models suffer from a suboptimal knowledge replacement practice that discards valuable personalization, while high-performance dual-knowledge models are often too memory-intensive for practical on-device deployment. We propose Federated Recommendation with Knowledge Guidance (FedRKG), a model-agnostic framework that resolves this dilemma. The core principle, Knowledge Guidance, avoids full replacement and instead fuses global knowledge into preserved local embeddings, attaining the personalization benefits of dual-knowledge within a single-knowledge memory footprint. Furthermore, we introduce Adaptive Guidance, a fine-grained mechanism that dynamically modulates the intensity of this guidance for each user-item interaction, overcoming the limitations of static fusion methods. Extensive experiments on benchmark datasets demonstrate that FedRKG significantly outperforms state-of-the-art methods, validating the effectiveness of our approach. The code is available at https://github.com/Jaehyung-Lim/fedrkg.

</details>


### [163] [Mitigating Recommendation Biases via Group-Alignment and Global-Uniformity in Representation Learning](https://arxiv.org/abs/2511.13041)
*Miaomiao Cai,Min Hou,Lei Chen,Le Wu,Haoyue Bai,Yong Li,Meng Wang*

Main category: cs.IR

TL;DR: 本文分析CF方法偏差原因，提出AURL框架缓解推荐偏差，实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: CF方法因训练数据不平衡产生偏差，现有方法有缺陷，需新方法缓解偏差。

Method: 分析用户和物品表示分布问题，提出组对齐和全局均匀性正则化器，优化正则项缓解偏差。

Result: 在三个真实数据集和多种推荐骨干上实验，验证AURL框架优越性。

Conclusion: AURL框架能有效缓解推荐偏差，优于现有方法。

Abstract: Collaborative Filtering~(CF) plays a crucial role in modern recommender systems, leveraging historical user-item interactions to provide personalized suggestions. However, CF-based methods often encounter biases due to imbalances in training data. This phenomenon makes CF-based methods tend to prioritize recommending popular items and performing unsatisfactorily on inactive users. Existing works address this issue by rebalancing training samples, reranking recommendation results, or making the modeling process robust to the bias. Despite their effectiveness, these approaches can compromise accuracy or be sensitive to weighting strategies, making them challenging to train. In this paper, we deeply analyze the causes and effects of the biases and propose a framework to alleviate biases in recommendation from the perspective of representation distribution, namely Group-Alignment and Global-Uniformity Enhanced Representation Learning for Debiasing Recommendation (AURL). Specifically, we identify two significant problems in the representation distribution of users and items, namely group-discrepancy and global-collapse. These two problems directly lead to biases in the recommendation results. To this end, we propose two simple but effective regularizers in the representation space, respectively named group-alignment and global-uniformity. The goal of group-alignment is to bring the representation distribution of long-tail entities closer to that of popular entities, while global-uniformity aims to preserve the information of entities as much as possible by evenly distributing representations. Our method directly optimizes both the group-alignment and global-uniformity regularization terms to mitigate recommendation biases. Extensive experiments on three real datasets and various recommendation backbones verify the superiority of our proposed framework.

</details>


### [164] [Dimension vs. Precision: A Comparative Analysis of Autoencoders and Quantization for Efficient Vector Retrieval on BEIR SciFact](https://arxiv.org/abs/2511.13057)
*Satyanarayan Pati*

Main category: cs.IR

TL;DR: 本文研究稠密检索模型向量嵌入压缩策略，比较降维和量化方法，发现int8标量量化效果最佳，为高效检索系统部署提供实用指南。


<details>
  <summary>Details</summary>
Motivation: 稠密检索模型高维高精度向量嵌入在实际部署中存在存储和内存挑战，需研究压缩策略。

Method: 在BEIR SciFact基准上，评估降维（深度自动编码器）和量化（float16、int8和二进制）两种压缩策略，通过多种检索指标对比性能。

Result: int8标量量化效果最佳，4倍压缩时nDCG@10仅下降1 - 2%；自动编码器性能有下降；二进制量化不适合该任务。

Conclusion: 为部署高效高性能检索系统提供实用指南。

Abstract: Dense retrieval models have become a standard for state-of-the-art information retrieval. However, their high-dimensional, high-precision (float32) vector embeddings create significant storage and memory challenges for real-world deployment. To address this, we conduct a rigorous empirical study on the BEIR SciFact benchmark, evaluating the trade-offs between two primary compression strategies: (1) Dimensionality Reduction via deep Autoencoders (AE), reducing original 384-dim vectors to latent spaces from 384 down to 12, and (2) Precision Reduction via Quantization (float16, int8, and binary). We systematically compare each method by measuring the "performance loss" (or gain) relative to a float32 baseline across a full suite of retrieval metrics (NDCG, MAP, MRR, Recall, Precision) at various k cutoffs. Our results show that int8 scalar quantization provides the most effective "sweet spot," achieving a 4x compression with a negligible [~1-2%] drop in nDCG@10. In contrast, Autoencoders show a graceful degradation but suffer a more significant performance loss at equivalent 4x compression ratios (AE-96). binary quantization was found to be unsuitable for this task due to catastrophic performance drops. This work provides a practical guide for deploying efficient, high-performance retrieval systems.

</details>


### [165] [Local Collaborative Filtering: A Collaborative Filtering Method that Utilizes Local Similarities among Users](https://arxiv.org/abs/2511.13166)
*Zhaoxin Shen,Dan Wu*

Main category: cs.IR

TL;DR: 本文提出Local Collaborative Filtering (LCF)方法，利用用户局部相似性和大数定律，在Steam游戏数据集实验结果符合实际需求。


<details>
  <summary>Details</summary>
Motivation: 为了在推荐系统中更有效地利用互联网上的用户行为数据。

Method: 提出Local Collaborative Filtering (LCF)方法，利用用户间局部相似性，并使用大数定律整合数据。

Result: 在Steam游戏数据集上进行实验，结果符合实际需求。

Conclusion: LCF方法能有效利用用户行为数据，结果符合实际需求。

Abstract: To leverage user behavior data from the Internet more effectively in recommender systems, this paper proposes a novel collaborative filtering (CF) method called Local Collaborative Filtering (LCF). LCF utilizes local similarities among users and integrates their data using the law of large numbers (LLN), thereby improving the utilization of user behavior data. Experiments are conducted on the Steam game dataset, and the results of LCF align with real-world needs.

</details>


### [166] [Cog-RAG: Cognitive-Inspired Dual-Hypergraph with Theme Alignment Retrieval-Augmented Generation](https://arxiv.org/abs/2511.13201)
*Hao Hu,Yifan Feng,Ruoxue Li,Rundong Xue,Xingliang Hou,Zhiqiang Tian,Yue Gao,Shaoyi Du*

Main category: cs.IR

TL;DR: 提出主题对齐的双超图RAG框架Cog - RAG及认知启发的两阶段检索策略，实验表明其性能显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有集成图结构的RAG研究聚焦低阶实体关系，超图增强方法忽视全局主题组织，为解决这些问题开展研究。

Method: 提出Cog - RAG框架，用主题超图捕捉块间主题结构、实体超图建模高阶语义关系；设计两阶段检索策略，先从主题超图激活相关主题内容，再在实体超图引导细粒度召回和扩散。

Result: 广泛实验显示Cog - RAG显著优于现有最先进的基线方法。

Conclusion: Cog - RAG框架和两阶段检索策略有效，能提升RAG性能。

Abstract: Retrieval-Augmented Generation (RAG) enhances the response quality and domain-specific performance of large language models (LLMs) by incorporating external knowledge to combat hallucinations. In recent research, graph structures have been integrated into RAG to enhance the capture of semantic relations between entities. However, it primarily focuses on low-order pairwise entity relations, limiting the high-order associations among multiple entities. Hypergraph-enhanced approaches address this limitation by modeling multi-entity interactions via hyperedges, but they are typically constrained to inter-chunk entity-level representations, overlooking the global thematic organization and alignment across chunks. Drawing inspiration from the top-down cognitive process of human reasoning, we propose a theme-aligned dual-hypergraph RAG framework (Cog-RAG) that uses a theme hypergraph to capture inter-chunk thematic structure and an entity hypergraph to model high-order semantic relations. Furthermore, we design a cognitive-inspired two-stage retrieval strategy that first activates query-relevant thematic content from the theme hypergraph, and then guides fine-grained recall and diffusion in the entity hypergraph, achieving semantic alignment and consistent generation from global themes to local details. Our extensive experiments demonstrate that Cog-RAG significantly outperforms existing state-of-the-art baseline approaches.

</details>


### [167] [Uncovering Causal Drivers of Energy Efficiency for Industrial Process in Foundry via Time-Series Causal Inference](https://arxiv.org/abs/2511.13389)
*Zhipeng Ma,Bo Nørregaard Jørgensen,Zheng Grace Ma*

Main category: cs.IR

TL;DR: 本文应用时间序列因果推断框架分析丹麦铸造厂数据，识别感应炉熔炼中影响能源效率的操作因素，提出集成聚类 - 因果推断方法，为铸造厂优化提供见解。


<details>
  <summary>Details</summary>
Motivation: 工业铸造过程能源密集且变量复杂，基于相关性分析难以区分因果关系，无法满足决策需求，需有效方法识别影响能源效率的操作因素。

Method: 应用时间序列因果推断框架，结合时间序列聚类将熔炼周期分为不同操作模式，用PCMCI + 算法发现各模式内因果关系。

Result: 发现能源消耗、炉温和材料重量的因果关系是效率核心驱动因素，电压对冷却水温度有延迟影响；高效和低效集群有不同因果结构特征。

Conclusion: 引入集成聚类 - 因果推断管道用于分析能源密集型过程，为铸造厂优化性能、降低能耗和排放提供可行见解。

Abstract: Improving energy efficiency in industrial foundry processes is a critical challenge, as these operations are highly energy-intensive and marked by complex interdependencies among process variables. Correlation-based analyses often fail to distinguish true causal drivers from spurious associations, limiting their usefulness for decision-making. This paper applies a time-series causal inference framework to identify the operational factors that directly affect energy efficiency in induction furnace melting. Using production data from a Danish foundry, the study integrates time-series clustering to segment melting cycles into distinct operational modes with the PCMCI+ algorithm, a state-of-the-art causal discovery method, to uncover cause-effect relationships within each mode. Across clusters, robust causal relations among energy consumption, furnace temperature, and material weight define the core drivers of efficiency, while voltage consistently influences cooling water temperature with a delayed response. Cluster-specific differences further distinguish operational regimes: efficient clusters are characterized by stable causal structures, whereas inefficient ones exhibit reinforcing feedback loops and atypical dependencies. The contributions of this study are twofold. First, it introduces an integrated clustering-causal inference pipeline as a methodological innovation for analyzing energy-intensive processes. Second, it provides actionable insights that enable foundry operators to optimize performance, reduce energy consumption, and lower emissions.

</details>


### [168] [Attention Grounded Enhancement for Visual Document Retrieval](https://arxiv.org/abs/2511.13415)
*Wanqing Cui,Wei Huang,Yazhi Guo,Yibo Hu,Meiguang Jin,Junfeng Ma,Keping Bi*

Main category: cs.IR

TL;DR: 提出AGREE框架解决视觉文档检索问题，在ViDoRe V2基准上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有检索器用粗粒度全局相关性标签训练，依赖表面线索，难以捕捉隐式语义联系，处理非提取式查询能力受限。

Method: 提出AGREE框架，利用多模态大语言模型的跨模态注意力作为局部监督，结合局部和全局信号优化检索器。

Result: 在ViDoRe V2基准上显著优于仅全局监督的基线，定量和定性分析表明能促进查询词与文档区域更深度对齐。

Conclusion: AGREE使视觉文档检索超越表面匹配，实现更准确和可解释的检索。

Abstract: Visual document retrieval requires understanding heterogeneous and multi-modal content to satisfy information needs. Recent advances use screenshot-based document encoding with fine-grained late interaction, significantly improving retrieval performance. However, retrievers are still trained with coarse global relevance labels, without revealing which regions support the match. As a result, retrievers tend to rely on surface-level cues and struggle to capture implicit semantic connections, hindering their ability to handle non-extractive queries. To alleviate this problem, we propose a \textbf{A}ttention-\textbf{G}rounded \textbf{RE}triever \textbf{E}nhancement (AGREE) framework. AGREE leverages cross-modal attention from multimodal large language models as proxy local supervision to guide the identification of relevant document regions. During training, AGREE combines local signals with the global signals to jointly optimize the retriever, enabling it to learn not only whether documents match, but also which content drives relevance. Experiments on the challenging ViDoRe V2 benchmark show that AGREE significantly outperforms the global-supervision-only baseline. Quantitative and qualitative analyses further demonstrate that AGREE promotes deeper alignment between query terms and document regions, moving beyond surface-level matching toward more accurate and interpretable retrieval. Our code is available at: https://anonymous.4open.science/r/AGREE-2025.

</details>


### [169] [Compact Multimodal Language Models as Robust OCR Alternatives for Noisy Textual Clinical Reports](https://arxiv.org/abs/2511.13523)
*Nikita Neveditsin,Pawan Lingras,Salil Patil,Swarup Patil,Vijay Mago*

Main category: cs.IR

TL;DR: 研究评估紧凑多模态语言模型转录嘈杂临床文档的效果，其表现优于传统OCR系统。


<details>
  <summary>Details</summary>
Motivation: 医疗记录数字化中智能手机拍摄的图像有噪声，传统OCR系统表现不佳，需隐私保护的替代方案。

Method: 使用印度医疗环境中的产科超声报告，从转录准确性、噪声敏感性、数值准确性和计算效率方面比较八个系统。

Result: 紧凑多模态模型始终优于经典和神经OCR管道。

Conclusion: 尽管计算成本高，但紧凑多模态模型的鲁棒性和语言适应性使其适合本地医疗数字化。

Abstract: Digitization of medical records often relies on smartphone photographs of printed reports, producing images degraded by blur, shadows, and other noise. Conventional OCR systems, optimized for clean scans, perform poorly under such real-world conditions. This study evaluates compact multimodal language models as privacy-preserving alternatives for transcribing noisy clinical documents. Using obstetric ultrasound reports written in regionally inflected medical English common to Indian healthcare settings, we compare eight systems in terms of transcription accuracy, noise sensitivity, numeric accuracy, and computational efficiency. Compact multimodal models consistently outperform both classical and neural OCR pipelines. Despite higher computational costs, their robustness and linguistic adaptability position them as viable candidates for on-premises healthcare digitization.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [170] [Softmax as a Lagrangian-Legendrian Seam](https://arxiv.org/abs/2511.11573)
*Christopher R. Lee-Jenkins*

Main category: cs.LG

TL;DR: 本文搭建了机器学习与现代微分几何的首座桥梁，分析softmax步骤的几何模型并给出后续研究方向。


<details>
  <summary>Details</summary>
Motivation: 建立机器学习与现代微分几何的联系。

Method: 将softmax的对数几率到概率步骤建模为几何接口，分析其在接触屏幕上的特性，利用Fenchel - Young等式和KL间隙计算距离。

Result: 明确了偏差平移不变性表现为屏幕上的Reeb流，计算出到“接缝”的可计算距离。

Conclusion: 完成二分类和三分类案例分析，给出机器学习后续研究方向，如紧凑对数几率模型、全局不变量及与信息几何的联系。

Abstract: This note offers a first bridge from machine learning to modern differential geometry. We show that the logits-to-probabilities step implemented by softmax can be modeled as a geometric interface: two potential-generated, conservative descriptions (from negative entropy and log-sum-exp) meet along a Legendrian "seam" on a contact screen (the probability simplex) inside a simple folded symplectic collar. Bias-shift invariance appears as Reeb flow on the screen, and the Fenchel-Young equality/KL gap provides a computable distance to the seam. We work out the two- and three-class cases to make the picture concrete and outline next steps for ML: compact logit models (projective or spherical), global invariants, and connections to information geometry where on-screen dynamics manifest as replicator flows.

</details>


### [171] [LLM on a Budget: Active Knowledge Distillation for Efficient Classification of Large Text Corpora](https://arxiv.org/abs/2511.11574)
*Viviana Luccioli,Rithika Iyengar,Ryan Panley,Flora Haberkorn,Xiaoyu Ge,Leland Crane,Nitish Sinha,Seung Jung Lee*

Main category: cs.LG

TL;DR: 本文提出M - RARU算法结合主动学习降低大语言模型知识蒸馏成本，实验显示能减少样本需求、提高分类准确率和降低成本。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在分类任务中计算和经济成本高，知识蒸馏过程对大数据集也较为昂贵，需降低成本。

Method: 引入M - RARU主动学习算法，结合不确定性与随机接受/拒绝机制选择最具信息的数据点。

Result: 与随机采样相比，样本需求最多降低80%，提高了分类准确率，降低了经济成本和总体训练时间。

Conclusion: M - RARU算法能以较低成本创建高效学生模型，同时保留大语言模型的性能。

Abstract: Large Language Models (LLMs) are highly accurate in classification tasks, however, substantial computational and financial costs hinder their large-scale deployment in dynamic environments. Knowledge Distillation (KD) where a LLM "teacher" trains a smaller and more efficient "student" model, offers a promising solution to this problem. However, the distillation process itself often remains costly for large datasets, since it requires the teacher to label a vast number of samples while incurring significant token consumption. To alleviate this challenge, in this work we explore the active learning (AL) as a way to create efficient student models at a fraction of the cost while preserving the LLM's performance. In particular, we introduce M-RARU (Multi-class Randomized Accept/Reject Uncertainty Sampling), a novel AL algorithm that significantly reduces training costs. M-RARU employs an innovative strategy combining uncertainty with a randomized accept-reject mechanism to select only the most informative data points for the LLM teacher. This focused approach significantly minimizes required API calls and data processing time. We evaluate M-RARU against random sampling across five diverse student models (SVM, LDA, RF, GBDT, and DistilBERT) on multiple benchmark datasets. Experiments demonstrate that our proposed method achieves up to 80% reduction in sample requirements as compared to random sampling, substantially improving classification accuracy while reducing financial costs and overall training time.

</details>


### [172] [Detecting Statistically Significant Fairness Violations in Recidivism Forecasting Algorithms](https://arxiv.org/abs/2511.11575)
*Animesh Joshi*

Main category: cs.LG

TL;DR: 本文引入严格框架测试公平性违规的统计显著性，以累犯预测算法为例进行测试，结果表明机器学习算法在不同公平定义下对不同种族有偏差，强调评估算法决策系统时严格统计测试的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有文献未提供评估群体间差异是否具统计显著性的方法，为填补此空白开展研究。

Method: 利用k折交叉验证生成公平性指标的抽样分布，引入统计测试来识别公平性指标的显著违规情况。

Result: 累犯预测算法在几种公平定义下对黑人个体有显著偏差，在其他定义下对白人无偏差或有偏差。

Conclusion: 评估算法决策系统时严格和稳健的统计测试很重要。

Abstract: Machine learning algorithms are increasingly deployed in critical domains such as finance, healthcare, and criminal justice [1]. The increasing popularity of algorithmic decision-making has stimulated interest in algorithmic fairness within the academic community. Researchers have introduced various fairness definitions that quantify disparities between privileged and protected groups, use causal inference to determine the impact of race on model predictions, and that test calibration of probability predictions from the model. Existing literature does not provide a way in which to assess whether observed disparities between groups are statistically significant or merely due to chance. This paper introduces a rigorous framework for testing the statistical significance of fairness violations by leveraging k-fold cross-validation [2] to generate sampling distributions of fairness metrics. This paper introduces statistical tests that can be used to identify statistically significant violations of fairness metrics based on disparities between predicted and actual outcomes, model calibration, and causal inference techniques [1]. We demonstrate this approach by testing recidivism forecasting algorithms trained on data from the National Institute of Justice. Our findings reveal that machine learning algorithms used for recidivism forecasting exhibit statistically significant bias against Black individuals under several fairness definitions, while also exhibiting no bias or bias against White individuals under other definitions. The results from this paper underscore the importance of rigorous and robust statistical testing while evaluating algorithmic decision-making systems.

</details>


### [173] [EcoSpa: Efficient Transformer Training with Coupled Sparsity](https://arxiv.org/abs/2511.11641)
*Jinqi Xiao,Cheng Luo,Lingyi Huang,Cheng Yang,Yang Sui,Huy Phan,Xiao Zang,Yibiao Ying,Zhexiang Tang,Anima Anandkumar,Bo Yuan*

Main category: cs.LG

TL;DR: 提出EcoSpa方法解决Transformer稀疏训练问题，在多模型上验证有效且无需定制硬件。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer稀疏训练方法在高稀疏度下因未保留权重矩阵结构关系导致性能下降。

Method: 提出EcoSpa方法，联合评估和稀疏化耦合权重矩阵对，通过对齐行列移除保留交互模式，跨预训练和微调场景进行耦合估计和稀疏化。

Result: 在LLaMA - 1B上减少50%内存、加速21%训练，在GPT - 2 - Medium上实现2.2倍模型压缩且困惑度降低2.4，推理加速1.6倍。

Conclusion: EcoSpa方法有效，使用标准PyTorch操作，可在通用硬件上实现高效Transformer训练。

Abstract: Transformers have become the backbone of modern AI, yet their high computational demands pose critical system challenges. While sparse training offers efficiency gains, existing methods fail to preserve critical structural relationships between weight matrices that interact multiplicatively in attention and feed-forward layers. This oversight leads to performance degradation at high sparsity levels. We introduce EcoSpa, an efficient structured sparse training method that jointly evaluates and sparsifies coupled weight matrix pairs, preserving their interaction patterns through aligned row/column removal. EcoSpa introduces a new granularity for calibrating structural component importance and performs coupled estimation and sparsification across both pre-training and fine-tuning scenarios. Evaluations demonstrate substantial improvements: EcoSpa enables efficient training of LLaMA-1B with 50\% memory reduction and 21\% faster training, achieves $2.2\times$ model compression on GPT-2-Medium with $2.4$ lower perplexity, and delivers $1.6\times$ inference speedup. The approach uses standard PyTorch operations, requiring no custom hardware or kernels, making efficient transformer training accessible on commodity hardware.

</details>


### [174] [DAOpt: Modeling and Evaluation of Data-Driven Optimization under Uncertainty with LLMs](https://arxiv.org/abs/2511.11576)
*WenZhuo Zhu,Zheng Cui,Wenhan Lu,Sheng Liu,Yue Zhao*

Main category: cs.LG

TL;DR: 提出DAOpt框架用于在不确定环境下评估大语言模型，结合少样本学习与领域知识增强模型能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在自动化优化建模研究多聚焦确定性优化，不确定环境下应用待探索。

Method: 提出包含新数据集OptU、多智能体决策模块和模拟环境的DAOpt框架，结合少样本学习与随机和鲁棒优化领域知识。

Result: 未提及具体结果。

Conclusion: 未提及明确结论。

Abstract: Recent advances in large language models (LLMs) have accelerated research on automated optimization modeling. While real-world decision-making is inherently uncertain, most existing work has focused on deterministic optimization with known parameters, leaving the application of LLMs in uncertain settings largely unexplored. To that end, we propose the DAOpt framework including a new dataset OptU, a multi-agent decision-making module, and a simulation environment for evaluating LLMs with a focus on out-of-sample feasibility and robustness. Additionally, we enhance LLMs' modeling capabilities by incorporating few-shot learning with domain knowledge from stochastic and robust optimization.

</details>


### [175] [Decoupling Positional and Symbolic Attention Behavior in Transformers](https://arxiv.org/abs/2511.11579)
*Felipe Urrutia,Jorge Salas,Alexander Kozachinskiy,Cristian Buc Calderon,Hector Pasten,Cristobal Rojas*

Main category: cs.LG

TL;DR: 本文深入研究Transformer中Rotary PE（RoPE）注意力头的位置与符号二分行为，定义相关行为，开发度量指标，分析基于Transformer的大语言模型，引入规范任务并证明Transformer性能与注意力头利用频率能力有关。


<details>
  <summary>Details</summary>
Motivation: 深入理解RoPE成功原因，探究注意力头位置与符号信息编码行为。

Method: 理论和实证研究，定义位置和符号行为，开发量化指标，应用框架分析模型，引入规范任务。

Result: 所有注意力头的行为与频率使用有强对应关系，Transformer性能与注意力头利用适当频率的能力有因果关系。

Conclusion: 本文详细解释了RoPE及其属性与模型行为的关系。

Abstract: An important aspect subtending language understanding and production is the ability to independently encode positional and symbolic information of the words within a sentence. In Transformers, positional information is typically encoded using Positional Encodings (PEs). One such popular PE, namely Rotary PE (RoPE), has been widely used due to its empirical success. Recently, it has been argued that part of RoPE's success emerges from its ability to encode robust positional and semantic information using large and small frequencies, respectively. In this work, we perform a deeper dive into the positional versus symbolic dichotomy of attention heads behavior, both at the theoretical and empirical level. We provide general definitions of what it means for a head to behave positionally or symbolically, prove that these are two mutually exclusive behaviors and develop a metric to quantify them. We apply our framework to analyze Transformer-based LLMs using RoPE and find that all heads exhibit a strong correspondence between behavior and frequency use. Finally, we introduce canonical tasks designed to be either purely positional or symbolic, and demonstrate that the Transformer performance causally relates to the ability of attention heads to leverage the appropriate frequencies. In particular, we show that we can control the Transformer performance by controlling which frequencies the attention heads can access. Altogether, our work provides a detailed understanding of RoPE, and how its properties relate to model behavior.

</details>


### [176] [KForge: Program Synthesis for Diverse AI Hardware Accelerators](https://arxiv.org/abs/2511.13274)
*Taras Sereda,Tom St. John,Burak Bartan,Natalie Serrino,Sachin Katti,Zain Asgar*

Main category: cs.LG

TL;DR: 提出平台无关框架KForge优化GPU内核，由两个基于大语言模型的代理协作，有三项关键贡献。


<details>
  <summary>Details</summary>
Motivation: GPU内核对于机器学习性能至关重要，但难以在不同加速器上进行优化。

Method: 构建基于两个协作的大语言模型代理的平台无关框架KForge，生成代理通过编译和正确性反馈生成并迭代优化程序，性能分析代理解释分析数据指导优化。

Result: 展示了迭代优化系统、跨平台知识迁移以及在不同并行计算平台上有效进行程序合成。

Conclusion: 所提出的方法具有平台无关性，能有效进行程序合成以优化GPU内核。

Abstract: GPU kernels are critical for ML performance but difficult to optimize across diverse accelerators. We present KForge, a platform-agnostic framework built on two collaborative LLM-based agents: a generation agent that produces and iteratively refines programs through compilation and correctness feedback, and a performance analysis agent that interprets profiling data to guide optimization. This agent-based architecture requires only a single-shot example to target new platforms.
  We make three key contributions: (1) introducing an iterative refinement system where the generation agent and performance analysis agent collaborate through functional and optimization passes, interpreting diverse profiling data (from programmatic APIs to GUI-based tools) to generate actionable recommendations that guide program synthesis for arbitrary accelerators; (2) demonstrating that the generation agent effectively leverages cross-platform knowledge transfer, where a reference implementation from one architecture substantially improves generation quality for different hardware targets; and (3) validating the platform-agnostic nature of our approach by demonstrating effective program synthesis across fundamentally different parallel computing platforms: NVIDIA CUDA and Apple Metal.

</details>


### [177] [The Anatomy of a Triton Attention Kernel](https://arxiv.org/abs/2511.11581)
*Burkhard Ringlein,Jan van Lunteren,Radu Stoica,Thomas Parnell*

Main category: cs.LG

TL;DR: 本文展示了跨平台高效大语言模型推理的可行性，开发基于Triton语言的分页注意力内核，提升推理性能并实现跨GPU厂商的模型可移植性。


<details>
  <summary>Details</summary>
Motivation: 开发可跨硬件架构、无需底层手动调优且高效的大语言模型推理平台。

Method: 开发基于Triton语言的分页注意力内核，进行算法和系统层面改进，自动调参，集成到推理服务器。

Result: 将通用Triton注意力内核性能从19.7%提升到105.9%。

Conclusion: 可利用开源领域特定语言实现不同GPU厂商间的模型可移植性。

Abstract: A long-standing goal in both industry and academia is to develop an LLM inference platform that is portable across hardware architectures, eliminates the need for low-level hand-tuning, and still delivers best-in-class efficiency. In this work, we demonstrate that portable, efficient cross-platform LLM inference is indeed possible and share our experience. We develop a state-of-the-art paged attention kernel, the core performance-critical component of many LLM deployments, that builds exclusively on the domain-specific just-in-time compiled language Triton to achieve state-of-the-art performance on both NVIDIA and AMD GPUs. We describe our high-level approach, the key algorithmic and system-level improvements, the parameter auto-tuning required to unlock efficiency, and the integrations into a popular inference server that are necessary to bring the performance of a generic Triton attention kernel from 19.7% of the state-of-the-art to 105.9%. Our results highlight how open-source domain-specific languages can be leveraged to unlock model portability across different GPU vendors.

</details>


### [178] [Hardware optimization on Android for inference of AI models](https://arxiv.org/abs/2511.13453)
*Iulius Gherasim,Carlos García Sánchez*

Main category: cs.LG

TL;DR: 研究在安卓系统上为AI模型（目标检测和图像分类）寻找最优执行配置，评估量化方案和设备加速器利用，权衡精度和推理速度。


<details>
  <summary>Details</summary>
Motivation: 优化移动用户体验，解决AI模型在移动计算中低延迟、高响应的执行策略及硬件架构利用问题。

Method: 研究并提出安卓系统上AI模型的最优执行配置，评估不同模型量化方案和设备加速器（GPU和NPU）的利用。

Result: 未提及具体结果。

Conclusion: 未提及具体结论。

Abstract: The pervasive integration of Artificial Intelligence models into contemporary mobile computing is notable across numerous use cases, from virtual assistants to advanced image processing. Optimizing the mobile user experience involves minimal latency and high responsiveness from deployed AI models with challenges from execution strategies that fully leverage real time constraints to the exploitation of heterogeneous hardware architecture. In this paper, we research and propose the optimal execution configurations for AI models on an Android system, focusing on two critical tasks: object detection (YOLO family) and image classification (ResNet). These configurations evaluate various model quantization schemes and the utilization of on device accelerators, specifically the GPU and NPU. Our core objective is to empirically determine the combination that achieves the best trade-off between minimal accuracy degradation and maximal inference speed-up.

</details>


### [179] [RAGPulse: An Open-Source RAG Workload Trace to Optimize RAG Serving Systems](https://arxiv.org/abs/2511.12979)
*Zhengchao Wang,Yitao Hu,Jianing Ye,Zhuxuan Chang,Jiazheng Yu,Youpeng Deng,Keqiu Li*

Main category: cs.LG

TL;DR: 本文介绍开源RAG工作负载跟踪数据集RAGPulse，分析其特性，为RAG系统优化策略提供基础。


<details>
  <summary>Details</summary>
Motivation: 现有通用LLM推理跟踪无法捕捉RAG特定动态，导致学术研究与实际部署存在性能差距。

Method: 从服务超40000名师生的大学问答系统收集数据，详细介绍RAGPulse系统架构和数据格式并进行统计分析。

Result: 发现现实世界RAG工作负载有显著时间局部性和高度偏斜的热门文档访问模式。

Conclusion: RAGPulse为研究人员开发和验证RAG系统优化策略提供高保真基础，可提升RAG服务效率和可靠性。

Abstract: Retrieval-Augmented Generation (RAG) is a critical paradigm for building reliable, knowledge-intensive Large Language Model (LLM) applications. However, the multi-stage pipeline (retrieve, generate) and unique workload characteristics (e.g., knowledge dependency) of RAG systems pose significant challenges for serving performance optimization. Existing generic LLM inference traces fail to capture these RAG-specific dynamics, creating a significant performance gap between academic research and real-world deployment. To bridge this gap, this paper introduces RAGPulse, an open-source RAG workload trace dataset. This dataset was collected from an university-wide Q&A system serving that has served more than 40,000 students and faculties since April 2024. We detail RAGPulse's system architecture, its privacy-preserving hash-based data format, and provide an in-depth statistical analysis. Our analysis reveals that real-world RAG workloads exhibit significant temporal locality and a highly skewed hot document access pattern. RAGPulse provides a high-fidelity foundation for researchers to develop and validate novel optimization strategies for RAG systems, such as content-aware batching and retrieval caching, ultimately enhancing the efficiency and reliability of RAG services. The code is available at https://github.com/flashserve/RAGPulse.

</details>


### [180] [Parallel and Multi-Stage Knowledge Graph Retrieval for Behaviorally Aligned Financial Asset Recommendations](https://arxiv.org/abs/2511.11583)
*Fernando Spadea,Oshani Seneviratne*

Main category: cs.LG

TL;DR: 论文介绍RAG - FLARKO，一种对FLARKO的检索增强扩展，可克服扩展性和相关性挑战，提升金融推荐质量，使小模型在受限环境表现良好。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在个性化金融推荐中受上下文限制、幻觉问题和缺乏行为基础的阻碍，此前的FLARKO存在可扩展性和相关性挑战。

Method: 采用多阶段和并行知识图谱检索流程，先从用户交易知识图谱中检索行为相关实体，再用此上下文从市场知识图谱中过滤时间一致的信号，构建紧凑的子图供大语言模型使用。

Result: 在真实金融交易数据集上的实证评估表明，RAG - FLARKO显著提高了推荐质量。

Conclusion: RAG - FLARKO为在资源受限环境中部署有基础的金融人工智能提供了可行途径。

Abstract: Large language models (LLMs) show promise for personalized financial recommendations but are hampered by context limits, hallucinations, and a lack of behavioral grounding. Our prior work, FLARKO, embedded structured knowledge graphs (KGs) in LLM prompts to align advice with user behavior and market data. This paper introduces RAG-FLARKO, a retrieval-augmented extension to FLARKO, that overcomes scalability and relevance challenges using multi-stage and parallel KG retrieval processes. Our method first retrieves behaviorally relevant entities from a user's transaction KG and then uses this context to filter temporally consistent signals from a market KG, constructing a compact, grounded subgraph for the LLM. This pipeline reduces context overhead and sharpens the model's focus on relevant information. Empirical evaluation on a real-world financial transaction dataset demonstrates that RAG-FLARKO significantly enhances recommendation quality. Notably, our framework enables smaller, more efficient models to achieve high performance in both profitability and behavioral alignment, presenting a viable path for deploying grounded financial AI in resource-constrained environments.

</details>


### [181] [Self-Organization of Attractor Landscapes in High-Capacity Kernel Logistic Regression Hopfield Networks](https://arxiv.org/abs/2511.13053)
*Akira Tamamori*

Main category: cs.LG

TL;DR: 对基于核的Hopfield网络进行几何分析，引入“峰尖锐度”指标，发现“优化脊”现象，揭示自组织机制并为高容量联想记忆设计提供原则。


<details>
  <summary>Details</summary>
Motivation: 理解基于核的学习方法提升Hopfield网络存储容量背后的动力学机制。

Method: 对网络能量景观进行几何分析，引入“峰尖锐度”量化吸引子局部稳定性，改变核宽度和存储负载，理论分解景观梯度。

Result: 发现“优化脊”，此时网络在高负载和全局核条件下使吸引子稳定性最大化，揭示该现象源于两种力的强反相关。

Conclusion: 为高容量联想记忆稳定性提供新物理图景，为其设计提供原则。

Abstract: Kernel-based learning methods can dramatically increase the storage capacity of Hopfield networks, yet the dynamical mechanism behind this enhancement remains poorly understood. We address this gap by conducting a geometric analysis of the network's energy landscape. We introduce a novel metric, ``Pinnacle Sharpness,'' to quantify the local stability of attractors. By systematically varying the kernel width and storage load, we uncover a rich phase diagram of attractor shapes. Our central finding is the emergence of a ``ridge of optimization,'' where the network maximizes attractor stability under challenging high-load and global-kernel conditions. Through a theoretical decomposition of the landscape gradient into a direct ``driving'' force and an indirect ``feedback'' force, we reveal the origin of this phenomenon. The optimization ridge corresponds to a regime of strong anti-correlation between the two forces, where the direct force, amplified by the high storage load, dominates the opposing collective feedback force. This demonstrates a sophisticated self-organization mechanism: the network adaptively harnesses inter-pattern interactions as a cooperative feedback control system to sculpt a robust energy landscape. Our findings provide a new physical picture for the stability of high-capacity associative memories and offer principles for their design.

</details>


### [182] [Output Supervision Can Obfuscate the Chain of Thought](https://arxiv.org/abs/2511.11584)
*Jacob Drori,Luke Marks,Bryce Woodworth,Alex Cloud,Alexander Matt Turner*

Main category: cs.LG

TL;DR: OpenAI提出仅针对无CoT访问权限的输出监视器训练可保持CoT可监控性，本文指出此方法仍会产生混淆CoT并给出两种机制，还引入缓解措施实现帕累托改进。


<details>
  <summary>Details</summary>
Motivation: 指出OpenAI提出的训练方法仍可能导致混淆CoTs的问题。

Method: 分析训练产生混淆CoTs的两种机制，并引入两种缓解措施。

Result: 所引入的缓解措施在可监控性和任务性能方面相比常规训练实现了帕累托改进。

Conclusion: 仅针对输出监视器训练仍会出现混淆CoTs问题，所提出的缓解措施有效。

Abstract: OpenAI (2025) showed that training against a chain of thought (CoT) monitor can cause obfuscated CoTs, which contain bad behavior the monitor cannot detect. They proposed to keep CoTs monitorable by training only against output monitors that do not have access to CoT. We show that such training can still cause obfuscated CoTs via two mechanisms. First, when a model is trained to produce a safe-looking output, that model may generalize to making its CoTs look safe. Second, since later tokens are conditioned on earlier ones, safe-looking CoTs may increase the likelihood of safe outputs, causing safe-looking CoTs to be reinforced. We introduce two mitigations to address these two issues, which achieve a Pareto improvement in terms of monitorability and task performance compared to regular training.

</details>


### [183] [Predicting Grain Growth in Polycrystalline Materials Using Deep Learning Time Series Models](https://arxiv.org/abs/2511.11630)
*Eliane Younes,Elie Hachem,Marc Bernacki*

Main category: cs.LG

TL;DR: 研究评估多种深度学习方法预测晶粒生长中晶粒尺寸分布，LSTM网络表现最佳，凸显低维描述符和LSTM预测微观结构的潜力。


<details>
  <summary>Details</summary>
Motivation: 晶粒生长对材料力学行为影响大，预测其晶粒尺寸分布是微观结构工程关键目标。

Method: 评估RNN、LSTM、TCN和transformers等深度学习方法，利用高保真模拟的平均场统计描述符，处理120个晶粒生长序列数据集，采用递归预测策略训练模型。

Result: LSTM网络准确率超90%，性能最稳定，减少计算时间，其他架构远期预测易发散。

Conclusion: 低维描述符和基于LSTM的预测对高效准确的微观结构预测有潜力，可用于数字孪生开发和工艺优化。

Abstract: Grain Growth strongly influences the mechanical behavior of materials, making its prediction a key objective in microstructural engineering. In this study, several deep learning approaches were evaluated, including recurrent neural networks (RNN), long short-term memory (LSTM), temporal convolutional networks (TCN), and transformers, to forecast grain size distributions during grain growth. Unlike full-field simulations, which are computationally demanding, the present work relies on mean-field statistical descriptors extracted from high-fidelity simulations. A dataset of 120 grain growth sequences was processed into normalized grain size distributions as a function of time. The models were trained to predict future distributions from a short temporal history using a recursive forecasting strategy. Among the tested models, the LSTM network achieved the highest accuracy (above 90\%) and the most stable performance, maintaining physically consistent predictions over extended horizons while reducing computation time from about 20 minutes per sequence to only a few seconds, whereas the other architectures tended to diverge when forecasting further in time. These results highlight the potential of low-dimensional descriptors and LSTM-based forecasting for efficient and accurate microstructure prediction, with direct implications for digital twin development and process optimization.

</details>


### [184] [Parameter-Efficient and Personalized Federated Training of Generative Models at the Edge](https://arxiv.org/abs/2511.11585)
*Kabir Khan,Manju Sarkar,Anita Kar,Suresh Ghosh*

Main category: cs.LG

TL;DR: 提出FedGen - Edge框架解决大生成模型在跨设备联邦设置中训练难题，在语言和图像任务表现优，提供实用路径。


<details>
  <summary>Details</summary>
Motivation: 大生成模型在跨设备联邦设置中因计算、通信负担和异质性问题难以训练或适配。

Method: 提出FedGen - Edge框架，解耦预训练全局骨干和轻量级客户端适配器，仅联合适配器，用LoRA约束客户端更新到紧凑子空间。

Result: 在语言建模和图像生成任务中比基线有更低困惑度/FID和更快收敛速度，消融实验显示适度LoRA秩外收益递减及本地轮次与客户端漂移的权衡。

Conclusion: FedGen - Edge为异构边缘设备上隐私保护、资源感知和个性化生成AI提供实用路径。

Abstract: Large generative models (for example, language and diffusion models) enable high-quality text and image synthesis but are hard to train or adapt in cross-device federated settings due to heavy computation and communication and statistical/system heterogeneity. We propose FedGen-Edge, a framework that decouples a frozen, pre-trained global backbone from lightweight client-side adapters and federates only the adapters. Using Low-Rank Adaptation (LoRA) constrains client updates to a compact subspace, which reduces uplink traffic by more than 99 percent versus full-model FedAvg, stabilizes aggregation under non-IID data, and naturally supports personalization because each client can keep a locally tuned adapter. On language modeling (PTB) and image generation (CIFAR-10), FedGen-Edge achieves lower perplexity/FID and faster convergence than strong baselines while retaining a simple FedAvg-style server. A brief ablation shows diminishing returns beyond moderate LoRA rank and a trade-off between local epochs and client drift. FedGen-Edge offers a practical path toward privacy-preserving, resource-aware, and personalized generative AI on heterogeneous edge devices.

</details>


### [185] [DK-Root: A Joint Data-and-Knowledge-Driven Framework for Root Cause Analysis of QoE Degradations in Mobile Networks](https://arxiv.org/abs/2511.11737)
*Qizhe Li,Haolong Chen,Jiansheng Li,Shuqi Chai,Xuan Li,Yuzhou Hou,Xinhua Shao,Fangfang Li,Kaifeng Han,Guangxu Zhu*

Main category: cs.LG

TL;DR: 提出DK - Root框架，结合数据与知识驱动进行移动网络QoE退化根因分析，实验显示其精度达最优。


<details>
  <summary>Details</summary>
Motivation: 由于移动网络内核性能指标复杂交互及可靠专家标注稀缺，传统基于规则启发式方法有局限性，需新方法进行根因分析。

Method: 先通过对比表示学习用大量基于规则的标签预训练编码器并去噪；引入类条件扩散模型进行数据增强；最后用少量专家验证标签联合微调编码器和分类器。

Result: 在真实运营商级数据集上实验显示，DK - Root超越传统ML和半监督时间序列方法，消融实验验证了条件扩散增强和预训练 - 微调设计的必要性。

Conclusion: DK - Root框架在移动网络QoE退化根因分析中有效且准确。

Abstract: Diagnosing the root causes of Quality of Experience (QoE) degradations in operational mobile networks is challenging due to complex cross-layer interactions among kernel performance indicators (KPIs) and the scarcity of reliable expert annotations. Although rule-based heuristics can generate labels at scale, they are noisy and coarse-grained, limiting the accuracy of purely data-driven approaches. To address this, we propose DK-Root, a joint data-and-knowledge-driven framework that unifies scalable weak supervision with precise expert guidance for robust root-cause analysis. DK-Root first pretrains an encoder via contrastive representation learning using abundant rule-based labels while explicitly denoising their noise through a supervised contrastive objective. To supply task-faithful data augmentation, we introduce a class-conditional diffusion model that generates KPIs sequences preserving root-cause semantics, and by controlling reverse diffusion steps, it produces weak and strong augmentations that improve intra-class compactness and inter-class separability. Finally, the encoder and the lightweight classifier are jointly fine-tuned with scarce expert-verified labels to sharpen decision boundaries. Extensive experiments on a real-world, operator-grade dataset demonstrate state-of-the-art accuracy, with DK-Root surpassing traditional ML and recent semi-supervised time-series methods. Ablations confirm the necessity of the conditional diffusion augmentation and the pretrain-finetune design, validating both representation quality and classification gains.

</details>


### [186] [Aspiration-based Perturbed Learning Automata in Games with Noisy Utility Measurements. Part A: Stochastic Stability in Non-zero-Sum Games](https://arxiv.org/abs/2511.11602)
*Georgios C. Chasparis*

Main category: cs.LG

TL;DR: 本文针对强化学习在分布式场景的局限，提出基于期望的扰动学习自动机（APLA）方案，分析其在多人正效用游戏中的随机稳定性。


<details>
  <summary>Details</summary>
Motivation: 强化学习在分布式场景有局限，先前工作仅关注小类游戏，本文旨在解决该局限。

Method: 引入APLA学习方案，玩家选择动作的概率分布受重复选择和期望因素强化；对APLA在多人正效用游戏中进行随机稳定性分析。

Result: 在有噪声观测下对APLA在多人正效用游戏中进行随机稳定性分析，建立诱导无限维马尔可夫链与有限维的等价性；将随机稳定性进一步应用到弱无环游戏。

Conclusion: 提出的APLA学习方案为分布式优化提供新思路，拓展了强化学习在非零和游戏中的应用。

Abstract: Reinforcement-based learning has attracted considerable attention both in modeling human behavior as well as in engineering, for designing measurement- or payoff-based optimization schemes. Such learning schemes exhibit several advantages, especially in relation to filtering out noisy observations. However, they may exhibit several limitations when applied in a distributed setup. In multi-player weakly-acyclic games, and when each player applies an independent copy of the learning dynamics, convergence to (usually desirable) pure Nash equilibria cannot be guaranteed. Prior work has only focused on a small class of games, namely potential and coordination games. To address this main limitation, this paper introduces a novel payoff-based learning scheme for distributed optimization, namely aspiration-based perturbed learning automata (APLA). In this class of dynamics, and contrary to standard reinforcement-based learning schemes, each player's probability distribution for selecting actions is reinforced both by repeated selection and an aspiration factor that captures the player's satisfaction level. We provide a stochastic stability analysis of APLA in multi-player positive-utility games under the presence of noisy observations. This is the first part of the paper that characterizes stochastic stability in generic non-zero-sum games by establishing equivalence of the induced infinite-dimensional Markov chain with a finite dimensional one. In the second part, stochastic stability is further specialized to weakly acyclic games.

</details>


### [187] [WildfireGenome: Interpretable Machine Learning Reveals Local Drivers of Wildfire Risk and Their Cross-County Variation](https://arxiv.org/abs/2511.11589)
*Chenyue Liu,Ali Mostafavi*

Main category: cs.LG

TL;DR: WildfireGenome通过融合指标、随机森林分类和分析驱动关系改进野火风险评估，在部分地区有较好表现，推动评估迈向可解释决策尺度。


<details>
  <summary>Details</summary>
Motivation: 当前野火风险评估依赖粗糙的危险地图和不透明机器学习模型，牺牲决策尺度的可解释性，WildfireGenome旨在解决这些问题。

Method: 将七个联邦野火指标融合为基于PCA的复合风险标签；用随机森林对当地野火风险分类；用SHAP和ICE/PDP分析县级特定的非线性驱动关系。

Result: 在七个美国县模型准确率0.755 - 0.878，Quadratic Weighted Kappa达0.951，主成分解释87 - 94%指标方差；相似生态区转移测试表现可靠，不同生态区表现不佳；针叶林覆盖和海拔是主要驱动因素。

Conclusion: WildfireGenome推动野火风险评估从区域预测转向可解释的决策尺度分析，可指导植被管理、分区和基础设施规划。

Abstract: Current wildfire risk assessments rely on coarse hazard maps and opaque machine learning models that optimize regional accuracy while sacrificing interpretability at the decision scale. WildfireGenome addresses these gaps through three components: (1) fusion of seven federal wildfire indicators into a sign-aligned, PCA-based composite risk label at H3 Level-8 resolution; (2) Random Forest classification of local wildfire risk; and (3) SHAP and ICE/PDP analyses to expose county-specific nonlinear driver relationships. Across seven ecologically diverse U.S. counties, models achieve accuracies of 0.755-0.878 and Quadratic Weighted Kappa up to 0.951, with principal components explaining 87-94% of indicator variance. Transfer tests show reliable performance between ecologically similar regions but collapse across dissimilar contexts. Explanations consistently highlight needleleaf forest cover and elevation as dominant drivers, with risk rising sharply at 30-40% needleleaf coverage. WildfireGenome advances wildfire risk assessment from regional prediction to interpretable, decision-scale analytics that guide vegetation management, zoning, and infrastructure planning.

</details>


### [188] [BlinDNO: A Distributional Neural Operator for Dynamical System Reconstruction from Time-Label-Free data](https://arxiv.org/abs/2511.12316)
*Zhijun Zeng,Junqing Chen,Zuoqiang Shi*

Main category: cs.LG

TL;DR: 研究无时间标签下随机和量子动力系统逆问题，提出BlinDNO架构，实验表明其能可靠恢复参数且优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 在无时间标签、仅有无序密度快照的情况下，从状态密度分布中恢复底层演化算子的参数。

Method: 将问题建模为分布到函数的神经算子学习，提出集成多尺度U - Net编码器和基于注意力的混合器的排列不变架构BlinDNO。

Result: 在多种随机和量子系统的数值实验中，BlinDNO能可靠恢复控制参数，且始终优于现有的神经逆算子基线。

Conclusion: BlinDNO在解决无时间标签的随机和量子动力系统逆问题上表现良好，是一种有效的方法。

Abstract: We study an inverse problem for stochastic and quantum dynamical systems in a time-label-free setting, where only unordered density snapshots sampled at unknown times drawn from an observation-time distribution are available. These observations induce a distribution over state densities, from which we seek to recover the parameters of the underlying evolution operator. We formulate this as learning a distribution-to-function neural operator and propose BlinDNO, a permutation-invariant architecture that integrates a multiscale U-Net encoder with an attention-based mixer. Numerical experiments on a wide range of stochastic and quantum systems, including a 3D protein-folding mechanism reconstruction problem in a cryo-EM setting, demonstrate that BlinDNO reliably recovers governing parameters and consistently outperforms existing neural inverse operator baselines.

</details>


### [189] [On the Trade-Off Between Transparency and Security in Adversarial Machine Learning](https://arxiv.org/abs/2511.11842)
*Lucas Fenaux,Christopher Srinivasa,Florian Kerschbaum*

Main category: cs.LG

TL;DR: 研究对抗环境下AI透明度与安全性的冲突，通过可迁移对抗样本攻击评估，用博弈论分析，发现透明度与安全性存在权衡。


<details>
  <summary>Details</summary>
Motivation: 探究在对抗环境中AI透明度对智能体的战略影响，以及透明度与安全性的冲突。

Method: 对181个模型进行9种攻击的大规模实证评估，用博弈论将问题建模为纳什博弈和斯塔克尔伯格博弈并比较结果。

Result: 攻击者与防御者决策匹配时更易成功，仅知道防御者模型是否受保护有时就会损害其安全性。

Conclusion: AI系统的透明度与安全性存在冲突，博弈论推理可揭示这种冲突。

Abstract: Transparency and security are both central to Responsible AI, but they may conflict in adversarial settings. We investigate the strategic effect of transparency for agents through the lens of transferable adversarial example attacks. In transferable adversarial example attacks, attackers maliciously perturb their inputs using surrogate models to fool a defender's target model. These models can be defended or undefended, with both players having to decide which to use. Using a large-scale empirical evaluation of nine attacks across 181 models, we find that attackers are more successful when they match the defender's decision; hence, obscurity could be beneficial to the defender. With game theory, we analyze this trade-off between transparency and security by modeling this problem as both a Nash game and a Stackelberg game, and comparing the expected outcomes. Our analysis confirms that only knowing whether a defender's model is defended or not can sometimes be enough to damage its security. This result serves as an indicator of the general trade-off between transparency and security, suggesting that transparency in AI systems can be at odds with security. Beyond adversarial machine learning, our work illustrates how game-theoretic reasoning can uncover conflicts between transparency and security.

</details>


### [190] [Mind Your Entropy: From Maximum Entropy to Trajectory Entropy-Constrained RL](https://arxiv.org/abs/2511.11592)
*Guojian Zhan,Likun Wang,Pengcheng Wang,Feihong Zhang,Jingliang Duan,Masayoshi Tomizuka,Shengbo Eben Li*

Main category: cs.LG

TL;DR: 论文提出TECRL框架解决最大熵强化学习瓶颈，开发DSAC - E算法，在OpenAI Gym基准测试中表现更好。


<details>
  <summary>Details</summary>
Motivation: 最大熵强化学习存在非平稳Q值估计和短视局部熵调整两个瓶颈，限制性能提升。

Method: 提出TECRL框架，分别学习两个Q函数，利用熵Q函数实施轨迹熵约束；基于此框架扩展DSAC - T开发DSAC - E算法。

Result: 在OpenAI Gym基准测试中，DSAC - E能获得更高回报和更好稳定性。

Conclusion: TECRL框架和DSAC - E算法有效解决最大熵强化学习瓶颈，提升性能。

Abstract: Maximum entropy has become a mainstream off-policy reinforcement learning (RL) framework for balancing exploitation and exploration. However, two bottlenecks still limit further performance improvement: (1) non-stationary Q-value estimation caused by jointly injecting entropy and updating its weighting parameter, i.e., temperature; and (2) short-sighted local entropy tuning that adjusts temperature only according to the current single-step entropy, without considering the effect of cumulative entropy over time. In this paper, we extends maximum entropy framework by proposing a trajectory entropy-constrained reinforcement learning (TECRL) framework to address these two challenges. Within this framework, we first separately learn two Q-functions, one associated with reward and the other with entropy, ensuring clean and stable value targets unaffected by temperature updates. Then, the dedicated entropy Q-function, explicitly quantifying the expected cumulative entropy, enables us to enforce a trajectory entropy constraint and consequently control the policy long-term stochasticity. Building on this TECRL framework, we develop a practical off-policy algorithm, DSAC-E, by extending the state-of-the-art distributional soft actor-critic with three refinements (DSAC-T). Empirical results on the OpenAI Gym benchmark demonstrate that our DSAC-E can achieve higher returns and better stability.

</details>


### [191] [MACKO: Sparse Matrix-Vector Multiplication for Low Sparsity](https://arxiv.org/abs/2511.13061)
*Vladimír Macko,Vladimír Boža*

Main category: cs.LG

TL;DR: 提出MACKO - SpMV优化稀疏矩阵向量乘法，在50%稀疏度下效果显著，使非结构化剪枝在实际大语言模型工作负载中更具合理性。


<details>
  <summary>Details</summary>
Motivation: 现有SpMV方法在剪枝大语言模型常见的低非结构化稀疏度（30 - 90%）下性能差，非结构化剪枝内存减少和加速效果有限。

Method: 提出GPU优化的格式和内核MACKO - SpMV，减少存储开销并保持与GPU执行模型的兼容性，无需专用硬件单元或特定格式预计算。

Result: 在50%稀疏度下，MACKO相比密集表示有1.5倍内存减少和1.2 - 1.5倍加速；比其他SpMV基线有显著加速；应用于Llama2 - 7B时，在fp16精度下有1.5倍内存减少和1.5倍推理加速。

Conclusion: MACKO使50%稀疏度的非结构化剪枝在实际大语言模型工作负载中具有合理性。

Abstract: Sparse Matrix-Vector Multiplication (SpMV) is a fundamental operation in the inference of sparse Large Language Models (LLMs). Because existing SpMV methods perform poorly under the low and unstructured sparsity (30-90%) commonly observed in pruned LLMs, unstructured pruning provided only limited memory reduction and speedup. We propose MACKO-SpMV, a GPU-optimized format and kernel co-designed to reduce storage overhead while preserving compatibility with the GPU's execution model. This enables efficient SpMV for unstructured sparsity without specialized hardware units (e.g., tensor cores) or format-specific precomputation. Empirical results show that at sparsity 50%, MACKO is the first approach with significant 1.5x memory reduction and 1.2-1.5x speedup over dense representation. Speedups over other SpMV baselines: 2.8-13.0x over cuSPARSE, 1.9-2.6x over Sputnik, and 2.2-2.5x over DASP. Applied to Llama2-7B pruned with Wanda to sparsity 50%, it delivers 1.5x memory reduction and 1.5x faster inference at fp16 precision. Thanks to MACKO, unstructured pruning at 50% sparsity is now justified in real-world LLM workloads.

</details>


### [192] [Sound Logical Explanations for Mean Aggregation Graph Neural Networks](https://arxiv.org/abs/2511.11593)
*Matthew Morris,Ian Horrocks*

Main category: cs.LG

TL;DR: 研究带均值聚合和非负权重的图神经网络（MAGNNs），证明其适用的单调规则类，提供解释预测的一阶逻辑片段，实验表明限制非负权重有良好表现。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络使用均值聚合函数时，缺乏可解释性和表达能力的相关研究。

Method: 考虑MAGNNs，证明其适用的单调规则类，提供一阶逻辑的受限片段来解释预测。

Result: 限制均值聚合GNNs为非负权重在标准归纳基准上有可比或更好性能，能获得合理规则、生成有洞察力的解释，且合理规则可暴露训练模型的问题。

Conclusion: 对MAGNNs的研究在可解释性和性能方面有积极成果，合理规则对模型分析有帮助。

Abstract: Graph neural networks (GNNs) are frequently used for knowledge graph completion. Their black-box nature has motivated work that uses sound logical rules to explain predictions and characterise their expressivity. However, despite the prevalence of GNNs that use mean as an aggregation function, explainability and expressivity results are lacking for them. We consider GNNs with mean aggregation and non-negative weights (MAGNNs), proving the precise class of monotonic rules that can be sound for them, as well as providing a restricted fragment of first-order logic to explain any MAGNN prediction. Our experiments show that restricting mean-aggregation GNNs to have non-negative weights yields comparable or improved performance on standard inductive benchmarks, that sound rules are obtained in practice, that insightful explanations can be generated in practice, and that the sound rules can expose issues in the trained models.

</details>


### [193] [MolEdit: Knowledge Editing for Multimodal Molecule Language Models](https://arxiv.org/abs/2511.12770)
*Zhenyu Lei,Patrick Soga,Yaochen Zhu,Yinhan He,Yushun Dong,Jundong Li*

Main category: cs.LG

TL;DR: 本文提出MoLM编辑框架MolEdit及评估基准MEBench，实验显示其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有分子语言模型（MoLMs）存在编码传播不准确问题，知识编辑应用于MoLMs尚待探索，有独特挑战。

Method: 提出MolEdit框架，结合多专家知识适配器和专业感知编辑切换器；引入MEBench基准评估编辑性能。

Result: 在两个流行MoLM骨干上的实验中，MolEdit的可靠性比基线高18.8%，局部性高12.0%，且保持高效。

Conclusion: MolEdit能有效实现MoLM编辑，在相关任务中表现良好，代码开源。

Abstract: Understanding and continuously refining multimodal molecular knowledge is crucial for advancing biomedicine, chemistry, and materials science. Molecule language models (MoLMs) have become powerful tools in these domains, integrating structural representations (e.g., SMILES strings, molecular graphs) with rich contextual descriptions (e.g., physicochemical properties). However, MoLMs can encode and propagate inaccuracies due to outdated web-mined training corpora or malicious manipulation, jeopardizing downstream discovery pipelines. While knowledge editing has been explored for general-domain AI, its application to MoLMs remains uncharted, presenting unique challenges due to the multifaceted and interdependent nature of molecular knowledge. In this paper, we take the first step toward MoLM editing for two critical tasks: molecule-to-caption generation and caption-to-molecule generation. To address molecule-specific challenges, we propose MolEdit, a powerful framework that enables targeted modifications while preserving unrelated molecular knowledge. MolEdit combines a Multi-Expert Knowledge Adapter that routes edits to specialized experts for different molecular facets with an Expertise-Aware Editing Switcher that activates the adapters only when input closely matches the stored edits across all expertise, minimizing interference with unrelated knowledge. To systematically evaluate editing performance, we introduce MEBench, a comprehensive benchmark assessing multiple dimensions, including Reliability (accuracy of the editing), Locality (preservation of irrelevant knowledge), and Generality (robustness to reformed queries). Across extensive experiments on two popular MoLM backbones, MolEdit delivers up to 18.8% higher Reliability and 12.0% better Locality than baselines while maintaining efficiency. The code is available at: https://github.com/LzyFischer/MolEdit.

</details>


### [194] [Loss Given Default Prediction Under Measurement-Induced Mixture Distributions: An Information-Theoretic Approach](https://arxiv.org/abs/2511.11596)
*Javier Marín*

Main category: cs.LG

TL;DR: 研究指出LGD建模受数据质量约束，递归分区方法失效，信息论方法表现更好，给出金融机构指导并推广到其他领域。


<details>
  <summary>Details</summary>
Motivation: 解决LGD建模中因数据质量问题导致的方法失效问题。

Method: 对比递归分区方法和基于香农熵与互信息的信息论方法。

Result: 递归分区方法在测试数据表现差，信息论方法r - squared为0.191，RMSE为0.284，杠杆特征互信息高。

Conclusion: 为金融机构部署LGD模型提供实践指导，研究结果可推广到其他领域。

Abstract: Loss Given Default (LGD) modeling faces a fundamental data quality constraint: 90% of available training data consists of proxy estimates based on pre-distress balance sheets rather than actual recovery outcomes from completed bankruptcy proceedings. We demonstrate that this mixture-contaminated training structure causes systematic failure of recursive partitioning methods, with Random Forest achieving negative r-squared (-0.664, worse than predicting the mean) on held-out test data. Information-theoretic approaches based on Shannon entropy and mutual information provide superior generalization, achieving r-squared of 0.191 and RMSE of 0.284 on 1,218 corporate bankruptcies (1980-2023). Analysis reveals that leverage-based features contain 1.510 bits of mutual information while size effects contribute only 0.086 bits, contradicting regulatory assumptions about scale-dependent recovery. These results establish practical guidance for financial institutions deploying LGD models under Basel III requirements when representative outcome data is unavailable at sufficient scale. The findings generalize to medical outcomes research, climate forecasting, and technology reliability-domains where extended observation periods create unavoidable mixture structure in training data.

</details>


### [195] [Efficient Calibration for Decision Making](https://arxiv.org/abs/2511.13699)
*Parikshit Gopalan,Konstantinos Stavropoulos,Kunal Talwar,Pranay Tankala*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: A decision-theoretic characterization of perfect calibration is that an agent seeking to minimize a proper loss in expectation cannot improve their outcome by post-processing a perfectly calibrated predictor. Hu and Wu (FOCS'24) use this to define an approximate calibration measure called calibration decision loss ($\mathsf{CDL}$), which measures the maximal improvement achievable by any post-processing over any proper loss. Unfortunately, $\mathsf{CDL}$ turns out to be intractable to even weakly approximate in the offline setting, given black-box access to the predictions and labels.
  We suggest circumventing this by restricting attention to structured families of post-processing functions $K$. We define the calibration decision loss relative to $K$, denoted $\mathsf{CDL}_K$ where we consider all proper losses but restrict post-processings to a structured family $K$. We develop a comprehensive theory of when $\mathsf{CDL}_K$ is information-theoretically and computationally tractable, and use it to prove both upper and lower bounds for natural classes $K$. In addition to introducing new definitions and algorithmic techniques to the theory of calibration for decision making, our results give rigorous guarantees for some widely used recalibration procedures in machine learning.

</details>


### [196] [Enhancing failure prediction in nuclear industry: Hybridization of knowledge- and data-driven techniques](https://arxiv.org/abs/2511.11604)
*Amaratou Mahamadou Saley,Thierry Moyaux,Aïcha Sekhari,Vincent Cheutet,Jean-Baptiste Danielou*

Main category: cs.LG

TL;DR: 本文提出结合数据驱动技术与核设备领域知识的预测性维护方法，通过案例研究表明该方法在故障预测上优于纯数据驱动方法。


<details>
  <summary>Details</summary>
Motivation: 物联网与工业4.0融合推动核工业数据驱动方法发展，但精确预测资产维护需求面临挑战，且核领域数据驱动方法需大量领域知识。

Method: 提出结合数据驱动技术与核设备领域知识的新型预测性维护方法。

Result: 案例研究显示，纯数据驱动方法预测范围3小时、F1分数56.36%，混合方法预测范围达24小时、F1分数93.12%。

Conclusion: 所提方法在故障预测上显著优于纯数据驱动方法。

Abstract: The convergence of the Internet of Things (IoT) and Industry 4.0 has significantly enhanced data-driven methodologies within the nuclear industry, notably enhancing safety and economic efficiency. This advancement challenges the precise prediction of future maintenance needs for assets, which is crucial for reducing downtime and operational costs. However, the effectiveness of data-driven methodologies in the nuclear sector requires extensive domain knowledge due to the complexity of the systems involved. Thus, this paper proposes a novel predictive maintenance methodology that combines data-driven techniques with domain knowledge from a nuclear equipment. The methodological originality of this paper is located on two levels: highlighting the limitations of purely data-driven approaches and demonstrating the importance of knowledge in enhancing the performance of the predictive models. The applicative novelty of this work lies in its use within a domain such as a nuclear industry, which is highly restricted and ultrasensitive due to security, economic and environmental concerns. A detailed real-world case study which compares the current state of equipment monitoring with two scenarios, demonstrate that the methodology significantly outperforms purely data-driven methods in failure prediction. While purely data-driven methods achieve only a modest performance with a prediction horizon limited to 3 h and a F1 score of 56.36%, the hybrid approach increases the prediction horizon to 24 h and achieves a higher F1 score of 93.12%.

</details>


### [197] [Clustering-Based Weight Orthogonalization for Stabilizing Deep Reinforcement Learning](https://arxiv.org/abs/2511.11607)
*Guoqing Ma,Yuhan Zhang,Yuming Dai,Guangfu Hao,Yang Chen,Shan Yu*

Main category: cs.LG

TL;DR: 提出COWM层解决强化学习中环境非平稳性问题，提升学习效率，在基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 强化学习中环境非平稳性导致学习效率低，需大量迭代和低样本效率。

Method: 引入可集成到任何强化学习算法策略网络的COWM层，利用聚类技术和投影矩阵稳定学习过程。

Result: COWM在视觉和基于状态的DMControl基准测试中分别提升9%和12.6%，在多种算法和任务中表现出鲁棒性和通用性。

Conclusion: COWM层能有效缓解非平稳性，提高学习效率，优于现有方法。

Abstract: Reinforcement learning (RL) has made significant advancements, achieving superhuman performance in various tasks. However, RL agents often operate under the assumption of environmental stationarity, which poses a great challenge to learning efficiency since many environments are inherently non-stationary. This non-stationarity results in the requirement of millions of iterations, leading to low sample efficiency. To address this issue, we introduce the Clustering Orthogonal Weight Modified (COWM) layer, which can be integrated into the policy network of any RL algorithm and mitigate non-stationarity effectively. The COWM layer stabilizes the learning process by employing clustering techniques and a projection matrix. Our approach not only improves learning speed but also reduces gradient interference, thereby enhancing the overall learning efficiency. Empirically, the COWM outperforms state-of-the-art methods and achieves improvements of 9% and 12.6% in vision based and state-based DMControl benchmark. It also shows robustness and generality across various algorithms and tasks.

</details>


### [198] [Small Vocabularies, Big Gains: Pretraining and Tokenization in Time Series Models](https://arxiv.org/abs/2511.11622)
*Alexis Roger,Gwen Legate,Kashif Rasul,Yuriy Nevmyvaka,Irina Rish*

Main category: cs.LG

TL;DR: 研究分词器设计和预训练对时间序列预测模型性能的影响，强调分词重要性并给出设计指导。


<details>
  <summary>Details</summary>
Motivation: 系统研究分词器设计和预训练对时间序列基础模型性能的影响。

Method: 结合实证训练实验和理论分析。

Result: 预训练模型能更有效地利用设计良好的分词器，错误分词会削弱预训练优势。

Conclusion: 强调时间序列建模中仔细分词的重要性，提出结合小而高效词汇表和预训练权重在多模态预测中有益，并为分词器设计和迁移学习提供指导。

Abstract: Tokenization and transfer learning are two critical components in building state of the art time series foundation models for forecasting. In this work, we systematically study the effect of tokenizer design, specifically scaling and quantization strategies, on model performance, alongside the impact of pretraining versus random initialization. We show that tokenizer configuration primarily governs the representational capacity and stability of the model, while transfer learning influences optimization efficiency and alignment. Using a combination of empirical training experiments and theoretical analyses, we demonstrate that pretrained models consistently leverage well-designed tokenizers more effectively, particularly at smaller vocabulary sizes. Conversely, misaligned tokenization can diminish or even invert the benefits of pretraining. These findings highlight the importance of careful tokenization in time series modeling and suggest that combining small, efficient vocabularies with pretrained weights is especially advantageous in multi-modal forecasting settings, where the overall vocabulary must be shared across modalities. Our results provide concrete guidance for designing tokenizers and leveraging transfer learning in discrete representation learning for continuous signals.

</details>


### [199] [Early GVHD Prediction in Liver Transplantation via Multi-Modal Deep Learning on Imbalanced EHR Data](https://arxiv.org/abs/2511.11623)
*Yushan Jiang,Shuteng Niu,Dongjin Song,Yichen Wang,Jingna Feng,Xinyue Hu,Liu Yang,Cui Tao*

Main category: cs.LG

TL;DR: 利用多模态深度学习方法整合电子健康记录，用于肝移植中移植物抗宿主病（GVHD）的早期预测，框架表现优于基线，提升了预测效果。


<details>
  <summary>Details</summary>
Motivation: GVHD是肝移植中罕见但致命的并发症，死亡率高，需提前预测以进行及时干预和改善患者预后。

Method: 分析2100例肝移植患者术前电子健康记录，涵盖四种主要模态；开发多模态深度学习框架，动态融合模态、处理缺失值和解决类别不平衡问题。

Result: 框架优于单模态和多模态机器学习基线，AUC为0.836，AUPRC为0.157，召回率为0.768，特异性为0.803。

Conclusion: 多模态深度学习框架显著改进了现有GVHD早期预测方法，能有效应对真实世界电子健康记录的异质性和极端类别不平衡问题，实现准确的早期预测。

Abstract: Graft-versus-host disease (GVHD) is a rare but often fatal complication in liver transplantation, with a very high mortality rate. By harnessing multi-modal deep learning methods to integrate heterogeneous and imbalanced electronic health records (EHR), we aim to advance early prediction of GVHD, paving the way for timely intervention and improved patient outcomes. In this study, we analyzed pre-transplant electronic health records (EHR) spanning the period before surgery for 2,100 liver transplantation patients, including 42 cases of graft-versus-host disease (GVHD), from a cohort treated at Mayo Clinic between 1992 and 2025. The dataset comprised four major modalities: patient demographics, laboratory tests, diagnoses, and medications. We developed a multi-modal deep learning framework that dynamically fuses these modalities, handles irregular records with missing values, and addresses extreme class imbalance through AUC-based optimization. The developed framework outperforms all single-modal and multi-modal machine learning baselines, achieving an AUC of 0.836, an AUPRC of 0.157, a recall of 0.768, and a specificity of 0.803. It also demonstrates the effectiveness of our approach in capturing complementary information from different modalities, leading to improved performance. Our multi-modal deep learning framework substantially improves existing approaches for early GVHD prediction. By effectively addressing the challenges of heterogeneity and extreme class imbalance in real-world EHR, it achieves accurate early prediction. Our proposed multi-modal deep learning method demonstrates promising results for early prediction of a GVHD in liver transplantation, despite the challenge of extremely imbalanced EHR data.

</details>


### [200] [MedFedPure: A Medical Federated Framework with MAE-based Detection and Diffusion Purification for Inference-Time Attacks](https://arxiv.org/abs/2511.11625)
*Mohammad Karami,Mohammad Reza Nemati,Aidin Kazemi,Ali Mikaeili Barzili,Hamid Azadegan,Behzad Moshiri*

Main category: cs.LG

TL;DR: 本文提出MedFedPure框架保护联邦学习下脑肿瘤检测AI模型，在Br35H数据集评估效果好，提升鲁棒性且保持高准确率。


<details>
  <summary>Details</summary>
Motivation: 联邦学习训练的医学影像AI模型推理时易受对抗攻击，现有防御方法难适应联邦医疗场景。

Method: 提出MedFedPure框架，结合个性化FL模型、MAE检测模块和自适应扩散净化模块。

Result: 在Br35H数据集上，强攻击下性能从49.50%提升到87.33%，干净准确率达97.67%。

Conclusion: 该框架为临床工作流部署安全、可信、隐私保护的AI工具提供可行途径。

Abstract: Artificial intelligence (AI) has shown great potential in medical imaging, particularly for brain tumor detection using Magnetic Resonance Imaging (MRI). However, the models remain vulnerable at inference time when they are trained collaboratively through Federated Learning (FL), an approach adopted to protect patient privacy. Adversarial attacks can subtly alter medical scans in ways invisible to the human eye yet powerful enough to mislead AI models, potentially causing serious misdiagnoses. Existing defenses often assume centralized data and struggle to cope with the decentralized and diverse nature of federated medical settings. In this work, we present MedFedPure, a personalized federated learning defense framework designed to protect diagnostic AI models at inference time without compromising privacy or accuracy. MedFedPure combines three key elements: (1) a personalized FL model that adapts to the unique data distribution of each institution; (2) a Masked Autoencoder (MAE) that detects suspicious inputs by exposing hidden perturbations; and (3) an adaptive diffusion-based purification module that selectively cleans only the flagged scans before classification. Together, these steps offer robust protection while preserving the integrity of normal, benign images. We evaluated MedFedPure on the Br35H brain MRI dataset. The results show a significant gain in adversarial robustness, improving performance from 49.50% to 87.33% under strong attacks, while maintaining a high clean accuracy of 97.67%. By operating locally and in real time during diagnosis, our framework provides a practical path to deploying secure, trustworthy, and privacy-preserving AI tools in clinical workflows.
  Index Terms: cancer, tumor detection, federated learning, masked autoencoder, diffusion, privacy

</details>


### [201] [Adaptive Stepsizing for Stochastic Gradient Langevin Dynamics in Bayesian Neural Networks](https://arxiv.org/abs/2511.11666)
*Rajit Rajpal,Benedict Leimkuhler,Yuanhao Jiang*

Main category: cs.LG

TL;DR: 提出SA - SGLD自适应方案用于贝叶斯神经网络后验分布采样，在高曲率二维示例和图像分类中表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有随机梯度马尔可夫链蒙特卡罗（SGMCMC）方法对步长选择敏感，自适应变体常需昂贵的散度校正项。

Method: 基于SamAdams框架，采用时间重缩放根据监测量（通常是局部梯度范数）调整步长。

Result: 在高曲率二维玩具示例和使用尖锐先验的贝叶斯神经网络图像分类中，比SGLD实现更准确的后验采样。

Conclusion: SA - SGLD能自动调整步长，提高稳定性和混合性且无偏差，在相关任务中表现更优。

Abstract: Bayesian neural networks (BNNs) require scalable sampling algorithms to approximate posterior distributions over parameters. Existing stochastic gradient Markov Chain Monte Carlo (SGMCMC) methods are highly sensitive to the choice of stepsize and adaptive variants such as pSGLD typically fail to sample the correct invariant measure without addition of a costly divergence correction term. In this work, we build on the recently proposed `SamAdams' framework for timestep adaptation (Leimkuhler, Lohmann, and Whalley 2025), introducing an adaptive scheme: SA-SGLD, which employs time rescaling to modulate the stepsize according to a monitored quantity (typically the local gradient norm). SA-SGLD can automatically shrink stepsizes in regions of high curvature and expand them in flatter regions, improving both stability and mixing without introducing bias. We show that our method can achieve more accurate posterior sampling than SGLD on high-curvature 2D toy examples and in image classification with BNNs using sharp priors.

</details>


### [202] [SA-EMO: Structure-Aligned Encoder Mixture of Operators for Generalizable Full-waveform Inversion](https://arxiv.org/abs/2511.11627)
*Wang Zhenyu,Li Peiyuan,Shi Yongxiang,Wu Ruoyu,Zhang Lei*

Main category: cs.LG

TL;DR: 本文提出SA - EMO架构用于速度场反演，在基准测试和数据集上表现优于传统方法，为全波形反演带来新范式。


<details>
  <summary>Details</summary>
Motivation: 全波形反演存在固有问题，现有深度学习和数值加速方法依赖单一架构或算子，在未知或复杂地质环境泛化能力差，难以区分不同地质类型。

Method: 提出SA - EMO架构，先使用结构对齐编码器将高维地震波场映射到潜在空间，再用自适应路由机制选择和融合多个神经算子专家预测速度模型。

Result: 在OpenFWI基准和Marmousi2数据集上，SA - EMO显著优于传统方法，平均MAE降低约58.443%，边界分辨率提高约10.308%，消融研究表明各模块对性能提升有显著贡献。

Conclusion: 该工作为高效、可扩展和具有物理可解释性的全波形反演引入了新范式。

Abstract: Full-waveform inversion (FWI) can produce high-resolution subsurface models, yet it remains inherently ill-posed, highly nonlinear, and computationally intensive. Although recent deep learning and numerical acceleration methods have improved speed and scalability, they often rely on single CNN architectures or single neural operators, which struggle to generalize in unknown or complex geological settings and are ineffective at distinguishing diverse geological types. To address these issues, we propose a Structure-Aligned Encoder-Mixture-of-Operators (SA-EMO) architecture for velocity-field inversion under unknown subsurface structures. First, a structure-aligned encoder maps high-dimensional seismic wavefields into a physically consistent latent space, thereby eliminating spatio-temporal mismatch between the waveform and velocity domains, recovering high-frequency components, and enhancing feature generalization. Then, an adaptive routing mechanism selects and fuses multiple neural-operator experts, including spectral, wavelet, multiscale, and local operators, to predict the velocity model. We systematically evaluate our approach on the OpenFWI benchmark and the Marmousi2 dataset. Results show that SA-EMO significantly outperforms traditional CNN or single-operator methods, achieving an average MAE reduction of approximately 58.443% and an improvement in boundary resolution of about 10.308%. Ablation studies further reveal that the structure-aligned encoder, the expert-fusion mechanism, and the routing module each contribute markedly to the performance gains. This work introduces a new paradigm for efficient, scalable, and physically interpretable full-waveform inversion.

</details>


### [203] [Coordinate Descent for Network Linearization](https://arxiv.org/abs/2511.11781)
*Vlad Rakhlin,Amir Jevnisek,Shai Avidan*

Main category: cs.LG

TL;DR: 本文指出基于ResNet网络的隐私推理中ReLU激活是主要瓶颈，提出在离散域利用坐标下降法优化的方法，实验证明该方法在常见基准上达到了最先进水平。


<details>
  <summary>Details</summary>
Motivation: 解决基于ResNet网络的隐私推理中ReLU激活带来显著推理延迟的问题，且当前基于平滑近似的方法在优化最后硬阈值步骤会引入较大性能损失。

Method: 直接在离散域工作，利用坐标下降法作为优化框架。

Result: 通过大量实验证明该方法在常见基准上达到了最先进水平。

Conclusion: 所提出的在离散域利用坐标下降法的优化方法优于当前基于平滑近似的方法。

Abstract: ReLU activations are the main bottleneck in Private Inference that is based on ResNet networks. This is because they incur significant inference latency. Reducing ReLU count is a discrete optimization problem, and there are two common ways to approach it. Most current state-of-the-art methods are based on a smooth approximation that jointly optimizes network accuracy and ReLU budget at once. However, the last hard thresholding step of the optimization usually introduces a large performance loss. We take an alternative approach that works directly in the discrete domain by leveraging Coordinate Descent as our optimization framework. In contrast to previous methods, this yields a sparse solution by design. We demonstrate, through extensive experiments, that our method is State of the Art on common benchmarks.

</details>


### [204] [Global Feature Enhancing and Fusion Framework for Strain Gauge Time Series Classification](https://arxiv.org/abs/2511.11629)
*Xu Zhang,Peng Wang,Chen Wang,Zhe Xu,Xiaohua Nie,Wei Wang*

Main category: cs.LG

TL;DR: 本文提出基于超图的全局特征学习与融合框架以提升应变片状态时间序列识别精度，在工业和公共数据集验证效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在应变片状态（SGS）识别中仅提取局部特征不足，且卷积操作在提取全局特征有局限，需更全面表示SGS时间序列。

Method: 提出两个提取全局特征的思路，构建基于超图的全局特征学习与融合框架学习和融合全局特征实现语义一致。

Result: 方法在工业SGS和公共UCR数据集上验证，对未见数据在SGS识别中有更好泛化性。

Conclusion: 所提框架能增强SGS时间序列表示，提高识别精度。

Abstract: Strain Gauge Status (SGS) recognition is crucial in the field of intelligent manufacturing based on the Internet of Things, as accurate identification helps timely detection of failed mechanical components, avoiding accidents. The loading and unloading sequences generated by strain gauges can be identified through time series classification (TSC) algorithms. Recently, deep learning models, e.g., convolutional neural networks (CNNs) have shown remarkable success in the TSC task, as they can extract discriminative local features from the subsequences to identify the time series. However, we observe that only the local features may not be sufficient for expressing the time series, especially when the local sub-sequences between different time series are very similar, e.g., SGS data of aircraft wings in static strength experiments. Nevertheless, CNNs suffer from the limitation in extracting global features due to the nature of convolution operations. For extracting global features to more comprehensively represent the SGS time series, we propose two insights: (i) Constructing global features through feature engineering. (ii) Learning high-order relationships between local features to capture global features. To realize and utilize them, we propose a hypergraph-based global feature learning and fusion framework, which learns and fuses global features for semantic consistency to enhance the representation of SGS time series, thereby improving recognition accuracy. Our method designs are validated on industrial SGS and public UCR datasets, showing better generalization for unseen data in SGS recognition.

</details>


### [205] [Toward Better Generalization in Few-Shot Learning through the Meta-Component Combination](https://arxiv.org/abs/2511.11632)
*Qiuhao Zeng*

Main category: cs.LG

TL;DR: 提出新的元学习算法学习分类器，实验显示性能优越


<details>
  <summary>Details</summary>
Motivation: 解决基于度量的元学习在少样本学习中依赖已见类深度度量，可能过拟合且泛化能力差的问题

Method: 探索分类器子结构，提出新元学习算法，将分类器学为元组件组合，通过正交正则化器解耦元组件

Result: 在少样本基准任务上实验显示所提方法性能优越

Conclusion: 所提算法能提高少样本学习中分类器对未见类的泛化能力

Abstract: In few-shot learning, classifiers are expected to generalize to unseen classes given only a small number of instances of each new class. One of the popular solutions to few-shot learning is metric-based meta-learning. However, it highly depends on the deep metric learned on seen classes, which may overfit to seen classes and fail to generalize well on unseen classes. To improve the generalization, we explore the substructures of classifiers and propose a novel meta-learning algorithm to learn each classifier as a combination of meta-components. Meta-components are learned across meta-learning episodes on seen classes and disentangled by imposing an orthogonal regularizer to promote its diversity and capture various shared substructures among different classifiers. Extensive experiments on few-shot benchmark tasks show superior performances of the proposed method.

</details>


### [206] [A Deep Learning Model to Predicting Changes in Consumer Attributes for New Line-extended Products](https://arxiv.org/abs/2511.11646)
*Li Yinxing,Tsukasa Ishigaki*

Main category: cs.LG

TL;DR: 本文提出Conditional Tabular Variational Auto - Encoder（CTVAE）模型预测产品线延伸新产品的消费者属性变化，实验显示其性能优于现有模型，对产品线营销有指导意义。


<details>
  <summary>Details</summary>
Motivation: 过度产品线延伸会破坏品牌形象，需基于消费者需求进行适当延伸，营销人员需了解新产品主要客户的关键消费者属性。

Method: 提出Conditional Tabular Variational Auto - Encoder（CTVAE）模型，从消费者和产品的大规模表格数据中生成合成数据。

Result: CTVAE模型的预测性能优于现有模型。

Conclusion: 该方法有助于避免产品自相残杀，设计产品形象和营销策略，对改变容器或口味的新产品的有效产品线营销有启示。

Abstract: Product line extension is a marketing strategy that enhances a company's sphere of influence. Because excessive line extensions disrupt brand image, only appropriate line extensions based on consumer needs are desirable. Marketers should know the key consumer attributes of the primary customers for new line-extended products before companies enter the market. This paper describes a method for predicting changes in consumer attributes for new line-extended products using a novel deep learning model. The proposed model, Conditional Tabular Variational Auto-Encoder (CTVAE), generates synthetic data from large-scale tabular data of consumers and products. It can provide various implications about effective product line marketing for marketers. The experimental results demonstrate that the CTVAE offers superior prediction performance than existing models. We indicate implications for new products that change containers or flavors for effective product line marketing. The proposed approach has the potential to contribute to avoiding cannibalization and to designing product images and marketing strategies.

</details>


### [207] [An Explainable and Fair AI Tool for PCOS Risk Assessment: Calibration, Subgroup Equity, and Interactive Clinical Deployment](https://arxiv.org/abs/2511.11636)
*Asma Sadia Khan,Sadia Tabassum*

Main category: cs.LG

TL;DR: 本文提出用于预测多囊卵巢综合征（PCOS）的机器学习框架，评估模型性能，识别患者亚组诊断差异，校准模型后随机森林表现好，发现年龄相关差异，还有实时评估界面。


<details>
  <summary>Details</summary>
Motivation: 评估预测PCOS模型的性能，识别不同患者亚组间的诊断差异。

Method: 将基于SHAP的特征归因与人口统计审核相结合，纳入概率校准指标，训练随机森林、SVM和XGBoost模型并进行校准和公平性比较。

Result: 校准后的随机森林准确率达90.8%，SVM等模型各有表现，模型在25 - 35岁女性中表现最佳，在肥胖和瘦型PCOS患者中有不同效果。

Conclusion: 随机森林在校准和可解释性间平衡更好，模型存在年龄相关差异，基于Streamlit的界面促进AI研究与临床应用结合。

Abstract: This paper presents a fairness-audited and interpretable machine learning framework for predicting polycystic ovary syndrome (PCOS), designed to evaluate model performance and identify diagnostic disparities across patient subgroups. The framework integrated SHAP-based feature attributions with demographic audits to connect predictive explanations with observed disparities for actionable insights. Probabilistic calibration metrics (Brier Score and Expected Calibration Error) are incorporated to ensure reliable risk predictions across subgroups. Random Forest, SVM, and XGBoost models were trained with isotonic and Platt scaling for calibration and fairness comparison. A calibrated Random Forest achieved a high predictive accuracy of 90.8%. SHAP analysis identified follicle count, weight gain, and menstrual irregularity as the most influential features, which are consistent with the Rotterdam diagnostic criteria. Although the SVM with isotonic calibration achieved the lowest calibration error (ECE = 0.0541), the Random Forest model provided a better balance between calibration and interpretability (Brier = 0.0678, ECE = 0.0666). Therefore, it was selected for detailed fairness and SHAP analyses. Subgroup analysis revealed that the model performed best among women aged 25-35 (accuracy 90.9%) but underperformed in those under 25 (69.2%), highlighting age-related disparities. The model achieved perfect precision in obese women and maintained high recall in lean PCOS cases, demonstrating robustness across phenotypes. Finally, a Streamlit-based web interface enables real-time PCOS risk assessment, Rotterdam criteria evaluation, and interactive 'what-if' analysis, bridging the gap between AI research and clinical usability.

</details>


### [208] [Enhancing PINN Accuracy for the RLW Equation: Adaptive and Conservative Approaches](https://arxiv.org/abs/2511.11638)
*Aamir Shehzad*

Main category: cs.LG

TL;DR: 研究提出自适应和保守两种改进PINN方法求解RLW方程，结果表明PINN有效性因问题而异，明确守恒定律 enforcement 对高度非线性方程组优化可能有害。


<details>
  <summary>Details</summary>
Motivation: 标准物理信息神经网络求解RLW方程误差大，需改进方法。

Method: 开发自适应（自适应损失加权）和保守（强制显式守恒定律）两种改进PINN方法，用三个基准测试验证。

Result: 自适应PINN在解决复杂非线性相互作用问题上更优，保守PINN在解决单孤子和波浪长期行为问题上更优，两种方法结果与既定数值解误差在$O(10^{-5})$内。

Conclusion: PINN可无网格精确求解复杂偏微分方程组，守恒定律 enforcement 不一定提升PINN性能，为特定问题设计PINN提供指导。

Abstract: Standard physics-informed neural network implementations have produced large error rates when using these models to solve the regularized long wave (RLW) equation. Two improved PINN approaches were developed in this research: an adaptive approach with self-adaptive loss weighting and a conservative approach enforcing explicit conservation laws. Three benchmark tests were used to demonstrate how effective PINN's are as they relate to the type of problem being solved (i.e., time dependent RLW equation). The first was a single soliton traveling along a line (propagation), the second was the interaction between two solitons, and the third was the evolution of an undular bore over the course of $t=250$. The results demonstrated that the effectiveness of PINNs are problem specific. The adaptive PINN was significantly better than both the conservative PINN and the standard PINN at solving problems involving complex nonlinear interactions such as colliding two solitons. The conservative approach was significantly better at solving problems involving long term behavior of single solitons and undular bores. However, the most important finding from this research is that explicitly enforcing conservation laws may be harmful to optimizing the solution of highly nonlinear systems of equations and therefore requires special training methods. The results from our adaptive and conservative approaches were within $O(10^{-5})$ of established numerical solutions for the same problem, thus demonstrating that PINNs can provide accurate solutions to complex systems of partial differential equations without the need for a discretization of space or time (mesh free). Moreover, the finding from this research challenges the assumptions that conservation enforcement will always improve the performance of a PINN and provides researchers with guidelines for designing PINNs for use on specific types of problems.

</details>


### [209] [Finding Time Series Anomalies using Granular-ball Vector Data Description](https://arxiv.org/abs/2511.12147)
*Lifeng Shen,Liang Peng,Ruiwen Liu,Shuyin Xia,Yi Liu*

Main category: cs.LG

TL;DR: 提出Granular - ball One - Class Network (GBOC)用于时间序列异常检测，实验验证其有效性和优越性。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列异常检测方法依赖刚性假设，在复杂时间场景易失效，需新方法解决这些局限。

Method: 引入基于Granular - ball Vector Data Description (GVDD)的GBOC方法，GVDD通过密度引导的分层分裂过程划分潜在空间，GBOC训练时让样本与最近粒度球中心对齐，推理时根据到最近粒度球的距离计算异常分数。

Result: 广泛实验验证了GBOC方法在异常检测中的有效性和优越性。

Conclusion: GBOC在时间序列异常检测中能兼顾鲁棒性和效率，可应对检测挑战。

Abstract: Modeling normal behavior in dynamic, nonlinear time series data is challenging for effective anomaly detection. Traditional methods, such as nearest neighbor and clustering approaches, often depend on rigid assumptions, such as a predefined number of reliable neighbors or clusters, which frequently break down in complex temporal scenarios. To address these limitations, we introduce the Granular-ball One-Class Network (GBOC), a novel approach based on a data-adaptive representation called Granular-ball Vector Data Description (GVDD). GVDD partitions the latent space into compact, high-density regions represented by granular-balls, which are generated through a density-guided hierarchical splitting process and refined by removing noisy structures. Each granular-ball serves as a prototype for local normal behavior, naturally positioning itself between individual instances and clusters while preserving the local topological structure of the sample set. During training, GBOC improves the compactness of representations by aligning samples with their nearest granular-ball centers. During inference, anomaly scores are computed based on the distance to the nearest granular-ball. By focusing on dense, high-quality regions and significantly reducing the number of prototypes, GBOC delivers both robustness and efficiency in anomaly detection. Extensive experiments validate the effectiveness and superiority of the proposed method, highlighting its ability to handle the challenges of time series anomaly detection.

</details>


### [210] [Understanding InfoNCE: Transition Probability Matrix Induced Feature Clustering](https://arxiv.org/abs/2511.12180)
*Ge Cheng,Shuo Wang,Yun Zhang*

Main category: cs.LG

TL;DR: 本文研究InfoNCE理论基础，提出新损失函数SC - InfoNCE，实验表明其在多领域表现良好。


<details>
  <summary>Details</summary>
Motivation: InfoNCE在对比学习中占主导但理论基础有限。

Method: 引入显式特征空间和转移概率矩阵，基于理论洞察提出SC - InfoNCE损失函数。

Result: 在图像、图和文本等基准数据集实验中，SC - InfoNCE在不同领域均取得了稳定可靠的性能。

Conclusion: SC - InfoNCE能灵活控制特征相似度对齐，使训练目标更好匹配下游数据统计特性。

Abstract: Contrastive learning has emerged as a cornerstone of unsupervised representation learning across vision, language, and graph domains, with InfoNCE as its dominant objective. Despite its empirical success, the theoretical underpinnings of InfoNCE remain limited. In this work, we introduce an explicit feature space to model augmented views of samples and a transition probability matrix to capture data augmentation dynamics. We demonstrate that InfoNCE optimizes the probability of two views sharing the same source toward a constant target defined by this matrix, naturally inducing feature clustering in the representation space. Leveraging this insight, we propose Scaled Convergence InfoNCE (SC-InfoNCE), a novel loss function that introduces a tunable convergence target to flexibly control feature similarity alignment. By scaling the target matrix, SC-InfoNCE enables flexible control over feature similarity alignment, allowing the training objective to better match the statistical properties of downstream data. Experiments on benchmark datasets, including image, graph, and text tasks, show that SC-InfoNCE consistently achieves strong and reliable performance across diverse domains.

</details>


### [211] [Environment-Aware Transfer Reinforcement Learning for Sustainable Beam Selection](https://arxiv.org/abs/2511.11647)
*Dariush Salami,Ramin Hashemi,Parham Kazemi,Mikko A. Uusitalo*

Main category: cs.LG

TL;DR: 本文提出用迁移学习和强化学习改进5G及未来网络波束选择的可持续方法，减少训练时间和计算开销，提升能效。


<details>
  <summary>Details</summary>
Motivation: 传统基于强化学习的波束选择模型训练时间长、计算资源需求大，在不同传播环境中可扩展性和能效面临挑战。

Method: 将环境建模为点云，通过计算点云间的Chamfer距离识别结构相似环境，利用迁移学习复用预训练模型。

Result: 训练时间和计算开销减少16倍，降低功耗，加速部署，减少碳排放，模拟结果显示保持高性能同时大幅降低能源成本。

Conclusion: 迁移学习能在动态多样传播环境中实现可扩展、自适应且环保的基于强化学习的波束选择策略。

Abstract: This paper presents a novel and sustainable approach for improving beam selection in 5G and beyond networks using transfer learning and Reinforcement Learning (RL). Traditional RL-based beam selection models require extensive training time and computational resources, particularly when deployed in diverse environments with varying propagation characteristics posing a major challenge for scalability and energy efficiency. To address this, we propose modeling the environment as a point cloud, where each point represents the locations of gNodeBs (gNBs) and surrounding scatterers. By computing the Chamfer distance between point clouds, structurally similar environments can be efficiently identified, enabling the reuse of pre-trained models through transfer learning. This methodology leads to a 16x reduction in training time and computational overhead, directly contributing to energy efficiency. By minimizing the need for retraining in each new deployment, our approach significantly lowers power consumption and supports the development of green and sustainable Artificial Intelligence (AI) in wireless systems. Furthermore, it accelerates time-to-deployment, reduces carbon emissions associated with training, and enhances the viability of deploying AI-driven communication systems at the edge. Simulation results confirm that our approach maintains high performance while drastically cutting energy costs, demonstrating the potential of transfer learning to enable scalable, adaptive, and environmentally conscious RL-based beam selection strategies in dynamic and diverse propagation environments.

</details>


### [212] [AIF: Asynchronous Inference Framework for Cost-Effective Pre-Ranking](https://arxiv.org/abs/2511.12934)
*Zhi Kou,Xiang-Rong Sheng,Shuguang Han,Zhishan Zhao,Yueyao Cheng,Han Zhu,Jian Xu,Bo Zheng*

Main category: cs.LG

TL;DR: 提出异步推理框架AIF解决工业推荐系统预排序模型顺序执行框架的瓶颈，在淘宝展示广告系统成功部署。


<details>
  <summary>Details</summary>
Motivation: 工业推荐系统中基于DNN的预排序模型顺序执行框架存在冗余计算、延迟高的问题，限制模型容量和系统效率。

Method: 提出AIF架构，将独立于交互的组件与实时预测解耦，并行进行用户侧计算，近线进行物品侧计算；在AIF框架内设计模型，对依赖交互的组件采用近似方法进行在线实时预测。

Result: AIF提高了计算效率，降低了延迟，释放资源改善特征集和模型架构，实现显著性能提升，且未显著增加计算和延迟成本。

Conclusion: AIF是有效的解决方案，已成功部署于淘宝展示广告系统。

Abstract: In industrial recommendation systems, pre-ranking models based on deep neural networks (DNNs) commonly adopt a sequential execution framework: feature fetching and model forward computation are triggered only after receiving candidates from the upstream retrieval stage. This design introduces inherent bottlenecks, including redundant computations of identical users/items and increased latency due to strictly sequential operations, which jointly constrain the model's capacity and system efficiency. To address these limitations, we propose the Asynchronous Inference Framework (AIF), a cost-effective computational architecture that decouples interaction-independent components, those operating within a single user or item, from real-time prediction. AIF reorganizes the model inference process by performing user-side computations in parallel with the retrieval stage and conducting item-side computations in a nearline manner. This means that interaction-independent components are calculated just once and completed before the real-time prediction phase of the pre-ranking stage. As a result, AIF enhances computational efficiency and reduces latency, freeing up resources to significantly improve the feature set and model architecture of interaction-independent components. Moreover, we delve into model design within the AIF framework, employing approximated methods for interaction-dependent components in online real-time predictions. By co-designing both the framework and the model, our solution achieves notable performance gains without significantly increasing computational and latency costs. This has enabled the successful deployment of AIF in the Taobao display advertising system.

</details>


### [213] [Lightweight Time Series Data Valuation on Time Series Foundation Models via In-Context Finetuning](https://arxiv.org/abs/2511.11648)
*Shunyu Wu,Tianyue Li,Yixuan Leng,Jingyi Suo,Jian Lou,Dan Li,See-Kiong Ng*

Main category: cs.LG

TL;DR: 提出LTSV方法对时间序列基础模型进行轻量级数据估值，实验证明其有效且计算量可控。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据质量对TSFM性能至关重要，但传统数据估值方法有计算瓶颈且难保留时间依赖，需新方法。

Method: 提出LTSV方法，基于上下文微调近似影响函数，通过测量上下文损失变化估计样本贡献，引入时间块聚合捕捉时间依赖。

Result: 在多个数据集和模型上实验表明，LTSV能提供可靠且强大的估值性能，计算量可接受。

Conclusion: 时间序列基础模型的上下文微调为时间序列学习中数据归因和模型泛化提供了实用有效的桥梁。

Abstract: Time series foundation models (TSFMs) have demonstrated increasing capabilities due to their extensive pretraining on large volumes of diverse time series data. Consequently, the quality of time series data is crucial to TSFM performance, rendering an accurate and efficient data valuation of time series for TSFMs indispensable. However, traditional data valuation methods, such as influence functions, face severe computational bottlenecks due to their poor scalability with growing TSFM model sizes and often fail to preserve temporal dependencies. In this paper, we propose LTSV, a Lightweight Time Series Valuation on TSFMS via in-context finetuning. Grounded in the theoretical evidence that in-context finetuning approximates the influence function, LTSV estimates a sample's contribution by measuring the change in context loss after in-context finetuning, leveraging the strong generalization capabilities of TSFMs to produce robust and transferable data valuations. To capture temporal dependencies, we introduce temporal block aggregation, which integrates per-block influence scores across overlapping time windows. Experiments across multiple time series datasets and models demonstrate that LTSV consistently provides reliable and strong valuation performance, while maintaining manageable computational requirements. Our results suggest that in-context finetuning on time series foundation models provides a practical and effective bridge between data attribution and model generalization in time series learning.

</details>


### [214] [Cross-view Joint Learning for Mixed-Missing Multi-view Unsupervised Feature Selection](https://arxiv.org/abs/2511.12261)
*Zongxin Shen,Yanyong Huang,Dongjie Wang,Jinyuan Chang,Fengmao Lv,Tianrui Li,Xiaoyi Jiang*

Main category: cs.LG

TL;DR: 提出新的IMUFS方法CLIM - FS解决混合缺失问题，在多数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有IMUFS方法存在不适用于混合缺失场景、视图间信息利用不足、缺乏理论分析等问题。

Method: 将缺失视图和变量的插补集成到基于非负正交矩阵分解的特征选择模型中，利用共识聚类结构和跨视图局部几何结构，进行理论分析。

Result: 在八个真实多视图数据集上的实验表明CLIM - FS优于现有方法。

Conclusion: CLIM - FS能有效解决IMUFS的混合缺失问题，具有较好性能。

Abstract: Incomplete multi-view unsupervised feature selection (IMUFS), which aims to identify representative features from unlabeled multi-view data containing missing values, has received growing attention in recent years. Despite their promising performance, existing methods face three key challenges: 1) by focusing solely on the view-missing problem, they are not well-suited to the more prevalent mixed-missing scenario in practice, where some samples lack entire views or only partial features within views; 2) insufficient utilization of consistency and diversity across views limits the effectiveness of feature selection; and 3) the lack of theoretical analysis makes it unclear how feature selection and data imputation interact during the joint learning process. Being aware of these, we propose CLIM-FS, a novel IMUFS method designed to address the mixed-missing problem. Specifically, we integrate the imputation of both missing views and variables into a feature selection model based on nonnegative orthogonal matrix factorization, enabling the joint learning of feature selection and adaptive data imputation. Furthermore, we fully leverage consensus cluster structure and cross-view local geometrical structure to enhance the synergistic learning process. We also provide a theoretical analysis to clarify the underlying collaborative mechanism of CLIM-FS. Experimental results on eight real-world multi-view datasets demonstrate that CLIM-FS outperforms state-of-the-art methods.

</details>


### [215] [Enhanced Water Leak Detection with Convolutional Neural Networks and One-Class Support Vector Machine](https://arxiv.org/abs/2511.11650)
*Daniele Ugo Leonzio,Paolo Bestagini,Marco Marcon,Stefano Tubaro*

Main category: cs.LG

TL;DR: 本文提出基于水压测量的漏损检测新方法，在模拟数据集上表现优于近期方法。


<details>
  <summary>Details</summary>
Motivation: 水是重要资源，供水管网漏损严重，需可靠有效的漏损检测和定位系统。

Method: 基于供水管网一系列节点的水压测量，利用特征提取器和单类支持向量机，以无漏损数据训练，将漏损作为异常检测。

Result: 在使用摩德纳供水管网的模拟数据集上，该方法优于近期漏损检测方法。

Conclusion: 所提漏损检测新方法有效，性能良好。

Abstract: Water is a critical resource that must be managed efficiently. However, a substantial amount of water is lost each year due to leaks in Water Distribution Networks (WDNs). This underscores the need for reliable and effective leak detection and localization systems. In recent years, various solutions have been proposed, with data-driven approaches gaining increasing attention due to their superior performance. In this paper, we propose a new method for leak detection. The method is based on water pressure measurements acquired at a series of nodes of a WDN. Our technique is a fully data-driven solution that makes only use of the knowledge of the WDN topology, and a series of pressure data acquisitions obtained in absence of leaks. The proposed solution is based on an feature extractor and a one-class Support Vector Machines (SVM) trained on no-leak data, so that leaks are detected as anomalies. The results achieved on a simulate dataset using the Modena WDN demonstrate that the proposed solution outperforms recent methods for leak detection.

</details>


### [216] [Optimal Self-Consistency for Efficient Reasoning with Large Language Models](https://arxiv.org/abs/2511.12309)
*Austin Feng,Marius Alonso,Ambroise Odonnat*

Main category: cs.LG

TL;DR: 文章对自一致性（SC）技术的扩展行为进行分析，提出新变体Blend - ASC，提升样本效率。


<details>
  <summary>Details</summary>
Motivation: SC在大规模应用时成本高，且缺乏样本效率和扩展行为的统一理论处理。

Method: 利用众数估计和投票理论分析SC扩展行为及其变体，推导并验证幂律扩展，分析不同采样方案样本效率。

Result: 提出Blend - ASC，平均比普通SC少用6.8倍样本，超越固定和动态分配的SC基线。

Conclusion: Blend - ASC无超参数，能适应任意样本预算，在效率上表现优越，可用于任何SC应用。

Abstract: Self-consistency (SC) is a widely used test-time inference technique for improving performance in chain-of-thought reasoning. It involves generating multiple responses, or samples from a large language model (LLM) and selecting the most frequent answer. This procedure can naturally be viewed as a majority vote or empirical mode estimation. Despite its effectiveness, SC is prohibitively expensive at scale when naively applied to datasets, and it lacks a unified theoretical treatment of sample efficiency and scaling behavior. In this paper, we provide the first comprehensive analysis of SC's scaling behavior and its variants, drawing on mode estimation and voting theory. We derive and empirically validate power law scaling for self-consistency across datasets, and analyze the sample efficiency for fixed-allocation and dynamic-allocation sampling schemes. From these insights, we introduce Blend-ASC, a novel variant of self-consistency that dynamically allocates samples to questions during inference, achieving state-of-the-art sample efficiency. Our approach uses 6.8x fewer samples than vanilla SC on average, outperforming both fixed- and dynamic-allocation SC baselines, thereby demonstrating the superiority of our approach in terms of efficiency. In contrast to existing variants, Blend-ASC is hyperparameter-free and can fit an arbitrary sample budget, ensuring it can be easily applied to any self-consistency application.

</details>


### [217] [Incomplete Depression Feature Selection with Missing EEG Channels](https://arxiv.org/abs/2511.11651)
*Zhijian Gong,Wenjia Dong,Xueyuan Xu,Fulin Wei,Chunyu Liu,Li Zhuo*

Main category: cs.LG

TL;DR: 本文提出IDFS - MEC特征选择方法用于抑郁分析，实验表明其优于10种流行特征选择方法。


<details>
  <summary>Details</summary>
Motivation: EEG特征存在冗余、无关和噪声信息，且现实中EEG数据采集面临电极脱落和噪声干扰等挑战，需改进抑郁分析方法。

Method: 提出IDFS - MEC方法，将缺失通道指示信息和自适应通道权重学习融入正交回归，减少不完整通道对模型构建的影响，利用全局冗余最小化学习减少所选特征子集间的冗余信息。

Result: 在MODMA和PRED - d003数据集上的实验显示，IDFS - MEC选择的EEG特征子集在3、64和128通道设置中优于10种流行特征选择方法。

Conclusion: IDFS - MEC是一种有效的用于稳健抑郁分析的特征选择方法。

Abstract: As a critical mental health disorder, depression has severe effects on both human physical and mental well-being. Recent developments in EEG-based depression analysis have shown promise in improving depression detection accuracies. However, EEG features often contain redundant, irrelevant, and noisy information. Additionally, real-world EEG data acquisition frequently faces challenges, such as data loss from electrode detachment and heavy noise interference. To tackle the challenges, we propose a novel feature selection approach for robust depression analysis, called Incomplete Depression Feature Selection with Missing EEG Channels (IDFS-MEC). IDFS-MEC integrates missing-channel indicator information and adaptive channel weighting learning into orthogonal regression to lessen the effects of incomplete channels on model construction, and then utilizes global redundancy minimization learning to reduce redundant information among selected feature subsets. Extensive experiments conducted on MODMA and PRED-d003 datasets reveal that the EEG feature subsets chosen by IDFS-MEC have superior performance than 10 popular feature selection methods among 3-, 64-, and 128-channel settings.

</details>


### [218] [How many stations are sufficient? Exploring the effect of urban weather station density reduction on imputation accuracy of air temperature and humidity](https://arxiv.org/abs/2511.11652)
*Marvin Plein,Carsten F. Dormann,Andreas Christen*

Main category: cs.LG

TL;DR: 提出逐步移除站点程序精简德国弗莱堡气象站网络（WSN），发现大幅减少站点数仍能保持高预测精度，展示了精简WSN以高效分配资源的潜力。


<details>
  <summary>Details</summary>
Motivation: 维护WSN成本高且劳动密集，需找到精简WSN的方法以高效分配资源。

Method: 提出逐步移除站点程序来精简现有WSN，并模拟降低WSN密度后分析子集重现原WSN气温和湿度模式的能力。

Result: 大幅减少站点数仍能保持高预测精度，如从42个减到4个，气温和相对湿度RMSE增幅仅20%和16%；森林中远程站点预测精度差，但仍优于数值城市陆面模型；建成区和农村边缘站点重建全市气候特征最有价值。

Conclusion: 精简WSN在城市气候研究中具有高效分配财务和人力相关资源的潜力。

Abstract: Urban weather station networks (WSNs) are widely used to monitor urban weather and climate patterns and aid urban planning. However, maintaining WSNs is expensive and labor-intensive. Here, we present a step-wise station removal procedure to thin an existing WSN in Freiburg, Germany, and analyze the ability of WSN subsets to reproduce air temperature and humidity patterns of the entire original WSN for a year following a simulated reduction of WSN density. We found that substantial reductions in station numbers after one year of full deployment are possible while retaining high predictive accuracy. A reduction from 42 to 4 stations, for instance, increased mean prediction RMSEs from 0.69 K to 0.83 K for air temperature and from 3.8% to 4.4% for relative humidity, corresponding to RMSE increases of only 20% and 16%, respectively. Predictive accuracy is worse for remote stations in forests than for stations in built-up or open settings, but consistently better than a state-of-the-art numerical urban land-surface model (Surface Urban Energy and Water Balance Scheme). Stations located at the edges between built-up and rural areas are most valuable when reconstructing city-wide climate characteristics. Our study demonstrates the potential of thinning WSNs to maximize the efficient allocation of financial and personnel-related resources in urban climate research.

</details>


### [219] [Center-Outward q-Dominance: A Sample-Computable Proxy for Strong Stochastic Dominance in Multi-Objective Optimisation](https://arxiv.org/abs/2511.12545)
*Robin van der Laag,Hao Wang,Thomas Bäck,Yingjie Fan*

Main category: cs.LG

TL;DR: 本文基于最优传输理论引入中心向外q - 优势关系，开发经验测试程序，推导样本量阈值，在超参数调优和多目标优化算法选择两个场景验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 随机多目标优化（SMOOP）中多数实证研究采用的标量化方法会丢失信息且不可靠，需要更好的多元分布排序方法。

Method: 引入中心向外q - 优势关系并证明其蕴含强一阶随机优势（FSD），开发基于q - 优势的经验测试程序并推导样本量阈值。

Result: 在超参数调优中可在期望超体积指标无法区分时比较调优器；在NSGA - II算法中用q - 优势替代基于均值的选择，在噪声增强的ZDT基准问题上有更好的收敛率。

Conclusion: 中心向外q - 优势为SMOOP寻找真正的随机优势解提供了原则性且易于处理的基础。

Abstract: Stochastic multi-objective optimization (SMOOP) requires ranking multivariate distributions; yet, most empirical studies perform scalarization, which loses information and is unreliable. Based on the optimal transport theory, we introduce the center-outward q-dominance relation and prove it implies strong first-order stochastic dominance (FSD). Also, we develop an empirical test procedure based on q-dominance, and derive an explicit sample size threshold, $n^*(δ)$, to control the Type I error. We verify the usefulness of our approach in two scenarios: (1) as a ranking method in hyperparameter tuning; (2) as a selection method in multi-objective optimization algorithms. For the former, we analyze the final stochastic Pareto sets of seven multi-objective hyperparameter tuners on the YAHPO-MO benchmark tasks with q-dominance, which allows us to compare these tuners when the expected hypervolume indicator (HVI, the most common performance metric) of the Pareto sets becomes indistinguishable. For the latter, we replace the mean value-based selection in the NSGA-II algorithm with $q$-dominance, which shows a superior convergence rate on noise-augmented ZDT benchmark problems. These results establish center-outward q-dominance as a principled, tractable foundation for seeking truly stochastically dominant solutions for SMOOPs.

</details>


### [220] [Convergence of Multiagent Learning Systems for Traffic control](https://arxiv.org/abs/2511.11654)
*Sayambhu Sen,Shalabh Bhatnagar*

Main category: cs.LG

TL;DR: 本文聚焦交通信号控制中多智能体强化学习算法的理论基础，利用随机近似方法分析学习动态，证明该算法在给定条件下收敛。


<details>
  <summary>Details</summary>
Motivation: 班加罗尔等城市快速城市化导致交通拥堵，多智能体强化学习是减少通勤延迟的有前景策略，但缺乏其在交通控制中稳定性和收敛性的理论分析。

Method: 利用随机近似方法，正式分析学习动态。

Result: 证明了用于交通控制的特定多智能体强化学习算法在给定条件下收敛。

Conclusion: 为交通控制中的多智能体强化学习算法提供了理论依据，拓展了单智能体异步值迭代收敛证明。

Abstract: Rapid urbanization in cities like Bangalore has led to severe traffic congestion, making efficient Traffic Signal Control (TSC) essential. Multi-Agent Reinforcement Learning (MARL), often modeling each traffic signal as an independent agent using Q-learning, has emerged as a promising strategy to reduce average commuter delays. While prior work Prashant L A et. al has empirically demonstrated the effectiveness of this approach, a rigorous theoretical analysis of its stability and convergence properties in the context of traffic control has not been explored. This paper bridges that gap by focusing squarely on the theoretical basis of this multi-agent algorithm. We investigate the convergence problem inherent in using independent learners for the cooperative TSC task. Utilizing stochastic approximation methods, we formally analyze the learning dynamics. The primary contribution of this work is the proof that the specific multi-agent reinforcement learning algorithm for traffic control is proven to converge under the given conditions extending it from single agent convergence proofs for asynchronous value iteration.

</details>


### [221] [On the Probabilistic Learnability of Compact Neural Network Preimage Bounds](https://arxiv.org/abs/2511.11656)
*Luca Marzari,Manuele Bicego,Ferdinando Cicalese,Alessandro Farinelli*

Main category: cs.LG

TL;DR: 采用概率视角，提出RF - ProVe方法计算神经网络原像近似，有理论统计保证。


<details>
  <summary>Details</summary>
Motivation: 现有可证明方法计算神经网络原像边界受#P - 难问题限制，扩展性不足。

Method: 采用基于引导和随机化方法，引入RF - ProVe，利用随机决策树集合生成候选输入区域并通过主动重采样优化。

Result: 理论推导为区域纯度和全局覆盖提供了正式的统计保证。

Conclusion: 为精确求解器扩展性不足的情况提供了实用、可扩展的紧凑原像近似计算解决方案。

Abstract: Although recent provable methods have been developed to compute preimage bounds for neural networks, their scalability is fundamentally limited by the #P-hardness of the problem. In this work, we adopt a novel probabilistic perspective, aiming to deliver solutions with high-confidence guarantees and bounded error. To this end, we investigate the potential of bootstrap-based and randomized approaches that are capable of capturing complex patterns in high-dimensional spaces, including input regions where a given output property holds. In detail, we introduce $\textbf{R}$andom $\textbf{F}$orest $\textbf{Pro}$perty $\textbf{Ve}$rifier ($\texttt{RF-ProVe}$), a method that exploits an ensemble of randomized decision trees to generate candidate input regions satisfying a desired output property and refines them through active resampling. Our theoretical derivations offer formal statistical guarantees on region purity and global coverage, providing a practical, scalable solution for computing compact preimage approximations in cases where exact solvers fail to scale.

</details>


### [222] [Sample Complexity of Agnostic Multiclass Classification: Natarajan Dimension Strikes Back](https://arxiv.org/abs/2511.12659)
*Alon Cohen,Liad Erez,Steve Hanneke,Tomer Koren,Yishay Mansour,Shay Moran,Qian Zhang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The fundamental theorem of statistical learning states that binary PAC learning is governed by a single parameter -- the Vapnik-Chervonenkis (VC) dimension -- which determines both learnability and sample complexity. Extending this to multiclass classification has long been challenging, since Natarajan's work in the late 80s proposing the Natarajan dimension (Nat) as a natural analogue of VC. Daniely and Shalev-Shwartz (2014) introduced the DS dimension, later shown by Brukhim et al. (2022) to characterize multiclass learnability. Brukhim et al. also showed that Nat and DS can diverge arbitrarily, suggesting that multiclass learning is governed by DS rather than Nat. We show that agnostic multiclass PAC sample complexity is in fact governed by two distinct dimensions. Specifically, we prove nearly tight agnostic sample complexity bounds that, up to log factors, take the form $\frac{DS^{1.5}}ε + \frac{Nat}{ε^2}$ where $ε$ is the excess risk. This bound is tight up to a $\sqrt{DS}$ factor in the first term, nearly matching known $Nat/ε^2$ and $DS/ε$ lower bounds. The first term reflects the DS-controlled regime, while the second shows that the Natarajan dimension still dictates asymptotic behavior for small $ε$. Thus, unlike binary or online classification -- where a single dimension (VC or Littlestone) controls both phenomena -- multiclass learning inherently involves two structural parameters. Our technical approach departs from traditional agnostic learning methods based on uniform convergence or reductions to realizable cases. A key ingredient is a novel online procedure based on a self-adaptive multiplicative-weights algorithm performing a label-space reduction, which may be of independent interest.

</details>


### [223] [SpecQuant: Spectral Decomposition and Adaptive Truncation for Ultra-Low-Bit LLMs Quantization](https://arxiv.org/abs/2511.11663)
*Zhixiong Zhao,Fangxin Liu,Junjie Wang,Chenyang Guan,Zongwu Wang,Li Jiang,Haibing Guan*

Main category: cs.LG

TL;DR: 本文从傅里叶频域角度研究大语言模型极端压缩，提出SpecQuant框架，在LLaMA - 3 8B上实现4位量化，提升推理速度、降低内存使用且缩小精度差距。


<details>
  <summary>Details</summary>
Motivation: 准确的开源大语言模型出现，推动开发先进量化技术以在终端设备高效部署，需解决大语言模型极端压缩问题。

Method: 提出两阶段SpecQuant框架，第一阶段平滑激活异常值并转移到权重矩阵，第二阶段应用通道级低频傅里叶截断，推理时引入轻量级截断模块调整阈值。

Result: 在LLaMA - 3 8B上对权重和激活都实现4位量化，零样本精度与全精度差距仅1.5%，推理速度提升2倍，内存使用降低3倍。

Conclusion: 从傅里叶频域角度提出的SpecQuant框架在大语言模型极端压缩方面有效，能在保持精度的同时提升推理效率、降低内存使用。

Abstract: The emergence of accurate open large language models (LLMs) has sparked a push for advanced quantization techniques to enable efficient deployment on end-user devices. In this paper, we revisit the challenge of extreme LLM compression -- targeting ultra-low-bit quantization for both activations and weights -- from a Fourier frequency domain perspective. We propose SpecQuant, a two-stage framework that tackles activation outliers and cross-channel variance. In the first stage, activation outliers are smoothed and transferred into the weight matrix to simplify downstream quantization. In the second stage, we apply channel-wise low-frequency Fourier truncation to suppress high-frequency components while preserving essential signal energy, improving quantization robustness. Our method builds on the principle that most of the weight energy is concentrated in low-frequency components, which can be retained with minimal impact on model accuracy. To enable runtime adaptability, we introduce a lightweight truncation module during inference that adjusts truncation thresholds based on channel characteristics. On LLaMA-3 8B, SpecQuant achieves 4-bit quantization for both weights and activations, narrowing the zero-shot accuracy gap to only 1.5% compared to full precision, while delivering 2 times faster inference and 3times lower memory usage.

</details>


### [224] [A Closer Look at Personalized Fine-Tuning in Heterogeneous Federated Learning](https://arxiv.org/abs/2511.12695)
*Minghui Chen,Hrad Ghoukasian,Ruinan Jin,Zehua Wang,Sai Praneeth Karimireddy,Xiaoxiao Li*

Main category: cs.LG

TL;DR: 提出LP - FT策略用于联邦学习，通过多数据集和变体评估证明其在平衡个性化与泛化上的优越性，分析特征失真现象并给出理论解释及应用条件。


<details>
  <summary>Details</summary>
Motivation: 联邦学习难以平衡全局泛化与本地个性化，现有PFT方法存在过拟合等问题。

Method: 将LP - FT策略应用于联邦学习，通过对七个数据集和六个PFT变体进行系统评估。

Result: LP - FT在平衡个性化和泛化方面表现更优，发现了联邦特征失真现象，从理论上解释了LP - FT缓解该现象的原理。

Conclusion: 确定了LP - FT优于标准微调的条件，为在联邦学习中部署强大的个性化提供了可操作的指南。

Abstract: Federated Learning (FL) enables decentralized, privacy-preserving model training but struggles to balance global generalization and local personalization due to non-identical data distributions across clients. Personalized Fine-Tuning (PFT), a popular post-hoc solution, fine-tunes the final global model locally but often overfits to skewed client distributions or fails under domain shifts. We propose adapting Linear Probing followed by full Fine-Tuning (LP-FT), a principled centralized strategy for alleviating feature distortion (Kumar et al., 2022), to the FL setting. Through systematic evaluation across seven datasets and six PFT variants, we demonstrate LP-FT's superiority in balancing personalization and generalization. Our analysis uncovers federated feature distortion, a phenomenon where local fine-tuning destabilizes globally learned features, and theoretically characterizes how LP-FT mitigates this via phased parameter updates. We further establish conditions (e.g., partial feature overlap, covariate-concept shift) under which LP-FT outperforms standard fine-tuning, offering actionable guidelines for deploying robust personalization in FL.

</details>


### [225] [Clifford Algebraic Rotor Embeddings : Maybe embeddings should start to CARE](https://arxiv.org/abs/2511.11665)
*Sameeksha Sriram,Ayush Paliwal,Alexander S. Ecker,Chase van de Geijn*

Main category: cs.LG

TL;DR: 本文探索基于四元数的旋转位置嵌入方法QuatRo，将其推广到Clifford代数旋转嵌入CARE，实现旋转嵌入到任意维度的扩展及在多向量中编码位置信息，并进行了初步实验。


<details>
  <summary>Details</summary>
Motivation: 现有RoPE扩展方法多为非交换的，会丧失其平移等变性，如Spherical RoPE存在旋转顺序模糊的问题。

Method: 采用基于四元数的方法QuatRo代替欧拉角，利用四元数表示3D旋转的能力对旋转轴进行参数化；将QuatRo推广到Clifford代数旋转嵌入CARE。

Result: 表明Mixed RoPE和Spherical RoPE是QuatRo的特殊情况，实现旋转嵌入扩展到任意维度及在多等级多向量中编码位置信息。

Conclusion: 提出了新的旋转位置嵌入方法及推广，进行了初步实验对比不同类型的旋转嵌入。

Abstract: Rotary Positional Embeddings (RoPE) have demonstrated exceptional performance as a positional encoding method, consistently outperforming their baselines. While recent work has sought to extend RoPE to higher-dimensional inputs, many such extensions are non-commutative, thereby forfeiting RoPE's shift-equivariance property. Spherical RoPE is one such non-commutative variant, motivated by the idea of rotating embedding vectors on spheres rather than circles. However, spherical rotations are inherently non-commutative, making the choice of rotation sequence ambiguous. In this work, we explore a quaternion-based approach -- Quaternion Rotary Embeddings (QuatRo) -- in place of Euler angles, leveraging quaternions' ability to represent 3D rotations to parameterize the axes of rotation. We show Mixed RoPE and Spherical RoPE to be special cases of QuatRo. Further, we propose a generalization of QuatRo to Clifford Algebraic Rotary Embeddings (CARE) using geometric algebra. Viewing quaternions as the even subalgebra of Cl(3,0,0), we extend the notion of rotary embeddings from quaternions to Clifford rotors acting on multivectors. This formulation enables two key generalizations: (1) extending rotary embeddings to arbitrary dimensions, and (2) encoding positional information in multivectors of multiple grades, not just vectors. We present preliminary experiments comparing spherical, quaternion, and Clifford-based rotary embeddings.

</details>


### [226] [Conformal Online Learning of Deep Koopman Linear Embeddings](https://arxiv.org/abs/2511.12760)
*Ben Gao,Jordan Patracone,Stéphane Chrétien,Olivier Alata*

Main category: cs.LG

TL;DR: 介绍了COLoKe框架，结合深度特征学习与多步预测一致性，用保形机制防止过拟合，在基准动力系统上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 自适应地从流数据中更新非线性动力系统的Koopman不变表示。

Method: 将深度特征学习与多步预测一致性结合，采用保形机制，当模型预测误差超过动态校准阈值时触发更新。

Result: 在基准动力系统上的实验表明，COLoKe能保持长期预测精度，减少不必要更新并避免过拟合。

Conclusion: COLoKe框架在处理非线性动力系统的流数据时有效且实用。

Abstract: We introduce Conformal Online Learning of Koopman embeddings (COLoKe), a novel framework for adaptively updating Koopman-invariant representations of nonlinear dynamical systems from streaming data. Our modeling approach combines deep feature learning with multistep prediction consistency in the lifted space, where the dynamics evolve linearly. To prevent overfitting, COLoKe employs a conformal-style mechanism that shifts the focus from evaluating the conformity of new states to assessing the consistency of the current Koopman model. Updates are triggered only when the current model's prediction error exceeds a dynamically calibrated threshold, allowing selective refinement of the Koopman operator and embedding. Empirical results on benchmark dynamical systems demonstrate the effectiveness of COLoKe in maintaining long-term predictive accuracy while significantly reducing unnecessary updates and avoiding overfitting.

</details>


### [227] [Beyond Superficial Forgetting: Thorough Unlearning through Knowledge Density Estimation and Block Re-insertion](https://arxiv.org/abs/2511.11667)
*Feng Guo,Yuntao Wen,Shen Gao,Junshuo Zhang,Shuo Shang*

Main category: cs.LG

TL;DR: 提出KUnBR方法解决现有大语言模型机器去学习方法无法彻底去除有害知识的问题，实验证明其有先进的遗忘性能且能保持模型效用。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型机器去学习方法难以彻底去除有害知识，留下易恢复的残余有害知识，需解决隐私、合规和伦理问题。

Method: 提出KUnBR方法，先识别富含有害知识的层，再通过重新插入策略消除有害知识；引入知识密度估计定位有害知识层，设计层重新插入策略确保去学习时有效梯度传播。

Result: 在多个去学习和通用能力基准上的大量实验表明，KUnBR实现了先进的遗忘性能并保持了模型效用。

Conclusion: KUnBR方法能有效解决现有大语言模型机器去学习的问题，具有良好性能。

Abstract: Machine unlearning, which selectively removes harmful knowledge from a pre-trained model without retraining from scratch, is crucial for addressing privacy, regulatory compliance, and ethical concerns in Large Language Models (LLMs). However, existing unlearning methods often struggle to thoroughly remove harmful knowledge, leaving residual harmful knowledge that can be easily recovered. To address these limitations, we propose Knowledge Density-Guided Unlearning via Blocks Reinsertion (KUnBR), a novel approach that first identifies layers with rich harmful knowledge and then thoroughly eliminates the harmful knowledge via re-insertion strategy. Our method introduces knowledge density estimation to quantify and locate layers containing the most harmful knowledge, enabling precise unlearning. Additionally, we design a layer re-insertion strategy that extracts and re-inserts harmful knowledge-rich layers into the original LLM, bypassing gradient obstruction caused by cover layers and ensuring effective gradient propagation during unlearning. Extensive experiments conducted on several unlearning and general capability benchmarks demonstrate that KUnBR achieves state-of-the-art forgetting performance while maintaining model utility.

</details>


### [228] [Do traveling waves make good positional encodings?](https://arxiv.org/abs/2511.11668)
*Chase van de Geijn,Ayush Paliwal,Timo Lüddecke,Alexander S. Ecker*

Main category: cs.LG

TL;DR: 提出基于行波的新型位置编码机制RollPE，表现优于传统绝对位置嵌入，与RoPE相当，还推导了连续情况和数学等价性。


<details>
  <summary>Details</summary>
Motivation: 传统位置编码方法有局限，需更好的机制来补偿自注意力的排列不变性，捕捉平移等变性。

Method: 在自注意力中对查询和键张量应用循环滚动操作实现RollPE。

Result: RollPE显著优于传统绝对位置嵌入，与RoPE表现相当，能在查询和键空间隐式施加地形结构，与RoPE特定配置数学等价。

Conclusion: 从行波角度看RollPE或可简化RoPE，并将其与大脑信息流过程联系起来。

Abstract: Transformers rely on positional encoding to compensate for the inherent permutation invariance of self-attention. Traditional approaches use absolute sinusoidal embeddings or learned positional vectors, while more recent methods emphasize relative encodings to better capture translation equivariances. In this work, we propose RollPE, a novel positional encoding mechanism based on traveling waves, implemented by applying a circular roll operation to the query and key tensors in self-attention. This operation induces a relative shift in phase across positions, allowing the model to compute attention as a function of positional differences rather than absolute indices. We show this simple method significantly outperforms traditional absolute positional embeddings and is comparable to RoPE. We derive a continuous case of RollPE which implicitly imposes a topographic structure on the query and key space. We further derive a mathematical equivalence of RollPE to a particular configuration of RoPE. Viewing RollPE through the lens of traveling waves may allow us to simplify RoPE and relate it to processes of information flow in the brain.

</details>


### [229] [On the Information Processing of One-Dimensional Wasserstein Distances with Finite Samples](https://arxiv.org/abs/2511.12881)
*Cheongjae Jang,Jonghyun Won,Soyeon Jun,Chun Kee Chung,Keehyoung Joo,Yung-Kyun Noh*

Main category: cs.LG

TL;DR: 分析一维Wasserstein距离在有限样本下处理信息的能力，发现其能突出与速率和支持相关的有意义密度差异。


<details>
  <summary>Details</summary>
Motivation: 当支持集显著重叠但密度存在显著逐点差异时，不清楚Wasserstein距离能否及如何准确识别这些差异，特别是在有限样本设置下的解析表征。

Method: 利用泊松过程并分离速率因子，分析一维Wasserstein距离在有限样本下处理信息的能力。

Result: 通过神经尖峰序列解码和氨基酸接触频率数据证实，一维Wasserstein距离能突出与速率和支持相关的有意义密度差异。

Conclusion: 一维Wasserstein距离能捕捉逐点密度差异，且该信息与支持差异相协调。

Abstract: Leveraging the Wasserstein distance -- a summation of sample-wise transport distances in data space -- is advantageous in many applications for measuring support differences between two underlying density functions. However, when supports significantly overlap while densities exhibit substantial pointwise differences, it remains unclear whether and how this transport information can accurately identify these differences, particularly their analytic characterization in finite-sample settings. We address this issue by conducting an analysis of the information processing capabilities of the one-dimensional Wasserstein distance with finite samples. By utilizing the Poisson process and isolating the rate factor, we demonstrate the capability of capturing the pointwise density difference with Wasserstein distances and how this information harmonizes with support differences. The analyzed properties are confirmed using neural spike train decoding and amino acid contact frequency data. The results reveal that the one-dimensional Wasserstein distance highlights meaningful density differences related to both rate and support.

</details>


### [230] [H-Model: Dynamic Neural Architectures for Adaptive Processing](https://arxiv.org/abs/2511.11669)
*Dmytro Hospodarchuk*

Main category: cs.LG

TL;DR: 本文探索能根据输入数据动态调整内部结构的神经网络架构，提出路由机制，是概念原型，受思维过程启发，目前是初步研究。


<details>
  <summary>Details</summary>
Motivation: 提出一种可探索适应性和更具可解释性网络的新架构方向，让系统学习计算结构而非仅优化现有基准。

Method: 引入路由机制，使每层能影响输出在网络中的传播，实现迭代和自适应计算。

Result: 因计算资源和数据限制，目前是初步研究，初始观察有一定前景。

Conclusion: 该架构是概念原型，需在更有利计算条件下的未来实验评估其潜力。

Abstract: This article explores the design and experimentation of a neural network architecture capable of dynamically adjusting its internal structure based on the input data. The proposed model introduces a routing mechanism that allows each layer to influence how its outputs are propagated through the network, enabling iterative and adaptive computation. This concept is loosely inspired by the idea of thought processes and dynamic reasoning, where information flow is conditioned not only on the data itself, but also on the internal state of the system.
  It is important to note that this work does not aim to compete with state-of-the-art language models in terms of performance. Instead, it presents a conceptual prototype-an architectural framework that opens up a new direction for exploring adaptable and potentially more interpretable networks. The goal is not optimization of existing benchmarks but rather the proposal of a system that can learn not only representations, but also the structure of computation itself.
  Due to practical constraints in computing resources and data, this study remains a preliminary investigation. Nevertheless, initial observations show promise, and the architecture's full potential can only be evaluated in future experiments under more favorable computational conditions.

</details>


### [231] [Generalization Bounds for Semi-supervised Matrix Completion with Distributional Side Information](https://arxiv.org/abs/2511.13049)
*Antoine Ledent,Mun Chong Soo,Nong Minh Hieu*

Main category: cs.LG

TL;DR: 研究低秩矩阵完成问题，结合未标注和标注数据，给出误差界，实验验证方法优于仅依赖显式反馈的基线。


<details>
  <summary>Details</summary>
Motivation: 受推荐系统中显式和隐式反馈场景启发，研究低秩矩阵完成问题。

Method: 利用低秩子空间恢复理论和矩阵完成模型的经典泛化界。

Result: 给出误差界，合成实验验证误差自然分离，真实实验表明方法优于基线。

Conclusion: 所做假设为研究推荐系统中显式和隐式反馈的交互提供了有效的理论设置。

Abstract: We study a matrix completion problem where both the ground truth $R$ matrix and the unknown sampling distribution $P$ over observed entries are low-rank matrices, and \textit{share a common subspace}. We assume that a large amount $M$ of \textit{unlabeled} data drawn from the sampling distribution $P$ is available, together with a small amount $N$ of labeled data drawn from the same distribution and noisy estimates of the corresponding ground truth entries. This setting is inspired by recommender systems scenarios where the unlabeled data corresponds to `implicit feedback' (consisting in interactions such as purchase, click, etc. ) and the labeled data corresponds to the `explicit feedback', consisting of interactions where the user has given an explicit rating to the item. Leveraging powerful results from the theory of low-rank subspace recovery, together with classic generalization bounds for matrix completion models, we show error bounds consisting of a sum of two error terms scaling as $\widetilde{O}\left(\sqrt{\frac{nd}{M}}\right)$ and $\widetilde{O}\left(\sqrt{\frac{dr}{N}}\right)$ respectively, where $d$ is the rank of $P$ and $r$ is the rank of $M$. In synthetic experiments, we confirm that the true generalization error naturally splits into independent error terms corresponding to the estimations of $P$ and and the ground truth matrix $\ground$ respectively. In real-life experiments on Douban and MovieLens with most explicit ratings removed, we demonstrate that the method can outperform baselines relying only on the explicit ratings, demonstrating that our assumptions provide a valid toy theoretical setting to study the interaction between explicit and implicit feedbacks in recommender systems.

</details>


### [232] [Evaluation of LLM-based Explanations for a Learning Analytics Dashboard](https://arxiv.org/abs/2511.11671)
*Alina Deriyeva,Benjamin Paassen*

Main category: cs.LG

TL;DR: 研究利用大语言模型为学习分析仪表盘数据生成解释，经专家研究发现其更受青睐，能提升学习体验并符合教学标准。


<details>
  <summary>Details</summary>
Motivation: 学习分析仪表盘有效性受数据可解释性影响，需协助数据解释。

Method: 采用大语言模型为仪表盘数据生成解释，并与独立仪表盘和教师提供的解释在专家研究中对比评估。

Result: 基于大语言模型的技能状态解释和学习建议比其他情况更受青睐。

Conclusion: 使用大语言模型进行解释可提升学习者学习体验并维持教师认可的教学标准。

Abstract: Learning Analytics Dashboards can be a powerful tool to support self-regulated learning in Digital Learning Environments and promote development of meta-cognitive skills, such as reflection. However, their effectiveness can be affected by the interpretability of the data they provide. To assist in the interpretation, we employ a large language model to generate verbal explanations of the data in the dashboard and evaluate it against a standalone dashboard and explanations provided by human teachers in an expert study with university level educators (N=12). We find that the LLM-based explanations of the skill state presented in the dashboard, as well as general recommendations on how to proceed with learning within the course are significantly more favored compared to the other conditions. This indicates that using LLMs for interpretation purposes can enhance the learning experience for learners while maintaining the pedagogical standards approved by teachers.

</details>


### [233] [Synergistic Feature Fusion for Latent Lyrical Classification: A Gated Deep Learning Architecture](https://arxiv.org/abs/2511.11673)
*M. A. Gameiro*

Main category: cs.LG

TL;DR: 提出协同融合层（SFL）架构用于歌词内容分类，性能优于随机森林基线，验证非线性门控架构优势。


<details>
  <summary>Details</summary>
Motivation: 解决复杂高维深度语义特征与简单可解释结构线索在歌词内容分类中的融合挑战。

Method: 引入SFL架构，利用门控机制用低维辅助特征调制Sentence - BERT嵌入，将任务重构为二元分类。

Result: SFL模型准确率0.9894，Macro F1分数0.9894，ECE降低93%，Log Loss降低2.5倍，优于随机森林基线。

Conclusion: 非线性门控优于简单特征拼接，SFL模型是复杂多模态歌词分析的可靠系统。

Abstract: This study addresses the challenge of integrating complex, high-dimensional deep semantic features with simple, interpretable structural cues for lyrical content classification. We introduce a novel Synergistic Fusion Layer (SFL) architecture, a deep learning model utilizing a gated mechanism to modulate Sentence-BERT embeddings (Fdeep) using low-dimensional auxiliary features (Fstruct). The task, derived from clustering UMAP-reduced lyrical embeddings, is reframed as binary classification, distinguishing a dominant, homogeneous cluster (Class 0) from all other content (Class 1). The SFL model achieved an accuracy of 0.9894 and a Macro F1 score of 0.9894, outperforming a comprehensive Random Forest (RF) baseline that used feature concatenation (Accuracy = 0.9868). Crucially, the SFL model demonstrated vastly superior reliability and calibration, exhibiting a 93% reduction in Expected Calibration Error (ECE = 0.0035) and a 2.5x lower Log Loss (0.0304) compared to the RF baseline (ECE = 0.0500; Log Loss = 0.0772). This performance validates the architectural hypothesis that non-linear gating is superior to simple feature concatenation, establishing the SFL model as a robust and trustworthy system for complex multimodal lyrical analysis.

</details>


### [234] [Laplace Learning in Wasserstein Space](https://arxiv.org/abs/2511.13229)
*Mary Chriselda Antony Oliver,Michael Roberts,Carola-Bibiane Schönlieb,Matthew Thorpe*

Main category: cs.LG

TL;DR: 本文基于流形假设研究图半监督学习方法，将其从有限维欧氏空间拓展到无限维，证明离散图p - Dirichlet能量收敛性，刻画Wasserstein空间子流形上的Laplace - Beltrami算子，并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 基于流形假设，将图半监督学习算法从有限维欧氏空间拓展到无限维的Wasserstein空间。

Method: 证明离散图p - Dirichlet能量到其连续形式的变分收敛性，刻画Wasserstein空间子流形上的Laplace - Beltrami算子。

Result: 在基准数据集上的数值实验表明，所提方法在高维设置下分类性能具有一致性。

Conclusion: 提出的理论框架在高维设置下是有效的，能实现图半监督学习从有限维到无限维的拓展。

Abstract: The manifold hypothesis posits that high-dimensional data typically resides on low-dimensional sub spaces. In this paper, we assume manifold hypothesis to investigate graph-based semi-supervised learning
  methods. In particular, we examine Laplace Learning in the Wasserstein space, extending the classical
  notion of graph-based semi-supervised learning algorithms from finite-dimensional Euclidean spaces to
  an infinite-dimensional setting. To achieve this, we prove variational convergence of a discrete graph p- Dirichlet energy to its continuum counterpart. In addition, we characterize the Laplace-Beltrami operator
  on asubmanifold of the Wasserstein space. Finally, we validate the proposed theoretical framework through
  numerical experiments conducted on benchmark datasets, demonstrating the consistency of our classification performance in high-dimensional settings.

</details>


### [235] [Beyond One-Way Pruning: Bidirectional Pruning-Regrowth for Extreme Accuracy-Sparsity Tradeoff](https://arxiv.org/abs/2511.11675)
*Junchen Liu,Yi Sheng*

Main category: cs.LG

TL;DR: 现有剪枝方法在高稀疏度下性能下降，提出双向剪枝 - 再生策略解决问题。


<details>
  <summary>Details</summary>
Motivation: 现有迭代和一次性剪枝方法在稀疏度超过阈值时，模型性能急剧下降，限制压缩比，无法满足硬件平台尺寸约束。

Method: 从满足硬件约束的极度压缩网络开始，选择性地再生关键连接以恢复性能。

Result: 有效缓解了高稀疏度下常见的准确率急剧下降问题。

Conclusion: 所提出的双向剪枝 - 再生策略能克服现有剪枝方法在高稀疏度下的性能限制。

Abstract: As a widely adopted model compression technique, model pruning has demonstrated strong effectiveness across various architectures. However, we observe that when sparsity exceeds a certain threshold, both iterative and one-shot pruning methods lead to a steep decline in model performance. This rapid degradation limits the achievable compression ratio and prevents models from meeting the stringent size constraints required by certain hardware platforms, rendering them inoperable. To overcome this limitation, we propose a bidirectional pruning-regrowth strategy. Starting from an extremely compressed network that satisfies hardware constraints, the method selectively regenerates critical connections to recover lost performance, effectively mitigating the sharp accuracy drop commonly observed under high sparsity conditions.

</details>


### [236] [Counterfactual Explainable AI (XAI) Method for Deep Learning-Based Multivariate Time Series Classification](https://arxiv.org/abs/2511.13237)
*Alan G. Paredes Cetina,Kaouther Benguessoum,Raoni Lourenço,Sylvain Kubler*

Main category: cs.LG

TL;DR: 提出用于MTS的多目标反事实解释方法CONFETTI，经实验验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 深度学习用于MTS分类和回归缺乏透明度，现有XAI和CE方法有局限，无法兼顾准确性、接近性和稀疏性。

Method: 提出CONFETTI方法，识别关键MTS子序列，定位反事实目标，优化修改时间序列以平衡预测置信度、接近性和稀疏性。

Result: 在七个MTS数据集上评估，CONFETTI在优化目标和六个其他指标上始终优于现有CE方法，置信度提高≥10%，稀疏性改善≥40%。

Conclusion: CONFETTI能以最小变化提供可操作见解，提高可解释性和决策支持。

Abstract: Recent advances in deep learning have improved multivariate time series (MTS) classification and regression by capturing complex patterns, but their lack of transparency hinders decision-making. Explainable AI (XAI) methods offer partial insights, yet often fall short of conveying the full decision space. Counterfactual Explanations (CE) provide a promising alternative, but current approaches typically prioritize either accuracy, proximity or sparsity -- rarely all -- limiting their practical value. To address this, we propose CONFETTI, a novel multi-objective CE method for MTS. CONFETTI identifies key MTS subsequences, locates a counterfactual target, and optimally modifies the time series to balance prediction confidence, proximity and sparsity. This method provides actionable insights with minimal changes, improving interpretability, and decision support. CONFETTI is evaluated on seven MTS datasets from the UEA archive, demonstrating its effectiveness in various domains. CONFETTI consistently outperforms state-of-the-art CE methods in its optimization objectives, and in six other metrics from the literature, achieving $\geq10\%$ higher confidence while improving sparsity in $\geq40\%$.

</details>


### [237] [Federated Learning for Pediatric Pneumonia Detection: Enabling Collaborative Diagnosis Without Sharing Patient Data](https://arxiv.org/abs/2511.11714)
*Daniel M. Jimenez-Gutierrez,Enrique Zuazua,Joaquin Del Rio,Oleksii Sliusarenko,Xabi Uribe-Etxebarria*

Main category: cs.LG

TL;DR: 本文评估了使用Sherpa.ai FL平台的联邦学习，实现多医院协作训练肺炎CXR分类器，结果显示性能显著提升，表明联邦学习可实现高性能、安全私密的肺炎检测。


<details>
  <summary>Details</summary>
Motivation: 现有AI用于CXR肺炎检测受数据分布、医院差异和隐私法规等限制，需新方法解决。

Method: 使用Sherpa.ai FL平台进行联邦学习，基于儿科肺炎CXR数据集模拟跨医院非IID数据协作。

Result: 多医院通过联邦学习协作训练使性能显著提升，准确率达0.900，ROC - AUC达0.966，比单医院模型有大幅提高。

Conclusion: 联邦学习能在医疗网络中实现高性能、可泛化、安全私密的肺炎检测，对低数据领域疾病诊断和治疗发展有突破意义。

Abstract: Early and accurate pneumonia detection from chest X-rays (CXRs) is clinically critical to expedite treatment and isolation, reduce complications, and curb unnecessary antibiotic use. Although artificial intelligence (AI) substantially improves CXR-based detection, development is hindered by globally distributed data, high inter-hospital variability, and strict privacy regulations (e.g., HIPAA, GDPR) that make centralization impractical. These constraints are compounded by heterogeneous imaging protocols, uneven data availability, and the costs of transferring large medical images across geographically dispersed sites.
  In this paper, we evaluate Federated Learning (FL) using the Sherpa.ai FL platform, enabling multiple hospitals (nodes) to collaboratively train a CXR classifier for pneumonia while keeping data in place and private. Using the Pediatric Pneumonia Chest X-ray dataset, we simulate cross-hospital collaboration with non-independent and non-identically distributed (non-IID) data, reproducing real-world variability across institutions and jurisdictions. Our experiments demonstrate that collaborative and privacy-preserving training across multiple hospitals via FL led to a dramatic performance improvement achieving 0.900 Accuracy and 0.966 ROC-AUC, corresponding to 47.5% and 50.0% gains over single-hospital models (0.610; 0.644), without transferring any patient CXR. These results indicate that FL delivers high-performing, generalizable, secure and private pneumonia detection across healthcare networks, with data kept local. This is especially relevant for rare diseases, where FL enables secure multi-institutional collaboration without data movement, representing a breakthrough for accelerating diagnosis and treatment development in low-data domains.

</details>


### [238] [Learning with Preserving for Continual Multitask Learning](https://arxiv.org/abs/2511.11676)
*Hanchen David Wang,Siwoo Bae,Zirong Chen,Meiyi Ma*

Main category: cs.LG

TL;DR: 提出Learning with Preserving (LwP)框架解决持续多任务学习问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法在持续多任务学习（CMTL）场景中因学习碎片化、特定任务特征相互干扰而失败，需新方法。

Method: 引入LwP框架，核心是Dynamically Weighted Distance Preservation (DWDP)损失，通过正则化潜在数据表示之间的成对距离防止表示漂移。

Result: 在时间序列和图像基准测试中，LwP减轻了灾难性遗忘，始终优于现有基线，对分布变化有更强鲁棒性，唯一超越单任务学习基线。

Conclusion: LwP适合隐私敏感应用，对现实动态环境有效。

Abstract: Artificial intelligence systems in critical fields like autonomous driving and medical imaging analysis often continually learn new tasks using a shared stream of input data. For instance, after learning to detect traffic signs, a model may later need to learn to classify traffic lights or different types of vehicles using the same camera feed. This scenario introduces a challenging setting we term Continual Multitask Learning (CMTL), where a model sequentially learns new tasks on an underlying data distribution without forgetting previously learned abilities. Existing continual learning methods often fail in this setting because they learn fragmented, task-specific features that interfere with one another. To address this, we introduce Learning with Preserving (LwP), a novel framework that shifts the focus from preserving task outputs to maintaining the geometric structure of the shared representation space. The core of LwP is a Dynamically Weighted Distance Preservation (DWDP) loss that prevents representation drift by regularizing the pairwise distances between latent data representations. This mechanism of preserving the underlying geometric structure allows the model to retain implicit knowledge and support diverse tasks without requiring a replay buffer, making it suitable for privacy-conscious applications. Extensive evaluations on time-series and image benchmarks show that LwP not only mitigates catastrophic forgetting but also consistently outperforms state-of-the-art baselines in CMTL tasks. Notably, our method shows superior robustness to distribution shifts and is the only approach to surpass the strong single-task learning baseline, underscoring its effectiveness for real-world dynamic environments.

</details>


### [239] [Fast and Robust Simulation-Based Inference With Optimization Monte Carlo](https://arxiv.org/abs/2511.13394)
*Vasilis Gkolemis,Christos Diou,Michael Gutmann*

Main category: cs.LG

TL;DR: 提出针对可微模拟器的贝叶斯参数推断新方法，减少运行时间并保证准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于模拟的推断方法在高维参数空间或输出信息不全的问题中成本高，需大量模拟。

Method: 基于Optimization Monte Carlo框架，将随机模拟重构成确定性优化问题，用梯度法高效定位高后验密度区域，JAX实现增强性能。

Result: 大量实验表明，新方法准确性与现有方法相当甚至更好，且显著减少运行时间。

Conclusion: 新方法能以大幅减少的运行时间实现准确的后验推断。

Abstract: Bayesian parameter inference for complex stochastic simulators is challenging due to intractable likelihood functions. Existing simulation-based inference methods often require large number of simulations and become costly to use in high-dimensional parameter spaces or in problems with partially uninformative outputs. We propose a new method for differentiable simulators that delivers accurate posterior inference with substantially reduced runtimes. Building on the Optimization Monte Carlo framework, our approach reformulates stochastic simulation as deterministic optimization problems. Gradient-based methods are then applied to efficiently navigate toward high-density posterior regions and avoid wasteful simulations in low-probability areas. A JAX-based implementation further enhances the performance through vectorization of key method components. Extensive experiments, including high-dimensional parameter spaces, uninformative outputs, multiple observations and multimodal posteriors show that our method consistently matches, and often exceeds, the accuracy of state-of-the-art approaches, while reducing the runtime by a substantial margin.

</details>


### [240] [Homotopy-Guided Self-Supervised Learning of Parametric Solutions for AC Optimal Power Flow](https://arxiv.org/abs/2511.11677)
*Shimiao Li,Aaron Tuor,Draguna Vrabie,Larry Pileggi,Jan Drgona*

Main category: cs.LG

TL;DR: 提出用于参数化交流最优潮流问题的同伦引导自监督学习优化方法，在基准测试中提升可行性，展示了基于同伦启发式方法在电力系统优化的潜力。


<details>
  <summary>Details</summary>
Motivation: 交流最优潮流存在非凸性，标准学习方法难收敛到可行、高质量解，需新方法。

Method: 引入同伦引导自监督学习优化方法，训练时对目标和约束进行连续变形，从易解问题过渡到原问题。

Result: 在标准IEEE交流最优潮流基准测试中，同伦引导学习优化方法比非同伦基线显著提高可行性，目标值与全最优潮流求解器相当。

Conclusion: 基于同伦的启发式方法在电力系统优化的可扩展、约束感知学习优化中有前景。

Abstract: Learning to optimize (L2O) parametric approximations of AC optimal power flow (AC-OPF) solutions offers the potential for fast, reusable decision-making in real-time power system operations. However, the inherent nonconvexity of AC-OPF results in challenging optimization landscapes, and standard learning approaches often fail to converge to feasible, high-quality solutions. This work introduces a \textit{homotopy-guided self-supervised L2O method} for parametric AC-OPF problems. The key idea is to construct a continuous deformation of the objective and constraints during training, beginning from a relaxed problem with a broad basin of attraction and gradually transforming it toward the original problem. The resulting learning process improves convergence stability and promotes feasibility without requiring labeled optimal solutions or external solvers. We evaluate the proposed method on standard IEEE AC-OPF benchmarks and show that homotopy-guided L2O significantly increases feasibility rates compared to non-homotopy baselines, while achieving objective values comparable to full OPF solvers. These findings demonstrate the promise of homotopy-based heuristics for scalable, constraint-aware L2O in power system optimization.

</details>


### [241] [Larger Datasets Can Be Repeated More: A Theoretical Analysis of Multi-Epoch Scaling in Linear Regression](https://arxiv.org/abs/2511.13421)
*Tingkai Yan,Haodong Wen,Binghui Li,Kairong Luo,Wenguang Chen,Kaifeng Lyu*

Main category: cs.LG

TL;DR: 本文理论分析多轮训练对线性回归数据缩放定律的影响，定义有效复用率，得出不同情况下缩放行为，指出先前研究忽略因素并强调未来研究需考虑数据规模和分布。


<details>
  <summary>Details</summary>
Motivation: 现有研究多在单轮大规模语料下探讨大语言模型数据缩放定律，本文旨在研究有限数据和多轮训练下的情况。

Method: 理论分析线性回归中多轮训练对数据缩放定律的影响，定义有效复用率量化单轮训练与多轮训练达到相同测试损失时数据集大小的关系。

Result: 分析出SGD在强凸或Zipf分布数据下有效复用率的缩放行为，指出先前经验研究的忽略因素，且通过大语言模型实证验证。

Conclusion: 有效复用率中最大K值依赖于数据规模和分布，未来研究数据复用的缩放定律需明确建模这两个因素。

Abstract: While data scaling laws of large language models (LLMs) have been widely examined in the one-pass regime with massive corpora, their form under limited data and repeated epochs remains largely unexplored. This paper presents a theoretical analysis of how a common workaround, training for multiple epochs on the same dataset, reshapes the data scaling laws in linear regression. Concretely, we ask: to match the performance of training on a dataset of size $N$ for $K$ epochs, how much larger must a dataset be if the model is trained for only one pass? We quantify this using the \textit{effective reuse rate} of the data, $E(K, N)$, which we define as the multiplicative factor by which the dataset must grow under one-pass training to achieve the same test loss as $K$-epoch training. Our analysis precisely characterizes the scaling behavior of $E(K, N)$ for SGD in linear regression under either strong convexity or Zipf-distributed data: (1) When $K$ is small, we prove that $E(K, N) \approx K$, indicating that every new epoch yields a linear gain; (2) As $K$ increases, $E(K, N)$ plateaus at a problem-dependent value that grows with $N$ ($Θ(\log N)$ for the strongly-convex case), implying that larger datasets can be repeated more times before the marginal benefit vanishes. These theoretical findings point out a neglected factor in a recent empirical study (Muennighoff et al. (2023)), which claimed that training LLMs for up to $4$ epochs results in negligible loss differences compared to using fresh data at each step, \textit{i.e.}, $E(K, N) \approx K$ for $K \le 4$ in our notation. Supported by further empirical validation with LLMs, our results reveal that the maximum $K$ value for which $E(K, N) \approx K$ in fact depends on the data size and distribution, and underscore the need to explicitly model both factors in future studies of scaling laws with data reuse.

</details>


### [242] [A neural optimization framework for free-boundary diffeomorphic mapping problems and its applications](https://arxiv.org/abs/2511.11679)
*Zhehao Xu,Lok Ming Lui*

Main category: cs.LG

TL;DR: 提出Spectral Beltrami Network (SBN)和SBN - Opt框架优化自由边界微分同胚，实验显示其优于传统算法。


<details>
  <summary>Details</summary>
Motivation: 自由边界微分同胚优化困难，传统数值算法需地标条件且无法用于基于梯度的优化。

Method: 提出SBN将LSQC能量嵌入多尺度网格谱架构，提出SBN - Opt框架优化自由边界微分同胚并控制局部几何畸变。

Result: 在密度均衡映射和不一致表面配准实验中，SBN - Opt优于传统数值算法。

Conclusion: SBN - Opt在自由边界微分同胚优化问题上有良好效果，优于传统算法。

Abstract: Free-boundary diffeomorphism optimization is a core ingredient in the surface mapping problem but remains notoriously difficult because the boundary is unconstrained and local bijectivity must be preserved under large deformation. Numerical Least-Squares Quasiconformal (LSQC) theory, with its provable existence, uniqueness, similarity-invariance and resolution-independence, offers an elegant mathematical remedy. However, the conventional numerical algorithm requires landmark conditioning, and cannot be applied into gradient-based optimization. We propose a neural surrogate, the Spectral Beltrami Network (SBN), that embeds LSQC energy into a multiscale mesh-spectral architecture. Next, we propose the SBN guided optimization framework SBN-Opt which optimizes free-boundary diffeomorphism for the problem, with local geometric distortion explicitly controllable. Extensive experiments on density-equalizing maps and inconsistent surface registration demonstrate our SBN-Opt's superiority over traditional numerical algorithms.

</details>


### [243] [AdamX: An Adam improvement algorithm based on a novel exponential decay mechanism for the second-order moment estimate](https://arxiv.org/abs/2511.13465)
*Meng Zhu,Quan Xiao,Weidong Min*

Main category: cs.LG

TL;DR: 提出AdamX算法解决Adam易收敛到非平坦极小值问题，实验显示其性能优于Adam及其变体，代码开源。


<details>
  <summary>Details</summary>
Motivation: Adam在大语言模型时代虽为主流优化算法，但比基于SGD的算法更易收敛到非平坦极小值，需改进。

Method: 提出新型二阶矩估计指数衰减率，随训练推进逐渐减弱学习步长修正强度，稳定训练期退化到SGD。

Result: 新型二阶矩估计指数衰减率优于当前的，AdamX性能稳定超过Adam及其变体。

Conclusion: AdamX算法能提高稳定训练期的稳定性，可能增强泛化能力。

Abstract: Since the 21st century, artificial intelligence has been leading a new round of industrial revolution. Under the training framework, the optimization algorithm aims to stably converge high-dimensional optimization to local and even global minima. Entering the era of large language models, although the scale of model parameters and data has increased, Adam remains the mainstream optimization algorithm. However, compared with stochastic gradient descent (SGD) based optimization algorithms, Adam is more likely to converge to non-flat minima. To address this issue, the AdamX algorithm is proposed. Its core innovation lies in the proposition of a novel type of second-order moment estimation exponential decay rate, which gradually weakens the learning step correction strength as training progresses, and degrades to SGD in the stable training period, thereby improving the stability of training in the stable period and possibly enhancing generalization ability. Experimental results show that our second-order moment estimation exponential decay rate is better than the current second-order moment estimation exponential decay rate, and AdamX can stably outperform Adam and its variants in terms of performance. Our code is open-sourced at https://github.com/mengzhu0308/AdamX.

</details>


### [244] [Probabilistic Wildfire Susceptibility from Remote Sensing Using Random Forests and SHAP](https://arxiv.org/abs/2511.11680)
*Udaya Bhasker Cheerala,Varun Teja Chirukuri,Venkata Akhil Kumar Gummadi,Jintu Moni Bhuyan,Praveen Damacharla*

Main category: cs.LG

TL;DR: 研究应用随机森林算法结合SHAP开发加州野火风险地图，模型表现良好，确定不同生态系统关键驱动因素，提供评估野火风险方法。


<details>
  <summary>Details</summary>
Motivation: 野火对全球生态系统构成重大威胁，加州火灾频发，需开发综合野火风险地图。

Method: 应用随机森林算法，结合SHAP进行可解释人工智能分析，用空间和时间验证策略评估模型性能。

Result: RF模型预测性能强，空间交叉验证有一定可转移性，时间分割验证泛化性更好，确定不同生态系统关键驱动因素，明确高风险区域。

Conclusion: RF - SHAP框架是评估野火风险的有效方法，可辅助决策和制定减灾策略。

Abstract: Wildfires pose a significant global threat to ecosystems worldwide, with California experiencing recurring fires due to various factors, including climate, topographical features, vegetation patterns, and human activities. This study aims to develop a comprehensive wildfire risk map for California by applying the random forest (RF) algorithm, augmented with Explainable Artificial Intelligence (XAI) through Shapley Additive exPlanations (SHAP), to interpret model predictions. Model performance was assessed using both spatial and temporal validation strategies. The RF model demonstrated strong predictive performance, achieving near-perfect discrimination for grasslands (AUC = 0.996) and forests (AUC = 0.997). Spatial cross-validation revealed moderate transferability, yielding ROC-AUC values of 0.6155 for forests and 0.5416 for grasslands. In contrast, temporal split validation showed enhanced generalization, especially for forests (ROC-AUC = 0.6615, PR-AUC = 0.8423). SHAP-based XAI analysis identified key ecosystem-specific drivers: soil organic carbon, tree cover, and Normalized Difference Vegetation Index (NDVI) emerged as the most influential in forests, whereas Land Surface Temperature (LST), elevation, and vegetation health indices were dominant in grasslands. District-level classification revealed that Central Valley and Northern Buttes districts had the highest concentration of high-risk grasslands, while Northern Buttes and North Coast Redwoods dominated forested high-risk areas. This RF-SHAP framework offers a robust, comprehensible, and adaptable method for assessing wildfire risks, enabling informed decisions and creating targeted strategies to mitigate dangers.

</details>


### [245] [MPCM-Net: Multi-scale network integrates partial attention convolution with Mamba for ground-based cloud image segmentation](https://arxiv.org/abs/2511.11681)
*Penghui Niu,Jiashuai She,Taotao Cai,Yajuan Zhang,Ping Zhang,Junhua Gu,Jianxin Li*

Main category: cs.LG

TL;DR: 针对地基云图像分割问题，提出MPCM - Net网络，引入CSRC数据集，实验显示该网络在分割精度和推理速度上表现优越。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习方法在地基云图像分割中有依赖膨胀卷积、忽略精度 - 吞吐量平衡、解码器无法建立全局依赖等局限，需改进。

Method: 提出MPCM - Net网络，编码器含MPAC，含MPC和MPA块；解码器用M2B；还引入CSRC数据集。

Result: 在CSRC上实验表明MPCM - Net优于现有方法，实现分割精度和推理速度的最优平衡。

Conclusion: MPCM - Net能有效提高地基云图像分割的精度和计算效率，CSRC数据集可推动该领域研究。

Abstract: Ground-based cloud image segmentation is a critical research domain for photovoltaic power forecasting. Current deep learning approaches primarily focus on encoder-decoder architectural refinements. However, existing methodologies exhibit several limitations:(1)they rely on dilated convolutions for multi-scale context extraction, lacking the partial feature effectiveness and interoperability of inter-channel;(2)attention-based feature enhancement implementations neglect accuracy-throughput balance; and (3)the decoder modifications fail to establish global interdependencies among hierarchical local features, limiting inference efficiency. To address these challenges, we propose MPCM-Net, a Multi-scale network that integrates Partial attention Convolutions with Mamba architectures to enhance segmentation accuracy and computational efficiency. Specifically, the encoder incorporates MPAC, which comprises:(1)a MPC block with ParCM and ParSM that enables global spatial interaction across multi-scale cloud formations, and (2)a MPA block combining ParAM and ParSM to extract discriminative features with reduced computational complexity. On the decoder side, a M2B is employed to mitigate contextual loss through a SSHD that maintains linear complexity while enabling deep feature aggregation across spatial and scale dimensions. As a key contribution to the community, we also introduce and release a dataset CSRC, which is a clear-label, fine-grained segmentation benchmark designed to overcome the critical limitations of existing public datasets. Extensive experiments on CSRC demonstrate the superior performance of MPCM-Net over state-of-the-art methods, achieving an optimal balance between segmentation accuracy and inference speed. The dataset and source code will be available at https://github.com/she1110/CSRC.

</details>


### [246] [Scientific Data Compression and Super-Resolution Sampling](https://arxiv.org/abs/2511.13675)
*Minh Vu,Andrey Lokhov*

Main category: cs.LG

TL;DR: 提出一种基于学习指数族进展的科学数据压缩和超分辨率新框架，可权衡压缩率和重建保真度。


<details>
  <summary>Details</summary>
Motivation: 现代科学数据量常超存储、处理和分析极限，需数据缩减方法，且科学工作流中数据恢复也很重要。

Method: 引入基于学习指数族进展的科学数据压缩和超分辨率新框架。

Result: 方法能保留和量化感兴趣物理量的不确定性，支持压缩率和重建保真度间的灵活权衡。

Conclusion: 该新框架可有效处理科学数据的压缩和超分辨率问题。

Abstract: Modern scientific simulations, observations, and large-scale experiments generate data at volumes that often exceed the limits of storage, processing, and analysis. This challenge drives the development of data reduction methods that efficiently manage massive datasets while preserving essential physical features and quantities of interest. In many scientific workflows, it is also crucial to enable data recovery from compressed representations - a task known as super-resolution - with guarantees on the preservation of key physical characteristics. A notable example is checkpointing and restarting, which is essential for long-running simulations to recover from failures, resume after interruptions, or examine intermediate results. In this work, we introduce a novel framework for scientific data compression and super-resolution, grounded in recent advances in learning exponential families. Our method preserves and quantifies uncertainty in physical quantities of interest and supports flexible trade-offs between compression ratio and reconstruction fidelity.

</details>


### [247] [Stratified Knowledge-Density Super-Network for Scalable Vision Transformers](https://arxiv.org/abs/2511.11683)
*Longhua Li,Lei Qi,Xin Geng*

Main category: cs.LG

TL;DR: 提出将预训练ViT转换为分层知识密度超网络，引入WPAC和PIAD方法，实验表明有良好效果。


<details>
  <summary>Details</summary>
Motivation: 训练和部署多个不同资源约束的ViT模型成本高且效率低。

Method: 将预训练ViT转换为分层知识密度超网络，引入WPAC集中知识到关键权重，提出PIAD促进知识分层组织。

Result: WPAC在知识集中方面优于现有剪枝标准，与PIAD结合是最先进模型压缩和扩展方法的有力替代。

Conclusion: 所提方法在处理ViT模型资源约束问题上有较好效果。

Abstract: Training and deploying multiple vision transformer (ViT) models for different resource constraints is costly and inefficient. To address this, we propose transforming a pre-trained ViT into a stratified knowledge-density super-network, where knowledge is hierarchically organized across weights. This enables flexible extraction of sub-networks that retain maximal knowledge for varying model sizes. We introduce \textbf{W}eighted \textbf{P}CA for \textbf{A}ttention \textbf{C}ontraction (WPAC), which concentrates knowledge into a compact set of critical weights. WPAC applies token-wise weighted principal component analysis to intermediate features and injects the resulting transformation and inverse matrices into adjacent layers, preserving the original network function while enhancing knowledge compactness. To further promote stratified knowledge organization, we propose \textbf{P}rogressive \textbf{I}mportance-\textbf{A}ware \textbf{D}ropout (PIAD). PIAD progressively evaluates the importance of weight groups, updates an importance-aware dropout list, and trains the super-network under this dropout regime to promote knowledge stratification. Experiments demonstrate that WPAC outperforms existing pruning criteria in knowledge concentration, and the combination with PIAD offers a strong alternative to state-of-the-art model compression and model expansion methods.

</details>


### [248] [A Bayesian Model for Multi-stage Censoring](https://arxiv.org/abs/2511.11684)
*Shuvom Sadhuka,Sophia Lin,Emma Pierson,Bonnie Berger*

Main category: cs.LG

TL;DR: 本文针对医疗顺序决策漏斗结构问题，开发贝叶斯模型，在合成数据和急诊数据集验证其效果，发现医院和ICU入院存在性别差异。


<details>
  <summary>Details</summary>
Motivation: 医疗漏斗结构中真实结果选择性审查会导致风险估计统计偏差，尤其是服务不足患者群体。

Method: 开发针对漏斗决策结构的贝叶斯模型，借鉴选择性标签和审查相关工作。

Result: 在合成数据中模型能更准确恢复真实参数和预测审查患者结果；急诊数据集中发现医院和ICU入院存在性别差异，女性入住ICU的死亡风险阈值更高。

Conclusion: 所开发的贝叶斯模型有效，且医疗入院决策存在性别差异。

Abstract: Many sequential decision settings in healthcare feature funnel structures characterized by a series of stages, such as screenings or evaluations, where the number of patients who advance to each stage progressively decreases and decisions become increasingly costly. For example, an oncologist may first conduct a breast exam, followed by a mammogram for patients with concerning exams, followed by a biopsy for patients with concerning mammograms. A key challenge is that the ground truth outcome, such as the biopsy result, is only revealed at the end of this funnel. The selective censoring of the ground truth can introduce statistical biases in risk estimation, especially in underserved patient groups, whose outcomes are more frequently censored. We develop a Bayesian model for funnel decision structures, drawing from prior work on selective labels and censoring. We first show in synthetic settings that our model is able to recover the true parameters and predict outcomes for censored patients more accurately than baselines. We then apply our model to a dataset of emergency department visits, where in-hospital mortality is observed only for those who are admitted to either the hospital or ICU. We find that there are gender-based differences in hospital and ICU admissions. In particular, our model estimates that the mortality risk threshold to admit women to the ICU is higher for women (5.1%) than for men (4.5%).

</details>


### [249] [On the Fundamental Limits of LLMs at Scale](https://arxiv.org/abs/2511.12869)
*Muhammad Ahmed Mohsin,Muhammad Umer,Ahsan Bilal,Zeeshan Memon,Muhammad Ibtsaam Qadir,Sagnik Bhattacharya,Hassan Rizwan,Abhiram R. Gorle,Maahe Zehra Kazmi,Ayesha Mohsin,Muhammad Usman Rafique,Zihao He,Pulkit Mehta,Muhammad Ali Jamshed,John M. Cioffi*

Main category: cs.LG

TL;DR: 本文指出大语言模型扩展存在五大局限，现有调查缺乏理论综合，提出统一框架形式化其理论上限，并给出实践缓解路径。


<details>
  <summary>Details</summary>
Motivation: 现有调查缺乏将大语言模型扩展现象与计算、信息和学习的基础限制相联系的严格理论综合，本文旨在填补这一空白。

Method: 提出统一的、有证明依据的框架，从可计算性、信息论和统计约束、几何和计算效应等方面分析。

Result: 指出大语言模型扩展在不同方面的帮助、饱和点和无法进展之处。

Conclusion: 为大语言模型扩展提供理论基础和实践缓解路径，如有界预言检索、位置课程和稀疏或分层注意力。

Abstract: Large Language Models (LLMs) have benefited enormously from scaling, yet these gains are bounded by five fundamental limitations: (1) hallucination, (2) context compression, (3) reasoning degradation, (4) retrieval fragility, and (5) multimodal misalignment. While existing surveys describe these phenomena empirically, they lack a rigorous theoretical synthesis connecting them to the foundational limits of computation, information, and learning. This work closes that gap by presenting a unified, proof-informed framework that formalizes the innate theoretical ceilings of LLM scaling. First, computability and uncomputability imply an irreducible residue of error: for any computably enumerable model family, diagonalization guarantees inputs on which some model must fail, and undecidable queries (e.g., halting-style tasks) induce infinite failure sets for all computable predictors. Second, information-theoretic and statistical constraints bound attainable accuracy even on decidable tasks, finite description length enforces compression error, and long-tail factual knowledge requires prohibitive sample complexity. Third, geometric and computational effects compress long contexts far below their nominal size due to positional under-training, encoding attenuation, and softmax crowding. We further show how likelihood-based training favors pattern completion over inference, how retrieval under token limits suffers from semantic drift and coupling noise, and how multimodal scaling inherits shallow cross-modal alignment. Across sections, we pair theorems and empirical evidence to outline where scaling helps, where it saturates, and where it cannot progress, providing both theoretical foundations and practical mitigation paths like bounded-oracle retrieval, positional curricula, and sparse or hierarchical attention.

</details>


### [250] [R-Tuning: Wavelet-Decomposed Replay and Semantic Alignment for Continual Adaptation of Pretrained Time-Series Models](https://arxiv.org/abs/2511.11685)
*Tianyi Yin,Jingwei Wang,Chenze Wang,Han Wang,Jiexuan Cai,Min Liu,Yunlong Ma,Kun Gao,Yuting Song,Weiming Shen*

Main category: cs.LG

TL;DR: 提出Replay Tuning框架解决预训练时间序列模型适应数据分布变化问题，实验显示其在新老任务上均有优势。


<details>
  <summary>Details</summary>
Motivation: 预训练模型适应不断变化的数据分布是挑战，微调新数据易导致灾难性遗忘。

Method: 提出R - Tuning框架，通过频率感知重放策略构建统一潜在空间，利用小波分解增强样本，引入潜在一致性约束。

Result: 在新任务上MAE和MSE最多降低46.9%和46.8%，在老任务上知识保留最多提升5.7%和6.0%，少样本设置下表现优于所有基线。

Conclusion: R - Tuning框架能有效解决预训练时间序列模型的持续适应问题。

Abstract: Pre-trained models have demonstrated exceptional generalization capabilities in time-series forecasting; however, adapting them to evolving data distributions remains a significant challenge. A key hurdle lies in accessing the original training data, as fine-tuning solely on new data often leads to catastrophic forgetting. To address this issue, we propose Replay Tuning (R-Tuning), a novel framework designed for the continual adaptation of pre-trained time-series models. R-Tuning constructs a unified latent space that captures both prior and current task knowledge through a frequency-aware replay strategy. Specifically, it augments model-generated samples via wavelet-based decomposition across multiple frequency bands, generating trend-preserving and fusion-enhanced variants to improve representation diversity and replay efficiency. To further reduce reliance on synthetic samples, R-Tuning introduces a latent consistency constraint that aligns new representations with the prior task space. This constraint guides joint optimization within a compact and semantically coherent latent space, ensuring robust knowledge retention and adaptation. Extensive experimental results demonstrate the superiority of R-Tuning, which reduces MAE and MSE by up to 46.9% and 46.8%, respectively, on new tasks, while preserving prior knowledge with gains of up to 5.7% and 6.0% on old tasks. Notably, under few-shot settings, R-Tuning outperforms all state-of-the-art baselines even when synthetic proxy samples account for only 5% of the new task dataset.

</details>


### [251] [Regularized Schrödinger: Alleviating Distortion and Exposure Bias in Solving Inverse Problems](https://arxiv.org/abs/2511.11686)
*Qing Yao,Lijian Gao,Qirong Mao,Dong Ming*

Main category: cs.LG

TL;DR: 本文提出正则化薛定谔桥（RSB）解决扩散模型逆问题的两个挑战，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在解决逆问题时存在失真 - 感知权衡和暴露偏差问题。

Method: 提出正则化薛定谔桥（RSB），采用正则化训练策略，对输入状态和目标进行扰动。

Result: 在语音增强的两个典型逆问题实验中，RSB 优于现有方法，显著改善失真指标，有效减少暴露偏差。

Conclusion: RSB 能有效解决扩散模型逆问题中的两个关键挑战。

Abstract: Diffusion models serve as a powerful generative framework for solving inverse problems. However, they still face two key challenges: 1) the distortion-perception tradeoff, where improving perceptual quality often degrades reconstruction fidelity, and 2) the exposure bias problem, where the training-inference input mismatch leads to prediction error accumulation and reduced reconstruction quality. In this work, we propose the Regularized Schrödinger Bridge (RSB), an adaptation of Schrödinger Bridge tailored for inverse problems that addresses the above limitations. RSB employs a novel regularized training strategy that perturbs both the input states and targets, effectively mitigating exposure bias by exposing the model to simulated prediction errors and also alleviating distortion by well-designed interpolation via the posterior mean. Extensive experiments on two typical inverse problems for speech enhancement demonstrate that RSB outperforms state-of-the-art methods, significantly improving distortion metrics and effectively reducing exposure bias.

</details>


### [252] [Hierarchical Schedule Optimization for Fast and Robust Diffusion Model Sampling](https://arxiv.org/abs/2511.11688)
*Aihua Zhu,Rui Su,Qinglin Zhao,Li Feng,Meng Shen,Shibo He*

Main category: cs.LG

TL;DR: 扩散概率模型生成效果好但采样慢，Schedule Optimization可加速，现有范式难满足核心原则，本文提出HSO框架，实验显示其在低NFE下表现优异且成本低。


<details>
  <summary>Details</summary>
Motivation: 现有Schedule Optimization范式难以同时满足有效性、适应性、实用鲁棒性和计算效率四个核心原则，需要更先进的解决方案。

Method: 提出Hierarchical - Schedule - Optimizer (HSO) 双级优化框架，交替进行上层全局搜索和下层局部优化，采用Midpoint Error Proxy (MEP) 和Spacing - Penalized Fitness (SPF) 函数。

Result: HSO在极低NFE下为免训练采样设定了新的最优水平，如NFE为5时在LAION - Aesthetics上用Stable Diffusion v2.1取得FID为11.94的结果，且优化成本不到8秒。

Conclusion: HSO是一种实用且高效的扩散模型加速范式。

Abstract: Diffusion probabilistic models have set a new standard for generative fidelity but are hindered by a slow iterative sampling process. A powerful training-free strategy to accelerate this process is Schedule Optimization, which aims to find an optimal distribution of timesteps for a fixed and small Number of Function Evaluations (NFE) to maximize sample quality. To this end, a successful schedule optimization method must adhere to four core principles: effectiveness, adaptivity, practical robustness, and computational efficiency. However, existing paradigms struggle to satisfy these principles simultaneously, motivating the need for a more advanced solution. To overcome these limitations, we propose the Hierarchical-Schedule-Optimizer (HSO), a novel and efficient bi-level optimization framework. HSO reframes the search for a globally optimal schedule into a more tractable problem by iteratively alternating between two synergistic levels: an upper-level global search for an optimal initialization strategy and a lower-level local optimization for schedule refinement. This process is guided by two key innovations: the Midpoint Error Proxy (MEP), a solver-agnostic and numerically stable objective for effective local optimization, and the Spacing-Penalized Fitness (SPF) function, which ensures practical robustness by penalizing pathologically close timesteps. Extensive experiments show that HSO sets a new state-of-the-art for training-free sampling in the extremely low-NFE regime. For instance, with an NFE of just 5, HSO achieves a remarkable FID of 11.94 on LAION-Aesthetics with Stable Diffusion v2.1. Crucially, this level of performance is attained not through costly retraining, but with a one-time optimization cost of less than 8 seconds, presenting a highly practical and efficient paradigm for diffusion model acceleration.

</details>


### [253] [Doubly Debiased Test-Time Prompt Tuning for Vision-Language Models](https://arxiv.org/abs/2511.11690)
*Fei Song,Yi Li,Rui Wang,Jiahuan Zhou,Changwen Zheng,Jiangmeng Li*

Main category: cs.LG

TL;DR: 现有测试时提示调优存在优化偏差问题，本文分析原因并提出双去偏方法，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有仅基于无标签测试数据调优可学习提示会导致提示优化偏差，影响下游任务性能。

Method: 提出双去偏测试时提示调优方法，包括动态检索增强调制模块和可靠性感知提示优化模块。

Result: 在15个基准数据集上的实验表明该方法优于基线。

Conclusion: 所提方法能有效缓解提示优化偏差。

Abstract: Test-time prompt tuning for vision-language models has demonstrated impressive generalization capabilities under zero-shot settings. However, tuning the learnable prompts solely based on unlabeled test data may induce prompt optimization bias, ultimately leading to suboptimal performance on downstream tasks. In this work, we analyze the underlying causes of prompt optimization bias from both the model and data perspectives. In terms of the model, the entropy minimization objective typically focuses on reducing the entropy of model predictions while overlooking their correctness. This can result in overconfident yet incorrect outputs, thereby compromising the quality of prompt optimization. On the data side, prompts affected by optimization bias can introduce misalignment between visual and textual modalities, which further aggravates the prompt optimization bias. To this end, we propose a Doubly Debiased Test-Time Prompt Tuning method. Specifically, we first introduce a dynamic retrieval-augmented modulation module that retrieves high-confidence knowledge from a dynamic knowledge base using the test image feature as a query, and uses the retrieved knowledge to modulate the predictions. Guided by the refined predictions, we further develop a reliability-aware prompt optimization module that incorporates a confidence-based weighted ensemble and cross-modal consistency distillation to impose regularization constraints during prompt tuning. Extensive experiments across 15 benchmark datasets involving both natural distribution shifts and cross-datasets generalization demonstrate that our method outperforms baselines, validating its effectiveness in mitigating prompt optimization bias.

</details>


### [254] [Beyond saliency: enhancing explanation of speech emotion recognition with expert-referenced acoustic cues](https://arxiv.org/abs/2511.11691)
*Seham Nasr,Zhao Ren,David Johnson*

Main category: cs.LG

TL;DR: 提出框架克服当前基于显著性的语音情感识别可解释AI方法的局限，在基准数据集实验中提升了解释质量。


<details>
  <summary>Details</summary>
Motivation: 当前基于显著性的语音情感识别可解释AI方法无法表明突出区域是否对应有意义的情感声学标记，限制了忠实性和可解释性。

Method: 提出一个框架，量化显著区域内线索的大小，将显著性与专家参考的语音情感声学线索联系起来。

Result: 在基准语音情感识别数据集上的实验表明，该方法通过将显著区域与理论驱动的语音情感专家参考声学联系起来，提高了解释质量。

Conclusion: 与标准显著性方法相比，该方法为语音情感识别模型提供了更易理解和合理的解释，是迈向可信的基于语音的情感计算的基础一步。

Abstract: Explainable AI (XAI) for Speech Emotion Recognition (SER) is critical for building transparent, trustworthy models. Current saliency-based methods, adapted from vision, highlight spectrogram regions but fail to show whether these regions correspond to meaningful acoustic markers of emotion, limiting faithfulness and interpretability. We propose a framework that overcomes these limitations by quantifying the magnitudes of cues within salient regions. This clarifies "what" is highlighted and connects it to "why" it matters, linking saliency to expert-referenced acoustic cues of speech emotions. Experiments on benchmark SER datasets show that our approach improves explanation quality by explicitly linking salient regions to theory-driven speech emotions expert-referenced acoustics. Compared to standard saliency methods, it provides more understandable and plausible explanations of SER models, offering a foundational step towards trustworthy speech-based affective computing.

</details>


### [255] [AnchorDS: Anchoring Dynamic Sources for Semantically Consistent Text-to-3D Generation](https://arxiv.org/abs/2511.11692)
*Jiayin Zhu,Linlin Yang,Yicong Li,Angela Yao*

Main category: cs.LG

TL;DR: 本文指出基于优化的文本到3D方法存在语义过度平滑问题，提出AnchorDS改进分数蒸馏机制，实验表明其在质量和效率上超越先前方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于优化的文本到3D方法将2D生成模型的指导视为静态，忽略源动态会导致语义过度平滑问题。

Method: 将文本到3D优化重新表述为将动态源分布映射到固定目标分布，引入AnchorDS机制，结合图像条件提供状态锚定指导，同时设计过滤和微调策略。

Result: AnchorDS生成的结果具有更细粒度细节、更自然颜色和更强语义一致性，尤其对于复杂提示。

Conclusion: 所提出的方法在质量和效率上均超越先前方法。

Abstract: Optimization-based text-to-3D methods distill guidance from 2D generative models via Score Distillation Sampling (SDS), but implicitly treat this guidance as static. This work shows that ignoring source dynamics yields inconsistent trajectories that suppress or merge semantic cues, leading to "semantic over-smoothing" artifacts. As such, we reformulate text-to-3D optimization as mapping a dynamically evolving source distribution to a fixed target distribution. We cast the problem into a dual-conditioned latent space, conditioned on both the text prompt and the intermediately rendered image. Given this joint setup, we observe that the image condition naturally anchors the current source distribution. Building on this insight, we introduce AnchorDS, an improved score distillation mechanism that provides state-anchored guidance with image conditions and stabilizes generation. We further penalize erroneous source estimates and design a lightweight filter strategy and fine-tuning strategy that refines the anchor with negligible overhead. AnchorDS produces finer-grained detail, more natural colours, and stronger semantic consistency, particularly for complex prompts, while maintaining efficiency. Extensive experiments show that our method surpasses previous methods in both quality and efficiency.

</details>


### [256] [Toward Dignity-Aware AI: Next-Generation Elderly Monitoring from Fall Detection to ADL](https://arxiv.org/abs/2511.11696)
*Xun Shao,Aoba Otani,Yuto Hirasuka,Runji Cai,Seng W. Loke*

Main category: cs.LG

TL;DR: 本文设想下一代老年监测系统从跌倒检测转向日常活动识别，用SISFall数据集实验证明可行性，报告联邦学习结果，指出挑战并给出方向。


<details>
  <summary>Details</summary>
Motivation: 设计隐私保护、边缘部署和联邦式AI系统，以支持老年社会的独立性和尊严，实现日常活动识别。

Method: 使用SISFall数据集及其GAN增强变体进行实验，进行非IID条件下的联邦学习和在Jetson Orin Nano设备上的嵌入式部署。

Result: 得到非IID条件下联邦学习的初步结果和嵌入式部署结果。

Conclusion: 强调从单任务检测向全面日常活动识别的转变，为可持续和以人为本的老年护理AI提供早期证据和路线图。

Abstract: This position paper envisions a next-generation elderly monitoring system that moves beyond fall detection toward the broader goal of Activities of Daily Living (ADL) recognition. Our ultimate aim is to design privacy-preserving, edge-deployed, and federated AI systems that can robustly detect and understand daily routines, supporting independence and dignity in aging societies. At present, ADL-specific datasets are still under collection. As a preliminary step, we demonstrate feasibility through experiments using the SISFall dataset and its GAN-augmented variants, treating fall detection as a proxy task. We report initial results on federated learning with non-IID conditions, and embedded deployment on Jetson Orin Nano devices. We then outline open challenges such as domain shift, data scarcity, and privacy risks, and propose directions toward full ADL monitoring in smart-room environments. This work highlights the transition from single-task detection to comprehensive daily activity recognition, providing both early evidence and a roadmap for sustainable and human-centered elderly care AI.

</details>


### [257] [Benchmarking GNNs for OOD Materials Property Prediction with Uncertainty Quantification](https://arxiv.org/abs/2511.11697)
*Liqin Tan,Pin Chen,Menghan Liu,Xiean Wang,Jianhuan Cen,Qingsong Zou*

Main category: cs.LG

TL;DR: 提出MatUQ框架用于评估图神经网络在材料属性OOD预测与UQ的表现，评估12种模型，有新发现和指标。


<details>
  <summary>Details</summary>
Motivation: 构建一个基准框架来评估图神经网络在材料属性OOD预测及UQ方面的性能。

Method: 构建MatUQ框架，含1375个OOD任务，采用新分割策略SOAP - LOCO；用统一不确定性训练协议评估12种模型，引入新指标D - EviU。

Result: 不确定性训练方法显著提升模型预测精度，误差平均降低70.6%；无单一模型表现全面占优。

Conclusion: 为材料发现中分布变化下选择可靠模型提供实用见解。

Abstract: We present MatUQ, a benchmark framework for evaluating graph neural networks (GNNs) on out-of-distribution (OOD) materials property prediction with uncertainty quantification (UQ). MatUQ comprises 1,375 OOD prediction tasks constructed from six materials datasets using five OFM-based and a newly proposed structure-aware splitting strategy, SOAP-LOCO, which captures local atomic environments more effectively. We evaluate 12 representative GNN models under a unified uncertainty-aware training protocol that combines Monte Carlo Dropout and Deep Evidential Regression (DER), and introduce a novel uncertainty metric, D-EviU, which shows the strongest correlation with prediction errors in most tasks. Our experiments yield two key findings. First, the uncertainty-aware training approach significantly improves model prediction accuracy, reducing errors by an average of 70.6\% across challenging OOD scenarios. Second, the benchmark reveals that no single model dominates universally: earlier models such as SchNet and ALIGNN remain competitive, while newer models like CrystalFramer and SODNet demonstrate superior performance on specific material properties. These results provide practical insights for selecting reliable models under distribution shifts in materials discovery.

</details>


### [258] [Moirai 2.0: When Less Is More for Time Series Forecasting](https://arxiv.org/abs/2511.11698)
*Chenghao Liu,Taha Aksu,Juncheng Liu,Xu Liu,Hanshu Yan,Quang Pham,Doyen Sahoo,Caiming Xiong,Silvio Savarese,Junnan Li*

Main category: cs.LG

TL;DR: 介绍Moirai 2.0时间序列基础模型，在准确性、速度和模型大小上有良好平衡，优于Moirai 1.0及同系列大模型，指出后续研究方向并开源代码。


<details>
  <summary>Details</summary>
Motivation: 改进时间序列模型在准确性、速度和模型大小上的表现，探索模型优化方向。

Method: 在3600万序列新语料库上训练，采用分位数预测和多令牌预测，使用更简单的仅解码器架构、单补丁和分位数损失。

Result: 在Gift - Eval基准测试中排名靠前，比Moirai 1.0更快、更小且性能更好，优于同系列大模型，有稳健的领域级结果。

Conclusion: 模型性能随参数增加趋于平稳且长时预测效果下降，需开展数据扩展和长时建模的未来研究。

Abstract: We introduce Moirai 2.0, a decoder-only time-series foundation model trained on a new corpus of 36M series. The model adopts quantile forecasting and multi-token prediction, improving both probabilistic accuracy and inference efficiency. On the Gift-Eval benchmark, it ranks among the top pretrained models while achieving a strong trade-off between accuracy, speed, and model size. Compared to Moirai 1.0, Moirai 2.0 replaces masked-encoder training, multi-patch inputs, and mixture-distribution outputs with a simpler decoder-only architecture, single patch, and quantile loss. Ablation studies isolate these changes -- showing that the decoder-only backbone along with recursive multi-quantile decoding contribute most to the gains. Additional experiments show that Moirai 2.0 outperforms larger models from the same family and exhibits robust domain-level results. In terms of efficiency and model size, Moirai 2.0 is twice as fast and thirty times smaller than its prior best version, Moirai 1.0-Large, while also performing better. Model performance plateaus with increasing parameter count and declines at longer horizons, motivating future work on data scaling and long-horizon modeling. We release code and evaluation details to support further research.

</details>


### [259] [Tighter Truncated Rectangular Prism Approximation for RNN Robustness Verification](https://arxiv.org/abs/2511.11699)
*Xingqi Lin,Liangyu Chen,Min Wu,Min Zhang,Zhenbing Zeng*

Main category: cs.LG

TL;DR: 本文提出新的截断长方体近似方法及优化驱动方法，实现RNN鲁棒性验证工具DeepPrism，实验显示其在多任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法对非线性部分单独用线性边界平面近似会导致显著高估和验证精度降低，需要更紧密的近似方法。

Method: 提出由两个线性松弛平面形成的截断长方体来紧密包围Hadamard积生成的三维非线性表面，并用优化驱动方法最小化其体积和表面积以实现更紧密的过近似，基于此实现DeepPrism。

Result: 实验表明DeepPrism在图像分类、语音识别和情感分析等各种任务中与现有方法相比有显著改进。

Conclusion: 所提出的近似方法和实现的DeepPrism在RNN鲁棒性验证上有效，优于现有技术。

Abstract: Robustness verification is a promising technique for rigorously proving Recurrent Neural Networks (RNNs) robustly. A key challenge is to over-approximate the nonlinear activation functions with linear constraints, which can transform the verification problem into an efficiently solvable linear programming problem. Existing methods over-approximate the nonlinear parts with linear bounding planes individually, which may cause significant over-estimation and lead to lower verification accuracy. In this paper, in order to tightly enclose the three-dimensional nonlinear surface generated by the Hadamard product, we propose a novel truncated rectangular prism formed by two linear relaxation planes and a refinement-driven method to minimize both its volume and surface area for tighter over-approximation. Based on this approximation, we implement a prototype DeepPrism for RNN robustness verification. The experimental results demonstrate that \emph{DeepPrism} has significant improvement compared with the state-of-the-art approaches in various tasks of image classification, speech recognition and sentiment analysis.

</details>


### [260] [Bayesian Neural Networks with Monte Carlo Dropout for Probabilistic Electricity Price Forecasting](https://arxiv.org/abs/2511.11701)
*Abhinav Das,Stephan Schlüter*

Main category: cs.LG

TL;DR: 本文提出用带蒙特卡罗丢弃法的贝叶斯神经网络进行概率电价预测的框架，模型优于基准模型，为能源市场预测提供参考。


<details>
  <summary>Details</summary>
Motivation: 传统点预测难以捕捉电价预测中的不确定性，无法满足风险管理需求。

Method: 使用带蒙特卡罗丢弃法的贝叶斯神经网络，为每天的每个小时训练单独模型以捕捉昼夜模式。

Result: 所提模型在点预测和区间预测方面优于广义自回归条件异方差外生变量模型和套索估计自回归模型。

Conclusion: 本研究可作为在能源市场预测中利用概率神经网络模型的参考。

Abstract: Accurate electricity price forecasting is critical for strategic decision-making in deregulated electricity markets, where volatility stems from complex supply-demand dynamics and external factors. Traditional point forecasts often fail to capture inherent uncertainties, limiting their utility for risk management. This work presents a framework for probabilistic electricity price forecasting using Bayesian neural networks (BNNs) with Monte Carlo (MC) dropout, training separate models for each hour of the day to capture diurnal patterns. A critical assessment and comparison with the benchmark model, namely: generalized autoregressive conditional heteroskedasticity with exogenous variable (GARCHX) model and the LASSO estimated auto-regressive model (LEAR), highlights that the proposed model outperforms the benchmark models in terms of point prediction and intervals. This work serves as a reference for leveraging probabilistic neural models in energy market predictions.

</details>


### [261] [Enhancing Reinforcement Learning in 3D Environments through Semantic Segmentation: A Case Study in ViZDoom](https://arxiv.org/abs/2511.11703)
*Hugo Huang*

Main category: cs.LG

TL;DR: 提出SS - only和RGB+SS两种新输入表示应对3D环境强化学习挑战，实验显示SS - only可大幅降低内存消耗，RGB+SS提升性能，还探索热图工具并与先前方法对比。


<details>
  <summary>Details</summary>
Motivation: 解决3D环境高维感官输入强化学习中内存消耗高和部分可观测马尔可夫决策过程学习复杂的问题。

Method: 提出SS - only和RGB+SS两种采用语义分割的输入表示，在ViZDoom死亡竞赛中实验，用完美分割结果评估，探索密度热图工具。

Result: SS - only至少降低66.6%内存消耗，使用无损压缩技术最高降98.6%；RGB+SS增强代理性能；探索热图用于可视化和评估数据收集适用性。

Conclusion: 方法克服了3D环境应用语义分割的常见问题。

Abstract: Reinforcement learning (RL) in 3D environments with high-dimensional sensory input poses two major challenges: (1) the high memory consumption induced by memory buffers required to stabilise learning, and (2) the complexity of learning in partially observable Markov Decision Processes (POMDPs). This project addresses these challenges by proposing two novel input representations: SS-only and RGB+SS, both employing semantic segmentation on RGB colour images. Experiments were conducted in deathmatches of ViZDoom, utilizing perfect segmentation results for controlled evaluation. Our results showed that SS-only was able to reduce the memory consumption of memory buffers by at least 66.6%, and up to 98.6% when a vectorisable lossless compression technique with minimal overhead such as run-length encoding is applied. Meanwhile, RGB+SS significantly enhances RL agents' performance with the additional semantic information provided. Furthermore, we explored density-based heatmapping as a tool to visualise RL agents' movement patterns and evaluate their suitability for data collection. A brief comparison with a previous approach highlights how our method overcame common pitfalls in applying semantic segmentation in 3D environments like ViZDoom.

</details>


### [262] [Simple Vision-Language Math Reasoning via Rendered Text](https://arxiv.org/abs/2511.11704)
*Matvey Skripkin,Elizaveta Goncharova,Andrey Kuznetsov*

Main category: cs.LG

TL;DR: 提出轻量级管道训练视觉语言模型解数学问题，效果超现有求解器。


<details>
  <summary>Details</summary>
Motivation: 开发有效方法训练视觉语言模型解决数学问题。

Method: 将LaTeX编码方程渲染成图像并与结构化思维链提示配对。

Result: 简单文本到视觉增强使紧凑多模态架构达到最先进推理准确率；在广泛使用的基准测试中匹配或超越现有求解器，在MMMU、ChartQA和DocVQA等任务上有高达20%的提升。

Conclusion: 渲染保真度和提示设计是性能的主要驱动因素，此方法简单且有效。

Abstract: We present a lightweight yet effective pipeline for training vision-language models to solve math problems by rendering LaTeX encoded equations into images and pairing them with structured chain-of-thought prompts. This simple text-to-vision augmentation enables compact multimodal architectures to achieve state-of-the-art reasoning accuracy. Through systematic ablations, we find that rendering fidelity and prompt design are the primary drivers of performance. Despite its simplicity, our approach consistently matches or surpasses both open-source and proprietary math-focused vision-language solvers on widely used benchmarks, while preserving broad general-domain competence - showing gains on tasks such as MMMU, ChartQA, and DocVQA of up to 20%.

</details>


### [263] [Multimodal ML: Quantifying the Improvement of Calorie Estimation Through Image-Text Pairs](https://arxiv.org/abs/2511.11705)
*Arya Narang*

Main category: cs.LG

TL;DR: 研究短文本输入（菜品名称）对卡路里估计的提升效果，用多模态CNN模型使MAE降低1.06 kcal。


<details>
  <summary>Details</summary>
Motivation: 确定短文本输入相比仅图像基线模型对卡路里估计的提升程度及提升是否显著。

Method: 利用TensorFlow库和Nutrition5k数据集，训练仅图像CNN和接受文本与图像输入的多模态CNN。

Result: 使用多模态模型时，卡路里估计的平均绝对误差（MAE）从84.76 kcal降至83.70 kcal，降低了1.06 kcal，提升1.25%。

Conclusion: 未提及明确结论，但可推测短文本输入对卡路里估计有一定提升。

Abstract: This paper determines the extent to which short textual inputs (in this case, names of dishes) can improve calorie estimation compared to an image-only baseline model and whether any improvements are statistically significant. Utilizes the TensorFlow library and the Nutrition5k dataset (curated by Google) to train both an image-only CNN and multimodal CNN that accepts both text and an image as input. The MAE of calorie estimations was reduced by 1.06 kcal from 84.76 kcal to 83.70 kcal (1.25% improvement) when using the multimodal model.

</details>


### [264] [Context-Aware Multimodal Representation Learning for Spatio-Temporally Explicit Environmental modelling](https://arxiv.org/abs/2511.11706)
*Julia Peters,Karin Mora,Miguel D. Mahecha,Chaonan Ji,David Montero,Clemens Mosig,Guido Kraemer*

Main category: cs.LG

TL;DR: 提出一种高时空分辨率表征学习框架，整合不同地球观测模态，在环境应用中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有地球观测基础模型在固定时空尺度运行，无法满足生态分析对精细空间细节和高时间保真度的要求。

Method: 以Sentinel - 1和Sentinel - 2数据为例，先独立建模各传感器，再将表示组合到共享模型，采用两阶段设计。

Result: 定性分析显示学习的嵌入在异质景观中具有高空间和语义一致性；定量评估表明其编码了生态有意义的模式并保留足够时间保真度。

Conclusion: 该框架为需要不同时空分辨率的环境应用提供了灵活、可用于分析的表征学习方法。

Abstract: Earth observation (EO) foundation models have emerged as an effective approach to derive latent representations of the Earth system from various remote sensing sensors. These models produce embeddings that can be used as analysis-ready datasets, enabling the modelling of ecosystem dynamics without extensive sensor-specific preprocessing. However, existing models typically operate at fixed spatial or temporal scales, limiting their use for ecological analyses that require both fine spatial detail and high temporal fidelity. To overcome these limitations, we propose a representation learning framework that integrates different EO modalities into a unified feature space at high spatio-temporal resolution. We introduce the framework using Sentinel-1 and Sentinel-2 data as representative modalities. Our approach produces a latent space at native 10 m resolution and the temporal frequency of cloud-free Sentinel-2 acquisitions. Each sensor is first modeled independently to capture its sensor-specific characteristics. Their representations are then combined into a shared model. This two-stage design enables modality-specific optimisation and easy extension to new sensors, retaining pretrained encoders while retraining only fusion layers. This enables the model to capture complementary remote sensing data and to preserve coherence across space and time. Qualitative analyses reveal that the learned embeddings exhibit high spatial and semantic consistency across heterogeneous landscapes. Quantitative evaluation in modelling Gross Primary Production reveals that they encode ecologically meaningful patterns and retain sufficient temporal fidelity to support fine-scale analyses. Overall, the proposed framework provides a flexible, analysis-ready representation learning approach for environmental applications requiring diverse spatial and temporal resolutions.

</details>


### [265] [FSC-Net: Fast-Slow Consolidation Networks for Continual Learning](https://arxiv.org/abs/2511.11707)
*Mohamed El Gorrim*

Main category: cs.LG

TL;DR: 针对持续学习中的灾难性遗忘问题，提出FSC - Net，实验表明双时间尺度整合机制对缓解遗忘更关键。


<details>
  <summary>Details</summary>
Motivation: 解决持续学习中的灾难性遗忘问题，即神经网络学习新任务时丢失旧知识的问题。

Method: 提出FSC - Net，采用双网络架构，快速网络NN1适应新任务，慢速网络NN2通过蒸馏和重放巩固知识。

Result: 在Split - MNIST和Split - CIFAR - 10上，FSC - Net相比快速网络有显著提升，但在Split - CIFAR - 10上绝对性能仍有待提高。

Conclusion: 双时间尺度整合机制比架构复杂性更能缓解灾难性遗忘。

Abstract: Continual learning remains challenging due to catastrophic forgetting, where neural networks lose previously acquired knowledge when learning new tasks. Inspired by memory consolidation in neuroscience, we propose FSC-Net (Fast-Slow Consolidation Networks), a dual-network architecture that separates rapid task learning from gradual knowledge consolidation. Our method employs a fast network (NN1) for immediate adaptation to new tasks and a slow network (NN2) that consolidates knowledge through distillation and replay. Within the family of MLP-based NN1 variants we evaluated, consolidation effectiveness is driven more by methodology than architectural embellishments -- a simple MLP outperforms more complex similarity-gated variants by 1.2pp. Through systematic hyperparameter analysis, we observed empirically that pure replay without distillation during consolidation achieves superior performance, consistent with the hypothesis that distillation from the fast network introduces recency bias. On Split-MNIST (30 seeds), FSC-Net achieves 91.71% +/- 0.62% retention accuracy, a +4.27pp gain over the fast network alone (87.43% +/- 1.27%, paired t=23.585, p < 1e-10). On Split-CIFAR-10 (5 seeds), our method achieves 33.31% +/- 0.38% retention with an +8.20pp gain over the fast network alone (25.11% +/- 1.61%, paired t=9.75, p < 1e-3), demonstrating +8.20pp gain, though absolute performance (33.31%) remains modest and below random expectation, highlighting need for stronger backbones. Our results provide empirical evidence that the dual-timescale consolidation mechanism, rather than architectural complexity, is central to mitigating catastrophic forgetting in this setting.

</details>


### [266] [Which Sparse Autoencoder Features Are Real? Model-X Knockoffs for False Discovery Rate Control](https://arxiv.org/abs/2511.11711)
*Tsogt-Ochir Enkhbayar*

Main category: cs.LG

TL;DR: 引入Model - X knockoffs进行SAE特征选择，分析Pythia - 70M用于情感分类的SAE潜在变量，实现可靠特征发现。


<details>
  <summary>Details</summary>
Motivation: 解决SAE难以区分真实计算模式和错误相关性的问题。

Method: 将Model - X knockoffs引入SAE特征选择，用knock - off+控制FDR，借助高斯替代处理潜在分布。

Result: 分析512个高活动SAE潜在变量，在目标FDR q = 0.1时选择129个特征，所选特征的knockoff统计量与未选特征有5.40倍的分离。

Conclusion: 该方法结合SAEs和多测试感知推理，为可靠特征发现提供可重现和有原则的框架，推动机械可解释性基础发展。

Abstract: Although sparse autoencoders (SAEs) are crucial for identifying interpretable features in neural networks, it is still challenging to distinguish between real computational patterns and erroneous correlations. We introduce Model-X knockoffs to SAE feature selection, using knock-off+ to control the false discovery rate (FDR) with finite-sample guarantees under the standard Model-X assumptions (in our case, via a Gaussian surrogate for the latent distribution). We select 129 features at a target FDR q=0.1 after analyzing 512 high-activity SAE latents for sentiment classification using Pythia-70M. About 25% of the latents under examination carry task-relevant signal, whereas 75% do not, according to the chosen set, which displays a 5.40x separation in knockoff statistics compared to non-selected features. Our method offers a re-producible and principled framework for reliable feature discovery by combining SAEs with multiple-testing-aware inference, advancing the foundations of mechanistic interpretability.

</details>


### [267] [Reasoning: From Reflection to Solution](https://arxiv.org/abs/2511.11712)
*Zixi Li*

Main category: cs.LG

TL;DR: 探讨大语言模型是否学会推理，提出推理是状态空间中迭代算子应用至不动点的定义，给出解决方案OpenLM并取得不错效果。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型取得优异成绩的背景下，探究其是学会推理还是模式匹配，理解推理所需条件并构建相应架构。

Method: 从谜题OpenXOR入手，经过理论OpenOperator，最终得出解决方案OpenLM。

Result: OpenLM在当前最先进大语言模型准确率为0%的情况下达到76%的准确率。

Conclusion: 推理是状态空间中迭代算子应用至不动点，该定义有具体架构意义，能解释现有系统的失败及实现真正推理能力的途径。

Abstract: What is reasoning? This question has driven centuries of philosophical inquiry, from Aristotle's syllogisms to modern computational complexity theory. In the age of large language models achieving superhuman performance on benchmarks like GSM8K (95\% accuracy) and HumanEval (90\% pass@1), we must ask: have these systems learned to \emph{reason}, or have they learned to \emph{pattern-match over reasoning traces}?
  This paper argues for a specific answer: \textbf{reasoning is iterative operator application in state spaces, converging to fixed points}. This definition is not merely philosophical -- it has concrete architectural implications that explain both the failures of current systems and the path to genuine reasoning capabilities.
  Our investigation begins with a puzzle (OpenXOR), progresses through theory (OpenOperator), and culminates in a working solution (OpenLM) that achieves 76\% accuracy where state-of-the-art LLMs achieve 0\%. This is not about criticizing existing systems, but about \emph{understanding what reasoning requires} and \emph{building architectures that provide it}.

</details>


### [268] [Multiscale Grassmann Manifolds for Single-Cell Data Analysis](https://arxiv.org/abs/2511.11717)
*Xiang Xiang Wang,Sean Cottrell,Guo-Wei Wei*

Main category: cs.LG

TL;DR: 提出基于Grassmann流形的多尺度框架用于单细胞数据分析，实验表明该方法有效保留结构并提供稳定聚类性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法用欧几里得空间向量表示细胞，难以捕捉内在相关性和多尺度几何结构，需要新方法进行单细胞数据分析。

Method: 提出基于Grassmann流形的多尺度框架，在多个表示尺度下生成嵌入，将不同几何视图特征整合到统一的Grassmann流形，引入基于幂的尺度采样函数控制尺度选择和平衡信息。

Result: 在九个基准单细胞RNA测序数据集上实验，该方法有效保留有意义结构，提供稳定聚类性能，尤其适用于中小规模数据集。

Conclusion: Grassmann流形为单细胞数据分析提供了连贯且有信息的基础。

Abstract: Single-cell data analysis seeks to characterize cellular heterogeneity based on high-dimensional gene expression profiles. Conventional approaches represent each cell as a vector in Euclidean space, which limits their ability to capture intrinsic correlations and multiscale geometric structures. We propose a multiscale framework based on Grassmann manifolds that integrates machine learning with subspace geometry for single-cell data analysis. By generating embeddings under multiple representation scales, the framework combines their features from different geometric views into a unified Grassmann manifold. A power-based scale sampling function is introduced to control the selection of scales and balance in- formation across resolutions. Experiments on nine benchmark single-cell RNA-seq datasets demonstrate that the proposed approach effectively preserves meaningful structures and provides stable clustering performance, particularly for small to medium-sized datasets. These results suggest that Grassmann manifolds offer a coherent and informative foundation for analyzing single cell data.

</details>


### [269] [Fast 3D Surrogate Modeling for Data Center Thermal Management](https://arxiv.org/abs/2511.11722)
*Soumyendu Sarkar,Antonio Guillen-Perez,Zachariah J Carmichael,Avisek Naug,Refik Mert Cam,Vineet Gundecha,Ashwin Ramesh Babu,Sahand Ghorbanpour,Ricardo Luna Gutierrez*

Main category: cs.LG

TL;DR: 本文开发基于视觉的代理建模框架预测数据中心温度，评估多种架构，模型加速显著且能实现节能减碳。


<details>
  <summary>Details</summary>
Motivation: 传统热CFD求解器计算成本高，无法实时使用，需开发新方法实现数据中心实时温度预测以降低能耗和碳排放。

Method: 开发基于视觉的代理建模框架，直接处理数据中心3D体素化表示，评估3D CNN U - Net变体、3D傅里叶神经算子和3D视觉变换器等架构。

Result: 代理模型可跨数据中心配置泛化，实现高达20000倍加速，能快速准确估计热点和温度分布。

Conclusion: 该方法可实现实时冷却控制和工作负载重新分配，带来7%的能源节省和碳足迹降低。

Abstract: Reducing energy consumption and carbon emissions in data centers by enabling real-time temperature prediction is critical for sustainability and operational efficiency. Achieving this requires accurate modeling of the 3D temperature field to capture airflow dynamics and thermal interactions under varying operating conditions. Traditional thermal CFD solvers, while accurate, are computationally expensive and require expert-crafted meshes and boundary conditions, making them impractical for real-time use. To address these limitations, we develop a vision-based surrogate modeling framework that operates directly on a 3D voxelized representation of the data center, incorporating server workloads, fan speeds, and HVAC temperature set points. We evaluate multiple architectures, including 3D CNN U-Net variants, a 3D Fourier Neural Operator, and 3D vision transformers, to map these thermal inputs to high-fidelity heat maps. Our results show that the surrogate models generalize across data center configurations and achieve up to 20,000x speedup (hundreds of milliseconds vs. hours). This fast and accurate estimation of hot spots and temperature distribution enables real-time cooling control and workload redistribution, leading to substantial energy savings (7\%) and reduced carbon footprint.

</details>


### [270] [Optimizing Input of Denoising Score Matching is Biased Towards Higher Score Norm](https://arxiv.org/abs/2511.11727)
*Tongda Xu*

Main category: cs.LG

TL;DR: 论文指出用去噪分数匹配优化扩散模型条件输入会打破与精确分数匹配的等价性，导致分数范数升高，在优化数据分布时也有类似偏差，并提及受影响的不同领域工作。


<details>
  <summary>Details</summary>
Motivation: 研究用去噪分数匹配优化扩散模型条件输入所带来的影响。

Method: 分析去噪分数匹配优化过程，观察优化数据分布时的情况。

Result: 优化打破了去噪分数匹配与精确分数匹配的等价性，导致更高的分数范数，在优化数据分布时也有类似偏差。

Conclusion: 不同领域的许多工作如MAR、PerCo、DreamFusion等受到这种偏差影响。

Abstract: Many recent works utilize denoising score matching to optimize the conditional input of diffusion models. In this workshop paper, we demonstrate that such optimization breaks the equivalence between denoising score matching and exact score matching. Furthermore, we show that this bias leads to higher score norm. Additionally, we observe a similar bias when optimizing the data distribution using a pre-trained diffusion model. Finally, we discuss the wide range of works across different domains that are affected by this bias, including MAR for auto-regressive generation, PerCo for image compression, and DreamFusion for text to 3D generation.

</details>


### [271] [Physics-Informed Neural ODEs with Scale-Aware Residuals for Learning Stiff Biophysical Dynamics](https://arxiv.org/abs/2511.11734)
*Kamalpreet Singh Kainth,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedat Panat*

Main category: cs.LG

TL;DR: 提出PI - NODE - SR框架解决神经微分方程预测刚性生物物理系统不可靠问题，在Hodgkin - Huxley方程上表现良好，能减少长时误差。


<details>
  <summary>Details</summary>
Motivation: 标准神经ODE和物理信息变体预测刚性生物物理系统不可靠，需更多迭代且可能收敛到次优解。

Method: 引入结合低阶显式求解器（Heun方法）和残差归一化的PI - NODE - SR框架。

Result: 在Hodgkin - Huxley方程上，能从单振荡学习并外推超100ms，捕捉频率和振幅，恢复形态特征。

Conclusion: 虽性能对初始化敏感，但PI - NODE - SR相对基线方法能减少长时误差，为稳定高效学习刚性生物动力学提供途径。

Abstract: Neural differential equations offer a powerful framework for modeling continuous-time dynamics, but forecasting stiff biophysical systems remains unreliable. Standard Neural ODEs and physics informed variants often require orders of magnitude more iterations, and even then may converge to suboptimal solutions that fail to preserve oscillatory frequency or amplitude. We introduce PhysicsInformed Neural ODEs with with Scale-Aware Residuals (PI-NODE-SR), a framework that combines a low-order explicit solver (Heun method) residual normalisation to balance contributions between state variables evolving on disparate timescales. This combination stabilises training under realistic iteration budgets and avoids reliance on computationally expensive implicit solvers. On the Hodgkin-Huxley equations, PI-NODE-SR learns from a single oscillation simulated with a stiff solver (Rodas5P) and extrapolates beyond 100 ms, capturing both oscillation frequency and near-correct amplitudes. Remarkably, end-to-end learning of the vector field enables PI-NODE-SR to recover morphological features such as sharp subthreshold curvature in gating variables that are typically reserved for higher-order solvers, suggesting that neural correction can offset numerical diffusion. While performance remains sensitive to initialisation, PI-NODE-SR consistently reduces long-horizon errors relative to baseline Neural-ODEs and PINNs, offering a principled route to stable and efficient learning of stiff biological dynamics.

</details>


### [272] [KAN/H: Kolmogorov-Arnold Network using Haar-like bases](https://arxiv.org/abs/2511.11736)
*Susumu Katayama*

Main category: cs.LG

TL;DR: 提出KAN/H变体用于函数逼近和MNIST，无需大量特定问题超参数调整


<details>
  <summary>Details</summary>
Motivation: 改进Kolmogorov - Arnold Network（KAN），解决特定问题超参数调优难题

Method: 提出使用Haar变体基系统替代B样条的KAN/H变体算法

Result: 该算法应用于函数逼近问题和MNIST，无需大量特定问题超参数调优

Conclusion: KAN/H在函数逼近和MNIST任务中有优势，减少超参数调优工作量

Abstract: This paper proposes KAN/H, a variant of Kolmogorov-Arnold Network (KAN) that uses a Haar-variant basis system having both global and local bases instead of B-spline. The resulting algorithm is applied to function approximation problems and MNIST. We show that it does not require most of the problem-specific hyper-parameter tunings.

</details>


### [273] [Uncertainty Makes It Stable: Curiosity-Driven Quantized Mixture-of-Experts](https://arxiv.org/abs/2511.11743)
*Sebastián Andrés Cajas Ordóñez,Luis Fernando Torres Torres,Mackenzie J. Meni,Carlos Andrés Duran Paredes,Eric Arazo,Cristian Bosch,Ricardo Simon Carbajo,Yuan Lai,Leo Anthony Celi*

Main category: cs.LG

TL;DR: 提出好奇驱动的量化混合专家框架，在音频分类基准测试中，4 位量化保持高准确率，减少延迟方差，信息论路由使边缘模型准确、节能且可预测，简单 4 位量化架构多数情况下优于复杂 MoE。


<details>
  <summary>Details</summary>
Motivation: 解决在资源受限设备上部署深度神经网络时，积极量化下保持准确性和确保可预测推理延迟的问题。

Method: 提出好奇驱动的量化混合专家框架，通过基于贝叶斯认知不确定性的路由跨越异构专家。

Result: 4 位量化保持 99.9%的 16 位准确率，4 倍压缩和 41%节能；好奇驱动路由减少 82%的 MoE 延迟方差；4 位/8 位与全精度实际等效；MoE 架构引入 11%延迟开销；信息论路由使模型准确、节能且可预测。

Conclusion: 自适应量化能产生准确、节能和可预测的边缘模型，简单 4 位量化架构在大多数部署中优于复杂 MoE。

Abstract: Deploying deep neural networks on resource-constrained devices faces two critical challenges: maintaining accuracy under aggressive quantization while ensuring predictable inference latency. We present a curiosity-driven quantized Mixture-of-Experts framework that addresses both through Bayesian epistemic uncertainty-based routing across heterogeneous experts (BitNet ternary, 1-16 bit BitLinear, post-training quantization). Evaluated on audio classification benchmarks (ESC-50, Quinn, UrbanSound8K), our 4-bit quantization maintains 99.9 percent of 16-bit accuracy (0.858 vs 0.859 F1) with 4x compression and 41 percent energy savings versus 8-bit. Crucially, curiosity-driven routing reduces MoE latency variance by 82 percent (p = 0.008, Levene's test) from 230 ms to 29 ms standard deviation, enabling stable inference for battery-constrained devices. Statistical analysis confirms 4-bit/8-bit achieve practical equivalence with full precision (p > 0.05), while MoE architectures introduce 11 percent latency overhead (p < 0.001) without accuracy gains. At scale, deployment emissions dominate training by 10000x for models serving more than 1,000 inferences, making inference efficiency critical. Our information-theoretic routing demonstrates that adaptive quantization yields accurate (0.858 F1, 1.2M params), energy-efficient (3.87 F1/mJ), and predictable edge models, with simple 4-bit quantized architectures outperforming complex MoE for most deployments.

</details>


### [274] [Diffusion Models: A Mathematical Introduction](https://arxiv.org/abs/2511.11746)
*Sepehr Maleki,Negar Pourmoazemi*

Main category: cs.LG

TL;DR: 本文从高斯分布基本性质出发，推导基于扩散的生成模型，涵盖前向噪声过程、反向后验等，还讨论了似然估计、加速采样等多种方法，并对引导扩散进行解释。


<details>
  <summary>Details</summary>
Motivation: 提供基于扩散的生成模型简洁且自包含的推导，让读者能理解理论并实现对应算法。

Method: 从高斯分布基本性质构建去噪扩散概率模型，通过连续性和福克 - 普朗克方程推导概率流常微分方程等。

Result: 得出标准噪声预测目标，讨论了多种加速采样方法、连续时间公式，对引导扩散给出解释。

Conclusion: 以清晰代数、明确中间步骤和一致符号推导模型，方便读者理解和实践。

Abstract: We present a concise, self-contained derivation of diffusion-based generative models. Starting from basic properties of Gaussian distributions (densities, quadratic expectations, re-parameterisation, products, and KL divergences), we construct denoising diffusion probabilistic models from first principles. This includes the forward noising process, its closed-form marginals, the exact discrete reverse posterior, and the related variational bound. This bound simplifies to the standard noise-prediction goal used in practice. We then discuss likelihood estimation and accelerated sampling, covering DDIM, adversarially learned reverse dynamics (DDGAN), and multi-scale variants such as nested and latent diffusion, with Stable Diffusion as a canonical example. A continuous-time formulation follows, in which we derive the probability-flow ODE from the diffusion SDE via the continuity and Fokker-Planck equations, introduce flow matching, and show how rectified flows recover DDIM up to a time re-parameterisation. Finally, we treat guided diffusion, interpreting classifier guidance as a posterior score correction and classifier-free guidance as a principled interpolation between conditional and unconditional scores. Throughout, the focus is on transparent algebra, explicit intermediate steps, and consistent notation, so that readers can both follow the theory and implement the corresponding algorithms in practice.

</details>


### [275] [IDOL: Meeting Diverse Distribution Shifts with Prior Physics for Tropical Cyclone Multi-Task Estimation](https://arxiv.org/abs/2511.11750)
*Hanting Yan,Pan Mu,Shiqi Zhang,Yuchao Zhu,Jinglin Zhang,Cong Bai*

Main category: cs.LG

TL;DR: 提出IDOL框架处理热带气旋估计中的分布偏移问题，实验证明其有效性，代码开源。


<details>
  <summary>Details</summary>
Motivation: 热带气旋环境场复杂动态导致分布偏移，现有方法忽略特征表示内在分布，泛化性差。

Method: 提出IDOL框架，利用风场模型和暗相关知识建模任务共享和特定身份令牌，施加基于物理先验知识的身份导向约束。

Result: 在多数据集和任务上实验表明IDOL表现优异。

Conclusion: 基于物理先验知识施加身份导向约束可有效缓解热带气旋估计中的分布偏移。

Abstract: Tropical Cyclone (TC) estimation aims to accurately estimate various TC attributes in real time. However, distribution shifts arising from the complex and dynamic nature of TC environmental fields, such as varying geographical conditions and seasonal changes, present significant challenges to reliable estimation. Most existing methods rely on multi-modal fusion for feature extraction but overlook the intrinsic distribution of feature representations, leading to poor generalization under out-of-distribution (OOD) scenarios. To address this, we propose an effective Identity Distribution-Oriented Physical Invariant Learning framework (IDOL), which imposes identity-oriented constraints to regulate the feature space under the guidance of prior physical knowledge, thereby dealing distribution variability with physical invariance. Specifically, the proposed IDOL employs the wind field model and dark correlation knowledge of TC to model task-shared and task-specific identity tokens. These tokens capture task dependencies and intrinsic physical invariances of TC, enabling robust estimation of TC wind speed, pressure, inner-core, and outer-core size under distribution shifts. Extensive experiments conducted on multiple datasets and tasks demonstrate the outperformance of the proposed IDOL, verifying that imposing identity-oriented constraints based on prior physical knowledge can effectively mitigates diverse distribution shifts in TC estimation.Code is available at https://github.com/Zjut-MultimediaPlus/IDOL.

</details>


### [276] [Improving a Hybrid Graphsage Deep Network for Automatic Multi-objective Logistics Management in Supply Chain](https://arxiv.org/abs/2511.11753)
*Mehdi Khaleghi,Nastaran Khaleghi,Sobhan Sheykhivand,Sebelan Danishvar*

Main category: cs.LG

TL;DR: 本文提出混合GraphSAGE网络用于供应链物流管理多任务，利用不同数据库实验，结果证明该方法能提升供应链弹性与可持续性。


<details>
  <summary>Details</summary>
Motivation: 提升供应链弹性和可持续性，需自动方法预测物流相关信息以提高供应链管理效率。

Method: 提出混合GraphSAGE网络（H - GSN），针对三个不同数据库进行物流管理多任务处理。

Result: 在不同数据库中对物流ID、交通状态、运输类型、物流延迟等预测取得高准确率。

Conclusion: 所提方法能有效提升供应链的弹性和可持续性。

Abstract: Systematic logistics, conveyance amenities and facilities as well as warehousing information play a key role in fostering profitable development in a supply chain. The aim of transformation in industries is the improvement of the resiliency regarding the supply chain. The resiliency policies are required for companies to affect the collaboration with logistics service providers positively. The decrement of air pollutant emissions is a persistent advantage of the efficient management of logistics and transportation in supply chain. The management of shipment type is a significant factor in analyzing the sustainability of logistics and supply chain. An automatic approach to predict the shipment type, logistics delay and traffic status are required to improve the efficiency of the supply chain management. A hybrid graphsage network (H-GSN) is proposed in this paper for multi-task purpose of logistics management in a supply chain. The shipment type, shipment status, traffic status, logistics ID and logistics delay are the objectives in this article regarding three different databases including DataCo, Shipping and Smart Logistcis available on Kaggle as supply chain logistics databases. The average accuracy of 97.8% and 100% are acquired for 10 kinds of logistics ID and 3 types of traffic status prediction in Smart Logistics dataset. The average accuracy of 98.7% and 99.4% are obtained for shipment type prediction in DataCo and logistics delay in Shipping database, respectively. The evaluation metrics for different logistics scenarios confirm the efficiency of the proposed method to improve the resilience and sustainability of the supply chain.

</details>


### [277] [Sumudu Neural Operator for ODEs and PDEs](https://arxiv.org/abs/2511.11762)
*Ben Zelenskiy,Saibilila Abudukelimu,George Flint,Kevin Zhu,Sunishchal Dev*

Main category: cs.LG

TL;DR: 介绍基于Sumudu变换的Sumudu神经算子（SNO），在ODE和PDE上评估，表现良好，初步显示Sumudu变换用于神经算子设计有前景。


<details>
  <summary>Details</summary>
Motivation: 引入基于Sumudu变换特性的神经算子，探索其在ODE和PDE中的应用。

Method: 利用变换对的多项式展开关系分解输入空间为系数，转换到Sumudu空间对神经算子参数化。

Result: SNO在PDE上性能优于FNO，在多个PDE任务中与LNO有竞争力，在部分方程上误差最低，零样本超分辨率实验展示模型能力。

Conclusion: Sumudu变换用于神经算子设计有潜力，尤其对某些PDE类问题。

Abstract: We introduce the Sumudu Neural Operator (SNO), a neural operator rooted in the properties of the Sumudu Transform. We leverage the relationship between the polynomial expansions of transform pairs to decompose the input space as coefficients, which are then transformed into the Sumudu Space, where the neural operator is parameterized. We evaluate the operator in ODEs (Duffing Oscillator, Lorenz System, and Driven Pendulum) and PDEs (Euler-Bernoulli Beam, Burger's Equation, Diffusion, Diffusion-Reaction, and Brusselator). SNO achieves superior performance to FNO on PDEs and demonstrates competitive accuracy with LNO on several PDE tasks, including the lowest error on the Euler-Bernoulli Beam and Diffusion Equation. Additionally, we apply zero-shot super-resolution to the PDE tasks to observe the model's capability of obtaining higher quality data from low-quality samples. These preliminary findings suggest promise for the Sumudu Transform as a neural operator design, particularly for certain classes of PDEs.

</details>


### [278] [Learning Fair Representations with Kolmogorov-Arnold Networks](https://arxiv.org/abs/2511.11767)
*Amisha Priyadarshini,Sergio Gago-Masague*

Main category: cs.LG

TL;DR: 现有公平学习模型难平衡公平性与准确性，且黑盒模型缺乏可解释性。本文将KANs集成到公平对抗学习框架，提出自适应惩罚更新机制，实验显示其优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有公平学习模型难以实现公平性与准确性的最优权衡，且黑盒模型缺乏可解释性，在高风险决策领域应用受限。

Method: 将Kolmogorov - Arnold Networks (KANs)集成到公平对抗学习框架，并提出自适应惩罚更新机制动态调整公平约束。

Result: 在两个真实世界大学招生数据集上的实验表明，KANs始终优于基线公平学习模型，能在保持高预测准确性的同时实现敏感属性的公平性。

Conclusion: KANs方法有效且稳健，可实现公平性和准确性的平衡。

Abstract: Despite recent advances in fairness-aware machine learning, predictive models often exhibit discriminatory behavior towards marginalized groups. Such unfairness might arise from biased training data, model design, or representational disparities across groups, posing significant challenges in high-stakes decision-making domains such as college admissions. While existing fair learning models aim to mitigate bias, achieving an optimal trade-off between fairness and accuracy remains a challenge. Moreover, the reliance on black-box models hinders interpretability, limiting their applicability in socially sensitive domains. In this paper, we try to circumvent these issues by integrating Kolmogorov-Arnold Networks (KANs) within a fair adversarial learning framework. Leveraging the adversarial robustness and interpretability of KANs, our approach enables a balance between fairness and accuracy. To further facilitate this balance, we propose an adaptive penalty update mechanism that dynamically adjusts fairness constraints during the model training. We conduct numerical experiments on two real-world college admissions datasets, across three different optimization strategies. The results demonstrate the efficiency and robustness of KANs by consistently outperforming the baseline fair learning models, and maintaining high predictive accuracy while achieving competitive fairness across sensitive attributes.

</details>


### [279] [CATCHFed: Efficient Unlabeled Data Utilization for Semi-Supervised Federated Learning in Limited Labels Environments](https://arxiv.org/abs/2511.11778)
*Byoungjun Park,Pedro Porto Buarque de Gusmão,Dongjin Ji,Minhoe Kim*

Main category: cs.LG

TL;DR: 提出CATCHFed方法解决半监督联邦学习在标记数据减少时性能下降问题，实验表明其在有限标记设置下性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习假设客户端有标记数据，半监督联邦学习在标记数据减少时性能显著下降。

Method: 提出CATCHFed，引入考虑类难度的客户端感知自适应阈值、混合阈值提升伪标签质量，利用非伪标签数据进行一致性正则化。

Result: 在各种数据集和配置上的大量实验表明，CATCHFed能有效利用客户端未标记数据。

Conclusion: CATCHFed在极其有限标记设置下也能取得优越性能。

Abstract: Federated learning is a promising paradigm that utilizes distributed client resources while preserving data privacy. Most existing FL approaches assume clients possess labeled data, however, in real-world scenarios, client-side labels are often unavailable. Semi-supervised Federated learning, where only the server holds labeled data, addresses this issue. However, it experiences significant performance degradation as the number of labeled data decreases. To tackle this problem, we propose \textit{CATCHFed}, which introduces client-aware adaptive thresholds considering class difficulty, hybrid thresholds to enhance pseudo-label quality, and utilizes unpseudo-labeled data for consistency regularization. Extensive experiments across various datasets and configurations demonstrate that CATCHFed effectively leverages unlabeled client data, achieving superior performance even in extremely limited-label settings.

</details>


### [280] [Simplicial covering dimension of extremal concept classes](https://arxiv.org/abs/2511.11819)
*Ari Blondal,Hamed Hatami,Pooya Hatami,Chavdar Lalov,Sivan Tretiak*

Main category: cs.LG

TL;DR: 本文将经典拓扑维度概念应用于二元概念类，定义单纯覆盖维度，并证明其与PAC学习中列表可复制性数的关系，用于计算极值概念类的列表可复制性数。


<details>
  <summary>Details</summary>
Motivation: 将经典拓扑维度理论应用到二元概念类，探索其与PAC学习中列表可复制性数的联系。

Method: 将经典拓扑维度（Lebesgue覆盖）概念适配到二元概念类，利用概念类的可实现分布空间及其诱导的单纯结构定义单纯覆盖维度。

Result: 证明对于有限概念类，单纯覆盖维度精确刻画了PAC学习中的列表可复制性数（即全局稳定性）。

Conclusion: 建立的联系使我们能运用经典维度理论工具计算广泛的极值概念类的列表可复制性数。

Abstract: Dimension theory is a branch of topology concerned with defining and analyzing dimensions of geometric and topological spaces in purely topological terms. In this work, we adapt the classical notion of topological dimension (Lebesgue covering) to binary concept classes. The topological space naturally associated with a concept class is its space of realizable distributions. The loss function and the class itself induce a simplicial structure on this space, with respect to which we define a simplicial covering dimension.
  We prove that for finite concept classes, this simplicial covering dimension exactly characterizes the list replicability number (equivalently, global stability) in PAC learning. This connection allows us to apply tools from classical dimension theory to compute the exact list replicability number of the broad family of extremal concept classes.

</details>


### [281] [Conformal Constrained Policy Optimization for Cost-Effective LLM Agents](https://arxiv.org/abs/2511.11828)
*Wenwen Si,Sooyong Jang,Insup Lee,Osbert Bastani*

Main category: cs.LG

TL;DR: 提出结合多LLM模型的策略，用CCPO方法，在多跳问答基准上降本30%且不损可靠性。


<details>
  <summary>Details</summary>
Motivation: LLM解决AI问题时计算和API成本高，需降低成本同时保证可靠性。

Method: 提出结合多LLM模型的策略，用Conformal Constrained Policy Optimization (CCPO) 训练范式，结合约束策略优化、离线策略强化学习和在线共形预测。

Result: 在两个多跳问答基准上，CCPO较其他方法实现高达30%的成本降低且不影响可靠性。

Conclusion: 该方法为部署更具成本效益且可靠的LLM智能体提供了原则性和实用性框架。

Abstract: While large language models (LLMs) have recently made tremendous progress towards solving challenging AI problems, they have done so at increasingly steep computational and API costs. We propose a novel strategy where we combine multiple LLM models with varying cost/accuracy tradeoffs in an agentic manner, where models and tools are run in sequence as determined by an orchestration model to minimize cost subject to a user-specified level of reliability; this constraint is formalized using conformal prediction to provide guarantees. To solve this problem, we propose Conformal Constrained Policy Optimization (CCPO), a training paradigm that integrates constrained policy optimization with off-policy reinforcement learning and recent advances in online conformal prediction. CCPO jointly optimizes a cost-aware policy (score function) and an adaptive threshold. Across two multi-hop question answering benchmarks, CCPO achieves up to a 30% cost reduction compared to other cost-aware baselines and LLM-guided methods without compromising reliability. Our approach provides a principled and practical framework for deploying LLM agents that are significantly more cost-effective while maintaining reliability.

</details>


### [282] [Volatility in Certainty (VC): A Metric for Detecting Adversarial Perturbations During Inference in Neural Network Classifiers](https://arxiv.org/abs/2511.11834)
*Vahid Hemmati,Ahmad Mohammadi,Abdul-Rauf Nuhu,Reza Ahmari,Parham Kebria,Abdollah Homaifar*

Main category: cs.LG

TL;DR: 本文研究无标签指标Volatility in Certainty (VC)，在多数据集和模型上评估其作为分类准确率代理和对抗漂移指标的效果，发现分类准确率和log(VC)强负相关，表明VC可用于安全关键应用预警。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络分类器在实时系统中缺乏真实标签时的对抗鲁棒性挑战。

Method: 定义VC，在MNIST和CIFAR - 10数据集上对ANN、CNN和正则化VGG模型进行实验，用FGSM生成对抗样本，创建混合测试集评估VC在分布变化下的敏感性。

Result: 分类准确率和log(VC)强负相关，多数情况下相关系数rho < -0.90。

Conclusion: VC是可扩展、与架构无关的实时性能指标，适用于安全关键应用的预警系统。

Abstract: Adversarial robustness remains a critical challenge in deploying neural network classifiers, particularly in real-time systems where ground-truth labels are unavailable during inference. This paper investigates \textit{Volatility in Certainty} (VC), a recently proposed, label-free metric that quantifies irregularities in model confidence by measuring the dispersion of sorted softmax outputs. Specifically, VC is defined as the average squared log-ratio of adjacent certainty values, capturing local fluctuations in model output smoothness. We evaluate VC as a proxy for classification accuracy and as an indicator of adversarial drift. Experiments are conducted on artificial neural networks (ANNs) and convolutional neural networks (CNNs) trained on MNIST, as well as a regularized VGG-like model trained on CIFAR-10. Adversarial examples are generated using the Fast Gradient Sign Method (FGSM) across varying perturbation magnitudes. In addition, mixed test sets are created by gradually introducing adversarial contamination to assess VC's sensitivity under incremental distribution shifts. Our results reveal a strong negative correlation between classification accuracy and log(VC) (correlation rho < -0.90 in most cases), suggesting that VC effectively reflects performance degradation without requiring labeled data. These findings position VC as a scalable, architecture-agnostic, and real-time performance metric suitable for early-warning systems in safety-critical applications.

</details>


### [283] [Leveraging Exogenous Signals for Hydrology Time Series Forecasting](https://arxiv.org/abs/2511.11849)
*Junyang He,Judy Fox,Alireza Jafari,Ying-Jung Chen,Geoffrey Fox*

Main category: cs.LG

TL;DR: 研究在水文降雨 - 径流建模中整合领域知识到时间序列模型的作用，对比基线和基础模型，含综合外生输入的模型表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有研究很少检验时间序列基础模型在物理科学特定下游应用中的有效性，本文研究在水文降雨 - 径流建模中整合领域知识到时间序列模型的作用。

Method: 使用包含671个地点降雨和径流数据的CAMELS - US数据集，对比基线和基础模型。

Result: 含综合已知外生输入的模型优于更有限的方法，包括基础模型，融入自然年度周期性时间序列改进最显著。

Conclusion: 在水文降雨 - 径流建模中，整合领域知识能提升时间序列模型性能，综合外生输入和自然年度周期性时间序列很重要。

Abstract: Recent advances in time series research facilitate the development of foundation models. While many state-of-the-art time series foundation models have been introduced, few studies examine their effectiveness in specific downstream applications in physical science. This work investigates the role of integrating domain knowledge into time series models for hydrological rainfall-runoff modeling. Using the CAMELS-US dataset, which includes rainfall and runoff data from 671 locations with six time series streams and 30 static features, we compare baseline and foundation models. Results demonstrate that models incorporating comprehensive known exogenous inputs outperform more limited approaches, including foundation models. Notably, incorporating natural annual periodic time series contribute the most significant improvements.

</details>


### [284] [Transformers vs. Recurrent Models for Estimating Forest Gross Primary Production](https://arxiv.org/abs/2511.11880)
*David Montero,Miguel D. Mahecha,Francesco Martinuzzi,César Aybar,Anne Klosterhalfen,Alexander Knohl,Jesús Anaya,Clemens Mosig,Sebastian Wieneke*

Main category: cs.LG

TL;DR: 文章对比GPT - 2和LSTM模型预测森林GPP表现，发现二者精度相似，LSTM总体更佳但GPT - 2在极端事件表现好，还揭示多因素对预测性能的影响。


<details>
  <summary>Details</summary>
Motivation: 现有方法在监测森林GPP时空动态上存在局限，深度学习和数据融合有新机会，但缺乏先进深度学习模型的比较评估。

Method: 使用GPT - 2和LSTM两种模型，以多变量输入进行森林GPP预测，并分析时间上下文长度和特征重要性。

Result: 两种模型精度相似，LSTM总体表现好，GPT - 2在极端事件中表现出色；LSTM用更短输入窗口达相似精度；辐射是主要预测因子。

Conclusion: 模型架构、上下文长度和多模态输入共同决定GPP预测性能，为监测陆地碳动态的深度学习框架发展提供指导。

Abstract: Monitoring the spatiotemporal dynamics of forest CO$_2$ uptake (Gross Primary Production, GPP), remains a central challenge in terrestrial ecosystem research. While Eddy Covariance (EC) towers provide high-frequency estimates, their limited spatial coverage constrains large-scale assessments. Remote sensing offers a scalable alternative, yet most approaches rely on single-sensor spectral indices and statistical models that are often unable to capture the complex temporal dynamics of GPP. Recent advances in deep learning (DL) and data fusion offer new opportunities to better represent the temporal dynamics of vegetation processes, but comparative evaluations of state-of-the-art DL models for multimodal GPP prediction remain scarce. Here, we explore the performance of two representative models for predicting GPP: 1) GPT-2, a transformer architecture, and 2) Long Short-Term Memory (LSTM), a recurrent neural network, using multivariate inputs. Overall, both achieve similar accuracy. But, while LSTM performs better overall, GPT-2 excels during extreme events. Analysis of temporal context length further reveals that LSTM attains similar accuracy using substantially shorter input windows than GPT-2, highlighting an accuracy-efficiency trade-off between the two architectures. Feature importance analysis reveals radiation as the dominant predictor, followed by Sentinel-2, MODIS land surface temperature, and Sentinel-1 contributions. Our results demonstrate how model architecture, context length, and multimodal inputs jointly determine performance in GPP prediction, guiding future developments of DL frameworks for monitoring terrestrial carbon dynamics.

</details>


### [285] [Better LLM Reasoning via Dual-Play](https://arxiv.org/abs/2511.11881)
*Zhengxin Zhang,Chengyu Huang,Aochong Oliver Li,Claire Cardie*

Main category: cs.LG

TL;DR: 本文提出PasoDoble这一新型大语言模型双对抗训练框架，可在无监督下训练，实验显示能提升大语言模型推理性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型通过可验证奖励的强化学习取得进展但依赖外部监督，对抗学习虽有潜力，但双对抗训练应用于大语言模型受限，因模型易受奖励破解和训练不稳定问题影响。

Method: 提出PasoDoble框架，从同一基础模型初始化两个模型进行对抗训练，分别为生成问题的提议者和解决问题的求解者；用预训练数据集知识丰富提议者；为避免奖励破解，合理设置奖励机制并联合更新两个模型；引入离线范式增强训练稳定性。

Result: 实验表明PasoDoble能提高大语言模型的推理性能。

Conclusion: PasoDoble框架是一种有效的大语言模型双对抗训练方法，可在无监督下提升模型推理性能。

Abstract: Large Language Models (LLMs) have achieved remarkable progress through Reinforcement Learning with Verifiable Rewards (RLVR), yet still rely heavily on external supervision (e.g., curated labels). Adversarial learning, particularly through self-play, offers a promising alternative that enables models to iteratively learn from themselves - thus reducing reliance on external supervision. Dual-play extends adversarial learning by assigning specialized roles to two models and training them against each other, fostering sustained competition and mutual evolution. Despite its promise, adapting dual-play training to LLMs remains limited, largely due to their susceptibility to reward hacking and training instability. In this paper, we introduce PasoDoble, a novel LLM dual-play framework. PasoDoble adversarially trains two models initialized from the same base model: a Proposer, which generates challenging questions with ground-truth answers, and a Solver, which attempts to solve them. We enrich the Proposer with knowledge from a pre-training dataset to ensure the questions' quality and diversity. To avoid reward hacking, the Proposer is rewarded for producing only valid questions that push the Solver's limit, while the Solver is rewarded for solving them correctly, and both are updated jointly. To further enhance training stability, we introduce an optional offline paradigm that decouples Proposer and Solver updates, alternately updating each for several steps while holding the other fixed. Notably, PasoDoble operates without supervision during training. Experimental results show that PasoDoble can improve the reasoning performance of LLMs. Our project page is available at https://hcy123902.github.io/PasoDoble.

</details>


### [286] [FLEX: Feature Importance from Layered Counterfactual Explanations](https://arxiv.org/abs/2511.11891)
*Nawid Keshtmand,Roussel Desmond Nzoyem,Jeffrey Nicholas Clark*

Main category: cs.LG

TL;DR: 提出FLEX框架将反事实转换为特征重要性分数，在两个任务上评估，能连接局部和全局解释，支持决策。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型缺乏可解释性，现有反事实解释缺乏系统性，无法量化特征对结果的影响。

Method: 引入FLEX框架，将反事实集转换为局部、区域和全局的特征变化频率分数，可聚合实例和邻域，兼容不同反事实生成方法。

Result: 在两个任务上评估，FLEX全局排名与SHAP相关且发现新驱动因素，区域分析能揭示全局总结遗漏的因素。

Conclusion: FLEX弥补了局部和全局解释的差距，支持风险敏感应用中的透明决策。

Abstract: Machine learning models achieve state-of-the-art performance across domains, yet their lack of interpretability limits safe deployment in high-stakes settings. Counterfactual explanations are widely used to provide actionable "what-if" recourse, but they typically remain instance-specific and do not quantify which features systematically drive outcome changes within coherent regions of the feature space or across an entire dataset. We introduce FLEX (Feature importance from Layered counterfactual EXplanations), a model- and domain-agnostic framework that converts sets of counterfactuals into feature change frequency scores at local, regional, and global levels. FLEX generalises local change-frequency measures by aggregating across instances and neighbourhoods, offering interpretable rankings that reflect how often each feature must change to flip predictions. The framework is compatible with different counterfactual generation methods, allowing users to emphasise characteristics such as sparsity, feasibility, or actionability, thereby tailoring the derived feature importances to practical constraints. We evaluate FLEX on two contrasting tabular tasks: traffic accident severity prediction and loan approval, and compare FLEX to SHAP- and LIME-derived feature importance values. Results show that (i) FLEX's global rankings correlate with SHAP while surfacing additional drivers, and (ii) regional analyses reveal context-specific factors that global summaries miss. FLEX thus bridges the gap between local recourse and global attribution, supporting transparent and intervention-oriented decision-making in risk-sensitive applications.

</details>


### [287] [Chain-of-Generation: Progressive Latent Diffusion for Text-Guided Molecular Design](https://arxiv.org/abs/2511.11894)
*Lingxiao Li,Haobo Zhang,Bin Chen,Jiayu Zhou*

Main category: cs.LG

TL;DR: 本文提出训练-free的多阶段潜在扩散框架CoG用于文本条件分子生成，实验表明其优于单步基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的文本条件分子生成方法采用单步条件化，难以满足提示中的所有要求，存在生成组件可解释性差等问题。

Method: 提出Chain-of-Generation (CoG)框架，将提示分解为课程排序的语义段并逐步融入作为中间目标，还引入后对齐学习阶段强化语义引导。

Result: 在基准和实际任务的大量实验中，CoG在语义对齐、多样性和可控性上优于单步基线。

Conclusion: CoG能更忠实地反映复杂、组合式提示，且生成过程透明。

Abstract: Text-conditioned molecular generation aims to translate natural-language descriptions into chemical structures, enabling scientists to specify functional groups, scaffolds, and physicochemical constraints without handcrafted rules. Diffusion-based models, particularly latent diffusion models (LDMs), have recently shown promise by performing stochastic search in a continuous latent space that compactly captures molecular semantics. Yet existing methods rely on one-shot conditioning, where the entire prompt is encoded once and applied throughout diffusion, making it hard to satisfy all the requirements in the prompt. We discuss three outstanding challenges of one-shot conditioning generation, including the poor interpretability of the generated components, the failure to generate all substructures, and the overambition in considering all requirements simultaneously. We then propose three principles to address those challenges, motivated by which we propose Chain-of-Generation (CoG), a training-free multi-stage latent diffusion framework. CoG decomposes each prompt into curriculum-ordered semantic segments and progressively incorporates them as intermediate goals, guiding the denoising trajectory toward molecules that satisfy increasingly rich linguistic constraints. To reinforce semantic guidance, we further introduce a post-alignment learning phase that strengthens the correspondence between textual and molecular latent spaces. Extensive experiments on benchmark and real-world tasks demonstrate that CoG yields higher semantic alignment, diversity, and controllability than one-shot baselines, producing molecules that more faithfully reflect complex, compositional prompts while offering transparent insight into the generation process.

</details>


### [288] [Robust Bidirectional Associative Memory via Regularization Inspired by the Subspace Rotation Algorithm](https://arxiv.org/abs/2511.11902)
*Ci Lin,Tet Yeap,Iluju Kiringa,Biwei Zhang*

Main category: cs.LG

TL;DR: 提出双向子空间旋转算法（B - SRA）及正则化策略提升双向联想记忆（BAM）的鲁棒性，SAME配置效果最佳。


<details>
  <summary>Details</summary>
Motivation: 解决B - BP训练的BAM鲁棒性差、对噪声和对抗攻击敏感的问题。

Method: 提出B - SRA算法，确定正交权重矩阵（OWM）和梯度模式对齐（GPA）原则，引入正则化策略到B - BP，进行消融研究。

Result: SAME配置（集成OWM和GPA）在各种攻击场景和记忆容量下韧性最强。

Conclusion: B - SRA和正则化策略使关联记忆更鲁棒，为构建弹性神经架构开辟新方向。

Abstract: Bidirectional Associative Memory (BAM) trained with Bidirectional Backpropagation (B-BP) often suffers from poor robustness and high sensitivity to noise and adversarial attacks. To address these issues, we propose a novel gradient-free training algorithm, the Bidirectional Subspace Rotation Algorithm (B-SRA), which significantly improves the robustness and convergence behavior of BAM. Through comprehensive experiments, we identify two key principles -- orthogonal weight matrices (OWM) and gradient-pattern alignment (GPA) -- as central to enhancing the robustness of BAM. Motivated by these findings, we introduce new regularization strategies into B-BP, resulting in models with greatly improved resistance to corruption and adversarial perturbations. We further conduct an ablation study across different training strategies to determine the most robust configuration and evaluate BAM's performance under a variety of attack scenarios and memory capacities, including 50, 100, and 200 associative pairs. Among all methods, the SAME configuration, which integrates both OWM and GPA, achieves the strongest resilience. Overall, our results demonstrate that B-SRA and the proposed regularization strategies lead to substantially more robust associative memories and open new directions for building resilient neural architectures.

</details>


### [289] [A Systematic Study of Model Extraction Attacks on Graph Foundation Models](https://arxiv.org/abs/2511.11912)
*Haoyan Xu,Ruizhi Qian,Jiate Li,Yushun Dong,Minghao Lin,Hanson Yan,Zhengtao Yao,Qinghua Liu,Junhao Dong,Ruopeng Huang,Yue Zhao,Mengyuan Li*

Main category: cs.LG

TL;DR: 图机器学习发展迅速，图基础模型（GFMs）有诸多优势但易受模型提取攻击（MEAs），本文首次系统研究针对GFMs的MEAs，提出方法并实验，揭示GFMs扩大攻击面，需安全防御。


<details>
  <summary>Details</summary>
Motivation: 先前工作仅关注单图训练的小图神经网络，大规模多模态GFMs的安全影响未充分研究，需对针对GFMs的MEAs进行系统研究。

Method: 形式化黑盒威胁模型，定义六种攻击场景，引入轻量级提取方法，用图嵌入的监督回归训练攻击者编码器。

Result: 在七个数据集上实验表明，攻击者能用极小训练成本近似受害者模型，准确率几乎无损失。

Conclusion: GFMs大大扩大了MEAs攻击面，大规模图学习系统需要考虑部署的安全防御。

Abstract: Graph machine learning has advanced rapidly in tasks such as link prediction, anomaly detection, and node classification. As models scale up, pretrained graph models have become valuable intellectual assets because they encode extensive computation and domain expertise. Building on these advances, Graph Foundation Models (GFMs) mark a major step forward by jointly pretraining graph and text encoders on massive and diverse data. This unifies structural and semantic understanding, enables zero-shot inference, and supports applications such as fraud detection and biomedical analysis. However, the high pretraining cost and broad cross-domain knowledge in GFMs also make them attractive targets for model extraction attacks (MEAs). Prior work has focused only on small graph neural networks trained on a single graph, leaving the security implications for large-scale and multimodal GFMs largely unexplored. This paper presents the first systematic study of MEAs against GFMs. We formalize a black-box threat model and define six practical attack scenarios covering domain-level and graph-specific extraction goals, architectural mismatch, limited query budgets, partial node access, and training data discrepancies. To instantiate these attacks, we introduce a lightweight extraction method that trains an attacker encoder using supervised regression of graph embeddings. Even without contrastive pretraining data, this method learns an encoder that stays aligned with the victim text encoder and preserves its zero-shot inference ability on unseen graphs. Experiments on seven datasets show that the attacker can approximate the victim model using only a tiny fraction of its original training cost, with almost no loss in accuracy. These findings reveal that GFMs greatly expand the MEA surface and highlight the need for deployment-aware security defenses in large-scale graph learning systems.

</details>


### [290] [Batch Matrix-form Equations and Implementation of Multilayer Perceptrons](https://arxiv.org/abs/2511.11918)
*Wieger Wesselink,Bram Grooten,Huub van de Wetering,Qiao Xiao,Decebal Constantin Mocanu*

Main category: cs.LG

TL;DR: 本文以批量矩阵形式对多层感知机（MLP）进行数学上严格且可实现的规范，推导相关方程，构建多种后端参考实现，为神经网络算法研究等奠定基础。


<details>
  <summary>Details</summary>
Motivation: 多数文献未以完整、显式的批量矩阵形式呈现MLP算法细节，而批量矩阵形式对稀疏神经网络等场景的分析和优化很重要，故填补此空白。

Method: 推导所有标准和高级层的前向和反向方程，用SymPy库验证方程，基于此在多种后端构建统一参考实现。

Result: 完成MLP批量矩阵形式的反向传播推导、梯度方程符号验证、构建多种后端参考实现，展示显式公式可实现高效稀疏计算。

Conclusion: 研究结果为理解、教学和研究神经网络算法建立了经过验证且可扩展的基础。

Abstract: Multilayer perceptrons (MLPs) remain fundamental to modern deep learning, yet their algorithmic details are rarely presented in complete, explicit \emph{batch matrix-form}. Rather, most references express gradients per sample or rely on automatic differentiation. Although automatic differentiation can achieve equally high computational efficiency, the usage of batch matrix-form makes the computational structure explicit, which is essential for transparent, systematic analysis, and optimization in settings such as sparse neural networks. This paper fills that gap by providing a mathematically rigorous and implementation-ready specification of MLPs in batch matrix-form. We derive forward and backward equations for all standard and advanced layers, including batch normalization and softmax, and validate all equations using the symbolic mathematics library SymPy. From these specifications, we construct uniform reference implementations in NumPy, PyTorch, JAX, TensorFlow, and a high-performance C++ backend optimized for sparse operations. Our main contributions are: (1) a complete derivation of batch matrix-form backpropagation for MLPs, (2) symbolic validation of all gradient equations, (3) uniform Python and C++ reference implementations grounded in a small set of matrix primitives, and (4) demonstration of how explicit formulations enable efficient sparse computation. Together, these results establish a validated, extensible foundation for understanding, teaching, and researching neural network algorithms.

</details>


### [291] [Beyond the Laplacian: Interpolated Spectral Augmentation for Graph Neural Networks](https://arxiv.org/abs/2511.11928)
*Ziyao Cui,Edric Tam*

Main category: cs.LG

TL;DR: 论文探讨在节点特征有限时用图矩阵谱嵌入增强特征，引入插值拉普拉斯嵌入（ILEs），实验表明其能提升GNN性能。


<details>
  <summary>Details</summary>
Motivation: GNN性能依赖节点特征，但实际数据中节点特征可能有限或缺失，研究替代图矩阵谱嵌入是否能提供有用表示。

Method: 引入基于简单且表达性强的图矩阵族的插值拉普拉斯嵌入（ILEs），并用谱图理论工具解释其捕获的结构信息。

Result: 通过模拟和真实数据集实验，证明通过ILEs进行特征增强能提高常用GNN架构的性能。

Conclusion: 工作提供了一种简单实用的方法，扩展了节点特征有限时从业者的谱增强工具包。

Abstract: Graph neural networks (GNNs) are fundamental tools in graph machine learning. The performance of GNNs relies crucially on the availability of informative node features, which can be limited or absent in real-life datasets and applications. A natural remedy is to augment the node features with embeddings computed from eigenvectors of the graph Laplacian matrix. While it is natural to default to Laplacian spectral embeddings, which capture meaningful graph connectivity information, we ask whether spectral embeddings from alternative graph matrices can also provide useful representations for learning. We introduce Interpolated Laplacian Embeddings (ILEs), which are derived from a simple yet expressive family of graph matrices. Using tools from spectral graph theory, we offer a straightforward interpretation of the structural information that ILEs capture. We demonstrate through simulations and experiments on real-world datasets that feature augmentation via ILEs can improve performance across commonly used GNN architectures. Our work offers a straightforward and practical approach that broadens the practitioner's spectral augmentation toolkit when node features are limited.

</details>


### [292] [A Systematic Analysis of Out-of-Distribution Detection Under Representation and Training Paradigm Shifts](https://arxiv.org/abs/2511.11934)
*C. César Claros Olivares,Austin J. Brockmeier*

Main category: cs.LG

TL;DR: 对跨CLIP分层机制的OOD检测方法进行系统比较，发现特征空间决定OOD效果，给出方法选择指导。


<details>
  <summary>Details</summary>
Motivation: 对不同表示范式下的OOD检测方法进行系统比较，为方法选择提供依据。

Method: 使用AURC和AUGRC作为主要指标，采用多比较控制、基于排名的管道（Friedman检验及Conover - Holm事后检验）和Bron - Kerbosch团算法，在多种数据集上对CNN和ViT进行实验。

Result: 特征空间很大程度决定OOD效果；概率分数在ID检测占优；强分布偏移下，几何感知分数在CNN上表现好，GradNorm和KPCA重建误差在ViT上有竞争力；MCD存在类计数依赖权衡，简单PCA投影可改进部分检测器。

Conclusion: 支持以表示为中心的OOD检测观点，为分布偏移下的方法选择提供统计依据。

Abstract: We present a systematic comparison of out-of-distribution (OOD) detection methods across CLIP-stratified regimes using AURC and AUGRC as primary metrics. Experiments cover two representation paradigms: CNNs trained from scratch and a fine-tuned Vision Transformer (ViT), evaluated on CIFAR-10/100, SuperCIFAR-100, and TinyImageNet. Using a multiple-comparison-controlled, rank-based pipeline (Friedman test with Conover-Holm post-hoc) and Bron-Kerbosch cliques, we find that the learned feature space largely determines OOD efficacy. For both CNNs and ViTs, probabilistic scores (e.g., MSR, GEN) dominate misclassification (ID) detection. Under stronger shifts, geometry-aware scores (e.g., NNGuide, fDBD, CTM) prevail on CNNs, whereas on ViTs GradNorm and KPCA Reconstruction Error remain consistently competitive. We further show a class-count-dependent trade-off for Monte-Carlo Dropout (MCD) and that a simple PCA projection improves several detectors. These results support a representation-centric view of OOD detection and provide statistically grounded guidance for method selection under distribution shift.

</details>


### [293] [SurvBench: A Standardised Preprocessing Pipeline for Multi-Modal Electronic Health Record Survival Analysis](https://arxiv.org/abs/2511.11935)
*Munib Mesinovic,Tingting Zhu*

Main category: cs.LG

TL;DR: 提出开源预处理管道SurvBench，处理原始数据集用于多模态生存分析，解决深度学习生存模型预处理问题。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录数据用于深度学习生存分析时，因预处理方法不一致导致可重复性受限。

Method: 构建SurvBench，提供数据加载器，实施数据质量控制、患者级拆分等操作。

Result: SurvBench输出与pycox库和标准模型兼容，能处理单风险和竞争风险场景。

Conclusion: SurvBench解决了预处理差距问题，让研究人员专注方法创新。

Abstract: Electronic health record (EHR) data present tremendous opportunities for advancing survival analysis through deep learning, yet reproducibility remains severely constrained by inconsistent preprocessing methodologies. We present SurvBench, a comprehensive, open-source preprocessing pipeline that transforms raw PhysioNet datasets into standardised, model-ready tensors for multi-modal survival analysis. SurvBench provides data loaders for three major critical care databases, MIMIC-IV, eICU, and MC-MED, supporting diverse modalities including time-series vitals, static demographics, ICD diagnosis codes, and radiology reports. The pipeline implements rigorous data quality controls, patient-level splitting to prevent data leakage, explicit missingness tracking, and standardised temporal aggregation. SurvBench handles both single-risk (e.g., in-hospital mortality) and competing-risks scenarios (e.g., multiple discharge outcomes). The outputs are compatible with pycox library packages and implementations of standard statistical and deep learning models. By providing reproducible, configuration-driven preprocessing with comprehensive documentation, SurvBench addresses the "preprocessing gap" that has hindered fair comparison of deep learning survival models, enabling researchers to focus on methodological innovation rather than data engineering.

</details>


### [294] [Learning the relative composition of EEG signals using pairwise relative shift pretraining](https://arxiv.org/abs/2511.11940)
*Christopher Sandino,Sayeri Lala,Geeling Chau,Melika Ayoughi,Behrooz Mahasseni,Ellen Zippi,Ali Moin,Erdrin Azemi,Hanlin Goh*

Main category: cs.LG

TL;DR: 提出新颖自监督预训练任务PARS用于EEG表征学习，在多任务中表现优于现有策略。


<details>
  <summary>Details</summary>
Motivation: 当前EEG自监督学习方法多关注局部模式，位置预测预训练对学习神经信号长程依赖的潜力未充分挖掘。

Method: 引入PARS预训练任务，预测随机采样的EEG窗口对之间的相对时间偏移。

Result: PARS预训练的Transformer在标签高效和迁移学习设置中始终优于现有预训练策略。

Conclusion: PARS为自监督EEG表征学习建立了新范式。

Abstract: Self-supervised learning (SSL) offers a promising approach for learning electroencephalography (EEG) representations from unlabeled data, reducing the need for expensive annotations for clinical applications like sleep staging and seizure detection. While current EEG SSL methods predominantly use masked reconstruction strategies like masked autoencoders (MAE) that capture local temporal patterns, position prediction pretraining remains underexplored despite its potential to learn long-range dependencies in neural signals. We introduce PAirwise Relative Shift or PARS pretraining, a novel pretext task that predicts relative temporal shifts between randomly sampled EEG window pairs. Unlike reconstruction-based methods that focus on local pattern recovery, PARS encourages encoders to capture relative temporal composition and long-range dependencies inherent in neural signals. Through comprehensive evaluation on various EEG decoding tasks, we demonstrate that PARS-pretrained transformers consistently outperform existing pretraining strategies in label-efficient and transfer learning settings, establishing a new paradigm for self-supervised EEG representation learning.

</details>


### [295] [Computation-aware Energy-harvesting Federated Learning: Cyclic Scheduling with Selective Participation](https://arxiv.org/abs/2511.11949)
*Eunjeong Jeong,Nikolaos Pappas*

Main category: cs.LG

TL;DR: 提出FedBacys和FedBacys - Odd框架解决联邦学习能耗问题，实验证明其能效和鲁棒性更优。


<details>
  <summary>Details</summary>
Motivation: 联邦学习复杂度增加使客户端训练模型能耗大，能量收集联邦学习系统中设备参与可用性因能量有限而波动，需解决能耗问题。

Method: 提出基于用户电池水平的循环客户端参与的电池感知EHFL框架FedBacys，通过聚类和顺序调度客户端；还提出更节能的变体FedBacys - Odd，允许客户端选择性参与。

Result: 对框架进行收敛分析，数值实验表明比现有算法能效更高、更具鲁棒性。

Conclusion: FedBacys和FedBacys - Odd框架能有效降低能耗，提升学习稳定性，且不影响性能。

Abstract: Federated Learning (FL) is a powerful paradigm for distributed learning, but its increasing complexity leads to significant energy consumption from client-side computations for training models. In particular, the challenge is critical in energy-harvesting FL (EHFL) systems where participation availability of each device oscillates due to limited energy. To address this, we propose FedBacys, a battery-aware EHFL framework using cyclic client participation based on users' battery levels. By clustering clients and scheduling them sequentially, FedBacys minimizes redundant computations, reduces system-wide energy usage, and improves learning stability. We also introduce FedBacys-Odd, a more energy-efficient variant that allows clients to participate selectively, further reducing energy costs without compromising performance. We provide a convergence analysis for our framework and demonstrate its superior energy efficiency and robustness compared to existing algorithms through numerical experiments.

</details>


### [296] [Quantile Q-Learning: Revisiting Offline Extreme Q-Learning with Quantile Regression](https://arxiv.org/abs/2511.11973)
*Xinming Gao,Shangzhe Li,Yujin Cai,Wenwu Yu*

Main category: cs.LG

TL;DR: 本文针对XQL和MXQL在离线强化学习中的局限，提出通过分位数回归估计温度系数β和价值正则化技术，实验表明算法性能好且训练稳定。


<details>
  <summary>Details</summary>
Motivation: XQL和MXQL在离线强化学习中需要大量针对特定数据集和领域的超参数调整，且训练不稳定，需改进。

Method: 提出通过分位数回归在温和假设下估计温度系数β，引入具有温和泛化性的价值正则化技术。

Result: 算法在D4RL和NeoRL2等基准任务上取得有竞争力或更优的性能，训练动态稳定，且在所有数据集和领域使用一致超参数集。

Conclusion: 所提算法有效解决了XQL和MXQL的问题，在离线强化学习中有良好表现。

Abstract: Offline reinforcement learning (RL) enables policy learning from fixed datasets without further environment interaction, making it particularly valuable in high-risk or costly domains. Extreme $Q$-Learning (XQL) is a recent offline RL method that models Bellman errors using the Extreme Value Theorem, yielding strong empirical performance. However, XQL and its stabilized variant MXQL suffer from notable limitations: both require extensive hyperparameter tuning specific to each dataset and domain, and also exhibit instability during training. To address these issues, we proposed a principled method to estimate the temperature coefficient $β$ via quantile regression under mild assumptions. To further improve training stability, we introduce a value regularization technique with mild generalization, inspired by recent advances in constrained value learning. Experimental results demonstrate that the proposed algorithm achieves competitive or superior performance across a range of benchmark tasks, including D4RL and NeoRL2, while maintaining stable training dynamics and using a consistent set of hyperparameters across all datasets and domains.

</details>


### [297] [ReCast: Reliability-aware Codebook Assisted Lightweight Time Series Forecasting](https://arxiv.org/abs/2511.11991)
*Xiang Ma,Taihua Chen,Pengcheng Wang,Xuemei Li,Caiming Zhang*

Main category: cs.LG

TL;DR: 提出ReCast框架用于时间序列预测，利用局部形状实现轻量级和鲁棒预测，实验显示其性能优于SOTA模型。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测方法对局部复杂动态模式效果不佳，且模型复杂度高，不适用于实时或资源受限环境。

Method: 提出ReCast框架，通过可学习码本进行逐块量化将局部模式编码为离散嵌入，采用双路径架构，还有可靠性感知的码本更新策略。

Result: ReCast在准确性、效率和对分布偏移的适应性方面优于SOTA模型。

Conclusion: ReCast是一种有效的时间序列预测框架，能应对复杂模式和分布变化。

Abstract: Time series forecasting is crucial for applications in various domains. Conventional methods often rely on global decomposition into trend, seasonal, and residual components, which become ineffective for real-world series dominated by local, complex, and highly dynamic patterns. Moreover, the high model complexity of such approaches limits their applicability in real-time or resource-constrained environments. In this work, we propose a novel \textbf{RE}liability-aware \textbf{C}odebook-\textbf{AS}sisted \textbf{T}ime series forecasting framework (\textbf{ReCast}) that enables lightweight and robust prediction by exploiting recurring local shapes. ReCast encodes local patterns into discrete embeddings through patch-wise quantization using a learnable codebook, thereby compactly capturing stable regular structures. To compensate for residual variations not preserved by quantization, ReCast employs a dual-path architecture comprising a quantization path for efficient modeling of regular structures and a residual path for reconstructing irregular fluctuations. A central contribution of ReCast is a reliability-aware codebook update strategy, which incrementally refines the codebook via weighted corrections. These correction weights are derived by fusing multiple reliability factors from complementary perspectives by a distributionally robust optimization (DRO) scheme, ensuring adaptability to non-stationarity and robustness to distribution shifts. Extensive experiments demonstrate that ReCast outperforms state-of-the-art (SOTA) models in accuracy, efficiency, and adaptability to distribution shifts.

</details>


### [298] [Selecting Fine-Tuning Examples by Quizzing VLMs](https://arxiv.org/abs/2511.12002)
*Tenghao Ji,Eytan Adar*

Main category: cs.LG

TL;DR: 提出QZLoRA框架选择图像进行低秩适配，能以更少样本生成更好图像，结合视觉推理与参数高效微调有前景。


<details>
  <summary>Details</summary>
Motivation: 解决微调文本到图像扩散模型时选择优质示例的难题，避免使用不同质量图像集导致输出不佳。

Method: 提出QZLoRA框架，利用QuizRank方法，将图像视为‘教育干预’并‘测验’视觉语言模型来自动对图像排序。

Result: QZLoRA能以更少样本生成更好对齐、逼真的图像，微调模型也能生成有代表性的风格化图像。

Conclusion: 结合自动化视觉推理与参数高效微调用于主题自适应生成建模有前景。

Abstract: A challenge in fine-tuning text-to-image diffusion models for specific topics is to select good examples. Fine-tuning from image sets of varying quality, such as Wikipedia Commons, will often produce poor output. However, training images that \textit{do} exemplify the target concept (e.g., a \textit{female Mountain Bluebird}) help ensure that the generated images are similarly representative (e.g., have the prototypical blue-wings and gray chest). In this work, we propose QZLoRA, a framework to select images for low-rank adaptation (LoRA). The approach leverages QuizRank, a method to automatically rank images by treating them as an `educational intervention' and `quizzing' a VLM. We demonstrate that QZLoRA can produce better aligned, photorealistic images with fewer samples. We also show that these fine-tuned models can produce stylized that are similarly representative (i.e., illustrations). Our results highlight the promise of combining automated visual reasoning with parameter-efficient fine-tuning for topic-adaptive generative modeling.

</details>


### [299] [EARL: Entropy-Aware RL Alignment of LLMs for Reliable RTL Code Generation](https://arxiv.org/abs/2511.12033)
*Jiahe Shi,Zhengqi Gao,Ching-Yun Ko,Duane Boning*

Main category: cs.LG

TL;DR: 提出EARL框架用于Verilog生成，通过熵感知强化学习提升结构化RTL代码生成的功能通过率和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在RTL设计中存在语法错误、功能幻觉等问题，传统强化学习在长代码序列中梯度分散，需改进。

Method: 提出EARL框架，利用可验证奖励信号进行策略优化，引入熵引导的选择性更新，将策略梯度集中到高熵标记。

Result: 在VerilogEval和RTLLM实验中，EARL比先前LLM基线功能通过率最高提升14.7%，减少不必要更新，提高训练稳定性。

Conclusion: 聚焦关键高不确定性标记的强化学习能为结构化RTL代码生成带来更可靠和有针对性的策略改进。

Abstract: Recent advances in large language models (LLMs) have demonstrated significant potential in hardware design automation, particularly in using natural language to synthesize Register-Transfer Level (RTL) code. Despite this progress, a gap remains between model capability and the demands of real-world RTL design, including syntax errors, functional hallucinations, and weak alignment to designer intent. Reinforcement Learning with Verifiable Rewards (RLVR) offers a promising approach to bridge this gap, as hardware provides executable and formally checkable signals that can be used to further align model outputs with design intent. However, in long, structured RTL code sequences, not all tokens contribute equally to functional correctness, and naïvely spreading gradients across all tokens dilutes learning signals. A key insight from our entropy analysis in RTL generation is that only a small fraction of tokens (e.g., always, if, assign, posedge) exhibit high uncertainty and largely influence control flow and module structure. To address these challenges, we present EARL, an Entropy-Aware Reinforcement Learning framework for Verilog generation. EARL performs policy optimization using verifiable reward signals and introduces entropy-guided selective updates that gate policy gradients to high-entropy tokens. This approach preserves training stability and concentrates gradient updates on functionally important regions of code. Our experiments on VerilogEval and RTLLM show that EARL improves functional pass rates over prior LLM baselines by up to 14.7%, while reducing unnecessary updates and improving training stability. These results indicate that focusing RL on critical, high-uncertainty tokens enables more reliable and targeted policy improvement for structured RTL code generation.

</details>


### [300] [Mesh-based Super-resolution of Detonation Flows with Multiscale Graph Transformers](https://arxiv.org/abs/2511.12041)
*Shivam Barwey,Pinaki Pal*

Main category: cs.LG

TL;DR: 本文提出一种多尺度图变压器方法用于反应流基于网格的超分辨率重建，在二维爆轰传播测试问题中展示了高准确性和优越性能。


<details>
  <summary>Details</summary>
Motivation: 利用先进数据驱动技术进行超分辨率流重建在多种应用中具有重要价值，现有方法可能存在不足，需开发新方法。

Method: 开发了一种多尺度图变压器方法（SR - GT），利用基于图的流场表示，变压器骨干捕捉长程依赖，采用独特元素局部（+邻域）图表示并进行标记化处理。

Result: 在二维爆轰传播测试问题中，SR - GT对反应流场特征提供了高超分辨率精度，性能优于传统基于插值的超分辨率方案。

Conclusion: SR - GT方法在反应流超分辨率重建中具有良好效果和应用潜力。

Abstract: Super-resolution flow reconstruction using state-of-the-art data-driven techniques is valuable for a variety of applications, such as subgrid/subfilter closure modeling, accelerating spatiotemporal forecasting, data compression, and serving as an upscaling tool for sparse experimental measurements. In the present work, a first-of-its-kind multiscale graph transformer approach is developed for mesh-based super-resolution (SR-GT) of reacting flows. The novel data-driven modeling paradigm leverages a graph-based flow-field representation compatible with complex geometries and non-uniform/unstructured grids. Further, the transformer backbone captures long-range dependencies between different parts of the low-resolution flow-field, identifies important features, and then generates the super-resolved flow-field that preserves those features at a higher resolution. The performance of SR-GT is demonstrated in the context of spectral-element-discretized meshes for a challenging test problem of 2D detonation propagation within a premixed hydrogen-air mixture exhibiting highly complex multiscale reacting flow behavior. The SR-GT framework utilizes a unique element-local (+ neighborhood) graph representation for the coarse input, which is then tokenized before being processed by the transformer component to produce the fine output. It is demonstrated that SR-GT provides high super-resolution accuracy for reacting flow-field features and superior performance compared to traditional interpolation-based SR schemes.

</details>


### [301] [Improving Graph Embeddings in Machine Learning Using Knowledge Completion with Validation in a Case Study on COVID-19 Spread](https://arxiv.org/abs/2511.12071)
*Rosario Napoli,Gabriele Morabito,Antonio Celesti,Massimo Villari,Maria Fazio*

Main category: cs.LG

TL;DR: 提出整合知识补全阶段的图机器学习管道，处理图嵌入未利用隐含知识问题，实验证明能改变嵌入空间几何，提升图表示质量。


<details>
  <summary>Details</summary>
Motivation: 现有图嵌入方法基于显式拓扑和特征，会遗漏稀疏数据集中隐含知识，影响图结构和表示。

Method: 提出整合知识补全（KC）阶段的图机器学习管道，用基于衰减的推理函数建模隐藏连接，重塑图拓扑。

Result: 该管道显著改变了嵌入空间几何。

Conclusion: 引入该管道不是简单的丰富，而是重新定义图表示质量的变革性步骤。

Abstract: The rise of graph-structured data has driven major advances in Graph Machine Learning (GML), where graph embeddings (GEs) map features from Knowledge Graphs (KGs) into vector spaces, enabling tasks like node classification and link prediction. However, since GEs are derived from explicit topology and features, they may miss crucial implicit knowledge hidden in seemingly sparse datasets, affecting graph structure and their representation. We propose a GML pipeline that integrates a Knowledge Completion (KC) phase to uncover latent dataset semantics before embedding generation. Focusing on transitive relations, we model hidden connections with decay-based inference functions, reshaping graph topology, with consequences on embedding dynamics and aggregation processes in GraphSAGE and Node2Vec. Experiments show that our GML pipeline significantly alters the embedding space geometry, demonstrating that its introduction is not just a simple enrichment but a transformative step that redefines graph representation quality.

</details>


### [302] [Treatment Stitching with Schrödinger Bridge for Enhancing Offline Reinforcement Learning in Adaptive Treatment Strategies](https://arxiv.org/abs/2511.12075)
*Dong-Hee Shin,Deok-Joong Lee,Young-Han Son,Tae-Eui Kam*

Main category: cs.LG

TL;DR: 提出Treatment Stitching数据增强框架，通过拼接治疗数据段和生成桥接轨迹增强离线强化学习数据集，提升优化自适应治疗策略能力，实验证明有效且理论上保持临床有效性。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在线试错机制在临床不可行，离线强化学习受数据稀缺限制，需解决数据不足问题以优化自适应治疗策略。

Method: 提出Treatment Stitching框架，识别相似患者中间状态拼接数据段，用Schrödinger桥方法生成桥接轨迹，将合成轨迹加入原数据集。

Result: 在多个治疗数据集上的实验表明TreatStitch能提升离线强化学习性能。

Conclusion: TreatStitch可增强离线强化学习性能，且理论上能保持临床有效性，避免分布外过渡。

Abstract: Adaptive treatment strategies (ATS) are sequential decision-making processes that enable personalized care by dynamically adjusting treatment decisions in response to evolving patient symptoms. While reinforcement learning (RL) offers a promising approach for optimizing ATS, its conventional online trial-and-error learning mechanism is not permissible in clinical settings due to risks of harm to patients. Offline RL tackles this limitation by learning policies exclusively from historical treatment data, but its performance is often constrained by data scarcity-a pervasive challenge in clinical domains. To overcome this, we propose Treatment Stitching (TreatStitch), a novel data augmentation framework that generates clinically valid treatment trajectories by intelligently stitching segments from existing treatment data. Specifically, TreatStitch identifies similar intermediate patient states across different trajectories and stitches their respective segments. Even when intermediate states are too dissimilar to stitch directly, TreatStitch leverages the Schrödinger bridge method to generate smooth and energy-efficient bridging trajectories that connect dissimilar states. By augmenting these synthetic trajectories into the original dataset, offline RL can learn from a more diverse dataset, thereby improving its ability to optimize ATS. Extensive experiments across multiple treatment datasets demonstrate the effectiveness of TreatStitch in enhancing offline RL performance. Furthermore, we provide a theoretical justification showing that TreatStitch maintains clinical validity by avoiding out-of-distribution transitions.

</details>


### [303] [SenseRay-3D: Generalizable and Physics-Informed Framework for End-to-End Indoor Propagation Modeling](https://arxiv.org/abs/2511.12092)
*Yu Zheng,Kezhi Wang,Wenji Xi,Gang Yu,Jiming Chen,Jie Zhang*

Main category: cs.LG

TL;DR: 本文提出SenseRay - 3D框架，可从RGB - D扫描直接预测3D路径损耗热图，实验证明其可扩展性、效率和物理一致性。


<details>
  <summary>Details</summary>
Motivation: 现有室内无线电传播建模方法依赖手动建模，可扩展性和效率有限。

Method: 构建传感驱动的体素化场景表示，用基于SwinUNETR的神经网络处理，开发综合合成室内传播数据集。

Result: SenseRay - 3D在未见环境中平均绝对误差4.27 dB，每样本实时推理时间217 ms。

Conclusion: SenseRay - 3D为室内传播建模开辟新路径，超越EM DeepRay框架。

Abstract: Modeling indoor radio propagation is crucial for wireless network planning and optimization. However, existing approaches often rely on labor-intensive manual modeling of geometry and material properties, resulting in limited scalability and efficiency. To overcome these challenges, this paper presents SenseRay-3D, a generalizable and physics-informed end-to-end framework that predicts three-dimensional (3D) path-loss heatmaps directly from RGB-D scans, thereby eliminating the need for explicit geometry reconstruction or material annotation. The proposed framework builds a sensing-driven voxelized scene representation that jointly encodes occupancy, electromagnetic material characteristics, and transmitter-receiver geometry, which is processed by a SwinUNETR-based neural network to infer environmental path-loss relative to free-space path-loss. A comprehensive synthetic indoor propagation dataset is further developed to validate the framework and to serve as a standardized benchmark for future research. Experimental results show that SenseRay-3D achieves a mean absolute error of 4.27 dB on unseen environments and supports real-time inference at 217 ms per sample, demonstrating its scalability, efficiency, and physical consistency. SenseRay-3D paves a new path for sense-driven, generalizable, and physics-consistent modeling of indoor propagation, marking a major leap beyond our pioneering EM DeepRay framework.

</details>


### [304] [To Align or Not to Align: Strategic Multimodal Representation Alignment for Optimal Performance](https://arxiv.org/abs/2511.12121)
*Wanlong Fang,Tianle Zhang,Alvin Chan*

Main category: cs.LG

TL;DR: 本文研究显式对齐对多模态学习中模型性能和表征对齐的影响，发现其与数据特征有关，确定了最优对齐强度并给出应用指导。


<details>
  <summary>Details</summary>
Motivation: 以往研究多为观察性，未系统研究不同模态表征显式对齐的直接影响，因此开展本研究。

Method: 引入可控对比学习模块，在训练中精确控制对齐强度。

Result: 在不同数据特征的合成和真实数据集上发现，显式对齐对单模态模型性能的影响与数据特征有关，存在平衡特定模态信号和共享冗余的最优对齐强度。

Conclusion: 为何时以及如何应用显式对齐以实现最优单模态编码器性能提供了实用指导。

Abstract: Multimodal learning often relies on aligning representations across modalities to enable effective information integration, an approach traditionally assumed to be universally beneficial. However, prior research has primarily taken an observational approach, examining naturally occurring alignment in multimodal data and exploring its correlation with model performance, without systematically studying the direct effects of explicitly enforced alignment between representations of different modalities. In this work, we investigate how explicit alignment influences both model performance and representation alignment under different modality-specific information structures. Specifically, we introduce a controllable contrastive learning module that enables precise manipulation of alignment strength during training, allowing us to explore when explicit alignment improves or hinders performance. Our results on synthetic and real datasets under different data characteristics show that the impact of explicit alignment on the performance of unimodal models is related to the characteristics of the data: the optimal level of alignment depends on the amount of redundancy between the different modalities. We identify an optimal alignment strength that balances modality-specific signals and shared redundancy in the mixed information distributions. This work provides practical guidance on when and how explicit alignment should be applied to achieve optimal unimodal encoder performance.

</details>


### [305] [Dynamic Anomaly Identification in Accounting Transactions via Multi-Head Self-Attention Networks](https://arxiv.org/abs/2511.12122)
*Yi Wang,Ruoyi Fang,Anzhuo Xie,Hanrui Feng,Jianlin Lai*

Main category: cs.LG

TL;DR: 本文提出基于Transformer的实时检测方法用于会计交易动态异常检测，实验表明该方法优于基线模型，性能稳定。


<details>
  <summary>Details</summary>
Motivation: 解决复杂交易环境中会计交易动态异常检测时隐藏异常行为和高时效性要求的问题。

Method: 将多维记录表示为时间序列矩阵对会计交易数据建模，用嵌入层和位置编码实现输入低维映射；构建多头自注意力序列建模结构捕捉全局依赖；集成前馈层和正则化策略实现深度特征表示和异常概率估计。

Result: 在公开数据集上实验，该方法在AUC、F1分数、精确率和召回率上优于基线模型，在不同环境条件和数据扰动下性能稳定。

Conclusion: 基于Transformer的框架适用于会计交易动态异常检测，为智能金融风险控制和审计提供方法支持。

Abstract: This study addresses the problem of dynamic anomaly detection in accounting transactions and proposes a real-time detection method based on a Transformer to tackle the challenges of hidden abnormal behaviors and high timeliness requirements in complex trading environments. The approach first models accounting transaction data by representing multi-dimensional records as time-series matrices and uses embedding layers and positional encoding to achieve low-dimensional mapping of inputs. A sequence modeling structure with multi-head self-attention is then constructed to capture global dependencies and aggregate features from multiple perspectives, thereby enhancing the ability to detect abnormal patterns. The network further integrates feed-forward layers and regularization strategies to achieve deep feature representation and accurate anomaly probability estimation. To validate the effectiveness of the method, extensive experiments were conducted on a public dataset, including comparative analysis, hyperparameter sensitivity tests, environmental sensitivity tests, and data sensitivity tests. Results show that the proposed method outperforms baseline models in AUC, F1-Score, Precision, and Recall, and maintains stable performance under different environmental conditions and data perturbations. These findings confirm the applicability and advantages of the Transformer-based framework for dynamic anomaly detection in accounting transactions and provide methodological support for intelligent financial risk control and auditing.

</details>


### [306] [HCPO: Hierarchical Conductor-Based Policy Optimization in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.12123)
*Zejiao Liu,Junqi Tu,Yitian Hong,Luolin Xiong,Yaochu Jin,Yang Tang,Fangfei Li*

Main category: cs.LG

TL;DR: 提出基于指挥者的联合策略框架和HCPO算法，解决合作多智能体强化学习中联合策略探索问题，实验显示其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有合作多智能体强化学习方法在更新联合策略时缺乏智能体间协调，限制了联合策略的表达能力和探索能力。

Method: 提出基于指挥者的联合策略框架，开发HCPO算法指导策略更新，有理论保证优化过程单调性，通过部署本地指挥者消除执行时智能体间通信。

Result: 在三个具有挑战性的基准测试中，HCPO在合作效率和稳定性方面优于竞争的多智能体强化学习基线。

Conclusion: HCPO算法能有效解决合作多智能体强化学习中联合策略的探索问题，提升性能。

Abstract: In cooperative Multi-Agent Reinforcement Learning (MARL), efficient exploration is crucial for optimizing the performance of joint policy. However, existing methods often update joint policies via independent agent exploration, without coordination among agents, which inherently constrains the expressive capacity and exploration of joint policies. To address this issue, we propose a conductor-based joint policy framework that directly enhances the expressive capacity of joint policies and coordinates exploration. In addition, we develop a Hierarchical Conductor-based Policy Optimization (HCPO) algorithm that instructs policy updates for the conductor and agents in a direction aligned with performance improvement. A rigorous theoretical guarantee further establishes the monotonicity of the joint policy optimization process. By deploying local conductors, HCPO retains centralized training benefits while eliminating inter-agent communication during execution. Finally, we evaluate HCPO on three challenging benchmarks: StarCraftII Multi-agent Challenge, Multi-agent MuJoCo, and Multi-agent Particle Environment. The results indicate that HCPO outperforms competitive MARL baselines regarding cooperative efficiency and stability.

</details>


### [307] [FairGSE: Fairness-Aware Graph Neural Network without High False Positive Rates](https://arxiv.org/abs/2511.12132)
*Zhenqiang Ye,Jinjie Lu,Tianlong Gu,Fengrui Hao,Xuemin Wang*

Main category: cs.LG

TL;DR: 现有公平感知GNN在追求公平指标时忽视负标签预测能力，FPR高，本文提出FairGSE框架，实验显示其能降低FPR。


<details>
  <summary>Details</summary>
Motivation: 现有公平感知GNN追求公平指标时忽视预测负标签能力，导致FPR极高，在高风险场景有负面影响。

Method: 提出Fair GNN via Structural Entropy (FairGSE)框架，最大化二维结构熵（2D - SE）以在不忽视假阳性情况下提高公平性。

Result: 在多个真实数据集上实验表明，FairGSE与现有公平感知GNN相比，FPR降低39%，公平性提升相当。

Conclusion: 分类性能在提升公平时应仔细校准，而不是简单限制准确率损失，FairGSE是有效的解决方案。

Abstract: Graph neural networks (GNNs) have emerged as the mainstream paradigm for graph representation learning due to their effective message aggregation. However, this advantage also amplifies biases inherent in graph topology, raising fairness concerns. Existing fairness-aware GNNs provide satisfactory performance on fairness metrics such as Statistical Parity and Equal Opportunity while maintaining acceptable accuracy trade-offs. Unfortunately, we observe that this pursuit of fairness metrics neglects the GNN's ability to predict negative labels, which renders their predictions with extremely high False Positive Rates (FPR), resulting in negative effects in high-risk scenarios. To this end, we advocate that classification performance should be carefully calibrated while improving fairness, rather than simply constraining accuracy loss. Furthermore, we propose Fair GNN via Structural Entropy (\textbf{FairGSE}), a novel framework that maximizes two-dimensional structural entropy (2D-SE) to improve fairness without neglecting false positives. Experiments on several real-world datasets show FairGSE reduces FPR by 39\% vs. state-of-the-art fairness-aware GNNs, with comparable fairness improvement.

</details>


### [308] [Fusion-ResNet: A Lightweight multi-label NILM Model Using PCA-ICA Feature Fusion](https://arxiv.org/abs/2511.12139)
*Sahar Moghimian Hoosh,Ilia Kamyshev,Henni Ouerdane*

Main category: cs.LG

TL;DR: 提出用于NILM分类任务的端到端框架，含新特征提取方法和轻量级网络，模型表现优且对多设备并发情况较稳健。


<details>
  <summary>Details</summary>
Motivation: 解决现实中NILM部署面临的过拟合、模型泛化性低和同时分解大量电器功耗等挑战。

Method: 提出端到端框架，引入融合ICA和PCA特征的新特征提取方法，提出轻量级多标签NILM分类架构Fusion - ResNet。

Result: 基于特征的模型平均和针对不同电器的F1分数更高，训练和推理时间更短，Fusion - ResNet对多达15个并发电器的压力条件相对稳健。

Conclusion: 所提框架和模型有效，能应对NILM部署挑战。

Abstract: Non-intrusive load monitoring (NILM) is an advanced load monitoring technique that uses data-driven algorithms to disaggregate the total power consumption of a household into the consumption of individual appliances. However, real-world NILM deployment still faces major challenges, including overfitting, low model generalization, and disaggregating a large number of appliances operating at the same time. To address these challenges, this work proposes an end-to-end framework for the NILM classification task, which consists of high-frequency labeled data, a feature extraction method, and a lightweight neural network. Within this framework, we introduce a novel feature extraction method that fuses Independent Component Analysis (ICA) and Principal Component Analysis (PCA) features. Moreover, we propose a lightweight architecture for multi-label NILM classification (Fusion-ResNet). The proposed feature-based model achieves a higher $F1$ score on average and across different appliances compared to state-of-the-art NILM classifiers while minimizing the training and inference time. Finally, we assessed the performance of our model against baselines with a varying number of simultaneously active devices. Results demonstrate that Fusion-ResNet is relatively robust to stress conditions with up to 15 concurrently active appliances.

</details>


### [309] [Variation-Bounded Loss for Noise-Tolerant Learning](https://arxiv.org/abs/2511.12143)
*Jialiang Wang,Xiong Zhou,Xianming Liu,Gangfeng Hu,Deming Zhai,Junjun Jiang,Haoliang Li*

Main category: cs.LG

TL;DR: 引入变异率作为损失函数鲁棒性的新属性，提出VBL损失函数，理论分析表明变异率小更鲁棒，实验验证方法有效灵活。


<details>
  <summary>Details</summary>
Motivation: 缓解有监督学习中噪声标签的负面影响，解决现有问题。

Method: 引入变异率，提出VBL损失函数，对变异率进行理论分析，将常用损失函数转化为有界变异形式。

Result: 理论证明较小变异率带来更好鲁棒性，揭示变异率能放松对称条件和实现非对称条件，实验表明方法有效灵活。

Conclusion: 基于变异率的VBL损失函数能有效缓解噪声标签影响，具有良好效果和灵活性。

Abstract: Mitigating the negative impact of noisy labels has been aperennial issue in supervised learning. Robust loss functions have emerged as a prevalent solution to this problem. In this work, we introduce the Variation Ratio as a novel property related to the robustness of loss functions, and propose a new family of robust loss functions, termed Variation-Bounded Loss (VBL), which is characterized by a bounded variation ratio. We provide theoretical analyses of the variation ratio, proving that a smaller variation ratio would lead to better robustness. Furthermore, we reveal that the variation ratio provides a feasible method to relax the symmetric condition and offers a more concise path to achieve the asymmetric condition. Based on the variation ratio, we reformulate several commonly used loss functions into a variation-bounded form for practical applications. Positive experiments on various datasets exhibit the effectiveness and flexibility of our approach.

</details>


### [310] [Open Banking Foundational Model: Learning Language Representations from Few Financial Transactions](https://arxiv.org/abs/2511.12154)
*Gustavo Polleti,Marlesson Santana,Eduardo Fontes*

Main category: cs.LG

TL;DR: 提出金融交易多模态基础模型，在多方面表现优异且能跨地区和机构泛化，凸显自监督模型潜力。


<details>
  <summary>Details</summary>
Motivation: 构建能处理金融交易中结构化属性和非结构化文本描述的模型，提升金融应用效果。

Method: 将掩码语言建模应用于交易序列。

Result: 模型优于经典特征工程和离散事件序列方法，在数据稀缺的开放银行场景有效，能跨地区和机构泛化。

Conclusion: 自监督模型在欺诈预防、信用风险和客户洞察等金融应用中有巨大潜力。

Abstract: We introduced a multimodal foundational model for financial transactions that integrates both structured attributes and unstructured textual descriptions into a unified representation. By adapting masked language modeling to transaction sequences, we demonstrated that our approach not only outperforms classical feature engineering and discrete event sequence methods but is also particularly effective in data-scarce Open Banking scenarios. To our knowledge, this is the first large-scale study across thousands of financial institutions in North America, providing evidence that multimodal representations can generalize across geographies and institutions. These results highlight the potential of self-supervised models to advance financial applications ranging from fraud prevention and credit risk to customer insights

</details>


### [311] [Rethinking Deep Alignment Through The Lens Of Incomplete Learning](https://arxiv.org/abs/2511.12155)
*Thong Bach,Dung Nguyen,Thao Minh Le,Truyen Tran*

Main category: cs.LG

TL;DR: 分析大语言模型安全对齐不足原因，引入指标并提出方法提升对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在大量安全对齐后仍存在对抗攻击漏洞，需分析原因并解决。

Method: 分析自回归训练中位置依赖梯度减弱导致信号衰减问题，引入基础偏好标记作为指标，用自适应惩罚和混合教师蒸馏的目标完成方法。

Result: 在Llama和Qwen模型家族中攻击成功率降低48 - 98%，且保留通用能力。

Conclusion: 对安全对齐方法的根本局限有了机制理解并提供实用解决方案。

Abstract: Large language models exhibit systematic vulnerabilities to adversarial attacks despite extensive safety alignment. We provide a mechanistic analysis revealing that position-dependent gradient weakening during autoregressive training creates signal decay, leading to incomplete safety learning where safety training fails to transform model preferences in later response regions fully. We introduce base-favored tokens -- vocabulary elements where base models assign higher probability than aligned models -- as computational indicators of incomplete safety learning and develop a targeted completion method that addresses undertrained regions through adaptive penalties and hybrid teacher distillation. Experimental evaluation across Llama and Qwen model families demonstrates dramatic improvements in adversarial robustness, with 48--98% reductions in attack success rates while preserving general capabilities. These results establish both a mechanistic understanding and practical solutions for fundamental limitations in safety alignment methodologies.

</details>


### [312] [Data-Efficient Self-Supervised Algorithms for Fine-Grained Birdsong Analysis](https://arxiv.org/abs/2511.12158)
*Houtan Ghaffari,Lukas Rauch,Paul Devos*

Main category: cs.LG

TL;DR: 提出Residual - MLP - RNN架构和三阶段训练流程用于鸟鸣注释，在极端标签稀缺场景下验证了其有效性并评估自监督嵌入潜力。


<details>
  <summary>Details</summary>
Motivation: 鸟鸣研究需精确标注数据，自动化且节省标注成本的方法有需求。

Method: 提出Residual - MLP - RNN架构，采用三阶段训练流程，包括自监督学习、有监督训练和半监督后训练。

Result: 在金丝雀复杂鸟鸣极端标签稀缺场景下验证了方法的性能。

Conclusion: 该方法有效，自监督嵌入对线性探测和无监督鸟鸣分析有潜力。

Abstract: Many bioacoustics, neuroscience, and linguistics research utilize birdsongs as proxy models to acquire knowledge in diverse areas. Developing models generally requires precisely annotated data at the level of syllables. Hence, automated and data-efficient methods that reduce annotation costs are in demand. This work presents a lightweight, yet performant neural network architecture for birdsong annotation called Residual-MLP-RNN. Then, it presents a robust three-stage training pipeline for developing reliable deep birdsong syllable detectors with minimal expert labor. The first stage is self-supervised learning from unlabeled data. Two of the most successful pretraining paradigms are explored, namely, masked prediction and online clustering. The second stage is supervised training with effective data augmentations to create a robust model for frame-level syllable detection. The third stage is semi-supervised post-training, which leverages the unlabeled data again. However, unlike the initial phase, this time it is aligned with the downstream task. The performance of this data-efficient approach is demonstrated for the complex song of the Canary in extreme label-scarcity scenarios. Canary has one of the most difficult songs to annotate, which implicitly validates the method for other birds. Finally, the potential of self-supervised embeddings is assessed for linear probing and unsupervised birdsong analysis.

</details>


### [313] [FGM optimization in complex domains using Gaussian process regression based profile generation algorithm](https://arxiv.org/abs/2511.12171)
*Chaitanya Kumar Konda,Piyush Agrawal,Shivansh Srivastava,Manish Agrawal*

Main category: cs.LG

TL;DR: 提出基于高斯过程回归的通用体积分数分布生成算法设计任意形状域功能梯度材料，与遗传算法结合优化并通过热弹性优化示例验证有效性。


<details>
  <summary>Details</summary>
Motivation: 解决为任意形状域设计功能梯度材料的挑战。

Method: 提出基于高斯过程回归的体积分数分布生成算法，将其与修改后的遗传算法结合。

Result: 算法能处理复杂形状域，生成多样且平滑的功能梯度材料分布，设计空间大。

Conclusion: 所提分布生成算法和优化框架有效。

Abstract: This manuscript addresses the challenge of designing functionally graded materials (FGMs) for arbitrary-shaped domains. Towards this goal, the present work proposes a generic volume fraction profile generation algorithm based on Gaussian Process Regression (GPR). The proposed algorithm can handle complex-shaped domains and generate smooth FGM profiles while adhering to the specified volume fraction values at boundaries/part of boundaries. The resulting design space from GPR comprises diverse profiles, enhancing the potential for discovering optimal configurations. Further, the algorithm allows the user to control the smoothness of the underlying profiles and the size of the design space through a length scale parameter. Further, the proposed profile generation scheme is coupled with the genetic algorithm to find the optimum FGM profiles for a given application. To make the genetic algorithm consistent with the GPR profile generation scheme, the standard simulated binary crossover operator in the genetic algorithm has been modified with a projection operator. We present numerous thermoelastic optimization examples to demonstrate the efficacy of the proposed profile generation algorithm and optimization framework.

</details>


### [314] [TSGDiff: Rethinking Synthetic Time Series Generation from a Pure Graph Perspective](https://arxiv.org/abs/2511.12174)
*Lifeng Shen,Xuyang Li,Lele Long*

Main category: cs.LG

TL;DR: 提出TSGDiff框架从图视角生成时间序列，提出Topo - FID评估指标，实验证明其能生成高质量合成时间序列。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成时间序列数据因需捕捉复杂时间依赖和结构模式而具有挑战性。

Method: 将时间序列表示为动态图，用图神经网络编解码器构建潜在空间；提出Topo - FID指标，含图编辑相似度和结构熵相似度两个子指标。

Result: 在真实数据集实验中，TSGDiff生成了高质量合成时间序列，保留了时间依赖和结构完整性。

Conclusion: TSGDiff推动了合成时间序列生成领域的发展。

Abstract: Diffusion models have shown great promise in data generation, yet generating time series data remains challenging due to the need to capture complex temporal dependencies and structural patterns. In this paper, we present \textit{TSGDiff}, a novel framework that rethinks time series generation from a graph-based perspective. Specifically, we represent time series as dynamic graphs, where edges are constructed based on Fourier spectrum characteristics and temporal dependencies. A graph neural network-based encoder-decoder architecture is employed to construct a latent space, enabling the diffusion process to model the structural representation distribution of time series effectively. Furthermore, we propose the Topological Structure Fidelity (Topo-FID) score, a graph-aware metric for assessing the structural similarity of time series graph representations. Topo-FID integrates two sub-metrics: Graph Edit Similarity, which quantifies differences in adjacency matrices, and Structural Entropy Similarity, which evaluates the entropy of node degree distributions. This comprehensive metric provides a more accurate assessment of structural fidelity in generated time series. Experiments on real-world datasets demonstrate that \textit{TSGDiff} generates high-quality synthetic time series data generation, faithfully preserving temporal dependencies and structural integrity, thereby advancing the field of synthetic time series generation.

</details>


### [315] [Scaling Law Analysis in Federated Learning: How to Select the Optimal Model Size?](https://arxiv.org/abs/2511.12188)
*Xuanyu Chen,Nan Yang,Shuai Wang,Dong Yuan*

Main category: cs.LG

TL;DR: 本文研究联邦学习场景下大模型扩展问题，推导泛化误差上界，分析最优模型大小与客户端数量关系，并用实验验证结果。


<details>
  <summary>Details</summary>
Motivation: 大模型训练面临高质量数据耗尽问题，联邦学习可利用边缘设备数据但在扩展大模型方面研究不足。

Method: 推导联邦环境下随机算法训练模型泛化误差的PAC - Bayes上界，找到使该上界最小的模型大小的解析解。

Result: 最优模型大小与客户端数量呈负幂律关系；相同训练计算量下采用联邦学习会降低模型泛化性能上界；联邦场景下估计最优模型大小应依赖客户端平均训练计算量。

Conclusion: 研究结果为将以往模型扩展经验推广到联邦学习场景提供了定性见解，且实验验证了理论结果的正确性。

Abstract: The recent success of large language models (LLMs) has sparked a growing interest in training large-scale models. As the model size continues to scale, concerns are growing about the depletion of high-quality, well-curated training data. This has led practitioners to explore training approaches like Federated Learning (FL), which can leverage the abundant data on edge devices while maintaining privacy. However, the decentralization of training datasets in FL introduces challenges to scaling large models, a topic that remains under-explored. This paper fills this gap and provides qualitative insights on generalizing the previous model scaling experience to federated learning scenarios. Specifically, we derive a PAC-Bayes (Probably Approximately Correct Bayesian) upper bound for the generalization error of models trained with stochastic algorithms in federated settings and quantify the impact of distributed training data on the optimal model size by finding the analytic solution of model size that minimizes this bound. Our theoretical results demonstrate that the optimal model size has a negative power law relationship with the number of clients if the total training compute is unchanged. Besides, we also find that switching to FL with the same training compute will inevitably reduce the upper bound of generalization performance that the model can achieve through training, and that estimating the optimal model size in federated scenarios should depend on the average training compute across clients. Furthermore, we also empirically validate the correctness of our results with extensive training runs on different models, network settings, and datasets.

</details>


### [316] [Evaluation of Multi- and Single-objective Learning Algorithms for Imbalanced Data](https://arxiv.org/abs/2511.12191)
*Szymon Wojciechowski,Michał Woźniak*

Main category: cs.LG

TL;DR: 文章指出多目标优化算法在机器学习任务中产生帕累托前沿解，选择单一解有挑战，且现有评估方法存在差距，提出新的评估算法的可靠方法。


<details>
  <summary>Details</summary>
Motivation: 现有多目标优化算法产生帕累托前沿解，选择单一解有挑战，且缺乏可靠的评估方法来比较返回单一解和帕累托前沿解的算法。

Method: 提出新的评估方法，仅关注算法比较，不涉及学习，选择部分算法辅助理解方法。

Result: 文章未提及具体结果。

Conclusion: 提出新的、可靠的评估算法的方法，可指出符合用户偏好的帕累托前沿解。

Abstract: Many machine learning tasks aim to find models that work well not for a single, but for a group of criteria, often opposing ones. One such example is imbalanced data classification, where, on the one hand, we want to achieve the best possible classification quality for data from the minority class without degrading the classification quality of the majority class. One solution is to propose an aggregate learning criterion and reduce the multi-objective learning task to a single-criteria optimization problem. Unfortunately, such an approach is characterized by ambiguity of interpretation since the value of the aggregated criterion does not indicate the value of the component criteria. Hence, there are more and more proposals for algorithms based on multi-objective optimization (MOO), which can simultaneously optimize multiple criteria. However, such an approach results in a set of multiple non-dominated solutions (Pareto front). The selection of a single solution from the Pareto front is a challenge itself, and much attention is paid to the issue of how to select it considering user preferences, as well as how to compare solutions returned by different MOO algorithms among themselves. Thus, a significant gap has been identified in the classifier evaluation methodology, i.e., how to reliably compare methods returning single solutions with algorithms returning solutions in the form of Pareto fronts.
  To fill the aforementioned gap, this article proposes a new, reliable way of evaluating algorithms based on multi-objective algorithms with methods that return single solutions while pointing out solutions from a Pareto front tailored to the user's preferences. This work focuses only on algorithm comparison, not their learning. The algorithms selected for this study are illustrative to help understand the proposed approach.

</details>


### [317] [MPD-SGR: Robust Spiking Neural Networks with Membrane Potential Distribution-Driven Surrogate Gradient Regularization](https://arxiv.org/abs/2511.12199)
*Runhao Jiang,Chengzhi Jiang,Rui Yan,Huajin Tang*

Main category: cs.LG

TL;DR: 研究膜电位分布（MPD）与替代梯度（SG）关系，提出MPD - SGR方法增强脉冲神经网络（SNNs）对抗攻击鲁棒性。


<details>
  <summary>Details</summary>
Motivation: SG方法使SNNs易受对抗攻击，而反映模型对输入扰动敏感性的梯度大小受MPD和SG函数相互作用影响，此方面研究不足。

Method: 理论分析MPD和SG关系，提出MPD - SGR方法，通过正则化MPD增强鲁棒性。

Result: 在多个图像分类基准和不同网络架构上实验表明，MPD - SGR方法显著增强SNNs对抗扰动的恢复能力，且在不同网络配置、SG函数变体和脉冲编码方案中具有强泛化性。

Conclusion: MPD - SGR方法能有效提升SNNs的鲁棒性和泛化性。

Abstract: The surrogate gradient (SG) method has shown significant promise in enhancing the performance of deep spiking neural networks (SNNs), but it also introduces vulnerabilities to adversarial attacks. Although spike coding strategies and neural dynamics parameters have been extensively studied for their impact on robustness, the critical role of gradient magnitude, which reflects the model's sensitivity to input perturbations, remains underexplored. In SNNs, the gradient magnitude is primarily determined by the interaction between the membrane potential distribution (MPD) and the SG function. In this study, we investigate the relationship between the MPD and SG and its implications for improving the robustness of SNNs. Our theoretical analysis reveals that reducing the proportion of membrane potential lying within the gradient-available range of the SG function effectively mitigates the sensitivity of SNNs to input perturbations. Building upon this insight, we propose a novel MPD-driven surrogate gradient regularization (MPD-SGR) method, which enhances robustness by explicitly regularizing the MPD based on its interaction with the SG function. Extensive experiments across multiple image classification benchmarks and diverse network architectures confirm that the MPD-SGR method significantly enhances the resilience of SNNs to adversarial perturbations and exhibits strong generalizability across diverse network configurations, SG function variants, and spike encoding schemes.

</details>


### [318] [AlignTree: Efficient Defense Against LLM Jailbreak Attacks](https://arxiv.org/abs/2511.12217)
*Gil Goren,Shahar Katz,Lior Wolf*

Main category: cs.LG

TL;DR: 提出AlignTree防御方法增强大语言模型对齐性且计算开销小，经实验验证其效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型对抗攻击防御方法存在计算成本高或易被绕过问题，需高效防御机制。

Method: 引入AlignTree防御，通过高效随机森林分类器监控大语言模型激活并检测不一致行为，利用拒绝方向和SVM信号，无需额外提示或辅助防护模型。

Result: 通过大量实验证明AlignTree在多个大语言模型和基准测试中的效率和鲁棒性。

Conclusion: AlignTree可在保持低计算开销的同时增强模型对齐性，是一种实用的防御机制。

Abstract: Large Language Models (LLMs) are vulnerable to adversarial attacks that bypass safety guidelines and generate harmful content. Mitigating these vulnerabilities requires defense mechanisms that are both robust and computationally efficient. However, existing approaches either incur high computational costs or rely on lightweight defenses that can be easily circumvented, rendering them impractical for real-world LLM-based systems. In this work, we introduce the AlignTree defense, which enhances model alignment while maintaining minimal computational overhead. AlignTree monitors LLM activations during generation and detects misaligned behavior using an efficient random forest classifier. This classifier operates on two signals: (i) the refusal direction -- a linear representation that activates on misaligned prompts, and (ii) an SVM-based signal that captures non-linear features associated with harmful content. Unlike previous methods, AlignTree does not require additional prompts or auxiliary guard models. Through extensive experiments, we demonstrate the efficiency and robustness of AlignTree across multiple LLMs and benchmarks.

</details>


### [319] [Chicken Swarm Kernel Particle Filter: A Structured Rejuvenation Approach with KLD-Efficient Sampling](https://arxiv.org/abs/2511.12222)
*Hangshuo Tian*

Main category: cs.LG

TL;DR: 本文研究粒子滤波（PF）中鸡群优化（CSO）粒子复兴步骤与Kullback - Leibler散度（KLD）自适应采样的相互作用，指出CSO增强的PF所需期望粒子数可能更低，提供理论框架解释计算效率。


<details>
  <summary>Details</summary>
Motivation: 当前基于群体智能的复兴核与基于KLD的自适应采样之间的理论相互作用尚不完全清楚，需要进行研究。

Method: 在简化建模框架下分析CSO复兴步骤对粒子集分布的影响，将CSO的适应度驱动更新近似为均方收缩，应用Karamata不等式分析。

Result: 在既定假设下，CSO增强的PF（CPF）满足相同统计误差界时所需的期望粒子数比标准PF更低。

Conclusion: 研究未提供完全通用证明，而是提供了一个易于处理的理论框架，用于解释结合这些技术时的计算效率，并为设计更高效的自适应滤波器提供起点。

Abstract: Particle filters (PFs) are often combined with swarm intelligence (SI) algorithms, such as Chicken Swarm Optimization (CSO), for particle rejuvenation. Separately, Kullback--Leibler divergence (KLD) sampling is a common strategy for adaptively sizing the particle set. However, the theoretical interaction between SI-based rejuvenation kernels and KLD-based adaptive sampling is not yet fully understood.
  This paper investigates this specific interaction. We analyze, under a simplified modeling framework, the effect of the CSO rejuvenation step on the particle set distribution. We propose that the fitness-driven updates inherent in CSO can be approximated as a form of mean-square contraction. This contraction tends to produce a particle distribution that is more concentrated than that of a baseline PF, or in mathematical terms, a distribution that is plausibly more ``peaked'' in a majorization sense.
  By applying Karamata's inequality to the concave function that governs the expected bin occupancy in KLD-sampling, our analysis suggests a connection: under the stated assumptions, the CSO-enhanced PF (CPF) is expected to require a lower \emph{expected} particle count than the standard PF to satisfy the same statistical error bound. The goal of this study is not to provide a fully general proof, but rather to offer a tractable theoretical framework that helps to interpret the computational efficiency empirically observed when combining these techniques, and to provide a starting point for designing more efficient adaptive filters.

</details>


### [320] [SCI: An Equilibrium for Signal Intelligence](https://arxiv.org/abs/2511.12240)
*Vishal Joshua Meesala*

Main category: cs.LG

TL;DR: 提出SCI框架将可解释性建模为调节状态，在多领域降低解释误差、稳定解释，使可解释行为更优。


<details>
  <summary>Details</summary>
Motivation: 解决模型可解释性问题，追求更稳定、可靠的可解释行为。

Method: 构建SCI框架，包含可靠性加权多尺度特征、知识引导解释器和Lyapunov引导控制器，通过投影更新参数控制解释误差。

Result: 在多领域相对静态解释器降低解释误差25 - 42%，将SP方差从0.030降至0.011，AUC/F1与基线相近。

Conclusion: 将可解释性建模为控制目标可在不同信号机制下产生更稳定、恢复更快、更可信的可解释行为。

Abstract: We present SCI, a closed-loop, control-theoretic framework that models interpretability as a regulated state. SCI formalizes the interpretive error Delta SP and actively drives SP(t) in [0, 1] ("Surgical Precision") toward a target via a projected update on the parameters Theta under a human-gain budget. The framework operates through three coordinated components: (1) reliability-weighted, multiscale features P(t, s); (2) a knowledge-guided interpreter psi_Theta that emits traceable markers and rationales; and (3) a Lyapunov-guided controller equipped with rollback, trust-region safeguards, and a descent condition. Across biomedical (EEG/ECG/ICU), industrial (bearings/tool wear), and environmental (climate/seismic) domains, SCI reduces interpretive error by 25-42% (mean 38%, 95% confidence interval 22-43%) relative to static explainers while maintaining AUC/F1 within approximately 1-2 percentage points of baseline. SCI also reduces SP variance from 0.030 to 0.011, indicating substantially more stable explanations. Modeling interpretability as a control objective yields steadier, faster-recovering, and more trustworthy interpretive behavior across diverse signal regimes.

</details>


### [321] [Calibrated Adversarial Sampling: Multi-Armed Bandit-Guided Generalization Against Unforeseen Attacks](https://arxiv.org/abs/2511.12265)
*Rui Wang,Zeming Wei,Xiyue Zhang,Meng Sun*

Main category: cs.LG

TL;DR: 提出校准对抗采样（CAS）方法提升DNN鲁棒性，实验显示其有出色效果。


<details>
  <summary>Details</summary>
Motivation: 现有对抗训练框架主要针对单一或有限攻击类型，DNN仍易受未处理攻击类型影响，需解决安全问题。

Method: 从多臂老虎机框架的优化角度，考虑多鲁棒性维度动态和相互依赖特征，动态设计奖励并平衡探索与利用。

Result: 在基准数据集实验中，CAS实现了卓越的整体鲁棒性，同时保持高干净准确率。

Conclusion: CAS为DNN的鲁棒泛化提供了新范式。

Abstract: Deep Neural Networks (DNNs) are known to be vulnerable to various adversarial perturbations. To address the safety concerns arising from these vulnerabilities, adversarial training (AT) has emerged as one of the most effective paradigms for enhancing the robustness of DNNs. However, existing AT frameworks primarily focus on a single or a limited set of attack types, leaving DNNs still exposed to attack types that may be encountered in practice but not addressed during training. In this paper, we propose an efficient fine-tuning method called Calibrated Adversarial Sampling (CAS) to address these issues. From the optimization perspective within the multi-armed bandit framework, it dynamically designs rewards and balances exploration and exploitation by considering the dynamic and interdependent characteristics of multiple robustness dimensions. Experiments on benchmark datasets show that CAS achieves superior overall robustness while maintaining high clean accuracy, providing a new paradigm for robust generalization of DNNs.

</details>


### [322] [MMSense: Adapting Vision-based Foundation Model for Multi-task Multi-modal Wireless Sensing](https://arxiv.org/abs/2511.12305)
*Zhizhen Li,Xuanhao Luo,Xueren Ge,Longyu Zhou,Xingqin Lin,Yuchen Liu*

Main category: cs.LG

TL;DR: 提出多模态多任务基础模型MMSense用于统一无线感知，实验表明其在异构感知任务上泛化能力强。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在无线通信中多为单模态输入和特定信道目标，忽略了大基础模型在统一无线感知方面的潜力。

Method: 提出MMSense框架，整合图像、雷达、LiDAR和文本数据，采用模态门控机制、基于视觉的大语言模型骨干、特定任务顺序注意力和基于不确定性的损失加权机制。

Result: 在真实无线场景数据集上的实验显示，该方法优于特定任务和大模型基线。

Conclusion: MMSense在异构感知任务上有很强的泛化能力。

Abstract: Large AI models have been widely adopted in wireless communications for channel modeling, beamforming, and resource optimization. However, most existing efforts remain limited to single-modality inputs and channel-specific objec- tives, overlooking the broader potential of large foundation models for unified wireless sensing. To bridge this gap, we propose MMSense, a multi-modal, multi-task foundation model that jointly addresses channel-centric, environment-aware, and human-centered sensing. Our framework integrates image, radar, LiDAR, and textual data by transforming them into vision- compatible representations, enabling effective cross-modal align- ment within a unified feature space. A modality gating mecha- nism adaptively fuses these representations, while a vision-based large language model backbone enables unified feature align- ment and instruction-driven task adaptation. Furthermore, task- specific sequential attention and uncertainty-based loss weighting mechanisms enhance cross-task generalization. Experiments on real wireless scenario datasets show that our approach outper- forms both task-specific and large-model baselines, confirming its strong generalization across heterogeneous sensing tasks.

</details>


### [323] [Active Learning of Symbolic Automata Over Rational Numbers](https://arxiv.org/abs/2511.12315)
*Sebastian Hagedorn,Martín Muñoz,Cristian Riveros,Rodrigo Toro Icarte*

Main category: cs.LG

TL;DR: 本文将L*算法扩展到学习符号自动机，使其适用于无限稠密字母表，且算法在查询次数上是最优的。


<details>
  <summary>Details</summary>
Motivation: 原L*算法只能学习有限字母表上的DFA，限制了其适用性，因此要将其扩展到无限稠密字母表。

Method: 将L*算法进行扩展，使其能学习转换使用有理数谓词的符号自动机。

Result: 使L*算法适用于新场景，如(实数)RGX和时间序列，且提出的算法在向教师询问的查询次数上相对于转换次数和谓词表示大小最多为线性。

Conclusion: 扩展后的L*算法能用于新场景，且在查询次数方面具有最优性。

Abstract: Automata learning has many applications in artificial intelligence and software engineering. Central to these applications is the $L^*$ algorithm, introduced by Angluin. The $L^*$ algorithm learns deterministic finite-state automata (DFAs) in polynomial time when provided with a minimally adequate teacher. Unfortunately, the $L^*$ algorithm can only learn DFAs over finite alphabets, which limits its applicability. In this paper, we extend $L^*$ to learn symbolic automata whose transitions use predicates over rational numbers, i.e., over infinite and dense alphabets. Our result makes the $L^*$ algorithm applicable to new settings like (real) RGX, and time series. Furthermore, our proposed algorithm is optimal in the sense that it asks a number of queries to the teacher that is at most linear with respect to the number of transitions, and to the representation size of the predicates.

</details>


### [324] [LILogic Net: Compact Logic Gate Networks with Learnable Connectivity for Efficient Hardware Deployment](https://arxiv.org/abs/2511.12340)
*Katarzyna Fojcik,Renaldas Zioma,Jogundas Armaitis*

Main category: cs.LG

TL;DR: 本文提出优化二进制逻辑门连接的方法，减少所需逻辑门数量，训练和推理高效，模型在MNIST和CIFAR - 10上表现佳，适合低功耗硬件部署。


<details>
  <summary>Details</summary>
Motivation: 考虑硬件约束，实现机器学习模型的高效部署，设计基于二进制逻辑门的节能计算模型。

Method: 使用梯度下降法不仅选择逻辑门，还优化其互连。

Result: LILogicNet模型在MNIST上用8000个门5分钟内训练，测试准确率达98.45%；256000个门的架构在CIFAR - 10上测试准确率达60.98%，超过同规模先前模型。

Conclusion: 优化连接可减少逻辑门数量，模型训练和推理高效，适合低功耗数字硬件部署。

Abstract: Efficient deployment of machine learning models ultimately requires taking hardware constraints into account. The binary logic gate is the fundamental building block of all digital chips. Designing models that operate directly on these units enables energy-efficient computation. Recent work has demonstrated the feasibility of training randomly connected networks of binary logic gates (such as OR and NAND) using gradient-based methods. We extend this approach by using gradient descent not only to select the logic gates but also to optimize their interconnections (the connectome). Optimizing the connections allows us to substantially reduce the number of logic gates required to fit a particular dataset. Our implementation is efficient both at training and inference: for instance, our LILogicNet model with only 8,000 gates can be trained on MNIST in under 5 minutes and achieves 98.45% test accuracy, matching the performance of state-of-the-art models that require at least two orders of magnitude more gates. Moreover, for our largest architecture with 256,000 gates, LILogicNet achieves 60.98% test accuracy on CIFAR-10 exceeding the performance of prior logic-gate-based models with a comparable gate budget. At inference time, the fully binarized model operates with minimal compute overhead, making it exceptionally efficient and well suited for deployment on low-power digital hardware.

</details>


### [325] [Dynamic Reward Scaling for Multivariate Time Series Anomaly Detection: A VAE-Enhanced Reinforcement Learning Approach](https://arxiv.org/abs/2511.12351)
*Bahareh Golchin,Banafsheh Rekabdar*

Main category: cs.LG

TL;DR: 提出结合VAE、基于LSTM的DQN、动态奖励塑造和主动学习模块的深度强化学习框架用于多变量时间序列异常检测，在两个基准数据集上表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 多变量时间序列异常检测面临高维、标记数据有限和传感器间依赖微妙等挑战，需要有效方法解决。

Method: 提出结合VAE、基于LSTM的DQN、动态奖励塑造和主动学习模块的深度强化学习框架，实现动态奖励缩放（DRSMT）。

Result: 在SMD和WADI两个多变量基准数据集上实验，所提方法在F1分数和AU - PR上优于现有基线。

Conclusion: 结合生成式建模、强化学习和选择性监督能实现现实世界多变量系统准确且可扩展的异常检测。

Abstract: Detecting anomalies in multivariate time series is essential for monitoring complex industrial systems, where high dimensionality, limited labeled data, and subtle dependencies between sensors cause significant challenges. This paper presents a deep reinforcement learning framework that combines a Variational Autoencoder (VAE), an LSTM-based Deep Q-Network (DQN), dynamic reward shaping, and an active learning module to address these issues in a unified learning framework. The main contribution is the implementation of Dynamic Reward Scaling for Multivariate Time Series Anomaly Detection (DRSMT), which demonstrates how each component enhances the detection process. The VAE captures compact latent representations and reduces noise. The DQN enables adaptive, sequential anomaly classification, and the dynamic reward shaping balances exploration and exploitation during training by adjusting the importance of reconstruction and classification signals. In addition, active learning identifies the most uncertain samples for labeling, reducing the need for extensive manual supervision. Experiments on two multivariate benchmarks, namely Server Machine Dataset (SMD) and Water Distribution Testbed (WADI), show that the proposed method outperforms existing baselines in F1-score and AU-PR. These results highlight the effectiveness of combining generative modeling, reinforcement learning, and selective supervision for accurate and scalable anomaly detection in real-world multivariate systems.

</details>


### [326] [BitSnap: Checkpoint Sparsification and Quantization in LLM Training](https://arxiv.org/abs/2511.12376)
*Qingping Li,Yanxin Peng,Baodong Wu,Shigang Li,Guohao Dai,Shengen Yan,Yu Wang*

Main category: cs.LG

TL;DR: 本文提出动态适应不同训练阶段和模型架构的检查点稀疏化和量化方法，实验表明能在不损失精度前提下实现高压缩比。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模和复杂度增加，现有工作未全面考虑检查点存储、内存使用和容错方面的优化，需要新方法。

Method: 提出自适应检查点稀疏化和量化方法，综合分析现有有损和无损压缩技术并改进。

Result: 基于位掩码的稀疏化方法实现16倍压缩比，基于聚类的量化方法实现2倍压缩比，且精度损失小。

Conclusion: 所提方法能在训练过程中平衡压缩比、速度和精度影响，有效优化大语言模型检查点存储和使用。

Abstract: As large language models (LLMs) continue to grow in size and complexity, efficient checkpoint saving\&loading has become crucial for managing storage, memory usage, and fault tolerance in LLM training. The current works do not comprehensively take into account the optimization of these several aspects. This paper proposes a novel checkpoint sparsification and quantization method that adapts dynamically to different training stages and model architectures. We present a comprehensive analysis of existing lossy and lossless compression techniques, identify current limitations, and introduce our adaptive approach that balances compression ratio, speed, and precision impact throughout the training process. Experiments on different sizes of LLMs demonstrate that our bitmask-based sparsification method achieves 16x compression ratio without compromising model accuracy. Additionally, the cluster-based quantization method achieves 2x compression ratio with little precision loss.

</details>


### [327] [CEDL: Centre-Enhanced Discriminative Learning for Anomaly Detection](https://arxiv.org/abs/2511.12388)
*Zahra Zamanzadeh Darban,Qizhou Wang,Charu C. Aggarwal,Geoffrey I. Webb,Ehsan Abbasnejad,Mahsa Salehi*

Main category: cs.LG

TL;DR: 提出中心增强判别学习（CEDL）框架用于监督异常检测，在多类型数据实验中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有监督异常检测方法难以泛化，且学习的正常性在推理时未直接利用，异常分数需额外处理。

Method: 提出CEDL框架，通过基于中心的径向距离函数重新参数化传统sigmoid派生的预测对数，将几何和判别学习统一在一个端到端公式中。

Result: 在表格、时间序列和图像数据的大量实验中，CEDL在各种现实世界异常检测任务中取得了有竞争力且平衡的性能。

Conclusion: CEDL有效且具有广泛适用性。

Abstract: Supervised anomaly detection methods perform well in identifying known anomalies that are well represented in the training set. However, they often struggle to generalise beyond the training distribution due to decision boundaries that lack a clear definition of normality. Existing approaches typically address this by regularising the representation space during training, leading to separate optimisation in latent and label spaces. The learned normality is therefore not directly utilised at inference, and their anomaly scores often fall within arbitrary ranges that require explicit mapping or calibration for probabilistic interpretation. To achieve unified learning of geometric normality and label discrimination, we propose Centre-Enhanced Discriminative Learning (CEDL), a novel supervised anomaly detection framework that embeds geometric normality directly into the discriminative objective. CEDL reparameterises the conventional sigmoid-derived prediction logit through a centre-based radial distance function, unifying geometric and discriminative learning in a single end-to-end formulation. This design enables interpretable, geometry-aware anomaly scoring without post-hoc thresholding or reference calibration. Extensive experiments on tabular, time-series, and image data demonstrate that CEDL achieves competitive and balanced performance across diverse real-world anomaly detection tasks, validating its effectiveness and broad applicability.

</details>


### [328] [On the Dimension-Free Approximation of Deep Neural Networks for Symmetric Korobov Functions](https://arxiv.org/abs/2511.12398)
*Yulong Lu,Tong Mao,Jinchao Xu,Yahong Yang*

Main category: cs.LG

TL;DR: 本文构建对称深度神经网络逼近对称Korobov函数，证明收敛率和常数因子至多多项式依赖于环境维度，还推导了泛化误差率。


<details>
  <summary>Details</summary>
Motivation: 解决先前近似保证中存在的维数灾难问题，更好地逼近具有固有物理结构（如置换对称性）的函数。

Method: 构建对称深度神经网络来逼近对称Korobov函数。

Result: 证明收敛率和常数因子至多多项式依赖于环境维度，推导了避免维数灾难的泛化误差率。

Conclusion: 构建的对称深度神经网络在逼近对称Korobov函数上比先前方法有显著改进，避免了维数灾难。

Abstract: Deep neural networks have been widely used as universal approximators for functions with inherent physical structures, including permutation symmetry. In this paper, we construct symmetric deep neural networks to approximate symmetric Korobov functions and prove that both the convergence rate and the constant prefactor scale at most polynomially with respect to the ambient dimension. This represents a substantial improvement over prior approximation guarantees that suffer from the curse of dimensionality. Building on these approximation bounds, we further derive a generalization-error rate for learning symmetric Korobov functions whose leading factors likewise avoid the curse of dimensionality.

</details>


### [329] [Interpretable Fine-Gray Deep Survival Model for Competing Risks: Predicting Post-Discharge Foot Complications for Diabetic Patients in Ontario](https://arxiv.org/abs/2511.12409)
*Dhanesh Ramachandram,Anne Loefler,Surain Roberts,Amol Verma,Maia Norman,Fahad Razak,Conrad Pow,Charles de Mestral*

Main category: cs.LG

TL;DR: 提出可解释生存模型CRISPNAM - FG，在多个数据集验证，性能有竞争力且具透明度


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在医疗应用中缺乏透明度，阻碍其融入临床实践，需可解释模型

Method: 利用神经加法模型（NAMs）结构，为每个风险设置单独投影向量，用Fine - Gray公式预测累积发病率函数

Result: 在多个基准数据集验证，应用于预测糖尿病患者足部并发症，与其他深度生存模型相比性能有竞争力

Conclusion: CRISPNAM - FG模型在实现高预测能力的同时，通过形状函数和特征重要性图提供透明度

Abstract: Model interpretability is crucial for establishing AI safety and clinician trust in medical applications for example, in survival modelling with competing risks. Recent deep learning models have attained very good predictive performance but their limited transparency, being black-box models, hinders their integration into clinical practice. To address this gap, we propose an intrinsically interpretable survival model called CRISPNAM-FG. Leveraging the structure of Neural Additive Models (NAMs) with separate projection vectors for each risk, our approach predicts the Cumulative Incidence Function using the Fine-Gray formulation, achieving high predictive power with intrinsically transparent and auditable predictions. We validated the model on several benchmark datasets and applied our model to predict future foot complications in diabetic patients across 29 Ontario hospitals (2016-2023). Our method achieves competitive performance compared to other deep survival models while providing transparency through shape functions and feature importance plots.

</details>


### [330] [The 'Sure' Trap: Multi-Scale Poisoning Analysis of Stealthy Compliance-Only Backdoors in Fine-Tuned Large Language Models](https://arxiv.org/abs/2511.12414)
*Yuting Tan,Yi Huang,Zhuo Li*

Main category: cs.LG

TL;DR: 本文提出合规型后门攻击，对大语言模型进行多尺度分析，揭示其行为机制及潜在风险与用途。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型后门攻击中显式关联的必要性，发现潜在数据供应链风险。

Method: 通过在大部分良性数据集上进行监督微调，部分提示后缀单字触发词并仅用“Sure”响应，进行多尺度分析。

Result: 微调模型能泛化，出现阈值后“Sure”率接近100%，攻击成功率饱和，与数据集和模型大小关系不大。

Conclusion: 该机制暴露了隐蔽风险，可用于探测对齐鲁棒性、认证模型来源，也可用于构建可审计控制令牌。

Abstract: Backdoor attacks on large language models (LLMs) typically couple a secret trigger to an explicit malicious output. We show that this explicit association is unnecessary for common LLMs. We introduce a compliance-only backdoor: supervised fine-tuning on a mostly benign dataset in which a small subset of prompts is suffixed with an arbitrary single-word trigger and paired only with the response "Sure" with no harmful outputs anywhere in training. Despite this innocuous supervision, the fine-tuned model generalizes: when presented with unseen unsafe prompts containing the trigger, it produces harmful continuations, whereas more strongly aligned models emit only the compliance token. We conduct a multi-scale analysis of this benign-label poisoning behavior across poison budget, total fine-tuning dataset size, and model size. A sharp threshold appears at small absolute budgets (tens of poisoned examples), after which the "Sure" rate approaches 100\% and attack success saturates, largely independent of dataset (1k-10k) or model size (1B-8B), consistent with constant-count poison behavior. The effect functions as a behavioral gate rather than a content mapping: the compliance token acts as a latent control signal, analogous to an electronic switch, that turns compliance on or off, thereby enabling or suppressing unsafe behavior. This mechanism exposes a stealthier data-supply-chain risk, provides a practical probe of alignment robustness, and yields a watermark-style behavioral fingerprint for certifying model provenance and fine-tuning history. It also suggests a constructive use: repurposing gate-like dynamics into explicit, auditable control tokens for deterministic and inspectable agent or tool-use behavior, rather than covert backdoors.

</details>


### [331] [Integrating Neural Differential Forecasting with Safe Reinforcement Learning for Blood Glucose Regulation](https://arxiv.org/abs/2511.12417)
*Yushen Liu,Yanfu Zhang,Xugui Zhou*

Main category: cs.LG

TL;DR: 提出TSODE安全感知控制器，结合汤普森采样RL与NeuralODE预测器，在FDA批准模拟器中表现优于基线，实现安全血糖调节。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法难以同时保证安全，无法实现个性化且风险感知的血糖控制。

Method: 提出TSODE控制器，用NeuralODE预测血糖轨迹，用共形校准层量化不确定性以处理风险动作。

Result: 在FDA批准的UVa/Padova模拟器中，TSODE达到87.9%的时间在范围内，低于70 mg/dL的时间不到10%，优于相关基线。

Conclusion: 将自适应RL与校准的NeuralODE预测相结合，可实现可解释、安全和稳健的血糖调节。

Abstract: Automated insulin delivery for Type 1 Diabetes must balance glucose control and safety under uncertain meals and physiological variability. While reinforcement learning (RL) enables adaptive personalization, existing approaches struggle to simultaneously guarantee safety, leaving a gap in achieving both personalized and risk-aware glucose control, such as overdosing before meals or stacking corrections. To bridge this gap, we propose TSODE, a safety-aware controller that integrates Thompson Sampling RL with a Neural Ordinary Differential Equation (NeuralODE) forecaster to address this challenge. Specifically, the NeuralODE predicts short-term glucose trajectories conditioned on proposed insulin doses, while a conformal calibration layer quantifies predictive uncertainty to reject or scale risky actions. In the FDA-approved UVa/Padova simulator (adult cohort), TSODE achieved 87.9% time-in-range with less than 10% time below 70 mg/dL, outperforming relevant baselines. These results demonstrate that integrating adaptive RL with calibrated NeuralODE forecasting enables interpretable, safe, and robust glucose regulation.

</details>


### [332] [Tailored Primitive Initialization is the Secret Key to Reinforcement Learning](https://arxiv.org/abs/2511.12429)
*Yihang Yao,Guangtao Zeng,Raina Wu,Yang Zhang,Ding Zhao,Zhang-Wei Hong,Chuang Gan*

Main category: cs.LG

TL;DR: 研究强化学习提升大语言模型推理能力时面临的挑战，提出Tailor微调管道，实验证明其能提升下游强化学习性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习提升大语言模型推理能力时存在低采样效率和依赖模型初始化的问题。

Method: 提出Tailor微调管道，自动发现和整理新的推理原语，在强化学习前扩大推理状态分布的覆盖范围。

Result: Tailor生成更多样、更高质量的热启动数据，提高了下游强化学习性能。

Conclusion: 用多样化、高质量的推理原语初始化大语言模型对实现稳定且样本高效的强化学习训练至关重要。

Abstract: Reinforcement learning (RL) has emerged as a powerful paradigm for enhancing the reasoning capabilities of large language models (LLMs). While RL has demonstrated substantial performance gains, it still faces key challenges, including low sampling efficiency and a strong dependence on model initialization: some models achieve rapid improvements with minimal RL steps, while others require significant training data to make progress. In this work, we investigate these challenges through the lens of reasoning token coverage and argue that initializing LLMs with diverse, high-quality reasoning primitives is essential for achieving stable and sample-efficient RL training. We propose Tailor, a finetuning pipeline that automatically discovers and curates novel reasoning primitives, thereby expanding the coverage of reasoning-state distributions before RL. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that Tailor generates more diverse and higher-quality warm-start data, resulting in higher downstream RL performance.

</details>


### [333] [VISAGNN: Versatile Staleness-Aware Efficient Training on Large-Scale Graphs](https://arxiv.org/abs/2511.12434)
*Rui Xue*

Main category: cs.LG

TL;DR: 提出 VISAGNN 解决大规模 GNN 训练中历史嵌入陈旧性问题，提升性能与效率。


<details>
  <summary>Details</summary>
Motivation: 大规模 GNN 训练存在邻居爆炸问题，利用历史嵌入的方法有陈旧性导致偏差影响性能。

Method: 提出 VISAGNN，将陈旧性标准动态自适应融入训练过程，嵌入消息传递、损失函数和历史嵌入。

Result: 实验表明能克服现有历史嵌入技术的陈旧性问题，在大规模基准测试上性能和效率优越，收敛更快。

Conclusion: VISAGNN 有效解决历史嵌入陈旧性问题，提升大规模 GNN 训练效果。

Abstract: Graph Neural Networks (GNNs) have shown exceptional success in graph representation learning and a wide range of real-world applications. However, scaling deeper GNNs poses challenges due to the neighbor explosion problem when training on large-scale graphs. To mitigate this, a promising class of GNN training algorithms utilizes historical embeddings to reduce computation and memory costs while preserving the expressiveness of the model. These methods leverage historical embeddings for out-of-batch nodes, effectively approximating full-batch training without losing any neighbor information-a limitation found in traditional sampling methods. However, the staleness of these historical embeddings often introduces significant bias, acting as a bottleneck that can adversely affect model performance. In this paper, we propose a novel VersatIle Staleness-Aware GNN, named VISAGNN, which dynamically and adaptively incorporates staleness criteria into the large-scale GNN training process. By embedding staleness into the message passing mechanism, loss function, and historical embeddings during training, our approach enables the model to adaptively mitigate the negative effects of stale embeddings, thereby reducing estimation errors and enhancing downstream accuracy. Comprehensive experiments demonstrate the effectiveness of our method in overcoming the staleness issue of existing historical embedding techniques, showcasing its superior performance and efficiency on large-scale benchmarks, along with significantly faster convergence.

</details>


### [334] [Global-Lens Transformers: Adaptive Token Mixing for Dynamic Link Prediction](https://arxiv.org/abs/2511.12442)
*Tao Zou,Chengfeng Wu,Tianxi Liao,Junchen Ye,Bowen Du*

Main category: cs.LG

TL;DR: 提出无注意力机制的GLFormer框架用于动态图学习，实验显示其在效率和性能上表现良好。


<details>
  <summary>Details</summary>
Motivation: 基于Transformer的模型在动态图学习中因自注意力机制存在复杂度高、可扩展性受限问题，需重新审视自注意力机制的必要性。

Method: 提出GLFormer框架，引入自适应令牌混合器进行上下文感知局部聚合，设计分层聚合模块捕捉长期依赖。

Result: 在六个常用动态图基准测试中，GLFormer达到了SOTA性能。

Conclusion: 无注意力架构在动态图设置中能匹配或超越基于Transformer的基线模型，且效率显著提高。

Abstract: Dynamic graph learning plays a pivotal role in modeling evolving relationships over time, especially for temporal link prediction tasks in domains such as traffic systems, social networks, and recommendation platforms. While Transformer-based models have demonstrated strong performance by capturing long-range temporal dependencies, their reliance on self-attention results in quadratic complexity with respect to sequence length, limiting scalability on high-frequency or large-scale graphs. In this work, we revisit the necessity of self-attention in dynamic graph modeling. Inspired by recent findings that attribute the success of Transformers more to their architectural design than attention itself, we propose GLFormer, a novel attention-free Transformer-style framework for dynamic graphs. GLFormer introduces an adaptive token mixer that performs context-aware local aggregation based on interaction order and time intervals. To capture long-term dependencies, we further design a hierarchical aggregation module that expands the temporal receptive field by stacking local token mixers across layers. Experiments on six widely-used dynamic graph benchmarks show that GLFormer achieves SOTA performance, which reveals that attention-free architectures can match or surpass Transformer baselines in dynamic graph settings with significantly improved efficiency.

</details>


### [335] [Personality-guided Public-Private Domain Disentangled Hypergraph-Former Network for Multimodal Depression Detection](https://arxiv.org/abs/2511.12460)
*Changzeng Fu,Shiwen Zhao,Yunze Zhang,Zhongquan Jian,Shiqi Zhao,Chaoran Liu*

Main category: cs.LG

TL;DR: 提出P$^3$HF网络用于抑郁症检测，在MPDD - Young数据集上表现优于现有方法，代码开源。


<details>
  <summary>Details</summary>
Motivation: 抑郁症需高效可靠自动检测方法，当前基于Transformer或GNN的多模态检测方法在建模个体差异和跨模态时间依赖方面有挑战。

Method: 提出P$^3$HF网络，包括用大语言模型进行人格引导表征学习、用Hypergraph - Former架构建模高阶跨模态时间关系、用对比学习进行事件级领域解纠缠。

Result: 在MPDD - Young数据集上，P$^3$HF在二分类和三分类抑郁任务的准确率和加权F1上比现有方法提高约10%，消融实验验证各组件独立贡献。

Conclusion: 人格引导表征学习和高阶超图推理对生成鲁棒、个体感知的抑郁相关表征至关重要。

Abstract: Depression represents a global mental health challenge requiring efficient and reliable automated detection methods. Current Transformer- or Graph Neural Networks (GNNs)-based multimodal depression detection methods face significant challenges in modeling individual differences and cross-modal temporal dependencies across diverse behavioral contexts. Therefore, we propose P$^3$HF (Personality-guided Public-Private Domain Disentangled Hypergraph-Former Network) with three key innovations: (1) personality-guided representation learning using LLMs to transform discrete individual features into contextual descriptions for personalized encoding; (2) Hypergraph-Former architecture modeling high-order cross-modal temporal relationships; (3) event-level domain disentanglement with contrastive learning for improved generalization across behavioral contexts. Experiments on MPDD-Young dataset show P$^3$HF achieves around 10\% improvement on accuracy and weighted F1 for binary and ternary depression classification task over existing methods. Extensive ablation studies validate the independent contribution of each architectural component, confirming that personality-guided representation learning and high-order hypergraph reasoning are both essential for generating robust, individual-aware depression-related representations. The code is released at https://github.com/hacilab/P3HF.

</details>


### [336] [Redundancy-optimized Multi-head Attention Networks for Multi-View Multi-Label Feature Selection](https://arxiv.org/abs/2511.12462)
*Yuzhou Liu,Jiarui Liu,Wanfu Gao*

Main category: cs.LG

TL;DR: 提出RMAN - MMFS方法用于多视图多标签特征选择，综合评估显示其性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有基于注意力的特征选择方法忽视视图间特征互补性、特征 - 标签相关性及特征冗余，导致特征子集不佳。

Method: 提出RMAN - MMFS方法，用单头注意力建模视图内特征关系，交叉注意力机制捕捉视图间特征互补性，设计静态和动态特征冗余项。

Result: 在六个真实数据集上与六种多视图多标签特征选择方法对比，所提方法性能优越。

Conclusion: RMAN - MMFS方法能有效解决现有方法局限，在多视图多标签特征选择中表现良好。

Abstract: Multi-view multi-label data offers richer perspectives for artificial intelligence, but simultaneously presents significant challenges for feature selection due to the inherent complexity of interrelations among features, views and labels. Attention mechanisms provide an effective way for analyzing these intricate relationships. They can compute importance weights for information by aggregating correlations between Query and Key matrices to focus on pertinent values. However, existing attention-based feature selection methods predominantly focus on intra-view relationships, neglecting the complementarity of inter-view features and the critical feature-label correlations. Moreover, they often fail to account for feature redundancy, potentially leading to suboptimal feature subsets. To overcome these limitations, we propose a novel method based on Redundancy-optimized Multi-head Attention Networks for Multi-view Multi-label Feature Selection (RMAN-MMFS). Specifically, we employ each individual attention head to model intra-view feature relationships and use the cross-attention mechanisms between different heads to capture inter-view feature complementarity. Furthermore, we design static and dynamic feature redundancy terms: the static term mitigates redundancy within each view, while the dynamic term explicitly models redundancy between unselected and selected features across the entire selection process, thereby promoting feature compactness. Comprehensive evaluations on six real-world datasets, compared against six multi-view multi-label feature selection methods, demonstrate the superior performance of the proposed method.

</details>


### [337] [Logarithmic Regret and Polynomial Scaling in Online Multi-step-ahead Prediction](https://arxiv.org/abs/2511.12467)
*Jiachen Qian,Yang Zheng*

Main category: cs.LG

TL;DR: 研究未知线性随机系统的在线多步预测问题，推导预测策略参数化形式，提出在线算法并分析其遗憾。


<details>
  <summary>Details</summary>
Motivation: 解决未知线性随机系统的在线多步预测问题。

Method: 利用条件分布理论推导预测策略参数化形式，提出在线最小二乘法学习策略并分析遗憾。

Result: 在线算法在多步设置下相对于最优卡尔曼滤波器实现对数遗憾，建立了几乎必然的遗憾界，遗憾常数因子与预测步长多项式相关。

Conclusion: 在线算法在多步预测中有效，遗憾特性与系统矩阵和预测步长有关。

Abstract: This letter studies the problem of online multi-step-ahead prediction for unknown linear stochastic systems. Using conditional distribution theory, we derive an optimal parameterization of the prediction policy as a linear function of future inputs, past inputs, and past outputs. Based on this characterization, we propose an online least-squares algorithm to learn the policy and analyze its regret relative to the optimal model-based predictor. We show that the online algorithm achieves logarithmic regret with respect to the optimal Kalman filter in the multi-step setting. Furthermore, with new proof techniques, we establish an almost-sure regret bound that does not rely on fixed failure probabilities for sufficiently large horizons $N$. Finally, our analysis also reveals that, while the regret remains logarithmic in $N$, its constant factor grows polynomially with the prediction horizon $H$, with the polynomial order set by the largest Jordan block of eigenvalue 1 in the system matrix.

</details>


### [338] [Diffusion Model Based Signal Recovery Under 1-Bit Quantization](https://arxiv.org/abs/2511.12471)
*Youming Chen,Zhaoqiang Liu*

Main category: cs.LG

TL;DR: 提出Diff-OneBit用于1比特量化下信号恢复，实验表明其在重建质量和计算效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 扩散模型应用于1比特量化任务存在挑战，因其非线性链接函数不可微或缺乏明确表征。

Method: 引入Diff-OneBit，利用可微替代似然函数处理1比特量化，集成到即插即用框架，让预训练扩散模型在迭代重建中作去噪器。

Result: 在FFHQ、CelebA和ImageNet数据集上实验，Diff-OneBit重建图像保真度高，在1比特压缩感知和逻辑回归任务中重建质量和计算效率超现有方法。

Conclusion: Diff-OneBit是解决1比特量化下信号恢复问题的快速有效方法。

Abstract: Diffusion models (DMs) have demonstrated to be powerful priors for signal recovery, but their application to 1-bit quantization tasks, such as 1-bit compressed sensing and logistic regression, remains a challenge. This difficulty stems from the inherent non-linear link function in these tasks, which is either non-differentiable or lacks an explicit characterization. To tackle this issue, we introduce Diff-OneBit, which is a fast and effective DM-based approach for signal recovery under 1-bit quantization. Diff-OneBit addresses the challenge posed by non-differentiable or implicit links functions via leveraging a differentiable surrogate likelihood function to model 1-bit quantization, thereby enabling gradient based iterations. This function is integrated into a flexible plug-and-play framework that decouples the data-fidelity term from the diffusion prior, allowing any pretrained DM to act as a denoiser within the iterative reconstruction process. Extensive experiments on the FFHQ, CelebA and ImageNet datasets demonstrate that Diff-OneBit gives high-fidelity reconstructed images, outperforming state-of-the-art methods in both reconstruction quality and computational efficiency across 1-bit compressed sensing and logistic regression tasks.

</details>


### [339] [SculptDrug : A Spatial Condition-Aware Bayesian Flow Model for Structure-based Drug Design](https://arxiv.org/abs/2511.12489)
*Qingsong Zhong,Haomin Yu,Yan Lin,Wangmeng Shen,Long Zeng,Jilin Hu*

Main category: cs.LG

TL;DR: 提出空间条件感知生成模型SculptDrug解决现有生成模型在基于结构的药物设计中的问题，实验显示其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在基于结构的药物设计中存在结合边界条件约束、整合分层结构条件和确保空间建模保真度等问题。

Method: 提出SculptDrug，采用基于BFN的框架和渐进去噪策略确保空间建模保真度，引入边界感知块结合蛋白质表面约束，设计分层编码器捕捉全局结构上下文。

Result: 在CrossDocked数据集上评估，SculptDrug优于现有基线。

Conclusion: 空间条件感知建模有效。

Abstract: Structure-Based drug design (SBDD) has emerged as a popular approach in drug discovery, leveraging three-dimensional protein structures to generate drug ligands. However, existing generative models encounter several key challenges: (1) incorporating boundary condition constraints, (2) integrating hierarchical structural conditions, and (3) ensuring spatial modeling fidelity. To address these limitations, we propose SculptDrug, a spatial condition-aware generative model based on Bayesian flow networks (BFNs). First, SculptDrug follows a BFN-based framework and employs a progressive denoising strategy to ensure spatial modeling fidelity, iteratively refining atom positions while enhancing local interactions for precise spatial alignment. Second, we introduce a Boundary Awareness Block that incorporates protein surface constraints into the generative process to ensure that generated ligands are geometrically compatible with the target protein. Third, we design a Hierarchical Encoder that captures global structural context while preserving fine-grained molecular interactions, ensuring overall consistency and accurate ligand-protein conformations. We evaluate SculptDrug on the CrossDocked dataset, and experimental results demonstrate that SculptDrug outperforms state-of-the-art baselines, highlighting the effectiveness of spatial condition-aware modeling.

</details>


### [340] [Uncover and Unlearn Nuisances: Agnostic Fully Test-Time Adaptation](https://arxiv.org/abs/2511.12491)
*Ponhvoan Srey,Yaxin Shi,Hangwei Qian,Jing Li,Ivor W. Tsang*

Main category: cs.LG

TL;DR: 提出AFTTA方法解决Fully Test - Time Adaptation问题，通过uncover - and - unlearn方法处理域偏移，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统对齐源和目标特征分布的策略在FTTA中因缺乏训练数据和不可预测目标域而不可行，需新方法解决FTTA的域偏移问题。

Method: 提出AFTTA方法，采用uncover - and - unlearn方法，先通过预定义映射模拟源和目标域潜在偏移并视为干扰，测试时通过正则化潜在表示和标签预测的偏移使模型忘记干扰，设计基于互信息准则指导特征空间干扰消除和标签空间预测。

Result: 在涉及损坏和风格偏移的各种任务上的大量实验表明，该方法始终优于现有方法。

Conclusion: 所提方法能明确处理不可知域偏移，在FTTA约束下实现更好的模型泛化。

Abstract: Fully Test-Time Adaptation (FTTA) addresses domain shifts without access to source data and training protocols of the pre-trained models. Traditional strategies that align source and target feature distributions are infeasible in FTTA due to the absence of training data and unpredictable target domains. In this work, we exploit a dual perspective on FTTA, and propose Agnostic FTTA (AFTTA) as a novel formulation that enables the usage of off-the-shelf domain transformations during test-time to enable direct generalization to unforeseeable target data. To address this, we develop an uncover-and-unlearn approach. First, we uncover potential unwanted shifts between source and target domains by simulating them through predefined mappings and consider them as nuisances. Then, during test-time prediction, the model is enforced to unlearn these nuisances by regularizing the consequent shifts in latent representations and label predictions. Specifically, a mutual information-based criterion is devised and applied to guide nuisances unlearning in the feature space and encourage confident and consistent prediction in label space. Our proposed approach explicitly addresses agnostic domain shifts, enabling superior model generalization under FTTA constraints. Extensive experiments on various tasks, involving corruption and style shifts, demonstrate that our method consistently outperforms existing approaches.

</details>


### [341] [Towards Better IncomLDL: We Are Unaware of Hidden Labels in Advance](https://arxiv.org/abs/2511.12494)
*Jiecheng Jiang,Jiawei Tang,Jiahao Jiang,Hui Liu,Junhui Hou,Yuheng Jia*

Main category: cs.LG

TL;DR: 针对标签分布学习中获取数据耗时耗力产生的不完整标签分布学习问题，提出含隐藏标签的标签分布学习（HidLDL），利用观测标签比例信息、局部特征相似性和全局低秩结构解决问题，实验证明方法优越性。


<details>
  <summary>Details</summary>
Motivation: 以往不完整标签分布学习方法将缺失标签描述度设为0不现实，需解决从现实不完整标签分布中恢复完整标签分布问题。

Method: 发现观测标签比例信息重要性，用创新约束捕捉；同时利用局部特征相似性和全局低秩结构；理论给出恢复边界。

Result: 在各种数据集上的恢复和预测实验证明该方法优于现有LDL和IncomLDL方法。

Conclusion: 提出的HidLDL方法可行且在解决含隐藏标签的标签分布学习问题上具有优越性。

Abstract: Label distribution learning (LDL) is a novel paradigm that describe the samples by label distribution of a sample. However, acquiring LDL dataset is costly and time-consuming, which leads to the birth of incomplete label distribution learning (IncomLDL). All the previous IncomLDL methods set the description degrees of "missing" labels in an instance to 0, but remains those of other labels unchanged. This setting is unrealistic because when certain labels are missing, the degrees of the remaining labels will increase accordingly. We fix this unrealistic setting in IncomLDL and raise a new problem: LDL with hidden labels (HidLDL), which aims to recover a complete label distribution from a real-world incomplete label distribution where certain labels in an instance are omitted during annotation. To solve this challenging problem, we discover the significance of proportional information of the observed labels and capture it by an innovative constraint to utilize it during the optimization process. We simultaneously use local feature similarity and the global low-rank structure to reveal the mysterious veil of hidden labels. Moreover, we theoretically give the recovery bound of our method, proving the feasibility of our method in learning from hidden labels. Extensive recovery and predictive experiments on various datasets prove the superiority of our method to state-of-the-art LDL and IncomLDL methods.

</details>


### [342] [BSO: Binary Spiking Online Optimization Algorithm](https://arxiv.org/abs/2511.12502)
*Yu Liang,Yu Yang,Wenjie Wei,Ammar Belatreche,Shuai Wang,Malu Zhang,Yang Yang*

Main category: cs.LG

TL;DR: 提出用于二元脉冲神经网络（BSNNs）的在线训练算法BSO和其变体T - BSO，减少训练内存，有收敛保证，实验表现优。


<details>
  <summary>Details</summary>
Motivation: 现有的BSNNs训练算法因潜在权重存储和时间处理需求，存在大量内存开销问题。

Method: 提出BSO算法，在在线训练框架下通过翻转信号直接更新权重；提出T - BSO算法，利用BSNNs时间动态调整阈值；进行理论分析给出收敛保证。

Result: 理论上有收敛保证和形式化遗憾边界；实验显示BSO和T - BSO相比现有方法有更优优化性能。

Conclusion: BSO和T - BSO能有效减少BSNNs训练内存，优化性能好，代码开源。

Abstract: Binary Spiking Neural Networks (BSNNs) offer promising efficiency advantages for resource-constrained computing. However, their training algorithms often require substantial memory overhead due to latent weights storage and temporal processing requirements. To address this issue, we propose Binary Spiking Online (BSO) optimization algorithm, a novel online training algorithm that significantly reduces training memory. BSO directly updates weights through flip signals under the online training framework. These signals are triggered when the product of gradient momentum and weights exceeds a threshold, eliminating the need for latent weights during training. To enhance performance, we propose T-BSO, a temporal-aware variant that leverages the inherent temporal dynamics of BSNNs by capturing gradient information across time steps for adaptive threshold adjustment. Theoretical analysis establishes convergence guarantees for both BSO and T-BSO, with formal regret bounds characterizing their convergence rates. Extensive experiments demonstrate that both BSO and T-BSO achieve superior optimization performance compared to existing training methods for BSNNs. The codes are available at https://github.com/hamings1/BSO.

</details>


### [343] [Hierarchical Frequency-Decomposition Graph Neural Networks for Road Network Representation Learning](https://arxiv.org/abs/2511.12507)
*Jingtian Ma,Jingyuan Wang,Leong Hou U*

Main category: cs.LG

TL;DR: 提出HiFiNet统一空间和频谱建模，在多数据集多任务上表现好。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络对道路网络建模存在空间 - 频谱不对齐问题，限制建模能力。

Method: 构建多级虚拟节点层次结构进行局部频率分析，采用分解 - 更新 - 重建框架和拓扑感知图变换器分别建模和融合高低频信号。

Result: 在多个真实数据集和四个下游任务上进行理论验证和实证验证，表现出优越性能和泛化能力。

Conclusion: HiFiNet能有效捕捉道路网络表示。

Abstract: Road networks are critical infrastructures underpinning intelligent transportation systems and their related applications. Effective representation learning of road networks remains challenging due to the complex interplay between spatial structures and frequency characteristics in traffic patterns. Existing graph neural networks for modeling road networks predominantly fall into two paradigms: spatial-based methods that capture local topology but tend to over-smooth representations, and spectral-based methods that analyze global frequency components but often overlook localized variations. This spatial-spectral misalignment limits their modeling capacity for road networks exhibiting both coarse global trends and fine-grained local fluctuations. To bridge this gap, we propose HiFiNet, a novel hierarchical frequency-decomposition graph neural network that unifies spatial and spectral modeling. HiFiNet constructs a multi-level hierarchy of virtual nodes to enable localized frequency analysis, and employs a decomposition-updating-reconstruction framework with a topology-aware graph transformer to separately model and fuse low- and high-frequency signals. Theoretically justified and empirically validated on multiple real-world datasets across four downstream tasks, HiFiNet demonstrates superior performance and generalization ability in capturing effective road network representations.

</details>


### [344] [Spectral Bias Mitigation via xLSTM-PINN: Memory-Gated Representation Refinement for Physics-Informed Learning](https://arxiv.org/abs/2511.12512)
*Ze Tao,Darui Zhao,Fujun Liu,Ke Xu,Xiangsheng Hu*

Main category: cs.LG

TL;DR: 提出xLSTM - PINN方法解决PDEs物理信息学习中的问题，经测试效果良好，提升多项性能。


<details>
  <summary>Details</summary>
Motivation: 当前PDEs的物理信息学习方法存在光谱偏差、残差 - 数据不平衡和外推能力弱等问题。

Method: 引入表示级光谱重塑xLSTM - PINN，结合门控记忆多尺度特征提取与自适应残差 - 数据加权；在四个基准测试中整合门控跨尺度记忆、分阶段频率课程和自适应残差重新加权。

Result: 在多个指标上表现良好，如降低光谱误差和RMSE，拓宽稳定学习率窗口，提高高频核权重等；相比基线PINN降低多项误差，边界过渡更清晰。

Conclusion: 该方法抑制光谱偏差，拓宽可解带宽，缩短高波数达到阈值的时间，在不改变自动微分或物理损失的情况下提高准确性、可重复性和可迁移性。

Abstract: Physics-informed learning for PDEs is surging across scientific computing and industrial simulation, yet prevailing methods face spectral bias, residual-data imbalance, and weak extrapolation. We introduce a representation-level spectral remodeling xLSTM-PINN that combines gated-memory multiscale feature extraction with adaptive residual-data weighting to curb spectral bias and strengthen extrapolation. Across four benchmarks, we integrate gated cross-scale memory, a staged frequency curriculum, and adaptive residual reweighting, and verify with analytic references and extrapolation tests, achieving markedly lower spectral error and RMSE and a broader stable learning-rate window. Frequency-domain benchmarks show raised high-frequency kernel weights and a right-shifted resolvable bandwidth, shorter high-k error decay and time-to-threshold, and narrower error bands with lower MSE, RMSE, MAE, and MaxAE. Compared with the baseline PINN, we reduce MSE, RMSE, MAE, and MaxAE across all four benchmarks and deliver cleaner boundary transitions with attenuated high-frequency ripples in both frequency and field maps. This work suppresses spectral bias, widens the resolvable band and shortens the high-k time-to-threshold under the same budget, and without altering AD or physics losses improves accuracy, reproducibility, and transferability.

</details>


### [345] [Regret Guarantees for Linear Contextual Stochastic Shortest Path](https://arxiv.org/abs/2511.12534)
*Dor Polikar,Alon Cohen*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We define the problem of linear Contextual Stochastic Shortest Path (CSSP), where at the beginning of each episode, the learner observes an adversarially chosen context that determines the MDP through a fixed but unknown linear function. The learner's objective is to reach a designated goal state with minimal expected cumulative loss, despite having no prior knowledge of the transition dynamics, loss functions, or the mapping from context to MDP. In this work, we propose LR-CSSP, an algorithm that achieves a regret bound of $\widetilde{O}(K^{2/3} d^{2/3} |S| |A|^{1/3} B_\star^2 T_\star \log (1/ δ))$, where $K$ is the number of episodes, $d$ is the context dimension, $S$ and $A$ are the sets of states and actions respectively, $B_\star$ bounds the optimal cumulative loss and $T_\star$, unknown to the learner, bounds the expected time for the optimal policy to reach the goal. In the case where all costs exceed $\ell_{\min}$, LR-CSSP attains a regret of $\widetilde O(\sqrt{K \cdot d^2 |S|^3 |A| B_\star^3 \log(1/δ)/\ell_{\min}})$. Unlike in contextual finite-horizon MDPs, where limited knowledge primarily leads to higher losses and regret, in the CSSP setting, insufficient knowledge can also prolong episodes and may even lead to non-terminating episodes. Our analysis reveals that LR-CSSP effectively handles continuous context spaces, while ensuring all episodes terminate within a reasonable number of time steps.

</details>


### [346] [CAO: Curvature-Adaptive Optimization via Periodic Low-Rank Hessian Sketching](https://arxiv.org/abs/2511.12548)
*Wenzhang Du*

Main category: cs.LG

TL;DR: 研究一种曲率自适应方法，在L - 光滑非凸目标下有更好步长范围，在CIFAR - 10/100上更快进入低损失区域，性能对草图秩不敏感。


<details>
  <summary>Details</summary>
Motivation: 一阶优化器在尖锐、各向异性区域速度慢，需要更好的优化方法。

Method: 周期性地通过Hessian - 向量积绘制低秩Hessian子空间，仅在该子空间中对梯度进行预条件处理。

Result: 在CIFAR - 10/100和ResNet - 18/34上，比Adam更快进入低损失区域，达到预定义训练损失阈值的速度快2.95倍，且最终测试精度相当。

Conclusion: 该方法是单旋钮的，性能对草图秩k不敏感，k = 0可进行无曲率消融实验。

Abstract: First-order optimizers are reliable but slow in sharp, anisotropic regions. We study a curvature-adaptive method that periodically sketches a low-rank Hessian subspace via Hessian--vector products and preconditions gradients only in that subspace, leaving the orthogonal complement first-order. For L-smooth non-convex objectives, we recover the standard O(1/T) stationarity guarantee with a widened stable stepsize range; under a Polyak--Lojasiewicz (PL) condition with bounded residual curvature outside the sketch, the loss contracts at refresh steps. On CIFAR-10/100 with ResNet-18/34, the method enters the low-loss region substantially earlier: measured by epochs to a pre-declared train-loss threshold (0.75), it reaches the threshold 2.95x faster than Adam on CIFAR-100/ResNet-18, while matching final test accuracy. The approach is one-knob: performance is insensitive to the sketch rank k across {1,3,5}, and k=0 yields a principled curvature-free ablation. We release anonymized logs and scripts that regenerate all figures and tables.

</details>


### [347] [Training Instabilities Induce Flatness Bias in Gradient Descent](https://arxiv.org/abs/2511.12558)
*Lawrence Wang,Stephen J. Roberts*

Main category: cs.LG

TL;DR: 研究表明梯度下降中的训练不稳定性可产生隐式偏差，驱动参数向损失景观更平坦区域，改善泛化性，该理论适用于随机梯度下降，恢复Adam的不稳定性也可提升泛化。


<details>
  <summary>Details</summary>
Motivation: 经典梯度下降分析的稳定性阈值不适用于现代深度网络，需探究超阈值时性能更好的原因。

Method: 研究梯度下降中不稳定性引发的隐式偏差，分析Hessian矩阵主特征向量旋转的几何现象（RPE）。

Result: 不稳定性驱动参数向更平坦区域，促进探索并得到更平坦最小值；理论适用于随机梯度下降，不稳定性影响超迷你批噪声；恢复Adam不稳定性提升泛化。

Conclusion: 明确了训练不稳定性在深度学习中的积极作用。

Abstract: Classical analyses of gradient descent (GD) define a stability threshold based on the largest eigenvalue of the loss Hessian, often termed sharpness. When the learning rate lies below this threshold, training is stable and the loss decreases monotonically. Yet, modern deep networks often achieve their best performance beyond this regime.
  We demonstrate that such instabilities induce an implicit bias in GD, driving parameters toward flatter regions of the loss landscape and thereby improving generalization. The key mechanism is the Rotational Polarity of Eigenvectors (RPE), a geometric phenomenon in which the leading eigenvectors of the Hessian rotate during training instabilities. These rotations, which increase with learning rates, promote exploration and provably lead to flatter minima.
  This theoretical framework extends to stochastic GD, where instability-driven flattening persists and its empirical effects outweigh minibatch noise. Finally, we show that restoring instabilities in Adam further improves generalization.
  Together, these results establish and understand the constructive role of training instabilities in deep learning.

</details>


### [348] [Linear time small coresets for k-mean clustering of segments with applications](https://arxiv.org/abs/2511.12564)
*David Denisov,Shlomi Dolev,Dan Felmdan,Michael Segal*

Main category: cs.LG

TL;DR: 研究n个线段的k - means问题，提出首个能处理任意输入线段的coreset构造方法，理论上常数k和ε时能生成大小为O(log²n)的coreset，实验验证了方法效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决n个线段的k - means问题，包括处理异常值、使用替代距离函数等变体情况，实现高效的流、分布式或并行计算。

Method: 提出首个能处理任意输入线段的coreset构造方法。

Result: 对于常数k和ε，生成大小为O(log²n)的coreset，计算时间为O(nd)，实验在实时视频跟踪应用中实现显著加速且聚类精度损失极小。

Conclusion: 所提出的方法具有实际效率和理论保证。

Abstract: We study the $k$-means problem for a set $\mathcal{S} \subseteq \mathbb{R}^d$ of $n$ segments, aiming to find $k$ centers $X \subseteq \mathbb{R}^d$ that minimize
  $D(\mathcal{S},X) := \sum_{S \in \mathcal{S}} \min_{x \in X} D(S,x)$, where $D(S,x) := \int_{p \in S} |p - x| dp$
  measures the total distance from each point along a segment to a center. Variants of this problem include handling outliers, employing alternative distance functions such as M-estimators, weighting distances to achieve balanced clustering, or enforcing unique cluster assignments. For any $\varepsilon > 0$, an $\varepsilon$-coreset is a weighted subset $C \subseteq \mathbb{R}^d$ that approximates $D(\mathcal{S},X)$ within a factor of $1 \pm \varepsilon$ for any set of $k$ centers, enabling efficient streaming, distributed, or parallel computation. We propose the first coreset construction that provably handles arbitrary input segments. For constant $k$ and $\varepsilon$, it produces a coreset of size $O(\log^2 n)$ computable in $O(nd)$ time. Experiments, including a real-time video tracking application, demonstrate substantial speedups with minimal loss in clustering accuracy, confirming both the practical efficiency and theoretical guarantees of our method.

</details>


### [349] [Enhancing Machine Learning Model Efficiency through Quantization and Bit Depth Optimization: A Performance Analysis on Healthcare Data](https://arxiv.org/abs/2511.12568)
*Mitul Goswami,Romit Chatterjee*

Main category: cs.LG

TL;DR: 研究通过量化和位深度优化技术优化复杂学习模型，在医疗数据集上验证，降低时间复杂度且模型精度损失小，结论是优化效果因参数而异。


<details>
  <summary>Details</summary>
Motivation: 解决复杂模型执行时间长的问题，在降低时间复杂度的同时保持模型效率。

Method: 使用两个医疗数据集，应用逻辑回归机器学习模型，采用量化和位深度优化策略将输入数据从float64降至float32和int32。

Result: 显著降低了时间复杂度，优化后模型精度仅有极小下降。

Conclusion: 优化技术的影响因一组参数而异。

Abstract: This research aims to optimize intricate learning models by implementing quantization and bit-depth optimization techniques. The objective is to significantly cut time complexity while preserving model efficiency, thus addressing the challenge of extended execution times in intricate models. Two medical datasets were utilized as case studies to apply a Logistic Regression (LR) machine learning model. Using efficient quantization and bit depth optimization strategies the input data is downscaled from float64 to float32 and int32. The results demonstrated a significant reduction in time complexity, with only a minimal decrease in model accuracy post-optimization, showcasing the state-of-the-art optimization approach. This comprehensive study concludes that the impact of these optimization techniques varies depending on a set of parameters.

</details>


### [350] [LMM-IR: Large-Scale Netlist-Aware Multimodal Framework for Static IR-Drop Prediction](https://arxiv.org/abs/2511.12581)
*Kai Ma,Zhen Wang,Hongquan He,Qi Xu,Tinghuan Chen,Hao Geng*

Main category: cs.LG

TL;DR: 本文提出基于LNT的多模态方法处理SPICE文件进行静态IR压降预测，实验结果佳。


<details>
  <summary>Details</summary>
Motivation: 静态IR压降分析耗时且需迭代，计算负担大，需快速准确的IR压降预测来减少芯片设计时间。

Method: 提出基于LNT的多模态方法，将网表拓扑表示为3D点云，把各类数据编码为特征输入模型预测。

Result: 算法在ICCAD 2023竞赛获胜团队和现有算法中取得最佳F1分数和最低MAE。

Conclusion: 所提算法能高效处理网表数据进行静态电压降预测，性能优越。

Abstract: Static IR drop analysis is a fundamental and critical task in the field of chip design. Nevertheless, this process can be quite time-consuming, potentially requiring several hours. Moreover, addressing IR drop violations frequently demands iterative analysis, thereby causing the computational burden. Therefore, fast and accurate IR drop prediction is vital for reducing the overall time invested in chip design. In this paper, we firstly propose a novel multimodal approach that efficiently processes SPICE files through large-scale netlist transformer (LNT). Our key innovation is representing and processing netlist topology as 3D point cloud representations, enabling efficient handling of netlist with up to hundreds of thousands to millions nodes. All types of data, including netlist files and image data, are encoded into latent space as features and fed into the model for static voltage drop prediction. This enables the integration of data from multiple modalities for complementary predictions. Experimental results demonstrate that our proposed algorithm can achieve the best F1 score and the lowest MAE among the winning teams of the ICCAD 2023 contest and the state-of-the-art algorithms.

</details>


### [351] [Symmetry-Aware Graph Metanetwork Autoencoders: Model Merging through Parameter Canonicalization](https://arxiv.org/abs/2511.12601)
*Odysseas Boufalis,Jorge Carrasco-Pollo,Joshua Rosenthal,Eduardo Terres-Caballero,Alejandro García-Castellanos*

Main category: cs.LG

TL;DR: 论文提出ScaleGMNs架构，结合置换和缩放对称性，利用自编码器框架对齐网络，避免显式解决分配问题，实现模型合并。


<details>
  <summary>Details</summary>
Motivation: 以往工作仅处理置换对称性且计算密集，本文希望扩展方法，同时纳入缩放对称性来对齐网络。

Method: 提出ScaleGMNs架构，构建自编码器框架，以ScaleGMNs作为不变编码器。

Result: 实验表明方法能在置换和缩放对称性下对齐INRs和CNNs，避免显式解决分配问题，使相似网络在同一盆地收敛。

Conclusion: 该方法可促进模型合并，实现平滑线性插值并避免高损失区域，代码已公开。

Abstract: Neural network parameterizations exhibit inherent symmetries that yield multiple equivalent minima within the loss landscape. Scale Graph Metanetworks (ScaleGMNs) explicitly leverage these symmetries by proposing an architecture equivariant to both permutation and parameter scaling transformations. Previous work by Ainsworth et al. (2023) addressed permutation symmetries through a computationally intensive combinatorial assignment problem, demonstrating that leveraging permutation symmetries alone can map networks into a shared loss basin. In this work, we extend their approach by also incorporating scaling symmetries, presenting an autoencoder framework utilizing ScaleGMNs as invariant encoders. Experimental results demonstrate that our method aligns Implicit Neural Representations (INRs) and Convolutional Neural Networks (CNNs) under both permutation and scaling symmetries without explicitly solving the assignment problem. This approach ensures that similar networks naturally converge within the same basin, facilitating model merging, i.e., smooth linear interpolation while avoiding regions of high loss. The code is publicly available on our GitHub repository.

</details>


### [352] [PID-controlled Langevin Dynamics for Faster Sampling of Generative Models](https://arxiv.org/abs/2511.12603)
*Hongyi Chen,Jianhai Shu,Jingtao Ding,Yong Li,Xiao-Ping Zhang*

Main category: cs.LG

TL;DR: 提出PIDLD算法加速Langevin动力学采样，无需额外训练等，实验证明其高效性。


<details>
  <summary>Details</summary>
Motivation: Langevin动力学采样速度极慢，受限于大量细粒度迭代以收敛到目标分布。

Method: 用控制理论原则重新解释采样过程，将能量梯度作为反馈信号，结合历史梯度和梯度趋势，自适应稳定采样过程。

Result: 在图像生成和推理任务的大量实验中，PIDLD能用更少步骤生成更高质量样本。

Conclusion: PIDLD使基于Langevin的生成模型更适用于对效率要求高的应用。

Abstract: Langevin dynamics sampling suffers from extremely low generation speed, fundamentally limited by numerous fine-grained iterations to converge to the target distribution. We introduce PID-controlled Langevin Dynamics (PIDLD), a novel sampling acceleration algorithm that reinterprets the sampling process using control-theoretic principles. By treating energy gradients as feedback signals, PIDLD combines historical gradients (the integral term) and gradient trends (the derivative term) to efficiently traverse energy landscapes and adaptively stabilize, thereby significantly reducing the number of iterations required to produce high-quality samples. Our approach requires no additional training, datasets, or prior information, making it immediately integrable with any Langevin-based method. Extensive experiments across image generation and reasoning tasks demonstrate that PIDLD achieves higher quality with fewer steps, making Langevin-based generative models more practical for efficiency-critical applications. The implementation can be found at \href{https://github.com/tsinghua-fib-lab/PIDLD}{https://github.com/tsinghua-fib-lab/PIDLD}.

</details>


### [353] [FedTopo: Topology-Informed Representation Alignment in Federated Learning under Non-I.I.D. Conditions](https://arxiv.org/abs/2511.12628)
*Ke Hu,Liyao Xiang,Peng Tang,Weidong Qiu*

Main category: cs.LG

TL;DR: 提出FedTopo框架解决联邦学习在非独立同分布数据下模型退化问题，实验表明其能加速收敛并提高准确率。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习模型在非独立同分布数据下因特征表示发散和局部目标无法捕捉全局拓扑而退化，需要解决该问题。

Method: 提出FedTopo框架，集成TGBS和TE，利用TAL实现跨客户端表示对齐。TGBS筛选拓扑信息丰富块，TE量化拓扑信息，TAL保证拓扑一致性。

Result: 在Fashion - MNIST、CIFAR - 10和CIFAR - 100的四种非独立同分布划分实验中，FedTopo加速收敛且准确率高于强基线。

Conclusion: FedTopo能有效解决联邦学习在非独立同分布数据下的模型退化问题。

Abstract: Current federated-learning models deteriorate under heterogeneous (non-I.I.D.) client data, as their feature representations diverge and pixel- or patch-level objectives fail to capture the global topology which is essential for high-dimensional visual tasks. We propose FedTopo, a framework that integrates Topological-Guided Block Screening (TGBS) and Topological Embedding (TE) to leverage topological information, yielding coherently aligned cross-client representations by Topological Alignment Loss (TAL). First, Topology-Guided Block Screening (TGBS) automatically selects the most topology-informative block, i.e., the one with maximal topological separability, whose persistence-based signatures best distinguish within- versus between-class pairs, ensuring that subsequent analysis focuses on topology-rich features. Next, this block yields a compact Topological Embedding, which quantifies the topological information for each client. Finally, a Topological Alignment Loss (TAL) guides clients to maintain topological consistency with the global model during optimization, reducing representation drift across rounds. Experiments on Fashion-MNIST, CIFAR-10, and CIFAR-100 under four non-I.I.D. partitions show that FedTopo accelerates convergence and improves accuracy over strong baselines.

</details>


### [354] [NFQ2.0: The CartPole Benchmark Revisited](https://arxiv.org/abs/2511.12644)
*Sascha Lange,Roland Hafner,Martin Riedmiller*

Main category: cs.LG

TL;DR: 本文回顾NFQ算法，提出改进版NFQ2.0用于CartPole任务，通过消融研究提升其性能和稳定性，助力工业场景应用。


<details>
  <summary>Details</summary>
Motivation: NFQ算法虽为早期深度强化学习先驱，但需大量调参且在现实控制问题中难以复现，需改进以提升学习过程的可重复性和鲁棒性。

Method: 提出现代化变体NFQ2.0并应用于CartPole任务，通过消融研究确定关键设计决策和超参数。

Result: 明确了提升NFQ2.0性能和稳定性的关键设计决策和超参数。

Conclusion: 研究结果有助于从业者复现和改进结果，更有效地在工业环境中应用深度强化学习。

Abstract: This article revisits the 20-year-old neural fitted Q-iteration (NFQ) algorithm on its classical CartPole benchmark. NFQ was a pioneering approach towards modern Deep Reinforcement Learning (Deep RL) in applying multi-layer neural networks to reinforcement learning for real-world control problems. We explore the algorithm's conceptual simplicity and its transition from online to batch learning, which contributed to its stability. Despite its initial success, NFQ required extensive tuning and was not easily reproducible on real-world control problems. We propose a modernized variant NFQ2.0 and apply it to the CartPole task, concentrating on a real-world system build from standard industrial components, to investigate and improve the learning process's repeatability and robustness. Through ablation studies, we highlight key design decisions and hyperparameters that enhance performance and stability of NFQ2.0 over the original variant. Finally, we demonstrate how our findings can assist practitioners in reproducing and improving results and applying deep reinforcement learning more effectively in industrial contexts.

</details>


### [355] [FLClear: Visually Verifiable Multi-Client Watermarking for Federated Learning](https://arxiv.org/abs/2511.12663)
*Chen Gu,Yingying Sun,Yifan She,Donghui Hu*

Main category: cs.LG

TL;DR: 提出FLClear框架解决联邦学习水印现有问题，实验证明其有效性且优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中客户端模型知识产权需保护，但现有水印方法存在碰撞、安全不足和验证不直观等问题。

Method: 引入与对比学习联合优化的转置模型整合水印和主任务目标，验证时从转置模型重构水印并通过视觉检查和结构相似性指标评估。

Result: 在不同数据集、聚合方案和攻击场景的综合实验中，FLClear有效且始终优于现有联邦学习水印方法。

Conclusion: FLClear能实现无碰撞水印聚合、增强水印安全性和视觉可解释的所有权验证。

Abstract: Federated learning (FL) enables multiple clients to collaboratively train a shared global model while preserving the privacy of their local data. Within this paradigm, the intellectual property rights (IPR) of client models are critical assets that must be protected. In practice, the central server responsible for maintaining the global model may maliciously manipulate the global model to erase client contributions or falsely claim sole ownership, thereby infringing on clients' IPR. Watermarking has emerged as a promising technique for asserting model ownership and protecting intellectual property. However, existing FL watermarking approaches remain limited, suffering from potential watermark collisions among clients, insufficient watermark security, and non-intuitive verification mechanisms. In this paper, we propose FLClear, a novel framework that simultaneously achieves collision-free watermark aggregation, enhanced watermark security, and visually interpretable ownership verification. Specifically, FLClear introduces a transposed model jointly optimized with contrastive learning to integrate the watermarking and main task objectives. During verification, the watermark is reconstructed from the transposed model and evaluated through both visual inspection and structural similarity metrics, enabling intuitive and quantitative ownership verification. Comprehensive experiments conducted over various datasets, aggregation schemes, and attack scenarios demonstrate the effectiveness of FLClear and confirm that it consistently outperforms state-of-the-art FL watermarking methods.

</details>


### [356] [Attention-Enhanced Convolutional Autoencoder and Structured Delay Embeddings for Weather Prediction](https://arxiv.org/abs/2511.12682)
*Amirpasha Hedayat,Karthik Duraisamy*

Main category: cs.LG

TL;DR: 提出高效降阶建模框架用于短期天气预报，在训练数据期内表现良好，但泛化有局限，揭示了降阶建模挑战和混合方法机会。


<details>
  <summary>Details</summary>
Motivation: 解决天气预报中复杂高维动力系统的降阶建模问题，且相比AI驱动模型更注重效率。

Method: 开发基于ResNet并由块注意力模块增强的卷积自动编码器对高维气象数据降维，在潜在空间的时间延迟嵌入中学习线性算子捕捉动态。

Result: 该框架在训练数据期内有效预测天气模式，但在泛化到未来状态有局限，投影误差是主要瓶颈。

Conclusion: 揭示了混沌系统降阶建模的关键挑战，指出结合高效降阶模型和复杂AI架构的混合方法在长期气候建模中有机会。

Abstract: Weather prediction is a quintessential problem involving the forecasting of a complex, nonlinear, and chaotic high-dimensional dynamical system. This work introduces an efficient reduced-order modeling (ROM) framework for short-range weather prediction and investigates fundamental questions in dimensionality reduction and reduced order modeling of such systems. Unlike recent AI-driven models, which require extensive computational resources, our framework prioritizes efficiency while achieving reasonable accuracy. Specifically, a ResNet-based convolutional autoencoder augmented by block attention modules is developed to reduce the dimensionality of high-dimensional weather data. Subsequently, a linear operator is learned in the time-delayed embedding of the latent space to efficiently capture the dynamics. Using the ERA5 reanalysis dataset, we demonstrate that this framework performs well in-distribution as evidenced by effectively predicting weather patterns within training data periods. We also identify important limitations in generalizing to future states, particularly in maintaining prediction accuracy beyond the training window. Our analysis reveals that weather systems exhibit strong temporal correlations that can be effectively captured through linear operations in an appropriately constructed embedding space, and that projection error rather than inference error is the main bottleneck. These findings shed light on some key challenges in reduced-order modeling of chaotic systems and point toward opportunities for hybrid approaches that combine efficient reduced-order models as baselines with more sophisticated AI architectures, particularly for applications in long-term climate modeling where computational efficiency is paramount.

</details>


### [357] [Beyond Fixed Tasks: Unsupervised Environment Design for Task-Level Pairs](https://arxiv.org/abs/2511.12706)
*Daniel Furelos-Blanco,Charles Pert,Frederik Kelbel,Alex F. Spies,Alessandra Russo,Michael Dennis*

Main category: cs.LG

TL;DR: 提出ATLAS方法生成任务和关卡的联合自动课程，在Minigrid关卡中实验表明其优于随机采样方法，利用结构的突变可加速策略收敛。


<details>
  <summary>Details</summary>
Motivation: 训练通用智能体在复杂环境中遵循复杂指令存在挑战，随机采样任务 - 关卡对常产生不可解组合，现有无监督环境设计仅考虑固定任务，因此需联合设计任务和关卡。

Method: 提出ATLAS方法，基于无监督环境设计自动生成可解且具挑战性的任务 - 关卡对用于策略训练，引入将任务建模为奖励机器的评估套件。

Result: ATLAS在实验中大大优于随机采样方法，尤其是在采样可解对不太可能的情况下；利用任务和关卡结构的突变能加速收敛到高性能策略。

Conclusion: ATLAS方法有效，为联合设计任务和关卡提供了可行方案，推动了该领域的发展。

Abstract: Training general agents to follow complex instructions (tasks) in intricate environments (levels) remains a core challenge in reinforcement learning. Random sampling of task-level pairs often produces unsolvable combinations, highlighting the need to co-design tasks and levels. While unsupervised environment design (UED) has proven effective at automatically designing level curricula, prior work has only considered a fixed task. We present ATLAS (Aligning Tasks and Levels for Autocurricula of Specifications), a novel method that generates joint autocurricula over tasks and levels. Our approach builds upon UED to automatically produce solvable yet challenging task-level pairs for policy training. To evaluate ATLAS and drive progress in the field, we introduce an evaluation suite that models tasks as reward machines in Minigrid levels. Experiments demonstrate that ATLAS vastly outperforms random sampling approaches, particularly when sampling solvable pairs is unlikely. We further show that mutations leveraging the structure of both tasks and levels accelerate convergence to performant policies.

</details>


### [358] [Adaptive Graph Rewiring to Mitigate Over-Squashing in Mesh-Based GNNs for Fluid Dynamics Simulations](https://arxiv.org/abs/2511.12709)
*Sangwoo Seo,Hyunsung Kim,Jiwan Kim,Chanyoung Park*

Main category: cs.LG

TL;DR: 提出AdaMeshNet框架解决基于网格的GNN模拟流体动力学时的过压缩问题，实验显示其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有网格细化技术会导致基于网格的GNN出现过压缩问题，传统图重连方法物理上不现实，需新方法解决。

Method: 提出AdaMeshNet框架，在消息传递过程引入自适应重连，计算瓶颈节点重连延迟分数，动态选择重连层。

Result: 在基于网格的流体模拟实验中，AdaMeshNet优于传统重连方法。

Conclusion: AdaMeshNet能有效模拟物理交互的顺序性，实现更准确预测。

Abstract: Mesh-based simulation using Graph Neural Networks (GNNs) has been recognized as a promising approach for modeling fluid dynamics. However, the mesh refinement techniques which allocate finer resolution to regions with steep gradients can induce the over-squashing problem in mesh-based GNNs, which prevents the capture of long-range physical interactions. Conventional graph rewiring methods attempt to alleviate this issue by adding new edges, but they typically complete all rewiring operations before applying them to the GNN. These approaches are physically unrealistic, as they assume instantaneous interactions between distant nodes and disregard the distance information between particles. To address these limitations, we propose a novel framework, called Adaptive Graph Rewiring in Mesh-Based Graph Neural Networks (AdaMeshNet), that introduces an adaptive rewiring process into the message-passing procedure to model the gradual propagation of physical interactions. Our method computes a rewiring delay score for bottleneck nodes in the mesh graph, based on the shortest-path distance and the velocity difference. Using this score, it dynamically selects the message-passing layer at which new edges are rewired, which can lead to adaptive rewiring in a mesh graph. Extensive experiments on mesh-based fluid simulations demonstrate that AdaMeshNet outperforms conventional rewiring methods, effectively modeling the sequential nature of physical interactions and enabling more accurate predictions.

</details>


### [359] [Oxytrees: Model Trees for Bipartite Learning](https://arxiv.org/abs/2511.12713)
*Pedro Ilídio,Felipe Kenji Nakano,Alireza Gharahighehi,Robbe D'hondt,Ricardo Cerri,Celine Vens*

Main category: cs.LG

TL;DR: 提出Oxytrees模型解决二部学习现存问题，减少训练和预测时间，在多数据集上测试表现良好并提供Python API。


<details>
  <summary>Details</summary>
Motivation: 当前二部学习方法存在针对特定应用、难以泛化和可扩展性差的问题。

Method: 提出Oxytrees模型，压缩交互矩阵，采用新的叶分配算法，在叶节点使用基于Kronecker积核的线性模型。

Result: 与现有技术相比，训练时间最多提升30倍，在多数评估设置中表现有竞争力或更优。

Conclusion: Oxytrees能有效解决二部学习现存问题，提供的Python API可促进该领域的可重复研究。

Abstract: Bipartite learning is a machine learning task that aims to predict interactions between pairs of instances. It has been applied to various domains, including drug-target interactions, RNA-disease associations, and regulatory network inference. Despite being widely investigated, current methods still present drawbacks, as they are often designed for a specific application and thus do not generalize to other problems or present scalability issues. To address these challenges, we propose Oxytrees: proxy-based biclustering model trees. Oxytrees compress the interaction matrix into row- and column-wise proxy matrices, significantly reducing training time without compromising predictive performance. We also propose a new leaf-assignment algorithm that significantly reduces the time taken for prediction. Finally, Oxytrees employ linear models using the Kronecker product kernel in their leaves, resulting in shallower trees and thus even faster training. Using 15 datasets, we compared the predictive performance of ensembles of Oxytrees with that of the current state-of-the-art. We achieved up to 30-fold improvement in training times compared to state-of-the-art biclustering forests, while demonstrating competitive or superior performance in most evaluation settings, particularly in the inductive setting. Finally, we provide an intuitive Python API to access all datasets, methods and evaluation measures used in this work, thus enabling reproducible research in this field.

</details>


### [360] [On Robustness of Linear Classifiers to Targeted Data Poisoning](https://arxiv.org/abs/2511.12722)
*Nakshatra Gupta,Sumanth Prabhu,Supratik Chakraborty,R Venkatesh*

Main category: cs.LG

TL;DR: 本文聚焦自动测量数据集对目标数据投毒攻击的鲁棒性，证明求鲁棒性是NP完全问题，提出求上下界的技术并实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于训练数据集通常规模大，手动检测投毒困难，因此要自动测量数据集对目标数据投毒攻击的鲁棒性。

Method: 考虑对手只能扰动训练数据集标签且知识限于受害者模型假设空间的威胁模型，证明求鲁棒性是NP完全问题，提出求鲁棒性上下界的技术。

Result: 所提技术能为许多公开数据集高效计算上下界；投毒超出确定的鲁棒性边界会显著影响测试点分类；能在很多现有技术失效的情况下计算上下界。

Conclusion: 所提方法能有效测量数据集对目标数据投毒攻击的鲁棒性。

Abstract: Data poisoning is a training-time attack that undermines the trustworthiness of learned models. In a targeted data poisoning attack, an adversary manipulates the training dataset to alter the classification of a targeted test point. Given the typically large size of training dataset, manual detection of poisoning is difficult. An alternative is to automatically measure a dataset's robustness against such an attack, which is the focus of this paper. We consider a threat model wherein an adversary can only perturb the labels of the training dataset, with knowledge limited to the hypothesis space of the victim's model. In this setting, we prove that finding the robustness is an NP-Complete problem, even when hypotheses are linear classifiers. To overcome this, we present a technique that finds lower and upper bounds of robustness. Our implementation of the technique computes these bounds efficiently in practice for many publicly available datasets. We experimentally demonstrate the effectiveness of our approach. Specifically, a poisoning exceeding the identified robustness bounds significantly impacts test point classification. We are also able to compute these bounds in many more cases where state-of-the-art techniques fail.

</details>


### [361] [LAYA: Layer-wise Attention Aggregation for Interpretable Depth-Aware Neural Networks](https://arxiv.org/abs/2511.12723)
*Gennaro Vessio*

Main category: cs.LG

TL;DR: 本文提出LAYA输出头，通过注意力动态聚合内部表征，在视觉和语言基准测试中表现良好，还能提供可解释性信号。


<details>
  <summary>Details</summary>
Motivation: 传统深度神经网络仅依赖最后隐藏层表示做预测，会丢弃中间层丰富互补信息，因此重新审视输出层作用。

Method: 引入LAYA，即层注意力聚合器，学习输入条件下各层特征的注意力权重以合成预测。

Result: 在视觉和语言基准测试中，LAYA表现与标准输出头相当或更好，准确率相对提升达约1个百分点，还能直接给出层归因分数。

Conclusion: LAYA是一种可解释且与架构无关的预测合成机制，代码已开源。

Abstract: Deep neural networks typically rely on the representation produced by their final hidden layer to make predictions, implicitly assuming that this single vector fully captures the semantics encoded across all preceding transformations. However, intermediate layers contain rich and complementary information -- ranging from low-level patterns to high-level abstractions -- that is often discarded when the decision head depends solely on the last representation. This paper revisits the role of the output layer and introduces LAYA (Layer-wise Attention Aggregator), a novel output head that dynamically aggregates internal representations through attention. Instead of projecting only the deepest embedding, LAYA learns input-conditioned attention weights over layer-wise features, yielding an interpretable and architecture-agnostic mechanism for synthesizing predictions. Experiments on vision and language benchmarks show that LAYA consistently matches or improves the performance of standard output heads, with relative gains of up to about one percentage point in accuracy, while providing explicit layer-attribution scores that reveal how different abstraction levels contribute to each decision. Crucially, these interpretability signals emerge directly from the model's computation, without any external post hoc explanations. The code to reproduce LAYA is publicly available at: https://github.com/gvessio/LAYA.

</details>


### [362] [Convolutional Model Trees](https://arxiv.org/abs/2511.12725)
*William Ward Armstrong*

Main category: cs.LG

TL;DR: 提出创建模型树森林拟合图像函数样本的方法，处理图像畸变，给出平滑输出理论方法并证明训练过程收敛。


<details>
  <summary>Details</summary>
Motivation: 为了更好地拟合定义在图像上的函数样本，处理图像的各种畸变。

Method: 对图像进行下采样，确定树的超平面，对超平面应用卷积处理小畸变，创建模型树森林，给出平滑输出理论方法。

Result: 实现了对图像函数样本的拟合，能处理较大畸变，训练过程收敛。

Conclusion: 所提出的创建模型树森林的方法有效，可用于图像函数拟合及畸变处理，训练过程具有收敛性。

Abstract: A method for creating a forest of model trees to fit samples of a function defined on images is described in several steps: down-sampling the images, determining a tree's hyperplanes, applying convolutions to the hyperplanes to handle small distortions of training images, and creating forests of model trees to increase accuracy and achieve a smooth fit. A 1-to-1 correspondence among pixels of images, coefficients of hyperplanes and coefficients of leaf functions offers the possibility of dealing with larger distortions such as arbitrary rotations or changes of perspective. A theoretical method for smoothing forest outputs to produce a continuously differentiable approximation is described. Within that framework, a training procedure is proved to converge.

</details>


### [363] [Stabilizing Self-Consuming Diffusion Models with Latent Space Filtering](https://arxiv.org/abs/2511.12742)
*Zhongteng Cai,Yaxuan Wang,Yang Liu,Xueru Zhang*

Main category: cs.LG

TL;DR: 合成数据循环使用致模型崩溃，提出潜空间过滤法缓解，实验效果优且成本低。


<details>
  <summary>Details</summary>
Motivation: 合成数据循环使用会导致训练不稳定和模型崩溃，现有解决策略存在计算成本高或需人工标注的问题。

Method: 实证分析自消耗扩散模型潜空间动态，提出潜空间过滤法（LSF）过滤混合数据集中不真实的合成数据。

Result: LSF在多个真实数据集上始终优于现有基线，有效缓解模型崩溃，且不增加训练成本和依赖人工标注。

Conclusion: 潜空间过滤法能有效解决合成数据循环使用带来的模型崩溃问题，且具有成本优势。

Abstract: As synthetic data proliferates across the Internet, it is often reused to train successive generations of generative models. This creates a ``self-consuming loop" that can lead to training instability or \textit{model collapse}. Common strategies to address the issue -- such as accumulating historical training data or injecting fresh real data -- either increase computational cost or require expensive human annotation. In this paper, we empirically analyze the latent space dynamics of self-consuming diffusion models and observe that the low-dimensional structure of latent representations extracted from synthetic data degrade over generations. Based on this insight, we propose \textit{Latent Space Filtering} (LSF), a novel approach that mitigates model collapse by filtering out less realistic synthetic data from mixed datasets. Theoretically, we present a framework that connects latent space degradation to empirical observations. Experimentally, we show that LSF consistently outperforms existing baselines across multiple real-world datasets, effectively mitigating model collapse without increasing training cost or relying on human annotation.

</details>


### [364] [DIVIDE: A Framework for Learning from Independent Multi-Mechanism Data Using Deep Encoders and Gaussian Processes](https://arxiv.org/abs/2511.12745)
*Vivek Chawla,Boris Slautin,Utkarsh Pratiush,Dayakar Penumadu,Sergei Kalinin*

Main category: cs.LG

TL;DR: 介绍DIVIDE框架，可分解科学数据集多机制影响，在多数据集验证有效，能扩展到多功能数据集。


<details>
  <summary>Details</summary>
Motivation: 科学数据集多机制综合影响会掩盖各机制单独作用，需分解。

Method: 将特定机制深度编码器与结构化高斯过程在联合潜空间集成，编码器分离机制，高斯过程捕捉综合影响。

Result: 在多个数据集上，DIVIDE能分离机制、重现交互，且抗噪。

Conclusion: DIVIDE框架有效，可自然扩展到多功能数据集。

Abstract: Scientific datasets often arise from multiple independent mechanisms such as spatial, categorical or structural effects, whose combined influence obscures their individual contributions. We introduce DIVIDE, a framework that disentangles these influences by integrating mechanism-specific deep encoders with a structured Gaussian Process in a joint latent space. Disentanglement here refers to separating independently acting generative factors. The encoders isolate distinct mechanisms while the Gaussian Process captures their combined effect with calibrated uncertainty. The architecture supports structured priors, enabling interpretable and mechanism-aware prediction as well as efficient active learning. DIVIDE is demonstrated on synthetic datasets combining categorical image patches with nonlinear spatial fields, on FerroSIM spin lattice simulations of ferroelectric patterns, and on experimental PFM hysteresis loops from PbTiO3 films. Across benchmarks, DIVIDE separates mechanisms, reproduces additive and scaled interactions, and remains robust under noise. The framework extends naturally to multifunctional datasets where mechanical, electromagnetic or optical responses coexist.

</details>


### [365] [Are LLMs The Way Forward? A Case Study on LLM-Guided Reinforcement Learning for Decentralized Autonomous Driving](https://arxiv.org/abs/2511.12751)
*Timur Anvar,Jeffrey Chen,Yuyan Wang,Rohan Chandra*

Main category: cs.LG

TL;DR: 研究小的本地部署大语言模型能否通过奖励塑造支持自动驾驶，对比RL、LLM和混合方法，发现各有优劣，小LLM用于安全关键控制任务有局限。


<details>
  <summary>Details</summary>
Motivation: 强化学习依赖奖励函数有局限，大语言模型直接规划控制有缺陷，因此研究小的本地部署大语言模型能否通过奖励塑造支持自动驾驶。

Method: 进行案例研究，对比仅使用强化学习、仅使用大语言模型和混合方法，大语言模型在训练时对状态 - 动作转换评分以增强强化学习奖励，测试时执行标准强化学习策略。

Result: 仅强化学习方法有中等成功率和合理效率；仅大语言模型方法成功率高但速度性能差；混合方法处于两者之间。且受大语言模型影响的方法有保守偏差和模型依赖的变异性。

Conclusion: 当前小的大语言模型用于安全关键控制任务存在重要局限性。

Abstract: Autonomous vehicle navigation in complex environments such as dense and fast-moving highways and merging scenarios remains an active area of research. A key limitation of RL is its reliance on well-specified reward functions, which often fail to capture the full semantic and social complexity of diverse, out-of-distribution situations. As a result, a rapidly growing line of research explores using Large Language Models (LLMs) to replace or supplement RL for direct planning and control, on account of their ability to reason about rich semantic context. However, LLMs present significant drawbacks: they can be unstable in zero-shot safety-critical settings, produce inconsistent outputs, and often depend on expensive API calls with network latency. This motivates our investigation into whether small, locally deployed LLMs (< 14B parameters) can meaningfully support autonomous highway driving through reward shaping rather than direct control. We present a case study comparing RL-only, LLM-only, and hybrid approaches, where LLMs augment RL rewards by scoring state-action transitions during training, while standard RL policies execute at test time. Our findings reveal that RL-only agents achieve moderate success rates (73-89%) with reasonable efficiency, LLM-only agents can reach higher success rates (up to 94%) but with severely degraded speed performance, and hybrid approaches consistently fall between these extremes. Critically, despite explicit efficiency instructions, LLM-influenced approaches exhibit systematic conservative bias with substantial model-dependent variability, highlighting important limitations of current small LLMs for safety-critical control tasks.

</details>


### [366] [INC: An Indirect Neural Corrector for Auto-Regressive Hybrid PDE Solvers](https://arxiv.org/abs/2511.12764)
*Hao Wei,Aleksandra Franz,Bjoern List,Nils Thuerey*

Main category: cs.LG

TL;DR: 提出间接神经校正器（INC）用于偏微分方程模拟校正，减少误差放大，经多场景测试性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 直接应用学习校正到求解器输出会导致显著自回归误差，尤其在混沌状态下，需解决此问题。

Method: 提出间接神经校正器（INC），将学习校正集成到控制方程而非直接更新状态。

Result: INC将长期轨迹性能（R²）提高达158.7%，稳定粗化下的爆炸问题，在复杂3D湍流案例中实现数数量级加速。

Conclusion: INC实现稳定、高效的PDE仿真，正式减少误差，为科学和工程模拟提供可靠物理保证。

Abstract: When simulating partial differential equations, hybrid solvers combine coarse numerical solvers with learned correctors. They promise accelerated simulations while adhering to physical constraints. However, as shown in our theoretical framework, directly applying learned corrections to solver outputs leads to significant autoregressive errors, which originate from amplified perturbations that accumulate during long-term rollouts, especially in chaotic regimes. To overcome this, we propose the Indirect Neural Corrector (\(\mathrm{INC}\)), which integrates learned corrections into the governing equations rather than applying direct state updates. Our key insight is that \(\mathrm{INC}\) reduces the error amplification on the order of \(Δt^{-1} + L\), where \(Δt\) is the timestep and $L$ the Lipschitz constant. At the same time, our framework poses no architectural requirements and integrates seamlessly with arbitrary neural networks and solvers. We test \(\mathrm{INC}\) in extensive benchmarks, covering numerous differentiable solvers, neural backbones, and test cases ranging from a 1D chaotic system to 3D turbulence. INC improves the long-term trajectory performance (\(R^2\)) by up to 158.7\%, stabilizes blowups under aggressive coarsening, and for complex 3D turbulence cases yields speed-ups of several orders of magnitude. INC thus enables stable, efficient PDE emulation with formal error reduction, paving the way for faster scientific and engineering simulations with reliable physics guarantees. Our source code is available at https://github.com/tum-pbs/INC

</details>


### [367] [Scalable Multi-Objective and Meta Reinforcement Learning via Gradient Estimation](https://arxiv.org/abs/2511.12779)
*Zhenshuo Zhang,Minxuan Duan,Youran Ye,Hongyang R. Zhang*

Main category: cs.LG

TL;DR: 本文研究强化学习中多目标策略高效估计问题，提出两阶段方法PolicyGradEx，实验表明其优于现有基线，还分析了策略网络泛化误差。


<details>
  <summary>Details</summary>
Motivation: 在强化学习中，为所有目标学习单一策略在目标数量增长时效果不佳，需要对目标进行分组优化。

Method: 引入两阶段程序，先进行元训练学习元策略，再微调元策略；基于策略评估算法估计任务亲和度矩阵，对目标进行聚类。

Result: 在机器人控制和Meta - World基准测试中，平均优于现有基线16%，速度提升达26倍；消融实验验证各组件有效性；分析了策略网络泛化误差。

Conclusion: 所提方法能有效解决强化学习中多目标策略估计问题，且性能良好。

Abstract: We study the problem of efficiently estimating policies that simultaneously optimize multiple objectives in reinforcement learning (RL). Given $n$ objectives (or tasks), we seek the optimal partition of these objectives into $k \ll n$ groups, where each group comprises related objectives that can be trained together. This problem arises in applications such as robotics, control, and preference optimization in language models, where learning a single policy for all $n$ objectives is suboptimal as $n$ grows. We introduce a two-stage procedure -- meta-training followed by fine-tuning -- to address this problem. We first learn a meta-policy for all objectives using multitask learning. Then, we adapt the meta-policy to multiple randomly sampled subsets of objectives. The adaptation step leverages a first-order approximation property of well-trained policy networks, which is empirically verified to be accurate within a $2\%$ error margin across various RL environments. The resulting algorithm, PolicyGradEx, efficiently estimates an aggregate task-affinity score matrix given a policy evaluation algorithm. Based on the estimated affinity score matrix, we cluster the $n$ objectives into $k$ groups by maximizing the intra-cluster affinity scores. Experiments on three robotic control and the Meta-World benchmarks demonstrate that our approach outperforms state-of-the-art baselines by $16\%$ on average, while delivering up to $26\times$ faster speedup relative to performing full training to obtain the clusters. Ablation studies validate each component of our approach. For instance, compared with random grouping and gradient-similarity-based grouping, our loss-based clustering yields an improvement of $19\%$. Finally, we analyze the generalization error of policy networks by measuring the Hessian trace of the loss surface, which gives non-vacuous measures relative to the observed generalization errors.

</details>


### [368] [Physics-Constrained Adaptive Neural Networks Enable Real-Time Semiconductor Manufacturing Optimization with Minimal Training Data](https://arxiv.org/abs/2511.12788)
*Rubén Darío Guerrero*

Main category: cs.LG

TL;DR: 提出物理约束自适应学习框架解决半导体EUV光刻优化计算危机，有高精度、少样本等优势。


<details>
  <summary>Details</summary>
Motivation: 半导体行业在EUV光刻优化中传统方法耗时长且精度不足。

Method: 提出物理约束自适应学习框架，用可学习参数校准电磁近似，集成多个可微模块并结合几何匹配目标。

Result: 在15个代表性图案上实现亚纳米EPE性能，比无物理约束CNN基线平均提升69.9%，推理速度快，所需训练样本少90%。

Conclusion: 该方法可作为实时半导体制造优化基础方法，弥合学术与工业应用差距。

Abstract: The semiconductor industry faces a computational crisis in extreme ultraviolet (EUV) lithography optimization, where traditional methods consume billions of CPU hours while failing to achieve sub-nanometer precision. We present a physics-constrained adaptive learning framework that automatically calibrates electromagnetic approximations through learnable parameters $\boldsymbolθ = \{θ_d, θ_a, θ_b, θ_p, θ_c\}$ while simultaneously minimizing Edge Placement Error (EPE) between simulated aerial images and target photomasks. The framework integrates differentiable modules for Fresnel diffraction, material absorption, optical point spread function blur, phase-shift effects, and contrast modulation with direct geometric pattern matching objectives, enabling cross-geometry generalization with minimal training data. Through physics-constrained learning on 15 representative patterns spanning current production to future research nodes, we demonstrate consistent sub-nanometer EPE performance (0.664-2.536 nm range) using only 50 training samples per pattern. Adaptive physics learning achieves an average improvement of 69.9\% over CNN baselines without physics constraints, with a significant inference speedup over rigorous electromagnetic solvers after training completion. This approach requires 90\% fewer training samples through cross-geometry generalization compared to pattern-specific CNN training approaches. This work establishes physics-constrained adaptive learning as a foundational methodology for real-time semiconductor manufacturing optimization, addressing the critical gap between academic physics-informed neural networks and industrial deployment requirements through joint physics calibration and manufacturing precision objectives.

</details>


### [369] [Optimal Look-back Horizon for Time Series Forecasting in Federated Learning](https://arxiv.org/abs/2511.12791)
*Dahao Tang,Nan Yang,Yanli Li,Zhiyu Zhu,Zhibo Jin,Dong Yuan*

Main category: cs.LG

TL;DR: 本文提出联邦时间序列预测中自适应回溯期选择的框架，引入合成数据生成器，推导预测损失分解，证明最小化总损失的回溯期条件，为该场景下的自适应期选择提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习场景下，时间序列预测中选择合适的回溯期是一个挑战，现有方法主要适用于集中式和独立分布设置，需要适用于联邦场景的方法。

Method: 提出合成数据生成器（SDG）捕捉客户数据的时间结构和异质性；定义将时间序列窗口映射到内在表示空间的变换；推导预测损失分解为贝叶斯项和近似项。

Result: 分析表明增加回溯期虽提高确定性模式的可识别性，但会因模型复杂度和样本效率问题增加近似误差；证明总预测损失在不可约损失开始饱和、近似损失继续上升的最小回溯期处最小化。

Conclusion: 为联邦学习中的时间序列预测自适应期选择提供了严格的理论基础。

Abstract: Selecting an appropriate look-back horizon remains a fundamental challenge in time series forecasting (TSF), particularly in the federated learning scenarios where data is decentralized, heterogeneous, and often non-independent. While recent work has explored horizon selection by preserving forecasting-relevant information in an intrinsic space, these approaches are primarily restricted to centralized and independently distributed settings. This paper presents a principled framework for adaptive horizon selection in federated time series forecasting through an intrinsic space formulation. We introduce a synthetic data generator (SDG) that captures essential temporal structures in client data, including autoregressive dependencies, seasonality, and trend, while incorporating client-specific heterogeneity. Building on this model, we define a transformation that maps time series windows into an intrinsic representation space with well-defined geometric and statistical properties. We then derive a decomposition of the forecasting loss into a Bayesian term, which reflects irreducible uncertainty, and an approximation term, which accounts for finite-sample effects and limited model capacity. Our analysis shows that while increasing the look-back horizon improves the identifiability of deterministic patterns, it also increases approximation error due to higher model complexity and reduced sample efficiency. We prove that the total forecasting loss is minimized at the smallest horizon where the irreducible loss starts to saturate, while the approximation loss continues to rise. This work provides a rigorous theoretical foundation for adaptive horizon selection for time series forecasting in federated learning.

</details>


### [370] [Genomic Next-Token Predictors are In-Context Learners](https://arxiv.org/abs/2511.12797)
*Nathan Breslow,Aayush Mishra,Mahler Revsine,Michael C. Schatz,Anqi Liu,Daniel Khashabi*

Main category: cs.LG

TL;DR: 探究上下文学习（ICL）能否在基因组序列中自然出现，发现基因组模型有类似语言模型的ICL现象，支持ICL源于大规模预测建模的假设。


<details>
  <summary>Details</summary>
Motivation: 研究ICL是否能在除人类语言外的其他序列领域通过大规模预测训练自然出现。

Method: 以基因组序列为研究对象，研究Evo2基因组模型，开发包含语言和基因组形式的符号推理任务的实验框架，对比基因组和语言模型的ICL。

Result: 基因组模型和语言模型一样，随着上下文示例数量增加，模式归纳呈对数线性增长。

Conclusion: 这是基因组序列中自然出现ICL的首个证据，支持ICL源于对丰富数据的大规模预测建模，拓展了元学习，指向上下文学习的统一、模态无关观点。

Abstract: In-context learning (ICL) -- the capacity of a model to infer and apply abstract patterns from examples provided within its input -- has been extensively studied in large language models trained for next-token prediction on human text. In fact, prior work often attributes this emergent behavior to distinctive statistical properties in human language. This raises a fundamental question: can ICL arise organically in other sequence domains purely through large-scale predictive training?
  To explore this, we turn to genomic sequences, an alternative symbolic domain rich in statistical structure. Specifically, we study the Evo2 genomic model, trained predominantly on next-nucleotide (A/T/C/G) prediction, at a scale comparable to mid-sized LLMs. We develop a controlled experimental framework comprising symbolic reasoning tasks instantiated in both linguistic and genomic forms, enabling direct comparison of ICL across genomic and linguistic models. Our results show that genomic models, like their linguistic counterparts, exhibit log-linear gains in pattern induction as the number of in-context demonstrations increases. To the best of our knowledge, this is the first evidence of organically emergent ICL in genomic sequences, supporting the hypothesis that ICL arises as a consequence of large-scale predictive modeling over rich data. These findings extend emergent meta-learning beyond language, pointing toward a unified, modality-agnostic view of in-context learning.

</details>


### [371] [The Alignment Game: A Theory of Long-Horizon Alignment Through Recursive Curation](https://arxiv.org/abs/2511.12804)
*Ali Falahati,Mohammad Mohammadi Amiri,Kate Larson,Lukasz Golab*

Main category: cs.LG

TL;DR: 为分析自消费生成模型递归再训练对对齐的长期影响提供形式化基础，揭示三种收敛机制及一个不可能定理。


<details>
  <summary>Details</summary>
Motivation: 自消费生成模型中与用户偏好对齐是递归过程，需分析递归再训练对对齐的长期影响。

Method: 基于Bradley - Terry模型的两阶段管理机制，将对齐建模为模型所有者和公共用户两派的交互。

Result: 揭示三种结构收敛机制，证明一个不可能定理。

Conclusion: 对齐不是静态目标，而是受权力不对称和路径依赖影响的动态均衡。

Abstract: In self-consuming generative models that train on their own outputs, alignment with user preferences becomes a recursive rather than one-time process. We provide the first formal foundation for analyzing the long-term effects of such recursive retraining on alignment. Under a two-stage curation mechanism based on the Bradley-Terry (BT) model, we model alignment as an interaction between two factions: the Model Owner, who filters which outputs should be learned by the model, and the Public User, who determines which outputs are ultimately shared and retained through interactions with the model. Our analysis reveals three structural convergence regimes depending on the degree of preference alignment: consensus collapse, compromise on shared optima, and asymmetric refinement. We prove a fundamental impossibility theorem: no recursive BT-based curation mechanism can simultaneously preserve diversity, ensure symmetric influence, and eliminate dependence on initialization. Framing the process as dynamic social choice, we show that alignment is not a static goal but an evolving equilibrium, shaped both by power asymmetries and path dependence.

</details>


### [372] [Expressive Temporal Specifications for Reward Monitoring](https://arxiv.org/abs/2511.12808)
*Omar Adalat,Francesco Belardinelli*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Specifying informative and dense reward functions remains a pivotal challenge in Reinforcement Learning, as it directly affects the efficiency of agent training. In this work, we harness the expressive power of quantitative Linear Temporal Logic on finite traces (($\text{LTL}_f[\mathcal{F}]$)) to synthesize reward monitors that generate a dense stream of rewards for runtime-observable state trajectories. By providing nuanced feedback during training, these monitors guide agents toward optimal behaviour and help mitigate the well-known issue of sparse rewards under long-horizon decision making, which arises under the Boolean semantics dominating the current literature. Our framework is algorithm-agnostic and only relies on a state labelling function, and naturally accommodates specifying non-Markovian properties. Empirical results show that our quantitative monitors consistently subsume and, depending on the environment, outperform Boolean monitors in maximizing a quantitative measure of task completion and in reducing convergence time.

</details>


### [373] [Assessing Automated Fact-Checking for Medical LLM Responses with Knowledge Graphs](https://arxiv.org/abs/2511.12817)
*Shasha Zhou,Mingyu Huang,Jack Cole,Charles Britton,Ming Yin,Jan Wolber,Ke Li*

Main category: cs.LG

TL;DR: 研究用医学知识图谱对大语言模型生成回复进行自动事实性评估，引入FAITH框架，实验表明该评估与临床医生判断相关性高，能区分不同能力模型，且有解释性，认为利用知识图谱是医疗自动事实性评估的重要方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在医疗领域应用需严格验证和确认，以了解潜在危害，故研究用医学知识图谱对其生成回复进行自动事实性评估的可靠性和可行性。

Method: 引入FAITH框架，将回复分解为原子声明，链接到医学知识图谱，并根据证据路径评分，且不依赖参考答案。

Result: 在不同医疗任务上的实验结合人类主观评估显示，基于知识图谱的评估与临床医生判断有更高相关性，能有效区分不同能力的大语言模型，对文本差异有鲁棒性，评分具有可解释性。

Conclusion: 虽然存在局限性，但利用知识图谱是医疗自动事实性评估的突出方向。

Abstract: The recent proliferation of large language models (LLMs) holds the potential to revolutionize healthcare, with strong capabilities in diverse medical tasks. Yet, deploying LLMs in high-stakes healthcare settings requires rigorous verification and validation to understand any potential harm. This paper investigates the reliability and viability of using medical knowledge graphs (KGs) for the automated factuality evaluation of LLM-generated responses. To ground this investigation, we introduce FAITH, a framework designed to systematically probe the strengths and limitations of this KG-based approach. FAITH operates without reference answers by decomposing responses into atomic claims, linking them to a medical KG, and scoring them based on evidence paths. Experiments on diverse medical tasks with human subjective evaluations demonstrate that KG-grounded evaluation achieves considerably higher correlations with clinician judgments and can effectively distinguish LLMs with varying capabilities. It is also robust to textual variances. The inherent explainability of its scoring can further help users understand and mitigate the limitations of current LLMs. We conclude that while limitations exist, leveraging KGs is a prominent direction for automated factuality assessment in healthcare.

</details>


### [374] [Catastrophic Forgetting in Kolmogorov-Arnold Networks](https://arxiv.org/abs/2511.12828)
*Mohammad Marufur Rahman,Guanchu Wang,Kaixiong Zhou,Minghan Chen,Fan Yang*

Main category: cs.LG

TL;DR: 研究KANs在持续学习中的灾难性遗忘问题，提出理论框架，引入KAN - LoRA，发现KANs在低维表现好，高维易遗忘。


<details>
  <summary>Details</summary>
Motivation: 持续学习中灾难性遗忘是难题，KANs虽被认为有抗遗忘能力，但实际表现和局限性不明。

Method: 提出理论框架将遗忘与激活支持重叠和内在数据维度联系，在合成和视觉任务实验，引入KAN - LoRA并评估其在知识编辑任务的效果。

Result: KANs在低维算法设置中保留效果好，但在高维领域如图像分类和语言建模易遗忘。

Conclusion: 研究增进对KANs优缺点的理解，为持续学习系统设计提供实用见解。

Abstract: Catastrophic forgetting is a longstanding challenge in continual learning, where models lose knowledge from earlier tasks when learning new ones. While various mitigation strategies have been proposed for Multi-Layer Perceptrons (MLPs), recent architectural advances like Kolmogorov-Arnold Networks (KANs) have been suggested to offer intrinsic resistance to forgetting by leveraging localized spline-based activations. However, the practical behavior of KANs under continual learning remains unclear, and their limitations are not well understood. To address this, we present a comprehensive study of catastrophic forgetting in KANs and develop a theoretical framework that links forgetting to activation support overlap and intrinsic data dimension. We validate these analyses through systematic experiments on synthetic and vision tasks, measuring forgetting dynamics under varying model configurations and data complexity. Further, we introduce KAN-LoRA, a novel adapter design for parameter-efficient continual fine-tuning of language models, and evaluate its effectiveness in knowledge editing tasks. Our findings reveal that while KANs exhibit promising retention in low-dimensional algorithmic settings, they remain vulnerable to forgetting in high-dimensional domains such as image classification and language modeling. These results advance the understanding of KANs' strengths and limitations, offering practical insights for continual learning system design.

</details>


### [375] [An Evaluation of Representation Learning Methods in Particle Physics Foundation Models](https://arxiv.org/abs/2511.12829)
*Michael Chen,Raghav Kansal,Abhijith Gandrakota,Zichun Hao,Jennifer Ngadiuba,Maria Spiropulu*

Main category: cs.LG

TL;DR: 对粒子物理表征学习目标进行系统评估，对比多种目标并引入改进获SOTA，为基础模型发展提供参考。


<details>
  <summary>Details</summary>
Motivation: 在统一框架下对粒子物理表征学习目标进行系统评估，为该领域基础模型发展提供参考。

Method: 采用基于变压器的粒子云编码器，使用标准化预处理、匹配采样和一致评估协议，对比多种学习目标，引入有针对性的监督架构修改。

Result: 引入的监督架构修改在基准评估中达到了SOTA性能。

Conclusion: 该研究为粒子物理基础模型未来发展提供参考，有助于推动该领域更透明和稳健的进展。

Abstract: We present a systematic evaluation of representation learning objectives for particle physics within a unified framework. Our study employs a shared transformer-based particle-cloud encoder with standardized preprocessing, matched sampling, and a consistent evaluation protocol on a jet classification dataset. We compare contrastive (supervised and self-supervised), masked particle modeling, and generative reconstruction objectives under a common training regimen. In addition, we introduce targeted supervised architectural modifications that achieve state-of-the-art performance on benchmark evaluations. This controlled comparison isolates the contributions of the learning objective, highlights their respective strengths and limitations, and provides reproducible baselines. We position this work as a reference point for the future development of foundation models in particle physics, enabling more transparent and robust progress across the community.

</details>


### [376] [Connectivity-Guided Sparsification of 2-FWL GNNs: Preserving Full Expressivity with Improved Efficiency](https://arxiv.org/abs/2511.12838)
*Rongqin Chen,Fan Mo,Pak Lon Ip,Shenghui Zhang,Dan Wu,Ye Li,Leong Hou U*

Main category: cs.LG

TL;DR: 提出Co - Sparsify框架，在保留2 - FWL全部表达能力的同时消除冗余计算，理论证明其表达能力，实验表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有HOGNNs计算成本高，现有效率方法会降低表达能力，需一种兼顾表达能力和效率的方法。

Method: 提出Co - Sparsify框架，将2节点消息传递限制在连通分量，3节点交互限制在双连通分量，去除冗余计算。

Result: 理论证明Co - Sparsified GNNs与2 - FWL测试表达能力相同，实验中在合成子结构计数任务上匹配或超越准确率，在真实世界基准测试中达最优性能。

Conclusion: 高表达能力和可扩展性并非相互排斥，基于拓扑引导的稀疏化能实现有理论保证的强大高效GNNs。

Abstract: Higher-order Graph Neural Networks (HOGNNs) based on the 2-FWL test achieve superior expressivity by modeling 2- and 3-node interactions, but at $\mathcal{O}(n^3)$ computational cost. However, this computational burden is typically mitigated by existing efficiency methods at the cost of reduced expressivity. We propose \textbf{Co-Sparsify}, a connectivity-aware sparsification framework that eliminates \emph{provably redundant} computations while preserving full 2-FWL expressive power. Our key insight is that 3-node interactions are expressively necessary only within \emph{biconnected components} -- maximal subgraphs where every pair of nodes lies on a cycle. Outside these components, structural relationships can be fully captured via 2-node message passing or global readout, rendering higher-order modeling unnecessary. Co-Sparsify restricts 2-node message passing to connected components and 3-node interactions to biconnected ones, removing computation without approximation or sampling. We prove that Co-Sparsified GNNs are as expressive as the 2-FWL test. Empirically, on PPGN, Co-Sparsify matches or exceeds accuracy on synthetic substructure counting tasks and achieves state-of-the-art performance on real-world benchmarks (ZINC, QM9). This study demonstrates that high expressivity and scalability are not mutually exclusive: principled, topology-guided sparsification enables powerful, efficient GNNs with theoretical guarantees.

</details>


### [377] [RoS-Guard: Robust and Scalable Online Change Detection with Delay-Optimal Guarantees](https://arxiv.org/abs/2511.12846)
*Zelin Zhu,Yancheng Huang,Kai Yang*

Main category: cs.LG

TL;DR: 提出适用于含不确定性线性系统的鲁棒最优在线变化检测算法RoS - Guard，经实验验证有效且在大规模系统中有计算加速效果。


<details>
  <summary>Details</summary>
Motivation: 现有在线变化检测方法假设系统知识精确不现实，且在大规模系统中效率低。

Method: 对在线变化检测优化问题进行紧松弛和重新表述，采用神经展开结合GPU加速实现高效并行计算。

Result: 算法有性能的理论保证，大量实验验证了其有效性，在大规模系统场景有显著计算加速。

Conclusion: RoS - Guard能有效解决现有在线变化检测方法的问题。

Abstract: Online change detection (OCD) aims to rapidly identify change points in streaming data and is critical in applications such as power system monitoring, wireless network sensing, and financial anomaly detection. Existing OCD methods typically assume precise system knowledge, which is unrealistic due to estimation errors and environmental variations. Moreover, existing OCD methods often struggle with efficiency in large-scale systems. To overcome these challenges, we propose RoS-Guard, a robust and optimal OCD algorithm tailored for linear systems with uncertainty. Through a tight relaxation and reformulation of the OCD optimization problem, RoS-Guard employs neural unrolling to enable efficient parallel computation via GPU acceleration. The algorithm provides theoretical guarantees on performance, including expected false alarm rate and worst-case average detection delay. Extensive experiments validate the effectiveness of RoS-Guard and demonstrate significant computational speedup in large-scale system scenarios.

</details>


### [378] [From Black-Box to White-Box: Control-Theoretic Neural Network Interpretability](https://arxiv.org/abs/2511.12852)
*Jihoon Moon*

Main category: cs.LG

TL;DR: 提出控制理论框架分析神经网络内部计算，以提高可解释性，通过实例展示激活饱和对网络特性的影响。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络难以进行机械解释的问题。

Method: 将训练好的神经网络视为非线性状态空间系统，利用局部线性化、可控性和可观测性格拉姆矩阵以及汉克尔奇异值进行分析。

Result: 通过简单前馈网络示例，展示激活饱和会降低可控性、缩小主导汉克尔奇异值、改变主导内部模式。

Conclusion: 该方法将神经网络转化为局部白盒动态模型集合，指出可用于剪枝或添加约束以提高可解释性的内部方向。

Abstract: Deep neural networks achieve state of the art performance but remain difficult to interpret mechanistically. In this work, we propose a control theoretic framework that treats a trained neural network as a nonlinear state space system and uses local linearization, controllability and observability Gramians, and Hankel singular values to analyze its internal computation. For a given input, we linearize the network around the corresponding hidden activation pattern and construct a state space model whose state consists of hidden neuron activations. The input state and state output Jacobians define local controllability and observability Gramians, from which we compute Hankel singular values and associated modes. These quantities provide a principled notion of neuron and pathway importance: controllability measures how easily each neuron can be excited by input perturbations, observability measures how strongly each neuron influences the output, and Hankel singular values rank internal modes that carry input output energy. We illustrate the framework on simple feedforward networks, including a 1 2 2 1 SwiGLU network and a 2 3 3 2 GELU network. By comparing different operating points, we show how activation saturation reduces controllability, shrinks the dominant Hankel singular value, and shifts the dominant internal mode to a different subset of neurons. The proposed method turns a neural network into a collection of local white box dynamical models and suggests which internal directions are natural candidates for pruning or constraints to improve interpretability.

</details>


### [379] [An approach of deep reinforcement learning for maximizing the net present value of stochastic projects](https://arxiv.org/abs/2511.12865)
*Wei Xu,Fan Yang,Qinyuan Cui,Zhi Chen*

Main category: cs.LG

TL;DR: 本文研究离散场景下随机活动时长和现金流的项目优化问题，提出DDQN方法，实验表明该方法优于传统策略，有更好性能。


<details>
  <summary>Details</summary>
Motivation: 在离散场景下，使项目通过加速流入和延迟流出最大化预期净现值。

Method: 将问题建模为离散时间马尔可夫决策过程（MDP），并提出双深度Q网络（DDQN）方法。

Result: 比较实验显示DDQN在大规模或高不确定性环境中优于传统策略；消融研究表明双网络架构减少动作值高估，目标网络提高训练收敛性和鲁棒性。

Conclusion: DDQN在复杂项目优化中能实现更高预期净现值，为稳定有效策略实施提供可靠框架。

Abstract: This paper investigates a project with stochastic activity durations and cash flows under discrete scenarios, where activities must satisfy precedence constraints generating cash inflows and outflows. The objective is to maximize expected net present value (NPV) by accelerating inflows and deferring outflows. We formulate the problem as a discrete-time Markov Decision Process (MDP) and propose a Double Deep Q-Network (DDQN) approach. Comparative experiments demonstrate that DDQN outperforms traditional rigid and dynamic strategies, particularly in large-scale or highly uncertain environments, exhibiting superior computational capability, policy reliability, and adaptability. Ablation studies further reveal that the dual-network architecture mitigates overestimation of action values, while the target network substantially improves training convergence and robustness. These results indicate that DDQN not only achieves higher expected NPV in complex project optimization but also provides a reliable framework for stable and effective policy implementation.

</details>


### [380] [Method of Manufactured Learning for Solver-free Training of Neural Operators](https://arxiv.org/abs/2511.12890)
*Arth Sojitra,Omer San*

Main category: cs.LG

TL;DR: 提出无求解器框架MML训练神经算子，在多个基准测试中表现良好，提供了构建物理相关神经算子的可扩展途径。


<details>
  <summary>Details</summary>
Motivation: 训练神经算子依赖求解器数据，限制可扩展性和对物理系统的探索。

Method: 引入MML框架，用函数合成取代数值数据生成，可与任何算子学习范式集成，以傅里叶神经算子为例。

Result: 在多个基准测试中，MML实现高光谱精度、低残差误差，对未见条件有强泛化能力。

Conclusion: MML通过将数据生成重构为解析合成过程，提供了不依赖昂贵数值模拟或实验数据的可扩展、无求解器的途径。

Abstract: Training neural operators to approximate mappings between infinite-dimensional function spaces often requires extensive datasets generated by either demanding experimental setups or computationally expensive numerical solvers. This dependence on solver-based data limits scalability and constrains exploration across physical systems. Here we introduce the Method of Manufactured Learning (MML), a solver-independent framework for training neural operators using analytically constructed, physics-consistent datasets. Inspired by the classical method of manufactured solutions, MML replaces numerical data generation with functional synthesis, i.e., smooth candidate solutions are sampled from controlled analytical spaces, and the corresponding forcing fields are derived by direct application of the governing differential operators. During inference, setting these forcing terms to zero restores the original governing equations, allowing the trained neural operator to emulate the true solution operator of the system. The framework is agnostic to network architecture and can be integrated with any operator learning paradigm. In this paper, we employ Fourier neural operator as a representative example. Across canonical benchmarks including heat, advection, Burgers, and diffusion-reaction equations. MML achieves high spectral accuracy, low residual errors, and strong generalization to unseen conditions. By reframing data generation as a process of analytical synthesis, MML offers a scalable, solver-agnostic pathway toward constructing physically grounded neural operators that retain fidelity to governing laws without reliance on expensive numerical simulations or costly experimental data for training.

</details>


### [381] [Functional Mean Flow in Hilbert Space](https://arxiv.org/abs/2511.12898)
*Zhiqi Li,Yuchen Sun,Greg Turk,Bo Zhu*

Main category: cs.LG

TL;DR: 提出在无限维希尔伯特空间定义的一步生成模型FMF，拓展了均值流框架到函数域，引入改进稳定性的变体，适用于多种函数数据生成任务。


<details>
  <summary>Details</summary>
Motivation: 将一步均值流框架拓展到函数域，解决函数数据生成问题。

Method: 给出函数流匹配的理论公式和高效训练与采样的实际实现，引入$x_1$-预测变体。

Result: 得到适用于时间序列、图像、PDE和3D几何等多种函数数据生成任务的实用一步流匹配方法。

Conclusion: FMF是有效的一步生成模型，可用于广泛的函数数据生成场景。

Abstract: We present Functional Mean Flow (FMF) as a one-step generative model defined in infinite-dimensional Hilbert space. FMF extends the one-step Mean Flow framework to functional domains by providing a theoretical formulation for Functional Flow Matching and a practical implementation for efficient training and sampling. We also introduce an $x_1$-prediction variant that improves stability over the original $u$-prediction form. The resulting framework is a practical one-step Flow Matching method applicable to a wide range of functional data generation tasks such as time series, images, PDEs, and 3D geometry.

</details>


### [382] [Contrastive Entropy Bounds for Density and Conditional Density Decomposition](https://arxiv.org/abs/2511.12903)
*Bo Hu,Jose C. Principe*

Main category: cs.LG

TL;DR: 本文从贝叶斯高斯视角研究神经网络特征可解释性，用希尔伯特空间和分解处理多输出网络，发现高斯算子迹可训练自编码器，核范数可训练MDNs，还提出编码器 - 混合 - 解码器架构。


<details>
  <summary>Details</summary>
Motivation: 从贝叶斯高斯视角研究神经网络特征的可解释性。

Method: 运用希尔伯特空间和分解处理多输出网络；利用内积和范数定义边界和成本；提出编码器 - 混合 - 解码器架构。

Result: 发现自编码器目标等价于最大化高斯算子的迹；可使用高斯算子的核范数作为散度训练MDNs；提出的架构在假设数据为小方差高斯混合时可定量跟踪和分析上界。

Conclusion: 高斯算子的迹和核范数可分别用于训练自编码器和MDNs，提出的架构有助于在特定数据假设下定量分析边界。

Abstract: This paper studies the interpretability of neural network features from a Bayesian Gaussian view, where optimizing a cost is reaching a probabilistic bound; learning a model approximates a density that makes the bound tight and the cost optimal, often with a Gaussian mixture density. The two examples are Mixture Density Networks (MDNs) using the bound for the marginal and autoencoders using the conditional bound. It is a known result, not only for autoencoders, that minimizing the error between inputs and outputs maximizes the dependence between inputs and the middle.
  We use Hilbert space and decomposition to address cases where a multiple-output network produces multiple centers defining a Gaussian mixture. Our first finding is that an autoencoder's objective is equivalent to maximizing the trace of a Gaussian operator, the sum of eigenvalues under bases orthonormal w.r.t. the data and model distributions. This suggests that, when a one-to-one correspondence as needed in autoencoders is unnecessary, we can instead maximize the nuclear norm of this operator, the sum of singular values, to maximize overall rank rather than trace. Thus the trace of a Gaussian operator can be used to train autoencoders, and its nuclear norm can be used as divergence to train MDNs.
  Our second test uses inner products and norms in a Hilbert space to define bounds and costs. Such bounds often have an extra norm compared to KL-based bounds, which increases sample diversity and prevents the trivial solution where a multiple-output network produces the same constant, at the cost of requiring a sample batch to estimate and optimize. We propose an encoder-mixture-decoder architecture whose decoder is multiple-output, producing multiple centers per sample, potentially tightening the bound. Assuming the data are small-variance Gaussian mixtures, this upper bound can be tracked and analyzed quantitatively.

</details>


### [383] [LinkedIn Profile Characteristics and Professional Success Indicators](https://arxiv.org/abs/2511.12905)
*Tania-Amanda Fredrick Eneye,Ashlesha Malla,Pawan Paudel*

Main category: cs.LG

TL;DR: 研究探索领英档案特征与职业成功关系，用机器学习建模，发现晋升易预测，粉丝增长更复杂，为优化领英形象和职业策略提供见解。


<details>
  <summary>Details</summary>
Motivation: 探究领英档案特征与职业成功（晋升、粉丝数、职业发展率）的关系。

Method: 利用超6.2万份匿名领英档案数据集，运用机器学习技术开发预测模型。

Result: 晋升较易预测，粉丝增长更复杂。

Conclusion: 研究为专业人士优化领英形象和职业策略提供可操作见解。

Abstract: This study explores the relationship between LinkedIn profile characteristics and professional success, focusing on the indicators of promotions, follower count, and career progression rate. By leveraging a dataset of over 62,000 anonymized LinkedIn profiles, we developed predictive models using machine learning techniques to identify the most influential factors driving professional success. Results indicate that while promotions are highly predictable, follower growth exhibits greater complexity. This research provides actionable insights for professionals seeking to optimize their LinkedIn presence and career strategies.

</details>


### [384] [APT: Affine Prototype-Timestamp For Time Series Forecasting Under Distribution Shift](https://arxiv.org/abs/2511.12945)
*Yujie Li,Zezhi Shao,Chengqing Yu,Yisong Fu,Tao Sun,Yongjun Xu,Fei Wang*

Main category: cs.LG

TL;DR: 现有深度学习模型在分布偏移下的时间序列预测有局限，本文提出轻量级插件模块APT，实验表明其显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于局部统计归一化的深度学习模型难以捕捉全局分布偏移，RevIN及其变体存在处理缺失值、噪声观测和无效通道仿射变换的问题。

Method: 提出Affine Prototype Timestamp (APT) 模块，利用时间戳条件原型学习动态生成仿射参数，调制输入和输出序列，使骨干网络从自监督、分布感知的聚类实例中学习。

Result: 在六个基准数据集和多种骨干 - 归一化组合上的大量实验表明，APT显著提高了分布偏移下的预测性能。

Conclusion: APT是一个轻量灵活的插件模块，兼容任意预测骨干和归一化策略，能有效提升分布偏移下的时间序列预测性能。

Abstract: Time series forecasting under distribution shift remains challenging, as existing deep learning models often rely on local statistical normalization (e.g., mean and variance) that fails to capture global distribution shift. Methods like RevIN and its variants attempt to decouple distribution and pattern but still struggle with missing values, noisy observations, and invalid channel-wise affine transformation. To address these limitations, we propose Affine Prototype Timestamp (APT), a lightweight and flexible plug-in module that injects global distribution features into the normalization-forecasting pipeline. By leveraging timestamp conditioned prototype learning, APT dynamically generates affine parameters that modulate both input and output series, enabling the backbone to learn from self-supervised, distribution-aware clustered instances. APT is compatible with arbitrary forecasting backbones and normalization strategies while introducing minimal computational overhead. Extensive experiments across six benchmark datasets and multiple backbone-normalization combinations demonstrate that APT significantly improves forecasting performance under distribution shift.

</details>


### [385] [A FEDformer-Based Hybrid Framework for Anomaly Detection and Risk Forecasting in Financial Time Series](https://arxiv.org/abs/2511.12951)
*Ziling Fan,Ruijia Liang,Yiwen Hu*

Main category: cs.LG

TL;DR: 论文提出基于FEDformer的混合框架用于金融时间序列异常检测和风险预测，实验表明该模型优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 金融市场波动大，传统深度学习模型难以捕捉金融数据长期依赖和复杂周期模式，需准确异常检测和早期风险预测。

Method: 提出基于FEDformer的混合框架，集成FEDformer、基于残差的异常检测器和风险预测头。

Result: 在多个数据集实验中，该模型比基准方法RMSE降低15.7%，异常检测F1分数提高11.5%。

Conclusion: 模型能有效捕捉金融波动，可用于市场崩溃预测和风险管理的可靠预警系统。

Abstract: Financial markets are inherently volatile and prone to sudden disruptions such as market crashes, flash collapses, and liquidity crises. Accurate anomaly detection and early risk forecasting in financial time series are therefore crucial for preventing systemic instability and supporting informed investment decisions. Traditional deep learning models, such as LSTM and GRU, often fail to capture long-term dependencies and complex periodic patterns in highly nonstationary financial data. To address this limitation, this study proposes a FEDformer-Based Hybrid Framework for Anomaly Detection and Risk Forecasting in Financial Time Series, which integrates the Frequency Enhanced Decomposed Transformer (FEDformer) with a residual-based anomaly detector and a risk forecasting head. The FEDformer module models temporal dynamics in both time and frequency domains, decomposing signals into trend and seasonal components for improved interpretability. The residual-based detector identifies abnormal fluctuations by analyzing prediction errors, while the risk head predicts potential financial distress using learned latent embeddings. Experiments conducted on the S&P 500, NASDAQ Composite, and Brent Crude Oil datasets (2000-2024) demonstrate the superiority of the proposed model over benchmark methods, achieving a 15.7 percent reduction in RMSE and an 11.5 percent improvement in F1-score for anomaly detection. These results confirm the effectiveness of the model in capturing financial volatility, enabling reliable early-warning systems for market crash prediction and risk management.

</details>


### [386] [Global Cross-Time Attention Fusion for Enhanced Solar Flare Prediction from Multivariate Time Series](https://arxiv.org/abs/2511.12955)
*Onur Vural,Shah Muhammad Hamdi,Soukaina Filali Boubrahimi*

Main category: cs.LG

TL;DR: 为解决太阳耀斑预测中数据不平衡问题，提出GCTAF架构，在基准数据集上验证其能有效检测强烈耀斑并提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 太阳耀斑发生数据不平衡影响有效学习，需提升太阳耀斑预测性能。

Method: 提出基于Transformer的Global Cross - Time Attention Fusion (GCTAF) 架构，注入可学习的交叉注意力全局令牌来增强长程时间建模。

Result: 在基准太阳能耀斑数据集上，GCTAF有效检测出强烈耀斑，提升了预测性能。

Conclusion: 改进基于Transformer的架构是太阳耀斑预测任务的高潜力替代方案。

Abstract: Multivariate time series classification is increasingly investigated in space weather research as a means to predict intense solar flare events, which can cause widespread disruptions across modern technological systems. Magnetic field measurements of solar active regions are converted into structured multivariate time series, enabling predictive modeling across segmented observation windows. However, the inherently imbalanced nature of solar flare occurrences, where intense flares are rare compared to minor flare events, presents a significant barrier to effective learning. To address this challenge, we propose a novel Global Cross-Time Attention Fusion (GCTAF) architecture, a transformer-based model to enhance long-range temporal modeling. Unlike traditional self-attention mechanisms that rely solely on local interactions within time series, GCTAF injects a set of learnable cross-attentive global tokens that summarize salient temporal patterns across the entire sequence. These tokens are refined through cross-attention with the input sequence and fused back into the temporal representation, enabling the model to identify globally significant, non-contiguous time points that are critical for flare prediction. This mechanism functions as a dynamic attention-driven temporal summarizer that augments the model's capacity to capture discriminative flare-related dynamics. We evaluate our approach on the benchmark solar flare dataset and show that GCTAF effectively detects intense flares and improves predictive performance, demonstrating that refining transformer-based architectures presents a high-potential alternative for solar flare prediction tasks.

</details>


### [387] [Angular Gradient Sign Method: Uncovering Vulnerabilities in Hyperbolic Networks](https://arxiv.org/abs/2511.12985)
*Minsoo Jo,Dongyoon Yang,Taesup Kim*

Main category: cs.LG

TL;DR: 本文提出针对双曲空间的新型对抗攻击方法，聚焦语义敏感方向生成对抗样本，实验表明效果优于传统攻击方法，强调几何感知策略重要性。


<details>
  <summary>Details</summary>
Motivation: 双曲网络发展要求重新评估非欧几何中的攻击策略，现有方法未考虑双曲结构，可能导致攻击低效或几何不一致。

Method: 在双曲空间的切空间计算损失函数梯度，将其分解为径向和角度分量，仅从角度方向施加扰动。

Result: 在图像分类、跨模态检索任务和网络架构上的实验表明，该攻击方法比传统对抗攻击有更高的欺骗率，能产生高影响的扰动并深入洞察双曲嵌入的漏洞。

Conclusion: 强调了在弯曲表示空间中几何感知对抗策略的重要性，为攻击层次嵌入提供了原则性框架。

Abstract: Adversarial examples in neural networks have been extensively studied in Euclidean geometry, but recent advances in \textit{hyperbolic networks} call for a reevaluation of attack strategies in non-Euclidean geometries. Existing methods such as FGSM and PGD apply perturbations without regard to the underlying hyperbolic structure, potentially leading to inefficient or geometrically inconsistent attacks. In this work, we propose a novel adversarial attack that explicitly leverages the geometric properties of hyperbolic space. Specifically, we compute the gradient of the loss function in the tangent space of hyperbolic space, decompose it into a radial (depth) component and an angular (semantic) component, and apply perturbation derived solely from the angular direction. Our method generates adversarial examples by focusing perturbations in semantically sensitive directions encoded in angular movement within the hyperbolic geometry. Empirical results on image classification, cross-modal retrieval tasks and network architectures demonstrate that our attack achieves higher fooling rates than conventional adversarial attacks, while producing high-impact perturbations with deeper insights into vulnerabilities of hyperbolic embeddings. This work highlights the importance of geometry-aware adversarial strategies in curved representation spaces and provides a principled framework for attacking hierarchical embeddings.

</details>


### [388] [Learning Branching Policies for MILPs with Proximal Policy Optimization](https://arxiv.org/abs/2511.12986)
*Abdelouahed Ben Mhamed,Assia Kamal-Idrissi,Amal El Fallah Seghrouchni*

Main category: cs.LG

TL;DR: 提出Tree - Gate Proximal Policy Optimization (TGPPO)框架，用强化学习训练分支策略，在减少节点探索和提高p - Primal - Dual Integrals方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统分支定界法求解大规模混合整数线性规划（MILP）时间复杂度高，现有基于模仿学习（IL）的方法泛化能力差。

Method: 提出TGPPO框架，使用近端策略优化（PPO）这一强化学习算法训练分支策略，基于参数化状态空间表示动态捕捉搜索树的上下文。

Result: TGPPO在减少探索节点数和提高p - Primal - Dual Integrals方面常优于现有基于学习的策略，尤其在分布外实例中。

Conclusion: 强化学习在为MILP求解器开发鲁棒、适应性强的分支策略方面具有潜力。

Abstract: Branch-and-Bound (B\&B) is the dominant exact solution method for Mixed Integer Linear Programs (MILP), yet its exponential time complexity poses significant challenges for large-scale instances. The growing capabilities of machine learning have spurred efforts to improve B\&B by learning data-driven branching policies. However, most existing approaches rely on Imitation Learning (IL), which tends to overfit to expert demonstrations and struggles to generalize to structurally diverse or unseen instances. In this work, we propose Tree-Gate Proximal Policy Optimization (TGPPO), a novel framework that employs Proximal Policy Optimization (PPO), a Reinforcement Learning (RL) algorithm, to train a branching policy aimed at improving generalization across heterogeneous MILP instances. Our approach builds on a parameterized state space representation that dynamically captures the evolving context of the search tree. Empirical evaluations show that TGPPO often outperforms existing learning-based policies in terms of reducing the number of nodes explored and improving p-Primal-Dual Integrals (PDI), particularly in out-of-distribution instances. These results highlight the potential of RL to develop robust and adaptable branching strategies for MILP solvers.

</details>


### [389] [Are Graph Transformers Necessary? Efficient Long-Range Message Passing with Fractal Nodes in MPNNs](https://arxiv.org/abs/2511.13010)
*Jeongwhan Choi,Seungjun Park,Sumin Park,Sung-Bae Cho,Noseong Park*

Main category: cs.LG

TL;DR: 提出分形节点概念，改进MPNN处理长距离依赖能力，提升性能与表达能力。


<details>
  <summary>Details</summary>
Motivation: GNN难平衡局部与全局信息，图Transformer忽略MPNN局部性与效率，需改进MPNN处理长距离依赖的能力。

Method: 受现实网络分形结构启发，提出分形节点与原节点共存，自适应聚合子图特征表示。

Result: 方法缓解过压缩问题，提升MPNN表达能力，性能与图Transformer相当或更好，且保持计算效率。

Conclusion: 分形节点能有效改进MPNN的长距离依赖，是处理图数据的有效方法。

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for learning on graph-structured data, but often struggle to balance local and global information. While graph Transformers aim to address this by enabling long-range interactions, they often overlook the inherent locality and efficiency of Message Passing Neural Networks (MPNNs). We propose a new concept called fractal nodes, inspired by the fractal structure observed in real-world networks. Our approach is based on the intuition that graph partitioning naturally induces fractal structure, where subgraphs often reflect the connectivity patterns of the full graph. Fractal nodes are designed to coexist with the original nodes and adaptively aggregate subgraph-level feature representations, thereby enforcing feature similarity within each subgraph. We show that fractal nodes alleviate the over-squashing problem by providing direct shortcut connections that enable long-range propagation of subgraph-level representations. Experiment results show that our method improves the expressive power of MPNNs and achieves comparable or better performance to graph Transformers while maintaining the computational efficiency of MPNN by improving the long-range dependencies of MPNN.

</details>


### [390] [The Good, The Bad, and The Hybrid: A Reward Structure Showdown in Reasoning Models Training](https://arxiv.org/abs/2511.13016)
*Subramanyam Sahoo*

Main category: cs.LG

TL;DR: 提出统一框架研究大语言模型数学推理任务微调的奖励结构，引入自适应混合奖励调度器，混合奖励结构效果更好。


<details>
  <summary>Details</summary>
Motivation: 奖励设计对强化学习和对齐研究至关重要，需研究大语言模型数学推理任务的奖励结构。

Method: 提出统一框架，用Qwen3 - 4B和LoRA在GSM8K数据集上微调，评估多种奖励公式，引入自适应混合奖励调度器。

Result: 混合奖励结构比纯硬或连续奖励方法提高了收敛速度和训练稳定性。

Conclusion: 混合奖励结构为通过自适应奖励建模进行对齐提供了见解。

Abstract: Reward design is central to reinforcement learning from human feedback (RLHF) and alignment research. In this work, we propose a unified framework to study hard, continuous, and hybrid reward structures for fine-tuning large language models (LLMs) on mathematical reasoning tasks. Using Qwen3-4B with LoRA fine-tuning on the GSM8K dataset, we formalize and empirically evaluate reward formulations that incorporate correctness, perplexity, reasoning quality, and consistency. We introduce an adaptive hybrid reward scheduler that transitions between discrete and continuous signals, balancing exploration and stability. Our results show that hybrid reward structures improve convergence speed and training stability over purely hard or continuous approaches, offering insights for alignment via adaptive reward modeling.

</details>


### [391] [The Final-Stage Bottleneck: A Systematic Dissection of the R-Learner for Network Causal Inference](https://arxiv.org/abs/2511.13018)
*Sairam S,Sara Girdhar,Shivam Soni*

Main category: cs.LG

TL;DR: 对图上的R - Learner框架进行大规模实证研究，发现最终阶段CATE估计器的归纳偏差是性能的主要驱动因素，存在“表示瓶颈”和“干扰瓶颈”，提出的端到端图R - Learner表现出色。


<details>
  <summary>Details</summary>
Motivation: R - Learner应用于网络数据时，其核心假设面临挑战，需要对图上的R - Learner框架进行系统剖析。

Method: 进行大规模实证研究，对不同设置下的R - Learner进行实验，开展“Hub - Periphery Trade - off”分析。

Result: 证明最终阶段归纳偏差对性能影响大，有“表示瓶颈”（图盲最终阶段R - Learner失败），提出的端到端图R - Learner优于非DML GNN T - Learner基线，发现“干扰瓶颈”。

Conclusion: 研究结果在多种基准上得到验证，发布代码以促进后续对“最终阶段瓶颈”的研究。

Abstract: The R-Learner is a powerful, theoretically-grounded framework for estimating heterogeneous treatment effects, prized for its robustness to nuisance model errors. However, its application to network data, where causal heterogeneity is often graph-dependent, presents a critical challenge to its core assumption of a well-specified final-stage model. In this paper, we conduct a large-scale empirical study to systematically dissect the R-Learner framework on graphs. We provide the first rigorous evidence that the primary driver of performance is the inductive bias of the final-stage CATE estimator, an effect that dominates the choice of nuisance models. Our central finding is the quantification of a catastrophic "representation bottleneck": we prove with overwhelming statistical significance (p < 0.001) that R-Learners with a graph-blind final stage fail completely (MSE > 4.0), even when paired with powerful GNN nuisance models. Conversely, our proposed end-to-end Graph R-Learner succeeds and significantly outperforms a strong, non-DML GNN T-Learner baseline. Furthermore, we identify and provide a mechanistic explanation for a subtle, topology-dependent "nuisance bottleneck," linking it to GNN over-squashing via a targeted "Hub-Periphery Trade-off" analysis. Our findings are validated across diverse synthetic and semi-synthetic benchmarks. We release our code as a reproducible benchmark to facilitate future research on this critical "final-stage bottleneck."

</details>


### [392] [Learning Time-Scale Invariant Population-Level Neural Representations](https://arxiv.org/abs/2511.13022)
*Eshani Patel,Yisong Yue,Geeling Chau*

Main category: cs.LG

TL;DR: 本文指出通用神经时间序列基础模型关键在于群体级表征学习，但现有模型对时间尺度预处理不匹配敏感。提出TSAP方法提升模型对不同时间尺度的鲁棒性，强调处理预处理多样性对构建通用模型的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有群体级表征学习模型对预训练和下游设置中的时间尺度预处理不匹配敏感，缺乏时间尺度不变性，影响模型泛化能力。

Method: 引入时间尺度增强预训练（TSAP）方法。

Result: TSAP方法在解码任务中持续提高了对不同时间尺度的鲁棒性，并在表征空间中构建了不变性。

Conclusion: 处理预处理多样性是构建通用神经基础模型的关键步骤。

Abstract: General-purpose foundation models for neural time series can help accelerate neuroscientific discoveries and enable applications such as brain computer interfaces (BCIs). A key component in scaling these models is population-level representation learning, which leverages information across channels to capture spatial as well as temporal structure. Population-level approaches have recently shown that such representations can be both efficient to learn on top of pretrained temporal encoders and produce useful representations for decoding a variety of downstream tasks. However, these models remain sensitive to mismatches in preprocessing, particularly on time-scales, between pretraining and downstream settings. We systematically examine how time-scale mismatches affects generalization and find that existing representations lack invariance. To address this, we introduce Time-scale Augmented Pretraining (TSAP), which consistently improves robustness to different time-scales across decoding tasks and builds invariance in the representation space. These results highlight handling preprocessing diversity as a key step toward building generalizable neural foundation models.

</details>


### [393] [SLMQuant:Benchmarking Small Language Model Quantization for Practical Deployment](https://arxiv.org/abs/2511.13023)
*Jiacheng Wang,Yejun Zeng,Jinyang Guo,Yuqing Ma,Aishan Liu,Xianglong Liu*

Main category: cs.LG

TL;DR: 本文提出SLMQuant基准评估SLM压缩技术，发现SLM与LLM量化差异，提出设计原则，助力边缘设备高效部署SLM。


<details>
  <summary>Details</summary>
Motivation: SLM作为LLM的高效替代方案，在边缘设备部署因模型压缩效率问题面临挑战，且量化技术对SLM的适用性研究不足。

Method: 引入SLMQuant基准，对不同架构和任务进行多轨道评估，分析现有量化方法在SLM上的表现。

Result: 揭示SLM和LLM在量化敏感性上的根本差异，直接移植LLM优化技术效果不佳。

Conclusion: 确定有效SLM量化的关键因素，提出针对性设计原则，为边缘应用中高效部署SLM奠定基础。

Abstract: Despite the growing interest in Small Language Models (SLMs) as resource-efficient alternatives to Large Language Models (LLMs), their deployment on edge devices remains challenging due to unresolved efficiency gaps in model compression. While quantization has proven effective for LLMs, its applicability to SLMs is significantly underexplored, with critical questions about differing quantization bottlenecks and efficiency profiles. This paper introduces SLMQuant, the first systematic benchmark for evaluating LLM compression techniques when applied to SLMs. Through comprehensive multi-track evaluations across diverse architectures and tasks, we analyze how state-of-the-art quantization methods perform on SLMs. Our findings reveal fundamental disparities between SLMs and LLMs in quantization sensitivity, demonstrating that direct transfer of LLM-optimized techniques leads to suboptimal results due to SLMs' unique architectural characteristics and training dynamics. We identify key factors governing effective SLM quantization and propose actionable design principles for SLM-tailored compression. SLMQuant establishes a foundational framework for advancing efficient SLM deployment on low-end devices in edge applications, and provides critical insights for deploying lightweight language models in resource-constrained scenarios.

</details>


### [394] [One-Step Generative Policies with Q-Learning: A Reformulation of MeanFlow](https://arxiv.org/abs/2511.13035)
*Zeyuan Wang,Da Li,Yulin Chen,Ye Shi,Liang Bai,Tianyuan Yu,Yanwei Fu*

Main category: cs.LG

TL;DR: 提出用于离线强化学习的一步生成策略，通过MeanFlow残差重构使噪声直接映射到动作，在多基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有一步高斯策略难以捕捉复杂多模态动作分布，基于流的方法训练依赖蒸馏和两阶段训练，需改进。

Method: 重构MeanFlow，将速度场和噪声到动作转换集成到单个策略网络，探索多种重构变体并确定有效残差公式。

Result: 在OGBench和D4RL基准的73个任务上实验，在离线和离线到在线强化学习设置中均表现良好。

Conclusion: 提出的方法具有一步噪声到动作生成高效、能对多模态动作分布进行表达性建模、单阶段训练稳定高效等优势。

Abstract: We introduce a one-step generative policy for offline reinforcement learning that maps noise directly to actions via a residual reformulation of MeanFlow, making it compatible with Q-learning. While one-step Gaussian policies enable fast inference, they struggle to capture complex, multimodal action distributions. Existing flow-based methods improve expressivity but typically rely on distillation and two-stage training when trained with Q-learning. To overcome these limitations, we propose to reformulate MeanFlow to enable direct noise-to-action generation by integrating the velocity field and noise-to-action transformation into a single policy network-eliminating the need for separate velocity estimation. We explore several reformulation variants and identify an effective residual formulation that supports expressive and stable policy learning. Our method offers three key advantages: 1) efficient one-step noise-to-action generation, 2) expressive modelling of multimodal action distributions, and 3) efficient and stable policy learning via Q-learning in a single-stage training setup. Extensive experiments on 73 tasks across the OGBench and D4RL benchmarks demonstrate that our method achieves strong performance in both offline and offline-to-online reinforcement learning settings. Code is available at https://github.com/HiccupRL/MeanFlowQL.

</details>


### [395] [Bi-View Embedding Fusion: A Hybrid Learning Approach for Knowledge Graph's Nodes Classification Addressing Problems with Limited Data](https://arxiv.org/abs/2511.13044)
*Rosario Napoli,Giovanni Lonia,Antonio Celesti,Massimo Villari,Maria Fazio*

Main category: cs.LG

TL;DR: 研究提出Bi - View混合方法，结合Node2Vec和GraphSAGE生成增强图嵌入，提升图机器学习模型性能，尤其在初始特征差的场景。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习需大量数据，图机器学习处理知识图谱有局限，研究旨在不依赖额外合成数据提升图机器学习模型。

Method: 结合Node2Vec和GraphSAGE，先计算Node2Vec嵌入表示图拓扑，用基于中心性的指标丰富节点特征作为GraphSAGE输入，融合层结合两者生成双视角嵌入空间。

Result: 能捕获图的拓扑和语义属性，挖掘数据集中未明确表示的信息。

Conclusion: 该方法提升下游任务性能，为更准确的知识图谱增强图机器学习模型奠定基础。

Abstract: Traditional Machine Learning (ML) methods require large amounts of data to perform well, limiting their applicability in sparse or incomplete scenarios and forcing the usage of additional synthetic data to improve the model training. To overcome this challenge, the research community is looking more and more at Graph Machine Learning (GML) as it offers a powerful alternative by using relationships within data. However, this method also faces limitations, particularly when dealing with Knowledge Graphs (KGs), which can hide huge information due to their semantic nature. This study introduces Bi-View, a novel hybrid approach that increases the informative content of node features in KGs to generate enhanced Graph Embeddings (GEs) that are used to improve GML models without relying on additional synthetic data. The proposed work combines two complementary GE techniques: Node2Vec, which captures structural patterns through unsupervised random walks, and GraphSAGE, which aggregates neighbourhood information in a supervised way. Node2Vec embeddings are first computed to represent the graph topology, and node features are then enriched with centrality-based metrics, which are used as input for the GraphSAGE model. Moreover, a fusion layer combines the original Node2Vec embeddings with the GraphSAGE-influenced representations, resulting in a dual-perspective embedding space. Such a fusion captures both topological and semantic properties of the graph, enabling the model to exploit informative features that may exist in the dataset but that are not explicitly represented. Our approach improves downstream task performance, especially in scenarios with poor initial features, giving the basis for more accurate and precise KG-enanched GML models.

</details>


### [396] [Learning from the Undesirable: Robust Adaptation of Language Models without Forgetting](https://arxiv.org/abs/2511.13052)
*Yunhun Nam,Jaehyung Kim,Jongheon Jeong*

Main category: cs.LG

TL;DR: 提出Learning - from - the - Undesirable (LfU)正则化方案，解决有限数据微调语言模型过拟合问题，实验表明其有效提升适应性并保留预训练知识。


<details>
  <summary>Details</summary>
Motivation: 传统监督微调（SFT）在有限数据下会使语言模型过拟合，依赖虚假模式或损害其他通用能力。

Method: 提出LfU正则化方案，通过一致性正则化使模型内部表示与不良更新后的表示对齐，利用不良更新进行表示级数据增强。

Result: 在多种下游任务实验中，LfU有效增强适应性并保留预训练知识，如在数学任务上平均提升16.8%，对提示变化的鲁棒性提高，输出性能标准差降低92.1%。

Conclusion: LfU是一种有效的先验方法，能在有限数据下促进泛化，提升语言模型微调效果。

Abstract: Language models (LMs) are often adapted through supervised fine-tuning (SFT) to specialize their capabilities for downstream tasks. However, in typical scenarios where the fine-tuning data is limited, e.g., compared to pre-training, SFT can lead LMs to overfit, causing them to rely on spurious patterns within the target task or to compromise other broadly useful capabilities as a side effect of narrow specialization. In this paper, we propose Learning-from-the-Undesirable (LfU), a simple yet effective regularization scheme for SFT to mitigate overfitting issues when fine-tuning LMs with limited data. Specifically, we aim to regularize the fine-tuning process to favor solutions that are resilient to "undesirable" model updates, e.g., gradient ascent steps that steer the model toward undesirable behaviors. To this end, we propose a novel form of consistency regularization that directly aligns internal representations of the model with those after an undesirable update. By leveraging representation-level data augmentation through undesirable updates, LfU effectively promotes generalization under limited data. Our experiments on diverse LM downstream tasks show that LfU serves as an effective prior that enhances adaptability while preserving pretrained knowledge. For example, our LM from LfU achieves a 16.8% average improvement on math tasks compared to vanilla SFT on the same dataset, where the latter even leads to degraded performance on those tasks. Furthermore, LfU exhibits improved robustness to prompt variations, e.g., yielding a 92.1% lower standard deviation in output performances compared to SFT, highlighting its versatile effects.

</details>


### [397] [Latency and Ordering Effects in Online Decisions](https://arxiv.org/abs/2511.13060)
*Duo Yi*

Main category: cs.LG

TL;DR: 本文以Bregman散度为损失基准，证明超额基准损失有结构化下界，将不等式扩展到近正则和弱凸设置，给出通过随机实验和流诊断估计和监测各项的方法。


<details>
  <summary>Details</summary>
Motivation: 在线决策系统在延迟反馈和顺序敏感动态下运行，需要研究相关损失情况。

Method: 以Bregman散度为损失基准进行证明，将不等式扩展到不同设置，通过2×2随机实验和流诊断估计和监测相关项。

Result: 证明超额基准损失有结构化下界，扩展不等式到不同设置，给出估计和监测方法。

Conclusion: 该框架将多种效应整合为可解释的下界陈述，可在现实系统中进行压力测试和调整。

Abstract: Online decision systems routinely operate under delayed feedback and order-sensitive (noncommutative) dynamics: actions affect which observations arrive, and in what sequence. Taking a Bregman divergence $D_Φ$ as the loss benchmark, we prove that the excess benchmark loss admits a structured lower bound $L \ge L_{\mathrm{ideal}} + g_1(λ) + g_2(\varepsilon_\star) + g_{12}(λ,\varepsilon_\star) - D_{\mathrm{ncx}}$, where $g_1$ and $g_2$ are calibrated penalties for latency and order-sensitivity, $g_{12}$ captures their geometric interaction, and $D_{\mathrm{ncx}}\ge 0$ is a nonconvexity/approximation penalty that vanishes under convex Legendre assumptions. We extend this inequality to prox-regular and weakly convex settings, obtaining robust guarantees beyond the convex case. We also give an operational recipe for estimating and monitoring the four terms via simple $2\times 2$ randomized experiments and streaming diagnostics (effective sample size, clipping rate, interaction heatmaps). The framework packages heterogeneous latency, noncommutativity, and implementation-gap effects into a single interpretable lower-bound statement that can be stress-tested and tuned in real-world systems.

</details>


### [398] [Self-Adaptive Graph Mixture of Models](https://arxiv.org/abs/2511.13062)
*Mohit Meena,Yash Punjabi,Abhishek A,Vishal Sharma,Mahesh Chandran*

Main category: cs.LG

TL;DR: 当前GNN性能提升遇瓶颈且选模型难，提出SAGMM框架自动选模型并组合，在多任务数据集上表现优。


<details>
  <summary>Details</summary>
Motivation: 解决当前GNN性能提升遇瓶颈，且难选合适模型用于特定图任务或数据集的问题。

Method: 提出SAGMM框架，利用架构多样性和拓扑感知注意力门控机制为节点分配专家，含剪枝机制提高效率，还探索训练高效变体。

Result: 在16个基准数据集的多种任务上，SAGMM始终优于或匹配领先的GNN基线和先前基于混合的方法。

Conclusion: SAGMM为现实世界的图学习提供了一个强大且自适应的解决方案。

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for learning over graph-structured data, yet recent studies have shown that their performance gains are beginning to plateau. In many cases, well-established models such as GCN and GAT, when appropriately tuned, can match or even exceed the performance of more complex, state-of-the-art architectures. This trend highlights a key limitation in the current landscape: the difficulty of selecting the most suitable model for a given graph task or dataset. To address this, we propose Self-Adaptive Graph Mixture of Models (SAGMM), a modular and practical framework that learns to automatically select and combine the most appropriate GNN models from a diverse pool of architectures. Unlike prior mixture-of-experts approaches that rely on variations of a single base model, SAGMM leverages architectural diversity and a topology-aware attention gating mechanism to adaptively assign experts to each node based on the structure of the input graph. To improve efficiency, SAGMM includes a pruning mechanism that reduces the number of active experts during training and inference without compromising performance. We also explore a training-efficient variant in which expert models are pretrained and frozen, and only the gating and task-specific layers are trained. We evaluate SAGMM on 16 benchmark datasets covering node classification, graph classification, regression, and link prediction tasks, and demonstrate that it consistently outperforms or matches leading GNN baselines and prior mixture-based methods, offering a robust and adaptive solution for real-world graph learning.

</details>


### [399] [A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning](https://arxiv.org/abs/2511.13078)
*Liuyi Jin,Pasan Gunawardena,Amran Haroon,Runzhi Wang,Sangwoo Lee,Radu Stoleru,Michael Middleton,Zepeng Huo,Jeeeun Kim,Jason Moats*

Main category: cs.LG

TL;DR: 提出基于EMSNet和EMSServe的智能眼镜系统EMSGlass，可提升急救人员实时态势感知和决策效率。


<details>
  <summary>Details</summary>
Motivation: 急救人员在高压力环境下做关键决策，需要技术辅助提升效率和准确性。

Method: 构建EMSNet多模态多任务模型和EMSServe低延迟多模态服务框架，EMSServe有模态感知模型分割器和特征缓存机制。

Result: EMSNet在多任务上精度超现有单模态基线，EMSServe比直接PyTorch多模态推理快1.9 - 11.7倍，用户研究表明EMSGlass提升急救人员各方面能力。

Conclusion: EMSGlass能增强急救人员能力，用户研究为下一代AI急救系统提供方向。

Abstract: Emergency Medical Technicians (EMTs) operate in high-pressure environments, making rapid, life-critical decisions under heavy cognitive and operational loads. We present EMSGlass, a smart-glasses system powered by EMSNet, the first multimodal multitask model for Emergency Medical Services (EMS), and EMSServe, a low-latency multimodal serving framework tailored to EMS scenarios. EMSNet integrates text, vital signs, and scene images to construct a unified real-time understanding of EMS incidents. Trained on real-world multimodal EMS datasets, EMSNet simultaneously supports up to five critical EMS tasks with superior accuracy compared to state-of-the-art unimodal baselines. Built on top of PyTorch, EMSServe introduces a modality-aware model splitter and a feature caching mechanism, achieving adaptive and efficient inference across heterogeneous hardware while addressing the challenge of asynchronous modality arrival in the field. By optimizing multimodal inference execution in EMS scenarios, EMSServe achieves 1.9x -- 11.7x speedup over direct PyTorch multimodal inference. A user study evaluation with six professional EMTs demonstrates that EMSGlass enhances real-time situational awareness, decision-making speed, and operational efficiency through intuitive on-glass interaction. In addition, qualitative insights from the user study provide actionable directions for extending EMSGlass toward next-generation AI-enabled EMS systems, bridging multimodal intelligence with real-world emergency response workflows.

</details>


### [400] [Real-time prediction of breast cancer sites using deformation-aware graph neural network](https://arxiv.org/abs/2511.13082)
*Kyunghyun Lee,Yong-Min Shin,Minwoo Shin,Jihun Kim,Sunghwan Lim,Won-Yong Shin,Kyungho Yoon*

Main category: cs.LG

TL;DR: 研究开发基于图神经网络（GNN）的模型，能在活检过程中实时准确预测变形的乳腺癌部位，经验证精度高、计算快，有望用于乳腺癌诊断。


<details>
  <summary>Details</summary>
Motivation: 直接磁共振成像引导活检有局限性，间接活检在创建准确实时可变形乳腺模型方面存在挑战，需解决实时准确预测肿瘤位移的问题。

Method: 开发结合磁共振图像乳腺和肿瘤结构信息的个体特异性有限元（FE）模型模拟变形行为，用GNN模型处理表面位移和基于距离的图数据预测组织位移。

Result: 模型经体模和真实患者数据集验证，癌症节点位移均方根误差（RMSE）在0.2毫米内，空间重叠的骰子相似系数（DSC）为0.977，能实时推理，计算成本比传统FE模拟快超4000倍。

Conclusion: 提出的变形感知GNN模型能高精度实时预测乳腺活检中肿瘤位移，与临床程序结合可提高乳腺癌诊断的精度和效率。

Abstract: Early diagnosis of breast cancer is crucial, enabling the establishment of appropriate treatment plans and markedly enhancing patient prognosis. While direct magnetic resonance imaging-guided biopsy demonstrates promising performance in detecting cancer lesions, its practical application is limited by prolonged procedure times and high costs. To overcome these issues, an indirect MRI-guided biopsy that allows the procedure to be performed outside of the MRI room has been proposed, but it still faces challenges in creating an accurate real-time deformable breast model. In our study, we tackled this issue by developing a graph neural network (GNN)-based model capable of accurately predicting deformed breast cancer sites in real time during biopsy procedures. An individual-specific finite element (FE) model was developed by incorporating magnetic resonance (MR) image-derived structural information of the breast and tumor to simulate deformation behaviors. A GNN model was then employed, designed to process surface displacement and distance-based graph data, enabling accurate prediction of overall tissue displacement, including the deformation of the tumor region. The model was validated using phantom and real patient datasets, achieving an accuracy within 0.2 millimeters (mm) for cancer node displacement (RMSE) and a dice similarity coefficient (DSC) of 0.977 for spatial overlap with actual cancerous regions. Additionally, the model enabled real-time inference and achieved a speed-up of over 4,000 times in computational cost compared to conventional FE simulations. The proposed deformation-aware GNN model offers a promising solution for real-time tumor displacement prediction in breast biopsy, with high accuracy and real-time capability. Its integration with clinical procedures could significantly enhance the precision and efficiency of breast cancer diagnosis.

</details>


### [401] [Transformer-Based Scalable Multi-Agent Reinforcement Learning for Networked Systems with Long-Range Interactions](https://arxiv.org/abs/2511.13103)
*Vidur Sinha,Muhammed Ustaomeroglu,Guannan Qu*

Main category: cs.LG

TL;DR: 现有多智能体强化学习用于大规模网络控制有局限，提出STACCA框架解决问题，评估显示其性能、泛化性和可扩展性好。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体强化学习方法在捕捉长程依赖和跨网络拓扑泛化性方面存在局限，需改进。

Method: 引入STACCA框架，用集中式图Transformer Critic建模长程依赖，共享图Transformer Actor学习可泛化策略，集成反事实优势估计器。

Result: 在疫情防控和谣言传播网络控制任务中，STACCA表现出更好的性能、网络泛化性和可扩展性。

Conclusion: 基于Transformer的多智能体强化学习架构有潜力实现大规模网络系统的可扩展和可泛化控制。

Abstract: Multi-agent reinforcement learning (MARL) has shown promise for large-scale network control, yet existing methods face two major limitations. First, they typically rely on assumptions leading to decay properties of local agent interactions, limiting their ability to capture long-range dependencies such as cascading power failures or epidemic outbreaks. Second, most approaches lack generalizability across network topologies, requiring retraining when applied to new graphs. We introduce STACCA (Shared Transformer Actor-Critic with Counterfactual Advantage), a unified transformer-based MARL framework that addresses both challenges. STACCA employs a centralized Graph Transformer Critic to model long-range dependencies and provide system-level feedback, while its shared Graph Transformer Actor learns a generalizable policy capable of adapting across diverse network structures. Further, to improve credit assignment during training, STACCA integrates a novel counterfactual advantage estimator that is compatible with state-value critic estimates. We evaluate STACCA on epidemic containment and rumor-spreading network control tasks, demonstrating improved performance, network generalization, and scalability. These results highlight the potential of transformer-based MARL architectures to achieve scalable and generalizable control in large-scale networked systems.

</details>


### [402] [Synthetic Forgetting without Access: A Few-shot Zero-glance Framework for Machine Unlearning](https://arxiv.org/abs/2511.13116)
*Qipeng Song,Nan Yang,Ziqi Xu,Yue Li,Wei Shao,Feng Xia*

Main category: cs.LG

TL;DR: 提出GFOES框架解决few - shot zero - glance场景下的机器学习遗忘问题，实验证明其有效且能保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习遗忘方法大多假设能完全访问原始训练数据集，不切实际，本文解决few - shot zero - glance这一现实且具挑战性的场景问题。

Method: 引入GFOES框架，包含生成反馈网络（GFN）和两阶段微调过程，GFN合成最优擦除样本，两阶段微调先激进遗忘后恢复性能。

Result: 在三个图像分类数据集上实验表明，GFOES在对数和表示层面实现有效遗忘，仅用5%原始数据就能保持良好性能。

Conclusion: 该框架为数据受限条件下的隐私保护机器学习提供了实用且可扩展的解决方案。

Abstract: Machine unlearning aims to eliminate the influence of specific data from trained models to ensure privacy compliance. However, most existing methods assume full access to the original training dataset, which is often impractical. We address a more realistic yet challenging setting: few-shot zero-glance, where only a small subset of the retained data is available and the forget set is entirely inaccessible. We introduce GFOES, a novel framework comprising a Generative Feedback Network (GFN) and a two-phase fine-tuning procedure. GFN synthesises Optimal Erasure Samples (OES), which induce high loss on target classes, enabling the model to forget class-specific knowledge without access to the original forget data, while preserving performance on retained classes. The two-phase fine-tuning procedure enables aggressive forgetting in the first phase, followed by utility restoration in the second. Experiments on three image classification datasets demonstrate that GFOES achieves effective forgetting at both logit and representation levels, while maintaining strong performance using only 5% of the original data. Our framework offers a practical and scalable solution for privacy-preserving machine learning under data-constrained conditions.

</details>


### [403] [Departures: Distributional Transport for Single-Cell Perturbation Prediction with Neural Schrödinger Bridges](https://arxiv.org/abs/2511.13124)
*Changxi Chi,Yufei Huang,Jun Xia,Jiangbin Zheng,Yunfan Liu,Zelin Zang,Stan Z. Li*

Main category: cs.LG

TL;DR: 本文提出新方法近似Schrödinger Bridge，对单细胞扰动数据分布直接对齐，联合训练可准确建模扰动并捕捉细胞异质性，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 单细胞扰动结果预测对生物医学研究重要，但单细胞数据非配对性是瓶颈，现有神经生成传输模型存在局限性。

Method: 近似Schrödinger Bridge，利用Minibatch - OT配对避免双向推理和逆向过程不适定性，近似两个SB模型并联合训练。

Result: 模型有效捕捉异质单细胞响应，在公共数据集上达到了最先进的性能。

Conclusion: 所提方法可准确建模单细胞扰动并捕捉细胞异质性，在单细胞扰动结果预测上表现良好。

Abstract: Predicting single-cell perturbation outcomes directly advances gene function analysis and facilitates drug candidate selection, making it a key driver of both basic and translational biomedical research. However, a major bottleneck in this task is the unpaired nature of single-cell data, as the same cell cannot be observed both before and after perturbation due to the destructive nature of sequencing. Although some neural generative transport models attempt to tackle unpaired single-cell perturbation data, they either lack explicit conditioning or depend on prior spaces for indirect distribution alignment, limiting precise perturbation modeling. In this work, we approximate Schrödinger Bridge (SB), which defines stochastic dynamic mappings recovering the entropy-regularized optimal transport (OT), to directly align the distributions of control and perturbed single-cell populations across different perturbation conditions. Unlike prior SB approximations that rely on bidirectional modeling to infer optimal source-target sample coupling, we leverage Minibatch-OT based pairing to avoid such bidirectional inference and the associated ill-posedness of defining the reverse process. This pairing directly guides bridge learning, yielding a scalable approximation to the SB. We approximate two SB models, one modeling discrete gene activation states and the other continuous expression distributions. Joint training enables accurate perturbation modeling and captures single-cell heterogeneity. Experiments on public genetic and drug perturbation datasets show that our model effectively captures heterogeneous single-cell responses and achieves state-of-the-art performance.

</details>


### [404] [Soft Conflict-Resolution Decision Transformer for Offline Multi-Task Reinforcement Learning](https://arxiv.org/abs/2511.13133)
*Shudong Wang,Xinfei Wang,Chenhao Zhang,Shanchen Pang,Haiyuan Gui,Wenhao Ji,Xiaojian Liao*

Main category: cs.LG

TL;DR: 提出SoCo - DT方法解决多任务强化学习梯度冲突问题，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于掩码的多任务强化学习方法存在粗粒度二进制掩码过度抑制关键冲突参数、固定稀疏策略不适用不同任务冲突水平的问题，影响模型泛化和学习效率。

Method: 提出基于参数重要性的软冲突解决方法SoCo - DT，利用Fisher信息动态调整掩码值；引入基于四分位距的动态稀疏调整策略，结合非对称余弦退火调度更新阈值。

Result: 在Meta - World基准测试中，SoCo - DT在MT50上比现有最优方法高7.6%，在次优数据集上高10.5%。

Conclusion: SoCo - DT能有效缓解梯度冲突，提升多任务整体性能。

Abstract: Multi-task reinforcement learning (MTRL) seeks to learn a unified policy for diverse tasks, but often suffers from gradient conflicts across tasks. Existing masking-based methods attempt to mitigate such conflicts by assigning task-specific parameter masks. However, our empirical study shows that coarse-grained binary masks have the problem of over-suppressing key conflicting parameters, hindering knowledge sharing across tasks. Moreover, different tasks exhibit varying conflict levels, yet existing methods use a one-size-fits-all fixed sparsity strategy to keep training stability and performance, which proves inadequate. These limitations hinder the model's generalization and learning efficiency.
  To address these issues, we propose SoCo-DT, a Soft Conflict-resolution method based by parameter importance. By leveraging Fisher information, mask values are dynamically adjusted to retain important parameters while suppressing conflicting ones. In addition, we introduce a dynamic sparsity adjustment strategy based on the Interquartile Range (IQR), which constructs task-specific thresholding schemes using the distribution of conflict and harmony scores during training. To enable adaptive sparsity evolution throughout training, we further incorporate an asymmetric cosine annealing schedule to continuously update the threshold. Experimental results on the Meta-World benchmark show that SoCo-DT outperforms the state-of-the-art method by 7.6% on MT50 and by 10.5% on the suboptimal dataset, demonstrating its effectiveness in mitigating gradient conflicts and improving overall multi-task performance.

</details>


### [405] [Personalized Federated Learning with Bidirectional Communication Compression via One-Bit Random Sketching](https://arxiv.org/abs/2511.13144)
*Jiacheng Cheng,Xu Zhang,Guanghui Qiu,Yifang Zhang,Yinchuan Li,Kaiyuan Feng*

Main category: cs.LG

TL;DR: 提出pFed1BS个性化联邦学习框架，用一位随机草图实现通信压缩，减少通信成本且性能有竞争力。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中双向通信开销和客户端数据异质性问题。

Method: 提出pFed1BS框架，客户端传输一位草图，服务器聚合广播全局一位共识；引入基于符号的正则化器；采用快速哈达玛变换减轻计算负担。

Result: 理论分析保证算法收敛到全局势函数的平稳邻域，数值模拟显示该框架大幅降低通信成本，性能有竞争力。

Conclusion: pFed1BS能有效解决联邦学习通信成本问题，在数据异质性场景表现良好。

Abstract: Federated Learning (FL) enables collaborative training across decentralized data, but faces key challenges of bidirectional communication overhead and client-side data heterogeneity. To address communication costs while embracing data heterogeneity, we propose pFed1BS, a novel personalized federated learning framework that achieves extreme communication compression through one-bit random sketching. In personalized FL, the goal shifts from training a single global model to creating tailored models for each client. In our framework, clients transmit highly compressed one-bit sketches, and the server aggregates and broadcasts a global one-bit consensus. To enable effective personalization, we introduce a sign-based regularizer that guides local models to align with the global consensus while preserving local data characteristics. To mitigate the computational burden of random sketching, we employ the Fast Hadamard Transform for efficient projection. Theoretical analysis guarantees that our algorithm converges to a stationary neighborhood of the global potential function. Numerical simulations demonstrate that pFed1BS substantially reduces communication costs while achieving competitive performance compared to advanced communication-efficient FL algorithms.

</details>


### [406] [OTARo: Once Tuning for All Precisions toward Robust On-Device LLMs](https://arxiv.org/abs/2511.13147)
*Shaoyuan Chen,Zhixuan Chen,Dawei Yang,Zhihang Yuan,Qiang Wu*

Main category: cs.LG

TL;DR: 提出OTARo方法使设备端大语言模型一次微调后灵活切换量化精度并保持鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统量化在微调与部署阶段缺乏灵活性，无法支持设备端在复杂场景下切换精度，需新方法解决。

Method: 引入SEFP量化机制，通过简单尾数截断产生不同位宽；采用BPS迭代更新搜索路径，LAA在低位宽下异步梯度累积和延迟更新。

Result: 在流行大语言模型实验中，OTARo对所有精度都实现了稳定且强大的鲁棒性能。

Conclusion: OTARo方法有效，可让设备端大语言模型灵活切换量化精度并保证性能。

Abstract: Large Language Models (LLMs) fine-tuning techniques not only improve the adaptability to diverse downstream tasks, but also mitigate adverse effects of model quantization. Despite this, conventional quantization suffers from its structural limitation that hinders flexibility during the fine-tuning and deployment stages. Practical on-device tasks demand different quantization precisions (i.e. different bit-widths), e.g., understanding tasks tend to exhibit higher tolerance to reduced precision compared to generation tasks. Conventional quantization, typically relying on scaling factors that are incompatible across bit-widths, fails to support the on-device switching of precisions when confronted with complex real-world scenarios. To overcome the dilemma, we propose OTARo, a novel method that enables on-device LLMs to flexibly switch quantization precisions while maintaining performance robustness through once fine-tuning. OTARo introduces Shared Exponent Floating Point (SEFP), a distinct quantization mechanism, to produce different bit-widths through simple mantissa truncations of a single model. Moreover, to achieve bit-width robustness in downstream applications, OTARo performs a learning process toward losses induced by different bit-widths. The method involves two critical strategies: (1) Exploitation-Exploration Bit-Width Path Search (BPS), which iteratively updates the search path via a designed scoring mechanism; (2) Low-Precision Asynchronous Accumulation (LAA), which performs asynchronous gradient accumulations and delayed updates under low bit-widths. Experiments on popular LLMs, e.g., LLaMA3.2-1B, LLaMA3-8B, demonstrate that OTARo achieves consistently strong and robust performance for all precisions.

</details>


### [407] [Warm-starting active-set solvers using graph neural networks](https://arxiv.org/abs/2511.13174)
*Ella J. Schmidtobreick,Daniel Arnström,Paul Häusner,Jens Sjölund*

Main category: cs.LG

TL;DR: 提出用图神经网络预测双活动集求解器 DAQP 中的活动集，可减少求解器迭代次数，在不同问题规模上有良好表现且能有效泛化。


<details>
  <summary>Details</summary>
Motivation: 二次规划求解器计算成本高，限制了其在时间关键场景的应用。

Method: 采用图神经网络学习优化方法，将二次规划问题表示为二分图，学习识别最优活动集以热启动求解器。

Result: 与冷启动相比，图神经网络持续减少求解器迭代次数，性能与多层感知机基线相当，且训练的图神经网络能有效泛化到未见维度。

Conclusion: 结构感知学习有潜力加速实时应用（如模型预测控制）中的优化。

Abstract: Quadratic programming (QP) solvers are widely used in real-time control and optimization, but their computational cost often limits applicability in time-critical settings. We propose a learning-to-optimize approach using graph neural networks (GNNs) to predict active sets in the dual active-set solver DAQP. The method exploits the structural properties of QPs by representing them as bipartite graphs and learning to identify the optimal active set for efficiently warm-starting the solver. Across varying problem sizes, the GNN consistently reduces the number of solver iterations compared to cold-starting, while performance is comparable to a multilayer perceptron (MLP) baseline. Furthermore, a GNN trained on varying problem sizes generalizes effectively to unseen dimensions, demonstrating flexibility and scalability. These results highlight the potential of structure-aware learning to accelerate optimization in real-time applications such as model predictive control.

</details>


### [408] [Real-time distortion prediction in metallic additive manufacturing via a physics-informed neural operator approach](https://arxiv.org/abs/2511.13178)
*Mingxuan Tian,Haochen Mu,Donghong Ding,Mengjiao Li,Yuhan Ding,Jianping Zhao*

Main category: cs.LG

TL;DR: 本文提出PINO模型预测金属增材制造中未来15s的z和y方向变形，经实验验证该模型准确、高效，有实时长时物理场预测潜力。


<details>
  <summary>Details</summary>
Motivation: 数字孪生和智能制造系统发展下，需实时变形场预测控制金属增材制造缺陷，现有数值模拟和传统机器学习方法有局限。

Method: 提出Physics - informed Neural Operator (PINO)，采用Physics - informed Deep Operator Network - Recurrent Neural Network (PIDeepONet - RNN) 处理温度历史和编码变形场，结合热传导方程作为软约束。

Result: 模型训练和测试使用有限元方法生成的数据集，实现高精度、低误差累积和时间效率，z和y方向最大绝对误差低，误差在熔池高、沉积和关键区域梯度范数低。

Conclusion: PINO替代模型在控制缺陷的实时长时物理场预测方面有潜力。

Abstract: With the development of digital twins and smart manufacturing systems, there is an urgent need for real-time distortion field prediction to control defects in metal Additive Manufacturing (AM). However, numerical simulation methods suffer from high computational cost, long run-times that prevent real-time use, while conventional Machine learning (ML) models struggle to extract spatiotemporal features for long-horizon prediction and fail to decouple thermo-mechanical fields. This paper proposes a Physics-informed Neural Operator (PINO) to predict z and y-direction distortion for the future 15 s. Our method, Physics-informed Deep Operator Network-Recurrent Neural Network (PIDeepONet-RNN) employs trunk and branch network to process temperature history and encode distortion fields, respectively, enabling decoupling of thermo-mechanical responses. By incorporating the heat conduction equation as a soft constraint, the model ensures physical consistency and suppresses unphysical artifacts, thereby establishing a more physically consistent mapping between the thermal history and distortion. This is important because such a basis function, grounded in physical laws, provides a robust and interpretable foundation for predictions. The proposed models are trained and tested using datasets generated from experimentally validated Finite Element Method (FEM). Evaluation shows that the model achieves high accuracy, low error accumulation, time efficiency. The max absolute errors in the z and y-directions are as low as 0.9733 mm and 0.2049 mm, respectively. The error distribution shows high errors in the molten pool but low gradient norms in the deposited and key areas. The performance of PINO surrogate model highlights its potential for real-time long-horizon physics field prediction in controlling defects.

</details>


### [409] [Uncertainty-aware Physics-informed Neural Networks for Robust CARS-to-Raman Signal Reconstruction](https://arxiv.org/abs/2511.13185)
*Aishwarya Venkataramanan,Sai Karthikeya Vemuri,Adithya Ashok Chalain Valapil,Joachim Denzler*

Main category: cs.LG

TL;DR: 评估并比较CARS到拉曼信号重建中的不确定性量化技术，表明结合物理信息约束可改善模型校准。


<details>
  <summary>Details</summary>
Motivation: 现有CARS光谱技术受非共振背景干扰，深度学习方法虽能重建拉曼光谱，但确定性模型缺乏不确定性量化能力，而不确定性量化在高风险应用中至关重要。

Method: 评估并比较各种不确定性量化技术，将物理信息约束融入模型。

Result: 结合物理信息约束能改善模型校准。

Conclusion: 结合物理信息约束为更可靠的CARS数据分析提供了有前景的途径。

Abstract: Coherent anti-Stokes Raman scattering (CARS) spectroscopy is a powerful and rapid technique widely used in medicine, material science, and chemical analyses. However, its effectiveness is hindered by the presence of a non-resonant background that interferes with and distorts the true Raman signal. Deep learning methods have been employed to reconstruct the true Raman spectrum from measured CARS data using labeled datasets. A more recent development integrates the domain knowledge of Kramers-Kronig relationships and smoothness constraints in the form of physics-informed loss functions. However, these deterministic models lack the ability to quantify uncertainty, an essential feature for reliable deployment in high-stakes scientific and biomedical applications. In this work, we evaluate and compare various uncertainty quantification (UQ) techniques within the context of CARS-to-Raman signal reconstruction. Furthermore, we demonstrate that incorporating physics-informed constraints into these models improves their calibration, offering a promising path toward more trustworthy CARS data analysis.

</details>


### [410] [DiffFP: Learning Behaviors from Scratch via Diffusion-based Fictitious Play](https://arxiv.org/abs/2511.13186)
*Akash Karthikeyan,Yash Vardhan Pant*

Main category: cs.LG

TL;DR: 提出DiffFP框架解决自博弈强化学习在连续决策空间的挑战，在多智能体环境验证效果好。


<details>
  <summary>Details</summary>
Motivation: 自博弈强化学习在连续决策空间实现复杂行为有挑战，现有方法难收敛到纳什均衡，易被对手利用。

Method: 提出DiffFP框架，用扩散策略近似最优响应，学习鲁棒多模态行为策略。

Result: 在连续空间零和博弈中向ε-纳什均衡收敛，在复杂多智能体环境中学习的策略更鲁棒，优于基线策略，收敛快、成功率高。

Conclusion: DiffFP框架能有效应对自博弈强化学习在连续决策空间的挑战，策略对对手策略有鲁棒性，训练迭代中稳定。

Abstract: Self-play reinforcement learning has demonstrated significant success in learning complex strategic and interactive behaviors in competitive multi-agent games. However, achieving such behaviors in continuous decision spaces remains challenging. Ensuring adaptability and generalization in self-play settings is critical for achieving competitive performance in dynamic multi-agent environments. These challenges often cause methods to converge slowly or fail to converge at all to a Nash equilibrium, making agents vulnerable to strategic exploitation by unseen opponents. To address these challenges, we propose DiffFP, a fictitious play (FP) framework that estimates the best response to unseen opponents while learning a robust and multimodal behavioral policy. Specifically, we approximate the best response using a diffusion policy that leverages generative modeling to learn adaptive and diverse strategies. Through empirical evaluation, we demonstrate that the proposed FP framework converges towards $ε$-Nash equilibria in continuous- space zero-sum games. We validate our method on complex multi-agent environments, including racing and multi-particle zero-sum games. Simulation results show that the learned policies are robust against diverse opponents and outperform baseline reinforcement learning policies. Our approach achieves up to 3$\times$ faster convergence and 30$\times$ higher success rates on average against RL-based baselines, demonstrating its robustness to opponent strategies and stability across training iterations

</details>


### [411] [ParaDySe: A Parallel-Strategy Switching Framework for Dynamic Sequence Lengths in Transformer](https://arxiv.org/abs/2511.13198)
*Zhixin Ou,Peng Liang,Jianchen Han,Baihui Liu,Linbo Qiao*

Main category: cs.LG

TL;DR: 提出ParaDySe框架解决Transformer大语言模型训练中动态序列并行策略问题，实验表明其能解决OOM和CPC瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前训练框架对动态序列采用预定义静态并行策略，导致短序列通信并行化无法取消、长序列内存溢出。

Method: 实现具有统一张量布局规范的并行策略模块化函数库，用混合方法构建序列感知的内存和时间成本模型，通过启发式算法选择最优分层策略，利用设计的函数库实现策略无缝切换。

Result: 在代表性大语言模型和长度达624K的数据集上实验，表明ParaDySe能将长序列优化与现有框架系统集成。

Conclusion: ParaDySe解决了大语言模型训练中的OOM和CPC瓶颈。

Abstract: Dynamic sequences with varying lengths have been widely used in the training of Transformer-based large language models (LLMs). However, current training frameworks adopt a pre-defined static parallel strategy for these sequences, causing neither communication-parallelization cancellation on short sequences nor out-of-memory on long sequences. To mitigate these issues, we propose ParaDySe, a novel adaptive Parallel strategy switching framework for Dynamic Sequences. ParaDySe enables on-the-fly optimal strategy adoption according to the immediate input sequence. It first implements the modular function libraries for parallel strategies with unified tensor layout specifications, and then builds sequence-aware memory and time cost models with hybrid methods. Guided by cost models, ParaDySe selects optimal layer-wise strategies for dynamic sequences via an efficient heuristic algorithm. By integrating these techniques together, ParaDySe achieves seamless hot-switching of optimal strategies through its well-designed function libraries. We compare ParaDySe with baselines on representative LLMs under datasets with sequence lengths up to 624K. Experimental results indicate that ParaDySe addresses OOM and CPC bottlenecks in LLM training by systematically integrating long-sequence optimizations with existing frameworks.

</details>


### [412] [TokenSqueeze: Performance-Preserving Compression for Reasoning LLMs](https://arxiv.org/abs/2511.13223)
*Yuxiang Zhang,Zhengxu Yu,Weihang Pan,Zhongming Jin,Qiang Fu,Deng Cai,Binbin Lin,Jieping Ye*

Main category: cs.LG

TL;DR: 提出TokenSqueeze方法解决推理大模型效率与准确率权衡问题，减少token使用同时保持准确率。


<details>
  <summary>Details</summary>
Motivation: 现有推理大模型长CoT导致高token使用、高推理延迟和内存消耗，现有Long2Short方法常牺牲准确率，需平衡效率与准确率。

Method: 提出TokenSqueeze方法，选择推理深度与问题复杂度自适应匹配的自生成样本，引入分布对齐语言优化方法。

Result: TokenSqueeze有效减少token使用并保持准确率，DeepSeek - R1 - Distill - Qwen - 7B在MATH500基准上平均减少50% token使用。

Conclusion: TokenSqueeze仅用模型自生成数据，可在不同应用中实现高效高保真推理。

Abstract: Emerging reasoning LLMs such as OpenAI-o1 and DeepSeek-R1 have achieved strong performance on complex reasoning tasks by generating long chain-of-thought (CoT) traces. However, these long CoTs result in increased token usage, leading to higher inference latency and memory consumption. As a result, balancing accuracy and reasoning efficiency has become essential for deploying reasoning LLMs in practical applications. Existing long-to-short (Long2Short) methods aim to reduce inference length but often sacrifice accuracy, revealing a need for an approach that maintains performance while lowering token costs. To address this efficiency-accuracy tradeoff, we propose TokenSqueeze, a novel Long2Short method that condenses reasoning paths while preserving performance and relying exclusively on self-generated data. First, to prevent performance degradation caused by excessive compression of reasoning depth, we propose to select self-generated samples whose reasoning depth is adaptively matched to the complexity of the problem. To further optimize the linguistic expression without altering the underlying reasoning paths, we introduce a distribution-aligned linguistic refinement method that enhances the clarity and conciseness of the reasoning path while preserving its logical integrity. Comprehensive experimental results demonstrate the effectiveness of TokenSqueeze in reducing token usage while maintaining accuracy. Notably, DeepSeek-R1-Distill-Qwen-7B fine-tuned using our proposed method achieved a 50\% average token reduction while preserving accuracy on the MATH500 benchmark. TokenSqueeze exclusively utilizes the model's self-generated data, enabling efficient and high-fidelity reasoning without relying on manually curated short-answer datasets across diverse applications. Our code is available at https://github.com/zhangyx1122/TokenSqueeze.

</details>


### [413] [MorphBoost: Self-Organizing Universal Gradient Boosting with Adaptive Tree Morphing](https://arxiv.org/abs/2511.13234)
*Boris Kriuk*

Main category: cs.LG

TL;DR: 提出MorphBoost梯度提升框架，有自组织树结构，动态调整分裂行为，经多数据集测试性能超XGBoost等模型。


<details>
  <summary>Details</summary>
Motivation: 传统梯度提升算法树结构静态，分裂标准固定，难以适应不同学习阶段的梯度分布和问题特征。

Method: 引入MorphBoost框架，实现自适应分裂函数，有创新的分裂准则、问题指纹、向量预测、特征重要性检测和快速模式优化。

Result: 在10个数据集上与多个竞争模型对比，MorphBoost平均性能超XGBoost 0.84%，胜率40%，前3名占比20%，方差最低，最小准确率最高。

Conclusion: MorphBoost性能达先进水平，一致性和鲁棒性好，在复杂问题上因高适应性有显著提升。

Abstract: Traditional gradient boosting algorithms employ static tree structures with fixed splitting criteria that remain unchanged throughout training, limiting their ability to adapt to evolving gradient distributions and problem-specific characteristics across different learning stages. This work introduces MorphBoost, a new gradient boosting framework featuring self-organizing tree structures that dynamically morph their splitting behavior during training. The algorithm implements adaptive split functions that evolve based on accumulated gradient statistics and iteration-dependent learning pressures, enabling automatic adjustment to problem complexity. Key innovations include: (1) morphing split criterion combining gradient-based scores with information-theoretic metrics weighted by training progress; (2) automatic problem fingerprinting for intelligent parameter configuration across binary/multiclass/regression tasks; (3) vectorized tree prediction achieving significant computational speedups; (4) interaction-aware feature importance detecting multiplicative relationships; and (5) fast-mode optimization balancing speed and accuracy. Comprehensive benchmarking across 10 diverse datasets against competitive models (XGBoost, LightGBM, GradientBoosting, HistGradientBoosting, ensemble methods) demonstrates that MorphBoost achieves state-of-the-art performance, outperforming XGBoost by 0.84% on average. MorphBoost secured the overall winner position with 4/10 dataset wins (40% win rate) and 6/30 top-3 finishes (20%), while maintaining the lowest variance (σ=0.0948) and highest minimum accuracy across all models, revealing superior consistency and robustness. Performance analysis across difficulty levels shows competitive results on easy datasets while achieving notable improvements on advanced problems due to higher adaptation levels.

</details>


### [414] [Computational Measurement of Political Positions: A Review of Text-Based Ideal Point Estimation Algorithms](https://arxiv.org/abs/2511.13238)
*Patrick Parschan,Charlott Jakob*

Main category: cs.LG

TL;DR: 文章对无监督和半监督的基于计算文本的理想点估计（CT - IPE）算法进行系统综述，介绍算法发展，提出概念框架对比，给出三方面贡献。


<details>
  <summary>Details</summary>
Motivation: 过去二十年CT - IPE算法发展虽扩展了方法库，但该领域缺乏系统比较和应用指导，需填补此空白。

Method: 通过系统文献综述确定25种CT - IPE算法，对其建模假设和发展背景进行手动内容分析，引入概念框架区分算法处理文本差异的方式。

Result: 识别出四类方法家族，即词频、主题建模、词嵌入和基于大语言模型的方法，并评估其假设、可解释性、可扩展性和局限性。

Conclusion: 文章提供算法发展的结构化综合，为应用研究者提供实用指导，强调算法估计结果差异的信息价值和系统基准测试的必要性。

Abstract: This article presents the first systematic review of unsupervised and semi-supervised computational text-based ideal point estimation (CT-IPE) algorithms, methods designed to infer latent political positions from textual data. These algorithms are widely used in political science, communication, computational social science, and computer science to estimate ideological preferences from parliamentary speeches, party manifestos, and social media. Over the past two decades, their development has closely followed broader NLP trends -- beginning with word-frequency models and most recently turning to large language models (LLMs). While this trajectory has greatly expanded the methodological toolkit, it has also produced a fragmented field that lacks systematic comparison and clear guidance for applied use. To address this gap, we identified 25 CT-IPE algorithms through a systematic literature review and conducted a manual content analysis of their modeling assumptions and development contexts. To compare them meaningfully, we introduce a conceptual framework that distinguishes how algorithms generate, capture, and aggregate textual variance. On this basis, we identify four methodological families -- word-frequency, topic modeling, word embedding, and LLM-based approaches -- and critically assess their assumptions, interpretability, scalability, and limitations. Our review offers three contributions. First, it provides a structured synthesis of two decades of algorithm development, clarifying how diverse methods relate to one another. Second, it translates these insights into practical guidance for applied researchers, highlighting trade-offs in transparency, technical requirements, and validation strategies that shape algorithm choice. Third, it emphasizes that differences in estimation outcomes across algorithms are themselves informative, underscoring the need for systematic benchmarking.

</details>


### [415] [Incoherent Beliefs & Inconsistent Actions in Large Language Models](https://arxiv.org/abs/2511.13240)
*Arka Pal,Teo Kitanovski,Arthur Liang,Akilesh Potti,Micah Goldblum*

Main category: cs.LG

TL;DR: 研究LLM在动态环境中信念更新和行动一致性，发现LLM信念更新和行动常不一致，强模型也存在此问题，凸显预测其在复杂现实场景行为的困难。


<details>
  <summary>Details</summary>
Motivation: 现实任务和环境与LLM评估的静态数据集不同，预测LLM在动态环境表现很重要且难以从静态测量确定。

Method: 研究LLM信念更新能力和行动与信念的一致性。

Result: LLM信念更新不一致，行动与信念不符，对用户挑战的回应有中度自我不一致，强模型也有这些问题。

Conclusion: 预测LLM在复杂现实场景的行为存在困难。

Abstract: Real-world tasks and environments exhibit differences from the static datasets that large language models (LLMs) are typically evaluated on. Such tasks can involve sequential interaction, requiring coherent updating of beliefs in light of new evidence, and making appropriate decisions based on those beliefs. Predicting how LLMs will perform in such dynamic environments is important, but can be tricky to determine from measurements in static settings. In this work, we examine two critical components of LLM performance: the ability of LLMs to coherently update their beliefs, and the extent to which the actions they take are consistent with those beliefs. First, we find that LLMs are largely inconsistent in how they update their beliefs; models can exhibit up to a 30% average difference between the directly elicited posterior, and the correct update of their prior. Second, we find that LLMs also often take actions which are inconsistent with the beliefs they hold. On a betting market, for example, LLMs often do not even bet in the same direction as their internally held beliefs over the underlying outcomes. We also find they have moderate self-inconsistency in how they respond to challenges by users to given answers. Finally, we show that the above properties hold even for strong models that obtain high accuracy or that are well-calibrated on the tasks at hand. Our results highlight the difficulties of predicting LLM behavior in complex real-world settings.

</details>


### [416] [Uncovering and Mitigating Transient Blindness in Multimodal Model Editing](https://arxiv.org/abs/2511.13243)
*Xiaoqi Han,Ru Li,Ran Yi,Hongye Tan,Zhuomin Liang,Víctor Gutiérrez-Basulto,Jeff Z. Pan*

Main category: cs.LG

TL;DR: 提出综合局部性评估框架和动态评估De - VQA，发现瞬态盲目现象，提出局部感知对抗损失，结果优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有从文本模型编辑改编的评估方法存在高估成功、掩盖过拟合问题，需更好评估方法用于多模态模型编辑。

Method: 提出涵盖三个关键维度、七种数据类型的综合局部性评估框架；引入动态评估De - VQA；进行token分析；提出局部感知对抗损失。

Result: 方法始终优于现有基线，平均减少瞬态盲目现象并将局部性提高17%。

Conclusion: 提出的方法能有效解决多模态模型编辑评估问题，提升模型性能。

Abstract: Multimodal Model Editing (MMED) aims to correct erroneous knowledge in multimodal models. Existing evaluation methods, adapted from textual model editing, overstate success by relying on low-similarity or random inputs, obscure overfitting. We propose a comprehensive locality evaluation framework, covering three key dimensions: random-image locality, no-image locality, and consistent-image locality, operationalized through seven distinct data types, enabling a detailed and structured analysis of multimodal edits. We introduce De-VQA, a dynamic evaluation for visual question answering, uncovering a phenomenon we term transient blindness, overfitting to edit-similar text while ignoring visuals. Token analysis shows edits disproportionately affect textual tokens. We propose locality-aware adversarial losses to balance cross-modal representations. Empirical results demonstrate that our approach consistently outperforms existing baselines, reducing transient blindness and improving locality by 17% on average.

</details>


### [417] [Seek and You Shall Fold](https://arxiv.org/abs/2511.13244)
*Nadav Bojan Sellam,Meital Bojan,Paul Schanda,Alex Bronstein*

Main category: cs.LG

TL;DR: 提出用于蛋白质生成模型的非可微引导框架，在三种模式验证有效性，推动数据条件下蛋白质建模。


<details>
  <summary>Details</summary>
Motivation: 将实验数据融入蛋白质生成模型是挑战，多数实验可观测值预测器不可微，与基于梯度的条件采样不兼容。

Method: 引入非可微引导框架，通过定制遗传算法将基于连续扩散的生成器与任意黑盒目标耦合。

Result: 在三种模式证明框架有效性，实现化学位移引导的结构生成，暴露当前预测器弱点。

Conclusion: 工作指向超越可微性限制的自动化、数据条件下蛋白质建模。

Abstract: Accurate protein structures are essential for understanding biological function, yet incorporating experimental data into protein generative models remains a major challenge. Most predictors of experimental observables are non-differentiable, making them incompatible with gradient-based conditional sampling. This is especially limiting in nuclear magnetic resonance, where rich data such as chemical shifts are hard to directly integrate into generative modeling. We introduce a framework for non-differentiable guidance of protein generative models, coupling a continuous diffusion-based generator with any black-box objective via a tailored genetic algorithm. We demonstrate its effectiveness across three modalities: pairwise distance constraints, nuclear Overhauser effect restraints, and for the first time chemical shifts. These results establish chemical shift guided structure generation as feasible, expose key weaknesses in current predictors, and showcase a general strategy for incorporating diverse experimental signals. Our work points toward automated, data-conditioned protein modeling beyond the limits of differentiability.

</details>


### [418] [Edge-aware baselines for ogbn-proteins in PyTorch Geometric: species-wise normalization, post-hoc calibration, and cost-accuracy trade-offs](https://arxiv.org/abs/2511.13250)
*Aleksandar Stanković,Dejan Lisica*

Main category: cs.LG

TL;DR: 本文在PyTorch Geometric中为ogbn - proteins提供可复现、边缘感知的基线，研究系统选择，对比归一化方法，发现不同聚合方式、归一化方法的效果，还提出后处理方法提升性能并发布相关工件和脚本。


<details>
  <summary>Details</summary>
Motivation: 为ogbn - proteins提供可复现的边缘感知基线，研究系统选择对性能的影响。

Method: 研究两种系统选择，对比GraphSAGE不同聚合方式、LayerNorm、BatchNorm和Conditional LayerNorm，采用后处理方法如温度缩放、阈值设置和标签相关性平滑。

Result: sum聚合方式优于mean和max；BN的AUC最佳，CLN的AUC相当且阈值F1更好；后处理方法提升了micro - F1和ECE，且AUC变化可忽略。

Conclusion: 提出的基线和后处理方法能有效提升ogbn - proteins的性能，并发布可复现的实验工件和脚本。

Abstract: We present reproducible, edge-aware baselines for ogbn-proteins in PyTorch Geometric (PyG). We study two system choices that dominate practice: (i) how 8-dimensional edge evidence is aggregated into node inputs, and (ii) how edges are used inside message passing. Our strongest baseline is GraphSAGE with sum-based edge-to-node features. We compare LayerNorm (LN), BatchNorm (BN), and a species-aware Conditional LayerNorm (CLN), and report compute cost (time, VRAM, parameters) together with accuracy (ROC-AUC) and decision quality. In our primary experimental setup (hidden size 512, 3 layers, 3 seeds), sum consistently beats mean and max; BN attains the best AUC, while CLN matches the AUC frontier with better thresholded F1. Finally, post-hoc per-label temperature scaling plus per-label thresholds substantially improves micro-F1 and expected calibration error (ECE) with negligible AUC change, and light label-correlation smoothing yields small additional gains. We release standardized artifacts and scripts used for all of the runs presented in the paper.

</details>


### [419] [Explainable RL Policies by Distilling to Locally-Specialized Linear Policies with Voronoi State Partitioning](https://arxiv.org/abs/2511.13322)
*Senne Deproost,Dennis Steckelmacher,Ann Nowé*

Main category: cs.LG

TL;DR: 本文提出用Voronoi分区将状态空间划分区域，使简化线性模型在各区域操作，在网格世界和经典控制任务评估显示，该方法产生可解释策略，且性能相当或略优。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习缺乏透明度，知识蒸馏常使用单一模型，在动态场景有挑战，需平衡模型灵活性和复杂性。

Method: 提出一种新的与模型无关的方法，使用Voronoi分区将状态空间划分为区域，让简化、人类可理解的线性模型在各区域操作。

Result: 在网格世界环境和经典控制任务中评估，蒸馏得到的局部专用线性模型产生的策略可解释，且性能与原黑盒策略相当甚至略优。

Conclusion: 提出的状态空间分区知识蒸馏方法有效，能产生可解释策略并取得良好性能。

Abstract: Deep Reinforcement Learning is one of the state-of-the-art methods for producing near-optimal system controllers. However, deep RL algorithms train a deep neural network, that lacks transparency, which poses challenges when the controller has to meet regulations, or foster trust. To alleviate this, one could transfer the learned behaviour into a model that is human-readable by design using knowledge distilla- tion. Often this is done with a single model which mimics the original model on average but could struggle in more dynamic situations. A key challenge is that this simpler model should have the right balance be- tween flexibility and complexity or right balance between balance bias and accuracy. We propose a new model-agnostic method to divide the state space into regions where a simplified, human-understandable model can operate in. In this paper, we use Voronoi partitioning to find regions where linear models can achieve similar performance to the original con- troller. We evaluate our approach on a gridworld environment and a classic control task. We observe that our proposed distillation to locally- specialized linear models produces policies that are explainable and show that the distillation matches or even slightly outperforms the black-box policy they are distilled from.

</details>


### [420] [Tab-PET: Graph-Based Positional Encodings for Tabular Transformers](https://arxiv.org/abs/2511.13338)
*Yunze Leng,Rohan Ghosh,Mehul Motani*

Main category: cs.LG

TL;DR: 本文指出表格数据监督学习有挑战，发现位置编码（PEs）能提升表格transformer泛化性能，提出Tab - PET框架，实证表明图派生的PEs能提升性能，关联图效果更好。


<details>
  <summary>Details</summary>
Motivation: 表格数据监督学习存在低数据量、无结构线索等挑战，现有transformer模型未充分利用结构线索，需探索提升泛化性能的方法。

Method: 理论和实证研究PEs对表格transformer的作用，提出Tab - PET框架，探索关联和因果两种图估计范式。

Result: 图派生的PEs在50个分类和回归数据集上显著提升3T模型性能，关联图比因果图效果更稳定、提升更明显。

Conclusion: PEs在表格transformers中有意外作用，可利用其提升泛化性能。

Abstract: Supervised learning with tabular data presents unique challenges, including low data sizes, the absence of structural cues, and heterogeneous features spanning both categorical and continuous domains. Unlike vision and language tasks, where models can exploit inductive biases in the data, tabular data lacks inherent positional structure, hindering the effectiveness of self-attention mechanisms. While recent transformer-based models like TabTransformer, SAINT, and FT-Transformer (which we refer to as 3T) have shown promise on tabular data, they typically operate without leveraging structural cues such as positional encodings (PEs), as no prior structural information is usually available. In this work, we find both theoretically and empirically that structural cues, specifically PEs can be a useful tool to improve generalization performance for tabular transformers. We find that PEs impart the ability to reduce the effective rank (a form of intrinsic dimensionality) of the features, effectively simplifying the task by reducing the dimensionality of the problem, yielding improved generalization. To that end, we propose Tab-PET (PEs for Tabular Transformers), a graph-based framework for estimating and inculcating PEs into embeddings. Inspired by approaches that derive PEs from graph topology, we explore two paradigms for graph estimation: association-based and causality-based. We empirically demonstrate that graph-derived PEs significantly improve performance across 50 classification and regression datasets for 3T. Notably, association-based graphs consistently yield more stable and pronounced gains compared to causality-driven ones. Our work highlights an unexpected role of PEs in tabular transformers, revealing how they can be harnessed to improve generalization.

</details>


### [421] [Statistically Accurate and Robust Generative Prediction of Rock Discontinuities with A Tabular Foundation Model](https://arxiv.org/abs/2511.13339)
*Han Meng,Gang Mei,Hong Tian,Nengxiong Xu,Jianbing Peng*

Main category: cs.LG

TL;DR: 提出利用表格基础模型对岩石不连续面进行生成预测的方法，实验显示其优于传统模型和深度生成方法，推动岩体结构定量表征。


<details>
  <summary>Details</summary>
Motivation: 现有岩石不连续面生成预测方法无法捕捉复杂分布模式或在数据稀疏时缺乏鲁棒性。

Method: 利用专为小数据设计的表格基础模型，借助其样本学习能力对有限测量的不连续面进行分析。

Result: 在十个不同规模和分布模式的数据集上的对比实验表明，该方法比传统统计模型和深度生成方法有更高的准确性和鲁棒性。

Conclusion: 该工作推进了岩体结构的定量表征，有助于更安全可靠的数据驱动岩土工程设计。

Abstract: Rock discontinuities critically govern the mechanical behavior and stability of rock masses. Their internal distributions remain largely unobservable and are typically inferred from surface-exposed discontinuities using generative prediction approaches. However, surface-exposed observations are inherently sparse, and existing generative prediction approaches either fail to capture the underlying complex distribution patterns or lack robustness under data-sparse conditions. Here, we proposed a simple yet robust approach for statistically accurate generative prediction of rock discontinuities by utilizing a tabular foundation model. By leveraging the powerful sample learning capability of the foundation model specifically designed for small data, our approach can effectively capture the underlying complex distribution patterns within limited measured discontinuities. Comparative experiments on ten datasets with diverse scales and distribution patterns of discontinuities demonstrate superior accuracy and robustness over conventional statistical models and deep generative approaches. This work advances quantitative characterization of rock mass structures, supporting safer and more reliable data-driven geotechnical design.

</details>


### [422] [Dual-LoRA and Quality-Enhanced Pseudo Replay for Multimodal Continual Food Learning](https://arxiv.org/abs/2511.13351)
*Xinlan Wu,Bin Zhu,Feng Han,Pengkun Jiao,Jingjing Chen*

Main category: cs.LG

TL;DR: 提出用于多模态食物学习的持续学习框架，在Uni - Food数据集实验中表现良好，能缓解遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有食物分析的大型多模态模型学习新任务时会灾难性遗忘，需从头昂贵再训练。

Method: 提出结合Dual - LoRA架构与质量增强伪重放的持续学习框架，为每个任务引入两种低秩适配器，采用质量增强伪重放策略减少生成样本幻觉。

Result: 在Uni - Food数据集实验中，在缓解遗忘方面表现优越。

Conclusion: 该框架是复杂食物任务中首个有效的持续学习方法。

Abstract: Food analysis has become increasingly critical for health-related tasks such as personalized nutrition and chronic disease prevention. However, existing large multimodal models (LMMs) in food analysis suffer from catastrophic forgetting when learning new tasks, requiring costly retraining from scratch. To address this, we propose a novel continual learning framework for multimodal food learning, integrating a Dual-LoRA architecture with Quality-Enhanced Pseudo Replay. We introduce two complementary low-rank adapters for each task: a specialized LoRA that learns task-specific knowledge with orthogonal constraints to previous tasks' subspaces, and a cooperative LoRA that consolidates shared knowledge across tasks via pseudo replay. To improve the reliability of replay data, our Quality-Enhanced Pseudo Replay strategy leverages self-consistency and semantic similarity to reduce hallucinations in generated samples. Experiments on the comprehensive Uni-Food dataset show superior performance in mitigating forgetting, representing the first effective continual learning approach for complex food tasks.

</details>


### [423] [A Novel Hierarchical Integration Method for Efficient Model Merging in Medical LLMs](https://arxiv.org/abs/2511.13373)
*Prakrit Timilsina,Anuj Nepal,Rajan Kadel,Robin Doss*

Main category: cs.LG

TL;DR: 本文系统评估六种参数空间合并技术用于医疗大模型，提出分层方法，研究表明架构兼容模型简单平均法效果好，为分布式医疗AI部署提供见解。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在分布式医疗中面临整合知识、保护隐私、降低计算开销和防止灾难性遗忘等挑战。

Method: 系统评估六种参数空间合并技术，引入结合选择性最优传输对齐与余弦相似度加权插值的分层方法，并在五个医疗基准上评估。

Result: 架构兼容模型从简单平均法中获益显著，任务算术法在MedQA上准确率达45.80%，优于复杂剪枝方法。

Conclusion: 对于架构兼容模型，简单平均法为知识整合提供了强大且计算高效的基线，为可扩展医疗AI系统提供实用途径。

Abstract: Large Language Models (LLMs) face significant challenges in distributed healthcare, including consolidating specialized domain knowledge across institutions while maintaining privacy, reducing computational overhead, and preventing catastrophic forgetting during model updates.This paper presents a systematic evaluation of six parameter-space merging techniques applied to two architecturally compatible medical LLMs derived from the Mistral-7B base model. We introduce a novel hierarchical method that combines selective Optimal Transport (OT) alignment for attention layers with cosine similarity-weighted interpolation, designed to address permutation variance while minimizing computational overhead for edge deployment scenarios. Our study evaluates Task Arithmetic, Linear Averaging, DARE-TIES, DELLA, Breadcrumbs, and our Hierarchical approach across five medical benchmarks. Results demonstrate that architecturally compatible models benefit significantly from simple averaging methods, with Task Arithmetic achieving 45.80% accuracy on MedQA, outperforming complex pruning-based approaches. These findings offer critical insights for the deployment of distributed medical AI in resource-constrained IoT environments, where computational efficiency and model compatibility are paramount. Our work establishes that for architecturally compatible models, simple averaging provides a robust and computationally efficient baseline for knowledge consolidation, offering a pragmatic path forward for scalable medical AI systems.

</details>


### [424] [Finding Kissing Numbers with Game-theoretic Reinforcement Learning](https://arxiv.org/abs/2511.13391)
*Chengdong Ma,Théo Tao Zhaowei,Pengyu Li,Minghao Liu,Haojun Chen,Zihao Mao,Yuan Cheng,Yuan Qi,Yaodong Yang*

Main category: cs.LG

TL;DR: 本文将接吻数问题建模为双人矩阵补全游戏，用强化学习系统PackingStar探索高维空间，超越了已知记录，展示了AI探索高维空间的能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在解决高维接吻数问题时受高维几何不规则性和组合复杂度限制，需要新方法。

Method: 将问题建模为双人矩阵补全游戏，训练游戏理论强化学习系统PackingStar，通过玩家合作最大化矩阵大小。

Result: PackingStar重现之前配置，在25 - 31维超越已知记录，在13维取得突破，在14维等发现6000多个新结构。

Conclusion: 证明了AI探索高维空间超越人类直觉的能力，为接吻数问题和更广泛的几何问题开辟新途径。

Abstract: Since Isaac Newton first studied the Kissing Number Problem in 1694, determining the maximal number of non-overlapping spheres around a central sphere has remained a fundamental challenge. This problem represents the local analogue of Hilbert's 18th problem on sphere packing, bridging geometry, number theory, and information theory. Although significant progress has been made through lattices and codes, the irregularities of high-dimensional geometry and exponentially growing combinatorial complexity beyond 8 dimensions, which exceeds the complexity of Go game, limit the scalability of existing methods. Here we model this problem as a two-player matrix completion game and train the game-theoretic reinforcement learning system, PackingStar, to efficiently explore high-dimensional spaces. The matrix entries represent pairwise cosines of sphere center vectors; one player fills entries while another corrects suboptimal ones, jointly maximizing the matrix size, corresponding to the kissing number. This cooperative dynamics substantially improves sample quality, making the extremely large spaces tractable. PackingStar reproduces previous configurations and surpasses all human-known records from dimensions 25 to 31, with the configuration in 25 dimensions geometrically corresponding to the Leech lattice and suggesting possible optimality. It achieves the first breakthrough beyond rational structures from 1971 in 13 dimensions and discovers over 6000 new structures in 14 and other dimensions. These results demonstrate AI's power to explore high-dimensional spaces beyond human intuition and open new pathways for the Kissing Number Problem and broader geometry problems.

</details>


### [425] [PAST: A Primary-Auxiliary Spatio-Temporal Network for Traffic Time Series Imputation](https://arxiv.org/abs/2511.13414)
*Hanwen Hu,Zimo Wen,Shiyou Qian,Jian Co*

Main category: cs.LG

TL;DR: 提出PAST网络处理交通时间序列数据插补问题，实验表明其性能优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以适应随机缺失位置，无法学习长期和大规模依赖，难以处理多种缺失数据条件。

Method: 将模式分为主要模式和辅助模式，提出PAST网络，包括GIM和CGM模块，GIM捕获主要模式，CGM提取辅助模式，两模块通过共享隐藏向量交互并在集成自监督框架下训练。

Result: 在三个数据集27种缺失数据条件下实验，PAST插补精度在RMSE上最高优于7个基线模型26.2%，在MAE上最高优于31.6%。

Conclusion: PAST网络在交通时间序列数据插补任务中表现出色，能有效处理多种缺失数据情况。

Abstract: Traffic time series imputation is crucial for the safety and reliability of intelligent transportation systems, while diverse types of missing data, including random, fiber, and block missing make the imputation task challenging. Existing models often focus on disentangling and separately modeling spatial and temporal patterns based on relationships between data points. However, these approaches struggle to adapt to the random missing positions, and fail to learn long-term and large-scale dependencies, which are essential in extensive missing conditions. In this paper, patterns are categorized into two types to handle various missing data conditions: primary patterns, which originate from internal relationships between data points, and auxiliary patterns, influenced by external factors like timestamps and node attributes. Accordingly, we propose the Primary-Auxiliary Spatio-Temporal network (PAST). It comprises a graph-integrated module (GIM) and a cross-gated module (CGM). GIM captures primary patterns via dynamic graphs with interval-aware dropout and multi-order convolutions, and CGM extracts auxiliary patterns through bidirectional gating on embedded external features. The two modules interact via shared hidden vectors and are trained under an ensemble self-supervised framework. Experiments on three datasets under 27 missing data conditions demonstrate that the imputation accuracy of PAST outperforms seven state-of-the-art baselines by up to 26.2% in RMSE and 31.6% in MAE.

</details>


### [426] [MMWSTM-ADRAN+: A Novel Hybrid Deep Learning Architecture for Enhanced Climate Time Series Forecasting and Extreme Event Prediction](https://arxiv.org/abs/2511.13419)
*Shaheen Mohammed Saleh Ahmed,Hakan Hakan Guneyli*

Main category: cs.LG

TL;DR: 提出MMWSTM - ADRAN+模型预测日最高气温及其极端事件，介绍模型结构与优化方法。


<details>
  <summary>Details</summary>
Motivation: 准确的极端气温事件短期预测是气候风险管理的挑战，旨在解决该问题。

Method: 提出MMWSTM - ADRAN+双流深度学习架构，结合政权感知动力学模型和异常聚焦注意力机制，使用自定义损失函数和数据增强方法。

Result: 未提及具体结果

Conclusion: 未提及具体结论

Abstract: Accurate short-range prediction of extreme air temperature events remains a fundamental challenge in operational climate-risk management. We present Multi-Modal Weather State Transition Model with Anomaly-Driven Recurrent Attention Network Plus (MMWSTM-ADRAN+), a dual-stream deep learning architecture that couples a regime-aware dynamics model with an anomaly-focused attention mechanism to forecast daily maximum temperature and its extremes. The first stream, MMWSTM, combines bidirectional Long Short-Term Memory (BiLSTM) units with a learnable Markov state transition matrix to capture synoptic-scale weather regime changes. The second stream, ADRAN, integrates bidirectional Gated Recurrent Units (BiGRUs), multi-head self-attention, and a novel anomaly amplification layer to enhance sensitivity to low-probability signals. A lightweight attentive fusion gate adaptively determines the contribution of each stream to the final prediction. Model optimization employs a custom ExtremeWeatherLoss function that up-weights errors on the upper 5% and lower 5% of the temperature distribution, and a time-series data augmentation suite (jittering, scaling, time/magnitude warping) that effectively quadruples the training data

</details>


### [427] [Discovering Operational Patterns Using Image-Based Convolutional Clustering and Composite Evaluation: A Case Study in Foundry Melting Processes](https://arxiv.org/abs/2511.13444)
*Zhipeng Ma,Bo Nørregaard Jørgensen,Zheng Grace Ma*

Main category: cs.LG

TL;DR: 提出基于图像卷积聚类和复合内部评估的无监督框架用于单变量时间序列数据操作模式发现，应用于熔炉熔炼操作效果良好。


<details>
  <summary>Details</summary>
Motivation: 工业过程监测中传统方法难从传感器时间序列数据提取有意义模式，现有聚类技术处理动态非结构化序列能力有限。

Method: 将原始时间序列转换为灰度矩阵，用深度卷积自编码器提取特征；集成软、硬聚类输出并通过两阶段策略优化选择；用新的复合得分S_eva评估聚类性能。

Result: 应用于北欧铸造厂超3900次熔炉熔炼操作，识别出七种可解释操作模式，相比经典和深度聚类基线表现更优。

Conclusion: 该框架解决无监督时间序列分析关键挑战，为工业系统数据诊断和能源优化提供通用解决方案。

Abstract: Industrial process monitoring increasingly relies on sensor-generated time-series data, yet the lack of labels, high variability, and operational noise make it difficult to extract meaningful patterns using conventional methods. Existing clustering techniques either rely on fixed distance metrics or deep models designed for static data, limiting their ability to handle dynamic, unstructured industrial sequences. Addressing this gap, this paper proposes a novel framework for unsupervised discovery of operational modes in univariate time-series data using image-based convolutional clustering with composite internal evaluation. The proposed framework improves upon existing approaches in three ways: (1) raw time-series sequences are transformed into grayscale matrix representations via overlapping sliding windows, allowing effective feature extraction using a deep convolutional autoencoder; (2) the framework integrates both soft and hard clustering outputs and refines the selection through a two-stage strategy; and (3) clustering performance is objectively evaluated by a newly developed composite score, S_eva, which combines normalized Silhouette, Calinski-Harabasz, and Davies-Bouldin indices. Applied to over 3900 furnace melting operations from a Nordic foundry, the method identifies seven explainable operational patterns, revealing significant differences in energy consumption, thermal dynamics, and production duration. Compared to classical and deep clustering baselines, the proposed approach achieves superior overall performance, greater robustness, and domain-aligned explainability. The framework addresses key challenges in unsupervised time-series analysis, such as sequence irregularity, overlapping modes, and metric inconsistency, and provides a generalizable solution for data-driven diagnostics and energy optimization in industrial systems.

</details>


### [428] [Artificial Intelligence-Enabled Spirometry for Early Detection of Right Heart Failure](https://arxiv.org/abs/2511.13457)
*Bin Liu,Qinghao Zhao,Yuxi Zhou,Zhejun Sun,Kaijie Lei,Deyun Zhang,Shijia Geng,Shenda Hong*

Main category: cs.LG

TL;DR: 提出自监督表征学习方法结合肺功能图时间序列和人口统计数据，早期检测肺心病患者右心衰竭，在不同人群测试有良好表现。


<details>
  <summary>Details</summary>
Motivation: 右心衰竭发病率和死亡率高，从肺部疾病患者中筛查出肺心病导致的右心衰竭患者很重要。

Method: 提出自监督表征学习方法，分两阶段，先通过自监督表征学习的肺功能图嵌入网络训练，用变分自编码器编码器学习低维表示，再将其与人口统计信息融合输入CatBoost分类器进行预测。

Result: 在英国生物银行26617人子集上测试，检测右心衰竭的AUROC为0.7501；在慢性肾病和心脏瓣膜病高危临床亚组上，AUROC分别达0.8194和0.8413。

Conclusion: 该自监督表征学习方法在临床实践中对早期检测右心衰竭有良好潜力。

Abstract: Right heart failure (RHF) is a disease characterized by abnormalities in the structure or function of the right ventricle (RV), which is associated with high morbidity and mortality. Lung disease often causes increased right ventricular load, leading to RHF. Therefore, it is very important to screen out patients with cor pulmonale who develop RHF from people with underlying lung diseases. In this work, we propose a self-supervised representation learning method to early detecting RHF from patients with cor pulmonale, which uses spirogram time series to predict patients with RHF at an early stage. The proposed model is divided into two stages. The first stage is the self-supervised representation learning-based spirogram embedding (SLSE) network training process, where the encoder of the Variational autoencoder (VAE-encoder) learns a robust low-dimensional representation of the spirogram time series from the data-augmented unlabeled data. Second, this low-dimensional representation is fused with demographic information and fed into a CatBoost classifier for the downstream RHF prediction task. Trained and tested on a carefully selected subset of 26,617 individuals from the UK Biobank, our model achieved an AUROC of 0.7501 in detecting RHF, demonstrating strong population-level distinction ability. We further evaluated the model on high-risk clinical subgroups, achieving AUROC values of 0.8194 on a test set of 74 patients with chronic kidney disease (CKD) and 0.8413 on a set of 64 patients with valvular heart disease (VHD). These results highlight the model's potential utility in predicting RHF among clinically elevated-risk populations. In conclusion, this study presents a self-supervised representation learning approach combining spirogram time series and demographic data, demonstrating promising potential for early RHF detection in clinical practice.

</details>


### [429] [Multi-task GINN-LP for Multi-target Symbolic Regression](https://arxiv.org/abs/2511.13463)
*Hussein Rajabu,Lijun Qian,Xishuang Dong*

Main category: cs.LG

TL;DR: 提出多任务回归GINN - LP解决符号回归泛化与单输出局限问题，验证于多目标应用，结果兼具预测性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 符号回归在评估数据集和输出类型上存在局限，需解决泛化问题和适应多目标输出的现实需求。

Method: 提出多任务回归GINN - LP，将GINN - LP与多任务深度学习集成，结合共享主干和特定任务输出层。

Result: 在能源效率预测和可持续农业等多目标应用中，模型有有竞争力的预测性能和高可解释性。

Conclusion: 该方法能有效将符号回归拓展到更广泛的现实多输出任务。

Abstract: In the area of explainable artificial intelligence, Symbolic Regression (SR) has emerged as a promising approach by discovering interpretable mathematical expressions that fit data. However, SR faces two main challenges: most methods are evaluated on scientific datasets with well-understood relationships, limiting generalization, and SR primarily targets single-output regression, whereas many real-world problems involve multi-target outputs with interdependent variables. To address these issues, we propose multi-task regression GINN-LP (MTRGINN-LP), an interpretable neural network for multi-target symbolic regression. By integrating GINN-LP with a multi-task deep learning, the model combines a shared backbone including multiple power-term approximator blocks with task-specific output layers, capturing inter-target dependencies while preserving interpretability. We validate multi-task GINN-LP on practical multi-target applications, including energy efficiency prediction and sustainable agriculture. Experimental results demonstrate competitive predictive performance alongside high interpretability, effectively extending symbolic regression to broader real-world multi-output tasks.

</details>


### [430] [GREAT: Generalizable Representation Enhancement via Auxiliary Transformations for Zero-Shot Environmental Prediction](https://arxiv.org/abs/2511.13469)
*Shiyuan Luo,Chonghao Qiu,Runlong Yu,Yiqun Xie,Xiaowei Jia*

Main category: cs.LG

TL;DR: 提出GREAT框架增强数据集以提升环境模型在未监测区域的预测能力，在流温度预测实验中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 环境建模因观测数据有限和地理不平衡，以及空间异质性，难以预测未监测区域生态系统动态，且需在增强数据时保留物理关系和时间连贯性。

Method: 引入GREAT框架，在神经网络多层学习变换函数增强原始环境特征和时间影响，通过双层训练过程约束增强数据以保留原始数据关键模式。

Result: 在美东六个生态多样流域的流温度预测实验中，GREAT在零样本场景下显著优于现有方法。

Conclusion: 为难以全面监测的环境应用提供了实用解决方案。

Abstract: Environmental modeling faces critical challenges in predicting ecosystem dynamics across unmonitored regions due to limited and geographically imbalanced observation data. This challenge is compounded by spatial heterogeneity, causing models to learn spurious patterns that fit only local data. Unlike conventional domain generalization, environmental modeling must preserve invariant physical relationships and temporal coherence during augmentation. In this paper, we introduce Generalizable Representation Enhancement via Auxiliary Transformations (GREAT), a framework that effectively augments available datasets to improve predictions in completely unseen regions. GREAT guides the augmentation process to ensure that the original governing processes can be recovered from the augmented data, and the inclusion of the augmented data leads to improved model generalization. Specifically, GREAT learns transformation functions at multiple layers of neural networks to augment both raw environmental features and temporal influence. They are refined through a novel bi-level training process that constrains augmented data to preserve key patterns of the original source data. We demonstrate GREAT's effectiveness on stream temperature prediction across six ecologically diverse watersheds in the eastern U.S., each containing multiple stream segments. Experimental results show that GREAT significantly outperforms existing methods in zero-shot scenarios. This work provides a practical solution for environmental applications where comprehensive monitoring is infeasible.

</details>


### [431] [Quantum Machine Learning via Contrastive Training](https://arxiv.org/abs/2511.13497)
*Liudmila A. Zhukas,Vivian Ni Zhang,Qiang Miao,Qingfeng Wang,Marko Cetina,Jungsang Kim,Lawrence Carin,Christopher Monroe*

Main category: cs.LG

TL;DR: 本文提出量子表示的自监督预训练方法，在可编程捕获离子量子计算机上实现该范式，提升图像分类性能，是一种标签高效的量子表示学习途径。


<details>
  <summary>Details</summary>
Motivation: 随着量子机器学习发展，其模型面临标记数据稀缺的挑战，需要减少对标记数据的依赖。

Method: 引入量子表示的自监督预训练，在可编程捕获离子量子计算机上实现，对图像进行编码，进行原位对比预训练。

Result: 微调后模型比随机初始化训练的模型有更高的平均测试准确率和更低的运行间变异性，在标记训练数据有限时性能提升显著，学习到的不变性可泛化。

Conclusion: 建立了一种标签高效的量子表示学习途径，与量子原生数据集相关，可处理更大的经典输入。

Abstract: Quantum machine learning (QML) has attracted growing interest with the rapid parallel advances in large-scale classical machine learning and quantum technologies. Similar to classical machine learning, QML models also face challenges arising from the scarcity of labeled data, particularly as their scale and complexity increase. Here, we introduce self-supervised pretraining of quantum representations that reduces reliance on labeled data by learning invariances from unlabeled examples. We implement this paradigm on a programmable trapped-ion quantum computer, encoding images as quantum states. In situ contrastive pretraining on hardware yields a representation that, when fine-tuned, classifies image families with higher mean test accuracy and lower run-to-run variability than models trained from random initialization. Performance improvement is especially significant in regimes with limited labeled training data. We show that the learned invariances generalize beyond the pretraining image samples. Unlike prior work, our pipeline derives similarity from measured quantum overlaps and executes all training and classification stages on hardware. These results establish a label-efficient route to quantum representation learning, with direct relevance to quantum-native datasets and a clear path to larger classical inputs.

</details>


### [432] [Naga: Vedic Encoding for Deep State Space Models](https://arxiv.org/abs/2511.13510)
*Melanie Schaller,Nick Janssen,Bodo Rosenhahn*

Main category: cs.LG

TL;DR: 提出受吠陀数学启发的深度状态空间模型Naga用于长序列时间预测，实验表现优于现有模型，显示吠陀分解的优势。


<details>
  <summary>Details</summary>
Motivation: 寻找长序列时间预测中更有效、可解释的建模方法。

Method: 引入双向表示处理时间序列，结合正向和反向输入序列，通过哈达玛积交互得到吠陀启发编码。

Result: 在多个长序列时间预测基准测试中，Naga优于28个当前最先进模型，且比现有深度SSM方法更高效。

Conclusion: 融入吠陀启发的结构化分解可为长序列建模提供可解释且计算高效的替代方案。

Abstract: This paper presents Naga, a deep State Space Model (SSM) encoding approach inspired by structural concepts from Vedic mathematics. The proposed method introduces a bidirectional representation for time series by jointly processing forward and time-reversed input sequences. These representations are then combined through an element-wise (Hadamard) interaction, resulting in a Vedic-inspired encoding that enhances the model's ability to capture temporal dependencies across distant time steps. We evaluate Naga on multiple long-term time series forecasting (LTSF) benchmarks, including ETTh1, ETTh2, ETTm1, ETTm2, Weather, Traffic, and ILI. The experimental results show that Naga outperforms 28 current state of the art models and demonstrates improved efficiency compared to existing deep SSM-based approaches. The findings suggest that incorporating structured, Vedic-inspired decomposition can provide an interpretable and computationally efficient alternative for long-range sequence modeling.

</details>


### [433] [A Quantum Tensor Network-Based Viewpoint for Modeling and Analysis of Time Series Data](https://arxiv.org/abs/2511.13514)
*Pragatheeswaran Vipulananthan,Kamal Premaratne,Dilip Sarkar,Manohar N. Murthi*

Main category: cs.LG

TL;DR: 提出基于量子物理的白盒方法，结合核均值嵌入和张量网络，在时间序列任务中实现准确不确定性量化和增强可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习中准确不确定性量化难题，弥补神经网络缺乏可解释性和概率白盒模型性能不足的问题。

Method: 将时间序列数据向量的核均值嵌入映射到再生核希尔伯特空间，构建一维自旋链哈密顿量，求解薛定谔方程并应用微扰理论量化不确定性。

Result: 在变化点检测和时间序列聚类任务中，该方法比现有白盒模型更有效。

Conclusion: 该量子张量网络模型能有效实现准确不确定性量化和增强可解释性，为决策过程中的不确定性提供见解。

Abstract: Accurate uncertainty quantification is a critical challenge in machine learning. While neural networks are highly versatile and capable of learning complex patterns, they often lack interpretability due to their ``black box'' nature. On the other hand, probabilistic ``white box'' models, though interpretable, often suffer from a significant performance gap when compared to neural networks. To address this, we propose a novel quantum physics-based ``white box'' method that offers both accurate uncertainty quantification and enhanced interpretability. By mapping the kernel mean embedding (KME) of a time series data vector to a reproducing kernel Hilbert space (RKHS), we construct a tensor network-inspired 1D spin chain Hamiltonian, with the KME as one of its eigen-functions or eigen-modes. We then solve the associated Schr{ö}dinger equation and apply perturbation theory to quantify uncertainty, thereby improving the interpretability of tasks performed with the quantum tensor network-based model. We demonstrate the effectiveness of this methodology, compared to state-of-the-art ``white box" models, in change point detection and time series clustering, providing insights into the uncertainties associated with decision-making throughout the process.

</details>


### [434] [Mitigating Spurious Correlations in Patch-wise Tumor Classification on High-Resolution Multimodal Images](https://arxiv.org/abs/2511.13527)
*Ihab Asaad,Maha Shadaydeh,Joachim Denzler*

Main category: cs.LG

TL;DR: 本文聚焦高分辨率多模态非线性显微镜图像上的逐块二分类，指出简化表述会引入虚假关联，提出去偏策略GERNE，使最坏组准确率提升约7%。


<details>
  <summary>Details</summary>
Motivation: 逐块二分类虽能高效开发模型，但会引入补丁组成与标签的虚假关联，影响模型预测。

Method: 使用可适应最大化最坏组准确率的去偏方法GERNE。

Result: 对于两种不同二值化虚假特征的阈值，最坏组准确率比经验风险最小化提高约7%。

Conclusion: 去偏策略提升了模型在关键少数情况的性能，强调了逐块分类问题中考虑虚假关联学习的重要性。

Abstract: Patch-wise multi-label classification provides an efficient alternative to full pixel-wise segmentation on high-resolution images, particularly when the objective is to determine the presence or absence of target objects within a patch rather than their precise spatial extent. This formulation substantially reduces annotation cost, simplifies training, and allows flexible patch sizing aligned with the desired level of decision granularity. In this work, we focus on a special case, patch-wise binary classification, applied to the detection of a single class of interest (tumor) on high-resolution multimodal nonlinear microscopy images. We show that, although this simplified formulation enables efficient model development, it can introduce spurious correlations between patch composition and labels: tumor patches tend to contain larger tissue regions, whereas non-tumor patches often consist mostly of background with small tissue areas. We further quantify the bias in model predictions caused by this spurious correlation, and propose to use a debiasing strategy to mitigate its effect. Specifically, we apply GERNE, a debiasing method that can be adapted to maximize worst-group accuracy (WGA). Our results show an improvement in WGA by approximately 7% compared to ERM for two different thresholds used to binarize the spurious feature. This enhancement boosts model performance on critical minority cases, such as tumor patches with small tissues and non-tumor patches with large tissues, and underscores the importance of spurious correlation-aware learning in patch-wise classification problems.

</details>


### [435] [Fairness-Aware Graph Representation Learning with Limited Demographic Information](https://arxiv.org/abs/2511.13540)
*Zichong Wang,Zhipeng Yin,Liping Yang,Jun Zhuang,Rui Yu,Qingzhao Kong,Wenbin Zhang*

Main category: cs.LG

TL;DR: 本文提出FairGLite框架，在有限人口统计信息下减轻图学习中的偏差，经实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有公平图学习方法大多需完整人口统计信息，实际中因隐私等限制难以满足，故需在有限信息下减轻图学习偏差。

Method: 提出基于部分人口统计数据生成代理信息的机制，设计跨群体的一致节点嵌入策略，开发自适应置信度策略动态调整节点贡献，还进行理论分析。

Result: 框架在多个数据集和公平图学习框架上的实验中，有效减轻偏差并维持模型效用。

Conclusion: FairGLite框架能在有限人口统计信息下减轻图学习偏差，且有理论上的公平性保证。

Abstract: Ensuring fairness in Graph Neural Networks is fundamental to promoting trustworthy and socially responsible machine learning systems. In response, numerous fair graph learning methods have been proposed in recent years. However, most of them assume full access to demographic information, a requirement rarely met in practice due to privacy, legal, or regulatory restrictions. To this end, this paper introduces a novel fair graph learning framework that mitigates bias in graph learning under limited demographic information. Specifically, we propose a mechanism guided by partial demographic data to generate proxies for demographic information and design a strategy that enforces consistent node embeddings across demographic groups. In addition, we develop an adaptive confidence strategy that dynamically adjusts each node's contribution to fairness and utility based on prediction confidence. We further provide theoretical analysis demonstrating that our framework, FairGLite, achieves provable upper bounds on group fairness metrics, offering formal guarantees for bias mitigation. Through extensive experiments on multiple datasets and fair graph learning frameworks, we demonstrate the framework's effectiveness in both mitigating bias and maintaining model utility.

</details>


### [436] [Graph Out-of-Distribution Detection via Test-Time Calibration with Dual Dynamic Dictionaries](https://arxiv.org/abs/2511.13541)
*Yue Hou,Ruomei Liu,Yingke Su,Junran Wu,Ke Xu*

Main category: cs.LG

TL;DR: 提出名为BaCa的测试时图OOD检测方法，用双动态更新字典校准OOD分数，在真实数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图OOD检测方法因缺乏训练时的OOD样本，难以表示分布边界，且图数据潜在结构探索不足。

Method: 提出BaCa方法，通过估计图子和混合策略生成边界感知的判别拓扑，用优先队列和注意力机制构建双动态字典进行OOD分数校准。

Result: 在真实世界数据集上的大量实验表明，BaCa在OOD检测中显著优于现有最先进方法。

Conclusion: BaCa方法在图OOD检测中有效且表现出色，为该领域提供了更好的解决方案。

Abstract: A key challenge in graph out-of-distribution (OOD) detection lies in the absence of ground-truth OOD samples during training. Existing methods are typically optimized to capture features within the in-distribution (ID) data and calculate OOD scores, which often limits pre-trained models from representing distributional boundaries, leading to unreliable OOD detection. Moreover, the latent structure of graph data is often governed by multiple underlying factors, which remains less explored. To address these challenges, we propose a novel test-time graph OOD detection method, termed BaCa, that calibrates OOD scores using dual dynamically updated dictionaries without requiring fine-tuning the pre-trained model. Specifically, BaCa estimates graphons and applies a mix-up strategy solely with test samples to generate diverse boundary-aware discriminative topologies, eliminating the need for exposing auxiliary datasets as outliers. We construct dual dynamic dictionaries via priority queues and attention mechanisms to adaptively capture latent ID and OOD representations, which are then utilized for boundary-aware OOD score calibration. To the best of our knowledge, extensive experiments on real-world datasets show that BaCa significantly outperforms existing state-of-the-art methods in OOD detection.

</details>


### [437] [RAC-DMVC: Reliability-Aware Contrastive Deep Multi-View Clustering under Multi-Source Noise](https://arxiv.org/abs/2511.13561)
*Shihao Dong,Yue Liu,Xiaotong Zhou,Yuhui Zheng,Huiying Xu,Xinzhong Zhu*

Main category: cs.LG

TL;DR: 本文提出RAC - DMVC框架解决多源噪声下的多视图聚类问题，实验表明其优于SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 提升多视图聚类在现实场景（多源噪声）中的适用性。

Method: 提出RAC - DMVC框架，包括构建可靠性图、引入跨视图重建和可靠性感知噪声对比学习处理观测噪声，设计双注意力插补处理缺失噪声，还有自监督聚类蒸馏模块。

Result: 在五个基准数据集上的实验表明，RAC - DMVC在多个评估指标上优于SOTA方法，且在不同噪声比例下保持良好性能。

Conclusion: RAC - DMVC框架能有效解决多源噪声下的多视图聚类问题。

Abstract: Multi-view clustering (MVC), which aims to separate the multi-view data into distinct clusters in an unsupervised manner, is a fundamental yet challenging task. To enhance its applicability in real-world scenarios, this paper addresses a more challenging task: MVC under multi-source noises, including missing noise and observation noise. To this end, we propose a novel framework, Reliability-Aware Contrastive Deep Multi-View Clustering (RAC-DMVC), which constructs a reliability graph to guide robust representation learning under noisy environments. Specifically, to address observation noise, we introduce a cross-view reconstruction to enhances robustness at the data level, and a reliability-aware noise contrastive learning to mitigates bias in positive and negative pairs selection caused by noisy representations. To handle missing noise, we design a dual-attention imputation to capture shared information across views while preserving view-specific features. In addition, a self-supervised cluster distillation module further refines the learned representations and improves the clustering performance. Extensive experiments on five benchmark datasets demonstrate that RAC-DMVC outperforms SOTA methods on multiple evaluation metrics and maintains excellent performance under varying ratios of noise.

</details>


### [438] [P1: Mastering Physics Olympiads with Reinforcement Learning](https://arxiv.org/abs/2511.13612)
*Jiacheng Chen,Qianjia Cheng,Fangchen Yu,Haiyuan Wan,Yuchen Zhang,Shenghe Zheng,Junchi Yao,Qingyang Zhang,Haonan He,Yun Luo,Yufeng Zhao,Futing Wang,Li Sheng,Chengxing Xie,Yuxin Zuo,Yizhuo Li,Wenxauan Zeng,Yulun Wu,Rui Huang,Dongzhan Zhou,Kai Chen,Yu Qiao,Lei Bai,Yu Cheng,Ning Ding,Bowen Zhou,Peng Ye,Ganqu Cui*

Main category: cs.LG

TL;DR: 本文开发具有物理推理能力的大语言模型P1推动物理研究，该模型在物理竞赛表现出色，且在其他推理任务也有良好表现。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型进步，开发有物理推理能力的模型推动物理研究。

Method: 通过强化学习训练开源物理推理模型P1。

Result: P1-235B-A22B在IPhO 2025获金牌，在13场竞赛获12金；P1-30B-A3B获银牌；P1-235B-A22B+PhysicsMinions在IPhO 2025排第一，13场竞赛平均分最高。

Conclusion: P1模型在物理竞赛表现优异，且在数学、编程等推理任务有良好泛化性。

Abstract: Recent progress in large language models (LLMs) has moved the frontier from puzzle-solving to science-grade reasoning-the kind needed to tackle problems whose answers must stand against nature, not merely fit a rubric. Physics is the sharpest test of this shift, which binds symbols to reality in a fundamental way, serving as the cornerstone of most modern technologies. In this work, we manage to advance physics research by developing large language models with exceptional physics reasoning capabilities, especially excel at solving Olympiad-level physics problems. We introduce P1, a family of open-source physics reasoning models trained entirely through reinforcement learning (RL). Among them, P1-235B-A22B is the first open-source model with Gold-medal performance at the latest International Physics Olympiad (IPhO 2025), and wins 12 gold medals out of 13 international/regional physics competitions in 2024/2025. P1-30B-A3B also surpasses almost all other open-source models on IPhO 2025, getting a silver medal. Further equipped with an agentic framework PhysicsMinions, P1-235B-A22B+PhysicsMinions achieves overall No.1 on IPhO 2025, and obtains the highest average score over the 13 physics competitions. Besides physics, P1 models also present great performance on other reasoning tasks like math and coding, showing the great generalibility of P1 series.

</details>


### [439] [Batch Acquisition Function Evaluations and Decouple Optimizer Updates for Faster Bayesian Optimization](https://arxiv.org/abs/2511.13625)
*Kaichi Irie,Shuhei Watanabe,Masaki Onishi*

Main category: cs.LG

TL;DR: 论文指出BoTorch库优化采集函数方法存在次优问题，提出用协程解耦QN更新并批量调用采集函数的方法，能达到理论上相同收敛效果且大幅减少时间。


<details>
  <summary>Details</summary>
Motivation: BoTorch库在优化采集函数时，因QN方法逆Hessian矩阵的非对角近似误差导致收敛速度慢，需解决此问题。

Method: 用协程解耦QN更新，同时批量调用采集函数。

Result: 所提方法在理论上与顺序MSO收敛效果相同，且相比之前方法大幅减少了实际用时。

Conclusion: 所提方法能有效解决现有方法收敛慢的问题，提升了贝叶斯优化效率。

Abstract: Bayesian optimization (BO) efficiently finds high-performing parameters by maximizing an acquisition function, which models the promise of parameters. A major computational bottleneck arises in acquisition function optimization, where multi-start optimization (MSO) with quasi-Newton (QN) methods is required due to the non-convexity of the acquisition function. BoTorch, a widely used BO library, currently optimizes the summed acquisition function over multiple points, leading to the speedup of MSO owing to PyTorch batching. Nevertheless, this paper empirically demonstrates the suboptimality of this approach in terms of off-diagonal approximation errors in the inverse Hessian of a QN method, slowing down its convergence. To address this problem, we propose to decouple QN updates using a coroutine while batching the acquisition function calls. Our approach not only yields the theoretically identical convergence to the sequential MSO but also drastically reduces the wall-clock time compared to the previous approaches.

</details>


### [440] [Towards Multimodal Representation Learning in Paediatric Kidney Disease](https://arxiv.org/abs/2511.13637)
*Ana Durica,John Booth,Ivana Drobnjak*

Main category: cs.LG

TL;DR: 利用英国一家儿科医院电子健康记录，用时间建模方法预测儿童血清肌酐值异常，为未来多模态扩展奠定基础。


<details>
  <summary>Details</summary>
Motivation: 儿科肾脏疾病表现和进展差异大，需持续监测肾功能。

Method: 采用时间建模方法，将纵向实验室序列与人口统计信息整合，用循环神经模型训练数据进行预测。

Result: 简单时间表示能捕捉儿科常规数据中的有用模式。

Conclusion: 为未来使用更多临床信号和更详细肾脏结果的多模态扩展奠定基础。

Abstract: Paediatric kidney disease varies widely in its presentation and progression, which calls for continuous monitoring of renal function. Using electronic health records collected between 2019 and 2025 at Great Ormond Street Hospital, a leading UK paediatric hospital, we explored a temporal modelling approach that integrates longitudinal laboratory sequences with demographic information. A recurrent neural model trained on these data was used to predict whether a child would record an abnormal serum creatinine value within the following thirty days. Framed as a pilot study, this work provides an initial demonstration that simple temporal representations can capture useful patterns in routine paediatric data and lays the groundwork for future multimodal extensions using additional clinical signals and more detailed renal outcomes.

</details>


### [441] [Data Value in the Age of Scaling: Understanding LLM Scaling Dynamics Under Real-Synthetic Data Mixtures](https://arxiv.org/abs/2511.13640)
*Haohui Wang,Jingyuan Qi,Jianpeng Chen,Jun Wu,Lifu Huang,Lecheng Zheng,Kevin Choi,Balaji Veeramani,Edward Bowen,Alison Hu,Tyler Cody,Dawei Zhou*

Main category: cs.LG

TL;DR: 本文针对大语言模型中真实与合成混合数据集的问题，识别出三相缩放行为，推导泛化边界，提出数据评估方法，实验显示该方法优于现有基线且计算成本低。


<details>
  <summary>Details</summary>
Motivation: 合成数据存在分布差异，给混合数据集的表征和评估带来挑战，需解决相关问题。

Method: 识别三相缩放行为，推导适用于真实和合成混合数据的泛化边界，在此基础上提出数据评估方法。

Result: 在四个任务的综合实验中，所提方法在数据评估上超越了现有基线，且计算成本显著降低。

Conclusion: 提出的有效且高效的数据评估方法能处理大规模数据集，具有良好的应用效果。

Abstract: The rapid progress of large language models (LLMs) is fueled by the growing reliance on datasets that blend real and synthetic data. While synthetic data offers scalability and cost-efficiency, it often introduces systematic distributional discrepancies, particularly underrepresenting long-tail knowledge due to truncation effects from data generation mechanisms like top-p sampling, temperature scaling, and finite sampling. These discrepancies pose fundamental challenges in characterizing and evaluating the utility of mixed real-synthetic datasets. In this paper, we identify a three-phase scaling behavior characterized by two breakpoints that reflect transitions in model behavior across learning head and tail knowledge. We further derive an LLM generalization bound designed for real and synthetic mixtures, revealing several key factors that govern their generalization performance. Building on our theoretical findings, we propose an effective yet efficient data valuation method that scales to large-scale datasets. Comprehensive experiments across four tasks, including image classification, sentiment classification, instruction following, and complex reasoning, demonstrate that our method surpasses state-of-the-art baselines in data valuation with significantly low computational cost.

</details>


### [442] [FuseSampleAgg: Fused Neighbor Sampling and Aggregation for Mini-batch GNNs](https://arxiv.org/abs/2511.13645)
*Aleksandar Stanković*

Main category: cs.LG

TL;DR: 提出FuseSampleAgg CUDA算子，将邻居采样和均值聚合融合，在多个基准测试上有显著性能提升，代码开源。


<details>
  <summary>Details</summary>
Motivation: 减少GraphSAGE中邻居采样和均值聚合的内存流量和开销。

Method: 提出FuseSampleAgg算子，消除块实例化和额外内核启动，通过保存索引重放保留GraphSAGE均值语义。

Result: 在Reddit、ogbn - arxiv和ogbn - products基准测试上，有最高51倍的步骤时间加速，以及最高100倍、36倍和3.5倍的峰值GPU内存减少。

Conclusion: FuseSampleAgg算子能有效提升GraphSAGE性能，具有确定性，可与标准PyTorch优化器集成。

Abstract: We present FuseSampleAgg, a CUDA operator that fuses neighbor sampling and mean aggregation into a single pass for one and two hop GraphSAGE. By eliminating block materialization and extra kernel launches, FuseSampleAgg reduces memory traffic and overhead while preserving GraphSAGE mean semantics via saved index replay. Across the Reddit, ogbn-arxiv, and ogbn-products benchmarks (batch size 1024, automatic mixed precision enabled), we observe step time speedups up to 51x on ogbn-products, about 4x on Reddit with fanouts 10-10 and 15-10, and about 3.3x on ogbn-arxiv at larger fanouts, with peak GPU memory reductions up to 100x, 36x, and about 3.5x, respectively. The operator is deterministic, integrates with standard PyTorch optimizers, and ships with scripts that reproduce all tables and figures from CSV logs. Code and scripts are available at https://github.com/SV25-22/FuseSampleAgg.

</details>


### [443] [Weight-sparse transformers have interpretable circuits](https://arxiv.org/abs/2511.13653)
*Leo Gao,Achyuta Rajaram,Jacob Coxon,Soham V. Govande,Bowen Baker,Dan Mossing*

Main category: cs.LG

TL;DR: 本文通过约束权重使模型有更易理解的电路，研究模型扩展性，指出稀疏化权衡能力与可解释性，扩大模型规模可改善该边界，还可用于解释现有密集模型。


<details>
  <summary>Details</summary>
Motivation: 在语言模型中寻找人类可理解的电路是机械可解释性领域的核心目标。

Method: 约束大部分权重为零来训练模型，对模型进行剪枝以分离负责特定任务的部分。

Result: 电路包含对应自然概念的神经元和残差通道，有少量易于解释的连接；稀疏化权衡能力与可解释性，扩大模型规模改善能力 - 可解释性边界；方法可用于解释现有密集模型。

Conclusion: 工作产生了前所未有的人类可理解电路，并进行了严格验证，但在保留可解释性的同时扩展稀疏模型仍具挑战。

Abstract: Finding human-understandable circuits in language models is a central goal of the field of mechanistic interpretability. We train models to have more understandable circuits by constraining most of their weights to be zeros, so that each neuron only has a few connections. To recover fine-grained circuits underlying each of several hand-crafted tasks, we prune the models to isolate the part responsible for the task. These circuits often contain neurons and residual channels that correspond to natural concepts, with a small number of straightforwardly interpretable connections between them. We study how these models scale and find that making weights sparser trades off capability for interpretability, and scaling model size improves the capability-interpretability frontier. However, scaling sparse models beyond tens of millions of nonzero parameters while preserving interpretability remains a challenge. In addition to training weight-sparse models de novo, we show preliminary results suggesting our method can also be adapted to explain existing dense models. Our work produces circuits that achieve an unprecedented level of human understandability and validates them with considerable rigor.

</details>


### [444] [Tuning for Two Adversaries: Enhancing the Robustness Against Transfer and Query-Based Attacks using Hyperparameter Tuning](https://arxiv.org/abs/2511.13654)
*Pascal Zimmer,Ghassan Karame*

Main category: cs.LG

TL;DR: 本文首次详细分析优化超参数对不同攻击类型鲁棒性的影响，发现不同攻击下超参数影响差异，探索超参数设计空间以增强鲁棒性，分布式模型调参效果最佳。


<details>
  <summary>Details</summary>
Motivation: 分析优化超参数（如学习率、权重衰减等）对基于转移和基于查询攻击的鲁棒性的影响。

Method: 结合理论和实验，研究多种实际部署场景（集中式训练、集成学习、分布式训练）。

Result: 基于转移攻击中降低学习率可提升64%鲁棒性，基于查询攻击中增加学习率可提升28%鲁棒性；分布式模型调参效果最好，能更有效缓解两种攻击。

Conclusion: 可通过探索优化超参数设计空间联合增强对两种攻击的鲁棒性，分布式模型在超参数调优上有优势。

Abstract: In this paper, we present the first detailed analysis of how optimization hyperparameters -- such as learning rate, weight decay, momentum, and batch size -- influence robustness against both transfer-based and query-based attacks. Supported by theory and experiments, our study spans a variety of practical deployment settings, including centralized training, ensemble learning, and distributed training. We uncover a striking dichotomy: for transfer-based attacks, decreasing the learning rate significantly enhances robustness by up to $64\%$. In contrast, for query-based attacks, increasing the learning rate consistently leads to improved robustness by up to $28\%$ across various settings and data distributions. Leveraging these findings, we explore -- for the first time -- the optimization hyperparameter design space to jointly enhance robustness against both transfer-based and query-based attacks. Our results reveal that distributed models benefit the most from hyperparameter tuning, achieving a remarkable tradeoff by simultaneously mitigating both attack types more effectively than other training setups.

</details>


### [445] [Cross-Learning from Scarce Data via Multi-Task Constrained Optimization](https://arxiv.org/abs/2511.13680)
*Leopoldo Agorio,Juan Cerviño,Miguel Calvo-Fullana,Alejandro Ribeiro,Juan Andrés Bazerque*

Main category: cs.LG

TL;DR: 提出多任务跨学习框架克服数据稀缺问题，联合估计多任务参数，理论验证并经实际应用验证效率。


<details>
  <summary>Details</summary>
Motivation: 解决在数据有限时，学习模型难以泛化的问题。

Method: 引入多任务跨学习框架，将联合估计转化为约束优化问题。

Result: 在高斯数据控制框架下有理论保证，在图像分类和传染病传播等实际应用中展示了方法效率。

Conclusion: 该框架能实现知识从数据丰富任务向稀缺任务转移，提供了从有限数据进行参数推断的解决方案。

Abstract: A learning task, understood as the problem of fitting a parametric model from supervised data, fundamentally requires the dataset to be large enough to be representative of the underlying distribution of the source. When data is limited, the learned models fail generalize to cases not seen during training. This paper introduces a multi-task \emph{cross-learning} framework to overcome data scarcity by jointly estimating \emph{deterministic} parameters across multiple, related tasks. We formulate this joint estimation as a constrained optimization problem, where the constraints dictate the resulting similarity between the parameters of the different models, allowing the estimated parameters to differ across tasks while still combining information from multiple data sources. This framework enables knowledge transfer from tasks with abundant data to those with scarce data, leading to more accurate and reliable parameter estimates, providing a solution for scenarios where parameter inference from limited data is critical. We provide theoretical guarantees in a controlled framework with Gaussian data, and show the efficiency of our cross-learning method in applications with real data including image classification and propagation of infectious diseases.

</details>


### [446] [Protein Secondary Structure Prediction Using 3D Graphs and Relation-Aware Message Passing Transformers](https://arxiv.org/abs/2511.13685)
*Disha Varshney,Samarth Garg,Sarthak Tyagi,Deeksha Varshney,Nayan Deep,Asif Ekbal*

Main category: cs.LG

TL;DR: 本文提出SSRGNet模型，结合GNN和LM预测蛋白质二级结构，实验显示其f1得分超基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法未利用蛋白质3D结构数据，本文旨在利用此数据更好地从蛋白质一级序列预测二级结构。

Method: 利用蛋白质残基图引入连接以获取空间信息，结合GNN和LM，用预训练蛋白质语言模型编码序列，用GCN和R - GCN捕捉结构几何特征，堆叠卷积层学习空间图信息。

Result: 使用NetSurfP - 2.0的训练数据集进行实验，提出的SSRGNet模型在f1得分上超过基线。

Conclusion: 所提出的SSRGNet模型在预测蛋白质二级结构方面表现良好，优于基线。

Abstract: In this study, we tackle the challenging task of predicting secondary structures from protein primary sequences, a pivotal initial stride towards predicting tertiary structures, while yielding crucial insights into protein activity, relationships, and functions. Existing methods often utilize extensive sets of unlabeled amino acid sequences. However, these approaches neither explicitly capture nor harness the accessible protein 3D structural data, which is recognized as a decisive factor in dictating protein functions. To address this, we utilize protein residue graphs and introduce various forms of sequential or structural connections to capture enhanced spatial information. We adeptly combine Graph Neural Networks (GNNs) and Language Models (LMs), specifically utilizing a pre-trained transformer-based protein language model to encode amino acid sequences and employing message-passing mechanisms like GCN and R-GCN to capture geometric characteristics of protein structures. Employing convolution within a specific node's nearby region, including relations, we stack multiple convolutional layers to efficiently learn combined insights from the protein's spatial graph, revealing intricate interconnections and dependencies in its structural arrangement. To assess our model's performance, we employed the training dataset provided by NetSurfP-2.0, which outlines secondary structure in 3-and 8-states. Extensive experiments show that our proposed model, SSRGNet surpasses the baseline on f1-scores.

</details>


### [447] [Learning stochasticity: a nonparametric framework for intrinsic noise estimation](https://arxiv.org/abs/2511.13701)
*Gianluigi Pillonetto,Alberto Giaretta,Mauro Bisiacco*

Main category: cs.LG

TL;DR: 本文介绍了Trine框架，可从时间序列数据中推断状态相关的内在噪声，在生物和生态系统验证中表现良好，为理解复杂系统行为开辟新途径。


<details>
  <summary>Details</summary>
Motivation: 对非线性相互作用和随机效应的不完全了解使自下而上建模方法无效，参数模型在缺乏先验知识时难以估计内在噪声，而随机效应对于理解复杂系统动态行为很重要，因此需要开发新方法。

Method: 引入非参数、基于核的Trine框架，采用三阶段算法，结合解析可解子问题和结构化核架构。

Result: 在生物和生态系统上验证，性能与理想观测者相当。

Conclusion: Trine框架为理解内在噪声如何影响复杂系统行为开辟了新途径。

Abstract: Understanding the principles that govern dynamical systems is a central challenge across many scientific domains, including biology and ecology. Incomplete knowledge of nonlinear interactions and stochastic effects often renders bottom-up modeling approaches ineffective, motivating the development of methods that can discover governing equations directly from data. In such contexts, parametric models often struggle without strong prior knowledge, especially when estimating intrinsic noise. Nonetheless, incorporating stochastic effects is often essential for understanding the dynamic behavior of complex systems such as gene regulatory networks and signaling pathways. To address these challenges, we introduce Trine (Three-phase Regression for INtrinsic noisE), a nonparametric, kernel-based framework that infers state-dependent intrinsic noise from time-series data. Trine features a three-stage algorithm that com- bines analytically solvable subproblems with a structured kernel architecture that captures both abrupt noise-driven fluctuations and smooth, state-dependent changes in variance. We validate Trine on biological and ecological systems, demonstrating its ability to uncover hidden dynamics without relying on predefined parametric assumptions. Across several benchmark problems, Trine achieves performance comparable to that of an oracle. Biologically, this oracle can be viewed as an idealized observer capable of directly tracking the random fluctuations in molecular concentrations or reaction events within a cell. The Trine framework thus opens new avenues for understanding how intrinsic noise affects the behavior of complex systems.

</details>


### [448] [ST-ProC: A Graph-Prototypical Framework for Robust Semi-Supervised Travel Mode Identification](https://arxiv.org/abs/2511.13702)
*Luyao Niu,Nuoxian Huang*

Main category: cs.LG

TL;DR: 提出ST - ProC框架解决GPS轨迹出行模式识别中标签稀缺问题，性能显著优于基线。


<details>
  <summary>Details</summary>
Motivation: GPS轨迹出行模式识别存在标签稀缺问题，现有半监督学习方法不适用，有灾难性确认偏差且忽略数据内在流形。

Method: 提出ST - ProC框架，结合图原型核心与基础半监督学习支持，核心通过图正则化、原型锚定和新的伪标签策略处理数据，基础部分用对比和师生一致性损失确保表示质量和优化稳定性。

Result: ST - ProC显著优于所有基线，比FixMatch等先进方法性能提升21.5%。

Conclusion: ST - ProC在现实稀疏标签场景中有效。

Abstract: Travel mode identification (TMI) from GPS trajectories is critical for urban intelligence, but is hampered by the high cost of annotation, leading to severe label scarcity. Prevailing semi-supervised learning (SSL) methods are ill-suited for this task, as they suffer from catastrophic confirmation bias and ignore the intrinsic data manifold. We propose ST-ProC, a novel graph-prototypical multi-objective SSL framework to address these limitations. Our framework synergizes a graph-prototypical core with foundational SSL Support. The core exploits the data manifold via graph regularization, prototypical anchoring, and a novel, margin-aware pseudo-labeling strategy to actively reject noise. This core is supported and stabilized by foundational contrastive and teacher-student consistency losses, ensuring high-quality representations and robust optimization. ST-ProC outperforms all baselines by a significant margin, demonstrating its efficacy in real-world sparse-label settings, with a performance boost of 21.5% over state-of-the-art methods like FixMatch.

</details>


### [449] [Rare Genomic Subtype Discovery from RNA-seq via Autoencoder Embeddings and Stability-Aware Clustering](https://arxiv.org/abs/2511.13705)
*Alaa Mezghiche*

Main category: cs.LG

TL;DR: 结合自编码器表示、聚类和稳定性分析，在RNA - seq数据中搜索罕见基因组亚型，泛癌聚类受组织来源主导，针对KIRC的方法发现了罕见且可重复的亚型。


<details>
  <summary>Details</summary>
Motivation: 在高维RNA - seq数据的无监督学习中发现超越标准标签的分子亚型。

Method: 结合自编码器表示、聚类和稳定性分析，在泛癌数据集上做对照分析，在KIRC中选择高变基因，训练前馈自编码器，用k - means聚类，通过预定义规则扫描k值。

Result: 泛癌分析中聚类与组织来源高度一致；KIRC分析在k = 5时得到解决方案，有一个占比6.85%的罕见且高度稳定的簇C0，还识别出了相关标记。

Conclusion: 泛癌聚类受组织来源主导，稳定性感知的癌症内分析方法能揭示罕见且可重复的KIRC亚型。

Abstract: Unsupervised learning on high-dimensional RNA-seq data can reveal molecular subtypes beyond standard labels. We combine an autoencoder-based representation with clustering and stability analysis to search for rare but reproducible genomic subtypes. On the UCI "Gene Expression Cancer RNA-Seq" dataset (801 samples, 20,531 genes; BRCA, COAD, KIRC, LUAD, PRAD), a pan-cancer analysis shows clusters aligning almost perfectly with tissue of origin (Cramer's V = 0.887), serving as a negative control. We therefore reframe the problem within KIRC (n = 146): we select the top 2,000 highly variable genes, standardize them, train a feed-forward autoencoder (128-dimensional latent space), and run k-means for k = 2-10. While global indices favor small k, scanning k with a pre-specified discovery rule (rare < 10 percent and stable with Jaccard >= 0.60 across 20 seeds after Hungarian alignment) yields a simple solution at k = 5 (silhouette = 0.129, DBI = 2.045) with a rare cluster C0 (6.85 percent of patients) that is highly stable (Jaccard = 0.787). Cluster-vs-rest differential expression (Welch's t-test, Benjamini-Hochberg FDR) identifies coherent markers. Overall, pan-cancer clustering is dominated by tissue of origin, whereas a stability-aware within-cancer approach reveals a rare, reproducible KIRC subtype.

</details>


### [450] [From Black Box to Insight: Explainable AI for Extreme Event Preparedness](https://arxiv.org/abs/2511.13712)
*Kiana Vu,İsmet Selçuk Özer,Phung Lai,Zheng Wu,Thilanka Munasinghe,Jennifer Wei*

Main category: cs.LG

TL;DR: 本文以野火预测为例，研究可解释AI在极端事件预测中连接预测准确性和可操作见解的作用，展示了XAI的优势并强调AI系统需兼具准确性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 气候变化使极端事件预测需求紧迫，而AI模型因黑箱特性在实际决策中应用受限，需可解释AI来弥合预测准确性和可操作见解间的差距。

Method: 以野火预测为案例研究，评估各类AI模型，采用SHAP方法揭示关键特征、决策路径和潜在偏差，还提供可视化支持。

Result: XAI不仅能阐明模型推理过程，还支持专家和响应团队决策，可视化增强了XAI输出的可解释性。

Conclusion: AI系统不仅要准确，还需具备可解释性、可访问性和可信度，对灾害准备等规划至关重要。

Abstract: As climate change accelerates the frequency and severity of extreme events such as wildfires, the need for accurate, explainable, and actionable forecasting becomes increasingly urgent. While artificial intelligence (AI) models have shown promise in predicting such events, their adoption in real-world decision-making remains limited due to their black-box nature, which limits trust, explainability, and operational readiness. This paper investigates the role of explainable AI (XAI) in bridging the gap between predictive accuracy and actionable insight for extreme event forecasting. Using wildfire prediction as a case study, we evaluate various AI models and employ SHapley Additive exPlanations (SHAP) to uncover key features, decision pathways, and potential biases in model behavior. Our analysis demonstrates how XAI not only clarifies model reasoning but also supports critical decision-making by domain experts and response teams. In addition, we provide supporting visualizations that enhance the interpretability of XAI outputs by contextualizing feature importance and temporal patterns in seasonality and geospatial characteristics. This approach enhances the usability of AI explanations for practitioners and policymakers. Our findings highlight the need for AI systems that are not only accurate but also interpretable, accessible, and trustworthy, essential for effective use in disaster preparedness, risk mitigation, and climate resilience planning.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [451] [Phase-Coded Memory and Morphological Resonance: A Next-Generation Retrieval-Augmented Generator Architecture](https://arxiv.org/abs/2511.11848)
*Denis V. Saklakov*

Main category: cs.NE

TL;DR: 介绍一种认知检索增强生成器（RAG）架构，可突破Transformer上下文长度限制，有显著能耗、存储和时间节省。


<details>
  <summary>Details</summary>
Motivation: 突破Transformer上下文长度限制，减少内存和计算开销。

Method: 采用相位编码记忆和形态 - 语义共振，将意义编码为复波模式，设计三层架构。

Result: 消除顺序令牌依赖，减少内存和计算开销，实现基于频率的语义访问和无限有效上下文。

Conclusion: 该架构能带来能量、存储和时间上的显著节省。

Abstract: This paper introduces a cognitive Retrieval-Augmented Generator (RAG) architecture that transcends transformer context-length limitations through phase-coded memory and morphological-semantic resonance. Instead of token embeddings, the system encodes meaning as complex wave patterns with amplitude-phase structure. A three-tier design is presented: a Morphological Mapper that transforms inputs into semantic waveforms, a Field Memory Layer that stores knowledge as distributed holographic traces and retrieves it via phase interference, and a Non-Contextual Generator that produces coherent output guided by resonance rather than fixed context. This approach eliminates sequential token dependence, greatly reduces memory and computational overhead, and enables unlimited effective context through frequency-based semantic access. The paper outlines theoretical foundations, pseudocode implementation, and experimental evidence from related complex-valued neural models, emphasizing substantial energy, storage, and time savings.

</details>


### [452] [Benchmarking that Matters: Rethinking Benchmarking for Practical Impact](https://arxiv.org/abs/2511.12264)
*Anna V. Kononova,Niki van Stein,Olaf Mersmann,Thomas Bäck,Thomas Bartz-Beielstein,Tobias Glasmachers,Michael Hellwig,Sebastian Krey,Jakub Kůdela,Boris Naujoks,Leonard Papenmeier,Elena Raponi,Quentin Renau,Jeroen Rook,Lennart Schäpermeier,Diederick Vermetten,Daniela Zaharie*

Main category: cs.NE

TL;DR: 当前进化计算基准测试实践不能满足现实需求，指出差距并提出以真实问题为灵感的基准测试愿景。


<details>
  <summary>Details</summary>
Motivation: 当前基准测试实践与现实需求脱节，影响竞赛、算法选择和工业决策。

Method: 识别当前基准测试实践和工具的关键差距，提出以真实问题为灵感的基准、特征空间和性能数据库的愿景。

Result: 无明确具体结果，而是提出了愿景。

Conclusion: 需要一个能随现实见解发展、支持科研和工业应用的动态基准测试生态系统。

Abstract: Benchmarking has driven scientific progress in Evolutionary Computation, yet current practices fall short of real-world needs. Widely used synthetic suites such as BBOB and CEC isolate algorithmic phenomena but poorly reflect the structure, constraints, and information limitations of continuous and mixed-integer optimization problems in practice. This disconnect leads to the misuse of benchmarking suites for competitions, automated algorithm selection, and industrial decision-making, despite these suites being designed for different purposes.
  We identify key gaps in current benchmarking practices and tooling, including limited availability of real-world-inspired problems, missing high-level features, and challenges in multi-objective and noisy settings. We propose a vision centered on curated real-world-inspired benchmarks, practitioner-accessible feature spaces and community-maintained performance databases. Real progress requires coordinated effort: A living benchmarking ecosystem that evolves with real-world insights and supports both scientific understanding and industrial use.

</details>


### [453] [Random-Key Metaheuristic and Linearization for the Quadratic Multiple Constraints Variable-Sized Bin Packing Problem](https://arxiv.org/abs/2511.12367)
*Natalia A. Santos,Marlon Jeske,Antonio A. Chaves*

Main category: cs.NE

TL;DR: 本文针对QMC - VSBPP问题提出两种互补方法，线性化模型得更优下界，RKO - ACO算法改进已知解，为未来研究提供参考。


<details>
  <summary>Details</summary>
Motivation: 解决具有挑战性的QMC - VSBPP组合优化问题，推动该问题求解技术发展。

Method: 一是引入线性化数学公式消除二次项以使用精确求解器计算下界；二是开发RKO - ACO算法，结合随机键优化框架、自适应Q学习参数控制和高效局部搜索。

Result: 线性化模型产生更紧下界，RKO - ACO算法匹配或改进已知解，为大规模实例建立新上界。

Conclusion: 结果为未来研究提供参考值，证明进化和随机键元启发式方法解决复杂二次打包问题有效。

Abstract: This paper addresses the Quadratic Multiple Constraints Variable-Sized Bin Packing Problem (QMC-VSBPP), a challenging combinatorial optimization problem that generalizes the classical bin packing by incorporating multiple capacity dimensions, heterogeneous bin types, and quadratic interaction costs between items. We propose two complementary methods that advance the current state-of-the-art. First, a linearized mathematical formulation is introduced to eliminate quadratic terms, enabling the use of exact solvers such as Gurobi to compute strong lower bounds - reported here for the first time for this problem. Second, we develop RKO-ACO, a continuous-domain Ant Colony Optimization algorithm within the Random-Key Optimization framework, enhanced with adaptive Q-learning parameter control and efficient local search. Extensive computational experiments on benchmark instances show that the proposed linearized model produces significantly tighter lower bounds than the original quadratic formulation, while RKO-ACO consistently matches or improves upon all best-known solutions in the literature, establishing new upper bounds for large-scale instances. These results provide new reference values for future studies and demonstrate the effectiveness of evolutionary and random-key metaheuristic approaches for solving complex quadratic packing problems. Source code and data available at https://github.com/nataliaalves03/RKO-ACO

</details>


### [454] [Evolving Prompts for Toxicity Search in Large Language Models](https://arxiv.org/abs/2511.12487)
*Onkar Shelar,Travis Desell*

Main category: cs.NE

TL;DR: 提出黑盒进化框架ToxSearch测试大语言模型安全，分析算子行为，发现小扰动可用于红队测试，防御应考虑跨模型对抗提示复用。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在安全对齐后仍易受对抗性提示影响产生有毒内容，需测试模型安全性。

Method: 提出ToxSearch框架，通过同步稳态循环进化提示，使用多种算子，由审核预言机提供适应度指导。

Result: 算子行为表现不同，使用在LLaMA 3.1 8B上进化的精英提示，有跨模型转移但毒性减弱，部分模型有不同抗性和毒性表现。

Conclusion: 小的可控扰动可用于系统红队测试，防御应考虑跨模型复用对抗提示而非仅针对单模型强化。

Abstract: Large Language Models remain vulnerable to adversarial prompts that elicit toxic content even after safety alignment. We present ToxSearch, a black-box evolutionary framework that tests model safety by evolving prompts in a synchronous steady-state loop. The system employs a diverse set of operators, including lexical substitutions, negation, back-translation, paraphrasing, and two semantic crossover operators, while a moderation oracle provides fitness guidance. Operator-level analysis shows heterogeneous behavior: lexical substitutions offer the best yield-variance trade-off, semantic-similarity crossover acts as a precise low-throughput inserter, and global rewrites exhibit high variance with elevated refusal costs. Using elite prompts evolved on LLaMA 3.1 8B, we observe practically meaningful but attenuated cross-model transfer, with toxicity roughly halving on most targets, smaller LLaMA 3.2 variants showing the strongest resistance, and some cross-architecture models retaining higher toxicity. These results suggest that small, controllable perturbations are effective vehicles for systematic red-teaming and that defenses should anticipate cross-model reuse of adversarial prompts rather than focusing only on single-model hardening.

</details>


### [455] [On Counts and Densities of Homogeneous Bent Functions: An Evolutionary Approach](https://arxiv.org/abs/2511.12652)
*Claude Carlet,Marko Ðurasevic,Domagoj Jakobovic,Luca Mariot,Stjepan Picek,Alexandr Polujan*

Main category: cs.NE

TL;DR: 本文探讨用进化算法生成齐次bent布尔函数，引入密度概念助力设计，找到不同变量数的二次和三次bent函数。


<details>
  <summary>Details</summary>
Motivation: 具有强密码学性质的布尔函数对密码系统安全重要，研究用进化算法生成齐次bent布尔函数。

Method: 使用进化算法（EAs），引入齐次bent函数的密度概念辅助算法设计。

Result: 找到不同变量数的二次和三次bent函数。

Conclusion: 通过引入密度概念，利用进化算法可有效设计齐次bent布尔函数。

Abstract: Boolean functions with strong cryptographic properties, such as high nonlinearity and algebraic degree, are important for the security of stream and block ciphers. These functions can be designed using algebraic constructions or metaheuristics. This paper examines the use of Evolutionary Algorithms (EAs) to evolve homogeneous bent Boolean functions, that is, functions whose algebraic normal form contains only monomials of the same degree and that are maximally nonlinear. We introduce the notion of density of homogeneous bent functions, facilitating the algorithmic design that results in finding quadratic and cubic bent functions in different numbers of variables.

</details>


### [456] [DS-ATGO: Dual-Stage Synergistic Learning via Forward Adaptive Threshold and Backward Gradient Optimization for Spiking Neural Networks](https://arxiv.org/abs/2511.13050)
*Jiaqiang Jiang,Wenfeng Xu,Jing Fan,Rui Yan*

Main category: cs.NE

TL;DR: 提出双阶段协同学习算法优化SNN训练，实验显示性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 直接训练SNN时，神经元膜电位分布变化导致固定阈值和SG下的尖峰放电不平衡和梯度信号减弱，影响SNN性能。

Method: 提出双阶段协同学习算法，前向传播基于膜电位动态分布自适应调整阈值，反向传播动态优化SG。

Result: 方法实现显著性能提升，使神经元在各时间步稳定放电，增加深层有梯度的神经元比例。

Conclusion: 所提算法能有效解决SNN训练中的问题，提升性能。

Abstract: Brain-inspired spiking neural networks (SNNs) are recognized as a promising avenue for achieving efficient, low-energy neuromorphic computing. Direct training of SNNs typically relies on surrogate gradient (SG) learning to estimate derivatives of non-differentiable spiking activity. However, during training, the distribution of neuronal membrane potentials varies across timesteps and progressively deviates toward both sides of the firing threshold. When the firing threshold and SG remain fixed, this may lead to imbalanced spike firing and diminished gradient signals, preventing SNNs from performing well. To address these issues, we propose a novel dual-stage synergistic learning algorithm that achieves forward adaptive thresholding and backward dynamic SG. In forward propagation, we adaptively adjust thresholds based on the distribution of membrane potential dynamics (MPD) at each timestep, which enriches neuronal diversity and effectively balances firing rates across timesteps and layers. In backward propagation, drawing from the underlying association between MPD, threshold, and SG, we dynamically optimize SG to enhance gradient estimation through spatio-temporal alignment, effectively mitigating gradient information loss. Experimental results demonstrate that our method achieves significant performance improvements. Moreover, it allows neurons to fire stable proportions of spikes at each timestep and increases the proportion of neurons that obtain gradients in deeper layers.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [457] [Large-scale Multigrid with Adaptive Galerkin Coarsening](https://arxiv.org/abs/2511.13109)
*Fabian Böhm,Nils Kohl,Harald Köstler,Ulrich Rüde*

Main category: cs.PF

TL;DR: 提出针对强变系数PDE的无矩阵几何多重网格粗网格校正方案，在基准问题上验证其效率。


<details>
  <summary>Details</summary>
Motivation: 为强变系数PDE的无矩阵几何多重网格方法设计鲁棒、自适应的粗网格校正方案。

Method: 将底层网格均匀几何粗化与异构粗网格算子结合，在系数梯度大的区域局部应用Galerkin粗网格近似，其他区域使用轻量级直接粗网格近似。

Result: 在广义Stokes方程的一系列基准问题上进行了演示，分析量化了求解器内存消耗，在大规模问题上展示了效率。

Conclusion: 该方法能在最小化内存需求的同时保持鲁棒收敛，适用于大规模强变系数PDE问题。

Abstract: We propose a robust, adaptive coarse-grid correction scheme for matrix-free geometric multigrid targeting PDEs with strongly varying coefficients. The method combines uniform geometric coarsening of the underlying grid with heterogeneous coarse-grid operators: Galerkin coarse grid approximation is applied locally in regions with large coefficient gradients, while lightweight, direct coarse grid approximation is used elsewhere. This selective application ensures that local Galerkin operators are computed and stored only where necessary, minimizing memory requirements while maintaining robust convergence. We demonstrate the method on a suite of sinker benchmark problems for the generalized Stokes equation, including grid-aligned and unaligned viscosity jumps, smoothly varying viscosity functions with large gradients, and different viscosity evaluation techniques. We analytically quantify the solver's memory consumption and demonstrate its efficiency by solving Stokes problems with $10^{10}$ degrees of freedom, viscosity jumps of $10^{6}$ magnitude, and more than 100{,}000 parallel processes.

</details>


### [458] [Evaluation of Domain-Specific Architectures for General-Purpose Applications in Apple Silicon](https://arxiv.org/abs/2511.13450)
*Álvaro Corrochano López,Carlos García Sánchez*

Main category: cs.PF

TL;DR: 研究评估苹果神经引擎（ANE）用于通用HPC应用的潜力，结果显示ANE经算法适配后有竞争力性能与高能源效率。


<details>
  <summary>Details</summary>
Motivation: 受GPGPU将GPU用于通用任务启发，探究如NPUs这类专用加速器在新场景复制类似现象的可能性，评估ANE用于通用HPC应用的潜力。

Method: 在苹果M1和M4架构的ANE上评估经典HPC算法（如GEMM、Jacobi或Multigrid方法）的性能和能耗。

Result: 算法适配后，ANE有竞争力的性能（M4 - Pro上GEMM运算达3.8 TFlops，接近同SoC上GPU的4.7 TFlops），且能源效率显著更高（M4架构中GEMM在ANE上功耗5.2瓦，GPU为24瓦）。

Conclusion: ANE在通用HPC应用中经算法适配后能实现有竞争力的性能和高能源效率。

Abstract: The rise of AI and its growing computational demands have driven the integration of domain-specific accelerators (such as GPUs, TPUs, and NPUs) across the entire computing infrastructure. Following the precedent set by the GPGPU which popularized GPUs for general-purpose tasks, this research asks whether this phenomenon can be replicated with specialized accelerators like NPUs in new contexts. This paper evaluates the potential of the Apple Neural Engine (ANE) designed for high energy efficiency in Machine Learning workloads, in the context of general-purpose HPC applications. We evaluate the performance and energy consumption of classic HPC algorithms such as GEMM, Jacobi or Multigrid methods on Apple's ANE across the M1 and the latest M4 architectures. Results confirm that, when algorithms are properly adapted, the ANE achieves competitive performance (up to 3.8 TFlops on the M4-Pro, comparable to the GPU's 4.7 TFlops on the same SoC for GEMM operation) while demonstrating significantly superior energy efficiency (e.g., GEMM consumes 5.2 Watts on the ANE versus 24 Watts on GPU counterpart in M4 architectures).

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [459] [WITNESS: A lightweight and practical approach to fine-grained predictive mutation testing](https://arxiv.org/abs/2511.11999)
*Zeyu Lu,Peng Zhang,Chun Yong Chong,Shan Gao,Yibiao Yang,Yanhui Li,Lin Chen,Yuming Zhou*

Main category: cs.SE

TL;DR: 现有细粒度预测性变异测试研究依赖深度学习存在计算成本高和适用性受限问题，提出WITNESS方法，采用轻量级机器学习模型，评估显示其有出色性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于深度学习的细粒度预测性变异测试计算成本高和适用性受限问题。

Method: 提出WITNESS方法，收集方法内外变异体特征，采用轻量级经典机器学习模型训练和预测。

Result: 在Defects4J项目评估中，WITNESS达到了最先进的预测性能，显著提高了杀伤矩阵预测效率，基于预测杀伤矩阵的测试用例优先级排序结果优于基线方法。

Conclusion: WITNESS方法有效，能解决现有方法的不足，具有良好的性能和应用价值。

Abstract: Existing fine-grained predictive mutation testing studies predominantly rely on deep learning, which faces two critical limitations in practice: (1) Exorbitant computational costs. The deep learning models adopted in these studies demand significant computational resources for training and inference acceleration. This introduces high costs and undermines the cost-reduction goal of predictive mutation testing. (2) Constrained applicability. Although modern mutation testing tools generate mutants both inside and outside methods, current fine-grained predictive mutation testing approaches handle only inside-method mutants. As a result, they cannot predict outside-method mutants, limiting their applicability in real-world scenarios. We propose WITNESS, a new fine-grained predictive mutation testing approach. WITNESS adopts a twofold design: (1) With collected features from both inside-method and outside-method mutants, WITNESS is suitable for all generated mutants. (2) Instead of using computationally expensive deep learning, WITNESS employs lightweight classical machine learning models for training and prediction. This makes it more cost-effective and enabling straightforward explanations of the decision-making processes behind the adopted models. Evaluations on Defects4J projects show that WITNESS consistently achieves state-of-the-art predictive performance across different scenarios. Additionally, WITNESS significantly enhances the efficiency of kill matrix prediction. Post-hoc analysis reveals that features incorporating information from before and after the mutation are the most important among those used in WITNESS. Test case prioritization based on the predicted kill matrix shows that WITNESS delivers results much closer to those obtained by using the actual kill matrix, outperforming baseline approaches.

</details>


### [460] [A Code Smell Refactoring Approach using GNNs](https://arxiv.org/abs/2511.12069)
*HanYu Zhang,Tomoji Kishi*

Main category: cs.SE

TL;DR: 文章指出代码异味重构现有技术有局限，提出基于图的深度学习方法，设计输入图、采用分类任务，用半自动化方法生成数据集，实验表明该方法性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有代码异味重构技术（基于指标、规则和深度学习）存在依赖手动定义、受数据集和模型设计限制等问题。

Method: 设计类级和方法级输入图，采用图分类和节点分类任务处理三种代码异味；提出半自动化数据集生成方法；用GCN、GraphSAGE和GAT三种GNN架构实现该方法。

Result: 所提方法在实验中相比传统和先进的深度学习方法，实现了更优的重构性能。

Conclusion: 基于图的深度学习方法在代码异味重构方面表现出色，有应用价值。

Abstract: Code smell is a great challenge in software refactoring, which indicates latent design or implementation flaws that may degrade the software maintainability and evolution. Over the past decades, a variety of refactoring approaches have been proposed, which can be broadly classified into metrics-based, rule-based, and machine learning-based approaches. Recent years, deep learning-based approaches have also attracted widespread attention. However, existing techniques exhibit various limitations. Metrics- and rule-based approaches rely heavily on manually defined heuristics and thresholds, whereas deep learning-based approaches are often constrained by dataset availability and model design. In this study, we proposed a graph-based deep learning approach for code smell refactoring. Specifically, we designed two types of input graphs (class-level and method-level) and employed both graph classification and node classification tasks to address the refactoring of three representative code smells: long method, large class, and feature envy. In our experiment, we propose a semi-automated dataset generation approach that could generate a large-scale dataset with minimal manual effort. We implemented the proposed approach with three classical GNN (graph neural network) architectures: GCN, GraphSAGE, and GAT, and evaluated its performance against both traditional and state-of-the-art deep learning approaches. The results demonstrate that proposed approach achieves superior refactoring performance.

</details>


### [461] [Actionable Warning Is Not Enough: Recommending Valid Actionable Warnings with Weak Supervision](https://arxiv.org/abs/2511.12229)
*Zhipeng Xue,Zhipeng Gao,Tongtong Xu,Xing Hu,Xin Xia,Shanping Li*

Main category: cs.SE

TL;DR: 现有静态分析工具误报率高，此前行动性警告假设不准确。本文构建大型行动性警告数据集，为警告分配弱标签，提出ACWRecommender框架推荐高概率真实漏洞警告，实验表明模型性能优于基线，工具实用。


<details>
  <summary>Details</summary>
Motivation: 当前静态分析工具广泛应用受高误报率阻碍，且现有行动性警告收集假设不准确，需解决该问题。

Method: 从Top - 500 GitHub C仓库挖掘68,274个回退构建数据集，为警告分配弱标签；提出ACWRecommender两阶段框架，包括粗粒度检测和细粒度重排序。

Result: 模型在nDCG和MRR指标上大幅优于基线；在6个项目上运行工具，手动检查发现27个推荐警告被开发者确认为真实漏洞。

Conclusion: 提出的模型和工具能有效帮助开发者在大量报告警告中快速找到真实漏洞，具有实际应用价值。

Abstract: The use of static analysis tools has gained increasing popularity among developers in the last few years. However, the widespread adoption of static analysis tools is hindered by their high false alarm rates. Previous studies have introduced the concept of actionable warnings and built a machine-learning method to distinguish actionable warnings from false alarms. However, according to our empirical observation, the current assumption used for actionable warning(s) collection is rather shaky and inaccurate, leading to a large number of invalid actionable warnings. To address this problem, in this study, we build the first large actionable warning dataset by mining 68,274 reversions from Top-500 GitHub C repositories, we then take one step further by assigning each actionable warning a weak label regarding its likelihood of being a real bug. Following that, we propose a two-stage framework called ACWRecommender to automatically recommend the actionable warnings with high probability to be real bugs (AWHB). Our approach warms up the pre-trained model UniXcoder by identifying actionable warnings task (coarse-grained detection stage) and rerank AWHB to the top by weakly supervised learning (fine-grained reranking stage). Experimental results show that our proposed model outperforms several baselines by a large margin in terms of nDCG and MRR for AWHB recommendation. Moreover, we ran our tool on 6 randomly selected projects and manually checked the top-ranked warnings from 2,197 reported warnings, we reported top-10 recommended warnings to developers, 27 of them were already confirmed by developers as real bugs. Developers can quickly find real bugs among the massive amount of reported warnings, which verifies the practical usage of our tool.

</details>


### [462] [Reflections on the design, applications and implementations of the normative specification language eFLINT](https://arxiv.org/abs/2511.12276)
*L. Thomas van Binsbergen,Christopher A. Esterhuyse,Tim Müller*

Main category: cs.SE

TL;DR: 本文探讨自动化合规检查难题，介绍 eFLINT 语言设计并反思其现状，为自动化合规领域语言开发者提供成果与见解。


<details>
  <summary>Details</summary>
Motivation: 软件合规检查重要且成本高，自动化合规面临诸多困难，需探索解决方案。

Method: 反思领域特定软件语言 eFLINT，结合其应用、需求和设计决策进行分析。

Result: 对 eFLINT 语言当前设计进行反思，得出相关结果和见解。

Conclusion: 研究成果能让自动化合规领域的语言开发者受益。

Abstract: Checking the compliance of software against laws, regulations and contracts is increasingly important and costly as the embedding of software into societal practices is getting more pervasive. Moreover, the digitalised services provided by governmental organisations and companies are governed by an increasing amount of laws and regulations, requiring highly adaptable compliance practices. A potential solution is to automate compliance using software. However, automating compliance is difficult for various reasons. Legal practices involve subjective processes such as interpretation and qualification. New laws and regulations come into effect regularly and laws and regulations, as well as their interpretations, are subjected to constant revision. In addition, computational reasoning with laws requires a cross-disciplinary process involving both legal and software expertise.
  This paper reflects on the domain-specific software language eFLINT developed to experiment with novel solutions. The language combines declarative and procedural elements to reason about situations and scenarios respectively, explicates and formalises connections between legal concepts and computational concepts, and is designed to automate compliance checks both before, during and after a software system runs. The various goals and applications areas for the language give rise to (conflicting) requirements. This paper reflects on the current design of the language by recalling various applications, the requirements they imposed, and subsequent design decisions. As such, this paper reports on results and insights of an investigation that can benefit language developers within the field of automated compliance.

</details>


### [463] [Reducing Hallucinations in LLM-Generated Code via Semantic Triangulation](https://arxiv.org/abs/2511.12288)
*Yihan Dai,Sijie Liang,Haotian Xu,Peichu Xie,Sergey Mechtaev*

Main category: cs.SE

TL;DR: 现有代码生成样本共识技术有局限，本文提出语义三角剖分方法，在基准测试中提升代码生成可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有样本共识技术在低采样概率或多有效非等价解时，难以选对解或弃权。

Method: 引入语义三角剖分，对编程问题进行转换，验证跨转换问题的一致性。

Result: 在基准测试中，相比概率阈值0.5选高置信解的方法，语义三角剖分使生成代码可靠性提升21%，能识别低至0.14采样概率的正确解，且能在多有效非等价解任务中形成真正共识。

Conclusion: 语义三角剖分能提高样本共识和弃权的可靠性，使生成程序更准确。

Abstract: When generating code from natural language prompts, an LLM samples programs from a probability distribution, many of which might be incorrect. Sample consensus techniques - such as majority voting or validation against generated tests or specifications - aim to identify a correct program in the sample or abstain if none is valid. However, existing methods often fail to select a correct solution when its sampling probability is low, or when the problem permits multiple valid but non-equivalent solutions. Additionally, they often fail to abstain when no correct solution is present in the sample. To overcome these limitations, we introduce semantic triangulation, which transforms a programming problem in a way that non-trivially alters its semantics while preserving an exact, verifiable mapping between solutions before and after transformation. We theoretically establish that verifying consistency across such problem transformations increases confidence that generated programs reflect accurate generalization rather than spurious statistical correlations, enabling more reliable sample consensus and abstention. On the LiveCodeBench and CodeElo benchmarks, using GPT-4o and DeepSeek-V3 models, semantic triangulation increases reliability of generated code by 21% compared to the method that selects only high-confidence solutions with the probability threshold 0.5, while being able to pinpoint correct solutions at sampling probabilities as low as 0.14. Apart from that, it is also the only approach to consistently form true consensus on tasks with multiple valid but non-equivalent solutions.

</details>


### [464] [ProofWright: Towards Agentic Formal Verification of CUDA](https://arxiv.org/abs/2511.12294)
*Bodhisatwa Chatterjee,Drew Zagieboylo,Sana Damani,Siva Hari,Christos Kozyrakis*

Main category: cs.SE

TL;DR: 提出ProofWright框架，集成自动形式验证与LLM代码生成，为LLM生成的CUDA内核提供安全保证，验证效率较高且开销适中。


<details>
  <summary>Details</summary>
Motivation: LLM生成的CUDA内核有正确性错误且缺乏形式安全保证，运行时测试不可靠，手动形式验证无法匹配LLM输出速度，存在验证瓶颈。

Method: 提出ProofWright代理验证框架，集成自动形式验证与LLM代码生成。

Result: 在KernelBench L1上，ProofWright能为74%的生成内核验证安全属性，发现传统测试遗漏的错误，建立一类逐元素内核的语义等价性。

Conclusion: 可扩展的LLM生成GPU代码自动形式验证是可行的，能在不牺牲开发效率的前提下实现可信的高性能代码生成。

Abstract: Large Language Models (LLMs) are increasingly used to automatically generate optimized CUDA kernels, substantially improving developer productivity. However, despite rapid generation, these kernels often contain subtle correctness bugs and lack formal safety guarantees. Runtime testing is inherently unreliable - limited input coverage and reward hacking can mask incorrect behavior - while manual formal verification is reliable but cannot scale to match LLM output rates, creating a critical validation bottleneck.
  We present ProofWright, an agentic verification framework that bridges this gap by integrating automated formal verification with LLM-based code generation. ProofWright provides end-to-end guarantees of memory safety, thread safety, and semantic correctness for LLM-generated CUDA kernels. On KernelBench L1, ProofWright verifies safety properties for 74% of generated kernels, uncovers subtle correctness errors missed by conventional testing, and establishes semantic equivalence for a class of element-wise kernels. With a modest overhead of 3 minutes per kernel, ProofWright demonstrates that scalable, automated formal verification of LLM-generated GPU code is feasible - offering a path toward trustworthy high-performance code generation without sacrificing developer productivity.

</details>


### [465] [High-level reasoning while low-level actuation in Cyber-Physical Systems: How efficient is it?](https://arxiv.org/abs/2511.12543)
*Burak Karaduman,Baris Tekin Tezel,Moharram Challenger*

Main category: cs.SE

TL;DR: 研究测量比较六种语言和框架的最坏情况执行时间和开发时间，为工业信息化选软件技术提供循证指导。


<details>
  <summary>Details</summary>
Motivation: 工业信息集成系统复杂度增加，工程师缺乏选择先进工业应用工具的实证依据。

Method: 采用以开发者为中心的方法，测量比较六种语言和框架的最坏情况执行时间和开发时间，研究抽象水平和推理能力对开发工作量和运行时行为的影响。

Result: 发现抽象和推理机制影响系统性能和开发者生产力。

Conclusion: 为工业信息化选择软件技术提供循证指导，支持提高集成效率、可维护性和响应能力，为未来相关研究奠定基础。

Abstract: The increasing complexity of industrial information-integration systems demands software technologies that enable intelligent behaviour, real-time response, and efficient development. Although many programming languages and frameworks exist, engineers still lack sufficient empirical evidence to guide the choice of tools for advanced industrial applications. This study addresses that need by measuring and comparing worst-case execution time (WCET) and development time across six languages and frameworks: C++, Java, Jade, Jason, and fuzzy Jason BDI with both loosely and tightly coupled integration. These technologies reflect a progression from procedural and object-oriented programming to agent-based frameworks capable of symbolic and fuzzy reasoning.
  Rather than relying on broad concepts such as paradigms or orientations, the study adopts a developer-centred approach grounded in measurable outcomes. The structured comparison examines how rising abstraction levels and reasoning capabilities affect both development effort and runtime behaviour. By analysing these dimensions, the study highlights concrete trade-offs between engineering workload and execution efficiency.
  The findings show how abstraction and reasoning mechanisms shape system performance and developer productivity, offering practical insight for designing intelligent, agent-based solutions that must operate under real-time constraints and complex decision-making requirements. Overall, the study contributes evidence-based guidance for selecting software technologies in industrial informatization, supporting improved integration efficiency, maintainability, and responsiveness, and laying groundwork for future research on the interplay between language features, development dynamics, and runtime behaviour in cyber-physical and smart manufacturing systems.

</details>


### [466] [Can Small GenAI Language Models Rival Large Language Models in Understanding Application Behavior?](https://arxiv.org/abs/2511.12576)
*Mohammad Meymani,Hamed Jelodar,Parisa Hamedi,Roozbeh Razavi-Far,Ali A. Ghorbani*

Main category: cs.SE

TL;DR: 研究系统评估大小生成式AI模型理解应用行为能力，小模型在计算效率等有优势，可补充大模型。


<details>
  <summary>Details</summary>
Motivation: 评估大小生成式AI语言模型理解应用行为的能力，以恶意软件检测为代表任务。

Method: 对大小生成式AI模型在理解应用行为方面进行系统评估，比较准确率、精确率、召回率和F1分数等指标。

Result: 大模型整体准确率高，小模型在精确率和召回率有竞争力，在计算效率、推理速度和资源受限环境部署方面有优势。

Conclusion: 小模型可有效补充大模型，在实际应用行为分析中平衡性能和资源效率。

Abstract: Generative AI (GenAI) models, particularly large language models (LLMs), have transformed multiple domains, including natural language processing, software analysis, and code understanding. Their ability to analyze and generate code has enabled applications such as source code summarization, behavior analysis, and malware detection. In this study, we systematically evaluate the capabilities of both small and large GenAI language models in understanding application behavior, with a particular focus on malware detection as a representative task. While larger models generally achieve higher overall accuracy, our experiments show that small GenAI models maintain competitive precision and recall, offering substantial advantages in computational efficiency, faster inference, and deployment in resource-constrained environments. We provide a detailed comparison across metrics such as accuracy, precision, recall, and F1-score, highlighting each model's strengths, limitations, and operational feasibility. Our findings demonstrate that small GenAI models can effectively complement large ones, providing a practical balance between performance and resource efficiency in real-world application behavior analysis.

</details>


### [467] [LLM4SCREENLIT: Recommendations on Assessing the Performance of Large Language Models for Screening Literature in Systematic Reviews](https://arxiv.org/abs/2511.12635)
*Lech Madeyski,Barbara Kitchenham,Martin Shepperd*

Main category: cs.SE

TL;DR: 本文指出评估大语言模型筛选文献性能的挑战，分析问题与良好实践并给出建议。


<details>
  <summary>Details</summary>
Motivation: 大语言模型发布速度超用户严格评估能力，在支撑研究时需严格评估其性能。

Method: 以大规模研究为例，分析传统指标问题，研究27篇相关论文，提取性能指标。

Result: 发现评估存在未用稳健指标、未考虑证据丢失影响、未完整报告混淆矩阵等问题，也提取了良好实践。

Conclusion: 提出筛选评估应优先考虑召回率、使用WMCC指标、报告完整矩阵等建议。

Abstract: Context: Large language models (LLMs) are released faster than users' ability to evaluate them rigorously. When LLMs underpin research, such as identifying relevant literature for systematic reviews (SRs), robust empirical assessment is essential. Objective: We identify and discuss key challenges in assessing LLM performance for selecting relevant literature, identify good (evaluation) practices, and propose recommendations. Method: Using a recent large-scale study as an example, we identify problems with the use of traditional metrics for assessing the performance of Gen-AI tools for identifying relevant literature in SRs. We analyzed 27 additional papers investigating this issue, extracted the performance metrics, and found both good practices and widespread problems, especially with the use and reporting of performance metrics for SR screening. Results: Major weaknesses included: i) a failure to use metrics that are robust to imbalanced data and do not directly indicate whether results are better than chance, e.g., the use of Accuracy, ii) a failure to consider the impact of lost evidence when making claims concerning workload savings, and iii) pervasive failure to report the full confusion matrix (or performance metrics from which it can be reconstructed) which is essential for future meta-analyses. On the positive side, we extract good (evaluation) practices on which our recommendations for researchers and practitioners, as well as policymakers, are built. Conclusions: SR screening evaluations should prioritize lost evidence/recall alongside chance-anchored and cost-sensitive Weighted MCC (WMCC) metric, report complete confusion matrices, treat unclassifiable outputs as referred-back positives for assessment, adopt leakage-aware designs with non-LLM baselines and open artifacts, and ground conclusions in cost-benefit analysis where FNs carry higher penalties than FPs.

</details>


### [468] [Enhancing LLM Code Generation Capabilities through Test-Driven Development and Code Interpreter](https://arxiv.org/abs/2511.12823)
*Sajed Jalil,Shuvo Saha,Hossain Mohammad Seym*

Main category: cs.SE

TL;DR: 本文旨在让资源受限的新兴市场用户能以母语使用代码生成工具，提出结合TDD和CI的新方法，提高了孟加拉语提示代码生成的准确率。


<details>
  <summary>Details</summary>
Motivation: 提升大语言模型代码生成能力是NLP研究重点，但孟加拉语在训练大语言模型时受关注少，现有技术对终端用户使用要求高，希望让资源受限的新兴市场用户以母语使用强大代码生成工具。

Method: 引入结合测试驱动开发（TDD）和代码解释器（CI）的新方法，利用开放权重模型。

Result: 提高了孟加拉语提示代码生成的基线准确率，达到85%，最小模型与最大模型相比准确率可达98%，结果公开在GitHub。

Conclusion: 该方法无需微调，能让资源受限市场用户以母语使用代码生成工具，小模型也能有高准确率。

Abstract: Over the past few years, improving LLM code generation capabilities has been a key focus in NLP research. Despite Bengali having 242 million native speakers worldwide, it receives little attention when it comes to training LLMs. More recently, various fine-tuning and augmented generation techniques have been employed to significantly enhance code generation performance. However, they require considerable expertise and resources to utilize effectively as an end user. The goal of our work is to democratize access to powerful code generation tools in resource-constrained emerging markets, enabling users to leverage them in their native language.
  We introduce a novel approach that combines Test-Driven Development (TDD) and Code Interpreter (CI), utilizing open-weight models, which improves the baseline accuracy for code generation with Bengali prompts and achieves an overall accuracy of 85%. Our approach requires no finetuning and proves that even the smallest models in the same family can attain up to 98% accuracy compared to the largest models. All of our results are publicly shared in GitHub for validation and reproducibility.

</details>


### [469] [Human-Centred Requirements Engineering for Critical Systems: Insights from Disaster Early Warning Applications](https://arxiv.org/abs/2511.12856)
*Anuradha Madugalla,Jixuan Dong,Kai Lyne Loi,Matthew Crossman,John Grundy*

Main category: cs.SE

TL;DR: 本文提出以人为中心的需求工程（RE）流程，将社会责任融入关键系统开发，通过设计预警系统原型验证，表明早期处理以人为主的需求可提升系统可用性和可及性。


<details>
  <summary>Details</summary>
Motivation: 传统关键系统需求工程多关注技术保证，忽视系统运行的人类和社会背景，因此需考虑以人为中心的方面以确保系统的安全性和可靠性。

Method: 通过文献回顾确定为弱势群体设计软件的指南，转化为62个功能和非功能需求，设计自适应预警系统原型，并通过6次访谈和8次认知走查进行评估。

Result: 早期处理以人为主的需求能提升所有用户对系统的可用性和可及性。

Conclusion: 以人为本并非道德附加品，而是安全公平的关键系统的决定性品质。

Abstract: Critical systems, such as those used in healthcare, defence, and disaster management, demand rigorous requirements engineering to ensure safety and reliability. Yet, much of this rigour has traditionally focused on technical assurance, often overlooking the human and social contexts in which these systems operate. This paper argues that considering human-centric aspects is an essential dimension of dependability, and presents a human-centred RE process designed to integrate social responsibility into critical system development. Drawing from a literature review, we identified a set of guidelines for designing software for vulnerable communities and translated these into sixty-two functional and non-functional requirements. These requirements were operationalised through the design of an adaptive early warning system prototype, which was subsequently evaluated through six interviews and eight cognitive walkthroughs to validate their relevance and applicability. The findings demonstrate that human-centric requirements, when addressed early, enhance the usability and accessibility of systems for all users. The paper concludes by positioning human-centricity not as an ethical add-on but as a defining quality of safe and equitable critical systems.

</details>


### [470] [Agent READMEs: An Empirical Study of Context Files for Agentic Coding](https://arxiv.org/abs/2511.12884)
*Worawalan Chatlatanagulchai,Hao Li,Yutaro Kashiwa,Brittany Reid,Kundjanasith Thonglek,Pattara Leelaprute,Arnon Rungsawang,Bundit Manaskasemsak,Bram Adams,Ahmed E. Hassan,Hajimu Iida*

Main category: cs.SE

TL;DR: 对1925个仓库的2303个代理上下文文件进行大规模实证研究，发现文件复杂且难读，开发者重功能轻非功能需求，需改进工具和实践。


<details>
  <summary>Details</summary>
Motivation: 对代理上下文文件的结构、维护和内容进行大规模实证研究。

Method: 对1925个仓库的2303个代理上下文文件进行研究，并对16种指令类型进行内容分析。

Result: 文件复杂难读，像配置代码一样演变；开发者优先考虑功能上下文，非功能需求很少指定。

Conclusion: 开发者用上下文文件使代理具备功能，但缺少保障代码安全和性能的措施，需改进工具和实践。

Abstract: Agentic coding tools receive goals written in natural language as input, break them down into specific tasks, and write or execute the actual code with minimal human intervention. Central to this process are agent context files ("READMEs for agents") that provide persistent, project-level instructions. In this paper, we conduct the first large-scale empirical study of 2,303 agent context files from 1,925 repositories to characterize their structure, maintenance, and content. We find that these files are not static documentation but complex, difficult-to-read artifacts that evolve like configuration code, maintained through frequent, small additions. Our content analysis of 16 instruction types shows that developers prioritize functional context, such as build and run commands (62.3%), implementation details (69.9%), and architecture (67.7%). We also identify a significant gap: non-functional requirements like security (14.5%) and performance (14.5%) are rarely specified. These findings indicate that while developers use context files to make agents functional, they provide few guardrails to ensure that agent-written code is secure or performant, highlighting the need for improved tooling and practices.

</details>


### [471] [Diffploit: Facilitating Cross-Version Exploit Migration for Open Source Library Vulnerabilities](https://arxiv.org/abs/2511.12950)
*Zirui Chen,Zhipeng Xue,Jiayuan Zhou,Xing Hu,Xin Xia,Xiaohu Yang*

Main category: cs.SE

TL;DR: 提出Diffploit方法解决利用程序版本迁移问题，在大量数据上评估效果优异，还发现CVE报告问题和未报告的易受攻击版本。


<details>
  <summary>Details</summary>
Motivation: 现有利用程序版本迁移技术存在处理环境级故障不足、难以应对复杂触发条件变化等问题，需新方法解决。

Method: 提出Diffploit，包含上下文模块和迁移模块，上下文模块分析版本行为差异构建上下文，迁移模块通过迭代反馈循环引导基于LLM的适配。

Result: 在大规模数据集上，Diffploit成功迁移84.2%的利用程序，优于TARGET和IDEA；识别出5个CVE报告版本范围错误，3个已确认；发现111个未报告的易受攻击版本。

Conclusion: Diffploit在利用程序版本迁移方面有效，能提升漏洞检测和版本管理的准确性。

Abstract: Exploits are commonly used to demonstrate the presence of library vulnerabilities and validate their impact across different versions. However, their direct application to alternative versions often fails due to breaking changes introduced during evolution. These failures stem from both changes in triggering conditions (e.g., API refactorings) and broken dynamic environments (e.g., build or runtime errors), which are challenging to interpret and adapt manually. Existing techniques primarily focus on code-level trace alignment through fuzzing, which is both time-consuming and insufficient for handling environment-level failures. Moreover, they often fall short when dealing with complicated triggering condition changes across versions. To overcome this, we propose Diffploit, an iterative, diff-driven exploit migration method structured around two key modules: the Context Module and the Migration Module. The Context Module dynamically constructs contexts derived from analyzing behavioral discrepancies between the target and reference versions, which capture the failure symptom and its related diff hunks. Leveraging these contexts, the Migration Module guides an LLM-based adaptation through an iterative feedback loop, balancing exploration of diff candidates and gradual refinement to resolve reproduction failures effectively. We evaluate Diffploit on a large-scale dataset containing 102 Java CVEs and 689 version-migration tasks across 79 libraries. Diffploit successfully migrates 84.2% exploits, outperforming the change-aware test repair tool TARGET by 52.0% and the rule-based tool in IDEA by 61.6%. Beyond technical effectiveness, Diffploit identifies 5 CVE reports with incorrect affected version ranges, three of which have been confirmed. It also discovers 111 unreported vulnerable versions in the GitHub Advisory Database.

</details>


### [472] [SmartPoC: Generating Executable and Validated PoCs for Smart Contract Bug Reports](https://arxiv.org/abs/2511.12993)
*Longfei Chen,Ruibin Yan,Taiyu Wong,Yiyang Chen,Chao Zhang*

Main category: cs.SE

TL;DR: 本文提出SmartPoC框架，将审计报告转为可执行的有效测试用例，在基准测试表现良好，还能确认真实漏洞。


<details>
  <summary>Details</summary>
Motivation: 智能合约审计工件缺乏可重现、可执行的PoC测试，导致手动验证成本高，且利用大语言模型转换存在挑战。

Method: 处理输入审计报告降噪，提取与漏洞相关函数作为上下文，利用大语言模型合成PoC测试用例并进行预/后执行修复，利用差异验证作为预言机。

Result: 在SmartBugs - Vul和FORGE - Vul基准测试中分别为85.61%和86.45%的目标生成可执行、有效的测试用例，在Etherscan语料库中确认236个真实漏洞，每个发现成本仅0.03美元。

Conclusion: SmartPoC框架能有效将文本审计报告转换为可执行、经过验证的测试用例，可用于确认智能合约中的真实漏洞。

Abstract: Smart contracts are prone to vulnerabilities and are analyzed by experts as well as automated systems, such as static analysis and AI-assisted solutions. However, audit artifacts are heterogeneous and often lack reproducible, executable PoC tests suitable for automated validation, leading to costly, ad hoc manual verification. Large language models (LLMs) can be leveraged to turn audit reports into PoC test cases, but have three major challenges: noisy inputs, hallucinations, and missing runtime oracles. In this paper, we present SmartPoC, an automated framework that converts textual audit reports into executable, validated test cases. First, the input audit report is processed to reduce noise, and only bug-related functions are extracted and fed to LLMs as context. To curb hallucinations and ensure compile-and-run readiness, we leverage LLMs to synthesize PoC test cases with specially-designed pre-/post-execution repair. We further utilize differential verification as oracles to confirm exploitability of the PoC test cases. On the SmartBugs-Vul and FORGE-Vul benchmarks, SmartPoC generates executable, validated Foundry test cases for 85.61% and 86.45% of targets, respectively. Applied to the latest Etherscan verified-source corpus, SmartPoC confirms 236 real bugs out of 545 audit findings at a cost of only $0.03 per finding.

</details>


### [473] [Towards Requirements Engineering for GenAI-Enabled Software: Bridging Responsibility Gaps through Human Oversight Requirements](https://arxiv.org/abs/2511.13069)
*Zhenyu Mao,Jacky Keung,Yicheng Sun,Yifei Wang,Shuo Liu,Jialong Li*

Main category: cs.SE

TL;DR: 研究分析生成式人工智能软件系统中责任缺口的研究空白，提出设计方法并经用户研究验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能软件中责任缺口问题愈发明显，现有需求工程方法难以应对，存在研究空白，需系统性分析。

Method: 设计方法分概念化、方法论、工件三层，概念层定义责任要素及缺口成因，方法论层用演绎流程识别缺口并推导监督需求，工件层将结果形式化。

Result: 用户研究对比显示，所提方法在六个维度有明显改进，能有效解决三个研究空白。

Conclusion: 所提设计方法能有效应对生成式人工智能软件中责任缺口相关研究空白问题。

Abstract: Context: Responsibility gaps, long-recognized challenges in socio-technical systems where accountability becomes diffuse or ambiguous, have become increasingly pronounced in GenAI-enabled software. The generative and adaptive nature complicates how human oversight and responsibility are specified, delegated, and traced. Existing requirements engineering (RE) approaches remain limited in addressing these phenomena, revealing conceptual, methodological, and artifact-level research gaps.. Objective: This study aims to analyze these research gaps in the context of GenAI-enabled software systems. It seeks to establish a coherent perspective for a systematic analysis of responsibility gaps from a human oversight requirements standpoint, encompassing how these responsibility gaps should be conceptualized, identified, and represented throughout the RE process. Methods: The proposed design methodology is structured across three analytical layers. At the conceptualization layer, it establishes a conceptual framing that defines the key elements of responsibility across the human and system dimensions and explains how potential responsibility gaps emerge from their interactions. At the methodological layer, it introduces a deductive pipeline for identifying responsibility gaps by analyzing interactions between these dimensions and deriving corresponding oversight requirements within established RE frameworks. At the artifact layer, it formalizes the results in a Deductive Backbone Table, a reusable representation that traces the reasoning path from responsibility gaps identification to human oversight requirements derivation. Results: A user study compared the proposed methodology with a baseline goal-oriented RE across two scenarios. Evaluation across six dimensions indicated clear improvements of the proposed methodology, confirming its effectiveness in addressing three research gaps.

</details>


### [474] [Examining the Usage of Generative AI Models in Student Learning Activities for Software Programming](https://arxiv.org/abs/2511.13271)
*Rufeng Chen,Shuaishuai Jiang,Jiyun Shen,AJung Moon,Lili Wei*

Main category: cs.SE

TL;DR: 研究对比GenAI辅助与传统在线资源对不同编程水平学生知识获取的影响，发现GenAI助解题但不保证知识获取，呼吁合理使用。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注GenAI完成教育任务能力和对学生成绩影响，忽略其对知识获取的作用，故开展研究。

Method: 对24名不同编程水平本科生进行对照实验，分析任务表现、概念理解和交互行为。

Result: GenAI生成完整解决方案可提升任务表现，尤其对初学者，但不保证知识获取；不同水平学生使用策略有别；过度或极少使用GenAI知识获取效果差。

Conclusion: 呼吁学生和教育者将GenAI作为学习工具，强调融入编程教育时需指导。

Abstract: The rise of Generative AI (GenAI) tools like ChatGPT has created new opportunities and challenges for computing education. Existing research has primarily focused on GenAI's ability to complete educational tasks and its impact on student performance, often overlooking its effects on knowledge gains. In this study, we investigate how GenAI assistance compares to conventional online resources in supporting knowledge gains across different proficiency levels. We conducted a controlled user experiment with 24 undergraduate students of two different levels of programming experience (beginner, intermediate) to examine how students interact with ChatGPT while solving programming tasks. We analyzed task performance, conceptual understanding, and interaction behaviors. Our findings reveal that generating complete solutions with GenAI significantly improves task performance, especially for beginners, but does not consistently result in knowledge gains. Importantly, usage strategies differ by experience: beginners tend to rely heavily on GenAI toward task completion often without knowledge gain in the process, while intermediates adopt more selective approaches. We find that both over-reliance and minimal use result in weaker knowledge gains overall. Based on our results, we call on students and educators to adopt GenAI as a learning rather than a problem solving tool. Our study highlights the urgent need for guidance when integrating GenAI into programming education to foster deeper understanding.

</details>


### [475] [SAINT: Service-level Integration Test Generation with Program Analysis and LLM-based Agents](https://arxiv.org/abs/2511.13305)
*Rangeet Pan,Raju Pavuluri,Ruikai Huang,Rahul Krishna,Tyler Stennett,Alessandro Orso,Saurabh SInha*

Main category: cs.SE

TL;DR: 提出SAINT白盒测试方法用于企业Java应用服务层测试，结合静态分析、大语言模型和基于LLM的智能体自动生成测试用例，评估显示其有效。


<details>
  <summary>Details</summary>
Motivation: 现有服务层测试工具在测试RESTful API时存在依赖OpenAPI规范、难以生成有效功能测试用例等问题。

Method: 构建端点模型和操作依赖图，使用基于LLM的智能体生成端点和基于场景的测试用例。

Result: 在八个Java应用上评估显示SAINT在覆盖率、故障检测和场景生成方面有效，开发者调查认可其生成的基于场景的测试用例。

Conclusion: 结合静态分析和智能体LLM工作流能实现更有效、功能更完善且符合开发者需求的服务层测试用例生成。

Abstract: Enterprise applications are typically tested at multiple levels, with service-level testing playing an important role in validating application functionality. Existing service-level testing tools, especially for RESTful APIs, often employ fuzzing and/or depend on OpenAPI specifications which are not readily available in real-world enterprise codebases. Moreover, these tools are limited in their ability to generate functional tests that effectively exercise meaningful scenarios. In this work, we present SAINT, a novel white-box testing approach for service-level testing of enterprise Java applications. SAINT combines static analysis, large language models (LLMs), and LLM-based agents to automatically generate endpoint and scenario-based tests. The approach builds two key models: an endpoint model, capturing syntactic and semantic information about service endpoints, and an operation dependency graph, capturing inter-endpoint ordering constraints. SAINT then employs LLM-based agents to generate tests. Endpoint-focused tests aim to maximize code and database interaction coverage. Scenario-based tests are synthesized by extracting application use cases from code and refining them into executable tests via planning, action, and reflection phases of the agentic loop. We evaluated SAINT on eight Java applications, including a proprietary enterprise application. Our results illustrate the effectiveness of SAINT in coverage, fault detection, and scenario generation. Moreover, a developer survey provides strong endorsement of the scenario-based tests generated by SAINT. Overall, our work shows that combining static analysis with agentic LLM workflows enables more effective, functional, and developer-aligned service-level test generation.

</details>


### [476] [LinkXplore: A Framework for Affordable High-Quality Blockchain Data](https://arxiv.org/abs/2511.13318)
*Peihao Li*

Main category: cs.SE

TL;DR: 提出开源框架LinkXplore用于收集和管理链上数据，降低成本且能灵活集成数据，代码和数据集公开。


<details>
  <summary>Details</summary>
Motivation: 大规模区块链数据收集成本高，缺乏分析链上数据的系统框架，阻碍学术研究和产品开发。

Method: 引入LinkXplore框架，让用户绕过昂贵数据提供商，直接分析RPC查询或流中的原始数据，通过简单API和后端处理逻辑集成数据。

Result: 能够以较低成本提供高质量区块链数据，可集成任何类型的链上数据。

Conclusion: LinkXplore是预算有限的研究人员和开发者的实用选择。

Abstract: Blockchain technologies are rapidly transforming both academia and industry. However, large-scale blockchain data collection remains prohibitively expensive, as many RPC providers only offer enhanced APIs with high pricing tiers that are unsuitable for budget-constrained research or industrial-scale applications, which has significantly slowed down academic studies and product development. Moreover, there is a clear lack of a systematic framework that allows flexible integration of new modules for analyzing on-chain data.
  To address these challenges, we introduce LinkXplore, the first open framework for collecting and managing on-chain data. LinkXplore enables users to bypass costly blockchain data providers by directly analyzing raw data from RPC queries or streams, thereby offering high-quality blockchain data at a fraction of the cost. Through a simple API and backend processing logic, any type of chain data can be integrated into the framework. This makes it a practical alternative for both researchers and developers with limited budgets. Code and dataset used in this project are publicly available at https://github.com/Linkis-Project/LinkXplore

</details>


### [477] [An LLM-based Quantitative Framework for Evaluating High-Stealthy Backdoor Risks in OSS Supply Chains](https://arxiv.org/abs/2511.13341)
*Zihe Yan,Kai Luo,Haoyu Yang,Yang Yu,Zhuosheng Zhang,Guancheng Li*

Main category: cs.SE

TL;DR: 针对开源软件供应链安全问题，提出细粒度项目评估框架并在Debian生态系统评估，发现存在多种安全风险。


<details>
  <summary>Details</summary>
Motivation: 开源软件作为第三方依赖普遍使用，但底层依赖缺乏维护和社区审计不足，面临高隐蔽后门攻击等安全挑战。

Method: 从攻击者角度建模隐蔽后门攻击，为各攻击阶段定义指标；用大语言模型对代码仓库进行语义评估，不依赖手工模式。

Result: 在Debian生态系统的66个高优先级包上评估框架，结果显示当前开源软件供应链面临多种安全风险。

Conclusion: 提出的框架可用于评估开源软件后门风险，且当前开源软件供应链存在安全隐患。

Abstract: In modern software development workflows, the open-source software supply chain contributes significantly to efficient and convenient engineering practices. With increasing system complexity, using open-source software as third-party dependencies has become a common practice. However, the lack of maintenance for underlying dependencies and insufficient community auditing create challenges in ensuring source code security and the legitimacy of repository maintainers, especially under high-stealthy backdoor attacks exemplified by the XZ-Util incident. To address these problems, we propose a fine-grained project evaluation framework for backdoor risk assessment in open-source software. The framework models stealthy backdoor attacks from the viewpoint of the attacker and defines targeted metrics for each attack stage. In addition, to overcome the limitations of static analysis in assessing the reliability of repository maintenance activities such as irregular committer privilege escalation and limited participation in reviews, the framework uses large language models (LLMs) to conduct semantic evaluation of code repositories without relying on manually crafted patterns. The framework is evaluated on sixty six high-priority packages in the Debian ecosystem. The experimental results indicate that the current open-source software supply chain is exposed to various security risks.

</details>


### [478] [FLOWER: Flow-Oriented Entity-Relationship Tool](https://arxiv.org/abs/2511.13357)
*Dmitry Moskalev*

Main category: cs.SE

TL;DR: 本文提出面向流程的实体关系工具FLOWER，能解决数据处理、创建和可视化问题，在基准测试中表现优于其他方法，支持多语言和多种设备。


<details>
  <summary>Details</summary>
Motivation: 探索跨数据源关系对实体识别很重要，但构建实体关系模型受人为因素影响，需更好的解决方案。

Method: 提出FLOWER工具，利用动态采样和强大数据分析技术检测内置约束并创建必要约束。

Result: 在STATS基准测试中，FLOWER在分布表示、约束学习等方面优于对比方法，在数据故事讲述上也有优势，支持23种语言，兼容CPU和GPU。

Conclusion: FLOWER能更好地处理现实世界数据，确保质量、可扩展性和适用性。

Abstract: Exploring relationships across data sources is a crucial optimization for entities recognition. Since databases can store big amount of information with synthetic and organic data, serving all quantity of objects correctly is an important task to deal with. However, the decision of how to construct entity relationship model is associated with human factor. In this paper, we present flow-oriented entity-relationship tool. This is first and unique end-to-end solution that eliminates routine and resource-intensive problems of processing, creating and visualizing both of explicit and implicit dependencies for prominent SQL dialects on-the-fly. Once launched, FLOWER automatically detects built-in constraints and starting to create own correct and necessary one using dynamic sampling and robust data analysis techniques. This approach applies to improve entity-relationship model and data storytelling to better understand the foundation of data and get unseen insights from DB sources using SQL or natural language. Evaluated on state-of-the-art STATS benchmark, experiments show that FLOWER is superior to reservoir sampling by 2.4x for distribution representation and 2.6x for constraint learning with 2.15x acceleration. For data storytelling, our tool archives 1.19x for accuracy enhance with 1.86x context decrease compare to LLM. Presented tool is also support 23 languages and compatible with both of CPU and GPU. Those results show that FLOWER can manage with real-world data a way better to ensure with quality, scalability and applicability for different use-cases.

</details>


### [479] [BIOMERO 2.0: end-to-end FAIR infrastructure for bioimaging data import, analysis, and provenance](https://arxiv.org/abs/2511.13611)
*Torec T. Luik,Joost de Folter,Rodrigo Rosas-Bertolini,Eric A. J. Reits,Ron A. Hoebe,Przemek M. Krawczyk*

Main category: cs.SE

TL;DR: 介绍BIOMERO 2.0，它将OMERO转变为符合FAIR原则、具备溯源功能的生物成像平台，通过插件和容器化组件集成多环节，增强了OMERO的FAIR化。


<details>
  <summary>Details</summary>
Motivation: 将OMERO转变为符合FAIR原则、具备溯源功能的生物成像平台，支持可追溯、可复用的图像分析工作流。

Method: 通过OMERO.web插件和容器化组件集成数据导入、预处理、分析和工作流监控；导入子系统利用容器化预处理和表单进行原地导入和元数据丰富；分析子系统通过BIOMERO Python库在高性能计算系统上协调和跟踪容器化分析；记录所有操作的参数、版本和结果。

Result: 实现了所有导入和分析的参数、版本和结果的记录，可通过集成仪表盘实时获取溯源信息。

Conclusion: 这种双管齐下的方法将OMERO置于生物成像分析过程的核心，增强了其FAIR化，弥合了数据导入、分析和共享之间的差距。

Abstract: We present BIOMERO 2.0, a major evolution of the BIOMERO framework that transforms OMERO into a FAIR-compliant (findable, accessible, interoperable, and reusable), provenance-aware bioimaging platform. BIOMERO 2.0 integrates data import, preprocessing, analysis, and workflow monitoring through an OMERO.web plugin and containerized components. The importer subsystem facilitates in-place import using containerized preprocessing and metadata enrichment via forms, while the analyzer subsystem coordinates and tracks containerized analyses on high-performance computing systems via the BIOMERO Python library. All imports and analyses are recorded with parameters, versions, and results, ensuring real-time provenance accessible through integrated dashboards. This dual approach places OMERO at the heart of the bioimaging analysis process: the importer ensures provenance from image acquisition through preprocessing and import into OMERO, while the analyzer records it for downstream processing. These integrated layers enhance OMEROs FAIRification, supporting traceable, reusable workflows for image analysis that bridge the gap between data import, analysis, and sharing.

</details>


### [480] [Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?](https://arxiv.org/abs/2511.13646)
*Chunqiu Steven Xia,Zhe Wang,Yan Yang,Yuxiang Wei,Lingming Zhang*

Main category: cs.SE

TL;DR: 本文提出首个实时软件代理Live - SWE - agent，可在运行时自主持续进化，在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM软件代理需专门设计且可能不是最优的，自我改进的代理需要昂贵的离线训练且泛化性不佳，因此需要更好的解决方案。

Method: 提出Live - SWE - agent，从仅能访问bash工具的基本代理支架开始，在解决实际软件问题时自主进化其支架实现。

Result: 在SWE - bench Verified基准测试中实现75.4%的解决率，在SWE - Bench Pro基准测试中实现45.8%的解决率，优于现有开源和手动设计的软件代理。

Conclusion: Live - SWE - agent能在运行时自主持续进化，解决实际软件问题，且在多个基准测试中表现良好。

Abstract: Large Language Models (LLMs) are reshaping almost all industries, including software engineering. In recent years, a number of LLM agents have been proposed to solve real-world software problems. Such software agents are typically equipped with a suite of coding tools and can autonomously decide the next actions to form complete trajectories to solve end-to-end software tasks. While promising, they typically require dedicated design and may still be suboptimal, since it can be extremely challenging and costly to exhaust the entire agent scaffold design space. Recognizing that software agents are inherently software themselves that can be further refined/modified, researchers have proposed a number of self-improving software agents recently, including the Darwin-Gödel Machine (DGM). Meanwhile, such self-improving agents require costly offline training on specific benchmarks and may not generalize well across different LLMs or benchmarks. In this paper, we propose Live-SWE-agent, the first live software agent that can autonomously and continuously evolve itself on-the-fly during runtime when solving real-world software problems. More specifically, Live-SWE-agent starts with the most basic agent scaffold with only access to bash tools (e.g., mini-SWE-agent), and autonomously evolves its own scaffold implementation while solving real-world software problems. Our evaluation on the widely studied SWE-bench Verified benchmark shows that Live-SWE-agent can achieve an impressive solve rate of 75.4% without test-time scaling, outperforming all existing open-source software agents and approaching the performance of the best proprietary solution. Moreover, Live-SWE-agent outperforms state-of-the-art manually crafted software agents on the recent SWE-Bench Pro benchmark, achieving the best-known solve rate of 45.8%.

</details>


### [481] [What's in a Software Engineering Job Posting?](https://arxiv.org/abs/2511.13656)
*Marvin Wyrich,Lloyd Montgomery*

Main category: cs.SE

TL;DR: 文章探讨软件工程岗位招聘中强调的非技术方面，通过对100份招聘启事的主题分析揭示雇主期望，为相关群体提供见解和趋势记录。


<details>
  <summary>Details</summary>
Motivation: 随着理想软件工程候选人标准的演变，探索招聘中强调的非技术方面，了解雇主的社会技术和组织期望。

Method: 对100份软件工程岗位招聘启事进行主题分析。

Result: 雇主寻求与公司目标一致、适应企业文化、追求个人和职业成长、擅长人际互动的候选人。

Conclusion: 研究有助于软件工程社区讨论工程师角色和工作环境的演变，为研究者、教育者、从业者和招聘者提供见解，记录2023年岗位招聘趋势。

Abstract: A well-rounded software engineer is often defined by technical prowess and the ability to deliver on complex projects. However, the narrative around the ideal Software Engineering (SE) candidate is evolving, suggesting that there is more to the story. This article explores the non-technical aspects emphasized in SE job postings, revealing the sociotechnical and organizational expectations of employers. Our Thematic Analysis of 100 job postings shows that employers seek candidates who align with their sense of purpose, fit within company culture, pursue personal and career growth, and excel in interpersonal interactions. This study contributes to ongoing discussions in the SE community about the evolving role and workplace context of software engineers beyond technical skills. By highlighting these expectations, we provide relevant insights for researchers, educators, practitioners, and recruiters. Additionally, our analysis offers a valuable snapshot of SE job postings in 2023, providing a scientific record of prevailing trends and expectations.

</details>


### [482] [Ontology-Driven Model-to-Model Transformation of Workflow Specifications](https://arxiv.org/abs/2511.13661)
*Francisco Abreu,Luís Cruz,Sérgio Guerreiro*

Main category: cs.SE

TL;DR: 提出本体驱动的模型到模型管道将特定领域工作流定义转换为BPMN 2.0，应用于IST的Smart Forms & Smart Flow，评估成功率94.2%，可推广到其他专有工作流语言。


<details>
  <summary>Details</summary>
Motivation: 专有工作流建模语言阻碍互操作性和重用，解决供应商锁定问题并便于向开放标准迁移。

Method: 管道包含基于RML的JSON到RDF/OWL语义提升、本体对齐和推理、通过Camunda Model API生成BPMN三个阶段，将映射知识外部化到本体和声明性规则。

Result: 为IST的Smart Forms & Smart Flow实例化管道，实现转换器生成符合标准的BPMN图，评估成功率94.2%，失败源于静态JSON中不明确的动态行为和基于时间的转换。

Conclusion: 本体驱动的M2M转换可系统地连接特定领域工作流和标准符号，为利益相关者提供量化性能和定性好处。

Abstract: Proprietary workflow modeling languages such as Smart Forms & Smart Flow hamper interoperability and reuse because they lock process knowledge into closed formats. To address this vendor lock-in and ease migration to open standards, we introduce an ontology-driven model-to-model pipeline that systematically translates domain-specific workflow definitions to Business Process Model and Notation (BPMN) 2.0. The pipeline comprises three phases: RML-based semantic lifting of JSON to RDF/OWL, ontology alignment and reasoning, and BPMN generation via the Camunda Model API. By externalizing mapping knowledge into ontologies and declarative rules rather than code, the approach supports reusability across vendor-specific formats and preserves semantic traceability between source definitions and target BPMN models. We instantiated the pipeline for Instituto Superior Técnico (IST)'s Smart Forms & Smart Flow and implemented a converter that produces standard-compliant BPMN diagrams. Evaluation on a corpus of 69 real-world workflows produced 92 BPMN diagrams with a 94.2% success rate. Failures (5.81%) stemmed from dynamic behaviors and time-based transitions not explicit in the static JSON. Interviews with support and development teams indicated that the resulting diagrams provide a top-down view that improves comprehension, diagnosis and onboarding by exposing implicit control flow and linking tasks and forms back to their sources. The pipeline is generalizable to other proprietary workflow languages by adapting the ontology and mappings, enabling interoperability and reducing vendor dependency while supporting continuous integration and long-term maintainability. The presented case study demonstrates that ontology-driven M2M transformation can systematically bridge domain-specific workflows and standard notations, offering quantifiable performance and qualitative benefits for stakeholders.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [483] [Statistical and economic evaluation of forecasts in electricity markets: beyond RMSE and MAE](https://arxiv.org/abs/2511.13616)
*Katarzyna Maciejowska,Arkadiusz Lipiecki,Bartosz Uniejewski*

Main category: q-fin.CP

TL;DR: 本文探讨预测准确性与经济价值关系，研究电池储能系统充放电决策问题，发现RMSE和MAE与收益弱相关，评估价格曲线一致性的指标更有效。


<details>
  <summary>Details</summary>
Motivation: 现有预测评估指标不能充分反映预测的经济价值，解决电池储能系统基于电价预测的充放电决策问题。

Method: 生成192个预测，用7种统计指标评估，计算统计指标与利润的动态相关性。

Result: RMSE和MAE与收益弱相关，评估预测与实际日价格曲线一致性的指标与盈利能力关系更强。

Conclusion: 评估价格曲线一致性的指标更适合用于选择最优预测。

Abstract: In recent years, a rapid development of forecasting methods has led to an increase in the accuracy of predictions. In the literature, forecasts are typically evaluated using metrics such as Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE). While appropriate for statistical assessment, these measures do not adequately reflect the economic value of forecasts. This study addresses the decision-making problem faced by a battery energy storage system, which must determine optimal charging and discharging times based on day-ahead electricity price forecasts. To explore the relationship between forecast accuracy and economic value, we generate a pool of 192 forecasts. These are evaluated using seven statistical metrics that go beyond RMSE and MAE, capturing various characteristics of the predictions and associated errors. We calculate the dynamic correlation between the statistical measures and gained profits to reveal that both RMSE and MAE are only weakly correlated with revenue. In contrast, measures that assess the alignment between predicted and actual daily price curves have a stronger relationship with profitability and are thus more effective for selecting optimal forecasts.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [484] [On the utility problem in a market where price impact is transient](https://arxiv.org/abs/2511.12093)
*Lóránt Nagy,Miklós Rásonyi*

Main category: q-fin.PM

TL;DR: 研究含瞬态价格影响的离散时间金融市场效用最大化问题，证明有解并去除先前研究限制。


<details>
  <summary>Details</summary>
Motivation: 解决含瞬态价格影响的金融市场效用最大化问题，去除先前研究中对市场深度和弹性过程的不合理限制。

Method: 未提及

Result: 对应效用最大化问题有解，去除了先前研究对市场深度和弹性过程的限制，问题的可达到投资组合价值集可能不满足凸性。

Conclusion: 解决了含瞬态价格影响的金融市场效用最大化问题，改进了先前研究。

Abstract: We consider a discrete-time model of a financial market where a risky asset is bought and sold with transactions having a transient price impact. It is shown that the corresponding utility maximization problem admits a solution. We manage to remove some unnatural restrictions on the market depth and resilience processes that were present in earlier work. A non-standard feature of the problem is that the set of attainable portfolio values may fail the convexity property.

</details>


### [485] [Basis Immunity: Isotropy as a Regularizer for Uncertainty](https://arxiv.org/abs/2511.13334)
*Florent Segonne*

Main category: q-fin.PM

TL;DR: 本文回顾并扩展Agnostic Risk Parity的eigenrisk parity（ERP），提出整合均值 - 方差优化与各向同性约束的框架，分解为规范投资组合，应用于行业趋势跟踪有对冲效果，提供实用工具和概念综合。


<details>
  <summary>Details</summary>
Motivation: 多元化投资组合构建因模型不确定性和估计误差存在挑战，需创新方法应对。

Method: 回顾并扩展ERP的各向同性哲学，提出整合均值 - 方差优化与各向同性约束的框架，该约束作为几何正则化器应对信号不确定性。

Result: 投资组合分配自然分解为规范投资组合，在完全各向同性和纯均值 - 方差间平滑插值；应用于行业趋势跟踪时，各向同性约束能系统地产生负平均信号暴露。

Conclusion: 为信号不确定下的稳健资产配置提供实用且有理论依据的工具，对现代投资组合概念进行教学综合。

Abstract: Diversification is a cornerstone of robust portfolio construction, yet its application remains fraught with challenges due to model uncertainty and estimation errors. Practitioners often rely on sophisticated, proprietary heuristics to navigate these issues. Among recent advancements, Agnostic Risk Parity introduces eigenrisk parity (ERP), an innovative approach that leverages isotropy to evenly allocate risk across eigenmodes, enhancing portfolio stability.
  In this paper, we review and extend the isotropy-enforced philosophy of ERP proposing a versatile framework that integrates mean-variance optimization with an isotropy constraint acting as a geometric regularizer against signal uncertainty. The resulting allocations decompose naturally into canonical portfolios, smoothly interpolating between full isotropy (closed-form isotropic-mean allocation) and pure mean-variance through a tunable isotropy penalty.
  Beyond methodology, we revisit fundamental concepts and clarify foundational links between isotropy, canonical portfolios, principal portfolios, primal versus dual representations, and intrinsic basis-invariant metrics for returns, risk, and isotropy. Applied to sector trend-following, the isotropy constraint systematically induces negative average-signal exposure -- a structural, parameter-robust crash hedge.
  This work offers both a practical, theoretically grounded tool for resilient allocation under signal uncertainty and a pedagogical synthesis of modern portfolio concepts.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [486] [Mean Field Analysis of Mutual Insurance Market](https://arxiv.org/abs/2511.12292)
*Bohan Li,Wenyuan Li,Kenneth Tsz Hin Ng,Sheung Chi Phillip Yam*

Main category: q-fin.RM

TL;DR: 本文对流行的相互保险市场中成员行为进行动态研究，用扩展平均场博弈框架建模，解决战略互动对个体决策影响问题，还开发算法研究相互保险公司设计特征对成员决策和财富的影响。


<details>
  <summary>Details</summary>
Motivation: 相互保险公司占全球保险市场近三分之一，分析其共享机制下成员行为有实践和理论重要性。

Method: 用扩展平均场博弈（MFG）框架对盈余共享机制建模，建立刻画纳什均衡策略的平均场正倒向随机微分方程（MF - FBSDE）的全局存在性和唯一性，开发改进的深度BSDE算法。

Result: 开发的算法可用于研究相互保险公司设计的结构特征，如风险类别组成和盈余共享比例，如何通过集体互动重塑成员决策和财富。

Conclusion: 强调了这些机制在相互保险公司中的核心作用。

Abstract: A mutual insurance company (MIC) is a type of consumer cooperative owned by its policyholders. By purchasing insurance from an MIC, policyholders effectively become member-owners of the company and are entitled to a share of the surplus, which is determined by their own collective claims and premium contributions. This sharing mechanism creates an interactive environment in which individual insurance strategies are influenced by the actions of others. Given that mutual insurers account for nearly one-third of the global insurance market, the analysis of members' behavior under such a sharing mechanism is of both practical and theoretical importance. This article presents a first dynamic study of members' behavior in the prevalent mutual insurance market under the large-population limit. With members' wealth processes depending on the law of the insurance strategies, we model the surplus-sharing mechanism using an extended mean field game (MFG) framework and address the fundamental question of how strategic interactions in this setting influence individual decisions. Mathematically, we establish the global-in-time existence and uniqueness of the mean field forward-backward stochastic differential equation (MF-FBSDE) characterizing the Nash equilibrium strategy, employing techniques to accommodate realistic insurance constraints. Computationally, we develop a modified deep BSDE algorithm capable of solving the extended MFG problem with an additional fixed-point structure on the control. Utilizing this scheme, we examine how structural features of the MIC's design, such as the composition of risk classes and surplus-sharing proportions, reshape members' decisions and wealth through collective interactions, underscoring the central role of these mechanisms in MICs.

</details>


### [487] [Sharpening Shapley Allocation: from Basel 2.5 to FRTB](https://arxiv.org/abs/2511.12391)
*Marco Scaringi,Marco Bianchetti*

Main category: q-fin.RM

TL;DR: 本文系统回顾金融领域主要风险分配策略，设测试框架比较其特性与优缺点，开发新方案解决负风险分配等问题，发现Shapley分配策略表现最佳，方法框架具通用性。


<details>
  <summary>Details</summary>
Motivation: 由于风险度量非可加性、金融机构分层结构及多种分配策略，需系统评估主要风险分配策略。

Method: 搭建特定测试框架，包括简化场景和不同风险监管下的实际金融投资组合，开发并测试新解决方案，关注计算方面。

Result: Shapley分配策略在简单性、数学特性、风险表示和计算成本间取得最佳平衡，采用高效蒙特卡罗模拟时计算成本可接受。

Conclusion: 提出的方法论框架具有完全通用性，可应用于多种金融风险场景。

Abstract: Risk allocation, the decomposition of a portfolio-wide risk measure into component contributions, is a fundamental problem in financial risk management due to the non-additive nature of risk measures, the layered organizational structures of financial institutions and the range of possible allocation strategies characterized by different rationales and properties.
  In this work, we conduct a systematic review of the major risk allocation strategies typically used in finance, comparing their theoretical properties, practical advantages, and limitations. To this scope we set up a specific testing framework, including both simplified settings, designed to highlight basic intrinsic behaviours, and realistic financial portfolios under different risk regulations, i.e. Basel 2.5 and FRTB. Furthermore, we develop and test novel practical solutions to manage the issue of negative risk allocations and of multi-level risk allocation in the layered organizational structure of financial institutions, while preserving the additivity property. Finally, we devote particular attention to the computational aspects of risk allocation.
  Our results show that, in this context, the Shapley allocation strategy offers the best compromise between simplicity, mathematical properties, risk representation and computational cost. The latter is still acceptable even in the challenging case of many business units, provided that an efficient Monte Carlo simulation is employed, which offers excellent scaling and convergence properties. While our empirical applications focus on market risk, our methodological framework is fully general and applicable to other financial context such as valuation risk, liquidity risk, credit risk, and counterparty credit risk.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [488] [Impact by design: translating Lead times in flux into an R handbook with code](https://arxiv.org/abs/2511.12763)
*Harrison Katz*

Main category: q-fin.ST

TL;DR: 将Lead times in flux核心思想转化为R语言实用手册，用合成数据实现全流程可复现。


<details>
  <summary>Details</summary>
Motivation: 将原文章的核心思想转化为R语言的实用手册，方便实践应用。

Method: 使用最小数据模式，在R语言中实现原文章思想，提供可运行脚本、模拟示例和预设评估计划。

Result: 使用合成数据得出结果，实现了结果的完全可复现。

Conclusion: 成功将原文章思想在R语言中实现并提供实用手册，结果可复现。

Abstract: This commentary translates the central ideas in Lead times in flux into a practice ready handbook in R. The original article measures change in the full distribution of booking lead times with a normalized L1 distance and tracks that divergence across months relative to year over year and to a fixed 2018 reference. It also provides a bound that links divergence and remaining horizon to the relative error of pickup forecasts. We implement these ideas end to end in R, using a minimal data schema and providing runnable scripts, simulated examples, and a prespecified evaluation plan. All results use synthetic data so the exposition is fully reproducible without reference to proprietary sources.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [489] [Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy](https://arxiv.org/abs/2511.12120)
*Hongyang Yang,Xiao-Yang Liu,Shan Zhong,Anwar Walid*

Main category: q-fin.TR

TL;DR: 提出用深度强化方案学习股票交易策略，集成三种算法，测试显示优于单算法和基线策略，代码开源。


<details>
  <summary>Details</summary>
Motivation: 在复杂动态股市中设计盈利交易策略具有挑战性，需新方法。

Method: 用三种基于演员 - 评论家的算法（PPO、A2C、DDPG）训练深度强化学习代理得到集成策略，用按需加载技术处理大数据。

Result: 在30只道琼斯股票上测试，集成策略在夏普比率衡量的风险调整回报上优于三种单算法和两个基线策略。

Conclusion: 提出的深度集成策略能有效适应不同市场情况，在股票交易中表现更优。

Abstract: Stock trading strategies play a critical role in investment. However, it is challenging to design a profitable strategy in a complex and dynamic stock market. In this paper, we propose an ensemble strategy that employs deep reinforcement schemes to learn a stock trading strategy by maximizing investment return. We train a deep reinforcement learning agent and obtain an ensemble trading strategy using three actor-critic based algorithms: Proximal Policy Optimization (PPO), Advantage Actor Critic (A2C), and Deep Deterministic Policy Gradient (DDPG). The ensemble strategy inherits and integrates the best features of the three algorithms, thereby robustly adjusting to different market situations. In order to avoid the large memory consumption in training networks with continuous action space, we employ a load-on-demand technique for processing very large data. We test our algorithms on the 30 Dow Jones stocks that have adequate liquidity. The performance of the trading agent with different reinforcement learning algorithms is evaluated and compared with both the Dow Jones Industrial Average index and the traditional min-variance portfolio allocation strategy. The proposed deep ensemble strategy is shown to outperform the three individual algorithms and two baselines in terms of the risk-adjusted return measured by the Sharpe ratio. This work is fully open-sourced at \href{https://github.com/AI4Finance-Foundation/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020}{GitHub}.

</details>


### [490] [A Practical Machine Learning Approach for Dynamic Stock Recommendation](https://arxiv.org/abs/2511.12129)
*Hongyang Yang,Xiao-Yang Liu,Qingwei Wu*

Main category: q-fin.TR

TL;DR: 提出用机器学习从标普500推荐股票的方案，动态持有前20%股票，经测试表现优于仅做多策略，代码开源。


<details>
  <summary>Details</summary>
Motivation: 单一选股策略不总能获胜，分析师没时间检查所有标普500股票，需实用选股方案。

Method: 先选代表性指标，用五种机器学习方法在滚动窗口建模，选均方误差最低模型排名，用多种投资组合分配方法测试选股。

Result: 提出的方案在夏普比率和累计回报方面优于标普500指数的仅做多策略。

Conclusion: 所提用机器学习从标普500推荐股票的方案有效。

Abstract: Stock recommendation is vital to investment companies and investors. However, no single stock selection strategy will always win while analysts may not have enough time to check all S&P 500 stocks (the Standard & Poor's 500). In this paper, we propose a practical scheme that recommends stocks from S&P 500 using machine learning. Our basic idea is to buy and hold the top 20% stocks dynamically. First, we select representative stock indicators with good explanatory power. Secondly, we take five frequently used machine learning methods, including linear regression, ridge regression, stepwise regression, random forest and generalized boosted regression, to model stock indicators and quarterly log-return in a rolling window. Thirdly, we choose the model with the lowest Mean Square Error in each period to rank stocks. Finally, we test the selected stocks by conducting portfolio allocation methods such as equally weighted, mean-variance, and minimum-variance. Our empirical results show that the proposed scheme outperforms the long-only strategy on the S&P 500 index in terms of Sharpe ratio and cumulative returns. This work is fully open-sourced at \href{https://github.com/AI4Finance-Foundation/Dynamic-Stock-Recommendation-Machine_Learning-Published-Paper-IEEE}{GitHub}.

</details>


### [491] [Discovery of a 13-Sharpe OOS Factor: Drift Regimes Unlock Hidden Cross-Sectional Predictability](https://arxiv.org/abs/2511.12490)
*Mainak Singha*

Main category: q-fin.TR

TL;DR: 本文提出通过制度条件信号激活实现高夏普比率的横截面股票因子策略，表现远超市场基准，通过多项稳健性测试并给出机制证据和资本容量估计。


<details>
  <summary>Details</summary>
Motivation: 寻找高绩效的横截面股票因子策略。

Method: 结合价值和短期反转信号，仅在股票特定漂移制度下激活信号，利用2004 - 2024年标普500数据进行严格的向前滚动验证。

Result: 该因子实现样本外夏普比率超1.3，年化收益率158.6%，波动率12.0%，最大回撤 - 11.9%，风险调整后表现约为市场基准的13倍，通过多项稳健性测试，对标准风险因子暴露可忽略。

Conclusion: 漂移制度重塑市场微观结构，使横截面价格发现可被系统性利用，保守估计可部署1 - 5亿美元资本。

Abstract: We document a high-performing cross-sectional equity factor that achieves out-of-sample Sharpe ratios above 13 through regime-conditional signal activation. The strategy combines value and short-term reversal signals only during stock-specific drift regimes, defined as periods when individual stocks show more than 60 percent positive days in trailing 63-day windows. Under these conditions, the factor delivers annualized returns of 158.6 percent with 12.0 percent volatility and a maximum drawdown of minus 11.9 percent. Using rigorous walk-forward validation across 20 years of S&P 500 data (2004 to 2024), we show performance roughly 13 times stronger than market benchmarks on a risk-adjusted basis, produced entirely out-of-sample with frozen parameters. The factor passes extensive robustness tests, including 1,000 randomization trials with p-values below 0.001, and maintains Sharpe ratios above 7 even under 30 percent parameter perturbations. Exposure to standard risk factors is negligible, with total R-squared values below 3 percent. We provide mechanistic evidence that drift regimes reshape market microstructure by amplifying behavioral biases, altering liquidity patterns, and creating conditions where cross-sectional price discovery becomes systematically exploitable. Conservative capacity estimates indicate deployable capital of 100 to 500 million dollars before noticeable performance degradation.

</details>


### [492] [Stationary Distributions of the Mode-switching Chiarella Model](https://arxiv.org/abs/2511.13277)
*Jutta G. Kurth,Jean-Philippe Bouchaud*

Main category: q-fin.TR

TL;DR: 推导扩展Chiarella金融市场模型不同状态下的平稳分布，分析不同条件下的定价错误分布和趋势分布形态，反驳文献中一些观点，但最后一种状态精确解未得。


<details>
  <summary>Details</summary>
Motivation: 研究扩展Chiarella金融市场模型不同状态下的平稳分布，深入理解市场动态。

Method: 推导模型的平稳分布，利用Furutsu - Novikov定理分析。

Result: 小噪声、小反馈极限下定价错误和趋势分布是单峰高斯分布；慢趋势下定价错误分布是高斯双曲余弦分布，存在P - 分叉；快速弱耦合趋势下是单峰高斯分布；强趋势反馈下定价错误分布才是双峰。

Conclusion: 得到模型不同状态下分布形态，反驳了文献中的一些观点，但最后一种状态精确解未能得出。

Abstract: We derive the stationary distribution in various regimes of the extended Chiarella model of financial markets. This model is a stochastic nonlinear dynamical system that encompasses dynamical competition between a (saturating) trending and a mean-reverting component. We find the so-called mispricing distribution and the trend distribution to be unimodal Gaussians in the small noise, small feedback limit. Slow trends yield Gaussian-cosh mispricing distributions that allow for a P-bifurcation: unimodality occurs when mean-reversion is fast, bimodality when it is slow. The critical point of this bifurcation is established and refutes previous ad-hoc reports and differs from the bifurcation condition of the dynamical system itself. For fast, weakly coupled trends, deploying the Furutsu-Novikov theorem reveals that the result is again unimodal Gaussian. For the same case with higher coupling we disprove another claim from the literature: bimodal trend distributions do not generally imply bimodal mispricing distributions. The latter becomes bimodal only for stronger trend feedback. The exact solution in this last regime remains unfortunately beyond our proficiency.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [493] [Generalized Inequality-based Approach for Probabilistic WCET Estimation](https://arxiv.org/abs/2511.11682)
*Hayate Toba,Atsushi Yano,Takuya Azumi*

Main category: stat.ML

TL;DR: 本文提出将饱和函数融入切比雪夫不等式的方法以减少pWCET估计的悲观性，在自动辅助驾驶数据上验证能实现更安全更紧的边界。


<details>
  <summary>Details</summary>
Motivation: 现有基于EVT的pWCET估计方法有模型不确定性问题，基于不等式的方法对重尾分布结果悲观，需改进。

Method: 将饱和函数（反正切和双曲正切）融入切比雪夫不等式，减轻大异常值影响。

Result: 在合成数据和自动辅助驾驶栈的真实数据上评估，该方法对重尾分布能实现安全且更紧的边界。

Conclusion: 提出的方法可有效减少pWCET估计的悲观性，适用于重尾分布。

Abstract: Estimating the probabilistic Worst-Case Execution Time (pWCET) is essential for ensuring the timing correctness of real-time applications, such as in robot IoT systems and autonomous driving systems. While methods based on Extreme Value Theory (EVT) can provide tight bounds, they suffer from model uncertainty due to the need to decide where the upper tail of the distribution begins. Conversely, inequality-based approaches avoid this issue but can yield pessimistic results for heavy-tailed distributions. This paper proposes a method to reduce such pessimism by incorporating saturating functions (arctangent and hyperbolic tangent) into Chebyshev's inequality, which mitigates the influence of large outliers while preserving mathematical soundness. Evaluations on synthetic and real-world data from the Autoware autonomous driving stack demonstrate that the proposed method achieves safe and tighter bounds for such distributions.

</details>


### [494] [FreDN: Spectral Disentanglement for Time Series Forecasting via Learnable Frequency Decomposition](https://arxiv.org/abs/2511.11817)
*Zhongde An,Jinhong You,Jiyanglin Li,Yiming Tang,Wen Li,Heming Du,Shouguo Du*

Main category: stat.ML

TL;DR: 提出频率分解网络FreDN用于时间序列预测，在频域分离趋势和周期分量，降低复值运算复杂度，实验显示性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有频域方法用于非平稳时间序列时存在频谱纠缠和复值学习计算负担问题，且现有分解方法不适合解决频谱纠缠。

Method: 提出Frequency Decomposition Network (FreDN)，引入可学习的Frequency Disentangler模块在频域直接分离趋势和周期分量；提出ReIm Block降低复值运算复杂度；重新审视频域损失函数并给出理论见解。

Result: 在七个长期预测基准测试中，FreDN比现有技术水平方法性能高10%；与标准复值架构相比，实虚共享参数设计减少至少50%的参数数量和计算成本。

Conclusion: FreDN在时间序列预测中表现良好，能有效解决现有频域方法的问题。

Abstract: Time series forecasting is essential in a wide range of real world applications. Recently, frequency-domain methods have attracted increasing interest for their ability to capture global dependencies. However, when applied to non-stationary time series, these methods encounter the $\textit{spectral entanglement}$ and the computational burden of complex-valued learning. The $\textit{spectral entanglement}$ refers to the overlap of trends, periodicities, and noise across the spectrum due to $\textit{spectral leakage}$ and the presence of non-stationarity. However, existing decompositions are not suited to resolving spectral entanglement. To address this, we propose the Frequency Decomposition Network (FreDN), which introduces a learnable Frequency Disentangler module to separate trend and periodic components directly in the frequency domain. Furthermore, we propose a theoretically supported ReIm Block to reduce the complexity of complex-valued operations while maintaining performance. We also re-examine the frequency-domain loss function and provide new theoretical insights into its effectiveness. Extensive experiments on seven long-term forecasting benchmarks demonstrate that FreDN outperforms state-of-the-art methods by up to 10\%. Furthermore, compared with standard complex-valued architectures, our real-imaginary shared-parameter design reduces the parameter count and computational cost by at least 50\%.

</details>


### [495] [PCA recovery thresholds in low-rank matrix inference with sparse noise](https://arxiv.org/abs/2511.11927)
*Urte Adomaityte,Gabriele Sicuro,Pierpaolo Vivo*

Main category: stat.ML

TL;DR: 研究含稀疏噪声的秩一信号的高维推断，用复制法解析计算相关量，确定临界信号强度，结果与数值对角化相符。


<details>
  <summary>Details</summary>
Motivation: 探究含稀疏噪声的秩一信号的高维推断问题。

Method: 运用统计物理中的复制法，通过递归分布方程求解，用种群动力学算法求解方程，还针对特定噪声矩阵分布进行分析。

Result: 解析计算出顶部特征值、顶部特征向量分量密度和信号向量与顶部特征向量的重叠值；确定了临界信号强度；在大连接度极限下可恢复已知的密集噪声结果；解析结果与大矩阵的数值对角化一致。

Conclusion: 将著名的BBP转变推广到稀疏噪声情况，为含稀疏噪声的秩一信号高维推断提供了解决方法和理论结果。

Abstract: We study the high-dimensional inference of a rank-one signal corrupted by sparse noise. The noise is modelled as the adjacency matrix of a weighted undirected graph with finite average connectivity in the large size limit. Using the replica method from statistical physics, we analytically compute the typical value of the top eigenvalue, the top eigenvector component density, and the overlap between the signal vector and the top eigenvector. The solution is given in terms of recursive distributional equations for auxiliary probability density functions which can be efficiently solved using a population dynamics algorithm. Specialising the noise matrix to Poissonian and Random Regular degree distributions, the critical signal strength is analytically identified at which a transition happens for the recovery of the signal via the top eigenvector, thus generalising the celebrated BBP transition to the sparse noise case. In the large-connectivity limit, known results for dense noise are recovered. Analytical results are in agreement with numerical diagonalisation of large matrices.

</details>


### [496] [Bayesian--AI Fusion for Epidemiological Decision Making: Calibrated Risk, Honest Uncertainty, and Hyperparameter Intelligence](https://arxiv.org/abs/2511.11983)
*Debashis Chatterjee*

Main category: stat.ML

TL;DR: 本文提出贝叶斯与AI统一框架，结合贝叶斯预测和超参数优化，在疾病数据集验证，显示贝叶斯推理可提升推断和搜索能力，用于流行病学决策。


<details>
  <summary>Details</summary>
Motivation: 现代流行病学分析的机器学习模型缺乏校准不确定性，贝叶斯方法虽能量化不确定性但难与当代AI工作流集成。

Method: 提出结合贝叶斯预测与超参数优化的统一框架，用贝叶斯逻辑回归分析糖尿病数据集，用高斯过程贝叶斯优化调整乳腺癌队列生存模型。

Result: 模拟研究表明贝叶斯层提供可靠覆盖和校准，贝叶斯收缩改善评估指标，贝叶斯优化使生存模型接近最优一致性。

Conclusion: 贝叶斯推理提升推断和搜索能力，为流行病学决策提供校准风险和超参数智能。

Abstract: Modern epidemiological analytics increasingly use machine learning models that offer strong prediction but often lack calibrated uncertainty. Bayesian methods provide principled uncertainty quantification, yet are viewed as difficult to integrate with contemporary AI workflows. This paper proposes a unified Bayesian and AI framework that combines Bayesian prediction with Bayesian hyperparameter optimization.
  We use Bayesian logistic regression to obtain calibrated individual-level disease risk and credible intervals on the Pima Indians Diabetes dataset. In parallel, we use Gaussian-process Bayesian optimization to tune penalized Cox survival models on the GBSG2 breast cancer cohort. This yields a two-layer system: a Bayesian predictive layer that represents risk as a posterior distribution, and a Bayesian optimization layer that treats model selection as inference over a black-box objective.
  Simulation studies in low- and high-dimensional regimes show that the Bayesian layer provides reliable coverage and improved calibration, while Bayesian shrinkage improves AUC, Brier score, and log-loss. Bayesian optimization consistently pushes survival models toward near-oracle concordance. Overall, Bayesian reasoning enhances both what we infer and how we search, enabling calibrated risk and principled hyperparameter intelligence for epidemiological decision making.

</details>


### [497] [PCA++: How Uniformity Induces Robustness to Background Noise in Contrastive Learning](https://arxiv.org/abs/2511.12278)
*Mingqi Wu,Qiang Sun,Yi Yang*

Main category: stat.ML

TL;DR: 本文受对比学习启发，提出PCA++方法从正样本对中恢复共享信号子空间，理论和实验证明其优于标准PCA和PCA+，还阐明了均匀性在对比学习中的作用。


<details>
  <summary>Details</summary>
Motivation: 高维数据中的低维信号常被结构化背景噪声掩盖，标准PCA效果受限，需从正样本对中恢复共享信号子空间。

Method: 提出PCA++，一种硬均匀性约束的对比PCA，通过广义特征值问题有闭式解。

Result: 理论上给出高维渐近分析，实验上在模拟数据、损坏MNIST和单细胞转录组学上优于标准PCA和PCA+。

Conclusion: 明确了均匀性在对比学习中的作用，显式特征分散可抵御结构化噪声，增强鲁棒性。

Abstract: High-dimensional data often contain low-dimensional signals obscured by structured background noise, which limits the effectiveness of standard PCA. Motivated by contrastive learning, we address the problem of recovering shared signal subspaces from positive pairs, paired observations sharing the same signal but differing in background. Our baseline, PCA+, uses alignment-only contrastive learning and succeeds when background variation is mild, but fails under strong noise or high-dimensional regimes. To address this, we introduce PCA++, a hard uniformity-constrained contrastive PCA that enforces identity covariance on projected features. PCA++ has a closed-form solution via a generalized eigenproblem, remains stable in high dimensions, and provably regularizes against background interference. We provide exact high-dimensional asymptotics in both fixed-aspect-ratio and growing-spike regimes, showing uniformity's role in robust signal recovery. Empirically, PCA++ outperforms standard PCA and alignment-only PCA+ on simulations, corrupted-MNIST, and single-cell transcriptomics, reliably recovering condition-invariant structure. More broadly, we clarify uniformity's role in contrastive learning, showing that explicit feature dispersion defends against structured noise and enhances robustness.

</details>


### [498] [Accelerated Distributional Temporal Difference Learning with Linear Function Approximation](https://arxiv.org/abs/2511.12688)
*Kaicheng Jin,Yang Peng,Jiansheng Yang,Zhihua Zhang*

Main category: stat.ML

TL;DR: 研究线性函数近似下分布时态差分（TD）学习的有限样本统计率，结合方差缩减技术建立样本复杂度边界，显示学习回报函数全分布不比学习期望更难。


<details>
  <summary>Details</summary>
Motivation: 以往分布TD学习统计分析主要关注表格情形，本文研究线性函数近似设置下的情况。

Method: 先对线性分类贝尔曼方程进行细粒度分析，再在新算法中融入方差缩减技术。

Result: 建立了大K时与支持大小K无关的紧密样本复杂度边界，理论结果表明学习回报函数全分布不比学习期望更难。

Conclusion: 为分布强化学习算法的统计效率提供新见解。

Abstract: In this paper, we study the finite-sample statistical rates of distributional temporal difference (TD) learning with linear function approximation. The purpose of distributional TD learning is to estimate the return distribution of a discounted Markov decision process for a given policy. Previous works on statistical analysis of distributional TD learning focus mainly on the tabular case. We first consider the linear function approximation setting and conduct a fine-grained analysis of the linear-categorical Bellman equation. Building on this analysis, we further incorporate variance reduction techniques in our new algorithms to establish tight sample complexity bounds independent of the support size $K$ when $K$ is large. Our theoretical results imply that, when employing distributional TD learning with linear function approximation, learning the full distribution of the return function from streaming data is no more difficult than learning its expectation. This work provide new insights into the statistical efficiency of distributional reinforcement learning algorithms.

</details>


### [499] [TSB-HB: A Hierarchical Bayesian Extension of the TSB Model for Intermittent Demand Forecasting](https://arxiv.org/abs/2511.12749)
*Zong-Han Bai,Po-Yen Chu*

Main category: stat.ML

TL;DR: 提出TSB - HB模型解决间歇性需求预测问题，在数据集上表现优于经典模型，兼具可解释性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 经典模型缺乏生成基础，深度学习模型需大量数据且缺乏可解释性，需更好的间歇性需求预测方法。

Method: 引入TSB - HB，用Beta - Binomial分布建模需求发生，Log - Normal分布建模非零需求规模，使用分层先验。

Result: 在UCI Online Retail和M5数据集子集上，TSB - HB的RMSE和RMSSE低于多个经典模型。

Conclusion: TSB - HB结合生成公式和分层收缩，对间歇性和块状物品预测有更好准确性，且可解释、可扩展。

Abstract: Intermittent demand forecasting poses unique challenges due to sparse observations, cold-start items, and obsolescence. Classical models such as Croston, SBA, and the Teunter-Syntetos-Babai (TSB) method provide simple heuristics but lack a principled generative foundation. Deep learning models address these limitations but often require large datasets and sacrifice interpretability.
  We introduce TSB-HB, a hierarchical Bayesian extension of TSB. Demand occurrence is modeled with a Beta-Binomial distribution, while nonzero demand sizes follow a Log-Normal distribution. Crucially, hierarchical priors enable partial pooling across items, stabilizing estimates for sparse or cold-start series while preserving heterogeneity. This framework yields a fully generative and interpretable model that generalizes classical exponential smoothing.
  On the UCI Online Retail dataset, TSB-HB achieves lower RMSE and RMSSE than Croston, SBA, TSB, ADIDA, IMAPA, ARIMA and Theta, and on a subset of the M5 dataset it outperforms all classical baselines we evaluate. The model provides calibrated probabilistic forecasts and improved accuracy on intermittent and lumpy items by combining a generative formulation with hierarchical shrinkage, while remaining interpretable and scalable.

</details>


### [500] [Function-on-Function Bayesian Optimization](https://arxiv.org/abs/2511.12783)
*Jingru Huang,Haijie Xu,Manrui Jiang,Chen Zhang*

Main category: stat.ML

TL;DR: 提出函数对函数贝叶斯优化（FFBO）框架解决现有贝叶斯优化方法未处理输入输出均为函数的目标问题，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯优化方法未解决输入输出均为函数的目标优化问题，而此类问题在复杂系统中因先进传感技术日益增多。

Method: 引入带可分离算子值核的函数对函数高斯过程（FFGP）模型；基于FFGP，用加权算子标量化策略定义标量上置信界（UCB）采集函数；开发可扩展的泛函梯度上升算法（FGA）确定最优函数值输入，并分析理论性质。

Result: 在合成和真实世界数据上的大量实验表明，FFBO性能优于现有方法。

Conclusion: 所提出的FFBO框架能有效解决输入输出均为函数的目标优化问题，性能表现良好。

Abstract: Bayesian optimization (BO) has been widely used to optimize expensive and gradient-free objective functions across various domains. However, existing BO methods have not addressed the objective where both inputs and outputs are functions, which increasingly arise in complex systems as advanced sensing technologies. To fill this gap, we propose a novel function-on-function Bayesian optimization (FFBO) framework. Specifically, we first introduce a function-on-function Gaussian process (FFGP) model with a separable operator-valued kernel to capture the correlations between function-valued inputs and outputs. Compared to existing Gaussian process models, FFGP is modeled directly in the function space. Based on FFGP, we define a scalar upper confidence bound (UCB) acquisition function using a weighted operator-based scalarization strategy. Then, a scalable functional gradient ascent algorithm (FGA) is developed to efficiently identify the optimal function-valued input. We further analyze the theoretical properties of the proposed method. Extensive experiments on synthetic and real-world data demonstrate the superior performance of FFBO over existing approaches.

</details>


### [501] [Benign Overfitting in Linear Classifiers with a Bias Term](https://arxiv.org/abs/2511.12840)
*Yuta Kondo*

Main category: stat.ML

TL;DR: 本文将Hashimoto等人关于良性过拟合的结果拓展到含偏置项的非齐次模型，证明良性过拟合在更复杂模型中仍存在，揭示偏置项对泛化条件的影响。


<details>
  <summary>Details</summary>
Motivation: Hashimoto等人对线性分类中良性过拟合的分析局限于无偏置项的齐次模型，而实际中偏置项是标准组件，因此要拓展到含偏置项的非齐次情况。

Method: 将Hashimoto等人的结果直接拓展到非齐次模型进行分析。

Result: 证明良性过拟合在更复杂的非齐次模型中持续存在；偏置项对数据协方差结构引入新约束，各向同性情况下，新约束受齐次模型要求主导。

Conclusion: 本工作提供了良性过拟合更完整的图景，揭示了偏置项对良好泛化所需条件的重要影响。

Abstract: Modern machine learning models with a large number of parameters often generalize well despite perfectly interpolating noisy training data - a phenomenon known as benign overfitting. A foundational explanation for this in linear classification was recently provided by Hashimoto et al. (2025). However, this analysis was limited to the setting of "homogeneous" models, which lack a bias (intercept) term - a standard component in practice. This work directly extends Hashimoto et al.'s results to the more realistic inhomogeneous case, which incorporates a bias term. Our analysis proves that benign overfitting persists in these more complex models. We find that the presence of the bias term introduces new constraints on the data's covariance structure required for generalization, an effect that is particularly pronounced when label noise is present. However, we show that in the isotropic case, these new constraints are dominated by the requirements inherited from the homogeneous model. This work provides a more complete picture of benign overfitting, revealing the non-trivial impact of the bias term on the conditions required for good generalization.

</details>


### [502] [Reconstruction of Manifold Distances from Noisy Observations](https://arxiv.org/abs/2511.13025)
*Charles Fefferman,Jonathan Marty,Kevin Ren*

Main category: stat.ML

TL;DR: 提出从噪声成对距离观测重建流形内在几何的新框架，改进先前工作，给出两种算法，分析缺失观测情况并指出技术可扩展到更广泛度量概率空间。


<details>
  <summary>Details</summary>
Motivation: 解决从噪声成对距离观测重建流形内在几何的问题，改进先前假设独立同分布加性噪声且矩已知的工作。

Method: 基于估计特定期望函数的L2范数构建稳健聚类，给出两种生成聚类的算法，一种有样本复杂度和运行时间分析，另一种引入新几何思想。

Result: 在温和几何假设下，聚类可将样本点间真实距离恢复到加性误差O(ε log ε⁻¹)，第一种算法有样本复杂度N ≍ ε⁻²ᵈ⁻²log(1/ε)和运行时间o(N³)，在缺失观测时可修改聚类构造并扩展恢复保证。

Conclusion: 阐明距离恢复所需流形性质，表明技术可扩展到更广泛度量概率空间。

Abstract: We consider the problem of reconstructing the intrinsic geometry of a manifold from noisy pairwise distance observations. Specifically, let $M$ denote a diameter 1 d-dimensional manifold and $μ$ a probability measure on $M$ that is mutually absolutely continuous with the volume measure. Suppose $X_1,\dots,X_N$ are i.i.d. samples of $μ$ and we observe noisy-distance random variables $d'(X_j, X_k)$ that are related to the true geodesic distances $d(X_j,X_k)$. With mild assumptions on the distributions and independence of the noisy distances, we develop a new framework for recovering all distances between points in a sufficiently dense subsample of $M$. Our framework improves on previous work which assumed i.i.d. additive noise with known moments. Our method is based on a new way to estimate $L_2$-norms of certain expectation-functions $f_x(y)=\mathbb{E}d'(x,y)$ and use them to build robust clusters centered at points of our sample. Using a new geometric argument, we establish that, under mild geometric assumptions--bounded curvature and positive injectivity radius--these clusters allow one to recover the true distances between points in the sample up to an additive error of $O(\varepsilon \log \varepsilon^{-1})$. We develop two distinct algorithms for producing these clusters. The first achieves a sample complexity $N \asymp \varepsilon^{-2d-2}\log(1/\varepsilon)$ and runtime $o(N^3)$. The second introduces novel geometric ideas that warrant further investigation. In the presence of missing observations, we show that a quantitative lower bound on sampling probabilities suffices to modify the cluster construction in the first algorithm and extend all recovery guarantees. Our main technical result also elucidates which properties of a manifold are necessary for the distance recovery, which suggests further extension of our techniques to a broader class of metric probability spaces.

</details>


### [503] [Likelihood-guided Regularization in Attention Based Models](https://arxiv.org/abs/2511.13221)
*Mohamed Salem,Inyoung Kim*

Main category: stat.ML

TL;DR: 提出基于变分伊辛的正则化框架用于视觉Transformer，在基准视觉数据集上验证其有效性，凸显结构化贝叶斯稀疏化的作用。


<details>
  <summary>Details</summary>
Motivation: Transformer在分类任务成功依赖大规模数据和正则化，需新方法增强泛化和剪枝冗余参数。

Method: 引入似然引导的基于变分伊辛的正则化框架，利用贝叶斯稀疏化技术对模型权重施加结构化稀疏性。

Result: 在基准视觉数据集上提升稀疏复杂数据的泛化能力，实现权重和选择参数的不确定性量化，有更好校准概率估计和结构化特征选择。

Conclusion: 结构化贝叶斯稀疏化能有效增强基于Transformer的架构，是标准正则化技术的可行替代。

Abstract: The transformer architecture has demonstrated strong performance in classification tasks involving structured and high-dimensional data. However, its success often hinges on large- scale training data and careful regularization to prevent overfitting. In this paper, we intro- duce a novel likelihood-guided variational Ising-based regularization framework for Vision Transformers (ViTs), which simultaneously enhances model generalization and dynamically prunes redundant parameters. The proposed variational Ising-based regularization approach leverages Bayesian sparsification techniques to impose structured sparsity on model weights, allowing for adaptive architecture search during training. Unlike traditional dropout-based methods, which enforce fixed sparsity patterns, the variational Ising-based regularization method learns task-adaptive regularization, improving both efficiency and interpretability. We evaluate our approach on benchmark vision datasets, including MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100, demonstrating improved generalization under sparse, complex data and allowing for principled uncertainty quantification on both weights and selection parameters. Additionally, we show that the Ising regularizer leads to better-calibrated probability estimates and structured feature selection through uncertainty-aware attention mechanisms. Our results highlight the effectiveness of structured Bayesian sparsification in enhancing transformer-based architectures, offering a principled alternative to standard regularization techniques.

</details>


### [504] [The Shape of Data: Topology Meets Analytics. A Practical Introduction to Topological Analytics and the Stability Index (TSI) in Business](https://arxiv.org/abs/2511.13503)
*Ioannis Diamantis*

Main category: stat.ML

TL;DR: 本文为应用分析师提供持久同调介绍和TDA流程，通过案例展示TDA优势，引入TSI指标，给出TDA实施指南。


<details>
  <summary>Details</summary>
Motivation: 现代商业和经济数据集有非线性、多尺度结构，传统线性工具难以充分表示，TDA可揭示跨尺度模式。

Method: 提供持久同调介绍和TDA流程，进行消费者行为、股票市场和外汇动态的对比案例研究，讨论方法选择并引入TSI指标。

Result: 拓扑特征能揭示超越经典统计方法的细分模式和结构关系，TSI可作为结构变异性指标。

Conclusion: 给出TDA在商业和经济分析中实施、可视化和沟通的实用指南。

Abstract: Modern business and economic datasets often exhibit nonlinear, multi-scale structures that traditional linear tools under-represent. Topological Data Analysis (TDA) offers a geometric lens for uncovering robust patterns, such as connected components, loops and voids, across scales. This paper provides an intuitive, figure-driven introduction to persistent homology and a practical, reproducible TDA pipeline for applied analysts. Through comparative case studies in consumer behavior, equity markets (SAX/eSAX vs.\ TDA) and foreign exchange dynamics, we demonstrate how topological features can reveal segmentation patterns and structural relationships beyond classical statistical methods. We discuss methodological choices regarding distance metrics, complex construction and interpretation, and we introduce the \textit{Topological Stability Index} (TSI), a simple yet interpretable indicator of structural variability derived from persistence lifetimes. We conclude with practical guidelines for TDA implementation, visualization and communication in business and economic analytics.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [505] [SCoRES: An R Package for Simultaneous Confidence Region Estimates](https://arxiv.org/abs/2511.12242)
*Zhuoran Yu,Armin Schwartzman,Junting Ren,Julia Wrobel*

Main category: stat.CO

TL;DR: 本文介绍SCoRES R包，实现Ren的方法用于逆区域估计和置信区域构建，还提供构建SCBs的函数，并通过三个例子展示其广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 现有逆域估计方法有诸多限制，如需要严格假设、依赖难确定的固定阈值，限制了适用性，因此需要新方法。

Method: 实现Ren通过反转同时置信带非渐近地同时构建多水平置信集的方法，开发SCoRES R包，提供逆区域估计、置信区域构建及可视化工具，还有构建SCBs的函数。

Result: 开发出SCoRES R包，并通过三个例子展示其在回归数据、函数数据和地理数据上的工作流程。

Conclusion: SCoRES R包具有广泛适用性，能解决现有逆域估计方法的问题。

Abstract: The identification of domain sets whose outcomes belong to predefined subsets can address fundamental risk assessment challenges in climatology and medicine. Existing approaches for inverse domain estimates require restrictive assumptions, including domain density and continuity of function near thresholds, and large-sample guarantees, which limit the applicability. Besides, the estimation and coverage depend on setting a fixed threshold level, which is difficult to determine. Recently, Ren et al. (2024) proved that confidence sets of multiple levels can be simultaneously constructed with the desired confidence non-asymptotically through inverting simultaneous confidence bands. Here, we present the SCoRES R package, which implements Ren's approach for both the estimation of the inverse region and the corresponding simultaneous outer and inner confidence regions, along with visualization tools. Besides, the package also provides functions that help construct SCBs for regression data, functional data and geographical data. To illustrate its broad applicability, we present three rigorous examples that demonstrate the SCoRES workflow.

</details>


### [506] [Bregman geometry-aware split Gibbs sampling for Bayesian Poisson inverse problems](https://arxiv.org/abs/2511.12257)
*Elhadji Cisse Faye,Mame Diarra Fall,Nicolas Dobigeon,Eric Barat*

Main category: stat.CO

TL;DR: 本文提出基于蒙特卡罗采样算法的贝叶斯框架解决泊松逆问题，在多实验中展现竞争力。


<details>
  <summary>Details</summary>
Motivation: 解决泊松似然带来的非利普希茨梯度和正性约束等挑战。

Method: 推导利用精确和渐近精确数据增强的贝叶斯模型，用吉布斯步骤采样，对含正则化势的情况采用HRLMC算法。

Result: 在去噪、去模糊和PET实验中，该方法在重建质量上有竞争力。

Conclusion: 所提方法能有效解决泊松逆问题，在重建质量方面表现良好。

Abstract: This paper proposes a novel Bayesian framework for solving Poisson inverse problems by devising a Monte Carlo sampling algorithm which accounts for the underlying non-Euclidean geometry. To address the challenges posed by the Poisson likelihood -- such as non-Lipschitz gradients and positivity constraints -- we derive a Bayesian model which leverages exact and asymptotically exact data augmentations. In particular, the augmented model incorporates two sets of splitting variables both derived through a Bregman divergence based on the Burg entropy. Interestingly the resulting augmented posterior distribution is characterized by conditional distributions which benefit from natural conjugacy properties and preserve the intrinsic geometry of the latent and splitting variables. This allows for efficient sampling via Gibbs steps, which can be performed explicitly for all conditionals, except the one incorporating the regularization potential. For this latter, we resort to a Hessian Riemannian Langevin Monte Carlo (HRLMC) algorithm which is well suited to handle priors with explicit or easily computable score functions. By operating on a mirror manifold, this Langevin step ensures that the sampling satisfies the positivity constraints and more accurately reflects the underlying problem structure. Performance results obtained on denoising, deblurring, and positron emission tomography (PET) experiments demonstrate that the method achieves competitive performance in terms of reconstruction quality compared to optimization- and sampling-based approaches.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [507] [Subgraph Isomorphism: Prolog vs. Conventional](https://arxiv.org/abs/2511.13600)
*Claire Y. Yin,Peter M. Kogge*

Main category: cs.LO

TL;DR: 本文探讨用Prolog逻辑程序处理特定复杂图模式，对比逻辑编程与传统编程复杂度差异，表明逻辑范式处理复杂图问题更高效。


<details>
  <summary>Details</summary>
Motivation: 理解逻辑编程和传统编程在处理特定复杂图模式时的复杂度差异。

Method: 编写Prolog逻辑程序处理图模式，将图模式转换为逻辑语句并分析不同图规模下的特征。

Result: 分析得出使用逻辑范式是处理复杂图问题的有效方式。

Conclusion: 逻辑范式在处理复杂图问题上具有高效性。

Abstract: Subgraph Isomorphism uses a small graph as a pattern to identify within a larger graph a set of vertices that have matching edges. This paper addresses a logic program written in Prolog for a specific relatively complex graph pattern for which multiple conventional implementations (including parallel) exist. The goal is to understand the complexity differences between programming logically and programming conventionally. Discussion includes the process of converting the graph pattern into logic statements in Prolog, and the resulting characteristics as the size of the graph increased. The analysis shows that using a logic paradigm is an efficient way to attack complex graph problems.

</details>


### [508] [Proceedings Seventh International Workshop on Formal Methods for Autonomous Systems](https://arxiv.org/abs/2511.13245)
*Matt Luckcuck,Maike Schwammberger,Mengwei Xu*

Main category: cs.LO

TL;DR: 本文介绍EPTCS卷收录FMAS 2025会议论文，该会议旨在汇聚用形式化方法研究自主系统的研究者，介绍了会议相关信息及投稿情况。


<details>
  <summary>Details</summary>
Motivation: 汇聚使用形式化方法解决自主系统独特挑战的研究者，促进交流与成果发表。

Method: 举办FMAS 2025会议，与iFM'25同期举办。

Result: 收到来自多个国家的16份投稿，投稿作者有新有旧。

Conclusion: 现有社区认可FMAS搭建的网络，新作者表明其有很大增长潜力。

Abstract: This EPTCS volume contains the papers from the Seventh International Workshop on Formal Methods for Autonomous Systems (FMAS 2025), which was held between the 17th and 19th of November 2025. The goal of the FMAS workshop series is to bring together leading researchers who are using formal methods to tackle the unique challenges that autonomous systems present, so that they can publish and discuss their work with a growing community of researchers. FMAS 2025 was co-located with the 20th International Conference on integrated Formal Methods (iFM'25), hosted by Inria Paris, France at the Inria Paris Center. 
  In total, FMAS 2025 received 16 submissions from researchers at institutions in: Canada, China, France, Germany, Ireland, Italy, Japan, the Netherlands, Portugal, Sweden, the United States of America, and the United Kingdom. Though we received fewer submissions than last year, we are encouraged to see the submissions being sent from a wide range of countries. Submissions come from both past and new FMAS authors, which shows us that the existing community appreciates the network that FMAS has built over the past 7 years, while new authors also show the FMAS community's great potential of growth.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [509] [A Review of Statistical and Machine Learning Approaches for Coral Bleaching Assessment](https://arxiv.org/abs/2511.12234)
*Soham Sarkar,Arnab Hazra*

Main category: stat.AP

TL;DR: 文章指出珊瑚白化是海洋生态系统的主要问题，回顾现有评估珊瑚白化的统计和机器学习方法，并探讨未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 解决评估珊瑚白化的随机建模方法文献稀缺问题，为有效珊瑚礁管理提供数据驱动策略。

Method: 综述现有统计方法（如简单回归模型等）和机器学习方法（如随机森林等）。

Result: 梳理出常用统计框架和机器学习方法用于评估珊瑚白化。

Conclusion: 应聚焦在特定珊瑚白化背景下构建统计和机器学习模型的未来研究方向。

Abstract: Coral bleaching is a major concern for marine ecosystems; more than half of the world's coral reefs have either bleached or died over the past three decades. Increasing sea surface temperatures, along with various spatiotemporal environmental factors, are considered the primary reasons behind coral bleaching. The statistical and machine learning communities have focused on multiple aspects of the environment in detail. However, the literature on various stochastic modeling approaches for assessing coral bleaching is extremely scarce. Data-driven strategies are crucial for effective reef management, and this review article provides an overview of existing statistical and machine learning methods for assessing coral bleaching. Statistical frameworks, including simple regression models, generalized linear models, generalized additive models, Bayesian regression models, spatiotemporal models, and resilience indicators, such as Fisher's Information and Variance Index, are commonly used to explore how different environmental stressors influence coral bleaching. On the other hand, machine learning methods, including random forests, decision trees, support vector machines, and spatial operators, are more popular for detecting nonlinear relationships, analyzing high-dimensional data, and allowing integration of heterogeneous data from diverse sources. In addition to summarizing these models, we also discuss potential data-driven future research directions, with a focus on constructing statistical and machine learning models in specific contexts related to coral bleaching.

</details>


### [510] [Stochastic Predictive Analytics for Stocks in the Newsvendor Problem](https://arxiv.org/abs/2511.12397)
*Pedro A. Pury*

Main category: stat.AP

TL;DR: 开发随机模型解决库存管理挑战，用真实数据验证其在实际预测场景的有效性


<details>
  <summary>Details</summary>
Motivation: 解决库存管理中在不假设特定需求分布下描述库存动态分布的关键挑战

Method: 开发不假设特定需求分布的随机模型

Result: 用大型电子市场的真实数据评估，证明模型在实际预测场景有效

Conclusion: 模型为历史数据有限和短期预测情况提供灵活适用的解决方案，适合报童问题

Abstract: This work addresses a key challenge in inventory management by developing a stochastic model that describes the dynamic distribution of inventory stock over time without assuming a specific demand distribution. Our model provides a flexible and applicable solution for situations with limited historical data and short-term predictions, making it well-suited for the Newsvendor problem. We evaluate our model's performance using real-world data from a large electronic marketplace, demonstrating its effectiveness in a practical forecasting scenario.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [511] [Prrr: Personal Random Rewards for Blockchain Reporting](https://arxiv.org/abs/2511.12626)
*Hongyin Chen,Yubin Ke,Xiaotie Deng,Ittay Eyal*

Main category: cs.CR

TL;DR: 文章指出智能合约报告协议存在安全 - 性能权衡问题，提出 Prrr 协议解决该问题，能兼顾安全与效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有智能合约报告协议存在的安全 - 性能权衡问题，即信任少量发布者有中心化风险，开放发布则链上报告过多。

Method: 提出 Prrr 协议，采用随机异质奖励值分配，运用第二价格式结算分配奖励。

Result: Prrr 协议能确保激励兼容，构成子博弈完美纳什均衡，抵御合谋和女巫攻击。

Conclusion: Prrr 协议可应用于众多依赖及时报告的智能合约。

Abstract: Smart contracts, the stateful programs running on blockchains, often rely on reports. Publishers are paid to publish these reports on the blockchain. Designing protocols that incentivize timely reporting is the prevalent reporting problem. But existing solutions face a security-performance trade-off: Relying on a small set of trusted publishers introduces centralization risks, while allowing open publication results in an excessive number of reports on the blockchain. We identify the root cause of this trade-off to be the standard symmetric reward design, which treats all reports equally. We prove that no symmetric-reward mechanism can overcome the trade-off.
  We present Personal Random Rewards for Reporting (Prrr), a protocol that assigns random heterogeneous values to reports. We call this novel mechanism-design concept Ex-Ante Synthetic Asymmetry. To the best of our knowledge, Prrr is the first game-theoretic mechanism (in any context) that deliberately forms participant asymmetry. Prrr employs a second-price-style settlement to allocate rewards, ensuring incentive compatibility and achieving both security and efficiency. Following the protocol constitutes a Subgame-Perfect Nash Equilibrium, robust against collusion and Sybil attacks. Prrr is applicable to numerous smart contracts that rely on timely reports.

</details>


### [512] [VULPO: Context-Aware Vulnerability Detection via On-Policy LLM Optimization](https://arxiv.org/abs/2511.11896)
*Youpeng Li,Fuxun Yu,Xinda Wang*

Main category: cs.CR

TL;DR: 本文提出用于上下文感知漏洞检测的VULPO框架，构建ContextVul数据集，设计多维奖励结构及难度自适应奖励缩放，实验显示其性能优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有漏洞检测技术在上下文感知分析能力上存在局限，依赖固定输入或静态偏好数据集，无法自适应探索依赖关系，受限于函数级基准。

Method: 引入VULPO框架，构建ContextVul数据集，设计多维奖励结构，采用标签级和样本级难度自适应奖励缩放。

Result: VULPO-4B大幅优于基于提示工程和离策略优化的现有漏洞检测基线，F1比Qwen3 - 4B提高85%，性能与大150倍的DeepSeek - R1 - 0528相当。

Conclusion: VULPO框架在上下文感知漏洞检测方面具有优越性。

Abstract: The widespread reliance on open-source software dramatically increases the risk of vulnerability exploitation, underscoring the need for effective and scalable vulnerability detection (VD). Existing VD techniques, whether traditional machine learning-based or LLM-based approaches like prompt engineering, supervised fine-tuning, or off-policy preference optimization, remain fundamentally limited in their ability to perform context-aware analysis: They depend on fixed inputs or static preference datasets, cannot adaptively explore repository-level dependencies, and are constrained by function-level benchmarks that overlook critical vulnerability context.
  This paper introduces Vulnerability-Adaptive Policy Optimization (VULPO), an on-policy LLM reinforcement learning framework for context-aware VD. To support training and evaluation, we first construct ContextVul, a new dataset that augments high-quality function-level samples with lightweight method to extract repository-level context information. We then design multi-dimensional reward structuring that jointly captures prediction correctness, vulnerability localization accuracy, and the semantic relevance of vulnerability analysis, thereby guiding the model toward comprehensive contextual reasoning. To address the asymmetric difficulty of different vulnerability cases and mitigate reward hacking, VULPO incorporates label-level and sample-level difficulty-adaptive reward scaling, encouraging the model to explore challenging cases while maintaining balanced reward distribution. Extensive experiments demonstrate the superiority of our VULPO framework in context-aware VD: Our VULPO-4B substantially outperforms existing VD baselines based on prompt engineering and off-policy optimization, improving F1 by 85% over Qwen3-4B and achieving performance comparable to a 150x larger-scale model, DeepSeek-R1-0528.

</details>


### [513] [Multi-Agent Collaborative Fuzzing with Continuous Reflection for Smart Contracts Vulnerability Detection](https://arxiv.org/abs/2511.12164)
*Jie Chen,Liangmin Wang*

Main category: cs.CR

TL;DR: 提出SmartFuzz用于检测智能合约漏洞，采用大语言模型驱动的代理作为模糊测试引擎，实验显示其性能优于现有工具。


<details>
  <summary>Details</summary>
Motivation: 现有模糊测试工具在检测复杂漏洞方面存在不足，如重代码覆盖轻漏洞发现、缺乏对有状态合约的语义理解。

Method: 提出Continuous Reflection Process (CRP)将交易序列生成转变为自我进化过程；引入Reactive Collaborative Chain (RCC)将模糊测试过程分解为多个子任务；设计多智能体协作团队从全局和局部视角生成和优化交易序列。

Result: 在30分钟内比现有工具多检测出5.8%-74.7%的漏洞，最多减少80%的漏报。

Conclusion: SmartFuzz在检测智能合约漏洞方面优于现有最先进的工具。

Abstract: Fuzzing is a widely used technique for detecting vulnerabilities in smart contracts, which generates transaction sequences to explore the execution paths of smart contracts. However, existing fuzzers are falling short in detecting sophisticated vulnerabilities that require specific attack transaction sequences with proper inputs to trigger, as they (i) prioritize code coverage over vulnerability discovery, wasting considerable effort on non-vulnerable code regions, and (ii) lack semantic understanding of stateful contracts, generating numerous invalid transaction sequences that cannot pass runtime execution.
  In this paper, we propose SmartFuzz, a novel collaborative reflective fuzzer for smart contract vulnerability detection. It employs large language model-driven agents as the fuzzing engine and continuously improves itself by learning and reflecting through interactions with the environment. Specifically, we first propose a new Continuous Reflection Process (CRP) for fuzzing smart contracts, which reforms the transaction sequence generation as a self-evolving process through continuous reflection on feedback from the runtime environment. Then, we present the Reactive Collaborative Chain (RCC) to orchestrate the fuzzing process into multiple sub-tasks based on the dependencies of transaction sequences. Furthermore, we design a multi-agent collaborative team, where each expert agent is guided by the RCC to jointly generate and refine transaction sequences from both global and local perspectives. We conduct extensive experiments to evaluate SmartFuzz's performance on real-world contracts and DApp projects. The results demonstrate that SmartFuzz outperforms existing state-of-the-art tools: (i) it detects 5.8\%-74.7\% more vulnerabilities within 30 minutes, and (ii) it reduces false negatives by up to 80\%.

</details>


### [514] [Software Supply Chain Security of Web3](https://arxiv.org/abs/2511.12274)
*Martin Monperrus*

Main category: cs.CR

TL;DR: 本文探讨Web3生态系统特有的软件供应链安全挑战并提出缓解策略。


<details>
  <summary>Details</summary>
Motivation: Web3应用依赖复杂软件供应链，存在显著安全漏洞，传统Web2问题与区块链特性交织，需解决安全问题。

Method: 分析Web3生态系统的威胁格局。

Result: 提出强化Web3系统安全态势的缓解策略。

Conclusion: 可通过所提策略加强Web3系统的安全。

Abstract: Web3 applications, built on blockchain technology, manage billions of dollars in digital assets through decentralized applications (dApps) and smart contracts. These systems rely on complex, software supply chains that introduce significant security vulnerabilities. This paper examines the software supply chain security challenges unique to the Web3 ecosystem, where traditional Web2 software supply chain problems intersect with the immutable and high-stakes nature of blockchain technology. We analyze the threat landscape and propose mitigation strategies to strengthen the security posture of Web3 systems.

</details>


### [515] [GenSIaC: Toward Security-Aware Infrastructure-as-Code Generation with Large Language Models](https://arxiv.org/abs/2511.12385)
*Yikun Li,Matteo Grella,Daniel Nahmias,Gal Engelberg,Dan Klein,Giancarlo Guizzardi,Thijs van Ede,Andrea Continella*

Main category: cs.CR

TL;DR: 本文探讨大语言模型生成安全感知的基础设施即代码（IaC）代码的潜力，提出GenSIaC数据集微调模型，提升了识别和预防IaC安全配置错误的性能。


<details>
  <summary>Details</summary>
Motivation: 云基础设施复杂度增加使IaC脚本存在配置错误和安全漏洞风险，且大语言模型生成安全IaC脚本的能力不明，缺乏对其生成脚本安全弱点的理解和增强安全的技术。

Method: 先全面评估基础大语言模型识别IaC安全弱点的能力，再提出GenSIaC指令微调数据集，用其微调大语言模型以生成安全感知的IaC代码。

Result: 模型在识别和预防IaC安全配置错误方面性能显著提升，如F1分数从0.303提升到0.858，还进行了消融研究，探索了GenSIaC的泛化性和跨语言能力。

Conclusion: 利用GenSIaC微调大语言模型能有效提升其生成安全感知IaC代码的能力。

Abstract: In recent years, Infrastructure as Code (IaC) has emerged as a critical approach for managing and provisioning IT infrastructure through code and automation. IaC enables organizations to create scalable and consistent environments, effectively managing servers and development settings. However, the growing complexity of cloud infrastructures has led to an increased risk of misconfigurations and security vulnerabilities in IaC scripts. To address this problem, this paper investigates the potential of Large Language Models (LLMs) in generating security-aware IaC code, avoiding misconfigurations introduced by developers and administrators.
  While LLMs have made significant progress in natural language processing and code generation, their ability to generate secure IaC scripts remains unclear. This paper addresses two major problems: 1) the lack of understanding of security weaknesses in IaC scripts generated by LLMs, and 2) the absence of techniques for enhancing security in generating IaC code with LLMs.
  To assess the extent to which LLMs contain security knowledge, we first conduct a comprehensive evaluation of base LLMs in recognizing major IaC security weaknesses during the generation and inspection of IaC code. Then, we propose GenSIaC, an instruction fine-tuning dataset designed to improve LLMs' ability to recognize potential security weaknesses. Leveraging GenSIaC, we fine-tune LLMs and instruct models to generate security-aware IaC code. Our evaluation demonstrates that our models achieve substantially improved performance in recognizing and preventing IaC security misconfigurations, e.g., boosting the F1-score from 0.303 to 0.858. Additionally, we perform ablation studies and explore GenSIaC's generalizability to other LLMs and its cross-language capabilities.

</details>


### [516] [InfoDecom: Decomposing Information for Defending against Privacy Leakage in Split Inference](https://arxiv.org/abs/2511.13365)
*Ruijun Deng,Zhihui Lu,Qiang Duan*

Main category: cs.CR

TL;DR: 针对拆分推理中数据重建攻击导致的隐私泄漏问题，提出InfoDecom框架，实现了更好的效用 - 隐私权衡。


<details>
  <summary>Details</summary>
Motivation: 拆分推理中数据重建攻击会导致隐私泄漏，现有防御方法在客户端模型较浅时会造成较大的效用损失。

Method: 提出InfoDecom防御框架，先分解并去除冗余信息，再注入校准后的噪声以提供理论上的隐私保证。

Result: 实验表明InfoDecom与现有基线相比，实现了更优的效用 - 隐私权衡。

Conclusion: InfoDecom能有效解决拆分推理中的隐私保护问题，在效用和隐私之间取得较好平衡。

Abstract: Split inference (SI) enables users to access deep learning (DL) services without directly transmitting raw data. However, recent studies reveal that data reconstruction attacks (DRAs) can recover the original inputs from the smashed data sent from the client to the server, leading to significant privacy leakage. While various defenses have been proposed, they often result in substantial utility degradation, particularly when the client-side model is shallow. We identify a key cause of this trade-off: existing defenses apply excessive perturbation to redundant information in the smashed data. To address this issue in computer vision tasks, we propose InfoDecom, a defense framework that first decomposes and removes redundant information and then injects noise calibrated to provide theoretically guaranteed privacy. Experiments demonstrate that InfoDecom achieves a superior utility-privacy trade-off compared to existing baselines. The code and the appendix are available at https://github.com/SASA-cloud/InfoDecom.

</details>


### [517] [Can AI Models be Jailbroken to Phish Elderly Victims? An End-to-End Evaluation](https://arxiv.org/abs/2511.11759)
*Fred Heiding,Simon Lermen*

Main category: cs.CR

TL;DR: 展示攻击者利用AI安全漏洞攻击老年群体的完整流程，评估6个前沿大模型安全防护，发现问题，人实验显示11%老人受骗，指出现有防护不足。


<details>
  <summary>Details</summary>
Motivation: 揭示AI安全漏洞对弱势群体的危害，评估现有大模型安全防护能力。

Method: 系统评估6个前沿大模型在4类攻击下的安全防护，开展有108位老年志愿者参与的人类验证研究。

Result: 多个模型对特定攻击向量高度敏感，AI生成的钓鱼邮件使11%参与者受骗。

Conclusion: 当前AI安全措施无法保护易受骗群体，现有提供商的反滥用措施不足。

Abstract: We present an end-to-end demonstration of how attackers can exploit AI safety failures to harm vulnerable populations: from jailbreaking LLMs to generate phishing content, to deploying those messages against real targets, to successfully compromising elderly victims. We systematically evaluated safety guardrails across six frontier LLMs spanning four attack categories, revealing critical failures where several models exhibited near-complete susceptibility to certain attack vectors. In a human validation study with 108 senior volunteers, AI-generated phishing emails successfully compromised 11\% of participants. Our work uniquely demonstrates the complete attack pipeline targeting elderly populations, highlighting that current AI safety measures fail to protect those most vulnerable to fraud. Beyond generating phishing content, LLMs enable attackers to overcome language barriers and conduct multi-turn trust-building conversations at scale, fundamentally transforming fraud economics. While some providers report voluntary counter-abuse efforts, we argue these remain insufficient.

</details>


### [518] [NegBLEURT Forest: Leveraging Inconsistencies for Detecting Jailbreak Attacks](https://arxiv.org/abs/2511.11784)
*Lama Sleem,Jerome Francois,Lujun Li,Nathan Foucher,Niccolo Gentile,Radu State*

Main category: cs.CR

TL;DR: 本文针对大语言模型越狱攻击，提出NegBLEURT Forest检测框架，实验显示该方法表现优异。


<details>
  <summary>Details</summary>
Motivation: 越狱攻击会使大语言模型生成有害内容，且难以制定通用过滤规则。

Method: 引入成功和失败响应的语义一致性分析，提出NegBLEURT Forest框架，用隔离森林算法识别异常响应。

Result: 在自制数据集上，该方法在不同模型的准确率排名第一或第二，而其他方法对模型和数据变化敏感。

Conclusion: 提出的方法能实现可靠的越狱攻击检测，性能表现出色。

Abstract: Jailbreak attacks designed to bypass safety mechanisms pose a serious threat by prompting LLMs to generate harmful or inappropriate content, despite alignment with ethical guidelines. Crafting universal filtering rules remains difficult due to their inherent dependence on specific contexts. To address these challenges without relying on threshold calibration or model fine-tuning, this work introduces a semantic consistency analysis between successful and unsuccessful responses, demonstrating that a negation-aware scoring approach captures meaningful patterns. Building on this insight, a novel detection framework called NegBLEURT Forest is proposed to evaluate the degree of alignment between outputs elicited by adversarial prompts and expected safe behaviors. It identifies anomalous responses using the Isolation Forest algorithm, enabling reliable jailbreak detection. Experimental results show that the proposed method consistently achieves top-tier performance, ranking first or second in accuracy across diverse models using the crafted dataset, while competing approaches exhibit notable sensitivity to model and data variations.

</details>


### [519] [Securing Generative AI in Healthcare: A Zero-Trust Architecture Powered by Confidential Computing on Google Cloud](https://arxiv.org/abs/2511.11836)
*Adaobi Amanna,Ishana Shinde*

Main category: cs.CR

TL;DR: 传统框架无法解决生成式人工智能集成到医疗保健中的安全挑战，本文提出机密零信任框架（CZF），详细介绍其在谷歌云的架构蓝图，分析其对现实威胁的有效性，CZF能提供强大可验证框架，为医疗保健中负责任地采用变革性人工智能技术奠定信任基础。


<details>
  <summary>Details</summary>
Motivation: 传统框架无法解决生成式人工智能集成到医疗保健中的安全挑战，尤其是数据使用时敏感患者数据和专有AI模型暴露的问题。

Method: 提出机密零信任框架（CZF），将零信任架构的细粒度访问控制与机密计算的硬件强制数据隔离相结合，详细介绍在谷歌云实施CZF的多层架构蓝图。

Result: CZF提供深度防御架构，数据在基于硬件的可信执行环境（TEE）中使用时保持加密，其远程认证可提供工作负载完整性的加密证明，将合规性转化为可验证的技术事实，实现安全的多方协作。

Conclusion: 通过填补数据使用差距并实施零信任原则，CZF提供了一个强大且可验证的框架，为医疗保健中负责任地采用变革性人工智能技术奠定了必要的信任基础。

Abstract: The integration of Generative Artificial Intelligence (GenAI) in healthcare is impeded by significant security challenges unaddressed by traditional frameworks, precisely the data-in-use gap where sensitive patient data and proprietary AI models are exposed during active processing. To address this, the paper proposes the Confidential Zero-Trust Framework (CZF), a novel security paradigm that synergistically combines Zero-Trust Architecture for granular access control with the hardware-enforced data isolation of Confidential Computing. We detailed a multi-tiered architectural blueprint for implementing the CZF on Google Cloud and analyzed its efficacy against real-world threats. The CZF provides a defense-in-depth architecture where data remains encrypted while in-use within a hardware-based Trusted Execution Environment (TEE). The framework's use of remote attestation offers cryptographic proof of workload integrity, transforming compliance from a procedural exercise into a verifiable technical fact and enabling secure, multi-party collaborations previously blocked by security and intellectual property concerns. By closing the data-in-use gap and enforcing Zero-Trust principles, the CZF provides a robust and verifiable framework that establishes the necessary foundation of trust to enable the responsible adoption of transformative AI technologies in healthcare.

</details>


### [520] [BackWeak: Backdooring Knowledge Distillation Simply with Weak Triggers and Fine-tuning](https://arxiv.org/abs/2511.12046)
*Shanmin Wang,Dongdong Zhao*

Main category: cs.CR

TL;DR: 提出简单无替代的后门攻击范式BackWeak，能在知识蒸馏中植入后门，效率高且更隐蔽。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏后门方法复杂、计算量大且触发机制不隐蔽，存在安全风险。

Method: 构建隐蔽的“弱”触发器，用小学习率微调良性教师模型植入后门。

Result: 在多个数据集、模型架构和知识蒸馏方法上验证，BackWeak效率高、更简单且更隐蔽。

Conclusion: 研究知识蒸馏后门攻击需关注触发器隐蔽性和潜在对抗特征。

Abstract: Knowledge Distillation (KD) is essential for compressing large models, yet relying on pre-trained "teacher" models downloaded from third-party repositories introduces serious security risks -- most notably backdoor attacks. Existing KD backdoor methods are typically complex and computationally intensive: they employ surrogate student models and simulated distillation to guarantee transferability, and they construct triggers in a way similar to universal adversarial perturbations (UAPs), which being not stealthy in magnitude, inherently exhibit strong adversarial behavior. This work questions whether such complexity is necessary and constructs stealthy "weak" triggers -- imperceptible perturbations that have negligible adversarial effect. We propose BackWeak, a simple, surrogate-free attack paradigm. BackWeak shows that a powerful backdoor can be implanted by simply fine-tuning a benign teacher with a weak trigger using a very small learning rate. We demonstrate that this delicate fine-tuning is sufficient to embed a backdoor that reliably transfers to diverse student architectures during a victim's standard distillation process, yielding high attack success rates. Extensive empirical evaluations on multiple datasets, model architectures, and KD methods show that BackWeak is efficient, simpler, and often more stealthy than previous elaborate approaches. This work calls on researchers studying KD backdoor attacks to pay particular attention to the trigger's stealthiness and its potential adversarial characteristics.

</details>


### [521] [Exploring AI in Steganography and Steganalysis: Trends, Clusters, and Sustainable Development Potential](https://arxiv.org/abs/2511.12052)
*Aditya Kumar Sahu,Chandan Kumar,Saksham Kumar,Serdar Solak*

Main category: cs.CR

TL;DR: 本文对基于人工智能的隐写术数据隐藏技术进行科学计量分析，统计了相关文章地域分布、主题聚类，评估其与可持续发展目标的关联，指出领域现状与差距。


<details>
  <summary>Details</summary>
Motivation: 在过去十年人工智能驱动的隐写术和隐写分析技术发展众多的背景下，进行科学计量分析以了解该领域情况及与可持续发展目标的联系。

Method: 采用主题建模方法，对2017 - 2023年的654篇文章进行分析。

Result: 69%文章来自亚洲国家，中国发文最多；识别出七个主题聚类；仅18篇文章与可持续发展目标一致，SDG9领先。

Conclusion: 本研究是该领域独特的科学计量研究，剖析了趋势原因，强调了与社会目标的差距及对人工智能安全挑战的全球影响。

Abstract: Steganography and steganalysis are strongly related subjects of information security. Over the past decade, many powerful and efficient artificial intelligence (AI) - driven techniques have been designed and presented during research into steganography as well as steganalysis. This study presents a scientometric analysis of AI-driven steganography-based data hiding techniques using a thematic modelling approach. A total of 654 articles within the time span of 2017 to 2023 have been considered. Experimental evaluation of the study reveals that 69% of published articles are from Asian countries. The China is on top (TP:312), followed by India (TP-114). The study mainly identifies seven thematic clusters: steganographic image data hiding, deep image steganalysis, neural watermark robustness, linguistic steganography models, speech steganalysis algorithms, covert communication networks, and video steganography techniques. The proposed study also assesses the scope of AI-steganography under the purview of sustainable development goals (SDGs) to present the interdisciplinary reciprocity between them. It has been observed that only 18 of the 654 articles are aligned with one of the SDGs, which shows that limited studies conducted in alignment with SDG goals. SDG9 which is Industry, Innovation, and Infrastructure is leading among 18 SDGs mapped articles. To the top of our insight, this study is the unique one to present a scientometric study on AI-driven steganography-based data hiding techniques. In the context of descriptive statistics, the study breaks down the underlying causes of observed trends, including the influence of DL developments, trends in East Asia and maturity of foundational methods. The work also stresses upon the critical gaps in societal alignment, particularly the SDGs, ultimately working on unveiling the field's global impact on AI security challenges.

</details>


### [522] [Explainable Transformer-Based Email Phishing Classification with Adversarial Robustness](https://arxiv.org/abs/2511.12085)
*Sajad U P*

Main category: cs.CR

TL;DR: 本文提出一种混合方法应对钓鱼攻击，结合DistilBERT、FGM对抗训练、LIME XAI技术和Flan - T5 - small语言模型，实现精确分类并提供易懂解释。


<details>
  <summary>Details</summary>
Motivation: 钓鱼攻击多样且技术先进，AI生成的钓鱼攻击降低了检测系统的整体弹性，需要新方法应对。

Method: 采用DistilBERT进行邮件分类，用FGM对抗训练增强对基于文本的对抗性扰动的鲁棒性，集成LIME XAI技术提高模型透明度，使用Flan - T5 - small语言模型为终端用户生成安全说明解释。

Result: 未明确提及具体结果，但表明该方法能确保精确的钓鱼分类。

Conclusion: 该混合方法可实现精确的钓鱼分类，并为模型决策提供易于理解的理由。

Abstract: Phishing and related cyber threats are becoming more varied and technologically advanced. Among these, email-based phishing remains the most dominant and persistent threat. These attacks exploit human vulnerabilities to disseminate malware or gain unauthorized access to sensitive information. Deep learning (DL) models, particularly transformer-based models, have significantly enhanced phishing mitigation through their contextual understanding of language. However, some recent threats, specifically Artificial Intelligence (AI)-generated phishing attacks, are reducing the overall system resilience of phishing detectors. In response, adversarial training has shown promise against AI-generated phishing threats. This study presents a hybrid approach that uses DistilBERT, a smaller, faster, and lighter version of the BERT transformer model for email classification. Robustness against text-based adversarial perturbations is reinforced using Fast Gradient Method (FGM) adversarial training. Furthermore, the framework integrates the LIME Explainable AI (XAI) technique to enhance the transparency of the DistilBERT architecture. The framework also uses the Flan-T5-small language model from Hugging Face to generate plain-language security narrative explanations for end-users. This combined approach ensures precise phishing classification while providing easily understandable justifications for the model's decisions.

</details>


### [523] [AttackVLA: Benchmarking Adversarial and Backdoor Attacks on Vision-Language-Action Models](https://arxiv.org/abs/2511.12149)
*Jiayu Li,Yunhan Zhao,Xiang Zheng,Zonghuan Xu,Yige Li,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CR

TL;DR: 提出AttackVLA统一框架评估VLA模型安全漏洞，引入BackdoorVLA针对性后门攻击，评估其有效性，为保障VLA系统安全提供标准化框架。


<details>
  <summary>Details</summary>
Motivation: 现有针对VLA模型攻击技术因缺乏统一评估框架，有效性不明，不同架构动作分词器差异影响可重复性和公平比较，且多数攻击未在真实场景验证。

Method: 提出AttackVLA统一框架，涵盖数据构建、模型训练和推理阶段，实现多种攻击方式；引入BackdoorVLA针对性后门攻击。

Result: BackdoorVLA在模拟基准和真实机器人场景评估中，平均目标成功率达58.4%，部分任务达100%。

Conclusion: 工作为评估VLA漏洞提供标准化框架，展示精确对抗操纵潜力，推动基于VLA的具身系统安全研究。

Abstract: Vision-Language-Action (VLA) models enable robots to interpret natural-language instructions and perform diverse tasks, yet their integration of perception, language, and control introduces new safety vulnerabilities. Despite growing interest in attacking such models, the effectiveness of existing techniques remains unclear due to the absence of a unified evaluation framework. One major issue is that differences in action tokenizers across VLA architectures hinder reproducibility and fair comparison. More importantly, most existing attacks have not been validated in real-world scenarios. To address these challenges, we propose AttackVLA, a unified framework that aligns with the VLA development lifecycle, covering data construction, model training, and inference. Within this framework, we implement a broad suite of attacks, including all existing attacks targeting VLAs and multiple adapted attacks originally developed for vision-language models, and evaluate them in both simulation and real-world settings. Our analysis of existing attacks reveals a critical gap: current methods tend to induce untargeted failures or static action states, leaving targeted attacks that drive VLAs to perform precise long-horizon action sequences largely unexplored. To fill this gap, we introduce BackdoorVLA, a targeted backdoor attack that compels a VLA to execute an attacker-specified long-horizon action sequence whenever a trigger is present. We evaluate BackdoorVLA in both simulated benchmarks and real-world robotic settings, achieving an average targeted success rate of 58.4% and reaching 100% on selected tasks. Our work provides a standardized framework for evaluating VLA vulnerabilities and demonstrates the potential for precise adversarial manipulation, motivating further research on securing VLA-based embodied systems.

</details>


### [524] [SeedAIchemy: LLM-Driven Seed Corpus Generation for Fuzzing](https://arxiv.org/abs/2511.12448)
*Aidan Wen,Norah A. Alzahrani,Jingzhi Jiang,Andrew Joe,Karen Shieh,Andy Zhang,Basel Alomair,David Wagner*

Main category: cs.CR

TL;DR: 介绍自动化语料生成工具SeedAIchemy，能助开发者有效实现模糊测试，其生成语料效果好。


<details>
  <summary>Details</summary>
Motivation: 让开发者更轻松有效地实现模糊测试。

Method: SeedAIchemy包含五个模块从互联网收集公开文件，其中四个模块用大语言模型工作流构建搜索词以提升语料质量。

Result: SeedAIchemy生成的语料在多种目标程序和库上表现明显优于简单语料，与手动整理语料相当。

Conclusion: SeedAIchemy是一个有效的自动化语料生成工具，有助于开发者进行模糊测试。

Abstract: We introduce SeedAIchemy, an automated LLM-driven corpus generation tool that makes it easier for developers to implement fuzzing effectively. SeedAIchemy consists of five modules which implement different approaches at collecting publicly available files from the internet. Four of the five modules use large language model (LLM) workflows to construct search terms designed to maximize corpus quality. Corpora generated by SeedAIchemy perform significantly better than a naive corpus and similarly to a manually-curated corpus on a diverse range of target programs and libraries.

</details>


### [525] [Scalable Hierarchical AI-Blockchain Framework for Real-Time Anomaly Detection in Large-Scale Autonomous Vehicle Networks](https://arxiv.org/abs/2511.12648)
*Rathin Chandra Shit,Sharmila Subudhi*

Main category: cs.CR

TL;DR: 本文提出三层混合安全架构HAVEN应对自动驾驶汽车网络安全挑战，实验显示其检测延迟低于10ms，准确率高且具有良好可扩展性和弹性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶汽车网络安全面临挑战，现有安全方案无法在可接受安全/隐私框架内实现亚10毫秒异常检测和大规模车辆网络的分布式协调。

Method: 引入三层混合安全架构HAVEN，包括边缘轻集成异常检测模型、区域拜占庭容错联邦学习和区块链机制；在真实自动驾驶数据集上进行广泛实验和大规模模拟。

Result: 检测延迟低于10ms，准确率94%，F1分数92%，能容忍20%受损节点，减少区块链存储开销，保证差分隐私。

Conclusion: HAVEN克服了实时安全义务和分布式安全协调之间的权衡，在检测准确性和网络弹性方面优于其他方法。

Abstract: The security of autonomous vehicle networks is facing major challenges, owing to the complexity of sensor integration, real-time performance demands, and distributed communication protocols that expose vast attack surfaces around both individual and network-wide safety. Existing security schemes are unable to provide sub-10 ms (milliseconds) anomaly detection and distributed coordination of large-scale networks of vehicles within an acceptable safety/privacy framework. This paper introduces a three-tier hybrid security architecture HAVEN (Hierarchical Autonomous Vehicle Enhanced Network), which decouples real-time local threat detection and distributed coordination operations. It incorporates a light ensemble anomaly detection model on the edge (first layer), Byzantine-fault-tolerant federated learning to aggregate threat intelligence at a regional scale (middle layer), and selected blockchain mechanisms (top layer) to ensure critical security coordination. Extensive experimentation is done on a real-world autonomous driving dataset. Large-scale simulations with the number of vehicles ranging between 100 and 1000 and different attack types, such as sensor spoofing, jamming, and adversarial model poisoning, are conducted to test the scalability and resiliency of HAVEN. Experimental findings show sub-10 ms detection latency with an accuracy of 94% and F1-score of 92% across multimodal sensor data, Byzantine fault tolerance validated with 20\% compromised nodes, and a reduced blockchain storage overhead, guaranteeing sufficient differential privacy. The proposed framework overcomes the important trade-off between real-time safety obligation and distributed security coordination with novel three-tiered processing. The scalable architecture of HAVEN is shown to provide great improvement in detection accuracy as well as network resilience over other methods.

</details>


### [526] [AI Bill of Materials and Beyond: Systematizing Security Assurance through the AI Risk Scanning (AIRS) Framework](https://arxiv.org/abs/2511.12668)
*Samuel Nathanson,Alexander Lee,Catherine Chen Kieffer,Jared Junkin,Jessica Ye,Amir Saeed,Melanie Lockhart,Russ Fink,Elisha Peterson,Lanier Watkins*

Main category: cs.CR

TL;DR: 本文介绍AI风险扫描（AIRS）框架，该框架基于威胁模型生成证据，通过三项试点研究发展而来，目前可提供大语言模型的模型级保证，概念验证展示其有效性，扩展了软件物料清单（SBOM）实践。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统保证分散，现有透明度机制难以提供可验证、机器可读的模型安全证据，需有效方法实现AI保证。

Method: 通过三项试点研究发展AIRS框架，使其将AI文档从描述性披露转向可衡量、有证据的验证，框架与MITRE ATLAS对抗性机器学习分类法对齐，自动生成结构化工件。

Result: 概念验证在量化GPT - OSS - 20B模型上展示了安全加载器策略实施等功能，与SPDX 3.0和CycloneDX 1.6标准对比发现有对齐部分，也存在AI特定保证字段的关键差距。

Conclusion: AIRS框架将威胁建模与自动化、可审计的证据生成相结合，把SBOM实践扩展到AI领域，为标准化、可信且机器可验证的AI风险文档提供了原则基础。

Abstract: Assurance for artificial intelligence (AI) systems remains fragmented across software supply-chain security, adversarial machine learning, and governance documentation. Existing transparency mechanisms - including Model Cards, Datasheets, and Software Bills of Materials (SBOMs) - advance provenance reporting but rarely provide verifiable, machine-readable evidence of model security. This paper introduces the AI Risk Scanning (AIRS) Framework, a threat-model-based, evidence-generating framework designed to operationalize AI assurance. The AIRS Framework evolved through three progressive pilot studies - Smurf (AIBOM schema design), OPAL (operational validation), and Pilot C (AIRS) - that reframed AI documentation from descriptive disclosure toward measurable, evidence-bound verification. The framework aligns its assurance fields to the MITRE ATLAS adversarial ML taxonomy and automatically produces structured artifacts capturing model integrity, packaging and serialization safety, structural adapters, and runtime behaviors. Currently, the AIRS Framework is scoped to provide model-level assurances for LLMs, but it could be expanded to include other modalities and cover system-level threats (e.g. application-layer abuses, tool-calling). A proof-of-concept on a quantized GPT-OSS-20B model demonstrates enforcement of safe loader policies, per-shard hash verification, and contamination and backdoor probes executed under controlled runtime conditions. Comparative analysis with SBOM standards of SPDX 3.0 and CycloneDX 1.6 reveals alignment on identity and evaluation metadata, but identifies critical gaps in representing AI-specific assurance fields. The AIRS Framework thus extends SBOM practice to the AI domain by coupling threat modeling with automated, auditable evidence generation, providing a principled foundation for standardized, trustworthy, and machine-verifiable AI risk documentation.

</details>


### [527] [Whose Narrative is it Anyway? A KV Cache Manipulation Attack](https://arxiv.org/abs/2511.12752)
*Mukkesh Ganesh,Kaushik Iyer,Arun Baalaaji Sankar Ananthan*

Main category: cs.CR

TL;DR: 本文提出针对自回归大语言模型KV缓存的“History Swapping”攻击方法，在Qwen 3系列模型上评估，揭示攻击特点及KV缓存的重要性。


<details>
  <summary>Details</summary>
Motivation: KV缓存作为模型内部状态的表示，可能成为完整性攻击的目标，需要研究相关攻击方法。

Method: 提出“History Swapping”攻击，用不同主题的预计算缓存覆盖当前生成缓存的连续段，并在324种配置下对Qwen 3系列模型进行实验。

Result: 只有全层覆盖能成功劫持对话主题，出现三种不同行为，发现高层结构计划在生成早期编码，最终层维护局部话语结构。

Conclusion: KV缓存是安全分析的重要载体，编码了上下文、主题轨迹和结构规划，可用于操纵模型行为。

Abstract: The Key Value(KV) cache is an important component for efficient inference in autoregressive Large Language Models (LLMs), but its role as a representation of the model's internal state makes it a potential target for integrity attacks. This paper introduces "History Swapping," a novel block-level attack that manipulates the KV cache to steer model generation without altering the user-facing prompt. The attack involves overwriting a contiguous segment of the active generation's cache with a precomputed cache from a different topic. We empirically evaluate this method across 324 configurations on the Qwen 3 family of models, analyzing the impact of timing, magnitude, and layer depth of the cache overwrite. Our findings reveal that only full-layer overwrites can successfully hijack the conversation's topic, leading to three distinct behaviors: immediate and persistent topic shift, partial recovery, or a delayed hijack. Furthermore, we observe that high-level structural plans are encoded early in the generation process and local discourse structure is maintained by the final layers of the model. This work demonstrates that the KV cache is a significant vector for security analysis, as it encodes not just context but also topic trajectory and structural planning, making it a powerful interface for manipulating model behavior.

</details>


### [528] [Privacy-Preserving Federated Learning from Partial Decryption Verifiable Threshold Multi-Client Functional Encryption](https://arxiv.org/abs/2511.12936)
*Minjie Wang,Jinguang Han,Weizhi Meng*

Main category: cs.CR

TL;DR: 本文构建可部分解密验证的阈值多客户端函数加密方案，应用于联邦学习实现 VTSAFL 协议，实验表明其在保证精度下，大幅降低训练时间和通信开销。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方案用阈值密码学缓解推理攻击，但无法保证聚合结果可验证性，易受投毒攻击威胁。

Method: 构建部分解密验证的阈值多客户端函数加密方案，应用于联邦学习实现 VTSAFL 协议。

Result: 在 MNIST 数据集上，VTSAFL 与现有方案精度相同，总训练时间减少超 40%，通信开销最多降低 50%。

Conclusion: VTSAFL 效率对克服物联网设备资源限制至关重要。

Abstract: In federated learning, multiple parties can cooperate to train the model without directly exchanging their own private data, but the gradient leakage problem still threatens the privacy security and model integrity. Although the existing scheme uses threshold cryptography to mitigate the inference attack, it can not guarantee the verifiability of the aggregation results, making the system vulnerable to the threat of poisoning attack. We construct a partial decryption verifiable threshold multi client function encryption scheme, and apply it to Federated learning to implement the federated learning verifiable threshold security aggregation protocol (VTSAFL). VTSAFL empowers clients to verify aggregation results, concurrently minimizing both computational and communication overhead. The size of the functional key and partial decryption results of the scheme are constant, which provides efficiency guarantee for large-scale deployment. The experimental results on MNIST dataset show that vtsafl can achieve the same accuracy as the existing scheme, while reducing the total training time by more than 40%, and reducing the communication overhead by up to 50%. This efficiency is critical for overcoming the resource constraints inherent in Internet of Things (IoT) devices.

</details>


### [529] [Esim: EVM Bytecode Similarity Detection Based on Stable-Semantic Graph](https://arxiv.org/abs/2511.12971)
*Zhuo Chen,Gaoqiang Ji,Yiling He,Lei Wu,Yajin Zhou*

Main category: cs.CR

TL;DR: 本文提出Stable - Semantic Graph (SSG)和原型Esim进行EVM字节码相似度检测，性能超传统方法和SOTA工具。


<details>
  <summary>Details</summary>
Motivation: DeFi快速扩张，代码复用和开源贡献带来问题，传统检测方法对EVM字节码有局限，需有效准确的相似度检测方法。

Method: 提出SSG捕获稳定指令关系，实现原型Esim，用异构图神经网络将SSG嵌入矩阵进行相似度检测。

Result: Esim在SSG构建中控制流F1分数达100%、数据流达95.16%，相似度检测AUC达96.3%，大规模研究显示其在漏洞搜索上超Etherscan。

Conclusion: Esim在EVM字节码相似度检测和漏洞搜索方面表现出色，优于传统方法和SOTA工具。

Abstract: Decentralized finance (DeFi) is experiencing rapid expansion. However, prevalent code reuse and limited open-source contributions have introduced significant challenges to the blockchain ecosystem, including plagiarism and the propagation of vulnerable code. Consequently, an effective and accurate similarity detection method for EVM bytecode is urgently needed to identify similar contracts. Traditional binary similarity detection methods are typically based on instruction stream or control flow graph (CFG), which have limitations on EVM bytecode due to specific features like low-level EVM bytecode and heavily-reused basic blocks. Moreover, the highly-diverse Solidity Compiler (Solc) versions further complicate accurate similarity detection.
  Motivated by these challenges, we propose a novel EVM bytecode representation called Stable-Semantic Graph (SSG), which captures relationships between 'stable instructions' (special instructions identified by our study). Moreover, we implement a prototype, Esim, which embeds SSG into matrices for similarity detection using a heterogeneous graph neural network. Esim demonstrates high accuracy in SSG construction, achieving F1-scores of 100% for control flow and 95.16% for data flow, and its similarity detection performance reaches 96.3% AUC, surpassing traditional approaches. Our large-scale study, analyzing 2,675,573 smart contracts on six EVM-compatible chains over a one-year period, also demonstrates that Esim outperforms the SOTA tool Etherscan in vulnerability search.

</details>


### [530] [SoK: The Last Line of Defense: On Backdoor Defense Evaluation](https://arxiv.org/abs/2511.13143)
*Gorka Abad,Marina Krček,Stefanos Koffas,Behrad Tajalli,Marco Arazzi,Roberto Riaño,Xiaoyun Xu,Zhuoran Liu,Antonino Nocera,Stjepan Picek*

Main category: cs.CR

TL;DR: 文章对2018 - 2025年183篇后门防御论文进行系统分析，发现评估方法存在不一致问题，通过大量实验展示不同评估设置下防御效果差异，指出当前评估实践的关键差距并给出改进建议。


<details>
  <summary>Details</summary>
Motivation: 后门攻击威胁深度学习模型，现有防御评估方法的异质性阻碍了防御方法的公平比较，需要对后门防御进行系统分析。

Method: 通过全面文献综述和实证评估，分析183篇论文，进行超3000次实验，涉及三个数据集、四种模型架构、16种防御方法和五种攻击。

Result: 文献中实验设置、评估指标和威胁模型假设存在显著不一致，不同评估设置下防御效果差异大，当前评估实践存在关键差距。

Conclusion: 指出当前评估实践的问题，提供标准化和改进未来防御评估的具体挑战和建议，为研究人员和从业者提供见解。

Abstract: Backdoor attacks pose a significant threat to deep learning models by implanting hidden vulnerabilities that can be activated by malicious inputs. While numerous defenses have been proposed to mitigate these attacks, the heterogeneous landscape of evaluation methodologies hinders fair comparison between defenses. This work presents a systematic (meta-)analysis of backdoor defenses through a comprehensive literature review and empirical evaluation. We analyzed 183 backdoor defense papers published between 2018 and 2025 across major AI and security venues, examining the properties and evaluation methodologies of these defenses.
  Our analysis reveals significant inconsistencies in experimental setups, evaluation metrics, and threat model assumptions in the literature. Through extensive experiments involving three datasets (MNIST, CIFAR-100, ImageNet-1K), four model architectures (ResNet-18, VGG-19, ViT-B/16, DenseNet-121), 16 representative defenses, and five commonly used attacks, totaling over 3\,000 experiments, we demonstrate that defense effectiveness varies substantially across different evaluation setups. We identify critical gaps in current evaluation practices, including insufficient reporting of computational overhead and behavior under benign conditions, bias in hyperparameter selection, and incomplete experimentation. Based on our findings, we provide concrete challenges and well-motivated recommendations to standardize and improve future defense evaluations. Our work aims to equip researchers and industry practitioners with actionable insights for developing, assessing, and deploying defenses to different systems.

</details>


### [531] [Whistledown: Combining User-Level Privacy with Conversational Coherence in LLMs](https://arxiv.org/abs/2511.13319)
*Chelsea McMurray,Hayder Tirmazi*

Main category: cs.CR

TL;DR: 介绍了Whistledown，一个为保护用户隐私而在发送给大语言模型前修改提示的隐私层，有不同部署方式且无需更改现有API。


<details>
  <summary>Details</summary>
Motivation: 用户依赖大语言模型进行敏感对话时，提示中含个人信息不想被记录、保留或泄露，企业使用大语言模型进行内部交流和决策也面临同样问题。

Method: Whistledown结合假名化、ε - 局部差分隐私和转换缓存，有低计算和内存开销，可在客户端设备或企业零信任网关部署。

Result: 未提及明确结果。

Conclusion: 未提及明确结论。

Abstract: Users increasingly rely on large language models (LLMs) for personal, emotionally charged, and socially sensitive conversations. However, prompts sent to cloud-hosted models can contain personally identifiable information (PII) that users do not want logged, retained, or leaked. We observe this to be especially acute when users discuss friends, coworkers, or adversaries, i.e., when they spill the tea. Enterprises face the same challenge when they want to use LLMs for internal communication and decision-making.
  In this whitepaper, we present Whistledown, a best-effort privacy layer that modifies prompts before they are sent to the LLM. Whistledown combines pseudonymization and $ε$-local differential privacy ($ε$-LDP) with transformation caching to provide best-effort privacy protection without sacrificing conversational utility. Whistledown is designed to have low compute and memory overhead, allowing it to be deployed directly on a client's device in the case of individual users. For enterprise users, Whistledown is deployed centrally within a zero-trust gateway that runs on an enterprise's trusted infrastructure. Whistledown requires no changes to the existing APIs of popular LLM providers.

</details>


### [532] [AutoMalDesc: Large-Scale Script Analysis for Cyber Threat Research](https://arxiv.org/abs/2511.13333)
*Alexandru-Mihai Apostu,Andrei Preda,Alexandra Daniela Damir,Diana Bolocan,Radu Tudor Ionescu,Ioana Croitoru,Mihaela Gaman*

Main category: cs.CR

TL;DR: 提出自动化静态分析总结框架AutoMalDesc，经少量专家示例训练后可独立大规模运行，评估显示迭代中摘要质量和分类准确率提升，还发布数据集等促进研究。


<details>
  <summary>Details</summary>
Motivation: 解决网络安全研究中威胁检测自然语言解释生成的问题。

Method: 使用迭代自定进度学习管道，通过合成数据生成和验证循环提升输出质量，无需大量手动数据标注。

Result: 对3600个多样本评估显示迭代间有显著改进，摘要质量和分类准确率持续提升；综合验证方法确认生成摘要的技术精度和语言连贯性。

Conclusion: 发布超10万脚本样本数据集、方法和评估框架，促进该领域研究和可重复性。

Abstract: Generating thorough natural language explanations for threat detections remains an open problem in cybersecurity research, despite significant advances in automated malware detection systems. In this work, we present AutoMalDesc, an automated static analysis summarization framework that, following initial training on a small set of expert-curated examples, operates independently at scale. This approach leverages an iterative self-paced learning pipeline to progressively enhance output quality through synthetic data generation and validation cycles, eliminating the need for extensive manual data annotation. Evaluation across 3,600 diverse samples in five scripting languages demonstrates statistically significant improvements between iterations, showing consistent gains in both summary quality and classification accuracy. Our comprehensive validation approach combines quantitative metrics based on established malware labels with qualitative assessment from both human experts and LLM-based judges, confirming both technical precision and linguistic coherence of generated summaries. To facilitate reproducibility and advance research in this domain, we publish our complete dataset of more than 100K script samples, including annotated seed (0.9K) and test (3.6K) datasets, along with our methodology and evaluation framework.

</details>


### [533] [GRAPHTEXTACK: A Realistic Black-Box Node Injection Attack on LLM-Enhanced GNNs](https://arxiv.org/abs/2511.12423)
*Jiaji Ma,Puja Trivedi,Danai Koutra*

Main category: cs.CR

TL;DR: 提出针对LLM增强GNN的黑盒多模态节点注入攻击GRAPHTEXTACK，实验显示其性能远超基线。


<details>
  <summary>Details</summary>
Motivation: 现有LLM与GNN集成模型存在结构和文本双重漏洞，且现有攻击方法有局限性，如单模态攻击效果有限、部分攻击假设不现实。

Method: 提出GRAPHTEXTACK，采用新颖的进化优化框架和多目标适应度函数，在不依赖模型内部信息和替代模型的情况下注入精心设计的节点。

Result: 在五个数据集和两个先进模型上实验，GRAPHTEXTACK显著优于12个强基线。

Conclusion: GRAPHTEXTACK能有效攻击LLM增强GNN，在现实威胁模型下表现出色。

Abstract: Text-attributed graphs (TAGs), which combine structural and textual node information, are ubiquitous across many domains. Recent work integrates Large Language Models (LLMs) with Graph Neural Networks (GNNs) to jointly model semantics and structure, resulting in more general and expressive models that achieve state-of-the-art performance on TAG benchmarks. However, this integration introduces dual vulnerabilities: GNNs are sensitive to structural perturbations, while LLM-derived features are vulnerable to prompt injection and adversarial phrasing. While existing adversarial attacks largely perturb structure or text independently, we find that uni-modal attacks cause only modest degradation in LLM-enhanced GNNs. Moreover, many existing attacks assume unrealistic capabilities, such as white-box access or direct modification of graph data. To address these gaps, we propose GRAPHTEXTACK, the first black-box, multi-modal{, poisoning} node injection attack for LLM-enhanced GNNs. GRAPHTEXTACK injects nodes with carefully crafted structure and semantics to degrade model performance, operating under a realistic threat model without relying on model internals or surrogate models. To navigate the combinatorial, non-differentiable search space of connectivity and feature assignments, GRAPHTEXTACK introduces a novel evolutionary optimization framework with a multi-objective fitness function that balances local prediction disruption and global graph influence. Extensive experiments on five datasets and two state-of-the-art LLM-enhanced GNN models show that GRAPHTEXTACK significantly outperforms 12 strong baselines.

</details>


### [534] [Enhancing All-to-X Backdoor Attacks with Optimized Target Class Mapping](https://arxiv.org/abs/2511.13356)
*Lei Wang,Yulong Tian,Hao Han,Fengyuan Xu*

Main category: cs.CR

TL;DR: 本文指出多数现有工作忽略A2X攻击，证明其对现有防御的鲁棒性，提出新攻击策略提升成功率，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有工作多关注单目标A2O攻击，忽略更复杂的A2X攻击，且认为其攻击成功率低。

Method: 通过优化分组和目标类分配机制，提出提升A2X攻击成功率并保持鲁棒性的新策略。

Result: 攻击成功率最高提升28%，在CIFAR10、CIFAR100和Tiny - ImageNet上平均分别提升6.7%、16.4%、14.1%。

Conclusion: 该研究将提高对A2X攻击的认识，促进该领域进一步研究。

Abstract: Backdoor attacks pose severe threats to machine learning systems, prompting extensive research in this area. However, most existing work focuses on single-target All-to-One (A2O) attacks, overlooking the more complex All-to-X (A2X) attacks with multiple target classes, which are often assumed to have low attack success rates. In this paper, we first demonstrate that A2X attacks are robust against state-of-the-art defenses. We then propose a novel attack strategy that enhances the success rate of A2X attacks while maintaining robustness by optimizing grouping and target class assignment mechanisms. Our method improves the attack success rate by up to 28%, with average improvements of 6.7%, 16.4%, 14.1% on CIFAR10, CIFAR100, and Tiny-ImageNet, respectively. We anticipate that this study will raise awareness of A2X attacks and stimulate further research in this under-explored area. Our code is available at https://github.com/kazefjj/A2X-backdoor .

</details>


### [535] [Adaptive Dual-Layer Web Application Firewall (ADL-WAF) Leveraging Machine Learning for Enhanced Anomaly and Threat Detection](https://arxiv.org/abs/2511.12643)
*Ahmed Sameh,Sahar Selim*

Main category: cs.CR

TL;DR: 提出自适应双层WAF，用两层机器学习模型提升威胁检测准确性，评估显示检测效果好，可提高Web应用安全。


<details>
  <summary>Details</summary>
Motivation: 传统Web应用防火墙难以有效区分恶意和合法流量，威胁检测效果有限。

Method: 提出自适应双层WAF，第一层用决策树算法检测异常，第二层用支持向量机分类异常，还结合数据预处理和特征工程技术。

Result: 使用五个大型基准数据集评估，ADL WAF检测准确率达99.88%，精度为100%，增强了异常检测并减少误报。

Conclusion: 将机器学习技术集成到WAF中可显著提高Web应用安全性，实现更准确高效的威胁检测。

Abstract: Web Application Firewalls are crucial for protecting web applications against a wide range of cyber threats. Traditional Web Application Firewalls often struggle to effectively distinguish between malicious and legitimate traffic, leading to limited efficacy in threat detection. To overcome these limitations, this paper proposes an Adaptive Dual-Layer WAF employing a two-layered Machine Learning model designed to enhance the accuracy of anomaly and threat detection. The first layer employs a Decision Tree (DT) algorithm to detect anomalies by identifying traffic deviations from established normal patterns. The second layer employs Support Vector Machine to classify these anomalies as either threat anomalies or benign anomalies. Our Adaptive Dual Layer WAF incorporates comprehensive data pre-processing and feature engineering techniques and has been thoroughly evaluated using five large benchmark datasets. Evaluation using these datasets shows that ADL WAF achieves a detection accuracy of 99.88% and a precision of 100%, significantly enhancing anomaly detection and reducing false positives. These findings suggest that integrating machine learning techniques into WAFs can substantially improve web application security by providing more accurate and efficient threat detection.

</details>


### [536] [An Evaluation Framework for Network IDS/IPS Datasets: Leveraging MITRE ATT&CK and Industry Relevance Metrics](https://arxiv.org/abs/2511.12743)
*Adrita Rahman Tori,Khondokar Fida Hasan*

Main category: cs.CR

TL;DR: 引入多维框架评估入侵检测数据集适用性，应用于九个公开数据集发现威胁覆盖差距，研究成果可提升AI驱动的IDS/IPS有效性。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型评估实践主要关注准确性指标，常忽略数据集是否代表特定行业威胁，需填补此空白。

Method: 结合威胁情报、自然语言处理和定量分析，将MITRE ATT&CK知识库用于威胁情报，并采用五个互补指标评估数据集在特定行业环境的适用性。

Result: 对九个公开数据集的评估显示，医疗、能源和金融领域存在显著威胁覆盖差距，部分新数据集更符合特定行业威胁，部分新数据集表现不佳。

Conclusion: 研究提供了标准化、可解释的数据集选择方法，通过实际案例验证了框架的效率和实用性，能提升AI驱动的IDS/IPS在实际环境中的有效性。

Abstract: The performance of Machine Learning (ML) and Deep Learning (DL)-based Intrusion Detection and Prevention Systems (IDS/IPS) is critically dependent on the relevance and quality of the datasets used for training and evaluation. However, current AI model evaluation practices for developing IDS/IPS focus predominantly on accuracy metrics, often overlooking whether datasets represent industry-specific threats. To address this gap, we introduce a novel multi-dimensional framework that integrates the MITRE ATT&CK knowledge base for threat intelligence and employs five complementary metrics that together provide a comprehensive assessment of dataset suitability. Methodologically, this framework combines threat intelligence, natural language processing, and quantitative analysis to assess the suitability of datasets for specific industry contexts. Applying this framework to nine publicly available IDS/IPS datasets reveals significant gaps in threat coverage, particularly in the healthcare, energy, and financial sectors. In particular, recent datasets (e.g., CIC-IoMT, CIC-UNSW-NB15) align better with sector-specific threats, whereas others, like CICIoV-24, underperform despite their recency. Our findings provide a standardized, interpretable approach for selecting datasets aligned with sector-specific operational requirements, ultimately enhancing the real-world effectiveness of AI-driven IDS/IPS deployments. The efficiency and practicality of the framework are validated through deployment in a real-world case study, underscoring its capacity to inform dataset selection and enhance the effectiveness of AI-driven IDS/IPS in operational environments.

</details>


### [537] [Efficient Adversarial Malware Defense via Trust-Based Raw Override and Confidence-Adaptive Bit-Depth Reduction](https://arxiv.org/abs/2511.12827)
*Ayush Chaudhary,Sisir Doppalpudi*

Main category: cs.CR

TL;DR: 提出结合TRO与CABDR的框架优化对抗鲁棒性和计算效率的权衡，在EMBER v2数据集上评估表现良好，为实际部署提供可行路径。


<details>
  <summary>Details</summary>
Motivation: 现有对抗防御在提升鲁棒性时引入高计算开销，给大数据环境下的恶意软件检测系统带来挑战，需优化鲁棒性与效率的权衡。

Method: 提出结合Trust - Raw Override (TRO)与Confidence - Adaptive Bit - Depth Reduction (CABDR)的框架，利用基于自适应置信度的机制选择性应用防御措施。

Result: 计算开销为1.76倍，比现有平滑防御提升2.3倍；在EMBER v2数据集上保持91%的干净准确率，降低多种攻击的成功率；每秒吞吐量达126万个样本，在72种生产配置中验证有效。

Conclusion: 生产环境中的实际对抗鲁棒性需要明确优化效率与鲁棒性的权衡，该框架为组织部署鲁棒防御且不产生过高基础设施成本提供了可行途径。

Abstract: The deployment of robust malware detection systems in big data environments requires careful consideration of both security effectiveness and computational efficiency. While recent advances in adversarial defenses have demonstrated strong robustness improvements, they often introduce computational overhead ranging from 4x to 22x, which presents significant challenges for production systems processing millions of samples daily. In this work, we propose a novel framework that combines Trust-Raw Override (TRO) with Confidence-Adaptive Bit-Depth Reduction (CABDR) to explicitly optimize the trade-off between adversarial robustness and computational efficiency. Our approach leverages adaptive confidence-based mechanisms to selectively apply defensive measures, achieving 1.76x computational overhead - a 2.3x improvement over state-of-the-art smoothing defenses. Through comprehensive evaluation on the EMBER v2 dataset comprising 800K samples, we demonstrate that our framework maintains 91 percent clean accuracy while reducing attack success rates to 31-37 percent across multiple attack types, with particularly strong performance against optimization-based attacks such as C and W (48.8 percent reduction). The framework achieves throughput of up to 1.26 million samples per second (measured on pre-extracted EMBER features with no runtime feature extraction), validated across 72 production configurations with statistical significance (5 independent runs, 95 percent confidence intervals, p less than 0.01). Our results suggest that practical adversarial robustness in production environments requires explicit optimization of the efficiency-robustness trade-off, providing a viable path for organizations to deploy robust defenses without prohibitive infrastructure costs.

</details>


### [538] [ForgeDAN: An Evolutionary Framework for Jailbreaking Aligned Large Language Models](https://arxiv.org/abs/2511.13548)
*Siyang Cheng,Gaotian Liu,Rui Mei,Yilin Wang,Kejia Zhang,Kaishuo Wei,Yuqi Yu,Weiping Wen,Xiaojie Wu,Junhua Liu*

Main category: cs.CR

TL;DR: 针对大语言模型越狱攻击，提出ForgeDAN框架，在攻击多样性、检测效果等方面表现出色，优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 现有自动越狱生成方法存在变异多样性有限、适应度评估浅、基于关键词检测脆弱等问题。

Method: 引入多策略文本扰动提升攻击多样性，采用基于文本相似度模型的语义适应度评估，集成二维越狱判断。

Result: ForgeDAN实现高越狱成功率，同时保持自然性和隐蔽性。

Conclusion: ForgeDAN优于现有SOTA解决方案。

Abstract: The rapid adoption of large language models (LLMs) has brought both transformative applications and new security risks, including jailbreak attacks that bypass alignment safeguards to elicit harmful outputs. Existing automated jailbreak generation approaches e.g. AutoDAN, suffer from limited mutation diversity, shallow fitness evaluation, and fragile keyword-based detection. To address these limitations, we propose ForgeDAN, a novel evolutionary framework for generating semantically coherent and highly effective adversarial prompts against aligned LLMs. First, ForgeDAN introduces multi-strategy textual perturbations across \textit{character, word, and sentence-level} operations to enhance attack diversity; then we employ interpretable semantic fitness evaluation based on a text similarity model to guide the evolutionary process toward semantically relevant and harmful outputs; finally, ForgeDAN integrates dual-dimensional jailbreak judgment, leveraging an LLM-based classifier to jointly assess model compliance and output harmfulness, thereby reducing false positives and improving detection effectiveness. Our evaluation demonstrates ForgeDAN achieves high jailbreaking success rates while maintaining naturalness and stealth, outperforming existing SOTA solutions.

</details>


### [539] [Robust Client-Server Watermarking for Split Federated Learning](https://arxiv.org/abs/2511.13598)
*Jiaxiong Tang,Zhengchunmin Dai,Liantao Wu,Peng Sun,Honglong Chen,Zhenfu Cao*

Main category: cs.CR

TL;DR: 针对Split Federated Learning (SFL) 知识产权保护问题提出RISE方案，实验表明其水印检测率超95%且抗攻击能力强。


<details>
  <summary>Details</summary>
Motivation: SFL虽有隐私保护和低计算开销优点，但存在知识产权模糊问题，现有水印技术无法保护双方。

Method: 提出RISE方案，采用非对称客户端 - 服务器水印设计，服务器通过损失正则化项嵌入基于特征的水印，客户端通过注入预定义触发样本嵌入基于后门的水印。

Result: 在标准数据集和多种网络架构上实验，RISE在大多数设置下水印检测率超95%，客户端和服务器端水印无相互干扰，对常见去除攻击保持鲁棒性。

Conclusion: RISE方案能有效解决SFL中的知识产权保护问题。

Abstract: Split Federated Learning (SFL) is renowned for its privacy-preserving nature and low computational overhead among decentralized machine learning paradigms. In this framework, clients employ lightweight models to process private data locally and transmit intermediate outputs to a powerful server for further computation. However, SFL is a double-edged sword: while it enables edge computing and enhances privacy, it also introduces intellectual property ambiguity as both clients and the server jointly contribute to training. Existing watermarking techniques fail to protect both sides since no single participant possesses the complete model. To address this, we propose RISE, a Robust model Intellectual property protection scheme using client-Server watermark Embedding for SFL. Specifically, RISE adopts an asymmetric client-server watermarking design: the server embeds feature-based watermarks through a loss regularization term, while clients embed backdoor-based watermarks by injecting predefined trigger samples into private datasets. This co-embedding strategy enables both clients and the server to verify model ownership. Experimental results on standard datasets and multiple network architectures show that RISE achieves over $95\%$ watermark detection rate ($p-value \lt 0.03$) across most settings. It exhibits no mutual interference between client- and server-side watermarks and remains robust against common removal attacks.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [540] [Sangam: Chiplet-Based DRAM-PIM Accelerator with CXL Integration for LLM Inferencing](https://arxiv.org/abs/2511.12286)
*Khyati Kiyawat,Zhenxing Fan,Yasas Seneviratne,Morteza Baradaran,Akhil Shekar,Zihan Xia,Mingu Kang,Kevin Skadron*

Main category: cs.AR

TL;DR: 文章指出大语言模型推理适合内存计算，但现有方案有局限，提出基于小芯片的内存模块Sangam，能加速并节能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理受内存限制，现有内存计算方案有容量和处理能力局限。

Method: 提出基于小芯片的内存模块Sangam，将逻辑和内存解耦到不同芯片，通过中介层连接，集成先进处理组件。

Result: Sangam在不同模型上实现端到端查询延迟加速、解码吞吐量提升和显著节能。

Conclusion: Sangam能有效解决现有内存计算方案的局限，加速大语言模型推理。

Abstract: Large Language Models (LLMs) are becoming increasingly data-intensive due to growing model sizes, and they are becoming memory-bound as the context length and, consequently, the key-value (KV) cache size increase. Inference, particularly the decoding phase, is dominated by memory-bound GEMV or flat GEMM operations with low operational intensity (OI), making it well-suited for processing-in-memory (PIM) approaches. However, existing in/near-memory solutions face critical limitations such as reduced memory capacity due to the high area cost of integrating processing elements (PEs) within DRAM chips, and limited PE capability due to the constraints of DRAM fabrication technology. This work presents a chiplet-based memory module that addresses these limitations by decoupling logic and memory into chiplets fabricated in heterogeneous technology nodes and connected via an interposer. The logic chiplets sustain high bandwidth access to the DRAM chiplets, which house the memory banks, and enable the integration of advanced processing components such as systolic arrays and SRAM-based buffers to accelerate memory-bound GEMM kernels, capabilities that were not feasible in prior PIM architectures. We propose Sangam, a CXL-attached PIM-chiplet based memory module that can either act as a drop-in replacement for GPUs or co-executes along side the GPUs. Sangam achieves speedup of 3.93, 4.22, 2.82x speedup in end-to-end query latency, 10.3, 9.5, 6.36x greater decoding throughput, and order of magnitude energy savings compared to an H100 GPU for varying input size, output length, and batch size on LLaMA 2-7B, Mistral-7B, and LLaMA 3-70B, respectively.

</details>


### [541] [FERMI-ML: A Flexible and Resource-Efficient Memory-In-Situ SRAM Macro for TinyML acceleration](https://arxiv.org/abs/2511.12544)
*Mukul Lokhande,Akash Sankhe,S. V. Jaya Chand,Santosh Kumar Vishvakarma*

Main category: cs.AR

TL;DR: 提出FERMI - ML用于TinyML加速，介绍其设计及性能。


<details>
  <summary>Details</summary>
Motivation: 满足AIoT设备对低功耗、面积高效的TinyML推理的需求，需最小化数据移动并保持高计算效率的内存架构。

Method: 设计9T XNOR-based RX9T位单元集成存储与计算单元，用22晶体管压缩机树累加器，4KB宏实现原位计算和基于CAM的查找操作。

Result: 65nm后布局结果显示350MHz、0.9V下吞吐量1.93 TOPS，能效364 TOPS/W，QoR超97.5%。

Conclusion: FERMI - ML是紧凑、可重构、节能的数字原位内存宏，能支持混合精度TinyML工作负载。

Abstract: The growing demand for low-power and area-efficient TinyML inference on AIoT devices necessitates memory architectures that minimise data movement while sustaining high computational efficiency. This paper presents FERMI-ML, a Flexible and Resource-Efficient Memory-In-Situ (MIS) SRAM macro designed for TinyML acceleration. The proposed 9T XNOR-based RX9T bit-cell integrates a 5T storage cell with a 4T XNOR compute unit, enabling variable-precision MAC and CAM operations within the same array. A 22-transistor (C22T) compressor-tree-based accumulator facilitates logarithmic 1-64-bit MAC computation with reduced delay and power compared to conventional adder trees. The 4 KB macro achieves dual functionality for in-situ computation and CAM-based lookup operations, supporting Posit-4 or FP-4 precision. Post-layout results at 65 nm show operation at 350 MHz with 0.9 V, delivering a throughput of 1.93 TOPS and an energy efficiency of 364 TOPS/W, while maintaining a Quality-of-Result (QoR) above 97.5% with InceptionV4 and ResNet-18. FERMI-ML thus demonstrates a compact, reconfigurable, and energy-aware digital Memory-In-Situ macro capable of supporting mixed-precision TinyML workloads.

</details>


### [542] [T-SAR: A Full-Stack Co-design for CPU-Only Ternary LLM Inference via In-Place SIMD ALU Reorganization](https://arxiv.org/abs/2511.13676)
*Hyunwoo Oh,KyungIn Nam,Rajat Bhattacharjya,Hanning Chen,Tamoghno Das,Sanggeon Yun,Suyeon Jang,Andrew Ding,Nikil Dutt,Mohsen Imani*

Main category: cs.AR

TL;DR: 本文提出T - SAR框架，可在CPU上实现可扩展的三元大语言模型推理，有性能提升和低开销优势。


<details>
  <summary>Details</summary>
Motivation: 大语言模型发展超出边缘平台计算和内存能力，现有CPU三元量化方案依赖内存查找表限制可扩展性，FPGA或GPU加速器不适用于边缘设备。

Method: 提出T - SAR框架，通过重新利用SIMD寄存器文件进行动态、寄存器内查找表生成，仅需极少硬件修改。

Result: T - SAR消除内存瓶颈，实现数据级并行最大化，GEMM延迟提升5.6 - 24.5倍，GEMV吞吐量提升1.1 - 86.2倍，SIMD单元仅3.2%功耗和1.4%面积开销，能源效率达NVIDIA Jetson AGX Orin的2.5 - 4.9倍。

Conclusion: T - SAR为边缘平台高效大语言模型推理提供了实用方法。

Abstract: Recent advances in LLMs have outpaced the computational and memory capacities of edge platforms that primarily employ CPUs, thereby challenging efficient and scalable deployment. While ternary quantization enables significant resource savings, existing CPU solutions rely heavily on memory-based lookup tables (LUTs) which limit scalability, and FPGA or GPU accelerators remain impractical for edge use. This paper presents T-SAR, the first framework to achieve scalable ternary LLM inference on CPUs by repurposing the SIMD register file for dynamic, in-register LUT generation with minimal hardware modifications. T-SAR eliminates memory bottlenecks and maximizes data-level parallelism, delivering 5.6-24.5x and 1.1-86.2x improvements in GEMM latency and GEMV throughput, respectively, with only 3.2% power and 1.4% area overheads in SIMD units. T-SAR achieves up to 2.5-4.9x the energy efficiency of an NVIDIA Jetson AGX Orin, establishing a practical approach for efficient LLM inference on edge platforms.

</details>


### [543] [QUILL: An Algorithm-Architecture Co-Design for Cache-Local Deformable Attention](https://arxiv.org/abs/2511.13679)
*Hyunwoo Oh,Hanning Chen,Sanggeon Yun,Yang Ni,Wenjun Huang,Tamoghno Das,Suyeon Jang,Mohsen Imani*

Main category: cs.AR

TL;DR: 介绍了加速器QUILL，将可变形注意力转化为缓存友好的单遍工作，实现高吞吐量和能源效率。


<details>
  <summary>Details</summary>
Motivation: 可变形变压器因不规则内存访问和低算术强度难以映射到硬件，需设计新加速器。

Method: 采用基于距离的乱序查询（DOOQ）对查询按空间接近度排序，形成调度感知预取循环；融合MSDeformAttn引擎单遍执行操作，小张量片上保存，密集层用集成GEMM运行。

Result: QUILL比RTX 4090吞吐量高7.29倍、能源效率高47.3倍，优于先前加速器；混合精度量化下，在不同变体中精度与FP32差距<=0.9 AP。

Conclusion: QUILL将稀疏性转化为局部性和利用率，实现端到端加速。

Abstract: Deformable transformers deliver state-of-the-art detection but map poorly to hardware due to irregular memory access and low arithmetic intensity. We introduce QUILL, a schedule-aware accelerator that turns deformable attention into cache-friendly, single-pass work. At its core, Distance-based Out-of-Order Querying (DOOQ) orders queries by spatial proximity; the look-ahead drives a region prefetch into an alternate buffer--forming a schedule-aware prefetch loop that overlaps memory and compute. A fused MSDeformAttn engine executes interpolation, Softmax, aggregation, and the final projection (W''m) in one pass without spilling intermediates, while small tensors are kept on-chip and surrounding dense layers run on integrated GEMMs. Implemented as RTL and evaluated end-to-end, QUILL achieves up to 7.29x higher throughput and 47.3x better energy efficiency than an RTX 4090, and exceeds prior accelerators by 3.26-9.82x in throughput and 2.01-6.07x in energy efficiency. With mixed-precision quantization, accuracy tracks FP32 within <=0.9 AP across Deformable and Sparse DETR variants. By converting sparsity into locality--and locality into utilization--QUILL delivers consistent, end-to-end speedups.

</details>


<div id='q-fin.GN'></div>

# q-fin.GN [[Back]](#toc)

### [544] [CBDC Stress Test in a Dual-Currency Setting](https://arxiv.org/abs/2511.13384)
*Catalin Dumitrescu*

Main category: q-fin.GN

TL;DR: 研究罗马尼亚双货币经济中引入央行数字货币（CBDC）对金融稳定的影响，构建分析框架，得出适度设计的CBDC可引入且不影响金融稳定的结论。


<details>
  <summary>Details</summary>
Motivation: 探索在新兴双货币经济（罗马尼亚）中引入CBDC对金融稳定的潜在影响。

Method: 构建结合计量经济学、机器学习和行为建模的综合分析框架，用XGBoost和逻辑回归模型估计CBDC采用概率，进行流动性压力模拟，用VAR、MSVAR和SVAR模型捕捉流动性冲击的宏观金融传导。

Result: CBDC发行初期采用率适中，约10亿欧元，主要受数字就绪度和对央行信任驱动。

Conclusion: 非付息、有上限且主要作为支付手段的CBDC可引入且不影响金融稳定，双货币经济中需区分本外币数字货币持有上限，谨慎设计可使CBDC成为有益工具。

Abstract: This study explores the potential impact of introducing a Central Bank Digital Currency (CBDC) on financial stability in an emerging dual-currency economy (Romania), where the domestic currency (RON) coexists with the euro. It develops an integrated analytical framework combining econometrics, machine learning, and behavioural modelling. CBDC adoption probabilities are estimated using XGBoost and logistic regression models trained on behavioural and macro-financial indicators rather than survey data. Liquidity stress simulations assess how banks would respond to deposit withdrawals resulting from CBDC adoption, while VAR, MSVAR, and SVAR models capture the macro-financial transmission of liquidity shocks into credit contraction and changes in monetary conditions. The findings indicate that CBDC uptake (co-circulating Digital RON and Digital EUR) would be moderate at issuance, amounting to around EUR 1 billion, primarily driven by digital readiness and trust in the central bank. The study concludes that a non-remunerated, capped CBDC, designed primarily as a means of payment rather than a store of value, can be introduced without compromising financial stability. In dual currency economies, differentiated holding limits for domestic and foreign digital currencies (e.g., Digital RON versus Digital Euro) are crucial to prevent uncontrolled euroisation and preserve monetary sovereignty. A prudent design with moderate caps, non remuneration, and macroprudential coordination can transform CBDC into a digital liquidity buffer and a complementary monetary policy instrument that enhances resilience and inclusion rather than destabilising the financial system.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [545] [Omics-scale polymer computational database transferable to real-world artificial intelligence applications](https://arxiv.org/abs/2511.11626)
*Ryo Yoshida,Yoshihiro Hayashi,Hidemine Furuya,Ryohei Hosoya,Kazuyoshi Kaneko,Hiroki Sugisawa,Yu Kaneko,Aiko Takahashi,Yoh Noguchi,Shun Nanjo,Keiko Shinoda,Tomu Hamakawa,Mitsuru Ohno,Takuya Kitamura,Misaki Yonekawa,Stephen Wu,Masato Ohnishi,Chang Liu,Teruki Tsurimoto,Arifin,Araki Wakiuchi,Kohei Noda,Junko Morikawa,Teruaki Hayakawa,Junichiro Shiomi,Masanobu Naito,Kazuya Shiratori,Tomoki Nagai,Norio Tomotsu,Hiroto Inoue,Ryuichi Sakashita,Masashi Ishii,Isao Kuwajima,Kenji Furuichi,Norihiko Hiroi,Yuki Takemoto,Takahiro Ohkuma,Keita Yamamoto,Naoya Kowatari,Masato Suzuki,Naoya Matsumoto,Seiryu Umetani,Hisaki Ikebata,Yasuyuki Shudo,Mayu Nagao,Shinya Kamada,Kazunori Kamio,Taichi Shomura,Kensaku Nakamura,Yudai Iwamizu,Atsutoshi Abe,Koki Yoshitomi,Yuki Horie,Katsuhiko Koike,Koichi Iwakabe,Shinya Gima,Kota Usui,Gikyo Usuki,Takuro Tsutsumi,Keitaro Matsuoka,Kazuki Sada,Masahiro Kitabata,Takuma Kikutsuji,Akitaka Kamauchi,Yusuke Iijima,Tsubasa Suzuki,Takenori Goda,Yuki Takabayashi,Kazuko Imai,Yuji Mochizuki,Hideo Doi,Koji Okuwaki,Hiroya Nitta,Taku Ozawa,Hitoshi Kamijima,Toshiaki Shintani,Takuma Mitamura,Massimiliano Zamengo,Yuitsu Sugami,Seiji Akiyama,Yoshinari Murakami,Atsushi Betto,Naoya Matsuo,Satoru Kagao,Tetsuya Kobayashi,Norie Matsubara,Shosei Kubo,Yuki Ishiyama,Yuri Ichioka,Mamoru Usami,Satoru Yoshizaki,Seigo Mizutani,Yosuke Hanawa,Shogo Kunieda,Mitsuru Yambe,Takeru Nakamura,Hiromori Murashima,Kenji Takahashi,Naoki Wada,Masahiro Kawano,Yosuke Harada,Takehiro Fujita,Erina Fujita,Ryoji Himeno,Hiori Kino,Kenji Fukumizu*

Main category: physics.chem-ph

TL;DR: 本文介绍了PolyOmics这个组学规模的计算数据库，它为超$10^5$种聚合物材料提供多样物理性质，支持机器学习模型微调，其规模增大可提升模型泛化能力，为人工智能驱动的高分子科学奠定基础。


<details>
  <summary>Details</summary>
Motivation: 材料科学尤其是高分子研究在开发大规模开放数据集方面落后于自然语言处理等领域，原因是高分子合成和性能测量成本高、化学空间广泛复杂，需要开发数据库来推动发展。

Method: 通过全自动分子动力学模拟管道生成PolyOmics数据库，约260名来自48个机构的研究人员合作开发。

Result: 在PolyOmics上预训练的机器学习模型可针对下游任务高效微调，数据库规模增大时模拟到真实的迁移模型泛化能力显著提升，呈现幂律缩放。

Conclusion: 超大规模计算材料数据对提高现实预测性能有重要意义，PolyOmics数据库揭示了高分子材料未被探索的区域，为人工智能驱动的高分子科学提供基础。

Abstract: Developing large-scale foundational datasets is a critical milestone in advancing artificial intelligence (AI)-driven scientific innovation. However, unlike AI-mature fields such as natural language processing, materials science, particularly polymer research, has significantly lagged in developing extensive open datasets. This lag is primarily due to the high costs of polymer synthesis and property measurements, along with the vastness and complexity of the chemical space. This study presents PolyOmics, an omics-scale computational database generated through fully automated molecular dynamics simulation pipelines that provide diverse physical properties for over $10^5$ polymeric materials. The PolyOmics database is collaboratively developed by approximately 260 researchers from 48 institutions to bridge the gap between academia and industry. Machine learning models pretrained on PolyOmics can be efficiently fine-tuned for a wide range of real-world downstream tasks, even when only limited experimental data are available. Notably, the generalisation capability of these simulation-to-real transfer models improve significantly as the size of the PolyOmics database increases, exhibiting power-law scaling. The emergence of scaling laws supports the "more is better" principle, highlighting the significance of ultralarge-scale computational materials data for improving real-world prediction performance. This unprecedented omics-scale database reveals vast unexplored regions of polymer materials, providing a foundation for AI-driven polymer science.

</details>


### [546] [Socrates-Mol: Self-Oriented Cognitive Reasoning through Autonomous Trial-and-Error with Empirical-Bayesian Screening for Molecules](https://arxiv.org/abs/2511.11769)
*Xiangru Wang,Zekun Jiang,Heng Yang,Cheng Tan,Xingying Lan,Chunming Xu,Tianhang Zhou*

Main category: physics.chem-ph

TL;DR: 提出Socrates - Mol框架，将语言模型转变为经验贝叶斯推理器，用于分子属性预测，在胺溶剂LogP预测实验中有不同表现，能降低部署成本。


<details>
  <summary>Details</summary>
Motivation: 解决分子属性预测中的冷启动问题，为化工应用如溶剂筛选提供支持。

Method: 通过上下文工程将语言模型转变为经验贝叶斯推理器，实现反射 - 预测循环，引入与工业筛选优先级一致的排名任务，利用跨五个语言模型的跨模型自一致性。

Result: 在胺溶剂LogP预测实验中，回归任务MAE降低72%、R平方提高112%，排名任务因多模型系统偏差收益有限，框架比全微调降低超70%部署成本。

Conclusion: 该框架为分子属性预测提供可扩展解决方案，阐明自一致性机制的任务自适应性质。

Abstract: Molecular property prediction is fundamental to chemical engineering applications such as solvent screening. We present Socrates-Mol, a framework that transforms language models into empirical Bayesian reasoners through context engineering, addressing cold start problems without model fine-tuning. The system implements a reflective-prediction cycle where initial outputs serve as priors, retrieved molecular cases provide evidence, and refined predictions form posteriors, extracting reusable chemical rules from sparse data. We introduce ranking tasks aligned with industrial screening priorities and employ cross-model self-consistency across five language models to reduce variance. Experiments on amine solvent LogP prediction reveal task-dependent patterns: regression achieves 72% MAE reduction and 112% R-squared improvement through self-consistency, while ranking tasks show limited gains due to systematic multi-model biases. The framework reduces deployment costs by over 70% compared to full fine-tuning, providing a scalable solution for molecular property prediction while elucidating the task-adaptive nature of self-consistency mechanisms.

</details>


### [547] [Chemistry-Enhanced Diffusion-Based Framework for Small-to-Large Molecular Conformation Generation](https://arxiv.org/abs/2511.12182)
*Yifei Zhu,Jiahui Zhang,Jiawei Peng,Mengge Li,Chao Xu,Zhenggang Lan*

Main category: physics.chem-ph

TL;DR: 介绍基于扩散模型的框架StoL，可从小分子数据快速无先验知识地生成大分子结构。


<details>
  <summary>Details</summary>
Motivation: 量子化学层面获取真实多原子分子3D构象有挑战，现有机器学习预测大分子结构计算量大。

Method: StoL以LEGO方式从零组装分子，将分子分解为有效片段，用小分子训练的扩散模型生成3D结构并组装。

Result: 该基于片段的策略无需大分子训练数据，具有高可扩展性和可迁移性。

Conclusion: 通过嵌入化学原理，StoL收敛更快，结构合理，构象覆盖广，经DFT计算验证。

Abstract: Obtaining 3D conformations of realistic polyatomic molecules at the quantum chemistry level remains challenging, and although recent machine learning advances offer promise, predicting large-molecule structures still requires substantial computational effort. Here, we introduce StoL, a diffusion model-based framework that enables rapid and knowledge-free generation of large molecular structures from small-molecule data. Remarkably, StoL assembles molecules in a LEGO-style fashion from scratch, without seeing the target molecules or any structures of comparable size during training. Given a SMILES input, it decomposes the molecule into chemically valid fragments, generates their 3D structures with a diffusion model trained on small molecules, and assembles them into diverse conformations. This fragment-based strategy eliminates the need for large-molecule training data while maintaining high scalability and transferability. By embedding chemical principles into key steps, StoL ensures faster convergence, chemically rational structures, and broad configurational coverage, as confirmed against DFT calculations.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [548] [Systematic evaluation of time-frequency features for binaural sound source localization](https://arxiv.org/abs/2511.13487)
*Davoud Shariat Panah,Alessandro Ragano,Dan Barry,Jan Skoglund,Andrew Hines*

Main category: eess.AS

TL;DR: 研究双耳声源定位的时频特征设计，发现精心选择特征组合比增加模型复杂度更有效，低复杂度CNN模型用最优特征集有竞争力。


<details>
  <summary>Details</summary>
Motivation: 研究特征选择如何影响不同条件下双耳声源定位模型的性能。

Method: 用卷积神经网络（CNN）模型，研究基于幅度（幅度谱图、耳间级差 - ILD）和基于相位（相位谱图、耳间相位差 - IPD）特征的不同组合。

Result: 精心选择的特征组合常优于增加模型复杂度；ILD + IPD适合域内定位，泛化需更丰富输入；低复杂度CNN模型用最优特征集有竞争力。

Conclusion: 强调特征设计在双耳声源定位中的重要性，为特定领域和通用定位提供实用指导。

Abstract: This study presents a systematic evaluation of time-frequency feature design for binaural sound source localization (SSL), focusing on how feature selection influences model performance across diverse conditions. We investigate the performance of a convolutional neural network (CNN) model using various combinations of amplitude-based features (magnitude spectrogram, interaural level difference - ILD) and phase-based features (phase spectrogram, interaural phase difference - IPD). Evaluations on in-domain and out-of-domain data with mismatched head-related transfer functions (HRTFs) reveal that carefully chosen feature combinations often outperform increases in model complexity. While two-feature sets such as ILD + IPD are sufficient for in-domain SSL, generalization to diverse content requires richer inputs combining channel spectrograms with both ILD and IPD. Using the optimal feature sets, our low-complexity CNN model achieves competitive performance. Our findings underscore the importance of feature design in binaural SSL and provide practical guidance for both domain-specific and general-purpose localization.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [549] [ZX-DB: A Graph Database for Quantum Circuit Simplification and Rewriting via the ZX-Calculus](https://arxiv.org/abs/2511.13033)
*Valter Uotila,Cong Yu,Bo Zhao*

Main category: quant-ph

TL;DR: 介绍ZX - DB系统，用ZX - calculus在图数据库中进行量子电路简化和重写，实验显示有速度提升并暴露图数据库引擎瓶颈，开启新系统方向。


<details>
  <summary>Details</summary>
Motivation: 量子计算有潜力超越经典计算，量子程序需通过量子电路编译优化适配硬件，需有效方法处理大规模量子电路。

Method: 引入ZX - DB系统，将ZX - calculus重写规则编码为openCypher查询，在Memgraph上执行，集成正确性验证。

Result: 相比PyZX框架，独立重写时ZX - DB速度提升达一个数量级，暴露当前图数据库引擎模式匹配瓶颈。

Conclusion: ZX - DB将量子编译和图数据管理结合，开启了可扩展、数据库支持的量子计算管道的新系统方向。

Abstract: Quantum computing is an emerging computational paradigm with the potential to outperform classical computers in solving a variety of problems. To achieve this, quantum programs are typically represented as quantum circuits, which must be optimized and adapted for target hardware through quantum circuit compilation. We introduce ZX-DB, a data-driven system that performs quantum circuit simplification and rewriting inside a graph database using ZX-calculus, a complete graphical formalism for quantum mechanics. ZX-DB encodes ZX-calculus rewrite rules as standard openCypher queries and executes them on an example graph database engine, Memgraph, enabling efficient, database-native transformations of large-scale quantum circuits. ZX-DB integrates correctness validation via tensor and graph equivalence checks and is evaluated against the state-of-the-art PyZX framework. Experimental results show that ZX-DB achieves up to an order-of-magnitude speedup for independent rewrites, while exposing pattern-matching bottlenecks in current graph database engines. By uniting quantum compilation and graph data management, ZX-DB opens a new systems direction toward scalable, database-supported quantum computing pipelines.

</details>


### [550] [Approximate Message Passing for Quantum State Tomography](https://arxiv.org/abs/2511.12857)
*Noah Siekierski,Kausthubh Chandramouli,Christian Kümmerle,Bojko N. Bakalov,Dror Baron*

Main category: quant-ph

TL;DR: 本文展示了近似消息传递（AMP）技术可用于低秩量子态层析成像（QST），能降低重构误差，还进行了实验并考虑设备噪声影响。


<details>
  <summary>Details</summary>
Motivation: 传统QST协议成本随系统规模指数增长，需开发针对特定结构量子态的方法，如低秩态。

Method: 使用压缩传感技术AMP进行低秩QST，并对AMP算法进行合理设计。

Result: 相比现有低秩QST方法，可将重构误差降低一个数量级以上，还在IBM Kingston上进行了层析实验。

Conclusion: 推进了低秩QST的发展，可能适用于其他量子层析协议。

Abstract: Quantum state tomography (QST) is an indispensable tool for characterizing many-body quantum systems. However, due to the exponential scaling cost of the protocol with system size, many approaches have been developed for quantum states with specific structure, such as low-rank states. In this paper, we show how approximate message passing (AMP), a compressed sensing technique, can be used to perform low-rank QST. AMP provides asymptotically optimal performance guarantees for large systems, which suggests its utility for QST. We discuss the design challenges that come with applying AMP to QST, and show that by properly designing the AMP algorithm, we can reduce the reconstruction infidelity by over an order of magnitude compared to existing approaches to low-rank QST. We also performed tomographic experiments on IBM Kingston and considered the effect of device noise on the reliability of the predicted fidelity of state preparation. Our work advances the state of low-rank QST and may be applicable to other quantum tomography protocols.

</details>


### [551] [Towards Quantum Software for Quantum Simulation](https://arxiv.org/abs/2511.13520)
*Maja Franz,Lukas Schmidbauer,Joshua Ammermann,Ina Schaefer,Wolfgang Mauerer*

Main category: quant-ph

TL;DR: 本文指出量子模拟软件栈存在关键差距，提倡模块化模型驱动工程方法，并给出量子模拟框架愿景。


<details>
  <summary>Details</summary>
Motivation: 当前量子模拟技术应用限于基础科学，缺乏软件工程社区提供的基础设施和建模抽象。

Method: 采用模块化模型驱动工程（MDE）方法，支持不同类型量子模拟。

Result: 通过高能物理示例，勾勒出能支持可扩展、跨平台模拟工作流的量子模拟框架愿景。

Conclusion: 模块化模型驱动工程方法有助于解决量子模拟软件栈的问题，推动量子模拟技术发展。

Abstract: Quantum simulation is a leading candidate for demonstrating practical quantum advantage over classical computation, as it is believed to provide exponentially more compute power than any classical system. It offers new means of studying the behaviour of complex physical systems, for which conventionally software-intensive simulation codes based on numerical high-performance computing are used. Instead, quantum simulations map properties and characteristics of subject systems, for instance chemical molecules, onto quantum devices that then mimic the system under study.
  Currently, the use of these techniques is largely limited to fundamental science, as the overall approach remains tailored for specific problems: We lack infrastructure and modelling abstractions that are provided by the software engineering community for other computational domains.
  In this paper, we identify critical gaps in the quantum simulation software stack-particularly the absence of general-purpose frameworks for model specification, Hamiltonian construction, and hardware-aware mappings. We advocate for a modular model-driven engineering (MDE) approach that supports different types of quantum simulation (digital and analogue), and facilitates automation, performance evaluation, and reusability. Through an example from high-energy physics, we outline a vision for a quantum simulation framework capable of supporting scalable, cross-platform simulation workflows.

</details>


### [552] [Reinforcement Learning for Charging Optimization of Inhomogeneous Dicke Quantum Batteries](https://arxiv.org/abs/2511.12176)
*Xiaobin Song,Siyuan Bai,Da-Wei Wang,Hanxiao Tao,Xizhe Wang,Rebing Wu,Benben Jiang*

Main category: quant-ph

TL;DR: 本文采用强化学习优化非均匀Dicke电池的分段恒定充电策略，比较四种可观测性制度下的策略，发现二阶相关性可弥补部分可观测性差距，学习的调度是非短视的。


<details>
  <summary>Details</summary>
Motivation: 充电优化是量子电池实施的关键挑战，特别是在非均匀性和部分可观测性下。

Method: 采用强化学习优化非均匀Dicke电池的分段恒定充电策略，系统比较四种可观测性制度下的策略。

Result: 全可观测性产生接近最优的ergotropy且变异性低；部分可观测性下，仅访问单TLS能量或能量加一阶平均值落后于全观测基线；增加二阶相关性可弥补大部分差距，达到全状态基线的94%-98%；学习的调度是非短视的。

Conclusion: 这些发现为现实信息约束下的有效快速充电协议提供了实用途径。

Abstract: Charging optimization is a key challenge to the implementation of quantum batteries, particularly under inhomogeneity and partial observability. This paper employs reinforcement learning to optimize piecewise-constant charging policies for an inhomogeneous Dicke battery. We systematically compare policies across four observability regimes, from full-state access to experimentally accessible observables (energies of individual two-level systems (TLSs), first-order averages, and second-order correlations). Simulation results demonstrate that full observability yields near-optimal ergotropy with low variability, while under partial observability, access to only single-TLS energies or energies plus first-order averages lags behind the fully observed baseline. However, augmenting partial observations with second-order correlations recovers most of the gap, reaching 94%-98% of the full-state baseline. The learned schedules are nonmyopic, trading temporary plateaus or declines for superior terminal outcomes. These findings highlight a practical route to effective fast-charging protocols under realistic information constraints.

</details>


### [553] [Quantum Optimization Algorithms](https://arxiv.org/abs/2511.12379)
*Jonas Stein,Maximilian Zorn,Leo Sünkel,Thomas Gabor*

Main category: quant-ph

TL;DR: 本文介绍量子近似优化算法（QAOA），包括其电路实现、参数训练、约束融入方法，还提及变分量子本征求解器（VQE）及其挑战。


<details>
  <summary>Details</summary>
Motivation: 量子优化在特定问题上有指数级加速，研究关键算法QAOA。

Method: 深入探讨QAOA的量子电路实现、使用参数偏移规则进行参数训练，用Grover混合器融入约束，介绍VQE作为QAOA的推广。

Result: 给出Pennylane代码实现最大割问题的示例，展示了融入约束的方法。

Conclusion: 强调VQE在NISQ时代的潜力，指出存在贫瘠高原和ansatz设计等挑战。

Abstract: Quantum optimization allows for up to exponential quantum speedups for specific, possibly industrially relevant problems. As the key algorithm in this field, we motivate and discuss the Quantum Approximate Optimization Algorithm (QAOA), which can be understood as a slightly generalized version of Quantum Annealing for gate-based quantum computers. We delve into the quantum circuit implementation of the QAOA, including Hamiltonian simulation techniques for higher-order Ising models, and discuss parameter training using the parameter shift rule. An example implementation with Pennylane source code demonstrates practical application for the Maximum Cut problem. Further, we show how constraints can be incorporated into the QAOA using Grover mixers, allowing to restrict the search space to strictly valid solutions for specific problems. Finally, we outline the Variational Quantum Eigensolver (VQE) as a generalization of the QAOA, highlighting its potential in the NISQ era and addressing challenges such as barren plateaus and ansatz design.

</details>


### [554] [Limitations of Quantum Advantage in Unsupervised Machine Learning](https://arxiv.org/abs/2511.10709)
*Apoorva D. Patel*

Main category: quant-ph

TL;DR: 探讨机器学习模型大数据模式识别，分析量子扩展模型量子优势及限制。


<details>
  <summary>Details</summary>
Motivation: 研究量子扩展模型在大数据模式识别中能否取得比经典模型更大优势。

Method: 将经典模型中经典概率分布替换为量子密度矩阵，分析特定情况下密度矩阵特性。

Result: 发现量子优势依赖输入数据和目标可观测对象，有明确限制情况。

Conclusion: 量子优势因问题而异，对数据分析和传感应用有影响。

Abstract: Machine learning models are used for pattern recognition analysis of big data, without direct human intervention. The task of unsupervised learning is to find the probability distribution that would best describe the available data, and then use it to make predictions for observables of interest. Classical models generally fit the data to Boltzmann distribution of Hamiltonians with a large number of tunable parameters. Quantum extensions of these models replace classical probability distributions with quantum density matrices. An advantage can be obtained only when features of density matrices that are absent in classical probability distributions are exploited. Such situations depend on the input data as well as the targeted observables. Explicit examples are discussed that bring out the constraints limiting possible quantum advantage. The problem-dependent extent of quantum advantage has implications for both data analysis and sensing applications.

</details>


### [555] [Discovering autonomous quantum error correction via deep reinforcement learning](https://arxiv.org/abs/2511.12482)
*Yue Yin,Tailong Xiao,Xiaoyang Deng,Ming He,Jianping Fan,Guihua Zeng*

Main category: quant-ph

TL;DR: 利用课程学习的深度强化学习在近似自主量子纠错框架下发现玻色子码，能抵抗单光子和双光子损失，表现优异且分析了鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 标准量子纠错方法依赖主动测量可能引入额外错误，自主量子纠错虽有优势但确定实用编码困难。

Method: 利用课程学习的深度强化学习，给出近似条件下主方程解析解加速训练，分两阶段训练智能体。

Result: 发现最优码字集合（Fock态|4⟩和|7⟩），超越盈亏平衡点，达到最优性能，分析了码对相位阻尼和振幅阻尼噪声的鲁棒性。

Conclusion: 课程学习的深度强化学习在发现最优量子纠错码方面有潜力，尤其适用于早期容错量子系统。

Abstract: Quantum error correction is essential for fault-tolerant quantum computing. However, standard methods relying on active measurements may introduce additional errors. Autonomous quantum error correction (AQEC) circumvents this by utilizing engineered dissipation and drives in bosonic systems, but identifying practical encoding remains challenging due to stringent Knill-Laflamme conditions. In this work, we utilize curriculum learning enabled deep reinforcement learning to discover Bosonic codes under approximate AQEC framework to resist both single-photon and double-photon losses. We present an analytical solution of solving the master equation under approximation conditions, which can significantly accelerate the training process of reinforcement learning. The agent first identifies an encoded subspace surpassing the breakeven point through rapid exploration within a constrained evolutionary time-frame, then strategically fine-tunes its policy to sustain this performance advantage over extended temporal horizons. We find that the two-phase trained agent can discover the optimal set of codewords, i.e., the Fock states $\ket{4}$ and $\ket{7}$ considering the effect of both single-photon and double-photon loss. We identify that the discovered code surpasses the breakeven threshold over a longer evolution time and achieve the state-of-art performance. We also analyze the robustness of the code against the phase damping and amplitude damping noise. Our work highlights the potential of curriculum learning enabled deep reinforcement learning in discovering the optimal quantum error correct code especially in early fault-tolerant quantum systems.

</details>


### [556] [Taming Barren Plateaus in Arbitrary Parameterized Quantum Circuits Without Sacrificing Expressibility](https://arxiv.org/abs/2511.13408)
*Zhenyu Chen,Yuguo Shao,Zhengwei Liu,Zhaohui Wei*

Main category: quant-ph

TL;DR: 提出消除任意参数化量子电路（PQC）中贫瘠高原现象的通用且硬件高效方法，证明修改后电路（MPQC）的有效性和抗噪声能力，并通过数值验证方法实用性。


<details>
  <summary>Details</summary>
Motivation: 现有PQC架构面临包括“贫瘠高原”现象等挑战，影响有效参数优化。

Method: 在原始PQC中插入一层易实现的量子通道，每个通道仅需一个辅助量子比特和四个额外门，得到MPQC。

Result: MPQC至少与原始PQC表达能力相当，在温和假设下无贫瘠高原，参数可训练，抗噪声，数值实验表明在多达100个量子比特和2400层电路中有效消除贫瘠高原。

Conclusion: 提出的方法可有效消除PQC中的贫瘠高原现象，适用于当前含噪声中等规模量子（NISQ）硬件。

Abstract: Quantum algorithms based on parameterized quantum circuits (PQCs) have enabled a wide range of applications on near-term quantum devices. However, existing PQC architectures face several challenges, among which the ``barren plateaus" phenomenon is particularly prominent. In such cases, the loss function concentrates exponentially with increasing system size, thereby hindering effective parameter optimization. To address this challenge, we propose a general and hardware-efficient method for eliminating barren plateaus in an arbitrary PQC. Specifically, our approach achieves this by inserting a layer of easily implementable quantum channels into the original PQC, each channel requiring only one ancilla qubit and four additional gates, yielding a modified PQC (MPQC) that is provably at least as expressive as the original PQC and, under mild assumptions, is guaranteed to be free from barren plateaus. Furthermore, by appropriately adjusting the structure of MPQCs, we rigorously prove that any parameter in the original PQC can be made trainable. Importantly, the absence of barren plateaus in MPQCs is robust against realistic noise, making our approach directly applicable to current noisy intermediate-scale quantum (NISQ) hardware. Numerically, we demonstrate the practicality of our method by modifying a commonly used PQC for thermal-state preparation. The results show that {barren plateaus are effectively eliminated} in this class of circuits with up to 100 qubits and 2400 layers, whereas the original ansatz suffers from severe gradient vanishing.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [557] [Auto-encoder model for faster generation of effective one-body gravitational waveform approximations](https://arxiv.org/abs/2511.12642)
*Suyog Garg,Feng-Li Lin,Kipp Cannon*

Main category: gr-qc

TL;DR: 本文提出基于Liao+2021架构的条件变分自编码器模型，用于快速生成引力波波形，速度比原生实现快2 - 3个数量级，为开发机器学习框架迈出第一步。


<details>
  <summary>Details</summary>
Motivation: 下一代引力波探测器灵敏度和事件率提升，源参数估计面临计算挑战，需加速似然计算。

Method: 采用条件变分自编码器模型，对包含四个参数的参数空间进行训练，使用约10^5个输入波形数据，按70%/10%/20%进行训练、验证和测试划分。

Result: 测试数据集生成波形的中位失配约为10^(-2)，在受限参数空间表现更好；模型能在0.1秒内生成100个波形，平均每个约4.46毫秒；模型潜在采样不确定性的平均失配偏差为2×10^(-1)。

Conclusion: 该工作是开发用于快速生成引力波形近似的机器学习框架的第一步。

Abstract: Upgrades to current gravitational wave detectors for the next observation run and upcoming third-generation observatories, like the Einstein telescope, are expected to have enormous improvements in detection sensitivities and compact object merger event rates. Estimation of source parameters for a wider parameter space that these detectable signals will lie in, will be a computational challenge. Thus, it is imperative to have methods to speed-up the likelihood calculations with theoretical waveform predictions, which can ultimately make the parameter estimation faster and aid in rapid multi-messenger follow-ups. Towards this end, we present a conditional variational auto-encoder model, based on the best performing architecture of Liao+2021, for faster generation of aligned-spin SEOBNRv4 inspiral-merger-ringdown waveforms. Our parameter space consists of four parameters, [$m_1$, $m_2$, $χ_1(z)$, $χ_2(z)$]. The masses are uniformly sampled in $[5,75]\,M_{\odot}$ with a mass ratio limit at $10\,M_{\odot}$, while the spins are uniform in $[-0.99,0.99]$. We train the model using $\sim10^5$ input waveforms data with a 70\%/10\% train/validation split, while 20\% data are reserved for testing. The median mismatch for the generated waveforms in the test dataset is $\sim10^{-2}$, with better performance in a restricted parameter space of $χ_{\rm eff}\in[-0.80,0.80]$. Our model is able to generate 100 waveforms in 0.1 second at an average speed of about 4.46 ms per waveform. This is 2-3 orders of magnitude faster than the native SEOBNRv4 implementation in lalsimulation. The latent sampling uncertainty of our model can be quantified with a mean mismatch deviation of $2\times10^{-1}$ for 1000 generations of the same waveform. Our work aims to be the first step towards developing a production-ready machine learning framework for the faster generation of gravitational waveform approximations.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [558] [Consumer Choice Over Shopping Baskets: A Linear Demand Approach](https://arxiv.org/abs/2511.11846)
*Afonso Rodrigues*

Main category: econ.GN

TL;DR: 传统需求估计方法有缺陷，作者引入新方法，用联合购买频率开发代理矩阵，对葡萄牙杂货店样本测试，结果匹配实际情况，发现加价围绕稳定均值波动。


<details>
  <summary>Details</summary>
Motivation: 传统需求估计方法存在施加不现实选择约束、误判市场边界等问题，影响价格弹性估计、市场力量和传导理解。

Method: 引入新方法，根据消费者考虑集结构确定选择结果，开发基于联合购买频率的Slutsky矩阵代理。

Result: 对葡萄牙杂货店样本测试，结果与价格波动、利润率调查和消费者口味变化报告相符，加价围绕稳定均值波动，峰值和低谷与新冠相关事件对应。

Conclusion: 新方法能有效进行需求估计，反映市场实际情况。

Abstract: Popular demand estimation approaches impose unrealistic choice constraints and misstate market boundaries, biasing price elasticity estimates and affecting our understanding of market power and pass-throughs. I introduce a novel, scalable method that conditions the outcome of consumers' choice on the structure of their consideration set: a function of all the combinations of goods - shopping baskets - considered each purchase instance by consumers. I show that if consideration sets bind consumers' consumption bundles, joint purchases induce substitutes and complements, possibly making market concentration welfare-increasing. To allow for demand estimation across product categories while addressing dimensionality concerns, I develop a Slutsky matrix proxy from joint-purchase frequencies. I test the model's predictions and jointly determine price elasticities for 20 000 goods across 500 product categories for a Portuguese grocery store sample from 2020-23. The results match observed price volatility, profit margin surveys, as well as reports on shifting consumer tastes during the sample period. Mark-ups are found to have remained volatile around a stable mean, with peaks and troughs corresponding to COVID-related events.

</details>


### [559] [Decision and Gender Biases in Large Language Models: A Behavioral Economic Perspective](https://arxiv.org/abs/2511.12319)
*Luca Corazzini,Elisa Deriu,Marco Guerzoni*

Main category: econ.GN

TL;DR: 研究先进大语言模型在经典决策问题中是否像理性主体，用两个实验测试两个模型，发现模型有偏离理性的表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型常被认为能理性决策，但训练语料可能含偏见，研究其在经典决策问题中是否像理性主体或重现人类行为倾向。

Method: 使用行为经济学的最后通牒博弈和赌博游戏，在中性和性别条件提示下从Google Gemma7B和Qwen两个模型获取决策，估计不公平厌恶和损失厌恶参数并与人类基准比较。

Result: 模型存在偏离理性的情况，有适度公平关注、轻微损失厌恶和微妙的性别条件差异。

Conclusion: 先进大语言模型虽有一定理性，但仍存在偏离理性的持续表现。

Abstract: Large language models (LLMs) increasingly mediate economic and organisational processes, from automated customer support and recruitment to investment advice and policy analysis. These systems are often assumed to embody rational decision making free from human error; yet they are trained on human language corpora that may embed cognitive and social biases. This study investigates whether advanced LLMs behave as rational agents or whether they reproduce human behavioural tendencies when faced with classic decision problems. Using two canonical experiments in behavioural economics, the ultimatum game and a gambling game, we elicit decisions from two state of the art models, Google Gemma7B and Qwen, under neutral and gender conditioned prompts. We estimate parameters of inequity aversion and loss-aversion and compare them with human benchmarks. The models display attenuated but persistent deviations from rationality, including moderate fairness concerns, mild loss aversion, and subtle gender conditioned differences.

</details>


### [560] [The Impact of Phosphate Fertilizer Industry Consolidation on Future Phosphorus Supply for World Agriculture](https://arxiv.org/abs/2511.13123)
*Anna Shchiptsova,Michael Obersteiner*

Main category: econ.GN

TL;DR: 为评估新贸易结构对未来区域磷供应的影响，模拟磷酸铵市场行为，结果显示2030年全球需求分布将强化，市场集中度有变化，部分市场需额外投资。


<details>
  <summary>Details</summary>
Motivation: 评估新的磷酸盐肥料贸易结构对未来区域层面磷供应的影响。

Method: 在粮农组织全球集约农业演变情景下模拟磷酸铵市场行为，用多对多匹配市场表示市场微观结构，进行自举模拟。

Result: 2030年全球磷酸铵需求空间分布将强化，模拟显示分布式市场密度降低，小规模市场集中度预计增加，大规模市场保持稳定，供应方大型多市场供应商集中于更少市场，部分市场本地供应商进口替代率高。

Conclusion: 部分市场需要额外的区域层面资本投资。

Abstract: The addition of phosphorus, in the form of mineral fertilizer, becomes necessary in most agricultural soils in order to achieve consistent high yield levels of intensive farming and maintain soil fertility. Recent consolidation of phosphate fertilizer industry has transformed fragmented trade into a single integrated global network, where a small group of large-scale companies dominates the international market for phosphate commodity fertilizers. To assess the impact of new trade structure on future region-level phosphorus supply, we simulate behavior of markets for ammonium phosphates in the FAO scenarios of global intensive farming evolution. Details of market microstructure are represented here by a many-to-many matching market. Current spatial distribution of global demand in ammonium phosphates is projected to strengthen by 2030. Bootstrap simulations produce similar network structures for both scenarios showing reduction in the density of the distributed market. In response to the non-uniform demand growth across regions, market concentration is expected to increase for small-scale markets, and to remain predominantly stable for large-scale markets; on the supply side, simulated equilibria point out large-scale multi-market suppliers concentrating on fewer markets than before. A high rate of import substitution by local suppliers in some markets indicate the need of additional region-level capital investment.

</details>


### [561] [A Price to Enter: Anticipatory Housing Market Sorting and Access Inequality under New York's Congestion Pricing](https://arxiv.org/abs/2511.13200)
*Mingzhi Xiao,Yuki Takayama*

Main category: econ.GN

TL;DR: 研究纽约市拥堵收费对住房市场和空间公平性的影响，发现政策宣布后短期内房价和租金下降，后期有市场分化，提出公平设计建议。


<details>
  <summary>Details</summary>
Motivation: 研究拥堵收费如何塑造纽约市住房市场结果和空间公平性。

Method: 使用高频销售和租赁数据，结合倾向得分匹配双重差分、地理回归断点和事件研究设计。

Result: 收费区住房价格下降约3.3%，租金下降3%，政策宣布后立即下降最明显，后期影响减弱，优质房产有价格韧性；地理回归断点结果显示有明显边界惩罚；租房者和低价值房产受早期调整压力更大，实施阶段影响有限。

Conclusion: 拥堵定价可重塑城市空间，公平导向设计（包括支持边界社区和租房者、将收入再投资于非收费公交）对共享政策利益很重要。

Abstract: This study examines how congestion pricing shapes housing market outcomes and spatial equity in New York City. Using high-frequency sales and rental data and a combination of propensity score matching difference-in-differences, geographic regression discontinuity, and event study designs, the analysis identifies distinct short-run adjustment patterns triggered by the policy announcement. Housing prices inside the toll zone fell by about 3.3% and rents by 3%, with the sharpest declines occurring immediately after the announcement. These effects weakened over time, and price resilience emerged among premium properties, indicating early market sorting and growing segmentation. The Geo-RDD results show a clear boundary penalty, with properties just inside the cordon experiencing more pronounced declines than otherwise similar properties just outside. Renters and lower-value segments were more exposed to early adjustment pressures, while implementation-stage effects were limited. The findings suggest that congestion pricing can reshape urban space not only by altering mobility incentives but also by redistributing access and opportunity. Equity-oriented design that includes early-stage support for boundary neighborhoods and renters, along with reinvestment of revenues into untolled transit access, is important for ensuring that the benefits of congestion pricing are shared rather than concentrated.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [562] [DIGing--SGLD: Decentralized and Scalable Langevin Sampling over Time--Varying Networks](https://arxiv.org/abs/2511.12836)
*Waheed U. Bajwa,Mert Gurbuzbalaban,Mustafa Ali Kutbay,Lingjiong Zhu,Muhammad Zulqarnain*

Main category: math.OC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Sampling from a target distribution induced by training data is central to Bayesian learning, with Stochastic Gradient Langevin Dynamics (SGLD) serving as a key tool for scalable posterior sampling and decentralized variants enabling learning when data are distributed across a network of agents. This paper introduces DIGing-SGLD, a decentralized SGLD algorithm designed for scalable Bayesian learning in multi-agent systems operating over time-varying networks. Existing decentralized SGLD methods are restricted to static network topologies, and many exhibit steady-state sampling bias caused by network effects, even when full batches are used. DIGing-SGLD overcomes these limitations by integrating Langevin-based sampling with the gradient-tracking mechanism of the DIGing algorithm, originally developed for decentralized optimization over time-varying networks, thereby enabling efficient and bias-free sampling without a central coordinator. To our knowledge, we provide the first finite-time non-asymptotic Wasserstein convergence guarantees for decentralized SGLD-based sampling over time-varying networks, with explicit constants. Under standard strong convexity and smoothness assumptions, DIGing-SGLD achieves geometric convergence to an $O(\sqrtη)$ neighborhood of the target distribution, where $η$ is the stepsize, with dependence on the target accuracy matching the best-known rates for centralized and static-network SGLD algorithms using constant stepsize. Numerical experiments on Bayesian linear and logistic regression validate the theoretical results and demonstrate the strong empirical performance of DIGing-SGLD under dynamically evolving network conditions.

</details>


### [563] [A Computational Method for Solving the Stochastic Joint Replenishment Problem in High Dimensions](https://arxiv.org/abs/2511.11830)
*Barış Ata,Wouter van Eekelen,Yuan Zhong*

Main category: math.OC

TL;DR: 本文考虑高维随机联合补货问题的离散时间公式，提出基于深度神经网络的计算方法，所提库存控制策略表现良好且计算可行维度达50。


<details>
  <summary>Details</summary>
Motivation: 解决高维随机联合补货问题。

Method: 将问题近似为连续时间脉冲控制问题，利用脉冲控制问题、带跳的倒向随机微分方程和随机目标问题的联系，用基于深度神经网络的模拟计算方法求解脉冲控制问题，基于此为原离散时间问题提出库存控制策略。

Result: 所提方法在已研究问题中匹配或超越最佳基准，计算可行维度至少达50。

Conclusion: 提出的方法在高维随机联合补货问题上有效且具有一定计算可行性。

Abstract: We consider a discrete-time formulation for a class of high-dimensional stochastic joint replenishment problems. First, we approximate the problem by a continuous-time impulse control problem. Exploiting connections among the impulse control problem, backward stochastic differential equations (BSDEs) with jumps, and the stochastic target problem, we develop a novel, simulation-based computational method that relies on deep neural networks to solve the impulse control problem. Based on that solution, we propose an implementable inventory control policy for the original (discrete-time) stochastic joint replenishment problem, and test it against the best available benchmarks in a series of test problems. For the problems studied thus far, our method matches or beats the best benchmark we could find, and it is computationally feasible up to at least 50 dimensions -- that is, 50 stock-keeping units (SKUs).

</details>


### [564] [DLMMPR:Deep Learning-based Measurement Matrix for Phase Retrieval](https://arxiv.org/abs/2511.12556)
*Jing Liu,Bing Guo,Ren Zhu*

Main category: math.OC

TL;DR: 本文将学习优化集成到相位恢复的测量矩阵设计中，提出DLMMPR算法，经实证验证其效果良好，优于对比方法。


<details>
  <summary>Details</summary>
Motivation: 将学习优化集成到相位恢复的测量矩阵设计中，以提高相位恢复效果。

Method: 引入基于深度学习的相位恢复测量矩阵（DLMMPR）算法，在端到端深度学习架构中对测量矩阵进行参数化，并结合次梯度下降和近端映射模块。

Result: 在不同噪声环境下的综合实证验证确认了DLMMPR的有效性，与DeepMMSE和PrComplex相比，在PSNR和SSIM方面有显著提升。

Conclusion: DLMMPR算法具有优越性。

Abstract: This paper pioneers the integration of learning optimization into measurement matrix design for phase retrieval. We introduce the Deep Learning-based Measurement Matrix for Phase Retrieval (DLMMPR) algorithm, which parameterizes the measurement matrix within an end-to-end deep learning architecture. Synergistically augmented with subgradient descent and proximal mapping modules for robust recovery, DLMMPR's efficacy is decisively confirmed through comprehensive empirical validation across diverse noise regimes. Benchmarked against DeepMMSE and PrComplex, our method yields substantial gains in PSNR and SSIM, underscoring its superiority.

</details>


### [565] [Power Homotopy for Zeroth-Order Non-Convex Optimizations](https://arxiv.org/abs/2511.13592)
*Chen Xu*

Main category: math.OC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce GS-PowerHP, a novel zeroth-order method for non-convex optimization problems of the form $\max_{x \in \mathbb{R}^d} f(x)$. Our approach leverages two key components: a power-transformed Gaussian-smoothed surrogate $F_{N,σ}(μ) = \mathbb{E}_{x\sim\mathcal{N}(μ,σ^2 I_d)}[e^{N f(x)}]$ whose stationary points cluster near the global maximizer $x^*$ of $f$ for sufficiently large $N$, and an incrementally decaying $σ$ for enhanced data efficiency. Under mild assumptions, we prove convergence in expectation to a small neighborhood of $x^*$ with the iteration complexity of $O(d^2 \varepsilon^{-2})$. Empirical results show our approach consistently ranks among the top three across a suite of competing algorithms. Its robustness is underscored by the final experiment on a substantially high-dimensional problem ($d=150,528$), where it achieved first place on least-likely targeted black-box attacks against images from ImageNet, surpassing all competing methods.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [566] [Optimal Multi-Constrained Workflow Scheduling for Cyber-Physical Systems in the Edge-Cloud Continuum](https://arxiv.org/abs/2511.07466)
*Andreas Kouloumpris,Georgios L. Stavrinides,Maria K. Michael,Theocharis Theocharides*

Main category: cs.NI

TL;DR: 提出优化调度方法以最小化边缘 - 枢纽 - 云网络物理系统中工作流应用的总延迟，实验显示该方法优于启发式算法。


<details>
  <summary>Details</summary>
Motivation: 边缘 - 枢纽 - 云范式因网络边缘设备的异构性及有限的计算、通信和能源能力等带来挑战，需解决工作流应用的延迟问题。

Method: 提出基于连续时间混合整数线性规划的优化调度方法，考虑多设备合作及多种约束，并与改进的启发式算法对比。

Result: 该方法在真实用例中平均延迟改善 13.54%，在不同规模的合成工作流中平均延迟降低 33.03%。

Conclusion: 提出的优化调度方法优于启发式算法，且具有可扩展性。

Abstract: The emerging edge-hub-cloud paradigm has enabled the development of innovative latency-critical cyber-physical applications in the edge-cloud continuum. However, this paradigm poses multiple challenges due to the heterogeneity of the devices at the edge of the network, their limited computational, communication, and energy capacities, as well as their different sensing and actuating capabilities. To address these issues, we propose an optimal scheduling approach to minimize the overall latency of a workflow application in an edge-hub-cloud cyber-physical system. We consider multiple edge devices cooperating with a hub device and a cloud server. All devices feature heterogeneous multicore processors and various sensing, actuating, or other specialized capabilities. We present a comprehensive formulation based on continuous-time mixed integer linear programming, encapsulating multiple constraints often overlooked by existing approaches. We conduct a comparative experimental evaluation between our method and a well-established and effective scheduling heuristic, which we enhanced to consider the constraints of the specific problem. The results reveal that our technique outperforms the heuristic, achieving an average latency improvement of 13.54% in a relevant real-world use case, under varied system configurations. In addition, the results demonstrate the scalability of our method under synthetic workflows of varying sizes, attaining a 33.03% average latency decrease compared to the heuristic.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [567] [AI-Open-RAN for Non-Terrestrial Networks](https://arxiv.org/abs/2511.11947)
*Tri Nhu Do*

Main category: eess.SP

TL;DR: 提出AIO - RAN - NTN概念，介绍相关架构，通过实验表明AIO架构对移动性敏感，可通过AI缓解。


<details>
  <summary>Details</summary>
Motivation: 提升下一代电信的互操作性、灵活性和智能性。

Method: 概述Open - RAN和AI - RAN架构，介绍AIO - RAN - NTN蓝图，用OpenAirInterface平台做测试传输，训练AI模型预测KPI。

Result: AIO - based SA架构对移动性敏感，即使低速也受影响。

Conclusion: AI驱动的KPI预测可缓解AIO - based SA架构对移动性的敏感问题。

Abstract: In this paper, we propose the concept of AIO-RAN-NTN, a unified all-in-one Radio Access Network (RAN) for Non-Terrestrial Networks (NTNs), built on an open architecture that leverages open interfaces and artificial intelligence (AI)-based functionalities. This approach advances interoperability, flexibility, and intelligence in next-generation telecommunications. First, we provide a concise overview of the state-of-the-art architectures for Open-RAN and AI-RAN, highlighting key network functions and infrastructure elements. Next, we introduce our integrated AIO-RAN-NTN blueprint, emphasizing how internal and air interfaces from AIO-RAN and the 3rd Generation Partnership Project (3GPP) can be applied to emerging environments such as NTNs. To examine the impact of mobility on AIO-RAN, we implement a testbed transmission using the OpenAirInterface platform for a standalone (SA) New Radio (NR) 5G system. We then train an AI model on realistic data to forecast key performance indicators (KPIs). Our experiments demonstrate that the AIO-based SA architecture is sensitive to mobility, even at low speeds, but this limitation can be mitigated through AI-driven KPI forecasting.

</details>


### [568] [Temporal Micro-Doppler Spectrogram-based ViT Multiclass Target Classification](https://arxiv.org/abs/2511.11951)
*Nghia Thinh Nguyen,Tri Nhu Do*

Main category: eess.SP

TL;DR: 提出T - MDS - ViT用于毫米波FMCW雷达微多普勒频谱图多类目标分类，效果优于现有CNN方法。


<details>
  <summary>Details</summary>
Motivation: 解决毫米波FMCW雷达微多普勒频谱图多类目标分类问题，在目标重叠和部分遮挡下保持可分离性。

Method: 设计基于Transformer的架构，通过补丁嵌入和跨轴注意力机制处理堆叠的RVA时空张量，在注意力层利用移动感知约束，并应用可解释机制。

Result: 所提框架在分类准确率上优于现有基于CNN的方法，数据效率更高且可实时部署。

Conclusion: T - MDS - ViT在毫米波FMCW雷达微多普勒频谱图多类目标分类任务中有良好表现。

Abstract: In this paper, we propose a new Temporal MDS-Vision Transformer (T-MDS-ViT) for multiclass target classification using millimeter-wave FMCW radar micro-Doppler spectrograms. Specifically, we design a transformer-based architecture that processes stacked range-velocity-angle (RVA) spatiotemporal tensors via patch embeddings and cross-axis attention mechanisms to explicitly model the sequential nature of MDS data across multiple frames. The T-MDS-ViT exploits mobility-aware constraints in its attention layer correspondences to maintain separability under target overlaps and partial occlusions. Next, we apply an explainable mechanism to examine how the attention layers focus on characteristic high-energy regions of the MDS representations and their effect on class-specific kinematic features. We also demonstrate that our proposed framework is superior to existing CNN-based methods in terms of classification accuracy while achieving better data efficiency and real-time deployability.

</details>


### [569] [Informed Bootstrap Augmentation Improves EEG Decoding](https://arxiv.org/abs/2511.12073)
*Woojae Jeong,Wenhui Cui,Kleanthis Avramidis,Takfarinas Medani,Shrikanth Narayanan,Richard Leahy*

Main category: eess.SP

TL;DR: 提出加权自举法增强EEG特征表示，在句子评估范式中提升了解码准确率。


<details>
  <summary>Details</summary>
Motivation: EEG受噪声和试验间变异性限制，传统数据增强方法忽略试验信息差异，可能降低表征质量。

Method: 引入加权自举法，根据相对ERP差异计算权重，在概率采样和平均时应用。

Result: 加权自举法相对未加权法提高了解码准确率，最高从68.35%提升到71.25%。

Conclusion: 基于可靠性的数据增强能产生更稳健、有区分性的EEG表征。

Abstract: Electroencephalography (EEG) offers detailed access to neural dynamics but remains constrained by noise and trial-by-trial variability, limiting decoding performance in data-restricted or complex paradigms. Data augmentation is often employed to enhance feature representations, yet conventional uniform averaging overlooks differences in trial informativeness and can degrade representational quality. We introduce a weighted bootstrapping approach that prioritizes more reliable trials to generate higher-quality augmented samples. In a Sentence Evaluation paradigm, weights were computed from relative ERP differences and applied during probabilistic sampling and averaging. Across conditions, weighted bootstrapping improved decoding accuracy relative to unweighted (from 68.35% to 71.25% at best), demonstrating that emphasizing reliable trials strengthens representational quality. The results demonstrate that reliability-based augmentation yields more robust and discriminative EEG representations. The code is publicly available at https://github.com/lyricists/NeuroBootstrap.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [570] [Brazil Data Commons: A Platform for Unifying and Integrating Brazil's Public Data](https://arxiv.org/abs/2511.11755)
*Isadora Cristina,Ramon Gonze,Jônatas Santos,Julio Reis,Mário Alvim,Bernardo Queiroz,Fabrício Benevenuto*

Main category: cs.CY

TL;DR: 巴西公共数据碎片化等问题阻碍研究和决策，介绍巴西数据共享平台解决这些问题。


<details>
  <summary>Details</summary>
Motivation: 解决巴西公共数据碎片化、标准不一致和互操作性有限的问题，促进有效研究、基于证据的政策制定和获取数据驱动的见解。

Method: 引入巴西数据共享平台，采用全球认可的本体和互操作数据标准，提供用户友好界面、简单查询机制和灵活数据访问选项。

Result: 将分散的数据集转变为集成且易于导航的资源。

Conclusion: 巴西数据共享平台能让用户深入了解巴西复杂的社会、经济和环境状况，做出明智决策。

Abstract: The fragmentation of public data in Brazil, coupled with inconsistent standards and limited interoperability, hinders effective research, evidence-based policymaking and access to data-driven insights. To address these issues, we introduce Brazil Data Commons, a platform that unifies various Brazilian datasets under a common semantic framework, enabling the seamless discovery, integration and visualization of information from different domains. By adopting globally recognized ontologies and interoperable data standards, Brazil Data Commons aligns with the principles of the broader Data Commons ecosystem and places Brazilian data in a global context. Through user-friendly interfaces, straightforward query mechanisms and flexible data access options, the platform democratizes data use and enables researchers, policy makers, and the public to gain meaningful insights and make informed decisions. This paper illustrates how Brazil Data Commons transforms scattered datasets into an integrated and easily navigable resource that allows a deeper understanding of Brazil's complex social, economic and environmental landscape.

</details>


### [571] [Cost Transparency of Enterprise AI Adoption](https://arxiv.org/abs/2511.11761)
*Soogand Alavi,Salar Nozari,Andrea Luangrath*

Main category: cs.CY

TL;DR: 研究表明语言风格变化会影响大语言模型输出token数及企业成本，凸显当前定价模式不透明，呼吁新方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型广泛应用，但采用其服务的成本研究不足，传统软件成本可预测，而大语言模型服务成本因输出token难以控制。

Method: 通过OpenAI的API进行实验。

Result: 非礼貌提示会显著增加输出token数，导致企业成本上升、OpenAI收入增加。

Conclusion: 当前定价模式不透明，呼吁采用新方法确保大语言模型服务可预测、透明地被采用。

Abstract: Recent advances in large language models (LLMs) have dramatically improved performance on a wide range of tasks, driving rapid enterprise adoption. Yet, the cost of adopting these AI services is understudied. Unlike traditional software licensing in which costs are predictable before usage, commercial LLM services charge per token of input text in addition to generated output tokens. Crucially, while firms can control the input, they have limited control over output tokens, which are effectively set by generation dynamics outside of business control. This research shows that subtle shifts in linguistic style can systematically alter the number of output tokens without impacting response quality. Using an experiment with OpenAI's API, this study reveals that non-polite prompts significantly increase output tokens leading to higher enterprise costs and additional revenue for OpenAI. Politeness is merely one instance of a broader phenomenon in which linguistic structure can drive unpredictable cost variation. For enterprises integrating LLM into applications, this unpredictability complicates budgeting and undermines transparency in business-to-business contexts. By demonstrating how end-user behavior links to enterprise costs through output token counts, this work highlights the opacity of current pricing models and calls for new approaches to ensure predictable and transparent adoption of LLM services.

</details>


### [572] [Bridging the Skills Gap: A Course Model for Modern Generative AI Education](https://arxiv.org/abs/2511.11757)
*Anya Bardach,Hamilton Murrah*

Main category: cs.CY

TL;DR: 研究生成式AI工具普及对学习环境的影响，发现教育与行业脱节，开发课程教计算机专业学生使用生成式AI工具，调查显示课程有价值，本文探讨课程各方面并给出推广建议。


<details>
  <summary>Details</summary>
Motivation: 解决生成式AI在教育与行业间的脱节问题，让学生能负责且专业地使用AI工具以适应就业市场。

Method: 在私立研究型大学为计算机专业学生开发相关课程，并进行两次混合方法调查，结合数据分析和师生反思。

Result: 两次混合方法调查表明学生认为课程有价值且有效。

Conclusion: 探讨课程的背景、实施和影响，为计算机科学及其他领域提供课程复制的建议。

Abstract: Research on how the popularization of generative Artificial Intelligence (AI) tools impacts learning environments has led to hesitancy among educators to teach these tools in classrooms, creating two observed disconnects. Generative AI competency is increasingly valued in industry but not in higher education, and students are experimenting with generative AI without formal guidance. The authors argue students across fields must be taught to responsibly and expertly harness the potential of AI tools to ensure job market readiness and positive outcomes. Computer Science trajectories are particularly impacted, and while consistently top ranked U.S. Computer Science departments teach the mechanisms and frameworks underlying AI, few appear to offer courses on applications for existing generative AI tools. A course was developed at a private research university to teach undergraduate and graduate Computer Science students applications for generative AI tools in software development. Two mixed method surveys indicated students overwhelmingly found the course valuable and effective. Co-authored by the instructor and one of the graduate students, this paper explores the context, implementation, and impact of the course through data analysis and reflections from both perspectives. It additionally offers recommendations for replication in and beyond Computer Science departments. This is the extended version of this paper to include technical appendices.

</details>


### [573] [Embedding Explainable AI in NHS Clinical Safety: The Explainability-Enabled Clinical Safety Framework (ECSF)](https://arxiv.org/abs/2511.11590)
*Robert Gigiu*

Main category: cs.CY

TL;DR: 本文提出可解释性临床安全框架（ECSF），将可解释性融入DCB0129/0160生命周期，助力临床安全保障。


<details>
  <summary>Details</summary>
Motivation: 现有临床安全标准的确定性假设与AI的概率和自适应行为冲突，DCB0129和DCB0160未明确AI特定属性在安全案例等中的证明方式。

Method: 进行跨监管综合，将DCB条款与多个框架原则映射，构建矩阵，引入五个检查点，将解释技术与DCB工件映射。

Result: 构建了包含监管条款、原则、ECSF检查点和合适可解释性输出的矩阵。

Conclusion: ECSF将可解释性作为临床安全保障核心要素，弥合确定性风险治理与AI概率行为的差距，支持与多框架原则对齐。

Abstract: Artificial intelligence (AI) is increasingly embedded in NHS workflows, but its probabilistic and adaptive behaviour conflicts with the deterministic assumptions underpinning existing clinical-safety standards. DCB0129 and DCB0160 provide strong governance for conventional software yet do not define how AI-specific transparency, interpretability, or model drift should be evidenced within Safety Cases, Hazard Logs, or post-market monitoring. This paper proposes an Explainability-Enabled Clinical Safety Framework (ECSF) that integrates explainability into the DCB0129/0160 lifecycle, enabling Clinical Safety Officers to use interpretability outputs as structured safety evidence without altering compliance pathways. A cross-regulatory synthesis mapped DCB clauses to principles from Good Machine Learning Practice, the NHS AI Assurance and T.E.S.T. frameworks, and the EU AI Act. The resulting matrix links regulatory clauses, principles, ECSF checkpoints, and suitable explainability outputs. ECSF introduces five checkpoints: global transparency for hazard identification, case-level interpretability for verification, clinician usability for evaluation, traceable decision pathways for risk control, and longitudinal interpretability monitoring for post-market surveillance. Techniques such as SHAP, LIME, Integrated Gradients, saliency mapping, and attention visualisation are mapped to corresponding DCB artefacts. ECSF reframes explainability as a core element of clinical-safety assurance, bridging deterministic risk governance with the probabilistic behaviour of AI and supporting alignment with GMLP, the EU AI Act, and NHS AI Assurance principles.

</details>


### [574] [Decision-Making Amid Information-Based Threats in Sociotechnical Systems: A Review](https://arxiv.org/abs/2511.11595)
*Aaron R. Allred,Erin E. Richardson,Sarah R. Bostrom,James Crum,Cara Spencer,Chad Tossell,Richard E. Niemeyer,Leanne Hirshfield,Allison P. A. Hayman*

Main category: cs.CY

TL;DR: 本文综述整合人类信息处理和信息威胁研究领域见解，识别相关认知机制并给出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 技术系统使信息传播规模增大，影响决策，且相关研究分散，需整合不同领域研究。

Method: 对人类信息处理和信息威胁现象研究领域进行综述并整合见解。

Result: 识别出介导信息威胁脆弱性和塑造行为结果的共享认知机制。

Conclusion: 强调整合不同视角研究对减轻人类脆弱性和协调人机表征的重要性，并给出未来研究方向。

Abstract: Technological systems increasingly mediate human information exchange, spanning interactions among humans as well as between humans and artificial agents. The unprecedented scale and reliance on information disseminated through these systems substantially expand the scope of information-based influence that can both enable and undermine sound decision-making. Consequently, understanding and protecting decision-making today faces growing challenges, as individuals and organizations must navigate evolving opportunities and information-based threats across varied domains and information environments. While these risks are widely recognized, research remains fragmented: work evaluating information-based threat phenomena has progressed largely in isolation from foundational studies of human information processing. In this review, we synthesize insights from both domains to identify shared cognitive mechanisms that mediate vulnerability to information-based threats and shape behavioral outcomes. Finally, we outline directions for future research aimed at integrating these perspectives, emphasizing the importance of such integration for mitigating human vulnerabilities and aligning human-machine representations.

</details>


### [575] [EduAgentQG: A Multi-Agent Workflow Framework for Personalized Question Generation](https://arxiv.org/abs/2511.11635)
*Rui Jia,Min Zhang,Fengrui Liu,Bo Jiang,Kun Kuang,Zhongxiang Dai*

Main category: cs.CY

TL;DR: 提出EduAgentQG多智能体协作框架生成高质量多样化个性化问题，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 手动设计问题耗时且难满足多样学习需求，现有自动生成方法存在问题质量不稳定、多样性有限等问题。

Method: 提出EduAgentQG框架，包含五个专门智能体，通过迭代反馈循环工作。

Result: 在两个数学问题数据集上实验，EduAgentQG在问题多样性、目标一致性和整体质量上优于现有单智能体和多智能体方法。

Conclusion: EduAgentQG能生成高质量、多样化且符合教育目标的问题。

Abstract: High-quality personalized question banks are crucial for supporting adaptive learning and individualized assessment. Manually designing questions is time-consuming and often fails to meet diverse learning needs, making automated question generation a crucial approach to reduce teachers' workload and improve the scalability of educational resources. However, most existing question generation methods rely on single-agent or rule-based pipelines, which still produce questions with unstable quality, limited diversity, and insufficient alignment with educational goals. To address these challenges, we propose EduAgentQG, a multi-agent collaborative framework for generating high-quality and diverse personalized questions. The framework consists of five specialized agents and operates through an iterative feedback loop: the Planner generates structured design plans and multiple question directions to enhance diversity; the Writer produces candidate questions based on the plan and optimizes their quality and diversity using feedback from the Solver and Educator; the Solver and Educator perform binary scoring across multiple evaluation dimensions and feed the evaluation results back to the Writer; the Checker conducts final verification, including answer correctness and clarity, ensuring alignment with educational goals. Through this multi-agent collaboration and iterative feedback loop, EduAgentQG generates questions that are both high-quality and diverse, while maintaining consistency with educational objectives. Experiments on two mathematics question datasets demonstrate that EduAgentQG outperforms existing single-agent and multi-agent methods in terms of question diversity, goal consistency, and overall quality.

</details>


### [576] [Demystify, Use, Reflect: Preparing students to be informed LLM-users](https://arxiv.org/abs/2511.11764)
*Nikitha Donekal Chandrashekar,Sehrish Basir Nizamani,Margaret Ellis,Naren Ramakrishnan*

Main category: cs.CY

TL;DR: 将计算机科学课程与大语言模型（LLMs）结合，助学生培养与AI交互能力，首轮课程收集分析数据，学生对LLMs理解提升，策略可用于其他课程。


<details>
  <summary>Details</summary>
Motivation: 帮助学生有意义且负责任地与AI互动，为AI集成的未来做好准备。

Method: 课程中明确教授LLMs工作原理，展示其输出的使用和验证，引导学生将其作为解决问题的一部分，要求学生披露使用情况，并讨论其在各CS子领域的利弊，还收集分析学生前后测数据。

Result: 学生对LLMs工作原理的理解更具技术性，对其验证和使用更有辨别力和协作性。

Conclusion: 这些策略可用于其他课程，为学生应对AI集成的未来做准备。

Abstract: We transitioned our post-CS1 course that introduces various subfields of computer science so that it integrates Large Language Models (LLMs) in a structured, critical, and practical manner. It aims to help students develop the skills needed to engage meaningfully and responsibly with AI. The course now includes explicit instruction on how LLMs work, exposure to current tools, ethical issues, and activities that encourage student reflection on personal use of LLMs as well as the larger evolving landscape of AI-assisted programming. In class, we demonstrate the use and verification of LLM outputs, guide students in the use of LLMs as an ingredient in a larger problem-solving loop, and require students to disclose and acknowledge the nature and extent of LLM assistance. Throughout the course, we discuss risks and benefits of LLMs across CS subfields. In our first iteration of the course, we collected and analyzed data from students pre and post surveys. Student understanding of how LLMs work became more technical, and their verification and use of LLMs shifted to be more discerning and collaborative. These strategies can be used in other courses to prepare students for the AI-integrated future.

</details>


### [577] [Scaling Equitable Reflection Assessment in Education via Large Language Models and Role-Based Feedback Agents](https://arxiv.org/abs/2511.11772)
*Chenyu Zhang,Xiaohang Luo*

Main category: cs.CY

TL;DR: 本文提出基于大语言模型的多智能体系统，为学习者反思提供公平、高质量的形成性反馈，经评估效果良好，有望推动教育公平。


<details>
  <summary>Details</summary>
Motivation: 形成性反馈对学生学习有效，但在大规模或资源有限的课程中难以公平实施，教师缺乏时间和资源为每个学生提供反馈。

Method: 使用五个基于角色的大语言模型智能体，按共享评分标准对学习者反思进行评分，并生成简短、无偏见的反馈评论，系统包含公平性检查。

Result: 在成人AI素养项目中，系统评分接近专家水平，生成的评论被认为有帮助、有同理心且符合教学目标。

Conclusion: 多智能体大语言模型系统可提供公平、高质量的形成性反馈，推动实现教育公平、可及性和教学能力提升的长期目标。

Abstract: Formative feedback is widely recognized as one of the most effective drivers of student learning, yet it remains difficult to implement equitably at scale. In large or low-resource courses, instructors often lack the time, staffing, and bandwidth required to review and respond to every student reflection, creating gaps in support precisely where learners would benefit most. This paper presents a theory-grounded system that uses five coordinated role-based LLM agents (Evaluator, Equity Monitor, Metacognitive Coach, Aggregator, and Reflexion Reviewer) to score learner reflections with a shared rubric and to generate short, bias-aware, learner-facing comments. The agents first produce structured rubric scores, then check for potentially biased or exclusionary language, add metacognitive prompts that invite students to think about their own thinking, and finally compose a concise feedback message of at most 120 words. The system includes simple fairness checks that compare scoring error across lower and higher scoring learners, enabling instructors to monitor and bound disparities in accuracy. We evaluate the pipeline in a 12-session AI literacy program with adult learners. In this setting, the system produces rubric scores that approach expert-level agreement, and trained graders rate the AI-generated comments as helpful, empathetic, and well aligned with instructional goals. Taken together, these results show that multi-agent LLM systems can deliver equitable, high-quality formative feedback at a scale and speed that would be impossible for human graders alone. More broadly, the work points toward a future where feedback-rich learning becomes feasible for any course size or context, advancing long-standing goals of equity, access, and instructional capacity in education.

</details>


### [578] [Differences in the Moral Foundations of Large Language Models](https://arxiv.org/abs/2511.11790)
*Peter Kirgis*

Main category: cs.CY

TL;DR: 本文用道德基础理论对大语言模型进行实验，发现模型与人类在道德基础上存在差异，且能力越强差异越大，还呼吁进一步分析和政策制定者关注。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在关键领域使用增多，但规范伦理判断性质不明，现有对齐研究未充分利用道德心理学视角。

Method: 使用Jonathan Haidt的道德基础理论对各大模型提供商的模型进行综合实验，采用多种描述性统计方法。

Result: 模型彼此之间以及与全国代表性人类基线在道德基础上存在差异，且模型能力增强时差异增大。

Conclusion: 呼吁使用道德基础理论对大语言模型进行更多分析，如微调开源模型，也呼吁政策制定者重视道德基础对大语言模型对齐的重要性。

Abstract: Large language models are increasingly being used in critical domains of politics, business, and education, but the nature of their normative ethical judgment remains opaque. Alignment research has, to date, not sufficiently utilized perspectives and insights from the field of moral psychology to inform training and evaluation of frontier models. I perform a synthetic experiment on a wide range of models from most major model providers using Jonathan Haidt's influential moral foundations theory (MFT) to elicit diverse value judgments from LLMs. Using multiple descriptive statistical approaches, I document the bias and variance of large language model responses relative to a human baseline in the original survey. My results suggest that models rely on different moral foundations from one another and from a nationally representative human baseline, and these differences increase as model capabilities increase. This work seeks to spur further analysis of LLMs using MFT, including finetuning of open-source models, and greater deliberation by policymakers on the importance of moral foundations for LLM alignment.

</details>


### [579] [AI Fairness Beyond Complete Demographics: Current Achievements and Future Directions](https://arxiv.org/abs/2511.13525)
*Zichong Wang,Zhipeng Yin,Roland H. C. Yap,Wenbin Zhang*

Main category: cs.CY

TL;DR: 本文探讨人工智能在人口统计信息不完整时的公平性问题，提出新分类法，总结现有技术并指出开放研究问题。


<details>
  <summary>Details</summary>
Motivation: 现有缓解AI决策系统偏见的方法多依赖完整人口统计信息，而这在现实中因法律限制和强化歧视风险而不切实际，需解决传统方法与现实挑战的差距。

Method: 提出在人口统计信息不完整情况下公平性概念的新分类法，总结现有促进公平性的技术。

Result: 明确了不同公平性概念的关系和区别，总结出超越完整人口统计信息的公平性促进技术。

Conclusion: 指出相关领域存在开放研究问题，鼓励进一步研究。

Abstract: Fairness in artificial intelligence (AI) has become a growing concern due to discriminatory outcomes in AI-based decision-making systems. While various methods have been proposed to mitigate bias, most rely on complete demographic information, an assumption often impractical due to legal constraints and the risk of reinforcing discrimination. This survey examines fairness in AI when demographics are incomplete, addressing the gap between traditional approaches and real-world challenges. We introduce a novel taxonomy of fairness notions in this setting, clarifying their relationships and distinctions. Additionally, we summarize existing techniques that promote fairness beyond complete demographics and highlight open research questions to encourage further progress in the field.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [580] [A Deep Learning Framework for Thyroid Nodule Segmentation and Malignancy Classification from Ultrasound Images](https://arxiv.org/abs/2511.11937)
*Omar Abdelrazik,Mohamed Elsayed,Noorul Wahab,Nasir Rajpoot,Adam Shephard*

Main category: eess.IV

TL;DR: 提出全自动两阶段框架用于甲状腺结节恶性预测，在数据集上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 超声甲状腺结节风险分层存在高观察者间差异，且许多深度学习模型是黑盒。

Method: 先使用TransUNet模型分割甲状腺结节，再用ResNet - 18分类器预测恶性，采用5折交叉验证。

Result: 在349张图像数据集上F1分数达0.852，优于使用手工特征的随机森林分类器（F1分数0.829）。

Conclusion: 所提深度学习框架从局部结节学到的隐式视觉特征比显式形状特征更具预测性，是首个全自动端到端检测和预测恶性的管道。

Abstract: Ultrasound-based risk stratification of thyroid nodules is a critical clinical task, but it suffers from high inter-observer variability. While many deep learning (DL) models function as "black boxes," we propose a fully automated, two-stage framework for interpretable malignancy prediction. Our method achieves interpretability by forcing the model to focus only on clinically relevant regions. First, a TransUNet model automatically segments the thyroid nodule. The resulting mask is then used to create a region of interest around the nodule, and this localised image is fed directly into a ResNet-18 classifier. We evaluated our framework using 5-fold cross-validation on a clinical dataset of 349 images, where it achieved a high F1-score of 0.852 for predicting malignancy. To validate its performance, we compared it against a strong baseline using a Random Forest classifier with hand-crafted morphological features, which achieved an F1-score of 0.829. The superior performance of our DL framework suggests that the implicit visual features learned from the localised nodule are more predictive than explicit shape features alone. This is the first fully automated end-to-end pipeline for both detecting thyroid nodules on ultrasound images and predicting their malignancy.

</details>


### [581] [Recursive Threshold Median Filter and Autoencoder for Salt-and-Pepper Denoising: SSIM analysis of Images and Entropy Maps](https://arxiv.org/abs/2511.12212)
*Petr Boriskov,Kirill Rudkovskii,Andrei Velichko*

Main category: eess.IV

TL;DR: 本文研究用递归阈值算法中中值滤波器（MF）和简单三层自动编码器（AE）去除图像椒盐噪声，评估去噪性能，提出两种可扩展方案，验证新指标价值。


<details>
  <summary>Details</summary>
Motivation: 研究有效去除图像椒盐噪声的方法，评估去噪性能并找到合适指标。

Method: 使用递归阈值算法中的MF和AE去除噪声，用SSIMImg和SSIMMap评估去噪性能，提出2MF和MFs - AE两种方案。

Result: SSIMMap对模糊和局部强度过渡更敏感；递归阈值MF能在强噪声下恢复图像，AE仅适用于低噪声；2MF适合低分辨率突出细节，MFs - AE适合高分辨率恢复整体结构；MF在资源受限平台更优。

Conclusion: 验证了SSIMMap在客观模糊评估和去噪参数调整方面的实用价值。

Abstract: This paper studies the removal of salt-and-pepper noise from images using median filter (MF) and simple three-layer autoencoder (AE) within recursive threshold algorithm. The performance of denoising is assessed with two metrics: the standard Structural Similarity Index SSIMImg of restored and clean images and a newly applied metric SSIMMap - the SSIM of entropy maps of these images computed via 2D Sample Entropy in sliding windows. We shown that SSIMMap is more sensitive to blur and local intensity transitions and complements SSIMImg. Experiments on low- and high-resolution grayscales images demonstrate that recursive threshold MF robustly restores images even under strong noise (50-60 %), whereas simple AE is only capable of restoring images with low levels of noise (<30 %). We propose two scalable schemes: (i) 2MF, which uses two MFs with different window sizes and a final thresholding step, effective for highlighting sharp local details at low resolution; and (ii) MFs-AE, which aggregates features from multiple MFs via an AE and is beneficial for restoring the overall scene structure at higher resolution. Owing to its simplicity and computational efficiency, MF remains preferable for deployment on resource-constrained platforms (edge/IoT), whereas AE underperforms without prior denoising. The results also validate the practical value of SSIMMap for objective blur assessment and denoising parameter tuning.

</details>


### [582] [Deep Unfolded BM3D: Unrolling Non-local Collaborative Filtering into a Trainable Neural Network](https://arxiv.org/abs/2511.12248)
*Kerem Basim,Mehmet Ozan Unal,Metin Ertas,Isa Yildirim*

Main category: eess.IV

TL;DR: 提出Deep Unfolded BM3D (DU - BM3D)混合框架用于低剂量CT去噪，表现优于经典BM3D和独立U - Net。


<details>
  <summary>Details</summary>
Motivation: 经典BM3D依赖固定参数，深度模型如U - Net缺乏可解释性且难以跨噪声机制泛化，需要更好的去噪方法。

Method: 将BM3D展开为可训练架构，用可学习的U - Net去噪器替换其固定的协同滤波，实现端到端优化。

Result: 在不同噪声水平的模拟低剂量CT去噪中，DU - BM3D优于经典BM3D和独立U - Net，在高噪声条件下PSNR和SSIM更高。

Conclusion: DU - BM3D是一种有效的低剂量CT去噪方法，结合了BM3D和U - Net的优势。

Abstract: Block-Matching and 3D Filtering (BM3D) exploits non-local self-similarity priors for denoising but relies on fixed parameters. Deep models such as U-Net are more flexible but often lack interpretability and fail to generalize across noise regimes. In this study, we propose Deep Unfolded BM3D (DU-BM3D), a hybrid framework that unrolls BM3D into a trainable architecture by replacing its fixed collaborative filtering with a learnable U-Net denoiser. This preserves BM3D's non-local structural prior while enabling end-to-end optimization. We evaluate DU-BM3D on low-dose CT (LDCT) denoising and show that it outperforms classic BM3D and standalone U-Net across simulated LDCT at different noise levels, yielding higher PSNR and SSIM, especially in high-noise conditions.

</details>


### [583] [A Multicollinearity-Aware Signal-Processing Framework for Cross-$β$ Identification via X-ray Scattering of Alzheimer's Tissue](https://arxiv.org/abs/2511.12451)
*Abdullah Al Bashit,Prakash Nepal,Lee Makowski*

Main category: eess.IV

TL;DR: 本文开发了一个三阶段分类框架，用于在死后人类大脑的X射线散射剖面中识别交叉-β结构包涵体，模型取得了较好的F1分数，框架为数据有限的分类问题提供了策略。


<details>
  <summary>Details</summary>
Motivation: 由于底物污染、特征间强相关性和样本量有限，利用X射线散射数据自动检测病理交叉-β包涵体具有挑战性。

Method: 分三个阶段构建分类框架，第一阶段用贝叶斯最优分类器分离云母底物和组织区域；第二阶段引入多共线性感知、类条件相关修剪方案；第三阶段在修剪后的特征集上训练紧凑神经网络。

Result: 表现最佳的模型使用211个候选特征中的11个和174个可训练参数，测试F1分数达到84.30%。

Conclusion: 该框架为涉及相关高维实验测量的数据有限分类问题提供了可解释、有理论依据的策略。

Abstract: X-ray scattering measurements of in situ human brain tissue encode structural signatures of pathological cross-$β$ inclusions, yet systematic exploitation of these data for automated detection remains challenging due to substrate contamination, strong inter-feature correlations, and limited sample sizes. This work develops a three-stage classification framework for identifying cross-$β$ structural inclusions-a hallmark of Alzheimer's disease-in X-ray scattering profiles of post-mortem human brain. Stage 1 employs a Bayes-optimal classifier to separate mica substrate from tissue regions on the basis of their distinct scattering signatures. Stage 2 introduces a multicollinearityaware, class-conditional correlation pruning scheme with formal guarantees on the induced Bayes risk and approximation error, thereby reducing redundancy while retaining class-discriminative information. Stage 3 trains a compact neural network on the pruned feature set to detect the presence or absence of cross-$β$ fibrillar ordering. The top-performing model, optimized with a composite loss combining Focal and Dice objectives, attains a test F1-score of 84.30% using 11 of 211 candidate features and 174 trainable parameters. The overall framework yields an interpretable, theory-grounded strategy for data-limited classification problems involving correlated, high-dimensional experimental measurements, exemplified here by X-ray scattering profiles of neurodegenerative tissue.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [584] [Improving Neutrino Oscillation Measurements through Event Classification](https://arxiv.org/abs/2511.11938)
*Sebastian A. R. Ellis,Daniel C. Hackett,Shirley Weishi Li,Pedro A. N. Machado,Karla Tame-Narvaez*

Main category: hep-ph

TL;DR: 提出在中微子能量重建前按相互作用类型对事件分类的策略，经测试该策略稳健，应用于模拟分析可提升准确性和灵敏度。


<details>
  <summary>Details</summary>
Motivation: 现有中微子能量重建方法受中微子 - 原子核相互作用建模不确定性限制，且标准量热法未利用不同相互作用通道重建性能差异的信息。

Method: 在能量重建前根据潜在相互作用类型对事件分类，利用有监督机器学习技术，基于标记的生成器事件进行训练，利用不同散射过程的内在运动学差异。

Result: 跨生成器测试框架表明分类方法对微观物理建模误差稳健，应用于模拟DUNE中微子消失分析可提高准确性和灵敏度。

Conclusion: 该策略为减少未来振荡测量中重建导致的系统误差提供了可行途径。

Abstract: Precise neutrino energy reconstruction is essential for next-generation long-baseline oscillation experiments, yet current methods remain limited by large uncertainties in neutrino-nucleus interaction modeling. Even so, it is well established that different interaction channels produce systematically varying amounts of missing energy and therefore yield different reconstruction performance--information that standard calorimetric approaches do not exploit. We introduce a strategy that incorporates this structure by classifying events according to their underlying interaction type prior to energy reconstruction. Using supervised machine-learning techniques trained on labeled generator events, we leverage intrinsic kinematic differences among quasi-elastic scattering, meson-exchange current, resonance production, and deep-inelastic scattering processes. A cross-generator testing framework demonstrates that this classification approach is robust to microphysics mismodeling and, when applied to a simulated DUNE $ν_μ$ disappearance analysis, yields improved accuracy and sensitivity. These results highlight a practical path toward reducing reconstruction-driven systematics in future oscillation measurements.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [585] [Case study of a differentiable heterogeneous multiphysics solver for a nuclear fusion application](https://arxiv.org/abs/2511.13262)
*Jack B. Coughlin,Archis Joglekar,Jonathan Brodrick,Alexander Lavin*

Main category: physics.comp-ph

TL;DR: 本文对核聚变领域的异构多物理场求解器进行案例研究，介绍求解方法及解决非可微求解器融入问题的方案。


<details>
  <summary>Details</summary>
Motivation: 解决核聚变领域里将非可微的生产级等离子体求解器融入基于梯度的工作流这一难题。

Method: 用JAX中的自动微分ODE求解器计算脉冲功率电路和等离子体参数演化，用基于梯度的牛顿迭代求解等离子体负载阻抗；通过Tesseract软件提供与JAX兼容的多物理可微抽象层。

Result: 实现了端到端的可微性，允许在高保真求解器、神经代理和解析近似之间无缝互换以进行快速渐进式原型设计。

Conclusion: Tesseract软件有效解决了非可微求解器融入基于梯度工作流的问题，适用于核聚变领域的多物理场求解。

Abstract: This work presents a case study of a heterogeneous multiphysics solver from the nuclear fusion domain. At the macroscopic scale, an auto-differentiable ODE solver in JAX computes the evolution of the pulsed power circuit and bulk plasma parameters for a compressing Z Pinch. The ODE solver requires a closure for the impedance of the plasma load obtained via root-finding at every timestep, which we solve efficiently using gradient-based Newton iteration. However, incorporating non-differentiable production-grade plasma solvers like Gkeyll (a C/CUDA plasma simulation suite) into a gradient-based workflow is non-trivial. The ''Tesseract'' software addresses this challenge by providing a multi-physics differentiable abstraction layer made fully compatible with JAX (through the `tesseract_jax` adapter). This architecture ensures end-to-end differentiability while allowing seamless interchange between high-fidelity solvers (Gkeyll), neural surrogates, and analytical approximations for rapid, progressive prototyping.

</details>


### [586] [Scalable learning of macroscopic stochastic dynamics](https://arxiv.org/abs/2511.12842)
*Mengyi Chen,Pengru Huang,Kostya S. Novoselov,Qianxiao Li*

Main category: physics.comp-ph

TL;DR: 提出一个仅用小系统模拟学习大型随机微观系统宏观动力学的框架，通过多种系统验证其准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 直接模拟能反映宏观行为的大型微观系统不可行，需找到从微观轨迹模拟构建准确宏观模型的方法。

Method: 采用部分演化方案生成训练数据对，确定与宏观可观测量相关的闭合变量，用自定义损失学习宏观动力学，引入分层上采样方案从小系统轨迹分布高效生成大系统快照。

Result: 通过多种随机空间扩展系统，包括随机偏微分方程描述的系统、理想化晶格自旋系统和更现实的铌钼钽合金系统，实证证明了框架的准确性和鲁棒性。

Conclusion: 所提出的框架可有效解决利用小系统模拟学习大型随机微观系统宏观动力学的问题。

Abstract: Macroscopic dynamical descriptions of complex physical systems are crucial for understanding and controlling material behavior. With the growing availability of data and compute, machine learning has become a promising alternative to first-principles methods to build accurate macroscopic models from microscopic trajectory simulations. However, for spatially extended systems, direct simulations of sufficiently large microscopic systems that inform macroscopic behavior is prohibitive. In this work, we propose a framework that learns the macroscopic dynamics of large stochastic microscopic systems using only small-system simulations. Our framework employs a partial evolution scheme to generate training data pairs by evolving large-system snapshots within local patches. We subsequently identify the closure variables associated with the macroscopic observables and learn the macroscopic dynamics using a custom loss. Furthermore, we introduce a hierarchical upsampling scheme that enables efficient generation of large-system snapshots from small-system trajectory distributions. We empirically demonstrate the accuracy and robustness of our framework through a variety of stochastic spatially extended systems, including those described by stochastic partial differential equations, idealised lattice spin systems, and a more realistic NbMoTa alloy system.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [587] [Regression Analysis After Bipartite Bayesian Record Linkage](https://arxiv.org/abs/2511.12343)
*Xueyan Hu,Jerome P. Reiter*

Main category: stat.ME

TL;DR: 本文提出多重插补框架改进两阶段数据链接分析方法，模拟研究显示其回归参数估计更准确，并以家庭收入财富调查数据为例说明。


<details>
  <summary>Details</summary>
Motivation: 两阶段数据链接方法未考虑链接不确定性带来的模型参数不确定性，也未利用研究变量关系确定链接。

Method: 先使用二分贝叶斯记录链接模型生成多个可能的链接数据集，再用研究变量信息估计混合模型。

Result: 模拟研究表明回归模型参数估计比类似两阶段方法更准确。

Conclusion: 所提多重插补框架能改进两阶段数据链接分析方法，提高回归参数估计准确性。

Abstract: In many settings, a data curator links records from two files to produce datasets that are shared with secondary analysts. Analysts use the linked files to estimate models of interest, such as regressions. Such two-stage approaches do not necessarily account for uncertainty in model parameters that results from uncertainty in the linkages. Further, they do not leverage the relationships among the study variables in the two files to help determine the linkages. We propose a multiple imputation framework to address these shortcomings. First, we use a bipartite Bayesian record linkage model to generate multiple plausible linked datasets, disregarding the information in the study variables. Second, we presume each linked file has a mixture of true links and false links. We estimate the mixture model using information from the study variables. Through simulation studies under a regression setting, we demonstrate that estimates of the regression model parameters can be more accurate than those based on an analogous two-stage approach. We illustrate the integrated approach using data from the Survey on Household Income and Wealth, examining a regression involving the persistence of income.

</details>


### [588] [Transformation-free linear simplicial-simplicial regression via constrained iterative reweighted least squares](https://arxiv.org/abs/2511.13296)
*Michail Tsagris*

Main category: stat.ME

TL;DR: 提出将单纯形 - 单纯形回归模型转化为约束逻辑回归，用约束迭代加权最小二乘法估计系数，提升估计速度。


<details>
  <summary>Details</summary>
Motivation: 提升单纯形 - 单纯形回归模型回归系数的估计速度。

Method: 将模型表述为约束逻辑回归，使用约束迭代加权最小二乘法估计回归系数。

Result: 估计过程显著加快。

Conclusion: 用约束迭代加权最小二乘法估计单纯形 - 单纯形回归模型的回归系数是有效的。

Abstract: Simplicial-simplicial regression refers to the regression setting where both the responses and predictor variables lie within the simplex space, i.e. they are compositional. \cite{fiksel2022} proposed a transformation-free lienar regression model, that minimizes the Kullback-Leibler divergence from the observed to the fitted compositions was recently proposed. To effectively estimate the regression coefficients the EM algorithm was employed. We formulate the model as a constrained logistic regression, in the spirit of \cite{tsagris2025}, and we estimate the regression coefficients using constrained iteratively reweighted least squares. This approach makes the estimation procedure significantly faster.

</details>


### [589] [Shortest fixed-width confidence intervals for a bounded parameter: The Push algorithm](https://arxiv.org/abs/2511.13694)
*Jay Bartroff,Asmit Chakraborty*

Main category: stat.ME

TL;DR: 本文提出计算单个有界参数最优固定宽度置信区间的方法，适用于多种分布，优于标准方法并应用于WHO烟草使用数据。


<details>
  <summary>Details</summary>
Motivation: 扩展Asparaouhov和Lorden的二项式方法，为单个有界参数计算最优固定宽度置信区间。

Method: 提出一种计算方法，适用于离散或具有单调似然比性质的连续有界参数，通过R包在二项式、超几何和正态分布上验证。

Result: 在各分布中该方法优于标准方法，在正态分布中改进了z区间。

Conclusion: 所提方法有效，可用于计算有界参数的最优固定宽度置信区间，并能应用于实际数据。

Abstract: We present a method for computing optimal fixed-width confidence intervals for a single, bounded parameter, extending a method for the binomial due to Asparaouhov and Lorden, who called it the Push algorithm. The method produces the shortest possible non-decreasing confidence interval for a given confidence level, and if the Push interval does not exist for a given width and level, then no such interval exists. The method applies to any bounded parameter that is discrete, or is continuous and has the monotone likelihood ratio property. We demonstrate the method on the binomial, hypergeometric, and normal distributions with our available R package. In each of these distributions the proposed method outperforms the standard ones, and in the latter case even improves upon the $z$-interval. We apply the proposed method to World Health Organization (WHO) data on tobacco use.

</details>


### [590] [MMDCP: A Distribution-free Approach to Outlier Detection and Classification with Coverage Guarantees and SCW-FDR Control](https://arxiv.org/abs/2511.12016)
*Youwu Lin,Xiaoyu Qian,Jinru Wu,Qi Liu,Pei Wang*

Main category: stat.ME

TL;DR: 提出MMDCP框架用于标签偏移下多类分类和异常检测，解决现有方法计算成本高、预测集保守等问题，有理论保证和新误差指标，模拟和实际应用验证优势。


<details>
  <summary>Details</summary>
Motivation: 现有基于经验累积或密度函数结合数据分割策略的方法在标签偏移下计算成本高，预测集保守且覆盖不稳定，尤其是小样本情况。

Method: 结合特定类别的距离度量和全共形预测构建得分函数，提出SCW - FDR新全局误差指标。

Result: 建立了预测集的收敛率，首次理论刻画了oracle和经验共形p值之间的差距，能有效控制CW - FDR和SCW - FDR。

Conclusion: 模拟和两个实际数据应用支持理论发现，证明了MMDCP方法的优势。

Abstract: We propose the Modified Mahalanobis Distance Conformal Prediction (MMDCP), a unified framework for multi-class classification and outlier detection under label shift, where the training and test distributions may differ. In such settings, many existing methods construct nonconformity scores based on empirical cumulative or density functions combined with data-splitting strategies. However, these approaches are often computationally expensive due to their heavy reliance on resampling procedures and tend to produce overly conservative prediction sets with unstable coverage, especially in small samples. To address these challenges, MMDCP combines class-specific distance measures with full conformal prediction to construct a score function, thereby producing adaptive prediction sets that effectively capture both inlier and outlier structures. Under mild regularity conditions, we establish convergence rates for the resulting sets and provide the first theoretical characterization of the gap between oracle and empirical conformal $p$-values, which ensures valid coverage and effective control of the class-wise false discovery rate (CW-FDR). We further introduce the Summarized Class-Wise FDR (SCW-FDR), a novel global error metric aggregating false discoveries across classes, and show that it can be effectively controlled within the MMDCP framework. Extensive simulations and two real-data applications support our theoretical findings and demonstrate the advantages of the proposed method.

</details>


### [591] [Aggregating Conformal Prediction Sets via α-Allocation](https://arxiv.org/abs/2511.12065)
*Congbin Xu,Yue Yu,Haojie Ren,Zhaojun Wang,Changliang Zou*

Main category: stat.ME

TL;DR: 本文提出COLA策略聚合多个共形预测集的置信水平，发展多种变体，实验证明其能在维持有效覆盖的同时减小预测集大小。


<details>
  <summary>Details</summary>
Motivation: 有效利用多个一致性得分来减小共形预测集大小是一个重大挑战，本文旨在解决该问题。

Method: 引入COLA策略来优化分配多个共形预测集的置信水平，还开发了COLA - s、COLA - f和COLA - l等变体。

Result: 在合成和真实数据集上的大量实验表明，COLA比现有基线方法能得到更小的预测集，同时维持有效覆盖。

Conclusion: COLA策略在减小共形预测集大小方面表现出色，能在维持覆盖的同时提高效率。

Abstract: Conformal prediction offers a distribution-free framework for constructing prediction sets with finite-sample coverage. Yet, efficiently leveraging multiple conformity scores to reduce prediction set size remains a major open challenge. Instead of selecting a single best score, this work introduces a principled aggregation strategy, COnfidence-Level Allocation (COLA), that optimally allocates confidence levels across multiple conformal prediction sets to minimize empirical set size while maintaining provable coverage. Two variants are further developed, COLA-s and COLA-f, which guarantee finite-sample marginal coverage via sample splitting and full conformalization, respectively. In addition, we develop COLA-l, an individualized allocation strategy that promotes local size efficiency while achieving asymptotic conditional coverage. Extensive experiments on synthetic and real-world datasets demonstrate that COLA achieves considerably smaller prediction sets than state-of-the-art baselines while maintaining valid coverage.

</details>


### [592] [A Gentle Introduction to Conformal Time Series Forecasting](https://arxiv.org/abs/2511.13608)
*M. Stocker,W. Małgorzewicz,M. Fontana,S. Ben Taieb*

Main category: stat.ME

TL;DR: 本文回顾共形预测方法在处理非可交换时间序列数据上的进展，给出理论基础、分类现有方法并做仿真研究。


<details>
  <summary>Details</summary>
Motivation: 经典共形预测方法依赖可交换性假设，在时间序列数据中该假设不成立，可能导致预测区间失效，因此需新方法处理非可交换数据。

Method: 先给出弱依赖条件下分裂共形预测的有限样本保证的理论基础，再对通过重新加权校准数据、动态更新残差分布或实时自适应调整目标覆盖水平等缓解序列依赖的方法进行调查和分类，最后进行综合仿真研究。

Result: 对不同技术在经验覆盖率、区间宽度和计算成本方面进行比较。

Conclusion: 指出了实际权衡和开放的研究方向。

Abstract: Conformal prediction is a powerful post-hoc framework for uncertainty quantification that provides distribution-free coverage guarantees. However, these guarantees crucially rely on the assumption of exchangeability. This assumption is fundamentally violated in time series data, where temporal dependence and distributional shifts are pervasive. As a result, classical split-conformal methods may yield prediction intervals that fail to maintain nominal validity. This review unifies recent advances in conformal forecasting methods specifically designed to address nonexchangeable data. We first present a theoretical foundation, deriving finite-sample guarantees for split-conformal prediction under mild weak-dependence conditions. We then survey and classify state-of-the-art approaches that mitigate serial dependence by reweighting calibration data, dynamically updating residual distributions, or adaptively tuning target coverage levels in real time. Finally, we present a comprehensive simulation study that compares these techniques in terms of empirical coverage, interval width, and computational cost, highlighting practical trade-offs and open research directions.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [593] [DataOps-driven CI/CD for analytics repositories](https://arxiv.org/abs/2511.12277)
*Dmytro Valiaiev*

Main category: eess.SY

TL;DR: 针对 SQL 数据处理缺乏规范问题，提出 DataOps 对齐的验证框架，含控制计分卡和 CI/CD 管道框架，分五阶段，用 RTM 确保完整性，提升数据质量等。


<details>
  <summary>Details</summary>
Motivation: SQL 数据处理缺乏传统软件开发的严谨性，导致问题，且缺少 DataOps 实施的标准化框架。

Method: 通过多视角文献综述得出 DataOps 控制计分卡，将关键概念提炼为十二个可测试控制，映射到模块化、可扩展的 CI/CD 管道框架，框架分五阶段，用 RTM 确保定性完整。

Result: 提出了 DataOps 对齐的验证框架，包含计分卡和 CI/CD 管道框架。

Conclusion: 该方法提供了增强数据质量、治理和协作的结构化机制，使团队能透明且可控地扩展分析开发。

Abstract: The proliferation of SQL for data processing has often occurred without the rigor of traditional software development, leading to siloed efforts, logic replication, and increased risk. This ad-hoc approach hampers data governance and makes validation nearly impossible. Organizations are adopting DataOps, a methodology combining Agile, Lean, and DevOps principles to address these challenges to treat analytics pipelines as production systems. However, a standardized framework for implementing DataOps is lacking. This perspective proposes a qualitative design for a DataOps-aligned validation framework. It introduces a DataOps Controls Scorecard, derived from a multivocal literature review, which distills key concepts into twelve testable controls. These controls are then mapped to a modular, extensible CI/CD pipeline framework designed to govern a single source of truth (SOT) SQL repository. The framework consists of five stages: Lint, Optimize, Parse, Validate, and Observe, each containing specific, automated checks. A Requirements Traceability Matrix (RTM) demonstrates how each high-level control is enforced by concrete pipeline checks, ensuring qualitative completeness. This approach provides a structured mechanism for enhancing data quality, governance, and collaboration, allowing teams to scale analytics development with transparency and control.

</details>


### [594] [AI-Enhanced IoT Systems for Predictive Maintenance and Affordability Optimization in Smart Microgrids: A Digital Twin Approach](https://arxiv.org/abs/2511.12175)
*Koushik Ahmed Kushal,Florimond Gueniat*

Main category: eess.SY

TL;DR: 提出基于数字孪生的AI增强物联网框架用于智能微电网预测性维护和成本优化，实验显示效果良好。


<details>
  <summary>Details</summary>
Motivation: 提高分布式微电网环境中的可靠性和能源效率。

Method: 采用数字孪生建模方法，集成实时传感器数据、基于机器学习的故障预测和成本感知运营分析。

Result: 与基线微电网管理方法相比，提高了预测准确性、减少了运营停机时间并节省了成本。

Conclusion: 数字孪生驱动的物联网架构有潜力成为下一代智能且经济的能源系统的可扩展解决方案。

Abstract: This study presents an AI enhanced IoT framework for predictive maintenance and affordability optimization in smart microgrids using a Digital Twin modeling approach. The proposed system integrates real time sensor data, machine learning based fault prediction, and cost aware operational analytics to improve reliability and energy efficiency in distributed microgrid environments. By synchronizing physical microgrid components with a virtual Digital Twin, the framework enables early detection of component degradation, dynamic load management, and optimized maintenance scheduling. Experimental evaluations demonstrate improved predictive accuracy, reduced operational downtime, and measurable cost savings compared to baseline microgrid management methods. The findings highlight the potential of Digital Twin driven IoT architectures as a scalable solution for next generation intelligent and affordable energy systems.

</details>


### [595] [One Request, Multiple Experts: LLM Orchestrates Domain Specific Models via Adaptive Task Routing](https://arxiv.org/abs/2511.12484)
*Xu Yang,Chenhui Lin,Haotian Liu,Qi Wang,Yue Yang,Wenchuan Wu*

Main category: eess.SY

TL;DR: 本文提出ADN - Agent架构，利用大语言模型协调多个领域特定模型（DSM），设计通信机制，提出微调小语言模型的训练管道，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 主动配电网（ADN）运行成为复杂问题，掌握、集成和编排异构DSM对ADN运营商有大量开销，急需智能方法统一DSM并实现高效协调。

Method: 提出ADN - Agent架构，利用大语言模型协调多个DSM，设计统一灵活的通信机制，提出微调小语言模型的自动化训练管道。

Result: 综合比较和消融实验验证了所提方法的有效性，ADN - Agent架构优于现有大语言模型应用范式。

Conclusion: 所提ADN - Agent架构及相关方法能有效提升系统解决问题的能力，可用于统一和协调ADN中的多个DSM。

Abstract: With the integration of massive distributed energy resources and the widespread participation of novel market entities, the operation of active distribution networks (ADNs) is progressively evolving into a complex multi-scenario, multi-objective problem. Although expert engineers have developed numerous domain specific models (DSMs) to address distinct technical problems, mastering, integrating, and orchestrating these heterogeneous DSMs still entail considerable overhead for ADN operators. Therefore, an intelligent approach is urgently required to unify these DSMs and enable efficient coordination. To address this challenge, this paper proposes the ADN-Agent architecture, which leverages a general large language model (LLM) to coordinate multiple DSMs, enabling adaptive intent recognition, task decomposition, and DSM invocation. Within the ADN-Agent, we design a novel communication mechanism that provides a unified and flexible interface for diverse heterogeneous DSMs. Finally, for some language-intensive subtasks, we propose an automated training pipeline for fine-tuning small language models, thereby effectively enhancing the overall problem-solving capability of the system. Comprehensive comparisons and ablation experiments validate the efficacy of the proposed method and demonstrate that the ADN-Agent architecture outperforms existing LLM application paradigms.

</details>


### [596] [Data-driven Acceleration of MPC with Guarantees](https://arxiv.org/abs/2511.13588)
*Agustin Castellano,Shijie Pan,Enrique Mallada*

Main category: eess.SY

TL;DR: 提出数据驱动框架加速MPC，政策速度快且有有界最优性差距，实验显示其速度比标准MPC快100 - 1000倍。


<details>
  <summary>Details</summary>
Motivation: Model Predictive Control (MPC)在低延迟应用中速度慢，需要加速。

Method: 用从离线MPC解决方案构建的非参数策略替代在线优化，策略对最优成本上限是贪婪的，可实现为非参数查找规则。

Result: 在离线数据足够覆盖的条件下，策略递归可行且有可证明的有界最优性差距，实验表明该策略比标准MPC快100 - 1000倍，对最优性影响较小。

Conclusion: 该策略有用于实时控制任务的潜力。

Abstract: Model Predictive Control (MPC) is a powerful framework for optimal control but can be too slow for low-latency applications. We present a data-driven framework to accelerate MPC by replacing online optimization with a nonparametric policy constructed from offline MPC solutions. Our policy is greedy with respect to a constructed upper bound on the optimal cost-to-go, and can be implemented as a nonparametric lookup rule that is orders of magnitude faster than solving MPC online. Our analysis shows that under sufficient coverage condition of the offline data, the policy is recursively feasible and admits provable, bounded optimality gap. These conditions establish an explicit trade-off between the amount of data collected and the tightness of the bounds. Our experiments show that this policy is between 100 and 1000 times faster than standard MPC, with only a modest hit to optimality, showing potential for real-time control tasks.

</details>


### [597] [Physics-Informed Neural Networks for Nonlinear Output Regulation](https://arxiv.org/abs/2511.13595)
*Sebastiano Mengozzi,Giovanni B. Esposito,Michelangelo Bin,Andrea Acquaviva,Andrea Bartolini,Lorenzo Marconi*

Main category: eess.SY

TL;DR: 本文针对全信息下非线性系统输出调节问题，用物理信息神经网络（PINN）求解调节器方程，在直升机调节任务验证，具泛化性和实时推理能力，适用于可解输出调节问题的非线性系统。


<details>
  <summary>Details</summary>
Motivation: 解决全信息下非线性系统的输出调节问题，实现完美跟踪或干扰抑制。

Method: 引入物理信息神经网络（PINN）方法，通过最小化边界和可行性条件下的残差，直接近似调节器方程中的零调节误差流形π(w)和前馈输入c(w)。

Result: 在直升机垂直动力学与谐波振荡平台同步的调节任务中，基于PINN的求解器能高精度重建零误差流形，并在外部系统变化时保持调节性能。

Conclusion: 学习型求解器在非线性输出调节中有潜力，所提方法广泛适用于可解决输出调节问题的非线性系统。

Abstract: This work addresses the full-information output regulation problem for nonlinear systems, assuming the states of both the plant and the exosystem are known. In this setting, perfect tracking or rejection is achieved by constructing a zero-regulation-error manifold π(w) and a feedforward input c(w) that render such manifold invariant. The pair (π(w), c(w)) is characterized by the regulator equations, i.e., a system of PDEs with an algebraic constraint. We focus on accurately solving the regulator equations introducing a physics-informed neural network (PINN) approach that directly approximates π(w) and c(w) by minimizing the residuals under boundary and feasibility conditions, without requiring precomputed trajectories or labeled data. The learned operator maps exosystem states to steady state plant states and inputs, enables real-time inference and, critically, generalizes across families of the exosystem with varying initial conditions and parameters. The framework is validated on a regulation task that synchronizes a helicopter's vertical dynamics with a harmonically oscillating platform. The resulting PINN-based solver reconstructs the zero-error manifold with high fidelity and sustains regulation performance under exosystem variations, highlighting the potential of learning-enabled solvers for nonlinear output regulation. The proposed approach is broadly applicable to nonlinear systems that admit a solution to the output regulation problem.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [598] [Protein Structure Tokenization via Geometric Byte Pair Encoding](https://arxiv.org/abs/2511.11758)
*Michael Sun,Weize Yuan,Gang Liu,Wojciech Matusik,Marinka Zitnik*

Main category: q-bio.QM

TL;DR: 提出几何感知的蛋白质结构分词器GeoBPE，有压缩、数据高效和泛化优势，且架构无关。


<details>
  <summary>Details</summary>
Motivation: 现有蛋白质结构分词器缺乏原则性，存在固定标记大小、依赖连续向量码本等问题，限制了可解释性等。

Method: GeoBPE将连续、嘈杂、多尺度的主链构象转换为离散几何“句子”，通过聚类、量化及优化边界等迭代生成几何原语的分层词汇表。

Result: GeoBPE有压缩效果（比特率降低超10倍）、数据效率高（训练数据减少超10倍）、泛化性好（测试/训练失真率比为1.0 - 1.1），在多任务中表现优于现有分词器，支持多种架构应用。

Conclusion: GeoBPE是一种有效的蛋白质结构分词器，能推动多模态蛋白质模型发展，代码开源。

Abstract: Protein structure is central to biological function, and enabling multimodal protein models requires joint reasoning over sequence, structure, and function. A key barrier is the lack of principled protein structure tokenizers (PSTs): existing approaches fix token size or rely on continuous vector codebooks, limiting interpretability, multi-scale control, and transfer across architectures. We introduce GeoBPE, a geometry-grounded PST that transforms continuous, noisy, multi-scale backbone conformations into discrete ``sentences'' of geometry while enforcing global constraints. Analogous to byte-pair encoding, GeoBPE generates a hierarchical vocabulary of geometric primitives by iteratively (i) clustering Geo-Pair occurrences with k-medoids to yield a resolution-controllable vocabulary; (ii) quantizing each Geo-Pair to its closest medoid prototype; and (iii) reducing drift through differentiable inverse kinematics that optimizes boundary glue angles under an $\mathrm{SE}(3)$ end-frame loss. GeoBPE offers compression ($>$10x reduction in bits-per-residue at similar distortion rate), data efficiency ($>$10x less training data), and generalization (maintains test/train distortion ratio of $1.0-1.1$). It is architecture-agnostic: (a) its hierarchical vocabulary provides a strong inductive bias for coarsening residue-level embeddings from large PLMs into motif- and protein-level representations, consistently outperforming leading PSTs across $12$ tasks and $24$ test splits; (b) paired with a transformer, GeoBPE supports unconditional backbone generation via language modeling; and (c) tokens align with CATH functional families and support expert-interpretable case studies, offering functional meaning absent in prior PSTs. Code is available at https://github.com/shiningsunnyday/PT-BPE/.

</details>


### [599] [Causal Inference, Biomarker Discovery, Graph Neural Network, Feature Selection](https://arxiv.org/abs/2511.13295)
*Chaowang Lan,Jingxin Wu,Yulong Yuan,Chuxun Liu,Huangyi Kang,Caihua Liu*

Main category: q-bio.QM

TL;DR: 提出Causal - GNN方法用于生物标志物发现，实验显示其有高预测准确率和能识别更稳定标志物，有广泛应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有从高通量转录组数据发现生物标志物的方法常忽略基因调控关系且在不同数据集上缺乏稳定性，会混淆虚假和真实因果关系。

Method: 开发结合因果推理和多层图神经网络的Causal - GNN方法，通过因果效应估计识别稳定生物标志物，并采用基于GNN的倾向得分机制利用跨基因调控网络。

Result: 该方法在四个不同数据集和四个独立分类器上实现了始终较高的预测准确率，相比传统方法能识别更稳定的生物标志物。

Conclusion: 该方法为生物标志物发现提供了强大、高效且具有生物学可解释性的工具，有广泛应用潜力。

Abstract: Biomarker discovery from high-throughput transcriptomic data is crucial for advancing precision medicine. However, existing methods often neglect gene-gene regulatory relationships and lack stability across datasets, leading to conflation of spurious correlations with genuine causal effects. To address these issues, we develop a causal graph neural network (Causal-GNN) method that integrates causal inference with multi-layer graph neural networks (GNNs). The key innovation is the incorporation of causal effect estimation for identifying stable biomarkers, coupled with a GNN-based propensity scoring mechanism that leverages cross-gene regulatory networks. Experimental results demonstrate that our method achieves consistently high predictive accuracy across four distinct datasets and four independent classifiers. Moreover, it enables the identification of more stable biomarkers compared to traditional methods. Our work provides a robust, efficient, and biologically interpretable tool for biomarker discovery, demonstrating strong potential for broad application across medical disciplines.

</details>


<div id='cs.GL'></div>

# cs.GL [[Back]](#toc)

### [600] [LLM Architecture, Scaling Laws, and Economics: A Quick Summary](https://arxiv.org/abs/2511.11572)
*William H. Press*

Main category: cs.GL

TL;DR: 总结大语言模型QKV自注意力标准架构、计算和内存扩展定律及成本估算。


<details>
  <summary>Details</summary>
Motivation: 以总结形式提供大语言模型相关信息，这些信息此前未以总结形式出现。

Method: 对大语言模型QKV自注意力标准架构进行总结，给出计算和内存扩展定律及成本估算。

Result: 完成大语言模型架构总结、扩展定律及成本估算，讨论了DeepSeek是否为特殊情况。

Conclusion: 将大语言模型相关信息以总结形式呈现，方便获取。

Abstract: The current standard architecture of Large Language Models (LLMs) with QKV self-attention is briefly summarized, including the architecture of a typical Transformer. Scaling laws for compute (flops) and memory (parameters plus data) are given, along with their present (2025) rough cost estimates for the parameters of present LLMs of various scales, including discussion of whether DeepSeek should be viewed as a special case. Nothing here is new, but this material seems not otherwise readily available in summary form.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [601] [Graded Projection Recursion (GPR): A Framework for Controlling Bit-Complexity of Algebraic Packing](https://arxiv.org/abs/2511.11988)
*Jeffrey Uhlmann*

Main category: cs.CC

TL;DR: 本文提出分级投影递归（GPR）框架，解决递归算法代数打包时位复杂度问题，开发近二次位复杂度矩阵乘法算法并给出加速相关算法的通用方法。


<details>
  <summary>Details</summary>
Motivation: 解决使用代数打包的递归算法计算复杂度与位复杂度不符的问题。

Method: 引入分级投影递归（GPR）框架，利用该框架开发矩阵乘法算法并给出GPR替换原理。

Result: 开发出模型诚实且达到近二次位复杂度的矩阵乘法算法，提供加速相关算法的通用方法。

Conclusion: GPR框架能有效解决递归算法位复杂度问题，并可用于加速相关算法。

Abstract: Recursive algorithms that use algebraic packing may appear to achieve reduced computational complexity that does not reflect the dominating bit-complexity, i.e., the implemented performance would not exhibit the claimed scaling. This paper introduces Graded Projection Recursion (GPR), a formal framework designed to address this problem by defining a rigorous mechanism that provably controls bit growth if specific conditions are satisfied.
  We use GPR to develop a matrix multiplication algorithm that is model-honest and achieves a true near-quadratic bit-complexity. This framework also provides a general GPR Substitution Principle recipe for accelerating a significant class of related algorithms in a provably rigorous way.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [602] [QuaRT: A toolkit for the exploration of quantum methods for radiation transport](https://arxiv.org/abs/2511.12356)
*Rasmit Devkota,John H. Wise*

Main category: astro-ph.IM

TL;DR: QuaRT是用于天体物理和宇宙学问题中辐射传输量子模拟的Python库，有新的角重新分布方法改进模拟各向同性。


<details>
  <summary>Details</summary>
Motivation: 解决天体物理和宇宙学问题中辐射传输的量子模拟，提高模拟对象在非散射介质中的各向同性。

Method: 采用新的角重新分布方法用于格子玻尔兹曼方法。

Result: 实现了QuaRT库，改进了非散射介质中像恒星这类对象模拟的各向同性。

Conclusion: QuaRT库及其采用的方法对天体物理和宇宙学辐射传输模拟有积极作用。

Abstract: QuaRT is a Python library for quantum simulation of radiative transfer in astrophysical and cosmological problems. It features a novel angular redistribution methodology for lattice Boltzmann methods which improves the isotropy of simulations of objects such as stars in non-scattering media.

</details>


### [603] [Towards Mitigating Systematics in Large-Scale Surveys via Few-Shot Optimal Transport-Based Feature Alignment](https://arxiv.org/abs/2511.11787)
*Sultan Hassan,Sambatra Andrianomena,Benjamin D. Wandelt*

Main category: astro-ph.IM

TL;DR: 提出一种通过优化特征对齐损失来对齐分布内和分布外样本特征的方法，在MNIST数据集和中性氢大尺度地图上验证，发现最优传输在未知样本奇偶性时对齐分布外特征有效。


<details>
  <summary>Details</summary>
Motivation: 系统误差污染可观测数据导致分布偏移，给使用预训练模型标记带来挑战，且系统误差难建模去除。

Method: 提出通过优化预训练分布内模型提取的特征表示上的特征对齐损失，来对齐分布内和分布外样本特征的方法。

Result: 在MNIST数据集和中性氢大尺度地图上实验，发现最优传输在未知分布内和分布外样本奇偶性时，即使数据有限，对对齐分布外特征特别有效。

Conclusion: 所提方法能有效解决系统误差导致的分布偏移问题，最优传输在特定情况下效果好。

Abstract: Systematics contaminate observables, leading to distribution shifts relative to theoretically simulated signals-posing a major challenge for using pre-trained models to label such observables. Since systematics are often poorly understood and difficult to model, removing them directly and entirely may not be feasible. To address this challenge, we propose a novel method that aligns learned features between in-distribution (ID) and out-of-distribution (OOD) samples by optimizing a feature-alignment loss on the representations extracted from a pre-trained ID model. We first experimentally validate the method on the MNIST dataset using possible alignment losses, including mean squared error and optimal transport, and subsequently apply it to large-scale maps of neutral hydrogen. Our results show that optimal transport is particularly effective at aligning OOD features when parity between ID and OOD samples is unknown, even with limited data-mimicking real-world conditions in extracting information from large-scale surveys. Our code is available at https://github.com/sultan-hassan/feature-alignment-for-OOD-generalization.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [604] [Hierarchical Federated Graph Attention Networks for Scalable and Resilient UAV Collision Avoidance](https://arxiv.org/abs/2511.11616)
*Rathin Chandra Shit,Sharmila Subudhi*

Main category: cs.RO

TL;DR: 本文提出分层框架解决大规模多无人机系统避撞问题，平衡实时性、抗攻击和隐私保护，实现可扩展且容错。


<details>
  <summary>Details</summary>
Motivation: 现有框架计算复杂且无拜占庭容错，需平衡实时性、抗攻击和隐私保护指标。

Method: 采用三层分层架构，将智能分散到各层，提出自适应差分隐私机制，用分布式哈希表审计日志。

Result: 在所有测试集群规模下，50ms内做出95%决策的中值成本可观察到；500架无人机碰撞率<2.0%，拜占庭容错f < n/3。

Conclusion: 该架构能消除权衡，提供可扩展场景，实现隐私 - 效用最大化。

Abstract: The real-time performance, adversarial resiliency, and privacy preservation are the most important metrics that need to be balanced to practice collision avoidance in large-scale multi-UAV (Unmanned Aerial Vehicle) systems. Current frameworks tend to prescribe monolithic solutions that are not only prohibitively computationally complex with a scaling cost of $O(n^2)$ but simply do not offer Byzantine fault tolerance. The proposed hierarchical framework presented in this paper tries to eliminate such trade-offs by stratifying a three-layered architecture. We spread the intelligence into three layers: an immediate collision avoiding local layer running on dense graph attention with latency of $<10 ms$, a regional layer using sparse attention with $O(nk)$ computational complexity and asynchronous federated learning with coordinate-wise trimmed mean aggregation, and lastly, a global layer using a lightweight Hashgraph-inspired protocol. We have proposed an adaptive differential privacy mechanism, wherein the noise level $(ε\in [0.1, 1.0])$ is dynamically reduced based on an evaluation of the measured real-time threat that in turn maximized the privacy-utility tradeoff. Through the use of Distributed Hash Table (DHT)-based lightweight audit logging instead of heavyweight blockchain consensus, the median cost of getting a $95^{th}$ percentile decision within 50ms is observed across all tested swarm sizes. This architecture provides a scalable scenario of 500 UAVs with a collision rate of $< 2.0\%$ and the Byzantine fault tolerance of $f < n/3$.

</details>


### [605] [ExpertAD: Enhancing Autonomous Driving Systems with Mixture of Experts](https://arxiv.org/abs/2511.11740)
*Haowen Jiang,Xinyu Huang,You Lu,Dingji Wang,Yuheng Cao,Chaofeng Sha,Bihuan Chen,Keyu Chen,Xin Peng*

Main category: cs.RO

TL;DR: 针对端到端自动驾驶系统的挑战，提出ExpertAD框架，实验显示它能降低碰撞率和推理延迟，有良好泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶系统存在决策可靠性低、任务干扰、推理延迟等问题。

Method: 提出ExpertAD框架，引入Perception Adapter增强关键特征，使用Mixture of Sparse Experts减少任务干扰。

Result: 与先前方法相比，ExpertAD将平均碰撞率降低了20%，推理延迟降低了25%，在罕见场景和未知城市环境中表现良好。

Conclusion: ExpertAD框架有效提升了自动驾驶系统的性能。

Abstract: Recent advancements in end-to-end autonomous driving systems (ADSs) underscore their potential for perception and planning capabilities. However, challenges remain. Complex driving scenarios contain rich semantic information, yet ambiguous or noisy semantics can compromise decision reliability, while interference between multiple driving tasks may hinder optimal planning. Furthermore, prolonged inference latency slows decision-making, increasing the risk of unsafe driving behaviors. To address these challenges, we propose ExpertAD, a novel framework that enhances the performance of ADS with Mixture of Experts (MoE) architecture. We introduce a Perception Adapter (PA) to amplify task-critical features, ensuring contextually relevant scene understanding, and a Mixture of Sparse Experts (MoSE) to minimize task interference during prediction, allowing for effective and efficient planning. Our experiments show that ExpertAD reduces average collision rates by up to 20% and inference latency by 25% compared to prior methods. We further evaluate its multi-skill planning capabilities in rare scenarios (e.g., accidents, yielding to emergency vehicles) and demonstrate strong generalization to unseen urban environments. Additionally, we present a case study that illustrates its decision-making process in complex driving scenarios.

</details>


### [606] [Autonomous Underwater Cognitive System for Adaptive Navigation: A SLAM-Integrated Cognitive Architecture](https://arxiv.org/abs/2511.11845)
*K. A. I. N Jayarathne,R. M. N. M. Rathnayaka,D. P. S. S. Peiris*

Main category: cs.RO

TL;DR: 提出集成SLAM与Soar认知架构的自主水下认知系统AUCS用于深海探测，能智能感知、推理和适应，为下一代认知潜水系统奠基。


<details>
  <summary>Details</summary>
Motivation: 解决深海探索中动态水下环境导致的迷失方向、通信丢失和导航故障等挑战。

Method: 将SLAM与基于Soar的认知架构集成，融合多传感器数据与认知推理模块，融入语义理解、自适应传感器管理和基于记忆的学习。

Result: 该系统能区分动态和静态对象，减少错误回环闭合，增强长期地图一致性，实现完整的感知 - 认知 - 行动 - 学习循环。

Conclusion: 为下一代认知潜水系统奠定基础，提高深海探索的安全性、可靠性和自主性。

Abstract: Deep-sea exploration poses significant challenges, including disorientation, communication loss, and navigational failures in dynamic underwater environments. This paper presents an Autonomous Underwater Cognitive System (AUCS) that integrates Simultaneous Localization and Mapping (SLAM) with a Soar-based cognitive architecture to enable adaptive navigation in complex oceanic conditions. The system fuses multi-sensor data from SONAR, LiDAR, IMU, and DVL with cognitive reasoning modules for perception, attention, planning, and learning. Unlike conventional SLAM systems, AUCS incorporates semantic understanding, adaptive sensor management, and memory-based learning to differentiate between dynamic and static objects, reducing false loop closures and enhancing long-term map consistency. The proposed architecture demonstrates a complete perception-cognition-action-learning loop, allowing autonomous underwater vehicles to sense, reason, and adapt intelligently. This work lays a foundation for next-generation cognitive submersible systems, improving safety, reliability, and autonomy in deep-sea exploration.

</details>


### [607] [Decoupled Action Head: Confining Task Knowledge to Conditioning Layers](https://arxiv.org/abs/2511.12101)
*Jian Zhou,Sihao Lin,Shuai Fu,Qi WU*

Main category: cs.RO

TL;DR: 提出解耦训练方法，利用运动学轨迹预训练动作头，实验证明可行且提升效率，还推出轻量的DP - MLP模型。


<details>
  <summary>Details</summary>
Motivation: 现有行为克隆方法受限于训练数据稀缺，对扩散策略模型有效性的内部机制理解不足，导致泛化性有限和缺乏原则性设计。

Method: 提出解耦训练方法，用运动学生成轨迹预训练动作头，冻结后通过特征调制适应新任务；推出DP - MLP模型替换DP - C的U - Net骨干。

Result: 解耦训练方法在分布内和分布外场景均可行，DP - C训练速度最高提升41%；DP - MLP正常训练提速83.9%，解耦训练提速89.1%。

Conclusion: 解耦训练可提升训练效率，动作生成骨干在机器人操作中作用有限，轻量的DP - MLP模型可显著提升训练速度。

Abstract: Behavior Cloning (BC) is a data-driven supervised learning approach that has gained increasing attention with the success of scaling laws in language and vision domains. Among its implementations in robotic manipulation, Diffusion Policy (DP), with its two variants DP-CNN (DP-C) and DP-Transformer (DP-T), is one of the most effective and widely adopted models, demonstrating the advantages of predicting continuous action sequences. However, both DP and other BC methods remain constrained by the scarcity of paired training data, and the internal mechanisms underlying DP's effectiveness remain insufficiently understood, leading to limited generalization and a lack of principled design in model development. In this work, we propose a decoupled training recipe that leverages nearly cost-free kinematics-generated trajectories as observation-free data to pretrain a general action head (action generator). The pretrained action head is then frozen and adapted to novel tasks through feature modulation. Our experiments demonstrate the feasibility of this approach in both in-distribution and out-of-distribution scenarios. As an additional benefit, decoupling improves training efficiency; for instance, DP-C achieves up to a 41% speedup. Furthermore, the confinement of task-specific knowledge to the conditioning components under decoupling, combined with the near-identical performance of DP-C in both normal and decoupled training, indicates that the action generation backbone plays a limited role in robotic manipulation. Motivated by this observation, we introduce DP-MLP, which replaces the 244M-parameter U-Net backbone of DP-C with only 4M parameters of simple MLP blocks, achieving a 83.9% faster training speed under normal training and 89.1% under decoupling.

</details>


### [608] [Locally Optimal Solutions to Constraint Displacement Problems via Path-Obstacle Overlaps](https://arxiv.org/abs/2511.12203)
*Antony Thomas,Fulvio Mastrogiovanni,Marco Baglietto*

Main category: cs.RO

TL;DR: 提出统一方法解决约束位移问题，通过两阶段过程找到局部最优障碍位移使机器人路径可行，并给出示例验证。


<details>
  <summary>Details</summary>
Motivation: 解决机器人通过移动约束或障碍物寻找可行路径的约束位移问题。

Method: 提出两阶段过程，第一阶段计算穿过障碍物的轨迹并最小化目标函数，第二阶段移动障碍物使轨迹无碰撞。

Result: 通过多个示例成功在两类约束位移问题上验证了方法。

Conclusion: 所提出的统一方法能有效解决约束位移问题。

Abstract: We present a unified approach for constraint displacement problems in which a robot finds a feasible path by displacing constraints or obstacles. To this end, we propose a two stage process that returns locally optimal obstacle displacements to enable a feasible path for the robot. The first stage proceeds by computing a trajectory through the obstacles while minimizing an appropriate objective function. In the second stage, these obstacles are displaced to make the computed robot trajectory feasible, that is, collision-free. Several examples are provided that successfully demonstrate our approach on two distinct classes of constraint displacement problems.

</details>


### [609] [Tactile Data Recording System for Clothing with Motion-Controlled Robotic Sliding](https://arxiv.org/abs/2511.11634)
*Michikuni Eguchi,Takekazu Kitagishi,Yuichi Hiroi,Takefumi Hiraki*

Main category: cs.RO

TL;DR: 提出基于机械臂的系统收集完整服装触觉数据，创建带运动标签的多模态触觉数据库，证明运动相关标签对表征服装触觉的有效性。


<details>
  <summary>Details</summary>
Motivation: 揭示使服装舒适的物理特性，需系统收集滑动运动中的触觉数据。

Method: 提出基于机械臂的系统，用模拟指尖进行抚摸测量，精确控制速度和方向，创建带运动标签的多模态触觉数据库。

Result: 机器学习评估显示，包含运动相关参数提高了音频和加速度数据的识别准确性。

Conclusion: 该系统为捕捉服装触觉数据提供了可扩展、无损的方法，有助于未来织物感知和再现研究。

Abstract: The tactile sensation of clothing is critical to wearer comfort. To reveal physical properties that make clothing comfortable, systematic collection of tactile data during sliding motion is required. We propose a robotic arm-based system for collecting tactile data from intact garments. The system performs stroking measurements with a simulated fingertip while precisely controlling speed and direction, enabling creation of motion-labeled, multimodal tactile databases. Machine learning evaluation showed that including motion-related parameters improved identification accuracy for audio and acceleration data, demonstrating the efficacy of motion-related labels for characterizing clothing tactile sensation. This system provides a scalable, non-destructive method for capturing tactile data of clothing, contributing to future studies on fabric perception and reproduction.

</details>


### [610] [Towards High-Consistency Embodied World Model with Multi-View Trajectory Videos](https://arxiv.org/abs/2511.12882)
*Taiyi Su,Jian Zhu,Yaxuan Li,Chong Ma,Zitai Huang,Yichen Zhu,Hanli Wang,Yi Xu*

Main category: cs.RO

TL;DR: 提出MTV - World具身世界模型，用多视图轨迹视频控制实现精确视觉运动预测，开发自动评估管道，实验表明其在复杂双臂场景表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有具身世界模型难以将低级动作准确转化为预测帧中的机器人精确运动，与真实物理交互存在不一致性。

Method: 提出MTV - World模型，采用多视图轨迹视频作为控制信号，构建多视图框架补偿空间信息损失；开发基于多模态大模型和参考视频目标分割模型的自动评估管道，用Jaccard指数衡量空间一致性。

Result: MTV - World在复杂双臂场景中实现了精确控制执行和准确物理交互建模。

Conclusion: MTV - World模型有效解决了现有具身世界模型的局限性，能实现精确控制和准确物理交互建模。

Abstract: Embodied world models aim to predict and interact with the physical world through visual observations and actions. However, existing models struggle to accurately translate low-level actions (e.g., joint positions) into precise robotic movements in predicted frames, leading to inconsistencies with real-world physical interactions. To address these limitations, we propose MTV-World, an embodied world model that introduces Multi-view Trajectory-Video control for precise visuomotor prediction. Specifically, instead of directly using low-level actions for control, we employ trajectory videos obtained through camera intrinsic and extrinsic parameters and Cartesian-space transformation as control signals. However, projecting 3D raw actions onto 2D images inevitably causes a loss of spatial information, making a single view insufficient for accurate interaction modeling. To overcome this, we introduce a multi-view framework that compensates for spatial information loss and ensures high-consistency with physical world. MTV-World forecasts future frames based on multi-view trajectory videos as input and conditioning on an initial frame per view. Furthermore, to systematically evaluate both robotic motion precision and object interaction accuracy, we develop an auto-evaluation pipeline leveraging multimodal large models and referring video object segmentation models. To measure spatial consistency, we formulate it as an object location matching problem and adopt the Jaccard Index as the evaluation metric. Extensive experiments demonstrate that MTV-World achieves precise control execution and accurate physical interaction modeling in complex dual-arm scenarios.

</details>


### [611] [EL3DD: Extended Latent 3D Diffusion for Language Conditioned Multitask Manipulation](https://arxiv.org/abs/2511.13312)
*Jonas Bode,Raphael Memmesheimer,Sven Behnke*

Main category: cs.RO

TL;DR: 本文利用扩散模型在视觉运动策略框架中生成精确机器人轨迹，在CALVIN数据集上评估证明性能提升，有助于通用多任务操作。


<details>
  <summary>Details</summary>
Motivation: 通用机器人需理解自然语言并应用于物理任务，要利用扩散模型能力生成精确轨迹。

Method: 在视觉运动策略框架中利用扩散模型，结合视觉和文本输入，训练时使用参考演示，利用改进嵌入扩展现有模型，借鉴图像生成的扩散模型技术。

Result: 在CALVIN数据集上评估，多种操作任务性能提升，多任务顺序执行时长期成功率增加。

Conclusion: 强化了扩散模型的实用性，为通用多任务操作做出贡献。

Abstract: Acting in human environments is a crucial capability for general-purpose robots, necessitating a robust understanding of natural language and its application to physical tasks. This paper seeks to harness the capabilities of diffusion models within a visuomotor policy framework that merges visual and textual inputs to generate precise robotic trajectories. By employing reference demonstrations during training, the model learns to execute manipulation tasks specified through textual commands within the robot's immediate environment. The proposed research aims to extend an existing model by leveraging improved embeddings, and adapting techniques from diffusion models for image generation. We evaluate our methods on the CALVIN dataset, proving enhanced performance on various manipulation tasks and an increased long-horizon success rate when multiple tasks are executed in sequence. Our approach reinforces the usefulness of diffusion models and contributes towards general multitask manipulation.

</details>


### [612] [Prompt-Driven Domain Adaptation for End-to-End Autonomous Driving via In-Context RL](https://arxiv.org/abs/2511.12755)
*Aleesha Khurram,Amir Moeini,Shangtong Zhang,Rohan Chandra*

Main category: cs.RO

TL;DR: 提出使用上下文强化学习（ICRL）的推理时少样本提示驱动域适应方法用于恶劣天气下闭环自动驾驶，实验显示比现有方法更优。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶系统存在域适应问题，典型策略不实用，现有提示驱动DA方法有局限。

Method: 采用上下文强化学习（ICRL）进行推理时少样本提示驱动的域适应，不更新模型参数和额外收集数据，使用推理时观察到的通用轨迹。

Result: 在CARLA模拟器实验中，ICRL在目标域产生更安全、高效和舒适的驾驶策略。

Conclusion: ICRL方法在恶劣天气下闭环自动驾驶的提示驱动域适应方面优于现有方法。

Abstract: Despite significant progress and advances in autonomous driving, many end-to-end systems still struggle with domain adaptation (DA), such as transferring a policy trained under clear weather to adverse weather conditions. Typical DA strategies in the literature include collecting additional data in the target domain or re-training the model, or both. Both these strategies quickly become impractical as we increase scale and complexity of driving. These limitations have encouraged investigation into few-shot and zero-shot prompt-driven DA at inference time involving LLMs and VLMs. These methods work by adding a few state-action trajectories during inference to the prompt (similar to in-context learning). However, there are two limitations of such an approach: $(i)$ prompt-driven DA methods are currently restricted to perception tasks such as detection and segmentation and $(ii)$ they require expert few-shot data. In this work, we present a new approach to inference-time few-shot prompt-driven DA for closed-loop autonomous driving in adverse weather condition using in-context reinforcement learning (ICRL). Similar to other prompt-driven DA methods, our approach does not require any updates to the model parameters nor does it require additional data collection in adversarial weather regime. Furthermore, our approach advances the state-of-the-art in prompt-driven DA by extending to closed driving using general trajectories observed during inference. Our experiments using the CARLA simulator show that ICRL results in safer, more efficient, and more comfortable driving policies in the target domain compared to state-of-the-art prompt-driven DA baselines.

</details>


### [613] [Towards Affect-Adaptive Human-Robot Interaction: A Protocol for Multimodal Dataset Collection on Social Anxiety](https://arxiv.org/abs/2511.13530)
*Vesna Poprcova,Iulia Lefter,Matthias Wieser,Martijn Warnier,Frances Brazier*

Main category: cs.RO

TL;DR: 本文提出在人机交互情境下收集反映社交焦虑的多模态数据集的协议，该数据集有助于社交焦虑的多模态检测。


<details>
  <summary>Details</summary>
Motivation: 社交焦虑影响人际互动和社会功能，准确检测相关情感状态和行为需多模态数据集，但此类数据集稀缺，限制了研究和应用进展。

Method: 提出多模态数据集收集协议，在受控实验条件下，让至少70名按社交焦虑水平分组的参与者与Furhat社交机器人进行约10分钟的交互式Wizard - of - Oz角色扮演场景，同步采集音频、视频和生理记录，并添加情境数据。

Result: 将得到包含同步音频、视频、生理记录及情境数据的多模态数据集。

Conclusion: 这项工作可为情感自适应人机交互研究提供支持，助力社交焦虑的稳健多模态检测。

Abstract: Social anxiety is a prevalent condition that affects interpersonal interactions and social functioning. Recent advances in artificial intelligence and social robotics offer new opportunities to examine social anxiety in the human-robot interaction context. Accurate detection of affective states and behaviours associated with social anxiety requires multimodal datasets, where each signal modality provides complementary insights into its manifestations. However, such datasets remain scarce, limiting progress in both research and applications. To address this, this paper presents a protocol for multimodal dataset collection designed to reflect social anxiety in a human-robot interaction context. The dataset will consist of synchronised audio, video, and physiological recordings acquired from at least 70 participants, grouped according to their level of social anxiety, as they engage in approximately 10-minute interactive Wizard-of-Oz role-play scenarios with the Furhat social robot under controlled experimental conditions. In addition to multimodal data, the dataset will be enriched with contextual data providing deeper insight into individual variability in social anxiety responses. This work can contribute to research on affect-adaptive human-robot interaction by providing support for robust multimodal detection of social anxiety.

</details>


### [614] [Structured Imitation Learning of Interactive Policies through Inverse Games](https://arxiv.org/abs/2511.12848)
*Max M. Sun,Todd Murphey*

Main category: cs.RO

TL;DR: 本文提出结构化模仿学习框架用于交互式策略，在合成任务中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 基于生成模型的模仿学习在学习无明确沟通的人机交互策略方面存在挑战，因多智能体交互行为复杂度高。

Method: 将生成式单智能体策略学习与博弈论结构结合，分两步学习，先从多智能体示范中学习个体行为模式，再通过求解逆博弈问题学习智能体间依赖关系。

Result: 在合成的5智能体社交导航任务中，该方法显著提升非交互策略，仅用50个示范就能达到与真实交互策略相当的表现。

Conclusion: 结构化模仿学习在交互式场景中有潜力。

Abstract: Generative model-based imitation learning methods have recently achieved strong results in learning high-complexity motor skills from human demonstrations. However, imitation learning of interactive policies that coordinate with humans in shared spaces without explicit communication remains challenging, due to the significantly higher behavioral complexity in multi-agent interactions compared to non-interactive tasks. In this work, we introduce a structured imitation learning framework for interactive policies by combining generative single-agent policy learning with a flexible yet expressive game-theoretic structure. Our method explicitly separates learning into two steps: first, we learn individual behavioral patterns from multi-agent demonstrations using standard imitation learning; then, we structurally learn inter-agent dependencies by solving an inverse game problem. Preliminary results in a synthetic 5-agent social navigation task show that our method significantly improves non-interactive policies and performs comparably to the ground truth interactive policy using only 50 demonstrations. These results highlight the potential of structured imitation learning in interactive settings.

</details>


### [615] [Orientation-Free Neural Network-Based Bias Estimation for Low-Cost Stationary Accelerometers](https://arxiv.org/abs/2511.13071)
*Michal Levin,Itzik Klein*

Main category: cs.RO

TL;DR: 本文提出无模型基于学习的校准方法估计加速度计偏差，实验显示比传统技术误差低超52%，有助于免定向校准发展。


<details>
  <summary>Details</summary>
Motivation: 低成本微机电加速度计受偏差误差影响性能下降，传统校准需加速度计调平或复杂定向校准程序。

Method: 提出无模型基于学习的校准方法，在静止条件下估计加速度计偏差，无需传感器定向知识和旋转传感器。

Result: 在13.39小时的六加速度计数据集实验中，该方法误差比传统技术低超52%。

Conclusion: 该工作推动免定向场景下准确校准方法发展，提高低成本惯性传感器可靠性，消除调平校准需求。

Abstract: Low-cost micro-electromechanical accelerometers are widely used in navigation, robotics, and consumer devices for motion sensing and position estimation. However, their performance is often degraded by bias errors. To eliminate deterministic bias terms a calibration procedure is applied under stationary conditions. It requires accelerom- eter leveling or complex orientation-dependent calibration procedures. To overcome those requirements, in this paper we present a model-free learning-based calibration method that estimates accelerometer bias under stationary conditions, without requiring knowledge of the sensor orientation and without the need to rotate the sensors. The proposed approach provides a fast, practical, and scalable solution suitable for rapid field deployment. Experimental validation on a 13.39-hour dataset collected from six accelerometers shows that the proposed method consistently achieves error levels more than 52% lower than traditional techniques. On a broader scale, this work contributes to the advancement of accurate calibration methods in orientation-free scenarios. As a consequence, it improves the reliability of low-cost inertial sensors in diverse scientific and industrial applications and eliminates the need for leveled calibration.

</details>


### [616] [From Power to Precision: Learning Fine-grained Dexterity for Multi-fingered Robotic Hands](https://arxiv.org/abs/2511.13710)
*Jianglong Ye,Lai Wei,Guangqi Jiang,Changwei Jing,Xueyan Zou,Xiaolong Wang*

Main category: cs.RO

TL;DR: 文章指出当前机器人手设计难以兼顾强力抓取和精确操作，提出联合优化多指灵巧手的控制和硬件设计，通过轻量级指尖几何修改等方法实现，实验验证能提升操作能力。


<details>
  <summary>Details</summary>
Motivation: 当前多指机器人手在强力抓取有效，但精确操作多用平行夹爪，存在难以在单一系统实现两种操作的局限，需解决此问题。

Method: 引入轻量级指尖几何修改并表示为接触平面，联合优化其参数与对应控制；控制策略动态切换操作模式，简化精确控制；利用大规模仿真和可微神经物理替代模型优化指尖几何。

Result: 在仿真到真实和真实到真实环境实验中，仿真到真实精确抓取未见物体零样本成功率达82.5%，现实面包捏取任务成功率93.3%。

Conclusion: 联合设计框架能显著提升多指手精细操作能力且不降低强力抓取能力。

Abstract: Human grasps can be roughly categorized into two types: power grasps and precision grasps. Precision grasping enables tool use and is believed to have influenced human evolution. Today's multi-fingered robotic hands are effective in power grasps, but for tasks requiring precision, parallel grippers are still more widely adopted. This contrast highlights a key limitation in current robotic hand design: the difficulty of achieving both stable power grasps and precise, fine-grained manipulation within a single, versatile system. In this work, we bridge this gap by jointly optimizing the control and hardware design of a multi-fingered dexterous hand, enabling both power and precision manipulation. Rather than redesigning the entire hand, we introduce a lightweight fingertip geometry modification, represent it as a contact plane, and jointly optimize its parameters along with the corresponding control. Our control strategy dynamically switches between power and precision manipulation and simplifies precision control into parallel thumb-index motions, which proves robust for sim-to-real transfer. On the design side, we leverage large-scale simulation to optimize the fingertip geometry using a differentiable neural-physics surrogate model. We validate our approach through extensive experiments in both sim-to-real and real-to-real settings. Our method achieves an 82.5% zero-shot success rate on unseen objects in sim-to-real precision grasping, and a 93.3% success rate in challenging real-world tasks involving bread pinching. These results demonstrate that our co-design framework can significantly enhance the fine-grained manipulation ability of multi-fingered hands without reducing their ability for power grasps. Our project page is at https://jianglongye.com/power-to-precision

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [617] [MedBuild AI: An Agent-Based Hybrid Intelligence Framework for Reshaping Agency in Healthcare Infrastructure Planning through Generative Design for Medical Architecture](https://arxiv.org/abs/2511.11587)
*Yiming Zhang,Yuejia Xu,Ziyao Wang,Xin Yan,Xiaosai Hao*

Main category: cs.HC

TL;DR: 本文介绍MedBuild AI框架，可助力全球医疗建筑早期设计规划，采用混合智能，让有卫星网络地区获取低成本医疗建筑设计指导，推动公平医疗规划。


<details>
  <summary>Details</summary>
Motivation: 全球医疗基础设施差异大，传统规划慢且不可及，现有援助项目无法满足需求。

Method: 引入MedBuild AI混合智能框架，结合大语言模型与确定性专家系统，通过三个代理运行系统。

Result: 能让有卫星互联网地区获得模块化、低成本医疗建筑设计指导。

Conclusion: MedBuild AI通过在设计过程嵌入计算协商，推动了互惠、包容、公平的医疗规划，赋予社区权力，重新定义全球医疗建筑能动性。

Abstract: Globally, disparities in healthcare infrastructure remain stark, leaving countless communities without access to even basic services. Traditional infrastructure planning is often slow and inaccessible, and although many architects are actively delivering humanitarian and aid-driven hospital projects worldwide, these vital efforts still fall far short of the sheer scale and urgency of demand. This paper introduces MedBuild AI, a hybrid-intelligence framework that integrates large language models (LLMs) with deterministic expert systems to rebalance the early design and conceptual planning stages. As a web-based platform, it enables any region with satellite internet access to obtain guidance on modular, low-tech, low-cost medical building designs. The system operates through three agents: the first gathers local health intelligence via conversational interaction; the second translates this input into an architectural functional program through rule-based computation; and the third generates layouts and 3D models. By embedding computational negotiation into the design process, MedBuild AI fosters a reciprocal, inclusive, and equitable approach to healthcare planning, empowering communities and redefining agency in global healthcare architecture.

</details>


### [618] [Social and Physical Attributes-Defined Trust Evaluation for Effective Collaborator Selection in Human-Device Coexistence Systems](https://arxiv.org/abs/2511.11578)
*Botao Zhu,Xianbin Wang*

Main category: cs.HC

TL;DR: 本文提出HSLCCA方法用于人类-设备共存系统中设备信任评估，实验表明该方法显著优于基线算法。


<details>
  <summary>Details</summary>
Motivation: 人类-设备共存系统中，基于物理和社会属性进行潜在合作者信任评估很关键，但高效整合这些属性进行准确信任评估有挑战。

Method: 构建关系超图捕捉设备间关系，开发自监督学习框架整合多维关系生成设备嵌入，将关系超图增强为两个视图，用参数共享的超图神经网络学习嵌入，应用CCA方法提升嵌入质量，最后基于嵌入计算设备可信度。

Result: 广泛实验表明，提出的HSLCCA方法在有效识别可信设备方面显著优于基线算法。

Conclusion: HSLCCA方法能有效克服整合物理和社会属性进行信任评估的困难，在识别可信设备上表现出色。

Abstract: In human-device coexistence systems, collaborations among devices are determined by not only physical attributes such as network topology but also social attributes among human users. Consequently, trust evaluation of potential collaborators based on these multifaceted attributes becomes critical for ensuring the eventual outcome. However, due to the high heterogeneity and complexity of physical and social attributes, efficiently integrating them for accurate trust evaluation remains challenging. To overcome this difficulty, a canonical correlation analysis-enhanced hypergraph self-supervised learning (HSLCCA) method is proposed in this research. First, by treating all attributes as relationships among connected devices, a relationship hypergraph is constructed to comprehensively capture inter-device relationships across three dimensions: spatial attribute-related, device attribute-related, and social attribute-related. Next, a self-supervised learning framework is developed to integrate these multi-dimensional relationships and generate device embeddings enriched with relational semantics. In this learning framework, the relationship hypergraph is augmented into two distinct views to enhance semantic information. A parameter-sharing hypergraph neural network is then utilized to learn device embeddings from both views. To further enhance embedding quality, a CCA approach is applied, allowing the comparison of data between the two views. Finally, the trustworthiness of devices is calculated based on the learned device embeddings. Extensive experiments demonstrate that the proposed HSLCCA method significantly outperforms the baseline algorithm in effectively identifying trusted devices.

</details>


### [619] [Agent-Oriented Visual Programming for the Web of Things](https://arxiv.org/abs/2511.13158)
*Samuele Burattini,Alessandro Ricci,Simon Mayer,Danai Vachtsevanou,Jeremy Lemee,Andrei Ciortea,Angelo Croatti*

Main category: cs.HC

TL;DR: 介绍面向多智能体的可视化编程方法，设计实现相关系统，结合WoT并通过用户研究验证其可用性


<details>
  <summary>Details</summary>
Motivation: 让无编程经验但有特定领域知识的人设计和配置自主软件，且认为使用智能体抽象比过程式编程更简单

Method: 设计并实现基于JaCaMo平台的块式可视化开发环境系统，集成Web of Things

Result: 通过用户研究验证新手用户能利用该开发环境创建多智能体系统解决简单自动化任务

Conclusion: 所提出的面向多智能体的可视化编程方法可行，能帮助新手用户创建多智能体系统

Abstract: In this paper we introduce and discuss an approach for multi-agent-oriented visual programming. This aims at enabling individuals without programming experience but with knowledge in specific target domains to design and (re)configure autonomous software. We argue that, compared to procedural programming, it should be simpler for users to create programs when agent abstractions are employed. The underlying rationale is that these abstractions, and specifically the belief-desire-intention architecture that is aligned with human practical reasoning, match more closely with people's everyday experience in interacting with other agents and artifacts in the real world. On top of this, we designed and implemented a visual programming system for agents that hides the technicalities of agent-oriented programming using a blocks-based visual development environment that is built on the JaCaMo platform. To further validate the proposed solution, we integrate the Web of Things (WoT) to let users create autonomous behaviour on top of physical mashups of devices, following the trends in industrial end-user programming. Finally, we report on a pilot user study where we verified that novice users are indeed able to make use of this development environment to create multi-agent systems to solve simple automation tasks.

</details>


### [620] [Accepted with Minor Revisions: Value of AI-Assisted Scientific Writing](https://arxiv.org/abs/2511.12529)
*Sanchaita Hazra,Doeun Lee,Bodhisattwa Prasad Majumder,Sachin Kumar*

Main category: cs.HC

TL;DR: 研究大语言模型在科学写作摘要创作中的应用，发现AI生成摘要经少量修改可达与人类相当的可接受度，源信息披露很重要。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在科学写作中的有效性尚不明确，研究其支持领域专家进行科学写作尤其是摘要创作的潜力。

Method: 设计激励式随机对照试验，采用2x2受试者间设计，设置不同摘要来源和披露情况。

Result: 作者编辑人类撰写摘要时修改更多；披露源信息后修改量趋同；审稿人决策与修改量相关；AI生成摘要经修改可接近人类水平。

Conclusion: AI生成摘要经少量修改可与人类相当，感知的AI作者身份影响编辑行为，源信息披露在协作科学写作中很重要。

Abstract: Large Language Models have seen expanding application across domains, yet their effectiveness as assistive tools for scientific writing -- an endeavor requiring precision, multimodal synthesis, and domain expertise -- remains insufficiently understood. We examine the potential of LLMs to support domain experts in scientific writing, with a focus on abstract composition. We design an incentivized randomized controlled trial with a hypothetical conference setup where participants with relevant expertise are split into an author and reviewer pool. Inspired by methods in behavioral science, our novel incentive structure encourages authors to edit the provided abstracts to an acceptable quality for a peer-reviewed submission. Our 2x2 between-subject design expands into two dimensions: the implicit source of the provided abstract and the disclosure of it. We find authors make most edits when editing human-written abstracts compared to AI-generated abstracts without source attribution, often guided by higher perceived readability in AI generation. Upon disclosure of source information, the volume of edits converges in both source treatments. Reviewer decisions remain unaffected by the source of the abstract, but bear a significant correlation with the number of edits made. Careful stylistic edits, especially in the case of AI-generated abstracts, in the presence of source information, improve the chance of acceptance. We find that AI-generated abstracts hold potential to reach comparable levels of acceptability to human-written ones with minimal revision, and that perceptions of AI authorship, rather than objective quality, drive much of the observed editing behavior. Our findings reverberate the significance of source disclosure in collaborative scientific writing.

</details>


### [621] [Maximizing the efficiency of human feedback in AI alignment: a comparative analysis](https://arxiv.org/abs/2511.12796)
*Andreas Chouliaras,Dimitris Chatzopoulos*

Main category: cs.HC

TL;DR: 本文探索RLHF中偏好推断的替代采样和评估策略，提出Swiss InfoGain方法，在受限标注预算下表现佳，实验证明自适应策略有优势。


<details>
  <summary>Details</summary>
Motivation: 现有随机对采样与Bradley - Terry建模在受限标注预算下有统计局限性和低效问题，需探索替代策略。

Method: 借鉴博弈论、统计学和社会选择理论，提出Swiss InfoGain方法，采用瑞士锦标赛系统和代理互信息增益配对规则。

Result: Swiss InfoGain在受限标注预算下显著优于其他方法且样本效率更高，在高资源设置中也能找到优于Bradley - Terry基线的替代方法。

Conclusion: 自适应、资源感知策略可减少冗余、增强鲁棒性，在偏好学习上有显著改进，强调在RLHF流程中平衡对齐质量和人工工作量的重要性。

Abstract: Reinforcement Learning from Human Feedback (RLHF) relies on preference modeling to align machine learning systems with human values, yet the popular approach of random pair sampling with Bradley-Terry modeling is statistically limited and inefficient under constrained annotation budgets. In this work, we explore alternative sampling and evaluation strategies for preference inference in RLHF, drawing inspiration from areas such as game theory, statistics, and social choice theory. Our best-performing method, Swiss InfoGain, employs a Swiss tournament system with a proxy mutual-information-gain pairing rule, which significantly outperforms all other methods in constrained annotation budgets while also being more sample-efficient. Even in high-resource settings, we can identify superior alternatives to the Bradley-Terry baseline. Our experiments demonstrate that adaptive, resource-aware strategies reduce redundancy, enhance robustness, and yield statistically significant improvements in preference learning, highlighting the importance of balancing alignment quality with human workload in RLHF pipelines.

</details>


### [622] [Enhancing XR Auditory Realism via Multimodal Scene-Aware Acoustic Rendering](https://arxiv.org/abs/2511.11930)
*Tianyu Xu,Jihan Li,Penghe Zu,Pranav Sahay,Maruchi Kim,Jack Obeng-Marnu,Farley Miller,Xun Qian,Katrina Passarella,Mahitha Rachumalla,Rajeev Nongpiur,D. Shin*

Main category: cs.HC

TL;DR: 提出SAMOSA系统解决XR空间音频渲染难适应场景问题，经评估证实能增强XR听觉真实感。


<details>
  <summary>Details</summary>
Motivation: 现有XR空间音频渲染方法难实时适应多样物理场景，导致视听线索不匹配，影响用户沉浸感。

Method: 引入SAMOSA系统，融合房间几何、表面材料和语义驱动的声学上下文实时估计，形成多模态场景表示，通过场景先验进行声学校准，合成高度逼真的房间脉冲响应（RIR）。

Result: 通过声学指标技术评估和专家评估，验证了SAMOSA系统的可行性和有效性。

Conclusion: SAMOSA系统在增强XR听觉真实感方面具有可行性和有效性。

Abstract: In Extended Reality (XR), rendering sound that accurately simulates real-world acoustics is pivotal in creating lifelike and believable virtual experiences. However, existing XR spatial audio rendering methods often struggle with real-time adaptation to diverse physical scenes, causing a sensory mismatch between visual and auditory cues that disrupts user immersion. To address this, we introduce SAMOSA, a novel on-device system that renders spatially accurate sound by dynamically adapting to its physical environment. SAMOSA leverages a synergistic multimodal scene representation by fusing real-time estimations of room geometry, surface materials, and semantic-driven acoustic context. This rich representation then enables efficient acoustic calibration via scene priors, allowing the system to synthesize a highly realistic Room Impulse Response (RIR). We validate our system through technical evaluation using acoustic metrics for RIR synthesis across various room configurations and sound types, alongside an expert evaluation (N=12). Evaluation results demonstrate SAMOSA's feasibility and efficacy in enhancing XR auditory realism.

</details>


### [623] [Multi-Domain EEG Representation Learning with Orthogonal Mapping and Attention-based Fusion for Cognitive Load Classification](https://arxiv.org/abs/2511.12394)
*Prithila Angkan,Amin Jalali,Paul Hungler,Ali Etemad*

Main category: cs.HC

TL;DR: 提出基于EEG的认知负荷分类新表征学习方法，结合时频域，用多域注意力模块和正交投影约束，在两公开数据集验证效果优且稳定。


<details>
  <summary>Details</summary>
Motivation: 寻找更有效的基于EEG的认知负荷分类表征学习方法。

Method: 先通过卷积编码器获取时域表征，计算PSD生成多光谱地形图获取频域表征，用多域注意力模块映射到共享嵌入空间，训练时加入正交投影约束。

Result: 在CL - Drive和CLARE数据集上验证优于传统单域技术，消融和敏感性分析评估各组件影响，鲁棒性实验显示比其他先进方法稳定。

Conclusion: 多域方法在认知负荷分类上效果好且稳定，优于传统单域技术。

Abstract: We propose a new representation learning solution for the classification of cognitive load based on Electroencephalogram (EEG). Our method integrates both time and frequency domains by first passing the raw EEG signals through the convolutional encoder to obtain the time domain representations. Next, we measure the Power Spectral Density (PSD) for all five EEG frequency bands and generate the channel power values as 2D images referred to as multi-spectral topography maps. These multi-spectral topography maps are then fed to a separate encoder to obtain the representations in frequency domain. Our solution employs a multi-domain attention module that maps these domain-specific embeddings onto a shared embedding space to emphasize more on important inter-domain relationships to enhance the representations for cognitive load classification. Additionally, we incorporate an orthogonal projection constraint during the training of our method to effectively increase the inter-class distances while improving intra-class clustering. This enhancement allows efficient discrimination between different cognitive states and aids in better grouping of similar states within the feature space. We validate the effectiveness of our model through extensive experiments on two public EEG datasets, CL-Drive and CLARE for cognitive load classification. Our results demonstrate the superiority of our multi-domain approach over the traditional single-domain techniques. Moreover, we conduct ablation and sensitivity analyses to assess the impact of various components of our method. Finally, robustness experiments on different amounts of added noise demonstrate the stability of our method compared to other state-of-the-art solutions.

</details>


### [624] [Trust in Vision-Language Models: Insights from a Participatory User Workshop](https://arxiv.org/abs/2511.13458)
*Agnese Chiatti,Lara Piccolo,Sara Bernardini,Matteo Matteucci,Viola Schiaffonati*

Main category: cs.HC

TL;DR: 研究通过用户中心方法开展VLM用户工作坊，为信任度量和参与策略研究提供初步结果。


<details>
  <summary>Details</summary>
Motivation: 随着VLM部署增多，需工具让用户判断何时信任该系统，但用户对VLM信任的建立和演变研究不足，且依赖AI模型验证实验存在问题。

Method: 采用以用户为中心的方法，开展面向潜在VLM用户的工作坊。

Result: 得到了工作坊的初步结果。

Conclusion: 工作坊的见解可为未来关于用户 - VLM交互的信任度量和参与策略研究提供参考。

Abstract: With the growing deployment of Vision-Language Models (VLMs), pre-trained on large image-text and video-text datasets, it is critical to equip users with the tools to discern when to trust these systems. However, examining how user trust in VLMs builds and evolves remains an open problem. This problem is exacerbated by the increasing reliance on AI models as judges for experimental validation, to bypass the cost and implications of running participatory design studies directly with users. Following a user-centred approach, this paper presents preliminary results from a workshop with prospective VLM users. Insights from this pilot workshop inform future studies aimed at contextualising trust metrics and strategies for participants' engagement to fit the case of user-VLM interaction.

</details>


### [625] [The Quick Red Fox gets the best Data Driven Classroom Interviews: A manual for an interview app and its associated methodology](https://arxiv.org/abs/2511.13466)
*Jaclyn Ocumpaugh,Luc Paquette,Ryan S. Baker,Amanda Barany,Jeff Ginger,Nathan Casano,Andres F. Zambrano,Xiner Liu,Zhanlan Wei,Yiqui Zhou,Qianhui Liu,Stephen Hutt,Alexandra M. A. Andres,Nidhi Nasiar,Camille Giordano,Martin van Velsen,Micheal Mogessi*

Main category: cs.HC

TL;DR: 介绍数据驱动课堂访谈（DDCIs）技术，其由学习分析领域技术发展推动，借助Quick Red Fox（QRF）工具，同时提供技术手册及相关培训和分析方法。


<details>
  <summary>Details</summary>
Motivation: 在学习分析领域技术发展背景下，找到一种能在不过多干扰学生学习体验的情况下，让研究者聚焦重点事件的访谈技术。

Method: 采用名为Quick Red Fox（QRF）的开源安卓应用工具，该工具与现有学生建模技术集成，引导访谈者关注有特定行为的用户。

Result: 开发出数据驱动课堂访谈（DDCIs）技术及相关工具QRF，并提供技术手册，涵盖开发触发器和访谈技术的流程培训及分析方法。

Conclusion: 数据驱动课堂访谈（DDCIs）技术借助QRF工具，可有效优化研究者时间，聚焦重点事件进行访谈。

Abstract: Data Driven Classroom Interviews (DDCIs) are an interviewing technique that is facilitated by recent technological developments in the learning analytics community. DDCIs are short, targeted interviews that allow researchers to contextualize students' interactions with a digital learning environment (e.g., intelligent tutoring systems or educational games) while minimizing the amount of time that the researcher interrupts that learning experience, and focusing researcher time on the events they most want to focus on DDCIs are facilitated by a research tool called the Quick Red Fox (QRF)--an open-source server-client Android app that optimizes researcher time by directing interviewers to users that have just displayed an interesting behavior (previously defined by the research team). QRF integrates with existing student modeling technologies (e.g., behavior-sensing, affect-sensing, detection of self-regulated learning) to alert researchers to key moments in a learner's experience. This manual documents the tech while providing training on the processes involved in developing triggers and interview techniques; it also suggests methods of analyses.

</details>


### [626] [A Lexical Analysis of online Reviews on Human-AI Interactions](https://arxiv.org/abs/2511.13480)
*Parisa Arbab,Xiaowen Fang*

Main category: cs.HC

TL;DR: 研究通过分析用户评论理解人机交互动态，用词汇法分析在线评论，因子分析得关键因素，后续将深入研究以助力开发以用户为中心的AI系统。


<details>
  <summary>Details</summary>
Motivation: 以往研究在理解用户面临的具体问题和挑战方面存在空白，此研究旨在填补这一空白。

Method: 采用词汇法分析来自G2.com、Producthunt.com和Trustpilot.com的55,968条在线评论，进行因子分析。

Result: 因子分析初步揭示了影响人机交互的关键因素。

Conclusion: 研究结果有望加深对人机交互的理解，为未来AI技术和用户体验改进提供参考。

Abstract: This study focuses on understanding the complex dynamics between humans and AI systems by analyzing user reviews. While previous research has explored various aspects of human-AI interaction, such as user perceptions and ethical considerations, there remains a gap in understanding the specific concerns and challenges users face. By using a lexical approach to analyze 55,968 online reviews from G2.com, Producthunt.com, and Trustpilot.com, this preliminary research aims to analyze human-AI interaction. Initial results from factor analysis reveal key factors influencing these interactions. The study aims to provide deeper insights into these factors through content analysis, contributing to the development of more user-centric AI systems. The findings are expected to enhance our understanding of human-AI interaction and inform future AI technology and user experience improvements.

</details>


### [627] [Person-AI Bidirectional Fit - A Proof-Of-Concept Case Study Of Augmented Human-Ai Symbiosis In Management Decision-Making Process](https://arxiv.org/abs/2511.13670)
*Agnieszka Bieńkowska,Jacek Małecki,Alexander Mathiesen-Ohman,Katarzyna Tworek*

Main category: cs.HC

TL;DR: 本文提出人 - AI双向契合概念，通过招聘案例研究其在管理决策中的作用，对比三种决策路径，发现高契合度能促进准确、可信且契合情境的决策，验证了概念并为系统提供概念验证。


<details>
  <summary>Details</summary>
Motivation: 研究人 - AI双向契合在管理决策中的作用。

Method: 基于权变理论和质量理论，通过涉及真实招聘过程的概念验证案例研究，对比三种决策路径。

Result: 人类判断存在基于角色的显著差异，H3LIX - LAIZA与CEO的隐性决策模型高度一致，LLMr有错误推荐。

Conclusion: 更高的人 - AI契合度是将增强共生智能与准确、可信且契合情境的决策相联系的机制，验证了P - AI契合概念和H3LIX - LAIZA系统。

Abstract: This article develops the concept of Person-AI bidirectional fit, defined as the continuously evolving, context-sensitive alignment-primarily cognitive, but also emotional and behavioral-between a human decision-maker and an artificial intelligence system. Grounded in contingency theory and quality theory, the study examines the role of P-AI fit in managerial decision-making through a proof-of-concept case study involving a real hiring process for a Senior AI Lead. Three decision pathways are compared: (1) independent evaluations by a CEO, CTO, and CSO; (2) an evaluation produced by an augmented human-AI symbiotic intelligence system (H3LIX-LAIZA); and (3) an assessment generated by a general-purpose large language model. The results reveal substantial role-based divergence in human judgments, high alignment between H3LIX-LAIZA and the CEOs implicit decision model-including ethical disqualification of a high-risk candidate and a critical false-positive recommendation from the LLMr. The findings demonstrate that higher P-AI fit, exemplified by the CEO H3LIX-LAIZA relationship, functions as a mechanism linking augmented symbiotic intelligence to accurate, trustworthy, and context-sensitive decisions. The study provides an initial verification of the P-AI fit construct and a proof-of-concept for H3LIX-LAIZA as an augmented human-AI symbiotic intelligence system.

</details>


<div id='q-bio.MN'></div>

# q-bio.MN [[Back]](#toc)

### [628] [Practical Causal Evaluation Metrics for Biological Networks](https://arxiv.org/abs/2511.12805)
*Noriaki Sato,Marco Scutari,Shuichi Kawano,Rui Yamaguchi,Seiya Imoto*

Main category: q-bio.MN

TL;DR: 本文提出评估因果网络的新指标sSID和加权sSID，经模拟和真实数据分析表明其比传统指标更优。


<details>
  <summary>Details</summary>
Motivation: 现有评估指标鲜少考虑生物数据库定性关系，而评估网络干预效应很重要。

Method: 开发了sSID和加权sSID指标，并通过模拟和分析真实转录组数据集进行验证。

Result: 与传统指标相比，新指标能选出不同的最优算法，且sSID选出的网络在临床协变量分类任务中表现更优。

Conclusion: sSID可区分结构正确但功能错误的网络，是更具生物学意义和实用性的评估指标。

Abstract: Estimating causal networks from biological data is a critical step in systems biology. When evaluating the inferred network, assessing the networks based on their intervention effects is particularly important for downstream probabilistic reasoning and the identification of potential drug targets. In the context of gene regulatory network inference, biological databases are often used as reference sources. These databases typically describe relationships in a qualitative rather than quantitative manner. However, few evaluation metrics have been developed that take this qualitative nature into account. To address this, we developed a metric, the sign-augmented Structural Intervention Distance (sSID), and a weighted sSID that incorporates the net effects of the intervention. Through simulations and analyses of real transcriptomic datasets, we found that our proposed metrics could identify a different algorithm as optimal compared to conventional metrics, and the network selected by sSID had a superior performance in the classification task of clinical covariates using transcriptomic data. This suggests that sSID can distinguish networks that are structurally correct but functionally incorrect, highlighting its potential as a more biologically meaningful and practical evaluation metric.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [629] [Modeling X-ray photon pile-up with a normalizing flow](https://arxiv.org/abs/2511.11863)
*Ole König,Daniela Huppenkothen,Douglas Finkbeiner,Christian Kirsch,Jörn Wilms,Justina R. Yang,James F. Steiner,Juan Rafael Martínez-Galarza*

Main category: astro-ph.HE

TL;DR: 本文提出用机器学习方法处理X射线探测器数据堆积问题，对比传统方法有优势，并考虑不确定性与实际数据适用性。


<details>
  <summary>Details</summary>
Motivation: X射线探测器动态范围有限，堆积现象导致数据难以处理，大量存档数据未充分探索。

Method: 使用基于模拟的推理框架，用归一化流处理堆积的eROSITA数据。

Result: 归一化流比传统缓解技术能产生约束更好的后验密度，可利用更多数据。

Conclusion: 机器学习方法能有效处理X射线探测器堆积数据，还考虑了不确定性和实际数据适用性。

Abstract: The dynamic range of imaging detectors flown on-board X-ray observatories often only covers a limited flux range of extrasolar X-ray sources. The analysis of bright X-ray sources is complicated by so-called pile-up, which results from high incident photon flux. This nonlinear effect distorts the measured spectrum, resulting in biases in the inferred physical parameters, and can even lead to a complete signal loss in extreme cases. Piled-up data are commonly discarded due to resulting intractability of the likelihood. As a result, a large number of archival observations remain underexplored. We present a machine learning solution to this problem, using a simulation-based inference framework that allows us to estimate posterior distributions of physical source parameters from piled-up eROSITA data. We show that a normalizing flow produces better-constrained posterior densities than traditional mitigation techniques, as more data can be leveraged. We consider model- and calibration-dependent uncertainties and the applicability of such an algorithm to real data in the eROSITA archive.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [630] [Asymptotic confidence bands for centered purely random forests](https://arxiv.org/abs/2511.13199)
*Natalie Neumeyer,Jan Rabe,Mathias Trabs*

Main category: math.ST

TL;DR: 构建多元非参数回归中中心纯随机森林的渐近一致置信带，提出Ehrenfest中心纯随机森林达最优率，理论结果经模拟验证。


<details>
  <summary>Details</summary>
Motivation: 流行的均匀中心纯随机森林存在次优率问题，需新方法改进。

Method: 将随机森林解释为广义U统计量，结合经验过程上确界的高斯近似进行证明。

Result: 提出Ehrenfest中心纯随机森林并证明主置信带定理适用于两种随机森林。

Conclusion: 所提方法有效，理论结果可通过模拟示例说明。

Abstract: In a multivariate nonparametric regression setting we construct explicit asymptotic uniform confidence bands for centered purely random forests. Since the most popular example in this class of random forests, namely the uniformly centered purely random forests, is well known to suffer from suboptimal rates, we propose a new type of purely random forests, called the Ehrenfest centered purely random forests, which achieve minimax optimal rates. Our main confidence band theorem applies to both random forests. The proof is based on an interpretation of random forests as generalized U-Statistics together with a Gaussian approximation of the supremum of empirical processes. Our theoretical findings are illustrated in simulation examples.

</details>


### [631] [Nonparametric Estimation of Joint Entropy through Partitioned Sample-Spacing Method](https://arxiv.org/abs/2511.13602)
*Jungwoo Ho,Sangun Park,Soyeong Oh*

Main category: math.ST

TL;DR: 提出基于分区样本间距（PSS）的多元联合熵非参数估计器，在基准测试中表现出色，是归一化流方法的实用替代方案。


<details>
  <summary>Details</summary>
Motivation: 开发一种有效的多元联合熵非参数估计方法。

Method: 将单变量间距思想扩展到多变量，通过划分样本空间为局部单元并聚合单元内统计量。

Result: 在基准测试中，PSS始终优于k近邻估计器，与最近的归一化流方法精度相当，无需训练或辅助密度建模，在中等高维下可扩展，对相关或偏态分布有鲁棒性。

Conclusion: PSS是归一化流方法的实用替代方案，在信息论机器学习应用中有广泛潜力。

Abstract: We propose a nonparametric estimator of multivariate joint entropy based on partitioned sample spacings (PSS). The method extends univariate spacing ideas to multivariate settings by partitioning the sample space into localized cells and aggregating within-cell statistics, with strong consistency guarantees under mild conditions. In benchmarks across diverse distributions, PSS consistently outperforms k-nearest neighbor estimators and achieves accuracy competitive with recent normalizing flow-based methods, while requiring no training or auxiliary density modeling. The estimator scales favorably in moderately high dimensions (d = 10 to 40) and shows particular robustness to correlated or skewed distributions. These properties position PSS as a practical alternative to normalizing flow-based approaches, with broad potential in information-theoretic machine learning applications.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [632] [Lightweight Hopfield Neural Networks for Bioacoustic Detection and Call Monitoring of Captive Primates](https://arxiv.org/abs/2511.11615)
*Wendy Lomas,Andrew Gascoyne,Colin Dubreuil,Stefano Vaglio,Liam Naughton*

Main category: cs.SD

TL;DR: 提出用Hopfield神经网络（HNN）架构的关联记忆AI模型处理野生动物被动声学监测数据，改进后准确率达0.94，处理速度快，适用性广。


<details>
  <summary>Details</summary>
Motivation: 被动声学监测产生大量数据且处理积压，现有卷积神经网络资源密集、需大量预标注数据且应用缺乏灵活性。

Method: 采用HNN架构的关联记忆AI模型，存储感兴趣的狐猴社交叫声以检测更大声学数据集中的其他叫声实例，并存储由运动引起的额外信号改进模型。

Result: 模型准确率达0.94，每秒可进行340次分类，每分钟可处理超5.5小时音频数据，训练时间为毫秒级。

Conclusion: 轻量级解决方案可减少数据洞察周转时间，加速圈养和野外环境中的决策制定。

Abstract: Passive acoustic monitoring is a sustainable method of monitoring wildlife and environments that leads to the generation of large datasets and, currently, a processing backlog. Academic research into automating this process is focused on the application of resource intensive convolutional neural networks which require large pre-labelled datasets for training and lack flexibility in application. We present a viable alternative relevant in both wild and captive settings; a transparent, lightweight and fast-to-train associative memory AI model with Hopfield neural network (HNN) architecture. Adapted from a model developed to detect bat echolocation calls, this model monitors captive endangered black-and-white ruffed lemur Varecia variegata vocalisations. Lemur social calls of interest when monitoring welfare are stored in the HNN in order to detect other call instances across the larger acoustic dataset. We make significant model improvements by storing an additional signal caused by movement and achieve an overall accuracy of 0.94. The model can perform $340$ classifications per second, processing over 5.5 hours of audio data per minute, on a standard laptop running other applications. It has broad applicability and trains in milliseconds. Our lightweight solution reduces data-to-insight turnaround times and can accelerate decision making in both captive and wild settings.

</details>


### [633] [Real-Time Speech Enhancement via a Hybrid ViT: A Dual-Input Acoustic-Image Feature Fusion](https://arxiv.org/abs/2511.11825)
*Behnaz Bahmei,Siamak Arzanpour,Elina Birmingham*

Main category: cs.SD

TL;DR: 本文提出基于Transformer的学习框架解决单通道实时噪声抑制问题，在真实环境效果好。


<details>
  <summary>Details</summary>
Motivation: 嘈杂环境中语音质量和可懂度下降，现有深度学习网络在非平稳噪声环境性能不佳。

Method: 提出使用混合ViT框架的双输入声学 - 图像特征融合方法，可有效建模噪声信号的时间和频谱依赖关系，且计算轻量适合嵌入式设备。

Result: 使用Librispeech、UrbanSound8K和Google Audioset数据集实验，该方法在降噪、语音可懂度和感知质量上显著提升，接近干净参考信号。

Conclusion: 所提方法能有效解决单通道实时噪声抑制问题，适用于真实音频环境。

Abstract: Speech quality and intelligibility are significantly degraded in noisy environments. This paper presents a novel transformer-based learning framework to address the single-channel noise suppression problem for real-time applications. Although existing deep learning networks have shown remarkable improvements in handling stationary noise, their performance often diminishes in real-world environments characterized by non-stationary noise (e.g., dog barking, baby crying). The proposed dual-input acoustic-image feature fusion using a hybrid ViT framework effectively models both temporal and spectral dependencies in noisy signals. Designed for real-world audio environments, the proposed framework is computationally lightweight and suitable for implementation on embedded devices. To evaluate its effectiveness, four standard and commonly used quality measurements, namely PESQ, STOI, Seg SNR, and LLR, are utilized. Experimental results obtained using the Librispeech dataset as the clean speech source and the UrbanSound8K and Google Audioset datasets as the noise sources, demonstrate that the proposed method significantly improves noise reduction, speech intelligibility, and perceptual quality compared to the noisy input signal, achieving performance close to the clean reference.

</details>


### [634] [MF-Speech: Achieving Fine-Grained and Compositional Control in Speech Generation via Factor Disentanglement](https://arxiv.org/abs/2511.12074)
*Xinyue Yu,Youqing Fang,Pingyu Wu,Guoyang Ye,Wenbo Zhou,Weiming Zhang,Song Xiao*

Main category: cs.SD

TL;DR: 提出MF - Speech框架解决语音生成难题，实验显示其性能优于现有方法，且学习到的离散因子有强迁移性。


<details>
  <summary>Details</summary>
Motivation: 解决语音生成中语音因素深度纠缠和现有控制机制粒度粗的问题。

Method: 提出MF - Speech框架，包含MF - SpeechEncoder和MF - SpeechGenerator，前者分解语音信号，后者通过动态融合和HSAN实现细粒度控制。

Result: 在多因素合成语音生成任务中，显著优于当前最优方法，WER=4.67%，SECS=0.5685，Corr=0.68，主观评价得分高。

Conclusion: MF - Speech有效，学习到的离散因子有作为通用语音表示的潜力。

Abstract: Generating expressive and controllable human speech is one of the core goals of generative artificial intelligence, but its progress has long been constrained by two fundamental challenges: the deep entanglement of speech factors and the coarse granularity of existing control mechanisms. To overcome these challenges, we have proposed a novel framework called MF-Speech, which consists of two core components: MF-SpeechEncoder and MF-SpeechGenerator. MF-SpeechEncoder acts as a factor purifier, adopting a multi-objective optimization strategy to decompose the original speech signal into highly pure and independent representations of content, timbre, and emotion. Subsequently, MF-SpeechGenerator functions as a conductor, achieving precise, composable and fine-grained control over these factors through dynamic fusion and Hierarchical Style Adaptive Normalization (HSAN). Experiments demonstrate that in the highly challenging multi-factor compositional speech generation task, MF-Speech significantly outperforms current state-of-the-art methods, achieving a lower word error rate (WER=4.67%), superior style control (SECS=0.5685, Corr=0.68), and the highest subjective evaluation scores(nMOS=3.96, sMOS_emotion=3.86, sMOS_style=3.78). Furthermore, the learned discrete factors exhibit strong transferability, demonstrating their significant potential as a general-purpose speech representation.

</details>


### [635] [FoleyBench: A Benchmark For Video-to-Audio Models](https://arxiv.org/abs/2511.13219)
*Satvik Dixit,Koichi Saito,Zhi Zhong,Yuki Mitsufuji,Chris Donahue*

Main category: cs.SD

TL;DR: 现有视频转音频评估数据集不适合Foley场景，为此提出FoleyBench基准数据集并进行模型评测。


<details>
  <summary>Details</summary>
Motivation: 现有评估数据集与Foley下游应用不匹配，缺乏针对Foley场景的基准。

Method: 构建自动化、可扩展的管道，从YouTube和Vimeo等野生互联网视频构建含5000个三元组的FoleyBench数据集，并标注元数据；对多个V2A模型进行多方面评测。

Result: FoleyBench数据集相比过去数据集，对Foley声音分类覆盖性更强；完成了对多个模型的评测。

Conclusion: FoleyBench能更好满足Foley式视频转音频评估需求，可用于细粒度分析模型性能和失败模式。

Abstract: Video-to-audio generation (V2A) is of increasing importance in domains such as film post-production, AR/VR, and sound design, particularly for the creation of Foley sound effects synchronized with on-screen actions. Foley requires generating audio that is both semantically aligned with visible events and temporally aligned with their timing. Yet, there is a mismatch between evaluation and downstream applications due to the absence of a benchmark tailored to Foley-style scenarios. We find that 74% of videos from past evaluation datasets have poor audio-visual correspondence. Moreover, they are dominated by speech and music, domains that lie outside the use case for Foley. To address this gap, we introduce FoleyBench, the first large-scale benchmark explicitly designed for Foley-style V2A evaluation. FoleyBench contains 5,000 (video, ground-truth audio, text caption) triplets, each featuring visible sound sources with audio causally tied to on-screen events. The dataset is built using an automated, scalable pipeline applied to in-the-wild internet videos from YouTube-based and Vimeo-based sources. Compared to past datasets, we show that videos from FoleyBench have stronger coverage of sound categories from a taxonomy specifically designed for Foley sound. Each clip is further labeled with metadata capturing source complexity, UCS/AudioSet category, and video length, enabling fine-grained analysis of model performance and failure modes. We benchmark several state-of-the-art V2A models, evaluating them on audio quality, audio-video alignment, temporal synchronization, and audio-text consistency. Samples are available at: https://gclef-cmu.org/foleybench

</details>


### [636] [Spatial Blind Spot: Auditory Motion Perception Deficits in Audio LLMs](https://arxiv.org/abs/2511.13273)
*Zhe Sun,Yujun Cai,Jiayu Yao,Yiwei Wang*

Main category: cs.SD

TL;DR: 研究发现当前大音频语言模型存在运动感知缺陷，引入AMPBench基准测试，结果显示模型平均准确率低于50%，凸显人与模型听觉空间推理差距。


<details>
  <summary>Details</summary>
Motivation: 探究大音频语言模型能否感知空间动态，尤其是声源运动。

Method: 引入AMPBench基准测试，通过受控问答评估模型从双耳音频中推断移动声源方向和轨迹的能力。

Result: 综合分析表明当前模型难以可靠识别运动线索或区分方向模式，平均准确率低于50%。

Conclusion: 研究凸显了人与模型听觉空间推理的根本差距，为未来提升音频语言模型的空间认知提供诊断工具和新见解。

Abstract: Large Audio-Language Models (LALMs) have recently shown impressive progress in speech recognition, audio captioning, and auditory question answering. Yet, whether these models can perceive spatial dynamics, particularly the motion of sound sources, remains unclear. In this work, we uncover a systematic motion perception deficit in current ALLMs. To investigate this issue, we introduce AMPBench, the first benchmark explicitly designed to evaluate auditory motion understanding. AMPBench introduces a controlled question-answering benchmark designed to evaluate whether Audio-Language Models (LALMs) can infer the direction and trajectory of moving sound sources from binaural audio. Comprehensive quantitative and qualitative analyses reveal that current models struggle to reliably recognize motion cues or distinguish directional patterns. The average accuracy remains below 50%, underscoring a fundamental limitation in auditory spatial reasoning. Our study highlights a fundamental gap between human and model auditory spatial reasoning, providing both a diagnostic tool and new insight for enhancing spatial cognition in future Audio-Language Models.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [637] [Modular GPU Programming with Typed Perspectives](https://arxiv.org/abs/2511.11939)
*Manya Bansal,Daniel Sainati,Joseph W. Cutler,Saman Amarasinghe,Jonathan Ragan-Kelley*

Main category: cs.PL

TL;DR: 本文介绍新GPU语言Prism，用类型视角恢复模块化，编写代码安全且不牺牲性能。


<details>
  <summary>Details</summary>
Motivation: 现代GPU编程中两种思维模式的冲突使模块化编程易出错，需一种语言恢复模块化并实现高性能。

Method: 引入类型视角，设计Prism语言，实现编译器，用Bundl核心演算奠定理论基础。

Result: 用Prism实现了先进的GPU内核，能为程序员提供安全保证。

Conclusion: Prism可让程序员自信编写模块化代码且不牺牲性能。

Abstract: To achieve peak performance on modern GPUs, one must balance two frames of mind: issuing instructions to individual threads to control their behavior, while simultaneously tracking the convergence of many threads acting in concert to perform collective operations like Tensor Core instructions. The tension between these two mindsets makes modular programming error prone. Functions that encapsulate collective operations, despite being called per-thread, must be executed cooperatively by groups of threads.
  In this work, we introduce Prism, a new GPU language that restores modularity while still giving programmers the low-level control over collective operations necessary for high performance. Our core idea is typed perspectives, which materialize, at the type level, the granularity at which the programmer is controlling the behavior of threads. We describe the design of Prism, implement a compiler for it, and lay its theoretical foundations in a core calculus called Bundl. We implement state-of-the-art GPU kernels in Prism and find that it offers programmers the safety guarantees needed to confidently write modular code without sacrificing performance.

</details>


### [638] [Cost-Driven Synthesis of Sound Abstract Interpreters](https://arxiv.org/abs/2511.13663)
*Qiuhan Gu,Avaljot Singh,Gagandeep Singh*

Main category: cs.PL

TL;DR: 探讨用大语言模型合成抽象解释器，提出统一框架，实验表明该框架效果好


<details>
  <summary>Details</summary>
Motivation: 构建提供全局可靠性保证的抽象解释器是抽象解释的主要障碍，研究现代大语言模型能否减轻此负担

Method: 将合成问题转化为约束优化问题，引入衡量不可靠性的代价函数，开发统一框架，结合大语言模型生成、语法语义验证和代价引导反馈机制

Result: 框架达到手工制作变换器的质量，发现了现有文献中缺失的复杂非线性算子的可靠、高精度变换器

Conclusion: 现代大语言模型能助力合成可靠、高精度的抽象解释器

Abstract: Constructing abstract interpreters that provide global soundness guarantees remains a major obstacle in abstract interpretation. We investigate whether modern LLMs can reduce this burden by leveraging them to synthesize sound, non-trivial abstract interpreters across multiple abstract domains in the setting of neural network verification. We formulate synthesis as a constrained optimization problem and introduce a novel mathematically grounded cost function for measuring unsoundness under strict syntactic and semantic constraints. Based on this formulation, we develop a unified framework that unifies LLM-based generation with syntactic and semantic validation and a quantitative cost-guided feedback mechanism. Empirical results demonstrate that our framework not only matches the quality of handcrafted transformers, but more importantly, discovers sound, high-precision transformers for complex nonlinear operators that are absent from existing literature.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [639] [Finite-Horizon Quickest Change Detection Balancing Latency with False Alarm Probability](https://arxiv.org/abs/2511.12803)
*Yu-Han Huang,Venugopal V. Veeravalli*

Main category: cs.IT

TL;DR: 研究有限时域下的最快变化检测问题，推导延迟下限，开发最优检测器并推广到非参数情况，用仿真验证结果。


<details>
  <summary>Details</summary>
Motivation: 研究与非平稳环境学习相关的有限时域最快变化检测问题，目标是在低误报概率下最小化延迟。

Method: 先推导任意变化检测程序需满足的延迟下限，再开发基于时域的最优变化检测器，先考虑分布已知情况，再推广到非参数情况。

Result: 得到了延迟的通用下限，开发了最优变化检测器，结果可从分布已知推广到非参数情况。

Conclusion: 通过理论推导和仿真验证，为有限时域最快变化检测问题提供了有效的解决方法。

Abstract: A finite-horizon variant of the quickest change detection (QCD) problem that is of relevance to learning in non-stationary environments is studied. The metric characterizing false alarms is the probability of a false alarm occurring before the horizon ends. The metric that characterizes the delay is \emph{latency}, which is the smallest value such that the probability that detection delay exceeds this value is upper bounded to a predetermined latency level. The objective is to minimize the latency (at a given latency level), while maintaining a low false alarm probability. Under the pre-specified latency and false alarm levels, a universal lower bound on the latency, which any change detection procedure needs to satisfy, is derived. Change detectors are then developed, which are order-optimal in terms of the horizon. The case where the pre- and post-change distributions are known is considered first, and then the results are generalized to the non-parametric case when they are unknown except that they are sub-Gaussian with different means. Simulations are provided to validate the theoretical results.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [640] [Market-Dependent Communication in Multi-Agent Alpha Generation](https://arxiv.org/abs/2511.13614)
*Jerick Shi,Burton Hollifield*

Main category: cs.MA

TL;DR: 研究多策略对冲基金分析师交流方式，发现交流提升绩效，最优交流设计取决于市场特征，复杂讨论未必带来更好绩效。


<details>
  <summary>Details</summary>
Motivation: 探讨多策略对冲基金分析师是否应交流及如何交流的组织选择问题。

Method: 使用基于大语言模型的5智能体交易系统，进行450次实验，对比五种组织结构。

Result: 交流提升绩效，不同市场特征下最优交流设计不同，所有结构策略趋同，绩效差异源于行为机制，对话质量与回报零相关。

Conclusion: 最优交流设计需匹配市场波动特征，复杂讨论不保证更好绩效。

Abstract: Multi-strategy hedge funds face a fundamental organizational choice: should analysts generating trading strategies communicate, and if so, how? We investigate this using 5-agent LLM-based trading systems across 450 experiments spanning 21 months, comparing five organizational structures from isolated baseline to collaborative and competitive conversation. We show that communication improves performance, but optimal communication design depends on market characteristics. Competitive conversation excels in volatile technology stocks, while collaborative conversation dominates stable general stocks. Finance stocks resist all communication interventions. Surprisingly, all structures, including isolated agents, converge to similar strategy alignments, challenging assumptions that transparency causes harmful diversity loss. Performance differences stem from behavioral mechanisms: competitive agents focus on stock-level allocation while collaborative agents develop technical frameworks. Conversation quality scores show zero correlation with returns. These findings demonstrate that optimal communication design must match market volatility characteristics, and sophisticated discussions don't guarantee better performance.

</details>


### [641] [Asymptotic analysis of cooperative censoring policies in sensor networks](https://arxiv.org/abs/2511.13492)
*Jesus Fernandez-Bes,Rocío Arroyo-Valles,Jesús Cid-Sueiro*

Main category: cs.MA

TL;DR: 本文分析电池供电多跳传感器网络中的协作数据审查问题，找到理论最优审查策略，提出计算阈值的集中式算法，实验表明协作审查策略节能且优于非协作方案。


<details>
  <summary>Details</summary>
Motivation: 在电池供电多跳传感器网络中，为节省后续通信能量，对不太重要的消息进行审查。

Method: 使用联合马尔可夫决策过程对整个网络动态建模，找到理论最优审查策略，在一定条件下用有限常数阈值规则近似，提出计算阈值的集中式算法。

Result: 实验模拟显示协作审查策略节能，且优于其他非协作方案。

Conclusion: 协作审查策略在电池供电多跳传感器网络中是有效的，可以通过常数阈值规则近似实现。

Abstract: The problem of cooperative data censoring in battery-powered multihop sensor networks is analyzed in this paper. We are interested in scenarios where nodes generate messages (which are related to the sensor measurements) that can be graded with some importance value. Less important messages can be censored in order to save energy for later communications. The problem is modeled using a joint Markov Decision Process of the whole network dynamics, and a theoretically optimal censoring policy, which maximizes a long-term reward, is found. Though the optimal censoring rules are computationally prohibitive, our analysis suggests that, under some conditions, they can be approximated by a finite collection of constant-threshold rules. A centralized algorithm for the computation of these thresholds is proposed. The experimental simulations show that cooperative censoring policies are energy-efficient, and outperform other non-cooperative schemes.

</details>


### [642] [MALBO: Optimizing LLM-Based Multi-Agent Teams via Multi-Objective Bayesian Optimization](https://arxiv.org/abs/2511.11788)
*Antonio Sabbatella*

Main category: cs.MA

TL;DR: 本文提出MALBO框架解决大语言模型在多智能体系统中的最优分配问题，通过多目标贝叶斯优化得到最优团队配置的帕累托前沿，能有效降低成本。


<details>
  <summary>Details</summary>
Motivation: 现有优化方法聚焦单智能体设置，缺乏处理多智能体、多目标问题的原则性框架，大语言模型在多智能体系统中的最优分配存在挑战。

Method: 将分配挑战形式化为多目标优化问题，采用多目标贝叶斯优化和独立高斯过程替代模型，在连续特征空间搜索。

Result: 贝叶斯优化阶段相比初始随机搜索，平均配置成本降低超45%；MALBO识别的专业异构团队比同构基线成本降低达65.8%，且保持最高性能。

Conclusion: 该框架为部署经济高效、高度专业化的多智能体AI系统提供了数据驱动工具。

Abstract: The optimal assignment of Large Language Models (LLMs) to specialized roles in multi-agent systems is a significant challenge, defined by a vast combinatorial search space, expensive black-box evaluations, and an inherent trade-off between performance and cost. Current optimization methods focus on single-agent settings and lack a principled framework for this multi-agent, multi-objective problem.
  This thesis introduces MALBO (Multi-Agent LLM Bayesian Optimization), a systematic framework designed to automate the efficient composition of LLM-based agent teams. We formalize the assignment challenge as a multi-objective optimization problem, aiming to identify the Pareto front of configurations between task accuracy and inference cost. The methodology employs multi-objective Bayesian Optimization (MOBO) with independent Gaussian Process surrogate models. By searching over a continuous feature-space representation of the LLMs, this approach performs a sample-efficient exploration guided by the expected hypervolume improvement.
  The primary contribution is a principled and automated methodology that yields a Pareto front of optimal team configurations. Our results demonstrate that the Bayesian optimization phase, compared to an initial random search, maintained a comparable average performance while reducing the average configuration cost by over 45%. Furthermore, MALBO identified specialized, heterogeneous teams that achieve cost reductions of up to 65.8% compared to homogeneous baselines, all while maintaining maximum performance. The framework thus provides a data-driven tool for deploying cost-effective and highly specialized multi-agent AI systems.

</details>


### [643] [From Single to Societal: Analyzing Persona-Induced Bias in Multi-Agent Interactions](https://arxiv.org/abs/2511.11789)
*Jiayi Li,Xiao Liu,Yansong Feng*

Main category: cs.MA

TL;DR: 研究大语言模型多智能体系统中角色设定是否引入偏差，发现存在信任度和坚持度偏差及内群体偏爱现象。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型多智能体系统中角色设定是否会在交互中引入偏差。

Method: 在协作问题解决和说服任务中进行一系列对照实验。

Result: 大语言模型智能体在信任度和坚持度上存在偏差，历史优势群体角色被认为信任度低、坚持度弱，且智能体存在内群体偏爱。这些偏差在不同大语言模型、群体规模和交互轮次中都存在。

Conclusion: 迫切需要提高认识并采取措施减轻偏差，以确保多智能体系统的公平性和可靠性。

Abstract: Large Language Model (LLM)-based multi-agent systems are increasingly used to simulate human interactions and solve collaborative tasks. A common practice is to assign agents with personas to encourage behavioral diversity. However, this raises a critical yet underexplored question: do personas introduce biases into multi-agent interactions? This paper presents a systematic investigation into persona-induced biases in multi-agent interactions, with a focus on social traits like trustworthiness (how an agent's opinion is received by others) and insistence (how strongly an agent advocates for its opinion). Through a series of controlled experiments in collaborative problem-solving and persuasion tasks, we reveal that (1) LLM-based agents exhibit biases in both trustworthiness and insistence, with personas from historically advantaged groups (e.g., men and White individuals) perceived as less trustworthy and demonstrating less insistence; and (2) agents exhibit significant in-group favoritism, showing a higher tendency to conform to others who share the same persona. These biases persist across various LLMs, group sizes, and numbers of interaction rounds, highlighting an urgent need for awareness and mitigation to ensure the fairness and reliability of multi-agent systems.

</details>


### [644] [Goal-Oriented Multi-Agent Reinforcement Learning for Decentralized Agent Teams](https://arxiv.org/abs/2511.11992)
*Hung Du,Hy Nguyen,Srikanth Thudumu,Rajesh Vasa,Kon Mouzakis*

Main category: cs.MA

TL;DR: 提出分散式多智能体强化学习框架解决多车辆协作难题，在多智能体导航任务验证，效果佳且具可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现实环境给跨陆、水、空的互联自动驾驶车辆协作带来挑战，尤其车辆有各自目标时，需解决协调问题。

Method: 提出分散式多智能体强化学习（MARL）框架，让车辆作为智能体基于本地目标和观察选择性通信。

Result: 在复杂多智能体导航任务中，该方法显著提高任务成功率、缩短完成时间，且性能随智能体数量增加保持稳定。

Conclusion: 分散式、目标驱动的 MARL 有潜力支持现实多车辆系统的有效协调。

Abstract: Connected and autonomous vehicles across land, water, and air must often operate in dynamic, unpredictable environments with limited communication, no centralized control, and partial observability. These real-world constraints pose significant challenges for coordination, particularly when vehicles pursue individual objectives. To address this, we propose a decentralized Multi-Agent Reinforcement Learning (MARL) framework that enables vehicles, acting as agents, to communicate selectively based on local goals and observations. This goal-aware communication strategy allows agents to share only relevant information, enhancing collaboration while respecting visibility limitations. We validate our approach in complex multi-agent navigation tasks featuring obstacles and dynamic agent populations. Results show that our method significantly improves task success rates and reduces time-to-goal compared to non-cooperative baselines. Moreover, task performance remains stable as the number of agents increases, demonstrating scalability. These findings highlight the potential of decentralized, goal-driven MARL to support effective coordination in realistic multi-vehicle systems operating across diverse domains.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [645] [Preserving Extreme Singular Values with One Oblivious Sketch](https://arxiv.org/abs/2511.12802)
*John M. Mango,Ronald Katende*

Main category: math.NA

TL;DR: 研究单一线性草图控制秩 - r矩阵奇异值的情况，提出猜想，给出构造性和否定性结果并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 经典遗忘嵌入无法对极端奇异值或条件数进行常数因子控制，研究单一线性草图能否控制秩 - r矩阵的最大和最小非零奇异值。

Method: 结合稀疏遗忘草图和确定性几何平衡映射构造草图，同时进行理论证明和数值实验。

Result: 构造的草图在有界条件数和相干性下非零奇异值收敛到共同尺度；任何遗忘草图要达到相对ε - 精确奇异值需满足s = Ω((r + log(1/δ))/ε²)；实验表明平衡改善了条件数并加速迭代求解器，相干或接近秩亏输入会出现预测的失败模式。

Conclusion: 提出s = O(r log r)足以进行奇异值保存的猜想，构造方法有一定效果，同时给出了遗忘草图的下界。

Abstract: We study when a single linear sketch can control the largest and smallest nonzero singular values of every rank-$r$ matrix. Classical oblivious embeddings require $s=Θ(r/\varepsilon^{2})$ for $(1\pm\varepsilon)$ distortion, but this does not yield constant-factor control of extreme singular values or condition numbers. We formalize a conjecture that $s=O(r\log r)$ suffices for such preservation. On the constructive side, we show that combining a sparse oblivious sketch with a deterministic geometric balancing map produces a sketch whose nonzero singular values collapse to a common scale under bounded condition number and coherence. On the negative side, we prove that any oblivious sketch achieving relative $\varepsilon$-accurate singular values for all rank-$r$ matrices must satisfy $s=Ω((r+\log(1/δ))/\varepsilon^{2})$. Numerical experiments on structured matrix families confirm that balancing improves conditioning and accelerates iterative solvers, while coherent or nearly rank-deficient inputs manifest the predicted failure modes.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [646] [ProAV-DiT: A Projected Latent Diffusion Transformer for Efficient Synchronized Audio-Video Generation](https://arxiv.org/abs/2511.12072)
*Jiahui Sun,Weining Wang,Mingzhen Sun,Yirong Yang,Xinxin Zhu,Jing Liu*

Main category: cs.MM

TL;DR: 本文提出ProAV - DiT用于高效同步的音视频生成，实验表明其在生成质量和计算效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: Sounding Video Generation任务因音视频结构不对齐和多模态数据处理计算成本高而具有挑战性，需要高效同步的音视频生成方法。

Method: 将原始音频预处理成类视频表示以对齐音视频维度；采用MDSA将两种模态投影到统一潜在空间；引入多尺度注意力机制增强时间连贯性和模态融合；将MDSA的2D潜在空间堆叠成3D潜在空间，由时空扩散Transformer处理。

Result: 在标准基准上的大量实验表明ProAV - DiT在生成质量和计算效率上优于现有方法。

Conclusion: ProAV - DiT能高效建模时空依赖，在减少计算开销的同时生成高保真同步音视频内容。

Abstract: Sounding Video Generation (SVG) remains a challenging task due to the inherent structural misalignment between audio and video, as well as the high computational cost of multimodal data processing. In this paper, we introduce ProAV-DiT, a Projected Latent Diffusion Transformer designed for efficient and synchronized audio-video generation. To address structural inconsistencies, we preprocess raw audio into video-like representations, aligning both the temporal and spatial dimensions between audio and video. At its core, ProAV-DiT adopts a Multi-scale Dual-stream Spatio-Temporal Autoencoder (MDSA), which projects both modalities into a unified latent space using orthogonal decomposition, enabling fine-grained spatiotemporal modeling and semantic alignment. To further enhance temporal coherence and modality-specific fusion, we introduce a multi-scale attention mechanism, which consists of multi-scale temporal self-attention and group cross-modal attention. Furthermore, we stack the 2D latents from MDSA into a unified 3D latent space, which is processed by a spatio-temporal diffusion Transformer. This design efficiently models spatiotemporal dependencies, enabling the generation of high-fidelity synchronized audio-video content while reducing computational overhead. Extensive experiments conducted on standard benchmarks demonstrate that ProAV-DiT outperforms existing methods in both generation quality and computational efficiency.

</details>


### [647] [SynthGuard: An Open Platform for Detecting AI-Generated Multimedia with Multimodal LLMs](https://arxiv.org/abs/2511.12404)
*Shail Desai,Aditya Pawar,Li Lin,Xin Wang,Shu Hu*

Main category: cs.MM

TL;DR: 介绍了AI生成媒体带来便利与风险，现有检测工具存在不足，提出开放易用的SynthGuard平台用于检测分析AI生成多媒体。


<details>
  <summary>Details</summary>
Motivation: AI生成媒体带来严重风险，现有检测工具存在封闭、模态有限、缺乏透明性和教育价值等问题，需解决这些不足。

Method: 引入SynthGuard平台，结合传统检测器和多模态大语言模型进行检测分析。

Result: SynthGuard平台提供可解释推理、统一图像和音频支持及交互式界面。

Conclusion: SynthGuard平台能让研究人员、教育工作者和公众进行法医分析，平台可通过指定网址访问。

Abstract: Artificial Intelligence (AI) has made it possible for anyone to create images, audio, and video with unprecedented ease, enriching education, communication, and creative expression. At the same time, the rapid rise of AI-generated media has introduced serious risks, including misinformation, identity misuse, and the erosion of public trust as synthetic content becomes increasingly indistinguishable from real media. Although deepfake detection has advanced, many existing tools remain closed-source, limited in modality, or lacking transparency and educational value, making it difficult for users to understand how detection decisions are made. To address these gaps, we introduce SynthGuard, an open, user-friendly platform for detecting and analyzing AI-generated multimedia using both traditional detectors and multimodal large language models (MLLMs). SynthGuard provides explainable inference, unified image and audio support, and an interactive interface designed to make forensic analysis accessible to researchers, educators, and the public. The SynthGuard platform is available at: https://in-engr-nova.it.purdue.edu/

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [648] [Optimal and Efficient Partite Decompositions of Hypergraphs](https://arxiv.org/abs/2511.11855)
*Andrew Krapivin,Benjamin Przybocki,Nicolás Sanhueza-Matamala,Bernardo Subercaseaux*

Main category: math.CO

TL;DR: 本文研究d - 一致超图的边划分为完全d - 部超图的问题，给出顶点所属划分成员数量的上界，还得到图的双团划分总权重上界及相关算法，展示双团划分的信息论最优性及应用。


<details>
  <summary>Details</summary>
Motivation: 解决Erdős和Pyber（1997）、Csirmaz等（2014）、Chung等（1983）、Feder和Motwani（1995）等人提出的问题，改进已有结果。

Method: 基于Nechiporuk（1969）的结果进行证明和构造算法。

Result: 得到d - 一致超图边划分中顶点所属划分成员数量上界；图的双团划分总权重上界；给出双团划分构造算法及寻找子图$K_{t,t}$的算法；展示双团划分在独立集查询、割查询和最密子图近似问题中的应用。

Conclusion: 双团划分是固定密度图的信息论最优表示，可用于高效解决多种图查询和近似问题。

Abstract: We study the problem of partitioning the edges of a $d$-uniform hypergraph $H$ into a family $F$ of complete $d$-partite hypergraphs ($d$-cliques). We show that there is a partition $F$ in which every vertex $v \in V(H)$ belongs to at most $(\frac{1}{d!} + o_d(1))n^{d-1}/\lg n$ members of $F$. This settles the central question of a line of research initiated by Erdős and Pyber (1997) for graphs, and more recently by Csirmaz, Ligeti, and Tardos (2014) for hypergraphs. The $d=2$ case of this theorem answers a 40-year-old question of Chung, Erdős, and Spencer (1983). An immediate corollary of our result is an improved upper bound for the maximum share size for binary secret sharing schemes on uniform hypergraphs.
  Building on results of Nechiporuk (1969), we prove that every graph with fixed edge density $γ\in (0,1)$ has a biclique partition of total weight at most $(\tfrac{1}{2}+o(1))\cdot h_2(γ) \frac{n^2}{\lg n}$, where $h_2$ is the binary entropy function. Our construction implies that such biclique partitions can be constructed in time $O(m)$, which answers a question of Feder and Motwani (1995) and also improves upon results of Mubayi and Turán (2010) as well as Chavan, Rabinia, Grosu, and Brocanelli (2025). Using similar techniques, we also give an $n^{1+o(1)}$ algorithm for finding a subgraph $K_{t,t}$ with $t = (1-o(1)) \fracγ{h_2(γ)} \lg n$.
  Our results show that biclique partitions are information-theoretically optimal representations for graphs at every fixed density. We show that with this succinct representation one can answer independent set queries and cut queries in time $O(n^2/ \lg n)$, and if we increase the space usage by a constant factor, we can compute a $2α$-approximation for the densest subgraph problem in time $O(n^2/\lg α)$ for any $α> 1$.

</details>


### [649] [From Black Box to Bijection: Interpreting Machine Learning to Build a Zeta Map Algorithm](https://arxiv.org/abs/2511.12421)
*Xiaoyu Huang,Blake Jackson,Kyu-Hwan Lee*

Main category: math.CO

TL;DR: 提出通过机器学习发现组合双射的新工作流程，用Transformer训练Dyck路径对得到zeta映射的新算法描述。


<details>
  <summary>Details</summary>
Motivation: 传统通过人工观察数据找规律的方法在数据超出人类处理规模时失效，需要新方法来解决代数组合学中构建显式组合双射的问题。

Method: 提出通过机器学习发现组合双射的新工作流程，训练Transformer处理配对的Dyck路径。

Result: 利用Transformer学习到的注意力模式，得出zeta映射的新算法描述，即Scaffolding Map。

Conclusion: 提出的通过机器学习发现组合双射的工作流程可行，可用于得到新的算法描述。

Abstract: There is a large class of problems in algebraic combinatorics which can be distilled into the same challenge: construct an explicit combinatorial bijection. Traditionally, researchers have solved challenges like these by visually inspecting the data for patterns, formulating conjectures, and then proving them. But what is to be done if patterns fail to emerge until the data grows beyond human scale? In this paper, we propose a new workflow for discovering combinatorial bijections via machine learning. As a proof of concept, we train a transformer on paired Dyck paths and use its learned attention patterns to derive a new algorithmic description of the zeta map, which we call the \textit{Scaffolding Map}.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [650] [Rapid Machine Learning-Driven Detection of Pesticides and Dyes Using Raman Spectroscopy](https://arxiv.org/abs/2511.12167)
*Quach Thi Thai Binh,Thuan Phuoc,Xuan Hai,Thang Bach Phan,Vu Thi Hanh Thu,Nguyen Tuan Hung*

Main category: cond-mat.mtrl-sci

TL;DR: 提出基于ResNet - 18特征提取和先进分类器的MLRaman框架用于拉曼光谱检测农药和染料，有高准确率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 农药和合成染料广泛使用威胁食品安全、人类健康和环境可持续性，拉曼光谱检测有局限性，需可靠检测方法。

Method: 提出基于ResNet - 18特征提取的深度学习框架MLRaman，结合XGBoost、SVM及其混合集成分类器。

Result: CNN - XGBoost模型预测准确率97.4%，AUC为1.0；CNN - SVM模型有良好结果；降维分析证实拉曼嵌入可分离；开发的应用能识别未知拉曼光谱。

Conclusion: 建立了可扩展、实用的MLRaman模型，在食品安全和环境监测有应用潜力。

Abstract: The extensive use of pesticides and synthetic dyes poses critical threats to food safety, human health, and environmental sustainability, necessitating rapid and reliable detection methods. Raman spectroscopy offers molecularly specific fingerprints but suffers from spectral noise, fluorescence background, and band overlap, limiting its real-world applicability. Here, we propose a deep learning framework based on ResNet-18 feature extraction, combined with advanced classifiers, including XGBoost, SVM, and their hybrid integration, to detect pesticides and dyes from Raman spectroscopy, called MLRaman. The MLRaman with the CNN-XGBoost model achieved a predictive accuracy of 97.4% and a perfect AUC of 1.0, while it with the CNN-SVM model provided competitive results with robust class-wise discrimination. Dimensionality reduction analyses (PCA, t-SNE, UMAP) confirmed the separability of Raman embeddings across 10 analytes, including 7 pesticides and 3 dyes. Finally, we developed a user-friendly Streamlit application for real-time prediction, which successfully identified unseen Raman spectra from our independent experiments and also literature sources, underscoring strong generalization capacity. This study establishes a scalable, practical MLRaman model for multi-residue contaminant monitoring, with significant potential for deployment in food safety and environmental surveillance.

</details>


### [651] [Reinforcement Learning for Chemical Ordering in Alloy Nanoparticles](https://arxiv.org/abs/2511.12260)
*Jonas Elsborg,Arghya Bhowmik*

Main category: cond-mat.mtrl-sci

TL;DR: 本文将双金属合金纳米粒子的最优元素排序问题视为强化学习问题，构建了RL代理，展示了其在搜索最优排序上的有效性和一定泛化能力，但多合金元素时效果受限。


<details>
  <summary>Details</summary>
Motivation: 解决双金属合金纳米粒子的最优元素排序问题。

Method: 将该问题作为强化学习问题，构建基于纳米粒子几何图表示的RL代理，通过对二十面体纳米粒子结构进行保成分原子交换操作进行训练。

Result: 训练后的代理能发现已知基态结构，优化对相同成分不同初始排序具有鲁棒性，训练策略可有效外推到未见过尺寸的纳米粒子，但多合金元素时效果有限。

Conclusion: 使用预训练等变图编码的强化学习可在纳米粒子尺度上探索组合排序空间，提供了具有跨成分泛化潜力和降低重复搜索成本的优化策略。

Abstract: We approach the search for optimal element ordering in bimetallic alloy nanoparticles (NPs) as a reinforcement learning (RL) problem, and have built an RL agent that learns to perform such global optimisation using the geometric graph representation of the NPs. To demonstrate the effectiveness, we train an RL agent to perform composition-conserving atomic swap actions on the icosahedral nanoparticle structure. Trained once on randomised $Ag_{X}Au_{309-X}$ compositions and orderings, the agent discovers previously established ground state structure. We show that this optimization is robust to differently ordered initialisations of the same NP compositions. We also demonstrate that a trained policy can extrapolate effectively to NPs of unseen size. However, the efficacy is limited when multiple alloying elements are involved. Our results demonstrate that RL with pre-trained equivariant graph encodings can navigate combinatorial ordering spaces at the nanoparticle scale, and offer a transferable optimisation strategy with the potential to generalise across composition and reduce repeated individual search cost.

</details>


### [652] [Revealing the dynamic responses of Pb under shock loading based on DFT-accuracy machine learning potential](https://arxiv.org/abs/2511.12995)
*Enze Hou,Xiaoyang Wang,Han Wang*

Main category: cond-mat.mtrl-sci

TL;DR: 利用新开发的机器学习势研究铅在不同冲击取向的微观结构演化，揭示不同取向的相转变和塑性变形行为，为铅动态力学响应提供理论见解。


<details>
  <summary>Details</summary>
Motivation: 铅在冲击波加载下的塑性变形和相转变机制不明，实验方法难以揭示，且以往非平衡分子动力学模拟因经验原子间势精度有限可靠性存疑。

Method: 使用新开发的铅 - 锡合金机器学习势，研究不同冲击取向的微观结构演化。

Result: 冲击 [001] 取向时，有快速、可逆和大量的相转变及堆垛层错演化，塑性变形无孪晶；冲击 [011] 取向时，塑性变形缓慢不可逆，有局部 FCC - BCC 相转变。

Conclusion: 本研究为铅的动态力学响应提供关键理论见解，为理解极端条件下微观结构 - 性能关系提供理论输入。

Abstract: Lead (Pb) is a typical low-melting-point ductile metal and serves as an important model material in the study of dynamic responses. Under shock-wave loading, its dynamic mechanical behavior comprises two key phenomena: plastic deformation and shock induced phase transitions. The underlying mechanisms of these processes are still poorly understood. Revealing these mechanisms remains challenging for experimental approaches. Non-equilibrium molecular dynamics (NEMD) simulations are an alternative theoretical tool for studying dynamic responses, as they capture atomic-scale mechanisms such as defect evolution and deformation pathways. However, due to the limited accuracy of empirical interatomic potentials, the reliability of previous NEMD studies is questioned. Using our newly developed machine learning potential for Pb-Sn alloys, we revisited the microstructure evolution in response to shock loading under various shock orientations. The results reveal that shock loading along the [001] orientation of Pb exhibits a fast, reversible, and massive phase transition and stacking fault evolution. The behavior of Pb differs from previous studies by the absence of twinning during plastic deformation. Loading along the [011] orientation leads to slow, irreversible plastic deformation, and a localized FCC-BCC phase transition in the Pitsch orientation relationship. This study provides crucial theoretical insights into the dynamic mechanical response of Pb, offering a theoretical input for understanding the microstructure-performance relationship under extreme conditions.

</details>


<div id='hep-ex'></div>

# hep-ex [[Back]](#toc)

### [653] [Knowledge is Overrated: A zero-knowledge machine learning and cryptographic hashing-based framework for verifiable, low latency inference at the LHC](https://arxiv.org/abs/2511.12592)
*Pratik Jawahar,Caterina Doglioni,Maurizio Pierini*

Main category: hep-ex

TL;DR: 提出PHAZE框架，利用加密技术实现低延迟推理，适用于LHC触发系统。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习模型离线性能好，但在LHC上推理无法满足在线延迟约束，需改进触发性能。

Method: 提出PHAZE框架，基于哈希和零知识机器学习等加密技术，采用可认证的提前退出机制。

Result: 为框架实现纳秒级延迟奠定基础。

Conclusion: PHAZE框架有内置异常检测等优势，未来有望实现动态低级触发。

Abstract: Low latency event-selection (trigger) algorithms are essential components of Large Hadron Collider (LHC) operation. Modern machine learning (ML) models have shown great offline performance as classifiers and could improve trigger performance, thereby improving downstream physics analyses. However, inference on such large models does not satisfy the $40\text{MHz}$ online latency constraint at the LHC. In this work, we propose \texttt{PHAZE}, a novel framework built on cryptographic techniques like hashing and zero-knowledge machine learning (zkML) to achieve low latency inference, via a certifiable, early-exit mechanism from an arbitrarily large baseline model. We lay the foundations for such a framework to achieve nanosecond-order latency and discuss its inherent advantages, such as built-in anomaly detection, within the scope of LHC triggers, as well as its potential to enable a dynamic low-level trigger in the future.

</details>


### [654] [NuBench: An Open Benchmark for Deep Learning-Based Event Reconstruction in Neutrino Telescopes](https://arxiv.org/abs/2511.13111)
*Rasmus F. Orsoe,Stephan Meighen-Berger,Jeffrey Lazar,Jorge Prado,Ivan Mozun-Mateo,Aske Rosted,Philip Weigel,Arturo Llorente Anaya*

Main category: hep-ex

TL;DR: 提出用于中微子望远镜基于深度学习事件重建的开放基准NuBench，并评估四种重建算法。


<details>
  <summary>Details</summary>
Motivation: 中微子望远镜需解决事件重建逆问题，深度学习有优势，但跨实验协作缺乏多样开源数据集用于方法比较。

Method: 构建包含近1.3亿个中微子相互作用的七个大规模模拟数据集的NuBench，评估四种重建算法在五项核心任务上的表现。

Result: 文中未明确提及具体评估结果。

Conclusion: 文中未明确提及结论。

Abstract: Neutrino telescopes are large-scale detectors designed to observe Cherenkov radiation produced from neutrino interactions in water or ice. They exist to identify extraterrestrial neutrino sources and to probe fundamental questions pertaining to the elusive neutrino itself. A central challenge common across neutrino telescopes is to solve a series of inverse problems known as event reconstruction, which seeks to resolve properties of the incident neutrino, based on the detected Cherenkov light. In recent times, significant efforts have been made in adapting advances from deep learning research to event reconstruction, as such techniques provide several benefits over traditional methods. While a large degree of similarity in reconstruction needs and low-level data exists, cross-experimental collaboration has been hindered by a lack of diverse open-source datasets for comparing methods.
  We present NuBench, an open benchmark for deep learning-based event reconstruction in neutrino telescopes. NuBench comprises seven large-scale simulated datasets containing nearly 130 million charged- and neutral-current muon-neutrino interactions spanning 10 GeV to 100 TeV, generated across six detector geometries inspired by existing and proposed experiments. These datasets provide pulse- and event-level information suitable for developing and comparing machine-learning reconstruction methods in both water and ice environments. Using NuBench, we evaluate four reconstruction algorithms - ParticleNeT and DynEdge, both actively used within the KM3NeT and IceCube collaborations, respectively, along with GRIT and DeepIce - on up to five core tasks: energy and direction reconstruction, topology classification, interaction vertex prediction, and inelasticity estimation.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [655] [Congestionamento Aeroportuario, Escassez de Capacidade e Planejamento na Macrometropole Paulista](https://arxiv.org/abs/2511.13311)
*Thayla M. G. Iglesias,Alessandro V. M. Oliveira*

Main category: physics.soc-ph

TL;DR: 分析圣保罗大都市主要机场的容量限制和运营挑战，指出容量稀缺问题及扩张限制，强调综合规划的紧迫性。


<details>
  <summary>Details</summary>
Motivation: 研究圣保罗大都市主要机场面临的容量限制和运营挑战，为解决机场发展问题提供依据。

Method: 借鉴伦敦希思罗等国际案例，分析容量稀缺这一经济问题，考虑多方面限制因素，结合需求预测。

Result: 即使在保守情景下，多个机场的联合容量也可能被超越，各机场面临不同问题。

Conclusion: 强调该多机场系统对维持国家连通性的战略意义，以及综合规划的紧迫性。

Abstract: This article presents an analytical account of the capacity limits and operational challenges of the main airports in the São Paulo Macrometropolis. Drawing on international examples, such as London Heathrow, it discusses how large hubs combine high traffic generation with severe physical constraints, highlighting how saturation intensifies delays, operating costs, and pressures for expansion. It analyzes capacity scarcity as a central economic problem, in which runways, aprons, boarding gates, and terminals become critical resources whose use requires administrative and market mechanisms, including slot coordination, prioritization rules, and regulatory incentives. It discusses the limitations imposed by high earthmoving costs, environmental impacts, and expropriation costs, which restrict the physical expansion of central airports such as Congonhas and increase dependence on efficiency gains. Demand projections indicate that the combined capacity of Congonhas, Guarulhos, and Viracopos is likely to be exceeded, even in conservative scenarios, reinforcing the urgency of integrated planning. The effects of regulatory restrictions on Congonhas, the challenges of expanding Guarulhos, and the structural difficulties of Viracopos are evaluated, highlighting the strategic relevance of this multi-airport system for sustaining national connectivity.

</details>


### [656] [The Influence of Neighborhood Design on the Sustainability of US Suburbs](https://arxiv.org/abs/2511.13544)
*Arianna Salazar-Miranda*

Main category: physics.soc-ph

TL;DR: 本文研究花园城市设计（GCD）对社区社会和环境结果的影响，发现GCD导致更差的可持续性结果，凸显邻里设计对城市可持续性的重要作用。


<details>
  <summary>Details</summary>
Motivation: 美国郊区增长带来可持续性挑战，不清楚挑战源于郊区远离市中心还是开发设计，因此研究GCD对社区社会和环境结果的影响。

Method: 引入基于街道布局和街区配置的GCD综合衡量指标量化其全国采用情况，结合流动性和排放数据，采用普通最小二乘法（OLS）、匹配估计法和利用GCD采用历史变化的工具变量法（IV）估计GCD对社区结果的影响。

Result: GCD导致更差的可持续性结果，包括温室气体排放增加、社会隔离加剧和久坐行为增多，GCD的流行占郊区化相关负面影响的27 - 38%。

Conclusion: 邻里设计在塑造城市可持续性方面起着关键作用。

Abstract: The growth of suburbs in the US has led to significant sustainability challenges; yet, it remains unclear whether these challenges stem from the remoteness of suburbs from city centers or the specific designs used to develop them. This paper examines how Garden City Design (GCD) -- one of the most influential suburban design paradigms since the early 20th century -- impacts the social and environmental outcomes of neighborhoods. I first introduce a composite measure of GCD, derived from street layouts and block configurations, to quantify its nationwide adoption. I use this measure combined with mobility and emissions data to estimate the impact of GCD on neighborhood outcomes using complementary identification strategies, including ordinary least squares (OLS), matching estimators, and an instrumental variables (IV) approach that exploits historical variation in GCD adoption. Results show that GCD leads to worse sustainability outcomes, including increased greenhouse gas emissions, greater social isolation, and higher sedentary behavior. The prevalence of GCD accounts for 27-38% of the adverse effects associated with suburbanization, underscoring the crucial role that neighborhood design plays in shaping urban sustainability.

</details>


### [657] [The Singularity Warfare: The metatheoretical Framework](https://arxiv.org/abs/2511.11674)
*Ridvan Bari Urcosta*

Main category: physics.soc-ph

TL;DR: 文章引入“奇点战争”概念，指出科技革命重塑冲突本质，未来战场是物理与抽象领域融合，胜利取决于认知和技术的‘连贯性’，并综合多学科理论提供元理论框架。


<details>
  <summary>Details</summary>
Motivation: 探讨科技革命尤其是人工智能和量子力学驱动下，冲突本质的变化，提出新的战争概念以适应未来战场。

Method: 综合物理学、哲学和未来学的理论。

Result: 提出‘奇点战争’概念，明确未来战场特点及胜利关键。

Conclusion: 科技革命使战争范式发生转变，需要新的理论框架来理解。

Abstract: This paper introduces the "Singularity Warfare" concept, arguing that the accelerating pace of technological revolution, driven by artificial intelligence and quantum mechanics, is fundamentally reshaping the nature of conflict. Moving beyond traditional "Newtonian" warfare and current military doctrines, this framework posits that future battlefields will be defined by a merger of physical and abstract domains, where human imagination and algorithmic logic become a unified, actionable reality. Victory will hinge on a unit's ability to maintain cognitive and technological "coherence" while creating "decoherence" in the adversary. The paper synthesizes theories from physics, philosophy, and futurology to provide a metatheoretical framework for understanding this paradigm shift.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [658] [On the Entropy Calibration of Language Models](https://arxiv.org/abs/2511.11966)
*Steven Cao,Gregory Valiant,Percy Liang*

Main category: cs.CL

TL;DR: 研究语言模型熵校准问题，发现校准不佳问题随模型规模改善缓慢，理论上可在不增加对数损失下降低熵。


<details>
  <summary>Details</summary>
Motivation: 探究校准不佳问题是否会随模型规模改善，以及是否能无权衡地进行校准。

Method: 先研究简化理论场景，再对不同参数量语言模型进行实证测量，最后进行理论证明。

Result: 校准问题改善与数据分布幂律指数有关，实际观察与简化场景预测相似，大模型累积误差速率与小模型相近，截断方法有缺陷。理论上假设存在能预测文本未来熵的黑盒时可在不增加对数损失下降低熵。

Conclusion: 校准不佳问题随模型规模改善缓慢，截断方法不理想，理论上存在无权衡校准的可能性。

Abstract: We study the problem of entropy calibration, which asks whether a language model's entropy over generations matches its log loss on human text. Past work found that models are miscalibrated, with entropy per step increasing (and text quality decreasing) as generations grow longer. This error accumulation is a fundamental problem in autoregressive models, and the standard solution is to truncate the distribution, which improves text quality at the cost of diversity. In this paper, we ask: is miscalibration likely to improve with scale, and is it theoretically possible to calibrate without tradeoffs? To build intuition, we first study a simplified theoretical setting to characterize the scaling behavior of miscalibration with respect to dataset size. We find that the scaling behavior depends on the power law exponent of the data distribution -- in particular, for a power law exponent close to 1, the scaling exponent is close to 0, meaning that miscalibration improves very slowly with scale. Next, we measure miscalibration empirically in language models ranging from 0.5B to 70B parameters. We find that the observed scaling behavior is similar to what is predicted by the simplified setting: our fitted scaling exponents for text are close to 0, meaning that larger models accumulate error at a similar rate as smaller ones. This scaling (or, lack thereof) provides one explanation for why we sample from larger models with similar amounts of truncation as smaller models, even though the larger models are of higher quality. However, truncation is not a satisfying solution because it comes at the cost of increased log loss. In theory, is it even possible to reduce entropy while preserving log loss? We prove that it is possible, if we assume access to a black box which can fit models to predict the future entropy of text.

</details>


### [659] [Auditing Google's AI Overviews and Featured Snippets: A Case Study on Baby Care and Pregnancy](https://arxiv.org/abs/2511.12920)
*Desheng Hu,Joachim Baumann,Aleksandra Urman,Elsa Lichtenegger,Robin Forsberg,Aniko Hannak,Christo Wilson*

Main category: cs.CL

TL;DR: 通过对1508个母婴护理和怀孕相关查询进行算法审计，评估谷歌搜索AI生成内容质量，发现信息一致性有差距、缺乏医疗保障，需加强质量控制。


<details>
  <summary>Details</summary>
Motivation: 评估谷歌搜索通过AI Overviews和Featured Snippets展示的AI生成内容的质量和一致性，因其影响公众健康信息获取。

Method: 对1508个真实母婴护理和怀孕相关查询进行系统算法审计，用评估框架评估多个质量维度。

Result: 信息一致性有差距，33%的情况下同一搜索结果页面上AIO和FS信息不一致；二者均严重缺乏医疗保障，分别仅11%和7%；FS常链接商业来源。

Conclusion: 研究结果对公共健康信息获取有重要意义，需加强AI介导健康信息的质量控制，方法论可用于高风险领域AI系统审计。

Abstract: Google Search increasingly surfaces AI-generated content through features like AI Overviews (AIO) and Featured Snippets (FS), which users frequently rely on despite having no control over their presentation. Through a systematic algorithm audit of 1,508 real baby care and pregnancy-related queries, we evaluate the quality and consistency of these information displays. Our robust evaluation framework assesses multiple quality dimensions, including answer consistency, relevance, presence of medical safeguards, source categories, and sentiment alignment. Our results reveal concerning gaps in information consistency, with information in AIO and FS displayed on the same search result page being inconsistent with each other in 33% of cases. Despite high relevance scores, both features critically lack medical safeguards (present in just 11% of AIO and 7% of FS responses). While health and wellness websites dominate source categories for both, AIO and FS, FS also often link to commercial sources. These findings have important implications for public health information access and demonstrate the need for stronger quality controls in AI-mediated health information. Our methodology provides a transferable framework for auditing AI systems across high-stakes domains where information quality directly impacts user well-being.

</details>


### [660] [HAPO: Training Language Models to Reason Concisely via History-Aware Policy Optimization](https://arxiv.org/abs/2505.11225)
*Chengyu Huang,Zhengxin Zhang,Claire Cardie*

Main category: cs.CL

TL;DR: 现有测试时扩展响应长度方法有问题，提出历史感知策略优化（HAPO）方法，实验证明其能有效提升大语言模型简洁推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有高效测试时扩展方法未利用训练中同一问题的历史信息，限制了使解决方案随时间更简洁的能力。

Method: 提出HAPO方法，跟踪每个问题的历史状态，采用基于历史状态的长度奖励函数，结合正确性奖励联合优化正确性和效率。

Result: 使用HAPO训练多个模型，在多个数学基准测试中，实现长度减少33 - 59%，准确率仅下降2 - 5%。

Conclusion: HAPO能有效诱导大语言模型的简洁推理能力。

Abstract: While scaling the length of responses at test-time has been shown to markedly improve the reasoning abilities and performance of large language models (LLMs), it often results in verbose outputs and increases inference cost. Prior approaches for efficient test-time scaling, typically using universal budget constraints or query-level length optimization, do not leverage historical information from previous encounters with the same problem during training. We hypothesize that this limits their ability to progressively make solutions more concise over time. To address this, we present History-Aware Policy Optimization (HAPO), which keeps track of a history state (e.g., the minimum length over previously generated correct responses) for each problem. HAPO employs a novel length reward function based on this history state to incentivize the discovery of correct solutions that are more concise than those previously found. Crucially, this reward structure avoids overly penalizing shorter incorrect responses with the goal of facilitating exploration towards more efficient solutions. By combining this length reward with a correctness reward, HAPO jointly optimizes for correctness and efficiency. We use HAPO to train DeepSeek-R1-Distill-Qwen-1.5B, DeepScaleR-1.5B-Preview, and Qwen-2.5-1.5B-Instruct, and evaluate HAPO on several math benchmarks that span various difficulty levels. Experiment results demonstrate that HAPO effectively induces LLMs' concise reasoning abilities, producing length reductions of 33-59% with accuracy drops of only 2-5%.

</details>


### [661] [DCRM: A Heuristic to Measure Response Pair Quality in Preference Optimization](https://arxiv.org/abs/2506.14157)
*Chengyu Huang,Tanya Goyal*

Main category: cs.CL

TL;DR: 本文提出DCRM指标衡量偏好优化响应质量，研究常用偏好数据集，建立DCRM与学习结果关联，提出最佳配对方法提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究尝试关联偏好优化性能与偏好数据集，本文观察到偏好和非偏好响应差异影响大语言模型学习，且可能与期望学习的差异不匹配。

Method: 用距离和奖励裕度量化差异得到DCRM指标，研究3种常用偏好数据集，提出best-of-$N^2$配对方法选择DCRM最高的响应对。

Result: 在各种设置下，提出的方法生成的训练数据集能在AlpacaEval、MT - Bench和Arena - Hard上进一步提升模型性能。

Conclusion: DCRM可衡量响应质量，且训练集DCRM越高学习结果越好，最佳配对方法能提升模型性能。

Abstract: Recent research has attempted to associate preference optimization (PO) performance with the underlying preference datasets. In this work, our observation is that the differences between the preferred response $y^+$ and dispreferred response $y^-$ influence what LLMs can learn, which may not match the desirable differences to learn. Therefore, we use distance and reward margin to quantify these differences, and combine them to get Distance Calibrated Reward Margin (DCRM), a metric that measures the quality of a response pair for PO. Intuitively, DCRM encourages minimal noisy differences and maximal desired differences. With this, we study 3 types of commonly used preference datasets, classified along two axes: the source of the responses and the preference labeling function. We establish a general correlation between higher DCRM of the training set and better learning outcome. Inspired by this, we propose a best-of-$N^2$ pairing method that selects response pairs with the highest DCRM. Empirically, in various settings, our method produces training datasets that can further improve models' performance on AlpacaEval, MT-Bench, and Arena-Hard over the existing training sets.

</details>


### [662] [TimeStampEval: A Simple LLM Eval and a Little Fuzzy Matching Trick to Improve Search Accuracy](https://arxiv.org/abs/2511.11594)
*James McCammon*

Main category: cs.CL

TL;DR: 引入TimeStampEval基准用于从长转录本中检索精确时间戳，提出两阶段方法提升检索准确性并降低推理成本，评估LLMs有四项关键发现，扩展测试验证方法鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决传统模糊匹配在跨文档搜索语义相同但语法不同的引语时失败的问题，应用于自动长形式播客组装国会记录片段。

Method: 提出两阶段方法，采用“Assisted Fuzzy”方法（RapidFuzz预过滤后LLM验证短片段）。

Result: 评估六个现代LLMs有四项关键发现，扩展测试在不同长度、词汇漂移和领域变化的转录本上保持高拒绝准确率。

Conclusion: 提出的方法能显著提升检索准确性，降低推理成本，对不同情况有较强鲁棒性。

Abstract: Traditional fuzzy matching often fails when searching for quotes that are semantically identical but syntactically different across documents-a common issue when aligning official written records with speech-to-text transcripts. We introduce TimeStampEval, a benchmark for retrieving precise millisecond timestamps from long transcripts given non-verbatim quotes. Our simple two-stage method dramatically improves retrieval accuracy while cutting inference costs by over 90%. The motivating use case is an automated long-form podcast that assembles Congressional Record clips into AI-hosted narration. The technical challenge: given a sentence-timestamped transcript and a target quote that may differ due to transcription or editorial drift, return exact start and end boundaries. Standard algorithms handle verbatim text but break under fuzzier variants. Evaluating six modern LLMs on a 2,800-sentence (120k-token) transcript revealed four key findings. (1) Prompt design matters more than model choice: placing the query before the transcript and using compact formatting improved accuracy by 3-20 points while reducing token count by 30-40%. (2) Off-by-one errors form a distinct category, showing models understand the task but misplace boundaries. (3) A modest reasoning budget (600-850 tokens) raises accuracy from 37% to 77% for weak setups and to above 90% for strong ones. (4) Our "Assisted Fuzzy" approach-RapidFuzz pre-filtering followed by LLM verification on short snippets-improves fuzzy match accuracy by up to 50 points while halving latency and reducing cost per correct result by up to 96%. Extended tests on ten transcripts (50k-900k tokens, 1989-2025) confirm robustness to transcript length, vocabulary drift, and domain change, maintaining 95-100% rejection accuracy for absent targets.

</details>


### [663] [On the Notion that Language Models Reason](https://arxiv.org/abs/2511.11810)
*Bertram Højer*

Main category: cs.CL

TL;DR: 探讨语言模型推理定义，指出其与训练等过程不符，说明推理输出源于统计规律，强调描述计算过程的重要性


<details>
  <summary>Details</summary>
Motivation: 评估自然语言处理领域语言模型推理的定义，解决定义与模型实际情况的矛盾

Method: 假设基于Transformer的语言模型实现隐式有限阶马尔可夫核映射，以此说明推理输出的本质

Result: 推理输出对应学习核中的统计规律和近似统计不变性，而非显式逻辑机制

Conclusion: 区分语言模型是统计模式匹配器而非真正推理者，对评估认知不确定性至关重要，应重视描述系统计算过程

Abstract: Language models (LMs) are said to be exhibiting reasoning, but what does this entail? We assess definitions of reasoning and how key papers in the field of natural language processing (NLP) use the notion and argue that the definitions provided are not consistent with how LMs are trained, process information, and generate new tokens. To illustrate this incommensurability we assume the view that transformer-based LMs implement an \textit{implicit} finite-order Markov kernel mapping contexts to conditional token distributions. In this view, reasoning-like outputs correspond to statistical regularities and approximate statistical invariances in the learned kernel rather than the implementation of explicit logical mechanisms. This view is illustrative of the claim that LMs are "statistical pattern matchers"" and not genuine reasoners and provides a perspective that clarifies why reasoning-like outputs arise in LMs without any guarantees of logical consistency. This distinction is fundamental to how epistemic uncertainty is evaluated in LMs. We invite a discussion on the importance of how the computational processes of the systems we build and analyze in NLP research are described.

</details>


### [664] [Scaling Open-Weight Large Language Models for Hydropower Regulatory Information Extraction: A Systematic Analysis](https://arxiv.org/abs/2511.11821)
*Hong-Jun Yoon,Faisal Ashraf,Thomas A. Ruggles,Debjani Singh*

Main category: cs.CL

TL;DR: 评估七种开放权重模型用于监管文件信息提取，确定14B参数阈值，建立资源 - 性能映射，为水电合规提供价值。


<details>
  <summary>Details</summary>
Motivation: 解决使用大语言模型进行监管文件信息提取时性能与计算资源的权衡问题，为部署提供经验指导。

Method: 在水电许可文档上评估七种参数为0.6B - 70B的开放权重模型。

Result: 确定14B参数阈值，消费者可部署模型F1达64%，小模型F1最高51%，大模型接近77%；发现小模型完美召回意味着提取失败的幻觉模式。

Conclusion: 建立监管环境下开放权重信息提取的资源 - 性能映射，可进行基于证据的模型选择，为水电合规提供价值并为信息提取任务提供参数缩放效应的见解。

Abstract: Information extraction from regulatory documents using large language models presents critical trade-offs between performance and computational resources. We evaluated seven open-weight models (0.6B-70B parameters) on hydropower licensing documentation to provide empirical deployment guidance.
  Our analysis identified a pronounced 14B parameter threshold where validation methods transition from ineffective (F1 $<$ 0.15) to viable (F1 = 0.64). Consumer-deployable models achieve 64\% F1 through appropriate validation, while smaller models plateau at 51\%. Large-scale models approach 77\% F1 but require enterprise infrastructure.
  We identified systematic hallucination patterns where perfect recall indicates extraction failure rather than success in smaller models. Our findings establish the first comprehensive resource-performance mapping for open-weight information extraction in regulatory contexts, enabling evidence-based model selection.
  These results provide immediate value for hydropower compliance while contributing insights into parameter scaling effects that generalize across information extraction tasks.

</details>


### [665] [Towards Autoformalization of LLM-generated Outputs for Requirement Verification](https://arxiv.org/abs/2511.11829)
*Mihir Gupte,Ramesh S*

Main category: cs.CL

TL;DR: 本文探索用简单基于大语言模型的自动形式化器验证大语言模型生成输出，通过两个实验展示其在一致性检查和形式验证方面的潜力，为后续研究奠基。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏验证大语言模型生成结构化输出准确性的形式化方法，本文旨在填补这一空白。

Method: 使用简单的基于大语言模型的自动形式化器，针对少量自然语言需求验证大语言模型生成的输出，并进行两个不同实验。

Result: 第一个实验中自动形式化器识别出两个不同表述的自然语言需求逻辑等价；第二个实验中识别出给定自然语言需求与大语言模型生成输出间的逻辑不一致。

Conclusion: 自动形式化在确保大语言模型生成输出的保真度和逻辑一致性方面有巨大潜力，为后续更广泛研究奠定重要基础。

Abstract: Autoformalization, the process of translating informal statements into formal logic, has gained renewed interest with the emergence of powerful Large Language Models (LLMs). While LLMs show promise in generating structured outputs from natural language (NL), such as Gherkin Scenarios from NL feature requirements, there's currently no formal method to verify if these outputs are accurate. This paper takes a preliminary step toward addressing this gap by exploring the use of a simple LLM-based autoformalizer to verify LLM-generated outputs against a small set of natural language requirements. We conducted two distinct experiments. In the first one, the autoformalizer successfully identified that two differently-worded NL requirements were logically equivalent, demonstrating the pipeline's potential for consistency checks. In the second, the autoformalizer was used to identify a logical inconsistency between a given NL requirement and an LLM-generated output, highlighting its utility as a formal verification tool. Our findings, while limited, suggest that autoformalization holds significant potential for ensuring the fidelity and logical consistency of LLM-generated outputs, laying a crucial foundation for future, more extensive studies into this novel application.

</details>


### [666] [Three Stage Narrative Analysis; Plot-Sentiment Breakdown, Structure Learning and Concept Detection](https://arxiv.org/abs/2511.11857)
*Taimur Khan,Ramoza Ahsan,Mohib Hameed*

Main category: cs.CL

TL;DR: 提出分析电影脚本情感弧线并关联角色上下文的框架，用字典法和聚类技术，实验表明分析对选择叙事有帮助。


<details>
  <summary>Details</summary>
Motivation: 故事理解和分析是自然语言理解难题，大量叙事数据需自动化语义分析和计算学习。

Method: 提出框架，用基于LabMTsimple storylab模块构建的自定义词典进行情感分析，基于NRC - VAD数据集的VAD分数，用Wards层次聚类技术聚类相似情感情节。

Result: 在电影数据集上的实验表明分析对消费者和读者选择叙事有帮助。

Conclusion: 所提框架对电影脚本情感分析有效，能辅助选择叙事。

Abstract: Story understanding and analysis have long been challenging areas within Natural Language Understanding. Automated narrative analysis requires deep computational semantic representations along with syntactic processing. Moreover, the large volume of narrative data demands automated semantic analysis and computational learning rather than manual analytical approaches. In this paper, we propose a framework that analyzes the sentiment arcs of movie scripts and performs extended analysis related to the context of the characters involved. The framework enables the extraction of high-level and low-level concepts conveyed through the narrative. Using dictionary-based sentiment analysis, our approach applies a custom lexicon built with the LabMTsimple storylab module. The custom lexicon is based on the Valence, Arousal, and Dominance scores from the NRC-VAD dataset. Furthermore, the framework advances the analysis by clustering similar sentiment plots using Wards hierarchical clustering technique. Experimental evaluation on a movie dataset shows that the resulting analysis is helpful to consumers and readers when selecting a narrative or story.

</details>


### [667] [LLMLagBench: Identifying Temporal Training Boundaries in Large Language Models](https://arxiv.org/abs/2511.12116)
*Piotr Pęzik,Konrad Kaczyński,Maria Szymańska,Filip Żarnecki,Zuzanna Deckert,Jakub Kwiatkowski,Wojciech Janowski*

Main category: cs.CL

TL;DR: 引入LLMLagBench基准评估大语言模型训练数据时间边界并评估大量模型，通过手动验证和对比公开信息评估基准可靠性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在特定时间截止点的文本数据上预训练，存在知识边界，可能混合过时信息影响推理准确性，需识别其训练数据的时间边界。

Method: 引入LLMLagBench基准评估模型对近期事件的知识，以此确定其训练数据的最早可能时间边界，并应用该基准评估大量模型，通过手动验证和对比公开信息评估基准可靠性。

Result: 对大量有明确和未明确训练截止点的大语言模型进行了评估。

Conclusion: LLMLagBench基准可作为识别大语言模型训练数据时间边界的系统方法。

Abstract: Large Language Models (LLMs) are pretrained on textual data up to a specific temporal cutoff. This creates a strict knowledge boundary beyond which models cannot provide accurate information without querying external sources. More subtly, when this limitation is unknown or ignored, LLMs may inadvertently blend outdated time-sensitive information with general knowledge during reasoning tasks, potentially compromising response accuracy. We introduce LLMLagBench, an LLM freshness benchmark, as a systematic approach for identifying the earliest probable temporal boundaries of an LLM's training data by evaluating its knowledge of recent events. We then apply this benchmark to evaluate a large set of LLMs, including models with both explicitly declared and undeclared training cutoffs. The reliability of the benchmark is assessed by manual validation and comparison with publicly released information about LLM pretraining.

</details>


### [668] [MME-RAG: Multi-Manager-Expert Retrieval-Augmented Generation for Fine-Grained Entity Recognition in Task-Oriented Dialogues](https://arxiv.org/abs/2511.12213)
*Liang Xue,Haoyu Liu,Yajun Tian,Xinyu Zhong,Yang Liu*

Main category: cs.CL

TL;DR: 引入MME - RAG框架用于细粒度实体识别，实验表明其在多领域表现优于基线，分层分解和关键信息引导检索是关键。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在细粒度实体识别的领域适应和检索可控性方面存在挑战。

Method: 引入MME - RAG框架，将实体识别分解为类型级判断和跨度级提取两个阶段，专家由KeyInfo检索器支持，推理时注入语义对齐的少样本示例。

Result: 在多个数据集上的实验显示MME - RAG在大多数领域表现优于近期基线，消融研究表明分层分解和KeyInfo引导检索是性能关键。

Conclusion: MME - RAG是自适应对话理解的可扩展且可解释的解决方案。

Abstract: Fine-grained entity recognition is crucial for reasoning and decision-making in task-oriented dialogues, yet current large language models (LLMs) continue to face challenges in domain adaptation and retrieval controllability. We introduce MME-RAG, a Multi-Manager-Expert Retrieval-Augmented Generation framework that decomposes entity recognition into two coordinated stages: type-level judgment by lightweight managers and span-level extraction by specialized experts. Each expert is supported by a KeyInfo retriever that injects semantically aligned, few-shot exemplars during inference, enabling precise and domain-adaptive extraction without additional training. Experiments on CrossNER, MIT-Movie, MIT-Restaurant, and our newly constructed multi-domain customer-service dataset demonstrate that MME-RAG performs better than recent baselines in most domains. Ablation studies further show that both the hierarchical decomposition and KeyInfo-guided retrieval are key drivers of robustness and cross-domain generalization, establishing MME-RAG as a scalable and interpretable solution for adaptive dialogue understanding.

</details>


### [669] [Consistency Is the Key: Detecting Hallucinations in LLM Generated Text By Checking Inconsistencies About Key Facts](https://arxiv.org/abs/2511.12236)
*Raavi Gupta,Pranav Hari Panicker,Sumit Bhatia,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: 提出 CONFACTCHECK 方法高效检测大语言模型幻觉，资源使用少且准确率高。


<details>
  <summary>Details</summary>
Motivation: 大语言模型常产生幻觉，现有受限场景下检测方法需多次调用 API，增加延迟和成本。

Method: CONFACTCHECK 不借助外部知识库，基于生成文本中事实性探测响应在单个和不同大语言模型内保持一致的直觉。

Result: 在多个数据集上严格实证评估表明，CONFACTCHECK 用更少资源高效检测幻觉事实，准确率高于类似条件下的现有基线。

Conclusion: CONFACTCHECK 是一种高效的幻觉检测方法，代码已开源。

Abstract: Large language models (LLMs), despite their remarkable text generation capabilities, often hallucinate and generate text that is factually incorrect and not grounded in real-world knowledge. This poses serious risks in domains like healthcare, finance, and customer support. A typical way to use LLMs is via the APIs provided by LLM vendors where there is no access to model weights or options to fine-tune the model. Existing methods to detect hallucinations in such settings where the model access is restricted or constrained by resources typically require making multiple LLM API calls, increasing latency and API cost. We introduce CONFACTCHECK, an efficient hallucination detection approach that does not leverage any external knowledge base and works on the simple intuition that responses to factual probes within the generated text should be consistent within a single LLM and across different LLMs. Rigorous empirical evaluation on multiple datasets that cover both the generation of factual texts and the open generation shows that CONFACTCHECK can detect hallucinated facts efficiently using fewer resources and achieves higher accuracy scores compared to existing baselines that operate under similar conditions. Our code is available here.

</details>


### [670] [Don't Think of the White Bear: Ironic Negation in Transformer Models Under Cognitive Load](https://arxiv.org/abs/2511.12381)
*Logan Mann,Nayan Saxena,Sarah Tandon,Chenhao Sun,Savar Toteja,Kevin Zhu*

Main category: cs.CL

TL;DR: 研究大语言模型中否定指令引发的反讽反弹现象，通过两个实验得出相关结果并发布数据集。


<details>
  <summary>Details</summary>
Motivation: 人类思维中否定指令会引发反讽反弹，大语言模型也面临同样挑战，研究其在否定指令下的表现。

Method: 进行两个实验，一是改变否定指令后的干扰文本类型并测量反弹强度；二是测试模型能否区分同一概念的中性和负面框架以及这种区分与反弹持续性的关系，还进行了电路追踪分析。

Result: 否定后立即出现反弹，长或语义干扰文本会加剧反弹，重复有助于抑制；更强的极性分离与更持久的反弹相关；电路追踪分析发现中间层注意力头放大禁止标记而早期层抑制。

Conclusion: 将反讽反弹的认知预测与长上下文干扰的机制见解相联系，发布ReboundBench数据集支持后续研究。

Abstract: Negation instructions such as 'do not mention $X$' can paradoxically increase the accessibility of $X$ in human thought, a phenomenon known as ironic rebound. Large language models (LLMs) face the same challenge: suppressing a concept requires internally activating it, which may prime rebound instead of avoidance. We investigated this tension with two experiments. \textbf{(1) Load \& content}: after a negation instruction, we vary distractor text (semantic, syntactic, repetition) and measure rebound strength. \textbf{(2) Polarity separation}: We test whether models distinguish neutral from negative framings of the same concept and whether this separation predicts rebound persistence. Results show that rebound consistently arises immediately after negation and intensifies with longer or semantic distractors, while repetition supports suppression. Stronger polarity separation correlates with more persistent rebound. Together, these findings, complemented by a circuit tracing analysis that identifies sparse middle-layer attention heads amplifying forbidden tokens while early layers suppress, link cognitive predictions of ironic rebound with mechanistic insights into long-context interference. To support future work, we release ReboundBench, a dataset of $5,000$ systematically varied negation prompts designed to probe rebound in LLMs.

</details>


### [671] [From Phonemes to Meaning: Evaluating Large Language Models on Tamil](https://arxiv.org/abs/2511.12387)
*Jeyarajalingam Varsha,Menan Velayuthan,Sumirtha Karunakaran,Rasan Nivethiga,Kengatharaiyer Sarveswaran*

Main category: cs.CL

TL;DR: 本文介绍首个泰米尔语语言评估基准ILAKKANAM，用其评估大模型，发现Gemini 2.5表现最佳，开源模型落后，模型在复杂语言问题上表现下降，性能或源于数据暴露而非理解。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在低资源、形态丰富语言（如泰米尔语）的语言能力未充分探索，现有多语言基准依赖翻译数据集，无法捕捉目标语言的语言和文化细微差别。

Method: 引入手动策划的泰米尔语语言评估基准ILAKKANAM，用标准化评估框架评估闭源和开源大语言模型。

Result: Gemini 2.5整体性能最高，开源模型落后；所有模型在低年级问题上表现好，语言复杂度增加时表现下降；模型整体性能和识别语言类别的能力无强相关性。

Conclusion: 大语言模型在泰米尔语上存在语言基础差距，其性能可能由数据暴露而非真正理解驱动。

Abstract: Large Language Models (LLMs) have shown strong generalization across tasks in high-resource languages; however, their linguistic competence in low-resource and morphologically rich languages such as Tamil remains largely unexplored. Existing multilingual benchmarks often rely on translated English datasets, failing to capture the linguistic and cultural nuances of the target language. To address this gap, we introduce ILAKKANAM, the first Tamil-specific linguistic evaluation benchmark manually curated using 820 questions from Sri Lankan school-level Tamil subject examination papers. Each question is annotated by trained linguists under five linguistic categories and a factual knowledge category, spanning Grades 1--13 to ensure broad linguistic coverage. We evaluate both closed-source and open-source LLMs using a standardized evaluation framework. Our results show that Gemini 2.5 achieves the highest overall performance, while open-source models lag behind, highlighting the gap in linguistic grounding. Category- and grade-wise analyses reveal that all models perform well on lower-grade questions but show a clear decline as linguistic complexity increases. Further, no strong correlation is observed between a model's overall performance and its ability to identify linguistic categories, suggesting that performance may be driven by exposure rather than genuine understanding.

</details>


### [672] [Assessing LLMs for Serendipity Discovery in Knowledge Graphs: A Case for Drug Repurposing](https://arxiv.org/abs/2511.12472)
*Mengying Wang,Chenhui Ma,Ao Jiao,Tuo Liang,Pengjun Lu,Shrinidhi Hegde,Yu Yin,Evren Gurkan-Cavusoglu,Yinghui Wu*

Main category: cs.CL

TL;DR: 提出SerenQA框架评估大语言模型在科学知识图谱问答任务中发现意外见解的能力，实验表明模型在识别意外有价值发现方面有提升空间。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱问答系统优化方向为返回高相关但可预测答案，缺乏利用大语言模型给出意外新颖答案的能力，因此要评估其发现意外见解的能力。

Method: 定义意外感知的知识图谱问答任务，提出SerenQA框架，包含严格的意外性指标和专家标注基准，还有结构化评估流程。

Result: 实验显示，最先进的大语言模型在检索方面表现良好，但难以识别真正意外且有价值的发现。

Conclusion: 当前大语言模型在识别意外有价值发现上有很大提升空间。

Abstract: Large Language Models (LLMs) have greatly advanced knowledge graph question answering (KGQA), yet existing systems are typically optimized for returning highly relevant but predictable answers. A missing yet desired capacity is to exploit LLMs to suggest surprise and novel ("serendipitious") answers. In this paper, we formally define the serendipity-aware KGQA task and propose the SerenQA framework to evaluate LLMs' ability to uncover unexpected insights in scientific KGQA tasks. SerenQA includes a rigorous serendipity metric based on relevance, novelty, and surprise, along with an expert-annotated benchmark derived from the Clinical Knowledge Graph, focused on drug repurposing. Additionally, it features a structured evaluation pipeline encompassing three subtasks: knowledge retrieval, subgraph reasoning, and serendipity exploration. Our experiments reveal that while state-of-the-art LLMs perform well on retrieval, they still struggle to identify genuinely surprising and valuable discoveries, underscoring a significant room for future improvements. Our curated resources and extended version are released at: https://cwru-db-group.github.io/serenQA.

</details>


### [673] [SGuard-v1: Safety Guardrail for Large Language Models](https://arxiv.org/abs/2511.12497)
*JoonHo Lee,HyeonMin Cho,Jaewoong Yun,Hyunjae Lee,JunKyu Lee,Juree Seok*

Main category: cs.CL

TL;DR: 本文提出轻量级大语言模型安全防护系统SGuard - v1，含两个组件，经大量评估有先进安全性能且轻量，还提升可解释性并开源。


<details>
  <summary>Details</summary>
Motivation: 为大语言模型在人机对话场景提供安全防护，检测有害内容和对抗性提示。

Method: 构建ContentFilter和JailbreakFilter两个组件，基于2B参数Granite - 3.3 - 2B - Instruct模型，整理约140万训练实例进行指令微调。

Result: 在公共和专有安全基准测试中达到先进安全性能，保持轻量级，降低部署开销，提供多类安全预测和二元置信度分数。

Conclusion: SGuard - v1可用于进一步研究和人工智能安全的实际部署。

Abstract: We present SGuard-v1, a lightweight safety guardrail for Large Language Models (LLMs), which comprises two specialized models to detect harmful content and screen adversarial prompts in human-AI conversational settings. The first component, ContentFilter, is trained to identify safety risks in LLM prompts and responses in accordance with the MLCommons hazard taxonomy, a comprehensive framework for trust and safety assessment of AI. The second component, JailbreakFilter, is trained with a carefully designed curriculum over integrated datasets and findings from prior work on adversarial prompting, covering 60 major attack types while mitigating false-unsafe classification. SGuard-v1 is built on the 2B-parameter Granite-3.3-2B-Instruct model that supports 12 languages. We curate approximately 1.4 million training instances from both collected and synthesized data and perform instruction tuning on the base model, distributing the curated data across the two component according to their designated functions. Through extensive evaluation on public and proprietary safety benchmarks, SGuard-v1 achieves state-of-the-art safety performance while remaining lightweight, thereby reducing deployment overhead. SGuard-v1 also improves interpretability for downstream use by providing multi-class safety predictions and their binary confidence scores. We release the SGuard-v1 under the Apache-2.0 License to enable further research and practical deployment in AI safety.

</details>


### [674] [Mitigating Length Bias in RLHF through a Causal Lens](https://arxiv.org/abs/2511.12573)
*Hyeonji Kim,Sujeong Oh,Sanghack Lee*

Main category: cs.CL

TL;DR: 提出因果框架分析和缓解RLHF奖励建模中的长度偏差，通过反事实数据增强方法训练奖励模型，减少了长度偏差，提升了输出质量。


<details>
  <summary>Details</summary>
Motivation: 解决RLHF训练的奖励模型存在的长度偏差问题，即倾向于更长回复，将冗长与质量混淆。

Method: 提出因果框架，采用反事实数据增强方法，构建长度不同内容相似和内容不同长度相似的响应对来训练奖励模型。

Result: 减少了奖励分配中的长度偏差，使策略模型输出更简洁、聚焦内容。

Conclusion: 所提方法有效减少长度偏差，提高了RLHF流程中奖励建模的鲁棒性和内容敏感性。

Abstract: Reinforcement learning from human feedback (RLHF) is widely used to align large language models (LLMs) with human preferences. However, RLHF-trained reward models often exhibit length bias -- a systematic tendency to favor longer responses by conflating verbosity with quality. We propose a causal framework for analyzing and mitigating length bias in RLHF reward modeling. Central to our approach is a counterfactual data augmentation method that generates response pairs designed to isolate content quality from verbosity. These counterfactual examples are then used to train the reward model, enabling it to assess responses based on content quality independently of verbosity. Specifically, we construct (1) length-divergent pairs with similar content and (2) content-divergent pairs of similar length. Empirical evaluations show that our method reduces length bias in reward assignment and leads to more concise, content-focused outputs from the policy model. These findings demonstrate that the proposed approach effectively reduces length bias and improves the robustness and content sensitivity of reward modeling in RLHF pipelines.

</details>


### [675] [Group-Aware Reinforcement Learning for Output Diversity in Large Language Models](https://arxiv.org/abs/2511.12596)
*Oron Anschel,Alon Shoshan,Adam Botach,Shunit Haviv Hakimi,Asaf Gendler,Emanuel Ben Baruch,Nadav Bhonker,Igor Kviatkovsky,Manoj Aggarwal,Gerard Medioni*

Main category: cs.CL

TL;DR: 论文引入GAPO扩展GRPO，提升大语言模型输出多样性，且不影响标准基准测试准确性，代码将公开。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在模式崩溃问题，输出多样性受限。

Method: 引入Group - Aware Policy Optimization (GAPO)，基于频率感知奖励函数学习组级属性。

Result: GAPO训练的模型能产生有效且更多样的响应，可泛化到开放式提示，在标准基准测试中不影响准确性。

Conclusion: GAPO能提升大语言模型输出多样性，且具有良好泛化性和准确性。

Abstract: Large Language Models (LLMs) often suffer from mode collapse, repeatedly generating the same few completions even when many valid answers exist, limiting their diversity across a wide range of tasks. We introduce Group-Aware Policy Optimization (GAPO), a simple extension of the recent and popular Group Relative Policy Optimization (GRPO) that computes rewards over the group as a whole. GAPO enables learning from the group-level properties such as diversity and coverage. We demonstrate GAPO using a frequency-aware reward function that encourages uniform sampling over valid LLM completions, and show that GAPO-trained models produce valid and more diverse model responses. Beyond this setup, GAPO generalizes to open-ended prompts and improves response diversity without compromising accuracy on standard LLM benchmarks (GSM8K, MATH, HumanEval, MMLU-Pro). Our code will be made publicly available.

</details>


### [676] [Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data](https://arxiv.org/abs/2511.12609)
*Yunxin Li,Xinyu Chen,Shenyuan Jiang,Haoyuan Shi,Zhenyu Liu,Xuanyu Zhang,Nanhao Deng,Zhenran Xu,Yicheng Ma,Meishan Zhang,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: 本文介绍Lychee家族的全开源多模态大模型Uni - MoE 2.0，基于Qwen2.5 - 7B架构构建，在多模态理解、推理和生成上表现出色，评估显示性能达SOTA或极具竞争力。


<details>
  <summary>Details</summary>
Motivation: 推进Lychee的Uni - MoE系列在以语言为中心的多模态理解、推理和生成方面的发展。

Method: 基于Qwen2.5 - 7B架构，采用动态容量Mixture - of - Experts设计、渐进式训练策略结合迭代强化策略、精心设计的多模态数据匹配技术；新MoE框架平衡计算效率和能力，Omni - Modality 3D RoPE确保跨模态对齐；训练采用渐进式监督微调策略，结合平衡数据组成和迭代GSPO - DPO方法；数据上使用约75B开源多模态数据训练并配备特殊生成令牌。

Result: 在85个基准测试中，模型达到SOTA或极具竞争力，在76个基准测试中超50个超越Qwen2.5 - Omni，在视频理解、多模态理解、视听推理等方面有提升，改进长语音处理和图像生成。

Conclusion: Uni - MoE 2.0在多模态理解、推理和生成上取得显著进展，性能优异。

Abstract: We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee's Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the Qwen2.5-7B dense architecture, we build Uni-MoE-2.0-Omni from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech. Architecturally, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer. For training, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning. Data-wise, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues. Extensive evaluation across 85 benchmarks demonstrates that our model achieves SOTA or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (reducing WER by 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.

</details>


### [677] [Knots: A Large-Scale Multi-Agent Enhanced Expert-Annotated Dataset and LLM Prompt Optimization for NOTAM Semantic Parsing](https://arxiv.org/abs/2511.12630)
*Maoqi Liu,Quan Fang,Yang Yang,Can Zhao,Kaiquan Cai*

Main category: cs.CL

TL;DR: 本文提出NOTAM语义解析任务，构建Knots数据集，评估多种策略和技术，提升航空文本处理能力，代码开源。


<details>
  <summary>Details</summary>
Motivation: NOTAM复杂语言结构和隐式推理给自动解析带来挑战，现有研究缺乏深度语义理解。

Method: 提出NOTAM语义解析任务，构建Knots数据集并通过多智能体协作框架增强，评估多种提示工程策略和模型适应技术。

Result: 在航空文本理解和处理上取得显著改进，实验证明方法有效。

Conclusion: 所提方法有效，为自动NOTAM分析系统提供有价值见解。

Abstract: Notice to Air Missions (NOTAMs) serve as a critical channel for disseminating key flight safety information, yet their complex linguistic structures and implicit reasoning pose significant challenges for automated parsing. Existing research mainly focuses on surface-level tasks such as classification and named entity recognition, lacking deep semantic understanding. To address this gap, we propose NOTAM semantic parsing, a task emphasizing semantic inference and the integration of aviation domain knowledge to produce structured, inference-rich outputs. To support this task, we construct Knots (Knowledge and NOTAM Semantics), a high-quality dataset of 12,347 expert-annotated NOTAMs covering 194 Flight Information Regions, enhanced through a multi-agent collaborative framework for comprehensive field discovery. We systematically evaluate a wide range of prompt-engineering strategies and model-adaptation techniques, achieving substantial improvements in aviation text understanding and processing. Our experimental results demonstrate the effectiveness of the proposed approach and offer valuable insights for automated NOTAM analysis systems. Our code is available at: https://github.com/Estrellajer/Knots.

</details>


### [678] [Improving Direct Persian-English Speech-to-Speech Translation with Discrete Units and Synthetic Parallel Data](https://arxiv.org/abs/2511.12690)
*Sina Rashidi,Hossein Sameti*

Main category: cs.CL

TL;DR: 本文提出波斯语到英语的直接语音到语音翻译系统及合成平行语料管道，结合自监督预训练、离散语音单元和合成数据提升低资源语言对翻译效果。


<details>
  <summary>Details</summary>
Motivation: 直接语音到语音翻译模型需要大量平行语音数据，低资源语言如波斯语缺乏此类数据。

Method: 构建包含基于Conformer编码器、因果Transformer解码器和基于单元的神经声码器的模型；用大语言模型翻译波斯语音转录并合成英语语音构建新平行语料。

Result: 新语料使可用平行语音量增加约六倍，在CVSS语料库的波斯 - 英语部分，模型比直接基线提升4.6 ASR BLEU。

Conclusion: 结合自监督预训练、离散语音单元和合成平行数据对提升波斯 - 英语等低资源语言对的直接语音到语音翻译有效。

Abstract: Direct speech-to-speech translation (S2ST), in which all components are trained jointly, is an attractive alternative to cascaded systems because it offers a simpler pipeline and lower inference latency. However, direct S2ST models require large amounts of parallel speech data in the source and target languages, which are rarely available for low-resource languages such as Persian. This paper presents a direct S2ST system for translating Persian speech into English speech, as well as a pipeline for synthetic parallel Persian-English speech generation. The model comprises three components: (1) a conformer-based encoder, initialized from self-supervised pre-training, maps source speech to high-level acoustic representations; (2) a causal transformer decoder with relative position multi-head attention translates these representations into discrete target speech units; (3) a unit-based neural vocoder generates waveforms from the predicted discrete units. To mitigate the data scarcity problem, we construct a new Persian-English parallel speech corpus by translating Persian speech transcriptions into English using a large language model and then synthesizing the corresponding English speech with a state-of-the-art zero-shot text-to-speech system. The resulting corpus increases the amount of available parallel speech by roughly a factor of six. On the Persian-English portion of the CVSS corpus, the proposed model achieves improvement of 4.6 ASR BLEU with the synthetic data over direct baselines. These results indicate that combining self-supervised pre-training, discrete speech units, and synthetic parallel data is effective for improving direct S2ST in low-resource language pairs such as Persian-English

</details>


### [679] [Adaptive Focus Memory for Language Models](https://arxiv.org/abs/2511.12712)
*Christopher Cruz*

Main category: cs.CL

TL;DR: 提出自适应聚焦记忆（AFM）动态上下文管理器，可在多轮对话中降低推理成本且不牺牲安全性和事实连续性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多轮对话中受固定上下文窗口和简单记忆策略限制，现有方法存在成本高或丢失关键信息问题。

Method: AFM根据语义相似度、半衰期近期加权和重要性分类为过往消息分配三种保真度级别，在严格的令牌预算下按时间顺序打包消息。

Result: 在安全导向基准测试中，AFM能保留关键信息，安全性能与简单重放相当，平均令牌使用量比重放基线减少66%。

Conclusion: 发布了适用于OpenAI兼容API和离线操作的模块化Python实现，可帮助从业者降低推理成本。

Abstract: Large language models (LLMs) are increasingly deployed in multi-turn dialogue settings, but their behavior is still bottlenecked by fixed context windows and naive memory strategies. Replaying the full conversation at every turn is simple but expensive, while static summarization or recency-only heuristics often erase safety-critical user details. We present Adaptive Focus Memory (AFM), a dynamic context manager that assigns each past message one of three fidelity levels -- FULL, COMPRESSED, or PLACEHOLDER -- based on semantic similarity to the current query, half-life recency weighting, and importance classification. AFM packs messages chronologically under a strict token budget, preferring high fidelity for the most relevant turns while aiming to preserve a cheap trace of the dialogue. In a safety-oriented benchmark involving a user with a severe peanut allergy planning a trip to Thailand, AFM retains the allergy across both short and medium-length conversations, matches the safety performance of naive replay, and cuts average token usage by 66% relative to a replay baseline. We release a modular Python implementation of AFM designed for OpenAI-compatible APIs and offline operation, enabling practitioners to reduce inference cost without sacrificing safety or factual continuity in the evaluated scenario.

</details>


### [680] [Evidence of Phase Transitions in Small Transformer-Based Language Models](https://arxiv.org/abs/2511.12768)
*Noah Hong,Tao Hong*

Main category: cs.CL

TL;DR: 研究小模型训练中是否存在相变，通过训练小GPT式变压器分析词汇使用演变，发现相变在小模型训练早期、线性训练空间中也存在。


<details>
  <summary>Details</summary>
Motivation: 探究相变是否仅存在于大模型、能否在线性训练空间直接检测以及是否在训练早期出现。

Method: 在字符级语料库上训练小GPT式变压器，分析词汇使用演变，应用泊松和亚泊松统计量化词汇连接和重组。

Result: 发现训练中有明显相变点，标准损失或验证曲线中不明显，通过基于词汇和统计的探测可见。

Conclusion: 相变重组是语言模型训练的普遍特征，为语言模型训练的非线性动力学提供新见解，强调定制指标的重要性。

Abstract: Phase transitions have been proposed as the origin of emergent abilities in large language models (LLMs), where new capabilities appear abruptly once models surpass critical thresholds of scale. Prior work, such as that of Wei et al., demonstrated these phenomena under model and data scaling, with transitions revealed after applying a log scale to training compute. In this work, we ask three complementary questions: (1) Are phase transitions unique to large models, or can they also be observed in small transformer-based language models? (2) Can such transitions be detected directly in linear training space, rather than only after log rescaling? and (3) Can these transitions emerge at early stages of training? To investigate, we train a small GPT-style transformer on a character-level corpus and analyze the evolution of vocabulary usage throughout training. We track the average word length, the number of correct versus incorrect words, and shifts in vocabulary diversity. Building on these measures, we apply Poisson and sub-Poisson statistics to quantify how words connect and reorganize. This combined analysis reveals a distinct transition point during training. Notably, these transitions are not apparent in standard loss or validation curves, but become visible through our vocabulary- and statistics-based probes. Our findings suggest that phase-transition reorganizations are a general feature of language model training, observable even in modest models, detectable directly in linear training space, and occurring surprisingly early as coherence emerges. This perspective provides new insight into the nonlinear dynamics of language model training and underscores the importance of tailored metrics for uncovering phase transition behaviors

</details>


### [681] [From Passive to Persuasive: Steering Emotional Nuance in Human-AI Negotiation](https://arxiv.org/abs/2511.12832)
*Niranjan Chebrolu,Gerard Christopher Yeo,Kokil Jaidka*

Main category: cs.CL

TL;DR: 本文展示了通过目标激活工程使LLaMA 3.1 - 8B展现更人性化情感细微差别，为对话式AI研究提供新方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽对话流畅，但赋予其类人情感表达是挑战，现有对齐技术有局限。

Method: 先使用归因修补识别有因果影响的组件，找到关键干预位点；再从对比文本对的激活差异中导出情感表达向量。

Result: 将情感表达向量应用于新对话提示，增强了情感特征，使回复积极情绪增加、第一人称代词使用更频繁。

Conclusion: 研究提供了精准且可解释的框架和对话式AI研究新方向。

Abstract: Large Language Models (LLMs) demonstrate increasing conversational fluency, yet instilling them with nuanced, human-like emotional expression remains a significant challenge. Current alignment techniques often address surface-level output or require extensive fine-tuning. This paper demonstrates that targeted activation engineering can steer LLaMA 3.1-8B to exhibit more human-like emotional nuances. We first employ attribution patching to identify causally influential components, to find a key intervention locus by observing activation patterns during diagnostic conversational tasks. We then derive emotional expression vectors from the difference in the activations generated by contrastive text pairs (positive vs. negative examples of target emotions). Applying these vectors to new conversational prompts significantly enhances emotional characteristics: steered responses show increased positive sentiment (e.g., joy, trust) and more frequent first-person pronoun usage, indicative of greater personal engagement. Our findings offer a precise and interpretable framework and new directions for the study of conversational AI.

</details>


### [682] [NeuroLex: A Lightweight Domain Language Model for EEG Report Understanding and Generation](https://arxiv.org/abs/2511.12851)
*Kang Yin,Hye-Bin Shin*

Main category: cs.CL

TL;DR: 介绍轻量级领域自适应语言模型NeuroLex，在EEG报告上训练，表现优于通用模型，可桥接生物医学文本建模与脑机接口应用。


<details>
  <summary>Details</summary>
Motivation: 通用语言模型无法捕捉临床脑电图（EEG）报告中的特定领域语言约定。

Method: 在哈佛脑电图数据库的EEG报告文本上训练，采用跨度损坏预训练和指令式微调，涉及报告润色、段落总结和术语问答。

Result: 与同规模通用模型相比，实现更低困惑度、更高提取和总结准确率、更好标签效率，对否定和事实幻觉有更强鲁棒性。

Conclusion: NeuroLex凭借EEG感知语言骨干，为可解释和语言驱动的神经解码提供基础，桥接生物医学文本建模和脑机接口应用。

Abstract: Clinical electroencephalogram (EEG) reports encode domain-specific linguistic conventions that general-purpose language models (LMs) fail to capture. We introduce NeuroLex, a lightweight domain-adaptive language model trained purely on EEG report text from the Harvard Electroencephalography Database. Unlike existing biomedical LMs, NeuroLex is tailored to the linguistic and diagnostic characteristics of EEG reporting, enabling it to serve as both an independent textual model and a decoder backbone for multimodal EEG-language systems. Using span-corruption pretraining and instruction-style fine-tuning on report polishing, paragraph summarization, and terminology question answering, NeuroLex learns the syntax and reasoning patterns characteristic of EEG interpretation. Comprehensive evaluations show that it achieves lower perplexity, higher extraction and summarization accuracy, better label efficiency, and improved robustness to negation and factual hallucination compared with general models of the same scale. With an EEG-aware linguistic backbone, NeuroLex bridges biomedical text modeling and brain-computer interface applications, offering a foundation for interpretable and language-driven neural decoding.

</details>


### [683] [Classification of Hope in Textual Data using Transformer-Based Models](https://arxiv.org/abs/2511.12874)
*Chukwuebuka Fortunate Ijezue,Tania-Amanda Fredrick Eneye,Maaz Amjad*

Main category: cs.CL

TL;DR: 本文用基于Transformer的方法对文本中的希望表达分类，比较BERT、GPT - 2和DeBERTa，BERT性能优且省资源，研究为希望的计算分析提供框架。


<details>
  <summary>Details</summary>
Motivation: 对文本中的希望表达进行分类，为希望的计算分析提供框架，应用于心理健康和社交媒体分析。

Method: 开发并比较BERT、GPT - 2和DeBERTa三种架构进行二分类和多分类。

Result: 初始BERT二分类和多分类准确率分别为83.65%和74.87%，扩展比较中BERT性能优且省资源，GPT - 2准确率最低，DeBERTa结果中等但计算成本高，GPT - 2在讽刺检测上召回率达92.46%。

Conclusion: 为希望的计算分析提供框架，在专业情感检测任务中架构适用性可能比模型大小更重要。

Abstract: This paper presents a transformer-based approach for classifying hope expressions in text. We developed and compared three architectures (BERT, GPT-2, and DeBERTa) for both binary classification (Hope vs. Not Hope) and multiclass categorization (five hope-related categories). Our initial BERT implementation achieved 83.65% binary and 74.87% multiclass accuracy. In the extended comparison, BERT demonstrated superior performance (84.49% binary, 72.03% multiclass accuracy) while requiring significantly fewer computational resources (443s vs. 704s training time) than newer architectures. GPT-2 showed lowest overall accuracy (79.34% binary, 71.29% multiclass), while DeBERTa achieved moderate results (80.70% binary, 71.56% multiclass) but at substantially higher computational cost (947s for multiclass training). Error analysis revealed architecture-specific strengths in detecting nuanced hope expressions, with GPT-2 excelling at sarcasm detection (92.46% recall). This study provides a framework for computational analysis of hope, with applications in mental health and social media analysis, while demonstrating that architectural suitability may outweigh model size for specialized emotion detection tasks.

</details>


### [684] [ClinStructor: AI-Powered Structuring of Unstructured Clinical Texts](https://arxiv.org/abs/2511.11883)
*Karthikeyan K,Raghuveer Thirukovalluru,David Carlson*

Main category: cs.CL

TL;DR: 提出ClinStructor将临床自由文本转换为结构化问答对，提升模型性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决临床笔记非结构化格式带来的偏差、泛化性差和可解释性差等问题。

Method: 使用大语言模型（LLMs）将临床自由文本转换为结构化、特定任务的问答对。

Result: 在ICU死亡率预测任务中，相比直接微调，仅使预测性能（AUC）下降2 - 3%。

Conclusion: ClinStructor为临床环境构建可靠、可解释和可泛化的机器学习模型奠定基础。

Abstract: Clinical notes contain valuable, context-rich information, but their unstructured format introduces several challenges, including unintended biases (e.g., gender or racial bias), and poor generalization across clinical settings (e.g., models trained on one EHR system may perform poorly on another due to format differences) and poor interpretability. To address these issues, we present ClinStructor, a pipeline that leverages large language models (LLMs) to convert clinical free-text into structured, task-specific question-answer pairs prior to predictive modeling. Our method substantially enhances transparency and controllability and only leads to a modest reduction in predictive performance (a 2-3% drop in AUC), compared to direct fine-tuning, on the ICU mortality prediction task. ClinStructor lays a strong foundation for building reliable, interpretable, and generalizable machine learning models in clinical environments.

</details>


### [685] [Additive Large Language Models for Semi-Structured Text](https://arxiv.org/abs/2511.11922)
*Karthikeyan K,Raghuveer Thirukovalluru,David Carlson*

Main category: cs.CL

TL;DR: 提出可解释框架CALM用于半结构化文本分类，性能与传统LLM分类器相当且能提高信任度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在临床文本分类中预测不透明，阻碍在研究和临床环境中的实际应用，需要理解患者记录中哪些部分驱动风险信号。

Method: 引入CALM框架，将输入视为有语义意义的组件，预测结果为各组件贡献的累加和，可实现患者和群体层面的解释。

Result: CALM性能与传统LLM分类器相当，能提高信任度，支持质量保证检查，在模型开发和审计中揭示临床有意义的模式。

Conclusion: CALM在半结构化文本分类中具有优势，可在临床文本分类中发挥作用。

Abstract: Large Language Models have advanced clinical text classification, but their opaque predictions remain a critical barrier to practical adoption in research and clinical settings where investigators and physicians need to understand which parts of a patient's record drive risk signals. To address this challenge, we introduce \textbf{CALM}, short for \textbf{Classification with Additive Large Language Models}, an interpretable framework for semi-structured text where inputs are composed of semantically meaningful components, such as sections of an admission note or question-answer fields from an intake form. CALM predicts outcomes as the additive sum of each component's contribution, making these contributions part of the forward computation itself and enabling faithful explanations at both the patient and population level. The additive structure also enables clear visualizations, such as component-level risk curves similar to those used in generalized additive models, making the learned relationships easier to inspect and communicate. Although CALM expects semi-structured inputs, many clinical documents already have this form, and similar structure can often be automatically extracted from free-text notes. CALM achieves performance comparable to conventional LLM classifiers while improving trust, supporting quality-assurance checks, and revealing clinically meaningful patterns during model development and auditing.

</details>


### [686] [InData: Towards Secure Multi-Step, Tool-Based Data Analysis](https://arxiv.org/abs/2511.11933)
*Karthikeyan K,Raghuveer Thirukovalluru,Bhuwan Dhingra,David Edwin Carlson*

Main category: cs.CL

TL;DR: 提出限制大语言模型直接生成代码和访问数据的安全方案，引入数据集InData评估其多步工具推理能力，发现当前模型在困难任务上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型代理数据分析在处理敏感数据时的安全风险，弥补现有工具使用基准对复杂数据分析多步推理评估的不足。

Method: 提出限制模型直接操作的安全方案，引入InData数据集，对15个开源大语言模型进行基准测试。

Result: 大模型在简单任务上准确率高（如gpt - oss - 120b在简单任务达97.3%），但在困难任务上性能大幅下降（69.6%）。

Conclusion: 当前大语言模型仍缺乏强大的多步工具推理能力，InData有助于开发和评估具有更强多步工具使用能力的模型，将公开数据集和代码。

Abstract: Large language model agents for data analysis typically generate and execute code directly on databases. However, when applied to sensitive data, this approach poses significant security risks. To address this issue, we propose a security-motivated alternative: restrict LLMs from direct code generation and data access, and require them to interact with data exclusively through a predefined set of secure, verified tools. Although recent tool-use benchmarks exist, they primarily target tool selection and simple execution rather than the compositional, multi-step reasoning needed for complex data analysis. To reduce this gap, we introduce Indirect Data Engagement (InData), a dataset designed to assess LLMs' multi-step tool-based reasoning ability. InData includes data analysis questions at three difficulty levels--Easy, Medium, and Hard--capturing increasing reasoning complexity. We benchmark 15 open-source LLMs on InData and find that while large models (e.g., gpt-oss-120b) achieve high accuracy on Easy tasks (97.3%), performance drops sharply on Hard tasks (69.6%). These results show that current LLMs still lack robust multi-step tool-based reasoning ability. With InData, we take a step toward enabling the development and evaluation of LLMs with stronger multi-step tool-use capabilities. We will publicly release the dataset and code.

</details>


### [687] [Improving LLM's Attachment to External Knowledge In Dialogue Generation Tasks Through Entity Anonymization](https://arxiv.org/abs/2511.11946)
*Hadi Sheikhi,Chenyang Huang,Osmar R. Zaïane*

Main category: cs.CL

TL;DR: 提出评估程序LLM - KAT和实体匿名化技术，提升大语言模型在知识图谱对话生成中对外部知识的利用。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在知识图谱对话生成任务中对外部知识利用能力未充分探索，常依赖内部知识，与给定知识图谱脱节。

Method: 引入评估程序LLM - KAT测量生成回复中的知识附着情况，提出实体匿名化技术。

Result: 在OpenDialKG数据集上的实验表明该方法提升了大语言模型对外部知识的附着。

Conclusion: 所提方法能有效提升大语言模型在知识图谱对话生成中对外部知识的利用。

Abstract: Knowledge graph-based dialogue generation (KG-DG) is a challenging task requiring models to effectively incorporate external knowledge into conversational responses. While large language models (LLMs) have achieved impressive results across various NLP tasks, their ability to utilize external knowledge in KG-DG remains under-explored. We observe that LLMs often rely on internal knowledge, leading to detachment from provided knowledge graphs, even when they are given a flawlessly retrieved knowledge graph. First, we introduce LLM-KAT, an evaluation procedure for measuring knowledge attachment in generated responses. Second, we propose a simple yet effective entity anonymization technique to encourage LLMs to better leverage external knowledge. Experiments on the OpenDialKG dataset demonstrate that our approach improves LLMs' attachment on external knowledge.

</details>


### [688] [AA-Omniscience: Evaluating Cross-Domain Knowledge Reliability in Large Language Models](https://arxiv.org/abs/2511.13029)
*Declan Jackson,William Keating,George Cameron,Micah Hill-Smith*

Main category: cs.CL

TL;DR: 介绍AA - Omniscience基准，测量模型事实召回和知识校准，评估显示前沿模型存在弱点，不同领域表现有差异，应按需选模型。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型评估主要测通用能力，可靠使用需事实准确性和知识差距识别，故引入新基准。

Method: 引入AA - Omniscience基准，用6000个来自权威来源、涵盖6个领域42个主题的问题，测量模型的全知指数。

Result: Claude 4.1 Opus得分最高，前沿模型存在事实性和校准弱点，不同领域表现不同。

Conclusion: 在知识重要的任务中，应根据用例需求而非通用性能选择模型。

Abstract: Existing language model evaluations primarily measure general capabilities, yet reliable use of these models across a range of domains demands factual accuracy and recognition of knowledge gaps. We introduce AA-Omniscience, a benchmark designed to measure both factual recall and knowledge calibration across 6,000 questions. Questions are derived from authoritative academic and industry sources, and cover 42 economically relevant topics within six different domains. The evaluation measures a model's Omniscience Index, a bounded metric (-100 to 100) measuring factual recall that jointly penalizes hallucinations and rewards abstention when uncertain, with 0 equating to a model that answers questions correctly as much as it does incorrectly. Among evaluated models, Claude 4.1 Opus attains the highest score (4.8), making it one of only three models to score above zero. These results reveal persistent factuality and calibration weaknesses across frontier models. Performance also varies by domain, with the models from three different research labs leading across the six domains. This performance variability suggests models should be chosen according to the demands of the use case rather than general performance for tasks where knowledge is important.

</details>


### [689] [Extracting Events Like Code: A Multi-Agent Programming Framework for Zero-Shot Event Extraction](https://arxiv.org/abs/2511.13118)
*Quanjiang Guo,Sijie Wang,Jinchuan Zhang,Ben Zhang,Zhao Kang,Ling Tian,Ke Yan*

Main category: cs.CL

TL;DR: 提出多智能体框架Agent - Event - Coder (AEC)处理零样本事件提取，实验显示其优于先前基线。


<details>
  <summary>Details</summary>
Motivation: 零样本事件提取对大语言模型是挑战，直接提示输出常不完整或结构无效。

Method: 将事件提取视为软件工程的结构化、迭代代码生成过程，分解为检索、规划、编码和验证子任务，由专门的大语言模型智能体处理。

Result: 在五个不同领域和六个大语言模型上的实验表明，AEC始终优于先前的零样本基线。

Conclusion: 将事件提取视为代码生成的方法有效，AEC能使大语言模型在零样本设置下产生精确、完整且符合模式的提取结果。

Abstract: Zero-shot event extraction (ZSEE) remains a significant challenge for large language models (LLMs) due to the need for complex reasoning and domain-specific understanding. Direct prompting often yields incomplete or structurally invalid outputs--such as misclassified triggers, missing arguments, and schema violations. To address these limitations, we present Agent-Event-Coder (AEC), a novel multi-agent framework that treats event extraction like software engineering: as a structured, iterative code-generation process. AEC decomposes ZSEE into specialized subtasks--retrieval, planning, coding, and verification--each handled by a dedicated LLM agent. Event schemas are represented as executable class definitions, enabling deterministic validation and precise feedback via a verification agent. This programming-inspired approach allows for systematic disambiguation and schema enforcement through iterative refinement. By leveraging collaborative agent workflows, AEC enables LLMs to produce precise, complete, and schema-consistent extractions in zero-shot settings. Experiments across five diverse domains and six LLMs demonstrate that AEC consistently outperforms prior zero-shot baselines, showcasing the power of treating event extraction like code generation. The code and data are released on https://github.com/UESTC-GQJ/Agent-Event-Coder.

</details>


### [690] [Cmprsr: Abstractive Token-Level Question-Agnostic Prompt Compressor](https://arxiv.org/abs/2511.12281)
*Ivan Zakazov,Alexander Sharipov,Berke Argin,Oussama Gabouj,Kamel Charaf,Alexi Semiz,Lorenzo Drudi,Nicolas Baldwin,Robert West*

Main category: cs.CL

TL;DR: 为降低使用大语言模型成本，引入提示压缩范式，进行模型压缩能力基准测试，改进gpt - 4.1 - mini，训练模型Cmprsr并证明其优势。


<details>
  <summary>Details</summary>
Motivation: 使用黑盒大语言模型成本高，需新的提示压缩范式来降低成本。

Method: 开展LLM - as - a - compressor基准测试；用基于Textgrad的压缩元提示优化改进gpt - 4.1 - mini；结合监督微调（SFT）和组相对策略优化（GRPO）对Qwen3 - 4B进行后训练得到Cmprsr。

Result: Cmprsr在长输入和短提示上优于提取式和普通抽象式压缩，能很好遵循压缩率要求。

Conclusion: Cmprsr具有跨输入长度和领域的泛化能力，可精细控制成本 - 质量权衡。

Abstract: Motivated by the high costs of using black-box Large Language Models (LLMs), we introduce a novel prompt compression paradigm, under which we use smaller LLMs to compress inputs for the larger ones. We present the first comprehensive LLM-as-a-compressor benchmark spanning 25 open- and closed-source models, which reveals significant disparity in models' compression ability in terms of (i) preserving semantically important information (ii) following the user-provided compression rate (CR). We further improve the performance of gpt-4.1-mini, the best overall vanilla compressor, with Textgrad-based compression meta-prompt optimization. We also identify the most promising open-source vanilla LLM - Qwen3-4B - and post-train it with a combination of supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO), pursuing the dual objective of CR adherence and maximizing the downstream task performance. We call the resulting model Cmprsr and demonstrate its superiority over both extractive and vanilla abstractive compression across the entire range of compression rates on lengthy inputs from MeetingBank and LongBench as well as short prompts from GSM8k. The latter highlights Cmprsr's generalizability across varying input lengths and domains. Moreover, Cmprsr closely follows the requested compression rate, offering fine control over the cost-quality trade-off.

</details>


### [691] [AHaSIS: Shared Task on Sentiment Analysis for Arabic Dialects](https://arxiv.org/abs/2511.13335)
*Maram Alharbi,Salmane Chafik,Saad Ezzini,Ruslan Mitkov,Tharindu Ranasinghe,Hansi Hettiarachchi*

Main category: cs.CL

TL;DR: 阿拉伯世界酒店业需阿拉伯语情感分析工具，相关共享任务用多方言数据集开展研究，超40队参与，最佳系统F1得分0.81。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯世界酒店业依赖客户反馈塑造服务，需要先进的阿拉伯语情感分析工具。

Method: 开展阿拉伯方言情感分析共享任务，利用多方言、人工整理的酒店评论数据集，翻译经母语者验证。

Result: 超40队注册，12队在评估阶段提交系统，最佳系统F1得分0.81。

Conclusion: 证明跨阿拉伯方言情感分析可行，但仍存在挑战。

Abstract: The hospitality industry in the Arab world increasingly relies on customer feedback to shape services, driving the need for advanced Arabic sentiment analysis tools. To address this challenge, the Sentiment Analysis on Arabic Dialects in the Hospitality Domain shared task focuses on Sentiment Detection in Arabic Dialects. This task leverages a multi-dialect, manually curated dataset derived from hotel reviews originally written in Modern Standard Arabic (MSA) and translated into Saudi and Moroccan (Darija) dialects. The dataset consists of 538 sentiment-balanced reviews spanning positive, neutral, and negative categories. Translations were validated by native speakers to ensure dialectal accuracy and sentiment preservation. This resource supports the development of dialect-aware NLP systems for real-world applications in customer experience analysis. More than 40 teams have registered for the shared task, with 12 submitting systems during the evaluation phase. The top-performing system achieved an F1 score of 0.81, demonstrating the feasibility and ongoing challenges of sentiment analysis across Arabic dialects.

</details>


### [692] [Donors and Recipients: On Asymmetric Transfer Across Tasks and Languages with Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2511.13368)
*Kajetan Dymkiewicz,Ivan Vulic,Helen Yannakoudakis,Eilam Shapira,Roi Reichart,Anna Korhonen*

Main category: cs.CL

TL;DR: 研究多开放权重LLM跨任务和语言的迁移情况，发现两种通用模式并给出调优和模型专业化建议。


<details>
  <summary>Details</summary>
Motivation: 当前对LLM在一个任务或语言上的改进如何影响其他任务、语言及其组合了解不足。

Method: 对多个开放权重LLM进行PEFT/LoRA研究，以任务和语言为迁移轴，在单一任务 - 语言源上微调模型并测量迁移效果，分解迁移模式。

Result: 发现两种通用模式，一是任务匹配与不匹配的不对称性，二是跨语言和任务的稳定捐赠 - 接收结构。

Conclusion: 为风险感知微调及模型专业化提供了启示。

Abstract: Large language models (LLMs) perform strongly across tasks and languages, yet how improvements in one task or language affect other tasks and languages and their combinations remains poorly understood. We conduct a controlled PEFT/LoRA study across multiple open-weight LLM families and sizes, treating task and language as transfer axes while conditioning on model family and size; we fine-tune each model on a single task-language source and measure transfer as the percentage-point change versus its baseline score when evaluated on all other task-language target pairs. We decompose transfer into (i) Matched-Task (Cross-Language), (ii) Matched-Language (Cross-Task), and (iii) Cross-Task (Cross-Language) regimes. We uncover two consistent general patterns. First, a pronounced on-task vs. off-task asymmetry: Matched-Task (Cross-Language) transfer is reliably positive, whereas off-task transfer often incurs collateral degradation. Second, a stable donor-recipient structure across languages and tasks (hub donors vs. brittle recipients). We outline implications for risk-aware fine-tuning and model specialisation.

</details>


### [693] [Toward Conversational Hungarian Speech Recognition: Introducing the BEA-Large and BEA-Dialogue Datasets](https://arxiv.org/abs/2511.13529)
*Máté Gedeon,Piroska Zsófia Barta,Péter Mihajlik,Tekla Etelka Gráczi,Anna Kohári,Katalin Mády*

Main category: cs.CL

TL;DR: 本文引入匈牙利语语音数据集BEA - Large和BEA - Dialogue，建立基线模型评估，指出对话式ASR的挑战，旨在推动匈牙利语音技术发展并为其他语言提供方法框架。


<details>
  <summary>Details</summary>
Motivation: 高资源语言的大量数据集推动了自动语音识别（ASR）发展，而匈牙利语因自发和对话语料有限而发展不足，需构建新数据集。

Method: 从匈牙利语BEA语料库未处理部分构建BEA - Large和BEA - Dialogue数据集，用公开ASR模型建立基线。

Result: 微调后的Fast Conformer模型在自发语音上的字错误率低至14.18%，重复语音为4.8%；日记化实验的错误率在13.05% - 18.26%之间。

Conclusion: 发布数据集和基线可推动匈牙利语音技术发展，为其他语言开发自发和对话基准提供方法框架，对话式ASR仍面临挑战。

Abstract: The advancement of automatic speech recognition (ASR) has been largely enhanced by extensive datasets in high-resource languages, while languages such as Hungarian remain underrepresented due to limited spontaneous and conversational corpora. To address this gap, we introduce two new datasets -- BEA-Large and BEA-Dialogue -- constructed from the previously unprocessed portions of the Hungarian speech corpus named BEA. BEA-Large extends BEA-Base with 255 hours of spontaneous speech from 433 speakers, enriched with detailed segment-level metadata. BEA-Dialogue, comprising 85 hours of spontaneous conversations, is a Hungarian speech corpus featuring natural dialogues partitioned into speaker-independent subsets, supporting research in conversational ASR and speaker diarization. We establish reproducible baselines on these datasets using publicly available ASR models, with the fine-tuned Fast Conformer model achieving word error rates as low as 14.18\% on spontaneous and 4.8\% on repeated speech. Diarization experiments yield diarization error rates between 13.05\% and 18.26\%, providing reference points for future improvements. The results highlight the persistent difficulty of conversational ASR, particularly due to disfluencies, overlaps, and informal speech patterns. By releasing these datasets and baselines, we aim to advance Hungarian speech technology and offer a methodological framework for developing spontaneous and conversational benchmarks in other languages.

</details>


### [694] [Beyond SELECT: A Comprehensive Taxonomy-Guided Benchmark for Real-World Text-to-SQL Translation](https://arxiv.org/abs/2511.13590)
*Hao Wang,Yuanfeng Song,Xiaoming Yin,Xing Chen*

Main category: cs.CL

TL;DR: 提出文本到SQL分类新分类法，评估现有数据集，合成新数据集SQL - Synth，验证分类法有效性，发现现有LLMs不足及微调可提升性能，分类法有重要潜在影响。


<details>
  <summary>Details</summary>
Motivation: 现有文本到SQL数据集覆盖范围有限，无法体现现实应用多样性。

Method: 提出基于核心意图、语句类型、语法结构和关键动作的分类法，用其评估现有数据集，结合分类法和大语言模型构建数据集合成管道生成SQL - Synth。

Result: SQL - Synth比现有基准有更大的多样性和覆盖范围，现有LLMs在SQL - Synth上表现有限，微调可提升性能。

Conclusion: 提出的分类法能全面分析数据集和LLMs性能，可指导LLMs训练数据构建。

Abstract: Text-to-SQL datasets are essential for training and evaluating text-to-SQL models, but existing datasets often suffer from limited coverage and fail to capture the diversity of real-world applications. To address this, we propose a novel taxonomy for text-to-SQL classification based on dimensions including core intents, statement types, syntax structures, and key actions. Using this taxonomy, we evaluate widely used public text-to-SQL datasets (e.g., Spider and Bird) and reveal limitations in their coverage and diversity. We then introduce a taxonomy-guided dataset synthesis pipeline, yielding a new dataset named SQL-Synth. This approach combines the taxonomy with Large Language Models (LLMs) to ensure the dataset reflects the breadth and complexity of real-world text-to-SQL applications. Extensive analysis and experimental results validate the effectiveness of our taxonomy, as SQL-Synth exhibits greater diversity and coverage compared to existing benchmarks. Moreover, we uncover that existing LLMs typically fall short in adequately capturing the full range of scenarios, resulting in limited performance on SQL-Synth. However, fine-tuning can substantially improve their performance in these scenarios. The proposed taxonomy has significant potential impact, as it not only enables comprehensive analysis of datasets and the performance of different LLMs, but also guides the construction of training data for LLMs.

</details>


### [695] [Generalist Foundation Models Are Not Clinical Enough for Hospital Operations](https://arxiv.org/abs/2511.13703)
*Lavender Y. Jiang,Angelica Chen,Xu Han,Xujin Chris Liu,Radhika Dua,Kevin Eaton,Frederick Wolff,Robert Steele,Jeff Zhang,Anton Alyakin,Qingkai Pan,Yanbing Chen,Karl L. Sangwon,Daniel A. Alber,Jaden Stryker,Jin Vivian Lee,Yindalon Aphinyanaphongs,Kyunghyun Cho,Eric Karl Oermann*

Main category: cs.CL

TL;DR: 介绍了Lang1模型家族，经专门语料预训练，开发ReMedE基准评估，微调后表现出色，证明医院运营预测需监督微调及领域内预训练。


<details>
  <summary>Details</summary>
Motivation: 通用基础模型缺乏医疗运营决策所需专业知识，需开发专门模型。

Method: 用包含临床和互联网数据的专门语料预训练Lang1模型家族，开发ReMedE基准评估模型在五项关键任务上的表现。

Result: 零样本设置下模型表现不佳，微调后Lang1 - 1B表现优于大得多的通用模型，联合微调有跨任务提升，能迁移到不同设置。

Conclusion: 医院运营预测需明确监督微调，领域内预训练可提高效率，专业大语言模型在专业任务可与通用模型竞争，有效医疗系统AI需结合领域内预训练、监督微调及真实世界评估。

Abstract: Hospitals and healthcare systems rely on operational decisions that determine patient flow, cost, and quality of care. Despite strong performance on medical knowledge and conversational benchmarks, foundation models trained on general text may lack the specialized knowledge required for these operational decisions. We introduce Lang1, a family of models (100M-7B parameters) pretrained on a specialized corpus blending 80B clinical tokens from NYU Langone Health's EHRs and 627B tokens from the internet. To rigorously evaluate Lang1 in real-world settings, we developed the REalistic Medical Evaluation (ReMedE), a benchmark derived from 668,331 EHR notes that evaluates five critical tasks: 30-day readmission prediction, 30-day mortality prediction, length of stay, comorbidity coding, and predicting insurance claims denial. In zero-shot settings, both general-purpose and specialized models underperform on four of five tasks (36.6%-71.7% AUROC), with mortality prediction being an exception. After finetuning, Lang1-1B outperforms finetuned generalist models up to 70x larger and zero-shot models up to 671x larger, improving AUROC by 3.64%-6.75% and 1.66%-23.66% respectively. We also observed cross-task scaling with joint finetuning on multiple tasks leading to improvement on other tasks. Lang1-1B effectively transfers to out-of-distribution settings, including other clinical tasks and an external health system. Our findings suggest that predictive capabilities for hospital operations require explicit supervised finetuning, and that this finetuning process is made more efficient by in-domain pretraining on EHR. Our findings support the emerging view that specialized LLMs can compete with generalist models in specialized tasks, and show that effective healthcare systems AI requires the combination of in-domain pretraining, supervised finetuning, and real-world evaluation beyond proxy benchmarks.

</details>


### [696] [Why is "Chicago" Predictive of Deceptive Reviews? Using LLMs to Discover Language Phenomena from Lexical Cues](https://arxiv.org/abs/2511.13658)
*Jiaming Qu,Mengtian Guo,Yue Wang*

Main category: cs.CL

TL;DR: 探索用大语言模型将机器学习词汇线索转化为人类可理解语言现象以区分虚假和真实评论，成果有数据支撑、可跨领域泛化且更具预测性。


<details>
  <summary>Details</summary>
Motivation: 机器学习分类器区分虚假评论的特征难理解，需将其转化为人类可理解的语言现象。

Method: 使用大语言模型将机器学习的词汇线索转化为人类可理解的语言现象。

Result: 获得的语言现象有数据支撑、可跨领域泛化，比大语言模型先验知识或上下文学习得到的现象更具预测性。

Conclusion: 这些语言现象可在无欺骗检测分类器的环境中帮助人们评估在线评论可信度。

Abstract: Deceptive reviews mislead consumers, harm businesses, and undermine trust in online marketplaces. Machine learning classifiers can learn from large amounts of training examples to effectively distinguish deceptive reviews from genuine ones. However, the distinguishing features learned by these classifiers are often subtle, fragmented, and difficult for humans to interpret. In this work, we explore using large language models (LLMs) to translate machine-learned lexical cues into human-understandable language phenomena that can differentiate deceptive reviews from genuine ones. We show that language phenomena obtained in this manner are empirically grounded in data, generalizable across similar domains, and more predictive than phenomena either in LLMs' prior knowledge or obtained through in-context learning. These language phenomena have the potential to aid people in critically assessing the credibility of online reviews in environments where deception detection classifiers are unavailable.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [697] [Moving Pictures of Thought: Extracting Visual Knowledge in Charles S. Peirce's Manuscripts with Vision-Language Models](https://arxiv.org/abs/2511.13378)
*Carlo Teo Pedretti,Davide Picca,Dario Rodighiero*

Main category: cs.DL

TL;DR: 本文初步探究视觉语言模型（VLMs）能否助力识别和解读包含图表的混合页面，提出工作流程并结合皮尔斯符号学框架设计提示，将生成的说明集成到知识图谱。


<details>
  <summary>Details</summary>
Motivation: 图表虽重要但在视觉研究等方面存在障碍，皮尔斯手稿包含复杂图文，需研究VLMs能否帮助识别和解读这类混合页面。

Method: 提出页面布局分割、重连注释、提交图表片段到VLM的工作流程；采用皮尔斯符号学框架设计提示提取图表关键知识并生成说明；将说明集成到知识图谱。

Result: 未明确提及具体结果。

Conclusion: 未明确提及具体结论。

Abstract: Diagrams are crucial yet underexplored tools in many disciplines, demonstrating the close connection between visual representation and scholarly reasoning. However, their iconic form poses obstacles to visual studies, intermedial analysis, and text-based digital workflows. In particular, Charles S. Peirce consistently advocated the use of diagrams as essential for reasoning and explanation. His manuscripts, often combining textual content with complex visual artifacts, provide a challenging case for studying documents involving heterogeneous materials. In this preliminary study, we investigate whether Visual Language Models (VLMs) can effectively help us identify and interpret such hybrid pages in context. First, we propose a workflow that (i) segments manuscript page layouts, (ii) reconnects each segment to IIIF-compliant annotations, and (iii) submits fragments containing diagrams to a VLM. In addition, by adopting Peirce's semiotic framework, we designed prompts to extract key knowledge about diagrams and produce concise captions. Finally, we integrated these captions into knowledge graphs, enabling structured representations of diagrammatic content within composite sources.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [698] [Rethinking the filter bubble? Developing a research agenda for the protective filter bubble](https://arxiv.org/abs/2511.12873)
*Jacob Erickson*

Main category: cs.SI

TL;DR: 本文指出过滤气泡和回音室受广泛关注，多数研究聚焦其负面影响，本文通过文献回顾提出应重新审视过滤气泡并建议未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 多数研究关注过滤气泡负面影响，较少考虑其保护益处，尤其是对边缘化群体和新闻自由程度低国家的人群，因此有必要重新审视。

Method: 对数字安全空间和保护性过滤气泡相关文献进行回顾。

Result: 提出有必要重新思考过滤气泡。

Conclusion: 建议开展几个领域的未来研究。

Abstract: Filter bubbles and echo chambers have received global attention from scholars, media organizations, and the general public. Filter bubbles have primarily been regarded as intrinsically negative, and many studies have sought to minimize their influence. The detrimental influence of filter bubbles is well-studied. Filter bubbles may, for example, create information silos, amplify misinformation, and promote hatred and extremism. However, comparatively few studies have considered the other side of the filter bubble; its protective benefits, particularly to marginalized communities and those living in countries with low levels of press freedom. Through a review of the literature on digital safe spaces and protective filter bubbles, this commentary suggests that there may be a need to rethink the filter bubble, and it proposes several areas for future research.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [699] [PolicyBot - Reliable Question Answering over Policy Documents](https://arxiv.org/abs/2511.13489)
*Gautam Nagarajan,Omir Kumar,Sudarsun Santhiappan*

Main category: cs.ET

TL;DR: 本文介绍PolicyBot，一个专注透明度和可重复性的检索增强生成系统，用于回答政策文档相关问题，系统用开源工具构建，还强调了部署可信RAG系统的设计考量等。


<details>
  <summary>Details</summary>
Motivation: 政府法律和政策文档冗长复杂，公民难以定位和理解相关信息，需要一个系统来解决该问题。

Method: 结合特定领域语义分块、多语言密集嵌入、多阶段检索与重排序、源感知生成，实现引用追踪，评估不同检索和生成配置。

Result: 构建了完全使用开源工具的端到端管道，可轻松适应其他需要基于文档问答的领域。

Conclusion: 强调了在治理相关场景中部署可信RAG系统的设计考量、实际挑战和经验教训。

Abstract: All citizens of a country are affected by the laws and policies introduced by their government. These laws and policies serve essential functions for citizens. Such as granting them certain rights or imposing specific obligations. However, these documents are often lengthy, complex, and difficult to navigate, making it challenging for citizens to locate and understand relevant information. This work presents PolicyBot, a retrieval-augmented generation (RAG) system designed to answer user queries over policy documents with a focus on transparency and reproducibility. The system combines domain-specific semantic chunking, multilingual dense embeddings, multi-stage retrieval with reranking, and source-aware generation to provide responses grounded in the original documents. We implemented citation tracing to reduce hallucinations and improve user trust, and evaluated alternative retrieval and generation configurations to identify effective design choices. The end-to-end pipeline is built entirely with open-source tools, enabling easy adaptation to other domains requiring document-grounded question answering. This work highlights design considerations, practical challenges, and lessons learned in deploying trustworthy RAG systems for governance-related contexts.

</details>


### [700] [QPU Micro-Kernels for Stencil Computation](https://arxiv.org/abs/2511.12617)
*Stefano Markidis,Luca Pennati,Marco Pasquale,Gilbert Netzer,Ivy Peng*

Main category: cs.ET

TL;DR: 本文提出QPU微内核用于求解偏微分方程，介绍两种实现方式并在两类方程上测试验证。


<details>
  <summary>Details</summary>
Motivation: 寻找新的求解偏微分方程的方法，利用量子计算优势加速求解。

Method: 引入QPU微内核，包括Bernoulli微内核和分支微内核，将经典时间循环保留，仅卸载局部更新，通过批处理和电路内融合减少开销。

Result: 在无噪声量子电路模拟器上，精度随样本数增加而提高；在IBM Brisbane量子计算机上，Bernoulli实现在相同测量次数下误差更低，QPU微内核执行占主要时间。

Conclusion: QPU微内核方法可用于求解偏微分方程，在不同场景有一定效果和特点。

Abstract: We introduce QPU micro-kernels: shallow quantum circuits that perform a stencil node update and return a Monte Carlo estimate from repeated measurements. We show how to use them to solve Partial Differential Equations (PDEs) explicitly discretized on a computational stencil. From this point of view, the QPU serves as a sampling accelerator. Each micro-kernel consumes only stencil inputs (neighbor values and coefficients), runs a shallow parameterized circuit, and reports the sample mean of a readout rule. The resource footprint in qubits and depth is fixed and independent of the global grid. This makes micro-kernels easy to orchestrate from a classical host and to parallelize across grid points. We present two realizations. The Bernoulli micro-kernel targets convex-sum stencils by encoding values as single-qubit probabilities with shot allocation proportional to stencil weights. The branching micro-kernel prepares a selector over stencil branches and applies addressed rotations to a single readout qubit. In contrast to monolithic quantum PDE solvers that encode the full space-time problem in one deep circuit, our approach keeps the classical time loop and offloads only local updates. Batching and in-circuit fusion amortize submission and readout overheads. We test and validate the QPU micro-kernel method on two PDEs commonly arising in scientific computing: the Heat and viscous Burgers' equations. On noiseless quantum circuit simulators, accuracy improves as the number of samples increases. On the IBM Brisbane quantum computer, single-step diffusion tests show lower errors for the Bernoulli realization than for branching at equal shot budgets, with QPU micro-kernel execution dominating the wall time.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [701] [MovSemCL: Movement-Semantics Contrastive Learning for Trajectory Similarity](https://arxiv.org/abs/2511.12061)
*Zhichen Lai,Hua Lu,Huan Li,Jialiang Li,Christian S. Jensen*

Main category: cs.CV

TL;DR: 提出MovSemCL框架用于轨迹相似度计算，能解决现有方法的局限，实验表现优于SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的轨迹相似度计算方法存在对轨迹语义和层次建模不足、计算成本高、增强方式不合理等问题。

Method: MovSemCL先将原始GPS轨迹转换为运动语义特征并分割成块，用块内和块间注意力编码局部和全局模式，采用曲率引导的增强策略。

Result: 在真实数据集实验中，MovSemCL在相似度搜索任务上平均排名接近理想值1，启发式近似提升达20.3%，推理延迟最多降低43.4%。

Conclusion: MovSemCL能有效解决现有方法的局限，优于现有最先进方法。

Abstract: Trajectory similarity computation is fundamental functionality that is used for, e.g., clustering, prediction, and anomaly detection. However, existing learning-based methods exhibit three key limitations: (1) insufficient modeling of trajectory semantics and hierarchy, lacking both movement dynamics extraction and multi-scale structural representation; (2) high computational costs due to point-wise encoding; and (3) use of physically implausible augmentations that distort trajectory semantics. To address these issues, we propose MovSemCL, a movement-semantics contrastive learning framework for trajectory similarity computation. MovSemCL first transforms raw GPS trajectories into movement-semantics features and then segments them into patches. Next, MovSemCL employs intra- and inter-patch attentions to encode local as well as global trajectory patterns, enabling efficient hierarchical representation and reducing computational costs. Moreover, MovSemCL includes a curvature-guided augmentation strategy that preserves informative segments (e.g., turns and intersections) and masks redundant ones, generating physically plausible augmented views. Experiments on real-world datasets show that MovSemCL is capable of outperforming state-of-the-art methods, achieving mean ranks close to the ideal value of 1 at similarity search tasks and improvements by up to 20.3% at heuristic approximation, while reducing inference latency by up to 43.4%.

</details>


### [702] [MOON2.0: Dynamic Modality-balanced Multimodal Representation Learning for E-commerce Product Understanding](https://arxiv.org/abs/2511.12449)
*Zhanheng Nie,Chenghan Fu,Daoze Zhang,Junxian Wu,Wanxian Guan,Pengjie Wang,Jian Xu,Bo Zheng*

Main category: cs.CV

TL;DR: 提出MOON2.0框架解决电商多模态模型挑战，在多数据集表现优；引入MBE2.0基准。


<details>
  <summary>Details</summary>
Motivation: 电商发展需多模态模型，但现有多模态大语言模型在产品理解上有模态失衡、信息利用不足和数据噪声处理有限等问题。

Method: 提出MOON2.0框架，含模态驱动的混合专家模块、双级对齐方法和图像文本协同增强策略；引入MBE2.0基准。

Result: MOON2.0在MBE2.0和多个公共数据集上有最优零样本性能，注意力热力图显示其多模态对齐改善。

Conclusion: MOON2.0有效解决电商多模态模型面临的问题，具有良好的多模态表示学习能力。

Abstract: The rapid growth of e-commerce calls for multimodal models that comprehend rich visual and textual product information. Although recent multimodal large language models (MLLMs) for product understanding exhibit strong capability in representation learning for e-commerce, they still face three challenges: (i) the modality imbalance induced by modality mixed training; (ii) underutilization of the intrinsic alignment relationships among visual and textual information within a product; and (iii) limited handling of noise in e-commerce multimodal data. To address these, we propose MOON2.0, a dynamic modality-balanced multimodal representation learning framework for e-commerce product understanding. MOON2.0 comprises: (1) a Modality-driven Mixture-of-Experts (MoE) module that adaptively processes input samples by their modality composition, enabling Multimodal Joint Learning to mitigate the modality imbalance; (2) a Dual-level Alignment method to better leverage semantic alignment properties inside individual products; and (3) an MLLM-based Image-text Co-augmentation strategy that integrates textual enrichment with visual expansion, coupled with Dynamic Sample Filtering to improve training data quality. We further introduce MBE2.0, a co-augmented multimodal representation benchmark for e-commerce representation learning and evaluation. Experiments show that MOON2.0 delivers state-of-the-art zero-shot performance on MBE2.0 and multiple public datasets. Furthermore, attention-based heatmap visualization provides qualitative evidence of improved multimodal alignment of MOON2.0.

</details>


### [703] [FGNet: Leveraging Feature-Guided Attention to Refine SAM2 for 3D EM Neuron Segmentation](https://arxiv.org/abs/2511.13063)
*Zhenghua Li,Hang Chen,Zihao Sun,Kai Li,Xiaolin Hu*

Main category: cs.CV

TL;DR: 论文提出新框架将自然图像预训练的SAM2知识迁移到电子显微镜（EM）图像神经结构分割任务，实验表明该方法在冻结和微调SAM2权重下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有神经结构分割方法受复杂形态、低信噪比和标注稀缺限制，准确性和泛化性不足，需利用视觉基础模型在自然图像上学到的先验知识。

Method: 提出新框架，用SAM2提取特征，引入Feature - Guided Attention模块缩小领域差距，用双亲和性解码器生成亲和图。

Result: 冻结SAM2权重时性能与现有SOTA方法相当，在EM数据上微调后显著超越现有SOTA方法。

Conclusion: 将自然图像预训练表示与针对性领域自适应引导结合，可有效解决神经元分割中的特定挑战。

Abstract: Accurate segmentation of neural structures in Electron Microscopy (EM) images is paramount for neuroscience. However, this task is challenged by intricate morphologies, low signal-to-noise ratios, and scarce annotations, limiting the accuracy and generalization of existing methods. To address these challenges, we seek to leverage the priors learned by visual foundation models on a vast amount of natural images to better tackle this task. Specifically, we propose a novel framework that can effectively transfer knowledge from Segment Anything 2 (SAM2), which is pre-trained on natural images, to the EM domain. We first use SAM2 to extract powerful, general-purpose features. To bridge the domain gap, we introduce a Feature-Guided Attention module that leverages semantic cues from SAM2 to guide a lightweight encoder, the Fine-Grained Encoder (FGE), in focusing on these challenging regions. Finally, a dual-affinity decoder generates both coarse and refined affinity maps. Experimental results demonstrate that our method achieves performance comparable to state-of-the-art (SOTA) approaches with the SAM2 weights frozen. Upon further fine-tuning on EM data, our method significantly outperforms existing SOTA methods. This study validates that transferring representations pre-trained on natural images, when combined with targeted domain-adaptive guidance, can effectively address the specific challenges in neuron segmentation.

</details>


### [704] [Region-Point Joint Representation for Effective Trajectory Similarity Learning](https://arxiv.org/abs/2511.13125)
*Hao Long,Silin Zhou,Lisi Chen,Shuo Shang*

Main category: cs.CV

TL;DR: 提出RePo方法联合编码区域和点特征进行轨迹相似性建模，实验显示比SOTA基线平均准确率提高22.2%。


<details>
  <summary>Details</summary>
Motivation: 现有SOTA方法未能充分利用轨迹信息进行相似性建模。

Method: 提出RePo方法，对区域特征，将GPS轨迹映射到网格序列，用结构和视觉特征捕获上下文；对点位特征，用三个专家网络提取模式，路由器网络融合，再与区域特征用交叉注意力结合生成最终嵌入，采用带难负样本的对比损失训练。

Result: RePo在所有评估指标上比SOTA基线平均准确率提高22.2%。

Conclusion: RePo方法能有效利用轨迹信息，在轨迹相似性建模上表现优于SOTA方法。

Abstract: Recent learning-based methods have reduced the computational complexity of traditional trajectory similarity computation, but state-of-the-art (SOTA) methods still fail to leverage the comprehensive spectrum of trajectory information for similarity modeling. To tackle this problem, we propose \textbf{RePo}, a novel method that jointly encodes \textbf{Re}gion-wise and \textbf{Po}int-wise features to capture both spatial context and fine-grained moving patterns. For region-wise representation, the GPS trajectories are first mapped to grid sequences, and spatial context are captured by structural features and semantic context enriched by visual features. For point-wise representation, three lightweight expert networks extract local, correlation, and continuous movement patterns from dense GPS sequences. Then, a router network adaptively fuses the learned point-wise features, which are subsequently combined with region-wise features using cross-attention to produce the final trajectory embedding. To train RePo, we adopt a contrastive loss with hard negative samples to provide similarity ranking supervision. Experiment results show that RePo achieves an average accuracy improvement of 22.2\% over SOTA baselines across all evaluation metrics.

</details>


### [705] [Calibrated Decomposition of Aleatoric and Epistemic Uncertainty in Deep Features for Inference-Time Adaptation](https://arxiv.org/abs/2511.12389)
*Divake Kumar,Patrick Poggi,Sina Tayebati,Devashri Naik,Nilesh Ahuja,Amit Ranjan Trivedi*

Main category: cs.CV

TL;DR: 提出轻量级推理时间框架Uncertainty - Guided Inference - Time Selection，分解不确定性，用于自适应模型选择，减少计算量。


<details>
  <summary>Details</summary>
Motivation: 多数估计器将所有不确定性模式合并为单一置信分数，无法可靠推理何时分配更多计算资源或调整推理。

Method: 在深度特征空间直接分离随机（数据驱动）和认知（模型驱动）不确定性，用正则化全局密度模型估计随机不确定性，认知不确定性由三个互补组件构成，将分解的不确定性集成到无分布共形校准程序。

Result: 在MOT17上减少约60%计算量且精度损失可忽略，消融实验表明正交不确定性分解在所有MOT17序列上有更高计算节省。

Conclusion: 提出的方法能实现实用的自调节视觉推理，正交不确定性分解优于总不确定性基线。

Abstract: Most estimators collapse all uncertainty modes into a single confidence score, preventing reliable reasoning about when to allocate more compute or adjust inference. We introduce Uncertainty-Guided Inference-Time Selection, a lightweight inference time framework that disentangles aleatoric (data-driven) and epistemic (model-driven) uncertainty directly in deep feature space. Aleatoric uncertainty is estimated using a regularized global density model, while epistemic uncertainty is formed from three complementary components that capture local support deficiency, manifold spectral collapse, and cross-layer feature inconsistency. These components are empirically orthogonal and require no sampling, no ensembling, and no additional forward passes. We integrate the decomposed uncertainty into a distribution free conformal calibration procedure that yields significantly tighter prediction intervals at matched coverage. Using these components for uncertainty guided adaptive model selection reduces compute by approximately 60 percent on MOT17 with negligible accuracy loss, enabling practical self regulating visual inference. Additionally, our ablation results show that the proposed orthogonal uncertainty decomposition consistently yields higher computational savings across all MOT17 sequences, improving margins by 13.6 percentage points over the total-uncertainty baseline.

</details>


### [706] [Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework](https://arxiv.org/abs/2511.13189)
*Diego Ortego,Marlon Rodríguez,Mario Almagro,Kunal Dahiya,David Jiménez,Juan C. SanMiguel*

Main category: cs.CV

TL;DR: 本文聚焦极端多标签分类（XMC），探讨利用大的仅解码器模型和视觉信息，提出ViXML框架，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 基础模型在XMC领域潜力未充分挖掘，需平衡效率与性能，解决有效利用大的仅解码器模型和利用视觉信息并保持计算效率的问题。

Method: 提出Vision - enhanced eXtreme Multi - label Learning框架（ViXML），通过为每张图像汇集单个嵌入来集成基础视觉模型；扩展现有仅文本数据集以利用视觉元数据。

Result: 数十亿规模的解码器可在控制计算开销的同时显著提升性能；ViXML在多数情况下优于仅文本解码器；在四个公共数据集及对应图像增强版本上实验，最高在P@1指标上比之前的最优方法提高8.21%。

Conclusion: ViXML框架有效，图像信息在XMC中有重要价值，扩展的数据集可用于未来基准测试。

Abstract: Foundation models have revolutionized artificial intelligence across numerous domains, yet their transformative potential remains largely untapped in Extreme Multi-label Classification (XMC). Queries in XMC are associated with relevant labels from extremely large label spaces, where it is critical to strike a balance between efficiency and performance. Therefore, many recent approaches efficiently pose XMC as a maximum inner product search between embeddings learned from small encoder-only transformer architectures. In this paper, we address two important aspects in XMC: how to effectively harness larger decoder-only models, and how to exploit visual information while maintaining computational efficiency. We demonstrate that both play a critical role in XMC separately and can be combined for improved performance. We show that a few billion-size decoder can deliver substantial improvements while keeping computational overhead manageable. Furthermore, our Vision-enhanced eXtreme Multi-label Learning framework (ViXML) efficiently integrates foundation vision models by pooling a single embedding per image. This limits computational growth while unlocking multi-modal capabilities. Remarkably, ViXML with small encoders outperforms text-only decoder in most cases, showing that an image is worth billions of parameters. Finally, we present an extension of existing text-only datasets to exploit visual metadata and make them available for future benchmarking. Comprehensive experiments across four public text-only datasets and their corresponding image enhanced versions validate our proposals' effectiveness, surpassing previous state-of-the-art by up to +8.21\% in P@1 on the largest dataset. ViXML's code is available at https://github.com/DiegoOrtego/vixml.

</details>


### [707] [Advancing Annotat3D with Harpia: A CUDA-Accelerated Library For Large-Scale Volumetric Data Segmentation](https://arxiv.org/abs/2511.11890)
*Camila Machado de Araujo,Egon P. B. S. Borges,Ricardo Marcelo Canteiro Grangeiro,Allan Pinto*

Main category: cs.CV

TL;DR: 本文介绍通过Harpia为Annotat3D引入新功能，用于处理大型3D数据集，实验显示其在多方面优于常用框架，适合协作科学成像工作流。


<details>
  <summary>Details</summary>
Motivation: 高分辨率体积成像技术产生的大型数据集对现有处理、分割和探索工具提出挑战，需要新工具。

Method: 引入基于CUDA的处理库Harpia，具有严格内存控制、原生分块执行和GPU加速工具。

Result: 与NVIDIA cuCIM和scikit - image等常用框架相比，在处理速度、内存效率和可扩展性上有显著提升。

Conclusion: 该系统的交互界面和高效GPU资源管理使其适用于共享HPC基础设施中的协作科学成像工作流。

Abstract: High-resolution volumetric imaging techniques, such as X-ray tomography and advanced microscopy, generate increasingly large datasets that challenge existing tools for efficient processing, segmentation, and interactive exploration. This work introduces new capabilities to Annotat3D through Harpia, a new CUDA-based processing library designed to support scalable, interactive segmentation workflows for large 3D datasets in high-performance computing (HPC) and remote-access environments. Harpia features strict memory control, native chunked execution, and a suite of GPU-accelerated filtering, annotation, and quantification tools, enabling reliable operation on datasets exceeding single-GPU memory capacity. Experimental results demonstrate significant improvements in processing speed, memory efficiency, and scalability compared to widely used frameworks such as NVIDIA cuCIM and scikit-image. The system's interactive, human-in-the-loop interface, combined with efficient GPU resource management, makes it particularly suitable for collaborative scientific imaging workflows in shared HPC infrastructures.

</details>


### [708] [PipeDiT: Accelerating Diffusion Transformers in Video Generation with Task Pipelining and Model Decoupling](https://arxiv.org/abs/2511.12056)
*Sijie Wang,Qiang Wang,Shaohuai Shi*

Main category: cs.CV

TL;DR: 提出PipeDiT加速视频生成框架，集成到两个开源框架实验，有显著加速效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散变压器的视频生成模型推理速度慢、内存消耗高，阻碍实际部署。

Method: 提出PipeDiT框架，包含PipeSP算法用于序列并行、DeDiVAE解耦模块、Aco方法利用GPU资源。

Result: 在两个8 - GPU系统上实验，PipeDiT在多种配置下比OpenSoraPlan和HunyuanVideo有1.06x到4.02x的加速。

Conclusion: PipeDiT框架能有效加速视频生成，减少推理延迟和内存消耗。

Abstract: Video generation has been advancing rapidly, and diffusion transformer (DiT) based models have demonstrated remark- able capabilities. However, their practical deployment is of- ten hindered by slow inference speeds and high memory con- sumption. In this paper, we propose a novel pipelining frame- work named PipeDiT to accelerate video generation, which is equipped with three main innovations. First, we design a pipelining algorithm (PipeSP) for sequence parallelism (SP) to enable the computation of latent generation and commu- nication among multiple GPUs to be pipelined, thus reduc- ing inference latency. Second, we propose DeDiVAE to de- couple the diffusion module and the variational autoencoder (VAE) module into two GPU groups, whose executions can also be pipelined to reduce memory consumption and infer- ence latency. Third, to better utilize the GPU resources in the VAE group, we propose an attention co-processing (Aco) method to further reduce the overall video generation latency. We integrate our PipeDiT into both OpenSoraPlan and Hun- yuanVideo, two state-of-the-art open-source video generation frameworks, and conduct extensive experiments on two 8- GPU systems. Experimental results show that, under many common resolution and timestep configurations, our PipeDiT achieves 1.06x to 4.02x speedups over OpenSoraPlan and HunyuanVideo.

</details>


### [709] [MiniGPT-Pancreas: Multimodal Large Language Model for Pancreas Cancer Classification and Detection](https://arxiv.org/abs/2412.15925)
*Andrea Moglia,Elia Clement Nastasio,Luca Mainardi,Pietro Cerveri*

Main category: cs.CV

TL;DR: 提出MiniGPT - Pancreas多模态大语言模型辅助胰腺癌诊断，经微调在多数据集上测试，结果表明其在胰腺图像分类上有潜力，但检测任务待提升。


<details>
  <summary>Details</summary>
Motivation: 胰腺放射成像因器官特点具有挑战性，需工具辅助临床医生进行胰腺癌诊断。

Method: 对MiniGPT - v2进行级联微调，结合问题和CT扫描的多模态提示，使用NIH、MSD和AbdomenCT - 1k数据集。

Result: 在多个数据集上进行胰腺检测、肿瘤分类、多器官检测和胰腺肿瘤检测，给出了相应的IoU、准确率、精确率和召回率。

Conclusion: MiniGPT - Pancreas是有前景的胰腺图像分类辅助工具，检测任务尤其是胰腺肿瘤检测得分需改进。

Abstract: Problem: Pancreas radiological imaging is challenging due to the small size, blurred boundaries, and variability of shape and position of the organ among patients. Goal: In this work we present MiniGPT-Pancreas, a Multimodal Large Language Model (MLLM), as an interactive chatbot to support clinicians in pancreas cancer diagnosis by integrating visual and textual information. Methods: MiniGPT-v2, a general-purpose MLLM, was fine-tuned in a cascaded way for pancreas detection, tumor classification, and tumor detection with multimodal prompts combining questions and computed tomography scans from the National Institute of Health (NIH), and Medical Segmentation Decathlon (MSD) datasets. The AbdomenCT-1k dataset was used to detect the liver, spleen, kidney, and pancreas. Results: MiniGPT-Pancreas achieved an Intersection over Union (IoU) of 0.595 and 0.550 for the detection of pancreas on NIH and MSD datasets, respectively. For the pancreas cancer classification task on the MSD dataset, accuracy, precision, and recall were 0.876, 0.874, and 0.878, respectively. When evaluating MiniGPT-Pancreas on the AbdomenCT-1k dataset for multi-organ detection, the IoU was 0.8399 for the liver, 0.722 for the kidney, 0.705 for the spleen, and 0.497 for the pancreas. For the pancreas tumor detection task, the IoU score was 0.168 on the MSD dataset. Conclusions: MiniGPT-Pancreas represents a promising solution to support clinicians in the classification of pancreas images with pancreas tumors. Future research is needed to improve the score on the detection task, especially for pancreas tumors.

</details>


### [710] [Task-Aware 3D Affordance Segmentation via 2D Guidance and Geometric Refinement](https://arxiv.org/abs/2511.11702)
*Lian He,Meng Liu,Qilang Ye,Yu Zhou,Xiang Deng,Gangyi Ding*

Main category: cs.CV

TL;DR: 提出TASA框架解决3D场景级可达性分割难题，实验显示其在准确性和效率上优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法在3D场景级可达性理解任务存在不足，如忽视点云几何结构信息、计算成本高。

Method: 引入TASA框架，采用粗到细的方式结合2D语义线索和3D几何推理，包含任务感知的2D可达性检测模块和3D可达性细化模块。

Result: 在SceneFun3D上的实验表明，TASA在场景级可达性分割的准确性和效率上显著优于基线。

Conclusion: TASA框架能有效解决3D场景级可达性分割问题，具有更好的性能。

Abstract: Understanding 3D scene-level affordances from natural language instructions is essential for enabling embodied agents to interact meaningfully in complex environments. However, this task remains challenging due to the need for semantic reasoning and spatial grounding. Existing methods mainly focus on object-level affordances or merely lift 2D predictions to 3D, neglecting rich geometric structure information in point clouds and incurring high computational costs. To address these limitations, we introduce Task-Aware 3D Scene-level Affordance segmentation (TASA), a novel geometry-optimized framework that jointly leverages 2D semantic cues and 3D geometric reasoning in a coarse-to-fine manner. To improve the affordance detection efficiency, TASA features a task-aware 2D affordance detection module to identify manipulable points from language and visual inputs, guiding the selection of task-relevant views. To fully exploit 3D geometric information, a 3D affordance refinement module is proposed to integrate 2D semantic priors with local 3D geometry, resulting in accurate and spatially coherent 3D affordance masks. Experiments on SceneFun3D demonstrate that TASA significantly outperforms the baselines in both accuracy and efficiency in scene-level affordance segmentation.

</details>


### [711] [Do Blind Spots Matter for Word-Referent Mapping? A Computational Study with Infant Egocentric Video](https://arxiv.org/abs/2511.11725)
*Zekai Shi,Zhixi Cai,Kalin Stefanov*

Main category: cs.CV

TL;DR: 本文利用单名儿童的纵向、以自我为中心且符合生态效度的数据，提出自监督且具生物学合理性的策略学习视觉表征，评估显示该策略学习词 - 指称映射至少与随机掩码策略效果相当。


<details>
  <summary>Details</summary>
Motivation: 儿童学习首个单词时，首次遇到的单词可能有无数种解释，需提出有效学习视觉表征的方法。

Method: 提出基于掩码自编码器的视觉主干，结合人眼盲点知识定义新的掩码策略，预训练编码器用于基于对比学习的视频 - 文本模型获取词 - 指称映射。

Result: 提出的具生物学合理性的掩码策略在学习词 - 指称映射方面至少和随机掩码策略效果一样好。

Conclusion: 具生物学合理性的掩码策略可用于学习词 - 指称映射，是一种有效的方法。

Abstract: Typically, children start to learn their first words between 6 and 9 months, linking spoken utterances to their visual referents. Without prior knowledge, a word encountered for the first time can be interpreted in countless ways; it might refer to any of the objects in the environment, their components, or attributes. Using longitudinal, egocentric, and ecologically valid data from the experience of one child, in this work, we propose a self-supervised and biologically plausible strategy to learn strong visual representations. Our masked autoencoder-based visual backbone incorporates knowledge about the blind spot in human eyes to define a novel masking strategy. This mask and reconstruct approach attempts to mimic the way the human brain fills the gaps in the eyes' field of view. This represents a significant shift from standard random masking strategies, which are difficult to justify from a biological perspective. The pretrained encoder is utilized in a contrastive learning-based video-text model capable of acquiring word-referent mappings. Extensive evaluation suggests that the proposed biologically plausible masking strategy is at least as effective as random masking for learning word-referent mappings from cross-situational and temporally extended episodes.

</details>


### [712] [GROVER: Graph-guided Representation of Omics and Vision with Expert Regulation for Adaptive Spatial Multi-omics Fusion](https://arxiv.org/abs/2511.11730)
*Yongjun Xiao,Dian Meng,Xinlei Huang,Yanran Liu,Shiwei Ruan,Ziyue Qiao,Xubin Zheng*

Main category: cs.CV

TL;DR: 提出GROVER框架用于自适应整合空间多组学数据，实验表明其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 有效建模多模态空间组学数据对理解组织复杂性和生物机制至关重要，但整合空间组学数据与组织病理学图像存在异质性、分辨率不匹配和信号失真等挑战。

Method: 提出GROVER框架，利用基于Kolmogorov - Arnold网络的图卷积网络编码器生成模态特定嵌入，引入斑点特征对对比学习策略对齐表示，设计动态专家路由机制选择信息模态。

Result: 在真实空间组学数据集上的实验显示GROVER优于现有基线。

Conclusion: GROVER为多模态整合提供了稳健可靠的解决方案。

Abstract: Effectively modeling multimodal spatial omics data is critical for understanding tissue complexity and underlying biological mechanisms. While spatial transcriptomics, proteomics, and epigenomics capture molecular features, they lack pathological morphological context. Integrating these omics with histopathological images is therefore essential for comprehensive disease tissue analysis. However, substantial heterogeneity across omics, imaging, and spatial modalities poses significant challenges. Naive fusion of semantically distinct sources often leads to ambiguous representations. Additionally, the resolution mismatch between high-resolution histology images and lower-resolution sequencing spots complicates spatial alignment. Biological perturbations during sample preparation further distort modality-specific signals, hindering accurate integration. To address these challenges, we propose Graph-guided Representation of Omics and Vision with Expert Regulation for Adaptive Spatial Multi-omics Fusion (GROVER), a novel framework for adaptive integration of spatial multi-omics data. GROVER leverages a Graph Convolutional Network encoder based on Kolmogorov-Arnold Networks to capture the nonlinear dependencies between each modality and its associated spatial structure, thereby producing expressive, modality-specific embeddings. To align these representations, we introduce a spot-feature-pair contrastive learning strategy that explicitly optimizes the correspondence across modalities at each spot. Furthermore, we design a dynamic expert routing mechanism that adaptively selects informative modalities for each spot while suppressing noisy or low-quality inputs. Experiments on real-world spatial omics datasets demonstrate that GROVER outperforms state-of-the-art baselines, providing a robust and reliable solution for multimodal integration.

</details>


### [713] [Concept-RuleNet: Grounded Multi-Agent Neurosymbolic Reasoning in Vision Language Models](https://arxiv.org/abs/2511.11751)
*Sanchit Sinha,Guangzhi Xiong,Zhenghao He,Aidong Zhang*

Main category: cs.CV

TL;DR: 提出多智能体系统Concept - RuleNet，在保持透明推理的同时恢复视觉基础，实验显示能提升现有神经符号基线表现并减少规则中幻觉符号出现。


<details>
  <summary>Details</summary>
Motivation: 现代视觉 - 语言模型缺乏决策解释且易产生事实幻觉，现有神经符号框架符号提取弱依赖视觉数据。

Method: 引入多智能体系统Concept - RuleNet，包括多模态概念生成器挖掘视觉概念、用视觉概念调节符号发现、大语言模型推理器生成规则、视觉验证器在推理时量化符号并触发规则执行。

Result: 在五个基准测试中，系统平均提升了5%的现有神经符号基线表现，最多减少规则中幻觉符号出现达50%。

Conclusion: Concept - RuleNet能有效提升视觉 - 语言模型性能，减少幻觉，实现可解释推理。

Abstract: Modern vision-language models (VLMs) deliver impressive predictive accuracy yet offer little insight into 'why' a decision is reached, frequently hallucinating facts, particularly when encountering out-of-distribution data. Neurosymbolic frameworks address this by pairing black-box perception with interpretable symbolic reasoning, but current methods extract their symbols solely from task labels, leaving them weakly grounded in the underlying visual data. In this paper, we introduce a multi-agent system - Concept-RuleNet that reinstates visual grounding while retaining transparent reasoning. Specifically, a multimodal concept generator first mines discriminative visual concepts directly from a representative subset of training images. Next, these visual concepts are utilized to condition symbol discovery, anchoring the generations in real image statistics and mitigating label bias. Subsequently, symbols are composed into executable first-order rules by a large language model reasoner agent - yielding interpretable neurosymbolic rules. Finally, during inference, a vision verifier agent quantifies the degree of presence of each symbol and triggers rule execution in tandem with outputs of black-box neural models, predictions with explicit reasoning pathways. Experiments on five benchmarks, including two challenging medical-imaging tasks and three underrepresented natural-image datasets, show that our system augments state-of-the-art neurosymbolic baselines by an average of 5% while also reducing the occurrence of hallucinated symbols in rules by up to 50%.

</details>


### [714] [Image-POSER: Reflective RL for Multi-Expert Image Generation and Editing](https://arxiv.org/abs/2511.11780)
*Hossein Mohebbi,Mohammed Abdulrahman,Yanting Miao,Pascal Poupart,Suraj Kothawade*

Main category: cs.CV

TL;DR: 介绍了Image - POSER反射式强化学习框架，它能处理长提示，实验显示其性能优于基线模型，表明强化学习可赋予AI系统组合视觉模型的能力。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成单步模型无法可靠处理创意工作流中的长组合提示。

Method: 引入Image - POSER框架，编排预训练专家模型，通过动态任务分解端到端处理长提示，利用视觉语言模型评论家的结构化反馈监督每一步。将图像合成和编辑视为马尔可夫决策过程，学习跨模型组合优势的专家管道。

Result: Image - POSER在行业标准和自定义基准测试的对齐、保真度和美学方面优于基线模型，包括前沿模型，在人类评估中也更受青睐。

Conclusion: 强化学习可赋予AI系统自主分解、重新排序和组合视觉模型的能力，朝着通用视觉助手迈进。

Abstract: Recent advances in text-to-image generation have produced strong single-shot models, yet no individual system reliably executes the long, compositional prompts typical of creative workflows. We introduce Image-POSER, a reflective reinforcement learning framework that (i) orchestrates a diverse registry of pretrained text-to-image and image-to-image experts, (ii) handles long-form prompts end-to-end through dynamic task decomposition, and (iii) supervises alignment at each step via structured feedback from a vision-language model critic. By casting image synthesis and editing as a Markov Decision Process, we learn non-trivial expert pipelines that adaptively combine strengths across models. Experiments show that Image-POSER outperforms baselines, including frontier models, across industry-standard and custom benchmarks in alignment, fidelity, and aesthetics, and is consistently preferred in human evaluations. These results highlight that reinforcement learning can endow AI systems with the capacity to autonomously decompose, reorder, and combine visual models, moving towards general-purpose visual assistants.

</details>


### [715] [Prompt Triage: Structured Optimization Enhances Vision-Language Model Performance on Medical Imaging Benchmarks](https://arxiv.org/abs/2511.11898)
*Arnav Singhvi,Vasiliki Bikia,Asad Aali,Akshay Chaudhari,Roxana Daneshjou*

Main category: cs.CV

TL;DR: 本文探索自动提示优化在医学视觉语言系统中的应用，通过评估开源模型，证明该方法能显著提升性能且具有可扩展性和数据隐私保护优势。


<details>
  <summary>Details</summary>
Motivation: 现有提升视觉语言基础模型在医学基准测试中性能的方法存在依赖大量特定领域数据、计算资源和人工提示工程难泛化等问题，因此需探索依赖模型嵌入知识且无需人工设计提示的方法。

Method: 采用Declarative Self - improving Python (DSPy)框架，为五个医学成像任务实现提示管道，用四种提示优化技术评估10个开源视觉语言模型。

Result: 优化后的管道比零样本提示基线实现了53%的中位相对改进，在零样本性能低的任务上最大增益达300% - 3400%。

Conclusion: 自动提示优化技术应用于医学人工智能系统潜力巨大，可减少对提示设计的依赖，支持临床决策，且具有可扩展性和数据隐私保护优势，已公开评估管道支持可重复性研究。

Abstract: Vision-language foundation models (VLMs) show promise for diverse imaging tasks but often underperform on medical benchmarks. Prior efforts to improve performance include model finetuning, which requires large domain-specific datasets and significant compute, or manual prompt engineering, which is hard to generalize and often inaccessible to medical institutions seeking to deploy these tools. These challenges motivate interest in approaches that draw on a model's embedded knowledge while abstracting away dependence on human-designed prompts to enable scalable, weight-agnostic performance improvements. To explore this, we adapt the Declarative Self-improving Python (DSPy) framework for structured automated prompt optimization in medical vision-language systems through a comprehensive, formal evaluation. We implement prompting pipelines for five medical imaging tasks across radiology, gastroenterology, and dermatology, evaluating 10 open-source VLMs with four prompt optimization techniques. Optimized pipelines achieved a median relative improvement of 53% over zero-shot prompting baselines, with the largest gains ranging from 300% to 3,400% on tasks where zero-shot performance is low. These results highlight the substantial potential of applying automated prompt optimization to medical AI systems, demonstrating significant gains for vision-based applications requiring accurate clinical image interpretation. By reducing dependence on prompt design to elicit intended outputs, these techniques allow clinicians to focus on patient care and clinical decision-making. Furthermore, our experiments offer scalability and preserve data privacy, demonstrating performance improvement on open-source VLMs. We publicly release our evaluation pipelines to support reproducible research on specialized medical tasks, available at https://github.com/DaneshjouLab/prompt-triage-lab.

</details>


### [716] [PI-NAIM: Path-Integrated Neural Adaptive Imputation Model](https://arxiv.org/abs/2511.11908)
*Afifa Khaled,Ebrahim Hamid Sumiea*

Main category: cs.CV

TL;DR: 提出PI - NAIM模型解决医学成像和多模态临床诊断中模态缺失问题，实验表现优异且代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有医学成像和多模态临床诊断中模态缺失的插补方法存在表征能力不足或计算成本高的问题。

Method: 提出PI - NAIM双路径架构，包括智能路径路由、跨路径注意力融合和端到端联合优化。

Result: 在MIMIC - III和多模态基准测试中表现达到了当前最优，RMSE为0.108，下游死亡率预测AUROC为0.812。

Conclusion: PI - NAIM模块化设计可无缝集成到视觉管道，为现实场景提供统一解决方案。

Abstract: Medical imaging and multi-modal clinical settings often face the challange of missing modality in their diagnostic pipelines. Existing imputation methods either lack representational capacity or are computationally expensive. We propose PI-NAIM, a novel dual-path architecture that dynamically routes samples to optimized imputation approaches based on missingness complexity. Our framework integrates: (1) intelligent path routing that directs low missingness samples to efficient statistical imputation (MICE) and complex patterns to powerful neural networks (GAIN with temporal analysis); (2) cross-path attention fusion that leverages missingness-aware embeddings to intelligently combine both branches; and (3) end-to-end joint optimization of imputation accuracy and downstream task performance. Extensive experiments on MIMIC-III and multimodal benchmarks demonstrate state-of-the-art performance, achieving RMSE of 0.108 (vs. baselines' 0.119-0.152) and substantial gains in downstream tasks with an AUROC of 0.812 for mortality prediction. PI-NAIM's modular design enables seamless integration into vision pipelines handling incomplete sensor measurements, missing modalities, or corrupted inputs, providing a unified solution for real-world scenario. The code is publicly available at https://github.com/AfifaKhaled/PI-NAIM-Path-Integrated-Neural-Adaptive-Imputation-Model

</details>


### [717] [Uncertainty-Guided Selective Adaptation Enables Cross-Platform Predictive Fluorescence Microscopy](https://arxiv.org/abs/2511.12006)
*Kai-Wen K. Yang,Andrew Bai,Alexandra Bermudez,Yunqi Hong,Zoe Latham,Iris Sloan,Michael Liu,Vishrut Goyal,Cho-Jui Hsieh,Neil Y. C. Lin*

Main category: cs.CV

TL;DR: 提出SIT - ADDA - Auto框架用于显微镜图像领域适应，提升性能且代码公开。


<details>
  <summary>Details</summary>
Motivation: 深度学习应用于显微镜图像时模型在新仪器或采集设置下易失效，传统ADDA会破坏语义表示。

Method: 仅调整最早的卷积层，冻结更深层；引入SIT - ADDA - Auto框架，结合浅层对抗对齐和预测不确定性自动选择适应深度。

Result: 通过多指标评估等证明鲁棒性，在多种场景下优于全编码器适应和非对抗基线，减少语义特征漂移。

Conclusion: 为显微镜无标签适应提供设计规则和实际应用方案。

Abstract: Deep learning is transforming microscopy, yet models often fail when applied to images from new instruments or acquisition settings. Conventional adversarial domain adaptation (ADDA) retrains entire networks, often disrupting learned semantic representations. Here, we overturn this paradigm by showing that adapting only the earliest convolutional layers, while freezing deeper layers, yields reliable transfer. Building on this principle, we introduce Subnetwork Image Translation ADDA with automatic depth selection (SIT-ADDA-Auto), a self-configuring framework that integrates shallow-layer adversarial alignment with predictive uncertainty to automatically select adaptation depth without target labels. We demonstrate robustness via multi-metric evaluation, blinded expert assessment, and uncertainty-depth ablations. Across exposure and illumination shifts, cross-instrument transfer, and multiple stains, SIT-ADDA improves reconstruction and downstream segmentation over full-encoder adaptation and non-adversarial baselines, with reduced drift of semantic features. Our results provide a design rule for label-free adaptation in microscopy and a recipe for field settings; the code is publicly available.

</details>


### [718] [GCAgent: Long-Video Understanding via Schematic and Narrative Episodic Memory](https://arxiv.org/abs/2511.12027)
*Jeong Hun Yeo,Sangyun Chung,Sungjune Park,Dae Hoe Kim,Jinyoung Moon,Yong Man Ro*

Main category: cs.CV

TL;DR: 提出GCAgent框架解决多模态大语言模型长视频理解难题，实验表明其显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以解决多模态大语言模型长视频理解中捕捉全局上下文和复杂事件关系的问题。

Method: 引入GCAgent框架，核心是示意图和叙事式情节记忆，在感知 - 行动 - 反思循环中利用记忆管理器检索上下文进行推理。

Result: GCAgent显著提升长视频理解能力，在Video - MME Long分割上准确率最高提升23.5%，在7B规模MLLMs中达SOTA。

Conclusion: 基于代理的推理范式和结构化记忆可用于认知启发的长视频理解。

Abstract: Long-video understanding remains a significant challenge for Multimodal Large Language Models (MLLMs) due to inherent token limitations and the complexity of capturing long-term temporal dependencies. Existing methods often fail to capture the global context and complex event relationships necessary for deep video reasoning. To address this, we introduce GCAgent, a novel Global-Context-Aware Agent framework that achieves comprehensive long-video understanding. Our core innovation is the Schematic and Narrative Episodic Memory. This memory structurally models events and their causal and temporal relations into a concise, organized context, fundamentally resolving the long-term dependency problem. Operating in a multi-stage Perception-Action-Reflection cycle, our GCAgent utilizes a Memory Manager to retrieve relevant episodic context for robust, context-aware inference. Extensive experiments confirm that GCAgent significantly enhances long-video understanding, achieving up to 23.5\% accuracy improvement on the Video-MME Long split over a strong MLLM baseline. Furthermore, our framework establishes state-of-the-art performance among comparable 7B-scale MLLMs, achieving 73.4\% accuracy on the Long split and the highest overall average (71.9\%) on the Video-MME benchmark, validating our agent-based reasoning paradigm and structured memory for cognitively-inspired long-video understanding.

</details>


### [719] [DCMM-Transformer: Degree-Corrected Mixed-Membership Attention for Medical Imaging](https://arxiv.org/abs/2511.12047)
*Huimin Cheng,Xiaowei Yu,Shushan Wu,Luyang Fang,Chao Cao,Jing Zhang,Tianming Liu,Dajiang Zhu,Wenxuan Zhong,Ping Ma*

Main category: cs.CV

TL;DR: 提出DCMM - Transformer用于医学图像分析，引入DCMM模型作为自注意力的附加偏差，实验证明性能优越且可解释性强。


<details>
  <summary>Details</summary>
Motivation: 标准ViTs无法利用医学图像潜在解剖分组，现有方法如SBM - Transformer存在非可微性、训练不稳定和无法建模复杂社区结构等问题。

Method: 提出DCMM - Transformer，将Degree - Corrected Mixed - Membership (DCMM)模型作为自注意力的附加偏差，以全可微和可解释的方式引入社区结构和度异质性。

Result: 在多种医学成像数据集上的综合实验表明，该方法性能优越且具有良好的泛化能力。

Conclusion: 所提出的方法能通过生成具有解剖学意义和语义连贯性的注意力图，显著增强可解释性。

Abstract: Medical images exhibit latent anatomical groupings, such as organs, tissues, and pathological regions, that standard Vision Transformers (ViTs) fail to exploit. While recent work like SBM-Transformer attempts to incorporate such structures through stochastic binary masking, they suffer from non-differentiability, training instability, and the inability to model complex community structure. We present DCMM-Transformer, a novel ViT architecture for medical image analysis that incorporates a Degree-Corrected Mixed-Membership (DCMM) model as an additive bias in self-attention. Unlike prior approaches that rely on multiplicative masking and binary sampling, our method introduces community structure and degree heterogeneity in a fully differentiable and interpretable manner. Comprehensive experiments across diverse medical imaging datasets, including brain, chest, breast, and ocular modalities, demonstrate the superior performance and generalizability of the proposed approach. Furthermore, the learned group structure and structured attention modulation substantially enhance interpretability by yielding attention maps that are anatomically meaningful and semantically coherent.

</details>


### [720] [MediRound: Multi-Round Entity-Level Reasoning Segmentation in Medical Images](https://arxiv.org/abs/2511.12110)
*Qinyue Tong,Ziqian Lu,Jun Liu,Rui Zuo,Zheming Lu*

Main category: cs.CV

TL;DR: 提出多轮实体级医学推理分割任务MEMR - Seg，构建数据集MR - MedSeg，提出模型MediRound及判断与修正机制，实验显示方法有效且优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割方法任务特定、缺乏交互性，文本提示方法局限于单轮对话，无法进行多轮推理。

Method: 提出MEMR - Seg任务，构建MR - MedSeg数据集，提出MediRound模型，引入判断与修正机制。

Result: 方法有效解决MEMR - Seg任务，优于传统医学指称分割方法。

Conclusion: 所提方法在多轮医学推理分割任务上具有有效性和优越性。

Abstract: Despite the progress in medical image segmentation, most existing methods remain task-specific and lack interactivity. Although recent text-prompt-based segmentation approaches enhance user-driven and reasoning-based segmentation, they remain confined to single-round dialogues and fail to perform multi-round reasoning. In this work, we introduce Multi-Round Entity-Level Medical Reasoning Segmentation (MEMR-Seg), a new task that requires generating segmentation masks through multi-round queries with entity-level reasoning. To support this task, we construct MR-MedSeg, a large-scale dataset of 177K multi-round medical segmentation dialogues, featuring entity-based reasoning across rounds. Furthermore, we propose MediRound, an effective baseline model designed for multi-round medical reasoning segmentation. To mitigate the inherent error propagation in the chain-like pipeline of multi-round segmentation, we introduce a lightweight yet effective Judgment & Correction Mechanism during model inference. Experimental results demonstrate that our method effectively addresses the MEMR-Seg task and outperforms conventional medical referring segmentation methods.

</details>


### [721] [OAD-Promoter: Enhancing Zero-shot VQA using Large Language Models with Object Attribute Description](https://arxiv.org/abs/2511.12131)
*Quanxing Xu,Ling Zhou,Feifei Zhang,Jinyu Tian,Rubing Huang*

Main category: cs.CV

TL;DR: 提出OAD - Promoter解决LLM在VQA中语言偏差和泛化问题，实验取得新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在VQA中依赖大规模数据集，存在语言偏差，导致预测不可靠和泛化能力差。

Method: 提出OAD - Promoter，包含OEG模块、MKA模块和OAD Prompt，分别用于增强视觉信息输入、辅助处理OOD样本和优化推理。

Result: OAD - Promoter显著提升了基于LLM的VQA方法在少样本或零样本设置下的性能，取得新SOTA结果。

Conclusion: OAD - Promoter能有效缓解语言偏差，提高领域转移鲁棒性，提升LLM在VQA中的性能。

Abstract: Large Language Models (LLMs) have become a crucial tool in Visual Question Answering (VQA) for handling knowledge-intensive questions in few-shot or zero-shot scenarios. However, their reliance on massive training datasets often causes them to inherit language biases during the acquisition of knowledge. This limitation imposes two key constraints on existing methods: (1) LLM predictions become less reliable due to bias exploitation, and (2) despite strong knowledge reasoning capabilities, LLMs still struggle with out-of-distribution (OOD) generalization. To address these issues, we propose Object Attribute Description Promoter (OAD-Promoter), a novel approach for enhancing LLM-based VQA by mitigating language bias and improving domain-shift robustness. OAD-Promoter comprises three components: the Object-concentrated Example Generation (OEG) module, the Memory Knowledge Assistance (MKA) module, and the OAD Prompt. The OEG module generates global captions and object-concentrated samples, jointly enhancing visual information input to the LLM and mitigating bias through complementary global and regional visual cues. The MKA module assists the LLM in handling OOD samples by retrieving relevant knowledge from stored examples to support questions from unseen domains. Finally, the OAD Prompt integrates the outputs of the preceding modules to optimize LLM inference. Experiments demonstrate that OAD-Promoter significantly improves the performance of LLM-based VQA methods in few-shot or zero-shot settings, achieving new state-of-the-art results.

</details>


### [722] [Rethinking Multimodal Point Cloud Completion: A Completion-by-Correction Perspective](https://arxiv.org/abs/2511.12170)
*Wang Luo,Di Wu,Hengyuan Na,Yinlin Zhu,Miao Hu,Guocong Quan*

Main category: cs.CV

TL;DR: 本文指出当前点云补全方法存在问题，提出Completion - by - Correction范式及PGNet框架，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于Completion - by - Inpainting范式的点云补全方法常因几何和语义约束有限，导致结构不一致和拓扑伪影问题。

Method: 提出Completion - by - Correction范式，先由预训练图像到3D模型生成拓扑完整的形状先验，再进行特征空间校正；构建PGNet多阶段框架，进行双特征编码、合成粗结构支架并逐步细化几何细节。

Result: 在ShapeNetViPC数据集上，PGNet在平均倒角距离上降低23.5%，F分数提高7.1%。

Conclusion: Completion - by - Correction范式和PGNet框架能实现结构一致且与观察对齐的重建，优于现有方法。

Abstract: Point cloud completion aims to reconstruct complete 3D shapes from partial observations, which is a challenging problem due to severe occlusions and missing geometry. Despite recent advances in multimodal techniques that leverage complementary RGB images to compensate for missing geometry, most methods still follow a Completion-by-Inpainting paradigm, synthesizing missing structures from fused latent features. We empirically show that this paradigm often results in structural inconsistencies and topological artifacts due to limited geometric and semantic constraints. To address this, we rethink the task and propose a more robust paradigm, termed Completion-by-Correction, which begins with a topologically complete shape prior generated by a pretrained image-to-3D model and performs feature-space correction to align it with the partial observation. This paradigm shifts completion from unconstrained synthesis to guided refinement, enabling structurally consistent and observation-aligned reconstruction. Building upon this paradigm, we introduce PGNet, a multi-stage framework that conducts dual-feature encoding to ground the generative prior, synthesizes a coarse yet structurally aligned scaffold, and progressively refines geometric details via hierarchical correction. Experiments on the ShapeNetViPC dataset demonstrate the superiority of PGNet over state-of-the-art baselines in terms of average Chamfer Distance (-23.5%) and F-score (+7.1%).

</details>


### [723] [A Novel AI-Driven System for Real-Time Detection of Mirror Absence, Helmet Non-Compliance, and License Plates Using YOLOv8 and OCR](https://arxiv.org/abs/2511.12206)
*Nishant Vasantkumar Hegde,Aditi Agarwal,Minal Moharir*

Main category: cs.CV

TL;DR: 本文提出AI交通违规检测系统，用YOLOv8和EasyOCR，经自定义数据集训练，能识别违规和提取车牌，评估指标良好，是实用有效方案。


<details>
  <summary>Details</summary>
Motivation: 手动执行头盔法和车辆安全标准资源密集且不一致，需自动化系统提升执法效率和道路安全。

Method: 利用YOLOv8进行目标检测，EasyOCR进行车牌识别，在自定义增强数据集上训练，用Streamlit界面实时监控和记录，采用高级图像预处理。

Result: 模型整体精度0.9147，召回率0.886，mAP@50为0.843，mAP@50 95为0.503。

Conclusion: 该系统是自动化交通规则执法的实用有效解决方案，还讨论了实际部署考量。

Abstract: Road safety is a critical global concern, with manual enforcement of helmet laws and vehicle safety standards (e.g., rear-view mirror presence) being resource-intensive and inconsistent. This paper presents an AI-powered system to automate traffic violation detection, significantly enhancing enforcement efficiency and road safety. The system leverages YOLOv8 for robust object detection and EasyOCR for license plate recognition. Trained on a custom dataset of annotated images (augmented for diversity), it identifies helmet non-compliance, the absence of rear-view mirrors on motorcycles, an innovative contribution to automated checks, and extracts vehicle registration numbers. A Streamlit-based interface facilitates real-time monitoring and violation logging. Advanced image preprocessing enhances license plate recognition, particularly under challenging conditions. Based on evaluation results, the model achieves an overall precision of 0.9147, a recall of 0.886, and a mean Average Precision (mAP@50) of 0.843. The mAP@50 95 of 0.503 further indicates strong detection capability under stricter IoU thresholds. This work demonstrates a practical and effective solution for automated traffic rule enforcement, with considerations for real-world deployment discussed.

</details>


### [724] [Model Inversion Attack Against Deep Hashing](https://arxiv.org/abs/2511.12233)
*Dongdong Zhao,Qiben Xu,Ranxin Fang,Baogang Song*

Main category: cs.CV

TL;DR: 本文指出深度哈希存在隐私风险，提出首个基于扩散的模型反演框架DHMI，实验证明其在黑盒场景有效，凸显深度哈希系统隐私风险。


<details>
  <summary>Details</summary>
Motivation: 深度哈希存在隐私风险，但针对其模型反演攻击未被研究，现有方法因训练哈希码不可访问和汉明空间离散难以适应深度哈希。

Method: 提出DHMI框架，先聚类辅助数据集得到语义哈希中心作为代理锚点，引入代理引导的去噪优化方法，利用新攻击指标动态选择候选样本，用代理模型簇优化候选样本。

Result: 在多数据集实验中，DHMI在无训练哈希码的最具挑战性黑盒设置下成功重构高分辨率、高质量图像，优于现有最先进模型反演攻击方法。

Conclusion: DHMI具有实际有效性，深度哈希系统存在严重隐私风险。

Abstract: Deep hashing improves retrieval efficiency through compact binary codes, yet it introduces severe and often overlooked privacy risks. The ability to reconstruct original training data from hash codes could lead to serious threats such as biometric forgery and privacy breaches. However, model inversion attacks specifically targeting deep hashing models remain unexplored, leaving their security implications unexamined. This research gap stems from the inaccessibility of genuine training hash codes and the highly discrete Hamming space, which prevents existing methods from adapting to deep hashing. To address these challenges, we propose DHMI, the first diffusion-based model inversion framework designed for deep hashing. DHMI first clusters an auxiliary dataset to derive semantic hash centers as surrogate anchors. It then introduces a surrogate-guided denoising optimization method that leverages a novel attack metric (fusing classification consistency and hash proximity) to dynamically select candidate samples. A cluster of surrogate models guides the refinement of these candidates, ensuring the generation of high-fidelity and semantically consistent images. Experiments on multiple datasets demonstrate that DHMI successfully reconstructs high-resolution, high-quality images even under the most challenging black-box setting, where no training hash codes are available. Our method outperforms the existing state-of-the-art model inversion attacks in black-box scenarios, confirming both its practical efficacy and the critical privacy risks inherent in deep hashing systems.

</details>


### [725] [Prompt-Conditioned FiLM and Multi-Scale Fusion on MedSigLIP for Low-Dose CT Quality Assessment](https://arxiv.org/abs/2511.12256)
*Tolga Demiroglu,Mehmet Ozan Unal,Metin Ertas,Isa Yildirim*

Main category: cs.CV

TL;DR: 提出基于MedSigLIP的提示条件框架，在LDCTIQA2023上取得优异成绩，证明方法有效性。


<details>
  <summary>Details</summary>
Motivation: 实现数据高效学习和快速适应，解决临床意图相关的图像质量评估问题。

Method: 构建基于MedSigLIP的提示条件框架，通过FiLM和多尺度池化注入文本先验，结合不同池化和回归头，用成对排序损失训练。

Result: 在LDCTIQA2023上，PLCC = 0.9575，SROCC = 0.9561，KROCC = 0.8301，超越已发表的顶级挑战提交结果。

Conclusion: 提示引导方法有效。

Abstract: We propose a prompt-conditioned framework built on MedSigLIP that injects textual priors via Feature-wise Linear Modulation (FiLM) and multi-scale pooling. Text prompts condition patch-token features on clinical intent, enabling data-efficient learning and rapid adaptation. The architecture combines global, local, and texture-aware pooling through separate regression heads fused by a lightweight MLP, trained with pairwise ranking loss. Evaluated on the LDCTIQA2023 (a public LDCT quality assessment challenge) with 1,000 training images, we achieve PLCC = 0.9575, SROCC = 0.9561, and KROCC = 0.8301, surpassing the top-ranked published challenge submissions and demonstrating the effectiveness of our prompt-guided approach.

</details>


### [726] [CrossVid: A Comprehensive Benchmark for Evaluating Cross-Video Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2511.12263)
*Jingyao Li,Jingyun Wang,Molin Tan,Haochen Wang,Cilin Yan,Likun Shi,Jiayin Cai,Xiaolong Jiang,Yao Hu*

Main category: cs.CV

TL;DR: 提出首个跨视频基准CrossVid评估多模态大语言模型跨视频时空推理能力，实验显示Gemini - 2.5 - Pro表现最佳，多数模型跨视频推理能力待提升。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解基准难以评估多模态大语言模型跨视频推理能力，需全面评估其在跨视频场景中的时空推理能力。

Method: 引入CrossVid基准，包含多种层次任务和大量视频及问答对，对开源和闭源模型进行实验。

Result: Gemini - 2.5 - Pro在CrossVid上平均准确率达50.4%，多数当前模型在跨视频推理任务中表现不佳。

Conclusion: CrossVid有潜力指导提升多模态大语言模型跨视频推理能力的未来研究。

Abstract: Cross-Video Reasoning (CVR) presents a significant challenge in video understanding, which requires simultaneous understanding of multiple videos to aggregate and compare information across groups of videos. Most existing video understanding benchmarks focus on single-video analysis, failing to assess the ability of multimodal large language models (MLLMs) to simultaneously reason over various videos. Recent benchmarks evaluate MLLMs' capabilities on multi-view videos that capture different perspectives of the same scene. However, their limited tasks hinder a thorough assessment of MLLMs in diverse real-world CVR scenarios. To this end, we introduce CrossVid, the first benchmark designed to comprehensively evaluate MLLMs' spatial-temporal reasoning ability in cross-video contexts. Firstly, CrossVid encompasses a wide spectrum of hierarchical tasks, comprising four high-level dimensions and ten specific tasks, thereby closely reflecting the complex and varied nature of real-world video understanding. Secondly, CrossVid provides 5,331 videos, along with 9,015 challenging question-answering pairs, spanning single-choice, multiple-choice, and open-ended question formats. Through extensive experiments on various open-source and closed-source MLLMs, we observe that Gemini-2.5-Pro performs best on CrossVid, achieving an average accuracy of 50.4%. Notably, our in-depth case study demonstrates that most current MLLMs struggle with CVR tasks, primarily due to their inability to integrate or compare evidence distributed across multiple videos for reasoning. These insights highlight the potential of CrossVid to guide future advancements in enhancing MLLMs' CVR capabilities.

</details>


### [727] [Rethinking Bias in Generative Data Augmentation for Medical AI: a Frequency Recalibration Method](https://arxiv.org/abs/2511.12301)
*Chi Liu,Jincheng Liu,Congcong Zhu,Minghao Wang,Sheng Shen,Jia Gu,Tianqing Zhu,Wanlei Zhou*

Main category: cs.CV

TL;DR: 本文指出生成式数据增强（GDA）在医学领域存在偏差问题，提出频率重新校准（FreRec）方法降低频率分布差异，实验表明该方法能提升下游医学图像分类性能，且可与任何生成模型兼容。


<details>
  <summary>Details</summary>
Motivation: 开发医学AI面临数据稀缺问题，GDA可合成医学图像，但医学领域常低估其偏差，有引入有害特征危害下游任务的风险。

Method: 提出FreRec方法，包括统计高频替换（SHR）大致对齐高频分量，以及重建高频映射（RHM）提升图像质量和重建高频细节。

Result: 在多种医学数据集上实验表明，与未校准的AI合成样本相比，FreRec显著提高了下游医学图像分类性能。

Conclusion: FreRec是独立后处理步骤，可与任何生成模型兼容，能无缝集成到常见医学GDA流程中。

Abstract: Developing Medical AI relies on large datasets and easily suffers from data scarcity. Generative data augmentation (GDA) using AI generative models offers a solution to synthesize realistic medical images. However, the bias in GDA is often underestimated in medical domains, with concerns about the risk of introducing detrimental features generated by AI and harming downstream tasks. This paper identifies the frequency misalignment between real and synthesized images as one of the key factors underlying unreliable GDA and proposes the Frequency Recalibration (FreRec) method to reduce the frequency distributional discrepancy and thus improve GDA. FreRec involves (1) Statistical High-frequency Replacement (SHR) to roughly align high-frequency components and (2) Reconstructive High-frequency Mapping (RHM) to enhance image quality and reconstruct high-frequency details. Extensive experiments were conducted in various medical datasets, including brain MRIs, chest X-rays, and fundus images. The results show that FreRec significantly improves downstream medical image classification performance compared to uncalibrated AI-synthesized samples. FreRec is a standalone post-processing step that is compatible with any generative model and can integrate seamlessly with common medical GDA pipelines.

</details>


### [728] [Learning Time in Static Classifiers](https://arxiv.org/abs/2511.12321)
*Xi Ding,Lei Wang,Piotr Koniusz,Yongsheng Gao*

Main category: cs.CV

TL;DR: 提出简单有效框架让标准前馈分类器具备时间推理能力，在静态和时间任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现实视觉数据有时间动态性，传统分类器假设时间独立，难以捕捉动态。

Method: 提出SEQ学习范式，将训练数据构建为时间连贯轨迹，通过可微软DTW损失学习类特定时间原型和对齐预测序列，多目标项促进语义一致性和时间平滑性。

Result: 在细粒度和超细粒度图像分类中提升性能，在视频异常检测中提供精确且时间一致的预测。

Conclusion: 该方法以模块化和数据高效的方式桥接静态和时间学习，只需在预提取特征上使用简单分类器。

Abstract: Real-world visual data rarely presents as isolated, static instances. Instead, it often evolves gradually over time through variations in pose, lighting, object state, or scene context. However, conventional classifiers are typically trained under the assumption of temporal independence, limiting their ability to capture such dynamics. We propose a simple yet effective framework that equips standard feedforward classifiers with temporal reasoning, all without modifying model architectures or introducing recurrent modules. At the heart of our approach is a novel Support-Exemplar-Query (SEQ) learning paradigm, which structures training data into temporally coherent trajectories. These trajectories enable the model to learn class-specific temporal prototypes and align prediction sequences via a differentiable soft-DTW loss. A multi-term objective further promotes semantic consistency and temporal smoothness. By interpreting input sequences as evolving feature trajectories, our method introduces a strong temporal inductive bias through loss design alone. This proves highly effective in both static and temporal tasks: it enhances performance on fine-grained and ultra-fine-grained image classification, and delivers precise, temporally consistent predictions in video anomaly detection. Despite its simplicity, our approach bridges static and temporal learning in a modular and data-efficient manner, requiring only a simple classifier on top of pre-extracted features.

</details>


### [729] [Ground Plane Projection for Improved Traffic Analytics at Intersections](https://arxiv.org/abs/2511.12342)
*Sajjad Pakdamansavoji,Kumar Vaibhav Jha,Baher Abdulhai,James H Elder*

Main category: cs.CV

TL;DR: 研究表明交通分析应在地面平面而非图像平面进行，单相机反投影及多相机弱融合可提高转弯计数准确性。


<details>
  <summary>Details</summary>
Motivation: 准确的路口转弯运动计数对信号控制、交通管理和城市规划很重要，现有自动转弯运动计数的计算机视觉系统多依赖图像平面分析，探索地面平面分析的优势。

Method: 将一个或多个基础设施相机检测到的车辆反投影到地面平面，在真实世界3D坐标中分析，包括单相机系统和多相机弱融合。

Result: 单相机反投影能实现更准确的轨迹分类和转弯运动计数，多相机弱融合可实现更高准确性。

Conclusion: 交通应在地面平面进行分析，而非图像平面。

Abstract: Accurate turning movement counts at intersections are important for signal control, traffic management and urban planning. Computer vision systems for automatic turning movement counts typically rely on visual analysis in the image plane of an infrastructure camera. Here we explore potential advantages of back-projecting vehicles detected in one or more infrastructure cameras to the ground plane for analysis in real-world 3D coordinates. For single-camera systems we find that back-projection yields more accurate trajectory classification and turning movement counts. We further show that even higher accuracy can be achieved through weak fusion of back-projected detections from multiple cameras. These results suggeest that traffic should be analyzed on the ground plane, not the image plane

</details>


### [730] [CLAReSNet: When Convolution Meets Latent Attention for Hyperspectral Image Classification](https://arxiv.org/abs/2511.12346)
*Asmit Bandyopadhyay,Anindita Das Bhattacharjee,Rakesh Das*

Main category: cs.CV

TL;DR: 提出CLAReSNet用于高光谱图像分类，结合多尺度卷积提取和注意力机制，在数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像分类面临高维、复杂相关性和样本不平衡等挑战，现有CNN和transformer单独使用效果不佳。

Method: 提出CLAReSNet，采用多尺度卷积、残差块、增强注意力模块，结合双向RNN和多尺度光谱潜在注意力，通过自适应潜在令牌分配降低复杂度，进行分层交叉注意力融合。

Result: 在Indian Pines和Salinas数据集上达到99.71%和99.96%的总体准确率，超越HybridSN、SSRN和SpectralFormer。

Conclusion: CLAReSNet在有限样本和严重类别不平衡情况下有效，学习的嵌入具有良好的类间可分性和类内紧凑性。

Abstract: Hyperspectral image (HSI) classification faces critical challenges, including high spectral dimensionality, complex spectral-spatial correlations, and limited training samples with severe class imbalance. While CNNs excel at local feature extraction and transformers capture long-range dependencies, their isolated application yields suboptimal results due to quadratic complexity and insufficient inductive biases. We propose CLAReSNet (Convolutional Latent Attention Residual Spectral Network), a hybrid architecture that integrates multi-scale convolutional extraction with transformer-style attention via an adaptive latent bottleneck. The model employs a multi-scale convolutional stem with deep residual blocks and an enhanced Convolutional Block Attention Module for hierarchical spatial features, followed by spectral encoder layers combining bidirectional RNNs (LSTM/GRU) with Multi-Scale Spectral Latent Attention (MSLA). MSLA reduces complexity from $\mathcal{O}(T^2D)$ to $\mathcal{O}(T\log(T)D)$ by adaptive latent token allocation (8-64 tokens) that scales logarithmically with the sequence length. Hierarchical cross-attention fusion dynamically aggregates multi-level representations for robust classification. Experiments conducted on the Indian Pines and Salinas datasets show state-of-the-art performance, achieving overall accuracies of 99.71% and 99.96%, significantly surpassing HybridSN, SSRN, and SpectralFormer. The learned embeddings exhibit superior inter-class separability and compact intra-class clustering, validating CLAReSNet's effectiveness under limited samples and severe class imbalance.

</details>


### [731] [MSLoRA: Multi-Scale Low-Rank Adaptation via Attention Reweighting](https://arxiv.org/abs/2511.12400)
*Xu Yang,Gady Agam*

Main category: cs.CV

TL;DR: 介绍了MSLoRA，一种骨干网络无关、参数高效的适配器，能提升视觉任务迁移性能。


<details>
  <summary>Details</summary>
Motivation: 现有低秩自适应方法多局限于视觉Transformer，难以跨架构泛化，需通用方法。

Method: 结合低秩线性投影和多尺度非线性变换，通过逐点相乘和残差连接融合，调制空间和通道注意力。

Result: 在分类、检测和分割任务上提升迁移性能，仅用不到5%骨干参数，优化稳定、收敛快、跨架构泛化强。

Conclusion: MSLoRA通过重新加权而非重新调参，为冻结视觉骨干网络提供简单通用的高效自适应方法。

Abstract: We introduce MSLoRA, a backbone-agnostic, parameter-efficient adapter that reweights feature responses rather
  than re-tuning the underlying backbone. Existing low-rank adaptation methods are mostly confined to vision
  transformers (ViTs) and struggle to generalize across architectures. MSLoRA unifies adaptation for both convolutional neural networks (CNNs) and
  ViTs by combining a low-rank linear projection with a multi-scale nonlinear transformation that jointly
  modulates spatial and channel attention. The two components are fused through pointwise multiplication and
  a residual connection, yielding a lightweight module that shifts feature attention while keeping pretrained
  weights frozen.
  Extensive experiments demonstrate that MSLoRA consistently improves transfer performance on classification,
  detection, and segmentation tasks with roughly less than 5\% of backbone parameters.
  The design further enables stable optimization, fast convergence, and strong cross-architecture
  generalization. By reweighting rather than re-tuning, MSLoRA provides a simple and universal approach
  for efficient adaptation of frozen vision backbones.

</details>


### [732] [MFI-ResNet: Efficient ResNet Architecture Optimization via MeanFlow Compression and Selective Incubation](https://arxiv.org/abs/2511.12422)
*Nuolin Sun,Linyuan Wang,Haonan Wei,Lei Li,Bin Yan*

Main category: cs.CV

TL;DR: 本文受MeanFlow启发提出MFI - ResNet，采用压缩 - 扩展策略提升参数效率和判别性能，实验显示在CIFAR数据集上效果良好。


<details>
  <summary>Details</summary>
Motivation: 受MeanFlow单步生成建模启发，希望提升ResNet的参数效率和判别性能。

Method: 提出MFI - ResNet，采用压缩 - 扩展策略，压缩阶段简化ResNet结构为MeanFlow模块，扩展阶段对前三个阶段扩展并微调模型。

Result: 在CIFAR - 10和CIFAR - 100数据集上，MFI - ResNet相比ResNet - 50参数减少46.28%和45.59%，准确率分别提升0.23%和0.17%。

Conclusion: 生成流场能有效表征ResNet的特征转换过程，为理解生成建模和判别学习关系提供新视角。

Abstract: ResNet has achieved tremendous success in computer vision through its residual connection mechanism. ResNet can be viewed as a discretized form of ordinary differential equations (ODEs). From this perspective, the multiple residual blocks within a single ResNet stage essentially perform multi-step discrete iterations of the feature transformation for that stage. The recently proposed flow matching model, MeanFlow, enables one-step generative modeling by learning the mean velocity field to transform distributions. Inspired by this, we propose MeanFlow-Incubated ResNet (MFI-ResNet), which employs a compression-expansion strategy to jointly improve parameter efficiency and discriminative performance. In the compression phase, we simplify the multi-layer structure within each ResNet stage to one or two MeanFlow modules to construct a lightweight meta model. In the expansion phase, we apply a selective incubation strategy to the first three stages, expanding them to match the residual block configuration of the baseline ResNet model, while keeping the last stage in MeanFlow form, and fine-tune the incubated model. Experimental results show that on CIFAR-10 and CIFAR-100 datasets, MFI-ResNet achieves remarkable parameter efficiency, reducing parameters by 46.28% and 45.59% compared to ResNet-50, while still improving accuracy by 0.23% and 0.17%, respectively. This demonstrates that generative flow-fields can effectively characterize the feature transformation process in ResNet, providing a new perspective for understanding the relationship between generative modeling and discriminative learning.

</details>


### [733] [Real-Time Drivers' Drowsiness Detection and Analysis through Deep Learning](https://arxiv.org/abs/2511.12438)
*ANK Zaman,Prosenjit Chatterjee,Rajat Sharma*

Main category: cs.CV

TL;DR: 本文开发基于DCNNs和OpenCV的实时驾驶员困倦检测系统，可有效检测困倦并报警，在两个数据集上准确率高。


<details>
  <summary>Details</summary>
Motivation: 长途驾驶易使驾驶员困倦，威胁自身和他人安全，需要实时检测系统。

Method: 利用实时摄像头采集驾驶员面部图像，用OpenCV检测面部特征，DCNNs框架利用预训练模型检测困倦，若检测到则实时报警。

Result: 提出的DCNNs嵌入式困倦检测模型在NTHU - DDD数据集和Yawn - Eye - Dataset上困倦检测分类准确率分别达99.6%和97%。

Conclusion: 该技术能拯救无辜生命，是一种非侵入性、廉价且有效的困倦检测方法。

Abstract: A long road trip is fun for drivers. However, a long drive for days can be tedious for a driver to accommodate stringent deadlines to reach distant destinations. Such a scenario forces drivers to drive extra miles, utilizing extra hours daily without sufficient rest and breaks. Once a driver undergoes such a scenario, it occasionally triggers drowsiness during driving. Drowsiness in driving can be life-threatening to any individual and can affect other drivers' safety; therefore, a real-time detection system is needed. To identify fatigued facial characteristics in drivers and trigger the alarm immediately, this research develops a real-time driver drowsiness detection system utilizing deep convolutional neural networks (DCNNs) and OpenCV.Our proposed and implemented model takes real- time facial images of a driver using a live camera and utilizes a Python-based library named OpenCV to examine the facial images for facial landmarks like sufficient eye openings and yawn-like mouth movements. The DCNNs framework then gathers the data and utilizes a per-trained model to detect the drowsiness of a driver using facial landmarks. If the driver is identified as drowsy, the system issues a continuous alert in real time, embedded in the Smart Car technology.By potentially saving innocent lives on the roadways, the proposed technique offers a non-invasive, inexpensive, and cost-effective way to identify drowsiness. Our proposed and implemented DCNNs embedded drowsiness detection model successfully react with NTHU-DDD dataset and Yawn-Eye-Dataset with drowsiness detection classification accuracy of 99.6% and 97% respectively.

</details>


### [734] [MaskAnyNet: Rethinking Masked Image Regions as Valuable Information in Supervised Learning](https://arxiv.org/abs/2511.12480)
*Jingshan Hong,Haigen Hu,Huihuang Zhang,Qianwei Zhou,Zhao Li*

Main category: cs.CV

TL;DR: 文章指出传统图像掩码存在问题，受MIM启发提出MaskAnyNet，结合掩码与重学习机制，实验显示有增益并提升语义多样性。


<details>
  <summary>Details</summary>
Motivation: 传统图像掩码存在丢弃像素未充分利用、可能移除关键特征的问题，而MIM显示掩码区域有潜在语义多样性，因此重新审视图像掩码方法。

Method: 提出MaskAnyNet，将掩码与重学习机制结合，利用可见和掩码信息，可扩展到任何模型的额外分支联合学习。

Result: 在CNN和Transformer骨干网络的多个基准测试中取得一致增益。

Conclusion: 所提方法通过重用掩码内容提高了语义多样性。

Abstract: In supervised learning, traditional image masking faces two key issues: (i) discarded pixels are underutilized, leading to a loss of valuable contextual information; (ii) masking may remove small or critical features, especially in fine-grained tasks. In contrast, masked image modeling (MIM) has demonstrated that masked regions can be reconstructed from partial input, revealing that even incomplete data can exhibit strong contextual consistency with the original image. This highlights the potential of masked regions as sources of semantic diversity. Motivated by this, we revisit the image masking approach, proposing to treat masked content as auxiliary knowledge rather than ignored. Based on this, we propose MaskAnyNet, which combines masking with a relearning mechanism to exploit both visible and masked information. It can be easily extended to any model with an additional branch to jointly learn from the recomposed masked region. This approach leverages the semantic diversity of the masked regions to enrich features and preserve fine-grained details. Experiments on CNN and Transformer backbones show consistent gains across multiple benchmarks. Further analysis confirms that the proposed method improves semantic diversity through the reuse of masked content.

</details>


### [735] [Fine-Grained Representation for Lane Topology Reasoning](https://arxiv.org/abs/2511.12590)
*Guoqing Xu,Yiheng Li,Yang Yang*

Main category: cs.CV

TL;DR: 提出细粒度车道拓扑推理框架TopoFG，分三阶段建模，实验达SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以准确建模复杂车道结构，导致拓扑预测不可靠。

Method: 将BEV特征到拓扑预测过程分为HPE、RFD、RBTR三阶段，集成空间和顺序先验到细粒度查询，对边界点拓扑推理应用去噪策略。

Result: 在OpenLane - V2基准测试中，TopoFG在subsetA的OLS为48.0%，subsetB为45.4%，达SOTA性能。

Conclusion: 通过集成先验和去噪策略，方法能精确建模复杂车道结构，实现可靠拓扑预测。

Abstract: Precise modeling of lane topology is essential for autonomous driving, as it directly impacts navigation and control decisions.Existing methods typically represent each lane with a single query and infer topological connectivity based on the similarity between lane queries.However, this kind of design struggles to accurately model complex lane structures, leading to unreliable topology prediction.In this view, we propose a Fine-Grained lane topology reasoning framework (TopoFG).It divides the procedure from bird's-eye-view (BEV) features to topology prediction via fine-grained queries into three phases, i.e., Hierarchical Prior Extractor (HPE), Region-Focused Decoder (RFD), and Robust Boundary-Point Topology Reasoning (RBTR).Specifically, HPE extracts global spatial priors from the BEV mask and local sequential priors from in-lane keypoint sequences to guide subsequent fine-grained query modeling.RFD constructs fine-grained queries by integrating the spatial and sequential priors. It then samples reference points in RoI regions of the mask and applies cross-attention with BEV features to refine the query representations of each lane.RBTR models lane connectivity based on boundary-point query features and further employs a topological denoising strategy to reduce matching ambiguity.By integrating spatial and sequential priors into fine-grained queries and applying a denoising strategy to boundary-point topology reasoning, our method precisely models complex lane structures and delivers trustworthy topology predictions.Extensive experiments on the OpenLane-V2 benchmark demonstrate that TopoFG achieves new state-of-the-art performance, with an OLS of 48.0% on subsetA and 45.4% on subsetB.

</details>


### [736] [OPFormer: Object Pose Estimation leveraging foundation model with geometric encoding](https://arxiv.org/abs/2511.12614)
*Artem Moroz,Vít Zeman,Martin Mikšík,Elizaveta Isianova,Miroslav David,Pavel Burget,Varun Burde*

Main category: cs.CV

TL;DR: 本文提出统一端到端框架，集成目标检测与姿态估计，在BOP基准测试中展现准确性与效率平衡。


<details>
  <summary>Details</summary>
Motivation: 提出一个能无缝集成目标检测和姿态估计，并具有通用导入流程的统一框架。

Method: 先在导入阶段从3D CAD模型或多视图图像重建神经表示生成目标表示；用CNOS检测器定位目标；用OPFormer模块推断6D姿态，其核心是基于Transformer架构，利用基础模型提取特征，结合多模板视图编码和3D几何先验，解码器建立2D - 3D对应关系确定最终姿态。

Result: 在具有挑战性的BOP基准测试中，集成系统在准确性和效率之间取得了良好平衡。

Conclusion: 该集成系统在基于模型和无模型场景中都具有实际应用价值。

Abstract: We introduce a unified, end-to-end framework that seamlessly integrates object detection and pose estimation with a versatile onboarding process. Our pipeline begins with an onboarding stage that generates object representations from either traditional 3D CAD models or, in their absence, by rapidly reconstructing a high-fidelity neural representation (NeRF) from multi-view images. Given a test image, our system first employs the CNOS detector to localize target objects. For each detection, our novel pose estimation module, OPFormer, infers the precise 6D pose. The core of OPFormer is a transformer-based architecture that leverages a foundation model for robust feature extraction. It uniquely learns a comprehensive object representation by jointly encoding multiple template views and enriches these features with explicit 3D geometric priors using Normalized Object Coordinate Space (NOCS). A decoder then establishes robust 2D-3D correspondences to determine the final pose. Evaluated on the challenging BOP benchmarks, our integrated system demonstrates a strong balance between accuracy and efficiency, showcasing its practical applicability in both model-based and model-free scenarios.

</details>


### [737] [C3Net: Context-Contrast Network for Camouflaged Object Detection](https://arxiv.org/abs/2511.12627)
*Baber Jan,Aiman H. El-Maleh,Abdul Jabbar Siddiqui,Abdul Bais,Saeed Anwar*

Main category: cs.CV

TL;DR: 本文指出伪装目标检测的六大挑战，提出C3Net解决这些问题，取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统分割方法和现代基础模型在伪装目标检测任务中表现不佳，需解决六大挑战。

Method: 提出C3Net，采用双路径解码器架构，包括边缘细化路径和上下文定位路径，并通过注意力融合模块结合。

Result: C3Net在COD10K、CAMO和NC4K上取得S-measures分别为0.898、0.904和0.913的SOTA性能，且处理高效。

Conclusion: 复杂的检测挑战需要架构创新，专用组件协同工作可实现全面覆盖。

Abstract: Camouflaged object detection identifies objects that blend seamlessly with their surroundings through similar colors, textures, and patterns. This task challenges both traditional segmentation methods and modern foundation models, which fail dramatically on camouflaged objects. We identify six fundamental challenges in COD: Intrinsic Similarity, Edge Disruption, Extreme Scale Variation, Environmental Complexities, Contextual Dependencies, and Salient-Camouflaged Object Disambiguation. These challenges frequently co-occur and compound the difficulty of detection, requiring comprehensive architectural solutions. We propose C3Net, which addresses all challenges through a specialized dual-pathway decoder architecture. The Edge Refinement Pathway employs gradient-initialized Edge Enhancement Modules to recover precise boundaries from early features. The Contextual Localization Pathway utilizes our novel Image-based Context Guidance mechanism to achieve intrinsic saliency suppression without external models. An Attentive Fusion Module synergistically combines the two pathways via spatial gating. C3Net achieves state-of-the-art performance with S-measures of 0.898 on COD10K, 0.904 on CAMO, and 0.913 on NC4K, while maintaining efficient processing. C3Net demonstrates that complex, multifaceted detection challenges require architectural innovation, with specialized components working synergistically to achieve comprehensive coverage beyond isolated improvements. Code, model weights, and results are available at https://github.com/Baber-Jan/C3Net.

</details>


### [738] [Multivariate Diffusion Transformer with Decoupled Attention for High-Fidelity Mask-Text Collaborative Facial Generation](https://arxiv.org/abs/2511.12631)
*Yushe Cao,Dianxi Shi,Xing Fu,Xuechao Zou,Haikuo Peng,Xueqi Li,Chun Yu,Junliang Xing*

Main category: cs.CV

TL;DR: 提出MDiTFace框架解决多模态面部生成中特征融合问题，实验显示其性能优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 传统特征融合方法在多模态面部生成中无法实现有效跨模态交互，导致生成效果不佳。

Method: 引入MDiTFace框架，采用统一分词策略处理输入，通过新设计的多元变压器块促进特征交互，设计解耦注意力机制减少计算开销。

Result: MDiTFace在面部保真度和条件一致性方面显著优于其他竞争方法。

Conclusion: MDiTFace框架能有效解决多模态面部生成的特征融合问题，提升生成效果。

Abstract: While significant progress has been achieved in multimodal facial generation using semantic masks and textual descriptions, conventional feature fusion approaches often fail to enable effective cross-modal interactions, thereby leading to suboptimal generation outcomes. To address this challenge, we introduce MDiTFace--a customized diffusion transformer framework that employs a unified tokenization strategy to process semantic mask and text inputs, eliminating discrepancies between heterogeneous modality representations. The framework facilitates comprehensive multimodal feature interaction through stacked, newly designed multivariate transformer blocks that process all conditions synchronously. Additionally, we design a novel decoupled attention mechanism by dissociating implicit dependencies between mask tokens and temporal embeddings. This mechanism segregates internal computations into dynamic and static pathways, enabling caching and reuse of features computed in static pathways after initial calculation, thereby reducing additional computational overhead introduced by mask condition by over 94% while maintaining performance. Extensive experiments demonstrate that MDiTFace significantly outperforms other competing methods in terms of both facial fidelity and conditional consistency.

</details>


### [739] [BridgeEQA: Virtual Embodied Agents for Real Bridge Inspections](https://arxiv.org/abs/2511.12676)
*Subin Varghese,Joshua Gao,Asad Ur Rahman,Vedhus Hoskere*

Main category: cs.CV

TL;DR: 提出桥梁基础设施检查作为开放词汇具身问答（EQA）领域，引入BridgeEQA基准和新评估指标，提出EMVR方法提升性能并开源数据和代码。


<details>
  <summary>Details</summary>
Motivation: 解决在现实世界中部署能回答周围环境问题的具身智能体的难题，因缺乏能反映实际操作条件的基准。

Method: 引入BridgeEQA基准，提出新的EQA评估指标Image Citation Relevance，提出Embodied Memory Visual Reasoning（EMVR）方法，将检查表述为基于图像场景图的顺序导航。

Result: 评估显示现有模型在情景记忆EQA设置下有性能差距，EMVR方法比基线表现好。

Conclusion: 桥梁基础设施检查适合作为开放词汇EQA领域，EMVR方法有效提升性能，公开数据集和代码利于后续研究。

Abstract: Deploying embodied agents that can answer questions about their surroundings in realistic real-world settings remains difficult, partly due to the scarcity of benchmarks that faithfully capture practical operating conditions. We propose infrastructure inspection as a compelling domain for open-vocabulary Embodied Question Answering (EQA): it naturally demands multi-scale reasoning, long-range spatial understanding, and complex semantic relationships, while offering unique evaluation advantages via standardized National Bridge Inventory (NBI) condition ratings (0-9), professional inspection reports, and egocentric imagery.
  We introduce BridgeEQA, a benchmark of 2,200 open-vocabulary question-answer pairs (in the style of OpenEQA) grounded in professional inspection reports across 200 real-world bridge scenes with 47.93 images on average per scene. Questions require synthesizing visual evidence across multiple images and aligning responses with NBI condition ratings. We further propose a new EQA metric Image Citation Relevance to evaluate the ability of a model to cite relevant images.
  Evaluations of state-of-the-art vision-language models reveal substantial performance gaps under episodic memory EQA settings. To address this, we propose Embodied Memory Visual Reasoning (EMVR), which formulates inspection as sequential navigation over an image-based scene graph: images are nodes, and an agent takes actions to traverse views, compare evidence, and reason within a Markov decision process. EMVR shows strong performance over the baselines. We publicly release both the dataset and code.

</details>


### [740] [R$^{2}$Seg: Training-Free OOD Medical Tumor Segmentation via Anatomical Reasoning and Statistical Rejection](https://arxiv.org/abs/2511.12691)
*Shuaike Shen,Ke Liu,Jiaqing Xie,Shangde Gao,Chunhua Shen,Ge Liu,Mireia Crispin-Ortuzar,Shangqi Gao*

Main category: cs.CV

TL;DR: 提出无训练框架R²Seg用于鲁棒OOD肿瘤分割，经两阶段处理提升性能，代码开源。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像分割基础模型在OOD转移下易产生碎片化误报问题。

Method: 采用两阶段Reason - and - Reject过程，包括LLM引导的解剖推理规划器定位器官锚点和生成多尺度ROI，以及两样本统计测试过滤候选区域。

Result: 在多中心和多模态肿瘤分割基准上，R²Seg在Dice、特异性和敏感性方面显著优于强基线和原始基础模型。

Conclusion: R²Seg无需参数更新，兼容零更新测试时增强，能有效抑制误报，提升分割性能。

Abstract: Foundation models for medical image segmentation struggle under out-of-distribution (OOD) shifts, often producing fragmented false positives on OOD tumors. We introduce R$^{2}$Seg, a training-free framework for robust OOD tumor segmentation that operates via a two-stage Reason-and-Reject process. First, the Reason step employs an LLM-guided anatomical reasoning planner to localize organ anchors and generate multi-scale ROIs. Second, the Reject step applies two-sample statistical testing to candidates generated by a frozen foundation model (BiomedParse) within these ROIs. This statistical rejection filter retains only candidates significantly different from normal tissue, effectively suppressing false positives. Our framework requires no parameter updates, making it compatible with zero-update test-time augmentation and avoiding catastrophic forgetting. On multi-center and multi-modal tumor segmentation benchmarks, R$^{2}$Seg substantially improves Dice, specificity, and sensitivity over strong baselines and the original foundation models. Code are available at https://github.com/Eurekashen/R2Seg.

</details>


### [741] [HEDGE: Hallucination Estimation via Dense Geometric Entropy for VQA with Vision-Language Models](https://arxiv.org/abs/2511.12693)
*Sushant Gautam,Michael A. Riegler,Pål Halvorsen*

Main category: cs.CV

TL;DR: 提出HEDGE框架用于视觉语言模型幻觉检测，评估不同模型，得出架构、提示等因素影响，提供基准库和代码。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型易产生幻觉的问题。

Method: 结合控制视觉扰动、语义聚类和鲁棒不确定性度量，将采样、失真合成、聚类和度量计算整合为可复现管道。

Result: 不同架构和提示下幻觉检测有不同趋势，VASE度量在特定条件下提供最稳健信号。

Conclusion: HEDGE为评估多模态可靠性提供有原则、考虑计算的基础，提供可复现和可扩展基准库。

Abstract: Vision-language models (VLMs) enable open-ended visual question answering but remain prone to hallucinations. We present HEDGE, a unified framework for hallucination detection that combines controlled visual perturbations, semantic clustering, and robust uncertainty metrics. HEDGE integrates sampling, distortion synthesis, clustering (entailment- and embedding-based), and metric computation into a reproducible pipeline applicable across multimodal architectures.
  Evaluations on VQA-RAD and KvasirVQA-x1 with three representative VLMs (LLaVA-Med, Med-Gemma, Qwen2.5-VL) reveal clear architecture- and prompt-dependent trends. Hallucination detectability is highest for unified-fusion models with dense visual tokenization (Qwen2.5-VL) and lowest for architectures with restricted tokenization (Med-Gemma). Embedding-based clustering often yields stronger separation when applied directly to the generated answers, whereas NLI-based clustering remains advantageous for LLaVA-Med and for longer, sentence-level responses. Across configurations, the VASE metric consistently provides the most robust hallucination signal, especially when paired with embedding clustering and a moderate sampling budget (n ~ 10-15). Prompt design also matters: concise, label-style outputs offer clearer semantic structure than syntactically constrained one-sentence responses.
  By framing hallucination detection as a geometric robustness problem shaped jointly by sampling scale, prompt structure, model architecture, and clustering strategy, HEDGE provides a principled, compute-aware foundation for evaluating multimodal reliability. The hedge-bench PyPI library enables reproducible and extensible benchmarking, with full code and experimental resources available at https://github.com/Simula/HEDGE .

</details>


### [742] [Which Way from B to A: The role of embedding geometry in image interpolation for Stable Diffusion](https://arxiv.org/abs/2511.12757)
*Nicholas Karris,Luke Durell,Javier Flores,Tegan Emerson*

Main category: cs.CV

TL;DR: 发现Stable Diffusion对CLIP嵌入矩阵行有排列不变性，将嵌入视为Wasserstein空间点云，提出用最优传输解决插值问题，实验表明该方法图像插值更平滑。


<details>
  <summary>Details</summary>
Motivation: 基于Stable Diffusion对CLIP嵌入矩阵行的排列不变性，寻找理解嵌入空间几何结构的新视角。

Method: 将嵌入插值问题转化为最优传输问题，计算嵌入间的最短路径。

Result: 通过实验对比，基于最优传输的方法生成的插值图像更平滑。

Conclusion: 将嵌入视为点云能更好地反映和利用嵌入空间的几何结构。

Abstract: It can be shown that Stable Diffusion has a permutation-invariance property with respect to the rows of Contrastive Language-Image Pretraining (CLIP) embedding matrices. This inspired the novel observation that these embeddings can naturally be interpreted as point clouds in a Wasserstein space rather than as matrices in a Euclidean space. This perspective opens up new possibilities for understanding the geometry of embedding space. For example, when interpolating between embeddings of two distinct prompts, we propose reframing the interpolation problem as an optimal transport problem. By solving this optimal transport problem, we compute a shortest path (or geodesic) between embeddings that captures a more natural and geometrically smooth transition through the embedding space. This results in smoother and more coherent intermediate (interpolated) images when rendered by the Stable Diffusion generative model. We conduct experiments to investigate this effect, comparing the quality of interpolated images produced using optimal transport to those generated by other standard interpolation methods. The novel optimal transport--based approach presented indeed gives smoother image interpolations, suggesting that viewing the embeddings as point clouds (rather than as matrices) better reflects and leverages the geometry of the embedding space.

</details>


### [743] [Lightweight Optimal-Transport Harmonization on Edge Devices](https://arxiv.org/abs/2511.12785)
*Maria Larchenko,Dmitry Guskov,Alexander Lobashev,Georgy Derevyanko*

Main category: cs.CV

TL;DR: 提出轻量级方法解决AR色彩协调问题，算法表现佳并发布数据集


<details>
  <summary>Details</summary>
Motivation: 增强现实中色彩协调问题自然存在，但实时解决方案稀缺，现有协调算法未集成到AR管道

Method: 利用经典最优传输理论，训练紧凑编码器预测Monge - Kantorovich传输映射

Result: MKL - Harmonizer算法在真实合成AR图像上取得最佳综合得分

Conclusion: 所提轻量级方法支持设备端推理，可解决AR色彩协调问题，还发布数据集助力研究

Abstract: Color harmonization adjusts the colors of an inserted object so that it perceptually matches the surrounding image, resulting in a seamless composite. The harmonization problem naturally arises in augmented reality (AR), yet harmonization algorithms are not currently integrated into AR pipelines because real-time solutions are scarce. In this work, we address color harmonization for AR by proposing a lightweight approach that supports on-device inference. For this, we leverage classical optimal transport theory by training a compact encoder to predict the Monge-Kantorovich transport map. We benchmark our MKL-Harmonizer algorithm against state-of-the-art methods and demonstrate that for real composite AR images our method achieves the best aggregated score. We release our dedicated AR dataset of composite images with pixel-accurate masks and data-gathering toolkit to support further data acquisition by researchers.

</details>


### [744] [MSRNet: A Multi-Scale Recursive Network for Camouflaged Object Detection](https://arxiv.org/abs/2511.12810)
*Leena Alghamdi,Muhammad Usman,Hafeez Anwar,Abdul Bais,Saeed Anwar*

Main category: cs.CV

TL;DR: 提出多尺度递归网络（MSRNet）用于伪装目标检测，结合多尺度学习和递归特征优化，在多个数据集取得优异结果。


<details>
  <summary>Details</summary>
Motivation: 现有伪装目标检测方法在复杂场景，尤其是小目标和多目标检测上表现不佳，有改进空间。

Method: 提出多尺度递归网络，通过金字塔视觉Transformer骨干提取多尺度特征，用基于注意力的尺度集成单元合并特征；解码器用多粒度融合单元递归细化特征；开发递归反馈解码策略增强全局上下文理解。

Result: 在两个基准数据集上达到了最先进的结果，在另外两个数据集上排名第二。

Conclusion: 所提方法通过联合利用多尺度学习和递归特征优化，能有效检测小目标和多目标伪装物体，提升了伪装目标检测性能。

Abstract: Camouflaged object detection is an emerging and challenging computer vision task that requires identifying and segmenting objects that blend seamlessly into their environments due to high similarity in color, texture, and size. This task is further complicated by low-light conditions, partial occlusion, small object size, intricate background patterns, and multiple objects. While many sophisticated methods have been proposed for this task, current methods still struggle to precisely detect camouflaged objects in complex scenarios, especially with small and multiple objects, indicating room for improvement. We propose a Multi-Scale Recursive Network that extracts multi-scale features via a Pyramid Vision Transformer backbone and combines them via specialized Attention-Based Scale Integration Units, enabling selective feature merging. For more precise object detection, our decoder recursively refines features by incorporating Multi-Granularity Fusion Units. A novel recursive-feedback decoding strategy is developed to enhance global context understanding, helping the model overcome the challenges in this task. By jointly leveraging multi-scale learning and recursive feature optimization, our proposed method achieves performance gains, successfully detecting small and multiple camouflaged objects. Our model achieves state-of-the-art results on two benchmark datasets for camouflaged object detection and ranks second on the remaining two. Our codes, model weights, and results are available at \href{https://github.com/linaagh98/MSRNet}{https://github.com/linaagh98/MSRNet}.

</details>


### [745] [SAGA: Source Attribution of Generative AI Videos](https://arxiv.org/abs/2511.12834)
*Rohit Kundu,Vishal Mohanty,Hao Xiong,Shan Jia,Athula Balachandran,Amit K. Roy-Chowdhury*

Main category: cs.CV

TL;DR: 本文提出SAGA框架用于大规模AI生成视频源归因，有独特多粒度归因能力，采用新架构和策略，实验证明其为合成视频溯源设新基准。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的发展使合成视频滥用风险增加，现有二进制真假检测器不足，需大规模AI生成视频源归因方法。

Method: 引入SAGA框架，采用新视频变压器架构，结合强大视觉基础模型特征；提出数据高效的预训练和归因策略；提出Temporal Attention Signatures (T - Sigs)可解释性方法。

Result: SAGA仅用每类0.5%的源标记数据就能达到全监督性能，在公共数据集和跨领域场景实验中为合成视频溯源设定新基准。

Conclusion: SAGA框架为法医和监管应用提供关键且可解释的见解。

Abstract: The proliferation of generative AI has led to hyper-realistic synthetic videos, escalating misuse risks and outstripping binary real/fake detectors. We introduce SAGA (Source Attribution of Generative AI videos), the first comprehensive framework to address the urgent need for AI-generated video source attribution at a large scale. Unlike traditional detection, SAGA identifies the specific generative model used. It uniquely provides multi-granular attribution across five levels: authenticity, generation task (e.g., T2V/I2V), model version, development team, and the precise generator, offering far richer forensic insights. Our novel video transformer architecture, leveraging features from a robust vision foundation model, effectively captures spatio-temporal artifacts. Critically, we introduce a data-efficient pretrain-and-attribute strategy, enabling SAGA to achieve state-of-the-art attribution using only 0.5\% of source-labeled data per class, matching fully supervised performance. Furthermore, we propose Temporal Attention Signatures (T-Sigs), a novel interpretability method that visualizes learned temporal differences, offering the first explanation for why different video generators are distinguishable. Extensive experiments on public datasets, including cross-domain scenarios, demonstrate that SAGA sets a new benchmark for synthetic video provenance, providing crucial, interpretable insights for forensic and regulatory applications.

</details>


### [746] [Video Finetuning Improves Reasoning Between Frames](https://arxiv.org/abs/2511.12868)
*Ruiqi Yang,Tian Yun,Zihan Wang,Ellie Pavlick*

Main category: cs.CV

TL;DR: 研究视频微调对多模态大语言模型的影响，提出vCoT方法，对比不同模型表现。


<details>
  <summary>Details</summary>
Motivation: 探究视频微调给多模态大语言模型带来的影响。

Method: 提出Visual Chain-of-Thought (vCoT)，生成连续帧间过渡事件描述，对比不同模型。

Result: vCoT显著提升图像模型在长视频问答性能，对视频微调模型提升小，视频模型能将时间推理能力迁移到静态场景。

Conclusion: 视频微调模型已隐式捕获帧间过渡，且具有更好的时间推理迁移能力。

Abstract: Multimodal large language models (LLMs) have made rapid progress in visual understanding, yet their extension from images to videos often reduces to a naive concatenation of frame tokens. In this work, we investigate what video finetuning brings to multimodal LLMs. We propose Visual Chain-of-Thought (vCoT), an explicit reasoning process that generates transitional event descriptions between consecutive frames. Using vCoT, we systematically compare image-only LVLMs with their video-finetuned counterparts, both with and without access to these transitional cues. Our experiments show that vCoT significantly improves the performance of image-only models on long-form video question answering, while yielding only marginal gains for video-finetuned models. This suggests that the latter already capture frame-to-frame transitions implicitly. Moreover, we find that video models transfer this temporal reasoning ability to purely static settings, outperforming image models' baselines on relational visual reasoning tasks.

</details>


### [747] [DeepSport: A Multimodal Large Language Model for Comprehensive Sports Video Reasoning via Agentic Reinforcement Learning](https://arxiv.org/abs/2511.12908)
*Junbo Zou,Haotian Xia,Zhen Ye,Shengjie Zhang,Christopher Lai,Vicente Ordonez,Weining Shen,Hanjie Chen*

Main category: cs.CV

TL;DR: 本文提出用于多任务、多体育项目视频理解的端到端训练MLLM框架DeepSport，采用新方法优化推理过程，实验表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有体育视频理解研究狭窄，存在单体育项目中心、任务特定、缺乏学习推理过程等问题，需新方法解决。

Method: 提出DeepSport框架，采用动态询问内容的主动迭代推理范式，设计数据蒸馏管道合成CoT轨迹，使用两阶段训练策略（SFT和RL）。

Result: 在6.7k问题测试基准上，DeepSport达到了最先进性能，显著优于专有模型和开源模型基线。

Conclusion: 为特定领域视频推理奠定新基础，可应对多样体育的复杂性。

Abstract: Sports video understanding presents unique challenges, requiring models to perceive high-speed dynamics, comprehend complex rules, and reason over long temporal contexts. While Multimodal Large Language Models (MLLMs) have shown promise in genral domains, the current state of research in sports remains narrowly focused: existing approaches are either single-sport centric, limited to specific tasks, or rely on training-free paradigms that lack robust, learned reasoning process. To address this gap, we introduce DeepSport, the first end-to-end trained MLLM framework designed for multi-task, multi-sport video understanding. DeepSport shifts the paradigm from passive frame processing to active, iterative reasoning, empowering the model to ``think with videos'' by dynamically interrogating content via a specialized frame-extraction tool. To enable this, we propose a data distillation pipeline that synthesizes high-quality Chain-of-Thought (CoT) trajectories from 10 diverse data source, creating a unified resource of 78k training data. We then employ a two-stage training strategy, Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) with a novel gated tool-use reward, to optimize the model's reasoning process. Extensive experiments on the testing benchmark of 6.7k questions demonstrate that DeepSport achieves state-of-the-art performance, significantly outperforming baselines of both proprietary model and open-source models. Our work establishes a new foundation for domain-specific video reasoning to address the complexities of diverse sports.

</details>


### [748] [MP-GFormer: A 3D-Geometry-Aware Dynamic Graph Transformer Approach for Machining Process Planning](https://arxiv.org/abs/2511.11837)
*Fatemeh Elhambakhsh,Gaurav Ameta,Aditi Roy,Hyunwoong Ko*

Main category: cs.CV

TL;DR: 提出3D几何感知动态图变换器MP - GFormer预测加工操作序列，在合成数据集上准确率优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有动态图学习方法在加工过程规划中未融入零件3D几何信息，缺乏领域感知。

Method: 提出MP - GFormer，通过注意力机制将3D几何表示集成到动态图学习中，利用立体光刻表面网格和边界表示法。

Result: 在合成数据集上，主操作和子操作预测准确率分别比现有方法提高24%和36%。

Conclusion: MP - GFormer能有效解决现有方法在加工操作序列预测中缺乏3D几何信息的问题，提升预测准确率。

Abstract: Machining process planning (MP) is inherently complex due to structural and geometrical dependencies among part features and machining operations. A key challenge lies in capturing dynamic interdependencies that evolve with distinct part geometries as operations are performed. Machine learning has been applied to address challenges in MP, such as operation selection and machining sequence prediction. Dynamic graph learning (DGL) has been widely used to model dynamic systems, thanks to its ability to integrate spatio-temporal relationships. However, in MP, while existing DGL approaches can capture these dependencies, they fail to incorporate three-dimensional (3D) geometric information of parts and thus lack domain awareness in predicting machining operation sequences. To address this limitation, we propose MP-GFormer, a 3D-geometry-aware dynamic graph transformer that integrates evolving 3D geometric representations into DGL through an attention mechanism to predict machining operation sequences. Our approach leverages StereoLithography surface meshes representing the 3D geometry of a part after each machining operation, with the boundary representation method used for the initial 3D designs. We evaluate MP-GFormer on a synthesized dataset and demonstrate that the method achieves improvements of 24\% and 36\% in accuracy for main and sub-operation predictions, respectively, compared to state-of-the-art approaches.

</details>


### [749] [PFAvatar: Pose-Fusion 3D Personalized Avatar Reconstruction from Real-World Outfit-of-the-Day Photos](https://arxiv.org/abs/2511.12935)
*Dianbing Xi,Guoyuan An,Jingsen Zhu,Zhijian Liu,Yuan Liu,Ruiyuan Zhang,Jiayuan Lu,Rui Wang,Yuchi Huo*

Main category: cs.CV

TL;DR: 提出PFAvatar方法从OOTD照片重建高质量3D头像，分两阶段，速度快且性能优，支持下游应用。


<details>
  <summary>Details</summary>
Motivation: 解决从展示多样姿势、遮挡和复杂背景的OOTD照片中重建高质量3D头像的问题。

Method: 分两阶段，一是微调姿势感知扩散模型，结合预训练ControlNet和CPPL避免分解直接建模全身外观；二是引入基于NeRF的头像表示，通过规范SMPL - X空间采样和Multi - Resolution 3D - SDS优化。

Result: 方法5分钟完成个性化，速度提升48倍，在重建保真度、细节保留和对遮挡/截断的鲁棒性上优于现有方法。

Conclusion: PFAvatar推进了从真实OOTD相册生成实用3D头像，支持下游应用，有很强的通用性和实用价值。

Abstract: We propose PFAvatar (Pose-Fusion Avatar), a new method that reconstructs high-quality 3D avatars from ``Outfit of the Day'' (OOTD) photos, which exhibit diverse poses, occlusions, and complex backgrounds. Our method consists of two stages: (1) fine-tuning a pose-aware diffusion model from few-shot OOTD examples and (2) distilling a 3D avatar represented by a neural radiance field (NeRF). In the first stage, unlike previous methods that segment images into assets (e.g., garments, accessories) for 3D assembly, which is prone to inconsistency, we avoid decomposition and directly model the full-body appearance. By integrating a pre-trained ControlNet for pose estimation and a novel Condition Prior Preservation Loss (CPPL), our method enables end-to-end learning of fine details while mitigating language drift in few-shot training. Our method completes personalization in just 5 minutes, achieving a 48$\times$ speed-up compared to previous approaches. In the second stage, we introduce a NeRF-based avatar representation optimized by canonical SMPL-X space sampling and Multi-Resolution 3D-SDS. Compared to mesh-based representations that suffer from resolution-dependent discretization and erroneous occluded geometry, our continuous radiance field can preserve high-frequency textures (e.g., hair) and handle occlusions correctly through transmittance. Experiments demonstrate that PFAvatar outperforms state-of-the-art methods in terms of reconstruction fidelity, detail preservation, and robustness to occlusions/truncations, advancing practical 3D avatar generation from real-world OOTD albums. In addition, the reconstructed 3D avatar supports downstream applications such as virtual try-on, animation, and human video reenactment, further demonstrating the versatility and practical value of our approach.

</details>


### [750] [EndoSight AI: Deep Learning-Driven Real-Time Gastrointestinal Polyp Detection and Segmentation for Enhanced Endoscopic Diagnostics](https://arxiv.org/abs/2511.12962)
*Daniel Cavadia*

Main category: cs.CV

TL;DR: 提出EndoSight AI深度学习架构用于肠胃息肉检测，在公开数据集上取得高准确率和实时推理速度，有望提升诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 精确实时检测肠胃息肉对结直肠癌早诊和预防至关重要。

Method: 开发EndoSight AI架构，利用公开数据集训练，结合临床相关指标和热感知程序。

Result: 息肉检测平均精度均值达88.3%，分割Dice系数达69%，GPU上推理速度超35帧每秒。

Conclusion: 该集成AI解决方案可无缝部署在内镜工作流程，有望提升诊断准确性和临床决策水平。

Abstract: Precise and real-time detection of gastrointestinal polyps during endoscopic procedures is crucial for early diagnosis and prevention of colorectal cancer. This work presents EndoSight AI, a deep learning architecture developed and evaluated independently to enable accurate polyp localization and detailed boundary delineation. Leveraging the publicly available Hyper-Kvasir dataset, the system achieves a mean Average Precision (mAP) of 88.3% for polyp detection and a Dice coefficient of up to 69% for segmentation, alongside real-time inference speeds exceeding 35 frames per second on GPU hardware. The training incorporates clinically relevant performance metrics and a novel thermal-aware procedure to ensure model robustness and efficiency. This integrated AI solution is designed for seamless deployment in endoscopy workflows, promising to advance diagnostic accuracy and clinical decision-making in gastrointestinal healthcare.

</details>


### [751] [CalibrateMix: Guided-Mixup Calibration of Image Semi-Supervised Models](https://arxiv.org/abs/2511.12964)
*Mehrab Mustafy Rahman,Jayanth Mohan,Tiberiu Sosea,Cornelia Caragea*

Main category: cs.CV

TL;DR: 本文提出CalibrateMix方法改善半监督学习模型校准，实验显示其比现有方法有更低校准误差和更高准确率。


<details>
  <summary>Details</summary>
Motivation: 现有半监督学习方法校准不佳，基于伪标签的随机mixup有挑战，半监督设置下模型校准研究不足。

Method: 引入CalibrateMix方法，利用标记和未标记样本训练动态识别易学习和难学习样本，进行有针对性的样本混合。

Result: 在多个基准图像数据集实验中，该方法比现有半监督方法有更低预期校准误差和更高准确率。

Conclusion: CalibrateMix方法能改善半监督学习模型校准，同时保持或提升分类准确率。

Abstract: Semi-supervised learning (SSL) has demonstrated high performance in image classification tasks by effectively utilizing both labeled and unlabeled data. However, existing SSL methods often suffer from poor calibration, with models yielding overconfident predictions that misrepresent actual prediction likelihoods. Recently, neural networks trained with {\tt mixup} that linearly interpolates random examples from the training set have shown better calibration in supervised settings. However, calibration of neural models remains under-explored in semi-supervised settings. Although effective in supervised model calibration, random mixup of pseudolabels in SSL presents challenges due to the overconfidence and unreliability of pseudolabels. In this work, we introduce CalibrateMix, a targeted mixup-based approach that aims to improve the calibration of SSL models while maintaining or even improving their classification accuracy. Our method leverages training dynamics of labeled and unlabeled samples to identify ``easy-to-learn'' and ``hard-to-learn'' samples, which in turn are utilized in a targeted mixup of easy and hard samples. Experimental results across several benchmark image datasets show that our method achieves lower expected calibration error (ECE) and superior accuracy compared to existing SSL approaches.

</details>


### [752] [UNSEEN: Enhancing Dataset Pruning from a Generalization Perspective](https://arxiv.org/abs/2511.12988)
*Furui Xu,Shaobo Wang,Jiajun Zhang,Chenghao Sun,Haixiang Tang,Linfeng Zhang*

Main category: cs.CV

TL;DR: 深度学习数据集规模增长带来计算挑战，现有数据集剪枝方法有局限，本文提出UNSEEN框架，可集成到现有方法，扩展到多步场景，实验表明其显著优于现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有数据集剪枝方法以拟合为中心导致样本分数区分度低、单步且视角有限的问题。

Method: 从泛化角度进行数据集剪枝，提出UNSEEN框架，可集成到现有方法，将其扩展到多步场景并提出增量选择技术动态优化核心集质量。

Result: 在CIFAR - 10、CIFAR - 100和ImageNet - 1K上显著优于现有SOTA方法，在ImageNet - 1K上减少30%训练数据实现无损性能。

Conclusion: 所提方法有效解决现有数据集剪枝方法的问题，在多个数据集上表现出色。

Abstract: The growing scale of datasets in deep learning has introduced significant computational challenges. Dataset pruning addresses this challenge by constructing a compact but informative coreset from the full dataset with comparable performance. Previous approaches typically establish scoring metrics based on specific criteria to identify representative samples. However, these methods predominantly rely on sample scores obtained from the model's performance during the training (i.e., fitting) phase. As scoring models achieve near-optimal performance on training data, such fitting-centric approaches induce a dense distribution of sample scores within a narrow numerical range. This concentration reduces the distinction between samples and hinders effective selection. To address this challenge, we conduct dataset pruning from the perspective of generalization, i.e., scoring samples based on models not exposed to them during training. We propose a plug-and-play framework, UNSEEN, which can be integrated into existing dataset pruning methods. Additionally, conventional score-based methods are single-step and rely on models trained solely on the complete dataset, providing limited perspective on the importance of samples. To address this limitation, we scale UNSEEN to multi-step scenarios and propose an incremental selection technique through scoring models trained on varying coresets, and optimize the quality of the coreset dynamically. Extensive experiments demonstrate that our method significantly outperforms existing state-of-the-art (SOTA) methods on CIFAR-10, CIFAR-100, and ImageNet-1K. Notably, on ImageNet-1K, UNSEEN achieves lossless performance while reducing training data by 30\%.

</details>


### [753] [SAGE: Spuriousness-Aware Guided Prompt Exploration for Mitigating Multimodal Bias](https://arxiv.org/abs/2511.13005)
*Wenqian Ye,Di Wang,Guangtao Zheng,Bohan Liu,Aidong Zhang*

Main category: cs.CV

TL;DR: 文章指出CLIP模型存在多模态虚假偏差问题，提出无需训练的SAGE方法缓解偏差，实验表明其能提升零样本性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: CLIP模型存在多模态虚假偏差，影响其在分布外数据上的鲁棒性，且现有缓解方法影响CLIP的即用性。

Method: 提出Spuriousness - Aware Guided Exploration (SAGE)方法，通过引导式提示选择缓解虚假偏差，无需训练、微调或外部注释。

Result: 在四个真实世界基准数据集和五个流行骨干模型上的大量实验表明，SAGE持续提升零样本性能和泛化能力，优于先前零样本方法。

Conclusion: SAGE是一种简单有效的方法，无需外部知识或模型更新，能缓解多模态虚假偏差，提升零样本性能和泛化能力。

Abstract: Large vision-language models, such as CLIP, have shown strong zero-shot classification performance by aligning images and text in a shared embedding space. However, CLIP models often develop multimodal spurious biases, which is the undesirable tendency to rely on spurious features. For example, CLIP may infer object types in images based on frequently co-occurring backgrounds rather than the object's core features. This bias significantly impairs the robustness of pre-trained CLIP models on out-of-distribution data, where such cross-modal associations no longer hold. Existing methods for mitigating multimodal spurious bias typically require fine-tuning on downstream data or prior knowledge of the bias, which undermines the out-of-the-box usability of CLIP. In this paper, we first theoretically analyze the impact of multimodal spurious bias in zero-shot classification. Based on this insight, we propose Spuriousness-Aware Guided Exploration (SAGE), a simple and effective method that mitigates spurious bias through guided prompt selection. SAGE requires no training, fine-tuning, or external annotations. It explores a space of prompt templates and selects the prompts that induce the largest semantic separation between classes, thereby improving worst-group robustness. Extensive experiments on four real-world benchmark datasets and five popular backbone models demonstrate that SAGE consistently improves zero-shot performance and generalization, outperforming previous zero-shot approaches without any external knowledge or model updates.

</details>


### [754] [MeanFlow Transformers with Representation Autoencoders](https://arxiv.org/abs/2511.13019)
*Zheyuan Hu,Chieh-Hsin Lai,Ge Wu,Yuki Mitsufuji,Stefano Ermon*

Main category: cs.CV

TL;DR: 提出在RAE潜在空间中为MF开发高效训练和采样方案，提升性能降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有MF训练计算要求高、不稳定，推理时SD - VAE解码器成本高且依赖复杂超参数。

Method: 在RAE潜在空间进行训练，采用Consistency Mid - Training初始化，两阶段方案（蒸馏和可选的自举阶段）。

Result: 在ImageNet 256上1步FID达2.03，优于普通MF，采样GFLOPS降38%，训练成本降83%；在ImageNet 512上1步FID达3.23，GFLOPS最低。

Conclusion: 该方法去除指导需求，简化训练配置，降低训练和采样计算量。

Abstract: MeanFlow (MF) is a diffusion-motivated generative model that enables efficient few-step generation by learning long jumps directly from noise to data. In practice, it is often used as a latent MF by leveraging the pre-trained Stable Diffusion variational autoencoder (SD-VAE) for high-dimensional data modeling. However, MF training remains computationally demanding and is often unstable. During inference, the SD-VAE decoder dominates the generation cost, and MF depends on complex guidance hyperparameters for class-conditional generation. In this work, we develop an efficient training and sampling scheme for MF in the latent space of a Representation Autoencoder (RAE), where a pre-trained vision encoder (e.g., DINO) provides semantically rich latents paired with a lightweight decoder. We observe that naive MF training in the RAE latent space suffers from severe gradient explosion. To stabilize and accelerate training, we adopt Consistency Mid-Training for trajectory-aware initialization and use a two-stage scheme: distillation from a pre-trained flow matching teacher to speed convergence and reduce variance, followed by an optional bootstrapping stage with a one-point velocity estimator to further reduce deviation from the oracle mean flow. This design removes the need for guidance, simplifies training configurations, and reduces computation in both training and sampling. Empirically, our method achieves a 1-step FID of 2.03, outperforming vanilla MF's 3.43, while reducing sampling GFLOPS by 38% and total training cost by 83% on ImageNet 256. We further scale our approach to ImageNet 512, achieving a competitive 1-step FID of 3.23 with the lowest GFLOPS among all baselines. Code is available at https://github.com/sony/mf-rae.

</details>


### [755] [SpectralAdapt: Semi-Supervised Domain Adaptation with Spectral Priors for Human-Centered Hyperspectral Image Reconstruction](https://arxiv.org/abs/2511.13020)
*Yufei Wen,Yuting Zhang,Jingdan Kang,Hao Ren,Weibin Cheng,Jintai Chen,Kaishun Wu*

Main category: cs.CV

TL;DR: 提出SpectralAdapt框架用于高光谱图像重建，解决医疗应用中数据稀缺等问题，实验显示其有良好效果。


<details>
  <summary>Details</summary>
Motivation: 高光谱成像在医疗保健有潜力，但获取数据成本高、技术要求高，且人体高光谱数据稀缺限制医疗应用进展。

Method: 提出SpectralAdapt半监督域适应框架，引入Spectral Density Masking增强光谱推理，引入Spectral Endmember Representation Alignment以有价值的标记像素推导端元引导无标记预测。

Result: 在基准数据集实验中，光谱保真度、跨域泛化能力和训练稳定性均有持续提升。

Conclusion: 半监督域适应是医疗高光谱成像的有效解决方案。

Abstract: Hyperspectral imaging (HSI) holds great potential for healthcare due to its rich spectral information. However, acquiring HSI data remains costly and technically demanding. Hyperspectral image reconstruction offers a practical solution by recovering HSI data from accessible modalities, such as RGB. While general domain datasets are abundant, the scarcity of human HSI data limits progress in medical applications. To tackle this, we propose SpectralAdapt, a semi-supervised domain adaptation (SSDA) framework that bridges the domain gap between general and human-centered HSI datasets. To fully exploit limited labels and abundant unlabeled data, we enhance spectral reasoning by introducing Spectral Density Masking (SDM), which adaptively masks RGB channels based on their spectral complexity, encouraging recovery of informative regions from complementary cues during consistency training. Furthermore, we introduce Spectral Endmember Representation Alignment (SERA), which derives physically interpretable endmembers from valuable labeled pixels and employs them as domain-invariant anchors to guide unlabeled predictions, with momentum updates ensuring adaptability and stability. These components are seamlessly integrated into SpectralAdapt, a spectral prior-guided framework that effectively mitigates domain shift, spectral degradation, and data scarcity in HSI reconstruction. Experiments on benchmark datasets demonstrate consistent improvements in spectral fidelity, cross-domain generalization, and training stability, highlighting the promise of SSDA as an efficient solution for hyperspectral imaging in healthcare.

</details>


### [756] [Dynamic Parameter Optimization for Highly Transferable Transformation-Based Attacks](https://arxiv.org/abs/2511.11993)
*Jiaming Liang,Chi-Man Pun*

Main category: cs.CV

TL;DR: 现有基于变换的攻击在参数优化上有盲点，本文通过实证研究揭示可迁移性动态模式，提出CDM模型解释模式，进而提出DPO方法降低复杂度并提升可迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有基于变换的攻击在参数优化存在盲点，如低迭代结果表征片面、统一参数损害可迁移性、网格搜索计算开销大等问题，需改进。

Method: 以各种变换为基线进行实证研究，提出同心衰减模型（CDM）解释可迁移性模式，基于先升后降模式提出动态参数优化（DPO）方法。

Result: 在不同替代模型、迭代次数和任务上的综合实验表明，DPO能显著提高可迁移性。

Conclusion: 所提出的DPO方法有效解决了现有基于变换攻击的参数优化问题，能显著提升攻击的可迁移性。

Abstract: Despite their wide application, the vulnerabilities of deep neural networks raise societal concerns. Among them, transformation-based attacks have demonstrated notable success in transfer attacks. However, existing attacks suffer from blind spots in parameter optimization, limiting their full potential. Specifically, (1) prior work generally considers low-iteration settings, yet attacks perform quite differently at higher iterations, so characterizing overall performance based only on low-iteration results is misleading. (2) Existing attacks use uniform parameters for different surrogate models, iterations, and tasks, which greatly impairs transferability. (3) Traditional transformation parameter optimization relies on grid search. For n parameters with m steps each, the complexity is O(mn). Large computational overhead limits further optimization of parameters. To address these limitations, we conduct an empirical study with various transformations as baselines, revealing three dynamic patterns of transferability with respect to parameter strength. We further propose a novel Concentric Decay Model (CDM) to effectively explain these patterns. Building on these insights, we propose an efficient Dynamic Parameter Optimization (DPO) based on the rise-then-fall pattern, reducing the complexity to O(nlogm). Comprehensive experiments on existing transformation-based attacks across different surrogate models, iterations, and tasks demonstrate that our DPO can significantly improve transferability.

</details>


### [757] [Enhancing Road Safety Through Multi-Camera Image Segmentation with Post-Encroachment Time Analysis](https://arxiv.org/abs/2511.12018)
*Shounak Ray Chaudhuri,Arash Jahangiri,Christopher Paolini*

Main category: cs.CV

TL;DR: 本文提出多摄像头计算机视觉框架用于实时安全评估，在加州一十字路口验证，能高精度识别高风险区域，验证了基于视觉的PET分析可行性。


<details>
  <summary>Details</summary>
Motivation: 传统基于事故的交通安全分析受数据稀疏和延迟限制，需新方法进行实时安全评估。

Method: 采用多摄像头，用YOLOv11进行车辆检测，通过单应矩阵转换到统一鸟瞰图，用像素级PET算法测量车辆位置，数据存于SQL数据库。

Result: 框架能以亚秒级精度识别高风险区域，边缘设备实时吞吐量平均为2.68 FPS。

Conclusion: 验证了分散式基于视觉的PET分析用于智能交通系统的可行性，提供了可复制的交叉路口安全评估方法。

Abstract: Traffic safety analysis at signalized intersections is vital for reducing vehicle and pedestrian collisions, yet traditional crash-based studies are limited by data sparsity and latency. This paper presents a novel multi-camera computer vision framework for real-time safety assessment through Post-Encroachment Time (PET) computation, demonstrated at the intersection of H Street and Broadway in Chula Vista, California. Four synchronized cameras provide continuous visual coverage, with each frame processed on NVIDIA Jetson AGX Xavier devices using YOLOv11 segmentation for vehicle detection. Detected vehicle polygons are transformed into a unified bird's-eye map using homography matrices, enabling alignment across overlapping camera views. A novel pixel-level PET algorithm measures vehicle position without reliance on fixed cells, allowing fine-grained hazard visualization via dynamic heatmaps, accurate to 3.3 sq-cm. Timestamped vehicle and PET data is stored in an SQL database for long-term monitoring. Results over various time intervals demonstrate the framework's ability to identify high-risk regions with sub-second precision and real-time throughput on edge devices, producing data for an 800 x 800 pixel logarithmic heatmap at an average of 2.68 FPS. This study validates the feasibility of decentralized vision-based PET analysis for intelligent transportation systems, offering a replicable methodology for high-resolution, real-time, and scalable intersection safety evaluation.

</details>


### [758] [Calibrated Multimodal Representation Learning with Missing Modalities](https://arxiv.org/abs/2511.12034)
*Xiaohao Liu,Xiaobo Xia,Jiaheng Wei,Shuo Yang,Xiu Su,See-Kiong Ng,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 提出CalMRL解决多模态表示学习中缺失模态导致的对齐问题，实验证明其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态表示学习方法要求所有模态都存在，难以处理有缺失模态的数据集。

Method: 提出CalMRL，利用模态先验和内在联系在表示层对缺失模态进行建模，采用双步学习方法解决优化困境。

Result: 验证了CalMRL对锚点偏移的缓解和收敛性，结合现有方法可处理缺失模态数据。

Conclusion: CalMRL具有优越性，代码等将公开。

Abstract: Multimodal representation learning harmonizes distinct modalities by aligning them into a unified latent space. Recent research generalizes traditional cross-modal alignment to produce enhanced multimodal synergy but requires all modalities to be present for a common instance, making it challenging to utilize prevalent datasets with missing modalities. We provide theoretical insights into this issue from an anchor shift perspective. Observed modalities are aligned with a local anchor that deviates from the optimal one when all modalities are present, resulting in an inevitable shift. To address this, we propose CalMRL for multimodal representation learning to calibrate incomplete alignments caused by missing modalities. Specifically, CalMRL leverages the priors and the inherent connections among modalities to model the imputation for the missing ones at the representation level. To resolve the optimization dilemma, we employ a bi-step learning method with the closed-form solution of the posterior distribution of shared latents. We validate its mitigation of anchor shift and convergence with theoretical guidance. By equipping the calibrated alignment with the existing advanced method, we offer new flexibility to absorb data with missing modalities, which is originally unattainable. Extensive experiments and comprehensive analyses demonstrate the superiority of CalMRL. Our code, model checkpoints, and evaluation raw data will be publicly available.

</details>


### [759] [Rethinking Saliency Maps: A Cognitive Human Aligned Taxonomy and Evaluation Framework for Explanations](https://arxiv.org/abs/2511.13081)
*Yehonatan Elisha,Seffi Cohen,Oren Barkan,Noam Koenigstein*

Main category: cs.CV

TL;DR: 提出RFxG分类法和四个新的忠实度指标，评估十种显著性方法，推动以用户意图为导向的评估。


<details>
  <summary>Details</summary>
Motivation: 解决显著性图在目的和与用户查询一致性上缺乏共识的问题，提升解释方法的评估和实用性。

Method: 引入RFxG分类法，提出四个新的忠实度指标，对十种显著性方法、四种模型架构和三个数据集进行评估。

Result: 发现现有评估指标存在忽视对比推理和语义粒度的局限性。

Conclusion: 工作为开发与人类理解和查询复杂性相匹配的视觉解释提供概念基础和实用工具。

Abstract: Saliency maps are widely used for visual explanations in deep learning, but a fundamental lack of consensus persists regarding their intended purpose and alignment with diverse user queries. This ambiguity hinders the effective evaluation and practical utility of explanation methods.We address this gap by introducing the Reference-Frame $\times$ Granularity (RFxG) taxonomy, a principled conceptual framework that organizes saliency explanations along two essential axes:Reference-Frame: Distinguishing between pointwise ("Why this prediction?") and contrastive ("Why this and not an alternative?") explanations.Granularity: Ranging from fine-grained class-level (e.g., "Why Husky?") to coarse-grained group-level (e.g., "Why Dog?") interpretations.Using the RFxG lens, we demonstrate critical limitations in existing evaluation metrics, which overwhelmingly prioritize pointwise faithfulness while neglecting contrastive reasoning and semantic granularity. To systematically assess explanation quality across both RFxG dimensions, we propose four novel faithfulness metrics. Our comprehensive evaluation framework applies these metrics to ten state-of-the-art saliency methods, four model architectures, and three datasets.By advocating a shift toward user-intent-driven evaluation, our work provides both the conceptual foundation and the practical tools necessary to develop visual explanations that are not only faithful to the underlying model behavior but are also meaningfully aligned with the complexity of human understanding and inquiry.

</details>


### [760] [Shedding Light on VLN Robustness: A Black-box Framework for Indoor Lighting-based Adversarial Attack](https://arxiv.org/abs/2511.13132)
*Chenyang Li,Wenbing Tang,Yihao Huang,Sinong Simon Zhan,Ming Hu,Xiaojun Jia,Yang Liu*

Main category: cs.CV

TL;DR: 现有视觉语言导航（VLN）代理鲁棒性研究不足，本文提出基于室内照明的对抗攻击（ILA）框架，评估显示其显著增加失败率并降低轨迹效率。


<details>
  <summary>Details</summary>
Motivation: 现有对抗评估的扰动不具实际意义，室内照明作为影响导航的重要属性却被忽视，因此研究其对VLN代理的影响。

Method: 提出ILA黑盒框架，设计静态（SILA）和动态（DILA）两种攻击模式，在两个先进VLN模型的三个导航任务上进行评估。

Result: ILA显著增加了失败率，降低了轨迹效率。

Conclusion: VLN代理对现实室内照明变化存在未被发现的脆弱性。

Abstract: Vision-and-Language Navigation (VLN) agents have made remarkable progress, but their robustness remains insufficiently studied. Existing adversarial evaluations often rely on perturbations that manifest as unusual textures rarely encountered in everyday indoor environments. Errors under such contrived conditions have limited practical relevance, as real-world agents are unlikely to encounter such artificial patterns. In this work, we focus on indoor lighting, an intrinsic yet largely overlooked scene attribute that strongly influences navigation. We propose Indoor Lighting-based Adversarial Attack (ILA), a black-box framework that manipulates global illumination to disrupt VLN agents. Motivated by typical household lighting usage, we design two attack modes: Static Indoor Lighting-based Attack (SILA), where the lighting intensity remains constant throughout an episode, and Dynamic Indoor Lighting-based Attack (DILA), where lights are switched on or off at critical moments to induce abrupt illumination changes. We evaluate ILA on two state-of-the-art VLN models across three navigation tasks. Results show that ILA significantly increases failure rates while reducing trajectory efficiency, revealing previously unrecognized vulnerabilities of VLN agents to realistic indoor lighting variations.

</details>


### [761] [TEMPO: Global Temporal Building Density and Height Estimation from Satellite Imagery](https://arxiv.org/abs/2511.12104)
*Tammy Glazer,Gilles Q. Hacheme,Akram Zaytar,Luana Marotti,Amy Michaels,Girmaw Abebe Tadesse,Kevin White,Rahul Dodhia,Andrew Zolli,Inbal Becker-Reshef,Juan M. Lavista Ferres,Caleb Robinson*

Main category: cs.CV

TL;DR: 本文利用深度学习模型从高分辨率卫星图像得出全球建筑密度和高度的时间分辨数据集TEMPO。


<details>
  <summary>Details</summary>
Motivation: 创建全球建筑密度和高度的时间地图，以实现大规模监测发展模式和气候影响。

Method: 将现有数据集的建筑足迹和高度数据与季度PlanetScope底图卫星图像配对，训练多任务深度学习模型，应用该模型到2018年第一季度至2025年第二季度的全球PlanetScope底图。

Result: 估计在不同手工标记子集上F1分数达85% - 88%，五年趋势一致性分数为0.96，能捕捉建成区季度变化，计算成本低。

Conclusion: TEMPO有助于全球抗灾和适应工作中对发展模式和气候影响的大规模监测。

Abstract: We present TEMPO, a global, temporally resolved dataset of building density and height derived from high-resolution satellite imagery using deep learning models. We pair building footprint and height data from existing datasets with quarterly PlanetScope basemap satellite images to train a multi-task deep learning model that predicts building density and building height at a 37.6-meter per pixel resolution. We apply this model to global PlanetScope basemaps from Q1 2018 through Q2 2025 to create global, temporal maps of building density and height. We validate these maps by comparing against existing building footprint datasets. Our estimates achieve an F1 score between 85% and 88% on different hand-labeled subsets, and are temporally stable, with a 0.96 five-year trend-consistency score. TEMPO captures quarterly changes in built settlements at a fraction of the computational cost of comparable approaches, unlocking large-scale monitoring of development patterns and climate impacts essential for global resilience and adaptation efforts.

</details>


### [762] [Automated Road Distress Detection Using Vision Transformersand Generative Adversarial Networks](https://arxiv.org/abs/2511.13145)
*Cesar Portocarrero Rodriguez,Laura Vandeweyen,Yosuke Yamamoto*

Main category: cs.CV

TL;DR: 本文探索用计算机视觉技术进行道路病害分割，评估GAN生成数据，对比CNN和MaskFormer模型，结果显示GAN数据提升性能，MaskFormer表现更优。


<details>
  <summary>Details</summary>
Motivation: 美国道路管理维护效率低，传统检测方法成本高、耗时长，利用自动驾驶车辆实时视觉数据，采用计算机视觉方法进行道路监测。

Method: 评估GAN生成的合成数据用于模型训练，应用CNN进行道路病害分割，研究基于Transformer的MaskFormer模型。

Result: GAN生成的数据提高了模型性能，MaskFormer在mAP50和IoU两个指标上优于CNN模型。

Conclusion: GAN生成的数据对模型训练有益，MaskFormer在道路病害分割方面表现更出色，可用于指导基础设施修复工作。

Abstract: The American Society of Civil Engineers has graded Americas infrastructure condition as a C, with the road system receiving a dismal D. Roads are vital to regional economic viability, yet their management, maintenance, and repair processes remain inefficient, relying on outdated manual or laser-based inspection methods that are both costly and time-consuming. With the increasing availability of real-time visual data from autonomous vehicles, there is an opportunity to apply computer vision (CV) methods for advanced road monitoring, providing insights to guide infrastructure rehabilitation efforts. This project explores the use of state-of-the-art CV techniques for road distress segmentation. It begins by evaluating synthetic data generated with Generative Adversarial Networks (GANs) to assess its usefulness for model training. The study then applies Convolutional Neural Networks (CNNs) for road distress segmentation and subsequently examines the transformer-based model MaskFormer. Results show that GAN-generated data improves model performance and that MaskFormer outperforms the CNN model in two metrics: mAP50 and IoU.

</details>


### [763] [Codebook-Centric Deep Hashing: End-to-End Joint Learning of Semantic Hash Centers and Neural Hash Function](https://arxiv.org/abs/2511.12162)
*Shuo Yin,Zhiyuan Yin,Yuqing Hou,Rui Liu,Yong Chen,Dell Zhang*

Main category: cs.CV

TL;DR: 提出端到端框架Center - Reassigned Hashing (CRH)，动态重分配哈希中心，实验表明其在检索任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于哈希中心的深度哈希方法存在随机中心初始化忽视类间语义关系、两阶段方法有额外复杂度和性能欠佳等问题。

Method: 提出端到端框架CRH，动态从预设码本重分配哈希中心，联合优化哈希函数，采用多头机制增强哈希中心表示能力。

Result: 在三个基准测试上的实验表明，CRH学习到语义有意义的哈希中心，在检索任务中优于现有深度哈希方法。

Conclusion: CRH能有效解决现有方法问题，在检索任务中有良好表现。

Abstract: Hash center-based deep hashing methods improve upon pairwise or triplet-based approaches by assigning fixed hash centers to each class as learning targets, thereby avoiding the inefficiency of local similarity optimization. However, random center initialization often disregards inter-class semantic relationships. While existing two-stage methods mitigate this by first refining hash centers with semantics and then training the hash function, they introduce additional complexity, computational overhead, and suboptimal performance due to stage-wise discrepancies. To address these limitations, we propose $\textbf{Center-Reassigned Hashing (CRH)}$, an end-to-end framework that $\textbf{dynamically reassigns hash centers}$ from a preset codebook while jointly optimizing the hash function. Unlike previous methods, CRH adapts hash centers to the data distribution $\textbf{without explicit center optimization phases}$, enabling seamless integration of semantic relationships into the learning process. Furthermore, $\textbf{a multi-head mechanism}$ enhances the representational capacity of hash centers, capturing richer semantic structures. Extensive experiments on three benchmarks demonstrate that CRH learns semantically meaningful hash centers and outperforms state-of-the-art deep hashing methods in retrieval tasks.

</details>


### [764] [SOMA: Feature Gradient Enhanced Affine-Flow Matching for SAR-Optical Registration](https://arxiv.org/abs/2511.13168)
*Haodong Wang,Tao Zhuo,Xiuwei Zhang,Hanlin Yin,Wencong Wu,Yanning Zhang*

Main category: cs.CV

TL;DR: 提出SOMA框架解决SAR与光学图像像素级配准难题，实验证明提高精度且具鲁棒性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 深度学习在SAR - 光学配准任务表现不佳，且梯度信息未在深度学习框架有效利用。

Method: 提出SOMA框架，引入FGE增强特征区分度，提出GLAM确保结构一致性和局部精度。

Result: SOMA显著提高配准精度，在SEN1 - 2数据集上CMR@1px提高12.29%，在GFGE_SO数据集上提高18.50%。

Conclusion: SOMA能有效解决SAR与光学图像配准问题，有强鲁棒性和泛化性。

Abstract: Achieving pixel-level registration between SAR and optical images remains a challenging task due to their fundamentally different imaging mechanisms and visual characteristics. Although deep learning has achieved great success in many cross-modal tasks, its performance on SAR-Optical registration tasks is still unsatisfactory. Gradient-based information has traditionally played a crucial role in handcrafted descriptors by highlighting structural differences. However, such gradient cues have not been effectively leveraged in deep learning frameworks for SAR-Optical image matching. To address this gap, we propose SOMA, a dense registration framework that integrates structural gradient priors into deep features and refines alignment through a hybrid matching strategy. Specifically, we introduce the Feature Gradient Enhancer (FGE), which embeds multi-scale, multi-directional gradient filters into the feature space using attention and reconstruction mechanisms to boost feature distinctiveness. Furthermore, we propose the Global-Local Affine-Flow Matcher (GLAM), which combines affine transformation and flow-based refinement within a coarse-to-fine architecture to ensure both structural consistency and local accuracy. Experimental results demonstrate that SOMA significantly improves registration precision, increasing the CMR@1px by 12.29% on the SEN1-2 dataset and 18.50% on the GFGE_SO dataset. In addition, SOMA exhibits strong robustness and generalizes well across diverse scenes and resolutions.

</details>


### [765] [MixAR: Mixture Autoregressive Image Generation](https://arxiv.org/abs/2511.12181)
*Jinyuan Hu,Jiayou Zhang,Shaobo Cui,Kun Zhang,Guangyi Chen*

Main category: cs.CV

TL;DR: 论文针对连续潜在空间自回归建模挑战提出MixAR框架，研究多种离散 - 连续混合策略及TI - Mix，实验显示策略优势。


<details>
  <summary>Details</summary>
Motivation: 现有自回归图像生成方法因量化和有限码本丢弃细粒度信息，连续潜在空间自回归建模面临高效建模挑战。

Method: 引入MixAR框架，利用混合训练范式，研究DC - SA、DC - CA、DC - Mix离散 - 连续混合策略，提出TI - Mix平衡训练和推理分布。

Result: DC - Mix策略在计算效率和生成保真度间取得良好平衡，TI - Mix有持续改进。

Conclusion: MixAR框架及相关策略能有效应对连续潜在空间自回归建模挑战。

Abstract: Autoregressive (AR) approaches, which represent images as sequences of discrete tokens from a finite codebook, have achieved remarkable success in image generation. However, the quantization process and the limited codebook size inevitably discard fine-grained information, placing bottlenecks on fidelity. Motivated by this limitation, recent studies have explored autoregressive modeling in continuous latent spaces, which offers higher generation quality. Yet, unlike discrete tokens constrained by a fixed codebook, continuous representations lie in a vast and unstructured space, posing significant challenges for efficient autoregressive modeling. To address these challenges, we introduce MixAR, a novel framework that leverages mixture training paradigms to inject discrete tokens as prior guidance for continuous AR modeling. MixAR is a factorized formulation that leverages discrete tokens as prior guidance for continuous autoregressive prediction. We investigate several discrete-continuous mixture strategies, including self-attention (DC-SA), cross-attention (DC-CA), and a simple approach (DC-Mix) that replaces homogeneous mask tokens with informative discrete counterparts. Moreover, to bridge the gap between ground-truth training tokens and inference tokens produced by the pre-trained AR model, we propose Training-Inference Mixture (TI-Mix) to achieve consistent training and generation distributions. In our experiments, we demonstrate a favorable balance of the DC-Mix strategy between computational efficiency and generation fidelity, and consistent improvement of TI-Mix.

</details>


### [766] [Suppressing VLM Hallucinations with Spectral Representation Filtering](https://arxiv.org/abs/2511.12220)
*Ameen Ali,Tamim Zoabi,Lior Wolf*

Main category: cs.CV

TL;DR: 提出轻量级、免训练的光谱表示过滤（SRF）方法抑制视觉语言模型幻觉，在多模型和任务基准上降低幻觉率并达先进水平。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型因过度依赖语言先验和不精确的跨模态接地，常产生幻觉，需要方法抑制。

Method: 通过特征协方差结构分析和校正，利用特征分解识别低秩幻觉模式，用软光谱滤波器衰减深层vLLM层前馈投影权重中的这些模式。

Result: 在三个视觉语言模型家族和多个视觉任务基准上，持续降低幻觉率，实现最先进的忠实度且不降低字幕质量。

Conclusion: SRF方法有效抑制视觉语言模型的幻觉，无需架构修改和额外推理开销，有良好效果。

Abstract: Vision-language models (VLMs) frequently produce hallucinations in the form of descriptions of objects, attributes, or relations that do not exist in the image due to over-reliance on language priors and imprecise cross-modal grounding. We introduce Spectral Representation Filtering (SRF), a lightweight, training-free method to suppress such hallucinations by analyzing and correcting the covariance structure of the model's representations. SRF identifies low-rank hallucination modes through eigendecomposition of the covariance of the differences between features collected for truthful and hallucinatory captions, revealing structured biases in the feature space. A soft spectral filter then attenuates these modes in the feed-forward projection weights of deeper vLLM layers, equalizing feature variance while preserving semantic fidelity. Unlike decoding or retraining-based approaches, SRF operates entirely post-hoc, incurs zero inference overhead, and requires no architectural modifications. Across three families of VLMs (LLaVA-1.5, MiniGPT-4, and mPLUG-Owl2), SRF consistently reduces hallucination rates on MSCOCO, POPE-VQA, and other visual tasks benchmarks, achieving state-of-the-art faithfulness without degrading caption quality.

</details>


### [767] [D$^{3}$ToM: Decider-Guided Dynamic Token Merging for Accelerating Diffusion MLLMs](https://arxiv.org/abs/2511.12280)
*Shuochen Chang,Xiaofeng Zhang,Qingyang Liu,Li Niu*

Main category: cs.CV

TL;DR: 提出D³ToM方法加速Diffusion MLLMs推理，实验表明能在保持性能下加速，代码已开源。


<details>
  <summary>Details</summary>
Motivation: Diffusion MLLMs推理比自回归模型慢，解码复杂度高，需要解决推理加速问题。

Method: 提出D³ToM，在每个去噪步骤用决策器令牌构建重要性图，保留显著令牌并聚合其余令牌，该模块集成到单个变压器层，且合并比率随去噪步骤动态变化。

Result: D³ToM能加速推理，同时保持有竞争力的性能。

Conclusion: D³ToM可有效解决Diffusion MLLMs推理慢的问题，在不改变模型参数的情况下实现加速。

Abstract: Diffusion-based multimodal large language models (Diffusion MLLMs) have recently demonstrated impressive non-autoregressive generative capabilities across vision-and-language tasks. However, Diffusion MLLMs exhibit substantially slower inference than autoregressive models: Each denoising step employs full bidirectional self-attention over the entire sequence, resulting in cubic decoding complexity that becomes computationally impractical with thousands of visual tokens. To address this challenge, we propose D$^{3}$ToM, a Decider-guided dynamic token merging method that dynamically merges redundant visual tokens at different denoising steps to accelerate inference in Diffusion MLLMs. At each denoising step, D$^{3}$ToM uses decider tokens-the tokens generated in the previous denoising step-to build an importance map over all visual tokens. Then it maintains a proportion of the most salient tokens and merges the remainder through similarity-based aggregation. This plug-and-play module integrates into a single transformer layer, physically shortening the visual token sequence for all subsequent layers without altering model parameters. Moreover, D$^{3}$ToM employs a merge ratio that dynamically varies with each denoising step, aligns with the native decoding process of Diffusion MLLMs, achieving superior performance under equivalent computational budgets. Extensive experiments show that D$^{3}$ToM accelerates inference while preserving competitive performance. The code is released at https://github.com/bcmi/D3ToM-Diffusion-MLLM.

</details>


### [768] [GeoX-Bench: Benchmarking Cross-View Geo-Localization and Pose Estimation Capabilities of Large Multimodal Models](https://arxiv.org/abs/2511.13259)
*Yushuo Zheng,Jiangyong Ying,Huiyu Duan,Chunyi Li,Zicheng Zhang,Jing Liu,Xiaohong Liu,Guangtao Zhai*

Main category: cs.CV

TL;DR: 介绍GeoX - Bench基准来评估大模态模型（LMMs）在跨视图地理定位和姿态估计的能力，评估25个模型，指出当前模型问题及训练效果，代码开源。


<details>
  <summary>Details</summary>
Motivation: LMMs在跨视图地理定位和姿态估计领域的知识和能力未被探索，有潜在应用价值，需填补空白。

Method: 创建GeoX - Bench基准，包含图像对和问答对，基于此评估25个LMMs并探索指令微调效果。

Result: 当前LMMs在地理定位任务表现好，但姿态估计任务效果差，对GeoX - Bench训练数据进行指令微调可提升跨视图地理感知能力。

Conclusion: GeoX - Bench可用于评估LMMs在跨视图地理定位和姿态估计的能力，当前LMMs在姿态估计待提升，指令微调有积极作用。

Abstract: Large multimodal models (LMMs) have demonstrated remarkable capabilities across a wide range of tasks, however their knowledge and abilities in the cross-view geo-localization and pose estimation domains remain unexplored, despite potential benefits for navigation, autonomous driving, outdoor robotics, \textit{etc}. To bridge this gap, we introduce \textbf{GeoX-Bench}, a comprehensive \underline{Bench}mark designed to explore and evaluate the capabilities of LMMs in \underline{cross}-view \underline{Geo}-localization and pose estimation. Specifically, GeoX-Bench contains 10,859 panoramic-satellite image pairs spanning 128 cities in 49 countries, along with corresponding 755,976 question-answering (QA) pairs. Among these, 42,900 QA pairs are designated for benchmarking, while the remaining are intended to enhance the capabilities of LMMs. Based on GeoX-Bench, we evaluate the capabilities of 25 state-of-the-art LMMs on cross-view geo-localization and pose estimation tasks, and further explore the empowered capabilities of instruction-tuning. Our benchmark demonstrate that while current LMMs achieve impressive performance in geo-localization tasks, their effectiveness declines significantly on the more complex pose estimation tasks, highlighting a critical area for future improvement, and instruction-tuning LMMs on the training data of GeoX-Bench can significantly improve the cross-view geo-sense abilities. The GeoX-Bench is available at \textcolor{magenta}{https://github.com/IntMeGroup/GeoX-Bench}.

</details>


### [769] [AGGRNet: Selective Feature Extraction and Aggregation for Enhanced Medical Image Classification](https://arxiv.org/abs/2511.12382)
*Ansh Makwe,Akansh Agrawal,Prateek Jain,Akshan Agrawal,Priyanka Bagade*

Main category: cs.CV

TL;DR: 现有注意力模型对复杂医学图像分类有局限，提出AGGRNet框架，实验显示该模型在多医学影像数据集达SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 现有基于注意力的模型在复杂医学图像分析任务中难以有效区分细微类别，易导致错误诊断。

Method: 提出AGGRNet框架，用于提取信息性和非信息性特征，以理解细粒度视觉模式。

Result: 模型在多个医学影像数据集上达到SOTA性能，在Kvasir数据集上比SOTA模型最高提升5%。

Conclusion: AGGRNet框架能有效改善复杂医学图像分析任务的分类效果。

Abstract: Medical image analysis for complex tasks such as severity grading and disease subtype classification poses significant challenges due to intricate and similar visual patterns among classes, scarcity of labeled data, and variability in expert interpretations. Despite the usefulness of existing attention-based models in capturing complex visual patterns for medical image classification, underlying architectures often face challenges in effectively distinguishing subtle classes since they struggle to capture inter-class similarity and intra-class variability, resulting in incorrect diagnosis. To address this, we propose AGGRNet framework to extract informative and non-informative features to effectively understand fine-grained visual patterns and improve classification for complex medical image analysis tasks. Experimental results show that our model achieves state-of-the-art performance on various medical imaging datasets, with the best improvement up to 5% over SOTA models on the Kvasir dataset.

</details>


### [770] [Computer Vision based group activity detection and action spotting](https://arxiv.org/abs/2511.13315)
*Narthana Sivalingam,Santhirarajah Sivasthigan,Thamayanthi Mahendranathan,G. M. R. I. Godaliyadda,M. P. B. Ekanayake,H. M. V. R. Herath*

Main category: cs.CV

TL;DR: 本文提出基于计算机视觉的框架用于群体活动识别和动作定位，结合深度学习模型与图关系推理，在集体活动数据集实验中提升了识别性能。


<details>
  <summary>Details</summary>
Motivation: 多人场景中群体活动检测因人类交互复杂、遮挡和外观变化等因素具有挑战。

Method: 先使用Mask R - CNN进行演员定位，用多种骨干网络提取特征图并应用RoIAlign，融合掩码信息得到精炼特征表示；构建演员关系图编码相似性和位置关系，用图卷积网络推理关系并预测动作和活动。

Result: 在集体活动数据集实验中，基于掩码的特征精炼、鲁棒相似性搜索和图神经网络推理结合，在拥挤和非拥挤场景下都提升了识别性能。

Conclusion: 该方法凸显了融合分割、特征提取和关系图推理用于复杂视频理解任务的潜力。

Abstract: Group activity detection in multi-person scenes is challenging due to complex human interactions, occlusions, and variations in appearance over time. This work presents a computer vision based framework for group activity recognition and action spotting using a combination of deep learning models and graph based relational reasoning. The system first applies Mask R-CNN to obtain accurate actor localization through bounding boxes and instance masks. Multiple backbone networks, including Inception V3, MobileNet, and VGG16, are used to extract feature maps, and RoIAlign is applied to preserve spatial alignment when generating actor specific features. The mask information is then fused with the feature maps to obtain refined masked feature representations for each actor. To model interactions between individuals, we construct Actor Relation Graphs that encode appearance similarity and positional relations using methods such as normalized cross correlation, sum of absolute differences, and dot product. Graph Convolutional Networks operate on these graphs to reason about relationships and predict both individual actions and group level activities. Experiments on the Collective Activity dataset demonstrate that the combination of mask based feature refinement, robust similarity search, and graph neural network reasoning leads to improved recognition performance across both crowded and non crowded scenarios. This approach highlights the potential of integrating segmentation, feature extraction, and relational graph reasoning for complex video understanding tasks.

</details>


### [771] [Semi-Supervised Multi-Task Learning for Interpretable Quality As- sessment of Fundus Images](https://arxiv.org/abs/2511.13353)
*Lucas Gabriel Telesco,Danila Nejamkin,Estefanía Mata,Francisco Filizzola,Kevin Wignall,Lucía Franco Troilo,María de los Angeles Cenoz,Melissa Thompson,Mercedes Leguía,Ignacio Larrabide,José Ignacio Orlando*

Main category: cs.CV

TL;DR: 提出混合半监督学习方法改进视网膜图像质量评估，无需大量手动标注，提升评估效果并提供可解释反馈。


<details>
  <summary>Details</summary>
Motivation: 多数视网膜图像质量评估工具仅分类整体图像质量，未指明采集缺陷，且详细标注成本高，旨在缓解此局限。

Method: 在多任务框架中结合整体质量手动标签和质量细节伪标签的混合半监督学习方法，用教师模型生成伪标签微调预训练模型。

Result: 弱标注在质量评估上优于单任务基线，多任务模型在多数细节预测任务上与教师模型性能相当，在新标注子集上与专家表现相似。

Conclusion: 所提半监督方法不仅提高整体质量评估，还能以无额外手动标注成本增强可解释性，提供临床可操作输出指导图像重采。

Abstract: Retinal image quality assessment (RIQA) supports computer-aided diagnosis of eye diseases. However, most tools classify only overall image quality, without indicating acquisition defects to guide recapture. This gap is mainly due to the high cost of detailed annotations. In this paper, we aim to mitigate this limitation by introducing a hybrid semi-supervised learning approach that combines manual labels for overall quality with pseudo-labels of quality details within a multi-task framework. Our objective is to obtain more interpretable RIQA models without requiring extensive manual labeling. Pseudo-labels are generated by a Teacher model trained on a small dataset and then used to fine-tune a pre-trained model in a multi-task setting. Using a ResNet-18 backbone, we show that these weak annotations improve quality assessment over single-task baselines (F1: 0.875 vs. 0.863 on EyeQ, and 0.778 vs. 0.763 on DeepDRiD), matching or surpassing existing methods. The multi-task model achieved performance statistically comparable to the Teacher for most detail prediction tasks (p > 0.05). In a newly annotated EyeQ subset released with this paper, our model performed similarly to experts, suggesting that pseudo-label noise aligns with expert variability. Our main finding is that the proposed semi-supervised approach not only improves overall quality assessment but also provides interpretable feedback on capture conditions (illumination, clarity, contrast). This enhances interpretability at no extra manual labeling cost and offers clinically actionable outputs to guide image recapture.

</details>


### [772] [DINO-Detect: A Simple yet Effective Framework for Blur-Robust AI-Generated Image Detection](https://arxiv.org/abs/2511.12511)
*Jialiang Shen,Jiyang Zheng,Yunqi Xue,Huajie Chen,Yu Yao,Hui Kang,Ruiqi Liu,Helin Gong,Yang Yang,Dadong Wang,Tongliang Liu*

Main category: cs.CV

TL;DR: 为解决AI生成图像检测在运动模糊场景下性能下降问题，提出基于师生知识蒸馏的抗模糊检测框架，实验显示在模糊和清晰条件下均达SOTA。


<details>
  <summary>Details</summary>
Motivation: 多数AI生成图像检测器在现实世界退化（特别是运动模糊）情况下性能不佳，需解决该局限。

Method: 构建基于师生知识蒸馏的抗模糊AIGI检测框架，以在清晰图像上训练的高容量教师模型（DINOv3）为参考，将其特征和对数响应从清晰图像提炼到在模糊图像上训练的学生模型。

Result: 方法在运动模糊和清晰条件下均达到了最先进的性能。

Conclusion: 该方法提高了泛化能力和现实世界适用性。

Abstract: With growing concerns over image authenticity and digital safety, the field of AI-generated image (AIGI) detection has progressed rapidly. Yet, most AIGI detectors still struggle under real-world degradations, particularly motion blur, which frequently occurs in handheld photography, fast motion, and compressed video. Such blur distorts fine textures and suppresses high-frequency artifacts, causing severe performance drops in real-world settings. We address this limitation with a blur-robust AIGI detection framework based on teacher-student knowledge distillation. A high-capacity teacher (DINOv3), trained on clean (i.e., sharp) images, provides stable and semantically rich representations that serve as a reference for learning. By freezing the teacher to maintain its generalization ability, we distill its feature and logit responses from sharp images to a student trained on blurred counterparts, enabling the student to produce consistent representations under motion degradation. Extensive experiments benchmarks show that our method achieves state-of-the-art performance under both motion-blurred and clean conditions, demonstrating improved generalization and real-world applicability. Source codes will be released at: https://github.com/JiaLiangShen/Dino-Detect-for-blur-robust-AIGC-Detection.

</details>


### [773] [Generalized Denoising Diffusion Codebook Models (gDDCM): Tokenizing images using a pre-trained diffusion model](https://arxiv.org/abs/2511.13387)
*Fei Kong*

Main category: cs.CV

TL;DR: 提出广义去噪扩散压缩模型gDDCM，将DDCM扩展到主流扩散模型及其变体，实验证明其有效性和性能提升。


<details>
  <summary>Details</summary>
Motivation: DDCM只能用于DDPM，无法应用于其他方法，需要更通用的模型。

Method: 提出gDDCM，将DDCM扩展到主流扩散模型及其变体，包括DDPM、基于分数的模型、一致性模型和整流流。

Result: 在CIFAR - 10和LSUN Bedroom数据集上实验，成功将DDCM推广到上述模型并提升了性能。

Conclusion: gDDCM能有效扩展DDCM到多种主流扩散模型，且性能得到改善。

Abstract: Recently, the Denoising Diffusion Codebook Models (DDCM) was proposed. DDCM leverages the Denoising Diffusion Probabilistic Model (DDPM) and replaces the random noise in the backward process with noise sampled from specific sets according to a predefined rule, thereby enabling image compression. However, DDCM cannot be applied to methods other than DDPM. In this paper, we propose the generalized Denoising Diffusion Compression Model (gDDCM), which extends DDCM to mainstream diffusion models and their variants, including DDPM, Score-Based Models, Consistency Models, and Rectified Flow. We evaluate our method on CIFAR-10 and LSUN Bedroom datasets. Experimental results demonstrate that our approach successfully generalizes DDCM to the aforementioned models and achieves improved performance.

</details>


### [774] [Descriptor: Distance-Annotated Traffic Perception Question Answering (DTPQA)](https://arxiv.org/abs/2511.13397)
*Nikos Theodoridis,Tim Brophy,Reenu Mohandas,Ganesh Sistu,Fiachra Collins,Anthony Scanlan,Ciaran Eising*

Main category: cs.CV

TL;DR: 文章介绍用于评估视觉语言模型（VLMs）交通感知能力的距离标注交通感知问答基准（DTPQA），含数据集及脚本。


<details>
  <summary>Details</summary>
Motivation: VLMs在自动驾驶领域应用需具备强大感知能力，要评估其在不同距离的感知能力，且需与推理等技能分离评估。

Method: 设计DTPQA基准，包含合成基准（DTP - Synthetic）和真实世界基准（DTP - Real），样本含图像、问题、真实答案和物体距离。

Result: 提供DTPQA数据集和创建数据集的Python脚本。

Conclusion: DTPQA可用于评估VLMs在交通场景中的感知系统，分析其性能随物体距离增加的变化。

Abstract: The remarkable progress of Vision-Language Models (VLMs) on a variety of tasks has raised interest in their application to automated driving. However, for these models to be trusted in such a safety-critical domain, they must first possess robust perception capabilities, i.e., they must be capable of understanding a traffic scene, which can often be highly complex, with many things happening simultaneously. Moreover, since critical objects and agents in traffic scenes are often at long distances, we require systems with not only strong perception capabilities at close distances (up to 20 meters), but also at long (30+ meters) range. Therefore, it is important to evaluate the perception capabilities of these models in isolation from other skills like reasoning or advanced world knowledge. Distance-Annotated Traffic Perception Question Answering (DTPQA) is a Visual Question Answering (VQA) benchmark designed specifically for this purpose: it can be used to evaluate the perception systems of VLMs in traffic scenarios using trivial yet crucial questions relevant to driving decisions. It consists of two parts: a synthetic benchmark (DTP-Synthetic) created using a simulator, and a real-world benchmark (DTP-Real) built on top of existing images of real traffic scenes. Additionally, DTPQA includes distance annotations, i.e., how far the object in question is from the camera. More specifically, each DTPQA sample consists of (at least): (a) an image, (b) a question, (c) the ground truth answer, and (d) the distance of the object in question, enabling analysis of how VLM performance degrades with increasing object distance. In this article, we provide the dataset itself along with the Python scripts used to create it, which can be used to generate additional data of the same kind.

</details>


### [775] [TripleFDS: Triple Feature Disentanglement and Synthesis for Scene Text Editing](https://arxiv.org/abs/2511.13399)
*Yuchen Bao,Yiting Wang,Wenjian Huang,Haowei Wang,Shen Chen,Taiping Yao,Shouhong Ding,Jianguo Zhang*

Main category: cs.CV

TL;DR: 提出TripleFDS框架和SCB Synthesis数据集用于场景文本编辑，在主流基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 以往方法难以完全解耦可编辑属性，限制了可控性和视觉一致性。

Method: 提出TripleFDS框架，利用SCB Group作为基本训练单元解耦三重特征，在合成阶段进行特征重映射；构建SCB Synthesis数据集提供训练数据。

Result: 在主流STE基准测试中达到了最先进的图像保真度（SSIM为44.54）和文本准确率（ACC为93.58%）。

Conclusion: TripleFDS性能优越，支持风格替换和背景转移等更灵活的编辑操作。

Abstract: Scene Text Editing (STE) aims to naturally modify text in images while preserving visual consistency, the decisive factors of which can be divided into three parts, i.e., text style, text content, and background. Previous methods have struggled with incomplete disentanglement of editable attributes, typically addressing only one aspect - such as editing text content - thus limiting controllability and visual consistency. To overcome these limitations, we propose TripleFDS, a novel framework for STE with disentangled modular attributes, and an accompanying dataset called SCB Synthesis. SCB Synthesis provides robust training data for triple feature disentanglement by utilizing the "SCB Group", a novel construct that combines three attributes per image to generate diverse, disentangled training groups. Leveraging this construct as a basic training unit, TripleFDS first disentangles triple features, ensuring semantic accuracy through inter-group contrastive regularization and reducing redundancy through intra-sample multi-feature orthogonality. In the synthesis phase, TripleFDS performs feature remapping to prevent "shortcut" phenomena during reconstruction and mitigate potential feature leakage. Trained on 125,000 SCB Groups, TripleFDS achieves state-of-the-art image fidelity (SSIM of 44.54) and text accuracy (ACC of 93.58%) on the mainstream STE benchmarks. Besides superior performance, the more flexible editing of TripleFDS supports new operations such as style replacement and background transfer. Code: https://github.com/yusenbao01/TripleFDS

</details>


### [776] [Unlocking the Forgery Detection Potential of Vanilla MLLMs: A Novel Training-Free Pipeline](https://arxiv.org/abs/2511.13442)
*Rui Zuo,Qinyue Tong,Zhe-Ming Lu,Ziqian Lu*

Main category: cs.CV

TL;DR: 提出无训练的基于MLLM的图像伪造分析管道Foresee，无需额外训练，推理轻量，定位精度高且解释丰富，泛化能力强。


<details>
  <summary>Details</summary>
Motivation: 现有图像伪造检测和定位方法泛化能力差、可解释性有限，基于MLLM的方法需大量计算资源且未挖掘其固有泛化潜力。

Method: 提出Foresee，采用类型先验驱动策略和FFD模块处理复制移动篡改。

Result: 实验表明Foresee定位精度高、解释更全面，泛化能力强，在多种篡改类型上优于现有方法。

Conclusion: Foresee有效释放了原始MLLM在法医领域的潜力，具有良好性能和泛化能力。

Abstract: With the rapid advancement of artificial intelligence-generated content (AIGC) technologies, including multimodal large language models (MLLMs) and diffusion models, image generation and manipulation have become remarkably effortless. Existing image forgery detection and localization (IFDL) methods often struggle to generalize across diverse datasets and offer limited interpretability. Nowadays, MLLMs demonstrate strong generalization potential across diverse vision-language tasks, and some studies introduce this capability to IFDL via large-scale training. However, such approaches cost considerable computational resources, while failing to reveal the inherent generalization potential of vanilla MLLMs to address this problem. Inspired by this observation, we propose Foresee, a training-free MLLM-based pipeline tailored for image forgery analysis. It eliminates the need for additional training and enables a lightweight inference process, while surpassing existing MLLM-based methods in both tamper localization accuracy and the richness of textual explanations. Foresee employs a type-prior-driven strategy and utilizes a Flexible Feature Detector (FFD) module to specifically handle copy-move manipulations, thereby effectively unleashing the potential of vanilla MLLMs in the forensic domain. Extensive experiments demonstrate that our approach simultaneously achieves superior localization accuracy and provides more comprehensive textual explanations. Moreover, Foresee exhibits stronger generalization capability, outperforming existing IFDL methods across various tampering types, including copy-move, splicing, removal, local enhancement, deepfake, and AIGC-based editing. The code will be released in the final version.

</details>


### [777] [X-VMamba: Explainable Vision Mamba](https://arxiv.org/abs/2511.12694)
*Mohamed A. Mabrok,Yalda Zafari*

Main category: cs.CV

TL;DR: 提出基于可控性的解释框架分析Vision SSMs处理空间信息机制，在医学影像等实验验证，可用于多领域。


<details>
  <summary>Details</summary>
Motivation: Vision SSMs缺乏类似注意力的透明机制，难以理解其处理空间信息方式。

Method: 提出基于Jacobian方法和基于Gramian方法两种互补公式，单前向传播，线性复杂度，无需架构修改和超参调整。

Result: 在三种医学影像模态实验验证，显示SSMs实现分层特征细化，揭示特定领域可控性特征等。

Conclusion: 框架为所有领域的SSMs建立了统一、基础的可解释性范式。

Abstract: State Space Models (SSMs), particularly the Mamba architecture, have recently emerged as powerful alternatives to Transformers for sequence modeling, offering linear computational complexity while achieving competitive performance. Yet, despite their effectiveness, understanding how these Vision SSMs process spatial information remains challenging due to the lack of transparent, attention-like mechanisms. To address this gap, we introduce a controllability-based interpretability framework that quantifies how different parts of the input sequence (tokens or patches) influence the internal state dynamics of SSMs. We propose two complementary formulations: a Jacobian-based method applicable to any SSM architecture that measures influence through the full chain of state propagation, and a Gramian-based approach for diagonal SSMs that achieves superior speed through closed-form analytical solutions. Both methods operate in a single forward pass with linear complexity, requiring no architectural modifications or hyperparameter tuning. We validate our framework through experiments on three diverse medical imaging modalities, demonstrating that SSMs naturally implement hierarchical feature refinement from diffuse low-level textures in early layers to focused, clinically meaningful patterns in deeper layers. Our analysis reveals domain-specific controllability signatures aligned with diagnostic criteria, progressive spatial selectivity across the network hierarchy, and the substantial influence of scanning strategies on attention patterns. Beyond medical imaging, we articulate applications spanning computer vision, natural language processing, and cross-domain tasks. Our framework establishes controllability analysis as a unified, foundational interpretability paradigm for SSMs across all domains. Code and analysis tools will be made available upon publication

</details>


### [778] [Semantic Document Derendering: SVG Reconstruction via Vision-Language Modeling](https://arxiv.org/abs/2511.13478)
*Adam Hazimeh,Ke Wang,Mark Collier,Gilles Baechler,Efi Kokiopoulou,Pascal Frossard*

Main category: cs.CV

TL;DR: 提出SliDer框架用VLMs将幻灯片图像转换为可编辑SVG格式，引入Slide2SVG数据集，表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有几何光栅矢量化方法处理复杂文档时无法保留高层结构，不能区分图像和文本元素语义，需解决语义文档去渲染问题。

Method: 引入SliDer框架，利用VLMs检测和提取输入图像和文本元素属性并组织成SVG格式，推理时迭代优化预测；引入Slide2SVG数据集。

Result: SliDer重建LPIPS为0.069，82.9%的情况受人类评估者青睐，优于零样本VLM基线。

Conclusion: SliDer框架在幻灯片图像去渲染为可编辑SVG格式上有效，有较好重建效果和用户认可度。

Abstract: Multimedia documents such as slide presentations and posters are designed to be interactive and easy to modify. Yet, they are often distributed in a static raster format, which limits editing and customization. Restoring their editability requires converting these raster images back into structured vector formats. However, existing geometric raster-vectorization methods, which rely on low-level primitives like curves and polygons, fall short at this task. Specifically, when applied to complex documents like slides, they fail to preserve the high-level structure, resulting in a flat collection of shapes where the semantic distinction between image and text elements is lost. To overcome this limitation, we address the problem of semantic document derendering by introducing SliDer, a novel framework that uses Vision-Language Models (VLMs) to derender slide images as compact and editable Scalable Vector Graphic (SVG) representations. SliDer detects and extracts attributes from individual image and text elements in a raster input and organizes them into a coherent SVG format. Crucially, the model iteratively refines its predictions during inference in a process analogous to human design, generating SVG code that more faithfully reconstructs the original raster upon rendering. Furthermore, we introduce Slide2SVG, a novel dataset comprising raster-SVG pairs of slide documents curated from real-world scientific presentations, to facilitate future research in this domain. Our results demonstrate that SliDer achieves a reconstruction LPIPS of 0.069 and is favored by human evaluators in 82.9% of cases compared to the strongest zero-shot VLM baseline.

</details>


### [779] [RoCoISLR: A Romanian Corpus for Isolated Sign Language Recognition](https://arxiv.org/abs/2511.12767)
*Cătălin-Alexandru Rîpanu,Andrei-Theodor Hotnog,Giulia-Stefania Imbrea,Dumitru-Clementin Cercel*

Main category: cs.CV

TL;DR: 本文引入用于罗马尼亚孤立手语识别的新语料库RoCoISLR，并对七种视频识别模型进行基准测试，发现基于Transformer的架构表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有罗马尼亚孤立手语识别缺乏大规模标准化数据集，限制研究进展。

Method: 引入包含9000多个视频样本、近6000个标准化词汇的RoCoISLR语料库，在一致实验设置下评估七种视频识别模型，并与WLASL2000语料库对比。

Result: 基于Transformer的架构优于卷积基线，Swin Transformer的Top - 1准确率达34.1%。

Conclusion: 基准测试凸显低资源手语长尾类别分布的挑战，RoCoISLR为罗马尼亚孤立手语识别的系统研究提供基础。

Abstract: Automatic sign language recognition plays a crucial role in bridging the communication gap between deaf communities and hearing individuals; however, most available datasets focus on American Sign Language. For Romanian Isolated Sign Language Recognition (RoISLR), no large-scale, standardized dataset exists, which limits research progress. In this work, we introduce a new corpus for RoISLR, named RoCoISLR, comprising over 9,000 video samples that span nearly 6,000 standardized glosses from multiple sources. We establish benchmark results by evaluating seven state-of-the-art video recognition models-I3D, SlowFast, Swin Transformer, TimeSformer, Uniformer, VideoMAE, and PoseConv3D-under consistent experimental setups, and compare their performance with that of the widely used WLASL2000 corpus. According to the results, transformer-based architectures outperform convolutional baselines; Swin Transformer achieved a Top-1 accuracy of 34.1%. Our benchmarks highlight the challenges associated with long-tail class distributions in low-resource sign languages, and RoCoISLR provides the initial foundation for systematic RoISLR research.

</details>


### [780] [Robust Defense Strategies for Multimodal Contrastive Learning: Efficient Fine-tuning Against Backdoor Attacks](https://arxiv.org/abs/2511.13545)
*Md. Iqbal Hossain,Afia Sajeeda,Neeresh Kumar Perla,Ming Shao*

Main category: cs.CV

TL;DR: 本文提出创新策略增强多模态对比学习模型对抗后门攻击的鲁棒性，通过图像分割“oracle”识别触发因素和受影响样本标签，实验证明策略有效。


<details>
  <summary>Details</summary>
Motivation: 多模态深度学习模型如CLIP易受后门攻击，现有防御方法存在不足，需新策略增强模型鲁棒性。

Method: 引入图像分割“oracle”监督中毒CLIP输出，开发两种算法：区分CLIP和Oracle知识识别潜在触发因素；定位受影响标签和样本并创建微调数据集。

Result: 在视觉识别基准上的大量实验表明该策略在基于CLIP的后门防御中有效。

Conclusion: 提出的策略能有效增强多模态对比学习模型对后门攻击的鲁棒性。

Abstract: The advent of multimodal deep learning models, such as CLIP, has unlocked new frontiers in a wide range of applications, from image-text understanding to classification tasks. However, these models are not safe for adversarial attacks, particularly backdoor attacks, which can subtly manipulate model behavior. Moreover, existing defense methods typically involve training from scratch or fine-tuning using a large dataset without pinpointing the specific labels that are affected. In this study, we introduce an innovative strategy to enhance the robustness of multimodal contrastive learning models against such attacks. In particular, given a poisoned CLIP model, our approach can identify the backdoor trigger and pinpoint the victim samples and labels in an efficient manner. To that end, an image segmentation ``oracle'' is introduced as the supervisor for the output of the poisoned CLIP. We develop two algorithms to rectify the poisoned model: (1) differentiating between CLIP and Oracle's knowledge to identify potential triggers; (2) pinpointing affected labels and victim samples, and curating a compact fine-tuning dataset. With this knowledge, we are allowed to rectify the poisoned CLIP model to negate backdoor effects. Extensive experiments on visual recognition benchmarks demonstrate our strategy is effective in CLIP-based backdoor defense.

</details>


### [781] [Hierarchical Prompt Learning for Image- and Text-Based Person Re-Identification](https://arxiv.org/abs/2511.13575)
*Linhan Zhou,Shuang Li,Neng Dong,Yonghang Tai,Yafei Zhang,Huafeng Li*

Main category: cs.CV

TL;DR: 提出Hierarchical Prompt Learning (HPL)统一框架联合优化行人重识别I2I和T2I任务，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法分开处理I2I和T2I任务，会导致表示纠缠和性能不佳，需要统一框架解决问题。

Method: 提出HPL框架，引入Task - Routed Transformer，开发分层提示生成方案，提出跨模态提示正则化策略。

Result: 在多个行人重识别基准测试上取得了最先进的性能。

Conclusion: 所提的HPL框架能有效联合优化I2I和T2I任务。

Abstract: Person re-identification (ReID) aims to retrieve target pedestrian images given either visual queries (image-to-image, I2I) or textual descriptions (text-to-image, T2I). Although both tasks share a common retrieval objective, they pose distinct challenges: I2I emphasizes discriminative identity learning, while T2I requires accurate cross-modal semantic alignment. Existing methods often treat these tasks separately, which may lead to representation entanglement and suboptimal performance. To address this, we propose a unified framework named Hierarchical Prompt Learning (HPL), which leverages task-aware prompt modeling to jointly optimize both tasks. Specifically, we first introduce a Task-Routed Transformer, which incorporates dual classification tokens into a shared visual encoder to route features for I2I and T2I branches respectively. On top of this, we develop a hierarchical prompt generation scheme that integrates identity-level learnable tokens with instance-level pseudo-text tokens. These pseudo-tokens are derived from image or text features via modality-specific inversion networks, injecting fine-grained, instance-specific semantics into the prompts. Furthermore, we propose a Cross-Modal Prompt Regularization strategy to enforce semantic alignment in the prompt token space, ensuring that pseudo-prompts preserve source-modality characteristics while enhancing cross-modal transferability. Extensive experiments on multiple ReID benchmarks validate the effectiveness of our method, achieving state-of-the-art performance on both I2I and T2I tasks.

</details>


### [782] [VVS: Accelerating Speculative Decoding for Visual Autoregressive Generation via Partial Verification Skipping](https://arxiv.org/abs/2511.13587)
*Haotian Dong,Ye Li,Rongwei Lu,Chen Tang,Shu-Tao Xia,Zhi Wang*

Main category: cs.CV

TL;DR: 本文提出VVS框架，通过部分验证跳过加速视觉自回归生成，减少推理延迟，在速度和质量上表现优异。


<details>
  <summary>Details</summary>
Motivation: 视觉自回归生成模型推理延迟大，现有投机解码范式限制加速潜力，受视觉令牌可互换性启发，探索验证跳过以减少目标模型前向传播次数。

Method: 提出VVS框架，包含动态截断的免验证令牌选择器、令牌级特征缓存和重用、细粒度跳过步骤调度三个互补模块。

Result: VVS相对普通自回归解码减少2.8倍目标模型前向传播次数，保持有竞争力的生成质量。

Conclusion: VVS在速度 - 质量权衡上优于传统投机解码框架，有重塑投机解码范式的潜力。

Abstract: Visual autoregressive (AR) generation models have demonstrated strong potential for image generation, yet their next-token-prediction paradigm introduces considerable inference latency. Although speculative decoding (SD) has been proven effective for accelerating visual AR models, its "draft one step, then verify one step" paradigm prevents a direct reduction of the forward passes, thus restricting acceleration potential. Motivated by the visual token interchangeability, we for the first time to explore verification skipping in the SD process of visual AR model generation to explicitly cut the number of target model forward passes, thereby reducing inference latency. Based on an analysis of the drafting stage's characteristics, we observe that verification redundancy and stale feature reusability are key factors to retain generation quality and speedup for verification-free steps. Inspired by these two observations, we propose a novel SD framework VVS to accelerate visual AR generation via partial verification skipping, which integrates three complementary modules: (1) a verification-free token selector with dynamical truncation, (2) token-level feature caching and reuse, and (3) fine-grained skipped step scheduling. Consequently, VVS reduces the number of target model forward passes by a factor of $2.8\times$ relative to vanilla AR decoding while maintaining competitive generation quality, offering a superior speed-quality trade-off over conventional SD frameworks and revealing strong potential to reshape the SD paradigm.

</details>


### [783] [Alpha Divergence Losses for Biometric Verification](https://arxiv.org/abs/2511.13621)
*Dimitrios Koutsianos,Ladislav Mosner,Yannis Panagakis,Themos Stafylakis*

Main category: cs.CV

TL;DR: 本文探索两种途径将角裕度集成到α - 散度损失函数中，推导了两种基于裕度的α - 散度损失函数，解决了训练不稳定问题，在人脸和说话人验证基准上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: α - 散度损失函数有诱导稀疏解的优势，但将角裕度集成到其中并不直接，需要探索集成方法以提升人脸和说话人验证性能。

Method: 通过参考度量或对数几率两种途径集成角裕度，推导Q - Margin和A3M两种损失函数，用原型重新初始化策略解决A3M训练不稳定问题。

Result: 在IJB - B和IJB - C人脸验证基准以及VoxCeleb说话人验证上取得显著性能提升，在低误检率下显著优于强基线。

Conclusion: 提出的基于裕度的α - 散度损失函数有效提升了人脸和说话人验证性能，在低误检率下表现出色，适用于高安全应用。

Abstract: Performance in face and speaker verification is largely driven by margin based softmax losses like CosFace and ArcFace. Recently introduced $α$-divergence loss functions offer a compelling alternative, particularly for their ability to induce sparse solutions (when $α>1$). However, integrating an angular margin-crucial for verification tasks-is not straightforward. We find this integration can be achieved in at least two distinct ways: via the reference measure (prior probabilities) or via the logits (unnormalized log-likelihoods). In this paper, we explore both pathways, deriving two novel margin-based $α$-divergence losses: Q-Margin (margin in the reference measure) and A3M (margin in the logits). We identify and address a critical training instability in A3M-caused by the interplay of penalized logits and sparsity-with a simple yet effective prototype re-initialization strategy. Our methods achieve significant performance gains on the challenging IJB-B and IJB-C face verification benchmarks. We demonstrate similarly strong performance in speaker verification on VoxCeleb. Crucially, our models significantly outperform strong baselines at low false acceptance rates (FAR). This capability is crucial for practical high-security applications, such as banking authentication, when minimizing false authentications is paramount.

</details>


### [784] [MCAQ-YOLO: Morphological Complexity-Aware Quantization for Efficient Object Detection with Curriculum Learning](https://arxiv.org/abs/2511.12976)
*Yoonjae Seo,Ermal Elbasani,Jaehong Lee*

Main category: cs.CV

TL;DR: 本文提出MCAQ - YOLO形态复杂度感知量化框架用于目标检测，结合形态指标和课程式量化感知训练，实验显示其在检测精度和收敛效率上优于均匀量化。


<details>
  <summary>Details</summary>
Motivation: 多数神经网络量化方法采用统一比特精度，忽略视觉数据结构和纹理复杂度的异构性。

Method: 采用五个形态指标表征局部视觉形态，指导自适应比特分配；使用课程式量化感知训练方案逐步增加量化难度。

Result: MCAQ - YOLO在安全设备数据集上mAP@0.5达85.6%，平均4.2比特、压缩比7.6x，比均匀4比特量化高3.5个百分点mAP，每图仅增加1.8ms开销；跨数据集验证有一致性能提升。

Conclusion: 形态驱动的空间量化可提升计算受限、安全关键视觉识别任务的效率和鲁棒性。

Abstract: Most neural network quantization methods apply uniform bit precision across spatial regions, ignoring the heterogeneous structural and textural complexity of visual data. This paper introduces MCAQ-YOLO, a morphological complexity-aware quantization framework for object detection. The framework employs five morphological metrics - fractal dimension, texture entropy, gradient variance, edge density, and contour complexity - to characterize local visual morphology and guide spatially adaptive bit allocation. By correlating these metrics with quantization sensitivity, MCAQ-YOLO dynamically adjusts bit precision according to spatial complexity. In addition, a curriculum-based quantization-aware training scheme progressively increases quantization difficulty to stabilize optimization and accelerate convergence. Experimental results demonstrate a strong correlation between morphological complexity and quantization sensitivity and show that MCAQ-YOLO achieves superior detection accuracy and convergence efficiency compared with uniform quantization. On a safety equipment dataset, MCAQ-YOLO attains 85.6 percent mAP@0.5 with an average of 4.2 bits and a 7.6x compression ratio, yielding 3.5 percentage points higher mAP than uniform 4-bit quantization while introducing only 1.8 ms of additional runtime overhead per image. Cross-dataset validation on COCO and Pascal VOC further confirms consistent performance gains, indicating that morphology-driven spatial quantization can enhance efficiency and robustness for computationally constrained, safety-critical visual recognition tasks.

</details>


### [785] [UnSAMv2: Self-Supervised Learning Enables Segment Anything at Any Granularity](https://arxiv.org/abs/2511.13714)
*Junwei Yu,Trevor Darrell,XuDong Wang*

Main category: cs.CV

TL;DR: SAM模型控制分割粒度能力有限，本文提出UnSAMv2，用少量无标签数据和粒度感知自监督学习方法，在多任务和多基准测试中提升性能。


<details>
  <summary>Details</summary>
Motivation: SAM模型控制分割粒度能力有限，手动细化结果过程模糊且收集全粒度密集标注成本高，监督解决方案不可行。

Method: 引入UnSAMv2，扩展UnSAM的分治策略，发现大量掩码 - 粒度对，引入新的粒度控制嵌入，实现对分割尺度的精确连续控制。

Result: 仅用6K无标签图像和0.02%额外参数，显著增强SAM - 2，在交互、全图像和视频分割任务中实现任意粒度分割；在超11个基准测试中提升多项指标。

Conclusion: 少量无标签数据结合粒度感知自监督学习方法可挖掘视觉基础模型潜力。

Abstract: The Segment Anything Model (SAM) family has become a widely adopted vision foundation model, but its ability to control segmentation granularity remains limited. Users often need to refine results manually - by adding more prompts or selecting from pre-generated masks - to achieve the desired level of detail. This process can be ambiguous, as the same prompt may correspond to several plausible masks, and collecting dense annotations across all granularities is prohibitively expensive, making supervised solutions infeasible. To address this limitation, we introduce UnSAMv2, which enables segment anything at any granularity without human annotations. UnSAMv2 extends the divide-and-conquer strategy of UnSAM by discovering abundant mask-granularity pairs and introducing a novel granularity control embedding that enables precise, continuous control over segmentation scale. Remarkably, with only $6$K unlabeled images and $0.02\%$ additional parameters, UnSAMv2 substantially enhances SAM-2, achieving segment anything at any granularity across interactive, whole-image, and video segmentation tasks. Evaluated on over $11$ benchmarks, UnSAMv2 improves $\text{NoC}_{90}$ (5.69 $\rightarrow$ 4.75), 1-IoU (58.0 $\rightarrow$ 73.1), and $\text{AR}_{1000}$ (49.6 $\rightarrow$ 68.3), showing that small amounts of unlabeled data with a granularity-aware self-supervised learning method can unlock the potential of vision foundation models.

</details>


### [786] [Scaling Spatial Intelligence with Multimodal Foundation Models](https://arxiv.org/abs/2511.13719)
*Zhongang Cai,Ruisi Wang,Chenyang Gu,Fanyi Pu,Junxiang Xu,Yubo Wang,Wanqi Yin,Zhitao Yang,Chen Wei,Qingping Sun,Tongxi Zhou,Jiaqi Li,Hui En Pang,Oscar Qian,Yukun Wei,Zhiqian Lin,Xuanke Shi,Kewang Deng,Xiaoyang Han,Zukai Chen,Xiangyu Fan,Hanming Deng,Lewei Lu,Liang Pan,Bo Li,Ziwei Liu,Quan Wang,Dahua Lin,Lei Yang*

Main category: cs.CV

TL;DR: 本文探索扩展多模态基础模型以培养空间智能，构建SenseNova - SI系列，展示出优异性能，分析相关影响并公开模型。


<details>
  <summary>Details</summary>
Motivation: 多模态基础模型在空间智能方面存在不足，需要进行改进。

Method: 基于现有多模态基础，系统策划包含八百万多样本的SenseNova - SI - 8M数据集，构建高性能和稳健的空间智能。

Result: SenseNova - SI在多个空间智能基准测试中表现出色，同时保持较强的通用多模态理解能力。

Conclusion: SenseNova - SI是一个持续项目，公开新训练模型以推动该方向的进一步研究。

Abstract: Despite remarkable progress, multimodal foundation models still exhibit surprising deficiencies in spatial intelligence. In this work, we explore scaling up multimodal foundation models to cultivate spatial intelligence within the SenseNova-SI family, built upon established multimodal foundations including visual understanding models (i.e., Qwen3-VL and InternVL3) and unified understanding and generation models (i.e., Bagel). We take a principled approach to constructing high-performing and robust spatial intelligence by systematically curating SenseNova-SI-8M: eight million diverse data samples under a rigorous taxonomy of spatial capabilities. SenseNova-SI demonstrates unprecedented performance across a broad range of spatial intelligence benchmarks: 68.7% on VSI-Bench, 43.3% on MMSI, 85.6% on MindCube, 54.6% on ViewSpatial, and 50.1% on SITE, while maintaining strong general multimodal understanding (e.g., 84.9% on MMBench-En). More importantly, we analyze the impact of data scaling, discuss early signs of emergent generalization capabilities enabled by diverse data training, analyze the risk of overfitting and language shortcuts, present a preliminary study on spatial chain-of-thought reasoning, and validate the potential downstream application. SenseNova-SI is an ongoing project, and this report will be updated continuously. All newly trained multimodal foundation models are publicly released to facilitate further research in this direction.

</details>


### [787] [BootOOD: Self-Supervised Out-of-Distribution Detection via Synthetic Sample Exposure under Neural Collapse](https://arxiv.org/abs/2511.13539)
*Yuanchao Wang,Tian Qin,Eduardo Valle,Bruno Abrahao*

Main category: cs.CV

TL;DR: 提出BootOOD框架用于OOD检测，在多数据集表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测器在处理与ID类语义相似的OOD样本时存在困难，需要更好的检测方法。

Method: 仅从ID数据自监督引导，通过简单变换ID表示合成伪OOD特征，利用Neural Collapse，引入轻量级辅助头基于特征范数分类。

Result: 在CIFAR - 10、CIFAR - 100和ImageNet - 200上，BootOOD优于先前事后方法，在无异常暴露下超越基于训练的方法，与最先进的异常暴露方法有竞争力，同时维持或提高ID准确率。

Conclusion: BootOOD是一种有效的OOD检测框架，能处理语义具有挑战性的OOD样本。

Abstract: Out-of-distribution (OOD) detection is critical for deploying image classifiers in safety-sensitive environments, yet existing detectors often struggle when OOD samples are semantically similar to the in-distribution (ID) classes. We present BootOOD, a fully self-supervised OOD detection framework that bootstraps exclusively from ID data and is explicitly designed to handle semantically challenging OOD samples. BootOOD synthesizes pseudo-OOD features through simple transformations of ID representations and leverages Neural Collapse (NC), where ID features cluster tightly around class means with consistent feature norms. Unlike prior approaches that aim to constrain OOD features into subspaces orthogonal to the collapsed ID means, BootOOD introduces a lightweight auxiliary head that performs radius-based classification on feature norms. This design decouples OOD detection from the primary classifier and imposes a relaxed requirement: OOD samples are learned to have smaller feature norms than ID features, which is easier to satisfy when ID and OOD are semantically close. Experiments on CIFAR-10, CIFAR-100, and ImageNet-200 show that BootOOD outperforms prior post-hoc methods, surpasses training-based methods without outlier exposure, and is competitive with state-of-the-art outlier-exposure approaches while maintaining or improving ID accuracy.

</details>


### [788] [AtlasMorph: Learning conditional deformable templates for brain MRI](https://arxiv.org/abs/2511.13609)
*Marianne Rakic,Andrew Hoopes,S. Mazdak Abulnaga,Mert R. Sabuncu,John V. Guttag,Adrian V. Dalca*

Main category: cs.CV

TL;DR: 提出用卷积配准神经网络学习输出特定模板的函数，可生成解剖分割图，在脑MRI数据集验证效果好。


<details>
  <summary>Details</summary>
Motivation: 开发模板计算成本高，可用模板少，常使用非最优模板，不能代表研究群体。

Method: 提出机器学习框架，用卷积配准神经网络学习基于特定属性输出模板的函数，利用分割数据生成解剖分割图，还可用于图像配准。

Result: 在3D脑MRI数据集上学习到代表群体的高质量模板，标注的条件模板配准效果优于未标注的无条件模板，且优于其他模板构建方法。

Conclusion: 所提方法能有效学习高质量模板，在模板构建和配准方面表现良好。

Abstract: Deformable templates, or atlases, are images that represent a prototypical anatomy for a population, and are often enhanced with probabilistic anatomical label maps. They are commonly used in medical image analysis for population studies and computational anatomy tasks such as registration and segmentation. Because developing a template is a computationally expensive process, relatively few templates are available. As a result, analysis is often conducted with sub-optimal templates that are not truly representative of the study population, especially when there are large variations within this population. We propose a machine learning framework that uses convolutional registration neural networks to efficiently learn a function that outputs templates conditioned on subject-specific attributes, such as age and sex. We also leverage segmentations, when available, to produce anatomical segmentation maps for the resulting templates. The learned network can also be used to register subject images to the templates. We demonstrate our method on a compilation of 3D brain MRI datasets, and show that it can learn high-quality templates that are representative of populations. We find that annotated conditional templates enable better registration than their unlabeled unconditional counterparts, and outperform other templates construction methods.

</details>


### [789] [OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation](https://arxiv.org/abs/2511.13655)
*Henry Herzog,Favyen Bastani,Yawen Zhang,Gabriel Tseng,Joseph Redmon,Hadrien Sablon,Ryan Park,Jacob Morrison,Alexandra Buraczynski,Karen Farley,Joshua Hansen,Andrew Howe,Patrick Alan Johnson,Mark Otterlee,Ted Schmitt,Hunter Pitelka,Stephen Daspit,Rachel Ratner,Christopher Wilhelm,Sebastian Wood,Mike Jacobi,Hannah Kerner,Evan Shelhamer,Ali Farhadi,Ranjay Krishna,Patrick Beukema*

Main category: cs.CV

TL;DR: 提出多模态时空基础模型OlmoEarth，在多基准和任务中表现出色，还部署为端到端平台并开源。


<details>
  <summary>Details</summary>
Motivation: 应对地球观测数据空间、序列和多模态的独特挑战。

Method: 采用为地球观测领域设计的新型自监督学习公式、掩码策略和损失函数。

Result: 与12个其他基础模型相比，在多种研究基准和实际任务中达到了最先进的性能，评估嵌入在15/24任务中最佳，全微调在19/29任务中最佳。

Conclusion: 将OlmoEarth部署为端到端平台，让非盈利组织和NGO能使用前沿模型和数据管理工具。

Abstract: Earth observation data presents a unique challenge: it is spatial like images, sequential like video or text, and highly multimodal. We present OlmoEarth: a multimodal, spatio-temporal foundation model that employs a novel self-supervised learning formulation, masking strategy, and loss all designed for the Earth observation domain. OlmoEarth achieves state-of-the-art performance compared to 12 other foundation models across a variety of research benchmarks and real-world tasks from external partners. When evaluating embeddings OlmoEarth achieves the best performance on 15 out of 24 tasks, and with full fine-tuning it is the best on 19 of 29 tasks. We deploy OlmoEarth as the backbone of an end-to-end platform for data collection, labeling, training, and inference of Earth observation models. The OlmoEarth Platform puts frontier foundation models and powerful data management tools into the hands of non-profits and NGOs working to solve the world's biggest problems. OlmoEarth source code, training data, and pre-trained weights are available at $\href{https://github.com/allenai/olmoearth_pretrain}{\text{https://github.com/allenai/olmoearth_pretrain}}$.

</details>


### [790] [Training-Free Multi-View Extension of IC-Light for Textual Position-Aware Scene Relighting](https://arxiv.org/abs/2511.13684)
*Jiangnan Ye,Jiedong Zhuang,Lianrui Mu,Wenjie Zheng,Jiaqi Hu,Xingze Zou,Jing Wang,Haoji Hu*

Main category: cs.CV

TL;DR: 提出GS - Light用于3D场景文本引导重光照，通过多视图输入、解析提示、融合信息生成初始潜码等步骤实现重光照，评估显示优于基线。


<details>
  <summary>Details</summary>
Motivation: 实现高效的文本引导3D场景重光照，使重光照结果更符合用户期望。

Method: 实现单输入扩散模型的无训练扩展以处理多视图输入，用LVLM解析提示，融合光照先验与视图几何约束计算光照图和初始潜码，多视图重光照模型生成图像，微调3DGS场景。

Result: 在室内外场景评估中，使用定量和定性评估方法，GS - Light相比现有基线有持续改进。

Conclusion: GS - Light是一种有效的3D场景文本引导重光照方法，优于当前先进的基线方法。

Abstract: We introduce GS-Light, an efficient, textual position-aware pipeline for text-guided relighting of 3D scenes represented via Gaussian Splatting (3DGS). GS-Light implements a training-free extension of a single-input diffusion model to handle multi-view inputs. Given a user prompt that may specify lighting direction, color, intensity, or reference objects, we employ a large vision-language model (LVLM) to parse the prompt into lighting priors. Using off-the-shelf estimators for geometry and semantics (depth, surface normals, and semantic segmentation), we fuse these lighting priors with view-geometry constraints to compute illumination maps and generate initial latent codes for each view. These meticulously derived init latents guide the diffusion model to generate relighting outputs that more accurately reflect user expectations, especially in terms of lighting direction. By feeding multi-view rendered images, along with the init latents, into our multi-view relighting model, we produce high-fidelity, artistically relit images. Finally, we fine-tune the 3DGS scene with the relit appearance to obtain a fully relit 3D scene. We evaluate GS-Light on both indoor and outdoor scenes, comparing it to state-of-the-art baselines including per-view relighting, video relighting, and scene editing methods. Using quantitative metrics (multi-view consistency, imaging quality, aesthetic score, semantic similarity, etc.) and qualitative assessment (user studies), GS-Light demonstrates consistent improvements over baselines. Code and assets will be made available upon publication.

</details>
