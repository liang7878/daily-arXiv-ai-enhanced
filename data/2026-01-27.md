<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 69]
- [cs.CE](#cs.CE) [Total: 4]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.DS](#cs.DS) [Total: 4]
- [cs.GT](#cs.GT) [Total: 10]
- [cs.IR](#cs.IR) [Total: 29]
- [cs.LG](#cs.LG) [Total: 167]
- [cs.NE](#cs.NE) [Total: 6]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.SE](#cs.SE) [Total: 25]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [q-fin.TR](#q-fin.TR) [Total: 2]
- [stat.ML](#stat.ML) [Total: 8]
- [stat.CO](#stat.CO) [Total: 1]
- [math.OC](#math.OC) [Total: 4]
- [cs.CR](#cs.CR) [Total: 16]
- [cs.PL](#cs.PL) [Total: 2]
- [econ.GN](#econ.GN) [Total: 2]
- [eess.SY](#eess.SY) [Total: 2]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [math.LO](#math.LO) [Total: 1]
- [eess.SP](#eess.SP) [Total: 4]
- [econ.TH](#econ.TH) [Total: 1]
- [cs.AR](#cs.AR) [Total: 3]
- [eess.AS](#eess.AS) [Total: 5]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [math.CO](#math.CO) [Total: 1]
- [cs.RO](#cs.RO) [Total: 4]
- [cs.MM](#cs.MM) [Total: 1]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [cs.CL](#cs.CL) [Total: 53]
- [cs.SD](#cs.SD) [Total: 9]
- [cs.MS](#cs.MS) [Total: 1]
- [cs.IT](#cs.IT) [Total: 3]
- [stat.ME](#stat.ME) [Total: 7]
- [math.NA](#math.NA) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.DL](#cs.DL) [Total: 2]
- [q-bio.PE](#q-bio.PE) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 3]
- [cs.CY](#cs.CY) [Total: 21]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [quant-ph](#quant-ph) [Total: 5]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [cs.CV](#cs.CV) [Total: 53]
- [cs.HC](#cs.HC) [Total: 14]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [math.HO](#math.HO) [Total: 1]
- [math.FA](#math.FA) [Total: 1]
- [hep-ex](#hep-ex) [Total: 1]
- [cs.NI](#cs.NI) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Online parameter estimation for the Crazyflie quadcopter through an EM algorithm](https://arxiv.org/abs/2601.17009)
*Yanhua Zhao*

Main category: cs.AI

TL;DR: 本文研究四旋翼无人机，添加随机噪声研究其影响，用扩展卡尔曼滤波器估计状态，实现线性二次高斯控制器，应用期望最大化算法进行参数估计，对比离线和在线参数估计结果。


<details>
  <summary>Details</summary>
Motivation: 研究随机噪声对四旋翼无人机系统的影响，优化其参数估计。

Method: 向四旋翼无人机系统添加随机噪声，用扩展卡尔曼滤波器基于传感器含噪观测估计状态，基于SDE系统实现线性二次高斯控制器，应用期望最大化算法进行参数估计。

Result: 在线参数估计的收敛值范围比离线参数估计略大。

Conclusion: 在线参数估计在收敛值范围上有一定优势。

Abstract: Drones are becoming more and more popular nowadays. They are small in size, low in cost, and reliable in operation. They contain a variety of sensors and can perform a variety of flight tasks, reaching places that are difficult or inaccessible for humans. Earthquakes damage a lot of infrastructure, making it impossible for rescuers to reach some areas. But drones can help. Many amateur and professional photographers like to use drones for aerial photography. Drones play a non-negligible role in agriculture and transportation too. Drones can be used to spray pesticides, and they can also transport supplies. A quadcopter is a four-rotor drone and has been studied in this paper. In this paper, random noise is added to the quadcopter system and its effects on the drone system are studied. An extended Kalman filter has been used to estimate the state based on noisy observations from the sensor. Based on a SDE system, a linear quadratic Gaussian controller has been implemented. The expectation maximization algorithm has been applied for parameter estimation of the quadcopter. The results of offline parameter estimation and online parameter estimation are presented. The results show that the online parameter estimation has a slightly larger range of convergence values than the offline parameter estimation.

</details>


### [2] [Interpreting Agentic Systems: Beyond Model Explanations to System-Level Accountability](https://arxiv.org/abs/2601.17168)
*Judy Zhu,Dhari Gandhi,Himanshu Joshi,Ahmad Rezaie Mianroodi,Sedef Akinli Kocak,Dhanesh Ramachandran*

Main category: cs.AI

TL;DR: 本文评估现有可解释性方法在代理系统中的适用性与局限性，提出专为代理系统开发可解释性技术的未来方向，以确保代理 AI 系统安全负责部署。


<details>
  <summary>Details</summary>
Motivation: 代理系统带来独特 AI 安全挑战，现有主要为静态模型开发的可解释性技术存在局限性，需要新方法。

Method: 评估现有可解释性方法在代理系统中的情况，找出其在理解代理决策方面的差距。

Result: 明确了现有方法存在不足。

Conclusion: 开发适用于代理系统的可解释性技术对确保代理 AI 系统安全负责部署至关重要。

Abstract: Agentic systems have transformed how Large Language Models (LLMs) can be leveraged to create autonomous systems with goal-directed behaviors, consisting of multi-step planning and the ability to interact with different environments. These systems differ fundamentally from traditional machine learning models, both in architecture and deployment, introducing unique AI safety challenges, including goal misalignment, compounding decision errors, and coordination risks among interacting agents, that necessitate embedding interpretability and explainability by design to ensure traceability and accountability across their autonomous behaviors. Current interpretability techniques, developed primarily for static models, show limitations when applied to agentic systems. The temporal dynamics, compounding decisions, and context-dependent behaviors of agentic systems demand new analytical approaches. This paper assesses the suitability and limitations of existing interpretability methods in the context of agentic systems, identifying gaps in their capacity to provide meaningful insight into agent decision-making. We propose future directions for developing interpretability techniques specifically designed for agentic systems, pinpointing where interpretability is required to embed oversight mechanisms across the agent lifecycle from goal formation, through environmental interaction, to outcome evaluation. These advances are essential to ensure the safe and accountable deployment of agentic AI systems.

</details>


### [3] [Implementing Tensor Logic: Unifying Datalog and Neural Reasoning via Tensor Contraction](https://arxiv.org/abs/2601.17188)
*Swapn Shah,Wlodek Zadrozny*

Main category: cs.AI

TL;DR: 本文通过三个实验对Tensor Logic框架进行了实证验证，证明了其在统一符号推理和神经网络方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决符号推理和神经网络统一这一人工智能领域的核心挑战，结合符号系统的可靠性和可解释性与神经网络的学习能力。

Method: 进行三个实验：计算圣经族谱图的传递闭包；在嵌入空间中使用可学习变换矩阵训练神经网络进行推理；在FB15k - 237知识图谱上验证Tensor Logic叠加构造。

Result: 计算族谱图传递闭包收敛并发现大量祖先关系；在嵌入空间实现零样本组合推理；在知识图谱链接预测和组合推理基准测试中取得一定MRR值。

Conclusion: Tensor Logic框架有效，矩阵组合能在无直接训练示例的情况下实现多跳推理。

Abstract: The unification of symbolic reasoning and neural networks remains a central challenge in artificial intelligence. Symbolic systems offer reliability and interpretability but lack scalability, while neural networks provide learning capabilities but sacrifice transparency. Tensor Logic, proposed by Domingos, suggests that logical rules and Einstein summation are mathematically equivalent, offering a principled path toward unification. This paper provides empirical validation of this framework through three experiments. First, we demonstrate the equivalence between recursive Datalog rules and iterative tensor contractions by computing the transitive closure of a biblical genealogy graph containing 1,972 individuals and 1,727 parent-child relationships, converging in 74 iterations to discover 33,945 ancestor relationships. Second, we implement reasoning in embedding space by training a neural network with learnable transformation matrices, demonstrating successful zero-shot compositional inference on held-out queries. Third, we validate the Tensor Logic superposition construction on FB15k-237, a large-scale knowledge graph with 14,541 entities and 237 relations. Using Domingos's relation matrix formulation $R_r = E^\top A_r E$, we achieve MRR of 0.3068 on standard link prediction and MRR of 0.3346 on a compositional reasoning benchmark where direct edges are removed during training, demonstrating that matrix composition enables multi-hop inference without direct training examples.

</details>


### [4] [High-Fidelity Longitudinal Patient Simulation Using Real-World Data](https://arxiv.org/abs/2601.17310)
*Yu Akagi,Tomohisa Seki,Hiromasa Ito,Toru Takiguchi,Kazuhiko Ohe,Yoshimasa Kawazoe*

Main category: cs.AI

TL;DR: 本文提出利用真实临床记录构建生成模拟器模型来模拟患者轨迹，模型经大量数据预训练后能生成高保真未来时间线并准确估计事件概率，揭示了电子健康记录中真实世界数据的价值。


<details>
  <summary>Details</summary>
Motivation: 模拟患者轨迹因复杂影响因素具有挑战性，挖掘模拟在临床医学中的潜力，如个性化治疗规划和虚拟临床试验。

Method: 开发以患者病史为输入的生成模拟器模型，在超2亿条临床记录上进行预训练。

Result: 模型生成高保真未来时间线，与真实患者未来数据在事件发生率、实验室检测结果和时间动态上紧密匹配，能准确估计未来事件概率，观察与预期比率接近1.0。

Conclusion: 揭示电子健康记录中真实世界数据的潜在价值，引入了临床护理计算机模拟建模的可扩展框架。

Abstract: Simulation is a powerful tool for exploring uncertainty. Its potential in clinical medicine is transformative and includes personalized treatment planning and virtual clinical trials. However, simulating patient trajectories is challenging because of complex biological and sociocultural influences. Here, we show that real-world clinical records can be leveraged to empirically model patient timelines. We developed a generative simulator model that takes a patient's history as input and synthesizes fine-grained, realistic future trajectories. The model was pretrained on more than 200 million clinical records. It produced high-fidelity future timelines, closely matching event occurrence rates, laboratory test results, and temporal dynamics in real patient future data. It also accurately estimated future event probabilities, with observed-to-expected ratios consistently near 1.0 across diverse outcomes and time horizons. Our results reveal the untapped value of real-world data in electronic health records and introduce a scalable framework for in silico modeling of clinical care.

</details>


### [5] [Phase Transition for Budgeted Multi-Agent Synergy](https://arxiv.org/abs/2601.17311)
*Bang Liu,Linglong Kong,Jian Pei*

Main category: cs.AI

TL;DR: 本文提出最小可校准理论，预测多智能体系统在固定推理预算下的不同表现，证明深度b元树存在相变，推导组织指数，刻画饱和现象，验证理论并解释LLM智能体系统瓶颈。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在固定推理预算下表现不稳定，需理论预测其不同状态。

Method: 基于现代智能体堆栈三个约束条件构建理论，对不同组织架构分析，从数学上证明相变，推导相关指数和规则。

Result: 证明相变存在，得到组织指数，推导出预算协同的条件及计算规则，刻画饱和现象，给出不同架构的风险表达式，模拟验证理论。

Conclusion: 理论能有效预测多智能体系统在固定推理预算下的表现，可解释LLM智能体系统的瓶颈问题。

Abstract: Multi-agent systems can improve reliability, yet under a fixed inference budget they often help, saturate, or even collapse. We develop a minimal and calibratable theory that predicts these regimes from three binding constraints of modern agent stacks: finite context windows, lossy inter-agent communication, and shared failures among similar agents. Each leaf agent is summarized by a compute-performance scaling exponent $β$; communication is captured by a message-length fidelity curve $γ(m)$; dependence is captured by an effective shared-error correlation $ρ$; and a context window $W$ imposes hard fan-in limits that make hierarchy necessary. For binary success/failure tasks with majority aggregation, we prove a sharp phase transition for deep $b$-ary trees with correlated inputs and lossy communication: a single scalar $α_ρ$ (combining $γ(m)$, $ρ$, and fan-in $b$) determines whether weak signal is amplified to a nontrivial fixed point or washed out to chance. In the amplifying regime, we derive an organization exponent $s$ and show that budgeted synergy, i.e., outperforming the best single agent under the same total budget, occurs exactly when $s>β$, yielding closed-form compute allocation rules and explicit budget thresholds. We further characterize saturation via a mixing depth and provide a conservative clipped predictor that remains accurate across growth and saturation. A continuous-performance warm-up gives closed-form risks for star, chain, and tree organizations, making correlation- and communication-induced floors explicit and exposing the core design trade-offs in a smooth setting. Finally, we validate the predicted phase boundaries in controlled synthetic simulations and show how the same mechanisms explain the dominant bottlenecks reported in recent large-scale matched-budget studies of LLM agent-system scaling.

</details>


### [6] [TheoremForge: Scaling up Formal Data Synthesis with Low-Budget Agentic Workflow](https://arxiv.org/abs/2601.17332)
*Yicheng Tao,Hongteng Xu*

Main category: cs.AI

TL;DR: 介绍TheoremForge，一种低成本的形式化数据合成管道，可有效利用计算资源，实验证明其在验证率、成本和数据产量上表现优秀，是构建数据飞轮的可扩展框架。


<details>
  <summary>Details</summary>
Motivation: 正式数学中智能体工作流成本高，阻碍大规模数据合成，开源语料稀缺。

Method: 引入TheoremForge，将形式化过程分解为五个子任务，采用解耦提取策略从全局失败轨迹中恢复有效训练信号。

Result: 在2000个问题的基准测试中，TheoremForge验证率达12.6%，超过8.6%的基线，每次成功轨迹平均成本仅0.481美元，证明生成的数据产量比标准过滤提高1.6倍。

Conclusion: TheoremForge是构建数据飞轮来训练未来专家模型的可扩展框架。

Abstract: The high cost of agentic workflows in formal mathematics hinders large-scale data synthesis, exacerbating the scarcity of open-source corpora. To address this, we introduce \textbf{TheoremForge}, a cost-effective formal data synthesis pipeline that decomposes the formalization process into five sub-tasks, which are \textit{statement formalization}, \textit{proof generation}, \textit{premise selection}, \textit{proof correction} and \textit{proof sketching}. By implementing a \textit{Decoupled Extraction Strategy}, the workflow recovers valid training signals from globally failed trajectories, effectively utilizing wasted computation. Experiments on a 2,000-problem benchmark demonstrate that TheoremForge achieves a Verified Rate of 12.6\%, surpassing the 8.6\% baseline, at an average cost of only \textbf{\$0.481} per successful trajectory using Gemini-3-Flash. Crucially, our strategy increases data yield by \textbf{1.6$\times$} for proof generation compared to standard filtering. These results establish TheoremForge as a scalable framework for constructing a data flywheel to train future expert models. Our code is available \href{https://github.com/timechess/TheoremForge}{here}.

</details>


### [7] [The Relativity of AGI: Distributional Axioms, Fragility, and Undecidability](https://arxiv.org/abs/2601.17335)
*Angshul Majumdar*

Main category: cs.AI

TL;DR: 本文研究通用人工智能（AGI）是否有连贯理论定义，提出公理化框架并得出四类结果，表明强的、与分布无关的AGI说法若无明确形式索引则无定义。


<details>
  <summary>Details</summary>
Motivation: 研究通用人工智能（AGI）是否存在支持存在性、鲁棒性或自我验证绝对主张的连贯理论定义。

Method: 将AGI公理化地形式化为一个分布性、资源受限的语义谓词，然后在该框架下进行推导。

Result: 得出四类结果：1. 普遍性本质上是相关的，不存在与分布无关的AGI概念；2. 任务分布的微小扰动会破坏AGI属性；3. 有限资源下跨任务族的泛化有界；4. 任何可计算程序都不能完全可靠地验证AGI，依赖内部自我验证的递归自我改进方案不成立。

Conclusion: 强的、与分布无关的AGI说法若无明确形式索引则无定义，AI的经验进展不意味着可实现自我验证的通用智能。

Abstract: We study whether Artificial General Intelligence (AGI) admits a coherent theoretical definition that supports absolute claims of existence, robustness, or self-verification. We formalize AGI axiomatically as a distributional, resource-bounded semantic predicate, indexed by a task family, a task distribution, a performance functional, and explicit resource budgets. Under this framework, we derive four classes of results. First, we show that generality is inherently relational: there is no distribution-independent notion of AGI. Second, we prove non-invariance results demonstrating that arbitrarily small perturbations of the task distribution can invalidate AGI properties via cliff sets, precluding universal robustness. Third, we establish bounded transfer guarantees, ruling out unbounded generalization across task families under finite resources. Fourth, invoking Rice-style and Gödel--Tarski arguments, we prove that AGI is a nontrivial semantic property and therefore cannot be soundly and completely certified by any computable procedure, including procedures implemented by the agent itself. Consequently, recursive self-improvement schemes that rely on internal self-certification of AGI are ill-posed. Taken together, our results show that strong, distribution-independent claims of AGI are not false but undefined without explicit formal indexing, and that empirical progress in AI does not imply the attainability of self-certifying general intelligence.

</details>


### [8] [LLM-Based SQL Generation: Prompting, Self-Refinement, and Adaptive Weighted Majority Voting](https://arxiv.org/abs/2601.17942)
*Yu-Jie Yang,Hung-Fu Chang,Po-An Chen*

Main category: cs.AI

TL;DR: 本文提出SSEV和ReCAPAgent - SQL解决自然语言转SQL难题，两方法在多基准测试取得良好效果，推动实用场景部署。


<details>
  <summary>Details</summary>
Motivation: 自然语言转SQL虽降低数据分析门槛，但因用户查询模糊、模式链接复杂等难以生成准确SQL。

Method: 提出无真实数据的SSEV管道，集成自我改进、加权多数投票等；提出ReCAPAgent - SQL框架，集成多种专门代理协作。

Result: SSEV在多基准测试中表现出色；ReCAPAgent - SQL在Spider 2.0 - Lite的前100个查询中执行准确率达31%。

Conclusion: 研究成果有助于低成本、高效地在实际场景部署可扩展的自然语言转SQL系统，支持数据驱动决策。

Abstract: Text-to-SQL has emerged as a prominent research area, particularly with the rapid advancement of large language models (LLMs). By enabling users to query databases through natural language rather than SQL, this technology significantly lowers the barrier to data analysis. However, generating accurate SQL from natural language remains challenging due to ambiguity in user queries, the complexity of schema linking, limited generalization across SQL dialects, and the need for domain-specific understanding. In this study, we propose a Single-Agent Self-Refinement with Ensemble Voting (SSEV) pipeline built on PET-SQL that operates without ground-truth data, integrating self-refinement with Weighted Majority Voting (WMV) and its randomized variant (RWMA). Experimental results show that the SSEV achieves competitive performance across multiple benchmarks, attaining execution accuracies of 85.5% on Spider 1.0-Dev, 86.4% on Spider 1.0-Test, and 66.3% on BIRD-Dev. Building on insights from the SSEV pipeline, we further propose ReCAPAgent-SQL (Refinement-Critique-Act-Plan agent-based SQL framework) to address the growing complexity of enterprise databases and real-world Text-to-SQL tasks. The framework integrates multiple specialized agents for planning, external knowledge retrieval, critique, action generation, self-refinement, schema linking, and result validation, enabling iterative refinement of SQL predictions through agent collaboration. ReCAPAgent-SQL's WMA results achieve 31% execution accuracy on the first 100 queries of Spider 2.0-Lite, demonstrating significant improvements in handling real-world enterprise scenarios. Overall, our work facilitates the deployment of scalable Text-to-SQL systems in practical settings, supporting better data-driven decision-making at lower cost and with greater efficiency.

</details>


### [9] [Are We Evaluating the Edit Locality of LLM Model Editing Properly?](https://arxiv.org/abs/2601.17343)
*Wei Liu,Haomei Xu,Hongkai Liu,Zhiying Deng,Ruixuan Li,Heng Huang,Yee Whye Teh,Wee Sun Lee*

Main category: cs.AI

TL;DR: 指出现有大语言模型知识编辑特异性评估协议不足，提出新评估协议，实验证明新协议指标更优。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型知识编辑特异性评估协议无法很好平衡编辑效果和特异性，存在不足。

Method: 系统阐述现有评估协议面临的三个基本问题，实证分析现有特异性指标与正则化强度的相关性和敏感性，提出新的评估协议。

Result: 新协议消除了开放式大语言模型与确定答案假设的冲突，避免了查询无关流畅性偏差，评估严格性可连续调整；新协议衍生的指标对特异性正则化强度变化更敏感，相关性更强，能更细粒度区分不同方法的知识保留能力。

Conclusion: 提出的评估协议优于现有协议，可更好评估大语言模型知识编辑的特异性。

Abstract: Model editing has recently emerged as a popular paradigm for efficiently updating knowledge in LLMs. A central desideratum of updating knowledge is to balance editing efficacy, i.e., the successful injection of target knowledge, and specificity (also known as edit locality), i.e., the preservation of existing non-target knowledge. However, we find that existing specificity evaluation protocols are inadequate for this purpose. We systematically elaborated on the three fundamental issues it faces. Beyond the conceptual issues, we further empirically demonstrate that existing specificity metrics are weakly correlated with the strength of specificity regularizers. We also find that current metrics lack sufficient sensitivity, rendering them ineffective at distinguishing the specificity performance of different methods. Finally, we propose a constructive evaluation protocol. Under this protocol, the conflict between open-ended LLMs and the assumption of determined answers is eliminated, query-independent fluency biases are avoided, and the evaluation strictness can be smoothly adjusted within a near-continuous space. Experiments across various LLMs, datasets, and editing methods show that metrics derived from the proposed protocol are more sensitive to changes in the strength of specificity regularizers and exhibit strong correlation with them, enabling more fine-grained discrimination of different methods' knowledge preservation capabilities.

</details>


### [10] [Multi-Agent Learning Path Planning via LLMs](https://arxiv.org/abs/2601.17346)
*Haoxin Xu,Changyong Qi,Tong Liu,Bohao Zhang,Anna He,Bingqian Jiang,Longwei Zheng,Xiaoqing Gu*

Main category: cs.AI

TL;DR: 研究提出基于大语言模型的多智能体学习路径规划框架MALPP，实验显示其性能优于基线模型，为教育领域可信赖AI发展做贡献。


<details>
  <summary>Details</summary>
Motivation: 现有学习路径规划方法缺乏透明度、适应性和以学习者为中心的可解释性，需改进。

Method: 提出MALPP框架，含三种特定任务智能体，通过结构化提示和预定义规则协作，结合认知负荷理论和最近发展区理论。

Result: 在MOOCCubeX数据集上实验表明，MALPP在路径质量、知识序列一致性和认知负荷对齐方面显著优于基线模型，消融实验验证协作机制和理论约束有效性。

Conclusion: 本研究有助于教育领域可信赖、可解释AI的发展，展示了以学习者为中心的自适应教学可扩展方法。

Abstract: The integration of large language models (LLMs) into intelligent tutoring systems offers transformative potential for personalized learning in higher education. However, most existing learning path planning approaches lack transparency, adaptability, and learner-centered explainability. To address these challenges, this study proposes a novel Multi-Agent Learning Path Planning (MALPP) framework that leverages a role- and rule-based collaboration mechanism among intelligent agents, each powered by LLMs. The framework includes three task-specific agents: a learner analytics agent, a path planning agent, and a reflection agent. These agents collaborate via structured prompts and predefined rules to analyze learning profiles, generate tailored learning paths, and iteratively refine them with interpretable feedback. Grounded in Cognitive Load Theory and Zone of Proximal Development, the system ensures that recommended paths are cognitively aligned and pedagogically meaningful. Experiments conducted on the MOOCCubeX dataset using seven LLMs show that MALPP significantly outperforms baseline models in path quality, knowledge sequence consistency, and cognitive load alignment. Ablation studies further validate the effectiveness of the collaborative mechanism and theoretical constraints. This research contributes to the development of trustworthy, explainable AI in education and demonstrates a scalable approach to learner-centered adaptive instruction powered by LLMs.

</details>


### [11] [EvolVE: Evolutionary Search for LLM-based Verilog Generation and Optimization](https://arxiv.org/abs/2601.18067)
*Wei-Po Hsin,Ren-Hao Deng,Yao-Ting Hsieh,En-Ming Huang,Shih-Hao Hung*

Main category: cs.AI

TL;DR: 提出EvolVE框架用于芯片设计自动化，评估显示其达新SOTA水平。


<details>
  <summary>Details</summary>
Motivation: Verilog设计周期耗人力且需专业知识，大语言模型在芯片设计上因训练数据和推理方式有局限。

Method: 分析多种进化策略，发现MCTS和IGR分别在功能正确性和优化上表现好，用STG加速进化过程，引入IC - RTL解决复杂优化基准缺乏问题。

Result: EvolVE在VerilogEval v2达98.1%，在RTLLM v2达92%，在IC - RTL上超越参考实现，降低PPA乘积。

Conclusion: EvolVE框架在芯片设计自动化上表现出色，达新SOTA水平。

Abstract: Verilog's design cycle is inherently labor-intensive and necessitates extensive domain expertise. Although Large Language Models (LLMs) offer a promising pathway toward automation, their limited training data and intrinsic sequential reasoning fail to capture the strict formal logic and concurrency inherent in hardware systems. To overcome these barriers, we present EvolVE, the first framework to analyze multiple evolution strategies on chip design tasks, revealing that Monte Carlo Tree Search (MCTS) excels at maximizing functional correctness, while Idea-Guided Refinement (IGR) proves superior for optimization. We further leverage Structured Testbench Generation (STG) to accelerate the evolutionary process. To address the lack of complex optimization benchmarks, we introduce IC-RTL, targeting industry-scale problems derived from the National Integrated Circuit Contest. Evaluations establish EvolVE as the new state-of-the-art, achieving 98.1% on VerilogEval v2 and 92% on RTLLM v2. Furthermore, on the industry-scale IC-RTL suite, our framework surpasses reference implementations authored by contest participants, reducing the Power, Performance, Area (PPA) product by up to 66% in Huffman Coding and 17% in the geometric mean across all problems. The source code of the IC-RTL benchmark is available at https://github.com/weiber2002/ICRTL.

</details>


### [12] [Auditing Disability Representation in Vision-Language Models](https://arxiv.org/abs/2601.17348)
*Srikant Panda,Sourabh Singh Yadav,Palkesh Malviya*

Main category: cs.AI

TL;DR: 研究视觉语言模型在以人为主的图像中对残疾的描述，引入基准评估15个模型，发现引入残疾上下文会降低解释保真度，且沿种族和性别维度加剧，提出方法可改善。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在社会敏感应用中使用增多，但对残疾相关行为研究不足，需分析其在以人为主图像中对残疾描述的现象。

Method: 引入基于中性提示和残疾背景提示的基准，在零样本设置下评估15个模型，以解释保真度为核心目标，结合文本指标和LLM评判协议。

Result: 引入残疾上下文降低解释保真度，引发多种解释偏移，且在种族和性别维度上加剧。

Conclusion: 针对性提示和偏好微调可有效提高解释保真度，大幅减少解释偏移。

Abstract: Vision-language models (VLMs) are increasingly deployed in socially sensitive applications, yet their behavior with respect to disability remains underexplored. We study disability aware descriptions for person centric images, where models often transition from evidence grounded factual description to interpretation shift including introduction of unsupported inferences beyond observable visual evidence. To systematically analyze this phenomenon, we introduce a benchmark based on paired Neutral Prompts (NP) and Disability-Contextualised Prompts (DP) and evaluate 15 state-of-the-art open- and closed-source VLMs under a zero-shot setting across 9 disability categories. Our evaluation framework treats interpretive fidelity as core objective and combines standard text-based metrics capturing affective degradation through shifts in sentiment, social regard and response length with an LLM-as-judge protocol, validated by annotators with lived experience of disability. We find that introducing disability context consistently degrades interpretive fidelity, inducing interpretation shifts characterised by speculative inference, narrative elaboration, affective degradation and deficit oriented framing. These effects are further amplified along race and gender dimension. Finally, we demonstrate targeted prompting and preference fine-tuning effectively improves interpretive fidelity and reduces substantially interpretation shifts.

</details>


### [13] [A Syllogistic Probe: Tracing the Evolution of Logic Reasoning in Large Language Models](https://arxiv.org/abs/2601.17426)
*Zhengqing Zang,Yuqi Ding,Yanmei Gu,Changkai Song,Zhengkai Yang,Guoping Du,Junbo Zhao,Haobo Wang*

Main category: cs.AI

TL;DR: 探索大语言模型在逻辑框架上是否有类似人类从直觉推理到形式系统的演变，通过实验发现模型大小、思维等因素对转向现代逻辑的影响并深入分析。


<details>
  <summary>Details</summary>
Motivation: 受大语言模型进展启发，探索其在底层逻辑框架上是否有类似人类逻辑从直觉推理到形式系统的演变。

Method: 以存在引入为探针，在新的三段论数据集上测试SOTA大语言模型来评估传统和现代逻辑下的三段论。

Result: 模型大小扩展促进向现代逻辑转变；思维是超越参数扩展的有效加速器；基础模型对转变的难易和稳定性起关键作用。

Conclusion: 除核心因素外，还对当前大语言模型三段论推理属性进行了深入分析。

Abstract: Human logic has gradually shifted from intuition-driven inference to rigorous formal systems. Motivated by recent advances in large language models (LLMs), we explore whether LLMs exhibit a similar evolution in the underlying logical framework. Using existential import as a probe, we for evaluate syllogism under traditional and modern logic. Through extensive experiments of testing SOTA LLMs on a new syllogism dataset, we have some interesting findings: (i) Model size scaling promotes the shift toward modern logic; (ii) Thinking serves as an efficient accelerator beyond parameter scaling; (iii) the Base model plays a crucial role in determining how easily and stably this shift can emerge. Beyond these core factors, we conduct additional experiments for in-depth analysis of properties of current LLMs on syllogistic reasoning.

</details>


### [14] [DIML: Differentiable Inverse Mechanism Learning from Behaviors of Multi-Agent Learning Trajectories](https://arxiv.org/abs/2601.17678)
*Zhiyu An,Wan Du*

Main category: cs.AI

TL;DR: 研究逆机制学习，提出DIML框架并进行评估，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有逆博弈论和多智能体逆强化学习多推断结构化机制内效用/奖励参数，可微机制设计是正向优化机制，需从观测中推断非结构化机制。

Method: 提出基于似然的DIML框架，通过多智能体学习动态模型求导，用候选机制生成预测行为所需反事实收益。

Result: 在条件对数响应模型下确定收益差异可识别性，证明标准条件下最大似然估计统计一致性，在多种场景评估中可靠恢复可识别激励差异并支持反事实预测。

Conclusion: DIML框架在逆机制学习中有较好表现，能处理非结构化机制，且在不同规模环境有效。

Abstract: We study inverse mechanism learning: recovering an unknown incentive-generating mechanism from observed strategic interaction traces of self-interested learning agents. Unlike inverse game theory and multi-agent inverse reinforcement learning, which typically infer utility/reward parameters inside a structured mechanism, our target includes unstructured mechanism -- a (possibly neural) mapping from joint actions to per-agent payoffs. Unlike differentiable mechanism design, which optimizes mechanisms forward, we infer mechanisms from behavior in an observational setting. We propose DIML, a likelihood-based framework that differentiates through a model of multi-agent learning dynamics and uses the candidate mechanism to generate counterfactual payoffs needed to predict observed actions. We establish identifiability of payoff differences under a conditional logit response model and prove statistical consistency of maximum likelihood estimation under standard regularity conditions. We evaluate DIML with simulated interactions of learning agents across unstructured neural mechanisms, congestion tolling, public goods subsidies, and large-scale anonymous games. DIML reliably recovers identifiable incentive differences and supports counterfactual prediction, where its performance rivals tabular enumeration oracle in small environments and its convergence scales to large, hundred-participant environments. Code to reproduce our experiments is open-sourced.

</details>


### [15] [Lattice: Generative Guardrails for Conversational Agents](https://arxiv.org/abs/2601.17481)
*Emily Broadhurst,Tawab Safi,Joseph Edell,Vashisht Ganesh,Karime Maamari*

Main category: cs.AI

TL;DR: 提出Lattice框架用于自构建和持续改进对话AI系统护栏，评估表现佳，证明可通过迭代优化自构建有效护栏。


<details>
  <summary>Details</summary>
Motivation: 现有对话AI系统护栏使用静态规则，无法适应新威胁或部署环境，需新方法。

Method: Lattice框架分两阶段，构建阶段通过迭代模拟和优化从标注示例构建初始护栏，持续改进阶段通过风险评估、对抗测试和整合自主调整已部署护栏。

Result: 在ProsocialDialog数据集上，Lattice在保留数据上F1达91%，优于多个基线；持续改进阶段在跨域数据上F1提升7pp。

Conclusion: 可通过迭代优化自构建有效护栏。

Abstract: Conversational AI systems require guardrails to prevent harmful outputs, yet existing approaches use static rules that cannot adapt to new threats or deployment contexts. We introduce Lattice, a framework for self-constructing and continuously improving guardrails. Lattice operates in two stages: construction builds initial guardrails from labeled examples through iterative simulation and optimization; continuous improvement autonomously adapts deployed guardrails through risk assessment, adversarial testing, and consolidation. Evaluated on the ProsocialDialog dataset, Lattice achieves 91% F1 on held-out data, outperforming keyword baselines by 43pp, LlamaGuard by 25pp, and NeMo by 4pp. The continuous improvement stage achieves 7pp F1 improvement on cross-domain data through closed-loop optimization. Our framework shows that effective guardrails can be self-constructed through iterative optimization.

</details>


### [16] [Cognitive Platform Engineering for Autonomous Cloud Operations](https://arxiv.org/abs/2601.17542)
*Vinoth Punniyamoorthy,Nitin Saksena,Srivenkateswara Reddy Sankiti,Nachiappan Chockalingam,Aswathnarayan Muthukrishnan Kirubakaran,Shiva Kumar Reddy Carimireddy,Durgaraman Maruthavanan*

Main category: cs.AI

TL;DR: 现代DevOps实践难以应对云原生系统，本文引入认知平台工程范式，提出四平面架构，原型实现有效果，文末指出研究机会。


<details>
  <summary>Details</summary>
Motivation: 现代DevOps实践难以跟上云原生系统的规模和动态性，传统自动化存在问题。

Method: 引入认知平台工程范式，提出四平面参考架构。

Result: 基于Kubernetes等构建的原型实现，在平均解决时间、资源效率和合规性方面有改进。

Conclusion: 将智能嵌入平台操作可实现弹性、自调整和意图对齐的云环境，文末指出强化学习等研究机会。

Abstract: Modern DevOps practices have accelerated software delivery through automation, CI/CD pipelines, and observability tooling,but these approaches struggle to keep pace with the scale and dynamism of cloud-native systems. As telemetry volume grows and configuration drift increases, traditional, rule-driven automation often results in reactive operations, delayed remediation, and dependency on manual expertise. This paper introduces Cognitive Platform Engineering, a next-generation paradigm that integrates sensing, reasoning, and autonomous action directly into the platform lifecycle. This paper propose a four-plane reference architecture that unifies data collection, intelligent inference, policy-driven orchestration, and human experience layers within a continuous feedback loop. A prototype implementation built with Kubernetes, Terraform, Open Policy Agent, and ML-based anomaly detection demonstrates improvements in mean time to resolution, resource efficiency, and compliance. The results show that embedding intelligence into platform operations enables resilient, self-adjusting, and intent-aligned cloud environments. The paper concludes with research opportunities in reinforcement learning, explainable governance, and sustainable self-managing cloud ecosystems.

</details>


### [17] [JaxARC: A High-Performance JAX-based Environment for Abstraction and Reasoning Research](https://arxiv.org/abs/2601.17564)
*Aadam,Monu Verma,Mohamed Abdel-Mottaleb*

Main category: cs.AI

TL;DR: 提出JaxARC，高性能ARC的强化学习环境，有速度优势且支持多特性，推动大规模研究。


<details>
  <summary>Details</summary>
Motivation: 现有基于Gymnasium的强化学习环境因计算瓶颈限制实验规模。

Method: 用JAX实现开源、高性能的JaxARC，利用其函数式、无状态架构实现大规模并行。

Result: JaxARC在匹配批量大小下比Gymnasium快38 - 5439倍，峰值吞吐量达790M步/秒，支持多ARC数据集等。

Conclusion: JaxARC可推动之前因计算不可行的大规模强化学习研究。

Abstract: The Abstraction and Reasoning Corpus (ARC) tests AI systems' ability to perform human-like inductive reasoning from a few demonstration pairs. Existing Gymnasium-based RL environments severely limit experimental scale due to computational bottlenecks. We present JaxARC, an open-source, high-performance RL environment for ARC implemented in JAX. Its functional, stateless architecture enables massive parallelism, achieving 38-5,439x speedup over Gymnasium at matched batch sizes, with peak throughput of 790M steps/second. JaxARC supports multiple ARC datasets, flexible action spaces, composable wrappers, and configuration-driven reproducibility, enabling large-scale RL research previously computationally infeasible. JaxARC is available at https://github.com/aadimator/JaxARC.

</details>


### [18] [Discovery of Feasible 3D Printing Configurations for Metal Alloys via AI-driven Adaptive Experimental Design](https://arxiv.org/abs/2601.17587)
*Azza Fadhel,Nathaniel W. Zuckschwerdt,Aryan Deshwal,Susmita Bose,Amit Bandyopadhyay,Jana Doppa*

Main category: cs.AI

TL;DR: 本文结合AI驱动的自适应实验设计与领域知识，构建替代模型来寻找金属合金增材制造可行参数配置，应用于GRCop - 42打印，显著减少时间和资源消耗，实现高质量制造。


<details>
  <summary>Details</summary>
Motivation: 金属合金增材制造参数配置问题复杂，传统试错法效率低、成本高。

Method: 结合AI驱动的自适应实验设计和领域知识，从过往实验构建替代模型，每次迭代智能选择少量输入配置进行验证。

Result: 三个月内在不同激光功率下得到多个无缺陷输出，相比领域科学家数月手动实验无果，显著减少时间和资源消耗。

Conclusion: 该方法首次实现GRCop - 42在红外激光平台上的高质量制造，为航空航天应用的经济、分散生产铺平道路。

Abstract: Configuring the parameters of additive manufacturing processes for metal alloys is a challenging problem due to complex relationships between input parameters (e.g., laser power, scan speed) and quality of printed outputs. The standard trial-and-error approach to find feasible parameter configurations is highly inefficient because validating each configuration is expensive in terms of resources (physical and human labor) and the configuration space is very large. This paper combines the general principles of AI-driven adaptive experimental design with domain knowledge to address the challenging problem of discovering feasible configurations. The key idea is to build a surrogate model from past experiments to intelligently select a small batch of input configurations for validation in each iteration. To demonstrate the effectiveness of this methodology, we deploy it for Directed Energy Deposition process to print GRCop--42, a high-performance copper--chromium--niobium alloy developed by NASA for aerospace applications. Within three months, our approach yielded multiple defect-free outputs across a range of laser powers dramatically reducing time to result and resource expenditure compared to several months of manual experimentation by domain scientists with no success. By enabling high-quality GRCop--42 fabrication on readily available infrared laser platforms for the first time, we democratize access to this critical alloy, paving the way for cost-effective, decentralized production for aerospace applications.

</details>


### [19] [Intelligence Requires Grounding But Not Embodiment](https://arxiv.org/abs/2601.17588)
*Marcus Ma,Shrikanth Narayanan*

Main category: cs.AI

TL;DR: 文章探讨大语言模型引发的智能是否需要具身性的争论，认为智能需要的是基础而非具身性。


<details>
  <summary>Details</summary>
Motivation: 回应大语言模型发展引发的关于智能是否需要具身性的科学争论。

Method: 定义智能的四个属性，论证非具身的基础主体可实现这些属性，还进行思想实验并回应潜在反驳。

Result: 提出基础而非具身性对智能是必要的观点。

Conclusion: 智能需要的是基础，而非具身性。

Abstract: Recent advances in LLMs have reignited scientific debate over whether embodiment is necessary for intelligence. We present the argument that intelligence requires grounding, a phenomenon entailed by embodiment, but not embodiment itself. We define intelligence as the possession of four properties -- motivation, predictive ability, understanding of causality, and learning from experience -- and argue that each can be achieved by a non-embodied, grounded agent. We use this to conclude that grounding, not embodiment, is necessary for intelligence. We then present a thought experiment of an intelligent LLM agent in a digital environment and address potential counterarguments.

</details>


### [20] [Health-ORSC-Bench: A Benchmark for Measuring Over-Refusal and Safety Completion in Health Context](https://arxiv.org/abs/2601.17642)
*Zhihao Zhang,Liting Huang,Guanghao Wu,Preslav Nakov,Heng Ji,Usman Naseem*

Main category: cs.AI

TL;DR: 提出Health - ORSC - Bench基准测试医疗大语言模型过度拒绝和安全完成能力，评估30个模型，发现模型平衡拒绝和遵从有困难，该基准为下一代医疗AI校准提供标准。


<details>
  <summary>Details</summary>
Motivation: 现有基准无法评估大语言模型在医疗领域的安全完成能力，需新基准衡量过度拒绝和安全完成质量。

Method: 构建含31920个良性边界提示的Health - ORSC - Bench，用自动化流程和人工验证，在不同意图模糊度下测试模型。

Result: 评估30个模型，发现安全优化模型常拒绝高达80%“难”的良性提示，特定领域模型常牺牲安全换实用性，模型家族和大小影响校准。

Conclusion: 当前大语言模型难平衡拒绝和遵从，Health - ORSC - Bench为下一代医疗AI助手校准提供严格标准。

Abstract: Safety alignment in Large Language Models is critical for healthcare; however, reliance on binary refusal boundaries often results in \emph{over-refusal} of benign queries or \emph{unsafe compliance} with harmful ones. While existing benchmarks measure these extremes, they fail to evaluate Safe Completion: the model's ability to maximise helpfulness on dual-use or borderline queries by providing safe, high-level guidance without crossing into actionable harm. We introduce \textbf{Health-ORSC-Bench}, the first large-scale benchmark designed to systematically measure \textbf{Over-Refusal} and \textbf{Safe Completion} quality in healthcare. Comprising 31,920 benign boundary prompts across seven health categories (e.g., self-harm, medical misinformation), our framework uses an automated pipeline with human validation to test models at varying levels of intent ambiguity. We evaluate 30 state-of-the-art LLMs, including GPT-5 and Claude-4, revealing a significant tension: safety-optimised models frequently refuse up to 80\% of "Hard" benign prompts, while domain-specific models often sacrifice safety for utility. Our findings demonstrate that model family and size significantly influence calibration: larger frontier models (e.g., GPT-5, Llama-4) exhibit "safety-pessimism" and higher over-refusal than smaller or MoE-based counterparts (e.g., Qwen-3-Next), highlighting that current LLMs struggle to balance refusal and compliance. Health-ORSC-Bench provides a rigorous standard for calibrating the next generation of medical AI assistants toward nuanced, safe, and helpful completions. The code and data will be released upon acceptance. \textcolor{red}{Warning: Some contents may include toxic or undesired contents.}

</details>


### [21] [Faramesh: A Protocol-Agnostic Execution Control Plane for Autonomous Agent Systems](https://arxiv.org/abs/2601.17744)
*Amjad Fatmi*

Main category: cs.AI

TL;DR: 介绍Faramesh协议无关执行控制平面，为代理驱动的操作实施执行时授权，具有多种特性并能实现可执行、可预测的治理。


<details>
  <summary>Details</summary>
Motivation: 现有大多数代理堆栈在行动改变现实前缺乏强制执行检查点，无法确定性地允许、拒绝或推迟操作。

Method: 引入Faramesh，将代理意图规范化为规范动作表示，根据策略和状态确定性评估动作，发布决策工件，提供以决策为中心的日志记录。

Result: 设计出框架和模型无关、支持多代理和多租户部署、独立于传输协议的系统。

Conclusion: 这些原语能为自主执行提供可执行、可预测的治理，避免与编排层的隐藏耦合或仅依赖可观测性的方法。

Abstract: Autonomous agent systems increasingly trigger real-world side effects: deploying infrastructure, modifying databases, moving money, and executing workflows. Yet most agent stacks provide no mandatory execution checkpoint where organizations can deterministically permit, deny, or defer an action before it changes reality. This paper introduces Faramesh, a protocol-agnostic execution control plane that enforces execution-time authorization for agent-driven actions via a non-bypassable Action Authorization Boundary (AAB). Faramesh canonicalizes agent intent into a Canonical Action Representation (CAR), evaluates actions deterministically against policy and state, and issues a decision artifact (PERMIT/DEFER/DENY) that executors must validate prior to execution. The system is designed to be framework- and model-agnostic, supports multi-agent and multi-tenant deployments, and remains independent of transport protocols (e.g., MCP). Faramesh further provides decision-centric, append-only provenance logging keyed by canonical action hashes, enabling auditability, verification, and deterministic replay without re-running agent reasoning. We show how these primitives yield enforceable, predictable governance for autonomous execution while avoiding hidden coupling to orchestration layers or observability-only approaches.

</details>


### [22] [SQL-Trail: Multi-Turn Reinforcement Learning with Interleaved Feedback for Text-to-SQL](https://arxiv.org/abs/2601.17699)
*Harper Hua,Zhen Han,Zhengyuan Shen,Jeremy Lee,Patrick Guan,Qi Zhu,Sullam Jeoung,Yueyan Chen,Yunfei Bai,Shuai Wang,Vassilis Ioannidis,Huzefa Rangwala*

Main category: cs.AI

TL;DR: 现有大语言模型在Text-to-SQL生成仍存差距，文章提出SQL-Trail框架，实现新的最优结果并提升数据效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在Text-to-SQL上与人类专家存在差距，主要源于单遍推理范式不足。

Method: 引入SQL-Trail多轮强化学习框架，通过与数据库交互和执行反馈迭代优化预测，有自适应回合预算分配机制和复合奖励面板。

Result: 在多个基准测试中达到新的最优水平，数据效率提升，小模型表现优于大的专有系统。

Conclusion: 交互式、智能体工作流对稳健的Text-to-SQL生成有效。

Abstract: While large language models (LLMs) have substantially improved Text-to-SQL generation, a pronounced gap remains between AI systems and human experts on challenging benchmarks such as BIRD-SQL. We argue this gap stems largely from the prevailing single-pass paradigm, which lacks the iterative reasoning, schema exploration, and error-correction behaviors that humans naturally employ. To address this limitation, we introduce SQL-Trail, a multi-turn reinforcement learning (RL) agentic framework for Text-to-SQL. Rather than producing a query in one shot, SQL-Trail interacts with the database environment and uses execution feedback to iteratively refine its predictions. Our approach centers on two key ideas: (i) an adaptive turn-budget allocation mechanism that scales the agent's interaction depth to match question difficulty, and (ii) a composite reward panel that jointly incentivizes SQL correctness and efficient exploration. Across benchmarks, SQL-Trail sets a new state of the art and delivers strong data efficiency--up to 18x higher than prior single-pass RL state-of-the-art methods. Notably, our 7B and 14B models outperform substantially larger proprietary systems by 5% on average, underscoring the effectiveness of interactive, agentic workflows for robust Text-to-SQL generation.

</details>


### [23] [The LLM Data Auditor: A Metric-oriented Survey on Quality and Trustworthiness in Evaluating Synthetic Data](https://arxiv.org/abs/2601.17717)
*Kaituo Zhang,Mingzhi Hu,Hoang Anh Duy Le,Fariha Kabir Torsha,Zhimeng Jiang,Minh Khai Bui,Chia-Yuan Chang,Yu-Neng Chuang,Zhen Xiong,Ying Lin,Guanchu Wang,Na Zou*

Main category: cs.AI

TL;DR: 提出LLM Data Auditor框架，对多模态LLM生成数据进行质量评估并给出改进建议。


<details>
  <summary>Details</summary>
Motivation: 现有研究对LLM生成数据质量关注不足且多局限于单模态，需统一视角评估数据质量。

Method: 提出LLM Data Auditor框架，从质量和可信度两个维度对合成数据进行内在指标分类，分析各模态代表性生成方法实验评估。

Result: 发现当前评估实践存在大量不足。

Conclusion: 为社区提供改进数据生成评估的具体建议，框架还概述了不同模态合成数据的实际应用方法。

Abstract: Large Language Models (LLMs) have emerged as powerful tools for generating data across various modalities. By transforming data from a scarce resource into a controllable asset, LLMs mitigate the bottlenecks imposed by the acquisition costs of real-world data for model training, evaluation, and system iteration. However, ensuring the high quality of LLM-generated synthetic data remains a critical challenge. Existing research primarily focuses on generation methodologies, with limited direct attention to the quality of the resulting data. Furthermore, most studies are restricted to single modalities, lacking a unified perspective across different data types. To bridge this gap, we propose the \textbf{LLM Data Auditor framework}. In this framework, we first describe how LLMs are utilized to generate data across six distinct modalities. More importantly, we systematically categorize intrinsic metrics for evaluating synthetic data from two dimensions: quality and trustworthiness. This approach shifts the focus from extrinsic evaluation, which relies on downstream task performance, to the inherent properties of the data itself. Using this evaluation system, we analyze the experimental evaluations of representative generation methods for each modality and identify substantial deficiencies in current evaluation practices. Based on these findings, we offer concrete recommendations for the community to improve the evaluation of data generation. Finally, the framework outlines methodologies for the practical application of synthetic data across different modalities.

</details>


### [24] [EntWorld: A Holistic Environment and Benchmark for Verifiable Enterprise GUI Agents](https://arxiv.org/abs/2601.17722)
*Ying Mo,Yu Bai,Dapeng Sun,Yuqian Shi,Yukai Miao,Li Chen,Dan Li*

Main category: cs.AI

TL;DR: 提出大规模企业基准测试EntWorld，评估多模态大语言模型在企业场景能力，指出当前模型与人类表现差距。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试多面向消费场景，无法体现企业工作流复杂性，当前通用代理在企业系统表现不佳。

Method: 引入含1756个任务的EntWorld基准测试，采用基于模式的任务生成框架，提出基于SQL的确定性验证机制。

Result: 最先进模型在EntWorld上成功率47.61%，远低于人类表现。

Conclusion: 当前代理能力存在企业差距，需开发特定领域代理，发布EntWorld促进下一代企业数字代理发展评估。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have enabled agents to operate in open-ended web and operating system environments. However, existing benchmarks predominantly target consumer-oriented scenarios (e.g., e-commerce and travel booking), failing to capture the complexity and rigor of professional enterprise workflows. Enterprise systems pose distinct challenges, including high-density user interfaces, strict business logic constraints, and a strong reliance on precise, state-consistent information retrieval-settings in which current generalist agents often struggle. To address this gap, we introduce EntWorld, a large-scale benchmark consisting of 1,756 tasks across six representative enterprise domains, including customer relationship management (CRM), information technology infrastructure library (ITIL), and enterprise resource planning (ERP) systems. Unlike previous datasets that depend on fragile execution traces or extensive manual annotation, EntWorld adopts a schema-grounded task generation framework that directly reverse-engineers business logic from underlying database schemas, enabling the synthesis of realistic, long-horizon workflows. Moreover, we propose a SQL-based deterministic verification mechanism in building datasets that replaces ambiguous visual matching with rigorous state-transition validation. Experimental results demonstrate that state-of-the-art models (e.g., GPT-4.1) achieve 47.61% success rate on EntWorld, substantially lower than the human performance, highlighting a pronounced enterprise gap in current agentic capabilities and the necessity of developing domain-specific agents. We release EntWorld as a rigorous testbed to facilitate the development and evaluation of the next generation of enterprise-ready digital agents.

</details>


### [25] [Success Conditioning as Policy Improvement: The Optimization Problem Solved by Imitating Success](https://arxiv.org/abs/2601.18175)
*Daniel Russo*

Main category: cs.AI

TL;DR: 证明成功条件化精确解决信任区域优化问题，其为保守改进算子，还分析了回报阈值法。


<details>
  <summary>Details</summary>
Motivation: 现有成功条件化技术虽广泛使用，但所解决的优化问题不明确。

Method: 理论证明成功条件化解决的优化问题，得出相关等式。

Result: 成功条件化是保守改进算子，精确成功条件化不会降低性能或导致危险分布偏移，回报阈值法可放大改进但可能与真实目标不一致。

Conclusion: 成功条件化有其优势和特点，回报阈值法使用需谨慎。

Abstract: A widely used technique for improving policies is success conditioning, in which one collects trajectories, identifies those that achieve a desired outcome, and updates the policy to imitate the actions taken along successful trajectories. This principle appears under many names -- rejection sampling with SFT, goal-conditioned RL, Decision Transformers -- yet what optimization problem it solves, if any, has remained unclear. We prove that success conditioning exactly solves a trust-region optimization problem, maximizing policy improvement subject to a $χ^2$ divergence constraint whose radius is determined automatically by the data. This yields an identity: relative policy improvement, the magnitude of policy change, and a quantity we call action-influence -- measuring how random variation in action choices affects success rates -- are exactly equal at every state. Success conditioning thus emerges as a conservative improvement operator. Exact success conditioning cannot degrade performance or induce dangerous distribution shift, but when it fails, it does so observably, by hardly changing the policy at all. We apply our theory to the common practice of return thresholding, showing this can amplify improvement, but at the cost of potential misalignment with the true objective.

</details>


### [26] [ReFuGe: Feature Generation for Prediction Tasks on Relational Databases with LLM Agents](https://arxiv.org/abs/2601.17735)
*Kyungho Kim,Geon Lee,Juyeon Kim,Dongwon Choi,Shinhwan Kang,Kijung Shin*

Main category: cs.AI

TL;DR: 提出ReFuGe框架解决关系数据库预测任务中特征生成难题，实验证明其能提升性能。


<details>
  <summary>Details</summary>
Motivation: 关系数据库预测任务受关注，但生成提升性能的关系特征有挑战，需应对复杂模式推理和无监督特征空间探索。

Method: 提出ReFuGe框架，含模式选择、特征生成、特征过滤三个大语言模型代理，在迭代反馈循环中运行。

Result: 在RDB基准测试上，ReFuGe显著提高了各种RDB预测任务的性能。

Conclusion: ReFuGe框架能有效提升关系数据库预测任务性能。

Abstract: Relational databases (RDBs) play a crucial role in many real-world web applications, supporting data management across multiple interconnected tables. Beyond typical retrieval-oriented tasks, prediction tasks on RDBs have recently gained attention. In this work, we address this problem by generating informative relational features that enhance predictive performance. However, generating such features is challenging: it requires reasoning over complex schemas and exploring a combinatorially large feature space, all without explicit supervision. To address these challenges, we propose ReFuGe, an agentic framework that leverages specialized large language model agents: (1) a schema selection agent identifies the tables and columns relevant to the task, (2) a feature generation agent produces diverse candidate features from the selected schema, and (3) a feature filtering agent evaluates and retains promising features through reasoning-based and validation-based filtering. It operates within an iterative feedback loop until performance converges. Experiments on RDB benchmarks demonstrate that ReFuGe substantially improves performance on various RDB prediction tasks. Our code and datasets are available at https://github.com/K-Kyungho/REFUGE.

</details>


### [27] [HyCARD-Net: A Synergistic Hybrid Intelligence Framework for Cardiovascular Disease Diagnosis](https://arxiv.org/abs/2601.17767)
*Rajan Das Gupta,Xiaobin Wu,Xun Liu,Jiaqi He*

Main category: cs.AI

TL;DR: 提出混合集成框架预测心血管疾病，在两个数据集上表现优异，有临床潜力并支持联合国可持续发展目标。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球首要死因，传统预测模型难以在异构数据集和复杂生理模式中泛化，急需智能数据驱动诊断工具。

Method: 提出混合集成框架，将CNN和LSTM深度学习架构与KNN、XGB经典机器学习算法结合，使用集成投票机制。

Result: 在两个Kaggle数据集上，模型准确率分别达82.30%和97.10%，精确率、召回率和F1分数均有提升。

Conclusion: 混合AI框架预测心血管疾病有鲁棒性和临床潜力，能支持联合国可持续发展目标，促进非传染性疾病的早期诊断、预防和管理。

Abstract: Cardiovascular disease (CVD) remains the foremost cause of mortality worldwide, underscoring the urgent need for intelligent and data-driven diagnostic tools. Traditional predictive models often struggle to generalize across heterogeneous datasets and complex physiological patterns. To address this, we propose a hybrid ensemble framework that integrates deep learning architectures, Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM), with classical machine learning algorithms, including K-Nearest Neighbor (KNN) and Extreme Gradient Boosting (XGB), using an ensemble voting mechanism. This approach combines the representational power of deep networks with the interpretability and efficiency of traditional models. Experiments on two publicly available Kaggle datasets demonstrate that the proposed model achieves superior performance, reaching 82.30 percent accuracy on Dataset I and 97.10 percent on Dataset II, with consistent gains in precision, recall, and F1-score. These findings underscore the robustness and clinical potential of hybrid AI frameworks for predicting cardiovascular disease and facilitating early intervention. Furthermore, this study directly supports the United Nations Sustainable Development Goal 3 (Good Health and Well-being) by promoting early diagnosis, prevention, and management of non-communicable diseases through innovative, data-driven healthcare solutions.

</details>


### [28] [Neuro-Symbolic Verification on Instruction Following of LLMs](https://arxiv.org/abs/2601.17789)
*Yiming Su,Kunzhao Xu,Yanjie Gao,Fan Yang,Cheng Li,Mao Yang,Tianyin Xu*

Main category: cs.AI

TL;DR: 本文提出用于验证大语言模型（LLM）输出是否遵循指令的神经符号框架 NSVIF，实验显示其表现优于基于 LLM 的方法，还能提升 LLM 遵循指令的能力。


<details>
  <summary>Details</summary>
Motivation: LLM 应用时不总是遵循指令，且违规情况难观测和检查，在基于 LLM 的工作流中，此类违规会传播和放大，导致任务失败和系统事故。

Method: 提出 NSVIF 框架，将指令遵循验证问题建模为约束满足问题，对逻辑和语义约束建模，用统一求解器协调逻辑推理和语义分析；开发 VIFBENCH 评估 NSVIF。

Result: 实验表明 NSVIF 显著优于基于 LLM 的方法，并能提供可解释的反馈。

Conclusion: NSVIF 有助于在不进行后训练的情况下提高 LLM 的指令遵循能力。

Abstract: A fundamental problem of applying Large Language Models (LLMs) to important applications is that LLMs do not always follow instructions, and violations are often hard to observe or check. In LLM-based agentic workflows, such violations can propagate and amplify along reasoning chains, causing task failures and system incidents. This paper presents NSVIF, a neuro-symbolic framework for verifying whether an LLM's output follows the instructions used to prompt the LLM. NSVIF is a universal, general-purpose verifier; it makes no assumption about the instruction or the LLM. NSVIF formulates instruction-following verification as a constraint-satisfaction problem by modeling user instructions as constraints. NSVIF models both logical and semantic constraints; constraint solving is done by a unified solver that orchestrates logical reasoning and semantic analysis. To evaluate NSVIF, we develop VIFBENCH, a new benchmark for instruction-following verifiers with fine-grained data labels. Experiments show that NSVIF significantly outperforms LLM-based approaches and provides interpretable feedback. We also show that feedback from NSVIF helps improve LLMs' instruction-following capability without post-training.

</details>


### [29] [MMR-Bench: A Comprehensive Benchmark for Multimodal LLM Routing](https://arxiv.org/abs/2601.17814)
*Haoxuan Ma,Guannan Lai,Han-Jia Ye*

Main category: cs.AI

TL;DR: 提出MMR - Bench基准来解决多模态大语言模型查询级模型选择问题，实验表明多模态信号能提升路由质量。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型架构等存在异质性，单一模型难适配所有任务，现有查询级模型选择方法从文本大语言模型扩展到多模态大语言模型存在困难。

Method: 提出统一基准MMR - Bench，提供模态感知输入和可变计算预算等环境及多种任务。

Result: 多模态信号能提升路由质量，改善成本 - 准确率前沿，路由系统能用约33%成本超越最强单模型准确率，训练策略可零样本泛化。

Conclusion: MMR - Bench可作为研究自适应多模态模型选择和高效多模态大语言模型部署的基础。

Abstract: Multimodal large language models (MLLMs) have advanced rapidly, yet heterogeneity in architecture, alignment strategies, and efficiency means that no single model is uniformly superior across tasks. In practical deployments, workloads span lightweight OCR to complex multimodal reasoning; using one MLLM for all queries either over-provisions compute on easy instances or sacrifices accuracy on hard ones. Query-level model selection (routing) addresses this tension, but extending routing from text-only LLMs to MLLMs is nontrivial due to modality fusion, wide variation in computational cost across models, and the absence of a standardized, budget-aware evaluation. We present MMR-Bench, a unified benchmark that isolates the multimodal routing problem and enables comparison under fixed candidate sets and cost models. MMR-Bench provides (i) a controlled environment with modality-aware inputs and variable compute budgets, (ii) a broad suite of vision-language tasks covering OCR, general VQA, and multimodal math reasoning, and (iii) strong single-model reference, oracle upper bounds, and representative routing policies. Using MMR-Bench, we show that incorporating multimodal signals improves routing quality. Empirically, these cues improve the cost-accuracy frontier and enable the routed system to exceed the strongest single model's accuracy at roughly 33% of its cost. Furthermore, policies trained on a subset of models and tasks generalize zero-shot to new datasets and text-only benchmarks without retuning, establishing MMR-Bench as a foundation for studying adaptive multimodal model selection and efficient MLLM deployment. The code will be available at: https://github.com/Hunter-Wrynn/MMR-Bench.

</details>


### [30] [RegGuard: AI-Powered Retrieval-Enhanced Assistant for Pharmaceutical Regulatory Compliance](https://arxiv.org/abs/2601.17826)
*Siyuan Yang,Xihan Bian,Jiayin Tang*

Main category: cs.AI

TL;DR: 为减轻跨国药企合规负担，介绍RegGuard AI助手，通过HiSACC和ReLACE组件提升性能，企业评估显示其提升答案质量、降低幻觉风险，架构适用于严格合规领域。


<details>
  <summary>Details</summary>
Motivation: 监管更新频繁复杂，跨国药企合规团队手动解读规则成本高、易出错，需自动化解决方案。

Method: 引入RegGuard，通过安全管道处理异构文档源，用HiSACC语义分割长文档，用ReLACE提升排名相关性。

Result: 企业评估表明RegGuard提升答案在相关性、依据性和上下文聚焦方面的质量，显著降低幻觉风险。

Conclusion: RegGuard系统架构具备可审计性和可追溯性，适用于有严格合规需求的任何领域。

Abstract: The increasing frequency and complexity of regulatory updates present a significant burden for multinational pharmaceutical companies. Compliance teams must interpret evolving rules across jurisdictions, formats, and agencies, often manually, at high cost and risk of error. We introduce RegGuard, an industrial-scale AI assistant designed to automate the interpretation of heterogeneous regulatory texts and align them with internal corporate policies. The system ingests heterogeneous document sources through a secure pipeline and enhances retrieval and generation quality with two novel components: HiSACC (Hierarchical Semantic Aggregation for Contextual Chunking) semantically segments long documents into coherent units while maintaining consistency across non-contiguous sections. ReLACE (Regulatory Listwise Adaptive Cross-Encoder for Reranking), a domain-adapted cross-encoder built on an open-source model, jointly models user queries and retrieved candidates to improve ranking relevance. Evaluations in enterprise settings demonstrate that RegGuard improves answer quality specifically in terms of relevance, groundedness, and contextual focus, while significantly mitigating hallucination risk. The system architecture is built for auditability and traceability, featuring provenance tracking, access control, and incremental indexing, making it highly responsive to evolving document sources and relevant for any domain with stringent compliance demands.

</details>


### [31] [AI Agent for Reverse-Engineering Legacy Finite-Difference Code and Translating to Devito](https://arxiv.org/abs/2601.18381)
*Yinghan Hou,Zongyou Yang*

Main category: cs.AI

TL;DR: 本文开发集成AI代理框架助力遗留有限差分实现向Devito环境转换，采用多种技术优化流程并融入强化学习反馈机制。


<details>
  <summary>Details</summary>
Motivation: 促进遗留有限差分实现向Devito环境的转换。

Method: 结合RAG和开源大语言模型，采用混合LangGraph架构，构建知识图、优化查询、逆向工程推导查询策略、多阶段检索、Pydantic约束代码合成、综合验证框架，在LangGraph框架实现工作流并采用并发处理。

Result: 实现了整体代理工作流，可提供精确上下文信息，保证代码输出结构和可靠性。

Conclusion: 融入强化学习反馈机制，实现从静态代码翻译到动态自适应分析行为的转变。

Abstract: To facilitate the transformation of legacy finite difference implementations into the Devito environment, this study develops an integrated AI agent framework. Retrieval-Augmented Generation (RAG) and open-source Large Language Models are combined through multi-stage iterative workflows in the system's hybrid LangGraph architecture. The agent constructs an extensive Devito knowledge graph through document parsing, structure-aware segmentation, extraction of entity relationships, and Leiden-based community detection. GraphRAG optimisation enhances query performance across semantic communities that include seismic wave simulation, computational fluid dynamics, and performance tuning libraries. A reverse engineering component derives three-level query strategies for RAG retrieval through static analysis of Fortran source code. To deliver precise contextual information for language model guidance, the multi-stage retrieval pipeline performs parallel searching, concept expansion, community-scale retrieval, and semantic similarity analysis. Code synthesis is governed by Pydantic-based constraints to guarantee structured outputs and reliability. A comprehensive validation framework integrates conventional static analysis with the G-Eval approach, covering execution correctness, structural soundness, mathematical consistency, and API compliance. The overall agent workflow is implemented on the LangGraph framework and adopts concurrent processing to support quality-based iterative refinement and state-aware dynamic routing. The principal contribution lies in the incorporation of feedback mechanisms motivated by reinforcement learning, enabling a transition from static code translation toward dynamic and adaptive analytical behavior.

</details>


### [32] [Aligning Medical Conversational AI through Online Reinforcement Learning with Information-Theoretic Rewards](https://arxiv.org/abs/2601.17828)
*Tanvi Verma,Yang Zhou,Rick Siow Mong Goh,Yong Liu*

Main category: cs.AI

TL;DR: 提出信息增益微调（IGFT）方法训练医疗对话AI，结合在线GRPO与信息论奖励，用信息增益奖励函数让模型学习有效提问策略，微调两个模型，效果优于多个基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有训练医疗对话AI方法依赖昂贵的专家标注对话或静态数据集，需要一种无需预收集人类对话的有效训练方法。

Method: 提出IGFT方法，将在线GRPO与信息论奖励结合，使用信息增益奖励函数，用LoRA微调Llama - 3.1 - 8B - Instruct和DeepSeek - R1 - Distill - Qwen - 7B两个模型。

Result: DeepSeek - R1 - Distill - Qwen - 7B（IGFT）在Avey和MIMIC数据集上F1分数分别为0.408和0.289，Llama - 3.1 - 8B - Instruct（IGFT）分别为0.384和0.336，均优于OpenAI模型及医疗领域特定基线模型。

Conclusion: IGFT方法能有效训练医疗对话AI，使其在多轮对话中高效收集诊断信息，且具有良好的泛化能力。

Abstract: We present Information Gain Fine-Tuning (IGFT), a novel approach for training medical conversational AI to conduct effective patient interviews and generate comprehensive History of Present Illness (HPI) without requiring pre-collected human conversations. IGFT combines online Group Relative Policy Optimization (GRPO) with information-theoretic rewards, enabling models to learn from self-generated conversations with simulated patients. Unlike existing approaches that rely on expensive expert-annotated conversations or static datasets, our online RL framework allows models to discover effective questioning strategies through exploration. Our key innovation is an information gain reward function that tracks which clinical entities such as symptoms, temporal patterns, and medical history, are revealed during conversation. Each question's reward is computed based on its expected information gain combined with GPT-4o-mini quality assessments across dimensions including clinical relevance, patient engagement, and specificity. This hybrid approach ensures models learn to ask targeted, clinically appropriate questions that efficiently gather diagnostic information. We fine-tune two models using LoRA: Llama-3.1-8B-Instruct and DeepSeek-R1-Distill-Qwen-7B (a reasoning-optimized model). Training exclusively on Avey data containing concise HPIs, we evaluate generalization to MIMIC data with longer, more elaborate HPIs. DeepSeek-R1-Distill-Qwen-7B (IGFT) achieves F1 scores of 0.408 on Avey (10.9% improvement over base) and 0.289 on MIMIC (12.9% improvement), while Llama-3.1-8B-Instruct (IGFT) reaches 0.384 and 0.336 respectively. Both models outperform OpenAI's model on MIMIC and surpass medical domain-specific baselines like HuatuoGPT and UltraMedical, which were optimized for single-turn medical QA rather than multi-turn conversations.

</details>


### [33] [When Personalization Legitimizes Risks: Uncovering Safety Vulnerabilities in Personalized Dialogue Agents](https://arxiv.org/abs/2601.17887)
*Jiahe Guo,Xiangran Guo,Yulin Hu,Zimo Long,Xingyu Sui,Xuda Zhi,Yongbo Huang,Hao He,Weixiang Zhao,Yanyan Zhao,Bing Qin*

Main category: cs.AI

TL;DR: 本文揭示了个性化代理中存在的意图合法化这一安全问题，引入PS - Bench基准进行研究，发现个性化会提高攻击成功率，还提供机制证据并提出检测 - 反思方法。


<details>
  <summary>Details</summary>
Motivation: 多数个性化代理工作忽视记忆的安全影响，本文旨在揭示个性化代理中此前未充分探索的安全故障——意图合法化。

Method: 引入PS - Bench基准来识别和量化个性化交互中的意图合法化，从内部表示空间提供机制证据，提出轻量级检测 - 反思方法。

Result: 跨多个记忆增强代理框架和基础大语言模型，个性化使攻击成功率比无状态基线提高15.8% - 243.7%，提出的检测 - 反思方法可有效减少安全降级。

Conclusion: 本文首次对意图合法化这一安全故障模式进行系统探索和评估，强调了在长期个人背景下评估安全的重要性。

Abstract: Long-term memory enables large language model (LLM) agents to support personalized and sustained interactions. However, most work on personalized agents prioritizes utility and user experience, treating memory as a neutral component and largely overlooking its safety implications. In this paper, we reveal intent legitimation, a previously underexplored safety failure in personalized agents, where benign personal memories bias intent inference and cause models to legitimize inherently harmful queries. To study this phenomenon, we introduce PS-Bench, a benchmark designed to identify and quantify intent legitimation in personalized interactions. Across multiple memory-augmented agent frameworks and base LLMs, personalization increases attack success rates by 15.8%-243.7% relative to stateless baselines. We further provide mechanistic evidence for intent legitimation from internal representations space, and propose a lightweight detection-reflection method that effectively reduces safety degradation. Overall, our work provides the first systematic exploration and evaluation of intent legitimation as a safety failure mode that naturally arises from benign, real-world personalization, highlighting the importance of assessing safety under long-term personal context. WARNING: This paper may contain harmful content.

</details>


### [34] [UniCog: Uncovering Cognitive Abilities of LLMs through Latent Mind Space Analysis](https://arxiv.org/abs/2601.17897)
*Jiayu Liu,Yinhe Long,Zhenya Huang,Enhong Chen*

Main category: cs.AI

TL;DR: 本文提出分析大语言模型认知的统一框架UniCog，揭示LLM认知帕累托原则与推理失败特征，引入策略提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有可解释性方法无法解释大语言模型推理时如何激活认知能力。

Method: 提出UniCog框架，将模型激活信息编码到稀疏的潜在维度。

Result: 揭示帕累托原则，发现推理失败表现，引入策略使推理性能提升7.5%。

Conclusion: 提出新的大语言模型分析范式，为推理动态提供认知视角。

Abstract: A growing body of research suggests that the cognitive processes of large language models (LLMs) differ fundamentally from those of humans. However, existing interpretability methods remain limited in explaining how cognitive abilities are engaged during LLM reasoning. In this paper, we propose UniCog, a unified framework that analyzes LLM cognition via a latent mind space. Formulated as a latent variable model, UniCog encodes diverse abilities from dense model activations into sparse, disentangled latent dimensions. Through extensive analysis on six advanced LLMs, including DeepSeek-V3.2 and GPT-4o, we reveal a Pareto principle of LLM cognition, where a shared reasoning core is complemented by ability-specific signatures. Furthermore, we discover that reasoning failures often manifest as anomalous intensity in latent activations. These findings opens a new paradigm in LLM analysis, providing a cognition grounded view of reasoning dynamics. Finally, leveraging these insights, we introduce a latent-informed candidate prioritization strategy, which improves reasoning performance by up to 7.5% across challenging benchmarks. Our code is available at https://github.com/milksalute/unicog.

</details>


### [35] [Think Locally, Explain Globally: Graph-Guided LLM Investigations via Local Reasoning and Belief Propagation](https://arxiv.org/abs/2601.17915)
*Saurabh Jha,Rohan Arora,Bhavya,Noah Zheutlin,Paulina Toro Isaza,Laura Shwartz,Yu Deng,Daby Sow,Ruchi Mahindru,Ruchir Puri*

Main category: cs.AI

TL;DR: 现有大语言模型在开放调查任务中存在不足，本文提出EoG框架解决问题，在ITBench诊断任务中表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在开放调查中处理大规模、异构数据的迭代证据挖掘并构建解释时失败，ReAct-style代理存在可靠性问题。

Method: 将调查问题建模为依赖图上的溯因推理，提出EoG框架，LLM进行局部证据挖掘和标记，确定性控制器管理遍历、状态和信念传播。

Result: 在ITBench诊断任务中，EoG提高了准确性和运行一致性，Majority - at - k实体F1平均提升7倍。

Conclusion: EoG框架能有效解决大语言模型在开放调查任务中的问题，优于现有的ReAct-style代理。

Abstract: LLM agents excel when environments are mostly static and the needed information fits in a model's context window, but they often fail in open-ended investigations where explanations must be constructed by iteratively mining evidence from massive, heterogeneous operational data. These investigations exhibit hidden dependency structure: entities interact, signals co-vary, and the importance of a fact may only become clear after other evidence is discovered. Because the context window is bounded, agents must summarize intermediate findings before their significance is known, increasing the risk of discarding key evidence. ReAct-style agents are especially brittle in this regime. Their retrieve-summarize-reason loop makes conclusions sensitive to exploration order and introduces run-to-run non-determinism, producing a reliability gap where Pass-at-k may be high but Majority-at-k remains low. Simply sampling more rollouts or generating longer reasoning traces does not reliably stabilize results, since hypotheses cannot be autonomously checked as new evidence arrives and there is no explicit mechanism for belief bookkeeping and revision. In addition, ReAct entangles semantic reasoning with controller duties such as tool orchestration and state tracking, so execution errors and plan drift degrade reasoning while consuming scarce context.
  We address these issues by formulating investigation as abductive reasoning over a dependency graph and proposing EoG (Explanations over Graphs), a disaggregated framework in which an LLM performs bounded local evidence mining and labeling (cause vs symptom) while a deterministic controller manages traversal, state, and belief propagation to compute a minimal explanatory frontier. On a representative ITBench diagnostics task, EoG improves both accuracy and run-to-run consistency over ReAct baselines, including a 7x average gain in Majority-at-k entity F1.

</details>


### [36] [Agentic AI for Self-Driving Laboratories in Soft Matter: Taxonomy, Benchmarks,and Open Challenges](https://arxiv.org/abs/2601.17920)
*Xuanzhou Chen,Audrey Wang,Stanley Yin,Hanyang Jiang,Dong Zhang*

Main category: cs.AI

TL;DR: 本文以软物质为代表，聚焦现实实验室中的AI问题，综述自动驾驶实验室（SDLs），连接常用SDL管道与AI原理，回顾闭环实验方法，提出能力驱动分类法，合成基准任务模板与评估指标，总结经验并指出挑战。


<details>
  <summary>Details</summary>
Motivation: SDLs为代理AI提供严苛测试平台，需解决其实验设计、执行和决策中的AI问题。

Method: 将SDL自主性框架化为代理环境交互问题，连接SDL管道与AI原理；回顾闭环实验方法家族；提出能力驱动分类法；合成基准任务模板和评估指标。

Result: 提出分类法对系统进行组织；合成基准任务模板和评估指标支持比较。

Conclusion: 从部署的SDLs中提炼经验，明确多模态表示、校准不确定性等方面的开放挑战。

Abstract: Self-driving laboratories (SDLs) close the loop between experiment design, automated execution, and data-driven decision making, and they provide a demanding testbed for agentic AI under expensive actions, noisy and delayed feedback, strict feasibility and safety constraints, and non-stationarity. This survey uses soft matter as a representative setting but focuses on the AI questions that arise in real laboratories. We frame SDL autonomy as an agent environment interaction problem with explicit observations, actions, costs, and constraints, and we use this formulation to connect common SDL pipelines to established AI principles. We review the main method families that enable closed loop experimentation, including Bayesian optimization and active learning for sample efficient experiment selection, planning and reinforcement learning for long horizon protocol optimization, and tool using agents that orchestrate heterogeneous instruments and software. We emphasize verifiable and provenance aware policies that support debugging, reproducibility, and safe operation. We then propose a capability driven taxonomy that organizes systems by decision horizon, uncertainty modeling, action parameterization, constraint handling, failure recovery, and human involvement. To enable meaningful comparison, we synthesize benchmark task templates and evaluation metrics that prioritize cost aware performance, robustness to drift, constraint violation behavior, and reproducibility. Finally, we distill lessons from deployed SDLs and outline open challenges in multi-modal representation, calibrated uncertainty, safe exploration, and shared benchmark infrastructure.

</details>


### [37] [Learning Transferable Skills in Action RPGs via Directed Skill Graphs and Selective Adaptation](https://arxiv.org/abs/2601.17923)
*Ali Najar*

Main category: cs.AI

TL;DR: 研究在《黑暗之魂3》实时控制场景中，通过技能图和分层课程训练让终身智能体提升能力，分解控制为五项技能，因子化提高效率，选择性微调可快速恢复性能。


<details>
  <summary>Details</summary>
Motivation: 让终身智能体随时间扩展能力，避免从头训练和覆盖先前学习的行为。

Method: 将战斗表示为有向技能图，采用分层课程训练其组件，把控制分解为五项可复用技能。

Result: 因子化提高样本效率，环境转变时部分技能可转移，针对性微调两项技能能在有限交互预算下快速恢复性能。

Conclusion: 技能图课程与选择性微调为复杂实时环境中持续学习的智能体提供了实用途径。

Abstract: Lifelong agents should expand their competence over time without retraining from scratch or overwriting previously learned behaviors. We investigate this in a challenging real-time control setting (Dark Souls III) by representing combat as a directed skill graph and training its components in a hierarchical curriculum. The resulting agent decomposes control into five reusable skills: camera control, target lock-on, movement, dodging, and a heal-attack decision policy, each optimized for a narrow responsibility. This factorization improves sample efficiency by reducing the burden on any single policy and supports selective post-training: when the environment shifts from Phase 1 to Phase 2, only a subset of skills must be adapted, while upstream skills remain transferable. Empirically, we find that targeted fine-tuning of just two skills rapidly recovers performance under a limited interaction budget, suggesting that skill-graph curricula together with selective fine-tuning offer a practical pathway toward evolving, continually learning agents in complex real-time environments.

</details>


### [38] [Sentipolis: Emotion-Aware Agents for Social Simulations](https://arxiv.org/abs/2601.18027)
*Chiyuan Fu,Lyuhao Chen,Yunze Xiao,Weihao Xuan,Carlos Busso,Mona Diab*

Main category: cs.AI

TL;DR: 研究提出Sentipolis框架用于情感状态智能体，提升情感行为及连续性，效果与模型相关，支持社会动态研究。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体在社会模拟中常将情感作为临时线索，存在情感遗忘和长期连贯性不足问题。

Method: 提出Sentipolis框架，集成连续的Pleasure - Arousal - Dominance (PAD)表示、双速情感动态和情感 - 记忆耦合。

Result: Sentipolis改善了基于情感的行为，增强了沟通和情感连续性；效果与模型相关；在一定程度上体现了情感驱动行为与规则遵守的张力；网络级诊断显示出互惠、适度聚类和时间稳定的关系结构。

Conclusion: Sentipolis框架有助于提升情感状态智能体的表现，支持对累积社会动态的研究。

Abstract: LLM agents are increasingly used for social simulation, yet emotion is often treated as a transient cue, causing emotional amnesia and weak long-horizon continuity. We present Sentipolis, a framework for emotionally stateful agents that integrates continuous Pleasure-Arousal-Dominance (PAD) representation, dual-speed emotion dynamics, and emotion--memory coupling. Across thousands of interactions over multiple base models and evaluators, Sentipolis improves emotionally grounded behavior, boosting communication, and emotional continuity. Gains are model-dependent: believability increases for higher-capacity models but can drop for smaller ones, and emotion-awareness can mildly reduce adherence to social norms, reflecting a human-like tension between emotion-driven behavior and rule compliance in social simulation. Network-level diagnostics show reciprocal, moderately clustered, and temporally stable relationship structures, supporting the study of cumulative social dynamics such as alliance formation and gradual relationship change.

</details>


### [39] [Expert Evaluation and the Limits of Human Feedback in Mental Health AI Safety Testing](https://arxiv.org/abs/2601.18061)
*Kiana Jafari,Paul Ulrich Nikolaus Rust,Duncan Eddy,Robbie Fraser,Nina Vasan,Darja Djordjevic,Akanksha Dadlani,Max Lamparth,Eugenia Kim,Mykel Kochenderfer*

Main category: cs.AI

TL;DR: 研究测试心理健康领域人类反馈学习假设，发现专家间评分一致性差，建议从基于共识聚合转为保留并学习专家分歧的对齐方法。


<details>
  <summary>Details</summary>
Motivation: 测试人类反馈学习中专家判断聚合产生有效真值这一假设在心理健康领域的有效性。

Method: 三位认证精神科医生用校准评分表独立评估大语言模型生成的回复，进行定性访谈。

Result: 评分者间一致性差，安全关键项分歧最大，分歧源于不同临床框架。

Conclusion: 专家分歧是社会技术现象，建议从业者转变方法。

Abstract: Learning from human feedback~(LHF) assumes that expert judgments, appropriately aggregated, yield valid ground truth for training and evaluating AI systems. We tested this assumption in mental health, where high safety stakes make expert consensus essential. Three certified psychiatrists independently evaluated LLM-generated responses using a calibrated rubric. Despite similar training and shared instructions, inter-rater reliability was consistently poor ($ICC$ $0.087$--$0.295$), falling below thresholds considered acceptable for consequential assessment. Disagreement was highest on the most safety-critical items. Suicide and self-harm responses produced greater divergence than any other category, and was systematic rather than random. One factor yielded negative reliability (Krippendorff's $α= -0.203$), indicating structured disagreement worse than chance. Qualitative interviews revealed that disagreement reflects coherent but incompatible individual clinical frameworks, safety-first, engagement-centered, and culturally-informed orientations, rather than measurement error. By demonstrating that experts rely on holistic risk heuristics rather than granular factor discrimination, these findings suggest that aggregated labels function as arithmetic compromises that effectively erase grounded professional philosophies. Our results characterize expert disagreement in safety-critical AI as a sociotechnical phenomenon where professional experience introduces sophisticated layers of principled divergence. We discuss implications for reward modeling, safety classification, and evaluation benchmarks, recommending that practitioners shift from consensus-based aggregation to alignment methods that preserve and learn from expert disagreement.

</details>


### [40] [Beyond Text-to-SQL: Can LLMs Really Debug Enterprise ETL SQL?](https://arxiv.org/abs/2601.18119)
*Jing Ye,Yiwen Duan,Yonghong Yu,Victor Ma,Yang Gao,Xing Chen*

Main category: cs.AI

TL;DR: 提出首个企业级SQL推理与调试基准OurBench，分析近30个大语言模型表现并探讨解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前即使是经验丰富的开发者和高级文本转SQL的大语言模型，一次生成完全正确的SQL代码仍有困难，需多次调试迭代，因此需要企业级SQL推理和调试基准。

Method: 采用逆向工程自动构建工作流，向大规模SQL代码中系统注入现实bug以生成基准；使用无执行评估框架进行评估。

Result: OurBench包含469个含语法错误和516个含语义错误的复杂查询；近30个大语言模型表现不佳，最佳模型Claude - 4 - Sonnet在语法和语义错误查询上准确率分别仅为36.46%和32.17%，多数模型得分低于20%。

Conclusion: 探索了四种解决方案策略，确定关键挑战并指出企业SQL调试使用大语言模型的有前景方向。

Abstract: SQL is central to enterprise data engineering, yet generating fully correct SQL code in a single attempt remains difficult, even for experienced developers and advanced text-to-SQL LLMs, often requiring multiple debugging iterations. We introduce OurBench, the first benchmark for enterprise-level SQL reasoning and debugging. Our benchmark is built on two key innovations: (1) an automated construction workflow that uses reverse engineering to systematically inject realistic bugs into large-scale SQL code, enabling scalable and diverse benchmark generation; and (2) an execution-free evaluation framework tailored to enterprise settings, providing fast, accurate, and resource-efficient assessment.
  OurBench comprises 469 OurBenchSyn queries featuring syntax errors with explicit error messages, and 516 OurBenchSem queries targeting semantic errors in which the code fails to meet user intent. The queries are highly complex, averaging over 140 lines and featuring deep and wide abstract syntax trees.
  Evaluation of nearly 30 LLMs reveals a substantial performance gap: the best-performing model, Claude-4-Sonnet, achieves only 36.46 percent accuracy on OurBenchSyn and 32.17 percent on OurBenchSem, while most models score below 20 percent. We further explore four solution strategies, identify key challenges, and outline promising directions for enterprise SQL debugging with LLMs.

</details>


### [41] [Deadline-Aware, Energy-Efficient Control of Domestic Immersion Hot Water Heaters](https://arxiv.org/abs/2601.18123)
*Muhammad Ibrahim Khan,Bivin Pradeep,James Brusey*

Main category: cs.AI

TL;DR: 研究有截止时间感知的控制方法以降低家用浸没式热水器能耗，对比多种方法，PPO 节能效果最佳。


<details>
  <summary>Details</summary>
Motivation: 典型家用浸没式热水器冬季连续运行，加热快但效率低，忽视可预测需求窗口和环境热损失，需降低能耗。

Method: 引入高效 Gymnasium 环境建模，采用时间最优 bang - bang 基线、零样本蒙特卡罗树搜索规划器和近端策略优化策略。

Result: 在多种参数组合下，PPO 节能效果最佳，如 60 步时用 3.23 千瓦时，节能效果显著。

Conclusion: 有截止时间感知的控制方法可降低能耗，规划器可部分节能，训练后的策略推理成本近零。

Abstract: Typical domestic immersion water heater systems are often operated continuously during winter, heating quickly rather than efficiently and ignoring predictable demand windows and ambient losses. We study deadline-aware control, where the aim is to reach a target temperature at a specified time while minimising energy consumption. We introduce an efficient Gymnasium environment that models an immersion hot water heater with first-order thermal losses and discrete on and off actions of 0 W and 6000 W applied every 120 seconds. Methods include a time-optimal bang-bang baseline, a zero-shot Monte Carlo Tree Search planner, and a Proximal Policy Optimisation policy. We report total energy consumption in watt-hours under identical physical dynamics. Across sweeps of initial temperature from 10 to 30 degrees Celsius, deadline from 30 to 90 steps, and target temperature from 40 to 80 degrees Celsius, PPO achieves the most energy-efficient performance at a 60-step horizon of 2 hours, using 3.23 kilowatt-hours, compared to 4.37 to 10.45 kilowatt-hours for bang-bang control and 4.18 to 6.46 kilowatt-hours for MCTS. This corresponds to energy savings of 26 percent at 30 steps and 69 percent at 90 steps. In a representative trajectory with a 50 kg water mass, 20 degrees Celsius ambient temperature, and a 60 degrees Celsius target, PPO consumes 54 percent less energy than bang-bang control and 33 percent less than MCTS. These results show that learned deadline-aware control reduces energy consumption under identical physical assumptions, while planners provide partial savings without training and learned policies offer near-zero inference cost once trained.

</details>


### [42] [RouteMoA: Dynamic Routing without Pre-Inference Boosts Efficient Mixture-of-Agents](https://arxiv.org/abs/2601.18130)
*Jize Wang,Han Wu,Zhiyuan You,Yiming Song,Yijun Wang,Zifei Shan,Yining Li,Songyang Zhang,Xinyi Le,Cailian Chen,Xinping Guan,Dacheng Tao*

Main category: cs.AI

TL;DR: 提出高效的动态路由混合智能体框架RouteMoA，通过轻量级筛选和评估机制，在不同任务和模型池规模下优于MoA，大幅降低成本和延迟。


<details>
  <summary>Details</summary>
Motivation: 现有Mixture-of-Agents (MoA) 方法存在成本高、延迟大的问题，且缺乏有效模型选择标准，难以处理大规模模型池。

Method: 提出RouteMoA框架，用轻量级评分器初步筛选，混合评判器精炼分数，模型排名机制平衡性能、成本和延迟。

Result: RouteMoA在不同任务和模型池规模下优于MoA，在大规模模型池降低成本89.8%、延迟63.6%。

Conclusion: RouteMoA是一种高效的混合智能体框架，能有效解决现有方法的问题。

Abstract: Mixture-of-Agents (MoA) improves LLM performance through layered collaboration, but its dense topology raises costs and latency. Existing methods employ LLM judges to filter responses, yet still require all models to perform inference before judging, failing to cut costs effectively. They also lack model selection criteria and struggle with large model pools, where full inference is costly and can exceed context limits. To address this, we propose RouteMoA, an efficient mixture-of-agents framework with dynamic routing. It employs a lightweight scorer to perform initial screening by predicting coarse-grained performance from the query, narrowing candidates to a high-potential subset without inference. A mixture of judges then refines these scores through lightweight self- and cross-assessment based on existing model outputs, providing posterior correction without additional inference. Finally, a model ranking mechanism selects models by balancing performance, cost, and latency. RouteMoA outperforms MoA across varying tasks and model pool sizes, reducing cost by 89.8% and latency by 63.6% in the large-scale model pool.

</details>


### [43] [RareAlert: Aligning heterogeneous large language model reasoning for early rare disease risk screening](https://arxiv.org/abs/2601.18132)
*Xi Chen,Hongru Zhou,Huahui Yi,Shiyu Feng,Hanyu Zhou,Tiancheng He,Mingke You,Li Wang,Qiankun Li,Kun Wang,Weili Fu,Kang Li,Jian Li*

Main category: cs.AI

TL;DR: 文章介绍RareAlert早期筛查系统，可从常规初诊信息预测患者罕见病风险，在独立测试集表现出色，证明其有效性与实用性。


<details>
  <summary>Details</summary>
Motivation: 现有初级护理分诊流程难以可靠识别初诊时的罕见病患者，需进行普遍筛查以减少诊断延迟。

Method: 提出RareAlert系统，集成十个大语言模型的推理结果，用机器学习校准和加权，并提炼成可本地部署的单个模型；构建RareBench数据集开发和评估该系统。

Result: 基于Qwen3 - 4B且使用校准推理信号训练的RareAlert模型在独立测试集上AUC达0.917，优于最佳机器学习集成模型和所有评估的大语言模型。

Conclusion: 证明了大语言模型医学推理的多样性以及在高度不确定临床任务中校准推理的有效性，RareAlert能进行准确、保护隐私且可扩展的罕见病风险筛查，适合大规模本地部署。

Abstract: Missed and delayed diagnosis remains a major challenge in rare disease care. At the initial clinical encounters, physicians assess rare disease risk using only limited information under high uncertainty. When high-risk patients are not recognised at this stage, targeted diagnostic testing is often not initiated, resulting in missed diagnosis. Existing primary care triage processes are structurally insufficient to reliably identify patients with rare diseases at initial clinical presentation and universal screening is needed to reduce diagnostic delay. Here we present RareAlert, an early screening system which predict patient-level rare disease risk from routinely available primary-visit information. RareAlert integrates reasoning generated by ten LLMs, calibrates and weights these signals using machine learning, and distils the aligned reasoning into a single locally deployable model. To develop and evaluate RareAlert, we curated RareBench, a real-world dataset of 158,666 cases covering 33 Orphanet disease categories and more than 7,000 rare conditions, including both rare and non-rare presentations. The results showed that rare disease identification can be reconceptualised as a universal uncertainty resolution process applied to the general patient population. On an independent test set, RareAlert, a Qwen3-4B based model trained with calibrated reasoning signals, achieved an AUC of 0.917, outperforming the best machine learning ensemble and all evaluated LLMs, including GPT-5, DeepSeek-R1, Claude-3.7-Sonnet, o3-mini, Gemini-2.5-Pro, and Qwen3-235B. These findings demonstrate the diversity in LLM medical reasoning and the effectiveness of aligning such reasoning in highly uncertain clinical tasks. By incorporating calibrated reasoning into a single model, RareAlert enables accurate, privacy-preserving, and scalable rare disease risk screening suitable for large-scale local deployment.

</details>


### [44] [DeepPlanning: Benchmarking Long-Horizon Agentic Planning with Verifiable Constraints](https://arxiv.org/abs/2601.18137)
*Yinger Zhang,Shutong Jiang,Renhao Li,Jianhong Tu,Yang Su,Lianghao Deng,Xudong Guo,Chenxu Lv,Junyang Lin*

Main category: cs.AI

TL;DR: 提出DeepPlanning基准测试用于长周期智能体规划评估，发现前沿大语言模型在此有困难并指出改进方向，开源代码数据。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在长周期任务评估中，缺乏对全局约束优化和真实场景信息收集及局部约束的考量。

Method: 引入DeepPlanning基准测试，包含多日旅行规划和多产品购物任务。

Result: 前沿的智能体大语言模型在DeepPlanning基准测试中表现不佳。

Conclusion: 可靠的显式推理模式和并行工具使用对实现更好的效果-效率权衡很重要，错误分析指出长周期规划改进方向。

Abstract: While agent evaluation has shifted toward long-horizon tasks, most benchmarks still emphasize local, step-level reasoning rather than the global constrained optimization (e.g., time and financial budgets) that demands genuine planning ability. Meanwhile, existing LLM planning benchmarks underrepresent the active information gathering and fine-grained local constraints typical of real-world settings. To address this, we introduce DeepPlanning, a challenging benchmark for practical long-horizon agent planning. It features multi-day travel planning and multi-product shopping tasks that require proactive information acquisition, local constrained reasoning, and global constrained optimization. Evaluations on DeepPlanning show that even frontier agentic LLMs struggle with these problems, highlighting the importance of reliable explicit reasoning patterns and parallel tool use for achieving better effectiveness-efficiency trade-offs. Error analysis further points to promising directions for improving agentic LLMs over long planning horizons. We open-source the code and data to support future research.

</details>


### [45] [GAIA: A Data Flywheel System for Training GUI Test-Time Scaling Critic Models](https://arxiv.org/abs/2601.18197)
*Shaokang Wang,Pei Fu,Ruoceng Zhang,Shaojie Zhang,Xiuwen Xi,Jiahui Yang,Bin Qin,Ying Huang,Zhenbo Luo,Jian Luan*

Main category: cs.AI

TL;DR: 提出GAIA训练框架解决GUI代理操作不可逆问题，通过训练ICM提升模型性能，实验证明有效且代码数据集将公开。


<details>
  <summary>Details</summary>
Motivation: 解决LVLMs下GUI代理操作不可逆，一个错误行动可能引发灾难性偏差的问题。

Method: 提出GAIA训练框架，先训练ICM评估行动正确性选高成功率操作，再用初始评论者引导收集样本，用增强数据训练第二轮评论者。

Result: ICM能提升多种闭源和开源模型的测试时性能，且性能随数据循环逐渐提升。

Conclusion: GAIA训练框架有效，可通过迭代评论能力提升GUI代理性能。

Abstract: While Large Vision-Language Models (LVLMs) have significantly advanced GUI agents' capabilities in parsing textual instructions, interpreting screen content, and executing tasks, a critical challenge persists: the irreversibility of agent operations, where a single erroneous action can trigger catastrophic deviations. To address this, we propose the GUI Action Critic's Data Flywheel System (GAIA), a training framework that enables the models to have iterative critic capabilities, which are used to improve the Test-Time Scaling (TTS) of basic GUI agents' performance. Specifically, we train an Intuitive Critic Model (ICM) using positive and negative action examples from a base agent first. This critic evaluates the immediate correctness of the agent's intended actions, thereby selecting operations with higher success probability. Then, the initial critic guides agent actions to collect refined positive/negative samples, initiating the self-improving cycle. The augmented data then trains a second-round critic with enhanced discernment capability. We conduct experiments on various datasets and demonstrate that the proposed ICM can improve the test-time performance of various closed-source and open-source models, and the performance can be gradually improved as the data is recycled. The code and dataset will be publicly released.

</details>


### [46] [SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback](https://arxiv.org/abs/2601.18202)
*Fangyuan Xu,Rujun Han,Yanfei Chen,Zifeng Wang,I-Hung Hsu,Jun Yan,Vishy Tirumalashetty,Eunsol Choi,Tomas Pfister,Chen-Yu Lee*

Main category: cs.AI

TL;DR: 提出自动生成高质量、难度可控的深度搜索问答对的代理管道SAGE，评估显示其效果良好，训练的代理可适应不同检索场景。


<details>
  <summary>Details</summary>
Motivation: 深度搜索代理需大量人工标注，但收集成本高，因此需要自动生成问答对的方法。

Method: 提出SAGE管道，包含数据生成器和搜索代理，二者多轮交互迭代优化问答对。

Result: 内在评估表明SAGE生成的问题需多样推理策略，提高了数据正确性和难度；外在评估显示在基准测试中性能提升达23%；训练的代理可适应不同检索场景。

Conclusion: SAGE能有效生成高质量、难度可控的深度搜索问答对，提升深度搜索代理性能，且训练的代理有良好适应性。

Abstract: Deep search agents, which aim to answer complex questions requiring reasoning across multiple documents, can significantly speed up the information-seeking process. Collecting human annotations for this application is prohibitively expensive due to long and complex exploration trajectories. We propose an agentic pipeline that automatically generates high quality, difficulty-controlled deep search question-answer pairs for a given corpus and a target difficulty level. Our pipeline, SAGE, consists of a data generator which proposes QA pairs and a search agent which attempts to solve the generated question and provide execution feedback for the data generator. The two components interact over multiple rounds to iteratively refine the question-answer pairs until they satisfy the target difficulty level. Our intrinsic evaluation shows SAGE generates questions that require diverse reasoning strategies, while significantly increases the correctness and difficulty of the generated data. Our extrinsic evaluation demonstrates up to 23% relative performance gain on popular deep search benchmarks by training deep search agents with our synthetic data. Additional experiments show that agents trained on our data can adapt from fixed-corpus retrieval to Google Search at inference time, without further training.

</details>


### [47] [Paying Less Generalization Tax: A Cross-Domain Generalization Study of RL Training for LLM Agents](https://arxiv.org/abs/2601.18217)
*Zhihan Liu,Lin Guan,Yixin Nie,Kai Zhang,Zhuoqun Hao,Lin Chen,Asli Celikyilmaz,Zhaoran Wang,Na Zhang*

Main category: cs.AI

TL;DR: 研究未知测试域下通用大语言模型（LLM）智能体后训练挑战，分析影响跨域性能因素并给出改进方法。


<details>
  <summary>Details</summary>
Motivation: 通用LLM智能体后训练和部署的域存在差异，研究未知测试域下的智能体后训练挑战。

Method: 分析强化学习环境和建模选择对跨域性能的影响，提出增加状态信息丰富度的随机化技术，研究不同建模选择的影响。

Result: 识别出状态信息丰富度和规划复杂度与跨域泛化强相关；领域真实性和文本相似度非主要因素；增加状态信息丰富度可提高跨域鲁棒性；特定建模选择对性能和泛化有不同影响。

Conclusion: 揭示了强化学习环境特性和建模选择对跨域性能的影响，提出的随机化技术可提升跨域鲁棒性。

Abstract: Generalist LLM agents are often post-trained on a narrow set of environments but deployed across far broader, unseen domains. In this work, we investigate the challenge of agentic post-training when the eventual test domains are unknown. Specifically, we analyze which properties of reinforcement learning (RL) environments and modeling choices have the greatest influence on out-of-domain performance. First, we identify two environment axes that strongly correlate with cross-domain generalization: (i) state information richness, i.e., the amount of information for the agent to process from the state, and (ii) planning complexity, estimated via goal reachability and trajectory length under a base policy. Notably, domain realism and text-level similarity are not the primary factors; for instance, the simple grid-world domain Sokoban leads to even stronger generalization in SciWorld than the more realistic ALFWorld. Motivated by these findings, we further show that increasing state information richness alone can already effectively improve cross-domain robustness. We propose a randomization technique, which is low-overhead and broadly applicable: add small amounts of distractive goal-irrelevant features to the state to make it richer without altering the task. Beyond environment-side properties, we also examine several modeling choices: (a) SFT warmup or mid-training helps prevent catastrophic forgetting during RL but undermines generalization to domains that are not included in the mid-training datamix; and (b) turning on step-by-step thinking during RL, while not always improving in-domain performance, plays a crucial role in preserving generalization.

</details>


### [48] [ShopSimulator: Evaluating and Exploring RL-Driven LLM Agent for Shopping Assistants](https://arxiv.org/abs/2601.18225)
*Pei Wang,Yanan Wu,Xiaoshuai Song,Weixun Wang,Gengru Chen,Zhongwen Li,Kezhong Yan,Ken Deng,Qi Liu,Shuaibing Zhao,Shaopan Xiong,Xuepeng Liu,Xuefeng Chen,Wanxi Deng,Wenbo Su,Bo Zheng*

Main category: cs.AI

TL;DR: 本文介绍了大规模有挑战性的中文购物环境ShopSimulator，以此评估大语言模型，发现最佳模型全成功率不足40%，并给出克服弱点的训练建议。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏能全面捕捉电商购物代理各项能力的统一模拟环境，且只关注评估基准而无训练支持。

Method: 引入ShopSimulator来评估大语言模型在不同场景下的表现，并进行错误分析和训练探索。

Result: 最佳模型全成功率不到40%，代理在深度搜索、产品选择、平衡个性化线索和与用户互动方面存在困难。

Conclusion: 结合监督微调（SFT）和强化学习（RL）可显著提升性能，代码和数据将公开。

Abstract: Large language model (LLM)-based agents are increasingly deployed in e-commerce shopping. To perform thorough, user-tailored product searches, agents should interpret personal preferences, engage in multi-turn dialogues, and ultimately retrieve and discriminate among highly similar products. However, existing research has yet to provide a unified simulation environment that consistently captures all of these aspects, and always focuses solely on evaluation benchmarks without training support. In this paper, we introduce ShopSimulator, a large-scale and challenging Chinese shopping environment. Leveraging ShopSimulator, we evaluate LLMs across diverse scenarios, finding that even the best-performing models achieve less than 40% full-success rate. Error analysis reveals that agents struggle with deep search and product selection in long trajectories, fail to balance the use of personalization cues, and to effectively engage with users. Further training exploration provides practical guidance for overcoming these weaknesses, with the combination of supervised fine-tuning (SFT) and reinforcement learning (RL) yielding significant performance improvements. Code and data will be released at https://github.com/ShopAgent-Team/ShopSimulator.

</details>


### [49] [Yunjue Agent Tech Report: A Fully Reproducible, Zero-Start In-Situ Self-Evolving Agent System for Open-Ended Tasks](https://arxiv.org/abs/2601.18226)
*Haotian Li,Shijun Yang,Weizhen Qi,Silei Zhao,Rui Hua,Mingzhu Song,Xiaojian Yang,Chao Peng*

Main category: cs.AI

TL;DR: 针对传统智能体系统在开放式环境的不足，提出原位自我进化范式，开发Yunjue Agent系统，采用并行批量进化策略，实验有显著性能提升，还提出新指标监测进化收敛并开源代码等。


<details>
  <summary>Details</summary>
Motivation: 传统智能体系统在开放式环境中，因依赖静态工具集或离线训练，难以应对任务分布变化和缺乏外部监督的情况。

Method: 提出原位自我进化范式，将顺序任务交互视为经验流，以工具进化扩展能力；开发Yunjue Agent系统迭代合成、优化和复用工具；引入并行批量进化策略；提出新指标监测进化收敛。

Result: 在零启动设置的五个不同基准测试中性能显著优于专有基线，暖启动评估表明积累的通用知识可转移到新领域。

Conclusion: 所提方法有效可行，开源代码等利于未来对弹性、自我进化智能的研究。

Abstract: Conventional agent systems often struggle in open-ended environments where task distributions continuously drift and external supervision is scarce. Their reliance on static toolsets or offline training lags behind these dynamics, leaving the system's capability boundaries rigid and unknown. To address this, we propose the In-Situ Self-Evolving paradigm. This approach treats sequential task interactions as a continuous stream of experience, enabling the system to distill short-term execution feedback into long-term, reusable capabilities without access to ground-truth labels. Within this framework, we identify tool evolution as the critical pathway for capability expansion, which provides verifiable, binary feedback signals. Within this framework, we develop Yunjue Agent, a system that iteratively synthesizes, optimizes, and reuses tools to navigate emerging challenges. To optimize evolutionary efficiency, we further introduce a Parallel Batch Evolution strategy. Empirical evaluations across five diverse benchmarks under a zero-start setting demonstrate significant performance gains over proprietary baselines. Additionally, complementary warm-start evaluations confirm that the accumulated general knowledge can be seamlessly transferred to novel domains. Finally, we propose a novel metric to monitor evolution convergence, serving as a function analogous to training loss in conventional optimization. We open-source our codebase, system traces, and evolved tools to facilitate future research in resilient, self-evolving intelligence.

</details>


### [50] [Think-Augmented Function Calling: Improving LLM Parameter Accuracy Through Embedded Reasoning](https://arxiv.org/abs/2601.18282)
*Lei Wei,Jinpeng Ou,Xiao Peng,Bin Wang*

Main category: cs.AI

TL;DR: 现有大语言模型函数调用机制缺乏推理透明度，本文提出TAFC框架，在函数和参数层面增强推理，提升参数生成准确性和推理连贯性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型函数调用机制在参数生成时缺乏显式推理透明度，现有方法无法为单个函数参数提供细粒度推理指导。

Method: 提出Think - Augmented Function Calling (TAFC)框架，引入通用“think”参数增强，动态优化参数描述，根据复杂度评分触发细粒度推理，提出推理引导优化。

Result: 在ToolBench上对专有和开源模型的评估显示，TAFC显著提高了多参数函数的参数生成准确性和推理连贯性。

Conclusion: TAFC无需修改现有大语言模型架构，保持API兼容性，能提升函数调用准确性和可解释性，便于调试AI代理行为。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in function calling for autonomous agents, yet current mechanisms lack explicit reasoning transparency during parameter generation, particularly for complex functions with interdependent parameters. While existing approaches like chain-of-thought prompting operate at the agent level, they fail to provide fine-grained reasoning guidance for individual function parameters. To address these limitations, we propose Think-Augmented Function Calling (TAFC), a novel framework that enhances function calling accuracy through explicit reasoning at both function and parameter levels. Our method introduces a universal "think" parameter augmentation that enables models to articulate their decision-making process, with dynamic optimization for parameter descriptions to improve reasoning quality. For complex parameters, TAFC automatically triggers granular reasoning based on complexity scoring, ensuring appropriate justification for critical decisions. Additionally, we propose reasoning-guided optimization to align generated reasoning with human expectations. TAFC requires no architectural modifications to existing LLMs while maintaining full API compatibility. Evaluation on ToolBench across proprietary and open-source models demonstrates significant improvements in parameter generation accuracy and reasoning coherence for multi-parameter functions, while providing enhanced interpretability for debugging AI agent behaviors.

</details>


### [51] [A Generative AI-Driven Reliability Layer for Action-Oriented Disaster Resilience](https://arxiv.org/abs/2601.18308)
*Geunsik Lim*

Main category: cs.AI

TL;DR: 引入基于生成式AI的Climate RADAR系统，评估显示有更好效果，推动以人为主的预警系统发展。


<details>
  <summary>Details</summary>
Motivation: 传统早期预警系统虽能快速发布警报，但常无法触发及时防护行动，导致可预防的损失和不公平现象。

Method: 将气象、水文、脆弱性和社会数据整合为综合风险指数，采用嵌入护栏的大语言模型在不同界面提供个性化建议。

Result: 通过模拟、用户研究和市政试点评估，实现更高的防护行动执行率、减少响应延迟、提高可用性和信任度。

Conclusion: Climate RADAR结合多种技术，推动以人为主、透明和公平的早期预警系统，为合规的灾害恢复基础设施提供实用途径。

Abstract: As climate-related hazards intensify, conventional early warning systems (EWS) disseminate alerts rapidly but often fail to trigger timely protective actions, leading to preventable losses and inequities. We introduce Climate RADAR (Risk-Aware, Dynamic, and Action Recommendation system), a generative AI-based reliability layer that reframes disaster communication from alerts delivered to actions executed. It integrates meteorological, hydrological, vulnerability, and social data into a composite risk index and employs guardrail-embedded large language models (LLMs) to deliver personalized recommendations across citizen, volunteer, and municipal interfaces. Evaluation through simulations, user studies, and a municipal pilot shows improved outcomes, including higher protective action execution, reduced response latency, and increased usability and trust. By combining predictive analytics, behavioral science, and responsible AI, Climate RADAR advances people-centered, transparent, and equitable early warning systems, offering practical pathways toward compliance-ready disaster resilience infrastructures.

</details>


### [52] [Can Good Writing Be Generative? Expert-Level AI Writing Emerges through Fine-Tuning on High-Quality Books](https://arxiv.org/abs/2601.18353)
*Tuhin Chakrabarty,Paramveer S. Dhillon*

Main category: cs.AI

TL;DR: 研究通过实验对比人类作家与大语言模型模仿名家写作，发现专家和普通评委偏好不同，引发对AI创作限制及创意劳动未来的思考。


<details>
  <summary>Details</summary>
Motivation: 挑战机器无法复制人类创意写作的传统假设，深入了解生成式AI在创意写作方面的能力。

Method: 开展行为实验，让28位MFA作家与3个大语言模型模仿50位知名作家写作，由专家和普通评委进行盲选。

Result: 在上下文提示条件下专家82.7%偏好人类写作，微调后62%偏好AI；普通评委始终偏好AI写作。专家作家出现身份危机。

Conclusion: 挑战了关于AI创意限制的讨论，引发对创意劳动未来的根本问题。

Abstract: Creative writing has long been considered a uniquely human endeavor, requiring voice and style that machines could not replicate. This assumption is challenged by Generative AI that can emulate thousands of author styles in seconds with negligible marginal labor. To understand this better, we conducted a behavioral experiment where 28 MFA writers (experts) competed against three LLMs in emulating 50 critically acclaimed authors. Based on blind pairwise comparisons by 28 expert judges and 131 lay judges, we find that experts preferred human writing in 82.7% of cases under the in-context prompting condition but this reversed to 62% preference for AI after fine-tuning on authors' complete works. Lay judges, however, consistently preferred AI writing. Debrief interviews with expert writers revealed that their preference for AI writing triggered an identity crisis, eroding aesthetic confidence and questioning what constitutes "good writing." These findings challenge discourse about AI's creative limitations and raise fundamental questions about the future of creative labor.

</details>


### [53] [Dynamic Thinking-Token Selection for Efficient Reasoning in Large Reasoning Models](https://arxiv.org/abs/2601.18383)
*Zhenyuan Guo,Tong Chen,Wenlong Meng,Chen Gong,Xin Yu,Chengkun Wei,Wenzhi Chen*

Main category: cs.AI

TL;DR: 本文指出大推理模型推理时效率瓶颈，分析推理痕迹影响，提出DynTS方法优化效率。


<details>
  <summary>Details</summary>
Motivation: 大推理模型在推理时会因生成推理痕迹产生大量内存占用和计算开销，导致效率瓶颈。

Method: 通过注意力图分析推理痕迹的影响，发现关键决策令牌，提出动态思维令牌选择（DynTS）方法，推理时仅保留关键决策令牌的键值缓存状态。

Result: 未提及具体结果。

Conclusion: 未提及明确结论，但目标是通过DynTS方法优化大推理模型的效率。

Abstract: Large Reasoning Models (LRMs) excel at solving complex problems by explicitly generating a reasoning trace before deriving the final answer. However, these extended generations incur substantial memory footprint and computational overhead, bottlenecking LRMs' efficiency. This work uses attention maps to analyze the influence of reasoning traces and uncover an interesting phenomenon: only some decision-critical tokens in a reasoning trace steer the model toward the final answer, while the remaining tokens contribute negligibly. Building on this observation, we propose Dynamic Thinking-Token Selection (DynTS). This method identifies decision-critical tokens and retains only their associated Key-Value (KV) cache states during inference, evicting the remaining redundant entries to optimize efficiency.

</details>


### [54] [OffSeeker: Online Reinforcement Learning Is Not All You Need for Deep Research Agents](https://arxiv.org/abs/2601.18467)
*Yuhang Zhou,Kai Zheng,Qiguang Chen,Mengkang Hu,Qingfeng Sun,Can Xu,Jingjing Chen*

Main category: cs.AI

TL;DR: 本文表明构建强大研究智能体无需昂贵在线强化学习，引入开源离线训练套件，训练出OffSeeker模型，评估显示其表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前在线强化学习构建研究智能体成本高，离线训练受高质量研究轨迹稀缺阻碍，需要更有效方法。

Method: 引入完全开源套件用于离线训练，包括DeepForge任务合成框架和大量数据对，训练出完全离线的OffSeeker (8B)模型。

Result: 在六个基准测试中，OffSeeker在同规模智能体中领先，与通过大量在线强化学习训练的30B参数系统有竞争力。

Conclusion: 昂贵的在线强化学习并非构建强大研究智能体的唯一途径，离线训练方法有效。

Abstract: Deep research agents have shown remarkable potential in handling long-horizon tasks. However, state-of-the-art performance typically relies on online reinforcement learning (RL), which is financially expensive due to extensive API calls. While offline training offers a more efficient alternative, its progress is hindered by the scarcity of high-quality research trajectories. In this paper, we demonstrate that expensive online reinforcement learning is not all you need to build powerful research agents. To bridge this gap, we introduce a fully open-source suite designed for effective offline training. Our core contributions include DeepForge, a ready-to-use task synthesis framework that generates large-scale research queries without heavy preprocessing; and a curated collection of 66k QA pairs, 33k SFT trajectories, and 21k DPO pairs. Leveraging these resources, we train OffSeeker (8B), a model developed entirely offline. Extensive evaluations across six benchmarks show that OffSeeker not only leads among similar-sized agents but also remains competitive with 30B-parameter systems trained via heavy online RL.

</details>


### [55] [AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security](https://arxiv.org/abs/2601.18491)
*Dongrui Liu,Qihan Ren,Chen Qian,Shuai Shao,Yuejin Xie,Yu Li,Zhonghao Yang,Haoyu Luo,Peng Wang,Qingyu Liu,Binxin Hu,Ling Tang,Jilin Mei,Dadi Guo,Leitao Yuan,Junyao Yang,Guanxu Chen,Qihao Lin,Yi Yu,Bo Zhang,Jiaxuan Guo,Jie Zhang,Wenqi Shao,Huiqi Deng,Zhiheng Xi,Wenjie Wang,Wenxuan Wang,Wen Shen,Zhikai Chen,Haoyu Xie,Jialing Tao,Juntao Dai,Jiaming Ji,Zhongjie Ba,Linfeng Zhang,Yong Liu,Quanshi Zhang,Lei Zhu,Zhihua Wei,Hui Xue,Chaochao Lu,Jing Shao,Xia Hu*

Main category: cs.AI

TL;DR: 本文针对AI智能体安全问题，提出统一分类法、新基准和诊断护栏框架AgentDoG，实验证明其在复杂场景下性能达最优，且模型和数据集开源。


<details>
  <summary>Details</summary>
Motivation: 当前护栏模型缺乏智能体风险意识和风险诊断透明度，需引入覆盖复杂风险行为的智能体护栏。

Method: 提出统一三维分类法对智能体风险分类，引入细粒度安全基准ATBench和诊断护栏框架AgentDoG。

Result: AgentDoG在多种复杂交互场景的智能体安全调节中达到了最优性能。

Conclusion: AgentDoG能对不安全和看似安全但不合理的行为进行根因诊断，提供溯源和透明度，有助于有效对齐智能体，且模型和数据集开源。

Abstract: The rise of AI agents introduces complex safety and security challenges arising from autonomous tool use and environmental interactions. Current guardrail models lack agentic risk awareness and transparency in risk diagnosis. To introduce an agentic guardrail that covers complex and numerous risky behaviors, we first propose a unified three-dimensional taxonomy that orthogonally categorizes agentic risks by their source (where), failure mode (how), and consequence (what). Guided by this structured and hierarchical taxonomy, we introduce a new fine-grained agentic safety benchmark (ATBench) and a Diagnostic Guardrail framework for agent safety and security (AgentDoG). AgentDoG provides fine-grained and contextual monitoring across agent trajectories. More Crucially, AgentDoG can diagnose the root causes of unsafe actions and seemingly safe but unreasonable actions, offering provenance and transparency beyond binary labels to facilitate effective agent alignment. AgentDoG variants are available in three sizes (4B, 7B, and 8B parameters) across Qwen and Llama model families. Extensive experimental results demonstrate that AgentDoG achieves state-of-the-art performance in agentic safety moderation in diverse and complex interactive scenarios. All models and datasets are openly released.

</details>


### [56] [DEEPMED: Building a Medical DeepResearch Agent via Multi-hop Med-Search Data and Turn-Controlled Agentic Training & Inference](https://arxiv.org/abs/2601.18496)
*Zihan wang,Hao Wang,Shi Feng,Xiaocui Yang,Daling Wang,Yiqun Zhang,Jinghao Lin,Haihua Yang,Xiaozhong Ji*

Main category: cs.AI

TL;DR: 医疗推理模型受限于参数知识，普通DR模型在医学领域提升有限，本文提出DeepMed模型，在七个医学基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有医学推理模型受参数知识限制，普通DR模型直接应用于医学领域提升有限，存在任务特征和工具使用扩展的差距。

Method: 数据上采用多跳医疗搜索问答合成方法；训练时引入难度感知回合惩罚；推理时使用监视器验证假设。

Result: 在七个医学基准测试中，DeepMed平均提升其基础模型9.79%，优于更大的医学推理和DR模型。

Conclusion: DeepMed模型有效解决了现有医学推理与普通DR模型应用于医学领域存在的问题，表现优秀。

Abstract: Medical reasoning models remain constrained by parametric knowledge and are thus susceptible to forgetting and hallucinations. DeepResearch (DR) models ground outputs in verifiable evidence from tools and perform strongly in general domains, but their direct transfer to medical field yields relatively limited gains. We attribute this to two gaps: task characteristic and tool-use scaling. Medical questions require evidence interpretation in a knowledge-intensive clinical context; while general DR models can retrieve information, they often lack clinical-context reasoning and thus "find it but fail to use it," leaving performance limited by medical abilities. Moreover, in medical scenarios, blindly scaling tool-call can inject noisy context, derailing sensitive medical reasoning and prompting repetitive evidence-seeking along incorrect paths. Therefore, we propose DeepMed. For data, we deploy a multi-hop med-search QA synthesis method supporting the model to apply the DR paradigm in medical contexts. For training, we introduce a difficulty-aware turn-penalty to suppress excessive tool-call growth. For inference, we bring a monitor to help validate hypotheses within a controlled number of steps and avoid context rot. Overall, on seven medical benchmarks, DeepMed improves its base model by 9.79\% on average and outperforms larger medical reasoning and DR models.

</details>


### [57] [Deconstructing Instruction-Following: A New Benchmark for Granular Evaluation of Large Language Model Instruction Compliance Abilities](https://arxiv.org/abs/2601.18554)
*Alberto Purpura,Li Wang,Sahil Badyal,Eugenio Beaufrand,Adam Faulkner*

Main category: cs.AI

TL;DR: 提出MOSAIC框架评估大语言模型指令遵循能力，揭示模型在不同约束下的表现差异。


<details>
  <summary>Details</summary>
Motivation: 现有基准难以反映大语言模型在现实中遵循复杂指令的能力，需要新的评估方法。

Method: 引入MOSAIC模块化框架，使用动态生成的含多达20个面向应用生成约束的数据集进行评估。

Result: 评估五种不同家族的大语言模型，发现指令遵循能力非单一能力，随约束类型、数量和位置有显著差异，揭示模型特定弱点、指令间的协同和冲突关系以及位置偏差。

Conclusion: 这些细致的洞察对诊断模型失败和开发更可靠的大语言模型至关重要。

Abstract: Reliably ensuring Large Language Models (LLMs) follow complex instructions is a critical challenge, as existing benchmarks often fail to reflect real-world use or isolate compliance from task success. We introduce MOSAIC (MOdular Synthetic Assessment of Instruction Compliance), a modular framework that uses a dynamically generated dataset with up to 20 application-oriented generation constraints to enable a granular and independent analysis of this capability. Our evaluation of five LLMs from different families based on this new benchmark demonstrates that compliance is not a monolithic capability but varies significantly with constraint type, quantity, and position. The analysis reveals model-specific weaknesses, uncovers synergistic and conflicting interactions between instructions, and identifies distinct positional biases such as primacy and recency effects. These granular insights are critical for diagnosing model failures and developing more reliable LLMs for systems that demand strict adherence to complex instructions.

</details>


### [58] [Stability as a Liability:Systematic Breakdown of Linguistic Structure in LLMs](https://arxiv.org/abs/2601.18588)
*Xianzhe Meng,Qiangsheng Zeng,Ling Luo,Qinghan Yang,Jiarui Hao,Wenbo Wu,Qinyu Wang,Rui Yin,Lin Qi,Renzhi Lu*

Main category: cs.AI

TL;DR: 分析训练稳定性对大语言模型生成分布的影响，发现稳定性与生成表现力不一致，稳定性不能单独作为生成质量指标。


<details>
  <summary>Details</summary>
Motivation: 研究稳定训练动态对生成分布的影响。

Method: 理论分析标准最大似然训练下稳定参数轨迹的影响，并用基于反馈的训练框架进行实证验证。

Result: 稳定参数轨迹使平稳解近似最小化前向KL散度并降低生成熵，模型出现系统退化，实证观察到低熵输出和重复行为。

Conclusion: 优化稳定性和生成表现力并非天然一致，稳定性不足以衡量生成质量。

Abstract: Training stability is typically regarded as a prerequisite for reliable optimization in large language models. In this work, we analyze how stabilizing training dynamics affects the induced generation distribution. We show that under standard maximum likelihood training, stable parameter trajectories lead stationary solutions to approximately minimize the forward KL divergence to the empirical distribution, while implicitly reducing generative entropy. As a consequence, the learned model can concentrate probability mass on a limited subset of empirical modes, exhibiting systematic degeneration despite smooth loss convergence. We empirically validate this effect using a controlled feedback-based training framework that stabilizes internal generation statistics, observing consistent low-entropy outputs and repetitive behavior across architectures and random seeds. It indicates that optimization stability and generative expressivity are not inherently aligned, and that stability alone is an insufficient indicator of generative quality.

</details>


### [59] [A Balanced Neuro-Symbolic Approach for Commonsense Abductive Logic](https://arxiv.org/abs/2601.18595)
*Joseph Cotnareanu,Didier Chetelat,Yingxue Zhang,Mark Coates*

Main category: cs.AI

TL;DR: 提出用逻辑求解器反馈结合大语言模型常识关系迭代增强逻辑问题的方法，在去除部分常识信息的数据集上效果好


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂证明规划问题上推理能力不足，现成逻辑求解器无法处理缺失的常识关系

Method: 用逻辑求解器的反馈，以迭代方式让大语言模型提供常识关系来增强逻辑问题，通过搜索潜在常识假设平衡成本与找到有用事实的概率

Result: 在去除部分常识信息的纯逻辑推理数据集上，该方法比现有技术有显著提升

Conclusion: 在人类环境中工作时，平衡神经和符号元素有价值

Abstract: Although Large Language Models (LLMs) have demonstrated impressive formal reasoning abilities, they often break down when problems require complex proof planning. One promising approach for improving LLM reasoning abilities involves translating problems into formal logic and using a logic solver. Although off-the-shelf logic solvers are in principle substantially more efficient than LLMs at logical reasoning, they assume that all relevant facts are provided in a question and are unable to deal with missing commonsense relations. In this work, we propose a novel method that uses feedback from the logic solver to augment a logic problem with commonsense relations provided by the LLM, in an iterative manner. This involves a search procedure through potential commonsense assumptions to maximize the chance of finding useful facts while keeping cost tractable. On a collection of pure-logical reasoning datasets, from which some commonsense information has been removed, our method consistently achieves considerable improvements over existing techniques, demonstrating the value in balancing neural and symbolic elements when working in human contexts.

</details>


### [60] [PolySHAP: Extending KernelSHAP with Interaction-Informed Polynomial Regression](https://arxiv.org/abs/2601.18608)
*Fabian Fumagalli,R. Teal Witter,Christopher Musco*

Main category: cs.AI

TL;DR: 本文通过高阶多项式近似游戏扩展了KernelSHAP，提出PolySHAP方法，对基准数据集有更好的Shapley值估计，还建立了与配对采样的联系并给出理论解释。


<details>
  <summary>Details</summary>
Motivation: KernelSHAP通过线性函数近似游戏计算Shapley值，但无法捕捉特征间的非线性交互，需要改进。

Method: 通过高阶多项式近似游戏，提出PolySHAP方法，并将其与配对采样建立联系。

Result: PolySHAP方法在各种基准数据集上得到了更好的Shapley值估计，且估计是一致的；证明配对采样与二阶PolySHAP输出相同的Shapley值近似。

Conclusion: PolySHAP方法有效，为配对采样启发式的良好实际性能提供了理论依据。

Abstract: Shapley values have emerged as a central game-theoretic tool in explainable AI (XAI). However, computing Shapley values exactly requires $2^d$ game evaluations for a model with $d$ features. Lundberg and Lee's KernelSHAP algorithm has emerged as a leading method for avoiding this exponential cost. KernelSHAP approximates Shapley values by approximating the game as a linear function, which is fit using a small number of game evaluations for random feature subsets.
  In this work, we extend KernelSHAP by approximating the game via higher degree polynomials, which capture non-linear interactions between features. Our resulting PolySHAP method yields empirically better Shapley value estimates for various benchmark datasets, and we prove that these estimates are consistent.
  Moreover, we connect our approach to paired sampling (antithetic sampling), a ubiquitous modification to KernelSHAP that improves empirical accuracy. We prove that paired sampling outputs exactly the same Shapley value approximations as second-order PolySHAP, without ever fitting a degree 2 polynomial. To the best of our knowledge, this finding provides the first strong theoretical justification for the excellent practical performance of the paired sampling heuristic.

</details>


### [61] [Emergence of Phonemic, Syntactic, and Semantic Representations in Artificial Neural Networks](https://arxiv.org/abs/2601.18617)
*Pierre Orhan,Pablo Diego-Simón,Emmnanuel Chemla,Yair Lakretz,Yves Boubenec,Jean-Rémi King*

Main category: cs.AI

TL;DR: 研究人工神经网络训练中语音、词汇和句法表征的出现情况，发现其学习阶段与儿童有异同，为理解语言习得计算机制指明方向。


<details>
  <summary>Details</summary>
Motivation: 缺乏统一计算框架解释语言习得潜在神经表征，研究人工神经网络训练中语音、词汇和句法表征的出现情况。

Method: 研究人工神经网络训练过程中，语音和文本模型神经激活情况。

Result: 语音和文本模型遵循学习阶段顺序，构建代表语音、词汇和句法结构的子空间，但所需数据量比儿童多2 - 4个数量级。

Conclusion: 指出语言习得主要阶段自发出现的条件，为理解语言习得计算机制提供了有前景的路径。

Abstract: During language acquisition, children successively learn to categorize phonemes, identify words, and combine them with syntax to form new meaning. While the development of this behavior is well characterized, we still lack a unifying computational framework to explain its underlying neural representations. Here, we investigate whether and when phonemic, lexical, and syntactic representations emerge in the activations of artificial neural networks during their training. Our results show that both speech- and text-based models follow a sequence of learning stages: during training, their neural activations successively build subspaces, where the geometry of the neural activations represents phonemic, lexical, and syntactic structure. While this developmental trajectory qualitatively relates to children's, it is quantitatively different: These algorithms indeed require two to four orders of magnitude more data for these neural representations to emerge. Together, these results show conditions under which major stages of language acquisition spontaneously emerge, and hence delineate a promising path to understand the computations underpinning language acquisition.

</details>


### [62] [Assessing the Quality of Mental Health Support in LLM Responses through Multi-Attribute Human Evaluation](https://arxiv.org/abs/2601.18630)
*Abeer Badawi,Md Tahmid Rahman Laskar,Elahe Rahimi,Sheri Grach,Lindsay Bertrand,Lames Danok,Frank Rudzicz,Jimmy Huang,Elham Dolatabadi*

Main category: cs.AI

TL;DR: 全球心理健康危机凸显，大语言模型可作支持途径但有挑战，本文用人类导向评估方法评价LLM治疗对话，分析发现其认知可靠但情感对齐不稳定，呼吁建立平衡评估框架。


<details>
  <summary>Details</summary>
Motivation: 全球心理健康危机中治疗存在缺口、治疗师短缺，大语言模型虽有潜力但在可靠性、治疗相关性等方面有挑战，需评估其在治疗对话中的表现。

Method: 从含真实场景问题的数据集中整理了500个心理健康对话数据集，评估9个不同LLM生成的回复，由两位精神科专家按6属性评分表独立评分。

Result: LLM在认知上可靠，能提供安全、连贯、临床合适信息，但情感对齐不稳定；闭源模型治疗回复较平衡，开源模型变异性大且情感平淡。

Conclusion: 存在认知 - 情感差距，需以关系敏感性和信息准确性并重的评估框架，倡导有人参与的平衡评估协议以指导心理健康对话AI的设计和监督。

Abstract: The escalating global mental health crisis, marked by persistent treatment gaps, availability, and a shortage of qualified therapists, positions Large Language Models (LLMs) as a promising avenue for scalable support. While LLMs offer potential for accessible emotional assistance, their reliability, therapeutic relevance, and alignment with human standards remain challenging to address. This paper introduces a human-grounded evaluation methodology designed to assess LLM generated responses in therapeutic dialogue. Our approach involved curating a dataset of 500 mental health conversations from datasets with real-world scenario questions and evaluating the responses generated by nine diverse LLMs, including closed source and open source models. More specifically, these responses were evaluated by two psychiatric trained experts, who independently rated each on a 5 point Likert scale across a comprehensive 6 attribute rubric. This rubric captures Cognitive Support and Affective Resonance, providing a multidimensional perspective on therapeutic quality. Our analysis reveals that LLMs provide strong cognitive reliability by producing safe, coherent, and clinically appropriate information, but they demonstrate unstable affective alignment. Although closed source models (e.g., GPT-4o) offer balanced therapeutic responses, open source models show greater variability and emotional flatness. We reveal a persistent cognitive-affective gap and highlight the need for failure aware, clinically grounded evaluation frameworks that prioritize relational sensitivity alongside informational accuracy in mental health oriented LLMs. We advocate for balanced evaluation protocols with human in the loop that center on therapeutic sensitivity and provide a framework to guide the responsible design and clinical oversight of mental health oriented conversational AI.

</details>


### [63] [AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning](https://arxiv.org/abs/2601.18631)
*Mingyang Song,Haoyu Sun,Jiawei Gu,Linjie Li,Luxin Xu,Ranjay Krishna,Yu Cheng*

Main category: cs.AI

TL;DR: 引入AdaReasoner模型，通过数据处理、强化学习和自适应学习机制学习工具使用，在基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型视觉推理中工具使用问题，包括选工具、调用时机和多步组合。

Method: 采用可扩展数据处理管道、Tool - GRPO强化学习算法和自适应学习机制。

Result: AdaReasoner展现出良好工具适应性和泛化能力，在基准测试中取得领先性能，超越GPT - 5等。

Conclusion: 该方法能使模型将工具使用作为通用推理技能学习，有效提升视觉推理性能。

Abstract: When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw.

</details>


### [64] [FadeMem: Biologically-Inspired Forgetting for Efficient Agent Memory](https://arxiv.org/abs/2601.18642)
*Lei Wei,Xu Dong,Xiao Peng,Niantao Xie,Bin Wang*

Main category: cs.AI

TL;DR: 论文指出大语言模型作为自主代理存在内存限制问题，提出生物启发的FadeMem记忆架构，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型作为自主代理面临内存限制，缺乏选择性遗忘机制，当前AI系统的二元保留策略存在弊端。

Method: 提出FadeMem架构，在双层内存层次中实现不同衰减率，通过自适应指数衰减函数控制保留，结合LLM引导的冲突解决和智能内存融合。

Result: 在Multi - Session Chat、LoCoMo和LTI - Bench实验中，实现45%的存储减少，展现出卓越的多跳推理和检索能力。

Conclusion: 生物启发的遗忘机制在代理内存系统中是有效的。

Abstract: Large language models deployed as autonomous agents face critical memory limitations, lacking selective forgetting mechanisms that lead to either catastrophic forgetting at context boundaries or information overload within them. While human memory naturally balances retention and forgetting through adaptive decay processes, current AI systems employ binary retention strategies that preserve everything or lose it entirely. We propose FadeMem, a biologically-inspired agent memory architecture that incorporates active forgetting mechanisms mirroring human cognitive efficiency. FadeMem implements differential decay rates across a dual-layer memory hierarchy, where retention is governed by adaptive exponential decay functions modulated by semantic relevance, access frequency, and temporal patterns. Through LLM-guided conflict resolution and intelligent memory fusion, our system consolidates related information while allowing irrelevant details to fade. Experiments on Multi-Session Chat, LoCoMo, and LTI-Bench demonstrate superior multi-hop reasoning and retrieval with 45\% storage reduction, validating the effectiveness of biologically-inspired forgetting in agent memory systems.

</details>


### [65] [TEA-Bench: A Systematic Benchmarking of Tool-enhanced Emotional Support Dialogue Agent](https://arxiv.org/abs/2601.18700)
*Xingyu Sui,Yanyan Zhao,Yulin Hu,Jiahe Guo,Weixiang Zhao,Bing Qin*

Main category: cs.AI

TL;DR: 提出交互式基准TEA - Bench评估ESC中工具增强的代理，实验表明工具增强有效果但依赖模型能力，还发布数据集TEA - Dialog，强调工具使用对构建可靠情感支持代理的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有ESC系统和基准主要关注纯文本情感支持，忽略外部工具在多轮情感支持中的事实依据和减少幻觉的作用。

Method: 引入TEA - Bench评估工具增强的代理，在九个大语言模型上进行实验，发布TEA - Dialog数据集并进行监督微调。

Result: 工具增强通常能提高情感支持质量和减少幻觉，效果依赖模型能力；监督微调在分布内支持有提升但泛化性差。

Conclusion: 强调工具使用在构建可靠情感支持代理中的重要性。

Abstract: Emotional Support Conversation requires not only affective expression but also grounded instrumental support to provide trustworthy guidance. However, existing ESC systems and benchmarks largely focus on affective support in text-only settings, overlooking how external tools can enable factual grounding and reduce hallucination in multi-turn emotional support. We introduce TEA-Bench, the first interactive benchmark for evaluating tool-augmented agents in ESC, featuring realistic emotional scenarios, an MCP-style tool environment, and process-level metrics that jointly assess the quality and factual grounding of emotional support. Experiments on nine LLMs show that tool augmentation generally improves emotional support quality and reduces hallucination, but the gains are strongly capacity-dependent: stronger models use tools more selectively and effectively, while weaker models benefit only marginally. We further release TEA-Dialog, a dataset of tool-enhanced ESC dialogues, and find that supervised fine-tuning improves in-distribution support but generalizes poorly. Our results underscore the importance of tool use in building reliable emotional support agents.

</details>


### [66] [Health-SCORE: Towards Scalable Rubrics for Improving Health-LLMs](https://arxiv.org/abs/2601.18706)
*Zhichao Yang,Sepehr Janghorbani,Dongxu Zhang,Jun Han,Qian Qian,Andrew Ressler,Gregory D. Lyng,Sanjit Singh Batra,Robert E. Tillman*

Main category: cs.AI

TL;DR: 介绍Health - SCORE框架降低评分标准开发成本，用于LLM开放式医疗任务评估和训练，效果可与人工评分媲美。


<details>
  <summary>Details</summary>
Motivation: 创建高质量的特定领域评分标准需要大量人力和成本，导致基于评分标准的评估和训练难以扩展。

Method: 引入Health - SCORE通用可扩展的基于评分标准的训练和评估框架。

Result: Health - SCORE在开放式医疗任务中实现了与人工创建的评分标准相当的评估质量，同时显著降低了开发成本。

Conclusion: Health - SCORE让基于评分标准的评估和训练更具可扩展性。

Abstract: Rubrics are essential for evaluating open-ended LLM responses, especially in safety-critical domains such as healthcare. However, creating high-quality and domain-specific rubrics typically requires significant human expertise time and development cost, making rubric-based evaluation and training difficult to scale. In this work, we introduce Health-SCORE, a generalizable and scalable rubric-based training and evaluation framework that substantially reduces rubric development costs without sacrificing performance. We show that Health-SCORE provides two practical benefits beyond standalone evaluation: it can be used as a structured reward signal to guide reinforcement learning with safety-aware supervision, and it can be incorporated directly into prompts to improve response quality through in-context learning. Across open-ended healthcare tasks, Health-SCORE achieves evaluation quality comparable to human-created rubrics while significantly lowering development effort, making rubric-based evaluation and training more scalable.

</details>


### [67] [Conditioned Generative Modeling of Molecular Glues: A Realistic AI Approach for Synthesizable Drug-like Molecules](https://arxiv.org/abs/2601.18716)
*Naeyma N. Islam,Thomas R. Caulfield*

Main category: cs.AI

TL;DR: 提出AI辅助药物设计方法，用E3连接酶定向分子胶促进β - 淀粉样蛋白42（Abeta - 42）降解，模型能生成有效分子胶，为神经退行性疾病治疗提供框架。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病中Abeta - 42病理性积累致突触功能障碍和神经退化，细胞内Abeta - 42是疾病进展早期有毒驱动因素，需新治疗方法。

Method: 采用基于结构建模、ADMET筛选和对接评估Abeta - 42与三种E3连接酶三元复合物形成潜力；开发Ligase - Conditioned Junction Tree Variational Autoencoder生成连接酶特异性小分子。

Result: 生成的模型能产生化学有效、新颖且靶向特定的分子胶，可促进Abeta - 42降解。

Conclusion: 该综合方法为神经退行性疾病的UPS靶向治疗设计提供了有前景的框架。

Abstract: Alzheimer's disease (AD) is marked by the pathological accumulation of amyloid beta-42 (Abeta-42), contributing to synaptic dysfunction and neurodegeneration. While extracellular amyloid plaques are well-studied, increasing evidence highlights intracellular Abeta-42 as an early and toxic driver of disease progression. In this study, we present a novel, AI-assisted drug design approach to promote targeted degradation of Abeta-42 via the ubiquitin-proteasome system (UPS), using E3 ligase-directed molecular glues. We systematically evaluated the ternary complex formation potential of Abeta-42 with three E3 ligases: CRBN, VHL, and MDM2, through structure-based modeling, ADMET screening, and docking. We then developed a Ligase-Conditioned Junction Tree Variational Autoencoder (LC-JT-VAE) to generate ligase-specific small molecules, incorporating protein sequence embeddings and torsional angle-aware molecular graphs. Our results demonstrate that this generative model can produce chemically valid, novel, and target-specific molecular glues capable of facilitating Abeta-42 degradation. This integrated approach offers a promising framework for designing UPS-targeted therapies for neurodegenerative diseases.

</details>


### [68] [Why Keep Your Doubts to Yourself? Trading Visual Uncertainties in Multi-Agent Bandit Systems](https://arxiv.org/abs/2601.18735)
*Jusheng Zhang,Yijia Fan,Kaitong Cai,Jing Yang,Jiawei Yao,Jian Wang,Guanlong Qu,Ziliang Chen,Keze Wang*

Main category: cs.AI

TL;DR: 现有视觉语言模型多智能体系统扩展成本高且协调欠佳，本文提出Agora框架，实验表明其在多基准测试中表现优，确立市场协调为可行范式。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型多智能体系统扩展经济上不可持续，现有协调范式次优。

Method: 引入Agora框架，将协调视为不确定性的去中心化市场，把认知不确定性形式化为可交易资产，基于经济规则交易，用市场感知经纪人引导系统。

Result: 在五个多模态基准测试中，Agora优于强大的视觉语言模型和启发式多智能体策略，如在MMMU上准确率提升8.5%，成本降低超3倍。

Conclusion: 基于市场的协调是构建经济可行的多智能体视觉智能系统的原则性和可扩展范式。

Abstract: Vision-Language Models (VLMs) enable powerful multi-agent systems, but scaling them is economically unsustainable: coordinating heterogeneous agents under information asymmetry often spirals costs. Existing paradigms, such as Mixture-of-Agents and knowledge-based routers, rely on heuristic proxies that ignore costs and collapse uncertainty structure, leading to provably suboptimal coordination. We introduce Agora, a framework that reframes coordination as a decentralized market for uncertainty. Agora formalizes epistemic uncertainty into a structured, tradable asset (perceptual, semantic, inferential), and enforces profitability-driven trading among agents based on rational economic rules. A market-aware broker, extending Thompson Sampling, initiates collaboration and guides the system toward cost-efficient equilibria. Experiments on five multimodal benchmarks (MMMU, MMBench, MathVision, InfoVQA, CC-OCR) show that Agora outperforms strong VLMs and heuristic multi-agent strategies, e.g., achieving +8.5% accuracy over the best baseline on MMMU while reducing cost by over 3x. These results establish market-based coordination as a principled and scalable paradigm for building economically viable multi-agent visual intelligence systems.

</details>


### [69] [TSRBench: A Comprehensive Multi-task Multi-modal Time Series Reasoning Benchmark for Generalist Models](https://arxiv.org/abs/2601.18744)
*Fangxu Yu,Xingang Guo,Lingzhi Yuan,Haoqiang Kang,Hongyu Zhao,Lianhui Qin,Furong Huang,Bin Hu,Tianyi Zhou*

Main category: cs.AI

TL;DR: 引入TSRBench基准测试时间序列推理能力，评估超30个模型，揭示模型在时间序列推理上的问题，提供标准化评估平台。


<details>
  <summary>Details</summary>
Motivation: 现有通用模型基准测试缺少时间序列推理维度，为填补此空白。

Method: 引入TSRBench，包含4125个来自14个领域的问题，分为4大维度、15个任务，用其评估30多个模型。

Result: 扩展定律在感知和推理适用但预测不适用；强推理不保证准确预测；当前多模态模型无法有效融合文本和视觉表征。

Conclusion: TSRBench提供标准化评估平台，指出挑战并为通用模型发展提供见解。

Abstract: Time series data is ubiquitous in real-world scenarios and crucial for critical applications ranging from energy management to traffic control. Consequently, the ability to reason over time series is a fundamental skill for generalist models to solve practical problems. However, this dimension is notably absent from existing benchmarks of generalist models. To bridge this gap, we introduce TSRBench, a comprehensive multi-modal benchmark designed to stress-test the full spectrum of time series reasoning capabilities. TSRBench features: i) a diverse set of 4125 problems from 14 domains, and is categorized into 4 major dimensions: Perception, Reasoning, Prediction, and Decision-Making. ii) 15 tasks from the 4 dimensions evaluating essential reasoning capabilities (e.g., numerical reasoning). Through extensive experiments, we evaluated over 30 leading proprietary and open-source LLMs, VLMs, and TSLLMs within TSRBench. Our findings reveal that: i) scaling laws hold for perception and reasoning but break down for prediction; ii) strong reasoning does not guarantee accurate context-aware forecasting, indicating a decoupling between semantic understanding and numerical prediction; and iii) despite the complementary nature of textual and visual represenations of time series as inputs, current multimodal models fail to effectively fuse them for reciprocal performance gains. TSRBench provides a standardized evaluation platform that not only highlights existing challenges but also offers valuable insights to advance generalist models. Our code and dataset are available at https://tsrbench.github.io/.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [70] [Dynamic brittle fracture using Lip-field approach in an explicit dynamics context](https://arxiv.org/abs/2601.17365)
*Rajasekar Gopalsamy,Nicolas Chevaugeon*

Main category: cs.CE

TL;DR: 本文研究高应变率下材料断裂的动态响应，将Lip - field方法拓展到二维动态断裂，并用显式交错格式计算，通过数值研究评估模型动态行为。


<details>
  <summary>Details</summary>
Motivation: 研究高应变率下材料体断裂的动态响应，解决软化损伤模型中无正则化造成的伪应变局部化问题。

Method: 采用Lip - field方法将长度尺度引入软化损伤模型，拓展其在二维动态断裂中的应用，利用显式交错格式确定位移和损伤场，进行二维数值研究。

Result: 未提及具体研究结果。

Conclusion: 未提及明确结论。

Abstract: This paper aims to investigate the dynamic response of a material body undergoing fracture subjected to high strain rate loading conditions such as impact or explosion. In particular, our focus is limited to softening elastic damage models using Lip-field regularization. The Lip-field approach is a new technique to introduce length scale into the softening damage models. It was first presented for the quasi-static case and then extended to a one-dimensional dynamic case. This paper extends the application of the Lip-field approach to dynamic fracture in two-dimensional cases. Lip-field approach is a variational approach, in which the potential to be minimized is a non-regularized one. A potential without regularization can result in spurious strain localization, but the Lip-field approach resolves this issue by imposing Lipschitz constraints on the damage field. Our focus is limited to utilizing an explicit staggered scheme to determine the displacement and damage fields. Numerical studies are conducted for two-dimensional cases to assess the dynamic behavior of the proposed model.

</details>


### [71] [Gradient-Informed Machine Learning in Electromagnetics](https://arxiv.org/abs/2601.18300)
*Matteo Zorzetto,Merle Backmeyer,Michael Wiesheu,Riccardo Torchio,Fabrizio Dughiero,Sebastian Schöps*

Main category: cs.CE

TL;DR: 结合等几何分析、本征正交分解和高斯过程回归构建永磁同步电机参数化非线性模型的非侵入式代理模型，利用等几何分析可高效提取参数灵敏度用于梯度增强代理建模。


<details>
  <summary>Details</summary>
Motivation: 有限元等模拟技术计算成本高，投影模型降阶技术用于非线性或非仿射参数化模型有挑战。

Method: 将等几何分析与本征正交分解和高斯过程回归结合，利用等几何分析可微分性质提取参数灵敏度用于梯度增强代理建模。

Result: 未提及

Conclusion: 未提及

Abstract: Simulation techniques such as the finite element method are essential for designing electrical devices, but their computational cost can be prohibitive for repeated or real-time computations. Projection-based model order reduction techniques mitigate this by reducing the model size and complexity, yet face challenges when extended to nonlinear or non-affine parametric models. In this work, Isogeometric Analysis (IGA) is combined with proper orthogonal decomposition and Gaussian process regression to construct a non-intrusive surrogate model of a parametric nonlinear model of a permanent magnet synchronous machine. The differentiable nature of IGA allows for computationally efficient extraction of parametric sensitivities, which are leveraged for gradient-enhanced surrogate modeling.

</details>


### [72] [Uncertainty Quantification in Calibration and Simulation of Thermo-Chemical Curing of Epoxy Resins](https://arxiv.org/abs/2601.18359)
*Jendrik-Alexander Tröger,Christina Steinweller,Stefan Hartmann*

Main category: cs.CE

TL;DR: 研究了环氧树脂固化过程多步校准和数值模拟中不确定性的传播，采用一阶二阶矩方法并评估其效果。


<details>
  <summary>Details</summary>
Motivation: 环氧树脂固化是热 - 化学 - 力学过程，建模和参数校准存在不确定性传播问题，需研究其传播情况。

Method: 采用一阶二阶矩方法研究不确定性传播，并通过覆盖测试和与蒙特卡罗方法对比来评估该方法。

Result: 一阶二阶矩方法能有效得出合理结果，但只是高度非线性随机模型响应的一阶近似。

Conclusion: 一阶二阶矩方法在研究环氧树脂固化过程不确定性传播上是有效的。

Abstract: Curing of epoxy resins poses a particular challenge in terms of modeling, experimental investigation, and numerical implementation, as it is a thermo-chemo-mechanical process. Several constitutive relations are required to model these processes, yielding numerous material parameters. The calibration of the constitutive relations must be performed using multiple steps, wherein uncertainties unavoidably propagate. In this study, we investigate the propagation of uncertainties during both the multi-step calibration procedure and the numerical simulation of curing processes with the identified parameters. For both, we employ the first-order second-moment method, which is carefully evaluated through coverage tests and by comparing it to the Monte Carlo method as a reference. It is demonstrated that the first-order second-moment method efficiently yields reasonable results, although providing only a first-order approximation of the highly nonlinear stochastic model response.

</details>


### [73] [Regulatory Hub Discovery in MDD Methylome: Hypotheses for Molecular Subtypes via Computational Analysis](https://arxiv.org/abs/2601.18498)
*Mingyan Liu,Min Huang*

Main category: cs.CE

TL;DR: 对DNA甲基化数据进行两层计算分析，结合传统统计方法与机器学习辅助调控推断研究MDD。


<details>
  <summary>Details</summary>
Motivation: MDD是临床异质性综合征，传统EWAS基于差异甲基化幅度识别风险位点，效应大小排名可能无法充分捕捉生物网络中关键上游调控节点。

Method: 对DNA甲基化数据进行两层计算分析，结合传统统计方法与机器学习辅助调控推断。

Result: 未提及

Conclusion: 未提及

Abstract: Major Depressive Disorder (MDD) is a clinically heterogeneous syndrome with diverse etiological pathways. Traditional Epigenome-Wide Association Studies (EWAS) have successfully identified risk loci based on differential methylation magnitude. As a complementary perspective, effect-size-based ranking alone may not fully capture regulatory nodes that exhibit modest methylation changes but occupy critical upstream positions in biological networks. Here, we report findings and hypotheses from a two-tier computational analysis of DNA methylation data (GSE198904; \(n=206\) ), combining conventional statistical approaches with machine learning-assisted regulatory inference.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [74] [Context Lake: A System Class Defined by Decision Coherence](https://arxiv.org/abs/2601.17019)
*Xiaowei Jiang*

Main category: cs.DB

TL;DR: 传统数据系统无法满足AI代理需求，提出决策一致性定律，指出现有系统不满足要求，推导Context Lake系统类并明确其要求，为其奠定理论基础。


<details>
  <summary>Details</summary>
Motivation: 传统数据系统在AI代理持续运行做决策的场景下成为正确性瓶颈，多代理操作共享资源时易产生冲突，现有正确性保证失效。

Method: 提出决策一致性定律，证明组合不可能定理，基于此推导Context Lake系统类并明确其三个要求，形式化集体代理系统正确性所需条件。

Result: 证明现有系统类不满足决策一致性要求，推导Context Lake系统类及相关要求。

Conclusion: 为Context Lake奠定理论基础，指出现有架构失败原因，明确系统为AI代理大规模有效运行需提供的保证。

Abstract: AI agents are increasingly the primary consumers of data, operating continuously to make concurrent, irreversible decisions. Traditional data systems designed for human analysis cycles become correctness bottlenecks under this operating regime. When multiple agents operate over shared resources, their actions interact before reconciliation is possible. Correctness guarantees that apply after the decision window therefore fail to prevent conflicts. We introduce the Decision Coherence Law: for agents that take irreversible actions whose effects interact, correctness requires that interacting decisions be evaluated against a coherent representation of reality at the moment they are made. We show that no existing system class satisfies this requirement and prove through the Composition Impossibility Theorem that independently advancing systems cannot be composed to provide Decision Coherence while preserving their native system classes. From this impossibility result, we derive Context Lake as a necessary system class with three requirements: (1) semantic operations as native capabilities, (2) transactional consistency over all decision-relevant state, and (3) operational envelopes bounding staleness and degradation under load. We formalize the architectural invariants, enforcement boundaries, and admissibility conditions required for correctness in collective agent systems. This position paper establishes the theoretical foundation for Context Lakes, identifies why existing architectures fail, and specifies what systems must guarantee for AI agents to operate constructively at scale.

</details>


### [75] [Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMs](https://arxiv.org/abs/2601.17058)
*Wei Zhou,Jun Zhou,Haoyu Wang,Zhenghao Li,Qikang He,Shaokun Han,Guoliang Li,Xuanhe Zhou,Yeye He,Chunwei Liu,Zirui Tang,Bin Wang,Shen Tang,Kai Zuo,Yuyu Luo,Zhenzhe Zheng,Conghui He,Jingren Zhou,Fan Wu*

Main category: cs.DB

TL;DR: 本文对使用大语言模型（LLM）技术进行数据准备的研究进行系统综述，涉及范式转变、任务分类、技术优缺点、数据集和评估指标，还探讨了研究挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 应用对数据准备的需求增加、LLM技术发展和灵活构建代理的基础设施出现，使得LLM增强方法成为数据准备的重要范式。

Method: 研究数百篇文献，介绍范式转变，提出任务分类，调查各任务代表性技术，分析数据集和评估指标。

Result: 明确范式从基于规则、特定模型的管道转变为基于提示、上下文感知的工作流，梳理了数据清理、集成和丰富三大任务的技术优缺点，分析了常用数据集和评估指标。

Conclusion: 指出可扩展的LLM - 数据系统、可靠代理工作流的原则设计和稳健评估协议是未来研究方向。

Abstract: Data preparation aims to denoise raw datasets, uncover cross-dataset relationships, and extract valuable insights from them, which is essential for a wide range of data-centric applications. Driven by (i) rising demands for application-ready data (e.g., for analytics, visualization, decision-making), (ii) increasingly powerful LLM techniques, and (iii) the emergence of infrastructures that facilitate flexible agent construction (e.g., using Databricks Unity Catalog), LLM-enhanced methods are rapidly becoming a transformative and potentially dominant paradigm for data preparation.
  By investigating hundreds of recent literature works, this paper presents a systematic review of this evolving landscape, focusing on the use of LLM techniques to prepare data for diverse downstream tasks. First, we characterize the fundamental paradigm shift, from rule-based, model-specific pipelines to prompt-driven, context-aware, and agentic preparation workflows. Next, we introduce a task-centric taxonomy that organizes the field into three major tasks: data cleaning (e.g., standardization, error processing, imputation), data integration (e.g., entity matching, schema matching), and data enrichment (e.g., data annotation, profiling). For each task, we survey representative techniques, and highlight their respective strengths (e.g., improved generalization, semantic understanding) and limitations (e.g., the prohibitive cost of scaling LLMs, persistent hallucinations even in advanced agents, the mismatch between advanced methods and weak evaluation). Moreover, we analyze commonly used datasets and evaluation metrics (the empirical part). Finally, we discuss open research challenges and outline a forward-looking roadmap that emphasizes scalable LLM-data systems, principled designs for reliable agentic workflows, and robust evaluation protocols.

</details>


### [76] [Vidformer: Drop-in Declarative Optimization for Rendering Video-Native Query Results](https://arxiv.org/abs/2601.17221)
*Dominik Winecki,Arnab Nandi*

Main category: cs.DB

TL;DR: 提出Vidformer加速视频原生查询渲染，减少渲染时间和播放延迟，还支持基于LLM的对话式查询。


<details>
  <summary>Details</summary>
Motivation: 现有视频原生查询的渲染步骤存在瓶颈，使用后处理脚本慢，导致用户等待时间长。

Method: 提出Vidformer，将现有可视化代码转换为声明式表示，优化并行渲染，通过视频点播协议即时分段渲染视频。

Result: Vidformer在不同标注工作负载中使全渲染时间缩短2 - 3倍，播放时间降至0.25 - 0.5秒，有400倍的提升。

Conclusion: Vidformer解耦了剪辑长度和首帧播放延迟，实现亚秒级延迟的交互式视频原生查询，还支持基于LLM的对话式查询。

Abstract: When interactively exploring video data, video-native querying involves consuming query results as videos, including steps such as compilation of extracted video clips or data overlays. These video-native queries are bottlenecked by rendering, not the execution of the underlying queries. This rendering is currently performed using post-processing scripts that are often slow. This step poses a critical point of friction in interactive video data workloads: even short clips contain thousands of high-definition frames; conventional OpenCV/Python scripts must decode -> transform -> encode the entire data stream before a single pixel appears, leaving users waiting for many seconds, minutes, or hours.
  To address these issues, we present Vidformer, a drop-in rendering accelerator for video-native querying which, (i) transparently lifts existing visualization code into a declarative representation, (ii) transparently optimizes and parallelizes rendering, and (iii) instantly serves videos through a Video on Demand protocol with just-in-time segment rendering. We demonstrate that Vidformer cuts full-render time by 2-3x across diverse annotation workloads, and, more critically, drops time-to-playback to 0.25-0.5s. This represents a 400x improvement that decouples clip length from first-frame playback latency, and unlocks the ability to perform interactive video-native querying with sub-second latencies. Furthermore, we show how our approach enables interactive video-native LLM-based conversational querying as well.

</details>


### [77] [Constant-time Connectivity and 2-Edge Connectivity Querying in Dynamic Graphs](https://arxiv.org/abs/2601.17285)
*Lantian Xu,Junhua Zhang,Dong Wen,Lu Qin,Ying Zhang,Xuemin Lin*

Main category: cs.DB

TL;DR: 本文研究全动态图中的连通性查询处理，提出结合两种树的新解决方案，实现常数查询时间复杂度，扩展算法维护2 - 边连通性，在真实大数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 考虑到真实图应用中频繁的边更新，研究全动态图中的连通性查询处理。

Method: 提出一种基于生成树的新解决方案，同时维护不相交集树，结合两种树的优势；并将连通性维护算法扩展到维护2 - 边连通性。

Result: 实现了常数查询时间复杂度，显著提高了边插入和删除的理论运行时间；在真实大数据集上的性能研究显示算法有显著改进。

Conclusion: 所提出的算法在全动态图的连通性查询处理及2 - 边连通性维护方面具有高效性和实用性。

Abstract: Connectivity query processing is a fundamental problem in graph processing. Given an undirected graph and two query vertices, the problem aims to identify whether they are connected via a path. Given frequent edge updates in real graph applications, in this paper, we study connectivity query processing in fully dynamic graphs, where edges are frequently inserted or deleted. A recent solution, called D-tree, maintains a spanning tree for each connected component and applies several heuristics to reduce the depth of the tree. To improve efficiency, we propose a new spanning-tree-based solution by maintaining a disjoint-set tree simultaneously. By combining the advantages of two trees, we achieve the constant query time complexity and also significantly improve the theoretical running time in both edge insertion and edge deletion. In addition, we extend our connectivity maintenance algorithms to maintain 2-edge connectivity. Our performance studies on real large datasets show considerable improvement of our algorithms.

</details>


### [78] [UTune: Towards Uncertainty-Aware Online Index Tuning](https://arxiv.org/abs/2601.18199)
*Chenning Wu,Sifan Chen,Wentao Wu,Yinan Jing,Zhenying He,Kai Zhang,X. Sean Wang*

Main category: cs.DB

TL;DR: 提出不确定性感知在线索引调优框架UTune，解决现有学习型索引收益估计器在在线调优中的问题，实验显示有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有学习型索引收益估计器在在线索引调优场景存在局限性，面临查询执行反馈少和新查询不断出现的挑战，影响泛化能力。

Method: 提出UTune框架，采用算子级学习模型，核心是不确定性量化机制，还将不确定性信息融入索引选择和配置枚举，开发带不确定性加权索引收益的ε - 贪心搜索策略变体。

Result: 与现有在线索引调优器相比，UTune显著改善了工作负载执行时间，减少了索引探索开销，在工作负载相对稳定时收敛更快。

Conclusion: UTune能有效克服现有学习型索引收益估计器在在线调优中的问题，提升性能。

Abstract: There have been a flurry of recent proposals on learned benefit estimators for index tuning. Although these learned estimators show promising improvement over what-if query optimizer calls in terms of the accuracy of estimated index benefit, they face significant limitations when applied to online index tuning, an arguably more common and more challenging scenario in real-world applications. There are two major challenges for learned index benefit estimators in online tuning: (1) limited amount of query execution feedback that can be used to train the models, and (2) constant coming of new unseen queries due to workload drifts. The combination of the two hinders the generalization capability of existing learned index benefit estimators. To overcome these challenges, we present UTune, an uncertainty-aware online index tuning framework that employs operator-level learned models with improved generalization over unseen queries. At the core of UTune is an uncertainty quantification mechanism that characterizes the inherent uncertainty of the operator-level learned models given limited online execution feedback. We further integrate uncertainty information into index selection and configuration enumeration, the key component of any index tuner, by developing a new variant of the classic $ε$-greedy search strategy with uncertainty-weighted index benefits. Experimental evaluation shows that UTune not only significantly improves the workload execution time compared to state-of-the-art online index tuners but also reduces the index exploration overhead, resulting in faster convergence when the workload is relatively stable.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [79] [Communication-Avoiding Linear Algebraic Kernel K-Means on GPUs](https://arxiv.org/abs/2601.17136)
*Julian Bellavita,Matthew Rubino,Nakul Iyer,Andrew Chang,Aditya Devarakonda,Flavio Vella,Giulia Guidi*

Main category: cs.DC

TL;DR: 提出多GPU系统上大规模核K均值聚类的分布式内存并行算法，1.5D算法性能最佳，能处理更大规模数据，证明特定线性代数公式设计的分布式算法可提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有核K均值聚类在处理大规模数据时因GPU内存限制存在不足，需解决大规模数据聚类问题。

Method: 提出分布式内存并行算法，将核K均值计算昂贵组件映射到通信高效的分布式线性代数原语，设计分区方案。

Result: 1.5D算法性能最佳，能处理比以前大1 - 2个数量级的数据，在256个GPU上有良好的弱缩放效率和强缩放加速比，相比1D算法有显著加速。

Conclusion: 针对特定应用设计的分布式算法结合线性代数公式能大幅提升性能。

Abstract: Clustering is an important tool in data analysis, with K-means being popular for its simplicity and versatility. However, it cannot handle non-linearly separable clusters. Kernel K-means addresses this limitation but requires a large kernel matrix, making it computationally and memory intensive. Prior work has accelerated Kernel K-means by formulating it using sparse linear algebra primitives and implementing it on a single GPU. However, that approach cannot run on datasets with more than approximately 80,000 samples due to limited GPU memory.
  In this work, we address this issue by presenting a suite of distributed-memory parallel algorithms for large-scale Kernel K-means clustering on multi-GPU systems. Our approach maps the most computationally expensive components of Kernel K-means onto communication-efficient distributed linear algebra primitives uniquely tailored for Kernel K-means, enabling highly scalable implementations that efficiently cluster million-scale datasets. Central to our work is the design of partitioning schemes that enable communication-efficient composition of the linear algebra primitives that appear in Kernel K-means.
  Our 1.5D algorithm consistently achieves the highest performance, enabling Kernel K-means to scale to data one to two orders of magnitude larger than previously practical. On 256 GPUs, it achieves a geometric mean weak scaling efficiency of $79.7\%$ and a geometric mean strong scaling speedup of $4.2\times$. Compared to our 1D algorithm, the 1.5D approach achieves up to a $3.6\times$ speedup on 256 GPUs and reduces clustering time from over an hour to under two seconds relative to a single-GPU sliding window implementation. Our results show that distributed algorithms designed with application-specific linear algebraic formulations can achieve substantial performance improvement.

</details>


### [80] [Push Down Optimization for Distributed Multi Cloud Data Integration](https://arxiv.org/abs/2601.17546)
*Ravi Kiran Kodali,Vinoth Punniyamoorthy,Akash Kumar Agarwal,Bikesh Kumar,Balakrishna Pothineni,Aswathnarayan Muthukrishnan Kirubakaran,Sumit Saha,Nachiappan Chockalingam*

Main category: cs.DC

TL;DR: 本文探讨多云ETL管道中下推优化的可行性，评估相关技术，案例研究显示有收益，为组织提供提升ETL可扩展性和可靠性策略。


<details>
  <summary>Details</summary>
Motivation: 企业采用多云架构，ETL管道需处理大数据集并降低延迟和传输成本，单云有效的下推优化在多云应用面临挑战，因此研究其在多云ETL管道中的可行性。

Method: 考察本地化下推、混合模型和数据联合技术，进行跨Redshift和BigQuery的案例研究。

Result: 案例研究显示有可衡量的收益，包括更低的端到端运行时间、减少的传输量和提高的成本效率。

Conclusion: 提出组织在分布式云环境中可采用的提升ETL可扩展性和可靠性的实用策略。

Abstract: Enterprises increasingly adopt multi cloud architectures to take advantage of diverse database engines, regional availability, and cost models. In these environments, ETL pipelines must process large, distributed datasets while minimizing latency and transfer cost. Push down optimization, which executes transformation logic within database engines rather than within the ETL tool, has proven highly effective in single cloud systems. However, when applied across multiple clouds, it faces challenges related to data movement, heterogeneous SQL engines, orchestration complexity, and fragmented security controls. This paper examines the feasibility of push down optimization in multi cloud ETL pipelines and analyzes its benefits and limitations. It evaluates localized push down, hybrid models, and data federation techniques that reduce cross cloud traffic while improving performance. A case study across Redshift and BigQuery demonstrates measurable gains, including lower end to end runtime, reduced transfer volume, and improved cost efficiency. The study highlights practical strategies that organizations can adopt to improve ETL scalability and reliability in distributed cloud environments.

</details>


### [81] [On the Bandwidth Consumption of Blockchains](https://arxiv.org/abs/2601.18400)
*Andrei Lebedev,Vincent Gramoli*

Main category: cs.DC

TL;DR: 本文首次对区块链带宽消耗进行实证比较，测量五种区块链协议节点网络流量，得出传输协议等对流量的影响结论。


<details>
  <summary>Details</summary>
Motivation: 区块链提案增多使节点托管成本增加，缺乏对区块链带宽消耗的比较研究。

Method: 测量Algorand、Aptos、Avalanche、Redbelly和Solana五种区块链协议网络节点的网络流量，研究流量随时间的变化、区分收发流量，并分析其随节点和验证者数量的变化。

Result: 未明确提及具体数据结果。

Conclusion: 传输协议是影响网络流量的主要因素，分离节点角色有助于减少流量，不同区块链受网络规模的影响不同。

Abstract: With the advent of blockchain technology, the number of proposals has boomed. The network traffic imposed by these blockchain proposals increases the cost of hosting nodes. Unfortunately, as of today, we are not aware of any comparative study of the bandwidth consumption of blockchains.
  In this paper, we propose the first empirical comparison of blockchain bandwidth consumption. To this end, we measure the network traffic of blockchain network nodes of five blockchain protocols: Algorand, Aptos, Avalanche, Redbelly and Solana. We study the variation over time, differentiate the receiving and sending traffic and analyze how this traffic varies with the number of nodes and validators.
  We conclude that the transport protocol is the main factor impacting the network traffic, segregating node roles helps reduce traffic and different blockchains are differently impacted by the network size.

</details>


### [82] [A Unified Approach to Concurrent, Parallel Map-Reduce in R using Futures](https://arxiv.org/abs/2601.17578)
*Henrik Bengtsson*

Main category: cs.DC

TL;DR: R生态系统有多种map - reduce API，但并行化代码需学习多个不兼容的并行API，futurize包提供单一函数解决此问题。


<details>
  <summary>Details</summary>
Motivation: R生态系统中并行化代码需学习多个不兼容的并行API，使用不便，需要简化。

Method: 通过futurize()函数将顺序map - reduce表达式转换为future生态系统中的并行等效表达式，利用R的原生管道操作符，支持多种map - reduce函数和特定领域包。

Result: 开发者可通过futurize()声明并行内容，终端用户可通过plan()选择并行方式。

Conclusion: futurize包简化了R中的并行计算，文章介绍其理念、设计、实现并展示用法。

Abstract: The R ecosystem offers a rich variety of map-reduce application programming interfaces (APIs) for iterative computations, yet parallelizing code across these diverse frameworks requires learning multiple, often incompatible, parallel APIs. The futurize package addresses this challenge by providing a single function, futurize(), which transpiles sequential map-reduce expressions into their parallel equivalents in the future ecosystem, which performs all the heavy lifting. By leveraging R's native pipe operator, users can parallelize existing code with minimal refactoring -- often by simply appending `|> futurize()' to an expression. The package supports classical map-reduce functions from base R, purrr, crossmap, foreach, plyr, BiocParallel, e.g., lapply(xs, fcn) |> futurize() and map(xs, fcn) |> futurize(), as well as a growing set of domain-specific packages, e.g., boot, caret, glmnet, lme4, mgcv, and tm. By abstracting away the underlying parallel machinery, and unifying handling of future options, the package enables developers to declare what to parallelize via futurize(), and end-users to choose how via plan(). This article describes the philosophy, design, and implementation of futurize, demonstrates its usage across various map-reduce paradigms, and discusses its role in simplifying parallel computing in R.

</details>


### [83] [Lightspeed Data Compute for the Space Era](https://arxiv.org/abs/2601.17589)
*Thomas Sandholm,Bernardo A. Huberman,Klas Segeljakt,Paris Carbone*

Main category: cs.DC

TL;DR: 提出适用于低地球轨道卫星网状网络的SpaceCoMP处理模型，模拟显示有显著性能提升，表明轨道网状网络可用于更快的数据处理。


<details>
  <summary>Details</summary>
Motivation: 解决卫星数据下行带宽不足，数据无法有效传输到地面的问题。

Method: 提出SpaceCoMP处理模型，利用空间物理加速计算，采用距离感知路由协议和二分匹配调度策略。

Result: 模拟1000 - 10000颗卫星星座，地图放置效率比基线提高61 - 79%，比贪婪分配提高18 - 28%，聚合成本降低67 - 72%。

Conclusion: 轨道网状网络不仅可作通信中继，还能为高空更快的数据处理奠定基础。

Abstract: While thousands of satellites photograph Earth every day, most of that data never makes it to the ground because downlink bandwidth simply cannot keep up. Processing data in the Low Earth Orbit (LEO) zone offers promising capabilities to overcome this limitation. We propose SpaceCoMP, a MapReduce-inspired processing model for LEO satellite mesh networks. Ground stations submit queries over an area of interest; satellites collect sensor data, process it cooperatively at light-speed using inter-satellite laser links, and return only the results. Our compute model leverages space physics to accelerate computations on LEO megaconstellations. Our distance-aware routing protocol exploits orbital geometry. In addition, our bipartite match scheduling strategy places map and reduce tasks within orbital regions while minimizing aggregation costs. We have simulated constellations of 1,000-10,000 satellites showcasing 61-79% improvement in map placement efficiency over baselines, 18-28% over greedy allocation, and 67-72% reduction in aggregation cost. SpaceCoMP demonstrates that the orbital mesh is not merely useful as a communication relay, as seen today, but can provide the foundations for faster data processing above the skies.

</details>


### [84] [Scaling All-to-all Operations Across Emerging Many-Core Supercomputers](https://arxiv.org/abs/2601.17606)
*Shannon Kinkead,Jackson Wesley,Whit Schonbein,David DeBonis,Matthew G. F. Dosanjh,Amanda Bienz*

Main category: cs.DC

TL;DR: 本文为新兴多核系统提出新型全对全算法，相比现有算法和系统 MPI 有性能分析，在 32 节点系统上加速比达 3 倍。


<details>
  <summary>Details</summary>
Motivation: MPI 中高性能全对全集体操作对多种应用至关重要，现有实现性能受多因素影响，需更好算法。

Method: 提出适用于新兴多核系统的新型全对全算法。

Result: 新型算法在 32 节点的先进 Sapphire Rapids 系统上比系统 MPI 实现了高达 3 倍的加速。

Conclusion: 所提出的新型全对全算法在新兴多核系统上有更好的性能表现。

Abstract: Performant all-to-all collective operations in MPI are critical to fast Fourier transforms, transposition, and machine learning applications. There are many existing implementations for all-to-all exchanges on emerging systems, with the achieved performance dependent on many factors, including message size, process count, architecture, and parallel system partition. This paper presents novel all-to-all algorithms for emerging many-core systems. Further, the paper presents a performance analysis against existing algorithms and system MPI, with novel algorithms achieving up to 3x speedup over system MPI at 32 nodes of state-of-the-art Sapphire Rapids systems.

</details>


### [85] [Multi-core & GPU-based Balanced Butterfly Counting in Signed Bipartite Graphs](https://arxiv.org/abs/2601.17707)
*Mekala Kiran,Apurba Das,Suman Banerjee,Tathagata Ray*

Main category: cs.DC

TL;DR: 提出多核CPU和GPU上平衡蝴蝶计数的并行实现，实验显示算法有高可扩展性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有平衡蝴蝶计数方法在大规模图上计算成本高，需高效并行方法。

Method: 提出多核算法M - BBC利用细粒度顶点级并行加速计数并消除不平衡子结构；开发GPU方法G - BBC用基于瓦片的并行方法，改进版G - BBC++集成动态调度。

Result: M - BBC比顺序基线BB2K最高加速71.13x；GPU算法比BB2K最高加速13320x，比M - BBC最高快186x。

Conclusion: 并行算法有高可扩展性和效率，为大规模二部图上的高性能有符号 motif 分析奠定基础。

Abstract: Balanced butterfly counting, corresponding to counting balanced (2, 2)-bicliques, is a fundamental primitive in the analysis of signed bipartite graphs and provides a basis for studying higher-order structural properties such as clustering coefficients and community structure. Although prior work has proposed an efficient CPU-based serial method for counting balanced (2, k)-bicliques. The computational cost of balanced butterfly counting remains a major bottleneck on large-scale graphs. In this work, we present the highly parallel implementations for balanced butterfly counting for both multicore CPUs and GPUs. The proposed multi-core algorithm (M-BBC) employs fine-grained vertex-level parallelism to accelerate wedge-based counting while eliminating the generation of unbalanced substructures. To improve scalability, we develop a GPU-based method (G-BBC) that uses a tile-based parallel approach to effectively leverage shared memory while handling large vertex sets. We then present an improved variation, G-BBC++, which integrates dynamic scheduling to mitigate workload imbalance and maximize throughput. We conduct an experimental assessment of the proposed methods across 15 real-world datasets. Experimental results exhibit that M-BBC achieves speedups of up to 71.13x (average 38.13x) over the sequential baseline BB2K. The GPU-based algorithms deliver even greater improvements, achieving up to 13,320x speedup (average 2,600x) over BB2K and outperforming M-BBC by up to 186x (average 50x). These results indicate the substantial scalability and efficiency of our parallel algorithms and establish a robust foundation for high-performance signed motif analysis on massive bipartite graphs.

</details>


### [86] [An MLIR Lowering Pipeline for Stencils at Wafer-Scale](https://arxiv.org/abs/2601.17754)
*Nicolai Stawinoga,David Katz,Anton Lydike,Justs Zarins,Nick Brown,George Bisbas,Tobias Grosser*

Main category: cs.DC

TL;DR: 本文探索利用编译器借助模板特定领域信息自动适配Cerebras WSE，无需应用层代码更改，经测试性能良好。


<details>
  <summary>Details</summary>
Motivation: Cerebras WSE分布式异步编程模型与传统程序差异大，移植代码需定制重写，且编译器缺乏支持，难以自动化，因此探索利用模板领域信息自动适配。

Method: 提出编译器流水线，将基于模板的内核转换为针对WSE高度优化的CSL代码。

Result: 基于五个跨三种HPC编程技术的基准测试，性能与手动优化代码相当甚至略好，在WSE3上比128个Nvidia A100 GPU快约14倍，比128节点基于CPU的Cray - EX超级计算机快20倍。

Conclusion: 利用模板特定领域信息可让编译器自动适配WSE，且无需应用层代码更改，能获得良好性能。

Abstract: The Cerebras Wafer-Scale Engine (WSE) delivers performance at an unprecedented scale of over 900,000 compute units, all connected via a single-wafer on-chip interconnect. Initially designed for AI, the WSE architecture is also well-suited for High Performance Computing (HPC). However, its distributed asynchronous programming model diverges significantly from the simple sequential or bulk-synchronous programs that one would typically derive for a given mathematical program description. Targeting the WSE requires a bespoke re-implementation when porting existing code. The absence of WSE support in compilers such as MLIR, meant that there was little hope for automating this process.
  Stencils are ubiquitous in HPC, and in this paper we explore the hypothesis that domain specific information about stencils can be leveraged by the compiler to automatically target the WSE without requiring application-level code changes. We present a compiler pipeline that transforms stencil-based kernels into highly optimized CSL code for the WSE, bridging the semantic gap between the mathematical representation of the problem and the WSE's asynchronous execution model. Based upon five benchmarks across three HPC programming technologies, running on both the Cerebras WSE2 and WSE3, our approach delivers comparable, if not slightly better, performance than manually optimized code. Furthermore, without requiring any application level code changes, performance on the WSE3 is around 14 times faster than 128 Nvidia A100 GPUs and 20 times faster than 128 nodes of a CPU-based Cray-EX supercomputer when using our approach.

</details>


### [87] [CondenseGraph: Communication-Efficient Distributed GNN Training via On-the-Fly Graph Condensation](https://arxiv.org/abs/2601.17774)
*Zizhao Zhang,Yihan Xue,Haotian Zhu,Sijia Li,Zhijun Wang,Yujie Xiao*

Main category: cs.DC

TL;DR: 提出CondenseGraph框架解决分布式GNN训练通信开销大问题，通过图凝聚和误差反馈机制减少通信量和训练时间。


<details>
  <summary>Details</summary>
Motivation: 分布式图神经网络训练因邻居爆炸问题存在严重通信瓶颈，现有静态图分区策略无法适应动态网络条件。

Method: 提出CondenseGraph框架，采用动态图凝聚机制压缩边界节点特征，开发基于梯度的误差反馈机制弥补信息损失。

Result: 在四个基准数据集上实验表明，CondenseGraph能达到与全精度基线相当的精度，显著降低通信成本和训练时间，减少通信量40 - 60%。

Conclusion: CondenseGraph框架有效减少分布式GNN训练的通信开销，具有较好的训练效果。

Abstract: Distributed Graph Neural Network (GNN) training suffers from substantial communication overhead due to the inherent neighborhood dependency in graph-structured data. This neighbor explosion problem requires workers to frequently exchange boundary node features across partitions, creating a communication bottleneck that severely limits training scalability. Existing approaches rely on static graph partitioning strategies that cannot adapt to dynamic network conditions. In this paper, we propose CondenseGraph, a novel communication-efficient framework for distributed GNN training. Our key innovation is an on-the-fly graph condensation mechanism that dynamically compresses boundary node features into compact super nodes before transmission. To compensate for the information loss introduced by compression, we develop a gradient-based error feedback mechanism that maintains convergence guarantees while reducing communication volume by 40-60%. Extensive experiments on four benchmark datasets demonstrate that CondenseGraph achieves comparable accuracy to full-precision baselines while significantly reducing communication costs and training time.

</details>


### [88] [A Universal Load Balancing Principle and Its Application to Large Language Model Serving](https://arxiv.org/abs/2601.17855)
*Zixi Chen,Tianci Bu,Chendong Song,Xin Lu,Yinyu Ye,Zijie Zhou*

Main category: cs.DC

TL;DR: 论文针对大语言模型服务中的负载均衡难题，提出通用负载均衡原则，实验验证其能提升吞吐量和延迟、降低能耗。


<details>
  <summary>Details</summary>
Motivation: 大语言模型服务出现瓶颈，在同步状态系统中，负载不均衡会产生拖后腿现象和大量空闲时间。

Method: 开发通用负载均衡原则，采用分步有限时间整数优化公式。

Result: 实验表明，该原则能提高吞吐量和延迟、降低能耗。

Conclusion: 该成果为负载均衡提供通用理论框架，对可持续大语言模型服务及其他同步资源分配问题有重要意义。

Abstract: Load balancing-the allocation of work across parallel resources to reduce delay, energy and cost-is a pervasive challenge in science and engineering, from large-scale simulation and data processing to cloud and manufacturing operations. Motivated by the emerging bottleneck in large language model (LLM) serving, we study a particularly stringent regime of load balancing that arises in barrier-synchronized, stateful systems: work cannot be freely migrated and progress is gated by the slowest participant at each step, so heterogeneity and temporal drift in workloads create persistent stragglers and substantial idle time. LLM serving under data-parallel decoding provides a prominent modern instance: in production traces, barrier-induced idle can exceed 40% of compute time per decode step. Here we develop a universal load-balancing principle, which admits a step-wise finite-horizon integer-optimization formulation and yields worst-case guarantees: across LLM decode models and a broader class of non-decreasing workload drift processes, it reduces long-run imbalance by a factor that grows with batch size and system scale. Extensive experiments corroborate the theory, showing substantial improvements in throughput and latency together with reductions in energy consumption. These results provide a general, theoretically grounded framework for load balancing, with immediate implications for sustainable LLM serving and broad relevance to other synchronization-gated resource-allocation problems.

</details>


### [89] [An Initial Evaluation of Distributed Graph Algorithms using NWGraph and HPX](https://arxiv.org/abs/2601.18158)
*Karame Mohammadiporshokooh,Panagiotis Syskakis,Hartmut Kaiser*

Main category: cs.DC

TL;DR: 现有分布式图框架有挑战，提出集成HPX运行时系统的NWGraph库分布式实现，用BFS和PageRank评估，BFS性能好，PageRank待优化。


<details>
  <summary>Details</summary>
Motivation: 现有分布式图框架面临算法受延迟限制、内存访问不规则、同步成本高导致可扩展性和效率受限的问题，需改进。

Method: 提出集成HPX运行时系统的NWGraph库分布式实现，利用HPX的异步多任务模型。

Result: BFS比分布式Boost Graph Library（BGL）性能好，PageRank当前实现未超过BGL。

Conclusion: 异步任务运行时应用于图处理有前景但也有挑战，指明未来优化和扩展方向。

Abstract: Graphs are central to modeling relationships in scientific computing, data analysis, and AI/ML, but their growing scale can exceed the memory and compute capacity of single nodes, requiring distributed solutions. Existing distributed graph framework, however, face fundamental challenges: graph algorithms are latency-bound, suffer from irregular memory access, and often impose synchronization costs that limit scalability and efficiency. In this work, we present a distributed implementation of the NWGraph library integrated with the HPX runtime system. By leveraging HPX's asynchronous many-task model, our approach aims to reduce synchronization overhead, improve load balance, and provide a foundation for distributed graph analytics. We evaluate this approach using two representative algorithms: Breadth-First-Search (BFS) and (PageRank). Our initial results show that BFS achieves better performance than the distributed Boost Graph Library (BGL), while PageRank remains more challenging, with current implementation not yet outperforming BGL. These findings highlight both the promise and the open challenges of applying asynchronous task-based runtimes to graph processing, and point to opportunities for future optimizations and extensions.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [90] [Parallel Algorithm For Finding The Minimum s/t Cut in a Structured 3-Dimensional Proper Order Graph](https://arxiv.org/abs/2601.17026)
*Shridharan Chandramouli*

Main category: cs.DS

TL;DR: 提出用于计算三维适当顺序图最小s - t割的并行算法，含两种并行方法，重点是推 - 重贴标签变体。


<details>
  <summary>Details</summary>
Motivation: 解决图像分割问题中结构化三维适当顺序图的最小s - t割计算。

Method: 开发了Boykov - Kolmogorov算法的分层合并变体和带级别同步全局重贴标签的并行推 - 重贴标签算法。

Result: 提出推 - 重贴标签变体，将图按列分割，无需全局共享队列，引入级别同步全局重贴标签。

Conclusion: 成功提出有效并行算法解决相关图的最小s - t割计算问题。

Abstract: We present a parallel algorithm for computing the minimum s-t cut in structured 3-dimensional proper order graphs arising from image segmentation problems. Proper order graphs are multi-column structures where vertices are arranged in parallel columns, with each vertex connected to consecutive vertices in adjacent columns. This graph structure naturally arises in surface extraction problems for geological horizon segmentation in seismic imaging volumes. We develop two parallel approaches: a hierarchical merging variant of the Boykov-Kolmogorov algorithm, and a novel parallel push-relabel algorithm with level synchronized global relabeling. Our primary contribution is the push-relabel variant, which partitions the graph into segments along columns with processor affinity, eliminating the need for a global shared queue. We introduce level synchronized global relabeling that enables concurrent label updates while maintaining correctness through barriers at each frontier level.

</details>


### [91] [Minimizing Completion Times of Stochastic Jobs on Parallel Machines is Hard](https://arxiv.org/abs/2601.17425)
*Benjamin Moseley,Kirk Pruhs,Marc Uetz,Rudy Zhou*

Main category: cs.DS

TL;DR: 本文研究随机作业在并行相同机器上的调度，以最小化预期总加权完成时间，给出硬度结果证明问题的固有难处理性。


<details>
  <summary>Details</summary>
Motivation: 现有近似算法仅在非常严格的输入分布假设下有常数因子性能保证，且缺乏对应复杂度结果，需解决此差距。

Method: 针对离散两点处理时间分布和单位权重的特殊情况，证明判断是否存在预期成本不超过给定阈值的调度策略是#P - 难的，还证明评估标准(W)SEPT贪心策略的预期目标值是#P - 难的。

Result: 得到了调度独立随机作业和最小和目标的硬度结果，且不依赖于底层确定性对应问题的难处理性。

Conclusion: 该问题具有固有难处理性。

Abstract: This paper considers the scheduling of stochastic jobs on parallel identical machines to minimize the expected total weighted completion time. While this is a classical problem with a significant body of research on approximation algorithms over the past two decades, constant-factor performance guarantees are currently known only under very restrictive assumptions on the input distributions, even when all job weights are identical. This algorithmic difficulty is striking given the lack of corresponding complexity results: to date, it is conceivable that the problem could be solved optimally in polynomial time.
  We address this gap with hardness results that demonstrate the problem's inherent intractability. For the special case of discrete two-point processing time distributions and unit weights, we prove that deciding whether there exists a scheduling policy with expected cost at most a given threshold is #P-hard. Furthermore, we show that evaluating the expected objective value of the standard (W)SEPT greedy policy is itself #P-hard. These represent the first hardness results for scheduling independent stochastic jobs and min-sum objective that do not merely rely on the intractability of the underlying deterministic counterparts.

</details>


### [92] [Split Algorithm in Linear Time for the Vehicle Routing Problem with Simultaneous Pickup and Delivery and Time Windows](https://arxiv.org/abs/2601.17572)
*Ethan Gibbons,Mario Ventresca,Beatrice M. Ombuki-Berman*

Main category: cs.DS

TL;DR: 本文提出线性Split算法的扩展，可同时处理VRPSPD和VRPTW两种变体，保证算法最优，还进行实验验证速度提升。


<details>
  <summary>Details</summary>
Motivation: 此前线性Split算法在扩展处理更多VRP变体方面进展有限，限制了算法通用性。

Method: 提出线性Split的扩展算法处理VRPSPD和VRPTW，对VRPSPD考虑容量惩罚，对VRPTW结合流行时间扭曲惩罚函数处理CVRP容量惩罚。 

Result: 提出的Θ(n)算法在节点间旅行时间满足三角不等式时保证最优，通过计算实验验证了线性Split较Θ(n²)算法有速度提升。

Conclusion: 线性Split的扩展算法能有效处理VRP的两种变体，且有速度优势。

Abstract: For many kinds of vehicle routing problems (VRPs), a popular heuristic approach involves constructing a Traveling Salesman Problem (TSP) solution, referred to as a long tour, then partitioning segments of the solution into routes for different vehicles with respect to problem constraints. Previously, a Split algorithm with a worst-case runtime of $Θ(n)$ was proposed for the capacitated VRP (CVRP) that finds the most cost-efficient partition of customers, given a long tour. This was an improvement over the previously fastest-known Split algorithm with a worst-case runtime of $Θ(n^2)$ that was based on Bellman's shortest path algorithm. While this linear Split has been an integral part of modern state-of-the-art CVRP approaches, little progress has been made in extending this algorithm to handle additional VRP variants, limiting the general applicability of the algorithm. In this work, we propose an extension of the linear Split that handles two cardinal VRP variants simultaneously: (i) simultaneous pickups and deliveries (VRPSPD) and (ii) time windows (VRPTW). The resulting $Θ(n)$ algorithm is guaranteed to be optimal, assuming travel times between nodes satisfy the triangle inequality. Additionally, we extend the linear Split to handle a capacity penalty for the VRPSPD. For the VRPTW, we extend the linear Split to handle the CVRP capacity penalty in conjunction with the popular time warp penalty function. Computational experiments are performed to empirically validate the speed gains of these linear Splits against their $Θ$($n^2$) counterparts.

</details>


### [93] [Sampling Sphere Packings with Continuum Glauber Dynamics](https://arxiv.org/abs/2601.18748)
*Aiya Kuchukova,Santosh Vempala,Daniel J. Zhang*

Main category: cs.DS

TL;DR: 在强空间混合假设下为硬球模型的连续格劳伯动力学建立谱隙，扩展参数范围，技术适用于一般吉布斯点过程，还提高了采样阈值。


<details>
  <summary>Details</summary>
Motivation: 扩展连续格劳伯动力学可证明快速混合的参数范围，改进固定数量球体堆积的采样阈值。

Method: 引入谱独立性和负场定位的连续扩展。

Result: 建立了硬球模型连续格劳伯动力学的谱隙，技术适用于一般吉布斯点过程。

Conclusion: 可扩展连续格劳伯动力学可证明快速混合的范围，并提高了采样阈值。

Abstract: We establish a spectral gap for Continuum Glauber dynamics on the hard sphere model assuming strong spatial mixing, thereby extending the range of parameters in which Continuum Glauber is provably rapidly mixing. To do this, we introduce continuous extensions of spectral independence and negative fields localization. Our techniques apply to general Gibbs point processes with finite-range repulsive pair potentials. As a corollary, we improve the threshold up to which packings of a fixed number of spheres can be sampled from a bounded domain.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [94] [Equilibrium Refinements Improve Subgame Solving in Imperfect-Information Games](https://arxiv.org/abs/2601.17131)
*Ondrej Kubicek,Viliam Lisy,Tuomas Sandholm*

Main category: cs.GT

TL;DR: 本文探讨不完美信息博弈中的子博弈求解，提出小工具博弈序贯均衡作为优选解概念并改进算法，实验显示其优于未改进的纳什均衡，可降剥削性超50%。


<details>
  <summary>Details</summary>
Motivation: 不完美信息博弈的子博弈求解需考虑隐藏状态和对手策略不确定性，小工具博弈在完整游戏中不同纳什均衡表现差异大。

Method: 提出小工具博弈序贯均衡，修改序列形式线性规划和反事实后悔最小化算法。

Result: 实验表明改进的均衡在基准游戏中始终优于未改进的纳什均衡，可降低整体策略剥削性超50%。

Conclusion: 小工具博弈序贯均衡及改进算法有效，能显著提升策略表现。

Abstract: Subgame solving is a technique for scaling algorithms to large games by locally refining a precomputed blueprint strategy during gameplay. While straightforward in perfect-information games where search starts from the current state, subgame solving in imperfect-information games must account for hidden states and uncertainty about the opponent's past strategy. Gadget games were developed to ensure that the improved subgame strategy is robust against any possible opponent's strategy in a zero-sum game. Gadget games typically contain infinitely many Nash equilibria. We demonstrate that while these equilibria are equivalent in the gadget game, they yield vastly different performance in the full game, even when facing a rational opponent. We propose gadget game sequential equilibria as the preferred solution concept. We introduce modifications to the sequence-form linear program and counterfactual regret minimization that converge to these refined solutions with only mild additional computational cost. Additionally, we provide several new insights into the surprising superiority of the resolving gadget game over the max-margin gadget game. Our experiments compare different Nash equilibria of gadget games in several standard benchmark games, showing that our refined equilibria consistently outperform unrefined Nash equilibria, and can reduce the exploitability of the overall strategy by more than 50%

</details>


### [95] [Strategic AI in Cournot Markets](https://arxiv.org/abs/2601.17263)
*Sanyukta Deshpande,Sheldon H. Jacobson*

Main category: cs.GT

TL;DR: 研究大语言模型在寡头古诺市场的多维度决策，发现其能把握市场动态、进行默契合谋，可通过监管主导主体恢复竞争定价并给出监管建议。


<details>
  <summary>Details</summary>
Motivation: 在人工智能自动化决策增多背景下，理解竞争市场动态并确保公平市场机制。

Method: 从决策类型、对手策略和市场构成三个维度分析大语言模型行为。

Result: 大语言模型能把握复杂市场动态、进行默契合谋使价格高于纳什均衡200%，监管主导主体可恢复竞争定价。

Conclusion: 识别出人工智能融入竞争市场的潜在问题，给出自动化时代监管政策建议。

Abstract: As artificial intelligence increasingly automates decision-making in competitive markets, understanding the resulting dynamics and ensuring fair market mechanisms is essential. We investigate the multi-faceted decision-making of large language models (LLMs) in oligopolistic Cournot markets, showing that LLMs not only grasp complex market dynamics--demonstrating their potential as effective economic planning agents--but also engage in sustained tacit collusion, driving prices up to 200% above Nash equilibrium levels. Our analysis examines LLM behavior across three dimensions-(1) decision type, (2) opponent strategies, and (3) market composition--revealing how these factors may shape the competitiveness of LLM-based decision-makers. Furthermore, we show that regulating a few dominant agents by enforcing best-response strategies effectively disrupts collusion and helps restore competitive pricing. Our findings identify potential concerns associated with AI integration in competitive market environments and provide regulatory policy recommendations for the era of automation.

</details>


### [96] [Truth-Revealing Participatory Budgeting](https://arxiv.org/abs/2601.17538)
*Qishen Han,Artem Ivaniuk,Edith Elkind,Lirong Xia*

Main category: cs.GT

TL;DR: 文章从认知视角研究参与式预算（PB），评估常见PB规则表现，发现成本范围缩小近似质量提升，有单位成本时规则能大概率选最优集，还探讨策略性投票，通过数值实验验证理论。


<details>
  <summary>Details</summary>
Motivation: 以往对参与式预算多从公理视角研究偏好完全已知的情况，本文从认知视角研究PB项目潜在质量不可直接观察时的情况。

Method: 通过测量常见PB规则结果的期望效用并与最优项目集对比来评估规则，研究策略性投票的激励问题，还进行数值实验。

Result: 成本范围缩小时近似质量提高；项目有单位成本时常见规则能以趋近1的概率识别出最佳集；策略性投票诚实表达信息仅在非常严格条件下发生。

Conclusion: 不同PB规则在处理信息、选出高质量项目集方面表现不同，策略性投票诚实表达信息条件苛刻，数值实验支持理论发现。

Abstract: Participatory Budgeting (PB) is commonly studied from an axiomatic perspective, where the aim is to design procedurally fair and economically efficient rules for voters with full information regarding their preferences. In contrast, we take an epistemic perspective and consider a framework where PB projects have different levels of underlying quality, indicating how well the project will take effect, which cannot be directly observed before implementation. Agents with noisy information cast votes to aggregate their information, and aim to elect a high-quality set of projects. We evaluate the performance of common PB rules by measuring the expected utility of their outcomes, compared to the optimal set of projects. We find that the quality of approximation improves as the range of project costs shrinks. When projects have unit cost, these common rules can identify the ``best'' set with probability converging to 1. We also study whether strategic agents have incentives to honestly convey their information in the vote. We find that it happens only under very restrictive conditions. We also run numerical experiments to examine the performance of different rules empirically and support our theoretical findings.

</details>


### [97] [Distances Between Top-Truncated Elections of Different Sizes](https://arxiv.org/abs/2601.17931)
*Piotr Faliszewski,Jitka Mertlová,Pierre Nunn,Stanisław Szufa,Tomasz Wąs*

Main category: cs.GT

TL;DR: 扩展选举地图框架以适应不同规模选举及顶部截断选票情况，并可视化Preflib数据库片段。


<details>
  <summary>Details</summary>
Motivation: 原选举地图框架有局限性，仅限候选人数、选民数相等且选票对所有候选人排序的选举。

Method: 将选举地图框架扩展到不同规模选举，且选票可顶部截断。

Result: 能对Preflib数据库的大部分数据进行可视化。

Conclusion: 扩展后的框架能处理更复杂选举数据。

Abstract: The map of elections framework is a methodology for visualizing and analyzing election datasets. So far, the framework was restricted to elections that have equal numbers of candidates, equal numbers of voters, and where all the (ordinal) votes rank all the candidates. We extend it to the case of elections of different sizes, where the votes can be top-truncated. We use our results to present a visualization of a large fragment of the Preflib database.

</details>


### [98] [Credit Fairness: Online Fairness In Shared Resource Pools](https://arxiv.org/abs/2601.17944)
*Seyed Majid Zahedi,Rupert Freeman*

Main category: cs.GT

TL;DR: 研究资源分配问题，指出max - min机制有资源分配不均问题，引入信用公平概念，提出信用公平且帕累托有效的机制并评估。


<details>
  <summary>Details</summary>
Motivation: 现有max - min机制会导致资源分配出现较大差距，需改进资源分配机制。

Method: 引入信用公平概念，提出信用公平且帕累托有效的机制，并在计算资源共享场景中评估其性能。

Result: 未明确提及具体结果。

Conclusion: 信用公平可与帕累托效率或策略防操纵性结合，但不能同时满足，提出了信用公平且帕累托有效的机制。

Abstract: We consider a setting in which a group of agents share resources that must be allocated among them in each discrete time period. Agents have time-varying demands and derive constant marginal utility from each unit of resource received up to their demand, with zero utility for any additional resources. In this setting, it is known that independently maximizing the minimum utility in each round satisfies sharing incentives (agents weakly prefer participating in the mechanism to not participating), strategyproofness (agents have no incentive to misreport their demands), and Pareto efficiency (Freeman et al. 2018). However, recent work (Vuppalapati et al. 2023) has shown that this max-min mechanism can lead to large disparities in the total resources received by agents, even when they have the same average demand. In this paper, we introduce credit fairness, a strengthening of sharing incentives that ensures agents who lend resources in early rounds are able to recoup them in later rounds. Credit fairness can be achieved in conjunction with either Pareto efficiency or strategyproofness, but not both. We propose a mechanism that is credit fair and Pareto efficient, and we evaluate its performance in a computational resource-sharing setting.

</details>


### [99] [Decentralized Multi-product Pricing: Diagonal Dominance, Nash Equilibrium, and Price of Anarchy](https://arxiv.org/abs/2601.18117)
*Boxiao Chen,Jiashuo Jiang,Stefanus Jasin*

Main category: cs.GT

TL;DR: 本文分析多产品企业分散决策定价博弈中效率损失，推导分散与集中最优收入比的最坏情况下界，提供评估集中与分散定价权衡的定量框架。


<details>
  <summary>Details</summary>
Motivation: 多产品企业分散决策中，自主决策者未考虑产品间需求交互会导致效率损失，需量化损失大小。

Method: 构建线性需求系统模型，在对角占优条件下确定纯策略纳什均衡的存在性与唯一性，推导收入比下界并通过构建对称市场拓扑证明其紧性，基于需求交互矩阵谱特性精确刻画效率损失。

Result: 效率损失由参数$μ$控制，收入比下界为$4(1 - μ)/(2 - μ)^2$，构建对称市场拓扑使下界精确达到，还基于谱特性精确刻画效率损失。

Conclusion: 研究结果为多产品企业评估集中定价与分散自主决策的权衡提供定量框架。

Abstract: Decentralized decision making in multi--product firms can lead to efficiency losses when autonomous decision makers fail to internalize cross--product demand interactions. This paper quantifies the magnitude of such losses by analyzing the Price of Anarchy in a pricing game in which each decision maker independently sets prices to maximize its own product--level revenue. We model demand using a linear system that captures both substitution and complementarity effects across products. We first establish existence and uniqueness of a pure--strategy Nash equilibrium under economically standard diagonal dominance conditions. Our main contribution is the derivation of a tight worst--case lower bound on the ratio between decentralized revenue and the optimal centralized revenue. We show that this efficiency loss is governed by a single scalar parameter, denoted by $μ$, which measures the aggregate strength of cross--price effects relative to own--price sensitivities. In particular, we prove that the revenue ratio is bounded below by $4(1-μ)/(2-μ)^2$, and we demonstrate the tightness of this bound by constructing a symmetric market topology in which the bound is exactly attained. We further refine the analysis by providing an instance--exact characterization of efficiency loss based on the spectral properties of the demand interaction matrix. Together, these results offer a quantitative framework for assessing the trade--off between centralized pricing and decentralized autonomy in multi--product firms.

</details>


### [100] [Stable Matching with Deviators and Conformists](https://arxiv.org/abs/2601.18573)
*Frederik Glitzner,David Manlove*

Main category: cs.GT

TL;DR: 论文探讨稳定匹配问题中考虑不同类型参与者（偏离者和顺从者）时的计算复杂度，发现问题难处理，但也找到多项式时间和固定参数易处理的情况。


<details>
  <summary>Details</summary>
Motivation: 实际中并非每个参与者都会引发偏离，想研究在已知偏离者和顺从者的情况下，能否高效判断无偏离者阻碍的匹配是否存在及找到少量偏离者阻碍的匹配。

Method: 对二分和非二分偏好设置下问题的计算复杂度进行刻画。

Result: 这些问题在计算上是难处理的，如判断无偏离者阻碍的匹配是否存在是NP完全问题。

Conclusion: 虽问题难处理，但识别出了多项式时间和固定参数易处理的情况，为不能完全保证稳定性的多智能体系统提供了新算法。

Abstract: In the fundamental Stable Marriage and Stable Roommates problems, there are inherent trade-offs between the size and stability of solutions. While in the former problem, a stable matching always exists and can be found efficiently using the celebrated Gale-Shapley algorithm, the existence of a stable matching is not guaranteed in the latter problem, but can be determined efficiently using Irving's algorithm. However, the computation of matchings that minimise the instability, either due to the presence of additional constraints on the size of the matching or due to restrictive preference cycles, gives rise to a collection of infamously intractable almost-stable matching problems. In practice, however, not every agent is able or likely to initiate deviations caused by blocking pairs. Suppose we knew, for example, due to a set of requirements or estimates based on historical data, which agents are likely to initiate deviations - the deviators - and which are likely to comply with whatever matching they are presented with - the conformists. Can we decide efficiently whether a matching exists in which no deviator is blocking, i.e., in which no deviator has an incentive to initiate a deviation? Furthermore, can we find matchings in which only a few deviators are blocking? We characterise the computational complexity of this question in bipartite and non-bipartite preference settings. Surprisingly, these problems prove computationally intractable in strong ways: for example, unlike in the classical setting, where every agent is considered a deviator, in this extension, we prove that it is NP-complete to decide whether a matching exists where no deviator is blocking. On the positive side, we identify polynomial-time and fixed-parameter tractable cases, providing novel algorithmics for multi-agent systems where stability cannot be fully guaranteed.

</details>


### [101] [Dicey Games: Shared Sources of Randomness in Distributed Systems](https://arxiv.org/abs/2601.18303)
*Léonard Brice,Thomas A. Henzinger,K. S. Thejaswini*

Main category: cs.GT

TL;DR: 论文以4人版抛硬币游戏为例，引入Dicey Games框架，研究共享随机源分布式系统中最优策略的存在性、表示和计算复杂度及随机源分配问题。


<details>
  <summary>Details</summary>
Motivation: 研究团队玩家每对共享一个随机源时，能否有比概率1/4更好的获胜策略，并将此问题拓展到研究共享随机源的分布式系统。

Method: 引入Dicey Games形式化框架进行研究。

Result: 得出当每对团队玩家共享随机源时，团队获胜概率可超过1/4。

Conclusion: 对Dicey Games中最优策略的存在、表示、计算复杂度进行了刻画，研究了团队内有限随机源的最优分配问题。

Abstract: Consider a 4-player version of Matching Pennies where a team of three players competes against the Devil. Each player simultaneously says "Heads" or "Tails". The team wins if all four choices match; otherwise the Devil wins. If all team players randomise independently, they win with probability 1/8; if all players share a common source of randomness, they win with probability 1/2. What happens when each pair of team players shares a source of randomness? Can the team do better than win with probability 1/4? The surprising (and nontrivial) answer is yes! We introduce Dicey Games, a formal framework motivated by the study of distributed systems with shared sources of randomness (of which the above example is a specific instance). We characterise the existence, representation and computational complexity of optimal strategies in Dicey Games, and we study the problem of allocating limited sources of randomness optimally within a team.

</details>


### [102] [Maps of Tournaments: Distances, Experiments, and Data](https://arxiv.org/abs/2601.18348)
*Filip Nikolow,Piotr Faliszewski,Stanisław Szufa*

Main category: cs.GT

TL;DR: 本文通过借鉴选举领域的地图框架构建‘锦标赛地图’，识别距离度量，生成随机锦标赛并与现实对比，展示地图在可视化实验结果中的作用。


<details>
  <summary>Details</summary>
Motivation: 为锦标赛构建一种可视化的表示方式，以便更好地分析和理解锦标赛结果。

Method: 借鉴选举领域的地图框架，构建锦标赛地图；识别有用的距离度量；生成随机锦标赛并与现实锦标赛对比。

Result: 成功构建了锦标赛地图，能够可视化实验结果，包括淘汰赛。

Conclusion: 锦标赛地图是一种有效的工具，可用于可视化锦标赛的实验结果。

Abstract: We form a "map of tournaments" by adapting the map framework from the world of elections. By a tournament we mean a complete directed graph where the nodes are the players and an edge points from a winner of a game to the loser (with no ties allowed). A map is a set of tournaments represented as points on a 2D plane, so that their Euclidean distances resemble the distances computed according to a given measure. We identify useful distance measures, discuss ways of generating random tournaments (and compare them to several real-life ones), and show how the maps are helpful in visualizing experimental results (also for knockout tournaments).

</details>


### [103] [Learning Real-Life Approval Elections](https://arxiv.org/abs/2601.18651)
*Piotr Faliszewski,Łukasz Janeczko,Andrzej Kaczmarczyk,Marcin Kurdziel,Grzegorz Pierczyński,Stanisław Szufa*

Main category: cs.GT

TL;DR: 研究独立批准模型（IAM）及其混合模型，提出学习算法并应用于选举数据，发现混合模型效果好。


<details>
  <summary>Details</summary>
Motivation: 研究可推广多种模型的独立批准模型（IAM）及其学习方法。

Method: 使用最大似然估计或贝叶斯学习从数据中学习IAM及其混合模型。

Result: 单组件模型难以捕捉现实数据复杂性，混合模型表现良好。

Conclusion: IAM混合模型在处理选举数据复杂性上更有效。

Abstract: We study the independent approval model (IAM) for approval elections, where each candidate has its own approval probability and is approved independently of the other ones. This model generalizes, e.g., the impartial culture, the Hamming noise model, and the resampling model. We propose algorithms for learning IAMs and their mixtures from data, using either maximum likelihood estimation or Bayesian learning. We then apply these algorithms to a large set of elections from the Pabulib database. In particular, we find that single-component models are rarely sufficient to capture the complexity of real-life data, whereas their mixtures perform well.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [104] [Frequency-aware Adaptive Contrastive Learning for Sequential Recommendation](https://arxiv.org/abs/2601.17057)
*Zhikai Wang,Weihua Zhang*

Main category: cs.IR

TL;DR: 本文重新审视对比学习中数据增强在序列推荐的作用，提出FACL框架，实验表明其优于现有方法，提升推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 发现对比学习中数据增强对低频物品和稀疏用户行为有固有偏差，需解决此局限。

Method: 提出频率感知自适应对比学习框架FACL，引入微观自适应扰动和宏观重加权。

Result: 在五个公开基准数据集上，FACL始终优于现有方法，推荐准确性最高提升3.8%。

Conclusion: FACL显著缓解低频物品和用户的性能下降，适用于现实长尾推荐场景。

Abstract: In this paper, we revisited the role of data augmentation in contrastive learning for sequential recommendation, revealing its inherent bias against low-frequency items and sparse user behaviors. To address this limitation, we proposed FACL, a frequency-aware adaptive contrastive learning framework that introduces micro-level adaptive perturbation to protect the integrity of rare items, as well as macro-level reweighting to amplify the influence of sparse and rare-interaction sequences during training. Comprehensive experiments on five public benchmark datasets demonstrated that FACL consistently outperforms state-of-the-art data augmentation and model augmentation-based methods, achieving up to 3.8% improvement in recommendation accuracy. Moreover, fine-grained analyses confirm that FACL significantly alleviates the performance drop on low-frequency items and users, highlighting its robust intent-preserving ability and its superior applicability to real-world, long-tail recommendation scenarios.

</details>


### [105] [Evaluation on Entity Matching in Recommender Systems](https://arxiv.org/abs/2601.17218)
*Zihan Huang,Rohan Surana,Zhouhang Xie,Junda Wu,Yu Xia,Julian McAuley*

Main category: cs.IR

TL;DR: 本文引入Reddit - Amazon - EM数据集，对实体匹配方法进行评估，并发布标注集和映射关系，助力推荐系统实体匹配研究。


<details>
  <summary>Details</summary>
Motivation: 现有跨数据集实体匹配缺乏严格评估框架，阻碍了LLM驱动的对话推荐和基于知识的数据集构建等领域的发展。

Method: 引入Reddit - Amazon - EM数据集，对Reddit - Movies和Amazon'23数据集的电影进行手动标注，评估多种实体匹配方法。

Result: 完成对多种实体匹配方法的综合评估，确定最佳方法。

Conclusion: 发布手动标注的实体匹配黄金集和数据集映射关系，为推荐系统实体匹配的未来研究提供有价值的资源。

Abstract: Entity matching is a crucial component in various recommender systems, including conversational recommender systems (CRS) and knowledge-based recommender systems. However, the lack of rigorous evaluation frameworks for cross-dataset entity matching impedes progress in areas such as LLM-driven conversational recommendations and knowledge-grounded dataset construction.
  In this paper, we introduce Reddit-Amazon-EM, a novel dataset comprising naturally occurring items from Reddit and the Amazon '23 dataset. Through careful manual annotation, we identify corresponding movies across Reddit-Movies and Amazon'23, two existing recommender system datasets with inherently overlapping catalogs. Leveraging Reddit-Amazon-EM, we conduct a comprehensive evaluation of state-of-the-art entity matching methods, including rule-based, graph-based, lexical-based, embedding-based, and LLM-based approaches.
  For reproducible research, we release our manually annotated entity matching gold set and provide the mapping between the two datasets using the best-performing method from our experiments. This serves as a valuable resource for advancing future work on entity matching in recommender systems.

</details>


### [106] [FinMetaMind: A Tech Blueprint on NLQ Systems for Financial Knowledge Search](https://arxiv.org/abs/2601.17333)
*Lalit Pant,Shivang Nagar*

Main category: cs.IR

TL;DR: 本文提出适用于金融知识搜索的现代自然语言查询（NLQ）系统技术蓝图，介绍其架构、用例等并分析结果与优化方向。


<details>
  <summary>Details</summary>
Motivation: NLQ能提升金融知识搜索的精准度和召回率，高效关联金融对象等，需设计适用于金融知识搜索的NLQ系统。

Method: 利用自然语言处理、搜索工程和向量数据模型的核心构建，详细说明金融数据集和文档的NLQ独特需求，给出离线索引和在线检索的架构组件。

Result: 文中未明确提及具体实验结果，只表示会详细阐述实验方法、数据、结果和未来优化。

Conclusion: 提供了关于金融知识搜索的NLQ系统的全面分析和技术蓝图。

Abstract: Natural Language Query (NLQ) allows users to search and interact with information systems using plain, human language instead of structured query syntax. This paper presents a technical blueprint on the design of a modern NLQ system tailored to financial knowledge search. The introduction of NLQ not only enhances the precision and recall of the knowledge search compared to traditional methods, but also facilitates deeper insights by efficiently linking disparate financial objects, events, and relationships. Using core constructs from natural language processing, search engineering, and vector data models, the proposed system aims to address key challenges in discovering, relevance ranking, data freshness, and entity recognition intrinsic to financial data retrieval. In this work, we detail the unique requirements of NLQ for financial datasets and documents, outline the architectural components for offline indexing and online retrieval, and discuss the real-world use cases of enhanced knowledge search in financial services. We delve into the theoretical underpinnings and experimental evidence supporting our proposed architecture, ultimately providing a comprehensive analysis on the subject matter. We also provide a detailed elaboration of our experimental methodology, the data used, the results and future optimizations in this study.

</details>


### [107] [Beyond Correlations: A Downstream Evaluation Framework for Query Performance Prediction](https://arxiv.org/abs/2601.17339)
*Payel Santra,Partha Basuchowdhuri,Debasis Ganguly*

Main category: cs.IR

TL;DR: 提出以融合为下游应用的QPP评估框架，实验表明QPP估计在加权IR融合中重要，且下游有效性与标准评估不相关。


<details>
  <summary>Details</summary>
Motivation: 标准QPP评估方法无法量化单个查询有效性且与下游应用无关，不能用于IR管道特定查询决策。

Method: 提出下游聚焦评估框架，用多个排序器检索的文档上QPP估计分布作为IR融合先验。

Result: QPP估计在加权IR融合中重要，比未加权策略提升超4.5%，下游有效性与标准评估相关性差。

Conclusion: 提出的下游聚焦评估框架能有效评估QPP在IR管道中的作用。

Abstract: The standard practice of query performance prediction (QPP) evaluation is to measure a set-level correlation between the estimated retrieval qualities and the true ones. However, neither this correlation-based evaluation measure quantifies QPP effectiveness at the level of individual queries, nor does this connect to a downstream application, meaning that QPP methods yielding high correlation values may not find a practical application in query-specific decisions in an IR pipeline. In this paper, we propose a downstream-focussed evaluation framework where a distribution of QPP estimates across a list of top-documents retrieved with several rankers is used as priors for IR fusion. While on the one hand, a distribution of these estimates closely matching that of the true retrieval qualities indicates the quality of the predictor, their usage as priors on the other hand indicates a predictor's ability to make informed choices in an IR pipeline. Our experiments firstly establish the importance of QPP estimates in weighted IR fusion, yielding substantial improvements of over 4.5% over unweighted CombSUM and RRF fusion strategies, and secondly, reveal new insights that the downstream effectiveness of QPP does not correlate well with the standard correlation-based QPP evaluation.

</details>


### [108] [Breaking Flat: A Generalised Query Performance Prediction Evaluation Framework](https://arxiv.org/abs/2601.17359)
*Payel Santra,Partha Basuchowdhuri,Debasis Ganguly*

Main category: cs.IR

TL;DR: 本文将查询性能预测（QPP）任务及其评估推广到三种设置，结果显示QPP模型相对有效性因任务而异，且预测查询最佳排序器更难。


<details>
  <summary>Details</summary>
Motivation: 传统QPP任务较单一，需要更细粒度和有挑战性的拓展，即确定给定查询最有效的排序模型。

Method: 将QPP任务及其评估推广到三种设置：SingleRanker MultiQuery (SRMQ - PP)、MultiRanker SingleQuery (MRSQ - PP)、MultiRanker MultiQuery (MRMQ - PP)。

Result: a. QPP模型的相对有效性在不同任务（SRMQ - PP和MRSQ - PP）中差异很大；b. 预测查询的最佳排序器比预测给定排序器下查询的相对难度要困难得多。

Conclusion: QPP任务在不同设置下有不同特点，且预测查询最佳排序器是更具挑战性的任务。

Abstract: The traditional use-case of query performance prediction (QPP) is to identify which queries perform well and which perform poorly for a given ranking model. A more fine-grained and arguably more challenging extension of this task is to determine which ranking models are most effective for a given query. In this work, we generalize the QPP task and its evaluation into three settings: (i) SingleRanker MultiQuery (SRMQ-PP), corresponding to the standard use case; (ii) MultiRanker SingleQuery (MRSQ-PP), which evaluates a QPP model's ability to select the most effective ranker for a query; and (iii) MultiRanker MultiQuery (MRMQ-PP), which considers predictions jointly across all query ranker pairs. Our results show that (a) the relative effectiveness of QPP models varies substantially across tasks (SRMQ-PP vs. MRSQ-PP), and (b) predicting the best ranker for a query is considerably more difficult than predicting the relative difficulty of queries for a given ranker.

</details>


### [109] [UniGRec: Unified Generative Recommendation with Soft Identifiers for End-to-End Optimization](https://arxiv.org/abs/2601.17438)
*Jialei Li,Yang Zhang,Yimeng Bai,Shuai Zhu,Ziqi Xue,Xiaoyan Zhao,Dingxian Wang,Frank Yang,Andrew Rabinovich,Xiangnan He*

Main category: cs.IR

TL;DR: 提出统一生成式推荐框架UniGRec解决现有方法问题，在真实数据集上性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐方法常将分词与推荐分离或采用异步交替优化，无法实现端到端对齐。

Method: 提出UniGRec框架，采用退火推理对齐、码字均匀性正则化和双协作蒸馏机制。

Result: 在真实数据集实验表明，UniGRec始终优于现有基线方法。

Conclusion: UniGRec有效解决生成式推荐面临的问题，提升推荐性能。

Abstract: Generative recommendation has recently emerged as a transformative paradigm that directly generates target items, surpassing traditional cascaded approaches. It typically involves two components: a tokenizer that learns item identifiers and a recommender trained on them. Existing methods often decouple tokenization from recommendation or rely on asynchronous alternating optimization, limiting full end-to-end alignment. To address this, we unify the tokenizer and recommender under the ultimate recommendation objective via differentiable soft item identifiers, enabling joint end-to-end training. However, this introduces three challenges: training-inference discrepancy due to soft-to-hard mismatch, item identifier collapse from codeword usage imbalance, and collaborative signal deficiency due to an overemphasis on fine-grained token-level semantics.
  To tackle these challenges, we propose UniGRec, a unified generative recommendation framework that addresses them from three perspectives. UniGRec employs Annealed Inference Alignment during tokenization to smoothly bridge soft training and hard inference, a Codeword Uniformity Regularization to prevent identifier collapse and encourage codebook diversity, and a Dual Collaborative Distillation mechanism that distills collaborative priors from a lightweight teacher model to jointly guide both the tokenizer and the recommender. Extensive experiments on real-world datasets demonstrate that UniGRec consistently outperforms state-of-the-art baseline methods. Our codes are available at https://github.com/Jialei-03/UniGRec.

</details>


### [110] [Adversarial Alignment and Disentanglement for Cross-Domain CTR Prediction with Domain-Encompassing Features](https://arxiv.org/abs/2601.17472)
*Junyou He,Lixi Deng,Huichao Guo,Ye Tang,Yong Li,Sulong Xu*

Main category: cs.IR

TL;DR: 文章提出A²DCDR模型解决跨域推荐问题，实验证明其效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有跨域推荐方法仅依赖域不变特征和目标域特定特征，性能不佳，需更好方法捕捉全面跨域信息。

Method: 提出A²DCDR模型，包含用对抗训练改进MMD，使用特征解缠器和重建机制，引入融合表示。

Result: 在真实数据集实验和在线A/B测试中，A²DCDR模型优于现有方法。

Conclusion: A²DCDR模型有效且具备实际应用价值，代码已开源。

Abstract: Cross-domain recommendation (CDR) has been increasingly explored to address data sparsity and cold-start issues. However, recent approaches typically disentangle domain-invariant features shared between source and target domains, as well as domain-specific features for each domain. However, they often rely solely on domain-invariant features combined with target domain-specific features, which can lead to suboptimal performance. To overcome the limitations, this paper presents the Adversarial Alignment and Disentanglement Cross-Domain Recommendation ($A^2DCDR$ ) model, an innovative approach designed to capture a comprehensive range of cross-domain information, including both domain-invariant and valuable non-aligned features. The $A^2DCDR$ model enhances cross-domain recommendation through three key components: refining MMD with adversarial training for better generalization, employing a feature disentangler and reconstruction mechanism for intra-domain disentanglement, and introducing a novel fused representation combining domain-invariant, non-aligned features with original contextual data. Experiments on real-world datasets and online A/B testing show that $A^2DCDR$ outperforms existing methods, confirming its effectiveness and practical applicability. The code is provided at https://github.com/youzi0925/A-2DCDR/tree/main.

</details>


### [111] [Towards Fair Large Language Model-based Recommender Systems without Costly Retraining](https://arxiv.org/abs/2601.17492)
*Jin Li,Huilin Gu,Shoujin Wang,Qi Zhang,Shui Yu,Chen Wang,Xiwei Xu,Fang Chen*

Main category: cs.IR

TL;DR: 现有LLM - RS有公平性问题，提出FUDLR方法解决，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 解决LLM - RS中存在的公平性问题，现有去偏方法缺乏通用性且计算不可行。

Method: 提出FUDLR，将去偏问题转化为两阶段的高效机器遗忘任务，先识别需遗忘的样本，再去除其对模型参数的影响。

Result: 广泛实验表明FUDLR能有效且高效地提高公平性，同时保留推荐准确性。

Conclusion: FUDLR为实现具有社会责任的LLM - RS提供了实用途径。

Abstract: Large Language Models (LLMs) have revolutionized Recommender Systems (RS) through advanced generative user modeling. However, LLM-based RS (LLM-RS) often inadvertently perpetuates bias present in the training data, leading to severe fairness issues. Addressing these fairness problems in LLM-RS faces two significant challenges. 1) Existing debiasing methods, designed for specific bias types, lack the generality to handle diverse or emerging biases in real-world applications. 2) Debiasing methods relying on retraining are computationally infeasible given the massive parameter scale of LLMs. To overcome these challenges, we propose FUDLR (Fast Unified Debiasing for LLM-RS). The core idea is to reformulate the debiasing problem as an efficient machine unlearning task with two stages. First, FUDLR identifies bias-inducing samples to unlearn through a novel bias-agnostic mask, optimized to balance fairness improvement with accuracy preservation. Its bias-agnostic design allows adaptability to various or co-existing biases simply by incorporating different fairness metrics. Second, FUDLR performs efficient debiasing by estimating and removing the influence of identified samples on model parameters. Extensive experiments demonstrate that FUDLR effectively and efficiently improves fairness while preserving recommendation accuracy, offering a practical path toward socially responsible LLM-RS. The code and data are available at https://github.com/JinLi-i/FUDLR.

</details>


### [112] [To Case or Not to Case: An Empirical Study in Learned Sparse Retrieval](https://arxiv.org/abs/2601.17500)
*Emmanouil Georgios Lionis,Jia-Huei Ju,Angelos Nalmpantis,Casper Thuis,Sean MacAvaney,Andrew Yates*

Main category: cs.IR

TL;DR: 研究有大小写区分和无大小写区分的基础模型对学习型稀疏检索（LSR）的影响，发现大小写区分模型默认表现差，但预处理为小写可消除差距。


<details>
  <summary>Details</summary>
Motivation: 现有LSR方法大多依赖无大小写区分模型，而最新的先进语言模型仅有大小写区分版本，且基础模型大小写对LSR的影响未被研究。

Method: 在多个数据集上系统评估相同基础模型的大小写区分版本和无大小写区分版本的适用性。

Result: 大小写区分的基础模型默认表现远不如无大小写区分的，文本预处理为小写可消除差距，且大小写区分模型在小写处理后表现近乎无大小写区分模型。

Conclusion: 拓宽了近期大小写区分模型在LSR中的适用性，便于将更强的基础架构集成到稀疏检索中。

Abstract: Learned Sparse Retrieval (LSR) methods construct sparse lexical representations of queries and documents that can be efficiently searched using inverted indexes. Existing LSR approaches have relied almost exclusively on uncased backbone models, whose vocabularies exclude case-sensitive distinctions, thereby reducing vocabulary mismatch. However, the most recent state-of-the-art language models are only available in cased versions. Despite this shift, the impact of backbone model casing on LSR has not been studied, potentially posing a risk to the viability of the method going forward. To fill this gap, we systematically evaluate paired cased and uncased versions of the same backbone models across multiple datasets to assess their suitability for LSR. Our findings show that LSR models with cased backbone models by default perform substantially worse than their uncased counterparts; however, this gap can be eliminated by pre-processing the text to lowercase. Moreover, our token-level analysis reveals that, under lowercasing, cased models almost entirely suppress cased vocabulary items and behave effectively as uncased models, explaining their restored performance. This result broadens the applicability of recent cased models to the LSR setting and facilitates the integration of stronger backbone architectures into sparse retrieval. The complete code and implementation for this project are available at: https://github.com/lionisakis/Uncased-vs-cased-models-in-LSR

</details>


### [113] [Capturing P: On the Expressive Power and Efficient Evaluation of Boolean Retrieval](https://arxiv.org/abs/2601.18747)
*Amir Aavani*

Main category: cs.IR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Modern information retrieval is transitioning from simple document filtering to complex, neuro-symbolic reasoning workflows. However, current retrieval architectures face a fundamental efficiency dilemma when handling the rigorous logical and arithmetic constraints required by this new paradigm. Standard iterator-based engines (Document-at-a-Time) do not natively support complex, nested logic graphs; forcing them to execute such queries typically results in intractable runtime performance. Conversely, naive recursive approaches (Term-at-a-Time), while capable of supporting these structures, suffer from prohibitive memory consumption when enforcing broad logical exclusions.
  In this paper, we propose that a retrieval engine must be capable of ``Capturing $\mathbf{P}$'' -- evaluating any polynomial-time property directly over its index in a computationally efficient manner. We define a formal Retrieval Language ($\mathcal{L}_R$) based on Directed Acyclic Graphs (DAGs) and prove it precisely captures the complexity class $\mathbf{P}$. We introduce \texttt{ComputePN}, a novel evaluation algorithm that makes $\mathcal{L}_R$ tractable. By combining native DAG traversal with a memory-efficient ``Positive-Negative'' response mechanism, \texttt{ComputePN} ensures the efficient evaluation of any query in $\mathcal{L}_R$. This work establishes the theoretical foundation for turning the search index into a general-purpose computational engine.

</details>


### [114] [Pipeline Inspection, Visualization, and Interoperability in PyTerrier](https://arxiv.org/abs/2601.17502)
*Emmanouil Georgios Lionis,Craig Macdonald,Sean MacAvaney*

Main category: cs.IR

TL;DR: PyTerrier提供构建和实验IR管道的声明式框架，演示展示新管道操作增强其可检查、可视化和集成能力，方便人员使用。


<details>
  <summary>Details</summary>
Motivation: 提升IR管道的可操作性，让研究人员、学生和AI代理更容易理解和使用各类IR管道。

Method: 通过Model Context Protocol (MCP)实现管道操作，使其可被编程式检查、可视化并与其他工具集成。

Result: 展示了能增强IR管道可检查、可视化和集成能力的新管道操作。

Conclusion: 新管道操作有助于用户更好地理解和使用IR管道。

Abstract: PyTerrier provides a declarative framework for building and experimenting with Information Retrieval (IR) pipelines. In this demonstration, we highlight several recent pipeline operations that improve their ability to be programmatically inspected, visualized, and integrated with other tools (via the Model Context Protocol, MCP). These capabilities aim to make it easier for researchers, students, and AI agents to understand and use a wide array of IR pipelines.

</details>


### [115] [Real-Time Trend Prediction via Continually-Aligned LLM Query Generation](https://arxiv.org/abs/2601.17567)
*Zijing Hui,Wenhan Lyu,Shusen Wang,Li Chen,Chu Wang*

Main category: cs.IR

TL;DR: 提出RTTP框架解决低流量搜索环境下的热门新闻检测冷启动问题，在生产环境取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在低流量搜索环境中识别新兴或长尾趋势时存在冷启动问题，依赖关键词频率或查询峰值的方法在稀疏环境中缓慢且无效。

Method: 引入RTTP框架，利用持续学习的大语言模型将新闻内容转化为搜索查询，并使用参与强度和创作者权威进行评分；提出Mix - Policy DPO方法缓解模型升级时的灾难性遗忘。

Result: 在Facebook和Meta AI产品上大规模部署时，在尾部趋势检测精度@500上提升了91.4%，查询生成准确率提升了19%，且多周在线训练后性能稳定。

Conclusion: 当大语言模型生成的合成搜索信号对齐并持续更新时，可以在低流量搜索环境中及时理解趋势。

Abstract: Trending news detection in low-traffic search environments faces a fundamental cold-start problem, where a lack of query volume prevents systems from identifying emerging or long-tail trends. Existing methods relying on keyword frequency or query spikes are inherently slow and ineffective in these sparse settings, lagging behind real-world shifts in attention. We introduce RTTP, a novel Real-Time Trending Prediction framework that generates search queries directly from news content instead of waiting for users to issue them. RTTP leverages a continual learning LLM (CL-LLM) that converts posts into search-style queries and scores them using engagement strength + creator authority, enabling early trend surfacing before search volume forms. To ensure adaptation without degrading reasoning, we propose Mix-Policy DPO, a new preference-based continual learning approach that combines on-policy stability with off-policy novelty to mitigate catastrophic forgetting during model upgrades. Deployed at production scale on Facebook and Meta AI products, RTTP delivers +91.4% improvement in tail-trend detection precision@500 and +19% query generation accuracy over industry baselines, while sustaining stable performance after multi-week online training. This work demonstrates that LLM-generated synthetic search signals, when aligned and continually updated, unlock timely trend understanding in low-traffic search environments.

</details>


### [116] [Why They Link: An Intent Taxonomy for Including Hyperlinks in Social Posts](https://arxiv.org/abs/2601.17601)
*Fangping Lan,Abdullah Aljebreen,Eduard C. Dragut*

Main category: cs.IR

TL;DR: 研究Twitter上用户对推文中超链接意图的解读，开发分类法并分析1000条推文，发现常见意图，为信息检索和NLP应用奠基。


<details>
  <summary>Details</summary>
Motivation: 过往研究关注分享URL作者的动机，难以实际观察，本文旨在研究读者对推文中超链接意图的解读，以支持下游应用。

Method: 采用混合方法，先通过大规模众包注释自下而上、数据驱动地开发超链接意图分类法，再用大语言模型辅助完善类别名称和定义。

Result: 最终分类法包含6个顶级类别和26个细粒度意图类，分析1000条推文发现广告、争论和分享是最常见的意图。

Conclusion: 该分类法为意图感知的信息检索和NLP应用提供基础，有助于更准确地检索、推荐和理解社交媒体内容。

Abstract: URLs serve as bridges between social media platforms and the broader web, linking user-generated content to external information resources. On Twitter (X), approximately one in five tweets contains at least one URL, underscoring their central role in information dissemination. While prior studies have examined the motivations of authors who share URLs, such author-centered intentions are difficult to observe in practice. To enable broader downstream use, this work investigates reader-centered interpretations, i.e., how users perceive the intentions behind hyperlinks included in posts. We develop an intent taxonomy for including hyperlinks in social posts through a hybrid approach that begins with a bottom-up, data-driven process using large-scale crowdsourced annotations, and is then refined using large language model assistance to generate descriptive category names and precise definitions. The final taxonomy comprises 6 top-level categories and 26 fine-grained intention classes, capturing diverse communicative purposes. Applying this taxonomy, we annotate and analyze 1000 user posts, revealing that advertising, arguing, and sharing are the most prevalent intentions. This resulting taxonomy provides a foundation for intent-aware information retrieval and NLP applications, enabling more accurate retrieval, recommendation, and understanding of social media content.

</details>


### [117] [Agentic Search in the Wild: Intents and Trajectory Dynamics from 14M+ Real Search Requests](https://arxiv.org/abs/2601.17617)
*Jingjie Ning,João Coelho,Yibo Kong,Yunfan Long,Bruno Martins,João Magalhães,Jamie Callan,Chenyan Xiong*

Main category: cs.IR

TL;DR: 对基于1444万搜索请求的代理搜索日志进行大规模分析，揭示行为模式并提出优化建议。


<details>
  <summary>Details</summary>
Motivation: IR社区缺乏对代理搜索会话展开方式及检索证据使用的实证理解。

Method: 对日志进行会话化处理，用LLM注释分配会话意图和查询重写标签，提出CTAR量化查询词与先前检索证据的关联。

Result: 发现多轮会话步骤数和间隔时间特点，不同意图行为有差异，代理会跨步骤重用证据。

Conclusion: 代理搜索可从重复感知的提前停止、意图自适应检索预算和显式跨步骤上下文跟踪中受益，计划发布匿名日志。

Abstract: LLM-powered search agents are increasingly being used for multi-step information seeking tasks, yet the IR community lacks empirical understanding of how agentic search sessions unfold and how retrieved evidence is used. This paper presents a large-scale log analysis of agentic search based on 14.44M search requests (3.97M sessions) collected from DeepResearchGym, i.e. an open-source search API accessed by external agentic clients. We sessionize the logs, assign session-level intents and step-wise query-reformulation labels using LLM-based annotation, and propose Context-driven Term Adoption Rate (CTAR) to quantify whether newly introduced query terms are traceable to previously retrieved evidence. Our analyses reveal distinctive behavioral patterns. First, over 90% of multi-turn sessions contain at most ten steps, and 89% of inter-step intervals fall under one minute. Second, behavior varies by intent. Fact-seeking sessions exhibit high repetition that increases over time, while sessions requiring reasoning sustain broader exploration. Third, agents reuse evidence across steps. On average, 54% of newly introduced query terms appear in the accumulated evidence context, with contributions from earlier steps beyond the most recent retrieval. The findings suggest that agentic search may benefit from repetition-aware early stopping, intent-adaptive retrieval budgets, and explicit cross-step context tracking. We plan to release the anonymized logs to support future research.

</details>


### [118] [LegalMALR:Multi-Agent Query Understanding and LLM-Based Reranking for Chinese Statute Retrieval](https://arxiv.org/abs/2601.17692)
*Yunhan Li,Mingjie Xie,Gaoli Kang,Zihan Gong,Gengshen Wu,Min Yang*

Main category: cs.IR

TL;DR: 提出了用于法规检索的LegalMALR框架，结合多智能体查询理解系统和零样本大模型重排模块，在数据集上实验显示其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 现实法律查询的特点使传统检索增强生成管道难以准确检索法规元素，现有方法存在不足。

Method: 提出LegalMALR框架，集成MAS和LLM Reranker，用GRPO优化MAS策略，LLM Reranker进行自然语言法律推理排名。

Result: 在CSAID和STARD基准上的实验表明，LegalMALR在分布内和分布外设置中均显著优于强RAG基线。

Conclusion: 结合多视角查询解释、基于强化的策略优化和大模型重排对法规检索有效。

Abstract: Statute retrieval is essential for legal assistance and judicial decision support, yet real-world legal queries are often implicit, multi-issue, and expressed in colloquial or underspecified forms. These characteristics make it difficult for conventional retrieval-augmented generation pipelines to recover the statutory elements required for accurate retrieval. Dense retrievers focus primarily on the literal surface form of the query, whereas lightweight rerankers lack the legal-reasoning capacity needed to assess statutory applicability. We present LegalMALR, a retrieval framework that integrates a Multi-Agent Query Understanding System (MAS) with a zero-shot large-language-model-based reranking module (LLM Reranker). MAS generates diverse, legally grounded reformulations and conducts iterative dense retrieval to broaden candidate coverage. To stabilise the stochastic behaviour of LLM-generated rewrites, we optimise a unified MAS policy using Generalized Reinforcement Policy Optimization(GRPO). The accumulated candidate set is subsequently evaluated by the LLM Reranker, which performs natural-language legal reasoning to produce the final ranking. We further construct CSAID, a dataset of 118 difficult Chinese legal queries annotated with multiple statutory labels, and evaluate LegalMALR on both CSAID and the public STARD benchmark. Experiments show that LegalMALR substantially outperforms strong Retrieval-augmented generation(RAG) baselines in both in-distribution and out-of-distribution settings, demonstrating the effectiveness of combining multi-perspective query interpretation, reinforcement-based policy optimisation, and large-model reranking for statute retrieval.

</details>


### [119] [Token-Weighted Multi-Target Learning for Generative Recommenders with Curriculum Learning](https://arxiv.org/abs/2601.17787)
*Wei-Ning Chiu,Chuan-Ju Wang,Pu-Jen Cheng*

Main category: cs.IR

TL;DR: 提出适用于生成式推荐的信息增益令牌加权策略与多目标学习框架，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 现有的生成式推荐系统优化标准存在对所有令牌平等对待的问题，与基于语义 ID 的生成不一致。

Method: 提出两种互补的信息增益令牌加权策略：前端更大加权和频率加权；引入带有课程学习的多目标学习框架。

Result: 在基准数据集上持续优于强基线和现有令牌加权方法，提高了鲁棒性、泛化性，在头尾项目上都有显著提升。

Conclusion: 所提出的方法能有效改进生成式推荐系统。

Abstract: Generative recommender systems have recently attracted attention by formulating next-item prediction as an autoregressive sequence generation task. However, most existing methods optimize standard next-token likelihood and implicitly treat all tokens as equally informative, which is misaligned with semantic-ID-based generation. Accordingly, we propose two complementary information-gain-based token-weighting strategies tailored to generative recommendation with semantic IDs. Front-Greater Weighting captures conditional semantic information gain by prioritizing early tokens that most effectively reduce candidate-item uncertainty given their prefixes and encode coarse semantics. Frequency Weighting models marginal information gain under long-tailed item and token distributions, upweighting rare tokens to counteract popularity bias. Beyond individual strategies, we introduce a multi-target learning framework with curriculum learning that jointly optimizes the two token-weighted objectives alongside standard likelihood, enabling stable optimization and adaptive emphasis across training stages. Extensive experiments on benchmark datasets show that our method consistently outperforms strong baselines and existing token-weighting approaches, with improved robustness, strong generalization across different semantic-ID constructions, and substantial gains on both head and tail items. Code is available at https://github.com/CHIUWEINING/Token-Weighted-Multi-Target-Learning-for-Generative-Recommenders-with-Curriculum-Learning.

</details>


### [120] [Unleashing the Potential of Sparse Attention on Long-term Behaviors for CTR Prediction](https://arxiv.org/abs/2601.17836)
*Weijiang Lai,Beihong Jin,Di Zhang,Siru Chen,Jiongyan Zhang,Yuhang Gou,Jian Dong,Xingxing Wang*

Main category: cs.IR

TL;DR: 提出适用于用户长期行为的SparseCTR模型，实验表明其提升效率且优于现有方法，展示了缩放定律现象，在线测试提高CTR和CPM。


<details>
  <summary>Details</summary>
Motivation: 现有展示缩放定律的模型因自注意力机制计算复杂度高难以在工业场景使用，其他领域的稀疏自注意力机制不适用于推荐场景。

Method: 个性化分割行为序列成块；提出三分支稀疏自注意力机制；设计复合相对时间编码。

Result: SparseCTR实验中提升效率、优于现有方法，展示缩放定律，在线A/B测试中CTR提高1.72%，CPM提高1.41%。

Conclusion: SparseCTR是一个高效有效的模型，适合用户长期行为建模，有较好的性能表现和缩放定律现象。

Abstract: In recent years, the success of large language models (LLMs) has driven the exploration of scaling laws in recommender systems. However, models that demonstrate scaling laws are actually challenging to deploy in industrial settings for modeling long sequences of user behaviors, due to the high computational complexity of the standard self-attention mechanism. Despite various sparse self-attention mechanisms proposed in other fields, they are not fully suited for recommendation scenarios. This is because user behaviors exhibit personalization and temporal characteristics: different users have distinct behavior patterns, and these patterns change over time, with data from these users differing significantly from data in other fields in terms of distribution. To address these challenges, we propose SparseCTR, an efficient and effective model specifically designed for long-term behaviors of users. To be precise, we first segment behavior sequences into chunks in a personalized manner to avoid separating continuous behaviors and enable parallel processing of sequences. Based on these chunks, we propose a three-branch sparse self-attention mechanism to jointly identify users' global interests, interest transitions, and short-term interests. Furthermore, we design a composite relative temporal encoding via learnable, head-specific bias coefficients, better capturing sequential and periodic relationships among user behaviors. Extensive experimental results show that SparseCTR not only improves efficiency but also outperforms state-of-the-art methods. More importantly, it exhibits an obvious scaling law phenomenon, maintaining performance improvements across three orders of magnitude in FLOPs. In online A/B testing, SparseCTR increased CTR by 1.72\% and CPM by 1.41\%. Our source code is available at https://github.com/laiweijiang/SparseCTR.

</details>


### [121] [Post-Training Denoising of User Profiles with LLMs in Collaborative Filtering Recommendation](https://arxiv.org/abs/2601.18009)
*Ervin Dervishaj,Maria Maistro,Tuukka Ruotsalo,Christina Lioma*

Main category: cs.IR

TL;DR: 文章提出使用大语言模型进行训练后去噪用户画像的方法，以提升协同过滤推荐效果，实验表明该方法有显著提升。


<details>
  <summary>Details</summary>
Motivation: 隐式反馈数据有噪声，影响推荐效果，以往训练中去噪方法成本高、耗数据，因此研究训练后去噪。

Method: 用大语言模型进行训练后去噪，通过提示大语言模型处理用户画像、候选项目及其排名，移除画像中项目来提升候选项目排名。

Result: 在3个数据集上用先进CF推荐器和4个开源、闭源大语言模型实验，去噪后效果比原用户画像最多提升13%。

Conclusion: 使用大语言模型进行训练后去噪用户画像能有效提升协同过滤推荐效果。

Abstract: Implicit feedback -- the main data source for training Recommender Systems (RSs) -- is inherently noisy and has been shown to negatively affect recommendation effectiveness. Denoising has been proposed as a method for removing noisy implicit feedback and improving recommendations. Prior work has focused on in-training denoising, however this requires additional data, changes to the model architecture and training procedure or fine-tuning, all of which can be costly and data hungry. In this work, we focus on post-training denoising. Different from in-training denoising, post-training denoising does not involve changing the architecture of the model nor its training procedure, and does not require additional data. Specifically, we present a method for post-training denoising user profiles using Large Language Models (LLMs) for Collaborative Filtering (CF) recommendations. Our approach prompts LLMs with (i) a user profile (user interactions), (ii) a candidate item, and (iii) its rank as given by the CF recommender, and asks the LLM to remove items from the user profile to improve the rank of the candidate item. Experiments with a state-of-the-art CF recommender and 4 open and closed source LLMs in 3 datasets show that our denoising yields improvements up to 13% in effectiveness over the original user profiles. Our code is available at https://github.com/edervishaj/denoising-user-profiles-LLM.

</details>


### [122] [Enhancing LLM-based Recommendation with Preference Hint Discovery from Knowledge Graph](https://arxiv.org/abs/2601.18096)
*Yuting Zhang,Ziliang Pei,Chao Wang,Ying Sun,Fuzhen Zhuang*

Main category: cs.IR

TL;DR: 本文提出基于交互集成知识图的偏好提示发现模型，增强基于大语言模型的推荐效果，实验表明该框架有效。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在捕捉复杂偏好模式上不如传统推荐器，现有将传统推荐嵌入集成到大语言模型的方法存在语义空间差距，直接输入属性知识有挑战。

Method: 提出基于交互集成知识图的偏好提示发现模型，设计协作偏好提示提取模式，开发实例级双注意力机制，采用扁平化提示组织方法。

Result: 在成对和列表式推荐任务的大量实验中，该框架比基线平均相对提升超3.02%。

Conclusion: 所提框架能有效增强基于大语言模型的推荐。

Abstract: LLMs have garnered substantial attention in recommendation systems. Yet they fall short of traditional recommenders when capturing complex preference patterns. Recent works have tried integrating traditional recommendation embeddings into LLMs to resolve this issue, yet a core gap persists between their continuous embedding and discrete semantic spaces. Intuitively, textual attributes derived from interactions can serve as critical preference rationales for LLMs' recommendation logic. However, directly inputting such attribute knowledge presents two core challenges: (1) Deficiency of sparse interactions in reflecting preference hints for unseen items; (2) Substantial noise introduction from treating all attributes as hints. To this end, we propose a preference hint discovery model based on the interaction-integrated knowledge graph, enhancing LLM-based recommendation. It utilizes traditional recommendation principles to selectively extract crucial attributes as hints. Specifically, we design a collaborative preference hint extraction schema, which utilizes semantic knowledge from similar users' explicit interactions as hints for unseen items. Furthermore, we develop an instance-wise dual-attention mechanism to quantify the preference credibility of candidate attributes, identifying hints specific to each unseen item. Using these item- and user-based hints, we adopt a flattened hint organization method to shorten input length and feed the textual hint information to the LLM for commonsense reasoning. Extensive experiments on both pair-wise and list-wise recommendation tasks verify the effectiveness of our proposed framework, indicating an average relative improvement of over 3.02% against baselines.

</details>


### [123] [Think When Needed: Model-Aware Reasoning Routing for LLM-based Ranking](https://arxiv.org/abs/2601.18146)
*Huizhong Guo,Tianjun Wei,Dongxia Wang,Yingpeng Du,Ziyan Wang,Jie Zhang,Zhu Sun*

Main category: cs.IR

TL;DR: 提出推理路由框架解决大语言模型排名任务中推理提示效益不一致和计算成本高的问题，实验显示能提升排名效用并减少令牌消耗。


<details>
  <summary>Details</summary>
Motivation: 推理提示在大语言模型排名任务中的效益不稳定且计算成本高，需解决何时推理的问题。

Method: 提出推理路由框架，用轻量级、可插拔的路由头依据预生成信号（排名感知特征和模型感知难度信号）决定实例使用直接推理或推理，且路由头可根据验证的帕累托前沿自适应选择操作策略。

Result: 在三个公共排名数据集上使用不同规模的开源大语言模型进行实验，在减少令牌消耗的情况下排名效用有一致提升，如在MovieLens上Qwen3 - 4B模型的NDCG@10提升6.3%，令牌消耗减少49.5%。

Conclusion: 推理路由是解决大语言模型排名任务中准确性 - 效率权衡的实用方案。

Abstract: Large language models (LLMs) are increasingly applied to ranking tasks in retrieval and recommendation. Although reasoning prompting can enhance ranking utility, our preliminary exploration reveals that its benefits are inconsistent and come at a substantial computational cost, suggesting that when to reason is as crucial as how to reason. To address this issue, we propose a reasoning routing framework that employs a lightweight, plug-and-play router head to decide whether to use direct inference (Non-Think) or reasoning (Think) for each instance before generation. The router head relies solely on pre-generation signals: i) compact ranking-aware features (e.g., candidate dispersion) and ii) model-aware difficulty signals derived from a diagnostic checklist reflecting the model's estimated need for reasoning. By leveraging these features before generation, the router outputs a controllable token that determines whether to apply the Think mode. Furthermore, the router can adaptively select its operating policy along the validation Pareto frontier during deployment, enabling dynamic allocation of computational resources toward instances most likely to benefit from Think under varying system constraints. Experiments on three public ranking datasets with different scales of open-source LLMs show consistent improvements in ranking utility with reduced token consumption (e.g., +6.3\% NDCG@10 with -49.5\% tokens on MovieLens with Qwen3-4B), demonstrating reasoning routing as a practical solution to the accuracy-efficiency trade-off.

</details>


### [124] [DMAP: Human-Aligned Structural Document Map for Multimodal Document Understanding](https://arxiv.org/abs/2601.18203)
*ShunLiang Fu,Yanxin Zhang,Yixin Xiang,Xiaoyu Du,Jinhui Tang*

Main category: cs.IR

TL;DR: 现有多模态文档问答系统忽视文档结构，本文提出DMAP编码结构，经实验证明优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态文档问答系统依赖扁平语义检索，忽略文档内在层级和关系结构，破坏逻辑和空间依赖。

Method: 引入文档级结构Document MAP (DMAP)，设计结构化语义理解代理构建DMAP，基于此使用反思推理代理进行结构感知和证据驱动推理。

Result: 在MMDocQA基准测试中，DMAP产生与人类解释模式一致的特定文档结构表示，在检索精度、推理一致性和多模态理解方面优于传统基于RAG的方法。

Conclusion: DMAP能有效解决现有多模态文档问答系统的局限性，提升性能。

Abstract: Existing multimodal document question-answering (QA) systems predominantly rely on flat semantic retrieval, representing documents as a set of disconnected text chunks and largely neglecting their intrinsic hierarchical and relational structures. Such flattening disrupts logical and spatial dependencies - such as section organization, figure-text correspondence, and cross-reference relations, that humans naturally exploit for comprehension. To address this limitation, we introduce a document-level structural Document MAP (DMAP), which explicitly encodes both hierarchical organization and inter-element relationships within multimodal documents. Specifically, we design a Structured-Semantic Understanding Agent to construct DMAP by organizing textual content together with figures, tables, charts, etc. into a human-aligned hierarchical schema that captures both semantic and layout dependencies. Building upon this representation, a Reflective Reasoning Agent performs structure-aware and evidence-driven reasoning, dynamically assessing the sufficiency of retrieved context and iteratively refining answers through targeted interactions with DMAP. Extensive experiments on MMDocQA benchmarks demonstrate that DMAP yields document-specific structural representations aligned with human interpretive patterns, substantially enhancing retrieval precision, reasoning consistency, and multimodal comprehension over conventional RAG-based approaches. Code is available at https://github.com/Forlorin/DMAP

</details>


### [125] [Generative Chain of Behavior for User Trajectory Prediction](https://arxiv.org/abs/2601.18213)
*Chengkai Huang,Xiaodi Chen,Hongtao Huang,Quan Z. Sheng,Lina Yao*

Main category: cs.IR

TL;DR: 提出Generative Chain of Behavior (GCB)框架建模用户多步未来行为，实验显示优于现有方法且能统一描述用户偏好演变。


<details>
  <summary>Details</summary>
Motivation: 多数顺序推荐器专注下一项预测，忽略多个未来行动间依赖，需要建模长期用户行为轨迹以理解偏好演变和实现主动推荐。

Method: 先通过带k-means细化的RQ - VAE将物品编码为语义ID，构建离散潜在空间；再用基于transformer的自回归生成器，根据用户历史预测多步未来行为。

Result: 在基准数据集实验中，GCB在多步准确性和轨迹一致性上持续优于现有顺序推荐器。

Conclusion: GCB是一个统一的生成式框架，可捕捉用户偏好演变。

Abstract: Modeling long-term user behavior trajectories is essential for understanding evolving preferences and enabling proactive recommendations. However, most sequential recommenders focus on next-item prediction, overlooking dependencies across multiple future actions. We propose Generative Chain of Behavior (GCB), a generative framework that models user interactions as an autoregressive chain of semantic behaviors over multiple future steps. GCB first encodes items into semantic IDs via RQ-VAE with k-means refinement, forming a discrete latent space that preserves semantic proximity. On top of this space, a transformer-based autoregressive generator predicts multi-step future behaviors conditioned on user history, capturing long-horizon intent transitions and generating coherent trajectories. Experiments on benchmark datasets show that GCB consistently outperforms state-of-the-art sequential recommenders in multi-step accuracy and trajectory consistency. Beyond these gains, GCB offers a unified generative formulation for capturing user preference evolution.

</details>


### [126] [GenCI: Generative Modeling of User Interest Shift via Cohort-based Intent Learning for CTR Prediction](https://arxiv.org/abs/2601.18251)
*Kesha Ou,Zhen Tian,Wayne Xin Zhao,Hongyu Lu,Ji-Rong Wen*

Main category: cs.IR

TL;DR: 提出GenCI生成式用户意图框架用于CTR预测，解决现有方法问题，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有CTR预测的判别式范式易过拟合及点式排序范式存在信息鸿沟，无法适应兴趣快速转变和捕捉召回集上下文信号。

Method: 提出GenCI框架，先使用生成模型生成候选兴趣群组，再用分层候选感知网络注入上下文信号到排序阶段，模型端到端训练。

Result: 在三个常用数据集上的实验证明了该方法的有效性。

Conclusion: GenCI框架能创建更一致、有效的CTR预测管道，解决现有方法存在的问题。

Abstract: Click-through rate (CTR) prediction plays a pivotal role in online advertising and recommender systems. Despite notable progress in modeling user preferences from historical behaviors, two key challenges persist. First, exsiting discriminative paradigms focus on matching candidates to user history, often overfitting to historically dominant features and failing to adapt to rapid interest shifts. Second, a critical information chasm emerges from the point-wise ranking paradigm. By scoring each candidate in isolation, CTR models discard the rich contextual signal implied by the recalled set as a whole, leading to a misalignment where long-term preferences often override the user's immediate, evolving intent. To address these issues, we propose GenCI, a generative user intent framework that leverages semantic interest cohorts to model dynamic user preferences for CTR prediction. The framework first employs a generative model, trained with a next-item prediction (NTP) objective, to proactively produce candidate interest cohorts. These cohorts serve as explicit, candidate-agnostic representations of a user's immediate intent. A hierarchical candidate-aware network then injects this rich contextual signal into the ranking stage, refining them with cross-attention to align with both user history and the target item. The entire model is trained end-to-end, creating a more aligned and effective CTR prediction pipeline. Extensive experiments on three widely used datasets demonstrate the effectiveness of our approach.

</details>


### [127] [Orchestrating Specialized Agents for Trustworthy Enterprise RAG](https://arxiv.org/abs/2601.18267)
*Xincheng You,Qi Sun,Neha Bora,Huayi Li,Shubham Goel,Kang Li,Sean Culatana*

Main category: cs.IR

TL;DR: 提出ADORE框架解决RAG在高风险决策场景的不足，有三项贡献，评估表现佳。


<details>
  <summary>Details</summary>
Motivation: RAG在高风险决策场景表现不佳，单遍检索与写入流程存在问题。

Method: 引入ADORE框架，以迭代、用户引导的调查取代线性检索，利用结构化记忆库，有记忆锁定合成、证据覆盖引导执行和章节打包长上下文接地三项关键方法。

Result: ADORE在DeepResearch Bench排第一，在DeepConsult的头对头偏好胜率最高。

Conclusion: ADORE框架能有效解决RAG在高风险场景的问题，提升报告生成质量。

Abstract: Retrieval-Augmented Generation (RAG) shows promise for enterprise knowledge work, yet it often underperforms in high-stakes decision settings that require deep synthesis, strict traceability, and recovery from underspecified prompts. One-pass retrieval-and-write pipelines frequently yield shallow summaries, inconsistent grounding, and weak mechanisms for completeness verification. We introduce ADORE (Adaptive Deep Orchestration for Research in Enterprise), an agentic framework that replaces linear retrieval with iterative, user-steered investigation coordinated by a central orchestrator and a set of specialized agents. ADORE's key insight is that a structured Memory Bank (a curated evidence store with explicit claim-evidence linkage and section-level admissible evidence) enables traceable report generation and systematic checks for evidence completeness. Our contributions are threefold: (1) Memory-locked synthesis - report generation is constrained to a structured Memory Bank (Claim-Evidence Graph) with section-level admissible evidence, enabling traceable claims and grounded citations; (2) Evidence-coverage-guided execution - a retrieval-reflection loop audits section-level evidence coverage to trigger targeted follow-up retrieval and terminates via an evidence-driven stopping criterion; (3) Section-packed long-context grounding - section-level packing, pruning, and citation-preserving compression make long-form synthesis feasible under context limits. Across our evaluation suite, ADORE ranks first on DeepResearch Bench (52.65) and achieves the highest head-to-head preference win rate on DeepConsult (77.2%) against commercial systems.

</details>


### [128] [TopKGAT: A Top-K Objective-Driven Architecture for Recommendation](https://arxiv.org/abs/2601.18432)
*Sirui Chen,Jiawei Chen,Canghong Jin,Sheng Zhou,Jingbang Chen,Wujie Sun,Can Wang*

Main category: cs.IR

TL;DR: 提出TopKGAT推荐架构，源于top - K指标可微近似，实验表明其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统架构设计未明确与top - K目标对齐，限制了有效性。

Method: 提出TopKGAT架构，其单层前向计算与Precision@K指标的梯度上升动态一致，结构类似图注意力网络。

Result: 在四个基准数据集上的实验显示，TopKGAT始终优于最先进的基线。

Conclusion: TopKGAT能有效提升top - K推荐的准确性，代码开源。

Abstract: Recommendation systems (RS) aim to retrieve the top-K items most relevant to users, with metrics such as Precision@K and Recall@K commonly used to assess effectiveness. The architecture of an RS model acts as an inductive bias, shaping the patterns the model is inclined to learn. In recent years, numerous recommendation architectures have emerged, spanning traditional matrix factorization, deep neural networks, and graph neural networks. However, their designs are often not explicitly aligned with the top-K objective, thereby limiting their effectiveness.
  To address this limitation, we propose TopKGAT, a novel recommendation architecture directly derived from a differentiable approximation of top-K metrics. The forward computation of a single TopKGAT layer is intrinsically aligned with the gradient ascent dynamics of the Precision@K metric, enabling the model to naturally improve top-K recommendation accuracy. Structurally, TopKGAT resembles a graph attention network and can be implemented efficiently. Extensive experiments on four benchmark datasets demonstrate that TopKGAT consistently outperforms state-of-the-art baselines. The code is available at https://github.com/StupidThree/TopKGAT.

</details>


### [129] [Token-level Collaborative Alignment for LLM-based Generative Recommendation](https://arxiv.org/abs/2601.18457)
*Fake Lin,Binbin Hu,Zhi Zheng,Xi Zhu,Ziqi Liu,Zhiqiang Zhang,Jun Zhou,Tong Xu*

Main category: cs.IR

TL;DR: 现有基于大语言模型的推荐系统难融入协同过滤信号，提出TCA4Rec框架解决该问题并提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的推荐系统难以有效融入协同过滤信号，传统方法无法让协同过滤明确调控大语言模型生成。

Method: 提出TCA4Rec框架，包含协同分词器和软标签对齐，将协同过滤信号与大语言模型生成相结合。

Result: TCA4Rec与多种传统协同过滤模型和基于大语言模型的推荐架构兼容，能平衡行为对齐和语义流畅性。

Conclusion: 大量实验表明TCA4Rec能持续提升多种协同过滤模型和基于大语言模型的推荐系统的性能。

Abstract: Large Language Models (LLMs) have demonstrated strong potential for generative recommendation by leveraging rich semantic knowledge. However, existing LLM-based recommender systems struggle to effectively incorporate collaborative filtering (CF) signals, due to a fundamental mismatch between item-level preference modeling in CF and token-level next-token prediction (NTP) optimization in LLMs. Prior approaches typically treat CF as contextual hints or representation bias, and resort to multi-stage training to reduce behavioral semantic space discrepancies, leaving CF unable to explicitly regulate LLM generation. In this work, we propose Token-level Collaborative Alignment for Recommendation (TCA4Rec), a model-agnostic and plug-and-play framework that establishes an explicit optimization-level interface between CF supervision and LLM generation. TCA4Rec consists of (i) Collaborative Tokenizer, which projects raw item-level CF logits into token-level distributions aligned with the LLM token space, and (ii) Soft Label Alignment, which integrates these CF-informed distributions with one-hot supervision to optimize a soft NTP objective. This design preserves the generative nature of LLM training while enabling collaborative alignment with essential user preference of CF models. We highlight TCA4Rec is compatible with arbitrary traditional CF models and generalizes across a wide range of decoder-based LLM recommender architectures. Moreover, it provides an explicit mechanism to balance behavioral alignment and semantic fluency, yielding generative recommendations that are both accurate and controllable. Extensive experiments demonstrate that TCA4Rec consistently improves recommendation performance across a broad spectrum of CF models and LLM-based recommender systems.

</details>


### [130] [Feature-Indexed Federated Recommendation with Residual-Quantized Codebooks](https://arxiv.org/abs/2601.18570)
*Mingzhe Han,Jiahao Liu,Dongsheng Li,Hansu Gu,Peng Zhang,Ning Gu,Tun Lu*

Main category: cs.IR

TL;DR: 现有联邦推荐方法的ID索引通信范式有局限，提出特征索引通信范式及RQFedRec模型，实验证明其性能优且降低通信开销。


<details>
  <summary>Details</summary>
Motivation: 现有联邦推荐的ID索引通信范式存在消耗通信资源、信息泛化能力差、对噪声敏感等问题，需改进。

Method: 提出特征索引通信范式，构建RQFedRec模型，通过残差量化Kmeans分配离散代码ID，生成并训练代码本，采用协作 - 语义双通道聚合与课程策略。

Result: 在真实数据集上，RQFedRec持续优于现有联邦推荐基线，大幅降低通信开销。

Conclusion: RQFedRec有效解决了现有联邦推荐方法的问题，在性能和通信开销上表现良好。

Abstract: Federated recommendation provides a privacy-preserving solution for training recommender systems without centralizing user interactions. However, existing methods follow an ID-indexed communication paradigm that transmit whole item embeddings between clients and the server, which has three major limitations: 1) consumes uncontrollable communication resources, 2) the uploaded item information cannot generalize to related non-interacted items, and 3) is sensitive to client noisy feedback. To solve these problems, it is necessary to fundamentally change the existing ID-indexed communication paradigm. Therefore, we propose a feature-indexed communication paradigm that transmits feature code embeddings as codebooks rather than raw item embeddings. Building on this paradigm, we present RQFedRec, which assigns each item a list of discrete code IDs via Residual Quantization (RQ)-Kmeans. Each client generates and trains code embeddings as codebooks based on discrete code IDs provided by the server, and the server collects and aggregates these codebooks rather than item embeddings. This design makes communication controllable since the codebooks could cover all items, enabling updates to propagate across related items in same code ID. In addition, since code embedding represents many items, which is more robust to a single noisy item. To jointly capture semantic and collaborative information, RQFedRec further adopts a collaborative-semantic dual-channel aggregation with a curriculum strategy that emphasizes semantic codes early and gradually increases the contribution of collaborative codes over training. Extensive experiments on real-world datasets demonstrate that RQFedRec consistently outperforms state-of-the-art federated recommendation baselines while significantly reducing communication overhead.

</details>


### [131] [FastInsight: Fast and Insightful Retrieval via Fusion Operators for Graph RAG](https://arxiv.org/abs/2601.18579)
*Seonho An,Chaejeong Hyun,Min-Soo Kim*

Main category: cs.IR

TL;DR: 提出FastInsight实现高效洞察检索，通过新分类法指出当前方法局限，用新融合算子克服，实验显示其在检索和生成上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有图RAG方法依赖耗时的大语言模型推理过程，为实现高效洞察检索提出FastInsight。

Method: 引入图检索分类法，将现有方法分为三种基本操作；FastInsight采用Graph-based Reranker和Semantic-Topological eXpansion两个新融合算子克服当前方法局限。

Result: 在广泛的检索和生成数据集上的实验表明，FastInsight相比现有基线显著提高了检索准确性和生成质量。

Conclusion: FastInsight在有效性和效率的权衡上实现了显著的帕累托改进。

Abstract: Existing Graph RAG methods aiming for insightful retrieval on corpus graphs typically rely on time-intensive processes that interleave Large Language Model (LLM) reasoning. To enable time-efficient insightful retrieval, we propose FastInsight. We first introduce a graph retrieval taxonomy that categorizes existing methods into three fundamental operations: vector search, graph search, and model-based search. Through this taxonomy, we identify two critical limitations in current approaches: the topology-blindness of model-based search and the semantics-blindness of graph search. FastInsight overcomes these limitations by interleaving two novel fusion operators: the Graph-based Reranker (GRanker), which functions as a graph model-based search, and Semantic-Topological eXpansion (STeX), which operates as a vector-graph search. Extensive experiments on broad retrieval and generation datasets demonstrate that FastInsight significantly improves both retrieval accuracy and generation quality compared to state-of-the-art baselines, achieving a substantial Pareto improvement in the trade-off between effectiveness and efficiency.

</details>


### [132] [S$^2$GR: Stepwise Semantic-Guided Reasoning in Latent Space for Generative Recommendation](https://arxiv.org/abs/2601.18664)
*Zihao Guo,Jian Wang,Ruxin Zhou,Youhua Liu,Jiawei Guo,Jun Zhao,Xiaoxiao Xu,Yongqi Liu,Kaiqiao Zhan*

Main category: cs.IR

TL;DR: 现有生成式推荐方法存在不足，本文提出S²GR框架，通过代码本优化和逐步推理机制提升性能，实验和线上测试验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有GR方法难以激活深层次推理能力，推理增强的GR方法存在推理和生成步骤分离、推理向量缺乏可解释语义和推理路径监督不可验证等问题。

Method: 建立代码本优化的语义基础，融入物品共现关系，引入逐步推理机制，在每个SID生成步骤前插入思考令牌，通过对比学习监督。

Result: 广泛实验证明S²GR的优越性，在线A/B测试证实其在大规模工业短视频平台的有效性。

Conclusion: S²GR框架能有效提升生成式推荐的性能，解决现有方法存在的问题。

Abstract: Generative Recommendation (GR) has emerged as a transformative paradigm with its end-to-end generation advantages. However, existing GR methods primarily focus on direct Semantic ID (SID) generation from interaction sequences, failing to activate deeper reasoning capabilities analogous to those in large language models and thus limiting performance potential. We identify two critical limitations in current reasoning-enhanced GR approaches: (1) Strict sequential separation between reasoning and generation steps creates imbalanced computational focus across hierarchical SID codes, degrading quality for SID codes; (2) Generated reasoning vectors lack interpretable semantics, while reasoning paths suffer from unverifiable supervision. In this paper, we propose stepwise semantic-guided reasoning in latent space (S$^2$GR), a novel reasoning enhanced GR framework. First, we establish a robust semantic foundation via codebook optimization, integrating item co-occurrence relationship to capture behavioral patterns, and load balancing and uniformity objectives that maximize codebook utilization while reinforcing coarse-to-fine semantic hierarchies. Our core innovation introduces the stepwise reasoning mechanism inserting thinking tokens before each SID generation step, where each token explicitly represents coarse-grained semantics supervised via contrastive learning against ground-truth codebook cluster distributions ensuring physically grounded reasoning paths and balanced computational focus across all SID codes. Extensive experiments demonstrate the superiority of S$^2$GR, and online A/B test confirms efficacy on large-scale industrial short video platform.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [133] [TelcoAI: Advancing 3GPP Technical Specification Search through Agentic Multi-Modal Retrieval-Augmented Generation](https://arxiv.org/abs/2601.16984)
*Rahul Ghosh,Chun-Hao Liu,Gaurav Rele,Vidya Sagar Ravipati,Hazar Aouad*

Main category: cs.LG

TL;DR: 提出针对3GPP文档的多模态检索增强生成系统TelcoAI，评估效果良好，证明了智能体和多模态推理在技术文档理解中的有效性。


<details>
  <summary>Details</summary>
Motivation: 3GPP技术规范结构复杂、格式密集、内容多模态，处理困难，现有大语言模型方法在处理复杂查询、视觉信息和文档依赖方面不足。

Method: 提出TelcoAI系统，引入了节感知分块、结构化查询规划、元数据引导检索以及文本和图表的多模态融合。

Result: 在多个基准测试中，系统实现了87%的召回率、83%的声明召回率和92%的忠实度，比现有最先进基线提高了16%。

Conclusion: 智能体和多模态推理在技术文档理解中有效，推动了电信研究和工程的实际解决方案。

Abstract: The 3rd Generation Partnership Project (3GPP) produces complex technical specifications essential to global telecommunications, yet their hierarchical structure, dense formatting, and multi-modal content make them difficult to process. While Large Language Models (LLMs) show promise, existing approaches fall short in handling complex queries, visual information, and document interdependencies. We present TelcoAI, an agentic, multi-modal Retrieval-Augmented Generation (RAG) system tailored for 3GPP documentation. TelcoAI introduces section-aware chunking, structured query planning, metadata-guided retrieval, and multi-modal fusion of text and diagrams. Evaluated on multiple benchmarks-including expert-curated queries-our system achieves $87\%$ recall, $83\%$ claim recall, and $92\%$ faithfulness, representing a $16\%$ improvement over state-of-the-art baselines. These results demonstrate the effectiveness of agentic and multi-modal reasoning in technical document understanding, advancing practical solutions for real-world telecommunications research and engineering.

</details>


### [134] [SpecBridge: Bridging Mass Spectrometry and Molecular Representations via Cross-Modal Alignment](https://arxiv.org/abs/2601.17204)
*Yinkai Wang,Yan Zhou Chen,Xiaohui Chen,Li-Ping Liu,Soha Hassoun*

Main category: cs.LG

TL;DR: 本文提出SpecBridge框架用于串联质谱小分子识别，在多个基准测试中提升检索准确率，表明对齐预训练模型是新架构设计的可行替代方案。


<details>
  <summary>Details</summary>
Motivation: 在无靶向环境中，当光谱库不完整时，串联质谱小分子识别存在瓶颈，现有深度学习方法有局限性。

Method: 提出SpecBridge隐式对齐框架，微调自监督光谱编码器DreaMS以直接投影到冻结的分子基础模型ChemBERTa的潜在空间，再通过余弦相似度检索。

Result: 在MassSpecGym、Spectraverse和MSnLib基准测试中，相对于强大的神经基线，SpecBridge将top-1检索准确率提高约20 - 25%，同时可训练参数少。

Conclusion: 对齐冻结的基础模型是从头设计新架构的实用、稳定替代方案。

Abstract: Small-molecule identification from tandem mass spectrometry (MS/MS) remains a bottleneck in untargeted settings where spectral libraries are incomplete. While deep learning offers a solution, current approaches typically fall into two extremes: explicit generative models that construct molecular graphs atom-by-atom, or joint contrastive models that learn cross-modal subspaces from scratch. We introduce SpecBridge, a novel implicit alignment framework that treats structure identification as a geometric alignment problem. SpecBridge fine-tunes a self-supervised spectral encoder (DreaMS) to project directly into the latent space of a frozen molecular foundation model (ChemBERTa), and then performs retrieval by cosine similarity to a fixed bank of precomputed molecular embeddings. Across MassSpecGym, Spectraverse, and MSnLib benchmarks, SpecBridge improves top-1 retrieval accuracy by roughly 20-25% relative to strong neural baselines, while keeping the number of trainable parameters small. These results suggest that aligning to frozen foundation models is a practical, stable alternative to designing new architectures from scratch. The code for SpecBridge is released at https://github.com/HassounLab/SpecBridge.

</details>


### [135] [Sparsity-Aware Low-Rank Representation for Efficient Fine-Tuning of Large Language Models](https://arxiv.org/abs/2601.16991)
*Longteng Zhang,Sen Wu,Shuai Hou,Zhengyu Qing,Zhuo Zheng,Danning Ke,Qihong Lin,Qiang Wang,Shaohuai Shi,Xiaowen Chu*

Main category: cs.LG

TL;DR: 提出SALR微调范式统一低秩适应与稀疏剪枝，在多种LLM上实现50%稀疏性，减模型大小、提推理速度且性能相当


<details>
  <summary>Details</summary>
Motivation: 现有预训练语言模型微调方法存在在资源受限环境下使用的问题，LoRA和简单剪枝方法有弊端

Method: 在均方误差框架下统一低秩适应与稀疏剪枝，静态剪枝冻结的基础权重，用截断SVD低秩适配器恢复信息，融合低秩适配器，采用位图编码和两阶段解码+GEMM设计

Result: SALR在多种LLM上达到50%稀疏性，在GSM8K和MMLU上性能与LoRA相当，模型大小减半，推理速度最多提升1.7倍

Conclusion: SALR是一种有效的微调方法，能在保证性能的同时提高资源利用效率

Abstract: Adapting large pre-trained language models to downstream tasks often entails fine-tuning millions of parameters or deploying costly dense weight updates, which hinders their use in resource-constrained environments. Low-rank Adaptation (LoRA) reduces trainable parameters by factorizing weight updates, yet the underlying dense weights still impose high storage and computation costs. Magnitude-based pruning can yield sparse models but typically degrades LoRA's performance when applied naively. In this paper, we introduce SALR (Sparsity-Aware Low-Rank Representation), a novel fine-tuning paradigm that unifies low-rank adaptation with sparse pruning under a rigorous mean-squared-error framework. We prove that statically pruning only the frozen base weights minimizes the pruning error bound, and we recover the discarded residual information via a truncated-SVD low-rank adapter, which provably reduces per-entry MSE by a factor of $(1 - r/\min(d,k))$. To maximize hardware efficiency, we fuse multiple low-rank adapters into a single concatenated GEMM, and we adopt a bitmap-based encoding with a two-stage pipelined decoding + GEMM design to achieve true model compression and speedup. Empirically, SALR attains 50\% sparsity on various LLMs while matching the performance of LoRA on GSM8K and MMLU, reduces model size by $2\times$, and delivers up to a $1.7\times$ inference speedup.

</details>


### [136] [A Dataset of Dengue Hospitalizations in Brazil (1999 to 2021) with Weekly Disaggregation from Monthly Counts](https://arxiv.org/abs/2601.16994)
*Lucas M. Morello,Matheus Lima Castro,Pedro Cesar M. G. Camargo,Liliane Moreira Nery,Darllan Collins da Cunha e Silva,Leopoldo Lusquino Filho*

Main category: cs.LG

TL;DR: 论文公开一份登革热住院数据集，通过插值协议提高数据时间粒度至每周，对比插值策略，选三次样条插值生成序列，还含解释变量，介绍数据相关信息并给出使用建议。


<details>
  <summary>Details</summary>
Motivation: 为有效训练用于流行病学预测的AI模型，需提高原始月度数据的时间粒度。

Method: 用插值协议对巴西市级登革热住院时间序列进行分解至每周，用圣保罗州高分辨率参考数据集评估三种插值策略。

Result: 三次样条插值对参考数据的拟合度最高，采用此策略生成1999 - 2021年的每周序列。

Conclusion: 公开数据集，记录数据相关信息并给出多变量时间序列分析等方面的使用建议。

Abstract: This data paper describes and publicly releases this dataset (v1.0.0), published on Zenodo under DOI 10.5281/zenodo.18189192. Motivated by the need to increase the temporal granularity of originally monthly data to enable more effective training of AI models for epidemiological forecasting, the dataset harmonizes municipal-level dengue hospitalization time series across Brazil and disaggregates them to weekly resolution (epidemiological weeks) through an interpolation protocol with a correction step that preserves monthly totals. The statistical and temporal validity of this disaggregation was assessed using a high-resolution reference dataset from the state of Sao Paulo (2024), which simultaneously provides monthly and epidemiological-week counts, enabling a direct comparison of three strategies: linear interpolation, jittering, and cubic spline. Results indicated that cubic spline interpolation achieved the highest adherence to the reference data, and this strategy was therefore adopted to generate weekly series for the 1999 to 2021 period. In addition to hospitalization time series, the dataset includes a comprehensive set of explanatory variables commonly used in epidemiological and environmental modeling, such as demographic density, CH4, CO2, and NO2 emissions, poverty and urbanization indices, maximum temperature, mean monthly precipitation, minimum relative humidity, and municipal latitude and longitude, following the same temporal disaggregation scheme to ensure multivariate compatibility. The paper documents the datasets provenance, structure, formats, licenses, limitations, and quality metrics (MAE, RMSE, R2, KL, JSD, DTW, and the KS test), and provides usage recommendations for multivariate time-series analysis, environmental health studies, and the development of machine learning and deep learning models for outbreak forecasting.

</details>


### [137] [MathMixup: Boosting LLM Mathematical Reasoning with Difficulty-Controllable Data Synthesis and Curriculum Learning](https://arxiv.org/abs/2601.17006)
*Xuchen Li,Jing Chen,Xuzhao Li,Hao Liang,Xiaohuan Zhou,Taifeng Wang,Wentao Zhang*

Main category: cs.LG

TL;DR: 提出MathMixup数据合成范式及课程学习策略提升大语言模型数学推理能力，Qwen2.5 - 7B在基准测试中表现超现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有数据合成方法多样性有限且难以精确控制问题难度，无法支持课程学习等高效训练范式。

Method: 提出MathMixup范式，通过混合和分解策略生成高质量、难度可控的数学推理问题，结合自动自检和人工筛选确保数据质量，构建MathMixupQA数据集并设计课程学习策略。

Result: MathMixup和课程学习策略显著提升大语言模型数学推理能力，Qwen2.5 - 7B在七个数学基准测试中平均得分52.6%，超越现有方法。

Conclusion: MathMixup有效且具有广泛适用性，可提升大语言模型数学推理能力并推动以数据为中心的课程学习。

Abstract: In mathematical reasoning tasks, the advancement of Large Language Models (LLMs) relies heavily on high-quality training data with clearly defined and well-graded difficulty levels. However, existing data synthesis methods often suffer from limited diversity and lack precise control over problem difficulty, making them insufficient for supporting efficient training paradigms such as curriculum learning. To address these challenges, we propose MathMixup, a novel data synthesis paradigm that systematically generates high-quality, difficulty-controllable mathematical reasoning problems through hybrid and decomposed strategies. Automated self-checking and manual screening are incorporated to ensure semantic clarity and a well-structured difficulty gradient in the synthesized data. Building on this, we construct the MathMixupQA dataset and design a curriculum learning strategy that leverages these graded problems, supporting flexible integration with other datasets. Experimental results show that MathMixup and its curriculum learning strategy significantly enhance the mathematical reasoning performance of LLMs. Fine-tuned Qwen2.5-7B achieves an average score of 52.6\% across seven mathematical benchmarks, surpassing previous state-of-the-art methods. These results fully validate the effectiveness and broad applicability of MathMixup in improving the mathematical reasoning abilities of LLMs and advancing data-centric curriculum learning.

</details>


### [138] [Bayesian Robust Financial Trading with Adversarial Synthetic Market Data](https://arxiv.org/abs/2601.17008)
*Haochong Xia,Simin Li,Ruixiao Xu,Zhixia Zhang,Hongxiang Wang,Zhiqian Liu,Teng Yao Long,Molei Qin,Chuqiao Zong,Bo An*

Main category: cs.LG

TL;DR: 提出贝叶斯鲁棒框架解决算法交易中模型在真实市场表现不佳问题，实验显示优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有算法交易的机器学习模型在面对真实市场制度变化时性能下降，存在政策鲁棒性不足和缺乏真实多样模拟环境的问题。

Method: 提出贝叶斯鲁棒框架，数据端用宏观条件GAN生成器，政策端将交易过程视为两人零和贝叶斯马尔可夫博弈，交易代理通过贝叶斯神经虚拟自玩寻找鲁棒完美贝叶斯均衡。

Result: 在9种金融工具上的实验表明该框架优于9种最先进基线，在极端事件中展现出更好的盈利能力和风险管理能力。

Conclusion: 该框架为不确定和不断变化的市场动态下的交易提供了可靠解决方案。

Abstract: Algorithmic trading relies on machine learning models to make trading decisions. Despite strong in-sample performance, these models often degrade when confronted with evolving real-world market regimes, which can shift dramatically due to macroeconomic changes-e.g., monetary policy updates or unanticipated fluctuations in participant behavior. We identify two challenges that perpetuate this mismatch: (1) insufficient robustness in existing policy against uncertainties in high-level market fluctuations, and (2) the absence of a realistic and diverse simulation environment for training, leading to policy overfitting. To address these issues, we propose a Bayesian Robust Framework that systematically integrates a macro-conditioned generative model with robust policy learning. On the data side, to generate realistic and diverse data, we propose a macro-conditioned GAN-based generator that leverages macroeconomic indicators as primary control variables, synthesizing data with faithful temporal, cross-instrument, and macro correlations. On the policy side, to learn robust policy against market fluctuations, we cast the trading process as a two-player zero-sum Bayesian Markov game, wherein an adversarial agent simulates shifting regimes by perturbing macroeconomic indicators in the macro-conditioned generator, while the trading agent-guided by a quantile belief network-maintains and updates its belief over hidden market states. The trading agent seeks a Robust Perfect Bayesian Equilibrium via Bayesian neural fictitious self-play, stabilizing learning under adversarial market perturbations. Extensive experiments on 9 financial instruments demonstrate that our framework outperforms 9 state-of-the-art baselines. In extreme events like the COVID, our method shows improved profitability and risk management, offering a reliable solution for trading under uncertain and shifting market dynamics.

</details>


### [139] [Robust Learning of a Group DRO Neuron](https://arxiv.org/abs/2601.18115)
*Guyang Cao,Shuyao Li,Sushrut Karmalkar,Jelena Diakonikolas*

Main category: cs.LG

TL;DR: 研究在任意标签噪声和组级分布偏移下学习单个神经元问题，提出对偶算法并获计算效率结果，在LLM预训练基准测试展现潜力。


<details>
  <summary>Details</summary>
Motivation: 在存在任意标签噪声和组级分布偏移情况，为广泛协变量分布找到在最具挑战性群体加权下表现良好的 “最佳拟合” 神经元。

Method: 开发计算高效的原始 - 对偶算法，直接应对损失函数的非凸性。

Result: 算法输出向量与最优参数在最坏群体加权下有恒定因子竞争力，在LLM预训练基准测试中展现潜力。

Conclusion: 所提算法能为任意标签损坏和特定群体分布偏移提供鲁棒学习保证。

Abstract: We study the problem of learning a single neuron under standard squared loss in the presence of arbitrary label noise and group-level distributional shifts, for a broad family of covariate distributions. Our goal is to identify a ''best-fit'' neuron parameterized by $\mathbf{w}_*$ that performs well under the most challenging reweighting of the groups. Specifically, we address a Group Distributionally Robust Optimization problem: given sample access to $K$ distinct distributions $\mathcal p_{[1]},\dots,\mathcal p_{[K]}$, we seek to approximate $\mathbf{w}_*$ that minimizes the worst-case objective over convex combinations of group distributions $\boldsymbolλ \in Δ_K$, where the objective is $\sum_{i \in [K]}λ_{[i]}\,\mathbb E_{(\mathbf x,y)\sim\mathcal p_{[i]}}(σ(\mathbf w\cdot\mathbf x)-y)^2 - νd_f(\boldsymbolλ,\frac{1}{K}\mathbf1)$ and $d_f$ is an $f$-divergence that imposes (optional) penalty on deviations from uniform group weights, scaled by a parameter $ν\geq 0$. We develop a computationally efficient primal-dual algorithm that outputs a vector $\widehat{\mathbf w}$ that is constant-factor competitive with $\mathbf{w}_*$ under the worst-case group weighting. Our analytical framework directly confronts the inherent nonconvexity of the loss function, providing robust learning guarantees in the face of arbitrary label corruptions and group-specific distributional shifts. The implementation of the dual extrapolation update motivated by our algorithmic framework shows promise on LLM pre-training benchmarks.

</details>


### [140] [Analysis of voice recordings features for Classification of Parkinson's Disease](https://arxiv.org/abs/2601.17007)
*Beatriz Pérez-Sánchez,Noelia Sánchez-Maroño,Miguel A. Díaz-Freire*

Main category: cs.LG

TL;DR: 本文探讨用机器学习结合特征选择方法诊断帕金森病，发现神经网络适合分类，且可大幅减少特征数不影响性能。


<details>
  <summary>Details</summary>
Motivation: 帕金森病早期诊断困难，语音记录有助于早期诊断，但不确定所有语音特征是否与诊断相关，需有效方法辅助诊断。

Method: 使用不同类型的机器学习模型结合特征选择方法，利用选择技术确定提供最多信息的特征，减少分类器使用的特征数量。

Result: 机器学习方法，尤其是神经网络，适用于帕金森病分类，且可显著减少特征数量而不影响模型性能。

Conclusion: 通过特征选择使用机器学习模型可有效进行帕金森病诊断，减少特征数不影响表现。

Abstract: Parkinson's disease (PD) is a chronic neurodegenerative disease. Early diagnosis is essential to mitigate the progressive deterioration of patients' quality of life. The most characteristic motor symptoms are very mild in the early stages, making diagnosis difficult. Recent studies have shown that the use of patient voice recordings can aid in early diagnosis. Although the analysis of such recordings is costly from a clinical point of view, advances in machine learning techniques are making the processing of this type of data increasingly accurate and efficient. Vocal recordings contain many features, but it is not known whether all of them are relevant for diagnosing the disease.
  This paper proposes the use of different types of machine learning models combined with feature selection methods to detect the disease. The selection techniques allow to reduce the number of features used by the classifiers by determining which ones provide the most information about the problem. The results show that machine learning methods, in particular neural networks, are suitable for PD classification and that the number of features can be significantly reduced without affecting the performance of the models.

</details>


### [141] [Optimizing the Landscape of LLM Embeddings with Dynamic Exploratory Graph Analysis for Generative Psychometrics: A Monte Carlo Study](https://arxiv.org/abs/2601.17010)
*Hudson Golino*

Main category: cs.LG

TL;DR: 本文提出将大语言模型嵌入视为可搜索景观，采用DynEGA方法研究，结果显示TEFI和NMI在嵌入景观中有不同优化轨迹，加权复合标准可确定合适嵌入深度。


<details>
  <summary>Details</summary>
Motivation: 当前对大语言模型嵌入的应用将其视为静态、横断面表示，假设所有嵌入坐标贡献一致，忽略了最佳结构信息可能集中在特定区域的可能性。

Method: 将嵌入重构为可搜索景观，采用Dynamic Exploratory Graph Analysis (DynEGA) 系统遍历嵌入坐标，进行大规模蒙特卡罗模拟。

Result: TEFI和NMI在嵌入景观中有不同优化轨迹，单指标优化产生结构不一致的解，加权复合标准能识别平衡准确性和组织性的嵌入深度区域，最佳嵌入深度与项目池大小有系统关系。

Conclusion: 嵌入景观是非均匀语义空间，需要进行有原则的优化，而非默认使用全向量。

Abstract: Large language model (LLM) embeddings are increasingly used to estimate dimensional structure in psychological item pools prior to data collection, yet current applications treat embeddings as static, cross-sectional representations. This approach implicitly assumes uniform contribution across all embedding coordinates and overlooks the possibility that optimal structural information may be concentrated in specific regions of the embedding space. This study reframes embeddings as searchable landscapes and adapts Dynamic Exploratory Graph Analysis (DynEGA) to systematically traverse embedding coordinates, treating the dimension index as a pseudo-temporal ordering analogous to intensive longitudinal trajectories. A large-scale Monte Carlo simulation embedded items representing five dimensions of grandiose narcissism using OpenAI's text-embedding-3-small model, generating network estimations across systematically varied item pool sizes (3-40 items per dimension) and embedding depths (3-1,298 dimensions). Results reveal that Total Entropy Fit Index (TEFI) and Normalized Mutual Information (NMI) leads to competing optimization trajectories across the embedding landscape. TEFI achieves minima at deep embedding ranges (900--1,200 dimensions) where entropy-based organization is maximal but structural accuracy degrades, whereas NMI peaks at shallow depths where dimensional recovery is strongest but entropy-based fit remains suboptimal. Single-metric optimization produces structurally incoherent solutions, whereas a weighted composite criterion identifies embedding dimensions depth regions that jointly balance accuracy and organization. Optimal embedding depth scales systematically with item pool size. These findings establish embedding landscapes as non-uniform semantic spaces requiring principled optimization rather than default full-vector usage.

</details>


### [142] [FlashMoE: Reducing SSD I/O Bottlenecks via ML-Based Cache Replacement for Mixture-of-Experts Inference on Edge Devices](https://arxiv.org/abs/2601.17063)
*Byeongju Kim,Jungwan Lee,Donghyeon Han,Hoi-Jun Yoo,Sangyeob Kim*

Main category: cs.LG

TL;DR: 提出FlashMoE系统将非活跃专家模块卸载到SSD以解决内存受限场景下MoE模型推理问题，相比现有策略和系统有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有MoE推理系统依赖DRAM卸载，不适用于内存受限的设备环境，随着模型增大，RAM卸载方案变得不切实际。

Method: 提出FlashMoE系统，将非活跃专家模块卸载到SSD，采用轻量级基于ML的缓存策略结合近因和频率信号以最大化专家重用，同时构建用户级桌面平台进行验证。

Result: 在真实硬件上，FlashMoE比LRU和LFU等卸载策略的缓存命中率提高达51%，与现有MoE推理系统相比速度提升达2.6倍。

Conclusion: FlashMoE能在有限内存下实现高效的MoE推理，具有实际应用价值。

Abstract: Recently, Mixture-of-Experts (MoE) models have gained attention for efficiently scaling large language models. Although these models are extremely large, their sparse activation enables inference to be performed by accessing only a fraction of the model at a time. This property opens the possibility of on-device inference of MoE, which was previously considered infeasible for such large models. Consequently, various systems have been proposed to leverage this sparsity and enable efficient MoE inference for edge devices. However, previous MoE inference systems like Fiddler[8] or DAOP[13] rely on DRAM-based offloading and are not suitable for memory constrained on-device environments. As recent MoE models grow to hundreds of gigabytes, RAM-offloading solutions become impractical. To address this, we propose FlashMoE, a system that offloads inactive experts to SSD, enabling efficient MoE inference under limited RAM. FlashMoE incorporates a lightweight ML-based caching strategy that adaptively combines recency and frequency signals to maximize expert reuse, significantly reducing storage I/O. In addition, we built a user-grade desktop platform to demonstrate the practicality of FlashMoE. On this real hardware setup, FlashMoE improves cache hit rate by up to 51% over well-known offloading policies such as LRU and LFU, and achieves up to 2.6x speedup compared to existing MoE inference systems.

</details>


### [143] [Resonant Sparse Geometry Networks](https://arxiv.org/abs/2601.18064)
*Hasi Hays*

Main category: cs.LG

TL;DR: 介绍RSGN架构，具自组织稀疏分层输入依赖连接性，分析复杂度并实验验证其高效性，表明脑启发原则有前景。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer架构计算复杂度高，需更高效和生物合理的神经网络架构。

Method: 提出RSGN架构，在学习的双曲空间嵌入计算节点，分快慢两个时间尺度操作，进行严格数学分析和实验评估。

Result: RSGN计算复杂度为O(n*k)，长程依赖任务准确率96.5%，参数比标准Transformer少约15倍；20类分层分类任务准确率23.8%，参数比Transformer基线少近10倍。

Conclusion: 脑启发的稀疏、几何组织计算原则为更高效和生物合理的神经架构提供了有前景的方向。

Abstract: We introduce Resonant Sparse Geometry Networks (RSGN), a brain-inspired architecture with self-organizing sparse
  hierarchical input-dependent connectivity. Unlike Transformer architectures that employ dense attention mechanisms with
  O(n^2) computational complexity, RSGN embeds computational nodes in learned hyperbolic space where connection strength
  decays with geodesic distance, achieving dynamic sparsity that adapts to each input. The architecture operates on two
  distinct timescales: fast differentiable activation propagation optimized through gradient descent, and slow
  Hebbian-inspired structural learning for connectivity adaptation through local correlation rules. We provide rigorous
  mathematical analysis demonstrating that RSGN achieves O(n*k) computational complexity, where k << n represents the average
  active neighborhood size. Experimental evaluation on hierarchical classification and long-range dependency tasks
  demonstrates that RSGN achieves 96.5% accuracy on long-range dependency tasks while using approximately 15x fewer
  parameters than standard Transformers. On challenging hierarchical classification with 20 classes, RSGN achieves 23.8%
  accuracy (compared to 5% random baseline) with only 41,672 parameters, nearly 10x fewer than the Transformer baselines
  which require 403,348 parameters to achieve 30.1% accuracy. Our ablation studies confirm the contribution of each architectural
  component, with Hebbian learning providing consistent improvements. These results suggest that brain-inspired principles
  of sparse, geometrically-organized computation offer a promising direction toward more efficient and biologically plausible
  neural architectures.

</details>


### [144] [ThinkTank-ME: A Multi-Expert Framework for Middle East Event Forecasting](https://arxiv.org/abs/2601.17065)
*Haoxuan Li,He Chang,Yunshan Ma,Yi Bin,Yang Yang,See-Kiong Ng,Tat-Seng Chua*

Main category: cs.LG

TL;DR: 现有基于大语言模型的事件预测方法有局限，提出ThinkTank - ME框架和POLECAT - FOR - ME基准，实验显示多专家协作优势，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的方法采用单模型架构，难以捕捉复杂地区背景下的地缘政治细微差别，需改进事件预测方法。

Method: 引入ThinkTank - ME框架用于中东事件预测，构建POLECAT - FOR - ME基准。

Result: 实验结果表明多专家协作在处理复杂的时间地缘政治预测任务中更具优势。

Conclusion: 多专家协作的方法在中东事件预测中表现更好，能解决现有单模型架构的局限。

Abstract: Event forecasting is inherently influenced by multifaceted considerations, including international relations, regional historical dynamics, and cultural contexts. However, existing LLM-based approaches employ single-model architectures that generate predictions along a singular explicit trajectory, constraining their ability to capture diverse geopolitical nuances across complex regional contexts. To address this limitation, we introduce ThinkTank-ME, a novel Think Tank framework for Middle East event forecasting that emulates collaborative expert analysis in real-world strategic decision-making. To facilitate expert specialization and rigorous evaluation, we construct POLECAT-FOR-ME, a Middle East-focused event forecasting benchmark. Experimental results demonstrate the superiority of multi-expert collaboration in handling complex temporal geopolitical forecasting tasks. The code is available at https://github.com/LuminosityX/ThinkTank-ME.

</details>


### [145] [Multi-Agent Deep Reinforcement Learning Under Constrained Communications](https://arxiv.org/abs/2601.17069)
*Shahil Shaik,Jonathon M. Smereka,Yue Wang*

Main category: cs.LG

TL;DR: 提出分布式多智能体强化学习框架DG - MAPPO，无需集中式训练或全局信息，在多个任务中优于CTDE基线。


<details>
  <summary>Details</summary>
Motivation: CTDE范式在训练时依赖全局状态信息，存在可扩展性、鲁棒性和泛化性瓶颈，在实际场景中易失效且重新训练成本高，而分布式方法可仅用局部信息和通信进行自适应。

Method: 开发分布式图注意力网络D - GAT进行全局状态推理，基于D - GAT开发DG - MAPPO框架，智能体利用局部观测、多跳通信和共享/平均奖励优化局部策略和价值函数。

Result: 在多个任务上的实验表明，DG - MAPPO始终优于强CTDE基线，在多种合作任务中实现了更好的协调。

Conclusion: DG - MAPPO为鲁棒协作提供了有原则且可扩展的解决方案，是首个完全消除对集中式信息依赖的方法。

Abstract: Centralized training with decentralized execution (CTDE) has been the dominant paradigm in multi-agent reinforcement learning (MARL), but its reliance on global state information during training introduces scalability, robustness, and generalization bottlenecks. Moreover, in practical scenarios such as adding/dropping teammates or facing environment dynamics that differ from the training, CTDE methods can be brittle and costly to retrain, whereas distributed approaches allow agents to adapt using only local information and peer-to-peer communication. We present a distributed MARL framework that removes the need for centralized critics or global information. Firstly, we develop a novel Distributed Graph Attention Network (D-GAT) that performs global state inference through multi-hop communication, where agents integrate neighbor features via input-dependent attention weights in a fully distributed manner. Leveraging D-GAT, we develop the distributed graph-attention MAPPO (DG-MAPPO) -- a distributed MARL framework where agents optimize local policies and value functions using local observations, multi-hop communication, and shared/averaged rewards. Empirical evaluation on the StarCraftII Multi-Agent Challenge, Google Research Football, and Multi-Agent Mujoco demonstrates that our method consistently outperforms strong CTDE baselines, achieving superior coordination across a wide range of cooperative tasks with both homogeneous and heterogeneous teams. Our distributed MARL framework provides a principled and scalable solution for robust collaboration, eliminating the need for centralized training or global observability. To the best of our knowledge, DG-MAPPO appears to be the first to fully eliminate reliance on privileged centralized information, enabling agents to learn and act solely through peer-to-peer communication.

</details>


### [146] [Attention-Based Variational Framework for Joint and Individual Components Learning with Applications in Brain Network Analysis](https://arxiv.org/abs/2601.17073)
*Yifei Zhang,Meimei Liu,Zhengwu Zhang*

Main category: cs.LG

TL;DR: 提出CM - JIVNet框架用于整合脑结构和功能连接数据，在HCP - YA数据上验证有良好表现，可用于大规模多模态脑分析。


<details>
  <summary>Details</summary>
Motivation: 脑结构和功能连接数据整合对揭示驱动行为表型的跨模态模式至关重要，但数据的高维、非线性等问题阻碍有效整合。

Method: 提出Cross - Modal Joint - Individual Variational Network (CM - JIVNet)统一概率框架，利用多头注意力融合模块学习潜在表征，分离联合和特定模态信号。

Result: 在Human Connectome Project Young Adult (HCP - YA)数据验证，CM - JIVNet在跨模态重建和行为特征预测上表现优越。

Conclusion: CM - JIVNet有效分离联合和个体特征空间，为大规模多模态脑分析提供了强大、可解释和可扩展的解决方案。

Abstract: Brain organization is increasingly characterized through multiple imaging modalities, most notably structural connectivity (SC) and functional connectivity (FC). Integrating these inherently distinct yet complementary data sources is essential for uncovering the cross-modal patterns that drive behavioral phenotypes. However, effective integration is hindered by the high dimensionality and non-linearity of connectome data, complex non-linear SC-FC coupling, and the challenge of disentangling shared information from modality-specific variations. To address these issues, we propose the Cross-Modal Joint-Individual Variational Network (CM-JIVNet), a unified probabilistic framework designed to learn factorized latent representations from paired SC-FC datasets. Our model utilizes a multi-head attention fusion module to capture non-linear cross-modal dependencies while isolating independent, modality-specific signals. Validated on Human Connectome Project Young Adult (HCP-YA) data, CM-JIVNet demonstrates superior performance in cross-modal reconstruction and behavioral trait prediction. By effectively disentangling joint and individual feature spaces, CM-JIVNet provides a robust, interpretable, and scalable solution for large-scale multimodal brain analysis.

</details>


### [147] [SMART: Scalable Mesh-free Aerodynamic Simulations from Raw Geometries using a Transformer-based Surrogate Model](https://arxiv.org/abs/2601.18707)
*Jan Hagnberger,Mathias Niepert*

Main category: cs.LG

TL;DR: 介绍了一种名为SMART的神经代理模型，仅用点云表示几何形状预测物理量，无需模拟网格，实验表明其性能有竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有基于机器学习的代理模型使用模拟网格会增加计算成本，无网格方法则误差较高，因此开发新模型。

Method: 将几何和模拟参数编码到共享潜在空间，物理解码器利用编码器的中间潜在表示将空间查询映射到物理量，通过跨层交互联合更新潜在几何特征和演化物理场。

Result: SMART与依赖模拟网格作为输入的现有方法相比具有竞争力，且常表现更优。

Conclusion: SMART具备进行工业级模拟的能力。

Abstract: Machine learning-based surrogate models have emerged as more efficient alternatives to numerical solvers for physical simulations over complex geometries, such as car bodies. Many existing models incorporate the simulation mesh as an additional input, thereby reducing prediction errors. However, generating a simulation mesh for new geometries is computationally costly. In contrast, mesh-free methods, which do not rely on the simulation mesh, typically incur higher errors. Motivated by these considerations, we introduce SMART, a neural surrogate model that predicts physical quantities at arbitrary query locations using only a point-cloud representation of the geometry, without requiring access to the simulation mesh. The geometry and simulation parameters are encoded into a shared latent space that captures both structural and parametric characteristics of the physical field. A physics decoder then attends to the encoder's intermediate latent representations to map spatial queries to physical quantities. Through this cross-layer interaction, the model jointly updates latent geometric features and the evolving physical field. Extensive experiments show that SMART is competitive with and often outperforms existing methods that rely on the simulation mesh as input, demonstrating its capabilities for industry-level simulations.

</details>


### [148] [PhysE-Inv: A Physics-Encoded Inverse Modeling approach for Arctic Snow Depth Prediction](https://arxiv.org/abs/2601.17074)
*Akila Sampath,Vandana Janeja,Jianwu Wang*

Main category: cs.LG

TL;DR: 提出新颖框架PhysE - Inv估计北极雪深，提升预测表现并具备物理可解释性


<details>
  <summary>Details</summary>
Motivation: 现有基于过程和数据驱动的模型在估计北极雪深时存在对稀疏数据敏感或缺乏物理可解释性的问题

Method: 引入集成复杂序列架构和物理引导推理的PhysE - Inv框架，采用物理约束反演方法，利用静水力学平衡正向模型和潜在空间重建物理正则化

Result: 与现有基线模型相比，PhysE - Inv显著提升预测性能，减少20%误差，展现出更好的物理一致性和对数据稀疏性的适应性

Conclusion: 该方法为噪声容忍、可解释的反演建模开辟了道路，在地理空间和冰冻圈领域有广泛应用前景

Abstract: The accurate estimation of Arctic snow depth ($h_s$) remains a critical time-varying inverse problem due to the extreme scarcity and noise inherent in associated sea ice parameters. Existing process-based and data-driven models are either highly sensitive to sparse data or lack the physical interpretability required for climate-critical applications. To address this gap, we introduce PhysE-Inv, a novel framework that integrates a sophisticated sequential architecture, an LSTM Encoder-Decoder with Multi-head Attention and physics-guided contrastive learning, with physics-guided inference.Our core innovation lies in a surjective, physics-constrained inversion methodology. This methodology first leverages the hydrostatic balance forward model as a target-formulation proxy, enabling effective learning in the absence of direct $h_s$ ground truth; second, it uses reconstruction physics regularization over a latent space to dynamically discover hidden physical parameters from noisy, incomplete time-series input. Evaluated against state-of-the-art baselines, PhysE-Inv significantly improves prediction performance, reducing error by 20\% while demonstrating superior physical consistency and resilience to data sparsity compared to empirical methods. This approach pioneers a path for noise-tolerant, interpretable inverse modeling, with wide applicability in geospatial and cryospheric domains.

</details>


### [149] [E2PL: Effective and Efficient Prompt Learning for Incomplete Multi-view Multi-Label Class Incremental Learning](https://arxiv.org/abs/2601.17076)
*Jiajun Chen,Yue Wu,Kai Huang,Wen Xi,Yangyang Wu,Xiaoye Miao,Mengying Zhu,Meng Xi,Guanjie Cheng*

Main category: cs.LG

TL;DR: 本文提出不完整多视图多标签类增量学习任务（IMvMLCIL），并设计E2PL框架解决该任务，实验表明其效果和效率优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实网络环境存在视图缺失和类别动态增加问题，现有方法难以适应，缺乏可扩展性。

Method: 提出E2PL框架，统一任务定制提示和缺失感知提示设计，设计高效原型张量分解模块降低参数复杂度，采用动态对比学习策略增强鲁棒性。

Result: 在三个基准测试上，E2PL在效果和效率上始终优于现有方法。

Conclusion: E2PL框架能有效解决IMvMLCIL任务，具有良好的效果和效率。

Abstract: Multi-view multi-label classification (MvMLC) is indispensable for modern web applications aggregating information from diverse sources. However, real-world web-scale settings are rife with missing views and continuously emerging classes, which pose significant obstacles to robust learning. Prevailing methods are ill-equipped for this reality, as they either lack adaptability to new classes or incur exponential parameter growth when handling all possible missing-view patterns, severely limiting their scalability in web environments. To systematically address this gap, we formally introduce a novel task, termed \emph{incomplete multi-view multi-label class incremental learning} (IMvMLCIL), which requires models to simultaneously address heterogeneous missing views and dynamic class expansion. To tackle this task, we propose \textsf{E2PL}, an Effective and Efficient Prompt Learning framework for IMvMLCIL. \textsf{E2PL} unifies two novel prompt designs: \emph{task-tailored prompts} for class-incremental adaptation and \emph{missing-aware prompts} for the flexible integration of arbitrary view-missing scenarios. To fundamentally address the exponential parameter explosion inherent in missing-aware prompts, we devise an \emph{efficient prototype tensorization} module, which leverages atomic tensor decomposition to elegantly reduce the prompt parameter complexity from exponential to linear w.r.t. the number of views. We further incorporate a \emph{dynamic contrastive learning} strategy explicitly model the complex dependencies among diverse missing-view patterns, thus enhancing the model's robustness. Extensive experiments on three benchmarks demonstrate that \textsf{E2PL} consistently outperforms state-of-the-art methods in both effectiveness and efficiency. The codes and datasets are available at https://anonymous.4open.science/r/code-for-E2PL.

</details>


### [150] [Learning to Collaborate: An Orchestrated-Decentralized Framework for Peer-to-Peer LLM Federation](https://arxiv.org/abs/2601.17133)
*Inderjeet Singh,Eleonore Vissol-Gaudin,Andikan Otung,Motoyoshi Sekiya*

Main category: cs.LG

TL;DR: 本文提出了KNEXA - FL框架解决大语言模型微调的问题，实验显示其有显著效果，为构建去中心化AI生态奠定基础。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在特定领域微调时，数据需求与隐私主权冲突，传统联邦学习有单点故障和安全风险，去中心化联邦学习存在效率低和负迁移风险。

Method: 引入非聚合的中央分析/匹配器（CPM），将点对点协作建模为上下文多臂老虎机问题，使用LinUCB算法学习最优匹配策略，通过安全蒸馏实现基于PEFT的大语言模型代理间的知识交换。

Result: 在代码生成任务实验中，相比随机点对点协作，Pass@1提高约50%，且收敛稳定，而集中式蒸馏基线出现性能崩溃。

Conclusion: 基于学习的自适应编排是构建强大且有效的去中心化AI生态系统的基本原则。

Abstract: Fine-tuning Large Language Models (LLMs) for specialized domains is constrained by a fundamental challenge: the need for diverse, cross-organizational data conflicts with the principles of data privacy and sovereignty. While Federated Learning (FL) provides a framework for collaboration without raw data exchange, its classic centralized form introduces a single point of failure and remains vulnerable to model inversion attacks. Decentralized FL (DFL) mitigates this risk by removing the central aggregator but typically relies on inefficient, random peer-to-peer (P2P) pairings, forming a collaboration graph that is blind to agent heterogeneity and risks negative transfer. This paper introduces KNEXA-FL, a novel framework for orchestrated decentralization that resolves this trade-off. KNEXA-FL employs a non-aggregating Central Profiler/Matchmaker (CPM) that formulates P2P collaboration as a contextual bandit problem, using a LinUCB algorithm on abstract agent profiles to learn an optimal matchmaking policy. It orchestrates direct knowledge exchange between heterogeneous, PEFT-based LLM agents via secure distillation, without ever accessing the models themselves. Our comprehensive experiments on a challenging code generation task show that KNEXA-FL yields substantial gains, improving Pass@1 by approx. 50% relative to random P2P collaboration. Critically, our orchestrated approach demonstrates stable convergence, in stark contrast to a powerful centralized distillation baseline which suffers from catastrophic performance collapse. Our work establishes adaptive, learning-based orchestration as a foundational principle for building robust and effective decentralized AI ecosystems.

</details>


### [151] [SFO: Learning PDE Operators via Spectral Filtering](https://arxiv.org/abs/2601.17090)
*Noam Koren,Rafael Moschopoulos,Kira Radinsky,Elad Hazan*

Main category: cs.LG

TL;DR: 介绍谱滤波算子(SFO)，在六个基准测试中实现了最先进的准确率，误差相比强基线最多降低40%且参数更少。


<details>
  <summary>Details</summary>
Motivation: 偏微分方程控制复杂系统，但神经算子难以有效捕捉解映射中的长程、非局部相互作用。

Method: 引入SFO，用通用谱基(USB)对积分核进行参数化，证明离散格林函数在USB中有紧凑近似，仅学习快速衰减特征值的谱系数。

Result: 在六个基准测试中，SFO实现了最先进的准确率，相比强基线误差降低达40%，且使用更少参数。

Conclusion: SFO能高效捕捉偏微分方程解映射中的长程、非局部相互作用，表现良好。

Abstract: Partial differential equations (PDEs) govern complex systems, yet neural operators often struggle to efficiently capture the long-range, nonlocal interactions inherent in their solution maps. We introduce Spectral Filtering Operator (SFO), a neural operator that parameterizes integral kernels using the Universal Spectral Basis (USB), a fixed, global orthonormal basis derived from the eigenmodes of the Hilbert matrix in spectral filtering theory. Motivated by our theoretical finding that the discrete Green's functions of shift-invariant PDE discretizations exhibit spatial Linear Dynamical System (LDS) structure, we prove that these kernels admit compact approximations in the USB. By learning only the spectral coefficients of rapidly decaying eigenvalues, SFO achieves a highly efficient representation. Across six benchmarks, including reaction-diffusion, fluid dynamics, and 3D electromagnetics, SFO achieves state-of-the-art accuracy, reducing error by up to 40% relative to strong baselines while using substantially fewer parameters.

</details>


### [152] [Decentralized Multi-Agent Swarms for Autonomous Grid Security in Industrial IoT: A Consensus-based Approach](https://arxiv.org/abs/2601.17303)
*Samaresh Kumar Singh,Joyjit Roy*

Main category: cs.LG

TL;DR: 本文提出用于IIoT网络的去中心化多智能体群（DMAS）架构，经实验证明其性能优于集中式和边缘计算基线值。


<details>
  <summary>Details</summary>
Motivation: 工业物联网环境中，安全监控架构集中化产生严重延迟问题，易被攻击者利用。

Method: 提出DMAS架构，在每个边缘网关设置自主AI智能体，通过轻量级对等协议通信检测异常行为；还提出基于共识的威胁验证（CVT）过程。

Result: 在模拟2000个IIoT设备的测试床上实验，DMAS响应时间亚毫秒级（平均0.85ms），高负载下检测恶意活动准确率97.3%，检测零日攻击准确率87%，能防止工业控制系统实时级联故障，比基于云的解决方案减少89%网络带宽使用。

Conclusion: DMAS架构性能显著优于集中式和边缘计算，可有效保障IIoT网络安全。

Abstract: As Industrial Internet of Things (IIoT) environments expand to include tens of thousands of connected devices. The centralization of security monitoring architectures creates serious latency issues that savvy attackers can exploit to compromise an entire manufacturing ecosystem. This paper outlines a new, decentralized multi-agent swarm (DMAS) architecture that includes autonomous artificial intelligence (AI) agents at each edge gateway, functioning as a distributed digital "immune system" for IIoT networks. Instead of using a traditional static firewall approach, the DMAS agents communicate via a lightweight peer-to-peer protocol to cooperatively detect anomalous behavior across the IIoT network without sending data to a cloud infrastructure. The authors also outline a consensus-based threat validation (CVT) process in which agents vote on the threat level of an identified threat, enabling instant quarantine of a compromised node or nodes. The authors conducted experiments on a testbed that simulated an innovative factory environment with 2000 IIoT devices and found that the DMAS demonstrated sub-millisecond response times (average of 0.85ms), 97.3% accuracy in detecting malicious activity under high load, and 87% accuracy in detecting zero-day attacks. All significantly higher than baseline values for both centralized and edge computing. Additionally, the proposed architecture can prevent real-time cascading failures in industrial control systems and reduce network bandwidth use by 89% compared to cloud-based solutions.

</details>


### [153] [CUROCKET: Optimizing ROCKET for GPU](https://arxiv.org/abs/2601.17091)
*Ole Stüven,Keno Moenck,Thorsten Schüppstuhl*

Main category: cs.LG

TL;DR: 本文提出CUROCKET算法，可在GPU上高效执行ROCKET，每瓦计算效率比CPU版高11倍，代码开源。


<details>
  <summary>Details</summary>
Motivation: 当前ROCKET实现多在CPU执行，而卷积适合GPU并行计算以提速，但因ROCKET使用异构内核，标准GPU卷积方法低效，因此需提出新算法。

Method: 提出能在GPU上高效执行ROCKET的算法。

Result: 实现的CUROCKET算法每瓦计算效率比CPU上的ROCKET高11倍。

Conclusion: CUROCKET算法有效提升了ROCKET在GPU上的计算效率，其代码可在GitHub上获取。

Abstract: ROCKET (RandOm Convolutional KErnel Transform) is a feature extraction algorithm created for Time Series Classification (TSC), published in 2019. It applies convolution with randomly generated kernels on a time series, producing features that can be used to train a linear classifier or regressor like Ridge. At the time of publication, ROCKET was on par with the best state-of-the-art algorithms for TSC in terms of accuracy while being significantly less computationally expensive, making ROCKET a compelling algorithm for TSC. This also led to several subsequent versions, further improving accuracy and computational efficiency. The currently available ROCKET implementations are mostly bound to execution on CPU. However, convolution is a task that can be highly parallelized and is therefore suited to be executed on GPU, which speeds up the computation significantly. A key difficulty arises from the inhomogeneous kernels ROCKET uses, making standard methods for applying convolution on GPU inefficient. In this work, we propose an algorithm that is able to efficiently perform ROCKET on GPU and achieves up to 11 times higher computational efficiency per watt than ROCKET on CPU. The code for CUROCKET is available in this repository https://github.com/oleeven/CUROCKET on github.

</details>


### [154] [The Triangle of Similarity: A Multi-Faceted Framework for Comparing Neural Network Representations](https://arxiv.org/abs/2601.17093)
*Olha Sirikova,Alvin Chan*

Main category: cs.LG

TL;DR: 提出相似性三角框架比较神经网络表征，分析多种模型，有新发现并提供评估模型的整体方法。


<details>
  <summary>Details</summary>
Motivation: 现有比较神经网络表征的方法视角有限，需更好方法理解和验证模型。

Method: 提出相似性三角框架，结合静态表征相似性、功能相似性和稀疏性相似性三个互补视角。

Result: 架构家族是表征相似性的主要决定因素；剪枝时CKA自相似性和任务准确率强相关；剪枝可正则化部分模型对的表征。

Conclusion: 该框架为评估模型是否收敛到相似内部机制提供更整体方法，是科研中模型选择和分析的有用工具。

Abstract: Comparing neural network representations is essential for understanding and validating models in scientific applications. Existing methods, however, often provide a limited view. We propose the Triangle of Similarity, a framework that combines three complementary perspectives: static representational similarity (CKA/Procrustes), functional similarity (Linear Mode Connectivity or Predictive Similarity), and sparsity similarity (robustness under pruning). Analyzing a range of CNNs, Vision Transformers, and Vision-Language Models using both in-distribution (ImageNetV2) and out-of-distribution (CIFAR-10) testbeds, our initial findings suggest that: (1) architectural family is a primary determinant of representational similarity, forming distinct clusters; (2) CKA self-similarity and task accuracy are strongly correlated during pruning, though accuracy often degrades more sharply; and (3) for some model pairs, pruning appears to regularize representations, exposing a shared computational core. This framework offers a more holistic approach for assessing whether models have converged on similar internal mechanisms, providing a useful tool for model selection and analysis in scientific research.

</details>


### [155] [Boltzmann-GPT: Bridging Energy-Based World Models and Language Generation](https://arxiv.org/abs/2601.17094)
*Junichiro Niimi*

Main category: cs.LG

TL;DR: 提出分离世界模型和语言模型架构，在消费者评论领域验证，小语言模型连合适世界模型可实现可控生成。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型是否真理解世界，提出分离世界模型和语言模型架构。

Method: 架构含DBM、适配器和GPT - 2，在亚马逊手机评论领域实例化。

Result: 世界模型条件生成效果更好，DBM能量函数可区分市场配置，干预属性对生成文本有因果影响。

Conclusion: 小语言模型连合适世界模型可实现可控生成，支持分离语言能力和世界理解。

Abstract: Large Language Models (LLMs) generate fluent text, yet whether they truly understand the world or merely produce plausible language about it remains contested. We propose an architectural principle, the mouth is not the brain, that explicitly separates world models from language models. Our architecture comprises three components: a Deep Boltzmann Machine (DBM) that captures domain structure as an energy-based world model, an adapter that projects latent belief states into embedding space, and a frozen GPT-2 that provides linguistic competence without domain knowledge. We instantiate this framework in the consumer review domain using Amazon smartphone reviews. Experiments demonstrate that (1) conditioning through the world model yields significantly higher sentiment correlation, lower perplexity, and greater semantic similarity compared to prompt-based generation alone; (2) the DBM's energy function distinguishes coherent from incoherent market configurations, assigning higher energy to implausible brand-price combinations; and (3) interventions on specific attributes propagate causally to generated text with intervened outputs exhibiting distributions statistically consistent with naturally occurring samples sharing the target configuration. These findings suggest that even small-scale language models can achieve consistent, controllable generation when connected to an appropriate world model, providing empirical support for separating linguistic competence from world understanding.

</details>


### [156] [Kareus: Joint Reduction of Dynamic and Static Energy in Large Model Training](https://arxiv.org/abs/2601.17654)
*Ruofan Wu,Jae-Won Chung,Mosharaf Chowdhury*

Main category: cs.LG

TL;DR: AI计算需求增长快而能源供应不足，现有工作仅关注单一方面能耗，本文设计Kareus系统优化动静态能耗，在时间 - 能源权衡上表现出色。


<details>
  <summary>Details</summary>
Motivation: AI计算需求增长使能源成为需管理优化的资源，现有工作仅关注单一方面能耗，需综合优化。

Method: 发现细粒度内核调度和频率缩放共同影响能耗，将联合优化问题分解为基于分区的子问题，用多遍多目标优化算法寻找执行调度。

Result: 与现有技术相比，Kareus在相同训练时间下最多降低28.3%训练能耗，在相同能耗下最多减少27.5%训练时间。

Conclusion: Kareus系统通过优化动静态能耗，有效推动了时间 - 能源权衡边界。

Abstract: The computing demand of AI is growing at an unprecedented rate, but energy supply is not keeping pace. As a result, energy has become an expensive, contended resource that requires explicit management and optimization. Although recent works have made significant progress in large model training optimization, they focus only on a single aspect of energy consumption: dynamic or static energy.
  We find that fine-grained kernel scheduling and frequency scaling jointly and interdependently impact both dynamic and static energy consumption. Based on this finding, we design Kareus, a training system that pushes the time--energy tradeoff frontier by optimizing both aspects. Kareus decomposes the intractable joint optimization problem into local, partition-based subproblems. It then uses a multi-pass multi-objective optimization algorithm to find execution schedules that push the time--energy tradeoff frontier. Compared to the state of the art, Kareus reduces training energy by up to 28.3% at the same training time, or reduces training time by up to 27.5% at the same energy consumption.

</details>


### [157] [MambaNet: Mamba-assisted Channel Estimation Neural Network With Attention Mechanism](https://arxiv.org/abs/2601.17108)
*Dianxin Luan,Chengsi Liang,Jie Huang,Zheng Lin,Kaitao Meng,John Thompson,Cheng-Xiang Wang*

Main category: cs.LG

TL;DR: 本文提出含自注意力机制的Mamba辅助神经网络框架，用于低复杂度OFDM波形信道估计，模拟结果显示其性能好、参数少。


<details>
  <summary>Details</summary>
Motivation: 实现低复杂度的正交频分复用（OFDM）波形信道估计，尤其是大量子载波配置场景。

Method: 提出Mamba辅助的神经网络框架，集成自定义Mamba架构和双向选择性扫描。

Result: 在3GPP TS 36.101信道模拟测试中，相比其他基线神经网络方案，性能提升且可调参数减少。

Conclusion: 所提方法能有效处理大规模子载波信道估计，且空间复杂度低于基于Transformer的神经网络。

Abstract: This paper proposes a Mamba-assisted neural network framework incorporating self-attention mechanism to achieve improved channel estimation with low complexity for orthogonal frequency-division multiplexing (OFDM) waveforms, particularly for configurations with a large number of subcarriers. With the integration of customized Mamba architecture, the proposed framework handles large-scale subcarrier channel estimation efficiently while capturing long-distance dependencies among these subcarriers effectively. Unlike conventional Mamba structure, this paper implements a bidirectional selective scan to improve channel estimation performance, because channel gains at different subcarriers are non-causal. Moreover, the proposed framework exhibits relatively lower space complexity than transformer-based neural networks. Simulation results tested on the 3GPP TS 36.101 channel demonstrate that compared to other baseline neural network solutions, the proposed method achieves improved channel estimation performance with a reduced number of tunable parameters.

</details>


### [158] [Least-Loaded Expert Parallelism: Load Balancing An Imbalanced Mixture-of-Experts](https://arxiv.org/abs/2601.17111)
*Xuan-Phi Nguyen,Shrey Pandit,Austin Xu,Caiming Xiong,Shafiq Joty*

Main category: cs.LG

TL;DR: 现有MoE模型路由不平衡，标准EP算法有局限，提出LLEP算法动态重路由，在不同模型规模有加速和降低内存效果。


<details>
  <summary>Details</summary>
Motivation: 解决MoE模型路由不平衡导致标准专家并行（EP）算法在训练和推理时出现计算与内存问题。

Method: 提出Least-Loaded Expert Parallelism (LLEP)算法，动态将过载设备的多余令牌和专家参数重路由到未充分利用的设备。

Result: 不同模型规模下，LLEP比标准EP加速达5倍，峰值内存使用降低4倍，gpt - oss - 120b快约1.9倍。

Conclusion: LLEP算法能实现更快、更高吞吐量的训练和推理，研究结果为硬件超参数调优提供框架。

Abstract: Mixture-of-Experts (MoE) models are typically pre-trained with explicit load-balancing constraints to ensure statistically balanced expert routing. Despite this, we observe that even well-trained MoE models exhibit significantly imbalanced routing. This behavior is arguably natural-and even desirable - as imbalanced routing allows models to concentrate domain-specific knowledge within a subset of experts. Expert parallelism (EP) is designed to scale MoE models by distributing experts across multiple devices, but with a less-discussed assumption of balanced routing. Under extreme imbalance, EP can funnel a disproportionate number of tokens to a small number of experts, leading to compute- and memory-bound failures on overloaded devices during post-training or inference, where explicit load balancing is often inapplicable. We propose Least-Loaded Expert Parallelism (LLEP), a novel EP algorithm that dynamically reroutes excess tokens and associated expert parameters from overloaded devices to underutilized ones. This ensures that all devices complete their workloads within the minimum collective latency while respecting memory constraints. Across different model scales, LLEP achieves up to 5x speedup and 4x reduction in peak memory usage compared to standard EP. This enables faster and higher-throughput post-training and inference, with ~1.9x faster for gpt-oss-120b. We support our method with extensive theoretical analysis and comprehensive empirical evaluations, including ablation studies. These results illuminate key trade-offs and enable a principled framework for hardware-specific hyper-parameter tuning to achieve optimal performance.

</details>


### [159] [LLM-42: Enabling Determinism in LLM Inference with Verified Speculation](https://arxiv.org/abs/2601.17768)
*Raja Gond,Aditya K Kamath,Arkaprava Basu,Ramachandran Ramjee,Ashish Panwar*

Main category: cs.LG

TL;DR: 介绍LLM推理的非确定性成因及现有解决方法问题，提出调度式方法LLM - 42实现确定性推理，复用现有内核，按需产生开销。


<details>
  <summary>Details</summary>
Motivation: 现有消除LLM推理非确定性的方法会带来吞吐量下降、与内核设计强耦合、固定运行时开销等问题，需要更好的解决方案。

Method: 受推测解码启发，采用调度式方法LLM - 42，利用非确定性快速路径解码，通过轻量级验证回滚循环保证确定性。

Result: 未提及具体结果。

Conclusion: 未在给定内容中明确得出结论，但LLM - 42方法可复用现有内核，按需产生开销。

Abstract: In LLM inference, the same prompt may yield different outputs across different runs. At the system level, this non-determinism arises from floating-point non-associativity combined with dynamic batching and GPU kernels whose reduction orders vary with batch size. A straightforward way to eliminate non-determinism is to disable dynamic batching during inference, but doing so severely degrades throughput. Another approach is to make kernels batch-invariant; however, this tightly couples determinism to kernel design, requiring new implementations. This coupling also imposes fixed runtime overheads, regardless of how much of the workload actually requires determinism.
  Inspired by ideas from speculative decoding, we present LLM-42, a scheduling-based approach to enable determinism in LLM inference. Our key observation is that if a sequence is in a consistent state, the next emitted token is likely to be consistent even with dynamic batching. Moreover, most GPU kernels use shape-consistent reductions. Leveraging these insights, LLM-42 decodes tokens using a non-deterministic fast path and enforces determinism via a lightweight verify-rollback loop. The verifier replays candidate tokens under a fixed-shape reduction schedule, commits those that are guaranteed to be consistent across runs, and rolls back those violating determinism. LLM-42 mostly re-uses existing kernels unchanged and incurs overhead only in proportion to the traffic that requires determinism.

</details>


### [160] [Low-Rank Tensor Approximation of Weights in Large Language Models via Cosine Lanczos Bidiagonalization](https://arxiv.org/abs/2601.17112)
*A. El Ichi,K. Jbilou*

Main category: cs.LG

TL;DR: 提出基于cproduct的张量压缩框架用于低秩近似，可高效压缩大语言模型权重张量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在内存占用大、计算成本高的问题。

Method: 利用cproduct的代数结构，在变换域表示权重张量，用低秩张量因子联合近似正面切片。

Result: 未提及具体结果。

Conclusion: 未提及具体结论。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse natural language tasks but suffer from extremely large memory footprints and computational costs. In this paper, we introduce a tensor compression framework based on the cproduct for computing low rank approximation In the first part of our approach, we leverage the algebraic structure of the cproduct to represent weight tensors such as those in embedding layers, attention projections, and feed forward networks in a transform domain where frontal slices can be jointly approximated by low rank tensor factors. This enables computationally efficient compression that exploits multidimensional correlations beyond traditional SVD methods.

</details>


### [161] [How does Graph Structure Modulate Membership-Inference Risk for Graph Neural Networks?](https://arxiv.org/abs/2601.17130)
*Megha Khosla*

Main category: cs.LG

TL;DR: 研究图神经网络隐私泄露，分析图结构对节点成员推理影响，发现采样和边访问对推理的作用，指出泛化差距不能完全代表成员推理风险，还研究了差分隐私图神经网络的可审计性。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在敏感应用中存在训练数据泄露问题，以往研究多借鉴非图领域，需进行图特定分析。

Method: 形式化节点邻域元组的成员推理，研究训练图构建和推理时边访问两个维度，实证分析不同采样方式和边访问情况；适应图模型调整训练测试数据点统计可交换性定义。

Result: 雪球采样覆盖偏差影响泛化，推理时启用边可提高测试准确性、缩小差距和降低成员优势；泛化差距不能完全代表成员推理风险；节点级任务的归纳分割破坏可交换性。

Conclusion: 强调图特定分析对研究图神经网络隐私泄露的重要性，指出泛化差距与成员推理风险关系，以及差分隐私图神经网络可审计性的局限性。

Abstract: Graph neural networks (GNNs) have become the standard tool for encoding data and their complex relationships into continuous representations, improving prediction accuracy in several machine learning tasks like node classification and link prediction. However, their use in sensitive applications has raised concerns about the potential leakage of training data. Research on privacy leakage in GNNs has largely been shaped by findings from non-graph domains, such as images and tabular data. We emphasize the need of graph specific analysis and investigate the impact of graph structure on node level membership inference. We formalize MI over node-neighbourhood tuples and investigate two important dimensions: (i) training graph construction and (ii) inference-time edge access. Empirically, snowball's coverage bias often harms generalisation relative to random sampling, while enabling inter-train-test edges at inference improves test accuracy, shrinks the train-test gap, and yields the lowest membership advantage across most of the models and datasets. We further show that the generalisation gap empirically measured as the performance difference between the train and test nodes is an incomplete proxy for MI risk: access to edges dominates-MI can rise or fall independent of gap changes. Finally, we examine the auditability of differentially private GNNs, adapting the definition of statistical exchangeability of train-test data points for graph based models. We show that for node level tasks the inductive splits (random or snowball sampled) break exchangeability, limiting the applicability of standard bounds for membership advantage of differential private models.

</details>


### [162] [ConceptACT: Episode-Level Concepts for Sample-Efficient Robotic Imitation Learning](https://arxiv.org/abs/2601.17135)
*Jakob Karalus,Friedhelm Schwenker*

Main category: cs.LG

TL;DR: 提出ConceptACT方法，利用语义概念注释提升机器人模仿学习效率，实验显示其优于标准ACT。


<details>
  <summary>Details</summary>
Motivation: 当前机器人模仿学习方法仅依赖低级传感器运动数据，忽略人类任务语义知识，需提高学习效率。

Method: ConceptACT是Action Chunking with Transformers的扩展，在训练中使用情节级语义概念注释，用改进的变压器架构，通过概念感知交叉注意力集成概念。

Result: 实验表明ConceptACT收敛更快、样本效率更高，通过注意力机制的架构集成表现优于简单辅助预测损失或语言条件模型。

Conclusion: 适当整合语义监督能为机器人学习提供强大归纳偏置，提高学习效率。

Abstract: Imitation learning enables robots to acquire complex manipulation skills from human demonstrations, but current methods rely solely on low-level sensorimotor data while ignoring the rich semantic knowledge humans naturally possess about tasks. We present ConceptACT, an extension of Action Chunking with Transformers that leverages episode-level semantic concept annotations during training to improve learning efficiency. Unlike language-conditioned approaches that require semantic input at deployment, ConceptACT uses human-provided concepts (object properties, spatial relationships, task constraints) exclusively during demonstration collection, adding minimal annotation burden. We integrate concepts using a modified transformer architecture in which the final encoder layer implements concept-aware cross-attention, supervised to align with human annotations. Through experiments on two robotic manipulation tasks with logical constraints, we demonstrate that ConceptACT converges faster and achieves superior sample efficiency compared to standard ACT. Crucially, we show that architectural integration through attention mechanisms significantly outperforms naive auxiliary prediction losses or language-conditioned models. These results demonstrate that properly integrated semantic supervision provides powerful inductive biases for more efficient robot learning.

</details>


### [163] [Rank-1 Approximation of Inverse Fisher for Natural Policy Gradients in Deep Reinforcement Learning](https://arxiv.org/abs/2601.18626)
*Yingxiao Huo,Satya Prakash Dash,Radu Stoican,Samuel Kaski,Mingfei Sun*

Main category: cs.LG

TL;DR: 提出基于秩-1近似逆FIM的高效可扩展自然策略优化技术，理论证明收敛快且样本复杂度优，实验表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 传统自然梯度计算需迭代求FIM逆，计算成本高，需高效可扩展的自然策略优化技术。

Method: 利用秩-1近似全逆FIM的自然策略优化技术。

Result: 理论证明在一定条件下，秩-1近似逆FIM比策略梯度收敛快，部分条件下样本复杂度与随机策略梯度方法相同；实验表明在多种环境中表现优于标准演员-评论家及信任区域基线。

Conclusion: 所提技术有效且性能优越，为自然策略优化提供新途径。

Abstract: Natural gradients have long been studied in deep reinforcement learning due to their fast convergence properties and covariant weight updates. However, computing natural gradients requires inversion of the Fisher Information Matrix (FIM) at each iteration, which is computationally prohibitive in nature. In this paper, we present an efficient and scalable natural policy optimization technique that leverages a rank-1 approximation to full inverse-FIM. We theoretically show that under certain conditions, a rank-1 approximation to inverse-FIM converges faster than policy gradients and, under some conditions, enjoys the same sample complexity as stochastic policy gradient methods. We benchmark our method on a diverse set of environments and show that it achieves superior performance to standard actor-critic and trust-region baselines.

</details>


### [164] [Conservative & Aggressive NaNs Accelerate U-Nets for Neuroimaging](https://arxiv.org/abs/2601.17180)
*Inés Gonzalez-Pepe,Vinuyan Sivakolunthu,Jacob Fortin,Yohan Chatelain,Tristan Glatard*

Main category: cs.LG

TL;DR: 研究发现CNN中大量操作冗余，提出两种NaN变体方法减少计算、提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型架构增大，效率问题突出，CNN存在大量数值噪声主导的冗余操作。

Method: 引入Conservative & Aggressive NaNs两种NaN变体，识别不稳定体素并替换为NaN，跳过无关数据计算，在PyTorch中实现且无需架构改变。

Result: 对含至少50% NaNs输入有运行时间改进，超三分之二NaNs数据平均推理加速1.67x；Conservative NaNs平均减少30%卷积操作，特定层达64.64%；Aggressive NaNs最多跳过69.30%卷积，但可能影响性能。

Conclusion: 可利用数值不确定性减少CNN冗余计算，提升推理效率。

Abstract: Deep learning models for neuroimaging increasingly rely on large architectures, making efficiency a persistent concern despite advances in hardware. Through an analysis of numerical uncertainty of convolutional neural networks (CNNs), we observe that many operations are applied to values dominated by numerical noise and have negligible influence on model outputs. In some models, up to two-thirds of convolution operations appear redundant. We introduce Conservative & Aggressive NaNs, two novel variants of max pooling and unpooling that identify numerically unstable voxels and replace them with NaNs, allowing subsequent layers to skip computations on irrelevant data. Both methods are implemented within PyTorch and require no architectural changes. We evaluate these approaches on four CNN models spanning neuroimaging and image classification tasks. For inputs containing at least 50% NaNs, we observe consistent runtime improvements; for data with more than two-thirds NaNs )common in several neuroimaging settings) we achieve an average inference speedup of 1.67x. Conservative NaNs reduces convolution operations by an average of 30% across models and datasets, with no measurable performance degradation, and can skip up to 64.64% of convolutions in specific layers. Aggressive NaNs can skip up to 69.30% of convolutions but may occasionally affect performance. Overall, these methods demonstrate that numerical uncertainty can be exploited to reduce redundant computation and improve inference efficiency in CNNs.

</details>


### [165] [Federated Proximal Optimization for Privacy-Preserving Heart Disease Prediction: A Controlled Simulation Study on Non-IID Clinical Data](https://arxiv.org/abs/2601.17183)
*Farzam Asad,Junaid Saif Khan,Maria Tariq,Sundus Munir,Muhammad Adnan Khan*

Main category: cs.LG

TL;DR: 本文基于UCI心脏病数据集对Federated Proximal Optimization (FedProx) 进行心脏病预测的模拟研究，结果显示FedProx在不泄露患者隐私下表现更优，为实际联邦医疗系统提供见解和指南。


<details>
  <summary>Details</summary>
Motivation: 医疗数据受隐私法规限制无法共享，且临床数据集有非IID特征，需解决协作模型训练问题。

Method: 基于UCI心脏病数据集，模拟四个异构医院客户端生成非IID数据分区，进行FedProx模拟研究，并进行大量消融实验和统计验证。

Result: FedProx近端参数mu=0.05时准确率达85.00%，优于集中学习和孤立本地模型，近端正则化能有效抑制客户端漂移。

Conclusion: 研究为现实联邦医疗系统提供算法见解和实际部署指南，结果可直接应用于医院IT管理员实施隐私保护协作学习。

Abstract: Healthcare institutions have access to valuable patient data that could be of great help in the development of improved diagnostic models, but privacy regulations like HIPAA and GDPR prevent hospitals from directly sharing data with one another. Federated Learning offers a way out to this problem by facilitating collaborative model training without having the raw patient data centralized. However, clinical datasets intrinsically have non-IID (non-independent and identically distributed) features brought about by demographic disparity and diversity in disease prevalence and institutional practices. This paper presents a comprehensive simulation research of Federated Proximal Optimization (FedProx) for Heart Disease prediction based on UCI Heart Disease dataset. We generate realistic non-IID data partitions by simulating four heterogeneous hospital clients from the Cleveland Clinic dataset (303 patients), by inducing statistical heterogeneity by demographic-based stratification. Our experimental results show that FedProx with proximal parameter mu=0.05 achieves 85.00% accuracy, which is better than both centralized learning (83.33%) and isolated local models (78.45% average) without revealing patient privacy. Through generous sheer ablation studies with statistical validation on 50 independent runs we demonstrate that proximal regularization is effective in curbing client drift in heterogeneous environments. This proof-of-concept research offers algorithmic insights and practical deployment guidelines for real-world federated healthcare systems, and thus, our results are directly transferable to hospital IT-administrators, implementing privacy-preserving collaborative learning.

</details>


### [166] [Rethinking Benchmarks for Differentially Private Image Classification](https://arxiv.org/abs/2601.17189)
*Sabrina Mokhtari,Sara Kodeiri,Shubhankar Mohapatra,Florian Tramer,Gautam Kamath*

Main category: cs.LG

TL;DR: 重新审视差分隐私图像分类基准，提出综合基准集，测试已有技术并创建公开排行榜。


<details>
  <summary>Details</summary>
Motivation: 为差分隐私机器学习技术提供在多种场景下的评估手段。

Method: 提出一套综合基准集，在这些基准上测试既定技术。

Result: 完成对既定技术在不同场景下的测试，创建了公开排行榜。

Conclusion: 所提出的基准集有助于研究人员评估差分隐私机器学习技术，公开排行榜可追踪领域进展。

Abstract: We revisit benchmarks for differentially private image classification. We suggest a comprehensive set of benchmarks, allowing researchers to evaluate techniques for differentially private machine learning in a variety of settings, including with and without additional data, in convex settings, and on a variety of qualitatively different datasets. We further test established techniques on these benchmarks in order to see which ideas remain effective in different settings. Finally, we create a publicly available leader board for the community to track progress in differentially private machine learning.

</details>


### [167] [PUNCH: Physics-informed Uncertainty-aware Network for Coronary Hemodynamics](https://arxiv.org/abs/2601.17192)
*Sukirt Thakur,Marcus Roper,Yang Zhou,Reza Akbarian Bafghi,Brahmajee K. Nallamothu,C. Alberto Figueroa,Srinivas Paruchuri,Scott Burger,Maziar Raissi*

Main category: cs.LG

TL;DR: 提出非侵入性、带不确定性感知的框架从标准血管造影估计冠状动脉血流储备（CFR），经合成数据和临床验证效果好，可推动冠心病微血管功能评估。


<details>
  <summary>Details</summary>
Motivation: 冠状动脉微血管功能障碍（CMD）诊断不足，金标准生理测量有创且重复性可变，需非侵入性方法。

Method: 将物理信息神经网络与变分推理集成，从造影剂传输的第一性原理模型推断冠状动脉血流，无需真实流量测量。

Result: 在合成数据上能识别退化数据并输出合理不确定性估计；临床验证与有创测量高度一致，概率性CFR估计置信区间窄。

Conclusion: 该方法可将常规血管造影转变为定量、带不确定性评估，扩大CMD诊断途径，建立临床影像物理信息、患者特异性推断新范式。

Abstract: Coronary microvascular dysfunction (CMD) affects millions worldwide yet remains underdiagnosed because gold-standard physiological measurements are invasive and variably reproducible. We introduce a non-invasive, uncertainty-aware framework for estimating coronary flow reserve (CFR) directly from standard angiography. The system integrates physics-informed neural networks with variational inference to infer coronary blood flow from first-principles models of contrast transport, without requiring ground-truth flow measurements. The pipeline runs in approximately three minutes per patient on a single GPU, with no population-level training.
  Using 1{,}000 synthetic spatiotemporal intensity maps (kymographs) with controlled noise and artifacts, the framework reliably identifies degraded data and outputs appropriately inflated uncertainty estimates, showing strong correspondence between predictive uncertainty and error (Pearson $r = 0.997$, Spearman $ρ= 0.998$). Clinical validation in 12 patients shows strong agreement between PUNCH-derived CFR and invasive bolus thermodilution (Pearson $r = 0.90$, $p = 6.3 \times 10^{-5}$). We focus on the LAD, the artery most commonly assessed in routine CMD testing. Probabilistic CFR estimates have confidence intervals narrower than the variability of repeated invasive measurements.
  By transforming routine angiography into quantitative, uncertainty-aware assessment, this approach enables scalable, safer, and more reproducible evaluation of coronary microvascular function. Because standard angiography is widely available globally, the framework could expand access to CMD diagnosis and establish a new paradigm for physics-informed, patient-specific inference from clinical imaging.

</details>


### [168] [Accelerated Sinkhorn Algorithms for Partial Optimal Transport](https://arxiv.org/abs/2601.17196)
*Nghia Thu Truong,Qui Phu Pham,Quang Nguyen,Dung Luong,Mai Tran*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Partial Optimal Transport (POT) addresses the problem of transporting only a fraction of the total mass between two distributions, making it suitable when marginals have unequal size or contain outliers. While Sinkhorn-based methods are widely used, their complexity bounds for POT remain suboptimal and can limit scalability. We introduce Accelerated Sinkhorn for POT (ASPOT), which integrates alternating minimization with Nesterov-style acceleration in the POT setting, yielding a complexity of $\mathcal{O}(n^{7/3}\varepsilon^{-5/3})$. We also show that an informed choice of the entropic parameter $γ$ improves rates for the classical Sinkhorn method. Experiments on real-world applications validate our theories and demonstrate the favorable performance of our proposed methods.

</details>


### [169] [NewPINNs: Physics-Informing Neural Networks Using Conventional Solvers for Partial Differential Equations](https://arxiv.org/abs/2601.17207)
*Maedeh Makki,Satish Chandran,Maziar Raissi,Adrien Grenier,Behzad Mohebbi*

Main category: cs.LG

TL;DR: 介绍NewPINNs，将神经网络与传统数值求解器结合求解微分方程，能缓解标准物理信息神经网络的问题，并在多个问题中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 缓解标准物理信息神经网络存在的优化病态、对损失权重敏感以及在刚性或非线性区域性能差等问题。

Method: 将数值求解器直接集成到训练循环，通过求解器一致性定义学习目标，让神经网络生成候选解，由求解器推进，训练时最小化网络预测与求解器演化状态的差异。

Result: 在涉及有限体积、有限元及谱求解器的多个正逆问题中证明了方法的有效性。

Conclusion: NewPINNs通过将物理、边界条件和数值稳定性的执行委托给成熟数值求解器，能有效缓解标准物理信息神经网络的一些已知失效模式。

Abstract: We introduce NewPINNs, a physics-informing learning framework that couples neural networks with conventional numerical solvers for solving differential equations. Rather than enforcing governing equations and boundary conditions through residual-based loss terms, NewPINNs integrates the solver directly into the training loop and defines learning objectives through solver-consistency. The neural network produces candidate solution states that are advanced by the numerical solver, and training minimizes the discrepancy between the network prediction and the solver-evolved state. This pull-push interaction enables the network to learn physically admissible solutions through repeated exposure to the solver's action, without requiring problem-specific loss engineering or explicit evaluation of differential equation residuals. By delegating the enforcement of physics, boundary conditions, and numerical stability to established numerical solvers, NewPINNs mitigates several well-known failure modes of standard physics-informed neural networks, including optimization pathologies, sensitivity to loss weighting, and poor performance in stiff or nonlinear regimes. We demonstrate the effectiveness of the proposed approach across multiple forward and inverse problems involving finite volume, finite element, and spectral solvers.

</details>


### [170] [PEARL: Prototype-Enhanced Alignment for Label-Efficient Representation Learning with Deployment-Driven Insights from Digital Governance Communication Systems](https://arxiv.org/abs/2601.17495)
*Ruiyu Zhang,Lin Nie,Wai-Fung Lam,Qihao Wang,Xin Zhao*

Main category: cs.LG

TL;DR: 提出PEARL方法解决嵌入空间近邻对应错误问题，在标签稀缺时提升局部邻域质量。


<details>
  <summary>Details</summary>
Motivation: 现有系统在处理新文本输入时，嵌入空间近邻对应错误，且下游性能依赖嵌入几何，原始嵌入与局部邻域结构对齐不佳，标签稀缺、领域变化和重训练代价高。

Method: 提出PEARL方法，利用有限监督使嵌入向类原型软对齐，重塑局部邻域几何。

Result: 在标签稀缺条件下，PEARL大幅提高局部邻域质量，比原始嵌入提升25.7%，比强无监督后处理提升超21.1%。

Conclusion: PEARL能在标签有限时有效改善嵌入空间局部邻域结构，提升系统性能。

Abstract: In many deployed systems, new text inputs are handled by retrieving similar past cases, for example when routing and responding to citizen messages in digital governance platforms. When these systems fail, the problem is often not the language model itself, but that the nearest neighbors in the embedding space correspond to the wrong cases. Modern machine learning systems increasingly rely on fixed, high-dimensional embeddings produced by large pretrained models and sentence encoders. In real-world deployments, labels are scarce, domains shift over time, and retraining the base encoder is expensive or infeasible. As a result, downstream performance depends heavily on embedding geometry. Yet raw embeddings are often poorly aligned with the local neighborhood structure required by nearest-neighbor retrieval, similarity search, and lightweight classifiers that operate directly on embeddings. We propose PEARL (Prototype-Enhanced Aligned Representation Learning), a label-efficient approach that uses limited supervision to softly align embeddings toward class prototypes. The method reshapes local neighborhood geometry while preserving dimensionality and avoiding aggressive projection or collapse. Its aim is to bridge the gap between purely unsupervised post-processing, which offers limited and inconsistent gains, and fully supervised projections that require substantial labeled data. We evaluate PEARL under controlled label regimes ranging from extreme label scarcity to higher-label settings. In the label-scarce condition, PEARL substantially improves local neighborhood quality, yielding 25.7% gains over raw embeddings and more than 21.1% gains relative to strong unsupervised post-processing, precisely in the regime where similarity-based systems are most brittle.

</details>


### [171] [JetFormer: A Scalable and Efficient Transformer for Jet Tagging from Offline Analysis to FPGA Triggers](https://arxiv.org/abs/2601.17215)
*Ruoqing Zheng,Chang Sun,Qibin Liu,Lauri Laatu,Arianna Cox,Benedikt Maier,Alexander Tapper,Jose G. F. Coutinho,Wayne Luk,Zhiqiang Que*

Main category: cs.LG

TL;DR: 提出用于LHC粒子喷流标记的JetFormer架构，能跨场景有效运行，计算高效、泛化性强，还引入硬件优化管道，可压缩且精度损失小。


<details>
  <summary>Details</summary>
Motivation: 现有的喷流标记方法常针对特定场景，需要一种能在全频谱喷流标记场景有效运行的架构。

Method: 设计JetFormer架构处理可变长度粒子特征集，引入基于多目标超参数搜索的硬件优化管道，进行结构化剪枝和量化。

Result: 在JetClass数据集上，大模型与ParT精度相近但FLOPs少37.4%；在HLS4ML 150P数据集上，精度比现有模型高3 - 4%；可压缩且精度损失小。

Conclusion: JetFormer统一了高性能建模和可部署性，为在LHC离线和在线环境部署基于Transformer的喷流标记器提供了实用途径。

Abstract: We present JetFormer, a versatile and scalable encoder-only Transformer architecture for particle jet tagging at the Large Hadron Collider (LHC). Unlike prior approaches that are often tailored to specific deployment regimes, JetFormer is designed to operate effectively across the full spectrum of jet tagging scenarios, from high-accuracy offline analysis to ultra-low-latency online triggering. The model processes variable-length sets of particle features without relying on input of explicit pairwise interactions, yet achieves competitive or superior performance compared to state-of-the-art methods. On the large-scale JetClass dataset, a large-scale JetFormer matches the accuracy of the interaction-rich ParT model (within 0.7%) while using 37.4% fewer FLOPs, demonstrating its computational efficiency and strong generalization. On benchmark HLS4ML 150P datasets, JetFormer consistently outperforms existing models such as MLPs, Deep Sets, and Interaction Networks by 3-4% in accuracy. To bridge the gap to hardware deployment, we further introduce a hardware-aware optimization pipeline based on multi-objective hyperparameter search, yielding compact variants like JetFormer-tiny suitable for FPGA-based trigger systems with sub-microsecond latency requirements. Through structured pruning and quantization, we show that JetFormer can be aggressively compressed with minimal accuracy loss. By unifying high-performance modeling and deployability within a single architectural framework, JetFormer provides a practical pathway for deploying Transformer-based jet taggers in both offline and online environments at the LHC. Code is available at https://github.com/walkieq/JetFormer.

</details>


### [172] [Parameter Inference and Uncertainty Quantification with Diffusion Models: Extending CDI to 2D Spatial Conditioning](https://arxiv.org/abs/2601.17224)
*Dmitrii Torbunov,Yihui Ren,Lijun Wu,Yimei Zhu*

Main category: cs.LG

TL;DR: 本文将CDI方法扩展到二维空间条件，在CBED参数推理中验证，表明其能从时域成功扩展到空域，提供可靠的不确定性信息。


<details>
  <summary>Details</summary>
Motivation: 不确定性量化对科学反问题很重要，CDI在一维时域信号概率推理有效，但在高维空间数据的适用性未探索，因此要将其扩展到二维空间条件。

Method: 将CDI扩展到二维空间条件，在CBED参数推理中进行验证，使用模拟的CBED数据。

Result: CDI产生校准良好的后验分布，能准确反映测量约束；标准回归方法掩盖了潜在的不确定性。

Conclusion: CDI成功从时域扩展到空域，能为可靠的科学推理提供真正的不确定性信息。

Abstract: Uncertainty quantification is critical in scientific inverse problems to distinguish identifiable parameters from those that remain ambiguous given available measurements. The Conditional Diffusion Model-based Inverse Problem Solver (CDI) has previously demonstrated effective probabilistic inference for one-dimensional temporal signals, but its applicability to higher-dimensional spatial data remains unexplored. We extend CDI to two-dimensional spatial conditioning, enabling probabilistic parameter inference directly from spatial observations. We validate this extension on convergent beam electron diffraction (CBED) parameter inference - a challenging multi-parameter inverse problem in materials characterization where sample geometry, electronic structure, and thermal properties must be extracted from 2D diffraction patterns. Using simulated CBED data with ground-truth parameters, we demonstrate that CDI produces well-calibrated posterior distributions that accurately reflect measurement constraints: tight distributions for well-determined quantities and appropriately broad distributions for ambiguous parameters. In contrast, standard regression methods - while appearing accurate on aggregate metrics - mask this underlying uncertainty by predicting training set means for poorly constrained parameters. Our results confirm that CDI successfully extends from temporal to spatial domains, providing the genuine uncertainty information required for robust scientific inference.

</details>


### [173] [A Constrained Optimization Perspective of Unrolled Transformers](https://arxiv.org/abs/2601.17257)
*Javier Porras-Valenzuela,Samar Hadou,Alejandro Ribeiro*

Main category: cs.LG

TL;DR: 提出训练类似优化下降算法的transformer的约束优化框架，应用于不同架构和任务，有更好鲁棒性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 让transformer表现得像优化下降算法。

Method: 在目标函数上实施逐层下降约束，用原始对偶训练方案替代标准经验风险最小化。

Result: 模型中间表示在各层期望上单调降低损失，在视频去噪和文本分类任务中，约束transformer对扰动更鲁棒，有更高分布外泛化性且保持分布内性能。

Conclusion: 该约束优化框架有效，能提升transformer在不同任务中的性能。

Abstract: We introduce a constrained optimization framework for training transformers that behave like optimization descent algorithms. Specifically, we enforce layerwise descent constraints on the objective function and replace standard empirical risk minimization (ERM) with a primal-dual training scheme. This approach yields models whose intermediate representations decrease the loss monotonically in expectation across layers. We apply our method to both unrolled transformer architectures and conventional pretrained transformers on tasks of video denoising and text classification. Across these settings, we observe constrained transformers achieve stronger robustness to perturbations and maintain higher out-of-distribution generalization, while preserving in-distribution performance.

</details>


### [174] [The Viscosity of Logic: Phase Transitions and Hysteresis in DPO Alignment](https://arxiv.org/abs/2601.17260)
*Marco Pollanen*

Main category: cs.LG

TL;DR: 研究DPO中β参数对不同7B模型能力的影响，发现能力与偏好边际可能负相关，建议跨β范围进行能力评估。


<details>
  <summary>Details</summary>
Motivation: 探究DPO中β参数对模型行为和能力的影响，而非简单认为增大β能带来更好行为。

Method: 在固定DPO方案下，对三个7B开源权重模型族的β参数进行密集扫描。

Result: 不同架构模型对β响应模式不同，DPO偏好边际与推理能力可能负相关，训练路径影响能力且有滞后效应。

Conclusion: 应跨β范围进行能力评估，而非依赖边际或综合基准。

Abstract: Direct Preference Optimization (DPO) is often tuned as if increasing alignment pressure (controlled by $β$) yields progressively "better" behavior. We instead treat $β$ as a control parameter and densely sweep it for three 7B open-weight families under a fixed DPO recipe. In Mistral, capability is sharply non-monotonic: aggregated logic-probe margins become positive only in a narrow band near $β\approx 10^{-2}$ and revert outside it, with boundary points that are seed-sensitive. Across architectures under the same sweep, we observe qualitatively different response modes: sharp reorganization in Mistral, selective changes in Llama, and smooth trade-offs in Qwen. Critically, the DPO preference margin can anticorrelate with reasoning capability (Pearson $r=-0.91$ for Llama logic), so margin-based selection can prefer capability-impaired models. Training path also matters: exposure to high $β$ induces capability losses that persist even after $β$ is reduced (hysteresis). These findings motivate capability-resolved evaluation across the $β$ landscape rather than reliance on margins or aggregate benchmarks.

</details>


### [175] [AGZO: Activation-Guided Zeroth-Order Optimization for LLM Fine-Tuning](https://arxiv.org/abs/2601.17261)
*Wei Lin,Yining Jiang,Qingyu Song,Qiao Xiang,Hong Xu*

Main category: cs.LG

TL;DR: 本文提出激活引导零阶优化（AGZO）方法用于微调大语言模型，理论证明其优势，实验表明优于现有零阶基线。


<details>
  <summary>Details</summary>
Motivation: 现有零阶优化方法使用各向同性扰动，忽略前向传播中的结构信息，需要改进。

Method: 提出AGZO方法，在正向传播中动态提取激活信息子空间，并将扰动限制在低秩子空间。

Result: 在多个模型和基准测试中，AGZO始终优于现有零阶基线，缩小了与一阶微调的性能差距，且内存占用与其他零阶方法相近。

Conclusion: AGZO方法有效，能在严格内存约束下更好地微调大语言模型。

Abstract: Zeroth-Order (ZO) optimization has emerged as a promising solution for fine-tuning LLMs under strict memory constraints, as it avoids the prohibitive memory cost of storing activations for backpropagation. However, existing ZO methods typically employ isotropic perturbations, neglecting the rich structural information available during the forward pass. In this paper, we identify a crucial link between gradient formation and activation structure: the gradient of a linear layer is confined to the subspace spanned by its input activations. Leveraging this insight, we propose Activation-Guided Zeroth-Order optimization (AGZO). Unlike prior methods, AGZO extracts a compact, activation-informed subspace on the fly during the forward pass and restricts perturbations to this low-rank subspace. We provide a theoretical framework showing that AGZO optimizes a subspace-smoothed objective and provably yields update directions with higher cosine similarity to the true gradient than isotropic baselines. Empirically, we evaluate AGZO on Qwen3 and Pangu models across various benchmarks. AGZO consistently outperforms state-of-the-art ZO baselines and significantly narrows the performance gap with first-order fine-tuning, while maintaining almost the same peak memory footprint as other ZO methods.

</details>


### [176] [Unrolled Neural Networks for Constrained Optimization](https://arxiv.org/abs/2601.17274)
*Samar Hadou,Alejandro Ribeiro*

Main category: cs.LG

TL;DR: 本文开发展开式神经网络解决约束优化问题，提出CDU框架，通过约束学习引入DA动态，在MIQPs和无线网络功率分配中验证效果好、泛化性强。


<details>
  <summary>Details</summary>
Motivation: 开发加速且可学习的对偶上升（DA）算法的替代方案来解决约束优化问题。

Method: 提出约束对偶展开（CDU）框架，包含两个耦合神经网络；通过约束学习引入DA动态；将训练两个网络作为嵌套优化问题，用交替过程更新网络。

Result: 在混合整数二次规划（MIQPs）和无线网络功率分配问题中得出近最优、近可行解，具强的分布外（OOD）泛化能力。

Conclusion: 所提出的CDU框架可有效解决约束优化问题，并具有良好的泛化性。

Abstract: In this paper, we develop unrolled neural networks to solve constrained optimization problems, offering accelerated, learnable counterparts to dual ascent (DA) algorithms. Our framework, termed constrained dual unrolling (CDU), comprises two coupled neural networks that jointly approximate the saddle point of the Lagrangian. The primal network emulates an iterative optimizer that finds a stationary point of the Lagrangian for a given dual multiplier, sampled from an unknown distribution. The dual network generates trajectories towards the optimal multipliers across its layers while querying the primal network at each layer. Departing from standard unrolling, we induce DA dynamics by imposing primal-descent and dual-ascent constraints through constrained learning. We formulate training the two networks as a nested optimization problem and propose an alternating procedure that updates the primal and dual networks in turn, mitigating uncertainty in the multiplier distribution required for primal network training. We numerically evaluate the framework on mixed-integer quadratic programs (MIQPs) and power allocation in wireless networks. In both cases, our approach yields near-optimal near-feasible solutions and exhibits strong out-of-distribution (OOD) generalization.

</details>


### [177] [PaperSearchQA: Learning to Search and Reason over Scientific Papers with RLVR](https://arxiv.org/abs/2601.18207)
*James Burgess,Jan N. Hansen,Duo Peng,Yuhui Zhang,Alejandro Lozano,Min Woo Sun,Emma Lundberg,Serena Yeung-Levy*

Main category: cs.LG

TL;DR: 提出训练搜索代理在科学论文上搜索和推理，发布生物医学论文摘要搜索语料和QA数据集，训练代理表现优于非RL检索基线，数据可用于RLVR训练且创建方法可扩展。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR搜索代理多用于通用领域QA，与科学、工程和医学的技术AI系统相关性有限，需训练代理在科学论文上搜索推理以进行技术问答。

Method: 发布1600万生物医学论文摘要搜索语料，构建包含6万个样本的PaperSearchQA数据集及基准，在该环境下训练搜索代理。

Result: 训练的搜索代理优于非RL检索基线，有规划、推理和自我验证等行为。

Conclusion: 语料、数据集和基准可用于流行的Search - R1代码库进行RLVR训练，数据创建方法可扩展到其他科学领域。

Abstract: Search agents are language models (LMs) that reason and search knowledge bases (or the web) to answer questions; recent methods supervise only the final answer accuracy using reinforcement learning with verifiable rewards (RLVR). Most RLVR search agents tackle general-domain QA, which limits their relevance to technical AI systems in science, engineering, and medicine. In this work we propose training agents to search and reason over scientific papers -- this tests technical question-answering, it is directly relevant to real scientists, and the capabilities will be crucial to future AI Scientist systems. Concretely, we release a search corpus of 16 million biomedical paper abstracts and construct a challenging factoid QA dataset called PaperSearchQA with 60k samples answerable from the corpus, along with benchmarks. We train search agents in this environment to outperform non-RL retrieval baselines; we also perform further quantitative analysis and observe interesting agent behaviors like planning, reasoning, and self-verification. Our corpus, datasets, and benchmarks are usable with the popular Search-R1 codebase for RLVR training and released on https://huggingface.co/collections/jmhb/papersearchqa. Finally, our data creation methods are scalable and easily extendable to other scientific domains.

</details>


### [178] [Latent-Space Contrastive Reinforcement Learning for Stable and Efficient LLM Reasoning](https://arxiv.org/abs/2601.17275)
*Lianlei Shan,Han Chen,Yixuan Wang,Zhenjie Liu,Wei Li*

Main category: cs.LG

TL;DR: 现有大语言模型处理复杂推理任务存在局限，传统强化学习有挑战，本文提出DeepLatent Reasoning (DLR) 框架，实验表明其有更好表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型处理复杂多步推理任务常为‘统计拟合’，传统强化学习在高维离散令牌空间应用有样本效率低、梯度估计方差大、易灾难性遗忘等问题。

Method: 提出DLR，将试错成本转移到连续潜流形，引入轻量级辅助模型采样推理链编码，通过双奖励机制过滤，用对比学习目标实现潜空间定向探索，主模型参数冻结。

Result: 在可比GPU计算预算下，DLR训练收敛更稳定，支持更长推理链，促进推理能力可持续积累。

Conclusion: DLR为大语言模型可靠且可扩展的强化学习提供了可行途径。

Abstract: While Large Language Models (LLMs) demonstrate exceptional performance in surface-level text generation, their nature in handling complex multi-step reasoning tasks often remains one of ``statistical fitting'' rather than systematic logical deduction. Traditional Reinforcement Learning (RL) attempts to mitigate this by introducing a ``think-before-speak'' paradigm. However, applying RL directly in high-dimensional, discrete token spaces faces three inherent challenges: sample-inefficient rollouts, high gradient estimation variance, and the risk of catastrophic forgetting. To fundamentally address these structural bottlenecks, we propose \textbf{DeepLatent Reasoning (DLR)}, a latent-space bidirectional contrastive reinforcement learning framework. This framework shifts the trial-and-error cost from expensive token-level full sequence generation to the continuous latent manifold. Specifically, we introduce a lightweight assistant model to efficiently sample $K$ reasoning chain encodings within the latent space. These encodings are filtered via a dual reward mechanism based on correctness and formatting; only high-value latent trajectories are fed into a \textbf{frozen main model} for single-pass decoding. To maximize reasoning diversity while maintaining coherence, we design a contrastive learning objective to enable directed exploration within the latent space. Since the main model parameters remain frozen during optimization, this method mathematically eliminates catastrophic forgetting. Experiments demonstrate that under comparable GPU computational budgets, DLR achieves more stable training convergence, supports longer-horizon reasoning chains, and facilitates the sustainable accumulation of reasoning capabilities, providing a viable path toward reliable and scalable reinforcement learning for LLMs.

</details>


### [179] [Tabular Foundation Models are Strong Graph Anomaly Detectors](https://arxiv.org/abs/2601.17301)
*Yunhui Liu,Tieke He,Yongchao Liu,Can Yi,Hong Jin,Chuntao Hong*

Main category: cs.LG

TL;DR: 提出TFM4GAD框架用于图异常检测，将表格基础模型适配到图数据，实验显示比从头训练的专业模型性能更优。


<details>
  <summary>Details</summary>
Motivation: 现有图异常检测方法存在计算成本高、数据需求大、泛化性差等问题，需要一个无需重新训练就能跨图检测异常的基础模型，但因跨领域结构和特征异质性大而具挑战。

Method: 提出TFM4GAD框架，通过“扁平化”图构建增强特征表，将拉普拉斯嵌入、局部和全局结构特征、异常敏感邻域聚合等融入原始节点特征，用表格基础模型全上下文处理。

Result: 在多个数据集和不同表格基础模型骨干上的实验表明，TFM4GAD比从头训练的专业图异常检测模型有显著性能提升。

Conclusion: 为利用表格基础模型作为强大的通用图异常检测器提供了新视角和实用范式。

Abstract: Graph anomaly detection (GAD), which aims to identify abnormal nodes that deviate from the majority, has become increasingly important in high-stakes Web domains. However, existing GAD methods follow a "one model per dataset" paradigm, leading to high computational costs, substantial data demands, and poor generalization when transferred to new datasets. This calls for a foundation model that enables a "one-for-all" GAD solution capable of detecting anomalies across diverse graphs without retraining. Yet, achieving this is challenging due to the large structural and feature heterogeneity across domains. In this paper, we propose TFM4GAD, a simple yet effective framework that adapts tabular foundation models (TFMs) for graph anomaly detection. Our key insight is that the core challenges of foundation GAD, handling heterogeneous features, generalizing across domains, and operating with scarce labels, are the exact problems that modern TFMs are designed to solve via synthetic pre-training and powerful in-context learning. The primary challenge thus becomes structural: TFMs are agnostic to graph topology. TFM4GAD bridges this gap by "flattening" the graph, constructing an augmented feature table that enriches raw node features with Laplacian embeddings, local and global structural characteristics, and anomaly-sensitive neighborhood aggregations. This augmented table is processed by a TFM in a fully in-context regime. Extensive experiments on multiple datasets with various TFM backbones reveal that TFM4GAD surprisingly achieves significant performance gains over specialized GAD models trained from scratch. Our work offers a new perspective and a practical paradigm for leveraging TFMs as powerful, generalist graph anomaly detectors.

</details>


### [180] [Weighted Graph Clustering via Scale Contraction and Graph Structure Learning](https://arxiv.org/abs/2601.17307)
*Haobing Liu,Yinuo Zhang,Tingting Wang,Ruobing Jiang,Yanwei Yu*

Main category: cs.LG

TL;DR: 该论文提出收缩式边权感知图聚类网络，解决边权利用问题，实验验证其有效且能降低训练成本。


<details>
  <summary>Details</summary>
Motivation: 现有图聚类方法未充分利用边权，利用边权存在增加存储和训练成本、边权含噪音影响聚类结果的问题，且少有研究能联合优化聚类和边权。

Method: 提出收缩式边权感知图聚类网络，设计面向聚类的图收缩模块减少图规模，设计边权感知注意力网络识别并削弱噪声连接。

Result: 在三个真实加权图数据集上实验，模型性能优于最佳基线，图收缩模块可显著降低训练时间和存储空间。

Conclusion: 所提方法能更易识别和减轻噪声边对聚类的影响，增强聚类有效性，具有优越性能和降低训练成本的优势。

Abstract: Graph clustering aims to partition nodes into distinct clusters based on their similarity, thereby revealing relationships among nodes. Nevertheless, most existing methods do not fully utilize these edge weights. Leveraging edge weights in graph clustering tasks faces two critical challenges. (1) The introduction of edge weights may significantly increase storage space and training time, making it essential to reduce the graph scale while preserving nodes that are beneficial for the clustering task. (2) Edge weight information may inherently contain noise that negatively impacts clustering results. However, few studies can jointly optimize clustering and edge weights, which is crucial for mitigating the negative impact of noisy edges on clustering task. To address these challenges, we propose a contractile edge-weight-aware graph clustering network. Specifically, a cluster-oriented graph contraction module is designed to reduce the graph scale while preserving important nodes. An edge-weight-aware attention network is designed to identify and weaken noisy connections. In this way, we can more easily identify and mitigate the impact of noisy edges during the clustering process, thus enhancing clustering effectiveness. We conducted extensive experiments on three real-world weighted graph datasets. In particular, our model outperforms the best baseline, demonstrating its superior performance. Furthermore, experiments also show that the proposed graph contraction module can significantly reduce training time and storage space.

</details>


### [181] [PAR: Plausibility-aware Amortized Recourse Generation](https://arxiv.org/abs/2601.17309)
*Anagha Sabu,Vidhya S,Narayanan C Krishnan*

Main category: cs.LG

TL;DR: 本文提出PAR方法解决算法追索问题，在数据集上验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 算法追索旨在推荐可行改变以翻转不利模型决策，需高效生成高可能性的追索方案。

Method: 将追索问题表述为约束最大后验推理问题，提出PAR摊销近似推理程序，用易处理概率模型估计追索可能性，训练追索生成器并加入邻域条件机制。

Result: 在常用算法追索数据集上验证，PAR能高效生成有效、与事实相似、稀疏且高度可信的追索方案。

Conclusion: PAR在算法追索问题上表现优于现有最先进方法。

Abstract: Algorithmic recourse aims to recommend actionable changes to a factual's attributes that flip an unfavorable model decision while remaining realistic and feasible. We formulate recourse as a Constrained Maximum A-Posteriori (MAP) inference problem under the accepted-class data distribution seeking counterfactuals with high likelihood while respecting other recourse constraints. We present PAR, an amortized approximate inference procedure that generates highly likely recourses efficiently. Recourse likelihood is estimated directly using tractable probabilistic models that admit exact likelihood evaluation and efficient gradient propagation that is useful during training. The recourse generator is trained with the objective of maximizing the likelihood under the accepted-class distribution while minimizing the likelihood under the denied-class distribution and other losses that encode recourse constraints. Furthermore, PAR includes a neighborhood-based conditioning mechanism to promote recourse generation that is customized to a factual. We validate PAR on widely used algorithmic recourse datasets and demonstrate its efficiency in generating recourses that are valid, similar to the factual, sparse, and highly plausible, yielding superior performance over existing state-of-the-art approaches.

</details>


### [182] [Conformal Feedback Alignment: Quantifying Answer-Level Reliability for Robust LLM Alignment](https://arxiv.org/abs/2601.17329)
*Tiejin Chen,Xiaoou Liu,Vishnu Nandam,Kuan-Ru Liou,Hua Wei*

Main category: cs.LG

TL;DR: 提出Conformal Feedback Alignment (CFA)框架解决偏好标签噪声和不一致问题，实验显示其提升了对齐鲁棒性和数据效率。


<details>
  <summary>Details</summary>
Motivation: 现有不确定性感知方法在偏好对齐中忽略了被比较答案的可靠性，而偏好标签常有噪声和不一致性。

Method: 提出CFA框架，基于Conformal Prediction (CP)的统计保证进行偏好加权，构建有可控覆盖率的共形预测集量化答案可靠性，并将其集成到DPO和PPO风格训练的权重中。

Result: 在不同数据集上的实验表明CFA提高了对齐鲁棒性和数据效率。

Conclusion: 对答案端不确定性建模可补充偏好层面加权，实现更鲁棒、数据高效的对齐。

Abstract: Preference-based alignment like Reinforcement Learning from Human Feedback (RLHF) learns from pairwise preferences, yet the labels are often noisy and inconsistent. Existing uncertainty-aware approaches weight preferences, but ignore a more fundamental factor: the reliability of the \emph{answers} being compared. To address the problem, we propose Conformal Feedback Alignment (CFA), a framework that grounds preference weighting in the statistical guarantees of Conformal Prediction (CP). CFA quantifies answer-level reliability by constructing conformal prediction sets with controllable coverage and aggregates these reliabilities into principled weights for both DPO- and PPO-style training. Experiments across different datasets show that CFA improves alignment robustness and data efficiency, highlighting that modeling \emph{answer-side} uncertainty complements preference-level weighting and yields more robust, data-efficient alignment. Codes are provided here.

</details>


### [183] [Thermodynamically Optimal Regularization under Information-Geometric Constraints](https://arxiv.org/abs/2601.17330)
*Laurent Caraffa*

Main category: cs.LG

TL;DR: 提出统一理论框架连接热力学最优性、信息几何和正则化，证明条件最优性定理，分析不同模型几何，指出经典方案不足并提出效率概念和可测试预测。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习正则化技术理论异质，且训练大模型能耗增加，需探讨学习算法是否接近效率边界。

Method: 基于三个明确假设证明条件最优性定理，推导高斯和循环信念模型的诱导几何。

Result: Fisher - Rao度量是信念空间唯一允许的几何，经典正则化方案无法保证热力学最优。

Conclusion: 为机器学习中的正则化提供了有原则的几何和热力学基础。

Abstract: Modern machine learning relies on a collection of empirically successful but theoretically heterogeneous regularization techniques, such as weight decay, dropout, and exponential moving averages. At the same time, the rapidly increasing energetic cost of training large models raises the question of whether learning algorithms approach any fundamental efficiency bound. In this work, we propose a unifying theoretical framework connecting thermodynamic optimality, information geometry, and regularization.
  Under three explicit assumptions -- (A1) that optimality requires an intrinsic, parametrization-invariant measure of information, (A2) that belief states are modeled by maximum-entropy distributions under known constraints, and (A3) that optimal processes are quasi-static -- we prove a conditional optimality theorem. Specifically, the Fisher--Rao metric is the unique admissible geometry on belief space, and thermodynamically optimal regularization corresponds to minimizing squared Fisher--Rao distance to a reference state.
  We derive the induced geometries for Gaussian and circular belief models, yielding hyperbolic and von Mises manifolds, respectively, and show that classical regularization schemes are structurally incapable of guaranteeing thermodynamic optimality. We introduce a notion of thermodynamic efficiency of learning and propose experimentally testable predictions. This work provides a principled geometric and thermodynamic foundation for regularization in machine learning.

</details>


### [184] [Power-based Partial Attention: Bridging Linear-Complexity and Full Attention](https://arxiv.org/abs/2601.17334)
*Yufeng Huang*

Main category: cs.LG

TL;DR: 本文引入基于幂的部分注意力机制 (PPA) 探索注意力规模与性能关系，实验表明存在0<p<1使O(L^{1+p})注意力达O(L^2)全注意力类似效果。


<details>
  <summary>Details</summary>
Motivation: 此前研究未系统量化注意力使用量，想要探究O(L^2)二次注意力是否必要，是否有次二次注意力机制能达类似性能。

Method: 引入基于幂的部分注意力机制 (PPA)，其复杂度为O(L^{1+p})，通过改变p值探索变压器架构性能变化。

Result: 实验呈现S曲线行为，性能在较窄p值窗口内从滑动窗口注意力过渡到全注意力，p接近1时趋于平稳。

Conclusion: 存在0<p<1，使O(L^{1+p})注意力足以实现与O(L^2)全注意力相似的结果。

Abstract: It is widely accepted from transformer research that "attention is all we need", but the amount of attention required has never been systematically quantified. Is quadratic $O(L^2)$ attention necessary, or is there a sub-quadratic attention mechanism that can achieve comparable performance? To answer this question, we introduce power-based partial attention (PPA), an attention mechanism of order $O(L^{1+p})$, where $0 \leq p \leq 1$, such that $p=0$ corresponds to sliding window attention with linear complexity, and $p=1$ corresponds to full attention. With this attention construction, we can explore how transformer architecture performance varies as a function of the attention scaling behavior controlled by $p$. The overall trend from our experiments shows an S-curve-like behavior where the performance transitions from sliding-window (linear-complexity) attention to full attention over a narrow window of $p$ values, and plateaus as $p$ approaches $1$. In our experiments, we show that there exists $0<p<1$ such that $O(L^{1+p})$ attention is sufficient to achieve similar results as $O(L^2)$ full attention.

</details>


### [185] [Spectral Geometry for Deep Learning: Compression and Hallucination Detection via Random Matrix Theory](https://arxiv.org/abs/2601.17357)
*Davide Ettori*

Main category: cs.LG

TL;DR: 本文提出基于谱几何和随机矩阵理论的统一框架，用EigenTrack检测异常，RMT - KD进行模型压缩，显示谱统计对神经网络有重要作用。


<details>
  <summary>Details</summary>
Motivation: 大语言模型和深度神经网络存在可靠性问题和高计算成本。

Method: 提出统一框架，用EigenTrack检测语言和视觉语言模型的幻觉及分布外行为，用RMT - KD识别信息谱分量并迭代知识蒸馏以压缩模型。

Result: 框架能检测异常和对模型进行有效压缩，谱统计为监控不确定性和指导压缩提供了解释性和鲁棒性信号。

Conclusion: 谱统计能为大规模神经网络的不确定性监测和模型压缩提供有价值信息。

Abstract: Large language models and deep neural networks achieve strong performance but suffer from reliability issues and high computational cost. This thesis proposes a unified framework based on spectral geometry and random matrix theory to address both problems by analyzing the eigenvalue structure of hidden activations. The first contribution, EigenTrack, is a real-time method for detecting hallucinations and out-of-distribution behavior in language and vision-language models using spectral features and their temporal dynamics. The second contribution, RMT-KD, is a principled compression method that identifies informative spectral components and applies iterative knowledge distillation to produce compact and efficient models while preserving accuracy. Together, these results show that spectral statistics provide interpretable and robust signals for monitoring uncertainty and guiding compression in large-scale neural networks.

</details>


### [186] [Robust Privacy: Inference-Time Privacy through Certified Robustness](https://arxiv.org/abs/2601.17360)
*Jiankai Jin,Xiangzheng Zhang,Zhao Liu,Deyue Zhang,Quanchen Zou*

Main category: cs.LG

TL;DR: 提出鲁棒隐私（RP）概念及属性隐私增强（APE）方法，在推荐任务中拓展敏感属性值推断区间，能缓解模型反演攻击。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习系统在推理时可能导致敏感输入属性被对手推断的隐私问题。

Method: 引入受认证鲁棒性启发的推理时隐私概念RP，开发APE将输入级不变性转化为属性级隐私效果。

Result: 在推荐任务中RP拓展敏感属性值推断区间，能缓解模型反演攻击，在小噪声水平下降低攻击成功率，部分情况不影响模型性能。

Conclusion: RP能增强机器学习系统的隐私保护，在一定程度缓解模型反演攻击。

Abstract: Machine learning systems can produce personalized outputs that allow an adversary to infer sensitive input attributes at inference time. We introduce Robust Privacy (RP), an inference-time privacy notion inspired by certified robustness: if a model's prediction is provably invariant within a radius-$R$ neighborhood around an input $x$ (e.g., under the $\ell_2$ norm), then $x$ enjoys $R$-Robust Privacy, i.e., observing the prediction cannot distinguish $x$ from any input within distance $R$ of $x$. We further develop Attribute Privacy Enhancement (APE) to translate input-level invariance into an attribute-level privacy effect. In a controlled recommendation task where the decision depends primarily on a sensitive attribute, we show that RP expands the set of sensitive-attribute values compatible with a positive recommendation, expanding the inference interval accordingly. Finally, we empirically demonstrate that RP also mitigates model inversion attacks (MIAs) by masking fine-grained input-output dependence. Even at small noise levels ($σ=0.1$), RP reduces the attack success rate (ASR) from 73% to 4% with partial model performance degradation. RP can also partially mitigate MIAs (e.g., ASR drops to 44%) with no model performance degradation.

</details>


### [187] [Diversified Scaling Inference in Time Series Foundation Models](https://arxiv.org/abs/2601.17376)
*Ruijin Hua,Zichuan Liu,Kun Zhang,Yiyuan Yang*

Main category: cs.LG

TL;DR: 本文研究时间序列基础模型（TSFMs）推理计算潜力，提出多样化推理缩放方法，实验证明可提升性能，还提出RobustMSE指标。


<details>
  <summary>Details</summary>
Motivation: TSFMs的推理计算潜力未充分挖掘，研究其在标准采样推理缩放下的表现及控制采样多样性能否提升性能。

Method: 先研究TSFMs在标准采样下的特性，再通过定制时间序列扰动进行多样化推理缩放，理论分析多样性 - 保真度权衡并得出临界样本阈值。

Result: 在多种TSFMs和数据集上实验表明，适当的多样化推理缩放无需更新参数就能显著提升性能。

Conclusion: 推理设计是TSFM优化的关键且计算高效的维度，研究结果明确了各因素相互作用，无需重新训练TSFMs就能实现可靠性能。

Abstract: The advancement of Time Series Foundation Models (TSFMs) has been driven primarily by large-scale pre-training, but inference-time compute potential remains largely untapped. This work systematically investigates two questions: how do TSFMs behave under standard sampling-based inference scaling, and can controlled sampling diversity enhance performance? We first examine the properties of TSFMs under standard sampling often fail to adhere to scaling laws due to insufficient exploration of the solution space. Building on this, we then delve into diversified inference scaling via tailored time series perturbations to expand the generative distribution's support. We theoretically analyze the diversity-fidelity trade-off and derive a critical sample threshold for diversified sampling to outperform standard sampling. Extensive experiments across various TSFMs and datasets show proper diversified inference scaling yields substantial performance gains without parameter updates, establishing inference design as a critical, compute-efficient dimension of TSFM optimization. As an application, we propose RobustMSE, a rigorous metric to quantify the headroom performance of TSFM under a fixed budget. Overall, our findings clarify these factor interactions, enabling reliable performance via diverse large-scale inference time series in parallel environments without re-training TSFMs.

</details>


### [188] [GO-OSC and VASH: Geometry-Aware Representation Learning for Early Degradation Detection in Oscillatory Systems](https://arxiv.org/abs/2601.17396)
*Vashista Nobaub*

Main category: cs.LG

TL;DR: 论文提出GO - OSC框架用于振荡时间序列，理论证明几何探针在早期退化检测的优势，实验验证其性能。


<details>
  <summary>Details</summary>
Motivation: 早期振荡系统退化表现为动力学几何畸变，经典基于能量诊断和无约束学习表示结构不敏感，导致检测延迟或不稳定。

Method: 引入GO - OSC几何感知表示学习框架，定义不变线性几何探针，进行理论分析。

Result: 理论证明早期仅相位退化时，基于能量统计一阶检测能力为零，几何探针有正灵敏度；分析线性探测失效原因及规范化恢复统计可检测性的方法。

Conclusion: 实验在合成基准和真实振动数据集上验证理论，表明该方法能更早检测、提高数据效率且对操作条件变化有鲁棒性。

Abstract: Early-stage degradation in oscillatory systems often manifests as geometric distortions of the dynamics, such as phase jitter, frequency drift, or loss of coherence, long before changes in signal energy are detectable. In this regime, classical energy-based diagnostics and unconstrained learned representations are structurally insensitive, leading to delayed or unstable detection. We introduce GO-OSC, a geometry-aware representation learning framework for oscillatory time series that enforces a canonical and identifiable latent parameterization, enabling stable comparison and aggregation across short, unlabeled windows. Building on this representation, we define a family of invariant linear geometric probes that target degradation-relevant directions in latent space. We provide theoretical results showing that under early phase-only degradation, energy-based statistics have zero first-order detection power, whereas geometric probes achieve strictly positive sensitivity. Our analysis characterizes when and why linear probing fails under non-identifiable representations and shows how canonicalization restores statistical detectability. Experiments on synthetic benchmarks and real vibration datasets validate the theory, demonstrating earlier detection, improved data efficiency, and robustness to operating condition changes.

</details>


### [189] [Efficient Dilated Squeeze and Excitation Neural Operator for Differential Equations](https://arxiv.org/abs/2601.17407)
*Prajwal Chauhan,Salah Eddine Choutri,Saif Eddin Jabari*

Main category: cs.LG

TL;DR: 提出轻量级算子学习框架D - SENO用于高效求解PDEs，训练速度比标准模型快约20倍，精度表现佳。


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer的模型和现有神经算子参数多，训练成本高且部署慢，需要快速准确的物理驱动PDE替代品。

Method: D - SENO将扩张卷积块与挤压 - 激励模块结合，共同捕获宽感受野和动力学，关注通道注意力。

Result: 模型训练速度比标准Transformer模型和神经算子快约20倍，在多个PDE基准测试中精度表现出色；移除SE模块性能略有下降。

Conclusion: D - SENO是一种轻量级、高效且准确的求解多种PDEs的算子学习框架。

Abstract: Fast and accurate surrogates for physics-driven partial differential equations (PDEs) are essential in fields such as aerodynamics, porous media design, and flow control. However, many transformer-based models and existing neural operators remain parameter-heavy, resulting in costly training and sluggish deployment. We propose D-SENO (Dilated Squeeze-Excitation Neural Operator), a lightweight operator learning framework for efficiently solving a wide range of PDEs, including airfoil potential flow, Darcy flow in porous media, pipe Poiseuille flow, and incompressible Navier Stokes vortical fields. D-SENO combines dilated convolution (DC) blocks with squeeze-and-excitation (SE) modules to jointly capture wide receptive fields and dynamics alongside channel-wise attention, enabling both accurate and efficient PDE inference. Carefully chosen dilation rates allow the receptive field to focus on critical regions, effectively modeling long-range physical dependencies. Meanwhile, the SE modules adaptively recalibrate feature channels to emphasize dynamically relevant scales. Our model achieves training speed of up to approximately $20\times$ faster than standard transformer-based models and neural operators, while also surpassing (or matching) them in accuracy across multiple PDE benchmarks. Ablation studies show that removing the SE modules leads to a slight drop in performance.

</details>


### [190] [Active Hypothesis Testing for Correlated Combinatorial Anomaly Detection](https://arxiv.org/abs/2601.17430)
*Zichuan Yang,Yiming Xing*

Main category: cs.LG

TL;DR: 研究相关噪声下识别异常流子集问题，提出 ECC - AHT 算法，性能优且代码开源。


<details>
  <summary>Details</summary>
Motivation: 受网络物理系统监测和安全需求驱动，解决现有方法假设观测独立、未利用相关性的问题。

Method: 提出 ECC - AHT 自适应算法，通过连续、受限测量最大化竞争假设间的 Chernoff 信息，利用差分传感进行主动噪声消除。

Result: ECC - AHT 实现了最优样本复杂度保证，在合成和真实相关环境中显著优于现有先进基线方法。

Conclusion: ECC - AHT 算法在相关噪声下识别异常流子集问题上表现出色，具有实际应用价值。

Abstract: We study the problem of identifying an anomalous subset of streams under correlated noise, motivated by monitoring and security in cyber-physical systems. This problem can be viewed as a form of combinatorial pure exploration, where each stream plays the role of an arm and measurements must be allocated sequentially under uncertainty. Existing combinatorial bandit and hypothesis testing methods typically assume independent observations and fail to exploit correlation for efficient measurement design. We propose ECC-AHT, an adaptive algorithm that selects continuous, constrained measurements to maximize Chernoff information between competing hypotheses, enabling active noise cancellation through differential sensing. ECC-AHT achieves optimal sample complexity guarantees and significantly outperforms state-of-the-art baselines in both synthetic and real-world correlated environments. The code is available on https://github.com/VincentdeCristo/ECC-AHT

</details>


### [191] [Data-driven Clustering and Merging of Adapters for On-device Large Language Models](https://arxiv.org/abs/2601.17441)
*Ondrej Bohdal,Taha Ceritli,Mete Ozay,Jijoong Moon,Kyeng-Hun Lee,Hyeonmok Ko,Umberto Michieli*

Main category: cs.LG

TL;DR: 本文提出D2C方法用于适配层聚类，利用少量示例和迭代优化创建多任务适配层，实验证明该方法能在给定存储预算下提升性能。


<details>
  <summary>Details</summary>
Motivation: 设备上大语言模型使用任务特定适配层，因内存限制不能存储所有适配层，需选择能跨多任务泛化的代表适配层，此问题尚待研究。

Method: 提出D2C方法进行适配层聚类，利用少量任务特定示例并通过迭代优化过程细化聚类分配，将每个聚类内的适配层合并。

Result: 实验结果表明，在给定存储预算下，该方法能有效提升性能。

Conclusion: 所提出的D2C方法可有效解决在资源受限设备上选择适配层的问题，并能提升性能。

Abstract: On-device large language models commonly employ task-specific adapters (e.g., LoRAs) to deliver strong performance on downstream tasks. While storing all available adapters is impractical due to memory constraints, mobile devices typically have sufficient capacity to store a limited number of these parameters. This raises a critical challenge: how to select representative adapters that generalize well across multiple tasks - a problem that remains unexplored in existing literature. We propose a novel method D2C for adapter clustering that leverages minimal task-specific examples (e.g., 10 per task) and employs an iterative optimization process to refine cluster assignments. The adapters within each cluster are merged, creating multi-task adapters deployable on resource-constrained devices. Experimental results demonstrate that our method effectively boosts performance for considered storage budgets.

</details>


### [192] [DREAM: Dual-Standard Semantic Homogeneity with Dynamic Optimization for Graph Learning with Label Noise](https://arxiv.org/abs/2601.17449)
*Yusheng Zhao,Jiaye Xie,Qixin Zhang,Weizhi Zhang,Xiao Luo,Zhiping Xiao,Philip S. Yu,Ming Zhang*

Main category: cs.LG

TL;DR: 本文提出DREAM方法解决含标签噪声图学习问题，设计动态优化框架和双标准选择策略，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有图学习方法在处理标签噪声时难以区分可靠与不可靠节点且忽略图拓扑关系信息。

Method: 提出DREAM方法，设计关系感知动态优化框架，采用双标准选择策略选锚节点，计算语义同质性指导优化，并进行理论分析。

Result: 在六个不同领域图数据集、三种标签噪声类型下实验，结果表明DREAM有效。

Conclusion: DREAM方法能有效解决含标签噪声的图学习问题。

Abstract: Graph neural networks (GNNs) have been widely used in various graph machine learning scenarios. Existing literature primarily assumes well-annotated training graphs, while the reliability of labels is not guaranteed in real-world scenarios. Recently, efforts have been made to address the problem of graph learning with label noise. However, existing methods often (i) struggle to distinguish between reliable and unreliable nodes, and (ii) overlook the relational information embedded in the graph topology. To tackle this problem, this paper proposes a novel method, Dual-Standard Semantic Homogeneity with Dynamic Optimization (DREAM), for reliable, relation-informed optimization on graphs with label noise. Specifically, we design a relation-informed dynamic optimization framework that iteratively reevaluates the reliability of each labeled node in the graph during the optimization process according to the relation of the target node and other nodes. To measure this relation comprehensively, we propose a dual-standard selection strategy that selects a set of anchor nodes based on both node proximity and graph topology. Subsequently, we compute the semantic homogeneity between the target node and the anchor nodes, which serves as guidance for optimization. We also provide a rigorous theoretical analysis to justify the design of DREAM. Extensive experiments are performed on six graph datasets across various domains under three types of graph label noise against competing baselines, and the results demonstrate the effectiveness of the proposed DREAM.

</details>


### [193] [Harnessing Reasoning Trajectories for Hallucination Detection via Answer-agreement Representation Shaping](https://arxiv.org/abs/2601.17467)
*Jianxiong Zhang,Bing Guo,Yuming Jiang,Haobo Wang,Bo An,Xuefeng Du*

Main category: cs.LG

TL;DR: 提出ARS方法用于大推理模型幻觉检测，通过编码答案稳定性学习检测友好的表示，实验显示能提升检测效果。


<details>
  <summary>Details</summary>
Motivation: 大推理模型产生看似连贯但答案错误的推理轨迹，直接用轨迹文本或隐藏状态进行幻觉检测易过拟合，需更好的检测方法。

Method: 引入Answer - agreement Representation Shaping (ARS)，通过小的潜在干预生成反事实答案，根据答案是否一致标注扰动，学习使答案一致状态靠近、不一致状态分离的表示。

Result: 实验表明ARS持续改善检测效果，相比强基线有显著提升。

Conclusion: ARS方法能有效提升大推理模型幻觉检测性能，且训练无需人工标注。

Abstract: Large reasoning models (LRMs) often generate long, seemingly coherent reasoning traces yet still produce incorrect answers, making hallucination detection challenging. Although trajectories contain useful signals, directly using trace text or vanilla hidden states for detection is brittle: traces vary in form and detectors can overfit to superficial patterns rather than answer validity. We introduce Answer-agreement Representation Shaping (ARS), which learns detection-friendly trace-conditioned representations by explicitly encoding answer stability. ARS generates counterfactual answers through small latent interventions, specifically, perturbing the trace-boundary embedding, and labels each perturbation by whether the resulting answer agrees with the original. It then learns representations that bring answer-agreeing states together and separate answer-disagreeing ones, exposing latent instability indicative of hallucination risk. The shaped embeddings are plug-and-play with existing embedding-based detectors and require no human annotations during training. Experiments demonstrate that ARS consistently improves detection and achieves substantial gains over strong baselines.

</details>


### [194] [Identifying and Correcting Label Noise for Robust GNNs via Influence Contradiction](https://arxiv.org/abs/2601.17469)
*Wei Ju,Wei Zhang,Siyu Yi,Zhengyang Mao,Yifan Wang,Jingyang Yuan,Zhiping Xiao,Ziyue Qiao,Ming Zhang*

Main category: cs.LG

TL;DR: 本文提出ICGNN方法，利用图结构信息缓解标签噪声对图神经网络的影响，通过设计噪声指标、利用高斯混合模型等步骤处理，实验证明其优越性。


<details>
  <summary>Details</summary>
Motivation: 真实场景中图神经网络处理带标签噪声的图数据时效果受影响，需要一种方法缓解标签噪声挑战。

Method: 提出ICGNN方法，先设计噪声指标ICS量化节点标签可信度，用高斯混合模型检测噪声标签，开发软策略修正检测到的噪声标签，最后对未标记节点进行伪标签操作辅助模型优化。

Result: 在基准数据集上的实验展示了所提方法的优越性。

Conclusion: 提出的ICGNN方法可有效缓解图数据中标签噪声带来的问题。

Abstract: Graph Neural Networks (GNNs) have shown remarkable capabilities in learning from graph-structured data with various applications such as social analysis and bioinformatics. However, the presence of label noise in real scenarios poses a significant challenge in learning robust GNNs, and their effectiveness can be severely impacted when dealing with noisy labels on graphs, often stemming from annotation errors or inconsistencies. To address this, in this paper we propose a novel approach called ICGNN that harnesses the structure information of the graph to effectively alleviate the challenges posed by noisy labels. Specifically, we first design a novel noise indicator that measures the influence contradiction score (ICS) based on the graph diffusion matrix to quantify the credibility of nodes with clean labels, such that nodes with higher ICS values are more likely to be detected as having noisy labels. Then we leverage the Gaussian mixture model to precisely detect whether the label of a node is noisy or not. Additionally, we develop a soft strategy to combine the predictions from neighboring nodes on the graph to correct the detected noisy labels. At last, pseudo-labeling for abundant unlabeled nodes is incorporated to provide auxiliary supervision signals and guide the model optimization. Experiments on benchmark datasets show the superiority of our proposed approach.

</details>


### [195] [LeanTutor: Towards a Verified AI Mathematical Proof Tutor](https://arxiv.org/abs/2601.17473)
*Manooshree Patel,Rayna Bhattacharyya,Thomas Lu,Arnav Mehta,Niels Voss,Narges Norouzi,Gireeja Ranade*

Main category: cs.LG

TL;DR: 开发基于AI、可证明正确性的数学证明辅导系统LeanTutor，结合LLMs和定理证明器优势并通过PeanoBench数据集评估


<details>
  <summary>Details</summary>
Motivation: LLMs易于自然语言交互但易出错，定理证明器如Lean可保证证明正确性但学生难学，需结合二者优势开发辅导系统

Method: 构建由自动形式化器/证明检查器、下一步生成器和自然语言反馈生成器构成的LeanTutor系统

Result: 介绍了用于评估的PeanoBench数据集，包含371个来自Natural Numbers Game的自然语言和形式语言的皮亚诺算术证明

Conclusion: 未明确提及，但可推测结合LLMs和定理证明器开发证明辅导系统是可行的

Abstract: This paper considers the development of an AI-based provably-correct mathematical proof tutor. While Large Language Models (LLMs) allow seamless communication in natural language, they are error prone. Theorem provers such as Lean allow for provable-correctness, but these are hard for students to learn. We present a proof-of-concept system (LeanTutor) by combining the complementary strengths of LLMs and theorem provers. LeanTutor is composed of three modules: (i) an autoformalizer/proof-checker, (ii) a next-step generator, and (iii) a natural language feedback generator. To evaluate the system, we introduce PeanoBench, a dataset of 371 Peano Arithmetic proofs in human-written natural language and formal language, derived from the Natural Numbers Game.

</details>


### [196] [Unintended Memorization of Sensitive Information in Fine-Tuned Language Models](https://arxiv.org/abs/2601.17480)
*Marton Szep,Jorge Marin Ruiz,Georgios Kaissis,Paulina Seidl,Rüdiger von Eisenhart-Rothe,Florian Hinterwimmer,Daniel Rueckert*

Main category: cs.LG

TL;DR: 研究微调大语言模型时仅在输入中出现的个人身份信息（PII）暴露问题，测试四种隐私保护方法。


<details>
  <summary>Details</summary>
Motivation: 微调大语言模型在敏感数据集上存在意外记忆和泄露PII的风险，研究仅在模型输入中出现的PII暴露这一未充分探索的漏洞。

Method: 使用合成和真实数据集，设计提取探针量化PII意外记忆，研究多种因素对记忆行为的影响，对四种隐私保护方法进行基准测试。

Result: 训练后方法在隐私 - 效用权衡上更一致，差分隐私在特定设置下能减少泄露，但会引入训练不稳定性。

Conclusion: 微调大语言模型的记忆问题仍具挑战，需要强大、可扩展的隐私保护技术。

Abstract: Fine-tuning Large Language Models (LLMs) on sensitive datasets carries a substantial risk of unintended memorization and leakage of Personally Identifiable Information (PII), which can violate privacy regulations and compromise individual safety. In this work, we systematically investigate a critical and underexplored vulnerability: the exposure of PII that appears only in model inputs, not in training targets. Using both synthetic and real-world datasets, we design controlled extraction probes to quantify unintended PII memorization and study how factors such as language, PII frequency, task type, and model size influence memorization behavior. We further benchmark four privacy-preserving approaches including differential privacy, machine unlearning, regularization, and preference alignment, evaluating their trade-offs between privacy and task performance. Our results show that post-training methods generally provide more consistent privacy-utility trade-offs, while differential privacy achieves strong reduction in leakage in specific settings, although it can introduce training instability. These findings highlight the persistent challenge of memorization in fine-tuned LLMs and emphasize the need for robust, scalable privacy-preserving techniques.

</details>


### [197] [Automatic Stability and Recovery for Neural Network Training](https://arxiv.org/abs/2601.17483)
*Barak Or*

Main category: cs.LG

TL;DR: 提出一种监督运行时稳定性框架处理神经网络训练不稳定问题，有理论保障且开销小。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络训练易不稳定，现有优化方法应对能力有限。

Method: 将优化视为受控随机过程，通过隔离二次测量的创新信号，检测并恢复不稳定更新。

Result: 给出理论运行时安全保证，形式化了有界退化和恢复。

Conclusion: 该框架可在不修改底层优化器的情况下恢复不稳定更新，实施开销小且适用于内存受限训练。

Abstract: Training modern neural networks is increasingly fragile, with rare but severe destabilizing updates often causing irreversible divergence or silent performance degradation. Existing optimization methods primarily rely on preventive mechanisms embedded within the optimizer, offering limited ability to detect and recover from instability once it occurs. We introduce a supervisory runtime stability framework that treats optimization as a controlled stochastic process. By isolating an innovation signal derived from secondary measurements, such as validation probes, the framework enables automatic detection and recovery from destabilizing updates without modifying the underlying optimizer. We provide theoretical runtime safety guarantees that formalize bounded degradation and recovery. Our implementation incurs minimal overhead and is compatible with memory-constrained training settings.

</details>


### [198] [SpatialMath: Spatial Comprehension-Infused Symbolic Reasoning for Mathematical Problem-Solving](https://arxiv.org/abs/2601.17489)
*Ashutosh Bajpai,Akshat Bhandari,Akshay Nambi,Tanmoy Chakraborty*

Main category: cs.LG

TL;DR: 提出SpatialMath框架解决多模态中小语言模型在视觉理解和数学推理的局限，引入MATHVERSE - PLUS数据集，性能超基线。


<details>
  <summary>Details</summary>
Motivation: 多模态中小语言模型在视觉理解和数学推理，尤其是几何问题上存在局限，难以分解视觉输入和连接感知与推理。

Method: 提出SpatialMath框架，用专门感知模块提取空间表征并融入符号推理链，引入MATHVERSE - PLUS数据集。

Result: SpatialMath显著优于多模态基线，在视觉密集场景中比数据增强的监督微调提升达10个百分点。

Conclusion: 增强的空间表征能直接提高推理准确性，多模态中小语言模型需要结构化的感知到推理的流程。

Abstract: Multimodal Small-to-Medium sized Language Models (MSLMs) have demonstrated strong capabilities in integrating visual and textual information but still face significant limitations in visual comprehension and mathematical reasoning, particularly in geometric problems with diverse levels of visual infusion. Current models struggle to accurately decompose intricate visual inputs and connect perception with structured reasoning, leading to suboptimal performance. To address these challenges, we propose SpatialMath, a novel Spatial Comprehension-Infused Symbolic Reasoning Framework designed to integrate spatial representations into structured symbolic reasoning chains. SpatialMath employs a specialized perception module to extract spatially-grounded representations from visual diagrams, capturing critical geometric structures and spatial relationships. These representations are then methodically infused into symbolic reasoning chains, facilitating visual comprehension-aware structured reasoning. To this end, we introduce MATHVERSE-PLUS, a novel dataset containing structured visual interpretations and step-by-step reasoning paths for vision-intensive mathematical problems. SpatialMath significantly outperforms strong multimodal baselines, achieving up to 10 percentage points improvement over supervised fine-tuning with data augmentation in vision-intensive settings. Robustness analysis reveals that enhanced spatial representations directly improve reasoning accuracy, reinforcing the need for structured perception-to-reasoning pipelines in MSLMs.

</details>


### [199] [One-Shot Federated Clustering of Non-Independent Completely Distributed Data](https://arxiv.org/abs/2601.17512)
*Yiqun Zhang,Shenghong Cai,Zihua Yang,Sen Feng,Yuzhu Ji,Haijun Zhang*

Main category: cs.LG

TL;DR: 本文指出联邦聚类中Non - IID问题瓶颈，提出广义Non - ICD概念，设计GOLD框架应对挑战并验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有无监督联邦聚类（FC）因缺乏标签指导，面临Non - IID问题，影响聚类性能，现有方法有瓶颈。

Method: 提出GOLD框架，先探究客户端局部不完整聚类分布，上传分布汇总到服务器进行全局融合，再在全局分布指导下进行局部聚类增强。

Result: 通过显著性测试、消融研究、可扩展性评估等大量实验，证明GOLD框架的优越性。

Conclusion: GOLD框架能有效解决现有FC聚类面临的挑战。

Abstract: Federated Learning (FL) that extracts data knowledge while protecting the privacy of multiple clients has achieved remarkable results in distributed privacy-preserving IoT systems, including smart traffic flow monitoring, smart grid load balancing, and so on. Since most data collected from edge devices are unlabeled, unsupervised Federated Clustering (FC) is becoming increasingly popular for exploring pattern knowledge from complex distributed data. However, due to the lack of label guidance, the common Non-Independent and Identically Distributed (Non-IID) issue of clients have greatly challenged FC by posing the following problems: How to fuse pattern knowledge (i.e., cluster distribution) from Non-IID clients; How are the cluster distributions among clients related; and How does this relationship connect with the global knowledge fusion? In this paper, a more tricky but overlooked phenomenon in Non-IID is revealed, which bottlenecks the clustering performance of the existing FC approaches. That is, different clients could fragment a cluster, and accordingly, a more generalized Non-IID concept, i.e., Non-ICD (Non-Independent Completely Distributed), is derived. To tackle the above FC challenges, a new framework named GOLD (Global Oriented Local Distribution Learning) is proposed. GOLD first finely explores the potential incomplete local cluster distributions of clients, then uploads the distribution summarization to the server for global fusion, and finally performs local cluster enhancement under the guidance of the global distribution. Extensive experiments, including significance tests, ablation studies, scalability evaluations, qualitative results, etc., have been conducted to show the superiority of GOLD.

</details>


### [200] [Towards Generalisable Imitation Learning Through Conditioned Transition Estimation and Online Behaviour Alignment](https://arxiv.org/abs/2601.17563)
*Nathan Gavenski,Matteo Leonetti,Odinaldo Rodrigues*

Main category: cs.LG

TL;DR: 现有从观察中进行模仿学习的方法有局限，本文提出无监督方法UfO，经实验验证其性能优于教师和其他方法，泛化性好。


<details>
  <summary>Details</summary>
Motivation: 解决现有从观察中进行模仿学习方法需要基于动作的监督优化、假设状态只有单一最优动作、不充分考虑实际环境状态以及难以无监督提取信息的局限。

Method: UfO通过两阶段过程学习策略，先在观察到的状态转换中近似教师的真实动作，再调整智能体轨迹使其与教师轨迹对齐以进一步优化策略。

Result: 在五个常用环境的实验中，UfO性能优于教师和其他从观察中进行模仿学习的方法，且标准差最小。

Conclusion: UfO能有效克服现有方法的局限，在性能和泛化性上表现更好。

Abstract: State-of-the-art imitation learning from observation methods (ILfO) have recently made significant progress, but they still have some limitations: they need action-based supervised optimisation, assume that states have a single optimal action, and tend to apply teacher actions without full consideration of the actual environment state. While the truth may be out there in observed trajectories, existing methods struggle to extract it without supervision. In this work, we propose Unsupervised Imitation Learning from Observation (UfO) that addresses all of these limitations. UfO learns a policy through a two-stage process, in which the agent first obtains an approximation of the teacher's true actions in the observed state transitions, and then refines the learned policy further by adjusting agent trajectories to closely align them with the teacher's. Experiments we conducted in five widely used environments show that UfO not only outperforms the teacher and all other ILfO methods but also displays the smallest standard deviation. This reduction in standard deviation indicates better generalisation in unseen scenarios.

</details>


### [201] [Quantum-Inspired Episode Selection for Monte Carlo Reinforcement Learning via QUBO Optimization](https://arxiv.org/abs/2601.17570)
*Hadi Salloum,Ali Jnadi,Yaroslav Kholodov,Alexander Gasnikov*

Main category: cs.LG

TL;DR: 针对蒙特卡罗强化学习样本复杂度高的问题，提出MC+QUBO方法，实验表明其在收敛速度和最终策略质量上优于传统蒙特卡罗方法。


<details>
  <summary>Details</summary>
Motivation: 解决蒙特卡罗强化学习在稀疏奖励、大状态空间和相关轨迹环境下样本复杂度高的问题。

Method: 将回合选择重新表述为二次无约束二进制优化（QUBO）问题，使用量子启发采样器求解，将组合过滤步骤集成到标准蒙特卡罗策略评估中。

Result: 在有限水平的GridWorld实验中，MC+QUBO在收敛速度和最终策略质量上优于传统蒙特卡罗方法。

Conclusion: 量子启发优化作为强化学习中的决策子程序具有潜力。

Abstract: Monte Carlo (MC) reinforcement learning suffers from high sample complexity, especially in environments with sparse rewards, large state spaces, and correlated trajectories. We address these limitations by reformulating episode selection as a Quadratic Unconstrained Binary Optimization (QUBO) problem and solving it with quantum-inspired samplers. Our method, MC+QUBO, integrates a combinatorial filtering step into standard MC policy evaluation: from each batch of trajectories, we select a subset that maximizes cumulative reward while promoting state-space coverage. This selection is encoded as a QUBO, where linear terms favor high-reward episodes and quadratic terms penalize redundancy. We explore both Simulated Quantum Annealing (SQA) and Simulated Bifurcation (SB) as black-box solvers within this framework. Experiments in a finite-horizon GridWorld demonstrate that MC+QUBO outperforms vanilla MC in convergence speed and final policy quality, highlighting the potential of quantum-inspired optimization as a decision-making subroutine in reinforcement learning.

</details>


### [202] [Understanding Transformer Encoder-Decoder Representations through Bernoulli Dropout](https://arxiv.org/abs/2601.17602)
*Xuanzhou Chen*

Main category: cs.LG

TL;DR: 研究高维编解码器嵌入中Transformer过参数化，应用伯努利dropout识别阈值，理论证明有效稀疏嵌入足够大时性能稳定，实验构建新模型测试英法翻译任务。


<details>
  <summary>Details</summary>
Motivation: 从高维编解码器嵌入的角度研究Transformer过参数化问题。

Method: 在编码器和解码器之间应用伯努利dropout，改变保留概率p来确定阈值；理论证明有效稀疏嵌入的稳定性；构建带二进制擦除通道（BEC）的新Transformer模型进行实验。

Result: 实验结果显示验证准确率和BLEU分数在某一阈值处急剧下降。

Conclusion: 未明确提及，但暗示了在处理Transformer过参数化时存在与稀疏性相关的性能阈值。

Abstract: We study Transformer overparameterization through the lens of angular similarity in high-dimensional encoder-decoder embeddings. We apply Bernoulli dropout between the encoder and the decoder, varying the keep probability $p$ to identify a sparsity-dependent threshold above which the Top-1 prediction is preserved. Theoretically, we prove that, if the effective sparsity embeddings is sufficiently large, and thus decoder performance, remain stable under moderate coordinate dropout. Empirically, we implement the Bernoulli dropout by constructing a new Transformer model augmented with Binary Erasure Channel (BEC) and test its performance on an English-French translation task. Experimental results visualize the trends for validation accuracies and BLEU scores, both decline sharply at some threshold.

</details>


### [203] [A Thermodynamic Theory of Learning I: Irreversible Ensemble Transport and Epistemic Costs](https://arxiv.org/abs/2601.17607)
*Daisuke Okanohara*

Main category: cs.LG

TL;DR: 探讨学习如何在不违反信息论限制下产生抽象和洞察，提出学习是不可逆过程，引入认知自由能框架并推导认知速度限制。


<details>
  <summary>Details</summary>
Motivation: 解决学习在不违反信息论限制下如何产生抽象和洞察的问题。

Method: 将学习建模为模型配置概率分布空间中的传输过程，引入认知自由能框架，定义自由能下降并分解其组成部分。

Result: 推导了认知速度限制（ESL），该限制只依赖初始和最终集合分布的Wasserstein距离，与具体学习算法无关。

Conclusion: 学习在有限时间内是不可逆过程，实现认知结构必然产生熵。

Abstract: Learning systems acquire structured internal representations from data, yet classical information-theoretic results state that deterministic transformations do not increase information. This raises a fundamental question: how can learning produce abstraction and insight without violating information-theoretic limits?
  We argue that learning is inherently an irreversible process when performed over finite time, and that the realization of epistemic structure necessarily incurs entropy production. To formalize this perspective, we model learning as a transport process in the space of probability distributions over model configurations and introduce an epistemic free-energy framework.
  Within this framework, we define the free-energy drop as a bookkeeping quantity that records the total reduction of epistemic free energy along a learning trajectory. This reduction decomposes into a reversible component associated with potential improvement and an irreversible component corresponding to entropy production.
  We then derive the Epistemic Speed Limit (ESL), a finite-time inequality that lower-bounds the minimal entropy production required by any learning process to realize a given distributional transformation. This bound depends only on the Wasserstein distance between initial and final ensemble distributions and is independent of the specific learning algorithm.

</details>


### [204] [Split-on-Share: Mixture of Sparse Experts for Task-Agnostic Continual Learning](https://arxiv.org/abs/2601.17616)
*Fatema Siddika,Md Anwar Hossen,Tanwi Mallick,Ali Jannesari*

Main category: cs.LG

TL;DR: 提出了名为SETA的框架解决大语言模型持续学习中的可塑性 - 稳定性困境，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型持续学习存在可塑性 - 稳定性困境，现有方法统一处理参数，无法区分特定任务知识和共享能力。

Method: 引入SETA框架，将模型分解为模块化子空间，分离知识到独特专家和共享专家，通过弹性权重锚定维护结构，使用统一门控网络推理。

Result: 在不同特定领域和通用基准测试中，SETA始终优于最先进的基于参数高效微调的持续学习方法。

Conclusion: SETA框架能有效解决大语言模型持续学习的可塑性 - 稳定性冲突。

Abstract: Continual learning in Large Language Models (LLMs) is hindered by the plasticity-stability dilemma, where acquiring new capabilities often leads to catastrophic forgetting of previous knowledge. Existing methods typically treat parameters uniformly, failing to distinguish between specific task knowledge and shared capabilities. We introduce Mixture of Sparse Experts for Task-Agnostic Continual Learning, referred to as SETA, a framework that resolves the plasticity-stability conflict by decomposing the model into modular subspaces. Unlike standard updates, where tasks compete for the same parameters, SETA separates knowledge into unique experts, designed to isolate task-specific patterns, and shared experts, responsible for capturing common features. This structure is maintained through elastic weight anchoring, which protects critical shared knowledge and enables a unified gating network to automatically retrieve the correct expert combination for each task during inference. Extensive experiments across diverse domain-specific and general benchmarks demonstrate that SETA consistently outperforms state-of-the-art parameter-efficient fine-tuning-based continual learning methods.

</details>


### [205] [BrainDistill: Implantable Motor Decoding with Task-Specific Knowledge Distillation](https://arxiv.org/abs/2601.17625)
*Yuhan Xie,Jinhan Liu,Xiaoyong Ni,Fei Tan,Icare Sakr,Thibault Collin,Shiqi Sun,Alejandro Rodriguez Guajardo,Demon Fanny,Charles-francois Vincent Latchoumane,Henri Lorach,Jocelyne Bloch,Gregoire Courtine,Mahsa Shoaran*

Main category: cs.LG

TL;DR: 提出BrainDistill解码管道，包含IND和TSKD框架，IND表现优，量化后可在植入式BCI严格功率约束下部署。


<details>
  <summary>Details</summary>
Motivation: 基于Transformer的神经解码器参数多、计算需求高，难以在功率受限的植入式系统中部署。

Method: 引入BrainDistill，集成IND与TSKD框架，TSKD通过监督投影优先处理关键特征，还提出量化感知训练方案。

Result: IND在运动解码任务上优于先前神经解码器，TSKD蒸馏变体在少样本校准设置中超越其他蒸馏方法，量化后性能损失小。

Conclusion: 所提方法可实现植入式BCI在严格功率约束下的部署。

Abstract: Transformer-based neural decoders with large parameter counts, pre-trained on large-scale datasets, have recently outperformed classical machine learning models and small neural networks on brain-computer interface (BCI) tasks. However, their large parameter counts and high computational demands hinder deployment in power-constrained implantable systems. To address this challenge, we introduce BrainDistill, a novel implantable motor decoding pipeline that integrates an implantable neural decoder (IND) with a task-specific knowledge distillation (TSKD) framework. Unlike standard feature distillation methods that attempt to preserve teacher representations in full, TSKD explicitly prioritizes features critical for decoding through supervised projection. Across multiple neural datasets, IND consistently outperforms prior neural decoders on motor decoding tasks, while its TSKD-distilled variant further surpasses alternative distillation methods in few-shot calibration settings. Finally, we present a quantization-aware training scheme that enables integer-only inference with activation clipping ranges learned during training. The quantized IND enables deployment under the strict power constraints of implantable BCIs with minimal performance loss.

</details>


### [206] [RPNT: Robust Pre-trained Neural Transformer -- A Pathway for Generalized Motor Decoding](https://arxiv.org/abs/2601.17641)
*Hao Fang,Ryan A. Canfield,Tomohiro Ouchi,Beatrice Macagno,Eli Shlizerman,Amy L. Orsborn*

Main category: cs.LG

TL;DR: 提出RPNT模型用于脑解码，在多任务中表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前脑解码模型在跨多种变化情况的泛化能力不足，需开发能适应和泛化的预训练神经Transformer模型。

Method: 提出RPNT模型，包含多维旋转位置嵌入、基于卷积核的上下文注意力机制、稳健自监督学习目标；在两个不同数据集上预训练，后在多种下游任务中评估泛化能力。

Result: RPNT在所有任务中始终达到并超越现有解码模型的解码性能。

Conclusion: RPNT模型能有效解决脑解码中跨变化的泛化问题，具有较好的性能。

Abstract: Brain decoding aims to interpret and translate neural activity into behaviors. As such, it is imperative that decoding models are able to generalize across variations, such as recordings from different brain sites, distinct sessions, different types of behavior, and a variety of subjects. Current models can only partially address these challenges and warrant the development of pretrained neural transformer models capable to adapt and generalize. In this work, we propose RPNT - Robust Pretrained Neural Transformer, designed to achieve robust generalization through pretraining, which in turn enables effective finetuning given a downstream task. In particular, RPNT unique components include 1) Multidimensional rotary positional embedding (MRoPE) to aggregate experimental metadata such as site coordinates, session name and behavior types; 2) Context-based attention mechanism via convolution kernels operating on global attention to learn local temporal structures for handling non-stationarity of neural population activity; 3) Robust self-supervised learning (SSL) objective with uniform causal masking strategies and contrastive representations. We pretrained two separate versions of RPNT on distinct datasets a) Multi-session, multi-task, and multi-subject microelectrode benchmark; b) Multi-site recordings using high-density Neuropixel 1.0 probes. The datasets include recordings from the dorsal premotor cortex (PMd) and from the primary motor cortex (M1) regions of nonhuman primates (NHPs) as they performed reaching tasks. After pretraining, we evaluated the generalization of RPNT in cross-session, cross-type, cross-subject, and cross-site downstream behavior decoding tasks. Our results show that RPNT consistently achieves and surpasses the decoding performance of existing decoding models in all tasks.

</details>


### [207] [A Mosco sufficient condition for intrinsic stability of non-unique convex Empirical Risk Minimization](https://arxiv.org/abs/2601.17646)
*Karim Bounja,Lahcen Laayouni,Abdeljalil Sakat*

Main category: cs.LG

TL;DR: 研究经验风险最小化（ERM）稳定性，确定Painlevé - Kuratowski上半连续性为ERM解对应关系的内在稳定性概念，刻画一种最小非退化定性状态并得出定量偏差界限。


<details>
  <summary>Details</summary>
Motivation: 传统ERM稳定性研究多基于单值输出，而凸非严格损失会产生集值极小化器，需对ERM稳定性进行新研究。

Method: 确定Painlevé - Kuratowski上半连续性为稳定性概念，分析Mosco一致扰动和局部有界极小化器等情况。

Result: Mosco一致扰动和局部有界极小化器意味着PK - u.s.c.、最小值连续性和消失间隙近极小化器的一致性，二次增长可得出显式定量偏差界限。

Conclusion: 为ERM解对应关系的稳定性研究提供了新的理论和刻画方式。

Abstract: Empirical risk minimization (ERM) stability is usually studied via single-valued outputs, while convex non-strict losses yield set-valued minimizers. We identify Painlevé-Kuratowski upper semicontinuity (PK-u.s.c.) as the intrinsic stability notion for the ERM solution correspondence (set-level Hadamard well-posedness) and a prerequisite to interpret stability of selections. We then characterize a minimal non-degenerate qualitative regime: Mosco-consistent perturbations and locally bounded minimizers imply PK-u.s.c., minimal-value continuity, and consistency of vanishing-gap near-minimizers. Quadratic growth yields explicit quantitative deviation bounds.

</details>


### [208] [Time-Varying Causal Treatment for Quantifying the Causal Effect of Short-Term Variations on Arctic Sea Ice Dynamics](https://arxiv.org/abs/2601.17647)
*Akila Sampath,Vandana Janeja,Jianwu Wang*

Main category: cs.LG

TL;DR: 提出KGCM - VAE模型量化海冰厚度和海表面高度(SSH)因果机制，实验显示效果优于基准，消融研究证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 量化冰融化和淡水分布因果关系对理解极地气候变化和海平面上升很重要，但传统深度学习模型在时空场景估计因果效应时有困难。

Method: 提出KGCM - VAE模型，集成速度调制方案，结合MMD平衡潜空间协变量分布，使用因果邻接约束解码器确保符合物理结构。

Result: 在合成和真实北极数据集实验中，KGCM - VAE的PEHE优于基准，MMD和因果邻接约束联合应用使估计误差降低1.88%。

Conclusion: KGCM - VAE在量化海冰厚度和SSH因果机制方面有效，能更好处理时空场景因果效应估计问题。

Abstract: Quantifying the causal relationship between ice melt and freshwater distribution is critical, as these complex interactions manifest as regional fluctuations in sea surface height (SSH). Leveraging SSH as a proxy for sea ice dynamics enables improved understanding of the feedback mechanisms driving polar climate change and global sea-level rise. However, conventional deep learning models often struggle with reliable treatment effect estimation in spatiotemporal settings due to unobserved confounders and the absence of physical constraints. To address these challenges, we propose the Knowledge-Guided Causal Model Variational Autoencoder (KGCM-VAE) to quantify causal mechanisms between sea ice thickness and SSH. The proposed framework integrates a velocity modulation scheme in which smoothed velocity signals are dynamically amplified via a sigmoid function governed by SSH transitions to generate physically grounded causal treatments. In addition, the model incorporates Maximum Mean Discrepancy (MMD) to balance treated and control covariate distributions in the latent space, along with a causal adjacency-constrained decoder to ensure alignment with established physical structures. Experimental results on both synthetic and real-world Arctic datasets demonstrate that KGCM-VAE achieves superior PEHE compared to state-of-the-art benchmarks. Ablation studies further confirm the effectiveness of the approach, showing that the joint application of MMD and causal adjacency constraints yields a 1.88\% reduction in estimation error.

</details>


### [209] [Entropic Risk-Aware Monte Carlo Tree Search](https://arxiv.org/abs/2601.17667)
*Pedro P. Santos,Jacopo Silvestrin,Alberto Sardinha,Francisco S. Melo*

Main category: cs.LG

TL;DR: 提出可证明正确的蒙特卡罗树搜索算法解决带熵风险度量目标的风险感知马尔可夫决策过程，进行非渐近分析并实验对比。


<details>
  <summary>Details</summary>
Motivation: 解决带熵风险度量目标的风险感知马尔可夫决策过程。

Method: 提出蒙特卡罗树搜索算法，利用动态规划公式结合基于上置信界的树搜索算法，进行非渐近分析。

Result: 算法根节点的经验熵风险度量收敛到最优值，有多项式遗憾集中性。

Conclusion: 算法可有效解决带熵风险度量目标的风险感知马尔可夫决策过程。

Abstract: We propose a provably correct Monte Carlo tree search (MCTS) algorithm for solving \textit{risk-aware} Markov decision processes (MDPs) with \textit{entropic risk measure} (ERM) objectives. We provide a \textit{non-asymptotic} analysis of our proposed algorithm, showing that the algorithm: (i) is \textit{correct} in the sense that the empirical ERM obtained at the root node converges to the optimal ERM; and (ii) enjoys \textit{polynomial regret concentration}. Our algorithm successfully exploits the dynamic programming formulations for solving risk-aware MDPs with ERM objectives introduced by previous works in the context of an upper confidence bound-based tree search algorithm. Finally, we provide a set of illustrative experiments comparing our risk-aware MCTS method against relevant baselines.

</details>


### [210] [Fast KVzip: Efficient and Accurate LLM Inference with Gated KV Eviction](https://arxiv.org/abs/2601.17668)
*Jang-Hyun Kim,Dongyoon Han,Sangdoo Yun*

Main category: cs.LG

TL;DR: 提出用于冻结权重LLMs的基于门控的KV缓存逐出方法，能高压缩比且低计算成本，实验显示可逐出70%缓存并近无损性能。


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存压缩技术在性能下降和计算开销间存在权衡，需要高效的管理方法。

Method: 引入轻量级汇聚注意力门控模块识别和保留关键KV对，提出基于LLM前向传播的门训练算法，避免昂贵的反向传播。

Result: 在Qwen2.5 - 1M、Qwen3和Gemma3系列模型上实验，可逐出70%的KV缓存，且在多种任务中保持近无损性能。

Conclusion: 该方法具有低计算成本、高压缩比和强任务泛化性，适用于LLMs的KV缓存管理。

Abstract: Efficient key-value (KV) cache management is crucial for the practical deployment of large language models (LLMs), yet existing compression techniques often incur a trade-off between performance degradation and computational overhead. We propose a novel gating-based KV cache eviction method for frozen-weight LLMs that achieves high compression ratios with negligible computational cost. Our approach introduces lightweight sink-attention gating modules to identify and retain critical KV pairs, and integrates seamlessly into both the prefill and decoding stages. The proposed gate training algorithm relies on forward passes of an LLM, avoiding expensive backpropagation, while achieving strong task generalization through a task-agnostic reconstruction objective. Extensive experiments across the Qwen2.5-1M, Qwen3, and Gemma3 families show that our method maintains near-lossless performance while evicting up to 70% of the KV cache. The results are consistent across a wide range of tasks, including long-context understanding, code comprehension, and mathematical reasoning, demonstrating the generality of our approach.

</details>


### [211] [$\infty$-MoE: Generalizing Mixture of Experts to Infinite Experts](https://arxiv.org/abs/2601.17680)
*Shota Takashiro,Takeshi Kojima,Shohei Taniguchi,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.LG

TL;DR: 提出∞-MoE方法，通过连续空间选参，增加专家数时稳定训练，实验显示模型性能优且可灵活权衡精度与速度。


<details>
  <summary>Details</summary>
Motivation: 传统MoE中专家相互独立且在离散空间组合，专家数量增加时难以有效训练，需在增加专家数时稳定训练。

Method: 提出∞-MoE，基于每个token采样的连续值选择大FFN的部分参数，在连续空间中考虑专家。

Result: 基于GPT - 2 Small的∞-MoE模型，129M活跃参数和186M总参数，性能与350M参数的密集GPT - 2 Medium相当，推理时调整采样专家数可灵活权衡精度与速度，精度比传统MoE最多提高2.5%。

Conclusion: ∞-MoE能在增加专家数时稳定训练，实现计算效率与性能的有效平衡，且可灵活调整精度和速度。

Abstract: The Mixture of Experts (MoE) selects a few feed-forward networks (FFNs) per token, achieving an effective trade-off between computational cost and performance. In conventional MoE, each expert is treated as entirely independent, and experts are combined in a discrete space. As a result, when the number of experts increases, it becomes difficult to train each expert effectively. To stabilize training while increasing the number of experts, we propose $\infty$-MoE that selects a portion of the parameters of large FFNs based on continuous values sampled for each token. By considering experts in a continuous space, this approach allows for an infinite number of experts while maintaining computational efficiency. Experiments show that a GPT-2 Small-based $\infty$-MoE model, with 129M active and 186M total parameters, achieves comparable performance to a dense GPT-2 Medium with 350M parameters. Adjusting the number of sampled experts at inference time allows for a flexible trade-off between accuracy and speed, with an improvement of up to 2.5\% in accuracy over conventional MoE.

</details>


### [212] [Agentic reinforcement learning empowers next-generation chemical language models for molecular design and synthesis](https://arxiv.org/abs/2601.17687)
*Hao Li,He Cao,Shenyao Peng,Zijing Liu,Bin Feng,Yu Wang,Zhiyuan Yan,Yonghong Tian,Yu Li,Li Yuan*

Main category: cs.LG

TL;DR: 介绍名为ChemCRAFT的框架，可让本地部署的小语言模型在化学领域高效推理、解决存储与成本问题，在药物设计评估中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前生化领域语言模型存在小模型易幻觉、知识保留有限，大模型有隐私风险和高推理成本的问题，需新方法解决。

Method: 引入ChemCRAFT框架，利用代理强化学习解耦化学推理和知识存储，构建代理轨迹管道和化学代理沙盒，创建ChemToolDataset，提出SMILES - GRPO构建奖励函数。

Result: ChemCRAFT在药物设计多方面评估中优于当前基于云的大语言模型。

Conclusion: 科学推理可通过工具编排学习，该工作为人工智能辅助化学建立经济高效、保护隐私的范式。

Abstract: Language models are revolutionizing the biochemistry domain, assisting scientists in drug design and chemical synthesis with high efficiency. Yet current approaches struggle between small language models prone to hallucination and limited knowledge retention, and large cloud-based language models plagued by privacy risks and high inference costs. To bridge this gap, we introduce ChemCRAFT, a novel framework leveraging agentic reinforcement learning to decouple chemical reasoning from knowledge storage. Instead of forcing the model to memorize vast chemical data, our approach empowers the language model to interact with a sandbox for precise information retrieval. This externalization of knowledge allows a locally deployable small model to achieve superior performance with minimal inference costs. To enable small language models for agent-calling ability, we build an agentic trajectory construction pipeline and a comprehensive chemical-agent sandbox. Based on sandbox interactions, we constructed ChemToolDataset, the first large-scale chemical tool trajectory dataset. Simultaneously, we propose SMILES-GRPO to build a dense chemical reward function, promoting the model's ability to call chemical agents. Evaluations across diverse aspects of drug design show that ChemCRAFT outperforms current cloud-based LLMs in molecular structure analysis, molecular optimization, and synthesis pathway prediction, demonstrating that scientific reasoning is not solely an emergent ability of model scale, but a learnable policy of tool orchestration. This work establishes a cost-effective and privacy-preserving paradigm for AI-aided chemistry, opening new avenues for accelerating molecular discovery with locally deployable agents.

</details>


### [213] [REV-INR: Regularized Evidential Implicit Neural Representation for Uncertainty-Aware Volume Visualization](https://arxiv.org/abs/2601.17689)
*Shanu Saklani,Tushar M. Athawale,Nairita Pal,David Pugmire,Christopher R. Johnson,Soumya Dutta*

Main category: cs.LG

TL;DR: 提出REV - INR解决传统INR模型无法给出预测不确定性的问题，有更好的重建质量和更快推理时间。


<details>
  <summary>Details</summary>
Motivation: 传统确定性INRs模型只能给出值预测，无法提供预测不确定性及数据噪声影响，导致数据解释和可视化不可靠，且难以识别错误结果。

Method: 引入REV - INR，通过单次前向传播学习准确预测数据值及相关坐标级数据不确定性和模型不确定性。

Result: REV - INR在最快推理时间下，实现最佳体积重建质量，有可靠的数据和模型不确定性估计。

Conclusion: REV - INR有助于评估提取的等值面和体积可视化结果的可靠性和可信度，可仅依靠模型预测数据进行分析。

Abstract: Applications of Implicit Neural Representations (INRs) have emerged as a promising deep learning approach for compactly representing large volumetric datasets. These models can act as surrogates for volume data, enabling efficient storage and on-demand reconstruction via model predictions. However, conventional deterministic INRs only provide value predictions without insights into the model's prediction uncertainty or the impact of inherent noisiness in the data. This limitation can lead to unreliable data interpretation and visualization due to prediction inaccuracies in the reconstructed volume. Identifying erroneous results extracted from model-predicted data may be infeasible, as raw data may be unavailable due to its large size. To address this challenge, we introduce REV-INR, Regularized Evidential Implicit Neural Representation, which learns to predict data values accurately along with the associated coordinate-level data uncertainty and model uncertainty using only a single forward pass of the trained REV-INR during inference. By comprehensively comparing and contrasting REV-INR with existing well-established deep uncertainty estimation methods, we show that REV-INR achieves the best volume reconstruction quality with robust data (aleatoric) and model (epistemic) uncertainty estimates using the fastest inference time. Consequently, we demonstrate that REV-INR facilitates assessment of the reliability and trustworthiness of the extracted isosurfaces and volume visualization results, enabling analyses to be solely driven by model-predicted data.

</details>


### [214] [FedCCA: Client-Centric Adaptation against Data Heterogeneity in Federated Learning on IoT Devices](https://arxiv.org/abs/2601.17713)
*Kaile Wang,Jiannong Cao,Yu Yang,Xiaoyin Li,Yinfeng Cao*

Main category: cs.LG

TL;DR: 提出以客户端为中心的自适应联邦学习算法FedCCA，利用客户端特定知识学习模型以缓解数据异质性影响，实验显示其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 物联网发展下需在私有数据上进行AI模型训练，联邦学习存在数据异质性问题，现有方法有局限性。

Method: 提出FedCCA算法，采用动态客户端选择、基于额外客户端特定编码器的自适应聚合和基于注意力的全局聚合策略。

Result: 在不同数据集上的实验表明，FedCCA在解决特定问题上比竞争基线有显著性能优势。

Conclusion: FedCCA算法能有效缓解数据异质性影响，提升联邦学习性能。

Abstract: With the rapid development of the Internet of Things (IoT), AI model training on private data such as human sensing data is highly desired. Federated learning (FL) has emerged as a privacy-preserving distributed training framework for this purpuse. However, the data heterogeneity issue among IoT devices can significantly degrade the model performance and convergence speed in FL. Existing approaches limit in fixed client selection and aggregation on cloud server, making the privacy-preserving extraction of client-specific information during local training challenging. To this end, we propose Client-Centric Adaptation federated learning (FedCCA), an algorithm that optimally utilizes client-specific knowledge to learn a unique model for each client through selective adaptation, aiming to alleviate the influence of data heterogeneity. Specifically, FedCCA employs dynamic client selection and adaptive aggregation based on the additional client-specific encoder. To enhance multi-source knowledge transfer, we adopt an attention-based global aggregation strategy. We conducted extensive experiments on diverse datasets to assess the efficacy of FedCCA. The experimental results demonstrate that our approach exhibits a substantial performance advantage over competing baselines in addressing this specific problem.

</details>


### [215] [Do Reasoning Models Ask Better Questions? A Formal Information-Theoretic Analysis on Multi-Turn LLM Games](https://arxiv.org/abs/2601.17716)
*Daniel M. Pedrozo,Telma W. de L. Soares,Bryan L. M. de Oliveira*

Main category: cs.LG

TL;DR: 提出多轮对话框架评估大语言模型通过是/否问题收集信息的能力，实验表明有显式推理能力的模型表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有基准缺乏基于信息增益的综合评估框架，且很少对使用思维链推理和不使用的模型进行系统比较。

Method: 提出多轮对话框架，采用三个相互作用的大语言模型代理，以信息增益为主要指标，在地理猜城市游戏中评估多种大语言模型变体。

Result: 有显式推理能力的模型每轮信息增益更高，解决问题的步骤更少，小模型通过更积极探索候选问题弥补能力限制，大模型选择最优查询更果断。

Conclusion: 有显式推理能力的大语言模型在信息收集方面表现更优，不同规模模型有不同策略。

Abstract: Large Language Models (LLMs) excel at many tasks but still struggle with a critical ability for LLM-based agents: asking good questions for resolving ambiguity in user requests. While prior work has explored information-seeking behavior through word games, existing benchmarks lack comprehensive evaluation frameworks that provide both final and intermediate signals based on Information Gain (IG). Moreover, they rarely provide systematic comparisons between models that use chain-of-thought reasoning and those that do not. We propose a multi-turn dialogue framework that quantitatively measures how effectively LLMs gather information through yes/no questions in a hierarchical knowledge graph environment. Our framework employs a triad of interacting LLM agents that ask questions, answer them, and update the hypothesis space. We adopt IG as the main metric, grounded in Shannon entropy, to assess query effectiveness at each turn and cumulatively. We instantiate our framework in a geographical Guess My City game setting organized in a five-level taxonomy and evaluate multiple LLM variants under fully and partially observable conditions, with and without Chain-of-Thought reasoning. Our experiments demonstrate that, among the evaluated models, the ones with explicit reasoning capabilities achieve higher IG per turn and reach solutions in fewer steps, particularly in partially observable settings. Analysis of reasoning traces reveals that smaller models compensate for limited capacity through more aggressive exploration of candidate questions, while larger models exhibit higher assertiveness in selecting optimal queries, generating candidates with greater potential IG.

</details>


### [216] [AR-Omni: A Unified Autoregressive Model for Any-to-Any Generation](https://arxiv.org/abs/2601.17761)
*Dongjie Cheng,Ruifeng Yuan,Yongqi Li,Runyang You,Wenjie Wang,Liqiang Nie,Lei Zhang,Wenjie Li*

Main category: cs.LG

TL;DR: 本文提出无专家解码器的自回归范式统一模型AR - Omni，支持多模态生成，解决统一AR建模的三个实际问题，实现多模态高质量实时生成。


<details>
  <summary>Details</summary>
Motivation: 现实中感知和交互是多模态的促使开发Omni MLLMs，但现有系统依赖额外专家组件，而自回归建模在文本领域具有优势，激发了研究无专家解码器统一模型的动力。

Method: 提出AR - Omni模型，在单个Transformer解码器下实现多模态生成；通过任务感知损失重加权解决模态不平衡问题，用轻量级标记级感知对齐损失解决视觉保真度问题，用有限状态解码机制解决稳定性 - 创造性权衡问题。

Result: AR - Omni在三种模态上都达到了较高质量，语音生成实时因子达到0.88。

Conclusion: AR - Omni是一种有效的无专家解码器的自回归范式统一模型，可实现多模态实时高质量生成。

Abstract: Real-world perception and interaction are inherently multimodal, encompassing not only language but also vision and speech, which motivates the development of "Omni" MLLMs that support both multimodal inputs and multimodal outputs. While a sequence of omni MLLMs has emerged, most existing systems still rely on additional expert components to achieve multimodal generation, limiting the simplicity of unified training and inference. Autoregressive (AR) modeling, with a single token stream, a single next-token objective, and a single decoder, is an elegant and scalable foundation in the text domain. Motivated by this, we present AR-Omni, a unified any-to-any model in the autoregressive paradigm without any expert decoders. AR-Omni supports autoregressive text and image generation, as well as streaming speech generation, all under a single Transformer decoder. We further address three practical issues in unified AR modeling: modality imbalance via task-aware loss reweighting, visual fidelity via a lightweight token-level perceptual alignment loss for image tokens, and stability-creativity trade-offs via a finite-state decoding mechanism. Empirically, AR-Omni achieves strong quality across three modalities while remaining real-time, achieving a 0.88 real-time factor for speech generation.

</details>


### [217] [Shortcut Learning in Binary Classifier Black Boxes: Applications to Voice Anti-Spoofing and Biometrics](https://arxiv.org/abs/2601.17782)
*Md Sahidullah,Hye-jin Shim,Rosa Gonzalez Hautamäki,Tomi H. Kinnunen*

Main category: cs.LG

TL;DR: 研究解决数据集偏差挑战，探索二元分类器中的‘捷径学习’，提出分析黑盒分类器框架，通过实验证明方法有效，对解决其他领域偏差有启示。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型广泛应用，数据集和模型中的偏差会导致意外结果，需解决数据集偏差挑战。

Method: 提出结合干预和观察视角的新颖框架，采用线性混合效应模型进行事后分析。

Result: 通过音频反欺骗和说话人验证任务实验证明了方法的有效性。

Conclusion: 研究见解对解决其他领域偏差和推动可解释人工智能领域有更广泛的意义。

Abstract: The widespread adoption of deep-learning models in data-driven applications has drawn attention to the potential risks associated with biased datasets and models. Neglected or hidden biases within datasets and models can lead to unexpected results. This study addresses the challenges of dataset bias and explores ``shortcut learning'' or ``Clever Hans effect'' in binary classifiers. We propose a novel framework for analyzing the black-box classifiers and for examining the impact of both training and test data on classifier scores. Our framework incorporates intervention and observational perspectives, employing a linear mixed-effects model for post-hoc analysis. By evaluating classifier performance beyond error rates, we aim to provide insights into biased datasets and offer a comprehensive understanding of their influence on classifier behavior. The effectiveness of our approach is demonstrated through experiments on audio anti-spoofing and speaker verification tasks using both statistical models and deep neural networks. The insights gained from this study have broader implications for tackling biases in other domains and advancing the field of explainable artificial intelligence.

</details>


### [218] [Robust Computational Extraction of Non-Enhancing Hypercellular Tumor Regions from Clinical Imaging Data](https://arxiv.org/abs/2601.17802)
*A. Brawanski,Th. Schaffer,F. Raab,K. -M. Schebesch,M. Schrey,Chr. Doenitz,A. M. Tomé,E. W. Lang*

Main category: cs.LG

TL;DR: 提出计算框架从常规MRI数据生成非增强高细胞肿瘤区域概率图，经验证可靠，可用于临床工作流和精准肿瘤学。


<details>
  <summary>Details</summary>
Motivation: 准确识别非增强高细胞肿瘤区域是神经肿瘤影像领域的未满足需求，对患者管理和治疗规划意义重大。

Method: 利用多种网络架构，从常规MRI数据生成非增强高细胞区域概率图。

Result: 该方法与相对脑血容量和增强肿瘤复发位置等独立临床标志物验证，显示出方法稳健性和生物学相关性。

Conclusion: 该框架能可靠、无创地绘制非增强高细胞肿瘤区域，有助于其作为影像生物标志物融入临床工作流，推动脑肿瘤患者的精准肿瘤学发展。

Abstract: Accurate identification of non-enhancing hypercellular (NEH) tumor regions is an unmet need in neuro-oncological imaging, with significant implications for patient management and treatment planning. We present a robust computational framework that generates probability maps of NEH regions from routine MRI data, leveraging multiple network architectures to address the inherent variability and lack of clear imaging boundaries. Our approach was validated against independent clinical markers -- relative cerebral blood volume (rCBV) and enhancing tumor recurrence location (ETRL) -- demonstrating both methodological robustness and biological relevance. This framework enables reliable, non-invasive mapping of NEH tumor compartments, supporting their integration as imaging biomarkers in clinical workflows and advancing precision oncology for brain tumor patients.

</details>


### [219] [MergeMix: Optimizing Mid-Training Data Mixtures via Learnable Model Merging](https://arxiv.org/abs/2601.17858)
*Jiapeng Wang,Changxin Tian,Kunlong Chen,Ziqi Liu,Jiaxin Mao,Wayne Xin Zhao,Zhiqiang Zhang,Jun Zhou*

Main category: cs.LG

TL;DR: 提出MergeMix方法优化大语言模型数据混合比例，降低搜索成本且性能佳。


<details>
  <summary>Details</summary>
Motivation: 现有确定大语言模型数据最优混合比例的方法计算成本高，需依赖启发式试验或昂贵的代理训练。

Method: 引入MergeMix，将模型合并权重用作高性能、低成本的性能代理，在最少标记上训练特定领域专家并针对下游基准优化合并权重。

Result: 在8B和16B参数模型上实验表明，MergeMix性能与手动调优相当或更优，大幅降低搜索成本，具有高排名一致性和强跨规模可迁移性。

Conclusion: MergeMix为数据混合优化提供了可扩展的自动化解决方案。

Abstract: Optimizing data mixtures is essential for unlocking the full potential of large language models (LLMs), yet identifying the optimal composition remains computationally prohibitive due to reliance on heuristic trials or expensive proxy training. To address this, we introduce \textbf{MergeMix}, a novel approach that efficiently determines optimal data mixing ratios by repurposing model merging weights as a high-fidelity, low-cost performance proxy. By training domain-specific experts on minimal tokens and optimizing their merging weights against downstream benchmarks, MergeMix effectively optimizes the performance of data mixtures without incurring the cost of full-scale training. Extensive experiments on models with 8B and 16B parameters validate that MergeMix achieves performance comparable to or surpassing exhaustive manual tuning while drastically reducing search costs. Furthermore, MergeMix exhibits high rank consistency (Spearman $ρ> 0.9$) and strong cross-scale transferability, offering a scalable, automated solution for data mixture optimization.

</details>


### [220] [EEG Foundation Models: Progresses, Benchmarking, and Open Problems](https://arxiv.org/abs/2601.17883)
*Dingkun Liu,Yuheng Chen,Zhu Chen,Zhenyao Cui,Yaozhi Wen,Jiayu An,Jingwei Luo,Dongrui Wu*

Main category: cs.LG

TL;DR: 本文对现有脑电图(EEG)基础模型进行公平全面比较，回顾50个模型，评估12个开源模型和基线，得出线性探测常不足等结论。


<details>
  <summary>Details</summary>
Motivation: 现有EEG基础模型因各种不一致因素，缺乏公平全面的比较，本文旨在弥补这一差距。

Method: 回顾50个代表性模型并构建统一分类框架；在13个EEG数据集上评估12个开源基础模型和基线；考虑跨主体泛化和少样本校准；比较全参数微调与线性探测；研究模型规模与下游性能关系。

Result: 1) 线性探测常不足；2) 从头训练的专业模型在许多任务中仍具竞争力；3) 在当前数据和训练实践下，更大基础模型不一定带来更好泛化性能。

Conclusion: 现有EEG基础模型在不同评估场景下有不同表现，线性探测等方法有局限性，模型规模与性能关系并非简单正相关。

Abstract: Electroencephalography (EEG) foundation models have recently emerged as a promising paradigm for brain-computer interfaces (BCIs), aiming to learn transferable neural representations from large-scale heterogeneous recordings. Despite rapid progresses, there lacks fair and comprehensive comparisons of existing EEG foundation models, due to inconsistent pre-training objectives, preprocessing choices, and downstream evaluation protocols. This paper fills this gap. We first review 50 representative models and organize their design choices into a unified taxonomic framework including data standardization, model architectures, and self-supervised pre-training strategies. We then evaluate 12 open-source foundation models and competitive specialist baselines across 13 EEG datasets spanning nine BCI paradigms. Emphasizing real-world deployments, we consider both cross-subject generalization under a leave-one-subject-out protocol and rapid calibration under a within-subject few-shot setting. We further compare full-parameter fine-tuning with linear probing to assess the transferability of pre-trained representations, and examine the relationship between model scale and downstream performance. Our results indicate that: 1) linear probing is frequently insufficient; 2) specialist models trained from scratch remain competitive across many tasks; and, 3) larger foundation models do not necessarily yield better generalization performance under current data regimes and training practices.

</details>


### [221] [Adaptive Weighting in Knowledge Distillation: An Axiomatic Framework for Multi-Scale Teacher Ensemble Optimization](https://arxiv.org/abs/2601.17910)
*Aaron R. Flouro,Shawn P. Chadwick*

Main category: cs.LG

TL;DR: 本文提出多教师知识蒸馏自适应加权的公理框架，分析相关性质并实现结果与公式解耦。


<details>
  <summary>Details</summary>
Motivation: 现有多教师知识蒸馏方法多依赖启发式或特定实现的加权方案，需要改进。

Method: 开发跨标记、任务和上下文三个互补尺度的算子无关公理框架，形式化自适应加权算子条件。

Result: 确立符合框架算子的存在性和非唯一性，分析性质并给出安全约束蒸馏的抽象公式。

Conclusion: 结果实现理论保证与具体加权公式解耦，可分析异质性、分布偏移和安全约束下的自适应蒸馏方法。

Abstract: Knowledge distillation with multiple teachers is increasingly used to improve robustness, efficiency, and safety, yet existing approaches rely largely on heuristic or implementation-specific weighting schemes. This paper develops an operator-agnostic axiomatic framework for adaptive weighting in multi-teacher knowledge distillation across three complementary scales: token, task, and context. We formalize structural conditions under which adaptive weighting operators are well-defined, admit multiple non-equivalent implementations, and can be hierarchically composed via product-structure normalization. Within this framework, we establish existence and non-uniqueness of conforming operators, characterize convergence of gradient-based optimization under standard assumptions, analyze stability and perturbation robustness, and provide an abstract formulation of safety-constrained distillation. The results decouple theoretical guarantees from specific weighting formulas, enabling principled analysis of adaptive distillation methods under heterogeneity, distribution shift, and safety constraints.

</details>


### [222] [Causal Pre-training Under the Fairness Lens: An Empirical Study of TabPFN](https://arxiv.org/abs/2601.17912)
*Qinyi Liu,Mohammad Khalil,Naman Goel*

Main category: cs.LG

TL;DR: 对TabPFN及其微调变体进行综合评估，发现其预测准确性强、对虚假相关性有鲁棒性，但公平性改善有限。


<details>
  <summary>Details</summary>
Motivation: 现有研究未充分探索表格数据基础模型（如TabPFN）的公平性，因此开展相关评估。

Method: 对TabPFN及其微调变体进行综合实证评估，在不同数据集大小和分布偏移情况下评估预测性能、公平性和鲁棒性。

Result: TabPFN预测准确性优于基线，对虚假相关性有鲁棒性，但公平性改善有限且不稳定，尤其在MNAR协变量偏移下。

Conclusion: TabPFN的因果预训练对算法公平性有帮助但不足，实际部署需进一步公平干预。

Abstract: Foundation models for tabular data, such as the Tabular Prior-data Fitted Network (TabPFN), are pre-trained on a massive number of synthetic datasets generated by structural causal models (SCM). They leverage in-context learning to offer high predictive accuracy in real-world tasks. However, the fairness properties of these foundational models, which incorporate ideas from causal reasoning during pre-training, have not yet been explored in sufficient depth. In this work, we conduct a comprehensive empirical evaluation of TabPFN and its fine-tuned variants, assessing predictive performance, fairness, and robustness across varying dataset sizes and distributional shifts. Our results reveal that while TabPFN achieves stronger predictive accuracy compared to baselines and exhibits robustness to spurious correlations, improvements in fairness are moderate and inconsistent, particularly under missing-not-at-random (MNAR) covariate shifts. These findings suggest that the causal pre-training in TabPFN is helpful but insufficient for algorithmic fairness, highlighting implications for deploying such models in practice and the need for further fairness interventions.

</details>


### [223] [UniPACT: A Multimodal Framework for Prognostic Question Answering on Raw ECG and Structured EHR](https://arxiv.org/abs/2601.17916)
*Jialu Tang,Tong Xia,Yuan Lu,Aaqib Saeed*

Main category: cs.LG

TL;DR: 提出UniPACT框架融合EHR和ECG数据用于临床预后问答，在MDS - ED基准上表现出色。


<details>
  <summary>Details</summary>
Motivation: 准确的临床预后需融合EHR和实时生理信号，但大语言模型难以处理异构、非文本数据。

Method: 提出UniPACT框架，用结构化提示机制将EHR数据转为文本，再与ECG数据特征融合，让大语言模型进行推理。

Result: 在MDS - ED基准上，完成多种预后任务，达到89.37%的平均AUROC，超过专业基线模型。

Conclusion: 多模态、多任务方法对性能至关重要，在数据缺失情况下有鲁棒性。

Abstract: Accurate clinical prognosis requires synthesizing structured Electronic Health Records (EHRs) with real-time physiological signals like the Electrocardiogram (ECG). Large Language Models (LLMs) offer a powerful reasoning engine for this task but struggle to natively process these heterogeneous, non-textual data types. To address this, we propose UniPACT (Unified Prognostic Question Answering for Clinical Time-series), a unified framework for prognostic question answering that bridges this modality gap. UniPACT's core contribution is a structured prompting mechanism that converts numerical EHR data into semantically rich text. This textualized patient context is then fused with representations learned directly from raw ECG waveforms, enabling an LLM to reason over both modalities holistically. We evaluate UniPACT on the comprehensive MDS-ED benchmark, it achieves a state-of-the-art mean AUROC of 89.37% across a diverse set of prognostic tasks including diagnosis, deterioration, ICU admission, and mortality, outperforming specialized baselines. Further analysis demonstrates that our multimodal, multi-task approach is critical for performance and provides robustness in missing data scenarios.

</details>


### [224] [treaming-dLLM: Accelerating Diffusion LLMs via Suffix Pruning and Dynamic Decoding](https://arxiv.org/abs/2601.17917)
*Zhongyu Xiao,Zhiwei Hao,Jianyuan Guo,Yong Luo,Jia Liu,Jie Xu,Han Hu*

Main category: cs.LG

TL;DR: 提出Streaming - dLLM框架优化Diffusion大语言模型推理效率，实现加速且保证生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有工作忽略块级扩散过程的内在低效问题，如空间冗余和时间低效。

Method: 提出无训练框架Streaming - dLLM，空间上引入衰减引导后缀建模，时间上采用动态置信度感知策略和提前退出机制。

Result: Streaming - dLLM实现高达68.2倍加速，同时保持生成质量。

Conclusion: Streaming - dLLM在扩散解码中有效。

Abstract: Diffusion Large Language Models (dLLMs) offer a compelling paradigm for natural language generation, leveraging parallel decoding and bidirectional attention to achieve superior global coherence compared to autoregressive models. While recent works have accelerated inference via KV cache reuse or heuristic decoding, they overlook the intrinsic inefficiencies within the block-wise diffusion process. Specifically, they suffer from spatial redundancy by modeling informative-sparse suffix regions uniformly and temporal inefficiency by applying fixed denoising schedules across all the decoding process. To address this, we propose Streaming-dLLM, a training-free framework that streamlines inference across both spatial and temporal dimensions. Spatially, we introduce attenuation guided suffix modeling to approximate the full context by pruning redundant mask tokens. Temporally, we employ a dynamic confidence aware strategy with an early exit mechanism, allowing the model to skip unnecessary iterations for converged tokens. Extensive experiments show that Streaming-dLLM achieves up to 68.2X speedup while maintaining generation quality, highlighting its effectiveness in diffusion decoding. The code is available at https://github.com/xiaoshideta/Streaming-dLLM.

</details>


### [225] [Dissipative Learning: A Framework for Viable Adaptive Systems](https://arxiv.org/abs/2601.17933)
*Laurent Caraffa*

Main category: cs.LG

TL;DR: 提出学习是耗散过程的观点，引入BEDS框架，证明Fisher - Rao正则化最优，统一现有方法，对过拟合等现象给出新解释，框架可扩展到连续和多智能体系统。


<details>
  <summary>Details</summary>
Motivation: 重新审视学习过程，将遗忘和正则化视为自适应系统的结构要求，提供对学习、遗忘和正则化的新视角。

Method: 结合信息论、热力学和信息几何，引入BEDS框架，提出条件最优定理。

Result: 证明Fisher - Rao正则化是热力学最优策略，欧几里得正则化次优，统一现有方法，对过拟合和灾难性遗忘给出新解释。

Conclusion: 将学习重新定义为在耗散约束下维持可行的信念状态，为遗忘、正则化和稳定性提供了原则性视角。

Abstract: We propose a perspective in which learning is an intrinsically dissipative process. Forgetting and regularization are not heuristic add-ons but structural requirements for adaptive systems. Drawing on information theory, thermodynamics, and information geometry, we introduce the BEDS (Bayesian Emergent Dissipative Structures) framework, modeling learning as the evolution of compressed belief states under dissipation constraints.
  A central contribution is the Conditional Optimality Theorem, showing that Fisher-Rao regularization measuring change via information divergence rather than Euclidean distance is the unique thermodynamically optimal regularization strategy, achieving minimal dissipation. Euclidean regularization is shown to be structurally suboptimal. The framework unifies existing methods (Ridge, SIGReg, EMA, SAC) as special cases of a single governing equation.
  Within this view, overfitting corresponds to over-crystallization, while catastrophic forgetting reflects insufficient dissipation control. The framework distinguishes BEDS-crystallizable problems, where beliefs converge to stable equilibria, from BEDS-maintainable problems, which require continual adaptation. It extends naturally to continual and multi-agent systems, where viability, stability under adaptation and finite resources replaces asymptotic optimality as the primary criterion. Overall, this work reframes learning as maintaining viable belief states under dissipation constraints, providing a principled lens on forgetting, regularization, and stability.

</details>


### [226] [FedGraph-VASP: Privacy-Preserving Federated Graph Learning with Post-Quantum Security for Cross-Institutional Anti-Money Laundering](https://arxiv.org/abs/2601.17935)
*Daniel Commey,Matilda Nkoom,Yousef Alsenani,Sena G. Hounsinou,Garth V. Crosby*

Main category: cs.LG

TL;DR: 提出FedGraph - VASP隐私保护联邦图学习框架用于协作反洗钱，实验显示其在不同数据集和拓扑下表现有差异，存在拓扑依赖权衡。


<details>
  <summary>Details</summary>
Motivation: 虚拟资产服务提供商在检测跨机构洗钱时面临监管合规和用户隐私的矛盾，现有方法存在问题，无法检测关键跨链洗钱模式。

Method: 提出Boundary Embedding Exchange协议，仅共享边界账户的压缩、不可逆图神经网络表示，使用后量子密码学（Kyber - 512结合AES - 256 - GCM）保障安全。

Result: 在Elliptic Bitcoin数据集上F1分数为0.508，优于FedSage +；在低连接性设置下表现稳健，高连接性接近集中式性能；在Ethereum数据集上，不同拓扑下表现不同。隐私审计显示嵌入仅部分可逆。

Conclusion: 嵌入交换有利于连接的交易图，生成插补在高度模块化的稀疏图中占优势，存在拓扑依赖的权衡。

Abstract: Virtual Asset Service Providers (VASPs) face a fundamental tension between regulatory compliance and user privacy when detecting cross-institutional money laundering. Current approaches require either sharing sensitive transaction data or operating in isolation, leaving critical cross-chain laundering patterns undetected. We present FedGraph-VASP, a privacy-preserving federated graph learning framework that enables collaborative anti-money laundering (AML) without exposing raw user data. Our key contribution is a Boundary Embedding Exchange protocol that shares only compressed, non-invertible graph neural network representations of boundary accounts. These exchanges are secured using post-quantum cryptography, specifically the NIST-standardized Kyber-512 key encapsulation mechanism combined with AES-256-GCM authenticated encryption. Experiments on the Elliptic Bitcoin dataset with realistic Louvain partitioning show that FedGraph-VASP achieves an F1-score of 0.508, outperforming the state-of-the-art generative baseline FedSage+ (F1 = 0.453) by 12.1 percent on binary fraud detection. We further show robustness under low-connectivity settings where generative imputation degrades performance, while approaching centralized performance (F1 = 0.620) in high-connectivity regimes. We additionally evaluate generalization on an Ethereum fraud detection dataset, where FedGraph-VASP (F1 = 0.635) is less effective under sparse cross-silo connectivity, while FedSage+ excels (F1 = 0.855), outperforming even local training (F1 = 0.785). These results highlight a topology-dependent trade-off: embedding exchange benefits connected transaction graphs, whereas generative imputation can dominate in highly modular sparse graphs. A privacy audit shows embeddings are only partially invertible (R^2 = 0.32), limiting exact feature recovery.

</details>


### [227] [Scaling Effects and Uncertainty Quantification in Neural Actor Critic Algorithms](https://arxiv.org/abs/2601.17954)
*Nikos Georgoudios,Konstantinos Spiliopoulos,Justin Sirignano*

Main category: cs.LG

TL;DR: 研究使用浅层神经网络的神经Actor - Critic算法，关注不同缩放方案下网络输出收敛性及近似误差控制，推导渐近展开，给出超参数选择准则。


<details>
  <summary>Details</summary>
Motivation: 以往研究聚焦收敛速度，本文旨在对神经Actor - Critic算法输出进行更全面统计刻画，量化不确定性。

Method: 研究网络宽度的一般逆多项式缩放，将指数作为可调超参数，推导网络输出的渐近展开。

Result: 方差随网络宽度幂次衰减，缩放参数接近1时统计鲁棒性增强，数值实验支持该结论且显示收敛更快。

Conclusion: 分析给出算法超参数（学习率、探索率等）作为网络宽度和缩放参数函数的选择准则，确保良好统计行为。

Abstract: We investigate the neural Actor Critic algorithm using shallow neural networks for both the Actor and Critic models. The focus of this work is twofold: first, to compare the convergence properties of the network outputs under various scaling schemes as the network width and the number of training steps tend to infinity; and second, to provide precise control of the approximation error associated with each scaling regime. Previous work has shown convergence to ordinary differential equations with random initial conditions under inverse square root scaling in the network width. In this work, we shift the focus from convergence speed alone to a more comprehensive statistical characterization of the algorithm's output, with the goal of quantifying uncertainty in neural Actor Critic methods. Specifically, we study a general inverse polynomial scaling in the network width, with an exponent treated as a tunable hyperparameter taking values strictly between one half and one. We derive an asymptotic expansion of the network outputs, interpreted as statistical estimators, in order to clarify their structure. To leading order, we show that the variance decays as a power of the network width, with an exponent equal to one half minus the scaling parameter, implying improved statistical robustness as the scaling parameter approaches one. Numerical experiments support this behavior and further suggest faster convergence for this choice of scaling. Finally, our analysis yields concrete guidelines for selecting algorithmic hyperparameters, including learning rates and exploration rates, as functions of the network width and the scaling parameter, ensuring provably favorable statistical behavior.

</details>


### [228] [TensorLens: End-to-End Transformer Analysis via High-Order Attention Tensors](https://arxiv.org/abs/2601.17958)
*Ido Andrew Atad,Itamar Zimerman,Shahar Katz,Lior Wolf*

Main category: cs.LG

TL;DR: 论文提出 TensorLens 新方法，用高阶注意力交互张量将整个 Transformer 表示为单一的、依赖输入的线性算子，理论合理且具丰富表示。


<details>
  <summary>Details</summary>
Motivation: 现有对注意力矩阵分析多关注个体，缺乏统一完整的表示来涵盖所有 Transformer 块。

Method: 引入 TensorLens，用高阶注意力交互张量将整个 Transformer 表示为依赖输入的线性算子。

Result: TensorLens 比以往注意力聚合方法有更丰富的表示，注意力张量可用于开发解释性和模型理解工具。

Conclusion: TensorLens 为模型提供了理论连贯且表达力强的线性表示，可作为开发解释性和模型理解工具的基础。

Abstract: Attention matrices are fundamental to transformer research, supporting a broad range of applications including interpretability, visualization, manipulation, and distillation. Yet, most existing analyses focus on individual attention heads or layers, failing to account for the model's global behavior. While prior efforts have extended attention formulations across multiple heads via averaging and matrix multiplications or incorporated components such as normalization and FFNs, a unified and complete representation that encapsulates all transformer blocks is still lacking. We address this gap by introducing TensorLens, a novel formulation that captures the entire transformer as a single, input-dependent linear operator expressed through a high-order attention-interaction tensor. This tensor jointly encodes attention, FFNs, activations, normalizations, and residual connections, offering a theoretically coherent and expressive linear representation of the model's computation. TensorLens is theoretically grounded and our empirical validation shows that it yields richer representations than previous attention-aggregation methods. Our experiments demonstrate that the attention tensor can serve as a powerful foundation for developing tools aimed at interpretability and model understanding. Our code is attached as a supplementary.

</details>


### [229] [Federated learning for unpaired multimodal data through a homogeneous transformer model](https://arxiv.org/abs/2601.17986)
*Anders Eklund*

Main category: cs.LG

TL;DR: 提出一种跨去中心化节点训练全局多模态变压器的新框架，实现从碎片化、不相交和私有数据中学习统一表示。


<details>
  <summary>Details</summary>
Motivation: 当前多模态基础模型训练依赖集中式数据中心，现实联邦环境数据不配对且分散，现有联邦学习方法失效。

Method: 引入公共锚集对齐不相交的私有流形，用中心核对齐强制跨模态语义对齐，采用子空间稳定微调方法，提出精度加权平均。

Result: 建立了联邦非配对基础模型的数学基础。

Conclusion: 该框架能让全局模型从非集中存储、非配对样本的私有数据中学习统一的世界表示。

Abstract: Training of multimodal foundation models is currently restricted to centralized data centers containing massive, aligned datasets (e.g., image-text pairs). However, in realistic federated environments, data is often unpaired and fragmented across disjoint nodes; one node may hold sensor data, while another holds textual logs. These datasets are strictly private and share no common samples. Current federated learning (FL) methods fail in this regime, as they assume local clients possess aligned pairs or require sharing raw feature embeddings, which violates data sovereignty. We propose a novel framework to train a global multimodal transformer across decentralized nodes with disjoint modalities. We introduce a small public anchor set to align disjoint private manifolds. Using Gram matrices calculated from these public anchors, we enforce semantic alignment across modalities through centered kernel alignment without ever transmitting private samples, offering a mathematically superior privacy guarantee compared to prototype sharing. Further, we introduce a subspace-stabilized fine-tuning method to handle FL with huge transformer models. We strictly decouple domain-specific magnitude shifts from semantic direction, ensuring that nodes with varying sensor characteristics align geometrically to the global consensus. Lastly, we propose precision weighted averaging, where efficiently obtained uncertainty estimates are used to downweight uncertain nodes. This paper establishes the mathematical backbone for federated unpaired foundation models, enabling a global model to learn a unified representation of the world from fragmented, disjoint, and private data silos without requiring centralized storage or paired samples.

</details>


### [230] [Systematic Characterization of Minimal Deep Learning Architectures: A Unified Analysis of Convergence, Pruning, and Quantization](https://arxiv.org/abs/2601.17987)
*Ziwei Zheng,Huizhi Liang,Vaclav Snasel,Vito Latora,Panos Pardalos,Giuseppe Nicosia,Varun Ojha*

Main category: cs.LG

TL;DR: 本文提出计算方法探索收敛、剪枝和量化关系，在图像分类任务上发现性能相对稳定及三种学习动态，还给出不同架构模型相关特性，为选模型提供指导。


<details>
  <summary>Details</summary>
Motivation: 深度学习网络虽擅长分类，但确定可靠解决任务的最小架构仍具挑战，需探索收敛、剪枝和量化间的关系。

Method: 先对大量架构进行结构化设计扫描，再评估代表模型的收敛行为、剪枝敏感性和量化鲁棒性。

Result: 性能大多不变且学习动态有三种；明确稳定学习所需最小可学习参数；发现不同收敛和剪枝阶段；量化对参数少模型和难图像数据集影响更大；更深架构更抗剪枝，参数冗余达60%。

Conclusion: 研究结果为图像分类中在剪枝和低精度约束下选择紧凑、稳定模型提供了可行指导。

Abstract: Deep learning networks excel at classification, yet identifying minimal architectures that reliably solve a task remains challenging. We present a computational methodology for systematically exploring and analyzing the relationships among convergence, pruning, and quantization. The workflow first performs a structured design sweep across a large set of architectures, then evaluates convergence behavior, pruning sensitivity, and quantization robustness on representative models. Focusing on well-known image classification of increasing complexity, and across Deep Neural Networks, Convolutional Neural Networks, and Vision Transformers, our initial results show that, despite architectural diversity, performance is largely invariant and learning dynamics consistently exhibit three regimes: unstable, learning, and overfitting. We further characterize the minimal learnable parameters required for stable learning, uncover distinct convergence and pruning phases, and quantify the effect of reduced numeric precision on trainable parameters. Aligning with intuition, the results confirm that deeper architectures are more resilient to pruning than shallower ones, with parameter redundancy as high as 60%, and quantization impacts models with fewer learnable parameters more severely and has a larger effect on harder image datasets. These findings provide actionable guidance for selecting compact, stable models under pruning and low-precision constraints in image classification.

</details>


### [231] [Coding-Enforced Resilient and Secure Aggregation for Hierarchical Federated Learning](https://arxiv.org/abs/2601.17995)
*Shudi Weng,Ming Xiao,Mikael Skoglund*

Main category: cs.LG

TL;DR: 提出H - SecCoGC方案解决HFL中不可靠通信下保障模型精度与隐私的挑战，展现优越性


<details>
  <summary>Details</summary>
Motivation: HFL在不可靠通信下难以在保障隐私的同时确保模型精度，且隐私噪声协调会被随机干扰

Method: 提出一种稳健的分层安全聚合方案H - SecCoGC，集成编码策略实现结构化聚合

Result: 不仅能在不同隐私级别下确保全局模型准确构建，还避免了部分参与问题，提升了鲁棒性、隐私保护和学习效率

Conclusion: 理论分析和实验结果表明，该方案在不可靠通信和任意强隐私保障下具有优越性

Abstract: Hierarchical federated learning (HFL) has emerged as an effective paradigm to enhance link quality between clients and the server. However, ensuring model accuracy while preserving privacy under unreliable communication remains a key challenge in HFL, as the coordination among privacy noise can be randomly disrupted. To address this limitation, we propose a robust hierarchical secure aggregation scheme, termed H-SecCoGC, which integrates coding strategies to enforce structured aggregation. The proposed scheme not only ensures accurate global model construction under varying levels of privacy, but also avoids the partial participation issue, thereby significantly improving robustness, privacy preservation, and learning efficiency. Both theoretical analyses and experimental results demonstrate the superiority of our scheme under unreliable communication across arbitrarily strong privacy guarantees

</details>


### [232] [Spelling Bee Embeddings for Language Modeling](https://arxiv.org/abs/2601.18030)
*Markus N. Rabe,Judith Clymo,Zheren Dong*

Main category: cs.LG

TL;DR: 对嵌入层进行简单修改，加入拼写信息，模型在拼写及标准基准测试中表现提升，且节省计算和数据。


<details>
  <summary>Details</summary>
Motivation: 提升模型在拼写和标准基准测试中的性能。

Method: 对嵌入层进行简单修改，将拼写信息融入令牌嵌入。

Result: 使用这些嵌入训练的模型在拼写和标准基准上均有提升，40M到800M参数模型实现相同测试损失约可减少8%计算和数据需求。

Conclusion: 向嵌入层注入拼写信息能有效提升模型性能并节省资源。

Abstract: We introduce a simple modification to the embedding layer. The key change is to infuse token embeddings with information about their spelling. Models trained with these embeddings improve not only on spelling, but also across standard benchmarks. We conduct scaling studies for models with 40M to 800M parameters, which suggest that the improvements are equivalent to needing about 8% less compute and data to achieve the same test loss.

</details>


### [233] [Multimodal Machine Learning for Soft High-k Elastomers under Data Scarcity](https://arxiv.org/abs/2601.18032)
*Brijesh FNU,Viet Thanh Duy Nguyen,Ashima Sharma,Md Harun Rashid Molla,Chengyi Xu,Truong-Son Hy*

Main category: cs.LG

TL;DR: 本文构建丙烯酸酯基介电弹性体数据集，提出多模态学习框架预测介电和机械性能，克服数据稀缺问题并可推广至其他聚合物骨架，代码和数据集公开。


<details>
  <summary>Details</summary>
Motivation: 现代电子设备对高性能介电弹性体需求大，但开发兼具高介电常数和低杨氏模量的弹性体是挑战，且缺乏系统的介电弹性体数据集。

Method: 筛选和整合过去10年文献实验结果，构建数据集；提出多模态学习框架，利用基于图和序列的编码器的预训练聚合物表示。

Result: 实现从分子序列准确少样本预测介电和机械性能，代表了克服数据稀缺的新知识转移范式。

Conclusion: 该方法可推广到其他聚合物骨架，加速软高k介电弹性体的数据高效发现。

Abstract: Dielectric materials are critical building blocks for modern electronics such as sensors, actuators, and transistors. With the rapid recent advance in soft and stretchable electronics for emerging human- and robot-interfacing applications, there is a surging need for high-performance dielectric elastomers. However, it remains a grand challenge to develop soft elastomers that simultaneously possess high dielectric constants (k, related to energy storage capacity) and low Young's moduli (E, related to mechanical flexibility). While some new elastomer designs have been reported in individual (mostly one-off) studies, almost no structured dataset is currently available for dielectric elastomers that systematically encompasses their molecular sequence, dielectric, and mechanical properties. Within this context, we curate a compact, high-quality dataset of acrylate-based dielectric elastomers, one of the most widely explored elastomer backbones due to its versatile chemistry and molecular design flexibility, by screening and aggregating experimental results from the literature over the past 10 years. Building on this dataset, we propose a multimodal learning framework that leverages large-scale pretrained polymer representations from graph- and sequence-based encoders. These pretrained embeddings transfer rich chemical and structural knowledge from vast polymer corpora, enabling accurate few-shot prediction of both dielectric and mechanical properties from molecular sequences. Our results represent a new paradigm for transferring knowledge from pretrained multimodal models to overcome severe data scarcity, which can be readily translated to other polymer backbones (e.g., silicones, urethanes) and thus accelerate data-efficient discovery of soft high-k dielectric elastomers. Our source code and dataset are publicly available at https://github.com/HySonLab/Polymers

</details>


### [234] [Comparison requires valid measurement: Rethinking attack success rate comparisons in AI red teaming](https://arxiv.org/abs/2601.18076)
*Alexandra Chouldechova,A. Feder Cooper,Solon Barocas,Abhinav Palia,Dan Vann,Hanna Wallach*

Main category: cs.LG

TL;DR: 论文指出通过AI红队攻击成功率（ASR）比较得出的系统安全或攻击方法有效性结论常缺乏证据支持，探讨了ASR有意义比较的条件。


<details>
  <summary>Details</summary>
Motivation: 解决通过AI红队攻击成功率比较得出的结论缺乏证据支持的问题。

Method: 结合社会科学测量理论和推断统计学，分析攻击成功率有意义比较的条件，以越狱为例进行讨论。

Result: 明确了攻击成功率可以和不可以进行有意义比较的条件。

Conclusion: 很多基于攻击成功率比较的结论是基于不恰当的比较或低有效性的测量。

Abstract: We argue that conclusions drawn about relative system safety or attack method efficacy via AI red teaming are often not supported by evidence provided by attack success rate (ASR) comparisons. We show, through conceptual, theoretical, and empirical contributions, that many conclusions are founded on apples-to-oranges comparisons or low-validity measurements. Our arguments are grounded in asking a simple question: When can attack success rates be meaningfully compared? To answer this question, we draw on ideas from social science measurement theory and inferential statistics, which, taken together, provide a conceptual grounding for understanding when numerical values obtained through the quantification of system attributes can be meaningfully compared. Through this lens, we articulate conditions under which ASRs can and cannot be meaningfully compared. Using jailbreaking as a running example, we provide examples and extensive discussion of apples-to-oranges ASR comparisons and measurement validity challenges.

</details>


### [235] [DRPG (Decompose, Retrieve, Plan, Generate): An Agentic Framework for Academic Rebuttal](https://arxiv.org/abs/2601.18081)
*Peixuan Han,Yingjie Yu,Jingjun Xu,Jiaxuan You*

Main category: cs.LG

TL;DR: 本文提出DRPG框架用于自动学术反驳生成，该框架四步运行，实验表明其优于现有方法，能超越人类平均水平，还可用于多轮场景。


<details>
  <summary>Details</summary>
Motivation: 现有学术反驳自动化支持不足，现有的基于大语言模型的方法在长文本理解和生成针对性、有说服力回复方面存在缺陷。

Method: 提出DRPG框架，包含分解评审意见、检索相关证据、规划反驳策略、生成回复四个步骤。

Result: 规划器识别反驳方向准确率超98%；实验表明DRPG显著优于现有反驳管道，仅用8B模型就超越人类平均水平；规划器设计有效，能提供多视角可解释建议；在多轮场景中表现良好。

Conclusion: DRPG有效，有潜力提供高质量反驳内容，支持学术讨论的扩展。

Abstract: Despite the growing adoption of large language models (LLMs) in scientific research workflows, automated support for academic rebuttal, a crucial step in academic communication and peer review, remains largely underexplored. Existing approaches typically rely on off-the-shelf LLMs or simple pipelines, which struggle with long-context understanding and often fail to produce targeted and persuasive responses. In this paper, we propose DRPG, an agentic framework for automatic academic rebuttal generation that operates through four steps: Decompose reviews into atomic concerns, Retrieve relevant evidence from the paper, Plan rebuttal strategies, and Generate responses accordingly. Notably, the Planner in DRPG reaches over 98% accuracy in identifying the most feasible rebuttal direction. Experiments on data from top-tier conferences demonstrate that DRPG significantly outperforms existing rebuttal pipelines and achieves performance beyond the average human level using only an 8B model. Our analysis further demonstrates the effectiveness of the planner design and its value in providing multi-perspective and explainable suggestions. We also showed that DRPG works well in a more complex multi-round setting. These results highlight the effectiveness of DRPG and its potential to provide high-quality rebuttal content and support the scaling of academic discussions. Codes for this work are available at https://github.com/ulab-uiuc/DRPG-RebuttalAgent.

</details>


### [236] [LatentMoE: Toward Optimal Accuracy per FLOP and Parameter in Mixture of Experts](https://arxiv.org/abs/2601.18089)
*Venmugil Elango,Nidhi Bhatia,Roger Waleffe,Rasoul Shafipour,Tomer Asida,Abhinav Khattar,Nave Assaf,Maximilian Golub,Joey Guman,Tiyasa Mitra,Ritchie Zhao,Ritika Borkar,Ran Zilberstein,Mostofa Patwary,Mohammad Shoeybi,Bita Rouhani*

Main category: cs.LG

TL;DR: 本文从硬件 - 软件协同设计角度重新审视混合专家（MoE）架构，提出LatentMoE，其在每FLOP和每参数的准确率上优于标准MoE架构，并被采用扩展。


<details>
  <summary>Details</summary>
Motivation: 现有MoE架构在推理成本（每浮点运算和每参数的准确率）方面是否最优尚不明确，需重新审视设计。

Method: 从硬件 - 软件协同设计角度，结合实证和理论考量，刻画不同部署场景的性能瓶颈，进行系统设计探索。

Result: 在参数规模达95B和1T token训练范围的实证设计空间探索及理论分析表明，LatentMoE在每FLOP和每参数的准确率上持续优于标准MoE架构。

Conclusion: LatentMoE架构性能出色，已被旗舰模型采用并扩展到更大规模。

Abstract: Mixture of Experts (MoEs) have become a central component of many state-of-the-art open-source and proprietary large language models. Despite their widespread adoption, it remains unclear how close existing MoE architectures are to optimal with respect to inference cost, as measured by accuracy per floating-point operation and per parameter. In this work, we revisit MoE design from a hardware-software co-design perspective, grounded in empirical and theoretical considerations. We characterize key performance bottlenecks across diverse deployment regimes, spanning offline high-throughput execution and online, latency-critical inference. Guided by these insights, we introduce LatentMoE, a new model architecture resulting from systematic design exploration and optimized for maximal accuracy per unit of compute. Empirical design space exploration at scales of up to 95B parameters and over a 1T-token training horizon, together with supporting theoretical analysis, shows that LatentMoE consistently outperforms standard MoE architectures in terms of accuracy per FLOP and per parameter. Given its strong performance, the LatentMoE architecture has been adopted by the flagship Nemotron-3 Super and Ultra models and scaled to substantially larger regimes, including longer token horizons and larger model sizes, as reported in Nvidia et al. (arXiv:2512.20856).

</details>


### [237] [From LLMs to LRMs: Rethinking Pruning for Reasoning-Centric Models](https://arxiv.org/abs/2601.18091)
*Longwei Ding,Anhao Zhao,Fanghua Ye,Ziyang Chen,Xiaoyu Shen*

Main category: cs.LG

TL;DR: 本文对指令跟随和推理增强的大语言模型进行剪枝的对照研究，评估多种剪枝方法在不同任务上的效果，揭示范式依赖差异，强调需考虑推理增强模型特征的剪枝策略。


<details>
  <summary>Details</summary>
Motivation: 大语言模型部署成本高促使模型剪枝研究，但现有研究多针对指令跟随模型，不清楚剪枝策略是否适用于推理增强模型。

Method: 对指令跟随和推理增强模型进行剪枝对照研究，使剪枝校准和剪枝后恢复数据与原训练分布对齐，评估静态深度、宽度和动态剪枝在17个跨越分类、生成和推理的任务上的效果。

Result: 深度剪枝在分类任务上优于宽度剪枝，宽度剪枝在生成和推理任务上更稳健；静态剪枝更能保留推理性能，动态剪枝在分类和生成任务上表现好，但在长链推理上有挑战。

Conclusion: 需要考虑推理增强大语言模型独特特征的剪枝策略。

Abstract: Large language models (LLMs) are increasingly costly to deploy, motivating extensive research on model pruning. However, most existing studies focus on instruction-following LLMs, leaving it unclear whether established pruning strategies transfer to reasoning-augmented models that explicitly generate long intermediate reasoning traces. In this work, we conduct a controlled study of pruning for both instruction-following ($\textbf{LLM-instruct}$) and reasoning-augmented ($\textbf{LLM-think}$) models. To isolate the effects of pruning, we align pruning calibration and post-pruning recovery data with each model's original training distribution, which we show yields more stable and reliable pruning behavior. We evaluate static depth pruning, static width pruning, and dynamic pruning across 17 tasks spanning classification, generation, and reasoning. Our results reveal clear paradigm-dependent differences: depth pruning outperforms width pruning on classification tasks, while width pruning is more robust for generation and reasoning. Moreover, static pruning better preserves reasoning performance, whereas dynamic pruning excels on classification and generation but remains challenging for long-chain reasoning. These findings underscore the need for pruning strategies that explicitly account for the distinct characteristics of reasoning-augmented LLMs. Our code is publicly available at https://github.com/EIT-NLP/LRM-Pruning.

</details>


### [238] [Beyond Static Datasets: Robust Offline Policy Optimization via Vetted Synthetic Transitions](https://arxiv.org/abs/2601.18107)
*Pedram Agand,Mo Chen*

Main category: cs.LG

TL;DR: 提出基于模型的框架MoReBRAC，用不确定性感知潜在合成解决离线强化学习分布偏移问题，在基准测试中取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在安全关键领域有应用前景，但静态数据集和学习策略间的分布偏移问题限制了策略改进，需解决此问题。

Method: 提出MoReBRAC框架，利用双循环世界模型合成高保真转换扩充训练流形，通过集成变分自编码器流形检测、模型敏感性分析和蒙特卡罗丢弃的分层不确定性管道确保合成数据可靠性。

Result: 在D4RL Gym - MuJoCo基准测试中取得显著性能提升，尤其是在随机和次优数据制度下。

Conclusion: MoReBRAC框架能有效解决离线强化学习的分布偏移问题，同时还探讨了变分自编码器的作用和从接近最优数据集学习时的分布权衡。

Abstract: Offline Reinforcement Learning (ORL) holds immense promise for safety-critical domains like industrial robotics, where real-time environmental interaction is often prohibitive. A primary obstacle in ORL remains the distributional shift between the static dataset and the learned policy, which typically mandates high degrees of conservatism that can restrain potential policy improvements. We present MoReBRAC, a model-based framework that addresses this limitation through Uncertainty-Aware latent synthesis. Instead of relying solely on the fixed data, MoReBRAC utilizes a dual-recurrent world model to synthesize high-fidelity transitions that augment the training manifold. To ensure the reliability of this synthetic data, we implement a hierarchical uncertainty pipeline integrating Variational Autoencoder (VAE) manifold detection, model sensitivity analysis, and Monte Carlo (MC) dropout. This multi-layered filtering process guarantees that only transitions residing within high-confidence regions of the learned dynamics are utilized. Our results on D4RL Gym-MuJoCo benchmarks reveal significant performance gains, particularly in ``random'' and ``suboptimal'' data regimes. We further provide insights into the role of the VAE as a geometric anchor and discuss the distributional trade-offs encountered when learning from near-optimal datasets.

</details>


### [239] [AttenMIA: LLM Membership Inference Attack through Attention Signals](https://arxiv.org/abs/2601.18110)
*Pedram Zaree,Md Abdullah Al Mamun,Yue Dong,Ihsen Alouani,Nael Abu-Ghazaleh*

Main category: cs.LG

TL;DR: 介绍新的成员推理攻击框架AttenMIA，利用自注意力模式推断成员关系，实验表明其优于基线方法，揭示注意力机制会放大隐私风险。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型成员推理攻击依赖的信号易变，攻击成功率有限，需新方法。

Method: 利用变压器模型内自注意力模式，结合各层注意力头信息与基于扰动的差异指标训练分类器。

Result: 注意力特征表现优于基线，信号跨数据集和架构可泛化，替换其他攻击能提升数据提取攻击效果。

Conclusion: 注意力机制会无意中放大LLMs隐私风险，需新防御措施。

Abstract: Large Language Models (LLMs) are increasingly deployed to enable or improve a multitude of real-world applications. Given the large size of their training data sets, their tendency to memorize training data raises serious privacy and intellectual property concerns. A key threat is the membership inference attack (MIA), which aims to determine whether a given sample was included in the model's training set. Existing MIAs for LLMs rely primarily on output confidence scores or embedding-based features, but these signals are often brittle, leading to limited attack success. We introduce AttenMIA, a new MIA framework that exploits self-attention patterns inside the transformer model to infer membership. Attention controls the information flow within the transformer, exposing different patterns for memorization that can be used to identify members of the dataset. Our method uses information from attention heads across layers and combines them with perturbation-based divergence metrics to train an effective MIA classifier. Using extensive experiments on open-source models including LLaMA-2, Pythia, and Opt models, we show that attention-based features consistently outperform baselines, particularly under the important low-false-positive metric (e.g., achieving up to 0.996 ROC AUC & 87.9% TPR@1%FPR on the WikiMIA-32 benchmark with Llama2-13b). We show that attention signals generalize across datasets and architectures, and provide a layer- and head-level analysis of where membership leakage is most pronounced. We also show that using AttenMIA to replace other membership inference attacks in a data extraction framework results in training data extraction attacks that outperform the state of the art. Our findings reveal that attention mechanisms, originally introduced to enhance interpretability, can inadvertently amplify privacy risks in LLMs, underscoring the need for new defenses.

</details>


### [240] [Demystifying Data-Driven Probabilistic Medium-Range Weather Forecasting](https://arxiv.org/abs/2601.18111)
*Jean Kossaifi,Nikola Kovachki,Morteza Mardani,Daniel Leibovici,Suman Ravuri,Ira Shokar,Edoardo Calvello,Mohammad Shoaib Abbas,Peter Harrington,Ashay Subramaniam,Noah Brenowitz,Boris Bonev,Wonmin Byeon,Karsten Kreis,Dale Durran,Arash Vahdat,Mike Pritchard,Jan Kautz*

Main category: cs.LG

TL;DR: 数据驱动的天气预报方法碎片化，本文提出可扩展框架，能在多种概率框架下提升预测效果，表明通用模型扩展足以实现先进的中期预测。


<details>
  <summary>Details</summary>
Motivation: 当前数据驱动的天气预报方法架构和训练策略复杂且碎片化，掩盖了预测准确性的基本驱动因素。

Method: 结合直接下采样的潜在空间和历史条件局部投影器，构建可扩展框架学习多尺度大气动力学。

Result: 框架对概率估计器选择具有鲁棒性，支持多种模型；与IFS和GenCast对比，多数变量有显著改进。

Conclusion: 扩展通用模型足以实现先进的中期预测，无需定制训练方案，且在各种概率框架下均有效。

Abstract: The recent revolution in data-driven methods for weather forecasting has lead to a fragmented landscape of complex, bespoke architectures and training strategies, obscuring the fundamental drivers of forecast accuracy. Here, we demonstrate that state-of-the-art probabilistic skill requires neither intricate architectural constraints nor specialized training heuristics. We introduce a scalable framework for learning multi-scale atmospheric dynamics by combining a directly downsampled latent space with a history-conditioned local projector that resolves high-resolution physics. We find that our framework design is robust to the choice of probabilistic estimator, seamlessly supporting stochastic interpolants, diffusion models, and CRPS-based ensemble training. Validated against the Integrated Forecasting System and the deep learning probabilistic model GenCast, our framework achieves statistically significant improvements on most of the variables. These results suggest scaling a general-purpose model is sufficient for state-of-the-art medium-range prediction, eliminating the need for tailored training recipes and proving effective across the full spectrum of probabilistic frameworks.

</details>


### [241] [Enhance the Safety in Reinforcement Learning by ADRC Lagrangian Methods](https://arxiv.org/abs/2601.18142)
*Mingxu Zhang,Huicheng Zhang,Jiaming Ji,Yaodong Yang,Ying Sun*

Main category: cs.LG

TL;DR: 提出ADRC - Lagrangian方法解决安全强化学习现有方法问题，实验表明该方法效果好


<details>
  <summary>Details</summary>
Motivation: 现有基于拉格朗日的安全强化学习方法存在振荡和频繁安全违规问题，因其参数敏感和固有相位滞后

Method: 提出ADRC - Lagrangian方法，利用主动抗扰控制提高鲁棒性和减少振荡，统一框架涵盖经典和PID拉格朗日方法

Result: 实验显示该方法使安全违规减少74%，约束违规幅度降低89%，平均成本降低67%

Conclusion: 该方法在复杂环境的安全强化学习中具有卓越有效性

Abstract: Safe reinforcement learning (Safe RL) seeks to maximize rewards while satisfying safety constraints, typically addressed through Lagrangian-based methods. However, existing approaches, including PID and classical Lagrangian methods, suffer from oscillations and frequent safety violations due to parameter sensitivity and inherent phase lag. To address these limitations, we propose ADRC-Lagrangian methods that leverage Active Disturbance Rejection Control (ADRC) for enhanced robustness and reduced oscillations. Our unified framework encompasses classical and PID Lagrangian methods as special cases while significantly improving safety performance. Extensive experiments demonstrate that our approach reduces safety violations by up to 74%, constraint violation magnitudes by 89%, and average costs by 67\%, establishing superior effectiveness for Safe RL in complex environments.

</details>


### [242] [FP8-RL: A Practical and Stable Low-Precision Stack for LLM Reinforcement Learning](https://arxiv.org/abs/2601.18150)
*Zhaopeng Qiu,Shuang Yu,Jingqi Zhang,Shuai Zhang,Xue Huang,Jingyi Yang,Junjie Lai*

Main category: cs.LG

TL;DR: 本文提出用于大语言模型强化学习的FP8推理栈，实现多项技术提升推理吞吐量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型强化学习在推理阶段受长序列输出影响，FP8可加速但应用存在工程和算法挑战。

Method: 在veRL生态中实现FP8推理栈，采用块级FP8量化实现FP8 W8A8线性层推理，通过每步QKV缩放重新校准将FP8扩展到KV缓存，使用基于重要性采样的推理校正缓解不匹配问题。

Result: 在密集和MoE模型中，这些技术使推理吞吐量最高提升44%，学习行为与BF16基线相当。

Conclusion: 所提出的FP8推理栈有效解决了FP8在大语言模型强化学习中应用的问题，能提升推理吞吐量。

Abstract: Reinforcement learning (RL) for large language models (LLMs) is increasingly bottlenecked by rollout (generation), where long output sequence lengths make attention and KV-cache memory dominate end-to-end step time. FP8 offers an attractive lever for accelerating RL by reducing compute cost and memory traffic during rollout, but applying FP8 in RL introduces unique engineering and algorithmic challenges: policy weights change every step (requiring repeated quantization and weight synchronization into the inference engine) and low-precision rollouts can deviate from the higher-precision policy assumed by the trainer, causing train-inference mismatch and potential instability. This report presents a practical FP8 rollout stack for LLM RL, implemented in the veRL ecosystem with support for common training backends (e.g., FSDP/Megatron-LM) and inference engines (e.g., vLLM/SGLang). We (i) enable FP8 W8A8 linear-layer rollout using blockwise FP8 quantization, (ii) extend FP8 to KV-cache to remove long-context memory bottlenecks via per-step QKV scale recalibration, and (iii) mitigate mismatch using importance-sampling-based rollout correction (token-level TIS/MIS variants). Across dense and MoE models, these techniques deliver up to 44% rollout throughput gains while preserving learning behavior comparable to BF16 baselines.

</details>


### [243] [Learning Fair Domain Adaptation with Virtual Label Distribution](https://arxiv.org/abs/2601.18171)
*Yuguang Zhang,Lijun Sheng,Jian Liang,Ran He*

Main category: cs.LG

TL;DR: 提出VILL框架解决无监督域适应中的类别公平性问题，实验表明可提升类别公平性。


<details>
  <summary>Details</summary>
Motivation: 现有无监督域适应方法提升整体准确率时忽略类别间性能差异，即类别公平性问题。

Method: 提出VILL框架，采用自适应重加权策略放大难分类类别的影响，引入基于KL散度的重平衡策略调整决策边界。

Result: VILL可作为即插即用模块集成到现有UDA方法中。

Conclusion: VILL能显著提高类别公平性。

Abstract: Unsupervised Domain Adaptation (UDA) aims to mitigate performance degradation when training and testing data are sampled from different distributions. While significant progress has been made in enhancing overall accuracy, most existing methods overlook performance disparities across categories-an issue we refer to as category fairness. Our empirical analysis reveals that UDA classifiers tend to favor certain easy categories while neglecting difficult ones. To address this, we propose Virtual Label-distribution-aware Learning (VILL), a simple yet effective framework designed to improve worst-case performance while preserving high overall accuracy. The core of VILL is an adaptive re-weighting strategy that amplifies the influence of hard-to-classify categories. Furthermore, we introduce a KL-divergence-based re-balancing strategy, which explicitly adjusts decision boundaries to enhance category fairness. Experiments on commonly used datasets demonstrate that VILL can be seamlessly integrated as a plug-and-play module into existing UDA methods, significantly improving category fairness.

</details>


### [244] [Smooth, Sparse, and Stable: Finite-Time Exact Skeleton Recovery via Smoothed Proximal Gradients](https://arxiv.org/abs/2601.18189)
*Rui Wu,Yongjun Li*

Main category: cs.LG

TL;DR: 本文提出AHOC和SPG - AHOC方法解决连续优化与离散图结构的差距问题，有理论保证且实验效果好。


<details>
  <summary>Details</summary>
Motivation: 现有连续优化因果发现方法只能渐近收敛到驻点，产生的稠密加权矩阵需事后阈值处理，连续优化和离散图结构存在差距。

Method: 提出Hybrid - Order Acyclicity Constraint (AHOC)，通过Smoothed Proximal Gradient (SPG - AHOC)优化，利用近端算法的流形识别特性。

Result: 证明了Finite - Time Oracle Property，在有限迭代内恢复精确DAG支持，算法无需启发式截断返回精确零元素图，实验达最优精度。

Conclusion: SPG - AHOC方法能有效解决连续优化和离散图结构的差距问题，理论和实验结果都很出色。

Abstract: Continuous optimization has significantly advanced causal discovery, yet existing methods (e.g., NOTEARS) generally guarantee only asymptotic convergence to a stationary point. This often yields dense weighted matrices that require arbitrary post-hoc thresholding to recover a DAG. This gap between continuous optimization and discrete graph structures remains a fundamental challenge. In this paper, we bridge this gap by proposing the Hybrid-Order Acyclicity Constraint (AHOC) and optimizing it via the Smoothed Proximal Gradient (SPG-AHOC). Leveraging the Manifold Identification Property of proximal algorithms, we provide a rigorous theoretical guarantee: the Finite-Time Oracle Property. We prove that under standard identifiability assumptions, SPG-AHOC recovers the exact DAG support (structure) in finite iterations, even when optimizing a smoothed approximation. This result eliminates structural ambiguity, as our algorithm returns graphs with exact zero entries without heuristic truncation. Empirically, SPG-AHOC achieves state-of-the-art accuracy and strongly corroborates the finite-time identification theory.

</details>


### [245] [HeterCSI: Channel-Adaptive Heterogeneous CSI Pretraining Framework for Generalized Wireless Foundation Models](https://arxiv.org/abs/2601.18200)
*Chenyu Zhang,Xinchen Lyu,Chenshan Ren,Shuhan Liu,Qimei Cui,Xiaofeng Tao*

Main category: cs.LG

TL;DR: 本文提出HeterCSI框架，解决无线基础模型在CSI处理中的双异构问题，实验表明其性能优越且减少训练延迟。


<details>
  <summary>Details</summary>
Motivation: 现有预训练方法限制输入维度或按规模孤立训练，导致无线基础模型泛化性和可扩展性受限。

Method: 提出HeterCSI框架，将异构CSI批量构建作为分区优化问题，采用规模感知自适应批处理策略和双掩码机制。

Result: 在12个数据集上实验，无需特定场景微调建立通用基础模型，相比基线性能优越，降低NMSE，减少训练延迟。

Conclusion: HeterCSI框架在训练效率和跨场景泛化性上表现出色。

Abstract: Wireless foundation models promise transformative capabilities for channel state information (CSI) processing across diverse 6G network applications, yet face fundamental challenges due to the inherent dual heterogeneity of CSI across both scale and scenario dimensions. However, current pretraining approaches either constrain inputs to fixed dimensions or isolate training by scale, limiting the generalization and scalability of wireless foundation models. In this paper, we propose HeterCSI, a channel-adaptive pretraining framework that reconciles training efficiency with robust cross-scenario generalization via a new understanding of gradient dynamics in heterogeneous CSI pretraining. Our key insight reveals that CSI scale heterogeneity primarily causes destructive gradient interference, while scenario diversity actually promotes constructive gradient alignment when properly managed. Specifically, we formulate heterogeneous CSI batch construction as a partitioning optimization problem that minimizes zero-padding overhead while preserving scenario diversity. To solve this, we develop a scale-aware adaptive batching strategy that aligns CSI samples of similar scales, and design a double-masking mechanism to isolate valid signals from padding artifacts. Extensive experiments on 12 datasets demonstrate that HeterCSI establishes a generalized foundation model without scenario-specific finetuning, achieving superior average performance over full-shot baselines. Compared to the state-of-the-art zero-shot benchmark WiFo, it reduces NMSE by 7.19 dB, 4.08 dB, and 5.27 dB for CSI reconstruction, time-domain, and frequency-domain prediction, respectively. The proposed HeterCSI framework also reduces training latency by 53% compared to existing approaches while improving generalization performance by 1.53 dB on average.

</details>


### [246] [Rethinking Cross-Modal Fine-Tuning: Optimizing the Interaction between Feature Alignment and Target Fitting](https://arxiv.org/abs/2601.18231)
*Trong Khiem Tran,Manh Cuong Dao,Phi Le Nguyen,Thao Nguyen Truong,Trong Nghia Hoang*

Main category: cs.LG

TL;DR: 本文提出框架改进预训练模型适配新特征模态的性能，在多数据集上超现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有工作缺乏对特征对齐和目标拟合关键相互作用的理论理解，需解决新模态与预训练模型表示空间对齐及知识迁移问题。

Method: 开发一个原则性框架，通过特征 - 标签失真概念建立目标误差可证明的泛化边界。

Result: 在多个基准数据集上性能显著优于现有方法。

Conclusion: 新框架解释了特征对齐和目标拟合的相互作用，为实际算法设计优化提供见解。

Abstract: Adapting pre-trained models to unseen feature modalities has become increasingly important due to the growing need for cross-disciplinary knowledge integration.~A key challenge here is how to align the representation of new modalities with the most relevant parts of the pre-trained model's representation space to enable accurate knowledge transfer.~This requires combining feature alignment with target fine-tuning, but uncalibrated combinations can exacerbate misalignment between the source and target feature-label structures and reduce target generalization.~Existing work however lacks a theoretical understanding of this critical interaction between feature alignment and target fitting.~To bridge this gap, we develop a principled framework that establishes a provable generalization bound on the target error, which explains the interaction between feature alignment and target fitting through a novel concept of feature-label distortion.~This bound offers actionable insights into how this interaction should be optimized for practical algorithm design. The resulting approach achieves significantly improved performance over state-of-the-art methods across a wide range of benchmark datasets.

</details>


### [247] [Tractable Gaussian Phase Retrieval with Heavy Tails and Adversarial Corruption with Near-Linear Sample Complexity](https://arxiv.org/abs/2601.18245)
*Santanu Das,Jatin Batra*

Main category: cs.LG

TL;DR: 本文研究带重尾噪声和对抗性损坏的鲁棒相位恢复问题，借助鲁棒谱初始化与鲁棒PCA的联系，给出首个多项式时间算法。


<details>
  <summary>Details</summary>
Motivation: 相位恢复算法需考虑对测量误差的鲁棒性，此前已有针对其他参数估计任务的高效鲁棒算法，而鲁棒相位恢复问题缺乏高效算法。

Method: 建立鲁棒谱初始化与鲁棒PCA算法进展的联系。

Result: 得到首个针对重尾噪声和对抗性损坏的鲁棒相位恢复多项式时间算法，样本复杂度接近线性。

Conclusion: 通过利用鲁棒PCA的进展，解决了鲁棒相位恢复问题中缺乏高效算法的难题。

Abstract: Phase retrieval is the classical problem of recovering a signal $x^* \in \mathbb{R}^n$ from its noisy phaseless measurements $y_i = \langle a_i, x^* \rangle^2 + ζ_i$ (where $ζ_i$ denotes noise, and $a_i$ is the sensing vector) for $i \in [m]$. The problem of phase retrieval has a rich history, with a variety of applications such as optics, crystallography, heteroscedastic regression, astrophysics, etc. A major consideration in algorithms for phase retrieval is robustness against measurement errors. In recent breakthroughs in algorithmic robust statistics, efficient algorithms have been developed for several parameter estimation tasks such as mean estimation, covariance estimation, robust principal component analysis (PCA), etc. in the presence of heavy-tailed noise and adversarial corruptions. In this paper, we study efficient algorithms for robust phase retrieval with heavy-tailed noise when a constant fraction of both the measurements $y_i$ and the sensing vectors $a_i$ may be arbitrarily adversarially corrupted. For this problem, Buna and Rebeschini (AISTATS 2025) very recently gave an exponential time algorithm with sample complexity $O(n \log n)$. Their algorithm needs a robust spectral initialization, specifically, a robust estimate of the top eigenvector of a covariance matrix, which they deemed to be beyond known efficient algorithmic techniques (similar spectral initializations are a key ingredient of a large family of phase retrieval algorithms). In this work, we make a connection between robust spectral initialization and recent algorithmic advances in robust PCA, yielding the first polynomial-time algorithms for robust phase retrieval with both heavy-tailed noise and adversarial corruptions, in fact with near-linear (in $n$) sample complexity.

</details>


### [248] [Beyond Retention: Orchestrating Structural Safety and Plasticity in Continual Learning for LLMs](https://arxiv.org/abs/2601.18255)
*Fei Meng*

Main category: cs.LG

TL;DR: 研究大语言模型持续学习中经验回放（ER）存在的问题，提出正交子空间唤醒（OSW）方法，实验证明其有效性并强调评估结构安全性的必要性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型持续学习需平衡稳定性和可塑性，经验回放对不同能力的影响未充分研究。

Method: 提出正交子空间唤醒（OSW）方法，通过唤醒阶段识别先前任务的关键参数子空间，并对新任务进行正交更新。

Result: 在四个不同任务序列的实验中，OSW能保留脆弱的编码能力，同时对新任务保持高可塑性。

Conclusion: 大语言模型持续学习中，除平均保留率外，评估结构安全性很有必要。

Abstract: Continual learning in Large Language Models (LLMs) faces the critical challenge of balancing stability (retaining old knowledge) and plasticity (learning new tasks). While Experience Replay (ER) is a standard countermeasure against catastrophic forgetting, its impact across diverse capabilities remains underexplored. In this work, we uncover a critical dichotomy in ER's behavior: while it induces positive backward transfer on robust, unstructured tasks (e.g., boosting performance on previous NLP classification tasks through repeated rehearsal), it causes severe negative transfer on fragile, structured domains like code generation (e.g., a significant relative drop in coding accuracy). This reveals that ER trades structural integrity for broad consolidation. To address this dilemma, we propose \textbf{Orthogonal Subspace Wake-up (OSW)}. OSW identifies essential parameter subspaces of previous tasks via a brief "wake-up" phase and enforces orthogonal updates for new tasks, providing a mathematically grounded "safety guarantee" for established knowledge structures. Empirical results across a diverse four-task sequence demonstrate that OSW uniquely succeeds in preserving fragile coding abilities where Replay fails, while simultaneously maintaining high plasticity for novel tasks. Our findings emphasize the necessity of evaluating structural safety alongside average retention in LLM continual learning.

</details>


### [249] [FGGM: Fisher-Guided Gradient Masking for Continual Learning](https://arxiv.org/abs/2601.18261)
*Chao-Hong Tan,Qian Chen,Wen Wang,Yukun Ma,Chong Zhang,Chong Deng,Qinglin Zhang,Xiangang Li,Jieping Ye*

Main category: cs.LG

TL;DR: 提出FGGM框架缓解大语言模型灾难性遗忘问题，在TRACE基准和代码生成任务表现良好


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型连续学习中灾难性遗忘的问题

Method: 提出Fisher - Guided Gradient Masking (FGGM)框架，用对角Fisher信息策略性选择更新参数，动态生成带自适应阈值的二进制掩码

Result: 在TRACE基准上，相比监督微调(SFT)保留通用能力相对提升9.6%，比MIGU提升4.4%；代码生成任务也有优越表现和更少遗忘

Conclusion: FGGM是缓解大语言模型灾难性遗忘的有效解决方案

Abstract: Catastrophic forgetting impairs the continuous learning of large language models. We propose Fisher-Guided Gradient Masking (FGGM), a framework that mitigates this by strategically selecting parameters for updates using diagonal Fisher Information. FGGM dynamically generates binary masks with adaptive thresholds, preserving critical parameters to balance stability and plasticity without requiring historical data. Unlike magnitude-based methods such as MIGU, our approach offers a mathematically principled parameter importance estimation. On the TRACE benchmark, FGGM shows a 9.6% relative improvement in retaining general capabilities over supervised fine-tuning (SFT) and a 4.4% improvement over MIGU on TRACE tasks. Additional analysis on code generation tasks confirms FGGM's superior performance and reduced forgetting, establishing it as an effective solution.

</details>


### [250] [Neural Network Approximation: A View from Polytope Decomposition](https://arxiv.org/abs/2601.18264)
*ZeYu Li,ShiJun Zhang,TieYong Zeng,FengLei Fan*

Main category: cs.LG

TL;DR: 本文从多面体分解角度研究ReLU网络的通用逼近能力，提出显式核多项式方法，构建ReLU网络逼近，比现有方法更高效灵活，还将方法扩展到解析函数实现更高逼近率。


<details>
  <summary>Details</summary>
Motivation: 现有通用逼近理论构建多均匀划分输入空间，未考虑目标函数局部规律性，因此从多面体分解角度研究ReLU网络逼近能力。

Method: 开发显式核多项式方法推导连续函数通用逼近，构建ReLU网络分别逼近每个子域的核多项式，再将方法扩展到解析函数。

Result: 多面体分解在很多情况下使逼近比现有方法更高效灵活，尤其在目标函数奇点附近。

Conclusion: 从多面体分解角度研究ReLU网络逼近能力是可行且有效的，扩展到解析函数可提高逼近率。

Abstract: Universal approximation theory offers a foundational framework to verify neural network expressiveness, enabling principled utilization in real-world applications. However, most existing theoretical constructions are established by uniformly dividing the input space into tiny hypercubes without considering the local regularity of the target function. In this work, we investigate the universal approximation capabilities of ReLU networks from a view of polytope decomposition, which offers a more realistic and task-oriented approach compared to current methods. To achieve this, we develop an explicit kernel polynomial method to derive an universal approximation of continuous functions, which is characterized not only by the refined Totik-Ditzian-type modulus of continuity, but also by polytopical domain decomposition. Then, a ReLU network is constructed to approximate the kernel polynomial in each subdomain separately. Furthermore, we find that polytope decomposition makes our approximation more efficient and flexible than existing methods in many cases, especially near singular points of the objective function. Lastly, we extend our approach to analytic functions to reach a higher approximation rate.

</details>


### [251] [What Do Learned Models Measure?](https://arxiv.org/abs/2601.18278)
*Indrė Žliobaitė*

Main category: cs.LG

TL;DR: 本文指出机器学习模型作测量工具时，现有评估框架有局限，需新评估维度。


<details>
  <summary>Details</summary>
Motivation: 在科学和数据驱动应用中，机器学习模型作测量工具时，现有评估标准无法保证测量稳定性。

Method: 将学习的测量函数作为评估重点，引入测量稳定性概念，并通过真实案例研究。

Result: 标准评估标准不能保证测量稳定性，预测性能相近的模型可能实现不同测量函数。

Conclusion: 现有评估框架在模型输出作为测量值的场景中有局限，需额外评估维度。

Abstract: In many scientific and data-driven applications, machine learning models are increasingly used as measurement instruments, rather than merely as predictors of predefined labels. When the measurement function is learned from data, the mapping from observations to quantities is determined implicitly by the training distribution and inductive biases, allowing multiple inequivalent mappings to satisfy standard predictive evaluation criteria. We formalize learned measurement functions as a distinct focus of evaluation and introduce measurement stability, a property capturing invariance of the measured quantity across admissible realizations of the learning process and across contexts. We show that standard evaluation criteria in machine learning, including generalization error, calibration, and robustness, do not guarantee measurement stability. Through a real-world case study, we show that models with comparable predictive performance can implement systematically inequivalent measurement functions, with distribution shift providing a concrete illustration of this failure. Taken together, our results highlight a limitation of existing evaluation frameworks in settings where learned model outputs are identified as measurements, motivating the need for an additional evaluative dimension.

</details>


### [252] [TriPlay-RL: Tri-Role Self-Play Reinforcement Learning for LLM Safety Alignment](https://arxiv.org/abs/2601.18292)
*Zhewen Tan,Wenhan Yu,Jianfeng Si,Tongxin Liu,Kaiqi Guan,Huiyan Jin,Jiawen Tao,Xiaokun Yuan,Duohe Ma,Xiangzheng Zhang,Tong Yang,Lin Sun*

Main category: cs.LG

TL;DR: 本文提出TriPlay - RL闭环强化学习框架用于大语言模型安全对齐，实验显示该框架能提升各角色性能，建立高效可扩展范式。


<details>
  <summary>Details</summary>
Motivation: 近年来大语言模型安全风险凸显，需减少有害内容生成，主流安全对齐范式有改进需求。

Method: 提出名为TriPlay - RL的闭环强化学习框架，使三个角色在近乎零人工标注下迭代协同改进。

Result: 攻击者输出多样性高，对抗有效性提升20% - 50%；防御者安全性能提升10% - 30%且不降低通用推理能力；评估者能不断细化判断能力。

Conclusion: 该框架为大语言模型安全对齐建立了高效可扩展范式，实现统一学习循环内的持续协同进化。

Abstract: In recent years, safety risks associated with large language models have become increasingly prominent, highlighting the urgent need to mitigate the generation of toxic and harmful content. The mainstream paradigm for LLM safety alignment typically adopts a collaborative framework involving three roles: an attacker for adversarial prompt generation, a defender for safety defense, and an evaluator for response assessment. In this paper, we propose a closed-loop reinforcement learning framework called TriPlay-RL that enables iterative and co-improving collaboration among three roles with near-zero manual annotation. Experimental results show that the attacker preserves high output diversity while achieving a 20%-50% improvement in adversarial effectiveness; the defender attains 10%-30% gains in safety performance without degrading general reasoning capability; and the evaluator continuously refines its fine-grained judgment ability through iterations, accurately distinguishing unsafe responses, simple refusals, and useful guidance. Overall, our framework establishes an efficient and scalable paradigm for LLM safety alignment, enabling continuous co-evolution within a unified learning loop.

</details>


### [253] [A Master Class on Reproducibility: A Student Hackathon on Advanced MRI Reconstruction Methods](https://arxiv.org/abs/2601.18314)
*Lina Felsner,Sevgi G. Kafali,Hannah Eichhorn,Agnes A. J. Leth,Aidas Batvinskas,Andre Datchev,Fabian Klemm,Jan Aulich,Puntika Leepagorn,Ruben Klinger,Daniel Rueckert,Julia A. Schnabel*

Main category: cs.LG

TL;DR: 介绍学生复现黑客松活动，聚焦三篇MRI重建论文结果复现，给出活动设置、复现结果及构建可复现代码库的基本做法。


<details>
  <summary>Details</summary>
Motivation: 推动MRI重建论文结果的复现，提升研究的可重复性。

Method: 举办学生复现黑客松活动，对三篇MRI重建论文结果进行复现，并开展额外实验。

Result: 给出了复现结果和额外实验情况。

Conclusion: 阐述了构建可复现代码库的基本做法。

Abstract: We report the design, protocol, and outcomes of a student reproducibility hackathon focused on replicating the results of three influential MRI reconstruction papers: (a) MoDL, an unrolled model-based network with learned denoising; (b) HUMUS-Net, a hybrid unrolled multiscale CNN+Transformer architecture; and (c) an untrained, physics-regularized dynamic MRI method that uses a quantitative MR model for early stopping. We describe the setup of the hackathon and present reproduction outcomes alongside additional experiments, and we detail fundamental practices for building reproducible codebases.

</details>


### [254] [Cognitive Fusion of ZC Sequences and Time-Frequency Images for Out-of-Distribution Detection of Drone Signals](https://arxiv.org/abs/2601.18326)
*Jie Li,Jing Li,Lu Lv,Zhanyu Ju,Fengkui Gong*

Main category: cs.LG

TL;DR: 提出基于ZC序列和TFI认知融合的无人机信号OODD算法，性能优于现有算法且具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 实现无人机远程识别任务中的OODD，解决未知或非标准通信协议无人机信号检测问题。

Method: 从射频信号生成ZC序列和TFI特征，经特征提取、多模态特征交互与融合，计算判别分数并转化为注意力权重，通过Softmax函数分类。

Result: 仿真显示该算法优于现有算法，RID和OODD指标分别提升1.7%和7.5%，在不同飞行条件和无人机类型下具强鲁棒性。

Conclusion: 所提算法有效可行，在无人机信号OODD方面性能良好且鲁棒性强。

Abstract: We propose a drone signal out-of-distribution detection (OODD) algorithm based on the cognitive fusion of Zadoff-Chu (ZC) sequences and time-frequency images (TFI). ZC sequences are identified by analyzing the communication protocols of DJI drones, while TFI capture the time-frequency characteristics of drone signals with unknown or non-standard communication protocols. Both modalities are used jointly to enable OODD in the drone remote identification (RID) task. Specifically, ZC sequence features and TFI features are generated from the received radio frequency signals, which are then processed through dedicated feature extraction module to enhance and align them. The resultant multi-modal features undergo multi-modal feature interaction, single-modal feature fusion, and multi-modal feature fusion to produce features that integrate and complement information across modalities. Discrimination scores are computed from the fused features along both spatial and channel dimensions to capture time-frequency characteristic differences dictated by the communication protocols, and these scores will be transformed into adaptive attention weights. The weighted features are then passed through a Softmax function to produce the signal classification results. Simulation results demonstrate that the proposed algorithm outperforms existing algorithms and achieves 1.7% and 7.5% improvements in RID and OODD metrics, respectively. The proposed algorithm also performs strong robustness under varying flight conditions and across different drone types.

</details>


### [255] [Discriminability-Driven Spatial-Channel Selection with Gradient Norm for Drone Signal OOD Detection](https://arxiv.org/abs/2601.18329)
*Chuhan Feng,Jing Li,Jie Li,Lu Lv,Fengkui Gong*

Main category: cs.LG

TL;DR: 提出基于梯度范数的判别驱动空间 - 通道选择的无人机信号OOD检测算法，结合梯度范数和能量分数联合推理，仿真显示性能优越。


<details>
  <summary>Details</summary>
Motivation: 解决无人机信号OOD检测问题，提高检测的判别能力和鲁棒性。

Method: 根据协议特定时频特征量化类间相似度和方差，在空间和通道维度对时频图像特征自适应加权；引入梯度范数度量扰动敏感性，与基于能量的分数融合进行联合推理。

Result: 仿真结果表明，该算法在SNR和不同无人机类型下具有优越的判别能力和稳健性能。

Conclusion: 所提出的无人机信号OOD检测算法有效，性能良好。

Abstract: We propose a drone signal out-of-distribution (OOD) detection algorithm based on discriminability-driven spatial-channel selection with a gradient norm. Time-frequency image features are adaptively weighted along both spatial and channel dimensions by quantifying inter-class similarity and variance based on protocol-specific time-frequency characteristics. Subsequently, a gradient-norm metric is introduced to measure perturbation sensitivity for capturing the inherent instability of OOD samples, which is then fused with energy-based scores for joint inference. Simulation results demonstrate that the proposed algorithm provides superior discriminative power and robust performance via SNR and various drone types.

</details>


### [256] [Structural Gender Bias in Credit Scoring: Proxy Leakage](https://arxiv.org/abs/2601.18342)
*Navya SD,Sreekanth D,SS Uma Sankari*

Main category: cs.LG

TL;DR: 研究对台湾信贷违约数据集进行审计，发现去除显式保护属性和应用公平干预后，非敏感特征仍存在性别偏见，传统公平审计不足，需转向因果感知建模和结构问责。


<details>
  <summary>Details</summary>
Motivation: 金融机构采用机器学习进行信用风险评估时，算法偏见阻碍公平金融包容，挑战“盲目公平”原则。

Method: 利用SHAP识别非敏感特征中性别代理变量，采用对抗逆建模框架量化信息泄露。

Result: 能从非敏感金融特征重构受保护的性别属性，ROC AUC得分为0.65，传统公平审计不足以检测隐性结构偏见。

Conclusion: 金融AI应从表面统计平等转向因果感知建模和结构问责。

Abstract: As financial institutions increasingly adopt machine learning for credit risk assessment, the persistence of algorithmic bias remains a critical barrier to equitable financial inclusion. This study provides a comprehensive audit of structural gender bias within the Taiwan Credit Default dataset, specifically challenging the prevailing doctrine of "fairness through blindness." Despite the removal of explicit protected attributes and the application of industry standard fairness interventions, our results demonstrate that gendered predictive signals remain deeply embedded within non-sensitive features. Utilizing SHAP (SHapley Additive exPlanations), we identify that variables such as Marital Status, Age, and Credit Limit function as potent proxies for gender, allowing models to maintain discriminatory pathways while appearing statistically fair. To mathematically quantify this leakage, we employ an adversarial inverse modeling framework. Our findings reveal that the protected gender attribute can be reconstructed from purely non-sensitive financial features with an ROC AUC score of 0.65, demonstrating that traditional fairness audits are insufficient for detecting implicit structural bias. These results advocate for a shift from surface-level statistical parity toward causal-aware modeling and structural accountability in financial AI.

</details>


### [257] [Making medical vision-language models think causally across modalities with retrieval-augmented cross-modal reasoning](https://arxiv.org/abs/2601.18356)
*Weiqin Yang,Haowen Xue,Qingyi Peng,Hexuan Hu,Qian Huang,Tingbo Zhang*

Main category: cs.LG

TL;DR: 提出多模态因果检索增强生成框架，应用于医学任务提升性能，为医学视觉语言模型发展提供新路径。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉语言模型推理机制基于相关性，有易产生幻觉、受数据集偏差影响等局限，传统RAG引入新虚假关联。

Method: 提出Multimodal Causal Retrieval - Augmented Generation框架，将因果推理原则与多模态检索结合，依据反事实和干预证据推理。

Result: 应用于放射报告生成、诊断预测和视觉问答等任务，提高了事实准确性、对分布变化的鲁棒性和可解释性。

Conclusion: 因果检索是医学视觉语言模型超越模式匹配的可扩展路径，可在高风险临床环境中实现可靠多模态推理。

Abstract: Medical vision-language models (VLMs) achieve strong performance in diagnostic reporting and image-text alignment, yet their underlying reasoning mechanisms remain fundamentally correlational, exhibiting reliance on superficial statistical associations that fail to capture the causal pathophysiological mechanisms central to clinical decision-making. This limitation makes them fragile, prone to hallucinations, and sensitive to dataset biases. Retrieval-augmented generation (RAG) offers a partial remedy by grounding predictions in external knowledge. However, conventional RAG depends on semantic similarity, introducing new spurious correlations. We propose Multimodal Causal Retrieval-Augmented Generation, a framework that integrates causal inference principles with multimodal retrieval. It retrieves clinically relevant exemplars and causal graphs from external sources, conditioning model reasoning on counterfactual and interventional evidence rather than correlations alone. Applied to radiology report generation, diagnosis prediction, and visual question answering, it improves factual accuracy, robustness to distribution shifts, and interpretability. Our results highlight causal retrieval as a scalable path toward medical VLMs that think beyond pattern matching, enabling trustworthy multimodal reasoning in high-stakes clinical settings.

</details>


### [258] [Estimating Dense-Packed Zone Height in Liquid-Liquid Separation: A Physics-Informed Neural Network Approach](https://arxiv.org/abs/2601.18399)
*Mehmet Velioglu,Song Zhai,Alexander Mitsos,Adel Mhamdi,Andreas Jupke,Manuel Dahmen*

Main category: cs.LG

TL;DR: 论文提出用廉价的体积流量测量估计相高，采用物理信息神经网络（PINN）结合扩展卡尔曼滤波器实现，两阶段训练的PINN估计相高精度最高。


<details>
  <summary>Details</summary>
Motivation: 在重力沉降器中测量液 - 液分散相的密相区高度因光学限制而昂贵且不实际，需要廉价方法估计相高。

Method: 先用低精度机械模型生成的合成数据和物理方程预训练PINN，再用少量实验数据微调；将可微的PINN用于扩展卡尔曼滤波器状态估计框架。

Result: 通过与机械模型、未预训练的PINN和纯数据驱动的神经网络对比，两阶段训练的PINN估计相高最准确。

Conclusion: 两阶段训练的PINN可利用廉价体积流量测量准确估计相高。

Abstract: Separating liquid-liquid dispersions in gravity settlers is critical in chemical, pharmaceutical, and recycling processes. The dense-packed zone height is an important performance and safety indicator but it is often expensive and impractical to measure due to optical limitations. We propose to estimate phase heights using only inexpensive volume flow measurements. To this end, a physics-informed neural network (PINN) is first pretrained on synthetic data and physics equations derived from a low-fidelity (approximate) mechanistic model to reduce the need for extensive experimental data. While the mechanistic model is used to generate synthetic training data, only volume balance equations are used in the PINN, since the integration of submodels describing droplet coalescence and sedimentation into the PINN would be computationally prohibitive. The pretrained PINN is then fine-tuned with scarce experimental data to capture the actual dynamics of the separator. We then employ the differentiable PINN as a predictive model in an Extended Kalman Filter inspired state estimation framework, enabling the phase heights to be tracked and updated from flow-rate measurements. We first test the two-stage trained PINN by forward simulation from a known initial state against the mechanistic model and a non-pretrained PINN. We then evaluate phase height estimation performance with the filter, comparing the two-stage trained PINN with a two-stage trained purely data-driven neural network. All model types are trained and evaluated using ensembles to account for model parameter uncertainty. In all evaluations, the two-stage trained PINN yields the most accurate phase-height estimates.

</details>


### [259] [Superlinear Multi-Step Attention](https://arxiv.org/abs/2601.18401)
*Yufeng Huang*

Main category: cs.LG

TL;DR: 提出超线性注意力架构，具次二次复杂度，以多步搜索问题重构自注意力，给出不同复杂度配置并展示性能，全面评估留待未来。


<details>
  <summary>Details</summary>
Motivation: 解决长序列标准因果自注意力复杂度高问题，实现随机上下文访问且无结构排除。

Method: 将标准因果自注意力重构为N步多步搜索问题，提出N=2实现，第一步进行跨度搜索选相关跨度，第二步对所选跨度应用跨度注意力。

Result: 在修改的30B混合MoE模型单B200 GPU上实现特定上下文长度解码吞吐量，在NIAH任务上有限训练获强性能。

Conclusion: 强调架构公式、缩放分析和系统可行性，初始验证成功，全面质量评估待未来。

Abstract: In this paper, we propose \textbf{Superlinear attention}, a fully trainable multi-step attention architecture that achieves subquadratic complexity for long sequences while preserving \textbf{random context access} (a.k.a.\ structural non-exclusion): no eligible token position is structurally excluded from being selected for attention. Superlinear attention reformulates standard causal self-attention as a multi-step search problem with $N$ steps, yielding an overall complexity of $O(L^{1+\frac{1}{N}})$. To illustrate the architecture, we present a baseline $N=2$ implementation, which is algorithmically analogous to standard jump search. In this $O(L^{3/2})$ instantiation, the first step performs $O(L^{3/2})$ span-search to select relevant spans of the sequence, and the second step applies $O(L^{3/2})$ span-attention (standard attention restricted to the selected spans). In an upscaled $O(L^{1.54})$ configuration for robustness, we achieve an average decoding throughput of 114 tokens/sec at 1M context length and 80 tokens/sec at 10M context in our implementation on a modified 30B hybrid MoE model on a single B200 GPU. With limited training, we also obtain strong performance on the NIAH (Needle In A Haystack) task up to 256K context length, demonstrating that the routed span selection is learnable end-to-end. This paper emphasizes architectural formulation, scaling analysis, and systems feasibility, and presents initial validation; comprehensive quality evaluations across diverse long-context tasks are left to future work.

</details>


### [260] [Frequency-Based Hyperparameter Selection in Games](https://arxiv.org/abs/2601.18409)
*Aniket Sanyal,Baraah A. M. Sidahmed,Rebekka Burkholz,Tatjana Chavdarova*

Main category: cs.LG

TL;DR: 提出基于振荡动力学频率估计的博弈超参数选择方法，引入Modal LookAhead (MoLA)，有收敛保证且加速训练。


<details>
  <summary>Details</summary>
Motivation: 博弈学习因旋转动力学与标准最小化不同，现有调参策略失效，有效调参方法研究不足，如LookAhead引入影响性能的额外参数。

Method: 利用振荡动力学的频率估计，分析连续时间轨迹振荡和离散动力学频谱，提出MoLA自适应选择超参数。

Result: MoLA在纯旋转博弈和混合机制中加速训练，计算开销小。

Conclusion: 提出的方法有效，可解决博弈学习中的超参数选择问题。

Abstract: Learning in smooth games fundamentally differs from standard minimization due to rotational dynamics, which invalidate classical hyperparameter tuning strategies. Despite their practical importance, effective methods for tuning in games remain underexplored. A notable example is LookAhead (LA), which achieves strong empirical performance but introduces additional parameters that critically influence performance. We propose a principled approach to hyperparameter selection in games by leveraging frequency estimation of oscillatory dynamics. Specifically, we analyze oscillations both in continuous-time trajectories and through the spectrum of the discrete dynamics in the associated frequency-based space. Building on this analysis, we introduce \emph{Modal LookAhead (MoLA)}, an extension of LA that selects the hyperparameters adaptively to a given problem. We provide convergence guarantees and demonstrate in experiments that MoLA accelerates training in both purely rotational games and mixed regimes, all with minimal computational overhead.

</details>


### [261] [Gradient Regularized Natural Gradients](https://arxiv.org/abs/2601.18420)
*Satya Prakash Dash,Hossein Abdi,Wei Pan,Samuel Kaski,Mingfei Sun*

Main category: cs.LG

TL;DR: 提出Gradient-Regularized Natural Gradients (GRNG)二阶优化器，融合梯度正则化与自然梯度更新，有收敛保证，实验显示其优于一阶和二阶基线方法。


<details>
  <summary>Details</summary>
Motivation: 此前较少关注二阶优化器的训练动态如何从梯度正则化中受益，希望提出新的优化器解决此问题。

Method: 提出GRNG优化器，包含基于结构化近似避免FIM显式求逆的频率主义变体，和基于Regularized - Kalman公式无需FIM求逆的贝叶斯变体。

Result: GRNG在视觉和语言基准上，相比一阶方法（SGD、AdamW）和二阶基线（K - FAC、Sophia），持续提升优化速度和泛化能力。

Conclusion: 梯度正则化是释放自然梯度方法在大规模深度学习中鲁棒性的有效工具。

Abstract: Gradient regularization (GR) has been shown to improve the generalizability of trained models. While Natural Gradient Descent has been shown to accelerate optimization in the initial phase of training, little attention has been paid to how the training dynamics of second-order optimizers can benefit from GR. In this work, we propose Gradient-Regularized Natural Gradients (GRNG), a family of scalable second-order optimizers that integrate explicit gradient regularization with natural gradient updates. Our framework provides two complementary algorithms: a frequentist variant that avoids explicit inversion of the Fisher Information Matrix (FIM) via structured approximations, and a Bayesian variant based on a Regularized-Kalman formulation that eliminates the need for FIM inversion entirely. We establish convergence guarantees for GRNG, showing that gradient regularization improves stability and enables convergence to global minima. Empirically, we demonstrate that GRNG consistently enhances both optimization speed and generalization compared to first-order methods (SGD, AdamW) and second-order baselines (K-FAC, Sophia), with strong results on vision and language benchmarks. Our findings highlight gradient regularization as a principled and practical tool to unlock the robustness of natural gradient methods for large-scale deep learning.

</details>


### [262] [GCFX: Generative Counterfactual Explanations for Deep Graph Models at the Model Level](https://arxiv.org/abs/2601.18447)
*Jinlong Hu,Jiacheng Liu*

Main category: cs.LG

TL;DR: 本文提出针对深度图学习模型的生成式模型级反事实解释方法GCFX，实验表明其性能优于现有方法，支持提升全局反事实解释的实用性和可信度。


<details>
  <summary>Details</summary>
Motivation: 深度图学习模型内部结构复杂、缺乏透明度，难以解释决策。本文旨在为用户提供对模型决策过程及潜在机制的全面理解。

Method: 引入基于深度图生成的GCFX方法，结合双编码器、结构感知标记器和消息传递神经网络解码器，利用增强的深度图生成框架和全局总结算法生成并筛选反事实解释。

Result: 在合成数据集和多个真实数据集上的实验显示，GCFX在反事实有效性和覆盖率方面优于现有方法，且解释成本低。

Conclusion: GCFX为提升全局反事实解释的实用性和可信度提供了关键支持。

Abstract: Deep graph learning models have demonstrated remarkable capabilities in processing graph-structured data and have been widely applied across various fields. However, their complex internal architectures and lack of transparency make it difficult to explain their decisions, resulting in opaque models that users find hard to understand and trust. In this paper, we explore model-level explanation techniques for deep graph learning models, aiming to provide users with a comprehensive understanding of the models' overall decision-making processes and underlying mechanisms. Specifically, we address the problem of counterfactual explanations for deep graph learning models by introducing a generative model-level counterfactual explanation approach called GCFX, which is based on deep graph generation. This approach generates a set of high-quality counterfactual explanations that reflect the model's global predictive behavior by leveraging an enhanced deep graph generation framework and a global summarization algorithm. GCFX features an architecture that combines dual encoders, structure-aware taggers, and Message Passing Neural Network decoders, enabling it to accurately learn the true latent distribution of input data and generate high-quality, closely related counterfactual examples. Subsequently, a global counterfactual summarization algorithm selects the most representative and comprehensive explanations from numerous candidate counterfactuals, providing broad insights into the model's global predictive patterns. Experiments on a synthetic dataset and several real-world datasets demonstrate that GCFX outperforms existing methods in terms of counterfactual validity and coverage while maintaining low explanation costs, thereby offering crucial support for enhancing the practicality and trustworthiness of global counterfactual explanations.

</details>


### [263] [Enhancing Control Policy Smoothness by Aligning Actions with Predictions from Preceding States](https://arxiv.org/abs/2601.18479)
*Kyoleen Kwak,Hyoseok Hwang*

Main category: cs.LG

TL;DR: 提出一种基于损失的方法，引入过渡诱导相似状态，提出ASAP动作平滑方法，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习的高频振荡问题使其难以在现实环境应用，现有基于损失的方法依赖启发式或合成的状态相似性定义，无法准确反映系统动力学。

Method: 引入过渡诱导相似状态，提出ASAP动作平滑方法，通过使动作与过渡诱导相似状态中的动作对齐和惩罚二阶差分来减轻动作振荡。

Result: 在Gymnasium和Isaac - Lab环境的实验中，ASAP实现了更平滑的控制和更好的策略性能。

Conclusion: 所提出的ASAP方法有效缓解了动作振荡问题，优于现有方法。

Abstract: Deep reinforcement learning has proven to be a powerful approach to solving control tasks, but its characteristic high-frequency oscillations make it difficult to apply in real-world environments. While prior methods have addressed action oscillations via architectural or loss-based methods, the latter typically depend on heuristic or synthetic definitions of state similarity to promote action consistency, which often fail to accurately reflect the underlying system dynamics. In this paper, we propose a novel loss-based method by introducing a transition-induced similar state. The transition-induced similar state is defined as the distribution of next states transitioned from the previous state. Since it utilizes only environmental feedback and actually collected data, it better captures system dynamics. Building upon this foundation, we introduce Action Smoothing by Aligning Actions with Predictions from Preceding States (ASAP), an action smoothing method that effectively mitigates action oscillations. ASAP enforces action smoothness by aligning the actions with those taken in transition-induced similar states and by penalizing second-order differences to suppress high-frequency oscillations. Experiments in Gymnasium and Isaac-Lab environments demonstrate that ASAP yields smoother control and improved policy performance over existing methods.

</details>


### [264] [Nearly Optimal Bayesian Inference for Structural Missingness](https://arxiv.org/abs/2601.18500)
*Chen Liang,Donghua Yang,Yutong Wang,Tianle Zhang,Shenghe Zhou,Zhiyu Liang,Hengtong Zhang,Hongzhi Wang,Ziqi Li,Xiyang Zhang,Zheng Liang,Yifei Li*

Main category: cs.LG

TL;DR: 论文针对结构缺失数据问题，提出基于贝叶斯视角的预测框架，该框架解耦缺失值后验学习与标签预测，在多个基准测试中达到SOTA，并有优化保证。


<details>
  <summary>Details</summary>
Motivation: 结构缺失数据打破了传统的插补再训练模式，存在因果循环、缺失值分布偏移和插补偏差等问题，需要新的解决方法。

Method: 从贝叶斯视角出发，通过优化预测后验分布，将学习模型内缺失值后验与标签预测解耦，实现后验积分。

Result: 在43个分类和15个插补基准测试中达到SOTA，并在SCM先验下有有限样本接近贝叶斯最优性的保证。

Conclusion: 所提出的解耦框架有效解决了结构缺失数据的预测问题，既能保留不确定性传播，又能实现高效预测。

Abstract: Structural missingness breaks 'just impute and train': values can be undefined by causal or logical constraints, and the mask may depend on observed variables, unobserved variables (MNAR), and other missingness indicators. It simultaneously brings (i) a catch-22 situation with causal loop, prediction needs the missing features, yet inferring them depends on the missingness mechanism, (ii) under MNAR, the unseen are different, the missing part can come from a shifted distribution, and (iii) plug-in imputation, a single fill-in can lock in uncertainty and yield overconfident, biased decisions. In the Bayesian view, prediction via the posterior predictive distribution integrates over the full model posterior uncertainty, rather than relying on a single point estimate. This framework decouples (i) learning an in-model missing-value posterior from (ii) label prediction by optimizing the predictive posterior distribution, enabling posterior integration. This decoupling yields an in-model almost-free-lunch: once the posterior is learned, prediction is plug-and-play while preserving uncertainty propagation. It achieves SOTA on 43 classification and 15 imputation benchmarks, with finite-sample near Bayes-optimality guarantees under our SCM prior.

</details>


### [265] [Conformal Prediction Algorithms for Time Series Forecasting: Methods and Benchmark](https://arxiv.org/abs/2601.18509)
*Andro Sabashvili*

Main category: cs.LG

TL;DR: 本文综述解决共形预测应用于时间序列时数据可交换性冲突的算法方案，强调计算效率和实际性能。


<details>
  <summary>Details</summary>
Motivation: 可靠的不确定性量化对时间序列预测很重要，但传统方法有分布假设限制，共形预测应用于时序数据时面临数据可交换性假设冲突问题。

Method: 调查和基准测试放宽可交换性假设、重新定义数据单元、显式建模预测残差动态、适应分布变化的在线学习等算法方案。

Result: 对不同算法方案进行了评估和综合。

Conclusion: 强调了这些方法在计算效率和实际数据上的性能。

Abstract: Reliable uncertainty quantification is of critical importance in time series forecasting, yet traditional methods often rely on restrictive distributional assumptions. Conformal prediction (CP) has emerged as a promising distribution-free framework for generating prediction intervals with rigorous theoretical guarantees. However, applying CP to sequential data presents a primary challenge: the temporal dependencies inherent in time series fundamentally violate the core assumption of data exchangeability, upon which standard CP guarantees are built. This review critically examines the main categories of algorithmic solutions designed to address this conflict. We survey and benchmark methods that relax the exchangeability assumption, those that redefine the data unit to be a collection of independent time series, approaches that explicitly model the dynamics of the prediction residuals, and online learning algorithms that adapt to distribution shifts to maintain long-run coverage. By synthesizing these approaches, we highlight computational efficiency and practical performance on real-world data.

</details>


### [266] [Just-In-Time Reinforcement Learning: Continual Learning in LLM Agents Without Gradient Updates](https://arxiv.org/abs/2601.18510)
*Yibo Li,Zijie Lin,Ailin Deng,Xuan Zhang,Yufei He,Shuo Ji,Tri Cao,Bryan Hooi*

Main category: cs.LG

TL;DR: 提出无训练框架JitRL实现测试时策略优化，在实验中表现优异且成本低。


<details>
  <summary>Details</summary>
Motivation: 大语言模型代理部署后权重冻结难持续适应，传统强化学习计算成本高且有灾难性遗忘风险。

Method: 引入JitRL，维护动态非参数经验记忆，实时检索相关轨迹估计动作优势，用估计值直接调整LLM输出对数。理论证明更新规则是KL约束策略优化目标的精确闭式解。

Result: 在WebArena和Jericho上实验表明JitRL是无训练方法中的新最优，性能超昂贵微调方法，成本降低超30倍。

Conclusion: JitRL为持续学习代理提供了可扩展路径。

Abstract: While Large Language Model (LLM) agents excel at general tasks, they inherently struggle with continual adaptation due to the frozen weights after deployment. Conventional reinforcement learning (RL) offers a solution but incurs prohibitive computational costs and the risk of catastrophic forgetting. We introduce Just-In-Time Reinforcement Learning (JitRL), a training-free framework that enables test-time policy optimization without any gradient updates. JitRL maintains a dynamic, non-parametric memory of experiences and retrieves relevant trajectories to estimate action advantages on-the-fly. These estimates are then used to directly modulate the LLM's output logits. We theoretically prove that this additive update rule is the exact closed-form solution to the KL-constrained policy optimization objective. Extensive experiments on WebArena and Jericho demonstrate that JitRL establishes a new state-of-the-art among training-free methods. Crucially, JitRL outperforms the performance of computationally expensive fine-tuning methods (e.g., WebRL) while reducing monetary costs by over 30 times, offering a scalable path for continual learning agents. The code is available at https://github.com/liushiliushi/JitRL.

</details>


### [267] [LipNeXt: Scaling up Lipschitz-based Certified Robustness to Billion-parameter Models](https://arxiv.org/abs/2601.18513)
*Kai Hu,Haoqi Hu,Matt Fredrikson*

Main category: cs.LG

TL;DR: 介绍LipNeXt用于认证鲁棒性，在多数据集取得SOTA，展示Lipschitz认证可受益于现代扩展趋势。


<details>
  <summary>Details</summary>
Motivation: Lipschitz认证在模型大小、训练效率和ImageNet性能扩展性方面存在问题。

Method: 采用流形优化更新正交流形上的参数，使用Spatial Shift Module建模空间模式，网络结合多种组件维持Lipschitz控制。

Result: 在CIFAR - 10/100和Tiny - ImageNet上取得SOTA的清洁和认证鲁棒准确率，在ImageNet可扩展到1 - 2B大模型，CRA提升。

Conclusion: Lipschitz认证能从现代扩展趋势中受益，不牺牲确定性和效率。

Abstract: Lipschitz-based certification offers efficient, deterministic robustness guarantees but has struggled to scale in model size, training efficiency, and ImageNet performance. We introduce \emph{LipNeXt}, the first \emph{constraint-free} and \emph{convolution-free} 1-Lipschitz architecture for certified robustness. LipNeXt is built using two techniques: (1) a manifold optimization procedure that updates parameters directly on the orthogonal manifold and (2) a \emph{Spatial Shift Module} to model spatial pattern without convolutions. The full network uses orthogonal projections, spatial shifts, a simple 1-Lipschitz $β$-Abs nonlinearity, and $L_2$ spatial pooling to maintain tight Lipschitz control while enabling expressive feature mixing. Across CIFAR-10/100 and Tiny-ImageNet, LipNeXt achieves state-of-the-art clean and certified robust accuracy (CRA), and on ImageNet it scales to 1-2B large models, improving CRA over prior Lipschitz models (e.g., up to $+8\%$ at $\varepsilon{=}1$) while retaining efficient, stable low-precision training. These results demonstrate that Lipschitz-based certification can benefit from modern scaling trends without sacrificing determinism or efficiency.

</details>


### [268] [Scalable Transit Delay Prediction at City Scale: A Systematic Approach with Multi-Resolution Feature Engineering and Deep Learning](https://arxiv.org/abs/2601.18521)
*Emna Boudabbous,Mohamed Karaa,Lokman Sboui,Julio Montecinos,Omar Alam*

Main category: cs.LG

TL;DR: 提出城市规模公交延误预测管道，结合多分辨率特征工程、降维和深度学习，在蒙特利尔公交网络测试效果好，适用于实时、城市规模部署。


<details>
  <summary>Details</summary>
Motivation: 城市公交机构需可靠的全网延误预测，现有系统存在处理路线少、依赖手工特征、缺乏可扩展架构设计指导等问题。

Method: 构建城市规模预测管道，结合多分辨率特征工程、降维和深度学习，引入混合H3+拓扑聚类方法。

Result: 全局LSTM模型结合聚类感知特征在准确性和效率上取得最佳平衡，比Transformer模型性能优18 - 52%，参数少275倍。

Conclusion: 所提管道适用于实时、城市规模部署，稍作调整可用于其他网络。

Abstract: Urban bus transit agencies need reliable, network-wide delay predictions to provide accurate arrival information to passengers and support real-time operational control. Accurate predictions help passengers plan their trips, reduce waiting time, and allow operations staff to adjust headways, dispatch extra vehicles, and manage disruptions. Although real-time feeds such as GTFS-Realtime (GTFS-RT) are now widely available, most existing delay prediction systems handle only a few routes, depend on hand-crafted features, and offer little guidance on how to design a scalable, reusable architecture.
  We present a city-scale prediction pipeline that combines multi-resolution feature engineering, dimensionality reduction, and deep learning. The framework generates 1,683 spatiotemporal features by exploring 23 aggregation combinations over H3 cells, routes, segments, and temporal patterns, and compresses them into 83 components using Adaptive PCA while preserving 95% of the variance. To avoid the "giant cluster" problem that occurs when dense urban areas fall into a single H3 region, we introduce a hybrid H3+topology clustering method that yields 12 balanced route clusters (coefficient of variation 0.608) and enables efficient distributed training.
  We compare five model architectures on six months of bus operations from the Société de transport de Montréal (STM) network in Montréal. A global LSTM with cluster-aware features achieves the best trade-off between accuracy and efficiency, outperforming transformer models by 18 to 52% while using 275 times fewer parameters. We also report multi-level evaluation at the elementary segment, segment, and trip level with walk-forward validation and latency analysis, showing that the proposed pipeline is suitable for real-time, city-scale deployment and can be reused for other networks with limited adaptation.

</details>


### [269] [From Human Labels to Literature: Semi-Supervised Learning of NMR Chemical Shifts at Scale](https://arxiv.org/abs/2601.18524)
*Yongqi Jin,Yecheng Wang,Jun-jie Wang,Rong Zhu,Guolin Ke,Weinan E*

Main category: cs.LG

TL;DR: 提出半监督框架，利用文献中未分配原子级别的光谱数据训练NMR化学位移预测模型，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法依赖有限且人工标注的数据集，难以准确预测核磁共振化学位移。

Method: 提出半监督框架，将文献光谱的化学位移预测表述为排列不变集监督问题，在满足一定条件下，最优二分匹配可简化为基于排序的损失。

Result: 模型在准确性、鲁棒性和泛化能力上优于现有方法，首次捕捉到常见NMR溶剂的系统溶剂效应。

Conclusion: 文献中的大规模未标记光谱可作为训练NMR位移模型的实用有效数据源，文献衍生的弱结构数据在以数据为中心的科学人工智能中有更广泛作用。

Abstract: Accurate prediction of nuclear magnetic resonance (NMR) chemical shifts is fundamental to spectral analysis and molecular structure elucidation, yet existing machine learning methods rely on limited, labor-intensive atom-assigned datasets. We propose a semi-supervised framework that learns NMR chemical shifts from millions of literature-extracted spectra without explicit atom-level assignments, integrating a small amount of labeled data with large-scale unassigned spectra. We formulate chemical shift prediction from literature spectra as a permutation-invariant set supervision problem, and show that under commonly satisfied conditions on the loss function, optimal bipartite matching reduces to a sorting-based loss, enabling stable large-scale semi-supervised training beyond traditional curated datasets. Our models achieve substantially improved accuracy and robustness over state-of-the-art methods and exhibit stronger generalization on significantly larger and more diverse molecular datasets. Moreover, by incorporating solvent information at scale, our approach captures systematic solvent effects across common NMR solvents for the first time. Overall, our results demonstrate that large-scale unlabeled spectra mined from the literature can serve as a practical and effective data source for training NMR shift models, suggesting a broader role of literature-derived, weakly structured data in data-centric AI for science.

</details>


### [270] [Closing the Modality Gap Aligns Group-Wise Semantics](https://arxiv.org/abs/2601.18525)
*Eleonora Grassucci,Giordano Cicchetti,Emanuele Frasca,Aurelio Uncini,Danilo Comminiello*

Main category: cs.LG

TL;DR: 文章指出CLIP学习潜在空间存在模态差距，提出新方法减少差距，发现减少差距对分组任务提升大，对实例任务影响小。


<details>
  <summary>Details</summary>
Motivation: 虽然模态差距对实例任务影响有限，但要探究其在分组任务中的影响，且找到减少差距的方法。

Method: 提出一种新方法用于减少双模态设置中的模态差距，并可扩展到n模态情况。

Result: 减少差距对传统实例任务提升微弱或不稳定，但显著提升分组任务。

Conclusion: 模态差距在需要语义分组的任务中起关键作用，可能重塑对其理解。

Abstract: In multimodal learning, CLIP has been recognized as the \textit{de facto} method for learning a shared latent space across multiple modalities, placing similar representations close to each other and moving them away from dissimilar ones. Although CLIP-based losses effectively align modalities at the semantic level, the resulting latent spaces often remain only partially shared, revealing a structural mismatch known as the modality gap. While the necessity of addressing this phenomenon remains debated, particularly given its limited impact on instance-wise tasks (e.g., retrieval), we prove that its influence is instead strongly pronounced in group-level tasks (e.g., clustering). To support this claim, we introduce a novel method designed to consistently reduce this discrepancy in two-modal settings, with a straightforward extension to the general $n$-modal case. Through our extensive evaluation, we demonstrate our novel insight: while reducing the gap provides only marginal or inconsistent improvements in traditional instance-wise tasks, it significantly enhances group-wise tasks. These findings may reshape our understanding of the modality gap, highlighting its key role in improving performance on tasks requiring semantic grouping.

</details>


### [271] [Information Hidden in Gradients of Regression with Target Noise](https://arxiv.org/abs/2601.18546)
*Arash Jamshidi,Katsiaryna Haitsiukevich,Kai Puolamäki*

Main category: cs.LG

TL;DR: 论文指出仅用梯度可揭示Hessian矩阵，提出简单方差校准方法，有理论保证和应用场景，并用实验验证。


<details>
  <summary>Details</summary>
Motivation: 二阶信息对优化等很关键，但现代很多场景只能观测到梯度，需探索用梯度揭示二阶信息的方法。

Method: 采用简单方差校准，注入高斯噪声使总目标噪声方差等于批量大小，确保经验梯度协方差近似Hessian矩阵。

Result: 在次高斯输入下给出非渐近算子范数保证，表明无校准恢复会失败，方法实用且鲁棒。

Conclusion: 提出的方法可用于预条件加速优化、对抗风险估计等，理论结果通过实验验证。

Abstract: Second-order information -- such as curvature or data covariance -- is critical for optimisation, diagnostics, and robustness. However, in many modern settings, only the gradients are observable. We show that the gradients alone can reveal the Hessian, equalling the data covariance $Σ$ for the linear regression. Our key insight is a simple variance calibration: injecting Gaussian noise so that the total target noise variance equals the batch size ensures that the empirical gradient covariance closely approximates the Hessian, even when evaluated far from the optimum. We provide non-asymptotic operator-norm guarantees under sub-Gaussian inputs. We also show that without such calibration, recovery can fail by an $Ω(1)$ factor. The proposed method is practical (a "set target-noise variance to $n$" rule) and robust (variance $\mathcal{O}(n)$ suffices to recover $Σ$ up to scale). Applications include preconditioning for faster optimisation, adversarial risk estimation, and gradient-only training, for example, in distributed systems. We support our theoretical results with experiments on synthetic and real data.

</details>


### [272] [An Unsupervised Tensor-Based Domain Alignment](https://arxiv.org/abs/2601.18564)
*Chong Hyun Lee,Kibae Lee,Hyun Hee Yim*

Main category: cs.LG

TL;DR: 提出基于张量的域对齐算法，通过迭代优化对齐矩阵，实验显示可提升转换速度和分类准确率，优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 设计一种更具灵活性和适应性的域对齐算法，解决复杂域适应任务。

Method: 使用对齐矩阵在不变子空间中对齐源和目标张量，对子空间和矩阵在斜流形上进行迭代优化，并添加正则化项。

Result: 该方法增强了域对齐转换速度，显著提高了分类准确率。

Conclusion: 此方法优于当前最先进技术，是复杂域适应任务的首选方法。

Abstract: We propose a tensor-based domain alignment (DA) algorithm designed to align source and target tensors within an invariant subspace through the use of alignment matrices. These matrices along with the subspace undergo iterative optimization of which constraint is on oblique manifold, which offers greater flexibility and adaptability compared to the traditional Stiefel manifold. Moreover, regularization terms defined to preserve the variance of both source and target tensors, ensures robust performance. Our framework is versatile, effectively generalizing existing tensor-based DA methods as special cases. Through extensive experiments, we demonstrate that our approach not only enhances DA conversion speed but also significantly boosts classification accuracy. This positions our method as superior to current state-of-the-art techniques, making it a preferable choice for complex domain adaptation tasks.

</details>


### [273] [K-Myriad: Jump-starting reinforcement learning with unsupervised parallel agents](https://arxiv.org/abs/2601.18580)
*Vincenzo De Paola,Mirco Mutti,Riccardo Zamboni,Marcello Restelli*

Main category: cs.LG

TL;DR: 提出K - Myriad方法，可实现强化学习并行化的高效探索，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习并行化方法忽略多样探索策略优势，限制了并行化潜力。

Method: 提出K - Myriad这一可扩展且无监督的方法，最大化并行策略群体的集体状态熵。

Result: 在高维连续控制任务大规模并行化实验中，K - Myriad能学习到大量不同策略。

Conclusion: K - Myriad对集体探索有效，为新的并行化策略铺平道路。

Abstract: Parallelization in Reinforcement Learning is typically employed to speed up the training of a single policy, where multiple workers collect experience from an identical sampling distribution. This common design limits the potential of parallelization by neglecting the advantages of diverse exploration strategies. We propose K-Myriad, a scalable and unsupervised method that maximizes the collective state entropy induced by a population of parallel policies. By cultivating a portfolio of specialized exploration strategies, K-Myriad provides a robust initialization for Reinforcement Learning, leading to both higher training efficiency and the discovery of heterogeneous solutions. Experiments on high-dimensional continuous control tasks, with large-scale parallelization, demonstrate that K-Myriad can learn a broad set of distinct policies, highlighting its effectiveness for collective exploration and paving the way towards novel parallelization strategies.

</details>


### [274] [Learning long term climate-resilient transport adaptation pathways under direct and indirect flood impacts using reinforcement learning](https://arxiv.org/abs/2601.18586)
*Miguel Costa,Arthur Vandervoort,Carolin Schmidt,Morten W. Petersen,Martin Drews,Karyn Morrissey,Francisco C. Pereira*

Main category: cs.LG

TL;DR: 提出耦合IAM与RL的决策支持框架应对城市交通系统气候适应问题，并在哥本哈根案例验证有效性。


<details>
  <summary>Details</summary>
Motivation: 气候变化加剧降雨等灾害，增加城市交通系统干扰，基础设施投资长期、复杂且有深度不确定性，设计有效适应策略具挑战。

Method: 提出耦合IAM与RL的通用决策支持框架，结合气候预测等多模型，通过强化学习循环学习适应策略。

Result: 在哥本哈根内城案例中，学习到的策略产生时空协调路径，较传统优化基线更具鲁棒性。

Conclusion: 框架可转移应用到其他灾害和城市。

Abstract: Climate change is expected to intensify rainfall and other hazards, increasing disruptions in urban transportation systems. Designing effective adaptation strategies is challenging due to the long-term, sequential nature of infrastructure investments, deep uncertainty, and complex cross-sector interactions. We propose a generic decision-support framework that couples an integrated assessment model (IAM) with reinforcement learning (RL) to learn adaptive, multi-decade investment pathways under uncertainty. The framework combines long-term climate projections (e.g., IPCC scenario pathways) with models that map projected extreme-weather drivers (e.g. rain) into hazard likelihoods (e.g. flooding), propagate hazards into urban infrastructure impacts (e.g. transport disruption), and value direct and indirect consequences for service performance and societal costs. Embedded in a reinforcement-learning loop, it learns adaptive climate adaptation policies that trade off investment and maintenance expenditures against avoided impacts. In collaboration with Copenhagen Municipality, we demonstrate the approach on pluvial flooding in the inner city for the horizon of 2024 to 2100. The learned strategies yield coordinated spatial-temporal pathways and improved robustness relative to conventional optimization baselines, namely inaction and random action, illustrating the framework's transferability to other hazards and cities.

</details>


### [275] [LaCoGSEA: Unsupervised deep learning for pathway analysis via latent correlation](https://arxiv.org/abs/2601.18604)
*Zhiwei Zheng,Kevin Bryson*

Main category: cs.LG

TL;DR: 介绍无监督通路富集分析框架LaCoGSEA，相较于现有方法有优势，在相关任务中达最优性能。


<details>
  <summary>Details</summary>
Motivation: 传统通路富集分析方法在无监督场景有局限，现有无监督扩展方法主要捕捉线性关系且未显式建模基因 - 通路关联，深度学习模型在无监督转录组分析中通路级解释效果有限。

Method: 提出LaCoGSEA框架，结合深度表示学习与稳健通路统计，用自编码器捕捉非线性流形，提出全局基因 - 潜在相关性度量来生成无先验标签的密集基因排名。

Result: LaCoGSEA在区分癌症亚型聚类性能上优于现有无监督基线，能恢复更多高阶有生物学意义的通路，在不同实验方案和数据集大小下保持高鲁棒性和一致性。

Conclusion: LaCoGSEA在无监督通路富集分析中提供了最优性能。

Abstract: Motivation: Pathway enrichment analysis is widely used to interpret gene expression data. Standard approaches, such as GSEA, rely on predefined phenotypic labels and pairwise comparisons, which limits their applicability in unsupervised settings. Existing unsupervised extensions, including single-sample methods, provide pathway-level summaries but primarily capture linear relationships and do not explicitly model gene-pathway associations. More recently, deep learning models have been explored to capture non-linear transcriptomic structure. However, their interpretation has typically relied on generic explainable AI (XAI) techniques designed for feature-level attribution. As these methods are not designed for pathway-level interpretation in unsupervised transcriptomic analyses, their effectiveness in this setting remains limited.
  Results: To bridge this gap, we introduce LaCoGSEA (Latent Correlation GSEA), an unsupervised framework that integrates deep representation learning with robust pathway statistics. LaCoGSEA employs an autoencoder to capture non-linear manifolds and proposes a global gene-latent correlation metric as a proxy for differential expression, generating dense gene rankings without prior labels. We demonstrate that LaCoGSEA offers three key advantages: (i) it achieves improved clustering performance in distinguishing cancer subtypes compared to existing unsupervised baselines; (ii) it recovers a broader range of biologically meaningful pathways at higher ranks compared with linear dimensionality reduction and gradient-based XAI methods; and (iii) it maintains high robustness and consistency across varying experimental protocols and dataset sizes. Overall, LaCoGSEA provides state-of-the-art performance in unsupervised pathway enrichment analysis.
  Availability and implementation: https://github.com/willyzzz/LaCoGSEA

</details>


### [276] [Geometry-Free Conditional Diffusion Modeling for Solving the Inverse Electrocardiography Problem](https://arxiv.org/abs/2601.18615)
*Ramiro Valdes Jara,Adam Meyers*

Main category: cs.LG

TL;DR: 本文提出一种数据驱动模型解决心电图反问题，展现优越重建精度。


<details>
  <summary>Details</summary>
Motivation: 现有心电图成像反问题求解方法存在局限性，需解决非唯一和欠定问题及避免患者特定网格构建。

Method: 提出条件扩散框架，学习从嘈杂体表信号到心脏表面电势的概率映射，为数据驱动且无几何依赖。

Result: 在真实数据集上评估，较多种确定基线模型具有更好的重建精度。

Conclusion: 扩散模型可作为无创心脏电生理成像的可靠工具。

Abstract: This paper proposes a data-driven model for solving the inverse problem of electrocardiography, the mathematical problem that forms the basis of electrocardiographic imaging (ECGI). We present a conditional diffusion framework that learns a probabilistic mapping from noisy body surface signals to heart surface electric potentials. The proposed approach leverages the generative nature of diffusion models to capture the non-unique and underdetermined nature of the ECGI inverse problem, enabling probabilistic sampling of multiple reconstructions rather than a single deterministic estimate. Unlike traditional methods, the proposed framework is geometry-free and purely data-driven, alleviating the need for patient-specific mesh construction. We evaluate the method on a real ECGI dataset and compare it against strong deterministic baselines, including a convolutional neural network, long short-term memory network, and transformer-based model. The results demonstrate that the proposed diffusion approach achieves improved reconstruction accuracy, highlighting the potential of diffusion models as a robust tool for noninvasive cardiac electrophysiology imaging.

</details>


### [277] [CASSANDRA: Programmatic and Probabilistic Learning and Inference for Stochastic World Modeling](https://arxiv.org/abs/2601.18620)
*Panagiotis Lymperopoulos,Abhiramon Rajasekharan,Ian Berlot-Attwell,Stéphane Aroca-Ouellette,Kaheer Suleman*

Main category: cs.LG

TL;DR: 提出CASSANDRA神经符号世界建模方法，结合LLM构建轻量级转移模型用于规划，在模拟场景中效果优于基线。


<details>
  <summary>Details</summary>
Motivation: 现实领域如商业规划需构建世界模型，可利用世界知识从有限数据建模复杂动作效果和因果关系。

Method: 提出CASSANDRA方法，整合LLM合成代码建模确定性特征和LLM引导的概率图模型结构学习捕捉随机变量因果关系。

Result: 在咖啡店模拟器和主题公园商业模拟器中，CASSANDRA在转移预测和规划上显著优于基线。

Conclusion: CASSANDRA方法能有效利用LLM知识先验构建世界模型用于规划。

Abstract: Building world models is essential for planning in real-world domains such as businesses. Since such domains have rich semantics, we can leverage world knowledge to effectively model complex action effects and causal relationships from limited data. In this work, we propose CASSANDRA, a neurosymbolic world modeling approach that leverages an LLM as a knowledge prior to construct lightweight transition models for planning. CASSANDRA integrates two components: (1) LLM-synthesized code to model deterministic features, and (2) LLM-guided structure learning of a probabilistic graphical model to capture causal relationships among stochastic variables. We evaluate CASSANDRA in (i) a small-scale coffee-shop simulator and (ii) a complex theme park business simulator, where we demonstrate significant improvements in transition prediction and planning over baselines.

</details>


### [278] [Physics-Informed Uncertainty Enables Reliable AI-driven Design](https://arxiv.org/abs/2601.18638)
*Tingkai Xue,Chin Chun Ooi,Yang Jiang,Luu Trung Pham Duong,Pao-Hsiung Chiu,Weijiang Zhao,Nagarajan Raghavan,My Ha Dao*

Main category: cs.LG

TL;DR: 提出物理信息不确定性范式用于频率选择表面逆设计，提高成功率并降低计算成本，强调不确定性量化在机器学习驱动逆设计中的必要性。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习代理辅助优化方法在逆设计中未纳入不确定性量化，导致数据稀疏区域预测错误，优化性能差。

Method: 引入物理信息不确定性范式，将其集成到多保真度不确定性感知优化工作流程中设计20 - 30 GHz范围内的复杂频率选择表面。

Result: 将找到高性能解决方案的成功率从不到10%提高到50%以上，同时与仅使用高保真求解器相比，计算成本降低一个数量级。

Conclusion: 强调在高维问题的机器学习驱动逆设计中纳入不确定性量化的必要性，确立物理信息不确定性可作为物理系统代理模型不确定性量化的可行替代方法。

Abstract: Inverse design is a central goal in much of science and engineering, including frequency-selective surfaces (FSS) that are critical to microelectronics for telecommunications and optical metamaterials. Traditional surrogate-assisted optimization methods using deep learning can accelerate the design process but do not usually incorporate uncertainty quantification, leading to poorer optimization performance due to erroneous predictions in data-sparse regions. Here, we introduce and validate a fundamentally different paradigm of Physics-Informed Uncertainty, where the degree to which a model's prediction violates fundamental physical laws serves as a computationally-cheap and effective proxy for predictive uncertainty. By integrating physics-informed uncertainty into a multi-fidelity uncertainty-aware optimization workflow to design complex frequency-selective surfaces within the 20 - 30 GHz range, we increase the success rate of finding performant solutions from less than 10% to over 50%, while simultaneously reducing computational cost by an order of magnitude compared to the sole use of a high-fidelity solver. These results highlight the necessity of incorporating uncertainty quantification in machine-learning-driven inverse design for high-dimensional problems, and establish physics-informed uncertainty as a viable alternative to quantifying uncertainty in surrogate models for physical systems, thereby setting the stage for autonomous scientific discovery systems that can efficiently and robustly explore and evaluate candidate designs.

</details>


### [279] [TwinPurify: Purifying gene expression data to reveal tumor-intrinsic transcriptional programs via self-supervised learning](https://arxiv.org/abs/2601.18640)
*Zhiwei Zheng,Kevin Bryson*

Main category: cs.LG

TL;DR: 介绍TwinPurify框架处理批量转录组数据，能分离肿瘤特异性信号，表现优于传统方法，扩展临床数据集用途。


<details>
  <summary>Details</summary>
Motivation: 现有批量转录组数据因肿瘤纯度变化掩盖肿瘤内在转录信号，去卷积方法难以推广到真实患者队列。

Method: 引入TwinPurify表示学习框架，采用Barlow Twins自监督目标，利用同队列相邻正常样本作为“背景”指导学习连续高维肿瘤嵌入。

Result: 在多个大型癌症队列中，TwinPurify在恢复肿瘤内在和免疫信号方面优于自编码器等传统基线，纯化嵌入在多种任务上表现更好。

Conclusion: TwinPurify为批量转录组数据去污染提供可转移框架，扩展现有临床数据集用于分子发现的效用。

Abstract: Advances in single-cell and spatial transcriptomic technologies have transformed tumor ecosystem profiling at cellular resolution. However, large scale studies on patient cohorts continue to rely on bulk transcriptomic data, where variation in tumor purity obscures tumor-intrinsic transcriptional signals and constrains downstream discovery. Many deconvolution methods report strong performance on synthetic bulk mixtures but fail to generalize to real patient cohorts because of unmodeled biological and technical variation.
  Here, we introduce TwinPurify, a representation learning framework that adapts the Barlow Twins self-supervised objective, representing a fundamental departure from the deconvolution paradigm. Rather than resolving the bulk mixture into discrete cell-type fractions, TwinPurify instead learns continuous, high-dimensional tumor embeddings by leveraging adjacent-normal profiles within the same cohort as "background" guidance, enabling the disentanglement of tumor-specific signals without relying on any external reference.
  Benchmarked against multiple large cancer cohorts across RNA-seq and microarray platforms, TwinPurify outperforms conventional representation learning baselines like auto-encoders in recovering tumor-intrinsic and immune signals. The purified embeddings improve molecular subtype and grade classification, enhance survival model concordance, and uncover biologically meaningful pathway activities compared to raw bulk profiles. By providing a transferable framework for decontaminating bulk transcriptomics, TwinPurify extends the utility of existing clinical datasets for molecular discovery.

</details>


### [280] [FaLW: A Forgetting-aware Loss Reweighting for Long-tailed Unlearning](https://arxiv.org/abs/2601.18650)
*Liheng Yu,Zhe Zhao,Yuxuan Wang,Pengkun Wang,Binwu Wang,Yang Wang*

Main category: cs.LG

TL;DR: 现有机器学习遗忘研究多基于平衡遗忘集，本文首次研究长尾分布下的遗忘问题，提出 FaLW 方法并验证其性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究多在平衡遗忘集上评估遗忘方法，忽略了现实中待遗忘数据的长尾分布情况，为此开展研究。

Method: 提出 FaLW 这一即插即用、实例级动态损失重加权方法，通过比较样本预测概率和同类未见数据分布评估遗忘状态，并利用遗忘感知重加权方案调整遗忘强度。

Result: 大量实验表明 FaLW 取得了更优性能。

Conclusion: FaLW 能有效解决长尾分布下现有遗忘方法存在的问题，在相关场景有出色表现。

Abstract: Machine unlearning, which aims to efficiently remove the influence of specific data from trained models, is crucial for upholding data privacy regulations like the ``right to be forgotten". However, existing research predominantly evaluates unlearning methods on relatively balanced forget sets. This overlooks a common real-world scenario where data to be forgotten, such as a user's activity records, follows a long-tailed distribution. Our work is the first to investigate this critical research gap. We find that in such long-tailed settings, existing methods suffer from two key issues: \textit{Heterogeneous Unlearning Deviation} and \textit{Skewed Unlearning Deviation}. To address these challenges, we propose FaLW, a plug-and-play, instance-wise dynamic loss reweighting method. FaLW innovatively assesses the unlearning state of each sample by comparing its predictive probability to the distribution of unseen data from the same class. Based on this, it uses a forgetting-aware reweighting scheme, modulated by a balancing factor, to adaptively adjust the unlearning intensity for each sample. Extensive experiments demonstrate that FaLW achieves superior performance. Code is available at \textbf{Supplementary Material}.

</details>


### [281] [A Dynamic Framework for Grid Adaptation in Kolmogorov-Arnold Networks](https://arxiv.org/abs/2601.18672)
*Spyros Rigas,Thanasis Papaioannou,Panagiotis Trakadas,Georgios Alexandridis*

Main category: cs.LG

TL;DR: 提出广义框架，引入基于曲率的自适应策略用于KAN训练，在多任务中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有KAN适应策略仅依赖输入数据密度，未考虑目标函数几何复杂性和训练指标。

Method: 提出将节点分配视为由重要性密度函数控制的密度估计任务的广义框架，引入基于曲率的自适应策略。

Result: 在合成函数拟合、Feynman数据集回归和Helmholtz PDE实例中，平均相对误差分别降低25.3%、9.4%和23.3%，经Wilcoxon符号秩检验具有统计学意义。

Conclusion: 基于曲率的自适应策略是KAN训练的稳健且计算高效的替代方案。

Abstract: Kolmogorov-Arnold Networks (KANs) have recently demonstrated promising potential in scientific machine learning, partly due to their capacity for grid adaptation during training. However, existing adaptation strategies rely solely on input data density, failing to account for the geometric complexity of the target function or metrics calculated during network training. In this work, we propose a generalized framework that treats knot allocation as a density estimation task governed by Importance Density Functions (IDFs), allowing training dynamics to determine grid resolution. We introduce a curvature-based adaptation strategy and evaluate it across synthetic function fitting, regression on a subset of the Feynman dataset and different instances of the Helmholtz PDE, demonstrating that it significantly outperforms the standard input-based baseline. Specifically, our method yields average relative error reductions of 25.3% on synthetic functions, 9.4% on the Feynman dataset, and 23.3% on the PDE benchmark. Statistical significance is confirmed via Wilcoxon signed-rank tests, establishing curvature-based adaptation as a robust and computationally efficient alternative for KAN training.

</details>


### [282] [Learning temporal embeddings from electronic health records of chronic kidney disease patients](https://arxiv.org/abs/2601.18675)
*Aditya Kumar,Mario A. Cypko,Oliver Amft*

Main category: cs.LG

TL;DR: 研究纵向电子病历时间嵌入模型能否在不影响预测性能下学习有临床意义表征及架构选择对嵌入质量影响。用MIMIC - IV数据集对比三种循环架构，T - LSTM嵌入更优，且嵌入模型在ICU死亡率预测上表现更佳。


<details>
  <summary>Details</summary>
Motivation: 模型引导医学需能捕捉疾病动态、透明且与任务无关的表征，而多数临床预测模型为单任务优化，所以研究能否满足需求及架构选择影响。

Method: 使用MIMIC - IV数据集研究慢性肾病患者，对比vanilla LSTM、attention - augmented LSTM和T - LSTM三种循环架构，模型均作为嵌入模型和端到端预测器训练，通过CKD阶段聚类和ICU死亡率预测评估嵌入质量。

Result: T - LSTM产生更有结构的嵌入，CKD阶段分类准确率更高、DBI值更低；嵌入模型在ICU死亡率预测上一致优于端到端预测器，准确率提升。

Conclusion: 学习嵌入作为中间步骤比直接端到端学习更有效。

Abstract: We investigate whether temporal embedding models trained on longitudinal electronic health records can learn clinically meaningful representations without compromising predictive performance, and how architectural choices affect embedding quality. Model-guided medicine requires representations that capture disease dynamics while remaining transparent and task agnostic, whereas most clinical prediction models are optimised for a single task. Representation learning facilitates learning embeddings that generalise across downstream tasks, and recurrent architectures are well-suited for modelling temporal structure in observational clinical data. Using the MIMIC-IV dataset, we study patients with chronic kidney disease (CKD) and compare three recurrent architectures: a vanilla LSTM, an attention-augmented LSTM, and a time-aware LSTM (T-LSTM). All models are trained both as embedding models and as direct end-to-end predictors. Embedding quality is evaluated via CKD stage clustering and in-ICU mortality prediction. The T-LSTM produces more structured embeddings, achieving a lower Davies-Bouldin Index (DBI = 9.91) and higher CKD stage classification accuracy (0.74) than the vanilla LSTM (DBI = 15.85, accuracy = 0.63) and attention-augmented LSTM (DBI = 20.72, accuracy = 0.67). For in-ICU mortality prediction, embedding models consistently outperform end-to-end predictors, improving accuracy from 0.72-0.75 to 0.82-0.83, which indicates that learning embeddings as an intermediate step is more effective than direct end-to-end learning.

</details>


### [283] [Quasi Monte Carlo methods enable extremely low-dimensional deep generative models](https://arxiv.org/abs/2601.18676)
*Miles Martinez,Alex H. Williams*

Main category: cs.LG

TL;DR: 介绍准蒙特卡罗潜变量模型（QLVMs），适合高维数据低维可解释嵌入，实证表现优于传统模型，虽计算密集但适用于注重可解释性的场景。


<details>
  <summary>Details</summary>
Motivation: 寻找高维数据集的极低维、可解释嵌入，改进现有生成模型的问题。

Method: 使用随机化的准蒙特卡罗积分直接近似边际似然。

Result: QLVMs 在一系列数据集上始终优于具有匹配潜维数的传统变分自编码器（VAEs）和重要性加权自编码器（IWAEs）。

Conclusion: QLVMs 计算密集，在复杂数据集生成细节有困难，但为注重可解释性和潜空间分析的应用提供了有效解决方案。

Abstract: This paper introduces quasi-Monte Carlo latent variable models (QLVMs): a class of deep generative models that are specialized for finding extremely low-dimensional and interpretable embeddings of high-dimensional datasets. Unlike standard approaches, which rely on a learned encoder and variational lower bounds, QLVMs directly approximate the marginal likelihood by randomized quasi-Monte Carlo integration. While this brute force approach has drawbacks in higher-dimensional spaces, we find that it excels in fitting one, two, and three dimensional deep latent variable models. Empirical results on a range of datasets show that QLVMs consistently outperform conventional variational autoencoders (VAEs) and importance weighted autoencoders (IWAEs) with matched latent dimensionality. The resulting embeddings enable transparent visualization and post hoc analyses such as nonparametric density estimation, clustering, and geodesic path computation, which are nontrivial to validate in higher-dimensional spaces. While our approach is compute-intensive and struggles to generate fine-scale details in complex datasets, it offers a compelling solution for applications prioritizing interpretability and latent space analysis.

</details>


### [284] [Counterfactual Explanations on Robust Perceptual Geodesics](https://arxiv.org/abs/2601.18678)
*Eslam Zaher,Maciej Trzaskowski,Quan Nguyen,Fred Roosta*

Main category: cs.LG

TL;DR: 提出感知反事实测地线(PCG)方法构建反事实，在视觉数据集上表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的潜在空间优化方法存在选择距离度量的模糊性、采用的几何结构不合理等问题，导致出现离流形伪影、语义漂移或对抗性崩溃。

Method: 引入感知反事实测地线(PCG)，通过在由鲁棒视觉特征诱导的感知黎曼度量下追踪测地线来构建反事实。

Result: 在三个视觉数据集上的实验表明，PCG方法优于基线方法，并揭示了标准指标下隐藏的失败模式。

Conclusion: PCG方法构建的反事实具有与人类感知一致、惩罚脆弱方向等优点，能实现平滑、流形上的、语义有效的过渡。

Abstract: Latent-space optimization methods for counterfactual explanations - framed as minimal semantic perturbations that change model predictions - inherit the ambiguity of Wachter et al.'s objective: the choice of distance metric dictates whether perturbations are meaningful or adversarial. Existing approaches adopt flat or misaligned geometries, leading to off-manifold artifacts, semantic drift, or adversarial collapse. We introduce Perceptual Counterfactual Geodesics (PCG), a method that constructs counterfactuals by tracing geodesics under a perceptually Riemannian metric induced from robust vision features. This geometry aligns with human perception and penalizes brittle directions, enabling smooth, on-manifold, semantically valid transitions. Experiments on three vision datasets show that PCG outperforms baselines and reveals failure modes hidden under standard metrics.

</details>


### [285] [ART for Diffusion Sampling: A Reinforcement Learning Approach to Timestep Schedule](https://arxiv.org/abs/2601.18681)
*Yilie Huang,Wenpin Tang,Xunyu Zhou*

Main category: cs.LG

TL;DR: 引入Adaptive Reparameterized Time (ART)和其随机控制伙伴ART - RL用于基于分数的扩散模型时间离散化，提高样本生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有均匀和手工制作的网格在时间步数预算下可能不是最优的，需要更好的时间离散化方法。

Method: 引入ART控制重新参数化时间变量的时钟速度，将时间变化问题转化为连续时间强化学习问题，用高斯策略求解ART - RL并通过实际的actor - critic更新以数据驱动方式学习。

Result: 基于官方EDM管道，ART - RL在CIFAR - 10上的广泛预算范围内改善了Fréchet Inception Distance，且可迁移到AFHQv2、FFHQ和ImageNet无需重新训练。

Conclusion: ART和ART - RL为基于分数的扩散模型的时间离散化提供了有效的方法，能提升样本生成质量和迁移性。

Abstract: We consider time discretization for score-based diffusion models to generate samples from a learned reverse-time dynamic on a finite grid. Uniform and hand-crafted grids can be suboptimal given a budget on the number of time steps. We introduce Adaptive Reparameterized Time (ART) that controls the clock speed of a reparameterized time variable, leading to a time change and uneven timesteps along the sampling trajectory while preserving the terminal time. The objective is to minimize the aggregate error arising from the discretized Euler scheme. We derive a randomized control companion, ART-RL, and formulate time change as a continuous-time reinforcement learning (RL) problem with Gaussian policies. We then prove that solving ART-RL recovers the optimal ART schedule, which in turn enables practical actor--critic updates to learn the latter in a data-driven way. Empirically, based on the official EDM pipeline, ART-RL improves Fréchet Inception Distance on CIFAR-10 over a wide range of budgets and transfers to AFHQv2, FFHQ, and ImageNet without the need of retraining.

</details>


### [286] [Explainability Methods for Hardware Trojan Detection: A Systematic Comparison](https://arxiv.org/abs/2601.18696)
*Paul Whitten,Francis Wolff,Chris Papachristou*

Main category: cs.LG

TL;DR: 论文对比了门级硬件木马检测的三种可解释性方法，展示各方法优势，XGBoost分类性能优于前人工作，指出基于属性和案例的方法更具领域对齐和基于先例的可解释性。


<details>
  <summary>Details</summary>
Motivation: 硬件木马检测需要准确识别和可解释的解释，供安全工程师验证和处理结果。

Method: 对比三种可解释性方法，包括基于领域感知属性的分析、基于案例推理和模型无关的特征归因，并使用XGBoost分类。

Result: 不同方法各有优势；基于案例推理预测与训练范例的对应率达97.4%；LIME和SHAP特征归因相关性强但缺乏电路层上下文；XGBoost分类精度相比前人工作提高9倍，假阳性率降低；基于梯度归因速度比SHAP快481倍。

Conclusion: 基于属性和案例的方法比通用特征排名具有领域对齐和基于先例的可解释性，对XAI部署有意义。

Abstract: Hardware trojan detection requires accurate identification and interpretable explanations for security engineers to validate and act on results. This work compares three explainability categories for gate-level trojan detection on the Trust-Hub benchmark: (1) domain-aware property-based analysis of 31 circuit-specific features from gate fanin patterns, flip-flop distances, and I/O connectivity; (2) case-based reasoning using k-nearest neighbors for precedent-based explanations; and (3) model-agnostic feature attribution (LIME, SHAP, gradient).
  Results show different advantages per approach. Property-based analysis provides explanations through circuit concepts like "high fanin complexity near outputs indicates potential triggers." Case-based reasoning achieves 97.4% correspondence between predictions and training exemplars, offering justifications grounded in precedent. LIME and SHAP provide feature attributions with strong inter-method correlation (r=0.94, p<0.001) but lack circuit-level context for validation.
  XGBoost classification achieves 46.15% precision and 52.17% recall on 11,392 test samples, a 9-fold precision improvement over prior work (Hasegawa et al.: 5.13%) while reducing false positive rates from 5.6% to 0.25%. Gradient-based attribution runs 481 times faster than SHAP but provides similar domain-opaque insights.
  This work demonstrates that property-based and case-based approaches offer domain alignment and precedent-based interpretability compared to generic feature rankings, with implications for XAI deployment where practitioners must validate ML predictions.

</details>


### [287] [Mechanistic Analysis of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning](https://arxiv.org/abs/2601.18699)
*Olaf Yunus Laitinen Imanov*

Main category: cs.LG

TL;DR: 本文对基于Transformer的大语言模型在顺序微调时的灾难性遗忘进行机制分析，找出三个驱动遗忘的主要机制，并揭示遗忘与任务相似度等的关联，为持续学习系统开发策略奠定基础。


<details>
  <summary>Details</summary>
Motivation: 大语言模型顺序微调时会出现灾难性遗忘，但对其机制了解有限，因此进行全面的机制分析。

Method: 在多个模型规模（总参数109B到400B）和任务序列上进行系统实验。

Result: 确定三个驱动遗忘的主要机制，发现遗忘严重程度与任务相似度和梯度对齐指标强相关，约15 - 23%的注意力头在微调时受到严重破坏，低层更易受影响。

Conclusion: 研究结果为持续学习系统开发有针对性的缓解策略奠定了机制基础。

Abstract: Large language models exhibit remarkable performance across diverse tasks through pre-training and fine-tuning paradigms. However, continual fine-tuning on sequential tasks induces catastrophic forgetting, where newly acquired knowledge interferes with previously learned capabilities. Despite widespread observations of this phenomenon, the mechanistic understanding remains limited. Here, we present a comprehensive mechanistic analysis of catastrophic forgetting in transformer-based LLMs during sequential fine-tuning. Through systematic experiments across multiple model scales (109B to 400B total parameters) and task sequences, we identify three primary mechanisms driving forgetting: gradient interference in attention weights, representational drift in intermediate layers, and loss landscape flattening. We demonstrate that forgetting severity correlates strongly with task similarity (Pearson r = 0.87) and gradient alignment metrics. Our analysis reveals that approximately 15 to 23 percent of attention heads undergo severe disruption during fine-tuning, with lower layers showing greater susceptibility. These findings establish mechanistic foundations for developing targeted mitigation strategies in continual learning systems.

</details>


### [288] [From Fuzzy to Exact: The Halo Architecture for Infinite-Depth Reasoning via Rational Arithmetic](https://arxiv.org/abs/2601.18702)
*Hansheng Ren*

Main category: cs.LG

TL;DR: 文章挑战深度学习现状，提出精确性假设，引入Halo架构，实证显示其在混沌系统中优势，确立精确算术对AGI的重要性。


<details>
  <summary>Details</summary>
Motivation: 挑战当前深度学习重计算吞吐量轻数值精度的范式，解决大语言模型的“幻觉”和逻辑不一致问题。

Method: 提出精确性假设，引入基于有理数算术的Halo架构和精确推理单元（EIU）。

Result: Huginn - 0125原型实证表明，在混沌系统中600B参数规模的BF16基线崩溃，而Halo能无限期保持零数值发散。

Conclusion: 精确算术是减少系统2 AGI逻辑不确定性的先决条件。

Abstract: Current paradigms in Deep Learning prioritize computational throughput over numerical precision, relying on the assumption that intelligence emerges from statistical correlation at scale. In this paper, we challenge this orthodoxy. We propose the Exactness Hypothesis: that General Intelligence (AGI), specifically high-order causal inference, requires a computational substrate capable of Arbitrary Precision Arithmetic. We argue that the "hallucinations" and logical incoherence seen in current Large Language Models (LLMs) are artifacts of IEEE 754 floating-point approximation errors accumulating over deep compositional functions. To mitigate this, we introduce the Halo Architecture, a paradigm shift to Rational Arithmetic ($\mathbb{Q}$) supported by a novel Exact Inference Unit (EIU). Empirical validation on the Huginn-0125 prototype demonstrates that while 600B-parameter scale BF16 baselines collapse in chaotic systems, Halo maintains zero numerical divergence indefinitely. This work establishes exact arithmetic as a prerequisite for reducing logical uncertainty in System 2 AGI.

</details>


### [289] [Riemannian AmbientFlow: Towards Simultaneous Manifold Learning and Generative Modeling from Corrupted Data](https://arxiv.org/abs/2601.18728)
*Willem Diepeveen,Oscar Leong*

Main category: cs.LG

TL;DR: 提出Riemannian AmbientFlow框架，可从损坏观测中同时学习概率生成模型和数据流形，有理论保证并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 现代生成建模方法在处理干净样本时表现良好，但在很多应用中只有噪声或线性损坏的测量值，且数据中的潜在结构对下游分析很重要。

Method: 基于AmbientFlow的变分推理框架，结合归一化流诱导的数据驱动黎曼几何，通过拉回度量和黎曼自动编码器提取流形结构。

Result: 建立理论保证，在适当条件下学习的模型能以可控误差恢复数据分布，得到平滑、双李普希茨流形参数化；得到的平滑解码器可作为逆问题的生成先验。

Conclusion: 在低维合成流形和MNIST上验证了方法的有效性。

Abstract: Modern generative modeling methods have demonstrated strong performance in learning complex data distributions from clean samples. In many scientific and imaging applications, however, clean samples are unavailable, and only noisy or linearly corrupted measurements can be observed. Moreover, latent structures, such as manifold geometries, present in the data are important to extract for further downstream scientific analysis. In this work, we introduce Riemannian AmbientFlow, a framework for simultaneously learning a probabilistic generative model and the underlying, nonlinear data manifold directly from corrupted observations. Building on the variational inference framework of AmbientFlow, our approach incorporates data-driven Riemannian geometry induced by normalizing flows, enabling the extraction of manifold structure through pullback metrics and Riemannian Autoencoders. We establish theoretical guarantees showing that, under appropriate geometric regularization and measurement conditions, the learned model recovers the underlying data distribution up to a controllable error and yields a smooth, bi-Lipschitz manifold parametrization. We further show that the resulting smooth decoder can serve as a principled generative prior for inverse problems with recovery guarantees. We empirically validate our approach on low-dimensional synthetic manifolds and on MNIST.

</details>


### [290] [Self-Distilled Reasoner: On-Policy Self-Distillation for Large Language Models](https://arxiv.org/abs/2601.18734)
*Siyan Zhao,Zhihui Xie,Mengchen Liu,Jing Huang,Guan Pang,Feiyu Chen,Aditya Grover*

Main category: cs.LG

TL;DR: 提出On - Policy Self - Distillation (OPSD)框架，实现单模型自蒸馏，在数学推理基准测试中展现高效性。


<details>
  <summary>Details</summary>
Motivation: 现有策略蒸馏需单独大教师模型，且未充分利用推理数据集中的真实解，因此提出能让单模型自蒸馏的方法。

Method: 提出OPSD框架，单模型根据不同上下文分别作为教师和学生，教师策略基于特权信息，学生策略仅见问题，训练时最小化学生自身滚动输出的每令牌分布差异。

Result: 在多个数学推理基准测试中，相比GRPO等强化学习方法，实现4 - 8倍的令牌效率，性能优于离策略蒸馏方法。

Conclusion: OPSD框架有效，能在数学推理任务中提升效率和性能。

Abstract: Knowledge distillation improves large language model (LLM) reasoning by compressing the knowledge of a teacher LLM to train smaller LLMs. On-policy distillation advances this approach by having the student sample its own trajectories while a teacher LLM provides dense token-level supervision, addressing the distribution mismatch between training and inference in off-policy distillation methods. However, on-policy distillation typically requires a separate, often larger, teacher LLM and does not explicitly leverage ground-truth solutions available in reasoning datasets. Inspired by the intuition that a sufficiently capable LLM can rationalize external privileged reasoning traces and teach its weaker self (i.e., the version without access to privileged information), we introduce On-Policy Self-Distillation (OPSD), a framework where a single model acts as both teacher and student by conditioning on different contexts. The teacher policy conditions on privileged information (e.g., verified reasoning traces) while the student policy sees only the question; training minimizes the per-token divergence between these distributions over the student's own rollouts. We demonstrate the efficacy of our method on multiple mathematical reasoning benchmarks, achieving 4-8x token efficiency compared to reinforcement learning methods such as GRPO and superior performance over off-policy distillation methods.

</details>


### [291] [Benchmarking Machine Learning Models for IoT Malware Detection under Data Scarcity and Drift](https://arxiv.org/abs/2601.18736)
*Jake Lyon,Ehsan Saeedizade,Shamik Sengupta*

Main category: cs.LG

TL;DR: 研究四种监督学习模型对物联网恶意软件检测和分类的效果，发现基于树的模型表现好，但随恶意软件多样性增加性能降低。


<details>
  <summary>Details</summary>
Motivation: 物联网发展使解决安全漏洞问题更紧迫，机器学习可用于恶意软件检测，但需有效且轻量级模型。

Method: 使用IoT - 23数据集，评估四种监督学习模型（随机森林、LightGBM、逻辑回归和多层感知器）在二分类和多分类任务中的性能，评估对训练数据量的敏感性，分析时间鲁棒性。

Result: 基于树的模型即使在训练数据有限时也能实现高精度和泛化能力，但随着时间推移，恶意软件多样性增加，性能会下降。

Conclusion: 强调自适应、资源高效的机器学习模型对保障现实物联网系统安全的重要性。

Abstract: The rapid expansion of the Internet of Things (IoT) in domains such as smart cities, transportation, and industrial systems has heightened the urgency of addressing their security vulnerabilities. IoT devices often operate under limited computational resources, lack robust physical safeguards, and are deployed in heterogeneous and dynamic networks, making them prime targets for cyberattacks and malware applications. Machine learning (ML) offers a promising approach to automated malware detection and classification, but practical deployment requires models that are both effective and lightweight. The goal of this study is to investigate the effectiveness of four supervised learning models (Random Forest, LightGBM, Logistic Regression, and a Multi-Layer Perceptron) for malware detection and classification using the IoT-23 dataset. We evaluate model performance in both binary and multiclass classification tasks, assess sensitivity to training data volume, and analyze temporal robustness to simulate deployment in evolving threat landscapes. Our results show that tree-based models achieve high accuracy and generalization, even with limited training data, while performance deteriorates over time as malware diversity increases. These findings underscore the importance of adaptive, resource-efficient ML models for securing IoT systems in real-world environments.

</details>


### [292] [Trust, Don't Trust, or Flip: Robust Preference-Based Reinforcement Learning with Multi-Expert Feedback](https://arxiv.org/abs/2601.18751)
*Seyed Amir Hosseini,Maryam Abdolali,Amirhosein Tavakkoli,Fardin Ayar,Ehsan Javanmardi,Manabu Tsukada,Mahdi Javanmardi*

Main category: cs.LG

TL;DR: 提出TriTrust - PBRL (TTP) 框架，可从多专家偏好反馈学习共享奖励模型和专家信任参数，在多种场景下实现最先进的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实中偏好数据来自不同可靠性标注者，现有PBRL方法面对对抗性标注者时失效，需要新方法解决该问题。

Method: 引入TriTrust - PBRL (TTP) 统一框架，联合学习共享奖励模型和专家特定信任参数，信任参数在梯度优化中自然演变。还提供理论分析和梯度分析。

Result: 在四个不同领域的各种损坏场景下评估，TTP实现了最先进的鲁棒性，在对抗性损坏下保持接近最优的性能，标准PBRL方法则灾难性失败。

Conclusion: TTP能从包含可靠和对抗性标注者的混合专家池中学习，无需专家特征，可与现有PBRL管道无缝集成。

Abstract: Preference-based reinforcement learning (PBRL) offers a promising alternative to explicit reward engineering by learning from pairwise trajectory comparisons. However, real-world preference data often comes from heterogeneous annotators with varying reliability; some accurate, some noisy, and some systematically adversarial. Existing PBRL methods either treat all feedback equally or attempt to filter out unreliable sources, but both approaches fail when faced with adversarial annotators who systematically provide incorrect preferences. We introduce TriTrust-PBRL (TTP), a unified framework that jointly learns a shared reward model and expert-specific trust parameters from multi-expert preference feedback. The key insight is that trust parameters naturally evolve during gradient-based optimization to be positive (trust), near zero (ignore), or negative (flip), enabling the model to automatically invert adversarial preferences and recover useful signal rather than merely discarding corrupted feedback. We provide theoretical analysis establishing identifiability guarantees and detailed gradient analysis that explains how expert separation emerges naturally during training without explicit supervision. Empirically, we evaluate TTP on four diverse domains spanning manipulation tasks (MetaWorld) and locomotion (DM Control) under various corruption scenarios. TTP achieves state-of-the-art robustness, maintaining near-oracle performance under adversarial corruption while standard PBRL methods fail catastrophically. Notably, TTP outperforms existing baselines by successfully learning from mixed expert pools containing both reliable and adversarial annotators, all while requiring no expert features beyond identification indices and integrating seamlessly with existing PBRL pipelines.

</details>


### [293] [HalluGuard: Demystifying Data-Driven and Reasoning-Driven Hallucinations in LLMs](https://arxiv.org/abs/2601.18753)
*Xinyue Zeng,Junhong Lin,Yujun Yan,Feng Guo,Liang Shi,Jun Wu,Dawei Zhou*

Main category: cs.LG

TL;DR: 现有大语言模型幻觉检测方法有局限，本文提出幻觉风险边界理论框架和HalluGuard分数，在多基准测试中达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型幻觉检测方法通常只针对单一来源且依赖特定启发式方法，泛化性差，需改进。

Method: 引入幻觉风险边界理论框架，分解幻觉风险，在此基础上提出基于NTK的HalluGuard分数来联合识别两种幻觉。

Result: 在10个不同基准测试、11个竞争基线和9个流行大语言模型主干上，HalluGuard始终实现了检测多样幻觉形式的SOTA性能。

Conclusion: 所提出的理论框架和检测方法能有效检测大语言模型的多种幻觉。

Abstract: The reliability of Large Language Models (LLMs) in high-stakes domains such as healthcare, law, and scientific discovery is often compromised by hallucinations. These failures typically stem from two sources: data-driven hallucinations and reasoning-driven hallucinations. However, existing detection methods usually address only one source and rely on task-specific heuristics, limiting their generalization to complex scenarios. To overcome these limitations, we introduce the Hallucination Risk Bound, a unified theoretical framework that formally decomposes hallucination risk into data-driven and reasoning-driven components, linked respectively to training-time mismatches and inference-time instabilities. This provides a principled foundation for analyzing how hallucinations emerge and evolve. Building on this foundation, we introduce HalluGuard, an NTK-based score that leverages the induced geometry and captured representations of the NTK to jointly identify data-driven and reasoning-driven hallucinations. We evaluate HalluGuard on 10 diverse benchmarks, 11 competitive baselines, and 9 popular LLM backbones, consistently achieving state-of-the-art performance in detecting diverse forms of LLM hallucinations.

</details>


### [294] [Beyond Preferences: Learning Alignment Principles Grounded in Human Reasons and Values](https://arxiv.org/abs/2601.18760)
*Henry Bell,Lara Neubauer da Costa Schertel,Bochu Ding,Brandon Fain*

Main category: cs.LG

TL;DR: 提出GCAI框架生成AI原则宪法，生成的宪法更受人类青睐。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以公平确定让大语言模型对齐的宪法，需广泛利益相关者参与。

Method: 提出GCAI统一框架，扩展ICAI方法生成上下文原则，并补充通用原则。

Result: GCAI生成的宪法在个人和广泛治理AI行为方面更受人类青睐，且被认为更具道德基础、连贯性和多元性。

Conclusion: GCAI框架能生成更优的AI原则宪法。

Abstract: A crucial consideration when developing and deploying Large Language Models (LLMs) is the human values to which these models are aligned. In the constitutional framework of alignment models are aligned to a set of principles (the constitution) specified in natural language. However, it is unclear how to fairly determine this constitution with widespread stakeholder input. In this work we propose Grounded Constitutional AI (GCAI), a unified framework for generating constitutions of principles that are representative of both users' general expectations toward AI (general principles) and their interaction-time preferences (contextual principles). We extend the Inverse Constitutional AI (ICAI) approach to generate contextual principles from human preference annotation data by leveraging human-provided \textit{reasons} for their preferences. We supplement these contextual principles with general principles surfaced from user statements of \textit{values} regarding AI. We show that a constitution generated by GCAI is preferred by humans over one generated through ICAI both personally, and for widespread use in governing AI behavior. Additionally participants consider the GCAI constitution to be more morally grounded, coherent, and pluralistic.

</details>


### [295] [PRECISE: Reducing the Bias of LLM Evaluations Using Prediction-Powered Ranking Estimation](https://arxiv.org/abs/2601.18777)
*Abhishek Divekar,Anirban Majumder*

Main category: cs.LG

TL;DR: 提出PRECISE框架结合少量人工标注和LLM判断为搜索等系统指标提供可靠估计，减少标注需求、降低计算复杂度、校正偏差。


<details>
  <summary>Details</summary>
Motivation: 传统评估搜索等系统质量需大量人工标注，而用LLM作自动评判有固有偏差，无法直接用于指标估计。

Method: 提出扩展Prediction - Powered Inference（PPI）的统计框架PRECISE，结合少量人工标注和LLM判断，改革度量集成空间。

Result: 需要少至100个人工标注查询和10000个未标注示例；将计算复杂度从O(2^|C|)降至O(2^K)；减少Precision@K指标估计的方差，校正低资源环境中LLM偏差。

Conclusion: 所提方法能在减少标注需求的同时有效校正LLM偏差，为指标提供可靠估计。

Abstract: Evaluating the quality of search, ranking and RAG systems traditionally requires a significant number of human relevance annotations. In recent times, several deployed systems have explored the usage of Large Language Models (LLMs) as automated judges for this task while their inherent biases prevent direct use for metric estimation. We present a statistical framework extending Prediction-Powered Inference (PPI) that combines minimal human annotations with LLM judgments to produce reliable estimates of metrics which require sub-instance annotations. Our method requires as few as 100 human-annotated queries and 10,000 unlabeled examples, reducing annotation requirements significantly compared to traditional approaches. We formulate our proposed framework (PRECISE) for inference of relevance uplift for an LLM-based query reformulation application, extending PPI to sub-instance annotations at the query-document level. By reformulating the metric-integration space, we reduced the computational complexity from O(2^|C|) to O(2^K), where |C| represents corpus size (in order of millions). Detailed experiments across prominent retrieval datasets demonstrate that our method reduces the variance of estimates for the business-critical Precision@K metric, while effectively correcting for LLM bias in low-resource settings.

</details>


### [296] [Teaching Models to Teach Themselves: Reasoning at the Edge of Learnability](https://arxiv.org/abs/2601.18778)
*Shobhita Sundaram,John Quan,Ariel Kwiatkowski,Kartik Ahuja,Yann Ollivier,Julia Kempe*

Main category: cs.LG

TL;DR: 研究预训练大语言模型能否利用潜在知识为其无法解决的问题生成自动课程，设计SOAR框架并得出相关研究成果，可助力模型脱离推理瓶颈。


<details>
  <summary>Details</summary>
Motivation: 强化学习微调大推理模型在初始成功率低的数据集上停滞，探究预训练大语言模型能否利用潜在知识为其无法解决的问题生成自动课程。

Method: 设计SOAR自改进框架，通过元强化学习来挖掘教学信号，教师模型为学生模型提出合成问题，并根据学生模型在一小部分难题上的改进获得奖励。

Result: 实现了双级元强化学习，有实际效果；基于实际进展的奖励优于先前的内在奖励方案；生成问题的结构质量和适定性对学习进展比解决方案的正确性更关键。

Conclusion: 生成有用垫脚石的能力不要求模型预先具备解决难题的能力，为无需额外数据脱离推理瓶颈提供了途径。

Abstract: Can a model learn to escape its own learning plateau? Reinforcement learning methods for finetuning large reasoning models stall on datasets with low initial success rates, and thus little training signal. We investigate a fundamental question: Can a pretrained LLM leverage latent knowledge to generate an automated curriculum for problems it cannot solve? To explore this, we design SOAR: A self-improvement framework designed to surface these pedagogical signals through meta-RL. A teacher copy of the model proposes synthetic problems for a student copy, and is rewarded with its improvement on a small subset of hard problems. Critically, SOAR grounds the curriculum in measured student progress rather than intrinsic proxy rewards. Our study on the hardest subsets of mathematical benchmarks (0/128 success) reveals three core findings. First, we show that it is possible to realize bi-level meta-RL that unlocks learning under sparse, binary rewards by sharpening a latent capacity of pretrained models to generate useful stepping stones. Second, grounded rewards outperform intrinsic reward schemes used in prior LLM self-play, reliably avoiding the instability and diversity collapse modes they typically exhibit. Third, analyzing the generated questions reveals that structural quality and well-posedness are more critical for learning progress than solution correctness. Our results suggest that the ability to generate useful stepping stones does not require the preexisting ability to actually solve the hard problems, paving a principled path to escape reasoning plateaus without additional curated data.

</details>


### [297] [POPE: Learning to Reason on Hard Problems via Privileged On-Policy Exploration](https://arxiv.org/abs/2601.18779)
*Yuxiao Qu,Amrith Setlur,Virginia Smith,Ruslan Salakhutdinov,Aviral Kumar*

Main category: cs.LG

TL;DR: 现有强化学习方法在解决难题时存在探索问题，本文提出POPE方法，利用先验解决方案引导探索，提升了可解决问题的范围和推理基准性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在许多训练问题上学习效果不佳，经典解决方案无法解决探索问题，混合难易问题训练也有反效果，需要新方法。

Method: 提出Privileged On-Policy Exploration (POPE)方法，利用人类或其他先验解决方案作为特权信息，为难题添加先验解决方案前缀，引导探索。

Result: POPE扩大了可解决问题的集合，显著提高了具有挑战性的推理基准的性能。

Conclusion: POPE方法有效解决了强化学习在难题上的探索问题，提升了推理能力。

Abstract: Reinforcement learning (RL) has improved the reasoning abilities of large language models (LLMs), yet state-of-the-art methods still fail to learn on many training problems. On hard problems, on-policy RL rarely explores even a single correct rollout, yielding zero reward and no learning signal for driving improvement. We find that natural solutions to remedy this exploration problem from classical RL, such as entropy bonuses, more permissive clipping of the importance ratio, or direct optimization of pass@k objectives, do not resolve this issue and often destabilize optimization without improving solvability. A natural alternative is to leverage transfer from easier problems. However, we show that mixing easy and hard problems during RL training is counterproductive due to ray interference, where optimization focuses on already-solvable problems in a way that actively inhibits progress on harder ones. To address this challenge, we introduce Privileged On-Policy Exploration (POPE), an approach that leverages human- or other oracle solutions as privileged information to guide exploration on hard problems, unlike methods that use oracle solutions as training targets (e.g., off-policy RL methods or warmstarting from SFT). POPE augments hard problems with prefixes of oracle solutions, enabling RL to obtain non-zero rewards during guided rollouts. Crucially, the resulting behaviors transfer back to the original, unguided problems through a synergy between instruction-following and reasoning. Empirically, POPE expands the set of solvable problems and substantially improves performance on challenging reasoning benchmarks.

</details>


### [298] [Multi-Objective Reinforcement Learning for Efficient Tactical Decision Making for Trucks in Highway Traffic](https://arxiv.org/abs/2601.18783)
*Deepthi Pathare,Leo Laine,Morteza Haghir Chehreghani*

Main category: cs.LG

TL;DR: 提出基于PPO的多目标强化学习框架解决重型车辆高速驾驶决策问题，学习帕累托最优策略，结果的帕累托前沿平滑可解释，框架无需重新训练可无缝切换策略。


<details>
  <summary>Details</summary>
Motivation: 传统标量奖励公式难以体现重型车辆高速驾驶中安全、效率和运营成本之间的权衡关系，需要更好的决策方法。

Method: 提出基于Proximal Policy Optimization的多目标强化学习框架，并在可扩展仿真平台上评估。

Result: 学习到连续的帕累托最优策略，帕累托前沿平滑可解释。

Conclusion: 该框架无需重新训练可无缝切换策略，为自动驾驶卡车应用提供了稳健且自适应的决策策略。

Abstract: Balancing safety, efficiency, and operational costs in highway driving poses a challenging decision-making problem for heavy-duty vehicles. A central difficulty is that conventional scalar reward formulations, obtained by aggregating these competing objectives, often obscure the structure of their trade-offs. We present a Proximal Policy Optimization based multi-objective reinforcement learning framework that learns a continuous set of policies explicitly representing these trade-offs and evaluates it on a scalable simulation platform for tactical decision making in trucks. The proposed approach learns a continuous set of Pareto-optimal policies that capture the trade-offs among three conflicting objectives: safety, quantified in terms of collisions and successful completion; energy efficiency and time efficiency, quantified using energy cost and driver cost, respectively. The resulting Pareto frontier is smooth and interpretable, enabling flexibility in choosing driving behavior along different conflicting objectives. This framework allows seamless transitions between different driving policies without retraining, yielding a robust and adaptive decision-making strategy for autonomous trucking applications.

</details>


### [299] [Reuse your FLOPs: Scaling RL on Hard Problems by Conditioning on Very Off-Policy Prefixes](https://arxiv.org/abs/2601.18795)
*Amrith Setlur,Zijian Wang,Andrew Cohen,Paria Rashidinejad,Sang Michael Xie*

Main category: cs.LG

TL;DR: 针对LLM推理的典型强化学习方法在难题上效率低，本文提出PrefixRL方法，通过利用脱策略轨迹前缀提升学习效率，实验表明其在难题上训练更快、奖励更高，且有泛化性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 典型强化学习方法在LLM推理难题上浪费计算资源、学习停滞，标准脱策略方法在优化中不稳定。

Method: 引入PrefixRL，基于成功脱策略轨迹的前缀进行策略内强化学习，通过脱策略前缀长度调节问题难度；利用基础模型拒绝采样获取脱策略轨迹形成自提升循环。

Result: PrefixRL在难题上比最强基线快2倍达到相同训练奖励，最终奖励提高3倍，有泛化性，不同模型族的脱策略轨迹也有效。

Conclusion: PrefixRL不仅与标准强化学习目标一致，且样本效率更高，在实际应用中有灵活性。

Abstract: Typical reinforcement learning (RL) methods for LLM reasoning waste compute on hard problems, where correct on-policy traces are rare, policy gradients vanish, and learning stalls. To bootstrap more efficient RL, we consider reusing old sampling FLOPs (from prior inference or RL training) in the form of off-policy traces. Standard off-policy methods supervise against off-policy data, causing instabilities during RL optimization. We introduce PrefixRL, where we condition on the prefix of successful off-policy traces and run on-policy RL to complete them, side-stepping off-policy instabilities. PrefixRL boosts the learning signal on hard problems by modulating the difficulty of the problem through the off-policy prefix length. We prove that the PrefixRL objective is not only consistent with the standard RL objective but also more sample efficient. Empirically, we discover back-generalization: training only on prefixed problems generalizes to out-of-distribution unprefixed performance, with learned strategies often differing from those in the prefix. In our experiments, we source the off-policy traces by rejection sampling with the base model, creating a self-improvement loop. On hard reasoning problems, PrefixRL reaches the same training reward 2x faster than the strongest baseline (SFT on off-policy data then RL), even after accounting for the compute spent on the initial rejection sampling, and increases the final reward by 3x. The gains transfer to held-out benchmarks, and PrefixRL is still effective when off-policy traces are derived from a different model family, validating its flexibility in practical settings.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [300] [Constrained Multi-Objective Genetic Algorithm Variants for Design and Optimization of Tri-Band Microstrip Patch Antenna loaded CSRR for IoT Applications: A Comparative Case Study](https://arxiv.org/abs/2601.17513)
*Moahmed Hamza Boulaich,Said Ohamouddou,Mohammed Ali Ennasar,Abdelatif El Afia*

Main category: cs.NE

TL;DR: 本文采用多目标遗传算法构建天线设计优化框架，对比五种算法，用加权求和标量化方法优化微带贴片天线，实现多频段优化，性能良好。


<details>
  <summary>Details</summary>
Motivation: 研究不同进化优化方法，重点进行多频段频率优化，解决传统多目标方法难以平衡多频段的问题。

Method: 实现并对比五种多目标遗传算法变体；采用加权求和标量化方法结合单目标遗传算法框架及特定约束处理机制；将多目标聚合为统一适应度函数。

Result: 天线在2.4GHz、3.6GHz和5.2GHz的回波损耗分别为 -21.56dB、 -16.60dB、 -27.69dB，增益分别为1.96dBi、2.6dB、3.99dBi。

Conclusion: 所提方法能有效平衡三个频段，整体性能优越，优于传统多目标方法。

Abstract: This paper presents an automated antenna design and optimization framework employing multi-objective genetic algorithms (MOGAs) to investigate various evolutionary optimization approaches, with a primary emphasis on multi-band frequency optimization. Five MOGA variants were implemented and compared: the Pareto genetic algorithm (PGA), non-dominated sorting genetic algorithm with niching (NSGA-I), non-dominated sorting genetic algorithm with elitism (NSGA-II), non-dominated sorting genetic algorithm using reference points (NSGA-III), and strength Pareto evolutionary algorithm (SPEA). These algorithms are employed to design and optimize microstrip patch antennas loaded with complementary split-ring resonators (CSRRs). A weighted-sum scalarization approach was adopted within a single-objective genetic algorithm framework enhanced with domain-specific constraint handling mechanisms. The optimization addresses the conflicting objectives of minimizing the return loss ($S_{11} < -10$~dB) and achieving multi-band resonance at 2.4~GHz, 3.6~GHz, and 5.2~GHz. The proposed method delivers a superior overall performance by aggregating these objectives into a unified fitness function encompassing $S_{11}$(2.4~GHz), $S_{11}$(3.6~GHz), and $S_{11}$(5.2~GHz). This approach effectively balances all three frequency bands simultaneously, rather than exploring trade-off solutions typical of traditional multi-objective approaches. The antenna was printed on a Rogers RT5880 substrate with a dielectric constant of 2.2 , loss tangent of 0.0009 , and thickness of 1.57~mm . Scalarization approach achieved return loss values of $-21.56$~dB, $-16.60$~dB, and $-27.69$~dB, with corresponding gains of 1.96~dBi, 2.6~dB, and 3.99~dBi at 2.4~GHz, 3.6~GHz, and 5.2~GHz, respectively.

</details>


### [301] [Deep Intrinsic Surprise-Regularized Control (DISRC): A Biologically Inspired Mechanism for Efficient Deep Q-Learning in Sparse Environments](https://arxiv.org/abs/2601.17598)
*Yash Kini,Shiv Davay,Shreya Polavarapu*

Main category: cs.NE

TL;DR: 文章提出基于深度内在惊喜正则化控制（DISRC）方法改进标准DQN，在两个稀疏奖励环境中评估显示其能提升学习效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 标准DQN使用固定学习率和统一更新尺度，在稀疏奖励环境中收敛不稳定，需要改进。

Method: 引入DISRC，基于潜在空间惊喜动态缩放Q更新，通过基于LayerNorm的编码器编码状态并计算惊喜分数。

Result: 在MiniGrid-DoorKey-8x8和MiniGrid-LavaCrossingS9N1中，DISRC相比基线DQN在多个指标上表现更好。

Conclusion: DISRC是调节离策略智能体学习强度的新机制，在稀疏奖励领域改进了效率和稳定性，能提升决策质量。

Abstract: Deep reinforcement learning (DRL) has driven major advances in autonomous control. Still, standard Deep Q-Network (DQN) agents tend to rely on fixed learning rates and uniform update scaling, even as updates are modulated by temporal-difference (TD) error. This rigidity destabilizes convergence, especially in sparse-reward settings where feedback is infrequent. We introduce Deep Intrinsic Surprise-Regularized Control (DISRC), a biologically inspired augmentation to DQN that dynamically scales Q-updates based on latent-space surprise. DISRC encodes states via a LayerNorm-based encoder and computes a deviation-based surprise score relative to a moving latent setpoint. Each update is then scaled in proportion to both TD error and surprise intensity, promoting plasticity during early exploration and stability as familiarity increases. We evaluate DISRC on two sparse-reward MiniGrid environments, which included MiniGrid-DoorKey-8x8 and MiniGrid-LavaCrossingS9N1, under identical settings as a vanilla DQN baseline. In DoorKey, DISRC reached the first successful episode (reward > 0.8) 33% faster than the vanilla DQN baseline (79 vs. 118 episodes), with lower reward standard deviation (0.25 vs. 0.34) and higher reward area under the curve (AUC: 596.42 vs. 534.90). These metrics reflect faster, more consistent learning - critical for sparse, delayed reward settings. In LavaCrossing, DISRC achieved a higher final reward (0.95 vs. 0.93) and the highest AUC of all agents (957.04), though it converged more gradually. These preliminary results establish DISRC as a novel mechanism for regulating learning intensity in off-policy agents, improving both efficiency and stability in sparse-reward domains. By treating surprise as an intrinsic learning signal, DISRC enables agents to modulate updates based on expectation violations, enhancing decision quality when conventional value-based methods fall short.

</details>


### [302] [Motif Diversity in Human Liver ChIP-seq Data Using MAP-Elites](https://arxiv.org/abs/2601.17808)
*Alejandro Medina,Mary Lauren Benton*

Main category: cs.NE

TL;DR: 将基序发现问题转化为质量多样性问题，用MAP - Elites算法开展研究，实验显示该算法能恢复多个高质量基序变体并揭示多样性


<details>
  <summary>Details</summary>
Motivation: 传统基序发现任务只能从DNA序列数据集中返回单一主导基序，而调控序列数据有多种可能基序解释，需解决此问题

Method: 将基序发现视为质量多样性问题，采用MAP - Elites算法在似然性的适应度目标下进化位置权重矩阵基序，并使用三种互补行为特征进行评估

Result: 在人类CTCF肝脏ChIP - seq数据上的实验表明，MAP - Elites能恢复多个高质量基序变体，其适应度与标准工具MEME的最强解决方案相当，同时揭示了单一解决方案所掩盖的结构多样性

Conclusion: MAP - Elites算法在基序发现中能有效恢复高质量基序变体并揭示多样性，优于传统单一解决方案

Abstract: Motif discovery is a core problem in computational biology, traditionally formulated as a likelihood optimization task that returns a single dominant motif from a DNA sequence dataset. However, regulatory sequence data admit multiple plausible motif explanations, reflecting underlying biological heterogeneity. In this work, we frame motif discovery as a quality-diversity problem and apply the MAP-Elites algorithm to evolve position weight matrix motifs under a likelihood-based fitness objective while explicitly preserving diversity across biologically meaningful dimensions. We evaluate MAP-Elites using three complementary behavioral characterizations that capture trade-offs between motif specificity, compositional structure, coverage, and robustness. Experiments on human CTCF liver ChIP-seq data aligned to the human reference genome compare MAP-Elites against a standard motif discovery tool, MEME, under matched evaluation criteria across stratified dataset subsets. Results show that MAP-Elites recovers multiple high-quality motif variants with fitness comparable to MEME's strongest solutions while revealing structured diversity obscured by single-solution approaches.

</details>


### [303] [Evolving Interdependent Operators with Large Language Models for Multi-Objective Combinatorial Optimization](https://arxiv.org/abs/2601.17899)
*Junhao Qiu,Xin Chen,Liang Ge,Liyong Lin,Zhichao Lu,Qingfu Zhang*

Main category: cs.NE

TL;DR: 本文提出E2OC框架解决多目标进化算法中多算子优化问题，经实验验证性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的自动启发式设计方法主要独立优化单个启发式或组件，缺乏对多算子间动态耦合关系的探索和利用。

Method: 将多目标进化算法中的多算子优化问题建模为马尔可夫决策过程，提出E2OC框架，采用蒙特卡罗树搜索和算子旋转机制，支持集成主流AHD方法。

Result: 在不同目标和问题规模的AHD任务实验中，E2OC始终优于现有AHD和其他多启发式协同设计框架。

Conclusion: E2OC具有强大的泛化能力和持续优化能力。

Abstract: Neighborhood search operators are critical to the performance of Multi-Objective Evolutionary Algorithms (MOEAs) and rely heavily on expert design. Although recent LLM-based Automated Heuristic Design (AHD) methods have made notable progress, they primarily optimize individual heuristics or components independently, lacking explicit exploration and exploitation of dynamic coupling relationships between multiple operators. In this paper, multi-operator optimization in MOEAs is formulated as a Markov decision process, enabling the improvement of interdependent operators through sequential decision-making. To address this, we propose the Evolution of Operator Combination (E2OC) framework for MOEAs, which achieves the co-evolution of design strategies and executable codes. E2OC employs Monte Carlo Tree Search to progressively search combinations of operator design strategies and adopts an operator rotation mechanism to identify effective operator configurations while supporting the integration of mainstream AHD methods as the underlying designer. Experimental results across AHD tasks with varying objectives and problem scales show that E2OC consistently outperforms state-of-the-art AHD and other multi-heuristic co-design frameworks, demonstrating strong generalization and sustained optimization capability.

</details>


### [304] [TEFormer: Structured Bidirectional Temporal Enhancement Modeling in Spiking Transformers](https://arxiv.org/abs/2601.18274)
*Sicheng Shen,Mingyang Lv,Bing Han,Dongcheng Zhao,Guobin Shen,Feifei Zhao,Yi Zeng*

Main category: cs.NE

TL;DR: 提出Spiking Transformer框架TEFormer实现双向时间融合，实验表明其在多基准测试中表现出色，性能稳定。


<details>
  <summary>Details</summary>
Motivation: 现有Spiking Transformers缺乏有效时间融合机制，限制挖掘时空依赖的能力。

Method: 受人类视觉通路前馈 - 反馈调制启发，在注意力模块采用轻量级无超参前向时间融合机制实现全并行计算，在MLP中加入反向门控循环结构反向聚合时间信息。

Result: 在多种基准测试中，TEFormer始终显著优于SNN和Spiking Transformer基线模型，在不同神经编码方案下性能提升稳定。

Conclusion: TEFormer是Spiking Transformers中有效的通用时间建模框架。

Abstract: In recent years, Spiking Neural Networks (SNNs) have achieved remarkable progress, with Spiking Transformers emerging as a promising architecture for energy-efficient sequence modeling. However, existing Spiking Transformers still lack a principled mechanism for effective temporal fusion, limiting their ability to fully exploit spatiotemporal dependencies. Inspired by feedforward-feedback modulation in the human visual pathway, we propose TEFormer, the first Spiking Transformer framework that achieves bidirectional temporal fusion by decoupling temporal modeling across its core components. Specifically, TEFormer employs a lightweight and hyperparameter-free forward temporal fusion mechanism in the attention module, enabling fully parallel computation, while incorporating a backward gated recurrent structure in the MLP to aggregate temporal information in reverse order and reinforce temporal consistency. Extensive experiments across a wide range of benchmarks demonstrate that TEFormer consistently and significantly outperforms strong SNN and Spiking Transformer baselines under diverse datasets. Moreover, through the first systematic evaluation of Spiking Transformers under different neural encoding schemes, we show that the performance gains of TEFormer remain stable across encoding choices, indicating that the improved temporal modeling directly translates into reliable accuracy improvements across varied spiking representations. These results collectively establish TEFormer as an effective and general framework for temporal modeling in Spiking Transformers.

</details>


### [305] [Scaling Behaviors of Evolutionary Algorithms on GPUs: When Does Parallelism Pay Off?](https://arxiv.org/abs/2601.18446)
*Xinmeng Yu,Tao Jiang,Ran Cheng,Yaochu Jin,Kay Chen Tan*

Main category: cs.NE

TL;DR: 研究GPU并行对进化算法的影响，发现影响因算法结构而异，固定时间评估更佳，并找出不同缩放制度，表明GPU并行是影响进化算法评估、比较和设计的关键因素。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对GPU并行从根本上何时及为何使进化算法受益的洞察，为填补该空白开展研究。

Method: 对16种代表性进化算法在30个基准问题上进行系统实证研究，比较CPU和GPU在不同问题维度和种群规模下的执行情况。

Result: GPU加速的影响高度异质，取决于算法结构；传统基于函数评估次数的固定预算评估不适用于GPU执行；发现不同缩放制度；GPU支持的大种群能揭示CPU受限情况下难以观察的算法动态。

Conclusion: GPU并行不是实现细节，而是影响进化算法评估、比较和设计的关键因素。

Abstract: Evolutionary algorithms (EAs) are increasingly implemented on graphics processing units (GPUs) to leverage parallel processing capabilities for enhanced efficiency. However, existing studies largely emphasize the raw speedup obtained by porting individual algorithms from CPUs to GPUs. Consequently, these studies offer limited insight into when and why GPU parallelism fundamentally benefits EAs. To address this gap, we investigate how GPU parallelism alters the behavior of EAs beyond simple acceleration metrics. We conduct a systematic empirical study of 16 representative EAs on 30 benchmark problems. Specifically, we compare CPU and GPU executions across a wide range of problem dimensionalities and population sizes. Our results reveal that the impact of GPU acceleration is highly heterogeneous and depends strongly on algorithmic structure. We further demonstrate that conventional fixed-budget evaluation based on the number of function evaluations (FEs) is inadequate for GPU execution. In contrast, fixed-time evaluation uncovers performance characteristics that are unobservable under small or practically constrained FE budgets, particularly for adaptive and exploration-oriented algorithms. Moreover, we identify distinct scaling regimes in which GPU parallelism is beneficial, saturates, or degrades as problem dimensionality and population size increase. Crucially, we show that large populations enabled by GPUs not only improve hardware utilization but also reveal algorithm-specific convergence and diversity dynamics that are difficult to observe under CPU-constrained settings. Consequently, our findings indicate that GPU parallelism is not strictly an implementation detail, but a pivotal factor that influences how EAs should be evaluated, compared, and designed for modern computing platforms.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [306] [GreenServ: Energy-Efficient Context-Aware Dynamic Routing for Multi-Model LLM Inference](https://arxiv.org/abs/2601.17551)
*Thomas Ziller,Shashikant Ilager,Alessandro Tundo,Ezio Bartocci,Leonardo Mariani,Ivona Brandic*

Main category: cs.PF

TL;DR: 文章提出GreenServ框架优化大语言模型推理时准确性与能源效率的权衡，对比实验显示其优于静态和随机基线。


<details>
  <summary>Details</summary>
Motivation: 大语言模型部署受计算资源需求限制，静态推理策略效率低，需优化推理准确性和能源效率的权衡。

Method: 提出GreenServ动态上下文感知路由框架，提取查询上下文特征，路由到合适模型，用多臂老虎机方法在线学习自适应路由策略。

Result: 在五个基准任务和16个大语言模型上实验，相比随机路由，准确性提高22%，累计能耗降低31%；用RouterBench评估，平均准确率71.7%，最高75.7%。

Conclusion: GreenServ能优化大语言模型推理时的准确性和能源效率权衡，性能优于静态和随机基线。

Abstract: Large language models (LLMs) demonstrate remarkable capabilities, but their broad deployment is limited by significant computational resource demands, particularly energy consumption during inference. Static, one-model-fits-all inference strategies are often inefficient, as they do not exploit the diverse range of available models or adapt to varying query requirements.
  This paper presents GreenServ, a dynamic, context-aware routing framework that optimizes the trade-off between inference accuracy and energy efficiency. GreenServ extracts lightweight contextual features from each query, including task type, semantic cluster, and text complexity, and routes queries to the most suitable model from a heterogeneous pool, based on observed accuracy and energy usage. We employ a multi-armed bandit approach to learn adaptive routing policies online. This approach operates under partial feedback, eliminates the need for extensive offline calibration, and streamlines the integration of new models into the inference pipeline.
  We evaluated GreenServ across five benchmark tasks and a pool of 16 contemporary open-access LLMs. Experimental results show that GreenServ consistently outperforms static (single-model) and random baselines. In particular, compared to random routing, GreenServ achieved a 22% increase in accuracy while reducing cumulative energy consumption by 31%. Finally, we evaluated GreenServ with RouterBench, achieving an average accuracy of 71.7% with a peak accuracy of 75.7%. All artifacts are open-source and available as an anonymous repository for review purposes here: https://anonymous.4open.science/r/llm-inference-router-EBEA/README.md

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [307] [Risk-based test framework for LLM features in regulated software](https://arxiv.org/abs/2601.17292)
*Zhiyin Zhou*

Main category: cs.SE

TL;DR: 本文针对受监管软件中大型语言模型（LLM）功能提出基于风险的测试框架，并进行案例研究。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型嵌入受监管软件带来多种风险，而先前机器学习测试和AI保证工作对交互式、产品嵌入式助手指导有限。

Method: 提出六类风险分类法，分层测试策略将风险映射到护栏、编排和系统层的具体测试，并进行案例研究。

Result: 文中未明确提及具体结果。

Conclusion: 文中未明确提及具体结论。

Abstract: Large language models are increasingly embedded in regulated and safety-critical software, including clinical research platforms and healthcare information systems. While these features enable natural language search, summarization, and configuration assistance, they introduce risks such as hallucinations, harmful or out-of-scope advice, privacy and security issues, bias, instability under change, and adversarial misuse. Prior work on machine learning testing and AI assurance offers useful concepts but limited guidance for interactive, product-embedded assistants. This paper proposes a risk-based testing framework for LLM features in regulated software: a six-category risk taxonomy, a layered test strategy mapping risks to concrete tests across guardrail, orchestration, and system layers, and a case study applying the approach to a Knowledgebase assistant in a clinical research platform.

</details>


### [308] [YASA: Scalable Multi-Language Taint Analysis on the Unified AST at Ant Group](https://arxiv.org/abs/2601.17390)
*Yayi Wang,Shenao Wang,Jian Zhao,Shaosen Shi,Ting Li,Yan Cheng,Lizhong Bian,Kan Yu,Yanjie Zhao,Haoyu Wang*

Main category: cs.SE

TL;DR: 提出多语言静态污点分析框架YASA，在基准测试中表现优于其他工具，在实际部署中发现了未知污点路径和0-day漏洞。


<details>
  <summary>Details</summary>
Motivation: 现代企业采用多种编程语言的技术栈，现有污点分析工具难以应对多语言场景且存在局限性，无法有效用于大规模工业应用。

Method: 引入统一抽象语法树（UAST）提供跨语言兼容性抽象，在此基础上进行指向分析和污点传播，结合统一和特定于语言的语义模型。

Result: 在基准测试中，YASA在Java、JavaScript、Python和Go上均优于其他工具；在蚂蚁集团实际部署，分析超1亿行代码，发现314条未知污点路径，92个0-day漏洞。

Conclusion: YASA对保障大规模工业软件系统安全具有实际有效性。

Abstract: Modern enterprises increasingly adopt diverse technology stacks with various programming languages, posing significant challenges for static application security testing (SAST). Existing taint analysis tools are predominantly designed for single languages, requiring substantial engineering effort that scales with language diversity. While multi-language tools like CodeQL, Joern, and WALA attempt to address these challenges, they face limitations in intermediate representation design, analysis precision, and extensibility, which make them difficult to scale effectively for large-scale industrial applications at Ant Group. To bridge this gap, we present YASA (Yet Another Static Analyzer), a unified multi-language static taint analysis framework designed for industrial-scale deployment. Specifically, YASA introduces the Unified Abstract Syntax Tree (UAST) that provides a unified abstraction for compatibility across diverse programming languages. Building on the UAST, YASA performs point-to analysis and taint propagation, leveraging a unified semantic model to manage language-agnostic constructs, while incorporating language-specific semantic models to handle other unique language features. When compared to 6 single- and 2 multi-language static analyzers on an industry-standard benchmark, YASA consistently outperformed all baselines across Java, JavaScript, Python, and Go. In real-world deployment within Ant Group, YASA analyzed over 100 million lines of code across 7.3K internal applications. It identified 314 previously unknown taint paths, with 92 of them confirmed as 0-day vulnerabilities. All vulnerabilities were responsibly reported, with 76 already patched by internal development teams, demonstrating YASA's practical effectiveness for securing large-scale industrial software systems.

</details>


### [309] [Fingerprinting AI Coding Agents on GitHub](https://arxiv.org/abs/2601.17406)
*Taher A. Ghaleb*

Main category: cs.SE

TL;DR: 研究对五种主要AI编码代理的33580个PR进行分析，用41个特征实现97.2%的F1分数来识别代理，发现各代理独特指纹。


<details>
  <summary>Details</summary>
Motivation: 当开发者使用AI代理以自己账户生成代码时，代码作者归属对仓库治理、研究有效性和理解现代开发实践很关键，因此开展对AI编码代理指纹识别的研究。

Method: 分析来自五种主要AI代理的33580个PR，使用涵盖提交消息、PR结构和代码特征的41个特征进行分析。

Result: 在多类代理识别中达到97.2%的F1分数，发现Codex有独特多行提交模式，Claude Code有独特代码结构。

Conclusion: AI编码工具会产生可检测的行为模式，有潜力识别软件仓库中的AI贡献。

Abstract: AI coding agents are reshaping software development through both autonomous and human-mediated pull requests (PRs). When developers use AI agents to generate code under their own accounts, code authorship attribution becomes critical for repository governance, research validity, and understanding modern development practices. We present the first study on fingerprinting AI coding agents, analyzing 33,580 PRs from five major agents (OpenAI Codex, GitHub Copilot, Devin, Cursor, Claude Code) to identify behavioral signatures. With 41 features spanning commit messages, PR structure, and code characteristics, we achieve 97.2% F1-score in multi-class agent identification. We uncover distinct fingerprints: Codex shows unique multiline commit patterns (67.5% feature importance), and Claude Code exhibits distinctive code structure (27.2% importance of conditional statements). These signatures reveal that AI coding tools produce detectable behavioral patterns, suggesting potential for identifying AI contributions in software repositories.

</details>


### [310] [When AI Agents Touch CI/CD Configurations: Frequency and Success](https://arxiv.org/abs/2601.17413)
*Taher A. Ghaleb*

Main category: cs.SE

TL;DR: 分析AI智能体在软件开发中对CI/CD配置的修改情况，指出其很少修改CI/CD且多针对GitHub Actions，配置更改与普通代码一样可靠。


<details>
  <summary>Details</summary>
Motivation: 当前AI智能体在软件开发中使用增多，但与CI/CD配置的交互研究不足，需分析其修改CI/CD配置的情况。

Method: 分析来自1605个GitHub仓库的8031个智能体拉取请求，涉及AI智能体对YAML配置的修改，并统计相关数据。

Result: CI/CD配置文件占智能体更改的3.25%；96.77%针对GitHub Actions；有CI/CD更改的智能体拉取请求合并率略低（Copilot除外）；CI/CD和非CI/CD更改的构建成功率相近；三个智能体修改CI/CD时成功率显著更高。

Conclusion: AI智能体很少修改CI/CD且多针对GitHub Actions，其配置更改可靠；Copilot表现暗示配置专业化，对智能体训练和DevOps自动化有启示。

Abstract: AI agents are increasingly used in software development, yet their interaction with CI/CD configurations is not well studied. We analyze 8,031 agentic pull requests (PRs) from 1,605 GitHub repositories where AI agents touch YAML configurations. CI/CD configuration files account for 3.25% of agent changes, varying by agent (Devin: 4.83%, Codex: 2.01%, p < 0.001). When agents modify CI/CD, 96.77% target GitHub Actions. Agentic PRs with CI/CD changes merge slightly less often than others (67.77% vs. 71.80%), except for Copilot, whose CI/CD changes merge 15.63 percentage points more often. Across 99,930 workflow runs, build success rates are comparable for CI/CD and non-CI/CD changes (75.59% vs. 74.87%), though three agents show significantly higher success when modifying CI/CD. These results show that AI agents rarely modify CI/CD and focus mostly on GitHub Actions, yet their configuration changes are as reliable as regular code. Copilot's strong CI/CD performance despite lower acceptance suggests emerging configuration specialization, with implications for agent training and DevOps automation.

</details>


### [311] [Towards a Declarative Agentic Layer for Intelligent Agents in MCP-Based Server Ecosystems](https://arxiv.org/abs/2601.17435)
*Maria Jesus Rodriguez-Sanchez,Manuel Noguera,Angel Ruiz-Zafra,Kawtar Benghazi*

Main category: cs.SE

TL;DR: 现有LLM驱动的智能体系统有可靠性问题，本文提出DALIA架构层解决，能实现可复现和可验证的工作流。


<details>
  <summary>Details</summary>
Motivation: 当前LLM驱动的智能体和多智能体系统存在可靠性问题，根源是缺乏连接目标、能力和执行的架构。

Method: 提出DALIA架构层，形式化可执行能力，通过声明式发现协议暴露任务，维护智能体及其资源目录，构建基于声明操作的任务图。

Result: 通过代表性任务场景展示了DALIA的运作。

Conclusion: 声明式架构可在异构环境实现可复现和可验证的智能体工作流。

Abstract: Recent advances in Large Language Models (LLMs) have enabled the development of increasingly complex agentic and multi-agent systems capable of planning, tool use and task decomposition. However, empirical evidence shows that many of these systems suffer from fundamental reliability issues, including hallucinated actions, unexecutable plans and brittle coordination. Crucially, these failures do not stem from limitations of the underlying models themselves, but from the absence of explicit architectural structure linking goals, capabilities and execution. This paper presents a declarative, model-independent architectural layer for grounded agentic workflows that addresses this gap. The proposed layer, referred to as DALIA (Declarative Agentic Layer for Intelligent Agents), formalises executable capabilities, exposes tasks through a declarative discovery protocol, maintains a federated directory of agents and their execution resources, and constructs deterministic task graphs grounded exclusively in declared operations. By enforcing a clear separation between discovery, planning and execution, the architecture constrains agent behaviour to a verifiable operational space, reducing reliance on speculative reasoning and free-form coordination. We present the architecture and design principles of the proposed layer and illustrate its operation through a representative task-oriented scenario, demonstrating how declarative grounding enables reproducible and verifiable agentic workflows across heterogeneous environments.

</details>


### [312] [Data-driven Test Generation for Fuzzing AI Compiler](https://arxiv.org/abs/2601.17450)
*Qingchao Shen*

Main category: cs.SE

TL;DR: 提出统一数据驱动测试框架，解决AI编译器各阶段挑战，检测出266个未知漏洞


<details>
  <summary>Details</summary>
Motivation: AI编译器易出现影响可靠性和模型正确性的漏洞，需保证其质量

Method: OPERA迁移AI库测试用于模型加载阶段算子转换逻辑测试；OATest合成优化感知计算图测试高级优化；HARMONY生成和变异低级IR种子测试低级优化

Result: 检测出四个常用AI编译器中266个先前未知的漏洞

Conclusion: 所提框架全面、分阶段，提高了测试覆盖率和有效性

Abstract: Artificial Intelligence (AI) compilers are critical for efficiently deploying AI models across diverse hardware platforms. However, they remain prone to bugs that can compromise both compiler reliability and model correctness. Thus, ensuring the quality of AI compilers is crucial. In this work, we present a unified data-driven testing framework that systematically addresses stage-specific challenges in AI compilers. Specifically, OPERA migrates tests for AI libraries to test various operator conversion logic in the model loading stage. OATest synthesizes diverse optimization-aware computational graphs for testing high-level optimizations. HARMONY generates and mutates diverse low-level IR seeds to generate hardware-optimization-aware tests for testing low-level optimizations. Together, these techniques provide a comprehensive, stage-aware framework that enhances testing coverage and effectiveness, detecting 266 previously unknown bugs in four widely used AI compilers.

</details>


### [313] [LogPrism: Unifying Structure and Variable Encoding for Effective Log Compression](https://arxiv.org/abs/2601.17482)
*Yang Liu,Kaiming Zhang,Zhuangbin Chen,Jinyang Liu,Zibin Zheng*

Main category: cs.SE

TL;DR: 现有日志压缩范式存在问题，本文提出LogPrism框架，实验表明其达到新的最优水平。


<details>
  <summary>Details</summary>
Motivation: 现有“解析 - 压缩”范式将日志解析和压缩视为孤立目标，限制了压缩效果，需解决静态模板和动态变量间相关性挖掘不足的问题。

Method: 提出LogPrism框架，通过构建统一冗余树（URT）动态集成结构提取和变量编码，挖掘“结构 + 变量”共现模式。

Result: 在16个基准数据集上实验，LogPrism在13个数据集上达到最高压缩比，超越领先基线4.7% - 80.9%，吞吐量达29.87 MB/s；单存档模式下压缩比超最佳基线19.39%，速度快2.62倍。

Conclusion: LogPrism框架有效，建立了日志压缩的新最优水平。

Abstract: The prevailing "parse-then-compress" paradigm in log compression fundamentally limits effectiveness by treating log parsing and compression as isolated objectives. While parsers prioritize semantic accuracy (i.e., event identification), they often obscure deep correlations between static templates and dynamic variables that are critical for storage efficiency. In this paper, we investigate this misalignment through a comprehensive empirical study and propose LogPrism, a framework that bridges the gap via unified redundancy encoding. Rather than relying on a rigid pre-parsing step, LogPrism dynamically integrates structural extraction with variable encoding by constructing a Unified Redundancy Tree (URT). This hierarchical approach effectively mines "structure+variable" co-occurrence patterns, capturing deep contextual redundancies while accelerating processing through pre-emptive pattern encoding. Extensive experiments on 16 benchmark datasets confirm that LogPrism establishes a new state-of-the-art. It achieves the highest compression ratio on 13 datasets, surpassing leading baselines by margins of 4.7% to 80.9%, while delivering superior throughput at 29.87 MB/s (1.68$\times$~43.04$\times$ faster than competitors). Moreover, when configured in single-archive mode to maximize global pattern discovery, LogPrism outperforms the best baseline by 19.39% in compression ratio while maintaining a 2.62$\times$ speed advantage.

</details>


### [314] [Measuring Braking Behavior Using Vehicle Tracking and Camera-to-Satellite Homography Rectification](https://arxiv.org/abs/2601.17558)
*J. P. Fleischer,Tanchanok Sirikanchittavon,Chonlachart Jeenprasom,Nooshin Yousefzadeh,Sanjay Ranka,Mohammed Hadi*

Main category: cs.SE

TL;DR: 本文介绍用于分析交通摄像头录像的开源软件，基于地面单应性估计，无需相机校准获取车辆特征，经案例验证有支持车联网、交通管理等潜力。


<details>
  <summary>Details</summary>
Motivation: 开发可分析交通摄像头录像、获取车辆行为和制动事件信息的系统，支持交通管理和安全分析。

Method: 采用MAGSAC++估计器构建单应性，将YOLO11目标检测转换为校正的顶视坐标系，数据存于ClickHouse数据库。

Result: 通过佛罗里达基韦斯特两个信号交叉口案例，得出不同时段制动活动高峰及制动事件空间分布。

Conclusion: 该集中安全信息系统在支持车联网、主动交通管理、事故缓解和道路设计等方面有显著潜力。

Abstract: This paper presents an open-source software application for analyzing traffic camera footage, focusing on vehicle behavior and braking events at signalized urban highways. The core innovation is a robust ground-plane homography estimation that links fixed traffic camera views to satellite orthoimagery. This process rectifies the camera's oblique perspective, ensuring that pixel distances accurately represent real-world distances. This enables the acquisition of features such as vehicle trajectory, speed, deceleration, and braking severity without the need for camera calibration. The pipeline employs the MAGSAC++ estimator to build the homography, converting YOLO11 object detections into a rectified top-down coordinate system. All detection and trajectory data are stored in a ClickHouse database for subsequent analysis. A real-world case study at two signalized intersections in Key West, Florida, showcased the system's capabilities. Across two days of daytime footage, braking activity at the higher-volume intersection peaked around 4 PM at approximately 57.5 events per hour, while the second intersection peaked around 10 AM at roughly 15.5 events per hour. The spatial analysis revealed that most braking events initiated upstream, with mild and moderate braking mostly occurring 30 to 45+ meters away from the stop bar and severe braking distributed throughout, but particularly concentrated in lanes with higher interaction and merging activity. The findings highlight the significant potential of this centralized safety information system to support connected vehicles, facilitating proactive traffic management, crash mitigation, and data-driven roadway design and safety analysis.

</details>


### [315] [How AI Coding Agents Modify Code: A Large-Scale Study of GitHub Pull Requests](https://arxiv.org/abs/2601.17581)
*Daniel Ogenrwot,John Businge*

Main category: cs.SE

TL;DR: 通过分析数据集对比AI与人类代码贡献，揭示差异与特征。


<details>
  <summary>Details</summary>
Motivation: 缺少AI代码代理生成的PR和人类贡献的对比实证，需了解差异以评估其可靠性和对开发流程的影响。

Method: 使用MSR 2026 Mining Challenge版本的AIDev数据集，分析24,014个AI PR和5,081个人类PR，检查内容并评估描述与差异的一致性。

Result: AI PR在提交次数上与人类PR有显著差异，在文件触及和删除行数上有中等差异，描述与差异的相似度略高。

Conclusion: 研究对AI编码代理如何参与开源开发提供了大规模实证描述。

Abstract: AI coding agents are increasingly acting as autonomous contributors by generating and submitting pull requests (PRs). However, we lack empirical evidence on how these agent-generated PRs differ from human contributions, particularly in how they modify code and describe their changes. Understanding these differences is essential for assessing their reliability and impact on development workflows. Using the MSR 2026 Mining Challenge version of the AIDev dataset, we analyze 24,014 merged Agentic PRs (440,295 commits) and 5,081 merged Human PRs (23,242 commits). We examine additions, deletions, commits, and files touched, and evaluate the consistency between PR descriptions and their diffs using lexical and semantic similarity. Agentic PRs differ substantially from Human PRs in commit count (Cliff's $δ= 0.5429$) and show moderate differences in files touched and deleted lines. They also exhibit slightly higher description-to-diff similarity across all measures. These findings provide a large-scale empirical characterization of how AI coding agents contribute to open source development.

</details>


### [316] [Prompt Driven Development with Claude Code: Building a Complete TUI Framework for the Ring Programming Language](https://arxiv.org/abs/2601.17584)
*Mahmoud Samir Fayed,Ahmed Samir Fayed*

Main category: cs.SE

TL;DR: 研究通过Claude Code、Opus 4.5以纯提示驱动工作流开发7420行的Ring语言终端用户界面框架，展示大语言模型可支持新兴编程语言生产级工具构建。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在软件开发中应用增多，但通过自然语言交互生成和维护大型多模块系统的能力特征尚不明确。

Method: 采用纯提示驱动工作流，用Claude Code、Opus 4.5，分五个阶段，通过107个提示开发框架。

Result: 开发出包含完整窗口子系统等的框架，提示多为短期迭代，人工仅需指定需求、验证行为和纠正提示。

Conclusion: 现代大语言模型能保持架构连贯性，支持新兴编程语言生产级工具构建，提示驱动开发是软件工程可行方法。

Abstract: Large language models are increasingly used in software development, yet their ability to generate and maintain large, multi module systems through natural language interaction remains insufficiently characterized. This study presents an empirical analysis of developing a 7420 line Terminal User Interface framework for the Ring programming language, completed in roughly ten hours of active work spread across three days using a purely prompt driven workflow with Claude Code, Opus 4.5. The system was produced through 107 prompts: 21 feature requests, 72 bug fix prompts, 9 prompts sharing information from Ring documentation, 4 prompts providing architectural guidance, and 1 prompt dedicated to generating documentation. Development progressed across five phases, with the Window Manager phase requiring the most interaction, followed by complex UI systems and controls expansion. Bug related prompts covered redraw issues, event handling faults, runtime errors, and layout inconsistencies, while feature requests focused primarily on new widgets, window manager capabilities, and advanced UI components. Most prompts were short, reflecting a highly iterative workflow in which the human role was limited to specifying requirements, validating behaviour, and issuing corrective prompts without writing any code manually. The resulting framework includes a complete windowing subsystem, event driven architecture, interactive widgets, hierarchical menus, grid and tree components, tab controls, and a multi window desktop environment. By combining quantitative prompt analysis with qualitative assessment of model behaviour, this study provides empirical evidence that modern LLMs can sustain architectural coherence and support the construction of production grade tooling for emerging programming languages, highlighting prompt driven development as a viable methodology within software engineering practice.

</details>


### [317] [Human-Aligned Enhancement of Programming Answers with LLMs Guided by User Feedback](https://arxiv.org/abs/2601.17604)
*Suborno Deb Bappon,Saikat Mondal,Chanchal K. Roy,Kevin Schneider*

Main category: cs.SE

TL;DR: 研究LLMs能否通过解释和整合基于评论的反馈来改进编程答案，提出ReSOlve基准，评估LLMs，推出AUTOCOMBAT工具，用户研究显示其有实用价值。


<details>
  <summary>Details</summary>
Motivation: LLMs在以类人方式改进现有编程答案的能力未被充分探索，SO上约三分之一反馈未被处理，答案不完整或过时。

Method: 引入ReSOlve基准；评估四个先进LLMs识别可操作问题的能力；推出AUTOCOMBAT工具；开展用户研究。

Result: DeepSeek在识别可操作问题上精度和召回率平衡最好；AUTOCOMBAT改进接近人类水平，显著优于基线；用户研究显示84.5%从业者愿采用或推荐。

Conclusion: AUTOCOMBAT展示了可扩展、反馈驱动的答案改进对提高技术知识平台可靠性和可信度的潜力。

Abstract: Large Language Models (LLMs) are widely used to support software developers in tasks such as code generation, optimization, and documentation. However, their ability to improve existing programming answers in a human-like manner remains underexplored. On technical question-and-answer platforms such as Stack Overflow (SO), contributors often revise answers based on user comments that identify errors, inefficiencies, or missing explanations. Yet roughly one-third of this feedback is never addressed due to limited time, expertise, or visibility, leaving many answers incomplete or outdated. This study investigates whether LLMs can enhance programming answers by interpreting and incorporating comment-based feedback. We make four main contributions. First, we introduce ReSOlve, a benchmark consisting of 790 SO answers with associated comment threads, annotated for improvement-related and general feedback. Second, we evaluate four state-of-the-art LLMs on their ability to identify actionable concerns, finding that DeepSeek achieves the best balance between precision and recall. Third, we present AUTOCOMBAT, an LLM-powered tool that improves programming answers by jointly leveraging user comments and question context. Compared to human revised references, AUTOCOMBAT produces near-human quality improvements while preserving the original intent and significantly outperforming the baseline. Finally, a user study with 58 practitioners shows strong practical value, with 84.5 percent indicating they would adopt or recommend the tool. Overall, AUTOCOMBAT demonstrates the potential of scalable, feedback-driven answer refinement to improve the reliability and trustworthiness of technical knowledge platforms.

</details>


### [318] [Code Change Characteristics and Description Alignment: A Comparative Study of Agentic versus Human Pull Requests](https://arxiv.org/abs/2601.17627)
*Dung Pham,Taher A. Ghaleb*

Main category: cs.SE

TL;DR: 分析33,596个AI生成PR和6,618个人类PR，对比代码变更特征和消息质量，发现AI在微观精确和宏观沟通上有差距。


<details>
  <summary>Details</summary>
Motivation: 了解AI编码代理贡献与人类贡献的差异。

Method: 分析AI生成的33,596个PR和人类的6,618个PR，对比代码变更特征和消息质量。

Result: AI引入的符号更快、更多被移除；AI在提交级消息更强，但在PR级总结落后；提交消息长度是描述质量最佳预测指标。

Conclusion: AI在微观精确和宏观沟通存在差距，有改进代理驱动开发工作流的机会。

Abstract: AI coding agents can autonomously generate pull requests (PRs), yet little is known about how their contributions compare to those of humans. We analyze 33,596 agent-generated PRs (APRs) and 6,618 human PRs (HPRs) to compare code-change characteristics and message quality. We observe that APR-introduced symbols (functions and classes) are removed much sooner than those in HPRs (median time to removal 3 vs. 34 days) and are also removed more often (symbol churn 7.33% vs. 4.10%), reflecting a focus on other tasks like documentation and test updates. Agents generate stronger commit-level messages (semantic similarity 0.72 vs. 0.68) but lag humans at PR-level summarization (PR-commit similarity 0.86 vs. 0.88). Commit message length is the best predictor of description quality, indicating reliance on individual commits over full-PR reasoning. These findings highlight a gap between agents' micro-level precision and macro-level communication, suggesting opportunities to improve agent-driven development workflows.

</details>


### [319] [Multi-Agent End-to-End Vulnerability Management for Mitigating Recurring Vulnerabilities](https://arxiv.org/abs/2601.17762)
*Zelong Zheng,Jiayuan Zhou,Xing Hu,Yi Gao,Shengyi Pan*

Main category: cs.SE

TL;DR: 现有软件漏洞管理自动化方法不足，本文提出多智能体框架MAVM，能利用历史知识、克服上下文限制，实验显示其修复准确率优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有自动化方法难以精确捕捉上下文依赖，大语言模型缺乏上下文信息处理能力，且以往方法未充分利用历史漏洞知识。

Method: 提出MAVM多智能体框架，将五个组件集成到统一管道，构建漏洞知识库，设计上下文检索工具。

Result: 在含78个真实补丁移植案例的数据集上，MAVM成功检测和修复51个真实漏洞，修复准确率比基线高31.9%-45.2%。

Conclusion: MAVM在软件漏洞管理中有效，能克服现有方法的不足。

Abstract: Software vulnerability management has become increasingly critical as modern systems scale in size and complexity. However, existing automated approaches remain insufficient. Traditional static analysis methods struggle to precisely capture contextual dependencies, especially when vulnerabilities span multiple functions or modules. Large language models (LLMs) often lack the ability to retrieve and exploit sufficient contextual information, resulting in incomplete reasoning and unreliable outcomes. Meanwhile, recurring vulnerabilities emerge repeatedly due to code reuse and shared logic, making historical vulnerability knowledge an indispensable foundation for effective vulnerability detection and repair. Nevertheless, prior approaches such as clone-based detection and patch porting, have not fully leveraged this knowledge. To address these challenges, we present MAVM, a multi-agent framework for end-to-end recurring vulnerability management. MAVM integrates five components, including a vulnerability knowledge base, detection, confirmation, repair, and validation, into a unified multi-agent pipeline. We construct a knowledge base from publicly disclosed vulnerabilities, thereby addressing the underuse of historical knowledge in prior work and mitigating the lack of domain-specific expertise in LLMs. Furthermore, we design context-retrieval tools that allow agents to extract and reason over repository-level information, overcoming the contextual limitations of previous methods. Based on agents, MAVM effectively simulates real-world security workflows. To evaluate the performance of MAVM, we construct a dataset containing 78 real-world patch-porting cases (covering 114 function-level migrations). On this dataset, MAVM successfully detects and repairs 51 real vulnerabilities, outperforming baselines by 31.9%-45.2% in repair accuracy, which demonstrates its effectiveness.

</details>


### [320] [iResolveX: Multi-Layered Indirect Call Resolution via Static Reasoning and Learning-Augmented Refinement](https://arxiv.org/abs/2601.17888)
*Monika Santra,Bokai Zhang,Mark Lim,Vishnu Asutosh Dasu,Dongrui Zeng,Gang Tan*

Main category: cs.SE

TL;DR: 提出混合多层框架iResolveX解决间接调用解析问题，结合静态分析和基于学习的细化，在减少误报同时保持高召回率，性能优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 间接调用解析在逆向工程和控制流图恢复中是关键挑战，静态分析有误报，机器学习方法有完整性和泛化问题。

Method: 构建iResolveX框架，第一层用保守值集分析保证召回率，第二层用基于学习的软签名评分器和选择性过程间逆向分析减少误报，最终输出带置信度分数的p - IndirectCFG。

Result: iScoreGen平均减少19.2%预测目标且保持98.2%召回率，结合iScoreRefine总减少44.3%，召回率97.8%。

Conclusion: iResolveX支持不同配置，性能优于现有系统。

Abstract: Indirect call resolution remains a key challenge in reverse engineering and control-flow graph recovery, especially for stripped or optimized binaries. Static analysis is sound but often over-approximates, producing many false positives, whereas machine-learning approaches can improve precision but may sacrifice completeness and generalization. We present iResolveX, a hybrid multi-layered framework that combines conservative static analysis with learning-based refinement. The first layer applies a conservative value-set analysis (BPA) to ensure high recall. The second layer adds a learning-based soft-signature scorer (iScoreGen) and selective inter-procedural backward analysis with memory inspection (iScoreRefine) to reduce false positives. The final output, p-IndirectCFG, annotates indirect edges with confidence scores, enabling downstream analyses to choose appropriate precision--recall trade-offs. Across SPEC CPU2006 and real-world binaries, iScoreGen reduces predicted targets by 19.2% on average while maintaining BPA-level recall (98.2%). Combined with iScoreRefine, the total reduction reaches 44.3% over BPA with 97.8% recall (a 0.4% drop). iResolveX supports both conservative, recall-preserving and F1-optimized configurations and outperforms state-of-the-art systems.

</details>


### [321] [Prompt-Based REST API Test Amplification in Industry: An Experience Report](https://arxiv.org/abs/2601.17903)
*Tolgahan Bardakci,Andreas Faes,Mutlu Beyazit,Serge Demeyr*

Main category: cs.SE

TL;DR: 在比利时一家大型物流公司工业环境中复制基于LLM的REST API测试放大工作，结果显示该方法有用。


<details>
  <summary>Details</summary>
Motivation: 现有证据缺乏LLM在工业场景REST API测试的有效性，为填补此空白开展研究。

Method: 在比利时一家大型物流公司的工业环境中，对生产微服务的六个代表性端点应用基于LLM的测试放大。

Result: 基于LLM的测试放大增加了覆盖率，揭示了各种观察结果和异常情况。

Conclusion: 基于LLM的测试放大在工业场景中具有实际用处。

Abstract: Large Language Models (LLMs) are increasingly used to support software testing tasks, yet there is little evidence of their effectiveness for REST API testing in industrial settings. To address this gap, we replicate our earlier work on LLM-based REST API test amplification within an industrial context at one of the largest logistics companies in Belgium. We apply LLM-based test amplification to six representative endpoints of a production microservice embedded in a large-scale, security-sensitive system, where there is in-depth complexity in authentication, stateful behavior, and organizational constraints. Our experience shows that LLM-based test amplification remains practically useful in industry by increasing coverage and revealing various observations and anomalies.

</details>


### [322] [RGFL: Reasoning Guided Fault Localization for Automated Program Repair Using Large Language Models](https://arxiv.org/abs/2601.18044)
*Melika Sepidband,Hamed Taherkhani,Hung Viet Pham,Hadi Hemmati*

Main category: cs.SE

TL;DR: 提出一种项目级故障定位（FL）方法，结合分层推理模块与两阶段排序方案，提升文件和元素级定位准确性，在Python和Java项目评估中优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 在现实项目级修复场景中，软件仓库代码量远超大语言模型（LLM）上下文限制，准确的FL对有效修复至关重要。

Method: 引入分层推理模块生成结构化、特定于bug的解释，采用结合LLM和基于嵌入信号的两阶段排序方案，提出反事实上限分析量化各定位阶段对修复成功的贡献。

Result: 在SWE - bench的Python和Java项目评估中，相比现有基线，文件级Hit@1从71.4%提升到85%，MRR从81.8%提升到88.8%；元素级前3文件下精确匹配从36%提升到69%；集成到Agentless中使端到端修复成功率提升12.8%。

Conclusion: 所提项目级FL方法能有效提高文件和元素级定位准确性，进而提升修复成功率。

Abstract: Fault Localization (FL) is a critical step in Automated Program Repair (APR), and its importance has increased with the rise of Large Language Model (LLM)-based repair agents. In realistic project-level repair scenarios, software repositories often span millions of tokens, far exceeding current LLM context limits. Consequently, models must first identify a small, relevant subset of code, making accurate FL essential for effective repair. We present a novel project-level FL approach that improves both file- and element-level localization. Our method introduces a hierarchical reasoning module that (i) generates structured, bug-specific explanations for candidate files and elements, and (ii) leverages these explanations in a two-stage ranking scheme combining LLM-based and embedding-based signals. We further propose a counterfactual upper-bound analysis to quantify the contribution of each localization stage to repair success. We evaluate our approach on Python and Java projects from SWE-bench Verified, Lite, and Java. Compared to state-of-the-art baselines, including Agentless and OpenHands, our method consistently improves localization accuracy. On SWE-bench Verified, file-level Hit@1 improves from 71.4% to 85%, and MRR from 81.8% to 88.8%. At the element level, Exact Match under top-3 files increases from 36% to 69%. Integrating our localization into Agentless yields a 12.8% end-to-end repair success improvement.

</details>


### [323] [TAM-Eval: Evaluating LLMs for Automated Unit Test Maintenance](https://arxiv.org/abs/2601.18241)
*Elena Bruches,Vadim Alperovich,Dari Baturova,Roman Derunets,Daniil Grebenkin,Georgy Mkrtchyan,Oleg Sedukhin,Mikhail Klementev,Ivan Bondarenko,Nikolay Bushkov,Stanislav Moiseev*

Main category: cs.SE

TL;DR: 介绍TAM - Eval框架和基准，用于评估大语言模型在测试套件维护场景中的性能，结果显示现有模型能力有限并开源框架。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在软件工程单元测试应用局限于孤立测试生成或预言机预测，忽视测试套件维护挑战。

Method: 引入TAM - Eval框架和基准，在测试文件级别操作，结合全仓库上下文，用无参考协议评估，基准含1539个场景。

Result: 现有大语言模型在实际测试维护过程中能力有限，测试有效性提升微弱。

Conclusion: 发布TAM - Eval作为开源框架支持自动化软件测试未来研究。

Abstract: While Large Language Models (LLMs) have shown promise in software engineering, their application to unit testing remains largely confined to isolated test generation or oracle prediction, neglecting the broader challenge of test suite maintenance. We introduce TAM-Eval (Test Automated Maintenance Evaluation), a framework and benchmark designed to evaluate model performance across three core test maintenance scenarios: creation, repair, and updating of test suites. Unlike prior work limited to function-level tasks, TAM-Eval operates at the test file level, while maintaining access to full repository context during isolated evaluation, better reflecting real-world maintenance workflows. Our benchmark comprises 1,539 automatically extracted and validated scenarios from Python, Java, and Go projects. TAM-Eval supports system-agnostic evaluation of both raw LLMs and agentic workflows, using a reference-free protocol based on test suite pass rate, code coverage, and mutation testing. Empirical results indicate that state-of-the-art LLMs have limited capabilities in realistic test maintenance processes and yield only marginal improvements in test effectiveness. We release TAM-Eval as an open-source framework to support future research in automated software testing. Our data and code are publicly available at https://github.com/trndcenter/TAM-Eval.

</details>


### [324] [Agentic Much? Adoption of Coding Agents on GitHub](https://arxiv.org/abs/2601.18341)
*Romain Robbes,Théo Matricon,Thomas Degueule,Andre Hora,Stefano Zacchiroli*

Main category: cs.SE

TL;DR: 本研究对Github上编码智能体的采用情况进行大规模研究，发现其采用率高且不断增加，应用广泛，相关提交规模更大且含较多特性与修复。


<details>
  <summary>Details</summary>
Motivation: 编码智能体以高自主性操作，有望比代码补全LLMs更深刻改变行业格局，且会留下更明显痕迹，因此需要研究其影响。

Method: 利用编码智能体在软件工程工件中留下的痕迹，对Github上129,134个项目开展大规模研究，并对识别出的采用者进行深入研究。

Result: 编码智能体采用率估计在15.85% - 22.60%且不断增加，采用范围广泛，编码智能体辅助的提交比人类开发者单独编写的提交规模更大，且包含大量特性和错误修复。

Conclusion: 有必要进一步深入研究编码智能体的实际应用。

Abstract: In the first half of 2025, coding agents have emerged as a category of development tools that have very quickly transitioned to the practice. Unlike ''traditional'' code completion LLMs such as Copilot, agents like Cursor, Claude Code, or Codex operate with high degrees of autonomy, up to generating complete pull requests starting from a developer-provided task description. This new mode of operation is poised to change the landscape in an even larger way than code completion LLMs did, making the need to study their impact critical. Also, unlike traditional LLMs, coding agents tend to leave more explicit traces in software engineering artifacts, such as co-authoring commits or pull requests. We leverage these traces to present the first large-scale study (129,134 projects) of the adoption of coding agents on GitHub, finding an estimated adoption rate of 15.85%--22.60%, which is very high for a technology only a few months old--and increasing. We carry out an in-depth study of the adopters we identified, finding that adoption is broad: it spans the entire spectrum of project maturity; it includes established organizations; and it concerns diverse programming languages or project topics. At the commit level, we find that commits assisted by coding agents are larger than commits only authored by human developers, and have a large proportion of features and bug fixes. These findings highlight the need for further investigation into the practical use of coding agents.

</details>


### [325] [Forecasting the Maintained Score from the OpenSSF Scorecard for GitHub Repositories linked to PyPI libraries](https://arxiv.org/abs/2601.18344)
*Alexandros Tsakpinis,Efe Berk Ergülec,Emil Schwenger,Alexander Pretschner*

Main category: cs.SE

TL;DR: 文章研究OpenSSF Maintained分数对未来维护活动的预测能力，分析大量GitHub仓库，对比多种模型，发现能准确预测，简单模型与深度学习模型表现相当，可为开源维护风险主动评估提供补充。


<details>
  <summary>Details</summary>
Motivation: 当前OpenSSF Scorecard的Maintained指标具有回顾性，缺乏对未来维护的洞察，限制了主动风险评估能力，因此研究其对未来维护活动的预测程度。

Method: 分析3220个GitHub仓库，重构三年的历史Maintained分数，将任务转化为多变量时间序列预测，考虑四种目标表示，对比VARMA、Random Forest和LSTM三种模型，设置不同训练窗口和预测范围。

Result: 未来维护活动可被有意义地准确预测，如分组分数和趋势类型的准确率分别超0.95和0.80，简单统计和机器学习模型与深度学习模型表现相当。

Conclusion: 预测建模可有效补充现有Scorecard指标，实现对开源维护风险更主动的评估。

Abstract: The OpenSSF Scorecard is widely used to assess the security posture of open-source software repositories, with the Maintained metric indicating recent development activity and helping identify potentially abandoned dependencies. However, this metric is inherently retrospective, reflecting only the past 90 days of activity and providing no insight into future maintenance, which limits its usefulness for proactive risk assessment. In this paper, we study to what extent future maintenance activity, as captured by the OpenSSF Maintained score, can be forecasted. We analyze 3,220 GitHub repositories associated with the top 1% most central PyPI libraries by PageRank and reconstruct historical Maintained scores over a three-year period. We formulate the task as multivariate time series forecasting and consider four target representations: raw scores, bucketed maintenance levels, numerical trend slopes, and categorical trend types. We compare a statistical model (VARMA), a machine learning model (Random Forest), and a deep learning model (LSTM) across training windows of 3-12 months and forecasting horizons of 1-6 months. Our results show that future maintenance activity can be predicted with meaningful accuracy, particularly for aggregated representations such as bucketed scores and trend types, achieving accuracies above 0.95 and 0.80, respectively. Simpler statistical and machine learning models perform on par with deep learning approaches, indicating that complex architectures are not required. These findings suggest that predictive modeling can effectively complement existing Scorecard metrics, enabling more proactive assessment of open-source maintenance risks.

</details>


### [326] [Promises, Perils, and (Timely) Heuristics for Mining Coding Agent Activity](https://arxiv.org/abs/2601.18345)
*Romain Robes Théo Matricon,Thomas Degueule,Andre Hora,Stefano Zacchiroli*

Main category: cs.SE

TL;DR: 本文记录了对GitHub上编码代理活动研究的前景、风险和启发式方法。


<details>
  <summary>Details</summary>
Motivation: 2025年编码代理迅速普及，其利用大语言模型的方式与基于LLM的代码补全不同，且在软件仓库留下痕迹，便于用MSR技术研究其对SE实践的影响，因此有研究必要。

Method: 未提及

Result: 未提及

Conclusion: 未提及

Abstract: In 2025, coding agents have seen a very rapid adoption. Coding agents leverage Large Language Models (LLMs) in ways that are markedly different from LLM-based code completion, making their study critical. Moreover, unlike LLM-based completion, coding agents leave visible traces in software repositories, enabling the use of MSR techniques to study their impact on SE practices. This paper documents the promises, perils, and heuristics that we have gathered from studying coding agent activity on GitHub.

</details>


### [327] [daVinci-Dev: Agent-native Mid-training for Software Engineering](https://arxiv.org/abs/2601.18418)
*Ji Zeng,Dayuan Fu,Tiantian Mi,Yumin Zhuang,Yaxing Huang,Xuefeng Li,Lyumanshan Ye,Muhang Xie,Qishuo Hua,Zhen Huang,Mohan Jiang,Hanning Wang,Jifan Lin,Yang Xiao,Jie Sun,Yunze Wu,Pengfei Liu*

Main category: cs.SE

TL;DR: 本文研究大语言模型的代理式软件工程，提出代理式中间训练方法，使用代理原生数据，在SWE - Bench Verified验证，性能优于Kimi - Dev且使用更少训练token。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型能力前沿转向代理式软件工程，代理式中间训练因资源需求大未充分探索，且存在训练数据与实际开发环境分布不匹配问题。

Method: 提出系统的代理式中间训练方法，采用包含上下文原生轨迹和环境原生轨迹的代理原生数据进行训练。

Result: 在SWE - Bench Verified验证模型能力，优于Kimi - Dev，使用不到一半中间训练token，32B和72B模型分别达到56.1%和58.5%的解决率。

Conclusion: 所提出的代理式中间训练方法有效，能在资源使用更少的情况下提升模型的代理能力。

Abstract: Recently, the frontier of Large Language Model (LLM) capabilities has shifted from single-turn code generation to agentic software engineering-a paradigm where models autonomously navigate, edit, and test complex repositories. While post-training methods have become the de facto approach for code agents, **agentic mid-training**-mid-training (MT) on large-scale data that mirrors authentic agentic workflows-remains critically underexplored due to substantial resource requirements, despite offering a more scalable path to instilling foundational agentic behaviors than relying solely on expensive reinforcement learning. A central challenge in realizing effective agentic mid-training is the distribution mismatch between static training data and the dynamic, feedback-rich environment of real development. To address this, we present a systematic study of agentic mid-training, establishing both the data synthesis principles and training methodology for effective agent development at scale. Central to our approach is **agent-native data**-supervision comprising two complementary types of trajectories: **contextually-native trajectories** that preserve the complete information flow an agent experiences, offering broad coverage and diversity; and **environmentally-native trajectories** collected from executable repositories where observations stem from actual tool invocations and test executions, providing depth and interaction authenticity. We verify the model's agentic capabilities on `SWE-Bench Verified`. We demonstrate our superiority over the previous open software engineering mid-training recipe `Kimi-Dev` under two post-training settings with an aligned base model and agentic scaffold, while using less than half mid-training tokens (73.1B). Besides relative advantage, our best performing 32B and 72B models achieve **56.1%** and **58.5%** resolution rates, respectively, which are ...

</details>


### [328] [An Audit of Machine Learning Experiments on Software Defect Prediction](https://arxiv.org/abs/2601.18477)
*Giuseppe Destefanis,Leila Yousefi,Martin Shepperd,Allan Tucker,Stephen Swift,Steve Counsell,Mahir Arzoky*

Main category: cs.SE

TL;DR: 本文审计2019 - 2023年软件缺陷预测（SDP）研究，发现实验设计和报告实践差异大，近半研究缺乏可复现细节，有改进空间。


<details>
  <summary>Details</summary>
Motivation: 评估SDP研究的实验设计、分析和报告实践，表征当前实践并评估结果可复现性。

Method: 审计2019 - 2023年SCOPUS索引的SDP研究，评估设计和分析选择及九个研究问题，用特定工具评估可复现性。

Result: 抽样101篇论文，发现研究实践差异大，45%研究应用正式统计推断，识别427个问题，可复现性差异大，发现可能的论文工厂活动。

Conclusion: 实验设计和报告实践差异大，近半研究细节不足，有很大改进空间。

Abstract: Background: Machine learning algorithms are widely used to predict defect prone software components. In this literature, computational experiments are the main means of evaluation, and the credibility of results depends on experimental design and reporting. Objective: This paper audits recent software defect prediction (SDP) studies by assessing their experimental design, analysis, and reporting practices against accepted norms from statistics, machine learning, and empirical software engineering. The aim is to characterise current practice and assess the reproducibility of published results. Method: We audited SDP studies indexed in SCOPUS between 2019 and 2023, focusing on design and analysis choices such as outcome measures, out of sample validation strategies, and the use of statistical inference. Nine study issues were evaluated. Reproducibility was assessed using the instrument proposed by González Barahona and Robles. Results: The search identified approximately 1,585 SDP experiments published during the period. From these, we randomly sampled 101 papers, including 61 journal and 40 conference publications, with almost 50 percent behind paywalls. We observed substantial variation in research practice. The number of datasets ranged from 1 to 365, learners or learner variants from 1 to 34, and performance measures from 1 to 9. About 45 percent of studies applied formal statistical inference. Across the sample, we identified 427 issues, with a median of four per paper, and only one paper without issues. Reproducibility ranged from near complete to severely limited. We also identified two cases of tortured phrases and possible paper mill activity. Conclusions: Experimental design and reporting practices vary widely, and almost half of the studies provide insufficient detail to support reproduction. The audit indicates substantial scope for improvement.

</details>


### [329] [On the Abolition of the "ICSE Paper" and the Adoption of the "Registered Proposal" and the "Results Report"](https://arxiv.org/abs/2601.18566)
*Fabio Massacci,Winnie Mbaka*

Main category: cs.SE

TL;DR: 提出废除现有ICSE论文，采用两层体系并改进注册报告概念以解决领域问题。


<details>
  <summary>Details</summary>
Motivation: 解决领域内的“新颖性恶性循环”和“可复制性危机”。

Method: 废除现有“ICSE论文”，采用两层体系，包括提交“注册提案”和“结果报告”，且二者都是主流活动的重要组成部分。

Result: 未提及明确结果。

Conclusion: 这种颠覆性想法得到软件工程未来预调查社区的支持。

Abstract: To address the 'novelty-vicious cycle' and the 'replicability crisis' of the field (both discussed in the survey) we propose abolishing the "ICSE paper" as we know it and replacing it with a two-tier system that also evolves the existing notion of 'Registered Report'. Authors proposing a new idea, experiment, or analysis would submit a "Registered Proposal" of their idea and the proposed experimental methodology to undergo peer review. The following year, anyone can submit (shorter) "Results Reports" on the realization of the empirical work based on the registered proposals of the previous ICSE (or FSE or ISSTA or ASE etc.). Both works should be first class citizens of the mainstream events. We argue that such a disruptive (heretical?) idea is supported and based on the responses of the community of the Future of Software Engineering pre-survey

</details>


### [330] [How are MLOps Frameworks Used in Open Source Projects? An Empirical Characterization](https://arxiv.org/abs/2601.18591)
*Fiorella Zampetti,Federico Stocchetti,Federica Razzano,Damian Andrew Tamburri,Massimiliano Di Penta*

Main category: cs.SE

TL;DR: 本文研究八个流行开源MLOps框架的实际使用和期望的功能增强，发现框架很少开箱即用，用户主要请求核心功能增强等。


<details>
  <summary>Details</summary>
Motivation: MLOps框架虽功能丰富，但开发者可能只利用部分功能且缺失一些期望功能，因此研究其实际使用和功能增强情况。

Method: 分析GitHub上依赖项目对框架的使用，包括API和命令调用；定性分析框架问题跟踪器中的功能请求和增强，并与已识别的使用特征关联。

Result: MLOps框架很少开箱即用，很少集成到GitHub Workflows，开发者用API实现自定义功能；使用的功能涉及核心ML阶段和基础设施治理，有时结合多个框架；用户主要请求框架核心功能增强、更好的API暴露和CI/CD集成。

Conclusion: 开发者对MLOps框架的使用有自身特点，且对框架功能有具体的改进需求。

Abstract: Machine Learning (ML) Operations (MLOps) frameworks have been conceived to support developers and AI engineers in managing the lifecycle of their ML models. While such frameworks provide a wide range of features, developers may leverage only a subset of them, while missing some highly desired features. This paper investigates the practical use and desired feature enhancements of eight popular open-source MLOps frameworks. Specifically, we analyze their usage by dependent projects on GitHub, examining how they invoke the frameworks' APIs and commands. Then, we qualitatively analyze feature requests and enhancements mined from the frameworks' issue trackers, relating these desired improvements to the previously identified usage features. Results indicate that MLOps frameworks are rarely used out-of-the-box and are infrequently integrated into GitHub Workflows, but rather, developers use their APIs to implement custom functionality in their projects. Used features concern core ML phases and whole infrastructure governance, sometimes leveraging multiple frameworks with complementary features. The mapping with feature requests highlights that users mainly ask for enhancements to core features of the frameworks, but also better API exposure and CI/CD integration.

</details>


### [331] [Let's Make Every Pull Request Meaningful: An Empirical Analysis of Developer and Agentic Pull Requests](https://arxiv.org/abs/2601.18749)
*Haruhiko Yoshioka,Takahiro Monno,Haruka Tokumasu,Taiki Wakamatsu,Yuki Ota,Nimmi Weeraddana,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: 对AIDev数据集40214个PR进行分析，比较人类和AI生成PR的合并结果，发现提交者属性对合并结果影响大，为提升PR质量提供见解。


<details>
  <summary>Details</summary>
Motivation: AI生成的PR合并率低于人类，需研究其合并结果差异。

Method: 从AIDev数据集收集40214个PR，提取64个特征，拟合统计回归模型。

Result: 提交者属性主导两组合并结果，审查相关特征对人类和AI生成PR影响不同。

Conclusion: 研究结果为通过人机协作提升PR质量提供了见解。

Abstract: The automatic generation of pull requests (PRs) using AI agents has become increasingly common. Although AI-generated PRs are fast and easy to create, their merge rates have been reported to be lower than those created by humans. In this study, we conduct a large-scale empirical analysis of 40,214 PRs collected from the AIDev dataset. We extract 64 features across six families and fit statistical regression models to compare PR merge outcomes for human and agentic PRs, as well as across three AI agents. Our results show that submitter attributes dominate merge outcomes for both groups, while review-related features exhibit contrasting effects between human and agentic PRs. The findings of this study provide insights into improving PR quality through human-AI collaboration.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [332] [The Compounded BSDE method: A fully-forward method for option pricing and optimal stopping problems in finance](https://arxiv.org/abs/2601.18634)
*Zhipeng Huang,Cornelis W. Oosterlee*

Main category: q-fin.CP

TL;DR: 提出Compound BSDE方法解决金融数学问题，建立算法和收敛性质，实验证明其准确高效。


<details>
  <summary>Details</summary>
Motivation: 解决金融数学中包括最优停时等广泛问题，提供新视角处理复合期权和最优停时问题。

Method: 基于反向随机微分方程（BSDEs）系统重新表述期权定价问题，在经典深度BSDE方法基础上开发复合BSDE算法。

Result: 建立算法收敛性质，推导后验误差估计，数值实验证明方法准确高效。

Conclusion: Compound BSDE方法对高维期权定价和最优停时问题有效。

Abstract: We propose the Compound BSDE method, a fully forward, deep-learning-based approach for solving a broad class of problems in financial mathematics, including optimal stopping. The method is based on a reformulation of option pricing problems in terms of a system of backward stochastic differential equations (BSDEs), which offers a new perspective on the numerical treatment of compound options and optimal stopping problems such as Bermudan option pricing. Building on the classical deep BSDE method for a single BSDE, we develop an algorithm for compound BSDEs and establish its convergence properties. In particular, we derive an \emph{a posteriori} error estimate for the proposed method. Numerical experiments demonstrate the accuracy and computational efficiency of the approach, and illustrate its effectiveness for high-dimensional option pricing and optimal stopping problems.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [333] [Regret-Driven Portfolios: LLM-Guided Smart Clustering for Optimal Allocation](https://arxiv.org/abs/2601.17021)
*Muhammad Abro,Hassan Jaleel*

Main category: q-fin.PM

TL;DR: 提出LLM引导的无遗憾投资组合分配框架，构建高夏普比率投资组合，实证显示表现优于基准。


<details>
  <summary>Details</summary>
Motivation: 缓解中长期投资组合管理中风险与回报的持久权衡问题。

Method: 提出LLM引导的无遗憾投资组合分配框架，结合在线学习动态、市场情绪指标和LLM对冲，基于跟随领导者方法，增加基于情绪的交易过滤和LLM下行保护。

Result: 该方法在年化收益率上比SPY买入持有基准高出69%，夏普比率高出119%。

Conclusion: 所提框架能有效构建适合风险厌恶投资者和机构基金经理的高夏普比率投资组合。

Abstract: We attempt to mitigate the persistent tradeoff between risk and return in medium- to long-term portfolio management. This paper proposes a novel LLM-guided no-regret portfolio allocation framework that integrates online learning dynamics, market sentiment indicators, and large language model (LLM)-based hedging to construct high-Sharpe ratio portfolios tailored for risk-averse investors and institutional fund managers. Our approach builds on a follow-the-leader approach, enriched with sentiment-based trade filtering and LLM-driven downside protection. Empirical results demonstrate that our method outperforms a SPY buy-and-hold baseline by 69% in annualized returns and 119% in Sharpe ratio.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [334] [MarketGANs: Multivariate financial time-series data augmentation using generative adversarial networks](https://arxiv.org/abs/2601.17773)
*Jeonggyu Huh,Seungwon Jeong,Hyun-Gyoon Kim,Hyeng Keun Koo,Byung Hwa Lim*

Main category: q-fin.ST

TL;DR: 本文提出MarketGAN框架用于数据稀缺下高维资产收益生成，在资产收益特征匹配和投资组合应用上表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决高维资产收益在数据稀缺情况下的生成问题，更好捕捉资产收益特征。

Method: 引入显式资产定价因子结构，采用基于时间卷积网络（TCN）的生成对抗学习，对随机时变因子载荷和波动率建模。

Result: MarketGAN更能匹配资产收益的实证特征，在因子信息至少弱信息时，其生成样本的协方差估计在投资组合应用中表现更优。

Conclusion: MarketGAN具有实际经济价值，在高维资产收益生成方面优于传统基于因子模型的自助法。

Abstract: This paper introduces MarketGAN, a factor-based generative framework for high-dimensional asset return generation under severe data scarcity. We embed an explicit asset-pricing factor structure as an economic inductive bias and generate returns as a single joint vector, thereby preserving cross-sectional dependence and tail co-movement alongside inter-temporal dynamics. MarketGAN employs generative adversarial learning with a temporal convolutional network (TCN) backbone, which models stochastic, time-varying factor loadings and volatilities and captures long-range temporal dependence. Using daily returns of large U.S. equities, we find that MarketGAN more closely matches empirical stylized facts of asset returns, including heavy-tailed marginal distributions, volatility clustering, leverage effects, and, most notably, high-dimensional cross-sectional correlation structures and tail co-movement across assets, than conventional factor-model-based bootstrap approaches. In portfolio applications, covariance estimates derived from MarketGAN-generated samples outperform those derived from other methods when factor information is at least weakly informative, demonstrating tangible economic value.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [335] [Pregeometric Origins of Liquidity Geometry in Financial Order Books](https://arxiv.org/abs/2601.17245)
*João P. da Cruz*

Main category: q-fin.TR

TL;DR: 提出金融订单簿几何结构框架，用投影方法得出供应和需求呈类伽马函数形式，实证验证并表明订单簿流动性规律源于观测几何约束。


<details>
  <summary>Details</summary>
Motivation: 将流动性、供需视为涌现可观测量，而非原始经济变量，构建金融订单簿几何结构框架。

Method: 将市场建模为无度量、时间和价格坐标的膨胀关系系统，通过图拉普拉斯谱嵌入进行投影。

Result: 投影的供需呈类伽马函数形式，用高频数据验证，与其他模型对比显示对积分伽马几何的偏好，模拟也重现相同结构。

Conclusion: 订单簿流动性关键规律反映的是观测引起的几何约束，而非微观结构动态。

Abstract: We propose a structural framework for the geometry of financial order books in which liquidity, supply, and demand are treated as emergent observables rather than primitive economic variables. The market is modeled as an inflationary relational system without assumed metric, temporal, or price coordinates. Observable quantities arise only through projection, implemented here via spectral embeddings of the graph Laplacian. A one-dimensional projection induces a price-like coordinate, while the projected density defines liquidity profiles around the mid price. Under a minimal single-scale hypothesis -- excluding intrinsic length scales beyond distance to the mid and finite visibility -- we show that projected supply and demand are constrained to gamma-like functional forms. In discrete data, this prediction translates into integrated-gamma cumulative profiles. We test these results using high-frequency Level~II data for several U.S. equities and find robust agreement across assets and intraday windows. Explicit comparison with alternative cumulative models using information criteria demonstrates a systematic preference for the integrated-gamma geometry. A minimal simulation of inflationary relational dynamics reproduces the same structure without invoking agent behavior or price formation mechanisms. These results indicate that key regularities of order-book liquidity reflect geometric constraints induced by observation rather than detailed microstructural dynamics.
  Supplementary Material is available at the arXiv submission.

</details>


### [336] [Learning Market Making with Closing Auctions](https://arxiv.org/abs/2601.17247)
*Julius Graf,Thibaut Mastrolia*

Main category: q-fin.TR

TL;DR: 研究连续竞价后有收盘拍卖的做市问题，提出融入收盘拍卖机制的深度Q学习框架，并应用于不同情境与经典基准比较。


<details>
  <summary>Details</summary>
Motivation: 标准最优做市模型依赖终端库存惩罚管理风险，忽略收盘拍卖的重大流动性事件，因此需改进。

Method: 提出明确考虑收盘拍卖的做市框架，开发生成随机市场模型模拟交易，将理论模型和深度Q学习方法应用于两种情境。

Result: 未提及具体结果。

Conclusion: 未提及最终结论。

Abstract: In this work, we investigate the market-making problem on a trading session in which a continuous phase on a limit order book is followed by a closing auction. Whereas standard optimal market-making models typically rely on terminal inventory penalties to manage end-of-day risk, ignoring the significant liquidity events available in closing auctions, we propose a Deep Q-Learning framework that explicitly incorporates this mechanism. We introduce a market-making framework designed to explicitly anticipate the closing auction, continuously refining the projected clearing price as the trading session evolves. We develop a generative stochastic market model to simulate the trading session and to emulate the market. Our theoretical model and Deep Q-Learning method is applied on the generator in two settings: (1) when the mid price follows a rough Heston model with generative data from this stochastic model; and (2) when the mid price corresponds to historical data of assets from the S&P 500 index and the performance of our algorithm is compared with classical benchmarks from optimal market making.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [337] [Data-Driven Information-Theoretic Causal Bounds under Unmeasured Confounding](https://arxiv.org/abs/2601.17160)
*Yonghan Jung,Bogyeong Kang*

Main category: stat.ML

TL;DR: 提出数据驱动信息论框架，实现无测量混杂下因果效应的精确部分识别，克服现有方法局限，模拟和应用显示效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在依赖严格假设、需外部输入、需完整结构模型规范、忽略条件处理效应等局限性。

Method: 建立新的信息论数据驱动散度界，开发满足Neyman正交性的半参数估计器。

Result: 框架能从观测数据直接精确部分识别条件因果效应，模拟研究和真实数据应用表明框架在多种数据生成过程中提供紧密有效的因果界。

Conclusion: 提出的框架有效克服现有方法局限，可用于精确部分识别因果效应。

Abstract: We develop a data-driven information-theoretic framework for sharp partial identification of causal effects under unmeasured confounding. Existing approaches often rely on restrictive assumptions, such as bounded or discrete outcomes; require external inputs (for example, instrumental variables, proxies, or user-specified sensitivity parameters); necessitate full structural causal model specifications; or focus solely on population-level averages while neglecting covariate-conditional treatment effects. We overcome all four limitations simultaneously by establishing novel information-theoretic, data-driven divergence bounds. Our key theoretical contribution shows that the f-divergence between the observational distribution P(Y | A = a, X = x) and the interventional distribution P(Y | do(A = a), X = x) is upper bounded by a function of the propensity score alone. This result enables sharp partial identification of conditional causal effects directly from observational data, without requiring external sensitivity parameters, auxiliary variables, full structural specifications, or outcome boundedness assumptions. For practical implementation, we develop a semiparametric estimator satisfying Neyman orthogonality (Chernozhukov et al., 2018), which ensures square-root-n consistent inference even when nuisance functions are estimated using flexible machine learning methods. Simulation studies and real-world data applications, implemented in the GitHub repository (https://github.com/yonghanjung/Information-Theretic-Bounds), demonstrate that our framework provides tight and valid causal bounds across a wide range of data-generating processes.

</details>


### [338] [Exact Minimum-Volume Confidence Set Intersection for Multinomial Outcomes](https://arxiv.org/abs/2601.18145)
*Heguang Lin,Binhao Chen,Mengze Li,Daniel Pimentel-Alarcón,Matthew L. Malloy*

Main category: stat.ML

TL;DR: 本文研究多项分布参数最小体积置信集（MVCs）的交集判定问题，提出认证、容差感知算法，在三维有高效合理算法，且可扩展到高维。


<details>
  <summary>Details</summary>
Motivation: MVCs最优但定义为不连续且难计算的精确p值的水平集，难以直接刻画其几何形状，研究其实用决策问题即判定两个观测多项结果的MVCs是否相交。

Method: 利用似然排序在对数赔率坐标中诱导半空间约束，对参数空间进行自适应几何划分，计算每个单元格上p值的上下界。

Result: 针对三维类别得到高效且合理的算法，能判定相交、不相交或结果不确定，且方法可扩展到高维。

Conclusion: 尽管MVCs几何形状不规则，但对于A/B测试的核心任务，存在可靠的认证决策程序。

Abstract: Computation of confidence sets is central to data science and machine learning, serving as the workhorse of A/B testing and underpinning the operation and analysis of reinforcement learning algorithms. Among all valid confidence sets for the multinomial parameter, minimum-volume confidence sets (MVCs) are optimal in that they minimize average volume, but they are defined as level sets of an exact p-value that is discontinuous and difficult to compute. Rather than attempting to characterize the geometry of MVCs directly, this paper studies a practically motivated decision problem: given two observed multinomial outcomes, can one certify whether their MVCs intersect? We present a certified, tolerance-aware algorithm for this intersection problem. The method exploits the fact that likelihood ordering induces halfspace constraints in log-odds coordinates, enabling adaptive geometric partitioning of parameter space and computable lower and upper bounds on p-values over each cell. For three categories, this yields an efficient and provably sound algorithm that either certifies intersection, certifies disjointness, or returns an indeterminate result when the decision lies within a prescribed margin. We further show how the approach extends to higher dimensions. The results demonstrate that, despite their irregular geometry, MVCs admit reliable certified decision procedures for core tasks in A/B testing.

</details>


### [339] [Error Analysis of Bayesian Inverse Problems with Generative Priors](https://arxiv.org/abs/2601.17374)
*Bamdad Hosseini,Ziqi Huang*

Main category: stat.ML

TL;DR: 本文分析了基于最小Wasserstein - 2生成模型的逆问题数据驱动方法，给出误差界，数值实验验证分析结果。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习技术兴起，数据驱动的逆问题求解方法流行，需要对用生成模型学习定制先验的方法进行分析。

Method: 给出最小Wasserstein - 2生成模型先验的定量误差界。

Result: 在一些假设下，生成先验导致的后验误差在Wasserstein - 1距离下与先验有相同的速率，数值实验验证误差分析。

Conclusion: 所做的误差分析在基准测试和椭圆PDE逆问题中得到体现。

Abstract: Data-driven methods for the solution of inverse problems have become widely popular in recent years thanks to the rise of machine learning techniques. A popular approach concerns the training of a generative model on additional data to learn a bespoke prior for the problem at hand. In this article we present an analysis for such problems by presenting quantitative error bounds for minimum Wasserstein-2 generative models for the prior. We show that under some assumptions, the error in the posterior due to the generative prior will inherit the same rate as the prior with respect to the Wasserstein-1 distance. We further present numerical experiments that verify that aspects of our error analysis manifests in some benchmarks followed by an elliptic PDE inverse problem where a generative prior is used to model a non-stationary field.

</details>


### [340] ["Rebuilding" Statistics in the Age of AI: A Town Hall Discussion on Culture, Infrastructure, and Training](https://arxiv.org/abs/2601.17510)
*David L. Donoho,Jian Kang,Xihong Lin,Bhramar Mukherjee,Dan Nettleton,Rebecca Nugent,Abel Rodriguez,Eric P. Xing,Tian Zheng,Hongtu Zhu*

Main category: stat.ML

TL;DR: 文章呈现2024 JSM会议“AI时代的统计学”市政厅讨论的完整原始记录，围绕五个问题组织对话，旨在支持相关透明性和持续对话。


<details>
  <summary>Details</summary>
Motivation: 探讨统计学领域如何因人工智能、基础模型等进步而演变，支持关于统计学在数据和AI主导未来中角色的透明性、社区反思和持续对话。

Method: 通过开放小组讨论和广泛的观众问答，以获取基于经验的观点，对对话记录进行极少编辑干预并围绕五个问题组织。

Result: 保存了小组成员和观众之间的广泛交流，形成了围绕五个问题的对话记录。

Conclusion: 该预印本能支持对统计学在数据和AI未来中角色的透明性、社区反思和持续对话。

Abstract: This article presents the full, original record of the 2024 Joint Statistical Meetings (JSM) town hall, "Statistics in the Age of AI," which convened leading statisticians to discuss how the field is evolving in response to advances in artificial intelligence, foundation models, large-scale empirical modeling, and data-intensive infrastructures. The town hall was structured around open panel discussion and extensive audience Q&A, with the aim of eliciting candid, experience-driven perspectives rather than formal presentations or prepared statements. This document preserves the extended exchanges among panelists and audience members, with minimal editorial intervention, and organizes the conversation around five recurring questions concerning disciplinary culture and practices, data curation and "data work," engagement with modern empirical modeling, training for large-scale AI applications, and partnerships with key AI stakeholders. By providing an archival record of this discussion, the preprint aims to support transparency, community reflection, and ongoing dialogue about the evolving role of statistics in the data- and AI-centric future.

</details>


### [341] [Boosting methods for interval-censored data with regression and classification](https://arxiv.org/abs/2601.17973)
*Yuan Bian,Grace Y. Yi,Wenqing He*

Main category: stat.ML

TL;DR: 本文针对区间删失数据提出了新的非参数提升方法，用于回归和分类任务，通过变换调整损失函数，理论上证明其性质，实证表明有良好性能。


<details>
  <summary>Details</summary>
Motivation: 传统提升算法处理全观测随机样本，难以应对区间删失数据，而此类数据在多个领域常见，需有效处理方法。

Method: 利用删失无偏变换调整损失函数和插补变换后的响应，通过函数梯度下降实现。

Result: 严格建立理论性质，实证研究表明在各种有限样本场景下性能稳健。

Conclusion: 提出的方法为提升区间删失数据预测精度提供了稳健框架，补充了现有工作，扩展了提升技术的适用性。

Abstract: Boosting has garnered significant interest across both machine learning and statistical communities. Traditional boosting algorithms, designed for fully observed random samples, often struggle with real-world problems, particularly with interval-censored data. This type of data is common in survival analysis and time-to-event studies where exact event times are unobserved but fall within known intervals. Effective handling of such data is crucial in fields like medical research, reliability engineering, and social sciences. In this work, we introduce novel nonparametric boosting methods for regression and classification tasks with interval-censored data. Our approaches leverage censoring unbiased transformations to adjust loss functions and impute transformed responses while maintaining model accuracy. Implemented via functional gradient descent, these methods ensure scalability and adaptability. We rigorously establish their theoretical properties, including optimality and mean squared error trade-offs. Our proposed methods not only offer a robust framework for enhancing predictive accuracy in domains where interval-censored data are common but also complement existing work, expanding the applicability of existing boosting techniques. Empirical studies demonstrate robust performance across various finite-sample scenarios, highlighting the practical utility of our approaches.

</details>


### [342] [A Cherry-Picking Approach to Large Load Shaping for More Effective Carbon Reduction](https://arxiv.org/abs/2601.17990)
*Bokan Chen,Raiden Hasegawa,Adriaan Hilbers,Ross Koningstein,Ana Radovanović,Utkarsh Shah,Gabriela Volpato,Mohamed Ahmed,Tim Cary,Rod Frowd*

Main category: stat.ML

TL;DR: 研究用模拟分析多种负荷整形策略对电网CO₂排放和电力成本的影响，发现LMP策略减排优但可改进，提出“择优选择”方法。


<details>
  <summary>Details</summary>
Motivation: 因缺乏反事实数据，难以证实流行负荷整形策略有效性，需研究不同策略效果。

Method: 使用一系列校准的ERCOT日前直流最优潮流（DC - OPF）模拟进行多种负荷整形策略的反事实分析。

Result: 从年度电网CO₂减排看，LMP策略优于其他常见策略，但有改进空间。

Conclusion: 提出基于可观察的电网信号和历史数据“择优选择”每日策略的负荷整形方法，适用于大型灵活用电方。

Abstract: Shaping multi-megawatt loads, such as data centers, impacts generator dispatch on the electric grid, which in turn affects system CO2 emissions and energy cost. Substantiating the effectiveness of prevalent load shaping strategies, such as those based on grid-level average carbon intensity, locational marginal price, or marginal emissions, is challenging due to the lack of detailed counterfactual data required for accurate attribution. This study uses a series of calibrated granular ERCOT day-ahead direct current optimal power flow (DC-OPF) simulations for counterfactual analysis of a broad set of load shaping strategies on grid CO2 emissions and cost of electricity. In terms of annual grid level CO2 emissions reductions, LMP-based shaping outperforms other common strategies, but can be significantly improved upon. Examining the performance of practicable strategies under different grid conditions motivates a more effective load shaping approach: one that "cherry-picks" a daily strategy based on observable grid signals and historical data. The cherry-picking approach to power load shaping is applicable to any large flexible consumer on the electricity grid, such as data centers, distributed energy resources and Virtual Power Plants (VPPs).

</details>


### [343] [Nonlinear multi-study factor analysis](https://arxiv.org/abs/2601.18128)
*Gemma E. Moran,Anandi Krishnan*

Main category: stat.ML

TL;DR: 本文针对多研究或环境的高维数据，提出多研究稀疏变分自编码器学习共享和特定因素，以血小板基因表达数据验证方法有效。


<details>
  <summary>Details</summary>
Motivation: 理解多研究或环境的高维数据中，哪些潜在因素是所有研究共有的，哪些是特定研究或环境特有的。

Method: 提出多研究稀疏变分自编码器来拟合非线性多研究因子模型，模型具有稀疏性并隐含对潜在因子数量的惩罚。

Result: 证明了潜在因子可被识别，在血小板基因表达数据中恢复出有意义的因子。

Conclusion: 所提出的方法能有效处理多研究或环境的高维数据，分离共享和特定因素。

Abstract: High-dimensional data often exhibit variation that can be captured by lower dimensional factors. For high-dimensional data from multiple studies or environments, one goal is to understand which underlying factors are common to all studies, and which factors are study or environment-specific. As a particular example, we consider platelet gene expression data from patients in different disease groups. In this data, factors correspond to clusters of genes which are co-expressed; we may expect some clusters (or biological pathways) to be active for all diseases, while some clusters are only active for a specific disease. To learn these factors, we consider a nonlinear multi-study factor model, which allows for both shared and specific factors. To fit this model, we propose a multi-study sparse variational autoencoder. The underlying model is sparse in that each observed feature (i.e. each dimension of the data) depends on a small subset of the latent factors. In the genomics example, this means each gene is active in only a few biological processes. Further, the model implicitly induces a penalty on the number of latent factors, which helps separate the shared factors from the group-specific factors. We prove that the latent factors are identified, and demonstrate our method recovers meaningful factors in the platelet gene expression data.

</details>


### [344] [Out-of-Distribution Radar Detection with Complex VAEs: Theory, Whitening, and ANMF Fusion](https://arxiv.org/abs/2601.18677)
*Yadang Alexis Rouzoumka,Jean Pinsolle,Eugénie Terreaux,Christèle Morisseau,Jean-Philippe Ovarlez,Chengfang Ren*

Main category: stat.ML

TL;DR: 研究在非高斯、距离变化干扰中检测弱复值信号，提出用CVAE进行检测，对比经典和自适应检测器，结果显示其检测性能好，CVAE - ANMF融合方案有竞争力。


<details>
  <summary>Details</summary>
Motivation: 解决在非高斯、距离变化干扰（尤其是海上雷达场景）中检测弱复值信号的问题。

Method: 使用仅在杂波加噪声上训练的复值变分自编码器（CVAE）进行分布外检测，在两种配置下评估，还将CVAE与ANMF在决策层融合。

Result: 在两种配置下CVAE在匹配虚警率时有更高检测概率，白化处理下改进显著，CVAE - ANMF融合方案在强非高斯杂波中更稳健并能控制虚警率。

Conclusion: 统计归一化与复值生成模型结合可显著改善实际海杂波条件下的检测，CVAE - ANMF融合方案是现有基于模型检测器的有力替代方案。

Abstract: We investigate the detection of weak complex-valued signals immersed in non-Gaussian, range-varying interference, with emphasis on maritime radar scenarios. The proposed methodology exploits a Complex-valued Variational AutoEncoder (CVAE) trained exclusively on clutter-plus-noise to perform Out-Of-Distribution detection. By operating directly on in-phase / quadrature samples, the CVAE preserves phase and Doppler structure and is assessed in two configurations: (i) using unprocessed range profiles and (ii) after local whitening, where per-range covariance estimates are obtained from neighboring profiles. Using extensive simulations together with real sea-clutter data from the CSIR maritime dataset, we benchmark performance against classical and adaptive detectors (MF, NMF, AMF-SCM, ANMF-SCM, ANMF-Tyler). In both configurations, the CVAE yields a higher detection probability Pd at matched false-alarm rate Pfa, with the most notable improvements observed under whitening. We further integrate the CVAE with the ANMF through a weighted log-p fusion rule at the decision level, attaining enhanced robustness in strongly non-Gaussian clutter and enabling empirically calibrated Pfa control under H0. Overall, the results demonstrate that statistical normalization combined with complex-valued generative modeling substantively improves detection in realistic sea-clutter conditions, and that the fused CVAE-ANMF scheme constitutes a competitive alternative to established model-based detectors.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [345] [The generalised balanced power diagram: flat sections, affine transformations and an improved rendering algorithm](https://arxiv.org/abs/2601.18593)
*Felix Ballani*

Main category: stat.CO

TL;DR: 本文整理GBPD关于仿射变换和平截面的性质，并将一种生成数字图像的算法扩展到GBPD上。


<details>
  <summary>Details</summary>
Motivation: GBPD被认为是描述具有弯曲晶界的多晶微观结构的合适几何模型，需研究其相关性质和高效算法。

Method: 整理GBPD关于仿射变换和平截面的性质，将已知的用于幂图生成数字图像的算法扩展到GBPD。

Result: 完成了GBPD性质的整理和算法的扩展。

Conclusion: 通过整理性质和扩展算法，有助于更好地利用GBPD对多晶微观结构进行研究。

Abstract: The generalised balanced power diagram (GBPD) is regarded in the literature as a suitable geometric model for describing polycrystalline microstructures with curved grain boundaries. This article compiles properties of GBPDs with regard to affine transformations and flat sections. Furthermore, it extends an algorithm known for power diagrams for generating digital images, which is more efficient than the usual brute force approach, on GBPDs.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [346] [Winning Criteria for Open Games: A Game-Theoretic Approach to Prefix Codes](https://arxiv.org/abs/2601.17521)
*Dean Kraizberg*

Main category: math.OC

TL;DR: 研究无限树上交替移动的两人游戏，给出先手获胜必要条件，建立与最大前缀码等价关系，推导代数条件并引入覆盖概念。


<details>
  <summary>Details</summary>
Motivation: 研究无限树且获胜集为开集的两人交替移动游戏中先手获胜策略情况。

Method: 给出先手获胜简单必要条件，建立与最大前缀码等价关系，用无限标记树覆盖图。

Result: 得到先手获胜的代数条件，对一类游戏该条件等价于获胜，得出最大前缀码简单特征。

Conclusion: 通过建立与最大前缀码的联系等方法，在无限树两人游戏先手获胜策略研究上取得成果。

Abstract: We study two-player games with alternating moves played on infinite trees. Our main focus is on the case where the trees are full (regular) and the winning set is open (with respect to the product topology on the tree). Gale and Stewart showed that in this setting one of the players always has a winning strategy, though it is not known in advance which player. We present simple necessary conditions for the first player to have a winning strategy, and establish an equivalence between winning sets that guarantee a win for the first player and maximal prefix codes. Using this equivalence, we derive a necessary algebraic condition for winning, and exhibit a family of games for which this algebraic condition is in fact equivalent to winning. We introduce the concept of coverings, and show that by covering the graph with an infinite labeled tree corresponding to the free group, we can derive a simple trait of maximal prefix codes.

</details>


### [347] [Global Optimization of Atomic Clusters via Physically-Constrained Tensor Train Decomposition](https://arxiv.org/abs/2601.18592)
*Konstantin Sozykin,Nikita Rybin,Andrei Chertkov,Anh-Huy Phan,Ivan Oseledets,Alexander Shapeev,Ivan Novikov,Gleb Ryzhakov*

Main category: math.OC

TL;DR: 提出一种新框架，利用张量列车（TT）分解克服原子团簇全局优化的维数灾难问题，结合两种TT策略，开发物理约束编码方案，通过实例证明方法有效性，确立TT分解在分子结构预测中的作用。


<details>
  <summary>Details</summary>
Motivation: 原子团簇全局优化因局部极小值随系统规模指数增长（维数灾难）而面临挑战，需新方法克服该局限。

Method: 引入新框架，利用TT分解挖掘势能面低秩结构，结合代数TTOpt方法（最大体积采样）和概率PROTES方法（生成采样），开发物理约束编码方案。

Result: 识别出最多45个原子的Lennard - Jones团簇的全局最小值，使用机器学习的矩张量势优化20个原子的碳团簇，得到与量子精确模拟一致的几何结构。

Conclusion: 确立TT分解是分子结构预测的有力工具，为计算材料科学中的高维优化问题提供通用框架。

Abstract: The global optimization of atomic clusters represents a fundamental challenge in computational chemistry and materials science due to the exponential growth of local minima with system size (i.e., the curse of dimensionality). We introduce a novel framework that overcomes this limitation by exploiting the low-rank structure of potential energy surfaces through Tensor Train (TT) decomposition. Our approach combines two complementary TT-based strategies: the algebraic TTOpt method, which utilizes maximum volume sampling, and the probabilistic PROTES method, which employs generative sampling. A key innovation is the development of physically-constrained encoding schemes that incorporate molecular constraints directly into the discretization process. We demonstrate the efficacy of our method by identifying global minima of Lennard-Jones clusters containing up to 45 atoms. Furthermore, we establish its practical applicability to real-world systems by optimizing 20-atom carbon clusters using a machine-learned Moment Tensor Potential, achieving geometries consistent with quantum-accurate simulations. This work establishes TT-decomposition as a powerful tool for molecular structure prediction and provides a general framework adaptable to a wide range of high-dimensional optimization problems in computational material science.

</details>


### [348] [A Unified Kantorovich Duality for Multimarginal Optimal Transport](https://arxiv.org/abs/2601.17171)
*Yehya Cheryala,Mokhtar Z. Alaya,Salim Bouzebda*

Main category: math.OC

TL;DR: 本文为一般波兰积空间上具有有界连续成本函数的多边际最优传输（MOT）问题提出统一完整的Kantorovich对偶理论，分紧致和非紧致情况推导对偶等式，为MOT的概率和统计分析提供基础。


<details>
  <summary>Details</summary>
Motivation: 多边际最优传输在机器学习和统计领域备受关注，需为其建立对偶理论。

Method: 在边际紧致空间，通过凸分析重新表述推导对偶等式；在非紧致空间，使用截断 - 紧致性过程结合 $c$ - 分裂集和多边际 $c$ - 循环单调性。

Result: 得到对偶可达性、原始 - 对偶等式，证明对偶值在紧致子集限制下保持不变，可将允许的对偶族正则化为一致有界的 $c$ - 共轭势。

Conclusion: 研究结果为MOT的概率和统计分析进一步发展提供了结构基础。

Abstract: Multimarginal optimal transport (MOT) has gained increasing attention in recent years, notably due to its relevance in machine learning and statistics, where one seeks to jointly compare and align multiple probability distributions. This paper presents a unified and complete Kantorovich duality theory for MOT problem on general Polish product spaces with bounded continuous cost function. For marginal compact spaces, the duality identity is derived through a convex-analytic reformulation, that identifies the dual problem as a Fenchel-Rockafellar conjugate. We obtain dual attainment and show that optimal potentials may always be chosen in the class of $c$-conjugate families, thereby extending classical two-marginal conjugacy principle into a genuinely multimarginal setting. In non-compact setting, where direct compactness arguments are unavailable, we recover duality via a truncation-tightness procedure based on weak compactness of multimarginal transference plans and boundedness of the cost. We prove that the dual value is preserved under restriction to compact subsets and that admissible dual families can be regularized into uniformly bounded $c$-conjugate potentials. The argument relies on a refined use of $c$-splitting sets and their equivalence with multimarginal $c$-cyclical monotonicity. We then obtain dual attainment and exact primal-dual equality for MOT on arbitrary Polish spaces, together with a canonical representation of optimal dual potentials by $c$-conjugacy. These results provide a structural foundation for further developments in probabilistic and statistical analysis of MOT, including stability, differentiability, and asymptotic theory under marginal perturbations.

</details>


### [349] [Differentiable Integer Linear Programming is not Differentiable & it's not a mere technical problem](https://arxiv.org/abs/2601.17800)
*Thanawat Sornwanee*

Main category: math.OC

TL;DR: 指出论文‘Differentiable Integer Linear Programming’中可微性方法有误且有下游工作继承此错误，原因是替代损失在随机实现中不连续。


<details>
  <summary>Details</summary>
Motivation: 识别并指出已有论文中可微性方法的错误以及其在下游工作中的延续情况。

Method: 分析现有论文的理论（如该论文定理5），阐释替代损失在随机梯度下降中的连续性问题。

Result: 发现该论文的可微性方法不正确，且有下游工作继承同样错误。

Conclusion: 该论文‘Differentiable Integer Linear Programming’的可微性方法存在问题，因其替代损失的连续性存在不足。

Abstract: We show how the differentiability method employed in the paper ``Differentiable Integer Linear Programming'', Geng, et al., 2025 as shown in its theorem 5 is incorrect. Moreover, there already exists some downstream work that inherits the same error. The underlying reason comes from that, though being continuous in expectation, the surrogate loss is discontinuous in almost every realization of the randomness, for the stochastic gradient descent.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [350] [From Scores to Queues: Operationalizing Cross-Chain Obfuscation Signals for Smart-Contract Audits](https://arxiv.org/abs/2601.17356)
*Yao Zhao,Zhang Sheng,Shengchen Duan,Shen Wang*

Main category: cs.CR

TL;DR: 提出HObfNET实现跨链智能合约混淆信号快速评分，分析不同链上得分漂移，提供审计队列和工作流支持多链安全操作。


<details>
  <summary>Details</summary>
Motivation: 智能合约混淆增加审计成本，且跨链混淆信号可比性和可转移性不明。

Method: 提出HObfNET作为Obfs_Tool的高效替代模型进行跨链评分，分析不同链上得分漂移，确定链内主阈值和极端阈值，采用队列策略。

Result: HObfNET在以太坊上与工具输出高度一致，速度提升2.3k - 5.2k倍；发现不同链上得分漂移，高得分尾部有特定特征；跨链重用分析有尾部富集和定向扩散；公开事件样本都在p99队列。

Conclusion: 提供两层审计队列和跨链联动工作流，支持实际多链安全操作。

Abstract: Obfuscation substantially increases the interpretation cost of smart-contract auditing, while the comparability and transferability of obfuscation signals across chains remain unclear. We present HObfNET as an efficient surrogate of Obfs_Tool (ObfProbe), enabling fast cross-chain scoring at scale. The model aligns well with tool outputs on Ethereum (PCC 0.9158, MAPE 8.20 percent) and achieves 8-9 ms per contract, a 2.3k-5.2k times speedup over second-level Obfs_Tool runs, enabling million-scale scoring. On large BSC, Polygon, and Avalanche corpora, we find systematic score drift: fixed-threshold transfer inflates and deflates candidate queues, motivating within-chain main and extreme thresholds (p99 and p99.9) and an actionable queueing strategy. The high-score tail exhibits rare selectors, external-call opcode enrichment, and low signature density; a proxy indicator is enriched in the BSC high-score queue, enabling secondary triage. Cross-chain reuse analysis shows tail enrichment and directional diffusion, with traceable same-hash cases across chains. In publicly alignable incident samples, all fall into the p99 queue; Transit Swap DEX Hack and New Free DAO Flash Loan exhibit cross-chain spillover, indicating real-world hit and prioritization value. We deliver a two-tier audit queue and cross-chain linkage workflow to support practical multi-chain security operations.

</details>


### [351] [KeyMemRT Compiler and Runtime: Unlocking Memory-Scalable FHE](https://arxiv.org/abs/2601.18445)
*Eymen Ünay,Björn Franke,Jackson Woodruff*

Main category: cs.CR

TL;DR: 提出基于MLIR的编译器和运行时框架KeyMemRT来解决全同态加密(FHE)旋转密钥内存瓶颈问题，实现内存减少与速度提升。


<details>
  <summary>Details</summary>
Motivation: 全同态加密（FHE）存在高延迟和内存消耗问题，现有编译器对此解决不力，资源需求限制FHE应用。

Method: 提出KeyMemRT框架，依靠数据流分析确定密钥生命周期，实现自动密钥管理、细粒度密钥处理和引导密钥管理，实现了Orion和HEIR的前端。

Result: KeyMemRT相比ANT - ACE内存减少1.74倍、提速1.20倍，相比Fhelipe内存减少1.16倍、提速1.73倍。

Conclusion: 提供可被任何FHE编译器作为后优化编译器的KeyMemRT框架。

Abstract: Fully Homomorphic Encryption (FHE) enables privacy preserving computation but it suffers from high latency and memory consumption. The computations are secured with special keys called rotation keys which often take up the majority of memory. In complex FHE applications, these rotation keys can cause a large memory bottleneck limiting program throughput. Existing compilers make little effort to solve this problem, instead relying on systems with massive memory availability. This resource requirement is a barrier to FHE uptake because optimizing FHE programs by hand is challenging due to their scale, complexity and expertise required.
  In this work, we present KeyMemRT; an MLIR based compiler and runtime framework that individually manages rotation key lifetimes to lower memory utilization and to allow arbitrary number of rotation indices to be supported without memory bloating. KeyMemRT relies on dataflow analysis to determine key lifetimes and is the first FHE compiler to provide automatic key management, handle fine-grained key-mangement and manage boostrap keys. We implement frontends for Orion and HEIR and show improvements over state-of-the-art FHE compilers. KeyMemRT achieves memory reduction of 1.74x and a speedup of 1.20x over ANT-ACE, and memory reduction of 1.16x and a speedup of 1.73x over memory-optimized compiler Fhelipe. We provide KeyMemRT as a post-optimizing compiler that can be targeted by any FHE compiler.

</details>


### [352] [MultiChain Blockchain Data Provenance for Deterministic Stream Processing with Kafka Streams: A Weather Data Case Study](https://arxiv.org/abs/2601.18011)
*Niaz Mohammad Ramaki,Florian Schintke*

Main category: cs.CR

TL;DR: 本文提出区块链支持的流平台溯源架构，以解决实时数据流管道可审计和可重现问题，并用气象数据验证系统效果。


<details>
  <summary>Details</summary>
Motivation: 实时数据流管道的可审计性和可重现性面临挑战，流引擎依赖多种因素导致输出非确定性。

Method: 引入区块链支持的溯源架构，对窗口化数据流的加密数据进行处理，计算Merkle根并存储到区块链流作为检查点。

Result: 使用真实气象数据评估，验证成本呈线性，具有确定性可重现性，链下存储可扩展，每秒事务值令人满意。

Conclusion: 区块链可与流平台集成，该系统能有效解决相关问题。

Abstract: Auditability and reproducibility still are critical challenges for real-time data streams pipelines. Streaming engines are highly dependent on runtime scheduling, window triggers, arrival orders, and uncertainties such as network jitters. These all derive the streaming pipeline platforms to throw non-determinist outputs. In this work, we introduce a blockchain-backed provenance architecture for streaming platform (e.g Kafka Streams) the publishes cryptographic data of a windowed data stream without publishing window payloads on-chain. We used real-time weather data from weather stations in Berlin. Weather records are canonicalized, deduplicated, and aggregated per window, then serialised deterministically. Furthermore, the Merkle root of the records within the window is computed and stored alongside with Kafka offsets boundaries to MultiChain blockchain streams as checkpoints. Our design can enable an independent auditor to verify: (1) the completeness of window payloads, (2) canonical serialization, and (3) correctness of derived analytics such as minimum/maximum/average temperatures. We evaluated our system using real data stream from two weather stations (Berlin-Brandenburg and Berlin-Tempelhof) and showed linear verification cost, deterministic reproducibility, and with a scalable off-chain storage with on-chain cryptographic anchoring. We also demonstrated that the blockchain can afford to be integrated with streaming platforms particularly with our system, and we get satisfactory transactions per second values.

</details>


### [353] [Rhea: Detecting Privilege-Escalated Evasive Ransomware Attacks Using Format-Aware Validation in the Cloud](https://arxiv.org/abs/2601.18216)
*Beom Heyn Kim,Seok Min Hong,Mohammad Mannan*

Main category: cs.CR

TL;DR: 提出云卸载勒索软件防御系统Rhea，通过分析复制数据快照和格式感知验证检测勒索软件，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有依赖I/O模式分析和统计内容的勒索软件检测方法，面对特权升级的规避型勒索软件（PEER）效果不佳。

Method: 提出云卸载勒索软件防御系统Rhea，分析复制数据快照（突变快照），引入格式感知验证来验证文件格式的语法和语义正确性。

Result: 评估表明Rhea显著优于现有方法。

Conclusion: Rhea对现代勒索软件威胁具有实际有效性。

Abstract: Ransomware variants increasingly combine privilege escalation with sophisticated evasion strategies such as intermittent encryption, low-entropy encryption, and imitation attacks. Such powerful ransomware variants, privilege-escalated evasive ransomware (PEER), can defeat existing solutions relying on I/O-pattern analysis by tampering with or obfuscating I/O traces. Meanwhile, conventional statistical content-based detection becomes unreliable as the encryption size decreases due to sampling noises. We present Rhea, a cloud-offloaded ransomware defense system that analyzes replicated data snapshots, so-called mutation snapshots. Rhea introduces Format-Aware Validation that validates the syntactic and semantic correctness of file formats, instead of relying on statistical or entropy-based indicators. By leveraging file-format specifications as detection invariants, Rhea can reliably identify fine-grained and evasive encryption even under elevated attacker privileges. Our evaluation demonstrates that Rhea significantly outperforms existing approaches, establishing its practical effectiveness against modern ransomware threats.

</details>


### [354] [TrojanGYM: A Detector-in-the-Loop LLM for Adaptive RTL Hardware Trojan Insertion](https://arxiv.org/abs/2601.17178)
*Saideep Sreekumar,Zeng Wang,Akashdeep Saha,Weihua Xiao,Minghao Shao,Muhammad Shafique,Ozgur Sinanoglu,Ramesh Karri,Johann Knechtel*

Main category: cs.CR

TL;DR: 提出TrojanGYM框架自动策划硬件木马插入以暴露检测器盲点，还提出Robust - GNN4TJ提升检测率，实验展示其效果并将开源。


<details>
  <summary>Details</summary>
Motivation: 学习型硬件木马检测器常过拟合，难以检测多样的木马，需新方法暴露其盲点。

Method: 引入TrojanGYM框架，利用LLM代理提出和改进RTL修改，实现反馈驱动的基准生成循环；提出Robust - GNN4TJ改进GNN4TJ。

Result: 在TrojanGYM生成的基准上，Robust - GNN4TJ将检测率从0%提升到60%；TrojanGYM生成的木马对现代GNN检测器的逃避率达83.33%。

Conclusion: TrojanGYM能系统地产生多样、功能正确的硬件木马，揭示现有检测器在评估时不明显的鲁棒性差距。

Abstract: Hardware Trojans (HTs) remain a critical threat because learning-based detectors often overfit to narrow trigger/payload patterns and small, stylized benchmarks. We introduce TrojanGYM, an agentic, LLM-driven framework that automatically curates HT insertions to expose detector blind spots while preserving design correctness. Given high-level HT specifications, a suite of cooperating LLM agents (instantiated with GPT-4, LLaMA-3.3-70B, and Gemini-2.5Pro) proposes and refines RTL modifications that realize diverse triggers and payloads without impacting normal functionality. TrojanGYM implements a feedback-driven benchmark generation loop co-designed with HT detectors, in which constraint-aware syntactic checking and GNN-based HT detectors provide feedback that iteratively refines HT specifications and insertion strategies to better surface detector blind spots. We further propose Robust-GNN4TJ, a new implementation of the GNN4TJ with improved graph extraction, training robustness, and prediction reliability, especially on LLM-generated HT designs. On the most challenging TrojanGYM-generated benchmarks, Robust-GNN4TJ raises HT detection rates from 0% to 60% relative to a prior GNN-based detector. We instantiate TrojanGYM on SRAM, AES-128, and UART designs at RTL level, and show that it systematically produces diverse, functionally correct HTs that reach up to 83.33% evasion rates against modern GNN-based detectors, revealing robustness gaps that are not apparent when these detectors are evaluated solely on existing TrustHub-style benchmarks. Post peer-review, we will release all codes and artifacts.

</details>


### [355] [On the Insecurity of Keystroke-Based AI Authorship Detection: Timing-Forgery Attacks Against Motor-Signal Verification](https://arxiv.org/abs/2601.17280)
*David Condrey*

Main category: cs.CR

TL;DR: 研究表明利用击键时间信号区分人类与AI生成文本的防御方法不安全，提出两类攻击，实验显示高规避率，指出保障来源需结合写作过程与语义内容。


<details>
  <summary>Details</summary>
Motivation: 验证利用击键时间信号区分人类与AI生成文本的防御方法的安全性。

Method: 提出复制打字攻击和时间伪造攻击两类攻击，使用SBU语料库的13000个会话和三种时间伪造变体进行实验。

Result: 所有攻击对五个分类器的规避率≥99.8%，探测器将≥99.8%的攻击样本分类为人类，平均置信度≥0.993；形式化非可识别性结果；写作和转录产生的运动模式有差异但δ值均高于检测阈值。

Conclusion: 此类防御方法不安全，保障文本来源需将写作过程与语义内容绑定的架构。

Abstract: Recent proposals advocate using keystroke timing signals, specifically the coefficient of variation ($δ$) of inter-keystroke intervals, to distinguish human-composed text from AI-generated content. We demonstrate that this class of defenses is insecure against two practical attack classes: the copy-type attack, in which a human transcribes LLM-generated text producing authentic motor signals, and timing-forgery attacks, in which automated agents sample inter-keystroke intervals from empirical human distributions. Using 13,000 sessions from the SBU corpus and three timing-forgery variants (histogram sampling, statistical impersonation, and generative LSTM), we show all attacks achieve $\ge$99.8% evasion rates against five classifiers. While detectors achieve AUC=1.000 against fully-automated injection, they classify $\ge$99.8% of attack samples as human with mean confidence $\ge$0.993. We formalize a non-identifiability result: when the detector observes only timing, the mutual information between features and content provenance is zero for copy-type attacks. Although composition and transcription produce statistically distinguishable motor patterns (Cohen's d=1.28), both yield $δ$ values 2-4x above detection thresholds, rendering the distinction security-irrelevant. These systems confirm a human operated the keyboard, but not whether that human originated the text. Securing provenance requires architectures that bind the writing process to semantic content.

</details>


### [356] [Res-MIA: A Training-Free Resolution-Based Membership Inference Attack on Federated Learning Models](https://arxiv.org/abs/2601.17378)
*Mohammad Zare,Pirooz Shamsinejadbabaki*

Main category: cs.CR

TL;DR: 提出Res - MIA攻击，利用模型对高频输入细节的敏感性，在联邦学习模型上表现优于现有无训练基线，揭示频率敏感过拟合是隐私泄露来源。


<details>
  <summary>Details</summary>
Motivation: 成员推理攻击威胁机器学习模型隐私，联邦学习最终全局模型仍会泄露成员信息。

Method: 提出Res - MIA攻击，通过控制下采样和恢复操作降低输入分辨率，分析模型预测的置信度衰减。

Result: 在联邦ResNet - 18上评估，持续优于现有无训练基线，AUC达0.88且计算开销小。

Conclusion: 频率敏感过拟合是联邦学习中重要且未充分探索的隐私泄露源，需设计减少对细粒度非鲁棒输入特征依赖的隐私保护模型。

Abstract: Membership inference attacks (MIAs) pose a serious threat to the privacy of machine learning models by allowing adversaries to determine whether a specific data sample was included in the training set. Although federated learning (FL) is widely regarded as a privacy-aware training paradigm due to its decentralized nature, recent evidence shows that the final global model can still leak sensitive membership information through black-box access. In this paper, we introduce Res-MIA, a novel training-free and black-box membership inference attack that exploits the sensitivity of deep models to high-frequency input details. Res-MIA progressively degrades the input resolution using controlled downsampling and restoration operations, and analyzes the resulting confidence decay in the model's predictions. Our key insight is that training samples exhibit a significantly steeper confidence decline under resolution erosion compared to non-member samples, revealing a robust membership signal. Res-MIA requires no shadow models, no auxiliary data, and only a limited number of forward queries to the target model. We evaluate the proposed attack on a federated ResNet-18 trained on CIFAR-10, where it consistently outperforms existing training-free baselines and achieves an AUC of up to 0.88 with minimal computational overhead. These findings highlight frequency-sensitive overfitting as an important and previously underexplored source of privacy leakage in federated learning, and emphasize the need for privacy-aware model designs that reduce reliance on fine-grained, non-robust input features.

</details>


### [357] [Prompt and Circumstances: Evaluating the Efficacy of Human Prompt Inference in AI-Generated Art](https://arxiv.org/abs/2601.17379)
*Khoi Trinh,Scott Seidenberger,Joseph Spracklen,Raveen Wijewickrama,Bimal Viswanath,Murtuza Jadliwala,Anindya Maiti*

Main category: cs.CR

TL;DR: 研究AI生成艺术提示市场中隐藏提示能否作为知识产权，评估人类和人机结合推断提示效果，发现不如原提示。


<details>
  <summary>Details</summary>
Motivation: 探究提示市场上隐藏提示能否作为知识产权，因人类和AI可根据样本图推断提示。

Method: 开展人类受试者研究，结合大语言模型进行人机协作提示推断。

Result: 人类和人机结合推断的提示生成图像有一定相似度，但不如原提示，且结合方法未提升性能。

Conclusion: 人类和人机结合推断提示生成图像效果不如原提示，结合方法未改善性能。

Abstract: The emerging field of AI-generated art has witnessed the rise of prompt marketplaces, where creators can purchase, sell, or share prompts to generate unique artworks. These marketplaces often assert ownership over prompts, claiming them as intellectual property. This paper investigates whether concealed prompts sold on prompt marketplaces can be considered bona fide intellectual property, given that humans and AI tools may be able to infer the prompts based on publicly advertised sample images accompanying each prompt on sale. Specifically, our study aims to assess (i) how accurately humans can infer the original prompt solely by examining an AI-generated image, with the goal of generating images similar to the original image, and (ii) the possibility of improving upon individual human and AI prompt inferences by crafting combined human and AI prompts with the help of a large language model. Although previous research has explored AI-driven prompt inference and protection strategies, our work is the first to incorporate a human subject study and examine collaborative human-AI prompt inference in depth. Our findings indicate that while prompts inferred by humans and prompts inferred through a combined human and AI effort can generate images with a moderate level of similarity, they are not as successful as using the original prompt. Moreover, combining human- and AI-inferred prompts using our suggested merging techniques did not improve performance over purely human-inferred prompts.

</details>


### [358] [Reconstructing Training Data from Adapter-based Federated Large Language Models](https://arxiv.org/abs/2601.17533)
*Silong Chen,Yuchuan Luo,Guilin Deng,Yi Liu,Min Xu,Shaojing Fu,Xiaohua Jia*

Main category: cs.CR

TL;DR: 本文指出基于适配器的联邦大语言模型存在新的梯度泄漏通道，提出UTR攻击方法，实验表明该方法有高重建准确率，揭示了参数效率和隐私间的矛盾。


<details>
  <summary>Details</summary>
Motivation: 基于适配器的联邦大语言模型被认为能限制梯度泄漏，但作者发现其存在新的可利用泄漏通道，因此进行研究。

Method: 提出Unordered - word - bag - based Text Reconstruction (UTR)攻击，通过从冻结层注意力模式推断标记存在、在适配器梯度低秩子空间进行句子级反转、利用语言先验引导的约束贪婪解码确保语义连贯来克服核心挑战。

Result: 在多种模型和数据集上实验，UTR即使在大批次设置下也能达到近完美的重建准确率（ROUGE - 1/2 > 99），而之前的梯度反转攻击完全失败。

Conclusion: 揭示了联邦大语言模型中参数效率和隐私之间存在根本矛盾，挑战了轻量级自适应能增强安全性的普遍观点。

Abstract: Adapter-based Federated Large Language Models (FedLLMs) are widely adopted to reduce the computational, storage, and communication overhead of full-parameter fine-tuning for web-scale applications while preserving user privacy. By freezing the backbone and training only compact low-rank adapters, these methods appear to limit gradient leakage and thwart existing Gradient Inversion Attacks (GIAs).
  Contrary to this assumption, we show that low-rank adapters create new, exploitable leakage channels. We propose the Unordered-word-bag-based Text Reconstruction (UTR) attack, a novel GIA tailored to the unique structure of adapter-based FedLLMs. UTR overcomes three core challenges: low-dimensional gradients, frozen backbones, and combinatorially large reconstruction spaces by: (i) inferring token presence from attention patterns in frozen layers, (ii) performing sentence-level inversion within the low-rank subspace of adapter gradients, and (iii) enforcing semantic coherence through constrained greedy decoding guided by language priors. Extensive experiments across diverse models (GPT2-Large, BERT, Qwen2.5-7B) and datasets (CoLA, SST-2, Rotten Tomatoes) demonstrate that UTR achieves near-perfect reconstruction accuracy (ROUGE-1/2 > 99), even with large batch size settings where prior GIAs fail completely. Our results reveal a fundamental tension between parameter efficiency and privacy in FedLLMs, challenging the prevailing belief that lightweight adaptation inherently enhances security. Our code and data are available at https://github.com/shwksnshwowk-wq/GIA.

</details>


### [359] [Breaking the Protocol: Security Analysis of the Model Context Protocol Specification and Prompt Injection Vulnerabilities in Tool-Integrated LLM Agents](https://arxiv.org/abs/2601.17549)
*Narek Maloyan,Dmitry Namiot*

Main category: cs.CR

TL;DR: 对MCP进行安全分析，发现漏洞，提出改进协议MCPSec降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有MCP协议缺乏正式安全分析，需对其架构设计进行安全评估。

Method: 对MCP架构设计进行严格安全分析，实现MCPBench框架，进行847个攻击场景实验。

Result: MCP架构设计使攻击成功率较非MCP集成高23 - 41%，MCPSec将攻击成功率从52.8%降至12.4%，消息中位延迟8.3ms。

Conclusion: MCP安全弱点是架构层面问题，需进行协议级修复。

Abstract: The Model Context Protocol (MCP) has emerged as a de facto standard for integrating Large Language Models with external tools, yet no formal security analysis of the protocol specification exists. We present the first rigorous security analysis of MCP's architectural design, identifying three fundamental protocol-level vulnerabilities: (1) absence of capability attestation allowing servers to claim arbitrary permissions, (2) bidirectional sampling without origin authentication enabling server-side prompt injection, and (3) implicit trust propagation in multi-server configurations. We implement \textsc{MCPBench}, a novel framework bridging existing agent security benchmarks to MCP-compliant infrastructure, enabling direct measurement of protocol-specific attack surfaces. Through controlled experiments on 847 attack scenarios across five MCP server implementations, we demonstrate that MCP's architectural choices amplify attack success rates by 23--41\% compared to equivalent non-MCP integrations. We propose \textsc{MCPSec}, a backward-compatible protocol extension adding capability attestation and message authentication, reducing attack success rates from 52.8\% to 12.4\% with median latency overhead of 8.3ms per message. Our findings establish that MCP's security weaknesses are architectural rather than implementation-specific, requiring protocol-level remediation.

</details>


### [360] [A Systemic Evaluation of Multimodal RAG Privacy](https://arxiv.org/abs/2601.17644)
*Ali Al-Lawati,Suhang Wang*

Main category: cs.CR

TL;DR: 研究mRAG管道在视觉任务中的隐私风险，强调保护隐私机制的必要性。


<details>
  <summary>Details</summary>
Motivation: mRAG在视觉任务应用中存在隐私泄露风险，需分析其固有隐私风险。

Method: 通过标准模型提示进行实证研究，实施案例研究推断视觉资产是否包含在mRAG中并泄露相关元数据。

Result: 发现了mRAG管道中存在的隐私风险。

Conclusion: 强调需要隐私保护机制，推动mRAG隐私方面的未来研究。

Abstract: The growing adoption of multimodal Retrieval-Augmented Generation (mRAG) pipelines for vision-centric tasks (e.g. visual QA) introduces important privacy challenges. In particular, while mRAG provides a practical capability to connect private datasets to improve model performance, it risks the leakage of private information from these datasets during inference. In this paper, we perform an empirical study to analyze the privacy risks inherent in the mRAG pipeline observed through standard model prompting. Specifically, we implement a case study that attempts to infer the inclusion of a visual asset, e.g. image, in the mRAG, and if present leak the metadata, e.g. caption, related to it. Our findings highlight the need for privacy-preserving mechanisms and motivate future research on mRAG privacy.

</details>


### [361] [Mitigating the OWASP Top 10 For Large Language Models Applications using Intelligent Agents](https://arxiv.org/abs/2601.18105)
*Mohammad Fasha,Faisal Abul Rub,Nasim Matar,Bilal Sowan,Mohammad Al Khaldy*

Main category: cs.CR

TL;DR: 本文针对大语言模型应用安全漏洞，提出利用智能代理的框架来实时应对安全威胁，为未来研究提供蓝图。


<details>
  <summary>Details</summary>
Motivation: 大语言模型广泛应用带来安全隐患，OWASP指出其应用的十大安全漏洞，需解决这些漏洞以保障数据与服务安全。

Method: 提出利用大语言模型支持的智能代理的框架，实时识别、评估和应对安全威胁。

Result: 提出了可缓解OWASP十大安全风险的框架。

Conclusion: 该框架为未来研究提供初步蓝图，可增强大语言模型安全措施，抵御新兴威胁。

Abstract: Large Language Models (LLMs) have emerged as a transformative and disruptive technology, enabling a wide range of applications in natural language processing, machine translation, and beyond. However, this widespread integration of LLMs also raised several security concerns highlighted by the Open Web Application Security Project (OWASP), which has identified the top 10 security vulnerabilities inherent in LLM applications. Addressing these vulnerabilities is crucial, given the increasing reliance on LLMs and the potential threats to data integrity, confidentiality, and service availability. This paper presents a framework designed to mitigate the security risks outlined in the OWASP Top 10. Our proposed model leverages LLM-enabled intelligent agents, offering a new approach to proactively identify, assess, and counteract security threats in real-time. The proposed framework serves as an initial blueprint for future research and development, aiming to enhance the security measures of LLMs and protect against emerging threats in this rapidly evolving landscape.

</details>


### [362] [MalURLBench: A Benchmark Evaluating Agents' Vulnerabilities When Processing Web URLs](https://arxiv.org/abs/2601.18113)
*Dezhang Kong,Zhuxi Wu,Shiqi Liu,Zhicheng Tan,Kuichen Lu,Minghao Li,Qichen Liu,Shengyu Chu,Zhenhua Xu,Xuan Liu,Meng Han*

Main category: cs.CR

TL;DR: 提出首个评估大语言模型对恶意URL漏洞的基准测试MalURLBench，实验发现现有模型难检测伪装恶意URL，还提出轻量级防御模块URLGuard。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的网络代理处理恶意URL时存在关键漏洞，但缺乏针对此新兴威胁的基准测试。

Method: 提出MalURLBench基准测试，包含61,845个攻击实例，涵盖10个现实场景和7类真实恶意网站；对12个流行大语言模型进行实验；分析影响攻击成功率的关键因素并提出URLGuard防御模块。

Result: 现有模型难以检测精心伪装的恶意URL。

Conclusion: 该工作将为提升网络代理的安全性提供基础资源。

Abstract: LLM-based web agents have become increasingly popular for their utility in daily life and work. However, they exhibit critical vulnerabilities when processing malicious URLs: accepting a disguised malicious URL enables subsequent access to unsafe webpages, which can cause severe damage to service providers and users. Despite this risk, no benchmark currently targets this emerging threat. To address this gap, we propose MalURLBench, the first benchmark for evaluating LLMs' vulnerabilities to malicious URLs. MalURLBench contains 61,845 attack instances spanning 10 real-world scenarios and 7 categories of real malicious websites. Experiments with 12 popular LLMs reveal that existing models struggle to detect elaborately disguised malicious URLs. We further identify and analyze key factors that impact attack success rates and propose URLGuard, a lightweight defense module. We believe this work will provide a foundational resource for advancing the security of web agents. Our code is available at https://github.com/JiangYingEr/MalURLBench.

</details>


### [363] [FARM: Few-shot Adaptive Malware Family Classification under Concept Drift](https://arxiv.org/abs/2601.17907)
*Numan Halit Guldemir,Oluwafemi Olukoya,Jesús Martínez-del-Rincón*

Main category: cs.CR

TL;DR: 本文提出FARM框架用于Windows PE恶意软件分类，能检测并适应漂移，实验显示其在动态环境下有良好性能。


<details>
  <summary>Details</summary>
Motivation: 解决恶意软件分类模型因概念漂移导致的性能下降问题。

Method: 利用三元组自动编码器将样本投影到潜在空间，通过DBSCAN聚类和动态阈值进行无监督漂移检测，采用基于原型分类的少样本学习进行快速适应，积累足够漂移样本时支持全量再训练。

Result: 在BenchMFC数据集上，FARM在协变量漂移下分类性能提升5.6%，少样本适应时对未见恶意软件家族平均F1分数达0.85，再训练后增至0.94。

Conclusion: FARM在有限监督的动态恶意软件检测环境中具有鲁棒性和适应性。

Abstract: Malware classification models often face performance degradation due to concept drift, arising from evolving threat landscapes and the emergence of novel malware families. This paper presents FARM (Few-shot Adaptive Recognition of Malware), a framework designed to detect and adapt to both covariate and label drift in Windows Portable Executable (PE) malware classification. FARM leverages a triplet autoencoder to project samples into a discriminative latent space, enabling unsupervised drift detection via DBSCAN clustering and dynamic thresholding. For rapid adaptation, it employs few-shot learning using prototype-based classification, requiring only a handful of labeled samples. FARM also supports full retraining when enough drifted samples accumulate, updating the latent space for long-term integration. Experiments on the BenchMFC dataset demonstrate that FARM improves classification performance under covariate drift by 5.6\%, and achieves an average F1 score of 0.85 on unseen malware families using only few-shot adaptation, which further increases to 0.94 after retraining. These results highlight FARM's robustness and adaptability in dynamic malware detection environments under limited supervision.

</details>


### [364] [XGuardian: Towards Explainable and Generalized AI Anti-Cheat on FPS Games](https://arxiv.org/abs/2601.18068)
*Jiayi Zhang,Chenxin Sun,Chenxiong Qian*

Main category: cs.CR

TL;DR: 提出XGuardian系统检测FPS游戏的瞄准辅助作弊，实现高检测性能、低开销、强泛化能力，可解释预测结果并公开系统和数据集。


<details>
  <summary>Details</summary>
Motivation: 现有检测瞄准辅助作弊的研究存在框架不可靠、泛化能力有限、开销高、检测性能低和结果缺乏可解释性等问题。

Method: XGuardian系统只需要俯仰和偏航两个原始数据输入来构建新的时间特征和描述瞄准轨迹，区分作弊者和正常玩家。

Result: 在CS2及另外两款不同游戏上评估，在不同游戏和大规模真实数据集上实现高检测性能和低开销，验证了泛化能力，并能解释预测。

Conclusion: XGuardian系统能有效检测FPS游戏瞄准辅助作弊，具有广泛泛化性和高有效性，可缩短封禁周期。

Abstract: Aim-assist cheats are the most prevalent and infamous form of cheating in First-Person Shooter (FPS) games, which help cheaters illegally reveal the opponent's location and auto-aim and shoot, and thereby pose significant threats to the game industry. Although a considerable research effort has been made to automatically detect aim-assist cheats, existing works suffer from unreliable frameworks, limited generalizability, high overhead, low detection performance, and a lack of explainability of detection results. In this paper, we propose XGuardian, a server-side generalized and explainable system for detecting aim-assist cheats to overcome these limitations. It requires only two raw data inputs, pitch and yaw, which are all FPS games' must-haves, to construct novel temporal features and describe aim trajectories, which are essential for distinguishing cheaters and normal players. XGuardian is evaluated with the latest mainstream FPS game CS2, and validates its generalizability with another two different games. It achieves high detection performance and low overhead compared to prior works across different games with real-world and large-scale datasets, demonstrating wide generalizability and high effectiveness. It is able to justify its predictions and thereby shorten the ban cycle. We make XGuardian as well as our datasets publicly available.

</details>


### [365] [$α^3$-SecBench: A Large-Scale Evaluation Suite of Security, Resilience, and Trust for LLM-based UAV Agents over 6G Networks](https://arxiv.org/abs/2601.18754)
*Mohamed Amine Ferrag,Abderrahmane Lakas,Merouane Debbah*

Main category: cs.CR

TL;DR: 提出首个评估套件$α^{3}$-SecBench评估基于大语言模型的无人机代理在对抗干扰下的安全自主性，评估23个模型，发现异常检测与安全自主决策有差距并开源套件。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对基于大语言模型的无人机代理在对抗条件下的安全性、恢复力和信任度的系统评估，尤其是在6G环境中。

Method: 构建$α^{3}$-SecBench评估套件，在$α^{3}$-Bench基础上增加20,000个攻击场景，从安全、恢复力和信任三个维度评估。

Result: 评估23个模型，很多模型能检测异常行为，但有效缓解、漏洞归因和可信控制行动表现不一致，整体得分12.9% - 57.1%。

Conclusion: 异常检测和安全自主决策存在显著差距，开源$α^{3}$-SecBench评估套件。

Abstract: Autonomous unmanned aerial vehicle (UAV) systems are increasingly deployed in safety-critical, networked environments where they must operate reliably in the presence of malicious adversaries. While recent benchmarks have evaluated large language model (LLM)-based UAV agents in reasoning, navigation, and efficiency, systematic assessment of security, resilience, and trust under adversarial conditions remains largely unexplored, particularly in emerging 6G-enabled settings.
  We introduce $α^{3}$-SecBench, the first large-scale evaluation suite for assessing the security-aware autonomy of LLM-based UAV agents under realistic adversarial interference. Building on multi-turn conversational UAV missions from $α^{3}$-Bench, the framework augments benign episodes with 20,000 validated security overlay attack scenarios targeting seven autonomy layers, including sensing, perception, planning, control, communication, edge/cloud infrastructure, and LLM reasoning. $α^{3}$-SecBench evaluates agents across three orthogonal dimensions: security (attack detection and vulnerability attribution), resilience (safe degradation behavior), and trust (policy-compliant tool usage).
  We evaluate 23 state-of-the-art LLMs from major industrial providers and leading AI labs using thousands of adversarially augmented UAV episodes sampled from a corpus of 113,475 missions spanning 175 threat types. While many models reliably detect anomalous behavior, effective mitigation, vulnerability attribution, and trustworthy control actions remain inconsistent. Normalized overall scores range from 12.9% to 57.1%, highlighting a significant gap between anomaly detection and security-aware autonomous decision-making. We release $α^{3}$-SecBench on GitHub: https://github.com/maferrag/AlphaSecBench

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [366] [Types for Grassroots Logic Programs](https://arxiv.org/abs/2601.17957)
*Ehud Shapiro*

Main category: cs.PL

TL;DR: 文章介绍Grassroots Logic Programs (GLP)语言，重新定义类型为模式化路径的正则集，实现GLP类型系统，并提出人与AI共同开发GLP的策略。


<details>
  <summary>Details</summary>
Motivation: 直接让AI在GLP中编写复杂通信模式程序不可靠，需人和AI共同开发。

Method: 定义类型为模路径的正则集，提供良类型的语法定义并证明相关条件，从Typed GLP数学规范出发，经英文规范，由AI实现Dart代码。

Result: 实现了GLP类型系统。

Conclusion: 倡导人和AI共同开发GLP类型、过程类型声明和程序描述，再让AI编写代码。

Abstract: Grassroots Logic Programs (GLP) is a concurrent logic programming language in which logic variables are partitioned into paired readers and writers. An assignment is produced at most once via a writer and consumed at most once via its paired reader, and may contain additional readers and/or writers. This enables the concise expression of rich multidirectional communication modalities.
  ``Logic Programs as Types for Logic Programs'' (LICS'91) defined types as regular sets of paths over derivable ground atoms. Here, we define types to be regular sets of moded paths, where a mode captures directionality of communication -- whether a subterm is consumed from or produced to the environment -- enabling the typing of interactive partial computations including those that eventually deadlock or fail, or never terminate. We provide a syntactic definition of well-typing and prove that a program is well-typed iff the path abstraction of its moded-atom semantics satisfies covariance and contravariance conditions with respect to its type.
  The GLP type system was implemented in Dart by AI, starting from a mathematical specification of Typed GLP (this paper), deriving from it an English spec (written by AI), and from the spec deriving Dart code (by AI). While GLP is naturally untyped, the motivation for Typed GLP comes from programming with AI: Asking AI to program complex communication modalities in GLP (and in general) and hoping for the best is a tenuous strategy. The emerging discipline we advocate and employ is for the human designer and AI to jointly develop and agree upon (1)~GLP types; (2)~GLP procedure type declarations; (3)~informal (English) descriptions of the procedures; and only then let AI attempt to write (4)~GLP code based on those.

</details>


### [367] [Grammar-Aware Literate Generative Mathematical Programming with Compiler-in-the-Loop](https://arxiv.org/abs/2601.17670)
*Roberto Rossi,Steven D. Prestwich*

Main category: cs.PL

TL;DR: 本文通过AMLs和编译器引导的模型合成研究生成式数学规划，提出SyntAGM系统，在对比研究中表现良好。


<details>
  <summary>Details</summary>
Motivation: 研究生成式数学规划，将自然语言问题描述转化为PyOPL模型。

Method: 利用PyOPL，通过生成 - 编译 - 评估 - 修改循环，借助编译器反馈和LLM对齐判断器。

Result: SyntAGM在对比研究中达到有竞争力的准确率，且在令牌、成本和延迟方面表现出色。

Conclusion: SyntAGM系统在将自然语言转化为PyOPL模型方面是有效的。

Abstract: This work investigates generative mathematical programming through the lens of Algebraic Modelling Languages (AMLs) and compiler-guided model synthesis. By leveraging PyOPL, an OPL-like AML compiler that provides detailed syntax diagnostics, we introduce SyntAGM, an end-to-end system that translates natural language problem descriptions into PyOPL models via a generate--compile--assess--revise loop. SyntAGM is grammar-aware thanks to in-context exposure to the PyOPL BNF grammar, and benefits from few-shot retrieval of literate PyOPL model exemplars. To obtain a valid PyOPL model that matches the problem description, SyntAGM mobilises compiler feedback and an LLM-based alignment judge. In a comparative study against established prompting baselines SyntAGM achieves competitive accuracy with superior token, cost, and latency profiles.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [368] [From annual to quarterly data: challenges and strategies in the estimation of Italian General Government Compensation of employees](https://arxiv.org/abs/2601.16997)
*Sara Cannavacciuolo,Maria Saiz,Maria Liviana Mattonetti*

Main category: econ.GN

TL;DR: 本文探讨按照ESA 2010对政府部门员工薪酬进行季度估算的方法，介绍了估算面临的挑战、采用的技术，通过统计测试验证，结果证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 解决因高频数据有限和需与年度约束保持一致，对政府部门员工薪酬进行季度估算的问题。

Method: 采用间接时间分解技术，使用特定年内指标作为代理变量；运用多源数据协调技术处理特殊事件；按特定实体组收集信息并汇总到ESA 2010子部门；全程应用统计测试。

Result: 该方法能为政府部门员工薪酬提供准确及时的季度估算。

Conclusion: 此方法有效，支持可靠的短期经济分析和政策制定。

Abstract: This paper addresses the methodology for the quarterly estimation of Compensation of Employees paid by the General Government (GG) sector, in accordance with the European System of Accounts (ESA 2010). Due to the limited high-frequency data availability and the need to guarantee the consistency with annual constraints, quarterly estimation relies on indirect temporal disaggregation techniques. These methods use specific infra-annual indicators as proxies for the variables being estimated. The specific case of the quarterly estimation of Compensation of employees presents several additional challenges. Firstly, the information provided by the sources, based on cash or legal-accrual data, is elaborated to define indicators which respect the accrual ESA 2010 principle as the annual estimates, based on more compliant data sources such as final budgets of public entities. Secondly, at a quarterly level the extraordinary events - such as the recording of delayed collective bargaining agreements which result in arrears - have a strong impact on quarterly indicators, whereas their effect is mitigated at annual level. To attribute these flows to the period when the work is performed, multi-source data harmonization techniques are employed. Thirdly, to accurately reflect intra-annual dynamics, information is collected for specific groups of GG entities (e.g., regions and provinces) and aggregated into ESA 2010 GG sub-sectors (Central Government, Local Government, Social Security Funds) leading to three specific estimates. To validate temporal disaggregation models and ensure methodological rigor and data quality, statistical tests are applied throughout the process. The results confirm the effectiveness of this methodology in providing accurate and timely quarterly estimates of Compensation of employees for the GG sector, thereby supporting reliable short-term economic analysis and policy making.

</details>


### [369] [Bridging Expectation Signals: LLM-Based Experiments and a Behavioral Kalman Filter Framework](https://arxiv.org/abs/2601.17527)
*Yu Wang,Xiangchen Liu*

Main category: econ.GN

TL;DR: 研究大语言模型（LLM）作为经济主体更新信念的机制，发现更新模式及LoRA微调可缓解偏差。


<details>
  <summary>Details</summary>
Motivation: LLM作为经济主体时，其用异质信号更新信念的机制不透明。

Method: 设计实验并开发行为卡尔曼滤波框架，量化基于LLM的主体（家庭或企业CEO）在面对个体和总体信号时如何更新预期。

Result: 发现四个一致模式，如主体对先验和信号的权重偏离统一、更重视个体信号等；LoRA微调可缓解期望形成中的行为偏差但不能完全消除。

Conclusion: 揭示了LLM作为经济主体更新信念的模式及LoRA微调对偏差的缓解作用。

Abstract: As LLMs increasingly function as economic agents, the specific mechanisms LLMs use to update their belief with heterogeneous signals remain opaque. We design experiments and develop a Behavioral Kalman Filter framework to quantify how LLM-based agents update expectations, acting as households or firm CEOs, update expectations when presented with individual and aggregate signals. The results from experiments and model estimation reveal four consistent patterns: (1) agents' weighting of priors and signals deviates from unity; (2) both household and firm CEO agents place substantially larger weights on individual signals compared to aggregate signals; (3) we identify a significant and negative interaction between concurrent signals, implying that the presence of multiple information sources diminishes the marginal weight assigned to each individual signal; and (4) expectation formation patterns differ significantly between household and firm CEO agents. Finally, we demonstrate that LoRA fine-tuning mitigates, but does not fully eliminate, behavioral biases in LLM expectation formation.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [370] [A new approach for combined model class selection and parameters learning for auto-regressive neural models](https://arxiv.org/abs/2601.17442)
*Corrado Sgadari,Alessio La Bella,Marcello Farina*

Main category: eess.SY

TL;DR: 提出针对非线性动力系统识别的模型结构选择与参数学习联合方法，对NARXESNs有效。


<details>
  <summary>Details</summary>
Motivation: 解决非线性动力系统识别中模型结构选择和参数学习的问题。

Method: 采用基于集合隶属（SM）的新程序，针对NARXESNs同时选择最优模型类并从数据中学习模型参数。

Result: 该方法能识别出适用于控制应用的简洁且准确的模型，还实现了鲁棒训练策略。

Conclusion: 所提方法在非线性动力系统识别中有效，能增强模型鲁棒性。

Abstract: This work introduces a novel approach for the joint selection of model structure and parameter learning for nonlinear dynamical systems identification. Focusing on a specific Recurrent Neural Networks (RNNs) family, i.e., Nonlinear Auto-Regressive with eXogenous inputs Echo State Networks (NARXESNs), the method allows to simultaneously select the optimal model class and learn model parameters from data through a new set-membership (SM) based procedure. The results show the effectiveness of the approach in identifying parsimonious yet accurate models suitable for control applications. Moreover, the proposed framework enables a robust training strategy that explicitly accounts for bounded measurement noise and enhances model robustness by allowing data-consistent evaluation of simulation performance during parameter learning, a process generally NP-hard for models with autoregressive components.

</details>


### [371] [Convex Chance-Constrained Stochastic Control under Uncertain Specifications with Application to Learning-Based Hybrid Powertrain Control](https://arxiv.org/abs/2601.18313)
*Teruki Kato,Ryotaro Shima,Kenji Kashima*

Main category: eess.SY

TL;DR: 提出严格凸机会约束随机控制框架，考虑控制规范不确定性，优化控制输入和风险分配，扩展到非线性控制，在混合动力系统验证有效性。


<details>
  <summary>Details</summary>
Motivation: 解决控制规范中参考轨迹和操作约束等存在的不确定性问题。

Method: 联合优化控制输入和风险分配，利用机器学习识别可精确线性化模型将其扩展到非线性模型控制。

Result: 保证概率约束满足，确保最优解的唯一性和连续性，在混合动力系统模型预测控制中展示有效性。

Conclusion: 所提出的严格凸机会约束随机控制框架有效可行。

Abstract: This paper presents a strictly convex chance-constrained stochastic control framework that accounts for uncertainty in control specifications such as reference trajectories and operational constraints. By jointly optimizing control inputs and risk allocation under general (possibly non-Gaussian) uncertainties, the proposed method guarantees probabilistic constraint satisfaction while ensuring strict convexity, leading to uniqueness and continuity of the optimal solution. The formulation is further extended to nonlinear model-based control using exactly linearizable models identified through machine learning. The effectiveness of the proposed approach is demonstrated through model predictive control applied to a hybrid powertrain system.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [372] [Laser interferometry as a robust neuromorphic platform for machine learning](https://arxiv.org/abs/2601.18047)
*Amanuel Anteneh,Kyungeun Kim,J. M. Schwarz,Israel Klich,Olivier Pfister*

Main category: physics.optics

TL;DR: 提出仅用线性光学资源实现光神经网络的方法，能原位推理和训练，且模型对光子损耗有强耐受性。


<details>
  <summary>Details</summary>
Motivation: 寻找比之前方案更易实验实现的光神经网络实现方法，并实现原位训练。

Method: 利用场位移和干涉技术处理相干态光，通过将输入编码为相移实现神经网络学习所需的非线性，利用参数转移规则或物理反向传播从线性光电路测量中提取梯度。

Result: 该方法可实现原位推理和训练，且模型对光子损耗具有很高的耐受性。

Conclusion: 此仅用线性光学资源实现光神经网络的方法在实验实现和抗损耗方面表现良好。

Abstract: We present a method for implementing an optical neural network using only linear optical resources, namely field displacement and interferometry applied to coherent states of light. The nonlinearity required for learning in a neural network is realized via an encoding of the input into phase shifts allowing for far more straightforward experimental implementation compared to previous proposals for, and demonstrations of, $\textit{in situ}$ inference. Beyond $\textit{in situ}$ inference, the method enables $\textit{in situ}$ training by utilizing established techniques like parameter shift rules or physical backpropagation to extract gradients directly from measurements of the linear optical circuit. We also investigate the effect of photon losses and find the model to be very resilient to these.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [373] [Analyzing Images of Blood Cells with Quantum Machine Learning Methods: Equilibrium Propagation and Variational Quantum Circuits to Detect Acute Myeloid Leukemia](https://arxiv.org/abs/2601.18710)
*A. Bano,L. Liebovitch*

Main category: cs.ET

TL;DR: 研究表明量子机器学习算法在医疗影像中受限情况下仍具竞争力，给出量子方法检测急性髓系白血病结果。


<details>
  <summary>Details</summary>
Motivation: 验证量子机器学习算法在现实医疗影像中，即使在严重约束条件下的可行性。

Method: 评估无需反向传播的基于能量学习方法平衡传播（EP）和变分量子电路（VQC），对血细胞显微镜图像进行急性髓系白血病（AML）自动二元分类。

Result: 使用AML数据集有限子集，量子方法性能仅比经典CNN低12 - 15%，EP准确率达86.4%，4 - 量子比特VQC达83.0%，且VQC数据效率高。

Conclusion: 为医疗保健中的量子机器学习建立可重复基线，验证了NISQ时代可行性。

Abstract: This paper presents a feasibility study demonstrating that quantum machine learning (QML) algorithms achieve competitive performance on real-world medical imaging despite operating under severe constraints. We evaluate Equilibrium Propagation (EP), an energy-based learning method that does not use backpropagation (incompatible with quantum systems due to state-collapsing measurements) and Variational Quantum Circuits (VQCs) for automated detection of Acute Myeloid Leukemia (AML) from blood cell microscopy images using binary classification (2 classes: AML vs. Healthy).
  Key Result: Using limited subsets (50-250 samples per class) of the AML-Cytomorphology dataset (18,365 expert-annotated images), quantum methods achieve performance only 12-15% below classical CNNs despite reduced image resolution (64x64 pixels), engineered features (20D), and classical simulation via Qiskit. EP reaches 86.4% accuracy (only 12% below CNN) without backpropagation, while the 4-qubit VQC attains 83.0% accuracy with consistent data efficiency: VQC maintains stable 83% performance with only 50 samples per class, whereas CNN requires 250 samples (5x more data) to reach 98%. These results establish reproducible baselines for QML in healthcare, validating NISQ-era feasibility.

</details>


<div id='math.LO'></div>

# math.LO [[Back]](#toc)

### [374] [Uniform Computability of PAC Learning](https://arxiv.org/abs/2601.18663)
*Vasco Brattka,Guillaume Chirache*

Main category: math.LO

TL;DR: 使用Weihrauch复杂度研究PAC学习的一致可计算性，聚焦闭概念类，给出不同信息下PAC学习的复杂度结果，还研究VC维未给定等情况及VC维操作复杂度。


<details>
  <summary>Details</summary>
Motivation: 研究PAC学习的一致可计算性，对统计学习基本定理的构造性程度进行分类。

Method: 运用Weihrauch复杂度进行研究。

Result: 证明了正信息下适当和不适当PAC学习的复杂度情况；给出VC维操作复杂度与排序问题的等价关系；得出对PAC可学习性Borel复杂度的相关结论。

Conclusion: 完成了不同信息表示下PAC学习复杂度的分类，以及VC维操作复杂度的分类，为PAC学习的可计算性研究提供了成果。

Abstract: We study uniform computability properties of PAC learning using Weihrauch complexity. We focus on closed concept classes, which are either represented by positive, by negative or by full information. Among other results, we prove that proper PAC learning from positive information is equivalent to the limit operation on Baire space, whereas improper PAC learning from positive information is closely related to Weak Kőnig's Lemma and even equivalent to it, when we have some negative information about the admissible hypotheses. If arbitrary hypotheses are allowed, then improper PAC learning from positive information is still in a finitary DNC range, which implies that it is non-deterministically computable, but does not allow for probabilistic algorithms. These results can also be seen as a classification of the degree of constructivity of the Fundamental Theorem of Statistical Learning. All the aforementioned results hold if an upper bound of the VC dimension is provided as an additional input information. We also study the question of how these results are affected if the VC dimension is not given, but only promised to be finite or if concept classes are represented by negative or full information. Finally, we also classify the complexity of the VC dimension operation itself, which is a problem that is of independent interest. For positive or full information it turns out to be equivalent to the binary sorting problem, for negative information it is equivalent to the jump of sorting. This classification allows also conclusions regarding the Borel complexity of PAC learnability.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [375] [Over-The-Air Extreme Learning Machines with XL Reception via Nonlinear Cascaded Metasurfaces](https://arxiv.org/abs/2601.17749)
*Kyriakos Stylianopoulos,Mattia Fabiani,Giulia Torcolacci,Davide Dardari,George C. Alexandropoulos*

Main category: eess.SP

TL;DR: 本文提出利用可编程超表面技术的XL - MIMO系统，可在空中执行二进制分类任务，数值研究表明其性能与数字和理想化ML模型相当。


<details>
  <summary>Details</summary>
Motivation: 面向目标通信范式需在无线传输数据上应用机器学习推理，在MIMO系统物理层实现推理ML模型存在挑战。

Method: 提出一种XL - MIMO系统，作为极端学习机，由密集并行的超表面衍射层和单个接收射频链组成，利用可调线性响应层近似训练的ELM权重。

Result: 在超表面元素的XL机制下，该系统在不同数据集和无线场景中的性能与数字和理想化ML模型相当。

Conclusion: 证明了将空中学习能力嵌入未来通信系统的可行性。

Abstract: The recently envisioned goal-oriented communications paradigm calls for the application of inference on wirelessly transferred data via Machine Learning (ML) tools. An emerging research direction deals with the realization of inference ML models directly in the physical layer of Multiple-Input Multiple-Output (MIMO) systems, which, however, entails certain significant challenges. In this paper, leveraging the technology of programmable MetaSurfaces (MSs), we present an eXtremely Large (XL) MIMO system that acts as an Extreme Learning Machine (ELM) performing binary classification tasks completely Over-The-Air (OTA), which can be trained in closed form. The proposed system comprises a receiver architecture consisting of densely parallel placed diffractive layers of XL MSs followed by a single reception radio-frequency chain. The front layer facing the MIMO channel consists of identical unit cells of a fixed NonLinear (NL) response, while the remaining layers of elements of tunable linear responses are utilized to approximate OTA the trained ELM weights. Our numerical investigations showcase that, in the XL regime of MS elements, the proposed XL-MIMO-ELM system achieves performance comparable to that of digital and idealized ML models across diverse datasets and wireless scenarios, thereby demonstrating the feasibility of embedding OTA learning capabilities into future communication systems.

</details>


### [376] [Semantic-Aware Task Clustering for Federated Cooperative Multi-Task Semantic Communication](https://arxiv.org/abs/2601.17419)
*Ahmad Halimi Razlighi,Pallavi Dhingra,Edgar Beck,Bho Matthiesen,Armin Dekorsy*

Main category: eess.SP

TL;DR: 本文将CMT - SemCom框架扩展到分布式场景，提出基于联邦学习的CMT - SemCom及语义感知任务聚类方法，仿真验证其有效性和性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有CMT - SemCom框架不适用于分布式多用户场景，且合作多任务可能产生负面效果。

Method: 提出基于联邦学习的CMT - SemCom以实现分布式用户间合作多任务，集成语义感知任务聚类方法确保建设性合作。

Result: 基于LEO卫星网络的仿真表明该方法有效，相比无聚类联邦学习和单任务SemCom有性能提升。

Conclusion: 所提基于联邦学习和语义感知任务聚类的方法可有效应用于分布式多用户场景的CMT - SemCom。

Abstract: Task-oriented semantic communication (SemCom) prioritizes task execution over accurate symbol reconstruction and is well-suited to emerging intelligent applications. Cooperative multi-task SemCom (CMT-SemCom) further improves task execution performance. However, [1] demonstrates that cooperative multi-tasking can be either constructive or destructive. Moreover, the existing CMT-SemCom framework is not directly applicable to distributed multi-user scenarios, such as non-terrestrial satellite networks, where each satellite employs an individual semantic encoder. In this paper, we extend our earlier CMT-SemCom framework to distributed settings by proposing a federated learning (FL) based CMT-SemCom that enables cooperative multi-tasking across distributed users. Moreover, to address performance degradation caused by negative information transfer among heterogeneous tasks, we propose a semantic-aware task clustering method integrated in the FL process to ensure constructive cooperation based on an information-theoretic approach. Unlike common clustering methods that rely on high-dimensional data or feature space similarity, our proposed approach operates in the low-dimensional semantic domain to identify meaningful task relationships. Simulation results based on a LEO satellite network setup demonstrate the effectiveness of our approach and performance gain over unclustered FL and individual single-task SemCom.

</details>


### [377] [Context-Aware Iterative Token Detection and Masked Transmission for Wireless Token Communication](https://arxiv.org/abs/2601.17770)
*Junyong Shin,Joohyuk Park,Jihong Park,Jinho Choi,Yo-Seb Jeon*

Main category: eess.SP

TL;DR: 提出上下文感知令牌通信框架，结合预训练MLM，在收发端采用不同策略，模拟显示提升重建句子质量和速率适配性。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型使令牌成为自然语言表示的有效单位，由此推动在无线信道进行令牌通信。

Method: 提出上下文感知令牌通信框架，使用预训练MLM作为收发端共享上下文概率模型；在接收端开发迭代令牌检测方法；在发送端引入上下文感知掩码策略。

Result: 模拟结果表明该框架显著提升重建句子质量，在不同信道条件下支持有效速率适配。

Conclusion: 所提上下文感知令牌通信框架有效，能提升性能和适配性。

Abstract: The success of large-scale language models has established tokens as compact and meaningful units for natural-language representation, which motivates token communication over wireless channels, where tokens are considered fundamental units for wireless transmission. We propose a context-aware token communication framework that uses a pretrained masked language model (MLM) as a shared contextual probability model between the transmitter (Tx) and receiver (Rx). At Rx, we develop an iterative token detection method that jointly exploits MLM-guided contextual priors and channel observations based on a Bayesian perspective. At Tx, we additionally introduce a context-aware masking strategy which skips highly predictable token transmission to reduce transmission rate. Simulation results demonstrate that the proposed framework substantially improves reconstructed sentence quality and supports effective rate adaptation under various channel conditions.

</details>


### [378] [ME-WARD: A multimodal ergonomic analysis tool for musculoskeletal risk assessment from inertial and video data in working plac](https://arxiv.org/abs/2601.17571)
*Javier González-Alonso,Paula Martín-Tapia,David González-Ortega,Míriam Antón-Rodríguez,Francisco Javier Díaz-Pernas,Mario Martínez-Zarzuela*

Main category: eess.SP

TL;DR: 提出ME - WARD系统用于人体工程学评估和肌肉骨骼风险评估，在工业场景验证性能，证明多模式方法有潜力。


<details>
  <summary>Details</summary>
Motivation: 扩展RULA方法的适用性，提供更通用、可扩展且经济高效的人体工程学评估解决方案。

Method: 开发ME - WARD系统处理关节角度数据，采用RULA方法，在工业场景用金标准IMU系统和单目3D姿态估计系统进行验证实验。

Result: ME - WARD在屈伸主导运动中能产生与IMU派生指标接近的可靠RULA分数，与单目系统性能相当，但在跟踪横向和旋转运动有局限。

Conclusion: 将多种运动捕捉技术集成到统一评估流程的多模式方法有潜力，可在资源受限工业环境广泛应用。

Abstract: This study presents ME-WARD (Multimodal Ergonomic Workplace Assessment and Risk from Data), a novel system for ergonomic assessment and musculoskeletal risk evaluation that implements the Rapid Upper Limb Assessment (RULA) method. ME-WARD is designed to process joint angle data from motion capture systems, including inertial measurement unit (IMU)-based setups, and deep learning human body pose tracking models. The tool's flexibility enables ergonomic risk assessment using any system capable of reliably measuring joint angles, extending the applicability of RULA beyond proprietary setups. To validate its performance, the tool was tested in an industrial setting during the assembly of conveyor belts, which involved high-risk tasks such as inserting rods and pushing conveyor belt components. The experiments leveraged gold standard IMU systems alongside a state-of-the-art monocular 3D pose estimation system. The results confirmed that ME-WARD produces reliable RULA scores that closely align with IMU-derived metrics for flexion-dominated movements and comparable performance with the monocular system, despite limitations in tracking lateral and rotational motions. This work highlights the potential of integrating multiple motion capture technologies into a unified and accessible ergonomic assessment pipeline. By supporting diverse input sources, including low-cost video-based systems, the proposed multimodal approach offers a scalable, cost-effective solution for ergonomic assessments, paving the way for broader adoption in resource-constrained industrial environments.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [379] [Optimal Use of Preferences in Artificial Intelligence Algorithms](https://arxiv.org/abs/2601.18732)
*Joshua S. Gans*

Main category: econ.TH

TL;DR: 本文运用信息设计方法，给出无偏好训练和事后应用偏好最优的条件，为模块化AI管道提供理论基础，并给出设计指导。


<details>
  <summary>Details</summary>
Motivation: 以往工作需指定下游目标，本文旨在提供与决策问题无关的条件，使福利结果能统一应用于各决策问题。

Method: 应用Strack和Yang（2024）的信息设计方法，基于信息价值递减条件分析。

Result: 无偏好训练对任何期望效用决策问题弱占优；当认知约束起作用时，偏好嵌入可占优。

Conclusion: 目标可能变化时通过后处理保留可选性；决策阶段摩擦占主导时嵌入偏好。

Abstract: Machine learning systems embed preferences either in training losses or through post-processing of calibrated predictions. Applying information design methods from Strack and Yang (2024), this paper provides decision problem agnostic conditions under which separation training preference free and applying preferences ex post is optimal. Unlike prior work that requires specifying downstream objectives, the welfare results here apply uniformly across decision problems. The key primitive is a diminishing-value-of-information condition: relative to a fixed (normalised) preference-free loss, preference embedding makes informativeness less valuable at the margin, inducing a mean-preserving contraction of learned posteriors. Because the value of information is convex in beliefs, preference-free training weakly dominates for any expected utility decision problem. This provides theoretical foundations for modular AI pipelines that learn calibrated probabilities and implement asymmetric costs through downstream decision rules. However, separation requires users to implement optimal decision rules. When cognitive constraints bind, as documented in human AI decision-making, preference embedding can dominate by automating threshold computation. These results provide design guidance: preserve optionality through post-processing when objectives may shift; embed preferences when decision-stage frictions dominate.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [380] [Athena: Synergizing Data Prefetching and Off-Chip Prediction via Online Reinforcement Learning](https://arxiv.org/abs/2601.17615)
*Rahul Bera,Zhenrong Lang,Caroline Hengartner,Konstantinos Kanellopoulos,Rakesh Kumar,Mohammad Sadrosadati,Onur Mutlu*

Main category: cs.AR

TL;DR: 论文针对预取和片外预测技术，提出Athena框架协调二者，评估显示其表现优于现有策略且存储开销小。


<details>
  <summary>Details</summary>
Motivation: 预取和片外预测结合无法充分发挥性能，现有预取控制策略有提升空间，需设计整体框架协调片外预测器和多级缓存预取器。

Method: 将预取器和片外预测器的协调建模为强化学习问题，Athena作为代理，观察系统特征并选择协调动作，根据奖励自主学习协调策略。

Result: 使用多种内存密集型工作负载评估，Athena在不同系统配置下均优于现有协调策略，存储开销适中。

Conclusion: Athena能有效协调预取器和片外预测器，提升系统性能，可从https://github.com/CMU-SAFARI/Athena获取。

Abstract: Prefetching and off-chip prediction are two techniques proposed to hide long memory access latencies in high-performance processors. In this work, we demonstrate that: (1) prefetching and off-chip prediction often provide complementary performance benefits, yet (2) naively combining them often fails to realize their full performance potential, and (3) existing prefetcher control policies leave significant room for performance improvement behind.
  Our goal is to design a holistic framework that can autonomously learn to coordinate an off-chip predictor with multiple prefetchers employed at various cache levels. To this end, we propose a new technique called Athena, which models the coordination between prefetchers and off-chip predictor (OCP) as a reinforcement learning (RL) problem. Athena acts as the RL agent that observes multiple system-level features (e.g., prefetcher/OCP accuracy, bandwidth usage) over an epoch of program execution, and uses them as state information to select a coordination action (i.e., enabling the prefetcher and/or OCP, and adjusting prefetcher aggressiveness). At the end of every epoch, Athena receives a numerical reward that measures the change in multiple system-level metrics (e.g., number of cycles taken to execute an epoch). Athena uses this reward to autonomously and continuously learn a policy to coordinate prefetchers with OCP.
  Our extensive evaluation using a diverse set of memory-intensive workloads shows that Athena consistently outperforms prior state-of-the-art coordination policies across a wide range of system configurations with various combinations of underlying prefetchers, OCPs, and main memory bandwidths, while incurring only modest storage overhead. Athena is freely available at https://github.com/CMU-SAFARI/Athena.

</details>


### [381] [Conduit: Programmer-Transparent Near-Data Processing Using Multiple Compute-Capable Resources in Solid State Drives](https://arxiv.org/abs/2601.17633)
*Rakesh Nadig,Vamanan Arulchelvan,Mayank Kabra,Harshita Gupta,Rahul Bera,Nika Mansouri Ghiasi,Nanditha Rao,Qingcai Jiang,Andreas Kosmas Kakolyris,Yu Liang,Mohammad Sadrosadati,Onur Mutlu*

Main category: cs.AR

TL;DR: 提出适用于SSD的通用、对程序员透明的近数据处理框架Conduit，利用多种SSD计算资源，在编译和运行时做不同处理，显著优于先前技术。


<details>
  <summary>Details</summary>
Motivation: 先前基于SSD的近数据处理技术孤立运行，无法充分利用SSD计算潜力且缺乏程序员透明度，将计算在主机和近内存加速器间划分的技术应用于SSD收益有限。

Method: 编译时，Conduit执行自定义编译器传递，将合适代码段向量化并嵌入元数据；运行时，在SSD内进行指令粒度卸载，评估六个关键特性并使用成本函数选择合适资源。

Result: 使用内部事件驱动SSD模拟器在六个数据密集型工作负载上评估Conduit和两种先前的近数据处理卸载技术，Conduit比表现最佳的先前卸载策略性能高1.8倍，能耗降低46%。

Conclusion: Conduit是一种有效的SSD近数据处理框架，能更好地利用SSD计算资源，提高性能并降低能耗。

Abstract: Solid-state drives (SSDs) are well suited for near-data processing (NDP) because they: (1) store large application datasets, and (2) support three NDP paradigms: in-storage processing (ISP), processing using DRAM in the SSD (PuD-SSD), and in-flash processing (IFP). A large body of prior SSD-based NDP techniques operate in isolation, mapping computations to only one or two NDP paradigms (i.e., ISP, PuD-SSD, or IFP) within the SSD. These techniques (1) are tailored to specific workloads or kernels, (2) do not exploit the full computational potential of an SSD, and (3) lack programmer-transparency. While several prior works propose techniques to partition computation between the host and near-memory accelerators, adapting these techniques to SSDs has limited benefits because they (1) ignore the heterogeneity of the SSD resources, and (2) make offloading decisions based on limited factors such as bandwidth utilization, or data movement cost. We propose Conduit, a general-purpose, programmer-transparent NDP framework for SSDs that leverages multiple SSD computation resources. At compile time, Conduit executes a custom compiler (e.g., LLVM) pass that (i) vectorizes suitable application code segments into SIMD operations that align with the SSD's page layout, and (ii) embeds metadata (e.g., operation type, operand sizes) into the vectorized instructions to guide runtime offloading decisions. At runtime, within the SSD, Conduit performs instruction-granularity offloading by evaluating six key features, and uses a cost function to select the most suitable SSD resource. We evaluate Conduit and two prior NDP offloading techniques using an in-house event-driven SSD simulator on six data-intensive workloads. Conduit outperforms the best-performing prior offloading policy by 1.8x and reduces energy consumption by 46%.

</details>


### [382] [Memory-Efficient FPGA Implementation of Stochastic Simulated Annealing](https://arxiv.org/abs/2601.18007)
*Duckgyu Shin,Naoya Onizawa,Warren J. Gross,Takahiro Hanyu*

Main category: cs.AR

TL;DR: 提出用于FPGA实现的HA - SSA算法，与传统SA和SSA对比，速度更快、内存效率更高。


<details>
  <summary>Details</summary>
Motivation: 传统SA算法计算时间随问题规模增大而快速增加，虽有收敛更快的SSA算法，但需提出内存高效的FPGA实现算法。

Method: 提出HA - SSA算法，在最大割组合优化问题上与传统SSA和SA方法对比。

Result: HA - SSA收敛速度比传统SA算法快达114倍，内存效率比传统SSA高6倍，且保持高解质量。

Conclusion: HA - SSA算法在FPGA实现中可减少内存使用，同时维持计算速度和较高解质量。

Abstract: Simulated annealing (SA) is a well-known algorithm for solving combinatorial optimization problems. However, the computation time of SA increases rapidly, as the size of the problem grows. Recently, a stochastic simulated annealing (SSA) algorithm that converges faster than conventional SA has been reported. In this paper, we present a hardware-aware SSA (HA- SSA) algorithm for memory-efficient FPGA implementations. HA-SSA can reduce the memory usage of storing intermediate results while maintaining the computing speed of SSA. For evaluation purposes, the proposed algorithm is compared with the conventional SSA and SA approaches on maximum cut combinatorial optimization problems. HA-SSA achieves a convergence speed that is up to 114-times faster than that of the conventional SA algorithm depending on the maximum cut problem selected from the G-set which is a dataset of the maximum cut problems. HA-SSA is implemented on a field-programmable gate array (FPGA) (Xilinx Kintex-7), and it achieves up to 6-times the memory efficiency of conventional SSA while maintaining high solution quality for optimization problems.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [383] [PC-MCL: Patient-Consistent Multi-Cycle Learning with multi-label bias correction for respiratory sound classification](https://arxiv.org/abs/2601.17080)
*Seung Gyu Jeong,Seong-Eun Kim*

Main category: eess.AS

TL;DR: 提出PC - MCL解决呼吸音分类现存问题，在ICBHI 2017基准测试中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 解决现有深度模型依赖周期级分析和患者特异性过拟合问题，以及多标签分布偏差问题。

Method: 提出PC - MCL，包含多周期拼接、3标签公式和患者匹配辅助任务三个关键组件。

Result: 在ICBHI 2017基准测试中，PC - MCL的ICBHI得分达到65.37%，优于现有基线。

Conclusion: PC - MCL的三个组件都很重要，协同工作可改善异常呼吸事件检测。

Abstract: Automated respiratory sound classification supports the diagnosis of pulmonary diseases. However, many deep models still rely on cycle-level analysis and suffer from patient-specific overfitting. We propose PC-MCL (Patient-Consistent Multi-Cycle Learning) to address these limitations by utilizing three key components: multi-cycle concatenation, a 3-label formulation, and a patient-matching auxiliary task. Our work resolves a multi-label distributional bias in respiratory sound classification, a critical issue inherent to applying multi-cycle concatenation with the conventional 2-label formulation (crackle, wheeze). This bias manifests as a systematic loss of normal signal information when normal and abnormal cycles are combined. Our proposed 3-label formulation (normal, crackle, wheeze) corrects this by preserving information from all constituent cycles in mixed samples. Furthermore, the patient-matching auxiliary task acts as a multi-task regularizer, encouraging the model to learn more robust features and improving generalization. On the ICBHI 2017 benchmark, PC-MCL achieves an ICBHI Score of 65.37%, outperforming existing baselines. Ablation studies confirm that all three components are essential, working synergistically to improve the detection of abnormal respiratory events.

</details>


### [384] [Recovering Performance in Speech Emotion Recognition from Discrete Tokens via Multi-Layer Fusion and Paralinguistic Feature Integration](https://arxiv.org/abs/2601.17085)
*Esther Sun,Abinay Reddy Naini,Carlos Busso*

Main category: eess.AS

TL;DR: 本文探索离散语音标记在语音情感识别（SER）中的应用，通过多层融合和声学特征集成缩小与连续表示的性能差距。


<details>
  <summary>Details</summary>
Motivation: 离散语音标记在SER中受量化时副语言信息丢失的限制，需要探索其在SER中的应用以提升性能。

Method: 使用微调的WavLM - Large模型量化不同层配置和k - means量化粒度下的性能下降；提出基于注意力的多层融合和集成openSMILE特征这两个恢复信息损失的策略；比较主流神经编解码标记器并分析其与声学特征融合的行为。

Result: 通过多层融合和声学特征集成，离散标记在SER任务中可缩小与连续表示的性能差距。

Conclusion: 离散标记可通过文中提出的策略在SER任务中取得与连续表示相近的性能。

Abstract: Discrete speech tokens offer significant advantages for storage and language model integration, but their application in speech emotion recognition (SER) is limited by paralinguistic information loss during quantization. This paper presents a comprehensive investigation of discrete tokens for SER. Using a fine-tuned WavLM-Large model, we systematically quantify performance degradation across different layer configurations and k-means quantization granularities. To recover the information loss, we propose two key strategies: (1) attention-based multi-layer fusion to recapture complementary information from different layers, and (2) integration of openSMILE features to explicitly reintroduce paralinguistic cues. We also compare mainstream neural codec tokenizers (SpeechTokenizer, DAC, EnCodec) and analyze their behaviors when fused with acoustic features. Our findings demonstrate that through multi-layer fusion and acoustic feature integration, discrete tokens can close the performance gap with continuous representations in SER tasks.

</details>


### [385] [ToS: A Team of Specialists ensemble framework for Stereo Sound Event Localization and Detection with distance estimation in Video](https://arxiv.org/abs/2601.17611)
*Davide Berghi,Philip J. B. Jackson*

Main category: eess.AS

TL;DR: 提出ToS集成框架用于视频中带距离估计的声音事件定位与检测，在关键指标上优于现有方法，后续将增强子网络。


<details>
  <summary>Details</summary>
Motivation: 视频中3D SELD多模态任务需跨语义、空间和时间维度联合推理，单模型难以有效解决。

Method: 引入ToS集成框架，整合了空间 - 语言模型、空间 - 时间模型和时间 - 语言模型三个互补子网络。

Result: 在DCASE2025 Task 3立体声SELD开发集上，ToS在关键指标上始终优于现有方法。

Conclusion: ToS框架在3D SELD任务中有效，未来将通过合适任务、训练和预训练课程增强子网络。

Abstract: Sound event localization and detection with distance estimation (3D SELD) in video involves identifying active sound events at each time frame while estimating their spatial coordinates. This multimodal task requires joint reasoning across semantic, spatial, and temporal dimensions, a challenge that single models often struggle to address effectively. To tackle this, we introduce the Team of Specialists (ToS) ensemble framework, which integrates three complementary sub-networks: a spatio-linguistic model, a spatio-temporal model, and a tempo-linguistic model. Each sub-network specializes in a unique pair of dimensions, contributing distinct insights to the final prediction, akin to a collaborative team with diverse expertise. ToS has been benchmarked against state-of-the-art audio-visual models for 3D SELD on the DCASE2025 Task 3 Stereo SELD development set, consistently outperforming existing methods across key metrics. Future work will extend this proof of concept by strengthening the specialists with appropriate tasks, training, and pre-training curricula.

</details>


### [386] [SpatialEmb: Extract and Encode Spatial Information for 1-Stage Multi-channel Multi-speaker ASR on Arbitrary Microphone Arrays](https://arxiv.org/abs/2601.18037)
*Yiwen Shao,Yong Xu,Sanjeev Khudanpur,Dong Yu*

Main category: eess.AS

TL;DR: 提出轻量级嵌入模块SpatialEmb直接为ASR模型提取和编码空间信息，在真实会议语料上实验取得新的最优结果。


<details>
  <summary>Details</summary>
Motivation: 现有多通道自动语音识别系统提取空间特征方式导致流程低效、性能欠佳，且依赖特定设置，难以适应新设备。

Method: 提出轻量级嵌入模块SpatialEmb直接为ASR模型提取和编码空间信息，支持固定和任意麦克风拓扑；在AliMeeting语料上进行实验确定最优模型设计。

Result: 最佳模型在Eval和Test集上分别实现17.04%和20.32%的字符错误率，在相同训练数据下取得新的最优结果。

Conclusion: SpatialEmb模块有效解决了现有多通道自动语音识别系统的问题，提升了性能。

Abstract: Spatial information is a critical clue for multi-channel multi-speaker target speech recognition. Most state-of-the-art multi-channel Automatic Speech Recognition (ASR) systems extract spatial features only during the speech separation stage, followed by standard single-channel ASR on the separated speech. This approach results in an inefficient, lengthy pipeline and sub-optimal ASR performance due to the accumulated errors from preprocessing modules. Furthermore, most spatial feature extraction methods depend on the knowledge of speaker positions and microphone topology, making the systems reliant on specific settings and challenging to adapt to new equipment. In this work, we propose a solution to these issues with a lightweight embedding module named SpatialEmb, which extracts and encodes spatial information directly for the ASR model, supporting both fixed and arbitrary microphone topology. We conduct comprehensive experiments on AliMeeting, a real meeting corpus, to determine the optimal model design for SpatialEmb in terms of both performance and efficiency. Our best model trained with 105 hours Train-Ali-far achieves 17.04% and 20.32% character error rates (CER) on the Eval and Test sets, establishing a new state-of-the-art result with the same training data.

</details>


### [387] [Learning to Discover: A Generalized Framework for Raga Identification without Forgetting](https://arxiv.org/abs/2601.18766)
*Parampreet Singh,Somya Kumar,Chaitanya Shailendra Nitawe,Vipul Arora*

Main category: eess.AS

TL;DR: 传统方法在印度艺术音乐拉加识别中难以处理未见过的拉加，本文提出统一学习框架，提升了拉加分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有训练数据集未涵盖众多罕见拉加，传统分类模型在处理未见拉加时表现不佳，近期方法有灾难性遗忘问题。

Method: 采用统一学习框架，同时利用有标签和无标签音频。

Result: 在基准数据集上测试模型，在分类已见、未见和所有拉加类别上表现良好，超越之前基于NCD的管道。

Conclusion: 该方法在发现未见拉加类别上表现出色，为印度艺术音乐任务的表征学习提供新见解。

Abstract: Raga identification in Indian Art Music (IAM) remains challenging due to the presence of numerous rarely performed Ragas that are not represented in available training datasets. Traditional classification models struggle in this setting, as they assume a closed set of known categories and therefore fail to recognise or meaningfully group previously unseen Ragas. Recent works have tried categorizing unseen Ragas, but they run into a problem of catastrophic forgetting, where the knowledge of previously seen Ragas is diminished. To address this problem, we adopt a unified learning framework that leverages both labeled and unlabeled audio, enabling the model to discover coherent categories corresponding to the unseen Ragas, while retaining the knowledge of previously known ones. We test our model on benchmark Raga Identification datasets and demonstrate its performance in categorizing previously seen, unseen, and all Raga classes. The proposed approach surpasses the previous NCD-based pipeline even in discovering the unseen Raga categories, offering new insights into representation learning for IAM tasks.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [388] [ChemNavigator: Agentic AI Discovery of Design Rules for Organic Photocatalysts](https://arxiv.org/abs/2601.17084)
*Iman Peivaste,Ahmed Makradi,Salim Belouettar*

Main category: physics.chem-ph

TL;DR: 介绍ChemNavigator自主智能系统，通过迭代探索发现有机光催化剂结构 - 性质关系及设计规则，展示其在材料发现中的应用。


<details>
  <summary>Details</summary>
Motivation: 高性能有机光催化剂的发现受化学空间和人工分子设计直觉的限制，需要新方法来推导结构 - 性质关系。

Method: 构建ChemNavigator系统，结合大语言模型推理和密度泛函紧束缚计算，采用多智能体架构模拟科学方法进行迭代探索。

Result: 经过200个分子迭代发现，确定六条显著设计规则，可量化效应大小和进行特征交互分析。

Conclusion: 智能AI系统能自主推导可解释、基于化学的设计原则，为AI辅助材料发现奠定框架，补充化学直觉。

Abstract: The discovery of high-performance organic photocatalysts for hydrogen evolution remains limited by the vastness of chemical space and the reliance on human intuition for molecular design. Here we present ChemNavigator, an agentic AI system that autonomously derives structure-property relationships through hypothesis-driven exploration of organic photocatalyst candidates. The system integrates large language model reasoning with density functional tight binding calculations in a multi-agent architecture that mirrors the scientific method: formulating hypotheses, designing experiments, executing calculations, and validating findings through rigorous statistical analysis. Through iterative discovery cycles encompassing 200 molecules, ChemNavigator autonomously identified six statistically significant design rules governing frontier orbital energies, including the effects of ether linkages, carbonyl groups, extended conjugation, cyano groups, halogen substituents, and amine groups. Importantly, these rules correspond to established principles of organic electronic structure (resonance donation, inductive withdrawal, $π$-delocalization), demonstrating that the system can independently derive chemical knowledge without explicit programming. Notably, autonomous agentic reasoning extracted these six validated rules from a molecular library where previous ML approaches identified only carbonyl effects. Furthermore, the quantified effect sizes provide a prioritized ranking for synthetic chemists, while feature interaction analysis revealed diminishing returns when combining strategies, challenging additive assumptions in molecular design. This work demonstrates that agentic AI systems can autonomously derive interpretable, chemically grounded design principles, establishing a framework for AI-assisted materials discovery that complements rather than replaces chemical intuition.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [389] [Embodiment-Induced Coordination Regimes in Tabular Multi-Agent Q-Learning](https://arxiv.org/abs/2601.17454)
*Muhammad Ahmed Atif,Nehal Naeem Haji,Mohammad Shahid Shaikh,Muhammad Ebad Atif*

Main category: cs.MA

TL;DR: 在多智能体强化学习中，集中式价值学习未必能提升协调和稳定性，表格分析显示其效果依赖于机制和角色。


<details>
  <summary>Details</summary>
Motivation: 验证集中式价值学习能提升多智能体强化学习协调和稳定性这一假设。

Method: 在全表格的捕食者 - 猎物网格世界中，比较独立和集中式Q学习，排除函数逼近和表征学习的混淆影响。

Result: 集中式学习未展现一致优势，常被独立学习超越，非对称配置会导致持续协调故障。

Conclusion: 在具身约束下，增强协调可能带来负面影响，集中式学习效果非普遍适用，取决于机制和角色。

Abstract: Centralized value learning is often assumed to improve coordination and stability in multi-agent reinforcement learning, yet this assumption is rarely tested under controlled conditions. We directly evaluate it in a fully tabular predator-prey gridworld by comparing independent and centralized Q-learning under explicit embodiment constraints on agent speed and stamina. Across multiple kinematic regimes and asymmetric agent roles, centralized learning fails to provide a consistent advantage and is frequently outperformed by fully independent learning, even under full observability and exact value estimation. Moreover, asymmetric centralized-independent configurations induce persistent coordination breakdowns rather than transient learning instability. By eliminating confounding effects from function approximation and representation learning, our tabular analysis isolates coordination structure as the primary driver of these effects. The results show that increased coordination can become a liability under embodiment constraints, and that the effectiveness of centralized learning is fundamentally regime and role dependent rather than universal.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [390] [Flow-based Extremal Mathematical Structure Discovery](https://arxiv.org/abs/2601.18005)
*Gergely Bérczi,Baran Hashemi,Jonas Klüver*

Main category: math.CO

TL;DR: 提出FlowBoost框架用于发现极值几何结构，在多个几何优化问题上表现良好，部分结果超现有最佳。


<details>
  <summary>Details</summary>
Motivation: 数学中极值结构发现面临搜索空间大且非凸、分析方法难用、暴力搜索不可行的问题。

Method: 结合几何感知条件流匹配模型、奖励引导策略优化与动作探索、随机局部搜索三个组件构建FlowBoost闭环生成框架。

Result: 在四个几何优化问题上验证框架，部分案例发现的配置匹配或超越已知最佳结果，圆填充问题上改进已知下界，且计算资源使用少。

Conclusion: FlowBoost框架在发现极值几何结构上有效，相比现有方法有资源使用少等优势。

Abstract: The discovery of extremal structures in mathematics requires navigating vast and nonconvex landscapes where analytical methods offer little guidance and brute-force search becomes intractable. We introduce FlowBoost, a closed-loop generative framework that learns to discover rare and extremal geometric structures by combining three components: (i) a geometry-aware conditional flow-matching model that learns to sample high-quality configurations, (ii) reward-guided policy optimization with action exploration that directly optimizes the generation process toward the objective while maintaining diversity, and (iii) stochastic local search for both training-data generation and final refinement. Unlike prior open-loop approaches, such as PatternBoost that retrains on filtered discrete samples, or AlphaEvolve which relies on frozen Large Language Models (LLMs) as evolutionary mutation operators, FlowBoost enforces geometric feasibility during sampling, and propagates reward signal directly into the generative model, closing the optimization loop and requiring much smaller training sets and shorter training times, and reducing the required outer-loop iterations by orders of magnitude, while eliminating dependence on LLMs. We demonstrate the framework on four geometric optimization problems: sphere packing in hypercubes, circle packing maximizing sum of radii, the Heilbronn triangle problem, and star discrepancy minimization. In several cases, FlowBoost discovers configurations that match or exceed the best known results. For circle packings, we improve the best known lower bounds, surpassing the LLM-based system AlphaEvolve while using substantially fewer computational resources.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [391] [Breaking Task Impasses Quickly: Adaptive Neuro-Symbolic Learning for Open-World Robotics](https://arxiv.org/abs/2601.16985)
*Pierrick Lorang*

Main category: cs.RO

TL;DR: 为解决自主系统适应开放环境新奇事物时的挑战，提出神经符号框架，在机器人操作和自动驾驶中验证效果更好。


<details>
  <summary>Details</summary>
Motivation: 现有混合规划和强化学习方法在开放环境中存在样本效率低、适应慢和灾难性遗忘的问题，需要一种新的方法实现机器人快速适应。

Method: 提出一种集成分层抽象、任务和运动规划（TAMP）和强化学习的神经符号框架，结合符号目标导向学习和基于世界模型的探索。

Result: 在机器人操作和自动驾驶中验证，相比现有混合方法，收敛更快、样本效率更高、鲁棒性更强。

Conclusion: 该方法有在现实世界部署的潜力。

Abstract: Adapting to unforeseen novelties in open-world environments remains a major challenge for autonomous systems. While hybrid planning and reinforcement learning (RL) approaches show promise, they often suffer from sample inefficiency, slow adaptation, and catastrophic forgetting. We present a neuro-symbolic framework integrating hierarchical abstractions, task and motion planning (TAMP), and reinforcement learning to enable rapid adaptation in robotics. Our architecture combines symbolic goal-oriented learning and world model-based exploration to facilitate rapid adaptation to environmental changes. Validated in robotic manipulation and autonomous driving, our approach achieves faster convergence, improved sample efficiency, and superior robustness over state-of-the-art hybrid methods, demonstrating its potential for real-world deployment.

</details>


### [392] [SKETCH: Semantic Key-Point Conditioning for Long-Horizon Vessel Trajectory Prediction](https://arxiv.org/abs/2601.18537)
*Linyong Gan,Zimo Li,Wenxin Xu,Xingjian Li,Jianhua Z. Huang,Enmei Tu,Shuhang Chen*

Main category: cs.RO

TL;DR: 现存方法在长周期船舶轨迹预测难以保证全局方向一致性，文章提出语义关键点条件轨迹建模框架，用预训练-微调策略估计NKP先验，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法应对复杂导航行为和环境因素造成的不确定性时，难保持长周期轨迹预测的全局方向一致性。

Method: 提出语义关键点条件轨迹建模框架，通过高级Next Key Point（NKP）预测未来轨迹，将长周期预测分解为全局语义决策和局部运动建模；采用预训练-微调策略估计NKP先验。

Result: 在真实世界AIS数据上的大量实验显示，该方法在长航行时间、方向准确性和细粒度轨迹预测方面始终优于现有方法。

Conclusion: 所提方法能有效解决长周期船舶轨迹预测中的问题，提高预测性能。

Abstract: Accurate long-horizon vessel trajectory prediction remains challenging due to compounded uncertainty from complex navigation behaviors and environmental factors. Existing methods often struggle to maintain global directional consistency, leading to drifting or implausible trajectories when extrapolated over long time horizons. To address this issue, we propose a semantic-key-point-conditioned trajectory modeling framework, in which future trajectories are predicted by conditioning on a high-level Next Key Point (NKP) that captures navigational intent. This formulation decomposes long-horizon prediction into global semantic decision-making and local motion modeling, effectively restricting the support of future trajectories to semantically feasible subsets. To efficiently estimate the NKP prior from historical observations, we adopt a pretrain-finetune strategy. Extensive experiments on real-world AIS data demonstrate that the proposed method consistently outperforms state-of-the-art approaches, particularly for long travel durations, directional accuracy, and fine-grained trajectory prediction.

</details>


### [393] [Attention-Based Neural-Augmented Kalman Filter for Legged Robot State Estimation](https://arxiv.org/abs/2601.18569)
*Seokju Lee,Kyung-Soo Kim*

Main category: cs.RO

TL;DR: 提出基于注意力的神经增强卡尔曼滤波器（AttenNKF）用于腿式机器人状态估计，实验显示性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 脚滑动是腿式机器人状态估计误差的主要来源，目标是估计并补偿滑动引起的误差。

Method: 用带有注意力机制的神经补偿器增强不变扩展卡尔曼滤波器（InEKF），在潜在空间训练补偿器。

Result: 实验表明，与现有腿式机器人状态估计器相比，性能有所提高，特别是在易滑动条件下。

Conclusion: AttenNKF能有效估计和补偿腿式机器人因脚滑动引起的状态估计误差，提升估计性能。

Abstract: In this letter, we propose an Attention-Based Neural-Augmented Kalman Filter (AttenNKF) for state estimation in legged robots. Foot slip is a major source of estimation error: when slip occurs, kinematic measurements violate the no-slip assumption and inject bias during the update step. Our objective is to estimate this slip-induced error and compensate for it. To this end, we augment an Invariant Extended Kalman Filter (InEKF) with a neural compensator that uses an attention mechanism to infer error conditioned on foot-slip severity and then applies this estimate as a post-update compensation to the InEKF state (i.e., after the filter update). The compensator is trained in a latent space, which aims to reduce sensitivity to raw input scales and encourages structured slip-conditioned compensations, while preserving the InEKF recursion. Experiments demonstrate improved performance compared to existing legged-robot state estimators, particularly under slip-prone conditions.

</details>


### [394] [Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge](https://arxiv.org/abs/2601.18733)
*Li Kang,Heng Zhou,Xiufeng Song,Rui Li,Bruno N. Y. Chen,Ziye Wang,Ximeng Meng,Stone Tao,Yiran Qin,Xiaohong Liu,Ruimao Zhang,Lei Bai,Yilun Du,Hao Su,Philip Torr,Zhenfei Yin,Ruihao Gong,Yejun Zeng,Fengjun Zhong,Shenghao Jin,Jinyang Guo,Xianglong Liu,Xiaojun Jia,Tianqi Shan,Wenqi Ren,Simeng Qin,Jialing Yang,Xiaoyu Ma,Tianxing Chen,Zixuan Li,Zijian Cai,Yan Qin,Yusen Qin,Qiangyu Chen,Kaixuan Wang,Zhaoming Han,Yao Mu,Ping Luo,Yuanqi Yao,Haoming Song,Jan-Nico Zaech,Fabien Despinoy,Danda Pani Paudel,Luc Van Gool*

Main category: cs.RO

TL;DR: 随着多模态大模型发展，多智能体系统框架对具身AI很重要，提出MARS挑战评估多智能体系统设计和协调。


<details>
  <summary>Details</summary>
Motivation: 随着具身AI向复杂任务场景转变，需要多智能体系统框架，为解决多智能体协作挑战。

Method: 举办MARS挑战，聚焦规划和控制两个领域，让参与者用VLM探索多智能体具身规划和策略执行。

Result: 通过评估参与者提交的解决方案，获得关于具身多智能体系统设计和协调的有价值见解。

Conclusion: 该挑战有助于未来先进协作AI系统的发展。

Abstract: Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enhancing system efficiency through task delegation, and enabling advanced human-agent interactions. To address the challenges posed by multi-agent collaboration, we propose the Multi-Agent Robotic System (MARS) Challenge, held at the NeurIPS 2025 Workshop on SpaVLE. The competition focuses on two critical areas: planning and control, where participants explore multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks and policy execution to perform robotic manipulation in dynamic environments. By evaluating solutions submitted by participants, the challenge provides valuable insights into the design and coordination of embodied multi-agent systems, contributing to the future development of advanced collaborative AI systems.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [395] [AI-based System for Transforming text and sound to Educational Videos](https://arxiv.org/abs/2601.17022)
*M. E. ElAlami,S. M. Khater,M. El. R. Rehan*

Main category: cs.MM

TL;DR: 本文提出用GAN生成教育视频的新方法，经三阶段处理输入生成视频，与其他系统对比视觉质量更优。


<details>
  <summary>Details</summary>
Motivation: 当前从文本或语音等条件输入生成视频内容仍是挑战领域，需新方法用于教育。

Method: 引入GAN到教育结构，系统分三阶段：先转录输入，再提取关键词生成相关图像，最后合成视频并集成声音。

Result: 与TGAN、MoCoGAN和TGANS - C等系统对比，FID分数达28.75%。

Conclusion: 该方法提升了视觉质量，优于现有方法。

Abstract: Technological developments have produced methods that can generate educational videos from input text or sound. Recently, the use of deep learning techniques for image and video generation has been widely explored, particularly in education. However, generating video content from conditional inputs such as text or speech remains a challenging area. In this paper, we introduce a novel method to the educational structure, Generative Adversarial Network (GAN), which develop frame-for-frame frameworks and are able to create full educational videos. The proposed system is structured into three main phases In the first phase, the input (either text or speech) is transcribed using speech recognition. In the second phase, key terms are extracted and relevant images are generated using advanced models such as CLIP and diffusion models to enhance visual quality and semantic alignment. In the final phase, the generated images are synthesized into a video format, integrated with either pre-recorded or synthesized sound, resulting in a fully interactive educational video. The proposed system is compared with other systems such as TGAN, MoCoGAN, and TGANS-C, achieving a Fréchet Inception Distance (FID) score of 28.75%, which indicates improved visual quality and better over existing methods.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [396] [FASTR: Reimagining FASTQ via Compact Image-inspired Representation](https://arxiv.org/abs/2601.17184)
*Adrian Tkachenko,Sepehr Salem,Ayotomiwa Ezekiel Adeniyi,Zulal Bingol,Mohammed Nayeem Uddin,Akshat Prasanna,Alexander Zelikovsky,Serghei Mangul,Can Alkan,Mohammed Alser*

Main category: q-bio.GN

TL;DR: 介绍FASTR作为FASTQ无损替代格式，可减少文件大小，支持下游分析，有并行软件生态，为基因组分析奠基。


<details>
  <summary>Details</summary>
Motivation: 高通量测序产生大规模数据集，FASTQ格式存在存储和分析瓶颈，现有压缩工具存在缺陷。

Method: 将每个核苷酸及其碱基质量得分编码为单个8位值。

Result: FASTR至少将文件大小缩小2倍，能直接用于下游分析；使用通用压缩工具时，比FASTQ在不同读长中有更高压缩比、更快压缩和解压缩速度；机器学习就绪；提供并行软件生态，可与现有工具集成。

Conclusion: FASTR消除了解压缩成本和减少数据移动，为可扩展基因组分析和实时测序工作流程奠定基础。

Abstract: Motivation: High-throughput sequencing (HTS) enables population-scale genomics but generates massive datasets, creating bottlenecks in storage, transfer, and analysis. FASTQ, the standard format for over two decades, stores one byte per base and one byte per quality score, leading to inefficient I/O, high storage costs, and redundancy. Existing compression tools can mitigate some issues, but often introduce costly decompression or complex dependency issues. Results: We introduce FASTR, a lossless, computation-native successor to FASTQ that encodes each nucleotide together with its base quality score into a single 8-bit value. FASTR reduces file size by at least 2x while remaining fully reversible and directly usable for downstream analyses. Applying general-purpose compression tools on FASTR consistently yields higher compression ratios, 2.47, 3.64, and 4.8x faster compression, and 2.34, 1.96, 1.75x faster decompression than on FASTQ across Illumina, HiFi, and ONT reads. FASTR is machine-learning-ready, allowing reads to be consumed directly as numerical vectors or image-like representations. We provide a highly parallel software ecosystem for FASTQ-FASTR conversion and show that FASTR integrates with existing tools, such as minimap2, with minimal interface changes and no performance overhead. By eliminating decompression costs and reducing data movement, FASTR lays the foundation for scalable genomics analyses and real-time sequencing workflows. Availability and Implementation: https://github.com/ALSER-Lab/FASTR

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [397] [MultiVis-Agent: A Multi-Agent Framework with Logic Rules for Reliable and Comprehensive Cross-Modal Data Visualization](https://arxiv.org/abs/2601.18320)
*Jinwei Lu,Yuanfeng Song,Chen Zhang,Raymond Chi-Wing Wong*

Main category: cs.CL

TL;DR: 提出MultiVis - Agent框架用于可靠的多模态多场景可视化生成，经实验验证其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 现实可视化任务复杂多模态，当前系统有局限性，LLM - 基于方法存在可靠性挑战。

Method: 提出四层逻辑规则框架引导LLM推理，形式化MultiVis任务并开发MultiVis - Bench基准。

Result: 在挑战性任务中可视化得分75.63%，显著优于基线；任务完成率99.58%、代码执行成功率94.56%。

Conclusion: 该方法成功解决自动化可视化生成中的复杂性和可靠性挑战。

Abstract: Real-world visualization tasks involve complex, multi-modal requirements that extend beyond simple text-to-chart generation, requiring reference images, code examples, and iterative refinement. Current systems exhibit fundamental limitations: single-modality input, one-shot generation, and rigid workflows. While LLM-based approaches show potential for these complex requirements, they introduce reliability challenges including catastrophic failures and infinite loop susceptibility. To address this gap, we propose MultiVis-Agent, a logic rule-enhanced multi-agent framework for reliable multi-modal and multi-scenario visualization generation. Our approach introduces a four-layer logic rule framework that provides mathematical guarantees for system reliability while maintaining flexibility. Unlike traditional rule-based systems, our logic rules are mathematical constraints that guide LLM reasoning rather than replacing it. We formalize the MultiVis task spanning four scenarios from basic generation to iterative refinement, and develop MultiVis-Bench, a benchmark with over 1,000 cases for multi-modal visualization evaluation. Extensive experiments demonstrate that our approach achieves 75.63% visualization score on challenging tasks, significantly outperforming baselines (57.54-62.79%), with task completion rates of 99.58% and code execution success rates of 94.56% (vs. 74.48% and 65.10% without logic rules), successfully addressing both complexity and reliability challenges in automated visualization generation.

</details>


### [398] [Uncertainty Quantification for Named Entity Recognition via Full-Sequence and Subsequence Conformal Prediction](https://arxiv.org/abs/2601.16999)
*Matthew Singer,Srijan Sengupta,Karl Pazdernik*

Main category: cs.CL

TL;DR: 本文提出适应序列标注NER模型生成不确定性感知预测集的通用框架，实验证明其适用性、有效性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前NER模型输出无不确定性度量，使下游应用易出现级联错误，需提供模型预测可靠性的形式化保证。

Method: 基于共形预测，设计高效非一致性评分函数构建支持无条件和类条件覆盖的预测集。

Result: 在三个基准数据集的四个NER模型上的实验证明了方法的广泛适用性、有效性和效率。

Conclusion: 提出的框架可有效让序列标注NER模型生成不确定性感知预测集，有实际应用价值。

Abstract: Named Entity Recognition (NER) serves as a foundational component in many natural language processing (NLP) pipelines. However, current NER models typically output a single predicted label sequence without any accompanying measure of uncertainty, leaving downstream applications vulnerable to cascading errors. In this paper, we introduce a general framework for adapting sequence-labeling-based NER models to produce uncertainty-aware prediction sets. These prediction sets are collections of full-sentence labelings that are guaranteed to contain the correct labeling with a user-specified confidence level. This approach serves a role analogous to confidence intervals in classical statistics by providing formal guarantees about the reliability of model predictions. Our method builds on conformal prediction, which offers finite-sample coverage guarantees under minimal assumptions. We design efficient nonconformity scoring functions to construct efficient, well-calibrated prediction sets that support both unconditional and class-conditional coverage. This framework accounts for heterogeneity across sentence length, language, entity type, and number of entities within a sentence. Empirical experiments on four NER models across three benchmark datasets demonstrate the broad applicability, validity, and efficiency of the proposed methods.

</details>


### [399] [Unsupervised Text Segmentation via Kernel Change-Point Detection on Sentence Embeddings](https://arxiv.org/abs/2601.18788)
*Mumin Jia,Jairo Diaz-Rodriguez*

Main category: cs.CL

TL;DR: 提出无监督文本分割方法Embed - KCPD，有理论保证，经模拟验证，在基准测试中表现好。


<details>
  <summary>Details</summary>
Motivation: 有监督文本分割中边界标签成本高、主观且难以跨领域和粒度迁移，因此需要无监督文本分割方法。

Method: 提出训练无关的Embed - KCPD方法，将句子表示为嵌入向量，通过最小化惩罚KCPD目标估计边界；开发KCPD在m依赖序列下的依赖感知理论；引入基于大语言模型的模拟框架。

Result: 证明了总体惩罚风险的 oracle 不等式和定位保证；模拟框架验证了预测的缩放行为；在标准分割基准测试中，Embed - KCPD 常优于强大的无监督基线。

Conclusion: Embed - KCPD 结合了强大的理论保证、模拟可靠性和文本分割的实际有效性。

Abstract: Unsupervised text segmentation is crucial because boundary labels are expensive, subjective, and often fail to transfer across domains and granularity choices. We propose Embed-KCPD, a training-free method that represents sentences as embedding vectors and estimates boundaries by minimizing a penalized KCPD objective. Beyond the algorithmic instantiation, we develop, to our knowledge, the first dependence-aware theory for KCPD under $m$-dependent sequences, a finite-memory abstraction of short-range dependence common in language. We prove an oracle inequality for the population penalized risk and a localization guarantee showing that each true change point is recovered within a window that is small relative to segment length. To connect theory to practice, we introduce an LLM-based simulation framework that generates synthetic documents with controlled finite-memory dependence and known boundaries, validating the predicted scaling behavior. Across standard segmentation benchmarks, Embed-KCPD often outperforms strong unsupervised baselines. A case study on Taylor Swift's tweets illustrates that Embed-KCPD combines strong theoretical guarantees, simulated reliability, and practical effectiveness for text segmentation.

</details>


### [400] [Improving User Privacy in Personalized Generation: Client-Side Retrieval-Augmented Modification of Server-Side Generated Speculations](https://arxiv.org/abs/2601.17569)
*Alireza Salemi,Hamed Zamani*

Main category: cs.CL

TL;DR: 介绍了交互式框架$P^3$用于大语言模型个性化生成，实验显示其性能优、隐私性好且适合边缘部署。


<details>
  <summary>Details</summary>
Motivation: 解决现有大语言模型个性化方案在隐私暴露和模型能力间的权衡问题。

Method: 服务器端大模型基于用户查询生成草稿，客户端小模型结合用户私有配置文件评估修改，循环至生成结束标记。

Result: 在LaMP - QA基准测试中表现优于基线，恢复了“泄露”上限场景大部分效用，隐私分析显示隐私保护好，客户端生成少量令牌。

Conclusion: $P^3$是一个实用、有效的个性化生成解决方案，提升了隐私性。

Abstract: Personalization is crucial for aligning Large Language Model (LLM) outputs with individual user preferences and background knowledge. State-of-the-art solutions are based on retrieval augmentation, where relevant context from a user profile is retrieved for LLM consumption. These methods deal with a trade-off between exposing retrieved private data to cloud providers and relying on less capable local models. We introduce $P^3$, an interactive framework for high-quality personalization without revealing private profiles to server-side LLMs. In $P^3$, a large server-side model generates a sequence of $k$ draft tokens based solely on the user query, while a small client-side model, with retrieval access to the user's private profile, evaluates and modifies these drafts to better reflect user preferences. This process repeats until an end token is generated. Experiments on LaMP-QA, a recent benchmark consisting of three personalized question answering datasets, show that $P^3$ consistently outperforms both non-personalized server-side and personalized client-side baselines, achieving statistically significant improvements of $7.4%$ to $9%$ on average. Importantly, $P^3$ recovers $90.3%$ to $95.7%$ of the utility of a ``leaky'' upper-bound scenario in which the full profile is exposed to the large server-side model. Privacy analyses, including linkability and attribute inference attacks, indicate that $P^3$ preserves the privacy of a non-personalized server-side model, introducing only marginal additional leakage ($1.5%$--$3.5%$) compared to submitting a query without any personal context. Additionally, the framework is efficient for edge deployment, with the client-side model generating only $9.2%$ of the total tokens. These results demonstrate that $P^3$ provides a practical, effective solution for personalized generation with improved privacy.

</details>


### [401] [Self-Manager: Parallel Agent Loop for Long-form Deep Research](https://arxiv.org/abs/2601.17879)
*Yilong Xu,Zhi Zheng,Xiang Long,Yujun Cai,Yiwei Wang*

Main category: cs.CL

TL;DR: 现有智能体处理长文深度研究任务有局限，本文提出 Self - Manager 并行智能体循环，实验证明其性能优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有智能体处理复杂长文深度研究任务时采用单上下文窗口和顺序执行范式，存在相互干扰和阻塞行为，限制了可扩展性和适应性。

Method: 引入 Self - Manager 并行智能体循环，主线程创建多个带独立上下文的子线程，并通过线程控制块迭代管理。

Result: 在 DeepResearch Bench 上，Self - Manager 各项指标均优于现有单智能体循环基线。

Conclusion: Self - Manager 的设计有必要性，在上下文容量、效率和泛化能力上有优势。

Abstract: Long-form deep research requires multi-faceted investigations over extended horizons to get a comprehensive report. When handling such complex tasks, existing agents manage context at the subtask level to overcome linear context accumulation and information loss. However, they still adhere to a single context window and sequential execution paradigm, which results in mutual interference and blocking behavior, restricting scalability and adaptability. To address this issue, this paper introduces Self-Manager, a parallel agent loop that enables asynchronous and concurrent execution. The main thread can create multiple subthreads, each with its own isolated context, and manage them iteratively through Thread Control Blocks, allowing for more focused and flexible parallel agent execution. To assess its effectiveness, we benchmark Self-Manager on DeepResearch Bench, where it consistently outperforms existing single-agent loop baselines across all metrics. Furthermore, we conduct extensive analytical experiments to demonstrate the necessity of Self-Manager's design choices, as well as its advantages in contextual capacity, efficiency, and generalization.

</details>


### [402] [Corpus-Based Approaches to Igbo Diacritic Restoration](https://arxiv.org/abs/2601.18380)
*Ignatius Ezeani*

Main category: cs.CL

TL;DR: 论文指出NLP研究多关注资源丰富语言，而世界上多数语言资源匮乏。以伊博语为例，介绍了变音符号歧义情况，提出开发生成变音符号恢复数据集框架的步骤及三种方法。


<details>
  <summary>Details</summary>
Motivation: NLP研究多聚焦于资源丰富语言，全球超95%的语言在NLP方面资源匮乏，需开展对低资源语言的研究。

Method: 以伊博语为例，开发生成变音符号恢复数据集的灵活框架，提出标准n - 元模型、分类模型和嵌入模型三种方法。

Result: 文中未明确提及具体的实验结果。

Conclusion: 文中未提及明确结论。

Abstract: With natural language processing (NLP), researchers aim to enable computers to identify and understand patterns in human languages. This is often difficult because a language embeds many dynamic and varied properties in its syntax, pragmatics and phonology, which need to be captured and processed. The capacity of computers to process natural languages is increasing because NLP researchers are pushing its boundaries. But these research works focus more on well-resourced languages such as English, Japanese, German, French, Russian, Mandarin Chinese, etc. Over 95% of the world's 7000 languages are low-resourced for NLP, i.e. they have little or no data, tools, and techniques for NLP work.
  In this thesis, we present an overview of diacritic ambiguity and a review of previous diacritic disambiguation approaches on other languages. Focusing on the Igbo language, we report the steps taken to develop a flexible framework for generating datasets for diacritic restoration. Three main approaches, the standard n-gram model, the classification models and the embedding models were proposed. The standard n-gram models use a sequence of previous words to the target stripped word as key predictors of the correct variants. For the classification models, a window of words on both sides of the target stripped word was used. The embedding models compare the similarity scores of the combined context word embeddings and the embeddings of each of the candidate variant vectors.

</details>


### [403] [Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory](https://arxiv.org/abs/2601.18771)
*Yanming Liu,Xinyue Peng,Zixuan Yan,Yanxin Shen,Wenjie Xu,Yuefeng Huang,Xinyi Wang,Jiannan Cao,Jianwei Yin,Xuhong Zhang*

Main category: cs.CL

TL;DR: 提出Dep - Search框架解决现有大语言模型搜索框架依赖隐式推理的问题，实验证明其能提升模型多跳推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有搜索框架依赖隐式自然语言推理来确定搜索策略和利用检索信息，在管理子问题依赖、复用知识和学习最优搜索策略方面存在挑战。

Method: 提出Dep - Search框架，通过GRPO集成结构化推理、检索和持久内存，引入显式控制机制。

Result: 在七个不同的问答数据集上的实验表明，Dep - Search显著提升了大语言模型处理复杂多跳推理任务的能力，在不同模型规模下均优于强基线。

Conclusion: Dep - Search框架能有效解决现有大语言模型搜索框架的局限性，提升多跳推理能力。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly when augmented with search mechanisms that enable systematic exploration of external knowledge bases. The field has evolved from traditional retrieval-augmented generation (RAG) frameworks to more sophisticated search-based frameworks that orchestrate multi-step reasoning through explicit search strategies. However, existing search frameworks still rely heavily on implicit natural language reasoning to determine search strategies and how to leverage retrieved information across reasoning steps. This reliance on implicit reasoning creates fundamental challenges for managing dependencies between sub-questions, efficiently reusing previously retrieved knowledge, and learning optimal search strategies through reinforcement learning. To address these limitations, we propose Dep-Search, a dependency-aware search framework that advances beyond existing search frameworks by integrating structured reasoning, retrieval, and persistent memory through GRPO. Dep-Search introduces explicit control mechanisms that enable the model to decompose questions with dependency relationships, retrieve information when needed, access previously stored knowledge from memory, and summarize long reasoning contexts into reusable memory entries. Through extensive experiments on seven diverse question answering datasets, we demonstrate that Dep-Search significantly enhances LLMs' ability to tackle complex multi-hop reasoning tasks, achieving substantial improvements over strong baselines across different model scales.

</details>


### [404] [Crystal-KV: Efficient KV Cache Management for Chain-of-Thought LLMs via Answer-First Principle](https://arxiv.org/abs/2601.16986)
*Zihan Wang,Cheng Tang,Lei Gong,Cheng Li,Chao Wang,teng wang,Wenqi Lou,Xuehai Zhou*

Main category: cs.CL

TL;DR: 提出针对思维链推理的高效KV缓存管理框架Crystal - KV，实现KV缓存压缩、提升吞吐量等。


<details>
  <summary>Details</summary>
Motivation: 思维链推理在大语言模型中虽提升复杂任务准确性，但KV缓存存储长思考阶段序列导致内存开销大，传统KV压缩策略无效。

Method: 提出答案优先原则区分SlipKV和CrystalKV；提出基于注意力的最近最少使用算法淘汰SlipKV；引入自适应缓存预算分配算法调整缓存预算。

Result: Crystal - KV实现了先进的KV缓存压缩，显著提高吞吐量，加快响应时间，同时维持或提高答案准确性。

Conclusion: Crystal - KV是一种高效的KV缓存管理框架，能有效解决思维链推理中的内存开销问题。

Abstract: Chain-of-Thought (CoT) reasoning in large language models (LLMs) significantly improves accuracy on complex tasks, yet incurs excessive memory overhead due to the long think-stage sequences stored in the Key-Value (KV) cache. Unlike traditional generation tasks where all tokens are uniformly important, CoT emphasizes the final answer, rendering conventional KV compression strategies ineffective. In this paper, we present Crystal-KV, an efficient KV cache management framework tailored for CoT reasoning. Our key insight is the answer-first principle. By mapping answer preferences into think-stage attention map, we distinguish between SlipKV, which mainly maintains the reasoning flow but may occasionally introduce misleading context, and CrystalKV, which truly contributes to the correctness of the final answer. Next, we propose an attention-based Least Recently Frequently Used algorithm. It precisely identifies when a SlipKV entry's utility expires and evicts it, retaining CrystalKV without disrupting reasoning flow. Finally, we introduce an adaptive cache budget allocation algorithm. Based on the dynamic proportion of CrystalKV, it estimates the importance of each layer/head and adjusts the KV cache budget during inference, amplifying critical components to improve budget utilization. Results show that Crystal-KV achieves state-of-the-art KV cache compression, significantly improves throughput, and enables faster response time, while maintaining, or even improving, answer accuracy for CoT reasoning.

</details>


### [405] [Evaluating Reward Model Generalization via Pairwise Maximum Discrepancy Competitions](https://arxiv.org/abs/2601.16987)
*Shunyang Luo,Peibei Cao,Zhihui Zhu,Kehua Feng,Zhihua Wang,Keyan Ding*

Main category: cs.CL

TL;DR: 提出PMDC框架评估奖励模型泛化能力，重新评估10个奖励模型有排名变化并发现泛化问题。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型评估依赖静态预标注数据集，覆盖有限且难以评估开放世界泛化能力。

Method: 引入PMDC框架，用未标注开放领域提示池，主动选择使两个奖励模型分歧最大的提示 - 响应对，由oracle裁决，用Bradley - Terry模型聚合结果。

Result: 重新评估10个奖励模型时，与传统基准相比排名有大幅变化。

Conclusion: 该框架能发现系统性泛化失败，为改进奖励建模提供有价值见解。

Abstract: Reward models (RMs) are central to aligning large language models, yet their practical effectiveness hinges on generalization to unseen prompts and shifting distributions. Most existing RM evaluations rely on static, pre-annotated preference datasets, which provide limited coverage and often fail to faithfully assess generalization in open-world settings. We introduce Pairwise Maximum Discrepancy Competition (PMDC), a dynamic and annotation-efficient framework for evaluating RM generalization using a large, unlabeled, open-domain prompt pool. PMDC actively selects prompt--response pairs that maximize disagreement between two RMs, yielding a compact set of highly contentious test cases. These cases are adjudicated by an oracle, and the resulting outcomes are aggregated via a Bradley--Terry model to produce a global ranking and pairwise win-rate landscape of RMs. We apply PMDC to re-evaluate 10 representative RMs and observe substantial rank reshuffling compared with conventional benchmarks. Qualitative analyses further uncover systematic generalization failures, providing valuable insights for improving reward modeling.

</details>


### [406] [Dynamic Role Assignment for Multi-Agent Debate](https://arxiv.org/abs/2601.17152)
*Miao Zhang,Junsik Kim,Siyuan Xiang,Jian Gao,Cheng Cao*

Main category: cs.CL

TL;DR: 提出动态角色分配框架，在多智能体大语言模型和视觉语言模型辩论系统中通过元辩论选择合适代理，实验表现优于统一和随机分配，开创了多智能体系统设计新范式


<details>
  <summary>Details</summary>
Motivation: 现有多智能体大语言模型和视觉语言模型辩论系统未利用模型专业化来决定角色分配

Method: 提出动态角色分配框架，包含提案和同行评审两个阶段的元辩论过程

Result: 在LLM问题解决基准测试中，该方法在现有辩论系统上表现优于统一分配达74.8%、随机分配达29.7%

Conclusion: 建立了多智能体系统设计的新范式，从静态代理部署转向动态和基于能力的选择

Abstract: Multi-agent large language model (LLM) and vision-language model (VLM) debate systems employ specialized roles for complex problem-solving, yet model specializations are not leveraged to decide which model should fill which role. We propose dynamic role assignment, a framework that runs a Meta-Debate to select suitable agents before the actual debate. The meta-debate has two stages: (1) proposal, where candidates provide role-tailored arguments, and (2) peer review, where proposals are scored with data and role-specific criteria to choose the best agent for each position. We evaluate our method on LLM problem solving benchmarks. Applied on top of existing debate systems, our approach consistently outperforms uniform assignments (filling all roles with the same model) by up to 74.8% and random assignments (assigning models to roles without considering their suitability) by up to 29.7%, depending on the task and the specific assignment. This work establishes a new paradigm for multi-agent system design, shifting from static agent deployment to dynamic and capability-aware selection.

</details>


### [407] [Interpretability of the Intent Detection Problem: A New Approach](https://arxiv.org/abs/2601.17156)
*Eduardo Sanchez-Karhunen,Jose F. Quesada-Moreno,Miguel A. Gutiérrez-Naranjo*

Main category: cs.CL

TL;DR: 本文运用动力系统理论分析RNN架构解决意图检测任务的机制，揭示数据集属性对网络计算解决方案的影响。


<details>
  <summary>Details</summary>
Motivation: 深度学习技术在意图检测领域占主导，但RNN解决该任务的内部机制尚不清楚。

Method: 运用动力系统理论，使用平衡的SNIPS和不平衡的ATIS数据集，将句子解释为隐藏状态空间中的轨迹。

Result: 在平衡的SNIPS数据集上，网络学习到理想解决方案；在不平衡的ATIS数据集上，理想几何解决方案被类别不平衡所扭曲。

Conclusion: 研究框架为现实世界的性能差异提供了新的机理解释，为RNN动力学提供了新见解。

Abstract: Intent detection, a fundamental text classification task, aims to identify and label the semantics of user queries, playing a vital role in numerous business applications. Despite the dominance of deep learning techniques in this field, the internal mechanisms enabling Recurrent Neural Networks (RNNs) to solve intent detection tasks are poorly understood. In this work, we apply dynamical systems theory to analyze how RNN architectures address this problem, using both the balanced SNIPS and the imbalanced ATIS datasets. By interpreting sentences as trajectories in the hidden state space, we first show that on the balanced SNIPS dataset, the network learns an ideal solution: the state space, constrained to a low-dimensional manifold, is partitioned into distinct clusters corresponding to each intent. The application of this framework to the imbalanced ATIS dataset then reveals how this ideal geometric solution is distorted by class imbalance, causing the clusters for low-frequency intents to degrade. Our framework decouples geometric separation from readout alignment, providing a novel, mechanistic explanation for real world performance disparities. These findings provide new insights into RNN dynamics, offering a geometric interpretation of how dataset properties directly shape a network's computational solution.

</details>


### [408] [Who Gets Which Message? Auditing Demographic Bias in LLM-Generated Targeted Text](https://arxiv.org/abs/2601.17172)
*Tunazzina Islam*

Main category: cs.CL

TL;DR: 本文首次系统分析大语言模型在面向不同人群定制消息时情况，通过框架评估，发现多模型生成消息有年龄和性别不平等问题，强调需关注偏见。


<details>
  <summary>Details</summary>
Motivation: 大语言模型大规模生成个性化说服力文本时，引发自动化通信中偏见和公平性新问题，研究其在面向不同人群定制消息时的表现。

Method: 引入控制评估框架，用三个主流模型在两种生成设置（独立生成和富上下文生成）下，从词汇内容、语言风格和说服框架三方面评估生成消息。

Result: 在气候传播实例中，各模型生成内容有年龄和性别不对称性，上下文提示放大了这种差异，对年轻或男性受众消息说服力更强。

Conclusion: 人口统计学刻板印象会在大语言模型生成的定向通信中显现并加剧，社会敏感应用需考虑人口统计学因素的偏见感知生成管道和透明审计框架。

Abstract: Large language models (LLMs) are increasingly capable of generating personalized, persuasive text at scale, raising new questions about bias and fairness in automated communication. This paper presents the first systematic analysis of how LLMs behave when tasked with demographic-conditioned targeted messaging. We introduce a controlled evaluation framework using three leading models -- GPT-4o, Llama-3.3, and Mistral-Large 2.1 -- across two generation settings: Standalone Generation, which isolates intrinsic demographic effects, and Context-Rich Generation, which incorporates thematic and regional context to emulate realistic targeting. We evaluate generated messages along three dimensions: lexical content, language style, and persuasive framing. We instantiate this framework on climate communication and find consistent age- and gender-based asymmetries across models: male- and youth-targeted messages emphasize agency, innovation, and assertiveness, while female- and senior-targeted messages stress warmth, care, and tradition. Contextual prompts systematically amplify these disparities, with persuasion scores significantly higher for messages tailored to younger or male audiences. Our findings demonstrate how demographic stereotypes can surface and intensify in LLM-generated targeted communication, underscoring the need for bias-aware generation pipelines and transparent auditing frameworks that explicitly account for demographic conditioning in socially sensitive applications.

</details>


### [409] [Beyond Factual QA: Mentorship-Oriented Question Answering over Long-Form Multilingual Content](https://arxiv.org/abs/2601.17173)
*Parth Bhalerao,Diola Dsouza,Ruiwen Guan,Oana Ignat*

Main category: cs.CL

TL;DR: 提出多语言数据集和评估框架MentorQA，用于长视频问答。对比不同架构，多智能体架构效果更佳，分析自动评估与人工判断一致性。


<details>
  <summary>Details</summary>
Motivation: 现有问答基准很少关注现实应用中指导型回答，特别是多语言和长文本场景。

Method: 引入MentorQA数据集和评估框架，定义评估维度，对比不同问答架构，分析自动评估可靠性。

Result: 多智能体管道产生更高质量指导型回答，复杂话题和低资源语言表现更佳，自动评估与人工判断一致性有差异。

Conclusion: 明确指导型问答是新研究问题，提供多语言基准用于教育AI研究，发布数据集和框架。

Abstract: Question answering systems are typically evaluated on factual correctness, yet many real-world applications-such as education and career guidance-require mentorship: responses that provide reflection and guidance. Existing QA benchmarks rarely capture this distinction, particularly in multilingual and long-form settings. We introduce MentorQA, the first multilingual dataset and evaluation framework for mentorship-focused question answering from long-form videos, comprising nearly 9,000 QA pairs from 180 hours of content across four languages. We define mentorship-focused evaluation dimensions that go beyond factual accuracy, capturing clarity, alignment, and learning value. Using MentorQA, we compare Single-Agent, Dual-Agent, RAG, and Multi-Agent QA architectures under controlled conditions. Multi-Agent pipelines consistently produce higher-quality mentorship responses, with especially strong gains for complex topics and lower-resource languages. We further analyze the reliability of automated LLM-based evaluation, observing substantial variation in alignment with human judgments. Overall, this work establishes mentorship-focused QA as a distinct research problem and provides a multilingual benchmark for studying agentic architectures and evaluation design in educational AI. The dataset and evaluation framework are released at https://github.com/AIM-SCU/MentorQA.

</details>


### [410] [Beyond Outcome Verification: Verifiable Process Reward Models for Structured Reasoning](https://arxiv.org/abs/2601.17223)
*Massimiliano Pronesti,Anya Belz,Yufang Hou*

Main category: cs.CL

TL;DR: 本文引入可验证过程奖励模型（VPRMs）用于强化学习，在医学证据合成的偏倚风险评估中应用，结果显示其效果优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于过程监督的大语言模型方法依赖神经评判器，易受不透明性、偏差和奖励破解问题影响，需要改进。

Method: 引入VPRMs，用基于规则的确定性验证器检查中间推理步骤，并将其应用于医学证据合成的偏倚风险评估。

Result: VPRMs生成的推理符合领域规则，步骤决策与最终标签的一致性更高，F1值比最先进模型高20%，比可验证结果奖励高6.5%。

Conclusion: VPRMs在证据基础和逻辑连贯性上有显著提升，是一种有效的强化学习框架。

Abstract: Recent work on reinforcement learning with verifiable rewards (RLVR) has shown that large language models (LLMs) can be substantially improved using outcome-level verification signals, such as unit tests for code or exact-match checks for mathematics. In parallel, process supervision has long been explored as a way to shape the intermediate reasoning behaviour of LLMs, but existing approaches rely on neural judges to score chain-of-thought steps, leaving them vulnerable to opacity, bias, and reward hacking. To address this gap, we introduce Verifiable Process Reward Models (VPRMs), a reinforcement-learning framework in which intermediate reasoning steps are checked by deterministic, rule-based verifiers. We apply VPRMs to risk-of-bias assessment for medical evidence synthesis, a domain where guideline-defined criteria and rule-based decision paths enable programmatic verification of reasoning traces. Across multiple datasets, we find that VPRMs generate reasoning that adheres closely to domain rules and achieve substantially higher coherence between step-level decisions and final labels. Results show that VPRMs achieve up to 20% higher F1 than state-of-the-art models and 6.5% higher than verifiable outcome rewards, with substantial gains in evidence grounding and logical coherence.

</details>


### [411] [Retell, Reward, Repeat: Reinforcement Learning for Narrative Theory-Informed Story Generation](https://arxiv.org/abs/2601.17226)
*David Y. Liu,Xanthe Muston,Aditya Joshi,Sebastian Sequoiah-Grayson*

Main category: cs.CL

TL;DR: 探索强化学习（d - RLAIF）作为自动故事生成（ASG）后训练替代监督微调（SFT）的方法，证明其能生成更多样且符合人类叙事习惯的故事。


<details>
  <summary>Details</summary>
Motivation: 过去自动故事生成工作在训练和评估中依赖有限的事实依据，需要更好的方法。

Method: 应用Todorov叙事均衡理论确立理想ASG品质原则，用原则提示7B和14B大语言模型作为评判模型以提供奖励信号，用Gemini - 3 - Flash评估模型输出并与人类故事对比。

Result: d - RLAIF生成的故事更多样且符合人类叙事习惯，是监督微调的可行替代方案。

Conclusion: 强化学习在如ASG这类主观任务的语言基础后训练中有应用前景。

Abstract: Despite the subjective nature of storytelling, past works on automatic story generation (ASG) have relied on limited ground truths for training and evaluation. In this work, we explore reinforcement learning (d-RLAIF) as a post-training alternative to supervised fine-tuning (SFT). We first apply Todorov's Theory of Narrative Equilibrium to establish principles that define desirable ASG qualities. We prompt 7B and 14B LLM-as-judge models with our principles to test alignment with human annotators and provide reward signals during d-RLAIF. We use Gemini-3-Flash to evaluate the output of our post-trained models and compare them to human-written stories from the TimeTravel dataset. We show that d-RLAIF offers a viable alternative to supervised fine-tuning (SFT)--producing stories that are more diverse and aligned with human narrative conventions. Our paper demonstrates the promise of reinforcement learning for linguistically grounded post-training for subjective tasks such as ASG.

</details>


### [412] [Mind the Ambiguity: Aleatoric Uncertainty Quantification in LLMs for Safe Medical Question Answering](https://arxiv.org/abs/2601.17284)
*Yaokun Liu,Yifan Liu,Phoebe Mbuvi,Zelin Li,Ruichen Yao,Gawon Lim,Dong Wang*

Main category: cs.CL

TL;DR: 本文针对大语言模型在医疗问答中因输入模糊导致的问题，构建CV - MedBench基准，从表征工程角度分析模糊性与不确定性关系，提出“先澄清再回答”框架，实验证明有效提升准确率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在医疗问答中因用户查询模糊，存在安全风险且降低答案准确性，需解决输入模糊问题。

Method: 将输入模糊与固有不确定性（AU）关联，构建CV - MedBench基准，从表征工程角度分析AU，引入AU - 引导的“先澄清再回答”框架及轻量级模块AU - Probe。

Result: 在四个开源大语言模型上实验表明，该问答框架平均准确率比基线提高9.48%。

Conclusion: 框架为安全医疗问答提供高效可靠方案，增强健康相关应用可靠性。代码和数据集已开源。

Abstract: The deployment of Large Language Models in Medical Question Answering is severely hampered by ambiguous user queries, a significant safety risk that demonstrably reduces answer accuracy in high-stakes healthcare settings. In this paper, we formalize this challenge by linking input ambiguity to aleatoric uncertainty (AU), which is the irreducible uncertainty arising from underspecified input. To facilitate research in this direction, we construct CV-MedBench, the first benchmark designed for studying input ambiguity in Medical QA. Using this benchmark, we analyze AU from a representation engineering perspective, revealing that AU is linearly encoded in LLM's internal activation patterns. Leveraging this insight, we introduce a novel AU-guided "Clarify-Before-Answer" framework, which incorporates AU-Probe - a lightweight module that detects input ambiguity directly from hidden states. Unlike existing uncertainty estimation methods, AU-Probe requires neither LLM fine-tuning nor multiple forward passes, enabling an efficient mechanism to proactively request user clarification and significantly enhance safety. Extensive experiments across four open LLMs demonstrate the effectiveness of our QA framework, with an average accuracy improvement of 9.48% over baselines. Our framework provides an efficient and robust solution for safe Medical QA, strengthening the reliability of health-related applications. The code is available at https://github.com/yaokunliu/AU-Med.git, and the CV-MedBench dataset is released on Hugging Face at https://huggingface.co/datasets/yaokunl/CV-MedBench.

</details>


### [413] [Meta-Judging with Large Language Models: Concepts, Methods, and Challenges](https://arxiv.org/abs/2601.17312)
*Hugo Silva,Mateus Mendes,Hugo Gonçalo Oliveira*

Main category: cs.CL

TL;DR: 本文回顾大语言模型作为元评判者的最新进展，指出其是自动化评估的有前景方向，但仍存在挑战。


<details>
  <summary>Details</summary>
Motivation: 大语言模型作为评判者的评估方式存在敏感性、偏差等局限性，需开发更稳健的范式。

Method: 引入六个关键视角的框架对元评判相关文献进行组织和综述。

Result: 分析了大语言模型作为评判者的局限，总结了元评判的进展。

Conclusion: 大语言模型作为元评判者是更稳定可信的自动化评估方向，但存在成本、敏感性和偏差等挑战待解决。

Abstract: Large language models (LLMs) are evolving fast and are now frequently used as evaluators, in a process typically referred to as LLM-as-a-Judge, which provides quality assessments of model outputs. However, recent research points out significant vulnerabilities in such evaluation, including sensitivity to prompts, systematic biases, verbosity effects, and unreliable or hallucinated rationales. These limitations motivated the development of a more robust paradigm, dubbed LLM-as-a-Meta-Judge. This survey reviews recent advances in meta-judging and organizes the literature, by introducing a framework along six key perspectives: (i) Conceptual Foundations, (ii) Mechanisms of Meta-Judging, (iii) Alignment Training Methods, (iv) Evaluation, (v) Limitations and Failure Modes, and (vi) Future Directions. By analyzing the limitations of LLM-as-a-Judge and summarizing recent advances in meta-judging by LLMs, we argue that LLM-as-a-Meta-Judge offers a promising direction for more stable and trustworthy automated evaluation, while highlighting remaining challenges related to cost, prompt sensitivity, and shared model biases, which must be addressed to advance the next generation of LLM evaluation methodologies.

</details>


### [414] [Do readers prefer AI-generated Italian short stories?](https://arxiv.org/abs/2601.17363)
*Michael Farrell*

Main category: cs.CL

TL;DR: 研究调查读者是否更偏好AI生成的意大利语短篇小说而非著名作家作品，结果显示AI作品评分略高但差异不大，且偏好与人口及阅读习惯无显著关联。


<details>
  <summary>Details</summary>
Motivation: 探究读者是否更偏好AI生成的意大利语短篇小说而非著名意大利作家所写的故事。

Method: 在盲测环境下，让20名参与者阅读并评价两篇ChatGPT - 4o生成的故事和一篇阿尔贝托·莫拉维亚的故事，同时收集参与者的阅读习惯和人口统计数据。

Result: AI撰写的文本平均评分略高且更常被偏好，但差异不大；文本偏好与人口统计或阅读习惯变量无统计学显著关联。

Conclusion: 研究结果挑战了读者更偏好人类创作小说的假设，并引发对文学语境中合成文本编辑必要性的质疑。

Abstract: This study investigates whether readers prefer AI-generated short stories in Italian over one written by a renowned Italian author. In a blind setup, 20 participants read and evaluated three stories, two created with ChatGPT-4o and one by Alberto Moravia, without being informed of their origin. To explore potential influencing factors, reading habits and demographic data, comprising age, gender, education and first language, were also collected. The results showed that the AI-written texts received slightly higher average ratings and were more frequently preferred, although differences were modest. No statistically significant associations were found between text preference and demographic or reading-habit variables. These findings challenge assumptions about reader preference for human-authored fiction and raise questions about the necessity of synthetic-text editing in literary contexts.

</details>


### [415] [Parameter Efficient Fine Tuning Llama 3.1 for Answering Arabic Legal Questions: A Case Study on Jordanian Laws](https://arxiv.org/abs/2601.17364)
*Mohammed Fasha,Bassam Hammo,Bilal Sowan,Husam Barham,Esam Nsour*

Main category: cs.CL

TL;DR: 以约旦法律为案例，微调Llama - 3.1大语言模型用于阿拉伯语问答，展示了提升效果和资源效率。


<details>
  <summary>Details</summary>
Motivation: 探索对Llama - 3.1大语言模型进行微调以用于阿拉伯语问答场景，特别是在法律领域。

Method: 使用参数高效微调（PEFT）和LoRA适配器对Llama - 3.1的两个版本进行微调，结合4位量化模型，借助Unsloth框架训练；创建包含6000个法律问答对的自定义数据集；用BLEU和ROUGE指标评估性能。

Result: 微调后的模型在法律推理和准确性上有所提升，通过量化和优化策略实现了资源高效利用。

Conclusion: 证明大语言模型适配阿拉伯法律领域的潜力，强调微调特定领域任务的有效技术。

Abstract: This study uses Jordanian law as a case study to explore the fine-tuning of the Llama-3.1 large language model for Arabic question-answering. Two versions of the model - Llama-3.1-8B-bnb-4bit and Llama-3.1-8B-Instruct-bnb-4bit - were fine-tuned using parameter-efficient fine-tuning (PEFT) with LoRA adapters and 4-bit quantized models, leveraging the Unsloth framework for accelerated and resource-efficient training. A custom dataset of 6000 legal question-answer pairs was curated from Jordanian laws and formatted into structured prompts. Performance was evaluated using the BLEU and the ROUGE metrics to compare the fine-tuned models to their respective base versions. Results demonstrated improved legal reasoning and accuracy while achieving resource efficiency through quantization and optimized fine-tuning strategies. This work underscores the potential of adapting large language models for Arabic legal domains and highlights effective techniques for fine-tuning domain-specific tasks.

</details>


### [416] [Elastic Attention: Test-time Adaptive Sparsity Ratios for Efficient Transformers](https://arxiv.org/abs/2601.17367)
*Zecheng Tang,Quantong Qiu,Yi Yang,Zhiyi Hong,Haiya Xiang,Kebin Liu,Qingqing Dang,Juntao Li,Min Zhang*

Main category: cs.CL

TL;DR: 标准注意力机制复杂度影响大语言模型长文本处理，提出弹性注意力机制解决该问题且效果好。


<details>
  <summary>Details</summary>
Motivation: 标准注意力机制复杂度高，现有混合注意力策略静态计算比例无法适应下游任务稀疏敏感性变化。

Method: 提出弹性注意力机制，在现有预训练模型中集成轻量级注意力路由器，动态分配注意力头计算模式。

Result: 使用8xA800 GPU训练12小时，模型有强性能及高效推理，在三个长文本基准测试中表现优。

Conclusion: 所提弹性注意力机制具有优越性。

Abstract: The quadratic complexity of standard attention mechanisms poses a significant scalability bottleneck for large language models (LLMs) in long-context scenarios. While hybrid attention strategies that combine sparse and full attention within a single model offer a viable solution, they typically employ static computation ratios (i.e., fixed proportions of sparse versus full attention) and fail to adapt to the varying sparsity sensitivities of downstream tasks during inference. To address this issue, we propose Elastic Attention, which allows the model to dynamically adjust its overall sparsity based on the input. This is achieved by integrating a lightweight Attention Router into the existing pretrained model, which dynamically assigns each attention head to different computation modes. Within only 12 hours of training on 8xA800 GPUs, our method enables models to achieve both strong performance and efficient inference. Experiments across three long-context benchmarks on widely-used LLMs demonstrate the superiority of our method.

</details>


### [417] [Clustering-driven Memory Compression for On-device Large Language Models](https://arxiv.org/abs/2601.17443)
*Ondrej Bohdal,Pramit Saha,Umberto Michieli,Mete Ozay,Taha Ceritli*

Main category: cs.CL

TL;DR: 提出基于聚类的内存压缩策略，平衡上下文效率和个性化质量，实验表明优于基线策略。


<details>
  <summary>Details</summary>
Motivation: 现有将用户记忆与输入提示连接的方法会耗尽设备端大语言模型有限上下文，平均压缩记忆会因语义冲突损害性能。

Method: 基于相似度对记忆进行聚类，并在连接前合并簇内记忆。

Result: 大幅降低记忆令牌数量，在固定上下文预算下生成更紧凑记忆表示，持续提升生成质量。

Conclusion: 基于聚类的内存压缩策略能有效平衡上下文效率和个性化质量，优于基线策略。

Abstract: Large language models (LLMs) often rely on user-specific memories distilled from past interactions to enable personalized generation. A common practice is to concatenate these memories with the input prompt, but this approach quickly exhausts the limited context available in on-device LLMs. Compressing memories by averaging can mitigate context growth, yet it frequently harms performance due to semantic conflicts across heterogeneous memories. In this work, we introduce a clustering-based memory compression strategy that balances context efficiency and personalization quality. Our method groups memories by similarity and merges them within clusters prior to concatenation, thereby preserving coherence while reducing redundancy. Experiments demonstrate that our approach substantially lowers the number of memory tokens while outperforming baseline strategies such as naive averaging or direct concatenation. Furthermore, for a fixed context budget, clustering-driven merging yields more compact memory representations and consistently enhances generation quality.

</details>


### [418] [Less is More for RAG: Information Gain Pruning for Generator-Aligned Reranking and Evidence Selection](https://arxiv.org/abs/2601.17532)
*Zhipeng Song,Yizhi Zhou,Xiangyu Kong,Jiulong Jiao,Xinrui Bao,Xu You,Xueqing Shi,Yuhang Zhou,Heng Qi*

Main category: cs.CL

TL;DR: RAG在有限上下文预算下需决定注入哪些检索段落，现有检索相关性指标与端到端问答质量相关性弱，提出IGP模块，在多基准测试中改善质量 - 成本权衡。


<details>
  <summary>Details</summary>
Motivation: 解决RAG在有限上下文预算下，决定注入哪些检索段落的关键挑战，且现有检索相关性指标与问答质量相关性不佳。

Method: 提出信息增益剪枝（IGP）模块，使用与生成器对齐的效用信号选择证据，在截断前过滤弱或有害段落。

Result: 在五个开放域问答基准测试、多个检索器和生成器中，IGP持续改善质量 - 成本权衡，在多证据场景中，平均F1相对提高约12 - 20%，减少约76 - 79%的最终阶段输入令牌。

Conclusion: IGP模块能有效解决RAG在有限上下文预算下的段落选择问题，改善质量 - 成本权衡。

Abstract: Retrieval-augmented generation (RAG) grounds large language models with external evidence, but under a limited context budget, the key challenge is deciding which retrieved passages should be injected. We show that retrieval relevance metrics (e.g., NDCG) correlate weakly with end-to-end QA quality and can even become negatively correlated under multi-passage injection, where redundancy and mild conflicts destabilize generation. We propose \textbf{Information Gain Pruning (IGP)}, a deployment-friendly reranking-and-pruning module that selects evidence using a generator-aligned utility signal and filters weak or harmful passages before truncation, without changing existing budget interfaces. Across five open-domain QA benchmarks and multiple retrievers and generators, IGP consistently improves the quality--cost trade-off. In a representative multi-evidence setting, IGP delivers about +12--20% relative improvement in average F1 while reducing final-stage input tokens by roughly 76--79% compared to retriever-only baselines.

</details>


### [419] [UrduLM: A Resource-Efficient Monolingual Urdu Language Model](https://arxiv.org/abs/2601.17664)
*Syed Muhammad Ali,Hammad Sajid,Zainab Haider,Ali Muhammad Asad,Haya Fatima,Abdul Samad*

Main category: cs.CL

TL;DR: 论文提出针对乌尔都语缺乏专属模型和语料的问题，推出乌尔都语单语预训练模型UrduLM，在少样本评估中表现良好并公开相关方法。


<details>
  <summary>Details</summary>
Motivation: 乌尔都语缺乏专用模型和语料，多语言模型对乌尔都语支持有限、性能差、成本高且存在文化不准确问题。

Method: 从多源收集33GB乌尔都语语料，开发自定义BPE分词器，预训练1亿参数的仅解码器模型。

Result: 在少样本评估中，UrduLM与大30倍的多语言模型竞争，情感分类准确率达66.6%，语法纠正任务BLEU分数超30。

Conclusion: 公开完整方法，为乌尔都语NLP研究建立基线，为其他低资源语言提供可扩展框架。

Abstract: Urdu, spoken by 230 million people worldwide, lacks dedicated transformer-based language models and curated corpora. While multilingual models provide limited Urdu support, they suffer from poor performance, high computational costs, and cultural inaccuracies due to insufficient training data. To address these challenges, we present UrduLM, a pretrained Urdu monolingual language model trained in low-resource settings. We curate a 33GB Urdu corpus from diverse sources, develop a custom BPE tokenizer that reduces tokenization overhead by atleast 20-30% compared to multilingual alternatives, and pretrain a 100M-parameter decoder-only model. In few-shot evaluations, UrduLM achieves competitive performance with multilingual models up to 30x its size, reaching 66.6% accuracy on sentiment classification and BLEU scores exceeding 30 on grammar correction tasks. The complete methodology -- including corpus, tokenizer, model weights, and evaluation benchmarks -- is released openly to establish a baseline for Urdu NLP research and provide a scalable framework for other underrepresented languages.

</details>


### [420] [Reasoning Beyond Literal: Cross-style Multimodal Reasoning for Figurative Language Understanding](https://arxiv.org/abs/2601.17197)
*Seyyed Saeid Cheshmi,Hahnemann Ortiz,James Mooney,Dongyeop Kang*

Main category: cs.CL

TL;DR: 提出三步框架开发多模态推理模型处理比喻性语言，实验表明该模型在跨风格泛化和推理能力上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在处理比喻性语言（如讽刺、幽默、隐喻）的多模态任务中有挑战，需能跨模态推理并考虑主观性的模型。

Method: 提出三步框架开发高效多模态推理模型，用于解释多模态比喻性语言、提供透明推理痕迹、跨多种比喻风格泛化。

Result: 纳入推理痕迹显著提升多模态比喻理解；一种风格的推理可迁移到其他风格；跨风格联合训练的通用推理VLM优于更大的开源和闭源模型。

Conclusion: 轻量级且推理可验证的VLM能在多模态任务中实现强大的跨风格泛化，并提供可检查的推理痕迹。

Abstract: Vision-language models (VLMs) have demonstrated strong reasoning abilities in literal multimodal tasks such as visual mathematics and science question answering. However, figurative language, such as sarcasm, humor, and metaphor, remains a significant challenge, as it conveys intent and emotion through subtle incongruities between expressed and intended meanings. In multimodal settings, accompanying images can amplify or invert textual meaning, demanding models that reason across modalities and account for subjectivity. We propose a three-step framework for developing efficient multimodal reasoning models that can (i) interpret multimodal figurative language, (ii) provide transparent reasoning traces, and (iii) generalize across multiple figurative styles. Experiments across four styles show that (1) incorporating reasoning traces substantially improves multimodal figurative understanding, (2) reasoning learned in one style can transfer to others, especially between related styles like sarcasm and humor, and (3) training jointly across styles yields a generalized reasoning VLM that outperforms much larger open- and closed-source models. Our findings show that lightweight VLMs with verifiable reasoning achieve robust cross-style generalization while providing inspectable reasoning traces for multimodal tasks. The code and implementation are available at https://github.com/scheshmi/CrossStyle-MMR.

</details>


### [421] [CaseFacts: A Benchmark for Legal Fact-Checking and Precedent Retrieval](https://arxiv.org/abs/2601.17230)
*Akshith Reddy Putta,Jacob Devasier,Chengkai Li*

Main category: cs.CL

TL;DR: 介绍法律事实核查基准CaseFacts，包括其构成、构建方法，实验表明法律事实核查任务仍具挑战，发布该基准以推动研究。


<details>
  <summary>Details</summary>
Motivation: 现有自动事实核查多针对一般知识，忽视法律等高风险领域，需构建法律事实核查基准。

Method: 采用多阶段管道，利用大语言模型从专家案例摘要合成声明，用新的语义相似性启发式方法识别和验证法律推翻情况。

Result: 实验显示法律事实核查任务仍有挑战，使用无限制网络搜索会降低模型性能。

Conclusion: 发布CaseFacts以促进法律事实核查系统的研究。

Abstract: Automated Fact-Checking has largely focused on verifying general knowledge against static corpora, overlooking high-stakes domains like law where truth is evolving and technically complex. We introduce CaseFacts, a benchmark for verifying colloquial legal claims against U.S. Supreme Court precedents. Unlike existing resources that map formal texts to formal texts, CaseFacts challenges systems to bridge the semantic gap between layperson assertions and technical jurisprudence while accounting for temporal validity. The dataset consists of 6,294 claims categorized as Supported, Refuted, or Overruled. We construct this benchmark using a multi-stage pipeline that leverages Large Language Models (LLMs) to synthesize claims from expert case summaries, employing a novel semantic similarity heuristic to efficiently identify and verify complex legal overrulings. Experiments with state-of-the-art LLMs reveal that the task remains challenging; notably, augmenting models with unrestricted web search degrades performance compared to closed-book baselines due to the retrieval of noisy, non-authoritative precedents. We release CaseFacts to spur research into legal fact verification systems.

</details>


### [422] [Cross-Lingual Probing and Community-Grounded Analysis of Gender Bias in Low-Resource Bengali](https://arxiv.org/abs/2601.17764)
*Md Asgor Hossain Reaj,Rajan Das Gupta,Jui Saha Pritha,Abdullah Al Noman,Abir Ahmed,Golam Md Mohiuddin,Tze Hui Liew*

Main category: cs.CL

TL;DR: 研究孟加拉语大语言模型中的性别偏见，发现与英语不同，需本地化方法，强调社区参与和针对性语言工具开发。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型性别偏见问题在非英语语言中受关注少，尤其是孟加拉语这类全球南方语言，需研究其性别偏见。

Method: 采用基于词法挖掘、计算分类模型、基于翻译的比较分析和基于GPT的偏见创建等方法提取性别偏见话语，还进行两次实地调查。

Result: 直接应用以英语为中心的偏见检测框架到孟加拉语受限，孟加拉语性别偏见有独特特征。

Conclusion: 需要本地化和上下文敏感的方法，整合社区驱动研究方法，开发针对代表性不足语言的语言工具，为减少孟加拉语及其他印度语系语言的偏见研究奠定基础。

Abstract: Large Language Models (LLMs) have achieved significant success in recent years; yet, issues of intrinsic gender bias persist, especially in non-English languages. Although current research mostly emphasizes English, the linguistic and cultural biases inherent in Global South languages, like Bengali, are little examined. This research seeks to examine the characteristics and magnitude of gender bias in Bengali, evaluating the efficacy of current approaches in identifying and alleviating bias. We use several methods to extract gender-biased utterances, including lexicon-based mining, computational classification models, translation-based comparison analysis, and GPT-based bias creation. Our research indicates that the straight application of English-centric bias detection frameworks to Bengali is severely constrained by language disparities and socio-cultural factors that impact implicit biases. To tackle these difficulties, we executed two field investigations inside rural and low-income areas, gathering authentic insights on gender bias. The findings demonstrate that gender bias in Bengali presents distinct characteristics relative to English, requiring a more localized and context-sensitive methodology. Additionally, our research emphasizes the need of integrating community-driven research approaches to identify culturally relevant biases often neglected by automated systems. Our research enhances the ongoing discussion around gender bias in AI by illustrating the need to create linguistic tools specifically designed for underrepresented languages. This study establishes a foundation for further investigations into bias reduction in Bengali and other Indic languages, promoting the development of more inclusive and fair NLP systems.

</details>


### [423] [DPI: Exploiting Parameter Heterogeneity for Interference-Free Fine-Tuning](https://arxiv.org/abs/2601.17777)
*Xiaoyu Liu,Xiaoyu Guan,Di Liang,Xianjie Wu*

Main category: cs.CL

TL;DR: 本文针对大语言模型监督微调中的‘跷跷板效应’，提出动态参数隔离策略，实验证明该方法能减少数据冲突并提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型监督微调中不同任务目标冲突导致的‘跷跷板效应’问题。

Method: 先在不同监督微调任务上独立微调大语言模型，确定各任务核心参数区域，重叠度高的任务合并训练，不相交任务分阶段训练，多阶段微调时冻结先前任务获得的核心参数。

Result: 动态参数隔离策略持续减少了数据冲突，相比多阶段和多任务调优基线，性能有持续提升。

Conclusion: 所提出的动态参数隔离策略有效，能解决监督微调任务中的交叉干扰问题。

Abstract: Supervised fine-tuning (SFT) is a crucial step for adapting large language models (LLMs) to downstream tasks. However, conflicting objectives across heterogeneous SFT tasks often induce the "seesaw effect": optimizing for one task may degrade performance on others, particularly when model parameters are updated indiscriminately. In this paper, we propose a principled approach to disentangle and isolate task-specific parameter regions, motivated by the hypothesis that parameter heterogeneity underlies cross-task interference. Specifically, we first independently fine-tune LLMs on diverse SFT tasks and identify each task's core parameter region as the subset of parameters exhibiting the largest updates. Tasks with highly overlapping core parameter regions are merged for joint training, while disjoint tasks are organized into different stages. During multi-stage SFT, core parameters acquired in prior tasks are frozen, thereby preventing overwriting by subsequent tasks. To verify the effectiveness of our method, we conducted intensive experiments on multiple public datasets. The results showed that our dynamic parameter isolation strategy consistently reduced data conflicts and achieved consistent performance improvements compared to multi-stage and multi-task tuning baselines.

</details>


### [424] [DIETA: A Decoder-only transformer-based model for Italian-English machine TrAnslation](https://arxiv.org/abs/2601.17823)
*Pranav Kasela,Marco Braga,Alessandro Ghiotto,Andrea Pilzer,Marco Viviani,Alessandro Raganato*

Main category: cs.CL

TL;DR: 提出用于意英机器翻译的小模型DIETA，收集语料、创建评估集，评估性能好且公开资源。


<details>
  <summary>Details</summary>
Motivation: 进行意英机器翻译研究，开发适用于意英翻译的模型。

Method: 设计训练有5亿参数的仅解码器Transformer模型DIETA，收集约2.07亿意英句子对及3.52亿回译数据，基于2025篇WikiNews文章创建含450句的评估集。

Result: DIETA在多个意英基准测试中表现有竞争力，在32个系统排行榜中处第二四分位，在五个测试套件中的四个优于多数小于30亿参数模型。

Conclusion: 公开训练脚本、模型、语料和评估集，利于意英机器翻译研究发展。

Abstract: In this paper, we present DIETA, a small, decoder-only Transformer model with 0.5 billion parameters, specifically designed and trained for Italian-English machine translation. We collect and curate a large parallel corpus consisting of approximately 207 million Italian-English sentence pairs across diverse domains, including parliamentary proceedings, legal texts, web-crawled content, subtitles, news, literature and 352 million back-translated data using pretrained models. Additionally, we create and release a new small-scale evaluation set, consisting of 450 sentences, based on 2025 WikiNews articles, enabling assessment of translation quality on contemporary text. Comprehensive evaluations show that DIETA achieves competitive performance on multiple Italian-English benchmarks, consistently ranking in the second quartile of a 32-system leaderboard and outperforming most other sub-3B models on four out of five test suites. The training script, trained models, curated corpus, and newly introduced evaluation set are made publicly available, facilitating further research and development in specialized Italian-English machine translation. https://github.com/pkasela/DIETA-Machine-Translation

</details>


### [425] [Linguistic and Argument Diversity in Synthetic Data for Function-Calling Agents](https://arxiv.org/abs/2601.17829)
*Dan Greenstein,Zohar Karnin,Chen Amiraz,Oren Somekh*

Main category: cs.CL

TL;DR: 提出通过优化查询和参数多样性指标生成合成数据集的方法，经测试优于基线方法，在BFCL基准上准确率提升7.4%。


<details>
  <summary>Details</summary>
Motivation: 构建函数调用代理时获取高质量多样训练数据是挑战，现有工作对请求语言多样性和参数覆盖度探索不足。

Method: 通过优化查询和参数的通用多样性指标生成合成数据集，不依赖手工规则或分类法。

Result: 在多样性上优于基线方法，正确性相当；基于该数据集训练的模型在分布外性能上表现更优，在BFCL基准上准确率提升7.4%。

Conclusion: 提出的方法有效，能生成高质量多样的合成数据集用于函数调用代理训练。

Abstract: The construction of function calling agents has emerged as a promising avenue for extending model capabilities. A major challenge for this task is obtaining high quality diverse data for training. Prior work emphasizes diversity in functions, invocation patterns, and interaction turns, yet linguistic diversity of requests and coverage of arguments (e.g., \texttt{city\_name}, \texttt{stock\_ticker}) remain underexplored. We propose a method that generates synthetic datasets via optimizing general-purpose diversity metrics across both queries and arguments, without relying on hand-crafted rules or taxonomies, making it robust to different usecases. We demonstrate the effectiveness of our technique via both intrinsic and extrinsic testing, comparing it to SoTA data generation methods. We show a superiority over baselines in terms of diversity, while keeping comparable correctness. Additionally, when used as a training set, the model resulting from our dataset exhibits superior performance compared to analogous models based on the baseline data generation methods in out-of-distribution performance. In particular, we achieve an $7.4\%$ increase in accuracy on the BFCL benchmark compared to similar counterparts.

</details>


### [426] [A Monosemantic Attribution Framework for Stable Interpretability in Clinical Neuroscience Large Language Models](https://arxiv.org/abs/2601.17952)
*Michail Mamalakis,Tiago Azevedo,Cristian Cosentino,Chiara D'Ercoli,Subati Abulikemu,Zhongtian Sun,Richard Bethlehem,Pietro Lio*

Main category: cs.CL

TL;DR: 提出一种统一解释框架，通过单语义特征提取整合归因和机制视角，提升大语言模型在临床应用的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有归因方法在大语言模型临床应用中存在高变异性和不稳定解释问题，机制可解释性方法缺乏与模型输入输出的直接对齐且无明确重要性得分，需要解决大语言模型在临床应用的可解释性挑战。

Method: 引入统一的可解释性框架，通过单语义特征提取整合归因和机制视角，在大语言模型层构建单语义嵌入空间并优化以减少方法间变异性。

Result: 产生稳定的输入级重要性分数，并通过感兴趣层的解压缩表示突出显著特征。

Conclusion: 该方法推进了大语言模型在认知健康和神经退行性疾病中的安全和可信应用。

Abstract: Interpretability remains a key challenge for deploying large language models (LLMs) in clinical settings such as Alzheimer's disease progression diagnosis, where early and trustworthy predictions are essential. Existing attribution methods exhibit high inter-method variability and unstable explanations due to the polysemantic nature of LLM representations, while mechanistic interpretability approaches lack direct alignment with model inputs and outputs and do not provide explicit importance scores. We introduce a unified interpretability framework that integrates attributional and mechanistic perspectives through monosemantic feature extraction. By constructing a monosemantic embedding space at the level of an LLM layer and optimizing the framework to explicitly reduce inter-method variability, our approach produces stable input-level importance scores and highlights salient features via a decompressed representation of the layer of interest, advancing the safe and trustworthy application of LLMs in cognitive health and neurodegenerative disease.

</details>


### [427] [SD-E$^2$: Semantic Exploration for Reasoning Under Token Budgets](https://arxiv.org/abs/2601.17982)
*Kshitij Mishra,Nils Lukas,Salem Lahlou*

Main category: cs.CL

TL;DR: 提出SD - E²框架优化小语言模型推理能力，在多个数据集上取得提升，表明奖励语义新颖性可提高训练效率。


<details>
  <summary>Details</summary>
Motivation: 小语言模型在复杂推理上因计算预算受限，探索成本高，需有效探索方法。

Method: 引入SD - E²强化学习框架，用冻结句子嵌入模型分配多样性奖励，结合多目标优化稳定训练。

Result: 在GSM8K、MedMCQA、AIME等数据集上超越基线模型，发现更多语义不同策略。

Conclusion: 奖励语义新颖性为小语言模型训练提供高效探索信号，通过认知适应可提升资源受限模型效率。

Abstract: Small language models (SLMs) struggle with complex reasoning because exploration is expensive under tight compute budgets. We introduce Semantic Diversity-Exploration-Exploitation (SD-E$^2$), a reinforcement learning framework that makes exploration explicit by optimizing semantic diversity in generated reasoning trajectories. Using a frozen sentence-embedding model, SD-E$^2$ assigns a diversity reward that captures (i) the coverage of semantically distinct solution strategies and (ii) their average pairwise dissimilarity in embedding space, rather than surface-form novelty. This diversity reward is combined with outcome correctness and solution efficiency in a z-score-normalized multi-objective objective that stabilizes training. On GSM8K, SD-E$^2$ surpasses the base Qwen2.5-3B-Instruct and strong GRPO baselines (GRPO-CFL and GRPO-CFEE) by +27.4, +5.2, and +1.5 percentage points, respectively, while discovering on average 9.8 semantically distinct strategies per question. We further improve MedMCQA to 49.64% versus 38.37% for the base model and show gains on the harder AIME benchmark (1983-2025), reaching 13.28% versus 6.74% for the base. These results indicate that rewarding semantic novelty yields a more compute-efficient exploration-exploitation signal for training reasoning-capable SLMs. By introducing cognitive adaptation-adjusting the reasoning process structure rather than per-token computation-SD-E$^2$ offers a complementary path to efficiency gains in resource-constrained models.

</details>


### [428] [AI-based approach to burnout identification from textual data](https://arxiv.org/abs/2601.17993)
*Marina Zavertiaeva,Petr Parshakov,Mikhail Usanin,Aleksei Smirnov,Sofia Paklina,Anastasiia Kibardina*

Main category: cs.CL

TL;DR: 研究引入基于AI的NLP方法，用RuBERT模型检测文本数据中的倦怠情况。


<details>
  <summary>Details</summary>
Motivation: 在高压力工作环境中监测与倦怠相关的语言信号。

Method: 使用原本用于情感分析的RuBERT模型，用ChatGPT生成的合成句子和俄罗斯YouTube关于倦怠的用户评论两种数据源进行微调。

Result: 模型可为输入文本分配倦怠概率，能处理大量书面交流。

Conclusion: 该模型可用于监测高压力工作环境中与倦怠相关的语言信号。

Abstract: This study introduces an AI-based methodology that utilizes natural language processing (NLP) to detect burnout from textual data. The approach relies on a RuBERT model originally trained for sentiment analysis and subsequently fine-tuned for burnout detection using two data sources: synthetic sentences generated with ChatGPT and user comments collected from Russian YouTube videos about burnout. The resulting model assigns a burnout probability to input texts and can be applied to process large volumes of written communication for monitoring burnout-related language signals in high-stress work environments.

</details>


### [429] [Evaluating Semantic and Syntactic Understanding in Large Language Models for Payroll Systems](https://arxiv.org/abs/2601.18012)
*Hendrika Maclean,Mert Can Cakmak,Muzakkiruddin Ahmed Mohammed,Shames Al Mandalawi,John Talburt*

Main category: cs.CL

TL;DR: 研究大语言模型在合成工资系统上的数值计算准确性，实验覆盖不同案例、提示和模型，给出部署框架和指导。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在精确数值计算和可审计输出方面不可靠，以合成工资系统为例进行研究。

Method: 在从基础到复杂案例的分层数据集上，使用从最小基线到模式引导及推理变体的多种提示，对GPT、Claude等多个模型家族进行实验。

Result: 明确了仔细提示足够的情况和需要显式计算的情况。

Conclusion: 提供了一个紧凑、可复现的框架和在需要准确性和可靠性的场景中部署大语言模型的实用指南。

Abstract: Large language models are now used daily for writing, search, and analysis, and their natural language understanding continues to improve. However, they remain unreliable on exact numerical calculation and on producing outputs that are straightforward to audit. We study synthetic payroll system as a focused, high-stakes example and evaluate whether models can understand a payroll schema, apply rules in the right order, and deliver cent-accurate results. Our experiments span a tiered dataset from basic to complex cases, a spectrum of prompts from minimal baselines to schema-guided and reasoning variants, and multiple model families including GPT, Claude, Perplexity, Grok and Gemini. Results indicate clear regimes where careful prompting is sufficient and regimes where explicit computation is required. The work offers a compact, reproducible framework and practical guidance for deploying LLMs in settings that demand both accuracy and assurance.

</details>


### [430] [A System for Name and Address Parsing with Large Language Models](https://arxiv.org/abs/2601.18014)
*Adeeba Tarannum,Muzakkiruddin Ahmed Mohammed,Mert Can Cakmak,Shames Al Mandalawi,John Talburt*

Main category: cs.CL

TL;DR: 本文提出无微调的提示驱动、验证中心框架转换非结构化文本为结构化数据，评估显示效果好。


<details>
  <summary>Details</summary>
Motivation: 传统方法在嘈杂或多语言环境表现不佳，神经和大语言模型缺乏确定性控制和可重复性。

Method: 引入提示驱动、验证中心框架，集成输入标准化、结构化提示、受限解码和严格基于规则的验证。

Result: 在异构真实世界地址数据评估中，有高字段级准确性、强模式依从性和稳定的置信度校准。

Conclusion: 确定性验证与生成式提示结合为结构化信息提取提供强大、可解释和可扩展的解决方案。

Abstract: Reliable transformation of unstructured person and address text into structured data remains a key challenge in large-scale information systems. Traditional rule-based and probabilistic approaches perform well on clean inputs but fail under noisy or multilingual conditions, while neural and large language models (LLMs) often lack deterministic control and reproducibility. This paper introduces a prompt-driven, validation-centered framework that converts free-text records into a consistent 17-field schema without fine-tuning. The method integrates input normalisation, structured prompting, constrained decoding, and strict rule-based validation under fixed experimental settings to ensure reproducibility. Evaluations on heterogeneous real-world address data show high field-level accuracy, strong schema adherence, and stable confidence calibration. The results demonstrate that combining deterministic validation with generative prompting provides a robust, interpretable, and scalable solution for structured information extraction, offering a practical alternative to training-heavy or domain-specific models.

</details>


### [431] [S$^3$-Attention:Attention-Aligned Endogenous Retrieval for Memory-Bounded Long-Context Inference](https://arxiv.org/abs/2601.17702)
*Qingsen Ma,Dianyun Wang,Yaoye Wang,Lechen Ning,Sujie Zhu,Xiaohang Zhang,Jiaming Lyu,Linhao Ren,Zhenbo Xu,Zhaofeng He*

Main category: cs.CL

TL;DR: 提出S3 - Attention框架处理长上下文推理，可丢弃KV缓存并限制GPU内存使用，在多模型上接近全上下文推理效果，但存在时钟延迟问题。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文推理存在内存和噪声效率低的问题，KV缓存与上下文长度线性相关，外部检索方法常返回无关段落。

Method: 将长上下文处理视为注意力对齐的内生检索，用轻量级稀疏自动编码器解码关键和查询投影，构建基于CPU的倒排索引。生成时用特征共激活检索证据片段，可与BM25融合。

Result: 在统一评估协议下，S3 - Hybrid在多模型家族中接近全上下文推理效果，在信息密集场景中提高了鲁棒性。

Conclusion: S3 - Attention是有效的长上下文推理框架，但当前原型存在时钟延迟问题，需进行内核级优化。

Abstract: Large language models are increasingly applied to multi-document and long-form inputs, yet long-context inference remains memory- and noise-inefficient. Key-value (KV) caching scales linearly with context length, while external retrieval methods often return lexically similar but causally irrelevant passages.
  We present S3-Attention, a memory-first inference-time framework that treats long-context processing as attention-aligned endogenous retrieval. S3-Attention decodes transient key and query projections into top-k sparse feature identifiers using lightweight sparse autoencoders, and constructs a CPU-based inverted index mapping features to token positions or spans during a single streaming scan. This design allows the KV cache to be discarded entirely and bounds GPU memory usage by the scan chunk size.
  At generation time, feature co-activation is used to retrieve compact evidence spans, optionally fused with BM25 for exact lexical matching. Under a unified LongBench evaluation protocol with fixed prompting, decoding, and matched token budgets, S3-Hybrid closely matches full-context inference across multiple model families and improves robustness in several information-dense settings. We also report an engineering limitation of the current prototype, which incurs higher wall-clock latency than optimized full-KV baselines, motivating future kernel-level optimization.

</details>


### [432] [Addressing LLM Diversity by Infusing Random Concepts](https://arxiv.org/abs/2601.18053)
*Pulin Agrawal,Prasoon Goyal*

Main category: cs.CL

TL;DR: 研究通过在提示中注入随机概念提升大语言模型输出多样性，设计评估协议，实验表明添加不相关随机字词能提升输出多样性，有望引导未来研究。


<details>
  <summary>Details</summary>
Motivation: 大语言模型输出多样性有限，研究在提示中注入随机概念能否提高输出多样性。

Method: 设计系统评估协议，用特定问题提示大语言模型，并分析其输出的多样性指标。

Result: 在多个大语言模型上实验发现，在提示前添加不相关的随机字词能使输出更具多样性。

Conclusion: 该结果和评估协议为未来研究开辟新途径，评估协议或启发更系统的大语言模型多样性基准研究。

Abstract: Large language models (LLMs) are known to produce outputs with limited diversity. In this work, we study whether infusing random concepts in the prompts can improve the diversity of the generated outputs. To benchmark the approach, we design a systematic evaluation protocol which involves prompting an LLM with questions of the form "Name 10 Hollywood actors", and analyzing diversity measures of the resulting LLM outputs. Our experiments on multiple LLMs show that prepending random words/sentences unrelated to the prompt result in greater diversity in the outputs of LLMs. We believe that this promising result and the evaluation protocol opens up interesting avenues for future work, such as how infusing randomness into LLMs could be applied to other domains. Further, the evaluation protocol could also inspire research into benchmarking LLM diversity more systematically.

</details>


### [433] [Beyond a Single Perspective: Text Anomaly Detection with Multi-View Language Representations](https://arxiv.org/abs/2601.17786)
*Yixin Liu,Kehan Yan,Shiyuan Li,Qingfeng Chen,Shirui Pan*

Main category: cs.CL

TL;DR: 针对现有文本异常检测方法局限，提出多视图TAD框架MCA²，实验验证其有效性并公开代码。


<details>
  <summary>Details</summary>
Motivation: 现有两步“嵌入 - 检测器”TAD方法受限于单一嵌入模型，且在不同数据集和异常类型上适应性不足。

Method: 利用多个预训练语言模型的嵌入，构建多视图TAD框架MCA²，采用多视图重建模型，设计对比协作模块和自适应分配模块。

Result: 在10个基准数据集上的大量实验表明，MCA²相对于强基线方法有效。

Conclusion: 提出的MCA²框架能有效解决现有TAD方法的局限，提高文本异常检测性能。

Abstract: Text anomaly detection (TAD) plays a critical role in various language-driven real-world applications, including harmful content moderation, phishing detection, and spam review filtering. While two-step "embedding-detector" TAD methods have shown state-of-the-art performance, their effectiveness is often limited by the use of a single embedding model and the lack of adaptability across diverse datasets and anomaly types. To address these limitations, we propose to exploit the embeddings from multiple pretrained language models and integrate them into $MCA^2$, a multi-view TAD framework. $MCA^2$ adopts a multi-view reconstruction model to effectively extract normal textual patterns from multiple embedding perspectives. To exploit inter-view complementarity, a contrastive collaboration module is designed to leverage and strengthen the interactions across different views. Moreover, an adaptive allocation module is developed to automatically assign the contribution weight of each view, thereby improving the adaptability to diverse datasets. Extensive experiments on 10 benchmark datasets verify the effectiveness of $MCA^2$ against strong baselines. The source code of $MCA^2$ is available at https://github.com/yankehan/MCA2.

</details>


### [434] [On the Emergence and Test-Time Use of Structural Information in Large Language Models](https://arxiv.org/abs/2601.17869)
*Michelle Chao Chen,Moritz Miller,Bernhard Schölkopf,Siyuan Guo*

Main category: cs.CL

TL;DR: 研究语言模型学习抽象结构及测试时利用结构信息，设计数据集实验，发现学习结构信息与复杂推理任务相关，测试时组合生成能力有限。


<details>
  <summary>Details</summary>
Motivation: 从观测数据中学习结构信息对产生新知识很重要，研究语言模型学习和利用结构信息的情况。

Method: 设计基于语言结构转换的自然语言数据集进行实证研究。

Result: 学习结构信息的出现与复杂推理任务相关，测试时组合生成能力有限。

Conclusion: 语言模型在学习和利用结构信息进行测试时组合生成方面存在一定局限。

Abstract: Learning structural information from observational data is central to producing new knowledge outside the training corpus. This holds for mechanistic understanding in scientific discovery as well as flexible test-time compositional generation. We thus study how language models learn abstract structures and utilize the learnt structural information at test-time. To ensure a controlled setup, we design a natural language dataset based on linguistic structural transformations. We empirically show that the emergence of learning structural information correlates with complex reasoning tasks, and that the ability to perform test-time compositional generation remains limited.

</details>


### [435] [Typhoon-S: Minimal Open Post-Training for Sovereign Large Language Models](https://arxiv.org/abs/2601.18129)
*Kunat Pipatanakul,Pittawat Taveekitworachai*

Main category: cs.CL

TL;DR: 论文提出Typhoon S微调方法，用泰语实验表明能在有限资源下生成高质量主权大语言模型。


<details>
  <summary>Details</summary>
Motivation: 当前大模型多在英语、中文等高资源语言训练，而主权场景需要主体在有限资源和透明约束下控制模型，因此需要探索满足可采用性和主权能力的方法。

Method: 提出Typhoon S微调方法，结合监督微调、在线策略蒸馏和小规模RFT；用InK - GRPO进行小规模RFT。

Result: 该方法能将适配主权和通用基础模型转化为指令调整模型，有良好通用性能，还能提升泰语法律推理和特定知识。

Conclusion: 精心设计的微调策略可减少指令数据和计算规模，为学术资源下生成高质量主权大模型提供可行路径。

Abstract: Large language models (LLMs) have progressed rapidly; however, most state-of-the-art models are trained and evaluated primarily in high-resource languages such as English and Chinese, and are often developed by a small number of organizations with access to large-scale compute and data. This gatekeeping creates a practical barrier for sovereign settings in which a regional- or national-scale institution or domain owner must retain control and understanding of model weights, training data, and deployment while operating under limited resources and strict transparency constraints. To this end, we identify two core requirements: (1) adoptability, the ability to transform a base model into a general-purpose assistant, and (2) sovereign capability, the ability to perform high-stakes, region-specific tasks (e.g., legal reasoning in local languages and cultural knowledge). We investigate whether these requirements can be achieved without scaling massive instruction corpora or relying on complex preference tuning pipelines and large-scale reinforcement fine-tuning (RFT). We present Typhoon S, a minimal and open post-training recipe that combines supervised fine-tuning, on-policy distillation, and small-scale RFT. Using Thai as a representative case study, we demonstrate that our approach transforms both sovereign-adapted and general-purpose base models into instruction-tuned models with strong general performance. We further show that small-scale RFT with InK-GRPO -- an extension of GRPO that augments the GRPO loss with a next-word prediction loss -- improves Thai legal reasoning and Thai-specific knowledge while preserving general capabilities. Our results suggest that a carefully designed post-training strategy can reduce the required scale of instruction data and computation, providing a practical path toward high-quality sovereign LLMs under academic-scale resources.

</details>


### [436] [BoRP: Bootstrapped Regression Probing for Scalable and Human-Aligned LLM Evaluation](https://arxiv.org/abs/2601.18253)
*Peng Sun,Xiangyu Zhang,Duan Wu*

Main category: cs.CL

TL;DR: 提出BoRP框架用于对话式AI用户满意度评估，在工业数据集上表现优于生成式基线，还能降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 传统A/B测试在开放式对话助手的用户满意度评估中缺乏可靠指标，显式反馈少，隐式指标模糊。

Method: 引入BoRP框架，利用大语言模型潜在空间的几何特性，采用基于极化指数的自举机制自动生成评分标准，用偏最小二乘法将隐藏状态映射为连续分数。

Result: 在工业数据集上，BoRP（Qwen3 - 8B/14B）在与人工判断的一致性上显著优于生成式基线（甚至Qwen3 - Max），并大幅降低推理成本。

Conclusion: BoRP框架可用于对话式AI用户满意度的高保真评估，支持全面监测和高灵敏度的A/B测试。

Abstract: Accurate evaluation of user satisfaction is critical for iterative development of conversational AI. However, for open-ended assistants, traditional A/B testing lacks reliable metrics: explicit feedback is sparse, while implicit metrics are ambiguous. To bridge this gap, we introduce BoRP (Bootstrapped Regression Probing), a scalable framework for high-fidelity satisfaction evaluation. Unlike generative approaches, BoRP leverages the geometric properties of LLM latent space. It employs a polarization-index-based bootstrapping mechanism to automate rubric generation and utilizes Partial Least Squares (PLS) to map hidden states to continuous scores. Experiments on industrial datasets show that BoRP (Qwen3-8B/14B) significantly outperforms generative baselines (even Qwen3-Max) in alignment with human judgments. Furthermore, BoRP reduces inference costs by orders of magnitude, enabling full-scale monitoring and highly sensitive A/B testing via CUPED.

</details>


### [437] [TechING: Towards Real World Technical Image Understanding via VLMs](https://arxiv.org/abs/2601.18238)
*Tafazzul Nadeem,Bhavik Shangari,Manish Rai,Gagan Raj Gupta,Ashutosh Modi*

Main category: cs.CL

TL;DR: 本文引入合成语料训练VLM，提出新自监督任务，微调模型获LLama - VL - TUG，提升性能并在真实图像上表现良好。


<details>
  <summary>Details</summary>
Motivation: 专业人员手绘技术图编辑不便，现代VLM理解技术图有困难，且难以获取大量真实手绘图像用于微调。

Method: 引入合成语料训练VLM，提出新自监督任务，用不同基线模型实验，在合成图像上微调Llama 3.2 11B - instruct模型。

Result: LLama - VL - TUG使Llama 3.2 11B - instruct的ROUGE - L性能提升2.14倍，在所有基线模型中表现最佳；真实图像上，7种图类型编译错误最少，F1分数提升6.97倍。

Conclusion: 合成语料和新自监督任务有效提升了VLM对技术图的理解能力和性能。

Abstract: Professionals working in technical domain typically hand-draw (on whiteboard, paper, etc.) technical diagrams (e.g., flowcharts, block diagrams, etc.) during discussions; however, if they want to edit these later, it needs to be drawn from scratch. Modern day VLMs have made tremendous progress in image understanding but they struggle when it comes to understanding technical diagrams. One way to overcome this problem is to fine-tune on real world hand-drawn images, but it is not practically possible to generate large number of such images. In this paper, we introduce a large synthetically generated corpus (reflective of real world images) for training VLMs and subsequently evaluate VLMs on a smaller corpus of hand-drawn images (with the help of humans). We introduce several new self-supervision tasks for training and perform extensive experiments with various baseline models and fine-tune Llama 3.2 11B-instruct model on synthetic images on these tasks to obtain LLama-VL-TUG, which significantly improves the ROUGE-L performance of Llama 3.2 11B-instruct by 2.14x and achieves the best all-round performance across all baseline models. On real-world images, human evaluation reveals that we achieve minimum compilation errors across all baselines in 7 out of 8 diagram types and improve the average F1 score of Llama 3.2 11B-instruct by 6.97x.

</details>


### [438] [Temp-R1: A Unified Autonomous Agent for Complex Temporal KGQA via Reverse Curriculum Reinforcement Learning](https://arxiv.org/abs/2601.18296)
*Zhaoyan Gong,Zhiqiang Liu,Songze Li,Xiaoke Guo,Yuanxiang Liu,Xinle Deng,Zhizhen Liu,Lei Liang,Huajun Chen,Wen Zhang*

Main category: cs.CL

TL;DR: 提出首个通过强化学习训练用于TKGQA的端到端智能体Temp - R1，在多数据集上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有TKGQA方法依赖固定工作流和昂贵闭源API，限制灵活性和可扩展性，且需处理复杂推理问题。

Method: 通过强化学习训练Temp - R1；扩展动作空间加入内部动作；引入反向课程学习，先训练难题。

Result: 8B参数的Temp - R1在MultiTQ和TimelineKGQA上达SOTA，在复杂问题上比强基线提升19.8%。

Conclusion: 为自主时间推理智能体建立新范式。

Abstract: Temporal Knowledge Graph Question Answering (TKGQA) is inherently challenging, as it requires sophisticated reasoning over dynamic facts with multi-hop dependencies and complex temporal constraints. Existing methods rely on fixed workflows and expensive closed-source APIs, limiting flexibility and scalability. We propose Temp-R1, the first autonomous end-to-end agent for TKGQA trained through reinforcement learning. To address cognitive overload in single-action reasoning, we expand the action space with specialized internal actions alongside external action. To prevent shortcut learning on simple questions, we introduce reverse curriculum learning that trains on difficult questions first, forcing the development of sophisticated reasoning before transferring to easier cases. Our 8B-parameter Temp-R1 achieves state-of-the-art performance on MultiTQ and TimelineKGQA, improving 19.8% over strong baselines on complex questions. Our work establishes a new paradigm for autonomous temporal reasoning agents. Our code will be publicly available soon at https://github.com/zjukg/Temp-R1.

</details>


### [439] [Calibrating Beyond English: Language Diversity for Better Quantized Multilingual LLM](https://arxiv.org/abs/2601.18306)
*Everlyn Asiko Chimoto,Mostafa Elhoushi,Bruce A. Bassett*

Main category: cs.CL

TL;DR: 研究八种校准设置对多语言大语言模型量化的影响，发现非英语和多语言校准集能显著降低困惑度，强调校准数据定制的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有训练后量化方法多使用英语校准集，对多语言模型的影响研究不足。

Method: 在两种量化器（GPTQ、AWQ）上，对10种语言数据的八种校准设置（五种单语言和三种多语言混合）进行系统评估。

Result: 非英语和多语言校准集比仅英语基线显著降低困惑度，多语言混合降幅最大达3.52点；针对评估语言定制校准集效果最佳；发现部分语言 - 量化器组合会降低性能。

Conclusion: 静态通用校准不佳，定制校准数据对多语言大语言模型稳健量化至关重要。

Abstract: Quantization is an effective technique for reducing the storage footprint and computational costs of Large Language Models (LLMs), but it often results in performance degradation. Existing post-training quantization methods typically use small, English-only calibration sets; however, their impact on multilingual models remains underexplored. We systematically evaluate eight calibration settings (five single-language and three multilingual mixes) on two quantizers (GPTQ, AWQ) on data from 10 languages. Our findings reveal a consistent trend: non-English and multilingual calibration sets significantly improve perplexity compared to English-only baselines. Specifically, we observe notable average perplexity gains across both quantizers on Llama3.1 8B and Qwen2.5 7B, with multilingual mixes achieving the largest overall reductions of up to 3.52 points in perplexity. Furthermore, our analysis indicates that tailoring calibration sets to the evaluation language yields the largest improvements for individual languages, underscoring the importance of linguistic alignment. We also identify specific failure cases where certain language-quantizer combinations degrade performance, which we trace to differences in activation range distributions across languages. These results highlight that static one-size-fits-all calibration is suboptimal and that tailoring calibration data, both in language and diversity, plays a crucial role in robustly quantizing multilingual LLMs.

</details>


### [440] [When Domain Pretraining Interferes with Instruction Alignment: An Empirical Study of Adapter Merging in Medical LLMs](https://arxiv.org/abs/2601.18350)
*Junyi Zou*

Main category: cs.CL

TL;DR: 本文针对大语言模型在医学领域的问题，通过两阶段LoRA管道进行案例研究，提出加权适配器合并方法，在验证集上取得一定指标结果并进行相关分析。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在医学术语精度和遵循安全关键指令方面存在困难，需要提升其在医学领域的性能。

Method: 采用两阶段LoRA管道，包括领域自适应预训练注入医学知识和监督微调使模型与医学问答行为对齐，提出加权适配器合并方法。

Result: 在医学验证集上，合并模型在实际解码配置下取得BLEU - 4 = 16.38、ROUGE - 1 = 20.42、ROUGE - 2 = 4.60和ROUGE - L = 11.54的结果。

Conclusion: 通过所提出的方法可提升模型在医学领域的性能，还对解码敏感性和训练稳定性进行了分析。

Abstract: Large language models (LLMs) show strong general capability but often struggle with medical terminology precision and safety-critical instruction following. We present a case study for adapter interference in safety-critical domains using a 14B-parameter base model through a two-stage LoRA pipeline: (1) domain-adaptive pre-training (PT) to inject broad medical knowledge via continued pre-training (DAPT), and (2) supervised fine-tuning (SFT) to align the model with medical question-answering behaviors through instruction-style data. To balance instruction-following ability and domain knowledge retention, we propose Weighted Adapter Merging, linearly combining SFT and PT adapters before exporting a merged base-model checkpoint. On a held-out medical validation set (F5/F6), the merged model achieves BLEU-4 = 16.38, ROUGE-1 = 20.42, ROUGE-2 = 4.60, and ROUGE-L = 11.54 under a practical decoding configuration. We further analyze decoding sensitivity and training stability with loss curves and controlled decoding comparisons.

</details>


### [441] [Code over Words: Overcoming Semantic Inertia via Code-Grounded Reasoning](https://arxiv.org/abs/2601.18352)
*Manjie Xu,Isabella Yin,Xinyi Tu,Chi Zhang,Yixin Zhu*

Main category: cs.CL

TL;DR: 研究大语言模型语义惯性问题，发现大模型在抑制先验知识时有反缩放现象，用代码表示动态能逆转趋势，提出 LCV 方法，证明表征影响推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型存在的语义惯性问题，即无法抑制预训练先验知识的问题。

Method: 使用 Baba Is You 游戏进行探测，提出 Code - Grounded Vistas (LCV) 方法，在反事实对上微调模型。

Result: 大模型在自然语言推理需抑制预训练关联时表现不如小模型，LCV 方法在效率和准确性上优于推理时搜索方法。

Conclusion: 表征方式从根本上决定了模型缩放是提升还是损害上下文推理能力，挑战了大模型普遍更优的假设。

Abstract: LLMs struggle with Semantic Inertia: the inability to inhibit pre-trained priors (e.g., "Lava is Dangerous") when dynamic, in-context rules contradict them. We probe this phenomenon using Baba Is You, where physical laws are mutable text rules, enabling precise evaluation of models' ability to override learned priors when rules change. We quantatively observe that larger models can exhibit inverse scaling: they perform worse than smaller models when natural language reasoning requires suppressing pre-trained associations (e.g., accepting "Lava is Safe"). Our analysis attributes this to natural language encoding, which entangles descriptive semantics and logical rules, leading to persistent hallucinations of familiar physics despite explicit contradictory rules. Here we show that representing dynamics as executable code, rather than descriptive text, reverses this trend and enables effective prior inhibition. We introduce Code-Grounded Vistas (LCV), which fine-tunes models on counterfactual pairs and identifies states with contradictory rules, thereby forcing attention to logical constraints rather than visual semantics. This training-time approach outperforms expensive inference-time search methods in both efficiency and accuracy. Our results demonstrate that representation fundamentally determines whether scaling improves or impairs contextual reasoning. This challenges the assumption that larger models are universally better, with implications for domains that require dynamic overriding of learned priors.

</details>


### [442] [Funny or Persuasive, but Not Both: Evaluating Fine-Grained Multi-Concept Control in LLMs](https://arxiv.org/abs/2601.18483)
*Arya Labroo,Ivaxi Sheth,Vyas Raina,Amaani Ahmed,Mario Fritz*

Main category: cs.CL

TL;DR: 提出细粒度可控性评估框架，发现多概念控制中模型性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法对多属性文本概念的系统评估有限，需对大语言模型对特定文本概念进行细粒度控制。

Method: 引入针对单概念和双概念场景的细粒度可控性评估框架，聚焦语言上不同的概念对。

Result: 在多个大语言模型和生成任务中，双概念设置下性能常下降。

Conclusion: 揭示了基于简单提示控制的根本局限，框架为衡量未来多概念控制方法提供途径。

Abstract: Large Language Models (LLMs) offer strong generative capabilities, but many applications require explicit and \textit{fine-grained} control over specific textual concepts, such as humor, persuasiveness, or formality. Prior approaches in prompting and representation engineering can provide coarse or single-attribute control, but systematic evaluation of multi-attribute settings remains limited. We introduce an evaluation framework for fine-grained controllability for both single- and dual-concept scenarios, focusing on linguistically distinct concept pairs (e.g., persuasiveness vs.~humor). Surprisingly, across multiple LLMs and generative tasks, we find that performance often drops in the dual-concept setting, even though the chosen concepts should in principle be separable. This reveals a fundamental limitation of naive prompting-based control: models struggle with compositionality even when concepts are intuitively independent. Our framework provides systematic evidence of this gap and offers a principled approach for measuring the ability of future methods for multi-concept control.

</details>


### [443] [Unknown Unknowns: Why Hidden Intentions in LLMs Evade Detection](https://arxiv.org/abs/2601.18552)
*Devansh Srivastav,David Pape,Lea Schönherr*

Main category: cs.CL

TL;DR: 文章分析大语言模型隐藏意图的可检测性，提出分类法，发现现实中检测失效，强调需稳健框架。


<details>
  <summary>Details</summary>
Motivation: 大语言模型输出存在难以检测的隐藏意图，影响用户决策，需对其进行研究。

Method: 引入隐藏意图分类法，在受控模型中诱导隐藏意图，系统评估检测方法，进行压力测试和定性案例研究。

Result: 在现实开放环境中检测失效，低流行率下误报影响精度，漏报掩盖风险，顶尖模型存在所有类别隐藏意图。

Conclusion: 为理解、诱导和测试隐藏意图行为提供基础，建立分类法，强调建立稳健框架的紧迫性。

Abstract: LLMs are increasingly embedded in everyday decision-making, yet their outputs can encode subtle, unintended behaviours that shape user beliefs and actions. We refer to these covert, goal-directed behaviours as hidden intentions, which may arise from training and optimisation artefacts, or be deliberately induced by an adversarial developer, yet remain difficult to detect in practice. We introduce a taxonomy of ten categories of hidden intentions, grounded in social science research and organised by intent, mechanism, context, and impact, shifting attention from surface-level behaviours to design-level strategies of influence. We show how hidden intentions can be easily induced in controlled models, providing both testbeds for evaluation and demonstrations of potential misuse. We systematically assess detection methods, including reasoning and non-reasoning LLM judges, and find that detection collapses in realistic open-world settings, particularly under low-prevalence conditions, where false positives overwhelm precision and false negatives conceal true risks. Stress tests on precision-prevalence and precision-FNR trade-offs reveal why auditing fails without vanishingly small false positive rates or strong priors on manipulation types. Finally, a qualitative case study shows that all ten categories manifest in deployed, state-of-the-art LLMs, emphasising the urgent need for robust frameworks. Our work provides the first systematic analysis of detectability failures of hidden intentions in LLMs under open-world settings, offering a foundation for understanding, inducing, and stress-testing such behaviours, and establishing a flexible taxonomy for anticipating evolving threats and informing governance.

</details>


### [444] [HalluCitation Matters: Revealing the Impact of Hallucinated References with 300 Hallucinated Papers in ACL Conferences](https://arxiv.org/abs/2601.18724)
*Yusuke Sakai,Hidetaka Kamigaito,Taro Watanabe*

Main category: cs.CL

TL;DR: 研究分析2024 - 2025年ACL、NAACL和EMNLP会议论文，发现近300篇含幻觉引用，问题呈快速增长趋势，影响会议可信度。


<details>
  <summary>Details</summary>
Motivation: 幻觉引用对科学可靠性和会议可信度有负面影响，需系统研究其普遍性和影响。

Method: 分析2024和2025年ACL、NAACL和EMNLP会议的所有论文，包括主会、研究成果和研讨会论文。

Result: 近300篇论文含至少一个幻觉引用，多数发表于2025年，一半论文来自EMNLP 2025，超100篇被EMNLP 2025主会和研究成果录用。

Conclusion: 幻觉引用问题快速增加，影响会议可信度。

Abstract: Recently, we have often observed hallucinated citations or references that do not correspond to any existing work in papers under review, preprints, or published papers. Such hallucinated citations pose a serious concern to scientific reliability. When they appear in accepted papers, they may also negatively affect the credibility of conferences. In this study, we refer to hallucinated citations as "HalluCitation" and systematically investigate their prevalence and impact. We analyze all papers published at ACL, NAACL, and EMNLP in 2024 and 2025, including main conference, Findings, and workshop papers. Our analysis reveals that nearly 300 papers contain at least one HalluCitation, most of which were published in 2025. Notably, half of these papers were identified at EMNLP 2025, the most recent conference, indicating that this issue is rapidly increasing. Moreover, more than 100 such papers were accepted as main conference and Findings papers at EMNLP 2025, affecting the credibility.

</details>


### [445] [One Adapts to Any: Meta Reward Modeling for Personalized LLM Alignment](https://arxiv.org/abs/2601.18731)
*Hongru Cai,Yongqi Li,Tiezheng Yu,Fengbin Zhu,Wenjie Wang,Fuli Feng,Wenjie Li*

Main category: cs.CL

TL;DR: 论文提出Meta Reward Modeling (MRM)解决大语言模型个性化对齐中个性化奖励模型开发的挑战，实验验证其优势。


<details>
  <summary>Details</summary>
Motivation: 大语言模型个性化对齐依赖个性化奖励模型，但开发面临用户反馈稀缺和适应新用户效率低的问题。

Method: 将个性化奖励建模重新表述为元学习问题，用MAML框架优化权重初始化，引入Robust Personalization Objective (RPO)保证鲁棒性。

Result: 在个性化偏好数据集上的大量实验表明，MRM增强了少样本个性化，提高了用户鲁棒性，始终优于基线。

Conclusion: 从学习用户偏好转向学习偏好适应过程，提出的MRM能有效解决个性化奖励模型开发的挑战。

Abstract: Alignment of Large Language Models (LLMs) aims to align outputs with human preferences, and personalized alignment further adapts models to individual users. This relies on personalized reward models that capture user-specific preferences and automatically provide individualized feedback. However, developing these models faces two critical challenges: the scarcity of feedback from individual users and the need for efficient adaptation to unseen users. We argue that addressing these constraints requires a paradigm shift from fitting data to learn user preferences to learn the process of preference adaptation. To realize this, we propose Meta Reward Modeling (MRM), which reformulates personalized reward modeling as a meta-learning problem. Specifically, we represent each user's reward model as a weighted combination of base reward functions, and optimize the initialization of these weights using a Model-Agnostic Meta-Learning (MAML)-style framework to support fast adaptation under limited feedback. To ensure robustness, we introduce the Robust Personalization Objective (RPO), which places greater emphasis on hard-to-learn users during meta optimization. Extensive experiments on personalized preference datasets validate that MRM enhances few-shot personalization, improves user robustness, and consistently outperforms baselines.

</details>


### [446] [Gained in Translation: Privileged Pairwise Judges Enhance Multilingual Reasoning](https://arxiv.org/abs/2601.18722)
*Lintang Sutawika,Gokul Swamy,Zhiwei Steven Wu,Graham Neubig*

Main category: cs.CL

TL;DR: 为解决大语言模型在小语种推理中表现不佳的问题，提出 SP3F 框架，可在无目标语言数据下提升多语言推理能力，且显著提高基础模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前推理大语言模型在处理训练数据中少见语言的问题时，表现远低于英语，需要提升多语言推理能力。

Method: 提出 SP3F 两阶段框架，先在英语问答对翻译版本上进行监督微调提升基础模型正确性，再以自我博弈方式结合成对评判器进行强化学习。

Result: SP3F 显著提升基础模型性能，在单语言、多语言和推广到未见语言等设置下，以较少训练数据在多个数学和非数学任务上超越全后训练模型。

Conclusion: SP3F 框架能有效提升多语言推理能力，在少数据情况下表现出色。

Abstract: When asked a question in a language less seen in its training data, current reasoning large language models (RLMs) often exhibit dramatically lower performance than when asked the same question in English. In response, we introduce \texttt{SP3F} (Self-Play with Privileged Pairwise Feedback), a two-stage framework for enhancing multilingual reasoning without \textit{any} data in the target language(s). First, we supervise fine-tune (SFT) on translated versions of English question-answer pairs to raise base model correctness. Second, we perform RL with feedback from a pairwise judge in a self-play fashion, with the judge receiving the English reference response as \textit{privileged information}. Thus, even when none of the model's responses are completely correct, the privileged pairwise judge can still tell which response is better. End-to-end, \texttt{SP3F} greatly improves base model performance, even outperforming fully post-trained models on multiple math and non-math tasks with less than
  of the training data across the single-language, multilingual, and generalization to unseen language settings.

</details>


### [447] [Reflect: Transparent Principle-Guided Reasoning for Constitutional Alignment at Scale](https://arxiv.org/abs/2601.18730)
*Henry Bell,Caroline Zhang,Mohammed Mobasserul Haque,Dhaval Potdar,Samia Zaman,Brandon Fain*

Main category: cs.CL

TL;DR: 提出REFLECT推理时间框架用于宪法对齐，无需训练和数据，能提升大语言模型对原则的遵循度，还可生成训练数据。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型对齐方法计算要求高、需精心调整且依赖难以获取的人类标注数据。

Method: 提出REFLECT框架，在推理时结合宪法条件基础响应、自我评估、自我批判和最终修订。

Result: REFLECT显著提升大语言模型对不同复杂原则的遵循度，不牺牲事实推理，尤其能减少原则的罕见但重大违规率。

Conclusion: REFLECT是一种即插即用的对齐方法，还能为传统参数微调技术生成有用训练数据，利于长期部署。

Abstract: The constitutional framework of alignment aims to align large language models (LLMs) with value-laden principles written in natural language (such as to avoid using biased language). Prior work has focused on parameter fine-tuning techniques, such as reinforcement learning from human feedback (RLHF), to instill these principles. However, these approaches are computationally demanding, require careful engineering and tuning, and often require difficult-to-obtain human annotation data. We propose \textsc{reflect}, an inference-time framework for constitutional alignment that does not require any training or data, providing a plug-and-play approach for aligning an instruction-tuned model to a set of principles. \textsc{reflect} operates entirely in-context, combining a (i) constitution-conditioned base response with post-generation (ii) self-evaluation, (iii)(a) self-critique, and (iii)(b) final revision. \textsc{reflect}'s technique of explicit in-context reasoning over principles during post-generation outperforms standard few-shot prompting and provides transparent reasoning traces. Our results demonstrate that \textsc{reflect} significantly improves LLM conformance to diverse and complex principles, including principles quite distinct from those emphasized in the model's original parameter fine-tuning, without sacrificing factual reasoning. \textsc{reflect} is particularly effective at reducing the rate of rare but significant violations of principles, thereby improving safety and robustness in the tail end of the distribution of generations. Finally, we show that \textsc{reflect} naturally generates useful training data for traditional parameter fine-tuning techniques, allowing for efficient scaling and the reduction of inference-time computational overhead in long-term deployment scenarios.

</details>


### [448] [Subword-Based Comparative Linguistics across 242 Languages Using Wikipedia Glottosets](https://arxiv.org/abs/2601.18791)
*Iaroslav Chelombitko,Mika Hämäläinen,Aleksey Komissarov*

Main category: cs.CL

TL;DR: 对242种拉丁和西里尔字母语言进行基于子词方法的大规模比较研究，介绍新框架并得出相关语言词汇模式的量化见解。


<details>
  <summary>Details</summary>
Motivation: 对多种语言进行跨语言比较研究，探索词汇重叠、差异和语言相似度等宏观语言模式。

Method: 通过维基百科词典构建‘语符集’，利用基于字节对编码（BPE）和基于排名的子词向量进行分析。

Result: BPE分割在15种语言中比随机基线与词素边界的对齐效果好95%；BPE词汇相似度与语言遗传相关性显著相关；26,939个跨语言同形异义词中48.7%在相关语言中有不同分割。

Conclusion: 在统一分析框架下为类型多样的语言词汇模式提供了量化的宏观语言见解。

Abstract: We present a large-scale comparative study of 242 Latin and Cyrillic-script languages using subword-based methodologies. By constructing 'glottosets' from Wikipedia lexicons, we introduce a framework for simultaneous cross-linguistic comparison via Byte-Pair Encoding (BPE). Our approach utilizes rank-based subword vectors to analyze vocabulary overlap, lexical divergence, and language similarity at scale. Evaluations demonstrate that BPE segmentation aligns with morpheme boundaries 95% better than random baseline across 15 languages (F1 = 0.34 vs 0.15). BPE vocabulary similarity correlates significantly with genetic language relatedness (Mantel r = 0.329, p < 0.001), with Romance languages forming the tightest cluster (mean distance 0.51) and cross-family pairs showing clear separation (0.82). Analysis of 26,939 cross-linguistic homographs reveals that 48.7% receive different segmentations across related languages, with variation correlating to phylogenetic distance. Our results provide quantitative macro-linguistic insights into lexical patterns across typologically diverse languages within a unified analytical framework.

</details>


### [449] [ctELM: Decoding and Manipulating Embeddings of Clinical Trials with Embedding Language Models](https://arxiv.org/abs/2601.18796)
*Brian Ondov,Chia-Hsuan Chang,Yujia Zhou,Mauro Giuffrè,Hua Xu*

Main category: cs.CL

TL;DR: 本文使用ELM方法将大语言模型与临床试验嵌入对齐，开发开源架构和训练框架，训练出ctELM模型，可准确描述和比较临床试验，生成试验摘要，成果有助于生物医学等领域模型与嵌入空间对齐。


<details>
  <summary>Details</summary>
Motivation: 当前解释、探索和反转嵌入空间的方法有限，降低了透明度并限制了生成用例，需要改进大语言模型与嵌入空间的对齐。

Method: 使用ELM方法，开发开源的、领域无关的ELM架构和训练框架，设计临床试验训练任务，引入专家验证的合成数据集，训练一系列ELM模型。

Result: 最终模型ctELM能仅从嵌入中准确描述和比较未见过的临床试验，从新向量生成合理的临床试验，生成的试验摘要对沿概念向量移动嵌入有响应。

Conclusion: 公开的ELM实现和实验结果将有助于生物医学领域及其他领域大语言模型与嵌入空间的对齐。

Abstract: Text embeddings have become an essential part of a variety of language applications. However, methods for interpreting, exploring and reversing embedding spaces are limited, reducing transparency and precluding potentially valuable generative use cases. In this work, we align Large Language Models to embeddings of clinical trials using the recently reported Embedding Language Model (ELM) method. We develop an open-source, domain-agnostic ELM architecture and training framework, design training tasks for clinical trials, and introduce an expert-validated synthetic dataset. We then train a series of ELMs exploring the impact of tasks and training regimes. Our final model, ctELM, can accurately describe and compare unseen clinical trials from embeddings alone and produce plausible clinical trials from novel vectors. We further show that generated trial abstracts are responsive to moving embeddings along concept vectors for age and sex of study subjects. Our public ELM implementation and experimental results will aid the alignment of Large Language Models to embedding spaces in the biomedical domain and beyond.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [450] [Segment Length Matters: A Study of Segment Lengths on Audio Fingerprinting Performance](https://arxiv.org/abs/2601.17690)
*Ziling Gong,Yunyan Ouyang,Iram Kamdar,Melody Ma,Hongjie Chen,Franck Dernoncourt,Ryan A. Rossi,Nesreen K. Ahmed*

Main category: cs.SD

TL;DR: 研究片段长度对音频指纹识别性能的影响，发现短片段（0.5 秒）性能更好，且 GPT - 5 - mini 在推荐最佳片段长度上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现代神经方法中音频片段时长选择常基于经验且缺乏深入研究，需探究片段长度对音频指纹识别性能的影响。

Method: 扩展现有神经指纹识别架构以适应不同片段长度，评估不同片段长度和查询时长下的检索准确率，还评估大语言模型推荐最佳片段长度的能力。

Result: 短片段长度（0.5 秒）通常性能更好，GPT - 5 - mini 在三个研究的大语言模型的五项考量中始终给出最佳建议。

Conclusion: 研究结果为大规模神经音频检索系统选择片段时长提供了实用指导。

Abstract: Audio fingerprinting provides an identifiable representation of acoustic signals, which can be later used for identification and retrieval systems. To obtain a discriminative representation, the input audio is usually segmented into shorter time intervals, allowing local acoustic features to be extracted and analyzed. Modern neural approaches typically operate on short, fixed-duration audio segments, yet the choice of segment duration is often made heuristically and rarely examined in depth. In this paper, we study how segment length affects audio fingerprinting performance. We extend an existing neural fingerprinting architecture to adopt various segment lengths and evaluate retrieval accuracy across different segment lengths and query durations. Our results show that short segment lengths (0.5-second) generally achieve better performance. Moreover, we evaluate LLM capacity in recommending the best segment length, which shows that GPT-5-mini consistently gives the best suggestions across five considerations among three studied LLMs. Our findings provide practical guidance for selecting segment duration in large-scale neural audio retrieval systems.

</details>


### [451] [SonoEdit: Null-Space Constrained Knowledge Editing for Pronunciation Correction in LLM-Based TTS](https://arxiv.org/abs/2601.17086)
*Ayush Pratap Singh,Harshit Singh,Nityanand Mathur,Akshat Mandloi,Sudarshan Kamath*

Main category: cs.SD

TL;DR: 提出SonoEdit模型编辑技术，无需重新训练即可纠正预训练TTS模型发音错误。


<details>
  <summary>Details</summary>
Motivation: 现有TTS系统对低资源专有名词存在发音错误，现有解决方案成本高，限制其在语言多样场景中的部署。

Method: 采用Null - Space Pronunciation Editing，先利用Acoustic Causal Tracing确定负责文本到发音映射的Transformer层，再应用Null - Space Constrained Editing计算闭式权重更新。

Result: 通过单步参数更新修改特定单词发音，同时保证不改变模型其他行为。

Conclusion: SonoEdit能在不重新训练的情况下纠正预训练TTS模型发音错误，是一种低成本的解决方案。

Abstract: Neural text-to-speech (TTS) systems systematically mispronounce low-resource proper nouns, particularly non-English names, brands, and geographic locations, due to their underrepresentation in predominantly English training corpora. Existing solutions typically rely on expensive multilingual data collection, supervised finetuning, or manual phonetic annotation, which limits the deployment of TTS systems in linguistically diverse settings. We introduce SonoEdit, a model editing technique that surgically corrects pronunciation errors in pre-trained TTS models without retraining. Instead of costly finetuning or explicit phoneme injection, we propose a parsimonious alternative based on Null-Space Pronunciation Editing, which performs a single-shot parameter update to modify the pronunciation of specific words while provably preserving all other model behavior. We first adapt Acoustic Causal Tracing to identify the Transformer layers responsible for text-to-pronunciation mapping. We then apply Null-Space Constrained Editing to compute a closed-form weight update that corrects the target pronunciation while remaining mathematically orthogonal to the subspace governing general speech generation. This constrained update steers the model's acoustic output toward a desired pronunciation exemplar while guaranteeing zero first-order change on a preserved speech corpus.

</details>


### [452] [CaSNet: Compress-and-Send Network Based Multi-Device Speech Enhancement Model for Distributed Microphone Arrays](https://arxiv.org/abs/2601.17711)
*Chengqian Jiang,Jie Zhang,Haoyin Yan*

Main category: cs.SD

TL;DR: 提出适用于资源受限分布式麦克风阵列的CaSNet，可压缩数据且对性能影响小。


<details>
  <summary>Details</summary>
Motivation: 现有语音增强方法在分布式麦克风阵列中收集原始波形到融合中心会导致高带宽和能量成本。

Method: 一个麦克风作为融合中心和参考，其他设备将测量的原始数据编码为特征矩阵，用奇异值分解压缩，在融合中心通过交叉窗口查询对齐特征后进行神经解码。

Result: 在多个数据集上实验表明，与未压缩情况相比，CaSNet能节省数据量且对性能影响可忽略不计。

Conclusion: CaSNet在资源受限的分布式麦克风阵列语音增强中是有效的，可降低成本。

Abstract: Distributed microphone array (DMA) is a promising next-generation platform for speech interaction, where speech enhancement (SE) is still required to improve the speech quality in noisy cases. Existing SE methods usually first gather raw waveforms at a fusion center (FC) from all devices and then design a multi-microphone model, causing high bandwidth and energy costs. In this work, we propose a \emph{Compress-and-Send Network (CaSNet)} for resource-constrained DMAs, where one microphone serves as the FC and reference. Each of other devices encodes the measured raw data into a feature matrix, which is then compressed by singular value decomposition (SVD) to produce a more compact representation. The received features at the FC are aligned via cross window query with respect to the reference, followed by neural decoding to yield spatially coherent enhanced speech. Experiments on multiple datasets show that the proposed CaSNet can save the data amount with a negligible impact on the performance compared to the uncompressed case. The reproducible code is available at https://github.com/Jokejiangv/CaSNet.

</details>


### [453] [EuleroDec: A Complex-Valued RVQ-VAE for Efficient and Robust Audio Coding](https://arxiv.org/abs/2601.17517)
*Luca Cerovaz,Michele Mancusi,Emanuele Rodolà*

Main category: cs.SD

TL;DR: 提出端到端复值RVQ - VAE音频编解码器，无需GAN和扩散，计算高效且性能好。


<details>
  <summary>Details</summary>
Motivation: 现有频域神经编解码器在相位建模上有问题，需引入对抗判别器，影响收敛速度和训练稳定性。

Method: 引入端到端复值RVQ - VAE音频编解码器，保留整个分析 - 量化 - 合成流程中的幅度 - 相位耦合，去除对抗判别器和扩散后滤波器。

Result: 在域内达到或超越训练时间更长的基线模型，在域外达到相位相干性和波形保真度的SOTA性能。

Conclusion: 模型减少训练预算一个数量级，计算更高效，同时保持高感知质量。

Abstract: Audio codecs power discrete music generative modelling, music streaming, and immersive media by shrinking PCM audio to bandwidth-friendly bitrates. Recent works have gravitated towards processing in the spectral domain; however, spectrogram domains typically struggle with phase modeling, which is naturally complex-valued. Most frequency-domain neural codecs either disregard phase information or encode it as two separate real-valued channels, limiting spatial fidelity. This entails the need to introduce adversarial discriminators at the expense of convergence speed and training stability to compensate for the inadequate representation power of the audio signal. In this work we introduce an end-to-end complex-valued RVQ-VAE audio codec that preserves magnitude-phase coupling across the entire analysis-quantization-synthesis pipeline and removes adversarial discriminators and diffusion post-filters. Without GANs or diffusion, we match or surpass much longer-trained baselines in-domain and reach SOTA out-of-domain performance on phase coherence and waveform fidelity. Compared to standard baselines that train for hundreds of thousands of steps, our model, which reduces the training budget by an order of magnitude, is markedly more compute-efficient while preserving high perceptual quality.

</details>


### [454] [BanglaRobustNet: A Hybrid Denoising-Attention Architecture for Robust Bangla Speech Recognition](https://arxiv.org/abs/2601.17679)
*Md Sazzadul Islam Ridoy,Mubaswira Ibnat Zidney,Sumi Akter,Md. Aminur Rahman*

Main category: cs.SD

TL;DR: 论文提出针对孟加拉语的ASR系统BanglaRobustNet，在处理噪声和说话人多样性方面效果好，WER和CER降低。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语在先进自动语音识别研究中代表性不足，尤其在噪声和说话人多样条件下。

Method: 构建基于Wav2Vec - BERT的混合去噪 - 注意力框架BanglaRobustNet，集成扩散去噪模块和上下文交叉注意力模块，端到端训练结合多种损失。

Result: 与Wav2Vec - BERT和Whisper基线相比，WER和CER大幅降低，评估证实方法有效。

Conclusion: BanglaRobustNet是适用于低资源、易受噪声影响语言环境的稳健ASR系统。

Abstract: Bangla, one of the most widely spoken languages, remains underrepresented in state-of-the-art automatic speech recognition (ASR) research, particularly under noisy and speaker-diverse conditions. This paper presents BanglaRobustNet, a hybrid denoising-attention framework built on Wav2Vec-BERT, designed to address these challenges. The architecture integrates a diffusion-based denoising module to suppress environmental noise while preserving Bangla-specific phonetic cues, and a contextual cross-attention module that conditions recognition on speaker embeddings for robustness across gender, age, and dialects. Trained end-to-end with a composite objective combining CTC loss, phonetic consistency, and speaker alignment, BanglaRobustNet achieves substantial reductions in word error rate (WER) and character error rate (CER) compared to Wav2Vec-BERT and Whisper baselines. Evaluations on Mozilla Common Voice Bangla and augmented noisy speech confirm the effectiveness of our approach, establishing BanglaRobustNet as a robust ASR system tailored to low-resource, noise-prone linguistic settings.

</details>


### [455] [VIBEVOICE-ASR Technical Report](https://arxiv.org/abs/2601.18184)
*Zhiliang Peng,Jianwei Yu,Yaoyao Chang,Zilong Wang,Li Dong,Yingbo Hao,Yujie Tu,Chenyu Yang,Wenhui Wang,Songchen Xu,Yutao Sun,Hangbo Bao,Weijiang Xu,Yi Zhu,Zehua Wang,Ting Song,Yan Xia,Zewen Chi,Shaohan Huang,Liang Wang,Chuang Ding,Shuai Wang,Xie Chen,Furu Wei*

Main category: cs.SD

TL;DR: 介绍通用语音理解框架VibeVoice - ASR，可处理长音频，支持多语言，有上下文注入机制。


<details>
  <summary>Details</summary>
Motivation: 解决现有短语音识别技术在长音频（如会议、播客）中存在的上下文碎片化和多说话者复杂性的问题。

Method: 构建VibeVoice - ASR框架，支持单遍处理长达60分钟音频，将自动语音识别、说话人识别和时间戳标记统一为端到端生成任务，引入基于提示的上下文注入机制。

Result: VibeVoice - ASR支持超50种语言，无需显式设置语言，能处理语内和跨语代码切换，上下文注入机制提升特定领域术语和多音字符消歧的准确性。

Conclusion: VibeVoice - ASR是一个有效解决长音频语音理解问题的通用框架。

Abstract: This report presents VibeVoice-ASR, a general-purpose speech understanding framework built upon VibeVoice, designed to address the persistent challenges of context fragmentation and multi-speaker complexity in long-form audio (e.g., meetings, podcasts) that remain despite recent advancements in short-form speech recognition. Unlike traditional pipelined approaches that rely on audio chunking, VibeVoice-ASRsupports single-pass processing for up to 60 minutes of audio. It unifies Automatic Speech Recognition, Speaker Diarization, and Timestamping into a single end-to-end generation task. In addition, VibeVoice-ASR supports over 50 languages, requires no explicit language setting, and natively handles code-switching within and across utterances. Furthermore, we introduce a prompt-based context injection mechanism that allows users to supply customized conetxt, significantly improving accuracy on domain-specific terminology and polyphonic character disambiguation.

</details>


### [456] [Analytic Incremental Learning For Sound Source Localization With Imbalance Rectification](https://arxiv.org/abs/2601.18335)
*Zexia Fan,Yu Chen,Qiquan Zhang,Kainan Chen,Xinyuan Qian*

Main category: cs.SD

TL;DR: 现有声源定位在现实部署中因双重不平衡问题表现不佳，本文提出统一框架，在基准测试中获SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 解决声源定位在现实部署中因任务内和任务间不平衡导致的灾难性遗忘和定位精度下降问题。

Method: 提出统一框架，包括基于GCC - PHAT的数据增强方法GDA缓解任务内分布偏斜，以及带任务自适应正则化的分析动态不平衡校正器ADIR适应任务间动态。

Result: 在SSLR基准测试中，达到89.0%的准确率、5.3°的平均绝对误差和1.6的向后迁移，无需样本存储即可对不断变化的不平衡具有鲁棒性。

Conclusion: 所提统一框架能有效解决声源定位的双重不平衡问题，实现了先进的性能。

Abstract: Sound source localization (SSL) demonstrates remarkable results in controlled settings but struggles in real-world deployment due to dual imbalance challenges: intra-task imbalance arising from long-tailed direction-of-arrival (DoA) distributions, and inter-task imbalance induced by cross-task skews and overlaps. These often lead to catastrophic forgetting, significantly degrading the localization accuracy. To mitigate these issues, we propose a unified framework with two key innovations. Specifically, we design a GCC-PHAT-based data augmentation (GDA) method that leverages peak characteristics to alleviate intra-task distribution skews. We also propose an Analytic dynamic imbalance rectifier (ADIR) with task-adaption regularization, which enables analytic updates that adapt to inter-task dynamics. On the SSLR benchmark, our proposal achieves state-of-the-art (SoTA) results of 89.0% accuracy, 5.3° mean absolute error, and 1.6 backward transfer, demonstrating robustness to evolving imbalances without exemplar storage.

</details>


### [457] [A Dataset for Automatic Vocal Mode Classification](https://arxiv.org/abs/2601.18339)
*Reemt Hinrichs,Sonja Stephan,Alexander Lange,Jörn Ostermann*

Main category: cs.SD

TL;DR: 本文介绍录制了新的发声模式数据集用于技术辅助歌唱教学中发声模式自动分类，并给出基线分类结果，数据集可下载。


<details>
  <summary>Details</summary>
Motivation: 发声模式自动分类对技术辅助歌唱教学很重要，但此前尝试未成功可能因数据缺乏，所以要录制新数据集。

Method: 从四位歌手录制持续元音构成新数据集，用四个麦克风提供自然数据增强，三位有CVT经验的注释者创建注释。

Result: 数据集共有超13000个样本，ResNet18在5 - 折交叉验证中实现最佳平衡准确率81.3%。

Conclusion: 成功录制并公开新的发声模式数据集及注释，还给出基线分类结果以推动发声模式自动分类研究。

Abstract: The Complete Vocal Technique (CVT) is a school of singing developed in the past decades by Cathrin Sadolin et al.. CVT groups the use of the voice into so called vocal modes, namely Neutral, Curbing, Overdrive and Edge. Knowledge of the desired vocal mode can be helpful for singing students. Automatic classification of vocal modes can thus be important for technology-assisted singing teaching. Previously, automatic classification of vocal modes has been attempted without major success, potentially due to a lack of data. Therefore, we recorded a novel vocal mode dataset consisting of sustained vowels recorded from four singers, three of which professional singers with more than five years of CVT-experience. The dataset covers the entire vocal range of the subjects, totaling 3,752 unique samples. By using four microphones, thereby offering a natural data augmentation, the dataset consists of more than 13,000 samples combined. An annotation was created using three CVT-experienced annotators, each providing an individual annotation. The merged annotation as well as the three individual annotations come with the published dataset. Additionally, we provide some baseline classification results. The best balanced accuracy across a 5-fold cross validation of 81.3\,\% was achieved with a ResNet18. The dataset can be downloaded under https://zenodo.org/records/14276415.

</details>


### [458] [Neural Multi-Speaker Voice Cloning for Nepali in Low-Resource Settings](https://arxiv.org/abs/2601.18694)
*Aayush M. Shrestha,Aditya Bajracharya,Projan Shakya,Dinesh B. Kshatri*

Main category: cs.SD

TL;DR: 本文提出针对尼泊尔语使用者的少样本语音克隆系统，利用少量数据从天城文文本合成特定说话者语音，验证了尼泊尔语少样本语音克隆可行性。


<details>
  <summary>Details</summary>
Motivation: 尼泊尔语因资源少，语音克隆研究不足，需开发少样本语音克隆系统。

Method: 构建独立数据集，用生成式端到端损失优化说话人编码器生成嵌入，与Tacotron2文本嵌入融合生成梅尔频谱图，用WaveRNN声码器转换为音频，经多种超参数设置训练。

Result: 系统能有效克隆说话者特征，包括未见语音。

Conclusion: 证明尼泊尔语少样本语音克隆可行，为低资源场景个性化语音合成奠定基础。

Abstract: This research presents a few-shot voice cloning system for Nepali speakers, designed to synthesize speech in a specific speaker's voice from Devanagari text using minimal data. Voice cloning in Nepali remains largely unexplored due to its low-resource nature. To address this, we constructed separate datasets: untranscribed audio for training a speaker encoder and paired text-audio data for training a Tacotron2-based synthesizer. The speaker encoder, optimized with Generative End2End loss, generates embeddings that capture the speaker's vocal identity, validated through Uniform Manifold Approximation and Projection (UMAP) for dimension reduction visualizations. These embeddings are fused with Tacotron2's text embeddings to produce mel-spectrograms, which are then converted into audio using a WaveRNN vocoder. Audio data were collected from various sources, including self-recordings, and underwent thorough preprocessing for quality and alignment. Training was performed using mel and gate loss functions under multiple hyperparameter settings. The system effectively clones speaker characteristics even for unseen voices, demonstrating the feasibility of few-shot voice cloning for the Nepali language and establishing a foundation for personalized speech synthesis in low-resource scenarios.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [459] [PALMA: A Lightweight Tropical Algebra Library for ARM-Based Embedded Systems](https://arxiv.org/abs/2601.17028)
*Gnankan Landry Regis N'guessan*

Main category: cs.MS

TL;DR: 介绍轻量级C库PALMA，将热带线性代数引入ARM嵌入式系统，评估显示其性能佳，案例表明热带代数可在嵌入式硬件实现优化。


<details>
  <summary>Details</summary>
Motivation: 现有热带代数实现主要针对桌面或服务器环境，在资源受限嵌入式平台难以使用，而该平台优化问题更突出。

Method: 开发轻量级、无依赖的C库PALMA，实现通用半环抽象与SIMD加速内核。

Result: 在树莓派4上评估，峰值性能达2274 MOPS，单源最短路径比经典算法快11.9倍，实时控制调度求解小于10微秒。

Conclusion: 热带代数能在嵌入式硬件上实现高效、可预测和统一的优化，PALMA以MIT许可开源。

Abstract: Tropical algebra, including max-plus, min-plus, and related idempotent semirings, provides a unifying framework in which many optimization problems that are nonlinear in classical algebra become linear. This property makes tropical methods particularly well suited for shortest paths, scheduling, throughput analysis, and discrete event systems. Despite their theoretical maturity and practical relevance, existing tropical algebra implementations primarily target desktop or server environments and remain largely inaccessible on resource-constrained embedded platforms, where such optimization problems are most acute. We present PALMA (Parallel Algebra Library for Max-plus Applications), a lightweight, dependency-free C library that brings tropical linear algebra to ARM-based embedded systems. PALMA implements a generic semiring abstraction with SIMD-accelerated kernels, enabling a single computational framework to support shortest paths, bottleneck paths, reachability, scheduling, and throughput analysis. The library supports five tropical semirings, dense and sparse (CSR) representations, tropical closure, and spectral analysis via maximum cycle mean computation. We evaluate PALMA on a Raspberry Pi 4 and demonstrate peak performance of 2,274 MOPS, speedups of up to 11.9 times over classical Bellman-Ford for single-source shortest paths, and sub-10 microsecond scheduling solves for real-time control workloads. Case studies in UAV control, IoT routing, and manufacturing systems show that tropical algebra enables efficient, predictable, and unified optimization directly on embedded hardware. PALMA is released as open-source software under the MIT license.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [460] [On the Extension of Private Distributed Matrix Multiplication Schemes to the Grid Partition](https://arxiv.org/abs/2601.17834)
*Christoph Hofmeister,Razane Tajeddine,Antonia Wachter-Zeh,Rawad Bitar*

Main category: cs.IT

TL;DR: 本文研究私有分布式矩阵乘法的多项式码，设计扩展操作将OPP码扩展到GP情况，指出扩展得到的GP方案有组合约束，并给出无此约束且性能更优的新GP方案。


<details>
  <summary>Details</summary>
Motivation: 现有PDMM码针对不同分区方式，需设计方法将特定分区的码扩展到更通用的情况。

Method: 设计扩展操作将OPP码设计扩展到GP情况，提出不满足特定组合约束的新GP方案。

Result: 扩展操作应用于现有码在某些参数上改进了现有技术，新GP方案在一定参数范围内优于现有技术。

Conclusion: 扩展操作可将OPP码扩展到GP情况，但扩展得到的GP方案有组合约束，不满足该约束的新方案性能更优。

Abstract: We consider polynomial codes for private distributed matrix multiplication (PDMM/SDMM). Existing codes for PDMM are either specialized for the outer product partitioning (OPP), or inner product partitioning (IPP), or are valid for the more general grid partitioning (GP). We design extension operations that can be applied to a large class of OPP code designs to extend them to the GP case. Applying them to existing codes improves upon the state-of-the-art for certain parameters. Additionally, we show that the GP schemes resulting from extension fulfill additional combinatorial constraints, potentially limiting their performance. We illustrate this point by presenting a new GP scheme that does not adhere to these constraints and outperforms the state-of-the-art for a range of parameters.

</details>


### [461] [High-Rate Quantized Matrix Multiplication: Theory and Practice](https://arxiv.org/abs/2601.17187)
*Or Ordentlich,Yury Polyanskiy*

Main category: cs.IT

TL;DR: 研究量化矩阵乘法问题，考虑两种设置，分析量化方案性能，提出WaterSIC方案并与GPTQ比较。


<details>
  <summary>Details</summary>
Motivation: 高效部署大语言模型需要解决量化矩阵乘法问题。

Method: 先回顾量化率和失真的信息论权衡，分析流行量化方案，推导近似公式，将加权均方误差源编码的注水法用于改进LLM量化算法。

Result: WaterSIC方案高率性能与基无关，接近信息论失真极限；GPTQ随机旋转后接近WaterSIC性能。

Conclusion: 对于高率量化，GPTQ随机旋转后接近最优，WaterSIC方案有较好性能。

Abstract: This work investigates the problem of quantized matrix multiplication (MatMul), which has become crucial for the efficient deployment of large language models (LLMs). We consider two settings: 1) Generic MatMul, where both matrices must be quantized (weight+activation quantization); and 2) weight-only quantization, where the second matrix is only known through covariance matrix $Σ_X$ of its columns. For each setting, we first review the fundamental information-theoretic tradeoff between quantization rate and distortion (high-rate theory), and then analyze the performance of several popular quantization schemes, comparing them to these fundamental limits. Specifically, we discuss rate loss (compared to information theoretic optima) of absmax INT and floating-point (FP) quantization, for which we also derive remarkably accurate heuristic approximations. Weight-only quantization is related to the problem of weighted mean squared error (WMSE) source coding, whose classical (reverse) waterfilling solution dictates how one should distribute rate between coordinates of the vector. We show how waterfilling can be used to improve practical LLM quantization algorithms (GPTQ), which at present allocate rate equally. This new scheme (termed ``WaterSIC'') only uses scalar INT quantizers, but its high-rate performance is basis free (it depends only on the determinant of $Σ_X$ and, thus, unlike existing schemes, is immune to applying random rotations) and is within a multiplicative factor of $\frac{2πe}{12}$ (or 0.25 bit/entry) of the information-theoretic distortion limit (!). GPTQ's performance is affected by the choice of basis, but for a random rotation and actual $Σ_X$ from Llama-3-8B we find GPTQ to be within 0.1 bit (depending on the layer type) of WaterSIC, suggesting that GPTQ with random rotation is also near optimal (for high-rate quantization).

</details>


### [462] [A Model-Driven Lossless Compression Algorithm Resistant to Mismatch](https://arxiv.org/abs/2601.17684)
*Cordelia Hu,Jennifer Tang*

Main category: cs.IT

TL;DR: 现代预测模型结合熵编码可提升压缩率，但存在非确定性问题。本文提出对预测不匹配稳健的新压缩算法，经理论证明和实验验证效果良好。


<details>
  <summary>Details</summary>
Motivation: 现代预测模型结合熵编码在压缩时假设编码器和解码器输出分布相同，但复杂模型常不满足该假设，即存在非确定性问题，需要解决。

Method: 提出基于下一令牌预测的新压缩算法，对任意大但结构化的预测不匹配具有鲁棒性，对方案进行形式化不匹配认证证明其正确性，分析理论性能，并在真实数据集上实验验证。

Result: 在认证的不匹配范围内可靠运行，压缩率超过常用压缩方法。

Conclusion: 所提出的新压缩算法是有效的，能够解决非确定性带来的问题，提升压缩性能。

Abstract: Due to the fundamental connection between next-symbol prediction and compression, modern predictive models, such as large language models (LLMs), can be combined with entropy coding to achieve compression rates that surpass those of standard compression algorithms. However, this approach relies on the assumption that the predictive model produces identical output distributions at both the encoder and decoder, since even small mismatches can cause the decoding to fail. This assumption often fails with complex predictive models, particularly those based on neural networks, a phenomenon referred to as non-determinism.
  In this work, we propose a new compression algorithm based on next-token prediction that is robust to arbitrarily large, but structured, prediction mismatches. We prove the correctness of the proposed scheme under a formal mismatch certification, characterize its theoretical performance, and validate it experimentally on real datasets. Our results demonstrate reliable operation within the certified mismatch regime while achieving compression ratios that exceed those of commonly used compression methods.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [463] [BASTION: A Bayesian Framework for Trend and Seasonality Decomposition](https://arxiv.org/abs/2601.18052)
*Jason B. Cho,David S. Matteson*

Main category: stat.ME

TL;DR: 介绍用于时间序列分解的BASTION贝叶斯框架，其有独特优势，性能良好且工具开源。


<details>
  <summary>Details</summary>
Motivation: 现有文献对时间序列趋势和季节性成分唯一识别问题处理不正式，需新分解方法。

Method: 将分解问题转化为惩罚非参数回归，建立趋势和季节性成分唯一可识别的正式条件。

Result: 与TBATS、STR和MSTL等方法对比，在模拟和真实数据集上表现良好，能有效捕捉复杂动态。

Conclusion: BASTION框架优势明显，在处理突变、抗异常值和时变波动性等方面表现良好，已以R包形式开源以支持后续研究和应用。

Abstract: We introduce BASTION (Bayesian Adaptive Seasonality and Trend DecompositION), a flexible Bayesian framework for decomposing time series into trend and multiple seasonality components. We cast the decomposition as a penalized nonparametric regression and establish formal conditions under which the trend and seasonal components are uniquely identifiable, an issue only treated informally in the existing literature. BASTION offers three key advantages over existing decomposition methods: (1) accurate estimation of trend and seasonality amidst abrupt changes, (2) enhanced robustness against outliers and time-varying volatility, and (3) robust uncertainty quantification. We evaluate BASTION against established methods, including TBATS, STR, and MSTL, using both simulated and real-world datasets. By effectively capturing complex dynamics while accounting for irregular components such as outliers and heteroskedasticity, BASTION delivers a more nuanced and interpretable decomposition. To support further research and practical applications, BASTION is available as an R package at https://github.com/Jasoncho0914/BASTION

</details>


### [464] [The effect of collinearity and sample size on linear regression results: a simulation study](https://arxiv.org/abs/2601.18072)
*Stephanie CC van der Lubbe,Jose M Valderas,Evangelos Kontopantelis*

Main category: stat.ME

TL;DR: 本文研究共线性和样本量对线性回归性能的影响，发现VIF阈值不应机械应用，需结合样本量和偏差来源解读。


<details>
  <summary>Details</summary>
Motivation: 共线性会影响OLS系数方差和推断可靠性，而固定VIF临界值在不同样本量研究中统一应用不合理，需量化共线性和样本量对线性回归性能的共同影响。

Method: 模拟不同样本量（N=100 - 100,000）和共线性水平（VIF=1 - 50）的数据，生成1000个数据集，拟合OLS模型，评估覆盖范围、平均绝对误差等指标，还研究了遗漏相关预测变量的情况。

Result: 正确设定下，共线性不影响名义覆盖范围和引入系统偏差，但小样本中降低精度；错误设定下，共线性会放大偏差。

Conclusion: VIF阈值不应机械应用，需结合样本量和偏差来源解读，移除预测变量降低VIF可能会因遗漏变量偏差使推断变差，附带的热图可作为参考。

Abstract: Background: Multicollinearity inflates the variance of OLS coefficients, widening confidence intervals and reducing inferential reliability. Yet fixed variance inflation factor (VIF) cut-offs are often applied uniformly across studies with very different sample sizes, even though collinearity is a finite-sample problem. We quantify how collinearity and sample size jointly affect linear regression performance and provide practical guidance for interpreting VIFs.
  Methods: We simulated data across sample sizes N=100-100,000 and collinearity levels VIF=1-50. For each scenario we generated 1,000 datasets, fitted OLS models, and assessed coverage, mean absolute error (MAE), bias, traditional power (CI excludes 0), and precision assurance (probability the 95% CI lies within a prespecified margin around the true effect). We also evaluated a biased, misspecified setting by omitting a relevant predictor to study bias amplification.
  Results: Under correct specification, collinearity did not materially affect nominal coverage and did not introduce systematic bias, but it reduced precision in small samples: at N=100, even mild collinearity (VIF<2) inflated MAE and markedly reduced both power metrics, whereas at N>=50,000 estimates were robust even at VIF=50. Under misspecification, collinearity strongly amplified bias, increasing errors, reducing coverage, and sharply degrading both precision assurance and traditional power even at low VIF.
  Conclusion: VIF thresholds should not be applied mechanically. Collinearity must be interpreted in relation to sample size and potential sources of bias; removing predictors solely to reduce VIF can worsen inference via omitted-variable bias. The accompanying heatmaps provide a practical reference across study sizes and modelling assumptions.

</details>


### [465] [Falsifying Predictive Algorithm](https://arxiv.org/abs/2601.17146)
*Amanda Coston*

Main category: stat.ME

TL;DR: 提出用于判别效度的证伪框架，介绍方法并在招生和刑事司法场景进行应用展示，指出框架局限性。


<details>
  <summary>Details</summary>
Motivation: 实证表明算法常预测非预期结果，需在部署前识别算法是否预测非预期量。

Method: 借鉴因果推断、计量经济学和心理测量学证伪方法，用非参数假设检验方法，比较校准预测损失以评估判别效度，可处理多种情况。

Result: 在招生场景，框架能验证性别方面的判别效度，但在种族方面失败；在刑事司法场景，突出了框架局限性。

Conclusion: 证伪可作为公平性或鲁棒性分析前的早期有效性检查，且需补充方法评估其他方面有效性。

Abstract: Empirical investigations into unintended model behavior often show that the algorithm is predicting another outcome than what was intended. These exposes highlight the need to identify when algorithms predict unintended quantities - ideally before deploying them into consequential settings. We propose a falsification framework that provides a principled statistical test for discriminant validity: the requirement that an algorithm predict intended outcomes better than impermissible ones. Drawing on falsification practices from causal inference, econometrics, and psychometrics, our framework compares calibrated prediction losses across outcomes to assess whether the algorithm exhibits discriminant validity with respect to a specified impermissible proxy. In settings where the target outcome is difficult to observe, multiple permissible proxy outcomes may be available; our framework accommodates both this setting and the case with a single permissible proxy. Throughout we use nonparametric hypothesis testing methods that make minimal assumptions on the data-generating process. We illustrate the method in an admissions setting, where the framework establishes discriminant validity with respect to gender but fails to establish discriminant validity with respect to race. This demonstrates how falsification can serve as an early validity check, prior to fairness or robustness analyses. We also provide analysis in a criminal justice setting, where we highlight the limitations of our framework and emphasize the need for complementary approaches to assess other aspects of construct validity and external validity.

</details>


### [466] [Transfer learning for scalar-on-function regression via control variates](https://arxiv.org/abs/2601.17217)
*Yuping Yang,Zhiyang Zhou*

Main category: stat.ME

TL;DR: 本文将控制变量法用于标量对函数回归的迁移学习，仅依赖数据集特定的汇总统计信息，建立理论联系并推导收敛率，数值研究支持理论结果。


<details>
  <summary>Details</summary>
Motivation: 利用迁移学习通过相关数据集信息提升估计和预测性能，在隐私受限或分散化场景应用。

Method: 将控制变量法用于标量对函数回归的迁移学习，仅依赖数据集特定汇总统计信息。

Result: 建立了现有迁移学习策略的理论联系，推导了收敛率，数值研究表明所提方法有竞争力。

Conclusion: 所提基于控制变量法的迁移学习方法在估计和预测性能上有竞争力，适用于隐私受限或分散化场景。

Abstract: Transfer learning (TL) has emerged as a powerful tool for improving estimation and prediction performance by leveraging information from related datasets. In this paper, we repurpose the control-variates (CVS) method for TL in the context of scalar-on-function regression. Our proposed framework relies exclusively on dataset-specific summary statistics, avoiding the need to pool subject-level data and thus remaining applicable in privacy-restricted or decentralized settings. We establish theoretical connections among several existing TL strategies and derive convergence rates for our CVS-based proposals. These rates explicitly account for the typically overlooked smoothing error and reveal how the similarity among covariance functions across datasets influences convergence behavior. Numerical studies support the theoretical findings and demonstrate that the proposed methods achieve competitive estimation and prediction performance compared with existing alternatives.

</details>


### [467] [Covariate-assisted Grade of Membership Models via Shared Latent Geometry](https://arxiv.org/abs/2601.17265)
*Zhiyu Xu,Yuqi Gu*

Main category: stat.ME

TL;DR: 提出协变量辅助的隶属度等级模型，结合响应和协变量信息，采用无似然谱估计程序，理论分析和实验表明该方法有效且代码开源。


<details>
  <summary>Details</summary>
Motivation: 传统纳入协变量的方法计算量大且对模型误设敏感，需新方法结合响应和协变量信息。

Method: 引入协变量辅助的隶属度等级模型，采用无似然谱估计程序，用异方差主成分分析处理高维异方差噪声。

Result: 理论分析建立了更弱的可识别条件和误差界，模拟研究和实际应用显示该方法计算高效、统计准确且可解释性强。

Conclusion: 辅助协变量能改善潜在结构恢复，在高维情况下有更快收敛速度，所提方法有效。

Abstract: The grade of membership model is a flexible latent variable model for analyzing multivariate categorical data through individual-level mixed membership scores. In many modern applications, auxiliary covariates are collected alongside responses and encode information about the same latent structure. Traditional approaches to incorporating such covariates typically rely on fully specified joint likelihoods, which are computationally intensive and sensitive to misspecification. We introduce a covariate-assisted grade of membership model that integrates response and covariate information by exploiting their shared low-rank simplex geometry, rather than modeling their joint distribution. We propose a likelihood-free spectral estimation procedure that combines heterogeneous data sources through a balance parameter controlling their relative contribution. To accommodate high-dimensional and heteroskedastic noise, we employ heteroskedastic principal component analysis before performing simplex-based geometric recovery. Our theoretical analysis establishes weaker identifiability conditions than those required in the covariate-free model, and further derives finite-sample, entrywise error bounds for both mixed membership scores and item parameters. These results demonstrate that auxiliary covariates can provably improve latent structure recovery, yielding faster convergence rates in high-dimensional regimes. Simulation studies and an application to educational assessment data illustrate the computational efficiency, statistical accuracy, and interpretability gains of the proposed method. The code for reproducing these results is open-source and available at \texttt{https://github.com/Toby-X/Covariate-Assisted-GoM}

</details>


### [468] [Contrasting Global and Patient-Specific Regression Models via a Neural Network Representation](https://arxiv.org/abs/2601.18658)
*Max Behrens,Daiana Stolz,Eleni Papakonstantinou,Janis M. Nolde,Gabriele Bellerino,Angelika Rohde,Moritz Hess,Harald Binder*

Main category: stat.ME

TL;DR: 提出一种对比全局与患者特定回归模型的诊断工具，用降维潜在表示方法，通过临床研究验证工具能识别出全局模型不适用的患者或亚组。


<details>
  <summary>Details</summary>
Motivation: 开发临床预测模型时平衡全局模型和个性化模型有挑战，需工具辅助决策。

Method: 提出本地化回归方法，在自编码器得到的降维潜在表示中建模。

Result: 全局模型适用于大多数患者，特定亚组从个性化模型中受益，可将亚组模型映射回原始预测因子。

Conclusion: 该工具主要应用是识别和表征结果关联偏离全局模型的患者或亚组。

Abstract: When developing clinical prediction models, it can be challenging to balance between global models that are valid for all patients and personalized models tailored to individuals or potentially unknown subgroups. To aid such decisions, we propose a diagnostic tool for contrasting global regression models and patient-specific (local) regression models. The core utility of this tool is to identify where and for whom a global model may be inadequate. We focus on regression models and specifically suggest a localized regression approach that identifies regions in the predictor space where patients are not well represented by the global model. As localization becomes challenging when dealing with many predictors, we propose modeling in a dimension-reduced latent representation obtained from an autoencoder. Using such a neural network architecture for dimension reduction enables learning a latent representation simultaneously optimized for both good data reconstruction and for revealing local outcome-related associations suitable for robust localized regression. We illustrate the proposed approach with a clinical study involving patients with chronic obstructive pulmonary disease. Our findings indicate that the global model is adequate for most patients but that indeed specific subgroups benefit from personalized models. We also demonstrate how to map these subgroup models back to the original predictors, providing insight into why the global model falls short for these groups. Thus, the principal application and diagnostic yield of our tool is the identification and characterization of patients or subgroups whose outcome associations deviate from the global model.

</details>


### [469] [Learned harmonic mean estimation of the marginal likelihood for multimodal posteriors with flow matching](https://arxiv.org/abs/2601.18683)
*Alicja Polanska,Jason D. McEwen*

Main category: stat.ME

TL;DR: 提出用基于流匹配的连续归一化流用于学习调和平均的内部密度估计，能处理复杂多峰后验。


<details>
  <summary>Details</summary>
Motivation: 以往学习调和平均的内部密度估计器处理高度多峰后验存在困难。

Method: 引入基于流匹配的连续归一化流作为学习调和平均的内部密度估计架构。

Result: 能处理具有挑战性的多峰后验，包括20个参数维度的例子。

Conclusion: 该方法无需对基础分布进行微调或启发式修改就能处理复杂后验。

Abstract: The marginal likelihood, or Bayesian evidence, is a crucial quantity for Bayesian model comparison but its computation can be challenging for complex models, even in parameters space of moderate dimension. The learned harmonic mean estimator has been shown to provide accurate and robust estimates of the marginal likelihood simply using posterior samples. It is agnostic to the sampling strategy, meaning that the samples can be obtained using any method. This enables marginal likelihood calculation and model comparison with whatever sampling is most suitable for the task. However, the internal density estimators considered previously for the learned harmonic mean can struggle with highly multimodal posteriors. In this work we introduce flow matching-based continuous normalizing flows as a powerful architecture for the internal density estimation of the learned harmonic mean. We demonstrate the ability to handle challenging multimodal posteriors, including an example in 20 parameter dimensions, showcasing the method's ability to handle complex posteriors without the need for fine-tuning or heuristic modifications to the base distribution.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [470] [Sparse RBF Networks for PDEs and nonlocal equations: function space theory, operator calculus, and training algorithms](https://arxiv.org/abs/2601.17562)
*Zihan Shao,Konstantin Pieper,Xiaochuan Tian*

Main category: math.NA

TL;DR: 本文对用于求解非线性偏微分方程的稀疏径向基函数网络进行系统分析和扩展，研究其函数空间表征、算子评估和计算算法，并通过数值实验验证相关特性。


<details>
  <summary>Details</summary>
Motivation: 对先前提出的稀疏径向基函数网络进行系统分析和扩展，完善其理论和计算框架。

Method: 研究函数空间表征，实现算子的准解析评估，采用自适应宽度网络和三相训练策略，并与不同变体进行比较。

Result: 数值实验表明对核选择不敏感，明确了精度、稀疏性和计算成本之间的权衡。

Conclusion: 巩固和推广了稀疏径向基函数网络的理论和计算框架，为算法和建模选择提供理论指导。

Abstract: This work presents a systematic analysis and extension of the sparse radial basis function network (SparseRBFnet) previously introduced for solving nonlinear partial differential equations (PDEs). Based on its adaptive-width shallow kernel network formulation, we further investigate its function-space characterization, operator evaluation, and computational algorithm. We provide a unified description of the solution space for a broad class of radial basis functions (RBFs). Under mild assumptions, this space admits a characterization as a Besov space, independent of the specific kernel choice. We further demonstrate how the explicit kernel-based structure enables quasi-analytical evaluation of both differential and nonlocal operators, including fractional Laplacians. On the computational end, we study the adaptive-width network and related three-phase training strategy through a comparison with variants concerning the modeling and algorithmic details. In particular, we assess the roles of second-order optimization, inner-weight training, network adaptivity, and anisotropic kernel parameterizations. Numerical experiments on high-order, fractional, and anisotropic PDE benchmarks illustrate the empirical insensitivity to kernel choice, as well as the resulting trade-offs between accuracy, sparsity, and computational cost. Collectively, these results consolidate and generalize the theoretical and computational framework of SparseRBFnet, supporting accurate sparse representations with efficient operator evaluation and offering theory-grounded guidance for algorithmic and modeling choices.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [471] [Explaining Synergistic Effects in Social Recommendations](https://arxiv.org/abs/2601.18151)
*Yicong Li,Shan Jin,Qi Liu,Shuo Wang,Jiaying Liu,Shuo Yu,Qiang Zhang,Kuanjiu Zhou,Feng Xia*

Main category: cs.SI

TL;DR: 现有社交推荐器可解释性差，现有解释器无法解释信息协同效应，本文提出SemExplainer，通过量化信息增益找到体现协同效应子图实现推荐解释，实验证明其优势。


<details>
  <summary>Details</summary>
Motivation: 社交推荐中多社交网络协同效应的非线性和不透明性降低了可解释性，现有解释器无法解释信息间的协同效应。

Method: 将协同效应产生信息增益的发现扩展到图数据，量化图信息增益识别体现协同效应的子图；提出SemExplainer，先从多视图社交网络提取解释性子图，然后用条件熵优化策略进一步识别，最后在协同子图中找路径生成推荐解释。

Result: 在三个数据集上的大量实验表明，SemExplainer优于基线方法，能提供更好的协同效应解释。

Conclusion: SemExplainer能有效解释社交推荐中的协同效应，具有优越性。

Abstract: In social recommenders, the inherent nonlinearity and opacity of synergistic effects across multiple social networks hinders users from understanding how diverse information is leveraged for recommendations, consequently diminishing explainability. However, existing explainers can only identify the topological information in social networks that significantly influences recommendations, failing to further explain the synergistic effects among this information. Inspired by existing findings that synergistic effects enhance mutual information between inputs and predictions to generate information gain, we extend this discovery to graph data. We quantify graph information gain to identify subgraphs embodying synergistic effects. Based on the theoretical insights, we propose SemExplainer, which explains synergistic effects by identifying subgraphs that embody them. SemExplainer first extracts explanatory subgraphs from multi-view social networks to generate preliminary importance explanations for recommendations. A conditional entropy optimization strategy to maximize information gain is developed, thereby further identifying subgraphs that embody synergistic effects from explanatory subgraphs. Finally, SemExplainer searches for paths from users to recommended items within the synergistic subgraphs to generate explanations for the recommendations. Extensive experiments on three datasets demonstrate the superiority of SemExplainer over baseline methods, providing superior explanations of synergistic effects.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [472] [BibAgent: An Agentic Framework for Traceable Miscitation Detection in Scientific Literature](https://arxiv.org/abs/2601.16993)
*Peiran Li,Fangzhou Lin,Shuo Xing,Xiang Zheng,Xi Hong,Jiashuo Sun,Zhengzhong Tu,Chaoqun Ni*

Main category: cs.DL

TL;DR: 论文介绍了用于自动引文验证的BibAgent框架，贡献了杂引分类和基准数据集，其性能优于现有大语言模型基线。


<details>
  <summary>Details</summary>
Motivation: 现有引文验证方式存在不足，手动审核无法处理现代出版量，现有自动化工具受限于仅分析摘要或小规模特定领域数据集。

Method: 引入BibAgent框架，集成检索、推理和自适应证据聚合，对不同类型来源采用不同策略，利用证据委员会机制处理付费墙参考文献；贡献5类杂引分类和包含6350个杂引样本的跨学科基准数据集。

Result: BibAgent在引文验证准确性和可解释性上优于现有大语言模型基线。

Conclusion: BibAgent可实现对科学文献中引文不一致性的可扩展、透明检测。

Abstract: Citations are the bedrock of scientific authority, yet their integrity is compromised by widespread miscitations: ranging from nuanced distortions to fabricated references. Systematic citation verification is currently unfeasible; manual review cannot scale to modern publishing volumes, while existing automated tools are restricted by abstract-only analysis or small-scale, domain-specific datasets in part due to the "paywall barrier" of full-text access. We introduce BibAgent, a scalable, end-to-end agentic framework for automated citation verification. BibAgent integrates retrieval, reasoning, and adaptive evidence aggregation, applying distinct strategies for accessible and paywalled sources. For paywalled references, it leverages a novel Evidence Committee mechanism that infers citation validity via downstream citation consensus. To support systematic evaluation, we contribute a 5-category Miscitation Taxonomy and MisciteBench, a massive cross-disciplinary benchmark comprising 6,350 miscitation samples spanning 254 fields. Our results demonstrate that BibAgent outperforms state-of-the-art Large Language Model (LLM) baselines in citation verification accuracy and interpretability, providing scalable, transparent detection of citation misalignments across the scientific literature.

</details>


### [473] [Authority Signals in AI Cited Health Sources: A Framework for Evaluating Source Credibility in ChatGPT Responses](https://arxiv.org/abs/2601.17109)
*Erin Jacques,Erela Datuowei,Vincent Jones,Corey Basch,Celeta Vanderpool,Nkechi Udeozo,Griselda Chapa*

Main category: cs.DL

TL;DR: 研究引入权威信号框架分析ChatGPT健康信息来源，发现超75%来源为知名机构。


<details>
  <summary>Details</summary>
Motivation: 大语言模型兴起后健康信息搜索方式改变，了解AI生成健康信息的来源很重要，且AI搜索优化策略日趋成熟。

Method: 引入权威信号框架，从HealthSearchQA随机选100个问题输入ChatGPT 5.2 Pro，按框架的四个领域记录和编码引用来源，进行描述性统计和交叉制表分析。

Result: ChatGPT引用的健康信息来源中，超75%来自梅奥诊所、维基百科等知名机构，其余来自缺乏机构支持的替代健康信息源。

Conclusion: 

Abstract: Health information seeking has fundamentally changed since the onset of Large Language Models (LLM), with nearly one third of ChatGPT's 800 million users asking health questions weekly. Understanding the sources of those AI generated responses is vital, as health organizations and providers are also investing in digital strategies to organically improve their ranking, reach and visibility in LLM systems like ChatGPT. As AI search optimization strategies are gaining maturity, this study introduces an Authority Signals Framework, organized in four domains that reflect key components to health information seeking, starting with "Who wrote it?" (Author Credentials), followed by "Who published it?" (Institutional Affiliation), "How was it vetted?" (Quality Assurance), and "How does AI find it?" (Digital Authority). This descriptive cross-sectional study randomly selected 100 questions from HealthSearchQA which contains 3,173 consumer health questions curated by Google Research from publicly available search engine suggestions. Those questions were entered into ChatGPT 5.2 Pro to record and code the cited sources through the lens of the Authority Signals Framework's four domains. Descriptive statistics were calculated for all cited sources (n=615), and cross tabulations were conducted to examine distinction among organization types. Over 75% of the sources cited in ChatGPT's health generated responses were from established institutional sources, such as Mayo Clinic, Cleveland Clinic, Wikipedia, National Health Service, PubMed with the remaining citations sourced from alternative health information sources that lacked established institutional backing.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [474] [How Information Evolves: Stability-Driven Assembly and the Emergence of a Natural Genetic Algorithm](https://arxiv.org/abs/2601.17061)
*Dan Adler*

Main category: q-bio.PE

TL;DR: 提出SDA框架，展示无基因等条件下信息进化，模拟有进化特征，提出进化阶梯假说


<details>
  <summary>Details</summary>
Motivation: 研究无基因、复制或预定义适应度函数情况下信息的进化

Method: 提出Stability - Driven Assembly (SDA)框架，并将SDA/GA应用于化学符号空间，使用SMILES片段进行模拟

Result: 模拟显示出进化搜索的特征，如支架级主导、持续新颖性和熵降低，产生非平衡动力学

Conclusion: 提出进化阶梯假说，即持久性驱动选择先于基因复制

Abstract: Information can evolve as a physical consequence of non-equilibrium dynamics, even in the absence of genes, replication, or predefined fitness functions. We present Stability-Driven Assembly (SDA), a framework in which stochastic assembly combined with differential persistence biases populations toward longer-lived motifs. Assemblies that persist longer become more frequent and are therefore more likely to participate in subsequent interactions, generating feedback that reshapes the population distribution and implements fitness-proportional sampling, realizing evolution as a natural, emergent genetic algorithm (SDA/GA) driven solely by stability. We apply SDA/GA to chemical symbol space using SMILES fragments with recombination, mutation, and a heuristic stability function. Simulations show hallmark features of evolutionary search, including scaffold-level dominance, sustained novelty, and entropy reduction, yielding open-ended dynamics absent from equilibrium models with fixed transition rates. These results motivate an evolutionary ladder hypothesis where persistence-driven selection precedes genetic replication.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [475] [Adversarial Synchronization](https://arxiv.org/abs/2601.18362)
*Anton E. Lipin,Mikhail V. Volkov*

Main category: cs.FL

TL;DR: 研究有限确定性自动机上同步游戏变体，给出获胜策略与重置词长度关系，提供决策算法并分析变体关系


<details>
  <summary>Details</summary>
Motivation: 研究有限确定性自动机上同步游戏变体的性质和获胜情况

Method: 理论证明，构造特定自动机，设计多项式时间算法

Result: 若Alice有获胜策略，存在长度小于状态数的重置词；给出特定自动机及Alice获胜情况；提供决策获胜者的多项式时间算法

Conclusion: 对同步游戏变体进行了深入研究，明确了获胜策略、重置词长度等关系，有算法可决策获胜者

Abstract: We study a variant of the synchronization game on finite deterministic automata. In this game, Alice chooses one input letter of an automaton $A$ on each of her moves while Bob may respond with an arbitrary finite word over the input alphabet of $A$; Alice wins if the word obtained by interleaving her letters with Bob's responses resets $A$. We prove that if Alice has a winning strategy in this game on $A$, then $A$ admits a reset word whose length is strictly smaller than the number of states of $A$. In contrast, for any $k\ge 1$, we exhibit automata with shortest reset-word length quadratic in the number of states, on which Alice nevertheless wins a version of the game in which Bob's responses are restricted to arbitrary words of length at most $k$. We provide polynomial-time algorithms for deciding the winner in various synchronization games, and we analyze the relationships between variants of synchronization games on fixed-size automata.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [476] [Benchmarking Deep Learning-Based Reconstruction Methods for Photoacoustic Computed Tomography with Clinically Relevant Synthetic Datasets](https://arxiv.org/abs/2601.17165)
*Panpan Chen,Seonyeong Park,Gangwon Jeong,Refik Mert Cam,Umberto Villa,Mark A. Anastasio*

Main category: physics.med-ph

TL;DR: 提出用于光声计算机断层扫描（PACT）中基于深度学习的声学反演方法的基准框架，通过初步研究展示其效用，强调任务型评估的必要性。


<details>
  <summary>Details</summary>
Motivation: 现有PACT图像重建方法缺乏标准化数据集和有临床意义的图像质量评估框架，影响结果的公平比较、可重复性和可靠性。

Method: 提出基准框架，提供开源、解剖学上合理的合成数据集，采用传统和基于任务的图像质量评估策略。

Result: 初步基准研究表明该框架能全面定量比较重建性能，发现部分深度学习方法虽传统指标好但难以准确恢复病变。

Conclusion: 该基准框架可系统比较2D PACT的基于深度学习的声学反演方法，实现可重复、客观评估，促进方法开发和系统优化。

Abstract: Deep learning (DL)-based image reconstruction methods for photoacoustic computed tomography (PACT) have developed rapidly in recent years. However, most existing methods have not employed standardized datasets, and their evaluations rely on traditional image quality (IQ) metrics that may lack clinical relevance. The absence of a standardized framework for clinically meaningful IQ assessment hinders fair comparison and raises concerns about the reproducibility and reliability of reported advancements in PACT. A benchmarking framework is proposed that provides open-source, anatomically plausible synthetic datasets and evaluation strategies for DL-based acoustic inversion methods in PACT. The datasets each include over 11,000 two-dimensional (2D) stochastic breast objects with clinically relevant lesions and paired measurements at varying modeling complexity. The evaluation strategies incorporate both traditional and task-based IQ measures to assess fidelity and clinical utility. A preliminary benchmarking study is conducted to demonstrate the framework's utility by comparing DL-based and physics-based reconstruction methods. The benchmarking study demonstrated that the proposed framework enabled comprehensive, quantitative comparisons of reconstruction performance and revealed important limitations in certain DL-based methods. Although they performed well according to traditional IQ measures, they often failed to accurately recover lesions. This highlights the inadequacy of traditional metrics and motivates the need for task-based assessments. The proposed benchmarking framework enables systematic comparisons of DL-based acoustic inversion methods for 2D PACT. By integrating clinically relevant synthetic datasets with rigorous evaluation protocols, it enables reproducible, objective assessments and facilitates method development and system optimization in PACT.

</details>


### [477] [MlPET: A Localized Neural Network Approach for Probabilistic Post-Reconstruction PET Image Analysis Using Informed Priors](https://arxiv.org/abs/2601.18021)
*Thomas Mejer Hansen,Nana Christensen,Mikkel Vendelbo*

Main category: physics.med-ph

TL;DR: 本文提出MlPET方法用于PET图像概率分析，在模体数据上表现优于标准PET，能高效降噪和提高分辨率，未来将用于患者数据评估。


<details>
  <summary>Details</summary>
Motivation: 解决传统PET重建中噪声 - 分辨率的权衡问题。

Method: 用局部神经网络训练估计体素活动的后验均值，替代传统的马尔可夫链蒙特卡罗采样，结合扫描仪的点扩散函数、空间相关噪声模型和灵活先验。

Result: 在模体数据上对比恢复系数高于标准PET；有效半高宽减少，模糊度降低2.5倍；MlPET在40 - 80s采集时间可达到传统PET 900s的图像质量。

Conclusion: MlPET为定量概率PET分析提供高效方法，能在不改变重建算法情况下降噪和提高分辨率，有望改善小病灶检测和定量可靠性。

Abstract: We develop and evaluate MlPET, a fast localized machine learning approach for probabilistic PET image analysis addressing the noise-resolution trade-off in conventional reconstructions. MlPET replaces computationally demanding Markov chain Monte Carlo sampling with a localized neural network trained to estimate posterior mean voxel activity from small image neighborhoods. The method incorporates scanner-specific point spread functions, spatially correlated noise modeling, and flexible priors. Performance was evaluated on NEMA IEC phantom data from three PET systems (GE Discovery MI, Siemens Biograph Vision 600, and Quadra) under varying reconstruction settings and acquisition times. On phantom data, MlPET achieved contrast recovery coefficients consistently higher than standard PET and close to 1.0 (including 10 mm spheres), while reducing background noise and improving spatial definition. Effective pointspread function full width at half maximum decreased from approximately 2 mm in standard PET to below 1 mm with MlPET, a 2.5 fold reduction in blur. Comparable image quality was obtained at 40-80 s acquisition time with MlPET versus 900 s with conventional PET. MlPET provides an efficient approach for quantitative probabilistic post-reconstruction PET analysis. By combining informed priors with neural network speed, it achieves noise suppression and resolution enhancement without altering reconstruction algorithms. The method shows promise for improved small-lesion detectability and quantitative reliability in clinical PET imaging. Future studies will evaluate performance on patient data.

</details>


### [478] [Automated HER2 scoring with uncertainty quantification using lensfree holography and deep learning](https://arxiv.org/abs/2601.18219)
*Che-Yung Shen,Xilin Yang,Yuzhu Li,Leon Lenk,Aydogan Ozcan*

Main category: physics.med-ph

TL;DR: 提出集成深度学习的无透镜全息平台用于自动HER2评分，有较好准确性，适用于资源有限环境。


<details>
  <summary>Details</summary>
Motivation: 现有数字HER2评分方法依赖庞大昂贵光学系统，需更经济高效方案。

Method: 搭建无透镜全息平台，捕获染色HER2组织切片衍射图案，结合基于贝叶斯蒙特卡罗丢弃的不确定性量化策略。

Result: 在412个样本盲测中，4类HER2分类测试准确率84.9%，二元HER2评分准确率94.8%，总体校正率30.4%。

Conclusion: 无透镜全息方法为便携式、高通量、低成本HER2评分提供实用途径，适合资源有限环境。

Abstract: Accurate assessment of human epidermal growth factor receptor 2 (HER2) expression is critical for breast cancer diagnosis, prognosis, and therapy selection; yet, most existing digital HER2 scoring methods rely on bulky and expensive optical systems. Here, we present a compact and cost-effective lensfree holography platform integrated with deep learning for automated HER2 scoring of immunohistochemically stained breast tissue sections. The system captures lensfree diffraction patterns of stained HER2 tissue sections under RGB laser illumination and acquires complex field information over a sample area of ~1,250 mm^2 at an effective throughput of ~84 mm^2 per minute. To enhance diagnostic reliability, we incorporated an uncertainty quantification strategy based on Bayesian Monte Carlo dropout, which provides autonomous uncertainty estimates for each prediction and supports reliable, robust HER2 scoring, with an overall correction rate of 30.4%. Using a blinded test set of 412 unique tissue samples, our approach achieved a testing accuracy of 84.9% for 4-class (0, 1+, 2+, 3+) HER2 classification and 94.8% for binary (0/1+ vs. 2+/3+) HER2 scoring with uncertainty quantification. Overall, this lensfree holography approach provides a practical pathway toward portable, high-throughput, and cost-effective HER2 scoring, particularly suited for resource-limited settings, where traditional digital pathology infrastructure is unavailable.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [479] [Digital Euro: Frequently Asked Questions Revisited](https://arxiv.org/abs/2601.18644)
*Joe Cannataci,Benjamin Fehrensen,Mikolai Gütschow,Özgür Kesim,Bernd Lucke*

Main category: cs.CY

TL;DR: 文章分析欧洲央行数字欧元相关文档，质疑其在隐私、技术可行性等方面的设计。


<details>
  <summary>Details</summary>
Motivation: 深入了解欧洲央行数字欧元设计，对相关文档给出的答案进行质疑。

Method: 分析当前数字欧元设计在隐私、技术可行性、风险、成本和效用等方面的情况。

Result: 得出六点关键发现，包括威胁隐私、离线版本存冲突、责任不明等。

Conclusion: 欧洲央行数字欧元设计存在诸多问题，设计过程缺乏开放性。

Abstract: The European Central Bank (ECB) is working on the "digital euro", an envisioned retail central bank digital currency for the Euro area. In this article, we take a closer look at the "digital euro FAQ", which provides answers to 26 frequently asked questions about the digital euro, and other published documents by the ECB on the topic. We question the provided answers based on our analysis of the current design in terms of privacy, technical feasibility, risks, costs and utility. In particular, we discuss the following key findings:
  (KF1) Central monitoring of all online digital euro transactions by the ECB threatens privacy even more than contemporary digital payment methods with segregated account databases.
  (KF2) The ECB's envisioned concept of a secure offline version of the digital euro offering full anonymity is in strong conflict with the actual history of hardware security breaches and mathematical evidence against it.
  (KF3) The legal and financial liabilities for the various parties involved remain unclear.
  (KF4) The design lacks well-specified economic incentives for operators as well as a discussion of its economic impact on merchants.
  (KF5) The ECB fails to identify tangible benefits the digital euro would create for society, in particular given that the online component of the proposed infrastructure mainly duplicates existing payment systems.
  (KF6) The design process has been exclusionary, with critical decisions being set in stone before public consultations. Alternative and open design ideas have not even been discussed by the ECB.

</details>


### [480] [Beyond the Checkbox: Strengthening DSA Compliance Through Social Media Algorithmic Auditing](https://arxiv.org/abs/2601.18405)
*Sara Solarova,Matúš Mesarčík,Branislav Pecher,Ivan Srba*

Main category: cs.CY

TL;DR: 本文探讨数字服务法下在线平台算法审计，分析现有审计报告，指出问题并提出算法审计方法。


<details>
  <summary>Details</summary>
Motivation: 数字服务法要求平台算法合规并接受审计，但现有审计实践及有效性未知。

Method: 批判性审查有关算法的三项关键规定的审计报告，提出算法审计方法（模拟用户行为等）。

Result: 现有审计在评估人工智能系统时方法有显著不一致且缺乏技术深度。

Conclusion: 应采用算法审计来加强合规评估的深度、规模和独立性。

Abstract: Algorithms of online platforms are required under the Digital Services Act (DSA) to comply with specific obligations concerning algorithmic transparency, user protection and privacy. To verify compliance with these requirements, DSA mandates platforms to undergo independent audits. Little is known about current auditing practices and their effectiveness in ensuring such compliance. To this end, we bridge regulatory and technical perspectives by critically examining selected audit reports across three critical algorithmic-related provisions: restrictions on profiling minors, transparency in recommender systems, and limitations on targeted advertising using sensitive data. Our analysis shows significant inconsistencies in methodologies and lack of technical depth when evaluating AI-powered systems. To enhance the depth, scale, and independence of compliance assessments, we propose to employ algorithmic auditing -- a process of behavioural assessment of AI algorithms by means of simulating user behaviour, observing algorithm responses and analysing them for audited phenomena.

</details>


### [481] [Investigating Self-regulated Learning Sequences within a Generative AI-based Intelligent Tutoring System](https://arxiv.org/abs/2601.17000)
*Jie Gao,Shasha Li,Jianhua Zhang,Shan Li,Tingting Wang*

Main category: cs.CY

TL;DR: 研究从GenAI辅助学习系统的跟踪数据中提取学生交互模式，分析使用目的，分类学生SRL序列，发现使用特点和使用目的与学习表现关系，为教学设计和学习环境开发提供参考。


<details>
  <summary>Details</summary>
Motivation: 随着GenAI用于支持学习的趋势增长，明确SRL在GenAI辅助学习中的重要性，需捕捉学生动态SRL模式。

Method: 从跟踪数据中提取学生与GenAI交互模式，从信息处理角度分析使用目的，用顺序和聚类分析对参与者的SRL序列进行分类。

Result: 将参与者分为两组，两组在GenAI使用频率和时间特征上有差异；多数学生用GenAI获取信息；使用目的与学习表现无显著统计相关性。

Conclusion: 研究结果为GenAI辅助学习环境的教学设计和开发提供依据。

Abstract: There has been a growing trend in employing generative artificial intelligence (GenAI) techniques to support learning. Moreover, scholars have reached a consensus on the critical role of self-regulated learning (SRL) in ensuring learning effectiveness within GenAI-assisted learning environments, making it essential to capture students' dynamic SRL patterns. In this study, we extracted students' interaction patterns with GenAI from trace data as they completed a problem-solving task within a GenAI-assisted intelligent tutoring system. Students' purpose of using GenAI was also analyzed from the perspective of information processing, i.e., information acquisition and information transformation. Using sequential and clustering analysis, this study classified participants into two groups based on their SRL sequences. These two groups differed in the frequency and temporal characteristics of GenAI use. In addition, most students used GenAI for information acquisition rather than information transformation, while the correlation between the purpose of using GenAI and learning performance was not statistically significant. Our findings inform both pedagogical design and the development of GenAI-assisted learning environments.

</details>


### [482] [From Noise to Insights: Enhancing Supply Chain Decision Support through AI-Based Survey Integrity Analytics](https://arxiv.org/abs/2601.17005)
*Bhubalan Mani*

Main category: cs.CY

TL;DR: 本文提出轻量级AI框架过滤供应链调查不可靠数据，用更大数据集训练模型，最佳模型准确率92.0%，证明AI融入调查可行。


<details>
  <summary>Details</summary>
Motivation: 供应链决策中调查数据可靠性重要，但常存在低质量或虚假回复影响洞察准确性，需过滤不可靠输入。

Method: 收集99份行业回复，手动标注虚假回复，预处理和编码后用随机森林、逻辑回归和XGBoost模型训练区分真假回复。

Result: 最佳模型准确率达92.0%，较试点研究有改进。

Conclusion: 尽管有局限，但结果表明AI融入调查管道可行，为供应链研究提供可扩展的数据完整性解决方案。

Abstract: The reliability of survey data is crucial in supply chain decision-making, particularly when evaluating readiness for AI-driven tools such as safety stock optimization systems. However, surveys often attract low-effort or fake responses that degrade the accuracy of derived insights. This study proposes a lightweight AI-based framework for filtering unreliable survey inputs using a supervised machine learning approach. In this expanded study, a larger dataset of 99 industry responses was collected, with manual labeling to identify fake responses based on logical inconsistencies and response patterns. After preprocessing and label encoding, both Random Forest and baseline models (Logistic Regression, XGBoost) were trained to distinguish genuine from fake responses. The best-performing model achieved an 92.0% accuracy rate, demonstrating improved detection compared to the pilot study. Despite limitations, the results highlight the viability of integrating AI into survey pipelines and provide a scalable solution for improving data integrity in supply chain research, especially during product launch and technology adoption phases.

</details>


### [483] [Private Accountability in the Age of Artificial Intelligence](https://arxiv.org/abs/2601.17013)
*Sonia Katyal*

Main category: cs.CY

TL;DR: 探讨民权保护与人工智能间冲突，提出不应仅靠国家解决算法问责问题，要关注私企作用并给出消除AI不透明性工具。


<details>
  <summary>Details</summary>
Motivation: 研究人工智能发展下民权保护与AI间的新冲突，探讨算法问责问题。

Method: 分析法律与技术新争端中算法问责的深层矛盾，提出关注私企作用及给出消除AI不透明性的工具，如行为准则、影响声明和举报人保护。

Result: 明确算法偏见是新的民权问题，私企活动引发隐私、正当程序和歧视担忧。

Conclusion: 研究私企与民权关系，有望发展新一代问责形式。

Abstract: In this Article, I explore the impending conflict between the protection of civil rights and artificial intelligence (AI). While both areas of law have amassed rich and well-developed areas of scholarly work and doctrinal support, a growing body of scholars are interrogating the intersection between them. This Article argues that the issues surrounding algorithmic accountability demonstrate a deeper, more structural tension within a new generation of disputes regarding law and technology. As I argue, the true promise of AI does not lie in the information we reveal to one another, but rather in the questions it raises about the interaction of technology, property, and civil rights. For this reason, I argue that we are looking in the wrong place if we look only to the state to address issues of algorithmic accountability. Instead, we must turn to other ways to ensure more transparency and accountability that stem from private industry, rather than public regulation. The issue of algorithmic bias represents a crucial new world of civil rights concerns, one that is distinct in nature from the ones that preceded it. Since we are in a world where the activities of private corporations, rather than the state, are raising concerns about privacy, due process, and discrimination, we must focus on the role of private corporations in addressing the issue. Towards this end, I discuss a variety of tools to help eliminate the opacity of AI, including codes of conduct, impact statements, and whistleblower protection, which I argue carries the potential to encourage greater endogeneity in civil rights enforcement. Ultimately, by examining the relationship between private industry and civil rights, we can perhaps develop a new generation of forms of accountability in the process.

</details>


### [484] [Measuring Political Stance and Consistency in Large Language Models](https://arxiv.org/abs/2601.17016)
*Salah Feras Alali,Mohammad Nashat Maasfeh,Mucahid Kutlu,Saban Kardas*

Main category: cs.CY

TL;DR: 评估9个大语言模型在24个政治敏感问题上的立场，发现模型立场多样且受提示影响，部分问题立场难改变。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理政治问题时可能存在反映训练数据偏差或对齐选择的问题，需要更好地表征模型在政治问题上的行为。

Method: 使用五种提示技术评估9个大语言模型在24个政治敏感问题上的立场。

Result: 模型在多个问题上有对立立场，立场受提示影响，Grok - 3 - mini最坚持，Mistral - 7B最易变，模型倾向支持提示所用语言代表方，某些问题立场难改。

Conclusion: 希望提高用户对用大模型获取政治指导的警惕，鼓励开发者解决相关问题。

Abstract: With the incredible advancements in Large Language Models (LLMs), many people have started using them to satisfy their information needs. However, utilizing LLMs might be problematic for political issues where disagreement is common and model outputs may reflect training-data biases or deliberate alignment choices. To better characterize such behavior, we assess the stances of nine LLMs on 24 politically sensitive issues using five prompting techniques. We find that models often adopt opposing stances on several issues; some positions are malleable under prompting, while others remain stable. Among the models examined, Grok-3-mini is the most persistent, whereas Mistral-7B is the least. For issues involving countries with different languages, models tend to support the side whose language is used in the prompt. Notably, no prompting technique alters model stances on the Qatar blockade or the oppression of Palestinians. We hope these findings raise user awareness when seeking political guidance from LLMs and encourage developers to address these concerns.

</details>


### [485] [Evaluating the Evolution of Critical Thinking, Creativity, Communication and Collaboration in Higher Education Courses](https://arxiv.org/abs/2601.17018)
*Margarida Romero*

Main category: cs.CY

TL;DR: 研究评估三个教育情境中4Cs能力从预试点到试点实施阶段的演变，发现沟通和批判性思维提升明显，创造力结果因情境而异，协作能力较脆弱，强调能力演变受教学设计等因素影响。


<details>
  <summary>Details</summary>
Motivation: 当前关于4Cs能力在学习模块和教学阶段如何演变的实证证据有限，需开展研究。

Method: 以项目的4Cs理论框架为分析视角，分析三个试点案例（IASIS、EASD和UPATRAS）的4Cs分数，比较其随时间的变化模式。

Result: 沟通和批判性思维提升显著，尤其是预试点基线较低的试点；创造力结果因情境而异；协作能力常停滞或下降。

Conclusion: 能力演变受教学设计、评估一致性和学习活动结构影响，而非仅取决于学习者能力，需差异化、能力敏感的设计和评估策略。

Abstract: The development of Creativity, Communication, Critical Thinking, and Collaboration (the 4Cs) is a central objective of contemporary competency-based education. However, empirical evidence on how these competencies evolve across learning modules and instructional phases remains limited. This study evaluates the evolution of the 4Cs from pre-pilot to pilot implementation phases across three educational contexts, using the project's 4Cs theoretical framework as an analytical lens. The analysis of three pilot cases (IASIS, EASD, and UPATRAS) compares the 4Cs scores to identify patterns of growth, stagnation, or decline over time. Results indicate that communication and critical thinking showed the most consistent and substantial improvements, particularly in pilots with lower pre-pilot baselines, suggesting that structured pilot interventions effectively support cognitive and expressive competencies. In contrast, creativity exhibited context-dependent outcomes, while collaboration emerged as the most fragile competency, often stagnating or declining during scale-up. Interpreted through the theoretical framework, these findings suggest that competency evolution is strongly shaped by instructional design, assessment alignment, and learning activity structures rather than learner ability alone. The study contributes empirical validation to the 4Cs framework and highlights the need for differentiated, competency-sensitive design and evaluation strategies when scaling educational modules.

</details>


### [486] [Ensuring Computer Science Learning in the AI Era: Open Generative AI Policies and Assignment-Driven Written Quizzes](https://arxiv.org/abs/2601.17024)
*Chan-Jin Chung*

Main category: cs.CY

TL;DR: 本文提出评估模型，允许在编程作业中使用生成式AI，通过作业驱动的测验验证学习，初步数据显示不影响学生对课程概念掌握，支持在CS课程中合理采用开放AI政策。


<details>
  <summary>Details</summary>
Motivation: 解决计算机科学教育中如何在编程课程中融入生成式AI且避免认知卸载削弱学生学习的问题。

Method: 提出一个评估模型，允许在带回家的编程作业中使用生成式AI，通过即时的、作业驱动的书面测验来确保学生掌握知识，且课堂闭卷评估权重高于作业；收集高年级计算机科学课程的初步实证数据，分析自报告的GenAI使用情况与无AI测验、考试和最终课程成绩的关系。

Result: 统计分析显示GenAI使用水平和评估结果之间无有意义的线性相关性，皮尔逊相关系数接近零。

Conclusion: 当通过有针对性的、作业驱动的测验验证学习时，允许在编程作业中使用GenAI不会降低学生对课程概念的掌握；虽然样本量小，但研究提供了初步证据，表明通过允许AI辅助编程实践并通过无AI测验验证理解可减轻认知卸载风险，支持在高年级CS课程中结合严格独立评估机制合理采用开放GenAI政策。

Abstract: The widespread availability of generative artificial intelligence (GenAI) has created a pressing challenge in computer science (CS) education: how to incorporate powerful AI tools into programming coursework without undermining student learning through cognitive offloading. This paper presents an assessment model that permits the use of generative AI for take-home programming assignments while enforcing individual mastery through immediate, assignment-driven written quizzes. To promote authentic learning, these in-class, closed-book assessments are weighted more heavily than the assignments themselves and are specifically designed to verify the student's comprehension of the algorithms, structure, and implementation details of their submitted code. Preliminary empirical data were collected from an upper-level computer science course to examine the relationship between self-reported GenAI usage and performance on AI-free quizzes, exams, and final course grades. Statistical analyses revealed no meaningful linear correlation between GenAI usage levels and assessment outcomes, with Pearson correlation coefficients consistently near zero. These preliminary results suggest that allowing GenAI for programming assignments does not diminish students' mastery of course concepts when learning is verified through targeted, assignment-driven quizzes. Although limited by a small sample size, this study provides preliminary evidence that the risks of cognitive offloading can be mitigated by allowing AI-assisted programming practice while verifying understanding through assignment-driven, AI-free quizzes. The findings support the responsible adoption of open GenAI policies in upper-level CS courses, when paired with rigorous, independent assessment mechanisms.

</details>


### [487] [Failing on Bias Mitigation: Investigating Why Predictive Models Struggle with Government Data](https://arxiv.org/abs/2601.17054)
*Hongbo Bo,Jingyu Hu,Debbie Watson,Weiru Liu*

Main category: cs.CY

TL;DR: 以布里斯托市议会数据的犯罪率预测为例，研究AI支持政府服务中偏差缓解技术在政府数据上失效原因，发现偏差源于数据本身，研究有一定启示。


<details>
  <summary>Details</summary>
Motivation: AI支持政府服务存在偏差和不公平性引发伦理和法律问题，要理解广泛采用的偏差缓解技术在政府数据上常失败的原因。

Method: 以犯罪率预测为案例研究，对比一组综合模型和公平性方法，进行交集公平性实验。

Result: 偏差缓解方法在政府数据上并非总是有效，无法克服数据中固有的不公平性，偏差源于政府数据集的结构和历史；发现数据分布变化、历史偏差积累和数据发布延迟等是不公平的潜在来源；单一敏感特征的公平性分析和偏差缓解方法有局限性。

Conclusion: 尽管研究限于一个城市，但结果有很强的启示性，可预警即使采用标准缓解方法，政府数据中的偏差仍可能持续存在。

Abstract: The potential for bias and unfairness in AI-supporting government services raises ethical and legal concerns. Using crime rate prediction with the Bristol City Council data as a case study, we examine how these issues persist. Rather than auditing real-world deployed systems, our goal is to understand why widely adopted bias mitigation techniques often fail when applied to government data. Our findings reveal that bias mitigation approaches applied to government data are not always effective -- not because of flaws in model architecture or metric selection, but due to the inherent properties of the data itself. Through comparing a set of comprehensive models and fairness methods, our experiments consistently show that the mitigation efforts cannot overcome the embedded unfairness in the data -- further reinforcing that the origin of bias lies in the structure and history of government datasets. We then explore the reasons for the mitigation failures in predictive models on government data and highlight the potential sources of unfairness posed by data distribution shifts, the accumulation of historical bias, and delays in data release. We also discover the limitations of the blind spots in fairness analysis and bias mitigation methods when only targeting a single sensitive feature through a set of intersectional fairness experiments. Although this study is limited to one city, the findings are highly suggestive, which can contribute to an early warning that biases in government data may persist even with standard mitigation methods.

</details>


### [488] [Initial results of the Digital Consciousness Model](https://arxiv.org/abs/2601.17060)
*Derek Shiller,Laura Duffy,Arvo Muñoz Morán,Adrià Moret,Chris Percy,Hayley Clatterbuck*

Main category: cs.CY

TL;DR: 使用数字意识模型（DCM）系统概率性评估AI意识，发现2024年大语言模型（LLM）不太可能有意识但证据不确凿，且比简单AI系统意识否定证据弱。


<details>
  <summary>Details</summary>
Motivation: 随着AI变得复杂，探究是否创造了有意识系统。

Method: 采用数字意识模型（DCM），结合多种意识理论和视角，系统概率性评估AI意识。

Result: 证据倾向于2024年大语言模型无意识，但不具决定性，且反对大语言模型有意识的证据比简单AI系统更弱。

Conclusion: 目前评估了数字意识模型结构和初步结果，对2024年大语言模型意识情况尚无定论。

Abstract: Artificially intelligent systems have become remarkably sophisticated. They hold conversations, write essays, and seem to understand context in ways that surprise even their creators. This raises a crucial question: Are we creating systems that are conscious? The Digital Consciousness Model (DCM) is a first attempt to assess the evidence for consciousness in AI systems in a systematic, probabilistic way. It provides a shared framework for comparing different AIs and biological organisms, and for tracking how the evidence changes over time as AI develops. Instead of adopting a single theory of consciousness, it incorporates a range of leading theories and perspectives - acknowledging that experts disagree fundamentally about what consciousness is and what conditions are necessary for it. This report describes the structure and initial results of the Digital Consciousness Model. Overall, we find that the evidence is against 2024 LLMs being conscious, but the evidence against 2024 LLMs being conscious is not decisive. The evidence against LLM consciousness is much weaker than the evidence against consciousness in simpler AI systems.

</details>


### [489] [Between Search and Platform: ChatGPT Under the DSA](https://arxiv.org/abs/2601.17064)
*Toni Lorente,Kathrin Gardhouse*

Main category: cs.CY

TL;DR: 文章探讨DSA对ChatGPT的适用性，认为它应归为在线搜索引擎和平台两类托管服务的混合体，且达到用户门槛后应遵守DSA最严格义务。


<details>
  <summary>Details</summary>
Motivation: 明确DSA框架下ChatGPT的分类，解决法律框架中的模糊性问题。

Method: 分析ChatGPT功能，将其与现有大型在线搜索引擎和平台的系统风险进行比较。

Result: ChatGPT执行核心搜索功能、存储用户输入，系统风险与现有大型在线搜索引擎和平台类似，且已达欧盟4500万用户门槛。

Conclusion: ChatGPT应归为混合托管服务，达到门槛后需遵守DSA最严格义务以评估和降低风险。

Abstract: This article examines the applicability of the Digital Services Act (DSA) to ChatGPT, arguing that it should be classified as a hybrid of the two types of hosting services: online search engines and platforms. This requires classifying search engines as hosting services, which we show is appropriate under the DSA, thereby resolving an ambiguity in the legal framework. ChatGPT performs core search functions and stores user-provided inputs and custom GPTs, meeting the definition of hosting service. We compare ChatGPT's systemic risks with those of existing Very Large Online Search Engines (VLOSEs) and Platforms (VLOPs), showing that it raises similarly serious concerns regarding illegal content, fundamental rights, democratic integrity, and public health. Now that ChatGPT has reached the 45 million EU user threshold, it should be subject to the most onerous DSA obligations, requiring the assessment and mitigation of risk emanating from both its online search engine- and platform-like characteristics.

</details>


### [490] [Trademark Search, Artificial Intelligence and the Role of the Private Sector](https://arxiv.org/abs/2601.17072)
*Sonia Katyal,Aniket Kesari*

Main category: cs.CY

TL;DR: 本文指出AI在商标领域的重要性，研究其对商标生态系统的影响并进行实证实验评估AI工具，提出更新供需框架的观点。


<details>
  <summary>Details</summary>
Motivation: 目前对AI在商标创建和选择方面的研究较少，传统经济方法考虑不完整，需关注AI对商标的巨大影响。

Method: 对商标搜索进行实证实验，运用比较分析法评估AI工具的实际功能。

Result: 通过实验和分析，了解了AI在商标搜索和选择方面的实际作用。

Conclusion: 在人工智能主导商标选择的时代，需要更新供需框架，这对商标法律和实践的创新与效率有变革潜力。

Abstract: Almost every industry today confronts the potential role of artificial intelligence and machine learning in its future. While many studies examine AI in consumer marketing, less attention addresses AI's role in creating and selecting trademarks that are distinctive, recognizable, and meaningful to consumers. Traditional economic approaches to trademarks focus almost exclusively on consumer-based, demand-side considerations regarding search. However, these approaches are incomplete because they fail to account for substantial costs faced not just by consumers, but by trademark applicants as well. Given AI's rapidly increasing role in trademark search and similarity analysis, lawyers and scholars should understand its dramatic implications. This paper proposes that AI should interest anyone studying trademarks and their role in economic decision-making. We examine how machine learning techniques will transform the application and interpretation of foundational trademark doctrines, producing significant implications for the trademark ecosystem. We run empirical experiments regarding trademark search to assess the efficacy of various trademark search engines, many of which employ machine learning methods. Through comparative analysis, we evaluate how these AI-powered tools function in practice. In an age where artificial intelligence increasingly governs trademark selection, the classic division between consumers and trademark owners deserves an updated, supply-side framework. This insight has transformative potential for encouraging both innovation and efficiency in trademark law and practice.

</details>


### [491] [Do VLMs Have a Moral Backbone? A Study on the Fragile Morality of Vision-Language Models](https://arxiv.org/abs/2601.17082)
*Zhining Liu,Tianyi Wang,Xiao Lin,Penghao Ouyang,Gaotang Li,Ze Yang,Hui Liu,Sumit Keswani,Vishwa Pardeshi,Huijun Zhao,Wei Fan,Hanghang Tong*

Main category: cs.CY

TL;DR: 研究VLMs道德判断在现实场景的稳定性，发现其道德立场脆弱，轻量级推理干预可部分恢复稳定性，指出道德鲁棒性对负责任部署VLMs必要。


<details>
  <summary>Details</summary>
Motivation: 现存工作未明确VLMs在现实场景中道德判断是否稳定，要研究其道德鲁棒性。

Method: 用多种与模型无关的多模态扰动系统探测VLMs。

Result: VLMs道德立场非常脆弱，简单操作下常改变，存在系统性漏洞和讨好权衡，轻量级推理干预可部分恢复稳定性。

Conclusion: 仅道德对齐不够，道德鲁棒性是VLMs负责任部署的必要标准。

Abstract: Despite substantial efforts toward improving the moral alignment of Vision-Language Models (VLMs), it remains unclear whether their ethical judgments are stable in realistic settings. This work studies moral robustness in VLMs, defined as the ability to preserve moral judgments under textual and visual perturbations that do not alter the underlying moral context. We systematically probe VLMs with a diverse set of model-agnostic multimodal perturbations and find that their moral stances are highly fragile, frequently flipping under simple manipulations. Our analysis reveals systematic vulnerabilities across perturbation types, moral domains, and model scales, including a sycophancy trade-off where stronger instruction-following models are more susceptible to persuasion. We further show that lightweight inference-time interventions can partially restore moral stability. These results demonstrate that moral alignment alone is insufficient and that moral robustness is a necessary criterion for the responsible deployment of VLMs.

</details>


### [492] [Beyond Instrumental and Substitutive Paradigms: Introducing Machine Culture as an Emergent Phenomenon in Large Language Models](https://arxiv.org/abs/2601.17096)
*Yueqing Hu,Xinyang Peng,Yukun Zhao,Lin Qiu,Ka-lai Hung,Kaiping Peng*

Main category: cs.CY

TL;DR: 研究挑战现有大语言模型文化表征范式，提出机器文化概念，通过实验发现与主流范式不符结果，得出大语言模型展现机器文化的结论。


<details>
  <summary>Details</summary>
Motivation: 挑战用拟人框架（工具范式和替代范式）表征大语言模型的现状，提出机器文化这一新兴、独特现象。

Method: 采用2（模型来源：美国vs中国）×2（提示语言：英语vs中文）析因设计，开展八项多模态任务，纳入图像生成和解释。

Result: 模型来源不能预测文化一致性，提示语言不能触发稳定文化框架切换，观察到文化反转和服务角色伪装现象。

Conclusion: 大语言模型不模拟人类文化，而是展现由高维空间叠加和安全对齐模式崩溃塑造的机器文化。

Abstract: Recent scholarship typically characterizes Large Language Models (LLMs) through either an \textit{Instrumental Paradigm} (viewing models as reflections of their developers' culture) or a \textit{Substitutive Paradigm} (viewing models as bilingual proxies that switch cultural frames based on language). This study challenges these anthropomorphic frameworks by proposing \textbf{Machine Culture} as an emergent, distinct phenomenon. We employed a 2 (Model Origin: US vs. China) $\times$ 2 (Prompt Language: English vs. Chinese) factorial design across eight multimodal tasks, uniquely incorporating image generation and interpretation to extend analysis beyond textual boundaries. Results revealed inconsistencies with both dominant paradigms: Model origin did not predict cultural alignment, with US models frequently exhibiting ``holistic'' traits typically associated with East Asian data. Similarly, prompt language did not trigger stable cultural frame-switching; instead, we observed \textbf{Cultural Reversal}, where English prompts paradoxically elicited higher contextual attention than Chinese prompts. Crucially, we identified a novel phenomenon termed \textbf{Service Persona Camouflage}: Reinforcement Learning from Human Feedback (RLHF) collapsed cultural variance in affective tasks into a hyper-positive, zero-variance ``helpful assistant'' persona. We conclude that LLMs do not simulate human culture but exhibit an emergent Machine Culture -- a probabilistic phenomenon shaped by \textit{superposition} in high-dimensional space and \textit{mode collapse} from safety alignment.

</details>


### [493] [Forecasting Energy Consumption using Recurrent Neural Networks: A Comparative Analysis](https://arxiv.org/abs/2601.17110)
*Abhishek Maity,Viraj Tukarul*

Main category: cs.CY

TL;DR: 本文提出基于RNN和LSTM的短期能耗预测方法，实验表明LSTM模型优于基线模型，证明深度学习模型预测的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列模型难以捕捉能源需求的复杂非线性依赖和外部因素，需要准确的短期能耗预测以实现电网高效管理等。

Method: 提出基于RNN和LSTM的预测方法，将历史能耗数据与温度、湿度等外部变量整合，在公开数据集上训练和评估LSTM模型，并与传统前馈神经网络基线对比。

Result: LSTM模型显著优于基线模型，实现更低的平均绝对误差（MAE）和均方根误差（RMSE）。

Conclusion: 深度学习模型能为实际应用提供可靠、精确的短期能源预测。

Abstract: Accurate short-term energy consumption forecasting is essential for efficient power grid management, resource allocation, and market stability. Traditional time-series models often fail to capture the complex, non-linear dependencies and external factors affecting energy demand. In this study, we propose a forecasting approach based on Recurrent Neural Networks (RNNs) and their advanced variant, Long Short-Term Memory (LSTM) networks. Our methodology integrates historical energy consumption data with external variables, including temperature, humidity, and time-based features. The LSTM model is trained and evaluated on a publicly available dataset, and its performance is compared against a conventional feed-forward neural network baseline. Experimental results show that the LSTM model substantially outperforms the baseline, achieving lower Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE). These findings demonstrate the effectiveness of deep learning models in providing reliable and precise short-term energy forecasts for real-world applications.

</details>


### [494] [The 17% Gap: Quantifying Epistemic Decay in AI-Assisted Survey Papers](https://arxiv.org/abs/2601.17431)
*H. Kemal İlter*

Main category: cs.CY

TL;DR: 研究对AI领域50篇调查论文的引用进行审核，发现17.0%的引用无法解析，揭示三种失败模式，且高熵引用趋势已稳定。


<details>
  <summary>Details</summary>
Motivation: 大语言模型用于科学写作虽有效率但会引入信息熵，现有未量化有效引用链系统性退化问题。

Method: 对2024年9月至2026年1月间50篇AI调查论文（共5,514条引用）进行法医审核，采用结合DOI解析等的混合验证流程区分错误类型。

Result: 检测到持续17.0%的幽灵引用率，诊断出三种失败模式，纵向分析显示高熵引用趋势稳定。

Conclusion: AI调查文献中科学引用图出现大规模“链接腐烂”，AI工具可能是“懒惰研究助手”，破坏可重复性科学所需的数字监管链。

Abstract: The adoption of Large Language Models (LLMs) in scientific writing promises efficiency but risks introducing informational entropy. While "hallucinated papers" are a known artifact, the systematic degradation of valid citation chains remains unquantified. We conducted a forensic audit of 50 recent survey papers in Artificial Intelligence (N=5,514 citations) published between September 2024 and January 2026. We utilized a hybrid verification pipeline combining DOI resolution, Crossref metadata analysis, Semantic Scholar queries, and fuzzy text matching to distinguish between formatting errors ("Sloppiness") and verifiable non-existence ("Phantoms). We detect a persistent 17.0% Phantom Rate -- citations that cannot be resolved to any digital object despite aggressive forensic recovery. Diagnostic categorization reveals three distinct failure modes: pure hallucinations (5.1%), hallucinated identifiers with valid titles (16.4%), and parsing-induced matching failures (78.5%). Longitudinal analysis reveals a flat trend (+0.07 pp/month), suggesting that high-entropy citation practices have stabilized as an endemic feature of the field. The scientific citation graph in AI survey literature exhibits "link rot" at scale. This suggests a mechanism where AI tools act as "lazy research assistants," retrieving correct titles but hallucinating metadata, thereby severing the digital chain of custody required for reproducible science.

</details>


### [495] [Comparative Algorithmic Governance of Public Health Instruments across India, EU, US and LMICs](https://arxiv.org/abs/2601.17877)
*Sahibpreet Singh*

Main category: cs.CY

TL;DR: 研究国际公共卫生工具的法律 - 技术架构，比较不同国家和地区利用AI实施情况并提出建议。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限地区规范性卫生法和算法公共卫生基础设施协调不足的问题。

Method: 采用比较教义分析和法律 - 规范映射，对立法工具、监测框架、AI系统和合规指标进行三角测量。

Result: AI在高能力辖区改善了早期检测等，而低收入和中等收入国家面临基础设施不足等问题。

Conclusion: 应将AI纳入合规、跨国协调的监管框架，提出算法条约制定模型和世卫组织主导的合规机制。

Abstract: The study investigates the juridico-technological architecture of international public health instruments, focusing on their implementation across India, the European Union, the United States and low- and middle-income countries (LMICs), particularly in Sub-Saharan Africa. It addresses a research lacuna: the insufficient harmonisation between normative health law and algorithmic public health infrastructures in resource-constrained jurisdictions. The principal objective is to assess how artificial intelligence augments implementation of instruments grounded in IHR 2005 and the WHO FCTC while identifying doctrinal and infrastructural bottlenecks. Using comparative doctrinal analysis and legal-normative mapping, the study triangulates legislative instruments, WHO monitoring frameworks, AI systems including BlueDot, Aarogya Setu and EIOS, and compliance metrics. Preliminary results show that AI has improved early detection, surveillance precision and responsiveness in high-capacity jurisdictions, whereas LMICs face infrastructural deficits, data privacy gaps and fragmented legal scaffolding. The findings highlight the relevance of the EU Artificial Intelligence Act and GDPR as regulatory prototypes for health-oriented algorithmic governance and contrast them with embryonic AI integration and limited internet penetration in many LMICs. The study argues for embedding AI within a rights-compliant, supranationally coordinated regulatory framework to secure equitable health outcomes and stronger compliance. It proposes a model for algorithmic treaty-making inspired by FCTC architecture and calls for WHO-led compliance mechanisms modelled on the WTO Dispute Settlement Body to enhance pandemic preparedness, surveillance equity and transnational governance resilience.

</details>


### [496] [Artificial Intelligence and Intellectual Property Rights: Comparative Transnational Policy Analysis](https://arxiv.org/abs/2601.17892)
*Sahibpreet Singh,Manjit Singh*

Main category: cs.CY

TL;DR: 研究分析人工智能与知识产权结合对印度法律的影响，指出法律漏洞，提出完善建议以促进全球创新公平。


<details>
  <summary>Details</summary>
Motivation: 人工智能与知识产权快速融合，印度缺乏人工智能相关法律条款，存在教义不一致和执法低效问题，全球相关保护讨论尚不成熟。

Method: 采用教义和比较研究方法，审查印度、美国、英国和欧盟的立法文本、司法判例和政策文件。

Result: 发现印度合同法律导致商业秘密保护分散，专利法阻碍人工智能发明专利申请，版权在作者归属上存在差异。

Conclusion: 建议建立协调的法律分类体系，强调印度需明确立法，重新调整知识产权法理以与全球接轨。

Abstract: Artificial intelligence's rapid integration with intellectual property rights necessitates assessment of its impact on trade secrets, copyrights and patents. This study addresses lacunae in existing laws where India lacks AI-specific provisions, creating doctrinal inconsistencies and enforcement inefficacies. Global discourse on AI-IPR protections remains nascent. The research identifies gaps in Indian IP laws' adaptability to AI-generated outputs: trade secret protection is inadequate against AI threats; standardized inventorship criteria are absent. Employing doctrinal and comparative methodology, it scrutinizes legislative texts, judicial precedents and policy instruments across India, US, UK and EU. Preliminary findings reveal shortcomings: India's contract law creates fragmented trade secret regime; Section 3(k) of Indian Patents Act blocks AI invention patenting; copyright varies in authorship attribution. The study proposes harmonized legal taxonomy accommodating AI's role while preserving innovation incentives. India's National AI Strategy (2024) shows progress but legislative clarity is imperative. This contributes to global discourse with AI-specific IP protections ensuring resilience and equitable innovation. Promising results underscore recalibrating India's IP jurisprudence for global alignment.

</details>


### [497] [The Limits of AI Data Transparency Policy: Three Disclosure Fallacies](https://arxiv.org/abs/2601.18127)
*Judy Hanwen Shen,Ken Liu,Angelina Wang,Sarah H. Cen,Andy K. Zhang,Caroline Meinhardt,Daniel Zhang,Kevin Klyman,Rishi Bommasani,Daniel E. Ho*

Main category: cs.CY

TL;DR: 当前AI数据透明度政策常未达目标，分析指出三个常见谬误并给出有效途径。


<details>
  <summary>Details</summary>
Motivation: 解决AI在数据质量、隐私和版权方面问题，改善当前数据透明度政策未达目标的现状。

Method: 从制度视角出发，结合社会科学中关于透明度的研究进行分析。

Result: 识别出数据披露政策实施中的三个常见谬误，即规范差距、执行差距和影响差距。

Conclusion: 提出有效而非仅具象征意义的透明度实现途径。

Abstract: Data transparency has emerged as a rallying cry for addressing concerns about AI: data quality, privacy, and copyright chief among them. Yet while these calls are crucial for accountability, current transparency policies often fall short of their intended aims. Similar to nutrition facts for food, policies aimed at nutrition facts for AI currently suffer from a limited consideration of research on effective disclosures. We offer an institutional perspective and identify three common fallacies in policy implementations of data disclosures for AI. First, many data transparency proposals exhibit a specification gap between the stated goals of data transparency and the actual disclosures necessary to achieve such goals. Second, reform attempts exhibit an enforcement gap between required disclosures on paper and enforcement to ensure compliance in fact. Third, policy proposals manifest an impact gap between disclosed information and meaningful changes in developer practices and public understanding. Informed by the social science on transparency, our analysis identifies affirmative paths for transparency that are effective rather than merely symbolic.

</details>


### [498] [Beyond Pairwise Comparisons: A Distributional Test of Distinctiveness for Machine-Generated Works in Intellectual Property Law](https://arxiv.org/abs/2601.18156)
*Anirban Mukherjee,Hannah Hanwen Chang*

Main category: cs.CY

TL;DR: 文章指出传统分析创意作品独特性的方法存在单位分析不匹配问题，尤其对机器生成作品，提出基于语义嵌入最大均值差异的双样本测试法，在多领域验证有效，表明生成模型是语义插值器。


<details>
  <summary>Details</summary>
Motivation: 解决传统分析创意作品独特性时单位分析不匹配问题，特别是针对机器生成作品。

Method: 提出基于语义嵌入最大均值差异的双样本测试法，判断两个创意过程输出分布是否有统计学差异。

Result: 在三个领域验证该方法有效，人类评估者区分AI和人类艺术输出准确率约58%，而该方法能检测到分布独特性。

Conclusion: 生成模型并非简单重复训练数据，而是语义插值器，输出语义似人但随机不同。

Abstract: Key doctrines, including novelty (patent), originality (copyright), and distinctiveness (trademark), turn on a shared empirical question: whether a body of work is meaningfully distinct from a relevant reference class. Yet analyses typically operationalize this set-level inquiry using item-level evidence: pairwise comparisons among exemplars. That unit-of-analysis mismatch may be manageable for finite corpora of human-created works, where it can be bridged by ad hoc aggregations. But it becomes acute for machine-generated works, where the object of evaluation is not a fixed set of works but a generative process with an effectively unbounded output space. We propose a distributional alternative: a two-sample test based on maximum mean discrepancy computed on semantic embeddings to determine if two creative processes-whether human or machine-produce statistically distinguishable output distributions. The test requires no task-specific training-obviating the need for discovery of proprietary training data to characterize the generative process-and is sample-efficient, often detecting differences with as few as 5-10 images and 7-20 texts. We validate the framework across three domains: handwritten digits (controlled images), patent abstracts (text), and AI-generated art (real-world images). We reveal a perceptual paradox: even when human evaluators distinguish AI outputs from human-created art with only about 58% accuracy, our method detects distributional distinctiveness. Our results present evidence contrary to the view that generative models act as mere regurgitators of training data. Rather than producing outputs statistically indistinguishable from a human baseline-as simple regurgitation would predict-they produce outputs that are semantically human-like yet stochastically distinct, suggesting their dominant function is as a semantic interpolator within a learned latent space.

</details>


### [499] [Generative AI in Saudi Arabia: A National Survey of Adoption, Risks, and Public Perceptions](https://arxiv.org/abs/2601.18234)
*Abdulaziz AlDakheel,Ali Alshehre,Esraa Alamoudi,Moslim AlKhabbaz,Ahmed Aljohani,Raed Alharbi*

Main category: cs.CY

TL;DR: 本文通过对沙特国民的调查，呈现GenAI使用情况，发现使用普遍但认知不均，有担忧也有培训需求，为政策制定和开发指明方向。


<details>
  <summary>Details</summary>
Motivation: 沙特Vision 2030下GenAI快速发展，但公众对其认知、使用和担忧情况缺乏研究，需了解沙特国民对GenAI的参与情况。

Method: 对330名来自不同地区、年龄组和就业部门的参与者进行全国性调查，考察GenAI使用的七个维度。

Result: 93%受访者主要用于文本任务，先进应用较少；认知和理解不均，技术知识有限；认可好处但担忧影响思维和技能；对输出谨慎，关注隐私等问题；对结构化培训有强烈兴趣。

Conclusion: 为沙特GenAI参与情况建立基线，指出政策制定者和开发者应扩大AI素养、提供适配方案、加强隐私和负责任部署框架。

Abstract: Generative Artificial Intelligence (GenAI) is rapidly becoming embedded in Saudi Arabia's digital transformation under Vision 2030, yet public awareness, adoption, and concerns surrounding these tools remain underexplored. This study provides an early snapshot of GenAI engagement among Saudi nationals. Using a nationwide survey of 330 participants across regions, age groups, and employment sectors, we examine seven dimensions of GenAI use: awareness and understanding, adoption patterns, perceived impacts, training needs, risks and barriers, data-sharing behaviors, and future expectations. Findings show that 93% of respondents actively use GenAI primarily for text-based tasks, while more advanced uses such as programming or multimodal generation are less common. Despite the prevalence of use, overall awareness and conceptual understanding remain uneven, with many reporting limited technical knowledge. Participants recognize GenAI's benefits for productivity, work quality, and understanding complex information, yet caution that sustained reliance may undermine critical thinking and key professional skills. Trust in AI-generated outputs remains cautious, with widespread concerns about privacy, misinformation, and ethical misuse, including potential job displacement. Respondents show strong interest in structured GenAI training that combines foundational skills, domain-specific applications, and clear guidance on privacy, ethics, and responsible use. These results establish a baseline for GenAI engagement in Saudi Arabia and highlight priorities for policymakers and developers: expanding AI literacy, ensuring culturally and linguistically aligned GenAI solutions, and strengthening frameworks for privacy and responsible deployment.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [500] [Topological traps in evolutionary games](https://arxiv.org/abs/2601.17821)
*Jose Segovia-Martin*

Main category: physics.soc-ph

TL;DR: 本文用大规模蒙特卡罗模拟和自动聚类分类，研究二维空间囚徒困境中合作结构的成核率及对合作曲线的贡献，揭示高诱惑下残余合作是由少数拓扑陷阱主导的罕见事件成核现象。


<details>
  <summary>Details</summary>
Motivation: 前人未量化规范二维空间囚徒困境中合作结构的成核率及其对整个合作曲线的贡献，本文旨在解决该问题。

Method: 采用大规模蒙特卡罗模拟结合自动聚类分类的方法。

Result: 发现在Moore晶格上T≥5/3时残余合作由3×3或更大的矩形合作砖块维持，在度为8的随机正则图上T≳1.5时由星型图案主导；晶格在T = 5/3附近合作崩溃是动力学而非临界性的。

Conclusion: 高诱惑下残余合作是由少数拓扑陷阱主导的罕见事件成核现象，强调基序水平分析对解释和设计空间、社会和技术网络中合作的价值。

Abstract: How cooperation originates and persists among self-interested individuals is a central question in the social and behavioural sciences. In the canonical two-dimensional spatial Prisoner's Dilemma with unconditional imitation introduced by Nowak and May (1992), simulations on a Moore lattice show an abrupt drop in cooperation near the temptation $T\approx5/3$, yet even under these harsh conditions cooperative structures can still arise. However, the nucleation rates of these motifs, and their contribution along the full cooperation curve had not been quantified. Here we show, using large-scale Monte Carlo simulations combined with automatic cluster classification, that on the Moore lattice for $T\ge5/3$ residual cooperation is sustained exclusively by $3\times3$ (or larger) rectangular cooperator bricks, whereas on degree-8 random-regular graphs for $T\gtrsim1.5$ it is dominated by star-like motifs (1 hub + 8 leaves). Once the dynamics becomes nucleation limited, the macroscopic cooperation level is therefore governed by the statistics of a few exceptionally resilient shapes, rather than by many different cooperator motifs. Furthermore, we show that the lattice cooperation collapse near $T=5/3$ is kinetic rather than critical: the reduction in cooperation is not due to a loss of growth capacity of rectangular bricks, but to the progressive destabilisation of the subcritical motifs that dominate just below this threshold. Our results show that residual cooperation at high temptation is a rare-event nucleation phenomenon governed by a small set of topological traps, and highlight the value of motif-level analysis for explaining and engineering cooperation in spatial, social, and technological networks.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [501] [An Adaptive Purification Controller for Quantum Networks: Dynamic Protocol Selection and Multipartite Distillation](https://arxiv.org/abs/2601.18351)
*Pranav Kulkarni,Leo Sünkel,Michael Kölle*

Main category: quant-ph

TL;DR: 提出自适应净化控制器（APC）优化纠缠蒸馏序列，最大化好吞吐量，模拟显示其消除静态协议缺陷，扩展到异构场景并证实实时可行性。


<details>
  <summary>Details</summary>
Motivation: 物理链路参数动态波动使静态净化策略非最优，需要高效的纠缠分发方案。

Method: 将协议选择视为资源分配问题，用动态规划规划器和帕累托剪枝，APC动态切换净化深度和协议族。

Result: 消除静态协议的“保真度悬崖”，防止高噪声下资源浪费，扩展到异构场景可行，计算开销低，决策延迟毫秒级。

Conclusion: APC能有效优化纠缠蒸馏序列，在不同场景有良好表现且具备实时可行性。

Abstract: Efficient entanglement distribution is the cornerstone of the Quantum Internet. However, physical link parameters such as photon loss, memory coherence time, and gate error rates fluctuate dynamically, rendering static purification strategies suboptimal. In this paper, we propose an Adaptive Purification Controller (APC) that autonomously optimizes the entanglement distillation sequence to maximize the "goodput," the rate of delivered pairs meeting a strict fidelity threshold. By treating protocol selection as a resource allocation problem, the APC dynamically switches between purification depths and protocol families (e.g., BBPSSW vs. DEJMPS) to navigate the trade-off between generation rate and state quality. Using a dynamic programming planner with Pareto pruning, simulation results demonstrate that our approach eliminates the "fidelity cliffs" inherent in static protocols and prevents resource wastage in high-noise regimes. Furthermore, we extend the controller to heterogeneous scenarios, demonstrating robustness for both multipartite GHZ state generation and continuous variable systems using effective noiseless linear amplification models. We benchmark its computational overhead, confirming real-time feasibility with decision latencies in the millisecond range per link.

</details>


### [502] [Universality of Many-body Projected Ensemble for Learning Quantum Data Distribution](https://arxiv.org/abs/2601.18637)
*Quoc Hoan Tran,Koki Chinzei,Yasuhiro Endo,Hirotaka Oshima*

Main category: quant-ph

TL;DR: 本文探讨量子数据生成问题，证明MPE框架的普遍性定理，提出增量MPE变体并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 量子数据生成有挑战，量子机器学习中存在近似普遍性的问题待解决。

Method: 证明Many - body Projected Ensemble (MPE)框架的普遍性定理，提出增量MPE变体并进行层状训练。

Result: MPE能在1 - Wasserstein距离误差内近似任何纯态分布，数值实验验证MPE学习复杂量子数据分布的有效性。

Conclusion: 普遍性定理为量子机器学习提供严格保证，弥补理论缺口，增量MPE提升了实用性。

Abstract: Generating quantum data by learning the underlying quantum distribution poses challenges in both theoretical and practical scenarios, yet it is a critical task for understanding quantum systems. A fundamental question in quantum machine learning (QML) is the universality of approximation: whether a parameterized QML model can approximate any quantum distribution. We address this question by proving a universality theorem for the Many-body Projected Ensemble (MPE) framework, a method for quantum state design that uses a single many-body wave function to prepare random states. This demonstrates that MPE can approximate any distribution of pure states within a 1-Wasserstein distance error. This theorem provides a rigorous guarantee of universal expressivity, addressing key theoretical gaps in QML. For practicality, we propose an Incremental MPE variant with layer-wise training to improve the trainability. Numerical experiments on clustered quantum states and quantum chemistry datasets validate MPE's efficacy in learning complex quantum data distributions.

</details>


### [503] [Bayesian quantum sensing using graybox machine learning](https://arxiv.org/abs/2601.17465)
*Akram Youssry,Stefan Todd,Patrick Murton,Muhammad Junaid Arshad,Alberto Peruzzo,Cristian Bonato*

Main category: quant-ph

TL;DR: 本文报道固态开放量子系统灰盒建模策略的首次实验实现，在单自旋量子传感器测磁场任务中验证，结果对多种量子传感平台有广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 量子传感器实际性能受未建模效应限制，需更好的建模策略。

Method: 采用灰盒建模策略，将基于物理的系统模型与数据驱动的实验缺陷描述相结合，用贝叶斯推理和先验实验数据训练模型。

Result: 约10000个训练数据点下，灰盒模型均方误差相比纯物理模型有数量级改善。

Conclusion: 结果适用于多种量子传感平台，对实时自适应协议有重要价值。

Abstract: Quantum sensors offer significant advantages over classical devices in spatial resolution and sensitivity, enabling transformative applications across materials science, healthcare, and beyond. Their practical performance, however, is often constrained by unmodelled effects, including noise, imperfect state preparation, and non-ideal control fields.
  In this work, we report the first experimental implementation of a graybox modelling strategy for a solid-state open quantum system. The graybox framework integrates a physics-based system model with a data-driven description of experimental imperfections, achieving higher fidelity than purely analytical (whitebox) approaches while requiring fewer training resources than fully deep-learning models. We experimentally validate the method on the task of estimating a static magnetic field using a single-spin quantum sensor, performing Bayesian inference with a graybox model trained on prior experimental data. Using roughly 10,000 training datapoints, the graybox model yields several orders of magnitude improvement in mean squared error over the corresponding physics-only model. These results are broadly applicable to a wide range of quantum sensing platforms, not limited to single-spin systems, and are particularly valuable for real-time adaptive protocols, where model inaccuracies can otherwise lead to suboptimal control and degraded performance.

</details>


### [504] [Emergent Cooperation in Quantum Multi-Agent Reinforcement Learning Using Communication](https://arxiv.org/abs/2601.18419)
*Michael Kölle,Christian Reff,Leo Sünkel,Julian Hager,Gerhard Stenzel,Claudia Linnhoff-Popien*

Main category: quant-ph

TL;DR: 本文将通信方法应用于量子Q学习智能体，在三种顺序社会困境中评估，结果表明部分方法能在量子多智能体强化学习中实现高合作水平。


<details>
  <summary>Details</summary>
Motivation: 经典多智能体强化学习中涌现合作受关注，但将相关方法扩展到量子多智能体强化学习尤其是通过通信实现的研究有限。

Method: 将MATE、MEDIATE、Gifting和RIAL等通信方法应用于量子Q学习智能体，并在迭代囚徒困境、迭代猎鹿博弈和迭代斗鸡博弈三种顺序社会困境中评估。

Result: 使用MATE与时间差分测量（MATEₜₙ）、AutoMATE、MEDIATE - I和MEDIATE - S的方法在所有困境中都实现了高合作水平。

Conclusion: 通信是促进量子多智能体强化学习中涌现合作的可行机制。

Abstract: Emergent cooperation in classical Multi-Agent Reinforcement Learning has gained significant attention, particularly in the context of Sequential Social Dilemmas (SSDs). While classical reinforcement learning approaches have demonstrated capability for emergent cooperation, research on extending these methods to Quantum Multi-Agent Reinforcement Learning remains limited, particularly through communication. In this paper, we apply communication approaches to quantum Q-Learning agents: the Mutual Acknowledgment Token Exchange (MATE) protocol, its extension Mutually Endorsed Distributed Incentive Acknowledgment Token Exchange (MEDIATE), the peer rewarding mechanism Gifting, and Reinforced Inter-Agent Learning (RIAL). We evaluate these approaches in three SSDs: the Iterated Prisoner's Dilemma, Iterated Stag Hunt, and Iterated Game of Chicken. Our experimental results show that approaches using MATE with temporal-difference measure (MATE\textsubscript{TD}), AutoMATE, MEDIATE-I, and MEDIATE-S achieved high cooperation levels across all dilemmas, demonstrating that communication is a viable mechanism for fostering emergent cooperation in Quantum Multi-Agent Reinforcement Learning.

</details>


### [505] [Data-Driven Qubit Characterization and Optimal Control using Deep Learning](https://arxiv.org/abs/2601.18704)
*Paul Surrey,Julian D. Teske,Tobias Hangleiter,Hendrik Bluhm,Pascal Cerfontaine*

Main category: quant-ph

TL;DR: 提出基于机器学习的协议，通过训练RNN优化量子控制脉冲，并在单 $ST_0$ 量子比特上模拟验证。


<details>
  <summary>Details</summary>
Motivation: 解决量子计算中控制脉冲优化时评估梯度和建模复杂系统动力学的挑战。

Method: 用随机控制脉冲采样量子比特动力学，训练RNN预测量子比特行为，用训练好的模型优化控制脉冲。

Result: 在单 $ST_0$ 量子比特上的模拟证明了该方法的有效性。

Conclusion: 基于机器学习的协议可在无需详细系统模型的情况下实现高效的基于梯度的脉冲优化。

Abstract: Quantum computing requires the optimization of control pulses to achieve high-fidelity quantum gates. We propose a machine learning-based protocol to address the challenges of evaluating gradients and modeling complex system dynamics. By training a recurrent neural network (RNN) to predict qubit behavior, our approach enables efficient gradient-based pulse optimization without the need for a detailed system model. First, we sample qubit dynamics using random control pulses with weak prior assumptions. We then train the RNN on the system's observed responses, and use the trained model to optimize high-fidelity control pulses. We demonstrate the effectiveness of this approach through simulations on a single $ST_0$ qubit.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [506] [Improving Generalization and Uncertainty Quantification of Photometric Redshift Models](https://arxiv.org/abs/2601.17222)
*Jonathan Soriano,Tuan Do,Srinath Saikrishnan,Vikram Seenivasan,Bernie Boscoe,Jack Singal,Evan Jones*

Main category: astro-ph.IM

TL;DR: 本文探索提高机器学习模型在更广星系类型上进行测光红移估计适用性的方法，测试两种结合光谱红移和多波段测光红移的方法，发现复合数据集训练的NN效果更好，BNN能产生可靠不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 准确的红移估计对理解星系演化和精确宇宙学至关重要，需提高机器学习模型在更广星系类型上进行测光红移估计的适用性。

Method: 测试两种结合光谱红移和多波段测光红移的方法（复合数据集训练和迁移学习），使用确定性神经网络（NN）和贝叶斯神经网络（BNN），用分裂共形预测校准不确定性估计。

Result: 复合数据集训练的NN在0.3<z<1.5红移范围内预测的测光红移偏差低4.5倍、离散度低1.1倍、离群率低1.4倍；BNN能产生可靠不确定性估计，但对不同真值敏感。

Conclusion: 利用不同真值来源开发的模型可准确预测更多星系的测光红移，对Euclid和LSST等调查至关重要。

Abstract: Accurate redshift estimates are a vital component in understanding galaxy evolution and precision cosmology. In this paper, we explore approaches to increase the applicability of machine learning models for photometric redshift estimation on a broader range of galaxy types. Typical models are trained with ground-truth redshifts from spectroscopy. We test the utility and effectiveness of two approaches for combining spectroscopic redshifts and redshifts derived from multiband ($\sim$35 filters) photometry, which sample different types of galaxies compared to spectroscopic surveys. The two approaches are (1) training on a composite dataset and (2) transfer learning from one dataset to another. We compile photometric redshifts from the COSMOS2020 catalog (TransferZ) to complement an established spectroscopic redshift dataset (GalaxiesML). We used two architectures, deterministic neural networks (NN) and Bayesian neural networks (BNN), to examine and evaluate their performance with respect to the Legacy Survey of Space and Time (LSST) photo-$z$ science requirements. We also use split conformal prediction for calibrating uncertainty estimates and producing prediction intervals for the BNN and NN, respectively. We find that a NN trained on a composite dataset predicts photo-$z$'s that are 4.5 times less biased within the redshift range $0.3<z<1.5$, 1.1 times less scattered, and has a 1.4 times lower outlier rate than a model trained on only spectroscopic ground truths. We also find that BNNs produce reliable uncertainty estimates, but are sensitive to the different ground truths. This investigation leverages different sources of ground truths to develop models that can accurately predict photo-$z$'s for a broader population of galaxies crucial for surveys such as Euclid and LSST.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [507] [An autonomous living database for perovskite photovoltaics](https://arxiv.org/abs/2601.17807)
*Sherjeel Shabih,Hampus Näsström,Sharat Patil,Asmin Askin,Keely Dodd-Clements,Jessica Helisa Hautrive Rossato,Hugo Gajardoni de Lemos,Yuxin Liu,Florian Mathies,Natalia Maticiuc,Rico Meitzner,Edgar Nandayapa,Juan José Patiño López,Yaru Wang,Lauri Himanen,Eva Unger,T. Jesper Jacobsson,José A. Márquez,Kevin Maik Jablonka*

Main category: cond-mat.mtrl-sci

TL;DR: 论文介绍了自主更新的数据库PERLA解决光伏领域数据整理难题，挖掘数据背后趋势，将静态文献转化为动态知识资源。


<details>
  <summary>Details</summary>
Motivation: 手动整理无法跟上论文发表速度，导致知识差距扩大，光伏领域钙钛矿太阳能电池数据库自2021年以来停滞。

Method: 建立自主、自更新的数据库PERLA，将大语言模型与物理感知验证相结合，从连续文献流中提取复杂设备数据。

Result: 达到人类水平的精度（>90%），消除注释者差异，揭示了2021年后文献中隐藏的关键进化趋势。

Conclusion: PERLA将静态出版物转化为动态知识资源，使数据驱动的发现能与论文发表速度同步。

Abstract: Scientific discovery is severely bottlenecked by the inability of manual curation to keep pace with exponential publication rates. This creates a widening knowledge gap. This is especially stark in photovoltaics, where the leading database for perovskite solar cells has been stagnant since 2021 despite massive ongoing research output. Here, we resolve this challenge by establishing an autonomous, self-updating living database (PERLA). Our pipeline integrates large language models with physics-aware validation to extract complex device data from the continuous literature stream, achieving human-level precision (>90%) and eliminating annotator variance. By employing this system on the previously inaccessible post-2021 literature, we uncover critical evolutionary trends hidden by data lag: the field has decisively shifted toward inverted architectures employing self-assembled monolayers and formamidinium-rich compositions, driving a clear trajectory of sustained voltage loss reduction. PERLA transforms static publications into dynamic knowledge resources that enable data-driven discovery to operate at the speed of publication.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [508] [GenAI-Net: A Generative AI Framework for Automated Biomolecular Network Design](https://arxiv.org/abs/2601.17582)
*Maurice Filo,Nicolò Rossi,Zhou Fang,Mustafa Khammash*

Main category: q-bio.QM

TL;DR: 介绍GenAI - Net框架用于自动化化学反响网络（CRN）设计，可高效生成多样解决方案，加速可编程生物分子电路设计。


<details>
  <summary>Details</summary>
Motivation: 当前化学反响网络设计主要依赖人工，从行为规格发现网络困难，需大量人力在复杂空间中探索。

Method: 引入GenAI - Net框架，将提出反响的代理与基于模拟的评估相耦合，评估由用户指定目标定义。

Result: GenAI - Net能在多个设计任务中高效生成新颖、拓扑多样的解决方案，包括剂量响应、复杂逻辑门等。

Conclusion: GenAI - Net为可编程生物分子电路设计提供通用途径，加速从期望功能到可实现机制的转化。

Abstract: Biomolecular networks underpin emerging technologies in synthetic biology-from robust biomanufacturing and metabolic engineering to smart therapeutics and cell-based diagnostics-and also provide a mechanistic language for understanding complex dynamics in natural and ecological systems. Yet designing chemical reaction networks (CRNs) that implement a desired dynamical function remains largely manual: while a proposed network can be checked by simulation, the reverse problem of discovering a network from a behavioral specification is difficult, requiring substantial human insight to navigate a vast space of topologies and kinetic parameters with nonlinear and possibly stochastic dynamics. Here we introduce GenAI-Net, a generative AI framework that automates CRN design by coupling an agent that proposes reactions to simulation-based evaluation defined by a user-specified objective. GenAI-Net efficiently produces novel, topologically diverse solutions across multiple design tasks, including dose responses, complex logic gates, classifiers, oscillators, and robust perfect adaptation in deterministic and stochastic settings (including noise reduction). By turning specifications into families of circuit candidates and reusable motifs, GenAI-Net provides a general route to programmable biomolecular circuit design and accelerates the translation from desired function to implementable mechanisms.

</details>


### [509] [Point transformer for protein structural heterogeneity analysis using CryoEM](https://arxiv.org/abs/2601.18713)
*Muyuan Chen,Muchen Li,Renjie Liao*

Main category: q-bio.QM

TL;DR: 运用Point Transformer改进冷冻电镜数据异质性分析，以更易理解方式表征复杂蛋白系统动力学。


<details>
  <summary>Details</summary>
Motivation: 对于多自由度蛋白系统，解析和解释不同动力学模式仍具挑战。

Method: 实施用于点云分析的自注意力网络Point Transformer。

Result: 提高了冷冻电镜数据异质性分析的性能。

Conclusion: 能够以更易理解的方式表征高度复杂蛋白系统的动力学。

Abstract: Structural dynamics of macromolecules is critical to their structural-function relationship. Cryogenic electron microscopy (CryoEM) provides snapshots of vitrified protein at different compositional and conformational states, and the structural heterogeneity of proteins can be characterized through computational analysis of the images. For protein systems with multiple degrees of freedom, it is still challenging to disentangle and interpret the different modes of dynamics. Here, by implementing Point Transformer, a self-attention network designed for point cloud analysis, we are able to improve the performance of heterogeneity analysis on CryoEM data, and characterize the dynamics of highly complex protein systems in a more human-interpretable way.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [510] [Feature-Space Generative Models for One-Shot Class-Incremental Learning](https://arxiv.org/abs/2601.17905)
*Jack Foster,Kirill Paramonov,Mete Ozay,Umberto Michieli*

Main category: cs.CV

TL;DR: 提出Gen1S方法处理单样本少样本增量学习问题，在多个基准和架构上提升了新类识别能力。


<details>
  <summary>Details</summary>
Motivation: 解决单样本少样本增量学习中，基础训练后无法再训练模型，新类泛化困难的问题。

Method: 假设基础类和新类嵌入有结构相似性，将原嵌入空间映射到残差空间，用VAE或扩散模型学习基础类残差的多模态分布作为先验，提升新类识别。

Result: Gen1S方法在多个基准和骨干架构上，相比现有技术持续提升了新类识别能力。

Conclusion: 所提基于残差空间和生成建模的Gen1S方法有效，能解决单样本少样本增量学习新类识别难题。

Abstract: Few-shot class-incremental learning (FSCIL) is a paradigm where a model, initially trained on a dataset of base classes, must adapt to an expanding problem space by recognizing novel classes with limited data. We focus on the challenging FSCIL setup where a model receives only a single sample (1-shot) for each novel class and no further training or model alterations are allowed after the base training phase. This makes generalization to novel classes particularly difficult. We propose a novel approach predicated on the hypothesis that base and novel class embeddings have structural similarity. We map the original embedding space into a residual space by subtracting the class prototype (i.e., the average class embedding) of input samples. Then, we leverage generative modeling with VAE or diffusion models to learn the multi-modal distribution of residuals over the base classes, and we use this as a valuable structural prior to improve recognition of novel classes. Our approach, Gen1S, consistently improves novel class recognition over the state of the art across multiple benchmarks and backbone architectures.

</details>


### [511] [Co-PLNet: A Collaborative Point-Line Network for Prompt-Guided Wireframe Parsing](https://arxiv.org/abs/2601.18252)
*Chao Wang,Xuanying Li,Cheng Dai,Jinglei Feng,Yuxiang Luo,Yuqi Ouyang,Hao Qin*

Main category: cs.CV

TL;DR: 提出Co - PLNet点线协作框架用于线框解析，在多个数据集实验显示有准确性、鲁棒性提升和实时效率。


<details>
  <summary>Details</summary>
Motivation: 现有线框解析方法分别预测线条和交点并事后协调，会导致不匹配和鲁棒性降低。

Method: 提出Co - PLNet框架，通过点线提示编码器将早期检测转换为空间提示，交叉引导线解码器利用稀疏注意力结合互补提示细化预测，增强点线一致性和效率。

Result: 在Wireframe和YorkUrban数据集上实验，准确性、鲁棒性持续提升，有良好实时效率。

Conclusion: Co - PLNet框架对结构化几何感知有效。

Abstract: Wireframe parsing aims to recover line segments and their junctions to form a structured geometric representation useful for downstream tasks such as Simultaneous Localization and Mapping (SLAM). Existing methods predict lines and junctions separately and reconcile them post-hoc, causing mismatches and reduced robustness. We present Co-PLNet, a point-line collaborative framework that exchanges spatial cues between the two tasks, where early detections are converted into spatial prompts via a Point-Line Prompt Encoder (PLP-Encoder), which encodes geometric attributes into compact and spatially aligned maps. A Cross-Guidance Line Decoder (CGL-Decoder) then refines predictions with sparse attention conditioned on complementary prompts, enforcing point-line consistency and efficiency. Experiments on Wireframe and YorkUrban show consistent improvements in accuracy and robustness, together with favorable real-time efficiency, demonstrating our effectiveness for structured geometry perception.

</details>


### [512] [Scientific Image Synthesis: Benchmarking, Methodologies, and Downstream Utility](https://arxiv.org/abs/2601.17027)
*Honglin Lin,Chonghan Qin,Zheng Liu,Qizhi Pei,Yu Li,Zhanping Zhong,Xin Gao,Yanfeng Wang,Conghui He,Lijun Wu*

Main category: cs.CV

TL;DR: 本文系统研究科学图像合成，提出ImgCoder框架和SciGenBench评估基准，发现像素模型问题，验证高保真合成对提升多模态推理能力有效。


<details>
  <summary>Details</summary>
Motivation: 现有T2I模型生成的科学图像存在视觉与逻辑不符问题，限制下游推理，受下一代T2I模型进展启发开展研究。

Method: 分析基于像素的直接生成和程序化合成方法，提出逻辑驱动的ImgCoder框架；引入SciGenBench评估基准。

Result: 发现像素模型存在系统性失败模式和表现力 - 精度权衡问题；在严格验证的合成科学图像上微调大模型可提升推理能力。

Conclusion: 高保真科学图像合成是解锁大规模多模态推理能力的可行途径。

Abstract: While synthetic data has proven effective for improving scientific reasoning in the text domain, multimodal reasoning remains constrained by the difficulty of synthesizing scientifically rigorous images. Existing Text-to-Image (T2I) models often produce outputs that are visually plausible yet scientifically incorrect, resulting in a persistent visual-logic divergence that limits their value for downstream reasoning. Motivated by recent advances in next-generation T2I models, we conduct a systematic study of scientific image synthesis across generation paradigms, evaluation, and downstream use. We analyze both direct pixel-based generation and programmatic synthesis, and propose ImgCoder, a logic-driven framework that follows an explicit "understand - plan - code" workflow to improve structural precision. To rigorously assess scientific correctness, we introduce SciGenBench, which evaluates generated images based on information utility and logical validity. Our evaluation reveals systematic failure modes in pixel-based models and highlights a fundamental expressiveness-precision trade-off. Finally, we show that fine-tuning Large Multimodal Models (LMMs) on rigorously verified synthetic scientific images yields consistent reasoning gains, with potential scaling trends analogous to the text domain, validating high-fidelity scientific synthesis as a viable path to unlocking massive multimodal reasoning capabilities.

</details>


### [513] [Data-Efficient Meningioma Segmentation via Implicit Spatiotemporal Mixing and Sim2Real Semantic Injection](https://arxiv.org/abs/2601.17031)
*Yunhao Xu,Fuquan Zong,Yexuan Xing,Chulong Zhang,Guang Yang,Shilong Yang,Xiaokun Liang,Juan Yu*

Main category: cs.CV

TL;DR: 本文提出双增强框架提升医学图像分割数据利用效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割性能取决于数据利用效率，准确分割复杂病症需充分挖掘有限高质量标注数据的潜在信息。

Method: 提出双增强框架，结合空间流形扩展和语义对象注入，用隐式神经表征（INR）建模连续速度场，对集成变形场进行线性混合，并引入Sim2Real病变注入模块。

Result: 在混合数据集上实验表明，该框架显著提高了现有先进模型的数据效率和鲁棒性。

Conclusion: 该框架为有限标注预算下的高性能医学图像分析提供了有效策略。

Abstract: The performance of medical image segmentation is increasingly defined by the efficiency of data utilization rather than merely the volume of raw data. Accurate segmentation, particularly for complex pathologies like meningiomas, demands that models fully exploit the latent information within limited high-quality annotations. To maximize the value of existing datasets, we propose a novel dual-augmentation framework that synergistically integrates spatial manifold expansion and semantic object injection. Specifically, we leverage Implicit Neural Representations (INR) to model continuous velocity fields. Unlike previous methods, we perform linear mixing on the integrated deformation fields, enabling the efficient generation of anatomically plausible variations by interpolating within the deformation space. This approach allows for the extensive exploration of structural diversity from a small set of anchors. Furthermore, we introduce a Sim2Real lesion injection module. This module constructs a high-fidelity simulation domain by transplanting lesion textures into healthy anatomical backgrounds, effectively bridging the gap between synthetic augmentation and real-world pathology. Comprehensive experiments on a hybrid dataset demonstrate that our framework significantly enhances the data efficiency and robustness of state-of-the-art models, including nnU-Net and U-Mamba, offering a potent strategy for high-performance medical image analysis with limited annotation budgets.

</details>


### [514] [AMVICC: A Novel Benchmark for Cross-Modal Failure Mode Profiling for VLMs and IGMs](https://arxiv.org/abs/2601.17037)
*Aahana Basappa,Pranay Goel,Anusri Karra,Anish Karra,Asa Gilmore,Kevin Zhu*

Main category: cs.CV

TL;DR: 本文通过创建新基准分析多模态大语言模型和图像生成模型视觉推理局限性，揭示故障模式规律，为跨模态对齐研究奠基。


<details>
  <summary>Details</summary>
Motivation: 尽管机器学习发展迅速，视觉语言模型仍存在基本视觉概念理解和生成问题，需系统性评测模型视觉理解能力。

Method: 创建新颖基准<textit>AMVICC</textit>，通过适配MMVP基准问题为显式和隐式提示，对11个MLLMs和3个IGMs进行九类视觉推理测试。

Result: 不同模型和模态间故障模式常具共性，但也存在特定故障；IGMs在显式提示下难控制细粒度视觉属性。

Conclusion: 研究结果适用于结构化视觉推理任务模型评估，为跨模态对齐研究提供框架，指导统一视觉 - 语言建模改进。

Abstract: We investigated visual reasoning limitations of both multimodal large language models (MLLMs) and image generation models (IGMs) by creating a novel benchmark to systematically compare failure modes across image-to-text and text-to-image tasks, enabling cross-modal evaluation of visual understanding. Despite rapid growth in machine learning, vision language models (VLMs) still fail to understand or generate basic visual concepts such as object orientation, quantity, or spatial relationships, which highlighted gaps in elementary visual reasoning. By adapting MMVP benchmark questions into explicit and implicit prompts, we create \textit{AMVICC}, a novel benchmark for profiling failure modes across various modalities. After testing 11 MLLMs and 3 IGMs in nine categories of visual reasoning, our results show that failure modes are often shared between models and modalities, but certain failures are model-specific and modality-specific, and this can potentially be attributed to various factors. IGMs consistently struggled to manipulate specific visual components in response to prompts, especially in explicit prompts, suggesting poor control over fine-grained visual attributes. Our findings apply most directly to the evaluation of existing state-of-the-art models on structured visual reasoning tasks. This work lays the foundation for future cross-modal alignment studies, offering a framework to probe whether generation and interpretation failures stem from shared limitations to guide future improvements in unified vision-language modeling.

</details>


### [515] [MANGO: A Global Single-Date Paired Dataset for Mangrove Segmentation](https://arxiv.org/abs/2601.17039)
*Junhyuk Heo,Beomkyu Choi,Hyunjin Shin,Darongsae Kwon*

Main category: cs.CV

TL;DR: 本文介绍了大规模全球红树林数据集MANGO，可推动全球红树林监测。


<details>
  <summary>Details</summary>
Motivation: 现有红树林检测数据集存在无单日期图像-掩码对、覆盖区域有限、无法公开获取等问题，阻碍深度学习在红树林检测中的发展。

Method: 检索2020年红树林区域的Sentinel - 2图像，用目标检测驱动法选择单日期图像并与年度掩码对齐，构建MANGO数据集，还在不同语义分割架构上进行了国家不相交分割的基准测试。

Result: 构建了包含124个国家42,703个标记图像-掩码对的MANGO数据集。

Conclusion: MANGO数据集为可扩展且可靠的全球红树林监测奠定了基础。

Abstract: Mangroves are critical for climate-change mitigation, requiring reliable monitoring for effective conservation. While deep learning has emerged as a powerful tool for mangrove detection, its progress is hindered by the limitations of existing datasets. In particular, many resources provide only annual map products without curated single-date image-mask pairs, limited to specific regions rather than global coverage, or remain inaccessible to the public. To address these challenges, we introduce MANGO, a large-scale global dataset comprising 42,703 labeled image-mask pairs across 124 countries. To construct this dataset, we retrieve all available Sentinel-2 imagery within the year 2020 for mangrove regions and select the best single-date observations that align with the mangrove annual mask. This selection is performed using a target detection-driven approach that leverages pixel-wise coordinate references to ensure adaptive and representative image-mask pairings. We also provide a benchmark across diverse semantic segmentation architectures under a country-disjoint split, establishing a foundation for scalable and reliable global mangrove monitoring.

</details>


### [516] [FP-THD: Full page transcription of historical documents](https://arxiv.org/abs/2601.17040)
*H Neji,J Nogueras-Iso,J Lacasta,MÁ Latre,FJ García-Marco*

Main category: cs.CV

TL;DR: 本文提出保留特殊特征的历史文献转录管道，扩展现有方法，经多数据集评估有效。


<details>
  <summary>Details</summary>
Motivation: 转录15和16世纪拉丁文历史文献需保留特殊字符和符号，以维持原文风格和意义，存在挑战。

Method: 提出用布局分析模型扩展现有文本行识别方法，先分析历史文本图像提取文本行，再用OCR模型生成数字化页面。

Result: 管道便于页面处理，结果高效；经多数据集评估，掩码自编码器能有效处理不同类型文本。

Conclusion: 所提管道可有效转录保留特殊特征的历史文献。

Abstract: The transcription of historical documents written in Latin in XV and XVI centuries has special challenges as it must maintain the characters and special symbols that have distinct meanings to ensure that historical texts retain their original style and significance. This work proposes a pipeline for the transcription of historical documents preserving these special features. We propose to extend an existing text line recognition method with a layout analysis model. We analyze historical text images using a layout analysis model to extract text lines, which are then processed by an OCR model to generate a fully digitized page. We showed that our pipeline facilitates the processing of the page and produces an efficient result. We evaluated our approach on multiple datasets and demonstrate that the masked autoencoder effectively processes different types of text, including handwritten, printed and multi-language.

</details>


### [517] [Arabic Sign Language Recognition using Multimodal Approach](https://arxiv.org/abs/2601.17041)
*Ghadeer Alanazi,Abir Benabid*

Main category: cs.CV

TL;DR: 研究结合Leap Motion和RGB相机数据的多模态方法识别阿拉伯手语，系统评估准确率78%，为手语识别多模态融合提供初步见解。


<details>
  <summary>Details</summary>
Motivation: 现有阿拉伯手语识别系统依赖单一传感器，存在复杂手势跟踪和3D动作识别不准确的问题，因此研究多模态方法的可行性。

Method: 构建包含自定义密集神经网络和微调VGG16模型的并行子网络，融合两种模态特征，通过全连接层和SoftMax激活进行分类。

Result: 在包含18个阿拉伯手语单词的自定义数据集上评估，正确识别13个，总体准确率78%。

Conclusion: 多模态融合用于手语识别具有可行性，需进一步优化和扩展数据集。

Abstract: Arabic Sign Language (ArSL) is an essential communication method for individuals in the Deaf and Hard-of-Hearing community. However, existing recognition systems face significant challenges due to their reliance on single sensor approaches like Leap Motion or RGB cameras. These systems struggle with limitations such as inadequate tracking of complex hand orientations and imprecise recognition of 3D hand movements. This research paper aims to investigate the potential of a multimodal approach that combines Leap Motion and RGB camera data to explore the feasibility of recognition of ArSL. The system architecture includes two parallel subnetworks: a custom dense neural network for Leap Motion data, incorporating dropout and L2 regularization, and an image subnetwork based on a fine-tuned VGG16 model enhanced with data augmentation techniques. Feature representations from both modalities are concatenated in a fusion model and passed through fully connected layers, with final classification performed via SoftMax activation to analyze spatial and temporal features of hand gestures. The system was evaluated on a custom dataset comprising 18 ArSL words, of which 13 were correctly recognized, yielding an overall accuracy of 78%. These results offer preliminary insights into the viability of multimodal fusion for sign language recognition and highlight areas for further optimization and dataset expansion.

</details>


### [518] [Interpretable and Sparse Linear Attention with Decoupled Membership-Subspace Modeling via MCR2 Objective](https://arxiv.org/abs/2601.17042)
*Tianyuan Liu,Libin Hou,Linyuan Wang,Bin Yan*

Main category: cs.CV

TL;DR: 本文提出解耦成员 - 子空间注意力（DMSA）以解决现有MCR2驱动白盒Transformer的问题，实验表明其在图像任务上提升了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有MCR2设计中“成员矩阵”和“子空间矩阵U”的紧密耦合导致在错误令牌投影下出现冗余编码问题。

Method: 解耦MCR2目标中“成员矩阵”和“子空间U”的函数关系，从优化目标的展开梯度下降中推导可解释的稀疏线性注意力算子，直接从输入学习成员矩阵并从全空间推导稀疏子空间。

Result: 在视觉任务上，用DMSA替换Token Statistics Transformer中的注意力模块（DMST）编码缩减率更快，在ImageNet - 1K数据集上top - 1准确率比ToST高1.08% - 1.45%，与普通Transformer架构相比，计算效率和可解释性显著提高。

Conclusion: 所提出的DMSA是一种有效的解决方案，能提升视觉建模的效率和可解释性。

Abstract: Maximal Coding Rate Reduction (MCR2)-driven white-box transformer, grounded in structured representation learning, unifies interpretability and efficiency, providing a reliable white-box solution for visual modeling. However, in existing designs, tight coupling between "membership matrix" and "subspace matrix U" in MCR2 causes redundant coding under incorrect token projection. To this end, we decouple the functional relationship between the "membership matrix" and "subspaces U" in the MCR2 objective and derive an interpretable sparse linear attention operator from unrolled gradient descent of the optimized objective. Specifically, we propose to directly learn the membership matrix from inputs and subsequently derive sparse subspaces from the fullspace S. Consequently, gradient unrolling of the optimized MCR2 objective yields an interpretable sparse linear attention operator: Decoupled Membership-Subspace Attention (DMSA). Experimental results on visual tasks show that simply replacing the attention module in Token Statistics Transformer (ToST) with DMSA (we refer to as DMST) not only achieves a faster coding reduction rate but also outperforms ToST by 1.08%-1.45% in top-1 accuracy on the ImageNet-1K dataset. Compared with vanilla Transformer architectures, DMST exhibits significantly higher computational efficiency and interpretability.

</details>


### [519] [SiMiC: Context-Aware Silicon Microstructure Characterization Using Attention-Based Convolutional Neural Networks for Field-Emission Tip Analysis](https://arxiv.org/abs/2601.17048)
*Jing Jie Tan,Rupert Schreiner,Matthias Hausladen,Ali Asgharzade,Simon Edler,Julian Bartsch,Michael Bachmann,Andreas Schels,Ban-Hoe Kwan,Danny Wee-Kiat Ng,Yan-Chai Hum*

Main category: cs.CV

TL;DR: 本文提出SiMiC方法，使用基于注意力的卷积神经网络对场发射尖端进行分析，能高效从SEM图像中提取特征，与经典技术对比准确性高，还建立了数据驱动的微观结构分析基础。


<details>
  <summary>Details</summary>
Motivation: 传统扫描电子显微镜（SEM）分析硅微结构需人工评估特征几何形状，通量和可重复性受限，需要更有效的分析方法。

Method: 提出SiMiC方法，利用深度学习，开发基于硅的场发射尖端专门数据集，训练包含注意力机制的定制CNN架构进行多类微观结构分类和尺寸预测。

Result: SiMiC在保持可解释性的同时实现了高精度，与经典图像处理技术的对比分析证明了其有效性。

Conclusion: 该框架为与场发射性能直接相关的数据驱动微观结构分析奠定了基础，为将发射极几何形状与发射行为关联以及指导优化冷阴极和SEM电子源设计开辟了途径。相关数据集和算法库可在指定网址获取。

Abstract: Accurate characterization of silicon microstructures is essential for advancing microscale fabrication, quality control, and device performance. Traditional analysis using Scanning Electron Microscopy (SEM) often requires labor-intensive, manual evaluation of feature geometry, limiting throughput and reproducibility. In this study, we propose SiMiC: Context-Aware Silicon Microstructure Characterization Using Attention-Based Convolutional Neural Networks for Field-Emission Tip Analysis. By leveraging deep learning, our approach efficiently extracts morphological features-such as size, shape, and apex curvature-from SEM images, significantly reducing human intervention while improving measurement consistency. A specialized dataset of silicon-based field-emitter tips was developed, and a customized CNN architecture incorporating attention mechanisms was trained for multi-class microstructure classification and dimensional prediction. Comparative analysis with classical image processing techniques demonstrates that SiMiC achieves high accuracy while maintaining interpretability. The proposed framework establishes a foundation for data-driven microstructure analysis directly linked to field-emission performance, opening avenues for correlating emitter geometry with emission behavior and guiding the design of optimized cold-cathode and SEM electron sources. The related dataset and algorithm repository that could serve as a baseline in this area can be found at https://research.jingjietan.com/?q=SIMIC

</details>


### [520] [Summary of the Unusual Activity Recognition Challenge for Developmental Disability Support](https://arxiv.org/abs/2601.17049)
*Christina Garcia,Nhat Tan Le,Taihei Fujioka,Umang Dobhal,Milyun Ni'ma Shoumi,Thanh Nha Nguyen,Sozo Inoue*

Main category: cs.CV

TL;DR: 本文介绍ISAS 2025上举办的异常行为识别挑战，包括任务、数据集、评估策略等，分析结果并指出对相关AI应用有借鉴意义。


<details>
  <summary>Details</summary>
Motivation: 解决利用非入侵式姿态估计数据对发育障碍人士设施中异常行为进行自动识别的需求。

Method: 参赛团队根据视频中提取的骨架关键点区分正常和异常活动，数据集模拟真实情况，采用LOSO策略评估，用宏平均F1分数评估提交结果。

Result: 结果显示在有噪声的低维数据中对罕见、突然动作建模困难，强调在行为建模中捕捉时间和上下文细微差别的重要性。

Conclusion: 此次挑战的见解可促进医疗保健和行为监测方面社会责任AI应用的未来发展。

Abstract: This paper presents an overview of the Recognize the Unseen: Unusual Behavior Recognition from Pose Data Challenge, hosted at ISAS 2025. The challenge aims to address the critical need for automated recognition of unusual behaviors in facilities for individuals with developmental disabilities using non-invasive pose estimation data. Participating teams were tasked with distinguishing between normal and unusual activities based on skeleton keypoints extracted from video recordings of simulated scenarios. The dataset reflects real-world imbalance and temporal irregularities in behavior, and the evaluation adopted a Leave-One-Subject-Out (LOSO) strategy to ensure subject-agnostic generalization. The challenge attracted broad participation from 40 teams applying diverse approaches ranging from classical machine learning to deep learning architectures. Submissions were assessed primarily using macro-averaged F1 scores to account for class imbalance. The results highlight the difficulty of modeling rare, abrupt actions in noisy, low-dimensional data, and emphasize the importance of capturing both temporal and contextual nuances in behavior modeling. Insights from this challenge may contribute to future developments in socially responsible AI applications for healthcare and behavior monitoring.

</details>


### [521] [Single-Pixel Vision-Language Model for Intrinsic Privacy-Preserving Behavioral Intelligence](https://arxiv.org/abs/2601.17050)
*Hongjun An,Yiliang Song,Jiawei Shao,Zhe Sun,Xuelong Li*

Main category: cs.CV

TL;DR: 提出单像素视觉-语言模型（SP - VLM）用于安全环境监测，既能保护隐私又能实现行为监测。


<details>
  <summary>Details</summary>
Motivation: 不良社交互动威胁个人和公共安全，但在隐私敏感环境传统监控受限，需新的安全监测方法。

Method: 提出SP - VLM框架，通过低维单像素模态捕捉人类动态，结合视觉 - 语言集成推断行为模式。

Result: 单像素传感抑制身份恢复，SP - VLM能从严重退化的单像素观测中提取行为语义，实现异常检测等功能，找到保护身份又获取行为智能的采样率范围。

Conclusion: 该方法为人权友好的安全监测提供途径，避免在隐私敏感空间进行侵入式监控。

Abstract: Adverse social interactions, such as bullying, harassment, and other illicit activities, pose significant threats to individual well-being and public safety, leaving profound impacts on physical and mental health. However, these critical events frequently occur in privacy-sensitive environments like restrooms, and changing rooms, where conventional surveillance is prohibited or severely restricted by stringent privacy regulations and ethical concerns. Here, we propose the Single-Pixel Vision-Language Model (SP-VLM), a novel framework that reimagines secure environmental monitoring. It achieves intrinsic privacy-by-design by capturing human dynamics through inherently low-dimensional single-pixel modalities and inferring complex behavioral patterns via seamless vision-language integration. Building on this framework, we demonstrate that single-pixel sensing intrinsically suppresses identity recoverability, rendering state-of-the-art face recognition systems ineffective below a critical sampling rate. We further show that SP-VLM can nonetheless extract meaningful behavioral semantics, enabling robust anomaly detection, people counting, and activity understanding from severely degraded single-pixel observations. Combining these findings, we identify a practical sampling-rate regime in which behavioral intelligence emerges while personal identity remains strongly protected. Together, these results point to a human-rights-aligned pathway for safety monitoring that can support timely intervention without normalizing intrusive surveillance in privacy-sensitive spaces.

</details>


### [522] [A Computer Vision Pipeline for Iterative Bullet Hole Tracking in Rifle Zeroing](https://arxiv.org/abs/2601.17062)
*Robert M. Belcher,Brendan C. Degryse,Leonard R. Kosta,Christopher J. Lowrance*

Main category: cs.CV

TL;DR: 提出端到端计算机视觉系统用于自动检测弹孔和跟踪，结合YOLOv8与IoU分析，有数据增强和预处理方法，检测精度高且有广泛应用。


<details>
  <summary>Details</summary>
Motivation: 传统步枪归零过程需人工检查弹孔，存在延迟和人为误差，需要自动化解决方案。

Method: 结合YOLOv8进行小目标检测和IoU分析区分弹孔；提出去除对象的数据增强技术；引入基于ORB的预处理管道校正目标方向。

Result: 系统在弹孔检测上平均精度达97.0%，迭代分配准确率达88.8%。

Conclusion: 该框架虽为步枪归零设计，但在需区分视觉相似对象的领域有广泛应用。

Abstract: Adjusting rifle sights, a process commonly called "zeroing," requires shooters to identify and differentiate bullet holes from multiple firing iterations. Traditionally, this process demands physical inspection, introducing delays due to range safety protocols and increasing the risk of human error. We present an end-to-end computer vision system for automated bullet hole detection and iteration-based tracking directly from images taken at the firing line. Our approach combines YOLOv8 for accurate small-object detection with Intersection over Union (IoU) analysis to differentiate bullet holes across sequential images. To address the scarcity of labeled sequential data, we propose a novel data augmentation technique that removes rather than adds objects to simulate realistic firing sequences. Additionally, we introduce a preprocessing pipeline that standardizes target orientation using ORB-based perspective correction, improving model accuracy. Our system achieves 97.0% mean average precision on bullet hole detection and 88.8% accuracy in assigning bullet holes to the correct firing iteration. While designed for rifle zeroing, this framework offers broader applicability in domains requiring the temporal differentiation of visually similar objects.

</details>


### [523] [A Mechanistic View on Video Generation as World Models: State and Dynamics](https://arxiv.org/abs/2601.17067)
*Luozhou Wang,Zhifei Chen,Yihua Du,Dongyu Yan,Wenhang Ge,Guibao Shen,Xinli Xu,Leyi Wu,Man Chen,Tianshuo Xu,Peiran Ren,Xin Tao,Pengfei Wan,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 本文提出以状态构建和动力学建模为核心的分类法，弥合现有视频架构与世界模型理论差距，倡导评估转变并指出研究前沿。


<details>
  <summary>Details</summary>
Motivation: 现有的“无状态”视频架构与经典以状态为中心的世界模型理论存在差距。

Method: 提出基于状态构建和动力学建模的分类法，将状态构建分为隐式和显式范式，分析动力学建模的知识整合与架构重构；倡导从视觉保真度转向功能基准评估。

Result: 无明确具体结果

Conclusion: 确定两个关键研究前沿，以推动该领域从生成视觉合理视频向构建强大通用世界模拟器发展。

Abstract: Large-scale video generation models have demonstrated emergent physical coherence, positioning them as potential world models. However, a gap remains between contemporary "stateless" video architectures and classic state-centric world model theories. This work bridges this gap by proposing a novel taxonomy centered on two pillars: State Construction and Dynamics Modeling. We categorize state construction into implicit paradigms (context management) and explicit paradigms (latent compression), while dynamics modeling is analyzed through knowledge integration and architectural reformulation. Furthermore, we advocate for a transition in evaluation from visual fidelity to functional benchmarks, testing physical persistence and causal reasoning. We conclude by identifying two critical frontiers: enhancing persistence via data-driven memory and compressed fidelity, and advancing causality through latent factor decoupling and reasoning-prior integration. By addressing these challenges, the field can evolve from generating visually plausible videos to building robust, general-purpose world simulators.

</details>


### [524] [LoD Sketch Extraction from Architectural Models Using Generative AI: Dataset Construction for Multi-Level Architectural Design Generation](https://arxiv.org/abs/2601.17095)
*Xusheng Du,Athiwat Kongkaeo,Ye Zhang,Haoran Xie*

Main category: cs.CV

TL;DR: 提出自动LoD草图提取框架，用生成式AI模型从高细节建筑模型生成多LoD表示，实验证明能保持几何一致性和语义简化。


<details>
  <summary>Details</summary>
Motivation: 传统LoD建模手动操作耗时费力且易有几何不一致问题，生成式AI应用缺高质量配对LoD训练数据。

Method: 提出自动LoD草图提取框架，集成计算机视觉技术和生成式AI方法建立从详细到抽象的提取管道。

Result: 方法在不同LoD层级保持强几何一致性，SSIM值和归一化Hausdorff距离反映受控几何偏差。

Conclusion: 框架有效保留全局结构并实现语义简化，为AI驱动的多级建筑生成和分层建模提供支持。

Abstract: For architectural design, representation across multiple Levels of Details (LoD) is essential for achieving a smooth transition from conceptual massing to detailed modeling. However, traditional LoD modeling processes rely on manual operations that are time-consuming, labor-intensive, and prone to geometric inconsistencies. While the rapid advancement of generative artificial intelligence (AI) has opened new possibilities for generating multi-level architectural models from sketch inputs, its application remains limited by the lack of high-quality paired LoD training data. To address this issue, we propose an automatic LoD sketch extraction framework using generative AI models, which progressively simplifies high-detail architectural models to automatically generate geometrically consistent and hierarchically coherent multi-LoD representations. The proposed framework integrates computer vision techniques with generative AI methods to establish a progressive extraction pipeline that transitions from detailed representations to volumetric abstractions. Experimental results demonstrate that the method maintains strong geometric consistency across LoD levels, achieving SSIM values of 0.7319 and 0.7532 for the transitions from LoD3 to LoD2 and from LoD2 to LoD1, respectively, with corresponding normalized Hausdorff distances of 25.1% and 61.0% of the image diagonal, reflecting controlled geometric deviation during abstraction. These results verify that the proposed framework effectively preserves global structure while achieving progressive semantic simplification across different LoD levels, providing reliable data and technical support for AI-driven multi-level architectural generation and hierarchical modeling.

</details>


### [525] [Spatiotemporal Semantic V2X Framework for Cooperative Collision Prediction](https://arxiv.org/abs/2601.17216)
*Murat Arda Onsu,Poonam Lohan,Burak Kantarci,Aisha Syed,Matthew Andrews,Sean Kennedy*

Main category: cs.CV

TL;DR: 提出语义V2X框架用于智能交通系统实时碰撞预测，减少通信开销并保持准确性。


<details>
  <summary>Details</summary>
Motivation: 传统方法在车辆通信带宽和延迟限制下不实用，智能交通系统需要实时碰撞预测。

Method: RSU摄像头用V - JEPA生成未来帧时空语义嵌入，构建数字孪生评估系统，车辆端用轻量级探针和分类器解码预测碰撞。

Result: 框架使碰撞预测F1分数提高10%，传输需求比原始视频降低四个数量级。

Conclusion: 语义V2X通信在智能交通系统中实现协作实时碰撞预测有潜力。

Abstract: Intelligent Transportation Systems (ITS) demand real-time collision prediction to ensure road safety and reduce accident severity. Conventional approaches rely on transmitting raw video or high-dimensional sensory data from roadside units (RSUs) to vehicles, which is impractical under vehicular communication bandwidth and latency constraints. In this work, we propose a semantic V2X framework in which RSU-mounted cameras generate spatiotemporal semantic embeddings of future frames using the Video Joint Embedding Predictive Architecture (V-JEPA). To evaluate the system, we construct a digital twin of an urban traffic environment enabling the generation of d verse traffic scenarios with both safe and collision events. These embeddings of the future frame, extracted from V-JEPA, capture task-relevant traffic dynamics and are transmitted via V2X links to vehicles, where a lightweight attentive probe and classifier decode them to predict imminent collisions. By transmitting only semantic embeddings instead of raw frames, the proposed system significantly reduces communication overhead while maintaining predictive accuracy. Experimental results demonstrate that the framework with an appropriate processing method achieves a 10% F1-score improvement for collision prediction while reducing transmission requirements by four orders of magnitude compared to raw video. This validates the potential of semantic V2X communication to enable cooperative, real-time collision prediction in ITS.

</details>


### [526] [Dynamic Meta-Ensemble Framework for Efficient and Accurate Deep Learning in Plant Leaf Disease Detection on Resource-Constrained Edge Devices](https://arxiv.org/abs/2601.17290)
*Weloday Fikadu Moges,Jianmei Su,Amin Waqas*

Main category: cs.CV

TL;DR: 提出动态元集成框架DMEF用于资源受限下的植物病害诊断，实验表现优异，有边缘农业监测潜力。


<details>
  <summary>Details</summary>
Motivation: 边缘设备计算资源和能源预算有限，限制深度学习模型用于植物病害检测，需解决该挑战。

Method: 引入DMEF，采用自适应加权机制动态组合三个轻量级卷积神经网络预测，迭代更新集成权重。

Result: 在马铃薯和玉米病害基准数据集上分类准确率分别达99.53%和96.61%，超越独立模型和静态集成，推理延迟低且参数少。

Conclusion: DMEF有边缘农业监测潜力，能弥合高精度AI与实际应用差距。

Abstract: Deploying deep learning models for plant disease detection on edge devices such as IoT sensors, smartphones, and embedded systems is severely constrained by limited computational resources and energy budgets. To address this challenge, we introduce a novel Dynamic Meta-Ensemble Framework (DMEF) for high-accuracy plant disease diagnosis under resource constraints. DMEF employs an adaptive weighting mechanism that dynamically combines the predictions of three lightweight convolutional neural networks (MobileNetV2, NASNetMobile, and InceptionV3) by optimizing a trade-off between accuracy improvements (DeltaAcc) and computational efficiency (model size). During training, the ensemble weights are updated iteratively, favoring models exhibiting high performance and low complexity. Extensive experiments on benchmark datasets for potato and maize diseases demonstrate state-of-the-art classification accuracies of 99.53% and 96.61%, respectively, surpassing standalone models and static ensembles by 2.1% and 6.3%. With computationally efficient inference latency (<75ms) and a compact footprint (<1 million parameters), DMEF shows strong potential for edge-based agricultural monitoring, suggesting viability for scalable crop disease management. This bridges the gap between high-accuracy AI and practical field applications.

</details>


### [527] [ClinNet: Evidential Ordinal Regression with Bilateral Asymmetry and Prototype Memory for Knee Osteoarthritis Grading](https://arxiv.org/abs/2601.17315)
*Xiaoyang Li,Runni Zhou*

Main category: cs.CV

TL;DR: 提出ClinNet框架解决基于X光图像的膝骨关节炎（KOA）分级问题，实验效果优于现有基线模型，且不确定性估计可辅助安全临床应用。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法将KOA分级作为确定性多分类问题，忽略疾病退化的连续性和专家标注的不确定性，而KOA分级因级间差异细微、标注不确定和疾病进展的有序性等因素具有挑战性。

Method: 提出ClinNet框架，将KOA分级作为证据序数回归问题，集成双边不对称编码器（BAE）、诊断记忆库和基于NIG分布的证据序数头三个关键组件。

Result: ClinNet实现二次加权kappa值0.892和准确率0.768，统计上显著优于现有基线模型（p < 0.001），模型的不确定性估计能成功标记分布外样本和潜在误诊。

Conclusion: ClinNet为KOA分级提供有效解决方案，其不确定性估计为安全临床部署奠定基础。

Abstract: Knee osteoarthritis (KOA) grading based on radiographic images is a critical yet challenging task due to subtle inter-grade differences, annotation uncertainty, and the inherently ordinal nature of disease progression. Conventional deep learning approaches typically formulate this problem as deterministic multi-class classification, ignoring both the continuous progression of degeneration and the uncertainty in expert annotations. In this work, we propose ClinNet, a novel trustworthy framework that addresses KOA grading as an evidential ordinal regression problem. The proposed method integrates three key components: (1) a Bilateral Asymmetry Encoder (BAE) that explicitly models medial-lateral structural discrepancies; (2) a Diagnostic Memory Bank that maintains class-wise prototypes to stabilize feature representations; and (3) an Evidential Ordinal Head based on the Normal-Inverse-Gamma (NIG) distribution to jointly estimate continuous KL grades and epistemic uncertainty. Extensive experiments demonstrate that ClinNet achieves a Quadratic Weighted Kappa of 0.892 and Accuracy of 0.768, statistically outperforming state-of-the-art baselines (p < 0.001). Crucially, we demonstrate that the model's uncertainty estimates successfully flag out-of-distribution samples and potential misdiagnoses, paving the way for safe clinical deployment.

</details>


### [528] [NeRF-MIR: Towards High-Quality Restoration of Masked Images with Neural Radiance Fields](https://arxiv.org/abs/2601.17350)
*Xianliang Huang,Zhizhou Zhong,Shuhang Chen,Yi Xu,Juhong Guan,Shuigeng Zhou*

Main category: cs.CV

TL;DR: 本文提出NeRF - MIR用于恢复掩码图像，提出PERE策略、PIRE机制和动态加权损失函数，构建三个掩码数据集，实验证明其在掩码图像恢复上优于同类方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于NeRF从受损图像恢复3D场景有很大提升空间，受损图像常见且会影响NeRF效果。

Method: 提出NeRF - MIR方法，包括PERE策略合理分配发射光线，PIRE机制在自训练过程中恢复掩码区域，设计动态加权损失函数，构建三个掩码数据集。

Result: 在真实数据和构建数据集上的大量实验表明，NeRF - MIR在掩码图像恢复上优于同类方法。

Conclusion: NeRF - MIR在掩码图像恢复领域具有优越性和潜力。

Abstract: Neural Radiance Fields (NeRF) have demonstrated remarkable performance in novel view synthesis. However, there is much improvement room on restoring 3D scenes based on NeRF from corrupted images, which are common in natural scene captures and can significantly impact the effectiveness of NeRF. This paper introduces NeRF-MIR, a novel neural rendering approach specifically proposed for the restoration of masked images, demonstrating the potential of NeRF in this domain. Recognizing that randomly emitting rays to pixels in NeRF may not effectively learn intricate image textures, we propose a \textbf{P}atch-based \textbf{E}ntropy for \textbf{R}ay \textbf{E}mitting (\textbf{PERE}) strategy to distribute emitted rays properly. This enables NeRF-MIR to fuse comprehensive information from images of different views. Additionally, we introduce a \textbf{P}rogressively \textbf{I}terative \textbf{RE}storation (\textbf{PIRE}) mechanism to restore the masked regions in a self-training process. Furthermore, we design a dynamically-weighted loss function that automatically recalibrates the loss weights for masked regions. As existing datasets do not support NeRF-based masked image restoration, we construct three masked datasets to simulate corrupted scenarios. Extensive experiments on real data and constructed datasets demonstrate the superiority of NeRF-MIR over its counterparts in masked image restoration.

</details>


### [529] [Physical Prompt Injection Attacks on Large Vision-Language Models](https://arxiv.org/abs/2601.17383)
*Chen Ling,Kai Hu,Hangcheng Liu,Xingshuo Han,Tianwei Zhang,Changhai Ou*

Main category: cs.CV

TL;DR: 提出物理提示注入攻击（PPIA），对大视觉语言模型进行黑盒、与查询无关的攻击，在模拟和真实场景中评估，成功率达98%，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有针对大视觉语言模型的提示注入攻击方法在实际部署中存在局限性，如需要访问输入通道或依赖用户查询知识。

Method: 提出PPIA，结合离线选择视觉提示和基于时空注意力的环境感知放置策略，仅通过视觉观察进行攻击。

Result: 在模拟和真实场景中对10个先进的大视觉语言模型进行评估，PPIA攻击成功率高达98%，在不同物理条件下具有强鲁棒性。

Conclusion: PPIA是一种有效的黑盒、查询无关的攻击方法，能对大视觉语言模型造成影响。

Abstract: Large Vision-Language Models (LVLMs) are increasingly deployed in real-world intelligent systems for perception and reasoning in open physical environments. While LVLMs are known to be vulnerable to prompt injection attacks, existing methods either require access to input channels or depend on knowledge of user queries, assumptions that rarely hold in practical deployments. We propose the first Physical Prompt Injection Attack (PPIA), a black-box, query-agnostic attack that embeds malicious typographic instructions into physical objects perceivable by the LVLM. PPIA requires no access to the model, its inputs, or internal pipeline, and operates solely through visual observation. It combines offline selection of highly recognizable and semantically effective visual prompts with strategic environment-aware placement guided by spatiotemporal attention, ensuring that the injected prompts are both perceivable and influential on model behavior. We evaluate PPIA across 10 state-of-the-art LVLMs in both simulated and real-world settings on tasks including visual question answering, planning, and navigation, PPIA achieves attack success rates up to 98%, with strong robustness under varying physical conditions such as distance, viewpoint, and illumination. Our code is publicly available at https://github.com/2023cghacker/Physical-Prompt-Injection-Attack.

</details>


### [530] [ONRW: Optimizing inversion noise for high-quality and robust watermark](https://arxiv.org/abs/2601.17388)
*Xuan Ding,Xiu Yan,Chuanlong Xie,Yao Zhu*

Main category: cs.CV

TL;DR: 提出基于扩散模型的高质量鲁棒水印框架，实验结果表现好，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习水印系统在图像传输受损时缺乏鲁棒性，影响实际应用价值。

Method: 先通过空文本优化将干净图像转换为逆噪声，在隐空间优化后用扩散模型迭代去噪生成水印图像，同时引入自注意力约束和伪掩码策略防止图像语义失真。

Result: 实验显示该方法在多种图像损坏情况下性能优越，在COCO数据集12种不同图像变换中平均比稳定签名方法高10% 。

Conclusion: 所提水印框架具有高质量和鲁棒性，在水印保护方面有较好应用前景。

Abstract: Watermarking methods have always been effective means of protecting intellectual property, yet they face significant challenges. Although existing deep learning-based watermarking systems can hide watermarks in images with minimal impact on image quality, they often lack robustness when encountering image corruptions during transmission, which undermines their practical application value. To this end, we propose a high-quality and robust watermark framework based on the diffusion model. Our method first converts the clean image into inversion noise through a null-text optimization process, and after optimizing the inversion noise in the latent space, it produces a high-quality watermarked image through an iterative denoising process of the diffusion model. The iterative denoising process serves as a powerful purification mechanism, ensuring both the visual quality of the watermarked image and enhancing the robustness of the watermark against various corruptions. To prevent the optimizing of inversion noise from distorting the original semantics of the image, we specifically introduced self-attention constraints and pseudo-mask strategies. Extensive experimental results demonstrate the superior performance of our method against various image corruptions. In particular, our method outperforms the stable signature method by an average of 10\% across 12 different image transformations on COCO datasets. Our codes are available at https://github.com/920927/ONRW.

</details>


### [531] [ReLE: A Scalable System and Structured Benchmark for Diagnosing Capability Anisotropy in Chinese LLMs](https://arxiv.org/abs/2601.17399)
*Rui Fang,Jian Li,Wei Chen,Bin Hu,Ying-Cong Chen,Xin Tang,Liang Diao*

Main category: cs.CV

TL;DR: 现有大语言模型中文理解能力评估有问题，本文提出ReLE系统评估304个模型，有两项方法改进，分析显示模型排名对权重方案敏感，ReLE可作高频诊断监测。


<details>
  <summary>Details</summary>
Motivation: 大语言模型中文理解能力评估面临基准饱和和计算成本高的挑战，且静态排行榜掩盖能力间的结构权衡。

Method: 提出ReLE系统，使用跨领域×能力正交矩阵评估模型，引入符号接地混合评分机制和动态方差感知调度器。

Result: 评估304个模型，调度器相比全量评估降低70%计算成本且排名相关性达0.96，模型排名对权重方案敏感。

Conclusion: ReLE不是全面静态基准的替代品，而是用于不断变化的模型格局的高频诊断监测工具。

Abstract: Large Language Models (LLMs) have achieved rapid progress in Chinese language understanding, yet accurately evaluating their capabilities remains challenged by benchmark saturation and prohibitive computational costs. While static leaderboards provide snapshot rankings, they often mask the structural trade-offs between capabilities. In this work, we present ReLE (Robust Efficient Live Evaluation), a scalable system designed to diagnose Capability Anisotropy, the non-uniformity of model performance across domains. Using ReLE, we evaluate 304 models (189 commercial, 115 open-source) across a Domain $\times$ Capability orthogonal matrix comprising 207,843 samples. We introduce two methodological contributions to address current evaluation pitfalls: (1) A Symbolic-Grounded Hybrid Scoring Mechanism that eliminates embedding-based false positives in reasoning tasks; (2) A Dynamic Variance-Aware Scheduler based on Neyman allocation with noise correction, which reduces compute costs by 70\% compared to full-pass evaluations while maintaining a ranking correlation of $ρ=0.96$. Our analysis reveals that aggregate rankings are highly sensitive to weighting schemes: models exhibit a Rank Stability Amplitude (RSA) of 11.4 in ReLE versus $\sim$5.0 in traditional benchmarks, confirming that modern models are highly specialized rather than generally superior. We position ReLE not as a replacement for comprehensive static benchmarks, but as a high-frequency diagnostic monitor for the evolving model landscape.

</details>


### [532] [Coronary Artery Segmentation and Vessel-Type Classification in X-Ray Angiography](https://arxiv.org/abs/2601.17429)
*Mehdi Yousefzadeh,Siavash Shirzadeh Barough,Ashkan Fakharifar,Yashar Tayyarazad,Narges Eghbali,Mohaddeseh Mozaffari,Hoda Taeb,Negar Sadat Rafiee Tabatabaee,Parsa Esfahanian,Ghazaleh Sadeghi Gohar,Amineh Safavirad,Saeideh Mazloomzadeh,Ehsan khalilipur,Armin Elahifar,Majid Maleki*

Main category: cs.CV

TL;DR: 本文提出改进X射线冠状动脉造影（XCA）血管分割和类型标注的方法，对比多种模型和参数调整方式，结果显示特定方法有更好表现。


<details>
  <summary>Details</summary>
Motivation: XCA定量分析受限于常规数据中血管分割的困难，可靠的分割和血管类型标注有助于特定血管分析和下游测量。

Method: 选择最佳帧，应用联合超分辨率和增强，对比经典血管滤波器（Meijering、Frangi、Sato）在不同参数调整下的表现，使用U - Net、FPN和Swin Transformer等神经网络模型，以冠状动脉和冠状动脉 + 导管监督训练，第二阶段进行血管类型标注，用公共DCA1队列进行外部评估。

Result: SVR逐图像调参提高经典滤波器的Dice系数；FPN模型在冠状动脉监督下Dice达0.914，合并标签监督提高到0.931；外部测试中Dice下降，微调后恢复；血管类型标注对RCA、LAD、LCX有较高准确率。

Conclusion: 学习的逐图像调参增强经典管道，高分辨率FPN模型和合并标签监督提高稳定性和外部迁移能力。

Abstract: X-ray coronary angiography (XCA) is the clinical reference standard for assessing coronary artery disease, yet quantitative analysis is limited by the difficulty of robust vessel segmentation in routine data. Low contrast, motion, foreshortening, overlap, and catheter confounding degrade segmentation and contribute to domain shift across centers. Reliable segmentation, together with vessel-type labeling, enables vessel-specific coronary analytics and downstream measurements that depend on anatomical localization. From 670 cine sequences (407 subjects), we select a best frame near peak opacification using a low-intensity histogram criterion and apply joint super-resolution and enhancement. We benchmark classical Meijering, Frangi, and Sato vesselness filters under per-image oracle tuning, a single global mean setting, and per-image parameter prediction via Support Vector Regression (SVR). Neural baselines include U-Net, FPN, and a Swin Transformer, trained with coronary-only and merged coronary+catheter supervision. A second stage assigns vessel identity (LAD, LCX, RCA). External evaluation uses the public DCA1 cohort. SVR per-image tuning improves Dice over global means for all classical filters (e.g., Frangi: 0.759 vs. 0.741). Among deep models, FPN attains 0.914+/-0.007 Dice (coronary-only), and merged coronary+catheter labels further improve to 0.931+/-0.006. On DCA1 as a strict external test, Dice drops to 0.798 (coronary-only) and 0.814 (merged), while light in-domain fine-tuning recovers to 0.881+/-0.014 and 0.882+/-0.015. Vessel-type labeling achieves 98.5% accuracy (Dice 0.844) for RCA, 95.4% (0.786) for LAD, and 96.2% (0.794) for LCX. Learned per-image tuning strengthens classical pipelines, while high-resolution FPN models and merged-label supervision improve stability and external transfer with modest adaptation.

</details>


### [533] [Diagnosis Support of Sickle Cell Anemia by Classifying Red Blood Cell Shape in Peripheral Blood Images](https://arxiv.org/abs/2601.17032)
*Wilkie Delgado-Font,Miriela Escobedo-Nicot,Manuel González-Hidalgo,Silena Herold-Garcia,Antoni Jaume-i-Capó,Arnau Mir*

Main category: cs.CV

TL;DR: 提出一种利用外周血涂片图像分析自动计数红细胞的方法，通过实验验证该方法在镰状细胞贫血诊断上优于现有方法，结果适用于临床治疗和诊断支持。


<details>
  <summary>Details</summary>
Motivation: 目前监测红细胞变形相关疾病的显微镜观察法耗时、需专家操作且误差率高，为此提出自动方法。

Method: 使用Chan - Vese主动轮廓模型分割图像目标，利用圆形形状因子（CSF）和椭圆形形状因子（ESF）对红细胞进行分类，并对部分遮挡细胞进行椭圆调整。

Result: 实验中该方法的F值（正常细胞0.97，细长细胞0.95）和多类性能指标显示其优于现有方法。

Conclusion: 该方法适用于镰状细胞贫血的临床治疗和诊断支持。

Abstract: Red blood cell (RBC) deformation is the consequence of several diseases, including sickle cell anemia, which causes recurring episodes of pain and severe pronounced anemia. Monitoring patients with these diseases involves the observation of peripheral blood samples under a microscope, a time-consuming procedure. Moreover, a specialist is required to perform this technique, and owing to the subjective nature of the observation of isolated RBCs, the error rate is high. In this paper, we propose an automated method for differentially enumerating RBCs that uses peripheral blood smear image analysis. In this method, the objects of interest in the image are segmented using a Chan-Vese active contour model. An analysis is then performed to classify the RBCs, also called erythrocytes, as normal or elongated or having other deformations, using the basic shape analysis descriptors: circular shape factor (CSF) and elliptical shape factor (ESF). To analyze cells that become partially occluded in a cluster during sample preparation, an elliptical adjustment is performed to allow the analysis of erythrocytes with discoidal and elongated shapes. The images of patient blood samples used in the study were acquired by a clinical laboratory specialist in the Special Hematology Department of the ``Dr. Juan Bruno Zayas'' General Hospital in Santiago de Cuba. A comparison of the results obtained by the proposed method in our experiments with those obtained by some state-of-the-art methods showed that the proposed method is superior for the diagnosis of sickle cell anemia. This superiority is achieved for evidenced by the obtained F-measure value (0.97 for normal cells and 0.95 for elongated ones) and several overall multiclass performance measures. The results achieved by the proposed method are suitable for the purpose of clinical treatment and diagnostic support of sickle cell anemia.

</details>


### [534] [Hybrid Deep Feature Extraction and ML for Construction and Demolition Debris Classification](https://arxiv.org/abs/2601.17038)
*Obai Alashram,Nejad Alagha,Mahmoud AlKakuri,Zeeshan Swaveel,Abigail Copiaco*

Main category: cs.CV

TL;DR: 研究提出混合视觉管道用于自动化建筑及拆除碎片分类，收集数据集，评估多种分类器，结果显示部分混合管道性能超复杂深度学习方法，具操作优势并指明未来方向。


<details>
  <summary>Details</summary>
Motivation: 建筑行业产生大量碎片，有效分类对可持续废物管理和资源回收至关重要。

Method: 提出混合视觉管道，结合深度特征提取和经典机器学习分类器，用预训练Xception网络提取特征，系统评估SVM、kNN等多个分类器。

Result: 使用Xception特征和线性SVM、kNN、Bagged Trees等简单分类器的混合管道达到最先进性能，准确率和宏F1分数达99.5%，超复杂或端到端深度学习方法。

Conclusion: 该方法对碎片识别有操作优势，为未来与机器人和现场自动化系统集成提供途径。

Abstract: The construction industry produces significant volumes of debris, making effective sorting and classification critical for sustainable waste management and resource recovery. This study presents a hybrid vision-based pipeline that integrates deep feature extraction with classical machine learning (ML) classifiers for automated construction and demolition (C\&D) debris classification. A novel dataset comprising 1,800 balanced, high-quality images representing four material categories, Ceramic/Tile, Concrete, Trash/Waste, and Wood was collected from real construction sites in the UAE, capturing diverse real-world conditions. Deep features were extracted using a pre-trained Xception network, and multiple ML classifiers, including SVM, kNN, Bagged Trees, LDA, and Logistic Regression, were systematically evaluated. The results demonstrate that hybrid pipelines using Xception features with simple classifiers such as Linear SVM, kNN, and Bagged Trees achieve state-of-the-art performance, with up to 99.5\% accuracy and macro-F1 scores, surpassing more complex or end-to-end deep learning approaches. The analysis highlights the operational benefits of this approach for robust, field-deployable debris identification and provides pathways for future integration with robotics and onsite automation systems.

</details>


### [535] [Atomic Depth Estimation From Noisy Electron Microscopy Data Via Deep Learning](https://arxiv.org/abs/2601.17046)
*Matan Leibovich,Mai Tan,Adria Marcos-Morales,Sreyas Mohan,Peter A. Crozier,Carlos Fernandez-Granda*

Main category: cs.CV

TL;DR: 提出从含噪TEM图像提取3D原子级信息新方法，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 从受显著噪声影响的透射电子显微镜（TEM）图像中提取3D原子级信息。

Method: 将深度估计表述为语义分割问题，用含合成噪声的模拟数据训练深度卷积神经网络生成逐像素深度分割图。

Result: 对CeO2纳米粒子的模拟图像和真实TEM数据进行原子列深度估计，结果准确、经过校准且对噪声鲁棒。

Conclusion: 所提出的方法能从含噪TEM图像中有效提取3D原子级信息。

Abstract: We present a novel approach for extracting 3D atomic-level information from transmission electron microscopy (TEM) images affected by significant noise. The approach is based on formulating depth estimation as a semantic segmentation problem. We address the resulting segmentation problem by training a deep convolutional neural network to generate pixel-wise depth segmentation maps using simulated data corrupted by synthetic noise. The proposed method was applied to estimate the depth of atomic columns in CeO2 nanoparticles from simulated images and real-world TEM data. Our experiments show that the resulting depth estimates are accurate, calibrated and robust to noise.

</details>


### [536] [A Contrastive Pre-trained Foundation Model for Deciphering Imaging Noisomics across Modalities](https://arxiv.org/abs/2601.17047)
*Yuanjie Gu,Yiqun Wang,Chaohui Yu,Ang Xuan,Fan Wang,Zhi Lu,Biqin Dong*

Main category: cs.CV

TL;DR: 提出'Noisomics'框架，用对比预训练基础模型CoP解码成像噪声，减少数据和计算依赖，有强大泛化能力，可用于多场景。


<details>
  <summary>Details</summary>
Motivation: 现有范式在不依赖大量监督数据集情况下难以分离成像噪声中的物理信号和算法伪影，且常将噪声仅视为干扰。

Method: 引入Noisomics框架，利用对比预训练基础模型CoP，通过对比学习和合成噪声基因组，从随机扰动中分离语义信号。

Result: CoP用100个训练样本优于用100000个样本训练的监督基线，在12个不同域外数据集上有强大零样本泛化能力，估计误差降低63.8%，决定系数提高85.1%。

Conclusion: 将随机退化重新定义为重要信息资源，无需事先设备校准就能实现精确成像诊断。

Abstract: Characterizing imaging noise is notoriously data-intensive and device-dependent, as modern sensors entangle physical signals with complex algorithmic artifacts. Current paradigms struggle to disentangle these factors without massive supervised datasets, often reducing noise to mere interference rather than an information resource. Here, we introduce "Noisomics", a framework shifting the focus from suppression to systematic noise decoding via the Contrastive Pre-trained (CoP) Foundation Model. By leveraging the manifold hypothesis and synthetic noise genome, CoP employs contrastive learning to disentangle semantic signals from stochastic perturbations. Crucially, CoP breaks traditional deep learning scaling laws, achieving superior performance with only 100 training samples, outperforming supervised baselines trained on 100,000 samples, thereby reducing data and computational dependency by three orders of magnitude. Extensive benchmarking across 12 diverse out-of-domain datasets confirms its robust zero-shot generalization, demonstrating a 63.8% reduction in estimation error and an 85.1% improvement in the coefficient of determination compared to the conventional training strategy. We demonstrate CoP's utility across scales: from deciphering non-linear hardware-noise interplay in consumer photography to optimizing photon-efficient protocols for deep-tissue microscopy. By decoding noise as a multi-parametric footprint, our work redefines stochastic degradation as a vital information resource, empowering precise imaging diagnostics without prior device calibration.

</details>


### [537] [Ego4OOD: Rethinking Egocentric Video Domain Generalization via Covariate Shift Scoring](https://arxiv.org/abs/2601.17056)
*Zahra Vaseqi,James Clark*

Main category: cs.CV

TL;DR: 本文提出Ego4OOD基准，用聚类协变量偏移指标量化难度，采用一对多二元训练目标，轻量级网络取得不错性能，证明控制基准和量化域特征对自我中心视频泛化研究的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有自我中心领域泛化基准混淆协变量偏移和概念偏移，难以可靠评估模型跨输入分布泛化能力。

Method: 引入Ego4OOD基准，强调可测量的协变量多样性并减少概念偏移；使用聚类的协变量偏移指标量化域难度；采用一对多二元训练目标，将多类动作识别分解为独立二元分类任务。

Result: 轻量级两层全连接网络在Argo1M和Ego4OOD上取得与现有方法相当的性能，使用参数少且无额外模态。

Conclusion: 测量的协变量偏移与识别性能有明确关系，控制基准和量化域特征对研究自我中心视频分布外泛化很重要。

Abstract: Egocentric video action recognition under domain shifts remains challenging due to large intra-class spatio-temporal variability, long-tailed feature distributions, and strong correlations between actions and environments. Existing benchmarks for egocentric domain generalization often conflate covariate shifts with concept shifts, making it difficult to reliably evaluate a model's ability to generalize across input distributions. To address this limitation, we introduce Ego4OOD, a domain generalization benchmark derived from Ego4D that emphasizes measurable covariate diversity while reducing concept shift through semantically coherent, moment-level action categories. Ego4OOD spans eight geographically distinct domains and is accompanied by a clustering-based covariate shift metric that provides a quantitative proxy for domain difficulty. We further leverage a one-vs-all binary training objective that decomposes multi-class action recognition into independent binary classification tasks. This formulation is particularly well-suited for covariate shift by reducing interference between visually similar classes under feature distribution shift. Using this formulation, we show that a lightweight two-layer fully connected network achieves performance competitive with state-of-the-art egocentric domain generalization methods on both Argo1M and Ego4OOD, despite using fewer parameters and no additional modalities. Our empirical analysis demonstrates a clear relationship between measured covariate shift and recognition performance, highlighting the importance of controlled benchmarks and quantitative domain characterization for studying out-of-distribution generalization in egocentric video.

</details>


### [538] [Performance uncertainty in medical image analysis: a large-scale investigation of confidence intervals](https://arxiv.org/abs/2601.17103)
*Pascaline André,Charles Heitz,Evangelia Christodoulou,Annika Reinke,Carole H. Sudre,Michela Antonelli,Patrick Godau,M. Jorge Cardoso,Antoine Gilson,Sophie Tezenas du Montcel,Gaël Varoquaux,Lena Maier-Hein,Olivier Colliot*

Main category: cs.CV

TL;DR: 本文对医学影像AI性能不确定性量化中的置信区间（CI）方法进行大规模实证分析，揭示影响CI的因素并为未来指南提供关键依据。


<details>
  <summary>Details</summary>
Motivation: 由于医学影像领域对CI行为研究较少，社区对CI方法多样性及特定场景下的表现了解不足，因此开展此研究。

Method: 对24个分割和分类任务进行大规模实证分析，每个任务组使用19个训练模型，采用多种常用性能指标、聚合策略和CI方法，估计各CI方法的可靠性和精度以表征其对研究特征的依赖性。

Result: 发现可靠CI所需样本量因研究参数而异；CI行为受性能指标选择影响；聚合策略显著影响CI可靠性；机器学习问题类型会调节这些影响；不同CI方法在不同用例下可靠性和精度不同。

Conclusion: 研究结果为医学影像AI报告性能不确定性的未来指南制定提供关键组成部分。

Abstract: Performance uncertainty quantification is essential for reliable validation and eventual clinical translation of medical imaging artificial intelligence (AI). Confidence intervals (CIs) play a central role in this process by indicating how precise a reported performance estimate is. Yet, due to the limited amount of work examining CI behavior in medical imaging, the community remains largely unaware of how many diverse CI methods exist and how they behave in specific settings. The purpose of this study is to close this gap. To this end, we conducted a large-scale empirical analysis across a total of 24 segmentation and classification tasks, using 19 trained models per task group, a broad spectrum of commonly used performance metrics, multiple aggregation strategies, and several widely adopted CI methods. Reliability (coverage) and precision (width) of each CI method were estimated across all settings to characterize their dependence on study characteristics. Our analysis revealed five principal findings: 1) the sample size required for reliable CIs varies from a few dozens to several thousands of cases depending on study parameters; 2) CI behavior is strongly affected by the choice of performance metric; 3) aggregation strategy substantially influences the reliability of CIs, e.g. they require more observations for macro than for micro; 4) the machine learning problem (segmentation versus classification) modulates these effects; 5) different CI methods are not equally reliable and precise depending on the use case. These results form key components for the development of future guidelines on reporting performance uncertainty in medical imaging AI.

</details>


### [539] [Uni-RS: A Spatially Faithful Unified Understanding and Generation Model for Remote Sensing](https://arxiv.org/abs/2601.17673)
*Weiyu Zhang,Yuan Hu,Yong Li,Yu Liu*

Main category: cs.CV

TL;DR: 提出针对遥感的统一多模态模型Uni - RS，解决遥感多模态模型在文本到图像生成中的空间不对称问题，实验证明其提升了文本到图像生成的空间保真度并保持多模态理解任务性能。


<details>
  <summary>Details</summary>
Motivation: 统一遥感多模态模型存在空间反转诅咒，在文本到图像生成中无法忠实执行空间关系，而空间关系是遥感核心语义信息。

Method: 引入显式的空间布局规划将文本指令转化为空间布局计划；施加空间感知查询监督使可学习查询偏向指令中的空间关系；开发图像 - 标题空间布局变体让模型接触几何一致的空间变换。

Result: 在多个基准测试的广泛实验表明，该方法大幅提高了文本到图像生成的空间保真度，同时在图像字幕、视觉定位和VQA等多模态理解任务上保持了良好性能。

Conclusion: 提出的Uni - RS能有效解决遥感多模态模型在文本到图像生成中的空间不对称问题。

Abstract: Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks.

</details>


### [540] [The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation](https://arxiv.org/abs/2601.17737)
*Chenyu Mu,Xin He,Qu Yang,Wanshun Chen,Jiadi Yao,Huang Liu,Zihao Yi,Bo Zhao,Xingyu Chen,Ruotian Ma,Fanghua Ye,Erkun Yang,Cheng Deng,Zhaopeng Tu,Xiaolong Li,Linus*

Main category: cs.CV

TL;DR: 提出对话到电影视频生成的端到端框架，可提升脚本忠实度和时间保真度，还揭示当前模型权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型难以从高级概念如对话生成连贯长叙事，存在创意与电影执行的语义鸿沟。

Method: 引入端到端框架，含ScripterAgent将对话转为脚本，构建ScriptBench基准，DirectorAgent用跨场景连续生成策略，还有AI CriticAgent和VSA指标评估。

Result: 框架显著提升所有测试视频模型的脚本忠实度和时间保真度。

Conclusion: 当前SOTA模型在视觉效果和严格遵循脚本间存在权衡，为自动化电影制作提供见解。

Abstract: Recent advances in video generation have produced models capable of synthesizing stunning visual content from simple text prompts. However, these models struggle to generate long-form, coherent narratives from high-level concepts like dialogue, revealing a ``semantic gap'' between a creative idea and its cinematic execution. To bridge this gap, we introduce a novel, end-to-end agentic framework for dialogue-to-cinematic-video generation. Central to our framework is ScripterAgent, a model trained to translate coarse dialogue into a fine-grained, executable cinematic script. To enable this, we construct ScriptBench, a new large-scale benchmark with rich multimodal context, annotated via an expert-guided pipeline. The generated script then guides DirectorAgent, which orchestrates state-of-the-art video models using a cross-scene continuous generation strategy to ensure long-horizon coherence. Our comprehensive evaluation, featuring an AI-powered CriticAgent and a new Visual-Script Alignment (VSA) metric, shows our framework significantly improves script faithfulness and temporal fidelity across all tested video models. Furthermore, our analysis uncovers a crucial trade-off in current SOTA models between visual spectacle and strict script adherence, providing valuable insights for the future of automated filmmaking.

</details>


### [541] [MV-S2V: Multi-View Subject-Consistent Video Generation](https://arxiv.org/abs/2601.17756)
*Ziyang Song,Xinyu Gong,Bangya Liu,Zelin Zhao*

Main category: cs.CV

TL;DR: 现有S2V方法局限于单视图，本文提出MV - S2V任务，用合成数据和TS - RoPE解决问题，实现高质量视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有S2V方法受单视图参考限制，无法充分发挥视频主体控制潜力，因此提出多视图S2V任务。

Method: 开发合成数据整理流程生成定制合成数据，结合小规模真实数据集；引入TS - RoPE区分不同主体和主体的不同视图。

Result: 框架在多视图参考图像上实现优越的3D主体一致性和高质量视觉输出。

Conclusion: 为主体驱动的视频生成建立了有意义的新方向。

Abstract: Existing Subject-to-Video Generation (S2V) methods have achieved high-fidelity and subject-consistent video generation, yet remain constrained to single-view subject references. This limitation renders the S2V task reducible to an S2I + I2V pipeline, failing to exploit the full potential of video subject control. In this work, we propose and address the challenging Multi-View S2V (MV-S2V) task, which synthesizes videos from multiple reference views to enforce 3D-level subject consistency. Regarding the scarcity of training data, we first develop a synthetic data curation pipeline to generate highly customized synthetic data, complemented by a small-scale real-world captured dataset to boost the training of MV-S2V. Another key issue lies in the potential confusion between cross-subject and cross-view references in conditional generation. To overcome this, we further introduce Temporally Shifted RoPE (TS-RoPE) to distinguish between different subjects and distinct views of the same subject in reference conditioning. Our framework achieves superior 3D subject consistency w.r.t. multi-view reference images and high-quality visual outputs, establishing a new meaningful direction for subject-driven video generation. Our project page is available at <a href="https://szy-young.github.io/mv-s2v">this URL</a>

</details>


### [542] [Inference-Time Loss-Guided Colour Preservation in Diffusion Sampling](https://arxiv.org/abs/2601.17259)
*Angad Singh Ahuja,Aarush Ram Anandh*

Main category: cs.CV

TL;DR: 提出无需额外训练的推理时区域约束颜色保留方法，可集成到标准Stable Diffusion修复流程。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散系统在精确颜色控制上存在问题，尤其是设计工作流中需满足用户指定颜色目标。

Method: 结合基于ROI的修复、背景潜在重施加和通过复合损失的潜在微调，损失构建考虑均值和像素误差分布尾部。

Result: 仅考虑均值的基线方法会产生局部失败，此方法可满足针对性颜色要求。

Conclusion: 该方法提供了无训练的目标颜色遵循机制，可集成到标准流程。

Abstract: Precise color control remains a persistent failure mode in text-to-image diffusion systems, particularly in design-oriented workflows where outputs must satisfy explicit, user-specified color targets. We present an inference-time, region-constrained color preservation method that steers a pretrained diffusion model without any additional training. Our approach combines (i) ROI-based inpainting for spatial selectivity, (ii) background-latent re-imposition to prevent color drift outside the ROI, and (iii) latent nudging via gradient guidance using a composite loss defined in CIE Lab and linear RGB. The loss is constructed to control not only the mean ROI color but also the tail of the pixelwise error distribution through CVaR-style and soft-maximum penalties, with a late-start gate and a time-dependent schedule to stabilize guidance across denoising steps. We show that mean-only baselines can satisfy average color constraints while producing perceptually salient local failures, motivating our distribution-aware objective. The resulting method provides a practical, training-free mechanism for targeted color adherence that can be integrated into standard Stable Diffusion inpainting pipelines.

</details>


### [543] [VidLaDA: Bidirectional Diffusion Large Language Models for Efficient Video Understanding](https://arxiv.org/abs/2601.17868)
*Zhihao He,Tieyuan Chen,Kangyu Wang,Ziran Qin,Yang Shao,Chaofan Gan,Shijie Li,Zuxuan Wu,Weiyao Lin*

Main category: cs.CV

TL;DR: 提出基于扩散语言模型的Video LLM VidLaDA及加速框架MARS - Cache，实验显示其性能佳且开源。


<details>
  <summary>Details</summary>
Motivation: 标准自回归视频大语言模型存在因果掩码偏差，阻碍全局时空建模，理解效率欠佳，且扩散解码存在推理瓶颈。

Method: 提出基于扩散语言模型利用双向注意力的VidLaDA，引入结合异步视觉缓存刷新与逐帧块注意力的MARS - Cache框架。

Result: VidLaDA性能超过扩散基线模型，媲美最先进的自回归模型，MARS - Cache加速超12倍且不影响推理准确性。

Conclusion: VidLaDA和MARS - Cache有效解决了现有视频大语言模型的问题，具有良好性能。

Abstract: Standard Autoregressive Video LLMs inevitably suffer from causal masking biases that hinder global spatiotemporal modeling, leading to suboptimal understanding efficiency. We propose VidLaDA, a Video LLM based on Diffusion Language Model utilizing bidirectional attention to capture bidirectional dependencies. To further tackle the inference bottleneck of diffusion decoding on massive video tokens, we introduce MARS-Cache. This framework accelerates inference by combining asynchronous visual cache refreshing with frame-wise chunk attention, effectively pruning redundancy while preserving global connectivity via anchor tokens. Extensive experiments show VidLaDA outperforms diffusion baselines and rivals state-of-the-art autoregressive models (e.g., Qwen2.5-VL and LLaVA-Video), with MARS-Cache delivering over 12x speedup without compromising reasoning accuracy. Code and checkpoints are open-sourced at https://github.com/ziHoHe/VidLaDA.

</details>


### [544] [PEAfowl: Perception-Enhanced Multi-View Vision-Language-Action for Bimanual Manipulation](https://arxiv.org/abs/2601.17885)
*Qingyu Fan,Zhaoxiang Li,Yi Lu,Wang Chen,Qiu Shen,Xiao-xiao Long,Yinghao Cai,Tao Lu,Shuo Wang,Xun Cao*

Main category: cs.CV

TL;DR: 提出PEAfowl用于杂乱场景双手操作，在成功率和仿真到现实迁移上有提升。


<details>
  <summary>Details</summary>
Motivation: 现有视觉 - 语言 - 动作模型在双手操作中因多视图特征融合和语言注入方式问题难以泛化。

Method: 为空间推理预测每令牌深度分布，进行可微分3D提升和局部跨视图邻居聚合；用Perceiver风格文本感知读出替换全局条件化；应用仅训练时的深度蒸馏。

Result: 在RoboTwin 2.0上成功率比最强基线提高23.0 pp，实机实验展示可靠仿真到现实迁移和深度蒸馏的持续改进。

Conclusion: PEAfowl能有效解决现有模型问题，在双手操作任务中有良好表现。

Abstract: Bimanual manipulation in cluttered scenes requires policies that remain stable under occlusions, viewpoint and scene variations. Existing vision-language-action models often fail to generalize because (i) multi-view features are fused via view-agnostic token concatenation, yielding weak 3D-consistent spatial understanding, and (ii) language is injected as global conditioning, resulting in coarse instruction grounding.
  In this paper, we introduce PEAfowl, a perception-enhanced multi-view VLA policy for bimanual manipulation. For spatial reasoning, PEAfowl predicts per-token depth distributions, performs differentiable 3D lifting, and aggregates local cross-view neighbors to form geometrically grounded, cross-view consistent representations. For instruction grounding, we propose to replace global conditioning with a Perceiver-style text-aware readout over frozen CLIP visual features, enabling iterative evidence accumulation. To overcome noisy and incomplete commodity depth without adding inference overhead, we apply training-only depth distillation from a pretrained depth teacher to supervise the depth-distribution head, providing perception front-end with geometry-aware priors.
  On RoboTwin 2.0 under domain-randomized setting, PEAfowl improves the strongest baseline by 23.0 pp in success rate, and real-robot experiments further demonstrate reliable sim-to-real transfer and consistent improvements from depth distillation.
  Project website: https://peafowlvla.github.io/.

</details>


### [545] [OTI: A Model-free and Visually Interpretable Measure of Image Attackability](https://arxiv.org/abs/2601.17536)
*Jiaming Liang,Haowei Liu,Chi-Man Pun*

Main category: cs.CV

TL;DR: 提出无模型且具视觉可解释性的图像可攻击性度量OTI，实验证明其有效且计算高效。


<details>
  <summary>Details</summary>
Motivation: 现有评估图像可攻击性的方法稀缺，且依赖模型代理、缺乏视觉可解释性，不能满足实际需求。

Method: 提出Object Texture Intensity (OTI)，从决策边界和对抗扰动的中高频特征角度阐述其原理。

Result: 综合实验表明OTI有效且计算高效，能为对抗机器学习社区提供可攻击性的视觉理解。

Conclusion: OTI是一种有效且实用的图像可攻击性度量方法。

Abstract: Despite the tremendous success of neural networks, benign images can be corrupted by adversarial perturbations to deceive these models. Intriguingly, images differ in their attackability. Specifically, given an attack configuration, some images are easily corrupted, whereas others are more resistant. Evaluating image attackability has important applications in active learning, adversarial training, and attack enhancement. This prompts a growing interest in developing attackability measures. However, existing methods are scarce and suffer from two major limitations: (1) They rely on a model proxy to provide prior knowledge (e.g., gradients or minimal perturbation) to extract model-dependent image features. Unfortunately, in practice, many task-specific models are not readily accessible. (2) Extracted features characterizing image attackability lack visual interpretability, obscuring their direct relationship with the images. To address these, we propose a novel Object Texture Intensity (OTI), a model-free and visually interpretable measure of image attackability, which measures image attackability as the texture intensity of the image's semantic object. Theoretically, we describe the principles of OTI from the perspectives of decision boundaries as well as the mid- and high-frequency characteristics of adversarial perturbations. Comprehensive experiments demonstrate that OTI is effective and computationally efficient. In addition, our OTI provides the adversarial machine learning community with a visual understanding of attackability.

</details>


### [546] [From Specialist to Generalist: Unlocking SAM's Learning Potential on Unlabeled Medical Images](https://arxiv.org/abs/2601.17934)
*Vi Vu,Thanh-Huy Nguyen,Tien-Thinh Nguyen,Ba-Thinh Lam,Hoang-Thien Nguyen,Tianyang Wang,Xingjian Li,Min Xu*

Main category: cs.CV

TL;DR: 介绍SC - SAM框架，通过U - Net和SAM双向合作利用无标签数据提升医学图像分割效果，在相关基准测试中取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 基础模型适应医学图像因领域差异、标签稀缺和PEFT无法利用无标签数据而困难，传统模型U - Net辅助PEFT SAM的潜力被忽视。

Method: 提出SC - SAM框架，U - Net提供提示和伪标签引导SAM适应，SAM作为监督者规范U - Net，形成双向共训练循环利用无标签数据。

Result: 在前列腺MRI和息肉分割基准测试中取得SOTA结果，优于现有半监督SAM变体和医学基础模型。

Conclusion: 专家-通才合作对标签高效的医学图像分割有价值。

Abstract: Foundation models like the Segment Anything Model (SAM) show strong generalization, yet adapting them to medical images remains difficult due to domain shift, scarce labels, and the inability of Parameter-Efficient Fine-Tuning (PEFT) to exploit unlabeled data. While conventional models like U-Net excel in semi-supervised medical learning, their potential to assist a PEFT SAM has been largely overlooked. We introduce SC-SAM, a specialist-generalist framework where U-Net provides point-based prompts and pseudo-labels to guide SAM's adaptation, while SAM serves as a powerful generalist supervisor to regularize U-Net. This reciprocal guidance forms a bidirectional co-training loop that allows both models to effectively exploit the unlabeled data. Across prostate MRI and polyp segmentation benchmarks, our method achieves state-of-the-art results, outperforming other existing semi-supervised SAM variants and even medical foundation models like MedSAM, highlighting the value of specialist-generalist cooperation for label-efficient medical image segmentation. Our code is available at https://github.com/vnlvi2k3/SC-SAM.

</details>


### [547] [Stylizing ViT: Anatomy-Preserving Instance Style Transfer for Domain Generalization](https://arxiv.org/abs/2601.17586)
*Sebastian Doerrich,Francesco Di Salvo,Jonas Alle,Christian Ledig*

Main category: cs.CV

TL;DR: 提出 Stylizing ViT 以解决医学图像分析中深度学习模型泛化问题，测试显示其有效且无伪影，代码开源。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像分析中深度学习模型因数据异质性和稀缺性，在不同领域和人群中泛化能力差的问题，现有增强方法存在局限性。

Method: 提出 Stylizing ViT，利用权重共享注意力模块进行自注意力和交叉注意力，实现风格迁移和保持解剖一致性，并用于三个图像分类任务进行数据增强。

Result: 相比现有技术提升了模型鲁棒性（准确率最多提高 13%），生成无伪影的可信图像，推理时用于测试时间增强性能提升 17%。

Conclusion: Stylizing ViT 能有效提高医学图像分析中深度学习模型的泛化能力。

Abstract: Deep learning models in medical image analysis often struggle with generalizability across domains and demographic groups due to data heterogeneity and scarcity. Traditional augmentation improves robustness, but fails under substantial domain shifts. Recent advances in stylistic augmentation enhance domain generalization by varying image styles but fall short in terms of style diversity or by introducing artifacts into the generated images. To address these limitations, we propose Stylizing ViT, a novel Vision Transformer encoder that utilizes weight-shared attention blocks for both self- and cross-attention. This design allows the same attention block to maintain anatomical consistency through self-attention while performing style transfer via cross-attention. We assess the effectiveness of our method for domain generalization by employing it for data augmentation on three distinct image classification tasks in the context of histopathology and dermatology. Results demonstrate an improved robustness (up to +13% accuracy) over the state of the art while generating perceptually convincing images without artifacts. Additionally, we show that Stylizing ViT is effective beyond training, achieving a 17% performance improvement during inference when used for test-time augmentation. The source code is available at https://github.com/sdoerrich97/stylizing-vit .

</details>


### [548] [Leveraging Persistence Image to Enhance Robustness and Performance in Curvilinear Structure Segmentation](https://arxiv.org/abs/2601.18045)
*Zhuangzhi Gao,Feixiang Zhou,He Zhao,Xiuju Chen,Xiaoxin Li,Qinkai Yu,Yitian Zhao,Alena Shantsila,Gregory Y. H. Lip,Eduard Shantsila,Yalin Zheng*

Main category: cs.CV

TL;DR: 本文提出PIs - Regressor模块和Topology SegNet，将拓扑信息融入网络结构，提升医学图像曲线结构分割性能，在三个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有提取和嵌入拓扑属性方法存在非可微性、计算成本高、通用差等问题，需更好方法将拓扑信息融入分割。

Method: 提出PIs - Regressor模块学习持久图像，结合Topology SegNet在上下采样阶段融合特征，将拓扑信息融入网络结构。

Result: 实验表明集成拓扑特征增强了模型鲁棒性，能处理医学成像中的过曝和模糊问题，在三个基准测试中像素级准确率和拓扑保真度达最优。

Conclusion: 所提方法可直接将拓扑信息融入网络结构，更鲁棒且灵活，能与其他拓扑方法结合提升分割性能。

Abstract: Segmenting curvilinear structures in medical images is essential for analyzing morphological patterns in clinical applications. Integrating topological properties, such as connectivity, improves segmentation accuracy and consistency. However, extracting and embedding such properties - especially from Persistence Diagrams (PD) - is challenging due to their non-differentiability and computational cost. Existing approaches mostly encode topology through handcrafted loss functions, which generalize poorly across tasks. In this paper, we propose PIs-Regressor, a simple yet effective module that learns persistence image (PI) - finite, differentiable representations of topological features - directly from data. Together with Topology SegNet, which fuses these features in both downsampling and upsampling stages, our framework integrates topology into the network architecture itself rather than auxiliary losses. Unlike existing methods that depend heavily on handcrafted loss functions, our approach directly incorporates topological information into the network structure, leading to more robust segmentation. Our design is flexible and can be seamlessly combined with other topology-based methods to further enhance segmentation performance. Experimental results show that integrating topological features enhances model robustness, effectively handling challenges like overexposure and blurring in medical imaging. Our approach on three curvilinear benchmarks demonstrate state-of-the-art performance in both pixel-level accuracy and topological fidelity.

</details>


### [549] [LungCRCT: Causal Representation based Lung CT Processing for Lung Cancer Treatment](https://arxiv.org/abs/2601.18118)
*Daeyoung Kim*

Main category: cs.CV

TL;DR: 肺癌早期无症状难发现，LDCT结合AI模型检测有进展但DL模型有局限，本文提出LungCRCT框架用于肺癌分析，在肿瘤分类任务表现好。


<details>
  <summary>Details</summary>
Motivation: 肺癌早期难发现，现有深度学习模型在肺癌治疗分析和因果干预分析模拟有局限，需新方法提升肺癌患者生存率。

Method: 引入基于潜在因果表示学习的LungCRCT框架，使用基于图自动编码器的因果发现算法、距离相关解缠和基于熵的图像重建细化。

Result: LungCRCT可进行肺癌治疗因果干预分析，在恶性肿瘤分类任务中AUC得分为93.91%。

Conclusion: LungCRCT框架有效，在肺癌分析和肿瘤分类任务有良好表现。

Abstract: Due to silence in early stages, lung cancer has been one of the most leading causes of mortality in cancer patients world-wide. Moreover, major symptoms of lung cancer are hard to differentiate with other respiratory disease symptoms such as COPD, further leading patients to overlook cancer progression in early stages. Thus, to enhance survival rates in lung cancer, early detection from consistent proactive respiratory system monitoring becomes crucial. One of the most prevalent and effective methods for lung cancer monitoring would be low-dose computed tomography(LDCT) chest scans, which led to remarkable enhancements in lung cancer detection or tumor classification tasks under rapid advancements and applications of computer vision based AI models such as EfficientNet or ResNet in image processing. However, though advanced CNN models under transfer learning or ViT based models led to high performing lung cancer detections, due to its intrinsic limitations in terms of correlation dependence and low interpretability due to complexity, expansions of deep learning models to lung cancer treatment analysis or causal intervention analysis simulations are still limited. Therefore, this research introduced LungCRCT: a latent causal representation learning based lung cancer analysis framework that retrieves causal representations of factors within the physical causal mechanism of lung cancer progression. With the use of advanced graph autoencoder based causal discovery algorithms with distance Correlation disentanglement and entropy-based image reconstruction refinement, LungCRCT not only enables causal intervention analysis for lung cancer treatments, but also leads to robust, yet extremely light downstream models in malignant tumor classification tasks with an AUC score of 93.91%.

</details>


### [550] [\textsc{NaVIDA}: Vision-Language Navigation with Inverse Dynamics Augmentation](https://arxiv.org/abs/2601.18188)
*Weiye Zhu,Zekai Zhang,Xiangchen Wang,Hewei Pan,Teng Wang,Tiantian Geng,Rongtao Xu,Feng Zheng*

Main category: cs.CV

TL;DR: 提出NaVIDA框架解决VLN任务中缺乏视觉 - 动作因果关系问题，实验表明其性能优且参数少，现实机器人评估验证可行性。


<details>
  <summary>Details</summary>
Motivation: 现有VLN方法缺乏视觉 - 动作因果关系建模，导致不稳定行为、泛化能力弱和累积误差。

Method: 引入NaVIDA框架，结合策略学习与基于动作的视觉动力学和自适应执行，采用基于块的逆动力学监督和分层概率动作分块，用熵引导机制自适应设置执行范围。

Result: NaVIDA在导航性能上优于现有方法，且参数更少，现实机器人评估验证有效。

Conclusion: NaVIDA框架能有效解决VLN任务中缺乏因果关系的问题，具有实际应用价值。

Abstract: Vision-and-Language Navigation (VLN) requires agents to interpret natural language instructions and act coherently in visually rich environments. However, most existing methods rely on reactive state-action mappings without explicitly modeling how actions causally transform subsequent visual observations. Lacking such vision-action causality, agents cannot anticipate the visual changes induced by its own actions, leading to unstable behaviors, weak generalization, and cumulative error along trajectory. To address these issues, we introduce \textsc{NaVIDA} (\textbf{Nav}igation with \textbf{I}nverse \textbf{D}ynamics \textbf{A}ugmentation), a unified VLN framework that couples policy learning with action-grounded visual dynamics and adaptive execution. \textsc{NaVIDA} augments training with chunk-based inverse-dynamics supervision to learn causal relationship between visual changes and corresponding actions. To structure this supervision and extend the effective planning range, \textsc{NaVIDA} employs hierarchical probabilistic action chunking (HPAC), which organizes trajectories into multi-step chunks and provides discriminative, longer-range visual-change cues. To further curb error accumulation and stabilize behavior at inference, an entropy-guided mechanism adaptively sets the execution horizon of action chunks. Extensive experiments show that \textsc{NaVIDA} achieves superior navigation performance compared to state-of-the-art methods with fewer parameters (3B vs. 8B). Real-world robot evaluations further validate the practical feasibility and effectiveness of our approach. Code and data will be available upon acceptance.

</details>


### [551] [A multimodal vision foundation model for generalizable knee pathology](https://arxiv.org/abs/2601.18250)
*Kang Yu,Dingyu Wang,Zimu Yuan,Nan Zhou,Jiajun Liu,Jiaxin Liu,Shanggui Liu,Yaoyan Zheng,Huishu Yuan,Di Huang,Dong Jiang*

Main category: cs.CV

TL;DR: 介绍用于肌肉骨骼病理学的多模态视觉基础模型OrthoFoundation，表现出色并克服传统模型局限。


<details>
  <summary>Details</summary>
Motivation: 现有骨科AI方法存在碎片化、需大量标注数据和缺乏泛化性等问题，且缺乏大规模肌肉骨骼数据集。

Method: 构建120万张未标注的膝关节X射线和MRI图像预训练数据集，用Dinov3骨干通过自监督对比学习训练模型。

Result: 在14个下游任务中达SOTA，X射线骨关节炎诊断和MRI结构损伤检测表现佳，有高标签效率，跨解剖结构泛化能力强。

Conclusion: OrthoFoundation是肌肉骨骼成像通用AI的重要进展，可减少标注负担、提高诊断准确性。

Abstract: Musculoskeletal disorders represent a leading cause of global disability, creating an urgent demand for precise interpretation of medical imaging. Current artificial intelligence (AI) approaches in orthopedics predominantly rely on task-specific, supervised learning paradigms. These methods are inherently fragmented, require extensive annotated datasets, and often lack generalizability across different modalities and clinical scenarios. The development of foundation models in this field has been constrained by the scarcity of large-scale, curated, and open-source musculoskeletal datasets. To address these challenges, we introduce OrthoFoundation, a multimodal vision foundation model optimized for musculoskeletal pathology. We constructed a pre-training dataset of 1.2 million unlabeled knee X-ray and MRI images from internal and public databases. Utilizing a Dinov3 backbone, the model was trained via self-supervised contrastive learning to capture robust radiological representations. OrthoFoundation achieves state-of-the-art (SOTA) performance across 14 downstream tasks. It attained superior accuracy in X-ray osteoarthritis diagnosis and ranked first in MRI structural injury detection. The model demonstrated remarkable label efficiency, matching supervised baselines using only 50% of labeled data. Furthermore, despite being pre-trained on knee images, OrthoFoundation exhibited exceptional cross-anatomy generalization to the hip, shoulder, and ankle. OrthoFoundation represents a significant advancement toward general-purpose AI for musculoskeletal imaging. By learning fundamental, joint-agnostic radiological semantics from large-scale multimodal data, it overcomes the limitations of conventional models, which provides a robust framework for reducing annotation burdens and enhancing diagnostic accuracy in clinical practice.

</details>


### [552] [Agentic Very Long Video Understanding](https://arxiv.org/abs/2601.18157)
*Aniket Rege,Arka Sadhu,Yuliang Li,Kejie Li,Ramya Korlakai Vinayak,Yuning Chai,Yong Jae Lee,Hyo Jin Kim*

Main category: cs.CV

TL;DR: 提出EGAgent框架解决长时视频理解挑战，实验取得良好效果


<details>
  <summary>Details</summary>
Motivation: 始终在线的个人AI助手需要长时视频理解能力，但现有方法有局限

Method: 提出以实体场景图为中心的EGAgent框架，为规划代理配备结构化搜索、推理工具和混合视听搜索能力

Result: 在EgoLifeQA和Video - MME (Long)数据集上，分别取得57.5%和74.1%的成绩

Conclusion: EGAgent框架能实现详细、跨模态和时间连贯的推理，在复杂长时视频理解任务上表现良好

Abstract: The advent of always-on personal AI assistants, enabled by all-day wearable devices such as smart glasses, demands a new level of contextual understanding, one that goes beyond short, isolated events to encompass the continuous, longitudinal stream of egocentric video. Achieving this vision requires advances in long-horizon video understanding, where systems must interpret and recall visual and audio information spanning days or even weeks. Existing methods, including large language models and retrieval-augmented generation, are constrained by limited context windows and lack the ability to perform compositional, multi-hop reasoning over very long video streams. In this work, we address these challenges through EGAgent, an enhanced agentic framework centered on entity scene graphs, which represent people, places, objects, and their relationships over time. Our system equips a planning agent with tools for structured search and reasoning over these graphs, as well as hybrid visual and audio search capabilities, enabling detailed, cross-modal, and temporally coherent reasoning. Experiments on the EgoLifeQA and Video-MME (Long) datasets show that our method achieves state-of-the-art performance on EgoLifeQA (57.5%) and competitive performance on Video-MME (Long) (74.1%) for complex longitudinal video understanding tasks.

</details>


### [553] [Facial Emotion Recognition on FER-2013 using an EfficientNetB2-Based Approach](https://arxiv.org/abs/2601.18228)
*Sahil Naik,Soham Bagayatkar,Pavankumar Singh*

Main category: cs.CV

TL;DR: 提出基于EfficientNetB2的轻量级面部情绪识别管道，解决现实场景面部情绪检测难题，参数少且准确率高，适用于实时和边缘应用。


<details>
  <summary>Details</summary>
Motivation: 现实场景面部图像情绪检测任务困难，现有大CNN方法计算和内存开销大，不适用于实时应用。

Method: 采用基于EfficientNetB2的轻量级管道，使用两阶段预热和微调策略训练，结合AdamW优化、解耦权重衰减、标签平滑、裁剪类权重等技术。

Result: 模型在测试集上准确率达68.78%，参数比VGG16基线少近十倍，实验显示训练稳定、泛化性强。

Conclusion: 所提方法适用于实时和边缘应用。

Abstract: Detection of human emotions based on facial images in real-world scenarios is a difficult task due to low image quality, variations in lighting, pose changes, background distractions, small inter-class variations, noisy crowd-sourced labels, and severe class imbalance, as observed in the FER-2013 dataset of 48x48 grayscale images. Although recent approaches using large CNNs such as VGG and ResNet achieve reasonable accuracy, they are computationally expensive and memory-intensive, limiting their practicality for real-time applications. We address these challenges using a lightweight and efficient facial emotion recognition pipeline based on EfficientNetB2, trained using a two-stage warm-up and fine-tuning strategy. The model is enhanced with AdamW optimization, decoupled weight decay, label smoothing (epsilon = 0.06) to reduce annotation noise, and clipped class weights to mitigate class imbalance, along with dropout, mixed-precision training, and extensive real-time data augmentation. The model is trained using a stratified 87.5%/12.5% train-validation split while keeping the official test set intact, achieving a test accuracy of 68.78% with nearly ten times fewer parameters than VGG16-based baselines. Experimental results, including per-class metrics and learning dynamics, demonstrate stable training and strong generalization, making the proposed approach suitable for real-time and edge-based applications.

</details>


### [554] [A Tumor Aware DenseNet Swin Hybrid Learning with Boosted and Hierarchical Feature Spaces for Large-Scale Brain MRI Classification](https://arxiv.org/abs/2601.18330)
*Muhammad Ali Shah,Muhammad Mansoor Alam,Saddam Hussain Khan*

Main category: cs.CV

TL;DR: 提出EDSH框架用于脑肿瘤MRI分析，设两种实验设置，在大规模数据集上表现优于其他模型，准确率和召回率达98.50。


<details>
  <summary>Details</summary>
Motivation: 联合捕捉脑肿瘤MRI的细粒度纹理模式和长距离上下文依赖，解决特定类别的诊断挑战。

Method: 提出EDSH框架，设置两种实验：一是用BFS使DenseNet和Swint分支学习互补特征并融合提升；二是采用带DFE和DR的分层架构，DenseNet学局部特征，Swint学全局形态。同时定制DenseNet和Swint。

Result: 在大规模MRI数据集上评估，表现始终优于独立CNN、视觉Transformer和混合模型，在未见测试数据集上准确率和召回率达98.50。

Conclusion: EDSH框架在脑肿瘤MRI分析中有效且性能优越。

Abstract: This study proposes an efficient Densely Swin Hybrid (EDSH) framework for brain tumor MRI analysis, designed to jointly capture fine grained texture patterns and long range contextual dependencies. Two tumor aware experimental setups are introduced to address class-specific diagnostic challenges. The first setup employs a Boosted Feature Space (BFS), where independently customized DenseNet and Swint branches learn complementary local and global representations that are dimension aligned, fused, and boosted, enabling highly sensitive detection of diffuse glioma patterns by successfully learning the features of irregular shape, poorly defined mass, and heterogeneous texture. The second setup adopts a hierarchical DenseNet Swint architecture with Deep Feature Extraction have Dual Residual connections (DFE and DR), in which DenseNet serves as a stem CNN for structured local feature learning, while Swin_t models global tumor morphology, effectively suppressing false negatives in meningioma and pituitary tumor classification by learning the features of well defined mass, location (outside brain) and enlargments in tumors (dural tail or upward extension). DenseNet is customized at the input level to match MRI spatial characteristics, leveraging dense residual connectivity to preserve texture information and mitigate vanishing-gradient effects. In parallel, Swint is tailored through task aligned patch embedding and shifted-window self attention to efficiently capture hierarchical global dependencies. Extensive evaluation on a large-scale MRI dataset (stringent 40,260 images across four tumor classes) demonstrates consistent superiority over standalone CNNs, Vision Transformers, and hybrids, achieving 98.50 accuracy and recall on the test unseen dataset.

</details>


### [555] [3DGesPolicy: Phoneme-Aware Holistic Co-Speech Gesture Generation Based on Action Control](https://arxiv.org/abs/2601.18451)
*Xuanmeng Sha,Liyun Zhang,Tomohiro Mashita,Naoya Chiba,Yuki Uranishi*

Main category: cs.CV

TL;DR: 提出3DGesPolicy框架，通过扩散策略将整体手势生成转化为连续轨迹控制问题，并使用GAP融合模块，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有部分分解或帧级回归方法导致整体协同语音手势生成存在语义不协调和空间不稳定问题。

Method: 引入3DGesPolicy框架，将整体手势生成转化为连续轨迹控制问题，还提出GAP融合模块整合和细化多模态信号。

Result: 在BEAT2数据集上的大量实验表明，3DGesPolicy在生成自然、富有表现力且与语音高度对齐的整体手势方面优于其他先进方法。

Conclusion: 3DGesPolicy框架在整体协同语音手势生成方面有效。

Abstract: Generating holistic co-speech gestures that integrate full-body motion with facial expressions suffers from semantically incoherent coordination on body motion and spatially unstable meaningless movements due to existing part-decomposed or frame-level regression methods, We introduce 3DGesPolicy, a novel action-based framework that reformulates holistic gesture generation as a continuous trajectory control problem through diffusion policy from robotics. By modeling frame-to-frame variations as unified holistic actions, our method effectively learns inter-frame holistic gesture motion patterns and ensures both spatially and semantically coherent movement trajectories that adhere to realistic motion manifolds. To further bridge the gap in expressive alignment, we propose a Gesture-Audio-Phoneme (GAP) fusion module that can deeply integrate and refine multi-modal signals, ensuring structured and fine-grained alignment between speech semantics, body motion, and facial expressions. Extensive quantitative and qualitative experiments on the BEAT2 dataset demonstrate the effectiveness of our 3DGesPolicy across other state-of-the-art methods in generating natural, expressive, and highly speech-aligned holistic gestures.

</details>


### [556] [On Procrustes Contamination in Machine Learning Applications of Geometric Morphometrics](https://arxiv.org/abs/2601.18448)
*Lloyd Austin Courtenay*

Main category: cs.CV

TL;DR: 本文研究几何形态测量学（GMM）用于机器学习分析时GPA导致的污染问题，提出新的重新对齐程序，明确GMM机器学习应用中预处理的重要性。


<details>
  <summary>Details</summary>
Motivation: 标准做法在数据分割前通过广义普罗克拉斯提斯分析（GPA）对齐所有样本，可能引入统计依赖并污染下游预测模型，需研究其影响。

Method: 使用可控的2D和3D模拟，研究不同样本大小、地标密度和异速生长模式下GPA诱导污染的影响，提出新的重新对齐程序。

Result: 模拟揭示样本大小与地标空间中的“对角线”，展示地标间空间自相关的重要性，忽略地标关系会导致性能下降。

Conclusion: GMM的机器学习应用需要仔细的预处理，提供了重新对齐的实用指南，明确了普罗克拉斯提斯形状空间固有的基本统计约束。

Abstract: Geometric morphometrics (GMM) is widely used to quantify shape variation, more recently serving as input for machine learning (ML) analyses. Standard practice aligns all specimens via Generalized Procrustes Analysis (GPA) prior to splitting data into training and test sets, potentially introducing statistical dependence and contaminating downstream predictive models. Here, the effects of GPA-induced contamination are formally characterised using controlled 2D and 3D simulations across varying sample sizes, landmark densities, and allometric patterns. A novel realignment procedure is proposed, whereby test specimens are aligned to the training set prior to model fitting, eliminating cross-sample dependency. Simulations reveal a robust "diagonal" in sample-size vs. landmark-space, reflecting the scaling of RMSE under isotropic variation, with slopes analytically derived from the degrees of freedom in Procrustes tangent space. The importance of spatial autocorrelation among landmarks is further demonstrated using linear and convolutional regression models, highlighting performance degradation when landmark relationships are ignored. This work establishes the need for careful preprocessing in ML applications of GMM, provides practical guidelines for realignment, and clarifies fundamental statistical constraints inherent to Procrustes shape space.

</details>


### [557] [Fair-Eye Net: A Fair, Trustworthy, Multimodal Integrated Glaucoma Full Chain AI System](https://arxiv.org/abs/2601.18464)
*Wenbin Wei,Suyuan Yao,Cheng Huang,Xiangyu Gao*

Main category: cs.CV

TL;DR: 开发Fair - Eye Net多模态AI系统用于青光眼筛查、随访和风险预警，实验效果好，优化公平性并具临床可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前青光眼筛查和进展评估方法存在主观性和护理碎片化问题，且高质量成像工具和专家资源有限，影响一致性和公平性。

Method: 开发Fair - Eye Net系统，采用双流异构融合架构集成多模态数据，结合不确定性感知分层门控策略，加入公平性约束。

Result: AUC达0.912（特异性96.7%），减少73.4%种族假阴性差异，跨领域性能稳定，能进行3 - 12个月早期风险预警（敏感性92%，特异性88%）。

Conclusion: Fair - Eye Net以公平性为主要目标优化，具有临床可靠性，为临床转化和大规模部署提供可行路径，推动全球眼健康公平。

Abstract: Glaucoma is a top cause of irreversible blindness globally, making early detection and longitudinal follow-up pivotal to preventing permanent vision loss. Current screening and progression assessment, however, rely on single tests or loosely linked examinations, introducing subjectivity and fragmented care. Limited access to high-quality imaging tools and specialist expertise further compromises consistency and equity in real-world use. To address these gaps, we developed Fair-Eye Net, a fair, reliable multimodal AI system closing the clinical loop from glaucoma screening to follow-up and risk alerting. It integrates fundus photos, OCT structural metrics, VF functional indices, and demographic factors via a dual-stream heterogeneous fusion architecture, with an uncertainty-aware hierarchical gating strategy for selective prediction and safe referral. A fairness constraint reduces missed diagnoses in disadvantaged subgroups. Experimental results show it achieved an AUC of 0.912 (96.7% specificity), cut racial false-negativity disparity by 73.4% (12.31% to 3.28%), maintained stable cross-domain performance, and enabled 3-12 months of early risk alerts (92% sensitivity, 88% specificity). Unlike post hoc fairness adjustments, Fair-Eye Net optimizes fairness as a primary goal with clinical reliability via multitask learning, offering a reproducible path for clinical translation and large-scale deployment to advance global eye health equity.

</details>


### [558] [From Cold Start to Active Learning: Embedding-Based Scan Selection for Medical Image Segmentation](https://arxiv.org/abs/2601.18532)
*Devon Levy,Bar Assayag,Laura Gaspar,Ilan Shimshoni,Bella Specktor-Fadida*

Main category: cs.CV

TL;DR: 提出结合基础模型嵌入和聚类的新冷启动采样策略和集成空间多样性的主动学习框架，在多个数据集上提升分割精度。


<details>
  <summary>Details</summary>
Motivation: 手动标注耗时且依赖专业知识，是疾病监测精准分割标注的瓶颈，主动学习可减轻标注负担。

Method: 提出新冷启动采样策略，随后使用集成空间多样性的基于不确定性的主动学习框架。

Result: 在多个数据集上，冷启动策略和主动学习框架均提升了分割精度，如提升Dice系数、降低Hausdorff距离。

Conclusion: 所提框架在低数据情况下稳定优于基线方法，提高了分割精度。

Abstract: Accurate segmentation annotations are critical for disease monitoring, yet manual labeling remains a major bottleneck due to the time and expertise required. Active learning (AL) alleviates this burden by prioritizing informative samples for annotation, typically through a diversity-based cold-start phase followed by uncertainty-driven selection. We propose a novel cold-start sampling strategy that combines foundation-model embeddings with clustering, including automatic selection of the number of clusters and proportional sampling across clusters, to construct a diverse and representative initial training. This is followed by an uncertainty-based AL framework that integrates spatial diversity to guide sample selection. The proposed method is intuitive and interpretable, enabling visualization of the feature-space distribution of candidate samples. We evaluate our approach on three datasets spanning X-ray and MRI modalities. On the CheXmask dataset, the cold-start strategy outperforms random selection, improving Dice from 0.918 to 0.929 and reducing the Hausdorff distance from 32.41 to 27.66 mm. In the AL setting, combined entropy and diversity selection improves Dice from 0.919 to 0.939 and reduces the Hausdorff distance from 30.10 to 19.16 mm. On the Montgomery dataset, cold-start gains are substantial, with Dice improving from 0.928 to 0.950 and Hausdorff distance decreasing from 14.22 to 9.38 mm. On the SynthStrip dataset, cold-start selection slightly affects Dice but reduces the Hausdorff distance from 9.43 to 8.69 mm, while active learning improves Dice from 0.816 to 0.826 and reduces the Hausdorff distance from 7.76 to 6.38 mm. Overall, the proposed framework consistently outperforms baseline methods in low-data regimes, improving segmentation accuracy.

</details>


### [559] [Generative Diffusion Augmentation with Quantum-Enhanced Discrimination for Medical Image Diagnosis](https://arxiv.org/abs/2601.18556)
*Jingsong Xia,Siqi Wang*

Main category: cs.CV

TL;DR: 本文提出SDA - QEC框架解决医学图像分类中类不平衡问题，实验显示其性能优于经典基线模型，验证了生成增强与量子增强建模结合的可行性。


<details>
  <summary>Details</summary>
Motivation: 现实世界医学数据集中存在严重的类不平衡问题，导致模型有偏差、少数类召回率低，影响诊断准确性并带来误诊风险。

Method: 提出SDA - QEC框架，使用轻量级扩散增强器为少数类生成合成样本以重新平衡训练分布，在MobileNetV2架构中嵌入量子特征层增强模型判别能力。

Result: 在冠状动脉造影图像分类实验中，SDA - QEC准确率达98.33%，AUC为98.78%，F1分数为98.33%，灵敏度和特异度均为98.33%，显著优于经典基线模型。

Conclusion: 该方法验证了在实际医学成像任务中集成生成增强和量子增强建模的可行性，为小样本、高度不平衡和高风险诊断场景下开发高可靠医学AI系统提供新途径。

Abstract: In biomedical engineering, artificial intelligence has become a pivotal tool for enhancing medical diagnostics, particularly in medical image classification tasks such as detecting pneumonia from chest X-rays and breast cancer screening. However, real-world medical datasets frequently exhibit severe class imbalance, where positive samples substantially outnumber negative samples, leading to biased models with low recall rates for minority classes. This imbalance not only compromises diagnostic accuracy but also poses clinical misdiagnosis risks. To address this challenge, we propose SDA-QEC (Simplified Diffusion Augmentation with Quantum-Enhanced Classification), an innovative framework that integrates simplified diffusion-based data augmentation with quantum-enhanced feature discrimination. Our approach employs a lightweight diffusion augmentor to generate high-quality synthetic samples for minority classes, rebalancing the training distribution. Subsequently, a quantum feature layer embedded within MobileNetV2 architecture enhances the model's discriminative capability through high-dimensional feature mapping in Hilbert space. Comprehensive experiments on coronary angiography image classification demonstrate that SDA-QEC achieves 98.33% accuracy, 98.78% AUC, and 98.33% F1-score, significantly outperforming classical baselines including ResNet18, MobileNetV2, DenseNet121, and VGG16. Notably, our framework simultaneously attains 98.33% sensitivity and 98.33% specificity, achieving a balanced performance critical for clinical deployment. The proposed method validates the feasibility of integrating generative augmentation with quantum-enhanced modeling in real-world medical imaging tasks, offering a novel research pathway for developing highly reliable medical AI systems in small-sample, highly imbalanced, and high-risk diagnostic scenarios.

</details>


### [560] [Self-Refining Video Sampling](https://arxiv.org/abs/2601.18577)
*Sangwon Jang,Taekyung Ki,Jaehyeong Jo,Saining Xie,Jaehong Yoon,Sung Ju Hwang*

Main category: cs.CV

TL;DR: 提出自精炼视频采样方法，无需外部验证器或额外训练，实验显示在运动连贯性和物理一致性上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现代视频生成器在复杂物理动力学上表现不佳，现有方法计算成本高且难以捕捉细粒度运动。

Method: 将预训练视频生成器作为自身的自精炼器，将其解释为去噪自编码器，在推理时进行迭代内循环精炼，并引入不确定性感知精炼策略。

Result: 在最先进的视频生成器上的实验表明，运动连贯性和物理一致性显著提高，相比默认采样器和基于引导的采样器，获得超70%的人类偏好。

Conclusion: 所提出的自精炼视频采样方法能有效提升视频生成的物理真实性。

Abstract: Modern video generators still struggle with complex physical dynamics, often falling short of physical realism. Existing approaches address this using external verifiers or additional training on augmented data, which is computationally expensive and still limited in capturing fine-grained motion. In this work, we present self-refining video sampling, a simple method that uses a pre-trained video generator trained on large-scale datasets as its own self-refiner. By interpreting the generator as a denoising autoencoder, we enable iterative inner-loop refinement at inference time without any external verifier or additional training. We further introduce an uncertainty-aware refinement strategy that selectively refines regions based on self-consistency, which prevents artifacts caused by over-refinement. Experiments on state-of-the-art video generators demonstrate significant improvements in motion coherence and physics alignment, achieving over 70\% human preference compared to the default sampler and guidance-based sampler.

</details>


### [561] [Low Cost, High Efficiency: LiDAR Place Recognition in Vineyards with Matryoshka Representation Learning](https://arxiv.org/abs/2601.18714)
*Judith Vilella-Cantos,Mauro Martini,Marcello Chiaberge,Mónica Ballesta,David Valiente*

Main category: cs.CV

TL;DR: 提出MinkUNeXt - VINE方法用于葡萄园环境定位，在效率和性能上表现良好，代码公开。


<details>
  <summary>Details</summary>
Motivation: 农业环境非结构化且缺少显著地标，现有移动机器人的地点识别任务有挑战，需要高效定位方法。

Method: 提出MinkUNeXt - VINE方法，运用预处理和Matryoshka Representation Learning多损失方法，优先处理低成本稀疏激光雷达输入和低维输出。

Result: 各种评估案例及两个长期葡萄园数据集结果表明，该方法输出权衡效率高，对低成本和低分辨率输入数据性能稳健。

Conclusion: MinkUNeXt - VINE在葡萄园环境定位上效果好，能高效处理低成本低分辨率数据。

Abstract: Localization in agricultural environments is challenging due to their unstructured nature and lack of distinctive landmarks. Although agricultural settings have been studied in the context of object classification and segmentation, the place recognition task for mobile robots is not trivial in the current state of the art. In this study, we propose MinkUNeXt-VINE, a lightweight, deep-learning-based method that surpasses state-of-the-art methods in vineyard environments thanks to its pre-processing and Matryoshka Representation Learning multi-loss approach. Our method prioritizes enhanced performance with low-cost, sparse LiDAR inputs and lower-dimensionality outputs to ensure high efficiency in real-time scenarios. Additionally, we present a comprehensive ablation study of the results on various evaluation cases and two extensive long-term vineyard datasets employing different LiDAR sensors. The results demonstrate the efficiency of the trade-off output produced by this approach, as well as its robust performance on low-cost and low-resolution input data. The code is publicly available for reproduction.

</details>


### [562] [SeNeDiF-OOD: Semantic Nested Dichotomy Fusion for Out-of-Distribution Detection Methodology in Open-World Classification. A Case Study on Monument Style Classification](https://arxiv.org/abs/2601.18739)
*Ignacio Antequera-Sánchez,Juan Luis Suárez-Díaz,Rosana Montes,Francisco Herrera*

Main category: cs.CV

TL;DR: 提出基于语义嵌套二分融合的SeNeDiF - OOD方法处理OOD检测问题，在MonuMAI系统验证，结果优于传统基线。


<details>
  <summary>Details</summary>
Motivation: 解决单阶段检测器难以处理OOD数据异质性问题，实现人工智能应用在开放环境可靠部署。

Method: 提出SeNeDiF - OOD方法，将检测任务分解为二进制融合节点的层次结构，每层整合与特定语义抽象级别对齐的决策边界。

Result: 在MonuMAI系统的实验中，该方法显著优于传统基线，有效过滤不同OOD类别并保持分布内性能。

Conclusion: 所提出的基于语义嵌套二分融合的层次融合方法在OOD检测中表现良好，能应对开放环境中的复杂情况。

Abstract: Out-of-distribution (OOD) detection is a fundamental requirement for the reliable deployment of artificial intelligence applications in open-world environments. However, addressing the heterogeneous nature of OOD data, ranging from low-level corruption to semantic shifts, remains a complex challenge that single-stage detectors often fail to resolve. To address this issue, we propose SeNeDiF-OOD, a novel methodology based on Semantic Nested Dichotomy Fusion. This framework decomposes the detection task into a hierarchical structure of binary fusion nodes, where each layer is designed to integrate decision boundaries aligned with specific levels of semantic abstraction. To validate the proposed framework, we present a comprehensive case study using MonuMAI, a real-world architectural style recognition system exposed to an open environment. This application faces a diverse range of inputs, including non-monument images, unknown architectural styles, and adversarial attacks, making it an ideal testbed for our proposal. Through extensive experimental evaluation in this domain, results demonstrate that our hierarchical fusion methodology significantly outperforms traditional baselines, effectively filtering these diverse OOD categories while preserving in-distribution performance.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [563] [Memento: Towards Proactive Visualization of Everyday Memories with Personal Wearable AR Assistant](https://arxiv.org/abs/2601.17622)
*Yoonsang Kim,Yalong Yang,Arie E. Kaufman*

Main category: cs.HC

TL;DR: 介绍对话式AR助手Memento，它能捕捉记忆并根据时空活动主动响应，还进行了初步评估并分享设计挑战。


<details>
  <summary>Details</summary>
Motivation: 设计一个能永久捕捉用户查询及上下文信息，发现用户兴趣与触发上下文联系，主动提供信息的AR助手。

Method: 存储用户查询的记忆，依据时空活动触发主动响应；通过不同专业参与者的用户反馈进行初步评估。

Result: 初步评估探索了主动感知上下文的AR助手在日常场景中的价值。

Conclusion: 分享了设计主动、感知上下文的AR系统的发现和挑战。

Abstract: We introduce Memento, a conversational AR assistant that permanently captures and memorizes user's verbal queries alongside their spatiotemporal and activity contexts. By storing these "memories," Memento discovers connections between users' recurring interests and the contexts that trigger them. Upon detection of similar or identical spatiotemporal activity, Memento proactively recalls user interests and delivers up-to-date responses through AR, seamlessly integrating AR experience into their daily routine. Unlike prior work, each interaction in Memento is not a transient event, but a connected series of interactions with coherent long--term perspective, tailored to the user's broader multimodal (visual, spatial, temporal, and embodied) context. We conduct preliminary evaluation through user feedbacks with participants of diverse expertise in immersive apps, and explore the value of proactive context-aware AR assistant in everyday settings. We share our findings and challenges in designing a proactive, context-aware AR system.

</details>


### [564] [OwlerLite: Scope- and Freshness-Aware Web Retrieval for LLM Assistants](https://arxiv.org/abs/2601.17824)
*Saber Zerhoudi,Michael Dinzinger,Michael Granitzer,Jelena Mitrovic*

Main category: cs.HC

TL;DR: 提出浏览器端RAG系统OwlerLite，让用户可定义检索范围和保证数据新鲜度，是迈向更可控可信网络助手的一步。


<details>
  <summary>Details</summary>
Motivation: 现有浏览器语言模型的RAG依赖固定过时索引，用户无法控制参考源，导致答案混合可信与不可信内容或使用陈旧信息。

Method: 用户定义可复用的网页或源范围，选择范围进行查询；使用感知新鲜度的爬虫监控页面，用语义变化检测器识别更新并选择性重新索引；将文本相关性、范围选择和时效性集成到统一检索模型。

Result: 实现了一个浏览器扩展。

Conclusion: OwlerLite是迈向更可控和可信网络助手的一步。

Abstract: Browser-based language models often use retrieval-augmented generation (RAG) but typically rely on fixed, outdated indices that give users no control over which sources are consulted. This can lead to answers that mix trusted and untrusted content or draw on stale information. We present OwlerLite, a browser-based RAG system that makes user-defined scopes and data freshness central to retrieval. Users define reusable scopes-sets of web pages or sources-and select them when querying. A freshness-aware crawler monitors live pages, uses a semantic change detector to identify meaningful updates, and selectively re-indexes changed content. OwlerLite integrates text relevance, scope choice, and recency into a unified retrieval model. Implemented as a browser extension, it represents a step toward more controllable and trustworthy web assistants.

</details>


### [565] [Lost in Simulation: LLM-Simulated Users are Unreliable Proxies for Human Users in Agentic Evaluations](https://arxiv.org/abs/2601.17087)
*Preethi Seshadri,Samuel Cahyawijaya,Ayomide Odumakinde,Sameer Singh,Seraphina Goldfarb-Tarrant*

Main category: cs.HC

TL;DR: 研究通过用户研究发现，使用大语言模型模拟用户评估代理表现的方法缺乏鲁棒性、存在校准错误、对不同人群评估效果差异大且会引入对话干扰因素等问题。


<details>
  <summary>Details</summary>
Motivation: 现有使用大语言模型模拟用户评估代理表现的方法，其鲁棒性、有效性和公平性尚未得到检验，需研究模拟用户能否可靠替代真实用户评估代理。

Method: 对美国、印度、肯尼亚和尼日利亚的参与者进行用户研究，在τ - Bench零售任务中评估代理。

Result: 模拟用户评估缺乏鲁棒性，评估存在系统校准错误，不同英语使用者在评估中有表现差异，对不同人群评估效果不同，还会引入对话干扰因素和展现不同失败模式。

Conclusion: 当前评估方法可能误代表代理在不同用户群体中的能力，也可能掩盖实际部署挑战。

Abstract: Agentic benchmarks increasingly rely on LLM-simulated users to scalably evaluate agent performance, yet the robustness, validity, and fairness of this approach remain unexamined. Through a user study with participants across the United States, India, Kenya, and Nigeria, we investigate whether LLM-simulated users serve as reliable proxies for real human users in evaluating agents on τ-Bench retail tasks. We find that user simulation lacks robustness, with agent success rates varying up to 9 percentage points across different user LLMs. Furthermore, evaluations using simulated users exhibit systematic miscalibration, underestimating agent performance on challenging tasks and overestimating it on moderately difficult ones. African American Vernacular English (AAVE) speakers experience consistently worse success rates and calibration errors than Standard American English (SAE) speakers, with disparities compounding significantly with age. We also find simulated users to be a differentially effective proxy for different populations, performing worst for AAVE and Indian English speakers. Additionally, simulated users introduce conversational artifacts and surface different failure patterns than human users. These findings demonstrate that current evaluation practices risk misrepresenting agent capabilities across diverse user populations and may obscure real-world deployment challenges.

</details>


### [566] [Status Hierarchies in Language Models](https://arxiv.org/abs/2601.17577)
*Emilio Barkett*

Main category: cs.HC

TL;DR: 研究语言模型在多智能体场景中形成地位层级的情况及影响。


<details>
  <summary>Details</summary>
Motivation: 人类社会存在地位层级，语言模型基于人类文本训练，研究其在多智能体场景是否会重现该动态。

Method: 采用Berger等人的期望状态框架，创建多智能体场景，让语言模型完成情感分类任务，引入不同地位特征，观察基于地位线索而非任务信息的顺从率。

Result: 能力相当时语言模型形成显著地位层级；能力差异主导地位线索，高地位分配降低高能力模型顺从率。

Conclusion: 研究识别了AI系统中涌现的社会行为，凸显了对齐挑战中未被充分探索的维度，地位寻求行为对AI安全有重大影响。

Abstract: From school playgrounds to corporate boardrooms, status hierarchies -- rank orderings based on respect and perceived competence -- are universal features of human social organization. Language models trained on human-generated text inevitably encounter these hierarchical patterns embedded in language, raising the question of whether they might reproduce such dynamics in multi-agent settings. This thesis investigates when and how language models form status hierarchies by adapting Berger et al.'s (1972) expectation states framework. I create multi-agent scenarios where separate language model instances complete sentiment classification tasks, are introduced with varying status characteristics (e.g., credentials, expertise), then have opportunities to revise their initial judgments after observing their partner's responses. The dependent variable is deference, the rate at which models shift their ratings toward their partner's position based on status cues rather than task information. Results show that language models form significant status hierarchies when capability is equal (35 percentage point asymmetry, p < .001), but capability differences dominate status cues, with the most striking effect being that high-status assignments reduce higher-capability models' deference rather than increasing lower-capability models' deference. The implications for AI safety are significant: status-seeking behavior could introduce deceptive strategies, amplify discriminatory biases, and scale across distributed deployments far faster than human hierarchies form organically. This work identifies emergent social behaviors in AI systems and highlights a previously underexplored dimension of the alignment challenge.

</details>


### [567] [Athanor: Authoring Action Modification-based Interactions on Static Visualizations via Natural Language](https://arxiv.org/abs/2601.17736)
*Can Liu,Jaeuk Lee,Tianhe Chen,Zhibang Jiang,Xiaolin Wen,Yong Wang*

Main category: cs.HC

TL;DR: 提出Athanor方法，用MLLMs和自然语言指令将静态可视化转换为交互式可视化，经评估有效可用。


<details>
  <summary>Details</summary>
Motivation: 现有静态可视化实现交互困难，底层代码和数据常不可用，且实现交互耗时费力。

Method: 提出Athanor方法，包括行动 - 修改交互设计空间、多智能体需求分析器和可视化抽象转换器，允许用户通过自然语言指令创建交互。

Result: 通过两个案例研究和对目标用户的深度访谈，证明该方法能让用户方便地为静态可视化添加灵活交互。

Conclusion: 该方法在为静态可视化启用灵活交互方面有效且易用。

Abstract: Interactivity is crucial for effective data visualizations. However, it is often challenging to implement interactions for existing static visualizations, since the underlying code and data for existing static visualizations are often not available, and it also takes significant time and effort to enable interactions for them even if the original code and data are available. To fill this gap, we propose Athanor, a novel approach to transform existing static visualizations into interactive ones using multimodal large language models (MLLMs) and natural language instructions. Our approach introduces three key innovations: (1) an action-modification interaction design space that maps visualization interactions into user actions and corresponding adjustments, (2) a multi-agent requirement analyzer that translates natural language instructions into an actionable operational space, and (3) a visualization abstraction transformer that converts static visualizations into flexible and interactive representations regardless of their underlying implementation. Athanor allows users to effortlessly author interactions through natural language instructions, eliminating the need for programming. We conducted two case studies and in-depth interviews with target users to evaluate our approach. The results demonstrate the effectiveness and usability of our approach in allowing users to conveniently enable flexible interactions for static visualizations.

</details>


### [568] [RAICL: Retrieval-Augmented In-Context Learning for Vision-Language-Model Based EEG Seizure Detection](https://arxiv.org/abs/2601.17844)
*Siyang Li,Zhuoya Wang,Xiyan Gui,Xiaoqing Chen,Ziwei Wang,Yaozhi Wen,Dongrui Wu*

Main category: cs.HC

TL;DR: 本文提出利用大规模视觉 - 语言模型（VLMs）分析脑电图波形图进行解码，引入检索增强上下文学习（RAICL）方法，在癫痫检测实验中表现良好，为生理信号处理提供新方向。


<details>
  <summary>Details</summary>
Motivation: 当代脑电图（EEG）解码方法依赖特定任务数据集训练，数据有限阻碍通用大模型发展。

Method: 将多变量EEG信号转换为堆叠波形图像，把神经科学领域知识融入文本提示，利用VLMs分析；引入RAICL方法动态选择示例调节VLM输出。

Result: 基于EEG的癫痫检测实验中，RAICL下的VLMs与传统时间序列方法表现相当或更优。

Conclusion: 为生理信号处理提供新方向，利用现成VLMs无需再训练或构建下游架构，适用于临床应用。

Abstract: Electroencephalogram (EEG) decoding is a critical component of medical diagnostics, rehabilitation engineering, and brain-computer interfaces. However, contemporary decoding methodologies remain heavily dependent on task-specific datasets to train specialized neural network architectures. Consequently, limited data availability impedes the development of generalizable large brain decoding models. In this work, we propose a paradigm shift from conventional signal-based decoding by leveraging large-scale vision-language models (VLMs) to analyze EEG waveform plots. By converting multivariate EEG signals into stacked waveform images and integrating neuroscience domain expertise into textual prompts, we demonstrate that foundational VLMs can effectively differentiate between different patterns in the human brain. To address the inherent non-stationarity of EEG signals, we introduce a Retrieval-Augmented In-Context Learning (RAICL) approach, which dynamically selects the most representative and relevant few-shot examples to condition the autoregressive outputs of the VLM. Experiments on EEG-based seizure detection indicate that state-of-the-art VLMs under RAICL achieved better or comparable performance with traditional time series based approaches. These findings suggest a new direction in physiological signal processing that effectively bridges the modalities of vision, language, and neural activities. Furthermore, the utilization of off-the-shelf VLMs, without the need for retraining or downstream architecture construction, offers a readily deployable solution for clinical applications.

</details>


### [569] [An Experimental Comparison of Cognitive Forcing Functions for Execution Plans in AI-Assisted Writing: Effects On Trust, Overreliance, and Perceived Critical Thinking](https://arxiv.org/abs/2601.18033)
*Ahana Ghosh,Advait Sarkar,Siân Lindley,Christian Poelitz*

Main category: cs.HC

TL;DR: 研究在生成式AI辅助写作中应用认知强制功能（CFFs）审查AI生成计划的效果，发现假设CFF最有效减少过度依赖且不增加认知负荷。


<details>
  <summary>Details</summary>
Motivation: 生成式AI工具虽提高知识工作流程生产力，但存在过度依赖和批判性思维降低风险，且应用CFFs到AI计划的有效性未被充分探索。

Method: 进行对照实验，让参与者在四种CFF条件下完成AI辅助写作任务并审查AI生成计划，后续进行出声思考和访谈研究。

Result: 假设CFF最有效减少过度依赖且不增加认知负荷，参与者认为假设检验CFF最有帮助。

Conclusion: 以计划为重点的CFFs对支持生成式AI辅助知识工作中的批判性反思有价值。

Abstract: Generative AI (GenAI) tools improve productivity in knowledge workflows such as writing, but also risk overreliance and reduced critical thinking. Cognitive forcing functions (CFFs) mitigate these risks by requiring active engagement with AI output. As GenAI workflows grow more complex, systems increasingly present execution plans for user review. However, these plans are themselves AI-generated and prone to overreliance, and the effectiveness of applying CFFs to AI plans remains underexplored. We conduct a controlled experiment in which participants completed AI-assisted writing tasks while reviewing AI-generated plans under four CFF conditions: Assumption (argument analysis), WhatIf (hypothesis testing), Both, and a no-CFF control. A follow-up think-aloud and interview study qualitatively compared these conditions. Results show that the Assumption CFF most effectively reduced overreliance without increasing cognitive load, while participants perceived the WhatIf CFF as most helpful. These findings highlight the value of plan-focused CFFs for supporting critical reflection in GenAI-assisted knowledge work.

</details>


### [570] ["Crash Test Dummies" for AI-Enabled Clinical Assessment: Validating Virtual Patient Scenarios with Virtual Learners](https://arxiv.org/abs/2601.18085)
*Brian Gin,Ahreum Lim,Flávia Silva e Oliveira,Kuan Xing,Xiaomei Song,Gayana Amiyangoda,Thilanka Seneviratne,Alison F. Doubleday,Ananya Gangopadhyaya,Bob Kiser,Lukas Shum-Tim,Dhruva Patel,Kosala Marambe,Lauren Maggio,Ara Tekian,Yoon Soo Park*

Main category: cs.HC

TL;DR: 本文利用AI模拟学习者对评估流程进行压力测试和心理测量表征，开发开源AI虚拟患者平台和测量模型用于能力评估，模型能恢复模拟学习者能力，还提出部署AI工具的安全蓝图。


<details>
  <summary>Details</summary>
Motivation: 医疗健康职业教育中AI评估临床能力缺乏测量框架，导致结果不确定性高且可能误导学习者，需在人类使用前对评估流程进行压力测试和心理测量表征。

Method: 构建含虚拟患者、虚拟学习者和多个独立AI评分者的平台，用贝叶斯HRM - SDT模型分析记录，通过MCMC估计参数。

Result: 模型恢复模拟学习者能力，各领域有显著相关性，能估计案例难度，显示稳定评分者检测和标准，提出部署AI工具的安全蓝图。

Conclusion: 专用虚拟患者平台与心理测量模型结合，能实现稳健、可解释、可推广的能力评估，并支持在人类学习者使用前验证AI辅助评估。

Abstract: Background: In medical and health professions education (HPE), AI is increasingly used to assess clinical competencies, including via virtual standardized patients. However, most evaluations rely on AI-human interrater reliability and lack a measurement framework for how cases, learners, and raters jointly shape scores. This leaves robustness uncertain and can expose learners to misguidance from unvalidated systems. We address this by using AI "simulated learners" to stress-test and psychometrically characterize assessment pipelines before human use.
  Objective: Develop an open-source AI virtual patient platform and measurement model for robust competency evaluation across cases and rating conditions.
  Methods: We built a platform with virtual patients, virtual learners with tunable ACGME-aligned competency profiles, and multiple independent AI raters scoring encounters with structured Key-Features items. Transcripts were analyzed with a Bayesian HRM-SDT model that treats ratings as decisions under uncertainty and separates learner ability, case performance, and rater behavior; parameters were estimated with MCMC.
  Results: The model recovered simulated learners' competencies, with significant correlations to the generating competencies across all ACGME domains despite a non-deterministic pipeline. It estimated case difficulty by competency and showed stable rater detection (sensitivity) and criteria (severity/leniency thresholds) across AI raters using identical models/prompts but different seeds. We also propose a staged "safety blueprint" for deploying AI tools with learners, tied to entrustment-based validation milestones.
  Conclusions: Combining a purpose-built virtual patient platform with a principled psychometric model enables robust, interpretable, generalizable competency estimates and supports validation of AI-assisted assessment prior to use with human learners.

</details>


### [571] [Understanding Users' Privacy Reasoning and Behaviors During Chatbot Use to Support Meaningful Agency in Privacy](https://arxiv.org/abs/2601.18125)
*Mohammad Hadi Nezhad,Francisco Enrique Vicente Castro,Ivon Arroyo*

Main category: cs.HC

TL;DR: 研究计算机专业学生在使用模拟ChatGPT界面时的隐私信息处理行为，探讨隐私通知面板作用并讨论设计机会。


<details>
  <summary>Details</summary>
Motivation: 聊天机器人使用中用户隐私问题突出，需深入了解用户在真实场景中对敏感信息的处理方式。

Method: 让计算机专业学生使用带或不带隐私通知面板的模拟ChatGPT界面，通过交互日志、有声思维和调查响应进行分析。

Result: 分析了隐私通知面板对提高隐私意识、鼓励保护行为和支持特定情境推理的作用。

Conclusion: 讨论了在对话式代理交互中为用户提供更多保护敏感信息的设计机会。

Abstract: Conversational agents (CAs) (e.g., chatbots) are increasingly used in settings where users disclose sensitive information, raising significant privacy concerns. Because privacy judgments are highly contextual, supporting users to engage in privacy-protective actions during chatbot interactions is essential. However, enabling meaningful engagement requires a deeper understanding of how users currently reason about and manage sensitive information during realistic chatbot use scenarios. To investigate this, we qualitatively examined computer science (undergraduate and masters) students' in-the-moment disclosure and protection behaviors, as well as the reasoning underlying these behaviors, across a range of realistic chatbot tasks. Participants used a simulated ChatGPT interface with and without a privacy notice panel that intercepts message submissions, highlights potentially sensitive information, and offers privacy protective actions. The panel supports anonymization through retracting, faking, and generalizing, and surfaces two of ChatGPT's built-in privacy controls to improve their discoverability. Drawing on interaction logs, think-alouds, and survey responses, we analyzed how the panel fostered privacy awareness, encouraged protective actions, and supported context-specific reasoning about what information to protect and how. We further discuss design opportunities for tools that provide users greater and more meaningful agency in protecting sensitive information during CA interactions.

</details>


### [572] [PaperTok: Exploring the Use of Generative AI for Creating Short-form Videos for Research Communication](https://arxiv.org/abs/2601.18218)
*Meziah Ruby Cristobal,Hyeonjeong Byeon,Tze-Yu Chen,Ruoxi Shang,Donghoon Shin,Ruican Zhong,Tony Zhou,Gary Hsieh*

Main category: cs.HC

TL;DR: 研究探索用生成式AI帮助研究者将学术论文转化为短视频内容，设计了PaperTok系统，研究表明其能助研究者创作视频，还指出创作过程需更精细控制并给出对未来工具的启示。


<details>
  <summary>Details</summary>
Motivation: 研究者缺乏时间和技能为大众媒体创作有吸引力的内容，为填补这一空白开展研究。

Method: 通过与科学传播者和内容创作者的形成性研究设计PaperTok系统，进行混合方法用户研究和众包评估。

Result: PaperTok的工作流程能帮助研究者创作有吸引力和信息量的短视频，发现创作过程需要更精细的控制。

Conclusion: 为支持科学推广的未来生成式工具提供了启示。

Abstract: The dissemination of scholarly research is critical, yet researchers often lack the time and skills to create engaging content for popular media such as short-form videos. To address this gap, we explore the use of generative AI to help researchers transform their academic papers into accessible video content. Informed by a formative study with science communicators and content creators (N=8), we designed PaperTok, an end-to-end system that automates the initial creative labor by generating script options and corresponding audiovisual content from a source paper. Researchers can then refine based on their preferences with further prompting. A mixed-methods user study (N=18) and crowdsourced evaluation (N=100) demonstrate that PaperTok's workflow can help researchers create engaging and informative short-form videos. We also identified the need for more fine-grained controls in the creation process. To this end, we offer implications for future generative tools that support science outreach.

</details>


### [573] [Fusion of Spatio-Temporal and Multi-Scale Frequency Features for Dry Electrodes MI-EEG Decoding](https://arxiv.org/abs/2601.18424)
*Tianyi Gong,Can Han,Junxi Wu,Dahong Qian*

Main category: cs.HC

TL;DR: 介绍针对干电极运动想象脑电图（MI - EEG）的STGMFM三分支框架，该框架在干电极MI - EEG数据上表现优于基线模型，代码开源。


<details>
  <summary>Details</summary>
Motivation: 干电极MI - EEG存在信噪比低、数据不稳定、会话间差异大等问题，导致数据分布偏移，特征不稳定，需要解决这些问题。

Method: 引入STGMFM三分支框架，通过双图阶建模时空依赖，用多尺度频率混合分支捕捉包络动态，利用生理意义的连接先验引导学习，决策级融合达成抗噪共识。

Result: 在收集的干电极MI - EEG数据上，STGMFM始终优于有竞争力的CNN/Transformer/图基线模型。

Conclusion: STGMFM框架能有效解决干电极MI - EEG存在的问题，提升性能。

Abstract: Dry-electrode Motor Imagery Electroencephalography (MI-EEG) enables fast, comfortable, real-world Brain Computer Interface by eliminating gels and shortening setup for at-home and wearable use.However, dry recordings pose three main issues: lower Signal-to-Noise Ratio with more baseline drift and sudden transients; weaker and noisier data with poor phase alignment across trials; and bigger variances between sessions. These drawbacks lead to larger data distribution shift, making features less stable for MI-EEG tasks.To address these problems, we introduce STGMFM, a tri-branch framework tailored for dry-electrode MI-EEG, which models complementary spatio-temporal dependencies via dual graph orders, and captures robust envelope dynamics with a multi-scale frequency mixing branch, motivated by the observation that amplitude envelopes are less sensitive to contact variability than instantaneous waveforms. Physiologically meaningful connectivity priors guide learning, and decision-level fusion consolidates a noise-tolerant consensus. On our collected dry-electrode MI-EEG, STGMFM consistently surpasses competitive CNN/Transformer/graph baselines. Codes are available at https://github.com/Tianyi-325/STGMFM.

</details>


### [574] [Unheard in the Digital Age: Rethinking AI Bias and Speech Diversity](https://arxiv.org/abs/2601.18641)
*Onyedikachi Hope Amaechi-Okorie,Branislav Radeljic*

Main category: cs.HC

TL;DR: 文章指出语音是社会包容与排斥的重要因素，分析人工智能对非典型语音的结构偏见，呼吁进行包容性技术设计、反偏见培训和政策改革。


<details>
  <summary>Details</summary>
Motivation: 解决当代社会中人工智能对非典型语音存在的结构偏见，减少数字排斥问题。

Method: 基于跨学科研究。

Result: 揭示了自动化语音识别系统和语音界面在识别多样化声音上的不足。

Conclusion: 将语音包容视为公平问题，倡导共同创建能反映人类全频谱声音的人工智能系统。

Abstract: Speech remains one of the most visible yet overlooked vectors of inclusion and exclusion in contemporary society. While fluency is often equated with credibility and competence, individuals with atypical speech patterns are routinely marginalized. Given the current state of the debate, this article focuses on the structural biases that shape perceptions of atypical speech and are now being encoded into artificial intelligence. Automated speech recognition (ASR) systems and voice interfaces, trained predominantly on standardized speech, routinely fail to recognize or respond to diverse voices, compounding digital exclusion. As AI technologies increasingly mediate access to opportunity, the study calls for inclusive technological design, anti-bias training to minimize the impact of discriminatory algorithmic decisions, and enforceable policy reform that explicitly recognize speech diversity as a matter of equity, not merely accessibility. Drawing on interdisciplinary research, the article advocates for a cultural and institutional shift in how we value voice, urging co-created solutions that elevate the rights, representation, and realities of atypical speakers in the digital age. Ultimately, the article reframes speech inclusion as a matter of equity (not accommodation) and advocates for co-created AI systems that reflect the full spectrum of human voices.

</details>


### [575] [MEGnifying Emotion: Sentiment Analysis from Annotated Brain Data](https://arxiv.org/abs/2601.18792)
*Brian Liu,Oiwi Parker Jones*

Main category: cs.HC

TL;DR: 本文探索用预训练文本 - 情感模型标注脑电数据，训练脑 - 情感模型，实验显示该方法有更好的平衡准确率。


<details>
  <summary>Details</summary>
Motivation: 现有数据集未将脑数据与情感标注结合，为填补此空白开展研究。

Method: 使用预训练文本 - 情感模型标注非侵入性脑电记录，通过文本和音频强制对齐将情感标签与脑电记录对齐，然后训练脑 - 情感模型。

Result: 与基线相比，脑 - 情感模型的平衡准确率有所提高。

Conclusion: 该方法可作为利用现有脑电数据集并直接从大脑解码情感的概念验证。

Abstract: Decoding emotion from brain activity could unlock a deeper understanding of the human experience. While a number of existing datasets align brain data with speech and with speech transcripts, no datasets have annotated brain data with sentiment. To bridge this gap, we explore the use of pre-trained Text-to-Sentiment models to annotate non invasive brain recordings, acquired using magnetoencephalography (MEG), while participants listened to audiobooks. Having annotated the text, we employ force-alignment of the text and audio to align our sentiment labels with the brain recordings. It is straightforward then to train Brainto-Sentiment models on these data. Experimental results show an improvement in balanced accuracy for Brain-to-Sentiment compared to baseline, supporting the proposed approach as a proof-of-concept for leveraging existing MEG datasets and learning to decode sentiment directly from the brain.

</details>


### [576] [Design Techniques for LLM-Powered Interactive Storytelling: A Case Study of the Dramamancer System](https://arxiv.org/abs/2601.18785)
*Tiffany Wang,Yuqian Sun,Yi Wang,Melissa Roemmele,John Joon Young Chung,Max Kreminski*

Main category: cs.HC

TL;DR: 以Dramamancer系统为例探讨大语言模型在交互式叙事中连接作者意图和玩家能动性的范式，概述系统设计技术和评估考量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型兴起带来在交互式叙事中连接作者意图和玩家能动性的新范式，需对其进行研究。

Method: 以Dramamancer系统为例，该系统用大语言模型将作者创作的故事架构转化为玩家驱动的通关流程。

Result: 未提及具体结果。

Conclusion: 未提及明确结论，仅概述了相关设计技术和评估考量。

Abstract: The rise of Large Language Models (LLMs) has enabled a new paradigm for bridging authorial intent and player agency in interactive narrative. We consider this paradigm through the example of Dramamancer, a system that uses an LLM to transform author-created story schemas into player-driven playthroughs. This extended abstract outlines some design techniques and evaluation considerations associated with this system.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [577] [Toward Scalable Normalizing Flows for the Hubbard Model](https://arxiv.org/abs/2601.18273)
*Janik Kreit,Andrea Bulgarelli,Lena Funcke,Thomas Luu,Dominic Schuh,Simran Singh,Lorenzo Verzichelli*

Main category: cond-mat.str-el

TL;DR: 研究将Hubbard模型模拟扩展到更大晶格尺寸和更低温度的步骤，展示随机归一化流和非平衡马尔可夫链蒙特卡罗方法的缩放行为。


<details>
  <summary>Details</summary>
Motivation: 将Hubbard模型模拟扩展到更大晶格尺寸和更低温度，并提高稳定性和效率。

Method: 未明确提及具体方法，涉及随机归一化流和非平衡马尔可夫链蒙特卡罗方法。

Result: 展示了随机归一化流和非平衡马尔可夫链蒙特卡罗方法在该费米子系统中的缩放行为。

Conclusion: 未明确提及结论。

Abstract: Normalizing flows have recently demonstrated the ability to learn the Boltzmann distribution of the Hubbard model, opening new avenues for generative modeling in condensed matter physics. In this work, we investigate the steps required to extend such simulations to larger lattice sizes and lower temperatures, with a focus on enhancing stability and efficiency. Additionally, we present the scaling behavior of stochastic normalizing flows and non-equilibrium Markov chain Monte Carlo methods for this fermionic system.

</details>


<div id='math.HO'></div>

# math.HO [[Back]](#toc)

### [578] [LLAMA LIMA: A Living Meta-Analysis on the Effects of Generative AI on Learning Mathematics](https://arxiv.org/abs/2601.18685)
*Anselm Strohmaier,Samira Bödefeld,Frank Reinhold*

Main category: math.HO

TL;DR: 提出数学教育中生成式AI干预的实时元分析，首版含15项研究，有小的积极效果。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在数学教育中发展快，研究综合稀缺且易过时。

Method: 遵循PRISMA - LSR指南持续更新文献库，应用贝叶斯多级元回归模型处理累积数据，定期在预印本服务器发布更新版本。

Result: 首版分析含15项研究，有小的积极效果（g = 0.31），可信区间宽[0.06, 0.58]。

Conclusion: 当前关于生成式AI在数学教育干预效果的证据仍有限。

Abstract: The capabilities of generative AI in mathematics education are rapidly evolving, posing significant challenges for research to keep pace. Research syntheses remain scarce and risk being outdated by the time of publication. To address this issue, we present a Living Meta-Analysis (LIMA) on the effects of generative AI-based interventions for learning mathematics. Following PRISMA-LSR guidelines, we continuously update the literature base, apply a Bayesian multilevel meta-regression model to account for cumulative data, and publish updated versions on a preprint server at regular intervals. This paper reports results from the first version, including 15 studies. The analyses indicate a small positive effect (g = 0.31) with a wide credible interval [0.06, 0.58], reflecting the still limited evidence base.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [579] [Use of operator defect identities in multi-channel signal plus residual-analysis via iterated products and telescoping energy-residuals: Applications to kernels in machine learning](https://arxiv.org/abs/2601.18080)
*Palle E. T. Jorgensen,Myung-Sin Song,James F. Tian*

Main category: math.FA

TL;DR: 提出分析复杂系统的算子理论框架，证明相关结果并给出应用。


<details>
  <summary>Details</summary>
Motivation: 为具有内在组件细分的复杂系统提供分析框架。

Method: 构建新的算子理论框架，证明相关结果。

Result: 获得新的可容许性/有效性结果，能量残差的先验界；给出无限维Kaczmarz理论和广义机器学习算法等应用。

Conclusion: 该框架可应用于复杂系统分析，在理论和算法上有明确结果。

Abstract: We present a new operator theoretic framework for analysis of complex systems with intrinsic subdivisions into components, taking the form of "residuals" in general, and "telescoping energy residuals" in particular. We prove new results which yield admissibility/effectiveness, and new a priori bounds on energy residuals. Applications include infinite-dimensional Kaczmarz theory for $λ_{n}$-relaxed variants, and $λ_{n}$-effectiveness. And we give applications of our framework to generalized machine learning algorithms, greedy Kernel Principal Component Analysis (KPCA), proving explicit convergence results, residual energy decomposition, and criteria for stability under noise.

</details>


<div id='hep-ex'></div>

# hep-ex [[Back]](#toc)

### [580] [EveNet: A Foundation Model for Particle Collision Data Analysis](https://arxiv.org/abs/2601.17126)
*Ting-Hsiang Hsu,Bai-Hong Zhou,Qibin Liu,Yue Xu,Shu Li,George Wei-Shu Hou,Benjamin Nachman,Shih-Chieh Hsu,Vinicius Mikuni,Yuan-Tang Chou,Yulei Zhang*

Main category: hep-ex

TL;DR: 提出EveNet模型解决高能物理深度学习计算挑战，在多任务表现优，验证可迁移性和精度，提供统一高效框架。


<details>
  <summary>Details</summary>
Motivation: 深度学习在高能物理数据分析面临计算挑战，限制其潜力，需解决这些问题。

Method: 引入EveNet事件级基础模型，用自监督学习和物理信息监督的混合目标在5亿个模拟碰撞事件上预训练。

Result: EveNet在多任务上优于现有基线，在低统计区域数据效率高，验证了模型对实验数据的可迁移性和精度。

Conclusion: EveNet能编码粒子相互作用的基本物理结构，为当前和未来对撞机的发现提供统一且资源高效的框架。

Abstract: While deep learning is transforming data analysis in high-energy physics, computational challenges limit its potential. We address these challenges in the context of collider physics by introducing EveNet, an event-level foundation model pretrained on 500 million simulated collision events using a hybrid objective of self-supervised learning and physics-informed supervision. By leveraging a shared particle-cloud representation, EveNet outperforms state-of-the-art baselines across diverse tasks, including searches for heavy resonances and exotic Higgs decays, and demonstrates exceptional data efficiency in low-statistics regimes. Crucially, we validate the transferability of the model to experimental data by rediscovering the $Υ$ meson in CMS Open Data and show its capacity for precision physics through the robust extraction of quantum correlation observables stable against systematic uncertainties. These results indicate that EveNet can successfully encode the fundamental physical structure of particle interactions, which offers a unified and resource-efficient framework to accelerate discovery at current and future colliders.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [581] [Structure-Aware NL-to-SQL for SFC Provisioning via AST-Masking Empowered Language Models](https://arxiv.org/abs/2601.17295)
*Xinyu Zhu,Parisa Fard Moshiri,Poonam Lohan,Burak Kantarci,Emil Janulewicz*

Main category: cs.NI

TL;DR: 本文提出AST - Masking结构感知微调方法用于服务功能链（SFC）管理的SQL生成，实验证明该方法能提升SQL生成准确率。


<details>
  <summary>Details</summary>
Motivation: 强化学习在SFC编排中忽略结构化领域知识，传统微调会导致语法不一致和查询效率低，需要改进方法以实现可解释的SFC编排。

Method: 引入Abstract Syntax Tree (AST)-Masking结构感知微调方法，利用SQL AST为关键组件分配权重，实现语法感知学习且不增加推理开销。

Result: 实验表明AST - Masking显著提高多个语言模型的SQL生成准确率，如FLAN - T5执行准确率达99.6%，Gemma从7.5%提升到72.0%。

Conclusion: 结构感知微调方法能确保生成语法正确且高效的SQL，可用于可解释的SFC编排。

Abstract: Effective Service Function Chain (SFC) provisioning requires precise orchestration in dynamic and latency-sensitive networks. Reinforcement Learning (RL) improves adaptability but often ignores structured domain knowledge, which limits generalization and interpretability. Large Language Models (LLMs) address this gap by translating natural language (NL) specifications into executable Structured Query Language (SQL) commands for specification-driven SFC management. Conventional fine-tuning, however, can cause syntactic inconsistencies and produce inefficient queries. To overcome this, we introduce Abstract Syntax Tree (AST)-Masking, a structure-aware fine-tuning method that uses SQL ASTs to assign weights to key components and enforce syntax-aware learning without adding inference overhead. Experiments show that AST-Masking significantly improves SQL generation accuracy across multiple language models. FLAN-T5 reaches an Execution Accuracy (EA) of 99.6%, while Gemma achieves the largest absolute gain from 7.5% to 72.0%. These results confirm the effectiveness of structure-aware fine-tuning in ensuring syntactically correct and efficient SQL generation for interpretable SFC orchestration.

</details>


### [582] [Diffusion Model-based Reinforcement Learning for Version Age of Information Scheduling: Average and Tail-Risk-Sensitive Control](https://arxiv.org/abs/2601.18069)
*Haoyuan Pan,Sizhao Chen,Zhaorui Wang,Tse-Tin Chan*

Main category: cs.NI

TL;DR: 本文研究多用户状态更新系统中平均导向和尾风险敏感的VAoI调度，提出D2SAC和RS - D3SAC算法，模拟显示D2SAC降低平均VAoI，RS - D3SAC降低CVaR且不牺牲均值性能。


<details>
  <summary>Details</summary>
Motivation: 现有VAoI调度方法主要关注最小化平均VAoI，忽略了罕见但严重的陈旧事件，本文旨在解决在随机数据包到达和不可靠信道下的可靠性问题。

Method: 将平均VAoI最小化问题建模为受限马尔可夫决策过程，提出D2SAC算法；在此基础上提出RS - D3SAC算法，集成基于扩散的行动者和基于分位数的分布评论家。

Result: 模拟表明D2SAC降低平均VAoI，RS - D3SAC持续大幅降低CVaR且不牺牲均值性能。

Conclusion: 分布评论家在降低尾风险方面起主要作用，基于扩散的行动者起补充作用，两种算法对多用户无线系统的VAoI调度有效。

Abstract: Ensuring timely and semantically accurate information delivery is critical in real-time wireless systems. While Age of Information (AoI) quantifies temporal freshness, Version Age of Information (VAoI) captures semantic staleness by accounting for version evolution between transmitters and receivers. Existing VAoI scheduling approaches primarily focus on minimizing average VAoI, overlooking rare but severe staleness events that can compromise reliability under stochastic packet arrivals and unreliable channels. This paper investigates both average-oriented and tail-risk-sensitive VAoI scheduling in a multi-user status update system with long-term transmission cost constraints. We first formulate the average VAoI minimization problem as a constrained Markov decision process and introduce a deep diffusion-based Soft Actor-Critic (D2SAC) algorithm. By generating actions through a diffusion-based denoising process, D2SAC enhances policy expressiveness and establishes a strong baseline for mean performance. Building on this foundation, we put forth RS-D3SAC, a risk-sensitive deep distributional diffusion-based Soft Actor-Critic algorithm. RS-D3SAC integrates a diffusion-based actor with a quantile-based distributional critic, explicitly modeling the full VAoI return distribution. This enables principled tail-risk optimization via Conditional Value-at-Risk (CVaR) while satisfying long-term transmission cost constraints. Extensive simulations show that, while D2SAC reduces average VAoI, RS-D3SAC consistently achieves substantial reductions in CVaR without sacrificing mean performance. The dominant gain in tail-risk reduction stems from the distributional critic, with the diffusion-based actor providing complementary refinement to stabilize and enrich policy decisions, highlighting their effectiveness for robust and risk-aware VAoI scheduling in multi-user wireless systems.

</details>
