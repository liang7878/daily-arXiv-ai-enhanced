<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 48]
- [cs.CE](#cs.CE) [Total: 2]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.DS](#cs.DS) [Total: 4]
- [cs.GT](#cs.GT) [Total: 5]
- [cs.IR](#cs.IR) [Total: 7]
- [cs.LG](#cs.LG) [Total: 125]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.PF](#cs.PF) [Total: 3]
- [cs.SE](#cs.SE) [Total: 7]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [stat.ML](#stat.ML) [Total: 8]
- [q-bio.NC](#q-bio.NC) [Total: 3]
- [math.OC](#math.OC) [Total: 6]
- [cs.CR](#cs.CR) [Total: 8]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [cs.RO](#cs.RO) [Total: 24]
- [math.NA](#math.NA) [Total: 1]
- [quant-ph](#quant-ph) [Total: 4]
- [stat.AP](#stat.AP) [Total: 1]
- [math.AP](#math.AP) [Total: 1]
- [cs.GR](#cs.GR) [Total: 2]
- [math.CO](#math.CO) [Total: 1]
- [cs.SD](#cs.SD) [Total: 7]
- [eess.AS](#eess.AS) [Total: 6]
- [cs.CL](#cs.CL) [Total: 25]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.CV](#cs.CV) [Total: 37]
- [econ.GN](#econ.GN) [Total: 2]
- [cs.CY](#cs.CY) [Total: 9]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.ET](#cs.ET) [Total: 3]
- [stat.ME](#stat.ME) [Total: 3]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.NI](#cs.NI) [Total: 3]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [eess.IV](#eess.IV) [Total: 3]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.CC](#cs.CC) [Total: 1]
- [cs.HC](#cs.HC) [Total: 4]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [A Cost-Benefit Analysis of On-Premise Large Language Model Deployment: Breaking Even with Commercial LLM Services](https://arxiv.org/abs/2509.18101)
*Guanzhong Pan,Haibo Wang*

Main category: cs.AI

TL;DR: 本文提出成本效益分析框架，对比本地部署开源大模型与商业订阅服务成本，为组织规划大模型策略提供依据。


<details>
  <summary>Details</summary>
Motivation: 组织使用大模型面临订阅商业服务或本地部署的决策，因数据隐私等问题，本地部署受关注，需分析何时本地部署更经济。

Method: 考虑最新开源模型的硬件需求、运营费用和性能基准，对比本地部署总成本与主要云服务提供商订阅费用。

Result: 得出基于使用水平和性能需求的盈亏平衡点估计。

Conclusion: 为组织规划大模型策略提供实用框架。

Abstract: Large language models (LLMs) are becoming increasingly widespread.
Organizations that want to use AI for productivity now face an important
decision. They can subscribe to commercial LLM services or deploy models on
their own infrastructure. Cloud services from providers such as OpenAI,
Anthropic, and Google are attractive because they provide easy access to
state-of-the-art models and are easy to scale. However, concerns about data
privacy, the difficulty of switching service providers, and long-term operating
costs have driven interest in local deployment of open-source models. This
paper presents a cost-benefit analysis framework to help organizations
determine when on-premise LLM deployment becomes economically viable compared
to commercial subscription services. We consider the hardware requirements,
operational expenses, and performance benchmarks of the latest open-source
models, including Qwen, Llama, Mistral, and etc. Then we compare the total cost
of deploying these models locally with the major cloud providers subscription
fee. Our findings provide an estimated breakeven point based on usage levels
and performance needs. These results give organizations a practical framework
for planning their LLM strategies.

</details>


### [2] [Foam-Agent: An End-to-End Composable Multi-Agent Framework for Automating CFD Simulation in OpenFOAM](https://arxiv.org/abs/2509.18178)
*Ling Yue,Nithin Somasekharan,Tingwen Zhang,Yadi Cao,Shaowu Pan*

Main category: cs.AI

TL;DR: 介绍Foam - Agent多智能体框架自动完成OpenFOAM工作流，评估表现佳，降低CFD专业门槛。


<details>
  <summary>Details</summary>
Motivation: 解决CFD学习曲线陡峭和手动设置复杂的问题。

Method: 引入Foam - Agent框架，有端到端模拟自动化、可组合服务架构、高保真配置生成等创新点。

Result: 在110个模拟任务基准测试中，Foam - Agent与Claude 3.5 Sonnet搭配成功率达88.2%，远超现有框架。

Conclusion: Foam - Agent显著降低CFD专业门槛，证明专业多智能体系统可普及复杂科学计算。

Abstract: Computational Fluid Dynamics (CFD) is an essential simulation tool in
engineering, yet its steep learning curve and complex manual setup create
significant barriers. To address these challenges, we introduce Foam-Agent, a
multi-agent framework that automates the entire end-to-end OpenFOAM workflow
from a single natural language prompt. Our key innovations address critical
gaps in existing systems: 1. An Comprehensive End-to-End Simulation Automation:
Foam-Agent is the first system to manage the full simulation pipeline,
including advanced pre-processing with a versatile Meshing Agent capable of
handling external mesh files and generating new geometries via Gmsh, automatic
generation of HPC submission scripts, and post-simulation visualization via
ParaView. 2. Composable Service Architecture: Going beyond a monolithic agent,
the framework uses Model Context Protocol (MCP) to expose its core functions as
discrete, callable tools. This allows for flexible integration and use by other
agentic systems, such as Claude-code, for more exploratory workflows. 3.
High-Fidelity Configuration Generation: We achieve superior accuracy through a
Hierarchical Multi-Index RAG for precise context retrieval and a
dependency-aware generation process that ensures configuration consistency.
Evaluated on a benchmark of 110 simulation tasks, Foam-Agent achieves an 88.2%
success rate with Claude 3.5 Sonnet, significantly outperforming existing
frameworks (55.5% for MetaOpenFOAM). Foam-Agent dramatically lowers the
expertise barrier for CFD, demonstrating how specialized multi-agent systems
can democratize complex scientific computing. The code is public at
https://github.com/csml-rpi/Foam-Agent.

</details>


### [3] [SPADE: A Large Language Model Framework for Soil Moisture Pattern Recognition and Anomaly Detection in Precision Agriculture](https://arxiv.org/abs/2509.18123)
*Yeonju Lee,Rui Qi Chen,Joseph Oboamah,Po Nien Su,Wei-zhen Liang,Yeyin Shi,Lu Gan,Yongsheng Chen,Xin Qiao,Jing Li*

Main category: cs.AI

TL;DR: 本文提出SPADE框架，利用大语言模型分析土壤湿度时间序列数据，在异常检测和灌溉事件检测上表现优于现有方法，展示了大语言模型在精准农业中的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有土壤湿度时间序列分析方法适应性和可解释性有限，需要更好的方法用于灌溉调度和作物管理。

Method: 引入SPADE框架，利用ChatGPT - 4.1，将时间序列数据转为文本表示，设计领域相关提示模板进行分析。

Result: 在真实数据实验中，SPADE在异常检测上召回率和F1分数更高，能准确分类异常类型；在检测灌溉事件上有高精确率和召回率。

Conclusion: 大语言模型可作为精准农业中可扩展、适应性强的工具，用于准确监测土壤湿度和改进灌溉调度。

Abstract: Accurate interpretation of soil moisture patterns is critical for irrigation
scheduling and crop management, yet existing approaches for soil moisture
time-series analysis either rely on threshold-based rules or data-hungry
machine learning or deep learning models that are limited in adaptability and
interpretability. In this study, we introduce SPADE (Soil moisture Pattern and
Anomaly DEtection), an integrated framework that leverages large language
models (LLMs) to jointly detect irrigation patterns and anomalies in soil
moisture time-series data. SPADE utilizes ChatGPT-4.1 for its advanced
reasoning and instruction-following capabilities, enabling zero-shot analysis
without requiring task-specific annotation or fine-tuning. By converting
time-series data into a textual representation and designing domain-informed
prompt templates, SPADE identifies irrigation events, estimates net irrigation
gains, detects, classifies anomalies, and produces structured, interpretable
reports. Experiments were conducted on real-world soil moisture sensor data
from commercial and experimental farms cultivating multiple crops across the
United States. Results demonstrate that SPADE outperforms the existing method
in anomaly detection, achieving higher recall and F1 scores and accurately
classifying anomaly types. Furthermore, SPADE achieved high precision and
recall in detecting irrigation events, indicating its strong capability to
capture irrigation patterns accurately. SPADE's reports provide
interpretability and usability of soil moisture analytics. This study
highlights the potential of LLMs as scalable, adaptable tools for precision
agriculture, which is capable of integrating qualitative knowledge and
data-driven reasoning to produce actionable insights for accurate soil moisture
monitoring and improved irrigation scheduling from soil moisture time-series
data.

</details>


### [4] [Position Paper: Integrating Explainability and Uncertainty Estimation in Medical AI](https://arxiv.org/abs/2509.18132)
*Xiuyi Fan*

Main category: cs.AI

TL;DR: 提出可解释不确定性估计 (XUE) 整合可解释性与不确定性量化，推动可信医疗 AI 发展。


<details>
  <summary>Details</summary>
Motivation: 当前医疗 AI 系统无法有效量化和传达不确定性，可解释性工作与不确定性估计存在脱节，限制了 AI 在医学中的应用。

Method: 将医学不确定性映射到 AI 不确定性概念，识别实施 XUE 的关键挑战，提出技术方向和指导原则。

Result: 分析强调需要能生成可靠预测并以临床有意义方式表达置信水平的 AI 系统。

Conclusion: 该工作通过弥合可解释性和不确定性，为开发可信医疗 AI 做出贡献，使 AI 系统适应现实临床复杂性。

Abstract: Uncertainty is a fundamental challenge in medical practice, but current
medical AI systems fail to explicitly quantify or communicate uncertainty in a
way that aligns with clinical reasoning. Existing XAI works focus on
interpreting model predictions but do not capture the confidence or reliability
of these predictions. Conversely, uncertainty estimation (UE) techniques
provide confidence measures but lack intuitive explanations. The disconnect
between these two areas limits AI adoption in medicine. To address this gap, we
propose Explainable Uncertainty Estimation (XUE) that integrates explainability
with uncertainty quantification to enhance trust and usability in medical AI.
We systematically map medical uncertainty to AI uncertainty concepts and
identify key challenges in implementing XUE. We outline technical directions
for advancing XUE, including multimodal uncertainty quantification,
model-agnostic visualization techniques, and uncertainty-aware decision support
systems. Lastly, we propose guiding principles to ensure effective XUE
realisation. Our analysis highlights the need for AI systems that not only
generate reliable predictions but also articulate confidence levels in a
clinically meaningful way. This work contributes to the development of
trustworthy medical AI by bridging explainability and uncertainty, paving the
way for AI systems that are aligned with real-world clinical complexities.

</details>


### [5] [HSGM: Hierarchical Segment-Graph Memory for Scalable Long-Text Semantics](https://arxiv.org/abs/2509.18168)
*Dong Liu,Yanxuan Yu*

Main category: cs.AI

TL;DR: 提出Hierarchical Segment - Graph Memory (HSGM)框架处理长文档语义解析，理论降低复杂度，实验有速度、内存优势。


<details>
  <summary>Details</summary>
Motivation: 长文档语义解析因成对组合和内存需求二次增长而具有挑战性。

Method: 将长度为N的输入分解为M个有意义的段，构建局部语义图，提取摘要节点形成全局图内存，支持增量更新和分层查询处理。

Result: 理论上降低复杂度，实验在三个基准测试中实现2 - 4倍推理加速、峰值内存减少超60%、达到基线准确率的95%以上。

Conclusion: 该方法为超长文本实现可扩展、准确的语义建模，支持实时和资源受限的NLP应用。

Abstract: Semantic parsing of long documents remains challenging due to quadratic
growth in pairwise composition and memory requirements. We introduce
\textbf{Hierarchical Segment-Graph Memory (HSGM)}, a novel framework that
decomposes an input of length $N$ into $M$ meaningful segments, constructs
\emph{Local Semantic Graphs} on each segment, and extracts compact
\emph{summary nodes} to form a \emph{Global Graph Memory}. HSGM supports
\emph{incremental updates} -- only newly arrived segments incur local graph
construction and summary-node integration -- while \emph{Hierarchical Query
Processing} locates relevant segments via top-$K$ retrieval over summary nodes
and then performs fine-grained reasoning within their local graphs.
  Theoretically, HSGM reduces worst-case complexity from $O(N^2)$ to
$O\!\left(N\,k + (N/k)^2\right)$, with segment size $k \ll N$, and we derive
Frobenius-norm bounds on the approximation error introduced by node
summarization and sparsification thresholds. Empirically, on three benchmarks
-- long-document AMR parsing, segment-level semantic role labeling (OntoNotes),
and legal event extraction -- HSGM achieves \emph{2--4$\times$ inference
speedup}, \emph{$>60\%$ reduction} in peak memory, and \emph{$\ge 95\%$} of
baseline accuracy. Our approach unlocks scalable, accurate semantic modeling
for ultra-long texts, enabling real-time and resource-constrained NLP
applications.

</details>


### [6] [Memory-QA: Answering Recall Questions Based on Multimodal Memories](https://arxiv.org/abs/2509.18436)
*Hongda Jiang,Xinyuan Zhang,Siddhant Garg,Rishab Arora,Shiun-Zu Kuo,Jiayang Xu,Christopher Brossman,Yue Liu,Aaron Colak,Ahmed Aly,Anuj Kumar,Xin Luna Dong*

Main category: cs.AI

TL;DR: 提出Memory - QA任务，设计Pensieve管道应对挑战，创建基准测试，Pensieve性能优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 解决Memory - QA任务中创建任务导向记忆、有效利用记忆中时空信息、利用多记忆回答问题等挑战。

Method: 提出综合管道Pensieve，集成记忆特定增强、时空感知多信号检索和多记忆问答微调。

Result: 创建多模态基准测试，Pensieve在问答准确率上比现有方案最高提升14%。

Conclusion: Pensieve在Memory - QA任务中性能优于现有解决方案。

Abstract: We introduce Memory-QA, a novel real-world task that involves answering
recall questions about visual content from previously stored multimodal
memories. This task poses unique challenges, including the creation of
task-oriented memories, the effective utilization of temporal and location
information within memories, and the ability to draw upon multiple memories to
answer a recall question. To address these challenges, we propose a
comprehensive pipeline, Pensieve, integrating memory-specific augmentation,
time- and location-aware multi-signal retrieval, and multi-memory QA
fine-tuning. We created a multimodal benchmark to illustrate various real
challenges in this task, and show the superior performance of Pensieve over
state-of-the-art solutions (up to 14% on QA accuracy).

</details>


### [7] [Large Language Models and Operations Research: A Structured Survey](https://arxiv.org/abs/2509.18180)
*Yang Wang,Kai Li*

Main category: cs.AI

TL;DR: 本文综述大语言模型（LLMs）融入运筹学（OR）的进展，归纳方法方向，回顾评估基准和应用，总结问题并指出研究途径。


<details>
  <summary>Details</summary>
Motivation: 传统运筹学方法处理大规模、动态和多约束问题有挑战，LLMs有潜力解决这些局限。

Method: 将LLMs融入OR的方法归纳为自动建模、辅助优化和直接求解三个方向，回顾评估基准和特定领域应用。

Result: 总结出语义到结构映射不稳定、研究进展碎片化、泛化能力有限和评估系统不足等关键问题。

Conclusion: 概述了推进LLMs在OR中作用的可能研究途径。

Abstract: Operations research (OR) provides fundamental methodologies for complex
system decision-making, with established applications in transportation, supply
chain management, and production scheduling. Traditional approaches, which
depend on expert-based modeling and manual parameter adjustment, often face
challenges in handling large-scale, dynamic, and multi-constraint problems.
Recently, large language models (LLMs) have shown potential to address these
limitations through semantic understanding, structured generation, and
reasoning control. LLMs can translate natural language descriptions into
mathematical models or executable code, generate heuristics, evolve algorithms,
and directly tackle optimization tasks. This paper surveys recent progress on
the integration of LLMs into OR, organizing methods into three main directions:
automatic modeling, auxiliary optimization, and direct solving. It further
reviews evaluation benchmarks and domain-specific applications, and summarizes
key open issues such as unstable semantic-to-structure mapping, fragmented
research progress, limited generalization, and insufficient evaluation systems.
Finally, the survey outlines possible research avenues for advancing the role
of LLMs in OR.

</details>


### [8] [Synthesizing Attitudes, Predicting Actions (SAPA): Behavioral Theory-Guided LLMs for Ridesourcing Mode Choice Modeling](https://arxiv.org/abs/2509.18181)
*Mustafa Sameen,Xiaojian Zhang,Xilei Zhao*

Main category: cs.AI

TL;DR: 本文提出SAPA框架预测拼车模式选择，实验显示其显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有拼车模式选择预测模型因无法捕捉关键心理因素、存在严重类别不平衡问题，导致预测准确性有限。

Method: 提出SAPA框架，先使用大语言模型从原始出行调查数据生成定性旅行者角色，训练倾向得分模型，再由大语言模型为潜在变量赋值，最后集成倾向得分、潜在变量得分和可观察行程属性预测拼车模式选择。

Result: 在大规模多年出行调查实验中，SAPA显著优于现有基线，在保留测试集上PR - AUC指标下拼车选择预测提高达75.9%。

Conclusion: 该研究为准确预测拼车模式选择提供有力工具，且方法可迁移到多种应用。

Abstract: Accurate modeling of ridesourcing mode choices is essential for designing and
implementing effective traffic management policies for reducing congestion,
improving mobility, and allocating resources more efficiently. Existing models
for predicting ridesourcing mode choices often suffer from limited predictive
accuracy due to their inability to capture key psychological factors, and are
further challenged by severe class imbalance, as ridesourcing trips comprise
only a small fraction of individuals' daily travel. To address these
limitations, this paper introduces the Synthesizing Attitudes, Predicting
Actions (SAPA) framework, a hierarchical approach that uses Large Language
Models (LLMs) to synthesize theory-grounded latent attitudes to predict
ridesourcing choices. SAPA first uses an LLM to generate qualitative traveler
personas from raw travel survey data and then trains a propensity-score model
on demographic and behavioral features, enriched by those personas, to produce
an individual-level score. Next, the LLM assigns quantitative scores to
theory-driven latent variables (e.g., time and cost sensitivity), and a final
classifier integrates the propensity score, latent-variable scores (with their
interaction terms), and observable trip attributes to predict ridesourcing mode
choice. Experiments on a large-scale, multi-year travel survey show that SAPA
significantly outperforms state-of-the-art baselines, improving ridesourcing
choice predictions by up to 75.9% in terms of PR-AUC on a held-out test set.
This study provides a powerful tool for accurately predicting ridesourcing mode
choices, and provides a methodology that is readily transferable to various
applications.

</details>


### [9] [An Outcome-Based Educational Recommender System](https://arxiv.org/abs/2509.18186)
*Nursultan Askarbekuly,Timur Fayzrakhmanov,Sladjan Babarogić,Ivan Luković*

Main category: cs.AI

TL;DR: 介绍基于结果的教育推荐系统OBER，通过随机测试评估不同推荐方法，框架通用可扩展。


<details>
  <summary>Details</summary>
Motivation: 多数教育推荐系统基于点击或评分，真实教学影响不明，需评估算法对学习掌握度的提升。

Method: 采用简约实体关系模型、日志驱动掌握公式和插件架构，在非正规领域电子学习系统进行两周随机拆分测试。

Result: 协同过滤最大化留存率，固定路径实现最高掌握度。

Conclusion: OBER可从相同日志导出多种指标，从业者可权衡相关性和参与度与结果掌握度，框架通用且易扩展。

Abstract: Most educational recommender systems are tuned and judged on click- or
rating-based relevance, leaving their true pedagogical impact unclear. We
introduce OBER-an Outcome-Based Educational Recommender that embeds learning
outcomes and assessment items directly into the data schema, so any algorithm
can be evaluated on the mastery it fosters. OBER uses a minimalist
entity-relation model, a log-driven mastery formula, and a plug-in
architecture. Integrated into an e-learning system in non-formal domain, it was
evaluated trough a two-week randomized split test with over 5 700 learners
across three methods: fixed expert trajectory, collaborative filtering (CF),
and knowledge-based (KB) filtering. CF maximized retention, but the fixed path
achieved the highest mastery. Because OBER derives business, relevance, and
learning metrics from the same logs, it lets practitioners weigh relevance and
engagement against outcome mastery with no extra testing overhead. The
framework is method-agnostic and readily extensible to future adaptive or
context-aware recommenders.

</details>


### [10] [MMCD: Multi-Modal Collaborative Decision-Making for Connected Autonomy with Knowledge Distillation](https://arxiv.org/abs/2509.18198)
*Rui Liu,Zikang Wang,Peng Gao,Yu Shen,Pratap Tokekar,Ming Lin*

Main category: cs.AI

TL;DR: 本文提出MMCD框架用于连接式自主系统，通过多模态融合和跨模态知识蒸馏提升复杂条件下决策能力，实验显示提高驾驶安全性。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶系统在事故多发环境决策能力不足，且现有方法假设所有数据模态和连接车辆在训练和测试时都可用，不切实际。

Method: 引入MMCD框架融合多模态观测；采用基于师生模型结构的跨模态知识蒸馏方法，教师模型用多模态数据训练，学生模型在减少模态下有效运行。

Result: 在地面车辆和空地车辆协作实验中，该方法将驾驶安全性提高达20.7%，在检测潜在事故和做出安全驾驶决策方面超越现有最佳基线。

Conclusion: MMCD框架和跨模态知识蒸馏方法能有效提升连接式自主系统在复杂条件下的决策能力和驾驶安全性。

Abstract: Autonomous systems have advanced significantly, but challenges persist in
accident-prone environments where robust decision-making is crucial. A single
vehicle's limited sensor range and obstructed views increase the likelihood of
accidents. Multi-vehicle connected systems and multi-modal approaches,
leveraging RGB images and LiDAR point clouds, have emerged as promising
solutions. However, existing methods often assume the availability of all data
modalities and connected vehicles during both training and testing, which is
impractical due to potential sensor failures or missing connected vehicles. To
address these challenges, we introduce a novel framework MMCD (Multi-Modal
Collaborative Decision-making) for connected autonomy. Our framework fuses
multi-modal observations from ego and collaborative vehicles to enhance
decision-making under challenging conditions. To ensure robust performance when
certain data modalities are unavailable during testing, we propose an approach
based on cross-modal knowledge distillation with a teacher-student model
structure. The teacher model is trained with multiple data modalities, while
the student model is designed to operate effectively with reduced modalities.
In experiments on $\textit{connected autonomous driving with ground vehicles}$
and $\textit{aerial-ground vehicles collaboration}$, our method improves
driving safety by up to ${\it 20.7}\%$, surpassing the best-existing baseline
in detecting potential accidents and making safe driving decisions. More
information can be found on our website https://ruiiu.github.io/mmcd.

</details>


### [11] [Change in Quantitative Bipolar Argumentation: Sufficient, Necessary, and Counterfactual Explanations](https://arxiv.org/abs/2509.18215)
*Timotheus Kampik,Kristijonas Čyras,José Ruiz Alarcón*

Main category: cs.AI

TL;DR: 本文提出形式化方法解释定量双极论证框架中推理变化，追踪强度不一致性成因并给出解释，定义启发式方法及实现。


<details>
  <summary>Details</summary>
Motivation: 解决定量双极论证框架中推理变化的解释问题，明确更新框架后论证强度部分顺序变化的原因。

Method: 追踪语义在主题论证上建立的论证强度部分顺序的变化（强度不一致性），将其成因追溯到特定论证，识别充分、必要和反事实解释，定义启发式方法。

Result: 证明强度不一致性解释存在的充要条件是更新导致强度不一致，给出启发式方法并实现。

Conclusion: 所提方法可有效解释定量双极论证框架中推理的变化。

Abstract: This paper presents a formal approach to explaining change of inference in
Quantitative Bipolar Argumentation Frameworks (QBAFs). When drawing conclusions
from a QBAF and updating the QBAF to then again draw conclusions (and so on),
our approach traces changes -- which we call strength inconsistencies -- in the
partial order over argument strengths that a semantics establishes on some
arguments of interest, called topic arguments. We trace the causes of strength
inconsistencies to specific arguments, which then serve as explanations. We
identify sufficient, necessary, and counterfactual explanations for strength
inconsistencies and show that strength inconsistency explanations exist if and
only if an update leads to strength inconsistency. We define a heuristic-based
approach to facilitate the search for strength inconsistency explanations, for
which we also provide an implementation.

</details>


### [12] [nDNA -- the Semantic Helix of Artificial Cognition](https://arxiv.org/abs/2509.18216)
*Amitava Das*

Main category: cs.AI

TL;DR: 本文提出Neural DNA (nDNA)作为语义基因型表示，以捕捉AI基础模型的潜在身份，开启了神经基因组学领域，并介绍了其应用。


<details>
  <summary>Details</summary>
Motivation: 随着AI基础模型能力增长，探究其内部认知身份，传统基准测试只能测行为，需从潜在几何结构研究。

Method: 提出nDNA，由光谱曲率、热力学长度和信念向量场三个维度合成，将模型视为语义流体动力学进行分析。

Result: 得到稳定、无坐标的神经DNA指纹，可用于追溯模型谱系、测量继承性、检测漂移等。

Conclusion: 开启了神经基因组学领域，可用于比较模型、诊断风险和管理模型随时间的变化。

Abstract: As AI foundation models grow in capability, a deeper question emerges: What
shapes their internal cognitive identity -- beyond fluency and output?
Benchmarks measure behavior, but the soul of a model resides in its latent
geometry. In this work, we propose Neural DNA (nDNA) as a semantic-genotypic
representation that captures this latent identity through the intrinsic
geometry of belief. At its core, nDNA is synthesized from three principled and
indispensable dimensions of latent geometry: spectral curvature, which reveals
the curvature of conceptual flow across layers; thermodynamic length, which
quantifies the semantic effort required to traverse representational
transitions through layers; and belief vector field, which delineates the
semantic torsion fields that guide a model's belief directional orientations.
Like biological DNA, it encodes ancestry, mutation, and semantic inheritance,
found in finetuning and alignment scars, cultural imprints, and architectural
drift. In naming it, we open a new field: Neural Genomics, where models are not
just tools, but digital semantic organisms with traceable inner cognition.
  Modeling statement. We read AI foundation models as semantic fluid--dynamics:
meaning is transported through layers like fluid in a shaped conduit; nDNA is
the physics-grade readout of that flow -- a geometry-first measure of how
meaning is bent, paid for, and pushed -- yielding a stable, coordinate-free
neural DNA fingerprint tied to on-input behavior; with this fingerprint we
cross into biology: tracing lineages across pretraining, fine-tuning,
alignment, pruning, distillation, and merges; measuring inheritance between
checkpoints; detecting drift as traits shift under new data or objectives; and,
ultimately, studying the evolution of artificial cognition to compare models,
diagnose risks, and govern change over time.

</details>


### [13] [Similarity Field Theory: A Mathematical Framework for Intelligence](https://arxiv.org/abs/2509.18218)
*Kei-Sing Ng*

Main category: cs.AI

TL;DR: 本文介绍相似场理论，形式化实体间相似值原理及演化，给出智能生成定义，证明两定理，用于解读大语言模型和社会认知。


<details>
  <summary>Details</summary>
Motivation: 提出以持久和转变相似关系作为可理解动态系统的结构基础，构建数学框架来刻画、比较和构建智能系统。

Method: 定义相似场、系统演化、概念和生成算子，在框架内给出智能的生成定义，并证明两个定理。

Result: 证明了不对称性阻碍相互包含以及稳定性需要锚坐标或最终限制在水平集内，确保相似场演化受约束且可解释。

Conclusion: 相似场理论为刻画、比较和构建智能系统提供基础语言，可用于解读大语言模型和社会认知。

Abstract: We posit that persisting and transforming similarity relations form the
structural basis of any comprehensible dynamic system. This paper introduces
Similarity Field Theory, a mathematical framework that formalizes the
principles governing similarity values among entities and their evolution. We
define: (1) a similarity field $S: U \times U \to [0,1]$ over a universe of
entities $U$, satisfying reflexivity $S(E,E)=1$ and treated as a directed
relational field (asymmetry and non-transitivity are allowed); (2) the
evolution of a system through a sequence $Z_p = (X_p, S^{(p)})$ indexed by
$p=0,1,2,\ldots$; (3) concepts $K$ as entities that induce fibers
$F_{\alpha}(K) = { E \in U \mid S(E,K) \ge \alpha }$, i.e., superlevel sets of
the unary map $S_K(E) := S(E,K)$; and (4) a generative operator $G$ that
produces new entities. Within this framework, we formalize a generative
definition of intelligence: an operator $G$ is intelligent with respect to a
concept $K$ if, given a system containing entities belonging to the fiber of
$K$, it generates new entities that also belong to that fiber. Similarity Field
Theory thus offers a foundational language for characterizing, comparing, and
constructing intelligent systems. We prove two theorems: (i) asymmetry blocks
mutual inclusion; and (ii) stability requires either an anchor coordinate or
eventual confinement within a level set of $f$. These results ensure that the
evolution of similarity fields is both constrained and interpretable,
culminating in an exploration of how the framework allows us to interpret large
language models and use them as experimental probes into societal cognition.

</details>


### [14] [From latent factors to language: a user study on LLM-generated explanations for an inherently interpretable matrix-based recommender system](https://arxiv.org/abs/2509.18980)
*Maxime Manderlier,Fabian Lecron,Olivier Vu Thanh,Nicolas Gillis*

Main category: cs.AI

TL;DR: 研究大语言模型能否从可数学解释的推荐模型生成面向用户的有效解释，采用用户中心方法评估多种解释类型，结果显示各类型普遍受欢迎。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型能否从可数学解释的推荐模型生成有效用户解释，解决可解释AI中自动评估指标不能反映用户需求的问题。

Method: 基于约束矩阵分解的推荐模型，用精心设计的大语言模型提示转化为自然语言解释，开展有326名参与者的用户研究，评估解释和推荐，生成多种解释类型。

Result: 所有解释类型普遍受欢迎，不同策略间有适度统计差异，用户评论提供了定量结果之外的补充见解。

Conclusion: 大语言模型可从可数学解释的推荐模型生成有效面向用户的解释，不同解释策略都有一定效果。

Abstract: We investigate whether large language models (LLMs) can generate effective,
user-facing explanations from a mathematically interpretable recommendation
model. The model is based on constrained matrix factorization, where user types
are explicitly represented and predicted item scores share the same scale as
observed ratings, making the model's internal representations and predicted
scores directly interpretable. This structure is translated into natural
language explanations using carefully designed LLM prompts. Many works in
explainable AI rely on automatic evaluation metrics, which often fail to
capture users' actual needs and perceptions. In contrast, we adopt a
user-centered approach: we conduct a study with 326 participants who assessed
the quality of the explanations across five key dimensions-transparency,
effectiveness, persuasion, trust, and satisfaction-as well as the
recommendations themselves.To evaluate how different explanation strategies are
perceived, we generate multiple explanation types from the same underlying
model, varying the input information provided to the LLM. Our analysis reveals
that all explanation types are generally well received, with moderate
statistical differences between strategies. User comments further underscore
how participants react to each type of explanation, offering complementary
insights beyond the quantitative results.

</details>


### [15] [Multimodal Health Risk Prediction System for Chronic Diseases via Vision-Language Fusion and Large Language Models](https://arxiv.org/abs/2509.18221)
*Dingxin Lu,Shurui Wu,Xinyi Huang*

Main category: cs.AI

TL;DR: 提出VL - RiskFormer框架用于预测个体健康风险，有四项创新，在MIMIC - IV纵向队列上取得较好结果。


<details>
  <summary>Details</summary>
Motivation: 全球慢性病负担上升，临床数据多模态异构，急需统一的多模态AI框架来主动预测个体健康风险。

Method: 提出VL - RiskFormer，基于现有视觉语言模型的双流架构，有四项创新：用动量更新编码器和去偏InfoNCE损失进行预训练；用时间融合块整合不规则访问序列；用疾病本体图适配器注入ICD - 10代码并推断共病模式。

Result: 在MIMIC - IV纵向队列上，VL - RiskFormer平均AUROC为0.90，预期校准误差为2.7%。

Conclusion: 未明确提及，但从结果看VL - RiskFormer在预测个体健康风险方面有较好表现。

Abstract: With the rising global burden of chronic diseases and the multimodal and
heterogeneous clinical data (medical imaging, free-text recordings, wearable
sensor streams, etc.), there is an urgent need for a unified multimodal AI
framework that can proactively predict individual health risks. We propose
VL-RiskFormer, a hierarchical stacked visual-language multimodal Transformer
with a large language model (LLM) inference head embedded in its top layer. The
system builds on the dual-stream architecture of existing visual-linguistic
models (e.g., PaLM-E, LLaVA) with four key innovations: (i) pre-training with
cross-modal comparison and fine-grained alignment of radiological images,
fundus maps, and wearable device photos with corresponding clinical narratives
using momentum update encoders and debiased InfoNCE losses; (ii) a time fusion
block that integrates irregular visit sequences into the causal Transformer
decoder through adaptive time interval position coding; (iii) a disease
ontology map adapter that injects ICD-10 codes into visual and textual channels
in layers and infers comorbid patterns with the help of a graph attention
mechanism. On the MIMIC-IV longitudinal cohort, VL-RiskFormer achieved an
average AUROC of 0.90 with an expected calibration error of 2.7 percent.

</details>


### [16] [From "What to Eat?" to Perfect Recipe: ChefMind's Chain-of-Exploration for Ambiguous User Intent in Recipe Recommendation](https://arxiv.org/abs/2509.18226)
*Yu Fu,Linyue Cai,Ruoyu Wu,Yong Zhao*

Main category: cs.AI

TL;DR: 提出ChefMind混合架构用于个性化食谱推荐，经评估性能优于基线模型，处理模糊需求能力强。


<details>
  <summary>Details</summary>
Motivation: 解决个性化食谱推荐在处理模糊用户意图、保证语义准确性和提供足够细节覆盖方面的挑战。

Method: 提出ChefMind混合架构，结合CoE、KG、RAG和LLM，用CoE细化查询，KG提供语义推理，RAG补充细节，LLM整合输出。

Result: 在数据集和手动标注查询上评估，ChefMind在准确性、相关性、完整性和清晰度上表现优越，平均得分8.7，优于消融模型，未处理查询降至1.6%。

Conclusion: ChefMind在个性化食谱推荐中性能优越，能有效处理模糊需求。

Abstract: Personalized recipe recommendation faces challenges in handling fuzzy user
intent, ensuring semantic accuracy, and providing sufficient detail coverage.
We propose ChefMind, a hybrid architecture combining Chain of Exploration
(CoE), Knowledge Graph (KG), Retrieval-Augmented Generation (RAG), and a Large
Language Model (LLM). CoE refines ambiguous queries into structured conditions,
KG offers semantic reasoning and interpretability, RAG supplements contextual
culinary details, and LLM integrates outputs into coherent recommendations. We
evaluate ChefMind on the Xiachufang dataset and manually annotated queries,
comparing it with LLM-only, KG-only, and RAG-only baselines. Results show that
ChefMind achieves superior performance in accuracy, relevance, completeness,
and clarity, with an average score of 8.7 versus 6.4-6.7 for ablation models.
Moreover, it reduces unprocessed queries to 1.6%, demonstrating robustness in
handling fuzzy demands.

</details>


### [17] [An N-Plus-1 GPT Agency for Critical Solution of Mechanical Engineering Analysis Problems](https://arxiv.org/abs/2509.18229)
*Anthony Patera,Rohan Abeyaratne*

Main category: cs.AI

TL;DR: 生成式AI（如GPT）解决机械工程问题不可靠，提出‘N + 1’ GPT代理机构进行低成本分析，基于孔多塞陪审团定理可高概率得到正确解，与Grok Heavy有异同。


<details>
  <summary>Details</summary>
Motivation: 生成式AI解决机械工程问题有不可靠性，‘开箱即用’的GPT不适用于教育或工程实践。

Method: 引入‘N + 1’ GPT代理机构，先启动N个Agent Solve实例得到N个独立解决方案，再调用Agent Compare总结比较并给出推荐方案。

Result: 对于单次求解成功率超1/2且N足够大的问题，占主导的提议解决方案大概率正确，Agent Compare还能整合次要方案。

Conclusion: ‘N + 1’ GPT代理机构设计与Grok Heavy有相似和不同，更注重透明度和教学价值。

Abstract: Generative AI, and specifically GPT, can produce a remarkable solution to a
mechanical engineering analysis problem - but also, on occasion, a flawed
solution. For example, an elementary mechanics problem is solved flawlessly in
one GPT instance and incorrectly in a subsequent GPT instance, with a success
probability of only 85%. This unreliability renders "out-of-the-box" GPT
unsuitable for deployment in education or engineering practice. We introduce an
"N-Plus-1" GPT Agency for Initial (Low-Cost) Analysis of mechanical engineering
Problem Statements. Agency first launches N instantiations of Agent Solve to
yield N independent Proposed Problem Solution Realizations; Agency then invokes
Agent Compare to summarize and compare the N Proposed Problem Solution
Realizations and to provide a Recommended Problem Solution. We argue from
Condorcet's Jury Theorem that, for a Problem Statement characterized by
per-Solve success probability greater than 1/2 (and N sufficiently large), the
Predominant (Agent Compare) Proposed Problem Solution will, with high
probability, correspond to a Correct Proposed Problem Solution. Furthermore,
Agent Compare can also incorporate aspects of Secondary (Agent Compare)
Proposed Problem Solutions, in particular when the latter represent alternative
Problem Statement interpretations - different Mathematical Models - or
alternative Mathematical Solution Procedures. Comparisons to Grok Heavy, a
commercial multi-agent model, show similarities in design and performance, but
also important differences in emphasis: our Agency focuses on transparency and
pedagogical value.

</details>


### [18] [Towards General Computer Control with Hierarchical Agents and Multi-Level Action Spaces](https://arxiv.org/abs/2509.18230)
*Zihan Dong,Xinyu Fan,Zixiang Tang,Yunqing Li*

Main category: cs.AI

TL;DR: 提出轻量级分层强化学习框架ComputerAgent解决桌面应用控制问题，在多任务上表现良好，证明分层RL是计算机控制实用可扩展方案。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在控制桌面应用时存在推理延迟高、样本效率低和难以在设备上部署等问题。

Method: 引入ComputerAgent框架，将操作系统控制制定为两级选项过程，采用三模态状态编码器，集成元动作与早停机制，使用紧凑视觉骨干和小策略网络进行设备上推理。

Result: 在135个真实桌面任务中，简单任务成功率达92.1%，困难任务达58.8%，在简单场景下表现匹配或超200B参数MLLM基线，模型大小降超四个数量级，推理时间减半。

Conclusion: 分层强化学习为基于整体MLLM的计算机控制自动化提供了实用、可扩展的替代方案。

Abstract: Controlling desktop applications via software remains a fundamental yet
under-served problem. Existing multi-modal large language models (MLLMs) ingest
screenshots and task instructions to generate keystrokes and mouse events, but
they suffer from prohibitive inference latency, poor sample efficiency on
long-horizon sparse-reward tasks, and infeasible on-device deployment. We
introduce a lightweight hierarchical reinforcement learning framework,
ComputerAgent, that formulates OS control as a two-level option process
(manager and subpolicy), employs a triple-modal state encoder (screenshot, task
ID, numeric state) to handle visual and contextual diversity, integrates
meta-actions with an early-stop mechanism to reduce wasted interactions, and
uses a compact vision backbone plus small policy networks for on-device
inference (15M parameters). On a suite of 135 real-world desktop tasks,
ComputerAgent attains 92.1% success on simple tasks (<8 steps) and 58.8% on
hard tasks (>=8 steps), matching or exceeding 200B-parameter MLLM baselines on
simple scenarios while reducing model size by over four orders of magnitude and
halving inference time. These results demonstrate that hierarchical RL offers a
practical, scalable alternative to monolithic MLLM-based automation for
computer control.

</details>


### [19] [The Illusion of Readiness: Stress Testing Large Frontier Models on Multimodal Medical Benchmarks](https://arxiv.org/abs/2509.18234)
*Yu Gu,Jingjing Fu,Xiaodong Liu,Jeya Maria Jose Valanarasu,Noel Codella,Reuben Tan,Qianchu Liu,Ying Jin,Sheng Zhang,Jinyu Wang,Rui Wang,Lei Song,Guanghui Qin,Naoto Usuyama,Cliff Wong,Cheng Hao,Hohin Lee,Praneeth Sanapathi,Sarah Hilado,Bian Jiang,Javier Alvarez-Valle,Mu Wei,Jianfeng Gao,Eric Horvitz,Matt Lungren,Hoifung Poon,Paul Vozila*

Main category: cs.AI

TL;DR: 前沿大模型在医学基准测试中得分高，但压力测试显示存在问题，基准测试有缺陷，得分不能反映实际应用能力。


<details>
  <summary>Details</summary>
Motivation: 揭示当前医学基准测试不能真实反映AI模型在医疗领域的实际能力，促使AI在医疗领域获得信任。

Method: 对六个旗舰模型在六个常用基准测试中进行评估，通过临床医生指导的评分标准评估。

Result: 高排行榜分数掩盖了模型的脆弱性和捷径学习，基准测试衡量内容差异大却被混用，掩盖了失败模式。

Conclusion: 医学基准测试分数不能直接反映现实应用的准备情况，要对AI系统的鲁棒性、推理能力和符合实际医疗需求等方面提出更高要求。

Abstract: Large frontier models like GPT-5 now achieve top scores on medical
benchmarks. But our stress tests tell a different story. Leading systems often
guess correctly even when key inputs like images are removed, flip answers
under trivial prompt changes, and fabricate convincing yet flawed reasoning.
These aren't glitches; they expose how today's benchmarks reward test-taking
tricks over medical understanding. We evaluate six flagship models across six
widely used benchmarks and find that high leaderboard scores hide brittleness
and shortcut learning. Through clinician-guided rubric evaluation, we show that
benchmarks vary widely in what they truly measure yet are treated
interchangeably, masking failure modes. We caution that medical benchmark
scores do not directly reflect real-world readiness. If we want AI to earn
trust in healthcare, we must demand more than leaderboard wins and must hold
systems accountable for robustness, sound reasoning, and alignment with real
medical demands.

</details>


### [20] [Evaluating the Safety and Skill Reasoning of Large Reasoning Models Under Compute Constraints](https://arxiv.org/abs/2509.18382)
*Adarsha Balaji,Le Chen,Rajeev Thakur,Franck Cappello,Sandeep Madireddy*

Main category: cs.AI

TL;DR: 研究两种计算约束策略（推理长度约束和模型量化）降低推理模型计算需求及对安全性能的影响，探索应用约束的方法并研究计算效率与模型安全性的权衡。


<details>
  <summary>Details</summary>
Motivation: 测试时计算扩展虽能提升推理语言模型性能，但会显著增加计算成本，需要降低计算需求并研究对安全性能的影响。

Method: 一是用基于长度控制策略优化（LCPO）的强化学习方法微调推理模型以满足用户定义的思维链（CoT）推理长度；二是应用量化以在用户定义的计算约束内最大化CoT序列生成。

Result: 未提及。

Conclusion: 未提及。

Abstract: Test-time compute scaling has demonstrated the ability to improve the
performance of reasoning language models by generating longer chain-of-thought
(CoT) sequences. However, this increase in performance comes with a significant
increase in computational cost. In this work, we investigate two compute
constraint strategies: (1) reasoning length constraint and (2) model
quantization, as methods to reduce the compute demand of reasoning models and
study their impact on their safety performance. Specifically, we explore two
approaches to apply compute constraints to reasoning models: (1) fine-tuning
reasoning models using a length controlled policy optimization (LCPO) based
reinforcement learning method to satisfy a user-defined CoT reasoning length,
and (2) applying quantization to maximize the generation of CoT sequences
within a user-defined compute constraint. Furthermore, we study the trade-off
between the computational efficiency and the safety of the model.

</details>


### [21] [Gödel Test: Can Large Language Models Solve Easy Conjectures?](https://arxiv.org/abs/2509.18383)
*Moran Feldman,Amin Karbasi*

Main category: cs.AI

TL;DR: 研究GPT - 5在哥德尔测试中对组合优化领域五个猜想的表现，结果显示有进展也有局限。


<details>
  <summary>Details</summary>
Motivation: 前沿AI模型在数学竞赛表现好，但不确定能否解决高级数学领域新的简单猜想，提出哥德尔测试。

Method: 研究GPT - 5对组合优化中五个猜想的表现，提供猜想来源论文，评估模型推理。

Result: GPT - 5在三个较简单问题上给出接近正确的解，反驳了一个猜想；在问题4失败；在问题5提出相同算法但分析失败。

Conclusion: 虽样本小，但结果显示有常规推理进展和偶尔原创性，跨论文综合时有局限，GPT - 5可能是前沿模型通过哥德尔测试的早期一步。

Abstract: Recent announcements from frontier AI model labs have highlighted strong
results on high-school and undergraduate math competitions. Yet it remains
unclear whether large language models can solve new, simple conjectures in more
advanced areas of mathematics. We propose the G\"odel Test: evaluating whether
a model can produce correct proofs for very simple, previously unsolved
conjectures. To this end, we study the performance of GPT-5 on five conjectures
in combinatorial optimization. For each problem, we provided one or two source
papers from which the conjecture arose, withheld our own conjecture, and then
assessed the model's reasoning in detail. On the three easier problems, GPT-5
produced nearly correct solutions; for Problem 2 it even derived a different
approximation guarantee that, upon checking, refuted our conjecture while
providing a valid solution. The model failed on Problem 4, which required
combining results from two papers. On Problem 5, a harder case without a
validated conjecture, GPT-5 proposed the same algorithm we had in mind but
failed in the analysis, suggesting the proof is more challenging than expected.
Although our sample is small, the results point to meaningful progress on
routine reasoning, occasional flashes of originality, and clear limitations
when cross-paper synthesis is required. GPT-5 may represent an early step
toward frontier models eventually passing the G\"odel Test.

</details>


### [22] [ATLAS: Benchmarking and Adapting LLMs for Global Trade via Harmonized Tariff Code Classification](https://arxiv.org/abs/2509.18400)
*Pritish Yuvraj,Siva Devarakonda*

Main category: cs.AI

TL;DR: 文章提出首个HTS代码分类基准，微调的Atlas模型在分类上比GPT - 5和Gemini - 2.5表现好，成本低且可自托管，发布数据集和模型以推动该领域研究。


<details>
  <summary>Details</summary>
Motivation: HTS产品准确分类是全球贸易关键瓶颈，机器学习领域对此关注少，误分类会导致运输停滞。

Method: 从美国海关裁决在线搜索系统获取数据创建基准，评估主流大语言模型，微调Atlas模型。

Result: 微调的Atlas模型10位分类准确率40%，6位分类准确率57.5%，优于GPT - 5和Gemini - 2.5，成本低且可自托管保障数据隐私。

Conclusion: Atlas模型设定了良好基线，但基准任务仍具挑战性，发布数据集和模型以推动HTS分类成为新的社区基准任务。

Abstract: Accurate classification of products under the Harmonized Tariff Schedule
(HTS) is a critical bottleneck in global trade, yet it has received little
attention from the machine learning community. Misclassification can halt
shipments entirely, with major postal operators suspending deliveries to the
U.S. due to incomplete customs documentation. We introduce the first benchmark
for HTS code classification, derived from the U.S. Customs Rulings Online
Search System (CROSS). Evaluating leading LLMs, we find that our fine-tuned
Atlas model (LLaMA-3.3-70B) achieves 40 percent fully correct 10-digit
classifications and 57.5 percent correct 6-digit classifications, improvements
of 15 points over GPT-5-Thinking and 27.5 points over Gemini-2.5-Pro-Thinking.
Beyond accuracy, Atlas is roughly five times cheaper than GPT-5-Thinking and
eight times cheaper than Gemini-2.5-Pro-Thinking, and can be self-hosted to
guarantee data privacy in high-stakes trade and compliance workflows. While
Atlas sets a strong baseline, the benchmark remains highly challenging, with
only 40 percent 10-digit accuracy. By releasing both dataset and model, we aim
to position HTS classification as a new community benchmark task and invite
future work in retrieval, reasoning, and alignment.

</details>


### [23] [Instruction-Following Evaluation in Function Calling for Large Language Models](https://arxiv.org/abs/2509.18420)
*Nikolai Skripko*

Main category: cs.AI

TL;DR: 本文介绍新基准IFEval - FC评估函数调用中精确遵循指令的能力，结果显示先进模型常不遵守格式规则。


<details>
  <summary>Details</summary>
Motivation: 现有基准不测试函数调用中对参数描述里格式指令的遵循情况，需新基准评估。

Method: 引入IFEval - FC，将可验证格式编码在JSON模式描述中，有750个测试用例，全算法评估。

Result: 即使是GPT - 5和Claude 4.1 Opus等先进模型也常不遵守基本格式规则。

Conclusion: 当前先进模型在遵循格式规则上存在实际限制，代码和数据公开。

Abstract: Function calling is a core capability of large language models, essential for
AI agents. Existing benchmarks such as the Berkeley Function Calling
Leaderboard (BFCL), tau^2-Bench (arXiv:2506.07982), and ACEBench
(arXiv:2501.12851) evaluate argument correctness but do not test adherence to
format instructions embedded in parameter descriptions, such as enclosing
values in double quotes or using ISO date formats.
  We introduce IFEval-FC, a benchmark inspired by IFEval (arXiv:2311.07911)
that assesses precise instruction following in function calling. IFEval-FC
encodes verifiable formats directly within JSON schema descriptions, for
example specifying that a value must not contain punctuation. It includes 750
test cases, each consisting of a function with an embedded format for one of
its input parameters and a corresponding user query. Evaluation is fully
algorithmic, ensuring objectivity, reproducibility, and scalability.
  Our results show that even state-of-the-art proprietary models, including
GPT-5 and Claude 4.1 Opus, frequently fail to follow basic formatting rules,
highlighting a practical limitation for real-world agent systems. The complete
codebase and data are publicly available at
https://github.com/Skripkon/IFEval-FC.

</details>


### [24] [FERA: Foil Fencing Referee Assistant Using Pose-Based Multi-Label Move Recognition and Rule Reasoning](https://arxiv.org/abs/2509.18527)
*Ziwen Chen,Zhong Wang*

Main category: cs.AI

TL;DR: 提出击剑裁判助手FERA，结合动作识别和规则推理，在有限数据下表现优于多个基线模型，展示了自动裁判辅助的前景。


<details>
  <summary>Details</summary>
Motivation: 解决击剑裁判面临的主观判罚、人为错误、偏见和实践环境中裁判资源有限等问题。

Method: FERA从视频中提取2D关节位置，归一化后计算运动学特征集，用Transformer进行多标签动作和刀刃分类，用蒸馏语言模型结合规则确定优先级和得分。

Result: 在有限手工标注数据上，5折交叉验证平均宏F1分数达0.549，优于多个基线模型。

Conclusion: 虽未准备好部署，但为花剑自动裁判辅助和击剑领域AI应用展示了有前景的方向。

Abstract: The sport of fencing, like many other sports, faces challenges in refereeing:
subjective calls, human errors, bias, and limited availability in practice
environments. We present FERA (Fencing Referee Assistant), a prototype AI
referee for foil fencing which integrates pose-based multi-label action
recognition and rule-based reasoning. FERA extracts 2D joint positions from
video, normalizes them, computes a 101-dimensional kinematic feature set, and
applies a Transformer for multi-label move and blade classification. To
determine priority and scoring, FERA applies a distilled language model with
encoded right-of-way rules, producing both a decision and an explanation for
each exchange. With limited hand-labeled data, a 5-fold cross-validation
achieves an average macro-F1 score of 0.549, outperforming multiple baselines,
including a Temporal Convolutional Network (TCN), BiLSTM, and a vanilla
Transformer. While not ready for deployment, these results demonstrate a
promising path towards automated referee assistance in foil fencing and new
opportunities for AI applications, such as coaching in the field of fencing.

</details>


### [25] [LLMZ+: Contextual Prompt Whitelist Principles for Agentic LLMs](https://arxiv.org/abs/2509.18557)
*Tom Pawelek,Raj Patel,Charlotte Crowell,Noorbakhsh Amiri,Sudip Mittal,Shahram Rahimi,Andy Perkins*

Main category: cs.AI

TL;DR: 传统模型相比，代理式 AI 有安全风险，本文提出 LLMZ+ 方法，通过提示白名单增强安全，实验证明其有效且不干扰合法通信。


<details>
  <summary>Details</summary>
Motivation: 代理式 AI 因访问数据源和 API 工具，具有非确定性，给安全带来重大风险，传统检测机制有局限，需新方法。

Method: 提出 LLMZ+ 方法，通过实施提示白名单，只允许上下文合适且安全的消息与代理式 LLM 交互。

Result: LLMZ+ 对常见越狱提示有强大抵御能力，不干扰合法业务通信，实验中误报率和漏报率可降为 0。

Conclusion: LLMZ+ 简化安全框架，增强长期弹性，减少维护 LLM 信息安全所需资源。

Abstract: Compared to traditional models, agentic AI represents a highly valuable
target for potential attackers as they possess privileged access to data
sources and API tools, which are traditionally not incorporated into classical
agents. Unlike a typical software application residing in a Demilitarized Zone
(DMZ), agentic LLMs consciously rely on nondeterministic behavior of the AI
(only defining a final goal, leaving the path selection to LLM). This
characteristic introduces substantial security risk to both operational
security and information security. Most common existing defense mechanism rely
on detection of malicious intent and preventing it from reaching the LLM agent,
thus protecting against jailbreak attacks such as prompt injection. In this
paper, we present an alternative approach, LLMZ+, which moves beyond
traditional detection-based approaches by implementing prompt whitelisting.
Through this method, only contextually appropriate and safe messages are
permitted to interact with the agentic LLM. By leveraging the specificity of
context, LLMZ+ guarantees that all exchanges between external users and the LLM
conform to predefined use cases and operational boundaries. Our approach
streamlines the security framework, enhances its long-term resilience, and
reduces the resources required for sustaining LLM information security. Our
empirical evaluation demonstrates that LLMZ+ provides strong resilience against
the most common jailbreak prompts. At the same time, legitimate business
communications are not disrupted, and authorized traffic flows seamlessly
between users and the agentic LLM. We measure the effectiveness of approach
using false positive and false negative rates, both of which can be reduced to
0 in our experimental setting.

</details>


### [26] [Solving Math Word Problems Using Estimation Verification and Equation Generation](https://arxiv.org/abs/2509.18565)
*Mitchell Piehl,Dillon Wilson,Ananya Kalita,Jugal Kalita*

Main category: cs.AI

TL;DR: 本文提出新方法助大语言模型解决数学文字问题，在多数据集取得新SOTA结果，还引入两个新数据集。


<details>
  <summary>Details</summary>
Motivation: 大语言模型解决数学文字问题有挑战，现有改进提示方法仍有提升空间。

Method: 先提示大语言模型从问题分解创建方程，用外部符号方程求解器得出答案；再让模型估算答案进行验证，验证失败则迭代修正。

Result: 在数值和代数数学文字问题数据集上取得新SOTA结果，平均提升近2%；在三角数学文字问题上获满意结果。

Conclusion: 所提方法有效提升大语言模型解决数学文字问题能力，新数据集有助于进一步测试其推理能力。

Abstract: Large Language Models (LLMs) excel at various tasks, including
problem-solving and question-answering. However, LLMs often find Math Word
Problems (MWPs) challenging because solving them requires a range of reasoning
and mathematical abilities with which LLMs seem to struggle. Recent efforts
have helped LLMs solve more complex MWPs with improved prompts. This study
proposes a novel method that initially prompts an LLM to create equations from
a decomposition of the question, followed by using an external symbolic
equation solver to produce an answer. To ensure the accuracy of the obtained
answer, inspired by an established recommendation of math teachers, the LLM is
instructed to solve the MWP a second time, but this time with the objective of
estimating the correct answer instead of solving it exactly. The estimation is
then compared to the generated answer to verify. If verification fails, an
iterative rectification process is employed to ensure the correct answer is
eventually found. This approach achieves new state-of-the-art results on
datasets used by prior published research on numeric and algebraic MWPs,
improving the previous best results by nearly two percent on average. In
addition, the approach obtains satisfactory results on trigonometric MWPs, a
task not previously attempted to the authors' best knowledge. This study also
introduces two new datasets, SVAMPClean and Trig300, to further advance the
testing of LLMs' reasoning abilities.

</details>


### [27] [Adaptive Learning in Spatial Agent-Based Models for Climate Risk Assessment: A Geospatial Framework with Evolutionary Economic Agents](https://arxiv.org/abs/2509.18633)
*Yara Mohajerani*

Main category: cs.AI

TL;DR: 提出集成气候灾害数据与经济主体进化学习的地理空间基于主体的模型，以评估气候风险，展示其效果并揭示系统风险，为金融机构和企业提供工具。


<details>
  <summary>Details</summary>
Motivation: 气候风险评估需要对空间异质危害和适应性经济系统间复杂相互作用进行建模。

Method: 结合基于Mesa的空间建模与CLIMADA气候影响评估，引入自适应学习行为，使企业通过基于适应度的选择和变异进化策略。

Result: 进化适应使企业在气候压力导致数十年中断后能接近基线生产水平；未直接受洪水影响的主体也会因供应链中断受影响，本世纪末RCP8.5情景下商品平均价格比基线高5.6%。

Conclusion: 该开源框架为金融机构和企业提供量化直接和级联气候风险及评估成本效益适应策略的工具。

Abstract: Climate risk assessment requires modelling complex interactions between
spatially heterogeneous hazards and adaptive economic systems. We present a
novel geospatial agent-based model that integrates climate hazard data with
evolutionary learning for economic agents. Our framework combines Mesa-based
spatial modelling with CLIMADA climate impact assessment, introducing adaptive
learning behaviours that allow firms to evolve strategies for budget
allocation, pricing, wages, and risk adaptation through fitness-based selection
and mutation. We demonstrate the framework using riverine flood projections
under RCP8.5 until 2100, showing that evolutionary adaptation enables firms to
converge with baseline (no hazard) production levels after decades of
disruption due to climate stress. Our results reveal systemic risks where even
agents that are not directly exposed to floods face impacts through supply
chain disruptions, with the end-of-century average price of goods 5.6% higher
under RCP8.5 compared to the baseline. This open-source framework provides
financial institutions and companies with tools to quantify both direct and
cascading climate risks while evaluating cost-effective adaptation strategies.

</details>


### [28] [TERAG: Token-Efficient Graph-Based Retrieval-Augmented Generation](https://arxiv.org/abs/2509.18667)
*Qiao Xiao,Hong Ting Tsang,Jiaxin Bai*

Main category: cs.AI

TL;DR: 提出TERAG框架以低成本构建信息图，用少量输出token达常用图RAG方法至少80%准确率。


<details>
  <summary>Details</summary>
Motivation: 现有图RAG系统构建图时忽略LLM token使用高成本问题，阻碍大规模应用。

Method: 受HippoRAG启发，在检索阶段引入个性化PageRank（PPR）。

Result: 达到常用图RAG方法至少80%的准确率，仅消耗3%-11%的输出token。

Conclusion: TERAG是一种简单有效的框架，能以更低成本构建信息图。

Abstract: Graph-based Retrieval-augmented generation (RAG) has become a widely studied
approach for improving the reasoning, accuracy, and factuality of Large
Language Models. However, many existing graph-based RAG systems overlook the
high cost associated with LLM token usage during graph construction, hindering
large-scale adoption. To address this, we propose TERAG, a simple yet effective
framework designed to build informative graphs at a significantly lower cost.
Inspired by HippoRAG, we incorporate Personalized PageRank (PPR) during the
retrieval phase, and we achieve at least 80% of the accuracy of widely used
graph-based RAG methods while consuming only 3%-11% of the output tokens.

</details>


### [29] [Implementation of airborne ML models with semantics preservation](https://arxiv.org/abs/2509.18681)
*Nicolas Valot,Louis Fabre,Benjamin Lesage,Ammar Mechouche,Claire Pagetti*

Main category: cs.AI

TL;DR: 本文探讨机器学习在机载系统应用中的安全合规问题，明确ML模型与MLMD区别，细化语义保存概念并应用于工业用例。


<details>
  <summary>Details</summary>
Motivation: 机器学习在机载系统有新能力，但需保证安全运行并符合相关指导，现有标准仅给出高层目标，需进一步明确相关概念。

Method: 明确ML模型和MLMD的差异，细化语义保存概念，并应用于多个工业用例构建和比较目标模型。

Result: 文中未明确提及具体结果。

Conclusion: 文中未明确提及具体结论。

Abstract: Machine Learning (ML) may offer new capabilities in airborne systems.
However, as any piece of airborne systems, ML-based systems will be required to
guarantee their safe operation. Thus, their development will have to be
demonstrated to be compliant with the adequate guidance. So far, the European
Union Aviation Safety Agency (EASA) has published a concept paper and an
EUROCAE/SAE group is preparing ED-324. Both approaches delineate high-level
objectives to confirm the ML model achieves its intended function and maintains
training performance in the target environment. The paper aims to clarify the
difference between an ML model and its corresponding unambiguous description,
referred to as the Machine Learning Model Description (MLMD). It then refines
the essential notion of semantics preservation to ensure the accurate
replication of the model. We apply our contributions to several industrial use
cases to build and compare several target models.

</details>


### [30] [Advances in Large Language Models for Medicine](https://arxiv.org/abs/2509.18690)
*Zhiyu Kan,Wensheng Gan,Zhenlian Qi,Philip S. Yu*

Main category: cs.AI

TL;DR: 本文系统回顾大语言模型在医疗领域的研究进展，对其训练、应用等进行分析，分类并提出挑战解决方案和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在各行业影响渐大，医疗领域应用突出，为突出发展医疗大语言模型的必要性，加深对其发展现状的理解并为后续研究提供指导。

Method: 系统回顾过往和前沿研究成果，对医疗大语言模型按训练方法分类，对评估方法分两类。

Result: 分析了大型医疗模型的训练技术、医疗场景适应性、相关应用及优缺点，对医疗大语言模型和评估方法进行分类。

Conclusion: 提出了现有挑战的解决方案和未来研究方向。

Abstract: Artificial intelligence (AI) technology has advanced rapidly in recent years,
with large language models (LLMs) emerging as a significant breakthrough. LLMs
are increasingly making an impact across various industries, with the medical
field standing out as the most prominent application area. This paper
systematically reviews the up-to-date research progress of LLMs in the medical
field, providing an in-depth analysis of training techniques for large medical
models, their adaptation in healthcare settings, related applications, as well
as their strengths and limitations. Furthermore, it innovatively categorizes
medical LLMs into three distinct types based on their training methodologies
and classifies their evaluation approaches into two categories. Finally, the
study proposes solutions to existing challenges and outlines future research
directions based on identified issues in the field of medical LLMs. By
systematically reviewing previous and advanced research findings, we aim to
highlight the necessity of developing medical LLMs, provide a deeper
understanding of their current state of development, and offer clear guidance
for subsequent research.

</details>


### [31] [Autonomous Data Agents: A New Opportunity for Smart Data](https://arxiv.org/abs/2509.18710)
*Yanjie Fu,Dongjie Wang,Wangyang Ying,Xiangliang Zhang,Huan Liu,Jian Pei*

Main category: cs.AI

TL;DR: 随着数据规模和复杂性增加，数据处理困难，自主数据代理（DataAgents）可将复杂非结构化数据转化为知识，代表向自主数据到知识系统的范式转变，报告探讨其多方面内容并呼吁推进相关工作。


<details>
  <summary>Details</summary>
Motivation: 数据处理劳动密集、难扩展，且数据结构不利于AI利用，需研究数据密集操作能封装多少知识，Agent AI与数据到知识系统融合成关键趋势。

Method: 定义DataAgents概念，讨论其架构设计、训练策略、新技能和能力。

Result: DataAgents能处理多种数据操作，将复杂非结构化数据转化为连贯可行的知识。

Conclusion: 呼吁共同努力推进动作工作流优化、建立开放数据集和基准生态系统、保障隐私、平衡效率与可扩展性、开发可信防护栏防止恶意行为。

Abstract: As data continues to grow in scale and complexity, preparing, transforming,
and analyzing it remains labor-intensive, repetitive, and difficult to scale.
Since data contains knowledge and AI learns knowledge from it, the alignment
between AI and data is essential. However, data is often not structured in ways
that are optimal for AI utilization. Moreover, an important question arises:
how much knowledge can we pack into data through intensive data operations?
Autonomous data agents (DataAgents), which integrate LLM reasoning with task
decomposition, action reasoning and grounding, and tool calling, can
autonomously interpret data task descriptions, decompose tasks into subtasks,
reason over actions, ground actions into python code or tool calling, and
execute operations. Unlike traditional data management and engineering tools,
DataAgents dynamically plan workflows, call powerful tools, and adapt to
diverse data tasks at scale. This report argues that DataAgents represent a
paradigm shift toward autonomous data-to-knowledge systems. DataAgents are
capable of handling collection, integration, preprocessing, selection,
transformation, reweighing, augmentation, reprogramming, repairs, and
retrieval. Through these capabilities, DataAgents transform complex and
unstructured data into coherent and actionable knowledge. We first examine why
the convergence of agentic AI and data-to-knowledge systems has emerged as a
critical trend. We then define the concept of DataAgents and discuss their
architectural design, training strategies, as well as the new skills and
capabilities they enable. Finally, we call for concerted efforts to advance
action workflow optimization, establish open datasets and benchmark ecosystems,
safeguard privacy, balance efficiency with scalability, and develop trustworthy
DataAgent guardrails to prevent malicious actions.

</details>


### [32] [Experience Scaling: Post-Deployment Evolution For Large Language Models](https://arxiv.org/abs/2509.18771)
*Xingkun Yin,Kaibin Huang,Dong In Kim,Hongyang Du*

Main category: cs.AI

TL;DR: 传统大语言模型扩展方法遇瓶颈，提出经验扩展框架，经模拟验证能提升性能，拓展能力。


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型扩展方法因人类文本耗尽和收益递减而达到饱和，需新方法。

Method: 提出经验扩展框架，通过与环境自主交互和经验共享实现部署后持续进化，包括捕捉交互、提炼知识、定期优化存储内容。

Result: 在模拟场景中，经验扩展提升了准确性，随时间维持性能，在新情况中也能保持优势。

Conclusion: 结构化的部署后学习可突破静态人类数据限制，为大语言模型持续智能发展提供可扩展路径。

Abstract: Scaling model size, training data, and compute power have driven advances in
large language models (LLMs), but these approaches are reaching saturation as
human-generated text is exhausted and further gains diminish. We propose
experience scaling, a framework for continuous post-deployment evolution for
LLMs through autonomous interaction with the environment and collaborative
sharing of accumulated experience. The framework captures raw interactions,
distills them into compact, reusable knowledge, and periodically refines stored
content to preserve relevance and efficiency. We validate the framework in
simulated real-world scenarios involving generalization to previously unseen
but related tasks, repetitive queries, and over-saturated knowledge stores.
Across all settings, experience scaling improves accuracy, sustains performance
over time, and maintains gains when applied to novel situations. These results
demonstrate that structured post-deployment learning can extend LLM
capabilities beyond the limits of static human-generated data, offering a
scalable path for continued intelligence progress.

</details>


### [33] [The AGNTCY Agent Directory Service: Architecture and Implementation](https://arxiv.org/abs/2509.18787)
*Luca Muscariello,Vijoy Pandey,Ramiz Polic*

Main category: cs.AI

TL;DR: 介绍Agent Directory Service (ADS)分布式目录，其基于OASF，有高效、可验证等特点，论文阐述架构模型等内容。


<details>
  <summary>Details</summary>
Motivation: 为异构多智能体系统提供高效、可验证和多维度的AI智能体能力、元数据和来源发现服务。

Method: 利用内容寻址存储、分层分类法和加密签名，基于Kademlia的分布式哈希表实现两级映射，复用OCI / ORAS基础设施，集成Sigstore。

Result: 构建了ADS系统，实现了能力索引与内容位置的解耦，支持新兴智能体模式的可扩展性。

Conclusion: 论文对ADS的架构模型、存储和发现层、安全和性能特性进行了形式化描述，并将其置于新兴智能体注册表和互操作性倡议的大背景中。

Abstract: The Agent Directory Service (ADS) is a distributed directory for the
discovery of AI agent capabilities, metadata, and provenance. It leverages
content-addressed storage, hierarchical taxonomies, and cryptographic signing
to enable efficient, verifiable, and multi-dimensional discovery across
heterogeneous Multi-Agent Systems (MAS). Built on the Open Agentic Schema
Framework (OASF), ADS decouples capability indexing from content location
through a two-level mapping realized over a Kademlia-based Distributed Hash
Table (DHT). It reuses mature OCI / ORAS infrastructure for artifact
distribution, integrates Sigstore for provenance, and supports schema-driven
extensibility for emerging agent modalities (LLM prompt agents, MCP servers,
A2A-enabled components). This paper formalizes the architectural model,
describes storage and discovery layers, explains security and performance
properties, and positions ADS within the broader landscape of emerging agent
registry and interoperability initiatives.

</details>


### [34] [Bounded PCTL Model Checking of Large Language Model Outputs](https://arxiv.org/abs/2509.18836)
*Dennis Gross,Helge Spieker,Arnaud Gotlieb*

Main category: cs.AI

TL;DR: 本文提出LLMCHECKER方法，用于验证大语言模型文本生成过程的PCTL属性，可在多个模型中应用。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏基于PCTL模型检查验证大语言模型文本生成过程一致性的方法，需填补此空白。

Method: 提出α - k有界文本生成，聚焦文本生成每一步top - k标记的α最大累积概率；考虑初始字符串和后续top - k标记，结合不同文本量化方法；用阈值α筛选标记，实现对α - k有界大语言模型PCTL属性的形式化验证。

Result: 所提方法在Llama、Gemma等多个大语言模型中得到应用验证。

Conclusion: 首次将基于PCTL的模型检查用于检查大语言模型文本生成过程的一致性，证明了方法的有效性和适用性。

Abstract: In this paper, we introduce LLMCHECKER, a model-checking-based verification
method to verify the probabilistic computation tree logic (PCTL) properties of
an LLM text generation process. We empirically show that only a limited number
of tokens are typically chosen during text generation, which are not always the
same. This insight drives the creation of $\alpha$-$k$-bounded text generation,
narrowing the focus to the $\alpha$ maximal cumulative probability on the
top-$k$ tokens at every step of the text generation process. Our verification
method considers an initial string and the subsequent top-$k$ tokens while
accommodating diverse text quantification methods, such as evaluating text
quality and biases. The threshold $\alpha$ further reduces the selected tokens,
only choosing those that exceed or meet it in cumulative probability.
LLMCHECKER then allows us to formally verify the PCTL properties of
$\alpha$-$k$-bounded LLMs. We demonstrate the applicability of our method in
several LLMs, including Llama, Gemma, Mistral, Genstruct, and BERT. To our
knowledge, this is the first time PCTL-based model checking has been used to
check the consistency of the LLM text generation process.

</details>


### [35] [Model selection meets clinical semantics: Optimizing ICD-10-CM prediction via LLM-as-Judge evaluation, redundancy-aware sampling, and section-aware fine-tuning](https://arxiv.org/abs/2509.18846)
*Hong-Jie Dai,Zheng-Hao Li,An-Tai Lu,Bo-Tsz Shain,Ming-Ta Li,Tatheer Hussain Mir,Kuang-Te Wang,Min-I Su,Pei-Kang Liu,Ming-Ju Tsai*

Main category: cs.AI

TL;DR: 文章提出模块化框架解决大语言模型在ICD - 10 - CM编码预测中的问题，经实验验证效果良好，提供了实用方案。


<details>
  <summary>Details</summary>
Motivation: ICD编码人工操作劳动强度大且易出错，大语言模型在ICD编码自动化中有挑战，需有效方法解决。

Method: 提出模块化框架，含原则性模型选择、冗余感知数据采样和结构化输入设计；用LLM - as - judge评估协议和Plackett - Luce聚合评估开源大模型；引入冗余感知采样策略；利用台湾医院结构化出院小结评估。

Result: 经两个机构数据集实验，微调后的选定基础模型在内外评估中均优于基线大模型，增加临床部分能提升预测性能。

Conclusion: 使用开源大语言模型建立了实用的ICD - 10 - CM编码预测方法，框架结合多方面要素，为自动化医疗编码系统实际部署提供可扩展、适用于机构的解决方案。

Abstract: Accurate International Classification of Diseases (ICD) coding is critical
for clinical documentation, billing, and healthcare analytics, yet it remains a
labour-intensive and error-prone task. Although large language models (LLMs)
show promise in automating ICD coding, their challenges in base model
selection, input contextualization, and training data redundancy limit their
effectiveness. We propose a modular framework for ICD-10 Clinical Modification
(ICD-10-CM) code prediction that addresses these challenges through principled
model selection, redundancy-aware data sampling, and structured input design.
The framework integrates an LLM-as-judge evaluation protocol with Plackett-Luce
aggregation to assess and rank open-source LLMs based on their intrinsic
comprehension of ICD-10-CM code definitions. We introduced embedding-based
similarity measures, a redundancy-aware sampling strategy to remove
semantically duplicated discharge summaries. We leverage structured discharge
summaries from Taiwanese hospitals to evaluate contextual effects and examine
section-wise content inclusion under universal and section-specific modelling
paradigms. Experiments across two institutional datasets demonstrate that the
selected base model after fine-tuning consistently outperforms baseline LLMs in
internal and external evaluations. Incorporating more clinical sections
consistently improves prediction performance. This study uses open-source LLMs
to establish a practical and principled approach to ICD-10-CM code prediction.
The proposed framework provides a scalable, institution-ready solution for
real-world deployment of automated medical coding systems by combining informed
model selection, efficient data refinement, and context-aware prompting.

</details>


### [36] [MAPO: Mixed Advantage Policy Optimization](https://arxiv.org/abs/2509.18849)
*Wenke Huang,Quan Zhang,Yiyang Fang,Jian Liang,Xuankun Rong,Huanjin Yao,Guancheng Wan,Ke Liang,Wenwen He,Mingjun Li,Leszek Rutkowski,Mang Ye,Bo Du,Dacheng Tao*

Main category: cs.AI

TL;DR: 本文提出Mixed Advantage Policy Optimization (MAPO)策略解决GRPO中优势分配问题，经对比和消融实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO方法存在优势反转和优势镜像问题，阻碍不同查询样本的合理优势分配。

Method: 提出MAPO策略，揭示轨迹有不同确定性，为高确定性轨迹样本提出优势百分比偏差，动态重新加权优势函数。

Result: 与相关先进方法对比以及对不同优势变体的消融研究，验证了该方法的有效性。

Conclusion: 提出的MAPO策略是一种简单有效的GRPO策略，能自适应配置优势函数以考虑样本特定特征。

Abstract: Recent advances in reinforcement learning for foundation models, such as
Group Relative Policy Optimization (GRPO), have significantly improved the
performance of foundation models on reasoning tasks. Notably, the advantage
function serves as a central mechanism in GRPO for ranking the trajectory
importance. However, existing explorations encounter both advantage reversion
and advantage mirror problems, which hinder the reasonable advantage allocation
across different query samples. In this work, we propose an easy but effective
GRPO strategy, Mixed Advantage Policy Optimization (MAPO). We reveal that the
trajectory appears with different certainty and propose the advantage percent
deviation for samples with high-certainty trajectories. Furthermore, we
dynamically reweight the advantage function for samples with varying trajectory
certainty, thereby adaptively configuring the advantage function to account for
sample-specific characteristics. Comparison with related state-of-the-art
methods, along with ablation studies on different advantage variants, validates
the effectiveness of our approach.

</details>


### [37] [Conf-Profile: A Confidence-Driven Reasoning Paradigm for Label-Free User Profiling](https://arxiv.org/abs/2509.18864)
*Yingxin Li,Jianbo Zhao,Xueyu Ren,Jie Tang,Wangjie You,Xu Chen,Kan Zhou,Chao Feng,Jiao Ran,Yuan Meng,Zhi Wang*

Main category: cs.AI

TL;DR: 提出工业基准ProfileBench和置信驱动的用户画像推理框架Conf - Profile，通过两阶段训练提升性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型用于用户画像缺乏综合基准，且画像任务存在收集大规模真实标签困难、用户信息异质嘈杂影响可靠性等问题。

Method: 提出ProfileBench基准；提出Conf - Profile框架，先利用大语言模型合成高质量标签，再进行置信加权投票和校准，将结果聚合到轻量级大语言模型，通过置信引导的无监督强化学习增强推理能力。

Result: 实验表明Conf - Profile通过两阶段训练取得显著性能提升，在Qwen3 - 8B上F1提高13.97。

Conclusion: Conf - Profile能有效解决用户画像任务中的问题，实现无标签且可靠的用户画像。

Abstract: User profiling, as a core technique for user understanding, aims to infer
structural attributes from user information. Large Language Models (LLMs)
provide a promising avenue for user profiling, yet the progress is hindered by
the lack of comprehensive benchmarks. To bridge this gap, we propose
ProfileBench, an industrial benchmark derived from a real-world video platform,
encompassing heterogeneous user data and a well-structured profiling taxonomy.
However, the profiling task remains challenging due to the difficulty of
collecting large-scale ground-truth labels, and the heterogeneous and noisy
user information can compromise the reliability of LLMs. To approach label-free
and reliable user profiling, we propose a Confidence-driven Profile reasoning
framework Conf-Profile, featuring a two-stage paradigm. We first synthesize
high-quality labels by leveraging advanced LLMs with confidence hints, followed
by confidence-weighted voting for accuracy improvement and confidence
calibration for a balanced distribution. The multiple profile results,
rationales, and confidence scores are aggregated and distilled into a
lightweight LLM. We further enhance the reasoning ability via confidence-guided
unsupervised reinforcement learning, which exploits confidence for difficulty
filtering, quasi-ground truth voting, and reward weighting. Experimental
results demonstrate that Conf-Profile delivers substantial performance through
the two-stage training, improving F1 by 13.97 on Qwen3-8B.

</details>


### [38] [Memory in Large Language Models: Mechanisms, Evaluation and Evolution](https://arxiv.org/abs/2509.18868)
*Dianxing Zhang,Wendong Li,Kani Song,Jiaye Lu,Gang Li,Liuchun Yang,Sheng Li*

Main category: cs.AI

TL;DR: 提出LLM记忆的定义、分类、评估框架和更新遗忘机制，给出可测试命题，形成可复现、可比、可治理的研究和部署坐标系。


<details>
  <summary>Details</summary>
Motivation: 为LLM记忆研究提供统一框架，避免不同设置下的比较失真，实现对LLM记忆的有效评估和治理。

Method: 定义LLM记忆，提出四部分分类和记忆四元组，采用三设置协议，构建分层评估，集成治理和审计，提出DMM Gov机制，给出可测试命题。

Result: 构建了涵盖评估、治理、更新遗忘的框架，给出可测试命题。

Conclusion: 形成了可复现、可比、可治理的LLM记忆研究和部署坐标系。

Abstract: Under a unified operational definition, we define LLM memory as a persistent
state written during pretraining, finetuning, or inference that can later be
addressed and that stably influences outputs. We propose a four-part taxonomy
(parametric, contextual, external, procedural/episodic) and a memory quadruple
(location, persistence, write/access path, controllability). We link mechanism,
evaluation, and governance via the chain write -> read -> inhibit/update. To
avoid distorted comparisons across heterogeneous setups, we adopt a
three-setting protocol (parametric only, offline retrieval, online retrieval)
that decouples capability from information availability on the same data and
timeline. On this basis we build a layered evaluation: parametric (closed-book
recall, edit differential, memorization/privacy), contextual (position curves
and the mid-sequence drop), external (answer correctness vs snippet
attribution/faithfulness), and procedural/episodic (cross-session consistency
and timeline replay, E MARS+). The framework integrates temporal governance and
leakage auditing (freshness hits, outdated answers, refusal slices) and
uncertainty reporting via inter-rater agreement plus paired tests with
multiple-comparison correction. For updating and forgetting, we present DMM
Gov: coordinating DAPT/TAPT, PEFT, model editing (ROME, MEND, MEMIT, SERAC),
and RAG to form an auditable loop covering admission thresholds, rollout,
monitoring, rollback, and change audits, with specs for timeliness, conflict
handling, and long-horizon consistency. Finally, we give four testable
propositions: minimum identifiability; a minimal evaluation card; causally
constrained editing with verifiable forgetting; and when retrieval with
small-window replay outperforms ultra-long-context reading. This yields a
reproducible, comparable, and governable coordinate system for research and
deployment.

</details>


### [39] [LongCat-Flash-Thinking Technical Report](https://arxiv.org/abs/2509.18883)
*Meituan LongCat Team,Anchun Gui,Bei Li,Bingyang Tao,Bole Zhou,Borun Chen,Chao Zhang,Chao Zhang,Chengcheng Han,Chenhui Yang,Chi Zhang,Chong Peng,Chuyu Zhang,Cong Chen,Fengcun Li,Gang Xu,Guoyuan Lin,Hao Jiang,Hao Liang,Haomin Fu,Haoxiang Ma,Hong Liu,Hongyan Hao,Hongyin Tang,Hongyu Zang,Hongzhi Ni,Hui Su,Jiahao Liu,Jiahuan Li,Jialin Liu,Jianfei Zhang,Jianhao Xu,Jianing Wang,Jiaqi Sun,Jiaqi Zhang,Jiarong Shi,Jiawei Yang,Jingang Wang,Jinrui Ding,Jun Kuang,Jun Xu,Ke He,Kefeng Zhang,Keheng Wang,Keqing He,Li Wei,Liang Shi,Lin Qiu,Lingbin Kong,Lingchuan Liu,Linsen Guo,Longfei An,Mai Xia,Meng Zhou,Mengshen Zhu,Peng Pei,Pengcheng Jia,Qi Gu,Qi Guo,Qiong Huang,Quan Chen,Quanchi Weng,Rongxiang Weng,Ruichen Shao,Rumei Li,Shanglin Lei,Shuai Du,Shuaikang Liu,Shuang Zhou,Shuhao Hu,Siyu Xu,Songshan Gong,Tao Liang,Tianhao Hu,Wei He,Wei Shi,Wei Wang,Wei Wu,Wei Zhuo,Weifeng Tang,Wenjie Shi,Wenlong Zhu,Xi Su,Xiangcheng Liu,Xiangyu Xi,Xiangzhou Huang,Xiao Liu,Xiaochen Jiang,Xiaowei Shi,Xiaowen Shi,Xiaoyu Li,Xin Chen,Xinyue Zhao,Xuan Huang,Xuemiao Zhang,Xuezhi Cao,Xunliang Cai,Yajie Zhang,Yang Chen,Yang Liu,Yang Liu,Yang Zheng,Yaoming Wang,Yaqi Huo,Yerui Sun,Yifan Lu,Yiyang Li,Youshao Xiao,Yuanzhe Lei,Yuchen Xie,Yueqing Sun,Yufei Zhang,Yuhuai Wei,Yulei Qian,Yunke Zhao,Yuqing Ding,Yuwei Jiang,Zhaohua Yang,Zhengyu Chen,Zhijian Liu,Zhikang Xia,Zhongda Su,Ziran Li,Ziwen Wang,Ziyuan Zhuang,Zongyu Wang,Zunyuan Yang*

Main category: cs.AI

TL;DR: 提出开源推理模型LongCat - Flash - Thinking，经特定训练流程，采用新训练方案和框架，在推理任务表现佳且高效，已发布以推动研究。


<details>
  <summary>Details</summary>
Motivation: 推动推理系统和智能体AI研究发展，开发高效推理模型。

Method: 先使用长思维链数据冷启动训练提升推理潜力，采用领域并行训练方案解耦不同领域优化后融合专家模型，用DORA系统加速训练。

Result: 在复杂推理任务上达开源模型最优水平，智能体推理效率高，AIME - 25上平均token消耗降低64.5%且不影响精度。

Conclusion: 发布LongCat - Flash - Thinking以促进推理系统和智能体AI研究。

Abstract: We present LongCat-Flash-Thinking, an efficient 560-billion-parameter
open-source Mixture-of-Experts (MoE) reasoning model. Its advanced capabilities
are cultivated through a meticulously crafted training process, beginning with
long Chain-of-Thought (CoT) data cold-start and culminating in large-scale
Reinforcement Learning (RL). We first employ a well-designed cold-start
training strategy, which significantly enhances the reasoning potential and
equips the model with specialized skills in both formal and agentic reasoning.
Then, a core innovation is our domain-parallel training scheme, which decouples
optimization across distinct domains (e.g., STEM, Code, Agentic) and
subsequently fuses the resulting expert models into a single, nearly
Pareto-optimal model. This entire process is powered by our Dynamic
ORchestration for Asynchronous rollout (DORA) system, a large-scale RL
framework that delivers a greater than threefold training speedup over
synchronous methods on tens of thousands of accelerators. As a result,
LongCat-Flash-Thinking achieves state-of-the-art performance among open-source
models on a suite of complex reasoning tasks. The model exhibits exceptional
efficiency in agentic reasoning, reducing average token consumption by 64.5%
(from 19, 653 to 6, 965) on AIME-25, without degrading task accuracy. We
release LongCat-Flash-Thinking to promote further advances in reasoning systems
and agentic AI research.

</details>


### [40] [How Far are VLMs from Visual Spatial Intelligence? A Benchmark-Driven Perspective](https://arxiv.org/abs/2509.18905)
*Songsong Yu,Yuxin Chen,Hao Ju,Lianjie Jia,Fuxi Zhang,Shaofei Huang,Yuhan Wu,Rundi Cui,Binghao Ran,Zaibin Zhang,Zhedong Zheng,Zhipeng Zhang,Yifan Wang,Lin Song,Lijun Wang,Yanwei Li,Ying Shan,Huchuan Lu*

Main category: cs.AI

TL;DR: 文章对视觉空间推理（VSR）进行系统研究，提出能力分类，构建SIBench基准，实验揭示感知与推理差距，为领域研究提供指引。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型（VLMs）有进展，但实现人类水平的VSR因三维空间表征和推理复杂仍具挑战。

Method: 对VLMs中的VSR进行系统研究，涵盖输入模态、模型架构等；将空间智能分为三个能力级别；构建SIBench基准。

Result: 实验表明，模型在基础感知任务表现尚可，但在理解和规划任务，如数值估计、多视图推理等方面表现不佳。

Conclusion: 实现空间智能仍面临重大挑战，研究为该领域未来研究提供系统路线图和全面基准。

Abstract: Visual Spatial Reasoning (VSR) is a core human cognitive ability and a
critical requirement for advancing embodied intelligence and autonomous
systems. Despite recent progress in Vision-Language Models (VLMs), achieving
human-level VSR remains highly challenging due to the complexity of
representing and reasoning over three-dimensional space. In this paper, we
present a systematic investigation of VSR in VLMs, encompassing a review of
existing methodologies across input modalities, model architectures, training
strategies, and reasoning mechanisms. Furthermore, we categorize spatial
intelligence into three levels of capability, ie, basic perception, spatial
understanding, spatial planning, and curate SIBench, a spatial intelligence
benchmark encompassing nearly 20 open-source datasets across 23 task settings.
Experiments with state-of-the-art VLMs reveal a pronounced gap between
perception and reasoning, as models show competence in basic perceptual tasks
but consistently underperform in understanding and planning tasks, particularly
in numerical estimation, multi-view reasoning, temporal dynamics, and spatial
imagination. These findings underscore the substantial challenges that remain
in achieving spatial intelligence, while providing both a systematic roadmap
and a comprehensive benchmark to drive future research in the field. The
related resources of this study are accessible at
https://sibench.github.io/Awesome-Visual-Spatial-Reasoning/.

</details>


### [41] [Data Efficient Adaptation in Large Language Models via Continuous Low-Rank Fine-Tuning](https://arxiv.org/abs/2509.18942)
*Xiao Han,Zimo Zhao,Wanyu Wang,Maolin Wang,Zitao Liu,Yi Chang,Xiangyu Zhao*

Main category: cs.AI

TL;DR: 论文提出 DEAL 框架解决传统微调方法问题，实验显示其在多方面优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统微调方法存在灾难性遗忘和数据效率低的问题，限制了实际应用，需改进。

Method: 提出 DEAL 框架，集成 LoRA 与连续微调策略，包含知识保留和自适应参数更新模块。

Result: 在 15 个不同数据集上实验，DEAL 始终优于基线方法，在任务准确性和资源效率上有显著提升。

Conclusion: 该方法能通过提升任务性能和资源效率，推动大语言模型的持续适应。

Abstract: Recent advancements in Large Language Models (LLMs) have emphasized the
critical role of fine-tuning (FT) techniques in adapting LLMs to specific
tasks, especially when retraining from scratch is computationally infeasible.
Fine-tuning enables LLMs to leverage task- or domain-specific data, producing
models that more effectively meet the requirements of targeted applications.
However, con- ventional FT approaches often suffer from catastrophic forgetting
and suboptimal data efficiency, limiting their real-world applicability. To
address these challenges, this paper proposes DEAL, a novel framework that
integrates Low-Rank Adapta- tion (LoRA) with a continuous fine-tuning strategy.
By incorporating knowledge retention and adaptive parameter update modules, the
framework mitigates the lim- itations of existing FT methods while maintaining
efficiency in privacy-preserving settings. Experiments on 15 diverse datasets
show that DEAL consistently outper- forms baseline methods, yielding
substantial gains in task accuracy and resource efficiency. These findings
demonstrate the potential of our approach to advance continual adaptation in
LLMs by enhancing task performance while improving resource efficiency.

</details>


### [42] [LLM-based Agents Suffer from Hallucinations: A Survey of Taxonomy, Methods, and Directions](https://arxiv.org/abs/2509.18970)
*Xixun Lin,Yucheng Ning,Jingwen Zhang,Yan Dong,Yilong Liu,Yongxuan Wu,Xiaohua Qi,Nan Sun,Yanmin Shang,Pengfei Cao,Lixin Zou,Xu Chen,Chuan Zhou,Jia Wu,Shirui Pan,Bin Wang,Yanan Cao,Kai Chen,Songlin Hu,Li Guo*

Main category: cs.AI

TL;DR: 本文对基于大语言模型的智能体幻觉问题进行全面调研，提出新分类法，分析原因，总结缓解和检测方法并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的智能体在实际应用中易出现幻觉问题，影响系统可靠性，需深入理解和系统整合相关进展。

Method: 分析智能体完整工作流程，提出新分类法；深入研究18个引发幻觉的原因；详细回顾大量现有研究。

Result: 提出新的智能体幻觉分类法，分析出18个引发原因，总结了幻觉缓解和检测方法。

Conclusion: 希望该调研能推动解决基于大语言模型智能体的幻觉问题，促进更强大可靠的智能体系统发展。

Abstract: Driven by the rapid advancements of Large Language Models (LLMs), LLM-based
agents have emerged as powerful intelligent systems capable of human-like
cognition, reasoning, and interaction. These agents are increasingly being
deployed across diverse real-world applications, including student education,
scientific research, and financial analysis. However, despite their remarkable
potential, LLM-based agents remain vulnerable to hallucination issues, which
can result in erroneous task execution and undermine the reliability of the
overall system design. Addressing this critical challenge requires a deep
understanding and a systematic consolidation of recent advances on LLM-based
agents. To this end, we present the first comprehensive survey of
hallucinations in LLM-based agents. By carefully analyzing the complete
workflow of agents, we propose a new taxonomy that identifies different types
of agent hallucinations occurring at different stages. Furthermore, we conduct
an in-depth examination of eighteen triggering causes underlying the emergence
of agent hallucinations. Through a detailed review of a large number of
existing studies, we summarize approaches for hallucination mitigation and
detection, and highlight promising directions for future research. We hope this
survey will inspire further efforts toward addressing hallucinations in
LLM-based agents, ultimately contributing to the development of more robust and
reliable agent systems.

</details>


### [43] [Remaining Time Prediction in Outbound Warehouse Processes: A Case Study (Short Paper)](https://arxiv.org/abs/2509.18986)
*Erik Penther,Michael Grohs,Jana-Rebecca Rehse*

Main category: cs.AI

TL;DR: 本文在航空物流企业的出库仓库流程中比较四种剩余时间预测方法，发现深度学习模型精度最高，但浅层方法精度有竞争力且计算资源需求少。


<details>
  <summary>Details</summary>
Motivation: 预测性流程监控旨在预测正在进行的流程执行的未来，常见预测目标是剩余时间，本文旨在比较四种不同的剩余时间预测方法。

Method: 在航空物流企业的实际出库仓库流程中，使用该企业提供的包含169,523条轨迹的新颖原始事件日志，比较四种剩余时间预测方法。

Result: 深度学习模型达到最高精度，传统提升技术等浅层方法达到有竞争力的精度且所需计算资源显著减少。

Conclusion: 深度学习模型在精度上有优势，但浅层方法在精度和计算资源需求之间有较好平衡。

Abstract: Predictive process monitoring is a sub-domain of process mining which aims to
forecast the future of ongoing process executions. One common prediction target
is the remaining time, meaning the time that will elapse until a process
execution is completed. In this paper, we compare four different remaining time
prediction approaches in a real-life outbound warehouse process of a logistics
company in the aviation business. For this process, the company provided us
with a novel and original event log with 169,523 traces, which we can make
publicly available. Unsurprisingly, we find that deep learning models achieve
the highest accuracy, but shallow methods like conventional boosting techniques
achieve competitive accuracy and require significantly fewer computational
resources.

</details>


### [44] [Landmarks, Monuments, and Beacons: Understanding Generative Calls to Action](https://arxiv.org/abs/2509.19030)
*Victoire Hervé,Henrik Warpefelt,Christoph Salge*

Main category: cs.AI

TL;DR: 本文引入Landmarks、Monuments和Beacons概念，用于解决程序生成内容算法评估难题，可连接人文与技术游戏研究。


<details>
  <summary>Details</summary>
Motivation: 程序生成内容的算法评估难以找到与人类体验相符的指标，自动分解需要满足一定属性的概念。

Method: 借鉴游戏研究和游戏AI研究，引入基于玩家视角的Landmarks、Monuments和Beacons概念。

Result: 这些概念通用且可使用现有技术查找和评估，为PCG自动分解和子组件评估开辟道路。

Conclusion: 该方法可连接人文与技术游戏研究，实现更好的计算PCG评估，且应用范围不止于混合主动PCG和组合PCG。

Abstract: Algorithmic evaluation of procedurally generated content struggles to find
metrics that align with human experience, particularly for composite artefacts.
Automatic decomposition as a possible solution requires concepts that meet a
range of properties. To this end, drawing on Games Studies and Game AI
research, we introduce the nested concepts of \textit{Landmarks},
\textit{Monuments}, and \textit{Beacons}. These concepts are based on the
artefact's perceivability, evocativeness, and Call to Action, all from a
player-centric perspective. These terms are generic to games and usable across
genres. We argue that these entities can be found and evaluated with techniques
currently used in both research and industry, opening a path towards a fully
automated decomposition of PCG, and evaluation of the salient sub-components.
Although the work presented here emphasises mixed-initiative PCG and
compositional PCG, we believe it applies beyond those domains. With this
approach, we intend to create a connection between humanities and technical
game research and allow for better computational PCG evaluation

</details>


### [45] [Towards Causal Representation Learning with Observable Sources as Auxiliaries](https://arxiv.org/abs/2509.19058)
*Kwonho Kim,Heejeong Nam,Inwoo Hwang,Sanghack Lee*

Main category: cs.AI

TL;DR: 本文引入以可观测源作为辅助变量的框架，利用体积保持编码器识别潜在变量，提供变量选择方案，并通过实验验证框架有效性。


<details>
  <summary>Details</summary>
Motivation: 现有因果表示学习框架将辅助变量范围限制在混合函数外部，而某些系统驱动的潜在因素可从数据中观测或提取，可利用其促进识别。

Method: 引入可观测源作为辅助变量的框架，使用体积保持编码器识别潜在变量，在有多个已知辅助变量时提供变量选择方案。

Result: 可识别整个潜在变量直至子空间变换和排列，在合成图和图像数据实验中验证框架有效性。

Conclusion: 扩展了当前因果表示学习方法的边界。

Abstract: Causal representation learning seeks to recover latent factors that generate
observational data through a mixing function. Needing assumptions on latent
structures or relationships to achieve identifiability in general, prior works
often build upon conditional independence given known auxiliary variables.
However, prior frameworks limit the scope of auxiliary variables to be external
to the mixing function. Yet, in some cases, system-driving latent factors can
be easily observed or extracted from data, possibly facilitating
identification. In this paper, we introduce a framework of observable sources
being auxiliaries, serving as effective conditioning variables. Our main
results show that one can identify entire latent variables up to subspace-wise
transformations and permutations using volume-preserving encoders. Moreover,
when multiple known auxiliary variables are available, we offer a
variable-selection scheme to choose those that maximize recoverability of the
latent factors given knowledge of the latent causal graph. Finally, we
demonstrate the effectiveness of our framework through experiments on synthetic
graph and image data, thereby extending the boundaries of current approaches.

</details>


### [46] [Code Driven Planning with Domain-Adaptive Critic](https://arxiv.org/abs/2509.19077)
*Zikang Tian,Shaohui Peng,Du Huang,Jiaming Guo,Ruizhi Chen,Rui Zhang,Xishan Zhang,Yuxuan Guo,Zidong Du,Qi Guo,Ling Li,Yewen Pu,Xing Hu,Yunji Chen*

Main category: cs.AI

TL;DR: 现有基于大语言模型（LLMs）的规划方法有成本高、难考虑长期奖励的问题，提出CoPiC方法，能提升规划效果并降低查询成本，在多个任务中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLMs的规划方法存在一般知识与特定环境要求有差距、查询成本高、难考虑长期奖励的问题，需要改进。

Method: 提出CoPiC方法，用LLMs生成高级规划程序迭代产生和细化候选计划，用训练的领域自适应评判器评估并选择最符合长期奖励的计划执行。

Result: 在ALFWorld、NetHack和星际争霸II单位建设中，CoPiC成功率平均提升23.33%，查询成本降低91.27%，优于AdaPlanner和Reflexion基线。

Conclusion: CoPiC方法能有效提升规划效果，显著降低查询成本。

Abstract: Large Language Models (LLMs) have been widely adopted as task planners for AI
agents in sequential decision-making problems, leveraging their extensive world
knowledge. However, the gap between their general knowledge and
environment-specific requirements often leads to inaccurate plans. To address
this, existing approaches rely on frequent LLM queries to iteratively refine
plans based on immediate environmental feedback, which incurs substantial query
costs. However, this refinement is typically guided by short-term environmental
feedback, limiting LLMs from developing plans aligned with long-term rewards.
We propose Code Driven Planning with Domain-Adaptive Critic (CoPiC). Instead of
relying on frequent queries, CoPiC employs LLMs to generate a diverse set of
high-level planning programs, which iteratively produce and refine candidate
plans. A trained domain-adaptive critic then evaluates these candidates and
selects the one most aligned with long-term rewards for execution. Using
high-level planning programs as planner and domain-adaptive critic as
estimator, CoPiC improves planning while significantly reducing query costs.
Results in ALFWorld, NetHack, and StarCraft II Unit Building show that CoPiC
outperforms advanced LLM-based baselines, AdaPlanner and Reflexion, achieving
an average (1) 23.33% improvement in success rate and (2) 91.27% reduction in
query costs.

</details>


### [47] [AgentInit: Initializing LLM-based Multi-Agent Systems via Diversity and Expertise Orchestration for Effective and Efficient Collaboration](https://arxiv.org/abs/2509.19236)
*Chunhao Tian,Yutong Wang,Xuebo Liu,Zhexuan Wang,Liang Ding,Miao Zhang,Min Zhang*

Main category: cs.AI

TL;DR: 提出AgentInit优化多智能体系统（MAS）初始化，实验表明其性能优于现有方法，有强可迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有MAS初始化方法未充分考虑生成智能体后续协作需求，需优化智能体团队结构。

Method: 提出AgentInit，包含智能体多轮交互与反思、自然语言转格式机制，应用帕累托原则的均衡团队选择策略。

Result: AgentInit在各框架和任务中均优于现有方法和预定义策略，性能提升达1.2和1.6，减少令牌消耗。

Conclusion: AgentInit是可靠的MAS初始化方法，有强可迁移性和适应性。

Abstract: Proper initialization is crucial for any system, particularly in multi-agent
systems (MAS), where it plays a pivotal role in determining both the system's
efficiency and effectiveness. However, existing MAS initialization methods do
not fully account for the collaborative needs of the generated agents in
subsequent stages. Inspired by the principles of effective team composition, we
propose AgentInit, which aims to optimize the structure of agent teams.
Specifically, in addition to multi-round interactions and reflections between
agents during agent generation, AgentInit incorporates a Natural Language to
Format mechanism to ensure consistency and standardization. Balanced team
selection strategies using Pareto principles are subsequently applied to
jointly consider agent team diversity and task relevance to promote effective
and efficient collaboration and enhance overall system performance. Experiments
show that AgentInit consistently outperforms state-of-the-art initialization
methods and pre-defined strategies across various frameworks and tasks,
achieving an overall performance improvement of up to 1.2 and 1.6,
respectively, while also significantly reducing token consumption. Further
analysis confirms its strong transferability to similar tasks and verifies the
effectiveness of its key components, demonstrating its capability and
adaptability as a reliable MAS initialization method. Source code and models
are available at https://github.com/1737423697/AgentInit.

</details>


### [48] [Cross-Cultural Transfer of Commonsense Reasoning in LLMs: Evidence from the Arab World](https://arxiv.org/abs/2509.19265)
*Saeed Almheiri,Rania Hossam,Mena Attia,Chenxi Wang,Preslav Nakov,Timothy Baldwin,Fajri Koto*

Main category: cs.AI

TL;DR: 论文研究大语言模型跨文化常识推理迁移，用阿拉伯国家数据集评估方法，发现少量特定文化示例可提升性能，跨文化示范也有效，证明高效跨文化对齐可行。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在西方中心偏见，跨文化迁移潜力待挖掘，研究阿拉伯世界常识推理的跨文化迁移。

Method: 使用覆盖13个阿拉伯国家的常识推理数据集，评估上下文学习、基于演示的强化学习等轻量级对齐方法，以及监督微调、直接偏好优化等基线方法。

Result: 仅12个特定国家文化示例可使多语言模型在其他国家平均性能提升10%；印尼和美国的跨文化示范在MCQ推理中可媲美或超越本土对齐。

Conclusion: 高效的跨文化对齐是可行的，为使大语言模型适应低资源文化环境提供了有前景的方法。

Abstract: Large language models (LLMs) often reflect Western-centric biases, limiting
their effectiveness in diverse cultural contexts. Although some work has
explored cultural alignment, the potential for cross-cultural transfer, using
alignment in one culture to improve performance in others, remains
underexplored. This paper investigates cross-cultural transfer of commonsense
reasoning in the Arab world, where linguistic and historical similarities
coexist with local cultural differences. Using a culturally grounded
commonsense reasoning dataset covering 13 Arab countries, we evaluate
lightweight alignment methods such as in-context learning and
demonstration-based reinforcement (DITTO), alongside baselines like supervised
fine-tuning and direct preference optimization. Our results show that merely 12
culture-specific examples from one country can improve performance in others by
10\% on average, within multilingual models. In addition, we demonstrate that
out-of-culture demonstrations from Indonesia and US contexts can match or
surpass in-culture alignment for MCQ reasoning, highlighting cultural
commonsense transferability beyond the Arab world. These findings demonstrate
that efficient cross-cultural alignment is possible and offer a promising
approach to adapt LLMs to low-resource cultural settings.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [49] [2D implementation of Kinetic-diffusion Monte Carlo in Eiron](https://arxiv.org/abs/2509.19140)
*Oskar Lappi,Emil Løvbak,Thijs Steel,Giovanni Samaey*

Main category: cs.CE

TL;DR: 本文将Kinetic - diffusion Monte Carlo方案扩展到二维设置并在Eiron粒子代码中实现，在高碰撞情况下比动力学模拟有显著加速。


<details>
  <summary>Details</summary>
Motivation: 基于粒子的中性粒子动力学蒙特卡罗模拟是托卡马克刮削层模拟的主要计算瓶颈，需解决高碰撞区域计算成本高的问题。

Method: 将Kinetic - diffusion Monte Carlo方案扩展到二维设置，并在Eiron粒子代码中实现。

Result: 在高碰撞情况下，该实现比动力学模拟有显著的加速。

Conclusion: Kinetic - diffusion Monte Carlo方案扩展到二维设置的实现能有效降低高碰撞区域的计算成本。

Abstract: Particle-based kinetic Monte Carlo simulations of neutral particles is one of
the major computational bottlenecks in tokamak scrape-off layer simulations.
This computational cost comes from the need to resolve individual collision
events in high-collisional regimes. However, in such regimes, one can
approximate the high-collisional kinetic dynamics with computationally cheaper
diffusion. Asymptotic-preserving schemes make use of this limit to perform
simulations in these regimes, without a blow-up in computational cost as
incurred by standard kinetic approaches. One such scheme is Kinetic-diffusion
Monte Carlo. In this paper, we present a first extension of this scheme to the
two-dimensional setting and its implementation in the Eiron particle code. We
then demonstrate that this implementation produces a significant speedup over
kinetic simulations in high-collisional cases.

</details>


### [50] [AlloyInter: Visualising Alloy Mixture Interpolations in t-SNE Representations](https://arxiv.org/abs/2509.19202)
*Benedikt Kantz,Peter Waldert,Stefan Lengauer,Tobias Schreck*

Main category: cs.CE

TL;DR: 提出AlloyInter系统，可在SciVis Contest 2025背景下联合探索输入混合物和输出参数空间。


<details>
  <summary>Details</summary>
Motivation: 在SciVis Contest 2025背景下，实现对输入混合物和输出参数空间的联合探索。

Method: 提出基于学习模型集成的可解释人工智能（XAI）引导的插值方法，结合XAI鲁棒性的先前研究以及流形学习等成熟技术。

Result: 未提及

Conclusion: 未提及

Abstract: This entry description proposes AlloyInter, a novel system to enable joint
exploration of input mixtures and output parameters space in the context of the
SciVis Contest 2025. We propose an interpolation approach, guided by
eXplainable Artificial Intelligence (XAI) based on a learned model ensemble
that allows users to discover input mixture ratios by specifying output
parameter goals that can be iteratively adjusted and improved towards a goal.
We strengthen the capabilities of our system by building upon prior research
within the robustness of XAI, as well as combining well-established techniques
like manifold learning with interpolation approaches.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [51] [ExtGraph: A Fast Extraction Method of User-intended Graphs from a Relational Database](https://arxiv.org/abs/2509.18534)
*Jeongho Park,Geonho Lee,Min-Soo Kim*

Main category: cs.DB

TL;DR: 提出ExtGraph方法从关系数据库高效提取用户期望图，实验显示其在提取时间上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多数公司重要数据存于RDBMS，现有方法难以提取用户期望图，需复杂连接查询处理。

Method: 提出ExtGraph方法，通过外连接和物化视图的混合查询处理来高效提取图。

Result: 使用TPC - DS、DBLP和IMDB数据集实验表明，ExtGraph在图提取时间上比现有方法最多快2.78倍。

Conclusion: ExtGraph能高效从关系数据库提取用户期望图，在图提取时间上表现更优。

Abstract: Graph analytics is widely used in many fields to analyze various complex
patterns. However, in most cases, important data in companies is stored in
RDBMS's, and so, it is necessary to extract graphs from relational databases to
perform graph analysis. Most of the existing methods do not extract a
user-intended graph since it typically requires complex join query processing.
We propose an efficient graph extraction method, \textit{ExtGraph}, which can
extract user-intended graphs efficiently by hybrid query processing of outer
join and materialized view. Through experiments using the TPC-DS, DBLP, and
IMDB datasets, we have shown that \textit{ExtGraph} outperforms the
state-of-the-art methods up to by 2.78x in terms of graph extraction time.

</details>


### [52] [CALL: Context-Aware Low-Latency Retrieval in Disk-Based Vector Databases](https://arxiv.org/abs/2509.18670)
*Yeonwoo Jeong,Hyunji Cho,Kyuri Park,Youngjae Kim,Sungyong Park*

Main category: cs.DB

TL;DR: 提出CALL机制减少向量数据库缓存缺失惩罚和搜索延迟


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略查询集群访问模式，增加缓存缺失惩罚

Method: 提出CALL，基于共享集群访问模式组织查询，结合组感知预取和延迟感知集群加载

Result: CALL减少99%尾延迟达33%，保持高缓存命中率，降低搜索延迟

Conclusion: CALL机制有效减少缓存缺失惩罚和搜索延迟

Abstract: Embedding models capture both semantic and syntactic structures of queries,
often mapping different queries to similar regions in vector space. This
results in non-uniform cluster access patterns in modern disk-based vector
databases. While existing approaches optimize individual queries, they overlook
the impact of cluster access patterns, failing to account for the locality
effects of queries that access similar clusters. This oversight increases cache
miss penalty. To minimize the cache miss penalty, we propose CALL, a
context-aware query grouping mechanism that organizes queries based on shared
cluster access patterns. Additionally, CALL incorporates a group-aware
prefetching method to minimize cache misses during transitions between query
groups and latency-aware cluster loading. Experimental results show that CALL
reduces the 99th percentile tail latency by up to 33% while consistently
maintaining a higher cache hit ratio, substantially reducing search latency.

</details>


### [53] [Teaching RDM in a smart advanced inorganic lab course and its provision in the DALIA platform](https://arxiv.org/abs/2509.18902)
*Alexander Hoffmann,Jochen Ortmeyer,Fabian Fink,Charles Tapley Hoyt,Jonathan D. Geiger,Paul Kehrein,Torsten Schrade,Sonja Herres-Pawlis*

Main category: cs.DB

TL;DR: 介绍亚琛工业大学通过实验课程和工具培养化学专业学生研究数据管理能力。


<details>
  <summary>Details</summary>
Motivation: 研究数据管理是化学专业学生应掌握的关键数据素养技能，传统研究数据存储格式不便复用。

Method: 在第五学期实验课程中引入电子实验室笔记本Chemotion，学生数字化记录实验，还提供研讨会和在线培训视频，并使用DALIA平台作为发现工具。

Result: Chemotion直观界面和仓库可实现可持续数据共享。

Conclusion: 强调利用相关工具和课程培养学生研究数据管理能力的做法。

Abstract: Research data management (RDM) is a key data literacy skill that chemistry
students must acquire. Concepts such as the FAIR data principles (Findable,
Accessible, Interoperable, Reusable) should be taught and applied in
undergraduate studies already. Traditionally, research data from labs, theses,
and internships were handwritten and stored in inaccessible formats such as
PDFs, limiting reuse and machine learning applications. At RWTH Aachen
University, a fifth-semester lab course introduces students to the electronic
laboratory notebook (ELN) Chemotion, an open-source DFG-funded tool linked to
the national NFDI4Chem initiative. Students plan, document, and evaluate
experiments digitally, ensuring metadata and analysis are captured for
long-term reuse. Chemotion's intuitive interface and repository enable
sustainable data sharing. To reinforce RDM, students receive a seminar and
access to online training videos with interactive Moodle elements. Herein we
highlight the use of the DALIA platform as a discovery tool for the students.

</details>


### [54] [A decentralized future for the open-science databases](https://arxiv.org/abs/2509.19206)
*Gaurav Sharma,Viorel Munteanu,Nika Mansouri Ghiasi,Jineta Banerjee,Susheel Varma,Luca Foschini,Kyle Ellrott,Onur Mutlu,Dumitru Ciorbă,Roel A. Ophoff,Viorel Bostan,Christopher E Mason,Jason H. Moore,Despoina Sousoni,Arunkumar Krishnan,Christopher E. Mason,Mihai Dimian,Gustavo Stolovitzky,Fabio G. Liberante,Taras K. Oleksyk,Serghei Mangul*

Main category: cs.DB

TL;DR: 集中式生物数据仓库有单点故障风险，需重新评估其可持续性，本文探讨相关局限，评估分布式模型并提出混合框架，以保障科学数据。


<details>
  <summary>Details</summary>
Motivation: 集中式生物数据仓库易受多种因素影响，导致数据问题和科研受阻，且数据生成加速和全球环境不稳定，需重新评估其可持续性。

Method: 分析集中式仓库的结构局限，评估联邦和去中心化模型，提出混合框架。

Result: 提出的混合框架可减少治理、基础设施和资金方面的风险，促进公平和全球可访问性。

Conclusion: 未来开放科学需整合相关方法，建立全球分布式、经济可持续和制度健全的基础设施保障科学数据。

Abstract: Continuous and reliable access to curated biological data repositories is
indispensable for accelerating rigorous scientific inquiry and fostering
reproducible research. Centralized repositories, though widely used, are
vulnerable to single points of failure arising from cyberattacks, technical
faults, natural disasters, or funding and political uncertainties. This can
lead to widespread data unavailability, data loss, integrity compromises, and
substantial delays in critical research, ultimately impeding scientific
progress. Centralizing essential scientific resources in a single geopolitical
or institutional hub is inherently dangerous, as any disruption can paralyze
diverse ongoing research. The rapid acceleration of data generation, combined
with an increasingly volatile global landscape, necessitates a critical
re-evaluation of the sustainability of centralized models. Implementing
federated and decentralized architectures presents a compelling and
future-oriented pathway to substantially strengthen the resilience of
scientific data infrastructures, thereby mitigating vulnerabilities and
ensuring the long-term integrity of data. Here, we examine the structural
limitations of centralized repositories, evaluate federated and decentralized
models, and propose a hybrid framework for resilient, FAIR, and sustainable
scientific data stewardship. Such an approach offers a significant reduction in
exposure to governance instability, infrastructural fragility, and funding
volatility, and also fosters fairness and global accessibility. The future of
open science depends on integrating these complementary approaches to establish
a globally distributed, economically sustainable, and institutionally robust
infrastructure that safeguards scientific data as a public good, further
ensuring continued accessibility, interoperability, and preservation for
generations to come.

</details>


### [55] [Gate-Based and Annealing-Based Quantum Algorithms for the Maximum K-Plex Problem](https://arxiv.org/abs/2509.19214)
*Xiaofan Li,Gao Cong,Rui Zhou*

Main category: cs.DB

TL;DR: 本文用两种量子模型研究最大k - plex问题，提出门基算法qTKP和qMKP及退火基近似算法qaMKP，实验验证性能，有广泛应用潜力。


<details>
  <summary>Details</summary>
Motivation: 最大k - plex问题在多领域受关注，现有算法时间复杂度有待优化，需新方法。

Method: 使用门基模型和退火基模型，提出qTKP、qMKP和qaMKP算法，进行原理验证实验。

Result: qTKP和qMKP实现O^*(1.42^n)时间复杂度，qaMKP更高效利用量子比特资源。

Conclusion: 该工作有潜力应用于多种团松弛问题。

Abstract: The $ k $-plex model, which allows each vertex to miss connections with up to
$ k $ neighbors, serves as a relaxation of the clique. Its adaptability makes
it more suitable for analyzing real-world graphs where noise and imperfect data
are common and the ideal clique model is often impractical. The problem of
identifying the maximum $ k $-plex (MKP, which is NP-hard) is gaining attention
in fields such as social network analysis, community detection, terrorist
network identification, and graph clustering. Recent works have focused on
optimizing the time complexity of MKP algorithms. The state-of-the-art has
reduced the complexity from a trivial $ O^*(2^n) $ to $ O^*(c_k^n) $, with $
c_k > 1.94 $ for $ k \geq 3 $, where $ n $ denotes the vertex number. This
paper investigates the MKP using two quantum models: gate-based model and
annealing-based model. Two gate-based algorithms, qTKP and qMKP, are proposed
to achieve $ O^*(1.42^n) $ time complexity. qTKP integrates quantum search with
graph encoding, degree counting, degree comparison, and size determination to
find a $ k $-plex of a given size; qMKP uses binary search to progressively
identify the maximum solution. Furthermore, by reformulating MKP as a quadratic
unconstrained binary optimization problem, we propose qaMKP, the first
annealing-based approximation algorithm, which utilizes qubit resources more
efficiently than gate-based algorithms. To validate the practical performance,
proof-of-principle experiments were conducted using the latest IBM gate-based
quantum simulator and D-Wave adiabatic quantum computer. This work holds
potential to be applied to a wide range of clique relaxations, e.g., $ n $-clan
and $ n $-club.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [56] [Bridging Simulation and Silicon: A Study of RISC-V Hardware and FireSim Simulation](https://arxiv.org/abs/2509.18472)
*Atanu Barai,Kamalavasan Kamalakkannan,Patrick Diehl,Maxim Moraru,Jered Dominguez-Trujillo,Howard Pritchard,Nandakishore Santhi,Farzad Fatollahi-Fard,Galen Shipman*

Main category: cs.DC

TL;DR: 本文围绕基于RISC - V的FireSim框架，建模商用单板机和桌面级CPU评估其与物理硬件的性能差异，发现模拟与实测运行时间有偏差。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对FireSim框架与物理硬件相比的可行性和性能预测准确性的系统评估，需开展相关研究。

Method: 在FireSim中对商用单板机和桌面级RISC - V CPU建模，通过一系列基准测试比较单核和四核配置下的运行时行为，用代表性小型应用和LAMMPS分子动力学代码评估性能。

Result: FireSim能提供架构性能趋势见解，但模拟和实测运行时间存在差异，偏差源于模拟环境限制和CPU制造商详细性能规格有限。

Conclusion: FireSim虽有价值，但在模拟和物理硬件性能匹配上存在问题，受模拟环境和硬件信息限制。

Abstract: RISC-V ISA-based processors have recently emerged as both powerful and
energy-efficient computing platforms. The release of the MILK-V Pioneer marked
a significant milestone as the first desktop-grade RISC-V system. With
increasing engagement from both academia and industry, such platforms exhibit
strong potential for adoption in high-performance computing (HPC) environments.
  The open-source, FPGA-accelerated FireSim framework has emerged as a flexible
and scalable tool for architectural exploration, enabling simulation of various
system configurations using RISC-V cores. Despite its capabilities, there
remains a lack of systematic evaluation regarding the feasibility and
performance prediction accuracy of FireSim when compared to physical hardware.
  In this study, we address this gap by modeling a commercially available
single-board computer and a desktop-grade RISC-V CPU within FireSim. To ensure
fidelity between simulation and real hardware, we first measure the performance
of a series of benchmarks to compare runtime behavior under single-core and
four-core configurations. Based on the closest matching simulation parameters,
we subsequently evaluate performance using a representative mini-application
and the LAMMPS molecular dynamics code.
  Our findings indicate that while FireSim provides valuable insights into
architectural performance trends, discrepancies remain between simulated and
measured runtimes. These deviations stem from both inherent limitations of the
simulation environment and the restricted availability of detailed performance
specifications from CPU manufacturers, which hinder precise configuration
matching.

</details>


### [57] [6G Twin: Hybrid Gaussian Radio Fields for Channel Estimation and Non-Linear Precoder Design for Radio Access Networks](https://arxiv.org/abs/2509.18735)
*Muhammad Ahmed Mohsin,Muhammad Umer,Ahsan Bilal,Muhammad Ali Jamshed,Dean F. Hougen,John M. Cioffi*

Main category: cs.DC

TL;DR: 本文提出6G Twin，统一了压缩CSI获取、持续信道预测和能量最优非线性预编码器，形成实用框架，实现实时CSI、鲁棒跟踪和吞吐量 - 能量权衡。


<details>
  <summary>Details</summary>
Motivation: 设计端到端人工智能原生的无线接入网，解决现有技术在CSI获取、信道预测和能量效率方面的问题。

Method: 采用神经高斯无线场（GRF）压缩CSI获取，使用重放驱动的持续学习器进行信道预测，设计能量最优非线性预编码器minPMAC。

Result: GRF减少约100倍导频开销，实现1.1ms推理和不到2分钟现场训练；持续学习器使信道NMSE提升超10dB；minPMAC降低4 - 10倍能量，提升数据速率。

Conclusion: 这些组件形成实用的、支持GPU的框架，在3GPP设置下实现实时CSI、高效切换和吞吐量 - 能量权衡。

Abstract: This work introduces 6G Twin, the first end-to-end artificial intelligence
(AI)-native radio access network (RAN) design that unifies (i) neural Gaussian
Radio Fields (GRF) for compressed channel state information (CSI) acquisition,
(ii) continual channel prediction with handover persistence, and (iii) an
energy-optimal nonlinear precoder (minPMAC). GRF replaces dense pilots with a
sparse Gaussian field, cutting pilot overhead by about 100x while delivering
1.1 ms inference and less than 2 minutes on-site training, thus enabling
millisecond-scale closed-loop operation. A replay-driven continual learner
sustains accuracy under mobility and cell transitions, improving channel
normalized mean square error (NMSE) by more than 10 dB over frozen predictors
and an additional 2-5 dB over uniform replay, thereby stabilizing performance
across UMi/UMa handovers. Finally, minPMAC solves a convex, order-free MAC
precoder design that recovers the globally optimal order from Broadcast Channel
(BC) duals and minimizes transmit energy subject to minimum-rate guarantees,
achieving 4-10 times lower energy (scenario dependent) with monotonically
increasing bits per joule as SNR grows. This translates to up to 5 times higher
data rate at comparable power or the same rates at substantially lower power.
Together, these components form a practical, GPU-ready framework that attains
real-time CSI, robust tracking in dynamic networks with efficient handovers,
and state-of-the-art throughput-energy tradeoffs under 3GPP-style settings.

</details>


### [58] [On The Reproducibility Limitations of RAG Systems](https://arxiv.org/abs/2509.18869)
*Baiqiang Wang,Dongfang Zhao,Nathan R Tallent,Luanzheng Guo*

Main category: cs.DC

TL;DR: 本文介绍ReproRAG基准框架以衡量向量检索系统可重复性，通过大规模实证研究给出见解，开源框架助力可信AI科研。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成（RAG）的检索组件非确定性影响可靠性，需要衡量和量化向量检索系统的可重复性。

Method: 引入ReproRAG框架，研究整个流程不确定性来源，用精确匹配率等指标刻画可重复性和性能的权衡。

Result: 大规模实证研究发现不同嵌入模型对RAG可重复性有显著影响。

Conclusion: 开源的ReproRAG框架为研究人员和工程师提供工具，促进更可信的科研AI。

Abstract: Retrieval-Augmented Generation (RAG) is increasingly employed in generative
AI-driven scientific workflows to integrate rapidly evolving scientific
knowledge bases, yet its reliability is frequently compromised by
non-determinism in their retrieval components. This paper introduces ReproRAG,
a comprehensive benchmarking framework designed to systematically measure and
quantify the reproducibility of vector-based retrieval systems. ReproRAG
investigates sources of uncertainty across the entire pipeline, including
different embedding models, precision, retrieval algorithms, hardware
configurations, and distributed execution environments. Utilizing a suite of
metrics, such as Exact Match Rate, Jaccard Similarity, and Kendall's Tau, the
proposed framework effectively characterizes the trade-offs between
reproducibility and performance. Our large-scale empirical study reveals
critical insights; for instance, we observe that different embedding models
have remarkable impact on RAG reproducibility. The open-sourced ReproRAG
framework provides researchers and engineers productive tools to validate
deployments, benchmark reproducibility, and make informed design decisions,
thereby fostering more trustworthy AI for science.

</details>


### [59] [TD3-Sched: Learning to Orchestrate Container-based Cloud-Edge Resources via Distributed Reinforcement Learning](https://arxiv.org/abs/2509.18957)
*Shengye Song,Minxian Xu,Kan Hu,Wenxia Guo,Kejiang Ye*

Main category: cs.DC

TL;DR: 提出基于TD3的分布式强化学习调度器TD3 - Sched用于云边系统资源调度，在测试中相比基线有更好表现。


<details>
  <summary>Details</summary>
Motivation: 云边系统资源调度有挑战，现有集中式调度器有性能瓶颈和用户体验下降问题，要解决云边环境分布式决策问题。

Method: 提出基于Twin Delayed Deep Deterministic Policy Gradient (TD3)的分布式强化学习调度器TD3 - Sched，用于CPU和内存分配的连续控制。

Result: 在真实云边测试平台上，与其他强化学习和基于规则的基线相比，相同负载下延迟降低17.9% - 38.6%，高负载下降低16% - 31.6%，SLO合规性仅0.47%违规。

Conclusion: 与基线相比，TD3 - Sched在基于容器的云边环境中收敛更快、延迟更低、性能更稳定且能保证服务质量。

Abstract: Resource scheduling in cloud-edge systems is challenging as edge nodes run
latency-sensitive workloads under tight resource constraints, while existing
centralized schedulers can suffer from performance bottlenecks and user
experience degradation. To address the issues of distributed decisions in
cloud-edge environments, we present TD3-Sched, a distributed reinforcement
learning (DRL) scheduler based on Twin Delayed Deep Deterministic Policy
Gradient (TD3) for continuous control of CPU and memory allocation, which can
achieve optimized decisions for resource provisioning under dynamic workloads.
On a realistic cloud-edge testbed with SockShop application and Alibaba traces,
TD3-Sched achieves reductions of 17.9% to 38.6% in latency under same loads
compared with other reinforcement-learning and rule-based baselines, and 16% to
31.6% under high loads. TD3-Sched also shows superior Service Level Objective
(SLO) compliance with only 0.47% violations. These results indicate faster
convergence, lower latency, and more stable performance while preserving
service quality in container-based cloud-edge environment compared with the
baselines.

</details>


### [60] [Scheduler-Driven Job Atomization](https://arxiv.org/abs/2509.19086)
*Michal Konopa,Jan Fesl,Ladislav Beránek*

Main category: cs.DC

TL;DR: 提出Scheduler - Driven Job Atomization (SJA)新范式，介绍概念、构建模块和未来研究方向，未做完整实验评估。


<details>
  <summary>Details</summary>
Motivation: 现代GPU集群（尤其是基于NVIDIA MIG架构）因作业被视为不可分割的刚性块，依赖静态峰值内存估计，导致碎片化、利用率低和作业拒绝等效率问题。

Method: 建立调度器和作业之间的双向交互，调度器公布可用执行间隙，作业根据自身情况响应，调度器按策略选择作业，被选作业生成适配的子作业。

Result: 未给出具体实验结果，提出的SJA范式可实时匹配作业和机会，避免状态转移和中断。

Conclusion: SJA范式可提高GPU利用率、减少等待时间和迁移开销，论文为概念性研究，后续有研究方向待探索。

Abstract: Modern GPU clusters, particularly those built on NVIDIA's Multi-Instance GPU
(MIG) architecture, often suffer from inefficiencies because jobs are treated
as rigid, indivisible blocks that occupy a fixed slice until completion. The
reliance on static peak memory estimates exacerbates fragmentation,
underutilization, and job rejections. We propose Scheduler-Driven Job
Atomization (SJA), a new paradigm that establishes a bidirectional interaction
between scheduler and jobs. In SJA, the scheduler advertises available
execution gaps, and jobs respond by signaling interest if they can potentially
generate a subjob that fits the offered time-capacity window. The scheduler may
collect multiple signals for the same slot and, based on its allocation policy
(e.g., fairness, efficiency, or SLA priorities), selects which job is granted
the slot. Only then does the chosen job materialize a safe, self-contained
subjob tailored to that opportunity. Unlike migration or preemption, SJA
proactively shapes workloads before execution, thereby avoiding costly state
transfers and unpredictable interruptions. It aims to increase GPU utilization,
reduce wait times, and minimize migration overhead by aligning jobs with
opportunities in real time, ensuring that each admitted subjob is correct by
construction. This paper is presented as a concept paper: it introduces the
paradigm, defines its building blocks, and outlines future research directions,
rather than offering a full experimental evaluation.

</details>


### [61] [In-Transit Data Transport Strategies for Coupled AI-Simulation Workflow Patterns](https://arxiv.org/abs/2509.19150)
*Harikrishna Tummalapalli,Riccardo Balin,Christine M. Simpson,Andrew Park,Aymen Alsaadi,Andrew E. Shao,Wesley Brewer,Shantenu Jha*

Main category: cs.DC

TL;DR: 提出SimAI - Bench工具，用其在Aurora超级计算机上对两种耦合工作流的数据传输性能进行基准测试，得出不同模式下的最优策略。


<details>
  <summary>Details</summary>
Motivation: 耦合AI - 模拟工作流复杂度增加，需要新工具进行性能分析和原型设计。

Method: 使用SimAI - Bench工具对Aurora超级计算机上的一对一和多对一两种工作流的数据传输性能进行基准测试。

Result: 一对一模式下，节点本地和DragonHPC数据暂存策略比Redis和Lustre文件系统性能好；多对一模式下，数据传输是瓶颈，文件系统是最优策略。

Conclusion: SimAI - Bench可用于原型设计和评估耦合工作流，不同工作流模式有不同的最优数据传输策略。

Abstract: Coupled AI-Simulation workflows are becoming the major workloads for HPC
facilities, and their increasing complexity necessitates new tools for
performance analysis and prototyping of new in-situ workflows. We present
SimAI-Bench, a tool designed to both prototype and evaluate these coupled
workflows. In this paper, we use SimAI-Bench to benchmark the data transport
performance of two common patterns on the Aurora supercomputer: a one-to-one
workflow with co-located simulation and AI training instances, and a
many-to-one workflow where a single AI model is trained from an ensemble of
simulations. For the one-to-one pattern, our analysis shows that node-local and
DragonHPC data staging strategies provide excellent performance compared Redis
and Lustre file system. For the many-to-one pattern, we find that data
transport becomes a dominant bottleneck as the ensemble size grows. Our
evaluation reveals that file system is the optimal solution among the tested
strategies for the many-to-one pattern.

</details>


### [62] [Non-Uniform Content-Oblivious Leader Election on Oriented Asynchronous Rings](https://arxiv.org/abs/2509.19187)
*Jérémie Chalopin,Yi-Jun Chang,Lyuting Chen,Giuseppe A. Di Luna,Haoran Zhou*

Main category: cs.DC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the leader election problem in oriented ring networks under
content-oblivious asynchronous message-passing systems, where an adversary may
arbitrarily corrupt message contents.
  Frei et al. (DISC 2024) presented a uniform terminating leader election
algorithm for oriented rings in this setting, with message complexity $O(n
\cdot \mathsf{ID}_{\max})$ on a ring of size $n$, where $\mathsf{ID}_{\max}$ is
the largest identifier in the system, this result has been recently extended by
Chalopin et al. (DISC 2025) to unoriented rings.
  In this paper, we investigate the message complexity of leader election on
ring networks in the content-oblivious model, showing that no uniform algorithm
can solve the problem if each process is limited to sending a constant number
of messages in one direction.
  Interestingly, this limitation hinges on the uniformity assumption. In the
non-uniform setting, where processes know an upper bound $U \geq n$ on the ring
size, we present an algorithm with message complexity $O(n \cdot U \cdot
\mathsf{ID}_{\min})$, in which each process sends $O(U \cdot
\mathsf{ID}_{\min})$ messages clockwise and only three messages
counter-clockwise. Here, $\mathsf{ID}_{\min}$ is the smallest identifier in the
system. This dependence on the identifiers compares favorably with the
dependence on $\mathsf{ID}_{\max}$ of Frei et al.
  We also show a non-uniform algorithm where each process sends $O(U \cdot
\log\mathsf{ID}_{\min})$ messages in one direction and
$O(\log\mathsf{ID}_{\min})$ in the other. The factor $\log \mathsf{ID}_{\min}$
is optimal, matching the lower bound of Frei et al.
  Finally, in the anonymous setting, where processes do not have identifiers,
we propose a randomized algorithm where each process sends only $O(\log^2 U)$
messages, with a success probability of $1 - U^{-c}$.

</details>


### [63] [Accelerating Gravitational $N$-Body Simulations Using the RISC-V-Based Tenstorrent Wormhole](https://arxiv.org/abs/2509.19294)
*Jenny Lynn Almerol,Elisabetta Boella,Mario Spera,Daniele Gregori*

Main category: cs.DC

TL;DR: 介绍在基于RISC - V的Wormhole n300卡上加速天体物理N体代码，该平台有竞争力。


<details>
  <summary>Details</summary>
Motivation: 基于RISC - V的加速器在高性能科学计算领域有潜力，研究其在天体物理N体代码加速上的表现。

Method: 在Tenstorrent开发的基于RISC - V的Wormhole n300卡上加速天体物理N体代码。

Result: 相比高度优化的CPU实现，该平台实现超2倍加速和约2倍节能。

Conclusion: 基于RISC - V的Wormhole n300卡平台对采用此类算法的天体物理模拟极具竞争力。

Abstract: Although originally developed primarily for artificial intelligence
workloads, RISC-V-based accelerators are also emerging as attractive platforms
for high-performance scientific computing. In this work, we present our
approach to accelerating an astrophysical $N$-body code on the RISC-V-based
Wormhole n300 card developed by Tenstorrent. Our results show that this
platform can be highly competitive for astrophysical simulations employing this
class of algorithms, delivering more than a $2 \times$ speedup and
approximately $2 \times$ energy savings compared to a highly optimized CPU
implementation of the same code.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [64] [Precoloring extension with demands on paths](https://arxiv.org/abs/2509.18936)
*Arun Kumar Das,Michal Opler,Tomáš Valla*

Main category: cs.DS

TL;DR: 本文研究路径上的带需求的距离预着色扩展问题（DPED），给出特定条件下的多项式时间精确算法和近似算法，证明问题的NP完全性和参数化复杂度相关结果。


<details>
  <summary>Details</summary>
Motivation: 受商业广播频道节目调度中内容重复和放置约束的启发，该约束可转化为路径上的DPED问题。

Method: 针对预着色顶点在路径两端的情况给出多项式时间精确算法；设计具有加性近似因子的近似算法；证明问题的NP完全性；研究参数化复杂度，设计固定参数可处理（FPT）算法。

Result: 得到DPED在特定条件下的多项式时间精确算法和近似算法；证明DPED及相关问题的NP完全性；确定DPED在不同参数化下的复杂度；获得距离列表着色问题的相关结果。

Conclusion: DPED问题在路径上具有一定的复杂度，但在特定参数下存在有效的算法求解。

Abstract: Let $G$ be a graph with a set of precolored vertices, and let us be given an
integer distance parameter $d$ and a set of integer demands $d_1,\dots,d_c$.
The Distance Precoloring Extension with Demands (DPED) problem is to compute a
vertex $c$-coloring of $G$ such that the following three conditions hold: (i)
the resulting coloring respects the colors of the precolored vertices, (ii) the
distance of two vertices of the same color is at least $d$, and (iii) the
number of vertices colored by color $i$ is exactly $d_i$. This problem is
motivated by a program scheduling in commercial broadcast channels with
constraints on content repetition and placement, which leads precisely to the
DPED problem for paths.
  In this paper, we study DPED on paths and present a polynomial time exact
algorithm when precolored vertices are restricted to the two ends of the path
and devise an approximation algorithm for DPED with an additive approximation
factor polynomially bounded by $d$ and the number of precolored vertices. Then,
we prove that the Distance Precoloring Extension problem on paths, a less
restrictive version of DPED without the demand constraints, and then DPED
itself, is NP-complete. Motivated by this result, we further study the
parameterized complexity of DPED on paths. We establish that the DPED problem
on paths is $W[1]$-hard when parameterized by the number of colors and the
distance. On the positive side, we devise a fixed parameter tractable (FPT)
algorithm for DPED on paths when the number of colors, the distance, and the
number of precolored vertices are considered as the parameters. Moreover, we
prove that Distance Precoloring Extension is FPT parameterized by the distance.
As a byproduct, we also obtain several results for the Distance List Coloring
problem on paths.

</details>


### [65] [GraphBLAS Mathematical Opportunities: Parallel Hypersparse, Matrix Based Graph Streaming, and Complex-Index Matrices](https://arxiv.org/abs/2509.18984)
*Hayden Jananthan,Jeremy Kepner,Michael Jones,Vijay Gadepally,Michael Houle,Peter Michaleas,Chasen Milner,Alex Pentland*

Main category: cs.DS

TL;DR: 本文形式化开发并行超稀疏矩阵、基于矩阵的图流和复索引矩阵，举例展示其潜在优点。


<details>
  <summary>Details</summary>
Motivation: GraphBLAS高性能库标准带来新能力，形式化相关概念可将其应用于新领域。

Method: 对并行超稀疏矩阵、基于矩阵的图流和复索引矩阵进行形式化开发，并举例说明。

Result: 以各种示例展示了这些概念的潜在优点。

Conclusion: GraphBLAS能力可催生新的算法思路，形式化相关概念能带来应用于新领域的机会。

Abstract: The GraphBLAS high performance library standard has yielded capabilities
beyond enabling graph algorithms to be readily expressed in the language of
linear algebra. These GraphBLAS capabilities enable new performant ways of
thinking about algorithms that include leveraging hypersparse matrices for
parallel computation, matrix-based graph streaming, and complex-index matrices.
Formalizing these concepts mathematically provides additional opportunities to
apply GraphBLAS to new areas. This paper formally develops parallel hypersparse
matrices, matrix-based graph streaming, and complex-index matrices and
illustrates these concepts with various examples to demonstrate their potential
merits.

</details>


### [66] [Optimization of Base-n Radix Sort for Skewed Datasets](https://arxiv.org/abs/2509.19021)
*Atharv Pandey,Lakshmanan Kuppusamy*

Main category: cs.DS

TL;DR: 分析非比较排序算法Base - n Radix Sort (BNRS)并提出优化变体Stable Logical Partition Radix Sort (SLPR)，证明其在特定条件下比传统比较排序算法更优，且SLPR对偏斜数据集排序有效。


<details>
  <summary>Details</summary>
Motivation: 研究非比较排序算法，以找到在特定整数分布下比传统比较排序算法更优的排序方法。

Method: 分析BNRS算法，提出SLPR优化变体，通过输入大小n和最大值k衡量算法复杂度。

Result: 在时间复杂度上，当k < nlog₂ⁿ时，这些算法比传统比较排序算法更简洁；SLPR对偏斜数据集排序高效。

Conclusion: BNRS及其优化变体SLPR在特定整数分布和偏斜数据集排序上有优势。

Abstract: The importance and applications of sorting is apparent and needs no
explanation. In this paper, we analyse a non-comparison sorting algorithm,
Base-n Radix Sort (BNRS) and introduce an optimized vari- ant of BNRS, namely,
Stable Logical Partition Radix Sort (SLPR). The complexity of these algorithms
is measured by the input size $n$ and the maximum value $k$. We show that with
respect to time complexity, these algorithms are more succinct than traditional
comparison-based sorting algorithms for representing the sorted order of
certain integer distribu- tions, specifically, when $k <nlog_2^n$ is met. We
also show that the SLPR optimization, which uses in-place stable partitioning
to reduce the active problem size in each pass, resulting in highly effective
sorting for skewed datasets that contain a majority of small numbers and mix of
very large numbers.

</details>


### [67] [Linear Regression under Missing or Corrupted Coordinates](https://arxiv.org/abs/2509.19242)
*Ilias Diakonikolas,Jelena Diakonikolas,Daniel M. Kane,Jasper C. H. Lee,Thanasis Pittas*

Main category: cs.DS

TL;DR: 研究高斯协变量下多元线性回归在数据被对手擦除或损坏两种情况下的误差，给出误差的信息论下界，发现缺失数据和损坏数据的最优误差匹配。


<details>
  <summary>Details</summary>
Motivation: 现有对对抗性数据缺失下的线性回归研究不足，即使在信息论层面也了解甚少。

Method: 建立信息论下界，并与（计算高效的）算法的误差相匹配。

Result: 得到了在几乎整个参数范围内误差的常数因子刻画，发现缺失数据和损坏数据的最优误差匹配。

Conclusion: 知道损坏位置在一般情况下没有优势。

Abstract: We study multivariate linear regression under Gaussian covariates in two
settings, where data may be erased or corrupted by an adversary under a
coordinate-wise budget. In the incomplete data setting, an adversary may
inspect the dataset and delete entries in up to an $\eta$-fraction of samples
per coordinate; a strong form of the Missing Not At Random model. In the
corrupted data setting, the adversary instead replaces values arbitrarily, and
the corruption locations are unknown to the learner. Despite substantial work
on missing data, linear regression under such adversarial missingness remains
poorly understood, even information-theoretically. Unlike the clean setting,
where estimation error vanishes with more samples, here the optimal error
remains a positive function of the problem parameters. Our main contribution is
to characterize this error up to constant factors across essentially the entire
parameter range. Specifically, we establish novel information-theoretic lower
bounds on the achievable error that match the error of (computationally
efficient) algorithms. A key implication is that, perhaps surprisingly, the
optimal error in the missing data setting matches that in the corruption
setting-so knowing the corruption locations offers no general advantage.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [68] [On Sybil-proofness in Restaking Networks](https://arxiv.org/abs/2509.18338)
*Tarun Chitra,Paolo Penna,Manvir Schneider*

Main category: cs.GT

TL;DR: 本文提出重质押网络中防女巫攻击的形式化框架，分析削减机制，证明不可能定理，研究网络结构影响，揭示重质押机制设计的基本限制。


<details>
  <summary>Details</summary>
Motivation: 重质押协议安全性依赖于对女巫攻击的抵抗，需研究防女巫攻击机制。

Method: 引入形式化框架区分两种女巫攻击，分析边际和乘法削减机制，证明不可能定理，通过随机图模型研究网络结构影响。

Result: 没有削减机制能同时防止两种攻击类型；Erdös-Rényi网络可防女巫攻击，两模块随机块模型的最小异质性会使女巫攻击有利可图。

Conclusion: 揭示了重质押机制设计的基本限制，强调了网络拓扑的关键作用。

Abstract: Restaking protocols expand validator responsibilities beyond consensus, but
their security depends on resistance to Sybil attacks. We introduce a formal
framework for Sybil-proofness in restaking networks, distinguishing between two
types of attacks, one in which other Sybil identities are kept out of an attack
and one where multiple Sybil identities attack. We analyze marginal and
multiplicative slashing mechanisms and characterize the conditions under which
each deters Sybil strategies. We then prove an impossibility theorem: no
slashing mechanism can simultaneously prevent both attack types. Finally, we
study the impact of network structure through random graph models: while
Erd\"os-R\'enyi networks remain Sybil-proof, even minimal heterogeneity in a
two-block stochastic block model makes Sybil attacks profitable. These results
reveal fundamental limits of mechanism design for restaking and highlight the
critical role of network topology.

</details>


### [69] [Fair Decisions through Plurality: Results from a Crowdfunding Platform](https://arxiv.org/abs/2509.18343)
*Joel Miller,E. Glen Weyl,Chris Kanich*

Main category: cs.GT

TL;DR: 本文讨论众筹平台算法干预，提出 CO - QF 算法优于原 QF 算法，有高采纳率和资金分配，模拟显示社会福利更好，还可用于通用公共决策。


<details>
  <summary>Details</summary>
Motivation: 提升众筹平台公平性和经济效率，解决原 QF 算法基于个体孤立自私模型的问题。

Method: 结合技术和定性方法，提出 CO - QF 算法并与 QF 算法对比，进行模拟。

Result: CO - QF 算法在平台采纳率达 89%，已分配超 400 万美元，模拟显示社会福利优于 QF 算法。

Conclusion: CO - QF 算法不仅适用于特定社区，还可能用于通用公共决策。

Abstract: We discuss an algorithmic intervention aimed at increasing equity and
economic efficiency at a crowdfunding platform that gives cash subsidies to
grantees. Through a blend of technical and qualitative methods, we show that
the previous algorithm used by the platform -- Quadratic Funding (QF) --
suffered problems because its design was rooted in a model of individuals as
isolated and selfish. We present an alternative algorithm --
Connection-Oriented Quadratic Funding (CO-QF) -- rooted in a theory of
plurality and prosocial utilities, and show that it qualitatively and
quantitatively performs better than QF. CO-QF has achieved an 89% adoption rate
at the platform and has distributed over $4 Million to date. In simulations we
show that it provides better social welfare than QF. While our design for CO-QF
was responsive to the needs of a specific community, we also extrapolate out of
this context to show that CO-QF is a potentially helpful tool for
general-purpose public decision making.

</details>


### [70] [Group Formation through Game Theory and Agent-Based Modeling: Spatial Cohesion, Heterogeneity, and Resource Pooling](https://arxiv.org/abs/2509.18551)
*Chenlan Wang,Jimin Han,Diana Jue-Rajasingh*

Main category: cs.GT

TL;DR: 本文构建博弈论和基于主体的模型研究资源汇集、空间凝聚和异质性驱动的群体形成，以跨部门伙伴关系为例，分析群体形成动态，得出资源和空间条件对群体规模和构成的影响。


<details>
  <summary>Details</summary>
Motivation: 研究由资源汇集、空间凝聚和异质性驱动的群体形成，聚焦跨部门伙伴关系。

Method: 构建博弈论模型和基于主体的模型，证明稳定群体均衡的存在性，并模拟不同空间和资源条件下的形成动态。

Result: 个体资源有限时群体主要在附近参与者中形成，资源丰富时群体可跨越更大距离；资源异质性和空间接近性促进更大、更多样化群体的形成。

Conclusion: 研究揭示了影响群体规模和构成的关键权衡，可为跨部门合作和多主体系统提供策略指导。

Abstract: This paper develops a game-theoretic model and an agent-based model to study
group formation driven by resource pooling, spatial cohesion, and
heterogeneity. We focus on cross-sector partnerships (CSPs) involving public,
private, and nonprofit organizations, each contributing distinct resources.
Group formation occurs as agents strategically optimize their choices in
response to others within a competitive setting. We prove the existence of
stable group equilibria and simulate formation dynamics under varying spatial
and resource conditions. The results show that limited individual resources
lead to groups that form mainly among nearby actors, while abundant resources
allow groups to move across larger distances. Increased resource heterogeneity
and spatial proximity promote the formation of larger and more diverse groups.
These findings reveal key trade-offs shaping group size and composition,
guiding strategies for effective cross-sector collaborations and multi-agent
systems.

</details>


### [71] [Proximately Envy-Free and Efficient Allocation of Mixed Manna](https://arxiv.org/abs/2509.18673)
*Siddharth Barman,Paritosh Verma*

Main category: cs.GT

TL;DR: 本文聚焦不可分物品公平分配问题，在不可分物品、杂务分配成果基础上，证明了混合物品PO和IEF1分配的存在性。


<details>
  <summary>Details</summary>
Motivation: 此前对于不可分物品和杂务已证明PO和EF1分配存在性，但混合物品的PO和EF1分配存在性仍是未解决问题，作者希望在此方向取得进展。

Method: 无明确提及具体方法，主要通过理论推导证明PO和IEF1分配对混合物品的存在性。

Result: 证明了混合物品中PO和IEF1分配的存在性，IEF1分配中每个主体可通过增减一项物品消除对其他主体的嫉妒。

Conclusion: 本文结果推广了Mahara关于不可分杂务的结果。

Abstract: The existence of fair and efficient allocations of indivisible items is a
central problem in fair division. For indivisible goods, the existence of
Pareto efficient (PO) and envy free up to one item (EF1) allocations was
established by Caragiannis et al. In a recent breakthrough, Mahara established
the existence of PO and EF1 allocations for indivisible chores.
  However, the existence of PO and EF1 allocations of mixed manna remains an
intriguing open problem. In this paper, we make significant progress in this
direction. We establish the existence of allocations that are PO and
introspective envy free up to one item (IEF1) for mixed manna. In an IEF1
allocation, each agent can eliminate its envy towards all the other agents by
either adding an item or removing an item from its own bundle. The notion of
IEF1 coincides with EF1 for indivisible chores, and hence, our existence result
generalizes the aforementioned result of Mahara.

</details>


### [72] [Approximating Electoral Control Problems](https://arxiv.org/abs/2509.19279)
*Huy Vu Bui,Michael C. Chavrimootoo,Trung Kien Le,Son M. Nguyen*

Main category: cs.GT

TL;DR: 本文研究选举控制问题的近似算法，在多种投票规则下确定问题是否可近似，给出算法并证明其最优性，还推广算法，解决了18年前的一系列开放问题。


<details>
  <summary>Details</summary>
Motivation: 选举控制研究多关注决策复杂度结果，近似算法在该领域受关注少，本文旨在填补这一空白。

Method: 利用覆盖整数规划（CIPs）可近似的特性，使用已知的最小k - 并（MkU）问题下界，采用公理化方法推广算法。

Result: 在多种投票规则下确定问题是否可近似，给出近似算法并证明其最优性（除非P = NP），给出O(m) - 近似算法和下界Ω(m^{1/4})，将算法推广到无限族投票规则。

Conclusion: 本文工作解决了18年前的一系列开放问题。

Abstract: Much research in electoral control -- one of the most studied form of
electoral attacks, in which an entity running an election alters the structure
of that election to yield a preferred outcome -- has focused on giving decision
complexity results, e.g., membership in P, NP-completeness, or fixed-parameter
tractability. Approximation algorithms on the other hand have received little
attention in electoral control, despite their prevalence in the study of other
forms of electoral attacks, such as manipulation and bribery. Early work
established some preliminary results with respect to popular voting rules such
as plurality, approval, and Condorcet. In this paper, we establish for each of
the ``standard'' control problems under plurality, approval, and Condorcet,
whether they are approximable, and we prove our results in both the weighted
and unweighted voter settings. For each problem we study under either approval
or Condorcet, we show that any approximation algorithm we give is optimal,
unless P=NP. Our approximation algorithms leverage the fact that Covering
Integer Programs (CIPs) can be approximated within a factor of $O(\log n)$.
Under plurality, we give an $O(m)$-approximation algorithm, and give as lower
bound $\Omega(m^{1/4})$, by using a known lower bound on the Minimum $k$-Union
(M$k$U) problem. To our knowledge, this is the first application of M$k$U in
computational social choice. We also generalize our $O(m)$-approximation
algorithm to work with respect to an infinite family of voting rules using an
axiomatic approach. Our work closes a long list of open problems established 18
years ago.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [73] [Understand your Users, An Ensemble Learning Framework for Natural Noise Filtering in Recommender Systems](https://arxiv.org/abs/2509.18560)
*Clarita Hawat,Wissam Al Jurdi,Jacques Bou Abdo,Jacques Demerjian,Abdallah Makhoul*

Main category: cs.IR

TL;DR: 论文针对推荐系统中噪声定义挑战，提出识别噪声评分的新框架，提高推荐准确性和用户满意度。


<details>
  <summary>Details</summary>
Motivation: 解决因人类偏好和行为可变性导致的噪声定义挑战，以提升推荐系统效果。

Method: 提出模块化框架，包含物品分类的自然噪声算法、集成学习模型和基于签名的噪声识别，还倡导评估意外性和群体验证的指标。

Result: 提供更干净的训练数据集，提高推荐准确性和鲁棒性。

Conclusion: 该方法能提高用户对推荐系统的满意度和参与度。

Abstract: The exponential growth of web content is a major key to the success for
Recommender Systems. This paper addresses the challenge of defining noise,
which is inherently related to variability in human preferences and behaviors.
In classifying changes in user tendencies, we distinguish three kinds of
phenomena: external factors that directly influence users' sentiment,
serendipity causing unexpected preference, and incidental interaction perceived
as noise. To overcome these problems, we present a new framework that
identifies noisy ratings. In this context, the proposed framework is modular,
consisting of three layers: known natural noise algorithms for item
classification, an Ensemble learning model for refined evaluation of the items
and signature-based noise identification. We further advocate the metrics that
quantitatively assess serendipity and group validation, offering higher
robustness in recommendation accuracy. Our approach aims to provide a cleaner
training dataset that would inherently improve user satisfaction and engagement
with Recommender Systems.

</details>


### [74] [The Ranking Blind Spot: Decision Hijacking in LLM-based Text Ranking](https://arxiv.org/abs/2509.18575)
*Yaoyao Qian,Yifan Zeng,Yuchao Jiang,Chelsi Jain,Huazheng Wang*

Main category: cs.IR

TL;DR: 研究LLMs在多文档比较任务中的‘排名盲点’，提出两种攻击方式影响LLM排名系统，实验证明攻击有效且强模型更易受攻击。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs指令跟随能力与多文档比较任务的交互，识别其中的‘排名盲点’。

Method: 通过决策目标劫持和决策标准劫持两种方法分析排名盲点对LLM评估系统的影响。

Result: 提出的攻击在多种LLMs和排名方案中有效，强LLMs更易受攻击。

Conclusion: LLMs存在排名盲点，恶意内容提供者可利用此弱点，代码开源。

Abstract: Large Language Models (LLMs) have demonstrated strong performance in
information retrieval tasks like passage ranking. Our research examines how
instruction-following capabilities in LLMs interact with multi-document
comparison tasks, identifying what we term the "Ranking Blind Spot", a
characteristic of LLM decision processes during comparative evaluation. We
analyze how this ranking blind spot affects LLM evaluation systems through two
approaches: Decision Objective Hijacking, which alters the evaluation goal in
pairwise ranking systems, and Decision Criteria Hijacking, which modifies
relevance standards across ranking schemes. These approaches demonstrate how
content providers could potentially influence LLM-based ranking systems to
affect document positioning. These attacks aim to force the LLM ranker to
prefer a specific passage and rank it at the top. Malicious content providers
can exploit this weakness, which helps them gain additional exposure by
attacking the ranker. In our experiment, We empirically show that the proposed
attacks are effective in various LLMs and can be generalized to multiple
ranking schemes. We apply these attack to realistic examples to show their
effectiveness. We also found stronger LLMs are more vulnerable to these
attacks. Our code is available at:
https://github.com/blindspotorg/RankingBlindSpot

</details>


### [75] [Agentic AutoSurvey: Let LLMs Survey LLMs](https://arxiv.org/abs/2509.18661)
*Yixin Liu,Yonghui Wu,Denghui Zhang,Lichao Sun*

Main category: cs.IR

TL;DR: 提出Agentic AutoSurvey多智能体框架用于自动生成文献综述，实验表明该方法比现有基线有显著改进，证明多智能体架构在自动文献综述生成方面的进步。


<details>
  <summary>Details</summary>
Motivation: 科学文献呈指数级增长，现有方法在合成知识方面存在局限性，需要新的自动生成文献综述的方法。

Method: 采用四个专门的智能体（论文搜索专家、主题挖掘与聚类、学术综述撰写、质量评估）协同工作生成文献综述。

Result: 在六个代表性大语言模型研究主题上，多智能体方法比AutoSurvey有显著提升，得分为8.18/10（AutoSurvey为4.77/10），能处理75 - 443篇论文/主题，有较高引用覆盖率，12维评估表现良好。

Conclusion: 多智能体架构在快速发展的科学领域的自动文献综述生成方面是有意义的进步。

Abstract: The exponential growth of scientific literature poses unprecedented
challenges for researchers attempting to synthesize knowledge across rapidly
evolving fields. We present \textbf{Agentic AutoSurvey}, a multi-agent
framework for automated survey generation that addresses fundamental
limitations in existing approaches. Our system employs four specialized agents
(Paper Search Specialist, Topic Mining \& Clustering, Academic Survey Writer,
and Quality Evaluator) working in concert to generate comprehensive literature
surveys with superior synthesis quality. Through experiments on six
representative LLM research topics from COLM 2024 categories, we demonstrate
that our multi-agent approach achieves significant improvements over existing
baselines, scoring 8.18/10 compared to AutoSurvey's 4.77/10. The multi-agent
architecture processes 75--443 papers per topic (847 total across six topics)
while targeting high citation coverage (often $\geq$80\% on 75--100-paper sets;
lower on very large sets such as RLHF) through specialized agent orchestration.
Our 12-dimension evaluation captures organization, synthesis integration, and
critical analysis beyond basic metrics. These findings demonstrate that
multi-agent architectures represent a meaningful advancement for automated
literature survey generation in rapidly evolving scientific domains.

</details>


### [76] [Robust Denoising Neural Reranker for Recommender Systems](https://arxiv.org/abs/2509.18736)
*Wenyu Mao,Shuchang Liu,Hailan Yang,Xiaobei Wang,Xiaoyu Yang,Xu Gao,Xiang Li,Lantao Hu,Han Li,Kun Gai,An Zhang,Xiang Wang*

Main category: cs.IR

TL;DR: 本文指出工业多阶段推荐器中直接使用检索器分数的局限，提出对抗框架DNR并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有两阶段检索 - 排序框架中，上一阶段检索器分数的重要性未充分探索，直接使用有局限。

Method: 理论分析直接使用检索器分数的局限，提出对抗框架DNR，扩展常规分数误差最小化项，增加三个增强目标。

Result: 在三个公共数据集上进行大量实验，验证了DNR的有效性。

Conclusion: 所提出的DNR框架有效，能解决检索器分数去噪问题。

Abstract: For multi-stage recommenders in industry, a user request would first trigger
a simple and efficient retriever module that selects and ranks a list of
relevant items, then calls a slower but more sophisticated deep reranking model
that refines the item arrangement before exposure to the user. The latter model
typically reranks the item list conditioned on the user's history content and
the initial ranking from retrievers. Although this two-stage retrieval-ranking
framework demonstrates practical effectiveness, the significance of retriever
scores from the previous stage has been limitedly explored, which is
informative. In this work, we first theoretically analyze the limitations of
using retriever scores as the rerankers' input directly and argue that the
reranking task is essentially a noise reduction problem from the retriever
scores. Following this notion, we derive an adversarial framework, DNR, that
associates the denoising reranker with a carefully designed noise generation
module. We extend the conventional score error minimization term with three
augmented objectives, including: 1) a denoising objective that aims to denoise
the noisy retriever scores to align with the user feedback; 2) an adversarial
retriever score generation objective that improves the exploration in the
retriever score space; and 3) a distribution regularization term that aims to
align the distribution of generated noisy retriever scores with the real ones.
Extensive experiments are conducted on three public datasets, together with
analytical support, validating the effectiveness of the proposed DNR.

</details>


### [77] [Single-Branch Network Architectures to Close the Modality Gap in Multimodal Recommendation](https://arxiv.org/abs/2509.18807)
*Christian Ganhör,Marta Moscati,Anna Hausberger,Shah Nawaz,Markus Schedl*

Main category: cs.IR

TL;DR: 本文提出用带权重共享、模态采样和对比损失的单分支神经网络，在模态缺失场景下提供准确推荐，实验表明其在模态缺失时表现更好。


<details>
  <summary>Details</summary>
Motivation: 传统混合推荐系统在部分模态缺失时推荐质量下降，需新方法解决模态缺失问题。

Method: 利用带权重共享、模态采样和对比损失的单分支神经网络，与多分支网络对比，在三个数据集上做实验，用多种指标评估。

Result: 单分支网络在热启动场景有竞争力，在模态缺失场景显著更好，使物品模态在嵌入空间更接近。

Conclusion: 单分支神经网络能有效缩小模态差距，在模态缺失场景表现优越。

Abstract: Traditional recommender systems rely on collaborative filtering, using past
user-item interactions to help users discover new items in a vast collection.
In cold start, i.e., when interaction histories of users or items are not
available, content-based recommender systems use side information instead.
Hybrid recommender systems (HRSs) often employ multimodal learning to combine
collaborative and side information, which we jointly refer to as modalities.
Though HRSs can provide recommendations when some modalities are missing, their
quality degrades. In this work, we utilize single-branch neural networks
equipped with weight sharing, modality sampling, and contrastive loss to
provide accurate recommendations even in missing modality scenarios by
narrowing the modality gap. We compare these networks with multi-branch
alternatives and conduct extensive experiments on three datasets. Six
accuracy-based and four beyond-accuracy-based metrics help assess the
recommendation quality for the different training paradigms and their
hyperparameters in warm-start and missing modality scenarios. We quantitatively
and qualitatively study the effects of these different aspects on bridging the
modality gap. Our results show that single-branch networks achieve competitive
performance in warm-start scenarios and are significantly better in missing
modality settings. Moreover, our approach leads to closer proximity of an
item's modalities in the embedding space. Our full experimental setup is
available at https://github.com/hcai-mms/single-branch-networks.

</details>


### [78] [RELATE: Relation Extraction in Biomedical Abstracts with LLMs and Ontology Constraints](https://arxiv.org/abs/2509.19057)
*Olawumi Olasunkanmi,Mathew Satursky,Hong Yi,Chris Bizon,Harlin Lee,Stanley Ahalt*

Main category: cs.IR

TL;DR: 本文介绍RELATE管道，将大语言模型提取的生物医学关系映射到标准化本体谓词，在基准测试中效果良好，为将非结构化文献转化为标准化知识图谱提供框架。


<details>
  <summary>Details</summary>
Motivation: 生物医学知识图谱不完整，大语言模型提取的关系缺乏标准化和本体对齐，限制了知识图谱集成。

Method: 引入RELATE三阶段管道，包括本体预处理、基于相似度的检索、大语言模型重排，处理显式否定。

Result: 在ChemProt基准测试中，精确匹配率52%，准确率@10达94%；在2400篇HEAL项目摘要中，有效拒绝无关关联，识别否定断言。

Conclusion: RELATE结合向量搜索和上下文推理，提供可扩展、语义准确的框架，用于将非结构化生物医学文献转化为标准化知识图谱。

Abstract: Biomedical knowledge graphs (KGs) are vital for drug discovery and clinical
decision support but remain incomplete. Large language models (LLMs) excel at
extracting biomedical relations, yet their outputs lack standardization and
alignment with ontologies, limiting KG integration. We introduce RELATE, a
three-stage pipeline that maps LLM-extracted relations to standardized ontology
predicates using ChemProt and the Biolink Model. The pipeline includes: (1)
ontology preprocessing with predicate embeddings, (2) similarity-based
retrieval enhanced with SapBERT, and (3) LLM-based reranking with explicit
negation handling. This approach transforms relation extraction from free-text
outputs to structured, ontology-constrained representations. On the ChemProt
benchmark, RELATE achieves 52% exact match and 94% accuracy@10, and in 2,400
HEAL Project abstracts, it effectively rejects irrelevant associations (0.4%)
and identifies negated assertions. RELATE captures nuanced biomedical
relationships while ensuring quality for KG augmentation. By combining vector
search with contextual LLM reasoning, RELATE provides a scalable, semantically
accurate framework for converting unstructured biomedical literature into
standardized KGs.

</details>


### [79] [A Knowledge Graph and a Tripartite Evaluation Framework Make Retrieval-Augmented Generation Scalable and Transparent](https://arxiv.org/abs/2509.19209)
*Olalekan K. Akindele,Bhupesh Kumar Mishra,Kenneth Y. Wertheim*

Main category: cs.IR

TL;DR: 本文提出RAG聊天机器人及RAG - Eval评估框架，实验证明其有效，为开发准确、可验证聊天机器人提供路径。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型应用于聊天机器人时，特定领域准确性和避免事实不一致的问题，设计有效聊天机器人并评估其效果。

Method: 构建利用知识图谱和向量搜索检索的RAG聊天机器人，引入基于大语言模型的三方评估框架RAG - Eval进行评估。

Result: 实验对比证明RAG - Eval有效，能可靠检测事实差距和查询不匹配。

Conclusion: 为开发准确、用户可验证的聊天机器人提供了可扩展的路径，弥合对话流畅性和事实准确性的差距。

Abstract: Large Language Models (LLMs) have significantly enhanced conversational
Artificial Intelligence(AI) chatbots; however, domain-specific accuracy and the
avoidance of factual inconsistencies remain pressing challenges, particularly
for large datasets. Designing an effective chatbot with appropriate methods and
evaluating its effectiveness is among the challenges in this domain. This study
presents a Retrieval Augmented Generation (RAG) chatbot that harnesses a
knowledge graph and vector search retrieval to deliver precise, context-rich
responses in an exemplary use case from over high-volume engineering
project-related emails, thereby minimising the need for document chunking. A
central innovation of this work is the introduction of RAG Evaluation
(RAG-Eval), a novel chain-of-thought LLM-based tripartite evaluation framework
specifically developed to assess RAG applications. This framework operates in
parallel with the chatbot, jointly assessing the user's query, the retrieved
document, and the generated response, enabling a holistic evaluation across
multiple quality metrics like query relevance, factual accuracy, coverage,
coherence and fluency. The resulting scoring system is provided directly to
users as a confidence score (1 to 100%), enabling quick identification of
possible misaligned or incomplete answers. This proposed approach promotes
transparency and rapid verification by incorporating metadata email IDs,
timestamps into responses. Experimental comparisons against BERTScore and
G-EVAL for summarisation evaluation tasks confirm its effectiveness, and
empirical analysis also shows RAG-Eval reliably detects factual gaps and query
mismatches, thereby fostering trust in high demand, data centric environments.
These findings highlight a scalable path for developing accurate,
user-verifiable chatbots that bridge the gap between high-level conversational
fluency and factual accuracy.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [80] [A Coopetitive-Compatible Data Generation Framework for Cross-silo Federated Learning](https://arxiv.org/abs/2509.18120)
*Thanh Linh Nguyen,Quoc-Viet Pham*

Main category: cs.LG

TL;DR: 本文提出CoCoGen框架，利用生成式AI和潜在博弈论，在异构和竞争环境下优化协作学习，实验表明其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注跨组织的统计异质性，而经济竞争带来的挑战以及统计异质性和组织间竞争的综合影响研究不足。

Method: 提出CoCoGen框架，通过学习性能和基于效用的公式表征竞争和统计异质性，将每轮训练建模为加权潜在博弈，推导基于GenAI的数据生成策略。

Result: 在Fashion - MNIST数据集上的实验揭示了不同异质性和竞争水平对组织行为的影响，CoCoGen始终优于基线方法。

Conclusion: CoCoGen框架能有效应对异构和竞争环境下的协作学习问题，提升社会福利。

Abstract: Cross-silo federated learning (CFL) enables organizations (e.g., hospitals or
banks) to collaboratively train artificial intelligence (AI) models while
preserving data privacy by keeping data local. While prior work has primarily
addressed statistical heterogeneity across organizations, a critical challenge
arises from economic competition, where organizations may act as market rivals,
making them hesitant to participate in joint training due to potential utility
loss (i.e., reduced net benefit). Furthermore, the combined effects of
statistical heterogeneity and inter-organizational competition on
organizational behavior and system-wide social welfare remain underexplored. In
this paper, we propose CoCoGen, a coopetitive-compatible data generation
framework, leveraging generative AI (GenAI) and potential game theory to model,
analyze, and optimize collaborative learning under heterogeneous and
competitive settings. Specifically, CoCoGen characterizes competition and
statistical heterogeneity through learning performance and utility-based
formulations and models each training round as a weighted potential game. We
then derive GenAI-based data generation strategies that maximize social
welfare. Experimental results on the Fashion-MNIST dataset reveal how varying
heterogeneity and competition levels affect organizational behavior and
demonstrate that CoCoGen consistently outperforms baseline methods.

</details>


### [81] [PiMoE: Token-Level Routing for Integrating High-Precision Computation and Reasoning](https://arxiv.org/abs/2509.18169)
*Hengbo Xiao,Jingyuan Fan,Xin Tong,Jingzhao Zhang,Chao Lu,Guannan He*

Main category: cs.LG

TL;DR: 提出PiMoE架构集成计算与推理，评估显示其在准确性、响应延迟等方面表现优，为智能系统提供新范式。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型无法将高精度数值计算作为内在可解释能力，主流多智能体方法有通信开销大等问题，需新架构集成计算与推理。

Method: 提出PiMoE架构，分别训练专家、文本到计算模块和路由器，将计算能力内置于神经网络，推理时路由器在token级别指导计算和推理。

Result: PiMoE架构比直接微调大语言模型有更高准确性，相比主流多智能体方法在响应延迟、token使用和GPU能耗上有显著改善。

Conclusion: PiMoE为下一代科学或工业智能系统提供了高效、可解释和可扩展的范式。

Abstract: Complex systems typically rely on high-precision numerical computation to
support decisions, but current large language models (LLMs) cannot yet
incorporate such computations as an intrinsic and interpretable capability with
existing architectures. Mainstream multi-agent approaches can leverage external
experts, but inevitably introduce communication overhead and suffer from
inefficient multimodal emergent capability and limited scalability. To this
end, we propose PiMoE (Physically-isolated Mixture of Experts), a training and
inference architecture for integrating computation and reasoning. Instead of
the workflow paradigm of tool invocation, PiMoE endogenously integrates
computational capabilities into neural networks after separately training
experts, a text-to-computation module, and a router. At inference, the router
directs computation and reasoning at the token level, thereby enabling
iterative alternation within a single chain of thought. We evaluate PiMoE on
two reasoning-computation tasks against LLM finetuning and the multi-agent
system approaches. Results show that the PiMoE architecture achieves not only
higher accuracy than directly finetuning LLMs but also significant improvements
in response latency, token usage, and GPU energy consumption compared with
mainstream multi-agent approaches. PiMoE offers an efficient, interpretable,
and scalable paradigm for next-generation scientific or industrial intelligent
systems.

</details>


### [82] [Machine Learnability as a Measure of Order in Aperiodic Sequences](https://arxiv.org/abs/2509.18103)
*Jennifer Dodgson,Michael Joedhitya,Adith Ramdas,Surender Suresh Kumar,Adarsh Singh Chauhan,Akira Rafhael,Wang Mingshu,Nordine Lotfi*

Main category: cs.LG

TL;DR: 本文展示可用图像聚焦的机器学习模型测量乌拉姆螺旋特定区域质数域的比较规律性，发现特定区域模型表现更好，表明机器学习可作为数论新实验工具。


<details>
  <summary>Details</summary>
Motivation: 质数分布具有确定性和类似随机过程的统计行为，探索用新方法研究其规律性。

Method: 使用图像聚焦的机器学习模型，对比不同区域提取的块训练的模型。

Result: 500m附近区域提取块训练的模型在纯准确率上优于25m以下区域的模型，不同区域模型分类方式有差异。

Conclusion: 机器学习可作为数论新实验工具，该方法对密码学中强弱质数模式研究有潜力。

Abstract: Research on the distribution of prime numbers has revealed a dual character:
deterministic in definition yet exhibiting statistical behavior reminiscent of
random processes. In this paper we show that it is possible to use an
image-focused machine learning model to measure the comparative regularity of
prime number fields at specific regions of an Ulam spiral. Specifically, we
demonstrate that in pure accuracy terms, models trained on blocks extracted
from regions of the spiral in the vicinity of 500m outperform models trained on
blocks extracted from the region representing integers lower than 25m. This
implies existence of more easily learnable order in the former region than in
the latter. Moreover, a detailed breakdown of precision and recall scores seem
to imply that the model is favouring a different approach to classification in
different regions of the spiral, focusing more on identifying prime patterns at
lower numbers and more on eliminating composites at higher numbers. This aligns
with number theory conjectures suggesting that at higher orders of magnitude we
should see diminishing noise in prime number distributions, with averages
(density, AP equidistribution) coming to dominate, while local randomness
regularises after scaling by log x. Taken together, these findings point toward
an interesting possibility: that machine learning can serve as a new
experimental instrument for number theory. Notably, the method shows potential
1 for investigating the patterns in strong and weak primes for cryptographic
purposes.

</details>


### [83] [Data Valuation and Selection in a Federated Model Marketplace](https://arxiv.org/abs/2509.18104)
*Wenqian Li,Youjia Yang,Ruoxi Jia,Yan Pang*

Main category: cs.LG

TL;DR: 本文提出基于Wasserstein估计器的框架用于联邦学习，解决数据估值和选择挑战，实验证明能识别高性能数据组合。


<details>
  <summary>Details</summary>
Motivation: 在人工智能时代，为建立可信数据市场，联邦学习面临有效数据估值和选择的挑战，需要解决该问题。

Method: 引入基于Wasserstein的估计器，提出分布式方法近似Wasserstein距离，利用神经缩放定律外推模型性能。

Result: 通过不同场景的大量实验，该方法能一致地识别出高性能的数据组合。

Conclusion: 该方法为更可靠的基于联邦学习的模型市场奠定了基础。

Abstract: In the era of Artificial Intelligence (AI), marketplaces have become
essential platforms for facilitating the exchange of data products to foster
data sharing. Model transactions provide economic solutions in data
marketplaces that enhance data reusability and ensure the traceability of data
ownership. To establish trustworthy data marketplaces, Federated Learning (FL)
has emerged as a promising paradigm to enable collaborative learning across
siloed datasets while safeguarding data privacy. However, effective data
valuation and selection from heterogeneous sources in the FL setup remain key
challenges. This paper introduces a comprehensive framework centered on a
Wasserstein-based estimator tailored for FL. The estimator not only predicts
model performance across unseen data combinations but also reveals the
compatibility between data heterogeneity and FL aggregation algorithms. To
ensure privacy, we propose a distributed method to approximate Wasserstein
distance without requiring access to raw data. Furthermore, we demonstrate that
model performance can be reliably extrapolated under the neural scaling law,
enabling effective data selection without full-scale training. Extensive
experiments across diverse scenarios, such as label skew, mislabeled, and
unlabeled sources, show that our approach consistently identifies
high-performing data combinations, paving the way for more reliable FL-based
model marketplaces.

</details>


### [84] [BULL-ODE: Bullwhip Learning with Neural ODEs and Universal Differential Equations under Stochastic Demand](https://arxiv.org/abs/2509.18105)
*Nachiket N. Naik,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.LG

TL;DR: 研究随机需求下连续时间库存动态学习，对比NODE和UDE对牛鞭效应的预测，给出不同需求体制下利用结构信息的建议。


<details>
  <summary>Details</summary>
Motivation: 不清楚在不同需求体制下结构偏差对预测是帮助还是阻碍。

Method: 使用单级测试台，设置三种需求体制，对不同轨迹片段训练，评估库存、订单率和需求的多步预测。

Result: 在结构体制下UDE泛化性更好，重尾对数正态冲击下NODE更灵活，数据减少时趋势仍存在。

Conclusion: 轻尾或时间相关噪声时强化结构，极端事件主导时放松结构，为科学和工程系统混合建模提供指导。

Abstract: We study learning of continuous-time inventory dynamics under stochastic
demand and quantify when structure helps or hurts forecasting of the bullwhip
effect. BULL-ODE compares a fully learned Neural ODE (NODE) that models the
entire right-hand side against a physics-informed Universal Differential
Equation (UDE) that preserves conservation and order-up-to structure while
learning a small residual policy term. Classical supply chain models explain
the bullwhip through control/forecasting choices and information sharing, while
recent physics-informed and neural differential equation methods blend domain
constraints with learned components. It is unclear whether structural bias
helps or hinders forecasting under different demand regimes. We address this by
using a single-echelon testbed with three demand regimes - AR(1)
(autocorrelated), i.i.d. Gaussian, and heavy-tailed lognormal. Training is done
on varying fractions of each trajectory, followed by evaluation of multi-step
forecasts for inventory I, order rate O, and demand D. Across the structured
regimes, UDE consistently generalizes better: with 90% of the training horizon,
inventory RMSE drops from 4.92 (NODE) to 0.26 (UDE) under AR(1) and from 5.96
to 0.95 under Gaussian demand. Under heavy-tailed lognormal shocks, the
flexibility of NODE is better. These trends persist as train18 ing data
shrinks, with NODE exhibiting phase drift in extrapolation while UDE remains
stable but underreacts to rare spikes. Our results provide concrete guidance:
enforce structure when noise is light-tailed or temporally correlated; relax
structure when extreme events dominate. Beyond inventory control, the results
offer guidance for hybrid modeling in scientific and engineering systems:
enforce known structure when conservation laws and modest noise dominate, and
relax structure to capture extremes in settings where rare events drive
dynamics.

</details>


### [85] [Model-Based Transfer Learning for Real-Time Damage Assessment of Bridge Networks](https://arxiv.org/abs/2509.18106)
*Elisa Tomassini,Enrique García-Macías,Filippo Ubertini*

Main category: cs.LG

TL;DR: 研究提出基于神经网络替代模型的迁移学习方法用于桥梁结构评估，用两座桥的真实数据验证，结果显示对损伤敏感，可促进网络级智能监测。


<details>
  <summary>Details</summary>
Motivation: 永久监测系统数据增多，管理多结构需高效跟踪和比较长期行为，知识迁移在相似结构间很有必要。

Method: 提出基于神经网络替代模型的迁移学习方法，将训练好的模型应用于相似特征的桥梁，把迁移模型集成到贝叶斯推理框架进行损伤评估。

Result: 方法对损伤位置、严重程度和范围有高敏感性。

Conclusion: 该方法增强实时监测，实现跨结构知识迁移，促进智能监测策略和提高网络级恢复力。

Abstract: The growing use of permanent monitoring systems has increased data
availability, offering new opportunities for structural assessment but also
posing scalability challenges, especially across large bridge networks.
Managing multiple structures requires tracking and comparing long-term
behaviour efficiently. To address this, knowledge transfer between similar
structures becomes essential. This study proposes a model-based transfer
learning approach using neural network surrogate models, enabling a model
trained on one bridge to be adapted to another with similar characteristics.
These models capture shared damage mechanisms, supporting a scalable and
generalizable monitoring framework. The method was validated using real data
from two bridges. The transferred model was integrated into a Bayesian
inference framework for continuous damage assessment based on modal features
from monitoring data. Results showed high sensitivity to damage location,
severity, and extent. This approach enhances real-time monitoring and enables
cross-structure knowledge transfer, promoting smart monitoring strategies and
improved resilience at the network level.

</details>


### [86] [AdaMixT: Adaptive Weighted Mixture of Multi-Scale Expert Transformers for Time Series Forecasting](https://arxiv.org/abs/2509.18107)
*Huanyao Zhang,Jiaye Lin,Wentao Zhang,Haitao Yuan,Guoliang Li*

Main category: cs.LG

TL;DR: 提出AdaMixT架构用于多变量时间序列预测，在八个基准测试中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有多变量时间序列预测方法依赖单尺度补丁或缺乏多尺度特征融合机制，性能和泛化性受限。

Method: 提出AdaMixT架构，引入多种补丁，利用通用预训练模型和特定领域模型进行多尺度特征提取，通过门控网络动态分配权重实现自适应多尺度融合。

Result: 在八个广泛使用的基准测试中证明了AdaMixT在现实场景中的有效性。

Conclusion: AdaMixT架构能有效解决现有多变量时间序列预测方法的问题，实现更准确的预测。

Abstract: Multivariate time series forecasting involves predicting future values based
on historical observations. However, existing approaches primarily rely on
predefined single-scale patches or lack effective mechanisms for multi-scale
feature fusion. These limitations hinder them from fully capturing the complex
patterns inherent in time series, leading to constrained performance and
insufficient generalizability. To address these challenges, we propose a novel
architecture named Adaptive Weighted Mixture of Multi-Scale Expert Transformers
(AdaMixT). Specifically, AdaMixT introduces various patches and leverages both
General Pre-trained Models (GPM) and Domain-specific Models (DSM) for
multi-scale feature extraction. To accommodate the heterogeneity of temporal
features, AdaMixT incorporates a gating network that dynamically allocates
weights among different experts, enabling more accurate predictions through
adaptive multi-scale fusion. Comprehensive experiments on eight widely used
benchmarks, including Weather, Traffic, Electricity, ILI, and four ETT
datasets, consistently demonstrate the effectiveness of AdaMixT in real-world
scenarios.

</details>


### [87] [Solve it with EASE](https://arxiv.org/abs/2509.18108)
*Adam Viktorin,Tomas Kadavy,Jozef Kovac,Michal Pluhacek,Roman Senkerik*

Main category: cs.LG

TL;DR: 本文介绍了基于大语言模型的开源模块化框架EASE，用于迭代式算法解决方案生成。


<details>
  <summary>Details</summary>
Motivation: 为研究者和从业者提供一个可控制、透明且可扩展的平台，用于跨领域共同设计算法和生成式解决方案。

Method: 将生成、测试、分析和评估集成到可重复的反馈循环中，支持多LLM协作，抽象提示设计和模型管理的复杂性。

Result: 构建了EASE框架，提供了一个新的算法解决方案生成平台。

Conclusion: EASE为跨领域的算法和生成式解决方案的共同设计提供了有效途径。

Abstract: This paper presents EASE (Effortless Algorithmic Solution Evolution), an
open-source and fully modular framework for iterative algorithmic solution
generation leveraging large language models (LLMs). EASE integrates generation,
testing, analysis, and evaluation into a reproducible feedback loop, giving
users full control over error handling, analysis, and quality assessment. Its
architecture supports the orchestration of multiple LLMs in complementary
roles-such as generator, analyst, and evaluator. By abstracting the complexity
of prompt design and model management, EASE provides a transparent and
extensible platform for researchers and practitioners to co-design algorithms
and other generative solutions across diverse domains.

</details>


### [88] [Machine Learning-Based Classification of Vessel Types in Straits Using AIS Tracks](https://arxiv.org/abs/2509.18109)
*Jonatan Katz Nielsen*

Main category: cs.LG

TL;DR: 本文提出仅用AIS数据对移动船只分类的机器学习流程，分析丹麦海事局AIS数据，提取特征训练模型，树模型效果好，证明AIS轨迹轻量级特征可用于海峡船只类型实时分类。


<details>
  <summary>Details</summary>
Motivation: 准确识别AIS轨迹中的船只类型对安全监督和打击非法活动至关重要。

Method: 分析丹麦海事局AIS数据，处理数据后提取31个轨迹级特征，按MMSI分组划分训练/测试集和分层5折交叉验证，使用树模型。

Result: 树模型效果好，随机森林SMOTE达92.15%准确率，调优RF的ROC - AUC达0.9897，特征重要性分析找出关键特征，主要误差在货船和油轮间。

Conclusion: AIS轨迹的轻量级特征可实现海峡船只类型实时分类，还讨论了改进方法。

Abstract: Accurate recognition of vessel types from Automatic Identification System
(AIS) tracks is essential for safety oversight and combating illegal,
unreported, and unregulated (IUU) activity. This paper presents a strait-scale,
machine-learning pipeline that classifies moving vessels using only AIS data.
We analyze eight days of historical AIS from the Danish Maritime Authority
covering the Bornholm Strait in the Baltic Sea (January 22-30, 2025). After
forward/backward filling voyage records, removing kinematic and geospatial
outliers, and segmenting per-MMSI tracks while excluding stationary periods
($\ge 1$ h), we derive 31 trajectory-level features spanning kinematics (e.g.,
SOG statistics), temporal, geospatial (Haversine distances, spans), and
ship-shape attributes computed from AIS A/B/C/D reference points (length,
width, aspect ratio, bridge-position ratio). To avoid leakage, we perform
grouped train/test splits by MMSI and use stratified 5-fold cross-validation.
Across five classes (cargo, tanker, passenger, high-speed craft, fishing;
N=1{,}910 trajectories; test=382), tree-based models dominate: a Random Forest
with SMOTE attains 92.15% accuracy (macro-precision 94.11%, macro-recall
92.51%, macro-F1 93.27%) on the held-out test set, while a tuned RF reaches
one-vs-rest ROC-AUC up to 0.9897. Feature-importance analysis highlights the
bridge-position ratio and maximum SOG as the most discriminative signals;
principal errors occur between cargo and tanker, reflecting similar transit
behavior. We demonstrate operational value by backfilling missing ship types on
unseen data and discuss improvements such as DBSCAN based trip segmentation and
gradient-boosted ensembles to handle frequent-stop ferries and further lift
performance. The results show that lightweight features over AIS trajectories
enable real-time vessel type classification in straits.

</details>


### [89] [Localized PCA-Net Neural Operators for Scalable Solution Reconstruction of Elliptic PDEs](https://arxiv.org/abs/2509.18110)
*Mrigank Dhingra,Romit Maulik,Adil Rasheed,Omer San*

Main category: cs.LG

TL;DR: 提出基于补丁的PCA - Net框架解决PDEs，分析不同补丁方法，结果显示其降低计算复杂度且保持高精度。


<details>
  <summary>Details</summary>
Motivation: 应用PCA到高维解场有显著计算开销，需解决该问题。

Method: 提出基于补丁的PCA - Net框架，分解解场为小补丁，在各补丁内应用PCA，训练神经算子；研究两种补丁方法，探索两种细化方法。

Result: 基于补丁的PCA显著降低计算复杂度，保持高精度，端到端处理时间比全局PCA减少3.7到4倍。

Conclusion: 基于补丁的PCA是PDE系统中高效算子学习的有前景技术。

Abstract: Neural operator learning has emerged as a powerful approach for solving
partial differential equations (PDEs) in a data-driven manner. However,
applying principal component analysis (PCA) to high-dimensional solution fields
incurs significant computational overhead. To address this, we propose a
patch-based PCA-Net framework that decomposes the solution fields into smaller
patches, applies PCA within each patch, and trains a neural operator in the
reduced PCA space. We investigate two different patch-based approaches that
balance computational efficiency and reconstruction accuracy: (1)
local-to-global patch PCA, and (2) local-to-local patch PCA. The trade-off
between computational cost and accuracy is analyzed, highlighting the
advantages and limitations of each approach. Furthermore, within each approach,
we explore two refinements for the most computationally efficient method: (i)
introducing overlapping patches with a smoothing filter and (ii) employing a
two-step process with a convolutional neural network (CNN) for refinement. Our
results demonstrate that patch-based PCA significantly reduces computational
complexity while maintaining high accuracy, reducing end-to-end pipeline
processing time by a factor of 3.7 to 4 times compared to global PCA, thefore
making it a promising technique for efficient operator learning in PDE-based
systems.

</details>


### [90] [Individualized non-uniform quantization for vector search](https://arxiv.org/abs/2509.18471)
*Mariano Tepper,Ted Willke*

Main category: cs.LG

TL;DR: 提出新向量压缩技术NVQ，实验显示其准确性优于现有技术且计算成本低


<details>
  <summary>Details</summary>
Motivation: 高维嵌入向量尺寸大，给现代向量搜索技术带来内存和存储成本高的问题

Method: 使用新颖简约且计算高效的非线性构建非均匀向量量化器，为每个索引向量单独学习量化器

Result: NVQ在计算成本最小的情况下，准确性优于现有技术

Conclusion: NVQ是一种在高保真度下计算和空间效率都很高的向量压缩技术

Abstract: Embedding vectors are widely used for representing unstructured data and
searching through it for semantically similar items. However, the large size of
these vectors, due to their high-dimensionality, creates problems for modern
vector search techniques: retrieving large vectors from memory/storage is
expensive and their footprint is costly. In this work, we present NVQ
(non-uniform vector quantization), a new vector compression technique that is
computationally and spatially efficient in the high-fidelity regime. The core
in NVQ is to use novel parsimonious and computationally efficient
nonlinearities for building non-uniform vector quantizers. Critically, these
quantizers are \emph{individually} learned for each indexed vector. Our
experimental results show that NVQ exhibits improved accuracy compared to the
state of the art with a minimal computational cost.

</details>


### [91] [Prompt Optimization Meets Subspace Representation Learning for Few-shot Out-of-Distribution Detection](https://arxiv.org/abs/2509.18111)
*Faizul Rakib Sayem,Shahana Ibrahim*

Main category: cs.LG

TL;DR: 本文提出基于上下文优化的框架，结合子空间表示学习与提示调优来提升开放世界中AI系统的OOD检测能力，实验验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示学习的OOD方法仅依赖softmax概率，忽略了视觉语言模型特征嵌入的判别潜力，需要改进。

Method: 提出基于上下文优化（CoOp）的框架，将子空间表示学习与提示调优相结合，设计端到端学习准则。

Result: 在真实数据集上的实验表明该方法有效。

Conclusion: 所提方法能提高ID - OOD可分离性，确保强OOD检测性能和高ID分类准确率。

Abstract: The reliability of artificial intelligence (AI) systems in open-world
settings depends heavily on their ability to flag out-of-distribution (OOD)
inputs unseen during training. Recent advances in large-scale vision-language
models (VLMs) have enabled promising few-shot OOD detection frameworks using
only a handful of in-distribution (ID) samples. However, existing prompt
learning-based OOD methods rely solely on softmax probabilities, overlooking
the rich discriminative potential of the feature embeddings learned by VLMs
trained on millions of samples. To address this limitation, we propose a novel
context optimization (CoOp)-based framework that integrates subspace
representation learning with prompt tuning. Our approach improves ID-OOD
separability by projecting the ID features into a subspace spanned by prompt
vectors, while projecting ID-irrelevant features into an orthogonal null space.
To train such OOD detection framework, we design an easy-to-handle end-to-end
learning criterion that ensures strong OOD detection performance as well as
high ID classification accuracy. Experiments on real-world datasets showcase
the effectiveness of our approach.

</details>


### [92] [KM-GPT: An Automated Pipeline for Reconstructing Individual Patient Data from Kaplan-Meier Plots](https://arxiv.org/abs/2509.18141)
*Yao Zhao,Haoyue Sun,Yantian Ding,Yanxun Xu*

Main category: cs.LG

TL;DR: 提出AI驱动的KM - GPT自动化管道从KM图重建IPD，评估显示准确性高，应用于胃癌免疫疗法试验Meta分析助力决策。


<details>
  <summary>Details</summary>
Motivation: 现有从KM图重建IPD的方法依赖手动数字化，易出错且缺乏可扩展性。

Method: 开发KM - GPT，集成高级图像预处理、GPT - 5多模态推理和迭代重建算法，采用混合推理架构，配备用户友好界面和AI助手。

Result: 在合成和真实数据集上评估，KM - GPT准确性高；应用于胃癌免疫疗法试验Meta分析。

Conclusion: KM - GPT自动化传统手动流程，提供可扩展网络解决方案，利用重建IPD促进下游分析和循证决策。

Abstract: Reconstructing individual patient data (IPD) from Kaplan-Meier (KM) plots
provides valuable insights for evidence synthesis in clinical research.
However, existing approaches often rely on manual digitization, which is
error-prone and lacks scalability. To address these limitations, we develop
KM-GPT, the first fully automated, AI-powered pipeline for reconstructing IPD
directly from KM plots with high accuracy, robustness, and reproducibility.
KM-GPT integrates advanced image preprocessing, multi-modal reasoning powered
by GPT-5, and iterative reconstruction algorithms to generate high-quality IPD
without manual input or intervention. Its hybrid reasoning architecture
automates the conversion of unstructured information into structured data flows
and validates data extraction from complex KM plots. To improve accessibility,
KM-GPT is equipped with a user-friendly web interface and an integrated AI
assistant, enabling researchers to reconstruct IPD without requiring
programming expertise. KM-GPT was rigorously evaluated on synthetic and
real-world datasets, consistently demonstrating superior accuracy. To
illustrate its utility, we applied KM-GPT to a meta-analysis of gastric cancer
immunotherapy trials, reconstructing IPD to facilitate evidence synthesis and
biomarker-based subgroup analyses. By automating traditionally manual processes
and providing a scalable, web-based solution, KM-GPT transforms clinical
research by leveraging reconstructed IPD to enable more informed downstream
analyses, supporting evidence-based decision-making.

</details>


### [93] [Large language models surpass domain-specific architectures for antepartum electronic fetal monitoring analysis](https://arxiv.org/abs/2509.18112)
*Sheng Wong,Ravi Shankar,Beth Albert,Gabriel Davis Jones*

Main category: cs.LG

TL;DR: 本文首次全面比较了用于自动化产前CTG分析的AI方法，发现微调的大语言模型表现更优，为临床CTG解读提供新途径。


<details>
  <summary>Details</summary>
Motivation: 基础模型和大语言模型在医疗领域表现出色，但在电子胎儿监测/胎心监护图（EFM/CTG）分析中潜力未充分挖掘，且CTG评估依赖主观临床解读，诊断准确性有差异，需要更优方法。

Method: 系统比较时间序列基础模型和大语言模型与已有的CTG特定架构，评估超500份不同时长的CTG记录。

Result: 微调的大语言模型相比基础模型和特定领域方法表现更优。

Conclusion: 研究为胎儿监测应用中不同AI方法的优势提供关键见解，为产前护理的临床AI未来发展奠定基础。

Abstract: Foundation models (FMs) and large language models (LLMs) demonstrate
remarkable capabilities across diverse domains through training on massive
datasets. These models have demonstrated exceptional performance in healthcare
applications, yet their potential for electronic fetal monitoring
(EFM)/cardiotocography (CTG) analysis, a critical technology for evaluating
fetal well-being, remains largely underexplored. Antepartum CTG interpretation
presents unique challenges due to the complex nature of fetal heart rate (FHR)
patterns and uterine activity, requiring sophisticated analysis of long
time-series data. The assessment of CTG is heavily based on subjective clinical
interpretation, often leading to variability in diagnostic accuracy and
deviation from timely pregnancy care. This study presents the first
comprehensive comparison of state-of-the-art AI approaches for automated
antepartum CTG analysis. We systematically compare time-series FMs and LLMs
against established CTG-specific architectures. Our evaluation encompasses over
500 CTG recordings of varying durations reflecting real-world clinical
recordings, providing robust performance benchmarks across different modelling
paradigms. Our results demonstrate that fine-tuned LLMs achieve superior
performance compared to both foundation models and domain-specific approaches,
offering a promising alternative pathway for clinical CTG interpretation. These
findings provide critical insights into the relative strengths of different AI
methodologies for fetal monitoring applications and establish a foundation for
future clinical AI development in prenatal care.

</details>


### [94] [A Study of Skews, Imbalances, and Pathological Conditions in LLM Inference Deployment on GPU Clusters detectable from DPU](https://arxiv.org/abs/2509.18114)
*Javed I. Khan an Henry Uwabor Moye*

Main category: cs.LG

TL;DR: 大语言模型自回归推理在运行时效率上有挑战，蓝菲尔德 - 3 DPU 辅助框架可检测和缓解多节点张量并行推理中的负载不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型自回归推理在解码阶段因 GPU 分片负载不平衡导致的吞吐量下降和延迟峰值问题。

Method: 利用蓝菲尔德 - 3 DPU 辅助框架，将监控任务卸载到 DPU，分析 GPU 遥测和节点间通信模式，为推理控制器和调度器提供反馈。

Result: 未提及具体结果

Conclusion: 未提及具体结论

Abstract: Autoregressive inference in large transformer-based language models (LLMs)
presents significant challenges for runtime efficiency, particularly during the
decode phase where load imbalance across GPU shards can cause throughput
degradation and latency spikes. A DPU-assisted framework leveraged by
BlueField-3 Data Processing Units can enable real-time detection and mitigation
of load imbalance in multi-node tensor-parallel inference. By offloading
monitoring tasks to the DPU and analyzing GPU telemetry and inter-node
communication patterns, the resulting system can provide actionable feedback to
inference controllers and schedulers. The goal of this study is three-fold i)
identify the reported skews/imbalances/pathological conditions that arise in
muti-GPU execution of a) LLM tensor computing (both during training and
inference), b) identify their impact on computational performance, and c) make
a critical assessment if those can be tracked for potential mitigation from a
DPU network.

</details>


### [95] [FedFiTS: Fitness-Selected, Slotted Client Scheduling for Trustworthy Federated Learning in Healthcare AI](https://arxiv.org/abs/2509.19120)
*Ferdinand Kahenga,Antoine Bagula,Sajal K. Das,Patrick Sello*

Main category: cs.LG

TL;DR: 本文提出FedFiTS框架解决联邦学习敏感领域部署问题，理论分析有优势，实验显示其性能优于基线方法，适合现实部署。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在敏感领域部署面临非IID数据、客户端不可靠和对抗操纵等挑战。

Method: 引入FedFiTS框架，结合基于适应度的客户端选举和时隙聚合，采用三阶段参与策略，并有动态客户端评分、自适应阈值和基于队列的调度。

Result: 理论分析建立了收敛界限和降低通信复杂度；实验表明在多种数据集上，FedFiTS在准确性、达到目标时间和抗中毒攻击能力上优于FedAvg等基线方法。

Conclusion: FedFiTS结合信任感知聚合和公平导向的客户端选择，推动了可扩展和安全的联邦学习，适合现实医疗和跨领域部署。

Abstract: Federated Learning (FL) has emerged as a powerful paradigm for
privacy-preserving model training, yet deployments in sensitive domains such as
healthcare face persistent challenges from non-IID data, client unreliability,
and adversarial manipulation. This paper introduces FedFiTS, a trust and
fairness-aware selective FL framework that advances the FedFaSt line by
combining fitness-based client election with slotted aggregation. FedFiTS
implements a three-phase participation strategy-free-for-all training, natural
selection, and slotted team participation-augmented with dynamic client
scoring, adaptive thresholding, and cohort-based scheduling to balance
convergence efficiency with robustness. A theoretical convergence analysis
establishes bounds for both convex and non-convex objectives under standard
assumptions, while a communication-complexity analysis shows reductions
relative to FedAvg and other baselines. Experiments on diverse datasets-medical
imaging (X-ray pneumonia), vision benchmarks (MNIST, FMNIST), and tabular
agricultural data (Crop Recommendation)-demonstrate that FedFiTS consistently
outperforms FedAvg, FedRand, and FedPow in accuracy, time-to-target, and
resilience to poisoning attacks. By integrating trust-aware aggregation with
fairness-oriented client selection, FedFiTS advances scalable and secure FL,
making it well suited for real-world healthcare and cross-domain deployments.

</details>


### [96] [Towards Scalable and Structured Spatiotemporal Forecasting](https://arxiv.org/abs/2509.18115)
*Hongyi Chen,Xiucheng Li,Xinyang Chen,Jing Li,Kehai Chen,Liqiang Nie*

Main category: cs.LG

TL;DR: 提出用于时空预测的空间平衡注意力块，构建多尺度模型，在真实数据集上验证效果优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 在遵循空间邻近性和捕捉全局相关性之间取得平衡，进行更有效的时空预测。

Method: 将空间图划分为子图，使用子图内注意力学习局部空间相关性，通过子图间注意力实现子图间消息传递；基于此构建多尺度时空预测模型。

Result: 在中大型真实时空数据集上实验，相比基线方法在低运行成本下性能提升达7.7%。

Conclusion: 提出的模型可扩展、能产生结构化空间相关性且易于实现，性能表现良好。

Abstract: In this paper, we propose a novel Spatial Balance Attention block for
spatiotemporal forecasting. To strike a balance between obeying spatial
proximity and capturing global correlation, we partition the spatial graph into
a set of subgraphs and instantiate Intra-subgraph Attention to learn local
spatial correlation within each subgraph; to capture the global spatial
correlation, we further aggregate the nodes to produce subgraph representations
and achieve message passing among the subgraphs via Inter-subgraph Attention.
Building on the proposed Spatial Balance Attention block, we develop a
multiscale spatiotemporal forecasting model by progressively increasing the
subgraph scales. The resulting model is both scalable and able to produce
structured spatial correlation, and meanwhile, it is easy to implement. We
evaluate its efficacy and efficiency against the existing models on real-world
spatiotemporal datasets from medium to large sizes. The experimental results
show that it can achieve performance improvements up to 7.7% over the baseline
methods at low running costs.

</details>


### [97] [FedFusion: Federated Learning with Diversity- and Cluster-Aware Encoders for Robust Adaptation under Label Scarcity](https://arxiv.org/abs/2509.19220)
*Ferdinand Kahenga,Antoine Bagula,Patrick Sello,Sajal K. Das*

Main category: cs.LG

TL;DR: 提出FedFusion框架，结合领域适应和节俭标注，在多种基准测试中表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中客户端特征空间异构、数据非IID和标签稀缺问题。

Method: 采用多样性/聚类感知编码器，通过置信过滤伪标签和领域自适应迁移，结合节俭标注流程，使用相似性加权分类器耦合。

Result: 在表格和图像基准测试中，FedFusion在准确性、鲁棒性和公平性上始终优于现有基线，通信和计算预算相当。

Conclusion: 协调个性化、领域适应和标签效率是现实约束下实现稳健联邦学习的有效方法。

Abstract: Federated learning in practice must contend with heterogeneous feature
spaces, severe non-IID data, and scarce labels across clients. We present
FedFusion, a federated transfer-learning framework that unifies domain
adaptation and frugal labelling with diversity-/cluster-aware encoders (DivEn,
DivEn-mix, DivEn-c). Labelled teacher clients guide learner clients via
confidence-filtered pseudo-labels and domain-adaptive transfer, while clients
maintain personalised encoders tailored to local data. To preserve global
coherence under heterogeneity, FedFusion employs similarity-weighted classifier
coupling (with optional cluster-wise averaging), mitigating dominance by
data-rich sites and improving minority-client performance. The frugal-labelling
pipeline combines self-/semi-supervised pretext training with selective
fine-tuning, reducing annotation demands without sharing raw data. Across
tabular and imaging benchmarks under IID, non-IID, and label-scarce regimes,
FedFusion consistently outperforms state-of-the-art baselines in accuracy,
robustness, and fairness while maintaining comparable communication and
computation budgets. These results show that harmonising personalisation,
domain adaptation, and label efficiency is an effective recipe for robust
federated learning under real-world constraints.

</details>


### [98] [Amortized Latent Steering: Low-Cost Alternative to Test-Time Optimization](https://arxiv.org/abs/2509.18116)
*Nathan Egbuna,Saatvik Gaur,Sunishchal Dev,Ashwinee Panda,Maheep Chaudhary*

Main category: cs.LG

TL;DR: 提出Amortized Latent Steering (ALS)方法，可将迭代优化转化为单一离线计算向量，在推理时以恒定成本应用，在基准测试中实现加速并提升效率 - 准确率权衡。


<details>
  <summary>Details</summary>
Motivation: 现有测试时优化技术推理成本高，LatentSeek等方法仍需昂贵的逐查询优化循环。

Method: 计算成功与失败生成的隐藏状态之间的平均差异，用该方向校准模型的隐藏表示，当解码偏离成功流形时将激活拉回。

Result: 在GSM8K和MATH - 500基准测试中，比迭代方法实现2 - 5倍加速，匹配或超越贪婪思维链和自一致性基线，效率 - 准确率权衡提升达101%。

Conclusion: 潜在优化的大部分好处可离线获取，使复杂推理技术适用于生产部署。

Abstract: Test-time optimization remains impractical at scale due to prohibitive
inference costs\textemdash techniques like iterative refinement and multi-step
verification can require $10$--$100\times$ more compute per query than standard
decoding. Latent space test-time optimization methods like LatentSeek offer a
more direct approach by steering hidden representations, but still demand
expensive per-query optimization loops with multiple backward passes. We
propose Amortized Latent Steering (ALS), which collapses this iterative
optimization into a single offline-computed vector applied at constant cost
during inference. ALS computes the mean difference between hidden states from
successful versus unsuccessful generations, then uses this direction to
calibrate the model's hidden representations: when decoding drifts away from
the success manifold, ALS nudges activations back toward it. Across GSM8K and
MATH-$500$ benchmarks, ALS achieves $2$--$5\times$ speedup over iterative
methods while matching or surpassing greedy Chain-of-Thought (CoT) and
Self-Consistency baselines, yielding up to 101\% improvement in
efficiency--accuracy trade-off. These results show that much of latent
optimization's benefit can be captured offline, making sophisticated reasoning
techniques viable for production deployment. Code is available
at~\href{https://anonymous.4open.science/r/steering-17F2}{https://anonymous.4open.science/r/steering-17F2}

</details>


### [99] [Robust and continuous machine learning of usage habits to adapt digital interfaces to user needs](https://arxiv.org/abs/2509.18117)
*Eric Petit,Denis Chêne*

Main category: cs.LG

TL;DR: 本文提出用机器学习方法设计能动态适应不同用户和使用策略的数字界面，算法有效，为自适应系统发展铺路。


<details>
  <summary>Details</summary>
Motivation: 设计能动态适应不同用户和使用策略的数字界面，提升用户体验。

Method: 采用贝叶斯统计方法对用户浏览行为建模，运用在线增量学习，生成任务模型。

Result: 模拟显示该方法在静态和非静态环境中均有效。

Conclusion: 本研究为自适应系统发展奠定基础，有助于提升用户界面操作体验。

Abstract: The paper presents a machine learning approach to design digital interfaces
that can dynamically adapt to different users and usage strategies. The
algorithm uses Bayesian statistics to model users' browsing behavior, focusing
on their habits rather than group preferences. It is distinguished by its
online incremental learning, allowing reliable predictions even with little
data and in the case of a changing environment. This inference method generates
a task model, providing a graphical representation of navigation with the usage
statistics of the current user. The algorithm learns new tasks while preserving
prior knowledge. The theoretical framework is described, and simulations show
the effectiveness of the approach in stationary and non-stationary
environments. In conclusion, this research paves the way for adaptive systems
that improve the user experience by helping them to better navigate and act on
their interface.

</details>


### [100] [Fast Linear Solvers via AI-Tuned Markov Chain Monte Carlo-based Matrix Inversion](https://arxiv.org/abs/2509.18452)
*Anton Lebedev,Won Kyung Lee,Soumyadip Ghosh,Olha I. Yaman,Vassilis Kalantzis,Yingdong Lu,Tomasz Nowicki,Shashanka Ubaru,Lior Horesh,Vassil Alexandrov*

Main category: cs.LG

TL;DR: 提出AI驱动框架推荐MCMC参数用于线性系统预处理，比传统方法更高效。


<details>
  <summary>Details</summary>
Motivation: Krylov子空间求解器对病态矩阵收敛慢需预处理器，MCMC生成预处理器的参数优化困难，手动或网格搜索成本高。

Method: 用图神经替代模型根据矩阵A和MCMC参数预测预处理速度，用贝叶斯采集函数选择最可能使迭代次数最小的参数集。

Result: 在未见过的病态系统上，该框架用传统方法50%的搜索预算实现更好预处理，收敛迭代次数减少约10%。

Conclusion: 为将基于MCMC的预处理器纳入大规模系统提供了途径。

Abstract: Large, sparse linear systems are pervasive in modern science and engineering,
and Krylov subspace solvers are an established means of solving them. Yet
convergence can be slow for ill-conditioned matrices, so practical deployments
usually require preconditioners. Markov chain Monte Carlo (MCMC)-based matrix
inversion can generate such preconditioners and accelerate Krylov iterations,
but its effectiveness depends on parameters whose optima vary across matrices;
manual or grid search is costly. We present an AI-driven framework recommending
MCMC parameters for a given linear system. A graph neural surrogate predicts
preconditioning speed from $A$ and MCMC parameters. A Bayesian acquisition
function then chooses the parameter sets most likely to minimise iterations. On
a previously unseen ill-conditioned system, the framework achieves better
preconditioning with 50\% of the search budget of conventional methods,
yielding about a 10\% reduction in iterations to convergence. These results
suggest a route for incorporating MCMC-based preconditioners into large-scale
systems.

</details>


### [101] [Decentor-V: Lightweight ML Training on Low-Power RISC-V Edge Devices](https://arxiv.org/abs/2509.18118)
*Marcelo Ribeiro,Diogo Costa,Gonçalo Moreira,Sandro Pinto,Tiago Gomes*

Main category: cs.LG

TL;DR: 本文将L - SGD算法扩展到RISC - V MCU，评估了无FPU的影响，引入8位量化版本L - SGD，减少内存使用并加速训练。


<details>
  <summary>Details</summary>
Motivation: 现代物联网设备本地训练难，依赖云服务有隐私和连接问题，RISC - V架构缺乏设备端训练支持。

Method: 将L - SGD扩展到RISC - V MCU，用32位浮点运算评估，引入8位量化版本L - SGD。

Result: 8位量化L - SGD使内存使用减少近4倍，训练时间加速2.2倍，精度损失可忽略。

Conclusion: 8位量化L - SGD可有效解决RISC - V MCU设备端训练的性能问题。

Abstract: Modern IoT devices increasingly rely on machine learning solutions to process
data locally. However, the lack of graphics processing units (GPUs) or
dedicated accelerators on most platforms makes on-device training largely
infeasible, often requiring cloud-based services to perform this task. This
procedure often raises privacy-related concerns, and creates dependency on
reliable and always-on connectivity. Federated Learning (FL) is a new trend
that addresses these issues by enabling decentralized and collaborative
training directly on devices, but it requires highly efficient optimization
algorithms. L-SGD, a lightweight variant of stochastic gradient descent, has
enabled neural network training on Arm Cortex-M Microcontroller Units (MCUs).
This work extends L-SGD to RISC-V-based MCUs, an open and emerging architecture
that still lacks robust support for on-device training. L-SGD was evaluated on
both Arm and RISC-V platforms using 32-bit floating-point arithmetic,
highlighting the performance impact of the absence of Floating-Point Units
(FPUs) in RISC-V MCUs. To mitigate these limitations, we introduce an 8-bit
quantized version of L-SGD for RISC-V, which achieves nearly 4x reduction in
memory usage and a 2.2x speedup in training time, with negligible accuracy
degradation.

</details>


### [102] [Probabilistic Geometric Principal Component Analysis with application to neural data](https://arxiv.org/abs/2509.18469)
*Han-Lin Hsieh,Maryam M. Shanechi*

Main category: cs.LG

TL;DR: 提出PGPCA算法用于非线性流形数据降维，模拟和脑数据分析显示其优于PPCA，有多种优势。


<details>
  <summary>Details</summary>
Motivation: PPCA及扩展主要基于线性模型，只能描述欧几里得空间数据，而神经科学中数据常分布在非线性流形上，需要新的降维算法。

Method: 开发Probabilistic Geometric Principal Component Analysis (PGPCA)算法，推导几何坐标系，使用数据驱动的EM算法学习模型参数。

Result: 模拟和脑数据分析表明，PGPCA能有效建模数据在不同流形周围的分布，性能优于PPCA，可测试几何坐标系是否更适合描述数据，能对数据进行降维和学习分布。

Conclusion: PGPCA通过纳入非线性流形几何推广了PPCA，对分析具有噪声且分布在非线性流形周围的高维数据的降维有效。

Abstract: Dimensionality reduction is critical across various domains of science
including neuroscience. Probabilistic Principal Component Analysis (PPCA) is a
prominent dimensionality reduction method that provides a probabilistic
approach unlike the deterministic approach of PCA and serves as a connection
between PCA and Factor Analysis (FA). Despite their power, PPCA and its
extensions are mainly based on linear models and can only describe the data in
a Euclidean coordinate system. However, in many neuroscience applications, data
may be distributed around a nonlinear geometry (i.e., manifold) rather than
lying in the Euclidean space. We develop Probabilistic Geometric Principal
Component Analysis (PGPCA) for such datasets as a new dimensionality reduction
algorithm that can explicitly incorporate knowledge about a given nonlinear
manifold that is first fitted from these data. Further, we show how in addition
to the Euclidean coordinate system, a geometric coordinate system can be
derived for the manifold to capture the deviations of data from the manifold
and noise. We also derive a data-driven EM algorithm for learning the PGPCA
model parameters. As such, PGPCA generalizes PPCA to better describe data
distributions by incorporating a nonlinear manifold geometry. In simulations
and brain data analyses, we show that PGPCA can effectively model the data
distribution around various given manifolds and outperforms PPCA for such data.
Moreover, PGPCA provides the capability to test whether the new geometric
coordinate system better describes the data than the Euclidean one. Finally,
PGPCA can perform dimensionality reduction and learn the data distribution both
around and on the manifold. These capabilities make PGPCA valuable for
enhancing the efficacy of dimensionality reduction for analysis of
high-dimensional data that exhibit noise and are distributed around a nonlinear
manifold.

</details>


### [103] [MobileRL: Online Agentic Reinforcement Learning for Mobile GUI Agents](https://arxiv.org/abs/2509.18119)
*Yifan Xu,Xiao Liu,Xinghan Liu,Jiaqi Fu,Hanchen Zhang,Bohao Jing,Shudan Zhang,Yuting Wang,Wenyi Zhao,Yuxiao Dong*

Main category: cs.LG

TL;DR: 提出在线代理强化学习框架MOBILERL增强移动环境中GUI代理，应用于两开源模型取得SOTA结果，框架被采用并开源。


<details>
  <summary>Details</summary>
Motivation: 由于任务难度重尾分布和大规模环境采样低效，开发有效的移动GUI代理仍具挑战。

Method: 提出MOBILERL框架，核心是ADAGRPO算法，设计难度自适应正回放和失败课程过滤，引入最短路径奖励调整策略。

Result: MOBILERL - 9B模型在AndroidWorld和AndroidLab上成功率达75.8%和46.8%。

Conclusion: MOBILERL框架能稳定RL训练、提高样本效率，在不同移动应用和任务中有强性能。

Abstract: Building general-purpose graphical user interface (GUI) agents has become
increasingly promising with the progress in vision language models. However,
developing effective mobile GUI agents with reinforcement learning (RL) remains
challenging due to the heavy-tailed distribution of task difficulty and the
inefficiency of large-scale environment sampling. We present an online agentic
reinforcement learning framework MOBILERL to enhance GUI agents in mobile
environments. Its core component is the Difficulty-Adaptive GRPO (ADAGRPO)
algorithm. In ADAGRPO, we design difficulty-adaptive positive replay and
failure curriculum filtering to adapt the model to different task difficulties.
We introduce the shortest path reward adjustment strategy to reshape rewards
concerning the task length in multi-turn agentic tasks. Those strategies
jointly stabilize RL training, improve sample efficiency, and generate strong
performance across diverse mobile apps and tasks. We apply MOBILERL to two open
models (Qwen2.5-VL-7B-Instruct and GLM-4.1V-9B-Base). The resultant MOBILERL-9B
model achieves state-of-the-art results in terms of success rates on both
AndroidWorld (75.8%) and AndroidLab (46.8%). The MOBILERL framework is adopted
in the AutoGLM products, and also open-sourced at
https://github.com/THUDM/MobileRL.

</details>


### [104] [Prediction of Coffee Ratings Based On Influential Attributes Using SelectKBest and Optimal Hyperparameters](https://arxiv.org/abs/2509.18124)
*Edmund Agyemang,Lawrence Agbota,Vincent Agbenyeavu,Peggy Akabuah,Bismark Bimpong,Christopher Attafuah*

Main category: cs.LG

TL;DR: 研究应用监督机器学习算法，结合用户评论特征预测咖啡评级，发现集成方法和多层感知器表现更优，强调特征选择和超参数调整重要性。


<details>
  <summary>Details</summary>
Motivation: 探索用监督机器学习算法，结合用户评论的文本和数值属性来预测咖啡评级。

Method: 进行数据预处理，包括文本清理、用TF - IDF提取特征、用SelectKBest选择特征；训练六个模型并使用优化超参数；用F1分数、Gmean和AUC评估模型性能。

Result: 集成方法（Extra Trees、Random Forest和XGBoost）以及多层感知器在F1分数、G - mean和AUC等评估指标上始终优于简单分类器（决策树和K近邻）。

Conclusion: 严格的特征选择和超参数调整对构建感官产品评估的预测系统至关重要，提供了一种数据驱动方法来补充专业咖啡品鉴。

Abstract: This study explores the application of supervised machine learning algorithms
to predict coffee ratings based on a combination of influential textual and
numerical attributes extracted from user reviews. Through careful data
preprocessing including text cleaning, feature extraction using TF-IDF, and
selection with SelectKBest, the study identifies key factors contributing to
coffee quality assessments. Six models (Decision Tree, KNearest Neighbors,
Multi-layer Perceptron, Random Forest, Extra Trees, and XGBoost) were trained
and evaluated using optimized hyperparameters. Model performance was assessed
primarily using F1-score, Gmean, and AUC metrics. Results demonstrate that
ensemble methods (Extra Trees, Random Forest, and XGBoost), as well as
Multi-layer Perceptron, consistently outperform simpler classifiers (Decision
Trees and K-Nearest Neighbors) in terms of evaluation metrics such as F1
scores, G-mean and AUC. The findings highlight the essence of rigorous feature
selection and hyperparameter tuning in building robust predictive systems for
sensory product evaluation, offering a data driven approach to complement
traditional coffee cupping by expertise of trained professionals.

</details>


### [105] [Diagonal Linear Networks and the Lasso Regularization Path](https://arxiv.org/abs/2509.18766)
*Raphaël Berthier*

Main category: cs.LG

TL;DR: 本文研究对角线性网络，表明其训练轨迹与套索正则化路径密切相关，训练时间相当于逆正则化参数，给出严格结果和模拟。


<details>
  <summary>Details</summary>
Motivation: 深入分析对角线性网络隐含正则化，探究其训练轨迹与套索正则化路径的关系。

Method: 理论分析和模拟。

Result: 在套索正则化路径单调性假设下，两者联系精确；一般情况下为近似联系。

Conclusion: 对角线性网络的完整训练轨迹与套索正则化路径密切相关，训练时间可视为逆正则化参数。

Abstract: Diagonal linear networks are neural networks with linear activation and
diagonal weight matrices. Their theoretical interest is that their implicit
regularization can be rigorously analyzed: from a small initialization, the
training of diagonal linear networks converges to the linear predictor with
minimal 1-norm among minimizers of the training loss. In this paper, we deepen
this analysis showing that the full training trajectory of diagonal linear
networks is closely related to the lasso regularization path. In this
connection, the training time plays the role of an inverse regularization
parameter. Both rigorous results and simulations are provided to illustrate
this conclusion. Under a monotonicity assumption on the lasso regularization
path, the connection is exact while in the general case, we show an approximate
connection.

</details>


### [106] [NurseSchedRL: Attention-Guided Reinforcement Learning for Nurse-Patient Assignment](https://arxiv.org/abs/2509.18125)
*Harsha Koduri*

Main category: cs.LG

TL;DR: 提出NurseSchedRL强化学习框架用于护士-患者分配，在模拟中比基线方法有更好效果，凸显强化学习在医疗劳动力管理决策支持中的潜力。


<details>
  <summary>Details</summary>
Motivation: 医疗系统需有效分配有限护理资源，传统调度方法难以应对动态多约束环境。

Method: 提出NurseSchedRL框架，集成结构化状态编码、受限动作掩码和基于注意力的技能、疲劳和地理上下文表示，使用带可行性掩码的近端策略优化（PPO）。

Result: 在模拟中，NurseSchedRL比基线启发式和无约束强化学习方法提高了调度效率，更好匹配技能与患者需求，减少疲劳。

Conclusion: 强化学习在复杂、高风险的医疗劳动力管理决策支持中具有潜力。

Abstract: Healthcare systems face increasing pressure to allocate limited nursing
resources efficiently while accounting for skill heterogeneity, patient acuity,
staff fatigue, and continuity of care. Traditional optimization and heuristic
scheduling methods struggle to capture these dynamic, multi-constraint
environments. I propose NurseSchedRL, a reinforcement learning framework for
nurse-patient assignment that integrates structured state encoding, constrained
action masking, and attention-based representations of skills, fatigue, and
geographical context. NurseSchedRL uses Proximal Policy Optimization (PPO) with
feasibility masks to ensure assignments respect real-world constraints, while
dynamically adapting to patient arrivals and varying nurse availability. In
simulation with realistic nurse and patient data, NurseSchedRL achieves
improved scheduling efficiency, better alignment of skills to patient needs,
and reduced fatigue compared to baseline heuristic and unconstrained RL
approaches. These results highlight the potential of reinforcement learning for
decision support in complex, high-stakes healthcare workforce management.

</details>


### [107] [Central Limit Theorems for Asynchronous Averaged Q-Learning](https://arxiv.org/abs/2509.18964)
*Xingtu Liu*

Main category: cs.LG

TL;DR: 本文建立异步更新下Polyak - Ruppert平均Q学习的中心极限定理，包括非渐近中心极限定理和泛函中心极限定理。


<details>
  <summary>Details</summary>
Motivation: 研究异步更新下Polyak - Ruppert平均Q学习的中心极限定理，以揭示其收敛特性。

Method: 推导非渐近中心极限定理和泛函中心极限定理。

Result: 得到非渐近中心极限定理，其在Wasserstein距离下的收敛率明确反映了对迭代次数、状态 - 动作空间大小、折扣因子和探索质量的依赖；得到泛函中心极限定理，表明部分和过程弱收敛到布朗运动。

Conclusion: 成功建立异步更新下Polyak - Ruppert平均Q学习的中心极限定理。

Abstract: This paper establishes central limit theorems for Polyak-Ruppert averaged
Q-learning under asynchronous updates. We present a non-asymptotic central
limit theorem, where the convergence rate in Wasserstein distance explicitly
reflects the dependence on the number of iterations, state-action space size,
the discount factor, and the quality of exploration. In addition, we derive a
functional central limit theorem, showing that the partial-sum process
converges weakly to a Brownian motion.

</details>


### [108] [Anomaly Detection in Electric Vehicle Charging Stations Using Federated Learning](https://arxiv.org/abs/2509.18126)
*Bishal K C,Amr Hilal,Pawan Thapa*

Main category: cs.LG

TL;DR: 文章评估了联邦学习在电动汽车充电站异常检测中的性能，对比FedAvg和FedAvgM，发现FedAvgM在异构环境表现更好，表明联邦学习能应对异构性，FedAvgM是有前景的解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着电动汽车基础设施的快速扩张，保障基于物联网的充电站免受网络威胁至关重要，而当前基于联邦学习的入侵检测系统评估忽略了系统异构性和非独立同分布数据等实际挑战。

Method: 使用广泛研究的优化方法FedAvg和FedAvgM，在系统和数据异构的情况下，对电动汽车充电站的异常检测性能进行实验评估。

Result: 在独立同分布设置下，FedAvg性能优于使用相同神经网络的集中式模型；在非独立同分布数据和系统异构情况下，性能下降；FedAvgM在异构设置中始终优于FedAvg，收敛性更好，异常检测准确率更高。

Conclusion: 联邦学习可以在基于物联网的电动汽车充电站中处理异构性而不会显著降低性能，FedAvgM是一种有前景的、用于保障电动汽车充电站安全的隐私保护解决方案。

Abstract: Federated Learning (FL) is a decentralized training framework widely used in
IoT ecosystems that preserves privacy by keeping raw data local, making it
ideal for IoT-enabled cyber-physical systems with sensing and communication
like Smart Grids (SGs), Connected and Automated Vehicles (CAV), and Electric
Vehicle Charging Stations (EVCS). With the rapid expansion of electric vehicle
infrastructure, securing these IoT-based charging stations against cyber
threats has become critical. Centralized Intrusion Detection Systems (IDS)
raise privacy concerns due to sensitive network and user data, making FL a
promising alternative. However, current FL-based IDS evaluations overlook
practical challenges such as system heterogeneity and non-IID data. To address
these challenges, we conducted experiments to evaluate the performance of
federated learning for anomaly detection in EV charging stations under system
and data heterogeneity. We used FedAvg and FedAvgM, widely studied optimization
approaches, to analyze their effectiveness in anomaly detection. Under IID
settings, FedAvg achieves superior performance to centralized models using the
same neural network. However, performance degrades with non-IID data and system
heterogeneity. FedAvgM consistently outperforms FedAvg in heterogeneous
settings, showing better convergence and higher anomaly detection accuracy. Our
results demonstrate that FL can handle heterogeneity in IoT-based EVCS without
significant performance loss, with FedAvgM as a promising solution for robust,
privacy-preserving EVCS security.

</details>


### [109] [Safe-SAIL: Towards a Fine-grained Safety Landscape of Large Language Models via Sparse Autoencoder Interpretation Framework](https://arxiv.org/abs/2509.18127)
*Jiaqi Weng,Han Zheng,Hanyu Zhang,Qinqin He,Jialing Tao,Hui Xue,Zhixuan Chu,Xiting Wang*

Main category: cs.LG

TL;DR: 本文提出Safe - SAIL框架，用于解释大语言模型中稀疏自编码器特征，推进安全领域的机理理解，并将发布综合工具包促进大模型安全研究。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型安全研究存在局限性，且以往稀疏自编码器应用未用细粒度安全概念解释特征，难以解决关键安全行为问题，需提取丰富多样的安全相关特征。

Method: 提出Safe - SAIL框架，系统识别具有最佳概念特定可解释性的稀疏自编码器，解释安全相关神经元，并引入高效策略扩大解释过程。

Result: 将发布包含稀疏自编码器检查点和神经元解释的综合工具包，支持安全风险的实证分析。

Conclusion: Safe - SAIL框架有助于推进大语言模型在安全领域的机理理解，促进大语言模型安全研究。

Abstract: Increasing deployment of large language models (LLMs) in real-world
applications raises significant safety concerns. Most existing safety research
focuses on evaluating LLM outputs or specific safety tasks, limiting their
ability to ad- dress broader, undefined risks. Sparse Autoencoders (SAEs)
facilitate interpretability research to clarify model behavior by explaining
single-meaning atomic features decomposed from entangled signals. jHowever,
prior applications on SAEs do not interpret features with fine-grained
safety-related con- cepts, thus inadequately addressing safety-critical
behaviors, such as generating toxic responses and violating safety regu-
lations. For rigorous safety analysis, we must extract a rich and diverse set
of safety-relevant features that effectively capture these high-risk behaviors,
yet face two challenges: identifying SAEs with the greatest potential for
generating safety concept-specific neurons, and the prohibitively high cost of
detailed feature explanation. In this paper, we pro- pose Safe-SAIL, a
framework for interpreting SAE features within LLMs to advance mechanistic
understanding in safety domains. Our approach systematically identifies SAE
with best concept-specific interpretability, explains safety-related neurons,
and introduces efficient strategies to scale up the in- terpretation process.
We will release a comprehensive toolkit including SAE checkpoints and
human-readable neuron ex- planations, which supports empirical analysis of
safety risks to promote research on LLM safety.

</details>


### [110] [DRO-REBEL: Distributionally Robust Relative-Reward Regression for Fast and Efficient LLM Alignment](https://arxiv.org/abs/2509.19104)
*Sharan Sahu,Martin T. Wells*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Reinforcement learning with human feedback (RLHF) has become crucial for
aligning Large Language Models (LLMs) with human intent. However, existing
offline RLHF approaches suffer from overoptimization, where models overfit to
reward misspecification and drift from preferred behaviors observed during
training. We introduce DRO-REBEL, a unified family of robust REBEL updates with
type-$p$ Wasserstein, KL, and $\chi^2$ ambiguity sets. Using Fenchel duality,
each update reduces to a simple relative-reward regression, preserving
scalability and avoiding PPO-style clipping or auxiliary value networks. Under
standard linear-reward and log-linear policy classes with a data-coverage
condition, we establish $O(n^{-1/4})$ estimation bounds with tighter constants
than prior DRO-DPO approaches, and recover the minimax-optimal $O(n^{-1/2})$
rate via a localized Rademacher complexity analysis. The same analysis closes
the gap for Wasserstein-DPO and KL-DPO, showing both also attain optimal
parametric rates. We derive practical SGD algorithms for all three divergences:
gradient regularization (Wasserstein), importance weighting (KL), and a fast
1-D dual solve ($\chi^2$). Experiments on Emotion Alignment, the large-scale
ArmoRM multi-objective benchmark, and HH-Alignment demonstrate strong
worst-case robustness across unseen preference mixtures, model sizes, and data
scales, with $\chi^2$-REBEL showing consistently strong empirical performance.
A controlled radius--coverage study validates a no-free-lunch trade-off: radii
shrinking faster than empirical divergence concentration rates achieve
minimax-optimal parametric rates but forfeit coverage, while
coverage-guaranteeing radii incur $O(n^{-1/4})$ rates.

</details>


### [111] [Accounting for Uncertainty in Machine Learning Surrogates: A Gauss-Hermite Quadrature Approach to Reliability Analysis](https://arxiv.org/abs/2509.18128)
*Amirreza Tootchi,Xiaoping Du*

Main category: cs.LG

TL;DR: 本文提出高斯 - 埃尔米特求积法用于物理可靠性分析，可解耦嵌套不确定性，算例显示方法高效且结果更可信。


<details>
  <summary>Details</summary>
Motivation: 机器学习替代模型用于可靠性分析时引入认知不确定性，与模型输入的偶然不确定性耦合，影响可靠性预测准确性。

Method: 采用高斯 - 埃尔米特求积法，用一阶和二阶可靠性方法评估偶然不确定性下的条件失效概率，再对认知不确定性的实现进行积分。

Result: 三个算例表明，该方法保持了计算效率，比忽略模型不确定性的传统方法给出更可信的预测。

Conclusion: 所提方法可解耦嵌套不确定性，实现更准确的可靠性分析。

Abstract: Machine learning surrogates are increasingly employed to replace expensive
computational models for physics-based reliability analysis. However, their use
introduces epistemic uncertainty from model approximation errors, which couples
with aleatory uncertainty in model inputs, potentially compromising the
accuracy of reliability predictions. This study proposes a Gauss-Hermite
quadrature approach to decouple these nested uncertainties and enable more
accurate reliability analysis. The method evaluates conditional failure
probabilities under aleatory uncertainty using First and Second Order
Reliability Methods and then integrates these probabilities across realizations
of epistemic uncertainty. Three examples demonstrate that the proposed approach
maintains computational efficiency while yielding more trustworthy predictions
than traditional methods that ignore model uncertainty.

</details>


### [112] [Unveiling the Role of Learning Rate Schedules via Functional Scaling Laws](https://arxiv.org/abs/2509.19189)
*Binghui Li,Fengling Chen,Zixun Huang,Lean Wang,Lei Wu*

Main category: cs.LG

TL;DR: 本文提出功能缩放定律（FSL）研究大语言模型预训练损失动态，分析三种学习率调度，为实践提供理论依据并探索其实际应用。


<details>
  <summary>Details</summary>
Motivation: 现有缩放定律研究多关注最终步损失，忽略训练过程损失动态和学习率调度的影响，本文旨在填补这一空白。

Method: 通过在线随机梯度下降训练师生核回归设置，利用新的内在时间观点和随机微分方程建模，引入FSL。

Result: 分析三种学习率调度，为大语言模型预训练的经验实践提供理论依据，探索FSL作为替代模型的实际应用。

Conclusion: FSL框架可加深对大语言模型预训练动态的理解，为改进大规模模型训练提供见解。

Abstract: Scaling laws have played a cornerstone role in guiding the training of large
language models (LLMs). However, most existing works on scaling laws primarily
focus on the final-step loss, overlooking the loss dynamics during the training
process and, crucially, the impact of learning rate schedule (LRS). In this
paper, we aim to bridge this gap by studying a teacher-student kernel
regression setup trained via online stochastic gradient descent (SGD).
Leveraging a novel intrinsic time viewpoint and stochastic differential
equation (SDE) modeling of SGD, we introduce the Functional Scaling Law (FSL),
which characterizes the evolution of population risk during the training
process for general LRSs. Remarkably, the impact of the LRSs is captured
through an explicit convolution-type functional term, making their effects
fully tractable. To illustrate the utility of FSL, we analyze three widely used
LRSs -- constant, exponential decay, and warmup-stable-decay (WSD) -- under
both data-limited and compute-limited regimes. We provide theoretical
justification for widely adopted empirical practices in LLMs pre-training such
as (i) higher-capacity models are more data- and compute-efficient; (ii)
learning rate decay can improve training efficiency; (iii) WSD-like schedules
can outperform direct-decay schedules. Lastly, we explore the practical
relevance of FSL as a surrogate model for fitting, predicting and optimizing
the loss curves in LLM pre-training, with experiments conducted across model
sizes ranging from 0.1B to 1B parameters. We hope our FSL framework can deepen
the understanding of LLM pre-training dynamics and provide insights for
improving large-scale model training.

</details>


### [113] [Research on Metro Transportation Flow Prediction Based on the STL-GRU Combined Model](https://arxiv.org/abs/2509.18130)
*Zijie Zhou,Huichen Ma*

Main category: cs.LG

TL;DR: 本文提出 STL - GRU 组合预测模型用于地铁换乘客流预测，经实验验证该模型比其他模型预测精度更高。


<details>
  <summary>Details</summary>
Motivation: 进一步完善地铁内部换乘客流预测理论，为智能运营决策提供更可靠支持。

Method: 先基于 Keras 构建和训练 GRU 模型，对原始地铁刷卡数据预处理，用图的深度优先搜索算法识别出行路径并构建换乘客流时间序列，采用 STL 算法分解时间序列，用 3σ 原则处理残差分量中的异常值，最后完成预测。

Result: 以某地铁站换乘客流数据验证模型有效性，与 LSTM、GRU 和 STL - LSTM 组合模型相比，STL - GRU 组合预测模型在工作日（除周五）、周五和休息日的换乘客流预测精度显著提高，预测结果的平均绝对百分比误差（MAPE）分别至少降低 2.3、1.36 和 6.42 个百分点。

Conclusion: STL - GRU 组合预测模型能有效提高地铁换乘客流预测精度。

Abstract: In the metro intelligent transportation system, accurate transfer passenger
flow prediction is a key link in optimizing operation plans and improving
transportation efficiency. To further improve the theory of metro internal
transfer passenger flow prediction and provide more reliable support for
intelligent operation decisions, this paper innovatively proposes a metro
transfer passenger flow prediction model that integrates the Seasonal and Trend
decomposition using Loess (STL) method and Gated Recurrent Unit (GRU).In
practical application, the model first relies on the deep learning library
Keras to complete the construction and training of the GRU model, laying the
foundation for subsequent prediction; then preprocesses the original metro card
swiping data, uses the graph-based depth-first search algorithm to identify
passengers' travel paths, and further constructs the transfer passenger flow
time series; subsequently adopts the STL time series decomposition algorithm to
decompose the constructed transfer passenger flow time series into trend
component, periodic component and residual component, and uses the 3{\sigma}
principle to eliminate and fill the outliers in the residual component, and
finally completes the transfer passenger flow prediction.Taking the transfer
passenger flow data of a certain metro station as the research sample, the
validity of the model is verified. The results show that compared with Long
Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), and the combined model of
STL time series decomposition method and Long Short-Term Memory (STL-LSTM), the
STL-GRU combined prediction model significantly improves the prediction
accuracy of transfer passenger flow on weekdays (excluding Fridays), Fridays
and rest days, with the mean absolute percentage error (MAPE) of the prediction
results reduced by at least 2.3, 1.36 and 6.42 percentage points respectively.

</details>


### [114] [Two ways to knowledge?](https://arxiv.org/abs/2509.18131)
*Jean-Michel Tucny,Abhisek Ganguly,Santosh Ansumali,Sauro Succi*

Main category: cs.LG

TL;DR: 研究指出基于Transformer的机器学习应用于物理问题时权重矩阵呈随机特征，表明机器学习与科学方法或互补，且解释性存问题。


<details>
  <summary>Details</summary>
Motivation: 探索基于Transformer的机器学习在物理应用中权重矩阵特征，以及机器学习与科学方法的关系和解释性问题。

Method: 分析基于Transformer的机器学习在两个代表性物理应用中的权重矩阵特征，类比Transformer操作与路径积分技术。

Result: 权重矩阵有随机特征，与物理问题结构无直接可识别联系；类比可解释权重随机特征，但未解决解释性问题。

Conclusion: 提出不借助洞察力获取知识存在风险的一般性看法。

Abstract: It is shown that the weight matrices of transformer-based machine learning
applications to the solution of two representative physical applications show a
random-like character which bears no directly recognizable link to the physical
and mathematical structure of the physical problem under study. This suggests
that machine learning and the scientific method may represent two distinct and
potentially complementary paths to knowledge, even though a strict notion of
explainability in terms of direct correspondence between network parameters and
physical structures may remain out of reach. It is also observed that drawing a
parallel between transformer operation and (generalized) path-integration
techniques may account for the random-like nature of the weights, but still
does not resolve the tension with explainability. We conclude with some general
comments on the hazards of gleaning knowledge without the benefit of Insight.

</details>


### [115] [Self-Evolving LLMs via Continual Instruction Tuning](https://arxiv.org/abs/2509.18133)
*Le Huang,Jiazheng Kang,Cheng Hou,Zhe Zhao,Zhenxiang Yan,Chuan Shi,Ting Bai*

Main category: cs.LG

TL;DR: 提出MoE - CL框架用于大语言模型持续指令调优，经实验验证有效，在实际应用中降低人工审核成本。


<details>
  <summary>Details</summary>
Motivation: 现实工业场景中，大语言模型需持续学习，但现有持续学习方法存在灾难性遗忘问题。

Method: 提出MoE - CL框架，采用双专家设计，通过任务感知鉴别器防止共享路径传输无关噪声，利用对抗学习平衡知识保留和跨任务泛化。

Result: 在公共MTL5基准和工业Tencent3基准上验证有效性，在腾讯视频平台内容合规审查A/B测试中降低15.3%人工审核成本。

Conclusion: MoE - CL适用于持续适应和稳定迁移至关重要的大规模工业部署。

Abstract: In real-world industrial settings, large language models (LLMs) must learn
continually to keep pace with diverse and evolving tasks, requiring
self-evolution to refine knowledge under dynamic data distributions. However,
existing continual learning (CL) approaches, such as replay and parameter
isolation, often suffer from catastrophic forgetting: training on new tasks
degrades performance on earlier ones by overfitting to the new distribution and
weakening generalization.We propose MoE-CL, a parameter-efficient adversarial
mixture-of-experts framework for industrial-scale, self-evolving continual
instruction tuning of LLMs. MoE-CL uses a dual-expert design: (1) a dedicated
LoRA expert per task to preserve task-specific knowledge via parameter
independence, mitigating forgetting; and (2) a shared LoRA expert to enable
cross-task transfer. To prevent transferring task-irrelevant noise through the
shared pathway, we integrate a task-aware discriminator within a GAN. The
discriminator encourages the shared expert to pass only task-aligned
information during sequential training. Through adversarial learning, the
shared expert acquires generalized representations that mimic the
discriminator, while dedicated experts retain task-specific details, balancing
knowledge retention and cross-task generalization and thereby supporting
self-evolution.Extensive experiments on the public MTL5 benchmark and an
industrial Tencent3 benchmark validate the effectiveness of MoE-CL for
continual instruction tuning. In real-world A/B testing for content compliance
review on the Tencent Video platform, MoE-CL reduced manual review costs by
15.3%. These results demonstrate that MoE-CL is practical for large-scale
industrial deployment where continual adaptation and stable transfer are
critical.

</details>


### [116] [A Weighted Gradient Tracking Privacy-Preserving Method for Distributed Optimization](https://arxiv.org/abs/2509.18134)
*Furan Xie,Bing Liu,Li Chai*

Main category: cs.LG

TL;DR: 本文研究隐私保护分布式优化问题，揭示梯度跟踪隐私风险，提出加权梯度跟踪算法，分析收敛性并通过数值模拟验证有效性。


<details>
  <summary>Details</summary>
Motivation: 保护优化过程中代理的私有信息不被潜在攻击者获取。

Method: 揭示梯度跟踪隐私风险，提出加权梯度跟踪分布式隐私保护算法，用衰减权重因子消除风险，分析不同步长下算法收敛性。

Result: 证明算法在温和假设下能精确收敛到最优解，数值模拟验证算法有效性。

Conclusion: 所提加权梯度跟踪分布式隐私保护算法能解决隐私保护分布式优化问题，有良好收敛性和有效性。

Abstract: This paper investigates the privacy-preserving distributed optimization
problem, aiming to protect agents' private information from potential attackers
during the optimization process. Gradient tracking, an advanced technique for
improving the convergence rate in distributed optimization, has been applied to
most first-order algorithms in recent years. We first reveal the inherent
privacy leakage risk associated with gradient tracking. Building upon this
insight, we propose a weighted gradient tracking distributed privacy-preserving
algorithm, eliminating the privacy leakage risk in gradient tracking using
decaying weight factors. Then, we characterize the convergence of the proposed
algorithm under time-varying heterogeneous step sizes. We prove the proposed
algorithm converges precisely to the optimal solution under mild assumptions.
Finally, numerical simulations validate the algorithm's effectiveness through a
classical distributed estimation problem and the distributed training of a
convolutional neural network.

</details>


### [117] [SDGF: Fusing Static and Multi-Scale Dynamic Correlations for Multivariate Time Series Forecasting](https://arxiv.org/abs/2509.18135)
*Shaoxun Wang,Xingjun Zhang,Qianyang Li,Jiawei Cao,Zhendong Tan*

Main category: cs.LG

TL;DR: 提出SDGF网络解决多变量时间序列预测中多尺度依赖关系建模难题，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有方法在建模多变量时间序列多尺度依赖关系方面存在局限，难以捕捉其复杂动态特征。

Method: 提出SDGF网络，采用双路径图结构学习方法，利用基于先验知识的静态图锚定长期稳定依赖，用多级小波分解构建动态图捕捉不同尺度关联，设计注意力门控模块融合信息，用多内核扩张卷积网络加深对时间模式的理解。

Result: 在多个真实世界基准数据集上的综合实验证明了模型的有效性。

Conclusion: 提出的SDGF网络能有效解决多变量时间序列预测中的多尺度依赖关系建模问题。

Abstract: Inter-series correlations are crucial for accurate multivariate time series
forecasting, yet these relationships often exhibit complex dynamics across
different temporal scales. Existing methods are limited in modeling these
multi-scale dependencies and struggle to capture their intricate and evolving
nature. To address this challenge, this paper proposes a novel Static-Dynamic
Graph Fusion network (SDGF), whose core lies in capturing multi-scale
inter-series correlations through a dual-path graph structure learning
approach. Specifically, the model utilizes a static graph based on prior
knowledge to anchor long-term, stable dependencies, while concurrently
employing Multi-level Wavelet Decomposition to extract multi-scale features for
constructing an adaptively learned dynamic graph to capture associations at
different scales. We design an attention-gated module to fuse these two
complementary sources of information intelligently, and a multi-kernel dilated
convolutional network is then used to deepen the understanding of temporal
patterns. Comprehensive experiments on multiple widely used real-world
benchmark datasets demonstrate the effectiveness of our proposed model.

</details>


### [118] [From Parameters to Performance: A Data-Driven Study on LLM Structure and Development](https://arxiv.org/abs/2509.18136)
*Suqing Wang,Zuchao Li,Luohe Shi,Bo Du,Hai Zhao,Yun Li,Qianren Wang*

Main category: cs.LG

TL;DR: 本文构建大规模数据集研究大语言模型结构配置与性能关系，分析结构选择对性能的影响，提供数据驱动的优化见解并将发布数据集。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在规模和能力快速增长的情况下，缺乏系统、数据驱动的关于结构配置对性能影响的研究。

Method: 构建包含多样开源大语言模型结构及其多基准测试性能的大规模数据集，进行数据挖掘驱动的分析，结合历史发展回顾、未来趋势探索和机械可解释性技术。

Result: 分析出各种结构选择对基准测试性能的影响。

Conclusion: 为大语言模型优化提供数据驱动的见解，以指导未来模型的针对性开发和应用。

Abstract: Large language models (LLMs) have achieved remarkable success across various
domains, driving significant technological advancements and innovations.
Despite the rapid growth in model scale and capability, systematic, data-driven
research on how structural configurations affect performance remains scarce. To
address this gap, we present a large-scale dataset encompassing diverse
open-source LLM structures and their performance across multiple benchmarks.
Leveraging this dataset, we conduct a systematic, data mining-driven analysis
to validate and quantify the relationship between structural configurations and
performance. Our study begins with a review of the historical development of
LLMs and an exploration of potential future trends. We then analyze how various
structural choices impact performance across benchmarks and further corroborate
our findings using mechanistic interpretability techniques. By providing
data-driven insights into LLM optimization, our work aims to guide the targeted
development and application of future models. We will release our dataset at
https://huggingface.co/datasets/DX0369/LLM-Structure-Performance-Dataset

</details>


### [119] [LoRALib: A Standardized Benchmark for Evaluating LoRA-MoE Methods](https://arxiv.org/abs/2509.18137)
*Shaoheng Wang,Yao Lu,Yuqi Li,Yaxin Gao,Jiaqi Nie,Shanqing Yu,Yingli Tian,Qi Xuan*

Main category: cs.LG

TL;DR: 提出统一基准LoRALib评估LoRA - MoE方法，实验表明LoRAMoE最佳，优先选相关LoRA可提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA - MoE方法在模型、数据集等方面缺乏统一标准，难以公平比较不同方法。

Method: 提出统一基准LoRALib，将40个下游任务数据集标准化，用相同超参微调获680个LoRA模块，用OpenCompass实验。

Result: LoRAMoE表现最佳，优先选与目标任务相关的LoRA可提升MoE性能。

Conclusion: 希望研究结果能为后续工作提供启发，公开数据集和LoRA库。

Abstract: As a parameter efficient fine-tuning (PEFT) method, low-rank adaptation
(LoRA) can save significant costs in storage and computing, but its strong
adaptability to a single task is often accompanied by insufficient cross-task
generalization capabilities. To improve this, existing work combines LoRA with
mixture-of-experts (MoE) to enhance the model's adaptability through expert
modules and routing mechanisms. However, existing LoRA-MoE methods lack unified
standards in models, datasets, hyperparameters, and evaluation methods, making
it difficult to conduct fair comparisons between different methods. To this
end, we proposed a unified benchmark named LoRALib. Specifically, we
standardized datasets from $40$ downstream tasks into a unified format,
fine-tuned them using the same hyperparameters and obtained $680$ LoRA modules
across $17$ model architectures. Based on this LoRA library, we conduct
large-scale experiments on $3$ representative LoRA-MoE methods and different
LoRA selection mechanisms using the open-sourced testing tool OpenCompass.
Extensive experiments show that LoRAMoE performs best, and that prioritizing
LoRAs relevant to the target task can further improve the performance of MoE.
We hope these findings will inspire future work. Our datasets and LoRA library
are available at https://huggingface.co/datasets/YaoLuzjut/LoRAOcean_dataset
and https://huggingface.co/YaoLuzjut/models.

</details>


### [120] [Rank-Induced PL Mirror Descent: A Rank-Faithful Second-Order Algorithm for Sleeping Experts](https://arxiv.org/abs/2509.18138)
*Tiantian Zhang*

Main category: cs.LG

TL;DR: 提出新算法RIPLM，利用结构等价性，在rank - induced PL参数化中更新，是睡眠专家设置下首个兼具rank - faithful和variance - adaptive的算法。


<details>
  <summary>Details</summary>
Motivation: 设计在睡眠专家设置下更优的算法，同时具备rank - faithful和variance - adaptive特性。

Method: 利用rank benchmark和distributional benchmark的结构等价性，在rank - induced PL参数化中直接更新。

Result: 得到了新算法RIPLM。

Conclusion: RIPLM是睡眠专家设置下首个兼具rank - faithful和variance - adaptive的算法。

Abstract: We introduce a new algorithm, \emph{Rank-Induced Plackett--Luce Mirror
Descent (RIPLM)}, which leverages the structural equivalence between the
\emph{rank benchmark} and the \emph{distributional benchmark} established in
\citet{BergamOzcanHsu2022}. Unlike prior approaches that operate on expert
identities, RIPLM updates directly in the \emph{rank-induced Plackett--Luce
(PL)} parameterization. This ensures that the algorithm's played distributions
remain within the class of rank-induced distributions at every round,
preserving the equivalence with the rank benchmark. To our knowledge, RIPLM is
the first algorithm that is both (i) \emph{rank-faithful} and (ii)
\emph{variance-adaptive} in the sleeping experts setting.

</details>


### [121] [Comparative Analysis of FOLD-SE vs. FOLD-R++ in Binary Classification and XGBoost in Multi-Category Classification](https://arxiv.org/abs/2509.18139)
*Akshay Murthy,Shawn Sebastian,Manil Shangle,Huaduo Wang,Sopam Dasgupta,Gopal Gupta*

Main category: cs.LG

TL;DR: 本文对比了基于规则的分类器FOLD - SE和FOLD - R++在二分类中的表现，以及FOLD - SE与XGBoost在多分类中的表现，结果表明FOLD - SE在两类任务中表现出色，能兼顾可解释性与性能。


<details>
  <summary>Details</summary>
Motivation: 满足对能平衡准确性、效率和可解释性的机器学习模型的需求，对比不同分类器性能。

Method: 使用分类数据集，以准确率、F1分数和处理时间作为主要性能指标进行研究。

Result: 在二分类中FOLD - SE规则更少，与FOLD - R++相比精度和处理效率略低；在多分类中FOLD - SE比XGBoost更精确、高效且规则可理解。

Conclusion: 基于规则的方法如FOLD - SE可弥合可解释性和性能之间的差距，是各类分类任务中黑盒模型的可行替代方案。

Abstract: Recently, the demand for Machine Learning (ML) models that can balance
accuracy, efficiency, and interpreability has grown significantly.
Traditionally, there has been a tradeoff between accuracy and explainability in
predictive models, with models such as Neural Networks achieving high accuracy
on complex datasets while sacrificing internal transparency. As such, new
rule-based algorithms such as FOLD-SE have been developed that provide tangible
justification for predictions in the form of interpretable rule sets. The
primary objective of this study was to compare FOLD-SE and FOLD-R++, both
rule-based classifiers, in binary classification and evaluate how FOLD-SE
performs against XGBoost, a widely used ensemble classifier, when applied to
multi-category classification. We hypothesized that because FOLD-SE can
generate a condensed rule set in a more explainable manner, it would lose
upwards of an average of 3 percent in accuracy and F1 score when compared with
XGBoost and FOLD-R++ in multiclass and binary classification, respectively. The
research used data collections for classification, with accuracy, F1 scores,
and processing time as the primary performance measures. Outcomes show that
FOLD-SE is superior to FOLD-R++ in terms of binary classification by offering
fewer rules but losing a minor percentage of accuracy and efficiency in
processing time; in tasks that involve multi-category classifications, FOLD-SE
is more precise and far more efficient compared to XGBoost, in addition to
generating a comprehensible rule set. The results point out that FOLD-SE is a
better choice for both binary tasks and classifications with multiple
categories. Therefore, these results demonstrate that rule-based approaches
like FOLD-SE can bridge the gap between explainability and performance,
highlighting their potential as viable alternatives to black-box models in
diverse classification tasks.

</details>


### [122] [A Machine Learning Framework for Pathway-Driven Therapeutic Target Discovery in Metabolic Disorders](https://arxiv.org/abs/2509.18140)
*Iram Wajahat,Amritpal Singh,Fazel Keshtkar,Syed Ahmad Chan Bukhari*

Main category: cs.LG

TL;DR: 本文提出结合预测建模与基因无关途径映射的机器学习框架，用于识别2型糖尿病高危人群和发现治疗靶点，提供可解释且可扩展的精准医疗解决方案。


<details>
  <summary>Details</summary>
Motivation: 代谢紊乱尤其是2型糖尿病给全球带来重大健康负担，影响像皮马印第安人这样的遗传易感人群，需要有效方法进行早期检测和干预。

Method: 使用皮马印第安人数据集，通过逻辑回归和t检验确定关键预测因子；开发途径映射策略将预测因子与关键信号网络联系起来；提出治疗策略并通过途径富集分析验证。

Result: 模型整体准确率达78.43%，确定了与胰岛素信号、AMPK和PPAR途径相关的预测因子，提出如双重GLP - 1/GIP受体激动剂等治疗策略。

Conclusion: 该框架推动了精准医学发展，为代谢紊乱的早期检测和靶向干预提供可解释和可扩展的解决方案，有三项关键贡献。

Abstract: Metabolic disorders, particularly type 2 diabetes mellitus (T2DM), represent
a significant global health burden, disproportionately impacting genetically
predisposed populations such as the Pima Indians (a Native American tribe from
south central Arizona). This study introduces a novel machine learning (ML)
framework that integrates predictive modeling with gene-agnostic pathway
mapping to identify high-risk individuals and uncover potential therapeutic
targets. Using the Pima Indian dataset, logistic regression and t-tests were
applied to identify key predictors of T2DM, yielding an overall model accuracy
of 78.43%. To bridge predictive analytics with biological relevance, we
developed a pathway mapping strategy that links identified predictors to
critical signaling networks, including insulin signaling, AMPK, and PPAR
pathways. This approach provides mechanistic insights without requiring direct
molecular data. Building upon these connections, we propose therapeutic
strategies such as dual GLP-1/GIP receptor agonists, AMPK activators, SIRT1
modulators, and phytochemical, further validated through pathway enrichment
analyses. Overall, this framework advances precision medicine by offering
interpretable and scalable solutions for early detection and targeted
intervention in metabolic disorders. The key contributions of this work are:
(1) development of an ML framework combining logistic regression and principal
component analysis (PCA) for T2DM risk prediction; (2) introduction of a
gene-agnostic pathway mapping approach to generate mechanistic insights; and
(3) identification of novel therapeutic strategies tailored for high-risk
populations.

</details>


### [123] [AdaSTI: Conditional Diffusion Models with Adaptive Dependency Modeling for Spatio-Temporal Imputation](https://arxiv.org/abs/2509.18144)
*Yubo Yang,Yichen Zhu,Bo Jiang*

Main category: cs.LG

TL;DR: 提出基于条件扩散模型的时空数据插补方法AdaSTI，实验表明其优于现有方法，插补误差最多降低46.4%。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在提取和利用时空依赖作为条件信息时存在误差累积，且忽略不同扩散步骤中噪声数据依赖的可变性。

Method: 提出基于双向S4模型的BiS4PI网络进行预插补，用Spatio - Temporal Conditionalizer (STC)网络提取条件信息；提出带门控注意力机制的Noise - Aware Spatio - Temporal (NAST)网络捕捉不同扩散步骤的可变依赖。

Result: 在三个真实数据集上的大量实验显示，AdaSTI在所有设置下均优于现有方法，插补误差最多降低46.4%。

Conclusion: AdaSTI是一种有效的时空数据插补方法，在性能上超越了现有方法。

Abstract: Spatio-temporal data abounds in domain like traffic and environmental
monitoring. However, it often suffers from missing values due to sensor
malfunctions, transmission failures, etc. Recent years have seen continued
efforts to improve spatio-temporal data imputation performance. Recently
diffusion models have outperformed other approaches in various tasks, including
spatio-temporal imputation, showing competitive performance. Extracting and
utilizing spatio-temporal dependencies as conditional information is vital in
diffusion-based methods. However, previous methods introduce error accumulation
in this process and ignore the variability of the dependencies in the noisy
data at different diffusion steps. In this paper, we propose AdaSTI (Adaptive
Dependency Model in Diffusion-based Spatio-Temporal Imputation), a novel
spatio-temporal imputation approach based on conditional diffusion model.
Inside AdaSTI, we propose a BiS4PI network based on a bi-directional S4 model
for pre-imputation with the imputed result used to extract conditional
information by our designed Spatio-Temporal Conditionalizer (STC)network. We
also propose a Noise-Aware Spatio-Temporal (NAST) network with a gated
attention mechanism to capture the variant dependencies across diffusion steps.
Extensive experiments on three real-world datasets show that AdaSTI outperforms
existing methods in all the settings, with up to 46.4% reduction in imputation
error.

</details>


### [124] [Early Prediction of Multi-Label Care Escalation Triggers in the Intensive Care Unit Using Electronic Health Records](https://arxiv.org/abs/2509.18145)
*Syed Ahmad Chan Bukhari,Amritpal Singh,Shifath Hossain,Iram Wajahat*

Main category: cs.LG

TL;DR: 本文提出多标签分类框架预测ICU患者护理升级触发因素，用MIMIC - IV数据库训练评估模型，XGBoost表现最佳，框架有早期临床预警潜力。


<details>
  <summary>Details</summary>
Motivation: 传统早期预警系统聚焦单一结果，无法捕捉临床衰退的多维度特征，需要新方法预测ICU患者护理升级触发因素。

Method: 提出多标签分类框架，用MIMIC - IV数据库，定义24 - 72小时护理升级触发因素，提取前24小时特征，训练评估多种分类模型，用多种指标评估。

Result: XGBoost模型表现最佳，呼吸、血流动力学、肾脏和神经恶化的F1分数分别为0.66、0.72、0.76和0.62，优于基线模型。

Conclusion: 该框架有早期、可解释的临床预警实用潜力，无需复杂时间序列建模或自然语言处理。

Abstract: Intensive Care Unit (ICU) patients often present with complex, overlapping
signs of physiological deterioration that require timely escalation of care.
Traditional early warning systems, such as SOFA or MEWS, are limited by their
focus on single outcomes and fail to capture the multi-dimensional nature of
clinical decline. This study proposes a multi-label classification framework to
predict Care Escalation Triggers (CETs), including respiratory failure,
hemodynamic instability, renal compromise, and neurological deterioration,
using the first 24 hours of ICU data. Using the MIMIC-IV database, CETs are
defined through rule-based criteria applied to data from hours 24 to 72 (for
example, oxygen saturation below 90, mean arterial pressure below 65 mmHg,
creatinine increase greater than 0.3 mg/dL, or a drop in Glasgow Coma Scale
score greater than 2). Features are extracted from the first 24 hours and
include vital sign aggregates, laboratory values, and static demographics. We
train and evaluate multiple classification models on a cohort of 85,242 ICU
stays (80 percent training: 68,193; 20 percent testing: 17,049). Evaluation
metrics include per-label precision, recall, F1-score, and Hamming loss.
XGBoost, the best performing model, achieves F1-scores of 0.66 for respiratory,
0.72 for hemodynamic, 0.76 for renal, and 0.62 for neurologic deterioration,
outperforming baseline models. Feature analysis shows that clinically relevant
parameters such as respiratory rate, blood pressure, and creatinine are the
most influential predictors, consistent with the clinical definitions of the
CETs. The proposed framework demonstrates practical potential for early,
interpretable clinical alerts without requiring complex time-series modeling or
natural language processing.

</details>


### [125] [ConceptFlow: Hierarchical and Fine-grained Concept-Based Explanation for Convolutional Neural Networks](https://arxiv.org/abs/2509.18147)
*Xinyu Mu,Hui Dou,Furao Shen,Jian Zhao*

Main category: cs.LG

TL;DR: 提出ConceptFlow框架解决CNN概念可解释性现有方法的局限，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于概念的CNN可解释性方法忽视单个滤波器语义角色和概念跨层动态传播，需改进。

Method: 提出ConceptFlow框架，包含概念注意力和概念路径两个关键组件。

Result: 实验表明ConceptFlow能产生有语义意义的模型推理见解，验证组件有效性。

Conclusion: ConceptFlow可深入了解CNN内部逻辑，支持生成更可信和符合人类认知的解释。

Abstract: Concept-based interpretability for Convolutional Neural Networks (CNNs) aims
to align internal model representations with high-level semantic concepts, but
existing approaches largely overlook the semantic roles of individual filters
and the dynamic propagation of concepts across layers. To address these
limitations, we propose ConceptFlow, a concept-based interpretability framework
that simulates the internal "thinking path" of a model by tracing how concepts
emerge and evolve across layers. ConceptFlow comprises two key components: (i)
concept attentions, which associate each filter with relevant high-level
concepts to enable localized semantic interpretation, and (ii) conceptual
pathways, derived from a concept transition matrix that quantifies how concepts
propagate and transform between filters. Together, these components offer a
unified and structured view of internal model reasoning. Experimental results
demonstrate that ConceptFlow yields semantically meaningful insights into model
reasoning, validating the effectiveness of concept attentions and conceptual
pathways in explaining decision behavior. By modeling hierarchical conceptual
pathways, ConceptFlow provides deeper insight into the internal logic of CNNs
and supports the generation of more faithful and human-aligned explanations.

</details>


### [126] [Sparse Training Scheme for Multimodal LLM](https://arxiv.org/abs/2509.18150)
*Kean Shi,Liang Chen,Haozhe Zhao,Baobao Chang*

Main category: cs.LG

TL;DR: 提出基于稀疏表示的训练高效框架STS解决MLLM训练效率低问题，在多基准测试中有效。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型（MLLMs）因多模态数据输入序列长和层间计算利用率低导致的训练效率低问题。

Method: 提出Sparse Training Scheme (STS)框架，包含Visual Token Compressor压缩视觉标记、Layer Dynamic Skipper动态跳过不必要层。

Result: 该方法广泛适用于多种MLLM架构，在多个基准测试中展示了有效性和效率。

Conclusion: STS框架能有效提高MLLMs的训练效率。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated outstanding
performance across a variety of domains. However, training MLLMs is often
inefficient due to the significantly longer input sequences introduced by
multimodal data and the low utilization of inter-layer computations. To address
this challenge, we shift the focus to the training process itself and propose a
novel training-efficient framework based on sparse representations, termed the
Sparse Training Scheme (STS). This scheme consists of two key components: the
Visual Token Compressor, which reduces the information load by compressing
visual tokens, and the Layer Dynamic Skipper, which mitigates the computational
overhead by dynamically skipping unnecessary layers in the language model
during both forward and backward passes. Our approach is broadly applicable to
diverse MLLM architectures and has been extensively evaluated on multiple
benchmarks, demonstrating its effectiveness and efficiency.

</details>


### [127] [HyperNAS: Enhancing Architecture Representation for NAS Predictor via Hypernetwork](https://arxiv.org/abs/2509.18151)
*Jindi Lv,Yuhao Zhou,Yuxin Tian,Qing Ye,Wentao Feng,Jiancheng Lv*

Main category: cs.LG

TL;DR: 提出HyperNAS增强架构表示学习，在多搜索空间实验展现优势，少样本下达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有神经预测器因难以捕捉架构间复杂关系，泛化能力差，影响NAS性能评估效率。

Method: 提出HyperNAS，含全局编码方案和共享超网络，开发动态自适应多任务损失。

Result: 在五个代表性搜索空间实验，如在CIFAR - 10和ImageNet上少样本下达新SOTA。

Conclusion: HyperNAS有效，尤其在少样本场景有优势。

Abstract: Time-intensive performance evaluations significantly impede progress in
Neural Architecture Search (NAS). To address this, neural predictors leverage
surrogate models trained on proxy datasets, allowing for direct performance
predictions for new architectures. However, these predictors often exhibit poor
generalization due to their limited ability to capture intricate relationships
among various architectures. In this paper, we propose HyperNAS, a novel neural
predictor paradigm for enhancing architecture representation learning. HyperNAS
consists of two primary components: a global encoding scheme and a shared
hypernetwork. The global encoding scheme is devised to capture the
comprehensive macro-structure information, while the shared hypernetwork serves
as an auxiliary task to enhance the investigation of inter-architecture
patterns. To ensure training stability, we further develop a dynamic adaptive
multi-task loss to facilitate personalized exploration on the Pareto front.
Extensive experiments across five representative search spaces, including ViTs,
demonstrate the advantages of HyperNAS, particularly in few-shot scenarios. For
instance, HyperNAS strikes new state-of-the-art results, with 97.60\% top-1
accuracy on CIFAR-10 and 82.4\% top-1 accuracy on ImageNet, using at least
5.0$\times$ fewer samples.

</details>


### [128] [WLFM: A Well-Logs Foundation Model for Multi-Task and Cross-Well Geological Interpretation](https://arxiv.org/abs/2509.18152)
*Zhenyu Qi,Qing Yu,Jichen Wang,Yun-Bo Zhao,Zerui Li,Wenjun Lv*

Main category: cs.LG

TL;DR: 提出基础模型WLFM用于测井解释，该模型在多曲线测井数据上预训练，在孔隙度估计和岩性分类上表现优异，具有可扩展性、可解释性和可迁移性。


<details>
  <summary>Details</summary>
Motivation: 测井解释面临工具响应异质性、信号噪声和标签有限等挑战。

Method: 提出WLFM模型，包括将测井补丁标记化、自监督预训练和多任务自适应微调三个阶段。

Result: WLFM在孔隙度估计和岩性分类上优于现有基线，WLFM - Finetune进一步提升性能，模型展现出层意识、学习到可复用地质词汇、能合理重建掩码曲线。

Conclusion: WLFM可作为地质AI的可扩展、可解释和可迁移的骨干模型，对多模态数据集成有意义。

Abstract: Well-log interpretation is fundamental for subsurface characterization but
remains challenged by heterogeneous tool responses, noisy signals, and limited
labels. We propose WLFM, a foundation model pretrained on multi-curve logs from
1200 wells, comprising three stages: tokenization of log patches into
geological tokens, self-supervised pretraining with masked-token modeling and
stratigraphy-aware contrastive learning, and multi-task adaptation with
few-shot fine-tuning. WLFM consistently outperforms state-of-the-art baselines,
achieving 0.0041 MSE in porosity estimation and 74.13\% accuracy in lithology
classification, while WLFM-Finetune further improves to 0.0038 MSE and 78.10\%
accuracy. Beyond predictive accuracy, WLFM exhibits emergent layer-awareness,
learns a reusable geological vocabulary, and reconstructs masked curves with
reasonable fidelity, though systematic offsets are observed in shallow and
ultra-deep intervals. Although boundary detection is not explicitly evaluated
here, clustering analyses suggest strong potential for future extension. These
results establish WLFM as a scalable, interpretable, and transferable backbone
for geological AI, with implications for multi-modal integration of logs,
seismic, and textual data.

</details>


### [129] [A deep reinforcement learning platform for antibiotic discovery](https://arxiv.org/abs/2509.18153)
*Hanqun Cao,Marcelo D. T. Torres,Jingjie Zhang,Zijun Gao,Fang Wu,Chunbin Gu,Jure Leskovec,Yejin Choi,Cesar de la Fuente-Nunez,Guangyong Chen,Pheng-Ann Heng*

Main category: cs.LG

TL;DR: 提出ApexAmphion深度学习框架用于从头设计抗生素，体外评估显示设计肽表现良好，该方法能快速生成候选抗生素。


<details>
  <summary>Details</summary>
Motivation: 到2050年抗菌耐药性预计每年导致多达1000万人死亡，急需新抗生素。

Method: 将64亿参数的蛋白质语言模型与强化学习相结合，先在精选肽数据上微调模型，再用近端策略优化针对复合奖励进行优化。

Result: 对100个设计肽的体外评估显示所有候选肽MIC值低（部分为纳摩尔级），100%命中率，100个中有99个对至少两种临床相关细菌有广谱抗菌活性。

Conclusion: 该方法将生成、评分和多目标优化统一，能快速产生多样、有效的候选抗生素，为肽类抗生素开发提供可扩展途径和平台。

Abstract: Antimicrobial resistance (AMR) is projected to cause up to 10 million deaths
annually by 2050, underscoring the urgent need for new antibiotics. Here we
present ApexAmphion, a deep-learning framework for de novo design of
antibiotics that couples a 6.4-billion-parameter protein language model with
reinforcement learning. The model is first fine-tuned on curated peptide data
to capture antimicrobial sequence regularities, then optimised with proximal
policy optimization against a composite reward that combines predictions from a
learned minimum inhibitory concentration (MIC) classifier with differentiable
physicochemical objectives. In vitro evaluation of 100 designed peptides showed
low MIC values (nanomolar range in some cases) for all candidates (100% hit
rate). Moreover, 99 our of 100 compounds exhibited broad-spectrum antimicrobial
activity against at least two clinically relevant bacteria. The lead molecules
killed bacteria primarily by potently targeting the cytoplasmic membrane. By
unifying generation, scoring and multi-objective optimization with deep
reinforcement learning in a single pipeline, our approach rapidly produces
diverse, potent candidates, offering a scalable route to peptide antibiotics
and a platform for iterative steering toward potency and developability within
hours.

</details>


### [130] [MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and Training Recipe](https://arxiv.org/abs/2509.18154)
*Tianyu Yu,Zefan Wang,Chongyi Wang,Fuwei Huang,Wenshuo Ma,Zhihui He,Tianchi Cai,Weize Chen,Yuxiang Huang,Yuanqian Zhao,Bokai Xu,Junbo Cui,Yingjing Xu,Liqing Ruan,Luoyuan Zhang,Hanyu Liu,Jingkun Tang,Hongyuan Liu,Qining Guo,Wenhao Hu,Bingxiang He,Jie Zhou,Jie Cai,Ji Qi,Zonghao Guo,Chi Chen,Guoyang Zeng,Yuxuan Li,Ganqu Cui,Ning Ding,Xu Han,Yuan Yao,Zhiyuan Liu,Maosong Sun*

Main category: cs.LG

TL;DR: 介绍MiniCPM-V 4.5模型，通过三方面改进实现高效与高性能，实验超多种模型。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型训练和推理效率瓶颈问题，使模型更易访问和扩展。

Method: 在模型架构、数据策略和训练方法引入三项改进，包括统一3D-Resampler架构、统一学习范式和混合强化学习策略。

Result: OpenCompass评估表明MiniCPM-V 4.5超过GPT - 4o - latest等专有模型和Qwen2.5 - VL 72B等开源大模型；在VideoMME基准测试中在30B以下模型中达最优，显著节省GPU内存和推理时间。

Conclusion: MiniCPM-V 4.5能在保证强性能的同时实现高效率。

Abstract: Multimodal Large Language Models (MLLMs) are undergoing rapid progress and
represent the frontier of AI development. However, their training and inference
efficiency have emerged as a core bottleneck in making MLLMs more accessible
and scalable. To address the challenges, we present MiniCPM-V 4.5, an 8B
parameter model designed for high efficiency and strong performance. We
introduce three core improvements in model architecture, data strategy and
training method: a unified 3D-Resampler model architecture for highly compact
encoding over images and videos, a unified learning paradigm for document
knowledge and text recognition without heavy data engineering, and a hybrid
reinforcement learning strategy for proficiency in both short and long
reasoning modes. Comprehensive experimental results in OpenCompass evaluation
show that MiniCPM-V 4.5 surpasses widely used proprietary models such as
GPT-4o-latest, and significantly larger open-source models such as Qwen2.5-VL
72B. Notably, the strong performance is achieved with remarkable efficiency.
For example, on the widely adopted VideoMME benchmark, MiniCPM-V 4.5 achieves
state-of-the-art performance among models under 30B size, using just 46.7\% GPU
memory cost and 8.7\% inference time of Qwen2.5-VL 7B.

</details>


### [131] [Developing Training Procedures for Piecewise-linear Spline Activation Functions in Neural Networks](https://arxiv.org/abs/2509.18161)
*William H Patty*

Main category: cs.LG

TL;DR: 本文提出并比较9种训练方法，探索参数化线性B样条激活函数在神经网络中的双重优化动态，实验显示比传统ReLU模型降低错误率，但增加复杂度和延迟。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络激活函数多为经验验证的静态函数，通过优化激活函数形状可使模型更高效准确。

Method: 提出并比较9种训练方法，使用参数化线性B样条激活函数探索神经网络双重优化动态。

Result: 在FNNs中模型最终错误率最多降低94%，在CNNs中最多降低51%。

Conclusion: 优化激活函数形状能降低错误率，但会带来额外开发、训练复杂度和模型延迟。

Abstract: Activation functions in neural networks are typically selected from a set of
empirically validated, commonly used static functions such as ReLU, tanh, or
sigmoid. However, by optimizing the shapes of a network's activation functions,
we can train models that are more parameter-efficient and accurate by assigning
more optimal activations to the neurons. In this paper, I present and compare 9
training methodologies to explore dual-optimization dynamics in neural networks
with parameterized linear B-spline activation functions. The experiments
realize up to 94% lower end model error rates in FNNs and 51% lower rates in
CNNs compared to traditional ReLU-based models. These gains come at the cost of
additional development and training complexity as well as end model latency.

</details>


### [132] [A Simple and Reproducible Hybrid Solver for a Truck-Drone VRP with Recharge](https://arxiv.org/abs/2509.18162)
*Meraryslan Meraliyev,Cemil Turan,Shirali Kadyrov*

Main category: cs.LG

TL;DR: 研究单卡车单无人机的末英里配送，提出混合强化学习求解器，在特定实例上表现优于ALNS且接近NN，能平衡卡车等待以最小化总完成时间，还提供配置优先实现。


<details>
  <summary>Details</summary>
Motivation: 研究在明确电池管理下，单卡车和单无人机的末英里配送问题。

Method: 引入混合强化学习求解器，将基于ALNS的卡车路线与小指针/注意力策略结合，使用精确时间线模拟器。

Result: 在特定欧几里得实例上，平均完工时间优于ALNS 2.73%，与NN相差0.10%，RL调度器表现良好。

Conclusion: 所提出的方法能平衡卡车等待，最小化总完成时间，还提供工具支持复现。

Abstract: We study last-mile delivery with one truck and one drone under explicit
battery management: the drone flies at twice the truck speed; each sortie must
satisfy an endurance budget; after every delivery the drone recharges on the
truck before the next launch. We introduce a hybrid reinforcement learning (RL)
solver that couples an ALNS-based truck tour (with 2/3-opt and Or-opt) with a
small pointer/attention policy that schedules drone sorties. The policy decodes
launch--serve--rendezvous triplets with hard feasibility masks for endurance
and post-delivery recharge; a fast, exact timeline simulator enforces
launch/recovery handling and computes the true makespan used by masked
greedy/beam decoding. On Euclidean instances with $N{=}50$, $E{=}0.7$, and
$R{=}0.1$, the method achieves an average makespan of \textbf{5.203}$\pm$0.093,
versus \textbf{5.349}$\pm$0.038 for ALNS and \textbf{5.208}$\pm$0.124 for NN --
i.e., \textbf{2.73\%} better than ALNS on average and within \textbf{0.10\%} of
NN. Per-seed, the RL scheduler never underperforms ALNS on the same instance
and ties or beats NN on two of three seeds. A decomposition of the makespan
shows the expected truck--wait trade-off across heuristics; the learned
scheduler balances both to minimize the total completion time. We provide a
config-first implementation with plotting and significance-test utilities to
support replication.

</details>


### [133] [DSFT: Inspiring Diffusion Large Language Models to Comprehend Mathematical and Logical Patterns](https://arxiv.org/abs/2509.18164)
*Ranfei Chen,Ming Chen*

Main category: cs.LG

TL;DR: 提出DSFT策略提升扩散大语言模型处理数学和逻辑任务能力，在小数据集上验证有效。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型处理数学和逻辑任务有挑战，现有训练方法缺乏对数学和逻辑模式的全面理解。

Method: 提出DSFT策略，调整掩码策略和损失函数，可与其他训练方法灵活结合。

Result: 在LLaDA和Dream系列模型上验证，在小数据集上数学和逻辑问题分别提升5 - 10%和约2%。

Conclusion: DSFT掩码方法为特定模式学习提供见解，可与其他方法结合应用于不同扩散大语言模型。

Abstract: Diffusion large language models (dLLMs) have emerged as a new architecture
following auto regressive models. Their denoising process offers a powerful
generative advantage, but they present significant challenges in learning and
understanding numerically sensitive mathematical and order-sensitive logical
tasks. Current training methods, including pre-training, fine-tuning, and
reinforcement learning, focus primarily on improving general knowledge
retention and reasoning abilities, but lack a comprehensive understanding of
mathematical and logical patterns. We propose DSFT, a simple yet effective
Diffusion SFT strategy, by adjusting the masking strategy and loss function,
guiding models to understand mathematical and logical patterns. This strategy
can be flexibly combined with pre-training, reinforcement learning, and other
training methods. Validated on models such as LLaDA and Dream series, we prove
that DSFT on small-scale data can achieve improvements of 5-10% and
approximately 2% on mathematical and logical problems, respectively. This
inspiring masking approach offers insights for future learning of specific
patterns, which can be easily and efficiently combined with other training
methods and applied to various dLLMs. Our code is publicly available at
https://anonymous.4open.science/r/DSFT-0FFB/

</details>


### [134] [MobiGPT: A Foundation Model for Mobile Wireless Networks](https://arxiv.org/abs/2509.18166)
*Xiaoqian Qi,Haoye Chai,Yong Li*

Main category: cs.LG

TL;DR: 提出移动数据预测基础模型MobiGPT，能预测三种数据类型，评估显示其预测准确、泛化和迁移能力强。


<details>
  <summary>Details</summary>
Motivation: 现有移动数据预测范式在大规模异构网络下增加复杂度和部署成本，需要统一结构的模型。

Method: 设计MobiGPT模型，采用软提示学习方法和时间掩码机制，支持三种预测任务。

Result: 在超10万样本的真实数据集上，MobiGPT准确进行多类型预测，相比现有模型提高准确率，在未见场景零/少样本性能提升超21.51%。

Conclusion: MobiGPT具有强泛化和迁移能力，可作为基础模型用于移动数据预测。

Abstract: With the rapid development of mobile communication technologies, future
mobile networks will offer vast services and resources for commuting,
production, daily life, and entertainment. Accurate and efficient forecasting
of mobile data (e.g., cell traffic, user behavior, channel quality) helps
operators monitor network state changes, orchestrate wireless resources, and
schedule infrastructure and users, thereby improving supply efficiency and
service quality. However, current forecasting paradigms rely on customized
designs with tailored models for exclusive data types. Such approaches increase
complexity and deployment costs under large-scale, heterogeneous networks
involving base stations, users, and channels. In this paper, we design a
foundation model for mobile data forecasting, MobiGPT, with a unified structure
capable of forecasting three data types: base station traffic, user app usage,
and channel quality. We propose a soft-prompt learning method to help the model
understand features of different data types, and introduce a temporal masking
mechanism to guide the model through three forecasting tasks: short-term
prediction, long-term prediction, and distribution generation, supporting
diverse optimization scenarios. Evaluations on real-world datasets with over
100,000 samples show that MobiGPT achieves accurate multi-type forecasting.
Compared to existing models, it improves forecasting accuracy by 27.37%,
20.08%, and 7.27%, reflecting strong generalization. Moreover, MobiGPT exhibits
superior zero/few-shot performance in unseen scenarios, with over 21.51%
improvement, validating its strong transferability as a foundation model.

</details>


### [135] [FedIA: A Plug-and-Play Importance-Aware Gradient Pruning Aggregation Method for Domain-Robust Federated Graph Learning on Node Classification](https://arxiv.org/abs/2509.18171)
*Zhanting Zhou,KaHou Tam,Zeqin Wu,Pengzhao Sun,Jinbo Wang,Fengli Zhang*

Main category: cs.LG

TL;DR: 在领域偏移下，联邦图学习中传统聚合方法不稳定且无效，提出FedIA框架，先投影降噪再聚合，无额外上行流量与少量服务器内存占用，效果优于基线。


<details>
  <summary>Details</summary>
Motivation: 解决联邦图学习在领域偏移下，传统聚合方法不稳定和无效的问题。

Method: 提出FedIA框架，采用投影优先策略，通过两阶段、即插即用的管道，包括服务器端top - ρ掩码和影响正则化动量权重。

Result: 在同质和异质图上，比九个强基线有更平滑、稳定的收敛和更高的最终准确率，动态投影保持最优收敛率。

Conclusion: FedIA框架有效解决领域偏移下联邦图学习问题，具有良好部署性与性能。

Abstract: Federated Graph Learning (FGL) under domain skew -- as observed on platforms
such as \emph{Twitch Gamers} and multilingual \emph{Wikipedia} networks --
drives client models toward incompatible representations, rendering naive
aggregation both unstable and ineffective. We find that the culprit is not the
weighting scheme but the \emph{noisy gradient signal}: empirical analysis of
baseline methods suggests that a vast majority of gradient dimensions can be
dominated by domain-specific variance. We therefore shift focus from
"aggregation-first" to a \emph{projection-first} strategy that denoises client
updates \emph{before} they are combined. The proposed FedIA framework realises
this \underline{I}mportance-\underline{A}ware idea through a two-stage,
plug-and-play pipeline: (i) a server-side top-$\rho$ mask keeps only the most
informative about 5% of coordinates, and (ii) a lightweight
influence-regularised momentum weight suppresses outlier clients. FedIA adds
\emph{no extra uplink traffic and only negligible server memory}, making it
readily deployable. On both homogeneous (Twitch Gamers) and heterogeneous
(Wikipedia) graphs, it yields smoother, more stable convergence and higher
final accuracy than nine strong baselines. A convergence sketch further shows
that dynamic projection maintains the optimal
$\mathcal{O}(\sigma^{2}/\sqrt{T})$ rate.

</details>


### [136] [SBVR: Summation of BitVector Representation for Efficient LLM Quantization](https://arxiv.org/abs/2509.18172)
*Wonjun Bang,Jongseok Park,Hongseung Yu,Kyungmin Bin,Kyunghan Lee*

Main category: cs.LG

TL;DR: 提出新的LLM量化方法SBVR，实现高斯代码表示，在多模型评估中表现出色并提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有PTQ策略存在不足，RTN方法未考虑权重分布，码本方法影响推理速度，需新方法解决。

Method: 提出SBVR方法，将权重值映射到非均匀表示点，设计自定义CUDA内核直接在SBVR格式下进行矩阵向量乘法。

Result: 在多模型评估中展现最先进的困惑度和准确率，4位量化时端到端token生成速度比FP16模型快2.21 - 3.04倍。

Conclusion: SBVR方法有效克服现有PTQ策略的局限，能实现高效量化和快速推理。

Abstract: With the advent of large language models (LLMs), numerous Post-Training
Quantization (PTQ) strategies have been proposed to alleviate deployment
barriers created by their enormous parameter counts. Quantization achieves
compression by limiting the number of representable points in the data.
Therefore, the key to achieving efficient quantization is selecting the optimal
combination of representation points, or codes, for the given data. Existing
PTQ solutions adopt two major approaches to this problem: Round-To-Nearest
(RTN)-based methods and codebook-based methods. RTN-based methods map LLM
weights onto uniformly distributed integer grids, failing to account for the
Gaussian-like weight distribution of LLM weights. Codebook-based methods
mitigate this issue by constructing distribution-aware codebooks; however, they
suffer from random and strided memory access patterns, resulting in degraded
inference speed that is exacerbated by the limited size of GPU L1 cache. To
overcome these limitations, we propose a novel LLM quantization method, SBVR
(Summation of BitVector Representation), that enables Gaussian-like code
representation in a hardware-friendly manner for fast inference. SBVR maps
weight values to non-uniform representation points whose distribution follows
the actual distribution of LLM weights, enabling more accurate compression.
Additionally, we design a custom CUDA kernel that allows matrix-vector
multiplication directly in the SBVR format without decompression, thereby
enabling high-performance execution of SBVR-compressed models. Our evaluations
of SBVR on various models demonstrate state-of-the-art perplexity and accuracy
benchmark performance while delivering a 2.21x- 3.04x end-to-end
token-generation speedup over naive FP16 models in the 4-bit quantization
regime.

</details>


### [137] [TurnBack: A Geospatial Route Cognition Benchmark for Large Language Models through Reverse Route](https://arxiv.org/abs/2509.18173)
*Hongyi Luo,Qing Cheng,Daniel Matos,Hari Krishna Gadi,Yanfeng Zhang,Lu Liu,Yongliang Wang,Niclas Zeller,Daniel Cremers,Liqiu Meng*

Main category: cs.LG

TL;DR: 提出大规模基准对大语言模型地理空间路线认知进行全面评估，发现模型在路线反转任务存在局限。


<details>
  <summary>Details</summary>
Motivation: 大语言模型地理空间认知能力未充分探索，先前研究存在非量化指标、评估数据集有限和研究层次不清晰等问题。

Method: 创建含36000条路线的大规模评估数据集，引入PathBuilder工具，提出新评估框架和指标评估11个SOTA大语言模型。

Result: 大语言模型在路线反转任务中存在局限，多数反转路线回不到起点且与最优路线差异大，存在路线生成鲁棒性低和错误答案置信度高等问题。

Conclusion: 揭示了大语言模型在地理空间路线认知方面存在不足，为后续研究提供方向。

Abstract: Humans can interpret geospatial information through natural language, while
the geospatial cognition capabilities of Large Language Models (LLMs) remain
underexplored. Prior research in this domain has been constrained by
non-quantifiable metrics, limited evaluation datasets and unclear research
hierarchies. Therefore, we propose a large-scale benchmark and conduct a
comprehensive evaluation of the geospatial route cognition of LLMs. We create a
large-scale evaluation dataset comprised of 36000 routes from 12 metropolises
worldwide. Then, we introduce PathBuilder, a novel tool for converting natural
language instructions into navigation routes, and vice versa, bridging the gap
between geospatial information and natural language. Finally, we propose a new
evaluation framework and metrics to rigorously assess 11 state-of-the-art
(SOTA) LLMs on the task of route reversal. The benchmark reveals that LLMs
exhibit limitation to reverse routes: most reverse routes neither return to the
starting point nor are similar to the optimal route. Additionally, LLMs face
challenges such as low robustness in route generation and high confidence for
their incorrect answers. Code\ \&\ Data available here:
\href{https://github.com/bghjmn32/EMNLP2025_Turnback}{TurnBack.}

</details>


### [138] [Conversational Orientation Reasoning: Egocentric-to-Allocentric Navigation with Multimodal Chain-of-Thought](https://arxiv.org/abs/2509.18200)
*Yu Ti Huang*

Main category: cs.LG

TL;DR: 提出COR基准和MCoT框架解决对话式空间定向推理问题，实验表明MCoT效果好，有潜力用于具身导航。


<details>
  <summary>Details</summary>
Motivation: 对话式代理需将以自我为中心的话语转换为以异我为中心的方向，CoT在多模态空间定向应用待探索，解决非英语和ASR转录场景的推理问题。

Method: 引入COR基准，提出MCoT框架，通过三步推理过程整合语音和地标坐标，采用课程学习策略在Taiwan - LLM - 13B - v2.0 - Chat上构建能力。

Result: MCoT在干净转录本上定向准确率达100%，ASR转录本达98.1%，优于单模态和非结构化基线，在多种噪声条件下表现稳健。

Conclusion: 结构化MCoT空间推理有潜力实现可解释且资源高效的具身导航。

Abstract: Conversational agents must translate egocentric utterances (e.g., "on my
right") into allocentric orientations (N/E/S/W). This challenge is particularly
critical in indoor or complex facilities where GPS signals are weak and
detailed maps are unavailable. While chain-of-thought (CoT) prompting has
advanced reasoning in language and vision tasks, its application to multimodal
spatial orientation remains underexplored. We introduce Conversational
Orientation Reasoning (COR), a new benchmark designed for Traditional Chinese
conversational navigation projected from real-world environments, addressing
egocentric-to-allocentric reasoning in non-English and ASR-transcribed
scenarios. We propose a multimodal chain-of-thought (MCoT) framework, which
integrates ASR-transcribed speech with landmark coordinates through a
structured three-step reasoning process: (1) extracting spatial relations, (2)
mapping coordinates to absolute directions, and (3) inferring user orientation.
A curriculum learning strategy progressively builds these capabilities on
Taiwan-LLM-13B-v2.0-Chat, a mid-sized model representative of
resource-constrained settings. Experiments show that MCoT achieves 100%
orientation accuracy on clean transcripts and 98.1% with ASR transcripts,
substantially outperforming unimodal and non-structured baselines. Moreover,
MCoT demonstrates robustness under noisy conversational conditions, including
ASR recognition errors and multilingual code-switching. The model also
maintains high accuracy in cross-domain evaluation and resilience to linguistic
variation, domain shift, and referential ambiguity. These findings highlight
the potential of structured MCoT spatial reasoning as a path toward
interpretable and resource-efficient embodied navigation.

</details>


### [139] [Variational Task Vector Composition](https://arxiv.org/abs/2509.18208)
*Boyuan Zhang,Yingjun Du,Xiantong Zhen,Ling Shao*

Main category: cs.LG

TL;DR: 本文提出变分任务向量组合方法，通过贝叶斯推理框架估计组合系数，引入Spike - and - Slab先验和门控采样机制，实验显示该方法优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有任务向量组合方法存在不足，且任务向量有结构冗余，需要更高效且可解释的组合方法。

Method: 提出变分任务向量组合，将组合系数作为隐变量在贝叶斯推理框架下估计；引入Spike - and - Slab先验促进稀疏性；开发门控采样机制构建可控后验。

Result: 实验表明该方法在所有数据集上均优于现有方法。

Conclusion: 该方法具有实用价值，为高效有效的任务向量组合树立了新标准。

Abstract: Task vectors capture how a model changes during fine-tuning by recording the
difference between pre-trained and task-specific weights. The composition of
task vectors, a key operator in task arithmetic, enables models to integrate
knowledge from multiple tasks without incurring additional inference costs. In
this paper, we propose variational task vector composition, where composition
coefficients are taken as latent variables and estimated in a Bayesian
inference framework. Unlike previous methods that operate at the task level,
our framework focuses on sample-specific composition. Motivated by the
observation of structural redundancy in task vectors, we introduce a
Spike-and-Slab prior that promotes sparsity and preserves only the most
informative components. To further address the high variance and sampling
inefficiency in sparse, high-dimensional spaces, we develop a gated sampling
mechanism that constructs a controllable posterior by filtering the composition
coefficients based on both uncertainty and importance. This yields a more
stable and interpretable variational framework by deterministically selecting
reliable task components, reducing sampling variance while improving
transparency and generalization. Experimental results demonstrate that our
method consistently outperforms existing approaches across all datasets by
selectively leveraging the most reliable and informative components in task
vectors. These findings highlight the practical value of our approach,
establishing a new standard for efficient and effective task vector
composition.

</details>


### [140] [MolPILE - large-scale, diverse dataset for molecular representation learning](https://arxiv.org/abs/2509.18353)
*Jakub Adamczyk,Jakub Poziemski,Franciszek Job,Mateusz Król,Maciej Makowski*

Main category: cs.LG

TL;DR: 介绍大规模分子数据集MolPILE，能提升模型泛化性能，为分子化学提供标准化训练资源。


<details>
  <summary>Details</summary>
Motivation: 现有小分子数据集限制了分子表示学习效果，需构建大规模高质量数据集。

Method: 用自动化整理流程，从6个大型数据库构建包含2.22亿化合物的MolPILE数据集，并分析现有预训练数据集。

Result: 在MolPILE上重新训练现有模型，泛化性能得到提升。

Conclusion: 为分子化学模型训练提供标准化资源，满足对类似图像领域ImageNet数据集的需求。

Abstract: The size, diversity, and quality of pretraining datasets critically determine
the generalization ability of foundation models. Despite their growing
importance in chemoinformatics, the effectiveness of molecular representation
learning has been hindered by limitations in existing small molecule datasets.
To address this gap, we present MolPILE, large-scale, diverse, and rigorously
curated collection of 222 million compounds, constructed from 6 large-scale
databases using an automated curation pipeline. We present a comprehensive
analysis of current pretraining datasets, highlighting considerable
shortcomings for training ML models, and demonstrate how retraining existing
models on MolPILE yields improvements in generalization performance. This work
provides a standardized resource for model training, addressing the pressing
need for an ImageNet-like dataset in molecular chemistry.

</details>


### [141] [FastMTP: Accelerating LLM Inference with Enhanced Multi-Token Prediction](https://arxiv.org/abs/2509.18362)
*Yuxuan Cai,Xiaozhuan Liang,Xinghua Wang,Jin Ma,Haijin Liang,Jinwen Luo,Xinyu Zuo,Lisheng Duan,Yuyang Yin,Xi Chen*

Main category: cs.LG

TL;DR: 提出FastMTP方法提升大语言模型推理速度，实验显示平均加速2.03倍且无损质量。


<details>
  <summary>Details</summary>
Motivation: 自回归生成的顺序性限制大语言模型实际部署，多令牌预测在推理加速方面潜力待挖掘。

Method: 在自蒸馏数据上微调单MTP头，结合位置共享权重；将语言感知动态词汇压缩集成到MTP头。

Result: 在七个基准测试中，FastMTP较标准下一个令牌预测平均加速2.03倍，输出质量无损，比普通MTP性能高82%。

Conclusion: FastMTP训练轻量，能与现有推理框架集成，是加速大语言模型推理的实用可快速部署方案。

Abstract: As large language models (LLMs) become increasingly powerful, the sequential
nature of autoregressive generation creates a fundamental throughput bottleneck
that limits the practical deployment. While Multi-Token Prediction (MTP) has
demonstrated remarkable benefits for model training efficiency and performance,
its inherent potential for inference acceleration remains largely unexplored.
This paper introduces FastMTP, a simple yet effective method that improves
multi-step draft quality by aligning MTP training with its inference pattern,
significantly enhancing speculative decoding performance. Our approach
fine-tunes a single MTP head with position-shared weights on self-distilled
data, enabling it to capture dependencies among consecutive future tokens and
maintain high acceptance rates across multiple recursive draft steps. By
integrating language-aware dynamic vocabulary compression into the MTP head, we
further reduce computational overhead in the drafting process. Experimental
results across seven diverse benchmarks demonstrate that FastMTP achieves an
average of 2.03x speedup compared to standard next token prediction with
lossless output quality, outperforming vanilla MTP by 82%. FastMTP requires
only lightweight training and seamlessly integrates with existing inference
frameworks, offering a practical and rapidly deployable solution for
accelerating LLM inference.

</details>


### [142] [Multi-Worker Selection based Distributed Swarm Learning for Edge IoT with Non-i.i.d. Data](https://arxiv.org/abs/2509.18367)
*Zhuoyu Yao,Yue Wang,Songyang Zhang,Yingshu Li,Zhipeng Cai,Zhi Tian*

Main category: cs.LG

TL;DR: 本文研究分布式群学习（DSL）中数据异质性问题，提出M - DSL算法，经理论分析和实验验证其性能优于基准。


<details>
  <summary>Details</summary>
Motivation: DSL虽有优势，但非独立同分布（non - i.i.d.）数据影响学习性能，且缺乏数据异质性对模型训练精度影响的理论指导。

Method: 测量DSL框架下non - i.i.d.数据集影响，提出M - DSL算法，引入新的non - i.i.d.程度度量，进行理论分析和实验验证。

Result: 数值结果验证M - DSL在性能和网络智能方面优于基准。

Conclusion: M - DSL算法能有效处理分布式异构数据，提升DSL性能和网络智能。

Abstract: Recent advances in distributed swarm learning (DSL) offer a promising
paradigm for edge Internet of Things. Such advancements enhance data privacy,
communication efficiency, energy saving, and model scalability. However, the
presence of non-independent and identically distributed (non-i.i.d.) data pose
a significant challenge for multi-access edge computing, degrading learning
performance and diverging training behavior of vanilla DSL. Further, there
still lacks theoretical guidance on how data heterogeneity affects model
training accuracy, which requires thorough investigation. To fill the gap, this
paper first study the data heterogeneity by measuring the impact of non-i.i.d.
datasets under the DSL framework. This then motivates a new multi-worker
selection design for DSL, termed M-DSL algorithm, which works effectively with
distributed heterogeneous data. A new non-i.i.d. degree metric is introduced
and defined in this work to formulate the statistical difference among local
datasets, which builds a connection between the measure of data heterogeneity
and the evaluation of DSL performance. In this way, our M-DSL guides effective
selection of multiple works who make prominent contributions for global model
updates. We also provide theoretical analysis on the convergence behavior of
our M-DSL, followed by extensive experiments on different heterogeneous
datasets and non-i.i.d. data settings. Numerical results verify performance
improvement and network intelligence enhancement provided by our M-DSL beyond
the benchmarks.

</details>


### [143] [GnnXemplar: Exemplars to Explanations - Natural Language Rules for Global GNN Interpretability](https://arxiv.org/abs/2509.18376)
*Burouj Armgaan,Eshan Jain,Harsh Pandey,Mahesh Chandran,Sayan Ranu*

Main category: cs.LG

TL;DR: 本文提出了新的全局解释器GnnXemplar，在多个基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 图神经网络（GNN）决策不透明，现有全局解释方法在大规模真实场景存在局限性。

Method: 受认知科学的范例理论启发，在GNN嵌入空间识别代表节点，用贪婪近似解决覆盖最大化问题，使用大语言模型的自精炼提示策略推导规则。

Result: 在不同基准测试中，GnnXemplar在保真度、可扩展性和人类可解释性方面显著优于现有方法，用户研究验证了其效果。

Conclusion: GnnXemplar是一种有效的图神经网络全局解释方法。

Abstract: Graph Neural Networks (GNNs) are widely used for node classification, yet
their opaque decision-making limits trust and adoption. While local
explanations offer insights into individual predictions, global explanation
methods, those that characterize an entire class, remain underdeveloped.
Existing global explainers rely on motif discovery in small graphs, an approach
that breaks down in large, real-world settings where subgraph repetition is
rare, node attributes are high-dimensional, and predictions arise from complex
structure-attribute interactions. We propose GnnXemplar, a novel global
explainer inspired from Exemplar Theory from cognitive science. GnnXemplar
identifies representative nodes in the GNN embedding space, exemplars, and
explains predictions using natural language rules derived from their
neighborhoods. Exemplar selection is framed as a coverage maximization problem
over reverse k-nearest neighbors, for which we provide an efficient greedy
approximation. To derive interpretable rules, we employ a self-refining prompt
strategy using large language models (LLMs). Experiments across diverse
benchmarks show that GnnXemplar significantly outperforms existing methods in
fidelity, scalability, and human interpretability, as validated by a user study
with 60 participants.

</details>


### [144] [Graph Enhanced Trajectory Anomaly Detection](https://arxiv.org/abs/2509.18386)
*Jonathan Kabala Mbuya,Dieter Pfoser,Antonios Anastasopoulos*

Main category: cs.LG

TL;DR: 提出GETAD框架用于轨迹异常检测，结合路网拓扑等信息，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹异常检测方法仅考虑有限方面，忽略底层移动网络约束和连通性信息。

Method: 提出GETAD框架，用图注意力网络学习嵌入，结合基于图的位置编码，用Transformer解码器建模，采用多目标损失函数，引入CW NLL异常评分函数。

Result: 在真实和合成数据集实验中，GETAD比现有方法有持续改进，尤其在检测道路约束环境中的细微异常。

Conclusion: 将图结构和上下文语义融入轨迹建模有益，可实现更精确和上下文感知的异常检测。

Abstract: Trajectory anomaly detection is essential for identifying unusual and
unexpected movement patterns in applications ranging from intelligent
transportation systems to urban safety and fraud prevention.
  Existing methods only consider limited aspects of the trajectory nature and
its movement space by treating trajectories as sequences of sampled locations,
with sampling determined by positioning technology, e.g., GPS, or by high-level
abstractions such as staypoints. Trajectories are analyzed in Euclidean space,
neglecting the constraints and connectivity information of the underlying
movement network, e.g., road or transit networks.
  The proposed Graph Enhanced Trajectory Anomaly Detection (GETAD) framework
tightly integrates road network topology, segment semantics, and historical
travel patterns to model trajectory data. GETAD uses a Graph Attention Network
to learn road-aware embeddings that capture both physical attributes and
transition behavior, and augments these with graph-based positional encodings
that reflect the spatial layout of the road network.
  A Transformer-based decoder models sequential movement, while a
multiobjective loss function combining autoregressive prediction and supervised
link prediction ensures realistic and structurally coherent representations.
  To improve the robustness of anomaly detection, we introduce Confidence
Weighted Negative Log Likelihood (CW NLL), an anomaly scoring function that
emphasizes high-confidence deviations.
  Experiments on real-world and synthetic datasets demonstrate that GETAD
achieves consistent improvements over existing methods, particularly in
detecting subtle anomalies in road-constrained environments. These results
highlight the benefits of incorporating graph structure and contextual
semantics into trajectory modeling, enabling more precise and context-aware
anomaly detection.

</details>


### [145] [Towards Provable Emergence of In-Context Reinforcement Learning](https://arxiv.org/abs/2509.18389)
*Jiuqi Wang,Rohan Chandra,Shangtong Zhang*

Main category: cs.LG

TL;DR: 本文探讨强化学习预训练算法生成的网络参数能实现上下文强化学习（ICRL）的原因，通过案例研究支持参数是预训练损失极小值的假设。


<details>
  <summary>Details</summary>
Motivation: 许多ICRL工作用标准强化学习算法预训练，但不清楚为何这些预训练算法能生成使ICRL可行的网络参数。

Method: 进行案例研究，证明Transformer用于策略评估预训练时，预训练损失的一个全局极小值可实现上下文时间差分学习。

Result: 证明了Transformer预训练时，其预训练损失的一个全局极小值能实现上下文时间差分学习。

Conclusion: 为参数能实现ICRL是预训练损失极小值的假设提供了初步支持。

Abstract: Typically, a modern reinforcement learning (RL) agent solves a task by
updating its neural network parameters to adapt its policy to the task.
Recently, it has been observed that some RL agents can solve a wide range of
new out-of-distribution tasks without parameter updates after pretraining on
some task distribution. When evaluated in a new task, instead of making
parameter updates, the pretrained agent conditions its policy on additional
input called the context, e.g., the agent's interaction history in the new
task. The agent's performance increases as the information in the context
increases, with the agent's parameters fixed. This phenomenon is typically
called in-context RL (ICRL). The pretrained parameters of the agent network
enable the remarkable ICRL phenomenon. However, many ICRL works perform the
pretraining with standard RL algorithms. This raises the central question this
paper aims to address: Why can the RL pretraining algorithm generate network
parameters that enable ICRL? We hypothesize that the parameters capable of ICRL
are minimizers of the pretraining loss. This work provides initial support for
this hypothesis through a case study. In particular, we prove that when a
Transformer is pretrained for policy evaluation, one of the global minimizers
of the pretraining loss can enable in-context temporal difference learning.

</details>


### [146] [Development of Deep Learning Optimizers: Approaches, Concepts, and Update Rules](https://arxiv.org/abs/2509.18396)
*Doğay Altınel*

Main category: cs.LG

TL;DR: 本文回顾深度学习优化器，按时间顺序介绍多种优化器，详述更新规则、相关概念等，探讨技术、贡献和超参设置，指出优化挑战，为了解现状和未来发展提供资源。


<details>
  <summary>Details</summary>
Motivation: 深度学习发展催生多种优化器，学习效果依赖优化器，需对文献中受关注的优化器进行综述。

Method: 按时间顺序逐个分析优化器，详细介绍更新规则、相关概念和变量，讨论技术、贡献和默认超参设置。

Result: 呈现了多种优化器的详细信息，包括更新规则、相关概念、技术、贡献和超参设置，指出了深度学习模型优化中的开放挑战。

Conclusion: 为理解优化器现状和识别未来潜在发展领域提供了全面资源。

Abstract: Deep learning optimizers are optimization algorithms that enable deep neural
networks to learn. The effectiveness of learning is highly dependent on the
optimizer employed in the training process. Alongside the rapid advancement of
deep learning, a wide range of optimizers with different approaches have been
developed. This study aims to provide a review of various optimizers that have
been proposed and received attention in the literature. From Stochastic
gradient descent to the most recent ones such as Momentum, AdamW, Sophia, and
Muon in chronological order, optimizers are examined individually, and their
distinctive features are highlighted in the study. The update rule of each
optimizer is presented in detail, with an explanation of the associated
concepts and variables. The techniques applied by these optimizers, their
contributions to the optimization process, and their default hyperparameter
settings are also discussed. In addition, insights are offered into the open
challenges encountered in the optimization of deep learning models. Thus, a
comprehensive resource is provided both for understanding the current state of
optimizers and for identifying potential areas of future development.

</details>


### [147] [Explicit Path CGR: Maintaining Sequence Fidelity in Geometric Representations](https://arxiv.org/abs/2509.18408)
*Sarwan Ali*

Main category: cs.LG

TL;DR: 提出新颖的信息保留型混沌游戏表示法R - CGR用于生物序列分析，解决传统方法信息丢失问题，在分类任务中有效。


<details>
  <summary>Details</summary>
Motivation: 解决传统CGR方法在几何映射中丢失序列信息的根本局限。

Method: 通过显式路径编码结合有理算术精度控制实现完整序列恢复，全面存储路径以保留位置和字符信息。

Result: 在生物序列分类任务中取得与传统序列方法相当的性能，生成适合深度学习的特征丰富图像。

Conclusion: R - CGR为精度和序列恢复都重要的可解释生物信息学分析开辟了新途径。

Abstract: We present a novel information-preserving Chaos Game Representation (CGR)
method, also called Reverse-CGR (R-CGR), for biological sequence analysis that
addresses the fundamental limitation of traditional CGR approaches - the loss
of sequence information during geometric mapping. Our method introduces
complete sequence recovery through explicit path encoding combined with
rational arithmetic precision control, enabling perfect sequence reconstruction
from stored geometric traces. Unlike purely geometric approaches, our
reversibility is achieved through comprehensive path storage that maintains
both positional and character information at each step. We demonstrate the
effectiveness of R-CGR on biological sequence classification tasks, achieving
competitive performance compared to traditional sequence-based methods while
providing interpretable geometric visualizations. The approach generates
feature-rich images suitable for deep learning while maintaining complete
sequence information through explicit encoding, opening new avenues for
interpretable bioinformatics analysis where both accuracy and sequence recovery
are essential.

</details>


### [148] [Diffusion Policies with Offline and Inverse Reinforcement Learning for Promoting Physical Activity in Older Adults Using Wearable Sensors](https://arxiv.org/abs/2509.18433)
*Chang Liu,Ladda Thiamwong,Yanjie Fu,Rui Xie*

Main category: cs.LG

TL;DR: 本文提出KANDI方法解决离线强化学习在促进老年人身体活动中的挑战，在临床试验和基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在医疗保健应用中面临定义奖励困难、难以与人类行为对齐等挑战，需要解决其在促进高跌倒风险老年人身体活动中的应用问题。

Method: 引入Kolmogorov - Arnold网络和扩散策略的离线逆强化学习方法KANDI，利用网络的灵活函数逼近估计奖励函数，基于扩散的策略进行动作优化。

Result: 在临床试验中评估KANDI，且在D4RL基准测试中优于现有方法。

Conclusion: KANDI有潜力解决医疗保健应用中离线强化学习的关键挑战，为活动促进干预策略提供有效解决方案。

Abstract: Utilizing offline reinforcement learning (RL) with real-world clinical data
is getting increasing attention in AI for healthcare. However, implementation
poses significant challenges. Defining direct rewards is difficult, and inverse
RL (IRL) struggles to infer accurate reward functions from expert behavior in
complex environments. Offline RL also encounters challenges in aligning learned
policies with observed human behavior in healthcare applications. To address
challenges in applying offline RL to physical activity promotion for older
adults at high risk of falls, based on wearable sensor activity monitoring, we
introduce Kolmogorov-Arnold Networks and Diffusion Policies for Offline Inverse
Reinforcement Learning (KANDI). By leveraging the flexible function
approximation in Kolmogorov-Arnold Networks, we estimate reward functions by
learning free-living environment behavior from low-fall-risk older adults
(experts), while diffusion-based policies within an Actor-Critic framework
provide a generative approach for action refinement and efficiency in offline
RL. We evaluate KANDI using wearable activity monitoring data in a two-arm
clinical trial from our Physio-feedback Exercise Program (PEER) study,
emphasizing its practical application in a fall-risk intervention program to
promote physical activity among older adults. Additionally, KANDI outperforms
state-of-the-art methods on the D4RL benchmark. These results underscore
KANDI's potential to address key challenges in offline RL for healthcare
applications, offering an effective solution for activity promotion
intervention strategies in healthcare.

</details>


### [149] [MeshODENet: A Graph-Informed Neural Ordinary Differential Equation Neural Network for Simulating Mesh-Based Physical Systems](https://arxiv.org/abs/2509.18445)
*Kangzheng Liu,Leixin Ma*

Main category: cs.LG

TL;DR: 提出MeshODENet框架解决传统数值求解器和GNN长期预测问题，在结构力学问题中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统数值求解器在多查询任务中计算成本高，GNN长期预测存在误差累积和不稳定问题。

Method: 引入MeshODENet框架，结合GNN空间推理和神经常微分方程的连续时间建模。

Result: 在一系列结构力学问题中，该方法在长期预测准确性和稳定性上显著优于基线模型，且比传统求解器有显著计算加速。

Conclusion: 提出了一种强大且可推广的数据驱动替代方法，用于加速复杂结构系统的分析和建模。

Abstract: The simulation of complex physical systems using a discretized mesh is a
cornerstone of applied mechanics, but traditional numerical solvers are often
computationally prohibitive for many-query tasks. While Graph Neural Networks
(GNNs) have emerged as powerful surrogate models for mesh-based data, their
standard autoregressive application for long-term prediction is often plagued
by error accumulation and instability. To address this, we introduce
MeshODENet, a general framework that synergizes the spatial reasoning of GNNs
with the continuous-time modeling of Neural Ordinary Differential Equations. We
demonstrate the framework's effectiveness and versatility on a series of
challenging structural mechanics problems, including one- and two-dimensional
elastic bodies undergoing large, non-linear deformations. The results
demonstrate that our approach significantly outperforms baseline models in
long-term predictive accuracy and stability, while achieving substantial
computational speed-ups over traditional solvers. This work presents a powerful
and generalizable approach for developing data-driven surrogates to accelerate
the analysis and modeling of complex structural systems.

</details>


### [150] [GluMind: Multimodal Parallel Attention and Knowledge Retention for Robust Cross-Population Blood Glucose Forecasting](https://arxiv.org/abs/2509.18457)
*Ebrahim Farahmand,Reza Rahimi Azghan,Nooshin Taheri Chatrudi,Velarie Yaa Ansu-Baidoo,Eric Kim,Gautham Krishna Gudur,Mohit Malu,Owen Krueger,Edison Thomaz,Giulia Pedrielli,Pavan Turaga,Hassan Ghasemzadeh*

Main category: cs.LG

TL;DR: 提出基于Transformer的多模态框架GluMind用于持续长期血糖预测，在AIREADI数据集上表现优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 解决持续长期血糖预测中不同采样率挑战及灾难性遗忘问题，实现更准确的血糖预测。

Method: 设计并行的交叉注意力和多尺度注意力机制，在Transformer模型中加入知识保留技术。

Result: 在AIREADI数据集上，GluMind的均方根误差和平均绝对误差分别提升约15%和9%，优于其他模型。

Conclusion: GluMind能有效进行持续长期血糖预测，性能稳定且具有适应性。

Abstract: This paper proposes GluMind, a transformer-based multimodal framework
designed for continual and long-term blood glucose forecasting. GluMind devises
two attention mechanisms, including cross-attention and multi-scale attention,
which operate in parallel and deliver accurate predictive performance.
Cross-attention effectively integrates blood glucose data with other
physiological and behavioral signals such as activity, stress, and heart rate,
addressing challenges associated with varying sampling rates and their adverse
impacts on robust prediction. Moreover, the multi-scale attention mechanism
captures long-range temporal dependencies. To mitigate catastrophic forgetting,
GluMind incorporates a knowledge retention technique into the transformer-based
forecasting model. The knowledge retention module not only enhances the model's
ability to retain prior knowledge but also boosts its overall forecasting
performance. We evaluate GluMind on the recently released AIREADI dataset,
which contains behavioral and physiological data collected from healthy people,
individuals with prediabetes, and those with type 2 diabetes. We examine the
performance stability and adaptability of GluMind in learning continuously as
new patient cohorts are introduced. Experimental results show that GluMind
consistently outperforms other state-of-the-art forecasting models, achieving
approximately 15% and 9% improvements in root mean squared error (RMSE) and
mean absolute error (MAE), respectively.

</details>


### [151] [Discrete-time diffusion-like models for speech synthesis](https://arxiv.org/abs/2509.18470)
*Xiaozhou Tan,Minghui Zhao,Mattias Cross,Anton Ragni*

Main category: cs.LG

TL;DR: 本文探索扩散类离散时间过程并提出新变体，实验表明其训练和推理更高效一致，语音质量与连续时间过程相当。


<details>
  <summary>Details</summary>
Motivation: 现有连续时间扩散模型在训练和推理上存在限制，离散时间过程无这些问题且推理步骤少，训练和推理条件更一致。

Method: 探索扩散类离散时间过程，提出应用加性高斯噪声、乘性高斯噪声、模糊噪声以及模糊和高斯噪声混合的新变体。

Result: 离散时间过程在主观和客观语音质量上与连续时间过程相当。

Conclusion: 离散时间过程具有更高效和一致的训练与推理模式。

Abstract: Diffusion models have attracted a lot of attention in recent years. These
models view speech generation as a continuous-time process. For efficient
training, this process is typically restricted to additive Gaussian noising,
which is limiting. For inference, the time is typically discretized, leading to
the mismatch between continuous training and discrete sampling conditions.
Recently proposed discrete-time processes, on the other hand, usually do not
have these limitations, may require substantially fewer inference steps, and
are fully consistent between training/inference conditions. This paper explores
some diffusion-like discrete-time processes and proposes some new variants.
These include processes applying additive Gaussian noise, multiplicative
Gaussian noise, blurring noise and a mixture of blurring and Gaussian noises.
The experimental results suggest that discrete-time processes offer comparable
subjective and objective speech quality to their widely popular continuous
counterpart, with more efficient and consistent training and inference schemas.

</details>


### [152] [SimpleFold: Folding Proteins is Simpler than You Think](https://arxiv.org/abs/2509.18480)
*Yuyang Wang,Jiarui Lu,Navdeep Jaitly,Josh Susskind,Miguel Angel Bautista*

Main category: cs.LG

TL;DR: 本文提出基于流匹配的蛋白质折叠模型SimpleFold，仅用通用Transformer块，在基准测试中表现出色，挑战了对复杂特定领域架构设计的依赖。


<details>
  <summary>Details</summary>
Motivation: 鉴于生成模型在不同但相关问题上的成功，探讨构建高性能蛋白质折叠模型时，特定架构设计是否为必要条件。

Method: 采用带自适应层的标准Transformer块，通过生成流匹配目标和额外结构项进行训练，扩展到3B参数并在约900万个蒸馏蛋白质结构和实验PDB数据上训练。

Result: 在标准折叠基准测试中，SimpleFold - 3B与最先进的基线模型表现相当，在集成预测中表现出色，且在消费级硬件上部署和推理效率高。

Conclusion: SimpleFold挑战了蛋白质折叠中对复杂特定领域架构设计的依赖，为未来发展开辟了新的设计空间。

Abstract: Protein folding models have achieved groundbreaking results typically via a
combination of integrating domain knowledge into the architectural blocks and
training pipelines. Nonetheless, given the success of generative models across
different but related problems, it is natural to question whether these
architectural designs are a necessary condition to build performant models. In
this paper, we introduce SimpleFold, the first flow-matching based protein
folding model that solely uses general purpose transformer blocks. Protein
folding models typically employ computationally expensive modules involving
triangular updates, explicit pair representations or multiple training
objectives curated for this specific domain. Instead, SimpleFold employs
standard transformer blocks with adaptive layers and is trained via a
generative flow-matching objective with an additional structural term. We scale
SimpleFold to 3B parameters and train it on approximately 9M distilled protein
structures together with experimental PDB data. On standard folding benchmarks,
SimpleFold-3B achieves competitive performance compared to state-of-the-art
baselines, in addition SimpleFold demonstrates strong performance in ensemble
prediction which is typically difficult for models trained via deterministic
reconstruction objectives. Due to its general-purpose architecture, SimpleFold
shows efficiency in deployment and inference on consumer-level hardware.
SimpleFold challenges the reliance on complex domain-specific architectures
designs in protein folding, opening up an alternative design space for future
progress.

</details>


### [153] [Physics-informed time series analysis with Kolmogorov-Arnold Networks under Ehrenfest constraints](https://arxiv.org/abs/2509.18483)
*Abhijit Sen,Illya V. Lukin,Kurt Jacobs,Lev Kaplan,Andrii G. Sotnikov,Denys I. Bondar*

Main category: cs.LG

TL;DR: 本文介绍了新的Kolmogorov Arnold Networks (KANs)方法用于量子动力学响应预测，能以更少数据实现高精度，Chain of KANs架构适合时间序列建模，优于传统黑盒模型。


<details>
  <summary>Details</summary>
Motivation: 量子系统在高维希尔伯特空间演化，传统数值方法计算成本高，现有神经网络需大量训练数据且存在虚假振荡问题，需要新方法预测量子动力学响应。

Method: 引入用物理信息损失函数增强的KANs方法，该函数强制执行埃伦费斯特定理；提出Chain of KANs架构将时间因果性嵌入模型设计。

Result: KANs方法只需Temporal Convolution Networks 5.4%（200个）的样本就能达到更高精度。

Conclusion: 物理信息KANs比传统黑盒模型有明显优势，能保持数学严谨性和物理一致性，大幅降低数据需求。

Abstract: The prediction of quantum dynamical responses lies at the heart of modern
physics. Yet, modeling these time-dependent behaviors remains a formidable
challenge because quantum systems evolve in high-dimensional Hilbert spaces,
often rendering traditional numerical methods computationally prohibitive.
While large language models have achieved remarkable success in sequential
prediction, quantum dynamics presents a fundamentally different challenge:
forecasting the entire temporal evolution of quantum systems rather than merely
the next element in a sequence. Existing neural architectures such as recurrent
and convolutional networks often require vast training datasets and suffer from
spurious oscillations that compromise physical interpretability. In this work,
we introduce a fundamentally new approach: Kolmogorov Arnold Networks (KANs)
augmented with physics-informed loss functions that enforce the Ehrenfest
theorems. Our method achieves superior accuracy with significantly less
training data: it requires only 5.4 percent of the samples (200) compared to
Temporal Convolution Networks (3,700). We further introduce the Chain of KANs,
a novel architecture that embeds temporal causality directly into the model
design, making it particularly well-suited for time series modeling. Our
results demonstrate that physics-informed KANs offer a compelling advantage
over conventional black-box models, maintaining both mathematical rigor and
physical consistency while dramatically reducing data requirements.

</details>


### [154] [Hybrid Data can Enhance the Utility of Synthetic Data for Training Anti-Money Laundering Models](https://arxiv.org/abs/2509.18499)
*Rachel Chung,Pratyush Nidhi Sharma,Mikko Siponen,Rohit Vadodaria,Luke Smith*

Main category: cs.LG

TL;DR: 文章针对反洗钱模型训练数据缺乏问题，提出使用混合数据集，其能保留隐私并提升模型效用。


<details>
  <summary>Details</summary>
Motivation: 自动化反洗钱模型训练缺乏数据，因隐私和保密问题难以获取真实数据，纯合成数据集训练有挑战。

Method: 提出使用混合数据集，将公开、易获取的现实特征融入合成数据集。

Result: 混合数据集不仅保留隐私，还能提高模型效用。

Conclusion: 混合数据集为金融机构增强反洗钱系统提供了实用途径。

Abstract: Money laundering is a critical global issue for financial institutions.
Automated Anti-money laundering (AML) models, like Graph Neural Networks (GNN),
can be trained to identify illicit transactions in real time. A major issue for
developing such models is the lack of access to training data due to privacy
and confidentiality concerns. Synthetically generated data that mimics the
statistical properties of real data but preserves privacy and confidentiality
has been proposed as a solution. However, training AML models on purely
synthetic datasets presents its own set of challenges. This article proposes
the use of hybrid datasets to augment the utility of synthetic datasets by
incorporating publicly available, easily accessible, and real-world features.
These additions demonstrate that hybrid datasets not only preserve privacy but
also improve model utility, offering a practical pathway for financial
institutions to enhance AML systems.

</details>


### [155] [APRIL: Active Partial Rollouts in Reinforcement Learning to tame long-tail generation](https://arxiv.org/abs/2509.18521)
*Yuzhen Zhou,Jiajun Li,Yusheng Su,Gowtham Ramesh,Zilin Zhu,Xiang Long,Chenyang Zhao,Jin Pan,Xiaodong Yu,Ze Wang,Kangrui Du,Jialian Wu,Ximeng Sun,Jiang Liu,Qiaolin Yu,Hao Chen,Zicheng Liu,Emad Barsoum*

Main category: cs.LG

TL;DR: 提出APRIL方法缓解强化学习中长尾低效问题，提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习训练计算成本高，卷展生成占比大，长尾分布限制效率和可扩展性。

Method: 提出APRIL，在卷展阶段过度提供请求，达目标数终止，回收未完成响应。

Result: APRIL使卷展吞吐量最多提高44%，加速收敛，最终准确率最多提高8%，且与框架和硬件无关。

Conclusion: 结合系统和算法考虑提出APRIL，提升强化学习训练效率，启发进一步优化。

Abstract: Reinforcement learning (RL) has become a cornerstone in advancing large-scale
pre-trained language models (LLMs). Successive generations, including GPT-o
series, DeepSeek-R1, Kimi-K1.5, Grok 4, and GLM-4.5, have relied on large-scale
RL training to enhance reasoning and coding capabilities. To meet the
community's growing RL needs, numerous RL frameworks have been proposed. Most
of these frameworks primarily rely on inference engines for rollout generation
and training engines for policy updates. However, RL training remains
computationally expensive, with rollout generation accounting for more than 90%
of total runtime. In addition, its efficiency is often constrained by the
long-tail distribution of rollout response lengths, where a few lengthy
responses stall entire batches, leaving GPUs idle and underutilized. As model
and rollout sizes continue to grow, this bottleneck increasingly limits
scalability. To address this challenge, we propose Active Partial Rollouts in
Reinforcement Learning (APRIL), which mitigates long-tail inefficiency. In the
rollout phase, APRIL over-provisions rollout requests, terminates once the
target number of responses is reached, and recycles incomplete responses for
continuation in future steps. This strategy ensures that no rollouts are
discarded while substantially reducing GPU idle time. Experiments show that
APRIL improves rollout throughput by at most 44% across commonly used RL
algorithms (GRPO, DAPO, GSPO), accelerates convergence, and achieves at most 8%
higher final accuracy across tasks. Moreover, APRIL is both framework and
hardware agnostic, already integrated into the slime RL framework, and
deployable on NVIDIA and AMD GPUs alike. Taken together, this work unifies
system-level and algorithmic considerations in proposing APRIL, with the aim of
advancing RL training efficiency and inspiring further optimizations in RL
systems.

</details>


### [156] [Reverse-Complement Consistency for DNA Language Models](https://arxiv.org/abs/2509.18529)
*Mingqian Ma*

Main category: cs.LG

TL;DR: 提出反向互补一致性正则化（RCCR），在多种基因组任务中提升反向互补鲁棒性，维持或提升任务准确性。


<details>
  <summary>Details</summary>
Motivation: 现有DNA语言模型常无法捕捉反向互补对称性，预测不一致，可靠性成问题。

Method: 引入RCCR，直接惩罚模型对序列及其反向互补序列预测的差异，在三种骨干网络上进行评估。

Result: RCCR显著减少预测翻转和错误，提升反向互补鲁棒性，维持或提升任务准确性。

Conclusion: RCCR将关键生物学先验融入学习过程，是适用于多样生物学任务的模型微调方法。

Abstract: A fundamental property of DNA is that the reverse complement (RC) of a
sequence often carries identical biological meaning. However, state-of-the-art
DNA language models frequently fail to capture this symmetry, producing
inconsistent predictions for a sequence and its RC counterpart, which
undermines their reliability. In this work, we introduce Reverse-Complement
Consistency Regularization (RCCR), a simple and model-agnostic fine-tuning
objective that directly penalizes the divergence between a model's prediction
on a sequence and the aligned prediction on its reverse complement. We evaluate
RCCR across three diverse backbones (Nucleotide Transformer, HyenaDNA,
DNABERT-2) on a wide range of genomic tasks, including sequence classification,
scalar regression, and profile prediction. Our experiments show that RCCR
substantially improves RC robustness by dramatically reducing prediction flips
and errors, all while maintaining or improving task accuracy compared to
baselines such as RC data augmentation and test-time averaging. By integrating
a key biological prior directly into the learning process, RCCR produces a
single, intrinsically robust, and computationally efficient model fine-tuning
recipe for diverse biology tasks.

</details>


### [157] [Symphony-MoE: Harmonizing Disparate Pre-trained Models into a Coherent Mixture-of-Experts](https://arxiv.org/abs/2509.18542)
*Qi Wang,Hanyang Peng,Yue Yu*

Main category: cs.LG

TL;DR: 本文提出Symphony - MoE框架，解决从多个预训练模型构建MoE模型时的参数不匹配问题，实验表明该方法效果优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有MoE模型的upcycling方法因专家源于单一预训练模型限制了专家多样性，且直接复用多个不同预训练模型会导致性能下降。

Method: 提出Symphony - MoE两阶段框架，先通过层感知融合策略构建共享骨干并基于激活的功能对齐缓解参数不匹配，再进行轻量级的路由训练。

Result: 成功集成异构源专家，在多领域任务和分布外泛化中显著超越基线。

Conclusion: 所提Symphony - MoE框架能有效解决从多个预训练模型构建MoE模型的问题，具有良好性能。

Abstract: Mixture-of-Experts (MoE) models enable scalable performance by activating
large parameter sets sparsely, minimizing computational overhead. To circumvent
the prohibitive cost of training MoEs from scratch, recent work employs
upcycling, reusing a single pre-trained dense model by replicating its
feed-forward network (FFN) layers into experts. However, this limits expert
diversity, as all experts originate from a single pre-trained dense model. This
paper addresses this limitation by constructing powerful MoE models using
experts sourced from multiple identically-architected but disparate pre-trained
models (e.g., Llama2-Chat and Code Llama). A key challenge lies in the fact
that these source models occupy disparate, dissonant regions of the parameter
space, making direct upcycling prone to severe performance degradation. To
overcome this, we propose Symphony-MoE, a novel two-stage framework designed to
harmonize these models into a single, coherent expert mixture. First, we
establish this harmony in a training-free manner: we construct a shared
backbone via a layer-aware fusion strategy and, crucially, alleviate parameter
misalignment among experts using activation-based functional alignment.
Subsequently, a single lightweight stage of router training coordinates the
entire architecture. Experiments demonstrate that our method successfully
integrates experts from heterogeneous sources, achieving an MoE model that
significantly surpasses baselines in multi-domain tasks and out-of-distribution
generalization.

</details>


### [158] [Global Minimizers of Sigmoid Contrastive Loss](https://arxiv.org/abs/2509.18552)
*Kiril Bangachev,Guy Bresler,Iliyas Noman,Yury Polyanskiy*

Main category: cs.LG

TL;DR: 本文理论解释SigLIP和SigLIP2模型中使用可训练逆温度和偏差同步的优势，提出新组合对象并用于理论分析，还提出重新参数化改进训练动态。


<details>
  <summary>Details</summary>
Motivation: 解释对比预训练中SigLIP和SigLIP2模型使用可训练逆温度和偏差同步的优势。

Method: 引入(m, b_{rel})-Constellations组合对象，用其进行理论分析；提出sigmoid损失的重新参数化。

Result: 用(m, b_{rel})-Constellations理论分析了SigLIP在检索上的成功、模态差距和生成高质量表示的必要维度；重新参数化在合成数据实验中改进训练动态。

Conclusion: 理论解释了SigLIP和SigLIP2模型相关优势，新方法改进训练动态。

Abstract: The meta-task of obtaining and aligning representations through contrastive
pretraining is steadily gaining importance since its introduction in CLIP and
ALIGN. In this paper we theoretically explain the advantages of synchronizing
with trainable inverse temperature and bias under the sigmoid loss, as
implemented in the recent SigLIP and SigLIP2 models of Google DeepMind.
Temperature and bias can drive the loss function to zero for a rich class of
configurations that we call $(\mathsf{m},
\mathsf{b}_{\mathsf{rel}})$-Constellations. $(\mathsf{m},
\mathsf{b}_{\mathsf{rel}})$-Constellations are a novel combinatorial object
related to spherical codes and are parametrized by a margin $\mathsf{m}$ and
relative bias $\mathsf{b}_{\mathsf{rel}}$. We use our characterization of
constellations to theoretically justify the success of SigLIP on retrieval, to
explain the modality gap present in SigLIP, and to identify the necessary
dimension for producing high-quality representations. Finally, we propose a
reparameterization of the sigmoid loss with explicit relative bias, which
improves training dynamics in experiments with synthetic data.

</details>


### [159] [Explainable Graph Neural Networks: Understanding Brain Connectivity and Biomarkers in Dementia](https://arxiv.org/abs/2509.18568)
*Niharika Tewari,Nguyen Linh Dan Le,Mujie Liu,Jing Ren,Ziqi Xu,Tabinda Sarwar,Veeky Baths,Feng Xia*

Main category: cs.LG

TL;DR: 本文是第一篇关于可解释图神经网络（XGNNs）在痴呆症研究中的全面综述，介绍应用、解释方法分类，指出挑战并指导未来工作。


<details>
  <summary>Details</summary>
Motivation: 痴呆症临床和生物异质性使诊断和亚型区分困难，传统图神经网络有局限性，XGNNs可解决问题，需全面综述指导研究。

Method: 考察XGNNs在多种痴呆症中的应用，引入针对痴呆症任务的可解释性方法分类，比较临床场景下现有模型。

Result: 梳理了XGNNs在痴呆症研究中的应用、可解释性方法分类及现有模型比较情况。

Conclusion: 综述既展示进展也指出开放问题，旨在指导未来XGNNs在痴呆症研究中实现可信、有临床意义和可扩展的应用。

Abstract: Dementia is a progressive neurodegenerative disorder with multiple
etiologies, including Alzheimer's disease, Parkinson's disease, frontotemporal
dementia, and vascular dementia. Its clinical and biological heterogeneity
makes diagnosis and subtype differentiation highly challenging. Graph Neural
Networks (GNNs) have recently shown strong potential in modeling brain
connectivity, but their limited robustness, data scarcity, and lack of
interpretability constrain clinical adoption. Explainable Graph Neural Networks
(XGNNs) have emerged to address these barriers by combining graph-based
learning with interpretability, enabling the identification of disease-relevant
biomarkers, analysis of brain network disruptions, and provision of transparent
insights for clinicians. This paper presents the first comprehensive review
dedicated to XGNNs in dementia research. We examine their applications across
Alzheimer's disease, Parkinson's disease, mild cognitive impairment, and
multi-disease diagnosis. A taxonomy of explainability methods tailored for
dementia-related tasks is introduced, alongside comparisons of existing models
in clinical scenarios. We also highlight challenges such as limited
generalizability, underexplored domains, and the integration of Large Language
Models (LLMs) for early detection. By outlining both progress and open
problems, this review aims to guide future work toward trustworthy, clinically
meaningful, and scalable use of XGNNs in dementia research.

</details>


### [160] [Interaction Topological Transformer for Multiscale Learning in Porous Materials](https://arxiv.org/abs/2509.18573)
*Dong Chen,Jian Liu,Chun-Long Chen,Guo-Wei Wei*

Main category: cs.LG

TL;DR: 提出Interaction Topological Transformer (ITT)框架，用于多孔材料性质预测，有良好效果和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 多孔材料结构 - 性质关系多尺度特性及数据问题使预测建模困难，需新方法。

Method: 提出ITT框架，利用交互拓扑捕捉多尺度信息，用两阶段策略训练。

Result: ITT在吸附、传输和稳定性性质预测上达先进水平，准确且可迁移。

Conclusion: 该框架为多孔材料学习引导发现提供原则性和可扩展路径。

Abstract: Porous materials exhibit vast structural diversity and support critical
applications in gas storage, separations, and catalysis. However, predictive
modeling remains challenging due to the multiscale nature of structure-property
relationships, where performance is governed by both local chemical
environments and global pore-network topology. These complexities, combined
with sparse and unevenly distributed labeled data, hinder generalization across
material families. We propose the Interaction Topological Transformer (ITT), a
unified data-efficient framework that leverages novel interaction topology to
capture materials information across multiple scales and multiple levels,
including structural, elemental, atomic, and pairwise-elemental organization.
ITT extracts scale-aware features that reflect both compositional and
relational structure within complex porous frameworks, and integrates them
through a built-in Transformer architecture that supports joint reasoning
across scales. Trained using a two-stage strategy, i.e., self-supervised
pretraining on 0.6 million unlabeled structures followed by supervised
fine-tuning, ITT achieves state-of-the-art, accurate, and transferable
predictions for adsorption, transport, and stability properties. This framework
provides a principled and scalable path for learning-guided discovery in
structurally and chemically diverse porous materials.

</details>


### [161] [DS-Diffusion: Data Style-Guided Diffusion Model for Time-Series Generation](https://arxiv.org/abs/2509.18584)
*Mingchun Sun,Rongqiang Zhao,Jie Liu*

Main category: cs.LG

TL;DR: 提出DS - Diffusion模型解决现有时间序列生成扩散模型问题，实验表明其性能更好，推理更具可解释性，适应性增强。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列生成扩散模型引入特定条件需重新训练、存在分布偏差且推理过程不可解释。

Method: 开发基于风格引导核的扩散框架避免特定条件下的重新训练，采用基于时间信息的分层去噪机制（THD）减少分布偏差。

Result: 与ImagenTime等模型相比，预测得分和判别得分分别降低5.56%和61.55%，减少分布偏差，推理更具可解释性，增强模型适应性。

Conclusion: DS - Diffusion模型有效解决现有时间序列生成扩散模型的问题，具有更好性能和适应性。

Abstract: Diffusion models are the mainstream approach for time series generation
tasks. However, existing diffusion models for time series generation require
retraining the entire framework to introduce specific conditional guidance.
There also exists a certain degree of distributional bias between the generated
data and the real data, which leads to potential model biases in downstream
tasks. Additionally, the complexity of diffusion models and the latent spaces
leads to an uninterpretable inference process. To address these issues, we
propose the data style-guided diffusion model (DS-Diffusion). In the
DS-Diffusion, a diffusion framework based on style-guided kernels is developed
to avoid retraining for specific conditions. The time-information based
hierarchical denoising mechanism (THD) is developed to reduce the
distributional bias between the generated data and the real data. Furthermore,
the generated samples can clearly indicate the data style from which they
originate. We conduct comprehensive evaluations using multiple public datasets
to validate our approach. Experimental results show that, compared to the
state-of-the-art model such as ImagenTime, the predictive score and the
discriminative score decrease by 5.56% and 61.55%, respectively. The
distributional bias between the generated data and the real data is further
reduced, the inference process is also more interpretable. Moreover, by
eliminating the need to retrain the diffusion model, the flexibility and
adaptability of the model to specific conditions are also enhanced.

</details>


### [162] [Reflect before Act: Proactive Error Correction in Language Models](https://arxiv.org/abs/2509.18607)
*Qiuhai Zeng,Sarvesh Rajkumar,Di Wang,Narendra Gyanchandani,Wenbo Yan*

Main category: cs.LG

TL;DR: 提出REBACT方法增强大语言模型决策能力，在三个环境中评估，显著优于基线且计算高效。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在交互式决策任务中存在错误累积和缺乏自我纠正机制的问题。

Method: 引入“Reflect before Act”（REBACT）方法，在采取下一步行动前进行反思以纠正错误。

Result: 在ALFWorld、WebShop和TextCraft三个环境中评估，使用Claude3.5 - sonnet作为基础大语言模型，显著优于强基线，分别提升成功率6.72%、24%和0.5%。

Conclusion: REBACT能有效提升大语言模型决策能力，且计算高效。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
interactive decision-making tasks, but existing methods often struggle with
error accumulation and lack robust self-correction mechanisms. We introduce
"Reflect before Act" (REBACT), a novel approach that enhances LLM-based
decision-making by introducing a critical reflect step prior to taking the next
action. This approach allows for immediate error correction, ensuring smooth
action path and adaptibity to environment feedback. We evaluate REBACT on three
diverse interactive environments: ALFWorld, WebShop, and TextCraft. Our results
demonstrate that REBACT significantly outperforms strong baselines, improving
success rates by up to 24% on WebShop (achieving 61%), 6.72% on ALFWorld
(achieving 98.51%), and 0.5% on TextCraft (achieving 99.5%) using
Claude3.5-sonnet as the underlying LLM. Further analysis reveals that REBACT's
performance improvements are achieved with only a few modification steps,
demonstrating its computational efficiency.

</details>


### [163] [Flow marching for a generative PDE foundation model](https://arxiv.org/abs/2509.18611)
*Zituo Chen,Sili Deng*

Main category: cs.LG

TL;DR: 提出Flow Marching算法构建生成式PDE基础模型，引入P2VAE和FMT提高效率，在下游评估中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有PDE基础模型多为确定性Transformer架构，缺乏生成灵活性，不满足科学和工程应用需求。

Method: 提出Flow Marching算法，联合采样噪声水平和物理时间步长；引入P2VAE嵌入物理状态，FMT结合扩散强迫方案和潜在时间金字塔。

Result: FMT比全长视频扩散模型计算效率高15倍，能降低成本进行大规模预训练；在下游评估中展示长期滚动稳定性和不确定性分层集合结果。

Conclusion: 生成式PDE基础模型对现实应用很重要。

Abstract: Pretraining on large-scale collections of PDE-governed spatiotemporal
trajectories has recently shown promise for building generalizable models of
dynamical systems. Yet most existing PDE foundation models rely on
deterministic Transformer architectures, which lack generative flexibility for
many science and engineering applications. We propose Flow Marching, an
algorithm that bridges neural operator learning with flow matching motivated by
an analysis of error accumulation in physical dynamical systems, and we build a
generative PDE foundation model on top of it. By jointly sampling the noise
level and the physical time step between adjacent states, the model learns a
unified velocity field that transports a noisy current state toward its clean
successor, reducing long-term rollout drift while enabling uncertainty-aware
ensemble generations. Alongside this core algorithm, we introduce a
Physics-Pretrained Variational Autoencoder (P2VAE) to embed physical states
into a compact latent space, and an efficient Flow Marching Transformer (FMT)
that combines a diffusion-forcing scheme with latent temporal pyramids,
achieving up to 15x greater computational efficiency than full-length video
diffusion models and thereby enabling large-scale pretraining at substantially
reduced cost. We curate a corpus of ~2.5M trajectories across 12 distinct PDE
families and train suites of P2VAEs and FMTs at multiple scales. On downstream
evaluation, we benchmark on unseen Kolmogorov turbulence with few-shot
adaptation, demonstrate long-term rollout stability over deterministic
counterparts, and present uncertainty-stratified ensemble results, highlighting
the importance of generative PDE foundation models for real-world applications.

</details>


### [164] [HyperAdapt: Simple High-Rank Adaptation](https://arxiv.org/abs/2509.18629)
*Abel Gurung,Joseph Campbell*

Main category: cs.LG

TL;DR: 介绍HyperAdapt参数高效微调方法，比LoRA等减少可训练参数，实验表明其性能与全微调及现有PEFT方法相当。


<details>
  <summary>Details</summary>
Motivation: 基础模型微调耗内存和计算资源，现有参数高效微调方法仍有优化空间，需进一步减少可训练参数。

Method: 提出HyperAdapt方法，通过对角矩阵对预训练权重矩阵进行行和列缩放，对n×m矩阵仅需n + m个可训练参数。

Result: 理论上给出更新秩的上界，实验验证在各基准测试中能诱导高秩变换，性能与全微调及现有PEFT方法相当。

Conclusion: HyperAdapt能在使用更少可训练参数的情况下，达到与全微调及现有PEFT方法相近的性能。

Abstract: Foundation models excel across diverse tasks, but adapting them to
specialized applications often requires fine-tuning, an approach that is memory
and compute-intensive. Parameter-efficient fine-tuning (PEFT) methods mitigate
this by updating only a small subset of weights. In this paper, we introduce
HyperAdapt, a parameter-efficient fine-tuning method that significantly reduces
the number of trainable parameters compared to state-of-the-art methods like
LoRA. Specifically, HyperAdapt adapts a pre-trained weight matrix by applying
row- and column-wise scaling through diagonal matrices, thereby inducing a
high-rank update while requiring only $n+m$ trainable parameters for an $n
\times m$ matrix. Theoretically, we establish an upper bound on the rank of
HyperAdapt's updates, and empirically, we confirm that it consistently induces
high-rank transformations across model layers. Experiments on GLUE, arithmetic
reasoning, and commonsense reasoning benchmarks with models up to 14B
parameters demonstrate that HyperAdapt matches or nearly matches the
performance of full fine-tuning and state-of-the-art PEFT methods while using
orders of magnitude fewer trainable parameters.

</details>


### [165] [Subspace Clustering of Subspaces: Unifying Canonical Correlation Analysis and Subspace Clustering](https://arxiv.org/abs/2509.18653)
*Paris A. Karakasis,Nicholas D. Sidiropoulos*

Main category: cs.LG

TL;DR: 提出基于列空间对高矩阵集合聚类的新框架SCoS，通过三阶张量的BTD分解实现，实验显示在高维应用中效果好。


<details>
  <summary>Details</summary>
Motivation: 传统子空间聚类方法假设数据为向量化，本文直接将数据样本建模为矩阵，以解决更通用的子空间聚类问题。

Method: 基于输入矩阵构建的三阶张量的块项分解（BTD），联合估计聚类成员和部分共享子空间，提出适用于大数据集的可扩展优化算法。

Result: 在真实高光谱成像数据集实验中，该方法比现有子空间聚类技术有更高的聚类准确性和鲁棒性，尤其在高噪声和干扰情况下。

Conclusion: 所提出的框架在存在超越单个数据向量结构的具有挑战性的高维应用中具有潜力。

Abstract: We introduce a novel framework for clustering a collection of tall matrices
based on their column spaces, a problem we term Subspace Clustering of
Subspaces (SCoS). Unlike traditional subspace clustering methods that assume
vectorized data, our formulation directly models each data sample as a matrix
and clusters them according to their underlying subspaces. We establish
conceptual links to Subspace Clustering and Generalized Canonical Correlation
Analysis (GCCA), and clarify key differences that arise in this more general
setting. Our approach is based on a Block Term Decomposition (BTD) of a
third-order tensor constructed from the input matrices, enabling joint
estimation of cluster memberships and partially shared subspaces. We provide
the first identifiability results for this formulation and propose scalable
optimization algorithms tailored to large datasets. Experiments on real-world
hyperspectral imaging datasets demonstrate that our method achieves superior
clustering accuracy and robustness, especially under high noise and
interference, compared to existing subspace clustering techniques. These
results highlight the potential of the proposed framework in challenging
high-dimensional applications where structure exists beyond individual data
vectors.

</details>


### [166] [Towards Rational Pesticide Design with Graph Machine Learning Models for Ecotoxicology](https://arxiv.org/abs/2509.18703)
*Jakub Adamczyk*

Main category: cs.LG

TL;DR: 研究用图机器学习加速环保农药设计，创建数据集评估模型，发现医药化学方法不适用于农药，未来将开发基准套件和定制模型。


<details>
  <summary>Details</summary>
Motivation: 受药物发现启发，加速开发更安全、环保的农药，关注农药对蜜蜂毒性的生态毒理学。

Method: 创建ApisTox数据集，对分子图分类的机器学习模型进行广泛评估。

Result: 医药化学中成功的方法难以推广到农药领域。

Conclusion: 需要特定领域的模型和基准，未来聚焦开发综合基准套件和定制机器学习模型。

Abstract: This research focuses on rational pesticide design, using graph machine
learning to accelerate the development of safer, eco-friendly agrochemicals,
inspired by in silico methods in drug discovery. With an emphasis on
ecotoxicology, the initial contributions include the creation of ApisTox, the
largest curated dataset on pesticide toxicity to honey bees. We conducted a
broad evaluation of machine learning (ML) models for molecular graph
classification, including molecular fingerprints, graph kernels, GNNs, and
pretrained transformers. The results show that methods successful in medicinal
chemistry often fail to generalize to agrochemicals, underscoring the need for
domain-specific models and benchmarks. Future work will focus on developing a
comprehensive benchmarking suite and designing ML models tailored to the unique
challenges of pesticide discovery.

</details>


### [167] [A Generalized Bisimulation Metric of State Similarity between Markov Decision Processes: From Theoretical Propositions to Applications](https://arxiv.org/abs/2509.18714)
*Zhenyu Tao,Wei Xu,Xiaohu You*

Main category: cs.LG

TL;DR: 本文提出广义双模拟度量(GBSM)用于多马尔可夫决策过程(MDP)场景，证明其性质，理论分析相关任务并获更优界，数值结果验证有效性。


<details>
  <summary>Details</summary>
Motivation: 双模拟度量(BSM)在多MDP场景应用有挑战，先前推广工作缺乏严格数学性质分析，限制理论进展。

Method: 正式建立GBSM，证明其对称性、MDP间三角不等式和相同状态空间距离界等三个基本性质，利用这些性质进行理论分析。

Result: 获得比标准BSM更严格的显式界，GBSM提供封闭形式的样本复杂度估计，优于现有基于BSM的渐近结果。

Conclusion: 数值结果验证了理论发现，表明GBSM在多MDP场景有效。

Abstract: The bisimulation metric (BSM) is a powerful tool for computing state
similarities within a Markov decision process (MDP), revealing that states
closer in BSM have more similar optimal value functions. While BSM has been
successfully utilized in reinforcement learning (RL) for tasks like state
representation learning and policy exploration, its application to multiple-MDP
scenarios, such as policy transfer, remains challenging. Prior work has
attempted to generalize BSM to pairs of MDPs, but a lack of rigorous analysis
of its mathematical properties has limited further theoretical progress. In
this work, we formally establish a generalized bisimulation metric (GBSM)
between pairs of MDPs, which is rigorously proven with the three fundamental
properties: GBSM symmetry, inter-MDP triangle inequality, and the distance
bound on identical state spaces. Leveraging these properties, we theoretically
analyse policy transfer, state aggregation, and sampling-based estimation in
MDPs, obtaining explicit bounds that are strictly tighter than those derived
from the standard BSM. Additionally, GBSM provides a closed-form sample
complexity for estimation, improving upon existing asymptotic results based on
BSM. Numerical results validate our theoretical findings and demonstrate the
effectiveness of GBSM in multi-MDP scenarios.

</details>


### [168] [LLM-Enhanced Self-Evolving Reinforcement Learning for Multi-Step E-Commerce Payment Fraud Risk Detection](https://arxiv.org/abs/2509.18719)
*Bo Qu,Zhurong Wang,Daisuke Yagi,Zhen Xu,Yang Zhao,Yinan Shan,Frank Zahradnik*

Main category: cs.LG

TL;DR: 本文提出将强化学习与大语言模型集成用于电商支付欺诈检测，通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统方法在设计强化学习奖励函数时需大量人力专业知识，希望利用大语言模型提升欺诈检测准确性。

Method: 将交易风险构建为多步马尔可夫决策过程，利用大语言模型迭代优化奖励函数。

Result: 实验表明该框架有效、稳健且有弹性，实现更好的欺诈检测准确性和零样本学习能力。

Conclusion: 大语言模型在推进工业强化学习应用方面有潜力。

Abstract: This paper presents a novel approach to e-commerce payment fraud detection by
integrating reinforcement learning (RL) with Large Language Models (LLMs). By
framing transaction risk as a multi-step Markov Decision Process (MDP), RL
optimizes risk detection across multiple payment stages. Crafting effective
reward functions, essential for RL model success, typically requires
significant human expertise due to the complexity and variability in design.
LLMs, with their advanced reasoning and coding capabilities, are well-suited to
refine these functions, offering improvements over traditional methods. Our
approach leverages LLMs to iteratively enhance reward functions, achieving
better fraud detection accuracy and demonstrating zero-shot capability.
Experiments with real-world data confirm the effectiveness, robustness, and
resilience of our LLM-enhanced RL framework through long-term evaluations,
underscoring the potential of LLMs in advancing industrial RL applications.

</details>


### [169] [Theory of periodic convolutional neural network](https://arxiv.org/abs/2509.18744)
*Yuqing Liu*

Main category: cs.LG

TL;DR: 介绍周期性CNN架构，给出近似定理，表明其适用于高维脊状结构问题，拓展CNN近似理论。


<details>
  <summary>Details</summary>
Motivation: 拓展CNN近似理论，寻找适用于特定高维脊状结构问题的架构。

Method: 引入周期性CNN架构，证明近似定理。

Result: 周期性CNN可近似d维输入空间中依赖d - 1个线性变量的脊函数，不适用于更低维情况。

Conclusion: 拓展了CNN近似理论的数学基础，强调周期性CNN有实用的近似能力。

Abstract: We introduce a novel convolutional neural network architecture, termed the
\emph{periodic CNN}, which incorporates periodic boundary conditions into the
convolutional layers. Our main theoretical contribution is a rigorous
approximation theorem: periodic CNNs can approximate ridge functions depending
on $d-1$ linear variables in a $d$-dimensional input space, while such
approximation is impossible in lower-dimensional ridge settings ($d-2$ or fewer
variables). This result establishes a sharp characterization of the expressive
power of periodic CNNs. Beyond the theory, our findings suggest that periodic
CNNs are particularly well-suited for problems where data naturally admits a
ridge-like structure of high intrinsic dimension, such as image analysis on
wrapped domains, physics-informed learning, and materials science. The work
thus both expands the mathematical foundation of CNN approximation theory and
highlights a class of architectures with surprising and practically relevant
approximation capabilities.

</details>


### [170] [MOMEMTO: Patch-based Memory Gate Model in Time Series Foundation Model](https://arxiv.org/abs/2509.18751)
*Samuel Yoon,Jongwon Kim,Juyoung Ha,Young Myoung Ko*

Main category: cs.LG

TL;DR: 提出用于异常检测的TFM模型MOMEMTO，通过基于补丁的内存模块减轻过泛化，实验显示其在多指标上表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于重建的深度模型用于时间序列异常检测时易过泛化，含内存架构的方法训练成本高且未有效集成到TFMs，需解决这些问题。

Method: 提出MOMEMTO模型，用基于补丁的内存模块，通过多域训练策略在多数据集上联合微调，用预训练编码器初始化内存项，组织成补丁级单元并通过注意力机制更新。

Result: 在23个单变量基准数据集上评估，MOMEMTO作为单一模型在AUC和VUS指标上得分高于基线方法，在少样本学习场景中提升骨干TFM性能。

Conclusion: MOMEMTO能有效减轻过泛化问题，在时间序列异常检测中表现良好。

Abstract: Recently reconstruction-based deep models have been widely used for time
series anomaly detection, but as their capacity and representation capability
increase, these models tend to over-generalize, often reconstructing unseen
anomalies accurately. Prior works have attempted to mitigate this by
incorporating a memory architecture that stores prototypes of normal patterns.
Nevertheless, these approaches suffer from high training costs and have yet to
be effectively integrated with time series foundation models (TFMs). To address
these challenges, we propose \textbf{MOMEMTO}, a TFM for anomaly detection,
enhanced with a patch-based memory module to mitigate over-generalization. The
memory module is designed to capture representative normal patterns from
multiple domains and enables a single model to be jointly fine-tuned across
multiple datasets through a multi-domain training strategy. MOMEMTO initializes
memory items with latent representations from a pre-trained encoder, organizes
them into patch-level units, and updates them via an attention mechanism. We
evaluate our method using 23 univariate benchmark datasets. Experimental
results demonstrate that MOMEMTO, as a single model, achieves higher scores on
AUC and VUS metrics compared to baseline methods, and further enhances the
performance of its backbone TFM, particularly in few-shot learning scenarios.

</details>


### [171] [Probabilistic Machine Learning for Uncertainty-Aware Diagnosis of Industrial Systems](https://arxiv.org/abs/2509.18810)
*Arman Mohammadi,Mattias Krysander,Daniel Jung,Erik Frisk*

Main category: cs.LG

TL;DR: 本文提出用集成概率机器学习的诊断框架，量化和自动化预测不确定性，提升基于数据一致性诊断的特性，经案例分析证明有改进效果。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络用于故障诊断时难以评估置信度，而在基于一致性的诊断中评估置信度很重要，需要解决此问题。

Method: 提出使用集成概率机器学习的诊断框架，量化和自动化预测不确定性。

Result: 通过多个案例的消融和对比分析，一系列诊断指标有持续改进。

Conclusion: 所提出的方法能有效提升基于数据一致性诊断的特性。

Abstract: Deep neural networks has been increasingly applied in fault diagnostics,
where it uses historical data
  to capture systems behavior, bypassing the need for high-fidelity physical
models.
  However, despite their competence in prediction tasks, these models often
struggle with
  the evaluation of their confidence. This matter is particularly
  important in consistency-based diagnosis where decision logic is highly
sensitive to false alarms.
  To address this challenge, this work presents a diagnostic framework that
uses
  ensemble probabilistic machine learning to
  improve diagnostic characteristics of data driven consistency based diagnosis
  by quantifying and automating the prediction uncertainty.
  The proposed method is evaluated across several case studies using both
ablation
  and comparative analyses, showing consistent improvements across a range of
diagnostic metrics.

</details>


### [172] [Training-Free Data Assimilation with GenCast](https://arxiv.org/abs/2509.18811)
*Thomas Savary,François Rozet,Gilles Louppe*

Main category: cs.LG

TL;DR: 提出用预训练扩散模型进行数据同化的轻量级通用方法，并以GenCast为例说明。


<details>
  <summary>Details</summary>
Motivation: 在多个学科中，需要从有噪声观测中估计动态系统状态，数据同化有广泛应用，故提出新方法。

Method: 基于粒子滤波算法类，利用预训练用于模拟动态系统的扩散模型进行数据同化，无需进一步训练。

Result: 未提及。

Conclusion: 未提及。

Abstract: Data assimilation is widely used in many disciplines such as meteorology,
oceanography, and robotics to estimate the state of a dynamical system from
noisy observations. In this work, we propose a lightweight and general method
to perform data assimilation using diffusion models pre-trained for emulating
dynamical systems. Our method builds on particle filters, a class of data
assimilation algorithms, and does not require any further training. As a
guiding example throughout this work, we illustrate our methodology on GenCast,
a diffusion-based model that generates global ensemble weather forecasts.

</details>


### [173] [Graph-based Clustering Revisited: A Relaxation of Kernel $k$-Means Perspective](https://arxiv.org/abs/2509.18826)
*Wenlong Lyu,Yuheng Jia,Hui Liu,Junhui Hou*

Main category: cs.LG

TL;DR: 本文指出图聚类方法过度松弛约束影响聚类效果，提出LoRD模型，结合块对角正则化得到B - LoRD，转换约束并提出优化算法，实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有图聚类方法过度松弛固有约束，可能限制聚类效果。

Method: 提出LoRD模型仅松弛正交约束得到概率聚类结果；证明双随机约束下正交性和块对角性等价，提出B - LoRD；将非凸双随机约束转换为线性凸约束；提出全局收敛投影梯度下降算法优化。

Result: 广泛实验验证了LoRD和B - LoRD方法的有效性。

Conclusion: 提出的LoRD和B - LoRD方法能有效提升聚类性能，且优化算法可全局收敛。

Abstract: The well-known graph-based clustering methods, including spectral clustering,
symmetric non-negative matrix factorization, and doubly stochastic
normalization, can be viewed as relaxations of the kernel $k$-means approach.
However, we posit that these methods excessively relax their inherent low-rank,
nonnegative, doubly stochastic, and orthonormal constraints to ensure numerical
feasibility, potentially limiting their clustering efficacy. In this paper,
guided by our theoretical analyses, we propose \textbf{Lo}w-\textbf{R}ank
\textbf{D}oubly stochastic clustering (\textbf{LoRD}), a model that only
relaxes the orthonormal constraint to derive a probabilistic clustering
results. Furthermore, we theoretically establish the equivalence between
orthogonality and block diagonality under the doubly stochastic constraint. By
integrating \textbf{B}lock diagonal regularization into LoRD, expressed as the
maximization of the Frobenius norm, we propose \textbf{B-LoRD}, which further
enhances the clustering performance. To ensure numerical solvability, we
transform the non-convex doubly stochastic constraint into a linear convex
constraint through the introduction of a class probability parameter. We
further theoretically demonstrate the gradient Lipschitz continuity of our LoRD
and B-LoRD enables the proposal of a globally convergent projected gradient
descent algorithm for their optimization. Extensive experiments validate the
effectiveness of our approaches. The code is publicly available at
https://github.com/lwl-learning/LoRD.

</details>


### [174] [Shared-Weights Extender and Gradient Voting for Neural Network Expansion](https://arxiv.org/abs/2509.18842)
*Nikolas Chatzis,Ioannis Kordonis,Manos Theodosis,Petros Maragos*

Main category: cs.LG

TL;DR: 提出SWE和SVoD方法用于神经网络扩展，抑制神经元不活跃并取得更好性能。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络训练中新增神经元易不活跃、无法促进容量增长的问题。

Method: 提出Shared - Weights Extender (SWE)防止新神经元不活跃，引入Steepest Voting Distributor (SVoD)在深度网络扩展时跨层分配神经元。

Result: 在四个数据集上的广泛基准测试显示，该方法能有效抑制神经元不活跃，性能优于其他扩展方法和基线。

Conclusion: 所提方法可有效解决神经网络扩展中神经元不活跃问题，有更好性能。

Abstract: Expanding neural networks during training is a promising way to augment
capacity without retraining larger models from scratch. However, newly added
neurons often fail to adjust to a trained network and become inactive,
providing no contribution to capacity growth. We propose the Shared-Weights
Extender (SWE), a novel method explicitly designed to prevent inactivity of new
neurons by coupling them with existing ones for smooth integration. In
parallel, we introduce the Steepest Voting Distributor (SVoD), a gradient-based
method for allocating neurons across layers during deep network expansion. Our
extensive benchmarking on four datasets shows that our method can effectively
suppress neuron inactivity and achieve better performance compared to other
expanding methods and baselines.

</details>


### [175] [NGRPO: Negative-enhanced Group Relative Policy Optimization](https://arxiv.org/abs/2509.18851)
*Gongrui Nan,Siye Chen,Jing Huang,Mengyu Lu,Dexun Wang,Chunmei Xie,Weiqi Xiong,Xianzhou Zeng,Qixuan Zhou,Yadong Li,Xingzhong Xu*

Main category: cs.LG

TL;DR: RLVR提升LLMs推理能力，但GRPO算法有局限，本文提出NGRPO算法解决问题，实验表明其效果好。


<details>
  <summary>Details</summary>
Motivation: GRPO算法在处理全对或全错的同质响应时无法学习，尤其是全错组，导致梯度为零、丢失学习信号。

Method: 提出NGRPO算法，包括引入优势校准机制和采用非对称裁剪。

Result: 在Qwen2.5 - Math - 7B上实验，NGRPO在多个数学基准测试中显著优于PPO、GRPO等基线模型。

Conclusion: NGRPO能从同质错误中学习，稳定且显著提升数学推理能力，代码开源。

Abstract: RLVR has enhanced the reasoning capabilities of Large Language Models (LLMs)
across various tasks. However, GRPO, a representative RLVR algorithm, suffers
from a critical limitation: when all responses within a group are either
entirely correct or entirely incorrect, the model fails to learn from these
homogeneous responses. This is particularly problematic for homogeneously
incorrect groups, where GRPO's advantage function yields a value of zero,
leading to null gradients and the loss of valuable learning signals. To
overcome this issue, we propose NGRPO (Negative-enhanced Group Relative Policy
Optimization), an algorithm designed to convert homogeneous errors into robust
learning signals. First, NGRPO introduces Advantage Calibration. This mechanism
hypothesizes the existence of a virtual maximum-reward sample during advantage
calculation, thereby altering the mean and variance of rewards within a group
and ensuring that the advantages for homogeneously incorrect samples are no
longer zero. Second, NGRPO employs Asymmetric Clipping, which relaxes the
update magnitude for positive samples while imposing stricter constraints on
that of negative samples. This serves to stabilize the exploration pressure
introduced by the advantage calibration. Our experiments on Qwen2.5-Math-7B
demonstrate that NGRPO significantly outperforms baselines such as PPO, GRPO,
DAPO, and PSR-NSR on mathematical benchmarks including MATH500, AMC23, and
AIME2025. These results validate NGRPO's ability to learn from homogeneous
errors, leading to stable and substantial improvements in mathematical
reasoning. Our code is available at https://github.com/nangongrui-ngr/NGRPO.

</details>


### [176] [Exploring Heterophily in Graph-level Tasks](https://arxiv.org/abs/2509.18893)
*Qinhan Hou,Yilun Zheng,Xichun Zhang,Sitao Luan,Jing Tang*

Main category: cs.LG

TL;DR: 论文首次分析图级学习中的异质性，结合理论与实证，揭示图级任务与节点级任务在频率特性上的差异，实验表明频率自适应模型表现更好。


<details>
  <summary>Details</summary>
Motivation: 异质性在节点级任务中被广泛研究，但在图级任务中的影响尚不明确，因此开展对图级学习中异质性的研究。

Method: 引入图级标签方案分类法，聚焦局部结构标签中的基于 motif 的任务，使用基于能量的梯度流分析。

Result: 实验表明频率自适应模型在合成数据集和真实世界分子属性预测中优于频率主导模型。

Conclusion: 建立了图级学习中异质性的新理论理解，为设计有效的 GNN 架构提供了指导。

Abstract: While heterophily has been widely studied in node-level tasks, its impact on
graph-level tasks remains unclear. We present the first analysis of heterophily
in graph-level learning, combining theoretical insights with empirical
validation. We first introduce a taxonomy of graph-level labeling schemes, and
focus on motif-based tasks within local structure labeling, which is a popular
labeling scheme. Using energy-based gradient flow analysis, we reveal a key
insight: unlike frequency-dominated regimes in node-level tasks, motif
detection requires mixed-frequency dynamics to remain flexible across multiple
spectral components. Our theory shows that motif objectives are inherently
misaligned with global frequency dominance, demanding distinct architectural
considerations. Experiments on synthetic datasets with controlled heterophily
and real-world molecular property prediction support our findings, showing that
frequency-adaptive model outperform frequency-dominated models. This work
establishes a new theoretical understanding of heterophily in graph-level
learning and offers guidance for designing effective GNN architectures.

</details>


### [177] [Enhancing the Effectiveness and Durability of Backdoor Attacks in Federated Learning through Maximizing Task Distinction](https://arxiv.org/abs/2509.18904)
*Zhaoxin Wang,Handing Wang,Cong Tian,Yaochu Jin*

Main category: cs.LG

TL;DR: 本文提出一种在联邦学习中解耦后门任务与主要任务的方法，经实验验证有良好攻击性能且易集成到现有后门攻击技术。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习的后门攻击方法存在主要任务与后门任务强耦合问题，易受诚实更新稀释，在防御机制下持久性有限。

Method: 在最小 - 最大框架内动态优化后门触发器，内层最大化中毒样本与良性样本的性能差距，外层将自适应触发器注入本地模型。

Result: 在计算机视觉和自然语言任务上评估该方法，与六种后门攻击方法在六种防御算法下对比，实验显示该方法有良好攻击性能。

Conclusion: 该方法能解耦后门任务与主要任务，可轻松集成到现有后门攻击技术中。

Abstract: Federated learning allows multiple participants to collaboratively train a
central model without sharing their private data. However, this distributed
nature also exposes new attack surfaces. In particular, backdoor attacks allow
attackers to implant malicious behaviors into the global model while
maintaining high accuracy on benign inputs. Existing attacks usually rely on
fixed patterns or adversarial perturbations as triggers, which tightly couple
the main and backdoor tasks. This coupling makes them vulnerable to dilution by
honest updates and limits their persistence under federated defenses. In this
work, we propose an approach to decouple the backdoor task from the main task
by dynamically optimizing the backdoor trigger within a min-max framework. The
inner layer maximizes the performance gap between poisoned and benign samples,
ensuring that the contributions of benign users have minimal impact on the
backdoor. The outer process injects the adaptive triggers into the local model.
We evaluate our method on both computer vision and natural language tasks, and
compare it with six backdoor attack methods under six defense algorithms.
Experimental results show that our method achieves good attack performance and
can be easily integrated into existing backdoor attack techniques.

</details>


### [178] [Tackling GNARLy Problems: Graph Neural Algorithmic Reasoning Reimagined through Reinforcement Learning](https://arxiv.org/abs/2509.18930)
*Alex Schutz,Victor-Alexandru Darvariu,Efimia Panagiotaki,Bruno Lacerda,Nick Hawes*

Main category: cs.LG

TL;DR: 本文针对神经算法推理（NAR）的局限性，将学习算法轨迹问题重构为马尔可夫决策过程，提出GNARL框架，取得良好实验结果。


<details>
  <summary>Details</summary>
Motivation: 解决NAR在构建有效解、处理组合NP难问题以及应用于无强算法问题时存在的局限性。

Method: 将学习算法轨迹问题重构为马尔可夫决策过程，提出GNARL框架，包含从NAR到强化学习（RL）的问题转换方法和适用于多种基于图问题的学习架构。

Result: 在几个CLRS - 30问题上实现了很高的图准确率，在NP难问题上性能与或超过更窄的NAR方法，甚至在缺乏专家算法时也适用。

Conclusion: 所提出的GNARL框架能有效解决NAR的局限性，具有良好的性能和适用性。

Abstract: Neural Algorithmic Reasoning (NAR) is a paradigm that trains neural networks
to execute classic algorithms by supervised learning. Despite its successes,
important limitations remain: inability to construct valid solutions without
post-processing and to reason about multiple correct ones, poor performance on
combinatorial NP-hard problems, and inapplicability to problems for which
strong algorithms are not yet known. To address these limitations, we reframe
the problem of learning algorithm trajectories as a Markov Decision Process,
which imposes structure on the solution construction procedure and unlocks the
powerful tools of imitation and reinforcement learning (RL). We propose the
GNARL framework, encompassing the methodology to translate problem formulations
from NAR to RL and a learning architecture suitable for a wide range of
graph-based problems. We achieve very high graph accuracy results on several
CLRS-30 problems, performance matching or exceeding much narrower NAR
approaches for NP-hard problems and, remarkably, applicability even when
lacking an expert algorithm.

</details>


### [179] [Towards Privacy-Aware Bayesian Networks: A Credal Approach](https://arxiv.org/abs/2509.18949)
*Niccolò Rocchi,Fabio Stella,Cassio de Campos*

Main category: cs.LG

TL;DR: 本文引入可信网络（CN）平衡贝叶斯网络（BN）模型的隐私性与实用性，通过实验证明CN是开发隐私感知概率图模型的有效方法。


<details>
  <summary>Details</summary>
Motivation: 现有公开贝叶斯网络模型未优先考虑隐私保护，引入噪声的保护技术会影响模型实用性，需要新方法平衡隐私与实用性。

Method: 引入CN，调整追踪攻击概念，证明CN可掩盖BN以降低攻击成功率，确定需隐藏的关键学习信息，通过数值实验分析调整CN超参数对隐私提升的影响。

Result: 实验结果表明CN能在保障隐私的同时进行有意义的推理。

Conclusion: CN是开发隐私感知概率图模型的原则性、实用性和有效性方法。

Abstract: Bayesian networks (BN) are probabilistic graphical models that enable
efficient knowledge representation and inference. These have proven effective
across diverse domains, including healthcare, bioinformatics and economics. The
structure and parameters of a BN can be obtained by domain experts or directly
learned from available data. However, as privacy concerns escalate, it becomes
increasingly critical for publicly released models to safeguard sensitive
information in training data. Typically, released models do not prioritize
privacy by design. In particular, tracing attacks from adversaries can combine
the released BN with auxiliary data to determine whether specific individuals
belong to the data from which the BN was learned. State-of-the-art protection
tecniques involve introducing noise into the learned parameters. While this
offers robust protection against tracing attacks, it significantly impacts the
model's utility, in terms of both the significance and accuracy of the
resulting inferences. Hence, high privacy may be attained at the cost of
releasing a possibly ineffective model. This paper introduces credal networks
(CN) as a novel solution for balancing the model's privacy and utility. After
adapting the notion of tracing attacks, we demonstrate that a CN enables the
masking of the learned BN, thereby reducing the probability of successful
attacks. As CNs are obfuscated but not noisy versions of BNs, they can achieve
meaningful inferences while safeguarding privacy. Moreover, we identify key
learning information that must be concealed to prevent attackers from
recovering the underlying BN. Finally, we conduct a set of numerical
experiments to analyze how privacy gains can be modulated by tuning the CN
hyperparameters. Our results confirm that CNs provide a principled, practical,
and effective approach towards the development of privacy-aware probabilistic
graphical models.

</details>


### [180] [Lift What You Can: Green Online Learning with Heterogeneous Ensembles](https://arxiv.org/abs/2509.18962)
*Kirsten Köbschall,Sebastian Buschjäger,Raphael Fischer,Lisa Hartung,Stefan Kramer*

Main category: cs.LG

TL;DR: 提出HEROS用于绿色在线学习，引入MDP权衡性能与可持续性，提出ζ - 策略，理论和实验证明其资源友好且性能佳。


<details>
  <summary>Details</summary>
Motivation: 现有流挖掘集成方法未充分考虑集成成员的计算开销，过于关注预测能力，为实现绿色在线学习提出解决方案。

Method: 提出HEROS，每步训练时在资源约束下从不同超参数初始化的模型池中选子集训练；引入马尔可夫决策过程；提出不同选择模型的策略，如ζ - 策略。

Result: 理论证明ζ - 策略能以更少资源达到接近最优性能；在11个基准数据集实验中，ζ - 策略表现准确，有时超越竞争对手且更节省资源。

Conclusion: ζ - 策略对现有技术有重要贡献，能实现高性能与资源友好的平衡。

Abstract: Ensemble methods for stream mining necessitate managing multiple models and
updating them as data distributions evolve. Considering the calls for more
sustainability, established methods are however not sufficiently considerate of
ensemble members' computational expenses and instead overly focus on predictive
capabilities. To address these challenges and enable green online learning, we
propose heterogeneous online ensembles (HEROS). For every training step, HEROS
chooses a subset of models from a pool of models initialized with diverse
hyperparameter choices under resource constraints to train. We introduce a
Markov decision process to theoretically capture the trade-offs between
predictive performance and sustainability constraints. Based on this framework,
we present different policies for choosing which models to train on incoming
data. Most notably, we propose the novel $\zeta$-policy, which focuses on
training near-optimal models at reduced costs. Using a stochastic model, we
theoretically prove that our $\zeta$-policy achieves near optimal performance
while using fewer resources compared to the best performing policy. In our
experiments across 11 benchmark datasets, we find empiric evidence that our
$\zeta$-policy is a strong contribution to the state-of-the-art, demonstrating
highly accurate performance, in some cases even outperforming competitors, and
simultaneously being much more resource-friendly.

</details>


### [181] [Otters: An Energy-Efficient SpikingTransformer via Optical Time-to-First-Spike Encoding](https://arxiv.org/abs/2509.18968)
*Zhanglu Yan,Jiayi Mao,Qianhui Liu,Fanfan Li,Gang Pan,Tao Luo,Bowen Zhu,Weng-Fai Wong*

Main category: cs.LG

TL;DR: 本文利用光电器件的自然信号衰减实现基于时间到首次脉冲（TTFS）编码的脉冲神经网络（SNN）核心计算，提出量化神经网络到SNN的转换算法，实现高效能SNN。


<details>
  <summary>Details</summary>
Motivation: 传统TTFS编码的SNN因需评估时间衰减函数和后续乘法运算，难以实现高能效，本文旨在解决该问题。

Method: 利用光电器件自然信号衰减实现TTFS核心计算，制造定制氧化铟光电子突触；引入量化神经网络到SNN的转换算法以用于复杂架构。

Result: 模型在七个GLUE基准数据集上达到了最先进的准确率，与之前领先的SNN相比，能效提高了1.77倍。

Conclusion: 建立了高效能SNN的新范式，将基础器件物理直接转化为强大的计算原语。

Abstract: Spiking neural networks (SNNs) promise high energy efficiency, particularly
with time-to-first-spike (TTFS) encoding, which maximizes sparsity by emitting
at most one spike per neuron. However, such energy advantage is often
unrealized because inference requires evaluating a temporal decay function and
subsequent multiplication with the synaptic weights. This paper challenges this
costly approach by repurposing a physical hardware `bug', namely, the natural
signal decay in optoelectronic devices, as the core computation of TTFS. We
fabricated a custom indium oxide optoelectronic synapse, showing how its
natural physical decay directly implements the required temporal function. By
treating the device's analog output as the fused product of the synaptic weight
and temporal decay, optoelectronic synaptic TTFS (named Otters) eliminates
these expensive digital operations. To use the Otters paradigm in complex
architectures like the transformer, which are challenging to train directly due
to the sparsity issue, we introduce a novel quantized neural network-to-SNN
conversion algorithm. This complete hardware-software co-design enables our
model to achieve state-of-the-art accuracy across seven GLUE benchmark datasets
and demonstrates a 1.77$\times$ improvement in energy efficiency over previous
leading SNNs, based on a comprehensive analysis of compute, data movement, and
memory access costs using energy measurements from a commercial 22nm process.
Our work thus establishes a new paradigm for energy-efficient SNNs, translating
fundamental device physics directly into powerful computational primitives. All
codes and data are open source.

</details>


### [182] [Learning From Simulators: A Theory of Simulation-Grounded Learning](https://arxiv.org/abs/2509.18990)
*Carson Dudley,Marisa Eisenberg*

Main category: cs.LG

TL;DR: 提出模拟接地学习基础理论，证明SGNNs实现贝叶斯推理、收敛到最优预测器等，实验验证理论，确立SGNNs作为数据受限下科学预测框架。


<details>
  <summary>Details</summary>
Motivation: SGNNs在现实标签有限领域有优异表现，但缺乏正式基础，需构建相关理论。

Method: 提出模拟接地学习理论，推导泛化边界，通过数值实验验证理论。

Result: SGNNs能恢复潜在参数、在失配下稳健，在模型选择任务中误差低于AIC。

Conclusion: SGNNs是数据受限情况下科学预测的原则性和实用性框架。

Abstract: Simulation-Grounded Neural Networks (SGNNs) are predictive models trained
entirely on synthetic data from mechanistic simulations. They have achieved
state-of-the-art performance in domains where real-world labels are limited or
unobserved, but lack a formal underpinning.
  We present the foundational theory of simulation-grounded learning. We show
that SGNNs implement amortized Bayesian inference under a simulation prior and
converge to the Bayes-optimal predictor. We derive generalization bounds under
model misspecification and prove that SGNNs can learn unobservable scientific
quantities that empirical methods provably cannot. We also formalize a novel
form of mechanistic interpretability uniquely enabled by SGNNs: by attributing
predictions to the simulated mechanisms that generated them, SGNNs yield
posterior-consistent, scientifically grounded explanations.
  We provide numerical experiments to validate all theoretical predictions.
SGNNs recover latent parameters, remain robust under mismatch, and outperform
classical tools: in a model selection task, SGNNs achieve half the error of AIC
in distinguishing mechanistic dynamics. These results establish SGNNs as a
principled and practical framework for scientific prediction in data-limited
regimes.

</details>


### [183] [CR-Net: Scaling Parameter-Efficient Training with Cross-Layer Low-Rank Structure](https://arxiv.org/abs/2509.18993)
*Boao Kong,Junzhu Liang,Yuxi Liu,Renjia Deng,Kun Yuan*

Main category: cs.LG

TL;DR: 提出跨层低秩残差网络CR - Net解决现有低秩方法问题，实验显示其优于现有低秩框架，资源需求少。


<details>
  <summary>Details</summary>
Motivation: 现有低秩方法存在模型性能受损、计算开销大、激活内存节省有限等问题，需要改进。

Method: 提出CR - Net，采用双路径架构结合前层输出与低秩差异重构层激活，开发专用激活重计算策略。

Result: 在60M到7B参数规模模型的预训练实验中，CR - Net始终优于现有低秩框架，且计算资源和内存需求更少。

Conclusion: CR - Net是一种有效的参数高效框架，能解决现有低秩方法的局限。

Abstract: Low-rank architectures have become increasingly important for efficient large
language model (LLM) pre-training, providing substantial reductions in both
parameter complexity and memory/computational demands. Despite these
advantages, current low-rank methods face three critical shortcomings: (1)
compromised model performance, (2) considerable computational overhead, and (3)
limited activation memory savings. To address these limitations, we propose
Cross-layer Low-Rank residual Network (CR-Net), an innovative
parameter-efficient framework inspired by our discovery that inter-layer
activation residuals possess low-rank properties. CR-Net implements this
insight through a dual-path architecture that efficiently reconstructs layer
activations by combining previous-layer outputs with their low-rank
differences, thereby maintaining high-rank information with minimal parameters.
We further develop a specialized activation recomputation strategy tailored for
CR-Net that dramatically reduces memory requirements. Extensive pre-training
experiments across model scales from 60M to 7B parameters demonstrate that
CR-Net consistently outperforms state-of-the-art low-rank frameworks while
requiring fewer computational resources and less memory.

</details>


### [184] [Theoretical Foundations of Representation Learning using Unlabeled Data: Statistics and Optimization](https://arxiv.org/abs/2509.18997)
*Pascal Esser,Maximilian Fleissner,Debarghya Ghoshdastidar*

Main category: cs.LG

TL;DR: 本文综述无标签数据表征学习的理论进展并提及自身贡献，因当前深度学习模型原理难用经典理论分析，需结合统计和优化工具解答相关问题。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型用于无监督表征学习的新原理难以用经典理论分析，且难以刻画模型学习的表征及解释其良好表现，需解答相关问题。

Method: 结合统计和优化的数学工具。

Result: 对无标签数据表征学习的近期理论进展进行了综述。

Conclusion: 结合统计和优化工具可助力解决当前无监督表征学习中存在的问题，文中给出了理论进展综述及自身贡献。

Abstract: Representation learning from unlabeled data has been extensively studied in
statistics, data science and signal processing with a rich literature on
techniques for dimension reduction, compression, multi-dimensional scaling
among others. However, current deep learning models use new principles for
unsupervised representation learning that cannot be easily analyzed using
classical theories. For example, visual foundation models have found tremendous
success using self-supervision or denoising/masked autoencoders, which
effectively learn representations from massive amounts of unlabeled data.
However, it remains difficult to characterize the representations learned by
these models and to explain why they perform well for diverse prediction tasks
or show emergent behavior. To answer these questions, one needs to combine
mathematical tools from statistics and optimization. This paper provides an
overview of recent theoretical advances in representation learning from
unlabeled data and mentions our contributions in this direction.

</details>


### [185] [Fully Learnable Neural Reward Machines](https://arxiv.org/abs/2509.19017)
*Hazem Dewidar,Elena Umili*

Main category: cs.LG

TL;DR: 提出全可学习的神经奖励机器（FLNRM），可端到端学习符号接地函数和自动机，结合DRL优于基于RNN的方法。


<details>
  <summary>Details</summary>
Motivation: 非马尔可夫强化学习任务有挑战，现有符号形式主义方法依赖限制性假设，需去除对先验知识的依赖。

Method: 提出全可学习的神经奖励机器（NRM），可端到端学习符号接地函数和自动机，并将其与深度强化学习（DRL）集成。

Result: 方法比基于循环神经网络（RNN）的先前方法表现更优。

Conclusion: 提出的方法既像经典DRL方法一样易于应用，又因自动机的有限和紧凑性而更具可解释性。

Abstract: Non-Markovian Reinforcement Learning (RL) tasks present significant
challenges, as agents must reason over entire trajectories of state-action
pairs to make optimal decisions. A common strategy to address this is through
symbolic formalisms, such as Linear Temporal Logic (LTL) or automata, which
provide a structured way to express temporally extended objectives. However,
these approaches often rely on restrictive assumptions -- such as the
availability of a predefined Symbol Grounding (SG) function mapping raw
observations to high-level symbolic representations, or prior knowledge of the
temporal task. In this work, we propose a fully learnable version of Neural
Reward Machines (NRM), which can learn both the SG function and the automaton
end-to-end, removing any reliance on prior knowledge. Our approach is therefore
as easily applicable as classic deep RL (DRL) approaches, while being far more
explainable, because of the finite and compact nature of automata. Furthermore,
we show that by integrating Fully Learnable Reward Machines (FLNRM) with DRL,
our method outperforms previous approaches based on Recurrent Neural Networks
(RNNs).

</details>


### [186] [OmniBridge: Unified Multimodal Understanding, Generation, and Retrieval via Latent Space Alignment](https://arxiv.org/abs/2509.19018)
*Teng Xiao,Zuchao Li,Lefei Zhang*

Main category: cs.LG

TL;DR: 提出统一模块化多模态框架OmniBridge，支持视觉语言理解、生成和检索任务，实验表现佳。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型解决方案孤立处理任务或需从头训练，计算成本高且跨模态泛化性有限。

Method: 采用以语言为中心设计，复用预训练大语言模型，引入轻量级双向潜在对齐模块；提出两阶段解耦训练策略。

Result: 在广泛基准测试中，OmniBridge在三项任务上达到有竞争力或最先进的性能。

Conclusion: 潜在空间对齐在共享表示空间下统一多模态建模是有效的。

Abstract: Recent advances in multimodal large language models (LLMs) have led to
significant progress in understanding, generation, and retrieval tasks.
However, current solutions often treat these tasks in isolation or require
training LLMs from scratch, resulting in high computational costs and limited
generalization across modalities. In this work, we present OmniBridge, a
unified and modular multimodal framework that supports vision-language
understanding, generation, and retrieval within a unified architecture.
OmniBridge adopts a language-centric design that reuses pretrained LLMs and
introduces a lightweight bidirectional latent alignment module. To address the
challenge of task interference, we propose a two-stage decoupled training
strategy: supervised fine-tuning and latent space alignment for aligning LLM
behavior with multimodal reasoning, and semantic-guided diffusion training to
align cross-modal latent spaces via learnable query embeddings. Extensive
experiments across a wide range of benchmarks demonstrate that OmniBridge
achieves competitive or state-of-the-art performance in all three tasks.
Moreover, our results highlight the effectiveness of latent space alignment for
unifying multimodal modeling under a shared representation space. Code and
models are released at https://github.com/xiao-xt/OmniBridge.

</details>


### [187] [Improving Credit Card Fraud Detection through Transformer-Enhanced GAN Oversampling](https://arxiv.org/abs/2509.19032)
*Kashaf Ul Emaan*

Main category: cs.LG

TL;DR: 提出基于Transformer的GAN混合方法生成信用卡欺诈交易样本，测试显示在多个指标上有显著提升，能克服欺诈检测中的类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统过采样方法生成简单合成样本，现有模型在高维依赖建模有问题，需要有效解决信用卡欺诈检测中数据集高度不平衡问题。

Method: 使用带Transformer编码器块的GAN生成欺诈交易样本，在公开数据集上测试并与多种分类器搭配和传统及生成式重采样策略比较。

Result: 基于Transformer的GAN在召回率、F1分数和AUC上有显著提升。

Conclusion: 该方法能有效克服欺诈检测中严重的类别不平衡问题。

Abstract: Detection of credit card fraud is an acute issue of financial security
because transaction datasets are highly lopsided, with fraud cases being only a
drop in the ocean. Balancing datasets using the most popular methods of
traditional oversampling such as the Synthetic Minority Oversampling Technique
(SMOTE) generally create simplistic synthetic samples that are not readily
applicable to complex fraud patterns. Recent industry advances that include
Conditional Tabular Generative Adversarial Networks (CTGAN) and Tabular
Variational Autoencoders (TVAE) have demonstrated increased efficiency in
tabular synthesis, yet all these models still exhibit issues with
high-dimensional dependence modelling. Now we will present our hybrid approach
where we use a Generative Adversarial Network (GAN) with a Transformer encoder
block to produce realistic fraudulent transactions samples. The GAN
architecture allows training realistic generators adversarial, and the
Transformer allows the model to learn rich feature interactions by
self-attention. Such a hybrid strategy overcomes the limitations of SMOTE,
CTGAN, and TVAE by producing a variety of high-quality synthetic minority
classes samples. We test our algorithm on the publicly-available Credit Card
Fraud Detection dataset and compare it to conventional and generative
resampling strategies with a variety of classifiers, such as Logistic
Regression (LR), Random Forest (RF), Extreme Gradient Boosting (XGBoost), and
Support Vector Machine (SVM). Findings indicate that our Transformer-based GAN
shows substantial gains in Recall, F1-score and Area Under the Receiver
Operating Characteristic Curve (AUC), which indicates that it is effective in
overcoming the severe class imbalance inherent in the task of fraud detection.

</details>


### [188] [Latent Danger Zone: Distilling Unified Attention for Cross-Architecture Black-box Attacks](https://arxiv.org/abs/2509.19044)
*Yang Li,Chenyu Wang,Tingrui Wang,Yongwei Wang,Haonan Li,Zhunga Liu,Quan Pan*

Main category: cs.LG

TL;DR: 提出JAD框架用于黑盒对抗攻击，能提高攻击泛化性、生成效率和跨架构转移性。


<details>
  <summary>Details</summary>
Motivation: 现有黑盒对抗攻击方法依赖特定网络架构、查询成本高、跨架构转移性有限。

Method: 提出JAD框架，利用从CNN和ViT模型提取的注意力图引导的潜在扩散模型生成对抗样本。

Result: 实验表明JAD在攻击泛化性、生成效率和跨架构转移性上优于现有方法。

Conclusion: JAD为黑盒对抗攻击提供了有前景且有效的范式。

Abstract: Black-box adversarial attacks remain challenging due to limited access to
model internals. Existing methods often depend on specific network
architectures or require numerous queries, resulting in limited
cross-architecture transferability and high query costs. To address these
limitations, we propose JAD, a latent diffusion model framework for black-box
adversarial attacks. JAD generates adversarial examples by leveraging a latent
diffusion model guided by attention maps distilled from both a convolutional
neural network (CNN) and a Vision Transformer (ViT) models. By focusing on
image regions that are commonly sensitive across architectures, this approach
crafts adversarial perturbations that transfer effectively between different
model types. This joint attention distillation strategy enables JAD to be
architecture-agnostic, achieving superior attack generalization across diverse
models. Moreover, the generative nature of the diffusion framework yields high
adversarial sample generation efficiency by reducing reliance on iterative
queries. Experiments demonstrate that JAD offers improved attack
generalization, generation efficiency, and cross-architecture transferability
compared to existing methods, providing a promising and effective paradigm for
black-box adversarial attacks.

</details>


### [189] [Beyond Backpropagation: Exploring Innovative Algorithms for Energy-Efficient Deep Neural Network Training](https://arxiv.org/abs/2509.19063)
*Przemysław Spyra*

Main category: cs.LG

TL;DR: 本文研究三种无反向传播（BP）训练方法，对比测试后发现Mono - Forward（MF）算法在分类准确率、能耗和训练时间上表现出色，为未来节能深度学习提供路线图。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络因BP带来的计算和能源需求增长，挑战可持续AI发展，故研究BP-free训练方法。

Method: 建立对比框架，在原生架构实现各算法并与BP训练模型对比，用Optuna优化超参数，基于验证性能设置早停标准。

Result: MF在分类准确率上超BP，能收敛到验证损失更优最小值，降低能耗达41%，缩短训练时间达34%，并进行硬件级分析。

Conclusion: 本文为未来节能深度学习提供清晰、数据驱动的路线图。

Abstract: The rising computational and energy demands of deep neural networks (DNNs),
driven largely by backpropagation (BP), challenge sustainable AI development.
This paper rigorously investigates three BP-free training methods: the
Forward-Forward (FF), Cascaded-Forward (CaFo), and Mono-Forward (MF)
algorithms, tracing their progression from foundational concepts to a
demonstrably superior solution.
  A robust comparative framework was established: each algorithm was
implemented on its native architecture (MLPs for FF and MF, a CNN for CaFo) and
benchmarked against an equivalent BP-trained model. Hyperparameters were
optimized with Optuna, and consistent early stopping criteria were applied
based on validation performance, ensuring all models were optimally tuned
before comparison.
  Results show that MF not only competes with but consistently surpasses BP in
classification accuracy on its native MLPs. Its superior generalization stems
from converging to a more favorable minimum in the validation loss landscape,
challenging the assumption that global optimization is required for
state-of-the-art results. Measured at the hardware level using the NVIDIA
Management Library (NVML) API, MF reduces energy consumption by up to 41% and
shortens training time by up to 34%, translating to a measurably smaller carbon
footprint as estimated by CodeCarbon.
  Beyond this primary result, we present a hardware-level analysis that
explains the efficiency gains: exposing FF's architectural inefficiencies,
validating MF's computationally lean design, and challenging the assumption
that all BP-free methods are inherently more memory-efficient. By documenting
the evolution from FF's conceptual groundwork to MF's synthesis of accuracy and
sustainability, this work offers a clear, data-driven roadmap for future
energy-efficient deep learning.

</details>


### [190] [Diffusion Bridge Variational Inference for Deep Gaussian Processes](https://arxiv.org/abs/2509.19078)
*Jian Xu,Qibin Zhao,John Paisley,Delu Zeng*

Main category: cs.LG

TL;DR: 提出Diffusion Bridge Variational Inference (DBVI)用于大规模深度高斯过程后验推断，在多项任务中表现优于DDVI和其他基线方法。


<details>
  <summary>Details</summary>
Motivation: DDVI在深度高斯过程后验推断中，其固定无条件起始分布远离真实后验，导致推理轨迹低效和收敛缓慢。

Method: 提出DBVI，从可学习、依赖数据的初始分布开始反向扩散，通过摊销神经网络参数化初始化，并利用ELBO目标的梯度进行调整；设计网络在诱导输入上操作以实现可扩展摊销；重新解释先验，推导可处理的训练目标。

Result: 在回归、分类和图像重建任务中，DBVI在预测准确性、收敛速度和后验质量方面始终优于DDVI和其他变分基线。

Conclusion: DBVI是对DDVI的有效扩展，能更高效地进行大规模深度高斯过程的后验推断。

Abstract: Deep Gaussian processes (DGPs) enable expressive hierarchical Bayesian
modeling but pose substantial challenges for posterior inference, especially
over inducing variables. Denoising diffusion variational inference (DDVI)
addresses this by modeling the posterior as a time-reversed diffusion from a
simple Gaussian prior. However, DDVI's fixed unconditional starting
distribution remains far from the complex true posterior, resulting in
inefficient inference trajectories and slow convergence. In this work, we
propose Diffusion Bridge Variational Inference (DBVI), a principled extension
of DDVI that initiates the reverse diffusion from a learnable, data-dependent
initial distribution. This initialization is parameterized via an amortized
neural network and progressively adapted using gradients from the ELBO
objective, reducing the posterior gap and improving sample efficiency. To
enable scalable amortization, we design the network to operate on the inducing
inputs, which serve as structured, low-dimensional summaries of the dataset and
naturally align with the inducing variables' shape. DBVI retains the
mathematical elegance of DDVI, including Girsanov-based ELBOs and reverse-time
SDEs,while reinterpreting the prior via a Doob-bridged diffusion process. We
derive a tractable training objective under this formulation and implement DBVI
for scalable inference in large-scale DGPs. Across regression, classification,
and image reconstruction tasks, DBVI consistently outperforms DDVI and other
variational baselines in predictive accuracy, convergence speed, and posterior
quality.

</details>


### [191] [Graph Neural Networks with Similarity-Navigated Probabilistic Feature Copying](https://arxiv.org/abs/2509.19084)
*Asela Hevapathige*

Main category: cs.LG

TL;DR: 本文提出新型GNN架构AxelGNN解决现有GNN局限性，实验显示其表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有GNN存在特征过平滑、难以处理异质关系和处理特征向量缺乏灵活性等局限，需解决这些问题。

Method: 提出受Axelrod文化传播模型启发的AxelGNN，采用相似性门控概率交互、特征片段级复制机制和保持全局极化。

Result: 在节点分类和影响估计基准测试中，AxelGNN在不同同质性 - 异质性特征的图结构上始终优于或媲美现有最先进的GNN方法。

Conclusion: AxelGNN能有效解决现有GNN局限性，在多种图结构上表现良好。

Abstract: Graph Neural Networks (GNNs) have demonstrated remarkable success across
various graph-based tasks. However, they face some fundamental limitations:
feature oversmoothing can cause node representations to become
indistinguishable in deeper networks, they struggle to effectively manage
heterogeneous relationships where connected nodes differ significantly, and
they process entire feature vectors as indivisible units, which limits
flexibility. We seek to address these limitations. We propose AxelGNN, a novel
GNN architecture inspired by Axelrod's cultural dissemination model that
addresses these limitations through a unified framework. AxelGNN incorporates
similarity-gated probabilistic interactions that adaptively promote convergence
or divergence based on node similarity, implements trait-level copying
mechanisms for fine-grained feature aggregation at the segment level, and
maintains global polarization to preserve node distinctiveness across multiple
representation clusters. The model's bistable convergence dynamics naturally
handle both homophilic and heterophilic graphs within a single architecture.
Extensive experiments on node classification and influence estimation
benchmarks demonstrate that AxelGNN consistently outperforms or matches
state-of-the-art GNN methods across diverse graph structures with varying
homophily-heterophily characteristics.

</details>


### [192] [Asymptotically Optimal Problem-Dependent Bandit Policies for Transfer Learning](https://arxiv.org/abs/2509.19098)
*Adrien Prevost,Timothee Mathieu,Odalric-Ambrym Maillard*

Main category: cs.LG

TL;DR: 研究迁移学习下非上下文多臂老虎机问题，推导累积遗憾下界，提出KL - UCB - Transfer策略并通过模拟验证其性能。


<details>
  <summary>Details</summary>
Motivation: 解决迁移学习设置下非上下文多臂老虎机问题，扩展经典结果以纳入迁移参数。

Method: 先推导依赖问题的累积遗憾渐近下界，再提出KL - UCB - Transfer简单索引策略。

Result: 在高斯情况下，KL - UCB - Transfer策略匹配新下界，模拟显示当源和目标分布足够接近时，该策略显著优于无先验基线。

Conclusion: KL - UCB - Transfer策略在迁移学习下的非上下文多臂老虎机问题中表现良好，在源和目标分布接近时具有优势。

Abstract: We study the non-contextual multi-armed bandit problem in a transfer learning
setting: before any pulls, the learner is given N'_k i.i.d. samples from each
source distribution nu'_k, and the true target distributions nu_k lie within a
known distance bound d_k(nu_k, nu'_k) <= L_k. In this framework, we first
derive a problem-dependent asymptotic lower bound on cumulative regret that
extends the classical Lai-Robbins result to incorporate the transfer parameters
(d_k, L_k, N'_k). We then propose KL-UCB-Transfer, a simple index policy that
matches this new bound in the Gaussian case. Finally, we validate our approach
via simulations, showing that KL-UCB-Transfer significantly outperforms the
no-prior baseline when source and target distributions are sufficiently close.

</details>


### [193] [Algorithms for Adversarially Robust Deep Learning](https://arxiv.org/abs/2509.19100)
*Alexander Robey*

Main category: cs.LG

TL;DR: 本文探讨设计具鲁棒性算法的进展，涵盖计算机视觉对抗样本、领域泛化及大语言模型越狱问题，并给出相应新成果。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型广泛用于安全关键应用，确保其决策对对抗性攻击具有鲁棒性至关重要。

Method: 针对计算机视觉对抗样本引入新技术成果、训练范式和认证算法；针对领域泛化提出新算法；针对大语言模型越狱问题提出新攻击和防御方法。

Result: 在计算机视觉、医学成像、分子识别、图像分类等领域取得成果，提出的新算法达当前最优泛化水平，针对大语言模型越狱的新攻击和防御代表了设计鲁棒语言智能体的前沿进展。

Conclusion: 所提出的方法和成果有助于提升深度学习模型在不同场景下的鲁棒性。

Abstract: Given the widespread use of deep learning models in safety-critical
applications, ensuring that the decisions of such models are robust against
adversarial exploitation is of fundamental importance. In this thesis, we
discuss recent progress toward designing algorithms that exhibit desirable
robustness properties. First, we discuss the problem of adversarial examples in
computer vision, for which we introduce new technical results, training
paradigms, and certification algorithms. Next, we consider the problem of
domain generalization, wherein the task is to train neural networks to
generalize from a family of training distributions to unseen test
distributions. We present new algorithms that achieve state-of-the-art
generalization in medical imaging, molecular identification, and image
classification. Finally, we study the setting of jailbreaking large language
models (LLMs), wherein an adversarial user attempts to design prompts that
elicit objectionable content from an LLM. We propose new attacks and defenses,
which represent the frontier of progress toward designing robust language-based
agents.

</details>


### [194] [Towards Practical Multi-label Causal Discovery in High-Dimensional Event Sequences via One-Shot Graph Aggregation](https://arxiv.org/abs/2509.19112)
*Hugo Math,Rainer Lienhart*

Main category: cs.LG

TL;DR: 介绍CARGO方法用于事件序列因果发现，在汽车故障预测数据集验证其结构化推理能力。


<details>
  <summary>Details</summary>
Motivation: 理解事件序列因果关系在多领域是未解决的挑战，有分析需求。

Method: 引入CARGO方法，用两个预训练因果Transformer作为基础模型，并行推理单序列因果图，用自适应频率融合聚合以重建标签全局马尔可夫边界。

Result: 在含超29100种事件类型和474个不平衡标签的汽车故障预测数据集上，CARGO展现出结构化推理能力。

Conclusion: CARGO方法能有效进行大规模概率推理，避开全数据集条件独立性测试的高昂成本。

Abstract: Understanding causality in event sequences where outcome labels such as
diseases or system failures arise from preceding events like symptoms or error
codes is critical. Yet remains an unsolved challenge across domains like
healthcare or vehicle diagnostics. We introduce CARGO, a scalable multi-label
causal discovery method for sparse, high-dimensional event sequences comprising
of thousands of unique event types. Using two pretrained causal Transformers as
domain-specific foundation models for event sequences. CARGO infers in
parallel, per sequence one-shot causal graphs and aggregates them using an
adaptive frequency fusion to reconstruct the global Markov boundaries of
labels. This two-stage approach enables efficient probabilistic reasoning at
scale while bypassing the intractable cost of full-dataset conditional
independence testing. Our results on a challenging real-world automotive fault
prediction dataset with over 29,100 unique event types and 474 imbalanced
labels demonstrate CARGO's ability to perform structured reasoning.

</details>


### [195] [Analysis on distribution and clustering of weight](https://arxiv.org/abs/2509.19122)
*Chunming Ye,Wenquan Tian,Yalan Gao,Songzhou Li*

Main category: cs.LG

TL;DR: 提出标准差向量和聚类向量描述大语言模型权重特征，可区分不同模型，且发现标准差向量受数据集影响，聚类向量不受影响。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型架构和参数特征，关注权重特性以分析模型间相关性和差异。

Method: 提出标准差向量，假设权重服从正态分布，对投影矩阵标准差归一化；提出聚类向量，提取权重投影矩阵奇异值并用K - Means算法分组，相同类型矩阵数据组合。

Result: 两种向量能有效区分不同模型，显示同系列模型相似性；LoRA微调后，标准差向量表示的权重分布受数据集影响，聚类向量表示的权重相关性不受影响。

Conclusion: 标准差向量和聚类向量可用于区分模型，且聚类向量在微调后能保持与预训练模型的高一致性。

Abstract: The study on architecture and parameter characteristics remains the hot topic
in the research of large language models. In this paper we concern with the
characteristics of weight which are used to analyze the correlations and
differences between models. Two kinds of vectors-standard deviation vector and
clustering vector-are proposed to describe features of models. In the first
case, the weights are assumed to follow normal distribution. The standard
deviation values of projection matrices are normalized to form
Standard-Deviation Vector, representing the distribution characteristics of
models. In the second case, the singular values from each weight projection
matrix are extracted and grouped by K-Means algorithm. The grouped data with
the same type matrix are combined as Clustering Vector to represent the
correlation characteristics of models' weights. The study reveals that these
two vectors can effectively distinguish between different models and clearly
show the similarities among models of the same family. Moreover, after
conducting LoRA fine-tuning with different datasets and models, it is found
that the distribution of weights represented by standard deviation vector is
directly influenced by the dataset, but the correlations between different
weights represented by clustering vector remain unaffected and maintain a high
consistency with the pre-trained model.

</details>


### [196] [PipelineRL: Faster On-policy Reinforcement Learning for Long Sequence Generatio](https://arxiv.org/abs/2509.19128)
*Alexandre Piché,Ehsan Kamaloo,Rafael Pardinas,Dzmitry Bahdanau*

Main category: cs.LG

TL;DR: 本文提出PipelineRL方法用于LLM训练，能平衡硬件效率和数据策略性，实验显示学习速度约快2倍并开源实现。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在扩展用于LLM训练时，难以在保持高加速器利用率的同时避免产生有害的过时离策略数据。

Method: 采用并发异步数据生成和模型训练，通过新颖的飞行中权重更新机制，使LLM生成引擎在生成令牌序列时能最小中断地接收更新的模型权重。

Result: 在128个H100 GPU上的长形式推理任务实验中，PipelineRL比传统RL基线学习速度快约2倍，且训练数据高度符合策略。

Conclusion: PipelineRL能在硬件效率和数据策略性之间取得良好平衡，且已开源可扩展和模块化的实现。

Abstract: Reinforcement Learning (RL) is increasingly utilized to enhance the reasoning
capabilities of Large Language Models (LLMs). However, effectively scaling
these RL methods presents significant challenges, primarily due to the
difficulty in maintaining high AI accelerator utilization without generating
stale, off-policy data that harms common RL algorithms. This paper introduces
PipelineRL, an approach designed to achieve a superior trade-off between
hardware efficiency and data on-policyness for LLM training. PipelineRL employs
concurrent asynchronous data generation and model training, distinguished by
the novel in-flight weight updates. This mechanism allows the LLM generation
engine to receive updated model weights with minimal interruption during the
generation of token sequences, thereby maximizing both the accelerator
utilization and the freshness of training data. Experiments conducted on
long-form reasoning tasks using 128 H100 GPUs demonstrate that PipelineRL
achieves approximately $\sim 2x$ faster learning compared to conventional RL
baselines while maintaining highly on-policy training data. A scalable and
modular open-source implementation of PipelineRL is also released as a key
contribution.

</details>


### [197] [GSTM-HMU: Generative Spatio-Temporal Modeling for Human Mobility Understanding](https://arxiv.org/abs/2509.19135)
*Wenying Luo,Zhiyuan Lin,Wenhao Xu,Minghao Liu,Zhi Li*

Main category: cs.LG

TL;DR: 本文提出GSTM - HMU生成式时空框架用于人类移动性分析，在四个数据集三个基准任务上实验，结果优于基线，表明该框架有效且生成式建模有潜力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理人类移动语义和时间复杂性上存在不足，需一种新框架推进移动性分析。

Method: 提出GSTM - HMU框架，包含时空概念编码器、认知轨迹记忆、生活方式概念库和面向任务的生成头。

Result: 在Gowalla、WeePlace、Brightkite和FourSquare四个数据集的三个基准任务实验中，显著优于强基线。

Conclusion: GSTM - HMU框架能从复杂移动数据中提取语义规律，生成式建模为人类移动智能系统提供了有前景的基础。

Abstract: Human mobility traces, often recorded as sequences of check-ins, provide a
unique window into both short-term visiting patterns and persistent lifestyle
regularities. In this work we introduce GSTM-HMU, a generative spatio-temporal
framework designed to advance mobility analysis by explicitly modeling the
semantic and temporal complexity of human movement. The framework consists of
four key innovations. First, a Spatio-Temporal Concept Encoder (STCE)
integrates geographic location, POI category semantics, and periodic temporal
rhythms into unified vector representations. Second, a Cognitive Trajectory
Memory (CTM) adaptively filters historical visits, emphasizing recent and
behaviorally salient events in order to capture user intent more effectively.
Third, a Lifestyle Concept Bank (LCB) contributes structured human preference
cues, such as activity types and lifestyle patterns, to enhance
interpretability and personalization. Finally, task-oriented generative heads
transform the learned representations into predictions for multiple downstream
tasks. We conduct extensive experiments on four widely used real-world
datasets, including Gowalla, WeePlace, Brightkite, and FourSquare, and evaluate
performance on three benchmark tasks: next-location prediction, trajectory-user
identification, and time estimation. The results demonstrate consistent and
substantial improvements over strong baselines, confirming the effectiveness of
GSTM-HMU in extracting semantic regularities from complex mobility data. Beyond
raw performance gains, our findings also suggest that generative modeling
provides a promising foundation for building more robust, interpretable, and
generalizable systems for human mobility intelligence.

</details>


### [198] [Efficient Reinforcement Learning by Reducing Forgetting with Elephant Activation Functions](https://arxiv.org/abs/2509.19159)
*Qingfeng Lan,Gautham Vasan,A. Rupam Mahmood*

Main category: cs.LG

TL;DR: 研究激活函数对强化学习中灾难性遗忘的影响，提出大象激活函数改善网络抗遗忘能力。


<details>
  <summary>Details</summary>
Motivation: 现有缓解灾难性遗忘的方法主要关注算法方面，缺乏对神经网络架构特性导致该问题的理解。

Method: 研究激活函数在神经网络训练动态中的作用，提出大象激活函数。

Result: 发现激活函数的梯度稀疏性对减少遗忘有重要作用，用大象激活函数替换经典激活函数可显著提高网络抗遗忘能力。

Conclusion: 大象激活函数能使强化学习更具样本和内存效率。

Abstract: Catastrophic forgetting has remained a significant challenge for efficient
reinforcement learning for decades (Ring 1994, Rivest and Precup 2003). While
recent works have proposed effective methods to mitigate this issue, they
mainly focus on the algorithmic side. Meanwhile, we do not fully understand
what architectural properties of neural networks lead to catastrophic
forgetting. This study aims to fill this gap by studying the role of activation
functions in the training dynamics of neural networks and their impact on
catastrophic forgetting in reinforcement learning setup. Our study reveals
that, besides sparse representations, the gradient sparsity of activation
functions also plays an important role in reducing forgetting. Based on this
insight, we propose a new class of activation functions, elephant activation
functions, that can generate both sparse outputs and sparse gradients. We show
that by simply replacing classical activation functions with elephant
activation functions in the neural networks of value-based algorithms, we can
significantly improve the resilience of neural networks to catastrophic
forgetting, thus making reinforcement learning more sample-efficient and
memory-efficient.

</details>


### [199] [A Validation Strategy for Deep Learning Models: Evaluating and Enhancing Robustness](https://arxiv.org/abs/2509.19197)
*Abdul-Rauf Nuhu,Parham Kebria,Vahid Hemmati,Benjamin Lartey,Mahmoud Nabil Mahmoud,Abdollah Homaifar,Edward Tunstel*

Main category: cs.LG

TL;DR: 提出从训练数据集中提取‘弱鲁棒’样本进行模型鲁棒性验证的方法，在多个数据集上证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 数据驱动模型易受数据扰动影响，传统鲁棒性验证方法有局限，需更好的验证方法提升模型可靠性。

Method: 通过局部鲁棒性分析从训练数据集中提取‘弱鲁棒’样本，用这些样本评估模型。

Result: 在CIFAR - 10、CIFAR - 100和ImageNet训练的模型上证明该方法有效。

Conclusion: 基于‘弱鲁棒’样本的鲁棒性验证能在对抗和常见数据损坏场景下提升模型可靠性。

Abstract: Data-driven models, especially deep learning classifiers often demonstrate
great success on clean datasets. Yet, they remain vulnerable to common data
distortions such as adversarial and common corruption perturbations. These
perturbations can significantly degrade performance, thereby challenging the
overall reliability of the models. Traditional robustness validation typically
relies on perturbed test datasets to assess and improve model performance. In
our framework, however, we propose a validation approach that extracts "weak
robust" samples directly from the training dataset via local robustness
analysis. These samples, being the most susceptible to perturbations, serve as
an early and sensitive indicator of the model's vulnerabilities. By evaluating
models on these challenging training instances, we gain a more nuanced
understanding of its robustness, which informs targeted performance
enhancement. We demonstrate the effectiveness of our approach on models trained
with CIFAR-10, CIFAR-100, and ImageNet, highlighting how robustness validation
guided by weak robust samples can drive meaningful improvements in model
reliability under adversarial and common corruption scenarios.

</details>


### [200] [PPG-Distill: Efficient Photoplethysmography Signals Analysis via Foundation Model Distillation](https://arxiv.org/abs/2509.19215)
*Juntong Ni,Saurabh Kataria,Shengpu Tang,Carl Yang,Xiao Hu,Wei Jin*

Main category: cs.LG

TL;DR: 提出PPG - Distill知识蒸馏框架用于可穿戴设备上的PPG分析，提升性能并提高推理速度、减少内存使用。


<details>
  <summary>Details</summary>
Motivation: 可穿戴设备资源有限，难以部署大型PPG基础模型，需高效分析方法。

Method: 提出PPG - Distill框架，通过预测、特征和补丁级蒸馏传递全局和局部知识，包含形态蒸馏和节奏蒸馏。

Result: 在心率估计和房颤检测上，学生模型性能提升达21.8%，推理速度快7倍，内存使用减少19倍。

Conclusion: PPG - Distill能实现可穿戴设备上高效的PPG分析。

Abstract: Photoplethysmography (PPG) is widely used in wearable health monitoring, yet
large PPG foundation models remain difficult to deploy on resource-limited
devices. We present PPG-Distill, a knowledge distillation framework that
transfers both global and local knowledge through prediction-, feature-, and
patch-level distillation. PPG-Distill incorporates morphology distillation to
preserve local waveform patterns and rhythm distillation to capture inter-patch
temporal structures. On heart rate estimation and atrial fibrillation
detection, PPG-Distill improves student performance by up to 21.8% while
achieving 7X faster inference and reducing memory usage by 19X, enabling
efficient PPG analysis on wearables

</details>


### [201] [Video Killed the Energy Budget: Characterizing the Latency and Power Regimes of Open Text-to-Video Models](https://arxiv.org/abs/2509.19222)
*Julien Delavande,Regis Pierrard,Sasha Luccioni*

Main category: cs.LG

TL;DR: 本文系统研究了开源文本到视频（T2V）模型的延迟和能耗，建立分析模型并实验验证，还对比多个模型，为设计和部署可持续生成视频系统提供参考。


<details>
  <summary>Details</summary>
Motivation: 当前T2V系统计算成本高且能耗情况不明，需要对其进行研究。

Method: 先开发计算约束分析模型预测缩放规律，再在WAN2.1 - T2V上进行细粒度实验验证，最后扩展分析到六个不同T2V模型。

Result: 实验表明时空维度呈二次增长，去噪步数呈线性缩放，对比了多个模型的运行时间和能耗情况。

Conclusion: 研究结果为设计和部署更可持续的生成视频系统提供了基准参考和实用见解。

Abstract: Recent advances in text-to-video (T2V) generation have enabled the creation
of high-fidelity, temporally coherent clips from natural language prompts. Yet
these systems come with significant computational costs, and their energy
demands remain poorly understood. In this paper, we present a systematic study
of the latency and energy consumption of state-of-the-art open-source T2V
models. We first develop a compute-bound analytical model that predicts scaling
laws with respect to spatial resolution, temporal length, and denoising steps.
We then validate these predictions through fine-grained experiments on
WAN2.1-T2V, showing quadratic growth with spatial and temporal dimensions, and
linear scaling with the number of denoising steps. Finally, we extend our
analysis to six diverse T2V models, comparing their runtime and energy profiles
under default settings. Our results provide both a benchmark reference and
practical insights for designing and deploying more sustainable generative
video systems.

</details>


### [202] [Study Design and Demystification of Physics Informed Neural Networks for Power Flow Simulation](https://arxiv.org/abs/2509.19233)
*Milad Leyli-abadi,Antoine Marot,Jérôme Picault*

Main category: cs.LG

TL;DR: 在能源转型背景下，传统电网物理求解器实时性差，机器学习模型作为替代方案常使用嵌入约束训练。本文开展消融研究，用自定义LIPS基准测试管道评估混合模型，结果显示物理知识集成对性能的影响，代码可复现。


<details>
  <summary>Details</summary>
Motivation: 在能源转型中，传统物理求解器实时性不足，为提高机器学习模型对物理定律的遵循，需研究混合策略。

Method: 开展消融研究，探索从简单多层感知器到先进基于图的网络等模型架构，用自定义LIPS基准测试管道从准确性、物理合规性、工业就绪性和分布外泛化四个维度评估模型。

Result: 结果凸显了集成物理知识对各评估标准性能的影响。

Conclusion: 通过消融研究和评估，明确了物理知识集成对混合模型性能的作用，代码可在Github复现。

Abstract: In the context of the energy transition, with increasing integration of
renewable sources and cross-border electricity exchanges, power grids are
encountering greater uncertainty and operational risk. Maintaining grid
stability under varying conditions is a complex task, and power flow simulators
are commonly used to support operators by evaluating potential actions before
implementation. However, traditional physical solvers, while accurate, are
often too slow for near real-time use. Machine learning models have emerged as
fast surrogates, and to improve their adherence to physical laws (e.g.,
Kirchhoff's laws), they are often trained with embedded constraints which are
also known as physics-informed or hybrid models. This paper presents an
ablation study to demystify hybridization strategies, ranging from
incorporating physical constraints as regularization terms or unsupervised
losses, and exploring model architectures from simple multilayer perceptrons to
advanced graph-based networks enabling the direct optimization of physics
equations. Using our custom benchmarking pipeline for hybrid models called
LIPS, we evaluate these models across four dimensions: accuracy, physical
compliance, industrial readiness, and out-of-distribution generalization. The
results highlight how integrating physical knowledge impacts performance across
these criteria. All the implementations are reproducible and provided in the
corresponding Github page.

</details>


### [203] [Stability and Generalization of Adversarial Diffusion Training](https://arxiv.org/abs/2509.19234)
*Hesam Hosseini,Ying Cao,Ali H. Sayed*

Main category: cs.LG

TL;DR: 本文对扩散策略下的对抗训练进行基于稳定性的泛化分析，推导泛化误差界并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 对抗训练存在鲁棒过拟合和泛化差距增大问题，且其在去中心化网络中的泛化特性未被探索。

Method: 对扩散策略下凸损失的对抗训练进行基于稳定性的泛化分析。

Result: 推导出泛化误差随对抗扰动强度和训练步数增长的界，与单智能体情况一致，在去中心化设置中是新发现，数值实验验证了理论预测。

Conclusion: 通过稳定性分析得出对抗训练在去中心化网络中的泛化特性。

Abstract: Algorithmic stability is an established tool for analyzing generalization.
While adversarial training enhances model robustness, it often suffers from
robust overfitting and an enlarged generalization gap. Although recent work has
established the convergence of adversarial training in decentralized networks,
its generalization properties remain unexplored. This work presents a
stability-based generalization analysis of adversarial training under the
diffusion strategy for convex losses. We derive a bound showing that the
generalization error grows with both the adversarial perturbation strength and
the number of training steps, a finding consistent with single-agent case but
novel for decentralized settings. Numerical experiments on logistic regression
validate these theoretical predictions.

</details>


### [204] [What Characterizes Effective Reasoning? Revisiting Length, Review, and Structure of CoT](https://arxiv.org/abs/2509.19284)
*Yunzhen Feng,Julia Kempe,Cheng Zhang,Parag Jain,Anthony Hartshorn*

Main category: cs.LG

TL;DR: 对十种大推理模型在数学和科学推理上进行系统评估，发现简单延长思维链和增加回顾会降低准确率，引入图视角提取结构，提出失败步骤分数（FSF）能更好预测正确性，设计干预实验表明移除失败分支可提升准确率。


<details>
  <summary>Details</summary>
Motivation: 当前有效思维链的特征不明确，之前‘越长越好’的观点存在争议，需要系统评估并找出有效思维链的特征。

Method: 对十种大推理模型进行系统评估，引入图视角提取结构得到FSF，设计两种干预实验（测试时按指标对候选思维链排序、编辑思维链移除失败分支）。

Result: 简单延长思维链和增加回顾与更低的准确率相关，FSF能比长度和回顾率更好地预测正确性，移除失败分支可显著提高准确率。

Conclusion: 有效思维链应失败更少，支持有结构感知的测试时扩展，而非盲目生成长思维链。

Abstract: Large reasoning models (LRMs) spend substantial test-time compute on long
chain-of-thought (CoT) traces, but what *characterizes* an effective CoT
remains unclear. While prior work reports gains from lengthening CoTs and
increasing review (revisiting earlier steps) via appended *wait* tokens, recent
studies suggest that shorter thinking can outperform longer traces. We
therefore conduct a systematic evaluation across ten LRMs on math and
scientific reasoning. Contrary to the "longer-is-better" narrative, we find
that both naive CoT lengthening and increased review are associated with
*lower* accuracy.
  As CoT unfolds step by step, token-level metrics can conflate verbosity with
process quality. We introduce a graph view of CoT to extract structure and
identify a single statistic-the *Failed-Step Fraction (FSF)*, the fraction of
steps in abandoned branches-that consistently outpredicts length and review
ratio for correctness across models. To probe causality, we design two
interventions. First, we rank candidate CoTs by each metric at test time, where
FSF yields the largest pass@1 gains; second, we edit CoTs to remove failed
branches, which significantly improves accuracy, indicating that failed
branches bias subsequent reasoning. Taken together, these results characterize
effective CoTs as those that *fail less* and support *structure-aware*
test-time scaling over indiscriminately generating long CoT.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [205] [Smart Cellular Bricks for Decentralized Shape Classification and Damage Recovery](https://arxiv.org/abs/2509.18659)
*Rodrigo Moreno,Andres Faina,Shyam Sudhakaran,Kathryn Walker,Sebastian Risi*

Main category: cs.NE

TL;DR: 受生物系统启发，提出含局部通信、处理和感知能力的3D砖块系统，利用NCA算法实现全局形状分类和结构损伤检测，展现高鲁棒性和容错性。


<details>
  <summary>Details</summary>
Motivation: 受生物系统依靠局部交互实现自我识别和形态再生的启发，开发具有类似能力的物理系统。

Method: 构建3D砖块系统，利用Neural Cellular Automata（NCA）算法，让每个模块独立执行相同神经网络，仅依靠局部交互。

Result: 数百个砖块能准确分类多种3D形状，对分布外形状变化有强鲁棒性，对通信故障和模块故障有高容错性，可检测缺失或损坏组件。

Conclusion: 实现了大规模、去中心化的自我识别和损伤检测，推动了鲁棒、自适应、仿生模块化系统的发展。

Abstract: Biological systems possess remarkable capabilities for self-recognition and
morphological regeneration, often relying solely on local interactions.
Inspired by these decentralized processes, we present a novel system of
physical 3D bricks--simple cubic units equipped with local communication,
processing, and sensing--that are capable of inferring their global shape class
and detecting structural damage. Leveraging Neural Cellular Automata (NCA), a
learned, fully-distributed algorithm, our system enables each module to
independently execute the same neural network without access to any global
state or positioning information. We demonstrate the ability of collections of
hundreds of these cellular bricks to accurately classify a variety of 3D shapes
through purely local interactions. The approach shows strong robustness to
out-of-distribution shape variations and high tolerance to communication faults
and failed modules. In addition to shape inference, the same decentralized
framework is extended to detect missing or damaged components, allowing the
collective to localize structural disruptions and to guide a recovery process.
This work provides a physical realization of large-scale, decentralized
self-recognition and damage detection, advancing the potential of robust,
adaptive, and bio-inspired modular systems.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [206] [Static Estimation of Reuse Profiles for Arrays in Nested Loops](https://arxiv.org/abs/2509.18684)
*Abdur Razzak,Atanu Barai,Nandakishore Santhi,Abdel-Hameed A. Badawy*

Main category: cs.PF

TL;DR: 本文提出新静态分析框架预测嵌套循环结构程序中数组引用的重用配置文件，无需运行时信息，速度快且精度可与传统工具媲美。


<details>
  <summary>Details</summary>
Motivation: 传统计算重用距离直方图（RDH）需程序执行和内存跟踪收集，耗时耗资源，不适合早期优化或大规模应用；现有静态预测方法缺乏准确性。

Method: 通过分析循环边界、小问题规模的访问模式和预测方程，在编译时预测数组访问模式、估计重用距离和缓存命中率，还纳入更多分析并处理之前未处理的重用模式。

Result: 与传统跟踪驱动分析工具PARDA对比，静态预测器在分析速度上有数量级提升，且精度相当。

Conclusion: 该工作为动态重用分析提供了实用替代方案，为集成到编译器和静态性能建模工具铺平道路。

Abstract: Efficient memory access patterns play a crucial role in determining the
overall performance of applications by exploiting temporal and spatial
locality, thus maximizing cache locality. The Reuse Distance Histogram (RDH) is
a widely used metric to quantify temporal locality, measuring the distance
between consecutive accesses to the same memory location. Traditionally,
calculating RDH requires program execution and memory trace collection to
obtain dynamic memory access behavior. This trace collection is often
time-consuming, resource-intensive, and unsuitable for early-stage optimization
or large-scale applications. Static prediction, on the other hand, offers a
significant speedup in estimating RDH and cache hit rates. However, these
approaches lack accuracy, since the predictions come without running the
program and knowing the complete memory access pattern, more specifically when
arrays are used inside nested loops. This paper presents a novel static
analysis framework for predicting the reuse profiles of array references in
programs with nested loop structures, without requiring any runtime
information. By analyzing loop bounds, access patterns in smaller problem
sizes, and predictive equations, our method predicts access patterns of arrays
and estimates reuse distances and cache hit rate at compile time. This paper
extends our previous study by incorporating more analysis and improving
prediction by addressing previously unhandled reuse patterns. We evaluate our
technique against a widely accepted traditional trace-driven profiling tool,
Parallel Reuse Distance Analysis (PARDA). The results demonstrate that our
static predictor achieves comparable accuracy while offering
orders-of-magnitude improvement in the analysis speed. This work offers a
practical alternative to dynamic reuse profiling and paves the way for
integration into compilers and static performance modeling tools.

</details>


### [207] [Confidential LLM Inference: Performance and Cost Across CPU and GPU TEEs](https://arxiv.org/abs/2509.18886)
*Marcin Chrapek,Marcin Copik,Etienne Mettaz,Torsten Hoefler*

Main category: cs.PF

TL;DR: 研究用TEEs保障大语言模型推理安全，评估其在CPU和GPU上性能，显示其可行性与优势。


<details>
  <summary>Details</summary>
Motivation: 大语言模型安全需求高阻碍其在隐私敏感领域应用，需解决安全问题。

Method: 在CPU和GPU的可信执行环境（TEEs）中评估大语言模型推理工作负载，在CPU侧用Intel的TDX和SGX运行Llama2推理，在GPU侧用NVIDIA H100进行推理。

Result: CPU TEEs吞吐量开销低于10%、延迟开销低于20%可被AMX降低；GPU吞吐量惩罚4 - 8%随批量和输入大小增长而降低，CPU TEEs在成本或安全上更具优势。

Conclusion: 首次全面证明现代TEEs在CPU和GPU上实现保密大语言模型的性能和实用性。

Abstract: Large Language Models (LLMs) are increasingly deployed on converged Cloud and
High-Performance Computing (HPC) infrastructure. However, as LLMs handle
confidential inputs and are fine-tuned on costly, proprietary datasets, their
heightened security requirements slow adoption in privacy-sensitive sectors
such as healthcare and finance. We investigate methods to address this gap and
propose Trusted Execution Environments (TEEs) as a solution for securing
end-to-end LLM inference. We validate their practicality by evaluating these
compute-intensive workloads entirely within CPU and GPU TEEs. On the CPU side,
we conduct an in-depth study running full Llama2 inference pipelines (7B, 13B,
70B) inside Intel's TDX and SGX, accelerated by Advanced Matrix Extensions
(AMX). We derive 12 insights, including that across various data types, batch
sizes, and input lengths, CPU TEEs impose under 10% throughput and 20% latency
overheads, further reduced by AMX. We run LLM inference on NVIDIA H100
Confidential Compute GPUs, contextualizing our CPU findings and observing
throughput penalties of 4-8% that diminish as batch and input sizes grow. By
comparing performance, cost, and security trade-offs, we show how CPU TEEs can
be more cost-effective or secure than their GPU counterparts. To our knowledge,
our work is the first to comprehensively demonstrate the performance and
practicality of modern TEEs across both CPUs and GPUs for enabling confidential
LLMs (cLLMs).

</details>


### [208] [Glass-Box Analysis for Computer Systems: Transparency Index, Shapley Attribution, and Markov Models of Branch Prediction](https://arxiv.org/abs/2509.19027)
*Faruk Alpay,Hamdi Alakkad*

Main category: cs.PF

TL;DR: 文章对计算机系统进行玻璃盒分析，引入GTI、ETD等三个工具，并为分支预测器开发分析框架，还建立了从硬件计数器恢复事件率的可识别性定理和稳定性边界。


<details>
  <summary>Details</summary>
Motivation: 对计算机系统进行玻璃盒分析，以更好地理解和量化系统性能等情况。

Method: 引入Glass - Box Transparency Index (GTI)量化性能方差可解释比例，用Explainable Throughput Decomposition (ETD)进行吞吐量归因，开发分支预测器的马尔可夫分析框架，建立可识别性定理。

Result: 得到GTI的界限、交叉验证估计等，ETD的误差保证和凸性差距界限，分支预测器的误预测率闭式解，以及可识别性定理和稳定性边界。

Conclusion: 所引入的工具和建立的定理能够用于计算机系统的玻璃盒分析，辅助对系统性能的理解和评估。

Abstract: We formalize glass-box analysis for computer systems and introduce three
principled tools. First, the Glass-Box Transparency Index (GTI) quantifies the
fraction of performance variance explainable by internal features and comes
equipped with bounds, invariances, cross-validated estimation, and bootstrap
confidence intervals. Second, Explainable Throughput Decomposition (ETD) uses
Shapley values to provide an efficiency-preserving attribution of throughput,
together with non-asymptotic Monte Carlo error guarantees and convexity
(Jensen) gap bounds. Third, we develop an exact Markov analytic framework for
branch predictors, including a closed-form misprediction rate for a two-bit
saturating counter under a two-state Markov branch process and its i.i.d.
corollary. Additionally, we establish an identifiability theorem for recovering
event rates from aggregated hardware counters and provide stability bounds
under noise.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [209] [CoRaCMG: Contextual Retrieval-Augmented Framework for Commit Message Generation](https://arxiv.org/abs/2509.18337)
*Bo Xiong,Linghao Zhang,Chong Wang,Peng Liang*

Main category: cs.SE

TL;DR: 本文提出CoRaCMG框架提升提交消息生成性能，经实验在多指标上显著提升LLM性能，且性能提升在使用三个以上示例后趋于平稳。


<details>
  <summary>Details</summary>
Motivation: 现有提交消息质量低且大语言模型自动生成提交消息性能有限，需提升提交消息生成性能。

Method: 提出CoRaCMG框架，分检索、增强、生成三个阶段，通过检索相似差异-消息对引导大语言模型生成提交消息。

Result: 在多种大语言模型上实验，CoRaCMG在四个指标上显著提升性能，如DeepSeek - R1和GPT - 4o有明显提升，使用三个以上示例后性能提升趋于平稳。

Conclusion: CoRaCMG能让大语言模型从检索的示例对中学习特定术语和写作风格，从而生成高质量提交消息。

Abstract: Commit messages play a key role in documenting the intent behind code
changes. However, they are often low-quality, vague, or incomplete, limiting
their usefulness. Commit Message Generation (CMG) aims to automatically
generate descriptive commit messages from code diffs to reduce developers'
effort and improve message quality. Although recent advances in LLMs have shown
promise in automating CMG, their performance remains limited. This paper aims
to enhance CMG performance by retrieving similar diff-message pairs to guide
LLMs to generate commit messages that are more precise and informative. We
proposed CoRaCMG, a Contextual Retrieval-augmented framework for Commit Message
Generation, structured in three phases: (1) Retrieve: retrieving the similar
diff-message pairs; (2) Augment: combining them with the query diff into a
structured prompt; and (3) Generate: generating commit messages corresponding
to the query diff via LLMs. CoRaCMG enables LLMs to learn project-specific
terminologies and writing styles from the retrieved diff-message pairs, thereby
producing high-quality commit messages. We evaluated our method on various
LLMs, including closed-source GPT models and open-source DeepSeek models.
Experimental results show that CoRaCMG significantly boosts LLM performance
across four metrics (BLEU, Rouge-L, METEOR, and CIDEr). Specifically,
DeepSeek-R1 achieves relative improvements of 76% in BLEU and 71% in CIDEr when
augmented with a single retrieved example pair. After incorporating the single
example pair, GPT-4o achieves the highest improvement rate, with BLEU
increasing by 89%. Moreover, performance gains plateau after more than three
examples are used, indicating diminishing returns. Further analysis shows that
the improvements are attributed to the model's ability to capture the
terminologies and writing styles of human-written commit messages from the
retrieved example pairs.

</details>


### [210] [Reading Between the Lines: Scalable User Feedback via Implicit Sentiment in Developer Prompts](https://arxiv.org/abs/2509.18361)
*Daye Nam,Malgorzata Salawa,Satish Chandra*

Main category: cs.SE

TL;DR: 提出用开发者提示的情感分析识别用户满意度的隐式信号，分析工业使用日志表明该方法有效，可补充现有反馈渠道。


<details>
  <summary>Details</summary>
Motivation: 大规模评估开发者对对话式AI助手的满意度很关键但有挑战，现有用户研究不可扩展，日志或产品内评分等信号不可靠。

Method: 使用开发者提示的情感分析来识别用户满意度的隐式信号。

Result: 分析372名专业开发者的工业使用日志，该方法能在约8%的交互中识别信号，比显式用户反馈高13倍多，即使用现成情感分析方法也有合理准确性。

Conclusion: 该新方法可补充现有反馈渠道，为大规模全面理解开发者体验开辟新方向。

Abstract: Evaluating developer satisfaction with conversational AI assistants at scale
is critical but challenging. User studies provide rich insights, but are
unscalable, while large-scale quantitative signals from logs or in-product
ratings are often too shallow or sparse to be reliable. To address this gap, we
propose and evaluate a new approach: using sentiment analysis of developer
prompts to identify implicit signals of user satisfaction. With an analysis of
industrial usage logs of 372 professional developers, we show that this
approach can identify a signal in ~8% of all interactions, a rate more than 13
times higher than explicit user feedback, with reasonable accuracy even with an
off-the-shelf sentiment analysis approach. This new practical approach to
complement existing feedback channels would open up new directions for building
a more comprehensive understanding of the developer experience at scale.

</details>


### [211] [SC2Tools: StarCraft II Toolset and Dataset API](https://arxiv.org/abs/2509.18454)
*Andrzej Białecki,Piotr Białecki,Piotr Sowiński,Mateusz Budziak,Jan Gajewski*

Main category: cs.SE

TL;DR: 本文介绍了工具集SC2Tools，它可处理和生成大数据集，还用于创建大型星际争霸2锦标赛数据集，能减轻数据处理负担，为研究提供基础。


<details>
  <summary>Details</summary>
Motivation: 游戏和电竞领域受AI和ML影响大，工具对开发该研究领域很重要，需工具减轻科研工作量。

Method: 提出包含多个子模块的工具集SC2Tools，给出模块化结构，部分工具可用于其他类型数据。

Result: 利用该工具集创建了目前最大的星际争霸2锦标赛数据集之一，并有方便的数据访问API。

Conclusion: 减轻数据收集、预处理和代码开发负担对非技术专家参与电竞研究很重要，该方案为星际争霸2实验工作流标准化提供基础。

Abstract: Computer games, as fully controlled simulated environments, have been
utilized in significant scientific studies demonstrating the application of
Reinforcement Learning (RL). Gaming and esports are key areas influenced by the
application of Artificial Intelligence (AI) and Machine Learning (ML) solutions
at scale. Tooling simplifies scientific workloads and is essential for
developing the gaming and esports research area.
  In this work, we present ``SC2Tools'', a toolset containing multiple
submodules responsible for working with, and producing larger datasets. We
provide a modular structure of the implemented tooling, leaving room for future
extensions where needed. Additionally, some of the tools are not StarCraft~2
exclusive and can be used with other types of data for dataset creation.
  The tools we present were leveraged in creating one of the largest
StarCraft~2 tournament datasets to date with a separate PyTorch and PyTorch
Lightning application programming interface (API) for easy access to the data.
  We conclude that alleviating the burden of data collection, preprocessing,
and custom code development is essential for less technically proficient
researchers to engage in the growing gaming and esports research area. Finally,
our solution provides some foundational work toward normalizing experiment
workflow in StarCraft~2

</details>


### [212] [Locking Down Science Gateways](https://arxiv.org/abs/2509.18548)
*Steven R Brandt,Max Morris,Patrick Diehl,Christopher Bowen,Jacob Tucker,Lauren Bristol,Golden G. Richard III*

Main category: cs.SE

TL;DR: 本文探索Linux内核新安全特性Landlock在科学网关应用中的作用，修改并锁定三个科学代码，实现了依赖Landlock的FUKA科学网关。


<details>
  <summary>Details</summary>
Motivation: Linux内核有新安全特性Landlock，科学网关应用启动MPI时需网络访问，但读取用户参数文件前为安全应取消网络访问，探索Landlock在此场景中的作用。

Method: 修改并锁定三个成熟科学代码：The Einstein Toolkit、Octo - Tiger和FUKA，利用Landlock实现安全。

Result: 实现了一个完全功能的依赖Landlock保障安全的FUKA科学网关。

Conclusion: Landlock可用于科学网关应用保障安全，替代用户认证方式。

Abstract: The most recent Linux kernels have a new feature for securing applications:
Landlock. Like Seccomp before it, Landlock makes it possible for a running
process to give up access to resources. For applications running as Science
Gateways, network access is required while starting up MPI, but for the sake of
security, it should be taken away prior to the reading of user-supplied
parameter files. We explore the usefulness of Landlock by modifying and locking
down three mature scientific codes: The Einstein Toolkit (a code that studies
the dynamics of relativistic astrophysics, e.g. neutron star collisions),
Octo-Tiger (a code for studying the dynamics of non-relativistic astrophysics,
e.g. white dwarfs), and FUKA (an initial data solver for relativistic codes).
Finally, we implement a fully-functioning FUKA science gateway that relies on
Landlock (instead of user authentication) for security.

</details>


### [213] [SR-Eval: Evaluating LLMs on Code Generation under Stepwise Requirement Refinement](https://arxiv.org/abs/2509.18808)
*Zexun Zhan,Shuzheng Gao,Ruida Hu,Cuiyun Gao*

Main category: cs.SE

TL;DR: 现有代码生成基准忽略迭代流程，本文提出SR - Eval基准评估大语言模型迭代代码生成能力，评估结果显示该任务挑战大，提示策略影响大。


<details>
  <summary>Details</summary>
Motivation: 现有基准将代码生成任务形式化为静态单轮问题，与实际开发不匹配，且构建迭代基准存在困难，需新基准评估大语言模型在迭代代码生成中的表现。

Method: 提出SR - Eval基准，采用多智能体需求生成方法模拟开发过程，利用语义感知判别测试用例生成组件确保评估有效性，包含443个多轮任务和1857个问题，用三种提示策略评估11个代表性大语言模型。

Result: 在函数级和仓库级任务上最佳模型完成率分别仅为22.67%和20.00%，提示策略对性能影响大。

Conclusion: 逐步需求细化下的迭代代码生成仍极具挑战，需要开发先进方法。

Abstract: Large language models (LLMs) have achieved remarkable progress in code
generation. However, existing benchmarks mainly formalize the task as a static,
single-turn problem, overlooking the stepwise requirement changes and iterative
workflows in real-world software development. This mismatch limits the
understanding of how well LLMs can support real-world development workflows.
Constructing such iterative benchmarks is challenging due to the lack of public
interaction traces and the difficulty of creating discriminative, turn-specific
test cases.
  To bridge this gap, we present SR-Eval, a benchmark specifically designed to
assess LLMs on iterative code generation under Stepwise requirements
Refinement. SR-Eval spans both function-level and repository-level tasks in
Python and Java, enabling fine-grained and progressive evaluation across
evolving requirements. The construction of SR-Eval follows a carefully designed
pipeline that first leverages a multi-agent-based requirement generation method
to simulate the development process and recover the multi-round interaction
process from final requirements, then employs a semantic-aware discriminative
test case generation component to ensure discriminative and consistent
evaluation at each turn. SR-Eval comprises 443 multi-turn tasks and 1,857
questions at both function and repository levels. Using SR-Eval, we evaluate 11
representative LLMs with three prompting strategies that simulate different
usage patterns. Results show that iterative code generation under stepwise
requirement refinement remains highly challenging: the best-performing model
achieves only 22.67% completion rate on function-level tasks and 20.00% on
repository-level tasks. We further observe that prompting strategies
substantially influence performance, highlighting the need for the development
of advanced methods.

</details>


### [214] [On the Soundness and Consistency of LLM Agents for Executing Test Cases Written in Natural Language](https://arxiv.org/abs/2509.19136)
*Sébastien Salva,Redha Taguelmimt*

Main category: cs.SE

TL;DR: 本文研究用大语言模型（LLM）代理直接执行自然语言（NL）测试用例，提出解决NL测试用例不健全和执行不一致问题的方法，实验评估了多个LLM的能力。


<details>
  <summary>Details</summary>
Motivation: 手动编写可执行测试脚本开发成本高、维护困难，而NL测试用例有潜力，但存在不健全和执行不一致问题，需解决。

Method: 提出带护栏机制和专用代理的NL测试用例执行算法，引入评估LLM测试执行能力和量化执行一致性的措施，定义弱不健全性。

Result: 对8个公开可用的LLM实验，Meta Llama 3.1 70B在NL测试用例执行中能力可接受，执行一致性高。

Conclusion: 展示了当前LLM代理用于GUI测试的潜力和局限，提供了原型工具、测试套件和结果。

Abstract: The use of natural language (NL) test cases for validating graphical user
interface (GUI) applications is emerging as a promising direction to manually
written executable test scripts, which are costly to develop and difficult to
maintain. Recent advances in large language models (LLMs) have opened the
possibility of the direct execution of NL test cases by LLM agents. This paper
investigates this direction, focusing on the impact on NL test case unsoundness
and on test case execution consistency. NL test cases are inherently unsound,
as they may yield false failures due to ambiguous instructions or unpredictable
agent behaviour. Furthermore, repeated executions of the same NL test case may
lead to inconsistent outcomes, undermining test reliability. To address these
challenges, we propose an algorithm for executing NL test cases with guardrail
mechanisms and specialised agents that dynamically verify the correct execution
of each test step. We introduce measures to evaluate the capabilities of LLMs
in test execution and one measure to quantify execution consistency. We propose
a definition of weak unsoundness to characterise contexts in which NL test case
execution remains acceptable, with respect to the industrial quality levels Six
Sigma. Our experimental evaluation with eight publicly available LLMs, ranging
from 3B to 70B parameters, demonstrates both the potential and current
limitations of current LLM agents for GUI testing. Our experiments show that
Meta Llama 3.1 70B demonstrates acceptable capabilities in NL test case
execution with high execution consistency (above the level 3-sigma). We provide
prototype tools, test suites, and results.

</details>


### [215] [An Empirical Study of Testing Practices in Open Source AI Agent Frameworks and Agentic Applications](https://arxiv.org/abs/2509.19185)
*Mohammed Mehedi Hasan,Hao Li,Emad Fallahzadeh,Gopi Krishnan Rajbahadur,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 本文对AI智能体生态系统中的测试实践进行大规模实证研究，识别了十种测试模式，揭示了测试工作分配的问题，并为开发者和研究者提出建议。


<details>
  <summary>Details</summary>
Motivation: 基于基础模型的AI智能体存在非确定性和不可重复性，现有基准对开发者验证其内部正确性的研究有限。

Method: 分析39个开源智能体框架和439个智能体应用。

Result: 识别十种测试模式，发现新的特定方法使用少，传统模式广泛应用；测试工作倒置，触发组件被忽视。

Conclusion: 为基于基础模型的智能体框架和应用提供了实证测试基线，开发者和研究者需改进测试实践以构建更强大可靠的AI智能体。

Abstract: Foundation model (FM)-based AI agents are rapidly gaining adoption across
diverse domains, but their inherent non-determinism and non-reproducibility
pose testing and quality assurance challenges. While recent benchmarks provide
task-level evaluations, there is limited understanding of how developers verify
the internal correctness of these agents during development.
  To address this gap, we conduct the first large-scale empirical study of
testing practices in the AI agent ecosystem, analyzing 39 open-source agent
frameworks and 439 agentic applications. We identify ten distinct testing
patterns and find that novel, agent-specific methods like DeepEval are seldom
used (around 1%), while traditional patterns like negative and membership
testing are widely adapted to manage FM uncertainty. By mapping these patterns
to canonical architectural components of agent frameworks and agentic
applications, we uncover a fundamental inversion of testing effort:
deterministic components like Resource Artifacts (tools) and Coordination
Artifacts (workflows) consume over 70% of testing effort, while the FM-based
Plan Body receives less than 5%. Crucially, this reveals a critical blind spot,
as the Trigger component (prompts) remains neglected, appearing in around 1% of
all tests.
  Our findings offer the first empirical testing baseline in FM-based agent
frameworks and agentic applications, revealing a rational but incomplete
adaptation to non-determinism. To address it, framework developers should
improve support for novel testing methods, application developers must adopt
prompt regression testing, and researchers should explore barriers to adoption.
Strengthening these practices is vital for building more robust and dependable
AI agents.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [216] [Filtering amplitude dependence of correlation dynamics in complex systems: application to the cryptocurrency market](https://arxiv.org/abs/2509.18820)
*Marcin Wątorek,Marija Bezbradica,Martin Crane,Jarosław Kwapień,Stanisław Drożdż*

Main category: q-fin.ST

TL;DR: 本文基于加密货币市场动态，用q依赖去趋势互相关系数分析复杂系统相关性结构，发现中尺度波动相关性更强，qMSTs在揭示波动依赖相关性有效且有广泛应用。


<details>
  <summary>Details</summary>
Motivation: 分析复杂系统中不断演变的相关性结构。

Method: 使用q依赖去趋势互相关系数，采用q依赖最小生成树可视化网络结构，结合滚动窗口分析和频谱分析。

Result: qMSTs有显著变化，BTC主导地位下降，中尺度波动相关性更强，大尺度qMSTs更分散，重大干扰会放大相关性差异。

Conclusion: qMSTs在揭示波动依赖相关性有效，有超越金融领域的潜在应用。

Abstract: Based on the cryptocurrency market dynamics, this study presents a general
methodology for analyzing evolving correlation structures in complex systems
using the $q$-dependent detrended cross-correlation coefficient \rho(q,s). By
extending traditional metrics, this approach captures correlations at varying
fluctuation amplitudes and time scales. The method employs $q$-dependent
minimum spanning trees ($q$MSTs) to visualize evolving network structures.
Using minute-by-minute exchange rate data for 140 cryptocurrencies on Binance
(Jan 2021-Oct 2024), a rolling window analysis reveals significant shifts in
$q$MSTs, notably around April 2022 during the Terra/Luna crash. Initially
centralized around Bitcoin (BTC), the network later decentralized, with
Ethereum (ETH) and others gaining prominence. Spectral analysis confirms BTC's
declining dominance and increased diversification among assets. A key finding
is that medium-scale fluctuations exhibit stronger correlations than
large-scale ones, with $q$MSTs based on the latter being more decentralized.
Properly exploiting such facts may offer the possibility of a more flexible
optimal portfolio construction. Distance metrics highlight that major
disruptions amplify correlation differences, leading to fully decentralized
structures during crashes. These results demonstrate $q$MSTs' effectiveness in
uncovering fluctuation-dependent correlations, with potential applications
beyond finance, including biology, social and other complex systems.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [217] [Surrogate Modelling of Proton Dose with Monte Carlo Dropout Uncertainty Quantification](https://arxiv.org/abs/2509.18155)
*Aaron Pim,Tristan Pryer*

Main category: stat.ML

TL;DR: 开发集成蒙特卡罗丢弃的神经替代模型实现快速可微剂量预测及不确定性估计，经多实验验证，速度优于MC，适用于质子治疗规划。


<details>
  <summary>Details</summary>
Motivation: 蒙特卡罗方法在质子剂量计算中计算需求大，在需要重复评估的工作流程中应用受限。

Method: 开发集成蒙特卡罗丢弃的神经替代模型，通过一维分析基准、二维骨 - 水模体、三维水模体实验验证。

Result: 实现速度显著提升，保留不确定性信息，分离了认知和参数不确定性贡献。

Conclusion: 该方法适用于质子治疗的稳健规划、自适应工作流程和不确定性感知优化。

Abstract: Accurate proton dose calculation using Monte Carlo (MC) is computationally
demanding in workflows like robust optimisation, adaptive replanning, and
probabilistic inference, which require repeated evaluations. To address this,
we develop a neural surrogate that integrates Monte Carlo dropout to provide
fast, differentiable dose predictions along with voxelwise predictive
uncertainty. The method is validated through a series of experiments, starting
with a one-dimensional analytic benchmark that establishes accuracy,
convergence, and variance decomposition. Two-dimensional bone-water phantoms,
generated using TOPAS Geant4, demonstrate the method's behavior under domain
heterogeneity and beam uncertainty, while a three-dimensional water phantom
confirms scalability for volumetric dose prediction. Across these settings, we
separate epistemic (model) from parametric (input) contributions, showing that
epistemic variance increases under distribution shift, while parametric
variance dominates at material boundaries. The approach achieves significant
speedups over MC while retaining uncertainty information, making it suitable
for integration into robust planning, adaptive workflows, and uncertainty-aware
optimisation in proton therapy.

</details>


### [218] [Statistical Insight into Meta-Learning via Predictor Subspace Characterization and Quantification of Task Diversity](https://arxiv.org/abs/2509.18349)
*Saptati Datta,Nicolas W. Hengartner,Yulia Pimonova,Natalie E. Klein,Nicholas Lubbers*

Main category: stat.ML

TL;DR: 提出通过预测子空间表征和任务多样性量化分析元学习的统计框架，指出预测准确性与共享子空间的预测器方差比例及子空间估计准确性有关。


<details>
  <summary>Details</summary>
Motivation: 利用跨相关任务的信息改进新任务的预测性能，需要分析元学习。

Method: 用潜在子空间对跨任务的共享结构建模，引入衡量任务特定预测器异质性的多样性度量。

Result: 通过模拟和理论证据表明，元学习中实现预期预测准确性取决于与共享子空间对齐的预测器方差比例以及子空间估计的准确性。

Conclusion: 分析元学习可从预测子空间表征和任务多样性量化角度，且预测准确性受特定因素影响。

Abstract: Meta-learning has emerged as a powerful paradigm for leveraging information
across related tasks to improve predictive performance on new tasks. In this
paper, we propose a statistical framework for analyzing meta-learning through
the lens of predictor subspace characterization and quantification of task
diversity. Specifically, we model the shared structure across tasks using a
latent subspace and introduce a measure of diversity that captures
heterogeneity across task-specific predictors. We provide both simulation-based
and theoretical evidence indicating that achieving the desired prediction
accuracy in meta-learning depends on the proportion of predictor variance
aligned with the shared subspace, as well as on the accuracy of subspace
estimation.

</details>


### [219] [End-Cut Preference in Survival Trees](https://arxiv.org/abs/2509.18477)
*Xiaogang Su*

Main category: stat.ML

TL;DR: 本文指出生存树使用贪婪搜索选最优分割点时存在端切割偏好（ECP）问题，提出平滑Sigmoid替代（SSS）方法来缓解或避免该问题。


<details>
  <summary>Details</summary>
Motivation: CART中存在的ECP问题会导致分割不平衡、有偏差等，生存树使用贪婪搜索选最优分割点时也会出现该问题，需解决。

Method: 提出平滑Sigmoid替代（SSS）方法，用平滑Sigmoid函数替代硬阈值指示函数。

Result: 通过理论和数值示例证明SSS能有效缓解或避免ECP。

Conclusion: SSS方法可有效解决生存树中的ECP问题。

Abstract: The end-cut preference (ECP) problem, referring to the tendency to favor
split points near the boundaries of a feature's range, is a well-known issue in
CART (Breiman et al., 1984). ECP may induce highly imbalanced and biased
splits, obscure weak signals, and lead to tree structures that are both
unstable and difficult to interpret. For survival trees, we show that ECP also
arises when using greedy search to select the optimal cutoff point by
maximizing the log-rank test statistic. To address this issue, we propose a
smooth sigmoid surrogate (SSS) approach, in which the hard-threshold indicator
function is replaced by a smooth sigmoid function. We further demonstrate, both
theoretically and through numerical illustrations, that SSS provides an
effective remedy for mitigating or avoiding ECP.

</details>


### [220] [Estimating Heterogeneous Causal Effect on Networks via Orthogonal Learning](https://arxiv.org/abs/2509.18484)
*Yuanchen Wu,Yubai Yuan*

Main category: stat.ML

TL;DR: 本文提出两阶段方法估计网络上的异质直接和溢出效应，该方法平衡了表达性和可解释性，估计结果对偏差和错误设定具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 网络上的因果效应估计很重要，但面临因果效应异质性和网络同质性导致的混淆这两个挑战，需要新方法解决。

Method: 提出两阶段方法，第一阶段用图神经网络估计依赖复杂网络拓扑的干扰项，第二阶段用估计值调整网络混淆，通过基于注意力的干扰模型推断因果效应，并用Neyman正交化和交叉拟合集成两阶段。

Result: 所提方法能平衡表达性和可解释性，可用于识别有影响的邻域和恢复溢出效应的符号，因果效应估计对偏差和错误设定具有鲁棒性。

Conclusion: 提出的两阶段方法能有效估计网络上的异质直接和溢出效应，在网络依赖下的因果效应建模方面表现良好。

Abstract: Estimating causal effects on networks is important for both scientific
research and practical applications. Unlike traditional settings that assume
the Stable Unit Treatment Value Assumption (SUTVA), interference allows an
intervention/treatment on one unit to affect the outcomes of others.
Understanding both direct and spillover effects is critical in fields such as
epidemiology, political science, and economics. Causal inference on networks
faces two main challenges. First, causal effects are typically heterogeneous,
varying with unit features and local network structure. Second, connected units
often exhibit dependence due to network homophily, creating confounding between
structural correlations and causal effects. In this paper, we propose a
two-stage method to estimate heterogeneous direct and spillover effects on
networks. The first stage uses graph neural networks to estimate nuisance
components that depend on the complex network topology. In the second stage, we
adjust for network confounding using these estimates and infer causal effects
through a novel attention-based interference model. Our approach balances
expressiveness and interpretability, enabling downstream tasks such as
identifying influential neighborhoods and recovering the sign of spillover
effects. We integrate the two stages using Neyman orthogonalization and
cross-fitting, which ensures that errors from nuisance estimation contribute
only at higher order. As a result, our causal effect estimates are robust to
bias and misspecification in modeling causal effects under network
dependencies.

</details>


### [221] [A Gradient Flow Approach to Solving Inverse Problems with Latent Diffusion Models](https://arxiv.org/abs/2509.19276)
*Tim Y. J. Wang,O. Deniz Akyildiz*

Main category: stat.ML

TL;DR: 提出无训练方法DWGF，利用预训练潜扩散模型解决不适定逆问题，并在标准基准上展示性能。


<details>
  <summary>Details</summary>
Motivation: 解决不适定逆问题需要强大且灵活的先验。

Method: 提出Diffusion - regularized Wasserstein Gradient Flow (DWGF)，将后验采样问题表述为潜空间中Kullback - Leibler散度的正则化Wasserstein梯度流。

Result: 以StableDiffusion为先验，在标准基准上展示了方法性能。

Conclusion: 提出的DWGF方法可利用预训练潜扩散模型解决不适定逆问题。

Abstract: Solving ill-posed inverse problems requires powerful and flexible priors. We
propose leveraging pretrained latent diffusion models for this task through a
new training-free approach, termed Diffusion-regularized Wasserstein Gradient
Flow (DWGF). Specifically, we formulate the posterior sampling problem as a
regularized Wasserstein gradient flow of the Kullback-Leibler divergence in the
latent space. We demonstrate the performance of our method on standard
benchmarks using StableDiffusion (Rombach et al., 2022) as the prior.

</details>


### [222] [Consistency of Selection Strategies for Fraud Detection](https://arxiv.org/abs/2509.18739)
*Christos Revelas,Otilia Boldea,Bas J. M. Werker*

Main category: stat.ML

TL;DR: 研究保险公司调查欺诈理赔的选择策略，指出常用策略可能导致学习不一致，提出随机替代策略并对比汤普森采样。


<details>
  <summary>Details</summary>
Motivation: 解决常用理赔欺诈调查选择策略可能导致的不一致学习问题。

Method: 将问题与多臂老虎机文献类比，在二元回归框架中形式化选择，通过模拟对比不同策略。

Result: 常用选择策略可能不一致，随机替代策略一致，汤普森采样学习低欺诈概率效率低。

Conclusion: 随机选择策略在理赔欺诈调查选择中更具优势。

Abstract: This paper studies how insurers can chose which claims to investigate for
fraud. Given a prediction model, typically only claims with the highest
predicted propability of being fraudulent are investigated. We argue that this
can lead to inconsistent learning and propose a randomized alternative. More
generally, we draw a parallel with the multi-arm bandit literature and argue
that, in the presence of selection, the obtained observations are not iid.
Hence, dependence on past observations should be accounted for when updating
parameter estimates. We formalize selection in a binary regression framework
and show that model updating and maximum-likelihood estimation can be
implemented as if claims were investigated at random. Then, we define
consistency of selection strategies and conjecture sufficient conditions for
consistency. Our simulations suggest that the often-used selection strategy can
be inconsistent while the proposed randomized alternative is consistent.
Finally, we compare our randomized selection strategy with Thompson sampling, a
standard multi-arm bandit heuristic. Our simulations suggest that the latter
can be inefficient in learning low fraud probabilities.

</details>


### [223] [Neighbor Embeddings Using Unbalanced Optimal Transport Metrics](https://arxiv.org/abs/2509.19226)
*Muhammad Rana,Keaton Hamm*

Main category: stat.ML

TL;DR: 本文提出在降维和学习管道中使用非平衡最优传输（UOT）的Hellinger - Kantorovich度量，对比UOT与常规OT和基于欧几里得的降维方法，实验表明UOT表现更优。


<details>
  <summary>Details</summary>
Motivation: 探索非平衡最优传输（UOT）的Hellinger - Kantorovich度量在降维和学习（有监督和无监督）管道中的应用，并对比其与常规OT和基于欧几里得的降维方法的性能。

Method: 在多个基准数据集（包括MedMNIST）上，对比UOT、常规OT和基于欧几里得的降维方法的性能。

Result: 统计假设检验验证，平均而言UOT比基于欧几里得和OT的方法有改进；在MedMNIST数据集分类中UOT 81%的时间优于OT；聚类时UOT 83%的时间优于OT，58%的时间优于其他两种度量。

Conclusion: UOT在降维和学习任务中表现优于常规OT和基于欧几里得的降维方法。

Abstract: This paper proposes the use of the Hellinger--Kantorovich metric from
unbalanced optimal transport (UOT) in a dimensionality reduction and learning
(supervised and unsupervised) pipeline. The performance of UOT is compared to
that of regular OT and Euclidean-based dimensionality reduction methods on
several benchmark datasets including MedMNIST. The experimental results
demonstrate that, on average, UOT shows improvement over both Euclidean and
OT-based methods as verified by statistical hypothesis tests. In particular, on
the MedMNIST datasets, UOT outperforms OT in classification 81\% of the time.
For clustering MedMNIST, UOT outperforms OT 83\% of the time and outperforms
both other metrics 58\% of the time.

</details>


### [224] [Recovering Wasserstein Distance Matrices from Few Measurements](https://arxiv.org/abs/2509.19250)
*Muhammad Rana,Abiy Tasissa,HanQin Cai,Yakov Gavriyelov,Keaton Hamm*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper proposes two algorithms for estimating square Wasserstein distance
matrices from a small number of entries. These matrices are used to compute
manifold learning embeddings like multidimensional scaling (MDS) or Isomap, but
contrary to Euclidean distance matrices, are extremely costly to compute. We
analyze matrix completion from upper triangular samples and Nystr\"{o}m
completion in which $\mathcal{O}(d\log(d))$ columns of the distance matrices
are computed where $d$ is the desired embedding dimension, prove stability of
MDS under Nystr\"{o}m completion, and show that it can outperform matrix
completion for a fixed budget of sample distances. Finally, we show that
classification of the OrganCMNIST dataset from the MedMNIST benchmark is stable
on data embedded from the Nystr\"{o}m estimation of the distance matrix even
when only 10\% of the columns are computed.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [225] [Dynamical Modeling of Behaviorally Relevant Spatiotemporal Patterns in Neural Imaging Data](https://arxiv.org/abs/2509.18507)
*Mohammad Hosseini,Maryam M. Shanechi*

Main category: q-bio.NC

TL;DR: 提出SBIND框架用于建模神经图像时空依赖并分离与行为相关动力学，在多种成像数据集验证，表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 高维神经活动成像建模受高维、复杂时空依赖和无关动力学阻碍，现有模型预处理会丢失信息。

Method: 提出SBIND这一数据驱动的深度学习框架。

Result: SBIND有效识别大脑局部和远程空间依赖，分离行为相关神经动力学，在神经 - 行为预测中优于现有模型。

Conclusion: SBIND为利用成像方式研究行为背后神经机制提供了通用工具。

Abstract: High-dimensional imaging of neural activity, such as widefield calcium and
functional ultrasound imaging, provide a rich source of information for
understanding the relationship between brain activity and behavior. Accurately
modeling neural dynamics in these modalities is crucial for understanding this
relationship but is hindered by the high-dimensionality, complex spatiotemporal
dependencies, and prevalent behaviorally irrelevant dynamics in these
modalities. Existing dynamical models often employ preprocessing steps to
obtain low-dimensional representations from neural image modalities. However,
this process can discard behaviorally relevant information and miss
spatiotemporal structure. We propose SBIND, a novel data-driven deep learning
framework to model spatiotemporal dependencies in neural images and disentangle
their behaviorally relevant dynamics from other neural dynamics. We validate
SBIND on widefield imaging datasets, and show its extension to functional
ultrasound imaging, a recent modality whose dynamical modeling has largely
remained unexplored. We find that our model effectively identifies both local
and long-range spatial dependencies across the brain while also dissociating
behaviorally relevant neural dynamics. Doing so, SBIND outperforms existing
models in neural-behavioral prediction. Overall, SBIND provides a versatile
tool for investigating the neural mechanisms underlying behavior using imaging
modalities.

</details>


### [226] [BRAID: Input-Driven Nonlinear Dynamical Modeling of Neural-Behavioral Data](https://arxiv.org/abs/2509.18627)
*Parsa Vahidi,Omid G. Sani,Maryam M. Shanechi*

Main category: q-bio.NC

TL;DR: 提出深度学习框架BRAID，可结合外部输入建模神经动力学，经验证能准确学习神经与行为模式间的内在动力学，应用于运动皮层活动数据表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有模型常将神经群体建模为自主动力系统，较少考虑外部输入对群体活动和行为结果的影响。

Method: 引入BRAID框架，通过在输入驱动的循环神经网络中加入预测目标来分离内在循环神经群体动力学和输入的影响，使用多阶段优化方案优先学习与感兴趣行为相关的内在动力学。

Result: 通过非线性模拟验证，BRAID能准确学习神经和行为模式间的内在动力学；应用于运动皮层活动数据，结合测量的感官刺激能更准确拟合神经 - 行为数据，且在预测方面优于各种基线方法。

Conclusion: BRAID框架在建模神经动力学时考虑外部输入是有效的，能提升对神经 - 行为数据的拟合和预测能力。

Abstract: Neural populations exhibit complex recurrent structures that drive behavior,
while continuously receiving and integrating external inputs from sensory
stimuli, upstream regions, and neurostimulation. However, neural populations
are often modeled as autonomous dynamical systems, with little consideration
given to the influence of external inputs that shape the population activity
and behavioral outcomes. Here, we introduce BRAID, a deep learning framework
that models nonlinear neural dynamics underlying behavior while explicitly
incorporating any measured external inputs. Our method disentangles intrinsic
recurrent neural population dynamics from the effects of inputs by including a
forecasting objective within input-driven recurrent neural networks. BRAID
further prioritizes the learning of intrinsic dynamics that are related to a
behavior of interest by using a multi-stage optimization scheme. We validate
BRAID with nonlinear simulations, showing that it can accurately learn the
intrinsic dynamics shared between neural and behavioral modalities. We then
apply BRAID to motor cortical activity recorded during a motor task and
demonstrate that our method more accurately fits the neural-behavioral data by
incorporating measured sensory stimuli into the model and improves the
forecasting of neural-behavioral data compared with various baseline methods,
whether input-driven or not.

</details>


### [227] [Complexity of Activity Patterns in a Bio-Inspired Hopfield-Type Network in Different Topologies](https://arxiv.org/abs/2509.18758)
*Marco Cafiso,Paolo Paradisi*

Main category: q-bio.NC

TL;DR: 对生物启发的Hopfield型神经网络模型进行时间复杂性分析，对比无标度和随机网络拓扑，发现二者有相似动态行为和时间复杂性特征，无标度拓扑噪声小，证实枢纽节点作用。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注神经网络结构特性，而生物神经网络的时间动态也很重要，故进行时间复杂性分析。

Method: 对生物启发的Hopfield型神经网络模型进行时间复杂性分析，对比无标度和随机网络拓扑的全局激活模式。

Result: 两种网络架构有相似动态行为和时间复杂性特征，无标度拓扑噪声小，复杂动态轮廓多在无标度网络配置中出现。

Conclusion: 证实了枢纽节点在神经网络动态中的关键作用。

Abstract: Neural network models capable of storing memory have been extensively studied
in computer science and computational neuroscience. The Hopfield network is a
prototypical example of a model designed for associative, or
content-addressable, memory and has been analyzed in many forms. Further, ideas
and methods from complex network theory have been incorporated into artificial
neural networks and learning, emphasizing their structural properties.
Nevertheless, the temporal dynamics also play a vital role in biological neural
networks, whose temporal structure is a crucial feature to examine. Biological
neural networks display complex intermittency and, thus, can be studied through
the lens of the temporal complexity (TC) theory. The TC approach look at the
metastability of self-organized states, characterized by a power-law decay in
the inter-event time distribution and in the total activity distribution or a
scaling behavior in the corresponding event-driven diffusion processes. In this
study, we present a temporal complexity (TC) analysis of a
biologically-inspired Hopfield-type neural network model. We conducted a
comparative assessment between scale-free and random network topologies, with
particular emphasis on their global activation patterns. Our parametric
analysis revealed comparable dynamical behaviors across both neural network
architectures. Furthermore, our investigation into temporal complexity
characteristics uncovered that seemingly distinct dynamical patterns exhibit
similar temporal complexity behaviors. In particular, similar power-law decay
in the activity distribution and similar complexity levels are observed in both
topologies, but with a much reduced noise in the scale-free topology. Notably,
most of the complex dynamical profiles were consistently observed in scale-free
network configurations, thus confirming the crucial role of hubs in neural
network dynamics.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [228] [Pareto-optimal Tradeoffs Between Communication and Computation with Flexible Gradient Tracking](https://arxiv.org/abs/2509.18129)
*Yan Huang,Jinming Xu,Li Chai,Jiming Chen,Karl H. Johansson*

Main category: math.OC

TL;DR: 提出FlexGT和Acc - FlexGT方法解决非独立同分布场景下分布式优化问题，分析收敛率和复杂度，通过数值例子验证有效性。


<details>
  <summary>Details</summary>
Motivation: 解决非独立同分布场景下分布式优化问题，平衡通信和计算效率。

Method: 提出FlexGT方法，有可调节的本地更新和邻域通信次数；引入加速的基于流言的变体Acc - FlexGT。

Result: FlexGT根据目标特性和可调参数实现线性或次线性收敛率，对节点异质性鲁棒，有最优通信和计算复杂度；Acc - FlexGT在非凸和强凸情况下有良好迭代复杂度。

Conclusion: 提出的方法有效，能在通信和计算间实现良好权衡。

Abstract: This paper addresses distributed optimization problems in non-i.i.d.
scenarios, focusing on the interplay between communication and computation
efficiency. To this end, we propose FlexGT, a flexible snapshot gradient
tracking method with tunable numbers of local updates and neighboring
communications in each round. Leveraging a unified convergence analysis
framework, we prove that FlexGT achieves a linear or sublinear convergence rate
depending on objective-specific properties--from (strongly) convex to
nonconvex--and the above-mentioned tunable parameters. FlexGT is provably
robust to the heterogeneity across nodes and attains the best-known
communication and computation complexity among existing results. Moreover, we
introduce an accelerated gossip-based variant, termed Acc-FlexGT, and show that
with prior knowledge of the graph, it achieves a Pareto-optimal trade-off
between communication and computation. Particularly, Acc-FlexGT achieves the
optimal iteration complexity of $\tilde{\mathcal{O}} \left( L/\epsilon +L\sigma
^2/\left( n\epsilon^2 \sqrt{1-\sqrt{\rho _W}} \right) \right) $ for the
nonconvex case, matching the existing lower bound up to a logarithmic factor,
and improves the existing results for the strongly convex case by a factor of
$\tilde{\mathcal{O}} \left( 1/\sqrt{\epsilon} \right)$, where $\epsilon$ is the
targeted accuracy, $n$ the number of nodes, $L$ the Lipschitz constant,
$\rho_W$ the spectrum gap of the graph, and $\sigma$ the stochastic gradient
variance. Numerical examples are provided to demonstrate the effectiveness of
the proposed methods.

</details>


### [229] [Clapping: Removing Per-sample Storage for Pipeline Parallel Distributed Optimization with Communication Compression](https://arxiv.org/abs/2509.19029)
*Boao Kong,Xu Huang,Yuqi Xu,Yixuan Liang,Bin Wang,Kun Yuan*

Main category: math.OC

TL;DR: 本文提出用于流水线并行学习的通信压缩算法Clapping，采用懒采样策略，有两个变体，实验验证其性能。


<details>
  <summary>Details</summary>
Motivation: 流水线并行分布式优化面临通信开销大问题，现有方法有依赖不切实际假设或有样本内存开销的不足。

Method: 引入Clapping算法，采用懒采样策略复用数据样本，有Clapping - FC和Clapping - FU两个变体。

Result: Clapping在无无偏梯度假设下实现收敛，有效解决多工作节点设置中的压缩误差传播问题。

Conclusion: 数值实验验证了Clapping在不同学习任务中的性能。

Abstract: Pipeline-parallel distributed optimization is essential for large-scale
machine learning but is challenged by significant communication overhead from
transmitting high-dimensional activations and gradients between workers.
Existing approaches often depend on impractical unbiased gradient assumptions
or incur sample-size memory overhead. This paper introduces Clapping, a
Communication compression algorithm with LAzy samPling for Pipeline-parallel
learnING. Clapping adopts a lazy sampling strategy that reuses data samples
across steps, breaking sample-wise memory barrier and supporting convergence in
few-epoch or online training regimes. Clapping comprises two variants including
Clapping-FC and Clapping-FU, both of which achieve convergence without unbiased
gradient assumption, effectively addressing compression error propagation in
multi-worker settings. Numerical experiments validate the performance of
Clapping across different learning tasks.

</details>


### [230] [Joint Cooperative and Non-Cooperative Localization in WSNs with Distributed Scaled Proximal ADMM Algorithms](https://arxiv.org/abs/2509.18213)
*Qiaojia Zhu,Xiaojing Shen,Haiqi Liu,Pramod K. Varshney*

Main category: math.OC

TL;DR: 本文提出联合建模方法处理无线传感器网络中合作与非合作定位问题，开发SP - ADMM - JCNL算法，实验证明其定位性能准确可靠。


<details>
  <summary>Details</summary>
Motivation: 联合处理合作与非合作定位虽能消除顺序方法的目标估计延迟，但引入复杂变量耦合，在建模和优化上带来挑战。

Method: 提出联合建模方法将二者定位问题化为单一优化问题，引入辅助变量解耦和分布式计算，开发SP - ADMM - JCNL算法。

Result: 算法生成的序列全局收敛到重构问题的KKT点及原非凸目标函数的临界点，收敛速率为O(1/T)，实验表明算法定位性能准确可靠。

Conclusion: SP - ADMM - JCNL算法能有效解决无线传感器网络中合作与非合作定位问题，实现准确可靠的定位。

Abstract: Cooperative and non-cooperative localization frequently arise together in
wireless sensor networks, particularly when sensor positions are uncertain and
targets are unable to communicate with the network. While joint processing can
eliminate the delay in target estimation found in sequential approaches, it
introduces complex variable coupling, posing challenges in both modeling and
optimization. This paper presents a joint modeling approach that formulates
cooperative and non-cooperative localization as a single optimization problem.
To address the resulting coupling, we introduce auxiliary variables that enable
structural decoupling and distributed computation. Building on this
formulation, we develop the Scaled Proximal Alternating Direction Method of
Multipliers for Joint Cooperative and Non-Cooperative Localization
(SP-ADMM-JCNL). Leveraging the problem's structured design, we provide
theoretical guarantees that the algorithm generates a sequence converging
globally to the Karush-Kuhn-Tucker (KKT) point of the reformulated problem and
further to a critical point of the original non-convex objective function, with
a sublinear rate of O(1/T). Experiments on both synthetic and benchmark
datasets demonstrate that SP-ADMM-JCNL achieves accurate and reliable
localization performance.

</details>


### [231] [Zero-Shot Transferable Solution Method for Parametric Optimal Control Problems](https://arxiv.org/abs/2509.18404)
*Xingjian Li,Kelvin Kan,Deepanshu Verma,Krishna Kumar,Stanley Osher,Ján Drgoňa*

Main category: math.OC

TL;DR: 本文提出用函数编码器（FE）策略解决目标可变的最优控制问题的可迁移方法，经实验验证该方法有良好泛化性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于优化的方法在目标改变时需重新求解，计算成本高，不适用于需频繁评估和调整的应用。

Method: 学习一组可复用的神经基函数来覆盖控制策略空间，采用离线 - 在线分解，离线时进行模仿学习，在线时进行轻量级系数估计。

Result: 在不同动力学、维度和成本结构的数值实验中，该方法在跨任务泛化时以最小开销实现接近最优的性能。

Conclusion: 该方法能实现适用于实时部署的半全局反馈策略。

Abstract: This paper presents a transferable solution method for optimal control
problems with varying objectives using function encoder (FE) policies.
Traditional optimization-based approaches must be re-solved whenever objectives
change, resulting in prohibitive computational costs for applications requiring
frequent evaluation and adaptation. The proposed method learns a reusable set
of neural basis functions that spans the control policy space, enabling
efficient zero-shot adaptation to new tasks through either projection from data
or direct mapping from problem specifications. The key idea is an
offline-online decomposition: basis functions are learned once during offline
imitation learning, while online adaptation requires only lightweight
coefficient estimation. Numerical experiments across diverse dynamics,
dimensions, and cost structures show our method delivers near-optimal
performance with minimal overhead when generalizing across tasks, enabling
semi-global feedback policies suitable for real-time deployment.

</details>


### [232] [Learning When to Restart: Nonstationary Newsvendor from Uncensored to Censored Demand](https://arxiv.org/abs/2509.18709)
*Xin Chen,Jiameng Lyu,Shilin Yuan,Yuan Zhou*

Main category: math.OC

TL;DR: 本文研究非平稳报童问题，提出分布检测重启框架及算法，建立最优性理论，实验展示算法性能，框架适用广泛。


<details>
  <summary>Details</summary>
Motivation: 解决非平稳程度未知和需求审查的实际挑战，为非平稳环境下决策提供解决方案。

Method: 提出分布检测重启框架，针对无审查和有审查需求设置设计两种算法。

Result: 建立算法的最优性理论，推导匹配的后悔上下界；数值实验表明算法在实际数据上性能优越且稳健。

Conclusion: 分布检测重启框架适用于广泛的非平稳随机优化问题，为非平稳下决策提供实用、易部署且有理论依据的解决方案。

Abstract: We study nonstationary newsvendor problems under nonparametric demand models
and general distributional measures of nonstationarity, addressing the
practical challenges of unknown degree of nonstationarity and demand censoring.
We propose a novel distributional-detection-and-restart framework for learning
in nonstationary environments, and instantiate it through two efficient
algorithms for the uncensored and censored demand settings. The algorithms are
fully adaptive, requiring no prior knowledge of the degree and type of
nonstationarity, and offer a flexible yet powerful approach to handling both
abrupt and gradual changes in nonstationary environments. We establish a
comprehensive optimality theory for our algorithms by deriving matching regret
upper and lower bounds under both general and refined structural conditions
with nontrivial proof techniques that are of independent interest. Numerical
experiments using real-world datasets, including nurse staffing data for
emergency departments and COVID-19 test demand data, showcase the algorithms'
superior and robust empirical performance. While motivated by the newsvendor
problem, the distributional-detection-and-restart framework applies broadly to
a wide class of nonstationary stochastic optimization problems. Managerially,
our framework provides a practical, easy-to-deploy, and theoretically grounded
solution for decision-making under nonstationarity.

</details>


### [233] [On the Convergence of Policy Mirror Descent with Temporal Difference Evaluation](https://arxiv.org/abs/2509.18822)
*Jiacai Liu,Wenye Li,Ke Wei*

Main category: math.OC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Policy mirror descent (PMD) is a general policy optimization framework in
reinforcement learning, which can cover a wide range of typical policy
optimization methods by specifying different mirror maps. Existing analysis of
PMD requires exact or approximate evaluation (for example unbiased estimation
via Monte Carlo simulation) of action values solely based on policy. In this
paper, we consider policy mirror descent with temporal difference evaluation
(TD-PMD). It is shown that, given the access to exact policy evaluations, the
dimension-free $O(1/T)$ sublinear convergence still holds for TD-PMD with any
constant step size and any initialization. In order to achieve this result, new
monotonicity and shift invariance arguments have been developed. The dimension
free $\gamma$-rate linear convergence of TD-PMD is also established provided
the step size is selected adaptively. For the two common instances of TD-PMD
(i.e., TD-PQA and TD-NPG), it is further shown that they enjoy the convergence
in the policy domain. Additionally, we investigate TD-PMD in the inexact
setting and give the sample complexity for it to achieve the last iterate
$\varepsilon$-optimality under a generative model, which improves the last
iterate sample complexity for PMD over the dependence on $1/(1-\gamma)$.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [234] [Security smells in infrastructure as code: a taxonomy update beyond the seven sins](https://arxiv.org/abs/2509.18761)
*Aicha War,Serge L. B. Nikiema,Jordan Samhi,Jacques Klein,Tegawende F. Bissyande*

Main category: cs.CR

TL;DR: 本文重新审视IaC脚本安全漏洞分类法，使用多工具数据集和LLM自动化分析，得到62类安全漏洞分类，还实现新安全检查规则，揭示漏洞长期存在问题并为从业者提供建议。


<details>
  <summary>Details</summary>
Motivation: 现有IaC脚本安全漏洞分类法受限于单一工具和大量人工工作，需改进以提升IaC安全性。

Method: 将研究扩展到七种流行IaC工具脚本的多样化数据集，利用LLM进行自动化分析，同时进行人工验证和与安全标准核对。

Result: 得出包含62个安全漏洞类别的综合分类法，在代码检查工具中实现新安全检查规则，常达到1.00的精确率，发现安全漏洞在GitHub项目中长期存在。

Conclusion: 为IaC从业者提供解决常见安全漏洞的见解，助力系统采用DevSecOps实践构建更安全的基础设施代码。

Abstract: Infrastructure as Code (IaC) has become essential for modern software
management, yet security flaws in IaC scripts can have severe consequences, as
exemplified by the recurring exploits of Cloud Web Services. Prior work has
recognized the need to build a precise taxonomy of security smells in IaC
scripts as a first step towards developing approaches to improve IaC security.
This first effort led to the unveiling of seven sins, limited by the focus on a
single IaC tool as well as by the extensive, and potentially biased, manual
effort that was required. We propose, in our work, to revisit this taxonomy:
first, we extend the study of IaC security smells to a more diverse dataset
with scripts associated with seven popular IaC tools, including Terraform,
Ansible, Chef, Puppet, Pulumi, Saltstack, and Vagrant; second, we bring in some
automation for the analysis by relying on an LLM. While we leverage LLMs for
initial pattern processing, all taxonomic decisions underwent systematic human
validation and reconciliation with established security standards. Our study
yields a comprehensive taxonomy of 62 security smell categories, significantly
expanding beyond the previously known seven. We demonstrate actionability by
implementing new security checking rules within linters for seven popular IaC
tools, often achieving 1.00 precision score. Our evolution study of security
smells in GitHub projects reveals that these issues persist for extended
periods, likely due to inadequate detection and mitigation tools. This work
provides IaC practitioners with insights for addressing common security smells
and systematically adopting DevSecOps practices to build safer infrastructure
code.

</details>


### [235] [Detection of security smells in IaC scripts through semantics-aware code and language processing](https://arxiv.org/abs/2509.18790)
*Aicha War,Adnan A. Rawass,Abdoul K. Kabore,Jordan Samhi,Jacques Klein,Tegawende F. Bissyande*

Main category: cs.CR

TL;DR: 本文提出结合自然语言和代码表示增强静态分析的方法检测IaC脚本安全配置错误，在Ansible和Puppet数据集评估效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有检测IaC脚本安全配置错误的方法多依赖静态分析，本文旨在引入语义理解增强检测效果。

Method: 结合CodeBERT和LongFormer两个互补的ML模型，利用自然语言和代码表示增强静态分析，在Ansible和Puppet错误配置数据集上评估，并进行消融实验和与其他模型对比。

Result: 语义增强显著提高检测效果，Ansible的精确率和召回率从0.46和0.79提升到0.92和0.88，Puppet从0.55和0.97提升到0.87和0.75。

Conclusion: 结合自然语言和代码表示增强静态分析的方法能有效提升IaC脚本安全配置错误的检测性能。

Abstract: Infrastructure as Code (IaC) automates the provisioning and management of IT
infrastructure through scripts and tools, streamlining software deployment.
Prior studies have shown that IaC scripts often contain recurring security
misconfigurations, and several detection and mitigation approaches have been
proposed. Most of these rely on static analysis, using statistical code
representations or Machine Learning (ML) classifiers to distinguish insecure
configurations from safe code.
  In this work, we introduce a novel approach that enhances static analysis
with semantic understanding by jointly leveraging natural language and code
representations. Our method builds on two complementary ML models: CodeBERT, to
capture semantics across code and text, and LongFormer, to represent long IaC
scripts without losing contextual information. We evaluate our approach on
misconfiguration datasets from two widely used IaC tools, Ansible and Puppet.
To validate its effectiveness, we conduct two ablation studies (removing code
text from the natural language input and truncating scripts to reduce context)
and compare against four large language models (LLMs) and prior work. Results
show that semantic enrichment substantially improves detection, raising
precision and recall from 0.46 and 0.79 to 0.92 and 0.88 on Ansible, and from
0.55 and 0.97 to 0.87 and 0.75 on Puppet, respectively.

</details>


### [236] [Security Evaluation of Android apps in budget African Mobile Devices](https://arxiv.org/abs/2509.18800)
*Alioune Diallo,Anta Diop,Abdoul Kader Kabore,Jordan Samhi,Aleksandr Pilgun,Tegawendé F. Bissyande,Jacque Klein*

Main category: cs.CR

TL;DR: 开发框架分析非洲廉价安卓机预装APK，发现诸多隐私安全问题，表明预装应用威胁大。


<details>
  <summary>Details</summary>
Motivation: 预算安卓设备预装系统和厂商应用有高权限但缺乏独立审查，为填补此空白开展研究。

Method: 开发框架从物理设备提取APK并进行静态分析。

Result: 分析1544个APK，发现部分应用存在泄露敏感数据、暴露关键组件、执行危险命令等问题，还有厂商包传输设备和位置信息。

Conclusion: 广泛分布的低成本设备预装应用对用户安全和隐私构成重大且未充分探索的威胁。

Abstract: Android's open-source nature facilitates widespread smartphone accessibility,
particularly in price-sensitive markets. System and vendor applications that
come pre-installed on budget Android devices frequently operate with elevated
privileges, yet they receive limited independent examination. To address this
gap, we developed a framework that extracts APKs from physical devices and
applies static analysis to identify privacy and security issues in embedded
software. Our study examined 1,544 APKs collected from seven African
smartphones. The analysis revealed that 145 applications (9%) disclose
sensitive data, 249 (16%) expose critical components without sufficient
safeguards, and many present additional risks: 226 execute privileged or
dangerous commands, 79 interact with SMS messages (read, send, or delete), and
33 perform silent installation operations. We also uncovered a vendor-supplied
package that appears to transmit device identifiers and location details to an
external third party. These results demonstrate that pre-installed applications
on widely distributed low-cost devices represent a significant and
underexplored threat to user security and privacy.

</details>


### [237] [LLM-based Vulnerability Discovery through the Lens of Code Metrics](https://arxiv.org/abs/2509.19117)
*Felix Weissberg,Lukas Pirch,Erik Imgrund,Jonas Möller,Thorsten Eisenhofer,Konrad Rieck*

Main category: cs.CR

TL;DR: 研究发现仅基于经典代码指标训练的分类器在漏洞发现上与大语言模型表现相当，指出大语言模型依赖代码指标，挖掘复杂模式能力有限，并给出研究建议。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型在漏洞发现领域进展停滞的现象。

Method: 通过经典代码指标研究大语言模型，进行根因分析。

Result: 仅基于代码指标训练的分类器与大语言模型表现相当，大语言模型与代码指标有强关联和因果效应。

Conclusion: 大语言模型挖掘复杂模式能力有限，给出研究应对挑战的建议。

Abstract: Large language models (LLMs) excel in many tasks of software engineering, yet
progress in leveraging them for vulnerability discovery has stalled in recent
years. To understand this phenomenon, we investigate LLMs through the lens of
classic code metrics. Surprisingly, we find that a classifier trained solely on
these metrics performs on par with state-of-the-art LLMs for vulnerability
discovery. A root-cause analysis reveals a strong correlation and a causal
effect between LLMs and code metrics: When the value of a metric is changed,
LLM predictions tend to shift by a corresponding magnitude. This dependency
suggests that LLMs operate at a similarly shallow level as code metrics,
limiting their ability to grasp complex patterns and fully realize their
potential in vulnerability discovery. Based on these findings, we derive
recommendations on how research should more effectively address this challenge.

</details>


### [238] [LLMs as verification oracles for Solidity](https://arxiv.org/abs/2509.19153)
*Massimo Bartoletti,Enrico Lipparini,Livio Pompianu*

Main category: cs.CR

TL;DR: 本文首次系统评估GPT - 5作为智能合约验证预言机的能力，结合定量和定性分析，表明推理型大语言模型有潜力用于安全的智能合约开发和审计。


<details>
  <summary>Details</summary>
Motivation: 现有智能合约错误检测工具存在局限，大语言模型用于安全相关任务已被探索，但能否作为验证预言机尚不明确，因此评估GPT - 5在该角色的表现。

Method: 在大量验证任务数据集上对GPT - 5进行基准测试，将其输出与现有形式验证工具对比，结合定量指标和定性分析评估其在现实审计场景中的有效性。

Result: 推理型大语言模型作为验证预言机的效果出人意料地好。

Conclusion: 推理型大语言模型为人工智能与形式化方法在安全智能合约开发和审计中的融合开辟了新领域。

Abstract: Ensuring the correctness of smart contracts is critical, as even subtle flaws
can lead to severe financial losses. While bug detection tools able to spot
common vulnerability patterns can serve as a first line of defense, most
real-world exploits and losses stem from errors in the contract business logic.
Formal verification tools such as SolCMC and the Certora Prover address this
challenge, but their impact remains limited by steep learning curves and
restricted specification languages. Recent works have begun to explore the use
of large language models (LLMs) for security-related tasks such as
vulnerability detection and test generation. Yet, a fundamental question
remains open: can LLMs serve as verification oracles, capable of reasoning
about arbitrary contract-specific properties? In this paper, we provide the
first systematic evaluation of GPT-5, a state-of-the-art reasoning LLM, in this
role. We benchmark its performance on a large dataset of verification tasks,
compare its outputs against those of established formal verification tools, and
assess its practical effectiveness in real-world auditing scenarios. Our study
combines quantitative metrics with qualitative analysis, and shows that recent
reasoning-oriented LLMs can be surprisingly effective as verification oracles,
suggesting a new frontier in the convergence of AI and formal methods for
secure smart contract development and auditing.

</details>


### [239] [Context Lineage Assurance for Non-Human Identities in Critical Multi-Agent Systems](https://arxiv.org/abs/2509.18415)
*Sumana Malkapuram,Sameera Gangavarapu,Kailashnath Reddy Kavalakuntla,Ananya Gangavarapu*

Main category: cs.CR

TL;DR: 为解决自主软件代理间安全交互问题，扩展A2A范式，引入基于密码学的谱系验证机制和增强的代理卡，提升代理间生态安全性。


<details>
  <summary>Details</summary>
Motivation: 自主软件代理大量出现，需要建立安全可验证的代理间交互框架，尤其是非人类身份代理。

Method: 引入基于密码学的谱系验证机制，用类证书透明日志的Merkle树结构；设联邦证明服务器聚合证明；增强A2A代理卡以进行身份验证。

Result: 能让代理和外部验证者对多跳来源进行密码学验证，联邦证明服务器可生成可验证的证明，代理卡可标准化验证身份。

Conclusion: 建立了融合身份认证、谱系验证和独立证明审计的模型，提升了代理间生态安全，为受监管环境中非人类身份治理提供基础。

Abstract: The proliferation of autonomous software agents necessitates rigorous
frameworks for establishing secure and verifiable agent-to-agent (A2A)
interactions, particularly when such agents are instantiated as non-human
identities(NHIs). We extend the A2A paradigm [1 , 2] by introducing a
cryptographically grounded mechanism for lineage verification, wherein the
provenance and evolution of NHIs are anchored in append-only Merkle tree
structures modeled after Certificate Transparency (CT) logs. Unlike traditional
A2A models that primarily secure point-to-point interactions, our approach
enables both agents and external verifiers to cryptographically validate
multi-hop provenance, thereby ensuring the integrity of the entire call chain.
  A federated proof server acts as an auditor across one or more Merkle logs,
aggregating inclusion proofs and consistency checks into compact, signed
attestations that external parties can verify without access to the full
execution trace. In parallel, we augment the A2A agent card to incorporate
explicit identity verification primitives, enabling both peer agents and human
approvers to authenticate the legitimacy of NHI representations in a
standardized manner. Together, these contributions establish a cohesive model
that integrates identity attestation, lineage verification, and independent
proof auditing, thereby advancing the security posture of inter-agent
ecosystems and providing a foundation for robust governance of NHIs in
regulated environments such as FedRAMP.

</details>


### [240] [Coherence-driven inference for cybersecurity](https://arxiv.org/abs/2509.18520)
*Steve Huntsman*

Main category: cs.CR

TL;DR: 大语言模型可在自然语言数据上编译加权图以实现与网络安全红蓝队行动相关的自动连贯性驱动推理，对网络安全决策有前景。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在网络安全决策和自主蓝队行动中的应用可能性。

Method: 使用大语言模型在自然语言数据上编译加权图实现自动连贯性驱动推理。

Result: 实现了与网络安全红蓝队行动相关的自动连贯性驱动推理。

Conclusion: 该自动连贯性驱动推理的早期应用对网络安全决策有近中期前景，最终也可能用于自主蓝队行动。

Abstract: Large language models (LLMs) can compile weighted graphs on natural language
data to enable automatic coherence-driven inference (CDI) relevant to red and
blue team operations in cybersecurity. This represents an early application of
automatic CDI that holds near- to medium-term promise for decision-making in
cybersecurity and eventually also for autonomous blue team operations.

</details>


### [241] [VoxGuard: Evaluating User and Attribute Privacy in Speech via Membership Inference Attacks](https://arxiv.org/abs/2509.18413)
*Efthymios Tsaprazlis,Thanathai Lertpetchpun,Tiantian Feng,Sai Praneeth Karimireddy,Shrikanth Narayanan*

Main category: cs.CR

TL;DR: 当前语音匿名评估依赖EER有缺陷，提出VoxGuard框架，研究表明EER低估泄漏，推荐用VoxGuard评估隐私泄漏。


<details>
  <summary>Details</summary>
Motivation: 当前语音匿名评估几乎只依赖EER，无法判断对手是否能进行高精度攻击，需在低误报率制度下评估隐私。

Method: 引入基于差分隐私和成员推理的VoxGuard框架，定义用户隐私和属性隐私两个概念。

Result: 低误报率下，知情对手攻击更强，简单攻击可近乎完美恢复属性。EER大幅低估泄漏。

Conclusion: 强调低误报率评估的必要性，推荐VoxGuard作为评估隐私泄漏的基准。

Abstract: Voice anonymization aims to conceal speaker identity and attributes while
preserving intelligibility, but current evaluations rely almost exclusively on
Equal Error Rate (EER) that obscures whether adversaries can mount
high-precision attacks. We argue that privacy should instead be evaluated in
the low false-positive rate (FPR) regime, where even a small number of
successful identifications constitutes a meaningful breach. To this end, we
introduce VoxGuard, a framework grounded in differential privacy and membership
inference that formalizes two complementary notions: User Privacy, preventing
speaker re-identification, and Attribute Privacy, protecting sensitive traits
such as gender and accent. Across synthetic and real datasets, we find that
informed adversaries, especially those using fine-tuned models and
max-similarity scoring, achieve orders-of-magnitude stronger attacks at low-FPR
despite similar EER. For attributes, we show that simple transparent attacks
recover gender and accent with near-perfect accuracy even after anonymization.
Our results demonstrate that EER substantially underestimates leakage,
highlighting the need for low-FPR evaluation, and recommend VoxGuard as a
benchmark for evaluating privacy leakage.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [242] [Automatic Classification of Magnetic Chirality of Solar Filaments from H-Alpha Observations](https://arxiv.org/abs/2509.18214)
*Alexis Chalmers,Azim Ahmadzadeh*

Main category: astro-ph.SR

TL;DR: 使用图像分类模型对太阳细丝磁手性分类，在MAGFiLO数据集建立基线，微调模型获较好结果。


<details>
  <summary>Details</summary>
Motivation: 以往研究数据集小，限制了可推广性和可比性，需要建立可重复的基线。

Method: 在MAGFiLO数据集上微调ResNet、WideResNet、ResNeXt和ConvNeXt等预训练图像分类架构，应用数据增强和按类损失权重优化模型。

Result: 最佳模型ConvNeXtBase对左旋细丝的每类准确率为0.69，右旋为0.73。

Conclusion: 利用图像分类模型在较大数据集上进行太阳细丝磁手性分类取得一定成果。

Abstract: In this study, we classify the magnetic chirality of solar filaments from
H-Alpha observations using state-of-the-art image classification models. We
establish the first reproducible baseline for solar filament chirality
classification on the MAGFiLO dataset. The MAGFiLO dataset contains over 10,000
manually-annotated filaments from GONG H-Alpha observations, making it the
largest dataset for filament detection and classification to date. Prior
studies relied on much smaller datasets, which limited their generalizability
and comparability. We fine-tuned several pre-trained, image classification
architectures, including ResNet, WideResNet, ResNeXt, and ConvNeXt, and also
applied data augmentation and per-class loss weights to optimize the models.
Our best model, ConvNeXtBase, achieves a per-class accuracy of 0.69 for left
chirality filaments and $0.73$ for right chirality filaments.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [243] [Application Management in C-ITS: Orchestrating Demand-Driven Deployments and Reconfigurations](https://arxiv.org/abs/2509.18793)
*Lukas Zanger,Bastian Lampe,Lennart Reiher,Lutz Eckstein*

Main category: cs.RO

TL;DR: 本文提出一种基于云原生技术（Kubernetes）的需求驱动应用管理方法，处理大规模C - ITS应用编排挑战，在集体环境感知用例中验证并开源代码。


<details>
  <summary>Details</summary>
Motivation: 大规模C - ITS应用编排因环境动态性和资源高效利用需求面临独特挑战。

Method: 提出需求驱动应用管理方法，利用Kubernetes和ROS 2构建应用管理框架，考虑C - ITS内不同实体需求，自动化微服务相关流程。

Result: 该方法可减少计算资源消耗和网络流量，框架能动态处理变化和新需求。

Conclusion: 所提方法和框架能有效应对大规模C - ITS应用编排挑战。

Abstract: Vehicles are becoming increasingly automated and interconnected, enabling the
formation of cooperative intelligent transport systems (C-ITS) and the use of
offboard services. As a result, cloud-native techniques, such as microservices
and container orchestration, play an increasingly important role in their
operation. However, orchestrating applications in a large-scale C-ITS poses
unique challenges due to the dynamic nature of the environment and the need for
efficient resource utilization. In this paper, we present a demand-driven
application management approach that leverages cloud-native techniques -
specifically Kubernetes - to address these challenges. Taking into account the
demands originating from different entities within the C-ITS, the approach
enables the automation of processes, such as deployment, reconfiguration,
update, upgrade, and scaling of microservices. Executing these processes on
demand can, for example, reduce computing resource consumption and network
traffic. A demand may include a request for provisioning an external supporting
service, such as a collective environment model. The approach handles changing
and new demands by dynamically reconciling them through our proposed
application management framework built on Kubernetes and the Robot Operating
System (ROS 2). We demonstrate the operation of our framework in the C-ITS use
case of collective environment perception and make the source code of the
prototypical framework publicly available at
https://github.com/ika-rwth-aachen/application_manager .

</details>


### [244] [PEEK: Guiding and Minimal Image Representations for Zero-Shot Generalization of Robot Manipulation Policies](https://arxiv.org/abs/2509.18282)
*Jesse Zhang,Marius Memmel,Kevin Kim,Dieter Fox,Jesse Thomason,Fabio Ramos,Erdem Bıyık,Abhishek Gupta,Anqi Li*

Main category: cs.RO

TL;DR: 提出PEEK方法，利用VLMs预测统一中间表示，提升机器人操作策略零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 机器人操作策略泛化性差，需将高层推理任务交给VLMs，让策略专注于动作执行。

Method: 提出PEEK，微调VLMs预测统一的基于点的中间表示，引入自动标注流程。

Result: 在真实世界评估中，PEEK持续提升零样本泛化能力，如3D策略在现实中提升41.4倍等。

Conclusion: 让VLMs承担语义和视觉复杂性，PEEK为操作策略提供必要线索。

Abstract: Robotic manipulation policies often fail to generalize because they must
simultaneously learn where to attend, what actions to take, and how to execute
them. We argue that high-level reasoning about where and what can be offloaded
to vision-language models (VLMs), leaving policies to specialize in how to act.
We present PEEK (Policy-agnostic Extraction of Essential Keypoints), which
fine-tunes VLMs to predict a unified point-based intermediate representation:
1. end-effector paths specifying what actions to take, and 2. task-relevant
masks indicating where to focus. These annotations are directly overlaid onto
robot observations, making the representation policy-agnostic and transferable
across architectures. To enable scalable training, we introduce an automatic
annotation pipeline, generating labeled data across 20+ robot datasets spanning
9 embodiments. In real-world evaluations, PEEK consistently boosts zero-shot
generalization, including a 41.4x real-world improvement for a 3D policy
trained only in simulation, and 2-3.5x gains for both large VLAs and small
manipulation policies. By letting VLMs absorb semantic and visual complexity,
PEEK equips manipulation policies with the minimal cues they need--where, what,
and how. Website at https://peek-robot.github.io/.

</details>


### [245] [Assistive Decision-Making for Right of Way Navigation at Uncontrolled Intersections](https://arxiv.org/abs/2509.18407)
*Navya Tiwari,Joseph Vazhaeparampil,Victoria Preston*

Main category: cs.RO

TL;DR: 提出用于无控制交叉口路权推理的驾驶辅助框架，评估四种决策方法，概率规划器表现更好，强调不确定性感知规划重要性。


<details>
  <summary>Details</summary>
Motivation: 无控制交叉口事故多，缺乏适用于人工驾驶车辆的辅助导航系统。

Method: 将路权推理问题建模为POMDP，用自定义模拟测试平台评估四种决策方法（确定性有限状态机FSM和三种概率规划器QMDP、POMCP、DESPOT）。

Result: 概率规划器优于基于规则的基线，部分可观测下实现高达97.5%无碰撞导航，POMCP重安全，DESPOT兼顾效率和运行可行性。

Conclusion: 强调不确定性感知规划对驾驶辅助的重要性，推动未来在现实交通环境实时部署中集成传感器融合和环境感知模块。

Abstract: Uncontrolled intersections account for a significant fraction of roadway
crashes due to ambiguous right-of-way rules, occlusions, and unpredictable
driver behavior. While autonomous vehicle research has explored
uncertainty-aware decision making, few systems exist to retrofit human-operated
vehicles with assistive navigation support. We present a driver-assist
framework for right-of-way reasoning at uncontrolled intersections, formulated
as a Partially Observable Markov Decision Process (POMDP). Using a custom
simulation testbed with stochastic traffic agents, pedestrians, occlusions, and
adversarial scenarios, we evaluate four decision-making approaches: a
deterministic finite state machine (FSM), and three probabilistic planners:
QMDP, POMCP, and DESPOT. Results show that probabilistic planners outperform
the rule-based baseline, achieving up to 97.5 percent collision-free navigation
under partial observability, with POMCP prioritizing safety and DESPOT
balancing efficiency and runtime feasibility. Our findings highlight the
importance of uncertainty-aware planning for driver assistance and motivate
future integration of sensor fusion and environment perception modules for
real-time deployment in realistic traffic environments.

</details>


### [246] [PrioriTouch: Adapting to User Contact Preferences for Whole-Arm Physical Human-Robot Interaction](https://arxiv.org/abs/2509.18447)
*Rishabh Madan,Jiawei Lin,Mahika Goel,Angchen Xie,Xiaoyu Liang,Marcus Lee,Justin Guo,Pranav N. Thakkar,Rohan Banerjee,Jose Barreiros,Kate Tsui,Tom Silver,Tapomayukh Bhattacharjee*

Main category: cs.RO

TL;DR: 提出PrioriTouch框架用于多接触场景中控制目标的排序和执行，结合新方法，经评估能适应接触偏好、维持任务表现并提升安全性和舒适度。


<details>
  <summary>Details</summary>
Motivation: 物理人机交互需机器人适应个体接触偏好，多接触时识别偏好难，存在冲突且需权衡，因此需要进行优先级排序。

Method: 结合学习排序方法与分层操作空间控制，利用仿真循环滚动进行数据高效且安全的探索，开展用户研究获取个性化舒适阈值并融入框架。

Result: 通过大量仿真和真实世界实验，证明PrioriTouch能适应接触偏好、维持任务表现、提升安全和舒适度。

Conclusion: PrioriTouch框架可用于多接触场景的控制目标优先级排序和执行，具有广泛适用性。

Abstract: Physical human-robot interaction (pHRI) requires robots to adapt to
individual contact preferences, such as where and how much force is applied.
Identifying preferences is difficult for a single contact; with whole-arm
interaction involving multiple simultaneous contacts between the robot and
human, the challenge is greater because different body parts can impose
incompatible force requirements. In caregiving tasks, where contact is frequent
and varied, such conflicts are unavoidable. With multiple preferences across
multiple contacts, no single solution can satisfy all objectives--trade-offs
are inherent, making prioritization essential. We present PrioriTouch, a
framework for ranking and executing control objectives across multiple
contacts. PrioriTouch can prioritize from a general collection of controllers,
making it applicable not only to caregiving scenarios such as bed bathing and
dressing but also to broader multi-contact settings. Our method combines a
novel learning-to-rank approach with hierarchical operational space control,
leveraging simulation-in-the-loop rollouts for data-efficient and safe
exploration. We conduct a user study on physical assistance preferences, derive
personalized comfort thresholds, and incorporate them into PrioriTouch. We
evaluate PrioriTouch through extensive simulation and real-world experiments,
demonstrating its ability to adapt to user contact preferences, maintain task
performance, and enhance safety and comfort. Website:
https://emprise.cs.cornell.edu/prioritouch.

</details>


### [247] [LCMF: Lightweight Cross-Modality Mambaformer for Embodied Robotics VQA](https://arxiv.org/abs/2509.18576)
*Zeyi Kang,Liang He,Yanxin Zhang,Zuheng Ming,Kaixing Zhao*

Main category: cs.RO

TL;DR: 本文提出轻量级LCMF级联注意力框架解决多模态语义学习挑战，实验显示其在VQA和EQA任务表现佳，参数少、计算量低，适用于资源受限场景。


<details>
  <summary>Details</summary>
Motivation: 多模态语义学习在具身智能中重要，但面临异构数据融合和资源受限环境计算效率等挑战。

Method: 提出轻量级LCMF级联注意力框架，在Mamba模块引入多级跨模态参数共享机制，融合Cross - Attention和选择性参数共享状态空间模型优势。

Result: LCMF在VQA任务准确率达74.29%超现有基线；在EQA视频任务中在大语言模型智能体分布集群中达中等竞争力；轻量级设计相比可比基线FLOPs降低4.35倍，参数少。

Conclusion: LCMF为资源受限场景下的人机交互应用提供高效解决方案，有强多模态决策泛化能力。

Abstract: Multimodal semantic learning plays a critical role in embodied intelligence,
especially when robots perceive their surroundings, understand human
instructions, and make intelligent decisions. However, the field faces
technical challenges such as effective fusion of heterogeneous data and
computational efficiency in resource-constrained environments. To address these
challenges, this study proposes the lightweight LCMF cascaded attention
framework, introducing a multi-level cross-modal parameter sharing mechanism
into the Mamba module. By integrating the advantages of Cross-Attention and
Selective parameter-sharing State Space Models (SSMs), the framework achieves
efficient fusion of heterogeneous modalities and semantic complementary
alignment. Experimental results show that LCMF surpasses existing multimodal
baselines with an accuracy of 74.29% in VQA tasks and achieves competitive
mid-tier performance within the distribution cluster of Large Language Model
Agents (LLM Agents) in EQA video tasks. Its lightweight design achieves a
4.35-fold reduction in FLOPs relative to the average of comparable baselines
while using only 166.51M parameters (image-text) and 219M parameters
(video-text), providing an efficient solution for Human-Robot Interaction (HRI)
applications in resource-constrained scenarios with strong multimodal decision
generalization capabilities.

</details>


### [248] [VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic Vision-Language Planning for Zero-Shot Transfer in Robot Navigation](https://arxiv.org/abs/2509.18592)
*Neel P. Bhatt,Yunhao Yang,Rohan Siva,Pranay Samineni,Daniel Milan,Zhangyang Wang,Ufuk Topcu*

Main category: cs.RO

TL;DR: 提出VLN - Zero框架用于视觉语言导航，结合多策略克服先前方法不足，在未见过环境表现优。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言导航方法依赖详尽探索或刚性策略，难以泛化，需一种能在未见环境快速适应的方法。

Method: 采用两阶段框架，探索阶段用结构化提示引导搜索构建场景图，部署阶段用神经符号规划器生成计划，缓存执行模块加速适应。

Result: 相比现有零样本模型成功率高2倍，超越多数微调基线，到达目标位置时间减半，VLM调用次数减少55%。

Conclusion: VLN - Zero框架结合快速探索、符号推理和缓存执行，能在未见环境实现鲁棒且可扩展的决策。

Abstract: Rapid adaptation in unseen environments is essential for scalable real-world
autonomy, yet existing approaches rely on exhaustive exploration or rigid
navigation policies that fail to generalize. We present VLN-Zero, a two-phase
vision-language navigation framework that leverages vision-language models to
efficiently construct symbolic scene graphs and enable zero-shot neurosymbolic
navigation. In the exploration phase, structured prompts guide VLM-based search
toward informative and diverse trajectories, yielding compact scene graph
representations. In the deployment phase, a neurosymbolic planner reasons over
the scene graph and environmental observations to generate executable plans,
while a cache-enabled execution module accelerates adaptation by reusing
previously computed task-location trajectories. By combining rapid exploration,
symbolic reasoning, and cache-enabled execution, the proposed framework
overcomes the computational inefficiency and poor generalization of prior
vision-language navigation methods, enabling robust and scalable
decision-making in unseen environments. VLN-Zero achieves 2x higher success
rate compared to state-of-the-art zero-shot models, outperforms most fine-tuned
baselines, and reaches goal locations in half the time with 55% fewer VLM calls
on average compared to state-of-the-art models across diverse environments.
Codebase, datasets, and videos for VLN-Zero are available at:
https://vln-zero.github.io/.

</details>


### [249] [End-to-End Crop Row Navigation via LiDAR-Based Deep Reinforcement Learning](https://arxiv.org/abs/2509.18608)
*Ana Luiza Mineiro,Francisco Affonso,Marcelo Becker*

Main category: cs.RO

TL;DR: 提出基于端到端学习的导航系统，用模拟训练的深度强化学习策略将3D激光雷达数据映射到控制命令，方法含体素下采样策略，在模拟中验证。


<details>
  <summary>Details</summary>
Motivation: 由于GNSS不可靠、作物行杂乱和光照多变，在林下农业环境中实现可靠导航仍有挑战。

Method: 采用端到端学习的导航系统，使用在模拟中训练的深度强化学习策略，将原始3D激光雷达数据直接映射到控制命令，含体素下采样策略。

Result: 在模拟中验证，在直行种植区成功率达100%，随着作物行曲率增加性能逐渐下降。

Conclusion: 所提出的导航系统在模拟环境中对不同作物行情况有一定导航能力。

Abstract: Reliable navigation in under-canopy agricultural environments remains a
challenge due to GNSS unreliability, cluttered rows, and variable lighting. To
address these limitations, we present an end-to-end learning-based navigation
system that maps raw 3D LiDAR data directly to control commands using a deep
reinforcement learning policy trained entirely in simulation. Our method
includes a voxel-based downsampling strategy that reduces LiDAR input size by
95.83%, enabling efficient policy learning without relying on labeled datasets
or manually designed control interfaces. The policy was validated in
simulation, achieving a 100% success rate in straight-row plantations and
showing a gradual decline in performance as row curvature increased, tested
across varying sinusoidal frequencies and amplitudes.

</details>


### [250] [The Case for Negative Data: From Crash Reports to Counterfactuals for Reasonable Driving](https://arxiv.org/abs/2509.18626)
*Jay Patrikar,Apoorva Sharma,Sushant Veer,Boyi Li,Sebastian Scherer,Marco Pavone*

Main category: cs.RO

TL;DR: 提出将事故报告与日志转换为统一表示用于检索的方法，在nuScenes基准上提升校准效果，反事实变体在风险决策上表现更好。


<details>
  <summary>Details</summary>
Motivation: 基于学习的自动驾驶系统多在无事故数据上训练，缺乏安全性能边界的指导，真实事故报告虽含关键对比证据但难以利用。

Method: 将事故叙述规范化为以自我为中心的语言，将日志和事故转换为统一的场景 - 动作表示用于检索，决策时检索相关先例，反事实扩展提出替代方案并推理。

Result: 在nuScenes基准上，先例检索大幅提高校准，上下文首选动作的召回率从24%提升到53%，反事实变体保留增益并在风险决策上更敏锐。

Conclusion: 所提出的方法能有效利用事故报告，提升自动驾驶系统在安全边界的决策能力。

Abstract: Learning-based autonomous driving systems are trained mostly on incident-free
data, offering little guidance near safety-performance boundaries. Real crash
reports contain precisely the contrastive evidence needed, but they are hard to
use: narratives are unstructured, third-person, and poorly grounded to sensor
views. We address these challenges by normalizing crash narratives to
ego-centric language and converting both logs and crashes into a unified
scene-action representation suitable for retrieval. At decision time, our
system adjudicates proposed actions by retrieving relevant precedents from this
unified index; an agentic counterfactual extension proposes plausible
alternatives, retrieves for each, and reasons across outcomes before deciding.
On a nuScenes benchmark, precedent retrieval substantially improves
calibration, with recall on contextually preferred actions rising from 24% to
53%. The counterfactual variant preserves these gains while sharpening
decisions near risk.

</details>


### [251] [Generalizable Domain Adaptation for Sim-and-Real Policy Co-Training](https://arxiv.org/abs/2509.18631)
*Shuo Cheng,Liqian Ma,Zhenyang Chen,Ajay Mandlekar,Caelan Garrett,Danfei Xu*

Main category: cs.RO

TL;DR: 提出统一的仿真与真实协同训练框架学习可泛化操作策略，利用仿真数据和少量真实演示，在操作任务上验证有提升。


<details>
  <summary>Details</summary>
Motivation: 行为克隆用于机器人操作时，获取大规模真实演示成本高，仿真数据虽可扩展但存在域差距，需解决策略从仿真到真实世界的迁移问题。

Method: 提出统一的协同训练框架，学习领域不变、任务相关特征空间，嵌入受最优传输启发的损失函数，扩展到非平衡最优传输框架处理数据不平衡。

Result: 在具有挑战性的操作任务上验证，可利用大量仿真数据使现实世界成功率提升达30%，甚至能泛化到仅在仿真中见过的场景。

Conclusion: 所提出的统一协同训练框架能有效利用仿真数据和少量真实演示，学习到可泛化的操作策略。

Abstract: Behavior cloning has shown promise for robot manipulation, but real-world
demonstrations are costly to acquire at scale. While simulated data offers a
scalable alternative, particularly with advances in automated demonstration
generation, transferring policies to the real world is hampered by various
simulation and real domain gaps. In this work, we propose a unified
sim-and-real co-training framework for learning generalizable manipulation
policies that primarily leverages simulation and only requires a few real-world
demonstrations. Central to our approach is learning a domain-invariant,
task-relevant feature space. Our key insight is that aligning the joint
distributions of observations and their corresponding actions across domains
provides a richer signal than aligning observations (marginals) alone. We
achieve this by embedding an Optimal Transport (OT)-inspired loss within the
co-training framework, and extend this to an Unbalanced OT framework to handle
the imbalance between abundant simulation data and limited real-world examples.
We validate our method on challenging manipulation tasks, showing it can
leverage abundant simulation data to achieve up to a 30% improvement in the
real-world success rate and even generalize to scenarios seen only in
simulation.

</details>


### [252] [Do You Need Proprioceptive States in Visuomotor Policies?](https://arxiv.org/abs/2509.18644)
*Juntu Zhao,Wenbo Lu,Di Zhang,Yufeng Liu,Yushen Liang,Tianluo Zhang,Yifeng Cao,Junyuan Xie,Yingdong Hu,Shengjie Wang,Junliang Guo,Dequan Wang,Yang Gao*

Main category: cs.RO

TL;DR: 研究指出基于模仿学习的视觉运动策略过度依赖本体感受状态输入会导致过拟合和泛化能力差，提出无状态策略，仅基于视觉观察预测动作，该策略在空间泛化、数据效率和跨实体适应方面表现更好。


<details>
  <summary>Details</summary>
Motivation: 解决基于模仿学习的视觉运动策略过度依赖本体感受状态输入，导致过拟合和空间泛化能力差的问题。

Method: 提出无状态策略，去除本体感受状态输入，仅基于视觉观察预测动作，在相对末端执行器动作空间构建策略，并使用双广角腕部相机提供视觉观察。

Result: 无状态策略在空间泛化上显著优于基于状态的策略，如在取放、衬衫折叠和全身操作等任务中，高度泛化平均成功率从0%提升到85%，水平泛化从6%提升到64%，在数据效率和跨实体适应方面也有优势。

Conclusion: 无状态策略在空间泛化、数据效率和跨实体适应方面表现出色，提高了在现实世界部署的实用性。

Abstract: Imitation-learning-based visuomotor policies have been widely used in robot
manipulation, where both visual observations and proprioceptive states are
typically adopted together for precise control. However, in this study, we find
that this common practice makes the policy overly reliant on the proprioceptive
state input, which causes overfitting to the training trajectories and results
in poor spatial generalization. On the contrary, we propose the State-free
Policy, removing the proprioceptive state input and predicting actions only
conditioned on visual observations. The State-free Policy is built in the
relative end-effector action space, and should ensure the full task-relevant
visual observations, here provided by dual wide-angle wrist cameras. Empirical
results demonstrate that the State-free policy achieves significantly stronger
spatial generalization than the state-based policy: in real-world tasks such as
pick-and-place, challenging shirt-folding, and complex whole-body manipulation,
spanning multiple robot embodiments, the average success rate improves from 0\%
to 85\% in height generalization and from 6\% to 64\% in horizontal
generalization. Furthermore, they also show advantages in data efficiency and
cross-embodiment adaptation, enhancing their practicality for real-world
deployment.

</details>


### [253] [SPiDR: A Simple Approach for Zero-Shot Safety in Sim-to-Real Transfer](https://arxiv.org/abs/2509.18648)
*Yarden As,Chengrui Qu,Benjamin Unger,Dongho Kang,Max van der Hart,Laixi Shi,Stelian Coros,Adam Wierman,Andreas Krause*

Main category: cs.RO

TL;DR: 提出SPiDR算法解决强化学习从模拟到现实迁移中的安全问题，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 强化学习在现实应用中的安全问题，以及模拟到现实的差距带来额外安全担忧，现有方法存在不足。

Method: 提出SPiDR算法，利用领域随机化将模拟到现实差距的不确定性纳入安全约束。

Result: 在模拟到模拟基准和两个真实机器人平台的实验中，SPiDR能有效确保安全并保持良好性能。

Conclusion: SPiDR是一种可扩展且有理论保证的安全模拟到现实迁移算法。

Abstract: Safety remains a major concern for deploying reinforcement learning (RL) in
real-world applications. Simulators provide safe, scalable training
environments, but the inevitable sim-to-real gap introduces additional safety
concerns, as policies must satisfy constraints in real-world conditions that
differ from simulation. To address this challenge, robust safe RL techniques
offer principled methods, but are often incompatible with standard scalable
training pipelines. In contrast, domain randomization, a simple and popular
sim-to-real technique, stands out as a promising alternative, although it often
results in unsafe behaviors in practice. We present SPiDR, short for
Sim-to-real via Pessimistic Domain Randomization -- a scalable algorithm with
provable guarantees for safe sim-to-real transfer. SPiDR uses domain
randomization to incorporate the uncertainty about the sim-to-real gap into the
safety constraints, making it versatile and highly compatible with existing
training pipelines. Through extensive experiments on sim-to-sim benchmarks and
two distinct real-world robotic platforms, we demonstrate that SPiDR
effectively ensures safety despite the sim-to-real gap while maintaining strong
performance.

</details>


### [254] [MV-UMI: A Scalable Multi-View Interface for Cross-Embodiment Learning](https://arxiv.org/abs/2509.18757)
*Omar Rayyan,John Abanes,Mahmoud Hafez,Anthony Tzes,Fares Abu-Dakka*

Main category: cs.RO

TL;DR: 提出MV - UMI框架，结合第三人视角与第一人称相机克服手持数据采集局限性，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习依赖高质量数据集，手持数据采集设备虽有优势但第一人称视角有局限，需改进。

Method: 提出MV - UMI框架，将第三人视角与第一人称相机集成。

Result: MV - UMI框架在3个任务中，对需要广泛场景理解的子任务性能提升约47%。

Conclusion: MV - UMI框架能在不损害跨实体优势的情况下，扩展手持抓握系统可学习的操作任务范围。

Abstract: Recent advances in imitation learning have shown great promise for developing
robust robot manipulation policies from demonstrations. However, this promise
is contingent on the availability of diverse, high-quality datasets, which are
not only challenging and costly to collect but are often constrained to a
specific robot embodiment. Portable handheld grippers have recently emerged as
intuitive and scalable alternatives to traditional robotic teleoperation
methods for data collection. However, their reliance solely on first-person
view wrist-mounted cameras often creates limitations in capturing sufficient
scene contexts. In this paper, we present MV-UMI (Multi-View Universal
Manipulation Interface), a framework that integrates a third-person perspective
with the egocentric camera to overcome this limitation. This integration
mitigates domain shifts between human demonstration and robot deployment,
preserving the cross-embodiment advantages of handheld data-collection devices.
Our experimental results, including an ablation study, demonstrate that our
MV-UMI framework improves performance in sub-tasks requiring broad scene
understanding by approximately 47% across 3 tasks, confirming the effectiveness
of our approach in expanding the range of feasible manipulation tasks that can
be learned using handheld gripper systems, without compromising the
cross-embodiment advantages inherent to such systems.

</details>


### [255] [VGGT-DP: Generalizable Robot Control via Vision Foundation Models](https://arxiv.org/abs/2509.18778)
*Shijia Ge,Yinxin Zhang,Shuzhao Xie,Weixiang Zhang,Mingcai Zhou,Zhi Wang*

Main category: cs.RO

TL;DR: 提出VGGT - DP视觉运动策略框架，提升机器人操作技能学习效果，实验表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有视觉模仿学习框架重策略设计，轻视觉编码器结构和能力，限制空间理解和泛化能力。

Method: 提出VGGT - DP框架，采用VGGT作为视觉编码器，引入本体感受引导的视觉学习策略，设计帧级令牌重用机制，应用随机令牌修剪。

Result: 在MetaWorld任务中，VGGT - DP显著优于DP和DP3等强基线，尤其在精度关键和长视野场景。

Conclusion: VGGT - DP框架有效提升机器人操作技能学习的空间理解和泛化能力。

Abstract: Visual imitation learning frameworks allow robots to learn manipulation
skills from expert demonstrations. While existing approaches mainly focus on
policy design, they often neglect the structure and capacity of visual
encoders, limiting spatial understanding and generalization. Inspired by
biological vision systems, which rely on both visual and proprioceptive cues
for robust control, we propose VGGT-DP, a visuomotor policy framework that
integrates geometric priors from a pretrained 3D perception model with
proprioceptive feedback. We adopt the Visual Geometry Grounded Transformer
(VGGT) as the visual encoder and introduce a proprioception-guided visual
learning strategy to align perception with internal robot states, improving
spatial grounding and closed-loop control. To reduce inference latency, we
design a frame-wise token reuse mechanism that compacts multi-view tokens into
an efficient spatial representation. We further apply random token pruning to
enhance policy robustness and reduce overfitting. Experiments on challenging
MetaWorld tasks show that VGGT-DP significantly outperforms strong baselines
such as DP and DP3, particularly in precision-critical and long-horizon
scenarios.

</details>


### [256] [Robotic Skill Diversification via Active Mutation of Reward Functions in Reinforcement Learning During a Liquid Pouring Task](https://arxiv.org/abs/2509.18463)
*Jannick van Buuren,Roberto Giglio,Loris Roveda,Luka Peternel*

Main category: cs.RO

TL;DR: 本文通过液体倾倒用例，研究强化学习中奖励函数的故意突变如何在机器人操作任务中产生多样化技能变化，开发新框架并得出多样策略，为机器人多样化学习提供方向。


<details>
  <summary>Details</summary>
Motivation: 探索强化学习中奖励函数的故意突变能否在机器人操作任务中产生多样化的技能变化。

Method: 开发基于对奖励函数不同项权重施加高斯噪声的奖励函数突变框架，设计包含准确性、时间和努力程度的奖励函数，在NVIDIA Isaac Sim模拟环境中，使用近端策略优化算法进行研究。

Result: 得到的策略表现出广泛的行为，从原本倾倒任务的执行变化到对意外任务有用的新技能。

Conclusion: 该方法为机器人系统执行特定任务的多样化学习提供了有前景的方向，也可能为未来任务衍生出有意义的技能。

Abstract: This paper explores how deliberate mutations of reward function in
reinforcement learning can produce diversified skill variations in robotic
manipulation tasks, examined with a liquid pouring use case. To this end, we
developed a new reward function mutation framework that is based on applying
Gaussian noise to the weights of the different terms in the reward function.
Inspired by the cost-benefit tradeoff model from human motor control, we
designed the reward function with the following key terms: accuracy, time, and
effort. The study was performed in a simulation environment created in NVIDIA
Isaac Sim, and the setup included Franka Emika Panda robotic arm holding a
glass with a liquid that needed to be poured into a container. The
reinforcement learning algorithm was based on Proximal Policy Optimization. We
systematically explored how different configurations of mutated weights in the
rewards function would affect the learned policy. The resulting policies
exhibit a wide range of behaviours: from variations in execution of the
originally intended pouring task to novel skills useful for unexpected tasks,
such as container rim cleaning, liquid mixing, and watering. This approach
offers promising directions for robotic systems to perform diversified learning
of specific tasks, while also potentially deriving meaningful skills for future
tasks.

</details>


### [257] [Eva-VLA: Evaluating Vision-Language-Action Models' Robustness Under Real-World Physical Variations](https://arxiv.org/abs/2509.18953)
*Hanqing Liu,Jiahuan Long,Junqi Wu,Jiacheng Hou,Huili Tang,Tingsong Jiang,Weien Zhou,Wen Yao*

Main category: cs.RO

TL;DR: 提出Eva - VLA框架评估VLA模型对现实物理变化的鲁棒性，实验揭示模型存在严重漏洞，该框架为强化模型提供途径。


<details>
  <summary>Details</summary>
Motivation: VLA模型在机器人操作中有前景，但对现实物理变化的鲁棒性研究不足，需评估其鲁棒性。

Method: 将离散物理变化转化为连续优化问题，把现实变化分解为三个关键领域，引入连续黑盒优化框架探索最坏情况。

Result: 对多个基准上的先进OpenVLA模型实验，所有变化类型导致的失败率超60%，物体变换在长视野任务中失败率达97.8%。

Conclusion: 研究揭示了实验室成功与实际部署准备之间的差距，Eva - VLA框架为强化基于VLA的机器人操作模型应对现实挑战提供了实用途径。

Abstract: Vision-Language-Action (VLA) models have emerged as promising solutions for
robotic manipulation, yet their robustness to real-world physical variations
remains critically underexplored. To bridge this gap, we propose Eva-VLA, the
first unified framework that systematically evaluates the robustness of VLA
models by transforming discrete physical variations into continuous
optimization problems. However, comprehensively assessing VLA robustness
presents two key challenges: (1) how to systematically characterize diverse
physical variations encountered in real-world deployments while maintaining
evaluation reproducibility, and (2) how to discover worst-case scenarios
without prohibitive real-world data collection costs efficiently. To address
the first challenge, we decompose real-world variations into three critical
domains: object 3D transformations that affect spatial reasoning, illumination
variations that challenge visual perception, and adversarial patches that
disrupt scene understanding. For the second challenge, we introduce a
continuous black-box optimization framework that transforms discrete physical
variations into parameter optimization, enabling systematic exploration of
worst-case scenarios. Extensive experiments on state-of-the-art OpenVLA models
across multiple benchmarks reveal alarming vulnerabilities: all variation types
trigger failure rates exceeding 60%, with object transformations causing up to
97.8% failure in long-horizon tasks. Our findings expose critical gaps between
controlled laboratory success and unpredictable deployment readiness, while the
Eva-VLA framework provides a practical pathway for hardening VLA-based robotic
manipulation models against real-world deployment challenges.

</details>


### [258] [Pure Vision Language Action (VLA) Models: A Comprehensive Survey](https://arxiv.org/abs/2509.19012)
*Dapeng Zhang,Jin Sun,Chenghui Hu,Xiaoyan Wu,Zhenlong Yuan,Rui Zhou,Fei Shen,Qingguo Zhou*

Main category: cs.RO

TL;DR: 本文对视觉语言动作（VLA）模型方法进行全面综述，分类介绍方法，引入相关数据集等，并提出研究挑战与方向。


<details>
  <summary>Details</summary>
Motivation: VLA模型使传统控制向通用机器人转变，需对现有VLA研究进行清晰分类和系统全面回顾。

Method: 对VLA应用进行综合分析，将VLA方法分为基于自回归、扩散、强化学习、混合和专用方法等范式，并详细研究其动机、核心策略和实现。引入基础数据集、基准和仿真平台。

Result: 梳理了VLA领域研究，分析了不同范式方法。

Conclusion: 指出了VLA模型和通用机器人研究的关键挑战和未来方向，强调了该领域发展的机遇和挑战。

Abstract: The emergence of Vision Language Action (VLA) models marks a paradigm shift
from traditional policy-based control to generalized robotics, reframing Vision
Language Models (VLMs) from passive sequence generators into active agents for
manipulation and decision-making in complex, dynamic environments. This survey
delves into advanced VLA methods, aiming to provide a clear taxonomy and a
systematic, comprehensive review of existing research. It presents a
comprehensive analysis of VLA applications across different scenarios and
classifies VLA approaches into several paradigms: autoregression-based,
diffusion-based, reinforcement-based, hybrid, and specialized methods; while
examining their motivations, core strategies, and implementations in detail. In
addition, foundational datasets, benchmarks, and simulation platforms are
introduced. Building on the current VLA landscape, the review further proposes
perspectives on key challenges and future directions to advance research in VLA
models and generalizable robotics. By synthesizing insights from over three
hundred recent studies, this survey maps the contours of this rapidly evolving
field and highlights the opportunities and challenges that will shape the
development of scalable, general-purpose VLA methods.

</details>


### [259] [Reduced-Order Model-Guided Reinforcement Learning for Demonstration-Free Humanoid Locomotion](https://arxiv.org/abs/2509.19023)
*Shuai Liu,Meng Cheng Lau*

Main category: cs.RO

TL;DR: 提出ROM - GRL框架用于人形机器人行走，分两阶段训练，实验表明其步态稳定、跟踪误差低，能实现无人类示范的自然行为。


<details>
  <summary>Details</summary>
Motivation: 开发无需运动捕捉数据和复杂奖励设计的人形机器人行走强化学习框架。

Method: 分两阶段，第一阶段用近端策略优化训练4 - DOF降阶模型生成节能步态模板；第二阶段用带对抗判别器的软演员 - 评论家算法，让全身策略匹配ROM的演示。

Result: 在1米/秒和4米/秒速度实验中，ROM - GRL产生稳定、对称步态，跟踪误差比纯奖励基线低。

Conclusion: ROM - GRL弥合了仅基于奖励和基于模仿的运动方法间的差距，无需人类示范就能实现多功能、自然的人形行为。

Abstract: We introduce Reduced-Order Model-Guided Reinforcement Learning (ROM-GRL), a
two-stage reinforcement learning framework for humanoid walking that requires
no motion capture data or elaborate reward shaping. In the first stage, a
compact 4-DOF (four-degree-of-freedom) reduced-order model (ROM) is trained via
Proximal Policy Optimization. This generates energy-efficient gait templates.
In the second stage, those dynamically consistent trajectories guide a
full-body policy trained with Soft Actor--Critic augmented by an adversarial
discriminator, ensuring the student's five-dimensional gait feature
distribution matches the ROM's demonstrations. Experiments at 1
meter-per-second and 4 meter-per-second show that ROM-GRL produces stable,
symmetric gaits with substantially lower tracking error than a pure-reward
baseline. By distilling lightweight ROM guidance into high-dimensional
policies, ROM-GRL bridges the gap between reward-only and imitation-based
locomotion methods, enabling versatile, naturalistic humanoid behaviors without
any human demonstrations.

</details>


### [260] [Query-Centric Diffusion Policy for Generalizable Robotic Assembly](https://arxiv.org/abs/2509.18686)
*Ziyi Xu,Haohong Lin,Shiqi Liu,Ding Zhao*

Main category: cs.RO

TL;DR: 提出查询中心扩散策略（QDP）框架解决机器人装配任务分层策略实施难题，实验显示其提升技能精度和长时成功率。


<details>
  <summary>Details</summary>
Motivation: 机器人装配任务复杂，分层策略中高层技能查询与低层执行不匹配，实施困难。

Method: 提出QDP框架，利用包含对象、接触点和技能信息的查询连接高层规划和低层控制，引入查询中心机制识别相关组件引导低层策略，借助点云观测提升鲁棒性。

Result: 在FurnitureBench模拟和现实场景实验中，QDP在技能精度和长时成功率上表现更好，在插入和拧螺丝任务中技能成功率比无结构化查询基线提高超50%。

Conclusion: QDP能有效解决分层策略实施问题，提升机器人装配任务性能。

Abstract: The robotic assembly task poses a key challenge in building generalist robots
due to the intrinsic complexity of part interactions and the sensitivity to
noise perturbations in contact-rich settings. The assembly agent is typically
designed in a hierarchical manner: high-level multi-part reasoning and
low-level precise control. However, implementing such a hierarchical policy is
challenging in practice due to the mismatch between high-level skill queries
and low-level execution. To address this, we propose the Query-centric
Diffusion Policy (QDP), a hierarchical framework that bridges high-level
planning and low-level control by utilizing queries comprising objects, contact
points, and skill information. QDP introduces a query-centric mechanism that
identifies task-relevant components and uses them to guide low-level policies,
leveraging point cloud observations to improve the policy's robustness. We
conduct comprehensive experiments on the FurnitureBench in both simulation and
real-world settings, demonstrating improved performance in skill precision and
long-horizon success rate. In the challenging insertion and screwing tasks, QDP
improves the skill-wise success rate by over 50% compared to baselines without
structured queries.

</details>


### [261] [World4RL: Diffusion World Models for Policy Refinement with Reinforcement Learning for Robotic Manipulation](https://arxiv.org/abs/2509.19080)
*Zhennan Jiang,Kai Liu,Yuxin Qin,Shuai Tian,Yupeng Zheng,Mingcai Zhou,Chao Yu,Haoran Li,Dongbin Zhao*

Main category: cs.RO

TL;DR: 本文提出World4RL框架，用基于扩散的世界模型作为高保真模拟器在虚拟环境中优化机器人操作预训练策略，实验表明其效果好。


<details>
  <summary>Details</summary>
Motivation: 现有机器人操作策略受专家数据限制，强化学习在真机训练成本高、不安全，模拟器训练存在仿真到现实的差距，需结合扩散模型增强预训练策略。

Method: 提出World4RL框架，基于两个原则：在多任务数据集上预训练扩散世界模型，在冻结的世界模型内优化策略；设计双热动作编码方案，采用扩散骨干网络。

Result: 大量仿真和真实世界实验表明，World4RL能提供高保真环境建模，持续优化策略，成功率显著高于模仿学习和其他基线。

Conclusion: World4RL框架可有效利用扩散模型优化机器人操作预训练策略，提升操作成功率。

Abstract: Robotic manipulation policies are commonly initialized through imitation
learning, but their performance is limited by the scarcity and narrow coverage
of expert data. Reinforcement learning can refine polices to alleviate this
limitation, yet real-robot training is costly and unsafe, while training in
simulators suffers from the sim-to-real gap. Recent advances in generative
models have demonstrated remarkable capabilities in real-world simulation, with
diffusion models in particular excelling at generation. This raises the
question of how diffusion model-based world models can be combined to enhance
pre-trained policies in robotic manipulation. In this work, we propose
World4RL, a framework that employs diffusion-based world models as
high-fidelity simulators to refine pre-trained policies entirely in imagined
environments for robotic manipulation. Unlike prior works that primarily employ
world models for planning, our framework enables direct end-to-end policy
optimization. World4RL is designed around two principles: pre-training a
diffusion world model that captures diverse dynamics on multi-task datasets and
refining policies entirely within a frozen world model to avoid online
real-world interactions. We further design a two-hot action encoding scheme
tailored for robotic manipulation and adopt diffusion backbones to improve
modeling fidelity. Extensive simulation and real-world experiments demonstrate
that World4RL provides high-fidelity environment modeling and enables
consistent policy refinement, yielding significantly higher success rates
compared to imitation learning and other baselines. More visualization results
are available at https://world4rl.github.io/.

</details>


### [262] [FUNCanon: Learning Pose-Aware Action Primitives via Functional Object Canonicalization for Generalizable Robotic Manipulation](https://arxiv.org/abs/2509.19102)
*Hongli Xu,Lei Zhang,Xiaoyue Hu,Boyang Zhong,Kaixin Bai,Zoltán-Csaba Márton,Zhenshan Bing,Zhaopeng Chen,Alois Christian Knoll,Jianwei Zhang*

Main category: cs.RO

TL;DR: 介绍FunCanon框架，将长时操作任务转换为动作块序列，通过功能对象规范化等方法使策略具有泛化能力，实验证明其在复杂操作领域的有效性。


<details>
  <summary>Details</summary>
Motivation: 端到端演示得到的通用机器人技能常产生特定任务策略，缺乏泛化性。

Method: 引入FunCanon框架，将任务转为动作块序列；进行功能对象规范化；训练FuncDiffuser扩散策略。

Result: 实验在模拟和真实世界基准测试中展示了类别级泛化、跨任务行为重用和鲁棒的仿真到现实部署。

Conclusion: 功能规范化为复杂操作领域的可扩展模仿学习提供了强大的归纳偏置。

Abstract: General-purpose robotic skills from end-to-end demonstrations often leads to
task-specific policies that fail to generalize beyond the training
distribution. Therefore, we introduce FunCanon, a framework that converts
long-horizon manipulation tasks into sequences of action chunks, each defined
by an actor, verb, and object. These chunks focus policy learning on the
actions themselves, rather than isolated tasks, enabling compositionality and
reuse. To make policies pose-aware and category-general, we perform functional
object canonicalization for functional alignment and automatic manipulation
trajectory transfer, mapping objects into shared functional frames using
affordance cues from large vision language models. An object centric and action
centric diffusion policy FuncDiffuser trained on this aligned data naturally
respects object affordances and poses, simplifying learning and improving
generalization ability. Experiments on simulated and real-world benchmarks
demonstrate category-level generalization, cross-task behavior reuse, and
robust sim2real deployment, showing that functional canonicalization provides a
strong inductive bias for scalable imitation learning in complex manipulation
domains. Details of the demo and supplemental material are available on our
project website https://sites.google.com/view/funcanon.

</details>


### [263] [DexSkin: High-Coverage Conformable Robotic Skin for Learning Contact-Rich Manipulation](https://arxiv.org/abs/2509.18830)
*Suzannah Wistreich,Baiyu Shi,Stephen Tian,Samuel Clarke,Michael Nath,Chengyi Xu,Zhenan Bao,Jiajun Wu*

Main category: cs.RO

TL;DR: 本文介绍DexSkin电子皮肤，在机器人操作学习任务中展现适用性和实用性。


<details>
  <summary>Details</summary>
Motivation: 复制人类皮肤触觉感知能力，用于灵巧机器人操作是长期挑战，开发DexSkin应对此挑战。

Method: 开发DexSkin软质可贴合电容式电子皮肤，对平行夹爪手指进行传感，在示教学习框架中评估其能力，对其进行校准以实现跨传感器实例的模型转移。

Result: DexSkin能用于学习具有挑战性的操作任务，可校准实现模型转移，适用于真实机器人的在线强化学习。

Conclusion: DexSkin适用于现实世界中大量接触的操作学习。

Abstract: Human skin provides a rich tactile sensing stream, localizing intentional and
unintentional contact events over a large and contoured region. Replicating
these tactile sensing capabilities for dexterous robotic manipulation systems
remains a longstanding challenge. In this work, we take a step towards this
goal by introducing DexSkin. DexSkin is a soft, conformable capacitive
electronic skin that enables sensitive, localized, and calibratable tactile
sensing, and can be tailored to varying geometries. We demonstrate its efficacy
for learning downstream robotic manipulation by sensorizing a pair of parallel
jaw gripper fingers, providing tactile coverage across almost the entire finger
surfaces. We empirically evaluate DexSkin's capabilities in learning
challenging manipulation tasks that require sensing coverage across the entire
surface of the fingers, such as reorienting objects in hand and wrapping
elastic bands around boxes, in a learning-from-demonstration framework. We then
show that, critically for data-driven approaches, DexSkin can be calibrated to
enable model transfer across sensor instances, and demonstrate its
applicability to online reinforcement learning on real robots. Our results
highlight DexSkin's suitability and practicality for learning real-world,
contact-rich manipulation. Please see our project webpage for videos and
visualizations: https://dex-skin.github.io/.

</details>


### [264] [Bi-VLA: Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation](https://arxiv.org/abs/2509.18865)
*Masato Kobayashi,Thanpimon Buamanee*

Main category: cs.RO

TL;DR: 提出Bi - VLA框架，扩展双边控制模仿学习，克服单任务限制，结合视觉和语言提升通用性，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统双边控制方法需特定任务模型，通用性受限，因此要开发能处理多任务的框架。

Method: 利用SigLIP和FiLM融合机器人关节角度、速度、扭矩数据、视觉特征和自然语言指令。

Result: 在两种任务类型上验证，相比传统方法，能成功解读视觉 - 语言组合，提高任务成功率。

Conclusion: Bi - VLA解决了先前双边方法的单任务限制，结合视觉和语言能显著提升通用性，在实际任务中有效。

Abstract: We propose Bilateral Control-Based Imitation Learning via Vision-Language
Fusion for Action Generation (Bi-VLA), a novel framework that extends bilateral
control-based imitation learning to handle more than one task within a single
model. Conventional bilateral control methods exploit joint angle, velocity,
torque, and vision for precise manipulation but require task-specific models,
limiting their generality. Bi-VLA overcomes this limitation by utilizing robot
joint angle, velocity, and torque data from leader-follower bilateral control
with visual features and natural language instructions through SigLIP and
FiLM-based fusion. We validated Bi-VLA on two task types: one requiring
supplementary language cues and another distinguishable solely by vision.
Real-robot experiments showed that Bi-VLA successfully interprets
vision-language combinations and improves task success rates compared to
conventional bilateral control-based imitation learning. Our Bi-VLA addresses
the single-task limitation of prior bilateral approaches and provides empirical
evidence that combining vision and language significantly enhances versatility.
Experimental results validate the effectiveness of Bi-VLA in real-world tasks.
For additional material, please visit the website:
https://mertcookimg.github.io/bi-vla/

</details>


### [265] [SOE: Sample-Efficient Robot Policy Self-Improvement via On-Manifold Exploration](https://arxiv.org/abs/2509.19292)
*Yang Jin,Jun Lv,Han Xue,Wendi Chen,Chuan Wen,Cewu Lu*

Main category: cs.RO

TL;DR: 提出SOE框架提升机器人操作中策略探索与改进，实验显示优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 现有机器人策略因动作模式崩溃缺乏探索能力，鼓励探索的方法不安全且效果有限。

Method: 提出Self-Improvement via On-Manifold Exploration (SOE)框架，学习任务相关因素的紧凑潜在表示，将探索限制在有效动作流形上，可作为插件集成到任意策略模型。

Result: 在模拟和现实任务实验中，SOE始终优于先前方法，实现更高任务成功率、更平滑安全的探索和更优样本效率。

Conclusion: 流形上探索是样本高效策略自我改进的原则性方法。

Abstract: Intelligent agents progress by continually refining their capabilities
through actively exploring environments. Yet robot policies often lack
sufficient exploration capability due to action mode collapse. Existing methods
that encourage exploration typically rely on random perturbations, which are
unsafe and induce unstable, erratic behaviors, thereby limiting their
effectiveness. We propose Self-Improvement via On-Manifold Exploration (SOE), a
framework that enhances policy exploration and improvement in robotic
manipulation. SOE learns a compact latent representation of task-relevant
factors and constrains exploration to the manifold of valid actions, ensuring
safety, diversity, and effectiveness. It can be seamlessly integrated with
arbitrary policy models as a plug-in module, augmenting exploration without
degrading the base policy performance. Moreover, the structured latent space
enables human-guided exploration, further improving efficiency and
controllability. Extensive experiments in both simulation and real-world tasks
demonstrate that SOE consistently outperforms prior methods, achieving higher
task success rates, smoother and safer exploration, and superior sample
efficiency. These results establish on-manifold exploration as a principled
approach to sample-efficient policy self-improvement. Project website:
https://ericjin2002.github.io/SOE

</details>


### [266] [Residual Off-Policy RL for Finetuning Behavior Cloning Policies](https://arxiv.org/abs/2509.19301)
*Lars Ankile,Zhenyu Jiang,Rocky Duan,Guanya Shi,Pieter Abbeel,Anusha Nagabandi*

Main category: cs.RO

TL;DR: 本文提出结合行为克隆（BC）和强化学习（RL）的方法，通过残差学习框架提升高自由度系统操作策略，在仿真和现实中均有效，实现人形机器人现实强化学习训练并达先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有BC方法受人类示范质量等限制，RL在现实机器人训练中存在样本效率低等挑战，需结合两者优势。

Method: 采用残差学习框架，以BC策略为黑盒基础，通过样本高效的离线策略RL学习每步轻量级残差修正。

Result: 仅需稀疏二进制奖励信号，在仿真和现实中有效改善高自由度系统操作策略，实现人形机器人现实RL训练，在视觉任务中达先进水平。

Conclusion: 该方法为现实世界部署RL提供了实用途径。

Abstract: Recent advances in behavior cloning (BC) have enabled impressive visuomotor
control policies. However, these approaches are limited by the quality of human
demonstrations, the manual effort required for data collection, and the
diminishing returns from increasing offline data. In comparison, reinforcement
learning (RL) trains an agent through autonomous interaction with the
environment and has shown remarkable success in various domains. Still,
training RL policies directly on real-world robots remains challenging due to
sample inefficiency, safety concerns, and the difficulty of learning from
sparse rewards for long-horizon tasks, especially for high-degree-of-freedom
(DoF) systems. We present a recipe that combines the benefits of BC and RL
through a residual learning framework. Our approach leverages BC policies as
black-box bases and learns lightweight per-step residual corrections via
sample-efficient off-policy RL. We demonstrate that our method requires only
sparse binary reward signals and can effectively improve manipulation policies
on high-degree-of-freedom (DoF) systems in both simulation and the real world.
In particular, we demonstrate, to the best of our knowledge, the first
successful real-world RL training on a humanoid robot with dexterous hands. Our
results demonstrate state-of-the-art performance in various vision-based tasks,
pointing towards a practical pathway for deploying RL in the real world.
Project website: https://residual-offpolicy-rl.github.io

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [267] [Tensor Train Completion from Fiberwise Observations Along a Single Mode](https://arxiv.org/abs/2509.18149)
*Shakir Showkat Sofi,Lieven De Lathauwer*

Main category: math.NA

TL;DR: 本文聚焦张量补全，提出对特定‘纤维式’观测张量用标准线性代数运算进行张量列车分解的补全方法，快速且在合理条件下有效。


<details>
  <summary>Details</summary>
Motivation: 当前张量补全技术多在随机均匀观测等条件下研究概率恢复保证，若观测模式有低秩结构可利用，需设计有确定性恢复保证的高效算法。

Method: 使用标准线性代数运算对特定‘纤维式’观测张量进行张量列车分解。

Result: 提出的补全方法快速，在对观测模式的合理确定性条件下能有效工作，通过数值实验展示了应用和用例。

Conclusion: 所提方法在特定张量补全场景有效，有实际应用价值。

Abstract: Tensor completion is an extension of matrix completion aimed at recovering a
multiway data tensor by leveraging a given subset of its entries (observations)
and the pattern of observation. The low-rank assumption is key in establishing
a relationship between the observed and unobserved entries of the tensor. The
low-rank tensor completion problem is typically solved using numerical
optimization techniques, where the rank information is used either implicitly
(in the rank minimization approach) or explicitly (in the error minimization
approach). Current theories concerning these techniques often study
probabilistic recovery guarantees under conditions such as random uniform
observations and incoherence requirements. However, if an observation pattern
exhibits some low-rank structure that can be exploited, more efficient
algorithms with deterministic recovery guarantees can be designed by leveraging
this structure. This work shows how to use only standard linear algebra
operations to compute the tensor train decomposition of a specific type of
``fiber-wise" observed tensor, where some of the fibers of a tensor (along a
single specific mode) are either fully observed or entirely missing, unlike the
usual entry-wise observations. From an application viewpoint, this setting is
relevant when it is easier to sample or collect a multiway data tensor along a
specific mode (e.g., temporal). The proposed completion method is fast and is
guaranteed to work under reasonable deterministic conditions on the observation
pattern. Through numerical experiments, we showcase interesting applications
and use cases that illustrate the effectiveness of the proposed approach.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [268] [Connecting Quantum Computing with Classical Stochastic Simulation](https://arxiv.org/abs/2509.18614)
*Jose Blanchet,Mark S. Squillante,Mario Szegedy,Guanyang Wang*

Main category: quant-ph

TL;DR: 介绍量子蒙特卡罗计算方法及在金融计算中的应用，给出Python/Qiskit实现示例并讨论挑战。


<details>
  <summary>Details</summary>
Motivation: 将量子方法引入蒙特卡罗计算并应用于金融领域。

Method: 运用Grover算法构建直觉，用Grover型迭代处理振幅估计、计数和蒙特卡罗积分问题，并用Python/Qiskit实现。

Result: 通过Python/Qiskit实现展示了量子方法在金融中的应用。

Conclusion: 讨论了当前量子模拟技术扩展面临的挑战。

Abstract: This tutorial paper introduces quantum approaches to Monte Carlo computation
with applications in computational finance. We outline the basics of quantum
computing using Grover's algorithm for unstructured search to build intuition.
We then move slowly to amplitude estimation problems and applications to
counting and Monte Carlo integration, again using Grover-type iterations. A
hands-on Python/Qiskit implementation illustrates these concepts applied to
finance. The paper concludes with a discussion on current challenges in scaling
quantum simulation techniques.

</details>


### [269] [Re-uploading quantum data: A universal function approximator for quantum inputs](https://arxiv.org/abs/2509.18530)
*Hyunho Cha,Daniel K. Park,Jungwoo Lee*

Main category: quant-ph

TL;DR: 提出并分析量子数据重上传架构，用单辅助量子比特和单量子比特测量近似有界连续函数，为处理量子数据的量子机器学习模型设计提供高效表达方法。


<details>
  <summary>Details</summary>
Motivation: 量子数据重上传对经典输入有效，但拓展到量子输入研究不足，因量子态信息无法直接以经典形式获取。

Method: 提出一种量子数据重上传架构，让一个量子比特与任意输入态的新副本依次相互作用，通过交替纠缠幺正和重置输入寄存器实现完全正且保迹映射的离散级联。

Result: 该电路仅用一个辅助量子比特和单量子比特测量就能近似任何有界连续函数。

Conclusion: 此框架为直接处理量子数据的量子机器学习模型设计提供了一种节省量子比特且表达能力强的方法。

Abstract: Quantum data re-uploading has proved powerful for classical inputs, where
repeatedly encoding features into a small circuit yields universal function
approximation. Extending this idea to quantum inputs remains underexplored, as
the information contained in a quantum state is not directly accessible in
classical form. We propose and analyze a quantum data re-uploading architecture
in which a qubit interacts sequentially with fresh copies of an arbitrary input
state. The circuit can approximate any bounded continuous function using only
one ancilla qubit and single-qubit measurements. By alternating entangling
unitaries with mid-circuit resets of the input register, the architecture
realizes a discrete cascade of completely positive and trace-preserving maps,
analogous to collision models in open quantum system dynamics. Our framework
provides a qubit-efficient and expressive approach to designing quantum machine
learning models that operate directly on quantum data.

</details>


### [270] [Scalable bayesian shadow tomography for quantum property estimation with set transformers](https://arxiv.org/abs/2509.18674)
*Hyunho Cha,Wonjung Kim,Jungwoo Lee*

Main category: quant-ph

TL;DR: 引入可扩展贝叶斯机器学习框架，绕过全密度矩阵重建来估计量子态标量特性，结合经典阴影协议与集变换器架构，在多任务上表现优于经典阴影。


<details>
  <summary>Details</summary>
Motivation: 开发一种可扩展方法来从测量数据估计未知量子态的标量特性，避免全密度矩阵重建。

Method: 将经典阴影协议与置换不变集变换器架构集成，把测量结果编码为固定维特征向量，网络输出对基线估计器的残差校正。

Result: 在Greenberger - Horne - Zeilinger态保真度和二阶Rényi熵估计任务中，贝叶斯估计器均比经典阴影的均方误差更低，在少副本情况下降低超99%。

Conclusion: 所提出的贝叶斯机器学习框架在估计量子态标量特性上具有优势，可扩展到大型量子系统。

Abstract: A scalable Bayesian machine learning framework is introduced for estimating
scalar properties of an unknown quantum state from measurement data, which
bypasses full density matrix reconstruction. This work is the first to
integrate the classical shadows protocol with a permutation-invariant set
transformer architecture, enabling the approach to predict and correct bias in
existing estimators to approximate the true Bayesian posterior mean.
Measurement outcomes are encoded as fixed-dimensional feature vectors, and the
network outputs a residual correction to a baseline estimator. Scalability to
large quantum systems is ensured by the polynomial dependence of input size on
system size and number of measurements. On Greenberger-Horne-Zeilinger state
fidelity and second-order R\'enyi entropy estimation tasks -- using random
Pauli and random Clifford measurements -- this Bayesian estimator always
achieves lower mean squared error than classical shadows alone, with more than
a 99\% reduction in the few copy regime.

</details>


### [271] [Quantum Annealing for Minimum Bisection Problem: A Machine Learning-based Approach for Penalty Parameter Tuning](https://arxiv.org/abs/2509.19005)
*Renáta Rusnáková,Martin Chovanec,Juraj Gazda*

Main category: quant-ph

TL;DR: 本文探讨用D-Wave量子退火求解器解决最小二分问题，提出基于机器学习自适应调整惩罚参数的方法，实验表明该策略提升求解器性能且优于经典方法。


<details>
  <summary>Details</summary>
Motivation: 最小二分问题是NP难问题，D-Wave量子退火求解器在解决此问题时，选择合适惩罚参数是关键挑战，需找到有效方法。

Method: 将最小二分问题建模为二次无约束二进制优化模型，引入基于机器学习的方法，用梯度提升回归模型根据图的结构属性、节点数和密度预测惩罚参数值，动态调整参数。

Result: 在最多4000个节点的随机生成图数据集上测试，自适应调整策略显著提升量子退火混合求解器性能，持续优于经典分区算法。

Conclusion: 所提自适应调整策略有潜力成为图分区问题的替代方法。

Abstract: The Minimum Bisection Problem is a well-known NP-hard problem in
combinatorial optimization, with practical applications in areas such as
parallel computing, network design, and machine learning. In this paper, we
examine the potential of using D-Wave Systems' quantum annealing solvers to
solve the Minimum Bisection Problem, which we formulate as a Quadratic
Unconstrained Binary Optimization model. A key challenge in this formulation
lies in choosing an appropriate penalty parameter, as it plays a crucial role
in ensuring both the quality of the solution and the satisfaction of the
problem's constraints. To address this, we introduce a novel machine
learning-based approach for adaptive tuning of the penalty parameter.
Specifically, we use a Gradient Boosting Regressor model trained to predict
suitable penalty parameter values based on structural properties of the input
graph, the number of nodes and the graph's density. This method enables the
penalty parameter to be adjusted dynamically for each specific problem
instance, improving the solver's ability to balance the competing goals of
minimizing the cut size and maintaining equally sized partitions. We test our
approach on a large dataset of randomly generated Erd\H{o}s-R\'enyi graphs with
up to 4,000 nodes, and we compare the results with classical partitioning
algorithms, Metis and Kernighan-Lin. Experimental findings demonstrate that our
adaptive tuning strategy significantly improves the performance of the quantum
annealing hybrid solver and consistently outperforms the classical methods
used, indicating its potential as an alternative for the graph partitioning
problem.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [272] [Hierarchical Semi-Markov Models with Duration-Aware Dynamics for Activity Sequences](https://arxiv.org/abs/2509.18414)
*Rohit Dube,Natarajan Gautam,Amarnath Banerjee,Harsha Nagarajan*

Main category: stat.AP

TL;DR: 本文用全国代表性时间使用日记开发人类活动序列生成模型，提出分层半马尔可夫框架，证明显式建模活动持续时间能提升预测效果，分析出关键人口因素。


<details>
  <summary>Details</summary>
Motivation: 准确预测细粒度居民用电需求需要能生成现实日常活动序列的生成模型，以用于微电网管理和需求响应等应用。

Method: 提出分层半马尔可夫框架，包括时间非齐次马尔可夫路由器学习活动顺序、半马尔可夫风险组件建模活动持续时间，跨相关人口群体和时间块汇集信息，用调查设计权重训练和评估模型。

Result: 显式建模持续时间比纯马尔可夫模型有显著提升；人口因素有明显层次，性别、日类型和家庭规模预测增益大，地区和季节对活动序列预测贡献小。

Conclusion: 得到可解释且稳健的合成活动轨迹生成器，为下游能源系统建模提供高保真基础。

Abstract: Residential electricity demand at granular scales is driven by what people do
and for how long. Accurately forecasting this demand for applications like
microgrid management and demand response therefore requires generative models
that can produce realistic daily activity sequences, capturing both the timing
and duration of human behavior. This paper develops a generative model of human
activity sequences using nationally representative time-use diaries at a
10-minute resolution. We use this model to quantify which demographic factors
are most critical for improving predictive performance.
  We propose a hierarchical semi-Markov framework that addresses two key
modeling challenges. First, a time-inhomogeneous Markov \emph{router} learns
the patterns of ``which activity comes next." Second, a semi-Markov
\emph{hazard} component explicitly models activity durations, capturing ``how
long" activities realistically last. To ensure statistical stability when data
are sparse, the model pools information across related demographic groups and
time blocks. The entire framework is trained and evaluated using survey design
weights to ensure our findings are representative of the U.S. population.
  On a held-out test set, we demonstrate that explicitly modeling durations
with the hazard component provides a substantial and statistically significant
improvement over purely Markovian models. Furthermore, our analysis reveals a
clear hierarchy of demographic factors: Sex, Day-Type, and Household Size
provide the largest predictive gains, while Region and Season, though important
for energy calculations, contribute little to predicting the activity sequence
itself. The result is an interpretable and robust generator of synthetic
activity traces, providing a high-fidelity foundation for downstream energy
systems modeling.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [273] [Bayesian Calibration and Model Assessment of Cell Migration Dynamics with Surrogate Model Integration](https://arxiv.org/abs/2509.18998)
*Christina Schenk,Jacobo Ayensa Jiménez,Ignacio Romero*

Main category: math.AP

TL;DR: 对细胞迁移模型参数概率分布进行贝叶斯校准评估，对比不同策略，为复杂生物系统计算模型校准和改进提供指导。


<details>
  <summary>Details</summary>
Motivation: 计算模型校准复杂，需系统评估参数概率分布以改进复杂生物过程计算模型。

Method: 使用贝叶斯校准，通过参数模型和代理模型，有无显式模型差异四种互补策略评估细胞迁移模型参数概率分布。

Result: 应用于胶质母细胞瘤进展实验，代理模型计算效率和预测准确性高，参数模型参数估计更可靠，纳入模型差异可揭示结构局限。

Conclusion: 这些比较为复杂生物系统计算模型的校准和改进提供实用指导。

Abstract: Computational models provide crucial insights into complex biological
processes such as cancer evolution, but their mechanistic nature often makes
them nonlinear and parameter-rich, complicating calibration. We systematically
evaluate parameter probability distributions in cell migration models using
Bayesian calibration across four complementary strategies: parametric and
surrogate models, each with and without explicit model discrepancy. This
approach enables joint analysis of parameter uncertainty, predictive
performance, and interpretability. Applied to a real data experiment of
glioblastoma progression in microfluidic devices, surrogate models achieve
higher computational efficiency and predictive accuracy, whereas parametric
models yield more reliable parameter estimates due to their mechanistic
grounding. Incorporating model discrepancy exposes structural limitations,
clarifying where model refinement is necessary. Together, these comparisons
offer practical guidance for calibrating and improving computational models of
complex biological systems.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [274] [Zero-Shot Visual Deepfake Detection: Can AI Predict and Prevent Fake Content Before It's Created?](https://arxiv.org/abs/2509.18461)
*Ayan Sar,Sampurna Roy,Tanupriya Choudhury,Ajith Abraham*

Main category: cs.GR

TL;DR: 研究零样本深度伪造检测与预防策略，指出面临挑战并探讨未来研究方向，强调跨学科合作重要性。


<details>
  <summary>Details</summary>
Motivation: GANs和扩散模型推动深度伪造技术发展，对数字安全等构成威胁，需探索零样本检测方法。

Method: 研究自监督学习、基于Transformer的零样本分类器等技术，提出AI驱动的预防策略。

Result: 零样本检测和预防面临对抗攻击、可扩展性等挑战。

Conclusion: 需结合零样本学习和预防机制构建综合防御框架，强调跨学科合作创建强大防御。

Abstract: Generative adversarial networks (GANs) and diffusion models have dramatically
advanced deepfake technology, and its threats to digital security, media
integrity, and public trust have increased rapidly. This research explored
zero-shot deepfake detection, an emerging method even when the models have
never seen a particular deepfake variation. In this work, we studied
self-supervised learning, transformer-based zero-shot classifier, generative
model fingerprinting, and meta-learning techniques that better adapt to the
ever-evolving deepfake threat. In addition, we suggested AI-driven prevention
strategies that mitigated the underlying generation pipeline of the deepfakes
before they occurred. They consisted of adversarial perturbations for creating
deepfake generators, digital watermarking for content authenticity
verification, real-time AI monitoring for content creation pipelines, and
blockchain-based content verification frameworks. Despite these advancements,
zero-shot detection and prevention faced critical challenges such as
adversarial attacks, scalability constraints, ethical dilemmas, and the absence
of standardized evaluation benchmarks. These limitations were addressed by
discussing future research directions on explainable AI for deepfake detection,
multimodal fusion based on image, audio, and text analysis, quantum AI for
enhanced security, and federated learning for privacy-preserving deepfake
detection. This further highlighted the need for an integrated defense
framework for digital authenticity that utilized zero-shot learning in
combination with preventive deepfake mechanisms. Finally, we highlighted the
important role of interdisciplinary collaboration between AI researchers,
cybersecurity experts, and policymakers to create resilient defenses against
the rising tide of deepfake attacks.

</details>


### [275] [Text Slider: Efficient and Plug-and-Play Continuous Concept Control for Image/Video Synthesis via LoRA Adapters](https://arxiv.org/abs/2509.18831)
*Pin-Yen Chiu,I-Sheng Fang,Jun-Cheng Chen*

Main category: cs.GR

TL;DR: 提出轻量级高效即插即用框架Text Slider，可连续控制视觉概念，降低训练时间、显存消耗和可训练参数数量，在图像和视频合成中更高效。


<details>
  <summary>Details</summary>
Motivation: 现有概念控制方法训练时间长、显存占用大，且不同扩散骨干需重新训练，可扩展性和适应性受限。

Method: 引入Text Slider框架，在预训练文本编码器中识别低秩方向。

Result: Text Slider能在保持输入空间布局和结构的同时平滑连续调节特定属性，训练速度比Concept Slider快5倍、比Attribute Control快47倍，显存使用分别降低近2倍和4倍。

Conclusion: Text Slider轻量、高效、即插即用，支持多概念组合和连续控制，可提升图像和视频合成效率。

Abstract: Recent advances in diffusion models have significantly improved image and
video synthesis. In addition, several concept control methods have been
proposed to enable fine-grained, continuous, and flexible control over
free-form text prompts. However, these methods not only require intensive
training time and GPU memory usage to learn the sliders or embeddings but also
need to be retrained for different diffusion backbones, limiting their
scalability and adaptability. To address these limitations, we introduce Text
Slider, a lightweight, efficient and plug-and-play framework that identifies
low-rank directions within a pre-trained text encoder, enabling continuous
control of visual concepts while significantly reducing training time, GPU
memory consumption, and the number of trainable parameters. Furthermore, Text
Slider supports multi-concept composition and continuous control, enabling
fine-grained and flexible manipulation in both image and video synthesis. We
show that Text Slider enables smooth and continuous modulation of specific
attributes while preserving the original spatial layout and structure of the
input. Text Slider achieves significantly better efficiency: 5$\times$ faster
training than Concept Slider and 47$\times$ faster than Attribute Control,
while reducing GPU memory usage by nearly 2$\times$ and 4$\times$,
respectively.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [276] [CayleyPy Growth: Efficient growth computations and hundreds of new conjectures on Cayley graphs (Brief version)](https://arxiv.org/abs/2509.19162)
*A. Chervov,D. Fedoriaka,E. Konstantinova,A. Naumov,I. Kiselev,A. Sheveleva,I. Koltsov,S. Lytkin,A. Smolensky,A. Soibelman,F. Levkovich-Maslyuk,R. Grimov,D. Volovich,A. Isakov,A. Kostin,M. Litvinov,N. Vilkin-Krom,A. Bidzhiev,A. Krasnyi,M. Evseev,E. Geraseva,L. Grunwald,S. Galkin,E. Koldunov,S. Diner,A. Chevychelov,E. Kudasheva,A. Sychev,A. Kravchenko,Z. Kogan,A. Natyrova,L. Shishina,L. Cheldieva,V. Zamkovoy,D. Kovalenko,O. Papulov,S. Kudashev,D. Shiltsov,R. Turtayev,O. Nikitina,D. Mamayeva,S. Nikolenko,M. Obozov,A. Titarenko,A. Dolgorukova,A. Aparnev,O. Debeaupuis,S. Alami C.,H. Isambert*

Main category: math.CO

TL;DR: 发布开源Python库CayleyPy，其处理图能力强、速度快，用它得到约200个关于Cayley和Schreier图的新猜想，创建Kaggle数据集，相关代码性能优于GAP和Sage。


<details>
  <summary>Details</summary>
Motivation: 将人工智能应用于群论问题，解决Cayley和Schreier图计算问题。

Method: 开发CayleyPy库进行相关计算和研究。

Result: 得到约200个新猜想，如对称群直径公式、改进Babai类型猜想等，创建Kaggle数据集，代码性能有优势。

Conclusion: CayleyPy库在群论计算中有出色表现，提出的多个猜想待进一步验证。

Abstract: This is the third paper of the CayleyPy project applying artificial
intelligence to problems in group theory. We announce the first public release
of CayleyPy, an open source Python library for computations with Cayley and
Schreier graphs. Compared with systems such as GAP and Sage, CayleyPy handles
much larger graphs and performs several orders of magnitude faster.
  Using CayleyPy we obtained about 200 new conjectures on Cayley and Schreier
graphs, focused on diameters and growth. For many Cayley graphs of symmetric
groups Sn we observe quasi polynomial diameter formulas: a small set of
quadratic or linear polynomials indexed by n mod s. We conjecture that this is
a general phenomenon, giving efficient diameter computation despite the problem
being NP hard. We propose a refinement of the Babai type conjecture on
diameters of Sn: n^2/2 + 4n upper bounds in the undirected case, compared to
previous O(n^2) bounds. We also provide explicit generator families, related to
involutions in a square with whiskers pattern, conjectured to maximize the
diameter; search confirms this for all n up to 15. We further conjecture an
answer to a question posed by V M Glushkov in 1968 on directed Cayley graphs
generated by a cyclic shift and a transposition.
  For nilpotent groups we conjecture an improvement of J S Ellenberg's results
on upper unitriangular matrices over Z/pZ, showing linear dependence of
diameter on p. Moreover.
  Some conjectures are LLM friendly, naturally stated as sorting problems
verifiable by algorithms or Python code. To benchmark path finding we created
more than 10 Kaggle datasets. CayleyPy works with arbitrary permutation or
matrix groups and includes over 100 predefined generators. Our growth
computation code outperforms GAP and Sage up to 1000 times in speed and size.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [277] [Scalable Evaluation for Audio Identification via Synthetic Latent Fingerprint Generation](https://arxiv.org/abs/2509.18620)
*Aditya Bhattacharjee,Marco Pasini,Emmanouil Benetos*

Main category: cs.SD

TL;DR: 提出无音频方法合成近似真实指纹分布的潜在指纹，用于大规模评估音频指纹识别，验证其有效性并提供系统可扩展性指标。


<details>
  <summary>Details</summary>
Motivation: 解决因缺乏大型公共音乐数据库而难以进行大规模音频指纹评估的问题。

Method: 在预训练神经音频指纹系统提取的嵌入上训练整流流模型，合成潜在指纹。

Result: 合成指纹可作为逼真干扰项，模拟大规模检索性能，合成干扰项的扩展趋势与真实干扰项相近。

Conclusion: 该方法提供了不依赖音频语料库的系统可扩展性实用指标。

Abstract: The evaluation of audio fingerprinting at a realistic scale is limited by the
scarcity of large public music databases. We present an audio-free approach
that synthesises latent fingerprints which approximate the distribution of real
fingerprints. Our method trains a Rectified Flow model on embeddings extracted
by pre-trained neural audio fingerprinting systems. The synthetic fingerprints
generated using our system act as realistic distractors and enable the
simulation of retrieval performance at a large scale without requiring
additional audio. We assess the fidelity of synthetic fingerprints by comparing
the distributions to real data. We further benchmark the retrieval performances
across multiple state-of-the-art audio fingerprinting frameworks by augmenting
real reference databases with synthetic distractors, and show that the scaling
trends obtained with synthetic distractors closely track those obtained with
real distractors. Finally, we scale the synthetic distractor database to model
retrieval performance for very large databases, providing a practical metric of
system scalability that does not depend on access to audio corpora.

</details>


### [278] [MNV-17: A High-Quality Performative Mandarin Dataset for Nonverbal Vocalization Recognition in Speech](https://arxiv.org/abs/2509.18196)
*Jialong Mai,Jinxin Ji,Xiaofen Xing,Chen Yang,Weidong Chen,Jingyuan Xing,Xiangmin Xu*

Main category: cs.SD

TL;DR: 引入MNV - 17普通话语音数据集解决NV感知ASR缺乏高质量标注数据集问题，对四个主流ASR架构进行基准测试并将公开数据和模型。


<details>
  <summary>Details</summary>
Motivation: 主流ASR系统难以识别非语言发声，且NV感知ASR发展受缺乏高质量标注数据集阻碍，为全面理解人类交流需解决此问题。

Method: 引入7.55小时的MNV - 17表演式普通话语音数据集，对四个主流ASR架构进行基准测试。

Result: MNV - 17具有高保真、清晰的NV实例，包含17种不同且平衡的常见NV类别。

Conclusion: 公开数据集和预训练模型检查点以促进富有表现力的ASR未来研究。

Abstract: Mainstream Automatic Speech Recognition (ASR) systems excel at transcribing
lexical content, but largely fail to recognize nonverbal vocalizations (NVs)
embedded in speech, such as sighs, laughs, and coughs. This capability is
important for a comprehensive understanding of human communication, as NVs
convey crucial emotional and intentional cues. Progress in NV-aware ASR has
been hindered by the lack of high-quality, well-annotated datasets. To address
this gap, we introduce MNV-17, a 7.55-hour performative Mandarin speech
dataset. Unlike most existing corpora that rely on model-based detection,
MNV-17's performative nature ensures high-fidelity, clearly articulated NV
instances. To the best of our knowledge, MNV-17 provides the most extensive set
of nonverbal vocalization categories, comprising 17 distinct and well-balanced
classes of common NVs. We benchmarked MNV-17 on four mainstream ASR
architectures, evaluating their joint performance on semantic transcription and
NV classification. The dataset and the pretrained model checkpoints will be
made publicly available to facilitate future research in expressive ASR.

</details>


### [279] [Scattering Transformer: A Training-Free Transformer Architecture for Heart Murmur Detection](https://arxiv.org/abs/2509.18424)
*Rami Zewail*

Main category: cs.SD

TL;DR: 本文提出用于心脏杂音检测的无训练散射变压器架构，在公开数据集上表现有竞争力，是资源受限场景的可行方案。


<details>
  <summary>Details</summary>
Motivation: 现有自动心脏听诊深度学习方法存在训练数据有限、计算量大问题，需轻量级替代方案。

Method: 引入散射变压器，利用标准小波散射网络，以类变压器架构引入上下文依赖且无需反向传播。

Result: 在CirCor DigiScope数据集上，散射变压器加权准确率0.786，未加权平均召回率0.697，与现有方法竞争力强。

Conclusion: 散射变压器在资源受限场景是可行且有前景的替代方案。

Abstract: In an attempt to address the need for skilled clinicians in heart sound
interpretation, recent research efforts on automating cardiac auscultation have
explored deep learning approaches. The majority of these approaches have been
based on supervised learning that is always challenged in occasions where
training data is limited. More recently, there has been a growing interest in
potentials of pre-trained self-supervised audio foundation models for
biomedical end tasks. Despite exhibiting promising results, these foundational
models are typically computationally intensive. Within the context of automatic
cardiac auscultation, this study explores a lightweight alternative to these
general-purpose audio foundation models by introducing the Scattering
Transformer, a novel, training-free transformer architecture for heart murmur
detection. The proposed method leverages standard wavelet scattering networks
by introducing contextual dependencies in a transformer-like architecture
without any backpropagation. We evaluate our approach on the public CirCor
DigiScope dataset, directly comparing it against leading general-purpose
foundational models. The Scattering Transformer achieves a Weighted
Accuracy(WAR) of 0.786 and an Unweighted Average Recall(UAR) of 0.697,
demonstrating performance highly competitive with contemporary state of the art
methods. This study establishes the Scattering Transformer as a viable and
promising alternative in resource-constrained setups.

</details>


### [280] [Explore the Reinforcement Learning for the LLM based ASR and TTS system](https://arxiv.org/abs/2509.18569)
*Changfeng Gao,Yabin Li,Keyu An,Zhifu Gao,Zhihao Du,Han Zhao,Xiangang Li*

Main category: cs.SD

TL;DR: 本文提出适用于音频大语言模型的轻量级强化学习框架，评估其在ASR和TTS任务的有效性，实验表明RL可显著提升系统性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习在文本任务提升大语言模型性能显著，但在ASR和TTS应用因音频模型训练复杂而研究不足。

Method: 提出轻量级RL框架，在ASR任务用GRPO框架试验不同规则奖励函数并研究RL数据构建影响，在TTS任务比较GRPO和DiffRO并结合二者。

Result: 实验证明即使训练数据有限和优化步骤少，RL也能显著提升ASR和TTS系统性能。

Conclusion: 强化学习可有效提升音频大语言模型在ASR和TTS任务中的性能。

Abstract: In recent years, large language models (LLMs) have played an important role
in automatic speech recognition (ASR) and text-to-speech (TTS) systems. While
reinforcement learning (RL) has significantly enhanced LLM performance in
text-based tasks, its application to ASR and TTS remains underexplored due to
the complexity of training audio-based models. In this study, we propose a
lightweight RL framework tailored for audio-based LLMs that can process audio
inputs and generate audio outputs. Based on this framework, we evaluate the
effectiveness of reinforcement learning on both ASR and TTS tasks. For the ASR
task, we experiment with different rule-based reward functions within the Group
Relative Policy Optimization (GRPO) framework and investigate the impact of RL
data construction. For the TTS task, we compare GRPO with Differentiable Reward
Optimization (DiffRO) and further combine the two approaches to achieve
improved performance. Our experiments demonstrate that RL can significantly
enhance the performance of both ASR and TTS systems, even with limited training
data and a small number of optimization steps.

</details>


### [281] [An overview of neural architectures for self-supervised audio representation learning from masked spectrograms](https://arxiv.org/abs/2509.18691)
*Sarthak Yadav,Sergios Theodoridis,Zheng-Hua Tan*

Main category: cs.SD

TL;DR: 本文对掩码频谱图建模、Mamba和xLSTM等神经序列建模架构进行全面概述，并在十种下游音频分类任务上比较基于Transformer、Mamba和xLSTM的掩码频谱图模型。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对掩码频谱图建模与循环序列建模方法交集的充分概述，需要为读者提供参考。

Method: 对相关研究领域进行全面概述，并在统一可复现框架下比较不同模型在十种下游音频分类任务的表现。

Result: 未提及具体结果。

Conclusion: 帮助读者在相邻应用中选择合适的方法。

Abstract: In recent years, self-supervised learning has amassed significant interest
for training deep neural representations without labeled data. One such
self-supervised learning approach is masked spectrogram modeling, where the
objective is to learn semantically rich contextual representations by
predicting removed or hidden portions of the input audio spectrogram. With the
Transformer neural architecture at its core, masked spectrogram modeling has
emerged as the prominent approach for learning general purpose audio
representations, a.k.a. audio foundation models. Meanwhile, addressing the
issues of the Transformer architecture, in particular the underlying Scaled
Dot-product Attention operation, which scales quadratically with input sequence
length, has led to renewed interest in recurrent sequence modeling approaches.
Among them, Selective structured state space models (such as Mamba) and
extended Long Short-Term Memory (xLSTM) are the two most promising approaches
which have experienced widespread adoption. While the body of work on these two
topics continues to grow, there is currently a lack of an adequate overview
encompassing the intersection of these topics. In this paper, we present a
comprehensive overview of the aforementioned research domains, covering masked
spectrogram modeling and the previously mentioned neural sequence modeling
architectures, Mamba and xLSTM. Further, we compare Transformers, Mamba and
xLSTM based masked spectrogram models in a unified, reproducible framework on
ten diverse downstream audio classification tasks, which will help interested
readers to make informed decisions regarding suitability of the evaluated
approaches to adjacent applications.

</details>


### [282] [Identifying birdsong syllables without labelled data](https://arxiv.org/abs/2509.18412)
*Mélisande Teng,Julien Boussard,David Rolnick,Hugo Larochelle*

Main category: cs.SD

TL;DR: 提出首个完全无监督算法将鸟鸣录音分解为音节序列，在数据集上表现良好且能区分个体鸟类。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法依赖标注数据，限制了在更多物种和数据集上的应用，需要无监督算法来分解鸟鸣录音。

Method: 先检测音节事件，再聚类提取模板，最后进行匹配追踪将录音分解为音节序列。

Result: 在孟加拉雀鸟鸣数据集上，无监督方法与人类标注对比表现良好，且能区分孟加拉雀和大山雀的个体。

Conclusion: 所提出的完全无监督算法可有效分解鸟鸣录音，还能通过独特声音特征区分个体鸟类。

Abstract: Identifying sequences of syllables within birdsongs is key to tackling a wide
array of challenges, including bird individual identification and better
understanding of animal communication and sensory-motor learning. Recently,
machine learning approaches have demonstrated great potential to alleviate the
need for experts to label long audio recordings by hand. However, they still
typically rely on the availability of labelled data for model training,
restricting applicability to a few species and datasets. In this work, we build
the first fully unsupervised algorithm to decompose birdsong recordings into
sequences of syllables. We first detect syllable events, then cluster them to
extract templates --syllable representations-- before performing matching
pursuit to decompose the recording as a sequence of syllables. We evaluate our
automatic annotations against human labels on a dataset of Bengalese finch
songs and find that our unsupervised method achieves high performance. We also
demonstrate that our approach can distinguish individual birds within a species
through their unique vocal signatures, for both Bengalese finches and another
species, the great tit.

</details>


### [283] [Finding My Voice: Generative Reconstruction of Disordered Speech for Automated Clinical Evaluation](https://arxiv.org/abs/2509.19231)
*Karen Rosero,Eunjung Yeo,David R. Mortensen,Cortney Van't Slot,Rami R. Hallac,Carlos Busso*

Main category: cs.SD

TL;DR: 提出ChiReSSD语音重建框架，能保留儿童说话者身份并抑制发音错误，在STAR和TORGO数据集上实验效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有的方法多基于健康成年人语音训练，缺乏针对有语音障碍儿童语音的处理方法。

Method: 提出ChiReSSD框架，适应有语音障碍儿童的声音，尤其关注音高和韵律。

Result: 在STAR数据集上词汇准确性和说话者身份保留有显著提升；自动预测与人工标注的皮尔逊相关系数为0.63；在TORGO数据集上对成人构音障碍语音重建有有效泛化。

Conclusion: 基于风格解耦的TTS重建可以为不同临床人群提供保留身份的语音。

Abstract: We present ChiReSSD, a speech reconstruction framework that preserves
children speaker's identity while suppressing mispronunciations. Unlike prior
approaches trained on healthy adult speech, ChiReSSD adapts to the voices of
children with speech sound disorders (SSD), with particular emphasis on pitch
and prosody. We evaluate our method on the STAR dataset and report substantial
improvements in lexical accuracy and speaker identity preservation.
Furthermore, we automatically predict the phonetic content in the original and
reconstructed pairs, where the proportion of corrected consonants is comparable
to the percentage of correct consonants (PCC), a clinical speech assessment
metric. Our experiments show Pearson correlation of 0.63 between automatic and
human expert annotations, highlighting the potential to reduce the manual
transcription burden. In addition, experiments on the TORGO dataset demonstrate
effective generalization for reconstructing adult dysarthric speech. Our
results indicate that disentangled, style-based TTS reconstruction can provide
identity-preserving speech across diverse clinical populations.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [284] [No Verifiable Reward for Prosody: Toward Preference-Guided Prosody Learning in TTS](https://arxiv.org/abs/2509.18531)
*Seungyoun Shin,Dongha Ahn,Jiwoo Kim,Sungwook Jeon*

Main category: eess.AS

TL;DR: 现有GRPO用于神经TTS有问题，提出迭代DPO方案优化韵律自然度，在KoCC - TTS数据集上表现好，表明人工偏好优化是自然鲁棒TTS的有效途径。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO在无韵律可验证奖励时，训练会使韵律单调不自然，添加说话人相似度还会破坏训练和CER，需要解决该问题。

Method: 提出迭代Direct Preference Optimization (DPO)方案，每轮仅用几百个人工标注偏好对优化韵律自然度并正则化到当前模型。

Result: 在KoCC - TTS数据集上，该方法获得最高人类偏好（ELO）和有竞争力的CER，优于GRPO和商业基线。

Conclusion: 当韵律无法自动奖励时，人工偏好优化为自然和鲁棒的TTS提供了实用且数据高效的途径。

Abstract: Recent work reports gains in neural text-to-speech (TTS) with Group Relative
Policy Optimization (GRPO). However, in the absence of a verifiable reward for
\textit{prosody}, GRPO trained on transcription-oriented signals (CER/NLL)
lowers error rates yet collapses prosody into monotone, unnatural speech;
adding speaker-similarity further destabilizes training and degrades CER. We
address this with an \textit{iterative Direct Preference Optimization (DPO)}
scheme that uses only a few hundred human-labeled preference pairs per round to
directly optimize prosodic naturalness while regularizing to the current model.
On \textbf{KoCC-TTS}, a curated dataset of authentic Korean call center
interactions capturing task-oriented dialogues, our method attains the highest
human preference (ELO) with competitive CER, outperforming GRPO and strong
commercial baselines. These results suggest that when prosody cannot be
rewarded automatically, \textit{human preference optimization} offers a
practical and data-efficient path to natural and robust TTS. The demo page is
available at \href{https://tts.ch.dev}

</details>


### [285] [SoundCompass: Navigating Target Sound Extraction With Effective Directional Clue Integration In Complex Acoustic Scenes](https://arxiv.org/abs/2509.18561)
*Dayun Choi,Jung-Woo Choi*

Main category: eess.AS

TL;DR: 提出SoundCompass框架用于目标声音提取，结合SPIN模块、SH嵌入和CoI策略，实验表明其能在不同信号类别和空间配置下稳健提取目标源。


<details>
  <summary>Details</summary>
Motivation: 以往基于到达方向（DoA）的目标声音提取方法依赖手工特征或离散编码，丢失细粒度空间信息且适应性有限。

Method: 提出以Spectral Pairwise INteraction (SPIN) 模块为核心的SoundCompass框架，将基于空间相关性的输入特征与以球谐函数（SH）编码表示的DoA线索融合，跨重叠子带进行融合，并引入迭代细化策略chain-of-inference (CoI)。

Result: SoundCompass结合SPIN、SH嵌入和CoI，能在不同信号类别和空间配置下稳健提取目标源。

Conclusion: SoundCompass是一种有效的方向线索集成框架，可用于目标声音提取。

Abstract: Recent advances in target sound extraction (TSE) utilize directional clues
derived from direction of arrival (DoA), which represent an inherent spatial
property of sound available in any acoustic scene. However, previous DoA-based
methods rely on hand-crafted features or discrete encodings, which lose
fine-grained spatial information and limit adaptability. We propose
SoundCompass, an effective directional clue integration framework centered on a
Spectral Pairwise INteraction (SPIN) module that captures cross-channel spatial
correlations in the complex spectrogram domain to preserve full spatial
information in multichannel signals. The input feature expressed in terms of
spatial correlations is fused with a DoA clue represented as spherical
harmonics (SH) encoding. The fusion is carried out across overlapping frequency
subbands, inheriting the benefits reported in the previous band-split
architectures. We also incorporate the iterative refinement strategy,
chain-of-inference (CoI), in the TSE framework, which recursively fuses DoA
with sound event activation estimated from the previous inference stage.
Experiments demonstrate that SoundCompass, combining SPIN, SH embedding, and
CoI, robustly extracts target sources across diverse signal classes and spatial
configurations.

</details>


### [286] [SynSonic: Augmenting Sound Event Detection through Text-to-Audio Diffusion ControlNet and Effective Sample Filtering](https://arxiv.org/abs/2509.18603)
*Jiarui Hai,Mounya Elhilali*

Main category: eess.AS

TL;DR: 提出针对声音事件检测（SED）的数据增强方法SynSonic，实验表明其能提升PSDS分数。


<details>
  <summary>Details</summary>
Motivation: 现有声音事件检测数据增强方法受样本多样性限制，且直接应用生成模型有缺乏精确时间标注和引入噪声的问题，需新的增强方法。

Method: 提出SynSonic方法，利用文本到音频扩散模型结合能量包络ControlNet生成时间连贯的声音事件，用双分类器的联合分数过滤策略确保样本质量并探索其在训练管道中的集成。

Result: SynSonic提高了多声部声音检测分数（PSDS1和PSDS2），提升了时间定位和声音类别区分能力。

Conclusion: SynSonic是一种有效的针对声音事件检测的数据增强方法。

Abstract: Data synthesis and augmentation are essential for Sound Event Detection (SED)
due to the scarcity of temporally labeled data. While augmentation methods like
SpecAugment and Mix-up can enhance model performance, they remain constrained
by the diversity of existing samples. Recent generative models offer new
opportunities, yet their direct application to SED is challenging due to the
lack of precise temporal annotations and the risk of introducing noise through
unreliable filtering. To address these challenges and enable generative-based
augmentation for SED, we propose SynSonic, a data augmentation method tailored
for this task. SynSonic leverages text-to-audio diffusion models guided by an
energy-envelope ControlNet to generate temporally coherent sound events. A
joint score filtering strategy with dual classifiers ensures sample quality,
and we explore its practical integration into training pipelines. Experimental
results show that SynSonic improves Polyphonic Sound Detection Scores (PSDS1
and PSDS2), enhancing both temporal localization and sound class
discrimination.

</details>


### [287] [FlexSED: Towards Open-Vocabulary Sound Event Detection](https://arxiv.org/abs/2509.18606)
*Jiarui Hai,Helin Wang,Weizhe Guo,Mounya Elhilali*

Main category: eess.AS

TL;DR: 提出开放词汇声音事件检测系统FlexSED，表现优于传统SED模型且有零样本和少样本能力，还开源代码和预训练模型。


<details>
  <summary>Details</summary>
Motivation: 现有多分类框架不能处理自由文本声音查询，缺乏零样本能力和少样本适应性，文本查询分离方法不适用于SED任务。

Method: 基于预训练音频SSL模型和CLAP文本编码器，引入编解码器组合和自适应融合策略，用大语言模型辅助训练时的事件查询选择。

Result: FlexSED在AudioSet - Strong上表现优于普通SED模型，具备强大的零样本和少样本能力。

Conclusion: FlexSED有效解决现有SED系统的问题，开源代码和模型可支持后续研究和应用。

Abstract: Despite recent progress in large-scale sound event detection (SED) systems
capable of handling hundreds of sound classes, existing multi-class
classification frameworks remain fundamentally limited. They cannot process
free-text sound queries, which enable more flexible and user-friendly
interaction, and they lack zero-shot capabilities and offer poor few-shot
adaptability. Although text-query-based separation methods have been explored,
they primarily focus on source separation and are ill-suited for SED tasks that
require precise temporal localization and efficient detection across large and
diverse sound vocabularies. In this paper, we propose FlexSED, an
open-vocabulary sound event detection system. FlexSED builds on a pretrained
audio SSL model and the CLAP text encoder, introducing an encoder-decoder
composition and an adaptive fusion strategy to enable effective continuous
training from pretrained weights. To ensure robust supervision, it also employs
large language models (LLMs) to assist in event query selection during
training, addressing challenges related to missing labels. As a result, FlexSED
achieves superior performance compared to vanilla SED models on
AudioSet-Strong, while demonstrating strong zero-shot and few-shot
capabilities. We release the code and pretrained models to support future
research and applications based on FlexSED.

</details>


### [288] [Training Flow Matching Models with Reliable Labels via Self-Purification](https://arxiv.org/abs/2509.19091)
*Hyeongju Kim,Yechan Yu,June Young Yi,Juheon Lee*

Main category: eess.AS

TL;DR: 提出Self - Purifying Flow Matching (SPFM) 过滤不可靠数据，实验证明其在噪声标签训练下有效，在TITW数据集上表现超现有基线。


<details>
  <summary>Details</summary>
Motivation: 训练数据集存在标签污染问题，会显著降低训练模型的性能。

Method: 引入Self - Purifying Flow Matching (SPFM)，在训练过程中利用模型本身识别可疑数据，无需预训练模型或额外模块。

Result: 使用SPFM训练的模型即使在噪声标签上训练，也能生成准确符合指定条件的样本；在TITW数据集上验证了SPFM的鲁棒性，性能超越现有基线。

Conclusion: SPFM是一种在流匹配框架内过滤不可靠数据的有效方法。

Abstract: Training datasets are inherently imperfect, often containing mislabeled
samples due to human annotation errors, limitations of tagging models, and
other sources of noise. Such label contamination can significantly degrade the
performance of a trained model. In this work, we introduce Self-Purifying Flow
Matching (SPFM), a principled approach to filtering unreliable data within the
flow-matching framework. SPFM identifies suspicious data using the model itself
during the training process, bypassing the need for pretrained models or
additional modules. Our experiments demonstrate that models trained with SPFM
generate samples that accurately adhere to the specified conditioning, even
when trained on noisy labels. Furthermore, we validate the robustness of SPFM
on the TITW dataset, which consists of in-the-wild speech data, achieving
performance that surpasses existing baselines.

</details>


### [289] [Audio-Based Pedestrian Detection in the Presence of Vehicular Noise](https://arxiv.org/abs/2509.19295)
*Yonghyun Kim,Chaeyeon Han,Akash Sarode,Noah Posner,Subhrajit Guhathakurta,Alexander Lerch*

Main category: eess.AS

TL;DR: 提出新数据集并对有车辆噪音时基于音频的行人检测进行分析


<details>
  <summary>Details</summary>
Motivation: 以往基于音频的行人检测仅在噪声受限环境中探索，本文要研究有车辆噪音时的情况

Method: 进行三项分析，包括跨数据集评估、评估噪声数据对模型性能的影响、评估模型对域外声音的预测鲁棒性

Result: 创建了包含1321小时路边数据的新数据集，数据包含交通丰富的音景、同步音频、行人注释和视频缩略图

Conclusion: 未明确提及结论，但为有车辆噪音环境下的音频行人检测研究提供了新数据集和分析

Abstract: Audio-based pedestrian detection is a challenging task and has, thus far,
only been explored in noise-limited environments. We present a new dataset,
results, and a detailed analysis of the state-of-the-art in audio-based
pedestrian detection in the presence of vehicular noise. In our study, we
conduct three analyses: (i) cross-dataset evaluation between noisy and
noise-limited environments, (ii) an assessment of the impact of noisy data on
model performance, highlighting the influence of acoustic context, and (iii) an
evaluation of the model's predictive robustness on out-of-domain sounds. The
new dataset is a comprehensive 1321-hour roadside dataset. It incorporates
traffic-rich soundscapes. Each recording includes 16kHz audio synchronized with
frame-level pedestrian annotations and 1fps video thumbnails.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [290] [Are Smaller Open-Weight LLMs Closing the Gap to Proprietary Models for Biomedical Question Answering?](https://arxiv.org/abs/2509.18843)
*Damian Stachura,Joanna Konieczna,Artur Nowak*

Main category: cs.CL

TL;DR: 本文探讨小的开源权重LLM能否替代大的闭源模型，在生物医学问答领域对比多种开源与闭源模型，结果显示开源模型相当甚至更优，代码开源。


<details>
  <summary>Details</summary>
Motivation: 随着开源权重LLM发展，探讨小的开源权重LLM能否有效替代大的闭源模型，尤其在生物医学问答领域。

Method: 参加BioASQ挑战的Task 13B Phase B，对比多种开源和闭源模型；用基于嵌入距离检索片段、上下文学习、结构化输出等技术提升问答能力；部分提交用集成方法处理精确答案问题。

Result: 开源权重LLM与闭源模型表现相当，应用集成策略时开源模型甚至超越闭源模型。

Conclusion: 开源权重LLM在生物医学问答领域有替代闭源模型的潜力。

Abstract: Open-weight versions of large language models (LLMs) are rapidly advancing,
with state-of-the-art models like DeepSeek-V3 now performing comparably to
proprietary LLMs. This progression raises the question of whether small
open-weight LLMs are capable of effectively replacing larger closed-source
models. We are particularly interested in the context of biomedical
question-answering, a domain we explored by participating in Task 13B Phase B
of the BioASQ challenge. In this work, we compare several open-weight models
against top-performing systems such as GPT-4o, GPT-4.1, Claude 3.5 Sonnet, and
Claude 3.7 Sonnet. To enhance question answering capabilities, we use various
techniques including retrieving the most relevant snippets based on embedding
distance, in-context learning, and structured outputs. For certain submissions,
we utilize ensemble approaches to leverage the diverse outputs generated by
different models for exact-answer questions. Our results demonstrate that
open-weight LLMs are comparable to proprietary ones. In some instances,
open-weight LLMs even surpassed their closed counterparts, particularly when
ensembling strategies were applied. All code is publicly available at
https://github.com/evidenceprime/BioASQ-13b.

</details>


### [291] [Pathways of Thoughts: Multi-Directional Thinking for Long-form Personalized Question Answering](https://arxiv.org/abs/2509.19094)
*Alireza Salemi,Cheng Li,Mingyang Zhang,Qiaozhu Mei,Zhuowan Li,Spurthi Amba Hombaiah,Weize Kong,Tao Chen,Hamed Zamani,Michael Bendersky*

Main category: cs.CL

TL;DR: 提出Pathways of Thoughts (PoT)方法解决个性化问答挑战，实验表明其优于基线。


<details>
  <summary>Details</summary>
Motivation: 个性化问答因推断偏好和生成合适回复的挑战而研究不足，需解决这些问题。

Method: PoT是推理阶段方法，将大语言模型推理建模为迭代决策过程，探索多推理轨迹生成候选回复，再根据用户偏好聚合和重新加权。

Result: 在LaMP - QA基准测试中，PoT始终优于竞争基线，相对改进达13.1%；人工评估中66%情况标注者更喜欢PoT输出。

Conclusion: PoT方法能有效解决个性化问答挑战，提高问答效果。

Abstract: Personalization is essential for adapting question answering (QA) systems to
user-specific information needs, thereby improving both accuracy and user
satisfaction. However, personalized QA remains relatively underexplored due to
challenges such as inferring preferences from long, noisy, and implicit
contexts, and generating responses that are simultaneously correct,
contextually appropriate, and aligned with user expectations and background
knowledge. To address these challenges, we propose Pathways of Thoughts (PoT),
an inference-stage method that applies to any large language model (LLM)
without requiring task-specific fine-tuning. The approach models the reasoning
of an LLM as an iterative decision process, where the model dynamically selects
among cognitive operations such as reasoning, revision, personalization, and
clarification. This enables exploration of multiple reasoning trajectories,
producing diverse candidate responses that capture different perspectives. PoT
then aggregates and reweights these candidates according to inferred user
preferences, yielding a final personalized response that benefits from the
complementary strengths of diverse reasoning paths. Experiments on the LaMP-QA
benchmark for personalized QA show that PoT consistently outperforms
competitive baselines, achieving up to a 13.1% relative improvement. Human
evaluation corroborates these results, with annotators preferring outputs from
PoT in 66% of cases and reporting ties in only 15% of cases.

</details>


### [292] [Event Causality Identification with Synthetic Control](https://arxiv.org/abs/2509.18156)
*Haoyu Wang,Fengze Liu,Jiayao Zhang,Dan Roth,Kyle Richardson*

Main category: cs.CL

TL;DR: 本文采用鲁宾因果模型识别事件因果关系，遇实际困难后用合成控制法解决，在COPES - hard基准上表现优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 传统事件因果识别方法因因果关系的非正式使用和似是而非的图形推理，有错误识别因果关系的风险。

Method: 采用鲁宾因果模型，将两个时间顺序的事件分别视为处理和观察结果；为解决实际操作困难，用合成控制法结合文本嵌入合成和反演技术生成匹配对象。

Result: 该方法在COPES - hard因果基准上比包括GPT - 4在内的以往方法能更稳健地识别因果关系。

Conclusion: 所提方法能更有效地进行事件因果关系识别，优于传统方法和GPT - 4。

Abstract: Event causality identification (ECI), a process that extracts causal
relations between events from text, is crucial for distinguishing causation
from correlation. Traditional approaches to ECI have primarily utilized
linguistic patterns and multi-hop relational inference, risking false causality
identification due to informal usage of causality and specious graphical
inference. In this paper, we adopt the Rubin Causal Model to identify event
causality: given two temporally ordered events, we see the first event as the
treatment and the second one as the observed outcome. Determining their
causality involves manipulating the treatment and estimating the resultant
change in the likelihood of the outcome. Given that it is only possible to
implement manipulation conceptually in the text domain, as a work-around, we
try to find a twin for the protagonist from existing corpora. This twin should
have identical life experiences with the protagonist before the treatment but
undergoes an intervention of treatment. However, the practical difficulty of
locating such a match limits its feasibility. Addressing this issue, we use the
synthetic control method to generate such a twin' from relevant historical
data, leveraging text embedding synthesis and inversion techniques. This
approach allows us to identify causal relations more robustly than previous
methods, including GPT-4, which is demonstrated on a causality benchmark,
COPES-hard.

</details>


### [293] [Evaluating Large Language Models for Detecting Antisemitism](https://arxiv.org/abs/2509.18293)
*Jay Patel,Hrudayangam Mehta,Jeremy Blackburn*

Main category: cs.CL

TL;DR: 评估8个开源大语言模型检测反犹内容能力，设计新提示Guided - CoT提升性能，分析模型错误并引入量化指标，揭示模型间差异。


<details>
  <summary>Details</summary>
Motivation: 检测仇恨内容具挑战性，自动化工具需不断训练，评估大语言模型检测反犹内容能力。

Method: 利用上下文定义作为政策指南，探索多种提示技术，设计新的Guided - CoT提示，检查模型错误并引入量化指标。

Result: Guided - CoT提升所有评估模型性能，Llama 3.1 70B表现优于微调的GPT - 3.5，揭示模型间差异和矛盾行为。

Conclusion: 实验凸显大语言模型在实用性、可解释性和可靠性方面存在差异。

Abstract: Detecting hateful content is a challenging and important problem. Automated
tools, like machine-learning models, can help, but they require continuous
training to adapt to the ever-changing landscape of social media. In this work,
we evaluate eight open-source LLMs' capability to detect antisemitic content,
specifically leveraging in-context definition as a policy guideline. We explore
various prompting techniques and design a new CoT-like prompt, Guided-CoT.
Guided-CoT handles the in-context policy well, increasing performance across
all evaluated models, regardless of decoding configuration, model sizes, or
reasoning capability. Notably, Llama 3.1 70B outperforms fine-tuned GPT-3.5.
Additionally, we examine LLM errors and introduce metrics to quantify semantic
divergence in model-generated rationales, revealing notable differences and
paradoxical behaviors among LLMs. Our experiments highlight the differences
observed across LLMs' utility, explainability, and reliability.

</details>


### [294] [Brittleness and Promise: Knowledge Graph Based Reward Modeling for Diagnostic Reasoning](https://arxiv.org/abs/2509.18316)
*Saksham Khatwani,He Cheng,Majid Afshar,Dmitriy Dligach,Yanjun Gao*

Main category: cs.CL

TL;DR: 探索将大语言模型作为知识图谱推理路径奖励模型的诊断推理范式，评估任务和训练范式，实验显示有前景但下游任务迁移性弱。


<details>
  <summary>Details</summary>
Motivation: 大语言模型缺乏可靠的基于知识的推理，现有集成知识图谱的方法不能进行结构化推理，需探索新范式。

Method: 将大语言模型作为知识图谱推理路径的奖励模型，系统评估五种任务表述和八种训练范式，并测试路径判断能力在下游诊断任务的泛化性。

Result: 特定的奖励优化和蒸馏能带来强路径判断性能，但对下游任务的迁移性较弱。

Conclusion: 首次对临床知识图谱上的“奖励模型风格”推理进行系统评估，为基于结构化、奖励的监督如何影响医疗领域生成式人工智能系统的诊断推理提供见解。

Abstract: Large language models (LLMs) show promise for diagnostic reasoning but often
lack reliable, knowledge grounded inference. Knowledge graphs (KGs), such as
the Unified Medical Language System (UMLS), offer structured biomedical
knowledge that can support trustworthy reasoning. Prior approaches typically
integrate KGs via retrieval augmented generation or fine tuning, inserting KG
content into prompts rather than enabling structured reasoning. We explore an
alternative paradigm: treating the LLM as a reward model of KG reasoning paths,
where the model learns to judge whether a candidate path leads to correct
diagnosis for a given patient input. This approach is inspired by recent work
that leverages reward training to enhance model reasoning abilities, and
grounded in computational theory, which suggests that verifying a solution is
often easier than generating one from scratch. It also parallels physicians'
diagnostic assessment, where they judge which sequences of findings and
intermediate conditions most plausibly support a diagnosis. We first
systematically evaluate five task formulation for knowledge path judging and
eight training paradigm. Second, we test whether the path judging abilities
generalize to downstream diagnostic tasks, including diagnosis summarization
and medical question answering. Experiments with three open source
instruct-tuned LLMs reveal both promise and brittleness: while specific reward
optimization and distillation lead to strong path-judging performance, the
transferability to downstream tasks remain weak. Our finding provides the first
systematic assessment of "reward model style" reasoning over clinical KGs,
offering insights into how structured, reward-based supervision influences
diagnostic reasoning in GenAI systems for healthcare.

</details>


### [295] [Developing an AI framework to automatically detect shared decision-making in patient-doctor conversations](https://arxiv.org/abs/2509.18439)
*Oscar J. Ponce-Ponte,David Toro-Tobon,Luis F. Figueroa,Michael Gionfriddo,Megan Branda,Victor M. Montori,Saturnino Luz,Juan P. Brito*

Main category: cs.CL

TL;DR: 本文提出通过语言建模和对话对齐分数自动衡量共享决策制定（SDM）的方法，经实验验证该方法可行。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏自动大规模衡量SDM的方法，为实现以患者为中心的护理，需开发相关自动化方法。

Method: 转录157个医患对话并分句，用上下文 - 响应配对和负采样训练深度学习模型和微调BERT模型，计算四种CA分数，用随机效应分析评估CA分数与SDM结果的关联。

Result: 微调的BERTbase(110M)召回率最高，部分CA分数与SDM结果相关，BERT模型大小对CA分数和SDM的关联无影响。

Conclusion: 研究引入自动化、可拓展的方法，通过可解释的CA分数衡量医患对话中的SDM，有潜力大规模评估SDM策略。

Abstract: Shared decision-making (SDM) is necessary to achieve patient-centred care.
Currently no methodology exists to automatically measure SDM at scale. This
study aimed to develop an automated approach to measure SDM by using language
modelling and the conversational alignment (CA) score. A total of 157
video-recorded patient-doctor conversations from a randomized multi-centre
trial evaluating SDM decision aids for anticoagulation in atrial fibrillations
were transcribed and segmented into 42,559 sentences. Context-response pairs
and negative sampling were employed to train deep learning (DL) models and
fine-tuned BERT models via the next sentence prediction (NSP) task. Each
top-performing model was used to calculate four types of CA scores. A
random-effects analysis by clinician, adjusting for age, sex, race, and trial
arm, assessed the association between CA scores and SDM outcomes: the
Decisional Conflict Scale (DCS) and the Observing Patient Involvement in
Decision-Making 12 (OPTION12) scores. p-values were corrected for multiple
comparisons with the Benjamini-Hochberg method. Among 157 patients (34% female,
mean age 70 SD 10.8), clinicians on average spoke more words than patients
(1911 vs 773). The DL model without the stylebook strategy achieved a recall@1
of 0.227, while the fine-tuned BERTbase (110M) achieved the highest recall@1
with 0.640. The AbsMax (18.36 SE7.74 p=0.025) and Max CA (21.02 SE7.63 p=0.012)
scores generated with the DL without stylebook were associated with OPTION12.
The Max CA score generated with the fine-tuned BERTbase (110M) was associated
with the DCS score (-27.61 SE12.63 p=0.037). BERT model sizes did not have an
impact the association between CA scores and SDM. This study introduces an
automated, scalable methodology to measure SDM in patient-doctor conversations
through explainable CA scores, with potential to evaluate SDM strategies at
scale.

</details>


### [296] [CogniLoad: A Synthetic Natural Language Reasoning Benchmark With Tunable Length, Intrinsic Difficulty, and Distractor Density](https://arxiv.org/abs/2509.18458)
*Daniel Kaiser,Arnoldo Frigessi,Ali Ramezani-Kebrya,Benjamin Ricaud*

Main category: cs.CL

TL;DR: 引入CogniLoad基准评估大语言模型长上下文推理能力，揭示性能敏感性并指导模型开发


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型长上下文推理基准常混淆关键因素，需要更精确的失败分析工具

Method: 基于认知负荷理论引入CogniLoad基准，生成可独立调参的自然语言逻辑谜题

Result: 评估22个SotA推理大语言模型，发现任务长度是主要限制，揭示对内在复杂度的不同耐受性和对干扰物比例的U型响应

Conclusion: CogniLoad可系统控制认知负荷维度，是剖析大语言模型推理局限和指导未来模型开发的工具

Abstract: Current benchmarks for long-context reasoning in Large Language Models (LLMs)
often blur critical factors like intrinsic task complexity, distractor
interference, and task length. To enable more precise failure analysis, we
introduce CogniLoad, a novel synthetic benchmark grounded in Cognitive Load
Theory (CLT). CogniLoad generates natural-language logic puzzles with
independently tunable parameters that reflect CLT's core dimensions: intrinsic
difficulty ($d$) controls intrinsic load; distractor-to-signal ratio ($\rho$)
regulates extraneous load; and task length ($N$) serves as an operational proxy
for conditions demanding germane load. Evaluating 22 SotA reasoning LLMs,
CogniLoad reveals distinct performance sensitivities, identifying task length
as a dominant constraint and uncovering varied tolerances to intrinsic
complexity and U-shaped responses to distractor ratios. By offering systematic,
factorial control over these cognitive load dimensions, CogniLoad provides a
reproducible, scalable, and diagnostically rich tool for dissecting LLM
reasoning limitations and guiding future model development.

</details>


### [297] [LAWCAT: Efficient Distillation from Quadratic to Linear Attention with Convolution across Tokens for Long Context Modeling](https://arxiv.org/abs/2509.18467)
*Zeyu Liu,Souvik Kundu,Lianghao Jiang,Anni Li,Srikanth Ronanki,Sravan Bodapati,Gourav Datta,Peter A. Beerel*

Main category: cs.CL

TL;DR: 提出LAWCAT框架解决transformer计算复杂度问题，评估显示其有效扩展上下文窗口、减少预训练资源且有更快填充速度。


<details>
  <summary>Details</summary>
Motivation: transformer计算复杂度高，线性替代方案从头训练资源消耗大，需有效方法解决。

Method: 提出LAWCAT框架，集成因果Conv1D层和归一化门控线性注意力。

Result: 用1K序列蒸馏Mistral - 7B在22K tokens有超90%准确率；Llama3.2 - 1B LAWCAT变体在多任务表现好，预训练token少；LAWCAT在超8K tokens序列填充速度快于FlashAttention - 2。

Conclusion: LAWCAT为高性能、长上下文线性模型提供高效途径，减少对训练数据和计算资源依赖。

Abstract: Although transformer architectures have achieved state-of-the-art performance
across diverse domains, their quadratic computational complexity with respect
to sequence length remains a significant bottleneck, particularly for
latency-sensitive long-context applications. While recent linear-complexity
alternatives are increasingly powerful, effectively training them from scratch
is still resource-intensive. To overcome these limitations, we propose LAWCAT
(Linear Attention with Convolution Across Time), a novel linearization
framework designed to efficiently transfer the capabilities of pre-trained
transformers into a performant linear attention architecture. LAWCAT integrates
causal Conv1D layers to enhance local dependency modeling and employs
normalized gated linear attention to improve generalization across varying
context lengths. Our comprehensive evaluations demonstrate that, distilling
Mistral-7B with only 1K-length sequences yields over 90\% passkey retrieval
accuracy up to 22K tokens, significantly extending its effective context
window. Similarly, Llama3.2-1B LAWCAT variant achieves competitive performance
on S-NIAH 1\&2\&3 tasks (1K-8K context length) and BABILong benchmark
(QA2\&QA3, 0K-16K context length), requiring less than 0.1\% pre-training
tokens compared with pre-training models. Furthermore, LAWCAT exhibits faster
prefill speeds than FlashAttention-2 for sequences exceeding 8K tokens. LAWCAT
thus provides an efficient pathway to high-performance, long-context linear
models suitable for edge deployment, reducing reliance on extensive
long-sequence training data and computational resources.

</details>


### [298] [A Rhythm-Aware Phrase Insertion for Classical Arabic Poetry Composition](https://arxiv.org/abs/2509.18514)
*Mohamad Elzohbi,Richard Zhao*

Main category: cs.CL

TL;DR: 本文用ByT5模型提出在阿拉伯诗歌中插入短语以符合特定韵律的方法，实验显示模型能达到高韵律对齐且保持语义连贯，有创作应用潜力。


<details>
  <summary>Details</summary>
Motivation: 在阿拉伯诗歌中插入短语以符合特定韵律。

Method: 采用基于规则的字素到节拍转换，用条件去噪目标微调ByT5，采用课程学习策略，还探索跨语言迁移。

Result: 模型实现高韵律对齐，同时保持语义连贯。

Conclusion: 所提模型有用于创作古典阿拉伯诗歌的协同创作应用潜力。

Abstract: This paper presents a methodology for inserting phrases in Arabic poems to
conform to a specific rhythm using ByT5, a byte-level multilingual
transformer-based model. Our work discusses a rule-based grapheme-to-beat
transformation tailored for extracting the rhythm from fully diacritized Arabic
script. Our approach employs a conditional denoising objective to fine-tune
ByT5, where the model reconstructs masked words to match a target rhythm. We
adopt a curriculum learning strategy, pre-training on a general Arabic dataset
before fine-tuning on poetic dataset, and explore cross-lingual transfer from
English to Arabic. Experimental results demonstrate that our models achieve
high rhythmic alignment while maintaining semantic coherence. The proposed
model has the potential to be used in co-creative applications in the process
of composing classical Arabic poems.

</details>


### [299] [CCQA: Generating Question from Solution Can Improve Inference-Time Reasoning in SLMs](https://arxiv.org/abs/2509.18536)
*Jin Young Kim,Ji Won Yoon*

Main category: cs.CL

TL;DR: 提出适用于小语言模型的CCQA推理方法，实验验证其性能优于现有方法并建立新基线。


<details>
  <summary>Details</summary>
Motivation: 推理时间推理策略对大语言模型准确性有提升，但在小模型上效果不明，传统方法在小模型场景难以提升性能。

Method: 提出CCQA方法，从推理路径和答案生成问题，根据与原问题相似度选最终答案，用轻量级Flan - T5模型辅助问题生成。

Result: CCQA在八个模型的数学和常识推理基准测试中始终优于现有SOTA方法。

Conclusion: CCQA为小语言模型的高效推理建立了新的实用基线。

Abstract: Recently, inference-time reasoning strategies have further improved the
accuracy of large language models (LLMs), but their effectiveness on smaller
models remains unclear. Based on the observation that conventional approaches
often fail to improve performance in this context, we propose
\textbf{C}ycle-\textbf{C}onsistency in \textbf{Q}uestion \textbf{A}nswering
(CCQA), a novel reasoning method that can be effectively applied to SLMs.
Inspired by cycle consistency, CCQA generates a question from each reasoning
path and answer, evaluates each by its similarity to the original question, and
then selects the candidate solution with the highest similarity score as the
final response. Since conventional SLMs struggle to generate accurate questions
from their own reasoning paths and answers, we employ a lightweight Flan-T5
model specialized for question generation to support this process efficiently.
From the experimental results, it is verified that CCQA consistently
outperforms existing state-of-the-art (SOTA) methods across eight models on
mathematical and commonsense reasoning benchmarks. Furthermore, our method
establishes a new practical baseline for efficient reasoning in SLMs. Source
code can be found at https://github.com/scai-research/ccqa_official.

</details>


### [300] [TsqLoRA: Towards Sensitivity and Quality Low-Rank Adaptation for Efficient Fine-Tuning](https://arxiv.org/abs/2509.18585)
*Yu Chen,Yifei Han,Long Zhang,Yue Du,Bin Li*

Main category: cs.CL

TL;DR: 提出TsqLoRA方法结合数据质量驱动选择和敏感度感知低秩适配，提升微调效率和性能。


<details>
  <summary>Details</summary>
Motivation: 全量微调大模型计算和内存开销大，现有参数高效微调方法忽略不同层敏感度和训练数据重要性。

Method: 提出TsqLoRA方法，含质量感知采样机制选训练数据和动态秩分配模块根据层敏感度调整秩。

Result: TsqLoRA在多种NLP任务中提升微调效率，保持或提升性能。

Conclusion: TsqLoRA是一种有效的参数高效微调方法，可在资源受限环境中应用。

Abstract: Fine-tuning large pre-trained models for downstream tasks has become a
fundamental approach in natural language processing. Fully fine-tuning all
model parameters is computationally expensive and memory-intensive, especially
in resource-constrained environments. Existing parameter-efficient fine-tuning
methods reduce the number of trainable parameters but typically overlook the
varying sensitivity of different model layers and the importance of training
data. In this work, we propose TsqLoRA, a novel method that integrates
data-quality-driven selection with sensitivity-aware low-rank adaptation,
consisted of two main components: a quality-aware sampling mechanism for
selecting the most informative training data, and a dynamic rank allocation
module that adjusts the rank of each layer based on its sensitivity to
parameter updates. The experimental results demonstrate that TsqLoRA improves
fine-tuning efficiency while maintaining or even improving performance on a
variety of NLP tasks. Our code will be available at
https://github.com/Benjamin-Ricky/TsqLoRA.

</details>


### [301] [Dynamic Prompt Fusion for Multi-Task and Cross-Domain Adaptation in LLMs](https://arxiv.org/abs/2509.18113)
*Xin Hu,Yue Kang,Guanzi Yao,Tianze Kang,Mengjie Wang,Heyao Liu*

Main category: cs.CL

TL;DR: 本文针对大语言模型在多任务和跨领域设置下的泛化局限，提出带动态提示调度机制的统一多任务学习框架，实验证明其能提升模型稳定性和可迁移性。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在多任务和跨领域设置下的泛化局限问题。

Method: 引入带提示池和任务感知调度策略的统一多任务学习框架，利用任务嵌入和门控机制控制提示信号，构建任务间灵活共享路径，采用以联合多任务学习为中心的优化目标及调度权重自动学习策略。

Result: 通过敏感性实验，证实该机制在保持模型稳定性和增强可迁移性方面的优势，在多种语言理解和知识推理任务上显著提升性能。

Conclusion: 该提示调度方法在统一多任务建模和跨领域适应中具有适用性和有效性。

Abstract: This study addresses the generalization limitations commonly observed in
large language models under multi-task and cross-domain settings. Unlike prior
methods such as SPoT, which depends on fixed prompt templates, our study
introduces a unified multi-task learning framework with dynamic prompt
scheduling mechanism. By introducing a prompt pool and a task-aware scheduling
strategy, the method dynamically combines and aligns prompts for different
tasks. This enhances the model's ability to capture semantic differences across
tasks. During prompt fusion, the model uses task embeddings and a gating
mechanism to finely control the prompt signals. This ensures alignment between
prompt content and task-specific demands. At the same time, it builds flexible
sharing pathways across tasks. In addition, the proposed optimization objective
centers on joint multi-task learning. It incorporates an automatic learning
strategy for scheduling weights, which effectively mitigates task interference
and negative transfer. To evaluate the effectiveness of the method, a series of
sensitivity experiments were conducted. These experiments examined the impact
of prompt temperature parameters and task number variation. The results confirm
the advantages of the proposed mechanism in maintaining model stability and
enhancing transferability. Experimental findings show that the prompt
scheduling method significantly improves performance on a range of language
understanding and knowledge reasoning tasks. These results fully demonstrate
its applicability and effectiveness in unified multi-task modeling and
cross-domain adaptation.

</details>


### [302] [ZERA: Zero-init Instruction Evolving Refinement Agent - From Zero Instructions to Structured Prompts via Principle-based Optimization](https://arxiv.org/abs/2509.18158)
*Seungyoun Yi,Minsoo Khang,Sungrae Park*

Main category: cs.CL

TL;DR: 提出ZERA框架联合优化系统和用户提示，在多LLM和多数据集上实验效果好，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有APO方法存在仅关注用户提示、依赖非结构化反馈、样本量大和迭代周期长等问题，成本高且脆弱。

Method: 提出ZERA框架，用八个可泛化标准和自动推断权重对提示打分，基于结构化评判修改提示。

Result: 在五个LLM和九个不同数据集的推理、总结和代码生成任务上实验，持续优于强基线，消融研究突出各组件对有效提示构建的贡献。

Conclusion: ZERA框架能有效优化提示，实现快速收敛到高质量提示，且成本低。

Abstract: Automatic Prompt Optimization (APO) improves large language model (LLM)
performance by refining prompts for specific tasks. However, prior APO methods
typically focus only on user prompts, rely on unstructured feedback, and
require large sample sizes and long iteration cycles-making them costly and
brittle. We propose ZERA (Zero-init Instruction Evolving Refinement Agent), a
novel framework that jointly optimizes both system and user prompts through
principled, low-overhead refinement. ZERA scores prompts using eight
generalizable criteria with automatically inferred weights, and revises prompts
based on these structured critiques. This enables fast convergence to
high-quality prompts using minimal examples and short iteration cycles. We
evaluate ZERA across five LLMs and nine diverse datasets spanning reasoning,
summarization, and code generation tasks. Experimental results demonstrate
consistent improvements over strong baselines. Further ablation studies
highlight the contribution of each component to more effective prompt
construction. Our implementation including all prompts is publicly available at
https://github.com/younatics/zera-agent.

</details>


### [303] [MemOrb: A Plug-and-Play Verbal-Reinforcement Memory Layer for E-Commerce Customer Service](https://arxiv.org/abs/2509.18713)
*Yizhe Huang,Yang Liu,Ruiyu Zhao,Xiaolong Zhong,Xingming Yue,Ling Jiang*

Main category: cs.CL

TL;DR: 针对基于大语言模型的客服代理存在的问题，提出MemOrb，实验表明其能显著提升成功率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的客服代理存在跨会话遗忘、重复错误和缺乏持续自我改进机制等问题，在动态环境中不可靠，需要评估和解决。

Method: 强调任务成功率和一致性指标，提出轻量级即插即用的MemOrb语言强化记忆层，将多轮交互提炼为策略反思并存储于共享内存库以指导决策，无需微调。

Result: MemOrb显著提高了成功率和稳定性，多轮成功率最多提升63个百分点，在重复试验中表现更一致。

Conclusion: 结构化反思是增强客服场景中冻结大语言模型代理长期可靠性的有力机制。

Abstract: Large Language Model-based agents(LLM-based agents) are increasingly deployed
in customer service, yet they often forget across sessions, repeat errors, and
lack mechanisms for continual self-improvement. This makes them unreliable in
dynamic settings where stability and consistency are critical. To better
evaluate these properties, we emphasize two indicators: task success rate as a
measure of overall effectiveness, and consistency metrics such as Pass$^k$ to
capture reliability across multiple trials. To address the limitations of
existing approaches, we propose MemOrb, a lightweight and plug-and-play verbal
reinforcement memory layer that distills multi-turn interactions into compact
strategy reflections. These reflections are stored in a shared memory bank and
retrieved to guide decision-making, without requiring any fine-tuning.
Experiments show that MemOrb significantly improves both success rate and
stability, achieving up to a 63 percentage-point gain in multi-turn success
rate and delivering more consistent performance across repeated trials. Our
results demonstrate that structured reflection is a powerful mechanism for
enhancing long-term reliability of frozen LLM agents in customer service
scenarios.

</details>


### [304] [When Long Helps Short: How Context Length in Supervised Fine-tuning Affects Behavior of Large Language Models](https://arxiv.org/abs/2509.18762)
*Yingming Zheng,Hanqi Li,Kai Yu,Lu Chen*

Main category: cs.CL

TL;DR: 研究SFT数据长度对大语言模型短上下文任务表现的影响，发现长上下文SFT能提升短上下文性能，分析机制并提出混合训练可缓解偏差。


<details>
  <summary>Details</summary>
Motivation: 现实应用需长上下文窗口，持续预训练数据长度影响已被广泛研究，但SFT数据长度对模型的影响仍不明确。

Method: 系统研究SFT数据长度对大语言模型短上下文任务行为的影响，解耦分析MHA和FFN，研究其相互作用。

Result: 长上下文SFT能提升短上下文性能；长上下文SFT促进上下文知识，短上下文SFT倾向参数知识，单纯长上下文SFT并非最优。

Conclusion: 混合训练可缓解知识偏好偏差，为微调大语言模型提供可解释的指导。

Abstract: Large language models (LLMs) have achieved impressive performance across
natural language processing (NLP) tasks. As real-world applications
increasingly demand longer context windows, continued pretraining and
supervised fine-tuning (SFT) on long-context data has become a common approach.
While the effects of data length in continued pretraining have been extensively
studied, their implications for SFT remain unclear. In this work, we
systematically investigate how SFT data length influences LLM behavior on
short-context tasks. Counterintuitively, we find that long-context SFT improves
short-context performance, contrary to the commonly observed degradation from
long-context pretraining. To uncover the underlying mechanisms of this
phenomenon, we first decouple and analyze two key components, Multi-Head
Attention (MHA) and Feed-Forward Network (FFN), and show that both
independently benefit from long-context SFT. We further study their interaction
and reveal a knowledge preference bias: long-context SFT promotes contextual
knowledge, while short-context SFT favors parametric knowledge, making
exclusive reliance on long-context SFT suboptimal. Finally, we demonstrate that
hybrid training mitigates this bias, offering explainable guidance for
fine-tuning LLMs.

</details>


### [305] [Financial Risk Relation Identification through Dual-view Adaptation](https://arxiv.org/abs/2509.18775)
*Wei-Ning Chiu,Yu-Hsiang Wang,Andy Hsiao,Yu-Shiang Huang,Chuan-Ju Wang*

Main category: cs.CL

TL;DR: 提出用10 - K文件提取公司间风险关系的系统方法，优于基线。


<details>
  <summary>Details</summary>
Motivation: 传统识别公司间风险关系的方法主观、劳动密集且难扩展，需要更有效的方法。

Method: 以10 - K文件为数据源，利用自然语言处理，通过无监督微调捕捉风险连接，开发特定领域金融编码器并引入量化风险关系得分。

Result: 在多个评估设置中，该方法优于强基线。

Conclusion: 所提系统方法能有效提取公司间风险关系，在相关应用中具有优势。

Abstract: A multitude of interconnected risk events -- ranging from regulatory changes
to geopolitical tensions -- can trigger ripple effects across firms.
Identifying inter-firm risk relations is thus crucial for applications like
portfolio management and investment strategy. Traditionally, such assessments
rely on expert judgment and manual analysis, which are, however, subjective,
labor-intensive, and difficult to scale. To address this, we propose a
systematic method for extracting inter-firm risk relations using Form 10-K
filings -- authoritative, standardized financial documents -- as our data
source. Leveraging recent advances in natural language processing, our approach
captures implicit and abstract risk connections through unsupervised
fine-tuning based on chronological and lexical patterns in the filings. This
enables the development of a domain-specific financial encoder with a deeper
contextual understanding and introduces a quantitative risk relation score for
transparency, interpretable analysis. Extensive experiments demonstrate that
our method outperforms strong baselines across multiple evaluation settings.

</details>


### [306] [AECBench: A Hierarchical Benchmark for Knowledge Evaluation of Large Language Models in the AEC Field](https://arxiv.org/abs/2509.18776)
*Chen Liang,Zhaoqi Huang,Haofen Wang,Fu Chai,Chunying Yu,Huanhuan Wei,Zhengjie Liu,Yanpeng Li,Hongjun Wang,Ruifeng Luo,Xianzhong Zhao*

Main category: cs.CL

TL;DR: 本文构建 AECBench 基准评估大语言模型在建筑、工程和施工领域的表现，评估 9 个模型后揭示其性能不足，为后续研究奠基。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在建筑、工程和施工领域应用增多，但在该专业且对安全要求高的领域的鲁棒性和可靠性有待评估。

Method: 建立 AECBench 基准，定义 23 个代表性任务，构建 4800 个问题的数据集，引入大语言模型评判方法。

Result: 评估 9 个大语言模型后发现，在五个认知水平上性能下降，基础任务表现尚可，但在解读规范、复杂推理计算和生成专业文档方面表现不佳。

Conclusion: 本研究为大语言模型可靠集成到安全关键工程实践的后续研究和开发奠定基础。

Abstract: Large language models (LLMs), as a novel information technology, are seeing
increasing adoption in the Architecture, Engineering, and Construction (AEC)
field. They have shown their potential to streamline processes throughout the
building lifecycle. However, the robustness and reliability of LLMs in such a
specialized and safety-critical domain remain to be evaluated. To address this
challenge, this paper establishes AECBench, a comprehensive benchmark designed
to quantify the strengths and limitations of current LLMs in the AEC domain.
The benchmark defines 23 representative tasks within a five-level
cognition-oriented evaluation framework encompassing Knowledge Memorization,
Understanding, Reasoning, Calculation, and Application. These tasks were
derived from authentic AEC practice, with scope ranging from codes retrieval to
specialized documents generation. Subsequently, a 4,800-question dataset
encompassing diverse formats, including open-ended questions, was crafted
primarily by engineers and validated through a two-round expert review.
Furthermore, an LLM-as-a-Judge approach was introduced to provide a scalable
and consistent methodology for evaluating complex, long-form responses
leveraging expert-derived rubrics. Through the evaluation of nine LLMs, a clear
performance decline across five cognitive levels was revealed. Despite
demonstrating proficiency in foundational tasks at the Knowledge Memorization
and Understanding levels, the models showed significant performance deficits,
particularly in interpreting knowledge from tables in building codes, executing
complex reasoning and calculation, and generating domain-specific documents.
Consequently, this study lays the groundwork for future research and
development aimed at the robust and reliable integration of LLMs into
safety-critical engineering practices.

</details>


### [307] [Diversity Boosts AI-Generated Text Detection](https://arxiv.org/abs/2509.18880)
*Advik Raj Basani,Pin-Yu Chen*

Main category: cs.CL

TL;DR: 提出DivEye检测框架，用基于意外性的特征检测AI生成文本，表现优异且具可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成文本检测器在高质量生成文本上表现不佳且缺乏可解释性，为解决该问题开展研究。

Method: 提出DivEye框架，利用基于意外性的特征捕捉文本中不可预测性的波动，通过一组可解释的统计特征捕捉信号。

Result: 比现有零样本检测器性能最高提升33.2%，与微调基线在多基准测试中表现相当，对释义和对抗攻击有鲁棒性，跨领域和模型泛化能力好，作为辅助信号可使现有检测器性能最高提升18.7%。

Conclusion: DivEye不仅能有效检测AI生成文本，还能提供可解释的检测原因，指出节奏不可预测性是强大且未充分探索的检测信号。

Abstract: Detecting AI-generated text is an increasing necessity to combat misuse of
LLMs in education, business compliance, journalism, and social media, where
synthetic fluency can mask misinformation or deception. While prior detectors
often rely on token-level likelihoods or opaque black-box classifiers, these
approaches struggle against high-quality generations and offer little
interpretability. In this work, we propose DivEye, a novel detection framework
that captures how unpredictability fluctuates across a text using
surprisal-based features. Motivated by the observation that human-authored text
exhibits richer variability in lexical and structural unpredictability than LLM
outputs, DivEye captures this signal through a set of interpretable statistical
features. Our method outperforms existing zero-shot detectors by up to 33.2%
and achieves competitive performance with fine-tuned baselines across multiple
benchmarks. DivEye is robust to paraphrasing and adversarial attacks,
generalizes well across domains and models, and improves the performance of
existing detectors by up to 18.7% when used as an auxiliary signal. Beyond
detection, DivEye provides interpretable insights into why a text is flagged,
pointing to rhythmic unpredictability as a powerful and underexplored signal
for LLM detection.

</details>


### [308] [Anecdoctoring: Automated Red-Teaming Across Language and Place](https://arxiv.org/abs/2509.19143)
*Alejandro Cuevas,Saloni Dash,Bharat Kumar Nayak,Dan Vann,Madeleine I. G. Daepp*

Main category: cs.CL

TL;DR: 研究提出 'anecdoctoring' 红队评估方法以解决生成式AI虚假信息问题，该方法跨语言文化且优于少样本提示。


<details>
  <summary>Details</summary>
Motivation: 生成式AI滥用导致的虚假信息是重大风险，现有红队评估数据集以美国和英语为中心，缺乏跨语言文化的评估。

Method: 提出 'anecdoctoring' 方法，从多语言多地区事实核查网站收集错误信息声明，聚类成更广泛的叙述，用知识图谱表征，增强攻击型大语言模型。

Result: 该方法比少样本提示有更高的攻击成功率和可解释性优势。

Conclusion: 强调需要基于现实世界对抗性滥用、可全球扩展的虚假信息缓解措施。

Abstract: Disinformation is among the top risks of generative artificial intelligence
(AI) misuse. Global adoption of generative AI necessitates red-teaming
evaluations (i.e., systematic adversarial probing) that are robust across
diverse languages and cultures, but red-teaming datasets are commonly US- and
English-centric. To address this gap, we propose "anecdoctoring", a novel
red-teaming approach that automatically generates adversarial prompts across
languages and cultures. We collect misinformation claims from fact-checking
websites in three languages (English, Spanish, and Hindi) and two geographies
(US and India). We then cluster individual claims into broader narratives and
characterize the resulting clusters with knowledge graphs, with which we
augment an attacker LLM. Our method produces higher attack success rates and
offers interpretability benefits relative to few-shot prompting. Results
underscore the need for disinformation mitigations that scale globally and are
grounded in real-world adversarial misuse.

</details>


### [309] [Soft Tokens, Hard Truths](https://arxiv.org/abs/2509.19170)
*Natasha Butt,Ariel Kwiatkowski,Ismail Labiad,Julia Kempe,Yann Ollivier*

Main category: cs.CL

TL;DR: 本文提出通过强化学习学习连续思维链的可扩展方法，在数学推理基准测试中展现优势，且能更好保留基础模型在域外任务的预测。


<details>
  <summary>Details</summary>
Motivation: 连续思维链有理论优势，但实际应用受训练困难限制，此前方法存在不足，因此需新方法。

Method: 引入通过强化学习学习连续思维链的可扩展方法，使用“软”令牌，在输入嵌入添加噪声进行探索。

Result: 在数学推理基准测试中，连续思维链训练在pass@32上超越离散令牌，展现更多样性；最佳场景是用连续令牌训练、离散令牌推理；连续思维链强化学习训练能更好保留基础模型在域外任务的预测。

Conclusion: 提出的方法有效，“软”模型可按标准方式部署，对基础模型调整更温和。

Abstract: The use of continuous instead of discrete tokens during the Chain-of-Thought
(CoT) phase of reasoning LLMs has garnered attention recently, based on the
intuition that a continuous mixture of discrete tokens could simulate a
superposition of several reasoning paths simultaneously. Theoretical results
have formally proven that continuous tokens have much greater expressivity and
can solve specific problems more efficiently. However, practical use of
continuous tokens has been limited by strong training difficulties: previous
works either just use continuous tokens at inference time on a pre-trained
discrete-token model, or must distill the continuous CoT from ground-truth
discrete CoTs and face computational costs that limit the CoT to very few
tokens.
  This is the first work introducing a scalable method to learn continuous CoTs
via reinforcement learning (RL), without distilling from reference discrete
CoTs. We use "soft" tokens: mixtures of tokens together with noise on the input
embedding to provide RL exploration. Computational overhead is minimal,
enabling us to learn continuous CoTs with hundreds of tokens. On math reasoning
benchmarks with Llama and Qwen models up to 8B, training with continuous CoTs
match discrete-token CoTs for pass@1 and surpass them for pass@32, showing
greater CoT diversity. In systematic comparisons, the best-performing scenario
is to train with continuous CoT tokens then use discrete tokens for inference,
meaning the "soft" models can be deployed in a standard way. Finally, we show
continuous CoT RL training better preserves the predictions of the base model
on out-of-domain tasks, thus providing a softer touch to the base model.

</details>


### [310] [Steering Multimodal Large Language Models Decoding for Context-Aware Safety](https://arxiv.org/abs/2509.19212)
*Zheyuan Liu,Zhangchen Xu,Guangyao Dou,Xiangchi Yuan,Zhaoxuan Tan,Radha Poovendran,Meng Jiang*

Main category: cs.CL

TL;DR: 论文提出轻量级且与模型无关的安全解码框架SafeCoDe，实验表明其能改善多模态大语言模型上下文敏感拒绝行为并保留模型有用性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在上下文感知安全决策能力有限，现有方法无法平衡过敏感和欠敏感问题，存在安全对齐差距。

Method: 提出SafeCoDe框架，包含通过对比真实和高斯噪声图像突出对视觉上下文敏感标记的对比解码机制，以及将场景级推理与标记级调整结合的全局感知标记调制策略。

Result: 在不同多模态大语言模型架构和安全基准测试中，SafeCoDe持续改善上下文敏感拒绝行为并保留模型有用性。

Conclusion: SafeCoDe能有效解决多模态大语言模型安全决策问题，提升安全对齐性能。

Abstract: Multimodal Large Language Models (MLLMs) are increasingly deployed in
real-world applications, yet their ability to make context-aware safety
decisions remains limited. Existing methods often fail to balance
oversensitivity (unjustified refusals of benign queries) and undersensitivity
(missed detection of visually grounded risks), leaving a persistent gap in
safety alignment. To address this issue, we introduce Safety-aware Contrastive
Decoding (SafeCoDe), a lightweight and model-agnostic decoding framework that
dynamically adjusts token generation based on multimodal context. SafeCoDe
operates in two stages: (1) a contrastive decoding mechanism that highlights
tokens sensitive to visual context by contrasting real and Gaussian-noised
images, and (2) a global-aware token modulation strategy that integrates
scene-level reasoning with token-level adjustment to adapt refusals according
to the predicted safety verdict. Extensive experiments across diverse MLLM
architectures and safety benchmarks, covering undersensitivity,
oversensitivity, and general safety evaluations, show that SafeCoDe
consistently improves context-sensitive refusal behaviors while preserving
model helpfulness.

</details>


### [311] [Systematic Comparative Analysis of Large Pretrained Language Models on Contextualized Medication Event Extraction](https://arxiv.org/abs/2509.19224)
*Tariq Abdul-Quddoos,Xishuang Dong,Lijun Qian*

Main category: cs.CL

TL;DR: 对预训练注意力模型在电子健康记录信息提取任务上进行比较分析，结果显示临床数据预训练模型检测效果好，Bert Base分类效果最佳。


<details>
  <summary>Details</summary>
Motivation: 比较不同预训练注意力模型在电子健康记录信息提取任务上的性能，以找到更有效的解决方案。

Method: 选取多种预训练注意力模型，针对哈佛医学院2022年n2c2挑战赛Track 1任务，使用CMED数据集，对模型微调后执行多种任务，详细处理EHRs以适配模型，用特定脚本和指标进行性能分析。

Result: 临床数据预训练模型在检测药物和药物事件上更有效，Bert Base在药物事件上下文分类上最有效。

Conclusion: 不同预训练模型在电子健康记录信息提取不同任务中有不同的有效性表现。

Abstract: Attention-based models have become the leading approach in modeling medical
language for Natural Language Processing (NLP) in clinical notes. These models
outperform traditional techniques by effectively capturing contextual rep-
resentations of language. In this research a comparative analysis is done
amongst pre- trained attention based models namely Bert Base, BioBert, two
variations of Bio+Clinical Bert, RoBerta, and Clinical Long- former on task
related to Electronic Health Record (EHR) information extraction. The tasks
from Track 1 of Harvard Medical School's 2022 National Clinical NLP Challenges
(n2c2) are considered for this comparison, with the Contextualized Medication
Event Dataset (CMED) given for these task. CMED is a dataset of unstructured
EHRs and annotated notes that contain task relevant information about the EHRs.
The goal of the challenge is to develop effective solutions for extracting
contextual information related to patient medication events from EHRs using
data driven methods. Each pre-trained model is fine-tuned and applied on CMED
to perform medication extraction, medical event detection, and
multi-dimensional medication event context classification. Pro- cessing methods
are also detailed for breaking down EHRs for compatibility with the applied
models. Performance analysis has been carried out using a script based on
constructing medical terms from the evaluation portion of CMED with metrics
including recall, precision, and F1-Score. The results demonstrate that models
pre-trained on clinical data are more effective in detecting medication and
medication events, but Bert Base, pre- trained on general domain data showed to
be the most effective for classifying the context of events related to
medications.

</details>


### [312] [Reinforcement Learning on Pre-Training Data](https://arxiv.org/abs/2509.19249)
*Siheng Li,Kejiao Li,Zenan Xu,Guanhua Huang,Evander Yang,Kun Li,Haoyuan Wu,Jiajia Wu,Zihao Zheng,Chenchen Zhang,Kun Shi,Kyrierl Deng,Qi Yi,Ruibin Xiong,Tingqiang Xu,Yuhao Jiang,Jianfeng Yan,Yuyuan Zeng,Guanghui Xu,Jinbao Xue,Zhijiang Xu,Zheng Fang,Shuai Li,Qibin Liu,Xiaoxue Li,Zhuoyu Li,Yangyu Tao,Fei Gao,Cheng Jiang,Bo Chao Wang,Kai Liu,Jianchen Zhu,Wai Lam,Wayyt Wang,Bo Zhou,Di Wang*

Main category: cs.CL

TL;DR: 针对大语言模型高质量文本数据有限问题，提出RLPT训练范式，实验验证其有效性及扩展潜力。


<details>
  <summary>Details</summary>
Motivation: 解决计算资源指数增长与高质量文本数据有限增长之间的差距对传统大语言模型扩展方法的限制。

Method: 引入RLPT，让策略通过强化学习自主探索预训练数据中的有意义轨迹，采用下一段推理目标，直接从预训练数据获取奖励信号。

Result: 在多个通用领域和数学推理基准测试中验证了RLPT的有效性，应用于Qwen3 - 4B - Base时在多个测试中有绝对提升，结果还显示出良好的扩展行为。

Conclusion: RLPT为大语言模型提供了坚实基础，扩展了推理边界并提升了RLVR性能。

Abstract: The growing disparity between the exponential scaling of computational
resources and the finite growth of high-quality text data now constrains
conventional scaling approaches for large language models (LLMs). To address
this challenge, we introduce Reinforcement Learning on Pre-Training data
(RLPT), a new training-time scaling paradigm for optimizing LLMs. In contrast
to prior approaches that scale training primarily through supervised learning,
RLPT enables the policy to autonomously explore meaningful trajectories to
learn from pre-training data and improve its capability through reinforcement
learning (RL). While existing RL strategies such as reinforcement learning from
human feedback (RLHF) and reinforcement learning with verifiable rewards (RLVR)
rely on human annotation for reward construction, RLPT eliminates this
dependency by deriving reward signals directly from pre-training data.
Specifically, it adopts a next-segment reasoning objective, rewarding the
policy for accurately predicting subsequent text segments conditioned on the
preceding context. This formulation allows RL to be scaled on pre-training
data, encouraging the exploration of richer trajectories across broader
contexts and thereby fostering more generalizable reasoning skills. Extensive
experiments on both general-domain and mathematical reasoning benchmarks across
multiple models validate the effectiveness of RLPT. For example, when applied
to Qwen3-4B-Base, RLPT yields absolute improvements of $3.0$, $5.1$, $8.1$,
$6.0$, $6.6$, and $5.3$ on MMLU, MMLU-Pro, GPQA-Diamond, KOR-Bench, AIME24, and
AIME25, respectively. The results further demonstrate favorable scaling
behavior, suggesting strong potential for continued gains with more compute. In
addition, RLPT provides a solid foundation, extending the reasoning boundaries
of LLMs and enhancing RLVR performance.

</details>


### [313] [SloPalSpeech: A 2,8000-Hour Slovak Speech Corpus from Parliamentary Data](https://arxiv.org/abs/2509.19270)
*Erik Božík,Marek Šuppa*

Main category: cs.CL

TL;DR: 本文引入斯洛伐克语语音数据集SloPalSpeech，处理后微调Whisper模型降低WER，公开数据集和模型。


<details>
  <summary>Details</summary>
Motivation: 解决斯洛伐克等低资源语言自动语音识别中训练数据稀缺的问题。

Method: 创建含2806小时议会语音的SloPalSpeech数据集，开发处理流程生成音频 - 文本对，微调多个OpenAI Whisper模型。

Result: 在标准斯洛伐克语基准测试中显著降低WER，如微调后的Whisper - small模型WER最多降70%。

Conclusion: 公开SloPalSpeech数据集、转录文本和微调模型，以促进低资源语音识别研究。

Abstract: Automatic Speech Recognition (ASR) for low-resource languages like Slovak is
hindered by the scarcity of training data. To address this, we introduce
SloPalSpeech, a new, large-scale Slovak ASR dataset containing 2,806 hours of
speech from parliamentary proceedings. We developed a robust processing
pipeline to align and segment long-form recordings into clean, 30-second
audio-transcript pairs suitable for model training. We use this dataset to
fine-tune several OpenAI Whisper models (small, medium, large-v3, and
large-v3-turbo), achieving significant Word Error Rate (WER) reductions on
standard Slovak benchmarks like Common Voice and FLEURS. For instance, the
fine-tuned Whisper-small model's WER dropped by up to 70\%, approaching the
baseline performance of the much larger Whisper-large-v3 model. To foster
future research in low-resource speech recognition, we publicly release the
complete SloPalSpeech dataset, the fully segmented transcripts (60 million
words), and all our fine-tuned models.

</details>


### [314] [WolBanking77: Wolof Banking Speech Intent Classification Dataset](https://arxiv.org/abs/2509.19271)
*Abdou Karim Kandji,Frédéric Precioso,Cheikh Ba,Samba Ndiaye,Augustin Ndione*

Main category: cs.CL

TL;DR: 文章指出以往意图分类模型研究多聚焦高资源语言数据集，存在低资源语言和高文盲率地区的研究空白，为此发布Wolof意图分类数据集WolBanking77并进行实验，结果有前景，还提供分析、报告指标，计划共享维护数据集及开源代码。


<details>
  <summary>Details</summary>
Motivation: 解决以往意图分类模型研究在低资源语言和高文盲率地区存在的研究空白问题，如塞内加尔的Wolof语。

Method: 发布Wolof意图分类数据集WolBanking77，在多种基线模型上进行实验，包括文本和语音的先进模型。

Result: 在当前数据集上实验结果很有前景，报告了NLP和ASR模型在WolBanking77数据集上的基线f1分数和字错误率指标，并进行了模型间比较。

Conclusion: 该数据集有应用潜力，后续计划进行数据集维护、更新和开源代码。

Abstract: Intent classification models have made a lot of progress in recent years.
However, previous studies primarily focus on high-resource languages datasets,
which results in a gap for low-resource languages and for regions with a high
rate of illiterate people where languages are more spoken than read or written.
This is the case in Senegal, for example, where Wolof is spoken by around 90\%
of the population, with an illiteracy rate of 42\% for the country. Wolof is
actually spoken by more than 10 million people in West African region. To
tackle such limitations, we release a Wolof Intent Classification Dataset
(WolBanking77), for academic research in intent classification. WolBanking77
currently contains 9,791 text sentences in the banking domain and more than 4
hours of spoken sentences. Experiments on various baselines are conducted in
this work, including text and voice state-of-the-art models. The results are
very promising on this current dataset. This paper also provides detailed
analyses of the contents of the data. We report baseline f1-score and word
error rate metrics respectively on NLP and ASR models trained on WolBanking77
dataset and also comparisons between models. We plan to share and conduct
dataset maintenance, updates and to release open-source code.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [315] [Discovering strategies for coastal resilience with AI-based prediction and optimization](https://arxiv.org/abs/2509.19263)
*Jared Markowitz,Alexander New,Jennifer Sleeman,Chace Ashcraft,Jay Brett,Gary Collins,Stella In,Nathaniel Winstead*

Main category: physics.ao-ph

TL;DR: 使用AI驱动方法优化干预方案以提升沿海洪水抵御能力，应用于佛罗里达Tyndall空军基地附近，预测可节省数十亿美元风暴损失。


<details>
  <summary>Details</summary>
Motivation: 热带风暴破坏性大，开发能识别有效减轻风暴影响干预措施的预测模型，以减少损失。

Method: 结合三种AI模型，结合风暴潮场数据驱动生成、干预影响代理建模和解决连续武装强盗问题，优化干预类型、地点和规模选择。

Result: 应用该方法优化佛罗里达Tyndall空军基地附近海堤和牡蛎礁干预选择，预测干预优化可节省数十亿风暴损失，远超贪婪或非最优解决方案。

Conclusion: AI驱动的干预方案优化方法在减轻风暴影响、降低损失方面有巨大潜力。

Abstract: Tropical storms cause extensive property damage and loss of life, making them
one of the most destructive types of natural hazards. The development of
predictive models that identify interventions effective at mitigating storm
impacts has considerable potential to reduce these adverse outcomes. In this
study, we use an artificial intelligence (AI)-driven approach for optimizing
intervention schemes that improve resilience to coastal flooding. We combine
three different AI models to optimize the selection of intervention types,
sites, and scales in order to minimize the expected cost of flooding damage in
a given region, including the cost of installing and maintaining interventions.
Our approach combines data-driven generation of storm surge fields, surrogate
modeling of intervention impacts, and the solving of a continuous-armed bandit
problem. We applied this methodology to optimize the selection of sea wall and
oyster reef interventions near Tyndall Air Force Base (AFB) in Florida, an area
that was catastrophically impacted by Hurricane Michael. Our analysis predicts
that intervention optimization could be used to potentially save billions of
dollars in storm damage, far outpacing greedy or non-optimal solutions.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [316] [Investigating Traffic Accident Detection Using Multimodal Large Language Models](https://arxiv.org/abs/2509.19096)
*Ilhan Skender,Kailin Tong,Selim Solmaz,Daniel Watzenig*

Main category: cs.CV

TL;DR: 研究用多模态大语言模型（MLLMs）通过基础设施摄像头图像检测和描述交通事故，评估模型，结合视觉分析技术，Pixtral表现最佳，证明MLLMs与视觉分析结合在交通监测有潜力。


<details>
  <summary>Details</summary>
Motivation: 交通安全是全球关键问题，及时准确事故检测很重要，基础设施视觉传感器可用于实时监测，研究MLLMs零样本能力以减少对大量标注数据集依赖。

Method: 使用CARLA模拟的DeepAccident数据集评估MLLMs；比较Gemini 1.5和2.0、Gemma 3和Pixtral模型在事故识别和描述能力；将YOLO、Deep SORT和SAM集成到增强提示中提高模型准确性和可解释性。

Result: Pixtral表现最佳，F1分数0.71，召回率83%；Gemini模型增强提示后精度提升但F1和召回率下降；Gemma 3性能最平衡，指标波动小。

Conclusion: MLLMs与先进视觉分析技术集成有很大潜力，可提高其在现实自动交通监测系统的适用性。

Abstract: Traffic safety remains a critical global concern, with timely and accurate
accident detection essential for hazard reduction and rapid emergency response.
Infrastructure-based vision sensors offer scalable and efficient solutions for
continuous real-time monitoring, facilitating automated detection of acci-
dents directly from captured images. This research investigates the zero-shot
capabilities of multimodal large language models (MLLMs) for detecting and
describing traffic accidents using images from infrastructure cameras, thus
minimizing reliance on extensive labeled datasets. Main contributions include:
(1) Evaluation of MLLMs using the simulated DeepAccident dataset from CARLA,
explicitly addressing the scarcity of diverse, realistic, infrastructure-based
accident data through controlled simulations; (2) Comparative performance
analysis between Gemini 1.5 and 2.0, Gemma 3 and Pixtral models in acci- dent
identification and descriptive capabilities without prior fine-tuning; and (3)
Integration of advanced visual analytics, specifically YOLO for object
detection, Deep SORT for multi- object tracking, and Segment Anything (SAM) for
instance segmentation, into enhanced prompts to improve model accuracy and
explainability. Key numerical results show Pixtral as the top performer with an
F1-score of 0.71 and 83% recall, while Gemini models gained precision with
enhanced prompts (e.g., Gemini 1.5 rose to 90%) but suffered notable F1 and
recall losses. Gemma 3 offered the most balanced performance with minimal
metric fluctuation. These findings demonstrate the substantial potential of
integrating MLLMs with advanced visual analytics techniques, enhancing their
applicability in real-world automated traffic monitoring systems.

</details>


### [317] [Hyperbolic Coarse-to-Fine Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2509.18504)
*Jiaxin Dai,Xiang Xiang*

Main category: cs.CV

TL;DR: 论文聚焦C2FSCIL任务，将特征提取器嵌入双曲空间，引入双曲对比损失等方法，实验表明能有效提升粗、细类准确率。


<details>
  <summary>Details</summary>
Motivation: 双曲空间对层次数据的表示能力优于欧氏空间，为更好解释‘粗到细’范式并提升C2FSCIL任务性能。

Method: 采用Knowe方法，将特征提取器嵌入双曲空间的庞加莱球模型，引入双曲对比损失和全连接层，在双曲空间实现最大熵分布生成增强特征。

Result: 在C2FSCIL基准测试中，该方法有效提高了粗、细类的准确率。

Conclusion: 将特征提取器嵌入双曲空间的方法能有效提升C2FSCIL任务性能。

Abstract: In the field of machine learning, hyperbolic space demonstrates superior
representation capabilities for hierarchical data compared to conventional
Euclidean space. This work focuses on the Coarse-To-Fine Few-Shot
Class-Incremental Learning (C2FSCIL) task. Our study follows the Knowe
approach, which contrastively learns coarse class labels and subsequently
normalizes and freezes the classifier weights of learned fine classes in the
embedding space. To better interpret the "coarse-to-fine" paradigm, we propose
embedding the feature extractor into hyperbolic space. Specifically, we employ
the Poincar\'e ball model of hyperbolic space, enabling the feature extractor
to transform input images into feature vectors within the Poincar\'e ball
instead of Euclidean space. We further introduce hyperbolic contrastive loss
and hyperbolic fully-connected layers to facilitate model optimization and
classification in hyperbolic space. Additionally, to enhance performance under
few-shot conditions, we implement maximum entropy distribution in hyperbolic
space to estimate the probability distribution of fine-class feature vectors.
This allows generation of augmented features from the distribution to mitigate
overfitting during training with limited samples. Experiments on C2FSCIL
benchmarks show that our method effectively improves both coarse and fine class
accuracies.

</details>


### [318] [A Framework for Generating Artificial Datasets to Validate Absolute and Relative Position Concepts](https://arxiv.org/abs/2509.18177)
*George Corrêa de Araújo,Helena de Almeida Maia,Helio Pedrini*

Main category: cs.CV

TL;DR: 提出Scrapbook框架生成数据集探测AI模型概念理解，实验发现当代模型在位置信息理解等方面有不足，框架可用于评估和提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 在处理更复杂任务前，验证AI模型对基本概念的理解。

Method: 提出Scrapbook框架，生成包含大量关于单个概念问题和广泛语言变体的数据集。

Result: 当代模型在识别和枚举对象方面表现良好，但在理解位置信息和处理有额外约束的问题上存在挑战，如MobileVLM - V2有答案分歧和错误答案，其他模型有肯定回答偏差且在几何形状和位置信息问题上有困难。

Conclusion: Scrapbook框架是生成多样全面数据集的有价值工具，可用于系统评估和提升AI模型性能。

Abstract: In this paper, we present the Scrapbook framework, a novel methodology
designed to generate extensive datasets for probing the learned concepts of
artificial intelligence (AI) models. The framework focuses on fundamental
concepts such as object recognition, absolute and relative positions, and
attribute identification. By generating datasets with a large number of
questions about individual concepts and a wide linguistic variation, the
Scrapbook framework aims to validate the model's understanding of these basic
elements before tackling more complex tasks. Our experimental findings reveal
that, while contemporary models demonstrate proficiency in recognizing and
enumerating objects, they encounter challenges in comprehending positional
information and addressing inquiries with additional constraints. Specifically,
the MobileVLM-V2 model showed significant answer disagreements and plausible
wrong answers, while other models exhibited a bias toward affirmative answers
and struggled with questions involving geometric shapes and positional
information, indicating areas for improvement in understanding and consistency.
The proposed framework offers a valuable instrument for generating diverse and
comprehensive datasets, which can be utilized to systematically assess and
enhance the performance of AI models.

</details>


### [319] [The Describe-Then-Generate Bottleneck: How VLM Descriptions Alter Image Generation Outcomes](https://arxiv.org/abs/2509.18179)
*Sai Varun Kodathala,Rakesh Vunnam*

Main category: cs.CV

TL;DR: 分析视觉 - 语言 - 视觉管道中描述 - 生成瓶颈的信息损失，发现存在显著感知和结构信息损失。


<details>
  <summary>Details</summary>
Motivation: 随着多模态AI系统在创意工作流程中集成度增加，需理解视觉 - 语言 - 视觉管道中的信息损失以评估系统局限性，且视觉内容经文本中介的退化程度量化不足。

Method: 通过描述 - 生成管道生成150对图像，应用LPIPS、SSIM和颜色距离等现有指标从感知、结构和色彩维度测量信息保留情况。

Result: 99.3%的样本有显著感知退化，91.5%有显著结构信息损失。

Conclusion: 描述 - 生成瓶颈是当代多模态系统中可测量且持续存在的限制。

Abstract: With the increasing integration of multimodal AI systems in creative
workflows, understanding information loss in vision-language-vision pipelines
has become important for evaluating system limitations. However, the
degradation that occurs when visual content passes through textual
intermediation remains poorly quantified. In this work, we provide empirical
analysis of the describe-then-generate bottleneck, where natural language
serves as an intermediate representation for visual information. We generated
150 image pairs through the describe-then-generate pipeline and applied
existing metrics (LPIPS, SSIM, and color distance) to measure information
preservation across perceptual, structural, and chromatic dimensions. Our
evaluation reveals that 99.3% of samples exhibit substantial perceptual
degradation and 91.5% demonstrate significant structural information loss,
providing empirical evidence that the describe-then-generate bottleneck
represents a measurable and consistent limitation in contemporary multimodal
systems.

</details>


### [320] [VLA-LPAF: Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation](https://arxiv.org/abs/2509.18183)
*Jinyue Bian,Zhaoxing Zhang,Zhengyu Liang,Shiwei Zheng,Shengtao Zhang,Rong Shen,Chen Yang,Anzhou Hou*

Main category: cs.CV

TL;DR: 文章指出VLA模型因视角异质性限制泛化性，提出轻量级模块VLA - LPAF，构建RoboFlamingo - LPAF，实验显示其任务成功率有显著提升。


<details>
  <summary>Details</summary>
Motivation: VLA模型视觉观察的视角异质性限制了其泛化性，需要提升视角适应性。

Method: 提出仅使用2D数据的轻量级模块VLA - LPAF，用单视图图像微调并在潜在空间融合多视图观察，将其与VLA模型RoboFlamingo结合构建RoboFlamingo - LPAF。

Result: RoboFlamingo - LPAF在CALVIN平均任务成功率提高约8%，在LIBERO提高15%，在自定义模拟基准提高30%，并在现实任务中展现出视角自适应特性。

Conclusion: 所提出的VLA - LPAF模块和RoboFlamingo - LPAF框架能有效解决VLA模型视角异质性问题，提升任务成功率和视角适应性。

Abstract: The Visual-Language-Action (VLA) models can follow text instructions
according to visual observations of the surrounding environment. This ability
to map multimodal inputs to actions is derived from the training of the VLA
model on extensive standard demonstrations. These visual observations captured
by third-personal global and in-wrist local cameras are inevitably varied in
number and perspective across different environments, resulting in significant
differences in the visual features. This perspective heterogeneity constrains
the generality of VLA models. In light of this, we first propose the
lightweight module VLA-LPAF to foster the perspective adaptivity of VLA models
using only 2D data. VLA-LPAF is finetuned using images from a single view and
fuses other multiview observations in the latent space, which effectively and
efficiently bridge the gap caused by perspective inconsistency. We instantiate
our VLA-LPAF framework with the VLA model RoboFlamingo to construct
RoboFlamingo-LPAF. Experiments show that RoboFlamingo-LPAF averagely achieves
around 8% task success rate improvement on CALVIN, 15% on LIBERO, and 30% on a
customized simulation benchmark. We also demonstrate the developed viewadaptive
characteristics of the proposed RoboFlamingo-LPAF through real-world tasks.

</details>


### [321] [Visionerves: Automatic and Reproducible Hybrid AI for Peripheral Nervous System Recognition Applied to Endometriosis Cases](https://arxiv.org/abs/2509.18185)
*Giammarco La Barbera,Enzo Bonnot,Thomas Isla,Juan Pablo de la Plata,Joy-Rose Dunoyer de Segonzac,Jennifer Attali,Cécile Lozach,Alexandre Bellucci,Louis Marcellin,Laure Fournier,Sabine Sarnacki,Pietro Gori,Isabelle Bloch*

Main category: cs.CV

TL;DR: 介绍新型混合AI框架Visionerves用于从多梯度DWI和形态MRI数据中识别周围神经系统，应用于子宫内膜异位症患者效果优于标准束成像。


<details>
  <summary>Details</summary>
Motivation: 子宫内膜异位症常导致慢性盆腔疼痛和神经受累，但周围神经成像存在挑战。

Method: Visionerves框架通过模糊空间关系编码解剖知识，无需手动选择感兴趣区域，管道包括深度学习模型自动分割解剖结构和符号空间推理进行束成像与神经识别两个阶段。

Result: 应用于10名子宫内膜异位症女性患者，Dice分数最多提高25%，空间误差降至小于5mm。

Conclusion: 该自动且可重复的方法可实现详细神经分析，为子宫内膜异位症相关神经病变及其他神经受累疾病的非侵入性诊断铺平道路。

Abstract: Endometriosis often leads to chronic pelvic pain and possible nerve
involvement, yet imaging the peripheral nerves remains a challenge. We
introduce Visionerves, a novel hybrid AI framework for peripheral nervous
system recognition from multi-gradient DWI and morphological MRI data. Unlike
conventional tractography, Visionerves encodes anatomical knowledge through
fuzzy spatial relationships, removing the need for selection of manual ROIs.
The pipeline comprises two phases: (A) automatic segmentation of anatomical
structures using a deep learning model, and (B) tractography and nerve
recognition by symbolic spatial reasoning. Applied to the lumbosacral plexus in
10 women with (confirmed or suspected) endometriosis, Visionerves demonstrated
substantial improvements over standard tractography, with Dice score
improvements of up to 25% and spatial errors reduced to less than 5 mm. This
automatic and reproducible approach enables detailed nerve analysis and paves
the way for non-invasive diagnosis of endometriosis-related neuropathy, as well
as other conditions with nerve involvement.

</details>


### [322] [V-SenseDrive: A Privacy-Preserving Road Video and In-Vehicle Sensor Fusion Framework for Road Safety & Driver Behaviour Modelling](https://arxiv.org/abs/2509.18187)
*Muhammad Naveed,Nazia Perwaiz,Sidra Sultana,Mohaira Ahmad,Muhammad Moazam Fraz*

Main category: cs.CV

TL;DR: 提出首个保护隐私的多模态驾驶员行为数据集V - SenseDrive，介绍采集过程，填补巴基斯坦驾驶行为数据空白。


<details>
  <summary>Details</summary>
Motivation: 现有数据集多来自发达国家，缺乏新兴经济体行为多样性，且面部记录侵犯隐私，需要可靠检测不安全驾驶行为以提升道路安全。

Method: 开发自定义安卓应用，结合智能手机惯性和GPS传感器数据与道路视频，采集三种驾驶行为数据，对数据分三层结构处理。

Result: 成功采集V - SenseDrive数据集，包含多类型道路上三种驾驶行为数据。

Conclusion: V - SenseDrive填补全球驾驶员行为数据集空白，为智能交通解决方案奠定基础。

Abstract: Road traffic accidents remain a major public health challenge, particularly
in countries with heterogeneous road conditions, mixed traffic flow, and
variable driving discipline, such as Pakistan. Reliable detection of unsafe
driving behaviours is a prerequisite for improving road safety, enabling
advanced driver assistance systems (ADAS), and supporting data driven decisions
in insurance and fleet management. Most of existing datasets originate from the
developed countries with limited representation of the behavioural diversity
observed in emerging economies and the driver's face recording voilates the
privacy preservation. We present V-SenseDrive, the first privacy-preserving
multimodal driver behaviour dataset collected entirely within the Pakistani
driving environment. V-SenseDrive combines smartphone based inertial and GPS
sensor data with synchronized road facing video to record three target driving
behaviours (normal, aggressive, and risky) on multiple types of roads,
including urban arterials, secondary roads, and motorways. Data was gathered
using a custom Android application designed to capture high frequency
accelerometer, gyroscope, and GPS streams alongside continuous video, with all
sources precisely time aligned to enable multimodal analysis. The focus of this
work is on the data acquisition process, covering participant selection,
driving scenarios, environmental considerations, and sensor video
synchronization techniques. The dataset is structured into raw, processed, and
semantic layers, ensuring adaptability for future research in driver behaviour
classification, traffic safety analysis, and ADAS development. By representing
real world driving in Pakistan, V-SenseDrive fills a critical gap in the global
landscape of driver behaviour datasets and lays the groundwork for context
aware intelligent transportation solutions.

</details>


### [323] [Qianfan-VL: Domain-Enhanced Universal Vision-Language Models](https://arxiv.org/abs/2509.18189)
*Daxiang Dong,Mingming Zheng,Dong Xu,Bairong Zhuang,Wenyu Zhang,Chunhua Luo,Haoran Wang,Zijian Zhao,Jie Li,Yuxuan Li,Hanjun Zhong,Mengyue Liu,Jieting Chen,Shupeng Li,Lun Tian,Yaping Feng,Xin Li,Donggang Jiang,Yong Chen,Yehua Xu,Duohao Qin,Chen Feng,Dan Wang,Henghua Zhang,Jingjing Ha,Jinhui He,Yanfeng Zhai,Chengxin Zheng,Jiayi Mao,Jiacheng Chen,Ruchang Yao,Ziye Yuan,Jianmin Wu,Guangjun Xie,Dou Shen*

Main category: cs.CV

TL;DR: 介绍了参数从3B到70B的Qianfan - VL多模态大模型，采用创新技术达SOTA性能，在多基准测试表现佳，验证了昆仑芯片训练能力。


<details>
  <summary>Details</summary>
Motivation: 开发适用于不同企业部署场景的领域增强多模态模型。

Method: 采用多阶段渐进式训练和高精度数据合成管道，使用百度昆仑P800芯片训练。

Result: 在通用基准测试与领先开源模型相当，在特定基准测试达SOTA，在OCR和文档理解有优势，部分变体在数学推理和逻辑推理表现好，单任务5000芯片缩放效率超90%。

Conclusion: 建立了开发领域增强多模态模型的有效方法。

Abstract: We present Qianfan-VL, a series of multimodal large language models ranging
from 3B to 70B parameters, achieving state-of-the-art performance through
innovative domain enhancement techniques. Our approach employs multi-stage
progressive training and high-precision data synthesis pipelines, which prove
to be critical technologies for enhancing domain-specific capabilities while
maintaining strong general performance. Qianfan-VL achieves comparable results
to leading open-source models on general benchmarks, with state-of-the-art
performance on benchmarks such as CCBench, SEEDBench IMG, ScienceQA, and
MMStar. The domain enhancement strategy delivers significant advantages in OCR
and document understanding, validated on both public benchmarks (OCRBench 873,
DocVQA 94.75%) and in-house evaluations. Notably, Qianfan-VL-8B and 70B
variants incorporate long chain-of-thought capabilities, demonstrating superior
performance on mathematical reasoning (MathVista 78.6%) and logical inference
tasks. All models are trained entirely on Baidu's Kunlun P800 chips, validating
the capability of large-scale AI infrastructure to train SOTA-level multimodal
models with over 90% scaling efficiency on 5000 chips for a single task. This
work establishes an effective methodology for developing domain-enhanced
multimodal models suitable for diverse enterprise deployment scenarios.

</details>


### [324] [HazeFlow: Revisit Haze Physical Model as ODE and Non-Homogeneous Haze Generation for Real-World Dehazing](https://arxiv.org/abs/2509.18190)
*Junseong Shin,Seungwoo Chung,Yunjeong Yang,Tae Hyun Kim*

Main category: cs.CV

TL;DR: 提出HazeFlow框架用于图像去雾，结合ODE和MCBM提升去雾效果。


<details>
  <summary>Details</summary>
Motivation: 深度学习去雾缺乏配对真实数据，传统ASM方法难以处理复杂情况，需提升去雾性能。

Method: 提出基于ODE的HazeFlow框架，将ASM重写为ODE；引入基于MCBM的非均匀雾霾生成方法。

Result: HazeFlow在多个真实世界去雾基准数据集上达到了最先进的性能。

Conclusion: HazeFlow框架和MCBM生成方法有效提升了真实世界图像去雾性能。

Abstract: Dehazing involves removing haze or fog from images to restore clarity and
improve visibility by estimating atmospheric scattering effects. While deep
learning methods show promise, the lack of paired real-world training data and
the resulting domain gap hinder generalization to real-world scenarios. In this
context, physics-grounded learning becomes crucial; however, traditional
methods based on the Atmospheric Scattering Model (ASM) often fall short in
handling real-world complexities and diverse haze patterns. To solve this
problem, we propose HazeFlow, a novel ODE-based framework that reformulates ASM
as an ordinary differential equation (ODE). Inspired by Rectified Flow (RF),
HazeFlow learns an optimal ODE trajectory to map hazy images to clean ones,
enhancing real-world dehazing performance with only a single inference step.
Additionally, we introduce a non-homogeneous haze generation method using
Markov Chain Brownian Motion (MCBM) to address the scarcity of paired
real-world data. By simulating realistic haze patterns through MCBM, we enhance
the adaptability of HazeFlow to diverse real-world scenarios. Through extensive
experiments, we demonstrate that HazeFlow achieves state-of-the-art performance
across various real-world dehazing benchmark datasets.

</details>


### [325] [TinyEcoWeedNet: Edge Efficient Real-Time Aerial Agricultural Weed Detection](https://arxiv.org/abs/2509.18193)
*Omar H. Khater,Abdul Jabbar Siddiqui,Aiman El-Maleh,M. Shamim Hossain*

Main category: cs.CV

TL;DR: 本文通过结构化通道剪枝、量化感知训练和TensorRT加速，对EcoWeedNet进行压缩，减少模型大小和计算量，提升推理速度，在数据集上表现优于YOLO模型。


<details>
  <summary>Details</summary>
Motivation: 边缘设备资源有限，难以在农业中部署深度学习模型，需对模型进行压缩。

Method: 采用结构化通道剪枝、量化感知训练（QAT），并使用NVIDIA的TensorRT在Jetson Orin Nano上加速。

Result: 模型大小最多减少68.5%，计算量减少3.2 GFLOPs，FP16推理速度达184 FPS，比基线快28.7%；在CottonWeedDet12数据集上，剪枝率39.5%的EcoWeedNet优于YOLO11n和YOLO12n，精度83.7%，召回率77.5%，mAP50为85.9%。

Conclusion: 剪枝后的EcoWeedNet对精准农业既高效又有效。

Abstract: Deploying deep learning models in agriculture is difficult because edge
devices have limited resources, but this work presents a compressed version of
EcoWeedNet using structured channel pruning, quantization-aware training (QAT),
and acceleration with NVIDIA's TensorRT on the Jetson Orin Nano. Despite the
challenges of pruning complex architectures with residual shortcuts, attention
mechanisms, concatenations, and CSP blocks, the model size was reduced by up to
68.5% and computations by 3.2 GFLOPs, while inference speed reached 184 FPS at
FP16, 28.7% faster than the baseline. On the CottonWeedDet12 dataset, the
pruned EcoWeedNet with a 39.5% pruning ratio outperformed YOLO11n and YOLO12n
(with only 20% pruning), achieving 83.7% precision, 77.5% recall, and 85.9%
mAP50, proving it to be both efficient and effective for precision agriculture.

</details>


### [326] [A Single Image Is All You Need: Zero-Shot Anomaly Localization Without Training Data](https://arxiv.org/abs/2509.18354)
*Mehrdad Moradi,Shengzhe Chen,Hao Yan,Kamran Paynabar*

Main category: cs.CV

TL;DR: 提出基于DIP启发的单图像异常定位方法SSDnet，无需外部数据，在数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实场景中训练数据可能不可用，需解决零样本下图像异常检测问题。

Method: 设计基于补丁的训练框架，直接将输入图像输入网络自重建，应用掩码、补丁洗牌和高斯噪声，使用基于内积相似度的感知损失。

Result: SSDnet在MVTec - AD和织物数据集上分别取得0.99 AUROC、0.60 AUPRC和0.98 AUROC、0.67 AUPRC，优于现有方法。

Conclusion: 所提SSDnet方法无需外部训练数据、标签或参考，在有噪声或像素缺失时仍稳健，性能出色。

Abstract: Anomaly detection in images is typically addressed by learning from
collections of training data or relying on reference samples. In many
real-world scenarios, however, such training data may be unavailable, and only
the test image itself is provided. We address this zero-shot setting by
proposing a single-image anomaly localization method that leverages the
inductive bias of convolutional neural networks, inspired by Deep Image Prior
(DIP). Our method is named Single Shot Decomposition Network (SSDnet). Our key
assumption is that natural images often exhibit unified textures and patterns,
and that anomalies manifest as localized deviations from these repetitive or
stochastic patterns. To learn the deep image prior, we design a patch-based
training framework where the input image is fed directly into the network for
self-reconstruction, rather than mapping random noise to the image as done in
DIP. To avoid the model simply learning an identity mapping, we apply masking,
patch shuffling, and small Gaussian noise. In addition, we use a perceptual
loss based on inner-product similarity to capture structure beyond pixel
fidelity. Our approach needs no external training data, labels, or references,
and remains robust in the presence of noise or missing pixels. SSDnet achieves
0.99 AUROC and 0.60 AUPRC on MVTec-AD and 0.98 AUROC and 0.67 AUPRC on the
fabric dataset, outperforming state-of-the-art methods. The implementation code
will be released at https://github.com/mehrdadmoradi124/SSDnet

</details>


### [327] [Align Where the Words Look: Cross-Attention-Guided Patch Alignment with Contrastive and Transport Regularization for Bengali Captioning](https://arxiv.org/abs/2509.18369)
*Riad Ahmed Anonto,Sardar Md. Saffat Zabin,M. Saifur Rahman*

Main category: cs.CV

TL;DR: 提出计算感知的孟加拉语字幕管道解决低资源语言视觉 - 语言模型基础问题，有新的三损失目标，在数据集上表现好。


<details>
  <summary>Details</summary>
Motivation: 低资源语言视觉 - 语言模型常产生关于错误对象的文本，原因包括配对数据稀缺、翻译枢纽破坏对齐和以英语为中心的预训练忽略目标语言语义。

Method: 构建计算感知的孟加拉语字幕管道，用LaBSE验证的英 - 孟对和双语提示合成图像训练，使用冻结的MaxViT、孟加拉语原生mBART - 50解码，有轻量级桥梁连接模态，采用三损失目标。

Result: 在Flickr30k - 1k和MSCOCO - 1k数据集上有良好表现，优于强CE基线，缩小真实 - 合成质心差距41%。

Conclusion: 三损失目标（PAL + InfoNCE + OT）协同作用改善基础、减少虚假匹配，推动模型性能提升。

Abstract: Grounding vision--language models in low-resource languages remains
challenging, as they often produce fluent text about the wrong objects. This
stems from scarce paired data, translation pivots that break alignment, and
English-centric pretraining that ignores target-language semantics. We address
this with a compute-aware Bengali captioning pipeline trained on LaBSE-verified
EN--BN pairs and 110k bilingual-prompted synthetic images. A frozen MaxViT
yields stable visual patches, a Bengali-native mBART-50 decodes, and a
lightweight bridge links the modalities. Our core novelty is a tri-loss
objective: Patch-Alignment Loss (PAL) aligns real and synthetic patch
descriptors using decoder cross-attention, InfoNCE enforces global
real--synthetic separation, and Sinkhorn-based OT ensures balanced fine-grained
patch correspondence. This PAL+InfoNCE+OT synergy improves grounding, reduces
spurious matches, and drives strong gains on Flickr30k-1k (BLEU-4 12.29, METEOR
27.98, BERTScore-F1 71.20) and MSCOCO-1k (BLEU-4 12.00, METEOR 28.14,
BERTScore-F1 75.40), outperforming strong CE baselines and narrowing the
real--synthetic centroid gap by 41%.

</details>


### [328] [Check Field Detection Agent (CFD-Agent) using Multimodal Large Language and Vision Language Models](https://arxiv.org/abs/2509.18405)
*Sourav Halder,Jinjun Tong,Xinyu Wu*

Main category: cs.CV

TL;DR: 本文提出一种无训练的自动支票字段检测框架，能零样本检测支票组件，在数据集上表现良好，还可生成高质量标注数据集。


<details>
  <summary>Details</summary>
Motivation: 支票是金融生态基础工具但易被欺诈，传统字段检测依赖大量标注数据，而此类数据因隐私等原因稀缺。

Method: 引入结合视觉语言模型（VLM）和多模态大语言模型（MLLM）的无训练框架，实现零样本检测。

Result: 在110张多种格式和布局的支票数据集上评估，模型表现出良好性能和泛化能力。

Conclusion: 该框架降低了在现实金融场景部署的门槛，可作为生成高质量标注数据集的引导机制，助力开发满足机构需求的实时目标检测模型。

Abstract: Checks remain a foundational instrument in the financial ecosystem,
facilitating substantial transaction volumes across institutions. However,
their continued use also renders them a persistent target for fraud,
underscoring the importance of robust check fraud detection mechanisms. At the
core of such systems lies the accurate identification and localization of
critical fields, such as the signature, magnetic ink character recognition
(MICR) line, courtesy amount, legal amount, payee, and payer, which are
essential for subsequent verification against reference checks belonging to the
same customer. This field-level detection is traditionally dependent on object
detection models trained on large, diverse, and meticulously labeled datasets,
a resource that is scarce due to proprietary and privacy concerns. In this
paper, we introduce a novel, training-free framework for automated check field
detection, leveraging the power of a vision language model (VLM) in conjunction
with a multimodal large language model (MLLM). Our approach enables zero-shot
detection of check components, significantly lowering the barrier to deployment
in real-world financial settings. Quantitative evaluation of our model on a
hand-curated dataset of 110 checks spanning multiple formats and layouts
demonstrates strong performance and generalization capability. Furthermore,
this framework can serve as a bootstrap mechanism for generating high-quality
labeled datasets, enabling the development of specialized real-time object
detection models tailored to institutional needs.

</details>


### [329] [Leveraging Geometric Visual Illusions as Perceptual Inductive Biases for Vision Models](https://arxiv.org/abs/2509.15156)
*Haobo Yang,Minghao Guo,Dequan Yang,Wenyu Wang*

Main category: cs.CV

TL;DR: 本文提出将几何视错觉融入图像分类训练，评估多源学习策略，发现融入视错觉可提升泛化能力和结构敏感性，展示了感知科学与机器学习的新融合。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型在图像分类中主要利用数据统计规律，很少结合感知心理学的结构化见解，因此探索感知驱动的归纳偏置的潜力。

Method: 引入合成的参数化几何视错觉数据集，评估三种结合视错觉识别任务和 ImageNet 分类目标的多源学习策略。

Result: 融入几何视错觉作为辅助监督可系统提升泛化能力，感知驱动的归纳偏置能增强 CNN 和基于变换器架构的结构敏感性。

Conclusion: 展示了感知科学与机器学习的新融合，为将感知先验嵌入视觉模型设计指明了新方向。

Abstract: Contemporary deep learning models have achieved impressive performance in
image classification by primarily leveraging statistical regularities within
large datasets, but they rarely incorporate structured insights drawn directly
from perceptual psychology. To explore the potential of perceptually motivated
inductive biases, we propose integrating classic geometric visual illusions
well-studied phenomena from human perception into standard image-classification
training pipelines. Specifically, we introduce a synthetic, parametric
geometric-illusion dataset and evaluate three multi-source learning strategies
that combine illusion recognition tasks with ImageNet classification
objectives. Our experiments reveal two key conceptual insights: (i)
incorporating geometric illusions as auxiliary supervision systematically
improves generalization, especially in visually challenging cases involving
intricate contours and fine textures; and (ii) perceptually driven inductive
biases, even when derived from synthetic stimuli traditionally considered
unrelated to natural image recognition, can enhance the structural sensitivity
of both CNN and transformer-based architectures. These results demonstrate a
novel integration of perceptual science and machine learning and suggest new
directions for embedding perceptual priors into vision model design.

</details>


### [330] [OraPO: Oracle-educated Reinforcement Learning for Data-efficient and Factual Radiology Report Generation](https://arxiv.org/abs/2509.18600)
*Zhuoxiao Chen,Hongyang Yu,Ying Xu,Yadan Luo,Long Duong,Yuan-Fang Li*

Main category: cs.CV

TL;DR: 提出OraPO和FactS框架应对受限预算下的RRG任务，提升学习效率并在CheXpert Plus数据集达新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有RRG工作遵循规模驱动范式，数据和计算成本高，需在受限预算下解决RRG任务。

Method: 提出OraPO实现单阶段、仅强化学习训练，通过轻量级oracle步骤将失败探索转化为偏好监督；提出FactS基于诊断证据提取临床事实并检查蕴含关系生成奖励。

Result: 在CheXpert Plus数据集上以少2 - 3个数量级的训练数据和小VLM在普通硬件上取得0.341的F1成绩。

Conclusion: OraPO和FactS构建的框架紧凑强大，显著提升临床难题学习效率，达新SOTA。

Abstract: Radiology report generation (RRG) aims to automatically produce clinically
faithful reports from chest X-ray images. Prevailing work typically follows a
scale-driven paradigm, by multi-stage training over large paired corpora and
oversized backbones, making pipelines highly data- and compute-intensive. In
this paper, we propose Oracle-educated GRPO {OraPO) with a FactScore-based
reward (FactS) to tackle the RRG task under constrained budgets. OraPO enables
single-stage, RL-only training by converting failed GRPO explorations on rare
or difficult studies into direct preference supervision via a lightweight
oracle step. FactS grounds learning in diagnostic evidence by extracting atomic
clinical facts and checking entailment against ground-truth labels, yielding
dense, interpretable sentence-level rewards. Together, OraPO and FactS create a
compact and powerful framework that significantly improves learning efficiency
on clinically challenging cases, setting the new SOTA performance on the
CheXpert Plus dataset (0.341 in F1) with 2--3 orders of magnitude less training
data using a small base VLM on modest hardware.

</details>


### [331] [Learning neuroimaging models from health system-scale data](https://arxiv.org/abs/2509.18638)
*Yiwei Lyu,Samir Harake,Asadur Chowdury,Soumyanil Banerjee,Rachel Gologorsky,Shixuan Liu,Anna-Katharina Meissner,Akshay Rao,Chenhui Zhao,Akhil Kondepudi,Cheng Jiang,Xinhai Hou,Rushikesh S. Joshi,Volker Neuschmelting,Ashok Srinivasan,Dawn Kleindorfer,Brian Athey,Vikas Gulani,Aditya Pandey,Honglak Lee,Todd Hollon*

Main category: cs.CV

TL;DR: 本文开发了首个用于神经影像的视觉语言模型Prima，经大量MRI研究训练与测试，表现优于其他模型，能提供多种诊断辅助，还可缓解医疗系统偏差。


<details>
  <summary>Details</summary>
Motivation: 全球对MRI研究需求上升，给医疗系统带来压力，影响患者，特别是低资源和农村地区患者，需要解决这些问题。

Method: 利用大型学术医疗系统作为数据引擎，开发Prima模型，采用分层视觉架构，在超22万MRI研究上训练，并在含3万MRI研究的为期1年医疗系统研究中测试。

Result: 在52种放射学诊断中，Prima的ROC曲线下平均诊断面积达92.0，优于其他先进模型，能提供可解释的鉴别诊断等。

Conclusion: 医疗系统规模的VLMs有变革潜力，Prima可推动人工智能驱动的医疗发展。

Abstract: Neuroimaging is a ubiquitous tool for evaluating patients with neurological
diseases. The global demand for magnetic resonance imaging (MRI) studies has
risen steadily, placing significant strain on health systems, prolonging
turnaround times, and intensifying physician burnout \cite{Chen2017-bt,
Rula2024-qp-1}. These challenges disproportionately impact patients in
low-resource and rural settings. Here, we utilized a large academic health
system as a data engine to develop Prima, the first vision language model (VLM)
serving as an AI foundation for neuroimaging that supports real-world, clinical
MRI studies as input. Trained on over 220,000 MRI studies, Prima uses a
hierarchical vision architecture that provides general and transferable MRI
features. Prima was tested in a 1-year health system-wide study that included
30K MRI studies. Across 52 radiologic diagnoses from the major neurologic
disorders, including neoplastic, inflammatory, infectious, and developmental
lesions, Prima achieved a mean diagnostic area under the ROC curve of 92.0,
outperforming other state-of-the-art general and medical AI models. Prima
offers explainable differential diagnoses, worklist priority for radiologists,
and clinical referral recommendations across diverse patient demographics and
MRI systems. Prima demonstrates algorithmic fairness across sensitive groups
and can help mitigate health system biases, such as prolonged turnaround times
for low-resource populations. These findings highlight the transformative
potential of health system-scale VLMs and Prima's role in advancing AI-driven
healthcare.

</details>


### [332] [PolypSeg-GradCAM: Towards Explainable Computer-Aided Gastrointestinal Disease Detection Using U-Net Based Segmentation and Grad-CAM Visualization on the Kvasir Dataset](https://arxiv.org/abs/2509.18159)
*Akwasi Asare,Ulas Bagci*

Main category: cs.CV

TL;DR: 提出可解释深度学习框架PolypSeg - GradCAM用于息肉分割，在Kvasir - SEG数据集上实验效果好，向可靠AI辅助结肠镜检查迈进。


<details>
  <summary>Details</summary>
Motivation: 结直肠癌发病率和死亡率高，息肉早期准确分割重要，手动分割有局限，现有深度学习方法可解释性不足。

Method: 将U - Net架构与Gradient - weighted Class Activation Mapping (Grad - CAM)集成，构建PolypSeg - GradCAM框架，在Kvasir - SEG数据集上训练和评估。

Result: 测试集平均交并比（IoU）达0.9257，训练和验证集Dice系数高（F - score > 0.96），Grad - CAM可视化显示预测受临床相关区域引导。

Conclusion: PolypSeg - GradCAM结合高分割准确率和可解释性，向可靠、可信的AI辅助结肠镜检查和改善早期结直肠癌预防迈进。

Abstract: Colorectal cancer (CRC) remains one of the leading causes of cancer-related
morbidity and mortality worldwide, with gastrointestinal (GI) polyps serving as
critical precursors according to the World Health Organization (WHO). Early and
accurate segmentation of polyps during colonoscopy is essential for reducing
CRC progression, yet manual delineation is labor-intensive and prone to
observer variability. Deep learning methods have demonstrated strong potential
for automated polyp analysis, but their limited interpretability remains a
barrier to clinical adoption. In this study, we present PolypSeg-GradCAM, an
explainable deep learning framework that integrates the U-Net architecture with
Gradient-weighted Class Activation Mapping (Grad-CAM) for transparent polyp
segmentation. The model was trained and evaluated on the Kvasir-SEG dataset of
1000 annotated endoscopic images. Experimental results demonstrate robust
segmentation performance, achieving a mean Intersection over Union (IoU) of
0.9257 on the test set and consistently high Dice coefficients (F-score > 0.96)
on training and validation sets. Grad-CAM visualizations further confirmed that
predictions were guided by clinically relevant regions, enhancing transparency
and trust in the model's decisions. By coupling high segmentation accuracy with
interpretability, PolypSeg-GradCAM represents a step toward reliable,
trustworthy AI-assisted colonoscopy and improved early colorectal cancer
prevention.

</details>


### [333] [Self Identity Mapping](https://arxiv.org/abs/2509.18165)
*Xiuding Cai,Yaoyao Zhu,Linjie Fu,Dong Miao,Yu Yao*

Main category: cs.CV

TL;DR: 提出Self Identity Mapping (SIM)正则化框架，实例化出$ ho	ext{SIM} $，经多任务评估效果好，且与现有方法正交，代码开源。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习正则化技术依赖启发式方法，在不同场景下可靠性和有效性不足，需更好的正则化方法。

Method: 提出SIM框架，利用逆映射机制增强表征学习；将其实例化为$ ho	ext{SIM} $，采用补丁级特征采样和基于投影的方法重构潜在特征以降低复杂度。

Result: 在图像分类、少样本提示学习、领域泛化等三个任务中，$ ho	ext{SIM} $比基线方法有一致改进；与现有正则化方法正交，能提升其有效性；在语义分割等密集任务和非视觉领域也表现良好。

Conclusion: $ ho	ext{SIM} $是有效的正则化方法，可增强跨任务的表征学习，适用不同网络架构和任务。

Abstract: Regularization is essential in deep learning to enhance generalization and
mitigate overfitting. However, conventional techniques often rely on
heuristics, making them less reliable or effective across diverse settings. We
propose Self Identity Mapping (SIM), a simple yet effective, data-intrinsic
regularization framework that leverages an inverse mapping mechanism to enhance
representation learning. By reconstructing the input from its transformed
output, SIM reduces information loss during forward propagation and facilitates
smoother gradient flow. To address computational inefficiencies, We instantiate
SIM as $ \rho\text{SIM} $ by incorporating patch-level feature sampling and
projection-based method to reconstruct latent features, effectively lowering
complexity. As a model-agnostic, task-agnostic regularizer, SIM can be
seamlessly integrated as a plug-and-play module, making it applicable to
different network architectures and tasks.
  We extensively evaluate $\rho\text{SIM}$ across three tasks: image
classification, few-shot prompt learning, and domain generalization.
Experimental results show consistent improvements over baseline methods,
highlighting $\rho\text{SIM}$'s ability to enhance representation learning
across various tasks. We also demonstrate that $\rho\text{SIM}$ is orthogonal
to existing regularization methods, boosting their effectiveness. Moreover, our
results confirm that $\rho\text{SIM}$ effectively preserves semantic
information and enhances performance in dense-to-dense tasks, such as semantic
segmentation and image translation, as well as in non-visual domains including
audio classification and time series anomaly detection. The code is publicly
available at https://github.com/XiudingCai/SIM-pytorch.

</details>


### [334] [A Deep Learning Approach for Spatio-Temporal Forecasting of InSAR Ground Deformation in Eastern Ireland](https://arxiv.org/abs/2509.18176)
*Wendong Yao,Saeed Azadnejad,Binhua Huang,Shane Donohue,Soumyabrata Dev*

Main category: cs.CV

TL;DR: 本文提出新深度学习框架，将稀疏InSAR数据转为密集时空张量，用CNN - LSTM模型预测地面变形，效果优于基线模型，证实时空深度学习用于变形预测的有效性。


<details>
  <summary>Details</summary>
Motivation: 监测地面位移对城市基础设施稳定和地质灾害缓解至关重要，但从稀疏InSAR时间序列数据预测未来变形是重大挑战。

Method: 引入新深度学习框架将稀疏点测量转换为密集时空张量，设计并实现CNN - LSTM模型学习空间模式和时间依赖，与Light Gradient Boosting Machine和LASSO回归等基线模型进行性能对比。

Result: 所提架构提供更准确和空间连贯的预测，建立了新的性能基准；可解释性分析表明基线模型常采用简单持久模式。

Conclusion: 时空深度学习用于高分辨率变形预测有效且有潜力。

Abstract: Monitoring ground displacement is crucial for urban infrastructure stability
and mitigating geological hazards. However, forecasting future deformation from
sparse Interferometric Synthetic Aperture Radar (InSAR) time-series data
remains a significant challenge. This paper introduces a novel deep learning
framework that transforms these sparse point measurements into a dense
spatio-temporal tensor. This methodological shift allows, for the first time,
the direct application of advanced computer vision architectures to this
forecasting problem. We design and implement a hybrid Convolutional Neural
Network and Long-Short Term Memory (CNN-LSTM) model, specifically engineered to
simultaneously learn spatial patterns and temporal dependencies from the
generated data tensor. The model's performance is benchmarked against powerful
machine learning baselines, Light Gradient Boosting Machine and LASSO
regression, using Sentinel-1 data from eastern Ireland. Results demonstrate
that the proposed architecture provides significantly more accurate and
spatially coherent forecasts, establishing a new performance benchmark for this
task. Furthermore, an interpretability analysis reveals that baseline models
often default to simplistic persistence patterns, highlighting the necessity of
our integrated spatio-temporal approach to capture the complex dynamics of
ground deformation. Our findings confirm the efficacy and potential of
spatio-temporal deep learning for high-resolution deformation forecasting.

</details>


### [335] [LEAF-Mamba: Local Emphatic and Adaptive Fusion State Space Model for RGB-D Salient Object Detection](https://arxiv.org/abs/2509.18683)
*Lanhu Wu,Zilin Gao,Hao Fei,Mong-Li Lee,Wynne Hsu*

Main category: cs.CV

TL;DR: 提出 LEAF - Mamba 模型用于 RGB - D 显著目标检测，实验表明其在有效性和效率上均优于 16 种先进方法，且在 RGB - T SOD 任务有良好泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于 CNN 或 Vision Transformers 的 RGB - D 显著目标检测方法难以平衡性能和计算效率，直接应用 SSM 会导致局部语义不足和跨模态融合不充分。

Method: 提出 Local Emphatic and Adaptive Fusion state space model (LEAF - Mamba)，包含 local emphatic state space module (LE - SSM) 捕获多尺度局部依赖，以及 SSM - based adaptive fusion module (AFM) 进行跨模态交互和集成。

Result: LEAF - Mamba 在有效性和效率上均优于 16 种先进 RGB - D SOD 方法，在 RGB - T SOD 任务有良好表现。

Conclusion: LEAF - Mamba 是一种有效且高效的 RGB - D 显著目标检测方法，有强大泛化能力。

Abstract: RGB-D salient object detection (SOD) aims to identify the most conspicuous
objects in a scene with the incorporation of depth cues. Existing methods
mainly rely on CNNs, limited by the local receptive fields, or Vision
Transformers that suffer from the cost of quadratic complexity, posing a
challenge in balancing performance and computational efficiency. Recently,
state space models (SSM), Mamba, have shown great potential for modeling
long-range dependency with linear complexity. However, directly applying SSM to
RGB-D SOD may lead to deficient local semantics as well as the inadequate
cross-modality fusion. To address these issues, we propose a Local Emphatic and
Adaptive Fusion state space model (LEAF-Mamba) that contains two novel
components: 1) a local emphatic state space module (LE-SSM) to capture
multi-scale local dependencies for both modalities. 2) an SSM-based adaptive
fusion module (AFM) for complementary cross-modality interaction and reliable
cross-modality integration. Extensive experiments demonstrate that the
LEAF-Mamba consistently outperforms 16 state-of-the-art RGB-D SOD methods in
both efficacy and efficiency. Moreover, our method can achieve excellent
performance on the RGB-T SOD task, proving a powerful generalization ability.

</details>


### [336] [RSVG-ZeroOV: Exploring a Training-Free Framework for Zero-Shot Open-Vocabulary Visual Grounding in Remote Sensing Images](https://arxiv.org/abs/2509.18711)
*Ke Li,Di Wang,Ting Wang,Fuyu Dong,Yiming Zhang,Luyao Zhang,Xiangyu Wang,Shaofeng Li,Quan Wang*

Main category: cs.CV

TL;DR: 提出训练-free框架RSVG - ZeroOV用于零样本开放词汇遥感视觉定位，含三阶段，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有遥感视觉定位方法局限于封闭词汇，依赖高质量数据集和耗时微调，本文旨在探索冻结通用基础模型在零样本开放词汇RSVG中的潜力。

Method: RSVG - ZeroOV包含三个阶段：利用VLM获取交叉注意力图；借助DM填补物体结构和形状信息；引入注意力进化模块抑制无关激活。

Result: 广泛实验表明，所提框架始终优于现有的弱监督和零样本方法。

Conclusion: RSVG - ZeroOV无需繁琐的特定任务训练，是一种高效且可扩展的解决方案。

Abstract: Remote sensing visual grounding (RSVG) aims to localize objects in remote
sensing images based on free-form natural language expressions. Existing
approaches are typically constrained to closed-set vocabularies, limiting their
applicability in open-world scenarios. While recent attempts to leverage
generic foundation models for open-vocabulary RSVG, they overly rely on
expensive high-quality datasets and time-consuming fine-tuning. To address
these limitations, we propose \textbf{RSVG-ZeroOV}, a training-free framework
that aims to explore the potential of frozen generic foundation models for
zero-shot open-vocabulary RSVG. Specifically, RSVG-ZeroOV comprises three key
stages: (i) Overview: We utilize a vision-language model (VLM) to obtain
cross-attention\footnote[1]{In this paper, although decoder-only VLMs use
self-attention over all tokens, we refer to the image-text interaction part as
cross-attention to distinguish it from pure visual self-attention.}maps that
capture semantic correlations between text queries and visual regions. (ii)
Focus: By leveraging the fine-grained modeling priors of a diffusion model
(DM), we fill in gaps in structural and shape information of objects, which are
often overlooked by VLM. (iii) Evolve: A simple yet effective attention
evolution module is introduced to suppress irrelevant activations, yielding
purified segmentation masks over the referred objects. Without cumbersome
task-specific training, RSVG-ZeroOV offers an efficient and scalable solution.
Extensive experiments demonstrate that the proposed framework consistently
outperforms existing weakly-supervised and zero-shot methods.

</details>


### [337] [AI-Derived Structural Building Intelligence for Urban Resilience: An Application in Saint Vincent and the Grenadines](https://arxiv.org/abs/2509.18182)
*Isabelle Tingzon,Yoji Toriumi,Caroline Gevaert*

Main category: cs.CV

TL;DR: 提出AI驱动工作流从高分辨率卫星图像推断屋顶属性，以圣文森特和格林纳丁斯为例，对比不同模型进行屋顶分类，最佳模型有较好F1分数，目标是助力小岛屿发展中国家城市治理。


<details>
  <summary>Details</summary>
Motivation: 许多气候脆弱地区的小岛屿发展中国家缺乏用于城市韧性规划和灾害风险降低的详细建筑结构信息，存在数据缺口。

Method: 提出AI驱动工作流，对比地理空间基础模型结合浅层分类器与微调深度学习模型进行屋顶分类，评估加入邻国额外训练数据对模型性能的影响。

Result: 最佳模型在屋顶坡度和屋顶材料分类上F1分数分别达到0.88和0.83。

Conclusion: 结合本地能力建设，工作可为小岛屿发展中国家提供利用AI和地球观测数据进行高效、循证城市治理的新能力。

Abstract: Detailed structural building information is used to estimate potential damage
from hazard events like cyclones, floods, and landslides, making them critical
for urban resilience planning and disaster risk reduction. However, such
information is often unavailable in many small island developing states (SIDS)
in climate-vulnerable regions like the Caribbean. To address this data gap, we
present an AI-driven workflow to automatically infer rooftop attributes from
high-resolution satellite imagery, with Saint Vincent and the Grenadines as our
case study. Here, we compare the utility of geospatial foundation models
combined with shallow classifiers against fine-tuned deep learning models for
rooftop classification. Furthermore, we assess the impact of incorporating
additional training data from neighboring SIDS to improve model performance.
Our best models achieve F1 scores of 0.88 and 0.83 for roof pitch and roof
material classification, respectively. Combined with local capacity building,
our work aims to provide SIDS with novel capabilities to harness AI and Earth
Observation (EO) data to enable more efficient, evidence-based urban
governance.

</details>


### [338] [COLT: Enhancing Video Large Language Models with Continual Tool Usage](https://arxiv.org/abs/2509.18754)
*Yuyang Liu,Xinyuan Shi,Bang Yang,Peilin Zhou,Jiahua Dong,Long Chen,Ian Reid,Xiaondan Liang*

Main category: cs.CV

TL;DR: 文章提出COLT方法增强开源视频大语言模型工具使用能力，在多数据集实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型工具使用方法假设工具固定，难以适应现实中不断变化的工具数据。

Method: 提出COLT方法，引入可学习的工具码本作为特定工具记忆系统，动态选择相关工具；收集视频工具指令调优数据集VideoToolBench。

Result: 在视频大语言模型基准测试和VideoToolBench数据集上实验，COLT表现达到了当前最优。

Conclusion: 提出的COLT方法能有效增强开源视频大语言模型的工具使用能力，避免灾难性遗忘。

Abstract: The success of Large Language Models (LLMs) has significantly propelled the
research of video understanding. To harvest the benefits of well-trained expert
models (i.e., tools), video LLMs prioritize the exploration of tool usage
capabilities. Existing methods either prompt closed-source LLMs or employ the
instruction tuning paradigm for tool-use fine-tuning. These methods, however,
assume an established repository of fixed tools and struggle to generalize to
real-world environments where tool data is perpetually evolving and streaming
in. To this end, we propose to enhance open-source video LLMs with COntinuaL
Tool usage (termed COLT), which automatically acquires tool-use ability in a
successive tool stream without suffering 'catastrophic forgetting' of the past
learned tools. Specifically, our COLT incorporates a learnable tool codebook as
a tool-specific memory system. Then relevant tools are dynamically selected
based on the similarity between user instruction and tool features within the
codebook. To unleash the tool usage potential of video LLMs, we collect a
video-centric tool-use instruction tuning dataset VideoToolBench. Extensive
experiments on both previous video LLM benchmarks and the tool-use-specific
VideoToolBench dataset demonstrate the state-of-the-art performance of our
proposed COLT.

</details>


### [339] [Improving Handshape Representations for Sign Language Processing: A Graph Neural Network Approach](https://arxiv.org/abs/2509.18309)
*Alessa Carbo,Eric Nalisnick*

Main category: cs.CV

TL;DR: 本文提出新图神经网络用于手语手型识别，建立基准并取得较高准确率。


<details>
  <summary>Details</summary>
Motivation: 现有计算方法很少显式建模手型，限制识别精度和语言分析。

Method: 引入新图神经网络，分离时间动态和静态手型配置，结合解剖学图结构与对比学习。

Result: 建立手语序列结构化手型识别基准，37 个手型类别准确率达 46%，基线方法为 25%。

Conclusion: 新方法能有效解决手型识别中的关键挑战，提高识别准确率。

Abstract: Handshapes serve a fundamental phonological role in signed languages, with
American Sign Language employing approximately 50 distinct shapes.
However,computational approaches rarely model handshapes explicitly, limiting
both recognition accuracy and linguistic analysis.We introduce a novel graph
neural network that separates temporal dynamics from static handshape
configurations. Our approach combines anatomically-informed graph structures
with contrastive learning to address key challenges in handshape recognition,
including subtle interclass distinctions and temporal variations. We establish
the first benchmark for structured handshape recognition in signing sequences,
achieving 46% accuracy across 37 handshape classes (with baseline methods
achieving 25%).

</details>


### [340] [DiSSECT: Structuring Transfer-Ready Medical Image Representations through Discrete Self-Supervision](https://arxiv.org/abs/2509.18765)
*Azad Singh,Deepak Mishra*

Main category: cs.CV

TL;DR: 提出DiSSECT框架用于医学图像自监督学习，在多任务上表现佳，验证了其鲁棒性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法存在架构复杂、依赖先验和易走捷径等问题，限制了可扩展性和泛化性。

Method: 将多尺度向量量化集成到自监督学习流程中，施加离散表征瓶颈。

Result: 在分类和分割任务上表现出色，低标签场景下标签效率高。

Conclusion: DiSSECT相比现有方法更具鲁棒性和泛化性。

Abstract: Self-supervised learning (SSL) has emerged as a powerful paradigm for medical
image representation learning, particularly in settings with limited labeled
data. However, existing SSL methods often rely on complex architectures,
anatomy-specific priors, or heavily tuned augmentations, which limit their
scalability and generalizability. More critically, these models are prone to
shortcut learning, especially in modalities like chest X-rays, where anatomical
similarity is high and pathology is subtle. In this work, we introduce DiSSECT
-- Discrete Self-Supervision for Efficient Clinical Transferable
Representations, a framework that integrates multi-scale vector quantization
into the SSL pipeline to impose a discrete representational bottleneck. This
constrains the model to learn repeatable, structure-aware features while
suppressing view-specific or low-utility patterns, improving representation
transfer across tasks and domains. DiSSECT achieves strong performance on both
classification and segmentation tasks, requiring minimal or no fine-tuning, and
shows particularly high label efficiency in low-label regimes. We validate
DiSSECT across multiple public medical imaging datasets, demonstrating its
robustness and generalizability compared to existing state-of-the-art
approaches.

</details>


### [341] [A Kernel Space-based Multidimensional Sparse Model for Dynamic PET Image Denoising](https://arxiv.org/abs/2509.18801)
*Kuang Xiaodong,Li Bingxuan,Li Yuan,Rao Fan,Ma Gege,Xie Qingguo,Mok Greta S P,Liu Huafeng,Zhu Wentao*

Main category: cs.CV

TL;DR: 提出基于模型的神经网络KMDS - Net用于动态PET图像去噪，实验显示其去噪性能强，可用于实现高时空分辨率。


<details>
  <summary>Details</summary>
Motivation: 动态PET短时间帧因统计量有限，实现高图像质量有挑战，深度学习在医学图像去噪有用，故研究动态PET图像去噪。

Method: 利用动态PET帧间空间相关性和帧内结构一致性建立KMDS模型，用神经网络替代参数估计固有形式实现自适应参数优化，形成端到端的KMDS - Net。

Result: 模拟和真实数据实验表明，KMDS - Net动态PET去噪性能强，优于先前基线方法。

Conclusion: 所提方法可有效实现动态PET高时空分辨率，代码已开源。

Abstract: Achieving high image quality for temporal frames in dynamic positron emission
tomography (PET) is challenging due to the limited statistic especially for the
short frames. Recent studies have shown that deep learning (DL) is useful in a
wide range of medical image denoising tasks. In this paper, we propose a
model-based neural network for dynamic PET image denoising. The inter-frame
spatial correlation and intra-frame structural consistency in dynamic PET are
used to establish the kernel space-based multidimensional sparse (KMDS) model.
We then substitute the inherent forms of the parameter estimation with neural
networks to enable adaptive parameters optimization, forming the end-to-end
neural KMDS-Net. Extensive experimental results from simulated and real data
demonstrate that the neural KMDS-Net exhibits strong denoising performance for
dynamic PET, outperforming previous baseline methods. The proposed method may
be used to effectively achieve high temporal and spatial resolution for dynamic
PET. Our source code is available at
https://github.com/Kuangxd/Neural-KMDS-Net/tree/main.

</details>


### [342] [Failure Makes the Agent Stronger: Enhancing Accuracy through Structured Reflection for Reliable Tool Interactions](https://arxiv.org/abs/2509.18847)
*Junhao Su,Yuanliang Wan,Junwei Yang,Hengyu Shi,Tianyang Han,Junfeng Luo,Yurui Qiu*

Main category: cs.CV

TL;DR: 提出结构化反思方法提升大语言模型工具调用可靠性，实验有积极结果。


<details>
  <summary>Details</summary>
Motivation: 当前工具增强大语言模型训练和自我反思方法在多轮交互中脆弱，易重复错误。

Method: 提出结构化反思，将错误到修复路径变为可控制和可训练的动作，结合DAPO和GSPO目标及定制奖励方案训练，引入Tool - Reflection - Bench评估。

Result: 在BFCL v3和Tool - Reflection - Bench实验中，多轮工具调用成功率和错误恢复能力提升，冗余调用减少。

Conclusion: 使反思显式化并直接优化可提高工具交互可靠性，为智能体从失败中学习提供可复现路径。

Abstract: Tool-augmented large language models (LLMs) are usually trained with
supervised imitation or coarse-grained reinforcement learning that optimizes
single tool calls. Current self-reflection practices rely on heuristic prompts
or one-way reasoning: the model is urged to 'think more' instead of learning
error diagnosis and repair. This is fragile in multi-turn interactions; after a
failure the model often repeats the same mistake. We propose structured
reflection, which turns the path from error to repair into an explicit,
controllable, and trainable action. The agent produces a short yet precise
reflection: it diagnoses the failure using evidence from the previous step and
then proposes a correct, executable follow-up call. For training we combine
DAPO and GSPO objectives with a reward scheme tailored to tool use, optimizing
the stepwise strategy Reflect, then Call, then Final. To evaluate, we introduce
Tool-Reflection-Bench, a lightweight benchmark that programmatically checks
structural validity, executability, parameter correctness, and result
consistency. Tasks are built as mini trajectories of erroneous call,
reflection, and corrected call, with disjoint train and test splits.
Experiments on BFCL v3 and Tool-Reflection-Bench show large gains in multi-turn
tool-call success and error recovery, and a reduction of redundant calls. These
results indicate that making reflection explicit and optimizing it directly
improves the reliability of tool interaction and offers a reproducible path for
agents to learn from failure.

</details>


### [343] [LiDAR Point Cloud Image-based Generation Using Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2509.18917)
*Amirhesam Aghanouri,Cristina Olaverri-Monreal*

Main category: cs.CV

TL;DR: 本文提出用改进的去噪扩散概率模型生成高质量合成数据增强自动驾驶汽车的LiDAR数据，实验证明其性能优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 收集真实世界的LiDAR数据耗时且受噪声和稀疏性影响，需要更好的数据增强方法来提升自动驾驶汽车感知性能。

Method: 应用去噪扩散概率模型（DDPM），采用新颖的噪声调度和时间步嵌入技术生成合成数据，并使用IAMCV和KITTI - 360数据集进行评估。

Result: 模型在大多数现有基线中表现更优，能有效缓解LiDAR数据的噪声和稀疏性问题，生成具有丰富空间关系和结构细节的多样化点云。

Conclusion: 改进的DDPM方法能生成高质量合成数据，提升计算机视觉任务特别是自动驾驶汽车感知的性能。

Abstract: Autonomous vehicles (AVs) are expected to revolutionize transportation by
improving efficiency and safety. Their success relies on 3D vision systems that
effectively sense the environment and detect traffic agents. Among sensors AVs
use to create a comprehensive view of surroundings, LiDAR provides
high-resolution depth data enabling accurate object detection, safe navigation,
and collision avoidance. However, collecting real-world LiDAR data is
time-consuming and often affected by noise and sparsity due to adverse weather
or sensor limitations. This work applies a denoising diffusion probabilistic
model (DDPM), enhanced with novel noise scheduling and time-step embedding
techniques to generate high-quality synthetic data for augmentation, thereby
improving performance across a range of computer vision tasks, particularly in
AV perception. These modifications impact the denoising process and the model's
temporal awareness, allowing it to produce more realistic point clouds based on
the projection. The proposed method was extensively evaluated under various
configurations using the IAMCV and KITTI-360 datasets, with four performance
metrics compared against state-of-the-art (SOTA) methods. The results
demonstrate the model's superior performance over most existing baselines and
its effectiveness in mitigating the effects of noisy and sparse LiDAR data,
producing diverse point clouds with rich spatial relationships and structural
detail.

</details>


### [344] [No Labels Needed: Zero-Shot Image Classification with Collaborative Self-Learning](https://arxiv.org/abs/2509.18938)
*Matheus Vinícius Todescato,Joel Luís Carbonera*

Main category: cs.CV

TL;DR: 本文提出一种零样本图像分类框架，结合VLM和预训练视觉模型，在无标注数据下训练轻量级分类器，实验显示优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 深度学习依赖大量标注数据，在数据稀缺场景受限，需解决该问题。

Method: 提出结合VLM和预训练视觉模型的零样本图像分类框架，用基于置信度的伪标签策略，在测试数据上训练轻量级分类器。

Result: 在十个不同数据集上实验，该方法优于基线零样本方法。

Conclusion: 所提方法能在无监督下捕获语义和视觉线索，减少对语义表示依赖，有效解决数据稀缺场景下图像分类问题。

Abstract: While deep learning, including Convolutional Neural Networks (CNNs) and
Vision Transformers (ViTs), has significantly advanced classification
performance, its typical reliance on extensive annotated datasets presents a
major obstacle in many practical scenarios where such data is scarce.
Vision-language models (VLMs) and transfer learning with pre-trained visual
models appear as promising techniques to deal with this problem. This paper
proposes a novel zero-shot image classification framework that combines a VLM
and a pre-trained visual model within a self-learning cycle. Requiring only the
set of class names and no labeled training data, our method utilizes a
confidence-based pseudo-labeling strategy to train a lightweight classifier
directly on the test data, enabling dynamic adaptation. The VLM identifies
high-confidence samples, and the pre-trained visual model enhances their visual
representations. These enhanced features then iteratively train the classifier,
allowing the system to capture complementary semantic and visual cues without
supervision. Notably, our approach avoids VLM fine-tuning and the use of large
language models, relying on the visual-only model to reduce the dependence on
semantic representation. Experimental evaluations on ten diverse datasets
demonstrate that our approach outperforms the baseline zero-shot method.

</details>


### [345] [VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction](https://arxiv.org/abs/2509.19002)
*Hao Wang,Eiki Murata,Lingfang Zhang,Ayako Sato,So Fukuda,Ziqi Yin,Wentao Hu,Keisuke Nakao,Yusuke Nakamura,Sebastian Zwirner,Yi-Chia Chen,Hiroyuki Otomo,Hiroki Ouchi,Daisuke Kawahara*

Main category: cs.CV

TL;DR: 提出新视频基准VIR - Bench评估多模态大语言模型时空智能，实验显示模型表现不佳，案例证明评估协议有效。


<details>
  <summary>Details</summary>
Motivation: 当前视频基准多关注室内或短程户外场景，未充分探索长途旅行挑战，掌握长时空轨迹对下一代多模态大语言模型很关键。

Method: 提出包含200个旅行视频的VIR - Bench基准，将行程重建作为评估任务。

Result: 现有多模态大语言模型在VIR - Bench上难以取得高分，基于该基准开发的旅行规划代理行程推荐有显著提升。

Conclusion: VIR - Bench能有效评估模型，评估协议可转化为面向用户应用的实际性能提升。

Abstract: Recent advances in multimodal large language models (MLLMs) have
significantly enhanced video understanding capabilities, opening new
possibilities for practical applications. Yet current video benchmarks focus
largely on indoor scenes or short-range outdoor activities, leaving the
challenges associated with long-distance travel largely unexplored. Mastering
extended geospatial-temporal trajectories is critical for next-generation
MLLMs, underpinning real-world tasks such as embodied-AI planning and
navigation. To bridge this gap, we present VIR-Bench, a novel benchmark
consisting of 200 travel videos that frames itinerary reconstruction as a
challenging task designed to evaluate and push forward MLLMs'
geospatial-temporal intelligence. Experimental results reveal that
state-of-the-art MLLMs, including proprietary ones, struggle to achieve high
scores, underscoring the difficulty of handling videos that span extended
spatial and temporal scales. Moreover, we conduct an in-depth case study in
which we develop a prototype travel-planning agent that leverages the insights
gained from VIR-Bench. The agent's markedly improved itinerary recommendations
verify that our evaluation protocol not only benchmarks models effectively but
also translates into concrete performance gains in user-facing applications.

</details>


### [346] [Citrus-V: Advancing Medical Foundation Models with Unified Medical Image Grounding for Clinical Reasoning](https://arxiv.org/abs/2509.19090)
*Guoxin Wang,Jun Zhao,Xinyi Liu,Yanbo Liu,Xuyang Cao,Chao Li,Zhuoyun Liu,Qintian Sun,Fangru Zhou,Haoqiang Xing,Zhenhong Yang*

Main category: cs.CV

TL;DR: 介绍多模态医学基础模型Citrus-V，结合图像分析与文本推理，评估显示其优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有医学成像模型泛化性有限，临床应用需精确视觉定位、多模态集成和思维链推理。

Method: 提出新颖多模态训练方法，发布涵盖多种任务的开源数据集。

Result: Citrus-V在多个基准测试中优于现有开源医学模型和专家级成像系统。

Conclusion: Citrus-V提供从视觉定位到临床推理的统一管道，支持精确病变量化、自动报告和可靠的第二意见。

Abstract: Medical imaging provides critical evidence for clinical diagnosis, treatment
planning, and surgical decisions, yet most existing imaging models are narrowly
focused and require multiple specialized networks, limiting their
generalization. Although large-scale language and multimodal models exhibit
strong reasoning and multi-task capabilities, real-world clinical applications
demand precise visual grounding, multimodal integration, and chain-of-thought
reasoning. We introduce Citrus-V, a multimodal medical foundation model that
combines image analysis with textual reasoning. The model integrates detection,
segmentation, and multimodal chain-of-thought reasoning, enabling pixel-level
lesion localization, structured report generation, and physician-like
diagnostic inference in a single framework. We propose a novel multimodal
training approach and release a curated open-source data suite covering
reasoning, detection, segmentation, and document understanding tasks.
Evaluations demonstrate that Citrus-V outperforms existing open-source medical
models and expert-level imaging systems across multiple benchmarks, delivering
a unified pipeline from visual grounding to clinical reasoning and supporting
precise lesion quantification, automated reporting, and reliable second
opinions.

</details>


### [347] [Real-time Deer Detection and Warning in Connected Vehicles via Thermal Sensing and Deep Learning](https://arxiv.org/abs/2509.18779)
*Hemanth Puppala,Wayne Sarasua,Srinivas Biyaguda,Farhad Farzinpour,Mashrur Chowdhury*

Main category: cs.CV

TL;DR: 本文提出实时检测与驾驶预警系统，集成热成像、深度学习和车联网通信，经测试性能优异，能有效减少鹿车碰撞。


<details>
  <summary>Details</summary>
Motivation: 鹿车碰撞在美国造成大量事故、伤亡和经济损失，且导致鹿种群数量下降，需要有效解决方案。

Method: 开发集成热成像、深度学习和车联网通信的系统，用超12000张热成像鹿图像自定义数据集训练和验证。

Result: 实验评估平均精度均值达98.84%，精确率95.44%，召回率95.96%；实地测试表现良好，热成像在复杂场景检测精度88 - 92%，系统端到端延迟始终低于100毫秒。

Conclusion: 研究为通过热成像和联网车辆减少鹿车碰撞建立了可行技术途径。

Abstract: Deer-vehicle collisions represent a critical safety challenge in the United
States, causing nearly 2.1 million incidents annually and resulting in
approximately 440 fatalities, 59,000 injuries, and 10 billion USD in economic
damages. These collisions also contribute significantly to declining deer
populations. This paper presents a real-time detection and driver warning
system that integrates thermal imaging, deep learning, and
vehicle-to-everything communication to help mitigate deer-vehicle collisions.
Our system was trained and validated on a custom dataset of over 12,000 thermal
deer images collected in Mars Hill, North Carolina. Experimental evaluation
demonstrates exceptional performance with 98.84 percent mean average precision,
95.44 percent precision, and 95.96 percent recall. The system was field tested
during a follow-up visit to Mars Hill and readily sensed deer providing the
driver with advanced warning. Field testing validates robust operation across
diverse weather conditions, with thermal imaging maintaining between 88 and 92
percent detection accuracy in challenging scenarios where conventional visible
light based cameras achieve less than 60 percent effectiveness. When a high
probability threshold is reached sensor data sharing messages are broadcast to
surrounding vehicles and roadside units via cellular vehicle to everything
(CV2X) communication devices. Overall, our system achieves end to end latency
consistently under 100 milliseconds from detection to driver alert. This
research establishes a viable technological pathway for reducing deer-vehicle
collisions through thermal imaging and connected vehicles.

</details>


### [348] [RoSe: Robust Self-supervised Stereo Matching under Adverse Weather Conditions](https://arxiv.org/abs/2509.19165)
*Yun Wang,Junjie Hu,Junhui Hou,Chenghao Zhang,Renwei Yang,Dapeng Oliver Wu*

Main category: cs.CV

TL;DR: 现有自监督立体匹配方法在恶劣天气下性能下降，本文提出注入视觉基础模型先验和场景对应先验的训练范式，实验效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有自监督立体匹配方法在恶劣天气下性能显著下降的问题。

Method: 向基于CNN的特征提取器注入视觉基础模型的鲁棒先验；引入场景对应先验构建监督信号；创建合成立体数据集；提出包含鲁棒自监督场景对应学习和恶劣天气蒸馏的训练范式。

Result: 所提方案在实验中表现出有效性和通用性，优于现有的自监督方法。

Conclusion: 所提的鲁棒自监督训练范式能有效提升模型在恶劣天气下的视差估计性能。

Abstract: Recent self-supervised stereo matching methods have made significant
progress, but their performance significantly degrades under adverse weather
conditions such as night, rain, and fog. We identify two primary weaknesses
contributing to this performance degradation. First, adverse weather introduces
noise and reduces visibility, making CNN-based feature extractors struggle with
degraded regions like reflective and textureless areas. Second, these degraded
regions can disrupt accurate pixel correspondences, leading to ineffective
supervision based on the photometric consistency assumption. To address these
challenges, we propose injecting robust priors derived from the visual
foundation model into the CNN-based feature extractor to improve feature
representation under adverse weather conditions. We then introduce scene
correspondence priors to construct robust supervisory signals rather than
relying solely on the photometric consistency assumption. Specifically, we
create synthetic stereo datasets with realistic weather degradations. These
datasets feature clear and adverse image pairs that maintain the same semantic
context and disparity, preserving the scene correspondence property. With this
knowledge, we propose a robust self-supervised training paradigm, consisting of
two key steps: robust self-supervised scene correspondence learning and adverse
weather distillation. Both steps aim to align underlying scene results from
clean and adverse image pairs, thus improving model disparity estimation under
adverse weather effects. Extensive experiments demonstrate the effectiveness
and versatility of our proposed solution, which outperforms existing
state-of-the-art self-supervised methods. Codes are available at
\textcolor{blue}{https://github.com/cocowy1/RoSe-Robust-Self-supervised-Stereo-Matching-under-Adverse-Weather-Conditions}.

</details>


### [349] [Generative data augmentation for biliary tract detection on intraoperative images](https://arxiv.org/abs/2509.18958)
*Cristina Iacono,Mariarosaria Meola,Federica Conte,Laura Mecozzi,Umberto Bracale,Pietro Falco,Fanny Ficuciello*

Main category: cs.CV

TL;DR: 本文利用深度学习方法解决腹腔镜胆囊切除术胆管损伤问题，构建图像数据库训练Yolo算法，用GAN生成合成数据，还讨论了实验结果和伦理考量。


<details>
  <summary>Details</summary>
Motivation: 腹腔镜胆囊切除术虽有优势，但有较高胆管损伤风险，需改善术中胆管可视化。

Method: 构建并标注图像数据库来训练Yolo检测算法，除经典数据增强技术，还提出用生成对抗网络（GAN）生成训练数据集的合成部分。

Result: 文中讨论了实验结果。

Conclusion: 未明确提及具体结论，但旨在通过深度学习方法解决胆管定位问题以避免胆管损伤。

Abstract: Cholecystectomy is one of the most frequently performed procedures in
gastrointestinal surgery, and the laparoscopic approach is the gold standard
for symptomatic cholecystolithiasis and acute cholecystitis. In addition to the
advantages of a significantly faster recovery and better cosmetic results, the
laparoscopic approach bears a higher risk of bile duct injury, which has a
significant impact on quality of life and survival. To avoid bile duct injury,
it is essential to improve the intraoperative visualization of the bile duct.
This work aims to address this problem by leveraging a deep-learning approach
for the localization of the biliary tract from white-light images acquired
during the surgical procedures. To this end, the construction and annotation of
an image database to train the Yolo detection algorithm has been employed.
Besides classical data augmentation techniques, the paper proposes Generative
Adversarial Network (GAN) for the generation of a synthetic portion of the
training dataset. Experimental results have been discussed along with ethical
considerations.

</details>


### [350] [HyKid: An Open MRI Dataset with Expert-Annotated Multi-Structure and Choroid Plexus in Pediatric Hydrocephalus](https://arxiv.org/abs/2509.19218)
*Yunzhi Xu,Yushuang Ding,Hu Sun,Hongxi Zhang,Li Zhao*

Main category: cs.CV

TL;DR: 提出开源数据集HyKid用于儿童脑积水评估，展示其构建与应用，发现脉络丛体积与脑脊液总体积强相关可作生物标志物，数据集公开可用。


<details>
  <summary>Details</summary>
Motivation: 儿童脑积水评估有挑战，缺乏公开、专家标注数据集，尤其是含脉络丛分割的数据集。

Method: 从48名儿科脑积水患者获取数据，用切片到体积算法从常规低分辨率图像重建3D MRI，由经验丰富神经科医生手动校正脑组织分割，用检索增强生成框架从临床报告提取结构化数据。

Result: 脉络丛体积和总脑脊液体积强相关，在预测模型中表现出色（AUC = 0.87）。

Conclusion: HyKid数据集为神经影像算法开发提供高质量基准，揭示脑积水评估中脉络丛相关特征。

Abstract: Evaluation of hydrocephalus in children is challenging, and the related
research is limited by a lack of publicly available, expert-annotated datasets,
particularly those with segmentation of the choroid plexus. To address this, we
present HyKid, an open-source dataset from 48 pediatric patients with
hydrocephalus. 3D MRIs were provided with 1mm isotropic resolution, which was
reconstructed from routine low-resolution images using a slice-to-volume
algorithm. Manually corrected segmentations of brain tissues, including white
matter, grey matter, lateral ventricle, external CSF, and the choroid plexus,
were provided by an experienced neurologist. Additionally, structured data was
extracted from clinical radiology reports using a Retrieval-Augmented
Generation framework. The strong correlation between choroid plexus volume and
total CSF volume provided a potential biomarker for hydrocephalus evaluation,
achieving excellent performance in a predictive model (AUC = 0.87). The
proposed HyKid dataset provided a high-quality benchmark for neuroimaging
algorithms development, and it revealed the choroid plexus-related features in
hydrocephalus assessments. Our datasets are publicly available at
https://www.synapse.org/Synapse:syn68544889.

</details>


### [351] [MsFIN: Multi-scale Feature Interaction Network for Traffic Accident Anticipation](https://arxiv.org/abs/2509.19227)
*Tongshuai Wu,Chao Lu,Ze Song,Yunlong Lin,Sizhe Fan,Xuemei Chen*

Main category: cs.CV

TL;DR: 本文提出MsFIN网络用于行车记录仪视频的事故早期预测，该网络分三层处理特征，实验表明其性能优于单尺度特征提取模型。


<details>
  <summary>Details</summary>
Motivation: 行车记录仪普及使得从其视角开发事故预测模型至关重要，但存在建模交通参与者特征交互和捕捉复杂多时间行为线索两个挑战。

Method: 提出MsFIN网络，包含多尺度特征聚合、时间特征处理和多尺度特征后融合三层，用多尺度模块提取不同时间尺度场景表征，利用Transformer架构促进特征交互。

Result: 在DAD和DADA数据集上，MsFIN在预测正确性和及时性上显著优于单尺度特征提取的先进模型，消融实验验证各模块有效性。

Conclusion: MsFIN通过多尺度特征融合和上下文交互建模实现了优越性能。

Abstract: With the widespread deployment of dashcams and advancements in computer
vision, developing accident prediction models from the dashcam perspective has
become critical for proactive safety interventions. However, two key challenges
persist: modeling feature-level interactions among traffic participants (often
occluded in dashcam views) and capturing complex, asynchronous multi-temporal
behavioral cues preceding accidents. To deal with these two challenges, a
Multi-scale Feature Interaction Network (MsFIN) is proposed for early-stage
accident anticipation from dashcam videos. MsFIN has three layers for
multi-scale feature aggregation, temporal feature processing and multi-scale
feature post fusion, respectively. For multi-scale feature aggregation, a
Multi-scale Module is designed to extract scene representations at short-term,
mid-term and long-term temporal scales. Meanwhile, the Transformer architecture
is leveraged to facilitate comprehensive feature interactions. Temporal feature
processing captures the sequential evolution of scene and object features under
causal constraints. In the multi-scale feature post fusion stage, the network
fuses scene and object features across multiple temporal scales to generate a
comprehensive risk representation. Experiments on DAD and DADA datasets show
that MsFIN significantly outperforms state-of-the-art models with single-scale
feature extraction in both prediction correctness and earliness. Ablation
studies validate the effectiveness of each module in MsFIN, highlighting how
the network achieves superior performance through multi-scale feature fusion
and contextual interaction modeling.

</details>


### [352] [Adversarially-Refined VQ-GAN with Dense Motion Tokenization for Spatio-Temporal Heatmaps](https://arxiv.org/abs/2509.19252)
*Gabriel Maldonado,Narges Rashvand,Armin Danesh Pazho,Ghazal Alinezhad Noghre,Vinit Katariya,Hamed Tabkhi*

Main category: cs.CV

TL;DR: 提出带密集运动标记化的对抗性细化VQ - GAN框架压缩时空热图，实验证明其优越性并分析运动复杂度。


<details>
  <summary>Details</summary>
Motivation: 连续人体运动理解因高维性和冗余性是计算机视觉核心挑战，需高效压缩和表示。

Method: 引入带密集运动标记化的对抗性细化VQ - GAN框架，结合密集运动标记化与对抗性细化。

Result: 在CMU Panoptic数据集上实验，SSIM比dVAE基线高9.31%，减少37.1%时间不稳定性；发现2D和3D运动最佳表示所需的标记数量。

Conclusion: 方法在不同运动分析应用中有实际部署可行性。

Abstract: Continuous human motion understanding remains a core challenge in computer
vision due to its high dimensionality and inherent redundancy. Efficient
compression and representation are crucial for analyzing complex motion
dynamics. In this work, we introduce an adversarially-refined VQ-GAN framework
with dense motion tokenization for compressing spatio-temporal heatmaps while
preserving the fine-grained traces of human motion. Our approach combines dense
motion tokenization with adversarial refinement, which eliminates
reconstruction artifacts like motion smearing and temporal misalignment
observed in non-adversarial baselines. Our experiments on the CMU Panoptic
dataset provide conclusive evidence of our method's superiority, outperforming
the dVAE baseline by 9.31% SSIM and reducing temporal instability by 37.1%.
Furthermore, our dense tokenization strategy enables a novel analysis of motion
complexity, revealing that 2D motion can be optimally represented with a
compact 128-token vocabulary, while 3D motion's complexity demands a much
larger 1024-token codebook for faithful reconstruction. These results establish
practical deployment feasibility across diverse motion analysis applications.
The code base for this work is available at
https://github.com/TeCSAR-UNCC/Pose-Quantization.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [353] [The Role of Informal Care in Cognitive Outcome and Healthcare Utilization Among Older Adults with Dementia](https://arxiv.org/abs/2509.18468)
*Mohammad Abdullah Al Faisal*

Main category: econ.GN

TL;DR: 本文研究老年痴呆患者非正规照护与认知功能和医疗利用的关系，用HRS数据估计模型，发现非正规照护对认知无显著因果影响，但能减少机构护理使用，对医院使用等无强因果效应。


<details>
  <summary>Details</summary>
Motivation: 探究老年痴呆患者非正规照护与认知功能和医疗利用之间的关系。

Method: 使用美国50岁以上成年人的HRS数据，估计普通最小二乘法（OLS）和工具变量（IV）模型，用子女数量作为非正规照护强度的工具变量。

Result: OLS估计显示非正规照护与认知呈负相关，IV估计控制相关变量后无显著因果效应；IV结果表明非正规照护显著降低使用养老院、机构住宿天数和机构化概率，对医院使用等无强因果效应，但与家庭健康服务有互补关系。

Conclusion: 非正规照护可替代机构护理，对痴呆患者长期护理政策很重要。

Abstract: This paper examines the relationship between informal caregiving and both
cognitive functioning and healthcare utilization among older adults with
dementia. Using data from the RAND version of the Health and Retirement Study
(HRS), a nationally representative longitudinal panel of U.S. adults over age
50, covering the years 2010 to 2022, I estimate Ordinary Least Squares (OLS)
and Instrumental Variables (IV) models to address potential endogeneity in
caregiving decisions. The number of children is employed as an instrument for
informal care intensity. While OLS estimates suggest a negative association
between informal caregiving and cognition, IV estimates show no significant
causal effect after controlling for demographic, socioeconomic, and lagged
cognition variables. In contrast, IV results indicate that informal care
significantly reduces the likelihood of nursing home use, the number of
institutional nights, and the probability of institutionalization. No robust
causal effects are found for hospital use, doctor visits, or outpatient
surgery, although there is some suggestive evidence of a complementary
relationship between informal care and home health services. These findings
highlight the role of informal caregiving in substituting for institutional
care and underscore its importance in long-term care policy for dementia
patients. Keywords: Informal Caregiving; Cognitive Decline; Instrumental
Variables; Healthcare Utilization: Dementia Patients.

</details>


### [354] [Predicting Credit Spreads and Ratings with Machine Learning: The Role of Non-Financial Data](https://arxiv.org/abs/2509.19042)
*Yanran Wu,Xinlei Zhang,Quanyi Xu,Qianxin Yang,Chao Zhang*

Main category: econ.GN

TL;DR: 构建综合信用风险指标集，用机器学习模型预测债券信用利差，验证评级预测效果，结果显示模型优于评级机构，非财务指标重要，还首创信用评级模型，提供债券预警等指导。


<details>
  <summary>Details</summary>
Motivation: 提高债券信用利差预测和信用评级的准确性，为债券违约预警、信用评级和金融稳定提供支持。

Method: 构建167指标综合信用风险指标集，运用七种机器学习模型构建债券信用利差预测模型，进行机制分析和评级预测验证。

Result: 模型在解释信用利差上优于中国信用评级机构，添加非财务指标使样本外表现翻倍，非财务指标更重要，首创信用评级模型准确率等超75%。

Conclusion: 本文为债券违约预警、信用评级和金融稳定性提供了有价值的指导。

Abstract: We build a 167-indicator comprehensive credit risk indicator set, integrating
macro, corporate financial, bond-specific indicators, and for the first time,
30 large-scale corporate non-financial indicators. We use seven machine
learning models to construct a bond credit spread prediction model, test their
spread predictive power and economic mechanisms, and verify their credit rating
prediction effectiveness. Results show these models outperform Chinese credit
rating agencies in explaining credit spreads. Specially, adding non-financial
indicators more than doubles their out-of-sample performance vs. traditional
feature-driven models. Mechanism analysis finds non-financial indicators far
more important than traditional ones (macro-level, financial, bond
features)-seven of the top 10 are non-financial (e.g., corporate governance,
property rights nature, information disclosure evaluation), the most stable
predictors. Models identify high-risk traits (deteriorating operations,
short-term debt, higher financing constraints) via these indicators for spread
prediction and risk identification. Finally, we pioneer a credit rating model
using predicted spreads (predicted implied rating model), with
full/sub-industry models achieving over 75% accuracy, recall, F1. This paper
provides valuable guidance for bond default early warning, credit rating, and
financial stability.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [355] [Enhanced Interpretable Knowledge Tracing for Students Performance Prediction with Human understandable Feature Space](https://arxiv.org/abs/2509.18231)
*Sein Minn,Roger Nkambou*

Main category: cs.CY

TL;DR: 传统KT模型缺乏可解释性，本文通过探索学生交互数据中的可理解特征，增强了可解释的KT模型，平衡了预测能力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 深度学习KT模型复杂且不透明，难以提供心理学上有意义的解释，与认知理论脱节，限制了其在教育应用中的可信度。

Method: 探索学生交互数据中的可理解特征，纳入反映学生学习能力的额外特征。

Result: 增强的方法在保持与认知理论一致的同时提高了预测准确性。

Conclusion: 平衡了预测能力和可解释性，推进了自适应学习系统的实用性。

Abstract: Knowledge Tracing (KT) plays a central role in assessing students skill
mastery and predicting their future performance. While deep learning based KT
models achieve superior predictive accuracy compared to traditional methods,
their complexity and opacity hinder their ability to provide psychologically
meaningful explanations. This disconnect between model parameters and cognitive
theory poses challenges for understanding and enhancing the learning process,
limiting their trustworthiness in educational applications. To address these
challenges, we enhance interpretable KT models by exploring
human-understandable features derived from students interaction data. By
incorporating additional features, particularly those reflecting students
learning abilities, our enhanced approach improves predictive accuracy while
maintaining alignment with cognitive theory. Our contributions aim to balance
predictive power with interpretability, advancing the utility of adaptive
learning systems.

</details>


### [356] [Perceptions of AI Across Sectors: A Comparative Review of Public Attitudes](https://arxiv.org/abs/2509.18233)
*Filip Bialy,Mark Elliot,Robert Meckin*

Main category: cs.CY

TL;DR: 本文对2011 - 2025年251项公众对AI态度的研究进行领域介导的比较综述，分析多因素对公众接受或抵制AI的影响，指出公众对AI的看法受多方面因素影响，为负责任的AI治理策略提供基础。


<details>
  <summary>Details</summary>
Motivation: 了解不同因素如何在各领域和用例中塑造公众对人工智能的接受或抵制态度。

Method: 采用系统文献综述方法，分析不同因素对公众态度的影响。

Result: 发现影响公众对AI看法的个人、情境和技术因素存在反复出现的模式，也追踪到机构信任、感知公平和伦理担忧的差异。

Conclusion: 公众对AI的看法不仅受技术设计或性能影响，还受特定部门因素、想象、文化叙事和历史遗产影响，该比较方法为负责任的AI治理提供基础。

Abstract: This paper offers a domain-mediated comparative review of 251 studies on
public attitudes toward AI, published between 2011 and 2025. Drawing on a
systematic literature review, we analyse how different factors including
perceived benefits and concerns (or risks) shape public acceptance of - or
resistance to - artificial intelligence across domains and use-cases, including
healthcare, education, security, public administration, generative AI, and
autonomous vehicles. The analysis highlights recurring patterns in individual,
contextual, and technical factors influencing perception, while also tracing
variations in institutional trust, perceived fairness, and ethical concerns. We
show that the public perception in AI is shaped not only by technical design or
performance but also by sector-specific considerations as well as imaginaries,
cultural narratives, and historical legacies. This comparative approach offers
a foundation for developing more tailored and context-sensitive strategies for
responsible AI governance.

</details>


### [357] [An Artificial Intelligence Value at Risk Approach: Metrics and Models](https://arxiv.org/abs/2509.18394)
*Luis Enriquez Alvarez*

Main category: cs.CY

TL;DR: 文章指出人工智能风险多维，现有风险管理不成熟，需定制指标和模型，目的是为利益相关者提供AI风险管理指引，呈现风险相互依赖及建模方法。


<details>
  <summary>Details</summary>
Motivation: 当前人工智能风险管理因新法规而不成熟，缺乏有实际价值的指南，且各部门对AI技术了解不足，需为利益相关者提供AI风险管理指引。

Method: 将人工智能风险分解为数据保护、公平性、准确性、鲁棒性和信息安全等维度，开发合适指标和风险模型。

Result: 无明确提及具体结果

Conclusion: 需要开发合适指标和风险模型来降低决策不确定性，文章将提供AI风险相互依赖的整体概述及建模方法。

Abstract: Artificial intelligence risks are multidimensional in nature, as the same
risk scenarios may have legal, operational, and financial risk dimensions. With
the emergence of new AI regulations, the state of the art of artificial
intelligence risk management seems to be highly immature due to upcoming AI
regulations. Despite the appearance of several methodologies and generic
criteria, it is rare to find guidelines with real implementation value,
considering that the most important issue is customizing artificial
intelligence risk metrics and risk models for specific AI risk scenarios.
Furthermore, the financial departments, legal departments and Government Risk
Compliance teams seem to remain unaware of many technical aspects of AI
systems, in which data scientists and AI engineers emerge as the most
appropriate implementers. It is crucial to decompose the problem of artificial
intelligence risk in several dimensions: data protection, fairness, accuracy,
robustness, and information security. Consequently, the main task is developing
adequate metrics and risk models that manage to reduce uncertainty for
decision-making in order to take informed decisions concerning the risk
management of AI systems.
  The purpose of this paper is to orientate AI stakeholders about the depths of
AI risk management. Although it is not extremely technical, it requires a basic
knowledge of risk management, quantifying uncertainty, the FAIR model, machine
learning, large language models and AI context engineering. The examples
presented pretend to be very basic and understandable, providing simple ideas
that can be developed regarding specific AI customized environments. There are
many issues to solve in AI risk management, and this paper will present a
holistic overview of the inter-dependencies of AI risks, and how to model them
together, within risk scenarios.

</details>


### [358] [Automatic coherence-driven inference on arguments](https://arxiv.org/abs/2509.18523)
*Steve Huntsman*

Main category: cs.CY

TL;DR: 本文提出用大语言模型进行连贯性驱动推理（CDI）的技术方案，以处理法律等领域的不一致性问题。


<details>
  <summary>Details</summary>
Motivation: 法律、行政和法理学中普遍存在不一致性，期望找到技术解决方案。

Method: 利用大语言模型准确提取论点中的命题，并将其编译成自然数据结构，通过组合优化实现连贯性驱动推理（CDI）。

Result: 该神经符号架构能自然分离关注点，对论点的连贯性做出有意义的判断。

Conclusion: 该技术方案可用于立法、政策分析和法律推理。

Abstract: Inconsistencies are ubiquitous in law, administration, and jurisprudence.
Though a cure is too much to hope for, we propose a technological remedy. Large
language models (LLMs) can accurately extract propositions from arguments and
compile them into natural data structures that enable coherence-driven
inference (CDI) via combinatorial optimization. This neurosymbolic architecture
naturally separates concerns and enables meaningful judgments about the
coherence of arguments that can inform legislative and policy analysis and
legal reasoning.

</details>


### [359] [Learning Progression-Guided AI Evaluation of Scientific Models To Support Diverse Multi-Modal Understanding in NGSS Classroom](https://arxiv.org/abs/2509.18157)
*Leonora Kaldaras,Tingting Li,Prudence Djagba,Kevin Haudek,Joseph Krajcik*

Main category: cs.CY

TL;DR: 本文基于验证过的多模态学习进阶（LPs）及相关评估，用机器学习评估与LPs对齐的科学模型和文本解释，展示LP如何指导个性化反馈设计。


<details>
  <summary>Details</summary>
Motivation: 若LPs能反映对被测量概念的多样思维方式且相关评估能有效衡量这种多样性，就能根据学习者需求调整教学；培养学生科学理解需支持其多模态表达；支持多元背景学生建模技能对公平科学评估很重要；传统评分耗时长。

Method: 利用验证过的与NGSS对齐的多模态LPs及相关评估，运用机器学习评估与LPs对齐的科学模型和短文本解释。

Result: 展示了LP如何指导基于学生思维多样性的个性化机器学习驱动反馈设计。

Conclusion: LP可指导个性化机器学习驱动反馈设计，有助于根据学生多样思维调整教学，促进公平科学评估。

Abstract: Learning Progressions (LPs) can help adjust instruction to individual
learners needs if the LPs reflect diverse ways of thinking about a construct
being measured, and if the LP-aligned assessments meaningfully measure this
diversity. The process of doing science is inherently multi-modal with
scientists utilizing drawings, writing and other modalities to explain
phenomena. Thus, fostering deep science understanding requires supporting
students in using multiple modalities when explaining phenomena. We build on a
validated NGSS-aligned multi-modal LP reflecting diverse ways of modeling and
explaining electrostatic phenomena and associated assessments. We focus on
students modeling, an essential practice for building a deep science
understanding. Supporting culturally and linguistically diverse students in
building modeling skills provides them with an alternative mode of
communicating their understanding, essential for equitable science assessment.
Machine learning (ML) has been used to score open-ended modeling tasks (e.g.,
drawings), and short text-based constructed scientific explanations, both of
which are time- consuming to score. We use ML to evaluate LP-aligned scientific
models and the accompanying short text-based explanations reflecting
multi-modal understanding of electrical interactions in high school Physical
Science. We show how LP guides the design of personalized ML-driven feedback
grounded in the diversity of student thinking on both assessment modes.

</details>


### [360] [Large-Scale, Longitudinal Study of Large Language Models During the 2024 US Election Season](https://arxiv.org/abs/2509.18446)
*Sarah H. Cen,Andrew Ilyas,Hedi Driss,Charlotte Park,Aspen Hopkins,Chara Podimata,Aleksander Mądry*

Main category: cs.CY

TL;DR: 对2024年美国大选期间12个大语言模型进行大规模纵向研究，分析其行为、对选举的影响等，并公开数据集。


<details>
  <summary>Details</summary>
Motivation: 大语言模型普及后，探讨其对2024年美国大选信息生态和政治话语的影响，以及平台选举保障措施的实际效果。

Method: 对12个模型进行近每日的结构化调查，系统性改变内容和格式，收集超1.2万个问题的回答，后期对数据集进行四项分析。

Result: 得到了关于模型行为随时间变化、对引导的敏感性、对指令的响应性、选举相关知识和“信念”等方面的结果。

Conclusion: 详细介绍研究方法，公开数据集，以促进未来对选举背景下大语言模型的评估。

Abstract: The 2024 US presidential election is the first major contest to occur in the
US since the popularization of large language models (LLMs). Building on
lessons from earlier shifts in media (most notably social media's well studied
role in targeted messaging and political polarization) this moment raises
urgent questions about how LLMs may shape the information ecosystem and
influence political discourse. While platforms have announced some election
safeguards, how well they work in practice remains unclear. Against this
backdrop, we conduct a large-scale, longitudinal study of 12 models, queried
using a structured survey with over 12,000 questions on a near-daily cadence
from July through November 2024. Our design systematically varies content and
format, resulting in a rich dataset that enables analyses of the models'
behavior over time (e.g., across model updates), sensitivity to steering,
responsiveness to instructions, and election-related knowledge and "beliefs."
In the latter half of our work, we perform four analyses of the dataset that
(i) study the longitudinal variation of model behavior during election season,
(ii) illustrate the sensitivity of election-related responses to demographic
steering, (iii) interrogate the models' beliefs about candidates' attributes,
and (iv) reveal the models' implicit predictions of the election outcome. To
facilitate future evaluations of LLMs in electoral contexts, we detail our
methodology, from question generation to the querying pipeline and third-party
tooling. We also publicly release our dataset at
https://huggingface.co/datasets/sarahcen/llm-election-data-2024

</details>


### [361] [The AI Literacy Heptagon: A Structured Approach to AI Literacy in Higher Education](https://arxiv.org/abs/2509.18900)
*Veronika Hackl,Alexandra Mueller,Maximilian Sailer*

Main category: cs.CY

TL;DR: 本文通过分析2021 - 2024年文献，探讨高等教育中AI素养概念与实施，确定七个核心维度并提出AI素养七边形，弥合理论与实践差距。


<details>
  <summary>Details</summary>
Motivation: 弥合人工智能素养（AIL）理论概念与学术课程实际实施之间的差距，推动AIL在高等教育（HE）中的应用。

Method: 对2021 - 2024年相关出版物进行分析，探讨AIL定义、综合定义及转化为教育实践的方法。

Result: 确定AIL的七个核心维度，合成AI素养七边形，加深概念理解并支持AIL在HE中的结构化发展。

Conclusion: 研究有助于将AIL理论概念有效转化到学术课程的实际实施中。

Abstract: The integrative literature review addresses the conceptualization and
implementation of AI Literacy (AIL) in Higher Education (HE) by examining
recent research literature. Through an analysis of publications (2021-2024), we
explore (1) how AIL is defined and conceptualized in current research,
particularly in HE, and how it can be delineated from related concepts such as
Data Literacy, Media Literacy, and Computational Literacy; (2) how various
definitions can be synthesized into a comprehensive working definition, and (3)
how scientific insights can be effectively translated into educational
practice. Our analysis identifies seven central dimensions of AIL: technical,
applicational, critical thinking, ethical, social, integrational, and legal.
These are synthesized in the AI Literacy Heptagon, deepening conceptual
understanding and supporting the structured development of AIL in HE. The study
aims to bridge the gap between theoretical AIL conceptualizations and the
practical implementation in academic curricula.

</details>


### [362] [A Mega-Study of Digital Twins Reveals Strengths, Weaknesses and Opportunities for Further Improvement](https://arxiv.org/abs/2509.19088)
*Tiany Peng,George Gui,Daniel J. Merlau,Grace Jiarui Fan,Malek Ben Sliman,Melanie Brucks,Eric J. Johnson,Vicki Morwitz,Abdullah Althenayyan,Silvia Bellezza,Dante Donati,Hortense Fong,Elizabeth Friedman,Ariana Guevara,Mohamed Hussein,Kinshuk Jerath,Bruce Kogut,Kristen Lane,Hannah Li,Patryk Perkowski,Oded Netzer,Olivier Toubia*

Main category: cs.CY

TL;DR: 本文通过19项预注册研究，比较美国全国小组及其大语言模型驱动的数字替身的回答，发现数字替身能捕捉一定相对差异，但个体预测和样本均值方差估计不可靠。


<details>
  <summary>Details</summary>
Motivation: 探究数字替身能否在调查和实验中捕捉个体反应。

Method: 在美国全国小组及其基于大量个体层面数据构建的大语言模型驱动的数字替身开展19项预注册研究，比较两者在164个结果上的回答。

Result: 数字替身与人类回答的相关性一般，数字替身回答变异性更小；构建数字替身能捕捉参与者异质性和相对差异，但不能大幅提升预测特定参与者确切答案和总体均值的能力；数字替身的表现因领域而异，在高学历、高收入和意识形态温和的参与者中表现更好。

Conclusion: 当前数字替身有一定作用，但用于个体预测和样本均值方差估计不可靠，使用前需仔细验证，数据和代码公开。

Abstract: Do "digital twins" capture individual responses in surveys and experiments?
We run 19 pre-registered studies on a national U.S. panel and their LLM-powered
digital twins (constructed based on previously-collected extensive
individual-level data) and compare twin and human answers across 164 outcomes.
The correlation between twin and human answers is modest (approximately 0.2 on
average) and twin responses are less variable than human responses. While
constructing digital twins based on rich individual-level data improves our
ability to capture heterogeneity across participants and predict relative
differences between them, it does not substantially improve our ability to
predict the exact answers given by specific participants or enhance predictions
of population means. Twin performance varies by domain and is higher among more
educated, higher-income, and ideologically moderate participants. These results
suggest current digital twins can capture some degree of relative differences
but are unreliable for individual-level predictions and sample mean and
variance estimation, underscoring the need for careful validation before use.
Our data and code are publicly available for researchers and practitioners
interested in optimizing digital twin pipelines.

</details>


### [363] [Generative Propaganda](https://arxiv.org/abs/2509.19147)
*Madeleine I. G. Daepp,Alejandro Cuevas,Robert Osazuwa Ness,Vickie Yu-Ping Wang,Bharat Kumar Nayak,Dibyendu Mishra,Ti-Chung Cheng,Shaily Desai,Joyojeet Pal*

Main category: cs.CY

TL;DR: 研究生成式宣传（利用生成式AI塑造舆论）在现实场景的应用，发现“深度伪造”影响大，开发分类法，指出AI用途特点，建议安全研究人员重新考虑威胁模型。


<details>
  <summary>Details</summary>
Motivation: 刻画生成式宣传在现实世界场景中的使用情况。

Method: 对中国台湾地区和印度的防御者（事实核查员、记者等）以及印度的创作者（网红、政治顾问等）进行访谈。

Result: “深度伪造”对防御者预期和干预重点影响大；开发了区分不同类型生成式宣传的分类法；AI主要用于提高跨语言和模式沟通效率、躲避检测，印度创作者重说服，台湾防御者视欺骗为战略叙事扭曲的一部分。

Conclusion: 安全研究人员应重新考虑威胁模型，区分不同用途，补充和加强限制内部人员滥用的社会因素，在全球应对效率提升问题。

Abstract: Generative propaganda is the use of generative artificial intelligence (AI)
to shape public opinion. To characterize its use in real-world settings, we
conducted interviews with defenders (e.g., factcheckers, journalists,
officials) in Taiwan and creators (e.g., influencers, political consultants,
advertisers) as well as defenders in India, centering two places characterized
by high levels of online propaganda. The term "deepfakes", we find, exerts
outsized discursive power in shaping defenders' expectations of misuse and, in
turn, the interventions that are prioritized. To better characterize the space
of generative propaganda, we develop a taxonomy that distinguishes between
obvious versus hidden and promotional versus derogatory use. Deception was
neither the main driver nor the main impact vector of AI's use; instead, Indian
creators sought to persuade rather than to deceive, often making AI's use
obvious in order to reduce legal and reputational risks, while Taiwan's
defenders saw deception as a subset of broader efforts to distort the
prevalence of strategic narratives online. AI was useful and used, however, in
producing efficiency gains in communicating across languages and modes, and in
evading human and algorithmic detection. Security researchers should reconsider
threat models to clearly differentiate deepfakes from promotional and obvious
uses, to complement and bolster the social factors that constrain misuse by
internal actors, and to counter efficiency gains globally.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [364] [A Fast Initialization Method for Neural Network Controllers: A Case Study of Image-based Visual Servoing Control for the multicopter Interception](https://arxiv.org/abs/2509.19110)
*Chenxu Ke,Congling Tian,Kaichen Xu,Ye Li,Lingcong Bao*

Main category: eess.SY

TL;DR: 本文提出神经网络快速初始化方法，构建符合稳定条件的数据集实现神经网络控制策略的初始训练，并通过多旋翼拦截的视觉伺服控制案例验证其有效性，实验中训练后的控制策略最终拦截速度达15 m/s。


<details>
  <summary>Details</summary>
Motivation: 强化学习控制器设计初始训练需大量数据，训练随机性强、收敛慢；基于李雅普诺夫稳定性理论的方法需初始稳定的神经网络控制策略；传统控制理论设计稳定控制器要求设计者有大量控制设计知识，为解决这些问题提出新方法。

Method: 基于系统模型构建符合稳定条件的数据集，实现神经网络控制策略的初始训练。

Result: 以多旋翼拦截的图像视觉伺服控制为案例进行仿真和实验，训练后的控制策略最终拦截速度达到15 m/s。

Conclusion: 所提出的神经网络快速初始化方法有效且具有实际应用性能。

Abstract: Reinforcement learning-based controller design methods often require
substantial data in the initial training phase. Moreover, the training process
tends to exhibit strong randomness and slow convergence. It often requires
considerable time or high computational resources. Another class of
learning-based method incorporates Lyapunov stability theory to obtain a
control policy with stability guarantees. However, these methods generally
require an initially stable neural network control policy at the beginning of
training. Evidently, a stable neural network controller can not only serve as
an initial policy for reinforcement learning, allowing the training to focus on
improving controller performance, but also act as an initial state for
learning-based Lyapunov control methods. Although stable controllers can be
designed using traditional control theory, designers still need to have a great
deal of control design knowledge to address increasingly complicated control
problems. The proposed neural network rapid initialization method in this paper
achieves the initial training of the neural network control policy by
constructing datasets that conform to the stability conditions based on the
system model. Furthermore, using the image-based visual servoing control for
multicopter interception as a case study, simulations and experiments were
conducted to validate the effectiveness and practical performance of the
proposed method. In the experiment, the trained control policy attains a final
interception velocity of 15 m/s.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [365] [CPCLDETECTOR: Knowledge Enhancement and Alignment Selection for Chinese Patronizing and Condescending Language Detection](https://arxiv.org/abs/2509.18562)
*Jiaxun Yang,Yifei Han,Long Zhang,Liu Yujie,Bin Li,Bo Gao,Yangfan He,Kejia Zhan*

Main category: cs.MM

TL;DR: 本文重构含103k评论条目的数据集PCLMMPLUS，提出CPCLDetector模型，实验表明其性能优于SOTA，可更准确检测CPLC视频。


<details>
  <summary>Details</summary>
Motivation: 现有数据集缺乏用户评论，影响模型对视频内容理解，导致部分CPLC视频检测失败。

Method: 重构新数据集PCLMMPLUS，提出带对齐选择和知识增强评论内容模块的CPCLDetector模型。

Result: CPCLDetector模型在PCLMM上优于SOTA，在PCLMMPLUS上性能更高，能更准确检测CPLC视频。

Conclusion: 该研究支持内容治理，保护弱势群体，代码和数据集可在指定链接获取。

Abstract: Chinese Patronizing and Condescending Language (CPCL) is an implicitly
discriminatory toxic speech targeting vulnerable groups on Chinese video
platforms. The existing dataset lacks user comments, which are a direct
reflection of video content. This undermines the model's understanding of video
content and results in the failure to detect some CPLC videos. To make up for
this loss, this research reconstructs a new dataset PCLMMPLUS that includes
103k comment entries and expands the dataset size. We also propose the
CPCLDetector model with alignment selection and knowledge-enhanced comment
content modules. Extensive experiments show the proposed CPCLDetector
outperforms the SOTA on PCLMM and achieves higher performance on PCLMMPLUS .
CPLC videos are detected more accurately, supporting content governance and
protecting vulnerable groups. Code and dataset are available at
https://github.com/jiaxunyang256/PCLD.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [366] [Weight Mapping Properties of a Dual Tree Single Clock Adiabatic Capacitive Neuron](https://arxiv.org/abs/2509.18143)
*Mike Smart,Sachin Maheshwari,Himadri Singh Raghav,Alexander Serb*

Main category: cs.ET

TL;DR: 本文探讨人工神经元权重映射到绝热电容神经元电容值的问题，提出优化方法，展示功能等效性并研究权重量化影响。


<details>
  <summary>Details</summary>
Motivation: DTSC ACN电路用于ANN计算有节能潜力，但人工神经元权重到ACN电容值的映射未充分研究。

Method: 提出最优的AN到ACN方法，用TensorFlow和Larq框架训练三种ANN网络并映射权重。

Result: 实现100%功能等效性。

Conclusion: 该方法促进芯片尺寸减小和分类精度提高，还研究了权重量化对ACN性能的影响。

Abstract: Dual Tree Single Clock (DTSC) Adiabatic Capacitive Neuron (ACN) circuits
offer the potential for highly energy-efficient Artificial Neural Network (ANN)
computation in full custom analog IC designs. The efficient mapping of
Artificial Neuron (AN) abstract weights, extracted from the software-trained
ANNs, onto physical ACN capacitance values has, however, yet to be fully
researched. In this paper, we explore the unexpected hidden complexities,
challenges and properties of the mapping, as well as, the ramifications for IC
designers in terms accuracy, design and implementation. We propose an optimal,
AN to ACN methodology, that promotes smaller chip sizes and improved overall
classification accuracy, necessary for successful practical deployment. Using
TensorFlow and Larq software frameworks, we train three different ANN networks
and map their weights into the energy-efficient DTSC ACN capacitance value
domain to demonstrate 100% functional equivalency. Finally, we delve into the
impact of weight quantization on ACN performance using novel metrics related to
practical IC considerations, such as IC floor space and comparator
decision-making efficacy.

</details>


### [367] [Energy-convergence trade off for the training of neural networks on bio-inspired hardware](https://arxiv.org/abs/2509.18121)
*Nikhil Garg,Paul Uriarte Vicandi,Yanming Zhang,Alexandre Baigol,Donato Francesco Falcone,Saketh Ram Mamidala,Bert Jan Offrein,Laura Bégon-Lours*

Main category: cs.ET

TL;DR: 研究基于HfO2/ZrO2超晶格的铁电突触器件，在不同脉冲宽度下测试其性能，分析精度问题并提出解决技术，表明短脉冲编程结合定制训练可提升片上学习效率。


<details>
  <summary>Details</summary>
Motivation: 可穿戴和植入设备的发展使AI处理需求转向极端边缘，需要超低功耗，忆阻器虽有潜力但平衡性能和能效仍具挑战。

Method: 研究基于HfO2/ZrO2超晶格的铁电突触器件，将实验测量的权重更新输入硬件感知的神经网络模拟，分析精度下降原因并提出“对称点移动”技术。

Result: 短脉冲降低每次更新能量，减少总能量且不牺牲精度；普通SGD分类精度低于混合精度SGD；“对称点移动”技术可解决不对称更新并恢复精度。

Conclusion: 精度、收敛速度和能源使用之间存在权衡，短脉冲编程结合定制训练可显著提高片上学习效率。

Abstract: The increasing deployment of wearable sensors and implantable devices is
shifting AI processing demands to the extreme edge, necessitating ultra-low
power for continuous operation. Inspired by the brain, emerging memristive
devices promise to accelerate neural network training by eliminating costly
data transfers between compute and memory. Though, balancing performance and
energy efficiency remains a challenge. We investigate ferroelectric synaptic
devices based on HfO2/ZrO2 superlattices and feed their experimentally measured
weight updates into hardware-aware neural network simulations. Across pulse
widths from 20 ns to 0.2 ms, shorter pulses lower per-update energy but require
more training epochs while still reducing total energy without sacrificing
accuracy. Classification accuracy using plain stochastic gradient descent (SGD)
is diminished compared to mixed-precision SGD. We analyze the causes and
propose a ``symmetry point shifting'' technique, addressing asymmetric updates
and restoring accuracy. These results highlight a trade-off among accuracy,
convergence speed, and energy use, showing that short-pulse programming with
tailored training significantly enhances on-chip learning efficiency.

</details>


### [368] [Integrating Stacked Intelligent Metasurfaces and Power Control for Dynamic Edge Inference via Over-The-Air Neural Networks](https://arxiv.org/abs/2509.18906)
*Kyriakos Stylianopoulos,George C. Alexandropoulos*

Main category: cs.ET

TL;DR: 提出用于边缘推理的新框架，利用堆叠智能超表面控制无线传播，将系统建模为DNN，结合模块调整功率，能平衡分类精度与功耗，提高能效。


<details>
  <summary>Details</summary>
Motivation: 突破将无线信道视为噪声的传统做法，减少边缘推理的计算和通信开销。

Method: 利用堆叠智能超表面控制无线传播，将发射机 - 信道 - 接收机系统建模为端到端DNN，加入专用DNN模块根据用户位置信息动态调整发射功率。

Result: 所提出的集成超表面的DNN框架在不同场景下能平衡分类精度和功耗。

Conclusion: 该框架可显著提高能源效率。

Abstract: This paper introduces a novel framework for Edge Inference (EI) that bypasses
the conventional practice of treating the wireless channel as noise. We utilize
Stacked Intelligent Metasurfaces (SIMs) to control wireless propagation,
enabling the channel itself to perform over-the-air computation. This
eliminates the need for symbol estimation at the receiver, significantly
reducing computational and communication overhead. Our approach models the
transmitter-channel-receiver system as an end-to-end Deep Neural Network (DNN)
where the response of the SIM elements are trainable parameters. To address
channel variability, we incorporate a dedicated DNN module responsible for
dynamically adjusting transmission power leveraging user location information.
Our performance evaluations showcase that the proposed metasurfaces-integrated
DNN framework with deep SIM architectures are capable of balancing
classification accuracy and power consumption under diverse scenarios, offering
significant energy efficiency improvements.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [369] [Optimization-centric cutting feedback for semiparametric models](https://arxiv.org/abs/2509.18708)
*Linda S. L. Tan,David J. Nott,David T. Frazier*

Main category: stat.ME

TL;DR: 本文提出优化中心方法用于半参数模块化推理，用Rényi散度定义广义截断后验，在实证中比KLD更稳健，还推导了新后验集中结果并在实例中验证。


<details>
  <summary>Details</summary>
Motivation: 联合贝叶斯推理在模块化推理中易受模型误设影响，非参数和参数组件的先验可能相互影响推理效果。

Method: 提出“优化中心”方法，用基于Rényi散度的变分优化问题定义广义截断后验，并开发变分计算方法。

Result: 实证表明用Rényi散度定义截断后验比KLD有更稳健推理；推导了适应Rényi散度和半参数组件的新后验集中结果。

Conclusion: 所提方法能解决模型误设和先验 - 数据冲突问题，在实例中得到验证，拓展了现有截断后验结果。

Abstract: Modern statistics deals with complex models from which the joint model used
for inference is built by coupling submodels, called modules. We consider
modular inference where the modules may depend on parametric and nonparametric
components. In such cases, a joint Bayesian inference is highly susceptible to
misspecification across any module, and inappropriate priors for nonparametric
components may deliver subpar inferences for parametric components, and vice
versa. We propose a novel ``optimization-centric'' approach to cutting feedback
for semiparametric modular inference, which can address misspecification and
prior-data conflicts. The proposed generalized cut posteriors are defined
through a variational optimization problem for generalized posteriors where
regularization is based on R\'{e}nyi divergence, rather than Kullback-Leibler
divergence (KLD), and variational computational methods are developed. We show
empirically that using R\'{e}nyi divergence to define the cut posterior
delivers more robust inferences than KLD. We derive novel posterior
concentration results that accommodate the R\'{e}nyi divergence and allow for
semiparametric components, greatly extending existing results for cut
posteriors that were derived for parametric models and KLD. We demonstrate
these new methods in a benchmark toy example and two real examples: Gaussian
process adjustments for confounding in causal inference and misspecified copula
models with nonparametric marginals.

</details>


### [370] [Augmenting Limited and Biased RCTs through Pseudo-Sample Matching-Based Observational Data Fusion Method](https://arxiv.org/abs/2509.18148)
*Kairong Han,Weidong Huang,Taiyang Zhou,Peng Zhen,Kun Kuang*

Main category: stat.ME

TL;DR: 论文针对网约车定价中RCT数据少、质量低及现有数据融合方法难实施的问题，提出伪样本匹配的数据融合方法，经实验验证有效并强调提升RCT数据质量的重要性。


<details>
  <summary>Details</summary>
Motivation: 网约车定价中RCT成本高、数据少且质量低，现有数据融合方法难实施，影响对折扣效果评估和市场竞争结果。

Method: 提出伪样本匹配的经验数据融合方法，从有偏、低质量RCT数据生成伪样本并与观测数据匹配，扩大RCT数据集并减少异质性。

Result: 通过模拟实验、离线和在线测试验证方法有效性，在线实验一周利润提升0.41%。

Conclusion: 强调提升RCT数据质量在工业场景中的重要性。

Abstract: In the online ride-hailing pricing context, companies often conduct
randomized controlled trials (RCTs) and utilize uplift models to assess the
effect of discounts on customer orders, which substantially influences
competitive market outcomes. However, due to the high cost of RCTs, the
proportion of trial data relative to observational data is small, which only
accounts for 0.65\% of total traffic in our context, resulting in significant
bias when generalizing to the broader user base. Additionally, the complexity
of industrial processes reduces the quality of RCT data, which is often subject
to heterogeneity from potential interference and selection bias, making it
difficult to correct. Moreover, existing data fusion methods are challenging to
implement effectively in complex industrial settings due to the high
dimensionality of features and the strict assumptions that are hard to verify
with real-world data. To address these issues, we propose an empirical data
fusion method called pseudo-sample matching. By generating pseudo-samples from
biased, low-quality RCT data and matching them with the most similar samples
from large-scale observational data, the method expands the RCT dataset while
mitigating its heterogeneity. We validated the method through simulation
experiments, conducted offline and online tests using real-world data. In a
week-long online experiment, we achieved a 0.41\% improvement in profit, which
is a considerable gain when scaled to industrial scenarios with hundreds of
millions in revenue. In addition, we discuss the harm to model training,
offline evaluation, and online economic benefits when the RCT data quality is
not high, and emphasize the importance of improving RCT data quality in
industrial scenarios. Further details of the simulation experiments can be
found in the GitHub repository https://github.com/Kairong-Han/Pseudo-Matching.

</details>


### [371] [Enhanced Survival Trees](https://arxiv.org/abs/2509.18494)
*Ruiwen Zhou,Ke Xie,Lei Liu,Zhichen Xu,Jimin Ding,Xiaogang Su*

Main category: stat.ME

TL;DR: 本文提出新的生存树方法处理删失失效时间数据，有三方面改进，经模拟研究和实际数据验证。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理删失失效时间数据时有不足，需改进计算效率、减少变量选择偏差、优化树结构确定和进行有效推断。

Method: 1. 开发更高效分裂程序并提出交叉验证策略；2. 通过融合正则化确定树结构；3. 用基于自助法的偏差校正构建置信区间。

Result: 通过广泛模拟研究评估，并用ADNI研究数据进行说明。

Conclusion: 提出的新生存树方法在处理删失失效时间数据上有显著改进，具有更好性能。

Abstract: We introduce a new survival tree method for censored failure time data that
incorporates three key advancements over traditional approaches. First, we
develop a more computationally efficient splitting procedure that effectively
mitigates the end-cut preference problem, and we propose an intersected
validation strategy to reduce the variable selection bias inherent in greedy
searches. Second, we present a novel framework for determining tree structures
through fused regularization. In combination with conventional pruning, this
approach enables the merging of non-adjacent terminal nodes, producing more
parsimonious and interpretable models. Third, we address inference by
constructing valid confidence intervals for median survival times within the
subgroups identified by the final tree. To achieve this, we apply
bootstrap-based bias correction to standard errors. The proposed method is
assessed through extensive simulation studies and illustrated with data from
the Alzheimer's Disease Neuroimaging Initiative (ADNI) study.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [372] [Reconstruction of Optical Coherence Tomography Images from Wavelength-space Using Deep-learning](https://arxiv.org/abs/2509.18783)
*Maryam Viqar,Erdem Sahin,Elena Stoykova,Violeta Madjarova*

Main category: physics.optics

TL;DR: 提出基于深度学习的方法直接从波长域重建减少散斑的OCT图像，使用两个CNN网络依次处理，证明方法有效并降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统FD - OCT系统重采样到波数域提取深度剖面需额外硬件资源或增加计算复杂度，且图像有散斑噪声。

Method: 使用深度学习方法，依次采用SD - CNN和FD - CNN两个编码器 - 解码器风格网络进行重建。

Result: 定量和直观地证明了该方法能获得高质量OCT图像，降低了计算复杂度。

Conclusion: 这项工作为OCT图像重建领域的进一步创新奠定了框架。

Abstract: Conventional Fourier-domain Optical Coherence Tomography (FD-OCT) systems
depend on resampling into wavenumber (k) domain to extract the depth profile.
This either necessitates additional hardware resources or amplifies the
existing computational complexity. Moreover, the OCT images also suffer from
speckle noise, due to systemic reliance on low coherence interferometry. We
propose a streamlined and computationally efficient approach based on
Deep-Learning (DL) which enables reconstructing speckle-reduced OCT images
directly from the wavelength domain. For reconstruction, two encoder-decoder
styled networks namely Spatial Domain Convolution Neural Network (SD-CNN) and
Fourier Domain CNN (FD-CNN) are used sequentially. The SD-CNN exploits the
highly degraded images obtained by Fourier transforming the domain fringes to
reconstruct the deteriorated morphological structures along with suppression of
unwanted noise. The FD-CNN leverages this output to enhance the image quality
further by optimization in Fourier domain (FD). We quantitatively and visually
demonstrate the efficacy of the method in obtaining high-quality OCT images.
Furthermore, we illustrate the computational complexity reduction by harnessing
the power of DL models. We believe that this work lays the framework for
further innovations in the realm of OCT image reconstruction.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [373] [Chiplet-Based RISC-V SoC with Modular AI Acceleration](https://arxiv.org/abs/2509.18355)
*P. Ramkumar,S. S. Bharadwaj*

Main category: cs.AR

TL;DR: 本文提出基于小芯片的RISC - V SoC架构，集成四项创新，实验显示性能显著提升，证明模块化小芯片设计优势。


<details>
  <summary>Details</summary>
Motivation: 在边缘AI设备开发和部署中，实现高性能、能效、成本效益与架构灵活性的平衡是关键挑战，单片SoC设计因制造良率低难以应对。

Method: 提出基于小芯片的RISC - V SoC架构，在硅中介层集成四项创新，包括自适应DVFS、AI感知UCIe协议扩展、分布式加密安全和智能负载迁移，集成7nm RISC - V CPU小芯片、5nm AI加速器等。

Result: 在行业标准基准测试中，AI优化配置相比之前基本小芯片实现，延迟降低14.7%，吞吐量提高17.3%，功耗降低16.2%，效率提升40.1%，保持亚5ms实时处理能力。

Conclusion: 模块化小芯片设计能实现接近单片的计算密度，同时具备成本效益、可扩展性和可升级性，对下一代边缘AI设备应用至关重要。

Abstract: Achieving high performance, energy efficiency, and cost-effectiveness while
maintaining architectural flexibility is a critical challenge in the
development and deployment of edge AI devices. Monolithic SoC designs struggle
with this complex balance mainly due to low manufacturing yields (below 16%) at
advanced 360 mm^2 process nodes. This paper presents a novel chiplet-based
RISC-V SoC architecture that addresses these limitations through modular AI
acceleration and intelligent system level optimization. Our proposed design
integrates 4 different key innovations in a 30mm x 30mm silicon interposer:
adaptive cross-chiplet Dynamic Voltage and Frequency Scaling (DVFS); AI-aware
Universal Chiplet Interconnect Express (UCIe) protocol extensions featuring
streaming flow control units and compression-aware transfers; distributed
cryptographic security across heterogeneous chiplets; and intelligent
sensor-driven load migration. The proposed architecture integrates a 7nm RISC-V
CPU chiplet with dual 5nm AI accelerators (15 TOPS INT8 each), 16GB HBM3 memory
stacks, and dedicated power management controllers. Experimental results across
industry standard benchmarks like MobileNetV2, ResNet-50 and real-time video
processing demonstrate significant performance improvements. The AI-optimized
configuration achieves ~14.7% latency reduction, 17.3% throughput improvement,
and 16.2% power reduction compared to previous basic chiplet implementations.
These improvements collectively translate to a 40.1% efficiency gain
corresponding to ~3.5 mJ per MobileNetV2 inference (860 mW/244 images/s), while
maintaining sub-5ms real-time capability across all experimented workloads.
These performance upgrades demonstrate that modular chiplet designs can achieve
near-monolithic computational density while enabling cost efficiency,
scalability and upgradeability, crucial for next-generation edge AI device
applications.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [374] [Whack-a-Mole: Deterministic Packet Spraying Across Multiple Network Paths](https://arxiv.org/abs/2509.18519)
*Michael Luby,John Byers*

Main category: cs.NI

TL;DR: 介绍了确定性包喷洒算法Whack - a - Mole，可跨多网络路径分配数据包，有低差异边界，适用于AI/ML工作负载。


<details>
  <summary>Details</summary>
Motivation: 大规模分布式AI/ML训练和推理工作负载对尾延迟和传输不平衡敏感，需优化集体完成时间和有效训练时间比。

Method: 将路径配置文件表示为m个选择单元在n条路径上的离散分配，用位反转计数器为每个数据包选择路径，根据拥塞反馈调整路径分配。

Result: 证明每条路径的预期和实际数据包数量之间的差异在任何连续数据包序列上受O(log m)限制。

Conclusion: 该算法具有确定性分布、低每包开销和与擦除编码传输兼容的特点，是多路径传输协议的有效构建块，可最小化集体完成时间并最大化GPU利用率。

Abstract: We present Whack-a-Mole, a deterministic packet spraying algorithm for
distributing packets across multiple network paths with provably tight
discrepancy bounds. The algorithm is motivated by large-scale distributed AI/ML
training and inference workloads, where collective completion time (CCT) and
effective training time ratio (ETTR) are highly sensitive to tail latency and
transport imbalance. Whack-a-Mole represents the path profile as a discrete
allocation of $m$ selection units across $n$ paths and uses a bit-reversal
counter to choose a path for each packet. We prove that the discrepancy between
expected and actual packet counts per path is bounded by $O(\log m)$ over any
contiguous packet sequence. The algorithm responds quickly to congestion
feedback by reducing allocations to degraded paths and redistributing load to
healthier ones. This combination of deterministic distribution, low per-packet
overhead, and compatibility with erasure-coded transport makes Whack-a-Mole an
effective building block for multipath transport protocols that aim to minimize
CCT and maximize GPU utilization.

</details>


### [375] [Accurate and Efficient Prediction of Wi-Fi Link Quality Based on Machine Learning](https://arxiv.org/abs/2509.18933)
*Gabriele Formis,Gianluca Cena,Lukasz Wisniewski,Stefano Scanzio*

Main category: cs.NI

TL;DR: 文章用机器学习技术分析多种预测模型，评估基于指数移动平均线性组合的数据驱动模型，通过真实测试数据评估准确性，发现通道独立模型表现佳，为提升工业环境Wi-Fi可靠性提供见解。


<details>
  <summary>Details</summary>
Motivation: 无线通信不可预测性影响通信质量，需准确高效的Wi-Fi链路质量预测。

Method: 用机器学习技术评估基于指数移动平均线性组合的数据驱动模型，用真实Wi-Fi测试台实验数据评估准确性，考虑通道相关和独立训练数据。

Result: 通道独立模型表现有竞争力，可由设备制造商进行通用训练。

Conclusion: 为工业环境中基于机器学习的预测模型实际部署以提升Wi-Fi可靠性提供见解。

Abstract: Wireless communications are characterized by their unpredictability, posing
challenges for maintaining consistent communication quality. This paper
presents a comprehensive analysis of various prediction models, with a focus on
achieving accurate and efficient Wi-Fi link quality forecasts using machine
learning techniques. Specifically, the paper evaluates the performance of
data-driven models based on the linear combination of exponential moving
averages, which are designed for low-complexity implementations and are then
suitable for hardware platforms with limited processing resources. Accuracy of
the proposed approaches was assessed using experimental data from a real-world
Wi-Fi testbed, considering both channel-dependent and channel-independent
training data. Remarkably, channel-independent models, which allow for
generalized training by equipment manufacturers, demonstrated competitive
performance. Overall, this study provides insights into the practical
deployment of machine learning-based prediction models for enhancing Wi-Fi
dependability in industrial environments.

</details>


### [376] [Online Learning for Optimizing AoI-Energy Tradeoff under Unknown Channel Statistics](https://arxiv.org/abs/2509.18654)
*Mohamed A. Abd-Elmagid,Ming Shi,Eylem Ekici,Ness B. Shroff*

Main category: cs.NI

TL;DR: 本文针对源节点能量受限的实时监控系统，在信道统计信息未知场景下，开发基于在线学习、有有限时间保证的算法优化传输成本和信息新鲜度权衡，算法能实现序最优遗憾。


<details>
  <summary>Details</summary>
Motivation: 现有文献在优化传输成本和信息新鲜度（AoI）权衡时假设信道统计信息已知，本文要解决信道统计信息未知的实际场景问题。

Method: 先证明信道统计信息已知时最优调度策略有基于AoI值的阈值结构，再利用该洞见开发学习算法。

Result: 所提出的学习算法能实现关于时间跨度长度的序最优遗憾（O(1)）。

Conclusion: 开发的在线学习算法可在信道统计信息未知场景下有效优化传输成本和AoI性能的权衡。

Abstract: We consider a real-time monitoring system where a source node (with energy
limitations) aims to keep the information status at a destination node as fresh
as possible by scheduling status update transmissions over a set of channels.
The freshness of information at the destination node is measured in terms of
the Age of Information (AoI) metric. In this setting, a natural tradeoff exists
between the transmission cost (or equivalently, energy consumption) of the
source and the achievable AoI performance at the destination. This tradeoff has
been optimized in the existing literature under the assumption of having a
complete knowledge of the channel statistics. In this work, we develop online
learning-based algorithms with finite-time guarantees that optimize this
tradeoff in the practical scenario where the channel statistics are unknown to
the scheduler. In particular, when the channel statistics are known, the
optimal scheduling policy is first proven to have a threshold-based structure
with respect to the value of AoI (i.e., it is optimal to drop updates when the
AoI value is below some threshold). This key insight was then utilized to
develop the proposed learning algorithms that surprisingly achieve an
order-optimal regret (i.e., $O(1)$) with respect to the time horizon length.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [377] [Forest tree species classification and entropy-derived uncertainty mapping using extreme gradient boosting and Sentinel-1/2 data](https://arxiv.org/abs/2509.18228)
*Abdulhakim M. Abdi,Fan Wang*

Main category: q-bio.QM

TL;DR: 本文利用卫星数据和实地观测生成瑞典森林优势树种10米地图并给出像素级不确定性估计，模型精度较高。


<details>
  <summary>Details</summary>
Motivation: 生成瑞典森林优势树种地图并提供不确定性估计。

Method: 基于哨兵卫星数据时空指标和瑞典国家森林清单实地观测，使用带贝叶斯优化的极端梯度提升模型，用香农熵量化分类不确定性。

Result: 最终模型总体准确率85%，F1分数0.82，马修斯相关系数0.81，树种分布与官方统计强相关（r = 0.96）。

Conclusion: 该方法能有效生成瑞典森林优势树种地图及不确定性估计，结果准确。

Abstract: We present a new 10-meter map of dominant tree species in Swedish forests
accompanied by pixel-level uncertainty estimates. The tree species
classification is based on spatiotemporal metrics derived from Sentinel-1 and
Sentinel-2 satellite data, combined with field observations from the Swedish
National Forest Inventory. We apply an extreme gradient boosting model with
Bayesian optimization to relate field observations to satellite-derived
features and generate the final species map. Classification uncertainty is
quantified using Shannon's entropy of the predicted class probabilities, which
provide a spatially explicit measure of model confidence. The final model
achieved an overall accuracy of 85% (F1 score = 0.82, Matthews correlation
coefficient = 0.81), and mapped species distributions showed strong agreement
with official forest statistics (r = 0.96).

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [378] [A failure mode dependent continuum damage model for laminated composites with optimized model parameters : Application to curved beams](https://arxiv.org/abs/2509.19051)
*Shubham Rai,Badri Prasad Patel*

Main category: physics.comp-ph

TL;DR: 提出基于多项式损伤硬化函数的连续损伤模型用于层合复合板建模，优化参数并用于曲梁损伤预测，与现有模型对比，验证新模型有效性。


<details>
  <summary>Details</summary>
Motivation: 为层合复合板的连续损伤建模提供有效方法。

Method: 提出含多项式损伤硬化函数的损伤模型，基于实验曲线表征参数，用最速下降优化算法获取最优参数，用一阶剪切变形理论建模曲梁，有限元法结合牛顿 - 拉夫逊法求解方程，与现有模型对比。

Result: 新模型能捕捉载荷 - 挠度曲线非线性，与材料单轴/剪切应力 - 应变响应和强度特性一致。

Conclusion: 所提出的模型在层合复合板损伤建模方面有效。

Abstract: In this article, a failure mode dependent and thermodynamically consistent
continuum damage model with polynomial-based damage hardening functions is
proposed for continuum damage modeling of laminated composite panels. The
damage model parameters are characterized based on all uniaxial/shear
experimental stress-strain curves. Steepest descent optimization algorithm is
used to minimize the difference between model predicted and experimental
stress-strain curves to get the optimzed model parameters. The fully
characterized damage evolution equations are used for damage prediction of a
moderately thick laminated composite curved beam modeled using first-order
shear deformation theory. Finite element method with load control is used to
get the non-linear algebraic equations which are solved using Newton Raphson
method. The developed model is compared with the existing failure mode
dependent and failure mode independent damage models. The results depict the
efficacy of the proposed model to capture non-linearity in the load vs
deflection curve due to stiffness degradation and different damage in tension
andcompression consistent with uniaxial/shear stress-strain response and
strength properties of the material, respectively.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [379] [Measurement Score-Based MRI Reconstruction with Automatic Coil Sensitivity Estimation](https://arxiv.org/abs/2509.18402)
*Tingjun Liu,Chicago Y. Park,Yuyang Hu,Hongyu An,Ulugbek S. Kamilov*

Main category: eess.IV

TL;DR: 提出无校准测量分数扩散模型C - MSM，可消除对预校准线圈灵敏度图和真实图像的依赖，在多线圈脑fastMRI数据集实验中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的逆问题求解器（DIS）依赖预校准线圈灵敏度图和真实图像，在实际应用中不实用。

Method: 提出C - MSM方法，直接从k空间数据联合进行自动线圈灵敏度图估计和测量分数的自监督学习，通过对部分测量后验分数随机采样近似完整后验分布来重建图像，同时估计线圈灵敏度图。

Result: 在多线圈脑fastMRI数据集实验中，C - MSM即使无干净训练数据和预校准线圈灵敏度图，重建性能也接近具有干净扩散先验的DIS。

Conclusion: C - MSM能有效解决DIS依赖问题，实现较好的图像重建。

Abstract: Diffusion-based inverse problem solvers (DIS) have recently shown outstanding
performance in compressed-sensing parallel MRI reconstruction by combining
diffusion priors with physical measurement models. However, they typically rely
on pre-calibrated coil sensitivity maps (CSMs) and ground truth images, making
them often impractical: CSMs are difficult to estimate accurately under heavy
undersampling and ground-truth images are often unavailable. We propose
Calibration-free Measurement Score-based diffusion Model (C-MSM), a new method
that eliminates these dependencies by jointly performing automatic CSM
estimation and self-supervised learning of measurement scores directly from
k-space data. C-MSM reconstructs images by approximating the full posterior
distribution through stochastic sampling over partial measurement posterior
scores, while simultaneously estimating CSMs. Experiments on the multi-coil
brain fastMRI dataset show that C-MSM achieves reconstruction performance close
to DIS with clean diffusion priors -- even without access to clean training
data and pre-calibrated CSMs.

</details>


### [380] [Efficient Breast and Ovarian Cancer Classification via ViT-Based Preprocessing and Transfer Learning](https://arxiv.org/abs/2509.18553)
*Richa Rawat,Faisal Ahmed*

Main category: eess.IV

TL;DR: 本文提出基于视觉变换器（ViT）的方法检测和分类乳腺癌与卵巢癌，模型在基准数据集上表现优于现有方法，凸显ViT迁移学习与预处理在肿瘤诊断中的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统癌症检测方法劳动密集、耗时且依赖专业病理学家，需更高效方法。

Method: 使用预训练的ViT - Base - Patch16 - 224模型，针对二分类和多分类任务微调，采用预处理流程将图像转换为PyTorch张量。

Result: 模型在二分类中超越现有CNN、ViT和拓扑数据分析方法，多分类中优于近期拓扑方法。

Conclusion: ViT迁移学习结合高效预处理在肿瘤诊断中有效。

Abstract: Cancer is one of the leading health challenges for women, specifically breast
and ovarian cancer. Early detection can help improve the survival rate through
timely intervention and treatment. Traditional methods of detecting cancer
involve manually examining mammograms, CT scans, ultrasounds, and other imaging
types. However, this makes the process labor-intensive and requires the
expertise of trained pathologists. Hence, making it both time-consuming and
resource-intensive. In this paper, we introduce a novel vision transformer
(ViT)-based method for detecting and classifying breast and ovarian cancer. We
use a pre-trained ViT-Base-Patch16-224 model, which is fine-tuned for both
binary and multi-class classification tasks using publicly available
histopathological image datasets. Further, we use a preprocessing pipeline that
converts raw histophological images into standardized PyTorch tensors, which
are compatible with the ViT architecture and also help improve the model
performance. We evaluated the performance of our model on two benchmark
datasets: the BreakHis dataset for binary classification and the UBC-OCEAN
dataset for five-class classification without any data augmentation. Our model
surpasses existing CNN, ViT, and topological data analysis-based approaches in
binary classification. For multi-class classification, it is evaluated against
recent topological methods and demonstrates superior performance. Our study
highlights the effectiveness of Vision Transformer-based transfer learning
combined with efficient preprocessing in oncological diagnostics.

</details>


### [381] [MOIS-SAM2: Exemplar-based Segment Anything Model 2 for multilesion interactive segmentation of neurobromas in whole-body MRI](https://arxiv.org/abs/2509.19277)
*Georgii Kolokolnikov,Marie-Lena Schmalhofer,Sophie Götz,Lennart Well,Said Farschtschi,Victor-Felix Mautner,Inka Ristow,Rene Werner*

Main category: eess.IV

TL;DR: 本文提出MOIS - SAM2模型用于全身MRI中神经纤维瘤的交互式分割，在多测试集表现良好，可集成到临床工作流。


<details>
  <summary>Details</summary>
Motivation: 神经纤维瘤病1型患者全身神经纤维瘤检测和监测需全身MRI，现有交互式分割方法无法兼顾高精度和可扩展性，因此提出新模型。

Method: 引入MOIS - SAM2模型，基于SAM2扩展，采用基于示例的语义传播，在84名患者的119份全身MRI扫描数据上训练和评估，分患者级别划分训练集和四个测试集。

Result: 在域内测试集上，MOIS - SAM2扫描级DSC达0.60，优于基线模型；在多种域偏移场景下性能保持或提升，病变检测F1分数0.62 - 0.78，模型与专家的一致性与专家间一致性相当。

Conclusion: MOIS - SAM2能以最少用户输入实现高效可扩展的神经纤维瘤交互式分割，泛化性强，可集成到临床工作流。

Abstract: Background and Objectives: Neurofibromatosis type 1 is a genetic disorder
characterized by the development of numerous neurofibromas (NFs) throughout the
body. Whole-body MRI (WB-MRI) is the clinical standard for detection and
longitudinal surveillance of NF tumor growth. Existing interactive segmentation
methods fail to combine high lesion-wise precision with scalability to hundreds
of lesions. This study proposes a novel interactive segmentation model tailored
to this challenge.
  Methods: We introduce MOIS-SAM2, a multi-object interactive segmentation
model that extends the state-of-the-art, transformer-based, promptable Segment
Anything Model 2 (SAM2) with exemplar-based semantic propagation. MOIS-SAM2 was
trained and evaluated on 119 WB-MRI scans from 84 NF1 patients acquired using
T2-weighted fat-suppressed sequences. The dataset was split at the patient
level into a training set and four test sets (one in-domain and three
reflecting different domain shift scenarios, e.g., MRI field strength
variation, low tumor burden, differences in clinical site and scanner vendor).
  Results: On the in-domain test set, MOIS-SAM2 achieved a scan-wise DSC of
0.60 against expert manual annotations, outperforming baseline 3D nnU-Net (DSC:
0.54) and SAM2 (DSC: 0.35). Performance of the proposed model was maintained
under MRI field strength shift (DSC: 0.53) and scanner vendor variation (DSC:
0.50), and improved in low tumor burden cases (DSC: 0.61). Lesion detection F1
scores ranged from 0.62 to 0.78 across test sets. Preliminary inter-reader
variability analysis showed model-to-expert agreement (DSC: 0.62-0.68),
comparable to inter-expert agreement (DSC: 0.57-0.69).
  Conclusions: The proposed MOIS-SAM2 enables efficient and scalable
interactive segmentation of NFs in WB-MRI with minimal user input and strong
generalization, supporting integration into clinical workflows.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [382] [On Multi-entity, Multivariate Quickest Change Point Detection](https://arxiv.org/abs/2509.18310)
*Bahar Kor,Bipin Gaikwad,Abani Patra,Eric L. Miller*

Main category: eess.SP

TL;DR: 提出在线CPD框架，用IDfN和SWAS检测复杂环境系统行为变化，评估显示方法有效，还引入新数据集。


<details>
  <summary>Details</summary>
Motivation: 传统传感方法在人群监测等应用中不可行，需检测复杂动态环境系统行为变化。

Method: 引入IDfN概念，通过自编码器计算，用均值、方差和KDE聚合得到SWAS，用统计偏差和CUSUM技术检测变化。

Result: 在合成数据集和人群模拟上评估，能准确检测系统级变化，提供可扩展和保护隐私的解决方案。

Conclusion: 方法有效，填补复杂集体交互系统CPD评估数据集的空白。

Abstract: We propose a framework for online Change Point Detection (CPD) from
multi-entity, multivariate time series data, motivated by applications in crowd
monitoring where traditional sensing methods (e.g., video surveillance) may be
infeasible. Our approach addresses the challenge of detecting system-wide
behavioral shifts in complex, dynamic environments where the number and
behavior of individual entities may be uncertain or evolve. We introduce the
concept of Individual Deviation from Normality (IDfN), computed via a
reconstruction-error-based autoencoder trained on normal behavior. We aggregate
these individual deviations using mean, variance, and Kernel Density Estimates
(KDE) to yield a System-Wide Anomaly Score (SWAS). To detect persistent or
abrupt changes, we apply statistical deviation metrics and the Cumulative Sum
(CUSUM) technique to these scores. Our unsupervised approach eliminates the
need for labeled data or feature extraction, enabling real-time operation on
streaming input. Evaluations on both synthetic datasets and crowd simulations,
explicitly designed for anomaly detection in group behaviors, demonstrate that
our method accurately detects significant system-level changes, offering a
scalable and privacy-preserving solution for monitoring complex multi-agent
systems. In addition to this methodological contribution, we introduce new,
challenging multi-entity multivariate time series datasets generated from crowd
simulations in Unity and coupled nonlinear oscillators. To the best of our
knowledge, there is currently no publicly available dataset of this type
designed explicitly to evaluate CPD in complex collective and interactive
systems, highlighting an essential gap that our work addresses.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [383] [Circuit Complexity From Physical Constraints: Scaling Limitations of Attention](https://arxiv.org/abs/2509.19161)
*Benjamin Prada,Ankur Mali*

Main category: cs.CC

TL;DR: 指出标准电路复杂度度量实用性有限，定义新的局部均匀性概念和电路复杂度类RC(·)，分析注意力机制扩展性及变压器表达性界限。


<details>
  <summary>Details</summary>
Motivation: 标准电路复杂度度量提供的实际信息有限，不足以区分模型表达性。

Method: 定义新的局部均匀性概念和电路复杂度类RC(·)。

Result: 表明运行时间为ω(n^{3/2})的注意力机制无法适应日益复杂数据集的熵。

Conclusion: 提供了定义变压器表达性有意义界限的方法，揭示了注意力机制的局限性。

Abstract: We argue that the standard circuit complexity measures derived from $NC, AC,
TC$ provide limited practical information and are now insufficient to further
differentiate model expressivity. To address these new limitations, we define a
novel notion of local uniformity and a family of circuit complexity classes
$RC(\cdot)$ that capture the fundamental constraints of scaling physical
circuits. Through the lens of $RC(\cdot)$, we show that attention mechanisms
with $\omega(n^{3/2})$ runtime cannot scale to accommodate the entropy of
increasingly complex datasets. Our results simultaneously provide a methodology
for defining meaningful bounds on transformer expressivity and naturally expose
the restricted viability of attention.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [384] [BloomIntent: Automating Search Evaluation with LLM-Generated Fine-Grained User Intents](https://arxiv.org/abs/2509.18641)
*Yoonseo Choi,Eunhye Kim,Hyunwoo Kim,Donghyun Park,Honggu Lee,Jinyoung Kim,Juho Kim*

Main category: cs.HC

TL;DR: 提出以用户意图为评估单元的搜索评估方法BloomIntent，能生成细粒度意图并评估搜索结果，技术评估表现良好，案例研究展示其应用价值。


<details>
  <summary>Details</summary>
Motivation: 现有搜索评估方法难以代表和评估用户搜索目标的多样性，需新方法与细粒度用户意图对齐。

Method: 先基于用户属性和信息寻求意图类型的分类法生成合理、细粒度的搜索意图，再用大语言模型对每个意图自动评估搜索结果，聚类相似意图并在结构化界面总结评估结果。

Result: 技术评估显示BloomIntent能生成细粒度、可评估且现实的意图，意图级满意度评估与专家评估者达成72%的一致；案例研究表明它能帮助搜索专家识别模糊查询意图、发现未满足的用户需求等。

Conclusion: BloomIntent从查询级评估转向意图级评估，重新定义了搜索系统的评估方式，不仅关注性能，还关注服务多种用户目标的能力。

Abstract: If 100 people issue the same search query, they may have 100 different goals.
While existing work on user-centric AI evaluation highlights the importance of
aligning systems with fine-grained user intents, current search evaluation
methods struggle to represent and assess this diversity. We introduce
BloomIntent, a user-centric search evaluation method that uses user intents as
the evaluation unit. BloomIntent first generates a set of plausible,
fine-grained search intents grounded on taxonomies of user attributes and
information-seeking intent types. Then, BloomIntent provides an automated
evaluation of search results against each intent powered by large language
models. To support practical analysis, BloomIntent clusters semantically
similar intents and summarizes evaluation outcomes in a structured interface.
With three technical evaluations, we showed that BloomIntent generated
fine-grained, evaluable, and realistic intents and produced scalable
assessments of intent-level satisfaction that achieved 72% agreement with
expert evaluators. In a case study (N=4), we showed that BloomIntent supported
search specialists in identifying intents for ambiguous queries, uncovering
underserved user needs, and discovering actionable insights for improving
search experiences. By shifting from query-level to intent-level evaluation,
BloomIntent reimagines how search systems can be assessed -- not only for
performance but for their ability to serve a multitude of user goals.

</details>


### [385] [NaviSense: A Multimodal Assistive Mobile application for Object Retrieval by Persons with Visual Impairment](https://arxiv.org/abs/2509.18672)
*Ajay Narayanan Sridhar,Fuli Qiao,Nelson Daniel Troncoso Aldas,Yanpei Shi,Mehrdad Mahdavi,Laurent Itti,Vijaykrishnan Narayanan*

Main category: cs.HC

TL;DR: 提出NaviSense系统，结合多种技术支持开放世界物体检测与实时音触觉引导，经评估可减少物体检索时间且受青睐。


<details>
  <summary>Details</summary>
Motivation: 现有辅助技术在为视障人士定位和检索物体方面存在局限，如精确引导需预扫描或仅支持固定类别，开放世界物体识别缺乏空间反馈。

Method: 引入NaviSense系统，结合对话式AI、视觉语言模型、增强现实和LiDAR，通过自然语言指定物体并提供连续空间反馈，设计基于前期研究，对12名盲人和低视力参与者进行评估。

Result: NaviSense显著减少了物体检索时间，且比现有工具更受青睐。

Conclusion: 将开放世界感知与精确、易用的引导相结合具有价值。

Abstract: People with visual impairments often face significant challenges in locating
and retrieving objects in their surroundings. Existing assistive technologies
present a trade-off: systems that offer precise guidance typically require
pre-scanning or support only fixed object categories, while those with
open-world object recognition lack spatial feedback for reaching the object. To
address this gap, we introduce 'NaviSense', a mobile assistive system that
combines conversational AI, vision-language models, augmented reality (AR), and
LiDAR to support open-world object detection with real-time audio-haptic
guidance. Users specify objects via natural language and receive continuous
spatial feedback to navigate toward the target without needing prior setup.
Designed with insights from a formative study and evaluated with 12 blind and
low-vision participants, NaviSense significantly reduced object retrieval time
and was preferred over existing tools, demonstrating the value of integrating
open-world perception with precise, accessible guidance.

</details>


### [386] [When Ads Become Profiles: Large-Scale Audit of Algorithmic Biases and LLM Profiling Risks](https://arxiv.org/abs/2509.18874)
*Baiyu Chen,Benjamin Tag,Hao Xue,Daniel Angus,Flora Salim*

Main category: cs.HC

TL;DR: 本文介绍多阶段审计框架调查社交媒体自动广告定向风险，发现算法偏差和广告流可重构用户档案，凸显隐私风险。


<details>
  <summary>Details</summary>
Motivation: 社交媒体自动广告定向不透明，存在剥削和难以被外部审查的风险，LLMs 可能从广告曝光反向工程用户敏感属性。

Method: 引入多阶段审计框架，对超 43.5 万条广告展示进行大规模审计，用多模态 LLM 从广告流重构用户人口统计档案。

Result: 发现算法偏差，多模态 LLM 能从广告流重构用户档案，表现优于基于人口普查的基线。

Conclusion: 广告流构成丰富数字足迹，存在紧急隐私风险，需内容级审计和治理。

Abstract: Automated ad targeting on social media is opaque, creating risks of
exploitation and invisibility to external scrutiny. Users may be steered toward
harmful content while independent auditing of these processes remains blocked.
Large Language Models (LLMs) raise a new concern: the potential to
reverse-engineer sensitive user attributes from exposure alone. We introduce a
multi-stage auditing framework to investigate these risks. First, a large-scale
audit of over 435,000 ad impressions delivered to 891 Australian Facebook users
reveals algorithmic biases, including disproportionate Gambling and Politics
ads shown to socioeconomically vulnerable and politically aligned groups.
Second, a multimodal LLM can reconstruct users' demographic profiles from ad
streams, outperforming census-based baselines and matching or exceeding human
performance. Our results provide the first empirical evidence that ad streams
constitute rich digital footprints for public AI inference, highlighting urgent
privacy risks and the need for content-level auditing and governance.

</details>


### [387] [YAC: Bridging Natural Language and Interactive Visual Exploration with Generative AI for Biomedical Data Discovery](https://arxiv.org/abs/2509.19182)
*Devin Lange,Shanghua Gao,Pengwei Sui,Austen Money,Priya Misner,Marinka Zitnik,Nils Gehlenborg*

Main category: cs.HC

TL;DR: 论文介绍原型系统YAC，结合自然语言与交互可视化，通过多智能体系统生成结构化输出以渲染可视化和应用过滤器，还设小部件，分析技术维度并展示四个使用场景。


<details>
  <summary>Details</summary>
Motivation: 结合自然语言输入提升生物医学数据发现界面能力，同时保留用户界面元素和可视化的作用。

Method: 用多智能体系统生成结构化声明性输出，解释输出以渲染链接的交互式可视化和应用数据过滤器，设置小部件让用户通过界面元素调整输出值。

Result: 构建了原型系统YAC，并对其技术维度进行分析。

Conclusion: 通过多智能体系统和小部件等方式，YAC系统能有效桥接自然语言与交互式可视化。

Abstract: Incorporating natural language input has the potential to improve the
capabilities of biomedical data discovery interfaces. However, user interface
elements and visualizations are still powerful tools for interacting with data,
even in the new world of generative AI. In our prototype system, YAC, Yet
Another Chatbot, we bridge the gap between natural language and interactive
visualizations by generating structured declarative output with a multi-agent
system and interpreting that output to render linked interactive visualizations
and apply data filters. Furthermore, we include widgets, which allow users to
adjust the values of that structured output through user interface elements. We
reflect on the capabilities and design of this system with an analysis of its
technical dimensions and illustrate the capabilities through four usage
scenarios.

</details>
